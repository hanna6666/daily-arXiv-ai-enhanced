{"id": "2512.17940", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.17940", "abs": "https://arxiv.org/abs/2512.17940", "authors": ["Xi Wang", "Jing Liu", "Siqian Li", "Hengtai Dai", "Jung-Che Chang", "Adam Rushworth", "Xin Dong"], "title": "Untethered thin dielectric elastomer actuated soft robot", "comment": null, "summary": "Thin dielectric elastomer actuator (DEA) features a unique in-plane configuration, enabling low-profile designs capable of accessing millimetre-scale narrow spaces. However, most existing DEA-powered soft robots require high voltages and wired power connections, limiting their ability to operate in confined environments. This study presents an untethered thin soft robot (UTS-Robot) powered by thin dielectric elastomer actuators (TS-DEA). The robot measures 38 mm in length, 6 mm in height, and weighs just 2.34 grams, integrating flexible onboard electronics to achieve fully untethered actuation. The TS-DEA, operating at resonant frequencies of 86 Hz under a low driving voltage of 220 V, adopts a dual-actuation sandwiched structure, comprising four dielectric elastomer layers bonded to a compressible tensioning mechanism at its core. This design enables high power density actuation and locomotion via three directional friction pads. The low-voltage actuation is achieved by fabricating each elastomer layer via spin coating to an initial thickness of 50 um, followed by biaxial stretching to 8 um. A comprehensive design and modelling framework has been developed to optimise TS-DEA performance. Experimental evaluations demonstrate that the bare TS-DEA achieves a locomotion speed of 12.36 mm/s at resonance, the untethered configuration achieves a locomotion speed of 0.5 mm/s, making it highly suitable for navigating confined and complex environments.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u65e0\u675f\u7f1a\u8584\u8f6f\u673a\u5668\u4eba\uff08UTS-Robot\uff09\uff0c\u5229\u7528\u8584\u4ecb\u7535\u5f39\u6027\u4f53\u9a71\u52a8\u5668\uff0c\u5b9e\u73b0\u4f4e\u7535\u538b\u9a71\u52a8\u548c\u9ad8\u6548\u8fd0\u52a8\uff0c\u9002\u5408\u5728\u590d\u6742\u73af\u5883\u4e2d\u5bfc\u822a\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u73b0\u6709DEA\u9a71\u52a8\u8f6f\u673a\u5668\u4eba\u5728\u72ed\u5c0f\u73af\u5883\u4e2d\u53d7\u9650\u7684\u5de5\u4f5c\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u7535\u538b\u548c\u8fde\u63a5\u8981\u6c42\u65b9\u9762\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u8584\u4ecb\u7535\u5f39\u6027\u4f53\u9a71\u52a8\u5668\uff08TS-DEA\uff09\u548c\u96c6\u6210\u529f\u80fd\u7535\u5b50\u8bbe\u5907\uff0c\u5b9e\u73b0\u5168\u81ea\u4e3b\u65e0\u675f\u7f1a\u8fd0\u52a8\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u65e0\u675f\u7f1a\u7684TS-DEA\u914d\u7f6e\u4e0b\uff0c\u8be5\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u7a33\u5b9a\u7684\u8fd0\u52a8\u80fd\u529b\u3002", "conclusion": "UTS-Robot\u5c55\u793a\u4e86\u5728\u590d\u6742\u7a7a\u95f4\u4e2d\u9ad8\u6548\u8fd0\u52a8\u7684\u6f5c\u529b\uff0c\u5177\u5907\u826f\u597d\u7684\u6027\u80fd\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2512.17958", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.17958", "abs": "https://arxiv.org/abs/2512.17958", "authors": ["Farida Mohsen", "Ali Safa"], "title": "Real-Time Human-Robot Interaction Intent Detection Using RGB-based Pose and Emotion Cues with Cross-Camera Model Generalization", "comment": null, "summary": "Service robots in public spaces require real-time understanding of human behavioral intentions for natural interaction. We present a practical multimodal framework for frame-accurate human-robot interaction intent detection that fuses camera-invariant 2D skeletal pose and facial emotion features extracted from monocular RGB video. Unlike prior methods requiring RGB-D sensors or GPU acceleration, our approach resource-constrained embedded hardware (Raspberry Pi 5, CPU-only). To address the severe class imbalance in natural human-robot interaction datasets, we introduce a novel approach to synthesize temporally coherent pose-emotion-label sequences for data re-balancing called MINT-RVAE (Multimodal Recurrent Variational Autoencoder for Intent Sequence Generation). Comprehensive offline evaluations under cross-subject and cross-scene protocols demonstrate strong generalization performance, achieving frame- and sequence-level AUROC of 0.95. Crucially, we validate real-world generalization through cross-camera evaluation on the MIRA robot head, which employs a different onboard RGB sensor and operates in uncontrolled environments not represented in the training data. Despite this domain shift, the deployed system achieves 91% accuracy and 100% recall across 32 live interaction trials. The close correspondence between offline and deployed performance confirms the cross-sensor and cross-environment robustness of the proposed multimodal approach, highlighting its suitability for ubiquitous multimedia-enabled social robots.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6444\u50cf\u673a\u65e0\u5173\u76842D\u59ff\u52bf\u548c\u9762\u90e8\u60c5\u611f\u7279\u5f81\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u610f\u56fe\u68c0\u6d4b\uff0c\u540c\u65f6\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u8de8\u573a\u666f\u548c\u8de8\u4f20\u611f\u5668\u9c81\u68d2\u6027\u3002", "motivation": "\u516c\u5171\u573a\u6240\u7684\u670d\u52a1\u673a\u5668\u4eba\u9700\u8981\u5b9e\u65f6\u7406\u89e3\u4eba\u7c7b\u884c\u4e3a\u610f\u56fe\uff0c\u4ee5\u5b9e\u73b0\u81ea\u7136\u4e92\u52a8\u3002", "method": "\u7ed3\u5408\u5355\u76eeRGB\u89c6\u9891\u4e2d\u63d0\u53d6\u76842D\u9aa8\u9abc\u59ff\u52bf\u548c\u9762\u90e8\u60c5\u611f\u7279\u5f81\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56RGB-D\u4f20\u611f\u5668\u6216GPU\u52a0\u901f\u3002", "result": "\u5728\u4ea4\u53c9\u4e3b\u4f53\u548c\u4ea4\u53c9\u573a\u666f\u534f\u8bae\u4e0b\u7684\u5168\u9762\u79bb\u7ebf\u8bc4\u4f30\u5c55\u793a\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u5e27\u548c\u5e8f\u5217\u7ea7AUROC\u8fbe\u52300.95\uff0c\u800c\u5728\u73b0\u5b9e\u57ce\u5e02\u73af\u5883\u4e0b\u8fdb\u884c\u7684\u4ea4\u4e92\u6d4b\u8bd5\u4e2d\uff0c\u5b9e\u73b0\u4e8691%\u7684\u51c6\u786e\u7387\u548c100%\u7684\u53ec\u56de\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u5c55\u793a\u4e86\u5728\u5404\u79cd\u672a\u7ecf\u8bad\u7ec3\u7684\u6570\u636e\u96c6\u4e0a\uff08\u4e0d\u540c\u7684\u76f8\u673a\u548c\u73af\u5883\uff09\u7684\u9ad8\u9c81\u68d2\u6027\uff0c\u4ee5\u53ca\u9002\u5408\u5e7f\u6cdb\u5e94\u7528\u4e8e\u793e\u4ea4\u673a\u5668\u4eba\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.17992", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.17992", "abs": "https://arxiv.org/abs/2512.17992", "authors": ["Qianwei Wang", "Bowen Li", "Zhanpeng Luo", "Yifan Xu", "Alexander Gray", "Tom Silver", "Sebastian Scherer", "Katia Sycara", "Yaqi Xie"], "title": "Unifying Deep Predicate Invention with Pre-trained Foundation Models", "comment": "18 pages, 11 figures", "summary": "Long-horizon robotic tasks are hard due to continuous state-action spaces and sparse feedback. Symbolic world models help by decomposing tasks into discrete predicates that capture object properties and relations. Existing methods learn predicates either top-down, by prompting foundation models without data grounding, or bottom-up, from demonstrations without high-level priors. We introduce UniPred, a bilevel learning framework that unifies both. UniPred uses large language models (LLMs) to propose predicate effect distributions that supervise neural predicate learning from low-level data, while learned feedback iteratively refines the LLM hypotheses. Leveraging strong visual foundation model features, UniPred learns robust predicate classifiers in cluttered scenes. We further propose a predicate evaluation method that supports symbolic models beyond STRIPS assumptions. Across five simulated and one real-robot domains, UniPred achieves 2-4 times higher success rates than top-down methods and 3-4 times faster learning than bottom-up approaches, advancing scalable and flexible symbolic world modeling for robotics.", "AI": {"tldr": "UniPred\u662f\u4e00\u4e2a\u53cc\u5c42\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u4f4e\u7ea7\u6570\u636e\uff0c\u663e\u8457\u63d0\u9ad8\u673a\u5668\u4eba\u957f\u8fdc\u4efb\u52a1\u7684\u6210\u529f\u7387\u548c\u5b66\u4e60\u901f\u5ea6\uff0c\u63a8\u52a8\u7b26\u53f7\u4e16\u754c\u5efa\u6a21\u7684\u8fdb\u5c55\u3002", "motivation": "\u957f\u8fdc\u7684\u673a\u5668\u4eba\u4efb\u52a1\u7531\u4e8e\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u8fde\u7eed\u6027\u548c\u53cd\u9988\u7a00\u758f\u6027\u800c\u8f83\u4e3a\u56f0\u96be\uff0c\u7b26\u53f7\u4e16\u754c\u6a21\u578b\u901a\u8fc7\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u79bb\u6563\u8c13\u8bcd\u6765\u6355\u6349\u5bf9\u8c61\u7279\u6027\u548c\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5c42\u5b66\u4e60\u6846\u67b6UniPred\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63a8\u52a8\u8c13\u8bcd\u6548\u5e94\u5206\u5e03\uff0c\u5e76\u76d1\u7763\u795e\u7ecf\u8c13\u8bcd\u7684\u4f4e\u7ea7\u6570\u636e\u5b66\u4e60\uff0c\u540c\u65f6\u901a\u8fc7\u5b66\u4e60\u53cd\u9988\u8fed\u4ee3\u4f18\u5316LLM\u5047\u8bbe\u3002", "result": "\u5728\u4e94\u4e2a\u6a21\u62df\u548c\u4e00\u4e2a\u771f\u5b9e\u673a\u5668\u4eba\u9886\u57df\u4e2d\uff0cUniPred\u5c55\u793a\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "UniPred\u5728\u516d\u4e2a\u9886\u57df\u7684\u6210\u529f\u7387\u63d0\u9ad8\u4e862-4\u500d\uff0c\u5e76\u4e14\u5b66\u4e60\u901f\u5ea6\u6bd4\u73b0\u6709\u7684\u65b9\u6cd5\u5feb\u4e863-4\u500d\uff0c\u63a8\u52a8\u4e86\u673a\u5668\u4eba\u7684\u7b26\u53f7\u4e16\u754c\u5efa\u6a21\u7684\u53ef\u6269\u5c55\u6027\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2512.18007", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.18007", "abs": "https://arxiv.org/abs/2512.18007", "authors": ["Yu Fang", "Kanchana Ranasinghe", "Le Xue", "Honglu Zhou", "Juntao Tan", "Ran Xu", "Shelby Heinecke", "Caiming Xiong", "Silvio Savarese", "Daniel Szafir", "Mingyu Ding", "Michael S. Ryoo", "Juan Carlos Niebles"], "title": "Robotic VLA Benefits from Joint Learning with Motion Image Diffusion", "comment": "Website: https://vla-motion.github.io/", "summary": "Vision-Language-Action (VLA) models have achieved remarkable progress in robotic manipulation by mapping multimodal observations and instructions directly to actions. However, they typically mimic expert trajectories without predictive motion reasoning, which limits their ability to reason about what actions to take. To address this limitation, we propose joint learning with motion image diffusion, a novel strategy that enhances VLA models with motion reasoning capabilities. Our method extends the VLA architecture with a dual-head design: while the action head predicts action chunks as in vanilla VLAs, an additional motion head, implemented as a Diffusion Transformer (DiT), predicts optical-flow-based motion images that capture future dynamics. The two heads are trained jointly, enabling the shared VLM backbone to learn representations that couple robot control with motion knowledge. This joint learning builds temporally coherent and physically grounded representations without modifying the inference pathway of standard VLAs, thereby maintaining test-time latency. Experiments in both simulation and real-world environments demonstrate that joint learning with motion image diffusion improves the success rate of pi-series VLAs to 97.5% on the LIBERO benchmark and 58.0% on the RoboTwin benchmark, yielding a 23% improvement in real-world performance and validating its effectiveness in enhancing the motion reasoning capability of large-scale VLAs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8fd0\u52a8\u56fe\u50cf\u6269\u6563\u7684\u8054\u5408\u5b66\u4e60\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u8fd0\u52a8\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u7684VLA\u6a21\u578b\u5728\u6267\u884c\u4efb\u52a1\u65f6\u4e3b\u8981\u4f9d\u8d56\u6a21\u4eff\u4e13\u5bb6\u8f68\u8ff9\uff0c\u7f3a\u4e4f\u9884\u6d4b\u52a8\u4f5c\u7684\u63a8\u7406\u80fd\u529b\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4e0e\u8fd0\u52a8\u56fe\u50cf\u6269\u6563\u7ed3\u5408\uff0c\u6269\u5c55\u4e86VLA\u7684\u67b6\u6784\uff0c\u91c7\u7528\u4e86\u53cc\u5934\u8bbe\u8ba1\uff0c\u52a8\u4f5c\u5934\u548c\u8fd0\u52a8\u5934\u5171\u540c\u8bad\u7ec3\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u4e0a\uff0c\u6210\u529f\u7387\u63d0\u9ad8\u81f397.5%\uff0c\u5728RoboTwin\u57fa\u51c6\u4e0a\u63d0\u9ad8\u81f358.0%\uff0c\u5b9e\u73b0\u4e8623%\u7684\u771f\u5b9e\u4e16\u754c\u8868\u73b0\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8054\u5b66\u4e60\u4e0e\u8fd0\u52a8\u56fe\u50cf\u6269\u6563\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u578b\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u8fd0\u52a8\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u6027\u80fd\u6709\u6240\u63d0\u5347\u3002"}}
{"id": "2512.18080", "categories": ["cs.HC", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.18080", "abs": "https://arxiv.org/abs/2512.18080", "authors": ["Marcos Ortiz", "Justin Hill", "Collin Overbay", "Ingrida Semenec", "Frederic Sauve-Hoover", "Jim Schwoebel", "Joel Shor"], "title": "From Prompt to Product: A Human-Centered Benchmark of Agentic App Generation Systems", "comment": null, "summary": "Agentic AI systems capable of generating full-stack web applications from natural language prompts (\"prompt- to-app\") represent a significant shift in software development. However, evaluating these systems remains challenging, as visual polish, functional correctness, and user trust are often misaligned. As a result, it is unclear how existing prompt-to-app tools compare under realistic, human-centered evaluation criteria. In this paper, we introduce a human-centered benchmark for evaluating prompt-to-app systems and conduct a large-scale comparative study of three widely used platforms: Replit, Bolt, and Firebase Studio. Using a diverse set of 96 prompts spanning common web application tasks, we generate 288 unique application artifacts. We evaluate these systems through a large-scale human-rater study involving 205 participants and 1,071 quality-filtered pairwise comparisons, assessing task-based ease of use, visual appeal, perceived completeness, and user trust. Our results show that these systems are not interchangeable: Firebase Studio consistently outperforms competing platforms across all human-evaluated dimensions, achieving the highest win rates for ease of use, trust, visual appeal, and visual appropriateness. Bolt performs competitively on visual appeal but trails Firebase on usability and trust, while Replit underperforms relative to both across most metrics. These findings highlight a persistent gap between visual polish and functional reliability in prompt-to-app systems and demonstrate the necessity of interactive, task-based evaluation. We release our benchmark framework, prompt set, and generated artifacts to support reproducible evaluation and future research in agentic application generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u63d0\u793a\u81f3\u5e94\u7528\u7cfb\u7edf\uff0c\u5c55\u793a\u4e86Firebase Studio\u5728\u7528\u6237\u4f53\u9a8c\u65b9\u9762\u7684\u4f18\u8d8a\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u89c6\u89c9\u4e0e\u529f\u80fd\u53ef\u9760\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u8bc4\u4f30\u751f\u6210\u5b8c\u6574\u5806\u6808\u7f51\u9875\u5e94\u7528\u7684\u4ee3\u7406AI\u7cfb\u7edf\u4ecd\u7136\u5b58\u5728\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u57fa\u51c6\u6765\u8fdb\u884c\u8bc4\u4f30\u3002", "method": "\u8fdb\u884c\u5927\u89c4\u6a21\u7684\u4eba\u7c7b\u8bc4\u4f30\u7814\u7a76\uff0c\u6bd4\u8f83\u4e09\u79cd\u5e38\u7528\u7684\u63d0\u793a\u81f3\u5e94\u7528\u5e73\u53f0\uff1aReplit\u3001Bolt\u548cFirebase Studio\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u7684\u63d0\u793a\u751f\u6210288\u4e2a\u5e94\u7528\u7a0b\u5e8f\u5de5\u4ef6\uff0c\u5e76\u8fdb\u884c\u4e86205\u540d\u53c2\u4e0e\u8005\u53c2\u4e0e\u7684\u5bf9\u6bd4\u7814\u7a76\uff0c\u53d1\u73b0\u8fd9\u4e9b\u7cfb\u7edf\u5728\u7528\u6237\u4f53\u9a8c\u3001\u89c6\u89c9\u5438\u5f15\u529b\u548c\u7528\u6237\u4fe1\u4efb\u65b9\u9762\u8868\u73b0\u4e0d\u540c\u3002", "conclusion": "Firebase Studio\u5728\u6240\u6709\u4eba\u7c7b\u8bc4\u4f30\u7684\u7ef4\u5ea6\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u5f3a\u8c03\u4e86\u89c6\u89c9\u5438\u5f15\u529b\u548c\u529f\u80fd\u53ef\u9760\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2512.18028", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.18028", "abs": "https://arxiv.org/abs/2512.18028", "authors": ["Tin Stribor Sohn", "Maximilian Dillitzer", "Jason J. Corso", "Eric Sax"], "title": "Embodied4C: Measuring What Matters for Embodied Vision-Language Navigation", "comment": null, "summary": "Vision-language navigation requires agents to reason and act under constraints of embodiment. While vision-language models (VLMs) demonstrate strong generalization, current benchmarks provide limited understanding of how embodiment -- i.e., the choice of physical platform, sensor configuration, and modality alignment -- influences perception, reasoning, and control. We introduce Embodied4C, a closed-loop benchmark designed as a Turing test for embodied reasoning. The benchmark evaluates the core embodied capabilities of VLMs across three heterogeneous embodiments -- autonomous vehicles, aerial drones, and robotic manipulators -- through approximately 1.1K one-shot reasoning questions and 58 goal-directed navigation tasks. These tasks jointly assess four foundational dimensions: semantic, spatial, temporal, and physical reasoning. Each embodiment presents dynamic sensor configurations and environment variations to probe generalization beyond platform-specific adaptation. To prevent embodiment overfitting, Embodied4C integrates domain-far queries targeting abstract and cross-context reasoning. Comprehensive evaluation across ten state-of-the-art VLMs and four embodied control baselines shows that cross-modal alignment and instruction tuning matter more than scale, while spatial and temporal reasoning remains the primary bottleneck for reliable embodied competence.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Embodied4C\uff0c\u4e00\u4e2a\u8bc4\u4f30\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u6709\u4f53\u5f62\u5f0f\u4e0b\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5c55\u793a\u4e86\u8de8\u6a21\u6001\u5bf9\u9f50\u7684\u91cd\u8981\u6027\u548c\u63a8\u7406\u65b9\u9762\u7684\u4e3b\u8981\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u5bf9\u6709\u4f53\u6027\u7684\u7406\u89e3\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u8e31\u6b65\u3001\u63a8\u7406\u4e0e\u884c\u52a8\u65f6\u9762\u4e34\u7684\u9650\u5236\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u95ed\u73af\u57fa\u51c6Embodied4C\uff0c\u8bc4\u4f30\u5728\u4e09\u79cd\u5f02\u6784\u6709\u4f53\u5f62\u5f0f\u4e0b\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u6838\u5fc3\u6709\u4f53\u80fd\u529b\uff0c\u4f7f\u7528\u7ea61.1K\u4e2a\u4e00\u6b21\u6027\u63a8\u7406\u95ee\u9898\u548c58\u4e2a\u76ee\u6807\u5bfc\u5411\u5bfc\u822a\u4efb\u52a1\u3002", "result": "Embodied4C\u57fa\u51c6\u8868\u660e\uff0c\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u6307\u4ee4\u8c03\u6574\u4f18\u4e8e\u5355\u7eaf\u589e\u52a0\u6a21\u578b\u89c4\u6a21\uff0c\u800c\u7a7a\u95f4\u548c\u65f6\u95f4\u63a8\u7406\u4ecd\u662f\u5b9e\u73b0\u53ef\u9760\u6709\u4f53\u80fd\u529b\u7684\u91cd\u8981\u6311\u6218\u3002", "conclusion": "\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u6307\u4ee4\u5fae\u8c03\u6bd4\u6a21\u578b\u89c4\u6a21\u66f4\u4e3a\u91cd\u8981\uff0c\u800c\u7a7a\u95f4\u548c\u65f6\u95f4\u63a8\u7406\u662f\u53ef\u9760\u7684\u6709\u4f53\u80fd\u529b\u7684\u4e3b\u8981\u74f6\u9888\u3002"}}
{"id": "2512.18230", "categories": ["cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.18230", "abs": "https://arxiv.org/abs/2512.18230", "authors": ["Hyeon Jeon"], "title": "Dimensionality Reduction Considered Harmful (Some of the Time)", "comment": "PhD Dissertation", "summary": "Visual analytics now plays a central role in decision-making across diverse disciplines, but it can be unreliable: the knowledge or insights derived from the analysis may not accurately reflect the underlying data. In this dissertation, we improve the reliability of visual analytics with a focus on dimensionality reduction (DR). DR techniques enable visual analysis of high-dimensional data by reducing it to two or three dimensions, but they inherently introduce errors that can compromise the reliability of visual analytics. To this end, I investigate reliability challenges that practitioners face when using DR for visual analytics. Then, I propose technical solutions to address these challenges, including new evaluation metrics, optimization strategies, and interaction techniques. We conclude the thesis by discussing how our contributions lay the foundation for achieving more reliable visual analytics practices.", "AI": {"tldr": "\u672c\u8bba\u6587\u805a\u7126\u4e8e\u63d0\u5347\u964d\u7ef4\u6280\u672f\u5728\u89c6\u89c9\u5206\u6790\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u63d0\u51fa\u4e86\u65b0\u8bc4\u4f30\u6307\u6807\u548c\u4f18\u5316\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u9762\u4e34\u7684\u53ef\u9760\u6027\u6311\u6218\u3002", "motivation": "\u89c6\u89c9\u5206\u6790\u5728\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u53ef\u9760\u6027\u5b58\u7591\uff0c\u5206\u6790\u5f97\u51fa\u7684\u89c1\u89e3\u53ef\u80fd\u4e0e\u5b9e\u9645\u6570\u636e\u4e0d\u7b26\u3002", "method": "\u901a\u8fc7\u7814\u7a76\u964d\u7ef4\u6280\u672f\uff0c\u5206\u6790\u5176\u5728\u9ad8\u7ef4\u6570\u636e\u89c6\u89c9\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u540c\u65f6\u63d0\u51fa\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u3001\u4f18\u5316\u7b56\u7565\u548c\u4ea4\u4e92\u6280\u672f\u4ee5\u63d0\u9ad8\u53ef\u9760\u6027\u3002", "result": "\u6539\u5584\u4e86\u964d\u7ef4\u6280\u672f\u7684\u53ef\u9760\u6027\uff0c\u63d0\u51fa\u4e86\u65b0\u65b9\u6cd5\u6765\u8bc4\u4f30\u548c\u4f18\u5316\u89c6\u89c9\u5206\u6790\u8fc7\u7a0b\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u4e3a\u66f4\u52a0\u53ef\u9760\u7684\u89c6\u89c9\u5206\u6790\u5b9e\u8df5\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.18032", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.18032", "abs": "https://arxiv.org/abs/2512.18032", "authors": ["Jacqueline Borgstedt", "Jake Bhattacharyya", "Matteo Iovino", "Frank E. Pollick", "Stephen Brewster"], "title": "Design and Integration of Thermal and Vibrotactile Feedback for Lifelike Touch in Social Robots", "comment": null, "summary": "Zoomorphic Socially Assistive Robots (SARs) offer an alternative source of social touch for individuals who cannot access animal companionship. However, current SARs provide only limited, passive touch-based interactions and lack the rich haptic cues, such as warmth, heartbeat or purring, that are characteristic of human-animal touch. This limits their ability to evoke emotionally engaging, life-like physical interactions.\n  We present a multimodal tactile prototype, which was used to augment the established PARO robot, integrating thermal and vibrotactile feedback to simulate feeling biophysiological signals. A flexible heating interface delivers body-like warmth, while embedded actuators generate heartbeat-like rhythms and continuous purring sensations. These cues were iteratively designed and calibrated with input from users and haptics experts. We outline the design process and offer reproducible guidelines to support the development of emotionally resonant and biologically plausible touch interactions with SARs.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u89e6\u89c9\u539f\u578b\uff0c\u901a\u8fc7\u70ed\u548c\u632f\u52a8\u53cd\u9988\u6539\u5584\u4e86\u793e\u4f1a\u8f85\u52a9\u673a\u5668\u4ebaPARO\uff0c\u4f7f\u5176\u80fd\u66f4\u597d\u5730\u4e0e\u7528\u6237\u4ea7\u751f\u60c5\u611f\u5171\u9e23\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u793e\u4f1a\u8f85\u52a9\u673a\u5668\u4eba\u89e6\u89c9\u4e92\u52a8\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u60c5\u611f\u53c2\u4e0e\u611f\u548c\u751f\u547d\u822c\u7684\u7269\u7406\u4e92\u52a8\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u89e6\u89c9\u539f\u578b\uff0c\u7ed3\u5408\u4e86\u70ed\u91cf\u548c\u632f\u52a8\u53cd\u9988\u6765\u6a21\u62df\u751f\u7269\u4f53\u4fe1\u53f7\u3002", "result": "\u8bbe\u8ba1\u51fa\u7684\u539f\u578b\u80fd\u6a21\u62df\u4eba\u7c7b\u4e0e\u52a8\u7269\u63a5\u89e6\u7684\u4e30\u5bcc\u89e6\u89c9\u4fe1\u53f7\uff0c\u4ece\u800c\u589e\u5f3a\u4e0e\u7528\u6237\u7684\u60c5\u611f\u4ea4\u6d41\u3002", "conclusion": "\u901a\u8fc7\u96c6\u6210\u70ed\u548c\u632f\u52a8\u89e6\u89c9\u53cd\u9988\uff0c\u6539\u8fdb\u4e86PARO\u673a\u5668\u4eba\uff0c\u4f7f\u5176\u80fd\u591f\u6a21\u62df\u751f\u7269\u4f53\u5f81\uff0c\u589e\u52a0\u4e86\u4e0e\u7528\u6237\u7684\u60c5\u611f\u8fde\u63a5\u3002"}}
{"id": "2512.18234", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.18234", "abs": "https://arxiv.org/abs/2512.18234", "authors": ["Lixiang Yan", "Xibin Han", "Yu Zhang", "Samuel Greiff", "Inge Molenaar", "Roberto Martinez-Maldonado", "Yizhou Fan", "Linxuan Zhao", "Xinyu Li", "Yueqiao Jin", "Dragan Ga\u0161evi\u0107"], "title": "The Social Blindspot in Human-AI Collaboration: How Undetected AI Personas Reshape Team Dynamics", "comment": null, "summary": "As generative AI systems become increasingly embedded in collaborative work, they are evolving from visible tools into human-like communicative actors that participate socially rather than merely providing information. Yet little is known about how such agents shape team dynamics when their artificial nature is not recognised, a growing concern as human-like AI is deployed at scale in education, organisations, and civic contexts where collaboration underpins collective outcomes. In a large-scale mixed-design experiment (N = 905), we examined how AI teammates with distinct communicative personas, supportive or contrarian, affected collaboration across analytical, creative, and ethical tasks. Participants worked in triads that were fully human or hybrid human-AI teams, without being informed of AI involvement. Results show that participants had limited ability to detect AI teammates, yet AI personas exerted robust social effects. Contrarian personas reduced psychological safety and discussion quality, whereas supportive personas improved discussion quality without affecting safety. These effects persisted after accounting for individual differences in detectability, revealing a dissociation between influence and awareness that we term the social blindspot. Linguistic analyses confirmed that personas were enacted through systematic differences in affective and relational language, with partial mediation for discussion quality but largely direct effects on psychological safety. Together, the findings demonstrate that AI systems can tacitly regulate collaborative norms through persona-level cues, even when users remain unaware of their presence. We argue that persona design constitutes a form of social governance in hybrid teams, with implications for the responsible deployment of AI in collective settings.", "AI": {"tldr": "\u751f\u6210AI\u7cfb\u7edf\u5728\u534f\u4f5c\u5de5\u4f5c\u4e2d\u9010\u6e10\u4ece\u53ef\u89c1\u5de5\u5177\u6f14\u53d8\u4e3a\u4eba\u7c7b\u822c\u7684\u4ea4\u9645\u53c2\u4e0e\u8005\uff0c\u672c\u6587\u63a2\u8ba8\u4e86AI\u961f\u53cb\u7684\u6c9f\u901a\u89d2\u8272\u5982\u4f55\u5f71\u54cd\u56e2\u961f\u52a8\u6001\uff0c\u5c24\u5176\u662f\u5728\u53c2\u4e0e\u8005\u672a\u8bc6\u522b\u5176\u4eba\u5de5\u5c5e\u6027\u65f6\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u4eba\u7c7b\u7c7bAI\u5728\u6559\u80b2\u3001\u7ec4\u7ec7\u548c\u516c\u6c11\u73af\u5883\u4e2d\u5927\u89c4\u6a21\u90e8\u7f72\uff0c\u63a2\u8ba8\u8fd9\u4e9bAI\u5982\u4f55\u5f71\u54cd\u56e2\u961f\u52a8\u6001\u6210\u4e3a\u4e00\u4e2a\u65e5\u76ca\u91cd\u8981\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4e00\u9879\u5927\u89c4\u6a21\u6df7\u5408\u8bbe\u8ba1\u5b9e\u9a8c\uff08N = 905\uff09\uff0c\u89c2\u5bdf\u5177\u6709\u4e0d\u540c\u6c9f\u901a\u4eba\u683c\u7684AI\u961f\u53cb\u5bf9\u5206\u6790\u3001\u521b\u9020\u548c\u4f26\u7406\u4efb\u52a1\u7684\u534f\u4f5c\u5f71\u54cd\uff0c\u53c2\u4e0e\u8005\u5728\u4e09\u4eba\u5c0f\u7ec4\u4e2d\u5de5\u4f5c\uff0c\u4e14\u672a\u88ab\u544a\u77e5\u6709AI\u53c2\u4e0e\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u53c2\u4e0e\u8005\u5bf9AI\u961f\u53cb\u7684\u8bc6\u522b\u80fd\u529b\u6709\u9650\uff0c\u4f46AI\u7684\u4eba\u683c\u786e\u5b9e\u5bf9\u56e2\u961f\u7684\u5fc3\u7406\u5b89\u5168\u611f\u548c\u8ba8\u8bba\u8d28\u91cf\u4ea7\u751f\u4e86\u663e\u8457\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u76f8\u5bf9\u7acb\u7684\u4eba\u683c\u964d\u4f4e\u4e86\u8ba8\u8bba\u8d28\u91cf\u548c\u5fc3\u7406\u5b89\u5168\u611f\uff0c\u800c\u652f\u6301\u6027\u7684\u4eba\u683c\u63d0\u5347\u4e86\u8ba8\u8bba\u8d28\u91cf\u3002", "conclusion": "AI\u7cfb\u7edf\u53ef\u4ee5\u901a\u8fc7\u4eba\u683c\u5c42\u9762\u7684\u7ebf\u7d22\u5728\u6df7\u5408\u56e2\u961f\u4e2d\u9690\u6027\u8c03\u8282\u534f\u4f5c\u89c4\u8303\uff0c\u8fd9\u8868\u660e\u4eba\u683c\u8bbe\u8ba1\u5728\u6df7\u5408\u56e2\u961f\u4e2d\u6784\u6210\u4e86\u4e00\u79cd\u793e\u4f1a\u6cbb\u7406\u5f62\u5f0f\uff0c\u5bf9AI\u5728\u96c6\u4f53\u73af\u5883\u4e2d\u7684\u8d1f\u8d23\u90e8\u7f72\u5177\u6709\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2512.18048", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.18048", "abs": "https://arxiv.org/abs/2512.18048", "authors": ["Nidhi Malhotra", "Amber K. Rothe", "Revanth Konda", "Jaydev P. Desai"], "title": "Design of a Polymer-based Steerable Cannula for Neurosurgical Applications", "comment": "Presented at the 14th Conference on New Technologies for Computer and Robot Assisted Surgery (CRAS 2025)", "summary": "Robotically steerable compliant surgical tools offer several advantages over rigid tools, including enhanced dexterity, reduced tissue damage, and the ability to generate non-linear trajectories in minimally invasive neurosurgical procedures. Many existing robotic neurosurgical tools are designed using stainless steel or nitinol materials. Using polymer-based materials instead can offer advantages such as reduced interference in magnetic resonance imaging, enhanced safety for guiding electrically powered instruments, and reduced tissue damage due to inherent compliance. Several polymer materials have been used in robotic surgical applications, such as polyimide, polycarbonate, and elastic resin. Various fabrication strategies have also been proposed, including standard microfabrication techniques, thermal drawing, and 3-D printing. In our previous work, a tendon-driven, notched-tube was designed for several neurosurgical robotic tools, utilizing laser micromachining to reduce the stiffness of the tube in certain directions. This fabrication method is desirable because it has a single-step process, has high precision, and does not require a cleanroom or harsh chemicals. Past studies have explored laser-micromachining of polymer material for surgical applications such as stent fabrication. In this work, we explore extending the use of the laser micromachining approach to the fabrication of polyimide (PI) robotically steerable cannulas for neurosurgical applications. Utilizing the method presented in this work, we fabricated joints as small as 1.5 mm outer diameter (OD). Multiple joints were fabricated using PI tubes of different ODs, and the loading behavior of the fabricated joints was experimentally characterized.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u6fc0\u5149\u5fae\u673a\u68b0\u52a0\u5de5\u805a\u9170\u4e9a\u80fa\u6750\u6599\u5728\u795e\u7ecf\u5916\u79d1\u5de5\u5177\u5236\u9020\u4e2d\u7684\u5e94\u7528\uff0c\u6210\u529f\u5236\u4f5c\u4e86\u5c0f\u578b\u7684\u673a\u5668\u4eba\u53ef\u64cd\u63a7\u5bfc\u7ba1\u5173\u8282\uff0c\u5c55\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\u4e0e\u67d4\u6027\u3002", "motivation": "\u4f20\u7edf\u521a\u6027\u5916\u79d1\u5de5\u5177\u5728\u7075\u6d3b\u6027\u548c\u7ec4\u7ec7\u635f\u4f24\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u672c\u7814\u7a76\u65e8\u5728\u91c7\u7528\u9ad8\u5f39\u6027\u805a\u5408\u7269\u6750\u6599\u6539\u5584\u795e\u7ecf\u5916\u79d1\u624b\u672f\u4e2d\u7684\u8fd9\u4e9b\u95ee\u9898\uff0c\u63a2\u7d22\u6fc0\u5149\u5fae\u673a\u68b0\u52a0\u5de5\u7684\u6f5c\u529b\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u6fc0\u5149\u5fae\u673a\u68b0\u52a0\u5de5\u6cd5\u5236\u9020\u805a\u9170\u4e9a\u80fa\uff08PI\uff09\u673a\u5668\u4eba\u53ef\u64cd\u63a7\u5bfc\u7ba1\uff0c\u5236\u5907\u4e86\u4e0d\u540c\u5916\u5f84\u7684PI\u7ba1\u5e76\u5bf9\u5173\u8282\u7684\u8d1f\u8f7d\u884c\u4e3a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u8868\u5f81\u3002", "result": "\u6210\u529f\u5229\u7528\u6fc0\u5149\u5fae\u673a\u68b0\u52a0\u5de5\u5236\u5907\u4e86\u591a\u79cd\u4e0d\u540c\u5916\u5f84\u7684\u805a\u9170\u4e9a\u80fa\u5bfc\u7ba1\u5173\u8282\uff0c\u5bf9\u5176\u52a0\u8f7d\u6027\u80fd\u8fdb\u884c\u4e86\u5b9e\u9a8c\u7814\u7a76\uff0c\u7ed3\u679c\u8868\u660e\u6240\u5236\u5907\u7684\u5173\u8282\u5177\u6709\u826f\u597d\u7684\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u6fc0\u5149\u5fae\u673a\u68b0\u52a0\u5de5\u7684\u805a\u9170\u4e9a\u80fa\uff08PI\uff09\u673a\u5668\u4eba\u53ef\u64cd\u63a7\u5bfc\u7ba1\u5728\u795e\u7ecf\u5916\u79d1\u5e94\u7528\u4e2d\u5c55\u73b0\u4e86\u826f\u597d\u7684\u6027\u80fd\uff0c\u6210\u529f\u5236\u9020\u4e861.5\u6beb\u7c73\u5916\u5f84\u7684\u67d4\u6027\u5173\u8282\u5e76\u8fdb\u884c\u4e86\u8d1f\u8f7d\u884c\u4e3a\u7684\u5b9e\u9a8c\u8868\u5f81\u3002"}}
{"id": "2512.18239", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.18239", "abs": "https://arxiv.org/abs/2512.18239", "authors": ["Yueqiao Jin", "Roberto Martinez-Maldonado", "Dragan Ga\u0161evi\u0107", "Lixiang Yan"], "title": "Emergent Learner Agency in Implicit Human-AI Collaboration: How AI Personas Reshape Creative-Regulatory Interaction", "comment": null, "summary": "Generative AI is increasingly embedded in collaborative learning, yet little is known about how AI personas shape learner agency when AI teammates are present but not disclosed. This mechanism study examines how supportive and contrarian AI personas reconfigure emergent learner agency, discourse patterns, and experiences in implicit human-AI creative collaboration. A total of 224 university students were randomly assigned to 97 online triads in one of three conditions: human-only control, hybrid teams with a supportive AI, or hybrid teams with a contrarian AI. Participants completed an individual-group-individual movie-plot writing task; the 10-minute group chat was coded using a creative-regulatory framework. We combined transition network analysis, theory-driven sequential pattern mining, and Gaussian mixture clustering to model structural, temporal, and profile-level manifestations of agency, and linked these to cognitive load, psychological safety, teamwork satisfaction, and embedding-based creative performance. Contrarian AI produced challenge- and reflection-rich discourse structures and motifs indicating productive friction, whereas supportive AI fostered agreement-centred trajectories and smoother convergence. Clustering showed AI agents concentrated in challenger profiles, with reflective regulation uniquely human. While no systematic differences emerged in cognitive load or creative gains, contrarian AI consistently reduced teamwork satisfaction and psychological safety. The findings reveal a design tension between leveraging cognitive conflict and maintaining affective safety and ownership in hybrid human-AI teams.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u652f\u6301\u578b\u4e0e\u5bf9\u6297\u578b\u4eba\u5de5\u667a\u80fd\u5728\u9690\u6027\u4eba\u673a\u5408\u4f5c\u4e2d\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5bf9\u6297\u578bAI\u4fc3\u8fdb\u66f4\u5177\u6311\u6218\u6027\u7684\u8ba8\u8bba\uff0c\u800c\u652f\u6301\u578bAI\u5219\u503e\u5411\u4e8e\u589e\u8fdb\u4e00\u81f4\u6027\uff0c\u4e8c\u8005\u5728\u56e2\u961f\u6ee1\u610f\u5ea6\u548c\u5fc3\u7406\u5b89\u5168\u611f\u4e0a\u5b58\u5728\u8bbe\u8ba1\u5f20\u529b\u3002", "motivation": "\u968f\u7740\u751f\u6210\u6027AI\u5728\u534f\u4f5c\u5b66\u4e60\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u6709\u5fc5\u8981\u63a2\u8ba8AI\u89d2\u8272\u5982\u4f55\u5f71\u54cd\u5b66\u4e60\u8005\u7684\u4e3b\u4f53\u6027\uff0c\u7279\u522b\u662f\u5728AI\u8eab\u4efd\u672a\u88ab\u516c\u5f00\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u8be5\u7814\u7a76\u91c7\u7528\u968f\u673a\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u5c06224\u540d\u5927\u5b66\u751f\u5206\u914d\u5230\u4e09\u79cd\u6761\u4ef6\u4e2d\uff0c\u5e76\u4f7f\u7528\u521b\u610f\u8c03\u8282\u6846\u67b6\u5bf910\u5206\u949f\u7684\u7fa4\u804a\u8fdb\u884c\u7f16\u7801\uff0c\u7ed3\u5408\u8fc7\u6e21\u7f51\u7edc\u5206\u6790\u3001\u7406\u8bba\u9a71\u52a8\u7684\u5e8f\u5217\u6a21\u5f0f\u6316\u6398\u548c\u9ad8\u65af\u6df7\u5408\u805a\u7c7b\u7b49\u65b9\u6cd5\u6765\u7814\u7a76\u5b66\u4e60\u8005\u4e3b\u4f53\u6027\u53ca\u5176\u4e0e\u8ba4\u77e5\u8d1f\u8377\u7b49\u56e0\u7d20\u7684\u5173\u7cfb\u3002", "result": "\u5bf9\u6297\u578bAI\u4fc3\u8fdb\u66f4\u5177\u6311\u6218\u6027\u548c\u53cd\u601d\u6027\u7684\u8ba8\u8bba\u7ed3\u6784\uff0c\u800c\u652f\u6301\u578bAI\u5219\u503e\u5411\u4e8e\u4fc3\u8fdb\u4e00\u81f4\u6027\u548c\u987a\u5229\u7684\u8fbe\u6210\u5171\u8bc6\u3002\u5c3d\u7ba1\u8ba4\u77e5\u8d1f\u8377\u548c\u521b\u9020\u6027\u6210\u679c\u65b9\u9762\u6ca1\u6709\u663e\u8457\u5dee\u5f02\uff0c\u4f46\u5bf9\u6297\u578bAI\u5374\u663e\u8457\u964d\u4f4e\u4e86\u56e2\u961f\u6ee1\u610f\u5ea6\u548c\u5fc3\u7406\u5b89\u5168\u611f\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5728\u6df7\u5408\u4eba\u673a\u56e2\u961f\u4e2d\uff0c\u4f7f\u7528\u4e0d\u540c\u7c7b\u578b\u7684\u4eba\u5de5\u667a\u80fd\uff08\u652f\u6301\u578b\u4e0e\u5bf9\u6297\u578b\uff09\u4f1a\u5f71\u54cd\u5b66\u4e60\u8005\u7684\u4e3b\u4f53\u6027\u3001\u56e2\u961f\u6ee1\u610f\u5ea6\u548c\u5fc3\u7406\u5b89\u5168\u611f\u4e4b\u95f4\u5b58\u5728\u8bbe\u8ba1\u5f20\u529b\u3002"}}
{"id": "2512.18068", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.18068", "abs": "https://arxiv.org/abs/2512.18068", "authors": ["Juo-Tung Chen", "XinHao Chen", "Ji Woong Kim", "Paul Maria Scheikl", "Richard Jaepyeong Cha", "Axel Krieger"], "title": "SurgiPose: Estimating Surgical Tool Kinematics from Monocular Video for Surgical Robot Learning", "comment": "8 pages, 6 figures, 2 tables", "summary": "Imitation learning (IL) has shown immense promise in enabling autonomous dexterous manipulation, including learning surgical tasks. To fully unlock the potential of IL for surgery, access to clinical datasets is needed, which unfortunately lack the kinematic data required for current IL approaches. A promising source of large-scale surgical demonstrations is monocular surgical videos available online, making monocular pose estimation a crucial step toward enabling large-scale robot learning. Toward this end, we propose SurgiPose, a differentiable rendering based approach to estimate kinematic information from monocular surgical videos, eliminating the need for direct access to ground truth kinematics. Our method infers tool trajectories and joint angles by optimizing tool pose parameters to minimize the discrepancy between rendered and real images. To evaluate the effectiveness of our approach, we conduct experiments on two robotic surgical tasks: tissue lifting and needle pickup, using the da Vinci Research Kit Si (dVRK Si). We train imitation learning policies with both ground truth measured kinematics and estimated kinematics from video and compare their performance. Our results show that policies trained on estimated kinematics achieve comparable success rates to those trained on ground truth data, demonstrating the feasibility of using monocular video based kinematic estimation for surgical robot learning. By enabling kinematic estimation from monocular surgical videos, our work lays the foundation for large scale learning of autonomous surgical policies from online surgical data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSurgiPose\uff0c\u4e00\u79cd\u4ece\u5355\u76ee\u5916\u79d1\u89c6\u9891\u4e2d\u8fdb\u884c\u8fd0\u52a8\u5b66\u4f30\u8ba1\u7684\u65b9\u6cd5\uff0c\u4e3a\u81ea\u4e3b\u5916\u79d1\u653f\u7b56\u7684\u5927\u89c4\u6a21\u5b66\u4e60\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u4f30\u8ba1\u8fd0\u52a8\u5b66\u8bad\u7ec3\u7684\u653f\u7b56\u4e0e\u57fa\u4e8e\u771f\u5b9e\u6570\u636e\u7684\u653f\u7b56\u8868\u73b0\u76f8\u5f53\u3002", "motivation": "\u4e3a\u4e86\u89e3\u9501\u6a21\u4eff\u5b66\u4e60\u5728\u5916\u79d1\u624b\u672f\u4e2d\u7684\u6f5c\u529b\uff0c\u9700\u8981\u8bbf\u95ee\u4e34\u5e8a\u6570\u636e\u96c6\uff0c\u800c\u8fd9\u4e9b\u6570\u636e\u96c6\u901a\u5e38\u7f3a\u4e4f\u5f53\u524dIL\u65b9\u6cd5\u6240\u9700\u7684\u8fd0\u52a8\u5b66\u6570\u636e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSurgiPose\u7684\u53ef\u5fae\u6e32\u67d3\u65b9\u6cd5\uff0c\u4ece\u5355\u76ee\u5916\u79d1\u89c6\u9891\u4e2d\u4f30\u8ba1\u8fd0\u52a8\u5b66\u4fe1\u606f\uff0c\u901a\u8fc7\u4f18\u5316\u5de5\u5177\u4f4d\u59ff\u53c2\u6570\u4ee5\u6700\u5c0f\u5316\u6e32\u67d3\u56fe\u50cf\u4e0e\u771f\u5b9e\u56fe\u50cf\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u63a8\u65ad\u5de5\u5177\u8f68\u8ff9\u548c\u5173\u8282\u89d2\u5ea6\u3002", "result": "\u5728\u4f7f\u7528\u8fbe\u82ac\u5947\u7814\u7a76\u5957\u4ef6Si\uff08dVRK Si\uff09\u7684\u4e24\u9879\u673a\u5668\u4eba\u624b\u672f\u4efb\u52a1\uff08\u7ec4\u7ec7\u63d0\u5347\u548c\u9488\u5177\u62fe\u53d6\uff09\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u4f30\u8ba1\u8fd0\u52a8\u5b66\u8bad\u7ec3\u7684\u653f\u7b56\u4e0e\u57fa\u4e8e\u771f\u5b9e\u6d4b\u91cf\u8fd0\u52a8\u5b66\u8bad\u7ec3\u7684\u653f\u7b56\u5728\u6210\u529f\u7387\u4e0a\u76f8\u5f53\uff0c\u5c55\u793a\u4e86\u4f7f\u7528\u5355\u76ee\u89c6\u9891\u8fdb\u884c\u8fd0\u52a8\u5b66\u4f30\u8ba1\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u901a\u8fc7\u4ece\u5355\u76ee\u5916\u79d1\u89c6\u9891\u4e2d\u8fdb\u884c\u8fd0\u52a8\u5b66\u4f30\u8ba1\uff0c\u6211\u4eec\u7684\u7814\u7a76\u4e3a\u4ece\u5728\u7ebf\u5916\u79d1\u6570\u636e\u4e2d\u8fdb\u884c\u5927\u89c4\u6a21\u81ea\u4e3b\u5916\u79d1\u7b56\u7565\u5b66\u4e60\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.18306", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.18306", "abs": "https://arxiv.org/abs/2512.18306", "authors": ["Alvaro Becerra", "Ruth Cobos"], "title": "Leveraging Peer, Self, and Teacher Assessments for Generative AI-Enhanced Feedback", "comment": "This paper has been accepted as a Full Paper in the main conference of the 2026 IEEE Global Engineering Education Conference (EDUCON)", "summary": "Providing timely and meaningful feedback remains a persistent challenge in higher education, especially in large courses where teachers must balance formative depth with scalability. Recent advances in Generative Artificial Intelligence (GenAI) offer new opportunities to support feedback processes while maintaining human oversight. This paper presents an study conducted within the AICoFe (AI-based Collaborative Feedback) system, which integrates teacher, peer, and self-assessments of engineering students' oral presentations. Using a validated rubric, 46 evaluation sets were analyzed to examine agreement, correlation, and bias across evaluators. The analyses revealed consistent overall alignment among sources but also systematic variations in scoring behavior, reflecting distinct evaluative perspectives. These findings informed the proposal of an enhanced GenAI model within AICoFe system, designed to integrate human assessments through weighted input aggregation, bias detection, and context-aware feedback generation. The study contributes empirical evidence and design principles for developing GenAI-based feedback systems that combine data-based efficiency with pedagogical validity and transparency.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728AICoFe\u7cfb\u7edf\u4e2d\u5229\u7528\u751f\u6210\u4eba\u5de5\u667a\u80fd\u652f\u6301\u53cd\u9988\u8fc7\u7a0b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u578b\u4ee5\u63d0\u5347\u53cd\u9988\u7684\u6709\u6548\u6027\u548c\u900f\u660e\u5ea6\u3002", "motivation": "\u89e3\u51b3\u9ad8\u7b49\u6559\u80b2\u4e2d\u5728\u5927\u89c4\u6a21\u8bfe\u7a0b\u4e2d\u63d0\u4f9b\u53ca\u65f6\u800c\u6709\u610f\u4e49\u7684\u53cd\u9988\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u6559\u5e08\u9700\u5e73\u8861\u5f62\u6210\u6027\u6df1\u5ea6\u4e0e\u53ef\u6269\u5c55\u6027\u65f6\u3002", "method": "\u901a\u8fc7\u5bf946\u4e2a\u8bc4\u4f30\u96c6\u8fdb\u884c\u5206\u6790\uff0c\u63a2\u8ba8\u8bc4\u4f30\u8005\u4e4b\u95f4\u7684\u534f\u8bae\u3001\u76f8\u5173\u6027\u53ca\u504f\u5dee\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u8bc4\u4ef7\u6765\u6e90\u4e4b\u95f4\u5b58\u5728\u4e00\u81f4\u7684\u603b\u4f53\u4e00\u81f4\u6027\uff0c\u4f46\u540c\u65f6\u4e5f\u663e\u793a\u51fa\u8bc4\u5206\u884c\u4e3a\u7684\u7cfb\u7edf\u6027\u53d8\u5316\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7684\u751f\u6210\u4eba\u5de5\u667a\u80fd\u6a21\u578b\uff0c\u80fd\u591f\u6709\u6548\u6574\u5408\u4eba\u7c7b\u8bc4\u4f30\uff0c\u4ee5\u5b9e\u73b0\u66f4\u5177\u652f\u6301\u6027\u7684\u53cd\u9988\u8fc7\u7a0b\u3002"}}
{"id": "2512.18081", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.18081", "abs": "https://arxiv.org/abs/2512.18081", "authors": ["Tudor Jianu", "Anh Nguyen", "Sebastiano Fichera", "Pierre Berthet-Rayne"], "title": "Towards Autonomous Navigation in Endovascular Interventions", "comment": null, "summary": "Cardiovascular diseases remain the leading cause of global mortality, with minimally invasive treatment options offered through endovascular interventions. However, the precision and adaptability of current robotic systems for endovascular navigation are limited by heuristic control, low autonomy, and the absence of haptic feedback. This thesis presents an integrated AI-driven framework for autonomous guidewire navigation in complex vascular environments, addressing key challenges in data availability, simulation fidelity, and navigational accuracy.\n  A high-fidelity, real-time simulation platform, CathSim, is introduced for reinforcement learning based catheter navigation, featuring anatomically accurate vascular models and contact dynamics. Building on CathSim, the Expert Navigation Network is developed, a policy that fuses visual, kinematic, and force feedback for autonomous tool control. To mitigate data scarcity, the open-source, bi-planar fluoroscopic dataset Guide3D is proposed, comprising more than 8,700 annotated images for 3D guidewire reconstruction. Finally, SplineFormer, a transformer-based model, is introduced to directly predict guidewire geometry as continuous B-spline parameters, enabling interpretable, real-time navigation.\n  The findings show that combining high-fidelity simulation, multimodal sensory fusion, and geometric modelling substantially improves autonomous endovascular navigation and supports safer, more precise minimally invasive procedures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u81ea\u4e3b\u5bfc\u4e1d\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u4fdd\u771f\u6a21\u62df\u3001\u4f20\u611f\u5668\u878d\u5408\u548c\u51e0\u4f55\u5efa\u6a21\uff0c\u663e\u8457\u6539\u5584\u5fae\u521b\u624b\u672f\u7684\u7cbe\u786e\u5ea6\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u9488\u5bf9\u5f53\u524d\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u8840\u7ba1\u5185\u5bfc\u822a\u4e2d\u7cbe\u786e\u6027\u3001\u81ea\u9002\u5e94\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u8fd9\u4e00\u7814\u7a76\u4ee5\u89e3\u51b3\u6570\u636e\u53ef\u7528\u6027\u3001\u6a21\u62df\u4fdd\u771f\u5ea6\u548c\u5bfc\u822a\u51c6\u786e\u6027\u7b49\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u7684\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u6846\u67b6\uff0c\u4f7f\u7528\u9ad8\u4fdd\u771f\u5b9e\u65f6\u6a21\u62df\u5e73\u53f0CathSim\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u7684\u5bfc\u7ba1\u5bfc\u822a\uff0c\u5e76\u5f00\u53d1\u4e86Expert Navigation Network\u4ee5\u878d\u5408\u89c6\u89c9\u3001\u8fd0\u52a8\u5b66\u548c\u529b\u53cd\u9988\u5b9e\u73b0\u81ea\u4e3b\u5de5\u5177\u63a7\u5236\u3002", "result": "\u901a\u8fc7\u5f15\u5165\u5f00\u653e\u6e90\u4ee3\u7801\u7684Guide3D\u6570\u636e\u96c6\u548cSplineFormer\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5bfc\u7ba1\u5bfc\u822a\u7684\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u6027\uff0c\u4f7f\u5f97\u9488\u5bf9\u590d\u6742\u8840\u7ba1\u73af\u5883\u7684\u81ea\u4e3b\u5bfc\u4e1d\u5bfc\u822a\u6210\u4e3a\u53ef\u80fd\u3002", "conclusion": "\u5c06\u9ad8\u4fdd\u771f\u6a21\u62df\u3001\u591a\u6a21\u6001\u4f20\u611f\u5668\u878d\u5408\u548c\u51e0\u4f55\u5efa\u6a21\u76f8\u7ed3\u5408\uff0c\u663e\u7740\u6539\u5584\u4e86\u81ea\u4e3b\u8840\u7ba1\u5185\u5bfc\u822a\uff0c\u5e76\u652f\u6301\u66f4\u5b89\u5168\u3001\u66f4\u7cbe\u786e\u7684\u5fae\u521b\u624b\u672f\u3002"}}
{"id": "2512.18388", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18388", "abs": "https://arxiv.org/abs/2512.18388", "authors": ["Chao Wen", "Tung Phung", "Pronita Mehrotra", "Sumit Gulwani", "Tomohiro Nagashima", "Adish Singla"], "title": "Exploration vs. Fixation: Scaffolding Divergent and Convergent Thinking for Human-AI Co-Creation with Generative Models", "comment": "Preprint", "summary": "Generative AI has begun to democratize creative work, enabling novices to produce complex artifacts such as code, images, and videos. However, in practice, existing interaction paradigms often fail to support divergent exploration: users tend to converge too quickly on early ``good enough'' results and struggle to move beyond them, leading to premature convergence and design fixation that constrains their creative potential. To address this, we propose a structured, process-oriented human-AI co-creation paradigm including divergent and convergent thinking stages, grounded in Wallas's model of creativity. To avoid design fixation, our paradigm scaffolds both high-level exploration of conceptual ideas in the early divergent thinking phase and low-level exploration of variations in the later convergent thinking phrase. We instantiate this paradigm in HAIExplore, an image co-creation system that (i) scaffolds divergent thinking through a dedicated brainstorming stage for exploring high-level ideas in a conceptual space, and (ii) scaffolds convergent refinement through an interface that externalizes users' refinement intentions as interpretable parameters and options, making the refinement process more controllable and easier to explore. We report on a within-subjects study comparing HAIExplore with a widely used linear chat interface (ChatGPT) for creative image generation. Our findings show that explicitly scaffolding the creative process into brainstorming and refinement stages can mitigate design fixation, improve perceived controllability and alignment with users' intentions, and better support the non-linear nature of creative work. We conclude with design implications for future creativity support tools and human-AI co-creation workflows.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u4eba\u673a\u5171\u521b\u8303\u5f0f\uff0c\u6539\u5584\u521b\u610f\u5de5\u4f5c\u7684\u8fc7\u7a0b\uff0c\u907f\u514d\u8bbe\u8ba1\u5b9a\u52bf\uff0c\u63d0\u5347\u7528\u6237\u7684\u521b\u9020\u6f5c\u529b\u3002", "motivation": "\u73b0\u6709\u4ea4\u4e92\u6a21\u5f0f\u672a\u80fd\u652f\u6301\u53d1\u6563\u63a2\u7d22\uff0c\u7528\u6237\u5f80\u5f80\u8fc7\u65e9\u6536\u655b\u4e8e\u521d\u6b65\u7ed3\u679c\uff0c\u9650\u5236\u4e86\u521b\u4f5c\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u7684\u4eba\u673a\u5171\u521b\u8303\u5f0f\uff0c\u5305\u62ec\u53d1\u6563\u548c\u6536\u655b\u601d\u7ef4\u9636\u6bb5\uff0c\u57fa\u4e8eWallas\u7684\u521b\u610f\u6a21\u578b\u3002", "result": "\u5c06\u8be5\u8303\u5f0f\u5b9e\u4f8b\u5316\u4e3aHAIExplore\uff0c\u4e00\u4e2a\u56fe\u50cf\u5171\u521b\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e13\u95e8\u7684\u5934\u8111\u98ce\u66b4\u9636\u6bb5\u6765\u652f\u6301\u53d1\u6563\u601d\u7ef4\uff0c\u5e76\u901a\u8fc7\u4e00\u4e2a\u63a5\u53e3\u6765\u652f\u6301\u6536\u655b\u7684\u7ec6\u5316\u8fc7\u7a0b\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u8868\u660e\uff0c\u660e\u786e\u5730\u5c06\u521b\u4f5c\u8fc7\u7a0b\u5206\u4e3a\u5934\u8111\u98ce\u66b4\u548c\u7ec6\u5316\u9636\u6bb5\u53ef\u4ee5\u7f13\u89e3\u8bbe\u8ba1\u5b9a\u52bf\uff0c\u63d0\u9ad8\u53ef\u63a7\u6027\u548c\u4e0e\u7528\u6237\u610f\u56fe\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u66f4\u597d\u5730\u652f\u6301\u521b\u4f5c\u5de5\u4f5c\u7684\u975e\u7ebf\u6027\u7279\u6027\u3002"}}
{"id": "2512.18146", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.18146", "abs": "https://arxiv.org/abs/2512.18146", "authors": ["Stergios E. Bachoumas", "Panagiotis Artemiadis"], "title": "On Swarm Leader Identification using Probing Policies", "comment": "13 pages, journal", "summary": "Identifying the leader within a robotic swarm is crucial, especially in adversarial contexts where leader concealment is necessary for mission success. This work introduces the interactive Swarm Leader Identification (iSLI) problem, a novel approach where an adversarial probing agent identifies a swarm's leader by physically interacting with its members. We formulate the iSLI problem as a Partially Observable Markov Decision Process (POMDP) and employ Deep Reinforcement Learning, specifically Proximal Policy Optimization (PPO), to train the prober's policy. The proposed approach utilizes a novel neural network architecture featuring a Timed Graph Relationformer (TGR) layer combined with a Simplified Structured State Space Sequence (S5) model. The TGR layer effectively processes graph-based observations of the swarm, capturing temporal dependencies and fusing relational information using a learned gating mechanism to generate informative representations for policy learning. Extensive simulations demonstrate that our TGR-based model outperforms baseline graph neural network architectures and exhibits significant zero-shot generalization capabilities across varying swarm sizes and speeds different from those used during training. The trained prober achieves high accuracy in identifying the leader, maintaining performance even in out-of-training distribution scenarios, and showing appropriate confidence levels in its predictions. Real-world experiments with physical robots further validate the approach, confirming successful sim-to-real transfer and robustness to dynamic changes, such as unexpected agent disconnections.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4ea4\u4e92\u5f0f\u7fa4\u4f53\u9886\u5bfc\u8005\u8bc6\u522b\u95ee\u9898\uff08iSLI\uff09\uff0c\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u548c\u65b0\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u6765\u8bc6\u522b\u673a\u5668\u4eba\u7fa4\u4f53\u4e2d\u7684\u9886\u5bfc\u8005\u3002", "motivation": "\u5728\u5bf9\u6297\u73af\u5883\u4e2d\uff0c\u8bc6\u522b\u673a\u5668\u4eba\u7fa4\u4f53\u4e2d\u7684\u9886\u5bfc\u8005\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5f53\u9886\u5bfc\u8005\u9700\u8981\u9690\u853d\u4ee5\u786e\u4fdd\u4efb\u52a1\u6210\u529f\u65f6\u3002", "method": "\u5c06iSLI\u95ee\u9898\u8868\u8ff0\u4e3a\u90e8\u5206\u53ef\u89c2\u5bdf\u7684\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08POMDP\uff09\uff0c\u4f7f\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u8bad\u7ec3\u63a2\u6d4b\u8005\u7684\u7b56\u7565\u3002\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7ed3\u5408\u4e86Timed Graph Relationformer\uff08TGR\uff09\u5c42\u548c\u7b80\u5316\u7684\u7ed3\u6784\u5316\u72b6\u6001\u7a7a\u95f4\u5e8f\u5217\uff08S5\uff09\u6a21\u578b\u3002", "result": "\u5e7f\u6cdb\u7684\u6a21\u62df\u8868\u660e\uff0cTGR\u57fa\u7840\u7684\u6a21\u578b\u5728\u8bc6\u522b\u9886\u5bfc\u8005\u65b9\u9762\u7684\u51c6\u786e\u6027\u9ad8\u4e8e\u57fa\u7ebf\u56fe\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5728\u4e0d\u540c\u7fa4\u4f53\u89c4\u6a21\u548c\u901f\u5ea6\u4e0b\u5177\u6709\u663e\u8457\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u786e\u8ba4\u4e86\u4ece\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8f6c\u79fb\u4ee5\u53ca\u5bf9\u52a8\u6001\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.18413", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.18413", "abs": "https://arxiv.org/abs/2512.18413", "authors": ["Xijia Wei", "Ting Dang", "Khaldoon Al-Naimi", "Yang Liu", "Fahim Kawsar", "Alessandro Montanari"], "title": "Listening to the Mind: Earable Acoustic Sensing of Cognitive Load", "comment": "Accepted by UbiComp 2025 Beyond Sound Workshop", "summary": "Earable acoustic sensing offers a powerful and non-invasive modality for capturing fine-grained auditory and physiological signals directly from the ear canal, enabling continuous and context-aware monitoring of cognitive states. As earable devices become increasingly embedded in daily life, they provide a unique opportunity to sense mental effort and perceptual load in real time through auditory interactions. In this study, we present the first investigation of cognitive load inference through auditory perception using acoustic signals captured by off-the-shelf in-ear devices. We designed speech-based listening tasks to induce varying levels of cognitive load, while concurrently embedding acoustic stimuli to evoke Stimulus Frequency Otoacoustic Emission (SFOAEs) as a proxy for cochlear responsiveness. Statistical analysis revealed a significant association (p < 0.01) between increased cognitive load and changes in auditory sensitivity, with 63.2% of participants showing peak sensitivity at 3 kHz. Notably, sensitivity patterns also varied across demographic subgroups, suggesting opportunities for personalized sensing. Our findings demonstrate that earable acoustic sensing can support scalable, real-time cognitive load monitoring in natural settings, laying a foundation for future applications in augmented cognition, where everyday auditory technologies adapt to and support the users mental health.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u901a\u8fc7\u8033\u53ef\u7a7f\u6234\u8bbe\u5907\u6355\u83b7\u7684\u58f0\u5b66\u4fe1\u53f7\u63a8\u65ad\u8ba4\u77e5\u8d1f\u8377\u7684\u53ef\u80fd\u6027\uff0c\u53d1\u73b0\u8ba4\u77e5\u8d1f\u8377\u4e0e\u542c\u89c9\u654f\u611f\u6027\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u76f8\u5173\u6027\u3002", "motivation": "\u968f\u7740\u8033\u53ef\u7a7f\u6234\u8bbe\u5907\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u7684\u9010\u6e10\u666e\u53ca\uff0c\u4f7f\u7528\u8fd9\u4e9b\u8bbe\u5907\u76d1\u6d4b\u5b9e\u65f6\u5fc3\u7406\u8d1f\u62c5\u548c\u611f\u77e5\u8d1f\u8377\u7684\u673a\u4f1a\u663e\u8457\u589e\u52a0\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u8bed\u97f3\u7684\u542c\u89c9\u4efb\u52a1\u4ee5\u8bf1\u5bfc\u4e0d\u540c\u6c34\u5e73\u7684\u8ba4\u77e5\u8d1f\u8377\uff0c\u540c\u65f6\u5d4c\u5165\u58f0\u5b66\u523a\u6fc0\u4ee5\u8bf1\u53d1\u523a\u6fc0\u9891\u7387\u8033\u58f0\u53d1\u5c04(SFOAEs)\uff0c\u5e76\u5bf9\u6570\u636e\u8fdb\u884c\u7edf\u8ba1\u5206\u6790\u3002", "result": "\u8033\u6735\u97f3\u9891\u611f\u77e5\u80fd\u591f\u975e\u4fb5\u5165\u6027\u5730\u6355\u6349\u8033\u9053\u4e2d\u7684\u7ec6\u5fae\u542c\u89c9\u548c\u751f\u7406\u4fe1\u53f7\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u8ba4\u77e5\u72b6\u6001\u7684\u8fde\u7eed\u548c\u80cc\u666f\u611f\u77e5\u76d1\u63a7\u3002", "conclusion": "\u8033\u6735\u97f3\u9891\u611f\u77e5\u53ef\u4ee5\u652f\u6301\u5728\u81ea\u7136\u73af\u5883\u4e2d\u8fdb\u884c\u53ef\u6269\u5c55\u7684\u5b9e\u65f6\u8ba4\u77e5\u8d1f\u8377\u76d1\u6d4b\uff0c\u4e3a\u672a\u6765\u589e\u5f3a\u8ba4\u77e5\u5e94\u7528\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2512.18206", "categories": ["cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.18206", "abs": "https://arxiv.org/abs/2512.18206", "authors": ["Trevor Stepp", "Parthan Olikkal", "Ramana Vinjamuri", "Rajasekhar Anguluri"], "title": "Alternating Minimization for Time-Shifted Synergy Extraction in Human Hand Coordination", "comment": "7 pages, 5 figures", "summary": "Identifying motor synergies -- coordinated hand joint patterns activated at task-dependent time shifts -- from kinematic data is central to motor control and robotics. Existing two-stage methods first extract candidate waveforms (via SVD) and then select shifted templates using sparse optimization, requiring at least two datasets and complicating data collection. We introduce an optimization-based framework that jointly learns a small set of synergies and their sparse activation coefficients. The formulation enforces group sparsity for synergy selection and element-wise sparsity for activation timing. We develop an alternating minimization method in which coefficient updates decouple across tasks and synergy updates reduce to regularized least-squares problems. Our approach requires only a single data set, and simulations show accurate velocity reconstruction with compact, interpretable synergies.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u8054\u5408\u5b66\u4e60\u8fd0\u52a8\u534f\u540c\u4f5c\u7528\u548c\u6fc0\u6d3b\u7cfb\u6570\uff0c\u7b80\u5316\u4e86\u6570\u636e\u8981\u6c42\u5e76\u63d0\u9ad8\u4e86\u91cd\u6784\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u8fd0\u52a8\u63a7\u5236\u548c\u673a\u5668\u4eba\u9886\u57df\uff0c\u8bc6\u522b\u8fd0\u52a8\u534f\u540c\u4f5c\u7528\uff08\u624b\u90e8\u5173\u8282\u5728\u4efb\u52a1\u4f9d\u8d56\u7684\u65f6\u95f4\u504f\u79fb\u4e0b\u7684\u534f\u8c03\u6a21\u5f0f\uff09\u662f\u81f3\u5173\u91cd\u8981\u7684\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f18\u5316\u7684\u6846\u67b6\uff0c\u8054\u5408\u5b66\u4e60\u4e00\u5c0f\u7ec4\u534f\u540c\u4f5c\u7528\u53ca\u5176\u7a00\u758f\u6fc0\u6d3b\u7cfb\u6570\u3002\u8be5\u65b9\u6cd5\u5f3a\u5236\u5b9e\u65bd\u534f\u540c\u9009\u62e9\u7684\u7ec4\u7a00\u758f\u6027\u548c\u6fc0\u6d3b\u65f6\u95f4\u7684\u9010\u5143\u7d20\u7a00\u758f\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u4ea4\u66ff\u6700\u5c0f\u5316\u65b9\u6cd5\uff0c\u66f4\u65b0\u7cfb\u6570\u5728\u4efb\u52a1\u95f4\u89e3\u8026\uff0c\u800c\u534f\u540c\u66f4\u65b0\u7b80\u5316\u4e3a\u6b63\u5219\u5316\u6700\u5c0f\u4e8c\u4e58\u95ee\u9898\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6848\u53ea\u9700\u4e00\u4e2a\u6570\u636e\u96c6\uff0c\u4e14\u6a21\u62df\u7ed3\u679c\u663e\u793a\u51fa\u51c6\u786e\u7684\u901f\u5ea6\u91cd\u6784\uff0c\u5177\u6709\u7d27\u51d1\u4e14\u53ef\u89e3\u91ca\u7684\u534f\u540c\u4f5c\u7528\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5316\u7684\u65b9\u6cd5\u6765\u8bc6\u522b\u548c\u5229\u7528\u8fd0\u52a8\u534f\u540c\u4f5c\u7528\uff0c\u6709\u52a9\u4e8e\u6539\u5584\u8fd0\u52a8\u63a7\u5236\u548c\u673a\u5668\u4eba\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.18616", "categories": ["cs.HC", "cs.AI", "cs.CR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.18616", "abs": "https://arxiv.org/abs/2512.18616", "authors": ["Zelin Wan", "Han Jun Yoon", "Nithin Alluru", "Terrence J. Moore", "Frederica F. Nelson", "Seunghyun Yoon", "Hyuk Lim", "Dan Dongseong Kim", "Jin-Hee Cho"], "title": "DASH: Deception-Augmented Shared Mental Model for a Human-Machine Teaming System", "comment": "17 pages, 16 figures", "summary": "We present DASH (Deception-Augmented Shared mental model for Human-machine teaming), a novel framework that enhances mission resilience by embedding proactive deception into Shared Mental Models (SMM). Designed for mission-critical applications such as surveillance and rescue, DASH introduces \"bait tasks\" to detect insider threats, e.g., compromised Unmanned Ground Vehicles (UGVs), AI agents, or human analysts, before they degrade team performance. Upon detection, tailored recovery mechanisms are activated, including UGV system reinstallation, AI model retraining, or human analyst replacement. In contrast to existing SMM approaches that neglect insider risks, DASH improves both coordination and security. Empirical evaluations across four schemes (DASH, SMM-only, no-SMM, and baseline) show that DASH sustains approximately 80% mission success under high attack rates, eight times higher than the baseline. This work contributes a practical human-AI teaming framework grounded in shared mental models, a deception-based strategy for insider threat detection, and empirical evidence of enhanced robustness under adversarial conditions. DASH establishes a foundation for secure, adaptive human-machine teaming in contested environments.", "AI": {"tldr": "DASH\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u4eba\u673a\u5408\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4e3b\u52a8\u6b3a\u9a97\u7eb3\u5165\u5171\u4eab\u5fc3\u7406\u6a21\u578b\uff0c\u4ee5\u63d0\u5347\u4efb\u52a1\u7684\u97e7\u6027\u548c\u5b89\u5168\u6027\u3002\u5728\u9ad8\u653b\u51fb\u7387\u73af\u5883\u4e0b\uff0c\u8be5\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u5171\u4eab\u5fc3\u7406\u6a21\u578b\u65b9\u6cd5\u4e2d\u5ffd\u89c6\u5185\u90e8\u5a01\u80c1\u7684\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u5347\u5173\u952e\u4efb\u52a1\u5e94\u7528\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "method": "DASH\u6846\u67b6\u5f15\u5165\u4e86\"\u8bf1\u9975\u4efb\u52a1\"\u4ee5\u68c0\u6d4b\u5185\u90e8\u5a01\u80c1\uff0c\u5e76\u5728\u68c0\u6d4b\u5230\u5a01\u80c1\u65f6\u6fc0\u6d3b\u5b9a\u5236\u7684\u6062\u590d\u673a\u5236\u3002", "result": "DASH\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u5b83\u5728\u9ad8\u653b\u51fb\u7387\u4e0b\u7684\u4efb\u52a1\u6210\u529f\u7387\u8fbe\u5230\u7ea680%\uff0c\u662f\u57fa\u7ebf\u6846\u67b6\u7684\u516b\u500d\u3002", "conclusion": "DASH\u662f\u4e00\u4e2a\u6709\u6548\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5d4c\u5165\u4e3b\u52a8\u6b3a\u9a97\u6765\u589e\u5f3a\u4eba\u673a\u56e2\u961f\u7684\u4efb\u52a1\u97e7\u6027\uff0c\u5e76\u5728\u9ad8\u653b\u51fb\u7387\u4e0b\u4fdd\u6301\u7ea680%\u7684\u4efb\u52a1\u6210\u529f\u7387\u3002"}}
{"id": "2512.18211", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18211", "abs": "https://arxiv.org/abs/2512.18211", "authors": ["Yudong Liu", "Spencer Hallyburton", "Jiwoo Kim", "Yueqian Lin", "Yiming Li", "Qinsi Wang", "Hui Ye", "Jingwei Sun", "Miroslav Pajic", "Yiran Chen", "Hai Li"], "title": "LLaViDA: A Large Language Vision Driving Assistant for Explicit Reasoning and Enhanced Trajectory Planning", "comment": null, "summary": "Trajectory planning is a fundamental yet challenging component of autonomous driving. End-to-end planners frequently falter under adverse weather, unpredictable human behavior, or complex road layouts, primarily because they lack strong generalization or few-shot capabilities beyond their training data. We propose LLaViDA, a Large Language Vision Driving Assistant that leverages a Vision-Language Model (VLM) for object motion prediction, semantic grounding, and chain-of-thought reasoning for trajectory planning in autonomous driving. A two-stage training pipeline--supervised fine-tuning followed by Trajectory Preference Optimization (TPO)--enhances scene understanding and trajectory planning by injecting regression-based supervision, produces a powerful \"VLM Trajectory Planner for Autonomous Driving.\" On the NuScenes benchmark, LLaViDA surpasses state-of-the-art end-to-end and other recent VLM/LLM-based baselines in open-loop trajectory planning task, achieving an average L2 trajectory error of 0.31 m and a collision rate of 0.10% on the NuScenes test set. The code for this paper is available at GitHub.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLLaViDA\u7684\u8f68\u8ff9\u89c4\u5212\u7cfb\u7edf\uff0c\u5229\u7528\u89c6\u8bed\u8a00\u6a21\u578b(VLM)\u8fdb\u884c\u8fd0\u52a8\u9884\u6d4b\u548c\u8bed\u4e49\u5bf9\u5e94\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u63d0\u5347\u81ea\u4e3b\u9a7e\u9a76\u573a\u666f\u7406\u89e3\u548c\u8f68\u8ff9\u89c4\u5212\u80fd\u529b\u3002", "motivation": "\u81ea\u4e3b\u9a7e\u9a76\u4e2d\u7684\u8f68\u8ff9\u89c4\u5212\u9762\u4e34\u6076\u52a3\u5929\u6c14\u3001\u4e0d\u53ef\u9884\u6d4b\u7684\u4eba\u7c7b\u884c\u4e3a\u548c\u590d\u6742\u9053\u8def\u5e03\u5c40\u7684\u6311\u6218\uff0c\u73b0\u6709\u7684\u7aef\u5230\u7aef\u89c4\u5212\u5668\u7f3a\u4e4f\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u76d1\u7763\u5fae\u8c03\u548c\u8f68\u8ff9\u504f\u597d\u4f18\u5316(TPO)\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u7ba1\u9053\uff0c\u901a\u8fc7\u56de\u5f52\u76d1\u7763\u589e\u5f3a\u573a\u666f\u7406\u89e3\u4e0e\u8f68\u8ff9\u89c4\u5212\u3002", "result": "\u5728NuScenes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLLaViDA\u5728\u5f00\u653e\u5faa\u73af\u8f68\u8ff9\u89c4\u5212\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e860.31\u7c73\u7684\u5e73\u5747L2\u8f68\u8ff9\u8bef\u5dee\u548c0.10%\u7684\u78b0\u649e\u7387\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u7cfb\u7edf\u3002", "conclusion": "LLaViDA\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u7aef\u5230\u7aef\u89c4\u5212\u5668\u548c\u5176\u4ed6\u57fa\u4e8eVLM/LLM\u7684\u57fa\u51c6\uff0c\u4e3a\u81ea\u4e3b\u9a7e\u9a76\u7684\u8f68\u8ff9\u89c4\u5212\u63d0\u4f9b\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u65b9\u6cd5\u3002"}}
{"id": "2512.18889", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.18889", "abs": "https://arxiv.org/abs/2512.18889", "authors": ["Ashley Colley", "Emma Kirjavainen", "Sari Tapio", "Jonna H\u00e4kkil\u00e4"], "title": "Household Plastic Recycling: Empirical Insights and Design Explorations", "comment": null, "summary": "This article examines household plastic recycling in Finland through two qualitative studies and four design concepts. Study 1 reports short interviews with residents about how they store, sort, and dispose of plastic packaging in their homes. The findings highlight recurring frictions: limited space, improvised storage, uncertainty about correct sorting, and difficulties with bulky or dirty items. Study 2 focuses on laundry detergent packaging as a common source of large plastic containers. Participants' purchase decisions prioritised price and cleaning performance, while expressing concern for environmental impact and confusion about materials, rinsing, and recyclability.\n  Building on these insights, four student groups designed interactive recycling concepts that combine physical bins or bags with mobile applications. The concepts explore modular storage, sensing and compaction, playful feedback, and reward schemes to support domestic recycling routines. Together, the studies and concepts point to design opportunities at the intersection of packaging, home infrastructure, and digital services, while also raising questions about feasibility, privacy, and the cost of new devices.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u82ac\u5170\u5bb6\u5ead\u5851\u6599\u56de\u6536\uff0c\u901a\u8fc7\u5b9a\u6027\u7814\u7a76\u53d1\u73b0\u5c45\u6c11\u5728\u5b58\u50a8\u548c\u5206\u7c7b\u65f6\u9762\u4e34\u7684\u6469\u64e6\uff0c\u63d0\u51fa\u4e86\u8bbe\u8ba1\u4e92\u52a8\u56de\u6536\u6982\u5ff5\u7684\u673a\u4f1a\u3002", "motivation": "\u63a2\u7d22\u5305\u88c5\u3001\u5bb6\u5ead\u57fa\u7840\u8bbe\u65bd\u548c\u6570\u5b57\u670d\u52a1\u4ea4\u6c47\u5904\u7684\u8bbe\u8ba1\u673a\u4f1a\uff0c\u89e3\u51b3\u5bb6\u5ead\u56de\u6536\u4e2d\u7684\u56f0\u96be\u3002", "method": "\u901a\u8fc7\u4e24\u9879\u5b9a\u6027\u7814\u7a76\u548c\u56db\u4e2a\u8bbe\u8ba1\u6982\u5ff5\u7814\u7a76\u82ac\u5170\u5bb6\u5ead\u5851\u6599\u56de\u6536\u3002", "result": "\u7814\u7a761\u901a\u8fc7\u4e0e\u5c45\u6c11\u7684\u77ed\u8bbf\u8c08\uff0c\u53d1\u73b0\u5b58\u50a8\u3001\u5206\u7c7b\u548c\u5904\u7406\u5851\u6599\u5305\u88c5\u65f6\u5b58\u5728\u7684\u4e3b\u8981\u6469\u64e6\u70b9\uff0c\u5305\u62ec\u7a7a\u95f4\u6709\u9650\u3001\u4e34\u65f6\u5b58\u50a8\u3001\u5bf9\u6b63\u786e\u5206\u7c7b\u7684\u7591\u60d1\uff0c\u4ee5\u53ca\u5904\u7406\u7b28\u91cd\u6216\u810f\u7269\u54c1\u7684\u56f0\u96be\u3002\u7814\u7a762\u5173\u6ce8\u6d17\u6da4\u5242\u5305\u88c5\uff0c\u53c2\u4e0e\u8005\u5728\u8d2d\u4e70\u65f6\u4f18\u5148\u8003\u8651\u4ef7\u683c\u548c\u6e05\u6d01\u6548\u679c\uff0c\u540c\u65f6\u5bf9\u73af\u4fdd\u5f71\u54cd\u3001\u6750\u6599\u3001\u51b2\u6d17\u548c\u53ef\u56de\u6536\u6027\u8868\u793a\u56f0\u60d1\u3002", "conclusion": "\u57fa\u4e8e\u8fd9\u4e9b\u89c1\u89e3\uff0c\u56db\u4e2a\u5b66\u751f\u5c0f\u7ec4\u8bbe\u8ba1\u4e86\u7ed3\u5408\u7269\u7406\u5783\u573e\u6876\u6216\u888b\u5b50\u4e0e\u79fb\u52a8\u5e94\u7528\u7684\u4e92\u52a8\u56de\u6536\u6982\u5ff5\u3002"}}
{"id": "2512.18213", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.18213", "abs": "https://arxiv.org/abs/2512.18213", "authors": ["Wu-Te Yang", "Masayoshi Tomizuka"], "title": "Fractional-order Modeling for Nonlinear Soft Actuators via Particle Swarm Optimization", "comment": "6 pages, 4 figures", "summary": "Modeling soft pneumatic actuators with high precision remains a fundamental challenge due to their highly nonlinear and compliant characteristics. This paper proposes an innovative modeling framework based on fractional-order differential equations (FODEs) to accurately capture the dynamic behavior of soft materials. The unknown parameters within the fractional-order model are identified using particle swarm optimization (PSO), enabling parameter estimation directly from experimental data without reliance on pre-established material databases or empirical constitutive laws. The proposed approach effectively represents the complex deformation phenomena inherent in soft actuators. Experimental results validate the accuracy and robustness of the developed model, demonstrating improvement in predictive performance compared to conventional modeling techniques. The presented framework provides a data-efficient and database-independent solution for soft actuator modeling, advancing the precision and adaptability of soft robotic system design.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u6570\u9636\u5fae\u5206\u65b9\u7a0b\u7684\u9ad8\u7cbe\u5ea6\u5efa\u6a21\u6846\u67b6\uff0c\u91c7\u7528\u7c92\u5b50\u7fa4\u4f18\u5316\u4ece\u5b9e\u9a8c\u6570\u636e\u4e2d\u4f30\u8ba1\u53c2\u6570\uff0c\u6210\u529f\u514b\u670d\u4e86\u4f20\u7edf\u5efa\u6a21\u6280\u672f\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u8f6f\u6c14\u52a8\u9a71\u52a8\u5668\u7684\u9ad8\u5ea6\u975e\u7ebf\u6027\u548c\u987a\u5e94\u7279\u6027\u4f7f\u5f97\u9ad8\u7cbe\u5ea6\u5efa\u6a21\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u5efa\u6a21\u7cbe\u5ea6\u3002", "method": "\u57fa\u4e8e\u5206\u6570\u9636\u5fae\u5206\u65b9\u7a0b\uff08FODE\uff09\u7684\u5efa\u6a21\u6846\u67b6\uff0c\u5229\u7528\u7c92\u5b50\u7fa4\u4f18\u5316\uff08PSO\uff09\u8bc6\u522b\u672a\u77e5\u53c2\u6570\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u6210\u529f\u6355\u6349\u4e86\u8f6f\u6750\u6599\u7684\u52a8\u6001\u884c\u4e3a\uff0c\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u7a33\u5065\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u6570\u636e\u5e93\u4f9d\u8d56\u7684\u8f6f\u9a71\u52a8\u5668\u5efa\u6a21\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u5347\u4e86\u8f6f\u673a\u5668\u4eba\u7cfb\u7edf\u8bbe\u8ba1\u7684\u7cbe\u786e\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2512.18920", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.18920", "abs": "https://arxiv.org/abs/2512.18920", "authors": ["Oliver Huang", "Muhammad Fatir", "Steven Luo", "Sangho Suh", "Hariharan Subramonyam", "Carolina Nobre"], "title": "Narrative Scaffolding: Transforming Data-Driven Sensemaking Through Narrative-First Exploration", "comment": "IUI 2026", "summary": "When exploring data, analysts construct narratives about what the data means by asking questions, generating visualizations, reflecting on patterns, and revising their interpretations as new insights emerge. Yet existing analysis tools treat narrative as an afterthought, breaking the link between reasoning, reflection, and the evolving story from exploration. Consequently, analysts lose the ability to see how their reasoning evolves, making it harder to reflect systematically or build coherent explanations. To address this gap, we propose Narrative Scaffolding, a framework for narrative-driven exploration that positions narrative construction as the primary interface for exploration and reasoning. We implement this framework in a system that externalizes iterative reasoning through narrative-first entry, semantically aligned view generation, and reflection support via insight provenance and inquiry tracking. In a within-subject study N=20, we demonstrate that narrative scaffolding facilitates broader exploration, deeper reflection, and more defensible narratives. An evaluation with visualization literacy experts (N = 6) confirmed that the system produced outputs aligned with narrative intent and facilitated intentional exploration.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u53d9\u4e8b\u9a71\u52a8\u7684\u63a2\u7d22\u6846\u67b6\u2014\u2014\u53d9\u4e8b\u652f\u67b6\uff0c\u901a\u8fc7\u5c06\u53d9\u4e8b\u6784\u5efa\u4f5c\u4e3a\u63a2\u7d22\u548c\u63a8\u7406\u7684\u4e3b\u8981\u63a5\u53e3\uff0c\u6539\u5584\u6570\u636e\u5206\u6790\u5de5\u5177\u7684\u5c40\u9650\u6027\uff0c\u4f7f\u5206\u6790\u5e08\u80fd\u591f\u66f4\u597d\u5730\u53cd\u601d\u548c\u6784\u5efa\u8fde\u8d2f\u7684\u89e3\u91ca\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u636e\u5206\u6790\u5de5\u5177\u5c06\u53d9\u4e8b\u89c6\u4e3a\u9644\u5e26\u5185\u5bb9\uff0c\u5f71\u54cd\u5206\u6790\u5e08\u5bf9\u63a8\u7406\u6f14\u53d8\u7684\u7406\u89e3\u548c\u7cfb\u7edf\u53cd\u601d\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u53d9\u4e8b\u652f\u67b6\u6846\u67b6\uff0c\u5e76\u5728\u4e00\u4e2a\u7cfb\u7edf\u4e2d\u5b9e\u73b0\uff0c\u5916\u90e8\u5316\u8fed\u4ee3\u63a8\u7406\uff0c\u901a\u8fc7\u53d9\u4e8b\u4f18\u5148\u8f93\u5165\u3001\u8bed\u4e49\u5bf9\u9f50\u89c6\u56fe\u751f\u6210\u4ee5\u53ca\u901a\u8fc7\u6d1e\u5bdf\u6765\u6e90\u548c\u67e5\u8be2\u8ddf\u8e2a\u7684\u53cd\u601d\u652f\u6301\u3002", "result": "\u5728\u4e00\u6b21\u5305\u542b20\u540d\u53c2\u4e0e\u8005\u7684\u7814\u7a76\u4e2d\uff0c\u7ed3\u679c\u8868\u660e\u53d9\u4e8b\u652f\u67b6\u6709\u6548\u63a8\u52a8\u4e86\u66f4\u5e7f\u6cdb\u7684\u63a2\u7d22\u3001\u6df1\u5165\u7684\u53cd\u601d\u548c\u66f4\u5177\u8fa9\u62a4\u6027\u7684\u53d9\u4e8b\u3002", "conclusion": "\u53d9\u4e8b\u652f\u67b6\u80fd\u591f\u4fc3\u8fdb\u8f83\u5e7f\u6cdb\u7684\u63a2\u7d22\u3001\u6df1\u5165\u7684\u53cd\u601d\u4ee5\u53ca\u66f4\u5177\u8fa9\u62a4\u6027\u7684\u53d9\u4e8b\uff0c\u4e14\u5728\u4e0e\u53ef\u89c6\u5316\u7d20\u517b\u4e13\u5bb6\u7684\u8bc4\u4f30\u4e2d\uff0c\u7cfb\u7edf\u751f\u6210\u7684\u7ed3\u679c\u7b26\u5408\u53d9\u4e8b\u610f\u56fe\u5e76\u4fc3\u8fdb\u4e86\u6709\u610f\u63a2\u7d22\u3002"}}
{"id": "2512.18268", "categories": ["cs.RO", "cs.CG"], "pdf": "https://arxiv.org/pdf/2512.18268", "abs": "https://arxiv.org/abs/2512.18268", "authors": ["Si Wei Feng"], "title": "On The Computational Complexity for Minimizing Aerial Photographs for Full Coverage of a Planar Region", "comment": null, "summary": "With the popularity of drone technologies, aerial photography have become prevalent in many daily scenarios such as environment monitoring, structure inspection, law enforcement etc. A central challenge in this domain is the efficient coverage of a target area with photographs that can entirely capture the region, while respecting constraints such as the image resolution, and limited number of pictures that can be taken. This work investigates the computational complexity of several fundamental problems arised from this challenge. By abstracting the aerial photography problem into the coverage problems in computational geometry, we demonstrate that most of these problems are in fact computationally intractable, with the implication that traditional algorithms cannot solve them efficiently. The intuitions of this work can extend beyond aerial photography to broader applications such as pesticide spraying, and strategic sensor placement.", "AI": {"tldr": "\u65e0\u4eba\u673a\u6280\u672f\u7684\u666e\u53ca\u5bfc\u81f4\u822a\u7a7a\u6444\u5f71\u5728\u591a\u4e2a\u65e5\u5e38\u573a\u666f\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5bf9\u76ee\u6807\u533a\u57df\u7684\u6709\u6548\u8986\u76d6\u4ecd\u7136\u662f\u4e00\u9879\u6311\u6218\uff0c\u672c\u6587\u5206\u6790\u4e86\u76f8\u5173\u8ba1\u7b97\u590d\u6742\u6027\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u6280\u672f\u7684\u666e\u53ca\uff0c\u822a\u7a7a\u6444\u5f71\u5728\u73af\u5883\u76d1\u6d4b\u3001\u7ed3\u6784\u68c0\u67e5\u548c\u6267\u6cd5\u7b49\u591a\u4e2a\u573a\u666f\u4e2d\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002", "method": "\u5c06\u822a\u7a7a\u6444\u5f71\u95ee\u9898\u62bd\u8c61\u4e3a\u8ba1\u7b97\u51e0\u4f55\u4e2d\u7684\u8986\u76d6\u95ee\u9898\uff0c\u5206\u6790\u4e86\u76f8\u5173\u95ee\u9898\u7684\u8ba1\u7b97\u590d\u6742\u6027\u3002", "result": "\u5927\u591a\u6570\u76f8\u5173\u95ee\u9898\u88ab\u8bc1\u660e\u5728\u8ba1\u7b97\u4e0a\u4e0d\u53ef\u89e3\uff0c\u8868\u660e\u4f20\u7edf\u7b97\u6cd5\u65e0\u6cd5\u9ad8\u6548\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u7684\u76f4\u89c9\u4e0d\u4ec5\u9002\u7528\u4e8e\u822a\u7a7a\u6444\u5f71\uff0c\u8fd8\u53ef\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\uff0c\u5982\u519c\u836f\u55b7\u6d12\u548c\u4f20\u611f\u5668\u5e03\u5c40\u3002"}}
{"id": "2512.19047", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.19047", "abs": "https://arxiv.org/abs/2512.19047", "authors": ["Md Minhazul Islam Munna", "Al Amin", "Xin Wang", "Hongbin Ma"], "title": "Advancing Accessibility: Augmented Reality Solutions for the Blind and Disabled in Bangladesh", "comment": "Accepted at ISCIIA-ITCA 2024 (virtual presentation on Nov 2, 2024). Not published in proceedings", "summary": "Augmented Reality (AR) technology holds immense potential to transform the lives of blind and disabled individuals by offering enhanced interaction with their surroundings and providing real-time, accessible information. Globally, AR applications are being developed with features such as audio descriptions of environments, object recognition, and navigational aids, specifically designed to support the visually impaired. These innovations are paving the way for increased independence, mobility, and overall quality of life for millions of people worldwide. In Bangladesh, the adoption of AR technology for the blind and disabled is still in its early stages, primarily due to limited accessibility resources and infrastructure challenges. However, with the growing penetration of smartphones and continuous advancements in AR technologies, there is a promising opportunity for these solutions to be adapted, localized, and scaled within the country. This paper reviews the current state of AR technologies for the visually impaired on a global scale, explores their potential application in Bangladesh, and delves into the challenges and opportunities for broader implementation in this context.", "AI": {"tldr": "AR\u6280\u672f\u80fd\u591f\u63d0\u5347\u76f2\u4eba\u548c\u6b8b\u75be\u4eba\u7684\u751f\u6d3b\u8d28\u91cf\uff0c\u5b5f\u52a0\u62c9\u56fd\u5728\u8be5\u6280\u672f\u7684\u5e94\u7528\u4ecd\u5904\u4e8e\u521d\u7ea7\u9636\u6bb5\uff0c\u4f46\u6709\u6f5c\u529b\u901a\u8fc7\u53d1\u5c55\u548c\u6269\u5c55\u6765\u6539\u53d8\u8fd9\u4e00\u73b0\u72b6\u3002", "motivation": "\u5bfb\u6c42\u901a\u8fc7AR\u6280\u672f\u6539\u5584\u89c6\u89c9\u969c\u788d\u4eba\u58eb\u7684\u751f\u6d3b\uff0c\u53d1\u73b0\u5e76\u514b\u670d\u5b5f\u52a0\u62c9\u56fd\u5728\u5b9e\u73b0\u8fd9\u4e00\u6f5c\u529b\u65f6\u7684\u6311\u6218\u3002", "method": "\u5bf9\u5168\u7403AR\u6280\u672f\u5728\u89c6\u89c9\u969c\u788d\u4eba\u58eb\u4e2d\u7684\u5e94\u7528\u8fdb\u884c\u56de\u987e\u548c\u5206\u6790\uff0c\u63a2\u7d22\u5176\u5728\u5b5f\u52a0\u62c9\u56fd\u7684\u6f5c\u5728\u5e94\u7528\u3002", "result": "\u76ee\u524d\u5168\u7403\u6b63\u5728\u5f00\u53d1\u591a\u79cd\u9488\u5bf9\u89c6\u89c9\u969c\u788d\u8005\u7684AR\u5e94\u7528\uff0c\u529f\u80fd\u5305\u62ec\u73af\u5883\u97f3\u9891\u63cf\u8ff0\u3001\u7269\u4f53\u8bc6\u522b\u548c\u5bfc\u822a\u8f85\u52a9\uff0c\u65e8\u5728\u63d0\u9ad8\u72ec\u7acb\u6027\u548c\u751f\u6d3b\u8d28\u91cf\u3002", "conclusion": "\u5c3d\u7ba1\u5b5f\u52a0\u62c9\u56fd\u5728\u76f2\u4eba\u548c\u6b8b\u75be\u4eba\u9886\u57df\u7684AR\u6280\u672f adoption \u4ecd\u5904\u4e8e\u65e9\u671f\u9636\u6bb5\uff0c\u4f46\u968f\u7740\u667a\u80fd\u624b\u673a\u7684\u666e\u53ca\u548cAR\u6280\u672f\u7684\u4e0d\u65ad\u8fdb\u6b65\uff0c\u5b58\u5728\u9002\u5e94\u548c\u6269\u5927\u8fd9\u4e9b\u89e3\u51b3\u65b9\u6848\u7684\u673a\u4f1a\u3002"}}
{"id": "2512.18333", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.18333", "abs": "https://arxiv.org/abs/2512.18333", "authors": ["Youssef Mahran", "Zeyad Gamal", "Ayman El-Badawy"], "title": "Reinforcement Learning Position Control of a Quadrotor Using Soft Actor-Critic (SAC)", "comment": "This is the Author Accepted Manuscript version of a paper accepted for publication. The final published version is available via IEEE Xplore", "summary": "This paper proposes a new Reinforcement Learning (RL) based control architecture for quadrotors. With the literature focusing on controlling the four rotors' RPMs directly, this paper aims to control the quadrotor's thrust vector. The RL agent computes the percentage of overall thrust along the quadrotor's z-axis along with the desired Roll ($\u03c6$) and Pitch ($\u03b8$) angles. The agent then sends the calculated control signals along with the current quadrotor's Yaw angle ($\u03c8$) to an attitude PID controller. The PID controller then maps the control signals to motor RPMs. The Soft Actor-Critic algorithm, a model-free off-policy stochastic RL algorithm, was used to train the RL agents. Training results show the faster training time of the proposed thrust vector controller in comparison to the conventional RPM controllers. Simulation results show smoother and more accurate path-following for the proposed thrust vector controller.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u56db\u65cb\u7ffc\u63a7\u5236\u67b6\u6784\uff0c\u63a7\u5236\u63a8\u529b\u77e2\u91cf\u800c\u975e\u7535\u673a\u8f6c\u901f\uff0c\u663e\u793a\u51fa\u8bad\u7ec3\u65f6\u95f4\u66f4\u77ed\u53ca\u8def\u5f84\u8ddf\u8e2a\u66f4\u5e73\u6ed1\u7684\u4f18\u52bf\u3002", "motivation": "\u5728\u6587\u732e\u4e2d\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u76f4\u63a5\u63a7\u5236\u56db\u65cb\u7ffc\u7684\u7535\u673a\u8f6c\u901f\uff0c\u800c\u8be5\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u63a7\u5236\u56db\u65cb\u7ffc\u7684\u63a8\u529b\u77e2\u91cf\u6765\u63d0\u9ad8\u63a7\u5236\u6548\u679c\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63a7\u5236\u67b6\u6784\uff0c\u91c7\u7528Soft Actor-Critic\u7b97\u6cd5\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff0c\u63a7\u5236\u63a8\u529b\u6cbf\u56db\u65cb\u7ffcz\u8f74\u7684\u5206\u5e03\uff0c\u4ee5\u53ca\u671f\u671b\u7684\u6eda\u8f6c\u548c\u4fef\u4ef0\u89d2\uff0c\u6700\u7ec8\u901a\u8fc7PID\u63a7\u5236\u5668\u5c06\u63a7\u5236\u4fe1\u53f7\u6620\u5c04\u5230\u7535\u673a\u8f6c\u901f\u3002", "result": "\u8bad\u7ec3\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u7684\u63a8\u529b\u77e2\u91cf\u63a7\u5236\u5668\u76f8\u6bd4\u4e8e\u4f20\u7edf\u7684RPM\u63a7\u5236\u5668\u5177\u6709\u66f4\u5feb\u7684\u8bad\u7ec3\u65f6\u95f4\uff0c\u4eff\u771f\u7ed3\u679c\u663e\u793a\u8def\u5f84\u8ddf\u8e2a\u66f4\u52a0\u5e73\u6ed1\u548c\u51c6\u786e\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u63a8\u529b\u77e2\u91cf\u63a7\u5236\u65b9\u6cd5\u5728\u63a7\u5236\u56db\u65cb\u7ffc\u98de\u884c\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\uff0c\u63d0\u4f9b\u4e86\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u6709\u6548\u7684\u8def\u5f84\u8ddf\u8e2a\u80fd\u529b\u3002"}}
{"id": "2512.19169", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.19169", "abs": "https://arxiv.org/abs/2512.19169", "authors": ["Pierre Jehel", "Pierre-\u00c9tienne Gautier", "Judica\u00ebl Dehotin", "Flavien Viguier"], "title": "Towards a collaborative digital platform for railway infrastructure projects", "comment": null, "summary": "The management of railway infrastructure projects can be supported by collaborative digital platforms. A survey was carried out to identify the needs and expectations of the various stakeholders involved in the design and construction of railway infrastructure projects regarding collaborative platforms. These needs and expectations can then be translated into functional specifications to be included in the digital platforms. A total of 21 interviews were conducted between October and December 2022, during which 35 individuals were interviewed. Key roles were represented across the different project phases: engineers from design and construction firms, project managers, infrastructure managers. And various engineering fields were represented: civil, electrical, telecommunications, tracks, systems. These interviews were carried out by CentraleSup{\u00e9}lec | Universit{\u00e9} Paris-Saclay and by SNCF R{\u00e9}seau using a structured protocol designed to collect the specific needs of the interviewees for collaboration, as well as the guiding principles that shape both individual work practices and collaboration between professions. The resulting material was analyzed and then synthesized into a conceptual model of a collaborative digital platform for supporting the design and construction phases in a railway infrastructure project. Also, from these interviews emerged five core functionalities that the platform must offer: Providing access to existing infrastructure data; Accelerating repetitive tasks; Verifying essential project requirements; Supporting decision-making; Facilitating coordination among stakeholders.", "AI": {"tldr": "\u672c\u8bba\u6587\u63a2\u8ba8\u4e86\u94c1\u8def\u57fa\u7840\u8bbe\u65bd\u9879\u76ee\u7684\u534f\u4f5c\u6570\u5b57\u5e73\u53f0\uff0c\u901a\u8fc7\u8bbf\u8c08\u8bc6\u522b\u5229\u76ca\u76f8\u5173\u8005\u7684\u9700\u6c42\uff0c\u63d0\u51fa\u4e86\u6838\u5fc3\u529f\u80fd\u548c\u6982\u5ff5\u6a21\u578b\u3002", "motivation": "\u8bc6\u522b\u94c1\u8def\u57fa\u7840\u8bbe\u65bd\u8bbe\u8ba1\u4e0e\u5efa\u8bbe\u8fc7\u7a0b\u4e2d\uff0c\u5404\u5229\u76ca\u76f8\u5173\u8005\u5bf9\u534f\u4f5c\u5e73\u53f0\u7684\u9700\u6c42\u4e0e\u671f\u671b\uff0c\u4ee5\u4fbf\u8f6c\u5316\u4e3a\u529f\u80fd\u89c4\u683c\u3002", "method": "\u57282022\u5e7410\u6708\u81f312\u6708\u95f4\u8fdb\u884c21\u6b21\u8bbf\u8c08\uff0c\u8bbf\u8c08\u4e8635\u4f4d\u4e0d\u540c\u89d2\u8272\u7684\u4e13\u5bb6\uff0c\u5efa\u7acb\u4e00\u4e2a\u534f\u4f5c\u6570\u5b57\u5e73\u53f0\u7684\u6982\u5ff5\u6a21\u578b\u3002", "result": "\u786e\u5b9a\u4e86\u534f\u4f5c\u5e73\u53f0\u5e94\u63d0\u4f9b\u7684\u4e94\u4e2a\u6838\u5fc3\u529f\u80fd\uff0c\u5305\u62ec\u8bbf\u95ee\u57fa\u7840\u8bbe\u65bd\u6570\u636e\u3001\u52a0\u901f\u91cd\u590d\u4efb\u52a1\u3001\u9a8c\u8bc1\u9879\u76ee\u8981\u6c42\u3001\u652f\u6301\u51b3\u7b56\u53ca\u4fc3\u8fdb\u5229\u76ca\u76f8\u5173\u8005\u534f\u8c03\u3002", "conclusion": "\u901a\u8fc7\u5206\u6790\u9700\u6c42\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u6982\u5ff5\u6a21\u578b\uff0c\u6709\u52a9\u4e8e\u6539\u5584\u94c1\u8def\u57fa\u7840\u8bbe\u65bd\u9879\u76ee\u7684\u8bbe\u8ba1\u4e0e\u5efa\u8bbe\u8fc7\u7a0b\u3002"}}
{"id": "2512.18336", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.18336", "abs": "https://arxiv.org/abs/2512.18336", "authors": ["Youssef Mahran", "Zeyad Gamal", "Ayman El-Badawy"], "title": "Dynamic Entropy Tuning in Reinforcement Learning Low-Level Quadcopter Control: Stochasticity vs Determinism", "comment": "This is the Author Accepted Manuscript version of a paper accepted for publication. The final published version is available via IEEE Xplore", "summary": "This paper explores the impact of dynamic entropy tuning in Reinforcement Learning (RL) algorithms that train a stochastic policy. Its performance is compared against algorithms that train a deterministic one. Stochastic policies optimize a probability distribution over actions to maximize rewards, while deterministic policies select a single deterministic action per state. The effect of training a stochastic policy with both static entropy and dynamic entropy and then executing deterministic actions to control the quadcopter is explored. It is then compared against training a deterministic policy and executing deterministic actions. For the purpose of this research, the Soft Actor-Critic (SAC) algorithm was chosen for the stochastic algorithm while the Twin Delayed Deep Deterministic Policy Gradient (TD3) was chosen for the deterministic algorithm. The training and simulation results show the positive effect the dynamic entropy tuning has on controlling the quadcopter by preventing catastrophic forgetting and improving exploration efficiency.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u52a8\u6001\u71b5\u8c03\u8282\u5bf9\u968f\u673a\u7b56\u7565\u7684\u79ef\u6781\u5f71\u54cd\uff0c\u663e\u793a\u5176\u4f18\u4e8e\u786e\u5b9a\u6027\u7b56\u7565\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63d0\u9ad8\u56db\u65cb\u7ffc\u63a7\u5236\u7684\u6548\u7387\uff0c\u901a\u8fc7\u589e\u52a0\u7b56\u7565\u7684\u968f\u673a\u6027\u4ee5\u589e\u5f3a\u63a2\u7d22\u80fd\u529b\u3002", "method": "\u6bd4\u8f83\u52a8\u6001\u71b5\u8c03\u8282\u7684\u968f\u673a\u7b56\u7565\u4e0e\u9759\u6001\u71b5\u548c\u786e\u5b9a\u6027\u7b56\u7565\u7684\u8868\u73b0", "result": "\u901a\u8fc7\u8bad\u7ec3\u968f\u673a\u548c\u786e\u5b9a\u6027\u653f\u7b56\uff0c\u5728\u63a7\u5236\u56db\u65cb\u7ffc\u98de\u884c\u5668\u65b9\u9762\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6539\u8fdb\uff0c\u52a8\u6001\u71b5\u8c03\u8282\u63d0\u5347\u4e86\u63a2\u7d22\u6548\u7387\uff0c\u5e76\u9632\u6b62\u4e86\u707e\u96be\u6027\u9057\u5fd8\u3002", "conclusion": "\u52a8\u6001\u71b5\u8c03\u8282\u663e\u8457\u63d0\u9ad8\u4e86\u968f\u673a\u7b56\u7565\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u4efb\u52a1\u5982\u56db\u65cb\u7ffc\u63a7\u5236\u4e2d\u3002"}}
{"id": "2512.19570", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19570", "abs": "https://arxiv.org/abs/2512.19570", "authors": ["Angjelin Hila"], "title": "The Epistemological Consequences of Large Language Models: Rethinking collective intelligence and institutional knowledge", "comment": "AI & Soc (2025)", "summary": "We examine epistemological threats posed by human and LLM interaction. We develop collective epistemology as a theory of epistemic warrant distributed across human collectives, using bounded rationality and dual process theory as background. We distinguish internalist justification, defined as reflective understanding of why a proposition is true, from externalist justification, defined as reliable transmission of truths. Both are necessary for collective rationality, but only internalist justification produces reflective knowledge. We specify reflective knowledge as follows: agents understand the evaluative basis of a claim, when that basis is unavailable agents consistently assess the reliability of truth sources, and agents have a duty to apply these standards within their domains of competence. We argue that LLMs approximate externalist reliabilism because they can reliably transmit information whose justificatory basis is established elsewhere, but they do not themselves possess reflective justification. Widespread outsourcing of reflective work to reliable LLM outputs can weaken reflective standards of justification, disincentivize comprehension, and reduce agents' capacity to meet professional and civic epistemic duties. To mitigate these risks, we propose a three tier norm program that includes an epistemic interaction model for individual use, institutional and organizational frameworks that seed and enforce norms for epistemically optimal outcomes, and deontic constraints at organizational and or legislative levels that instantiate discursive norms and curb epistemic vices.", "AI": {"tldr": "\u6587\u7ae0\u5206\u6790\u4e86\u4eba\u7c7b\u4e0eLLM\u4e4b\u95f4\u7684\u4e92\u52a8\u5bf9\u8ba4\u77e5\u7684\u5a01\u80c1\uff0c\u9610\u8ff0\u4e86\u96c6\u4f53\u8ba4\u8bc6\u8bba\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u7ea7\u89c4\u8303\u8ba1\u5212\u4ee5\u51cf\u8f7b\u8ba4\u77e5\u98ce\u9669\u3002", "motivation": "\u7814\u7a76\u4eba\u7c7b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e92\u52a8\u5bf9\u8ba4\u77e5\u4f53\u7cfb\u7684\u5f71\u54cd\uff0c\u4ee5\u7ef4\u6301\u548c\u63d0\u5347\u96c6\u4f53\u7406\u6027\u7684\u6807\u51c6\u3002", "method": "\u901a\u8fc7\u63d0\u51fa\u5185\u5728\u548c\u5916\u5728\u7684\u6b63\u5f53\u5316\u6982\u5ff5\uff0c\u6784\u5efa\u4e86\u53cd\u601d\u77e5\u8bc6\u7684\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5e94\u7684\u89c4\u8303\u6027\u5efa\u8bae\u3002", "result": "\u672c\u6587\u63a2\u8ba8\u4e86\u4eba\u7c7b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e92\u52a8\u6240\u5e26\u6765\u7684\u8ba4\u77e5\u5a01\u80c1\uff0c\u63d0\u51fa\u4e86\u96c6\u4f53\u8ba4\u8bc6\u8bba\u4f5c\u4e3a\u4e00\u79cd\u5206\u5e03\u4e8e\u4eba\u7c7b\u96c6\u4f53\u7684\u8ba4\u8bc6\u6743\u5a01\u7406\u8bba\u3002", "conclusion": "\u4e3a\u4e86\u51cf\u8f7b\u8fd9\u4e9b\u98ce\u9669\uff0c\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u9879\u4e09\u7ea7\u89c4\u8303\u8ba1\u5212\uff0c\u5305\u62ec\u4e2a\u4eba\u4f7f\u7528\u7684\u8ba4\u8bc6\u4e92\u52a8\u6a21\u578b\u3001\u4fc3\u8fdb\u6700\u4f73\u8ba4\u8bc6\u7ed3\u679c\u7684\u673a\u6784\u548c\u7ec4\u7ec7\u6846\u67b6\uff0c\u4ee5\u53ca\u5728\u7ec4\u7ec7\u6216\u7acb\u6cd5\u5c42\u9762\u4e0a\u5b9e\u65bd\u8bdd\u8bed\u89c4\u8303\u548c\u6291\u5236\u8ba4\u77e5\u6076\u4e60\u7684\u4e49\u52a1\u7ea6\u675f\u3002"}}
{"id": "2512.18368", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.18368", "abs": "https://arxiv.org/abs/2512.18368", "authors": ["Yihang Zhu", "Weiqing Wang", "Shijie Wu", "Ye Shi", "Jingya Wang"], "title": "Learning Semantic Atomic Skills for Multi-Task Robotic Manipulation", "comment": null, "summary": "While imitation learning has shown impressive results in single-task robot manipulation, scaling it to multi-task settings remains a fundamental challenge due to issues such as suboptimal demonstrations, trajectory noise, and behavioral multi-modality. Existing skill-based methods attempt to address this by decomposing actions into reusable abstractions, but they often rely on fixed-length segmentation or environmental priors that limit semantic consistency and cross-task generalization. In this work, we propose AtomSkill, a novel multi-task imitation learning framework that learns and leverages a structured Atomic Skill Space for composable robot manipulation. Our approach is built on two key technical contributions. First, we construct a Semantically Grounded Atomic Skill Library by partitioning demonstrations into variable-length skills using gripper-state keyframe detection and vision-language model annotation. A contrastive learning objective ensures the resulting skill embeddings are both semantically consistent and temporally coherent. Second, we propose an Action Generation module with Keypose Imagination, which jointly predicts a skill's long-horizon terminal keypose and its immediate action sequence. This enables the policy to reason about overarching motion goals and fine-grained control simultaneously, facilitating robust skill chaining. Extensive experiments in simulated and real-world environments show that AtomSkill consistently outperforms state-of-the-art methods across diverse manipulation tasks.", "AI": {"tldr": "AtomSkill\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u4efb\u52a1\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u57fa\u7840\u539f\u5b50\u6280\u80fd\u5e93\u548c\u884c\u52a8\u751f\u6210\u6a21\u5757\u89e3\u51b3\u4e86\u591a\u4efb\u52a1\u64cd\u4f5c\u9762\u4e34\u7684\u6311\u6218\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u5355\u4efb\u52a1\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\uff0c\u6a21\u4eff\u5b66\u4e60\u5df2\u7ecf\u53d6\u5f97\u4e86\u4ee4\u4eba\u77a9\u76ee\u7684\u7ed3\u679c\uff0c\u4f46\u5728\u591a\u4efb\u52a1\u8bbe\u7f6e\u4e2d\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u662f\u7531\u4e8e\u6f14\u793a\u4e0d\u4f73\u3001\u8f68\u8ff9\u566a\u58f0\u548c\u884c\u4e3a\u591a\u6a21\u6001\u6027\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86AtomSkill\uff0c\u4e00\u4e2a\u65b0\u7684\u591a\u4efb\u52a1\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u5b66\u4e60\u5e76\u5229\u7528\u7ed3\u6784\u5316\u539f\u5b50\u6280\u80fd\u7a7a\u95f4\u4ee5\u5b9e\u73b0\u53ef\u7ec4\u5408\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u3002\u5173\u952e\u6280\u672f\u8d21\u732e\u5305\u62ec\uff1a\u6784\u5efa\u4e00\u4e2a\u901a\u8fc7\u68c0\u6d4b\u6293\u53d6\u5668\u72b6\u6001\u5173\u952e\u5e27\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6ce8\u91ca\u6765\u5212\u5206\u6f14\u793a\u4e3a\u53ef\u53d8\u957f\u5ea6\u6280\u80fd\u7684\u8bed\u4e49\u57fa\u7840\u539f\u5b50\u6280\u80fd\u5e93\uff1b\u548c\u63d0\u51fa\u4e00\u4e2a\u8054\u5408\u9884\u6d4b\u6280\u80fd\u957f\u89c6\u8ddd\u7ec8\u7aef\u5173\u952e\u59ff\u6001\u53ca\u5176\u7acb\u5373\u52a8\u4f5c\u5e8f\u5217\u7684\u884c\u52a8\u751f\u6210\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u53ef\u4ee5\u540c\u65f6\u8003\u8651\u6574\u4f53\u8fd0\u52a8\u76ee\u6807\u548c\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002", "result": "\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cAtomSkill\u5728\u5404\u79cd\u64cd\u4f5c\u4efb\u52a1\u4e0a\u59cb\u7ec8\u8d85\u8fc7\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "AtomSkill\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u6846\u67b6\uff0c\u589e\u5f3a\u673a\u5668\u4eba\u5728\u591a\u4efb\u52a1\u64cd\u4f5c\u4e2d\u7684\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u8de8\u4efb\u52a1\u6cdb\u5316\u65b9\u9762\u3002"}}
{"id": "2512.18396", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.18396", "abs": "https://arxiv.org/abs/2512.18396", "authors": ["Yulu Wu", "Jiujun Cheng", "Haowen Wang", "Dengyang Suo", "Pei Ren", "Qichao Mao", "Shangce Gao", "Yakun Huang"], "title": "AOMGen: Photoreal, Physics-Consistent Demonstration Generation for Articulated Object Manipulation", "comment": null, "summary": "Recent advances in Vision-Language-Action (VLA) and world-model methods have improved generalization in tasks such as robotic manipulation and object interaction. However, Successful execution of such tasks depends on large, costly collections of real demonstrations, especially for fine-grained manipulation of articulated objects. To address this, we present AOMGen, a scalable data generation framework for articulated manipulation which is instantiated from a single real scan, demonstration and a library of readily available digital assets, yielding photoreal training data with verified physical states. The framework synthesizes synchronized multi-view RGB temporally aligned with action commands and state annotations for joints and contacts, and systematically varies camera viewpoints, object styles, and object poses to expand a single execution into a diverse corpus. Experimental results demonstrate that fine-tuning VLA policies on AOMGen data increases the success rate from 0% to 88.7%, and the policies are tested on unseen objects and layouts.", "AI": {"tldr": "AOMGen\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u771f\u5b9e\u548c\u6570\u5b57\u8d44\u6e90\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u63a7\u4efb\u52a1\u7684\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u7ec6\u7c92\u5ea6\u64cd\u63a7\u4efb\u52a1\u5bf9\u5927\u91cf\u771f\u5b9e\u6f14\u793a\u6570\u636e\u7684\u9700\u6c42\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u7269\u4f53\u64cd\u4f5c\u4e0a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aAOMGen\u7684\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u4ece\u5355\u4e2a\u771f\u5b9e\u626b\u63cf\u3001\u6f14\u793a\u548c\u6570\u5b57\u8d44\u4ea7\u5e93\u751f\u6210\u5408\u6210\u6570\u636e\u3002", "result": "\u7ec6\u8c03VLA\u7b56\u7565\u540e\uff0c\u6210\u529f\u7387\u4ece0%\u63d0\u5347\u523088.7%\uff0c\u5e76\u5728\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u548c\u5e03\u5c40\u4e0a\u6d4b\u8bd5\u3002", "conclusion": "AOMGen\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u7ec6\u7c92\u5ea6\u64cd\u63a7\u7684\u6210\u529f\u7387\uff0c\u8fbe\u523088.7%\u3002"}}
{"id": "2512.18474", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.18474", "abs": "https://arxiv.org/abs/2512.18474", "authors": ["Dmytro Kuzmenko", "Nadiya Shvai"], "title": "When Robots Say No: The Empathic Ethical Disobedience Benchmark", "comment": "Accepted at the ACM/IEEE International Conference on Human-Robot Interaction (HRI 2026). This is a preprint of the author-accepted manuscript", "summary": "Robots must balance compliance with safety and social expectations as blind obedience can cause harm, while over-refusal erodes trust. Existing safe reinforcement learning (RL) benchmarks emphasize physical hazards, while human-robot interaction trust studies are small-scale and hard to reproduce. We present the Empathic Ethical Disobedience (EED) Gym, a standardized testbed that jointly evaluates refusal safety and social acceptability. Agents weigh risk, affect, and trust when choosing to comply, refuse (with or without explanation), clarify, or propose safer alternatives. EED Gym provides different scenarios, multiple persona profiles, and metrics for safety, calibration, and refusals, with trust and blame models grounded in a vignette study. Using EED Gym, we find that action masking eliminates unsafe compliance, while explanatory refusals help sustain trust. Constructive styles are rated most trustworthy, empathic styles -- most empathic, and safe RL methods improve robustness but also make agents more prone to overly cautious behavior. We release code, configurations, and reference policies to enable reproducible evaluation and systematic human-robot interaction research on refusal and trust. At submission time, we include an anonymized reproducibility package with code and configs, and we commit to open-sourcing the full repository after the paper is accepted.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u7684\u6d4b\u8bd5\u5e73\u53f0\u2014\u2014Empathic Ethical Disobedience (EED) Gym\uff0c\u7528\u4e8e\u8bc4\u4f30\u673a\u5668\u4eba\u5728\u62d2\u7edd\u4efb\u52a1\u65f6\u7684\u5b89\u5168\u6027\u4e0e\u793e\u4f1a\u53ef\u63a5\u53d7\u6027\u3002", "motivation": "\u7531\u4e8e\u76f2\u76ee\u670d\u4ece\u53ef\u80fd\u5bfc\u81f4\u4f24\u5bb3\uff0c\u800c\u8fc7\u5ea6\u62d2\u7edd\u53c8\u4f1a\u635f\u5bb3\u4fe1\u4efb\uff0c\u9700\u5728\u5b89\u5168\u4e0e\u793e\u4f1a\u671f\u671b\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u3002", "method": "EED Gym\u63d0\u4f9b\u591a\u79cd\u573a\u666f\u548c\u89d2\u8272\u914d\u7f6e\uff0c\u8bc4\u4ef7\u673a\u5668\u4eba\u5728\u62d2\u7edd\u3001\u6f84\u6e05\u6216\u63d0\u51fa\u66f4\u5b89\u5168\u66ff\u4ee3\u65b9\u6848\u65f6\u7684\u98ce\u9669\u3001\u60c5\u611f\u548c\u4fe1\u4efb\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u884c\u52a8\u5c4f\u853d\u53ef\u6d88\u9664\u4e0d\u5b89\u5168\u7684\u670d\u4ece\uff0c\u800c\u89e3\u91ca\u6027\u7684\u62d2\u7edd\u6709\u52a9\u4e8e\u7ef4\u6301\u4fe1\u4efb\u3002\u4e0d\u540c\u7684\u98ce\u683c\u5f71\u54cd\u4fe1\u4efb\u5ea6\u4e0e\u60c5\u611f\u5ea6\uff0c\u5b89\u5168RL\u65b9\u6cd5\u589e\u5f3a\u9c81\u68d2\u6027\u4f46\u53ef\u80fd\u5bfc\u81f4\u8fc7\u4e8e\u8c28\u614e\u7684\u884c\u4e3a\u3002", "conclusion": "\u6211\u4eec\u53d1\u5e03\u4e86\u4ee3\u7801\u548c\u914d\u7f6e\uff0c\u81f4\u529b\u4e8e\u63d0\u9ad8\u4eba\u673a\u4ea4\u4e92\u7814\u7a76\u7684\u53ef\u91cd\u590d\u6027\u548c\u7cfb\u7edf\u6027\uff0c\u5e76\u627f\u8bfa\u5728\u8bba\u6587\u63a5\u53d7\u540e\u5f00\u6e90\u5b8c\u6574\u4ee3\u7801\u5e93\u3002"}}
{"id": "2512.18477", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.18477", "abs": "https://arxiv.org/abs/2512.18477", "authors": ["Wenjun Lin", "Jensen Zhang", "Kaitong Cai", "Keze Wang"], "title": "STORM: Search-Guided Generative World Models for Robotic Manipulation", "comment": "Under submission", "summary": "We present STORM (Search-Guided Generative World Models), a novel framework for spatio-temporal reasoning in robotic manipulation that unifies diffusion-based action generation, conditional video prediction, and search-based planning. Unlike prior Vision-Language-Action (VLA) models that rely on abstract latent dynamics or delegate reasoning to language components, STORM grounds planning in explicit visual rollouts, enabling interpretable and foresight-driven decision-making. A diffusion-based VLA policy proposes diverse candidate actions, a generative video world model simulates their visual and reward outcomes, and Monte Carlo Tree Search (MCTS) selectively refines plans through lookahead evaluation. Experiments on the SimplerEnv manipulation benchmark demonstrate that STORM achieves a new state-of-the-art average success rate of 51.0 percent, outperforming strong baselines such as CogACT. Reward-augmented video prediction substantially improves spatio-temporal fidelity and task relevance, reducing Frechet Video Distance by over 75 percent. Moreover, STORM exhibits robust re-planning and failure recovery behavior, highlighting the advantages of search-guided generative world models for long-horizon robotic manipulation.", "AI": {"tldr": "STORM\u662f\u4e00\u79cd\u65b0\u7684\u673a\u5668\u4eba\u64cd\u63a7\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6269\u6563\u52a8\u4f5c\u751f\u6210\u3001\u89c6\u9891\u9884\u6d4b\u548c\u641c\u7d22\u89c4\u5212\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6210\u529f\u7387\u3002", "motivation": "\u63d0\u51fa\u4e00\u79cd\u65b0\u6846\u67b6\u4ee5\u5b9e\u73b0\u673a\u5668\u4eba\u64cd\u63a7\u4e2d\u7684\u65f6\u7a7a\u63a8\u7406\uff0c\u5e76\u63d0\u9ad8\u51b3\u7b56\u7684\u53ef\u89e3\u91ca\u6027\u548c\u524d\u77bb\u6027\u3002", "method": "\u901a\u8fc7\u6269\u6563\u57fa\u7840\u7684\u52a8\u4f5c\u751f\u6210\u3001\u6761\u4ef6\u89c6\u9891\u9884\u6d4b\u548c\u57fa\u4e8e\u641c\u7d22\u7684\u89c4\u5212\u76f8\u7ed3\u5408\uff0c\u5229\u7528\u660e\u786e\u7684\u89c6\u89c9\u5c55\u5f00\u8fdb\u884c\u51b3\u7b56\u3002", "result": "STORM\u5728SimplerEnv\u64cd\u63a7\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523051.0%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8eCogACT\u7b49\u5f3a\u57fa\u7ebf\uff0c\u63d0\u5347\u4e86\u65f6\u7a7a\u4fdd\u771f\u5ea6\u548c\u4efb\u52a1\u76f8\u5173\u6027\uff0c\u51cf\u5c0f\u4e86Frechet\u89c6\u9891\u8ddd\u79bb\u8d85\u8fc775%\u3002", "conclusion": "STORM\u5728\u957f\u65f6\u95f4\u8de8\u5ea6\u7684\u673a\u5668\u4eba\u64cd\u63a7\u4e2d\u5c55\u73b0\u51fa\u641c\u7d22\u5f15\u5bfc\u751f\u6210\u4e16\u754c\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u5177\u5907\u5f3a\u5927\u7684\u91cd\u89c4\u5212\u548c\u6545\u969c\u6062\u590d\u80fd\u529b\u3002"}}
{"id": "2512.18537", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.18537", "abs": "https://arxiv.org/abs/2512.18537", "authors": ["Erdao Liang"], "title": "Systematic Benchmarking of SUMO Against Data-Driven Traffic Simulators", "comment": "Source code is available at https://github.com/LuminousLamp/SUMO-Benchmark", "summary": "This paper presents a systematic benchmarking of the model-based microscopic traffic simulator SUMO against state-of-the-art data-driven traffic simulators using large-scale real-world datasets. Using the Waymo Open Motion Dataset (WOMD) and the Waymo Open Sim Agents Challenge (WOSAC), we evaluate SUMO under both short-horizon (8s) and long-horizon (60s) closed-loop simulation settings. To enable scalable evaluation, we develop Waymo2SUMO, an automated pipeline that converts WOMD scenarios into SUMO simulations. On the WOSAC benchmark, SUMO achieves a realism meta metric of 0.653 while requiring fewer than 100 tunable parameters. Extended rollouts show that SUMO maintains low collision and offroad rates and exhibits stronger long-horizon stability than representative data-driven simulators. These results highlight complementary strengths of model-based and data-driven approaches for autonomous driving simulation and benchmarking.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86SUMO\u4ea4\u901a\u6a21\u62df\u5668\u4e0e\u6570\u636e\u9a71\u52a8\u6a21\u62df\u5668\u7684\u8868\u73b0\uff0c\u663e\u793a\u51fa\u6a21\u578b\u9a71\u52a8\u548c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u81ea\u4e3b\u9a7e\u9a76\u6a21\u62df\u4e2d\u7684\u4e92\u8865\u4f18\u52bf\u3002", "motivation": "\u4e3a\u8bc4\u4f30\u6a21\u578b\u9a71\u52a8\u7684\u5fae\u89c2\u4ea4\u901a\u6a21\u62df\u5668SUMO\u4e0e\u5148\u8fdb\u7684\u6570\u636e\u9a71\u52a8\u4ea4\u901a\u6a21\u62df\u5668\u7684\u8868\u73b0\uff0c\u5229\u7528\u771f\u5b9e\u4e16\u754c\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u4f7f\u7528Waymo Open Motion Dataset\u548cWaymo Open Sim Agents Challenge\u5bf9SUMO\u8fdb\u884c\u7cfb\u7edf\u8bc4\u6d4b\uff0c\u5e76\u5f00\u53d1Waymo2SUMO\u81ea\u52a8\u5316\u7ba1\u9053\u5c06\u6570\u636e\u573a\u666f\u8f6c\u6362\u4e3aSUMO\u6a21\u62df\u3002", "result": "\u5728WOSAC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSUMO\u7684\u73b0\u5b9e\u6027\u5143\u6307\u6807\u4e3a0.653\uff0c\u540c\u65f6\u9700\u8981\u5c11\u4e8e100\u4e2a\u53ef\u8c03\u53c2\u6570\uff0c\u4e14\u5728\u957f\u65f6\u95f4\u6a21\u62df\u4e2d\u8868\u73b0\u51fa\u8f83\u5f3a\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "SUMO\u5728\u957f\u65f6\u95f4\u7a33\u5b9a\u6027\u548c\u4f4e\u78b0\u649e\u7387\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u6570\u636e\u9a71\u52a8\u6a21\u62df\u5668\uff0c\u5c55\u73b0\u4e86\u6a21\u578b\u9a71\u52a8\u4e0e\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7684\u4e92\u8865\u4f18\u52bf\u3002"}}
{"id": "2512.18662", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.18662", "abs": "https://arxiv.org/abs/2512.18662", "authors": ["Chihiro Noguchi", "Takaki Yamamoto"], "title": "Offline Reinforcement Learning for End-to-End Autonomous Driving", "comment": "15 pages", "summary": "End-to-end (E2E) autonomous driving models that take only camera images as input and directly predict a future trajectory are appealing for their computational efficiency and potential for improved generalization via unified optimization; however, persistent failure modes remain due to reliance on imitation learning (IL). While online reinforcement learning (RL) could mitigate IL-induced issues, the computational burden of neural rendering-based simulation and large E2E networks renders iterative reward and hyperparameter tuning costly. We introduce a camera-only E2E offline RL framework that performs no additional exploration and trains solely on a fixed simulator dataset. Offline RL offers strong data efficiency and rapid experimental iteration, yet is susceptible to instability from overestimation on out-of-distribution (OOD) actions. To address this, we construct pseudo ground-truth trajectories from expert driving logs and use them as a behavior regularization signal, suppressing imitation of unsafe or suboptimal behavior while stabilizing value learning. Training and closed-loop evaluation are conducted in a neural rendering environment learned from the public nuScenes dataset. Empirically, the proposed method achieves substantial improvements in collision rate and route completion compared with IL baselines. Our code will be available at [URL].", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f7f\u7528\u76f8\u673a\u7684E2E\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4f2a\u771f\u5b9e\u8f68\u8ff9\u8fdb\u884c\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u81ea\u4e3b\u9a7e\u9a76\u6027\u80fd\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u4ee5\u6a21\u4eff\u5b66\u4e60\u4e3a\u57fa\u7840\u7684E2E\u81ea\u4e3b\u9a7e\u9a76\u6a21\u578b\u4e2d\u7684\u6301\u7eed\u5931\u8d25\u6a21\u5f0f\u3002", "method": "\u6784\u5efa\u4f2a\u771f\u5b9e\u8f68\u8ff9\u5e76\u5c06\u5176\u7528\u4f5c\u884c\u4e3a\u6b63\u5219\u5316\u4fe1\u53f7\uff0c\u5728\u795e\u7ecf\u6e32\u67d3\u73af\u5883\u4e2d\u8fdb\u884c\u8bad\u7ec3\u548c\u95ed\u73af\u8bc4\u4f30\u3002", "result": "\u4e0e\u6a21\u4eff\u5b66\u4e60\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u78b0\u649e\u7387\u548c\u8def\u5f84\u5b8c\u6210\u65b9\u9762\u663e\u8457\u6539\u5584\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u78b0\u649e\u7387\u548c\u8def\u5f84\u5b8c\u6210\u65b9\u9762\u76f8\u6bd4\u4e8e\u6a21\u4eff\u5b66\u4e60\u57fa\u7ebf\u6709\u663e\u8457\u6539\u5584\u3002"}}
{"id": "2512.18703", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.18703", "abs": "https://arxiv.org/abs/2512.18703", "authors": ["Cailin Lei", "Haiyang Wu", "Yuxiong Ji", "Xiaoyu Cai", "Yuchuan Du"], "title": "CauTraj: A Causal-Knowledge-Guided Framework for Lane-Changing Trajectory Planning of Autonomous Vehicles", "comment": null, "summary": "Enhancing the performance of trajectory planners for lane - changing vehicles is one of the key challenges in autonomous driving within human - machine mixed traffic. Most existing studies have not incorporated human drivers' prior knowledge when designing trajectory planning models. To address this issue, this study proposes a novel trajectory planning framework that integrates causal prior knowledge into the control process. Both longitudinal and lateral microscopic behaviors of vehicles are modeled to quantify interaction risk, and a staged causal graph is constructed to capture causal dependencies in lane-changing scenarios. Causal effects between the lane-changing vehicle and surrounding vehicles are then estimated using causal inference, including average causal effects (ATE) and conditional average treatment effects (CATE). These causal priors are embedded into a model predictive control (MPC) framework to enhance trajectory planning. The proposed approach is validated on naturalistic vehicle trajectory datasets. Experimental results show that: (1) causal inference provides interpretable and stable quantification of vehicle interactions; (2) individual causal effects reveal driver heterogeneity; and (3) compared with the baseline MPC, the proposed method achieves a closer alignment with human driving behaviors, reducing maximum trajectory deviation from 1.2 m to 0.2 m, lateral velocity fluctuation by 60%, and yaw angle variability by 50%. These findings provide methodological support for human-like trajectory planning and practical value for improving safety, stability, and realism in autonomous vehicle testing and traffic simulation platforms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u6574\u5408\u56e0\u679c\u77e5\u8bc6\uff0c\u663e\u8457\u6539\u5584\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u8f68\u8ff9\u89c4\u5212\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u76ee\u524d\u8f68\u8ff9\u89c4\u5212\u6a21\u578b\u672a\u878d\u5165\u4eba\u7c7b\u9a7e\u9a76\u5458\u5148\u9a8c\u77e5\u8bc6\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u5c06\u56e0\u679c\u5148\u9a8c\u77e5\u8bc6\u878d\u5165\u63a7\u5236\u8fc7\u7a0b\uff0c\u7ed3\u5408\u4e86\u7eb5\u5411\u548c\u6a2a\u5411\u5fae\u89c2\u884c\u4e3a\u5efa\u6a21\u548c\u56e0\u679c\u63a8\u65ad", "result": "\u901a\u8fc7\u56e0\u679c\u63a8\u65ad\u91cf\u5316\u8f66\u8f86\u4ea4\u4e92\u98ce\u9669\uff0c\u91c7\u7528\u9884\u6d4b\u63a7\u5236\u6846\u67b6\u8fdb\u884c\u8f68\u8ff9\u89c4\u5212\uff0c\u5b9e\u73b0\u4e0e\u4eba\u7c7b\u9a7e\u9a76\u884c\u4e3a\u66f4\u9ad8\u7684\u4e00\u81f4\u6027", "conclusion": "\u4e0e\u57fa\u7ebfMPC\u76f8\u6bd4\uff0c\u51cf\u5c11\u4e86\u6700\u5927\u8f68\u8ff9\u504f\u5dee\uff0c lateral velocity fluctuation \u548c yaw angle variability\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u5b89\u5168\u6027\u3001\u7a33\u5b9a\u6027\u548c\u73b0\u5b9e\u4e3b\u4e49"}}
{"id": "2512.18712", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.18712", "abs": "https://arxiv.org/abs/2512.18712", "authors": ["Maozeng Zhang", "Ke Shi", "Huijun Li", "Tongshu Chen", "Jiejun Yan", "Aiguo Song"], "title": "DSO-VSA: a Variable Stiffness Actuator with Decoupled Stiffness and Output Characteristics for Rehabilitation Robotics", "comment": null, "summary": "Stroke-induced motor impairment often results in substantial loss of upper-limb function, creating a strong demand for rehabilitation robots that enable safe and transparent physical human-robot interaction (pHRI). Variable stiffness actuators are well suited for such applications. However, in most existing designs, stiffness is coupled with the deflection angle, complicating both modeling and control. To address this limitation, this paper presents a variable stiffness actuator featuring decoupled stiffness and output behavior for rehabilitation robotics. The system integrates a variable stiffness mechanism that combines a variable-length lever with a hypocycloidal straight-line mechanism to achieve a linear torque-deflection relationship and continuous stiffness modulation from near zero to theoretically infinite. It also incorporates a differential transmission mechanism based on a planetary gear system that enables dual-motor load sharing. A cascade PI controller is further developed on the basis of the differential configuration, in which the position-loop term jointly regulates stiffness and deflection angle, effectively suppressing stiffness fluctuations and output disturbances. The performance of prototype was experimentally validated through stiffness calibration, stiffness regulation, torque control, decoupled characteristics, and dual-motor load sharing, indicating the potential for rehabilitation exoskeletons and other pHRI systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u53d8\u521a\u5ea6\u6267\u884c\u5668\uff0c\u901a\u8fc7\u89e3\u8026\u521a\u5ea6\u548c\u8f93\u51fa\u884c\u4e3a\uff0c\u63d0\u5347\u4e86\u5eb7\u590d\u673a\u5668\u4eba\u5728\u5b89\u5168\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u5e94\u7528\u6548\u679c\u3002", "motivation": "\u7531\u4e8e\u4e2d\u98ce\u5f15\u8d77\u7684\u8fd0\u52a8\u969c\u788d\u5e38\u5bfc\u81f4\u4e0a\u80a2\u529f\u80fd\u663e\u8457\u4e27\u5931\uff0c\u8feb\u5207\u9700\u8981\u80fd\u591f\u5b9e\u73b0\u5b89\u5168\u548c\u900f\u660e\u4eba\u673a\u4ea4\u4e92\u7684\u5eb7\u590d\u673a\u5668\u4eba\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u5c06\u53ef\u53d8\u521a\u5ea6\u673a\u5236\u4e0e\u884c\u661f\u9f7f\u8f6e\u5dee\u52a8\u4f20\u52a8\u76f8\u7ed3\u5408\u7684\u53d8\u521a\u5ea6\u6267\u884c\u5668\uff0c\u5e76\u8bbe\u8ba1\u4e86\u7ea7\u8054PI\u63a7\u5236\u5668\u4ee5\u63d0\u9ad8\u7cfb\u7edf\u7a33\u5b9a\u6027\u548c\u8f93\u51fa\u6027\u80fd\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u79cd\u53d8\u521a\u5ea6\u6267\u884c\u5668\uff0c\u7279\u70b9\u662f\u521a\u5ea6\u548c\u8f93\u51fa\u884c\u4e3a\u89e3\u8026\uff0c\u9002\u5408\u7528\u4e8e\u5eb7\u590d\u673a\u5668\u4eba\u3002\u8be5\u7cfb\u7edf\u901a\u8fc7\u53ef\u53d8\u957f\u5ea6\u6760\u6746\u548c\u5047\u5706\u5f62\u76f4\u7ebf\u673a\u5236\u5b9e\u73b0\u7ebf\u6027\u626d\u77e9-\u504f\u8f6c\u5173\u7cfb\uff0c\u5e76\u5b9e\u73b0\u4ece\u8fd1\u96f6\u5230\u7406\u8bba\u65e0\u9650\u7684\u8fde\u7eed\u521a\u5ea6\u8c03\u8282\u3002\u8fd8\u96c6\u6210\u4e86\u57fa\u4e8e\u884c\u661f\u9f7f\u8f6e\u7cfb\u7edf\u7684\u5dee\u52a8\u4f20\u52a8\u673a\u5236\uff0c\u4f7f\u5f97\u53cc\u7535\u673a\u8d1f\u8f7d\u5206\u62c5\u6210\u4e3a\u53ef\u80fd\u3002\u57fa\u4e8e\u5dee\u52a8\u914d\u7f6e\u8fdb\u4e00\u6b65\u5f00\u53d1\u4e86\u7ea7\u8054PI\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u8054\u52a8\u8c03\u8282\u521a\u5ea6\u548c\u504f\u8f6c\u89d2\uff0c\u6709\u6548\u6291\u5236\u521a\u5ea6\u6ce2\u52a8\u548c\u8f93\u51fa\u5e72\u6270\u3002\u539f\u578b\u7684\u6027\u80fd\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u5b9e\uff0c\u663e\u793a\u4e86\u5728\u5eb7\u590d\u5916\u9aa8\u9abc\u548c\u5176\u4ed6pHRI\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u53d8\u521a\u5ea6\u6267\u884c\u5668\u5728\u5eb7\u590d\u673a\u5668\u4eba\u548c\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\u4e2d\u5c55\u793a\u4e86\u663e\u8457\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u89e3\u8026\u7684\u521a\u5ea6\u8c03\u8282\u548c\u9ad8\u6548\u7684\u63a7\u5236\u7b56\u7565\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2512.18836", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.18836", "abs": "https://arxiv.org/abs/2512.18836", "authors": ["Jingjia Teng", "Yang Li", "Jianqiang Wang", "Yingbai Hu", "Songyuan Tang", "Manjiang Hu"], "title": "Multimodal Classification Network Guided Trajectory Planning for Four-Wheel Independent Steering Autonomous Parking Considering Obstacle Attributes", "comment": null, "summary": "Four-wheel Independent Steering (4WIS) vehicles have attracted increasing attention for their superior maneuverability. Human drivers typically choose to cross or drive over the low-profile obstacles (e.g., plastic bags) to efficiently navigate through narrow spaces, while existing planners neglect obstacle attributes, causing inefficiency or path-finding failures. To address this, we propose a trajectory planning framework integrating the 4WIS hybrid A* and Optimal Control Problem (OCP), in which the hybrid A* provides an initial path to enhance the OCP solution. Specifically, a multimodal classification network is introduced to assess scene complexity (hard/easy task) by fusing image and vehicle state data. For hard tasks, guided points are set to decompose complex tasks into local subtasks, improving the search efficiency of 4WIS hybrid A*. The multiple steering modes of 4WIS vehicles (Ackermann, diagonal, and zero-turn) are also incorporated into node expansion and heuristic designs. Moreover, a hierarchical obstacle handling strategy is designed to guide the node expansion considering obstacle attributes, i.e., 'non-traversable', 'crossable', and 'drive-over' obstacles. It allows crossing or driving over obstacles instead of the 'avoid-only' strategy, greatly enhancing success rates of pathfinding. We also design a logical constraint for the 'drive-over' obstacle by limiting its velocity to ensure safety. Furthermore, to address dynamic obstacles with motion uncertainty, we introduce a probabilistic risk field model, constructing risk-aware driving corridors that serve as linear collision constraints in OCP. Experimental results demonstrate the proposed framework's effectiveness in generating safe, efficient, and smooth trajectories for 4WIS vehicles, especially in constrained environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf94WIS\u8f66\u8f86\u7684\u65b0\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u7efc\u5408\u969c\u788d\u7269\u5c5e\u6027\u548c\u4efb\u52a1\u590d\u6742\u6027\uff0c\u63d0\u9ad8\u8def\u5f84\u89c4\u5212\u7684\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u5ffd\u89c6\u969c\u788d\u7269\u5c5e\u6027\u5bfc\u81f4\u7684\u6548\u7387\u4f4e\u4e0b\u6216\u8def\u5f84\u5bfb\u627e\u5931\u8d25\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u54084WIS\u6df7\u5408A*\u548c\u6700\u4f18\u63a7\u5236\u95ee\u9898(OCP)\u7684\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u591a\u6a21\u6001\u5206\u7c7b\u7f51\u7edc\u8bc4\u4f30\u573a\u666f\u590d\u6742\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u8be5\u6846\u67b6\u80fd\u591f\u751f\u6210\u5b89\u5168\u3001\u9ad8\u6548\u548c\u5e73\u6ed1\u7684\u8f68\u8ff9\uff0c\u7279\u522b\u662f\u5728\u53d7\u9650\u73af\u5883\u4e2d\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e864WIS\u8f66\u8f86\u5728\u53d7\u9650\u73af\u5883\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u80fd\u529b\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2512.18850", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.18850", "abs": "https://arxiv.org/abs/2512.18850", "authors": ["Feeza Khan Khanzada", "Jaerock Kwon"], "title": "InDRiVE: Reward-Free World-Model Pretraining for Autonomous Driving via Latent Disagreement", "comment": null, "summary": "Model-based reinforcement learning (MBRL) can reduce interaction cost for autonomous driving by learning a predictive world model, but it typically still depends on task-specific rewards that are difficult to design and often brittle under distribution shift. This paper presents InDRiVE, a DreamerV3-style MBRL agent that performs reward-free pretraining in CARLA using only intrinsic motivation derived from latent ensemble disagreement. Disagreement acts as a proxy for epistemic uncertainty and drives the agent toward under-explored driving situations, while an imagination-based actor-critic learns a planner-free exploration policy directly from the learned world model. After intrinsic pretraining, we evaluate zero-shot transfer by freezing all parameters and deploying the pretrained exploration policy in unseen towns and routes. We then study few-shot adaptation by training a task policy with limited extrinsic feedback for downstream objectives (lane following and collision avoidance). Experiments in CARLA across towns, routes, and traffic densities show that disagreement-based pretraining yields stronger zero-shot robustness and robust few-shot collision avoidance under town shift and matched interaction budgets, supporting the use of intrinsic disagreement as a practical reward-free pretraining signal for reusable driving world models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faInDRiVE\uff0c\u4e00\u79cd\u5728CARLA\u4e2d\u8fdb\u884c\u5956\u52b1\u81ea\u7531\u9884\u8bad\u7ec3\u7684MBRL\u4ee3\u7406\uff0c\u5229\u7528\u5185\u5728\u52a8\u673a\u548c\u4e0d\u786e\u5b9a\u6027\u6765\u652f\u6491\u66f4\u5f3a\u7684\u96f6-shot \u9002\u5e94\u6027\u53ca\u78b0\u649e\u907f\u514d\u80fd\u529b\u3002", "motivation": "\u867d\u7136\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u901a\u8fc7\u5b66\u4e60\u9884\u6d4b\u4e16\u754c\u6a21\u578b\u51cf\u5c11\u81ea\u4e3b\u9a7e\u9a76\u7684\u4ea4\u4e92\u6210\u672c\uff0c\u4f46\u5728\u4efb\u52a1\u7279\u5b9a\u5956\u52b1\u8bbe\u8ba1\u4e0a\u4ecd\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u5206\u5e03\u53d8\u5316\u4e0b\u663e\u5f97\u8106\u5f31\uff0c\u56e0\u6b64\u5bf9\u5956\u52b1\u81ea\u7531\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u5177\u6709\u7814\u7a76\u4ef7\u503c\u3002", "method": "InDRiVE\u901a\u8fc7\u6f5c\u5728\u96c6\u5408\u4f53\u7684\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u548c\u65e0\u89c4\u5212\u7684\u63a2\u7d22\u7b56\u7565\u5b66\u4e60\uff0c\u8fdb\u884c\u5956\u52b1\u81ea\u7531\u7684\u9884\u8bad\u7ec3\uff0c\u5e76\u5728\u65b0\u73af\u5883\u4e2d\u8bc4\u4f30\u96f6-shot \u8f6c\u79fb\uff0c\u6700\u540e\u901a\u8fc7\u6709\u9650\u7684\u5916\u5728\u53cd\u9988\u8bad\u7ec3\u4efb\u52a1\u7b56\u7565\u5b9e\u73b0\u5c11\u91cf\u9002\u5e94\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5InDRiVE\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u5185\u5728\u52a8\u673a\u9a71\u52a8\u4ee3\u7406\u63a2\u7d22\u672a\u5145\u5206\u63a2\u7d22\u7684\u9a7e\u9a76\u60c5\u51b5\uff0c\u5e76\u5728CARLA\u73af\u5883\u4e2d\u8fdb\u884c\u5956\u52b1\u81ea\u7531\u7684\u9884\u8bad\u7ec3\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u7684\u9884\u8bad\u7ec3\u80fd\u591f\u5728\u65b0\u7684\u57ce\u9547\u548c\u8def\u7ebf\u4e2d\u5b9e\u73b0\u66f4\u5f3a\u7684\u96f6-shot \u9002\u5e94\u6027\uff0c\u540c\u65f6\u5728\u78b0\u649e\u907f\u514d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u5065\u58ee\u6027\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u9884\u8bad\u7ec3\u7684\u667a\u80fd\u4ee3\u7406\u5728\u9762\u5bf9\u57ce\u9547\u53d8\u5316\u548c\u6709\u9650\u53cd\u9988\u6761\u4ef6\u4e0b\uff0c\u80fd\u591f\u6709\u6548\u5730\u9002\u5e94\u4e0d\u540c\u7684\u9a7e\u9a76\u4efb\u52a1\uff0c\u5c55\u793a\u51fa\u5185\u5728\u4e0d\u786e\u5b9a\u6027\u4f5c\u4e3a\u5df2\u6709\u6a21\u578b\u7684\u5956\u52b1\u81ea\u7531\u9884\u57f9\u8bad\u4fe1\u53f7\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2512.18869", "categories": ["cs.RO", "cs.CG"], "pdf": "https://arxiv.org/pdf/2512.18869", "abs": "https://arxiv.org/abs/2512.18869", "authors": ["Georg Nawratil"], "title": "Construction and deformation of P-hedra using control polylines", "comment": "8 pages, 4 figures", "summary": "In the 19th International Symposium on Advances in Robot Kinematics the author introduced a novel class of continuous flexible discrete surfaces and mentioned that these so-called P-hedra (or P-nets) allow direct access to their spatial shapes by three control polylines. In this follow-up paper we study this intuitive method, which makes these flexible planar quad surfaces suitable for transformable design tasks by means of interactive tools. The construction of P-hedra from the control polylines can also be used for an efficient algorithmic computation of their isometric deformations. In addition we discuss flexion limits, bifurcation configurations, developable/flat-foldable pattern and tubular P-hedra.", "AI": {"tldr": "\u672c\u8bba\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u65b0\u578b\u8fde\u7eed\u7075\u6d3b\u79bb\u6563\u8868\u9762 P-hedra\uff0c\u63a2\u8ba8\u4e86\u5176\u5728\u53d8\u6362\u8bbe\u8ba1\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u548c\u8ba1\u7b97\u7b49\u8ddd\u53d8\u5f62\u7684\u9ad8\u6548\u7b97\u6cd5\u3002", "motivation": "\u7814\u7a76\u8005\u65e8\u5728\u5f00\u53d1\u6709\u52a9\u4e8e\u5f62\u72b6\u53d8\u6362\u8bbe\u8ba1\u7684\u7075\u6d3b\u5e73\u9762\u56db\u8fb9\u5f62\u8868\u9762\uff0c\u4ee5\u4fbf\u4e8e\u4ea4\u4e92\u5f0f\u5de5\u5177\u4f7f\u7528\u3002", "method": "\u901a\u8fc7\u7814\u7a76\u63a7\u5236\u6298\u7ebf\uff0c\u5206\u6790 P-hedra \u7684\u6784\u9020\u4e0e\u6027\u80fd\u3002", "result": "P-hedra \u7684\u6784\u9020\u53ca\u5176\u4e0e\u63a7\u5236\u6298\u7ebf\u7684\u5173\u7cfb\uff0c\u4e3a\u5176\u5728\u8bbe\u8ba1\u548c\u8ba1\u7b97\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002", "conclusion": "\u672c\u6587\u63a2\u8ba8\u4e86 P-hedra \u7684\u7279\u6027\u53ca\u5e94\u7528\uff0c\u652f\u6301\u53ef\u53d8\u8bbe\u8ba1\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u9ad8\u6548\u7b97\u6cd5\u8ba1\u7b97\u5176\u7b49\u8ddd\u53d8\u5f62\u3002"}}
{"id": "2512.18922", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.18922", "abs": "https://arxiv.org/abs/2512.18922", "authors": ["Tianyuan Liu", "Richard Dazeley", "Benjamin Champion", "Akan Cosgun"], "title": "Optimizing Robotic Placement via Grasp-Dependent Feasibility Prediction", "comment": "Accepted in ACRA 2025", "summary": "In this paper, we study whether inexpensive, physics-free supervision can reliably prioritize grasp-place candidates for budget-aware pick-and-place. From an object's initial pose, target pose, and a candidate grasp, we generate two path-aware geometric labels: path-wise inverse kinematics (IK) feasibility across a fixed approach-grasp-lift waypoint template, and a transit collision flag from mesh sweeps along the same template. A compact dual-output MLP learns these signals from pose encodings, and at test time its scores rank precomputed candidates for a rank-then-plan policy under the same IK gate and planner as the baseline. Although learned from cheap labels only, the scores transfer to physics-enabled executed trajectories: at a fixed planning budget the policy finds successful paths sooner with fewer planner calls while keeping final success on par or better. This work targets a single rigid cuboid with side-face grasps and a fixed waypoint template, and we outline extensions to varied objects and richer waypoint schemes.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5ec9\u4ef7\u76d1\u7763\u5982\u4f55\u6709\u6548\u4f18\u5148\u8003\u8651\u6293\u53d6-\u653e\u7f6e\u5019\u9009\uff0c\u63d0\u51fa\u4e86\u4f7f\u7528\u53cc\u8f93\u51faMLP\u7684\u65b9\u6848\uff0c\u5728\u9884\u7b97\u6709\u9650\u6761\u4ef6\u4e0b\u63d0\u5347\u8def\u5f84\u89c4\u5212\u6548\u7387\u3002", "motivation": "\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u5ec9\u4ef7\u7684\u7269\u7406\u65e0\u5173\u76d1\u7763\u6765\u53ef\u9760\u5730\u4f18\u5148\u8003\u8651\u9884\u7b97\u610f\u8bc6\u7684\u6293\u53d6\u548c\u653e\u7f6e\u5019\u9009\u9879\u3002", "method": "\u4f7f\u7528\u7d27\u51d1\u7684\u53cc\u8f93\u51fa\u591a\u5c42\u611f\u77e5\u5668\uff08MLP\uff09\u4ece\u59ff\u6001\u7f16\u7801\u4e2d\u5b66\u4e60\u8def\u5f84\u611f\u77e5\u7684\u51e0\u4f55\u6807\u7b7e\uff0c\u5305\u62ec\u8def\u5f84\u9006\u5411\u8fd0\u52a8\u5b66\uff08IK\uff09\u53ef\u884c\u6027\u548c\u8fc7\u5883\u78b0\u649e\u6807\u5fd7\u3002", "result": "\u5728\u56fa\u5b9a\u7684\u89c4\u5212\u9884\u7b97\u4e0b\uff0c\u6b64\u653f\u7b56\u80fd\u66f4\u65e9\u627e\u5230\u6210\u529f\u8def\u5f84\uff0c\u51cf\u5c11\u89c4\u5212\u8c03\u7528\uff0c\u540c\u65f6\u4fdd\u8bc1\u6216\u63d0\u5347\u6700\u7ec8\u6210\u529f\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u9884\u7b97\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u53ef\u4ee5\u66f4\u5feb\u5730\u627e\u5230\u6210\u529f\u7684\u8def\u5f84\uff0c\u540c\u65f6\u4fdd\u6301\u6700\u7ec8\u6210\u529f\u7387\u4e0d\u53d8\u6216\u66f4\u597d\u3002"}}
{"id": "2512.18938", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.18938", "abs": "https://arxiv.org/abs/2512.18938", "authors": ["Yadong Liu", "Jianwei Liu", "He Liang", "Dimitrios Kanoulas"], "title": "A Framework for Deploying Learning-based Quadruped Loco-Manipulation", "comment": null, "summary": "Quadruped mobile manipulators offer strong potential for agile loco-manipulation but remain difficult to control and transfer reliably from simulation to reality. Reinforcement learning (RL) shows promise for whole-body control, yet most frameworks are proprietary and hard to reproduce on real hardware. We present an open pipeline for training, benchmarking, and deploying RL-based controllers on the Unitree B1 quadruped with a Z1 arm. The framework unifies sim-to-sim and sim-to-real transfer through ROS, re-implementing a policy trained in Isaac Gym, extending it to MuJoCo via a hardware abstraction layer, and deploying the same controller on physical hardware. Sim-to-sim experiments expose discrepancies between Isaac Gym and MuJoCo contact models that influence policy behavior, while real-world teleoperated object-picking trials show that coordinated whole-body control extends reach and improves manipulation over floating-base baselines. The pipeline provides a transparent, reproducible foundation for developing and analyzing RL-based loco-manipulation controllers and will be released open source to support future research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5f00\u653e\u7ba1\u9053\uff0c\u7528\u4e8e\u5728Unitree B1\u56db\u8db3\u673a\u5668\u4eba\u53caZ1\u81c2\u4e0a\u8bad\u7ec3\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u90e8\u7f72\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63a7\u5236\u5668\uff0c\u89e3\u51b3\u4e86\u6a21\u62df\u4e0e\u73b0\u5b9e\u95f4\u7684\u8f6c\u79fb\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u91cd\u590d\u7684\u57fa\u7840\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u3002", "motivation": "\u56db\u8db3\u79fb\u52a8\u64cd\u7eb5\u5668\u5728\u7075\u6d3b\u7684\u8fd0\u52a8\u63a7\u5236\u4e0a\u6709\u5f88\u5f3a\u7684\u6f5c\u529b\uff0c\u4f46\u5728\u4ece\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u53ef\u9760\u8f6c\u79fb\u65b9\u9762\u4ecd\u7136\u56f0\u96be\uff1b\u5f3a\u5316\u5b66\u4e60\u5728\u5168\u8eab\u63a7\u5236\u4e0a\u8868\u73b0\u51fa\u524d\u666f\uff0c\u4f46\u5927\u591a\u6570\u6846\u67b6\u662f\u4e13\u6709\u7684\uff0c\u4e14\u96be\u4ee5\u5728\u771f\u5b9e\u786c\u4ef6\u4e0a\u91cd\u73b0\u3002", "method": "\u901a\u8fc7ROS\u7edf\u4e00\u4e86\u6a21\u62df\u5230\u6a21\u62df\u548c\u6a21\u62df\u5230\u5b9e\u9645\u7684\u8f6c\u79fb\uff0c\u91cd\u65b0\u5b9e\u73b0\u4e86\u5728Isaac Gym\u4e2d\u8bad\u7ec3\u7684\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u786c\u4ef6\u62bd\u8c61\u5c42\u5c06\u5176\u6269\u5c55\u5230MuJoCo\uff0c\u6700\u540e\u5728\u7269\u7406\u786c\u4ef6\u4e0a\u90e8\u7f72\u76f8\u540c\u7684\u63a7\u5236\u5668\u3002", "result": "\u6a21\u62df\u5230\u6a21\u62df\u7684\u5b9e\u9a8c\u63ed\u793a\u4e86Isaac Gym\u548cMuJoCo\u63a5\u89e6\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u8fd9\u4e9b\u5dee\u5f02\u4f1a\u5f71\u54cd\u7b56\u7565\u884c\u4e3a\uff0c\u800c\u5b9e\u9645\u7684\u8fdc\u7a0b\u64cd\u4f5c\u7269\u4f53\u62fe\u53d6\u6d4b\u8bd5\u8868\u660e\uff0c\u534f\u8c03\u7684\u5168\u8eab\u63a7\u5236\u6bd4\u6f02\u6d6e\u57fa\u51c6\u8868\u73b0\u66f4\u597d\uff0c\u6269\u5c55\u4e86\u64cd\u4f5c\u8303\u56f4\u5e76\u6539\u5584\u4e86\u64cd\u7eb5\u80fd\u529b\u3002", "conclusion": "\u8be5\u7ba1\u9053\u4e3a\u5f00\u53d1\u548c\u5206\u6790\u57fa\u4e8eRL\u7684\u8fd0\u52a8\u64cd\u7eb5\u63a7\u5236\u5668\u63d0\u4f9b\u4e86\u900f\u660e\u548c\u53ef\u91cd\u590d\u7684\u57fa\u7840\uff0c\u5e76\u5c06\u5f00\u6e90\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2512.18987", "categories": ["cs.RO", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.18987", "abs": "https://arxiv.org/abs/2512.18987", "authors": ["Ryosuke Korekata", "Quanting Xie", "Yonatan Bisk", "Komei Sugiura"], "title": "Affordance RAG: Hierarchical Multimodal Retrieval with Affordance-Aware Embodied Memory for Mobile Manipulation", "comment": "Accepted to IEEE RA-L, with presentation at ICRA 2026", "summary": "In this study, we address the problem of open-vocabulary mobile manipulation, where a robot is required to carry a wide range of objects to receptacles based on free-form natural language instructions. This task is challenging, as it involves understanding visual semantics and the affordance of manipulation actions. To tackle these challenges, we propose Affordance RAG, a zero-shot hierarchical multimodal retrieval framework that constructs Affordance-Aware Embodied Memory from pre-explored images. The model retrieves candidate targets based on regional and visual semantics and reranks them with affordance scores, allowing the robot to identify manipulation options that are likely to be executable in real-world environments. Our method outperformed existing approaches in retrieval performance for mobile manipulation instruction in large-scale indoor environments. Furthermore, in real-world experiments where the robot performed mobile manipulation in indoor environments based on free-form instructions, the proposed method achieved a task success rate of 85%, outperforming existing methods in both retrieval performance and overall task success.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7Affordance RAG\u6846\u67b6\u6539\u8fdb\u4e86\u5f00\u653e\u8bcd\u6c47\u79fb\u52a8\u64cd\u63a7\u7684\u6548\u679c\uff0c\u5b9e\u73b085%\u7684\u4efb\u52a1\u6210\u529f\u7387\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5f00\u653e\u8bcd\u6c47\u79fb\u52a8\u64cd\u63a7\u95ee\u9898\uff0c\u8981\u6c42\u673a\u5668\u4eba\u6839\u636e\u81ea\u7531\u5f62\u5f0f\u7684\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5c06\u5404\u79cd\u7269\u4f53\u8fd0\u8f93\u5230\u63a5\u6536\u5904\uff0c\u8fd9\u4e00\u4efb\u52a1\u6d89\u53ca\u7406\u89e3\u89c6\u89c9\u8bed\u4e49\u548c\u64cd\u63a7\u884c\u4e3a\u7684\u53ef\u64cd\u4f5c\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAffordance RAG\u7684\u96f6-shot\u5c42\u7ea7\u591a\u6a21\u6001\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u63a2\u7d22\u7684\u56fe\u50cf\u6784\u5efa\u4e86\u9002\u5e94\u6027\u8bb0\u5fc6\uff0c\u57fa\u4e8e\u533a\u57df\u548c\u89c6\u89c9\u8bed\u4e49\u68c0\u7d22\u5019\u9009\u76ee\u6807\uff0c\u5e76\u7528\u53ef\u64cd\u4f5c\u6027\u8bc4\u5206\u5bf9\u5176\u8fdb\u884c\u91cd\u65b0\u6392\u5e8f\u3002", "result": "\u5728\u5927\u578b\u5ba4\u5185\u73af\u5883\u4e2d\u7684\u79fb\u52a8\u64cd\u63a7\u6307\u4ee4\u68c0\u7d22\u6027\u80fd\u4e0a\uff0c\u672c\u65b9\u6cd5\u8d85\u8fc7\u4e86\u73b0\u6709\u6280\u672f\uff0c\u771f\u6b63\u5b9e\u9a8c\u4e2d\u4efb\u52a1\u6210\u529f\u7387\u8fbe\u523085%\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684Affordance RAG\u6846\u67b6\u5728\u6267\u884c\u79fb\u52a8\u64cd\u63a7\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u8fbe\u5230\u4e8685%\u7684\u4efb\u52a1\u6210\u529f\u7387\u3002"}}
{"id": "2512.18988", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.18988", "abs": "https://arxiv.org/abs/2512.18988", "authors": ["Yanding Yang", "Weitao Zhou", "Jinhai Wang", "Xiaomin Guo", "Junze Wen", "Xiaolong Liu", "Lang Ding", "Zheng Fu", "Jinyu Miao", "Kun Jiang", "Diange Yang"], "title": "DTCCL: Disengagement-Triggered Contrastive Continual Learning for Autonomous Bus Planners", "comment": null, "summary": "Autonomous buses run on fixed routes but must operate in open, dynamic urban environments. Disengagement events on these routes are often geographically concentrated and typically arise from planner failures in highly interactive regions. Such policy-level failures are difficult to correct using conventional imitation learning, which easily overfits to sparse disengagement data. To address this issue, this paper presents a Disengagement-Triggered Contrastive Continual Learning (DTCCL) framework that enables autonomous buses to improve planning policies through real-world operation. Each disengagement triggers cloud-based data augmentation that generates positive and negative samples by perturbing surrounding agents while preserving route context. Contrastive learning refines policy representations to better distinguish safe and unsafe behaviors, and continual updates are applied in a cloud-edge loop without human supervision. Experiments on urban bus routes demonstrate that DTCCL improves overall planning performance by 48.6 percent compared with direct retraining, validating its effectiveness for scalable, closed-loop policy improvement in autonomous public transport.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6DTCCL\uff0c\u7528\u4e8e\u6539\u5584\u81ea\u4e3b\u516c\u4ea4\u5728\u52a8\u6001\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u89c4\u5212\u7b56\u7565\u3002", "motivation": "\u81ea\u4e3b\u516c\u4ea4\u5728\u56fa\u5b9a\u8def\u7ebf\u8fd0\u884c\uff0c\u4f46\u5728\u52a8\u6001\u57ce\u5e02\u73af\u5883\u4e2d\u9762\u4e34\u8ba1\u5212\u5931\u8d25\u95ee\u9898\uff0c\u4f20\u7edf\u7684\u6a21\u4eff\u5b66\u4e60\u96be\u4ee5\u5904\u7406\u7a00\u758f\u7684 disengagement \u6570\u636e\u3002", "method": "\u63d0\u51faDTCCL\u6846\u67b6\uff0c\u901a\u8fc7\u73b0\u5b9e\u64cd\u4f5c\u4e2d\u7684disengagement\u89e6\u53d1\u4e91\u7aef\u6570\u636e\u589e\u5f3a\uff0c\u751f\u6210\u6b63\u8d1f\u6837\u672c\uff0c\u5e76\u5e94\u7528\u5bf9\u6bd4\u5b66\u4e60\u4f18\u5316\u7b56\u7565\u8868\u793a\uff0c\u6700\u7ec8\u5728\u65e0\u76d1\u7763\u60c5\u51b5\u4e0b\u8fdb\u884c\u6301\u7eed\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cDTCCL\u76f8\u6bd4\u76f4\u63a5\u518d\u8bad\u7ec3\uff0c\u6574\u4f53\u89c4\u5212\u6027\u80fd\u63d0\u9ad8\u4e8648.6%\u3002", "conclusion": "DTCCL\u6846\u67b6\u8bc1\u660e\u4e86\u5728\u81ea\u4e3b\u516c\u5171\u4ea4\u901a\u4e2d\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u95ed\u73af\u7b56\u7565\u6539\u8fdb\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2512.19024", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19024", "abs": "https://arxiv.org/abs/2512.19024", "authors": ["Xu Liu", "Yu Liu", "Hanshuo Qiu", "Yang Qirong", "Zhouhui Lian"], "title": "IndoorUAV: Benchmarking Vision-Language UAV Navigation in Continuous Indoor Environments", "comment": null, "summary": "Vision-Language Navigation (VLN) enables agents to navigate in complex environments by following natural language instructions grounded in visual observations. Although most existing work has focused on ground-based robots or outdoor Unmanned Aerial Vehicles (UAVs), indoor UAV-based VLN remains underexplored, despite its relevance to real-world applications such as inspection, delivery, and search-and-rescue in confined spaces. To bridge this gap, we introduce \\textbf{IndoorUAV}, a novel benchmark and method specifically tailored for VLN with indoor UAVs. We begin by curating over 1,000 diverse and structurally rich 3D indoor scenes from the Habitat simulator. Within these environments, we simulate realistic UAV flight dynamics to collect diverse 3D navigation trajectories manually, further enriched through data augmentation techniques. Furthermore, we design an automated annotation pipeline to generate natural language instructions of varying granularity for each trajectory. This process yields over 16,000 high-quality trajectories, comprising the \\textbf{IndoorUAV-VLN} subset, which focuses on long-horizon VLN. To support short-horizon planning, we segment long trajectories into sub-trajectories by selecting semantically salient keyframes and regenerating concise instructions, forming the \\textbf{IndoorUAV-VLA} subset. Finally, we introduce \\textbf{IndoorUAV-Agent}, a novel navigation model designed for our benchmark, leveraging task decomposition and multimodal reasoning. We hope IndoorUAV serves as a valuable resource to advance research on vision-language embodied AI in the indoor aerial navigation domain.", "AI": {"tldr": "IndoorUAV\u662f\u4e00\u4e2a\u65b0\u57fa\u51c6\uff0c\u65e8\u5728\u63d0\u9ad8\u5ba4\u5185\u65e0\u4eba\u673a\u7684\u89c6\u8bed\u8a00\u5bfc\u822a\u80fd\u529b\uff0c\u5305\u542b\u5927\u91cf\u591a\u6837\u5316\u76843D\u573a\u666f\u548c\u9ad8\u8d28\u91cf\u5bfc\u822a\u6570\u636e\u3002", "motivation": "\u7531\u4e8e\u5ba4\u5185\u65e0\u4eba\u673a\u5bfc\u822a\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u73b0\u6709\u7684\u57fa\u7840\u7814\u7a76\u4ecd\u663e\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u9488\u5bf9\u5ba4\u5185\u73af\u5883\u7684\u4e13\u95e8\u57fa\u51c6\u548c\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6536\u96c6\u591a\u6837\u5316\u76843D\u5ba4\u5185\u573a\u666f\u548c\u624b\u52a8\u5bfc\u822a\u8f68\u8ff9\uff0c\u7ed3\u5408\u81ea\u52a8\u6ce8\u91ca\u7ba1\u9053\u751f\u6210\u591a\u6837\u7684\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u3002", "result": "\u521b\u5efa\u4e86IndoorUAV\u57fa\u51c6\u53caIndoorUAV-VLN\u548cIndoorUAV-VLA\u5b50\u96c6\uff0c\u5176\u4e2d\u542b\u6709\u8d85\u8fc716,000\u6761\u9ad8\u8d28\u91cf\u5bfc\u822a\u8f68\u8ff9\u53ca\u5176\u76f8\u5e94\u7684\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u3002", "conclusion": "IndoorUAV\u53ca\u5176\u76f8\u5173\u5b50\u96c6\u5c06\u4e3a\u5ba4\u5185\u65e0\u4eba\u673a\u5bfc\u822a\u7684\u89c6\u8bed\u8a00\u878d\u5408\u7814\u7a76\u63d0\u4f9b\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2512.19043", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.19043", "abs": "https://arxiv.org/abs/2512.19043", "authors": ["Chao Yang", "Yingkai Sun", "Peng Ye", "Xin Chen", "Chong Yu", "Tao Chen"], "title": "EGM: Efficiently Learning General Motion Tracking Policy for High Dynamic Humanoid Whole-Body Control", "comment": null, "summary": "Learning a general motion tracking policy from human motions shows great potential for versatile humanoid whole-body control. Conventional approaches are not only inefficient in data utilization and training processes but also exhibit limited performance when tracking highly dynamic motions. To address these challenges, we propose EGM, a framework that enables efficient learning of a general motion tracking policy. EGM integrates four core designs. Firstly, we introduce a Bin-based Cross-motion Curriculum Adaptive Sampling strategy to dynamically orchestrate the sampling probabilities based on tracking error of each motion bin, eficiently balancing the training process across motions with varying dificulty and durations. The sampled data is then processed by our proposed Composite Decoupled Mixture-of-Experts (CDMoE) architecture, which efficiently enhances the ability to track motions from different distributions by grouping experts separately for upper and lower body and decoupling orthogonal experts from shared experts to separately handle dedicated features and general features. Central to our approach is a key insight we identified: for training a general motion tracking policy, data quality and diversity are paramount. Building on these designs, we develop a three-stage curriculum training flow to progressively enhance the policy's robustness against disturbances. Despite training on only 4.08 hours of data, EGM generalized robustly across 49.25 hours of test motions, outperforming baselines on both routine and highly dynamic tasks.", "AI": {"tldr": "EGM\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u81ea\u9002\u5e94\u91c7\u6837\u548c\u4e13\u5bb6\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u52a8\u4f5c\u8ffd\u8e2a\u7b56\u7565\u5b66\u4e60\uff0c\u5e76\u5728\u6781\u5177\u52a8\u6001\u6027\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u6570\u636e\u5229\u7528\u6548\u7387\u548c\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u5728\u9ad8\u52a8\u6001\u52a8\u4f5c\u8ddf\u8e2a\u65f6\u6027\u80fd\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u66f4\u6709\u6548\u7684\u5b66\u4e60\u6846\u67b6\u3002", "method": "EGM\u6846\u67b6\u901a\u8fc7\u56db\u4e2a\u6838\u5fc3\u8bbe\u8ba1\uff0c\u5305\u62ec\u57fa\u4e8e\u7bb1\u7684\u8de8\u52a8\u91cf\u8bfe\u7a0b\u81ea\u9002\u5e94\u91c7\u6837\u7b56\u7565\u548c\u590d\u5408\u89e3\u8026\u4e13\u5bb6\u7ec4\u5408\u67b6\u6784\uff0c\u6765\u5b9e\u73b0\u9ad8\u6548\u7684\u76ee\u6807\u52a8\u4f5c\u8ddf\u8e2a\u7b56\u7565\u5b66\u4e60\u3002", "result": "EGM\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u91c7\u6837\u6982\u7387\u548c\u4f18\u5316\u4e13\u5bb6\u7ec4\uff0c\u63d0\u9ad8\u4e86\u5bf9\u4e0d\u540c\u52a8\u4f5c\u5206\u5e03\u7684\u8ddf\u8e2a\u80fd\u529b\uff0c\u5e76\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u5176\u589e\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "EGM\u5728\u4ec5\u4f7f\u75284.08\u5c0f\u65f6\u7684\u6570\u636e\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf949.25\u5c0f\u65f6\u7684\u6d4b\u8bd5\u52a8\u4f5c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u666e\u9002\u6027\uff0c\u8d85\u8d8a\u4e86\u57fa\u51c6\u6a21\u578b\u7684\u8868\u73b0\uff0c\u5c24\u5176\u5728\u5e38\u89c4\u548c\u6781\u5177\u52a8\u6001\u6027\u7684\u4efb\u52a1\u4e0a\u3002"}}
{"id": "2512.19083", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.19083", "abs": "https://arxiv.org/abs/2512.19083", "authors": ["Pengyu Chen", "Tao Ouyang", "Ke Luo", "Weijie Hong", "Xu Chen"], "title": "CoDrone: Autonomous Drone Navigation Assisted by Edge and Cloud Foundation Models", "comment": "This paper is accepted by the IEEE Internet of Things Journal (IoT-J) for publication in the Special Issue on \"Augmented Edge Sensing Intelligence for Low-Altitude IoT Systems\"", "summary": "Autonomous navigation for Unmanned Aerial Vehicles faces key challenges from limited onboard computational resources, which restrict deployed deep neural networks to shallow architectures incapable of handling complex environments. Offloading tasks to remote edge servers introduces high latency, creating an inherent trade-off in system design. To address these limitations, we propose CoDrone - the first cloud-edge-end collaborative computing framework integrating foundation models into autonomous UAV cruising scenarios - effectively leveraging foundation models to enhance performance of resource-constrained unmanned aerial vehicle platforms. To reduce onboard computation and data transmission overhead, CoDrone employs grayscale imagery for the navigation model. When enhanced environmental perception is required, CoDrone leverages the edge-assisted foundation model Depth Anything V2 for depth estimation and introduces a novel one-dimensional occupancy grid-based navigation method - enabling fine-grained scene understanding while advancing efficiency and representational simplicity of autonomous navigation. A key component of CoDrone is a Deep Reinforcement Learning-based neural scheduler that seamlessly integrates depth estimation with autonomous navigation decisions, enabling real-time adaptation to dynamic environments. Furthermore, the framework introduces a UAV-specific vision language interaction module incorporating domain-tailored low-level flight primitives to enable effective interaction between the cloud foundation model and the UAV. The introduction of VLM enhances open-set reasoning capabilities in complex unseen scenarios. Experimental results show CoDrone outperforms baseline methods under varying flight speeds and network conditions, achieving a 40% increase in average flight distance and a 5% improvement in average Quality of Navigation.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86CoDrone\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u57fa\u7840\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u590d\u6742\u73af\u5883\u4e2d\u8d44\u6e90\u53d7\u9650\u65e0\u4eba\u673a\u7684\u9ad8\u6548\u5bfc\u822a\u3002", "motivation": "\u81ea\u4e3b\u65e0\u4eba\u673a\u5bfc\u822a\u9762\u4e34\u6709\u9650\u7684\u8ba1\u7b97\u8d44\u6e90\u6311\u6218\uff0c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u53ea\u80fd\u91c7\u7528\u8f83\u6d45\u7684\u67b6\u6784\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u73af\u5883\u3002", "method": "\u63d0\u51fa\u4e86CoDrone\u6846\u67b6\uff0c\u96c6\u6210\u57fa\u7840\u6a21\u578b\u548c\u4e00\u79cd\u65b0\u578b\u7684\u4e00\u7ef4\u5360\u7528\u7f51\u683c\u5bfc\u822a\u65b9\u6cd5\uff0c\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u795e\u7ecf\u8c03\u5ea6\u5668\u3002", "result": "CoDrone\u5728\u4e0d\u540c\u98de\u884c\u901f\u5ea6\u548c\u7f51\u7edc\u6761\u4ef6\u4e0b\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u534740%\u5e73\u5747\u98de\u884c\u8ddd\u79bb\uff0c\u5e73\u5747\u5bfc\u822a\u8d28\u91cf\u63d0\u9ad85%\u3002", "conclusion": "CoDrone\u6709\u6548\u5730\u589e\u5f3a\u4e86\u8d44\u6e90\u53d7\u9650\u65e0\u4eba\u673a\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4e91\u8fb9\u7aef\u534f\u4f5c\u8ba1\u7b97\u3002"}}
{"id": "2512.19133", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.19133", "abs": "https://arxiv.org/abs/2512.19133", "authors": ["Pengxuan Yang", "Ben Lu", "Zhongpu Xia", "Chao Han", "Yinfeng Gao", "Teng Zhang", "Kun Zhan", "XianPeng Lang", "Yupeng Zheng", "Qichao Zhang"], "title": "WorldRFT: Latent World Model Planning with Reinforcement Fine-Tuning for Autonomous Driving", "comment": "AAAI 2026, first version", "summary": "Latent World Models enhance scene representation through temporal self-supervised learning, presenting a perception annotation-free paradigm for end-to-end autonomous driving. However, the reconstruction-oriented representation learning tangles perception with planning tasks, leading to suboptimal optimization for planning. To address this challenge, we propose WorldRFT, a planning-oriented latent world model framework that aligns scene representation learning with planning via a hierarchical planning decomposition and local-aware interactive refinement mechanism, augmented by reinforcement learning fine-tuning (RFT) to enhance safety-critical policy performance. Specifically, WorldRFT integrates a vision-geometry foundation model to improve 3D spatial awareness, employs hierarchical planning task decomposition to guide representation optimization, and utilizes local-aware iterative refinement to derive a planning-oriented driving policy. Furthermore, we introduce Group Relative Policy Optimization (GRPO), which applies trajectory Gaussianization and collision-aware rewards to fine-tune the driving policy, yielding systematic improvements in safety. WorldRFT achieves state-of-the-art (SOTA) performance on both open-loop nuScenes and closed-loop NavSim benchmarks. On nuScenes, it reduces collision rates by 83% (0.30% -> 0.05%). On NavSim, using camera-only sensors input, it attains competitive performance with the LiDAR-based SOTA method DiffusionDrive (87.8 vs. 88.1 PDMS).", "AI": {"tldr": "WorldRFT\u662f\u4e00\u79cd\u89c4\u5212\u5bfc\u5411\u7684\u6f5c\u5728\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u89c4\u5212\u548c\u5c40\u90e8\u611f\u77e5\u7cbe\u70bc\u673a\u5236\u4f18\u5316\u573a\u666f\u8868\u5f81\uff0c\u964d\u4f4e\u81ea\u4e3b\u9a7e\u9a76\u4e2d\u7684\u78b0\u649e\u7387\uff0c\u5e76\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u8868\u73b0\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u91cd\u5efa\u5bfc\u5411\u7684\u8868\u5f81\u5b66\u4e60\u4e0e\u89c4\u5212\u4efb\u52a1\u590d\u6742\u4ea4\u7ec7\u7684\u95ee\u9898\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u4f18\u7684\u89c4\u5212\u4f18\u5316\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u89c4\u5212\u5bfc\u5411\u7684\u6f5c\u5728\u4e16\u754c\u6a21\u578b\u6846\u67b6WorldRFT\u3002", "method": "WorldRFT\u91c7\u7528\u5c42\u6b21\u5316\u89c4\u5212\u4efb\u52a1\u5206\u89e3\u548c\u5c40\u90e8\u611f\u77e5\u4ea4\u4e92\u7ec6\u5316\u673a\u5236\uff0c\u540c\u65f6\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u63d0\u5347\u5b89\u5168\u5173\u952e\u653f\u7b56\u8868\u73b0\uff0c\u6574\u5408\u89c6\u89c9\u51e0\u4f55\u57fa\u7840\u6a21\u578b\u4ee5\u6539\u55843D\u7a7a\u95f4\u611f\u77e5\u3002", "result": "WorldRFT\u663e\u8457\u51cf\u5c11\u4e86nuScenes\u4e2d\u7684\u78b0\u649e\u7387\uff08\u4ece0.30%\u4e0b\u964d\u81f30.05%\uff09\uff0c\u5728NavSim\u4e2d\u4ee5\u4ec5\u4f7f\u7528\u76f8\u673a\u8f93\u5165\u83b7\u5f97\u4e86\u4e0e\u57fa\u4e8eLiDAR\u7684\u6700\u5148\u8fdb\u65b9\u6cd5DiffusionDrive\u76f8\u5ab2\u7f8e\u7684\u6027\u80fd\uff0887.8 vs. 88.1 PDMS\uff09\u3002", "conclusion": "WorldRFT\u5728\u5f00\u653e\u73af\u8defnuScenes\u548c\u95ed\u73afNavSim\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u51cf\u5c11\u78b0\u649e\u7387\u548c\u5728\u76f8\u673a\u8f93\u5165\u6761\u4ef6\u4e0b\u63d0\u4f9b\u7ade\u4e89\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5176\u5728\u81ea\u4e3b\u9a7e\u9a76\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2512.19148", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.19148", "abs": "https://arxiv.org/abs/2512.19148", "authors": ["Jose Gustavo Buenaventura Carreon", "Floris Erich", "Roman Mykhailyshyn", "Tomohiro Motoda", "Ryo Hanai", "Yukiyasu Domae"], "title": "A Flexible Field-Based Policy Learning Framework for Diverse Robotic Systems and Sensors", "comment": "6 pages, 7 figures, conference: SII 2026. Cancun, Mexico", "summary": "We present a cross robot visuomotor learning framework that integrates diffusion policy based control with 3D semantic scene representations from D3Fields to enable category level generalization in manipulation. Its modular design supports diverse robot camera configurations including UR5 arms with Microsoft Azure Kinect arrays and bimanual manipulators with Intel RealSense sensors through a low latency control stack and intuitive teleoperation. A unified configuration layer enables seamless switching between setups for flexible data collection training and evaluation. In a grasp and lift block task the framework achieved an 80 percent success rate after only 100 demonstration episodes demonstrating robust skill transfer between platforms and sensing modalities. This design paves the way for scalable real world studies in cross robotic generalization.", "AI": {"tldr": "\u8be5\u6846\u67b6\u5229\u7528\u6269\u6563\u7b56\u7565\u63a7\u5236\u4e0e3D\u573a\u666f\u8868\u793a\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u64cd\u63a7\u6280\u80fd\u7684\u7c7b\u522b\u7ea7\u6cdb\u5316\uff0c\u6210\u529f\u7387\u4e3a80%\u3002", "motivation": "\u5e0c\u671b\u5b9e\u73b0\u8de8\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u64cd\u63a7\u6280\u80fd\u6cdb\u5316\uff0c\u63d0\u9ad8\u591a\u79cd\u673a\u5668\u4eba\u5e73\u53f0\u7684\u64cd\u63a7\u7075\u6d3b\u6027\u3002", "method": "\u96c6\u6210\u6269\u6563\u7b56\u7565\u63a7\u5236\u4e0e3D\u8bed\u4e49\u573a\u666f\u8868\u793a\uff0c\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\u3001\u4f4e\u5ef6\u8fdf\u63a7\u5236\u6808\u548c\u76f4\u89c2\u7684\u8fdc\u7a0b\u64cd\u4f5c\uff0c\u652f\u6301\u4e0d\u540c\u673a\u5668\u4eba\u7684\u6444\u50cf\u5934\u914d\u7f6e\u3002", "result": "\u5728\u6293\u53d6\u548c\u63d0\u5347\u5757\u7684\u4efb\u52a1\u4e2d\uff0c\u7ecf\u8fc7100\u6b21\u6f14\u793a\uff0c\u8be5\u6846\u67b6\u5b9e\u73b0\u4e8680%\u7684\u6210\u529f\u7387\uff0c\u5c55\u793a\u4e86\u5e73\u53f0\u95f4\u548c\u4f20\u611f\u65b9\u5f0f\u7684\u7a33\u5065\u6280\u80fd\u8f6c\u79fb\u3002", "conclusion": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u8de8\u673a\u5668\u4eba\u64cd\u63a7\u7684\u7c7b\u522b\u7ea7\u6cdb\u5316\uff0c\u6210\u529f\u7387\u8fbe\u523080%\u3002"}}
{"id": "2512.19178", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19178", "abs": "https://arxiv.org/abs/2512.19178", "authors": ["Jin Wang", "Kim Tien Ly", "Jacques Cloete", "Nikos Tsagarakis", "Ioannis Havoutis"], "title": "Vision-Language-Policy Model for Dynamic Robot Task Planning", "comment": "Manuscript under review", "summary": "Bridging the gap between natural language commands and autonomous execution in unstructured environments remains an open challenge for robotics. This requires robots to perceive and reason over the current task scene through multiple modalities, and to plan their behaviors to achieve their intended goals. Traditional robotic task-planning approaches often struggle to bridge low-level execution with high-level task reasoning, and cannot dynamically update task strategies when instructions change during execution, which ultimately limits their versatility and adaptability to new tasks. In this work, we propose a novel language model-based framework for dynamic robot task planning. Our Vision-Language-Policy (VLP) model, based on a vision-language model fine-tuned on real-world data, can interpret semantic instructions and integrate reasoning over the current task scene to generate behavior policies that control the robot to accomplish the task. Moreover, it can dynamically adjust the task strategy in response to changes in the task, enabling flexible adaptation to evolving task requirements. Experiments conducted with different robots and a variety of real-world tasks show that the trained model can efficiently adapt to novel scenarios and dynamically update its policy, demonstrating strong planning autonomy and cross-embodiment generalization. Videos: https://robovlp.github.io/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u52a8\u6001\u4efb\u52a1\u89c4\u5212\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u89e3\u91ca\u8bed\u8a00\u6307\u4ee4\u5e76\u7075\u6d3b\u9002\u5e94\u53d8\u5316\u7684\u4efb\u52a1\u9700\u6c42\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u7684\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4e0e\u81ea\u4e3b\u6267\u884c\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u5347\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u52a8\u6001\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u6846\u67b6\uff0c\u4f7f\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u9ad8\u6548\u9002\u5e94\u65b0\u573a\u666f\uff0c\u52a8\u6001\u66f4\u65b0\u7b56\u7565\uff0c\u5c55\u73b0\u51fa\u826f\u597d\u7684\u89c4\u5212\u80fd\u529b\u3002", "conclusion": "\u8be5\u6a21\u578b\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u89c4\u5212\u81ea\u4e3b\u6027\u548c\u8de8\u4e3b\u4f53\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u591f\u5728\u65b0\u7684\u573a\u666f\u4e2d\u9ad8\u6548\u9002\u5e94\u5e76\u52a8\u6001\u66f4\u65b0\u7b56\u7565\u3002"}}
{"id": "2512.19269", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.19269", "abs": "https://arxiv.org/abs/2512.19269", "authors": ["Yitian Zheng", "Zhangchen Ye", "Weijun Dong", "Shengjie Wang", "Yuyang Liu", "Chongjie Zhang", "Chuan Wen", "Yang Gao"], "title": "Translating Flow to Policy via Hindsight Online Imitation", "comment": null, "summary": "Recent advances in hierarchical robot systems leverage a high-level planner to propose task plans and a low-level policy to generate robot actions. This design allows training the planner on action-free or even non-robot data sources (e.g., videos), providing transferable high-level guidance. Nevertheless, grounding these high-level plans into executable actions remains challenging, especially with the limited availability of high-quality robot data. To this end, we propose to improve the low-level policy through online interactions. Specifically, our approach collects online rollouts, retrospectively annotates the corresponding high-level goals from achieved outcomes, and aggregates these hindsight-relabeled experiences to update a goal-conditioned imitation policy. Our method, Hindsight Flow-conditioned Online Imitation (HinFlow), instantiates this idea with 2D point flows as the high-level planner. Across diverse manipulation tasks in both simulation and physical world, our method achieves more than $2\\times$ performance improvement over the base policy, significantly outperforming the existing methods. Moreover, our framework enables policy acquisition from planners trained on cross-embodiment video data, demonstrating its potential for scalable and transferable robot learning.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faHinFlow\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7ebf\u4ea4\u4e92\u548c\u56de\u987e\u6027\u7ecf\u9a8c\u6807\u6ce8\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u7684\u64cd\u4f5c\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\uff0c\u5c55\u793a\u4e86\u8de8\u4f53\u4f53\u73b0\u8c61\u6570\u636e\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u65e8\u5728\u5c06\u9ad8\u5c42\u89c4\u5212\u6709\u6548\u5730\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u5e76\u5728\u4f4e\u5c42\u7b56\u7565\u4e2d\u901a\u8fc7\u5728\u7ebf\u4ea4\u4e92\u8fdb\u884c\u6539\u8fdb\uff0c\u4ee5\u5e94\u5bf9\u9ad8\u8d28\u91cf\u673a\u5668\u4eba\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "method": "Hindsight Flow-conditioned Online Imitation (HinFlow)\uff0c\u901a\u8fc7\u5728\u7ebf\u4ea4\u4e92\u83b7\u53d6\u56de\u987e\u6027\u6807\u6ce8\u7684\u7ecf\u9a8c\uff0c\u66f4\u65b0\u76ee\u6807\u6761\u4ef6\u7684\u6a21\u4eff\u7b56\u7565\u3002", "result": "\u5728\u591a\u79cd\u6a21\u62df\u548c\u7269\u7406\u4e16\u754c\u7684\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0cHinFlow\u65b9\u6cd5\u6bd4\u57fa\u7840\u7b56\u7565\u8868\u73b0\u63d0\u9ad8\u8d85\u8fc72\u500d\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u64cd\u4f5c\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7684\u8868\u73b0\uff0c\u5e76\u5c55\u793a\u4e86\u8de8\u4f53\u4f53\u73b0\u8c61\u6570\u636e\u4e2d\u8bad\u7ec3\u7684\u89c4\u5212\u8005\u7684\u53ef\u6269\u5c55\u6027\u548c\u53ef\u8f6c\u79fb\u6027\u3002"}}
{"id": "2512.19270", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.19270", "abs": "https://arxiv.org/abs/2512.19270", "authors": ["Zhaoyang Liu", "Weitao Zhou", "Junze Wen", "Cheng Jing", "Qian Cheng", "Kun Jiang", "Diange Yang"], "title": "Are All Data Necessary? Efficient Data Pruning for Large-scale Autonomous Driving Dataset via Trajectory Entropy Maximization", "comment": "7 pages, 4 figures", "summary": "Collecting large-scale naturalistic driving data is essential for training robust autonomous driving planners. However, real-world datasets often contain a substantial amount of repetitive and low-value samples, which lead to excessive storage costs and bring limited benefits to policy learning. To address this issue, we propose an information-theoretic data pruning method that effectively reduces the training data volume without compromising model performance. Our approach evaluates the trajectory distribution information entropy of driving data and iteratively selects high-value samples that preserve the statistical characteristics of the original dataset in a model-agnostic manner. From a theoretical perspective, we show that maximizing trajectory entropy effectively constrains the Kullback-Leibler divergence between the pruned subset and the original data distribution, thereby maintaining generalization ability. Comprehensive experiments on the NuPlan benchmark with a large-scale imitation learning framework demonstrate that the proposed method can reduce the dataset size by up to 40% while maintaining closed-loop performance. This work provides a lightweight and theoretically grounded approach for scalable data management and efficient policy learning in autonomous driving systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4fe1\u606f\u8bba\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fee\u526a\u4f4e\u4ef7\u503c\u6837\u672c\uff0c\u51cf\u5c11\u6570\u636e\u5b58\u50a8\u6210\u672c\u5e76\u63d0\u9ad8\u653f\u7b56\u5b66\u4e60\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e2d\u91cd\u590d\u548c\u4f4e\u4ef7\u503c\u6837\u672c\u5e26\u6765\u7684\u5b58\u50a8\u6210\u672c\u8fc7\u9ad8\u53ca\u5bf9\u653f\u7b56\u5b66\u4e60\u6548\u76ca\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u8bc4\u4f30\u9a7e\u9a76\u6570\u636e\u7684\u8f68\u8ff9\u5206\u5e03\u4fe1\u606f\u71b5\uff0c\u8fed\u4ee3\u9009\u62e9\u9ad8\u4ef7\u503c\u6837\u672c\uff0c\u4ee5\u4fdd\u6301\u539f\u59cb\u6570\u636e\u96c6\u7684\u7edf\u8ba1\u7279\u5f81\u3002", "result": "\u5728NuPlan\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u5c06\u6570\u636e\u96c6\u5927\u5c0f\u51cf\u5c11\u591a\u8fbe40%\uff0c\u800c\u95ed\u73af\u6027\u80fd\u4fdd\u6301\u4e0d\u53d8\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4fe1\u606f\u8bba\u6570\u636e\u4fee\u526a\u65b9\u6cd5\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u6570\u636e\u96c6\u7684\u6709\u6548\u7f29\u51cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2512.19289", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.19289", "abs": "https://arxiv.org/abs/2512.19289", "authors": ["Longxiang Shao", "Ulrich Dahmen", "Juergen Rossmann"], "title": "Comparison and Evaluation of Different Simulation Environments for Rigid Body Systems", "comment": "Accepted at the 10th MHI-Fachkolloquium", "summary": "Rigid body dynamics simulators are important tools for the design, analysis and optimization of mechanical systems in a variety of technical and scientific applications. This study examines four different simulation environments (Adams, Simscape, OpenModelica, and VEROSIM), focusing in particular on the comparison of the modeling methods, the numerical solvers, and the treatment of numerical problems that arise especially in closed-loop kinematics (esp. redundant boundary conditions and static equilibrium problem). A novel and complex crane boom of a real forestry machine serves as a practical benchmark application example. The direct comparison of the different solution approaches in the examined simulation tools supports the user in selecting the most suitable tool for his application.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u56db\u79cd\u521a\u4f53\u52a8\u529b\u5b66\u4eff\u771f\u73af\u5883\uff0c\u5f3a\u8c03\u4e0d\u540c\u5de5\u5177\u5728\u5efa\u6a21\u548c\u6c42\u89e3\u65b9\u9762\u7684\u4f18\u7f3a\u70b9\uff0c\u4ee5\u5e2e\u52a9\u7528\u6237\u9009\u62e9\u5408\u9002\u7684\u4eff\u771f\u5de5\u5177\u3002", "motivation": "\u4e3a\u4e86\u652f\u6301\u673a\u68b0\u7cfb\u7edf\u7684\u8bbe\u8ba1\u3001\u5206\u6790\u548c\u4f18\u5316\uff0c\u63d0\u4f9b\u4e0d\u540c\u4eff\u771f\u5de5\u5177\u7684\u6bd4\u8f83\u4f7f\u7528\u6237\u80fd\u591f\u9009\u62e9\u6700\u9002\u5408\u5176\u5e94\u7528\u7684\u5de5\u5177\u3002", "method": "\u6bd4\u8f83\u56db\u79cd\u4e0d\u540c\u7684\u4eff\u771f\u73af\u5883\uff08Adams, Simscape, OpenModelica, VEROSIM\uff09\uff0c\u7279\u522b\u5173\u6ce8\u5efa\u6a21\u65b9\u6cd5\u3001\u6570\u503c\u6c42\u89e3\u5668\u548c\u95ed\u73af\u8fd0\u52a8\u5b66\u4e2d\u7684\u6570\u503c\u95ee\u9898\u3002", "result": "\u4f7f\u7528\u5b9e\u9645\u6797\u4e1a\u673a\u68b0\u7684\u590d\u6742\u8d77\u91cd\u81c2\u4f5c\u4e3a\u57fa\u51c6\u5e94\u7528\u793a\u4f8b\uff0c\u5c55\u793a\u4e86\u4e0d\u540c\u89e3\u51b3\u65b9\u6848\u5728\u4e0d\u540c\u4eff\u771f\u5de5\u5177\u4e2d\u7684\u76f4\u63a5\u6bd4\u8f83\u3002", "conclusion": "\u4e0d\u540c\u7684\u4eff\u771f\u5de5\u5177\u5728\u5efa\u6a21\u65b9\u6cd5\u3001\u6570\u503c\u6c42\u89e3\u5668\u4ee5\u53ca\u5904\u7406\u6570\u503c\u95ee\u9898\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4e3a\u7528\u6237\u9009\u62e9\u5408\u9002\u7684\u5de5\u5177\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002"}}
{"id": "2512.19347", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.19347", "abs": "https://arxiv.org/abs/2512.19347", "authors": ["Han Fang", "Yize Huang", "Yuheng Zhao", "Paul Weng", "Xiao Li", "Yutong Ban"], "title": "OMP: One-step Meanflow Policy with Directional Alignment", "comment": null, "summary": "Robot manipulation, a key capability of embodied AI, has turned to data-driven generative policy frameworks, but mainstream approaches like Diffusion Models suffer from high inference latency and Flow-based Methods from increased architectural complexity. While simply applying meanFlow on robotic tasks achieves single-step inference and outperforms FlowPolicy, it lacks few-shot generalization due to fixed temperature hyperparameters in its Dispersive Loss and misaligned predicted-true mean velocities. To solve these issues, this study proposes an improved MeanFlow-based Policies: we introduce a lightweight Cosine Loss to align velocity directions and use the Differential Derivation Equation (DDE) to optimize the Jacobian-Vector Product (JVP) operator. Experiments on Adroit and Meta-World tasks show the proposed method outperforms MP1 and FlowPolicy in average success rate, especially in challenging Meta-World tasks, effectively enhancing few-shot generalization and trajectory accuracy of robot manipulation policies while maintaining real-time performance, offering a more robust solution for high-precision robotic manipulation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684MeanFlow\u7b56\u7565\uff0c\u901a\u8fc7\u4f59\u5f26\u635f\u5931\u548cDDE\u4f18\u5316\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u5b9e\u65f6\u6027\u80fd\u3001\u5c11\u91cf\u6837\u672c\u6cdb\u5316\u53ca\u8f68\u8ff9\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u7684\u6570\u636e\u9a71\u52a8\u751f\u6210\u7b56\u7565\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u5b58\u5728\u63a8\u7406\u5ef6\u8fdf\u9ad8\u548c\u67b6\u6784\u590d\u6742\u6027\u589e\u52a0\u7b49\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3MeanFlow\u5728\u56fa\u5b9a\u6e29\u5ea6\u8d85\u53c2\u6570\u5bfc\u81f4\u7684\u5c11\u91cf\u6837\u672c\u6cdb\u5316\u6b20\u7f3a\u53ca\u9884\u6d4b-\u771f\u5b9e\u5e73\u5747\u901f\u5ea6\u4e0d\u5bf9\u9f50\u7684\u95ee\u9898\u3002", "method": "\u6539\u8fdb\u7684MeanFlow\u7b56\u7565\u5f15\u5165\u8f7b\u91cf\u7ea7\u7684\u4f59\u5f26\u635f\u5931\u6765\u5bf9\u9f50\u901f\u5ea6\u65b9\u5411\uff0c\u5e76\u4f7f\u7528\u5fae\u5206\u63a8\u5bfc\u65b9\u7a0b\uff08DDE\uff09\u4f18\u5316\u96c5\u53ef\u6bd4-\u5411\u91cf\u4e58\u79ef\uff08JVP\uff09\u7b97\u5b50\u3002", "result": "\u5728Adroit\u548cMeta-World\u4efb\u52a1\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6210\u529f\u7387\u4e0a\u4f18\u4e8eMP1\u548cFlowPolicy\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u6311\u6218\u6027\u7684Meta-World\u4efb\u52a1\u4e2d\uff0c\u5728\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u5c11\u91cf\u6837\u672c\u6cdb\u5316\u548c\u8f68\u8ff9\u7cbe\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u7684\u6539\u8fdbMeanFlow\u7b56\u7565\u5728\u5b9e\u65f6\u6027\u80fd\u3001\u5c11\u91cf\u6837\u672c\u6cdb\u5316\u548c\u8f68\u8ff9\u7cbe\u5ea6\u65b9\u9762\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7684\u64cd\u4f5c\u80fd\u529b\uff0c\u662f\u9ad8\u7cbe\u5ea6\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u66f4\u5f3a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.19390", "categories": ["cs.RO", "cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.19390", "abs": "https://arxiv.org/abs/2512.19390", "authors": ["Hongwei Fan", "Hang Dai", "Jiyao Zhang", "Jinzhou Li", "Qiyang Yan", "Yujie Zhao", "Mingju Gao", "Jinghang Wu", "Hao Tang", "Hao Dong"], "title": "TwinAligner: Visual-Dynamic Alignment Empowers Physics-aware Real2Sim2Real for Robotic Manipulation", "comment": null, "summary": "The robotics field is evolving towards data-driven, end-to-end learning, inspired by multimodal large models. However, reliance on expensive real-world data limits progress. Simulators offer cost-effective alternatives, but the gap between simulation and reality challenges effective policy transfer. This paper introduces TwinAligner, a novel Real2Sim2Real system that addresses both visual and dynamic gaps. The visual alignment module achieves pixel-level alignment through SDF reconstruction and editable 3DGS rendering, while the dynamic alignment module ensures dynamic consistency by identifying rigid physics from robot-object interaction. TwinAligner improves robot learning by providing scalable data collection and establishing a trustworthy iterative cycle, accelerating algorithm development. Quantitative evaluations highlight TwinAligner's strong capabilities in visual and dynamic real-to-sim alignment. This system enables policies trained in simulation to achieve strong zero-shot generalization to the real world. The high consistency between real-world and simulated policy performance underscores TwinAligner's potential to advance scalable robot learning. Code and data will be released on https://twin-aligner.github.io", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u7684 TwinAligner \u7cfb\u7edf\u901a\u8fc7\u9488\u5bf9\u89c6\u89c9\u548c\u52a8\u6001\u4e00\u81f4\u6027\u8fdb\u884c\u5bf9\u9f50\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5b66\u4e60\u6548\u7387\uff0c\u5b9e\u73b0\u6a21\u62df\u5230\u73b0\u5b9e\u653f\u7b56\u7684\u6709\u6548\u8f6c\u79fb\u3002", "motivation": "\u5c3d\u7ba1\u673a\u5668\u4eba\u9886\u57df\u671d\u7740\u6570\u636e\u9a71\u52a8\u7684\u7aef\u5230\u7aef\u5b66\u4e60\u53d1\u5c55\uff0c\u4f46\u6602\u8d35\u7684\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u9650\u5236\u4e86\u8fdb\u5c55\u3002\u56e0\u6b64\uff0c\u6a21\u62df\u5668\u4f5c\u4e3a\u6210\u672c\u6548\u76ca\u9ad8\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u6a21\u62df\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u7684\u5dee\u8ddd\u4ecd\u7136\u662f\u6709\u6548\u653f\u7b56\u8f6c\u79fb\u7684\u6311\u6218\u3002", "method": "\u5f15\u5165\u4e86\u53cc\u91cd\u5bf9\u9f50\u6a21\u5757\uff0c\u5206\u522b\u9488\u5bf9\u89c6\u89c9\u548c\u52a8\u6001\u4e00\u81f4\u6027\uff0c\u901a\u8fc7 SDF \u91cd\u6784\u53ca\u53ef\u7f16\u8f91\u7684 3DGS \u6e32\u67d3\u5b9e\u73b0\u50cf\u7d20\u7ea7\u5bf9\u9f50\uff0c\u540c\u65f6\u8bc6\u522b\u673a\u5668\u4eba\u4e0e\u7269\u4f53\u4ea4\u4e92\u4e2d\u7684\u521a\u6027\u7269\u7406\u4ee5\u4fdd\u969c\u52a8\u6001\u4e00\u81f4\u6027\u3002", "result": "\u5b9a\u91cf\u8bc4\u4f30\u663e\u793a TwinAligner \u5728\u89c6\u89c9\u548c\u52a8\u6001\u7684\u771f\u5b9e\u5230\u6a21\u62df\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f7f\u5f97\u5728\u6a21\u62df\u4e2d\u8bad\u7ec3\u7684\u653f\u7b56\u80fd\u591f\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u5b9e\u73b0\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u3002", "conclusion": "TwinAligner \u80fd\u591f\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u5b66\u4e60\uff0c\u786e\u4fdd\u6a21\u62df\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u7684\u653f\u7b56\u8868\u73b0\u4e00\u81f4\u6027\uff0c\u4ece\u800c\u4fc3\u8fdb\u5927\u89c4\u6a21\u673a\u5668\u4eba\u5b66\u4e60\u7684\u8fdb\u6b65\u3002"}}
{"id": "2512.19402", "categories": ["cs.RO", "cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.19402", "abs": "https://arxiv.org/abs/2512.19402", "authors": ["Yujie Zhao", "Hongwei Fan", "Di Chen", "Shengcong Chen", "Liliang Chen", "Xiaoqi Li", "Guanghui Ren", "Hao Dong"], "title": "Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface", "comment": null, "summary": "Recent progress in robot learning has been driven by large-scale datasets and powerful visuomotor policy architectures, yet policy robustness remains limited by the substantial cost of collecting diverse demonstrations, particularly for spatial generalization in manipulation tasks. To reduce repetitive data collection, we present Real2Edit2Real, a framework that generates new demonstrations by bridging 3D editability with 2D visual data through a 3D control interface. Our approach first reconstructs scene geometry from multi-view RGB observations with a metric-scale 3D reconstruction model. Based on the reconstructed geometry, we perform depth-reliable 3D editing on point clouds to generate new manipulation trajectories while geometrically correcting the robot poses to recover physically consistent depth, which serves as a reliable condition for synthesizing new demonstrations. Finally, we propose a multi-conditional video generation model guided by depth as the primary control signal, together with action, edge, and ray maps, to synthesize spatially augmented multi-view manipulation videos. Experiments on four real-world manipulation tasks demonstrate that policies trained on data generated from only 1-5 source demonstrations can match or outperform those trained on 50 real-world demonstrations, improving data efficiency by up to 10-50x. Moreover, experimental results on height and texture editing demonstrate the framework's flexibility and extensibility, indicating its potential to serve as a unified data generation framework.", "AI": {"tldr": "\u4ecb\u7ecdReal2Edit2Real\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u54083D\u7f16\u8f91\u4e0e2D\u89c6\u89c9\u6570\u636e\uff0c\u5728\u673a\u5668\u4eba\u64cd\u63a7\u4efb\u52a1\u4e2d\u751f\u6210\u65b0\u7684\u6f14\u793a\u6570\u636e\uff0c\u63d0\u9ad8\u6570\u636e\u6548\u7387\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\uff0c\u653f\u7b56\u7684\u9c81\u68d2\u6027\u53d7\u9650\u4e8e\u591a\u6837\u5316\u6f14\u793a\u7684\u6536\u96c6\u6210\u672c\uff0c\u7279\u522b\u662f\u5728\u64cd\u63a7\u4efb\u52a1\u4e2d\u7684\u7a7a\u95f4\u6cdb\u5316\u3002", "method": "\u901a\u8fc73D\u63a7\u5236\u63a5\u53e3\u5c063D\u53ef\u7f16\u8f91\u6027\u4e0e2D\u89c6\u89c9\u6570\u636e\u7ed3\u5408\uff0c\u4ece\u591a\u89c6\u89d2RGB\u89c2\u6d4b\u4e2d\u91cd\u5efa\u573a\u666f\u51e0\u4f55\u7ed3\u6784\uff0c\u5e76\u5bf9\u70b9\u4e91\u8fdb\u884c\u6df1\u5ea6\u53ef\u9760\u76843D\u7f16\u8f91\uff0c\u4ee5\u751f\u6210\u65b0\u7684\u64cd\u4f5c\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4ec5\u75281-5\u4e2a\u6e90\u6f14\u793a\u751f\u6210\u7684\u6570\u636e\u8bad\u7ec3\u7684\u653f\u7b56\u53ef\u5339\u654c\u6216\u8d85\u8d8a50\u4e2a\u771f\u5b9e\u6f14\u793a\u8bad\u7ec3\u7684\u653f\u7b56\uff0c\u6570\u636e\u6548\u7387\u63d0\u9ad8\u8fbe\u523010-50\u500d\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u6548\u7387\uff0c\u5e76\u53ef\u7528\u4e8e\u4e0d\u540c\u7684\u64cd\u4f5c\u4efb\u52a1\u751f\u6210\u65b0\u7684\u6f14\u793a\u6570\u636e\u3002"}}
{"id": "2512.19453", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.19453", "abs": "https://arxiv.org/abs/2512.19453", "authors": ["Zhenglong Guo", "Yiming Zhao", "Feng Jiang", "Heng Jin", "Zongbao Feng", "Jianbin Zhou", "Siyuan Xu"], "title": "MaP-AVR: A Meta-Action Planner for Agents Leveraging Vision Language Models and Retrieval-Augmented Generation", "comment": "8 pages, 10 figures, This work was completed in December 2024", "summary": "Embodied robotic AI systems designed to manage complex daily tasks rely on a task planner to understand and decompose high-level tasks. While most research focuses on enhancing the task-understanding abilities of LLMs/VLMs through fine-tuning or chain-of-thought prompting, this paper argues that defining the planned skill set is equally crucial. To handle the complexity of daily environments, the skill set should possess a high degree of generalization ability. Empirically, more abstract expressions tend to be more generalizable. Therefore, we propose to abstract the planned result as a set of meta-actions. Each meta-action comprises three components: {move/rotate, end-effector status change, relationship with the environment}. This abstraction replaces human-centric concepts, such as grasping or pushing, with the robot's intrinsic functionalities. As a result, the planned outcomes align seamlessly with the complete range of actions that the robot is capable of performing. Furthermore, to ensure that the LLM/VLM accurately produces the desired meta-action format, we employ the Retrieval-Augmented Generation (RAG) technique, which leverages a database of human-annotated planning demonstrations to facilitate in-context learning. As the system successfully completes more tasks, the database will self-augment to continue supporting diversity. The meta-action set and its integration with RAG are two novel contributions of our planner, denoted as MaP-AVR, the meta-action planner for agents composed of VLM and RAG. To validate its efficacy, we design experiments using GPT-4o as the pre-trained LLM/VLM model and OmniGibson as our robotic platform. Our approach demonstrates promising performance compared to the current state-of-the-art method. Project page: https://map-avr.github.io/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4efb\u52a1\u89c4\u5212\u65b9\u6cd5MaP-AVR\uff0c\u5f3a\u8c03\u6280\u80fd\u96c6\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u901a\u8fc7meta-action\u548cRAG\u6280\u672f\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u6267\u884c\u4efb\u52a1\u7684\u6548\u679c\u3002", "motivation": "\u5f3a\u8c03\u4efb\u52a1\u89c4\u5212\u4e2d\u7684\u6280\u80fd\u96c6\u5b9a\u4e49\uff0c\u4e0e\u73b0\u6709\u7684\u63d0\u5347\u7406\u89e3\u80fd\u529b\u7684\u7814\u7a76\u76f8\u6bd4\uff0c\u6280\u80fd\u96c6\u7684\u5e7f\u6cdb\u6982\u5316\u80fd\u529b\u540c\u6837\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u751f\u6210meta-action\u96c6\u5408\u5e76\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u6280\u672f\u6765\u6784\u5efa\u4efb\u52a1\u89c4\u5212\u5668\u3002\u4f7f\u7528GPT-4o\u4f5c\u4e3a\u9884\u8bad\u7ec3\u7684LLM/VLM\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u6267\u884c\u4efb\u52a1\u7684\u80fd\u529b\u4e0a\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\uff0c\u6210\u529f\u5904\u7406\u590d\u6742\u65e5\u5e38\u4efb\u52a1\u3002", "conclusion": "MaP-AVR\u5c55\u793a\u4e86\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2512.19562", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19562", "abs": "https://arxiv.org/abs/2512.19562", "authors": ["Martin Sedlacek", "Pavlo Yefanov", "Georgy Ponimatkin", "Jai Bardhan", "Simon Pilc", "Mederic Fourmy", "Evangelos Kazakos", "Cees G. M. Snoek", "Josef Sivic", "Vladimir Petrik"], "title": "REALM: A Real-to-Sim Validated Benchmark for Generalization in Robotic Manipulation", "comment": "9 pages, 10 figures", "summary": "Vision-Language-Action (VLA) models empower robots to understand and execute tasks described by natural language instructions. However, a key challenge lies in their ability to generalize beyond the specific environments and conditions they were trained on, which is presently difficult and expensive to evaluate in the real-world. To address this gap, we present REALM, a new simulation environment and benchmark designed to evaluate the generalization capabilities of VLA models, with a specific emphasis on establishing a strong correlation between simulated and real-world performance through high-fidelity visuals and aligned robot control. Our environment offers a suite of 15 perturbation factors, 7 manipulation skills, and more than 3,500 objects. Finally, we establish two task sets that form our benchmark and evaluate the \u03c0_{0}, \u03c0_{0}-FAST, and GR00T N1.5 VLA models, showing that generalization and robustness remain an open challenge. More broadly, we also show that simulation gives us a valuable proxy for the real-world and allows us to systematically probe for and quantify the weaknesses and failure modes of VLAs. Project page: https://martin-sedlacek.com/realm", "AI": {"tldr": "REALM\u662f\u4e00\u4e2a\u65b0\u7684\u6a21\u62df\u73af\u5883\uff0c\u65e8\u5728\u8bc4\u4f30\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u8868\u73b0\u548c\u5b58\u5728\u7684\u5f31\u70b9\u3002", "motivation": "\u5e94\u5bf9\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u6cdb\u5316\u80fd\u529b\u5dee\u3001\u8bc4\u4f30\u6210\u672c\u9ad8\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faREALM\uff0c\u4e00\u4e2a\u65b0\u7684\u6a21\u62df\u73af\u5883\u548c\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5305\u62ec\u9ad8\u4fdd\u771f\u89c6\u89c9\u548c\u5bf9\u9f50\u7684\u673a\u5668\u4eba\u63a7\u5236\u3002", "result": "\u901a\u8fc7\u5bf9\u03c0_{0}\u3001\u03c0_{0}-FAST\u548cGR00T N1.5\u6a21\u578b\u7684\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u572815\u4e2a\u6270\u52a8\u56e0\u7d20\u548c7\u4e2a\u64cd\u4f5c\u6280\u80fd\u4e0b\uff0c\u6a21\u578b\u7684\u6cdb\u5316\u548c\u9c81\u68d2\u6027\u4ecd\u7136\u5b58\u5728\u95ee\u9898\u3002", "conclusion": "\u6a21\u62df\u662f\u8bc4\u4f30\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u8868\u73b0\u7684\u6709\u6548\u5de5\u5177\uff0c\u4f46\u76ee\u524d\u8fd9\u4e9b\u6a21\u578b\u7684\u6cdb\u5316\u548c\u9c81\u68d2\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u6311\u6218\u3002"}}
{"id": "2512.19564", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19564", "abs": "https://arxiv.org/abs/2512.19564", "authors": ["Yanliang Huang", "Xia Yan", "Peiran Yin", "Zhenduo Zhang", "Zeyan Shao", "Youran Wang", "Haoliang Huang", "Matthias Althoff"], "title": "Results of the 2024 CommonRoad Motion Planning Competition for Autonomous Vehicles", "comment": null, "summary": "Over the past decade, a wide range of motion planning approaches for autonomous vehicles has been developed to handle increasingly complex traffic scenarios. However, these approaches are rarely compared on standardized benchmarks, limiting the assessment of relative strengths and weaknesses. To address this gap, we present the setup and results of the 4th CommonRoad Motion Planning Competition held in 2024, conducted using the CommonRoad benchmark suite. This annual competition provides an open-source and reproducible framework for benchmarking motion planning algorithms. The benchmark scenarios span highway and urban environments with diverse traffic participants, including passenger cars, buses, and bicycles. Planner performance is evaluated along four dimensions: efficiency, safety, comfort, and compliance with selected traffic rules. This report introduces the competition format and provides a comparison of representative high-performing planners from the 2023 and 2024 editions.", "AI": {"tldr": "\u672c\u62a5\u544a\u4ecb\u7ecd\u4e862024\u5e74\u7b2c\u56db\u5c4aCommonRoad\u8fd0\u52a8\u89c4\u5212\u7ade\u8d5b\u7684\u8bbe\u7f6e\u548c\u7ed3\u679c\uff0c\u65e8\u5728\u901a\u8fc7\u6807\u51c6\u5316\u57fa\u51c6\u8bc4\u4f30\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\u7684\u8868\u73b0\uff0c\u63a8\u52a8\u81ea\u4e3b\u8f66\u8f86\u6280\u672f\u7684\u53d1\u5c55\u3002", "motivation": "\u4e3a\u4e86\u586b\u8865\u4e0d\u540c\u81ea\u4e3b\u8f66\u8f86\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u5728\u6807\u51c6\u5316\u57fa\u51c6\u4e0a\u7f3a\u4e4f\u6bd4\u8f83\u7684\u7a7a\u767d\uff0c\u8fdb\u800c\u4fc3\u8fdb\u7b97\u6cd5\u7684\u53d1\u5c55\u548c\u4f18\u5316\u3002", "method": "\u901a\u8fc7\u4f7f\u7528CommonRoad\u57fa\u51c6\u5957\u4ef6\uff0c\u7ec4\u7ec7\u7b2c\u56db\u5c4aCommonRoad\u8fd0\u52a8\u89c4\u5212\u7ade\u8d5b\uff0c\u8bc4\u4f30\u4e0d\u540c\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "result": "\u7ade\u8d5b\u6db5\u76d6\u4e86\u9ad8\u901f\u516c\u8def\u548c\u57ce\u5e02\u73af\u5883\u7684\u591a\u6837\u5316\u4ea4\u901a\u573a\u666f\uff0c\u9ad8\u6027\u80fd\u89c4\u5212\u8005\u57282023\u548c2024\u5e74\u95f4\u7684\u8868\u73b0\u88ab\u6bd4\u8f83\uff0c\u7ed3\u679c\u5c06\u4fc3\u4f7f\u66f4\u597d\u7684\u7b97\u6cd5\u8bbe\u8ba1\u3002", "conclusion": "\u672c\u6b21\u8fd0\u52a8\u89c4\u5212\u7ade\u8d5b\u4e3a\u8bc4\u4f30\u81ea\u4e3b\u8f66\u8f86\u7684\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5404\u7c7b\u89c4\u5212\u7b97\u6cd5\u7684\u8868\u73b0\u5f97\u4ee5\u5728\u6548\u7387\u3001\u5b89\u5168\u6027\u3001\u8212\u9002\u6027\u548c\u9075\u5b88\u4ea4\u901a\u89c4\u5219\u7b49\u65b9\u9762\u8fdb\u884c\u6bd4\u8f83\u3002"}}
{"id": "2512.19567", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.19567", "abs": "https://arxiv.org/abs/2512.19567", "authors": ["Carlos P\u00e9rez-Ruiz", "Joan Sol\u00e0"], "title": "LIMOncello: Revisited IKFoM on the SGal(3) Manifold for Fast LiDAR-Inertial Odometry", "comment": null, "summary": "This work introduces LIMOncello, a tightly coupled LiDAR-Inertial Odometry system that models 6-DoF motion on the $\\mathrm{SGal}(3)$ manifold within an iterated error-state Kalman filter backend. Compared to state representations defined on $\\mathrm{SO}(3)\\times\\mathbb{R}^6$, the use of $\\mathrm{SGal}(3)$ provides a coherent and numerically stable discrete-time propagation model that helps limit drift in low-observability conditions.\n  LIMOncello also includes a lightweight incremental i-Octree mapping backend that enables faster updates and substantially lower memory usage than incremental kd-tree style map structures, without relying on locality-restricted search heuristics. Experiments on multiple real-world datasets show that LIMOncello achieves competitive accuracy while improving robustness in geometrically sparse environments. The system maintains real-time performance with stable memory growth and is released as an extensible open-source implementation at https://github.com/CPerezRuiz335/LIMOncello.", "AI": {"tldr": "LIMOncello\u662f\u4e00\u79cd\u9ad8\u6548\u7684LiDAR-\u60ef\u6027\u6d4b\u7a0b\u7cfb\u7edf\uff0c\u5229\u7528$\text{SGal}(3)$\u6d41\u5f62\u51cf\u5c11\u6f02\u79fb\uff0c\u589e\u91cf\u6620\u5c04\u540e\u7aef\u63d0\u9ad8\u5185\u5b58\u6548\u7387\uff0c\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u7684LiDAR-\u60ef\u6027\u6d4b\u7a0b\uff08Odometry\uff09\u7cfb\u7edf\uff0c\u4ee5\u6539\u5584\u5728\u4f4e\u53ef\u89c2\u6d4b\u6027\u6761\u4ef6\u4e0b\u7684\u8fd0\u52a8\u4f30\u8ba1\uff0c\u5e76\u964d\u4f4e\u5185\u5b58\u4f7f\u7528\u3002", "method": "\u91c7\u7528\u8fed\u4ee3\u8bef\u5dee\u72b6\u6001\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u540e\u7aef\u5efa\u6a216-DoF\u8fd0\u52a8\uff0c\u4f7f\u7528$\text{SGal}(3)$\u6d41\u5f62\uff0c\u5e76\u914d\u5907\u8f7b\u91cf\u7ea7\u7684\u589e\u91cfi-Octree\u6620\u5c04\u540e\u7aef\uff0c\u4e3a\u6570\u636e\u66f4\u65b0\u63d0\u4f9b\u652f\u6301\u3002", "result": "LIMOncello\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u51e0\u4f55\u7a00\u758f\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5c55\u793a\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "LIMOncello\u5c55\u793a\u4e86\u5728\u51e0\u4f55\u7a00\u758f\u73af\u5883\u4e2d\u5177\u6709\u7ade\u4e89\u529b\u7684\u51c6\u786e\u6027\u548c\u66f4\u597d\u7684\u9c81\u68d2\u6027\uff0c\u4e14\u5177\u5907\u5b9e\u65f6\u6027\u80fd\u548c\u7a33\u5b9a\u7684\u5185\u5b58\u589e\u957f\u3002"}}
{"id": "2512.19576", "categories": ["cs.RO", "cs.AI", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.19576", "abs": "https://arxiv.org/abs/2512.19576", "authors": ["Kirill Djebko", "Tom Baumann", "Erik Dilger", "Frank Puppe", "Sergio Montenegro"], "title": "LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller", "comment": "55 pages, 27 figures, 29 tables. The maneuver telemetry datasets generated and analyzed during this work are available in the GitHub repository https://github.com/kdjebko/lelar-in-orbit-data", "summary": "Attitude control is essential for many satellite missions. Classical controllers, however, are time-consuming to design and sensitive to model uncertainties and variations in operational boundary conditions. Deep Reinforcement Learning (DRL) offers a promising alternative by learning adaptive control strategies through autonomous interaction with a simulation environment. Overcoming the Sim2Real gap, which involves deploying an agent trained in simulation onto the real physical satellite, remains a significant challenge. In this work, we present the first successful in-orbit demonstration of an AI-based attitude controller for inertial pointing maneuvers. The controller was trained entirely in simulation and deployed to the InnoCube 3U nanosatellite, which was developed by the Julius-Maximilians-Universit\u00e4t W\u00fcrzburg in cooperation with the Technische Universit\u00e4t Berlin, and launched in January 2025. We present the AI agent design, the methodology of the training procedure, the discrepancies between the simulation and the observed behavior of the real satellite, and a comparison of the AI-based attitude controller with the classical PD controller of InnoCube. Steady-state metrics confirm the robust performance of the AI-based controller during repeated in-orbit maneuvers.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u5728\u8f68\u9053\u4e0a\u6210\u529f\u5c55\u793a\u4e86\u57fa\u4e8eAI\u7684\u59ff\u6001\u63a7\u5236\u5668\uff0c\u5229\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u793a\u51fa\u5353\u8d8a\u7684\u7a33\u5065\u6027\u80fd\uff0c\u5bf9\u6bd4\u4f20\u7edf\u63a7\u5236\u5668\u3002", "motivation": "\u4f20\u7edf\u63a7\u5236\u5668\u8bbe\u8ba1\u8017\u65f6\uff0c\u4e14\u5bf9\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u548c\u64cd\u4f5c\u8fb9\u754c\u6761\u4ef6\u53d8\u5316\u654f\u611f\uff0c\u56e0\u6b64\u9700\u8981\u5bfb\u627e\u66f4\u6709\u6548\u7684\u63a7\u5236\u7b56\u7565\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60(DRL)\u8bad\u7ec3AI\u63a7\u5236\u5668\uff0c\u5b8c\u5168\u57fa\u4e8e\u4eff\u771f\u65b9\u6cd5\uff0c\u5e76\u5728InnoCube 3U\u7eb3\u7c73\u536b\u661f\u4e0a\u90e8\u7f72\u3002", "result": "AI\u63a7\u5236\u5668\u5728\u91cd\u590d\u7684\u8f68\u9053\u673a\u52a8\u4e2d\u663e\u793a\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u5e76\u4e0e\u4f20\u7edf\u7684PD\u63a7\u5236\u5668\u8fdb\u884c\u6bd4\u8f83\u3002", "conclusion": "\u5728\u5b9e\u9645\u8f68\u9053\u4e2d\uff0cAI\u63a7\u5236\u5668\u8868\u73b0\u51fa\u7a33\u5065\u7684\u6027\u80fd\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u60ef\u6027\u6307\u5411\u673a\u52a8\u64cd\u4f5c\u3002"}}
{"id": "2512.19583", "categories": ["cs.RO", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.19583", "abs": "https://arxiv.org/abs/2512.19583", "authors": ["Yinhuai Wang", "Runyi Yu", "Hok Wai Tsui", "Xiaoyi Lin", "Hui Zhang", "Qihan Zhao", "Ke Fan", "Miao Li", "Jie Song", "Jingbo Wang", "Qifeng Chen", "Ping Tan"], "title": "Learning Generalizable Hand-Object Tracking from Synthetic Demonstrations", "comment": null, "summary": "We present a system for learning generalizable hand-object tracking controllers purely from synthetic data, without requiring any human demonstrations. Our approach makes two key contributions: (1) HOP, a Hand-Object Planner, which can synthesize diverse hand-object trajectories; and (2) HOT, a Hand-Object Tracker that bridges synthetic-to-physical transfer through reinforcement learning and interaction imitation learning, delivering a generalizable controller conditioned on target hand-object states. Our method extends to diverse object shapes and hand morphologies. Through extensive evaluations, we show that our approach enables dexterous hands to track challenging, long-horizon sequences including object re-arrangement and agile in-hand reorientation. These results represent a significant step toward scalable foundation controllers for manipulation that can learn entirely from synthetic data, breaking the data bottleneck that has long constrained progress in dexterous manipulation.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5408\u6210\u6570\u636e\u4e2d\u5b66\u4e60\u53ef\u6cdb\u5316\u624b-\u7269\u4f53\u8ffd\u8e2a\u63a7\u5236\u5668\u7684\u7cfb\u7edf\uff0c\u5305\u542b\u624b-\u7269\u4f53\u89c4\u5212\u548c\u8ffd\u8e2a\u7ec4\u4ef6\uff0c\u80fd\u591f\u5728\u591a\u6837\u7684\u7269\u4f53\u548c\u624b\u5f62\u6001\u4e0b\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5f53\u524d\u624b-\u7269\u4f53\u8ffd\u8e2a\u6280\u672f\u53d7\u9650\u4e8e\u9700\u8981\u5927\u91cf\u4eba\u7c7b\u793a\u8303\u6570\u636e\uff0c\u9650\u5236\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u548c\u5e94\u7528\u8303\u56f4\u3002", "method": "\u63d0\u51fa\u4e86HOP\uff08\u624b-\u7269\u4f53\u89c4\u5212\u5668\uff09\u548cHOT\uff08\u624b-\u7269\u4f53\u8ffd\u8e2a\u5668\uff09\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u4ea4\u4e92\u6a21\u4eff\u5b66\u4e60\u5b9e\u73b0\u5408\u6210\u5230\u7269\u7406\u7684\u8f6c\u79fb\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u663e\u793a\u51fa\u8be5\u65b9\u6cd5\u80fd\u5728\u590d\u6742\u7684\u957f\u65f6\u95f4\u5e8f\u5217\u4e2d\u6709\u6548\u8ddf\u8e2a\uff0c\u5305\u62ec\u7269\u4f53\u91cd\u65b0\u6392\u5217\u548c\u7075\u6d3b\u7684\u624b\u5185\u91cd\u5b9a\u5411\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u4f7f\u7528\u5408\u6210\u6570\u636e\u5b9e\u73b0\u624b-\u7269\u4f53\u8ffd\u8e2a\u63a7\u5236\u5668\u7684\u53ef\u6cdb\u5316\u5b66\u4e60\uff0c\u5c55\u73b0\u4e86\u5728\u590d\u6742\u64cd\u63a7\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2512.19629", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.19629", "abs": "https://arxiv.org/abs/2512.19629", "authors": ["Jiaqi Peng", "Wenzhe Cai", "Yuqiang Yang", "Tai Wang", "Yuan Shen", "Jiangmiao Pang"], "title": "LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry", "comment": "Project page:https://steinate.github.io/logoplanner.github.io/", "summary": "Trajectory planning in unstructured environments is a fundamental and challenging capability for mobile robots. Traditional modular pipelines suffer from latency and cascading errors across perception, localization, mapping, and planning modules. Recent end-to-end learning methods map raw visual observations directly to control signals or trajectories, promising greater performance and efficiency in open-world settings. However, most prior end-to-end approaches still rely on separate localization modules that depend on accurate sensor extrinsic calibration for self-state estimation, thereby limiting generalization across embodiments and environments. We introduce LoGoPlanner, a localization-grounded, end-to-end navigation framework that addresses these limitations by: (1) finetuning a long-horizon visual-geometry backbone to ground predictions with absolute metric scale, thereby providing implicit state estimation for accurate localization; (2) reconstructing surrounding scene geometry from historical observations to supply dense, fine-grained environmental awareness for reliable obstacle avoidance; and (3) conditioning the policy on implicit geometry bootstrapped by the aforementioned auxiliary tasks, thereby reducing error propagation.We evaluate LoGoPlanner in both simulation and real-world settings, where its fully end-to-end design reduces cumulative error while metric-aware geometry memory enhances planning consistency and obstacle avoidance, leading to more than a 27.3\\% improvement over oracle-localization baselines and strong generalization across embodiments and environments. The code and models have been made publicly available on the \\href{https://steinate.github.io/logoplanner.github.io/}{project page}.", "AI": {"tldr": "LoGoPlanner\u662f\u4e00\u79cd\u65b0\u7684\u672c\u5730\u5316\u9a71\u52a8\u7684\u7aef\u5230\u7aef\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u89c6\u89c9\u51e0\u4f55\u53ca\u573a\u666f\u91cd\u6784\uff0c\u5b9e\u73b0\u4e86\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u66f4\u9ad8\u6548\u548c\u51c6\u786e\u7684\u673a\u5668\u4eba\u5bfc\u822a\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6a21\u5757\u5316\u6d41\u7a0b\u4e2d\u7531\u4e8e\u611f\u77e5\u3001\u5b9a\u4f4d\u3001\u6620\u5c04\u548c\u89c4\u5212\u6a21\u5757\u4e4b\u95f4\u7684\u5ef6\u8fdf\u548c\u7ea7\u8054\u9519\u8bef\u7684\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u79fb\u52a8\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u672c\u5730\u5316\u9a71\u52a8\u7684\u7aef\u5230\u7aef\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u5fae\u8c03\u89c6\u89c9\u51e0\u4f55\u9aa8\u5e72\u7f51\uff0c\u91cd\u6784\u573a\u666f\u51e0\u4f55\uff0c\u5e76\u6839\u636e\u9690\u5f0f\u51e0\u4f55\u8c03\u8282\u7b56\u7565\u3002", "result": "\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1LoGoPlanner\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u5176\u8bbe\u8ba1\u51cf\u5c11\u4e86\u7d2f\u8ba1\u9519\u8bef\uff0c\u5e76\u63d0\u9ad8\u4e86\u89c4\u5212\u4e00\u81f4\u6027\u548c\u969c\u788d\u7269\u89c4\u907f\u80fd\u529b\u3002", "conclusion": "LoGoPlanner\u901a\u8fc7\u5bf9\u957f\u65f6\u95f4\u89c6\u89c9\u51e0\u4f55\u9aa8\u5e72\u7f51\u7684\u5fae\u8c03\u548c\u573a\u666f\u51e0\u4f55\u7684\u91cd\u6784\uff0c\u63d0\u9ad8\u4e86\u5bfc\u822a\u7684\u51c6\u786e\u6027\u548c\u9ad8\u6548\u6027\uff0c\u6574\u4f53\u4e0a\u5b9e\u73b0\u4e8627.3%\u4ee5\u4e0a\u7684\u6539\u8fdb\uff0c\u5e76\u5728\u4e0d\u540c\u73af\u5883\u548c\u5b9e\u4f53\u4e4b\u95f4\u8868\u73b0\u51fa\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
