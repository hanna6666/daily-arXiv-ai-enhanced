{"id": "2509.25200", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25200", "abs": "https://arxiv.org/abs/2509.25200", "authors": ["Christian Arzate Cruz", "Edwin C. Montiel-Vazquez", "Chikara Maeda", "Randy Gomez"], "title": "When and How to Express Empathy in Human-Robot Interaction Scenarios", "comment": null, "summary": "Incorporating empathetic behavior into robots can improve their social\neffectiveness and interaction quality. In this paper, we present whEE (when and\nhow to express empathy), a framework that enables social robots to detect when\nempathy is needed and generate appropriate responses. Using large language\nmodels, whEE identifies key behavioral empathy cues in human interactions. We\nevaluate it in human-robot interaction scenarios with our social robot, Haru.\nResults show that whEE effectively identifies and responds to empathy cues,\nproviding valuable insights for designing social robots capable of adaptively\nmodulating their empathy levels across various interaction contexts."}
{"id": "2509.25249", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25249", "abs": "https://arxiv.org/abs/2509.25249", "authors": ["Guancheng Chen", "Sheng Yang", "Tong Zhan", "Jian Wang"], "title": "BEV-VLM: Trajectory Planning via Unified BEV Abstraction", "comment": null, "summary": "This paper introduces BEV-VLM, a novel framework for trajectory planning in\nautonomous driving that leverages Vision-Language Models (VLMs) with Bird's-Eye\nView (BEV) feature maps as visual inputs. Unlike conventional approaches that\nrely solely on raw visual data such as camera images, our method utilizes\nhighly compressed and informative BEV representations, which are generated by\nfusing multi-modal sensor data (e.g., camera and LiDAR) and aligning them with\nHD Maps. This unified BEV-HD Map format provides a geometrically consistent and\nrich scene description, enabling VLMs to perform accurate trajectory planning.\nExperimental results on the nuScenes dataset demonstrate 44.8% improvements in\nplanning accuracy and complete collision avoidance. Our work highlights that\nVLMs can effectively interpret processed visual representations like BEV\nfeatures, expanding their applicability beyond raw images in trajectory\nplanning."}
{"id": "2509.25352", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25352", "abs": "https://arxiv.org/abs/2509.25352", "authors": ["Itamar Mishani", "Yorai Shaoul", "Ramkumar Natarajan", "Jiaoyang Li", "Maxim Likhachev"], "title": "SRMP: Search-Based Robot Motion Planning Library", "comment": "Submitted for Publication", "summary": "Motion planning is a critical component in any robotic system. Over the\nyears, powerful tools like the Open Motion Planning Library (OMPL) have been\ndeveloped, offering numerous motion planning algorithms. However, existing\nframeworks often struggle to deliver the level of predictability and\nrepeatability demanded by high-stakes applications -- ranging from ensuring\nsafety in industrial environments to the creation of high-quality motion\ndatasets for robot learning. Complementing existing tools, we introduce SRMP\n(Search-based Robot Motion Planning), a new software framework tailored for\nrobotic manipulation. SRMP distinguishes itself by generating consistent and\nreliable trajectories, and is the first software tool to offer motion planning\nalgorithms for multi-robot manipulation tasks. SRMP easily integrates with\nmajor simulators, including MuJoCo, Sapien, Genesis, and PyBullet via a Python\nand C++ API. SRMP includes a dedicated MoveIt! plugin that enables immediate\ndeployment on robot hardware and seamless integration with existing pipelines.\nThrough extensive evaluations, we demonstrate in this paper that SRMP not only\nmeets the rigorous demands of industrial and safety-critical applications but\nalso sets a new standard for consistency in motion planning across diverse\nrobotic systems. Visit srmp.readthedocs.io for SRMP documentation and\ntutorials."}
{"id": "2509.25358", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25358", "abs": "https://arxiv.org/abs/2509.25358", "authors": ["Qianzhong Chen", "Justin Yu", "Mac Schwager", "Pieter Abbeel", "Fred Shentu", "Philipp Wu"], "title": "SARM: Stage-Aware Reward Modeling for Long Horizon Robot Manipulation", "comment": null, "summary": "Large-scale robot learning has recently shown promise for enabling robots to\nperform complex tasks by integrating perception, control, and language\nunderstanding. Yet, it struggles with long-horizon, contact-rich manipulation\nsuch as deformable object handling, where demonstration quality is\ninconsistent. Reward modeling offers a natural solution: by providing grounded\nprogress signals, it transforms noisy demonstrations into stable supervision\nthat generalizes across diverse trajectories. We introduce a stage-aware,\nvideo-based reward modeling framework that jointly predicts high-level task\nstages and fine-grained progress. Reward labels are automatically derived from\nnatural language subtask annotations, ensuring consistent progress estimation\nacross variable-length demonstrations. This design overcomes frame-index\nlabeling, which fails in variable-duration tasks like folding a T-shirt. Our\nreward model demonstrates robustness to variability, generalization to\nout-of-distribution settings, and strong utility for policy training. Building\non it, we propose Reward-Aligned Behavior Cloning (RA-BC), which filters\nhigh-quality data and reweights samples by reward. Experiments show the reward\nmodel alone outperforms baselines on validation and real robot rollouts.\nIntegrated into RA-BC, our approach achieves 83\\% success on folding T-shirts\nfrom the flattened state and 67\\% from the crumpled state -- far surpassing\nvanilla behavior cloning, which attains only 8\\% and 0\\% success. Overall, our\nresults highlight reward modeling as a key enabler for scalable,\nannotation-efficient, and robust imitation learning in long-horizon\nmanipulation."}
{"id": "2509.25364", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2509.25364", "abs": "https://arxiv.org/abs/2509.25364", "authors": ["Chengxin Zhang", "Huizhong Guo", "Zifei Wang", "Fred Feng", "Anuj Pradhan", "Shan Bao"], "title": "Assessing the Effectiveness of Driver Training Interventions in Improving Safe Engagement with Vehicle Automation Systems", "comment": "27 pages, 10 figures", "summary": "This study investigates how targeted training interventions can improve safe\ndriver interaction with vehicle automation (VA) systems, focusing on Adaptive\nCruise Control (ACC) and Lane Keeping Assist (LKA), both safety-critical\nadvanced driver assistance systems (ADAS). Effective training reduces misuse\nand enhances road safety by promoting correct knowledge and application.\n  A review of multiple automakers' owners' manuals revealed inconsistencies in\ndescribing ACC and LKA functions. Three training formats were compared: (1)\nowners' manual (OM), (2) knowledge-based (KB) with summarized operational\nguidelines and visual aids, and (3) skill-based hands-on practice in a driving\nsimulator (SIM). Thirty-six participants with no prior VA experience were\nrandomly assigned to one group. Safety-relevant outcomes - system comprehension\n(quiz scores) and real-world engagement (frequency and duration of activations)\n- were analyzed using mixed-effects and negative binomial models.\n  KB training produced the greatest improvements in comprehension of system\nlimitations, as well as safer engagement patterns. Compared with OM\nparticipants, KB participants achieved significantly higher quiz scores and\nengaged LKA and ACC more often (1.4 and 1.45 times, respectively); they also\ndemonstrated greater awareness of scenarios requiring manual control,\nindicating reduced risk of inappropriate reliance. Older drivers exhibited\nlonger activations overall, highlighting age-related differences in reliance\nand potential safety implications.\n  Short, targeted training can significantly improve safe and effective VA\nsystem use, particularly for senior drivers. These results highlight training\nas a proactive safety intervention to reduce human-automation mismatch and\nenhance system reliability in real-world driving."}
{"id": "2509.25402", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25402", "abs": "https://arxiv.org/abs/2509.25402", "authors": ["Hanlan Yang", "Itamar Mishani", "Luca Pivetti", "Zachary Kingston", "Maxim Likhachev"], "title": "Parallel Heuristic Search as Inference for Actor-Critic Reinforcement Learning Models", "comment": "Submitted for Publication", "summary": "Actor-Critic models are a class of model-free deep reinforcement learning\n(RL) algorithms that have demonstrated effectiveness across various robot\nlearning tasks. While considerable research has focused on improving training\nstability and data sampling efficiency, most deployment strategies have\nremained relatively simplistic, typically relying on direct actor policy\nrollouts. In contrast, we propose \\pachs{} (\\textit{P}arallel\n\\textit{A}ctor-\\textit{C}ritic \\textit{H}euristic \\textit{S}earch), an\nefficient parallel best-first search algorithm for inference that leverages\nboth components of the actor-critic architecture: the actor network generates\nactions, while the critic network provides cost-to-go estimates to guide the\nsearch. Two levels of parallelism are employed within the search -- actions and\ncost-to-go estimates are generated in batches by the actor and critic networks\nrespectively, and graph expansion is distributed across multiple threads. We\ndemonstrate the effectiveness of our approach in robotic manipulation tasks,\nincluding collision-free motion planning and contact-rich interactions such as\nnon-prehensile pushing. Visit p-achs.github.io for demonstrations and examples."}
{"id": "2509.25383", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2509.25383", "abs": "https://arxiv.org/abs/2509.25383", "authors": ["Joanna Sorysz", "Lars Krupp", "Dominique Nshimyimana", "Meagan B. Loerakker", "Bo Zhou", "Paul Lukowicz", "Jakob Karolus"], "title": "Beyond the Pocket: A Large-Scale International Study on User Preferences on Bodily Placements of Commercial Wearables", "comment": null, "summary": "As wearable technologies continue to evolve-becoming smaller, more powerful,\nand more deeply embedded in daily life-their integration into diverse user\ncontexts raises critical design challenges. There remains a notable gap in\nlarge-scale empirical data on where users actually wear or carry these devices\nthroughout the day, systematically examining user preferences for wearable\nplacement across varied contexts and routines. In this work, we conducted a\nquestionnaire in several countries aimed at capturing real-world habits related\nto wearable device placement. The results from n = 320 participants reveal how\nwearable usage patterns shift depending on time of day and context. We propose\na set of practical, user-centered guidelines for sensor placement and discuss\nhow they align or diverge from assumptions seen in existing ISWC work. This\nstudy contributes to ongoing efforts within the community to design more\ninclusive, adaptable, and context-aware wearable systems."}
{"id": "2509.25443", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25443", "abs": "https://arxiv.org/abs/2509.25443", "authors": ["Zewen He", "Chenyuan Chen", "Dilshod Azizov", "Yoshihiko Nakamura"], "title": "CoTaP: Compliant Task Pipeline and Reinforcement Learning of Its Controller with Compliance Modulation", "comment": "Submitted to IEEE for possible publication, under review", "summary": "Humanoid whole-body locomotion control is a critical approach for humanoid\nrobots to leverage their inherent advantages. Learning-based control methods\nderived from retargeted human motion data provide an effective means of\naddressing this issue. However, because most current human datasets lack\nmeasured force data, and learning-based robot control is largely\nposition-based, achieving appropriate compliance during interaction with real\nenvironments remains challenging. This paper presents Compliant Task Pipeline\n(CoTaP): a pipeline that leverages compliance information in the learning-based\nstructure of humanoid robots. A two-stage dual-agent reinforcement learning\nframework combined with model-based compliance control for humanoid robots is\nproposed. In the training process, first a base policy with a position-based\ncontroller is trained; then in the distillation, the upper-body policy is\ncombined with model-based compliance control, and the lower-body agent is\nguided by the base policy. In the upper-body control, adjustable task-space\ncompliance can be specified and integrated with other controllers through\ncompliance modulation on the symmetric positive definite (SPD) manifold,\nensuring system stability. We validated the feasibility of the proposed\nstrategy in simulation, primarily comparing the responses to external\ndisturbances under different compliance settings."}
{"id": "2509.25457", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.25457", "abs": "https://arxiv.org/abs/2509.25457", "authors": ["Yuhao Kang", "Junda Chen", "Liu Liu", "Kshitij Sharmad", "Martina Mazzarello", "Simone Mora", "Fabio Duarte", "Carlo Ratti"], "title": "Human vs. AI Safety Perception? Decoding Human Safety Perception with Eye-Tracking Systems, Street View Images, and Explainable AI", "comment": "28 pages, 8 figures", "summary": "The way residents perceive safety plays an important role in how they use\npublic spaces. Studies have combined large-scale street view images and\nadvanced computer vision techniques to measure the perception of safety of\nurban environments. Despite their success, such studies have often overlooked\nthe specific environmental visual factors that draw human attention and trigger\npeople's feelings of safety perceptions. In this study, we introduce a\ncomputational framework that enriches the existing body of literature on place\nperception by using eye-tracking systems with street view images and deep\nlearning approaches. Eye-tracking systems quantify not only what users are\nlooking at but also how long they engage with specific environmental elements.\nThis allows us to explore the nuance of which visual environmental factors\ninfluence human safety perceptions. We conducted our research in Helsingborg,\nSweden, where we recruited volunteers outfitted with eye-tracking systems. They\nwere asked to indicate which of the two street view images appeared safer. By\nexamining participants' focus on specific features using Mean Object Ratio in\nHighlighted Regions (MoRH) and Mean Object Hue (MoH), we identified key visual\nelements that attract human attention when perceiving safe environments. For\ninstance, certain urban infrastructure and public space features draw more\nhuman attention while the sky is less relevant in influencing safety\nperceptions. These insights offer a more human-centered understanding of which\nurban features influence human safety perceptions. Furthermore, we compared the\nreal human attention from eye-tracking systems with attention maps obtained\nfrom eXplainable Artificial Intelligence (XAI) results. Several XAI models were\ntested, and we observed that XGradCAM and EigenCAM most closely align with\nhuman safety perceptual patterns."}
{"id": "2509.25542", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25542", "abs": "https://arxiv.org/abs/2509.25542", "authors": ["Zihan Zhang", "Abhijit Ravichandran", "Pragnya Korti", "Luobin Wang", "Henrik I. Christensen"], "title": "Online Mapping for Autonomous Driving: Addressing Sensor Generalization and Dynamic Map Updates in Campus Environments", "comment": "19th International Symposium on Experimental Robotics", "summary": "High-definition (HD) maps are essential for autonomous driving, providing\nprecise information such as road boundaries, lane dividers, and crosswalks to\nenable safe and accurate navigation. However, traditional HD map generation is\nlabor-intensive, expensive, and difficult to maintain in dynamic environments.\nTo overcome these challenges, we present a real-world deployment of an online\nmapping system on a campus golf cart platform equipped with dual front cameras\nand a LiDAR sensor. Our work tackles three core challenges: (1) labeling a 3D\nHD map for campus environment; (2) integrating and generalizing the SemVecMap\nmodel onboard; and (3) incrementally generating and updating the predicted HD\nmap to capture environmental changes. By fine-tuning with campus-specific data,\nour pipeline produces accurate map predictions and supports continual updates,\ndemonstrating its practical value in real-world autonomous driving scenarios."}
{"id": "2509.25460", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2509.25460", "abs": "https://arxiv.org/abs/2509.25460", "authors": ["Jared Hwang", "Chu Li", "Hanbyul Kang", "Maryam Hosseini", "Jon E. Froehlich"], "title": "\"Where Can I Park?\" Understanding Human Perspectives and Scalably Detecting Disability Parking from Aerial Imagery", "comment": "Accepted to ASSETS'25", "summary": "Accessible parking is critical for people with disabilities (PwDs), allowing\nequitable access to destinations, independent mobility, and community\nparticipation. Despite mandates, there has been no large-scale investigation of\nthe quality or allocation of disability parking in the US nor significant\nresearch on PwD perspectives and uses of disability parking. In this paper, we\nfirst present a semi-structured interview study with 11 PwDs to advance\nunderstanding of disability parking uses, concerns, and relevant technology\ntools. We find that PwDs often adapt to disability parking challenges according\nto their personal mobility needs and value reliable, real-time accessibility\ninformation. Informed by these findings, we then introduce a new deep learning\npipeline, called AccessParkCV, and parking dataset for automatically detecting\ndisability parking and inferring quality characteristics (e.g., width) from\northorectified aerial imagery. We achieve a micro-F1=0.89 and demonstrate how\nour pipeline can support new urban analytics and end-user tools. Together, we\ncontribute new qualitative understandings of disability parking, a novel\ndetection pipeline and open dataset, and design guidelines for future tools."}
{"id": "2509.25556", "categories": ["cs.RO", "cs.SY", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.25556", "abs": "https://arxiv.org/abs/2509.25556", "authors": ["Mohammad Merati", "David Castañón"], "title": "Exhaustive-Serve-Longest Control for Multi-robot Scheduling Systems", "comment": null, "summary": "We study online task allocation for multi-robot, multi-queue systems with\nstochastic arrivals and switching delays. Time is slotted; each location can\nhost at most one robot per slot; service consumes one slot; switching between\nlocations incurs a one-slot travel delay; and arrivals are independent\nBernoulli processes. We formulate a discounted-cost Markov decision process and\npropose Exhaustive-Serve-Longest (ESL), a simple real-time policy that serves\nexhaustively when the current location is nonempty and, when idle, switches to\na longest unoccupied nonempty location, and we prove the optimality of this\npolicy. As baselines, we tune a fixed-dwell cyclic policy via a discrete-time\ndelay expression and implement a first-come-first-serve policy. Across\nserver-to-location ratios and loads, ESL consistently yields lower discounted\nholding cost and smaller mean queue lengths, with action-time fractions showing\nmore serving and restrained switching. Its simplicity and robustness make ESL a\npractical default for real-time multi-robot scheduling systems."}
{"id": "2509.25491", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2509.25491", "abs": "https://arxiv.org/abs/2509.25491", "authors": ["Nick Hagar", "Ethan Silver", "Clare Spencer", "Nicholas Diakopoulos"], "title": "LLM-Assisted News Discovery in High-Volume Information Streams: A Case Study", "comment": "Accepted to Computation + Journalism Symposium 2025", "summary": "Journalists face mounting challenges in monitoring ever-expanding digital\ninformation streams to identify newsworthy content. While traditional\nautomation tools gather information at scale, they struggle with the editorial\njudgment needed to assess newsworthiness. This paper investigates whether large\nlanguage models (LLMs) can serve as effective first-pass filters for\njournalistic monitoring. We develop a prompt-based approach encoding\njournalistic news values - timeliness, impact, controversy, and\ngeneralizability - into LLM instructions to extract and evaluate potential\nstory leads. We validate our approach across multiple models against\nexpert-annotated ground truth, then deploy a real-world monitoring pipeline\nthat processes trade press articles daily. Our evaluation reveals strong\nperformance in extracting relevant leads from source material ($F1=0.94$) and\nin coarse newsworthiness assessment ($\\pm$1 accuracy up to 92%), but it\nconsistently struggles with nuanced editorial judgments requiring beat\nexpertise. The system proves most valuable as a hybrid tool combining automated\nmonitoring with human review, successfully surfacing novel, high-value leads\nwhile filtering obvious noise. We conclude with practical recommendations for\nintegrating LLM-powered monitoring into newsroom workflows that preserves\neditorial judgment while extending journalistic capacity."}
{"id": "2509.25663", "categories": ["cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.25663", "abs": "https://arxiv.org/abs/2509.25663", "authors": ["Nathaniel Hanson", "Benjamin Pyatski", "Samuel Hibbard", "Gary Lvov", "Oscar De La Garza", "Charles DiMarzio", "Kristen L. Dorsey", "Taşkın Padır"], "title": "Field Calibration of Hyperspectral Cameras for Terrain Inference", "comment": "Accepted to IEEE Robotics & Automation Letters", "summary": "Intra-class terrain differences such as water content directly influence a\nvehicle's ability to traverse terrain, yet RGB vision systems may fail to\ndistinguish these properties. Evaluating a terrain's spectral content beyond\nred-green-blue wavelengths to the near infrared spectrum provides useful\ninformation for intra-class identification. However, accurate analysis of this\nspectral information is highly dependent on ambient illumination. We\ndemonstrate a system architecture to collect and register multi-wavelength,\nhyperspectral images from a mobile robot and describe an approach to\nreflectance calibrate cameras under varying illumination conditions. To\nshowcase the practical applications of our system, HYPER DRIVE, we demonstrate\nthe ability to calculate vegetative health indices and soil moisture content\nfrom a mobile robot platform."}
{"id": "2509.25492", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2509.25492", "abs": "https://arxiv.org/abs/2509.25492", "authors": ["Tzu-Sheng Kuo", "Sophia Liu", "Quan Ze Chen", "Joseph Seering", "Amy X. Zhang", "Haiyi Zhu", "Kenneth Holstein"], "title": "Botender: Supporting Communities in Collaboratively Designing AI Agents through Case-Based Provocations", "comment": null, "summary": "AI agents, or bots, serve important roles in online communities. However,\nthey are often designed by outsiders or a few tech-savvy members, leading to\nbots that may not align with the broader community's needs. How might\ncommunities collectively shape the behavior of community bots? We present\nBotender, a system that enables communities to collaboratively design\nLLM-powered bots without coding. With Botender, community members can directly\npropose, iterate on, and deploy custom bot behaviors tailored to community\nneeds. Botender facilitates testing and iteration on bot behavior through\ncase-based provocations: interaction scenarios generated to spark user\nreflection and discussion around desirable bot behavior. A validation study\nfound these provocations more useful than standard test cases for revealing\nimprovement opportunities and surfacing disagreements. During a five-day\ndeployment across six Discord servers, Botender supported communities in\ntailoring bot behavior to their specific needs, showcasing the usefulness of\ncase-based provocations in facilitating collaborative bot design."}
{"id": "2509.25681", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25681", "abs": "https://arxiv.org/abs/2509.25681", "authors": ["Junjie Wen", "Minjie Zhu", "Jiaming Liu", "Zhiyuan Liu", "Yicun Yang", "Linfeng Zhang", "Shanghang Zhang", "Yichen Zhu", "Yi Xu"], "title": "dVLA: Diffusion Vision-Language-Action Model with Multimodal Chain-of-Thought", "comment": "technique report", "summary": "Vision-Language-Action (VLA) models are emerging as a next-generation\nparadigm for robotics. We introduce dVLA, a diffusion-based VLA that leverages\na multimodal chain-of-thought to unify visual perception, language reasoning,\nand robotic control in a single system. dVLA jointly optimizes perception,\nlanguage understanding, and action under a single diffusion objective, enabling\nstronger cross-modal reasoning and better generalization to novel instructions\nand objects. For practical deployment, we mitigate inference latency by\nincorporating two acceleration strategies, a prefix attention mask and KV\ncaching, yielding up to around times speedup at test-time inference. We\nevaluate dVLA in both simulation and the real world: on the LIBERO benchmark,\nit achieves state-of-the-art performance with a 96.4% average success rate,\nconsistently surpassing both discrete and continuous action policies; on a real\nFranka robot, it succeeds across a diverse task suite, including a challenging\nbin-picking task that requires multi-step planning, demonstrating robust\nreal-world performance. Together, these results underscore the promise of\nunified diffusion frameworks for practical, high-performance VLA robotics."}
{"id": "2509.25499", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2509.25499", "abs": "https://arxiv.org/abs/2509.25499", "authors": ["Chayapatr Archiwaranguprok", "Awu Chen", "Sheer Karny", "Hiroshi Ishii", "Pattie Maes", "Pat Pataranutaporn"], "title": "Atlas of Human-AI Interaction (v1): An Interactive Meta-Science Platform for Large-Scale Research Literature Sensemaking", "comment": "37 pages, 6 figures, 4 tables", "summary": "Human-AI interaction researchers face an overwhelming challenge: synthesizing\ninsights from thousands of empirical studies to understand how AI impacts\npeople and inform effective design. Existing approach for literature reviews\ncluster papers by similarities, keywords or citations, missing the crucial\ncause-and-effect relationships that reveal how design decisions impact user\noutcomes. We introduce the Atlas of Human-AI Interaction, an interactive web\ninterface that provides the first systematic mapping of empirical findings\nacross 1,000+ HCI papers using LLM-powered knowledge extraction. Our approach\nidentifies causal relationships, and visualizes them through an AI-enabled\ninteractive web interface as a navigable knowledge graph. We extracted 2,037\nempirical findings, revealing research topic clusters, common themes, and\ndisconnected areas. Expert evaluation with 20 researchers revealed the system's\neffectiveness for discovering research gaps. This work demonstrates how AI can\ntransform literature synthesis itself, offering a scalable framework for\nevidence-based design, opening new possibilities for computational meta-science\nacross HCI and beyond."}
{"id": "2509.25685", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25685", "abs": "https://arxiv.org/abs/2509.25685", "authors": ["Amelie Minji Kim", "Anqi Wu", "Ye Zhao"], "title": "Hierarchical Diffusion Motion Planning with Task-Conditioned Uncertainty-Aware Priors", "comment": null, "summary": "We propose a novel hierarchical diffusion planner that embeds task and motion\nstructure directly in the noise model. Unlike standard diffusion-based planners\nthat use zero-mean, isotropic Gaussian noise, we employ a family of\ntask-conditioned structured Gaussians whose means and covariances are derived\nfrom Gaussian Process Motion Planning (GPMP): sparse, task-centric key states\nor their associated timings (or both) are treated as noisy observations to\nproduce a prior instance. We first generalize the standard diffusion process to\nbiased, non-isotropic corruption with closed-form forward and posterior\nexpressions. Building on this, our hierarchy separates prior instantiation from\ntrajectory denoising: the upper level instantiates a task-conditioned\nstructured Gaussian (mean and covariance), and the lower level denoises the\nfull trajectory under that fixed prior. Experiments on Maze2D goal-reaching and\nKUKA block stacking show improved success rates, smoother trajectories, and\nstronger task alignment compared to isotropic baselines. Ablation studies\nindicate that explicitly structuring the corruption process offers benefits\nbeyond simply conditioning the neural network. Overall, our method concentrates\nprobability mass of prior near feasible, smooth, and semantically meaningful\ntrajectories while maintaining tractability. Our project page is available at\nhttps://hta-diffusion.github.io."}
{"id": "2509.25504", "categories": ["cs.HC", "cs.AI", "cs.GR", "cs.SE", "H.5.1; D.2.2; H.5.m; D.2.m"], "pdf": "https://arxiv.org/pdf/2509.25504", "abs": "https://arxiv.org/abs/2509.25504", "authors": ["David Li", "Nels Numan", "Xun Qian", "Yanhe Chen", "Zhongyi Zhou", "Evgenii Alekseev", "Geonsun Lee", "Alex Cooper", "Min Xia", "Scott Chung", "Jeremy Nelson", "Xiuxiu Yuan", "Jolica Dias", "Tim Bettridge", "Benjamin Hersh", "Michelle Huynh", "Konrad Piascik", "Ricardo Cabello", "David Kim", "Ruofei Du"], "title": "XR Blocks: Accelerating Human-centered AI + XR Innovation", "comment": null, "summary": "We are on the cusp where Artificial Intelligence (AI) and Extended Reality\n(XR) are converging to unlock new paradigms of interactive computing. However,\na significant gap exists between the ecosystems of these two fields: while AI\nresearch and development is accelerated by mature frameworks like JAX and\nbenchmarks like LMArena, prototyping novel AI-driven XR interactions remains a\nhigh-friction process, often requiring practitioners to manually integrate\ndisparate, low-level systems for perception, rendering, and interaction. To\nbridge this gap, we present XR Blocks, a cross-platform framework designed to\naccelerate human-centered AI + XR innovation. XR Blocks strives to provide a\nmodular architecture with plug-and-play components for core abstraction in AI +\nXR: user, world, peers; interface, context, and agents. Crucially, it is\ndesigned with the mission of \"reducing frictions from idea to reality\", thus\naccelerating rapid prototyping of AI + XR apps. Built upon accessible\ntechnologies (WebXR, three.js, TensorFlow, Gemini), our toolkit lowers the\nbarrier to entry for XR creators. We demonstrate its utility through a set of\nopen-source templates, samples, and advanced demos, empowering the community to\nquickly move from concept to interactive XR prototype. Site:\nhttps://xrblocks.github.io"}
{"id": "2509.25687", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25687", "abs": "https://arxiv.org/abs/2509.25687", "authors": ["Xinda Xue", "Junjun Hu", "Minghua Luo", "Xie Shichao", "Jintao Chen", "Zixun Xie", "Quan Kuichen", "Guo Wei", "Mu Xu", "Zedong Chu"], "title": "OmniNav: A Unified Framework for Prospective Exploration and Visual-Language Navigation", "comment": null, "summary": "Embodied navigation presents a core challenge for intelligent robots,\nrequiring the comprehension of visual environments, natural language\ninstructions, and autonomous exploration. Existing models often fall short in\noffering a unified solution across diverse navigation paradigms, resulting in\nlow success rates and limited generalization. We introduce OmniNav, a unified\nframework addressing instruct-goal, object-goal, point-goal navigation, and\nfrontier-based exploration within a single architecture. Our approach features\na lightweight, low-latency policy that accurately predicts continuous-space\nwaypoints (coordinates and orientations). This policy surpasses action-chunk\nmethods in precision and supports real-world deployment at control frequencies\nup to 5 Hz. Architecturally, OmniNav employs a fast-slow system design: a fast\nmodule generates waypoints using short-horizon visual context and subtasks,\nwhile a slow module performs deliberative planning with long-horizon\nobservations and candidate frontiers to select subsequent subgoals and\nsubtasks. This collaboration enhances path efficiency and maintains trajectory\ncoherence, particularly in exploration and memory-intensive scenarios.\nCrucially, we identify that the primary bottleneck isn't merely navigation\npolicy learning, but a robust understanding of general instructions and\nobjects. To boost generalization, OmniNav integrates large-scale,\ngeneral-purpose training datasets, including those for image captioning and\nvisual recognition, into a joint multi-task regimen. This significantly\nimproves success rates and robustness. Extensive experiments confirm OmniNav's\nstate-of-the-art performance across various navigation benchmarks, with\nreal-world deployment further validating its efficacy. OmniNav provides\npractical insights for embodied navigation, charting a scalable path towards\nversatile, highly generalizable robotic intelligence."}
{"id": "2509.25513", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2509.25513", "abs": "https://arxiv.org/abs/2509.25513", "authors": ["Haoning Xue", "Yoo Jung Oh", "Xinyi Zhou", "Xinyu Zhang", "Berit Oxley"], "title": "User Prompting Strategies and ChatGPT Contextual Adaptation Shape Conversational Information-Seeking Experiences", "comment": null, "summary": "Conversational AI, such as ChatGPT, is increasingly used for information\nseeking. However, little is known about how ordinary users actually prompt and\nhow ChatGPT adapts its responses in real-world conversational information\nseeking (CIS). In this study, a nationally representative sample of 937 U.S.\nadults engaged in multi-turn CIS with ChatGPT on both controversial and\nnon-controversial topics across science, health, and policy contexts. We\nanalyzed both user prompting strategies and the communication styles of ChatGPT\nresponses. The findings revealed behavioral signals of digital divide: only\n19.1% of users employed prompting strategies, and these users were\ndisproportionately more educated and Democrat-leaning. Further, ChatGPT\ndemonstrated contextual adaptation: responses to controversial topics contain\nmore cognitive complexity and more external references than to\nnon-controversial topics. Notably, cognitively complex responses were perceived\nas less favorable but produced more positive issue-relevant attitudes. This\nstudy highlights disparities in user prompting behaviors and shows how user\nprompts and AI responses together shape information-seeking with conversational\nAI."}
{"id": "2509.25718", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25718", "abs": "https://arxiv.org/abs/2509.25718", "authors": ["Si-Cheng Wang", "Tian-Yu Xiang", "Xiao-Hu Zhou", "Mei-Jiang Gui", "Xiao-Liang Xie", "Shi-Qi Liu", "Shuang-Yi Wang", "Ao-Qun Jin", "Zeng-Guang Hou"], "title": "VLA Model Post-Training via Action-Chunked PPO and Self Behavior Cloning", "comment": null, "summary": "Reinforcement learning (RL) is a promising avenue for post-training\nvision-language-action (VLA) models, but practical deployment is hindered by\nsparse rewards and unstable training. This work mitigates these challenges by\nintroducing an action chunk based on proximal policy optimization (PPO) with\nbehavior cloning using self-collected demonstrations. Aggregating consecutive\nactions into chunks improves the temporal consistency of the policy and the\ndensity of informative feedback. In addition, an auxiliary behavior cloning\nloss is applied with a dynamically updated demonstration buffer that\ncontinually collects high-quality task trials during training. The relative\nweight between the action-chunked PPO objective and the self behavior clone\nauxiliary loss is adapted online to stabilize the post-training process.\nExperiments on the MetaWorld benchmark indicate improved performance over\nsupervised fine-tuning, achieving a high success rate (0.93) and few steps to\nsuccess (42.17). These results demonstrate the viability of RL for VLA\npost-training and help lay the groundwork for downstream VLA applications."}
{"id": "2509.25537", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2509.25537", "abs": "https://arxiv.org/abs/2509.25537", "authors": ["Kyuha Jung", "Tyler Kim", "Yunan Chen"], "title": "Healthy Lifestyles and Self-Improvement Videos on YouTube: A Thematic Analysis of Teen-Targeted Social Media Content", "comment": "Forthcoming at the American Medical Informatics Association (AMIA)\n  Annual Symposium, November 15-19, 2025", "summary": "As teenagers increasingly turn to social media for health-related\ninformation, understanding the values of teen-targeted content has become\nimportant. Although videos on healthy lifestyles and self-improvement are\ngaining popularity on social media platforms like YouTube, little is known\nabout how these videos benefit and engage with teenage viewers. To address\nthis, we conducted a thematic analysis of 44 YouTube videos and 66,901\ncomments. We found that these videos provide various advice on teenagers'\ncommon challenges, use engaging narratives for authenticity, and foster\nteen-centered communities through comments. However, a few videos also gave\nmisleading advice to adolescents that can be potentially harmful. Based on our\nfindings, we discuss design implications for creating relatable and intriguing\nsocial media content for adolescents. Additionally, we suggest ways for social\nmedia platforms to promote healthier and safer experiences for teenagers."}
{"id": "2509.25746", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25746", "abs": "https://arxiv.org/abs/2509.25746", "authors": ["Shuaijun Wang", "Haoran Zhou", "Diyun Xiang", "Yangwei You"], "title": "TacRefineNet: Tactile-Only Grasp Refinement Between Arbitrary In-Hand Object Poses", "comment": "9 pages, 9 figures", "summary": "Despite progress in both traditional dexterous grasping pipelines and recent\nVision-Language-Action (VLA) approaches, the grasp execution stage remains\nprone to pose inaccuracies, especially in long-horizon tasks, which undermines\noverall performance. To address this \"last-mile\" challenge, we propose\nTacRefineNet, a tactile-only framework that achieves fine in-hand pose\nrefinement of known objects in arbitrary target poses using multi-finger\nfingertip sensing. Our method iteratively adjusts the end-effector pose based\non tactile feedback, aligning the object to the desired configuration. We\ndesign a multi-branch policy network that fuses tactile inputs from multiple\nfingers along with proprioception to predict precise control updates. To train\nthis policy, we combine large-scale simulated data from a physics-based tactile\nmodel in MuJoCo with real-world data collected from a physical system.\nComparative experiments show that pretraining on simulated data and fine-tuning\nwith a small amount of real data significantly improves performance over\nsimulation-only training. Extensive real-world experiments validate the\neffectiveness of the method, achieving millimeter-level grasp accuracy using\nonly tactile input. To our knowledge, this is the first method to enable\narbitrary in-hand pose refinement via multi-finger tactile sensing alone.\nProject website is available at https://sites.google.com/view/tacrefinenet"}
{"id": "2509.25834", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25834", "abs": "https://arxiv.org/abs/2509.25834", "authors": ["Stephen James Krol", "Maria Teresa Llano", "Jon McCormack"], "title": "Supporting Creative Ownership through Deep Learning-Based Music Variation", "comment": "Paper Accepted NeurIPS Creative AI Track 2025", "summary": "This paper investigates the importance of personal ownership in musical AI\ndesign, examining how practising musicians can maintain creative control over\nthe compositional process. Through a four-week ecological evaluation, we\nexamined how a music variation tool, reliant on the skill of musicians,\nfunctioned within a composition setting. Our findings demonstrate that the\ndependence of the tool on the musician's ability, to provide a strong initial\nmusical input and to turn moments into complete musical ideas, promoted\nownership of both the process and artefact. Qualitative interviews further\nrevealed the importance of this personal ownership, highlighting tensions\nbetween technological capability and artistic identity. These findings provide\ninsight into how musical AI can support rather than replace human creativity,\nhighlighting the importance of designing tools that preserve the humanness of\nmusical expression."}
{"id": "2509.25747", "categories": ["cs.RO", "I.2.9"], "pdf": "https://arxiv.org/pdf/2509.25747", "abs": "https://arxiv.org/abs/2509.25747", "authors": ["Jialei Huang", "Zhaoheng Yin", "Yingdong Hu", "Shuo Wang", "Xingyu Lin", "Yang Gao"], "title": "Best of Sim and Real: Decoupled Visuomotor Manipulation via Learning Control in Simulation and Perception in Real", "comment": "10 pages, 6 figures", "summary": "Sim-to-real transfer remains a fundamental challenge in robot manipulation\ndue to the entanglement of perception and control in end-to-end learning. We\npresent a decoupled framework that learns each component where it is most\nreliable: control policies are trained in simulation with privileged state to\nmaster spatial layouts and manipulation dynamics, while perception is adapted\nonly at deployment to bridge real observations to the frozen control policy.\nOur key insight is that control strategies and action patterns are universal\nacross environments and can be learned in simulation through systematic\nrandomization, while perception is inherently domain-specific and must be\nlearned where visual observations are authentic. Unlike existing end-to-end\napproaches that require extensive real-world data, our method achieves strong\nperformance with only 10-20 real demonstrations by reducing the complex\nsim-to-real problem to a structured perception alignment task. We validate our\napproach on tabletop manipulation tasks, demonstrating superior data efficiency\nand out-of-distribution generalization compared to end-to-end baselines. The\nlearned policies successfully handle object positions and scales beyond the\ntraining distribution, confirming that decoupling perception from control\nfundamentally improves sim-to-real transfer."}
{"id": "2509.25968", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2509.25968", "abs": "https://arxiv.org/abs/2509.25968", "authors": ["Chinatsu Ozawa", "Tatsuya Minagawa", "Yoichi Ochiai"], "title": "Photographic Conviviality: A Synchronic and Symbiotic Photographic Experience through a Body Paint Workshop", "comment": null, "summary": "This study explores \"Photo Tattooing,\" merging photography and body\nornamentation, and introduces the concept of \"Photographic Conviviality.\" Using\nour instant camera that prints images onto mesh screens for immediate body art,\nwe examine how this integration affects personal expression and challenges\ntraditional photography. Workshops revealed that this fusion redefines\nphotography's role, fostering intimacy and shared experiences, and opens new\navenues for self-expression by transforming static images into dynamic,\ncorporeal experiences."}
{"id": "2509.25756", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25756", "abs": "https://arxiv.org/abs/2509.25756", "authors": ["Yixian Zhang", "Shu'ang Yu", "Tonghe Zhang", "Mo Guang", "Haojia Hui", "Kaiwen Long", "Yu Wang", "Chao Yu", "Wenbo Ding"], "title": "SAC Flow: Sample-Efficient Reinforcement Learning of Flow-Based Policies via Velocity-Reparameterized Sequential Modeling", "comment": null, "summary": "Training expressive flow-based policies with off-policy reinforcement\nlearning is notoriously unstable due to gradient pathologies in the multi-step\naction sampling process. We trace this instability to a fundamental connection:\nthe flow rollout is algebraically equivalent to a residual recurrent\ncomputation, making it susceptible to the same vanishing and exploding\ngradients as RNNs. To address this, we reparameterize the velocity network\nusing principles from modern sequential models, introducing two stable\narchitectures: Flow-G, which incorporates a gated velocity, and Flow-T, which\nutilizes a decoded velocity. We then develop a practical SAC-based algorithm,\nenabled by a noise-augmented rollout, that facilitates direct end-to-end\ntraining of these policies. Our approach supports both from-scratch and\noffline-to-online learning and achieves state-of-the-art performance on\ncontinuous control and robotic manipulation benchmarks, eliminating the need\nfor common workarounds like policy distillation or surrogate objectives."}
{"id": "2509.26210", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2509.26210", "abs": "https://arxiv.org/abs/2509.26210", "authors": ["Jiugeng Sun", "Rita Sevastjanova", "Sina Ahmadi", "Rico Sennrich", "Mennatallah El-Assady"], "title": "Dia-Lingle: A Gamified Interface for Dialectal Data Collection", "comment": null, "summary": "Dialects suffer from the scarcity of computational textual resources as they\nexist predominantly in spoken rather than written form and exhibit remarkable\ngeographical diversity. Collecting dialect data and subsequently integrating it\ninto current language technologies present significant obstacles. Gamification\nhas been proven to facilitate remote data collection processes with great ease\nand on a substantially wider scale. This paper introduces Dia-Lingle, a\ngamified interface aimed to improve and facilitate dialectal data collection\ntasks such as corpus expansion and dialect labelling. The platform features two\nkey components: the first challenges users to rewrite sentences in their\ndialects, identifies them through a classifier and solicits feedback, and the\nother one asks users to match sentences to their geographical locations.\nDia-Lingle combines active learning with gamified difficulty levels,\nstrategically encouraging prolonged user engagement while efficiently enriching\nthe dialect corpus. Usability evaluation shows that our interface demonstrates\nhigh levels of user satisfaction. We provide the link to Dia-Lingle:\nhttps://dia-lingle.ivia.ch/, and demo video: https://youtu.be/0QyJsB8ym64."}
{"id": "2509.25822", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25822", "abs": "https://arxiv.org/abs/2509.25822", "authors": ["Jing Wang", "Weiting Peng", "Jing Tang", "Zeyu Gong", "Xihua Wang", "Bo Tao", "Li Cheng"], "title": "Act to See, See to Act: Diffusion-Driven Perception-Action Interplay for Adaptive Policies", "comment": "42 pages, 17 figures, 39th Conference on Neural Information\n  Processing Systems (NeurIPS 2025)", "summary": "Existing imitation learning methods decouple perception and action, which\noverlooks the causal reciprocity between sensory representations and action\nexecution that humans naturally leverage for adaptive behaviors. To bridge this\ngap, we introduce Action--Guided Diffusion Policy (DP--AG), a unified\nrepresentation learning that explicitly models a dynamic interplay between\nperception and action through probabilistic latent dynamics. DP--AG encodes\nlatent observations into a Gaussian posterior via variational inference and\nevolves them using an action-guided SDE, where the Vector-Jacobian Product\n(VJP) of the diffusion policy's noise predictions serves as a structured\nstochastic force driving latent updates. To promote bidirectional learning\nbetween perception and action, we introduce a cycle--consistent contrastive\nloss that organizes the gradient flow of the noise predictor into a coherent\nperception--action loop, enforcing mutually consistent transitions in both\nlatent updates and action refinements. Theoretically, we derive a variational\nlower bound for the action-guided SDE, and prove that the contrastive objective\nenhances continuity in both latent and action trajectories. Empirically, DP--AG\nsignificantly outperforms state--of--the--art methods across simulation\nbenchmarks and real-world UR5 manipulation tasks. As a result, our DP--AG\noffers a promising step toward bridging biological adaptability and artificial\npolicy learning."}
{"id": "2509.26466", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2509.26466", "abs": "https://arxiv.org/abs/2509.26466", "authors": ["Naaz Sibia", "Valeria Ramirez Osorio", "Jessica Wen", "Rutwa Engineer", "Angela Zavaleta Bernuy", "Andrew Petersen", "Michael Liut", "Carolina Nobre"], "title": "From Code to Concept: Evaluating Multiple Coordinated Views in Introductory Programming", "comment": null, "summary": "Novice programmers often struggle to understand how code executes and to form\nthe abstract mental models necessary for effective problem-solving, challenges\nthat are amplified in large, diverse introductory courses where students'\nbackgrounds, language proficiencies, and prior experiences vary widely. This\nstudy examines whether interactive, multi-representational visualizations,\ncombining synchronized code views, memory diagrams, and conceptual analogies,\ncan help manage cognitive load and foster engagement more effectively than\nsingle-visual or text-only approaches. Over a 12-week deployment in a\nhigh-enrolment introductory Python course (N = 829), students who relied solely\non text-based explanations reported significantly higher immediate mental\neffort than those using visual aids, although overall cognitive load did not\ndiffer significantly among conditions. The multi-representational approach\nconsistently yielded higher engagement than both single-visual and text-only\nmethods. Usage logs indicated that learners' interaction patterns varied with\ntopic complexity, and predictive modelling suggested that early experiences of\nhigh cognitive load were associated with lower longer-term perceptions of\nclarity and helpfulness. Individual differences, including language proficiency\nand prior programming experience, moderated these patterns. By integrating\nmultiple external representations with scaffolded support adapted to diverse\nlearner profiles, our findings highlight design considerations for creating\nvisualization tools that more effectively support novices learning to program."}
{"id": "2509.25852", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25852", "abs": "https://arxiv.org/abs/2509.25852", "authors": ["Zitong Bo", "Yue Hu", "Jinming Ma", "Mingliang Zhou", "Junhui Yin", "Yachen Kang", "Yuqi Liu", "Tong Wu", "Diyun Xiang", "Hao Chen"], "title": "Reinforced Embodied Planning with Verifiable Reward for Real-World Robotic Manipulation", "comment": null, "summary": "Enabling robots to execute long-horizon manipulation tasks from free-form\nlanguage instructions remains a fundamental challenge in embodied AI. While\nvision-language models (VLMs) have shown promise as high-level planners, their\ndeployment in the real world is hindered by two gaps: (i) the scarcity of\nlarge-scale, sequential manipulation data that couples natural language with\nmulti-step action plans, and (ii) the absence of dense, interpretable rewards\nfor fine-tuning VLMs on planning objectives. To address these issues, we\npropose REVER, a framework that empowers VLMs to generate and validate\nlong-horizon manipulation plans from natural language instructions in\nreal-world scenarios. Under REVER we train and release RoboFarseer, a VLM\nincentivized to emit chain-of-thought that perform temporal and spatial\nreasoning, ensuring physically plausible and logically coherent plans. To\nobtain training data, we leverage the Universal Manipulation Interface\nframework to capture hardware-agnostic demonstrations of atomic skills. An\nautomated annotation engine converts each demonstration into\nvision-instruction-plan triplet. We introduce a verifiable reward that scores\nthe generated plan by its ordered bipartite matching overlap with the\nground-truth skill sequence. At run time, the fine-tuned VLM functions both as\na planner and as a monitor, verifying step-wise completion. RoboFarseer matches\nor exceeds the performance of proprietary models that are orders of magnitude\nlarger, while on open-ended planning it surpasses the best baseline by more\nthan 40%. In real-world, long-horizon tasks, the complete system boosts overall\nsuccess by roughly 60% compared with the same low-level controller without the\nplanner. We will open-source both the dataset and the trained model upon\npublication."}
{"id": "2509.26557", "categories": ["cs.HC", "H.5.2"], "pdf": "https://arxiv.org/pdf/2509.26557", "abs": "https://arxiv.org/abs/2509.26557", "authors": ["Litao Yan", "Andrew Head", "Ken Milne", "Vu Le", "Sumit Gulwani", "Chris Parnin", "Emerson Murphy-Hill"], "title": "The Invisible Mentor: Inferring User Actions from Screen Recordings to Recommend Better Workflows", "comment": "15 pages, 6 figures", "summary": "Many users struggle to notice when a more efficient workflow exists in\nfeature-rich tools like Excel. Existing AI assistants offer help only after\nusers describe their goals or problems, which can be effortful and imprecise.\nWe present InvisibleMentor, a system that turns screen recordings of task\ncompletion into vision-grounded reflections on tasks. It detects issues such as\nrepetitive edits and recommends more efficient alternatives based on observed\nbehavior. Unlike prior systems that rely on logs, APIs, or user prompts,\nInvisibleMentor operates directly on screen recordings. It uses a two-stage\npipeline: a vision-language model reconstructs actions and context, and a\nlanguage model generates structured, high-fidelity suggestions. In evaluation,\nInvisibleMentor accurately identified inefficient workflows, and participants\nfound its suggestions more actionable, tailored, and more helpful for learning\nand improvement compared to a prompt-based spreadsheet assistant."}
{"id": "2509.25945", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25945", "abs": "https://arxiv.org/abs/2509.25945", "authors": ["Valentin Yuryev", "Max Polzin", "Josie Hughes"], "title": "State Estimation for Compliant and Morphologically Adaptive Robots", "comment": "8 pages, 10 figures, 1 table, submitted to ICRA 2026", "summary": "Locomotion robots with active or passive compliance can show robustness to\nuncertain scenarios, which can be promising for agricultural, research and\nenvironmental industries. However, state estimation for these robots is\nchallenging due to the lack of rigid-body assumptions and kinematic changes\nfrom morphing. We propose a method to estimate typical rigid-body states\nalongside compliance-related states, such as soft robot shape in different\nmorphologies and locomotion modes. Our neural network-based state estimator\nuses a history of states and a mechanism to directly influence unreliable\nsensors. We test our framework on the GOAT platform, a robot capable of passive\ncompliance and active morphing for extreme outdoor terrain. The network is\ntrained on motion capture data in a novel compliance-centric frame that\naccounts for morphing-related states. Our method predicts shape-related\nmeasurements within 4.2% of the robot's size, velocities within 6.3% and 2.4%\nof the top linear and angular speeds, respectively, and orientation within 1.5\ndegrees. We also demonstrate a 300% increase in travel range during a motor\nmalfunction when using our estimator for closed-loop autonomous outdoor\noperation."}
{"id": "2509.26593", "categories": ["cs.HC", "H.4, K.7"], "pdf": "https://arxiv.org/pdf/2509.26593", "abs": "https://arxiv.org/abs/2509.26593", "authors": ["Kichang Lee"], "title": "Exploring Large Language Model as an Interactive Sports Coach: Lessons from a Single-Subject Half Marathon Preparation", "comment": "23 pages, 21 figures", "summary": "Large language models (LLMs) are emerging as everyday assistants, but their\nrole as longitudinal virtual coaches is underexplored. This two-month single\nsubject case study documents LLM guided half marathon preparation\n(July-September 2025). Using text based interactions and consumer app logs, the\nLLM acted as planner, explainer, and occasional motivator. Performance improved\nfrom sustaining 2 km at 7min 54sec per km to completing 21.1 km at 6min 30sec\nper km, with gains in cadence, pace HR coupling, and efficiency index trends.\nWhile causal attribution is limited without a control, outcomes demonstrate\nsafe, measurable progress. At the same time, gaps were evident, no realtime\nsensor integration, text only feedback, motivation support that was user\ninitiated, and limited personalization or safety guardrails. We propose design\nrequirements for next generation systems, persistent athlete models with\nexplicit guardrails, multimodal on device sensing, audio, haptic, visual\nfeedback, proactive motivation scaffolds, and privacy-preserving\npersonalization. This study offers grounded evidence and a design agenda for\nevolving LLMs from retrospective advisors to closed-loop coaching companions."}
{"id": "2509.25951", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25951", "abs": "https://arxiv.org/abs/2509.25951", "authors": ["ChunPing Lam", "Xiangjia Chen", "Chenming Wu", "Hao Chen", "Binzhi Sun", "Guoxin Fang", "Charlie C. L. Wang", "Chengkai Dai", "Yeung Yam"], "title": "Towards Intuitive Human-Robot Interaction through Embodied Gesture-Driven Control with Woven Tactile Skins", "comment": null, "summary": "This paper presents a novel human-robot interaction (HRI) framework that\nenables intuitive gesture-driven control through a capacitance-based woven\ntactile skin. Unlike conventional interfaces that rely on panels or handheld\ndevices, the woven tactile skin integrates seamlessly with curved robot\nsurfaces, enabling embodied interaction and narrowing the gap between human\nintent and robot response. Its woven design combines fabric-like flexibility\nwith structural stability and dense multi-channel sensing through the\ninterlaced conductive threads. Building on this capability, we define a\ngesture-action mapping of 14 single- and multi-touch gestures that cover\nrepresentative robot commands, including task-space motion and auxiliary\nfunctions. A lightweight convolution-transformer model designed for gesture\nrecognition in real time achieves an accuracy of near-100%, outperforming prior\nbaseline approaches. Experiments on robot arm tasks, including pick-and-place\nand pouring, demonstrate that our system reduces task completion time by up to\n57% compared with keyboard panels and teach pendants. Overall, our proposed\nframework demonstrates a practical pathway toward more natural and efficient\nembodied HRI."}
{"id": "2509.25966", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25966", "abs": "https://arxiv.org/abs/2509.25966", "authors": ["Peilong Han", "Fan Jia", "Min Zhang", "Yutao Qiu", "Hongyao Tang", "Yan Zheng", "Tiancai Wang", "Jianye Hao"], "title": "MUVLA: Learning to Explore Object Navigation via Map Understanding", "comment": null, "summary": "In this paper, we present MUVLA, a Map Understanding Vision-Language-Action\nmodel tailored for object navigation. It leverages semantic map abstractions to\nunify and structure historical information, encoding spatial context in a\ncompact and consistent form. MUVLA takes the current and history observations,\nas well as the semantic map, as inputs and predicts the action sequence based\non the description of goal object. Furthermore, it amplifies supervision\nthrough reward-guided return modeling based on dense short-horizon progress\nsignals, enabling the model to develop a detailed understanding of action value\nfor reward maximization. MUVLA employs a three-stage training pipeline:\nlearning map-level spatial understanding, imitating behaviors from\nmixed-quality demonstrations, and reward amplification. This strategy allows\nMUVLA to unify diverse demonstrations into a robust spatial representation and\ngenerate more rational exploration strategies. Experiments on HM3D and Gibson\nbenchmarks demonstrate that MUVLA achieves great generalization and learns\neffective exploration behaviors even from low-quality or partially successful\ntrajectories."}
{"id": "2509.25984", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25984", "abs": "https://arxiv.org/abs/2509.25984", "authors": ["Shengpeng Wang", "Yulong Xie", "Qing Liao", "Wei Wang"], "title": "S$^3$E: Self-Supervised State Estimation for Radar-Inertial System", "comment": null, "summary": "Millimeter-wave radar for state estimation is gaining significant attention\nfor its affordability and reliability in harsh conditions. Existing\nlocalization solutions typically rely on post-processed radar point clouds as\nlandmark points. Nonetheless, the inherent sparsity of radar point clouds,\nghost points from multi-path effects, and limited angle resolution in\nsingle-chirp radar severely degrade state estimation performance. To address\nthese issues, we propose S$^3$E, a \\textbf{S}elf-\\textbf{S}upervised\n\\textbf{S}tate \\textbf{E}stimator that employs more richly informative radar\nsignal spectra to bypass sparse points and fuses complementary inertial\ninformation to achieve accurate localization. S$^3$E fully explores the\nassociation between \\textit{exteroceptive} radar and \\textit{proprioceptive}\ninertial sensor to achieve complementary benefits. To deal with limited angle\nresolution, we introduce a novel cross-fusion technique that enhances spatial\nstructure information by exploiting subtle rotational shift correlations across\nheterogeneous data. The experimental results demonstrate our method achieves\nrobust and accurate performance without relying on localization ground truth\nsupervision. To the best of our knowledge, this is the first attempt to achieve\nstate estimation by fusing radar spectra and inertial data in a complementary\nself-supervised manner."}
{"id": "2509.25986", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25986", "abs": "https://arxiv.org/abs/2509.25986", "authors": ["Elisabetta Zibetti", "Sureya Waheed Palmer", "Rebecca Stower", "Salvatore M Anzalone"], "title": "Emotionally Expressive Robots: Implications for Children's Behavior toward Robot", "comment": null, "summary": "The growing development of robots with artificial emotional expressiveness\nraises important questions about their persuasive potential in children's\nbehavior. While research highlights the pragmatic value of emotional\nexpressiveness in human social communication, the extent to which robotic\nexpressiveness can or should influence empathic responses in children is\ngrounds for debate. In a pilot study with 22 children (aged 7-11) we begin to\nexplore the ways in which different levels of embodied expressiveness (body\nonly, face only, body and face) of two basic emotions (happiness and sadness)\ndisplayed by an anthropomorphic robot (QTRobot) might modify children's\nbehavior in a child-robot cooperative turn-taking game. We observed that\nchildren aligned their behavior to the robot's inferred emotional state.\nHowever, higher levels of expressiveness did not result in increased alignment.\nThe preliminary results reported here provide a starting point for reflecting\non robotic expressiveness and its role in shaping children's social-emotional\nbehavior toward robots as social peers in the near future."}
{"id": "2509.25999", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25999", "abs": "https://arxiv.org/abs/2509.25999", "authors": ["Yann de Mont-Marin", "Louis Montaut", "Jean Ponce", "Martial Hebert", "Justin Carpentier"], "title": "On the Conic Complementarity of Planar Contacts", "comment": null, "summary": "We present a unifying theoretical result that connects two foundational\nprinciples in robotics: the Signorini law for point contacts, which underpins\nmany simulation methods for preventing object interpenetration, and the center\nof pressure (also known as the zero-moment point), a key concept used in, for\ninstance, optimization-based locomotion control. Our contribution is the planar\nSignorini condition, a conic complementarity formulation that models general\nplanar contacts between rigid bodies. We prove that this formulation is\nequivalent to enforcing the punctual Signorini law across an entire contact\nsurface, thereby bridging the gap between discrete and continuous contact\nmodels. A geometric interpretation reveals that the framework naturally\ncaptures three physical regimes -sticking, separating, and tilting-within a\nunified complementarity structure. This leads to a principled extension of the\nclassical center of pressure, which we refer to as the extended center of\npressure. By establishing this connection, our work provides a mathematically\nconsistent and computationally tractable foundation for handling planar\ncontacts, with implications for both the accurate simulation of contact\ndynamics and the design of advanced control and optimization algorithms in\nlocomotion and manipulation."}
{"id": "2509.26050", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.26050", "abs": "https://arxiv.org/abs/2509.26050", "authors": ["Shaoli Hu", "Shizhe Zhao", "Zhongqiang Ren"], "title": "Conflict-Based Search and Prioritized Planning for Multi-Agent Path Finding Among Movable Obstacles", "comment": null, "summary": "This paper investigates Multi-Agent Path Finding Among Movable Obstacles\n(M-PAMO), which seeks collision-free paths for multiple agents from their start\nto goal locations among static and movable obstacles. M-PAMO arises in\nlogistics and warehouses where mobile robots are among unexpected movable\nobjects. Although Multi-Agent Path Finding (MAPF) and single-agent Path\nplanning Among Movable Obstacles (PAMO) were both studied, M-PAMO remains\nunder-explored. Movable obstacles lead to new fundamental challenges as the\nstate space, which includes both agents and movable obstacles, grows\nexponentially with respect to the number of agents and movable obstacles. In\nparticular, movable obstacles often closely couple agents together spatially\nand temporally. This paper makes a first attempt to adapt and fuse the popular\nConflict-Based Search (CBS) and Prioritized Planning (PP) for MAPF, and a\nrecent single-agent PAMO planner called PAMO*, together to address M-PAMO. We\ncompare their performance with up to 20 agents and hundreds of movable\nobstacles, and show the pros and cons of these approaches."}
{"id": "2509.26082", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.26082", "abs": "https://arxiv.org/abs/2509.26082", "authors": ["Tianyi Jin", "Melya Boukheddimi", "Rohit Kumar", "Gabriele Fadini", "Frank Kirchner"], "title": "Evolutionary Continuous Adaptive RL-Powered Co-Design for Humanoid Chin-Up Performance", "comment": null, "summary": "Humanoid robots have seen significant advancements in both design and\ncontrol, with a growing emphasis on integrating these aspects to enhance\noverall performance. Traditionally, robot design has followed a sequential\nprocess, where control algorithms are developed after the hardware is\nfinalized. However, this can be myopic and prevent robots to fully exploit\ntheir hardware capabilities. Recent approaches advocate for co-design,\noptimizing both design and control in parallel to maximize robotic\ncapabilities. This paper presents the Evolutionary Continuous Adaptive RL-based\nCo-Design (EA-CoRL) framework, which combines reinforcement learning (RL) with\nevolutionary strategies to enable continuous adaptation of the control policy\nto the hardware. EA-CoRL comprises two key components: Design Evolution, which\nexplores the hardware choices using an evolutionary algorithm to identify\nefficient configurations, and Policy Continuous Adaptation, which fine-tunes a\ntask-specific control policy across evolving designs to maximize performance\nrewards. We evaluate EA-CoRL by co-designing the actuators (gear ratios) and\ncontrol policy of the RH5 humanoid for a highly dynamic chin-up task,\npreviously unfeasible due to actuator limitations. Comparative results against\nstate-of-the-art RL-based co-design methods show that EA-CoRL achieves higher\nfitness score and broader design space exploration, highlighting the critical\nrole of continuous policy adaptation in robot co-design."}
{"id": "2509.26106", "categories": ["cs.RO", "68T40, 68T05", "I.2.9; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.26106", "abs": "https://arxiv.org/abs/2509.26106", "authors": ["Nakhul Kalaivanan", "Senthil Arumugam Muthukumaraswamy", "Girish Balasubramanian"], "title": "Autonomous Multi-Robot Infrastructure for AI-Enabled Healthcare Delivery and Diagnostics", "comment": "11 pages, 5 figures, MSc dissertation submission draft, prepared for\n  conference/journal consideration", "summary": "This research presents a multi-robot system for inpatient care, designed\nusing swarm intelligence principles and incorporating wearable health sensors,\nRF-based communication, and AI-driven decision support. Within a simulated\nhospital environment, the system adopts a leader-follower swarm configuration\nto perform patient monitoring, medicine delivery, and emergency assistance. Due\nto ethical constraints, live patient trials were not conducted; instead,\nvalidation was carried out through controlled self-testing with wearable\nsensors. The Leader Robot acquires key physiological parameters, including\ntemperature, SpO2, heart rate, and fall detection, and coordinates other robots\nwhen required. The Assistant Robot patrols corridors for medicine delivery,\nwhile a robotic arm provides direct drug administration. The swarm-inspired\nleader-follower strategy enhanced communication reliability and ensured\ncontinuous monitoring, including automated email alerts to healthcare staff.\nThe system hardware was implemented using Arduino, Raspberry Pi, NRF24L01 RF\nmodules, and a HuskyLens AI camera. Experimental evaluation showed an overall\nsensor accuracy above 94%, a 92% task-level success rate, and a 96%\ncommunication reliability rate, demonstrating system robustness. Furthermore,\nthe AI-enabled decision support was able to provide early warnings of abnormal\nhealth conditions, highlighting the potential of the system as a cost-effective\nsolution for hospital automation and patient safety."}
{"id": "2509.26121", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.26121", "abs": "https://arxiv.org/abs/2509.26121", "authors": ["Julian Valdez", "Ignacio Torroba", "John Folkesson", "Ivan Stenius"], "title": "Side Scan Sonar-based SLAM for Autonomous Algae Farm Monitoring", "comment": null, "summary": "The transition of seaweed farming to an alternative food source on an\nindustrial scale relies on automating its processes through smart farming,\nequivalent to land agriculture. Key to this process are autonomous underwater\nvehicles (AUVs) via their capacity to automate crop and structural inspections.\nHowever, the current bottleneck for their deployment is ensuring safe\nnavigation within farms, which requires an accurate, online estimate of the AUV\npose and map of the infrastructure. To enable this, we propose an efficient\nside scan sonar-based (SSS) simultaneous localization and mapping (SLAM)\nframework that exploits the geometry of kelp farms via modeling structural\nropes in the back-end as sequences of individual landmarks from each SSS ping\ndetection, instead of combining detections into elongated representations. Our\nmethod outperforms state of the art solutions in hardware in the loop (HIL)\nexperiments on a real AUV survey in a kelp farm. The framework and dataset can\nbe found at https://github.com/julRusVal/sss_farm_slam."}
{"id": "2509.26222", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.26222", "abs": "https://arxiv.org/abs/2509.26222", "authors": ["Yizhe Liu", "Han Zhang"], "title": "Terrain-Awared LiDAR-Inertial Odometry for Legged-Wheel Robots Based on Radial Basis Function Approximation", "comment": null, "summary": "An accurate odometry is essential for legged-wheel robots operating in\nunstructured terrains such as bumpy roads and staircases. Existing methods\noften suffer from pose drift due to their ignorance of terrain geometry. We\npropose a terrain-awared LiDAR-Inertial odometry (LIO) framework that\napproximates the terrain using Radial Basis Functions (RBF) whose centers are\nadaptively selected and weights are recursively updated. The resulting smooth\nterrain manifold enables ``soft constraints\" that regularize the odometry\noptimization and mitigates the $z$-axis pose drift under abrupt elevation\nchanges during robot's maneuver. To ensure the LIO's real-time performance, we\nfurther evaluate the RBF-related terms and calculate the inverse of the sparse\nkernel matrix with GPU parallelization. Experiments on unstructured terrains\ndemonstrate that our method achieves higher localization accuracy than the\nstate-of-the-art baselines, especially in the scenarios that have continuous\nheight changes or sparse features when abrupt height changes occur."}
{"id": "2509.26236", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.26236", "abs": "https://arxiv.org/abs/2509.26236", "authors": ["Benjamin A. Richardson", "Felix Grüninger", "Lukas Mack", "Joerg Stueckler", "Katherine J. Kuchenbecker"], "title": "ISyHand: A Dexterous Multi-finger Robot Hand with an Articulated Palm", "comment": "Accepted at IEEE Humanoids 2025", "summary": "The rapid increase in the development of humanoid robots and customized\nmanufacturing solutions has brought dexterous manipulation to the forefront of\nmodern robotics. Over the past decade, several expensive dexterous hands have\ncome to market, but advances in hardware design, particularly in servo motors\nand 3D printing, have recently facilitated an explosion of cheaper open-source\nhands. Most hands are anthropomorphic to allow use of standard human tools, and\nattempts to increase dexterity often sacrifice anthropomorphism. We introduce\nthe open-source ISyHand (pronounced easy-hand), a highly dexterous, low-cost,\neasy-to-manufacture, on-joint servo-driven robot hand. Our hand uses\noff-the-shelf Dynamixel motors, fasteners, and 3D-printed parts, can be\nassembled within four hours, and has a total material cost of about 1,300 USD.\nThe ISyHands's unique articulated-palm design increases overall dexterity with\nonly a modest sacrifice in anthropomorphism. To demonstrate the utility of the\narticulated palm, we use reinforcement learning in simulation to train the hand\nto perform a classical in-hand manipulation task: cube reorientation. Our\nnovel, systematic experiments show that the simulated ISyHand outperforms the\ntwo most comparable hands in early training phases, that all three perform\nsimilarly well after policy convergence, and that the ISyHand significantly\noutperforms a fixed-palm version of its own design. Additionally, we deploy a\npolicy trained on cube reorientation on the real hand, demonstrating its\nability to perform real-world dexterous manipulation."}
{"id": "2509.26308", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.26308", "abs": "https://arxiv.org/abs/2509.26308", "authors": ["Niklas Grambow", "Lisa-Marie Fenner", "Felipe Kempkes", "Philip Hotz", "Dingyuan Wan", "Jörg Krüger", "Kevin Haninger"], "title": "Anomaly detection for generic failure monitoring in robotic assembly, screwing and manipulation", "comment": null, "summary": "Out-of-distribution states in robot manipulation often lead to unpredictable\nrobot behavior or task failure, limiting success rates and increasing risk of\ndamage. Anomaly detection (AD) can identify deviations from expected patterns\nin data, which can be used to trigger failsafe behaviors and recovery\nstrategies. Prior work has applied data-driven AD to time series data in\nspecific robotic tasks, but its transferability across control strategies and\ntask types has not been shown. Leveraging time series data, such as\nforce/torque signals, allows to directly capture robot-environment\ninteractions, crucial for manipulation and online failure detection. Their\nbroad availability, high sampling rates, and low dimensionality enable high\ntemporal resolution and efficient processing. As robotic tasks can have widely\nsignal characteristics and requirements, AD methods which can be applied in the\nsame way to a wide range of tasks is needed, ideally with good data efficiency.\nWe examine three industrial robotic tasks, each presenting several anomalies.\nTest scenarios in robotic cabling, screwing, and sanding are built, and\nmultimodal time series data is gathered. Several autoencoder-based methods are\ncompared, evaluating generalization across tasks and control methods (diffusion\npolicy, position, and impedance control). This allows us to validate the\nintegration of AD in complex tasks involving tighter tolerances and variation\nfrom both the robot and its environment. Additionally, we evaluate data\nefficiency, detection latency, and task characteristics which support robust\ndetection. The results indicate reliable detection with AUROC exceeding 0.93 in\nfailures in the cabling and screwing task, such as incorrect or misaligned\nparts and obstructed targets. In the polishing task, only severe failures were\nreliably detected, while more subtle failure types remained undetected."}
{"id": "2509.26324", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.26324", "abs": "https://arxiv.org/abs/2509.26324", "authors": ["Ruiyang Wang", "Haolun Tsu", "David Hunt", "Shaocheng Luo", "Jiwoo Kim", "Miroslav Pajic"], "title": "LLM-MCoX: Large Language Model-based Multi-robot Coordinated Exploration and Search", "comment": null, "summary": "Autonomous exploration and object search in unknown indoor environments\nremain challenging for multi-robot systems (MRS). Traditional approaches often\nrely on greedy frontier assignment strategies with limited inter-robot\ncoordination. In this work, we introduce LLM-MCoX (LLM-based Multi-robot\nCoordinated Exploration and Search), a novel framework that leverages Large\nLanguage Models (LLMs) for intelligent coordination of both homogeneous and\nheterogeneous robot teams tasked with efficient exploration and target object\nsearch. Our approach combines real-time LiDAR scan processing for frontier\ncluster extraction and doorway detection with multimodal LLM reasoning (e.g.,\nGPT-4o) to generate coordinated waypoint assignments based on shared\nenvironment maps and robot states. LLM-MCoX demonstrates superior performance\ncompared to existing methods, including greedy and Voronoi-based planners,\nachieving 22.7% faster exploration times and 50% improved search efficiency in\nlarge environments with 6 robots. Notably, LLM-MCoX enables natural\nlanguage-based object search capabilities, allowing human operators to provide\nhigh-level semantic guidance that traditional algorithms cannot interpret."}
{"id": "2509.26339", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.26339", "abs": "https://arxiv.org/abs/2509.26339", "authors": ["Eric R. Damm", "Thomas M. Howard"], "title": "Kinodynamic Motion Planning for Mobile Robot Navigation across Inconsistent World Models", "comment": "Presented at the Robotics: Science and Systems (RSS) 2025 Workshop on\n  Resilient Off-road Autonomous Robotics (ROAR)", "summary": "Mobile ground robots lacking prior knowledge of an environment must rely on\nsensor data to develop a model of their surroundings. In these scenarios,\nconsistent identification of obstacles and terrain features can be difficult\ndue to noise and algorithmic shortcomings, which can make it difficult for\nmotion planning systems to generate safe motions. One particular difficulty to\novercome is when regions of the cost map switch between being marked as\nobstacles and free space through successive planning cycles. One potential\nsolution to this, which we refer to as Valid in Every Hypothesis (VEH), is for\nthe planning system to plan motions that are guaranteed to be safe through a\nhistory of world models. Another approach is to track a history of world\nmodels, and adjust node costs according to the potential penalty of needing to\nreroute around previously hazardous areas. This work discusses three major\niterations on this idea. The first iteration, called PEH, invokes a sub-search\nfor every node expansion that crosses through a divergence point in the world\nmodels. The second and third iterations, called GEH and GEGRH respectively,\ndefer the sub-search until after an edge expands into the goal region. GEGRH\nuses an additional step to revise the graph based on divergent nodes in each\nworld. Initial results showed that, although PEH and GEH find more optimistic\nsolutions than VEH, they are unable to generate solutions in less than\none-second, which exceeds our requirements for field deployment. Analysis of\nresults from a field experiment in an unstructured, off-road environment on a\nClearpath Robotics Warthog UGV indicate that GEGRH finds lower cost\ntrajectories and has faster average planning times than VEH. Compared to\nsingle-hypothesis (SH) search, where only the latest world model is considered,\nGEGRH generates more conservative plans with a small increase in average\nplanning time."}
{"id": "2509.26375", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.26375", "abs": "https://arxiv.org/abs/2509.26375", "authors": ["Zichao Shen", "Chen Gao", "Jiaqi Yuan", "Tianchen Zhu", "Xingcheng Fu", "Qingyun Sun"], "title": "SDA-PLANNER: State-Dependency Aware Adaptive Planner for Embodied Task Planning", "comment": null, "summary": "Embodied task planning requires agents to produce executable actions in a\nclose-loop manner within the environment. With progressively improving\ncapabilities of LLMs in task decomposition, planning, and generalization,\ncurrent embodied task planning methods adopt LLM-based architecture.However,\nexisting LLM-based planners remain limited in three aspects, i.e., fixed\nplanning paradigms, lack of action sequence constraints, and error-agnostic. In\nthis work, we propose SDA-PLANNER, enabling an adaptive planning paradigm,\nstate-dependency aware and error-aware mechanisms for comprehensive embodied\ntask planning. Specifically, SDA-PLANNER introduces a State-Dependency Graph to\nexplicitly model action preconditions and effects, guiding the dynamic\nrevision. To handle execution error, it employs an error-adaptive replanning\nstrategy consisting of Error Backtrack and Diagnosis and Adaptive Action\nSubTree Generation, which locally reconstructs the affected portion of the plan\nbased on the current environment state. Experiments demonstrate that\nSDA-PLANNER consistently outperforms baselines in success rate and goal\ncompletion, particularly under diverse error conditions."}
{"id": "2509.26428", "categories": ["cs.RO", "cs.SY", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.26428", "abs": "https://arxiv.org/abs/2509.26428", "authors": ["Mattia Piazza", "Mattia Piccinini", "Sebastiano Taddei", "Francesco Biral", "Enrico Bertolazzi"], "title": "Real-time Velocity Profile Optimization for Time-Optimal Maneuvering with Generic Acceleration Constraints", "comment": null, "summary": "The computation of time-optimal velocity profiles along prescribed paths,\nsubject to generic acceleration constraints, is a crucial problem in robot\ntrajectory planning, with particular relevance to autonomous racing. However,\nthe existing methods either support arbitrary acceleration constraints at high\ncomputational cost or use conservative box constraints for computational\nefficiency. We propose FBGA, a new \\underline{F}orward-\\underline{B}ackward\nalgorithm with \\underline{G}eneric \\underline{A}cceleration constraints, which\nachieves both high accuracy and low computation time. FBGA operates forward and\nbackward passes to maximize the velocity profile in short, discretized path\nsegments, while satisfying user-defined performance limits. Tested on five\nracetracks and two vehicle classes, FBGA handles complex, non-convex\nacceleration constraints with custom formulations. Its maneuvers and lap times\nclosely match optimal control baselines (within $0.11\\%$-$0.36\\%$), while being\nup to three orders of magnitude faster. FBGA maintains high accuracy even with\ncoarse discretization, making it well-suited for online multi-query trajectory\nplanning. Our open-source \\texttt{C++} implementation is available at:\nhttps://anonymous.4open.science/r/FB_public_RAL."}
{"id": "2509.26439", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.26439", "abs": "https://arxiv.org/abs/2509.26439", "authors": ["Filip Kulisiewicz", "Basak Sakcak", "Evan G. Center", "Juho Kalliokoski", "Katherine J. Mimnaugh", "Steven M. LaValle", "Timo Ojala"], "title": "Unwinding Rotations Reduces VR Sickness in Nonsimulated Immersive Telepresence", "comment": "24th IEEE International Symposium on Mixed and Augmented Reality\n  (ISMAR)", "summary": "Immersive telepresence, when a user views the video stream of a $360^\\circ$\ncamera in a remote environment using a Head Mounted Display (HMD), has great\npotential to improve the sense of being in a remote environment. In most cases\nof immersive robotic telepresence, the camera is mounted on a mobile robot\nwhich increases the portion of the environment that the remote user can\nexplore. However, robot motions can induce unpleasant symptoms associated with\nVirtual Reality (VR) sickness, degrading the overall user experience. Previous\nresearch has shown that unwinding the rotations of the robot, that is,\ndecoupling the rotations that the camera undergoes due to robot motions from\nwhat is seen by the user, can increase user comfort and reduce VR sickness.\nHowever, that work considered a virtual environment and a simulated robot. In\nthis work, to test whether the same hypotheses hold when the video stream from\na real camera is used, we carried out a user study $(n=36)$ in which the\nunwinding rotations method was compared against coupled rotations in a task\ncompleted through a panoramic camera mounted on a robotic arm. Furthermore,\nwithin an inspection task which involved translations and rotations in three\ndimensions, we tested whether unwinding the robot rotations impacted the\nperformance of users. The results show that the users found the unwinding\nrotations method to be more comfortable and preferable, and that a reduced\nlevel of VR sickness can be achieved without a significant impact on task\nperformance."}
{"id": "2509.26459", "categories": ["cs.RO", "cs.CG"], "pdf": "https://arxiv.org/pdf/2509.26459", "abs": "https://arxiv.org/abs/2509.26459", "authors": ["Akshay Jaitly", "Devesh K. Jha", "Kei Ota", "Yuki Shirai"], "title": "Analytic Conditions for Differentiable Collision Detection in Trajectory Optimization", "comment": "8 pages, 8 figures. Accepted to the IEEE/RSJ International Conference\n  on Intelligent Robots and Systems (IROS) 2025", "summary": "Optimization-based methods are widely used for computing fast, diverse\nsolutions for complex tasks such as collision-free movement or planning in the\npresence of contacts. However, most of these methods require enforcing\nnon-penetration constraints between objects, resulting in a non-trivial and\ncomputationally expensive problem. This makes the use of optimization-based\nmethods for planning and control challenging. In this paper, we present a\nmethod to efficiently enforce non-penetration of sets while performing\noptimization over their configuration, which is directly applicable to problems\nlike collision-aware trajectory optimization. We introduce novel differentiable\nconditions with analytic expressions to achieve this. To enforce non-collision\nbetween non-smooth bodies using these conditions, we introduce a method to\napproximate polytopes as smooth semi-algebraic sets. We present several\nnumerical experiments to demonstrate the performance of the proposed method and\ncompare the performance with other baseline methods recently proposed in the\nliterature."}
{"id": "2509.26513", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.26513", "abs": "https://arxiv.org/abs/2509.26513", "authors": ["Saad Abdul Ghani", "Kameron Lee", "Xuesu Xiao"], "title": "Learning from Hallucinating Critical Points for Navigation in Dynamic Environments", "comment": null, "summary": "Generating large and diverse obstacle datasets to learn motion planning in\nenvironments with dynamic obstacles is challenging due to the vast space of\npossible obstacle trajectories. Inspired by hallucination-based data synthesis\napproaches, we propose Learning from Hallucinating Critical Points (LfH-CP), a\nself-supervised framework for creating rich dynamic obstacle datasets based on\nexisting optimal motion plans without requiring expensive expert demonstrations\nor trial-and-error exploration. LfH-CP factorizes hallucination into two\nstages: first identifying when and where obstacles must appear in order to\nresult in an optimal motion plan, i.e., the critical points, and then\nprocedurally generating diverse trajectories that pass through these points\nwhile avoiding collisions. This factorization avoids generative failures such\nas mode collapse and ensures coverage of diverse dynamic behaviors. We further\nintroduce a diversity metric to quantify dataset richness and show that LfH-CP\nproduces substantially more varied training data than existing baselines.\nExperiments in simulation demonstrate that planners trained on LfH-CP datasets\nachieves higher success rates compared to a prior hallucination method."}
{"id": "2509.26518", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.26518", "abs": "https://arxiv.org/abs/2509.26518", "authors": ["Shuoyu Yue", "Pengpeng Li", "Yang Xu", "Kunrui Ze", "Xingjian Long", "Huazi Cao", "Guibin Sun"], "title": "Memory-Efficient 2D/3D Shape Assembly of Robot Swarms", "comment": null, "summary": "Mean-shift-based approaches have recently emerged as the most effective\nmethods for robot swarm shape assembly tasks. These methods rely on image-based\nrepresentations of target shapes to compute local density gradients and perform\nmean-shift exploration, which constitute their core mechanism. However, such\nimage representations incur substantial memory overhead, which can become\nprohibitive for high-resolution or 3D shapes. To overcome this limitation, we\npropose a memory-efficient tree map representation that hierarchically encodes\nuser-specified shapes and is applicable to both 2D and 3D scenarios. Building\non this representation, we design a behavior-based distributed controller that\nenables assignment-free shape assembly. Comparative 2D and 3D simulations\nagainst a state-of-the-art mean-shift algorithm demonstrate one to two orders\nof magnitude lower memory usage and two to three times faster shape entry while\nmaintaining comparable uniformity. Finally, we validate the framework through\nphysical experiments with 6 to 7 UAVs, confirming its real-world practicality."}
{"id": "2509.26558", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.26558", "abs": "https://arxiv.org/abs/2509.26558", "authors": ["Andrés Martínez-Silva", "David Alejo", "Luis Merino", "Fernando Caballero"], "title": "Radio-based Multi-Robot Odometry and Relative Localization", "comment": null, "summary": "Radio-based methods such as Ultra-Wideband (UWB) and RAdio Detection And\nRanging (radar), which have traditionally seen limited adoption in robotics,\nare experiencing a boost in popularity thanks to their robustness to harsh\nenvironmental conditions and cluttered environments. This work proposes a\nmulti-robot UGV-UAV localization system that leverages the two technologies\nwith inexpensive and readily-available sensors, such as Inertial Measurement\nUnits (IMUs) and wheel encoders, to estimate the relative position of an aerial\nrobot with respect to a ground robot. The first stage of the system pipeline\nincludes a nonlinear optimization framework to trilaterate the location of the\naerial platform based on UWB range data, and a radar pre-processing module with\nloosely coupled ego-motion estimation which has been adapted for a multi-robot\nscenario. Then, the pre-processed radar data as well as the relative\ntransformation are fed to a pose-graph optimization framework with odometry and\ninter-robot constraints. The system, implemented for the Robotic Operating\nSystem (ROS 2) with the Ceres optimizer, has been validated in\nSoftware-in-the-Loop (SITL) simulations and in a real-world dataset. The\nproposed relative localization module outperforms state-of-the-art closed-form\nmethods which are less robust to noise. Our SITL environment includes a custom\nGazebo plugin for generating realistic UWB measurements modeled after real\ndata. Conveniently, the proposed factor graph formulation makes the system\nreadily extensible to full Simultaneous Localization And Mapping (SLAM).\nFinally, all the code and experimental data is publicly available to support\nreproducibility and to serve as a common open dataset for benchmarking."}
{"id": "2509.26581", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.26581", "abs": "https://arxiv.org/abs/2509.26581", "authors": ["Shishir Gopinath", "Karthik Dantu", "Steven Y. Ko"], "title": "Graphite: A GPU-Accelerated Mixed-Precision Graph Optimization Framework", "comment": null, "summary": "We present Graphite, a GPU-accelerated nonlinear graph optimization\nframework. It provides a CUDA C++ interface to enable the sharing of code\nbetween a realtime application, such as a SLAM system, and its optimization\ntasks. The framework supports techniques to reduce memory usage, including\nin-place optimization, support for multiple floating point types and\nmixed-precision modes, and dynamically computed Jacobians. We evaluate Graphite\non well-known bundle adjustment problems and find that it achieves similar\nperformance to MegBA, a solver specialized for bundle adjustment, while\nmaintaining generality and using less memory. We also apply Graphite to global\nvisual-inertial bundle adjustment on maps generated from stereo-inertial SLAM\ndatasets, and observe speed ups of up to 59x compared to a CPU baseline. Our\nresults indicate that our solver enables faster large-scale optimization on\nboth desktop and resource-constrained devices."}
{"id": "2509.26633", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.26633", "abs": "https://arxiv.org/abs/2509.26633", "authors": ["Lujie Yang", "Xiaoyu Huang", "Zhen Wu", "Angjoo Kanazawa", "Pieter Abbeel", "Carmelo Sferrazza", "C. Karen Liu", "Rocky Duan", "Guanya Shi"], "title": "OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction", "comment": "Project website: https://omniretarget.github.io", "summary": "A dominant paradigm for teaching humanoid robots complex skills is to\nretarget human motions as kinematic references to train reinforcement learning\n(RL) policies. However, existing retargeting pipelines often struggle with the\nsignificant embodiment gap between humans and robots, producing physically\nimplausible artifacts like foot-skating and penetration. More importantly,\ncommon retargeting methods neglect the rich human-object and human-environment\ninteractions essential for expressive locomotion and loco-manipulation. To\naddress this, we introduce OmniRetarget, an interaction-preserving data\ngeneration engine based on an interaction mesh that explicitly models and\npreserves the crucial spatial and contact relationships between an agent, the\nterrain, and manipulated objects. By minimizing the Laplacian deformation\nbetween the human and robot meshes while enforcing kinematic constraints,\nOmniRetarget generates kinematically feasible trajectories. Moreover,\npreserving task-relevant interactions enables efficient data augmentation, from\na single demonstration to different robot embodiments, terrains, and object\nconfigurations. We comprehensively evaluate OmniRetarget by retargeting motions\nfrom OMOMO, LAFAN1, and our in-house MoCap datasets, generating over 8-hour\ntrajectories that achieve better kinematic constraint satisfaction and contact\npreservation than widely used baselines. Such high-quality data enables\nproprioceptive RL policies to successfully execute long-horizon (up to 30\nseconds) parkour and loco-manipulation skills on a Unitree G1 humanoid, trained\nwith only 5 reward terms and simple domain randomization shared by all tasks,\nwithout any learning curriculum."}
{"id": "2509.26642", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.26642", "abs": "https://arxiv.org/abs/2509.26642", "authors": ["Zhuoyang Liu", "Jiaming Liu", "Jiadong Xu", "Nuowei Han", "Chenyang Gu", "Hao Chen", "Kaichen Zhou", "Renrui Zhang", "Kai Chin Hsieh", "Kun Wu", "Zhengping Che", "Jian Tang", "Shanghang Zhang"], "title": "MLA: A Multisensory Language-Action Model for Multimodal Understanding and Forecasting in Robotic Manipulation", "comment": null, "summary": "Vision-language-action models (VLAs) have shown generalization capabilities\nin robotic manipulation tasks by inheriting from vision-language models (VLMs)\nand learning action generation. Most VLA models focus on interpreting vision\nand language to generate actions, whereas robots must perceive and interact\nwithin the spatial-physical world. This gap highlights the need for a\ncomprehensive understanding of robotic-specific multisensory information, which\nis crucial for achieving complex and contact-rich control. To this end, we\nintroduce a multisensory language-action (MLA) model that collaboratively\nperceives heterogeneous sensory modalities and predicts future multisensory\nobjectives to facilitate physical world modeling. Specifically, to enhance\nperceptual representations, we propose an encoder-free multimodal alignment\nscheme that innovatively repurposes the large language model itself as a\nperception module, directly interpreting multimodal cues by aligning 2D images,\n3D point clouds, and tactile tokens through positional correspondence. To\nfurther enhance MLA's understanding of physical dynamics, we design a future\nmultisensory generation post-training strategy that enables MLA to reason about\nsemantic, geometric, and interaction information, providing more robust\nconditions for action generation. For evaluation, the MLA model outperforms the\nprevious state-of-the-art 2D and 3D VLA methods by 12% and 24% in complex,\ncontact-rich real-world tasks, respectively, while also demonstrating improved\ngeneralization to unseen configurations. Project website:\nhttps://sites.google.com/view/open-mla"}
