{"id": "2601.07945", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.07945", "abs": "https://arxiv.org/abs/2601.07945", "authors": ["Aabha Tamhankar", "Ron Alterovitz", "Ajit S. Puri", "Giovanni Pittiglio"], "title": "Contact-aware Path Planning for Autonomous Neuroendovascular Navigation", "comment": "8 pages, 7 figures, IROS(R-AL)", "summary": "We propose a deterministic and time-efficient contact-aware path planner for neurovascular navigation. The algorithm leverages information from pre- and intra-operative images of the vessels to navigate pre-bent passive tools, by intelligently predicting and exploiting interactions with the anatomy. A kinematic model is derived and employed by the sampling-based planner for tree expansion that utilizes simplified motion primitives. This approach enables fast computation of the feasible path, with negligible loss in accuracy, as demonstrated in diverse and representative anatomies of the vessels. In these anatomical demonstrators, the algorithm shows a 100% convergence rate within 22.8s in the worst case, with sub-millimeter tracking errors (less than 0.64 mm), and is found effective on anatomical phantoms representative of around 94% of patients.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u795e\u7ecf\u8840\u7ba1\u5bfc\u822a\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\uff0c\u5177\u6709\u9ad8\u6536\u655b\u7387\u548c\u4f4e\u8ddf\u8e2a\u8bef\u5dee\uff0c\u9002\u7528\u6027\u826f\u597d\u3002", "motivation": "\u53d1\u5c55\u4e00\u79cd\u786e\u5b9a\u6027\u4e14\u9ad8\u6548\u7684\u63a5\u89e6\u611f\u77e5\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\uff0c\u4ee5\u6539\u5584\u795e\u7ecf\u8840\u7ba1\u5bfc\u822a\u7684\u7cbe\u786e\u5ea6\u548c\u6548\u7387\u3002", "method": "\u5229\u7528\u9884\u624b\u672f\u548c\u672f\u4e2d\u5f71\u50cf\u4fe1\u606f\uff0c\u901a\u8fc7\u91c7\u6837\u57fa\u7840\u7684\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\u548c\u8fd0\u52a8\u539f\u8bed\u8fdb\u884c\u6811\u6269\u5c55\uff0c\u667a\u80fd\u9884\u6d4b\u548c\u5229\u7528\u4e0e\u89e3\u5256\u7ed3\u6784\u7684\u4ea4\u4e92\u3002", "result": "\u8be5\u7b97\u6cd5\u5728\u4e0d\u540c\u7684\u89e3\u5256\u6a21\u578b\u4e2d\u5b9e\u73b0\u4e86\u5feb\u901f\u8def\u5f84\u8ba1\u7b97\uff0c\u4fdd\u6301\u4e86\u6781\u5c0f\u7684\u7cbe\u5ea6\u635f\u5931\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728\u591a\u79cd\u8840\u7ba1\u89e3\u5256\u7ed3\u6784\u4e2d\u663e\u793a\u4e86100%\u7684\u6536\u655b\u7387\uff0c\u5e76\u4e14\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u8ba1\u7b97\u65f6\u95f4\u4e0d\u8d85\u8fc722.8\u79d2\uff0c\u8ddf\u8e2a\u8bef\u5dee\u5c0f\u4e8e0.64\u6beb\u7c73\uff0c\u572894%\u7684\u60a3\u8005\u7684\u89e3\u5256\u6a21\u578b\u4e0a\u8868\u73b0\u6709\u6548\u3002"}}
{"id": "2601.08034", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08034", "abs": "https://arxiv.org/abs/2601.08034", "authors": ["Cameron Smith", "Basile Van Hoorick", "Vitor Guizilini", "Yue Wang"], "title": "Fiducial Exoskeletons: Image-Centric Robot State Estimation", "comment": null, "summary": "We introduce Fiducial Exoskeletons, an image-based reformulation of 3D robot state estimation that replaces cumbersome procedures and motor-centric pipelines with single-image inference. Traditional approaches - especially robot-camera extrinsic estimation - often rely on high-precision actuators and require time-consuming routines such as hand-eye calibration. In contrast, modern learning-based robot control is increasingly trained and deployed from RGB observations on lower-cost hardware.\n  Our key insight is twofold. First, we cast robot state estimation as 6D pose estimation of each link from a single RGB image: the robot-camera base transform is obtained directly as the estimated base-link pose, and the joint state is recovered via a lightweight global optimization that enforces kinematic consistency with the observed link poses (optionally warm-started with encoder readings). Second, we make per-link 6D pose estimation robust and simple - even without learning - by introducing the fiducial exoskeleton: a lightweight 3D-printed mount with a fiducial marker on each link and known marker-link geometry.\n  This design yields robust camera-robot extrinsics, per-link SE(3) poses, and joint-angle state from a single image, enabling robust state estimation even on unplugged robots. Demonstrated on a low-cost robot arm, fiducial exoskeletons substantially simplify setup while improving calibration, state accuracy, and downstream 3D control performance. We release code and printable hardware designs to enable further algorithm-hardware co-design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFiducial Exoskeletons\uff0c\u901a\u8fc7\u5355\u56fe\u50cf\u63a8\u65ad\u91cd\u67843D\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\uff0c\u7b80\u5316\u4f20\u7edf\u6d41\u7a0b\uff0c\u589e\u5f3a\u72b6\u6001\u7cbe\u5ea6\u4e0e\u63a7\u5236\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\u4f9d\u8d56\u9ad8\u7cbe\u5ea6\u7684\u6267\u884c\u5668\u548c\u624b\u52a8\u6821\u51c6\uff0c\u6d41\u7a0b\u7e41\u7410\uff0c\u9650\u5236\u4e86\u4f4e\u6210\u672c\u786c\u4ef6\u7684\u5e94\u7528\u3002", "method": "\u5c06\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\u89c6\u4e3a\u4ece\u5355\u5e45RGB\u56fe\u50cf\u4f30\u8ba1\u6bcf\u4e2a\u94fe\u63a5\u76846D\u59ff\u6001\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5168\u5c40\u4f18\u5316\u6765\u6062\u590d\u5173\u8282\u72b6\u6001\uff0c\u5e76\u5f15\u5165Fiducial Exoskeleton\u4f5c\u4e3a\u7b80\u5316\u65b9\u6848\u3002", "result": "\u5728\u4f4e\u6210\u672c\u673a\u5668\u4eba\u81c2\u4e0a\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\uff0c\u663e\u8457\u7b80\u5316\u4e86\u8bbe\u7f6e\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u6821\u51c6\u548c\u540e\u7eed3D\u63a7\u5236\u6027\u80fd\u3002\u53d1\u5e03\u4e86\u4ee3\u7801\u548c\u53ef\u6253\u5370\u7684\u786c\u4ef6\u8bbe\u8ba1\u3002", "conclusion": "Fiducial Exoskeletons\u7684\u8bbe\u8ba1\u6781\u5927\u7b80\u5316\u4e86\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\u6d41\u7a0b\uff0c\u63d0\u5347\u4e86\u6821\u51c6\u7cbe\u5ea6\u4e0e\u63a7\u5236\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u6210\u672c\u8f83\u4f4e\u7684\u786c\u4ef6\u3002"}}
{"id": "2601.08110", "categories": ["cs.RO", "cs.IT", "eess.SP", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.08110", "abs": "https://arxiv.org/abs/2601.08110", "authors": ["Reza Arablouei"], "title": "Efficient Incremental SLAM via Information-Guided and Selective Optimization", "comment": null, "summary": "We present an efficient incremental SLAM back-end that achieves the accuracy of full batch optimization while substantially reducing computational cost. The proposed approach combines two complementary ideas: information-guided gating (IGG) and selective partial optimization (SPO). IGG employs an information-theoretic criterion based on the log-determinant of the information matrix to quantify the contribution of new measurements, triggering global optimization only when a significant information gain is observed. This avoids unnecessary relinearization and factorization when incoming data provide little additional information. SPO executes multi-iteration Gauss-Newton (GN) updates but restricts each iteration to the subset of variables most affected by the new measurements, dynamically refining this active set until convergence. Together, these mechanisms retain all measurements to preserve global consistency while focusing computation on parts of the graph where it yields the greatest benefit. We provide theoretical analysis showing that the proposed approach maintains the convergence guarantees of full GN. Extensive experiments on benchmark SLAM datasets show that our approach consistently matches the estimation accuracy of batch solvers, while achieving significant computational savings compared to conventional incremental approaches. The results indicate that the proposed approach offers a principled balance between accuracy and efficiency, making it a robust and scalable solution for real-time operation in dynamic data-rich environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u589e\u91cfSLAM\u540e\u7aef\uff0c\u901a\u8fc7\u4fe1\u606f\u5f15\u5bfc\u95e8\u63a7\u548c\u9009\u62e9\u6027\u90e8\u5206\u4f18\u5316\u65b9\u6cd5\uff0c\u5728\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u6279\u5904\u7406\u4f18\u5316\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u589e\u91cfSLAM\u4e2d\u5b9e\u73b0\u51c6\u786e\u6027\u4e0e\u6548\u7387\u7684\u5e73\u8861\uff0c\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u3002", "method": "\u4fe1\u606f\u5f15\u5bfc\u95e8\u63a7\uff08IGG\uff09\u4e0e\u9009\u62e9\u6027\u90e8\u5206\u4f18\u5316\uff08SPO\uff09\u7684\u7ed3\u5408", "result": "\u4e0e\u4f20\u7edf\u589e\u91cf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8ba1\u7b97\u4e0a\u663e\u8457\u8282\u7701\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6279\u5904\u7406\u6c42\u89e3\u5668\u7684\u4f30\u8ba1\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u52a8\u6001\u4fe1\u606f\u4e30\u5bcc\u7684\u73af\u5883\u4e2d\uff0c\u63d0\u4f9b\u4e86\u51c6\u786e\u6027\u4e0e\u6548\u7387\u4e4b\u95f4\u7684\u5408\u7406\u5e73\u8861\uff0c\u662f\u4e00\u79cd\u7a33\u5065\u4e14\u53ef\u6269\u5c55\u7684\u5b9e\u65f6\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.08143", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08143", "abs": "https://arxiv.org/abs/2601.08143", "authors": ["Takuya Kato", "Kentaro Uno", "Kazuya Yoshida"], "title": "A Pin-Array Structure for Gripping and Shape Recognition of Convex and Concave Terrain Profiles", "comment": "Author's version of a manuscript accepted at the 2022 IEEE International Conference on Robotics and Biomimetics (ROBIO). (c) IEEE", "summary": "This paper presents a gripper capable of grasping and recognizing terrain shapes for mobile robots in extreme environments. Multi-limbed climbing robots with grippers are effective on rough terrains, such as cliffs and cave walls. However, such robots may fall over by misgrasping the surface or getting stuck owing to the loss of graspable points in unknown natural environments. To overcome these issues, we need a gripper capable of adaptive grasping to irregular terrains, not only for grasping but also for measuring the shape of the terrain surface accurately. We developed a gripper that can grasp both convex and concave terrains and simultaneously measure the terrain shape by introducing a pin-array structure. We demonstrated the mechanism of the gripper and evaluated its grasping and terrain recognition performance using a prototype. Moreover, the proposed pin-array design works well for 3D terrain mapping as well as adaptive grasping for irregular terrains.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6293\u53d6\u5668\uff0c\u7528\u4e8e\u5728\u6781\u7aef\u73af\u5883\u4e2d\u7684\u79fb\u52a8\u673a\u5668\u4eba\uff0c\u5177\u5907\u6293\u53d6\u548c\u8bc6\u522b\u4e0d\u89c4\u5219\u5730\u5f62\u7684\u80fd\u529b\uff0c\u80fd\u591f\u7cbe\u51c6\u6d4b\u91cf\u5730\u5f62\u5f62\u72b6\uff0c\u5c55\u793a\u4e86\u5176\u826f\u597d\u7684\u6027\u80fd\u53ca\u5e94\u7528\u524d\u666f\u3002", "motivation": "\u5728\u6781\u7aef\u73af\u5883\u4e2d\uff0c\u79fb\u52a8\u673a\u5668\u4eba\u9700\u8981\u80fd\u591f\u6293\u53d6\u548c\u8bc6\u522b\u5730\u5f62\u5f62\u72b6\u7684\u6293\u53d6\u5668\uff0c\u4ee5\u63d0\u9ad8\u5728\u5d0e\u5c96\u5730\u5f62\uff08\u5982\u60ac\u5d16\u548c\u6d1e\u58c1\uff09\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u5177\u6709\u9488\u9635\u7ed3\u6784\u7684\u6293\u53d6\u5668\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u89c4\u5219\u5730\u5f62\uff0c\u7cbe\u786e\u6d4b\u91cf\u5730\u5f62\u7684\u5f62\u72b6\uff0c\u5e76\u540c\u65f6\u6293\u53d6\u51f8\u51f9\u5730\u5f62\u3002", "result": "\u901a\u8fc7\u539f\u578b\u5c55\u793a\u4e86\u6293\u53d6\u5668\u7684\u673a\u5236\uff0c\u5e76\u8bc4\u4f30\u4e86\u5176\u6293\u53d6\u548c\u5730\u5f62\u8bc6\u522b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u9488\u9635\u8bbe\u8ba1\u57283D\u5730\u5f62\u6620\u5c04\u548c\u4e0d\u89c4\u5219\u5730\u5f62\u81ea\u9002\u5e94\u6293\u53d6\u4e2d\u7684\u826f\u597d\u5e94\u7528\u3002", "conclusion": "\u8be5\u6293\u53d6\u5668\u5728\u56f0\u96be\u73af\u5883\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u5f3a\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u79fb\u52a8\u673a\u5668\u4eba\u5728\u672a\u77e5\u81ea\u7136\u73af\u5883\u4e2d\u56e0\u6293\u53d6\u5931\u8bef\u6216\u6293\u53d6\u70b9\u4e22\u5931\u800c\u5931\u63a7\u7684\u95ee\u9898\u3002"}}
{"id": "2601.08035", "categories": ["cs.HC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.08035", "abs": "https://arxiv.org/abs/2601.08035", "authors": ["David Elsweiler"], "title": "From Tool to Teacher: Rethinking Search Systems as Instructive Interfaces", "comment": null, "summary": "Information access systems such as search engines and generative AI are central to how people seek, evaluate, and interpret information. Yet most systems are designed to optimise retrieval rather than to help users develop better search strategies or critical awareness. This paper introduces a pedagogical perspective on information access, conceptualising search and conversational systems as instructive interfaces that can teach, guide, and scaffold users' learning. We draw on seven didactic frameworks from education and behavioural science to analyse how existing and emerging system features, including query suggestions, source labels, and conversational or agentic AI, support or limit user learning. Using two illustrative search tasks, we demonstrate how different design choices promote skills such as critical evaluation, metacognitive reflection, and strategy transfer. The paper contributes a conceptual lens for evaluating the instructional value of information access systems and outlines design implications for technologies that foster more effective, reflective, and resilient information seekers.", "AI": {"tldr": "\u672c\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u4ece\u6559\u80b2\u5b66\u89d2\u5ea6\u4f18\u5316\u4fe1\u606f\u83b7\u53d6\u7cfb\u7edf\uff0c\u5e2e\u52a9\u7528\u6237\u66f4\u597d\u5730\u53d1\u5c55\u641c\u7d22\u7b56\u7565\u548c\u6279\u5224\u610f\u8bc6\u3002", "motivation": "\u5f53\u524d\u4fe1\u606f\u83b7\u53d6\u7cfb\u7edf\u4e3b\u8981\u4f18\u5316\u68c0\u7d22\u6548\u7387\uff0c\u7f3a\u4e4f\u5e2e\u52a9\u7528\u6237\u63d0\u5347\u641c\u7d22\u7b56\u7565\u548c\u6279\u5224\u610f\u8bc6\u7684\u529f\u80fd\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6559\u80b2\u548c\u884c\u4e3a\u79d1\u5b66\u4e2d\u7684\u6559\u5b66\u6846\u67b6\uff0c\u5e76\u5e94\u7528\u4e8e\u4fe1\u606f\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u5c55\u793a\u4e0d\u540c\u8bbe\u8ba1\u9009\u62e9\u5982\u4f55\u5f71\u54cd\u7528\u6237\u7684\u5b66\u4e60\u548c\u6280\u80fd\u53d1\u5c55\u3002", "result": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6559\u80b2\u5b66\u89c6\u89d2\u6765\u5206\u6790\u4fe1\u606f\u83b7\u53d6\u7cfb\u7edf\u7684\u8bbe\u8ba1\uff0c\u5f3a\u8c03\u8fd9\u4e9b\u7cfb\u7edf\u4e0d\u4ec5\u5e94\u4f18\u5316\u4fe1\u606f\u68c0\u7d22\uff0c\u66f4\u8981\u52a9\u529b\u7528\u6237\u7684\u641c\u7d22\u7b56\u7565\u548c\u6279\u5224\u610f\u8bc6\u7684\u63d0\u5347\u3002\u901a\u8fc7\u6559\u80b2\u548c\u884c\u4e3a\u79d1\u5b66\u4e2d\u7684\u4e03\u79cd\u6559\u5b66\u6846\u67b6\uff0c\u7814\u7a76\u4e86\u73b0\u6709\u548c\u65b0\u5174\u7cfb\u7edf\u529f\u80fd\u5982\u4f55\u5f71\u54cd\u7528\u6237\u5b66\u4e60\uff0c\u7279\u522b\u662f\u8bc4\u4f30\u80fd\u529b\u3001\u5143\u8ba4\u77e5\u53cd\u601d\u548c\u7b56\u7565\u8f6c\u79fb\u7b49\u6280\u80fd\u7684\u63a8\u5e7f\u3002", "conclusion": "\u8be5\u8bba\u6587\u4e3a\u4fe1\u606f\u83b7\u53d6\u7cfb\u7edf\u7684\u6559\u80b2\u4ef7\u503c\u63d0\u4f9b\u4e86\u6982\u5ff5\u6027\u89c6\u89d2\uff0c\u5e76\u63d0\u51fa\u4e86\u4fc3\u8fdb\u7528\u6237\u6210\u4e3a\u66f4\u6709\u6548\u3001\u53cd\u601d\u6027\u548c\u97e7\u6027\u7684\u4fe1\u606f\u68c0\u7d22\u8005\u7684\u8bbe\u8ba1\u5efa\u8bae\u3002"}}
{"id": "2601.08161", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08161", "abs": "https://arxiv.org/abs/2601.08161", "authors": ["Jing Tao", "Banglei Guan", "Yang Shang", "Shunkun Liang", "Qifeng Yu"], "title": "Robust Subpixel Localization of Diagonal Markers in Large-Scale Navigation via Multi-Layer Screening and Adaptive Matching", "comment": "This paper has been accepted by Applied Optics", "summary": "This paper proposes a robust, high-precision positioning methodology to address localization failures arising from complex background interference in large-scale flight navigation and the computational inefficiency inherent in conventional sliding window matching techniques. The proposed methodology employs a three-tiered framework incorporating multi-layer corner screening and adaptive template matching. Firstly, dimensionality is reduced through illumination equalization and structural information extraction. A coarse-to-fine candidate selection strategy minimizes sliding window computational costs, enabling rapid estimation of the marker's position. Finally, adaptive templates are generated for candidate points, achieving subpixel precision through improved template matching with correlation coefficient extremum fitting. Experimental results demonstrate the method's effectiveness in extracting and localizing diagonal markers in complex, large-scale environments, making it ideal for field-of-view measurement in navigation tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u5c42\u6846\u67b6\u548c\u81ea\u9002\u5e94\u6a21\u677f\u5339\u914d\u514b\u670d\u4f20\u7edf\u6ed1\u52a8\u7a97\u53e3\u6280\u672f\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u53ef\u4ee5\u6709\u6548\u5e94\u5bf9\u590d\u6742\u80cc\u666f\u5e72\u6270\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u98de\u884c\u5bfc\u822a\u4e2d\u7684\u5b9a\u4f4d\u5931\u8d25\u548c\u4f20\u7edf\u8ba1\u7b97\u65b9\u6cd5\u7684\u4f4e\u6548\u7387\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u4e09\u5c42\u6846\u67b6\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u5c42\u89d2\u70b9\u7b5b\u9009\u548c\u81ea\u9002\u5e94\u6a21\u677f\u5339\u914d\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u3002", "result": "\u901a\u8fc7\u5149\u7167\u5747\u8861\u548c\u7ed3\u6784\u4fe1\u606f\u63d0\u53d6\u5b9e\u73b0\u964d\u7ef4\uff0c\u5e76\u91c7\u7528\u7c97\u5230\u7ec6\u7684\u5019\u9009\u9009\u62e9\u7b56\u7565\u4ee5\u964d\u4f4e\u6ed1\u52a8\u7a97\u53e3\u7684\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u6709\u6548\u63d0\u53d6\u548c\u5b9a\u4f4d\u5bf9\u89d2\u6807\u8bb0\uff0c\u9002\u7528\u4e8e\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u89c6\u57df\u6d4b\u91cf\u3002"}}
{"id": "2601.08178", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.08178", "abs": "https://arxiv.org/abs/2601.08178", "authors": ["Shangqian Li", "Tianwa Chen", "Gianluca Demartini"], "title": "The Impact of AI Generated Content on Decision Making for Topics Requiring Expertise", "comment": "2 figures, pre-print (not yet being produced) version for HEBT", "summary": "Modelling users' online decision-making and opinion change is a complex issue that needs to consider users' personal determinants, the nature of the topic and the information retrieval activities. Furthermore, generative-AIbased products like ChatGPT gradually become an essential element for the retrieval of online information. However, the interaction between domainspecific knowledge and AI-generated content during online decision-making is unclear. We conducted a lab-based explanatory sequential study with university students to overcome this research gap. In the experiment, we surveyed participants about a set of general domain topics that are easy to grasp and another set of domain-specific topics that require adequate levels of chemical science knowledge to fully comprehend. We provided participants with decision-supporting information that was either produced using generative AI or collected from selected expert human-written sources to explore the role of AI-generated content compared to ordinary information during decision-making. Our result revealed that participants are less likely to change opinions on domain-specific topics. Since participants without professional knowledge had difficulty performing in-depth and independent reasoning based on the information, they favoured relying on conclusions presented in the provided materials and tended to stick to their initial opinion. Besides, information that is labelled as AI-generated is equivalently helpful as information labelled as dedicatedly human-written for participants in this experiment, indicating the vast potential as well as concerns for AI replacing human experts to help users tackle professional topics or issues.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86AI\u751f\u6210\u5185\u5bb9\u4e0e\u4eba\u7c7b\u4fe1\u606f\u5728\u51b3\u7b56\u652f\u6301\u4e2d\u7684\u4f5c\u7528\uff0c\u53d1\u73b0\u524d\u8005\u5728\u67d0\u4e9b\u60c5\u5883\u4e0b\u540c\u6837\u6709\u6548\uff0c\u4f46\u7528\u6237\u5728\u7f3a\u4e4f\u4e13\u4e1a\u77e5\u8bc6\u65f6\u4ecd\u4f9d\u8d56\u4e8e\u521d\u59cb\u89c2\u70b9\u3002", "motivation": "\u5f25\u8865\u5728\u7ebf\u51b3\u7b56\u4e0e\u610f\u89c1\u53d8\u5316\u8fc7\u7a0b\u4e2d\u9886\u57df\u77e5\u8bc6\u4e0eAI\u751f\u6210\u5185\u5bb9\u4e4b\u95f4\u76f8\u4e92\u4f5c\u7528\u7684\u4e0d\u660e\u786e\u6027", "method": "\u8fdb\u884c\u5b9e\u9a8c\u5ba4\u57fa\u7840\u7684\u89e3\u91ca\u6027\u5e8f\u5217\u7814\u7a76\uff0c\u9488\u5bf9\u5927\u5b66\u751f\u8fdb\u884c\u8c03\u67e5", "result": "\u53c2\u4e0e\u8005\u5728\u4e13\u4e1a\u9886\u57df\u8bdd\u9898\u4e0a\u7684\u89c2\u70b9\u4e0d\u6613\u6539\u53d8\uff0cAI\u751f\u6210\u7684\u4fe1\u606f\u4e0e\u4eba\u7c7b\u64b0\u5199\u7684\u4fe1\u606f\u540c\u6837\u6709\u6548", "conclusion": "AI\u751f\u6210\u7684\u4fe1\u606f\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u80fd\u591f\u66ff\u4ee3\u4eba\u7c7b\u4e13\u5bb6\u7684\u4fe1\u606f\uff0c\u4f46\u7528\u6237\u5728\u7f3a\u4e4f\u4e13\u4e1a\u77e5\u8bc6\u65f6\u66f4\u503e\u5411\u4e8e\u4f9d\u8d56\u63d0\u4f9b\u7684\u7ed3\u8bba\uff0c\u96be\u4ee5\u8fdb\u884c\u6df1\u5ea6\u72ec\u7acb\u63a8\u7406"}}
{"id": "2601.08244", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08244", "abs": "https://arxiv.org/abs/2601.08244", "authors": ["Yaohua Liu", "Hengjun Zhang", "Binkai Ou"], "title": "A brain-inspired information fusion method for enhancing robot GPS outages navigation", "comment": null, "summary": "Low-cost inertial navigation systems (INS) are prone to sensor biases and measurement noise, which lead to rapid degradation of navigation accuracy during global positioning system (GPS) outages. To address this challenge and improve positioning continuity in GPS-denied environments, this paper proposes a brain-inspired GPS/INS fusion network (BGFN) based on spiking neural networks (SNNs). The BGFN architecture integrates a spiking Transformer with a spiking encoder to simultaneously extract spatial features from inertial measurement unit (IMU) signals and capture their temporal dynamics. By modeling the relationship between vehicle attitude, specific force, angular rate, and GPS-derived position increments, the network leverages both current and historical IMU data to estimate vehicle motion. The effectiveness of the proposed method is evaluated through real-world field tests and experiments on public datasets. Compared to conventional deep learning approaches, the results demonstrate that BGFN achieves higher accuracy and enhanced reliability in navigation performance, particularly under prolonged GPS outages.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684GPS/INS\u878d\u5408\u7f51\u7edc\uff0c\u80fd\u591f\u5728GPS\u5931\u6548\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u5bfc\u822a\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u4f4e\u6210\u672c\u60ef\u6027\u5bfc\u822a\u7cfb\u7edf\u5728GPS\u5931\u6548\u65f6\u5bfc\u822a\u7cbe\u5ea6\u8fc5\u901f\u964d\u4f4e\uff0c\u8feb\u5207\u9700\u8981\u6539\u8fdb\u5b9a\u4f4d\u8fde\u7eed\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u8111\u542f\u53d1\u5f0fGPS/INS\u878d\u5408\u7f51\u7edc\uff0c\u901a\u8fc7\u8109\u51b2Transformer\u548c\u8109\u51b2\u7f16\u7801\u5668\u540c\u65f6\u63d0\u53d6IMU\u4fe1\u53f7\u7684\u7a7a\u95f4\u7279\u5f81\u4e0e\u65f6\u95f4\u52a8\u6001\u3002", "result": "\u5728\u5b9e\u9645\u573a\u5730\u6d4b\u8bd5\u548c\u516c\u5171\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u4e2d\uff0cBGFN\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u5bfc\u822a\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "BGFN\u76f8\u8f83\u4e8e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u5bfc\u822a\u6027\u80fd\u4e0a\u5177\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u66f4\u5f3a\u7684\u53ef\u9760\u6027\uff0c\u7279\u522b\u662f\u5728GPS\u957f\u65f6\u95f4\u5931\u6548\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2601.08186", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.08186", "abs": "https://arxiv.org/abs/2601.08186", "authors": ["Cassidy R. Nelson", "Joseph L. Gabbard", "Jason B. Moats", "Ranjana K. Mehta"], "title": "Simulations for Augmented Reality Evaluation for Mass Casualty Incident Triage", "comment": null, "summary": "Mass casualty incidents (MCIs) are a high-risk, sensitive domain with profound implications for patient and responder safety. Augmented reality has shown promise as an assistive tool for high-stress work domains and MCI triage both in the field and for pre-field training. However, the vulnerability of MCIs makes it challenging to evaluate new tools designed to enhance MCI response. In other words, profound evolutions like the integration of augmented reality into field response require thorough proof-of-concept evaluations before being launched into real-world response. This paper describes two progressive simulation strategies for augmented reality that bridge the gap between computer-based simulation and actual field response.", "AI": {"tldr": "\u672c\u8bba\u6587\u63a2\u8ba8\u4e86\u589e\u5f3a\u73b0\u5b9e\u5728\u5927\u89c4\u6a21\u4f24\u4ea1\u4e8b\u6545\u5e94\u5bf9\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u6a21\u62df\u7b56\u7565\u4ee5\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u5927\u89c4\u6a21\u4f24\u4ea1\u4e8b\u6545\u7684\u98ce\u9669\u4e0a\u5347\uff0c\u63d0\u5347\u5e94\u5bf9\u80fd\u529b\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u589e\u5f3a\u73b0\u5b9e\u88ab\u8ba4\u4e3a\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u5de5\u5177\uff0c\u4f46\u9700\u8981\u7ecf\u8fc7\u5145\u5206\u7684\u6982\u5ff5\u9a8c\u8bc1\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e24\u79cd\u6e10\u8fdb\u7684\u589e\u5f3a\u73b0\u5b9e\u6a21\u62df\u7b56\u7565\uff0c\u65e8\u5728\u5f25\u5408\u8ba1\u7b97\u673a\u6a21\u62df\u4e0e\u73b0\u573a\u54cd\u5e94\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "result": "\u901a\u8fc7\u65b0\u6a21\u62df\u7b56\u7565\u7684\u5e94\u7528\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u8bc4\u4f30\u5e76\u63d0\u5347\u589e\u5f3a\u73b0\u5b9e\u5728MCI\u54cd\u5e94\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u4e24\u79cd\u589e\u5f3a\u73b0\u5b9e\u6a21\u62df\u7b56\u7565\u53ef\u4ee5\u6709\u6548\u5730\u8fde\u63a5\u8ba1\u7b97\u673a\u6a21\u62df\u4e0e\u5b9e\u9645\u73b0\u573a\u54cd\u5e94\uff0c\u4fc3\u8fdb\u5927\u89c4\u6a21\u4f24\u4ea1\u4e8b\u6545\u7684\u5e94\u5bf9\u80fd\u529b\u63d0\u5347\u3002"}}
{"id": "2601.08246", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08246", "abs": "https://arxiv.org/abs/2601.08246", "authors": ["Yifan Han", "Pengfei Yi", "Junyan Li", "Hanqing Wang", "Gaojing Zhang", "Qi Peng Liu", "Wenzhao Lian"], "title": "FSAG: Enhancing Human-to-Dexterous-Hand Finger-Specific Affordance Grounding via Diffusion Models", "comment": null, "summary": "Dexterous grasp synthesis remains a central challenge: the high dimensionality and kinematic diversity of multi-fingered hands prevent direct transfer of algorithms developed for parallel-jaw grippers. Existing approaches typically depend on large, hardware-specific grasp datasets collected in simulation or through costly real-world trials, hindering scalability as new dexterous hand designs emerge. To this end, we propose a data-efficient framework, which is designed to bypass robot grasp data collection by exploiting the rich, object-centric semantic priors latent in pretrained generative diffusion models. Temporally aligned and fine-grained grasp affordances are extracted from raw human video demonstrations and fused with 3D scene geometry from depth images to infer semantically grounded contact targets. A kinematics-aware retargeting module then maps these affordance representations to diverse dexterous hands without per-hand retraining. The resulting system produces stable, functionally appropriate multi-contact grasps that remain reliably successful across common objects and tools, while exhibiting strong generalization across previously unseen object instances within a category, pose variations, and multiple hand embodiments. This work (i) introduces a semantic affordance extraction pipeline leveraging vision-language generative priors for dexterous grasping, (ii) demonstrates cross-hand generalization without constructing hardware-specific grasp datasets, and (iii) establishes that a single depth modality suffices for high-performance grasp synthesis when coupled with foundation-model semantics. Our results highlight a path toward scalable, hardware-agnostic dexterous manipulation driven by human demonstrations and pretrained generative models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u9884\u8bad\u7ec3\u751f\u6210\u6269\u6563\u6a21\u578b\u7684\u8bed\u4e49\u4f18\u5148\u63d0\u53d6\u7ba1\u9053\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u6307\u6293\u53d6\u5408\u6210\uff0c\u80fd\u591f\u5728\u4e0d\u9700\u8981\u7279\u5b9a\u786c\u4ef6\u6570\u636e\u96c6\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u8de8\u624b\u6cdb\u5316\u3002", "motivation": "\u5f53\u524d\u7684\u591a\u6307\u6293\u53d6\u5408\u6210\u4e8e\u9ad8\u7ef4\u548c\u8fd0\u52a8\u5b66\u591a\u6837\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6602\u8d35\u7684\u6293\u53d6\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u65b0\u624b\u5f62\u8bbe\u8ba1\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u901a\u8fc7\u4f7f\u7528\u9884\u8bad\u7ec3\u751f\u6210\u6269\u6563\u6a21\u578b\u4e2d\u7684\u8bed\u4e49\u4f18\u5148\uff0c\u63d0\u53d6\u4e0e\u65f6\u5e8f\u5bf9\u9f50\u7684\u6293\u53d6\u80fd\u529b\uff0c\u7136\u540e\u5c06\u5176\u4e0e\u6df1\u5ea6\u56fe\u50cf\u4e2d\u76843D\u573a\u666f\u51e0\u4f55\u4f53\u7ed3\u5408\uff0c\u5f62\u6210\u8bed\u4e49\u57fa\u7840\u7684\u63a5\u89e6\u76ee\u6807\uff0c\u6700\u7ec8\u901a\u8fc7\u8fd0\u52a8\u5b66\u91cd\u5b9a\u6807\u6a21\u5757\u5b9e\u73b0\u5404\u79cd\u591a\u6307\u624b\u7684\u6293\u53d6\u6620\u5c04\u3002", "result": "\u7cfb\u7edf\u53ef\u4ee5\u751f\u6210\u7a33\u5b9a\u4e14\u529f\u80fd\u9002\u5f53\u7684\u591a\u63a5\u89e6\u6293\u53d6\uff0c\u5e76\u5728\u5e38\u89c1\u7269\u4f53\u548c\u5de5\u5177\u4e4b\u95f4\u4fdd\u6301\u9ad8\u6210\u529f\u7387\uff0c\u540c\u65f6\u5728\u672a\u89c1\u7269\u4f53\u5b9e\u4f8b\u548c\u4e0d\u540c\u59ff\u6001\u53d8\u5316\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u4e8e\u7279\u5b9a\u786c\u4ef6\u6293\u53d6\u6570\u636e\u96c6\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u7075\u6d3b\u7684\u6293\u53d6\u5408\u6210\uff0c\u5e76\u4e14\u5c55\u793a\u51fa\u4e86\u5728\u4e0d\u540c\u624b\u5f62\u6001\u548c\u65b0\u7269\u4f53\u5b9e\u4f8b\u4e0a\u7684\u826f\u597d\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2601.08194", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.08194", "abs": "https://arxiv.org/abs/2601.08194", "authors": ["Shakyani Jayasiriwardene", "Hongyu Zhou", "Weiwei Jiang", "Benjamin Tag", "Emmanuel Stamatakis", "Anusha Withana", "Zhanna Sarsenbayeva"], "title": "From Fixed to Flexible: Shaping AI Personality in Context-Sensitive Interaction", "comment": "Preprint", "summary": "Conversational agents are increasingly expected to adapt across contexts and evolve their personalities through interactions, yet most remain static once configured. We present an exploratory study of how user expectations form and evolve when agent personality is made dynamically adjustable. To investigate this, we designed a prototype conversational interface that enabled users to adjust an agent's personality along eight research-grounded dimensions across three task contexts: informational, emotional, and appraisal. We conducted an online mixed-methods study with 60 participants, employing latent profile analysis to characterize personality classes and trajectory analysis to trace evolving patterns of personality adjustment. These approaches revealed distinct personality profiles at initial and final configuration stages, and adjustment trajectories, shaped by context-sensitivity. Participants also valued the autonomy, perceived the agent as more anthropomorphic, and reported greater trust. Our findings highlight the importance of designing conversational agents that adapt alongside their users, advancing more responsive and human-centred AI.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u52a8\u6001\u53ef\u8c03\u7684\u5bf9\u8bdd\u4ee3\u7406\u4e2a\u6027\u5982\u4f55\u5f71\u54cd\u7528\u6237\u671f\u671b\u4e0e\u4fe1\u4efb\uff0c\u5f3a\u8c03\u4e86\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684AI\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u63a2\u8ba8\u7528\u6237\u671f\u5f85\u5728\u5bf9\u8bdd\u4ee3\u7406\u7684\u4e2a\u6027\u53ef\u4ee5\u52a8\u6001\u8c03\u6574\u65f6\u5982\u4f55\u5f62\u6210\u548c\u6f14\u53d8\u3002", "method": "\u8fdb\u884c\u4e86\u4e00\u9879\u5728\u7ebf\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\uff0c\u6d89\u53ca60\u540d\u53c2\u4e0e\u8005\uff0c\u4f7f\u7528\u6f5c\u5728\u6784\u578b\u5206\u6790\u6765\u63cf\u8ff0\u4e2a\u6027\u7c7b\uff0c\u5e76\u4f7f\u7528\u8f68\u8ff9\u5206\u6790\u6765\u8ffd\u8e2a\u4e2a\u6027\u8c03\u6574\u7684\u53d1\u5c55\u6a21\u5f0f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u7684\u4e2a\u6027\u6982\u51b5\u5728\u521d\u59cb\u548c\u6700\u7ec8\u914d\u7f6e\u9636\u6bb5\u663e\u73b0\uff0c\u8c03\u6574\u8f68\u8ff9\u53d7\u5230\u60c5\u5883\u654f\u611f\u6027\u5f71\u54cd\u3002\u53c2\u4e0e\u8005\u5bf9\u5bf9\u8bdd\u4ee3\u7406\u7684\u81ea\u4e3b\u6027\u7ed9\u4e88\u4e86\u9ad8\u5ea6\u8bc4\u4ef7\uff0c\u8ba4\u4e3a\u5176\u66f4\u5177\u4eba\u6027\u5316\uff0c\u5e76\u62a5\u544a\u4e86\u66f4\u9ad8\u7684\u4fe1\u4efb\u611f\u3002", "conclusion": "\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u8bbe\u8ba1\u5e94\u8be5\u80fd\u591f\u968f\u7528\u6237\u8c03\u6574\u7684\u5bf9\u8bdd\u4ee3\u7406\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u63a8\u8fdb\u66f4\u52a0\u54cd\u5e94\u548c\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u4eba\u5de5\u667a\u80fd\u3002"}}
{"id": "2601.08248", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08248", "abs": "https://arxiv.org/abs/2601.08248", "authors": ["Yaohua Liu", "Qiao Xu", "Yemin Wang", "Hui Yi Leong", "Binkai Ou"], "title": "Spiking Neural-Invariant Kalman Fusion for Accurate Localization Using Low-Cost IMUs", "comment": null, "summary": "Low-cost inertial measurement units (IMUs) are widely utilized in mobile robot localization due to their affordability and ease of integration. However, their complex, nonlinear, and time-varying noise characteristics often lead to significant degradation in localization accuracy when applied directly for dead reckoning. To overcome this limitation, we propose a novel brain-inspired state estimation framework that combines a spiking neural network (SNN) with an invariant extended Kalman filter (InEKF). The SNN is designed to extract motion-related features from long sequences of IMU data affected by substantial random noise and is trained via a surrogate gradient descent algorithm to enable dynamic adaptation of the covariance noise parameter within the InEKF. By fusing the SNN output with raw IMU measurements, the proposed method enhances the robustness and accuracy of pose estimation. Extensive experiments conducted on the KITTI dataset and real-world data collected using a mobile robot equipped with a low-cost IMU demonstrate that the proposed approach outperforms state-of-the-art methods in localization accuracy and exhibits strong robustness to sensor noise, highlighting its potential for real-world mobile robot applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u5927\u8111\u7684\u72b6\u6001\u4f30\u8ba1\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u548c\u4e0d\u53d8\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff0c\u4ee5\u63d0\u9ad8\u5ec9\u4ef7\u60ef\u6027\u6d4b\u91cf\u5355\u5143\u5728\u79fb\u52a8\u673a\u5668\u4eba\u5b9a\u4f4d\u4e2d\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f4e\u6210\u672c\u60ef\u6027\u6d4b\u91cf\u5355\u5143\u5728\u79fb\u52a8\u673a\u5668\u4eba\u5b9a\u4f4d\u4e2d\u666e\u904d\u4f7f\u7528\uff0c\u4f46\u56e0\u5176\u590d\u6742\u7684\u975e\u7ebf\u6027\u548c\u65f6\u53d8\u566a\u58f0\u7279\u6027\uff0c\u76f4\u63a5\u7528\u4e8e\u5feb\u901f\u5b9a\u4f4d\u65f6\u4f1a\u4e25\u91cd\u964d\u4f4e\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "method": "\u901a\u8fc7\u5c06\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u4e0e\u4e0d\u53d8\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08InEKF\uff09\u76f8\u7ed3\u5408\uff0cSNN\u4ece\u53d7\u968f\u673a\u566a\u58f0\u5f71\u54cd\u7684IMU\u6570\u636e\u4e2d\u63d0\u53d6\u8fd0\u52a8\u76f8\u5173\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u4ee3\u7406\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u52a8\u6001\u8c03\u6574\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u534f\u65b9\u5dee\u566a\u58f0\u53c2\u6570\u3002", "result": "\u5728KITTI\u6570\u636e\u96c6\u53ca\u4f7f\u7528\u4f4e\u6210\u672cIMU\u7684\u79fb\u52a8\u673a\u5668\u4eba\u5b9e\u9645\u6536\u96c6\u7684\u6570\u636e\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u8868\u73b0\u51fa\u5bf9\u4f20\u611f\u5668\u566a\u58f0\u7684\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u793a\u51fa\u5728\u5b9e\u9645\u79fb\u52a8\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u63d0\u5347\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.08203", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.08203", "abs": "https://arxiv.org/abs/2601.08203", "authors": ["Cassidy R. Nelson"], "title": "Scoping Review: Mental Health XR Games at ISMAR, IEEEVR, & TVCG", "comment": null, "summary": "Extended reality serious games for mental health are a promising research avenue to address the accessibility gap in mental health treatment by bringing therapy to patients in their homes, offering highly adaptable and immersive yet safe therapy opportunities, and increasing motivation and engagement with therapeutic exercises. However, the sensitive use case of mental health demands thoughtful integration with mental health concepts and a comprehensive understanding of prior literature. This paper presents a scoping literature review of the ISMAR, IEEEVR, and TVCG communities to assess the contributions of the XR community to the mental health serious game domain and explore potential weaknesses and strengths for future work by XR researchers. To this end, this review identified 204 possibly relevant articles in the XR community and fully evaluated 6 XR serious games for mental health. This relatively small number of articles for final inclusion suggests that XR mental health serious games are largely underexplored by the XR community (or not reported within the XR community). There is value in exploring the existing literature space as it is. Thus, this paper evaluates these six papers in terms of game elements and underlying psychological foundations, and discuss future directions for XR researchers in this wide-open research space within our community.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u6269\u5c55\u73b0\u5b9e\u4e25\u8083\u6e38\u620f\u5728\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u7684\u5e94\u7528\u8fdb\u884c\u4e86\u6587\u732e\u7efc\u8ff0\uff0c\u53d1\u73b0XR\u793e\u533a\u5bf9\u6b64\u4e3b\u9898\u63a2\u7d22\u6709\u9650\uff0c\u5efa\u8bae\u672a\u6765\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "motivation": "\u5fc3\u7406\u5065\u5eb7\u6cbb\u7597\u7684\u53ef\u53ca\u6027\u5dee\u8ddd\u4fc3\u4f7f\u63a2\u7d22\u6269\u5c55\u73b0\u5b9e\u4e25\u8083\u6e38\u620f\uff0c\u5c06\u6cbb\u7597\u5e26\u5230\u60a3\u8005\u5bb6\u4e2d\uff0c\u5e76\u63d0\u5347\u6cbb\u7597\u52a8\u529b\u548c\u53c2\u4e0e\u5ea6\u3002", "method": "\u8fdb\u884c\u6587\u732e\u7efc\u8ff0\uff0c\u8bc4\u4f30XR\u793e\u533a\u5728\u5fc3\u7406\u5065\u5eb7\u4e25\u8083\u6e38\u620f\u9886\u57df\u7684\u8d21\u732e\uff0c\u8bc6\u522b204\u7bc7\u76f8\u5173\u6587\u732e\uff0c\u6df1\u5165\u5206\u67906\u6b3eXR\u4e25\u8083\u6e38\u620f\u3002", "result": "\u53d1\u73b0XR\u5fc3\u7406\u5065\u5eb7\u4e25\u8083\u6e38\u620f\u7684\u76f8\u5173\u7814\u7a76\u6570\u91cf\u76f8\u5bf9\u8f83\u5c11\uff0c\u8868\u660e\u8be5\u9886\u57df\u4ecd\u5f85\u63a2\u7d22\u3002", "conclusion": "\u8bc4\u4f30\u7684\u6e38\u620f\u5143\u7d20\u548c\u5fc3\u7406\u57fa\u7840\u4e3aXR\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5f3a\u5316\u4e86\u8be5\u9886\u57df\u7684\u63a2\u7d22\u4ef7\u503c\u3002"}}
{"id": "2601.08325", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08325", "abs": "https://arxiv.org/abs/2601.08325", "authors": ["Zhenyang Liu", "Yongchong Gu", "Yikai Wang", "Xiangyang Xue", "Yanwei Fu"], "title": "ActiveVLA: Injecting Active Perception into Vision-Language-Action Models for Precise 3D Robotic Manipulation", "comment": null, "summary": "Recent advances in robot manipulation have leveraged pre-trained vision-language models (VLMs) and explored integrating 3D spatial signals into these models for effective action prediction, giving rise to the promising vision-language-action (VLA) paradigm. However, most existing approaches overlook the importance of active perception: they typically rely on static, wrist-mounted cameras that provide an end-effector-centric viewpoint. As a result, these models are unable to adaptively select optimal viewpoints or resolutions during task execution, which significantly limits their performance in long-horizon tasks and fine-grained manipulation scenarios. To address these limitations, we propose ActiveVLA, a novel vision-language-action framework that empowers robots with active perception capabilities for high-precision, fine-grained manipulation. ActiveVLA adopts a coarse-to-fine paradigm, dividing the process into two stages: (1) Critical region localization. ActiveVLA projects 3D inputs onto multi-view 2D projections, identifies critical 3D regions, and supports dynamic spatial awareness. (2) Active perception optimization. Drawing on the localized critical regions, ActiveVLA uses an active view selection strategy to choose optimal viewpoints. These viewpoints aim to maximize amodal relevance and diversity while minimizing occlusions. Additionally, ActiveVLA applies a 3D zoom-in to improve resolution in key areas. Together, these steps enable finer-grained active perception for precise manipulation. Extensive experiments demonstrate that ActiveVLA achieves precise 3D manipulation and outperforms state-of-the-art baselines on three simulation benchmarks. Moreover, ActiveVLA transfers seamlessly to real-world scenarios, enabling robots to learn high-precision tasks in complex environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6-\u8bed\u8a00-\u884c\u52a8\u6846\u67b6ActiveVLA\uff0c\u901a\u8fc7\u4e3b\u52a8\u611f\u77e5\u80fd\u529b\u63d0\u5347\u673a\u5668\u4eba\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u9ad8\u7cbe\u5ea6\u64cd\u4f5c\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u91cd\u89c6\u4e3b\u52a8\u611f\u77e5\uff0c\u5bf9\u9759\u6001\u6444\u50cf\u5934\u7684\u4f9d\u8d56\u9650\u5236\u4e86\u957f\u65f6\u95f4\u4efb\u52a1\u548c\u7cbe\u7ec6\u64cd\u4f5c\u7684\u8868\u73b0\u3002", "method": "ActiveVLA\u6846\u67b6\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a1) \u5173\u952e\u533a\u57df\u5b9a\u4f4d\uff0c2) \u4e3b\u52a8\u611f\u77e5\u4f18\u5316\uff0c\u4f7f\u7528\u52a8\u6001\u89c6\u89d2\u9009\u62e9\u548c3D\u653e\u5927\u6280\u672f\u3002", "result": "ActiveVLA\u5728\u4e09\u4e2a\u4eff\u771f\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6210\u529f\u5b9e\u73b0\u7cbe\u786e\u76843D\u64cd\u4f5c\u3002", "conclusion": "ActiveVLA\u80fd\u65e0\u7f1d\u8f6c\u79fb\u5230\u771f\u5b9e\u573a\u666f\uff0c\u5e2e\u52a9\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b66\u4e60\u9ad8\u7cbe\u5ea6\u4efb\u52a1\u3002"}}
{"id": "2601.08256", "categories": ["cs.HC", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.08256", "abs": "https://arxiv.org/abs/2601.08256", "authors": ["Yilan Jiang", "Cindy Xiong Bearfield", "Steven Franconeri", "Eugene Wu"], "title": "Data-Induced Groupings and How To Find Them", "comment": null, "summary": "Making sense of a visualization requires the reader to consider both the visualization design and the underlying data values. Existing work in the visualization community has largely considered affordances driven by visualization design elements, such as color or chart type, but how visual design interacts with data values to impact interpretation and reasoning has remained under-explored. Dot plots and bar graphs are commonly used to help users identify groups of points that form trends and clusters, but are liable to manifest groupings that are artifacts of spatial arrangement rather than inherent patterns in the data itself. These ``Data-induced Groups'' can drive suboptimal data comparisons and potentially lead the user to incorrect conclusions. We conduct two user studies using dot plots as a case study to understand the prevalence of data-induced groupings. We find that users rely on data-induced groupings in both conditions despite the fact that trend-based groupings are irrelevant in nominal data. Based on the study results, we build a model to predict whether users are likely to perceive a given set of dot plot points as a group. We discuss two use cases illustrating how the model can assist visualization designers by both diagnosing potential user-perceived groupings in dot plots and offering redesigns that better accentuate desired groupings through data rearrangement.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u53ef\u89c6\u5316\u8bbe\u8ba1\u4e0e\u6570\u636e\u503c\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u63d0\u51fa\u4e86\u8bc6\u522b\u548c\u6539\u5584\u7528\u6237\u5bf9\u6570\u636e\u5206\u7ec4\u7406\u89e3\u7684\u6a21\u578b\u3002", "motivation": "\u63a2\u8ba8\u53ef\u89c6\u5316\u8bbe\u8ba1\u4e0e\u6570\u636e\u503c\u5982\u4f55\u76f8\u4e92\u5f71\u54cd\uff0c\u4ee5\u6539\u5584\u7528\u6237\u5bf9\u6570\u636e\u7684\u89e3\u91ca\u4e0e\u63a8\u7406\u80fd\u529b\u3002", "method": "\u8fdb\u884c\u4e24\u9879\u7528\u6237\u7814\u7a76\uff0c\u4f7f\u7528\u70b9\u56fe\u4f5c\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u4ee5\u8bc6\u522b\u6570\u636e\u8bf1\u5bfc\u7684\u5206\u7ec4\u73b0\u8c61\u3002\u6784\u5efa\u6a21\u578b\u9884\u6d4b\u7528\u6237\u5982\u4f55\u770b\u5f85\u70b9\u56fe\u4e2d\u7684\u70b9\u5206\u7ec4\u3002", "result": "\u5373\u4f7f\u5728\u540d\u4e49\u6570\u636e\u4e2d\uff0c\u7528\u6237\u5728\u4e24\u79cd\u6761\u4ef6\u4e0b\u4ecd\u4f9d\u8d56\u6570\u636e\u8bf1\u5bfc\u7684\u5206\u7ec4\uff0c\u5c3d\u7ba1\u8fd9\u4e9b\u5206\u7ec4\u4e0e\u8d8b\u52bf\u65e0\u5173\u3002", "conclusion": "\u6a21\u578b\u53ef\u4ee5\u5e2e\u52a9\u53ef\u89c6\u5316\u8bbe\u8ba1\u5e08\u8bc6\u522b\u6f5c\u5728\u7684\u7528\u6237\u611f\u77e5\u5206\u7ec4\uff0c\u5e76\u63d0\u4f9b\u66f4\u597d\u7684\u8bbe\u8ba1\u5efa\u8bae\u4ee5\u7a81\u663e\u671f\u671b\u7684\u5206\u7ec4\u3002"}}
{"id": "2601.08327", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08327", "abs": "https://arxiv.org/abs/2601.08327", "authors": ["Gabriele Calzolari", "Vidya Sumathy", "Christoforos Kanellakis", "George Nikolakopoulos"], "title": "Safe Heterogeneous Multi-Agent RL with Communication Regularization for Coordinated Target Acquisition", "comment": "7 pages, 4 figures, submitted to the IFAC World Congress 2026", "summary": "This paper introduces a decentralized multi-agent reinforcement learning framework enabling structurally heterogeneous teams of agents to jointly discover and acquire randomly located targets in environments characterized by partial observability, communication constraints, and dynamic interactions. Each agent's policy is trained with the Multi-Agent Proximal Policy Optimization algorithm and employs a Graph Attention Network encoder that integrates simulated range-sensing data with communication embeddings exchanged among neighboring agents, enabling context-aware decision-making from both local sensing and relational information. In particular, this work introduces a unified framework that integrates graph-based communication and trajectory-aware safety through safety filters. The architecture is supported by a structured reward formulation designed to encourage effective target discovery and acquisition, collision avoidance, and de-correlation between the agents' communication vectors by promoting informational orthogonality. The effectiveness of the proposed reward function is demonstrated through a comprehensive ablation study. Moreover, simulation results demonstrate safe and stable task execution, confirming the framework's effectiveness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u7684\u591a\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4ee5\u652f\u6301\u5728\u590d\u6742\u73af\u5883\u4e2d\u56e2\u961f\u534f\u4f5c\uff0c\u5b9e\u73b0\u76ee\u6807\u53d1\u73b0\u548c\u83b7\u53d6\u3002", "motivation": "\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u3001\u901a\u4fe1\u53d7\u9650\u548c\u52a8\u6001\u4ea4\u4e92\u7684\u73af\u5883\u4e2d\uff0c\u4ee3\u7406\u9700\u8981\u6709\u6548\u5730\u5171\u540c\u53d1\u73b0\u548c\u83b7\u53d6\u76ee\u6807\u3002", "method": "\u91c7\u7528\u53bb\u4e2d\u5fc3\u5316\u7684\u591a\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u591a\u4ee3\u7406\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\u548c\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u7f16\u7801\u5668\u3002", "result": "\u901a\u8fc7\u5168\u9762\u7684\u6d88\u878d\u7814\u7a76\u8bc1\u660e\u4e86\u63d0\u51fa\u7684\u5956\u52b1\u51fd\u6570\u7684\u6709\u6548\u6027\uff0c\u4eff\u771f\u7ed3\u679c\u663e\u793a\u4efb\u52a1\u6267\u884c\u5b89\u5168\u7a33\u5b9a\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u652f\u6301\u7ed3\u6784\u5f02\u8d28\u56e2\u961f\u7684\u534f\u540c\u76ee\u6807\u53d1\u73b0\u548c\u83b7\u53d6\u3002"}}
{"id": "2601.08287", "categories": ["cs.HC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.08287", "abs": "https://arxiv.org/abs/2601.08287", "authors": ["Jiaman He", "Marta Micheli", "Damiano Spina", "Dana McKay", "Johanne R. Trippas", "Noriko Kando"], "title": "Characterizing Personality from Eye-Tracking: The Role of Gaze and Its Absence in Interactive Search Environments", "comment": "This paper is accepted at CHIIR 2026", "summary": "Personality traits influence how individuals engage, behave, and make decisions during the information-seeking process. However, few studies have linked personality to observable search behaviors. This study aims to characterize personality traits through a multimodal time-series model that integrates eye-tracking data and gaze missingness-periods when the user's gaze is not captured. This approach is based on the idea that people often look away when they think, signaling disengagement or reflection. We conducted a user study with 25 participants, who used an interactive application on an iPad, allowing them to engage with digital artifacts from a museum. We rely on raw gaze data from an eye tracker, minimizing preprocessing so that behavioral patterns can be preserved without substantial data cleaning. From this perspective, we trained models to predict personality traits using gaze signals. Our results from a five-fold cross-validation study demonstrate strong predictive performance across all five dimensions: Neuroticism (Macro F1 = 77.69%), Conscientiousness (74.52%), Openness (77.52%), Agreeableness (73.09%), and Extraversion (76.69%). The ablation study examines whether the absence of gaze information affects the model performance, demonstrating that incorporating missingness improves multimodal time-series modeling. The full model, which integrates both time-series signals and missingness information, achieves 10-15% higher accuracy and macro F1 scores across all Big Five traits compared to the model without time-series signals and missingness. These findings provide evidence that personality can be inferred from search-related gaze behavior and demonstrate the value of incorporating missing gaze data into time-series multimodal modeling.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u773c\u52a8\u8ddf\u8e2a\u6570\u636e\u548c\u89c6\u7ebf\u7f3a\u5931\u671f\u7684\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\uff0c\u63a2\u7d22\u4e2a\u6027\u7279\u5f81\u4e0e\u641c\u7d22\u884c\u4e3a\u7684\u5173\u8054\uff0c\u7ed3\u679c\u8868\u660e\u7ed3\u5408\u7f3a\u5931\u4fe1\u606f\u80fd\u663e\u8457\u63d0\u9ad8\u4e2a\u6027\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5c3d\u7ba1\u4e2a\u6027\u7279\u5f81\u5f71\u54cd\u4e2a\u4eba\u4fe1\u606f\u83b7\u53d6\u8fc7\u7a0b\uff0c\u4f46\u5f88\u5c11\u6709\u7814\u7a76\u5c06\u4e2a\u6027\u4e0e\u53ef\u89c2\u5bdf\u7684\u641c\u7d22\u884c\u4e3a\u8054\u7cfb\u8d77\u6765\uff0c\u56e0\u6b64\u9700\u8981\u5bf9\u6b64\u8fdb\u884c\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\uff0c\u7ed3\u5408\u773c\u52a8\u8ddf\u8e2a\u6570\u636e\u548c\u7528\u6237\u89c6\u7ebf\u7f3a\u5931\u65f6\u95f4\u6bb5\uff0c\u5bf9\u4e2a\u6027\u7279\u5f81\u8fdb\u884c\u5efa\u6a21\u548c\u9884\u6d4b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7\u7ed3\u5408\u65f6\u95f4\u5e8f\u5217\u4fe1\u53f7\u548c\u7f3a\u5931\u4fe1\u606f\u7684\u5b8c\u6574\u6a21\u578b\uff0c\u5728\u9884\u6d4b\u4e94\u5927\u4e2a\u6027\u7279\u5f81\u7684\u51c6\u786e\u6027\u548c\u5b8f\u89c2F1\u5206\u6570\u4e0a\u63d0\u9ad8\u4e8610-15%\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u548c\u4e2a\u6027\u63a8\u65ad\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "\u4e2a\u6027\u53ef\u4ee5\u901a\u8fc7\u641c\u7d22\u76f8\u5173\u7684\u51dd\u89c6\u884c\u4e3a\u63a8\u65ad\uff0c\u5e76\u4e14\u5c06\u7f3a\u5931\u7684\u51dd\u89c6\u6570\u636e\u7eb3\u5165\u65f6\u95f4\u5e8f\u5217\u591a\u6a21\u6001\u5efa\u6a21\u4e2d\u5177\u6709\u4ef7\u503c\u3002"}}
{"id": "2601.08405", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.08405", "abs": "https://arxiv.org/abs/2601.08405", "authors": ["Yizhan Feng", "Hichem Snoussi", "Jing Teng", "Abel Cherouat", "Tian Wang"], "title": "Large Language Models to Enhance Multi-task Drone Operations in Simulated Environments", "comment": "1st International Conference on Drones and Unmanned Systems (DAUS' 2025)", "summary": "Benefiting from the rapid advancements in large language models (LLMs), human-drone interaction has reached unprecedented opportunities. In this paper, we propose a method that integrates a fine-tuned CodeT5 model with the Unreal Engine-based AirSim drone simulator to efficiently execute multi-task operations using natural language commands. This approach enables users to interact with simulated drones through prompts or command descriptions, allowing them to easily access and control the drone's status, significantly lowering the operational threshold. In the AirSim simulator, we can flexibly construct visually realistic dynamic environments to simulate drone applications in complex scenarios. By combining a large dataset of (natural language, program code) command-execution pairs generated by ChatGPT with developer-written drone code as training data, we fine-tune the CodeT5 to achieve automated translation from natural language to executable code for drone tasks. Experimental results demonstrate that the proposed method exhibits superior task execution efficiency and command understanding capabilities in simulated environments. In the future, we plan to extend the model functionality in a modular manner, enhancing its adaptability to complex scenarios and driving the application of drone technologies in real-world environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5fae\u8c03\u7684CodeT5\u4e0eAirSim\u6a21\u62df\u5668\u7ed3\u5408\uff0c\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u5230\u53ef\u6267\u884c\u4ee3\u7801\u7684\u8f6c\u6362\uff0c\u63d0\u9ad8\u65e0\u4eba\u673a\u64cd\u4f5c\u6548\u7387\u3002", "motivation": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u6539\u5584\u4eba\u673a\u65e0\u4eba\u673a\u4ea4\u4e92\u7684\u673a\u4f1a\uff0c\u964d\u4f4e\u64cd\u4f5c\u95e8\u69db\uff0c\u5e76\u589e\u5f3a\u65e0\u4eba\u673a\u6280\u672f\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5c06\u5fae\u8c03\u7684CodeT5\u6a21\u578b\u4e0e\u57fa\u4e8eUnreal Engine\u7684AirSim\u65e0\u4eba\u673a\u6a21\u62df\u5668\u96c6\u6210\uff0c\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u547d\u4ee4\u6709\u6548\u6267\u884c\u591a\u4efb\u52a1\u64cd\u4f5c\u3002", "result": "\u5728\u6a21\u62df\u73af\u5883\u4e2d\uff0c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u4efb\u52a1\u6267\u884c\u6548\u7387\u548c\u547d\u4ee4\u7406\u89e3\u80fd\u529b\u65b9\u9762\u7684\u4f18\u8d8a\u8868\u73b0\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u964d\u4f4e\u4e86\u64cd\u4f5c\u95e8\u69db\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u901a\u8fc7\u63d0\u793a\u6216\u547d\u4ee4\u63cf\u8ff0\u4e0e\u6a21\u62df\u65e0\u4eba\u673a\u4ea4\u4e92\uff0c\u5e76\u8ba1\u5212\u4ee5\u6a21\u5757\u5316\u65b9\u5f0f\u6269\u5c55\u6a21\u578b\u529f\u80fd\u3002"}}
{"id": "2601.08565", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08565", "abs": "https://arxiv.org/abs/2601.08565", "authors": ["Sitong Wang", "Anh Truong", "Lydia B. Chilton", "Dingzeyu Li"], "title": "Rewriting Video: Text-Driven Reauthoring of Video Footage", "comment": null, "summary": "Video is a powerful medium for communication and storytelling, yet reauthoring existing footage remains challenging. Even simple edits often demand expertise, time, and careful planning, constraining how creators envision and shape their narratives. Recent advances in generative AI suggest a new paradigm: what if editing a video were as straightforward as rewriting text? To investigate this, we present a tech probe and a study on text-driven video reauthoring. Our approach involves two technical contributions: (1) a generative reconstruction algorithm that reverse-engineers video into an editable text prompt, and (2) an interactive probe, Rewrite Kit, that allows creators to manipulate these prompts. A technical evaluation of the algorithm reveals a critical human-AI perceptual gap. A probe study with 12 creators surfaced novel use cases such as virtual reshooting, synthetic continuity, and aesthetic restyling. It also highlighted key tensions around coherence, control, and creative alignment in this new paradigm. Our work contributes empirical insights into the opportunities and challenges of text-driven video reauthoring, offering design implications for future co-creative video tools.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u751f\u6210AI\u7b80\u5316\u89c6\u9891\u7f16\u8f91\uff0c\u901a\u8fc7\u6587\u672c\u63d0\u793a\u91cd\u65b0\u521b\u4f5c\u89c6\u9891\uff0c\u63ed\u793a\u4e86\u4eba\u673a\u611f\u77e5\u5dee\u8ddd\u53ca\u521b\u4f5c\u4e2d\u7684\u5f20\u529b\uff0c\u4e3a\u672a\u6765\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6d1e\u89c1\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7f16\u8f91\u590d\u6742\u4e14\u8017\u65f6\uff0c\u751f\u6210AI\u6280\u672f\u8868\u660e\u53ef\u4ee5\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u8ba9\u89c6\u9891\u7f16\u8f91\u50cf\u6587\u672c\u91cd\u5199\u4e00\u6837\u7b80\u5355\u3002", "method": "\u901a\u8fc7\u6280\u672f\u63a2\u6d4b\u5668\u548c\u7814\u7a76\u8fdb\u884c\u6587\u672c\u9a71\u52a8\u7684\u89c6\u9891\u91cd\u65b0\u521b\u4f5c", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u91cd\u5efa\u7b97\u6cd5\u548c\u4e92\u52a8\u5de5\u5177Rewrite Kit\uff0c\u652f\u6301\u521b\u4f5c\u8005\u901a\u8fc7\u6587\u672c\u63d0\u793a\u64cd\u63a7\u89c6\u9891\u5185\u5bb9\u3002", "conclusion": "\u6587\u672c\u9a71\u52a8\u7684\u89c6\u9891\u91cd\u65b0\u521b\u4f5c\u5b58\u5728\u4eba\u673a\u611f\u77e5\u5dee\u8ddd\uff0c\u540c\u65f6\u5448\u73b0\u51fa\u65b0\u673a\u9047\u4e0e\u6311\u6218\uff0c\u9700\u8bbe\u8ba1\u672a\u6765\u7684\u5408\u4f5c\u521b\u4f5c\u89c6\u9891\u5de5\u5177\u3002"}}
{"id": "2601.08422", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08422", "abs": "https://arxiv.org/abs/2601.08422", "authors": ["Taerim Yoon", "Dongho Kang", "Jin Cheng", "Fatemeh Zargarbashi", "Yijiang Huang", "Minsung Ahn", "Stelian Coros", "Sungjoon Choi"], "title": "Teaching Robots Like Dogs: Learning Agile Navigation from Luring, Gesture, and Speech", "comment": "10 pages, 7 figures", "summary": "In this work, we aim to enable legged robots to learn how to interpret human social cues and produce appropriate behaviors through physical human guidance. However, learning through physical engagement can place a heavy burden on users when the process requires large amounts of human-provided data. To address this, we propose a human-in-the-loop framework that enables robots to acquire navigational behaviors in a data-efficient manner and to be controlled via multimodal natural human inputs, specifically gestural and verbal commands. We reconstruct interaction scenes using a physics-based simulation and aggregate data to mitigate distributional shifts arising from limited demonstration data. Our progressive goal cueing strategy adaptively feeds appropriate commands and navigation goals during training, leading to more accurate navigation and stronger alignment between human input and robot behavior. We evaluate our framework across six real-world agile navigation scenarios, including jumping over or avoiding obstacles. Our experimental results show that our proposed method succeeds in almost all trials across these scenarios, achieving a 97.15% task success rate with less than 1 hour of demonstration data in total.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4eba\u673a\u534f\u4f5c\u6846\u67b6\uff0c\u4f7f\u56db\u8db3\u673a\u5668\u4eba\u53ef\u4ee5\u901a\u8fc7\u7269\u7406\u4eba\u7c7b\u5f15\u5bfc\u5b66\u4e60\u89e3\u8bfb\u4eba\u9645\u793e\u4f1a\u4fe1\u53f7\u5e76\u6267\u884c\u9002\u5f53\u884c\u4e3a\uff0c\u5728\u5bfc\u822a\u884c\u4e3a\u5b66\u4e60\u4e2d\u5b9e\u73b0\u6570\u636e\u9ad8\u6548\u6027\uff0c\u540c\u65f6\u517c\u5bb9\u591a\u6a21\u5f0f\u81ea\u7136\u4eba\u7c7b\u8f93\u5165\u3002", "motivation": "\u7075\u6d3b\u7684\u56db\u8db3\u673a\u5668\u4eba\u9700\u8981\u5b66\u4e60\u5982\u4f55\u89e3\u8bfb\u4eba\u7c7b\u7684\u793e\u4f1a\u4fe1\u53f7\uff0c\u4ee5\u4fbf\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e2d\u4e0e\u4eba\u7c7b\u6709\u6548\u4e92\u52a8\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4eba\u673a\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u7269\u7406\u5f15\u5bfc\u5b66\u4e60\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u7269\u7406\u7684\u4eff\u771f\u91cd\u6784\u4ea4\u4e92\u573a\u666f\uff0c\u7ed3\u5408\u8fdb\u9636\u7684\u76ee\u6807\u63d0\u793a\u7b56\u7565\uff0c\u9002\u5e94\u6027\u5730\u63d0\u4f9b\u547d\u4ee4\u548c\u5bfc\u822a\u76ee\u6807\u3002", "result": "\u5728\u516d\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u7075\u6d3b\u5bfc\u822a\u573a\u666f\u4e2d\u8fdb\u884c\u8bc4\u4f30\uff0c\u65b9\u6cd5\u5728\u51e0\u4e4e\u6240\u6709\u8bd5\u9a8c\u4e2d\u6210\u529f\uff0c\u4efb\u52a1\u6210\u529f\u7387\u4e3a97.15%\uff0c\u6240\u9700\u6f14\u793a\u6570\u636e\u5c11\u4e8e1\u5c0f\u65f6\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5bf9\u4eba\u7c7b\u8f93\u5165\u7684\u54cd\u5e94\u80fd\u529b\u548c\u5bfc\u822a\u51c6\u786e\u6027\uff0c\u5c55\u793a\u4e86\u9ad8\u6570\u636e\u6548\u7387\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.08640", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.08640", "abs": "https://arxiv.org/abs/2601.08640", "authors": ["Phuong Lien To"], "title": "Enhancing Financial Literacy and Management through Goal-Directed Design and Gamification in Personal Finance Application", "comment": "This is the author accepted manuscript (AAM) of a paper accepted for publication in Springer's Lecture Notes in Computer Science (LNCS) as part of the HCI International 2025 Conference Proceedings , published by Springer. This version has been peer reviewed but does not include the final publisher formatting or typesetting", "summary": "This study explores the development of a financial management application for young people using Alan Cooper's Goal-Directed Design method. Through interviews, surveys, and usability testing, the application was designed to improve financial literacy by combining personalised features and gamification. Findings highlight the effectiveness of gamified learning and tailored experiences in encouraging better financial behaviour among young users.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7Alan Cooper\u7684\u8bbe\u8ba1\u65b9\u6cd5\u5f00\u53d1\u4e86\u4e00\u6b3e\u8d22\u52a1\u7ba1\u7406\u5e94\u7528\uff0c\u4ee5\u63d0\u9ad8\u5e74\u8f7b\u4eba\u7684\u8d22\u52a1\u7d20\u517b\uff0c\u7ed3\u5408\u4e2a\u6027\u5316\u529f\u80fd\u4e0e\u6e38\u620f\u5316\u5143\u7d20\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6709\u6548\u7684\u5de5\u5177\u63d0\u5347\u5e74\u8f7b\u4eba\u7684\u8d22\u52a1\u7ba1\u7406\u80fd\u529b\uff0c\u4ee5\u5e94\u5bf9\u73b0\u4ee3\u793e\u4f1a\u5bf9\u8d22\u52a1\u7d20\u517b\u7684\u9700\u6c42\u3002", "method": "\u7814\u7a76\u91c7\u7528\u8bbf\u8c08\u3001\u95ee\u5377\u548c\u53ef\u7528\u6027\u6d4b\u8bd5\u7b49\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u51fa\u7b26\u5408\u5e74\u8f7b\u4eba\u9700\u6c42\u7684\u8d22\u52a1\u7ba1\u7406\u5e94\u7528\u3002", "result": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u6b3e\u65e8\u5728\u63d0\u5347\u5e74\u8f7b\u4eba\u8d22\u52a1\u7ba1\u7406\u80fd\u529b\u7684\u5e94\u7528\u7a0b\u5e8f\uff0c\u91c7\u7528\u4e86Alan Cooper\u7684\u76ee\u6807\u5bfc\u5411\u8bbe\u8ba1\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u51f8\u663e\u4e86\u6e38\u620f\u5316\u5b66\u4e60\u548c\u4e2a\u6027\u5316\u4f53\u9a8c\u5728\u4fc3\u8fdb\u5e74\u8f7b\u7528\u6237\u6539\u5584\u8d22\u52a1\u884c\u4e3a\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.08434", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08434", "abs": "https://arxiv.org/abs/2601.08434", "authors": ["Long Zhang", "Yuchen Xia"], "title": "Large Multimodal Models for Embodied Intelligent Driving: The Next Frontier in Self-Driving?", "comment": null, "summary": "The advent of Large Multimodal Models (LMMs) offers a promising technology to tackle the limitations of modular design in autonomous driving, which often falters in open-world scenarios requiring sustained environmental understanding and logical reasoning. Besides, embodied artificial intelligence facilitates policy optimization through closed-loop interactions to achieve the continuous learning capability, thereby advancing autonomous driving toward embodied intelligent (El) driving. However, such capability will be constrained by relying solely on LMMs to enhance EI driving without joint decision-making. This article introduces a novel semantics and policy dual-driven hybrid decision framework to tackle this challenge, ensuring continuous learning and joint decision. The framework merges LMMs for semantic understanding and cognitive representation, and deep reinforcement learning (DRL) for real-time policy optimization. We starts by introducing the foundational principles of EI driving and LMMs. Moreover, we examine the emerging opportunities this framework enables, encompassing potential benefits and representative use cases. A case study is conducted experimentally to validate the performance superiority of our framework in completing lane-change planning task. Finally, several future research directions to empower EI driving are identified to guide subsequent work.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u4e0e\u7b56\u7565\u53cc\u9a71\u52a8\u6df7\u5408\u51b3\u7b56\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u81ea\u4e3b\u9a7e\u9a76\u4e2d\u7684\u6301\u7eed\u5b66\u4e60\u548c\u8054\u5408\u51b3\u7b56\u6311\u6218\uff0c\u7ed3\u5408LMMs\u548cDRL\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u8f66\u9053\u53d8\u6362\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u9762\u5bf9\u5f53\u524d\u6a21\u5757\u5316\u8bbe\u8ba1\u5728\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\u9762\u4e34\u7684\u5c40\u9650\u6027\uff0c\u4e9f\u9700\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u652f\u6301\u6301\u7eed\u73af\u5883\u7406\u89e3\u548c\u903b\u8f91\u63a8\u7406\uff0c\u4ee5\u63a8\u52a8\u81ea\u4e3b\u9a7e\u9a76\u5411\u4f53\u73b0\u667a\u80fd\u7684\u53d1\u5c55\u3002", "method": "\u8be5\u6846\u67b6\u878d\u5408\u4e86\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u6765\u5b9e\u73b0\u8bed\u4e49\u7406\u89e3\u548c\u5b9e\u65f6\u7b56\u7565\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u672c\u6846\u67b6\u5728\u8f66\u9053\u53d8\u6362\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u4f18\u8d8a\u6027\uff0c\u5c55\u793a\u4e86\u8054\u5408\u51b3\u7b56\u7684\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u8bed\u4e49\u4e0e\u7b56\u7565\u53cc\u9a71\u52a8\u6df7\u5408\u51b3\u7b56\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u81ea\u4e3b\u9a7e\u9a76\u7684\u80fd\u529b\uff0c\u5c24\u5176\u5728\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\u3002"}}
{"id": "2601.08652", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.08652", "abs": "https://arxiv.org/abs/2601.08652", "authors": ["Elia Moscoso-Thompson", "Katia Lupinetti", "Irene Capasso", "Fabrizio Ravicchio", "Brigida Bonino", "Franca Giannini", "Andrea Canessa", "Silvio Sabatini", "Lucia Ferlino", "Chiara Malagoli"], "title": "Tailored Immersive Environments: Advancing Neurodivergent Support Through Virtual Reality", "comment": null, "summary": "Every day life tasks can present significant challenges for neurodivergent individuals, particularly those with Autism Spectrum Disorders (ASD) who are characterized by specific sensitivities. This contribution describes a virtual reality system that allows neurodivergent individuals to experience everyday situations in order to practice and implement strategies for overcoming their daily challenges. The key strength of the proposed system is the automatic personalization of the virtual environment, based on both the individual's abilities and their specific training needs. The proposed method has been evaluated on four synthetic user profiles, also proposing a metric able to evaluate the variance of the features within the same difficulty level. The results show that the method can produce a significant number of scenarios for the various difficulty levels. Furthermore, within the same difficulty, there is a wide variance of the non-constrained features for the specific profile.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u865a\u62df\u73b0\u5b9e\u7cfb\u7edf\uff0c\u65e8\u5728\u5e2e\u52a9\u795e\u7ecf\u591a\u6837\u6027\u4e2a\u4f53\uff08\u7279\u522b\u662f\u81ea\u95ed\u75c7\u8c31\u7cfb\u969c\u788d\u8005\uff09\u901a\u8fc7\u4e2a\u6027\u5316\u7684\u73af\u5883\u7ec3\u4e60\u65e5\u5e38\u4efb\u52a1\uff0c\u5e76\u7814\u7a76\u4e86\u5176\u5728\u4e0d\u540c\u96be\u5ea6\u6c34\u5e73\u4e0b\u7684\u5e94\u7528\u3002", "motivation": "\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\uff0c\u81ea\u95ed\u75c7\u8c31\u7cfb\u969c\u788d\uff08ASD\uff09\u4e2a\u4f53\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u8bad\u7ec3\u5de5\u5177\u6765\u5e2e\u52a9\u4ed6\u4eec\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u865a\u62df\u73b0\u5b9e\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u6839\u636e\u7528\u6237\u7684\u7279\u5b9a\u80fd\u529b\u548c\u8bad\u7ec3\u9700\u6c42\u81ea\u52a8\u4e2a\u6027\u5316\u865a\u62df\u73af\u5883\uff0c\u5e76\u5728\u56db\u4e2a\u5408\u6210\u7528\u6237\u6863\u6848\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u540c\u4e00\u96be\u5ea6\u6c34\u5e73\u5185\u80fd\u591f\u751f\u6210\u5927\u91cf\u4e0d\u540c\u573a\u666f\uff0c\u4e14\u7279\u5b9a\u6863\u6848\u4e0b\u7684\u975e\u7ea6\u675f\u7279\u5f81\u65b9\u5dee\u8f83\u5927\uff0c\u663e\u793a\u51fa\u7cfb\u7edf\u7684\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u865a\u62df\u73b0\u5b9e\u7cfb\u7edf\u80fd\u591f\u6839\u636e\u795e\u7ecf\u591a\u6837\u6027\u4e2a\u4f53\u7684\u80fd\u529b\u548c\u8bad\u7ec3\u9700\u6c42\u8fdb\u884c\u81ea\u52a8\u4e2a\u6027\u5316\uff0c\u4e3a\u514b\u670d\u65e5\u5e38\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u652f\u6301\u3002"}}
{"id": "2601.08454", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08454", "abs": "https://arxiv.org/abs/2601.08454", "authors": ["Alessandro Adami", "Sebastian Zudaire", "Ruggero Carli", "Pietro Falco"], "title": "Real2Sim based on Active Perception with automatically VLM-generated Behavior Trees", "comment": null, "summary": "Constructing an accurate simulation model of real-world environments requires reliable estimation of physical parameters such as mass, geometry, friction, and contact surfaces. Traditional real-to-simulation (Real2Sim) pipelines rely on manual measurements or fixed, pre-programmed exploration routines, which limit their adaptability to varying tasks and user intents. This paper presents a Real2Sim framework that autonomously generates and executes Behavior Trees for task-specific physical interactions to acquire only the parameters required for a given simulation objective, without relying on pre-defined task templates or expert-designed exploration routines. Given a high-level user request, an incomplete simulation description, and an RGB observation of the scene, a vision-language model performs multi-modal reasoning to identify relevant objects, infer required physical parameters, and generate a structured Behavior Tree composed of elementary robotic actions. The resulting behavior is executed on a torque-controlled Franka Emika Panda, enabling compliant, contact-rich interactions for parameter estimation. The acquired measurements are used to automatically construct a physics-aware simulation. Experimental results on the real manipulator demonstrate estimation of object mass, surface height, and friction-related quantities across multiple scenarios, including occluded objects and incomplete prior models. The proposed approach enables interpretable, intent-driven, and autonomously Real2Sim pipelines, bridging high-level reasoning with physically-grounded robotic interaction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u4e3b\u751f\u6210\u548c\u6267\u884c\u884c\u4e3a\u6811\u7684Real2Sim\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u63a8\u7406\u8fdb\u884c\u7269\u7406\u53c2\u6570\u7684\u9ad8\u6548\u4f30\u8ba1\uff0c\u4ece\u800c\u6539\u8fdb\u4f20\u7edf\u7684\u73b0\u5b9e\u5230\u4eff\u771f\u6d41\u7a0b\u3002", "motivation": "\u9700\u8981\u53ef\u9760\u4f30\u8ba1\u7269\u7406\u53c2\u6570\u4ee5\u6784\u5efa\u771f\u5b9e\u73af\u5883\u7684\u51c6\u786e\u4eff\u771f\u6a21\u578b\uff0c\u800c\u4f20\u7edf\u65b9\u6cd5\u53d7\u9650\u4e8e\u624b\u52a8\u6d4b\u91cf\u548c\u9884\u5b9a\u4e49\u6d41\u7a0b\u3002", "method": "\u91c7\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u591a\u6a21\u6001\u63a8\u7406\uff0c\u6839\u636e\u7528\u6237\u8bf7\u6c42\u548c\u73af\u5883\u89c2\u5bdf\u751f\u6210\u884c\u4e3a\u6811\uff0c\u6307\u5bfc\u673a\u5668\u4eba\u6267\u884c\u76f8\u5173\u52a8\u4f5c\u8fdb\u884c\u53c2\u6570\u4f30\u8ba1\u3002", "result": "\u5728\u591a\u79cd\u573a\u666f\u4e2d\u6210\u529f\u4f30\u8ba1\u4e86\u5bf9\u8c61\u8d28\u91cf\u3001\u8868\u9762\u9ad8\u5ea6\u4ee5\u53ca\u6469\u64e6\u76f8\u5173\u91cf\uff0c\u5305\u62ec\u906e\u6321\u7269\u4f53\u548c\u4e0d\u5b8c\u6574\u6a21\u578b\u7684\u60c5\u51b5\u3002", "conclusion": "\u63d0\u51fa\u7684\u81ea\u4e3bReal2Sim\u6846\u67b6\u5b9e\u73b0\u4e86\u89e3\u91ca\u6027\u3001\u4ee5\u610f\u56fe\u9a71\u52a8\u7684\u6d41\u7a0b\uff0c\u7ed3\u5408\u9ad8\u5c42\u63a8\u7406\u4e0e\u7269\u7406\u9a71\u52a8\u7684\u673a\u5668\u4eba\u4ea4\u4e92\u3002"}}
{"id": "2601.08697", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08697", "abs": "https://arxiv.org/abs/2601.08697", "authors": ["Nifu Dan"], "title": "Auditing Student-AI Collaboration: A Case Study of Online Graduate CS Students", "comment": null, "summary": "As generative AI becomes embedded in higher education, it increasingly shapes how students complete academic tasks. While these systems offer efficiency and support, concerns persist regarding over-automation, diminished student agency, and the potential for unreliable or hallucinated outputs. This study conducts a mixed-methods audit of student-AI collaboration preferences by examining the alignment between current AI capabilities and students' desired levels of automation in academic work. Using two sequential and complementary surveys, we capture students' perceived benefits, risks, and preferred boundaries when using AI. The first survey employs an existing task-based framework to assess preferences for and actual usage of AI across 12 academic tasks, alongside primary concerns and reasons for use. The second survey, informed by the first, explores how AI systems could be designed to address these concerns through open-ended questions. This study aims to identify gaps between existing AI affordances and students' normative expectations of collaboration, informing the development of more effective and trustworthy AI systems for education.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u63a2\u8ba8\u4e86\u5b66\u751f\u5728\u5b66\u672f\u4efb\u52a1\u4e2d\u5bf9\u4eba\u5de5\u667a\u80fd\u534f\u4f5c\u7684\u504f\u597d\u548c\u671f\u671b\uff0c\u7ed3\u679c\u63ed\u793a\u4e86\u5b66\u751f\u5bf9\u81ea\u52a8\u5316\u7684\u671f\u671b\u4e0e\u5b9e\u9645AI\u80fd\u529b\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u5584\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u8bbe\u8ba1\u7684\u5efa\u8bae\u3002", "motivation": "\u968f\u7740\u751f\u6210\u6027\u4eba\u5de5\u667a\u80fd\u5728\u9ad8\u7b49\u6559\u80b2\u4e2d\u7684\u9010\u6b65\u5d4c\u5165\uff0c\u4e86\u89e3\u5b66\u751f\u4e0eAI\u5408\u4f5c\u7684\u504f\u597d\u53ca\u5176\u5bf9\u5b66\u672f\u4efb\u52a1\u7684\u5f71\u54cd\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u4e24\u8f6e\u987a\u5e8f\u4e92\u8865\u7684\u8c03\u67e5\u65b9\u6cd5\uff0c\u7b2c\u4e00\u8f6e\u901a\u8fc7\u73b0\u6709\u4efb\u52a1\u6846\u67b6\u8bc4\u4f30\u5b66\u751f\u5bf912\u9879\u5b66\u672f\u4efb\u52a1\u7684AI\u4f7f\u7528\u504f\u597d\u53ca\u5b9e\u9645\u4f7f\u7528\u60c5\u51b5\uff0c\u7b2c\u4e8c\u8f6e\u5219\u901a\u8fc7\u5f00\u653e\u5f0f\u95ee\u9898\u63a2\u8ba8\u5982\u4f55\u8bbe\u8ba1AI\u7cfb\u7edf\u4ee5\u6ee1\u8db3\u5b66\u751f\u7684\u5173\u5207\u3002", "result": "\u672c\u7814\u7a76\u901a\u8fc7\u6df7\u5408\u65b9\u6cd5\u5ba1\u6838\u5b66\u751f\u4e0e\u4eba\u5de5\u667a\u80fd\u7684\u5408\u4f5c\u504f\u597d\uff0c\u65e8\u5728\u627e\u51fa\u5f53\u524d\u4eba\u5de5\u667a\u80fd\u80fd\u529b\u4e0e\u5b66\u751f\u5bf9\u5b66\u672f\u5de5\u4f5c\u81ea\u52a8\u5316\u7684\u671f\u671b\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5b66\u751f\u5728\u4f7f\u7528\u4eba\u5de5\u667a\u80fd\u65f6\u65e2\u5e0c\u671b\u83b7\u5f97\u4fbf\u5229\u53c8\u62c5\u5fe7\u81ea\u52a8\u5316\u8fc7\u5ea6\uff0c\u63d0\u51fa\u4e86\u5bf9\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u8bbe\u8ba1\u7684\u5177\u4f53\u5efa\u8bae\uff0c\u4ee5\u6ee1\u8db3\u5b66\u751f\u7684\u9700\u6c42\u548c\u671f\u671b\u3002"}}
{"id": "2601.08485", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08485", "abs": "https://arxiv.org/abs/2601.08485", "authors": ["Chong Zhang", "Victor Klemm", "Fan Yang", "Marco Hutter"], "title": "AME-2: Agile and Generalized Legged Locomotion via Attention-Based Neural Map Encoding", "comment": "under review", "summary": "Achieving agile and generalized legged locomotion across terrains requires tight integration of perception and control, especially under occlusions and sparse footholds. Existing methods have demonstrated agility on parkour courses but often rely on end-to-end sensorimotor models with limited generalization and interpretability. By contrast, methods targeting generalized locomotion typically exhibit limited agility and struggle with visual occlusions. We introduce AME-2, a unified reinforcement learning (RL) framework for agile and generalized locomotion that incorporates a novel attention-based map encoder in the control policy. This encoder extracts local and global mapping features and uses attention mechanisms to focus on salient regions, producing an interpretable and generalized embedding for RL-based control. We further propose a learning-based mapping pipeline that provides fast, uncertainty-aware terrain representations robust to noise and occlusions, serving as policy inputs. It uses neural networks to convert depth observations into local elevations with uncertainties, and fuses them with odometry. The pipeline also integrates with parallel simulation so that we can train controllers with online mapping, aiding sim-to-real transfer. We validate AME-2 with the proposed mapping pipeline on a quadruped and a biped robot, and the resulting controllers demonstrate strong agility and generalization to unseen terrains in simulation and in real-world experiments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86AME-2\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\u548c\u5b66\u4e60\u578b\u6620\u5c04\u7ba1\u9053\uff0c\u65e8\u5728\u5b9e\u73b0\u5feb\u901f\u4e14\u9ad8\u6548\u7684\u8de8\u5730\u5f62\u7684\u884c\u8d70\u63a7\u5236\u3002", "motivation": "\u5b9e\u73b0\u8de8\u5730\u5f62\u7684\u654f\u6377\u548c\u5e7f\u4e49\u884c\u8d70\u9700\u8981\u611f\u77e5\u548c\u63a7\u5236\u7684\u7d27\u5bc6\u96c6\u6210\uff0c\u5c24\u5176\u662f\u5728\u906e\u6321\u548c\u7a00\u758f\u652f\u6491\u70b9\u7684\u60c5\u51b5\u4e0b\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5728\u6cdb\u5316\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6620\u5c04\u7f16\u7801\u5668\u7684\u7edf\u4e00\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4ee5\u53ca\u4e00\u4e2a\u5feb\u901f\u3001\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u5b66\u4e60\u578b\u6620\u5c04\u7ba1\u9053\u3002", "result": "\u5728\u56db\u8db3\u548c\u53cc\u8db3\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1AME-2\uff0c\u5e76\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u5b9e\u9a8c\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u654f\u6377\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "AME-2\u5c55\u793a\u4e86\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u4e16\u754c\u4e2d\u5bf9\u672a\u77e5\u5730\u5f62\u5177\u6709\u5f3a\u5927\u7684\u654f\u6377\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u63a7\u5236\u5668\u3002"}}
{"id": "2601.08819", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.08819", "abs": "https://arxiv.org/abs/2601.08819", "authors": ["Roshni Kaushik", "Reid Simmons"], "title": "Older Adults' Preferences for Feedback Cadence from an Exercise Coach Robot", "comment": "Nonarchival submission to RO-MAN 2024 - poster session", "summary": "People can respond to feedback and guidance in different ways, and it is important for robots to personalize their interactions and utilize verbal and nonverbal communication cues. We aim to understand how older adults respond to different cadences of verbal and nonverbal feedback of a robot exercise coach. We conducted an online study of older adults, where participants evaluated videos of the robot giving feedback at different cadences for each modality. The results indicate that changing the cadence of one modality affects the perception of both it and the other modality. We can use the results from this study to better design the frequency of the robot coach's feedback during an exercise session with this population.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u8001\u5e74\u4eba\u5bf9\u673a\u5668\u4eba\u6559\u7ec3\u53cd\u9988\u8282\u594f\u7684\u53cd\u5e94\uff0c\u7ed3\u679c\u63ed\u793a\u8282\u594f\u6539\u53d8\u5bf9\u53cd\u9988\u611f\u77e5\u7684\u76f8\u4e92\u5f71\u54cd\uff0c\u4e3a\u8bbe\u8ba1\u4e2a\u6027\u5316\u53cd\u9988\u63d0\u4f9b\u4e86\u53c2\u8003\u3002", "motivation": "\u4e86\u89e3\u8001\u5e74\u4eba\u5982\u4f55\u54cd\u5e94\u673a\u5668\u4eba\u6559\u7ec3\u7684\u4e0d\u540c\u53cd\u9988\u8282\u594f\uff0c\u4ee5\u5b9e\u73b0\u4e2a\u6027\u5316\u4ea4\u4e92\u3002", "method": "\u901a\u8fc7\u5728\u7ebf\u7814\u7a76\uff0c\u8ba9\u8001\u5e74\u4eba\u8bc4\u4f30\u673a\u5668\u4eba\u5728\u4e0d\u540c\u8282\u594f\u4e0b\u7684\u53e3\u5934\u4e0e\u975e\u53e3\u5934\u53cd\u9988\u89c6\u9891\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u6539\u53d8\u4e00\u79cd\u53cd\u9988\u5f62\u5f0f\u7684\u8282\u594f\u4f1a\u5f71\u54cd\u5bf9\u53e6\u4e00\u79cd\u53cd\u9988\u5f62\u5f0f\u7684\u611f\u77e5\u3002", "conclusion": "\u8001\u5e74\u4eba\u5bf9\u673a\u5668\u4eba\u6559\u7ec3\u53cd\u9988\u7684\u53cd\u5e94\u53d7\u53cd\u9988\u8282\u594f\u7684\u5f71\u54cd\uff0c\u8bbe\u8ba1\u66f4\u5408\u9002\u7684\u53cd\u9988\u9891\u7387\u53ef\u4ee5\u63d0\u5347\u4e92\u52a8\u6548\u679c\u3002"}}
{"id": "2601.08491", "categories": ["cs.RO", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.08491", "abs": "https://arxiv.org/abs/2601.08491", "authors": ["Mohamed Afouene Melki", "Mohammad Shehab", "Mohamed-Slim Alouini"], "title": "AUV Trajectory Learning for Underwater Acoustic Energy Transfer and Age Minimization", "comment": null, "summary": "Internet of underwater things (IoUT) is increasingly gathering attention with the aim of monitoring sea life and deep ocean environment, underwater surveillance as well as maintenance of underwater installments. However, conventional IoUT devices, reliant on battery power, face limitations in lifespan and pose environmental hazards upon disposal. This paper introduces a sustainable approach for simultaneous information uplink from the IoUT devices and acoustic energy transfer (AET) to the devices via an autonomous underwater vehicle (AUV), potentially enabling them to operate indefinitely. To tackle the time-sensitivity, we adopt age of information (AoI), and Jain's fairness index. We develop two deep-reinforcement learning (DRL) algorithms, offering a high-complexity, high-performance frequency division duplex (FDD) solution and a low-complexity, medium-performance time division duplex (TDD) approach. The results elucidate that the proposed FDD and TDD solutions significantly reduce the average AoI and boost the harvested energy as well as data collection fairness compared to baseline approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6301\u7eed\u7684\u6c34\u4e0b\u7269\u8054\u7f51\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u81ea\u4e3b\u6c34\u4e0b\u822a\u884c\u5668\u5b9e\u73b0\u4fe1\u606f\u4e0a\u4f20\u548c\u58f0\u80fd\u4f20\u8f93\uff0c\u91c7\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4f18\u5316\u4fe1\u606f\u8001\u5316\u548c\u80fd\u91cf\u6536\u96c6\u3002", "motivation": "\u5e94\u5bf9\u4f20\u7edf\u4f9d\u8d56\u7535\u6c60\u7684\u6c34\u4e0b\u7269\u8054\u7f51\u8bbe\u5907\u5728\u5bff\u547d\u3001\u5e9f\u5f03\u7269\u5904\u7406\u53ca\u73af\u5883\u5b89\u5168\u65b9\u9762\u7684\u9650\u5236\uff0c\u5bfb\u6c42\u53ef\u6301\u7eed\u7684\u80fd\u6e90\u548c\u901a\u4fe1\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86\u4e24\u79cd\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60(DRL)\u7b97\u6cd5\uff0c\u5206\u522b\u63d0\u4f9b\u9ad8\u590d\u6742\u5ea6\u9ad8\u6027\u80fd\u7684\u9891\u5206\u53cc\u5de5(FDD)\u65b9\u6848\u548c\u4f4e\u590d\u6742\u5ea6\u4e2d\u7b49\u6027\u80fd\u7684\u65f6\u5206\u53cc\u5de5(TDD)\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684FDD\u548cTDD\u89e3\u51b3\u65b9\u6848\u5728\u4fe1\u606f\u8001\u5316\u65f6\u95f4\u548c\u80fd\u91cf\u6536\u96c6\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684FDD\u548cTDD\u65b9\u6848\u663e\u8457\u964d\u4f4e\u4e86\u5e73\u5747\u4fe1\u606f\u8001\u5316\u65f6\u95f4\uff0c\u63d0\u9ad8\u4e86\u80fd\u91cf\u6536\u96c6\u548c\u6570\u636e\u6536\u96c6\u7684\u516c\u5e73\u6027\u3002"}}
{"id": "2601.08514", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08514", "abs": "https://arxiv.org/abs/2601.08514", "authors": ["Davide Risi", "Vincenzo Petrone", "Antonio Langella", "Lorenzo Pagliara", "Enrico Ferrentino", "Pasquale Chiacchio"], "title": "Simplifying ROS2 controllers with a modular architecture for robot-agnostic reference generation", "comment": "5 pages, 7 figures", "summary": "This paper introduces a novel modular architecture for ROS2 that decouples the logic required to acquire, validate, and interpolate references from the control laws that track them. The design includes a dedicated component, named Reference Generator, that receives references, in the form of either single points or trajectories, from external nodes (e.g., planners), and writes single-point references at the controller's sampling period via the existing ros2_control chaining mechanism to downstream controllers. This separation removes duplicated reference-handling code from controllers and improves reusability across robot platforms. We implement two reference generators: one for handling joint-space references and one for Cartesian references, along with a set of new controllers (PD with gravity compensation, Cartesian pose, and admittance controllers) and validate the approach on simulated and real Universal Robots and Franka Emika manipulators. Results show that (i) references are tracked reliably in all tested scenarios, (ii) reference generators reduce duplicated reference-handling code across chained controllers to favor the construction and reuse of complex controller pipelines, and (iii) controller implementations remain focused only on control laws.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684ROS2\u6a21\u5757\u5316\u67b6\u6784\uff0c\u901a\u8fc7\u5206\u79bb\u53c2\u8003\u751f\u6210\u548c\u63a7\u5236\u6cd5\u5219\uff0c\u63d0\u5347\u4e86\u53c2\u8003\u5904\u7406\u7684\u91cd\u7528\u6027\u548c\u53ef\u9760\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u5b9e\u73b0\u673a\u5668\u4eba\u63a7\u5236\u7684\u9ad8\u590d\u7528\u6027\u548c\u9ad8\u6548\u6027\uff0c\u51cf\u5c11\u91cd\u590d\u4ee3\u7801\uff0c\u63d0\u9ad8\u5bf9\u590d\u6742\u63a7\u5236\u7cfb\u7edf\u7684\u652f\u6301\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u53c2\u8003\u751f\u6210\u7ec4\u4ef6\uff0c\u80fd\u591f\u5904\u7406\u5355\u70b9\u548c\u8f68\u8ff9\u53c2\u8003\uff0c\u5e76\u901a\u8fc7ros2_control\u673a\u5236\u5c06\u4fe1\u606f\u4f20\u9012\u7ed9\u4e0b\u6e38\u63a7\u5236\u5668\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5f15\u7528\u5728\u6240\u6709\u6d4b\u8bd5\u573a\u666f\u4e2d\u90fd\u80fd\u53ef\u9760\u8ffd\u8e2a\uff0c\u51cf\u5c11\u4e86\u91cd\u590d\u7684\u5f15\u7528\u5904\u7406\u4ee3\u7801\uff0c\u63a7\u5236\u5668\u5b9e\u73b0\u4e5f\u66f4\u4e13\u6ce8\u4e8e\u63a7\u5236\u6cd5\u5219\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5730\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u7684\u67b6\u6784\uff0c\u80fd\u591f\u6709\u6548\u5730\u89e3\u8026\u53c2\u8003\u751f\u6210\u548c\u63a7\u5236\u6cd5\u5219\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u63a7\u5236\u5668\u7684\u590d\u7528\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2601.08520", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08520", "abs": "https://arxiv.org/abs/2601.08520", "authors": ["Krzysztof Zielinski", "Dominik Belter"], "title": "Keyframe-based Dense Mapping with the Graph of View-Dependent Local Maps", "comment": "Accepted in ICRA 2020", "summary": "In this article, we propose a new keyframe-based mapping system. The proposed method updates local Normal Distribution Transform maps (NDT) using data from an RGB-D sensor. The cells of the NDT are stored in 2D view-dependent structures to better utilize the properties and uncertainty model of RGB-D cameras. This method naturally represents an object closer to the camera origin with higher precision. The local maps are stored in the pose graph which allows correcting global map after loop closure detection. We also propose a procedure that allows merging and filtering local maps to obtain a global map of the environment. Finally, we compare our method with Octomap and NDT-OM and provide example applications of the proposed mapping method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5173\u952e\u5e27\u7684\u65b0\u578b\u6620\u5c04\u7cfb\u7edf\uff0c\u5229\u7528RGB-D\u4f20\u611f\u5668\u6570\u636e\u66f4\u65b0\u5c40\u90e8NDT\u5730\u56fe\uff0c\u5e76\u5728\u95ed\u73af\u68c0\u6d4b\u540e\u7ea0\u6b63\u5168\u5c40\u5730\u56fe\u3002", "motivation": "\u9488\u5bf9\u73b0\u6709\u5730\u56fe\u6784\u5efa\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u65e8\u5728\u63d0\u9ad8\u57fa\u4e8eRGB-D\u76f8\u673a\u7684\u73af\u5883\u6620\u5c04\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "method": "\u5229\u7528RGB-D\u4f20\u611f\u5668\u6570\u636e\u66f4\u65b0\u672c\u5730NDT\u5730\u56fe\uff0c\u5e76\u5c06\u5730\u65b9\u5730\u56fe\u5b58\u50a8\u5728\u59ff\u6001\u56fe\u4e2d\uff0c\u6784\u5efa\u5168\u5c40\u5730\u56fe\u3002", "result": "\u901a\u8fc7\u4e0eOctomap\u548cNDT-OM\u8fdb\u884c\u6bd4\u8f83\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5e94\u7528\u5b9e\u4f8b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e3a\u73af\u5883\u6620\u5c04\u63d0\u4f9b\u4e86\u65b0\u9009\u62e9\u3002"}}
{"id": "2601.08523", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08523", "abs": "https://arxiv.org/abs/2601.08523", "authors": ["Nesserine Laribi", "Mohammed Rida Mokhtari", "Abdelaziz Benallegue", "Abdelhafid El-Hadri", "Mehdi Benallegue"], "title": "QP-Based Control of an Underactuated Aerial Manipulator under Constraints", "comment": null, "summary": "This paper presents a constraint-aware control framework for underactuated aerial manipulators, enabling accurate end-effector trajectory tracking while explicitly accounting for safety and feasibility constraints. The control problem is formulated as a quadratic program that computes dynamically consistent generalized accelerations subject to underactuation, actuator bounds, and system constraints. To enhance robustness against disturbances, modeling uncertainties, and steady-state errors, a passivity-based integral action is incorporated at the torque level without compromising feasibility. The effectiveness of the proposed approach is demonstrated through high-fidelity physics-based simulations, which include parameter perturbations, viscous joint friction, and realistic sensing and state-estimation effects. This demonstrates accurate tracking, smooth control inputs, and reliable constraint satisfaction under realistic operating conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ea6\u675f\u611f\u77e5\u7684\u63a7\u5236\u6846\u67b6\uff0c\u589e\u5f3a\u6b20\u9a71\u52a8\u65e0\u4eba\u673a\u7684\u8f68\u8ff9\u8ddf\u8e2a\u6027\u80fd\uff0c\u540c\u65f6\u786e\u4fdd\u5b89\u5168\u6027\u548c\u53ef\u884c\u6027\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5728\u8003\u8651\u5b89\u5168\u6027\u548c\u53ef\u884c\u6027\u7684\u7ea6\u675f\u4e0b\u5b9e\u73b0\u7cbe\u786e\u8f68\u8ff9\u8ddf\u8e2a\u7684\u65e0\u4eba\u673a\u64cd\u63a7\u7cfb\u7edf", "method": "\u5c06\u63a7\u5236\u95ee\u9898\u8868\u8ff0\u4e3a\u4e00\u4e2a\u4e8c\u6b21\u89c4\u5212\uff0c\u8ba1\u7b97\u52a8\u6001\u4e00\u81f4\u7684\u5e7f\u4e49\u52a0\u901f\u5ea6\uff0c\u5e76\u8003\u8651\u6b20\u9a71\u52a8\u3001\u6267\u884c\u5668\u754c\u9650\u548c\u7cfb\u7edf\u7ea6\u675f", "result": "\u901a\u8fc7\u9ad8\u4fdd\u771f\u5ea6\u7684\u7269\u7406\u57fa\u7840\u6a21\u62df\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u5728\u5904\u7406\u53c2\u6570\u6270\u52a8\u3001\u7c98\u6027\u5173\u8282\u6469\u64e6\u53ca\u73b0\u5b9e\u611f\u77e5\u548c\u72b6\u6001\u4f30\u8ba1\u6548\u5e94\u4e0b\u7684\u6709\u6548\u6027", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u5728\u73b0\u5b9e\u64cd\u4f5c\u6761\u4ef6\u4e0b\u5b9e\u73b0\u7cbe\u786e\u8ddf\u8e2a\u3001\u5e73\u6ed1\u63a7\u5236\u8f93\u5165\u548c\u53ef\u9760\u7684\u7ea6\u675f\u6ee1\u8db3"}}
{"id": "2601.08665", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08665", "abs": "https://arxiv.org/abs/2601.08665", "authors": ["Shaoan Wang", "Yuanfei Luo", "Xingyu Chen", "Aocheng Luo", "Dongyue Li", "Chang Liu", "Sheng Chen", "Yangang Zhang", "Junzhi Yu"], "title": "VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory", "comment": "Project page: https://wsakobe.github.io/VLingNav-web/", "summary": "VLA models have shown promising potential in embodied navigation by unifying perception and planning while inheriting the strong generalization abilities of large VLMs. However, most existing VLA models rely on reactive mappings directly from observations to actions, lacking the explicit reasoning capabilities and persistent memory required for complex, long-horizon navigation tasks. To address these challenges, we propose VLingNav, a VLA model for embodied navigation grounded in linguistic-driven cognition. First, inspired by the dual-process theory of human cognition, we introduce an adaptive chain-of-thought mechanism, which dynamically triggers explicit reasoning only when necessary, enabling the agent to fluidly switch between fast, intuitive execution and slow, deliberate planning. Second, to handle long-horizon spatial dependencies, we develop a visual-assisted linguistic memory module that constructs a persistent, cross-modal semantic memory, enabling the agent to recall past observations to prevent repetitive exploration and infer movement trends for dynamic environments. For the training recipe, we construct Nav-AdaCoT-2.9M, the largest embodied navigation dataset with reasoning annotations to date, enriched with adaptive CoT annotations that induce a reasoning paradigm capable of adjusting both when to think and what to think about. Moreover, we incorporate an online expert-guided reinforcement learning stage, enabling the model to surpass pure imitation learning and to acquire more robust, self-explored navigation behaviors. Extensive experiments demonstrate that VLingNav achieves state-of-the-art performance across a wide range of embodied navigation benchmarks. Notably, VLingNav transfers to real-world robotic platforms in a zero-shot manner, executing various navigation tasks and demonstrating strong cross-domain and cross-task generalization.", "AI": {"tldr": "VLingNav\u662f\u9488\u5bf9\u4f53\u4f53\u73b0\u5bfc\u822a\u7684\u65b0\u6a21\u578b\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u63a8\u7406\u548c\u8bb0\u5fc6\u673a\u5236\uff0c\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u5bfc\u822a\u6027\u80fd\uff0c\u5e76\u80fd\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u5b9e\u73b0\u5f3a\u5927\u8fc1\u79fb\u80fd\u529b\u3002", "motivation": "\u5927\u591a\u6570\u73b0\u6709\u7684VLA\u6a21\u578b\u5728\u9762\u5bf9\u590d\u6742\u7684\u957f\u65f6\u95f4\u5bfc\u822a\u4efb\u52a1\u65f6\uff0c\u7f3a\u4e4f\u660e\u786e\u7684\u63a8\u7406\u80fd\u529b\u548c\u6301\u4e45\u7684\u8bb0\u5fc6\uff0c\u56e0\u6b64\u63d0\u51faVLingNav\u65e8\u5728\u589e\u5f3a\u8fd9\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u8a00\u9a71\u52a8\u7684\u8ba4\u77e5\u7684VLA\u6a21\u578bVLingNav\uff0c\u91c7\u7528\u4e86\u81ea\u9002\u5e94Chain-of-Thought\u673a\u5236\u548c\u89c6\u89c9\u8f85\u52a9\u8bed\u8a00\u8bb0\u5fc6\u6a21\u5757\uff0c\u5e76\u7ed3\u5408\u5728\u7ebf\u4e13\u5bb6\u6307\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "VLingNav\u7ed3\u5408\u81ea\u9002\u5e94\u7684\u63a8\u7406\u673a\u5236\u548c\u89c6\u89c9\u8f85\u52a9\u8bb0\u5fc6\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u957f\u65f6\u95f4\u7684\u7a7a\u95f4\u4f9d\u8d56\uff0c\u540c\u65f6\u8d85\u8d8a\u7eaf\u6a21\u4eff\u5b66\u4e60\uff0c\u83b7\u5f97\u66f4\u5f3a\u7684\u81ea\u6211\u63a2\u7d22\u5bfc\u822a\u884c\u4e3a\u3002", "conclusion": "VLingNav\u5728\u591a\u79cd\u4f53\u4f53\u73b0\u5bfc\u822a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u80fd\u591f\u5728\u96f6-shot\u60c5\u5883\u4e0b\u8f6c\u79fb\u5230\u73b0\u5b9e\u4e16\u754c\u7684\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u6267\u884c\u5404\u79cd\u5bfc\u822a\u4efb\u52a1\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u9886\u57df\u548c\u8de8\u4efb\u52a1\u7684\u8fc1\u79fb\u80fd\u529b\u3002"}}
{"id": "2601.08711", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.08711", "abs": "https://arxiv.org/abs/2601.08711", "authors": ["Shifa Sulaiman", "Francesco Schetter", "Mehul Menon", "Fanny Ficuciello"], "title": "A Hybrid Model-based and Data-based Approach Developed for a Prosthetic Hand Wrist", "comment": null, "summary": "The incorporation of advanced control algorithms into prosthetic hands significantly enhances their ability to replicate the intricate motions of a human hand. This work introduces a model-based controller that combines an Artificial Neural Network (ANN) approach with a Sliding Mode Controller (SMC) designed for a tendon-driven soft continuum wrist integrated into a prosthetic hand known as \"PRISMA HAND II\". Our research focuses on developing a controller that provides a fast dynamic response with reduced computational effort during wrist motions. The proposed controller consists of an ANN for computing bending angles together with an SMC to regulate tendon forces. Kinematic and dynamic models of the wrist are formulated using the Piece-wise Constant Curvature (PCC) hypothesis. The performance of the proposed controller is compared with other control strategies developed for the same wrist. Simulation studies and experimental validations of the fabricated wrist using the controller are included in the paper.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408ANN\u4e0eSMC\u7684\u63a7\u5236\u5668\uff0c\u4ee5\u63d0\u9ad8\u5047\u80a2\u624b\u8155\u7684\u8fd0\u52a8\u63a7\u5236\u6548\u7387\uff0c\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u52a8\u6001\u54cd\u5e94\u548c\u8f83\u4f4e\u7684\u8ba1\u7b97\u8d1f\u62c5\u3002", "motivation": "\u63d0\u9ad8\u5047\u80a2\u624b\u7684\u63a7\u5236\u7cbe\u5ea6\u548c\u7075\u6d3b\u5ea6\uff0c\u5c24\u5176\u662f\u5728\u6a21\u4eff\u4eba\u7c7b\u624b\u590d\u6742\u8fd0\u52a8\u7684\u80fd\u529b\u65b9\u9762\u3002", "method": "\u7ed3\u5408\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANN\uff09\u548c\u6ed1\u6a21\u63a7\u5236\u5668\uff08SMC\uff09\u8bbe\u8ba1\u7684\u6a21\u578b\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u5bf9\u8098\u90e8\u8fd0\u52a8\u7684\u63a7\u5236\u5b9e\u73b0\u6709\u6548\u8c03\u8282\u3002", "result": "\u6240\u63d0\u51fa\u7684\u63a7\u5236\u5668\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u5176\u4ed6\u9488\u5bf9\u540c\u6837\u8155\u90e8\u7684\u63a7\u5236\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u7814\u7a76\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u63a7\u5236\u5668\u5728\u63d0\u9ad8PRISMA HAND II\u5047\u624b\u8155\u52a8\u6001\u54cd\u5e94\u901f\u5ea6\u548c\u51cf\u5c11\u8ba1\u7b97\u8d1f\u62c5\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2601.08713", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08713", "abs": "https://arxiv.org/abs/2601.08713", "authors": ["Naren Medarametla", "Sreejon Mondal"], "title": "Real-Time Localization Framework for Autonomous Basketball Robots", "comment": "8 pages, 12 figures, Project code: https://github.com/NarenTheNumpkin/Basketball-robot-localization", "summary": "Localization is a fundamental capability for autonomous robots, enabling them to operate effectively in dynamic environments. In Robocon 2025, accurate and reliable localization is crucial for improving shooting precision, avoiding collisions with other robots, and navigating the competition field efficiently. In this paper, we propose a hybrid localization algorithm that integrates classical techniques with learning based methods that rely solely on visual data from the court's floor to achieve self-localization on the basketball field.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u6570\u636e\u7684\u6df7\u5408\u5b9a\u4f4d\u7b97\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u673a\u5668\u4eba\u5728Robocon 2025\u4e2d\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u63d0\u5347\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728Robocon 2025\u6bd4\u8d5b\u4e2d\uff0c\u51c6\u786e\u53ef\u9760\u7684\u5b9a\u4f4d\u5bf9\u4e8e\u63d0\u9ad8\u5c04\u51fb\u7cbe\u5ea6\u548c\u907f\u5f00\u78b0\u649e\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u5b9a\u4f4d\u7b97\u6cd5\uff0c\u7ed3\u5408\u7ecf\u5178\u6280\u672f\u4e0e\u57fa\u4e8e\u89c6\u89c9\u6570\u636e\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b9e\u73b0\u5728\u7bee\u7403\u573a\u4e0a\u7684\u81ea\u6211\u5b9a\u4f4d\u3002", "result": "\u8be5\u7b97\u6cd5\u80fd\u591f\u5229\u7528\u7bee\u7403\u573a\u5730\u9762\u7684\u89c6\u89c9\u6570\u636e\u8fdb\u884c\u6709\u6548\u7684\u81ea\u6211\u5b9a\u4f4d\u3002", "conclusion": "\u6df7\u5408\u5b9a\u4f4d\u7b97\u6cd5\u80fd\u591f\u6539\u5584\u673a\u5668\u4eba\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u64cd\u4f5c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u6bd4\u8d5b\u73af\u5883\u3002"}}
