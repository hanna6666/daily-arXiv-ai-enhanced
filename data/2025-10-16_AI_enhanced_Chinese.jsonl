{"id": "2510.12944", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.12944", "abs": "https://arxiv.org/abs/2510.12944", "authors": ["Sameha Alshakhsi", "Ala Yankouskaya", "Dena Al-Thani", "Raian Ali"], "title": "Changing Oneself by Teaching Others? Exploring the Prot\u00e9g\u00e9 Effect in Digital Stress Self-Regulation", "comment": "26 pages, 3 Figures, 7 Tables", "summary": "The prot\\'eg\\'ee effect suggests that individuals learn more effectively when\nthey teach a subject. While this has shown potential for acquiring knowledge\nand skills, can it also support acquiring a new behaviour? This study evaluated\na prot\\'eg\\'e-based intervention designed to manage digital stress. Over three\nweeks, 137 participants with moderate to high digital stress were assigned to\nfour groups. Two were prot\\'eg\\'ee-based: a passive group, given material to\nteach, and an active group, received headlines and had to search for and\nprepare teaching content. Both groups completed three sessions, each focused on\none digital stress component: availability demand stress, approval anxiety, and\nfear of missing out. A digital literacy group received similar content and\nquizzes, and a control group. Outcomes measured included digital stress,\nproblematic social media use, word-of-mouth about its management, and issue\ninvolvement. Findings highlight the challenge of translating cognitive\nengagement into behavioural change, especially amid persistent digital habits\nand socially reinforced stressors. Results offer insights into the limitations\nof interventions based on the prot\\'eg\\'ee effect when applied to behaviour\nchange, particularly in the context of reflective digital wellbeing strategies.\nFuture research could explore interactive formats, such as peer engagement or\nself-regulatory elements, to enhance motivation and impact.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u8bc4\u4f30\u4e86\u4e00\u4e2a\u57fa\u4e8e\u987e\u95ee\u6548\u5e94\u7684\u5e72\u9884\u63aa\u65bd\uff0c\u4ee5\u7ba1\u7406\u6570\u5b57\u538b\u529b\uff0c\u7ed3\u679c\u663e\u793a\u5728\u884c\u4e3a\u6539\u53d8\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "motivation": "\u63a2\u7d22\u987e\u95ee\u6548\u5e94\u662f\u5426\u80fd\u652f\u6301\u65b0\u884c\u4e3a\u7684\u83b7\u53d6\uff0c\u7279\u522b\u662f\u5728\u7ba1\u7406\u6570\u5b57\u538b\u529b\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u672c\u7814\u7a76\u5bf9137\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u4e86\u4e3a\u671f\u4e09\u5468\u7684\u5e72\u9884\uff0c\u5206\u4e3a\u56db\u7ec4\uff1a\u4e24\u7ec4\u4e3a\u987e\u95ee\u578b\u5e72\u9884\uff0c\u6d89\u53ca\u6559\u5bfc\u6750\u6599\u548c\u51c6\u5907\u5185\u5bb9\uff0c\u53e6\u4e00\u7ec4\u4e3a\u6570\u5b57\u7d20\u517b\u57f9\u8bad\uff0c\u6700\u540e\u4e00\u7ec4\u4e3a\u5bf9\u7167\u7ec4\u3002", "result": "\u8fd9\u9879\u7814\u7a76\u8868\u660e\uff0c\u5c3d\u7ba1\u987e\u95ee\u6548\u5e94\u5728\u77e5\u8bc6\u548c\u6280\u80fd\u83b7\u53d6\u65b9\u9762\u663e\u793a\u4e86\u6f5c\u529b\uff0c\u4f46\u5728\u884c\u4e3a\u6539\u53d8\u65b9\u9762\u7684\u5e94\u7528\u4ecd\u9762\u4e34\u6311\u6218\u3002", "conclusion": "\u57fa\u4e8e\u987e\u95ee\u6548\u5e94\u7684\u5e72\u9884\u63aa\u65bd\u5728\u884c\u4e3a\u6539\u53d8\u65b9\u9762\u7684\u6709\u6548\u6027\u6709\u9650\uff0c\u5c24\u5176\u662f\u5728\u6570\u5b57\u538b\u529b\u80cc\u666f\u4e0b\u3002"}}
{"id": "2510.12972", "categories": ["cs.HC", "cs.SE", "H.5.2"], "pdf": "https://arxiv.org/pdf/2510.12972", "abs": "https://arxiv.org/abs/2510.12972", "authors": ["Mingyuan Zhong", "Xia Chen", "Davin Win Kyi", "Chen Li", "James Fogarty", "Jacob O. Wobbrock"], "title": "TaskAudit: Detecting Functiona11ity Errors in Mobile Apps via Agentic Task Execution", "comment": null, "summary": "Accessibility checkers are tools in support of accessible app development and\ntheir use is encouraged by accessibility best practices. However, most current\ncheckers evaluate static or mechanically-generated contexts, failing to capture\ncommon accessibility errors impacting mobile app functionality. We present\nTaskAudit, an accessibility evaluation system that focuses on detecting\nfunctiona11ity errors through simulated interactions. TaskAudit comprises three\ncomponents: a Task Generator that constructs interactive tasks from app\nscreens, a Task Executor that uses agents with a screen reader proxy to perform\nthese tasks, and an Accessibility Analyzer that detects and reports\naccessibility errors by examining interaction traces. Evaluation on real-world\napps shows that our strategy detects 48 functiona11ity errors from 54 app\nscreens, compared to between 4 and 20 with existing checkers. Our analysis\ndemonstrates common error patterns that TaskAudit can detect in addition to\nprior work, including label-functionality mismatch, cluttered navigation, and\ninappropriate feedback.", "AI": {"tldr": "TaskAudit\u901a\u8fc7\u6a21\u62df\u4ea4\u4e92\u68c0\u6d4b\u529f\u80fd\u6027\u53ef\u8bbf\u95ee\u6027\u9519\u8bef\uff0c\u76f8\u8f83\u4e8e\u73b0\u6709\u5de5\u5177\u66f4\u5177\u6548\u80fd\u3002", "motivation": "\u5f53\u524d\u7684\u53ef\u8bbf\u95ee\u6027\u68c0\u67e5\u5de5\u5177\u5927\u591a\u53ea\u8bc4\u4f30\u9759\u6001\u6216\u673a\u68b0\u751f\u6210\u7684\u4e0a\u4e0b\u6587\uff0c\u65e0\u6cd5\u6355\u83b7\u5f71\u54cd\u79fb\u52a8\u5e94\u7528\u529f\u80fd\u7684\u5e38\u89c1\u53ef\u8bbf\u95ee\u6027\u9519\u8bef\u3002", "method": "TaskAudit\uff0c\u91c7\u7528\u6a21\u62df\u4ea4\u4e92\u68c0\u6d4b\u529f\u80fd\u6027\u9519\u8bef\u7684\u53ef\u8bbf\u95ee\u6027\u8bc4\u4f30\u7cfb\u7edf\u3002", "result": "TaskAudit\u572854\u4e2a\u5e94\u7528\u5c4f\u5e55\u4e2d\u68c0\u6d4b\u523048\u4e2a\u529f\u80fd\u6027\u9519\u8bef\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u68c0\u67e5\u5668\u7684\u68c0\u6d4b\u80fd\u529b\uff084\u523020\u4e2a\u9519\u8bef\uff09\u3002", "conclusion": "TaskAudit\u80fd\u6709\u6548\u8bc6\u522b\u6807\u7b7e\u529f\u80fd\u4e0d\u5339\u914d\u3001\u5bfc\u822a\u6df7\u4e71\u548c\u4e0d\u5f53\u53cd\u9988\u7b49\u5e38\u89c1\u9519\u8bef\u6a21\u5f0f\uff0c\u63d0\u5347\u4e86\u53ef\u8bbf\u95ee\u6027\u8bc4\u4f30\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.12988", "categories": ["cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12988", "abs": "https://arxiv.org/abs/2510.12988", "authors": ["Numan Zafar", "Priyo Ranjan Kundu Prosun", "Shafique Ahmad Chaudhry"], "title": "Behavioral Biometrics for Automatic Detection of User Familiarity in VR", "comment": "7 pages, 7 figures, 17th International Conference on Quality of\n  Multimedia Experience", "summary": "As virtual reality (VR) devices become increasingly integrated into everyday\nsettings, a growing number of users without prior experience will engage with\nVR systems. Automatically detecting a user's familiarity with VR as an\ninteraction medium enables real-time, adaptive training and interface\nadjustments, minimizing user frustration and improving task performance. In\nthis study, we explore the automatic detection of VR familiarity by analyzing\nhand movement patterns during a passcode-based door-opening task, which is a\nwell-known interaction in collaborative virtual environments such as meeting\nrooms, offices, and healthcare spaces. While novice users may lack prior VR\nexperience, they are likely to be familiar with analogous real-world tasks\ninvolving keypad entry. We conducted a pilot study with 26 participants, evenly\nsplit between experienced and inexperienced VR users, who performed tasks using\nboth controller-based and hand-tracking interactions. Our approach uses\nstate-of-the-art deep classifiers for automatic VR familiarity detection,\nachieving the highest accuracies of 92.05% and 83.42% for hand-tracking and\ncontroller-based interactions, respectively. In the cross-device evaluation,\nwhere classifiers trained on controller data were tested using hand-tracking\ndata, the model achieved an accuracy of 78.89%. The integration of both\nmodalities in the mixed-device evaluation obtained an accuracy of 94.19%. Our\nresults underline the promise of using hand movement biometrics for the\nreal-time detection of user familiarity in critical VR applications, paving the\nway for personalized and adaptive VR experiences.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5206\u6790\u624b\u90e8\u8fd0\u52a8\u6a21\u5f0f\uff0c\u81ea\u52a8\u68c0\u6d4b\u7528\u6237\u5728VR\u4e2d\u7684\u719f\u6089\u7a0b\u5ea6\uff0c\u53d6\u5f9792.05%\u7684\u6700\u9ad8\u51c6\u786e\u7387\uff0c\u4e3a\u4e2a\u6027\u5316\u548c\u9002\u5e94\u6027VR\u4f53\u9a8c\u94fa\u5e73\u9053\u8def\u3002", "motivation": "\u968f\u7740VR\u8bbe\u5907\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u8bb8\u591a\u7528\u6237\u5728\u6ca1\u6709\u5148\u524d\u7ecf\u9a8c\u7684\u60c5\u51b5\u4e0b\u5f00\u59cb\u4f7f\u7528VR\u7cfb\u7edf\uff0c\u81ea\u52a8\u68c0\u6d4b\u7528\u6237\u719f\u6089\u7a0b\u5ea6\u6709\u52a9\u4e8e\u5b9e\u65f6\u9002\u5e94\u7528\u6237\u9700\u6c42\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528\u5c16\u7aef\u7684\u6df1\u5ea6\u5206\u7c7b\u5668\u5bf9\u7528\u6237\u5728\u8fdb\u884c\u95e8\u9501\u8f93\u5165\u4efb\u52a1\u65f6\u7684\u624b\u90e8\u8fd0\u52a8\u6a21\u5f0f\u8fdb\u884c\u5206\u6790\uff0c\u4ee5\u5b9e\u73b0\u81ea\u52a8\u8bc6\u522b\u7528\u6237\u7684VR\u719f\u6089\u7a0b\u5ea6\u3002", "result": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u901a\u8fc7\u5206\u6790\u624b\u90e8\u8fd0\u52a8\u6a21\u5f0f\u6765\u81ea\u52a8\u68c0\u6d4b\u7528\u6237\u5bf9\u865a\u62df\u73b0\u5b9e\uff08VR\uff09\u8bbe\u5907\u7684\u719f\u6089\u7a0b\u5ea6\uff0c\u4ee5\u5b9e\u65f6\u8c03\u6574\u57f9\u8bad\u548c\u754c\u9762\uff0c\u63d0\u9ad8\u4efb\u52a1\u8868\u73b0\u5e76\u51cf\u5c11\u7528\u6237\u632b\u8d25\u611f\u3002", "conclusion": "\u672c\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u624b\u90e8\u8fd0\u52a8\u751f\u7269\u7279\u5f81\u53ef\u4ee5\u5b9e\u65f6\u68c0\u6d4b\u7528\u6237\u5728\u5173\u952eVR\u5e94\u7528\u4e2d\u7684\u719f\u6089\u7a0b\u5ea6\uff0c\u6709\u52a9\u4e8e\u63d0\u4f9b\u4e2a\u6027\u5316\u548c\u9002\u5e94\u6027\u66f4\u5f3a\u7684VR\u4f53\u9a8c\u3002"}}
{"id": "2510.12866", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.12866", "abs": "https://arxiv.org/abs/2510.12866", "authors": ["Dantong Niu", "Yuvan Sharma", "Baifeng Shi", "Rachel Ding", "Matteo Gioia", "Haoru Xue", "Henry Tsai", "Konstantinos Kallidromitis", "Anirudh Pai", "Shankar Shastry", "Trevor Darrell", "Jitendra Malik", "Roei Herzig"], "title": "Learning to Grasp Anything by Playing with Random Toys", "comment": null, "summary": "Robotic manipulation policies often struggle to generalize to novel objects,\nlimiting their real-world utility. In contrast, cognitive science suggests that\nchildren develop generalizable dexterous manipulation skills by mastering a\nsmall set of simple toys and then applying that knowledge to more complex\nitems. Inspired by this, we study if similar generalization capabilities can\nalso be achieved by robots. Our results indicate robots can learn generalizable\ngrasping using randomly assembled objects that are composed from just four\nshape primitives: spheres, cuboids, cylinders, and rings. We show that training\non these \"toys\" enables robust generalization to real-world objects, yielding\nstrong zero-shot performance. Crucially, we find the key to this generalization\nis an object-centric visual representation induced by our proposed detection\npooling mechanism. Evaluated in both simulation and on physical robots, our\nmodel achieves a 67% real-world grasping success rate on the YCB dataset,\noutperforming state-of-the-art approaches that rely on substantially more\nin-domain data. We further study how zero-shot generalization performance\nscales by varying the number and diversity of training toys and the\ndemonstrations per toy. We believe this work offers a promising path to\nscalable and generalizable learning in robotic manipulation. Demonstration\nvideos, code, checkpoints and our dataset are available on our project page:\nhttps://lego-grasp.github.io/ .", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u673a\u5668\u4eba\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u5b66\u4e60\u7b80\u5355\u73a9\u5177\u6765\u5b9e\u73b0\u5bf9\u65b0\u7269\u4f53\u7684\u901a\u7528\u6293\u53d6\u80fd\u529b\uff0c\u7ed3\u679c\u663e\u793a\u53ef\u4ee5\uff0c\u4e14\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u7269\u4f53\u4e2d\u5fc3\u53ef\u89c6\u5316\u8868\u793a\u65b9\u6cd5\u3002", "motivation": "\u63a2\u8ba8\u673a\u5668\u4eba\u5728\u64cd\u4f5c\u65b0\u9896\u7269\u4f53\u65f6\u7684\u80fd\u529b\u5c40\u9650\uff0c\u501f\u9274\u8ba4\u77e5\u79d1\u5b66\u4e2d\u513f\u7ae5\u901a\u8fc7\u7b80\u5355\u73a9\u5177\u5b66\u4e60\u6765\u63d0\u5347\u64cd\u4f5c\u6280\u80fd\u7684\u539f\u7406\u3002", "method": "\u901a\u8fc7\u968f\u673a\u7ec4\u5408\u7684\u56db\u79cd\u57fa\u7840\u5f62\u72b6\u7269\u4f53\u8fdb\u884c\u8bad\u7ec3\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u68c0\u6d4b\u6c60\u5316\u673a\u5236\u4ee5\u5b9e\u73b0\u7269\u4f53\u4e2d\u5fc3\u7684\u53ef\u89c6\u5316\u8868\u793a\uff0c\u5e76\u5728\u4eff\u771f\u53ca\u5b9e\u9645\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u673a\u5668\u4eba\u80fd\u591f\u901a\u8fc7\u5b66\u4e60\u7531\u56db\u79cd\u57fa\u7840\u5f62\u72b6\uff08\u7403\u4f53\u3001\u957f\u65b9\u4f53\u3001\u5706\u67f1\u4f53\u548c\u73af\uff09\u7ec4\u6210\u7684\u968f\u673a\u7ec4\u5408\u7269\u4f53\uff0c\u5b9e\u73b0\u5177\u6709\u666e\u904d\u6027\u7406\u89e3\u7684\u6293\u53d6\u80fd\u529b\uff0c\u5e76\u5c55\u793a\u51fa\u5728\u771f\u5b9e\u4e16\u754c\u7269\u4f53\u4e0a\u7684\u5f3a\u5927\u96f6-shot\u6a21\u578b\u8868\u73b0\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u53ef\u6269\u5c55\u6027\u548c\u666e\u904d\u6027\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u524d\u666f\u7684\u8def\u5f84\uff0c\u4e14\u5728YCB\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e8667%\u7684\u771f\u5b9e\u6293\u53d6\u6210\u529f\u7387\u3002"}}
{"id": "2510.12994", "categories": ["cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12994", "abs": "https://arxiv.org/abs/2510.12994", "authors": ["Numan Zafar", "Johnathan Locke", "Shafique Ahmad Chaudhry"], "title": "Deep Learning-Based Visual Fatigue Detection Using Eye Gaze Patterns in VR", "comment": "8 pages, 3 figures, Accepted at IEEE International Symposium on\n  Emerging Metaverse (ISEMV 2025)", "summary": "Prolonged exposure to virtual reality (VR) systems leads to visual fatigue,\nimpairs user comfort, performance, and safety, particularly in high-stakes or\nlong-duration applications. Existing fatigue detection approaches rely on\nsubjective questionnaires or intrusive physiological signals, such as EEG,\nheart rate, or eye-blink count, which limit their scalability and real-time\napplicability. This paper introduces a deep learning-based study for detecting\nvisual fatigue using continuous eye-gaze trajectories recorded in VR. We use\nthe GazeBaseVR dataset comprising binocular eye-tracking data from 407\nparticipants across five immersive tasks, extract cyclopean eye-gaze angles,\nand evaluate six deep classifiers. Our results demonstrate that EKYT achieves\nup to 94% accuracy, particularly in tasks demanding high visual attention, such\nas video viewing and text reading. We further analyze gaze variance and\nsubjective fatigue measures, indicating significant behavioral differences\nbetween fatigued and non-fatigued conditions. These findings establish eye-gaze\ndynamics as a reliable and nonintrusive modality for continuous fatigue\ndetection in immersive VR, offering practical implications for adaptive\nhuman-computer interactions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fde\u7eed\u773c\u52a8\u8f68\u8ff9\u68c0\u6d4b\u865a\u62df\u73b0\u5b9e\u4e2d\u7684\u89c6\u89c9\u75b2\u52b3\uff0c\u8868\u73b0\u51fa\u9ad8\u8fbe94%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u4e3a\u81ea\u9002\u5e94\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "motivation": "\u957f\u671f\u4f7f\u7528\u865a\u62df\u73b0\u5b9e\u7cfb\u7edf\u4f1a\u5bfc\u81f4\u89c6\u89c9\u75b2\u52b3\uff0c\u5f71\u54cd\u7528\u6237\u7684\u8212\u9002\u5ea6\u3001\u6027\u80fd\u548c\u5b89\u5168\u6027\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u98ce\u9669\u6216\u957f\u65f6\u95f4\u5e94\u7528\u4e2d\u3002", "method": "\u901a\u8fc7\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u5206\u6790\u5728\u865a\u62df\u73b0\u5b9e\u4e2d\u8bb0\u5f55\u7684\u8fde\u7eed\u773c\u52a8\u8f68\u8ff9\uff0c\u4f7f\u7528407\u540d\u53c2\u4e0e\u8005\u7684\u53cc\u76ee\u773c\u52a8\u8ffd\u8e2a\u6570\u636e\u5e76\u8bc4\u4f30\u516d\u79cd\u6df1\u5ea6\u5206\u7c7b\u5668\u3002", "result": "EKYT\u6a21\u578b\u5728\u9ad8\u89c6\u89c9\u6ce8\u610f\u529b\u4efb\u52a1\uff08\u5982\u89c6\u9891\u89c2\u770b\u548c\u9605\u8bfb\u6587\u672c\uff09\u4e2d\u6700\u9ad8\u53ef\u8fbe94%\u7684\u51c6\u786e\u7387\uff0c\u4e14\u53d1\u73b0\u75b2\u52b3\u4e0e\u975e\u75b2\u52b3\u72b6\u6001\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u884c\u4e3a\u5dee\u5f02\u3002", "conclusion": "\u773c\u52a8\u52a8\u6001\u88ab\u786e\u7acb\u4e3a\u4e00\u79cd\u53ef\u9760\u4e14\u975e\u4fb5\u5165\u6027\u7684\u75b2\u52b3\u68c0\u6d4b\u65b9\u5f0f\uff0c\u5177\u6709\u5728\u6c89\u6d78\u5f0f\u865a\u62df\u73b0\u5b9e\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2510.12919", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.12919", "abs": "https://arxiv.org/abs/2510.12919", "authors": ["Mouhyemen Khan", "Tatsuya Ibuki", "Abhijit Chatterjee"], "title": "Gaussian Process Implicit Surfaces as Control Barrier Functions for Safe Robot Navigation", "comment": "8 pages, 7 figures, under review", "summary": "Level set methods underpin modern safety techniques such as control barrier\nfunctions (CBFs), while also serving as implicit surface representations for\ngeometric shapes via distance fields. Inspired by these two paradigms, we\npropose a unified framework where the implicit surface itself acts as a CBF. We\nleverage Gaussian process (GP) implicit surface (GPIS) to represent the safety\nboundaries, using safety samples which are derived from sensor measurements to\ncondition the GP. The GP posterior mean defines the implicit safety surface\n(safety belief), while the posterior variance provides a robust safety margin.\nAlthough GPs have favorable properties such as uncertainty estimation and\nanalytical tractability, they scale cubically with data. To alleviate this\nissue, we develop a sparse solution called sparse Gaussian CBFs. To the best of\nour knowledge, GPIS have not been explicitly used to synthesize CBFs. We\nvalidate the approach on collision avoidance tasks in two settings: a simulated\n7-DOF manipulator operating around the Stanford bunny, and a quadrotor\nnavigating in 3D around a physical chair. In both cases, Gaussian CBFs (with\nand without sparsity) enable safe interaction and collision-free execution of\ntrajectories that would otherwise intersect the objects.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5c06\u9ad8\u65af\u8fc7\u7a0b\u9690\u5f0f\u8868\u9762\u5e94\u7528\u4e8e\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff0c\u4f18\u5316\u4e86\u5b89\u5168\u8fb9\u754c\u7684\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u7a00\u758f\u6280\u672f\u63d0\u9ad8\u4e86\u6570\u636e\u5904\u7406\u6548\u7387\u3002", "motivation": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u4f7f\u9690\u5f0f\u8868\u9762\u4f5c\u4e3a\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff0c\u4ece\u800c\u589e\u5f3a\u5b89\u5168\u6027\u548c\u51e0\u4f55\u5f62\u72b6\u7684\u8868\u73b0\u529b\u3002", "method": "\u5229\u7528\u9ad8\u65af\u8fc7\u7a0b\u9690\u5f0f\u8868\u9762\u6765\u6784\u5efa\u5b89\u5168\u8fb9\u754c\uff0c\u5e76\u901a\u8fc7\u4f20\u611f\u5668\u6d4b\u91cf\u83b7\u5f97\u7684\u5b89\u5168\u6837\u672c\u6765\u8c03\u6574\u9ad8\u65af\u8fc7\u7a0b\u3002", "result": "\u5f00\u53d1\u4e00\u79cd\u79f0\u4e3a\u7a00\u758f\u9ad8\u65af\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u5e94\u5bf9\u6570\u636e\u89c4\u6a21\u589e\u957f\u5e26\u6765\u7684\u6311\u6218\u3002", "conclusion": "\u901a\u8fc7\u5bf9\u6bd4\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u9ad8\u65af\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u5728\u5b89\u5168\u4ea4\u4e92\u548c\u8f68\u8ff9\u6267\u884c\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.13009", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13009", "abs": "https://arxiv.org/abs/2510.13009", "authors": ["Basad Barajeeh", "Ala Yankouskaya", "Sameha AlShakhsi", "Chun Sing Maxwell Ho", "Guandong Xu", "Raian Ali"], "title": "Developing and Validating the Arabic Version of the Attitudes Toward Large Language Models Scale", "comment": "28 Pages", "summary": "As the use of large language models (LLMs) becomes increasingly global,\nunderstanding public attitudes toward these systems requires tools that are\nadapted to local contexts and languages. In the Arab world, LLM adoption has\ngrown rapidly with both globally dominant platforms and regional ones like\nFanar and Jais offering Arabic-specific solutions. This highlights the need for\nculturally and linguistically relevant scales to accurately measure attitudes\ntoward LLMs in the region. Tools assessing attitudes toward artificial\nintelligence (AI) can provide a base for measuring attitudes specific to LLMs.\nThe 5-item Attitudes Toward Artificial Intelligence (ATAI) scale, which\nmeasures two dimensions, the AI Fear and the AI Acceptance, has been recently\nadopted and adapted to develop new instruments in English using a sample from\nthe UK: the Attitudes Toward General LLMs (AT-GLLM) and Attitudes Toward\nPrimary LLM (AT-PLLM) scales. In this paper, we translate the two scales,\nAT-GLLM and AT-PLLM, and validate them using a sample of 249 Arabic-speaking\nadults. The results show that the scale, translated into Arabic, is a reliable\nand valid tool that can be used for the Arab population and language.\nPsychometric analyses confirmed a two-factor structure, strong measurement\ninvariance across genders, and good internal reliability. The scales also\ndemonstrated strong convergent and discriminant validity. Our scales will\nsupport research in a non-Western context, a much-needed effort to help draw a\nglobal picture of LLM perceptions, and will also facilitate localized research\nand policy-making in the Arab region.", "AI": {"tldr": "\u672c\u6587\u7ffb\u8bd1\u5e76\u9a8c\u8bc1\u4e86\u9488\u5bf9\u963f\u62c9\u4f2f\u8bed\u4eba\u7fa4\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6001\u5ea6\u91cf\u8868\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u5c3a\u5ea6\u5728\u6d4b\u91cf\u963f\u62c9\u4f2f\u5730\u533a\u5bf9LLMs\u7684\u6001\u5ea6\u65b9\u9762\u6709\u6548\u4e14\u53ef\u9760\u3002", "motivation": "\u5168\u7403\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4f7f\u7528\u65e5\u76ca\u589e\u957f\uff0c\u9700\u8981\u9002\u5e94\u5f53\u5730\u6587\u5316\u548c\u8bed\u8a00\u7684\u5de5\u5177\uff0c\u4ee5\u51c6\u786e\u6d4b\u91cf\u516c\u4f17\u6001\u5ea6\u3002", "method": "\u5c06\u4e24\u79cd\u6001\u5ea6\u91cf\u8868\uff08AT-GLLM\u548cAT-PLLM\uff09\u7ffb\u8bd1\u4e3a\u963f\u62c9\u4f2f\u8bed\uff0c\u5e76\u5728249\u540d\u963f\u62c9\u4f2f\u8bed\u6210\u4eba\u6837\u672c\u4e2d\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u7ffb\u8bd1\u540e\u7684\u5c3a\u5ea6\u5728\u963f\u62c9\u4f2f\u8bed\u4e2d\u663e\u793a\u51fa\u826f\u597d\u7684\u53ef\u9760\u6027\u548c\u6709\u6548\u6027\uff0c\u5e76\u8bc1\u5b9e\u4e86\u4e24\u56e0\u7d20\u7ed3\u6784\uff0c\u4e14\u5728\u6027\u522b\u95f4\u5177\u6709\u5f3a\u6d4b\u91cf\u4e0d\u53d8\u6027\uff0c\u5185\u90e8\u53ef\u9760\u6027\u826f\u597d\u3002", "conclusion": "\u7ffb\u8bd1\u5e76\u9a8c\u8bc1\u7684\u963f\u62c9\u4f2f\u8bed\u5c3a\u5ea6\u662f\u53ef\u9760\u6709\u6548\u7684\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u7814\u7a76\u963f\u62c9\u4f2f\u5730\u533a\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8ba4\u77e5\u3002"}}
{"id": "2510.12924", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12924", "abs": "https://arxiv.org/abs/2510.12924", "authors": ["Pavel Pochobradsk\u00fd", "Ond\u0159ej Proch\u00e1zka", "Robert P\u011bni\u010dka", "Vojt\u011bch Von\u00e1sek", "Martin Saska"], "title": "Geometric Model Predictive Path Integral for Agile UAV Control with Online Collision Avoidance", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "In this letter, we introduce Geometric Model Predictive Path Integral\n(GMPPI), a sampling-based controller capable of tracking agile trajectories\nwhile avoiding obstacles. In each iteration, GMPPI generates a large number of\ncandidate rollout trajectories and then averages them to create a nominal\ncontrol to be followed by the Unmanned Aerial Vehicle (UAV). We propose using\ngeometric SE(3) control to generate part of the rollout trajectories,\nsignificantly increasing precision in agile flight. Furthermore, we introduce\nvarying rollout simulation time step length and dynamic cost and noise\nparameters, vastly improving tracking performance of smooth and low-speed\ntrajectories over an existing Model Predictive Path Integral (MPPI)\nimplementation. Finally, we propose an integration of GMPPI with a stereo depth\ncamera, enabling online obstacle avoidance at high speeds, a crucial step\ntowards autonomous UAV flights in complex environments. The proposed controller\ncan track simulated agile reference trajectories with position error similar to\nthe geometric SE(3) controller. However, the same configuration of the proposed\ncontroller can avoid obstacles in a simulated forest environment at speeds of\nup to 13m/s, surpassing the performance of a state-of-the-art obstacle-aware\nplanner. In real-world experiments, GMPPI retains the capability to track agile\ntrajectories and avoids obstacles at speeds of up to 10m/s.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGMPPI\u63a7\u5236\u5668\uff0c\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e2d\u4ee5\u9ad8\u901f\u5ea6\u8ddf\u8e2a\u7075\u6d3b\u8f68\u8ff9\u5e76\u907f\u5f00\u969c\u788d\u7269\u3002", "motivation": "\u5728\u590d\u6742\u73af\u5883\u4e0b\u5b9e\u73b0\u65e0\u4eba\u673a\u9ad8\u6548\u907f\u969c\u548c\u7075\u6d3b\u8f68\u8ff9\u8ddf\u8e2a\u7684\u9700\u6c42\u3002", "method": "GMPPI\u901a\u8fc7\u751f\u6210\u4f17\u591a\u5019\u9009\u8f68\u8ff9\u8fdb\u884c\u91c7\u6837\uff0c\u5e76\u4f7f\u7528\u51e0\u4f55SE(3)\u63a7\u5236\u6765\u63d0\u9ad8\u7cbe\u5ea6\uff0c\u7ed3\u5408\u53d8\u5316\u7684\u4eff\u771f\u65f6\u95f4\u6b65\u957f\u548c\u52a8\u6001\u6210\u672c\u53ca\u566a\u58f0\u53c2\u6570\uff0c\u4ee5\u63d0\u5347\u8ddf\u8e2a\u6027\u80fd\u3002", "result": "GMPPI\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u73af\u5883\u4e2d\u5747\u80fd\u4ee5\u9ad8\u8fbe13m/s\u7684\u901f\u5ea6\u907f\u969c\uff0c\u4e14\u8ddf\u8e2a\u8bef\u5dee\u63a5\u8fd1\u51e0\u4f55SE(3)\u63a7\u5236\u5668\u3002", "conclusion": "GMPPI\u63a7\u5236\u5668\u5728\u6a21\u62df\u548c\u5b9e\u9645\u5b9e\u9a8c\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u5728\u4fdd\u8bc1\u7075\u6d3b\u8f68\u8ff9\u8ddf\u8e2a\u7684\u540c\u65f6\uff0c\u907f\u514d\u969c\u788d\u7269\u3002"}}
{"id": "2510.13011", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13011", "abs": "https://arxiv.org/abs/2510.13011", "authors": ["Crystal Qian", "Vivian Tsai", "Michael Behr", "Nada Hussein", "L\u00e9o Laugier", "Nithum Thain", "Lucas Dixon"], "title": "Deliberate Lab: A Platform for Real-Time Human-AI Social Experiments", "comment": null, "summary": "Social and behavioral scientists increasingly aim to study how humans\ninteract, collaborate, and make decisions alongside artificial intelligence.\nHowever, the experimental infrastructure for such work remains underdeveloped:\n(1) few platforms support real-time, multi-party studies at scale; (2) most\ndeployments require bespoke engineering, limiting replicability and\naccessibility, and (3) existing tools do not treat AI agents as first-class\nparticipants. We present Deliberate Lab, an open-source platform for\nlarge-scale, real-time behavioral experiments that supports both human\nparticipants and large language model (LLM)-based agents. We report on a\n12-month public deployment of the platform (N=88 experimenters, N=9195\nexperiment participants), analyzing usage patterns and workflows. Case studies\nand usage scenarios are aggregated from platform users, complemented by\nin-depth interviews with select experimenters. By lowering technical barriers\nand standardizing support for hybrid human-AI experimentation, Deliberate Lab\nexpands the methodological repertoire for studying collective decision-making\nand human-centered AI.", "AI": {"tldr": "Deliberate Lab\u662f\u4e00\u4e2a\u5f00\u6e90\u5e73\u53f0\uff0c\u65e8\u5728\u4fc3\u8fdb\u4eba\u7c7b\u4e0eAI\u7684\u5408\u4f5c\u7814\u7a76\uff0c\u964d\u4f4e\u6280\u672f\u95e8\u69db\uff0c\u62d3\u5c55\u5b9e\u9a8c\u65b9\u6cd5\u3002", "motivation": "\u793e\u4f1a\u548c\u884c\u4e3a\u79d1\u5b66\u5bb6\u5e0c\u671b\u7814\u7a76\u4eba\u7c7b\u4e0e\u4eba\u5de5\u667a\u80fd\u7684\u4e92\u52a8\u3001\u5408\u4f5c\u548c\u51b3\u7b56\uff0c\u4f46\u5b9e\u9a8c\u57fa\u7840\u8bbe\u65bd\u4e0d\u591f\u5b8c\u5584\uff0c\u9650\u5236\u4e86\u76f8\u5173\u7814\u7a76\u7684\u5f00\u5c55\u3002", "method": "\u62a5\u544a\u4e86\u5e73\u53f0\u768412\u4e2a\u6708\u516c\u5171\u90e8\u7f72\uff0c\u5206\u6790\u4f7f\u7528\u6a21\u5f0f\u548c\u5de5\u4f5c\u6d41\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u548c\u6df1\u5ea6\u8bbf\u8c08\u6536\u96c6\u6570\u636e\u3002", "result": "\u63d0\u51fa\u4e86Deliberate Lab\uff0c\u4e00\u4e2a\u5f00\u6e90\u5e73\u53f0\uff0c\u652f\u6301\u5927\u89c4\u6a21\u5b9e\u65f6\u884c\u4e3a\u5b9e\u9a8c\uff0c\u5305\u542b\u4eba\u7c7b\u53c2\u4e0e\u8005\u548c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u3002", "conclusion": "\u901a\u8fc7\u964d\u4f4e\u6280\u672f\u58c1\u5792\u548c\u6807\u51c6\u5316\u652f\u6301\uff0cDeliberate Lab\u4e3a\u96c6\u4f53\u51b3\u7b56\u548c\u4ee5\u4eba\u4e3a\u672c\u7684AI\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u8bba\u3002"}}
{"id": "2510.12962", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12962", "abs": "https://arxiv.org/abs/2510.12962", "authors": ["Michal Mina\u0159\u00edk", "Vojt\u011bch Von\u00e1sek", "Robert P\u011bni\u010dka"], "title": "Enhancing Sampling-based Planning with a Library of Paths", "comment": null, "summary": "Path planning for 3D solid objects is a challenging problem, requiring a\nsearch in a six-dimensional configuration space, which is, nevertheless,\nessential in many robotic applications such as bin-picking and assembly. The\ncommonly used sampling-based planners, such as Rapidly-exploring Random Trees,\nstruggle with narrow passages where the sampling probability is low, increasing\nthe time needed to find a solution. In scenarios like robotic bin-picking,\nvarious objects must be transported through the same environment. However,\ntraditional planners start from scratch each time, losing valuable information\ngained during the planning process. We address this by using a library of past\nsolutions, allowing the reuse of previous experiences even when planning for a\nnew, previously unseen object. Paths for a set of objects are stored, and when\nplanning for a new object, we find the most similar one in the library and use\nits paths as approximate solutions, adjusting for possible mutual\ntransformations. The configuration space is then sampled along the approximate\npaths. Our method is tested in various narrow passage scenarios and compared\nwith state-of-the-art methods from the OMPL library. Results show significant\nspeed improvements (up to 85% decrease in the required time) of our method,\noften finding a solution in cases where the other planners fail. Our\nimplementation of the proposed method is released as an open-source package.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u590d\u7528\u8fc7\u5f80\u89e3\u51b3\u65b9\u6848\u6765\u663e\u8457\u63d0\u9ad8\u72ed\u7a84\u901a\u9053\u4e2d\u7684\u6548\u7387\uff0c\u6d4b\u8bd5\u663e\u793a\u901f\u5ea6\u5927\u5e45\u63d0\u5347\uff0c\u5df2\u5f00\u6e90\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u5728\u72ed\u7a84\u901a\u9053\u548c\u73af\u5883\u4e2d\u591a\u7269\u4f53\u8fd0\u8f93\u65f6\u4f20\u7edf\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5e93\u7684\u8fc7\u5f80\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u9ad8\u65b0\u5bf9\u8c61\u8def\u5f84\u89c4\u5212\u7684\u6548\u7387\uff0c\u5e76\u901a\u8fc7\u91c7\u6837\u751f\u6210\u8fd1\u4f3c\u8def\u5f84\u3002", "result": "\u5728\u591a\u79cd\u72ed\u7a84\u901a\u9053\u573a\u666f\u4e2d\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u6bd4OMPL\u5e93\u4e2d\u7684\u5176\u4ed6\u6700\u5148\u8fdb\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u89c4\u5212\u901f\u5ea6\uff0c\u65f6\u95f4\u9700\u6c42\u51cf\u5c11\u4e86\u591a\u8fbe85%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u5229\u7528\u8fc7\u53bb\u7684\u89e3\u51b3\u65b9\u6848\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u72ed\u7a84\u901a\u9053\u4e2d\u7684\u89c4\u5212\u6548\u7387\u3002"}}
{"id": "2510.13091", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.13091", "abs": "https://arxiv.org/abs/2510.13091", "authors": ["Wugeng Zheng", "Guohou Shan"], "title": "Unmasking Hiring Bias: Platform Data Analysis and Controlled Experiments on Bias in Online Freelance Marketplaces via RAG-LLM Generated Contents", "comment": null, "summary": "Online freelance marketplaces, a rapidly growing part of the global labor\nmarket, are creating a fair environment where professional skills are the main\nfactor for hiring. While these platforms can reduce bias from traditional\nhiring, the personal information in user profiles raises concerns about ongoing\ndiscrimination. Past studies on this topic have mostly used existing data,\nwhich makes it hard to control for other factors and clearly see the effect of\nthings like gender or race. To solve these problems, this paper presents a new\nmethod that uses Retrieval-Augmented Generation (RAG) with a Large Language\nModel (LLM) to create realistic, artificial freelancer profiles for controlled\nexperiments. This approach effectively separates individual factors, enabling a\nclearer statistical analysis of how different variables influence the\nfreelancer project process. In addition to analyzing extracted data with\ntraditional statistical methods for post-project stage analysis, our research\nutilizes a dataset with highly controlled variables, generated by an RAG-LLM,\nto conduct a simulated hiring experiment for pre-project stage analysis. The\nresults of our experiments show that, regarding gender, while no significant\npreference emerged in initial hiring decisions, female freelancers are\nsubstantially more likely to receive imperfect ratings post-project stage.\nRegarding regional bias, a strong and consistent preference favoring US-based\nfreelancers shows that people are more likely to be selected in the simulated\nexperiments, perceived as more leader-like, and receive higher ratings on the\nlive platform.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7RAG\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u63a7\u5236\u53d8\u91cf\u7684\u865a\u5047\u81ea\u7531\u804c\u4e1a\u8005\u6863\u6848\uff0c\u4ee5\u7814\u7a76\u5728\u7ebf\u81ea\u7531\u804c\u4e1a\u5e02\u573a\u4e2d\u7684\u6027\u522b\u548c\u5730\u533a\u504f\u89c1\u3002", "motivation": "\u968f\u7740\u5728\u7ebf\u81ea\u7531\u804c\u4e1a\u5e02\u573a\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5c3d\u7ba1\u8fd9\u4e9b\u5e73\u53f0\u6709\u52a9\u4e8e\u51cf\u5c11\u4f20\u7edf\u62db\u8058\u4e2d\u7684\u504f\u89c1\uff0c\u4f46\u7528\u6237\u4e2a\u4eba\u4fe1\u606f\u7684\u4f7f\u7528\u4ecd\u5f15\u53d1\u4e86\u6301\u7eed\u7684\u6b67\u89c6\u62c5\u5fe7\u3002", "method": "\u672c\u6587\u91c7\u7528\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u7ec4\u5408\u65b9\u6cd5\uff0c\u521b\u5efa\u865a\u5047\u81ea\u7531\u804c\u4e1a\u8005\u6863\u6848\u7528\u4e8e\u6a21\u62df\u62db\u8058\u5b9e\u9a8c\uff0c\u5206\u6790\u53d8\u91cf\u5bf9\u62db\u8058\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "result": "\u4f7f\u7528\u57fa\u4e8eRAG\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u521b\u9020\u7684\u865a\u5047\u81ea\u7531\u804c\u4e1a\u8005\u6863\u6848\u8fdb\u884c\u63a7\u5236\u5b9e\u9a8c\uff0c\u5f97\u51fa\u6027\u522b\u548c\u5730\u533a\u504f\u89c1\u5bf9\u62db\u8058\u8fc7\u7a0b\u7684\u5f71\u54cd\u3002", "conclusion": "\u5c3d\u7ba1\u5728\u521d\u7ea7\u62db\u8058\u51b3\u7b56\u4e2d\u6027\u522b\u504f\u89c1\u4e0d\u663e\u8457\uff0c\u4f46\u5973\u6027\u81ea\u7531\u804c\u4e1a\u8005\u5728\u9879\u76ee\u540e\u671f\u66f4\u53ef\u80fd\u6536\u5230\u4e0d\u4f73\u8bc4\u4f30\uff0c\u4e14\u7f8e\u56fd\u81ea\u7531\u804c\u4e1a\u8005\u5728\u6a21\u62df\u62db\u8058\u4e2d\u5219\u8868\u73b0\u51fa\u660e\u663e\u7684\u504f\u597d\u3002"}}
{"id": "2510.12970", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12970", "abs": "https://arxiv.org/abs/2510.12970", "authors": ["Baxi Chong", "Tianyu Wang", "Kelimar Diaz", "Christopher J. Pierce", "Eva Erickson", "Julian Whitman", "Yuelin Deng", "Esteban Flores", "Ruijie Fu", "Juntao He", "Jianfeng Lin", "Hang Lu", "Guillaume Sartoretti", "Howie Choset", "Daniel I. Goldman"], "title": "The Omega Turn: A General Turning Template for Elongate Robots", "comment": null, "summary": "Elongate limbless robots have the potential to locomote through tightly\npacked spaces for applications such as search-and-rescue and industrial\ninspections. The capability to effectively and robustly maneuver elongate\nlimbless robots is crucial to realize such potential. However, there has been\nlimited research on turning strategies for such systems. To achieve effective\nand robust turning performance in cluttered spaces, we take inspiration from a\nmicroscopic nematode, C. elegans, which exhibits remarkable maneuverability in\nrheologically complex environments partially because of its ability to perform\nomega turns. Despite recent efforts to analyze omega turn kinematics, it\nremains unknown if there exists a wave equation sufficient to prescribe an\nomega turn, let alone its reconstruction on robot platforms. Here, using a\ncomparative theory-biology approach, we prescribe the omega turn as a\nsuperposition of two traveling waves. With wave equations as a guideline, we\ndesign a controller for limbless robots enabling robust and effective turning\nbehaviors in lab and cluttered field environments. Finally, we show that such\nomega turn controllers can also generalize to elongate multi-legged robots,\ndemonstrating an alternative effective body-driven turning strategy for\nelongate robots, with and without limbs.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ebf\u866bomega\u8f6c\u7684\u63a7\u5236\u5668\uff0c\u4ee5\u63d0\u9ad8\u65e0\u80a2\u673a\u5668\u4eba\u5728\u62e5\u6324\u73af\u5883\u4e2d\u7684\u8f6c\u5f2f\u80fd\u529b\u3002", "motivation": "\u65e8\u5728\u6539\u5584\u65e0\u80a2\u673a\u5668\u4eba\u7684\u8f6c\u5f2f\u7b56\u7565\uff0c\u4ee5\u589e\u5f3a\u5176\u5728\u641c\u7d22\u6551\u63f4\u548c\u5de5\u4e1a\u68c0\u67e5\u7b49\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u5c06omega\u8f6c\u63cf\u8ff0\u4e3a\u4e24\u4e2a\u4f20\u64ad\u6ce2\u7684\u53e0\u52a0\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u63a7\u5236\u5668\u6765\u5b9e\u73b0\u9ad8\u6548\u3001\u7a33\u5065\u7684\u8f6c\u5f2f\u884c\u4e3a\u3002", "result": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5fae\u89c2\u7ebf\u866b\u7684omega\u8f6c\u5f2f\u7b56\u7565\uff0c\u7528\u4e8e\u8ba9\u65e0\u80a2\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u9ad8\u6548\u673a\u52a8\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684omega\u8f6c\u5f2f\u63a7\u5236\u5668\u4e0d\u4ec5\u9002\u7528\u4e8e\u65e0\u80a2\u673a\u5668\u4eba\uff0c\u8fd8\u53ef\u4ee5\u63a8\u5e7f\u5230\u591a\u8db3\u673a\u5668\u4eba\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u9ad8\u7ea7\u9a71\u52a8\u8f6c\u5f2f\u7b56\u7565\u3002"}}
{"id": "2510.13123", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.13123", "abs": "https://arxiv.org/abs/2510.13123", "authors": ["Tangyao Li", "Yitong Zhu", "Hai-Ning Liang", "Yuyang Wang"], "title": "Adapting to the User: A Systematic Review of Personalized Interaction in VR", "comment": "35 pages, 7 figures, submitted to ACM", "summary": "As virtual reality (VR) systems become increasingly more advanced, they are\nlikewise expected to respond intelligently and adapt to individual user states,\nabilities, and preferences. Recent work has explored how VR can be adapted and\ntailored to individual users. However, existing reviews tend to address either\nuser-state sensing or adaptive interaction design in isolation, limiting our\nunderstanding of their combined implementation in VR. Therefore, in this paper,\nwe examine the growing research on personalized interaction in VR, with a\nparticular focus on utilizing participants' immersion information and\nadaptation mechanisms to modify virtual environments and enhance engagement,\nperformance, or a specific goal. We synthesize findings from studies that\nemploy adaptive techniques across diverse application domains and summarize a\nfive-stage conceptual framework that unifies adaptive mechanisms across\ndomains. Our analysis reveals emerging trends, including the integration of\nmultimodal sensors, an increasing reliance on user state inference, and the\nchallenge of balancing responsiveness with transparency. We conclude by\nproposing future directions for developing more user-centered VR systems.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4e2a\u6027\u5316\u4e92\u52a8\u5728\u865a\u62df\u73b0\u5b9e\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u7edf\u4e00\u9002\u5e94\u673a\u5236\u7684\u6846\u67b6\uff0c\u5e76\u8ba8\u8bba\u4e86\u6574\u5408\u591a\u6a21\u6001\u4f20\u611f\u5668\u53ca\u5176\u6311\u6218\u3002", "motivation": "\u63a2\u8ba8\u865a\u62df\u73b0\u5b9e\u7cfb\u7edf\u5982\u4f55\u667a\u80fd\u5730\u54cd\u5e94\u548c\u9002\u5e94\u5355\u4e2a\u7528\u6237\u7684\u72b6\u6001\u3001\u80fd\u529b\u548c\u504f\u597d\u3002", "method": "\u901a\u8fc7\u7efc\u5408\u591a\u9879\u7814\u7a76\u7684\u53d1\u73b0\uff0c\u5206\u6790\u4e2a\u6027\u5316\u4e92\u52a8\u4e2d\u7684\u9002\u5e94\u6027\u6280\u672f\uff0c\u5e76\u603b\u7ed3\u81ea\u9002\u5e94\u673a\u5236\u5728\u4e0d\u540c\u9886\u57df\u7684\u5e94\u7528\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e94\u9636\u6bb5\u6982\u5ff5\u6846\u67b6\uff0c\u4ee5\u7edf\u4e00\u4e0d\u540c\u5e94\u7528\u9886\u57df\u4e2d\u7684\u81ea\u9002\u5e94\u673a\u5236\uff0c\u5e76\u603b\u7ed3\u4e86\u4ece\u591a\u9879\u7814\u7a76\u4e2d\u63d0\u53d6\u7684\u53d1\u73b0\u3002", "conclusion": "\u5efa\u8bae\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\uff0c\u65e8\u5728\u5f00\u53d1\u66f4\u52a0\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u865a\u62df\u73b0\u5b9e\u7cfb\u7edf\u3002"}}
{"id": "2510.12971", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12971", "abs": "https://arxiv.org/abs/2510.12971", "authors": ["Anran Zhang", "Hanzhi Chen", "Yannick Burkhardt", "Yao Zhong", "Johannes Betz", "Helen Oleynikova", "Stefan Leutenegger"], "title": "Actron3D: Learning Actionable Neural Functions from Videos for Transferable Robotic Manipulation", "comment": "8 pages, 5 figures", "summary": "We present Actron3D, a framework that enables robots to acquire transferable\n6-DoF manipulation skills from just a few monocular, uncalibrated, RGB-only\nhuman videos. At its core lies the Neural Affordance Function, a compact\nobject-centric representation that distills actionable cues from diverse\nuncalibrated videos-geometry, visual appearance, and affordance-into a\nlightweight neural network, forming a memory bank of manipulation skills.\nDuring deployment, we adopt a pipeline that retrieves relevant affordance\nfunctions and transfers precise 6-DoF manipulation policies via coarse-to-fine\noptimization, enabled by continuous queries to the multimodal features encoded\nin the neural functions. Experiments in both simulation and the real world\ndemonstrate that Actron3D significantly outperforms prior methods, achieving a\n14.9 percentage point improvement in average success rate across 13 tasks while\nrequiring only 2-3 demonstration videos per task.", "AI": {"tldr": "Actron3D\u6846\u67b6\u901a\u8fc7\u5c11\u6570\u4eba\u7c7b\u89c6\u9891\u8d4b\u4e88\u673a\u5668\u4eba\u53ef\u8f6c\u79fb\u76846-DoF\u64cd\u63a7\u6280\u80fd\uff0c\u663e\u8457\u63d0\u5347\u6210\u529f\u7387\u3002", "motivation": "\u65e8\u5728\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u901a\u8fc7\u4eba\u7c7b\u89c6\u9891\u5feb\u901f\u5b66\u4e60\u591a\u6837\u5316\u7684\u64cd\u63a7\u6280\u80fd\uff0c\u800c\u4e0d\u9700\u8981\u590d\u6742\u7684\u6821\u51c6\u6216\u5927\u91cf\u7684\u6f14\u793a\u3002", "method": "\u63d0\u51fa\u4e86Actron3D\u6846\u67b6\uff0c\u901a\u8fc7\u5c11\u91cf\u5355\u76ee\u3001\u672a\u6821\u51c6\u7684RGB\u4eba\u7c7b\u89c6\u9891\uff0c\u4f7f\u673a\u5668\u4eba\u83b7\u5f97\u53ef\u8f6c\u79fb\u76846\u4e2a\u81ea\u7531\u5ea6\u64cd\u63a7\u6280\u80fd\u3002", "result": "\u572813\u4e2a\u4efb\u52a1\u4e2d\uff0cActron3D\u76f8\u6bd4\u4e8e\u4e4b\u524d\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5e73\u5747\u6210\u529f\u7387\uff0c\u63d0\u5347\u5e45\u5ea6\u8fbe\u5230\u4e8614.9\u4e2a\u767e\u5206\u70b9\uff0c\u540c\u65f6\u6bcf\u4e2a\u4efb\u52a1\u53ea\u9700\u89812-3\u4e2a\u6f14\u793a\u89c6\u9891\u3002", "conclusion": "\u5b9e\u9a8c\u8868\u660e\uff0cActron3D\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u673a\u5668\u4eba\u64cd\u63a7\u9886\u57df\u7684\u6709\u6548\u6027\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2510.13539", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.13539", "abs": "https://arxiv.org/abs/2510.13539", "authors": ["Mubaris Nadeem", "Johannes Zenkert", "Christian Weber", "Madjid Fathi", "Muhammad Hamza"], "title": "Smart UX-design for Rescue Operations Wearable - A Knowledge Graph Informed Visualization Approach for Information Retrieval in Emergency Situations", "comment": "Conference paper for 2023 IEEE International Conference on Electro\n  Information Technology (eIT), KIRETT Project, University of Siegen, Germany", "summary": "This paper presents a knowledge graph-informed smart UX-design approach for\nsupporting information retrieval for a wearable, providing treatment\nrecommendations during emergency situations to health professionals. This paper\ndescribes requirements that are unique to knowledge graph-based solutions, as\nwell as the direct requirements of health professionals. The resulting\nimplementation is provided for the project, which main goal is to improve\nfirst-aid rescue operations by supporting artificial intelligence in situation\ndetection and knowledge graph representation via a contextual-based\nrecommendation for treatment assistance.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u77e5\u8bc6\u56fe\u8c31\u9a71\u52a8\u7684\u667a\u80fdUX\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u6025\u6551\u60c5\u51b5\u4e0b\u5065\u5eb7\u4e13\u4e1a\u4eba\u5458\u7684\u4fe1\u606f\u68c0\u7d22\u548c\u6cbb\u7597\u5efa\u8bae\u80fd\u529b\u3002", "motivation": "\u5728\u6025\u6551\u60c5\u51b5\u4e0b\uff0c\u5065\u5eb7\u4e13\u4e1a\u4eba\u5458\u9700\u8981\u5b9e\u65f6\u548c\u51c6\u786e\u7684\u4fe1\u606f\u4ee5\u63d0\u4f9b\u66f4\u597d\u7684\u6cbb\u7597\uff0c\u5e76\u63d0\u9ad8\u6551\u63f4\u6548\u679c\u3002", "method": "\u91c7\u7528\u77e5\u8bc6\u56fe\u8c31\u7ed3\u5408\u4eba\u5de5\u667a\u80fd\u7684\u65b9\u6cd5\u6765\u652f\u6301\u6025\u6551\u65f6\u7684\u4fe1\u606f\u68c0\u7d22\u548c\u6cbb\u7597\u5efa\u8bae\u3002", "result": "\u5b9e\u73b0\u4e86\u4e00\u4e2a\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u53ef\u4ee5\u901a\u8fc7\u4e0a\u4e0b\u6587\u6811\u7acb\u6cbb\u7597\u5e2e\u52a9\u7684\u63a8\u8350\uff0c\u4ece\u800c\u63d0\u9ad8\u6025\u6551\u54cd\u5e94\u6548\u7387\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u77e5\u8bc6\u56fe\u8c31\u9a71\u52a8\u7684\u667a\u80fd\u7528\u6237\u4f53\u9a8c\u8bbe\u8ba1\u6709\u52a9\u4e8e\u63d0\u9ad8\u6025\u6551\u64cd\u4f5c\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.12992", "categories": ["cs.RO", "cs.CL", "cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.12992", "abs": "https://arxiv.org/abs/2510.12992", "authors": ["Neel P. Bhatt", "Po-han Li", "Kushagra Gupta", "Rohan Siva", "Daniel Milan", "Alexander T. Hogue", "Sandeep P. Chinchali", "David Fridovich-Keil", "Zhangyang Wang", "Ufuk Topcu"], "title": "UNCAP: Uncertainty-Guided Planning Using Natural Language Communication for Cooperative Autonomous Vehicles", "comment": null, "summary": "Safe large-scale coordination of multiple cooperative connected autonomous\nvehicles (CAVs) hinges on communication that is both efficient and\ninterpretable. Existing approaches either rely on transmitting high-bandwidth\nraw sensor data streams or neglect perception and planning uncertainties\ninherent in shared data, resulting in systems that are neither scalable nor\nsafe. To address these limitations, we propose Uncertainty-Guided Natural\nLanguage Cooperative Autonomous Planning (UNCAP), a vision-language model-based\nplanning approach that enables CAVs to communicate via lightweight natural\nlanguage messages while explicitly accounting for perception uncertainty in\ndecision-making. UNCAP features a two-stage communication protocol: (i) an ego\nCAV first identifies the subset of vehicles most relevant for information\nexchange, and (ii) the selected CAVs then transmit messages that quantitatively\nexpress their perception uncertainty. By selectively fusing messages that\nmaximize mutual information, this strategy allows the ego vehicle to integrate\nonly the most relevant signals into its decision-making, improving both the\nscalability and reliability of cooperative planning. Experiments across diverse\ndriving scenarios show a 63% reduction in communication bandwidth with a 31%\nincrease in driving safety score, a 61% reduction in decision uncertainty, and\na four-fold increase in collision distance margin during near-miss events.\nProject website: https://uncap-project.github.io/", "AI": {"tldr": "UNCAP\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6d88\u606f\u548c\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u8003\u8651\uff0c\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u534f\u4f5c\u7684\u6548\u7387\u4e0e\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u4e8e\u9ad8\u5e26\u5bbd\u539f\u59cb\u4f20\u611f\u5668\u6570\u636e\uff0c\u8981\u4e48\u5ffd\u89c6\u5171\u4eab\u6570\u636e\u4e2d\u7684\u611f\u77e5\u548c\u89c4\u5212\u4e0d\u786e\u5b9a\u6027\uff0c\u5bfc\u81f4\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b89\u5168\u6027\u4e0d\u8db3\u3002\u56e0\u6b64\uff0c\u63d0\u51fa\u7684UNCAP\u5bf9\u8fd9\u4e9b\u95ee\u9898\u8fdb\u884c\u4e86\u9488\u5bf9\u6027\u7684\u4f18\u5316\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u7684\u901a\u4fe1\u534f\u8bae\uff1a\u9996\u5148\u786e\u5b9a\u4e0e\u4fe1\u606f\u4ea4\u6362\u6700\u76f8\u5173\u7684\u8f66\u8f86\uff0c\u7136\u540e\u8fd9\u4e9b\u8f66\u8f86\u901a\u8fc7\u5b9a\u91cf\u65b9\u5f0f\u8868\u8fbe\u5b83\u4eec\u7684\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5UNCAP\uff0c\u65e8\u5728\u4f18\u5316\u591a\u8f86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e4b\u95f4\u7684\u901a\u4fe1\uff0c\u4f7f\u5176\u65e2\u9ad8\u6548\u53c8\u53ef\u89e3\u91ca\uff0c\u4ece\u800c\u63d0\u9ad8\u534f\u4f5c\u89c4\u5212\u7684\u5b89\u5168\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\uff0cUNCAP\u80fd\u591f\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u5e26\u5bbd\uff0c\u63d0\u9ad8\u9a7e\u9a76\u5b89\u5168\u6027\uff0c\u51cf\u5c11\u51b3\u7b56\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5728\u4e34\u8fd1\u78b0\u649e\u4e8b\u4ef6\u4e2d\u589e\u5f3a\u5b89\u5168\u8ddd\u79bb\u3002"}}
{"id": "2510.13731", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.13731", "abs": "https://arxiv.org/abs/2510.13731", "authors": ["Areen Khalaila", "Dylan Cashman"], "title": "Speculating a Tactile Grammar: Toward Task-Aligned Chart Design for Non-Visual Perception", "comment": null, "summary": "Tactile graphics are often adapted from visual chart designs, yet many of\nthese encodings do not translate effectively to non-visual exploration. Blind\nand low-vision (BLV) people employ a variety of physical strategies such as\nmeasuring lengths with fingers or scanning for texture differences to interpret\ntactile charts. These observations suggest an opportunity to move beyond direct\nvisual translation and toward a tactile-first design approach. We outline a\nspeculative tactile design framework that explores how data analysis tasks may\nalign with tactile strategies and encoding choices. While this framework is not\nyet validated, it offers a lens for generating tactile-first chart designs and\nsets the stage for future empirical exploration. We present speculative mockups\nto illustrate how the Tactile Perceptual Grammar might guide the design of an\naccessible COVID-19 dashboard. This scenario illustrates how the grammar can\nguide encoding choices that better support comparison, trend detection, and\nproportion estimation in tactile formats. We conclude with design implications\nand a discussion of future validation through co-design and task-based\nevaluation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u89e6\u89c9\u8bbe\u8ba1\u6846\u67b6\uff0c\u65e8\u5728\u4f18\u5316\u76f2\u4eba\u548c\u89c6\u529b\u969c\u788d\u8005\u7684\u89e6\u89c9\u56fe\u8868\u8bbe\u8ba1\uff0c\u5f3a\u8c03\u89e6\u89c9\u4f18\u5148\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u6570\u636e\u5206\u6790\u4efb\u52a1\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u89e6\u89c9\u56fe\u8868\u8bbe\u8ba1\u4e3b\u8981\u4ee5\u89c6\u89c9\u4e3a\u57fa\u7840\uff0c\u672a\u80fd\u6709\u6548\u652f\u6301\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u8005\u7684\u63a2\u7d22\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u6982\u5ff5\u5316\u7684\u89e6\u89c9\u8bbe\u8ba1\u6846\u67b6\u548c\u6a21\u62df\u6837\u672c\u63a2\u7d22\u56fe\u8868\u8bbe\u8ba1\uff0c\u5c55\u793a\u5982\u4f55\u66f4\u597d\u5730\u652f\u6301\u89e6\u89c9\u683c\u5f0f\u4e0b\u7684\u6bd4\u8f83\u3001\u8d8b\u52bf\u68c0\u6d4b\u548c\u6bd4\u4f8b\u4f30\u8ba1\u3002", "result": "\u6846\u67b6\u5c55\u793a\u4e86\u5982\u4f55\u6839\u636e\u89e6\u89c9\u7b56\u7565\u548c\u7f16\u7801\u9009\u62e9\u751f\u6210\u6709\u6548\u7684\u89e6\u89c9\u56fe\u8868\u8bbe\u8ba1\uff0c\u4ee5\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002", "conclusion": "\u63d0\u51fa\u7684\u8bbe\u8ba1\u6846\u67b6\u4e3a\u89e6\u89c9\u56fe\u8868\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u9700\u901a\u8fc7\u540e\u7eed\u7684\u8054\u5408\u8bbe\u8ba1\u548c\u4efb\u52a1\u8bc4\u4f30\u8fdb\u884c\u9a8c\u8bc1\u3002"}}
{"id": "2510.13005", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13005", "abs": "https://arxiv.org/abs/2510.13005", "authors": ["Robert Muldrow", "Channing Ludden", "Christopher Petersen"], "title": "Development of a Linear Guide-Rail Testbed for Physically Emulating ISAM Operations", "comment": "12 pages, 4 figures, AAS/AIAA Space Flight Mechanics", "summary": "In-Space Servicing, Assembly, and Manufacturing (ISAM) is a set of emerging\noperations that provides several benefits to improve the longevity, capacity,\nmo- bility, and expandability of existing and future space assets. Serial\nrobotic ma- nipulators are particularly vital in accomplishing ISAM operations,\nhowever, the complex perturbation forces and motions associated with movement\nof a robotic arm on a free-flying satellite presents a complex controls problem\nrequiring addi- tional study. While many dynamical models are developed,\nexperimentally test- ing and validating these models is challenging given that\nthe models operate in space, where satellites have six-degrees-of-freedom\n(6-DOF). This paper attempts to resolve those challenges by presenting the\ndesign and development of a new hardware-in-the-loop (HIL) experimental testbed\nutilized to emulate ISAM. This emulation will be accomplished by means of a\n6-DOF UR3e robotic arm attached to a satellite bus. This satellite bus is\nmounted to a 1-DOF guide-rail system, en- abling the satellite bus and robotic\narm to move freely in one linear direction. This experimental ISAM emulation\nsystem will explore and validate models for space motion, serial robot\nmanipulation, and contact mechanics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u786c\u4ef6\u5728\u73af\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7528\u4e8e\u6a21\u62df\u548c\u9a8c\u8bc1\u592a\u7a7a\u670d\u52a1\u4e0e\u7ec4\u88c5\u64cd\u4f5c\uff0c\u4ee5\u5e94\u5bf9\u590d\u6742\u7684\u52a8\u6001\u6d4b\u8bd5\u6311\u6218\u3002", "motivation": "\u5e94\u5bf9\u590d\u6742\u7684\u63a7\u5236\u95ee\u9898\u548c\u57286\u81ea\u7531\u5ea6\u73af\u5883\u4e2d\u9a8c\u8bc1\u673a\u5668\u4eba\u6a21\u578b\u7684\u6311\u6218\uff0c\u4ee5\u63d0\u9ad8\u592a\u7a7a\u8d44\u4ea7\u7684\u4f7f\u7528\u5bff\u547d\u548c\u529f\u80fd\u3002", "method": "\u8bbe\u8ba1\u548c\u5f00\u53d1\u4e00\u4e2a\u786c\u4ef6\u5728\u73af\uff08HIL\uff09\u5b9e\u9a8c\u6d4b\u8bd5\u53f0\uff0c\u901a\u8fc7\u5c066\u81ea\u7531\u5ea6\u7684UR3e\u673a\u5668\u4eba\u81c2\u4e0e\u536b\u661f\u603b\u7ebf\u7ed3\u5408\uff0c\u6a21\u62dfISAM\u64cd\u4f5c\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u5b9e\u9a8cISAM\u4eff\u771f\u7cfb\u7edf\uff0c\u4ee5\u63a2\u7d22\u548c\u9a8c\u8bc1\u592a\u7a7a\u8fd0\u52a8\u3001\u4e32\u8054\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u63a5\u89e6\u529b\u5b66\u6a21\u578b\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u786c\u4ef6\u5728\u73af\u5b9e\u9a8c\u5e73\u53f0\uff0c\u7528\u4e8e\u6a21\u62df\u592a\u7a7a\u670d\u52a1\u3001\u7ec4\u88c5\u548c\u5236\u9020\uff08ISAM\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u5b9e\u9645\u592a\u7a7a\u4e2d\u9a8c\u8bc1\u7684\u6311\u6218\u3002"}}
{"id": "2510.13048", "categories": ["cs.RO", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.13048", "abs": "https://arxiv.org/abs/2510.13048", "authors": ["Minghao Guo", "Victor Zordan", "Sheldon Andrews", "Wojciech Matusik", "Maneesh Agrawala", "Hsueh-Ti Derek Liu"], "title": "Kinematic Kitbashing for Modeling Functional Articulated Objects", "comment": null, "summary": "We introduce Kinematic Kitbashing, an automatic framework that synthesizes\nfunctionality-aware articulated objects by reusing parts from existing models.\nGiven a kinematic graph with a small collection of articulated parts, our\noptimizer jointly solves for the spatial placement of every part so that (i)\nattachments remain geometrically sound over the entire range of motion and (ii)\nthe assembled object satisfies user-specified functional goals such as\ncollision-free actuation, reachability, or trajectory following. At its core is\na kinematics-aware attachment energy that aligns vector distance function\nfeatures sampled across multiple articulation snapshots. We embed this\nattachment term within an annealed Riemannian Langevin dynamics sampler that\ntreats functionality objectives as additional energies, enabling robust global\nexploration while accommodating non-differentiable functionality objectives and\nconstraints. Our framework produces a wide spectrum of assembled articulated\nshapes, from trash-can wheels grafted onto car bodies to multi-segment lamps,\ngear-driven paddlers, and reconfigurable furniture, and delivers strong\nquantitative improvements over state-of-the-art baselines across geometric,\nkinematic, and functional metrics. By tightly coupling articulation-aware\ngeometry matching with functionality-driven optimization, Kinematic Kitbashing\nbridges part-based shape modeling and functional assembly design, empowering\nrapid creation of interactive articulated assets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u6846\u67b6Kinematic Kitbashing\uff0c\u901a\u8fc7\u4f18\u5316\u90e8\u4ef6\u4f4d\u7f6e\uff0c\u5408\u6210\u529f\u80fd\u6027\u5173\u8282\u7269\u4f53\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u901a\u8fc7\u91cd\u7528\u73b0\u6709\u6a21\u578b\u7684\u90e8\u4ef6\uff0c\u5408\u6210\u5177\u6709\u529f\u80fd\u6027\u7684\u5173\u8282\u7269\u4f53", "method": "Kinematic Kitbashing\u6846\u67b6", "result": "\u751f\u6210\u5e7f\u6cdb\u7684\u7ec4\u88c5\u5173\u8282\u5f62\u72b6\uff0c\u5e76\u5728\u51e0\u4f55\u3001\u8fd0\u52a8\u5b66\u548c\u529f\u80fd\u6307\u6807\u4e0a\u9886\u5148\u4e8e\u73b0\u6709\u57fa\u51c6", "conclusion": "Kinematic Kitbashing\u6709\u6548\u7ed3\u5408\u4e86\u5173\u8282\u51e0\u4f55\u5339\u914d\u548c\u529f\u80fd\u9a71\u52a8\u4f18\u5316\uff0c\u652f\u6301\u5feb\u901f\u521b\u5efa\u4e92\u52a8\u5173\u8282\u8d44\u4ea7\u3002"}}
{"id": "2510.13054", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13054", "abs": "https://arxiv.org/abs/2510.13054", "authors": ["Ankit Goyal", "Hugo Hadfield", "Xuning Yang", "Valts Blukis", "Fabio Ramos"], "title": "VLA-0: Building State-of-the-Art VLAs with Zero Modification", "comment": null, "summary": "Vision-Language-Action models (VLAs) hold immense promise for enabling\ngeneralist robot manipulation. However, the best way to build them remains an\nopen question. Current approaches often add complexity, such as modifying the\nexisting vocabulary of a Vision-Language Model (VLM) with action tokens or\nintroducing special action heads. Curiously, the simplest strategy of\nrepresenting actions directly as text has remained largely unexplored. This\nwork introduces VLA-0 to investigate this idea. We find that VLA-0 is not only\neffective; it is surprisingly powerful. With the right design, VLA-0\noutperforms more involved models. On LIBERO, a popular benchmark for evaluating\nVLAs, VLA-0 outperforms all existing methods trained on the same robotic data,\nincluding $\\pi_0.5$-KI, OpenVLA-OFT and SmolVLA. Furthermore, without\nlarge-scale robotics-specific training, it outperforms methods trained on\nlarge-scale robotic data, like $\\pi_0.5$-KI, $\\pi_0$, GR00T-N1 and MolmoAct.\nThese findings also translate to the real world, where VLA-0 outperforms\nSmolVLA, a VLA model pre-trained on large-scale real data. This paper\nsummarizes our unexpected findings and spells out the specific techniques\nrequired to unlock the high performance of this simple yet potent VLA design.\nVisual results, code, and trained models are provided here:\nhttps://vla0.github.io/.", "AI": {"tldr": "VLA-0\u6a21\u578b\u901a\u8fc7\u5c06\u52a8\u4f5c\u76f4\u63a5\u8868\u793a\u4e3a\u6587\u672c\uff0c\u8d85\u8d8a\u4e86\u590d\u6742\u7684\u73b0\u6709\u6a21\u578b\uff0c\u663e\u793a\u51fa\u5f3a\u5927\u7684\u6548\u80fd\uff0c\u5c24\u5176\u662f\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u3002", "motivation": "\u63a2\u7d22\u5c06\u52a8\u4f5c\u76f4\u63a5\u8868\u793a\u4e3a\u6587\u672c\u7684\u7b56\u7565\uff0c\u68c0\u9a8c\u8fd9\u79cd\u7b80\u5316\u65b9\u6cd5\u5728VLA\u6a21\u578b\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "VLA-0\u6a21\u578b\u901a\u8fc7\u5c06\u52a8\u4f5c\u7b80\u5355\u5730\u8868\u8fbe\u4e3a\u6587\u672c\uff0c\u800c\u4e0d\u662f\u501f\u52a9\u590d\u6742\u7684\u8bcd\u6c47\u6216\u4e13\u95e8\u7684\u52a8\u4f5c\u5934\u8fdb\u884c\u6784\u5efa\u3002", "result": "\u672c\u6587\u4ecb\u7ecd\u7684VLA-0\u6a21\u578b\u5728\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c(VLA)\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u51fa\u8272\u7684\u6027\u80fd\u3002", "conclusion": "VLA-0\u7684\u8bbe\u8ba1\u7b80\u5355\u5374\u5f3a\u5927\uff0c\u80fd\u591f\u5728\u7f3a\u4e4f\u5927\u89c4\u6a21\u7279\u5b9a\u4e8e\u673a\u5668\u4eba\u7684\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u4f9d\u7136 outperform \u73b0\u6709\u7684\u591a\u79cd\u590d\u6742\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2510.13149", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13149", "abs": "https://arxiv.org/abs/2510.13149", "authors": ["Yangtao Chen", "Zixuan Chen", "Nga Teng Chan", "Junting Chen", "Junhui Yin", "Jieqi Shi", "Yang Gao", "Yong-Lu Li", "Jing Huo"], "title": "RoboHiMan: A Hierarchical Evaluation Paradigm for Compositional Generalization in Long-Horizon Manipulation", "comment": "Under review. These first two authors contributed equally to this\n  work", "summary": "Enabling robots to flexibly schedule and compose learned skills for novel\nlong-horizon manipulation under diverse perturbations remains a core challenge.\nEarly explorations with end-to-end VLA models show limited success, as these\nmodels struggle to generalize beyond the training distribution. Hierarchical\napproaches, where high-level planners generate subgoals for low-level policies,\nbring certain improvements but still suffer under complex perturbations,\nrevealing limited capability in skill composition. However, existing benchmarks\nprimarily emphasize task completion in long-horizon settings, offering little\ninsight into compositional generalization, robustness, and the interplay\nbetween planning and execution. To systematically investigate these gaps, we\npropose RoboHiMan, a hierarchical evaluation paradigm for compositional\ngeneralization in long-horizon manipulation. RoboHiMan introduces HiMan-Bench,\na benchmark of atomic and compositional tasks under diverse perturbations,\nsupported by a multi-level training dataset for analyzing progressive data\nscaling, and proposes three evaluation paradigms (vanilla, decoupled, coupled)\nthat probe the necessity of skill composition and reveal bottlenecks in\nhierarchical architectures. Experiments highlight clear capability gaps across\nrepresentative models and architectures, pointing to directions for advancing\nmodels better suited to real-world long-horizon manipulation tasks. Videos and\nopen-source code can be found on our project website:\nhttps://chenyt31.github.io/robo-himan.github.io/.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86RoboHiMan\uff0c\u4e00\u4e2a\u65b0\u7684\u5206\u5c42\u8bc4\u4f30\u6846\u67b6\uff0c\u5173\u6ce8\u957f\u671f\u64cd\u4f5c\u4e2d\u7684\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u6280\u80fd\u7ec4\u5408\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u6848\u3002", "motivation": "\u65e8\u5728\u586b\u8865\u5f53\u524d\u77ed\u7f3a\u7684\u7ec4\u5408\u6cdb\u5316\u3001\u9c81\u68d2\u6027\u4e0e\u8ba1\u5212\u6267\u884c\u4e4b\u95f4\u76f8\u4e92\u4f5c\u7528\u7684\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7HiMan-Bench\u57fa\u51c6\u6d4b\u8bd5\u548c\u591a\u7ea7\u8bad\u7ec3\u6570\u636e\u96c6\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5c55\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u5728\u590d\u6742\u6270\u52a8\u4e0b\u7684\u80fd\u529b\u5dee\u8ddd\uff0c\u63d0\u4f9b\u4e86\u9488\u5bf9\u957f\u671f\u64cd\u4f5c\u4efb\u52a1\u7684\u6539\u8fdb\u65b9\u5411\u3002", "conclusion": "\u63d0\u51fa\u4e86RoboHiMan\u8bc4\u4f30\u8303\u5f0f\uff0c\u5f3a\u8c03\u6280\u80fd\u7ec4\u5408\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63ed\u793a\u73b0\u6709\u6a21\u578b\u7684\u80fd\u529b\u7f3a\u53e3\u3002"}}
{"id": "2510.13284", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13284", "abs": "https://arxiv.org/abs/2510.13284", "authors": ["Haoyang Wu", "Siheng Wu", "William X. Liu", "Fangui Zeng"], "title": "ALOHA2 Robot Kitchen Application Scenario Reproduction Report", "comment": null, "summary": "ALOHA2 is an enhanced version of the dual-arm teleoperated robot ALOHA,\nfeaturing higher performance and robustness compared to the original design,\nwhile also being more ergonomic. Like ALOHA, ALOHA2 consists of two grippers\nand two ViperX 6-DoF arms, as well as two smaller WidowX arms. Users control\nthe follower mechanical arms by operating the leader mechanical arms through\nback-driving. The device also includes cameras that generate images from\nmultiple viewpoints, allowing for RGB data collection during teleoperation. The\nrobot is mounted on a 48-inch x 30-inch table, equipped with an aluminum frame\nthat provides additional mounting points for cameras and gravity compensation\nsystems.", "AI": {"tldr": "ALOHA2\u662f\u4e00\u4e2a\u6539\u8fdb\u7248\u53cc\u81c2\u9065\u63a7\u673a\u5668\u4eba\uff0c\u63d0\u4f9b\u66f4\u9ad8\u6027\u80fd\u548c\u66f4\u597d\u7684\u4eba\u4f53\u5de5\u5b66\u8bbe\u8ba1\uff0c\u80fd\u591f\u8fdb\u884c\u591a\u89c6\u89d2\u56fe\u50cf\u91c7\u96c6\u3002", "motivation": "\u63d0\u9ad8\u673a\u68b0\u81c2\u7684\u6027\u80fd\u3001\u9c81\u68d2\u6027\u548c\u4eba\u4f53\u5de5\u7a0b\u5b66\u8bbe\u8ba1", "method": "\u4f7f\u7528\u53cc\u81c2\u9065\u63a7\u673a\u5668\u4eba", "result": "ALOHA2\u5728\u8bbe\u8ba1\u4e0a\u4f18\u4e8e\u539f\u59cbALOHA\uff0c\u7528\u6237\u80fd\u591f\u901a\u8fc7\u64cd\u4f5c\u4e3b\u673a\u68b0\u81c2\u6765\u63a7\u5236\u4ece\u673a\u68b0\u81c2\uff0c\u5e76\u5b9e\u73b0\u591a\u89c6\u89d2\u7684RGB\u6570\u636e\u91c7\u96c6\u3002", "conclusion": "ALOHA2\u7684\u8bbe\u8ba1\u63d0\u5347\u4e86\u9065\u64cd\u4f5c\u7684\u6548\u7387\u548c\u7528\u6237\u4f53\u9a8c\uff0c\u6709\u52a9\u4e8e\u672a\u6765\u7684\u673a\u5668\u4eba\u5f00\u53d1\u3002"}}
{"id": "2510.13287", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13287", "abs": "https://arxiv.org/abs/2510.13287", "authors": ["Nishant Chandna", "Akshat Kaushal"], "title": "DAMM-LOAM: Degeneracy Aware Multi-Metric LiDAR Odometry and Mapping", "comment": "Accepted at IROS Active Perception Workshop", "summary": "LiDAR Simultaneous Localization and Mapping (SLAM) systems are essential for\nenabling precise navigation and environmental reconstruction across various\napplications. Although current point-to-plane ICP algorithms perform effec-\ntively in structured, feature-rich environments, they struggle in scenarios\nwith sparse features, repetitive geometric structures, and high-frequency\nmotion. This leads to degeneracy in 6- DOF pose estimation. Most\nstate-of-the-art algorithms address these challenges by incorporating\nadditional sensing modalities, but LiDAR-only solutions continue to face\nlimitations under such conditions. To address these issues, we propose a novel\nDegeneracy-Aware Multi-Metric LiDAR Odometry and Map- ping (DAMM-LOAM) module.\nOur system improves mapping accuracy through point cloud classification based\non surface normals and neighborhood analysis. Points are classified into\nground, walls, roof, edges, and non-planar points, enabling accurate\ncorrespondences. A Degeneracy-based weighted least squares-based ICP algorithm\nis then applied for accurate odom- etry estimation. Additionally, a Scan\nContext based back-end is implemented to support robust loop closures.\nDAMM-LOAM demonstrates significant improvements in odometry accuracy,\nespecially in indoor environments such as long corridors", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684DAMM-LOAM\u6a21\u5757\uff0c\u901a\u8fc7\u70b9\u4e91\u5206\u7c7b\u548c\u52a0\u6743ICP\u7b97\u6cd5\u6539\u5584LiDAR SLAM\u5728\u7279\u5f81\u7a00\u758f\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u76ee\u524dLiDAR SLAM\u7cfb\u7edf\u5728\u7a00\u758f\u7279\u5f81\u548c\u590d\u6742\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u91cc\u7a0b\u8ba1\u548c\u5730\u56fe\u751f\u6210\u7684\u7cbe\u5ea6\u3002", "method": "\u901a\u8fc7\u8868\u9762\u6cd5\u7ebf\u548c\u90bb\u57df\u5206\u6790\u5bf9\u70b9\u4e91\u8fdb\u884c\u5206\u7c7b\uff0c\u7136\u540e\u5e94\u7528\u57fa\u4e8e\u9000\u5316\u7684\u52a0\u6743\u6700\u5c0f\u4e8c\u4e58ICP\u7b97\u6cd5\u8fdb\u884c\u91cc\u7a0b\u8ba1\u4f30\u8ba1\uff0c\u5e76\u5b9e\u65bd\u57fa\u4e8e\u626b\u63cf\u4e0a\u4e0b\u6587\u7684\u540e\u7aef\u652f\u6301\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u5ea6\u91cf\u6fc0\u5149\u96f7\u8fbe\u91cc\u7a0b\u8ba1\u548c\u5730\u56fe\u751f\u6210\u6a21\u5757DAMM-LOAM\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524dLiDAR SLAM\u7cfb\u7edf\u5728\u7279\u5f81\u7a00\u758f\u3001\u91cd\u590d\u51e0\u4f55\u7ed3\u6784\u548c\u9ad8\u9891\u8fd0\u52a8\u60c5\u51b5\u4e0b\u7684\u59ff\u6001\u4f30\u8ba1\u9000\u5316\u95ee\u9898\u3002\u901a\u8fc7\u5bf9\u70b9\u4e91\u8fdb\u884c\u5206\u7c7b\u548c\u57fa\u4e8e\u9000\u5316\u7684\u52a0\u6743\u6700\u5c0f\u4e8c\u4e58ICP\u7b97\u6cd5\uff0c\u8be5\u7cfb\u7edf\u663e\u8457\u63d0\u9ad8\u4e86\u91cc\u7a0b\u8ba1\u7cbe\u5ea6\uff0c\u5c24\u5176\u662f\u5728\u5ba4\u5185\u957f\u8d70\u5eca\u73af\u5883\u4e2d\u3002", "conclusion": "DAMM-LOAM\u7cfb\u7edf\u5728\u5ba4\u5185\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u91cc\u7a0b\u8ba1\u7cbe\u5ea6\u63d0\u5347\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u957f\u8d70\u5eca\u7b49\u573a\u666f\u3002"}}
{"id": "2510.13324", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13324", "abs": "https://arxiv.org/abs/2510.13324", "authors": ["Erik Helmut", "Niklas Funk", "Tim Schneider", "Cristiana de Farias", "Jan Peters"], "title": "Tactile-Conditioned Diffusion Policy for Force-Aware Robotic Manipulation", "comment": null, "summary": "Contact-rich manipulation depends on applying the correct grasp forces\nthroughout the manipulation task, especially when handling fragile or\ndeformable objects. Most existing imitation learning approaches often treat\nvisuotactile feedback only as an additional observation, leaving applied forces\nas an uncontrolled consequence of gripper commands. In this work, we present\nForce-Aware Robotic Manipulation (FARM), an imitation learning framework that\nintegrates high-dimensional tactile data to infer tactile-conditioned force\nsignals, which in turn define a matching force-based action space. We collect\nhuman demonstrations using a modified version of the handheld Universal\nManipulation Interface (UMI) gripper that integrates a GelSight Mini visual\ntactile sensor. For deploying the learned policies, we developed an actuated\nvariant of the UMI gripper with geometry matching our handheld version. During\npolicy rollouts, the proposed FARM diffusion policy jointly predicts robot\npose, grip width, and grip force. FARM outperforms several baselines across\nthree tasks with distinct force requirements -- high-force, low-force, and\ndynamic force adaptation -- demonstrating the advantages of its two key\ncomponents: leveraging force-grounded, high-dimensional tactile observations\nand a force-based control space. The codebase and design files are open-sourced\nand available at https://tactile-farm.github.io .", "AI": {"tldr": "FARM\u662f\u4e00\u79cd\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u7ef4\u89e6\u89c9\u6570\u636e\u5b9e\u73b0\u529b\u4fe1\u53f7\u63a8\u65ad\uff0c\u4ece\u800c\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u64cd\u63a7\u8fc7\u7a0b\u4e2d\u5bf9\u529b\u7684\u63a7\u5236\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\u65f6\u3002", "motivation": "\u7814\u7a76\u8054\u7cfb\u4e30\u5bcc\u7684\u64cd\u63a7\u6280\u672f\uff0c\u5728\u5904\u7406\u8106\u5f31\u6216\u53ef\u53d8\u5f62\u7269\u4f53\u65f6\uff0c\u5982\u4f55\u901a\u8fc7\u6b63\u786e\u7684\u6293\u53d6\u529b\u8fdb\u884c\u64cd\u63a7\u3002", "method": "\u91c7\u7528\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u9ad8\u7ef4\u89e6\u89c9\u6570\u636e\u63a8\u65ad\u89e6\u89c9\u6761\u4ef6\u7684\u529b\u4fe1\u53f7\uff0c\u4ece\u800c\u5b9a\u4e49\u529b\u57fa\u7840\u52a8\u4f5c\u7a7a\u95f4\u3002", "result": "FARM\u5728\u5e94\u5bf9\u9ad8\u529b\u3001\u4f4e\u529b\u548c\u52a8\u6001\u529b\u9002\u5e94\u7684\u4e09\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8bc1\u660e\u4e86\u9ad8\u7ef4\u89e6\u89c9\u89c2\u5bdf\u4ee5\u53ca\u57fa\u4e8e\u529b\u7684\u63a7\u5236\u7a7a\u95f4\u7684\u91cd\u8981\u6027\u3002", "conclusion": "FARM\u6846\u67b6\u5c55\u793a\u4e86\u5728\u5904\u7406\u4e0d\u540c\u529b\u8981\u6c42\u7684\u4efb\u52a1\u4e2d\uff0c\u7ed3\u5408\u89e6\u89c9\u53cd\u9988\u548c\u57fa\u4e8e\u529b\u7684\u63a7\u5236\u7a7a\u95f4\u7684\u4f18\u52bf\uff0c\u8d85\u8d8a\u4e86\u591a\u79cd\u57fa\u51c6\u7b97\u6cd5\u3002"}}
{"id": "2510.13356", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13356", "abs": "https://arxiv.org/abs/2510.13356", "authors": ["Jie Gu", "Tin Lun Lam", "Chunxu Tian", "Zhihao Xia", "Yongheng Xing", "Dan Zhang"], "title": "MODUR: A Modular Dual-reconfigurable Robot", "comment": null, "summary": "Modular Self-Reconfigurable Robot (MSRR) systems are a class of robots\ncapable of forming higher-level robotic systems by altering the topological\nrelationships between modules, offering enhanced adaptability and robustness in\nvarious environments. This paper presents a novel MSRR called MODUR, featuring\ndual-level reconfiguration capabilities designed to integrate reconfigurable\nmechanisms into MSRR. Specifically, MODUR can perform high-level\nself-reconfiguration among modules to create different configurations, while\neach module is also able to change its shape to execute basic motions. The\ndesign of MODUR primarily includes a compact connector and scissor linkage\ngroups that provide actuation, forming a parallel mechanism capable of\nachieving both connector motion decoupling and adjacent position migration\ncapabilities. Furthermore, the workspace, considering the interdependent\nconnectors, is comprehensively analyzed, laying a theoretical foundation for\nthe design of the module's basic motion. Finally, the motion of MODUR is\nvalidated through a series of experiments.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u5757\u5316\u81ea\u6211\u91cd\u914d\u7f6e\u673a\u5668\u4ebaMODUR\uff0c\u5177\u5907\u53cc\u5c42\u91cd\u914d\u7f6e\u80fd\u529b\uff0c\u80fd\u5728\u591a\u4e2a\u6a21\u5757\u95f4\u8fdb\u884c\u9ad8\u5c42\u6b21\u81ea\u6211\u91cd\u914d\u7f6e\uff0c\u540c\u65f6\u6a21\u5757\u53ef\u6539\u53d8\u5f62\u72b6\u6267\u884c\u57fa\u672c\u52a8\u4f5c\u3002", "motivation": "MSRR\u7cfb\u7edf\u901a\u8fc7\u6539\u53d8\u6a21\u5757\u4e4b\u95f4\u7684\u62d3\u6251\u5173\u7cfb\uff0c\u63d0\u4f9b\u66f4\u9ad8\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\uff0c\u56e0\u6b64\u5f00\u53d1\u4e00\u79cd\u65b0\u578bMSRR\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7d27\u51d1\u7684\u8fde\u63a5\u5668\u548c\u526a\u5200\u8fde\u6746\u7ec4\uff0c\u5f62\u6210\u5e76\u884c\u673a\u5236\uff0c\u5b9e\u73b0\u8fde\u63a5\u5668\u8fd0\u52a8\u89e3\u8026\u548c\u76f8\u90bb\u4f4d\u7f6e\u8fc1\u79fb\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u5bf9\u8fde\u63a5\u5668\u7684\u76f8\u4e92\u4f9d\u8d56\u8fdb\u884c\u5168\u9762\u5206\u6790\uff0c\u5efa\u7acb\u4e86\u57fa\u7840\u8fd0\u52a8\u8bbe\u8ba1\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86MODUR\u7684\u8fd0\u52a8\u80fd\u529b\u3002", "conclusion": "MODUR\u7684\u8fd0\u52a8\u901a\u8fc7\u4e00\u7cfb\u5217\u5b9e\u9a8c\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u6709\u6548\u5c55\u793a\u4e86\u5176\u8bbe\u8ba1\u7684\u53ef\u884c\u6027\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2510.13358", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13358", "abs": "https://arxiv.org/abs/2510.13358", "authors": ["Shingo Ayabe", "Hiroshi Kera", "Kazuhiko Kawamoto"], "title": "Adversarial Fine-tuning in Offline-to-Online Reinforcement Learning for Robust Robot Control", "comment": "16 pages, 8 figures", "summary": "Offline reinforcement learning enables sample-efficient policy acquisition\nwithout risky online interaction, yet policies trained on static datasets\nremain brittle under action-space perturbations such as actuator faults. This\nstudy introduces an offline-to-online framework that trains policies on clean\ndata and then performs adversarial fine-tuning, where perturbations are\ninjected into executed actions to induce compensatory behavior and improve\nresilience. A performance-aware curriculum further adjusts the perturbation\nprobability during training via an exponential-moving-average signal, balancing\nrobustness and stability throughout the learning process. Experiments on\ncontinuous-control locomotion tasks demonstrate that the proposed method\nconsistently improves robustness over offline-only baselines and converges\nfaster than training from scratch. Matching the fine-tuning and evaluation\nconditions yields the strongest robustness to action-space perturbations, while\nthe adaptive curriculum strategy mitigates the degradation of nominal\nperformance observed with the linear curriculum strategy. Overall, the results\nshow that adversarial fine-tuning enables adaptive and robust control under\nuncertain environments, bridging the gap between offline efficiency and online\nadaptability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79bb\u7ebf\u5230\u5728\u7ebf\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u5fae\u8c03\u63d0\u9ad8\u7b56\u7565\u7684\u9c81\u68d2\u6027\uff0c\u4ece\u800c\u65e0\u9700\u98ce\u9669\u7684\u5728\u7ebf\u4ea4\u4e92\u3002", "motivation": "\u89e3\u51b3\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u57fa\u4e8e\u9759\u6001\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u653f\u7b56\u5728\u52a8\u4f5c\u7a7a\u95f4\u6270\u52a8\u4e0b\u8106\u5f31\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6e05\u6d01\u6570\u636e\u8bad\u7ec3\u653f\u7b56\uff0c\u7136\u540e\u901a\u8fc7\u5bf9\u6297\u6027\u5fae\u8c03\u6ce8\u5165\u5e72\u6270\uff0c\u4ee5\u8bf1\u53d1\u8865\u507f\u884c\u4e3a\u5e76\u63d0\u9ad8\u9c81\u68d2\u6027\uff0c\u5e76\u91c7\u7528\u6027\u80fd\u611f\u77e5\u8bfe\u7a0b\u8c03\u6574\u5e72\u6270\u6982\u7387\u3002", "result": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5c06\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e0e\u5728\u7ebf\u9002\u5e94\u76f8\u7ed3\u5408\u7684\u6846\u67b6\uff0c\u589e\u5f3a\u4e86\u7b56\u7565\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u81ea\u9002\u5e94\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u5bf9\u6297\u6027\u5fae\u8c03\u80fd\u591f\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e0b\u5b9e\u73b0\u81ea\u9002\u5e94\u548c\u9c81\u68d2\u63a7\u5236\uff0c\u6210\u529f\u5f25\u5408\u4e86\u79bb\u7ebf\u6548\u7387\u4e0e\u5728\u7ebf\u9002\u5e94\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2510.13443", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13443", "abs": "https://arxiv.org/abs/2510.13443", "authors": ["Mojtaba Mollahossein", "Gholamreza Vossoughi", "Mohammad Hossein Rohban"], "title": "Real-Time Knee Angle Prediction Using EMG and Kinematic Data with an Attention-Based CNN-LSTM Network and Transfer Learning Across Multiple Datasets", "comment": null, "summary": "Electromyography (EMG) signals are widely used for predicting body joint\nangles through machine learning (ML) and deep learning (DL) methods. However,\nthese approaches often face challenges such as limited real-time applicability,\nnon-representative test conditions, and the need for large datasets to achieve\noptimal performance. This paper presents a transfer-learning framework for knee\njoint angle prediction that requires only a few gait cycles from new subjects.\nThree datasets - Georgia Tech, the University of California Irvine (UCI), and\nthe Sharif Mechatronic Lab Exoskeleton (SMLE) - containing four EMG channels\nrelevant to knee motion were utilized. A lightweight attention-based CNN-LSTM\nmodel was developed and pre-trained on the Georgia Tech dataset, then\ntransferred to the UCI and SMLE datasets. The proposed model achieved\nNormalized Mean Absolute Errors (NMAE) of 6.8 percent and 13.7 percent for\none-step and 50-step predictions on abnormal subjects using EMG inputs alone.\nIncorporating historical knee angles reduced the NMAE to 3.1 percent and 3.5\npercent for normal subjects, and to 2.8 percent and 7.5 percent for abnormal\nsubjects. When further adapted to the SMLE exoskeleton with EMG, kinematic, and\ninteraction force inputs, the model achieved 1.09 percent and 3.1 percent NMAE\nfor one- and 50-step predictions, respectively. These results demonstrate\nrobust performance and strong generalization for both short- and long-term\nrehabilitation scenarios.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\uff0c\u4ec5\u9700\u5c11\u91cf\u6b65\u6001\u5468\u671f\u5373\u53ef\u9884\u6d4b\u819d\u5173\u8282\u89d2\u5ea6\uff0c\u5e76\u5c55\u793a\u4e86\u826f\u597d\u7684\u901a\u7528\u6027\u4e0e\u9c81\u68d2\u6027\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u73b0\u6709EMG\u4fe1\u53f7\u9884\u6d4b\u65b9\u6cd5\u4e2d\u5b58\u5728\u7684\u5b9e\u65f6\u6027\u4e0d\u8db3\u3001\u6d4b\u8bd5\u6761\u4ef6\u4e0d\u5177\u4ee3\u8868\u6027\u53ca\u6570\u636e\u96c6\u9700\u6c42\u91cf\u5927\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u6709\u6548\u7684\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u7acb\u6ce8\u610f\u529b\u673a\u5236\u7684CNN-LSTM\u6a21\u578b\uff0c\u5229\u7528EMG\u4fe1\u53f7\u8fdb\u884c\u819d\u5173\u8282\u89d2\u5ea6\u9884\u6d4b\uff0c\u5e76\u7ed3\u5408\u4e0d\u540c\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u819d\u5173\u8282\u89d2\u5ea6\u9884\u6d4b\u7684\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\uff0c\u4ec5\u9700\u5c11\u91cf\u6b65\u6001\u5468\u671f\u4fbf\u53ef\u9002\u5e94\u65b0\u53d7\u8bd5\u8005\u3002\u91c7\u7528Georgia Tech\u3001\u52a0\u5dde\u5927\u5b66\u6b27\u6587\u5206\u6821(UCI)\u548cSharif\u673a\u7535\u5b9e\u9a8c\u5ba4\u5916\u9aa8\u9abc(SMLE)\u7684\u7535\u808c\u56fe(EMG)\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6ce8\u610f\u529b\u57faCNN-LSTM\u6a21\u578b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u5f02\u5e38\u53d7\u8bd5\u8005\u7684\u5355\u6b65\u548c50\u6b65\u9884\u6d4b\u4e2d\uff0cNMAE\u5206\u522b\u4e3a6.8%\u548c13.7%\u3002\u52a0\u5165\u5386\u53f2\u819d\u5173\u8282\u89d2\u5ea6\u540e\uff0c\u6b63\u5e38\u53d7\u8bd5\u8005\u7684NMAE\u964d\u4f4e\u81f33.1%\u548c3.5%\uff0c\u5f02\u5e38\u53d7\u8bd5\u8005\u964d\u4f4e\u81f32.8%\u548c7.5%\u3002\u5728SMLE\u5916\u9aa8\u9abc\u7684\u8fdb\u4e00\u6b65\u9002\u5e94\u4e2d\uff0c\u6a21\u578b\u5206\u522b\u5b9e\u73b0\u4e861.09%\u548c3.1%\u7684NMAE\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u5728\u4e0d\u540c\u53d7\u8bd5\u8005\u548c\u60c5\u5883\u4e0b\u8fdb\u884c\u819d\u5173\u8282\u8fd0\u52a8\u9884\u6d4b\u5177\u6709\u6709\u6548\u6027\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5eb7\u590d\u573a\u666f\u3002"}}
{"id": "2510.13488", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13488", "abs": "https://arxiv.org/abs/2510.13488", "authors": ["Maximilian Stasica", "Arne Bick", "Nico Bohlinger", "Omid Mohseni", "Max Johannes Alois Fritzsche", "Clemens H\u00fcbler", "Jan Peters", "Andr\u00e9 Seyfarth"], "title": "Bridge the Gap: Enhancing Quadruped Locomotion with Vertical Ground Perturbations", "comment": null, "summary": "Legged robots, particularly quadrupeds, excel at navigating rough terrains,\nyet their performance under vertical ground perturbations, such as those from\noscillating surfaces, remains underexplored. This study introduces a novel\napproach to enhance quadruped locomotion robustness by training the Unitree Go2\nrobot on an oscillating bridge - a 13.24-meter steel-and-concrete structure\nwith a 2.0 Hz eigenfrequency designed to perturb locomotion. Using\nReinforcement Learning (RL) with the Proximal Policy Optimization (PPO)\nalgorithm in a MuJoCo simulation, we trained 15 distinct locomotion policies,\ncombining five gaits (trot, pace, bound, free, default) with three training\nconditions: rigid bridge and two oscillating bridge setups with differing\nheight regulation strategies (relative to bridge surface or ground). Domain\nrandomization ensured zero-shot transfer to the real-world bridge. Our results\ndemonstrate that policies trained on the oscillating bridge exhibit superior\nstability and adaptability compared to those trained on rigid surfaces. Our\nframework enables robust gait patterns even without prior bridge exposure.\nThese findings highlight the potential of simulation-based RL to improve\nquadruped locomotion during dynamic ground perturbations, offering insights for\ndesigning robots capable of traversing vibrating environments.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5728\u632f\u8361\u6865\u9762\u4e0a\u7684\u8fd0\u52a8\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u7a33\u5b9a\u6027\u548c\u9002\u5e94\u6027\uff0c\u5c55\u793a\u4e86\u57fa\u4e8e\u6a21\u62df\u7684\u5f3a\u5316\u5b66\u4e60\u5728\u52a8\u6001\u6270\u52a8\u73af\u5883\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u63a2\u7d22\u56db\u8db3\u673a\u5668\u4eba\u5728\u5782\u76f4\u5730\u9762\u6270\u52a8\u4e0b\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u901a\u8fc7\u5728\u632f\u8361\u6865\u4e0a\u7684\u8bad\u7ec3\u6765\u589e\u5f3a\u5176\u884c\u8d70\u7684\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u548c\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\uff08PPO\uff09\u5728MuJoCo\u6a21\u62df\u73af\u5883\u4e2d\uff0c\u9488\u5bf915\u79cd\u4e0d\u540c\u7684\u8fd0\u52a8\u7b56\u7565\u8fdb\u884c\u8bad\u7ec3\uff0c\u7ed3\u5408\u4e94\u79cd\u6b65\u6001\u548c\u4e09\u79cd\u8bad\u7ec3\u6761\u4ef6\u3002", "result": "\u5728\u632f\u8361\u6865\u4e0a\u8bad\u7ec3\u7684\u8fd0\u52a8\u7b56\u7565\u5728\u7a33\u5b9a\u6027\u548c\u9002\u5e94\u6027\u4e0a\u8868\u73b0\u4f18\u4e8e\u5728\u521a\u6027\u8868\u9762\u4e0a\u8bad\u7ec3\u7684\u7b56\u7565\uff0c\u5e76\u4e14\u8fd9\u79cd\u6846\u67b6\u4f7f\u5f97\u673a\u5668\u4eba\u80fd\u591f\u5728\u52a8\u6001\u5730\u9762\u6270\u52a8\u4e0b\u8fdb\u884c\u8fd0\u52a8\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u632f\u8361\u6865\u4e0a\u8bad\u7ec3\u7684\u56db\u8db3\u673a\u5668\u4eba\u6bd4\u5728\u521a\u6027\u8868\u9762\u4e0a\u8bad\u7ec3\u7684\u8868\u73b0\u66f4\u4e3a\u4f18\u79c0\uff0c\u5177\u6709\u66f4\u597d\u7684\u7a33\u5b9a\u6027\u548c\u9002\u5e94\u6027\u3002\u8fd9\u4e00\u6846\u67b6\u4f7f\u673a\u5668\u4eba\u5728\u6ca1\u6709\u5148\u524d\u6865\u9762\u66b4\u9732\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u5f62\u6210\u5f3a\u5927\u7684\u6b65\u6001\u6a21\u5f0f\u3002"}}
{"id": "2510.13535", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13535", "abs": "https://arxiv.org/abs/2510.13535", "authors": ["Wentao Guo", "Yizhou Wang", "Wenzeng Zhang"], "title": "A Novel Robot Hand with Hoeckens Linkages and Soft Phalanges for Scooping and Self-Adaptive Grasping in Environmental Constraints", "comment": "Accepted by IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025, Hangzhou. This version includes updated contact\n  information", "summary": "This paper presents a novel underactuated adaptive robotic hand, Hockens-A\nHand, which integrates the Hoeckens mechanism, a double-parallelogram linkage,\nand a specialized four-bar linkage to achieve three adaptive grasping modes:\nparallel pinching, asymmetric scooping, and enveloping grasping. Hockens-A Hand\nrequires only a single linear actuator, leveraging passive mechanical\nintelligence to ensure adaptability and compliance in unstructured\nenvironments. Specifically, the vertical motion of the Hoeckens mechanism\nintroduces compliance, the double-parallelogram linkage ensures line contact at\nthe fingertip, and the four-bar amplification system enables natural\ntransitions between different grasping modes. Additionally, the inclusion of a\nmesh-textured silicone phalanx further enhances the ability to envelop objects\nof various shapes and sizes. This study employs detailed kinematic analysis to\noptimize the push angle and design the linkage lengths for optimal performance.\nSimulations validated the design by analyzing the fingertip motion and ensuring\nsmooth transitions between grasping modes. Furthermore, the grasping force was\nanalyzed using power equations to enhance the understanding of the system's\nperformance.Experimental validation using a 3D-printed prototype demonstrates\nthe three grasping modes of the hand in various scenarios under environmental\nconstraints, verifying its grasping stability and broad applicability.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u6b20\u9a71\u52a8\u81ea\u9002\u5e94\u673a\u5668\u4eba\u624b\uff0c\u91c7\u7528\u591a\u79cd\u673a\u68b0\u7ed3\u6784\uff0c\u5b9e\u73b0\u591a\u79cd\u6293\u53d6\u6a21\u5f0f\uff0c\u4e14\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u5e94\u6027\u548c\u6293\u53d6\u7a33\u5b9a\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u673a\u5668\u4eba\u624b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u6293\u53d6\u80fd\u529b\u548c\u9002\u5e94\u6027\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u578b\u673a\u5668\u4eba\u624b\uff0c\u80fd\u591f\u7075\u6d3b\u5e94\u5bf9\u4e0d\u540c\u5f62\u72b6\u548c\u5927\u5c0f\u7684\u7269\u4f53\u3002", "method": "\u91c7\u7528\u8be6\u7ec6\u7684\u8fd0\u52a8\u5b66\u5206\u6790\uff0c\u4f18\u5316\u63a8\u529b\u89d2\u5ea6\u548c\u8bbe\u8ba1\u8fde\u6746\u957f\u5ea6\uff1b\u901a\u8fc7\u6a21\u62df\u9a8c\u8bc1\u8bbe\u8ba1\uff0c\u5206\u6790\u6307\u5c16\u8fd0\u52a8\uff0c\u5e76\u786e\u4fdd\u4e0d\u540c\u6293\u53d6\u6a21\u5f0f\u4e4b\u95f4\u7684\u5e73\u6ed1\u8fc7\u6e21\uff1b\u4f7f\u7528\u529f\u7387\u65b9\u7a0b\u5206\u6790\u6293\u53d6\u529b\uff0c\u4ee5\u589e\u5f3a\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u7406\u89e3\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6b20\u9a71\u52a8\u81ea\u9002\u5e94\u673a\u5668\u4eba\u624b\u2014\u2014Hockens-A Hand\uff0c\u7ed3\u5408\u4e86Hoeckens\u673a\u6784\u3001\u53cc\u5e73\u884c\u56db\u8fb9\u5f62\u8fde\u6746\u548c\u4e13\u95e8\u7684\u56db\u8fde\u6746\u7ed3\u6784\uff0c\u5b9e\u73b0\u4e86\u4e09\u79cd\u81ea\u9002\u5e94\u6293\u53d6\u6a21\u5f0f\uff1a\u5e73\u884c\u5939\u6301\u3001\u975e\u5bf9\u79f0\u94f2\u53d6\u548c\u5305\u88f9\u6293\u53d6\u3002\u8be5\u624b\u53ea\u9700\u4e00\u4e2a\u7ebf\u6027\u6267\u884c\u5668\uff0c\u5229\u7528\u88ab\u52a8\u673a\u68b0\u667a\u80fd\u786e\u4fdd\u4e86\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u987a\u5e94\u6027\u3002", "conclusion": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cHockens-A Hand\u5728\u4e0d\u540c\u7684\u73af\u5883\u7ea6\u675f\u4e0b\uff0c\u80fd\u591f\u7a33\u5b9a\u5730\u5b9e\u73b0\u4e09\u79cd\u6293\u53d6\u6a21\u5f0f\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2510.13553", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13553", "abs": "https://arxiv.org/abs/2510.13553", "authors": ["Wentao Guo", "Wenzeng Zhang"], "title": "Hoecken-D Hand: A Novel Robotic Hand for Linear Parallel Pinching and Self-Adaptive Grasping", "comment": "Accepted by IEEE International Conference on Robotics and Biomimetics\n  (IROS) 2025, Hangzhou, China. This version includes updated contact\n  information", "summary": "This paper presents the Hoecken-D Hand, an underactuated robotic gripper that\ncombines a modified Hoecken linkage with a differential spring mechanism to\nachieve both linear parallel pinching and a mid-stroke transition to adaptive\nenvelope. The original Hoecken linkage is reconfigured by replacing one member\nwith differential links, preserving straight-line guidance while enabling\ncontact-triggered reconfiguration without additional actuators. A\ndouble-parallelogram arrangement maintains fingertip parallelism during\nconventional pinching, whereas the differential mechanism allows one finger to\nwrap inward upon encountering an obstacle, improving stability on irregular or\nthin objects. The mechanism can be driven by a single linear actuator,\nminimizing complexity and cost; in our prototype, each finger is driven by its\nown linear actuator for simplicity. We perform kinematic modeling and force\nanalysis to characterize grasp performance, including simulated grasping forces\nand spring-opening behavior under varying geometric parameters. The design was\nprototyped using PLA-based 3D printing, achieving a linear pinching span of\napproximately 200 mm. Preliminary tests demonstrate reliable grasping in both\nmodes across a wide range of object geometries, highlighting the Hoecken-D Hand\nas a compact, adaptable, and cost-effective solution for manipulation in\nunstructured environments.", "AI": {"tldr": "\u672c\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u6b3e\u540d\u4e3aHoecken-D Hand\u7684\u4e0b\u9a71\u52a8\u673a\u5668\u4eba\u6293\u53d6\u5668\uff0c\u7ed3\u5408\u4e86\u6539\u8fdb\u7684Hoecken\u8fde\u6746\u548c\u5dee\u52a8\u5f39\u7c27\u673a\u5236\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u7269\u4f53\u5f62\u72b6\u4e0b\u5b9e\u73b0\u6709\u6548\u7684\u6293\u53d6\uff0c\u9002\u7528\u4e8e\u975e\u7ed3\u6784\u5316\u73af\u5883\u3002", "motivation": "\u7814\u7a76\u76ee\u7684\u662f\u5f00\u53d1\u4e00\u79cd\u9002\u5e94\u6027\u5f3a\u3001\u6210\u672c\u4f4e\u3001\u80fd\u591f\u6293\u53d6\u591a\u79cd\u5f62\u72b6\u7269\u4f53\u7684\u673a\u5668\u4eba\u6293\u53d6\u5668\uff0c\u4ee5\u5e94\u5bf9\u975e\u7ed3\u6784\u5316\u73af\u5883\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u4fee\u6539\u7684Hoecken\u8fde\u6746\u4e0e\u5dee\u52a8\u5f39\u7c27\u673a\u5236\u76f8\u7ed3\u5408\uff0c\u8bbe\u8ba1\u51fa\u4e00\u79cd\u5728\u4e24\u79cd\u6293\u53d6\u6a21\u5f0f\u4e0b\u5747\u8868\u73b0\u51fa\u8272\u7684\u4e0b\u9a71\u52a8\u673a\u5668\u4eba\u6293\u53d6\u5668\u3002", "result": "\u539f\u578b\u5c55\u793a\u4e86\u53ef\u9760\u7684\u6293\u53d6\u6027\u80fd\uff0c\u5728\u591a\u79cd\u7269\u4f53\u5f62\u72b6\u4e0b\u5747\u8868\u73b0\u51fa\u4f18\u52bf\uff0c\u5177\u6709200 mm\u7684\u7ebf\u6027\u5939\u6301\u8303\u56f4\u3002", "conclusion": "Hoecken-D Hand\u662f\u4e00\u79cd\u7d27\u51d1\u3001\u9002\u5e94\u6027\u5f3a\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u673a\u5668\u4eba\u6293\u53d6\u5668\uff0c\u80fd\u591f\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u6709\u6548\u64cd\u63a7\u591a\u79cd\u5f62\u72b6\u7684\u7269\u4f53\u3002"}}
{"id": "2510.13594", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13594", "abs": "https://arxiv.org/abs/2510.13594", "authors": ["Austin Barret", "Meng Cheng Lau"], "title": "Development of an Intuitive GUI for Non-Expert Teleoperation of Humanoid Robots", "comment": "9 Figure. Presented at FIRA Summit 2025, Daegu, S. Korea", "summary": "The operation of humanoid robotics is an essential field of research with\nmany practical and competitive applications. Many of these systems, however, do\nnot invest heavily in developing a non-expert-centered graphical user interface\n(GUI) for operation. The focus of this research is to develop a scalable GUI\nthat is tailored to be simple and intuitive so non-expert operators can control\nthe robot through a FIRA-regulated obstacle course. Using common practices from\nuser interface development (UI) and understanding concepts described in\nhuman-robot interaction (HRI) and other related concepts, we will develop a new\ninterface with the goal of a non-expert teleoperation system.", "AI": {"tldr": "\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u7b80\u5355\u76f4\u89c2\u7684\u56fe\u5f62\u7528\u6237\u754c\u9762\uff0c\u4fbf\u4e8e\u975e\u4e13\u5bb6\u64cd\u4f5c\u4eba\u5458\u901a\u8fc7FIRA\u89c4\u5b9a\u7684\u969c\u788d\u8bfe\u7a0b\u63a7\u5236\u4eba\u5f62\u673a\u5668\u4eba\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u76ee\u524d\u4eba\u5f62\u673a\u5668\u4eba\u64cd\u63a7\u4e2d\u754c\u9762\u8bbe\u8ba1\u7684\u4e0d\u8db3\uff0c\u4f7f\u975e\u4e13\u5bb6\u7528\u6237\u4e5f\u80fd\u8f7b\u677e\u64cd\u4f5c\u3002", "method": "\u5229\u7528\u7528\u6237\u754c\u9762\u5f00\u53d1\u7684\u5e38\u89c1\u5b9e\u8df5\uff0c\u5e76\u7ed3\u5408\u4eba\u673a\u4ea4\u4e92\u9886\u57df\u7684\u6982\u5ff5\uff0c\u8bbe\u8ba1\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u56fe\u5f62\u7528\u6237\u754c\u9762\u3002", "result": "\u9884\u8ba1\u65b0\u5f00\u53d1\u7684\u754c\u9762\u5c06\u63d0\u9ad8\u975e\u4e13\u5bb6\u7528\u6237\u7684\u64cd\u4f5c\u80fd\u529b\u548c\u673a\u5668\u4eba\u7684\u53ef\u7528\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f00\u53d1\u7684\u754c\u9762\u65e8\u5728\u8ba9\u975e\u4e13\u5bb6\u7528\u6237\u66f4\u65b9\u4fbf\u5730\u64cd\u63a7\u673a\u5668\u4eba\uff0c\u4fc3\u8fdb\u4eba\u5f62\u673a\u5668\u4eba\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u666e\u53ca\u3002"}}
{"id": "2510.13595", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13595", "abs": "https://arxiv.org/abs/2510.13595", "authors": ["Ethan K. Gordon", "Bruke Baraki", "Hien Bui", "Michael Posa"], "title": "Active Tactile Exploration for Rigid Body Pose and Shape Estimation", "comment": "8 pages, 6 figures", "summary": "General robot manipulation requires the handling of previously unseen\nobjects. Learning a physically accurate model at test time can provide\nsignificant benefits in data efficiency, predictability, and reuse between\ntasks. Tactile sensing can compliment vision with its robustness to occlusion,\nbut its temporal sparsity necessitates careful online exploration to maintain\ndata efficiency. Direct contact can also cause an unrestrained object to move,\nrequiring both shape and location estimation. In this work, we propose a\nlearning and exploration framework that uses only tactile data to\nsimultaneously determine the shape and location of rigid objects with minimal\nrobot motion. We build on recent advances in contact-rich system identification\nto formulate a loss function that penalizes physical constraint violation\nwithout introducing the numerical stiffness inherent in rigid-body contact.\nOptimizing this loss, we can learn cuboid and convex polyhedral geometries with\nless than 10s of randomly collected data after first contact. Our exploration\nscheme seeks to maximize Expected Information Gain and results in significantly\nfaster learning in both simulated and real-robot experiments. More information\ncan be found at https://dairlab.github.io/activetactile", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u89e6\u89c9\u6570\u636e\u7684\u6846\u67b6\uff0c\u65e8\u5728\u9ad8\u6548\u5b66\u4e60\u672a\u89c1\u7269\u4f53\u7684\u5f62\u72b6\u548c\u4f4d\u7f6e\uff0c\u663e\u8457\u63d0\u9ad8\u6570\u636e\u6536\u96c6\u7684\u6548\u7387\u3002", "motivation": "\u5728\u6d4b\u8bd5\u65f6\u5b66\u4e60\u4e00\u4e2a\u7269\u7406\u7cbe\u786e\u7684\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u6570\u636e\u6548\u7387\u3001\u53ef\u9884\u6d4b\u6027\u548c\u4efb\u52a1\u4e4b\u95f4\u7684\u91cd\u7528\u6027\uff0c\u540c\u65f6\u514b\u670d\u89e6\u89c9\u4f20\u611f\u5668\u5728\u65f6\u95f4\u4e0a\u7684\u7a00\u758f\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4ec5\u4f7f\u7528\u89e6\u89c9\u6570\u636e\u7684\u5b66\u4e60\u4e0e\u63a2\u7d22\u6846\u67b6\uff0c\u4ee5\u6700\u5c0f\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u540c\u65f6\u786e\u5b9a\u521a\u6027\u7269\u4f53\u7684\u5f62\u72b6\u548c\u4f4d\u7f6e\u3002", "result": "\u5728\u7b2c\u4e00\u6b21\u63a5\u89e6\u540e\uff0c\u53ea\u9700\u5c11\u4e8e10\u79d2\u7684\u968f\u673a\u91c7\u96c6\u6570\u636e\u5373\u53ef\u5b66\u4e60\u957f\u65b9\u4f53\u548c\u51f8\u591a\u9762\u4f53\u51e0\u4f55\u5f62\u72b6\uff1b\u63a2\u7d22\u65b9\u6848\u663e\u8457\u52a0\u5feb\u4e86\u5b66\u4e60\u901f\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u6a21\u62df\u548c\u5b9e\u9645\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u52a0\u901f\u7684\u5b66\u4e60\u6548\u7387\uff0c\u8bc1\u660e\u4e86\u5728\u7ebf\u63a2\u7d22\u548c\u6570\u636e\u5229\u7528\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.13599", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13599", "abs": "https://arxiv.org/abs/2510.13599", "authors": ["Jiahao Wang", "Nived Chebrolu", "Yifu Tao", "Lintong Zhang", "Ayoung Kim", "Maurice Fallon"], "title": "PlanarMesh: Building Compact 3D Meshes from LiDAR using Incremental Adaptive Resolution Reconstruction", "comment": null, "summary": "Building an online 3D LiDAR mapping system that produces a detailed surface\nreconstruction while remaining computationally efficient is a challenging task.\nIn this paper, we present PlanarMesh, a novel incremental, mesh-based LiDAR\nreconstruction system that adaptively adjusts mesh resolution to achieve\ncompact, detailed reconstructions in real-time. It introduces a new\nrepresentation, planar-mesh, which combines plane modeling and meshing to\ncapture both large surfaces and detailed geometry. The planar-mesh can be\nincrementally updated considering both local surface curvature and free-space\ninformation from sensor measurements. We employ a multi-threaded architecture\nwith a Bounding Volume Hierarchy (BVH) for efficient data storage and fast\nsearch operations, enabling real-time performance. Experimental results show\nthat our method achieves reconstruction accuracy on par with, or exceeding,\nstate-of-the-art techniques-including truncated signed distance functions,\noccupancy mapping, and voxel-based meshing-while producing smaller output file\nsizes (10 times smaller than raw input and more than 5 times smaller than\nmesh-based methods) and maintaining real-time performance (around 2 Hz for a\n64-beam sensor).", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u589e\u91cf\u7f51\u683c\u5316LiDAR\u91cd\u5efa\u7cfb\u7edf\uff0c\u7ed3\u5408\u5e73\u9762\u5efa\u6a21\u4e0e\u7f51\u683c\u5316\u6280\u672f\uff0c\u5b9e\u65f6\u751f\u6210\u9ad8\u7cbe\u5ea6\u548c\u5c0f\u6587\u4ef6\u5927\u5c0f\u76843D\u91cd\u5efa\u3002", "motivation": "\u5728\u7ebf3D LiDAR\u6620\u5c04\u7cfb\u7edf\u9700\u5728\u7ec6\u81f4\u8868\u9762\u91cd\u5efa\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u3002", "method": "PlanarMesh\uff0c\u589e\u91cf\u5f0f\u7f51\u683c\u5316LiDAR\u91cd\u5efa\u7cfb\u7edf", "result": "PlanarMesh\u8fbe\u5230\u4e86\u4e0e\u6700\u5148\u8fdb\u6280\u672f\u76f8\u5f53\u6216\u66f4\u9ad8\u7684\u91cd\u5efa\u7cbe\u5ea6\uff0c\u540c\u65f6\u8f93\u51fa\u6587\u4ef6\u5927\u5c0f\u663e\u8457\u66f4\u5c0f\uff0c\u5e76\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u91c7\u7528\u591a\u7ebf\u7a0b\u67b6\u6784\u548cBounding Volume Hierarchy\u6280\u672f\uff0cPlanarMesh\u5728\u7cbe\u5ea6\u4e0e\u6548\u7387\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u5b9e\u65f63D LiDAR\u6620\u5c04\u3002"}}
{"id": "2510.13616", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.13616", "abs": "https://arxiv.org/abs/2510.13616", "authors": ["Preston Fairchild", "Claudia Chen", "Xiaobo Tan"], "title": "Efficient Force and Stiffness Prediction in Robotic Produce Handling with a Piezoresistive Pressure Sensor", "comment": "For supplementary videos, see\n  https://drive.google.com/drive/folders/1jol-_z6gaUfjpL1Qi7EG420usTbVSodv?usp=sharing", "summary": "Properly handling delicate produce with robotic manipulators is a major part\nof the future role of automation in agricultural harvesting and processing.\nGrasping with the correct amount of force is crucial in not only ensuring\nproper grip on the object, but also to avoid damaging or bruising the product.\nIn this work, a flexible pressure sensor that is both low cost and easy to\nfabricate is integrated with robotic grippers for working with produce of\nvarying shapes, sizes, and stiffnesses. The sensor is successfully integrated\nwith both a rigid robotic gripper, as well as a pneumatically actuated soft\nfinger. Furthermore, an algorithm is proposed for accelerated estimation of the\nsteady-state value of the sensor output based on the transient response data,\nto enable real-time applications. The sensor is shown to be effective in\nincorporating feedback to correctly grasp objects of unknown sizes and\nstiffnesses. At the same time, the sensor provides estimates for these values\nwhich can be utilized for identification of qualities such as ripeness levels\nand bruising. It is also shown to be able to provide force feedback for objects\nof variable stiffnesses. This enables future use not only for produce\nidentification, but also for tasks such as quality control and selective\ndistribution based on ripeness levels.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u6613\u4e8e\u5236\u9020\u7684\u67d4\u6027\u538b\u529b\u4f20\u611f\u5668\uff0c\u7528\u4e8e\u63d0\u9ad8\u673a\u5668\u4eba\u673a\u68b0\u624b\u5728\u6293\u53d6\u519c\u4ea7\u54c1\u65f6\u7684\u6027\u80fd\uff0c\u80fd\u591f\u5b9e\u65f6\u4f30\u8ba1\u7269\u4f53\u7684\u521a\u5ea6\u548c\u6210\u719f\u5ea6\u3002", "motivation": "\u63d0\u9ad8\u673a\u5668\u4eba\u5bf9\u519c\u4e1a\u4ea7\u54c1\u7684\u5904\u7406\u80fd\u529b\uff0c\u907f\u514d\u635f\u4f24\uff0c\u4fc3\u8fdb\u81ea\u52a8\u5316\u519c\u4e1a\u7684\u53d1\u5c55\u3002", "method": "\u5c06\u67d4\u6027\u538b\u529b\u4f20\u611f\u5668\u4e0e\u521a\u6027\u673a\u68b0\u624b\u548c\u6c14\u52a8\u8f6f\u6307\u7ed3\u5408\uff0c\u91c7\u7528\u5b9e\u65f6\u7b97\u6cd5\u52a0\u901f\u7a33\u6001\u8f93\u51fa\u4f30\u8ba1\u3002", "result": "\u8fd9\u9879\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u578b\u67d4\u6027\u538b\u529b\u4f20\u611f\u5668\uff0c\u4e0e\u673a\u68b0\u624b\u81c2\u96c6\u6210\uff0c\u53ef\u7528\u4e8e\u7cbe\u7ec6\u5904\u7406\u519c\u4ea7\u54c1\u3002", "conclusion": "\u8be5\u4f20\u611f\u5668\u7684\u5e94\u7528\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u5bf9\u4e0d\u540c\u5f62\u72b6\u3001\u5c3a\u5bf8\u548c\u521a\u5ea6\u7684\u7269\u4f53\u7684\u6293\u53d6\u80fd\u529b\uff0c\u8fd8\u80fd\u5728\u8d28\u91cf\u63a7\u5236\u548c\u9009\u62e9\u5206\u53d1\u4e2d\u53d1\u6325\u4f5c\u7528\uff0c\u4fc3\u8fdb\u672a\u6765\u519c\u4e1a\u81ea\u52a8\u5316\u3002"}}
{"id": "2510.13619", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13619", "abs": "https://arxiv.org/abs/2510.13619", "authors": ["Daniel Choate", "Jason Rife"], "title": "Characterizing Lidar Point-Cloud Adversities Using a Vector Field Visualization", "comment": "This is the preprint version of the paper published in: Proceedings\n  of the 37th International Technical Meeting of the Satellite Division of The\n  Institute of Navigation (ION GNSS+ 2024), September 2024 The final version is\n  available at https://doi.org/10.33012/2024.19864", "summary": "In this paper we introduce a visualization methodology to aid a human analyst\nin classifying adversity modes that impact lidar scan matching. Our methodology\nis intended for offline rather than real-time analysis. The method generates a\nvector-field plot that characterizes local discrepancies between a pair of\nregistered point clouds. The vector field plot reveals patterns that would be\ndifficult for the analyst to extract from raw point-cloud data. After\nintroducing our methodology, we apply the process to two proof-of-concept\nexamples: one a simulation study and the other a field experiment. For both\ndata sets, a human analyst was able to reason about a series of adversity\nmechanisms and iteratively remove those mechanisms from the raw data, to help\nfocus attention on progressively smaller discrepancies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\u5206\u6790\u7684\u53ef\u89c6\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u77e2\u91cf\u573a\u56fe\u63ed\u793a\u70b9\u4e91\u6570\u636e\u95f4\u7684\u5c40\u90e8\u5dee\u5f02\uff0c\u5e2e\u52a9\u5206\u6790\u5e08\u8bc6\u522b\u5e76\u6d88\u9664\u5f71\u54cd\u56e0\u7d20\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u6539\u5584\u6fc0\u5149\u96f7\u8fbe\u626b\u63cf\u6570\u636e\u7684\u5206\u6790\u6548\u7387\uff0c\u5e2e\u52a9\u5206\u6790\u5e08\u66f4\u6e05\u6670\u5730\u8bc6\u522b\u548c\u5904\u7406\u6570\u636e\u4e2d\u7684\u9006\u5883\u6a21\u5f0f\u3002", "method": "\u8be5\u65b9\u6cd5\u751f\u6210\u4e00\u4e2a\u77e2\u91cf\u573a\u56fe\uff0c\u5c55\u793a\u914d\u51c6\u70b9\u4e91\u4e4b\u95f4\u7684\u5c40\u90e8\u5dee\u5f02\uff0c\u8f85\u52a9\u5206\u6790\u5e08\u8bc6\u522b\u6f5c\u5728\u7684\u9006\u5883\u6a21\u5f0f\u3002", "result": "\u672c\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u53ef\u89c6\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u5e2e\u52a9\u4eba\u7c7b\u5206\u6790\u5e08\u5206\u7c7b\u5f71\u54cd\u6fc0\u5149\u96f7\u8fbe\u626b\u63cf\u5339\u914d\u7684\u9006\u5883\u6a21\u5f0f\u3002", "conclusion": "\u901a\u8fc7\u5e94\u7528\u6b64\u65b9\u6cd5\uff0c\u5206\u6790\u5e08\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u548c\u7406\u89e3\u9006\u5883\u673a\u5236\uff0c\u9010\u6b65\u53bb\u9664\u5f71\u54cd\u56e0\u7d20\u3002"}}
{"id": "2510.13625", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13625", "abs": "https://arxiv.org/abs/2510.13625", "authors": ["Nicolas Pottier", "Meng Cheng Lau"], "title": "A Modular Object Detection System for Humanoid Robots Using YOLO", "comment": "7 Figures, 5 tables. This article was presented at FIRA Summit 2025.\n  It will be updated for journal submission", "summary": "Within the field of robotics, computer vision remains a significant barrier\nto progress, with many tasks hindered by inefficient vision systems. This\nresearch proposes a generalized vision module leveraging YOLOv9, a\nstate-of-the-art framework optimized for computationally constrained\nenvironments like robots. The model is trained on a dataset tailored to the\nFIRA robotics Hurocup. A new vision module is implemented in ROS1 using a\nvirtual environment to enable YOLO compatibility. Performance is evaluated\nusing metrics such as frames per second (FPS) and Mean Average Precision (mAP).\nPerformance is then compared to the existing geometric framework in static and\ndynamic contexts. The YOLO model achieved comparable precision at a higher\ncomputational cost then the geometric model, while providing improved\nrobustness.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u8bae\u5229\u7528YOLOv9\u63d0\u5347\u673a\u5668\u4eba\u89c6\u89c9\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\u8be5\u6a21\u578b\u5728\u7cbe\u5ea6\u4e0a\u5177\u6709\u7ade\u4e89\u529b\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u8f83\u9ad8\u3002", "motivation": "\u673a\u5668\u4eba\u9886\u57df\u7684\u89c6\u89c9\u7cfb\u7edf\u6548\u7387\u4f4e\u4e0b\uff0c\u9650\u5236\u4e86\u6280\u672f\u8fdb\u6b65\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eYOLOv9\u7684\u901a\u7528\u89c6\u89c9\u6a21\u5757\uff0c\u8fdb\u884c\u673a\u5668\u4eba\u7535\u8111\u89c6\u89c9\u4efb\u52a1\u7684\u4f18\u5316\u3002", "result": "\u65b0\u7684\u89c6\u89c9\u6a21\u5757\u5728ROS1\u4e2d\u5b9e\u73b0\uff0c\u4f7f\u7528\u865a\u62df\u73af\u5883\u4ee5\u5b9e\u73b0YOLO\u517c\u5bb9\u6027\uff0c\u6027\u80fd\u901a\u8fc7FPS\u548cmAP\u8bc4\u4f30\u3002", "conclusion": "YOLO\u6a21\u578b\u5728\u9759\u6001\u548c\u52a8\u6001\u73af\u5883\u4e0b\u4e0e\u51e0\u4f55\u6a21\u578b\u76f8\u6bd4\uff0c\u5c55\u793a\u4e86\u53ef\u6bd4\u7cbe\u5ea6\u4e0e\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.13626", "categories": ["cs.RO", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13626", "abs": "https://arxiv.org/abs/2510.13626", "authors": ["Senyu Fei", "Siyin Wang", "Junhao Shi", "Zihao Dai", "Jikun Cai", "Pengfang Qian", "Li Ji", "Xinzhe He", "Shiduo Zhang", "Zhaoye Fei", "Jinlan Fu", "Jingjing Gong", "Xipeng Qiu"], "title": "LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models", "comment": null, "summary": "Visual-Language-Action (VLA) models report impressive success rates on\nrobotic manipulation benchmarks, yet these results may mask fundamental\nweaknesses in robustness. We perform a systematic vulnerability analysis by\nintroducing controlled perturbations across seven dimensions: objects layout,\ncamera viewpoints, robot initial states, language instructions, light\nconditions, background textures and sensor noise. We comprehensively analyzed\nmultiple state-of-the-art models and revealed consistent brittleness beneath\napparent competence. Our analysis exposes critical weaknesses: models exhibit\nextreme sensitivity to perturbation factors, including camera viewpoints and\nrobot initial states, with performance dropping from 95% to below 30% under\nmodest perturbations. Surprisingly, models are largely insensitive to language\nvariations, with further experiments revealing that models tend to ignore\nlanguage instructions completely. Our findings challenge the assumption that\nhigh benchmark scores equate to true competency and highlight the need for\nevaluation practices that assess reliability under realistic variation.", "AI": {"tldr": "VLA\u6a21\u578b\u8868\u73b0\u51fa\u8272\u4f46\u5728\u9c81\u68d2\u6027\u4e0a\u5b58\u5728\u4e25\u91cd\u95ee\u9898\uff0c\u6613\u53d7\u5230\u591a\u79cd\u6270\u52a8\u5f71\u54cd\u3002", "motivation": "\u65e8\u5728\u63ed\u793aVLA\u6a21\u578b\u5728\u9762\u5bf9\u5b9e\u9645\u64cd\u4f5c\u573a\u666f\u65f6\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u5e76\u6311\u6218\u73b0\u6709\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "method": "\u901a\u8fc7\u5728\u4e03\u4e2a\u7ef4\u5ea6\u5f15\u5165\u53d7\u63a7\u6270\u52a8\uff08\u5305\u62ec\u7269\u4f53\u5e03\u5c40\u3001\u76f8\u673a\u89c6\u89d2\u7b49\uff09\u8fdb\u884c\u7cfb\u7edf\u7684\u8106\u5f31\u6027\u5206\u6790\u3002", "result": "\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6210\u529f\u7387\uff0c\u4f46\u8fd9\u4e9b\u7ed3\u679c\u53ef\u80fd\u63a9\u76d6\u4e86\u5176\u5728\u9c81\u68d2\u6027\u65b9\u9762\u7684\u6839\u672c\u5f31\u70b9\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u63ed\u793a\u4e86\u6a21\u578b\u5bf9\u6270\u52a8\u56e0\u7d20\uff08\u5982\u76f8\u673a\u89c6\u89d2\u548c\u673a\u5668\u4eba\u521d\u59cb\u72b6\u6001\uff09\u7684\u6781\u5ea6\u654f\u611f\u6027\uff0c\u6027\u80fd\u5728\u8f7b\u5fae\u6270\u52a8\u4e0b\u53ef\u80fd\u4f1a\u5927\u5e45\u4e0b\u964d\uff0c\u540c\u65f6\u6a21\u578b\u5bf9\u8bed\u8a00\u6307\u4ee4\u7684\u53d8\u5316\u53cd\u5e94\u4e0d\u654f\u611f\uff0c\u8fd9\u6311\u6218\u4e86\u9ad8\u57fa\u51c6\u5206\u6570\u4e0e\u771f\u6b63\u80fd\u529b\u4e4b\u95f4\u7684\u5047\u8bbe\u3002"}}
{"id": "2510.13644", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13644", "abs": "https://arxiv.org/abs/2510.13644", "authors": ["Michael Bosello", "Flavio Pinzarrone", "Sara Kiade", "Davide Aguiari", "Yvo Keuter", "Aaesha AlShehhi", "Gyordan Caminati", "Kei Long Wong", "Ka Seng Chou", "Junaid Halepota", "Fares Alneyadi", "Jacopo Panerati", "Giovanni Pau"], "title": "On Your Own: Pro-level Autonomous Drone Racing in Uninstrumented Arenas", "comment": null, "summary": "Drone technology is proliferating in many industries, including agriculture,\nlogistics, defense, infrastructure, and environmental monitoring. Vision-based\nautonomy is one of its key enablers, particularly for real-world applications.\nThis is essential for operating in novel, unstructured environments where\ntraditional navigation methods may be unavailable. Autonomous drone racing has\nbecome the de facto benchmark for such systems. State-of-the-art research has\nshown that autonomous systems can surpass human-level performance in racing\narenas. However, direct applicability to commercial and field operations is\nstill limited as current systems are often trained and evaluated in highly\ncontrolled environments. In our contribution, the system's capabilities are\nanalyzed within a controlled environment -- where external tracking is\navailable for ground-truth comparison -- but also demonstrated in a\nchallenging, uninstrumented environment -- where ground-truth measurements were\nnever available. We show that our approach can match the performance of\nprofessional human pilots in both scenarios. We also publicly release the data\nfrom the flights carried out by our approach and a world-class human pilot.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u65e0\u4eba\u673a\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u9a7e\u9a76\u6027\u80fd\uff0c\u7ed3\u679c\u663e\u793a\u5176\u53ef\u4e0e\u4e13\u4e1a\u4eba\u7c7b\u98de\u884c\u5458\u76f8\u5ab2\u7f8e\uff0c\u5e76\u516c\u5f00\u4e86\u76f8\u5173\u6570\u636e\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u6280\u672f\u5728\u5404\u884c\u4e1a\u7684\u666e\u53ca\uff0c\u63d0\u9ad8\u5176\u5728\u771f\u5b9e\u3001\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u80fd\u529b\uff0c\u63d0\u9ad8\u65e0\u4eba\u673a\u7ade\u4e89\u529b\u4e0e\u5e94\u7528\u8303\u56f4\u3002", "method": "\u5728\u53d7\u63a7\u73af\u5883\u4e2d\u901a\u8fc7\u5916\u90e8\u8ffd\u8e2a\u8fdb\u884c\u80fd\u529b\u5206\u6790\uff0c\u5e76\u5728\u6ca1\u6709\u5730\u9762\u771f\u5b9e\u6d4b\u91cf\u7684\u6311\u6218\u6027\u73af\u5883\u4e2d\u8fdb\u884c\u6027\u80fd\u5c55\u793a\u3002", "result": "\u672c\u7814\u7a76\u7740\u773c\u4e8e\u65e0\u4eba\u673a\u6280\u672f\uff0c\u7279\u522b\u662f\u57fa\u4e8e\u89c6\u89c9\u7684\u81ea\u4e3b\u9a7e\u9a76\u80fd\u529b\uff0c\u5e76\u5728\u53d7\u63a7\u548c\u4e0d\u53d7\u63a7\u73af\u5883\u4e2d\u8fdb\u884c\u6027\u80fd\u5206\u6790\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u7cfb\u7edf\u5728\u4e24\u79cd\u73af\u5883\u4e2d\u7684\u8868\u73b0\u8fbe\u5230\u4e86\u4e13\u4e1a\u4eba\u7c7b\u98de\u884c\u5458\u7684\u6c34\u5e73\uff0c\u540c\u65f6\u516c\u5f00\u4e86\u98de\u884c\u6570\u636e\u4ee5\u4f9b\u7814\u7a76\u4f7f\u7528\u3002", "conclusion": "\u672c\u7814\u7a76\u7684\u7ed3\u679c\u8868\u660e\u8be5\u81ea\u4e3b\u7cfb\u7edf\u80fd\u591f\u5728\u53d7\u63a7\u548c\u4e0d\u53d7\u63a7\u73af\u5883\u4e2d\u4e0e\u4e13\u4e1a\u4eba\u7c7b\u98de\u884c\u5458\u7684\u8868\u73b0\u76f8\u5339\u914d\uff0c\u662f\u65e0\u4eba\u673a\u6280\u672f\u5411\u5546\u4e1a\u5e94\u7528\u8fc8\u8fdb\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2510.13686", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13686", "abs": "https://arxiv.org/abs/2510.13686", "authors": ["Miana Smith", "Paul Arthur Richard", "Alexander Htet Kyaw", "Neil Gershenfeld"], "title": "Hierarchical Discrete Lattice Assembly: An Approach for the Digital Fabrication of Scalable Macroscale Structures", "comment": "In ACM Symposium on Computational Fabrication (SCF '25), November\n  20-21, 2025, Cambridge, MA, USA. ACM, New York, NY, USA, 15 pages", "summary": "Although digital fabrication processes at the desktop scale have become\nproficient and prolific, systems aimed at producing larger-scale structures are\nstill typically complex, expensive, and unreliable. In this work, we present an\napproach for the fabrication of scalable macroscale structures using simple\nrobots and interlocking lattice building blocks. A target structure is first\nvoxelized so that it can be populated with an architected lattice. These voxels\nare then grouped into larger interconnected blocks, which are produced using\nstandard digital fabrication processes, leveraging their capability to produce\nhighly complex geometries at a small scale. These blocks, on the size scale of\ntens of centimeters, are then fed to mobile relative robots that are able to\ntraverse over the structure and place new blocks to form structures on the\nmeter scale. To facilitate the assembly of large structures, we introduce a\nlive digital twin simulation tool for controlling and coordinating assembly\nrobots that enables both global planning for a target structure and live user\ndesign, interaction, or intervention. To improve assembly throughput, we\nintroduce a new modular assembly robot, designed for hierarchical voxel\nhandling. We validate this system by demonstrating the voxelization,\nhierarchical blocking, path planning, and robotic fabrication of a set of\nmeter-scale objects.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u7b80\u5355\u673a\u5668\u4eba\u548c\u6a21\u5757\u5316\u7ec4\u4ef6\u9ad8\u6548\u5236\u9020\u5927\u89c4\u6a21\u7ed3\u6784\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f53\u7d20\u5316\u76ee\u6807\u7ed3\u6784\u53ca\u5176\u5b9e\u65f6\u6570\u5b57\u53cc\u80de\u80ce\u5de5\u5177\uff0c\u6765\u534f\u8c03\u673a\u5668\u4eba\u7ec4\u88c5\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u7ed3\u6784\u7684\u5236\u9020\u7cfb\u7edf\u901a\u5e38\u590d\u6742\u3001\u6602\u8d35\u4e14\u4e0d\u53ef\u9760\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u3001\u66f4\u7b80\u5316\u7684\u5236\u9020\u65b9\u5f0f\u3002", "method": "\u4f7f\u7528\u7b80\u5355\u673a\u5668\u4eba\u548c\u76f8\u4e92\u9501\u5b9a\u7684\u7f51\u683c\u5efa\u7b51\u5757\u7684\u89c4\u6a21\u5316\u5b8f\u7ed3\u6784\u5236\u9020\u65b9\u6cd5", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u76ee\u6807\u7ed3\u6784\u4f53\u7d20\u5316\u5e76\u4f7f\u7528\u79fb\u52a8\u76f8\u5bf9\u673a\u5668\u4eba\u6765\u653e\u7f6e\u5757\uff0c\u4ece\u800c\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u5927\u578b\u7ed3\u6784\u5236\u9020\u3002", "conclusion": "\u901a\u8fc7\u6f14\u793a\u4e00\u7cfb\u5217\u7c73\u5c3a\u5ea6\u5bf9\u8c61\u7684\u5236\u9020\u8fc7\u7a0b\uff0c\u9a8c\u8bc1\u4e86\u8be5\u7cfb\u7edf\u7684\u6709\u6548\u6027\uff0c\u5c55\u73b0\u4e86\u5176\u5728\u5927\u89c4\u6a21\u7ed3\u6784\u5236\u9020\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.13778", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13778", "abs": "https://arxiv.org/abs/2510.13778", "authors": ["Xinyi Chen", "Yilun Chen", "Yanwei Fu", "Ning Gao", "Jiaya Jia", "Weiyang Jin", "Hao Li", "Yao Mu", "Jiangmiao Pang", "Yu Qiao", "Yang Tian", "Bin Wang", "Bolun Wang", "Fangjing Wang", "Hanqing Wang", "Tai Wang", "Ziqin Wang", "Xueyuan Wei", "Chao Wu", "Shuai Yang", "Jinhui Ye", "Junqiu Yu", "Jia Zeng", "Jingjing Zhang", "Jinyu Zhang", "Shi Zhang", "Feng Zheng", "Bowen Zhou", "Yangkun Zhu"], "title": "InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy", "comment": "Technical report", "summary": "We introduce InternVLA-M1, a unified framework for spatial grounding and\nrobot control that advances instruction-following robots toward scalable,\ngeneral-purpose intelligence. Its core idea is spatially guided\nvision-language-action training, where spatial grounding serves as the critical\nlink between instructions and robot actions. InternVLA-M1 employs a two-stage\npipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning\ndata to determine ``where to act'' by aligning instructions with visual,\nembodiment-agnostic positions, and (ii) spatially guided action post-training\nto decide ``how to act'' by generating embodiment-aware actions through\nplug-and-play spatial prompting. This spatially guided training recipe yields\nconsistent gains: InternVLA-M1 outperforms its variant without spatial guidance\nby +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO\nFranka, while demonstrating stronger spatial reasoning capability in box,\npoint, and trace prediction. To further scale instruction following, we built a\nsimulation engine to collect 244K generalizable pick-and-place episodes,\nenabling a 6.2% average improvement across 200 tasks and 3K+ objects. In\nreal-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with\nsynthetic co-training, achieved +20.6% on unseen objects and novel\nconfigurations. Moreover, in long-horizon reasoning-intensive scenarios, it\nsurpassed existing works by over 10%. These results highlight spatially guided\ntraining as a unifying principle for scalable and resilient generalist robots.\nCode and models are available at\nhttps://github.com/InternRobotics/InternVLA-M1.", "AI": {"tldr": "\u63d0\u51faInternVLA-M1\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u5f15\u5bfc\u8bad\u7ec3\u63d0\u9ad8\u673a\u5668\u4eba\u6307\u4ee4\u9075\u5faa\u80fd\u529b\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u901a\u7528\u667a\u80fd\u3002", "motivation": "\u63a8\u52a8\u9075\u5faa\u6307\u4ee4\u7684\u673a\u5668\u4eba\u671d\u5411\u53ef\u6269\u5c55\u7684\u901a\u7528\u667a\u80fd\u53d1\u5c55\u3002", "method": "\u8be5\u6846\u67b6\u5305\u62ec\u4e24\u4e2a\u9636\u6bb5\uff1a\u4e00\u662f\u57fa\u4e8e\u7a7a\u95f4\u63a8\u7406\u6570\u636e\u7684\u9884\u8bad\u7ec3\uff0c\u4e8c\u662f\u901a\u8fc7\u7a7a\u95f4\u63d0\u793a\u751f\u6210\u4e0e\u5b9e\u4f53\u76f8\u5173\u7684\u884c\u4e3a\u3002", "result": "InternVLA-M1\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u63d0\u5347\uff0c\u5c24\u5176\u662f\u5728\u7a7a\u95f4\u63a8\u7406\u548c\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u4e0a\u3002", "conclusion": "\u7a7a\u95f4\u5f15\u5bfc\u8bad\u7ec3\u4e3a\u53ef\u6269\u5c55\u4e14\u5177\u97e7\u6027\u7684\u901a\u7528\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u539f\u5219\u3002"}}
