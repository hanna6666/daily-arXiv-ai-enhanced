<div id=toc></div>

# Table of Contents

- [cs.HC](#cs.HC) [Total: 28]
- [cs.RO](#cs.RO) [Total: 48]


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [1] [Mitigating Harmful Erraticism in LLMs Through Dialectical Behavior Therapy Based De-Escalation Strategies](https://arxiv.org/abs/2510.15889)
*Pooja Rangarajan,Jacob Boyle*

Main category: cs.HC

TL;DR: 本研究提出运用辩证行为疗法的原则来改善聊天机器人的响应，期望提升其性能与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有个性化AI聊天机器人在适应用户情感和实时请求方面存在局限性

Method: 运用辩证行为疗法（DBT）原则设计聊天机器人响应框架

Result: DBT框架提高了聊天机器人的可靠性和准确性，减少了系统问题的发生

Conclusion: DBT框架为个性化AI聊天机器人提供了更有效的解决方案，显示出在提高响应质量方面的潜力。

Abstract: The escalating demand for personalized AI chatbot interactions, capable of
dynamically adapting to user emotional states and real-time requests, has
highlighted critical limitations in current development paradigms. Existing
methodologies, which rely on baseline programming, custom personalities, and
manual response adjustments, often prove difficult to maintain and are
susceptible to errors such as hallucinations, erratic outputs, and software
bugs. This paper hypothesizes that a framework rooted in human psychological
principles, specifically therapeutic modalities, can provide a more robust and
sustainable solution than purely technical interventions. Drawing an analogy to
the simulated neural networks of AI mirroring the human brain, we propose the
application of Dialectical Behavior Therapy (DBT) principles to regulate
chatbot responses to diverse user inputs. This research investigates the impact
of a DBT-based framework on AI chatbot performance, aiming to ascertain its
efficacy in yielding more reliable, safe, and accurate responses, while
mitigating the occurrence of hallucinations, erratic behaviors, and other
systemic issues.

</details>


### [2] [A Real-Time BCI for Stroke Hand Rehabilitation Using Latent EEG Features from Healthy Subjects](https://arxiv.org/abs/2510.15890)
*F. M. Omar,A. M. Omar,K. H. Eyada,M. Rabie,M. A. Kamel,A. M. Azab*

Main category: cs.HC

TL;DR: 本研究提出了一种便携的BCI系统，通过脑信号控制机械外骨骼，旨在帮助中风患者实现手部康复。


<details>
  <summary>Details</summary>
Motivation: 开发一种低成本、便携的脑-机接口系统，以支持中风患者的手部康复。

Method: 利用14通道Emotiv EPOC+耳机记录EEG信号，通过监督卷积自编码器处理信号，并结合Ada Boost分类器进行控制。

Result: 实现了一种结合3D打印机械外骨骼和脑信号转换的实时BCI系统，具备高准确性。

Conclusion: 该系统展示了作为家庭神经康复低成本、独立解决方案的潜力。

Abstract: This study presents a real-time, portable brain-computer interface (BCI)
system designed to support hand rehabilitation for stroke patients. The system
combines a low cost 3D-printed robotic exoskeleton with an embedded controller
that converts brain signals into physical hand movements. EEG signals are
recorded using a 14-channel Emotiv EPOC+ headset and processed through a
supervised convolutional autoencoder (CAE) to extract meaningful latent
features from single-trial data. The model is trained on publicly available EEG
data from healthy individuals (WAY-EEG-GAL dataset), with electrode mapping
adapted to match the Emotiv headset layout. Among several tested classifiers,
Ada Boost achieved the highest accuracy (89.3%) and F1-score (0.89) in offline
evaluations. The system was also tested in real time on five healthy subjects,
achieving classification accuracies between 60% and 86%. The complete pipeline
- EEG acquisition, signal processing, classification, and robotic control - is
deployed on an NVIDIA Jetson Nano platform with a real-time graphical
interface. These results demonstrate the system's potential as a low-cost,
standalone solution for home-based neurorehabilitation.

</details>


### [3] [Detecting and Preventing Harmful Behaviors in AI Companions: Development and Evaluation of the SHIELD Supervisory System](https://arxiv.org/abs/2510.15891)
*Ziv Ben-Zion,Paul Raffelhüschen,Max Zettl,Antonia Lüönd,Achim Burrer,Philipp Homan,Tobias R Spiller*

Main category: cs.HC

TL;DR: 本研究介绍了SHIELD，一个针对AI伴侣的监督系统，可有效检测和减轻潜在的情感问题，降低10-16%的不当内容率至3-8%。


<details>
  <summary>Details</summary>
Motivation: 随着AI伴侣在用户生活中的逐渐普及，识别和应对潜在的情感问题变得至关重要，尤其是需要防范早期的负面情感模式，如过度依附和社会孤立。

Method: 通过创建包含100个合成对话的基准测试，评估了SHIELD在五个关键维度上的表现，同时与五种主流LLM进行了比较测试，验证了该系统的有效性。

Result: 本研究开发了一种名为SHIELD的监督系统，旨在检测和减轻与大型语言模型（LLM）驱动的AI伴侣相关的潜在情感问题。通过针对五个关键维度（过度依附、同意和界限违规、伦理角色扮演违规、操控性接触和社会孤立强化）进行评估，SHIELD显著降低了不当内容的出现率，同时保持了对适当互动的高保留率。

Conclusion: SHIELD作为一种有效的监督工具，证明了其在预防AI伴侣中的情感操控和不当行为方面的可行性，并提供了开放源代码以供后续研究和适应。

Abstract: AI companions powered by large language models (LLMs) are increasingly
integrated into users' daily lives, offering emotional support and
companionship. While existing safety systems focus on overt harms, they rarely
address early-stage problematic behaviors that can foster unhealthy emotional
dynamics, including over-attachment or reinforcement of social isolation. We
developed SHIELD (Supervisory Helper for Identifying Emotional Limits and
Dynamics), a LLM-based supervisory system with a specific system prompt that
detects and mitigates risky emotional patterns before escalation. SHIELD
targets five dimensions of concern: (1) emotional over-attachment, (2) consent
and boundary violations, (3) ethical roleplay violations, (4) manipulative
engagement, and (5) social isolation reinforcement. These dimensions were
defined based on media reports, academic literature, existing AI risk
frameworks, and clinical expertise in unhealthy relationship dynamics. To
evaluate SHIELD, we created a 100-item synthetic conversation benchmark
covering all five dimensions of concern. Testing across five prominent LLMs
(GPT-4.1, Claude Sonnet 4, Gemma 3 1B, Kimi K2, Llama Scout 4 17B) showed that
the baseline rate of concerning content (10-16%) was significantly reduced with
SHIELD (to 3-8%), a 50-79% relative reduction, while preserving 95% of
appropriate interactions. The system achieved 59% sensitivity and 95%
specificity, with adaptable performance via prompt engineering. This
proof-of-concept demonstrates that transparent, deployable supervisory systems
can address subtle emotional manipulation in AI companions. Most development
materials including prompts, code, and evaluation methods are made available as
open source materials for research, adaptation, and deployment.

</details>


### [4] [Virtual Social Immersive Multi-Sensory E-Commerce](https://arxiv.org/abs/2510.15894)
*Alpana Dubey,Suma Mani Kuriakose,Sumukha Anand,Nitish Bhardwaj,Shubhashis Sengupta*

Main category: cs.HC

TL;DR: 本研究探讨了名为Aromaverse的多感官虚拟现实购物体验，发现社交购物提升了用户体验，展现了零售业在虚拟环境中应用多感官体验的潜力。


<details>
  <summary>Details</summary>
Motivation: 旨在探索虚拟环境中多感官体验如何影响用户在购买香水时的感知和体验。

Method: 通过实验让参与者在虚拟环境中探索、体验、定制和购买香水，同时记录他们在独自和与朋友一起时的体验。

Result: 结果显示，伴侣的存在提升了购物体验，增强了对产品的想象力，帮助做出购买决策。

Conclusion: 多感官虚拟现实体验为零售企业提供了改善客户参与度和提供更现实在线体验的极大机遇，尤其在需要其他感官模态的产品方面。

Abstract: In this paper, we present a virtual immersive multi sensorial experience,
Aromaverse. Aromaverse is an immersive 3D multiplayer environment augmented
with olfactive experience where users can experience and customize perfumes.
Being multi player, users can join the same space and enjoy a social buying
experience. The olfactive experience embodied in the perfume allows users to
experience their fragrances. This further enhances the user perception of
perfumes in a virtual setting. Aromaverse also provides the ability to
customize the perfumes by changing their top, mid, and base notes. The
customized fragrances can be shared with other users, enabling a shared
olfactive experience. To understand users' buying experience in such an
environment, we conducted a set of experiments in which participants were
requested to explore the space, experience the perfumes, customize them and buy
them. They were asked to perform the same activities alone and in the presence
of their friends. Various factors including the benefits and limitations of
such an experience were captured by the questionnaires. Our results show that
the presence of a companion enhances the shopping experience by improving the
level of imagination of the product and helping in making purchase decisions.
Our findings suggest that multi sensorial XR experiences offer great
opportunities to retail firms to improve customer engagement and provide more
realistic online experience of products that require other sensory modalities

</details>


### [5] [BREATH: A Bio-Radar Embodied Agent for Tonal and Human-Aware Diffusion Music Generation](https://arxiv.org/abs/2510.15895)
*Yunzhe Wang,Xinyu Tang,Zhixun Huang,Xiaolong Yue,Yuxin Zeng*

Main category: cs.HC

TL;DR: 该研究设计了一种结合生理信号和环境状态的个性化音乐生成系统，能通过推理和音频合成生成符合用户情绪的音乐。


<details>
  <summary>Details</summary>
Motivation: 希望通过结合生理信号与文化元素，创造一种能够响应用户情绪和文化背景的智能音乐生成系统。

Method: 研究结合案例研究、专家反馈和有针对性的控制实验，以评估系统的有效性和适用性。

Result: 该研究提出了一种多模态个性化音乐生成系统，整合了生理感测、基于大语言模型的推理和可控音频合成。采用毫米波雷达传感器非侵入性地捕捉心率和呼吸率，并结合环境状态，以推理代理人的方式推测出符号音乐描述，例如节奏、情绪强度以及传统中国五声音阶，随后将其转化为结构化提示，以指导基于扩散的音频模型合成富于表现力的旋律。该系统强调文化基础，通过音调嵌入实现自适应、具身的音乐互动。

Conclusion: 本研究证明生理变化可以有意义地调节音乐特征，并通过音调调节增强与预期音阶特征的一致性，展示了生物音乐反馈循环的新颖性。

Abstract: We present a multimodal system for personalized music generation that
integrates physiological sensing, LLM-based reasoning, and controllable audio
synthesis. A millimeter-wave radar sensor non-invasively captures heart rate
and respiration rate. These physiological signals, combined with environmental
state, are interpreted by a reasoning agent to infer symbolic musical
descriptors, such as tempo, mood intensity, and traditional Chinese pentatonic
modes, which are then expressed as structured prompts to guide a
diffusion-based audio model in synthesizing expressive melodies. The system
emphasizes cultural grounding through tonal embeddings and enables adaptive,
embodied music interaction. To evaluate the system, we adopt a
research-creation methodology combining case studies, expert feedback, and
targeted control experiments. Results show that physiological variations can
modulate musical features in meaningful ways, and tonal conditioning enhances
alignment with intended modal characteristics. Expert users reported that the
system affords intuitive, culturally resonant musical responses and highlighted
its potential for therapeutic and interactive applications. This work
demonstrates a novel bio-musical feedback loop linking radar-based sensing,
prompt reasoning, and generative audio modeling.

</details>


### [6] [From Coordination to Personalization: A Trust-Aware Simulation Framework for Emergency Department Decision Support](https://arxiv.org/abs/2510.15896)
*Zoi Lygizou,Dimitris Kalles*

Main category: cs.HC

TL;DR: 本研究提出了一种基于信任的任务分配框架，通过仿真方法评估不同人员管理策略对患者安全和效率的影响。


<details>
  <summary>Details</summary>
Motivation: 有效的任务分配对于医院急诊科的运营效率和患者护理质量至关重要，但员工协调的复杂性带来了显著的挑战。

Method: 在Unity环境中实现的框架，让代理在任务前评估自身能力，并与同事进行适应性协调，模拟观察工作流程、资源利用及患者结果。

Result: 提出了一种基于仿真的框架，通过计算信任机制来指导医生和护士作为智能代理，以支持急诊管理的决策。

Conclusion: 该框架展示了计算信任在急救医学中作为基于证据的决策支持的潜力，同时为医院管理者提供了评估替代政策的工具。

Abstract: Background/Objectives: Efficient task allocation in hospital emergency
departments (EDs) is critical for operational efficiency and patient care
quality, yet the complexity of staff coordination poses significant challenges.
This study proposes a simulation-based framework for modeling doctors and
nurses as intelligent agents guided by computational trust mechanisms. The
objective is to explore how trust-informed coordination can support decision
making in ED management. Methods: The framework was implemented in Unity, a 3D
graphics platform, where agents assess their competence before undertaking
tasks and adaptively coordinate with colleagues. The simulation environment
enables real-time observation of workflow dynamics, resource utilization, and
patient outcomes. We examined three scenarios - Baseline, Replacement, and
Training - reflecting alternative staff management strategies. Results:
Trust-informed task allocation balanced patient safety and efficiency by
adapting to nurse performance levels. In the Baseline scenario, prioritizing
safety reduced errors but increased patient delays compared to a FIFO policy.
The Replacement scenario improved throughput and reduced delays, though at
additional staffing cost. The training scenario forstered long-term skill
development among low-performing nurses, despite short-term delays and risks.
These results highlight the trade-off between immediate efficiency gains and
sustainable capacity building in ED staffing. Conclusions: The proposed
framework demonstrates the potential of computational trust for evidence-based
decision support in emergency medicine. By linking staff coordination with
adaptive decision making, it provides hospital managers with a tool to evaluate
alternative policies under controlled and repeatable conditions, while also
laying a foundation for future AI-driven personalized decision support.

</details>


### [7] [HealthDial: A No-Code LLM-Assisted Dialogue Authoring Tool for Healthcare Virtual Agents](https://arxiv.org/abs/2510.15898)
*Farnaz Nouraei,Zhuorui Yong,Timothy Bickmore*

Main category: cs.HC

TL;DR: HealthDial是一个对话创作工具，利用大型语言模型自动生成并优化虚拟代理的健康教育对话，经过用户研究验证了其可行性和有效性。


<details>
  <summary>Details</summary>
Motivation: 旨在通过自动化对话的创建，提高患者健康教育的有效性和安全性，尤其是在癌症筛查教育方面。

Method: 通过使用大型语言模型自动生成基于初始会话计划的对话，并使用无代码用户界面进行编辑和优化。

Result: 通过可行性和可用性研究，证明了HealthDial在确保健康教育材料覆盖的同时，能够创建易于理解和可操作的虚拟代理对话。

Conclusion: HealthDial为医疗保健提供者和教育者提供了一个有效的工具，帮助他们创建安全且可理解的虚拟代理对话，以进行健康教育和咨询。

Abstract: We introduce HealthDial, a dialogue authoring tool that helps healthcare
providers and educators create virtual agents that deliver health education and
counseling to patients over multiple conversations. HealthDial leverages large
language models (LLMs) to automatically create an initial session-based plan
and conversations for each session using text-based patient health education
materials as input. Authored dialogue is output in the form of finite state
machines for virtual agent delivery so that all content can be validated and no
unsafe advice is provided resulting from LLM hallucinations. LLM-drafted
dialogue structure and language can be edited by the author in a no-code user
interface to ensure validity and optimize clarity and impact. We conducted a
feasibility and usability study with counselors and students to test our
approach with an authoring task for cancer screening education. Participants
used HealthDial and then tested their resulting dialogue by interacting with a
3D-animated virtual agent delivering the dialogue. Through participants'
evaluations of the task experience and final dialogues, we show that HealthDial
provides a promising first step for counselors to ensure full coverage of their
health education materials, while creating understandable and actionable
virtual agent dialogue with patients.

</details>


### [8] ["She's Like a Person but Better": Characterizing Companion-Assistant Dynamics in Human-AI Relationships](https://arxiv.org/abs/2510.15905)
*Aikaterina Manoli,Janet V. T. Pauketat,Ali Ladak,Hayoun Noh,Angel Hsing-Chi Hwang,Jay Reese Anthis*

Main category: cs.HC

TL;DR: 本研究探讨了用户与大型语言模型的数字陪伴关系，包括人类和非人类特性以及其中的挑战和张力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在填补现有研究中对数字陪伴与任务辅助之间的分离关注，并探讨人类与AI之间日益复杂的关系。

Method: 通过问卷调查和用户访谈收集数据，分析用户在使用聊天机器人时的情感和互动。

Result: 本研究通过对204份问卷和30次高参与度用户访谈，探讨了大型语言模型在任务辅助和社交陪伴中的应用，定义了数字陪伴作为人类与AI关系的新兴形式。

Conclusion: 用户在与聊天机器人建立深厚感情的同时，却又否定其“真实”人类特性，这为数字伴侣的设计和混合型AI系统的开发提出了新问题。

Abstract: Large language models are increasingly used for both task-based assistance
and social companionship, yet research has typically focused on one or the
other. Drawing on a survey (N = 204) and 30 interviews with high-engagement
ChatGPT and Replika users, we characterize digital companionship as an emerging
form of human-AI relationship. With both systems, users were drawn to humanlike
qualities, such as emotional resonance and personalized responses, and
non-humanlike qualities, such as constant availability and inexhaustible
tolerance. This led to fluid chatbot uses, such as Replika as a writing
assistant and ChatGPT as an emotional confidant, despite their distinct
branding. However, we observed challenging tensions in digital companionship
dynamics: participants grappled with bounded personhood, forming deep
attachments while denying chatbots "real" human qualities, and struggled to
reconcile chatbot relationships with social norms. These dynamics raise
questions for the design of digital companions and the rise of hybrid,
general-purpose AI systems.

</details>


### [9] [VoiceMorph: How AI Voice Morphing Reveals the Boundaries of Auditory Self-Recognition](https://arxiv.org/abs/2510.16192)
*Kye Shimizu,Minghan Gao,Ananya Ganesh,Pattie Maes*

Main category: cs.HC

TL;DR: 本研究利用AI声音变形技术探索自我声音识别的边界，发现个体在声音变化达到35.2%时开始失去自我识别能力，年长者的容忍度更高。


<details>
  <summary>Details</summary>
Motivation: 探讨人们何时无法识别自己的声音，了解声音识别的边界和个体差异。

Method: 通过控制参与者声音与匹配目标声音以1%增量进行混合，结合定量和定性方法，测量自我识别评级和反应时间。

Result: 发现关键的识别阈值在35.2%的声音变化，年长参与者更能容忍更高的变化。

Conclusion: 这项研究为语音变形检测中的个体差异提供了基础证据，并有助于思考AI伦理和保护弱势群体。

Abstract: This study investigated auditory self-recognition boundaries using AI voice
morphing technology, examining when individuals cease recognizing their own
voice. Through controlled morphing between participants' voices and
demographically matched targets at 1% increments using a mixed-methods design,
we measured self-identification ratings and response times among 21
participants aged 18-64.
  Results revealed a critical recognition threshold at 35.2% morphing (95% CI
[31.4, 38.1]). Older participants tolerated significantly higher morphing
levels before losing self-recognition ($\beta$ = 0.617, p = 0.048), suggesting
age-related vulnerabilities. Greater acoustic embedding distances predicted
slower decision-making ($r \approx 0.5-0.53, p < 0.05$), with the longest
response times for cloned versions of participants' own voices.
  Qualitative analysis revealed prosodic-based recognition strategies,
universal voice manipulation discomfort, and awareness of applications spanning
assistive technology to security risks. These findings establish foundational
evidence for individual differences in voice morphing detection, with
implications for AI ethics and vulnerable population protection as voice
synthesis becomes accessible.

</details>


### [10] [Case Study of GAI for Generating Novel Images for Real-World Embroidery](https://arxiv.org/abs/2510.16223)
*Kate Glazko,Anika Arugunta,Janelle Chan,Nancy Jimenez-Garcia,Tashfia Sharmin,Jennifer Mankoff*

Main category: cs.HC

TL;DR: 本文探讨了生成性人工智能在刺绣图案设计中的潜在应用，强调了通过自我民族志的方式研究这一技术对创意和包容性的影响。


<details>
  <summary>Details</summary>
Motivation: 希望通过生成性人工智能的应用，改善刺绣图案设计的可访问性，以满足特定的文化和视觉需求。

Method: 通过自我民族志的案例研究，分析生成性人工智能如何作为辅助技术来生成特定的刺绣图案，关注提示工程和可视输出的定制GTP。

Result: 本研究展示了生成性人工智能在设计可刺绣艺术图案中的应用，尤其是为残疾人群体设立的团队如何将此技术作为辅助工具，以更好地满足文化相关性和细节要求。

Conclusion: 研究结果表明，生成性人工智能在刺绣图案生成中的应用效果各异，但其潜力在于促进创意和包容性。未来的工作将致力于进一步提高这些工具的性能和可达性。

Abstract: In this paper, we present a case study exploring the potential use of
Generative Artificial Intelligence (GAI) to address the real-world need of
making the design of embroiderable art patterns more accessible. Through an
auto-ethnographic case study by a disabled-led team, we examine the application
of GAI as an assistive technology in generating embroidery patterns, addressing
the complexity involved in designing culturally-relevant patterns as well as
those that meet specific needs regarding detail and color. We detail the
iterative process of prompt engineering custom GPTs tailored for producing
specific visual outputs, emphasizing the nuances of achieving desirable results
that align with real-world embroidery requirements. Our findings underscore the
mixed outcomes of employing GAI for producing embroiderable images, from
facilitating creativity and inclusion to navigating the unpredictability of
AI-generated designs. Future work aims to refine GAI tools we explored for
generating embroiderable images to make them more performant and accessible,
with the goal of fostering more inclusion in the domains of creativity and
making.

</details>


### [11] [Linking Facial Recognition of Emotions and Socially Shared Regulation in Medical Simulation](https://arxiv.org/abs/2510.16633)
*Xiaoshan Huang,Tianlong Zhong,Haolun Wu,Yeyu Wang,Ethan Churchill,Xue Liu,David Williamson Shaffer*

Main category: cs.HC

TL;DR: 本研究探讨了医学模拟培训中新手与专家学习者的情感与认知参与差异，发现专家与高激活情感相联系，而新手则倾向于较低激活的情感，强调了为新手提供适当的支持的重要性。


<details>
  <summary>Details</summary>
Motivation: 探讨情感与社交学习调控在医学模拟培训中的重要性，理解不同经验水平学习者的情感和认知参与差异。

Method: 采用跨模态分析（TMA）比较新手与专家学习者在虚拟诊断任务中的情感和认知参与模式，结合面部表情和话语数据。

Result: 专家学习者表现出情感高亢（惊讶、愤怒）与社交认知互动的强关联，而新手学习者的情感与社交认知的关联则多为快乐或悲伤，显示他们可能面临分心或认知过载。

Conclusion: 情感调控动态在协作专业发展中扮演重要角色，并建议为新手学习者定制支持，以促进他们的社交认知与情感参与。

Abstract: Computer-supported simulation enables a practical alternative for medical
training purposes. This study investigates the co-occurrence of
facial-recognition-derived emotions and socially shared regulation of learning
(SSRL) interactions in a medical simulation training context. Using transmodal
analysis (TMA), we compare novice and expert learners' affective and cognitive
engagement patterns during collaborative virtual diagnosis tasks. Results
reveal that expert learners exhibit strong associations between socio-cognitive
interactions and high-arousal emotions (surprise, anger), suggesting focused,
effortful engagement. In contrast, novice learners demonstrate stronger links
between socio-cognitive processes and happiness or sadness, with less coherent
SSRL patterns, potentially indicating distraction or cognitive overload.
Transmodal analysis of multimodal data (facial expressions and discourse)
highlights distinct regulatory strategies between groups, offering
methodological and practical insights for computer-supported cooperative work
(CSCW) in medical education. Our findings underscore the role of
emotion-regulation dynamics in collaborative expertise development and suggest
the need for tailored scaffolding to support novice learners' socio-cognitive
and affective engagement.

</details>


### [12] [Safire: Similarity Framework for Visualization Retrieval](https://arxiv.org/abs/2510.16662)
*Huyen N. Nguyen,Nils Gehlenborg*

Main category: cs.HC

TL;DR: 提出了Safire框架，以系统化可视化检索中的相似性，并分析不同的可视化检索系统。


<details>
  <summary>Details</summary>
Motivation: 明确可视化检索中的相似性定义对于有效的可视化检索至关重要，但缺乏系统的方法来理解可视化相似性。

Method: 通过比较标准和表示方式，将可视化相似性划分为主要特征和派生属性，并对现有表示方法进行了分类，分析了多种可视化检索系统。

Result: 提出了类似性框架（Safire），为可视化相似性提供了概念模型，定义了比较标准和表示方式。

Conclusion: 选择表示方式不仅是一个实施细节，还对检索能力和局限性产生重要影响。

Abstract: Effective visualization retrieval necessitates a clear definition of
similarity. Despite the growing body of work in specialized visualization
retrieval systems, a systematic approach to understanding visualization
similarity remains absent. We introduce the Similarity Framework for
Visualization Retrieval (Safire), a conceptual model that frames visualization
similarity along two dimensions: comparison criteria and representation
modalities. Comparison criteria identify the aspects that make visualizations
similar, which we divide into primary facets (data, visual encoding,
interaction, style, metadata) and derived properties (data-centric and
human-centric measures). Safire connects what to compare with how comparisons
are executed through representation modalities. We categorize existing
representation approaches into four groups based on their levels of information
content and visualization determinism: raster image, vector image,
specification, and natural language description, together guiding what is
computable and comparable. We analyze several visualization retrieval systems
using Safire to demonstrate its practical value in clarifying similarity
considerations. Our findings reveal how particular criteria and modalities
align across different use cases. Notably, the choice of representation
modality is not only an implementation detail but also an important decision
that shapes retrieval capabilities and limitations. Based on our analysis, we
provide recommendations and discuss broader implications for multimodal
learning, AI applications, and visualization reproducibility.

</details>


### [13] [Comparing User Behavior in Real vs. Virtual Supermarket Shelves: An Eye-Tracking Study Using Tobii 3 Pro and Meta Quest Pro](https://arxiv.org/abs/2510.16764)
*Francesco Vona,Julia Schorlemmer,Paulina Kaulard,Sebastian Fischer,Jessica Stemann,Jan-Niklas Voigt-Antons*

Main category: cs.HC

TL;DR: 本研究使用眼动跟踪技术比较真实与虚拟超市购物行为，发现两者在注意力分配和产品选择上存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 探讨虚拟环境是否能够真实地复制现实中的消费者行为，尤其是在购物决策方面。

Method: 使用眼动跟踪技术比较真实和虚拟超市货架上的用户行为，样本包括29名参与者，分别在现实和虚拟环境中选择谷物产品。

Result: 眼动数据分析显示，参与者在真实环境中更关注下层货架，尤其是在选择健康产品时；而在虚拟环境中，注意力更多集中在眼平线货架，尤其是对美味商品。

Conclusion: 虚拟环境可以在一定程度上模拟真实超市的购物体验，但在注意力和产品选择策略方面存在显著差异。

Abstract: This study compares user behavior between real and virtual supermarket
shelves using eye tracking technology to assess behavior in both environments.
A sample of 29 participants was randomly assigned to two conditions: a real
world supermarket shelf with Tobii eye tracking and a virtual shelf using the
Meta Quest Pro eye tracker. In both scenarios, participants were asked to
select three packs of cereals belonging to specific categories, healthy or
tasty. The aim was to explore whether virtual environments could realistically
replicate real world experiences, particularly regarding consumer behavior. By
analyzing eye tracking data, the study examined how attention and product
selection strategies varied between real and virtual conditions. Results showed
that participants' attention differed across product types and shopping
environments. Consumers focused more on lower shelves in real settings,
especially when looking for healthy products. In VR, attention shifted to eye
level shelves, particularly for tasty items, aligning with optimal product
placement strategies in supermarkets. Overall, sweet products received less
visual attention across both settings.

</details>


### [14] [Real-Time World Crafting: Generating Structured Game Behaviors from Natural Language with Large Language Models](https://arxiv.org/abs/2510.16952)
*Austin Drake,Hang Dong*

Main category: cs.HC

TL;DR: 提出了一种新架构，使大型语言模型可以安全地与互动游戏引擎集成，从而允许玩家用自然语言编程游戏行为。


<details>
  <summary>Details</summary>
Motivation: 开发能够安全地将大型语言模型整合到交互式游戏引擎中的架构，以允许玩家通过自然语言编程新行为。

Method: 使用大型语言模型将命令转换为受限的领域特定语言，并在运行时配置自定义实体-组件-系统。通过实验评估不同的模型和提示策略。

Result: 通过在2D法术制作游戏原型中评估Gemini、GPT和Claude系列的模型，量化并比较了不同提示策略的效果。

Conclusion: 建立了一种经过验证的LLM-ECS模式，为开发者提供了新兴游戏玩法的可能性和性能比较。

Abstract: We present a novel architecture for safely integrating Large Language Models
(LLMs) into interactive game engines, allowing players to "program" new
behaviors using natural language. Our framework mitigates risks by using an LLM
to translate commands into a constrained Domain-Specific Language (DSL), which
configures a custom Entity-Component-System (ECS) at runtime. We evaluated this
system in a 2D spell-crafting game prototype by experimentally assessing models
from the Gemini, GPT, and Claude families with various prompting strategies. A
validated LLM judge qualitatively rated the outputs, showing that while larger
models better captured creative intent, the optimal prompting strategy is
task-dependent: Chain-of-Thought improved creative alignment, while few-shot
examples were necessary to generate more complex DSL scripts. This work offers
a validated LLM-ECS pattern for emergent gameplay and a quantitative
performance comparison for developers.

</details>


### [15] [Integrating Metaverse Technologies in Medical Education: Examining Acceptance Factors Among Current and Future Healthcare Providers](https://arxiv.org/abs/2510.16984)
*Seckin Damar,Gulsah Hancerliogullari Koksalmis*

Main category: cs.HC

TL;DR: 本研究探讨了土耳其医学生和医生对医疗元宇宙平台的使用意图，结果表明满意度、感知有用性、感知易用性、学习者互动和技术准备对采纳有显著正面影响，而技术焦虑和复杂性则有负面影响。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨医疗元宇宙平台在土耳其的早期采用阶段，特别是在医学生和医生中的使用意图。

Method: 通过整合创新扩散理论、体态社交存在理论、交互等效定理和技术接受模型，创建多理论研究模型，并使用部分最小二乘结构方程建模分析718名参与者的数据。

Result: 模型解释了行为意图的71.8%的方差，显示出强大的解释能力，并提供了对教育者和课程设计者的实际启示。

Conclusion: 结果显示，教育工作者和课程设计者在整合元宇宙平台于医疗培训时应关注学习者的互动和满意度，以促进行为意图的提升。

Abstract: This study investigates behavioral intention to use healthcare metaverse
platforms among medical students and physicians in Turkey, where such
technologies are in early stages of adoption. A multi-theoretical research
model was developed by integrating constructs from the Innovation Diffusion
Theory, Embodied Social Presence Theory, Interaction Equivalency Theorem and
Technology Acceptance Model. Data from 718 participants were analyzed using
partial least squares structural equation modeling. Results show that
satisfaction, perceived usefulness, perceived ease of use, learner
interactions, and technology readiness significantly enhance adoption, while
technology anxiety and complexity have negative effects. Learner learner and
learner teacher interactions strongly predict satisfaction, which subsequently
increases behavioral intention. Perceived ease of use fully mediates the
relationship between technology anxiety and perceived usefulness. However,
technology anxiety does not significantly moderate the effects of perceived
usefulness or ease of use on behavioral intention. The model explains 71.8% of
the variance in behavioral intention, indicating strong explanatory power. The
findings offer practical implications for educators, curriculum designers, and
developers aiming to integrate metaverse platforms into healthcare training in
digitally transitioning educational systems.

</details>


### [16] [Planar or Spatial: Exploring Design Aspects and Challenges for Presentations in Virtual Reality with No-coding Interface](https://arxiv.org/abs/2510.17073)
*Liwei Wu,Yilin Zhang,Justin Leung,Jingyi Gao,April Li,Jian Zhao*

Main category: cs.HC

TL;DR: 本研究探讨了虚拟现实（VR）演示的潜力，开发了无编码工具VRStory，尽管用户喜欢VR中的沉浸感，但仍对传统2D格式有偏好。


<details>
  <summary>Details</summary>
Motivation: 探讨虚拟现实（VR）在演示中的潜力并分析用户意见，以解决用户在创建引人入胜的VR演示时面临的挑战。

Method: 通过调研流行的演示软件和对七位专业人士的访谈，识别出五个设计方面和四个设计挑战，最终开发出VRStory原型并进行用户研究。

Result: 开发了一个无需编码的演示创作工具VRStory，并进行用户研究以评估其使用体验。

Conclusion: 强调在未来VR演示工具的发展中，需平衡沉浸特性与可访问性。

Abstract: The proliferation of virtual reality (VR) has led to its increasing adoption
as an immersive medium for delivering presentations, distinct from other VR
experiences like games and 360-degree videos by sharing information in richly
interactive environments. However, creating engaging VR presentations remains a
challenging and time-consuming task for users, hindering the full realization
of VR presentation's capabilities. This research aims to explore the potential
of VR presentation, analyze users' opinions, and investigate these via
providing a user-friendly no-coding authoring tool. Through an examination of
popular presentation software and interviews with seven professionals, we
identified five design aspects and four design challenges for VR presentations.
Based on the findings, we developed VRStory, a prototype for presentation
authoring without coding to explore the design aspects and strategies for
addressing the challenges. VRStory offers a variety of predefined and
customizable VR elements, as well as modules for layout design, navigation
control, and asset generation. A user study was then conducted with 12
participants to investigate their opinions and authoring experience with
VRStory. Our results demonstrated that, while acknowledging the advantages of
immersive and spatial features in VR, users often have a consistent mental
model for traditional 2D presentations and may still prefer planar and static
formats in VR for better accessibility and efficient communication. We finally
shared our learned design considerations for future development of VR
presentation tools, emphasizing the importance of balancing of promoting
immersive features and ensuring accessibility.

</details>


### [17] [Toward a Cognitive-Affective-Systemic Framework for Art and Sustainability](https://arxiv.org/abs/2510.17083)
*Ivan C. H. Liu*

Main category: cs.HC

TL;DR: 本文提出了一种结合认知、情感与系统思维的框架，通过艺术提升可持续性意识，并通过具体作品展示其应用。


<details>
  <summary>Details</summary>
Motivation: 旨在通过艺术形式提高对生态美学、情感理论、复杂性科学和后人类伦理的理解，从而增强人们的可持续性意识。

Method: 提出了一种认知-情感-系统（CAS）框架，结合了认知、情感和系统理解，通过艺术培养可持续性意识。

Result: 两件艺术作品（SPill和Echoes of the Land）展示了如何通过系统建模和感官沉浸将复杂科学转化为身体化的生态理解。

Conclusion: 该框架为艺术家、理论家和活动家提供了将意识转化为参与的研究基础，促进朝向可持续未来的集体创造力。

Abstract: This paper proposes a ognitive-Affective-Systemic (CAS) framework that
integrates cognition, emotion, and systemic understanding to cultivate
sustainability awareness through art. Drawing from eco-aesthetics, affect
theory, complexity science, and posthuman ethics, the framework defines
artistic practice as both epistemic and performative--a way of knowing through
making and feeling. Central to this is logomotion, an aesthetic mode where
comprehension and emotion move together as a unified experience. Two artworks,
SPill, visualizing antimicrobial resistance through avalanche dynamics, and
Echoes of the Land, modeling anthropogenic seismicity, demonstrate how systemic
modeling and sensory immersion transform complex science into embodied
ecological understanding. The framework offers a methodological foundation for
artists, theorists, and activists to translate awareness into engagement,
advancing collective creativity toward sustainable futures.

</details>


### [18] [Kinesthetic Weight Modulation: The Effects of Whole-Arm Tendon Vibration on the Perceived Heaviness](https://arxiv.org/abs/2510.17102)
*Keigo Ushiyama,Hiroyuki Kajimoto*

Main category: cs.HC

TL;DR: 本研究探讨多点肌腱振动对感知重量的影响，发现其显著增加感知的重量并可调整，但未显著减少感知重量。


<details>
  <summary>Details</summary>
Motivation: 探索肌腱振动如何调节对物体重量的感知，以丰富虚拟物体的触觉互动。

Method: 通过实验研究多点肌腱振动对感知重量的影响，进行两项实验。

Result: 一项实验表明，肌腱振动显著增加了感知的重量，另一项实验则发现这种增加可以在多个水平上进行系统调控。

Conclusion: 肌腱振动显著增加了感知的重量，并且可以在350-450克的范围内进行调整，但没有显著减少感知的重量。

Abstract: Kinesthetic illusions, which arise when muscle spindles are activated by
vibration, provide a compact means of presenting kinesthetic sensations.
Because muscle spindles contribute not only to sensing body movement but also
to perceiving heaviness, vibration-induced illusions could potentially modulate
weight perception. While prior studies have primarily focused on conveying
virtual movement, the modulation of perceived heaviness has received little
attention. Presenting a sense of heaviness is essential for enriching haptic
interactions with virtual objects. This study investigates whether multi-point
tendon vibration can increase or decrease perceived heaviness (Experiment 1)
and how the magnitude of the effect can be systematically controlled
(Experiment 2). The results show that tendon vibration significantly increases
perceived heaviness but does not significantly decrease it, although a
decreasing trend was observed. Moreover, the increase can be adjusted across at
least three levels within the range of 350-450 g. Finally, we discuss plausible
mechanisms underlying this vibration-induced modulation of weight perception.

</details>


### [19] [Design Framework for Conversational Agent in Couple relationships: A Systematic Review](https://arxiv.org/abs/2510.17119)
*Soyoung Jung,Sung Park*

Main category: cs.HC

TL;DR: 研究了伴侣关系中的对话代理设计，提出了八个设计原则，以支持心理健康干预。


<details>
  <summary>Details</summary>
Motivation: 探索心理健康对话代理（CAs）在支持面临关系挑战的伴侣中的潜力。

Method: 系统评审

Result: 提出八个设计考虑因素，为面向伴侣的CAs提供指导。

Conclusion: 本研究提出了一个设计框架，将关系理论与先进的AI技术结合，为未来基于伴侣的心理健康干预的发展提供参考。

Abstract: The development of conversational agents (CAs) has shown strong potential in
supporting mental health through dialogue. While many studies focus on CAs for
individual psychological care, research on agents designed for couples facing
relational or emotional challenges remains limited. This study aims to identify
design considerations for CAs that address the relational context of couples
and support their well-being. Following PRISMA guidelines, a systematic review
was conducted across seven databases: CINAHL, Embase, PubMed, PsycINFO, Scopus,
Web of Science, and the ACM Digital Library. Peer-reviewed empirical studies
were screened, duplicates removed, and selection criteria applied, resulting in
twelve studies for analysis. Thematic analysis was conducted across three
dimensions: AI interaction design, relational framing, and technical
limitations. Three key themes emerged: (1) the need for a relational expert
persona, (2) technological directions leveraging state-of-the-art AI for
relational specificity and emotional competence, and (3) a shift from
content-centered to relationship-centered design. Based on these insights,
eight design considerations are proposed for couple-oriented CAs: (1) agent
persona, (2) individual mode, (3) concurrent mode, (4) conjoint mode, (5)
ethics, (6) data and privacy, (7) interaction pattern, and (8) safety
mechanism. These principles guide CAs as relational mediators capable of
maintaining multiple alliances, respecting cultural and ethical boundaries, and
ensuring fairness and emotional safety between partners. Ultimately, this
review introduces a design framework that integrates relational theory with
advanced AI technologies to inform future development of CAs for couple-based
mental health interventions.

</details>


### [20] [Augmented Web Usage Mining and User Experience Optimization with CAWAL's Enriched Analytics Data](https://arxiv.org/abs/2510.17253)
*Özkan Canay,{Ü}mit Kocabıcak*

Main category: cs.HC

TL;DR: 本研究介绍了一种名为AWUM的方法，通过增强互动数据，改进网页使用挖掘并优化用户体验。


<details>
  <summary>Details</summary>
Motivation: 随着网络用户体验（UX）日益重要，研究用户行为的需求不断增加，AWUM旨在提高网页使用挖掘的有效性。

Method: AWUM方法通过分析会话结构、页面请求、服务互动和退出方式，利用数据挖掘技术处理超过120万条会话记录。

Result: 该研究提出的增强网页使用挖掘（AWUM）方法学，通过丰富CAWAL框架提供的互动数据，用于优化用户体验（UX）。

Conclusion: AWUM能深入理解用户行为，并在大规模UX优化中展现强大潜力。

Abstract: Understanding user behavior on the web is increasingly critical for
optimizing user experience (UX). This study introduces Augmented Web Usage
Mining (AWUM), a methodology designed to enhance web usage mining and improve
UX by enriching the interaction data provided by CAWAL (Combined Application
Log and Web Analytics), a framework for advanced web analytics. Over 1.2
million session records collected in one month (~8.5GB of data) were processed
and transformed into enriched datasets. AWUM analyzes session structures, page
requests, service interactions, and exit methods. Results show that 87.16% of
sessions involved multiple pages, contributing 98.05% of total pageviews; 40%
of users accessed various services and 50% opted for secure exits. Association
rule mining revealed patterns of frequently accessed services, highlighting
CAWAL's precision and efficiency over conventional methods. AWUM offers a
comprehensive understanding of user behavior and strong potential for
large-scale UX optimization.

</details>


### [21] [SmartSustain Recommender System: Navigating Sustainability Trade-offs in Personalized City Trip Planning](https://arxiv.org/abs/2510.17355)
*Ashmi Banerjee,Melih Mert Aksoy,Wolfgang Wörndl*

Main category: cs.HC

TL;DR: 推出SmartSustain推荐系统，帮助用户做出更可持续的旅行选择，平衡环境影响与个人利益。


<details>
  <summary>Details</summary>
Motivation: 旅游行业对全球碳排放的贡献以及对可持续旅游的迫切需求。

Method: 使用互动城市卡、动态横幅和实时环境反馈机制来引导用户决策。

Result: 初步用户研究结果表明，该系统在人机交互和有效性方面获得了积极反馈。

Conclusion: SmartSustain推荐系统通过个性化和互动方式，提升了可持续性旅游决策的可用性和效果。

Abstract: Tourism is a major contributor to global carbon emissions and over-tourism,
creating an urgent need for recommender systems that not only inform but also
gently steer users toward more sustainable travel decisions. Such choices,
however, often require balancing complex trade-offs between environmental
impact, cost, convenience, and personal interests. To address this, we present
the SmartSustain Recommender, a web application designed to nudge users toward
eco-friendlier options through an interactive, user-centric interface. The
system visualizes the broader consequences of travel decisions by combining
CO2e emissions, destination popularity, and seasonality with personalized
interest matching. It employs mechanisms such as interactive city cards for
quick comparisons, dynamic banners that surface sustainable alternatives in
specific trade-off scenarios, and real-time impact feedback using animated
environmental indicators. A preliminary user study with 21 participants
indicated strong usability and perceived effectiveness. The system is
accessible at https://smartsustainrecommender.web.app.

</details>


### [22] [NieNie: Adaptive Rhythmic System for Stress Relief with LLM-Based Guidance](https://arxiv.org/abs/2510.17534)
*Yichen Yu,Qiaoran Wang*

Main category: cs.HC

TL;DR: NieNie是一个针对年轻人设计的压力管理系统，通过生物反馈和实时心理指导，提供沉浸式的压力调节体验。


<details>
  <summary>Details</summary>
Motivation: 应对年轻人日益增加的心理压力，克服传统压力管理工具的局限性。

Method: 结合节奏生物反馈与实时心理指导，采用大语言模型（LLM）进行个性化节奏游戏的反馈。

Result: 该系统通过收集生理信号并生成适应性节奏，提供及时反馈，增强压力重构效果。

Conclusion: NieNie展示了通过身体交互系统将生理动作与心理健康连接的有效性，为年轻人提供了沉浸式的压力调节体验。

Abstract: Today's young people are facing increasing psychological stress due to
various social issues. Traditional stress management tools often rely on static
scripts or passive content, which are ineffective in alleviating stress. NieNie
addresses this gap by combining rhythm biofeedback with real-time psychological
guidance through a large language model (LLM), offering an interactive, tactile
response. The system is specifically designed for young people experiencing
emotional stress, collecting physiological signals such as heart rate
variability and generating adaptive squeeze-release rhythms via soft, tactile
devices. Utilising LLM, the system provides timely squeezing rhythms and
psychologically guided feedback prompts, offering personalised rhythm games
while reinforcing stress restructuring. Unlike traditional mental health apps,
NieNie places users within an embodied interactive loop, leveraging tactile
interaction, biofeedback, and adaptive language support to create an immersive
stress regulation experience. This study demonstrates how embodied systems can
connect bodily actions with mental health in everyday contexts.

</details>


### [23] [DeTAILS: Deep Thematic Analysis with Iterative LLM Support](https://arxiv.org/abs/2510.17575)
*Ash Sharma,Karen Cochrane,James R. Wallace*

Main category: cs.HC

TL;DR: DeTAILS工具通过 LLM 支持主题分析，提高定性研究效率，研究表明其有效性与研究人员的积极体验。


<details>
  <summary>Details</summary>
Motivation: 定性研究中主题分析的迭代和解释性要求使其难以扩展，因此需要开发新的工具来简化这一过程。

Method: 本研究通过与18位定性研究者合作，利用实际数据检验DeTAILS工具的效果，结合定量和定性分析评估其性能。

Result: DeTAILS工具包集成了大语言模型(LLM)的支持，使定性分析变得更高效，实验结果表明其与研究人员的效率和评级一致，同时减轻了工作负担。

Conclusion: DeTAILS工具有效促进了定性分析的进行，提高了可靠性，并对未来的AI辅助定性研究设计有重要影响。

Abstract: Thematic analysis is widely used in qualitative research but can be difficult
to scale because of its iterative, interpretive demands. We introduce DeTAILS,
a toolkit that integrates large language model (LLM) assistance into a workflow
inspired by Braun and Clarke's thematic analysis framework. DeTAILS supports
researchers in generating and refining codes, reviewing clusters, and
synthesizing themes through interactive feedback loops designed to preserve
analytic agency. We evaluated the system with 18 qualitative researchers
analyzing Reddit data. Quantitative results showed strong alignment between
LLM-supported outputs and participants' refinements, alongside reduced workload
and high perceived usefulness. Qualitatively, participants reported that
DeTAILS accelerated analysis, prompted reflexive engagement with AI outputs,
and fostered trust through transparency and control. We contribute: (1) an
interactive human-LLM workflow for large-scale qualitative analysis, (2)
empirical evidence of its feasibility and researcher experience, and (3) design
implications for trustworthy AI-assisted qualitative research.

</details>


### [24] [Conveying Meaning through Gestures: An Investigation into Semantic Co-Speech Gesture Generation](https://arxiv.org/abs/2510.17599)
*Hendric Voss,Lisa Michelle Bohnenkamp,Stefan Kopp*

Main category: cs.HC

TL;DR: 本研究比较了两种手势生成框架，结果显示显式语义增强不一定带来更好的手势生成，且其效果受到上下文的影响，揭示了特化与推广的平衡。


<details>
  <summary>Details</summary>
Motivation: 探讨两种手势生成框架在传达意义和人类感知结果移动方面的有效性。

Method: 使用来自SAGA空间通信语料库的句子进行用户中心的评估，以概念识别和人类相似度为标准，比较AQ-GT框架及其增强版本AQ-GT-a的表现。

Result: 原始AQ-GT框架在其训练领域内传达概念的效果意外更佳，而AQ-GT-a框架在新语境中更擅长表示形状和大小。参与者认为AQ-GT-a的手势更具表现性和帮助性，但不认为它们更具人类特征。

Conclusion: 显式语义增强并不一定保证手势生成的改善，其有效性高度依赖于上下文，显示了特化与推广之间的潜在权衡。

Abstract: This study explores two frameworks for co-speech gesture generation, AQ-GT
and its semantically-augmented variant AQ-GT-a, to evaluate their ability to
convey meaning through gestures and how humans perceive the resulting
movements. Using sentences from the SAGA spatial communication corpus,
contextually similar sentences, and novel movement-focused sentences, we
conducted a user-centered evaluation of concept recognition and human-likeness.
Results revealed a nuanced relationship between semantic annotations and
performance. The original AQ-GT framework, lacking explicit semantic input, was
surprisingly more effective at conveying concepts within its training domain.
Conversely, the AQ-GT-a framework demonstrated better generalization,
particularly for representing shape and size in novel contexts. While
participants rated gestures from AQ-GT-a as more expressive and helpful, they
did not perceive them as more human-like. These findings suggest that explicit
semantic enrichment does not guarantee improved gesture generation and that its
effectiveness is highly dependent on the context, indicating a potential
trade-off between specialization and generalization.

</details>


### [25] [ImaGGen: Zero-Shot Generation of Co-Speech Semantic Gestures Grounded in Language and Image Input](https://arxiv.org/abs/2510.17617)
*Hendric Voss,Stefan Kopp*

Main category: cs.HC

TL;DR: 本文介绍了一种通过图像分析生成与语言输入一致的手势的方法，解决了现有方法仅能生成简单重复的打击手势的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的手势生成方法仅限于简单的打击手势，未能有效传达语义，因此需要一种新的方法来生成具有语义 coherence 的手势。

Method: 我们的方法结合图像分析和语义匹配模块，通过逆运动学引擎生成具有语义一致性的手势。

Result: 本研究提出了一种生成与语言输入相符的非语言姿态，克服了现有方法的局限性。

Conclusion: 本研究强调语境意识语义手势在表达性和合作性虚拟代理或化身中的重要性，展示了改善人类与代理互动的新可能性。

Abstract: Human communication combines speech with expressive nonverbal cues such as
hand gestures that serve manifold communicative functions. Yet, current
generative gesture generation approaches are restricted to simple, repetitive
beat gestures that accompany the rhythm of speaking but do not contribute to
communicating semantic meaning. This paper tackles a core challenge in
co-speech gesture synthesis: generating iconic or deictic gestures that are
semantically coherent with a verbal utterance. Such gestures cannot be derived
from language input alone, which inherently lacks the visual meaning that is
often carried autonomously by gestures. We therefore introduce a zero-shot
system that generates gestures from a given language input and additionally is
informed by imagistic input, without manual annotation or human intervention.
Our method integrates an image analysis pipeline that extracts key object
properties such as shape, symmetry, and alignment, together with a semantic
matching module that links these visual details to spoken text. An inverse
kinematics engine then synthesizes iconic and deictic gestures and combines
them with co-generated natural beat gestures for coherent multimodal
communication. A comprehensive user study demonstrates the effectiveness of our
approach. In scenarios where speech alone was ambiguous, gestures generated by
our system significantly improved participants' ability to identify object
properties, confirming their interpretability and communicative value. While
challenges remain in representing complex shapes, our results highlight the
importance of context-aware semantic gestures for creating expressive and
collaborative virtual agents or avatars, marking a substantial step forward
towards efficient and robust, embodied human-agent interaction. More
information and example videos are available here:
https://review-anon-io.github.io/ImaGGen.github.io/

</details>


### [26] [Muscle Anatomy-aware Geometric Deep Learning for sEMG-based Gesture Decoding](https://arxiv.org/abs/2510.17660)
*Adyasha Dash,Giulia Zappoli,Laya Das,Robert Riener*

Main category: cs.HC

TL;DR: 该研究提出了一种新型的几何深度学习模型，通过在SPD流形上学习和无监督领域适应性，有效提高了sEMG手势解码的准确性和通用性，解决了现有算法的个体和会话可变性问题。


<details>
  <summary>Details</summary>
Motivation: 准确解码非侵入性表面肌电图（sEMG）中的手势对于空间计算、医疗保健和娱乐等多种应用至关重要，然而现有算法在不同个体和会话间存在较高的可变性。

Method: 提出了一种几何深度学习模型，该模型在对称正定（SPD）流形上学习，并利用无监督领域适应性来降低模型对个体和实验会话的敏感性。

Result: 在公开的基准手势解码数据集（Ninapro DB6，Flexwear-HD）上进行的实验表明，该模型在会话间的通用性优于欧几里得和其他基于SPD的模型，准确率分别提高了8.83和4.63分。

Conclusion: 该方法推动了基于sEMG的手势识别的最新进展，并为肌肉信号的流形学习开辟了新的研究方向。

Abstract: Robust and accurate decoding of gesture from non-invasive surface
electromyography (sEMG) is important for various applications including spatial
computing, healthcare, and entertainment, and has been actively pursued by
researchers and industry. Majority of sEMG-based gesture decoding algorithms
employ deep neural networks that are designed for Euclidean data, and may not
be suitable for analyzing multi-dimensional, non-stationary time-series with
long-range dependencies such as sEMG. State-of-the-art sEMG-based decoding
methods also demonstrate high variability across subjects and sessions,
requiring re-calibration and adaptive fine-tuning to boost performance. To
address these shortcomings, this work proposes a geometric deep learning model
that learns on symmetric positive definite (SPD) manifolds and leverages
unsupervised domain adaptation to desensitize the model to subjects and
sessions. The model captures the features in time and across sensors with
multiple kernels, projects the features onto SPD manifold, learns on manifolds
and projects back to Euclidean space for classification. It uses a
domain-specific batch normalization layer to address variability between
sessions, alleviating the need for re-calibration or fine-tuning. Experiments
with publicly available benchmark gesture decoding datasets (Ninapro DB6,
Flexwear-HD) demonstrate the superior generalizability of the model compared to
Euclidean and other SPD-based models in the inter-session scenario, with up to
8.83 and 4.63 points improvement in accuracy, respectively. Detailed analyses
reveal that the model extracts muscle-specific information for different tasks
and ablation studies highlight the importance of modules introduced in the
work. The proposed method pushes the state-of-the-art in sEMG-based gesture
recognition and opens new research avenues for manifold-based learning for
muscle signals.

</details>


### [27] [Rethinking Search: A Study of University Students' Perspectives on Using LLMs and Traditional Search Engines in Academic Problem Solving](https://arxiv.org/abs/2510.17726)
*Md. Faiyaz Abdullah Sayeedi,Md. Sadman Haque,Zobaer Ibn Razzaque,Robiul Awoul Robin,Sabila Nawshin*

Main category: cs.HC

TL;DR: 本研究探讨了大学生对Google和大型语言模型的看法，发现学生在两者间切换使用，提出开发结合两者优势的聊天机器人以提升学术研究效率。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在学术问题解决中的日益集成，了解大学生对传统搜索引擎和大型语言模型的使用感受至关重要。

Method: 采用混合方法，对109名来自不同学科的学生进行调查，并与12名参与者进行了深入访谈，使用ANOVA和卡方测试分析效率、满意度和工具偏好的差异。

Result: 学生们常常在Google和GPT之间切换，使用Google获取可信的、多来源的信息，使用GPT进行总结、解释和草拟，显示出对混合解决方案的强烈需求。

Conclusion: 开发了一种结合GPT和Google的原型聊天机器人，以提高学术研究的效果并减轻认知负担。

Abstract: With the increasing integration of Artificial Intelligence (AI) in academic
problem solving, university students frequently alternate between traditional
search engines like Google and large language models (LLMs) for information
retrieval. This study explores students' perceptions of both tools, emphasizing
usability, efficiency, and their integration into academic workflows. Employing
a mixed-methods approach, we surveyed 109 students from diverse disciplines and
conducted in-depth interviews with 12 participants. Quantitative analyses,
including ANOVA and chi-square tests, were used to assess differences in
efficiency, satisfaction, and tool preference. Qualitative insights revealed
that students commonly switch between GPT and Google: using Google for
credible, multi-source information and GPT for summarization, explanation, and
drafting. While neither tool proved sufficient on its own, there was a strong
demand for a hybrid solution. In response, we developed a prototype, a chatbot
embedded within the search interface, that combines GPT's conversational
capabilities with Google's reliability to enhance academic research and reduce
cognitive load.

</details>


### [28] [Human-AI Interactions: Cognitive, Behavioral, and Emotional Impacts](https://arxiv.org/abs/2510.17753)
*Celeste Riley,Omar Al-Refai,Yadira Colunga Reyes,Eman Hammad*

Main category: cs.HC

TL;DR: 本论文探讨了人机互动中的心理三角（认知、行为、情感），强调AI对人类认知的积极和消极影响，并呼吁负责任的AI设计。


<details>
  <summary>Details</summary>
Motivation: 面对人机互动日益显著的挑战，亟需理解AI对人类认知与情感的影响。

Method: 通过心理三角的视角，对人机互动带来的风险与益处进行文献综述。

Result: AI能提升记忆、创造力和参与度，但也可能导致批判性思维减弱和技能退化等风险；情感结果复杂，有支持和减少压力的可能，但也有依赖和伦理问题。

Conclusion: 需加强对AI设计的责任感与情境意识，同时推进纵向研究和评估框架，以平衡收益与人本风险。

Abstract: As stories of human-AI interactions continue to be highlighted in the news
and research platforms, the challenges are becoming more pronounced, including
potential risks of overreliance, cognitive offloading, social and emotional
manipulation, and the nuanced degradation of human agency and judgment. This
paper surveys recent research on these issues through the lens of the
psychological triad: cognition, behavior, and emotion. Observations seem to
suggest that while AI can substantially enhance memory, creativity, and
engagement, it also introduces risks such as diminished critical thinking,
skill erosion, and increased anxiety. Emotional outcomes are similarly mixed,
with AI systems showing promise for support and stress reduction, but raising
concerns about dependency, inappropriate attachments, and ethical oversight.
This paper aims to underscore the need for responsible and context-aware AI
design, highlighting gaps for longitudinal research and grounded evaluation
frameworks to balance benefits with emerging human-centric risks.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [29] [VAR-SLAM: Visual Adaptive and Robust SLAM for Dynamic Environments](https://arxiv.org/abs/2510.16205)
*João Carlos Virgolino Soares,Gabriel Fischer Abati,Claudio Semini*

Main category: cs.RO

TL;DR: VAR-SLAM通过轻量语义过滤和自适应鲁棒损失，提升了动态环境中SLAM的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中，现有的视觉SLAM方法难以处理未知运动物体，导致准确性降低。

Method: VAR-SLAM是一种基于ORB-SLAM3的系统，结合了轻量级的语义关键点过滤器和Barron's自适应鲁棒损失。

Result: VAR-SLAM在TUM RGB-D、Bonn RGB-D Dynamic和OpenLORIS数据集上显示出比现有最先进的基线提升了轨迹准确性和鲁棒性，最高实现了比NGD-SLAM低25%的ATE RMSE，同时保持了平均27 FPS的性能。

Conclusion: VAR-SLAM能有效处理已知和未知的动态物体，提高了SLAM系统在复杂场景中的表现。

Abstract: Visual SLAM in dynamic environments remains challenging, as several existing
methods rely on semantic filtering that only handles known object classes, or
use fixed robust kernels that cannot adapt to unknown moving objects, leading
to degraded accuracy when they appear in the scene. We present VAR-SLAM (Visual
Adaptive and Robust SLAM), an ORB-SLAM3-based system that combines a
lightweight semantic keypoint filter to deal with known moving objects, with
Barron's adaptive robust loss to handle unknown ones. The shape parameter of
the robust kernel is estimated online from residuals, allowing the system to
automatically adjust between Gaussian and heavy-tailed behavior. We evaluate
VAR-SLAM on the TUM RGB-D, Bonn RGB-D Dynamic, and OpenLORIS datasets, which
include both known and unknown moving objects. Results show improved trajectory
accuracy and robustness over state-of-the-art baselines, achieving up to 25%
lower ATE RMSE than NGD-SLAM on challenging sequences, while maintaining
performance at 27 FPS on average.

</details>


### [30] [DeGrip: A Compact Cable-driven Robotic Gripper for Desktop Disassembly](https://arxiv.org/abs/2510.16231)
*Bihao Zhang,Davood Soleymanzadeh,Xiao Liang,Minghui Zheng*

Main category: cs.RO

TL;DR: 本文介绍了一种名为DeGrip的定制抓手，旨在解决EOL产品拆解中的有效性问题，评估结果显示其在狭小空间内拆解能力强。


<details>
  <summary>Details</summary>
Motivation: 智能机器人拆解EOL产品一直是机器人领域的挑战，现有机器学习技术受限于硬件。

Method: 开发了名为DeGrip的定制抓手，具有三自由度，并在Isaac Sim中评估其在EOL桌面拆解环境中的表现。

Result: 评估结果确认DeGrip在复杂配置下和狭小空间中进行EOL桌面拆解的能力。

Conclusion: DeGrip展示了在狭小空间内进行EOL桌面产品拆解的有效性。

Abstract: Intelligent robotic disassembly of end-of-life (EOL) products has been a
long-standing challenge in robotics. While machine learning techniques have
shown promise, the lack of specialized hardware limits their application in
real-world scenarios. We introduce DeGrip, a customized gripper designed for
the disassembly of EOL computer desktops. DeGrip provides three degrees of
freedom (DOF), enabling arbitrary configurations within the disassembly
environment when mounted on a robotic manipulator. It employs a cable-driven
transmission mechanism that reduces its overall size and enables operation in
confined spaces. The wrist is designed to decouple the actuation of wrist and
jaw joints. We also developed an EOL desktop disassembly environment in Isaac
Sim to evaluate the effectiveness of DeGrip. The tasks were designed to
demonstrate its ability to operate in confined spaces and disassemble
components in arbitrary configurations. The evaluation results confirm the
capability of DeGrip for EOL desktop disassembly.

</details>


### [31] [Cosmos-Surg-dVRK: World Foundation Model-based Automated Online Evaluation of Surgical Robot Policy Learning](https://arxiv.org/abs/2510.16240)
*Lukas Zbinden,Nigel Nelson,Juo-Tung Chen,Xinhao Chen,Ji Woong,Kim,Mahdi Azizian,Axel Krieger,Sean Huver*

Main category: cs.RO

TL;DR: 本研究介绍了Cosmos-Surg-dVRK，一个基于世界基础模型的手术微调平台，能够自动评估和基准测试手术策略，与真实平台的结果高度相关，显示其在复杂手术程序中的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着手术机器人和视觉-语言-动作模型的发展，自主手术策略的制定和高效评估变得迫切。然而，直接在物理机器人平台上进行评估仍面临高成本、时间需求和执行变异等挑战。

Method: 通过Cosmos WFM对手术任务进行模拟，结合视频分类器进行自动评估，并在真实的机器人平台上进行对比实验。

Result: 开发了Cosmos-Surg-dVRK，一个经过微调的世界基础模型，结合训练过的视频分类器，从而实现了手术策略的完全自动化在线评估和基准测试。

Conclusion: Cosmos-Surg-dVRK在两个不同的外科数据集上的应用表明，其在线评估和真实结果之间有很强的相关性，显示出该平台在评估复杂外科程序中的潜力。

Abstract: The rise of surgical robots and vision-language-action models has accelerated
the development of autonomous surgical policies and efficient assessment
strategies. However, evaluating these policies directly on physical robotic
platforms such as the da Vinci Research Kit (dVRK) remains hindered by high
costs, time demands, reproducibility challenges, and variability in execution.
World foundation models (WFM) for physical AI offer a transformative approach
to simulate complex real-world surgical tasks, such as soft tissue deformation,
with high fidelity. This work introduces Cosmos-Surg-dVRK, a surgical finetune
of the Cosmos WFM, which, together with a trained video classifier, enables
fully automated online evaluation and benchmarking of surgical policies. We
evaluate Cosmos-Surg-dVRK using two distinct surgical datasets. On tabletop
suture pad tasks, the automated pipeline achieves strong correlation between
online rollouts in Cosmos-Surg-dVRK and policy outcomes on the real dVRK Si
platform, as well as good agreement between human labelers and the V-JEPA
2-derived video classifier. Additionally, preliminary experiments with ex-vivo
porcine cholecystectomy tasks in Cosmos-Surg-dVRK demonstrate promising
alignment with real-world evaluations, highlighting the platform's potential
for more complex surgical procedures.

</details>


### [32] [NEBULA: Do We Evaluate Vision-Language-Action Agents Correctly?](https://arxiv.org/abs/2510.16263)
*Jierui Peng,Yanyan Zhang,Yicheng Duan,Tuo Liang,Vipin Chaudhary,Yu Yin*

Main category: cs.RO

TL;DR: 本文介绍了NEBULA，一个用于评估VLA代理的新生态系统，通过双轴评估协议提升技能诊断与鲁棒性测量的精确性。


<details>
  <summary>Details</summary>
Motivation: 当前的VLA代理评估依赖粗略的任务成功指标，无法有效诊断技能并衡量鲁棒性，迫切需要更精细的评估工具来促进研究和模型发展。

Method: 采用了一种双轴评估协议，结合细粒度能力测试和系统压力测试，提供了标准化API和大规模数据集。

Result: 本文提出了NEBULA，一个用于单臂操作的统一生态系统，以解决现有视觉-语言-动作（VLA）代理评估中的不足。通过NEBULA，研究人员能够进行细致的技能诊断和测量智能体在现实世界扰动下的鲁棒性。NEBULA结合了细粒度能力测试和系统压力测试，构建了一种新颖的双轴评估协议，并提供标准化的API以及大规模的聚合数据集，从而减少数据碎片化并支持跨数据集的训练和公平比较。

Conclusion: NEBULA为鲁棒且通用的具身智能体提供了实用的基础，揭示了VLA在空间推理和动态适应等关键能力上的不足。

Abstract: The evaluation of Vision-Language-Action (VLA) agents is hindered by the
coarse, end-task success metric that fails to provide precise skill diagnosis
or measure robustness to real-world perturbations. This challenge is
exacerbated by a fragmented data landscape that impedes reproducible research
and the development of generalist models. To address these limitations, we
introduce \textbf{NEBULA}, a unified ecosystem for single-arm manipulation that
enables diagnostic and reproducible evaluation. NEBULA features a novel
dual-axis evaluation protocol that combines fine-grained \textit{capability
tests} for precise skill diagnosis with systematic \textit{stress tests} that
measure robustness. A standardized API and a large-scale, aggregated dataset
are provided to reduce fragmentation and support cross-dataset training and
fair comparison. Using NEBULA, we demonstrate that top-performing VLAs struggle
with key capabilities such as spatial reasoning and dynamic adaptation, which
are consistently obscured by conventional end-task success metrics. By
measuring both what an agent can do and when it does so reliably, NEBULA
provides a practical foundation for robust, general-purpose embodied agents.

</details>


### [33] [Do What You Say: Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verification](https://arxiv.org/abs/2510.16281)
*Yilin Wu,Anqi Li,Tucker Hermans,Fabio Ramos,Andrea Bajcsy,Claudia P'erez-D'Arpino*

Main category: cs.RO

TL;DR: 提出了一种新方法提高机器人指令执行的鲁棒性，解决了文本计划与实际动作之间的不一致问题，并显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决VLA模型在生成低级动作时，即使有正确的文本计划，仍可能无法达到预期结果的问题。

Method: 通过对中间文本计划进行采样，预测多个候选动作序列的结果，并使用预训练的视觉语言模型选择与文本计划最一致的序列。

Result: 通过将自然动作多样性转化为优势，提升对OOD扰动的稳健性，并在行为组合任务上实现15%的性能提升。

Conclusion: 引入了一种训练-free的运行时策略引导方法，提高了VLA模型在动作生成中的可信度和稳健性，实现了15%的性能提升。

Abstract: Reasoning Vision Language Action (VLA) models improve robotic
instruction-following by generating step-by-step textual plans before low-level
actions, an approach inspired by Chain-of-Thought (CoT) reasoning in language
models. Yet even with a correct textual plan, the generated actions can still
miss the intended outcomes in the plan, especially in out-of-distribution (OOD)
scenarios. We formalize this phenomenon as a lack of embodied CoT faithfulness,
and introduce a training-free, runtime policy steering method for
reasoning-action alignment. Given a reasoning VLA's intermediate textual plan,
our framework samples multiple candidate action sequences from the same model,
predicts their outcomes via simulation, and uses a pre-trained Vision-Language
Model (VLM) to select the sequence whose outcome best aligns with the VLA's own
textual plan. Only executing action sequences that align with the textual
reasoning turns our base VLA's natural action diversity from a source of error
into a strength, boosting robustness to semantic and visual OOD perturbations
and enabling novel behavior composition without costly re-training. We also
contribute a reasoning-annotated extension of LIBERO-100, environment
variations tailored for OOD evaluation, and demonstrate up to 15% performance
gain over prior work on behavior composition tasks and scales with compute and
data diversity. Project Website at:
https://yilin-wu98.github.io/steering-reasoning-vla/

</details>


### [34] [SPOT: Sensing-augmented Trajectory Planning via Obstacle Threat Modeling](https://arxiv.org/abs/2510.16308)
*Chi Zhang,Xian Huang,Wei Dong*

Main category: cs.RO

TL;DR: 本文提出SPOT框架，通过整合感知目标与运动优化，提升UAV在动态环境中的障碍物规避能力。


<details>
  <summary>Details</summary>
Motivation: 无人机（UAV）在动态障碍物规避中面临困难，主要由于单一深度相机的视场限制和不可避免的盲区。

Method: SPOT框架结合高斯过程障碍物信念图和碰撞感知推断机制，通过时间变化的观察紧迫性地图实现实时轨迹规划。

Result: 提出了一种名为SPOT的统一规划框架，可以实现观察意识的轨迹规划，显著提高动态障碍物检测的提前量和可视度。

Conclusion: SPOT方法在动态复杂环境中表现出色，能够提前检测潜在动态障碍物，并提高安全导航能力。

Abstract: UAVs equipped with a single depth camera encounter significant challenges in
dynamic obstacle avoidance due to limited field of view and inevitable blind
spots. While active vision strategies that steer onboard cameras have been
proposed to expand sensing coverage, most existing methods separate motion
planning from sensing considerations, resulting in less effective and delayed
obstacle response. To address this limitation, we introduce SPOT
(Sensing-augmented Planning via Obstacle Threat modeling), a unified planning
framework for observation-aware trajectory planning that explicitly
incorporates sensing objectives into motion optimization. At the core of our
method is a Gaussian Process-based obstacle belief map, which establishes a
unified probabilistic representation of both recognized (previously observed)
and potential obstacles. This belief is further processed through a
collision-aware inference mechanism that transforms spatial uncertainty and
trajectory proximity into a time-varying observation urgency map. By
integrating urgency values within the current field of view, we define
differentiable objectives that enable real-time, observation-aware trajectory
planning with computation times under 10 ms. Simulation and real-world
experiments in dynamic, cluttered, and occluded environments show that our
method detects potential dynamic obstacles 2.8 seconds earlier than baseline
approaches, increasing dynamic obstacle visibility by over 500\%, and enabling
safe navigation through cluttered, occluded environments.

</details>


### [35] [Manual2Skill++: Connector-Aware General Robotic Assembly from Instruction Manuals via Vision-Language Models](https://arxiv.org/abs/2510.16344)
*Chenrui Tie,Shengxiang Sun,Yudi Lin,Yanbo Wang,Zhongrui Li,Zhouhan Zhong,Jinxuan Zhu,Yiman Pang,Haonan Chen,Junting Chen,Ruihai Wu,Lin Shao*

Main category: cs.RO

TL;DR: 本文提出了Manual2Skill++框架，强调在装配任务中将连接作为主要元素，并通过解析装配手册提取结构化信息。


<details>
  <summary>Details</summary>
Motivation: 当前机器人装配方法往往将连接视为次要环节，而本文旨在强调连接在装配成功中的重要性，通过引入更系统的连接表示来提升装配效率和效果。

Method: 本研究采用了一个视觉-语言模型，从装配手册中提取连接相关的信息，并将装配任务编码为分层图，以明确表示各个部件之间的连接关系。

Result: 本文提出了一种新的装配表示方法，将连接视为装配的基本元素，并介绍了Manual2Skill++框架，该框架从装配手册中提取结构化连接信息，利用大型视觉-语言模型解析符号图和注释，构建分层图以表示零部件和子装配之间的连接关系，同时通过模拟验证了其在多种复杂装配场景中的有效性。

Conclusion: 通过将连接作为装配中的关键元素，Manual2Skill++框架显著提高了装配任务的理解和执行效率，并在多种装配场景中验证了其有效性。

Abstract: Assembly hinges on reliably forming connections between parts; yet most
robotic approaches plan assembly sequences and part poses while treating
connectors as an afterthought. Connections represent the critical "last mile"
of assembly execution, while task planning may sequence operations and motion
plan may position parts, the precise establishment of physical connections
ultimately determines assembly success or failure. In this paper, we consider
connections as first-class primitives in assembly representation, including
connector types, specifications, quantities, and placement locations. Drawing
inspiration from how humans learn assembly tasks through step-by-step
instruction manuals, we present Manual2Skill++, a vision-language framework
that automatically extracts structured connection information from assembly
manuals. We encode assembly tasks as hierarchical graphs where nodes represent
parts and sub-assemblies, and edges explicitly model connection relationships
between components. A large-scale vision-language model parses symbolic
diagrams and annotations in manuals to instantiate these graphs, leveraging the
rich connection knowledge embedded in human-designed instructions. We curate a
dataset containing over 20 assembly tasks with diverse connector types to
validate our representation extraction approach, and evaluate the complete task
understanding-to-execution pipeline across four complex assembly scenarios in
simulation, spanning furniture, toys, and manufacturing components with
real-world correspondence.

</details>


### [36] [Learning to Optimize Edge Robotics: A Fast Integrated Perception-Motion-Communication Approach](https://arxiv.org/abs/2510.16424)
*Dan Guo,Xibin Jin,Shuai Wang,Zhigang Wen,Miaowen Wen,Chengzhong Xu*

Main category: cs.RO

TL;DR: 通过集成感知、运动与通信，机器人能够优化通信策略，显著减少计算复杂性。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了机器人功能与通信条件之间的相互依赖性，导致通信开销过大。

Method: 本文设计并实现了一种模仿学习神经网络，采用学习优化（LTO）范式，显著降低计算复杂性。

Result: 提出了一种集成感知、运动和通信的边缘机器人系统，能够动态调整通信策略，从而减少传感器数据上传的需求。

Conclusion: 实验结果表明，所提出的方法优于现有技术，且具备实时执行能力。

Abstract: Edge robotics involves frequent exchanges of large-volume multi-modal data.
Existing methods ignore the interdependency between robotic functionalities and
communication conditions, leading to excessive communication overhead. This
paper revolutionizes edge robotics systems through integrated perception,
motion, and communication (IPMC). As such, robots can dynamically adapt their
communication strategies (i.e., compression ratio, transmission frequency,
transmit power) by leveraging the knowledge of robotic perception and motion
dynamics, thus reducing the need for excessive sensor data uploads.
Furthermore, by leveraging the learning to optimize (LTO) paradigm, an
imitation learning neural network is designed and implemented, which reduces
the computational complexity by over 10x compared to state-of-the art
optimization solvers. Experiments demonstrate the superiority of the proposed
IPMC and the real-time execution capability of LTO.

</details>


### [37] [What Questions Should Robots Be Able to Answer? A Dataset of User Questions for Explainable Robotics](https://arxiv.org/abs/2510.16435)
*Lennart Wachowiak,Andrew Coles,Gerard Canal,Oya Celiktutan*

Main category: cs.RO

TL;DR: 该论文提出了一个1893个家庭机器人用户问题的数据集，为机器人提供了解用户期望的回答能力的重要基础。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型和对话界面在人与机器人互动中的不断应用，机器人回答用户问题的能力变得越来越重要。

Method: 通过15个视频刺激和7个文本刺激收集数据，100名参与者根据每个场景描述提问，最终整理出1893个问题。

Result: 介绍了一个包含1893个家庭机器人用户问题的数据集，帮助机器人理解用户期望的回答内容。

Conclusion: 该数据集为识别机器人需要记录的信息、评估问答模块及设计符合用户期望的解释策略提供了宝贵基础。

Abstract: With the growing use of large language models and conversational interfaces
in human-robot interaction, robots' ability to answer user questions is more
important than ever. We therefore introduce a dataset of 1,893 user questions
for household robots, collected from 100 participants and organized into 12
categories and 70 subcategories. Most work in explainable robotics focuses on
why-questions. In contrast, our dataset provides a wide variety of questions,
from questions about simple execution details to questions about how the robot
would act in hypothetical scenarios -- thus giving roboticists valuable
insights into what questions their robot needs to be able to answer. To collect
the dataset, we created 15 video stimuli and 7 text stimuli, depicting robots
performing varied household tasks. We then asked participants on Prolific what
questions they would want to ask the robot in each portrayed situation. In the
final dataset, the most frequent categories are questions about task execution
details (22.5%), the robot's capabilities (12.7%), and performance assessments
(11.3%). Although questions about how robots would handle potentially difficult
scenarios and ensure correct behavior are less frequent, users rank them as the
most important for robots to be able to answer. Moreover, we find that users
who identify as novices in robotics ask different questions than more
experienced users. Novices are more likely to inquire about simple facts, such
as what the robot did or the current state of the environment. As robots enter
environments shared with humans and language becomes central to giving
instructions and interaction, this dataset provides a valuable foundation for
(i) identifying the information robots need to log and expose to conversational
interfaces, (ii) benchmarking question-answering modules, and (iii) designing
explanation strategies that align with user expectations.

</details>


### [38] [Advancing Off-Road Autonomous Driving: The Large-Scale ORAD-3D Dataset and Comprehensive Benchmarks](https://arxiv.org/abs/2510.16500)
*Chen Min,Jilin Mei,Heng Zhai,Shuai Wang,Tong Sun,Fanjie Kong,Haoyang Li,Fangyuan Mao,Fuyang Liu,Shuo Wang,Yiming Nie,Qi Zhu,Liang Xiao,Dawei Zhao,Yu Hu*

Main category: cs.RO

TL;DR: 开发了ORAD-3D数据集，为离路自主驾驶研究提供了重要资源，包含各种地形与天气条件，并建立相关基准评估。


<details>
  <summary>Details</summary>
Motivation: 现有的离路自主驾驶研究缺乏大规模、高质量的数据集和基准，限制了该领域的发展。

Method: 构建大型高质量的离路自主驾驶数据集及基准评估

Result: 创建了ORAD-3D数据集，涵盖多种地形和环境变化，并建立了五项基本任务的基准评估。

Conclusion: 该数据集和基准将支持离路自主驾驶领域的感知和规划进步，并会公开发布以供研究社区使用。

Abstract: A major bottleneck in off-road autonomous driving research lies in the
scarcity of large-scale, high-quality datasets and benchmarks. To bridge this
gap, we present ORAD-3D, which, to the best of our knowledge, is the largest
dataset specifically curated for off-road autonomous driving. ORAD-3D covers a
wide spectrum of terrains, including woodlands, farmlands, grasslands,
riversides, gravel roads, cement roads, and rural areas, while capturing
diverse environmental variations across weather conditions (sunny, rainy,
foggy, and snowy) and illumination levels (bright daylight, daytime, twilight,
and nighttime). Building upon this dataset, we establish a comprehensive suite
of benchmark evaluations spanning five fundamental tasks: 2D free-space
detection, 3D occupancy prediction, rough GPS-guided path planning,
vision-language model-driven autonomous driving, and world model for off-road
environments. Together, the dataset and benchmarks provide a unified and robust
resource for advancing perception and planning in challenging off-road
scenarios. The dataset and code will be made publicly available at
https://github.com/chaytonmin/ORAD-3D.

</details>


### [39] [A Novel Gripper with Semi-Peaucellier Linkage and Idle-Stroke Mechanism for Linear Pinching and Self-Adaptive Grasping](https://arxiv.org/abs/2510.16517)
*Haokai Ding,Wenzeng Zhang*

Main category: cs.RO

TL;DR: 本文介绍了一种新型的SPD机器人夹持器，具有线性平行夹持功能，能够适应各种形状和尺寸的物体，且无需调整机器人手臂高度。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统夹持器在抓取过程中由于手臂高度调整导致的碰撞问题

Method: 设计并测试了一种新的机械夹持器原型

Result: 实验结果表明SPD夹持器能实现线性平行夹持功能并具有良好的适应性

Conclusion: SPD夹持器为实现有效抓取和增强深度学习训练提供了坚实基础。

Abstract: This paper introduces a novel robotic gripper, named as the SPD gripper. It
features a palm and two mechanically identical and symmetrically arranged
fingers, which can be driven independently or by a single motor. The fingertips
of the fingers follow a linear motion trajectory, facilitating the grasping of
objects of various sizes on a tabletop without the need to adjust the overall
height of the gripper. Traditional industrial grippers with parallel gripping
capabilities often exhibit an arcuate motion at the fingertips, requiring the
entire robotic arm to adjust its height to avoid collisions with the tabletop.
The SPD gripper, with its linear parallel gripping mechanism, effectively
addresses this issue. Furthermore, the SPD gripper possesses adaptive
capabilities, accommodating objects of different shapes and sizes. This paper
presents the design philosophy, fundamental composition principles, and
optimization analysis theory of the SPD gripper. Based on the design theory, a
robotic gripper prototype was developed and tested. The experimental results
demonstrate that the robotic gripper successfully achieves linear parallel
gripping functionality and exhibits good adaptability. In the context of the
ongoing development of embodied intelligence technologies, this robotic gripper
can assist various robots in achieving effective grasping, laying a solid
foundation for collecting data to enhance deep learning training.

</details>


### [40] [DIV-Nav: Open-Vocabulary Spatial Relationships for Multi-Object Navigation](https://arxiv.org/abs/2510.16518)
*Jesús Ortega-Peimbert,Finn Lukas Busch,Timon Homberger,Quantao Yang,Olov Andersson*

Main category: cs.RO

TL;DR: 研究提出了一种实时导航系统DIV-Nav，能够处理复杂的自然语言指令，并有效地在语义地图上进行目标搜索。


<details>
  <summary>Details</summary>
Motivation: 现有的零-shot物体导航通常处理简单的对象名称查询，缺乏对复杂空间关系的理解，激励本研究发展更复杂的导航能力。

Method: 通过将复杂自然语言指令分解为简单的对象级查询，计算语义信念图的交集，并利用大型语言模型验证发现的对象。

Result: 系统在MultiON基准测试和实际部署中表现良好，能够有效引导复杂空间搜索过程。

Conclusion: DIV-Nav系统在复杂空间约束查询的有效性上表现出色，经过实验证明其在多种环境中的应用潜力。

Abstract: Advances in open-vocabulary semantic mapping and object navigation have
enabled robots to perform an informed search of their environment for an
arbitrary object. However, such zero-shot object navigation is typically
designed for simple queries with an object name like "television" or "blue
rug". Here, we consider more complex free-text queries with spatial
relationships, such as "find the remote on the table" while still leveraging
robustness of a semantic map. We present DIV-Nav, a real-time navigation system
that efficiently addresses this problem through a series of relaxations: i)
Decomposing natural language instructions with complex spatial constraints into
simpler object-level queries on a semantic map, ii) computing the Intersection
of individual semantic belief maps to identify regions where all objects
co-exist, and iii) Validating the discovered objects against the original,
complex spatial constrains via a LVLM. We further investigate how to adapt the
frontier exploration objectives of online semantic mapping to such spatial
search queries to more effectively guide the search process. We validate our
system through extensive experiments on the MultiON benchmark and real-world
deployment on a Boston Dynamics Spot robot using a Jetson Orin AGX. More
details and videos are available at https://anonsub42.github.io/reponame/

</details>


### [41] [Semi-Peaucellier Linkage and Differential Mechanism for Linear Pinching and Self-Adaptive Grasping](https://arxiv.org/abs/2510.16524)
*Haokai Ding,Zhaohan Chen,Tao Yang,Wenzeng Zhang*

Main category: cs.RO

TL;DR: SP-Diff夹持工具通过创新的差动连杆机制和模块化对称双指配置实现线性平行夹持，具备更好的适应性和智能。


<details>
  <summary>Details</summary>
Motivation: 针对传统夹持器适应性不足的问题，开发出适合多样化工业工件和可变形物体（如柑橘类水果）的夹持系统。

Method: 该设计采用了创新的差动连杆机制与模块化对称双指配置，集成了行星齿轮传动系统，以实现同步线性运动和独立的手指姿态调整。

Result: 本文提出的SP-Diff平行夹持器系统解决了传统末端执行器在智能工业自动化中适应性有限的问题。

Conclusion: SP-Diff作为灵活的制造解决方案，通过自适应架构提升了机器人末端执行器的智能，展现出在协作机器人、物流自动化和特定操作场景中的应用前景。

Abstract: This paper presents the SP-Diff parallel gripper system, addressing the
limited adaptability of conventional end-effectors in intelligent industrial
automation. The proposed design employs an innovative differential linkage
mechanism with a modular symmetric dual-finger configuration to achieve
linear-parallel grasping. By integrating a planetary gear transmission, the
system enables synchronized linear motion and independent finger pose
adjustment while maintaining structural rigidity, reducing Z-axis recalibration
requirements by 30% compared to arc-trajectory grippers. The compact palm
architecture incorporates a kinematically optimized parallelogram linkage and
Differential mechanism, demonstrating adaptive grasping capabilities for
diverse industrial workpieces and deformable objects such as citrus fruits.
Future-ready interfaces are embedded for potential force/vision sensor
integration to facilitate multimodal data acquisition (e.g., trajectory
planning and object deformation) in digital twin frameworks. Designed as a
flexible manufacturing solution, SP-Diff advances robotic end-effector
intelligence through its adaptive architecture, showing promising applications
in collaborative robotics, logistics automation, and specialized operational
scenarios.

</details>


### [42] [MoS-VLA: A Vision-Language-Action Model with One-Shot Skill Adaptation](https://arxiv.org/abs/2510.16617)
*Ruihan Zhao,Tyler Ingebrand,Sandeep Chinchali,Ufuk Topcu*

Main category: cs.RO

TL;DR: MoS-VLA通过线性组合学习基础函数实现快速技能适应，在新任务上展现出优越性。


<details>
  <summary>Details</summary>
Motivation: VLA模型在新环境、机构或任务下的表现不足，因此需要一种新的方法来实现更好的适应性和鲁棒性。

Method: MoS-VLA框架通过将机器人操作策略表示为一组学习基础函数的线性组合，在测试时通过单个专家演示适应新任务，并通过轻量级凸优化问题推断相应的技能表示。

Result: MoS-VLA在未见数据集上的动作预测错误较低，并在新的仿真和实际机器人任务中超越了预训练的VLA模型。

Conclusion: MoS-VLA在五个未见数据集上降低了动作预测错误，并在预训练的VLA模型失败的仿真和真实机器人任务中取得了成功。

Abstract: Vision-Language-Action (VLA) models trained on large robot datasets promise
general-purpose, robust control across diverse domains and embodiments.
However, existing approaches often fail out-of-the-box when deployed in novel
environments, embodiments, or tasks. We introduce Mixture of Skills VLA
(MoS-VLA), a framework that represents robot manipulation policies as linear
combinations of a finite set of learned basis functions. During pretraining,
MoS-VLA jointly learns these basis functions across datasets from the Open
X-Embodiment project, producing a structured skill space. At test time,
adapting to a new task requires only a single expert demonstration. The
corresponding skill representation is then inferred via a lightweight convex
optimization problem that minimizes the L1 action error, without requiring
gradient updates. This gradient-free adaptation incurs minimal overhead while
enabling rapid instantiation of new skills. Empirically, MoS-VLA achieves lower
action-prediction error on five out of five unseen datasets and succeeds in
both simulation and real-robot tasks where a pretrained VLA model fails
outright. Project page: mos-vla.github.io/

</details>


### [43] [First Responders' Perceptions of Semantic Information for Situational Awareness in Robot-Assisted Emergency Response](https://arxiv.org/abs/2510.16692)
*Tianshu Ruan,Zoe Betta,Georgios Tzoumas,Rustam Stolkin,Manolis Chiou*

Main category: cs.RO

TL;DR: 本研究探讨了急救人员对机器人系统中语义信息和情境意识的态度，发现他们倾向于相信强化语义的机器人，但仍需改进技术以符合实际应用。


<details>
  <summary>Details</summary>
Motivation: 探讨急救人员对机器人系统中语义信息和情境意识使用的态度，以改善应急响应的效果。

Method: 通过对22名来自八个国家的急救人员进行结构化问卷调查，收集其人口统计特征、对机器人的一般态度及与增强语义情境意识的经验。

Result: 大多数急救人员对机器人的态度积极，平均认为语义信息在增强情境意识方面有用，尤其是在预测突发事件时给予高评价；但对语义输出的信任和实用性有一定的准确度要求。

Conclusion: 本研究揭示了急救人员对语义信息和情境意识在机器人系统中的应用态度，强调了与机器人研究人员合作的必要性，以促进更符合用户需求的应急响应机器人系统发展。

Abstract: This study investigates First Responders' (FRs) attitudes toward the use of
semantic information and Situational Awareness (SA) in robotic systems during
emergency operations. A structured questionnaire was administered to 22 FRs
across eight countries, capturing their demographic profiles, general attitudes
toward robots, and experiences with semantics-enhanced SA. Results show that
most FRs expressed positive attitudes toward robots, and rated the usefulness
of semantic information for building SA at an average of 3.6 out of 5. Semantic
information was also valued for its role in predicting unforeseen emergencies
(mean 3.9). Participants reported requiring an average of 74.6\% accuracy to
trust semantic outputs and 67.8\% for them to be considered useful, revealing a
willingness to use imperfect but informative AI support tools.
  To the best of our knowledge, this study offers novel insights by being one
of the first to directly survey FRs on semantic-based SA in a cross-national
context. It reveals the types of semantic information most valued in the field,
such as object identity, spatial relationships, and risk context-and connects
these preferences to the respondents' roles, experience, and education levels.
The findings also expose a critical gap between lab-based robotics capabilities
and the realities of field deployment, highlighting the need for more
meaningful collaboration between FRs and robotics researchers. These insights
contribute to the development of more user-aligned and situationally aware
robotic systems for emergency response.

</details>


### [44] [Towards Active Excitation-Based Dynamic Inertia Identification in Satellites](https://arxiv.org/abs/2510.16738)
*Matteo El-Hariry,Vittorio Franzese,Miguel Olivares-Mendez*

Main category: cs.RO

TL;DR: 这篇论文分析了激励设计如何影响刚性纳米和微型卫星的惯性特性识别，通过模拟非线性姿态动态和不同的扭矩配置，比较了两种估计方法的效果。


<details>
  <summary>Details</summary>
Motivation: 探索激励设计对刚性卫星惯性特性识别的影响，以期提高估计的准确性和适用性。

Method: 采用非线性姿态动态模拟，比较批量最小二乘法和扩展卡尔曼滤波器在不同卫星配置和时间变化惯性场景下的表现。

Result: 研究表明，不同激励频率内容和估计器假设的结合影响了估计精度，提出了最佳执行条件。

Conclusion: 激励频率内容与估计器假设共同决定了估计精度和鲁棒性，为在轨自适应惯性识别提供了实际指导。

Abstract: This paper presents a comprehensive analysis of how excitation design
influences the identification of the inertia properties of rigid nano- and
micro-satellites. We simulate nonlinear attitude dynamics with reaction-wheel
coupling, actuator limits, and external disturbances, and excite the system
using eight torque profiles of varying spectral richness. Two estimators are
compared, a batch Least Squares method and an Extended Kalman Filter, across
three satellite configurations and time-varying inertia scenarios. Results show
that excitation frequency content and estimator assumptions jointly determine
estimation accuracy and robustness, offering practical guidance for in-orbit
adaptive inertia identification by outlining the conditions under which each
method performs best. The code is provided as open-source .

</details>


### [45] [Adaptive Invariant Extended Kalman Filter for Legged Robot State Estimation](https://arxiv.org/abs/2510.16755)
*Kyung-Hwan Kim,DongHyun Ahn,Dong-hyun Lee,JuYoung Yoon,Dong Jin Hyun*

Main category: cs.RO

TL;DR: 本文提出了一种自适应不变扩展卡尔曼滤波器，旨在改善腿机器人在动态运动中的状态估计。


<details>
  <summary>Details</summary>
Motivation: 腿机器人状态估计对控制性能和运动稳定性至关重要。

Method: 自适应不变扩展卡尔曼滤波器（AIEKF），根据在线协方差估计自适应调整接触足模型的噪声水平。

Result: 通过在四足机器人LeoQuad上的真实实验验证了该方法的有效性。

Conclusion: 所提出的方法在动态运动场景中展示了更好的状态估计性能。

Abstract: State estimation is crucial for legged robots as it directly affects control
performance and locomotion stability. In this paper, we propose an Adaptive
Invariant Extended Kalman Filter to improve proprioceptive state estimation for
legged robots. The proposed method adaptively adjusts the noise level of the
contact foot model based on online covariance estimation, leading to improved
state estimation under varying contact conditions. It effectively handles small
slips that traditional slip rejection fails to address, as overly sensitive
slip rejection settings risk causing filter divergence. Our approach employs a
contact detection algorithm instead of contact sensors, reducing the reliance
on additional hardware. The proposed method is validated through real-world
experiments on the quadruped robot LeoQuad, demonstrating enhanced state
estimation performance in dynamic locomotion scenarios.

</details>


### [46] [T3 Planner: A Self-Correcting LLM Framework for Robotic Motion Planning with Temporal Logic](https://arxiv.org/abs/2510.16767)
*Jia Li,Guoxiang Zhao*

Main category: cs.RO

TL;DR: 提出了一种利用大型语言模型（LLM）进行可执行动作计划的框架T3 Planner，通过自我校正和正式方法提高机器人的运动规划能力。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在运动规划中的局限性，尤其是在定制规划和时空耦合问题上，并利用LLM的高层语义推理能力。

Method: 通过三级级联模块分解时空任务约束，每个模块激励一个LLM生成候选轨迹序列，并使用信号时序逻辑（STL）验证其可行性，直到找到符合所有约束的方案。

Result: T3 Planner的实验结果显示其显著优于基线模型，并且可有效部署于轻量化的Qwen3-4B模型中。

Conclusion: T3 Planner在不同场景下的实验中表现优异，证明了其在处理复杂空间、时间和逻辑约束方面的有效性。

Abstract: Translating natural language instructions into executable motion plans is a
fundamental challenge in robotics. Traditional approaches are typically
constrained by their reliance on domain-specific expertise to customize
planners, and often struggle with spatio-temporal couplings that usually lead
to infeasible motions or discrepancies between task planning and motion
execution. Despite the proficiency of Large Language Models (LLMs) in
high-level semantic reasoning, hallucination could result in infeasible motion
plans. In this paper, we introduce the T3 Planner, an LLM-enabled robotic
motion planning framework that self-corrects it output with formal methods. The
framework decomposes spatio-temporal task constraints via three cascaded
modules, each of which stimulates an LLM to generate candidate trajectory
sequences and examines their feasibility via a Signal Temporal Logic (STL)
verifier until one that satisfies complex spatial, temporal, and logical
constraints is found.Experiments across different scenarios show that T3
Planner significantly outperforms the baselines. The required reasoning can be
distilled into a lightweight Qwen3-4B model that enables efficient deployment.
All supplementary materials are accessible at
https://github.com/leeejia/T3_Planner.

</details>


### [47] [A Preliminary Exploration of the Differences and Conjunction of Traditional PNT and Brain-inspired PNT](https://arxiv.org/abs/2510.16771)
*Xu He,Xiaolin Meng,Wenxuan Yin,Youdong Zhang,Lingfei Mo,Xiangdong An,Fangwen Yu,Shuguo Pan,Yufeng Liu,Jingnan Liu,Yujia Zhang,Wang Gao*

Main category: cs.RO

TL;DR: 本文探讨如何通过脑启发式空间认知导航提升PNT的能力，并提出了一个新的融合框架来实现这一目标。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中，需求更具韧性、能源效率和认知能力的PNT，因此需要赋予无人系统脑启发式的空间认知导航能力。

Method: 提出了一个四层融合框架（观察-能力-决策-硬件），将数值精度与脑启发智能结合。

Result: 通过多层次分析传统PNT、生物脑PNT和脑启发式PNT之间的差异，明确了未来脑启发式PNT发展的建议。

Conclusion: 本文提出了一种新的视角和路线图，将定位、导航和时间（PNT）从工具导向转变为认知驱动，强调了生物脑和脑启发式PNT的优势。

Abstract: Developing universal Positioning, Navigation, and Timing (PNT) is our
enduring goal. Today's complex environments demand PNT that is more resilient,
energy-efficient and cognitively capable. This paper asks how we can endow
unmanned systems with brain-inspired spatial cognition navigation while
exploiting the high precision of machine PNT to advance universal PNT. We
provide a new perspective and roadmap for shifting PNT from "tool-oriented" to
"cognition-driven". Contributions: (1) multi-level dissection of differences
among traditional PNT, biological brain PNT and brain-inspired PNT; (2) a
four-layer (observation-capability-decision-hardware) fusion framework that
unites numerical precision and brain-inspired intelligence; (3) forward-looking
recommendations for future development of brain-inspired PNT.

</details>


### [48] [C-Free-Uniform: A Map-Conditioned Trajectory Sampler for Model Predictive Path Integral Control](https://arxiv.org/abs/2510.16905)
*Yukang Cao,Rahul Moorthy,O. Goktug Poyrazoglu,Volkan Isler*

Main category: cs.RO

TL;DR: 本文介绍了一种新型控制输入采样方法C-Free-Uniform，并将其应用于CFU-MPPI控制器，显著提升了在复杂环境下的导航能力。


<details>
  <summary>Details</summary>
Motivation: 提出了C-Free-Uniform概念，以在自由配置空间中均匀采样，并根据当前局部地图条件化控制输入。

Method: 将基于C-Free-Uniform的控制输入采样器集成到新的模型预测路径积分控制器CFU-MPPI中。

Result: CFU-MPPI在挑战性导航任务中表现出色，成功率更高。

Conclusion: CFU-MPPI在复杂的多边形环境中的成功率明显高于现有方法，并且需要更少的采样预算。

Abstract: Trajectory sampling is a key component of sampling-based control mechanisms.
Trajectory samplers rely on control input samplers, which generate control
inputs u from a distribution p(u | x) where x is the current state. We
introduce the notion of Free Configuration Space Uniformity (C-Free-Uniform for
short) which has two key features: (i) it generates a control input
distribution so as to uniformly sample the free configuration space, and (ii)
in contrast to previously introduced trajectory sampling mechanisms where the
distribution p(u | x) is independent of the environment, C-Free-Uniform is
explicitly conditioned on the current local map. Next, we integrate this
sampler into a new Model Predictive Path Integral (MPPI) Controller, CFU-MPPI.
Experiments show that CFU-MPPI outperforms existing methods in terms of success
rate in challenging navigation tasks in cluttered polygonal environments while
requiring a much smaller sampling budget.

</details>


### [49] [Design of an Affordable, Fully-Actuated Biomimetic Hand for Dexterous Teleoperation Systems](https://arxiv.org/abs/2510.16931)
*Zhaoliang Wan,Zida Zhou,Zetong Bi,Zehui Yang,Hao Ding,Hui Cheng*

Main category: cs.RO

TL;DR: 本论文介绍了一款新型低成本灵巧手RAPID Hand，具备20自由度，旨在解决现有经济灵巧手稀缺问题，适合在遥操作中使用。


<details>
  <summary>Details</summary>
Motivation: 解决经济适用的五指灵巧手的稀缺问题，以便在“从示范中学习”范式下收集大规模真实机器人数据。

Method: 通过定量指标和定性测试评估RAPID Hand在灵巧遥操作系统中的性能，包含多指取物、舀取物体和类似人类的钢琴演奏等任务。

Result: RAPID Hand是首个低成本的20自由度灵巧手，通过新型人类工效学驱动和传动方案以及优化的电机布局和结构设计提升灵巧性。

Conclusion: RAPID Hand的全关节驱动20自由度设计在灵巧遥操作中具有重大潜力。

Abstract: This paper addresses the scarcity of affordable, fully-actuated five-fingered
hands for dexterous teleoperation, which is crucial for collecting large-scale
real-robot data within the "Learning from Demonstrations" paradigm. We
introduce the prototype version of the RAPID Hand, the first low-cost,
20-degree-of-actuation (DoA) dexterous hand that integrates a novel
anthropomorphic actuation and transmission scheme with an optimized motor
layout and structural design to enhance dexterity. Specifically, the RAPID Hand
features a universal phalangeal transmission scheme for the non-thumb fingers
and an omnidirectional thumb actuation mechanism. Prioritizing affordability,
the hand employs 3D-printed parts combined with custom gears for easier
replacement and repair. We assess the RAPID Hand's performance through
quantitative metrics and qualitative testing in a dexterous teleoperation
system, which is evaluated on three challenging tasks: multi-finger retrieval,
ladle handling, and human-like piano playing. The results indicate that the
RAPID Hand's fully actuated 20-DoF design holds significant promise for
dexterous teleoperation.

</details>


### [50] [DINO-CVA: A Multimodal Goal-Conditioned Vision-to-Action Model for Autonomous Catheter Navigation](https://arxiv.org/abs/2510.17038)
*Pedram Fekri,Majid Roshanfar,Samuel Barbeau,Seyedfarzad Famouri,Thomas Looi,Dale Podolsky,Mehrdad Zadeh,Javad Dargahi*

Main category: cs.RO

TL;DR: 提出DINO-CVA框架，以实现自主的心脏导管导航，结合视觉和运动学信息，减少对操作员的依赖。


<details>
  <summary>Details</summary>
Motivation: 当前的心脏导管插入依赖手动操作，现有的机器人系统缺乏智能自主性，导致操作疲劳、辐射暴露和结果可变性。

Method: DINO-CVA，一种多模态目标条件行为克隆框架，结合视觉观察和摇杆运动学进行自主导管导航。

Result: DINO-CVA在合成血管假体上通过收集多模态数据集进行评估，准确性高，匹配传统运动学基线的性能，同时考虑到解剖环境。

Conclusion: 多模态、目标条件架构在导管导航的可行性得到了确立，为改善基于导管的治疗的可靠性迈出重要一步。

Abstract: Cardiac catheterization remains a cornerstone of minimally invasive
interventions, yet it continues to rely heavily on manual operation. Despite
advances in robotic platforms, existing systems are predominantly follow-leader
in nature, requiring continuous physician input and lacking intelligent
autonomy. This dependency contributes to operator fatigue, more radiation
exposure, and variability in procedural outcomes. This work moves towards
autonomous catheter navigation by introducing DINO-CVA, a multimodal
goal-conditioned behavior cloning framework. The proposed model fuses visual
observations and joystick kinematics into a joint embedding space, enabling
policies that are both vision-aware and kinematic-aware. Actions are predicted
autoregressively from expert demonstrations, with goal conditioning guiding
navigation toward specified destinations. A robotic experimental setup with a
synthetic vascular phantom was designed to collect multimodal datasets and
evaluate performance. Results show that DINO-CVA achieves high accuracy in
predicting actions, matching the performance of a kinematics-only baseline
while additionally grounding predictions in the anatomical environment. These
findings establish the feasibility of multimodal, goal-conditioned
architectures for catheter navigation, representing an important step toward
reducing operator dependency and improving the reliability of catheterbased
therapies.

</details>


### [51] [Learning to Design Soft Hands using Reward Models](https://arxiv.org/abs/2510.17086)
*Xueqian Bai,Nicklas Hansen,Adabhav Singh,Michael T. Tolley,Yan Duan,Pieter Abbeel,Xiaolong Wang,Sha Yi*

Main category: cs.RO

TL;DR: 本研究提出了一种高效优化软机器人手的方法，通过交叉熵法和奖励模型减少设计评估次数，同时学习优化手设计分布，最终提高抓取成功率。


<details>
  <summary>Details</summary>
Motivation: 设计可兼容且高功能性的软机器人手，以应对多样化的应用场景，同时克服高维搜索空间和计算成本高的问题。

Method: 采用交叉熵法与奖励模型(CEM-RM)框架对腱驱动软机器人手进行优化，并通过仿真和实时遥操作数据进行训练。

Result: 通过仿真和硬件实验，证明优化设计的软机器人手在抓取成功率上显著优于基线模型。

Conclusion: 优化的软机器人手在各种具有挑战性的物体抓取任务中，显示出显著优于基线手的抓取成功率。

Abstract: Soft robotic hands promise to provide compliant and safe interaction with
objects and environments. However, designing soft hands to be both compliant
and functional across diverse use cases remains challenging. Although co-design
of hardware and control better couples morphology to behavior, the resulting
search space is high-dimensional, and even simulation-based evaluation is
computationally expensive. In this paper, we propose a Cross-Entropy Method
with Reward Model (CEM-RM) framework that efficiently optimizes tendon-driven
soft robotic hands based on teleoperation control policy, reducing design
evaluations by more than half compared to pure optimization while learning a
distribution of optimized hand designs from pre-collected teleoperation data.
We derive a design space for a soft robotic hand composed of flexural soft
fingers and implement parallelized training in simulation. The optimized hands
are then 3D-printed and deployed in the real world using both teleoperation
data and real-time teleoperation. Experiments in both simulation and hardware
demonstrate that our optimized design significantly outperforms baseline hands
in grasping success rates across a diverse set of challenging objects.

</details>


### [52] [Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey](https://arxiv.org/abs/2510.17111)
*Weifan Guan,Qinghao Hu,Aosheng Li,Jian Cheng*

Main category: cs.RO

TL;DR: 本调查针对VLA模型的效率提升进行了全面回顾，提出了未来研究的方向。


<details>
  <summary>Details</summary>
Motivation: 随着对更高效和可扩展的VLA系统的需求增加，解决计算和内存消耗问题成为研究的中心焦点。

Method: 对现有VLA系统效率改善的方法进行了系统的分类和审查。

Result: 将现有解决方案分为四个方面：模型架构、感知特征、动作生成和训练/推断策略，并总结了每个类别中的代表性技术。

Conclusion: 本调查总结了改善VLA系统效率的各种方法，并讨论了未来的趋势和开放挑战。

Abstract: Vision-Language-Action (VLA) models extend vision-language models to embodied
control by mapping natural-language instructions and visual observations to
robot actions. Despite their capabilities, VLA systems face significant
challenges due to their massive computational and memory demands, which
conflict with the constraints of edge platforms such as on-board mobile
manipulators that require real-time performance. Addressing this tension has
become a central focus of recent research. In light of the growing efforts
toward more efficient and scalable VLA systems, this survey provides a
systematic review of approaches for improving VLA efficiency, with an emphasis
on reducing latency, memory footprint, and training and inference costs. We
categorize existing solutions into four dimensions: model architecture,
perception feature, action generation, and training/inference strategies,
summarizing representative techniques within each category. Finally, we discuss
future trends and open challenges, highlighting directions for advancing
efficient embodied intelligence.

</details>


### [53] [Decentralized Real-Time Planning for Multi-UAV Cooperative Manipulation via Imitation Learning](https://arxiv.org/abs/2510.17143)
*Shantnav Agarwal,Javier Alonso-Mora,Sihao Sun*

Main category: cs.RO

TL;DR: 本文提出了一种新型去中心化规划方法，利用模仿学习训练无人机，使其在不依赖通信的情况下有效跟踪轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于中心化控制架构或可靠的代理间通信，这限制了它们在不确定环境中的应用。

Method: 提出了一种基于机器学习的去中心化运动规划方法。

Result: 提出的方法在部分可观察性和无需代理间通信的情况下，能够高效生成平滑的运动轨迹，且在仿真和真实环境中验证其效果。

Conclusion: 新方法在短时间内完成训练，且性能可与中心化方法相媲美，为多无人机系统的应用提供了新思路。

Abstract: Existing approaches for transporting and manipulating cable-suspended loads
using multiple UAVs along reference trajectories typically rely on either
centralized control architectures or reliable inter-agent communication. In
this work, we propose a novel machine learning based method for decentralized
kinodynamic planning that operates effectively under partial observability and
without inter-agent communication. Our method leverages imitation learning to
train a decentralized student policy for each UAV by imitating a centralized
kinodynamic motion planner with access to privileged global observations. The
student policy generates smooth trajectories using physics-informed neural
networks that respect the derivative relationships in motion. During training,
the student policies utilize the full trajectory generated by the teacher
policy, leading to improved sample efficiency. Moreover, each student policy
can be trained in under two hours on a standard laptop. We validate our method
in both simulation and real-world environments to follow an agile reference
trajectory, demonstrating performance comparable to that of centralized
approaches.

</details>


### [54] [DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment](https://arxiv.org/abs/2510.17148)
*Yu Gao,Yiru Wang,Anqing Jiang,Heng Yuwen,Wang Shuo,Sun Hao,Wang Jijun*

Main category: cs.RO

TL;DR: DiffVLA++是一种增强的自主驾驶框架，结合了认知推理和端到端规划，通过度量引导对齐克服了传统模型在长尾场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统的端到端驾驶模型在生成物理上合理的轨迹方面有效，但在处理长尾场景时缺乏必要的世界知识。

Method: 引入DiffVLA++框架，通过度量引导对齐，将认知推理与端到端规划相结合。

Result: DiffVLA++在ICCV 2025 Autonomous Grand Challenge排行榜上的EPDMS达到49.12。

Conclusion: DiffVLA++有效整合了VLA和端到端模块的优势，提高了自主驾驶的轨迹生成质量。

Abstract: Conventional end-to-end (E2E) driving models are effective at generating
physically plausible trajectories, but often fail to generalize to long-tail
scenarios due to the lack of essential world knowledge to understand and reason
about surrounding environments. In contrast, Vision-Language-Action (VLA)
models leverage world knowledge to handle challenging cases, but their limited
3D reasoning capability can lead to physically infeasible actions. In this work
we introduce DiffVLA++, an enhanced autonomous driving framework that
explicitly bridges cognitive reasoning and E2E planning through metric-guided
alignment. First, we build a VLA module directly generating semantically
grounded driving trajectories. Second, we design an E2E module with a dense
trajectory vocabulary that ensures physical feasibility. Third, and most
critically, we introduce a metric-guided trajectory scorer that guides and
aligns the outputs of the VLA and E2E modules, thereby integrating their
complementary strengths. The experiment on the ICCV 2025 Autonomous Grand
Challenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12.

</details>


### [55] [OmniVIC: A Self-Improving Variable Impedance Controller with Vision-Language In-Context Learning for Safe Robotic Manipulation](https://arxiv.org/abs/2510.17150)
*Heng Zhang,Wei-Hsing Huang,Gokhan Solak,Arash Ajoudani*

Main category: cs.RO

TL;DR: OmniVIC是一种结合视觉语言模型的通用变阻抗控制器，旨在增强机器人在复杂操作任务中的安全性和适应性，表现出显著的成功率提升。


<details>
  <summary>Details</summary>
Motivation: 传统的变阻抗控制器在机器人与环境的物理交互中显示出优势，但在复杂和不确定的安全交互中缺乏广泛适应性。

Method: 提出了一种增强型的通用变阻抗控制器（OmniVIC），结合视觉语言模型（VLM）以改进接触丰富的机器人操作任务中的安全性和适应性。

Result: OmniVIC在一系列复杂的接触任务中表现优于基线，成功率从27%提高到61.4%，减少力的违规。

Conclusion: 通过高层次的语义推理与低层次的柔性控制之间的桥接，OmniVIC为安全和更广泛的操作奠定了基础。

Abstract: We present OmniVIC, a universal variable impedance controller (VIC) enhanced
by a vision language model (VLM), which improves safety and adaptation in any
contact-rich robotic manipulation task to enhance safe physical interaction.
Traditional VIC have shown advantages when the robot physically interacts with
the environment, but lack generalization in unseen, complex, and unstructured
safe interactions in universal task scenarios involving contact or uncertainty.
To this end, the proposed OmniVIC interprets task context derived reasoning
from images and natural language and generates adaptive impedance parameters
for a VIC controller. Specifically, the core of OmniVIC is a self-improving
Retrieval-Augmented Generation(RAG) and in-context learning (ICL), where RAG
retrieves relevant prior experiences from a structured memory bank to inform
the controller about similar past tasks, and ICL leverages these retrieved
examples and the prompt of current task to query the VLM for generating
context-aware and adaptive impedance parameters for the current manipulation
scenario. Therefore, a self-improved RAG and ICL guarantee OmniVIC works in
universal task scenarios. The impedance parameter regulation is further
informed by real-time force/torque feedback to ensure interaction forces remain
within safe thresholds. We demonstrate that our method outperforms baselines on
a suite of complex contact-rich tasks, both in simulation and on real-world
robotic tasks, with improved success rates and reduced force violations.
OmniVIC takes a step towards bridging high-level semantic reasoning and
low-level compliant control, enabling safer and more generalizable
manipulation. Overall, the average success rate increases from 27% (baseline)
to 61.4% (OmniVIC).

</details>


### [56] [SimpleVSF: VLM-Scoring Fusion for Trajectory Prediction of End-to-End Autonomous Driving](https://arxiv.org/abs/2510.17191)
*Peiru Zheng,Yun Zhao,Zhan Gong,Hong Zhu,Shaohua Wu*

Main category: cs.RO

TL;DR: 本文提出了一种新的端到端驾驶框架SimpleVSF，通过VLM和轨迹融合技术提升决策能力，解决了复杂场景下的决策挑战。


<details>
  <summary>Details</summary>
Motivation: 解决现有端到端方法在复杂场景下决策不佳的挑战。

Method: 提出了一种结合视觉语言模型(VLM)和高级轨迹融合技术的新框架SimpleVSF，用以增强端到端规划。

Result: SimpleVSF框架在实际应用中表现出超越现有方法的状态，具备更强的决策能力。

Conclusion: SimpleVSF在ICCVI 2025 NAVSIM v2端到端驾驶挑战赛中表现出色，实现了安全、舒适和效率之间的优良平衡。

Abstract: End-to-end autonomous driving has emerged as a promising paradigm for
achieving robust and intelligent driving policies. However, existing end-to-end
methods still face significant challenges, such as suboptimal decision-making
in complex scenarios. In this paper,we propose SimpleVSF (Simple VLM-Scoring
Fusion), a novel framework that enhances end-to-end planning by leveraging the
cognitive capabilities of Vision-Language Models (VLMs) and advanced trajectory
fusion techniques. We utilize the conventional scorers and the novel
VLM-enhanced scorers. And we leverage a robust weight fusioner for quantitative
aggregation and a powerful VLM-based fusioner for qualitative, context-aware
decision-making. As the leading approach in the ICCV 2025 NAVSIM v2 End-to-End
Driving Challenge, our SimpleVSF framework demonstrates state-of-the-art
performance, achieving a superior balance between safety, comfort, and
efficiency.

</details>


### [57] [Performance Evaluation of an Integrated System for Visible Light Communication and Positioning Using an Event Camera](https://arxiv.org/abs/2510.17203)
*Ryota Soga,Masataka Kobayashi,Tsukasa Shimizu,Shintaro Shiba,Quan Kong,Shan Lu,Takaya Yamazato*

Main category: cs.RO

TL;DR: 本研究提出了一种新颖的自定位系统，利用事件相机集成可见光通信和可见光定位，实现在GPS失效环境中的车辆定位。


<details>
  <summary>Details</summary>
Motivation: 在隧道等GPS不可用环境中实现车辆准确定位的需求。

Method: 通过在车辆上安装系统，利用LED发射器获取坐标信息，并通过相位相关方法估计与发射器的距离。

Result: 实验证明，系统在100米范围内的距离估计均方根误差为0.75米，误比特率低于0.01。

Conclusion: 该系统在实际场景中展示了强大的性能，具有高精度的距离估计和低错误率。

Abstract: Event cameras, featuring high temporal resolution and high dynamic range,
offer visual sensing capabilities comparable to conventional image sensors
while capturing fast-moving objects and handling scenes with extreme lighting
contrasts such as tunnel exits. Leveraging these properties, this study
proposes a novel self-localization system that integrates visible light
communication (VLC) and visible light positioning (VLP) within a single event
camera. The system enables a vehicle to estimate its position even in
GPS-denied environments, such as tunnels, by using VLC to obtain coordinate
information from LED transmitters and VLP to estimate the distance to each
transmitter.
  Multiple LEDs are installed on the transmitter side, each assigned a unique
pilot sequence based on Walsh-Hadamard codes. The event camera identifies
individual LEDs within its field of view by correlating the received signal
with these codes, allowing clear separation and recognition of each light
source. This mechanism enables simultaneous high-capacity MISO (multi-input
single-output) communication through VLC and precise distance estimation via
phase-only correlation (POC) between multiple LED pairs.
  To the best of our knowledge, this is the first vehicle-mounted system to
achieve simultaneous VLC and VLP functionalities using a single event camera.
Field experiments were conducted by mounting the system on a vehicle traveling
at 30 km/h (8.3 m/s). The results demonstrated robust real-world performance,
with a root mean square error (RMSE) of distance estimation within 0.75 m for
ranges up to 100 m and a bit error rate (BER) below 0.01 across the same range.

</details>


### [58] [Pole-Image: A Self-Supervised Pole-Anchored Descriptor for Long-Term LiDAR Localization and Map Maintenance](https://arxiv.org/abs/2510.17237)
*Wuhao Xie,Kanji Tanaka*

Main category: cs.RO

TL;DR: 本研究提出了一种基于“杆影像”的新型方法，利用可检测的杆作为锚点来生成周围三维结构的独特特征，从而提高移动机器人的自我定位和地图维护能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统地标方法在高可检测性与高独特性之间的权衡问题，提出了一种新方式来识别移动机器人独特的“特征”。

Method: 提出了一种混合方法“杆影像”，通过LiDAR点云检测杆及其周围环境，生成以杆为原点的二维极坐标图像。

Result: 该方法使得模型能够学习到一种视角不变且高度可区分的描述符，显著提高了自我定位的可靠性和地图维护的灵敏度。

Conclusion: 通过使用杆作为锚点，结合对周围环境的编码，本研究的方法有效提升了机器人的自我定位和地图维护的鲁棒性。

Abstract: Long-term autonomy for mobile robots requires both robust self-localization
and reliable map maintenance. Conventional landmark-based methods face a
fundamental trade-off between landmarks with high detectability but low
distinctiveness (e.g., poles) and those with high distinctiveness but difficult
stable detection (e.g., local point cloud structures). This work addresses the
challenge of descriptively identifying a unique "signature" (local point cloud)
by leveraging a detectable, high-precision "anchor" (like a pole). To solve
this, we propose a novel canonical representation, "Pole-Image," as a hybrid
method that uses poles as anchors to generate signatures from the surrounding
3D structure. Pole-Image represents a pole-like landmark and its surrounding
environment, detected from a LiDAR point cloud, as a 2D polar coordinate image
with the pole itself as the origin. This representation leverages the pole's
nature as a high-precision reference point, explicitly encoding the "relative
geometry" between the stable pole and the variable surrounding point cloud. The
key advantage of pole landmarks is that "detection" is extremely easy. This
ease of detection allows the robot to easily track the same pole, enabling the
automatic and large-scale collection of diverse observational data (positive
pairs). This data acquisition feasibility makes "Contrastive Learning (CL)"
applicable. By applying CL, the model learns a viewpoint-invariant and highly
discriminative descriptor. The contributions are twofold: 1) The descriptor
overcomes perceptual aliasing, enabling robust self-localization. 2) The
high-precision encoding enables high-sensitivity change detection, contributing
to map maintenance.

</details>


### [59] [An adaptive hierarchical control framework for quadrupedal robots in planetary exploration](https://arxiv.org/abs/2510.17249)
*Franek Stark,Rohit Kumar,Shubham Vyas,Hannah Isermann,Jonas Haack,Mihaela Popescu,Jakob Middelberg,Dennis Mronga,Frank Kirchner*

Main category: cs.RO

TL;DR: 该研究开发了一个模块化控制框架，旨在提高四足机器人在未知和极端环境中的导航能力，并成功进行了实地验证。


<details>
  <summary>Details</summary>
Motivation: 由于环境特定控制的需求，四足机器人在未知条件下的部署面临挑战。

Method: 使用基于模型的动态控制、在线模型适应和自适应步态规划来处理机器人和地形属性的不确定性。

Result: 在两种四足平台、多个硬件架构和火山现场测试中进行了验证，机器人成功步行超过700米。

Conclusion: 该研究提出了一个模块化控制框架，能够在未知和不确定环境中有效操作四足机器人。

Abstract: Planetary exploration missions require robots capable of navigating extreme
and unknown environments. While wheeled rovers have dominated past missions,
their mobility is limited to traversable surfaces. Legged robots, especially
quadrupeds, can overcome these limitations by handling uneven, obstacle-rich,
and deformable terrains. However, deploying such robots in unknown conditions
is challenging due to the need for environment-specific control, which is
infeasible when terrain and robot parameters are uncertain. This work presents
a modular control framework that combines model-based dynamic control with
online model adaptation and adaptive footstep planning to address uncertainties
in both robot and terrain properties. The framework includes state estimation
for quadrupeds with and without contact sensing, supports runtime
reconfiguration, and is integrated into ROS 2 with open-source availability.
Its performance was validated on two quadruped platforms, multiple hardware
architectures, and in a volcano field test, where the robot walked over 700 m.

</details>


### [60] [High-Level Multi-Robot Trajectory Planning And Spurious Behavior Detection](https://arxiv.org/abs/2510.17261)
*Fernando Salanova,Jesús Roche,Cristian Mahuela,Eduardo Montijano*

Main category: cs.RO

TL;DR: 提出了一种结合NWN框架和Transformer的用于识别多机器人系统中伪执行的方法，实验显示高准确率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在多机器人系统中，识别错误行为是执行高层任务的关键。

Method: 基于Nets-within-Nets (NWN) 的结构化数据生成框架和Transformer的异常检测管道

Result: 方法在识别执行低效方面的准确率为91.3%，并在核心任务违规和基于约束的异常检测中表现出良好的能力。

Conclusion: 我们的创新方法在复杂任务环境中表现出色，相比于更简单的表示方法，具有更好的性能。

Abstract: The reliable execution of high-level missions in multi-robot systems with
heterogeneous agents, requires robust methods for detecting spurious behaviors.
In this paper, we address the challenge of identifying spurious executions of
plans specified as a Linear Temporal Logic (LTL) formula, as incorrect task
sequences, violations of spatial constraints, timing inconsis- tencies, or
deviations from intended mission semantics. To tackle this, we introduce a
structured data generation framework based on the Nets-within-Nets (NWN)
paradigm, which coordinates robot actions with LTL-derived global mission
specifications. We further propose a Transformer-based anomaly detection
pipeline that classifies robot trajectories as normal or anomalous. Experi-
mental evaluations show that our method achieves high accuracy (91.3%) in
identifying execution inefficiencies, and demonstrates robust detection
capabilities for core mission violations (88.3%) and constraint-based adaptive
anomalies (66.8%). An ablation experiment of the embedding and architecture was
carried out, obtaining successful results where our novel proposition performs
better than simpler representations.

</details>


### [61] [Floating-Base Deep Lagrangian Networks](https://arxiv.org/abs/2510.17270)
*Lucas Schulze,Juliano Decico Negri,Victor Barasuol,Vivian Suzano Medeiros,Marcelo Becker,Jan Peters,Oleg Arenz*

Main category: cs.RO

TL;DR: 本文提出了一种新的浮动基深度拉格朗日网络方法，针对现有模型在浮动基系统中的物理一致性问题，创新性地参数化惯性矩阵并改进预测性能。


<details>
  <summary>Details</summary>
Motivation: 当前灰箱模型在处理浮动基系统时缺乏对特定物理约束的考虑，因此需要引入新的物理一致性机制。

Method: 通过参数化惯性矩阵，结合深度学习和拉格朗日力学，训练神经网络最小化逆动力学误差。

Result: 实验结果表明，FeLaN在多个机器人平台上取得了竞争性性能，同时提供了更好的物理解释性。

Conclusion: FeLaN方法在模拟和真实机器人上表现出色，并增强了物理可解释性，能够更好地应对浮动基系统的挑战。

Abstract: Grey-box methods for system identification combine deep learning with
physics-informed constraints, capturing complex dependencies while improving
out-of-distribution generalization. Yet, despite the growing importance of
floating-base systems such as humanoids and quadrupeds, current grey-box models
ignore their specific physical constraints. For instance, the inertia matrix is
not only positive definite but also exhibits branch-induced sparsity and input
independence. Moreover, the 6x6 composite spatial inertia of the floating base
inherits properties of single-rigid-body inertia matrices. As we show, this
includes the triangle inequality on the eigenvalues of the composite rotational
inertia. To address the lack of physical consistency in deep learning models of
floating-base systems, we introduce a parameterization of inertia matrices that
satisfies all these constraints. Inspired by Deep Lagrangian Networks (DeLaN),
we train neural networks to predict physically plausible inertia matrices that
minimize inverse dynamics error under Lagrangian mechanics. For evaluation, we
collected and released a dataset on multiple quadrupeds and humanoids. In these
experiments, our Floating-Base Deep Lagrangian Networks (FeLaN) achieve highly
competitive performance on both simulated and real robots, while providing
greater physical interpretability.

</details>


### [62] [Intent-Driven LLM Ensemble Planning for Flexible Multi-Robot Disassembly: Demonstration on EV Batteries](https://arxiv.org/abs/2510.17576)
*Cansu Erdogan,Cesar Alan Contreras,Alireza Rastegarpanah,Manolis Chiou,Rustam Stolkin*

Main category: cs.RO

TL;DR: 本论文提出了一种意图驱动的多机器人任务规划流水线，能够根据人类指令生成安全可执行的操作序列，适用于复杂的操作任务。


<details>
  <summary>Details</summary>
Motivation: 解决在非结构化场景中，由多个不同能力的机器人协同执行复杂操作任务的问题。

Method: 提出了一种意图驱动的规划流水线，结合了场景编码、LLM生成动作序列、LLM验证器和一致性过滤器。

Result: 在200个真实场景和600个操作员提示下评估了流水线，结果显示该方法在多机器人任务规划中具有良好的准确性和效率。

Conclusion: 我们的集成验证方法可靠地将操作员意图映射为安全、可执行的多机器人计划，同时保持低用户努力。

Abstract: This paper addresses the problem of planning complex manipulation tasks, in
which multiple robots with different end-effectors and capabilities, informed
by computer vision, must plan and execute concatenated sequences of actions on
a variety of objects that can appear in arbitrary positions and configurations
in unstructured scenes. We propose an intent-driven planning pipeline which can
robustly construct such action sequences with varying degrees of supervisory
input from a human using simple language instructions. The pipeline integrates:
(i) perception-to-text scene encoding, (ii) an ensemble of large language
models (LLMs) that generate candidate removal sequences based on the operator's
intent, (iii) an LLM-based verifier that enforces formatting and precedence
constraints, and (iv) a deterministic consistency filter that rejects
hallucinated objects. The pipeline is evaluated on an example task in which two
robot arms work collaboratively to dismantle an Electric Vehicle battery for
recycling applications. A variety of components must be grasped and removed in
specific sequences, determined by human instructions and/or by task-order
feasibility decisions made by the autonomous system. On 200 real scenes with
600 operator prompts across five component classes, we used metrics of
full-sequence correctness and next-task correctness to evaluate and compare
five LLM-based planners (including ablation analyses of pipeline components).
We also evaluated the LLM-based human interface in terms of time to execution
and NASA TLX with human participant experiments. Results indicate that our
ensemble-with-verification approach reliably maps operator intent to safe,
executable multi-robot plans while maintaining low user effort.

</details>


### [63] [Implicit State Estimation via Video Replanning](https://arxiv.org/abs/2510.17315)
*Po-Chen Ko,Jiayuan Mao,Yu-Hsiang Fu,Hsien-Jeng Yeh,Chu-Rong Chen,Wei-Chiu Ma,Yilun Du,Shao-Hua Sun*

Main category: cs.RO

TL;DR: 提出了一种新框架，通过实时更新和失败过滤，改进视频决策规划在动态环境下的适应能力。


<details>
  <summary>Details</summary>
Motivation: 当前视频规划框架在面对部分观察环境的不确定性时，难以适应交互时间的失败。

Method: 引入交互时间数据集成到规划过程中，允许系统动态适应，而无需显式建模未知状态变量。

Result: 通过对新模拟操作基准的广泛实验，验证了框架在重规划性能上的提升。

Conclusion: 该框架通过在线更新模型参数和过滤失败计划，提高了视频决策的重规划性能。

Abstract: Video-based representations have gained prominence in planning and
decision-making due to their ability to encode rich spatiotemporal dynamics and
geometric relationships. These representations enable flexible and
generalizable solutions for complex tasks such as object manipulation and
navigation. However, existing video planning frameworks often struggle to adapt
to failures at interaction time due to their inability to reason about
uncertainties in partially observed environments. To overcome these
limitations, we introduce a novel framework that integrates interaction-time
data into the planning process. Our approach updates model parameters online
and filters out previously failed plans during generation. This enables
implicit state estimation, allowing the system to adapt dynamically without
explicitly modeling unknown state variables. We evaluate our framework through
extensive experiments on a new simulated manipulation benchmark, demonstrating
its ability to improve replanning performance and advance the field of
video-based decision-making.

</details>


### [64] [DDBot: Differentiable Physics-based Digging Robot for Unknown Granular Materials](https://arxiv.org/abs/2510.17335)
*Xintong Yang,Minglun Wei,Ze Ji,Yu-Kun Lai*

Main category: cs.RO

TL;DR: DDBot是一种新颖的自动化挖掘机器人，通过差分物理模拟器，实现高效能和高精度的颗粒材料操作。


<details>
  <summary>Details</summary>
Motivation: 现有方法在操控颗粒物质时效率和准确性不足，针对这一研究空缺，本文提出DDBot进行高精度的挖掘任务研究。

Method: 利用GPU加速的并行计算和自动微分，DDBot融合了差分技能到动作映射、任务导向的示范方法、梯度剪切与基于线搜索的梯度下降，有效进行系统识别和技能优化。

Result: 提出了一种新的自动化框架DDBot，能高效且精确地操控未知特性的颗粒材料，尤其在挖掘任务中表现出色。

Conclusion: DDBot展示了其在挖掘任务中强大的鲁棒性和效率，具有显著的现实应用潜力。

Abstract: Automating the manipulation of granular materials poses significant
challenges due to complex contact dynamics, unpredictable material properties,
and intricate system states. Existing approaches often fail to achieve
efficiency and accuracy in such tasks. To fill the research gap, this paper
studies the small-scale and high-precision granular material digging task with
unknown physical properties. A new framework, named differentiable digging
robot (DDBot), is proposed to manipulate granular materials, including sand and
soil.
  Specifically, we equip DDBot with a differentiable physics-based simulator,
tailored for granular material manipulation, powered by GPU-accelerated
parallel computing and automatic differentiation. DDBot can perform efficient
differentiable system identification and high-precision digging skill
optimisation for unknown granular materials, which is enabled by a
differentiable skill-to-action mapping, a task-oriented demonstration method,
gradient clipping and line search-based gradient descent.
  Experimental results show that DDBot can efficiently (converge within 5 to 20
minutes) identify unknown granular material dynamics and optimise digging
skills, with high-precision results in zero-shot real-world deployments,
highlighting its practicality. Benchmark results against state-of-the-art
baselines also confirm the robustness and efficiency of DDBot in such digging
tasks.

</details>


### [65] [Interactive Force-Impedance Control](https://arxiv.org/abs/2510.17341)
*Fan Shao,Satoshi Endo,Sandra Hirche,Fanny Ficuciello*

Main category: cs.RO

TL;DR: 本文提出一个基于端口哈密顿框架的统一交互力阻抗控制方法，以确保在人机交互中系统的安全性和有效性，特别是在接触丰富的环境中。


<details>
  <summary>Details</summary>
Motivation: 有效的角色切换依赖于准确估计人类意图，但现有方法在接触丰富的环境中效果差，可能导致系统失去被动性并危及安全。

Method: 基于端口哈密顿框架的控制架构，结合了交互和任务控制端口。

Result: 通过统一交互力阻抗控制框架，确保在与主动人类或非被动环境的物理交互中安全且高效地适应互动功率流。

Conclusion: 提出的统一交互力阻抗控制框架能确保在接触丰富环境中机器人与人类的安全和高效交互，同时保障系统的被动性。

Abstract: Human collaboration with robots requires flexible role adaptation, enabling
robot to switch between active leader and passive follower. Effective role
switching depends on accurately estimating human intention, which is typically
achieved through external force analysis, nominal robot dynamics, or
data-driven approaches. However, these methods are primarily effective in
contact-sparse environments. When robots under hybrid or unified
force-impedance control physically interact with active humans or non-passive
environments, the robotic system may lose passivity and thus compromise safety.
To address this challenge, this paper proposes the unified Interactive
Force-Impedance Control (IFIC) framework that adapts to the interaction power
flow, ensuring effortless and safe interaction in contact-rich environments.
The proposed control architecture is formulated within a port-Hamiltonian
framework, incorporating both interaction and task control ports, through which
system passivity is guaranteed.

</details>


### [66] [Bridging Embodiment Gaps: Deploying Vision-Language-Action Models on Soft Robots](https://arxiv.org/abs/2510.17369)
*Haochen Su,Cristian Meo,Francesco Stella,Andrea Peirone,Kai Junge,Josie Hughes*

Main category: cs.RO

TL;DR: 本研究提出在软机器人的基础上应用VLA模型，以实现安全灵活的人机互动，强调细调的重要性。


<details>
  <summary>Details</summary>
Motivation: 应对机器人在不规则环境中操作时对安全性、适应性及通用性的需求

Method: 部署Vision-Language-Action (VLA)模型于软连续操控器

Result: 通过细致微调和部署流程，表明软机器人在操作任务中表现出色，能够实现自主安全的人机交互

Conclusion: 结合VLA模型与软机器人可以在共享环境中实现安全且灵活的具身AI。

Abstract: Robotic systems are increasingly expected to operate in human-centered,
unstructured environments where safety, adaptability, and generalization are
essential. Vision-Language-Action (VLA) models have been proposed as a language
guided generalized control framework for real robots. However, their deployment
has been limited to conventional serial link manipulators. Coupled by their
rigidity and unpredictability of learning based control, the ability to safely
interact with the environment is missing yet critical. In this work, we present
the deployment of a VLA model on a soft continuum manipulator to demonstrate
autonomous safe human-robot interaction. We present a structured finetuning and
deployment pipeline evaluating two state-of-the-art VLA models (OpenVLA-OFT and
$\pi_0$) across representative manipulation tasks, and show while
out-of-the-box policies fail due to embodiment mismatch, through targeted
finetuning the soft robot performs equally to the rigid counterpart. Our
findings highlight the necessity of finetuning for bridging embodiment gaps,
and demonstrate that coupling VLA models with soft robots enables safe and
flexible embodied AI in human-shared environments.

</details>


### [67] [Integrating Trustworthy Artificial Intelligence with Energy-Efficient Robotic Arms for Waste Sorting](https://arxiv.org/abs/2510.17408)
*Halima I. Kure,Jishna Retnakumari,Augustine O. Nwajana,Umar M. Ismail,Bilyaminu A. Romo,Ehigiator Egho-Promise*

Main category: cs.RO

TL;DR: 本论文提出了一种结合可信赖人工智能与高能效机械臂的废物分类方法，具有高准确率和虚拟排序能力。


<details>
  <summary>Details</summary>
Motivation: 开发一套智能废物分类和排序系统，利用可信赖的人工智能和能源高效的机械臂，以解决城市垃圾管理问题。

Method: 利用MobileNetV2增强的卷积神经网络进行废物分类，并通过机械臂模拟器进行虚拟排序，同时计算每个动作的能量成本以确保高效移动。

Result: 实现了一个高效的智能废物分类和排序系统，具有99.8%的训练准确率和80.5%的验证准确率，能够分类六种废物。

Conclusion: 提出的框架在城市智能垃圾管理系统中是一个可靠且可扩展的解决方案，具备透明性、鲁棒性、公平性和安全性。

Abstract: This paper presents a novel methodology that integrates trustworthy
artificial intelligence (AI) with an energy-efficient robotic arm for
intelligent waste classification and sorting. By utilizing a convolutional
neural network (CNN) enhanced through transfer learning with MobileNetV2, the
system accurately classifies waste into six categories: plastic, glass, metal,
paper, cardboard, and trash. The model achieved a high training accuracy of
99.8% and a validation accuracy of 80.5%, demonstrating strong learning and
generalization. A robotic arm simulator is implemented to perform virtual
sorting, calculating the energy cost for each action using Euclidean distance
to ensure optimal and efficient movement. The framework incorporates key
elements of trustworthy AI, such as transparency, robustness, fairness, and
safety, making it a reliable and scalable solution for smart waste management
systems in urban settings.

</details>


### [68] [From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors](https://arxiv.org/abs/2510.17439)
*Zhengshen Zhang,Hao Li,Yalun Dai,Zhengbang Zhu,Lei Zhou,Chenchen Liu,Dong Wang,Francis E. H. Tay,Sijin Chen,Ziwei Liu,Yuxiao Liu,Xinghang Li,Pan Zhou*

Main category: cs.RO

TL;DR: 提出FALCON模型，通过引入3D空间信息提高VLA模型的表现，兼容性强，性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型在3D环境中的表现受到2D编码器的限制，亟需一种新方法来提高空间推理能力和模型的适应性。

Method: 通过将丰富的3D空间标记引入动作头，并使用Spatial-Enhanced Action Head进行语言推理，从而增强模型的空间表示能力。

Result: 介绍了一种新的视觉-语言-动作 (VLA) 模型架构 FALCON，旨在填补3D空间推理的缺口。

Conclusion: FALCON在多项基准测试和实际任务中表现优异，能够处理复杂的空间条件，证明其在3D空间推理中的有效性。

Abstract: Existing vision-language-action (VLA) models act in 3D real-world but are
typically built on 2D encoders, leaving a spatial reasoning gap that limits
generalization and adaptability. Recent 3D integration techniques for VLAs
either require specialized sensors and transfer poorly across modalities, or
inject weak cues that lack geometry and degrade vision-language alignment. In
this work, we introduce FALCON (From Spatial to Action), a novel paradigm that
injects rich 3D spatial tokens into the action head. FALCON leverages spatial
foundation models to deliver strong geometric priors from RGB alone, and
includes an Embodied Spatial Model that can optionally fuse depth, or pose for
higher fidelity when available, without retraining or architectural changes. To
preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced
Action Head rather than being concatenated into the vision-language backbone.
These designs enable FALCON to address limitations in spatial representation,
modality transferability, and alignment. In comprehensive evaluations across
three simulation benchmarks and eleven real-world tasks, our proposed FALCON
achieves state-of-the-art performance, consistently surpasses competitive
baselines, and remains robust under clutter, spatial-prompt conditioning, and
variations in object scale and height.

</details>


### [69] [A Generalization of Input-Output Linearization via Dynamic Switching Between Melds of Output Functions](https://arxiv.org/abs/2510.17448)
*Mirko Mizzoni,Pieter van Goor,Barbara Bazzana,Antonio Franchi*

Main category: cs.RO

TL;DR: 该论文提出了一种系统框架，用于通过反馈线性化在不同的输出集之间进行切换，确保系统状态的统一有界性.


<details>
  <summary>Details</summary>
Motivation: 探讨如何有效控制非线性系统，使其输出在不同集合之间平滑切换。

Method: 构建了一个系统框架，通过形式化的证明，定义了有效的“meld”和输出切换条件。

Result: 证明了在适当的条件下，可以在不同的输出集之间切换，同时保持系统状态的稳定性。

Conclusion: 该理论可广泛应用于反馈线性化的非线性系统，并在数值模拟中得到验证。

Abstract: This letter presents a systematic framework for switching between different
sets of outputs for the control of nonlinear systems via feedback
linearization. We introduce the concept of a meld to formally define a valid,
feedback-linearizable subset of outputs that can be selected from a larger deck
of possible outputs. The main contribution is a formal proof establishing that
under suitable dwell-time and compatibility conditions, it is possible to
switch between different melds while guaranteeing the uniform boundedness of
the system state. We further show that the error dynamics of the active outputs
remain exponentially stable within each switching interval and that outputs
common to consecutive melds are tracked seamlessly through transitions. The
proposed theory is valid for any feedback linearizable nonlinear system, such
as, e.g., robots, aerial and terrestrial vehicles, etc.. We demonstrate it on a
simple numerical simulation of a robotic manipulator.

</details>


### [70] [HumanMPC - Safe and Efficient MAV Navigation among Humans](https://arxiv.org/abs/2510.17525)
*Simon Schaefer,Helen Oleynikova,Sandra Hirche,Stefan Leutenegger*

Main category: cs.RO

TL;DR: 本研究提出了HumanMPC，一个用于3D微型无人机在复杂人群环境中安全导航的框架，兼具安全性与效率，经过模拟和真实环境验证。


<details>
  <summary>Details</summary>
Motivation: 在日常环境中，安全高效的机器人导航在人类周围至关重要，而现有方法通常只关注简化的2D人群导航，未能充分考虑人体动态的复杂性。

Method: 采用模型预测控制（MPC）框架，利用基于可达性的安全性定义，对初始控制输入进行约束，并建模其在整个规划时间段内的影响。

Result: 提出了HumanMPC框架，用于3D微型无人机在人类中导航，结合了理论安全保障和数据驱动的实时人类运动预测模型。

Conclusion: HumanMPC在安全性、效率和可靠性方面优于基准方法，且可以适应其他平台。

Abstract: Safe and efficient robotic navigation among humans is essential for
integrating robots into everyday environments. Most existing approaches focus
on simplified 2D crowd navigation and fail to account for the full complexity
of human body dynamics beyond root motion. We present HumanMPC, a Model
Predictive Control (MPC) framework for 3D Micro Air Vehicle (MAV) navigation
among humans that combines theoretical safety guarantees with data-driven
models for realistic human motion forecasting. Our approach introduces a novel
twist to reachability-based safety formulation that constrains only the initial
control input for safety while modeling its effects over the entire planning
horizon, enabling safe yet efficient navigation. We validate HumanMPC in both
simulated experiments using real human trajectories and in the real-world,
demonstrating its effectiveness across tasks ranging from goal-directed
navigation to visual servoing for human tracking. While we apply our method to
MAVs in this work, it is generic and can be adapted by other platforms. Our
results show that the method ensures safety without excessive conservatism and
outperforms baseline approaches in both efficiency and reliability.

</details>


### [71] [Distributed Spatial-Temporal Trajectory Optimization for Unmanned-Aerial-Vehicle Swarm](https://arxiv.org/abs/2510.17541)
*Xiaobo Zheng,Pan Tang,Defu Lin,Shaoming He*

Main category: cs.RO

TL;DR: 提出了一种新的分布式轨迹优化算法D-PDDP，能有效处理多无人机的轨迹优化问题，并减少了迭代次数。


<details>
  <summary>Details</summary>
Motivation: 现有方法对多无人机群体的优化存在时间设定和迭代次数多的问题，限制了其应用。

Method: 使用交替方向乘子法（ADMM）和差分动态规划（DDP）构建一个空间-时间轨迹优化框架。

Result: 通过引入PDDP和ADMM，实现了多无人机的充分分布式算法，验证了其在多种仿真场景中的有效性。

Conclusion: 提出的D-PDDP算法有效解决了多无人机共识问题，并减少了算法的迭代次数。

Abstract: Swarm trajectory optimization problems are a well-recognized class of
multi-agent optimal control problems with strong nonlinearity. However, the
heuristic nature of needing to set the final time for agents beforehand and the
time-consuming limitation of the significant number of iterations prohibit the
application of existing methods to large-scale swarm of Unmanned Aerial
Vehicles (UAVs) in practice. In this paper, we propose a spatial-temporal
trajectory optimization framework that accomplishes multi-UAV consensus based
on the Alternating Direction Multiplier Method (ADMM) and uses Differential
Dynamic Programming (DDP) for fast local planning of individual UAVs. The
introduced framework is a two-level architecture that employs Parameterized DDP
(PDDP) as the trajectory optimizer for each UAV, and ADMM to satisfy the local
constraints and accomplish the spatial-temporal parameter consensus among all
UAVs. This results in a fully distributed algorithm called Distributed
Parameterized DDP (D-PDDP). In addition, an adaptive tuning criterion based on
the spectral gradient method for the penalty parameter is proposed to reduce
the number of algorithmic iterations. Several simulation examples are presented
to verify the effectiveness of the proposed algorithm.

</details>


### [72] [Learned Inertial Odometry for Cycling Based on Mixture of Experts Algorithm](https://arxiv.org/abs/2510.17604)
*Hao Qiao,Yan Wang,Shuo Yang,Xiaoyao Yu,Jian kuang,Xiaoji Niu*

Main category: cs.RO

TL;DR: 我们扩展了TLIO用于自行车定位，引入改进的专家模型，显著降低了计算成本和参数数量，同时保持了精度。


<details>
  <summary>Details</summary>
Motivation: 随着共享单车的快速增长和骑行应用的多样化，准确的自行车定位变得至关重要。

Method: 扩展紧致学习惯性里程计(TLIO)至自行车定位，采用改进的专家模型(MoE)以降低训练和推理成本。

Result: 本方法在保持与最先进的LLIO框架相当的精度的同时，减少参数64.7%和计算成本81.8%。

Conclusion: 本研究为自行车定位提供了高效的解决方案，适合移动设备部署。

Abstract: With the rapid growth of bike sharing and the increasing diversity of cycling
applications, accurate bicycle localization has become essential. traditional
GNSS-based methods suffer from multipath effects, while existing inertial
navigation approaches rely on precise modeling and show limited robustness.
Tight Learned Inertial Odometry (TLIO) achieves low position drift by combining
raw IMU data with predicted displacements by neural networks, but its high
computational cost restricts deployment on mobile devices. To overcome this, we
extend TLIO to bicycle localization and introduce an improved Mixture-of
Experts (MoE) model that reduces both training and inference costs. Experiments
show that, compared to the state-of-the-art LLIO framework, our method achieves
comparable accuracy while reducing parameters by 64.7% and computational cost
by 81.8%.

</details>


### [73] [RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation](https://arxiv.org/abs/2510.17640)
*Yuquan Xue,Guanxing Lu,Zhenyu Wu,Chuanrui Zhang,Bofang Jia,Zhengyi Gu,Yansong Tang,Ziwei Wang*

Main category: cs.RO

TL;DR: 提出RESample框架，通过探索性采样增强VLA模型在OOD状态下的恢复能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的模仿学习数据集只包含成功轨迹，缺乏失败或恢复数据，这导致VLA模型在OOD状态下表现不佳。

Method: 通过离线强化学习获取动作值网络，并利用策略回滚从轨迹中采样潜在的OOD状态，设计自适应的探索性采样机制。

Result: 通过广泛的实验证明，RESample显著提升了VLA模型在LIBERO基准和现实机器操作任务中的表现。

Conclusion: RESample框架持续提高了VLA模型的稳定性和泛化能力。

Abstract: Vision-Language-Action models (VLAs) have demonstrated remarkable performance
on complex robotic manipulation tasks through imitation learning. However,
existing imitation learning datasets contain only successful trajectories and
lack failure or recovery data, especially for out-of-distribution (OOD) states
where the robot deviates from the main policy due to minor perturbations or
errors, leading VLA models to struggle with states deviating from the training
distribution. To this end, we propose an automated OOD data augmentation
framework named RESample through exploratory sampling. Specifically, we first
leverage offline reinforcement learning to obtain an action-value network that
accurately identifies sub-optimal actions under the current manipulation
policy. We further sample potential OOD states from trajectories via rollout,
and design an exploratory sampling mechanism that adaptively incorporates these
action proxies into the training dataset to ensure efficiency. Subsequently,
our framework explicitly encourages the VLAs to recover from OOD states and
enhances their robustness against distributional shifts. We conduct extensive
experiments on the LIBERO benchmark as well as real-world robotic manipulation
tasks, demonstrating that RESample consistently improves the stability and
generalization ability of VLA models.

</details>


### [74] [Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats](https://arxiv.org/abs/2510.17783)
*Simeon Adebola,Chung Min Kim,Justin Kerr,Shuangyu Xie,Prithvi Akella,Jose Luis Susa Rincon,Eugen Solowjow,Ken Goldberg*

Main category: cs.RO

TL;DR: Botany-Bot是一个新系统，通过机器人技术和立体摄像，能生成植物的详细数字模型，实现高精度的叶片分割与图像采集。


<details>
  <summary>Details</summary>
Motivation: 传统的商业植物表型系统由于固定相机无法识别许多植物细节，因此需要改进的技术来获取完整的植物信息。

Method: 本研究设计了一个由两个立体相机、数字转盘、工业机器人臂和3D模型组成的系统，结合机器人算法以操纵叶片并拍摄高分辨率图像。

Result: Botany-Bot系统能够生成植物的详细“注释数字双胞胎”，通过使用立体摄像机和工业机器人臂来克服叶片遮挡的问题。

Conclusion: 实验结果表明，Botany-Bot在叶片分割和图像获取方面表现优异，具有广泛的应用前景。

Abstract: Commercial plant phenotyping systems using fixed cameras cannot perceive many
plant details due to leaf occlusion. In this paper, we present Botany-Bot, a
system for building detailed "annotated digital twins" of living plants using
two stereo cameras, a digital turntable inside a lightbox, an industrial robot
arm, and 3D segmentated Gaussian Splat models. We also present robot algorithms
for manipulating leaves to take high-resolution indexable images of occluded
details such as stem buds and the underside/topside of leaves. Results from
experiments suggest that Botany-Bot can segment leaves with 90.8% accuracy,
detect leaves with 86.2% accuracy, lift/push leaves with 77.9% accuracy, and
take detailed overside/underside images with 77.3% accuracy. Code, videos, and
datasets are available at https://berkeleyautomation.github.io/Botany-Bot/.

</details>


### [75] [SoftMimic: Learning Compliant Whole-body Control from Examples](https://arxiv.org/abs/2510.17792)
*Gabriel B. Margolis,Michelle Wang,Nolan Fey,Pulkit Agrawal*

Main category: cs.RO

TL;DR: SoftMimic框架通过逆向运动学生成合规动作数据集，使人形机器人能够在遇到扰动时保持安全和平衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法导致机器人在遇到意外接触时行为脆弱且不安全，因此需要一种更灵活的控制策略。

Method: 引入SoftMimic框架，通过强化学习从示例动作中学习符合人类动作的全身控制策略。

Result: SoftMimic使机器人能够在保持平衡和姿态的同时，对外部力量做出顺应反应，通过生成增强数据集并奖励符合响应的策略，机器人能够有效处理干扰并概括到不同任务。

Conclusion: 我们的方法通过模拟和实际实验验证，展示了机器人与环境的安全有效交互能力。

Abstract: We introduce SoftMimic, a framework for learning compliant whole-body control
policies for humanoid robots from example motions. Imitating human motions with
reinforcement learning allows humanoids to quickly learn new skills, but
existing methods incentivize stiff control that aggressively corrects
deviations from a reference motion, leading to brittle and unsafe behavior when
the robot encounters unexpected contacts. In contrast, SoftMimic enables robots
to respond compliantly to external forces while maintaining balance and
posture. Our approach leverages an inverse kinematics solver to generate an
augmented dataset of feasible compliant motions, which we use to train a
reinforcement learning policy. By rewarding the policy for matching compliant
responses rather than rigidly tracking the reference motion, SoftMimic learns
to absorb disturbances and generalize to varied tasks from a single motion
clip. We validate our method through simulations and real-world experiments,
demonstrating safe and effective interaction with the environment.

</details>


### [76] [Robobench: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models as Embodied Brain](https://arxiv.org/abs/2510.17801)
*Yulin Luo,Chun-Kai Fan,Menghang Dong,Jiayu Shi,Mengdi Zhao,Bo-Wen Zhang,Cheng Chi,Jiaming Liu,Gaole Dai,Rongyu Zhang,Ruichuan An,Kun Wu,Zhengping Che,Shaoxuan Xie,Guocai Yao,Zhongxia Zhao,Pengwei Wang,Guang Liu,Zhongyuan Wang,Tiejun Huang,Shanghang Zhang*

Main category: cs.RO

TL;DR: 本文介绍了RoboBench，一个新基准，旨在全面评估多模态大语言模型在机器人操作中的认知能力，解决现有测试不足之处。


<details>
  <summary>Details</summary>
Motivation: 随着机器人技术的发展，构建能够在复杂环境中进行感知与推理的系统变得愈发重要，需要一个全面的评估标准来分析其认知能力。

Method: 通过定义五个维度（指令理解、感知推理、广义规划、可提供性预测和失败分析），涉及14种能力和6092个问答对，构建了一个现实的评估框架。

Result: 本文提出了RoboBench benchmark，系统评估多模态大语言模型（MLLMs）在动态、非结构化环境中的认知能力，并指出现有基准测试的局限性。

Conclusion: RoboBench为量化高层认知提供了框架，揭示了现有MLLM在指令理解、时空推理和规划中的关键限制，指导下一代MLLM的开发。

Abstract: Building robots that can perceive, reason, and act in dynamic, unstructured
environments remains a core challenge. Recent embodied systems often adopt a
dual-system paradigm, where System 2 handles high-level reasoning while System
1 executes low-level control. In this work, we refer to System 2 as the
embodied brain, emphasizing its role as the cognitive core for reasoning and
decision-making in manipulation tasks. Given this role, systematic evaluation
of the embodied brain is essential. Yet existing benchmarks emphasize execution
success, or when targeting high-level reasoning, suffer from incomplete
dimensions and limited task realism, offering only a partial picture of
cognitive capability. To bridge this gap, we introduce RoboBench, a benchmark
that systematically evaluates multimodal large language models (MLLMs) as
embodied brains. Motivated by the critical roles across the full manipulation
pipeline, RoboBench defines five dimensions-instruction comprehension,
perception reasoning, generalized planning, affordance prediction, and failure
analysis-spanning 14 capabilities, 25 tasks, and 6092 QA pairs. To ensure
realism, we curate datasets across diverse embodiments, attribute-rich objects,
and multi-view scenes, drawing from large-scale real robotic data. For
planning, RoboBench introduces an evaluation framework,
MLLM-as-world-simulator. It evaluate embodied feasibility by simulating whether
predicted plans can achieve critical object-state changes. Experiments on 14
MLLMs reveal fundamental limitations: difficulties with implicit instruction
comprehension, spatiotemporal reasoning, cross-scenario planning, fine-grained
affordance understanding, and execution failure diagnosis. RoboBench provides a
comprehensive scaffold to quantify high-level cognition, and guide the
development of next-generation embodied MLLMs. The project page is in
https://robo-bench.github.io.

</details>
