<div id=toc></div>

# Table of Contents

- [cs.HC](#cs.HC) [Total: 7]
- [cs.RO](#cs.RO) [Total: 23]


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [1] [Uncovering Patterns of Brain Activity from EEG Data Consistently Associated with Cybersickness Using Neural Network Interpretability Maps](https://arxiv.org/abs/2512.20620)
*Jacqueline Yau,Katherine J. Mimnaugh,Evan G. Center,Timo Ojala,Steven M. LaValle,Wenzhen Yuan,Nancy Amato,Minje Kim,Kara Federmeier*

Main category: cs.HC

TL;DR: 通过分析脑电图中的事件相关电位，本文开发了一种新的模型来改善虚拟现实技术中的网络晕动症检测，发现左前额皮层的电幅对分类至关重要。


<details>
  <summary>Details</summary>
Motivation: 随着虚拟现实技术的发展，网络晕动症的检测变得愈加重要，现有的方法缺乏针对性，因此本文旨在通过EEG中的ERP特征更准确地分类网络晕动症。

Method: 本文使用训练的卷积神经网络和变换器模型，结合集成梯度和类激活图进行可视化，以分析脑电图中的事件相关电位。

Result: 本文提出了一种基于脑电图（EEG）和事件相关电位（ERP）的方法，利用卷积神经网络和变换器模型来提高虚拟现实（VR）环境中网络晕动症的分类能力，探明左前额皮层电极的电幅在分类中的重要性。

Conclusion: 左前额皮层位置的电幅在网络晕动症分类中发挥了重要作用，为未来实时监测提供了新的研究方向。

Abstract: Cybersickness poses a serious challenge for users of virtual reality (VR) technology. Consequently, there has been significant effort to track its occurrence during VR use with brain activity through electroencephalography (EEG). However, a significant confound in current methods for detecting sickness from EEG is they do not account for the simultaneous processing of the sickening visual stimulus that is present in the brain data from VR. Using event-related potentials (ERPs) from an auditory stimulus shown to reflect cybersickness impacts, we can more precisely target EEG cybersickness features and use those to achieve better performance in online cybersickness classification. In this article, we introduce a method utilizing trained convolutional neural networks and transformer models and plot interpretability maps from integrated gradients and class activation to give a visual representation of what the model determined was most useful in sickness classification from an EEG dataset consisting of ERPs recorded during the elicitation of cybersickness. Across 12 runs of our method with three different neural networks, the models consistently pointed to a surprising finding: that amplitudes recorded at an electrode placed on the scalp near the left prefrontal cortex were important in the classification of cybersickness. These results help clarify a hidden pattern in other related research and point to exciting opportunities for future investigation: that this scalp location could be used as a tagged feature for better real-time cybersickness classification with EEG. We provide our code at: [anonymized].

</details>


### [2] [Cooperation Through Indirect Reciprocity in Child-Robot Interactions](https://arxiv.org/abs/2512.20621)
*Isabel Neto,Alexandre S. Pires,Filipa Correia,Fernando P. Santos*

Main category: cs.HC

TL;DR: 该研究考察了间接互惠机制在儿童和机器人互动中的适用性，探讨了人工智能如何学习和适应人类的合作策略。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在社交互动中的普及，理解人机合作中的信任和亲社会行为变得至关重要。

Method: 结合实验室实验和理论建模，研究儿童与机器人互动中的间接互惠机制，分析学习算法对人机合作的影响。

Result: 研究发现，间接互惠机制可以扩展到儿童与机器人之间，并且儿童的行为策略能够助力多臂赌博算法的学习，使其能进行有效合作。

Conclusion: 该研究表明，间接互惠机制适用于儿童和机器人之间的协调困境，且儿童的策略对人工智能的合作学习具有重要影响。

Abstract: Social interactions increasingly involve artificial agents, such as conversational or collaborative bots. Understanding trust and prosociality in these settings is fundamental to improve human-AI teamwork. Research in biology and social sciences has identified mechanisms to sustain cooperation among humans. Indirect reciprocity (IR) is one of them. With IR, helping someone can enhance an individual's reputation, nudging others to reciprocate in the future. Transposing IR to human-AI interactions is however challenging, as differences in human demographics, moral judgements, and agents' learning dynamics can affect how interactions are assessed. To study IR in human-AI groups, we combine laboratory experiments and theoretical modelling. We investigate whether 1) indirect reciprocity can be transposed to children-robot interactions; 2) artificial agents can learn to cooperate given children's strategies; and 3) how differences in learning algorithms impact human-AI cooperation. We find that IR extends to children and robots solving coordination dilemmas. Furthermore, we observe that the strategies revealed by children provide a sufficient signal for multi-armed bandit algorithms to learn cooperative actions. Beyond the experimental scenarios, we observe that cooperating through multi-armed bandit algorithms is highly dependent on the strategies revealed by humans.

</details>


### [3] [Pioneering Multimodal Emotion Recognition in the Era of Large Models: From Closed Sets to Open Vocabularies](https://arxiv.org/abs/2512.20938)
*Jing Han,Zhiqiang Gao,Shihao Gao,Jialing Liu,Hongyu Chen,Zixing Zhang,Björn W. Schuller*

Main category: cs.HC

TL;DR: 本研究首次对开放词汇的多模态情感识别进行大规模评估，发现视频模态在性能中尤为重要。


<details>
  <summary>Details</summary>
Motivation: 探索多模态大语言模型在细粒度情感理解中的潜力

Method: 大规模基准研究

Result: 评估19种主流MLLM，在OV-MERD数据集上的表现，揭示视频为关键模态

Conclusion: 研究结果为开放词汇和细粒度情感计算的推进奠定了基准，并为情感AI系统提供了实用指南。

Abstract: Recent advances in multimodal large language models (MLLMs) have demonstrated remarkable multi- and cross-modal integration capabilities. However, their potential for fine-grained emotion understanding remains systematically underexplored. While open-vocabulary multimodal emotion recognition (MER-OV) has emerged as a promising direction to overcome the limitations of closed emotion sets, no comprehensive evaluation of MLLMs in this context currently exists. To address this, our work presents the first large-scale benchmarking study of MER-OV on the OV-MERD dataset, evaluating 19 mainstream MLLMs, including general-purpose, modality-specialized, and reasoning-enhanced architectures. Through systematic analysis of model reasoning capacity, fusion strategies, contextual utilization, and prompt design, we provide key insights into the capabilities and limitations of current MLLMs for MER-OV. Our evaluation reveals that a two-stage, trimodal (audio, video, and text) fusion approach achieves optimal performance in MER-OV, with video emerging as the most critical modality. We further identify a surprisingly narrow gap between open- and closed-source LLMs. These findings establish essential benchmarks and offer practical guidelines for advancing open-vocabulary and fine-grained affective computing, paving the way for more nuanced and interpretable emotion AI systems. Associated code will be made publicly available upon acceptance.

</details>


### [4] [A Design Study Process Model for Medical Visualization](https://arxiv.org/abs/2512.21034)
*Mengjie Fan,Liang Zhou*

Main category: cs.HC

TL;DR: 本文提出了一种新的医疗可视化设计研究过程模型，强调用户和设计阶段的区分，并提供针对医疗问题的指导。


<details>
  <summary>Details</summary>
Motivation: 为了改进和适应复杂多样的医学问题，提出一个医疗可视化设计研究过程模型。

Method: 通过对现有医学可视化研究的文献回顾，并结合自身的跨学科研究经验，识别出医疗可视化特点，以此制定过程模型。

Result: 提出了一种新的医疗可视化设计研究过程模型，强调了不同利益相关者和目标用户的区分以及设计阶段的逻辑分析。

Conclusion: 新模型为跨学科医疗可视化研究提供了系统的理论框架和实践指导。

Abstract: We introduce a design study process model for medical visualization based on the analysis of existing medical visualization and visual analysis works, and our own interdisciplinary research experience. With a literature review of related works covering various data types and applications, we identify features of medical visualization and visual analysis research and formulate our model thereafter. Compared to previous design study process models, our new model emphasizes: distinguishing between different stakeholders and target users before initiating specific designs, distinguishing design stages according to analytic logic or cognitive habits, and classifying task types as inferential or descriptive, and further hypothesis-based or hypothesis-free based on whether they involve multiple subgroups. In addition, our model refines previous models according to the characteristics of medical problems and provides referable guidance for each step. These improvements make the visualization design targeted, generalizable, and operational, which can adapt to the complexity and diversity of medical problems. We apply this model to guide the design of a visual analysis method and reanalyze three medical visualization-related works. These examples suggest that the new process model can provide a systematic theoretical framework and practical guidance for interdisciplinary medical visualization research. We give recommendations that future researchers can refer to, report on reflections on the model, and delineate it from existing models.

</details>


### [5] [When LLMs fall short in Deductive Coding: Model Comparison and Human AI Collaboration Workflow Design](https://arxiv.org/abs/2512.21041)
*Zijian Li,Luzhen Tang,Mengyu Xia,Xinyu Li,Naping Chen,Dragan Gašević,Yizhou Fan*

Main category: cs.HC

TL;DR: 本文研究了大型语言模型在教育领域自动编码中的有效性，发现其在推断性编码方面存在局限性，强调了人机协作的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着生成性人工智能推动教育领域对对话数据的需求增长，自动编码成为提高学习分析效率的有希望的方向。

Method: 本研究比较了小型转换器分类器（如BERT）与大型语言模型在两个数据集上的编码性能，关注对话编码中的不平衡头尾分布。

Result: 研究发现，大型语言模型（LLMs）在推断性编码任务中不如BERT等小型转换器分类器，并且表现出系统性的错误和偏差。

Conclusion: 研究结果表明，虽然大型语言模型在某些方面表现不佳，但人机协作工作流程可以提高编码效率并保持可靠性。

Abstract: With generative artificial intelligence driving the growth of dialogic data in education, automated coding is a promising direction for learning analytics to improve efficiency. This surge highlights the need to understand the nuances of student-AI interactions, especially those rare yet crucial. However, automated coding may struggle to capture these rare codes due to imbalanced data, while human coding remains time-consuming and labour-intensive. The current study examined the potential of large language models (LLMs) to approximate or replace humans in deductive, theory-driven coding, while also exploring how human-AI collaboration might support such coding tasks at scale. We compared the coding performance of small transformer classifiers (e.g., BERT) and LLMs in two datasets, with particular attention to imbalanced head-tail distributions in dialogue codes. Our results showed that LLMs did not outperform BERT-based models and exhibited systematic errors and biases in deductive coding tasks. We designed and evaluated a human-AI collaborative workflow that improved coding efficiency while maintaining coding reliability. Our findings reveal both the limitations of LLMs -- especially their difficulties with semantic similarity and theoretical interpretations and the indispensable role of human judgment -- while demonstrating the practical promise of human-AI collaborative workflows for coding.

</details>


### [6] [Volatile Organic Compounds for Stress Detection: A Scoping Review and Exploratory Feasibility Study with Low-Cost Sensors](https://arxiv.org/abs/2512.21105)
*Nicolai Plintz,Marcus Vetter,Dirk Ifenthaler*

Main category: cs.HC

TL;DR: 本研究探讨了挥发性有机化合物（VOCs）在情感计算中的应用，显示低成本传感器能捕捉与压力相关的表现，但尚需扩大样本量确认结果。


<details>
  <summary>Details</summary>
Motivation: 探讨挥发性有机化合物在情绪识别中的应用，通过低成本技术实现情感计算，提高生活中情绪监测的可行性。

Method: 两项研究：研究1为系统综述，分析了不同生物源中VOCs与情感状态的关系；研究2通过低成本传感器与生理监测联合检测实验室诱导的压力。

Result: 本研究表明，挥发性有机化合物（VOCs）在情绪识别方面有潜力，通过低成本传感器进行情感计算的初步探索提供了实证支持。然而，结果显示了个体间、时间延迟及生理状态对检测的影响，且研究强调了现有技术的局限性。

Conclusion: VOCs在情绪识别中具有应用前景，但需要通过更大规模的样本验证并克服现有技术的局限。

Abstract: Volatile organic compounds (VOCs) represent a novel but underexplored modality for emotion recognition. This paper presents a systematic evidence synthesis and exploratory investigation of VOC-based affective computing using low-cost sensors. Study 1, a systematic scoping review following PRISMA-ScR guidelines, analyzed 16 studies from 610 records across breath, sweat, skin, and urine biosources. Evidence indicates that stress and affective states are reflected in VOC signatures (aldehydes, ketones, fatty acids, sulfur compounds), though with considerable heterogeneity. Current research relies predominantly on laboratory-grade GC-MS or PTR-MS, while wearable sensors provide pattern-level outputs without compound-specific identification - a critical gap for practical systems. Study 2 (n=25) investigated whether low-cost TVOC sensors (BME688, ENS160) combined with physiological monitoring (HR, HRV, GSR) can detect laboratory-induced stress. Exploratory analysis revealed that high cardiovascular reactors exhibited elevated TVOC during arithmetic stress (d=1.38), though requiring replication in larger samples. Substantial interindividual variability emerged (CV>80%), with coupling patterns moderated by baseline emission levels and temporal lags of 30-80 seconds. Random Forest-based multimodal classification achieved 77.3% accuracy (5-fold CV). SHAP analysis indicated VOC sensors contributed 24.9% of model performance. Leave-one-subject-out validation yielded 65.3% accuracy, highlighting the need for individual calibration. This work provides three contributions: (1) comprehensive mapping of VOC biomarker evidence and technological gaps, (2) initial demonstration that low-cost sensors can capture stress-related VOC patterns in multimodal fusion, and (3) identification of key implementation challenges. Findings require replication in larger samples (n>=50).

</details>


### [7] [Learning Factors in AI-Augmented Education: A Comparative Study of Middle and High School Students](https://arxiv.org/abs/2512.21246)
*Gaia Ebli,Bianca Raimondi,Maurizio Gabbrielli*

Main category: cs.HC

TL;DR: 研究了AI辅助学习环境中，经验、清晰度、舒适度和动机四个因素在不同年龄段学生中的关系，发现初中生表现出各因素的强相关性，而高中生则独立评估。


<details>
  <summary>Details</summary>
Motivation: 探讨AI工具在教育中的影响，尤其是不同年龄段学生在AI辅助学习环境中的学习因素关系。

Method: 在真实课堂环境中，通过多方法定量分析（相关性分析和文本挖掘）收集数据，对初中生和高中生的学习因素进行比较分析。

Result: 研究发现初中生和高中生在学习因素间的相关性存在显著差异，初中生各维度间存在强正相关，而高中生则相对独立。

Conclusion: AI增强学习的感知维度在不同年龄段的学生中存在差异，发展阶段影响这些维度间的相互依赖性。

Abstract: The increasing integration of AI tools in education has led prior research to explore their impact on learning processes. Nevertheless, most existing studies focus on higher education and conventional instructional contexts, leaving open questions about how key learning factors are related in AI-mediated learning environments and how these relationships may vary across different age groups. Addressing these gaps, our work investigates whether four critical learning factors, experience, clarity, comfort, and motivation, maintain coherent interrelationships in AI-augmented educational settings, and how the structure of these relationships differs between middle and high school students. The study was conducted in authentic classroom contexts where students interacted with AI tools as part of programming learning activities to collect data on the four learning factors and students' perceptions. Using a multimethod quantitative analysis, which combined correlation analysis and text mining, we revealed markedly different dimensional structures between the two age groups. Middle school students exhibit strong positive correlations across all dimensions, indicating holistic evaluation patterns whereby positive perceptions in one dimension generalise to others. In contrast, high school students show weak or near-zero correlations between key dimensions, suggesting a more differentiated evaluation process in which dimensions are assessed independently. These findings reveal that perception dimensions actively mediate AI-augmented learning and that the developmental stage moderates their interdependencies. This work establishes a foundation for the development of AI integration strategies that respond to learners' developmental levels and account for age-specific dimensional structures in student-AI interactions.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [8] [Anytime Metaheuristic Framework for Global Route Optimization in Expected-Time Mobile Search](https://arxiv.org/abs/2512.20711)
*Jan Mikula,Miroslav Kulich*

Main category: cs.RO

TL;DR: 提出了一种新的Milaps框架，通过结合辅助目标和实时算法，优化静态环境中的移动搜索路径。


<details>
  <summary>Details</summary>
Motivation: 针对静态2D连续环境中移动传感器的全局路径优化问题进行深入研究，以降低寻找隐藏目标的预期时间。

Method: 引入Milaps模型-based解决框架，通过整合新的辅助目标和适应近期的实时元启发式算法。

Result: 在新的大规模数据集上评估显示出比现有最优基线更好的解决方案质量与运行时间的权衡。

Conclusion: 该框架在多种场景中表现出较强的灵活性，同时提供了更快的解决方案生成和更优的全球成本优化。

Abstract: Expected-time mobile search (ETS) is a fundamental robotics task where a mobile sensor navigates an environment to minimize the expected time required to locate a hidden object. Global route optimization for ETS in static 2D continuous environments remains largely underexplored due to the intractability of objective evaluation, stemming from the continuous nature of the environment and the interplay of motion and visibility constraints. Prior work has addressed this through partial discretization, leading to discrete-sensing formulations tackled via utility-greedy heuristics. Others have taken an indirect approach by heuristically approximating the objective using minimum latency problems on fixed graphs, enabling global route optimization via efficient metaheuristics. This paper builds on and significantly extends the latter by introducing Milaps (Minimum latency problems), a model-based solution framework for ETS. Milaps integrates novel auxiliary objectives and adapts a recent anytime metaheuristic for the traveling deliveryman problem, chosen for its strong performance under tight runtime constraints. Evaluations on a novel large-scale dataset demonstrate superior trade-offs between solution quality and runtime compared to state-of-the-art baselines. The best-performing strategy rapidly generates a preliminary solution, assigns static weights to sensing configurations, and optimizes global costs metaheuristically. Additionally, a qualitative study highlights the framework's flexibility across diverse scenarios.

</details>


### [9] [A General Purpose Method for Robotic Interception of Non-Cooperative Dynamic Targets](https://arxiv.org/abs/2512.20769)
*Tanmay P. Patel,Erica L. Tevere,Erik H. Kramer,Rudranarayan M. Mukherjee*

Main category: cs.RO

TL;DR: 本研究提出了一种通用框架，用于动态目标的自主截获，验证了在多种移动平台上的鲁棒性与高效性。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够在有限可观测性和无全球定位的情况下，在不同动态的机器人中适应的通用自主截获框架。

Method: 使用单目相机和特征标进行目标追踪，集成扩展卡尔曼滤波器、历史条件运动预测器和逐步规划器，实现实时截获规划。

Result: 通过模拟和实物实验验证，该方法在多种动态目标任务中表现出低截获误差和高成功率。

Conclusion: 该框架在多个移动平台上表现出色，具备良好的通用性和鲁棒性，适用于动态目标的自主截获。

Abstract: This paper presents a general purpose framework for autonomous, vision-based interception of dynamic, non-cooperative targets, validated across three distinct mobility platforms: an unmanned aerial vehicle (UAV), a four-wheeled ground rover, and an air-thruster spacecraft testbed. The approach relies solely on a monocular camera with fiducials for target tracking and operates entirely in the local observer frame without the need for global information. The core contribution of this work is a streamlined and general approach to autonomous interception that can be adapted across robots with varying dynamics, as well as our comprehensive study of the robot interception problem across heterogenous mobility systems under limited observability and no global localization. Our method integrates (1) an Extended Kalman Filter for relative pose estimation amid intermittent measurements, (2) a history-conditioned motion predictor for dynamic target trajectory propagation, and (3) a receding-horizon planner solving a constrained convex program in real time to ensure time-efficient and kinematically feasible interception paths. Our operating regime assumes that observability is restricted by partial fields of view, sensor dropouts, and target occlusions. Experiments are performed in these conditions and include autonomous UAV landing on dynamic targets, rover rendezvous and leader-follower tasks, and spacecraft proximity operations. Results from simulated and physical experiments demonstrate robust performance with low interception errors (both during station-keeping and upon scenario completion), high success rates under deterministic and stochastic target motion profiles, and real-time execution on embedded processors such as the Jetson Orin, VOXL2, and Raspberry Pi 5. These results highlight the framework's generalizability, robustness, and computational efficiency.

</details>


### [10] [YCB-Handovers Dataset: Analyzing Object Weight Impact on Human Handovers to Adapt Robotic Handover Motion](https://arxiv.org/abs/2512.20847)
*Parag Khanna,Karen Jane Dsouza,Chunyu Wang,Mårten Björkman,Christian Smith*

Main category: cs.RO

TL;DR: 本文介绍了YCB-Handovers数据集，捕捉2771个不同物体重量的手交接运动数据，为人类-机器人协作提供重要支持，强调物体重量对交接动作的影响。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在填补人类-机器人协作研究中的空白，揭示物体重量在交接过程中的重要性，以支持更加直观的机器人运动规划。

Method: 通过捕捉2771个人类间手交接的运动数据，构建YCB-Handovers数据集，涉及不同的物体重量。

Result: YCB-Handovers数据集包含了广泛的物体重量，提供了关于手交接行为和重量变化的深入研究，适用于数据驱动和人类启发的模型。

Conclusion: YCB-Handovers数据集为人类-机器人协作研究提供了关键的运动数据，强调了物体重量对人类交接动作的影响，并为机器人运动规划提供了重要的准备信号。

Abstract: This paper introduces the YCB-Handovers dataset, capturing motion data of 2771 human-human handovers with varying object weights. The dataset aims to bridge a gap in human-robot collaboration research, providing insights into the impact of object weight in human handovers and readiness cues for intuitive robotic motion planning. The underlying dataset for object recognition and tracking is the YCB (Yale-CMU-Berkeley) dataset, which is an established standard dataset used in algorithms for robotic manipulation, including grasping and carrying objects. The YCB-Handovers dataset incorporates human motion patterns in handovers, making it applicable for data-driven, human-inspired models aimed at weight-sensitive motion planning and adaptive robotic behaviors. This dataset covers an extensive range of weights, allowing for a more robust study of handover behavior and weight variation. Some objects also require careful handovers, highlighting contrasts with standard handovers. We also provide a detailed analysis of the object's weight impact on the human reaching motion in these handovers.

</details>


### [11] [Early warning signals for loss of control](https://arxiv.org/abs/2512.20868)
*Jasper J. van Beers,Marten Scheffer,Prashant Solanki,Ingrid A. van de Leemput,Egbert H. van Nes,Coen C. de Visser*

Main category: cs.RO

TL;DR: 本研究提出了一种基于动态指标监测的反馈系统安全监测方法，能够在不依赖系统模型的情况下，识别系统接近不稳定的状态，具有广泛的应用前景。


<details>
  <summary>Details</summary>
Motivation: 传统的工程方法依赖准确的系统模型来计算安全操作指令，但这些在系统受损时失效，因此需要新的监测方法。

Method: 通过动态指标监测反馈系统接近不稳定的情况。

Result: 提出了一种不依赖系统模型的整体安全监测方法，使得可以在反馈系统接近不稳定时进行监控。

Conclusion: 该方法适用于多种受控系统，包括无人机、反应堆、飞机和自驾车，具有重要的实际应用价值。

Abstract: Maintaining stability in feedback systems, from aircraft and autonomous robots to biological and physiological systems, relies on monitoring their behavior and continuously adjusting their inputs. Incremental damage can make such control fragile. This tends to go unnoticed until a small perturbation induces instability (i.e. loss of control). Traditional methods in the field of engineering rely on accurate system models to compute a safe set of operating instructions, which become invalid when the, possibly damaged, system diverges from its model. Here we demonstrate that the approach of such a feedback system towards instability can nonetheless be monitored through dynamical indicators of resilience. This holistic system safety monitor does not rely on a system model and is based on the generic phenomenon of critical slowing down, shown to occur in the climate, biology and other complex nonlinear systems approaching criticality. Our findings for engineered devices opens up a wide range of applications involving real-time early warning systems as well as an empirical guidance of resilient system design exploration, or "tinkering". While we demonstrate the validity using drones, the generic nature of the underlying principles suggest that these indicators could apply across a wider class of controlled systems including reactors, aircraft, and self-driving cars.

</details>


### [12] [Proprioception Enhances Vision Language Model in Generating Captions and Subtask Segmentations for Robot Task](https://arxiv.org/abs/2512.20876)
*Kanata Suzuki,Shota Shimizu,Tetsuya Ogata*

Main category: cs.RO

TL;DR: 本研究评估了视觉语言模型在处理机器人运动信息时的能力，以提高机器人模仿学习的效率。


<details>
  <summary>Details</summary>
Motivation: 验证传统的视觉语言模型在理解机器人运动方面的能力，尤其是低级运动信息的缺失对视频理解的挑战。

Method: 视频标题生成与任务细分

Result: 通过结合机器人的运动数据和图像说明，提出了一种改进的自动标题生成和任务细分方法。

Conclusion: 实验结果表明，结合运动数据可以显著提升视频标题生成和任务细分的性能。

Abstract: From the perspective of future developments in robotics, it is crucial to verify whether foundation models trained exclusively on offline data, such as images and language, can understand the robot motion. In particular, since Vision Language Models (VLMs) do not include low-level motion information from robots in their training datasets, video understanding including trajectory information remains a significant challenge. In this study, we assess two capabilities of VLMs through a video captioning task with low-level robot motion information: (1) automatic captioning of robot tasks and (2) segmentation of a series of tasks. Both capabilities are expected to enhance the efficiency of robot imitation learning by linking language and motion and serve as a measure of the foundation model's performance. The proposed method generates multiple "scene" captions using image captions and trajectory data from robot tasks. The full task caption is then generated by summarizing these individual captions. Additionally, the method performs subtask segmentation by comparing the similarity between text embeddings of image captions. In both captioning tasks, the proposed method aims to improve performance by providing the robot's motion data - joint and end-effector states - as input to the VLM. Simulator experiments were conducted to validate the effectiveness of the proposed method.

</details>


### [13] [Stretchable and High-Precision Optical Tactile Sensor for Trajectory Tracking of Parallel Mechanisms](https://arxiv.org/abs/2512.20888)
*Yiding Nie,Dongliang Fan,Jiatai Huang,Chunyu Liu,Jian S. Dai*

Main category: cs.RO

TL;DR: 开发了一种基于连续光谱过滤原理的可拉伸触觉传感器，具有高空间分辨率和抗干扰能力，适用于软机器人、医疗设备等领域。


<details>
  <summary>Details</summary>
Motivation: 探索在软材料应用中实现高空间分辨率的传感器，克服现有触觉传感器的局限性。

Method: 通过连续光谱过滤原理开发可拉伸触觉传感器，确保高空间和力的分辨率。

Result: 传感器展现出0.996的线性空间响应和7微米的高分辨率，能够实时进行精确轨迹跟踪。

Conclusion: 该传感器在应对拉伸和弯曲时仍能保持高线性空间响应，展现出优越的性能和设计扩展性。

Abstract: Stretchable sensors indicate promising prospects for soft robotics, medical devices, and human-machine interactions due to the high compliance of soft materials. Discrete sensing strategies, including sensor arrays and distributed sensors, are broadly involved in tactile sensors across versatile applications. However, it remains a challenge to achieve high spatial resolution with self-decoupled capacity and insensitivity to other off-axis stimuli for stretchable tactile sensors. Herein, we develop a stretchable tactile sensor based on the proposed continuous spectral-filtering principle, allowing superhigh resolution for applied stimuli. This proposed sensor enables a high-linear spatial response (0.996) even during stretching and bending, and high continuous spatial (7 μm) and force (5 mN) resolutions with design scalability and interaction robustness to survive piercing and cutting. We further demonstrate the sensors' performance by integrating them into a planar parallel mechanism for precise trajectory tracking (rotational resolution: 0.02°) in real time.

</details>


### [14] [Certifiable Alignment of GNSS and Local Frames via Lagrangian Duality](https://arxiv.org/abs/2512.20931)
*Baoshan Song,Matthew Giamou,Penggao Yan,Chunxi Xia,Li-Ta Hsu*

Main category: cs.RO

TL;DR: 本研究提出了一种全球最优的定位解算器，能够在GNSS环境欠佳的情况下，通过将伪距或多普勒测量转化为凸放松问题，有效解决当地最优解和卫星可用性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 由于现有方法依赖于大量卫星，且在GNSS环境恶化时表现不佳，因此需要一种新的方法来优化本地系统的绝对定位。

Method: 将原始框架对齐问题构建为非凸的二次约束二次规划（QCQP）问题，并将其放松为一个凹Lagrangian对偶问题，进而计算原问题的下界。

Result: 仿真和实际实验表明，所提出的方法在具有挑战性的环境中可保证最优解，而其他方法可能失效或收敛到局部最优解。

Conclusion: 该方法在仅使用2颗卫星和2D车辆运动的情况下，能提供可认证的最优解，优于传统的基于速度的VOBA方法和先进的GVINS对齐技术。

Abstract: Estimating the absolute orientation of a local system relative to a global navigation satellite system (GNSS) reference often suffers from local minima and high dependency on satellite availability. Existing methods for this alignment task rely on abundant satellites unavailable in GNSS-degraded environments, or use local optimization methods which cannot guarantee the optimality of a solution. This work introduces a globally optimal solver that transforms raw pseudo-range or Doppler measurements into a convexly relaxed problem. The proposed method is certifiable, meaning it can numerically verify the correctness of the result, filling a gap where existing local optimizers fail. We first formulate the original frame alignment problem as a nonconvex quadratically constrained quadratic program (QCQP) problem and relax the QCQP problem to a concave Lagrangian dual problem that provides a lower cost bound for the original problem. Then we perform relaxation tightness and observability analysis to derive criteria for certifiable optimality of the solution. Finally, simulation and real world experiments are conducted to evaluate the proposed method. The experiments show that our method provides certifiably optimal solutions even with only 2 satellites with Doppler measurements and 2D vehicle motion, while the traditional velocity-based VOBA method and the advanced GVINS alignment technique may fail or converge to local optima without notice. To support the development of GNSS-based navigation techniques in robotics, all code and data are open-sourced at https://github.com/Baoshan-Song/Certifiable-Doppler-alignment.

</details>


### [15] [ETP-R1: Evolving Topological Planning with Reinforcement Fine-tuning for Vision-Language Navigation in Continuous Environments](https://arxiv.org/abs/2512.20940)
*Shuhao Ye,Sitong Mao,Yuxiang Cui,Xuan Yu,Shichao Zhai,Wen Chen,Shunbo Zhou,Rong Xiong,Yue Wang*

Main category: cs.RO

TL;DR: 本文提出ETP-R1框架，通过图基模型与强化学习的结合，超越现有技术，在视觉-语言导航任务中实现新性能。


<details>
  <summary>Details</summary>
Motivation: 虽然现有的图基方法在结构化导航中表现出色，但在利用大规模数据和先进训练技术方面未能与基于大型视觉-语言模型的方法相媲美。

Method: 本文采用了数据扩展和强化细化训练相结合的三阶段训练方法，并利用了Group Relative Policy Optimization算法。

Result: 本文提出了一种名为ETP-R1的框架，旨在通过将数据扩展和强化细化训练（RFT）应用于图基的视觉-语言导航（VLN）模型，来缩小当前基于图的方法与大型视觉-语言模型（LVLM）之间的差距。通过构建高质量的预训练数据集并引入新训练范式，该框架在多个基准上实现了新的性能卓越状态。

Conclusion: ETP-R1框架在大规模训练数据和强化学习策略的结合下，显著提升了基于图的视觉-语言导航模型的性能，成为该领域的新标杆。

Abstract: Vision-Language Navigation in Continuous Environments (VLN-CE) requires an embodied agent to navigate towards target in continuous environments, following natural language instructions. While current graph-based methods offer an efficient, structured approach by abstracting the environment into a topological map and simplifying the action space to waypoint selection, they lag behind methods based on Large Vision-Language Models (LVLMs) in leveraging large-scale data and advanced training paradigms. In this paper, we try to bridge this gap by introducing ETP-R1, a framework that applies the paradigm of scaling up data and Reinforcement Fine-Tuning (RFT) to a graph-based VLN-CE model. To build a strong foundation, we first construct a high-quality, large-scale pretraining dataset using the Gemini API. This dataset consists of diverse, low-hallucination instructions for topological trajectories, providing rich supervision for our graph-based policy to map language to topological paths. This foundation is further strengthened by unifying data from both R2R and RxR tasks for joint pretraining. Building on this, we introduce a three-stage training paradigm, which culminates in the first application of closed-loop, online RFT to a graph-based VLN-CE model, powered by the Group Relative Policy Optimization (GRPO) algorithm. Extensive experiments demonstrate that our approach is highly effective, establishing new state-of-the-art performance across all major metrics on both the R2R-CE and RxR-CE benchmarks. Our code is available at https://github.com/Cepillar/ETP-R1.

</details>


### [16] [From Human Bias to Robot Choice: How Occupational Contexts and Racial Priming Shape Robot Selection](https://arxiv.org/abs/2512.20951)
*Jiangen He,Wanqi Zhang,Jessica Barfield*

Main category: cs.RO

TL;DR: 人工智能代理在职业选择中受到肤色和社会刻板印象的影响，不同领域表现出偏见。


<details>
  <summary>Details</summary>
Motivation: 研究人工智能代理在职业环境中的选择决策是否受到社会偏见的影响。

Method: 进行了两项综合实验，参与者在不同职业背景下选择不同肤色和人性化特征的机器人代理。

Result: 通过两项实验发现，职业背景和刻板印象激活会影响对不同肤色和人性化特征的机器人代理的选择。

Conclusion: 人工选择中存在的职业偏见和基于肤色的歧视直接影响到人-机器人评估，可能会加剧社会不平等。

Abstract: As artificial agents increasingly integrate into professional environments, fundamental questions have emerged about how societal biases influence human-robot selection decisions. We conducted two comprehensive experiments (N = 1,038) examining how occupational contexts and stereotype activation shape robotic agent choices across construction, healthcare, educational, and athletic domains. Participants made selections from artificial agents that varied systematically in skin tone and anthropomorphic characteristics. Our study revealed distinct context-dependent patterns. Healthcare and educational scenarios demonstrated strong favoritism toward lighter-skinned artificial agents, while construction and athletic contexts showed greater acceptance of darker-toned alternatives. Participant race was associated with systematic differences in selection patterns across professional domains. The second experiment demonstrated that exposure to human professionals from specific racial backgrounds systematically shifted later robotic agent preferences in stereotype-consistent directions. These findings show that occupational biases and color-based discrimination transfer directly from human-human to human-robot evaluation contexts. The results highlight mechanisms through which robotic deployment may unintentionally perpetuate existing social inequalities.

</details>


### [17] [Multimodal Sensing for Robot-Assisted Sub-Tissue Feature Detection in Physiotherapy Palpation](https://arxiv.org/abs/2512.20992)
*Tian-Ao Ren,Jorge Garcia,Seongheon Hong,Jared Grinberg,Hojung Choi,Julia Di,Hao Li,Dmitry Grinberg,Mark R. Cutkosky*

Main category: cs.RO

TL;DR: 提出了一种结合触觉成像和力传感器的多模态传感器，提高机器人触诊在软组织中对隐蔽特征的检测能力。


<details>
  <summary>Details</summary>
Motivation: 传统的力传感器在软组织环境中的变异性使得对深层特征的探测不够可靠，本研究旨在通过结合触觉成像技术解决这一问题。

Method: 研究开发了一种紧凑的多模态传感器，集成视觉触觉成像与6轴力-扭矩传感器，并在不同的硅胶模型上进行实验验证。

Result: 本研究提出了一种紧凑的多模态传感器，该传感器将高分辨率视觉触觉成像与6轴力-扭矩传感器结合在一起，以提高机器人触诊的效果。实验结果表明，单独使用力信号在软组织环境中往往会产生模糊反应，而触觉图像能够清晰揭示潜在的结构差异。结合两者的优势能够实现稳健的潜在特征检测和控制精准的触诊过程。

Conclusion: 结合触觉成像和力信号的多模态传感器能够有效提高机器人触诊中对潜在特征的识别能力，同时保持安全接触。

Abstract: Robotic palpation relies on force sensing, but force signals in soft-tissue environments are variable and cannot reliably reveal subtle subsurface features. We present a compact multimodal sensor that integrates high-resolution vision-based tactile imaging with a 6-axis force-torque sensor. In experiments on silicone phantoms with diverse subsurface tendon geometries, force signals alone frequently produce ambiguous responses, while tactile images reveal clear structural differences in presence, diameter, depth, crossings, and multiplicity. Yet accurate force tracking remains essential for maintaining safe, consistent contact during physiotherapeutic interaction. Preliminary results show that combining tactile and force modalities enables robust subsurface feature detection and controlled robotic palpation.

</details>


### [18] [Tracing Energy Flow: Learning Tactile-based Grasping Force Control to Prevent Slippage in Dynamic Object Interaction](https://arxiv.org/abs/2512.21043)
*Cheng-Yu Kuo,Hirofumi Shin,Takamitsu Matsubara*

Main category: cs.RO

TL;DR: 本文提出了一种基于物理的能量抽象模型，使得机器人手可以在不依赖外部传感器的情况下，通过触觉学习来优化抓取力量，减少滑动现象。


<details>
  <summary>Details</summary>
Motivation: 由于传统的抓取力量调节在动态物体交互中存在困难，尤其在物体特性不明和感知不可靠的情况下，本文旨在借鉴人类的触觉能力，使机器人也能快速调节抓取力量。

Method: 研究中采用了一种基于物理的能量抽象模型，以及模型驱动的学习和规划方法，通过触觉传感器实时优化抓取力量。

Result: 该研究提出了一种新方法，利用物理模型来指导机器人手抓取过程中力量调节，从而减少滑动和提高稳定性。

Conclusion: 该方法能够在未事先了解物体特性情况下，以高效的速度学习抓取力量控制，并减少滑动现象，适用于多种运动和物体组合。

Abstract: Regulating grasping force to reduce slippage during dynamic object interaction remains a fundamental challenge in robotic manipulation, especially when objects are manipulated by multiple rolling contacts, have unknown properties (such as mass or surface conditions), and when external sensing is unreliable. In contrast, humans can quickly regulate grasping force by touch, even without visual cues. Inspired by this ability, we aim to enable robotic hands to rapidly explore objects and learn tactile-driven grasping force control under motion and limited sensing. We propose a physics-informed energy abstraction that models the object as a virtual energy container. The inconsistency between the fingers' applied power and the object's retained energy provides a physically grounded signal for inferring slip-aware stability. Building on this abstraction, we employ model-based learning and planning to efficiently model energy dynamics from tactile sensing and perform real-time grasping force optimization. Experiments in both simulation and hardware demonstrate that our method can learn grasping force control from scratch within minutes, effectively reduce slippage, and extend grasp duration across diverse motion-object pairs, all without relying on external sensing or prior object knowledge.

</details>


### [19] [Language-Guided Grasp Detection with Coarse-to-Fine Learning for Robotic Manipulation](https://arxiv.org/abs/2512.21065)
*Zebin Jiang,Tianle Jin,Xiangtong Yao,Alois Knoll,Hu Cao*

Main category: cs.RO

TL;DR: 本研究提出了一种新的语言指导抓取检测方法，通过改进的跨模态融合与动态卷积头，提高了机器人在复杂场景下的抓取能力和准确度。


<details>
  <summary>Details</summary>
Motivation: 解决现有语言条件抓取方法中的浅融合策略导致的语义基础有限和语言意图与视觉抓取推理之间的弱对齐问题。

Method: 提出语言指导的抓取检测（LGGD）和粗细学习范式用于机器人操作。

Result: LGGD超越了现有的语言指导抓取方法，在OCID-VLG和Grasp-Anything++数据集上表现出强大的泛化能力，并在未见物体和多样化语言查询中有效。

Conclusion: LGGD在实际机器人平台上的应用展示了处理准确、条件于语言指令的抓取动作的有效性。

Abstract: Grasping is one of the most fundamental challenging capabilities in robotic manipulation, especially in unstructured, cluttered, and semantically diverse environments. Recent researches have increasingly explored language-guided manipulation, where robots not only perceive the scene but also interpret task-relevant natural language instructions. However, existing language-conditioned grasping methods typically rely on shallow fusion strategies, leading to limited semantic grounding and weak alignment between linguistic intent and visual grasp reasoning.In this work, we propose Language-Guided Grasp Detection (LGGD) with a coarse-to-fine learning paradigm for robotic manipulation. LGGD leverages CLIP-based visual and textual embeddings within a hierarchical cross-modal fusion pipeline, progressively injecting linguistic cues into the visual feature reconstruction process. This design enables fine-grained visual-semantic alignment and improves the feasibility of the predicted grasps with respect to task instructions. In addition, we introduce a language-conditioned dynamic convolution head (LDCH) that mixes multiple convolution experts based on sentence-level features, enabling instruction-adaptive coarse mask and grasp predictions. A final refinement module further enhances grasp consistency and robustness in complex scenes.Experiments on the OCID-VLG and Grasp-Anything++ datasets show that LGGD surpasses existing language-guided grasping methods, exhibiting strong generalization to unseen objects and diverse language queries. Moreover, deployment on a real robotic platform demonstrates the practical effectiveness of our approach in executing accurate, instruction-conditioned grasp actions. The code will be released publicly upon acceptance.

</details>


### [20] [Global End-Effector Pose Control of an Underactuated Aerial Manipulator via Reinforcement Learning](https://arxiv.org/abs/2512.21085)
*Shlok Deshmukh,Javier Alonso-Mora,Sihao Sun*

Main category: cs.RO

TL;DR: 研究了一种轻量级双自由度机械臂与四旋翼结合的控制方法，通过强化学习和动态控制提高其在复杂环境中的操作能力。


<details>
  <summary>Details</summary>
Motivation: 面对无人机重量和机械复杂性限制，探索一种简单且高效的空中操作解决方案。

Method: 采用强化学习训练PPO代理以生成四旋翼加速度、机身速率及关节角目标的前馈指令，通过非线性动态反演和PID控制系统跟踪指令。

Result: 本文研究了一种轻量级的双自由度机械臂，该机械臂通过差分机制安装在四旋翼无人机上，能够实现全六自由度的末端执行器姿态控制。该设计的优点在于简化和减少负载，但也引入了在处理重载和推动任务时的欠驱动和外部干扰敏感性等挑战。

Conclusion: 实验结果表明，学习控制策略在实现丰富接触操作时具有潜力，并在简单、轻量平台上表现出色。

Abstract: Aerial manipulators, which combine robotic arms with multi-rotor drones, face strict constraints on arm weight and mechanical complexity. In this work, we study a lightweight 2-degree-of-freedom (DoF) arm mounted on a quadrotor via a differential mechanism, capable of full six-DoF end-effector pose control. While the minimal design enables simplicity and reduced payload, it also introduces challenges such as underactuation and sensitivity to external disturbances, including manipulation of heavy loads and pushing tasks. To address these, we employ reinforcement learning, training a Proximal Policy Optimization (PPO) agent in simulation to generate feedforward commands for quadrotor acceleration and body rates, along with joint angle targets. These commands are tracked by an incremental nonlinear dynamic inversion (INDI) attitude controller and a PID joint controller, respectively. Flight experiments demonstrate centimeter-level position accuracy and degree-level orientation precision, with robust performance under external force disturbances. The results highlight the potential of learning-based control strategies for enabling contact-rich aerial manipulation using simple, lightweight platforms.

</details>


### [21] [Quadrupped-Legged Robot Movement Plan Generation using Large Language Model](https://arxiv.org/abs/2512.21293)
*Muhtadin,Vincentius Gusti Putu A. B. M.,Ahmad Zaini,Mauridhi Hery Purnomo,I Ketut Eddy Purnama,Chastine Fatichah*

Main category: cs.RO

TL;DR: 本研究提出了一种新型控制框架，利用大型语言模型简化四足机器人导航操作，实验验证显示成功率超过90%。


<details>
  <summary>Details</summary>
Motivation: 传统的四足机器人控制接口对操作人员提出了较高的技术要求，限制了其使用的普及。

Method: 提出分布式架构，将高层指令处理卸载到外部服务器，并通过实时传感器融合生成可执行的ROS导航命令。

Result: 通过整合大型语言模型，提出了一种新的控制框架，实现了基于自然语言的直观导航。

Conclusion: 所提出的基于LLM的规划方法，能够在真实环境中有效支持四足机器人的自主部署。

Abstract: Traditional control interfaces for quadruped robots often impose a high barrier to entry, requiring specialized technical knowledge for effective operation. To address this, this paper presents a novel control framework that integrates Large Language Models (LLMs) to enable intuitive, natural language-based navigation. We propose a distributed architecture where high-level instruction processing is offloaded to an external server to overcome the onboard computational constraints of the DeepRobotics Jueying Lite 3 platform. The system grounds LLM-generated plans into executable ROS navigation commands using real-time sensor fusion (LiDAR, IMU, and Odometry). Experimental validation was conducted in a structured indoor environment across four distinct scenarios, ranging from single-room tasks to complex cross-zone navigation. The results demonstrate the system's robustness, achieving an aggregate success rate of over 90\% across all scenarios, validating the feasibility of offloaded LLM-based planning for autonomous quadruped deployment in real-world settings.

</details>


### [22] [Robust and Efficient MuJoCo-based Model Predictive Control via Web of Affine Spaces Derivatives](https://arxiv.org/abs/2512.21109)
*Chen Liang,Daniel Rakita*

Main category: cs.RO

TL;DR: 本研究提出在MuJoCo MPC中使用WASP导数替代有限差分，有效提升了计算效率，并提供了开源实现支持。


<details>
  <summary>Details</summary>
Motivation: 提高MPC的效率，尤其是在高自由度系统或复杂场景下，减少使用有限差分（FD）带来的时间瓶颈。

Method: 在MJPC中使用Web of Affine Spaces (WASP)导数替代有限差分（FD），以提高计算导数的速度和稳定性。

Result: WASP导数在多种MJPC任务中展示出最大2倍的速度提升，相比有限差分（FD）后端，拥有更高的效率和可靠性。

Conclusion: WASP导数在MJPC中表现出色，提供高效、可靠的性能，支持未来研究的开源实现。

Abstract: MuJoCo is a powerful and efficient physics simulator widely used in robotics. One common way it is applied in practice is through Model Predictive Control (MPC), which uses repeated rollouts of the simulator to optimize future actions and generate responsive control policies in real time. To make this process more accessible, the open source library MuJoCo MPC (MJPC) provides ready-to-use MPC algorithms and implementations built directly on top of the MuJoCo simulator. However, MJPC relies on finite differencing (FD) to compute derivatives through the underlying MuJoCo simulator, which is often a key bottleneck that can make it prohibitively costly for time-sensitive tasks, especially in high-DOF systems or complex scenes. In this paper, we introduce the use of Web of Affine Spaces (WASP) derivatives within MJPC as a drop-in replacement for FD. WASP is a recently developed approach for efficiently computing sequences of accurate derivative approximations. By reusing information from prior, related derivative calculations, WASP accelerates and stabilizes the computation of new derivatives, making it especially well suited for MPC's iterative, fine-grained updates over time. We evaluate WASP across a diverse suite of MJPC tasks spanning multiple robot embodiments. Our results suggest that WASP derivatives are particularly effective in MJPC: it integrates seamlessly across tasks, delivers consistently robust performance, and achieves up to a 2$\mathsf{x}$ speedup compared to an FD backend when used with derivative-based planners, such as iLQG. In addition, WASP-based MPC outperforms MJPC's stochastic sampling-based planners on our evaluation tasks, offering both greater efficiency and reliability. To support adoption and future research, we release an open-source implementation of MJPC with WASP derivatives fully integrated.

</details>


### [23] [SparScene: Efficient Traffic Scene Representation via Sparse Graph Learning for Large-Scale Trajectory Generation](https://arxiv.org/abs/2512.21133)
*Xiaoyu Mo,Jintian Ge,Zifan Wang,Chen Lv,Karl Henrik Johansson*

Main category: cs.RO

TL;DR: SparScene是一个高效的稀疏图学习框架，能够有效表示复杂交通场景，以提高多代理轨迹生成的效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 有效建模复杂场景中众多道路用户和基础设施之间的动态交互是自动驾驶和智能交通系统的核心问题。

Method: 提出了一种稀疏图学习框架SparScene，通过利用车道图拓扑构建代理与车道之间的稀疏连接。

Result: 在Waymo Open Motion Dataset的运动预测基准上，SparScene在效率上表现出色，能够在5毫秒内为超过200个代理生成轨迹，并能够扩展到5000多个代理和17000条车道。

Conclusion: SparScene在效率和可扩展性上表现优越，可以处理大规模交通场景中的多达5000个代理和17000条车道。

Abstract: Multi-agent trajectory generation is a core problem for autonomous driving and intelligent transportation systems. However, efficiently modeling the dynamic interactions between numerous road users and infrastructures in complex scenes remains an open problem. Existing methods typically employ distance-based or fully connected dense graph structures to capture interaction information, which not only introduces a large number of redundant edges but also requires complex and heavily parameterized networks for encoding, thereby resulting in low training and inference efficiency, limiting scalability to large and complex traffic scenes. To overcome the limitations of existing methods, we propose SparScene, a sparse graph learning framework designed for efficient and scalable traffic scene representation. Instead of relying on distance thresholds, SparScene leverages the lane graph topology to construct structure-aware sparse connections between agents and lanes, enabling efficient yet informative scene graph representation. SparScene adopts a lightweight graph encoder that efficiently aggregates agent-map and agent-agent interactions, yielding compact scene representations with substantially improved efficiency and scalability. On the motion prediction benchmark of the Waymo Open Motion Dataset (WOMD), SparScene achieves competitive performance with remarkable efficiency. It generates trajectories for more than 200 agents in a scene within 5 ms and scales to more than 5,000 agents and 17,000 lanes with merely 54 ms of inference time with a GPU memory of 2.9 GB, highlighting its superior scalability for large-scale traffic scenes.

</details>


### [24] [Flocking phase transition and threat responses in bio-inspired autonomous drone swarms](https://arxiv.org/abs/2512.21196)
*Matthieu Verdoucq,Dari Trendafilov,Clément Sire,Ramón Escobedo,Guy Theraulaz,Gautier Hattenberger*

Main category: cs.RO

TL;DR: 基于动物群体的集体运动为自主无人机群提供了强大的设计原则。


<details>
  <summary>Details</summary>
Motivation: 希望通过模仿动物群体的集体运动，提高无人机群的响应能力和灵活性。

Method: 提出了一种生物启发的三维集群算法，利用最小的影响邻居集进行局部对齐和吸引，系统调节交互增益以绘制相图。

Result: 在接触干扰者时，无人机群能快速集体转向、瞬时扩展，并在几秒内可靠地恢复高对齐度。

Conclusion: 通过简单的局部交互规则，可以生成多种集体相，并通过增益调节来有效地调整无人机群的稳定性、灵活性和韧性。

Abstract: Collective motion inspired by animal groups offers powerful design principles for autonomous aerial swarms. We present a bio-inspired 3D flocking algorithm in which each drone interacts only with a minimal set of influential neighbors, relying solely on local alignment and attraction cues. By systematically tuning these two interaction gains, we map a phase diagram revealing sharp transitions between swarming and schooling, as well as a critical region where susceptibility, polarization fluctuations, and reorganization capacity peak. Outdoor experiments with a swarm of ten drones, combined with simulations using a calibrated flight-dynamics model, show that operating near this transition enhances responsiveness to external disturbances. When confronted with an intruder, the swarm performs rapid collective turns, transient expansions, and reliably recovers high alignment within seconds. These results demonstrate that minimal local-interaction rules are sufficient to generate multiple collective phases and that simple gain modulation offers an efficient mechanism to adjust stability, flexibility, and resilience in drone swarms.

</details>


### [25] [Schrödinger's Navigator: Imagining an Ensemble of Futures for Zero-Shot Object Navigation](https://arxiv.org/abs/2512.21201)
*Yu He,Da Huang,Zhenyang Liu,Zixiao Gu,Qiang Sun,Guangnan Ye,Yanwei Fu*

Main category: cs.RO

TL;DR: 本研究提出的Schrödinger's Navigator框架通过轨迹条件的三维想象，提升了零-shot对象导航的效果，尤其在复杂的遮挡和动态环境中表现突出。


<details>
  <summary>Details</summary>
Motivation: 解决现有零-shot对象导航方法在真实、杂乱环境中面临的挑战，尤其是在存在重度遮挡和动态目标的情况下。

Method: 提出了一种基于轨迹条件的三维世界模型，利用未来观察的想象来引导导航。

Result: 在Go2四足机器人上进行的实验表明，Schrödinger's Navigator在自我定位、目标定位和重度遮挡环境中的成功率上优于其他基准方法。

Conclusion: Schrödinger's Navigator在复杂环境中的零-shot目标导航表现优越，能够有效处理遮挡和动态目标带来的挑战。

Abstract: Zero-shot object navigation (ZSON) requires a robot to locate a target object in a previously unseen environment without relying on pre-built maps or task-specific training. However, existing ZSON methods often struggle in realistic and cluttered environments, particularly when the scene contains heavy occlusions, unknown risks, or dynamically moving target objects. To address these challenges, we propose \textbf{Schrödinger's Navigator}, a navigation framework inspired by Schrödinger's thought experiment on uncertainty. The framework treats unobserved space as a set of plausible future worlds and reasons over them before acting. Conditioned on egocentric visual inputs and three candidate trajectories, a trajectory-conditioned 3D world model imagines future observations along each path. This enables the agent to see beyond occlusions and anticipate risks in unseen regions without requiring extra detours or dense global mapping. The imagined 3D observations are fused into the navigation map and used to update a value map. These updates guide the policy toward trajectories that avoid occlusions, reduce exposure to uncertain space, and better track moving targets. Experiments on a Go2 quadruped robot across three challenging scenarios, including severe static occlusions, unknown risks, and dynamically moving targets, show that Schrödinger's Navigator consistently outperforms strong ZSON baselines in self-localization, object localization, and overall Success Rate in occlusion-heavy environments. These results demonstrate the effectiveness of trajectory-conditioned 3D imagination in enabling robust zero-shot object navigation.

</details>


### [26] [Wireless Center of Pressure Feedback System for Humanoid Robot Balance Control using ESP32-C3](https://arxiv.org/abs/2512.21219)
*Muhtadin,Faris Rafi Pramana,Dion Hayu Fandiantoro,Moh Ismarintan Zazuli,Atar Fuady Babgei*

Main category: cs.RO

TL;DR: 本研究设计了一种无线平衡系统，提高了人形机器人在复杂舞蹈动作中的稳定性，实验结果显示高精度和100%成功率。


<details>
  <summary>Details</summary>
Motivation: 维护单脚支撑阶段的稳定性是人形机器人面临的基本挑战，尤其是在需要复杂动作和高机械灵活性的舞蹈机器人中。

Method: 研究采用自定义设计的足部单元，集成四个负载传感器和ESP32-C3微控制器，实时估算压力中心（CoP），并通过无线方式将数据传输给主控制器，使用PID控制策略调整机器人各关节。

Result: 实验表明传感器精度高，平均测量误差为14.8克，同时优化后的控制系统在3度倾斜单脚抬起任务中实现了100%的平衡保持成功率。

Conclusion: 该研究验证了无线CoP反馈在提高人形机器人站姿稳定性方面的有效性，同时不影响机械灵活性。

Abstract: Maintaining stability during the single-support phase is a fundamental challenge in humanoid robotics, particularly in dance robots that require complex maneuvers and high mechanical freedom. Traditional tethered sensor configurations often restrict joint movement and introduce mechanical noises. This study proposes a wireless embedded balance system designed to maintain stability on uneven surfaces. The system utilizes a custom-designed foot unit integrated with four load cells and an ESP32-C3 microcontroller to estimate the Center of Pressure (CoP) in real time. The CoP data were transmitted wirelessly to the main controller to minimize the wiring complexity of the 29-DoF VI-ROSE humanoid robot. A PID control strategy is implemented to adjust the torso, hip, and ankle roll joints based on CoP feedback. Experimental characterization demonstrated high sensor precision with an average measurement error of 14.8 g. Furthermore, the proposed control system achieved a 100% success rate in maintaining balance during single-leg lifting tasks at a 3-degree inclination with optimized PID parameters (Kp=0.10, Kd=0.005). These results validate the efficacy of wireless CoP feedback in enhancing the postural stability of humanoid robots, without compromising their mechanical flexibility.

</details>


### [27] [Relative Localization System Design for SnailBot: A Modular Self-reconfigurable Robot](https://arxiv.org/abs/2512.21226)
*Shuhan Zhang,Tin Lun Lam*

Main category: cs.RO

TL;DR: 本研究开发了一种适用于SnailBot的相对定位系统，结合多个技术以提高协作机器人的定位精度和可靠性.


<details>
  <summary>Details</summary>
Motivation: 为了提高模块化自重构机器人的协作能力和定位准确性，设计了一种集成多种定位技术的系统.

Method: 采用ArUco标记识别、光流分析与IMU数据处理的融合框架，进行实时定位和数据分析.

Result: 本文提出了SnailBot的相对定位系统的设计与实现，该系统为一种模块化自重构机器人. 系统将ArUco标记识别、光流分析和IMU数据处理集成到一个统一的融合框架中，使得在协作机器人任务中能够实现稳健和精准的相对定位. 实验验证了该系统在实时操作中的有效性，基于规则的融合策略确保在动态场景下的可靠性. 结果突显了该系统在模块化机器人系统中的可扩展部署潜力.

Conclusion: 该相对定位系统通过多种数据融合技术，展现了在复杂场景下的高效能，并为模块化机器人系统的应用提供了新的可能性.

Abstract: This paper presents the design and implementation of a relative localization system for SnailBot, a modular self reconfigurable robot. The system integrates ArUco marker recognition, optical flow analysis, and IMU data processing into a unified fusion framework, enabling robust and accurate relative positioning for collaborative robotic tasks. Experimental validation demonstrates the effectiveness of the system in realtime operation, with a rule based fusion strategy ensuring reliability across dynamic scenarios. The results highlight the potential for scalable deployment in modular robotic systems.

</details>


### [28] [UniTacHand: Unified Spatio-Tactile Representation for Human to Robotic Hand Skill Transfer](https://arxiv.org/abs/2512.21233)
*Chi Zhang,Penglin Cai,Haoqi Yuan,Chaoyi Xu,Zongqing Lu*

Main category: cs.RO

TL;DR: 本研究提出了一种名为UniTacHand的方法，通过统一的人体和机器人触觉数据表示，解决了大规模触觉数据收集的挑战，以实现人类政策的零-shot转移至机器人。


<details>
  <summary>Details</summary>
Motivation: 在视觉遮挡的情况下，触觉感知对机器人抓取的灵巧操控至关重要，但收集大规模的真实触觉数据存在困难。

Method: 通过将人体和机器人手的触觉信号投影到一致的二维表面空间，利用对比学习方法将其对齐到统一的潜在空间。

Result: 实现了从人类到真实机器人进行零-shot触觉政策转移，并在混合数据上的联合训练提高了性能和数据效率。

Conclusion: UniTacHand为触觉基础的灵巧手学习提供了一种普遍、可扩展且高效的数据学习路径。

Abstract: Tactile sensing is crucial for robotic hands to achieve human-level dexterous manipulation, especially in scenarios with visual occlusion. However, its application is often hindered by the difficulty of collecting large-scale real-world robotic tactile data. In this study, we propose to collect low-cost human manipulation data using haptic gloves for tactile-based robotic policy learning. The misalignment between human and robotic tactile data makes it challenging to transfer policies learned from human data to robots. To bridge this gap, we propose UniTacHand, a unified representation to align robotic tactile information captured by dexterous hands with human hand touch obtained from gloves. First, we project tactile signals from both human hands and robotic hands onto a morphologically consistent 2D surface space of the MANO hand model. This unification standardizes the heterogeneous data structures and inherently embeds the tactile signals with spatial context. Then, we introduce a contrastive learning method to align them into a unified latent space, trained on only 10 minutes of paired data from our data collection system. Our approach enables zero-shot tactile-based policy transfer from humans to a real robot, generalizing to objects unseen in the pre-training data. We also demonstrate that co-training on mixed data, including both human and robotic demonstrations via UniTacHand, yields better performance and data efficiency compared with using only robotic data. UniTacHand paves a path toward general, scalable, and data-efficient learning for tactile-based dexterous hands.

</details>


### [29] [RoboCade: Gamifying Robot Data Collection](https://arxiv.org/abs/2512.21235)
*Suvir Mirchandani,Mia Tang,Jiafei Duan,Jubayer Ibn Hamid,Michael Cho,Dorsa Sadigh*

Main category: cs.RO

TL;DR: 本文提出游戏化数据收集平台 RoboCade，通过加强用户参与感，提升数据收集效率和收集质量。


<details>
  <summary>Details</summary>
Motivation: 收集演示数据成本高昂，数据规模受限

Method: 开发一个游戏化的远程遥控平台 RoboCade 来收集机器人数据

Result: 通过该平台收集的数据可提高机器人在非游戏化任务中的成功率，用户更乐于使用游戏化平台

Conclusion: 游戏化数据收集方法为收集演示数据提供了可扩展且吸引人的解决方案。

Abstract: Imitation learning from human demonstrations has become a dominant approach for training autonomous robot policies. However, collecting demonstration datasets is costly: it often requires access to robots and needs sustained effort in a tedious, long process. These factors limit the scale of data available for training policies. We aim to address this scalability challenge by involving a broader audience in a gamified data collection experience that is both accessible and motivating. Specifically, we develop a gamified remote teleoperation platform, RoboCade, to engage general users in collecting data that is beneficial for downstream policy training. To do this, we embed gamification strategies into the design of the system interface and data collection tasks. In the system interface, we include components such as visual feedback, sound effects, goal visualizations, progress bars, leaderboards, and badges. We additionally propose principles for constructing gamified tasks that have overlapping structure with useful downstream target tasks. We instantiate RoboCade on three manipulation tasks -- including spatial arrangement, scanning, and insertion. To illustrate the viability of gamified robot data collection, we collect a demonstration dataset through our platform, and show that co-training robot policies with this data can improve success rate on non-gamified target tasks (+16-56%). Further, we conduct a user study to validate that novice users find the gamified platform significantly more enjoyable than a standard non-gamified platform (+24%). These results highlight the promise of gamified data collection as a scalable, accessible, and engaging method for collecting demonstration data.

</details>


### [30] [LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation](https://arxiv.org/abs/2512.21243)
*Anatoly O. Onishchenko,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.RO

TL;DR: 提出了一种新方法LookPlanGraph，通过实时更新场景图以适应环境变化，提升了机器人任务执行的效果，并引入了包含514个任务的GraSIF数据集。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于预构建的场景图，假设任务相关信息在规划开始时是可用的，但未考虑执行任务时环境的变化。

Method: LookPlanGraph利用静态资产和对象先验构建场景图，在执行计划时通过视觉语言模型不断更新图形，处理代理的自我视角摄像机视图。

Result: LookPlanGraph方法在动态调整场景图方面优于基于静态场景图的方法，且在真实世界环境中表现出实际可行性。

Conclusion: LookPlanGraph在多个实验环境中表现优于传统方法，验证了其在动态环境中的有效性和适用性。

Abstract: Methods that use Large Language Models (LLM) as planners for embodied instruction following tasks have become widespread. To successfully complete tasks, the LLM must be grounded in the environment in which the robot operates. One solution is to use a scene graph that contains all the necessary information. Modern methods rely on prebuilt scene graphs and assume that all task-relevant information is available at the start of planning. However, these approaches do not account for changes in the environment that may occur between the graph construction and the task execution. We propose LookPlanGraph - a method that leverages a scene graph composed of static assets and object priors. During plan execution, LookPlanGraph continuously updates the graph with relevant objects, either by verifying existing priors or discovering new entities. This is achieved by processing the agents egocentric camera view using a Vision Language Model. We conducted experiments with changed object positions VirtualHome and OmniGibson simulated environments, demonstrating that LookPlanGraph outperforms methods based on predefined static scene graphs. To demonstrate the practical applicability of our approach, we also conducted experiments in a real-world setting. Additionally, we introduce the GraSIF (Graph Scenes for Instruction Following) dataset with automated validation framework, comprising 514 tasks drawn from SayPlan Office, BEHAVIOR-1K, and VirtualHome RobotHow. Project page available at https://lookplangraph.github.io .

</details>
