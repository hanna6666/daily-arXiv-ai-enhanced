{"id": "2512.06002", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06002", "abs": "https://arxiv.org/abs/2512.06002", "authors": ["Evan Conway", "David Porfirio", "David Chan", "Mark Roberts", "Laura M. Hiatt"], "title": "POrTAL: Plan-Orchestrated Tree Assembly for Lookahead", "comment": "Submitted to ICRA 26", "summary": "Assigning tasks to robots often involves supplying the robot with an overarching goal, such as through natural language, and then relying on the robot to uncover and execute a plan to achieve that goal. In many settings common to human-robot interaction, however, the world is only partially observable to the robot, requiring that it create plans under uncertainty. Although many probabilistic planning algorithms exist for this purpose, these algorithms can be inefficient if executed with the robot's limited computational resources, or may require more steps than expected to achieve the goal. We thereby created a new, lightweight, probabilistic planning algorithm, Plan-Orchestrated Tree Assembly for Lookahead (POrTAL), that combines the strengths of two baseline planning algorithms, FF-Replan and POMCP. In a series of case studies, we demonstrate POrTAL's ability to quickly arrive at solutions that outperform these baselines in terms of number of steps. We additionally demonstrate how POrTAL performs under varying temporal constraints.", "AI": {"tldr": "POrTAL\u662f\u4e00\u79cd\u65b0\u578b\u7684\u8f7b\u91cf\u7ea7\u6982\u7387\u89c4\u5212\u7b97\u6cd5\uff0c\u80fd\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u73af\u5883\u4e2d\u9ad8\u6548\u6267\u884c\u4efb\u52a1\uff0c\u8d85\u8d8a\u4f20\u7edf\u7b97\u6cd5\u7684\u6548\u7387\u3002", "motivation": "\u5728\u8457\u540d\u7684\u673a\u5668\u4eba\u4efb\u52a1\u5206\u914d\u4e2d\uff0c\u673a\u5668\u4eba\u9700\u8981\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u7684\u73af\u5883\u4e2d\u6267\u884c\u8ba1\u5212\uff0c\u56e0\u6b64\u9700\u8981\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u5236\u5b9a\u8ba1\u5212\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u79cd\u65b0\u7684\u8f7b\u91cf\u7ea7\u6982\u7387\u89c4\u5212\u7b97\u6cd5\uff0c\u79f0\u4e3aPlan-Orchestrated Tree Assembly for Lookahead\uff08POrTAL\uff09\uff0c\u7ed3\u5408\u4e86FF-Replan\u548cPOMCP\u7684\u4f18\u52bf\u3002", "result": "\u5728\u4e00\u7cfb\u5217\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cPOrTAL\u5728\u89e3\u51b3\u65b9\u6848\u7684\u6b65\u9aa4\u6570\u4e0a\u4f18\u4e8e\u57fa\u51c6\u7b97\u6cd5\u3002", "conclusion": "POrTAL\u5728\u591a\u79cd\u65f6\u95f4\u7ea6\u675f\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u80fd\u591f\u5feb\u901f\u627e\u5230\u89e3\u51b3\u65b9\u6848\uff0c\u8d85\u8d8a\u57fa\u51c6\u7b97\u6cd5\u7684\u6b65\u9aa4\u6570\u3002"}}
{"id": "2512.06017", "categories": ["cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.06017", "abs": "https://arxiv.org/abs/2512.06017", "authors": ["Laurence Liang"], "title": "Training-Free Robot Pose Estimation using Off-the-Shelf Foundational Models", "comment": "Accepted at CVIS 2025", "summary": "Pose estimation of a robot arm from visual inputs is a challenging task. However, with the increasing adoption of robot arms for both industrial and residential use cases, reliable joint angle estimation can offer improved safety and performance guarantees, and also be used as a verifier to further train robot policies. This paper introduces using frontier vision-language models (VLMs) as an ``off-the-shelf\" tool to estimate a robot arm's joint angles from a single target image. By evaluating frontier VLMs on both synthetic and real-world image-data pairs, this paper establishes a performance baseline attained by current FLMs. In addition, this paper presents empirical results suggesting that test time scaling or parameter scaling alone does not lead to improved joint angle predictions.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f30\u8ba1\u673a\u5668\u4eba\u624b\u81c2\u5173\u8282\u89d2\u5ea6\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u5efa\u7acb\u4e86\u6027\u80fd\u57fa\u51c6\uff0c\u4f46\u53d1\u73b0\u67d0\u4e9b\u7f29\u653e\u7b56\u7565\u672a\u80fd\u6539\u5584\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u624b\u81c2\u5728\u5de5\u4e1a\u548c\u5bb6\u5ead\u5e94\u7528\u4e2d\u7684\u666e\u53ca\uff0c\u53ef\u9760\u7684\u5173\u8282\u89d2\u5ea6\u4f30\u8ba1\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u4ee5\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u6027\u80fd\u3002", "method": "\u8bc4\u4f30\u524d\u6cbf\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5408\u6210\u548c\u771f\u5b9e\u56fe\u50cf\u6570\u636e\u5bf9\u4e0a\u7684\u8868\u73b0\uff0c\u4ee5\u5efa\u7acb\u5f53\u524dFLMs\u7684\u6027\u80fd\u57fa\u51c6\u3002", "result": "\u901a\u8fc7\u4f7f\u7528\u524d\u6cbf\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\uff0c\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u9760\u7684\u673a\u5668\u4eba\u624b\u81c2\u5173\u8282\u89d2\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4ec5\u9700\u4ece\u5355\u4e00\u76ee\u6807\u56fe\u50cf\u4e2d\u63d0\u53d6\u4fe1\u606f\u3002", "conclusion": "\u672c\u6587\u7684\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u4ec5\u901a\u8fc7\u6d4b\u8bd5\u65f6\u95f4\u7f29\u653e\u6216\u53c2\u6570\u7f29\u653e\u5e76\u4e0d\u4f1a\u63d0\u9ad8\u5173\u8282\u89d2\u5ea6\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2512.06038", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06038", "abs": "https://arxiv.org/abs/2512.06038", "authors": ["Kelsey Fontenot", "Anjali Gorti", "Iva Goel", "Tonio Buonassisi", "Alexander E. Siemenn"], "title": "Closed-Loop Robotic Manipulation of Transparent Substrates for Self-Driving Laboratories using Deep Learning Micro-Error Correction", "comment": "15 pages, 8 figures", "summary": "Self-driving laboratories (SDLs) have accelerated the throughput and automation capabilities for discovering and improving chemistries and materials. Although these SDLs have automated many of the steps required to conduct chemical and materials experiments, a commonly overlooked step in the automation pipeline is the handling and reloading of substrates used to transfer or deposit materials onto for downstream characterization. Here, we develop a closed-loop method of Automated Substrate Handling and Exchange (ASHE) using robotics, dual-actuated dispensers, and deep learning-driven computer vision to detect and correct errors in the manipulation of fragile and transparent substrates for SDLs. Using ASHE, we demonstrate a 98.5% first-time placement accuracy across 130 independent trials of reloading transparent glass substrates into an SDL, where only two substrate misplacements occurred and were successfully detected as errors and automatically corrected. Through the development of more accurate and reliable methods for handling various types of substrates, we move toward an improvement in the automation capabilities of self-driving laboratories, furthering the acceleration of novel chemical and materials discoveries.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff08ASHE\uff09\u6765\u63d0\u9ad8\u81ea\u9a7e\u5b9e\u9a8c\u5ba4\u4e2d\u8106\u5f31\u900f\u660e\u57fa\u6750\u7684\u5904\u7406\u7cbe\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9e\u9a8c\u81ea\u52a8\u5316\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u81ea\u9a7e\u5b9e\u9a8c\u5ba4\u5df2\u7ecf\u81ea\u52a8\u5316\u4e86\u8bb8\u591a\u5316\u5b66\u548c\u6750\u6599\u5b9e\u9a8c\u7684\u6b65\u9aa4\uff0c\u4f46\u57fa\u6750\u7684\u5904\u7406\u548c\u91cd\u65b0\u52a0\u8f7d\u4ecd\u5e38\u5e38\u88ab\u5ffd\u89c6\uff0c\u56e0\u6b64\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u5904\u7406\u65b9\u6cd5\u662f\u5fc5\u8981\u7684\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u57fa\u6750\u5904\u7406\u4e0e\u4ea4\u6362\uff08ASHE\uff09\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u673a\u5668\u4eba\u3001\u53cc\u52a8\u4f5c\u7528\u91cf\u5668\u548c\u6df1\u5ea6\u5b66\u4e60\u9a71\u52a8\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u6765\u68c0\u6d4b\u548c\u7ea0\u6b63\u8106\u5f31\u900f\u660e\u57fa\u6750\u7684\u64cd\u4f5c\u9519\u8bef\u3002", "result": "ASHE\u5728130\u4e2a\u72ec\u7acb\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u4e8698.5%\u7684\u9996\u6b21\u653e\u7f6e\u51c6\u786e\u7387\uff0c\u4ec5\u53d1\u751f\u4e86\u4e24\u4e2a\u57fa\u6750\u9519\u8bef\u653e\u7f6e\uff0c\u5e76\u6210\u529f\u68c0\u6d4b\u548c\u81ea\u52a8\u4fee\u6b63\u3002", "conclusion": "\u901a\u8fc7\u5f00\u53d1\u66f4\u7cbe\u786e\u53ef\u9760\u7684\u57fa\u6750\u5904\u7406\u65b9\u6cd5\uff0c\u6211\u4eec\u63a8\u52a8\u4e86\u81ea\u9a7e\u5b9e\u9a8c\u5ba4\u7684\u81ea\u52a8\u5316\u80fd\u529b\uff0c\u4e3a\u65b0\u578b\u5316\u5b66\u548c\u6750\u6599\u7684\u53d1\u73b0\u52a0\u901f\u8fdb\u7a0b\u3002"}}
{"id": "2512.06112", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06112", "abs": "https://arxiv.org/abs/2512.06112", "authors": ["Yifang Xu", "Jiahao Cui", "Feipeng Cai", "Zhihao Zhu", "Hanlin Shang", "Shan Luan", "Mingwang Xu", "Neng Zhang", "Yaoyi Li", "Jia Cai", "Siyu Zhu"], "title": "WAM-Flow: Parallel Coarse-to-Fine Motion Planning via Discrete Flow Matching for Autonomous Driving", "comment": "18 pages, 11 figures", "summary": "We introduce WAM-Flow, a vision-language-action (VLA) model that casts ego-trajectory planning as discrete flow matching over a structured token space. In contrast to autoregressive decoders, WAM-Flow performs fully parallel, bidirectional denoising, enabling coarse-to-fine refinement with a tunable compute-accuracy trade-off. Specifically, the approach combines a metric-aligned numerical tokenizer that preserves scalar geometry via triplet-margin learning, a geometry-aware flow objective and a simulator-guided GRPO alignment that integrates safety, ego progress, and comfort rewards while retaining parallel generation. A multi-stage adaptation converts a pre-trained auto-regressive backbone (Janus-1.5B) from causal decoding to non-causal flow model and strengthens road-scene competence through continued multimodal pretraining. Thanks to the inherent nature of consistency model training and parallel decoding inference, WAM-Flow achieves superior closed-loop performance against autoregressive and diffusion-based VLA baselines, with 1-step inference attaining 89.1 PDMS and 5-step inference reaching 90.3 PDMS on NAVSIM v1 benchmark. These results establish discrete flow matching as a new promising paradigm for end-to-end autonomous driving. The code will be publicly available soon.", "AI": {"tldr": "WAM-Flow\u662f\u4e00\u79cd\u65b0\u578bVLA\u6a21\u578b\uff0c\u901a\u8fc7\u79bb\u6563\u6d41\u5339\u914d\u6539\u5584\u8f68\u8ff9\u89c4\u5212\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c06\u516c\u5f00\u4ee3\u7801\u3002", "motivation": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8\uff08VLA\uff09\u6a21\u578b\uff0c\u4ee5\u6539\u8fdb\u81ea\u6211\u8f68\u8ff9\u89c4\u5212\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "WAM-Flow\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u6807\u8bb0\u7a7a\u95f4\u8fdb\u884c\u79bb\u6563\u6d41\u5339\u914d\uff0c\u91c7\u7528\u5168\u5e76\u884c\u3001\u53cc\u5411\u53bb\u566a\u7684\u65b9\u5f0f\u7ed3\u5408\u6570\u503c\u6807\u8bb0\u5668\u548c\u6d41\u76ee\u6807\uff0c\u5b9e\u73b0\u5b89\u5168\u6027\u3001\u8fdb\u5ea6\u548c\u8212\u9002\u6027\u5956\u52b1\u7684\u6574\u5408\u3002", "result": "\u5728NAVSIM v1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cWAM-Flow\u5728\u95ed\u73af\u6027\u80fd\u4e0a\u4f18\u4e8e\u81ea\u56de\u5f52\u548c\u6269\u6563\u57fa\u7ebf\uff0c1\u6b65\u63a8\u7406\u8fbe\u523089.1 PDMS\uff0c5\u6b65\u63a8\u7406\u8fbe\u523090.3 PDMS\u3002", "conclusion": "WAM-Flow\u7684\u7ed3\u679c\u8bc1\u660e\u4e86\u79bb\u6563\u6d41\u5339\u914d\u4f5c\u4e3a\u4e00\u79cd\u65b0\u7684\u6709\u524d\u9014\u7684\u7aef\u5230\u7aef\u81ea\u4e3b\u9a7e\u9a76\u8303\u5f0f\u3002"}}
{"id": "2512.06106", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06106", "abs": "https://arxiv.org/abs/2512.06106", "authors": ["Constanze Albrecht", "Chayapatr Archiwaranguprok", "Rachel Poonsiriwong", "Awu Chen", "Peggy Yin", "Monchai Lertsutthiwong", "Kavin Winson", "Hal Hershfield", "Pattie Maes", "Pat Pataranutaporn"], "title": "Future You: Designing and Evaluating Multimodal AI-generated Digital Twins for Strengthening Future Self-Continuity", "comment": null, "summary": "What if users could meet their future selves today? AI-generated future selves simulate meaningful encounters with a digital twin decades in the future. As AI systems advance, combining cloned voices, age-progressed facial rendering, and autobiographical narratives, a central question emerges: Does the modality of these future selves alter their psychological and affective impact? How might a text-based chatbot, a voice-only system, or a photorealistic avatar shape present-day decisions and our feeling of connection to the future? We report a randomized controlled study (N=92) evaluating three modalities of AI-generated future selves (text, voice, avatar) against a neutral control condition. We also report a systematic model evaluation between Claude 4 and three other Large Language Models (LLMs), assessing Claude 4 across psychological and interaction dimensions and establishing conversational AI quality as a critical determinant of intervention effectiveness. All personalized modalities strengthened Future Self-Continuity (FSC), emotional well-being, and motivation compared to control, with avatar producing the largest vividness gains, yet with no significant differences between formats. Interaction quality metrics, particularly persuasiveness, realism, and user engagement, emerged as robust predictors of psychological and affective outcomes, indicating that how compelling the interaction feels matters more than the form it takes. Content analysis found thematic patterns: text emphasized career planning, while voice and avatar facilitated personal reflection. Claude 4 outperformed ChatGPT 3.5, Llama 4, and Qwen 3 in enhancing psychological, affective, and FSC outcomes.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86AI\u751f\u6210\u7684\u672a\u6765\u81ea\u6211\u5728\u4e0d\u540c\u6a21\u6001\u4e0b\u5bf9\u7528\u6237\u51b3\u7b56\u53ca\u5fc3\u7406\u5f71\u54cd\u7684\u6548\u679c\uff0c\u7ed3\u679c\u8868\u660e\u4ea4\u4e92\u8d28\u91cf\u6bd4\u5f62\u5f0f\u66f4\u91cd\u8981\uff0c\u800cAI\u751f\u6210\u7684\u672a\u6765\u81ea\u6211\u666e\u904d\u589e\u5f3a\u4e86\u7528\u6237\u7684\u672a\u6765\u81ea\u6211\u8fde\u7eed\u6027\u548c\u60c5\u611f\u798f\u7949\u3002", "motivation": "\u968f\u7740AI\u6280\u672f\u7684\u53d1\u5c55\uff0c\u7528\u6237\u672a\u6765\u81ea\u6211\u7684\u6a21\u62df\u53ef\u80fd\u5f71\u54cd\u4ed6\u4eec\u7684\u51b3\u7b56\u548c\u672a\u6765\u611f\uff0c\u56e0\u6b64\u63a2\u7d22\u4e0d\u540c\u6a21\u6001\u7684\u672a\u6765\u81ea\u6211\u5982\u4f55\u4f5c\u7528\u4e8e\u7528\u6237\u5fc3\u7406\u662f\u91cd\u8981\u7684\u3002", "method": "\u901a\u8fc7\u968f\u673a\u5bf9\u7167\u7814\u7a76\uff0c\u8bc4\u4f3092\u540d\u53c2\u4e0e\u8005\u5728\u4e09\u79cdAI\u751f\u6210\u7684\u672a\u6765\u81ea\u6211\u6a21\u6001\uff08\u6587\u672c\u3001\u8bed\u97f3\u3001\u5934\u50cf\uff09\u4e0e\u4e2d\u6027\u5bf9\u7167\u6761\u4ef6\u4e0b\u7684\u5fc3\u7406\u548c\u60c5\u611f\u53cd\u5e94\uff0c\u5e76\u5bf9Claude 4\u4e0e\u5176\u4ed6\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u672c\u7814\u7a76\u8868\u660e\uff0cAI\u751f\u6210\u7684\u672a\u6765\u81ea\u6211\uff08\u6587\u672c\u3001\u8bed\u97f3\u548c\u5934\u50cf\uff09\u6709\u6548\u589e\u5f3a\u4e86\u672a\u6765\u81ea\u6211\u8fde\u7eed\u6027\uff08FSC\uff09\u3001\u60c5\u611f\u798f\u7949\u548c\u52a8\u673a\uff0c\u76f8\u8f83\u4e8e\u5bf9\u7167\u7ec4\uff0c\u5c3d\u7ba1\u4e0d\u540c\u683c\u5f0f\u4e4b\u95f4\u672a\u8868\u73b0\u51fa\u663e\u8457\u5dee\u5f02\u3002\u4ea4\u4e92\u8d28\u91cf\uff08\u4f8b\u5982\u8bf4\u670d\u529b\u3001\u73b0\u5b9e\u611f\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\uff09\u88ab\u53d1\u73b0\u662f\u5fc3\u7406\u548c\u60c5\u611f\u7ed3\u679c\u7684\u5f3a\u9884\u6d4b\u56e0\u7d20\uff0c\u8868\u660e\u4ea4\u4e92\u7684\u5438\u5f15\u529b\u6bd4\u5176\u5f62\u5f0f\u66f4\u4e3a\u91cd\u8981\u3002\u5185\u5bb9\u5206\u6790\u663e\u793a\u6587\u672c\u5f3a\u8c03\u804c\u4e1a\u89c4\u5212\uff0c\u800c\u8bed\u97f3\u548c\u5934\u50cf\u5219\u4fc3\u8fdb\u4e2a\u4eba\u53cd\u601d\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cAI\u751f\u6210\u7684\u672a\u6765\u81ea\u6211\u80fd\u591f\u6709\u6548\u5730\u652f\u6301\u7528\u6237\u5728\u5fc3\u7406\u548c\u60c5\u611f\u5c42\u9762\u7684\u53d1\u5c55\uff0c\u5c24\u5176\u662f\u4ea4\u4e92\u8d28\u91cf\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2512.06130", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.06130", "abs": "https://arxiv.org/abs/2512.06130", "authors": ["Grant Stagg", "Isaac E. Weintraub", "Cameron K. Peterson"], "title": "Probabilistic Weapon Engagement Zones for a Turn Constrained Pursuer", "comment": "Accepted for presentation at AIAA SciTech 2026. 17 pages, 7 figures", "summary": "Curve-straight probabilistic engagement zones (CSPEZ) quantify the spatial regions an evader should avoid to reduce capture risk from a turn-rate-limited pursuer following a curve-straight path with uncertain parameters including position, heading, velocity, range, and maximum turn rate. This paper presents methods for generating evader trajectories that minimize capture risk under such uncertainty. We first derive an analytic solution for the deterministic curve-straight basic engagement zone (CSBEZ), then extend this formulation to a probabilistic framework using four uncertainty-propagation approaches: Monte Carlo sampling, linearization, quadratic approximation, and neural-network regression. We evaluate the accuracy and computational cost of each approximation method and demonstrate how CSPEZ constraints can be integrated into a trajectory-optimization algorithm to produce safe paths that explicitly account for pursuer uncertainty.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u66f2\u7ebf-\u76f4\u7ebf\u6982\u7387\u4ea4\u6218\u533a\uff08CSPEZ\uff09\uff0c\u7528\u4e8e\u91cf\u5316\u89c4\u907f\u6355\u83b7\u98ce\u9669\u7684\u7a7a\u95f4\u533a\u57df\uff0c\u5e76\u5c55\u793a\u4e86\u51cf\u5c11\u6355\u83b7\u98ce\u9669\u7684\u9003\u907f\u8005\u8f68\u8ff9\u751f\u6210\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u4e0d\u786e\u5b9a\u53c2\u6570\u4e0b\uff08\u5305\u62ec\u4f4d\u7f6e\u3001\u671d\u5411\u3001\u901f\u5ea6\u7b49\uff09\u51cf\u5c11\u88ab\u8ffd\u6355\u8005\u7684\u6355\u83b7\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u63a8\u5bfc\u786e\u5b9a\u6027\u66f2\u7ebf-\u76f4\u7ebf\u57fa\u672c\u4ea4\u6218\u533a\uff08CSBEZ\uff09\u7684\u89e3\u6790\u89e3\uff0c\u5e76\u4f7f\u7528\u56db\u79cd\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u65b9\u6cd5\uff08\u8499\u7279\u5361\u6d1b\u91c7\u6837\u3001\u7ebf\u6027\u5316\u3001\u4e8c\u6b21\u903c\u8fd1\u3001\u795e\u7ecf\u7f51\u7edc\u56de\u5f52\uff09\u6269\u5c55\u5230\u6982\u7387\u6846\u67b6\u3002", "result": "\u8bc4\u4f30\u4e86\u6bcf\u79cd\u8fd1\u4f3c\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u5c55\u793a\u5982\u4f55\u5c06CSPEZ\u7ea6\u675f\u96c6\u6210\u5230\u8f68\u8ff9\u4f18\u5316\u7b97\u6cd5\u4e2d\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u73b0\u8003\u8651\u8ffd\u6355\u8005\u4e0d\u786e\u5b9a\u6027\u7684\u5b89\u5168\u8def\u5f84\u751f\u6210\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2512.06408", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.06408", "abs": "https://arxiv.org/abs/2512.06408", "authors": ["Shuai Chen", "Lei Han", "Haoyu Wang", "Zhaoman Zhong"], "title": "CommentScope: A Comment-Embedded Assisted Reading System for a Long Text", "comment": "32 pages, 9 figures. Submitted to CHI 2025", "summary": "Long texts are ubiquitous on social platforms, yet readers often face information overload and struggle to locate key content. Comments provide valuable external perspectives for understanding, questioning, and complementing the text, but their potential is hindered by disorganized and unstructured presentation. Few studies have explored embedding comments directly into reading. As an exploratory step, we propose CommentScope, a system with two core modules: a pipeline that classifies comments into five types and aligns them with relevant sentences, and a presentation module that integrates comments inline or as side notes, supported by visual cues such as colors, charts, and highlights. Technical evaluation shows that the hybrid \"Rule+LLM\" pipeline achieved solid performance in semantic classification (accuracy=0.90) and position alignment (accuracy=0.88). A user study (N=12) further demonstrated that the sentence-end embedding significantly improved comment discovery accuracy and reading fluency while reducing mental demand and perceived effort.", "AI": {"tldr": "CommentScope\u662f\u4e00\u4e2a\u5c06\u8bc4\u8bba\u6709\u6548\u5d4c\u5165\u957f\u6587\u672c\u4e2d\u7684\u7cfb\u7edf\uff0c\u63d0\u5347\u4e86\u4fe1\u606f\u53d1\u73b0\u548c\u9605\u8bfb\u4f53\u9a8c\u3002", "motivation": "\u5e94\u5bf9\u793e\u4ea4\u5e73\u53f0\u4e0a\u957f\u6587\u672c\u7684\u4fe1\u606f\u8fc7\u8f7d\u95ee\u9898\uff0c\u5e2e\u52a9\u8bfb\u8005\u66f4\u6709\u6548\u5730\u5b9a\u4f4d\u5173\u952e\u4fe1\u606f\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\u7684\u7cfb\u7edf\uff0c\u7b2c\u4e00\u6a21\u5757\u8d1f\u8d23\u5c06\u8bc4\u8bba\u5206\u7c7b\u4e3a\u4e94\u79cd\u7c7b\u578b\u5e76\u5c06\u5176\u4e0e\u76f8\u5173\u53e5\u5b50\u5bf9\u9f50\uff0c\u7b2c\u4e8c\u6a21\u5757\u5219\u8d1f\u8d23\u5c06\u8bc4\u8bba\u96c6\u6210\u5230\u6587\u672c\u4e2d\uff0c\u652f\u6301\u53ef\u89c6\u5316\u63d0\u793a\u3002", "result": "\u8bbe\u8ba1\u548c\u5b9e\u65bd\u4e86\u4e00\u4e2a\u5206\u7c7b\u548c\u5448\u73b0\u8bc4\u8bba\u7684\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5728\u8bed\u4e49\u5206\u7c7b\u548c\u4f4d\u7f6e\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u7814\u7a76\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "CommentScope\u901a\u8fc7\u5c06\u8bc4\u8bba\u5d4c\u5165\u5230\u957f\u6587\u672c\u4e2d\uff0c\u63d0\u9ad8\u4e86\u8bc4\u8bba\u53d1\u73b0\u7684\u51c6\u786e\u6027\u548c\u9605\u8bfb\u6d41\u7545\u6027\uff0c\u540c\u65f6\u51cf\u8f7b\u4e86\u8bfb\u8005\u7684\u5fc3\u7406\u8d1f\u62c5\u548c\u611f\u77e5\u52aa\u529b\u3002"}}
{"id": "2512.06147", "categories": ["cs.RO", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.06147", "abs": "https://arxiv.org/abs/2512.06147", "authors": ["Hochul Hwang", "Soowan Yang", "Jahir Sadik Monon", "Nicholas A Giudice", "Sunghoon Ivan Lee", "Joydeep Biswas", "Donghyun Kim"], "title": "GuideNav: User-Informed Development of a Vision-Only Robotic Navigation Assistant For Blind Travelers", "comment": null, "summary": "While commendable progress has been made in user-centric research on mobile assistive systems for blind and low-vision (BLV) individuals, references that directly inform robot navigation design remain rare. To bridge this gap, we conducted a comprehensive human study involving interviews with 26 guide dog handlers, four white cane users, nine guide dog trainers, and one O\\&M trainer, along with 15+ hours of observing guide dog-assisted walking. After de-identification, we open-sourced the dataset to promote human-centered development and informed decision-making for assistive systems for BLV people. Building on insights from this formative study, we developed GuideNav, a vision-only, teach-and-repeat navigation system. Inspired by how guide dogs are trained and assist their handlers, GuideNav autonomously repeats a path demonstrated by a sighted person using a robot. Specifically, the system constructs a topological representation of the taught route, integrates visual place recognition with temporal filtering, and employs a relative pose estimator to compute navigation actions - all without relying on costly, heavy, power-hungry sensors such as LiDAR. In field tests, GuideNav consistently achieved kilometer-scale route following across five outdoor environments, maintaining reliability despite noticeable scene variations between teach and repeat runs. A user study with 3 guide dog handlers and 1 guide dog trainer further confirmed the system's feasibility, marking (to our knowledge) the first demonstration of a quadruped mobile system retrieving a path in a manner comparable to guide dogs.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86GuideNav\uff0c\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u7684\u81ea\u4e3b\u5bfc\u822a\u7cfb\u7edf\uff0c\u4e3a\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u7fa4\u4f53\u63d0\u4f9b\u4e86\u7c7b\u4f3c\u5bfc\u76f2\u72ac\u7684\u8f85\u52a9\u529f\u80fd\u3002", "motivation": "\u4e3a\u4e86\u586b\u8865\u79fb\u52a8\u8f85\u52a9\u7cfb\u7edf\u5728\u4e3a\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u4eba\u7fa4\u8bbe\u8ba1\u4e2d\u7684\u77e5\u8bc6\u7a7a\u767d\uff0c\u63a8\u52a8\u4ee5\u4eba\u4e3a\u672c\u7684\u6280\u672f\u5f00\u53d1\u3002", "method": "\u901a\u8fc7\u7efc\u5408\u4eba\u7c7b\u7814\u7a76\uff0c\u8fdb\u884c\u8bbf\u8c08\u5e76\u89c2\u5bdf\u5bfc\u76f2\u72ac\u8f85\u52a9\u7684\u6b65\u884c\uff0c\u5f62\u6210\u5bf9\u5bfc\u822a\u8bbe\u8ba1\u7684\u6df1\u5165\u7406\u89e3\uff0c\u7136\u540e\u5f00\u53d1\u4e86GuideNav\u5bfc\u822a\u7cfb\u7edf\u3002", "result": "\u5728\u4e94\u4e2a\u6237\u5916\u73af\u5883\u4e2d\uff0cGuideNav\u5b9e\u73b0\u4e86\u516c\u91cc\u7ea7\u7684\u8def\u5f84\u8ddf\u8e2a\uff0c\u5c3d\u7ba1\u6559\u5b66\u548c\u91cd\u590d\u8fd0\u884c\u4e4b\u95f4\u573a\u666f\u53d8\u5316\u660e\u663e\uff0c\u4f46\u4ecd\u4fdd\u6301\u4e86\u53ef\u9760\u6027\uff0c\u5e76\u4e14\u901a\u8fc7\u7528\u6237\u7814\u7a76\u8bc1\u5b9e\u4e86\u7cfb\u7edf\u7684\u53ef\u884c\u6027\u3002", "conclusion": "GuideNav\u662f\u4e00\u4e2a\u53ef\u884c\u7684\u5730\u9762\u56db\u8db3\u79fb\u52a8\u5bfc\u822a\u7cfb\u7edf\uff0c\u80fd\u591f\u4ee5\u7c7b\u4f3c\u4e8e\u5bfc\u76f2\u72ac\u7684\u65b9\u5f0f\u81ea\u4e3b\u91cd\u590d\u8def\u5f84\u3002"}}
{"id": "2512.06459", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.06459", "abs": "https://arxiv.org/abs/2512.06459", "authors": ["Shiliang Zhang", "Sabita Maharjan"], "title": "Cenergy3: An API for City Energy 3D Modeling", "comment": null, "summary": "The efficient management and planning of urban energy systems require integrated three-dimensional (3D) models that accurately represent both consumption nodes and distribution networks. This paper introduces our developed geospatial Application Programming Interface (API) that automates the generation of 3D urban digital model from open data. The API synthesizes data from OpenTopography, OpenStreetMap, and Overture Maps in generating 3D models. The rendered model visualizes and contextualizes power grid infrastructure alongside the built environment and transportation networks. The API provides interactive figures for the 3D models, which are essential for analyzing infrastructure alignment and spatially linking energy demand nodes (buildings) with energy supply (utility grids). Our API leverages standard Web Mercator coordinates (EPSG:3857) and JSON serialization to ensure interoperability within smart city and energy simulation platforms.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5730\u7406\u7a7a\u95f4API\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u4e09\u7ef4\u57ce\u5e02\u6570\u5b57\u6a21\u578b\uff0c\u4ee5\u652f\u6301\u57ce\u5e02\u80fd\u6e90\u7cfb\u7edf\u7684\u7ba1\u7406\u548c\u89c4\u5212\u3002", "motivation": "\u57ce\u5e02\u80fd\u6e90\u7cfb\u7edf\u7ba1\u7406\u548c\u89c4\u5212\u7684\u590d\u6742\u6027\u8981\u6c42\u4f7f\u7528\u51c6\u786e\u7684\u4e09\u7ef4\u6a21\u578b\u6765\u8868\u5f81\u80fd\u6e90\u6d88\u8d39\u548c\u5206\u914d\u7f51\u7edc\u3002", "method": "\u901a\u8fc7\u6574\u5408OpenTopography\u3001OpenStreetMap\u548cOverture Maps\u7684\u6570\u636e\uff0c\u5f00\u53d1API\u4ee5\u751f\u6210\u4ea4\u4e92\u5f0f\u7684\u4e09\u7ef4\u6570\u5b57\u6a21\u578b\uff0c\u5e76\u786e\u4fdd\u4e0e\u667a\u6167\u57ce\u5e02\u548c\u80fd\u6e90\u4eff\u771f\u5e73\u53f0\u7684\u4e92\u64cd\u4f5c\u6027\u3002", "result": "\u672c\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u9ad8\u6548\u7ba1\u7406\u548c\u89c4\u5212\u57ce\u5e02\u80fd\u6e90\u7cfb\u7edf\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u96c6\u6210\u7684\u4e09\u7ef4\u6a21\u578b\u51c6\u786e\u8868\u793a\u6d88\u8d39\u8282\u70b9\u548c\u914d\u7535\u7f51\u7edc\u3002", "conclusion": "\u5f00\u53d1\u7684\u5730\u7406\u7a7a\u95f4API\u80fd\u591f\u81ea\u52a8\u751f\u6210\u57fa\u4e8e\u5f00\u653e\u6570\u636e\u7684\u4e09\u7ef4\u57ce\u5e02\u6570\u5b57\u6a21\u578b\uff0c\u63d0\u5347\u4e86\u57fa\u7840\u8bbe\u65bd\u5206\u6790\u548c\u80fd\u6e90\u9700\u6c42\u4e0e\u4f9b\u7ed9\u7684\u7a7a\u95f4\u8054\u7cfb\u3002"}}
{"id": "2512.06151", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.06151", "abs": "https://arxiv.org/abs/2512.06151", "authors": ["Ratnangshu Das", "Siddhartha Upadhyay", "Pushpak Jagtap"], "title": "Real-Time Spatiotemporal Tubes for Dynamic Unsafe Sets", "comment": null, "summary": "This paper presents a real-time control framework for nonlinear pure-feedback systems with unknown dynamics to satisfy reach-avoid-stay tasks within a prescribed time in dynamic environments. To achieve this, we introduce a real-time spatiotemporal tube (STT) framework. An STT is defined as a time-varying ball in the state space whose center and radius adapt online using only real-time sensory input. A closed-form, approximation-free control law is then derived to constrain the system output within the STT, ensuring safety and task satisfaction. We provide formal guarantees for obstacle avoidance and on-time task completion. The effectiveness and scalability of the framework are demonstrated through simulations and hardware experiments on a mobile robot and an aerial vehicle, navigating in cluttered dynamic environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u63a7\u5236\u6846\u67b6\uff0c\u4f7f\u7528\u7a7a\u95f4\u65f6\u95f4\u7ba1\u786e\u4fdd\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b89\u5168\u4e14\u53ca\u65f6\u5730\u5b8c\u6210\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u975e\u7ebf\u6027\u7eaf\u53cd\u9988\u7cfb\u7edf\u4e2d\u7684\u672a\u77e5\u52a8\u6001\uff0c\u5e76\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u5230\u8fbe-\u907f\u514d-\u505c\u7559\u7684\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u65f6\u7a7a\u95f4\u65f6\u95f4\u7ba1\uff08STT\uff09\u6765\u7ea6\u675f\u7cfb\u7edf\u8f93\u51fa\uff0c\u786e\u4fdd\u5b89\u5168\u548c\u4efb\u52a1\u6ee1\u8db3\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u548c\u786c\u4ef6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u4e14\u53ef\u6269\u5c55\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u79fb\u52a8\u673a\u5668\u4eba\u548c\u7a7a\u4e2d\u98de\u884c\u5668\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u6267\u884c\u3002"}}
{"id": "2512.06534", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.06534", "abs": "https://arxiv.org/abs/2512.06534", "authors": ["Harold Triedman", "Jayshree Sarathy", "Priyanka Nanayakkara", "Rachel Cummings", "Gabriel Kaptchuk", "Sean Kross", "Elissa M. Redmiles"], "title": "\"Having Confidence in My Confidence Intervals\": How Data Users Engage with Privacy-Protected Wikipedia Data", "comment": null, "summary": "In response to calls for open data and growing privacy threats, organizations are increasingly adopting privacy-preserving techniques such as differential privacy (DP) that inject statistical noise when generating published datasets. These techniques are designed to protect privacy of data subjects while enabling useful analyses, but their reception by data users is under-explored. We developed documentation that presents the noise characteristics of two Wikipedia pageview datasets: one using rounding (heuristic privacy) and another using DP (formal privacy). After incorporating expert feedback (n=5), we used these documents to conduct a task-based contextual inquiry (n=15) exploring how data users--largely unfamiliar with these methods--perceive, interact with, and interpret privacy-preserving noise during data analysis.\n  Participants readily used simple uncertainty metrics from the documentation, but struggled when asked to compute confidence intervals across multiple noisy estimates. They were better able to devise simulation-based approaches for computing uncertainty with DP data compared to rounded data. Surprisingly, several participants incorrectly believed DP's stronger utility implied weaker privacy protections. Based on our findings, we offer design recommendations for documentation and tools to better support data users working with privacy-noised data.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u6570\u636e\u7528\u6237\u5bf9\u9690\u79c1\u4fdd\u62a4\u566a\u58f0\uff08\u5982\u5dee\u5206\u9690\u79c1\u548c\u56db\u820d\u4e94\u5165\uff09\u7684\u611f\u77e5\u548c\u4e92\u52a8\uff0c\u5e76\u63d0\u4f9b\u4e86\u6539\u5584\u6587\u6863\u548c\u5de5\u5177\u7684\u5efa\u8bae\u3002", "motivation": "\u5e94\u5bf9\u5f00\u653e\u6570\u636e\u7684\u9700\u6c42\u548c\u65e5\u76ca\u589e\u957f\u7684\u9690\u79c1\u5a01\u80c1\uff0c\u7ec4\u7ec7\u5f00\u59cb\u91c7\u7528\u9690\u79c1\u4fdd\u62a4\u6280\u672f\uff0c\u5982\u5dee\u5206\u9690\u79c1\u3002", "method": "\u5f00\u53d1\u6587\u6863\uff0c\u5c55\u793a\u4f7f\u7528\u4e0d\u540c\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\uff08\u5982\u56db\u820d\u4e94\u5165\u548c\u5dee\u5206\u9690\u79c1\uff09\u5904\u7406\u7684\u6570\u636e\u96c6\u4e2d\u7684\u566a\u58f0\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u4efb\u52a1\u5bfc\u5411\u7684\u80cc\u666f\u8c03\u67e5\uff0c\u63a2\u7d22\u6570\u636e\u7528\u6237\u5bf9\u9690\u79c1\u4fdd\u62a4\u566a\u58f0\u7684\u611f\u77e5\u548c\u4e92\u52a8\u3002", "result": "\u53c2\u4e0e\u8005\u5728\u4f7f\u7528\u6587\u6863\u4e2d\u7684\u7b80\u5355\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8ba1\u7b97\u591a\u4e2a\u566a\u58f0\u4f30\u8ba1\u7684\u7f6e\u4fe1\u533a\u95f4\u65f6\u9047\u5230\u56f0\u96be\u3002\u4e0e\u4f7f\u7528\u56db\u820d\u4e94\u5165\u7684\u6570\u636e\u76f8\u6bd4\uff0c\u53c2\u4e0e\u8005\u5728\u4f7f\u7528\u5dee\u5206\u9690\u79c1\u6570\u636e\u65f6\u66f4\u80fd\u8bbe\u8ba1\u51fa\u57fa\u4e8e\u6a21\u62df\u7684\u65b9\u6cd5\u6765\u8ba1\u7b97\u4e0d\u786e\u5b9a\u6027\u3002\u90e8\u5206\u53c2\u4e0e\u8005\u9519\u8bef\u5730\u8ba4\u4e3a\u5dee\u5206\u9690\u79c1\u7684\u66f4\u5f3a\u6548\u7528\u610f\u5473\u7740\u66f4\u5f31\u7684\u9690\u79c1\u4fdd\u62a4\u3002", "conclusion": "\u6839\u636e\u7814\u7a76\u7ed3\u679c\uff0c\u63d0\u51fa\u4e86\u6539\u8fdb\u6587\u6863\u548c\u5de5\u5177\u7684\u8bbe\u8ba1\u5efa\u8bae\uff0c\u4ee5\u66f4\u597d\u5730\u652f\u6301\u5904\u7406\u9690\u79c1\u566a\u58f0\u6570\u636e\u7684\u6570\u636e\u7528\u6237\u3002"}}
{"id": "2512.06182", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.06182", "abs": "https://arxiv.org/abs/2512.06182", "authors": ["Shuhao Qi", "Qiling Aori", "Luyao Zhang", "Mircea Lazar", "Sofie Haesaert"], "title": "Situation-Aware Interactive MPC Switching for Autonomous Driving", "comment": null, "summary": "To enable autonomous driving in interactive traffic scenarios, various model predictive control (MPC) formulations have been proposed, each employing different interaction models. While higher-fidelity models enable more intelligent behavior, they incur increased computational cost. Since strong interactions are relatively infrequent in traffic, a practical strategy for balancing performance and computational overhead is to invoke an appropriate controller based on situational demands. To achieve this approach, we first conduct a comparative study to assess and hierarchize the interactive capabilities of different MPC formulations. Furthermore, we develop a neural network-based classifier to enable situation-aware switching among controllers with different levels of interactive capability. We demonstrate that this situation-aware switching can both substantially improve overall performance by activating the most advanced interactive MPC in rare but critical situations, and significantly reduce computational load by using a basic MPC in the majority of scenarios.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u81ea\u4e3b\u9a7e\u9a76\u4e2d\u4e0d\u540cMPC\u6a21\u578b\u7684\u4e92\u52a8\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u63a7\u5236\u5668\u5207\u6362\u65b9\u6cd5\uff0c\u4ee5\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u4f18\u5316\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u7684\u53d1\u5c55\uff0c\u5982\u4f55\u5728\u590d\u6742\u7684\u4ea4\u901a\u573a\u666f\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7684\u63a7\u5236\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540cMPC\u7684\u4e92\u52a8\u80fd\u529b\uff0c\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u63a7\u5236\u5668\u5207\u6362\uff0c\u4ee5\u5b9e\u73b0\u60c5\u5883\u611f\u77e5\u7684\u63a7\u5236\u7b56\u7565\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u573a\u666f\u611f\u77e5\u63a7\u5236\u5668\u5207\u6362\u7b56\u7565\uff0c\u901a\u8fc7\u5bf9\u4e0d\u540c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u5f62\u5f0f\u7684\u6bd4\u8f83\u7814\u7a76\uff0c\u786e\u5b9a\u4e86\u5176\u4e92\u52a8\u80fd\u529b\u7684\u5c42\u6b21\u3002\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u4f7f\u7528\u57fa\u672cMPC\u4ee5\u964d\u4f4e\u8ba1\u7b97\u8d1f\u8f7d\uff0c\u800c\u5728\u5173\u952e\u60c5\u51b5\u4e0b\u5207\u6362\u81f3\u9ad8\u7ea7\u4e92\u52a8MPC\u4ee5\u63d0\u5347\u6027\u80fd\uff0c\u4ece\u800c\u4f18\u5316\u4e86\u81ea\u4e3b\u9a7e\u9a76\u7b97\u6cd5\u7684\u6548\u7387\u3002", "conclusion": "\u8fd9\u79cd\u60c5\u666f\u611f\u77e5\u7684\u63a7\u5236\u5668\u5207\u6362\u80fd\u591f\u5728\u5173\u952e\u60c5\u51b5\u4e0b\u6fc0\u6d3b\u6700\u5148\u8fdb\u7684\u4e92\u52a8MPC\uff0c\u5927\u5e45\u63d0\u9ad8\u6574\u4f53\u6027\u80fd\uff0c\u540c\u65f6\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u4f7f\u7528\u57fa\u672cMPC\u4ee5\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u8d1f\u62c5\u3002"}}
{"id": "2512.06591", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06591", "abs": "https://arxiv.org/abs/2512.06591", "authors": ["Joe Shymanski", "Jacob Brue", "Sandip Sen"], "title": "Beyond Satisfaction: From Placebic to Actionable Explanations For Enhanced Understandability", "comment": "21 pages, 7 figures, 6 tables. EXTRAAMAS 2025 submission. Preprint version", "summary": "Explainable AI (XAI) presents useful tools to facilitate transparency and trustworthiness in machine learning systems. However, current evaluations of system explainability often rely heavily on subjective user surveys, which may not adequately capture the effectiveness of explanations. This paper critiques the overreliance on user satisfaction metrics and explores whether these can differentiate between meaningful (actionable) and vacuous (placebic) explanations. In experiments involving optimal Social Security filing age selection tasks, participants used one of three protocols: no explanations, placebic explanations, and actionable explanations. Participants who received actionable explanations significantly outperformed the other groups in objective measures of their mental model, but users rated placebic and actionable explanations as equally satisfying. This suggests that subjective surveys alone fail to capture whether explanations truly support users in building useful domain understanding. We propose that future evaluations of agent explanation capabilities should integrate objective task performance metrics alongside subjective assessments to more accurately measure explanation quality. The code for this study can be found at https://github.com/Shymkis/social-security-explainer.", "AI": {"tldr": "\u672c\u7814\u7a76\u6307\u51fa\uff0c\u4f9d\u8d56\u4e3b\u89c2\u8c03\u67e5\u8bc4\u4f30\u89e3\u91ca\u6027\u4e0d\u8db3\uff0c\u5e94\u7ed3\u5408\u5ba2\u89c2\u6307\u6807\u6765\u66f4\u5168\u9762\u5730\u8bc4\u4ef7\u89e3\u91ca\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u7684\u7cfb\u7edf\u89e3\u91ca\u80fd\u529b\u8bc4\u4f30\u8fc7\u4e8e\u4f9d\u8d56\u7528\u6237\u6ee1\u610f\u5ea6\u7684\u4e3b\u89c2\u8c03\u67e5\uff0c\u672a\u80fd\u6709\u6548\u533a\u5206\u6709\u610f\u4e49\u4e0e\u65e0\u610f\u4e49\u7684\u89e3\u91ca\u3002", "method": "\u5b9e\u9a8c\u4e2d\u53c2\u4e0e\u8005\u88ab\u968f\u673a\u5206\u914d\u5230\u4e09\u79cd\u534f\u8bae\uff1a\u65e0\u89e3\u91ca\u3001\u65e0\u6548\u89e3\u91ca\u548c\u6709\u6548\u89e3\u91ca\uff0c\u5e76\u6d4b\u91cf\u5176\u5fc3\u7406\u6a21\u578b\u7684\u5ba2\u89c2\u8868\u73b0\u3002", "result": "\u63a5\u53d7\u6709\u6548\u89e3\u91ca\u7684\u53c2\u4e0e\u8005\u5728\u5fc3\u7406\u6a21\u578b\u7684\u5ba2\u89c2\u6307\u6807\u4e0a\u8868\u73b0\u660e\u663e\u4f18\u4e8e\u5176\u4ed6\u7ec4\uff0c\u4f46\u7528\u6237\u5bf9\u65e0\u6548\u89e3\u91ca\u548c\u6709\u6548\u89e3\u91ca\u7684\u6ee1\u610f\u5ea6\u8bc4\u5206\u76f8\u4f3c\u3002", "conclusion": "\u672a\u6765\u7684\u8bc4\u4f30\u5e94\u7ed3\u5408\u5ba2\u89c2\u4efb\u52a1\u8868\u73b0\u6307\u6807\u548c\u4e3b\u89c2\u8bc4\u4f30\uff0c\u4ee5\u66f4\u51c6\u786e\u5730\u6d4b\u91cf\u89e3\u91ca\u8d28\u91cf\u3002"}}
{"id": "2512.06192", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06192", "abs": "https://arxiv.org/abs/2512.06192", "authors": ["Takahiro Hattori", "Kento Kawaharazuka", "Temma Suzuki", "Keita Yoneda", "Kei Okada"], "title": "REWW-ARM -- Remote Wire-Driven Mobile Robot: Design, Control, and Experimental Validation", "comment": "Accepted on Advanced Intelligent Systems", "summary": "Electronic devices are essential for robots but limit their usable environments. To overcome this, methods excluding electronics from the operating environment while retaining advanced electronic control and actuation have been explored. These include the remote hydraulic drive of electronics-free mobile robots, which offer high reachability, and long wire-driven robot arms with motors consolidated at the base, which offer high environmental resistance. To combine the advantages of both, this study proposes a new system, \"Remote Wire Drive.\" As a proof-of-concept, we designed and developed the Remote Wire-Driven robot \"REWW-ARM\", which consists of the following components: 1) a novel power transmission mechanism, the \"Remote Wire Transmission Mechanism\" (RWTM), the key technology of the Remote Wire Drive; 2) an electronics-free distal mobile robot driven by it; and 3) a motor-unit that generates power and provides electronic closed-loop control based on state estimation via the RWTM. In this study, we evaluated the mechanical and control performance of REWW-ARM through several experiments, demonstrating its capability for locomotion, posture control, and object manipulation both on land and underwater. This suggests the potential for applying the Remote Wire-Driven system to various types of robots, thereby expanding their operational range.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684'\u8fdc\u7a0b\u7ebf\u9a71\u52a8'\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f00\u53d1REWW-ARM\u673a\u5668\u4eba\uff0c\u8bc1\u660e\u5176\u5728\u4e0d\u540c\u73af\u5883\u4e0b\u7684\u64cd\u4f5c\u80fd\u529b\uff0c\u5c55\u73b0\u4e86\u8be5\u7cfb\u7edf\u5728\u673a\u5668\u4eba\u6280\u672f\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u7531\u4e8e\u7535\u5b50\u8bbe\u5907\u9650\u5236\u4e86\u673a\u5668\u4eba\u53ef\u7528\u7684\u73af\u5883\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u975e\u7535\u5b50\u9a71\u52a8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u8fbe\u7684\u53ef\u8fbe\u6027\u548c\u73af\u5883\u62b5\u6297\u529b\u3002", "method": "\u8bbe\u8ba1\u5e76\u5f00\u53d1\u4e86\u540d\u4e3a'REWW-ARM'\u7684\u8fdc\u7a0b\u7ebf\u9a71\u52a8\u673a\u5668\u4eba\uff0c\u5305\u62ec\u65b0\u7684\u52a8\u529b\u4f20\u8f93\u673a\u5236\u548c\u4e00\u4e2a\u7535\u5b50\u9a71\u52a8\u7684\u8fdc\u7a0b\u79fb\u52a8\u673a\u5668\u4eba\u3002", "result": "\u901a\u8fc7\u4e00\u7cfb\u5217\u5b9e\u9a8c\u8bc4\u4f30\u4e86REWW-ARM\u7684\u673a\u68b0\u548c\u63a7\u5236\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u9646\u5730\u548c\u6c34\u4e0b\u7684\u79fb\u52a8\u3001\u59ff\u6001\u63a7\u5236\u548c\u7269\u4f53\u64cd\u63a7\u80fd\u529b\u3002", "conclusion": "\u8fdc\u7a0b\u7ebf\u9a71\u52a8\u7cfb\u7edf\u6709\u6f5c\u529b\u5e94\u7528\u4e8e\u5404\u79cd\u7c7b\u578b\u7684\u673a\u5668\u4eba\uff0c\u6269\u5c55\u4e86\u5b83\u4eec\u7684\u64cd\u4f5c\u8303\u56f4\u3002"}}
{"id": "2512.06616", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06616", "abs": "https://arxiv.org/abs/2512.06616", "authors": ["Rasam Dorri", "Rami Zwick"], "title": "Memory Power Asymmetry in Human-AI Relationships: Preserving Mutual Forgetting in the Digital Age", "comment": "31 pages, 2 tables, 2 figures", "summary": "As artificial intelligence (AI) becomes embedded in personal and professional relationships, a new kind of power imbalance emerges from asymmetric memory capabilities. Human relationships have historically relied on mutual forgetting, the natural tendency for both parties to forget details over time, as a foundation for psychological safety, forgiveness, and identity change. By contrast, AI systems can record, store, and recombine interaction histories at scale, often indefinitely. We introduce Memory Power Asymmetry (MPA): a structural power imbalance that arises when one relationship partner (typically an AI-enabled firm) possesses a substantially superior capacity to record, retain, retrieve, and integrate the shared history of the relationship, and can selectively deploy that history in ways the other partner (the human) cannot. Drawing on research in human memory, power-dependence theory, AI architecture, and consumer vulnerability, we develop a conceptual framework with four dimensions of MPA (persistence, accuracy, accessibility, integration) and four mechanisms by which memory asymmetry is translated into power (strategic memory deployment, narrative control, dependence asymmetry, vulnerability accumulation). We theorize downstream consequences at individual, relational/firm, and societal levels, formulate boundary-conditioned propositions, and articulate six design principles for restoring a healthier balance of memory in human-AI relationships (e.g., forgetting by design, contextual containment, symmetric access to records). Our analysis positions MPA as a distinct construct relative to information asymmetry, privacy, surveillance, and customer relationship management, and argues that protecting mutual forgetting, or at least mutual control over memory, should become a central design and policy goal in the AI age.", "AI": {"tldr": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u7684\u666e\u53ca\uff0c\u51fa\u73b0\u4e86\u4e00\u79cd\u65b0\u7684\u6743\u529b\u4e0d\u5e73\u8861\u2014\u2014\u8bb0\u5fc6\u6743\u529b\u4e0d\u5bf9\u79f0\uff08MPA\uff09\uff0c\u9700\u4fdd\u62a4\u8bb0\u5fc6\u7684\u76f8\u4e92\u63a7\u5236\u4ee5\u786e\u4fdd\u5fc3\u7406\u5b89\u5168\u548c\u8eab\u4efd\u8f6c\u53d8\u3002", "motivation": "\u56e0\u4eba\u5de5\u667a\u80fd\u7684\u8bb0\u5fc6\u80fd\u529b\u4e0d\u5bf9\u79f0\u800c\u4ea7\u751f\u7684\u6743\u529b\u4e0d\u5e73\u8861\u5bf9\u4eba\u7c7b\u5173\u7cfb\u7684\u5f71\u54cd\u9700\u8981\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u56db\u7ef4\u6846\u67b6\uff0c\u63a2\u8ba8\u4e86\u8bb0\u5fc6\u6743\u529b\u4e0d\u5bf9\u79f0\u53ca\u5176\u8f6c\u5316\u4e3a\u6743\u529b\u7684\u673a\u5236\uff0c\u540c\u65f6\u5206\u6790\u5176\u5bf9\u4e2a\u4f53\u3001\u5173\u7cfb/\u516c\u53f8\u548c\u793e\u4f1a\u5c42\u9762\u7684\u540e\u679c\u3002", "result": "\u6784\u5efa\u4e86\u8bb0\u5fc6\u6743\u529b\u4e0d\u5bf9\u79f0\uff08MPA\uff09\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u516d\u4e2a\u8bbe\u8ba1\u539f\u5219\u4ee5\u6062\u590d\u4eba\u7c7b\u4e0e\u4eba\u5de5\u667a\u80fd\u5173\u7cfb\u4e2d\u7684\u8bb0\u5fc6\u5e73\u8861\u3002", "conclusion": "\u4fdd\u62a4\u5171\u4eab\u8bb0\u5fc6\u7684\u76f8\u4e92\u63a7\u5236\u5e94\u6210\u4e3aAI\u65f6\u4ee3\u7684\u8bbe\u8ba1\u548c\u653f\u7b56\u76ee\u6807\u3002"}}
{"id": "2512.06198", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06198", "abs": "https://arxiv.org/abs/2512.06198", "authors": ["Oussama Sifour", "Soulaimane Berkane", "Abdelhamid Tayebi"], "title": "Cascaded Tightly-Coupled Observer Design for Single-Range-Aided Inertial Navigation", "comment": "8 pages", "summary": "This work introduces a single-range-aided navigation observer that reconstructs the full state of a rigid body using only an Inertial Measurement Unit (IMU), a body-frame vector measurement (e.g., magnetometer), and a distance measurement from a fixed anchor point. The design first formulates an extended linear time-varying (LTV) system to estimate body-frame position, body-frame velocity, and the gravity direction. The recovered gravity direction, combined with the body-frame vector measurement, is then used to reconstruct the full orientation on $\\mathrm{SO}(3)$, resulting in a cascaded observer architecture. Almost Global Asymptotic Stability (AGAS) of the cascaded design is established under a uniform observability condition, ensuring robustness to sensor noise and trajectory variations. Simulation studies on three-dimensional trajectories demonstrate accurate estimation of position, velocity, and orientation, highlighting single-range aiding as a lightweight and effective modality for autonomous navigation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a8\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u5bfc\u822a\u89c2\u5bdf\u5668\uff0c\u7ed3\u5408IMU\u3001\u4f53\u6846\u67b6\u5411\u91cf\u6d4b\u91cf\u548c\u6765\u81ea\u56fa\u5b9a\u951a\u70b9\u7684\u8ddd\u79bb\u6d4b\u91cf\uff0c\u53ef\u4ee5\u6709\u6548\u91cd\u5efa\u521a\u4f53\u7684\u72b6\u6001\uff0c\u5177\u6709\u5f88\u5f3a\u7684\u6297\u5e72\u6270\u80fd\u529b\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u5bfc\u822a\u7cfb\u7edf\u5728\u4f20\u611f\u5668\u4f9d\u8d56\u548c\u73af\u5883\u53d8\u5316\u4e0b\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u5e76\u5e0c\u671b\u901a\u8fc7\u6700\u5c0f\u5316\u4f20\u611f\u5668\u9700\u6c42\u6765\u63d0\u9ad8\u5bfc\u822a\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387\u3002", "method": "\u901a\u8fc7\u63d0\u51fa\u4e00\u79cd\u6269\u5c55\u7ebf\u6027\u65f6\u95f4\u53d8\u7cfb\u7edf\uff08LTV\uff09\uff0c\u7ed3\u5408\u4f53\u6846\u67b6\u7684\u5411\u91cf\u6d4b\u91cf\u548c\u8ddd\u79bb\u6d4b\u91cf\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u7ea7\u8054\u89c2\u5bdf\u5668\u67b6\u6784\u6765\u4f30\u8ba1\u4f4d\u7f6e\u3001\u901f\u5ea6\u548c\u91cd\u529b\u65b9\u5411\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e09\u7ef4\u8f68\u8ff9\u4e0a\u80fd\u591f\u51c6\u786e\u4f30\u8ba1\u4f4d\u7f6e\u3001\u901f\u5ea6\u548c\u59ff\u6001\uff0c\u8bc1\u660e\u4e86\u5355\u8303\u56f4\u8f85\u52a9\u5bfc\u822a\u5728\u6027\u80fd\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u5bfc\u822a\u89c2\u5bdf\u5668\u80fd\u591f\u5728\u4ec5\u4f9d\u8d56\u5c11\u91cf\u4f20\u611f\u5668\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u5bf9\u521a\u4f53\u7684\u5b8c\u6574\u72b6\u6001\u91cd\u5efa\uff0c\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u6709\u6548\u7684\u81ea\u4e3b\u5bfc\u822a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06647", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.06647", "abs": "https://arxiv.org/abs/2512.06647", "authors": ["Siyu Qiu", "Anqi Lin", "Shiya Wang", "Xingyu Lan"], "title": "Exploring Teenagers' Trust in Al Chatbots: An Empirical Study of Chinese Middle-School Students", "comment": null, "summary": "Chatbots have become increasingly prevalent. A growing body of research focused on the issue of human trust in AI. However, most existing user studies are conducted primarily with adult groups, overlooking teenagers who are also engaging more frequently with AI technologies. Based on previous theories about teenage education and psychology, this study investigates the correlation between teenagers' psychological characteristics and their trust in AI chatbots, examining four key variables: AI literacy, ego identity, social anxiety, and psychological resilience. We adopted a mixed-methods approach, combining an online survey with semi-structured interviews. Our findings reveal that psychological resilience is a significant positive predictor of trust in AI, and that age significantly moderates the relationship between social anxiety and trust. The interviews further suggest that teenagers generally report relatively high levels of trust in AI, tend to overestimate their AI literacy, and are influenced by external factors such as social media.", "AI": {"tldr": "\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u9752\u5c11\u5e74\u5fc3\u7406\u7279\u5f81\u4e0e\u5bf9AI\u804a\u5929\u673a\u5668\u4eba\u7684\u4fe1\u4efb\u5173\u7cfb\uff0c\u53d1\u73b0\u5fc3\u7406\u97e7\u6027\u4e3a\u4fe1\u4efb\u7684\u6b63\u5f71\u54cd\u56e0\u7d20\uff0c\u5e74\u9f84\u8c03\u8282\u793e\u4ea4\u7126\u8651\u4e0e\u4fe1\u4efb\u7684\u5173\u7cfb\u3002", "motivation": "\u5c11\u6570\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6210\u5e74\u4eba\uff0c\u672a\u80fd\u5145\u5206\u8003\u8651\u9752\u5c11\u5e74\u5bf9AI\u6280\u672f\u7684\u53c2\u4e0e\u3002", "method": "\u91c7\u7528\u6df7\u5408\u7814\u7a76\u65b9\u6cd5\uff0c\u7ed3\u5408\u5728\u7ebf\u8c03\u67e5\u548c\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5fc3\u7406\u97e7\u6027\u4e0e\u5bf9AI\u7684\u4fe1\u4efb\u663e\u8457\u6b63\u76f8\u5173\uff0c\u5e74\u9f84\u663e\u8457\u8c03\u8282\u793e\u4ea4\u7126\u8651\u4e0e\u4fe1\u4efb\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u8bbf\u8c08\u663e\u793a\u9752\u5c11\u5e74\u666e\u904d\u5bf9AI\u4fe1\u4efb\u8f83\u9ad8\uff0c\u4e14\u5e38\u5e38\u9ad8\u4f30\u81ea\u5df1\u7684AI\u7d20\u517b\uff0c\u53d7\u5230\u793e\u4ea4\u5a92\u4f53\u7b49\u5916\u90e8\u56e0\u7d20\u7684\u5f71\u54cd\u3002", "conclusion": "\u5fc3\u7406\u97e7\u6027\u662f\u4fe1\u4efbAI\u7684\u91cd\u8981\u6b63\u5411\u9884\u6d4b\u56e0\u7d20\uff0c\u800c\u5e74\u9f84\u5728\u793e\u4ea4\u7126\u8651\u4e0e\u4fe1\u4efb\u4e4b\u95f4\u7684\u5173\u7cfb\u4e2d\u8d77\u5230\u91cd\u8981\u7684\u8c03\u8282\u4f5c\u7528\u3002"}}
{"id": "2512.06207", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06207", "abs": "https://arxiv.org/abs/2512.06207", "authors": ["Harshil Suthar", "Dipankar Maity"], "title": "Where to Fly, What to Send: Communication-Aware Aerial Support for Ground Robots", "comment": "Submitted to conference", "summary": "In this work we consider a multi-robot team operating in an unknown environment where one aerial agent is tasked to map the environment and transmit (a portion of) the mapped environment to a group of ground agents that are trying to reach their goals. The entire operation takes place over a bandwidth-limited communication channel, which motivates the problem of determining what and how much information the assisting agent should transmit and when while simultaneously performing exploration/mapping. The proposed framework enables the assisting aerial agent to decide what information to transmit based on the Value-of-Information (VoI), how much to transmit using a Mixed-Integer Linear Programming (MILP), and how to acquire additional information through an utility score-based environment exploration strategy. We perform a communication-motion trade-off analysis between the total amount of map data communicated by the aerial agent and the navigation cost incurred by the ground agents.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u672a\u77e5\u73af\u5883\u4e2d\uff0c\u591a\u673a\u5668\u4eba\u56e2\u961f\u4e0b\u7a7a\u4e2d\u4ee3\u7406\u7684\u4fe1\u606f\u4f20\u8f93\u4e0e\u73af\u5883\u63a2\u7d22\u7684\u4f18\u5316\u7b56\u7565\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8eVoI\u548cMILP\u7684\u6846\u67b6\uff0c\u63d0\u5347\u4e86\u4efb\u52a1\u6548\u7387\u3002", "motivation": "\u591a\u673a\u5668\u4eba\u56e2\u961f\u5728\u672a\u77e5\u73af\u5883\u4e2d\u5de5\u4f5c\uff0c\u9700\u8981\u9ad8\u6548\u7684\u4fe1\u606f\u4f20\u8f93\u4ee5\u8f85\u52a9\u5730\u9762\u4ee3\u7406\u8fbe\u5230\u76ee\u6807\u3002", "method": "\u901a\u8fc7\u4ef7\u503c\u4fe1\u606f(VoI)\u7684\u5206\u6790\uff0c\u91c7\u7528\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212(MILP)\u6765\u51b3\u5b9a\u4f20\u8f93\u7684\u4fe1\u606f\u91cf\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u6548\u7528\u8bc4\u5206\u7684\u73af\u5883\u63a2\u7d22\u7b56\u7565\u6765\u83b7\u53d6\u989d\u5916\u4fe1\u606f\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u4f7f\u5f97\u7a7a\u4e2d\u4ee3\u7406\u80fd\u591f\u4f18\u5316\u4fe1\u606f\u4f20\u8f93\uff0c\u8fdb\u884c\u901a\u4fe1-\u8fd0\u52a8\u6743\u8861\u5206\u6790\u3002", "conclusion": "\u6b64\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u4fe1\u606f\u4f20\u8f93\u6548\u7387\uff0c\u5bf9\u591a\u673a\u5668\u4eba\u534f\u540c\u4efb\u52a1\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2512.06834", "categories": ["cs.HC", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.06834", "abs": "https://arxiv.org/abs/2512.06834", "authors": ["Zhiguang Zhou", "Ruiqi Yu", "Yuming Ma", "Hao Ni", "Guojun Li", "Li Ye", "Xiaoying Wang", "Yize Li", "Yong Wang"], "title": "COIVis: Eye tracking-based Visual Exploration of Concept Learning in MOOC Videos", "comment": "18pages, 5 figures", "summary": "Massive Open Online Courses (MOOCs) make high-quality instruction accessible. However, the lack of face-to-face interaction makes it difficult for instructors to obtain feedback on learners' performance and provide more effective instructional guidance. Traditional analytical approaches, such as clickstream logs or quiz scores, capture only coarse-grained learning outcomes and offer limited insight into learners' moment-to-moment cognitive states. In this study, we propose COIVis, an eye tracking-based visual analytics system that supports concept-level exploration of learning processes in MOOC videos. COIVis first extracts course concepts from multimodal video content and aligns them with the temporal structure and screen space of the lecture, defining Concepts of Interest (COIs), which anchor abstract concepts to specific spatiotemporal regions. Learners' gaze trajectories are transformed into COI sequences, and five interpretable learner-state features -- Attention, Cognitive Load, Interest, Preference, and Synchronicity -- are computed at the COI level based on eye tracking metrics. Building on these representations, COIVis provides a narrative, multi-view visualization enabling instructors to move from cohort-level overviews to individual learning paths, quickly locate problematic concepts, and compare diverse learning strategies. We evaluate COIVis through two case studies and in-depth user-feedback interviews. The results demonstrate that COIVis effectively provides instructors with valuable insights into the consistency and anomalies of learners' learning patterns, thereby supporting timely and personalized interventions for learners and optimizing instructional design.", "AI": {"tldr": "MOOC\u6559\u5b66\u4e2d\uff0cCOIVis\u7cfb\u7edf\u901a\u8fc7\u773c\u52a8\u8ffd\u8e2a\u6280\u672f\u5206\u6790\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5e2e\u52a9\u6559\u5e08\u83b7\u53d6\u5b66\u4e60\u8005\u53cd\u9988\uff0c\u4f18\u5316\u6559\u5b66\u8bbe\u8ba1\u3002", "motivation": "MOOCs\u7f3a\u4e4f\u9762\u5bf9\u9762\u4e92\u52a8\uff0c\u5bfc\u81f4\u6559\u5e08\u96be\u4ee5\u83b7\u5f97\u5b66\u4e60\u8005\u8868\u73b0\u7684\u53cd\u9988\uff0c\u8fdb\u800c\u5f71\u54cd\u6559\u5b66\u6307\u5bfc\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u773c\u52a8\u8ffd\u8e2a\u7684\u53ef\u89c6\u5206\u6790\u7cfb\u7edfCOIVis\uff0c\u652f\u6301MOOC\u89c6\u9891\u4e2d\u7684\u6982\u5ff5\u7ea7\u5b66\u4e60\u8fc7\u7a0b\u63a2\u7d22\uff0c\u901a\u8fc7\u4ece\u591a\u6a21\u6001\u89c6\u9891\u5185\u5bb9\u4e2d\u63d0\u53d6\u8bfe\u7a0b\u6982\u5ff5\uff0c\u5e76\u5c06\u5176\u4e0e\u8bb2\u5ea7\u7684\u65f6\u95f4\u7ed3\u6784\u548c\u5c4f\u5e55\u7a7a\u95f4\u5bf9\u9f50\uff0c\u5b9a\u4e49\u5174\u8da3\u6982\u5ff5\uff08COIs\uff09\u3002", "result": "COIVis\u53ef\u8ba1\u7b97\u4e94\u4e2a\u53ef\u89e3\u91ca\u7684\u5b66\u4e60\u8005\u72b6\u6001\u7279\u5f81\uff0c\u5e76\u63d0\u4f9b\u53d9\u4e8b\u591a\u89c6\u56fe\u53ef\u89c6\u5316\uff0c\u4f7f\u6559\u5e08\u80fd\u591f\u6df1\u5165\u4e86\u89e3\u4e2a\u4f53\u5b66\u4e60\u8def\u5f84\uff0c\u5feb\u901f\u8bc6\u522b\u95ee\u9898\u6982\u5ff5\uff0c\u4ee5\u53ca\u6bd4\u8f83\u4e0d\u540c\u7684\u5b66\u4e60\u7b56\u7565\u3002", "conclusion": "COIVis\u6709\u6548\u63d0\u4f9b\u4e86\u5bf9\u5b66\u4e60\u8005\u5b66\u4e60\u6a21\u5f0f\u4e00\u81f4\u6027\u548c\u5f02\u5e38\u7684\u6d1e\u5bdf\uff0c\u4ece\u800c\u652f\u6301\u53ca\u65f6\u548c\u4e2a\u6027\u5316\u7684\u5e72\u9884\uff0c\u5e76\u4f18\u5316\u6559\u5b66\u8bbe\u8ba1\u3002"}}
{"id": "2512.06261", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06261", "abs": "https://arxiv.org/abs/2512.06261", "authors": ["Taekyung Kim", "Keyvan Majd", "Hideki Okamoto", "Bardh Hoxha", "Dimitra Panagou", "Georgios Fainekos"], "title": "Safe Model Predictive Diffusion with Shielding", "comment": "Project page: https://www.taekyung.me/safe-mpd", "summary": "Generating safe, kinodynamically feasible, and optimal trajectories for complex robotic systems is a central challenge in robotics. This paper presents Safe Model Predictive Diffusion (Safe MPD), a training-free diffusion planner that unifies a model-based diffusion framework with a safety shield to generate trajectories that are both kinodynamically feasible and safe by construction. By enforcing feasibility and safety on all samples during the denoising process, our method avoids the common pitfalls of post-processing corrections, such as computational intractability and loss of feasibility. We validate our approach on challenging non-convex planning problems, including kinematic and acceleration-controlled tractor-trailer systems. The results show that it substantially outperforms existing safety strategies in success rate and safety, while achieving sub-second computation times.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6269\u6563\u89c4\u5212\u65b9\u6cd5\uff0c\u80fd\u5b89\u5168\u751f\u6210\u590d\u6742\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u53ef\u884c\u8f68\u8ff9\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7b56\u7565\u3002", "motivation": "\u751f\u6210\u5b89\u5168\u3001\u52a8\u673a\u5b66\u53ef\u884c\u4e14\u4f18\u5316\u7684\u8f68\u8ff9\u662f\u673a\u5668\u4eba\u6280\u672f\u4e2d\u7684\u6838\u5fc3\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5b89\u5168\u6a21\u578b\u9884\u6d4b\u6269\u6563\uff08Safe MPD\uff09\u7684\u65e0\u8bad\u7ec3\u6269\u6563\u89c4\u5212\u8005\uff0c\u7ed3\u5408\u4e86\u6a21\u578b\u57fa\u6269\u6563\u6846\u67b6\u4e0e\u5b89\u5168\u4fdd\u62a4\u5c42\u3002", "result": "\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u5bf9\u6240\u6709\u6837\u672c\u65bd\u52a0\u53ef\u884c\u6027\u4e0e\u5b89\u5168\u6027\u7ea6\u675f\uff0c\u907f\u514d\u4e86\u540e\u5904\u7406\u4fee\u6b63\u7684\u5e38\u89c1\u9677\u9631\u3002", "conclusion": "\u5728\u590d\u6742\u7684\u975e\u51f8\u89c4\u5212\u95ee\u9898\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u663e\u793a\u51fa\u6bd4\u73b0\u6709\u5b89\u5168\u7b56\u7565\u663e\u8457\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u5b89\u5168\u6027\uff0c\u540c\u65f6\u5b9e\u73b0\u4e9a\u79d2\u7ea7\u8ba1\u7b97\u65f6\u95f4\u3002"}}
{"id": "2512.06910", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.06910", "abs": "https://arxiv.org/abs/2512.06910", "authors": ["Dennis Becker", "Kyra Ahrens", "Connor G\u00e4de", "Erik Strahl", "Stefan Wermter"], "title": "Robots with Attitudes: Influence of LLM-Driven Robot Personalities on Motivation and Performance", "comment": null, "summary": "Large language models enable unscripted conversations while maintaining a consistent personality. One desirable personality trait in cooperative partners, known to improve task performance, is agreeableness. To explore the impact of large language models on personality modeling for robots, as well as the effect of agreeable and non-agreeable personalities in cooperative tasks, we conduct a two-part study. This includes an online pre-study for personality validation and a lab-based main study to evaluate the effects on likability, motivation, and task performance. The results demonstrate that the robot's agreeableness significantly enhances its likability. No significant difference in intrinsic motivation was observed between the two personality types. However, the findings suggest that a robot exhibiting agreeableness and openness to new experiences can enhance task performance. This study highlights the advantages of employing large language models for customized modeling of robot personalities and provides evidence that a carefully chosen agreeable robot personality can positively influence human perceptions and lead to greater success in cooperative scenarios.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u4e2a\u6027\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u5b9c\u4eba\u6027\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u7684\u53d7\u6b22\u8fce\u5ea6\u5e76\u5728\u5408\u4f5c\u4efb\u52a1\u4e2d\u6709\u79ef\u6781\u6548\u679c\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u673a\u5668\u4eba\u4e2a\u6027\u5efa\u6a21\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u5b9c\u4eba\u4e0e\u975e\u5b9c\u4eba\u4e2a\u6027\u5728\u5408\u4f5c\u4efb\u52a1\u4e2d\u7684\u6548\u679c\u3002", "method": "\u8fdb\u884c\u4e86\u4e00\u9879\u5206\u4e3a\u4e24\u90e8\u5206\u7684\u7814\u7a76\uff0c\u5305\u62ec\u5728\u7ebf\u7684\u4e2a\u6027\u9a8c\u8bc1\u9884\u7814\u7a76\u548c\u5b9e\u9a8c\u5ba4\u7684\u4e3b\u7814\u7a76\uff0c\u4ee5\u8bc4\u4f30\u4e2a\u6027\u5bf9\u5408\u4f5c\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "result": "\u673a\u5668\u4eba\u5b9c\u4eba\u6027\u663e\u8457\u63d0\u9ad8\u4e86\u5176\u53d7\u6b22\u8fce\u7a0b\u5ea6\uff0c\u7136\u800c\u5728\u5185\u5728\u52a8\u673a\u65b9\u9762\uff0c\u4e24\u79cd\u4e2a\u6027\u7c7b\u578b\u4e4b\u95f4\u672a\u89c2\u5bdf\u5230\u663e\u8457\u5dee\u5f02\u3002\u5177\u6709\u5b9c\u4eba\u6027\u548c\u5f00\u653e\u6027\u7279\u5f81\u7684\u673a\u5668\u4eba\u53ef\u4ee5\u63d0\u5347\u4efb\u52a1\u8868\u73b0\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u5177\u6709\u5b9c\u4eba\u6027\u7684\u673a\u5668\u4eba\u5728\u63d0\u9ad8\u4eba\u4eec\u5bf9\u673a\u5668\u4eba\u7684\u559c\u597d\u5ea6\u548c\u5408\u4f5c\u4efb\u52a1\u7684\u6210\u529f\u7387\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2512.06423", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.06423", "abs": "https://arxiv.org/abs/2512.06423", "authors": ["Leonardo F. Dos Santos", "Elisa G. Vergamini", "C\u00edcero Zanette", "Lucca Maitan", "Thiago Boaventura"], "title": "Leveraging Port-Hamiltonian Theory for Impedance Control Benchmarking", "comment": "This is the author's version of the paper accepted for publication in the 2025 International Conference on Advanced Robotics (ICAR). The final version will be available at IEEE Xplore", "summary": "This work proposes PH-based metrics for benchmarking impedance control. A causality-consistent PH model is introduced for mass-spring-damper impedance in Cartesian space. Based on this model, a differentiable, force-torque sensing-independent, n-DoF passivity condition is derived, valid for time-varying references. An impedance fidelity metric is also defined from step-response power in free motion, capturing dynamic decoupling. The proposed metrics are validated in Gazebo simulations with a six-DoF manipulator and a quadruped leg. Results demonstrate the suitability of the PH framework for standardized impedance control benchmarking.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u7528\u4e8e\u963b\u6297\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u7684PH\u57fa\u7840\u5ea6\u91cf\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u52a8\u6001\u89e3\u8026\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u963b\u6297\u63a7\u5236\u63d0\u4f9b\u57fa\u51c6\u6d4b\u8bd5\u7684PH\u57fa\u7840\u5ea6\u91cf\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u4e0d\u8db3\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u56e0\u679c\u4e00\u81f4\u7684PH\u6a21\u578b\uff0c\u57fa\u4e8e\u8be5\u6a21\u578b\u63a8\u5bfc\u51fa\u53ef\u5fae\u5206\u7684\u3001\u4e0e\u529b\u77e9\u4f20\u611f\u5668\u65e0\u5173\u7684n\u81ea\u7531\u5ea6\u88ab\u52a8\u6761\u4ef6\u3002", "result": "\u901a\u8fc7Gazebo\u4eff\u771f\u9a8c\u8bc1\uff0c\u7ed3\u679c\u663e\u793a\u4e86\u6240\u63d0\u51fa\u5ea6\u91cf\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684PH\u6846\u67b6\u9002\u5408\u7528\u4e8e\u6807\u51c6\u5316\u7684\u963b\u6297\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u3002"}}
{"id": "2512.07117", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.07117", "abs": "https://arxiv.org/abs/2512.07117", "authors": ["Yun Dai"], "title": "Human Agency and Creativity in AI-Assisted Learning Environments", "comment": null, "summary": "This chapter explores human creativity in AI-assisted learning environments through the lens of student agency. We begin by examining four theoretical perspectives on agency, including instrumental, effortful, dynamically emergent, and authorial agency, and analyze how each frames the relationship between agency and creativity. Under each theoretical perspective, we discuss how the integration of generative AI (GenAI) tools reshapes these dynamics by altering students' roles in cognitive, social, and creative processes. In the second part, we introduce a theoretical framework for AI agentic engagement, contextualizing agency within specific cognitive, relational, and ethical dynamics introduced by GenAI tools. This framework is linked to the concept of Mini-c creativity, emphasizing personal relevance and self-directed learning. Together, these perspectives support a shift from viewing creativity as product-oriented to understanding it as a process of agentive participation and meaning-making. We conclude with two directions for future research focused on the creative process and performance in AI-assisted learning.", "AI": {"tldr": "\u672c\u7ae0\u8282\u63a2\u8ba8\u5728AI\u8f85\u52a9\u5b66\u4e60\u73af\u5883\u4e2d\uff0c\u5b66\u751f\u4e3b\u52a8\u6027\u4e0e\u521b\u9020\u529b\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4ee5\u53ca\u751f\u6210\u6027AI\u5de5\u5177\u5982\u4f55\u5f71\u54cd\u8fd9\u4e00\u5173\u7cfb\u3002", "motivation": "\u63a2\u8ba8AI\u8f85\u52a9\u5b66\u4e60\u73af\u5883\u4e2d\u4eba\u7c7b\u521b\u9020\u529b\u4ee5\u53ca\u5b66\u751f\u7684\u4e3b\u52a8\u6027\u5982\u4f55\u76f8\u4e92\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u56db\u79cd\u7406\u8bba\u89c6\u89d2\u5206\u6790\u5b66\u751f\u7684\u4e3b\u52a8\u6027\u53ca\u5176\u4e0e\u521b\u9020\u529b\u7684\u5173\u7cfb\uff0c\u540c\u65f6\u6784\u5efaAI\u4ee3\u7406\u53c2\u4e0e\u7684\u7406\u8bba\u6846\u67b6\u3002", "result": "\u9610\u660e\u751f\u6210\u6027AI\u5de5\u5177\u5982\u4f55\u91cd\u65b0\u5851\u9020\u5b66\u751f\u5728\u8ba4\u77e5\u3001\u793e\u4ea4\u548c\u521b\u9020\u8fc7\u7a0b\u4e2d\u7684\u89d2\u8272\uff0c\u5e76\u63d0\u51fa\u4e86\u4e0eMini-c\u521b\u9020\u529b\u76f8\u5173\u7684\u7406\u8bba\u6846\u67b6\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5e94\u96c6\u4e2d\u4e8e\u5728AI\u8f85\u52a9\u5b66\u4e60\u4e2d\u521b\u9020\u6027\u8fc7\u7a0b\u548c\u8868\u73b0\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.06444", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06444", "abs": "https://arxiv.org/abs/2512.06444", "authors": ["Xuehui Ma", "Shiliang Zhang", "Zhiyong Sun"], "title": "Fault Tolerant Control of Mecanum Wheeled Mobile Robots", "comment": null, "summary": "Mecanum wheeled mobile robots (MWMRs) are highly susceptible to actuator faults that degrade performance and risk mission failure. Current fault tolerant control (FTC) schemes for MWMRs target complete actuator failures like motor stall, ignoring partial faults e.g., in torque degradation. We propose an FTC strategy handling both fault types, where we adopt posterior probability to learn real-time fault parameters. We derive the FTC law by aggregating probability-weighed control laws corresponding to predefined faults. This ensures the robustness and safety of MWMR control despite varying levels of fault occurrence. Simulation results demonstrate the effectiveness of our FTC under diverse scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578bFTC\u7b56\u7565\uff0c\u6709\u6548\u5904\u7406MWMR\u7684\u5b8c\u5168\u53ca\u90e8\u5206\u81f4\u52a8\u5668\u6545\u969c\uff0c\u786e\u4fdd\u63a7\u5236\u7684\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "Mecanum wheeled mobile robots\u5728\u6267\u884c\u4efb\u52a1\u65f6\u6613\u53d7\u81f4\u52a8\u5668\u6545\u969c\u5f71\u54cd\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u548c\u4efb\u52a1\u5931\u8d25\u7684\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6545\u969c\u5bb9\u5fcd\u63a7\u5236\uff08FTC\uff09\u7b56\u7565\uff0c\u53ef\u4ee5\u5904\u7406\u5b8c\u5168\u548c\u90e8\u5206\u6545\u969c\uff0c\u901a\u8fc7\u540e\u9a8c\u6982\u7387\u5b9e\u65f6\u5b66\u4e60\u6545\u969c\u53c2\u6570\uff0c\u5e76\u901a\u8fc7\u52a0\u6743\u7684\u63a7\u5236\u6cd5\u5219\u805a\u5408\u5f97\u5230FTC\u6cd5\u5219\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\u8be5FTC\u7b56\u7565\u5728\u591a\u79cd\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5373\u4fbf\u5728\u4e0d\u540c\u6545\u969c\u53d1\u751f\u7ea7\u522b\u4e0b\uff0c\u8be5\u63a7\u5236\u7b56\u7565\u80fd\u591f\u786e\u4fddMWMR\u7684\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2512.07143", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.07143", "abs": "https://arxiv.org/abs/2512.07143", "authors": ["Yun Dai", "Sichen Lai"], "title": "A Theoretical Framework of Student Agency in AI- Assisted Learning: A Grounded Theory Approach", "comment": null, "summary": "Generative AI(GenAI) is a kind of AI model capable of producing human-like content in various modalities, including text, image, audio, video, and computer programming. Although GenAI offers great potential for education, its value often depends on students' ability to engage with it actively, responsibly, and critically - qualities central to student agency. Nevertheless, student agency has long been a complex and ambiguous concept in educational discourses, with few empirical studies clarifying its distinct nature and process in AI-assisted learning environments. To address this gap, the qualitative study presented in this article examines how higher education students exercise agency in AI-assisted learning and proposes a theoretical framework using a grounded theory approach. Guided by agentic engagement theory, this article analyzes the authentic experiences of 26 students using data from their GenAI conversation records and cognitive interviews that capture their thought processes and decision-making. The findings identify four key aspects of student agency: initiating and (re)directing, mindful adoption, external help-seeking, and reflective learning. Together, these aspects form an empirically developed framework that characterizes student agency in AI-assisted learning as a proactive, intentional, adaptive, reflective, and iterative process. Based on the empirical findings, theoretical and practical implications are discussed for researchers, educators, and policymakers.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u5b66\u751f\u5728AI\u8f85\u52a9\u5b66\u4e60\u4e2d\u5982\u4f55\u53d1\u6325\u4e3b\u52a8\u6027\uff0c\u63d0\u51fa\u4e86\u56db\u4e2a\u5173\u952e\u65b9\u9762\uff0c\u5f62\u6210\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u5f3a\u8c03\u5b66\u751f\u4e3b\u52a8\u6027\u662f\u4e00\u4e2a\u79ef\u6781\u3001\u9002\u5e94\u6027\u5f3a\u7684\u8fc7\u7a0b\u3002", "motivation": "\u968f\u7740\u751f\u6210\u6027\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\uff0c\u7406\u89e3\u5b66\u751f\u5728AI\u8f85\u52a9\u5b66\u4e60\u4e2d\u7684\u4e3b\u52a8\u6027\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5bf9\u6b64\u7f3a\u4e4f\u6df1\u5165\u63a2\u8ba8\u3002", "method": "\u91c7\u7528\u8d28\u6027\u7814\u7a76\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf926\u540d\u5b66\u751f\u7684\u5bf9\u8bdd\u8bb0\u5f55\u548c\u8ba4\u77e5\u8bbf\u8c08\u7684\u6570\u636e\u5206\u6790\uff0c\u5e94\u7528\u624e\u6839\u7406\u8bba\u6784\u5efa\u6846\u67b6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5b66\u751f\u4e3b\u52a8\u6027\u5305\u542b\u56db\u4e2a\u5173\u952e\u65b9\u9762\uff1a\u542f\u52a8\u4e0e(\u91cd\u65b0)\u5f15\u5bfc\u3001\u610f\u8bc6\u5230\u91c7\u7528\u3001\u5bfb\u6c42\u5916\u90e8\u5e2e\u52a9\u4ee5\u53ca\u53cd\u601d\u5b66\u4e60\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u63cf\u8ff0\u4e86\u5b66\u751f\u5728AI\u8f85\u52a9\u5b66\u4e60\u4e2d\u7684\u4e3b\u52a8\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u5bf9\u7814\u7a76\u4eba\u5458\u3001\u6559\u80b2\u5de5\u4f5c\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u542f\u793a\u3002"}}
{"id": "2512.06486", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06486", "abs": "https://arxiv.org/abs/2512.06486", "authors": ["Wanru Gong", "Xinyi Zheng", "Xiaopeng Yang", "Xiaoqing Zhu"], "title": "Entropy-Controlled Intrinsic Motivation Reinforcement Learning for Quadruped Robot Locomotion in Complex Terrains", "comment": null, "summary": "Learning is the basis of both biological and artificial systems when it comes to mimicking intelligent behaviors. From the classical PPO (Proximal Policy Optimization), there is a series of deep reinforcement learning algorithms which are widely used in training locomotion policies for quadrupedal robots because of their stability and sample efficiency. However, among all these variants, experiments and simulations often converge prematurely, leading to suboptimal locomotion and reduced task performance. Therefore, in this paper, we introduce Entropy-Controlled Intrinsic Motivation (ECIM), an entropy-based reinforcement learning algorithm in contrast with the PPO series, that can reduce premature convergence by combining intrinsic motivation with adaptive exploration.\n  For experiments, in order to parallel with other baselines, we chose to apply it in Isaac Gym across six terrain categories: upward slopes, downward slopes, uneven rough terrain, ascending stairs, descending stairs, and flat ground as widely used. For comparison, our experiments consistently achieve better performance: task rewards increase by 4--12%, peak body pitch oscillation is reduced by 23--29%, joint acceleration decreases by 20--32%, and joint torque consumption declines by 11--20%. Overall, our model ECIM, by combining entropy control and intrinsic motivation control, achieves better results in stability across different terrains for quadrupedal locomotion, and at the same time reduces energetic cost and makes it a practical choice for complex robotic control tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u71b5\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5ECIM\uff0c\u901a\u8fc7\u5185\u5728\u52a8\u673a\u63a7\u5236\u4e0e\u81ea\u9002\u5e94\u63a2\u7d22\u7684\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5728\u4e0d\u540c\u5730\u5f62\u7684\u7a33\u5b9a\u6027\u548c\u80fd\u6548\u3002", "motivation": "\u751f\u7269\u548c\u4eba\u5de5\u7cfb\u7edf\u5728\u6a21\u4eff\u667a\u80fd\u884c\u4e3a\u65f6\uff0c\u5b66\u4e60\u662f\u5176\u57fa\u7840\u3002\u5bf9\u6bd4PPO\u7cfb\u5217\u7b97\u6cd5\uff0c\u63d0\u51fa\u89e3\u51b3\u5b9e\u9a8c\u548c\u6a21\u62df\u4e2d\u63d0\u524d\u6536\u655b\u5bfc\u81f4\u7684\u6b21\u4f18\u52a8\u4f5c\u53ca\u4efb\u52a1\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u71b5\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u79f0\u4e3a\u71b5\u63a7\u5236\u7684\u5185\u5728\u52a8\u673a\uff08ECIM\uff09\uff0c\u65e8\u5728\u51cf\u5c11\u65e9\u671f\u6536\u655b\uff0c\u901a\u8fc7\u7ed3\u5408\u5185\u5728\u52a8\u673a\u4e0e\u81ea\u9002\u5e94\u63a2\u7d22\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u4efb\u52a1\u5956\u52b1\u63d0\u9ad84%\u523012%\uff0c\u5cf0\u503c\u8f66\u8eab\u4fef\u4ef0\u632f\u8361\u51cf\u5c1123%\u523029%\uff0c\u5173\u8282\u52a0\u901f\u5ea6\u4e0b\u964d20%\u523032%\uff0c\u5173\u8282\u626d\u77e9\u6d88\u8017\u51cf\u5c1111%\u523020%\u3002", "conclusion": "ECIM\u6a21\u578b\u901a\u8fc7\u7ed3\u5408\u71b5\u63a7\u5236\u548c\u5185\u5728\u52a8\u673a\u63a7\u5236\uff0c\u5728\u56db\u8db3\u673a\u5668\u4eba\u4e0d\u540c\u5730\u5f62\u7684\u7a33\u5b9a\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u80fd\u91cf\u6d88\u8017\uff0c\u4f7f\u5176\u6210\u4e3a\u590d\u6742\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u7684\u5b9e\u7528\u9009\u62e9\u3002"}}
{"id": "2512.07357", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.07357", "abs": "https://arxiv.org/abs/2512.07357", "authors": ["Navid Ashrafi", "Francesco Vona", "Sina Hinzmann", "Juliane Henning", "Maurizio Vergari", "Maximilian Warsinke", "Catarina Pinto Moreira", "Jan-Niklas Voigt-Antons"], "title": "Size Matters: The Impact of Avatar Size on User Experience in Healthcare Applications", "comment": "7 pages, 3 figures", "summary": "The usage of virtual avatars in healthcare applications has become widely popular; however, certain critical aspects, such as social distancing and avatar size, remain insufficiently explored. This research investigates user experience and preferences when interacting with a healthcare application utilizing virtual avatars displayed in different sizes. For our study, we had 23 participants interacting with five different avatars (a human-size avatar followed by four smaller avatars in a randomized order) varying in size, projected on a wall in front of them. The avatars were fully integrated with an artificial intelligence chatbot to make them conversational. Users were asked to rate the usability of the system after interacting with each avatar and complete a survey regarding trust and an additional questionnaire on social presence. The results of this study show that avatar size significantly influences the perceived attractiveness and perspicuity, with the medium-sized avatars receiving the highest ratings. Social presence correlated strongly with stimulation and attractiveness, suggesting that an avatar's visual appeal and interactivity influenced user engagement more than its physical size. Additionally, we observed a tendency for gender-specific differences on some of the UEQ+ scales, with male participants tending to prefer human-sized representations, while female participants slightly favored smaller avatars. These findings highlight the importance of avatar design and representation in optimizing user experience and trust in virtual healthcare environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u533b\u7597\u5e94\u7528\u4e2d\u4f7f\u7528\u865a\u62df\u5316\u8eab\u7684\u7528\u6237\u4f53\u9a8c\u53ca\u504f\u597d\uff0c\u53d1\u73b0\u5316\u8eab\u5927\u5c0f\u663e\u8457\u5f71\u54cd\u5438\u5f15\u529b\u548c\u6e05\u6670\u5ea6\uff0c\u5e76\u4e14\u89c6\u89c9\u5438\u5f15\u529b\u4e0e\u4e92\u52a8\u6027\u5bf9\u7528\u6237\u53c2\u4e0e\u5ea6\u5f71\u54cd\u66f4\u5927\u3002\u6027\u522b\u5dee\u5f02\u5728\u67d0\u4e9b\u5c3a\u5ea6\u4e2d\u663e\u73b0\uff0c\u7537\u6027\u504f\u597d\u4eba\u5f62\u5316\u8eab\uff0c\u800c\u5973\u6027\u504f\u5411\u8f83\u5c0f\u7684\u5316\u8eab\u3002", "motivation": "\u5c3d\u7ba1\u865a\u62df\u5316\u8eab\u5728\u533b\u7597\u5e94\u7528\u4e2d\u5df2\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5173\u4e8e\u793e\u4ea4\u8ddd\u79bb\u548c\u5316\u8eab\u5927\u5c0f\u7b49\u5173\u952e\u56e0\u7d20\u7684\u7814\u7a76\u4ecd\u7136\u4e0d\u8db3\uff0c\u4e9f\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u7814\u7a76\u901a\u8fc723\u540d\u53c2\u4e0e\u8005\u4e0e\u4e94\u79cd\u4e0d\u540c\u5927\u5c0f\u7684\u5316\u8eab\uff08\u5305\u62ec\u4e00\u4e2a\u4eba\u5f62\u5316\u8eab\u548c\u56db\u4e2a\u8f83\u5c0f\u7684\u5316\u8eab\uff09\u8fdb\u884c\u4ea4\u4e92\uff0c\u7528\u6237\u5728\u6bcf\u6b21\u4ea4\u4e92\u540e\u5bf9\u7cfb\u7edf\u7684\u53ef\u7528\u6027\u3001\u4fe1\u4efb\u5ea6\u548c\u793e\u4ea4\u5b58\u5728\u611f\u8fdb\u884c\u4e86\u8bc4\u5206\u548c\u8c03\u67e5\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5316\u8eab\u5927\u5c0f\u5bf9\u5438\u5f15\u529b\u548c\u6e05\u6670\u5ea6\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4e2d\u7b49\u5927\u5c0f\u7684\u5316\u8eab\u83b7\u5f97\u4e86\u6700\u9ad8\u8bc4\u5206\u3002\u793e\u4ea4\u5b58\u5728\u611f\u4e0e\u523a\u6fc0\u6027\u548c\u5438\u5f15\u529b\u6709\u5f3a\u5173\u8054\uff0c\u8868\u660e\u5316\u8eab\u7684\u89c6\u89c9\u5438\u5f15\u548c\u4e92\u52a8\u6027\u6bd4\u7269\u7406\u5927\u5c0f\u66f4\u80fd\u5f71\u54cd\u7528\u6237\u53c2\u4e0e\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u865a\u62df\u533b\u7597\u73af\u5883\u4e2d\uff0c\u5316\u8eab\u8bbe\u8ba1\u53ca\u5176\u8868\u73b0\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u4f18\u5316\u7528\u6237\u4f53\u9a8c\u548c\u4fe1\u4efb\u611f\u3002"}}
{"id": "2512.06517", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06517", "abs": "https://arxiv.org/abs/2512.06517", "authors": ["Shifa Sulaiman", "Akash Bachhar", "Ming Shen", "Simon B\u00f8gh"], "title": "Vision-Guided Grasp Planning for Prosthetic Hands in Unstructured Environments", "comment": null, "summary": "Recent advancements in prosthetic technology have increasingly focused on enhancing dexterity and autonomy through intelligent control systems. Vision-based approaches offer promising results for enabling prosthetic hands to interact more naturally with diverse objects in dynamic environments. Building on this foundation, the paper presents a vision-guided grasping algorithm for a prosthetic hand, integrating perception, planning, and control for dexterous manipulation. A camera mounted on the set up captures the scene, and a Bounding Volume Hierarchy (BVH)-based vision algorithm is employed to segment an object for grasping and define its bounding box. Grasp contact points are then computed by generating candidate trajectories using Rapidly-exploring Random Tree Star algorithm, and selecting fingertip end poses based on the minimum Euclidean distance between these trajectories and the objects point cloud. Each finger grasp pose is determined independently, enabling adaptive, object-specific configurations. Damped Least Square (DLS) based Inverse kinematics solver is used to compute the corresponding joint angles, which are subsequently transmitted to the finger actuators for execution. This modular pipeline enables per-finger grasp planning and supports real-time adaptability in unstructured environments. The proposed method is validated in simulation, and experimental integration on a Linker Hand O7 platform.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89c6\u89c9\u5f15\u5bfc\u7684\u6293\u53d6\u7b97\u6cd5\uff0c\u53ef\u652f\u6301\u5047\u624b\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8fdb\u884c\u7075\u5de7\u64cd\u63a7\u3002", "motivation": "\u968f\u7740\u5047\u80a2\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u63d0\u9ad8\u7075\u5de7\u6027\u548c\u81ea\u4e3b\u6027\u6210\u4e3a\u7814\u7a76\u91cd\u70b9\uff0c\u5c24\u5176\u662f\u5229\u7528\u89c6\u89c9\u6765\u6539\u5584\u5047\u624b\u4e0e\u7269\u4f53\u7684\u4e92\u52a8\u3002", "method": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u89c6\u89c9\u5f15\u5bfc\u7684\u6293\u53d6\u7b97\u6cd5\uff0c\u6574\u5408\u4e86\u611f\u77e5\u3001\u89c4\u5212\u548c\u63a7\u5236\uff0c\u4ee5\u5b9e\u73b0\u7075\u5de7\u64cd\u63a7\u3002", "result": "\u901a\u8fc7\u4f7f\u7528\u8fb9\u754c\u4f53\u79ef\u5c42\u6b21(BVH)\u7b97\u6cd5\u8fdb\u884c\u7269\u4f53\u5206\u5272\uff0c\u7ed3\u5408\u5feb\u901f\u63a2\u7d22\u968f\u673a\u6811\u661f(RRT*)\u7b97\u6cd5\u751f\u6210\u6293\u53d6\u63a5\u89e6\u70b9\u8f68\u8ff9\uff0c\u5e76\u4f7f\u7528\u963b\u5c3c\u6700\u5c0f\u4e8c\u4e58\u6cd5(DLS)\u57fa\u7684\u9006\u8fd0\u52a8\u5b66\u6c42\u89e3\u5668\u6765\u8ba1\u7b97\u5173\u8282\u89d2\u5ea6\uff0c\u6700\u7ec8\u5c06\u5176\u4f20\u9012\u7ed9\u624b\u6307\u6267\u884c\u5668\u3002", "conclusion": "\u8fd9\u79cd\u6a21\u5757\u5316\u7684\u6d41\u7a0b\u652f\u6301\u6bcf\u4e2a\u624b\u6307\u7684\u6293\u53d6\u89c4\u5212\uff0c\u5e76\u80fd\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u65f6\u9002\u5e94\u3002"}}
{"id": "2512.07363", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.07363", "abs": "https://arxiv.org/abs/2512.07363", "authors": ["Michael Stern", "Maurizio Vergari", "Julia Schorlemmer", "Francesco Vona", "David Grieshammer", "Jan-Niklas Voigt-Antons"], "title": "The Impact of Spatial Misalignment and Time Delay on Collaborative Presence in Augmented Reality", "comment": null, "summary": "Precise temporal and spatial alignment is critical in collaborative Augmented Reality (AR) where users rely on shared visual information to coordinate actions. System latency and object misalignment can disrupt communication, reduce task efficiency, and negatively impact the overall user experience. While previous research has primarily focused on individual AR interactions, the impact of these inconsistencies on collaboration remains underexplored. This article investigates how user experience and task load are affected by object misalignment and time delay in a shared AR space. To examine these factors, we conducted an experiment with 32 participants, organized into 16 pairs, who collaboratively completed a spatial placement task. Within each condition, both participants alternated roles, taking turns as the leader-providing verbal placement instructions-and the builder-executing the placement. Six conditions were tested, manipulating object alignment (perfectly aligned vs. randomly misaligned) and time delay (0s, 0.1s, 0.4s). The misalignment was applied randomly to each virtual object with a shift of +-20 cm on every axis to create a clear distinction in spatial perception. User experience and task load were assessed to evaluate how these factors influence collaboration and interaction in AR environments. Results showed that spatial misalignment significantly increased perceived workload (NASA-TLX) and lowered user ratings in Pragmatic quality and Attractiveness (UEQ), while time delay had a more limited effect. These findings highlight the critical role of spatial accuracy in maintaining collaboration quality in AR.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5171\u4eab\u589e\u5f3a\u73b0\u5b9e\u4e2d\u5bf9\u8c61\u9519\u4f4d\u548c\u65f6\u95f4\u5ef6\u8fdf\u5bf9\u7528\u6237\u4f53\u9a8c\u548c\u4efb\u52a1\u8d1f\u8377\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u7a7a\u95f4\u9519\u4f4d\u663e\u8457\u5f71\u54cd\u534f\u4f5c\u8d28\u91cf\u3002", "motivation": "\u589e\u5f3a\u73b0\u5b9e\u7684\u7cbe\u786e\u65f6\u95f4\u548c\u7a7a\u95f4\u5bf9\u9f50\u5bf9\u4e8e\u534f\u4f5c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5bf9\u8c61\u9519\u4f4d\u548c\u5ef6\u8fdf\u5bf9\u534f\u4f5c\u7684\u5f71\u54cd\u5c1a\u672a\u6df1\u5165\u63a2\u8ba8\u3002", "method": "\u901a\u8fc7\u5bf932\u540d\u53c2\u4e0e\u8005\uff0816\u5bf9\uff09\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5b8c\u6210\u7a7a\u95f4\u653e\u7f6e\u4efb\u52a1\uff0c\u6d4b\u8bd5\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u5bf9\u8c61\u5bf9\u9f50\u548c\u65f6\u95f4\u5ef6\u8fdf\u7684\u5f71\u54cd\u3002", "result": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u5728\u5171\u4eab\u589e\u5f3a\u73b0\u5b9e(AR)\u7a7a\u95f4\u4e2d\uff0c\u5bf9\u8c61\u7684\u7a7a\u95f4\u9519\u4f4d\u548c\u65f6\u95f4\u5ef6\u8fdf\u5982\u4f55\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\u548c\u4efb\u52a1\u8d1f\u8377\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7a7a\u95f4\u9519\u4f4d\u663e\u8457\u589e\u52a0\u4e86\u611f\u77e5\u5de5\u4f5c\u8d1f\u8377\uff0c\u5e76\u964d\u4f4e\u4e86\u7528\u6237\u5728\u5b9e\u7528\u6027\u8d28\u91cf\u548c\u5438\u5f15\u529b\u65b9\u9762\u7684\u8bc4\u5206\uff0c\u800c\u65f6\u95f4\u5ef6\u8fdf\u7684\u5f71\u54cd\u76f8\u5bf9\u6709\u9650\u3002", "conclusion": "\u7a7a\u95f4\u51c6\u786e\u6027\u5728\u7ef4\u6301\u589e\u5f3a\u73b0\u5b9e\u5408\u4f5c\u8d28\u91cf\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2512.06524", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06524", "abs": "https://arxiv.org/abs/2512.06524", "authors": ["Saekwang Nam", "Bowen Deng", "Loong Yi Lee", "Jonathan M. Rossiter", "Nathan F. Lepora"], "title": "TacFinRay: Soft Tactile Fin-Ray Finger with Indirect Tactile Sensing for Robust Grasping", "comment": "Accepted in IEEE Robotics Automation Letters. S. Nam, B. Deng co-first authors", "summary": "We present a tactile-sensorized Fin-Ray finger that enables simultaneous detection of contact location and indentation depth through an indirect sensing approach. A hinge mechanism is integrated between the soft Fin-Ray structure and a rigid sensing module, allowing deformation and translation information to be transferred to a bottom crossbeam upon which are an array of marker-tipped pins based on the biomimetic structure of the TacTip vision-based tactile sensor. Deformation patterns captured by an internal camera are processed using a convolutional neural network to infer contact conditions without directly sensing the finger surface. The finger design was optimized by varying pin configurations and hinge orientations, achieving 0.1\\,mm depth and 2mm location-sensing accuracies. The perception demonstrated robust generalization to various indenter shapes and sizes, which was applied to a pick-and-place task under uncertain picking positions, where the tactile feedback significantly improved placement accuracy. Overall, this work provides a lightweight, flexible, and scalable tactile sensing solution suitable for soft robotic structures where the sensing needs situating away from the contact interface.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e6\u89c9\u4f20\u611fFin-Ray\u624b\u6307\uff0c\u901a\u8fc7\u95f4\u63a5\u4f20\u611f\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u63a5\u89e6\u4f4d\u7f6e\u548c\u538b\u75d5\u6df1\u5ea6\u7684\u540c\u65f6\u68c0\u6d4b\uff0c\u9002\u7528\u4e8e\u4e0d\u786e\u5b9a\u7684\u62fe\u53d6\u4f4d\u7f6e\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u653e\u7f6e\u7cbe\u5ea6\u3002", "motivation": "\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u521b\u9020\u4e00\u79cd\u80fd\u591f\u8fdc\u79bb\u63a5\u89e6\u754c\u9762\u7684\u89e6\u89c9\u4f20\u611f\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u9002\u5e94\u67d4\u6027\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\u4e00\u79cd\u94f0\u94fe\u673a\u5236\uff0c\u5c06\u8f6f\u6027Fin-Ray\u7ed3\u6784\u4e0e\u521a\u6027\u4f20\u611f\u6a21\u5757\u96c6\u6210\uff0c\u5229\u7528\u5185\u90e8\u76f8\u673a\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u53d8\u5f62\u6a21\u5f0f\uff0c\u4ece\u800c\u63a8\u65ad\u63a5\u89e6\u6761\u4ef6\u3002", "result": "\u7ecf\u8fc7\u4f18\u5316\u7684\u624b\u6307\u8bbe\u8ba1\u5728\u63a5\u89e6\u6df1\u5ea6\u548c\u4f4d\u7f6e\u611f\u77e5\u4e0a\u5206\u522b\u8fbe\u5230\u4e860.1mm\u548c2mm\u7684\u7cbe\u5ea6\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u5f62\u72b6\u548c\u5c3a\u5bf8\u7684\u538b\u5934\u4e0b\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u3001\u7075\u6d3b\u548c\u53ef\u6269\u5c55\u7684\u89e6\u89c9\u4f20\u611f\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u9700\u8981\u5c06\u4f20\u611f\u5668\u8fdc\u79bb\u63a5\u89e6\u754c\u9762\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u7ed3\u6784\u3002"}}
{"id": "2512.07388", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.07388", "abs": "https://arxiv.org/abs/2512.07388", "authors": ["Remi Poivet", "Catherine Pelachaud", "Malika Auvray"], "title": "Breaking Players' expectations: the Role of Non-player Characters' coherence and Consistency", "comment": null, "summary": "In video games, non-player characters (NPCs) play a pivotal role in shaping players' experiences. The design of these characters, encompassing their appearance and behaviors, can be manipulated in terms of coherence and consistency to maintain players' expectations or, on the contrary, to surprise them. The extent to which NPCs' coherence and consistency influence players' evaluation of them remains to be unveiled. To address this knowledge gap, two experiments were conducted in the context of a military shooter game. Players' evaluations of NPCs' perceived intelligence and believability were measured, as these two dimensions are fundamental to players' adoption of NPCs and subsequent commitment to them. The first experiment investigated the impact of disrupting players' initial expectations on their evaluations of NPCs. The second experiment focused on the influence of NPCs' coherence and consistency on both players' expectations and evaluation of NPCs, using a combination of questionnaires and behavioral and physiological measures. The results of our study show that disrupting players' initial expectations influences their assessment of NPCs, with coherent and consistent design reinforcing expectations and incoherent design challenging them.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0cNPC\u7684\u8bbe\u8ba1\u5bf9\u73a9\u5bb6\u7684\u671f\u671b\u548c\u8bc4\u4f30\u6709\u663e\u8457\u5f71\u54cd\uff0c\u8fde\u8d2f\u6027\u548c\u4e00\u81f4\u6027\u5728\u6b64\u8fc7\u7a0b\u4e2d\u53d1\u6325\u91cd\u8981\u4f5c\u7528\u3002", "motivation": "\u63a2\u7d22\u975e\u73a9\u5bb6\u89d2\u8272\uff08NPC\uff09\u5728\u89c6\u9891\u6e38\u620f\u4e2d\u5bf9\u73a9\u5bb6\u4f53\u9a8c\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u4ed6\u4eec\u7684\u5916\u89c2\u548c\u884c\u4e3a\u8bbe\u8ba1\u5982\u4f55\u5f71\u54cd\u73a9\u5bb6\u7684\u671f\u671b\u548c\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u5b9e\u9a8c\u8bc4\u4f30NPC\u7684\u611f\u77e5\u667a\u80fd\u548c\u53ef\u4fe1\u5ea6\uff0c\u5229\u7528\u95ee\u5377\u8c03\u67e5\u3001\u884c\u4e3a\u548c\u751f\u7406\u6d4b\u91cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u7834\u574f\u73a9\u5bb6\u7684\u521d\u59cb\u671f\u671b\u4f1a\u5f71\u54cd\u4ed6\u4eec\u5bf9NPC\u7684\u8bc4\u4f30\uff0c\u8fde\u8d2f\u548c\u4e00\u81f4\u7684\u8bbe\u8ba1\u5f3a\u5316\u4e86\u671f\u671b\uff0c\u800c\u4e0d\u8fde\u8d2f\u7684\u8bbe\u8ba1\u5219\u6311\u6218\u8fd9\u4e9b\u671f\u671b\u3002", "conclusion": "NPC\u7684\u8fde\u8d2f\u6027\u548c\u4e00\u81f4\u6027\u5728\u5851\u9020\u73a9\u5bb6\u5bf9\u4ed6\u4eec\u7684\u611f\u77e5\u548c\u8bc4\u4f30\u4e2d\u8d77\u7740\u91cd\u8981\u4f5c\u7528\u3002"}}
{"id": "2512.06558", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06558", "abs": "https://arxiv.org/abs/2512.06558", "authors": ["Md Mofijul Islam", "Alexi Gladstone", "Sujan Sarker", "Ganesh Nanduru", "Md Fahim", "Keyan Du", "Aman Chadha", "Tariq Iqbal"], "title": "Embodied Referring Expression Comprehension in Human-Robot Interaction", "comment": "14 pages, 7 figures, accepted at the ACM/IEEE International Conference on Human-Robot Interaction (HRI) 2026", "summary": "As robots enter human workspaces, there is a crucial need for them to comprehend embodied human instructions, enabling intuitive and fluent human-robot interaction (HRI). However, accurate comprehension is challenging due to a lack of large-scale datasets that capture natural embodied interactions in diverse HRI settings. Existing datasets suffer from perspective bias, single-view collection, inadequate coverage of nonverbal gestures, and a predominant focus on indoor environments. To address these issues, we present the Refer360 dataset, a large-scale dataset of embodied verbal and nonverbal interactions collected across diverse viewpoints in both indoor and outdoor settings. Additionally, we introduce MuRes, a multimodal guided residual module designed to improve embodied referring expression comprehension. MuRes acts as an information bottleneck, extracting salient modality-specific signals and reinforcing them into pre-trained representations to form complementary features for downstream tasks. We conduct extensive experiments on four HRI datasets, including the Refer360 dataset, and demonstrate that current multimodal models fail to capture embodied interactions comprehensively; however, augmenting them with MuRes consistently improves performance. These findings establish Refer360 as a valuable benchmark and exhibit the potential of guided residual learning to advance embodied referring expression comprehension in robots operating within human environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Refer360\u6570\u636e\u96c6\uff0c\u4ee5\u5e94\u5bf9\u673a\u5668\u4eba\u7406\u89e3\u4eba\u7c7b\u5177\u4f53\u6307\u4ee4\u4e2d\u7684\u6570\u636e\u96c6\u4e0d\u8db3\u95ee\u9898\uff0c\u5e76\u4ecb\u7ecd\u4e86MuRes\u6a21\u5757\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5bf9\u6307\u4ee3\u8868\u8fbe\u7684\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u8fdb\u5165\u4eba\u7c7b\u5de5\u4f5c\u9886\u57df\uff0c\u7406\u89e3\u4eba\u7c7b\u7684\u5177\u4f53\u6307\u4ee4\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7684\u6570\u636e\u96c6\u65e0\u6cd5\u6ee1\u8db3\u8fd9\u4e00\u9700\u6c42\uff0c\u5b58\u5728\u591a\u4e2a\u5c40\u9650\u6027\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86Refer360\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u6536\u96c6\u5305\u542b\u4eba\u7c7b\u8bed\u8a00\u548c\u975e\u8bed\u8a00\u4e92\u52a8\u7684\u5927\u89c4\u6a21\u6570\u636e\uff0c\u5e76\u5f15\u5165MuRes\u6a21\u5757\u6765\u589e\u5f3a\u5bf9\u8fd9\u4e9b\u4e92\u52a8\u7684\u7406\u89e3\u3002", "result": "\u901a\u8fc7\u5728\u591a\u4e2a\u4eba\u673a\u4e92\u52a8\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u76ee\u524d\u7684\u591a\u6a21\u6001\u6a21\u578b\u65e0\u6cd5\u5168\u9762\u6355\u6349\u4eba\u7c7b\u4e92\u52a8\uff0c\u800c\u589e\u5f3a\u6a21\u578b\u7684MuRes\u6a21\u5757\u5219\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "conclusion": "Refer360\u6570\u636e\u96c6\u4f5c\u4e3a\u4e00\u4e2a\u5b9d\u8d35\u7684\u57fa\u51c6\uff0c\u5c55\u793a\u4e86\u6307\u5bfc\u6b8b\u5dee\u5b66\u4e60\u5728\u63d0\u9ad8\u673a\u5668\u4eba\u7684\u5177\u4f53\u6307\u4ee3\u7406\u89e3\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u4e0e\u4eba\u7c7b\u73af\u5883\u4e2d\u7684\u4e92\u52a8\u4e2d\u3002"}}
{"id": "2512.07474", "categories": ["cs.HC", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07474", "abs": "https://arxiv.org/abs/2512.07474", "authors": ["Yifei Huang", "Tianyu Yan", "Sitong Gong", "Xiwei Gao", "Caixin Kang", "Ruicong Liu", "Huchuan Lu", "Bo Zheng"], "title": "Living the Novel: A System for Generating Self-Training Timeline-Aware Conversational Agents from Novels", "comment": null, "summary": "We present the Living Novel, an end-to-end system that transforms any literary work into an immersive, multi-character conversational experience. This system is designed to solve two fundamental challenges for LLM-driven characters. Firstly, generic LLMs suffer from persona drift, often failing to stay in character. Secondly, agents often exhibit abilities that extend beyond the constraints of the story's world and logic, leading to both narrative incoherence (spoiler leakage) and robustness failures (frame-breaking). To address these challenges, we introduce a novel two-stage training pipeline. Our Deep Persona Alignment (DPA) stage uses data-free reinforcement finetuning to instill deep character fidelity. Our Coherence and Robustness Enhancing (CRE) stage then employs a story-time-aware knowledge graph and a second retrieval-grounded training pass to architecturally enforce these narrative constraints. We validate our system through a multi-phase evaluation using Jules Verne's Twenty Thousand Leagues Under the Sea. A lab study with a detailed ablation of system components is followed by a 5-day in-the-wild diary study. Our DPA pipeline helps our specialized model outperform GPT-4o on persona-specific metrics, and our CRE stage achieves near-perfect performance in coherence and robustness measures. Our study surfaces practical design guidelines for AI-driven narrative systems: we find that character-first self-training is foundational for believability, while explicit story-time constraints are crucial for sustaining coherent, interruption-resilient mobile-web experiences.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86Living Novel\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u89e3\u51b3\u4e86\u89d2\u8272\u4e00\u81f4\u6027\u548c\u53d9\u4e8b\u8fde\u8d2f\u6027\u95ee\u9898\uff0c\u9a8c\u8bc1\u7ed3\u679c\u8868\u660e\u7cfb\u7edf\u6027\u80fd\u5353\u8d8a\uff0c\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684AI\u53d9\u4e8b\u7cfb\u7edf\u8bbe\u8ba1\u6307\u5357\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89d2\u8272\u5728\u53d9\u8ff0\u4e2d\u51fa\u73b0\u7684\u4eba\u7269\u6f02\u79fb\u548c\u53d9\u4e8b\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u4ee5\u63d0\u5347\u6587\u5b66\u4f5c\u54c1\u7684\u6c89\u6d78\u611f\u548c\u8fde\u8d2f\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u7cfb\u7edf\uff08Living Novel\uff09\uff0c\u5c06\u4efb\u4f55\u6587\u5b66\u4f5c\u54c1\u8f6c\u53d8\u4e3a\u6c89\u6d78\u5f0f\u7684\u591a\u89d2\u8272\u5bf9\u8bdd\u4f53\u9a8c\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u6c34\u7ebf\u6765\u89e3\u51b3\u89d2\u8272\u4e00\u81f4\u6027\u548c\u53d9\u4e8b\u8fde\u8d2f\u6027\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u591a\u9636\u6bb5\u8bc4\u4f30\uff08\u4f7f\u7528\u300a\u6d77\u5e95\u4e24\u4e07\u91cc\u300b\u8fdb\u884c\u9a8c\u8bc1\uff09\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u7cfb\u7edf\u5728\u89d2\u8272\u7279\u5b9a\u6307\u6807\u4e0a\u4f18\u4e8eGPT-4o\uff0c\u5e76\u5728\u8fde\u8d2f\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u51e0\u4e4e\u8fbe\u5230\u4e86\u5b8c\u7f8e\u8868\u73b0\u3002", "conclusion": "\u63d0\u51fa\u7684\u8bbe\u8ba1\u6307\u5357\u8868\u660e\uff0c\u89d2\u8272\u4f18\u5148\u7684\u81ea\u6211\u8bad\u7ec3\u5bf9\u4e8e\u53ef\u4fe1\u6027\u81f3\u5173\u91cd\u8981\uff0c\u800c\u660e\u786e\u7684\u6545\u4e8b\u65f6\u95f4\u7ea6\u675f\u5bf9\u4e8e\u4fdd\u6301\u8fde\u8d2f\u7684\u79fb\u52a8\u7f51\u7edc\u4f53\u9a8c\u4e5f\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2512.06571", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06571", "abs": "https://arxiv.org/abs/2512.06571", "authors": ["Zifan Xu", "Myoungkyu Seo", "Dongmyeong Lee", "Hao Fu", "Jiaheng Hu", "Jiaxun Cui", "Yuqian Jiang", "Zhihan Wang", "Anastasiia Brund", "Joydeep Biswas", "Peter Stone"], "title": "Learning Agile Striker Skills for Humanoid Soccer Robots from Noisy Sensory Input", "comment": null, "summary": "Learning fast and robust ball-kicking skills is a critical capability for humanoid soccer robots, yet it remains a challenging problem due to the need for rapid leg swings, postural stability on a single support foot, and robustness under noisy sensory input and external perturbations (e.g., opponents). This paper presents a reinforcement learning (RL)-based system that enables humanoid robots to execute robust continual ball-kicking with adaptability to different ball-goal configurations. The system extends a typical teacher-student training framework -- in which a \"teacher\" policy is trained with ground truth state information and the \"student\" learns to mimic it with noisy, imperfect sensing -- by including four training stages: (1) long-distance ball chasing (teacher); (2) directional kicking (teacher); (3) teacher policy distillation (student); and (4) student adaptation and refinement (student). Key design elements -- including tailored reward functions, realistic noise modeling, and online constrained RL for adaptation and refinement -- are critical for closing the sim-to-real gap and sustaining performance under perceptual uncertainty. Extensive evaluations in both simulation and on a real robot demonstrate strong kicking accuracy and goal-scoring success across diverse ball-goal configurations. Ablation studies further highlight the necessity of the constrained RL, noise modeling, and the adaptation stage. This work presents a system for learning robust continual humanoid ball-kicking under imperfect perception, establishing a benchmark task for visuomotor skill learning in humanoid whole-body control.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4eba\u5f62\u673a\u5668\u4eba\u8e22\u7403\u7cfb\u7edf\uff0c\u901a\u8fc7\u6559\u5e08-\u5b66\u751f\u8bad\u7ec3\u6846\u67b6\u5b66\u4e60\u9c81\u68d2\u7684\u8e22\u7403\u6280\u80fd\uff0c\u7ecf\u8fc7\u591a\u9636\u6bb5\u8bad\u7ec3\u548c\u7cbe\u70bc\uff0c\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4eba\u5f62\u8db3\u7403\u673a\u5668\u4eba\u9700\u8981\u5feb\u901f\u3001\u7a33\u5065\u7684\u8e22\u7403\u6280\u80fd\uff0c\u4f46\u7531\u4e8e\u5feb\u901f\u817f\u90e8\u6446\u52a8\u3001\u5355\u811a\u652f\u6491\u7a33\u5b9a\u6027\u53ca\u5728\u5608\u6742\u611f\u77e5\u4e0b\u7684\u9c81\u68d2\u6027\u7b49\u95ee\u9898\uff0c\u4f7f\u5f97\u8fd9\u4e00\u4efb\u52a1\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u8be5\u7cfb\u7edf\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff0c\u6269\u5c55\u4e86\u4f20\u7edf\u7684\u6559\u5e08-\u5b66\u751f\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u4e2a\u8bad\u7ec3\u9636\u6bb5\u5b9e\u73b0\uff0c\u5305\u62ec\u8fdc\u7a0b\u8ffd\u7403\u3001\u65b9\u5411\u8e22\u7403\u3001\u6559\u5e08\u7b56\u7565\u84b8\u998f\u548c\u5b66\u751f\u9002\u5e94\u4e0e\u7cbe\u7ec3\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u5728\u4e0d\u540c\u7403\u95e8\u914d\u7f6e\u4e0b\u5f3a\u5927\u7684\u8e22\u7403\u51c6\u786e\u6027\u548c\u8fdb\u7403\u6210\u529f\u7387\uff0c\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u5f3a\u8c03\u4e86\u53d7\u9650\u5f3a\u5316\u5b66\u4e60\u3001\u566a\u58f0\u5efa\u6a21\u548c\u9002\u5e94\u9636\u6bb5\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5b66\u4e60\u9c81\u68d2\u7684\u6301\u7eed\u4eba\u5f62\u8e22\u7403\u6280\u5de7\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cfb\u7edf\uff0c\u8bbe\u5b9a\u4e86\u4eba\u5f62\u5168\u8eab\u63a7\u5236\u4e2d\u89c6\u89c9\u8fd0\u52a8\u6280\u80fd\u5b66\u4e60\u7684\u57fa\u51c6\u4efb\u52a1\u3002"}}
{"id": "2512.07483", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.07483", "abs": "https://arxiv.org/abs/2512.07483", "authors": ["Daniel F\u00fcrst", "Matthijs Jansen op de Haar", "Mennatallah El-Assady", "Daniel A Keim", "Maximilian T. Fischer"], "title": "SemanticTours: A Conceptual Framework for Non-Linear, Knowledge Graph-Driven Data Tours", "comment": "14 pages, 9 figures, 2 tables", "summary": "Interactive tours help users explore datasets and provide onboarding. They rely on a linear sequence of views, showing a curated set of relevant data selections and introduce user interfaces. Existing frameworks of tours, however, often do not allow for branching and refining hypotheses outside of a rigid sequence, which is important in knowledge-centric domains such as law. For example, lawyers performing analytical case analysis need to iteratively weigh up different legal norms and construct strings of arguments. To address this gap, we propose SemanticTours, a semantic, graph-based model of tours that shifts from a sequence-based towards a graph-based navigation. Our model constructs a domain-specific knowledge graph that connects data elements based on user-definable semantic relationships. These relationships enable non-linear graph navigation that defines tours. We apply SemanticTours to the domain of law and conceptualize a visual analytics design and interaction concept for analytical reasoning in legal case analysis. Our concept accounts for the inherent complexity of graph-based tours using aggregated graph nodes and supporting navigation with a semantic lens. During an evaluation with six domain experts from law, they suggest that graph-based tours better support their analytical reasoning than sequences. Our work opens research opportunities for such tours to support analytical reasoning in law and other knowledge-centric domains.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86SemanticTours\u6a21\u578b\uff0c\u901a\u8fc7\u8bed\u4e49\u56fe\u4e66\u9986\u5b9e\u73b0\u975e\u7ebf\u6027\u6570\u636e\u5bfc\u822a\uff0c\u4f18\u5316\u4e86\u6cd5\u5f8b\u9886\u57df\u7684\u5206\u6790\u63a8\u7406\u80fd\u529b\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u7ebf\u6027\u65c5\u884c\u6a21\u5f0f\u3002", "motivation": "\u73b0\u6709\u7684\u4e92\u52a8\u65c5\u884c\u6846\u67b6\u9650\u5236\u4e86\u7528\u6237\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u9886\u57df\uff08\u5982\u6cd5\u5f8b\uff09\u4e2d\u975e\u7ebf\u6027\u63a2\u7d22\u548c\u5047\u8bbe\u63a8\u7406\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86SemanticTours\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u57fa\u4e8e\u8bed\u4e49\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u652f\u6301\u975e\u7ebf\u6027\u5bfc\u822a\u548c\u66f4\u7075\u6d3b\u7684\u7528\u6237\u4ea4\u4e92\u3002", "result": "\u901a\u8fc7\u4e0e\u516d\u4f4d\u6cd5\u5f8b\u9886\u57df\u4e13\u5bb6\u7684\u8bc4\u4f30\uff0c\u5f97\u51fa\u56fe\u5f62\u65c5\u884c\u5728\u652f\u6301\u5206\u6790\u63a8\u7406\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u7684\u7ebf\u6027\u5e8f\u5217\u7684\u7ed3\u8bba\u3002", "conclusion": "\u56fe\u5f62\u5316\u7684\u65c5\u884c\u6a21\u5f0f\u6bd4\u7ebf\u6027\u6a21\u5f0f\u66f4\u597d\u5730\u652f\u6301\u6cd5\u5f8b\u9886\u57df\u4e13\u5bb6\u7684\u5206\u6790\u63a8\u7406\u3002"}}
{"id": "2512.06578", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06578", "abs": "https://arxiv.org/abs/2512.06578", "authors": ["Waleed Razzaq"], "title": "Error-Centric PID Untrained Neural-Net (EC-PIDUNN) For Nonlinear Robotics Control", "comment": "Under review at SoftComputing", "summary": "Classical Proportional-Integral-Derivative (PID) control has been widely successful across various industrial systems such as chemical processes, robotics, and power systems. However, as these systems evolved, the increase in the nonlinear dynamics and the complexity of interconnected variables have posed challenges that classical PID cannot effectively handle, often leading to instability, overshooting, or prolonged settling times. Researchers have proposed PIDNN models that combine the function approximation capabilities of neural networks with PID control to tackle these nonlinear challenges. However, these models require extensive, highly refined training data and have significant computational costs, making them less favorable for real-world applications. In this paper, We propose a novel EC-PIDUNN architecture, which integrates an untrained neural network with an improved PID controller, incorporating a stabilizing factor (\\(\u03c4\\)) to generate the control signal. Like classical PID, our architecture uses the steady-state error \\(e_t\\) as input bypassing the need for explicit knowledge of the systems dynamics. By forming an input vector from \\(e_t\\) within the neural network, we increase the dimensionality of input allowing for richer data representation. Additionally, we introduce a vector of parameters \\( \u03c1_t \\) to shape the output trajectory and a \\textit{dynamic compute} function to adjust the PID coefficients from predefined values. We validate the effectiveness of EC-PIDUNN on multiple nonlinear robotics applications: (1) nonlinear unmanned ground vehicle systems that represent the Ackermann steering mechanism and kinematics control, (2) Pan-Tilt movement system. In both tests, it outperforms classical PID in convergence and stability achieving a nearly critically damped response.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684EC-PIDUNN\u67b6\u6784\uff0c\u5c06\u672a\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc\u4e0e\u6539\u8fdb\u7684PID\u63a7\u5236\u5668\u76f8\u7ed3\u5408\uff0c\u4ee5\u66f4\u597d\u5730\u5904\u7406\u975e\u7ebf\u6027\u52a8\u6001\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u975e\u7ebf\u6027\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u968f\u7740\u5de5\u4e1a\u7cfb\u7edf\u7684\u6f14\u53d8\uff0c\u4f20\u7edfPID\u63a7\u5236\u9762\u4e34\u975e\u7ebf\u6027\u52a8\u6001\u548c\u590d\u6742\u53d8\u91cf\u7684\u6311\u6218\uff0c\u96be\u4ee5\u6709\u6548\u5e94\u5bf9\u3002", "method": "\u63d0\u51fa\u4e86EC-PIDUNN\u67b6\u6784\uff0c\u6574\u5408\u672a\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc\u4e0e\u6539\u8fdb\u7684PID\u63a7\u5236\u5668\uff0c\u4f7f\u7528\u7a33\u6001\u8bef\u5dee\u4f5c\u4e3a\u8f93\u5165\uff0c\u589e\u52a0\u8f93\u5165\u7684\u7ef4\u5ea6\uff0c\u63d0\u5347\u6570\u636e\u8868\u793a\u80fd\u529b\uff0c\u540c\u65f6\u5f15\u5165\u53c2\u6570\u5411\u91cf\u6765\u8c03\u8282\u8f93\u51fa\u8f68\u8ff9\u3002", "result": "EC-PIDUNN\u67b6\u6784\u5728\u591a\u79cd\u975e\u7ebf\u6027\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u5f97\u5230\u4e86\u9a8c\u8bc1.", "conclusion": "EC-PIDUNN\u67b6\u6784\u6bd4\u4f20\u7edfPID\u5728\u6536\u655b\u6027\u548c\u7a33\u5b9a\u6027\u4e0a\u8868\u73b0\u66f4\u4f73\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u4e34\u754c\u963b\u5c3c\u7684\u54cd\u5e94."}}
{"id": "2512.07613", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.07613", "abs": "https://arxiv.org/abs/2512.07613", "authors": ["Arthur Fleig"], "title": "A Retrospective on Ultrasound Mid-Air Haptics in HCI", "comment": "3 pages; AlpCHI 2026 Revisiting HCI Research Track", "summary": "In 2013, the UltraHaptics system demonstrated that focused ultrasound could generate perceivable mid-air tactile sensations, building on earlier explorations of airborne ultrasound as a haptic medium. These contributions established ultrasound mid-air haptics (UMH) as a viable interaction modality and laid the technical and perceptual foundations for subsequent advances in Human-Computer Interaction (HCI). In this extended abstract, we revisit this formative work, trace the research and design trajectories it enabled, and reflect on how UMH has supported multisensory interaction, immersion, and inclusion. We also highlight how this line of research exemplifies the value of interdisciplinary collaboration to advance novel interactive technologies.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86\u8d85\u58f0\u6ce2\u4e2d\u7a7a\u89e6\u89c9\u7684\u53d1\u5c55\u5386\u7a0b\u53ca\u5176\u5728HCI\u4e2d\u7684\u5e94\u7528\uff0c\u5f3a\u8c03\u4e86\u8de8\u5b66\u79d1\u5408\u4f5c\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u63a2\u8ba8\u8d85\u58f0\u6ce2\u4e2d\u7a7a\u89e6\u89c9\u5982\u4f55\u5728\u591a\u611f\u5b98\u4e92\u52a8\u4e2d\u53d1\u6325\u4f5c\u7528\uff0c\u5e76\u63a8\u52a8HCI\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u56de\u987e2013\u5e74UltraHaptics\u7cfb\u7edf\u7684\u5de5\u4f5c", "result": "\u786e\u7acb\u4e86\u8d85\u58f0\u6ce2\u4e2d\u7a7a\u89e6\u89c9\u4f5c\u4e3a\u4e00\u79cd\u53ef\u884c\u7684\u4ea4\u4e92\u65b9\u5f0f", "conclusion": "\u8d85\u58f0\u6ce2\u4e2d\u7a7a\u89e6\u89c9\u652f\u6301\u591a\u611f\u5b98\u4e92\u52a8\u3001\u6c89\u6d78\u611f\u548c\u5305\u5bb9\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u8de8\u5b66\u79d1\u5408\u4f5c\u63a8\u52a8\u65b0\u4e92\u52a8\u6280\u672f\u7684\u4ef7\u503c"}}
{"id": "2512.06608", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06608", "abs": "https://arxiv.org/abs/2512.06608", "authors": ["Xinyu Zhou", "Songhao Piao", "Chao Gao", "Liguo Chen"], "title": "A New Trajectory-Oriented Approach to Enhancing Comprehensive Crowd Navigation Performance", "comment": "8 pages, 6 figures", "summary": "Crowd navigation has garnered considerable research interest in recent years, especially with the proliferating application of deep reinforcement learning (DRL) techniques. Many studies, however, do not sufficiently analyze the relative priorities among evaluation metrics, which compromises the fair assessment of methods with divergent objectives. Furthermore, trajectory-continuity metrics, specifically those requiring $C^2$ smoothness, are rarely incorporated. Current DRL approaches generally prioritize efficiency and proximal comfort, often neglecting trajectory optimization or addressing it only through simplistic, unvalidated smoothness reward. Nevertheless, effective trajectory optimization is essential to ensure naturalness, enhance comfort, and maximize the energy efficiency of any navigation system. To address these gaps, this paper proposes a unified framework that enables the fair and transparent assessment of navigation methods by examining the prioritization and joint evaluation of multiple optimization objectives. We further propose a novel reward-shaping strategy that explicitly emphasizes trajectory-curvature optimization. The resulting trajectory quality and adaptability are significantly enhanced across multi-scale scenarios. Through extensive 2D and 3D experiments, we demonstrate that the proposed method achieves superior performance compared to state-of-the-art approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6846\u67b6\u548c\u65b0\u7684\u5956\u52b1\u5851\u5f62\u7b56\u7565\uff0c\u4ee5\u6539\u8fdb\u7fa4\u4f53\u5bfc\u822a\u4e2d\u7684\u8f68\u8ff9\u4f18\u5316\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u7fa4\u4f53\u5bfc\u822a\u4e2d\u5e38\u5e38\u5ffd\u89c6\u8f68\u8ff9\u4f18\u5316\u548c\u8bc4\u4f30\u6307\u6807\u7684\u76f8\u5bf9\u4f18\u5148\u7ea7\uff0c\u5bfc\u81f4\u8bc4\u4f30\u4e0d\u516c\u6b63\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\uff0c\u91cd\u70b9\u5173\u6ce8\u591a\u4e2a\u4f18\u5316\u76ee\u6807\u7684\u8054\u5408\u8bc4\u4f30\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u5956\u52b1\u5851\u5f62\u7b56\u7565\uff0c\u5f3a\u8c03\u8f68\u8ff9\u66f2\u7387\u4f18\u5316\u3002", "result": "\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u901a\u8fc7\u4f18\u5148\u8003\u8651\u548c\u8054\u5408\u8bc4\u4f30\u591a\u4e2a\u4f18\u5316\u76ee\u6807\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5bfc\u822a\u65b9\u6cd5\u7684\u516c\u6b63\u900f\u660e\u8bc4\u4f30\uff0c\u6539\u8fdb\u4e86\u8f68\u8ff9\u8d28\u91cf\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u5956\u52b1\u5851\u5f62\u7b56\u7565\u6709\u6548\u7a81\u51fa\u4e86\u8f68\u8ff9\u66f2\u7387\u4f18\u5316\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2512.07623", "categories": ["cs.HC", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.07623", "abs": "https://arxiv.org/abs/2512.07623", "authors": ["Lalitha A R"], "title": "Context-Adaptive Color Optimization for Web Accessibility: Balancing Perceptual Fidelity and Functional Requirements", "comment": "8 pages, 2 figures", "summary": "We extend our OKLCH-based accessibility optimization with context-adaptive constraint strategies that achieve near-universal success rates across diverse use cases. Our original strict algorithm reached 66-77% success by prioritizing minimal perceptual change ($\u0394E \\leq 5.0$), optimizing for enterprise contexts where brand fidelity is paramount. However, this one-size-fits-all approach fails to serve the broader ecosystem of web developers who need accessible solutions even when strict perceptual constraints cannot be satisfied. We introduce recursive optimization (Mode~1) that compounds small adjustments across iterations, achieving 93.68% success on all color pairs and 100% success on reasonable pairs (contrast ratio $\u03c1> 2.0$), representing a +27.23 percentage point improvement. A relaxed fallback mode (Mode~2) handles pathological edge cases, reaching 98.73% overall success. Evaluation on 10,000 realistic web color pairs demonstrates that context-aware constraint relaxation, combined with absolute hue preservation, enables practical accessibility compliance while maintaining brand color identity. The median perceptual change remains zero across all modes (most pairs already comply), while the 90th percentile reaches $\u0394E_{2000} = 15.55$ in Mode~1 -- perceptually acceptable when hue invariance preserves the essential character of the original color. The approach is deployed in CM-Colors v0.5.0 (800+ monthly downloads), providing developers with explicit control over the accessibility-fidelity trade-off appropriate to their context.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528OKLCH\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u60c5\u5883\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u63d0\u9ad8\u4e86\u7f51\u7edc\u989c\u8272\u914d\u5bf9\u7684\u53ef\u53ca\u6027\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u529f\u7387\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4e25\u683c\u7684\u611f\u77e5\u7ea6\u675f\u65e0\u6cd5\u6ee1\u8db3\u7684\u9700\u6c42\uff0c\u4e3a\u5e7f\u5927\u7f51\u7edc\u5f00\u53d1\u8005\u63d0\u4f9b\u53ef\u63a5\u5165\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u9012\u5f52\u4f18\u5316\u548c\u653e\u5bbd\u7684\u5907\u7528\u6a21\u5f0f\uff0c\u4ee5\u9002\u5e94\u4e0d\u540c\u7684\u573a\u666f\u9700\u6c42\u548c\u5b9e\u73b0\u9ad8\u6210\u529f\u7387\u3002", "result": "\u572810,000\u4e2a\u73b0\u5b9e\u7684\u7f51\u7edc\u8272\u5f69\u5bf9\u4e0a\uff0c\u65b9\u6cd5\u7684\u603b\u4f53\u6210\u529f\u7387\u8fbe\u5230\u4e8698.73%\uff0c\u6781\u5927\u5730\u63d0\u5347\u4e86\u53ef\u63a5\u5165\u6027\uff0c\u800c\u5bf9\u54c1\u724c\u8272\u5f69\u7684\u4fdd\u7559\u5f97\u5230\u4e86\u6709\u6548\u63a7\u5236\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u60c5\u5883\u81ea\u9002\u5e94\u7ea6\u675f\u7b56\u7565\u548c\u9012\u5f52\u4f18\u5316\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7f51\u7edc\u8272\u5f69\u914d\u5bf9\u7684\u53ef\u53ca\u6027\uff0c\u786e\u4fdd\u4e86\u54c1\u724c\u8272\u5f69\u7684\u5b8c\u6574\u6027\u3002"}}
{"id": "2512.06610", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06610", "abs": "https://arxiv.org/abs/2512.06610", "authors": ["Marvin Harms", "Jaeyoung Lim", "David Rohr", "Friedrich Rockenbauer", "Nicholas Lawrance", "Roland Siegwart"], "title": "Robust Optimization-based Autonomous Dynamic Soaring with a Fixed-Wing UAV", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Dynamic soaring is a flying technique to exploit the energy available in wind shear layers, enabling potentially unlimited flight without the need for internal energy sources. We propose a framework for autonomous dynamic soaring with a fixed-wing unmanned aerial vehicle (UAV). The framework makes use of an explicit representation of the wind field and a classical approach for guidance and control of the UAV. Robustness to wind field estimation error is achieved by constructing point-wise robust reference paths for dynamic soaring and the development of a robust path following controller for the fixed-wing UAV. The framework is evaluated in dynamic soaring scenarios in simulation and real flight tests. In simulation, we demonstrate robust dynamic soaring flight subject to varied wind conditions, estimation errors and disturbances. Critical components of the framework, including energy predictions and path-following robustness, are further validated in real flights to assure small sim-to-real gap. Together, our results strongly indicate the ability of the proposed framework to achieve autonomous dynamic soaring flight in wind shear.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u56fa\u5b9a\u7ffc\u65e0\u4eba\u673a\u7684\u81ea\u4e3b\u52a8\u6001\u6ed1\u7fd4\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5176\u5728\u590d\u6742\u98ce\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u65e0\u9700\u5185\u6e90\u80fd\u91cf\u7684\u98de\u884c\u3002", "motivation": "\u52a8\u529b\u6ed1\u7fd4\u662f\u4e00\u79cd\u5229\u7528\u98ce\u526a\u5207\u5c42\u4e2d\u53ef\u7528\u80fd\u91cf\u7684\u98de\u884c\u6280\u672f\uff0c\u53ef\u80fd\u5b9e\u73b0\u65e0\u9650\u98de\u884c\uff0c\u800c\u65e0\u9700\u5185\u90e8\u80fd\u91cf\u6e90\u3002", "method": "\u8be5\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u7684\u98ce\u573a\u8868\u793a\u548c\u7ecf\u5178\u7684\u65e0\u4eba\u673a\u5f15\u5bfc\u63a7\u5236\u65b9\u6cd5\uff0c\u6784\u5efa\u9c81\u68d2\u7684\u53c2\u8003\u8def\u5f84\u548c\u8def\u5f84\u8ddf\u968f\u63a7\u5236\u5668\uff0c\u4ee5\u5e94\u5bf9\u98ce\u573a\u4f30\u8ba1\u8bef\u5dee\u3002", "result": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u56fa\u5b9a\u7ffc\u65e0\u4eba\u673a\u7684\u81ea\u4e3b\u52a8\u6001\u6ed1\u7fd4\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u8868\u793a\u98ce\u573a\u548c\u7ecf\u5178\u7684\u5f15\u5bfc\u63a7\u5236\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5145\u5206\u5229\u7528\u98ce\u526a\u5207\u5c42\u4e2d\u80fd\u91cf\u7684\u52a8\u6001\u6ed1\u7fd4\u6280\u672f\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5c55\u793a\u4e86\u5728\u591a\u79cd\u98ce\u6761\u4ef6\u3001\u4f30\u8ba1\u8bef\u5dee\u548c\u5e72\u6270\u4e0b\u7684\u52a8\u6001\u6ed1\u7fd4\u98de\u884c\u7684\u5f3a\u5927\u9c81\u68d2\u6027\uff0c\u5e76\u9a8c\u8bc1\u4e86\u80fd\u91cf\u9884\u6d4b\u548c\u8def\u5f84\u8ddf\u968f\u7684\u53ef\u9760\u6027\uff0c\u8868\u660e\u8be5\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u81ea\u4e3b\u52a8\u6001\u6ed1\u7fd4\u98de\u884c\u3002"}}
{"id": "2512.07820", "categories": ["cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07820", "abs": "https://arxiv.org/abs/2512.07820", "authors": ["Prithila Angkan", "Amin Jalali", "Paul Hungler", "Ali Etemad"], "title": "Graph-Based Learning of Spectro-Topographical EEG Representations with Gradient Alignment for Brain-Computer Interfaces", "comment": null, "summary": "We present a novel graph-based learning of EEG representations with gradient alignment (GEEGA) that leverages multi-domain information to learn EEG representations for brain-computer interfaces. Our model leverages graph convolutional networks to fuse embeddings from frequency-based topographical maps and time-frequency spectrograms, capturing inter-domain relationships. GEEGA addresses the challenge of achieving high inter-class separability, which arises from the temporally dynamic and subject-sensitive nature of EEG signals by incorporating the center loss and pairwise difference loss. Additionally, GEEGA incorporates a gradient alignment strategy to resolve conflicts between gradients from different domains and the fused embeddings, ensuring that discrepancies, where gradients point in conflicting directions, are aligned toward a unified optimization direction. We validate the efficacy of our method through extensive experiments on three publicly available EEG datasets: BCI-2a, CL-Drive and CLARE. Comprehensive ablation studies further highlight the impact of various components of our model.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684EEG\u8868\u793a\u5b66\u4e60\u6a21\u578bGEEGA\uff0c\u901a\u8fc7\u591a\u57df\u4fe1\u606f\u8fdb\u884c\u8111\u673a\u63a5\u53e3\u7684EEG\u8868\u793a\u5b66\u4e60\u3002", "motivation": "\u9488\u5bf9EEG\u4fe1\u53f7\u7684\u52a8\u6001\u7279\u6027\u548c\u4e2a\u4f53\u5dee\u5f02\u6027\uff0c\u63d0\u9ad8\u7c7b\u95f4\u53ef\u5206\u79bb\u6027\u3002", "method": "\u4f7f\u7528\u56fe\u5377\u79ef\u7f51\u7edc\u878d\u5408\u9891\u7387\u5730\u56fe\u548c\u65f6\u9891\u56fe\uff0c\u7ed3\u5408\u4e2d\u5fc3\u635f\u5931\u548c\u6210\u5bf9\u5dee\u5f02\u635f\u5931\uff0c\u5e76\u5f15\u5165\u68af\u5ea6\u5bf9\u9f50\u7b56\u7565\u3002", "result": "\u5728BCI-2a\u3001CL-Drive\u548cCLARE\u4e09\u4e2aEEG\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u7ed3\u679c\u6709\u6548\u3002", "conclusion": "\u6a21\u578b\u5404\u7ec4\u6210\u90e8\u5206\u7684\u5f71\u54cd\u901a\u8fc7\u5168\u9762\u7684\u6d88\u878d\u7814\u7a76\u8fdb\u884c\u4e86\u5f3a\u8c03\u3002"}}
{"id": "2512.06628", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06628", "abs": "https://arxiv.org/abs/2512.06628", "authors": ["Ruicheng Zhang", "Mingyang Zhang", "Jun Zhou", "Zhangrui Guo", "Xiaofan Liu", "Zunnan Xu", "Zhizhou Zhong", "Puxin Yan", "Haocheng Luo", "Xiu Li"], "title": "MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment", "comment": null, "summary": "Embodied imitation learning is constrained by the scarcity of diverse, long-horizon robotic manipulation data. Existing video generation models for this domain are limited to synthesizing short clips of simple actions and often rely on manually defined trajectories. To this end, we introduce MIND-V, a hierarchical framework designed to synthesize physically plausible and logically coherent videos of long-horizon robotic manipulation. Inspired by cognitive science, MIND-V bridges high-level reasoning with pixel-level synthesis through three core components: a Semantic Reasoning Hub (SRH) that leverages a pre-trained vision-language model for task planning; a Behavioral Semantic Bridge (BSB) that translates abstract instructions into domain-invariant representations; and a Motor Video Generator (MVG) for conditional video rendering. MIND-V employs Staged Visual Future Rollouts, a test-time optimization strategy to enhance long-horizon robustness. To align the generated videos with physical laws, we introduce a GRPO reinforcement learning post-training phase guided by a novel Physical Foresight Coherence (PFC) reward. PFC leverages the V-JEPA world model to enforce physical plausibility by aligning the predicted and actual dynamic evolutions in the feature space. MIND-V demonstrates state-of-the-art performance in long-horizon robotic manipulation video generation, establishing a scalable and controllable paradigm for embodied data synthesis.", "AI": {"tldr": "MIND-V\u662f\u4e00\u4e2a\u5c42\u6b21\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u63a8\u7406\u548c\u89c6\u9891\u751f\u6210\uff0c\u63d0\u9ad8\u4e86\u957f\u65f6\u95f4\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u89c6\u9891\u751f\u6210\u80fd\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u51fa\u8272\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u673a\u5668\u4eba\u64cd\u4f5c\u89c6\u9891\u751f\u6210\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u751f\u6210\u89c6\u9891\u7684\u7269\u7406\u53ef\u884c\u6027\u548c\u903b\u8f91\u4e00\u81f4\u6027\u3002", "method": "\u4ecb\u7ecdMIND-V\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u62ec\u8bed\u4e49\u63a8\u7406\u4e2d\u5fc3\uff08SRH\uff09\u3001\u884c\u4e3a\u8bed\u4e49\u6865\uff08BSB\uff09\u548c\u8fd0\u52a8\u89c6\u9891\u751f\u6210\u5668\uff08MVG\uff09\uff0c\u7528\u4e8e\u751f\u6210\u957f\u65f6\u95f4\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u89c6\u9891\u3002\u5229\u7528\u9636\u6bb5\u6027\u89c6\u89c9\u672a\u6765\u56de\u653e\u548c\u7269\u7406\u9884\u89c1\u4e00\u81f4\u6027\uff08PFC\uff09\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u6765\u63d0\u9ad8\u751f\u6210\u89c6\u9891\u7684\u7269\u7406\u53ef\u884c\u6027\u3002", "result": "MIND-V\u5728\u957f\u65f6\u95f4\u673a\u5668\u4eba\u64cd\u63a7\u89c6\u9891\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "MIND-V\u5efa\u7acb\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u53ef\u63a7\u7684\u673a\u5668\u4eba\u6570\u636e\u5408\u6210\u8303\u4f8b\uff0c\u5177\u6709\u826f\u597d\u7684\u7269\u7406\u53ef\u884c\u6027\u548c\u903b\u8f91\u4e00\u81f4\u6027\u3002"}}
{"id": "2512.06664", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06664", "abs": "https://arxiv.org/abs/2512.06664", "authors": ["Wei-Bin Kou", "Guangxu Zhu", "Jingreng Lei", "Chen Zhang", "Yik-Chung Wu", "Jianping Wang"], "title": "Statistic-Augmented, Decoupled MoE Routing and Aggregating in Autonomous Driving", "comment": "9 pages", "summary": "Autonomous driving (AD) scenarios are inherently complex and diverse, posing significant challenges for a single deep learning model to effectively cover all possible conditions, such as varying weather, traffic densities, and road types. Large Model (LM)-Driven Mixture of Experts (MoE) paradigm offers a promising solution, where LM serves as the backbone to extract latent features while MoE serves as the downstream head to dynamically select and aggregate specialized experts to adapt to different scenarios. However, routing and aggregating in MoE face intrinsic challenges, including imprecise expert selection due to flawed routing strategy and inefficient expert aggregation leading to suboptimal prediction. To address these issues, we propose a statistic-augmented, decoupled MoE }outing and Aggregating Mechanism (MoE-RAM) driven by LM. Specifically, on the one hand, MoE-RAM enhances expert routing by incorporating statistical retrieval mechanism to match LM-extracted latent features with cached prototypical features of the most relevant experts; on the other hand, MoE-RAM adaptively reweights experts' outputs in fusion by measuring statistical distances of experts' instant features against LM-extracted latent features. Benefiting from the synergy of the statistic-augmented MoE's routing and aggregating, MoE-RAM ultimately improves the prediction performance. We take the AD semantic segmentation task as an example to assess the proposed MoE-RAM. Extensive experiments on AD datasets demonstrate the superiority of MoE-RAM compared to other MoE baselines and conventional single-model approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u8ba1\u589e\u5f3a\u7684\u3001\u89e3\u8026\u7684\u6df7\u5408\u4e13\u5bb6\u8def\u7531\u4e0e\u805a\u5408\u673a\u5236\uff08MoE-RAM\uff09\uff0c\u901a\u8fc7\u6539\u8fdb\u4e13\u5bb6\u9009\u62e9\u548c\u8f93\u51fa\u805a\u5408\u6765\u4f18\u5316\u590d\u6742\u81ea\u4e3b\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u81ea\u4e3b\u9a7e\u9a76\u573a\u666f\u590d\u6742\u591a\u53d8\uff0c\u5355\u4e00\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u96be\u4ee5\u8986\u76d6\u6240\u6709\u60c5\u51b5\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u7075\u6d3b\u7684\u6a21\u578b\u7ed3\u6784\u6765\u5e94\u5bf9\u4e0d\u540c\u7684\u73af\u5883\u548c\u6761\u4ef6\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7edf\u8ba1\u589e\u5f3a\u7684\u89e3\u8026\u6df7\u5408\u4e13\u5bb6\u8def\u7531\u4e0e\u805a\u5408\u673a\u5236\uff08MoE-RAM\uff09\uff0c\u5229\u7528\u7edf\u8ba1\u68c0\u7d22\u673a\u5236\u6539\u5584\u4e13\u5bb6\u9009\u62e9\u548c\u81ea\u9002\u5e94\u91cd\u65b0\u52a0\u6743\u4e13\u5bb6\u8f93\u51fa\u3002", "result": "\u5728\u81ea\u4e3b\u9a7e\u9a76\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMoE-RAM\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u5176\u4ed6MoE\u57fa\u51c6\u548c\u4f20\u7edf\u5355\u6a21\u578b\u65b9\u6cd5\u3002", "conclusion": "MoE-RAM\u663e\u8457\u63d0\u5347\u4e86\u81ea\u4e3b\u9a7e\u9a76\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u4f18\u4e8e\u5176\u4ed6MoE\u57fa\u51c6\u6a21\u578b\u548c\u5e38\u89c4\u5355\u4e00\u6a21\u578b\u65b9\u6cd5\u3002"}}
{"id": "2512.06676", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06676", "abs": "https://arxiv.org/abs/2512.06676", "authors": ["Wei-Bin Kou", "Guangxu Zhu", "Bingyang Cheng", "Chen Zhang", "Yik-Chung Wu", "Jianping Wang"], "title": "FedDSR: Federated Deep Supervision and Regularization Towards Autonomous Driving", "comment": "9 pages", "summary": "Federated Learning (FL) enables collaborative training of autonomous driving (AD) models across distributed vehicles while preserving data privacy. However, FL encounters critical challenges such as poor generalization and slow convergence due to non-independent and identically distributed (non-IID) data from diverse driving environments. To overcome these obstacles, we introduce Federated Deep Supervision and Regularization (FedDSR), a paradigm that incorporates multi-access intermediate layer supervision and regularization within federated AD system. Specifically, FedDSR comprises following integral strategies: (I) to select multiple intermediate layers based on predefined architecture-agnostic standards. (II) to compute mutual information (MI) and negative entropy (NE) on those selected layers to serve as intermediate loss and regularizer. These terms are integrated into the output-layer loss to form a unified optimization objective, enabling comprehensive optimization across the network hierarchy. (III) to aggregate models from vehicles trained based on aforementioned rules of (I) and (II) to generate the global model on central server. By guiding and penalizing the learning of feature representations at intermediate stages, FedDSR enhances the model generalization and accelerates model convergence for federated AD. We then take the semantic segmentation task as an example to assess FedDSR and apply FedDSR to multiple model architectures and FL algorithms. Extensive experiments demonstrate that FedDSR achieves up to 8.93% improvement in mIoU and 28.57% reduction in training rounds, compared to other FL baselines, making it highly suitable for practical deployment in federated AD ecosystems.", "AI": {"tldr": "\u8054\u90a6\u6df1\u5ea6\u76d1\u7763\u548c\u6b63\u5219\u5316 (FedDSR) \u6539\u8fdb\u4e86\u5206\u5e03\u5f0f\u81ea\u4e3b\u9a7e\u9a76\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u679c\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u6027\u548c\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u5728\u81ea\u4e3b\u9a7e\u9a76\u9886\u57df\u9762\u4e34\u7684\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u5bfc\u81f4\u7684\u6cdb\u5316\u5dee\u548c\u6536\u655b\u6162\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u8054\u90a6\u6df1\u5ea6\u76d1\u7763\u548c\u6b63\u5219\u5316\u673a\u5236 (FedDSR)\uff0c\u7ed3\u5408\u591a\u5c42\u4e2d\u95f4\u5c42\u76d1\u7763\u548c\u6b63\u5219\u5316", "result": "\u901a\u8fc7\u9009\u62e9\u591a\u5c42\u4e2d\u95f4\u5c42\u3001\u8ba1\u7b97\u4e92\u4fe1\u606f\u548c\u8d1f\u71b5\u4f5c\u4e3a\u4e2d\u95f4\u635f\u5931\u548c\u6b63\u5219\u9879", "conclusion": "FedDSR \u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u52a0\u901f\u4e86\u8054\u90a6\u81ea\u4e3b\u9a7e\u9a76\u7684\u6a21\u578b\u6536\u655b"}}
{"id": "2512.06754", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.06754", "abs": "https://arxiv.org/abs/2512.06754", "authors": ["Shrreya Rajneesh", "Nikita Pavle", "Rakesh Kumar Sahoo", "Manoranjan Sinha"], "title": "Model-Less Feedback Control of Space-based Continuum Manipulators using Backbone Tension Optimization", "comment": null, "summary": "Continuum manipulators offer intrinsic dexterity and safe geometric compliance for navigation within confined and obstacle-rich environments. However, their infinite-dimensional backbone deformation, unmodeled internal friction, and configuration-dependent stiffness fundamentally limit the reliability of model-based kinematic formulations, resulting in inaccurate Jacobian predictions, artificial singularities, and unstable actuation behavior. Motivated by these limitations, this work presents a complete model-less control framework that bypasses kinematic modeling by using an empirically initialized Jacobian refined online through differential convex updates. Tip motion is generated via a real-time quadratic program that computes actuator increments while enforcing tendon slack avoidance and geometric limits. A backbone tension optimization term is introduced in this paper to regulate axial loading and suppress co-activation compression. The framework is validated across circular, pentagonal, and square trajectories, demonstrating smooth convergence, stable tension evolution, and sub-millimeter steady-state accuracy without any model calibration or parameter identification. These results establish the proposed controller as a scalable alternative to model-dependent continuum manipulation in a constrained environment.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u6a21\u578b\u63a7\u5236\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u8fde\u7eed\u64cd\u7eb5\u5668\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u5b66\u5efa\u6a21\u95ee\u9898\uff0c\u901a\u8fc7\u5b9e\u65f6\u4f18\u5316\u5b9e\u73b0\u4e86\u7cbe\u786e\u63a7\u5236\u3002", "motivation": "\u7531\u4e8e\u8fde\u7eed\u64cd\u7eb5\u5668\u5728\u590d\u6742\u73af\u5883\u4e2d\u8fd0\u52a8\u5b66\u5efa\u6a21\u7684\u5c40\u9650\u6027\uff0c\u5305\u62ec\u65e0\u9650\u7ef4\u80cc\u90e8\u53d8\u5f62\u3001\u672a\u5efa\u6a21\u7684\u5185\u90e8\u6469\u64e6\u548c\u914d\u7f6e\u4f9d\u8d56\u7684\u521a\u5ea6\uff0c\u5bfc\u81f4\u4e86\u4e0d\u51c6\u786e\u7684\u96c5\u53ef\u6bd4\u9884\u6d4b\u548c\u4e0d\u7a33\u5b9a\u7684\u6267\u884c\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u6a21\u578b\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u7ecf\u9a8c\u521d\u59cb\u5316\u7684\u96c5\u53ef\u6bd4\u77e9\u9635\u5728\u7ebf\u7ec6\u5316\uff0c\u4ee5\u907f\u514d\u8fd0\u52a8\u5b66\u5efa\u6a21\u7684\u9650\u5236\u3002", "result": "\u5728\u4e0d\u540c\u5f62\u72b6\u7684\u8f68\u8ff9\uff08\u5706\u5f62\u3001\u4e94\u8fb9\u5f62\u548c\u65b9\u5f62\uff09\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5e73\u6ed1\u6536\u655b\u3001\u7a33\u5b9a\u7684\u5f20\u529b\u6f14\u53d8\u548c\u4e9a\u6beb\u7c73\u7684\u7a33\u6001\u7cbe\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u7684\u63a7\u5236\u5668\u88ab\u5efa\u7acb\u4e3a\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u65e0\u9700\u6a21\u578b\u6821\u51c6\u6216\u53c2\u6570\u8bc6\u522b\u3002"}}
{"id": "2512.06796", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06796", "abs": "https://arxiv.org/abs/2512.06796", "authors": ["Akmaral Moldagalieva", "Keisuke Okumura", "Amanda Prorok", "Wolfgang H\u00f6nig"], "title": "db-LaCAM: Fast and Scalable Multi-Robot Kinodynamic Motion Planning with Discontinuity-Bounded Search and Lightweight MAPF", "comment": null, "summary": "State-of-the-art multi-robot kinodynamic motion planners struggle to handle more than a few robots due to high computational burden, which limits their scalability and results in slow planning time.\n  In this work, we combine the scalability and speed of modern multi-agent path finding (MAPF) algorithms with the dynamic-awareness of kinodynamic planners to address these limitations.\n  To this end, we propose discontinuity-Bounded LaCAM (db-LaCAM), a planner that utilizes a precomputed set of motion primitives that respect robot dynamics to generate horizon-length motion sequences, while allowing a user-defined discontinuity between successive motions.\n  The planner db-LaCAM is resolution-complete with respect to motion primitives and supports arbitrary robot dynamics.\n  Extensive experiments demonstrate that db-LaCAM scales efficiently to scenarios with up to 50 robots, achieving up to ten times faster runtime compared to state-of-the-art planners, while maintaining comparable solution quality.\n  The approach is validated in both 2D and 3D environments with dynamics such as the unicycle and 3D double integrator.\n  We demonstrate the safe execution of trajectories planned with db-LaCAM in two distinct physical experiments involving teams of flying robots and car-with-trailer robots.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fadb-LaCAM\u89c4\u5212\u5668\uff0c\u7ed3\u5408\u73b0\u4ee3\u591a\u4ee3\u7406\u8def\u5f84\u5bfb\u627e\u7b97\u6cd5\u4e0e\u52a8\u529b\u5b66\u89c4\u5212\uff0c\u663e\u8457\u63d0\u9ad8\u591a\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u7684\u53ef\u6269\u5c55\u6027\u548c\u901f\u5ea6\uff0c\u80fd\u591f\u5904\u7406\u9ad8\u8fbe50\u4e2a\u673a\u5668\u4eba\u5e76\u5728\u7269\u7406\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u5176\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u56e0\u8ba1\u7b97\u8d1f\u62c5\u8fc7\u91cd\u800c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\uff0c\u5bfc\u81f4\u89c4\u5212\u65f6\u95f4\u7f13\u6162\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u6765\u4f18\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u89c4\u5212\u5668db-LaCAM\uff0c\u5176\u5229\u7528\u9884\u8ba1\u7b97\u7684\u8fd0\u52a8\u539f\u8bed\u548c\u7528\u6237\u5b9a\u4e49\u7684\u8fd0\u52a8\u95f4\u65ad\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u8fd0\u52a8\u89c4\u5212\u3002", "result": "db-LaCAM\u80fd\u591f\u57282D\u548c3D\u73af\u5883\u4e2d\u9ad8\u6548\u6027\u80fd\u5e76\u9a8c\u8bc1\u4e86\u5176\u5b89\u5168\u6027\uff0c\u9002\u7528\u4e8e\u98de\u884c\u673a\u5668\u4eba\u548c\u62d6\u8f66\u8f66\u961f\u7b49\u52a8\u6001\u573a\u666f\u7684\u7269\u7406\u5b9e\u9a8c\u3002", "conclusion": "db-LaCAM\u89c4\u5212\u5668\u5728\u52a8\u6001\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u9ad8\u6548\u5730\u5904\u7406\u591a\u8fbe50\u4e2a\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u89c4\u5212\uff0c\u4e14\u8fd0\u884c\u901f\u5ea6\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7684\u89c4\u5212\u5668\u5feb\u5341\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u4f3c\u7684\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u3002"}}
{"id": "2512.06829", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06829", "abs": "https://arxiv.org/abs/2512.06829", "authors": ["Oluwatimilehin Tijani", "Zhuo Chen", "Jiankang Deng", "Shan Luo"], "title": "MagicSkin: Balancing Marker and Markerless Modes in Vision-Based Tactile Sensors with a Translucent Skin", "comment": "Submitted to ICRA2026", "summary": "Vision-based tactile sensors (VBTS) face a fundamental trade-off in marker and markerless design on the tactile skin: opaque ink markers enable measurement of force and tangential displacement but completely occlude geometric features necessary for object and texture classification, while markerless skin preserves surface details but struggles in measuring tangential displacements effectively. Current practice to solve the above problem via UV lighting or virtual transfer using learning-based models introduces hardware complexity or computing burdens. This paper introduces MagicSkin, a novel tactile skin with translucent, tinted markers balancing the modes of marker and markerless for VBTS. It enables simultaneous tangential displacement tracking, force prediction, and surface detail preservation. This skin is easy to plug into GelSight-family sensors without requiring additional hardware or software tools. We comprehensively evaluate MagicSkin in downstream tasks. The translucent markers impressively enhance rather than degrade sensing performance compared with traditional markerless and inked marker design: it achieves best performance in object classification (99.17\\%), texture classification (93.51\\%), tangential displacement tracking (97\\% point retention) and force prediction (66\\% improvement in total force error). These experimental results demonstrate that translucent skin eliminates the traditional performance trade-off in marker or markerless modes, paving the way for multimodal tactile sensing essential in tactile robotics. See videos at this \\href{https://zhuochenn.github.io/MagicSkin_project/}{link}.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u578b\u89e6\u89c9\u76ae\u80a4MagicSkin\uff0c\u8be5\u76ae\u80a4\u91c7\u7528\u534a\u900f\u660e\u7740\u8272\u6807\u8bb0\uff0c\u6709\u6548\u5e73\u8861\u4e86\u89e6\u89c9\u4f20\u611f\u5668\u6027\u80fd\u6743\u8861\uff0c\u63d0\u5347\u4e86\u591a\u6a21\u6001\u89e6\u89c9\u611f\u77e5\u80fd\u529b\uff0c\u4e3a\u89e6\u89c9\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u4f18\u65b9\u6848\u3002", "motivation": "\u5f53\u524d\u7684\u89e6\u89c9\u4f20\u611f\u5668\u9762\u4e34\u6807\u8bb0\u4e0e\u65e0\u6807\u8bb0\u8bbe\u8ba1\u7684\u6839\u672c\u6743\u8861\uff0c\u73b0\u6709\u7684\u89e3\u51b3\u65b9\u6848\u589e\u52a0\u4e86\u786c\u4ef6\u590d\u6742\u6027\u6216\u8ba1\u7b97\u8d1f\u62c5\uff0c\u56e0\u6b64\u6025\u9700\u4e00\u79cd\u65b0\u9896\u7684\u89e3\u51b3\u65b9\u6848\u4ee5\u4f18\u5316\u4f20\u611f\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u89e6\u89c9\u76ae\u80a4MagicSkin\uff0c\u91c7\u7528\u534a\u900f\u660e\u7740\u8272\u6807\u8bb0\u4ee5\u5e73\u8861\u6807\u8bb0\u548c\u65e0\u6807\u8bb0\u7684\u8bbe\u8ba1\uff0c\u80fd\u591f\u540c\u65f6\u8fdb\u884c\u5207\u5411\u4f4d\u79fb\u8ffd\u8e2a\u3001\u529b\u9884\u6d4b\u548c\u8868\u9762\u7ec6\u8282\u4fdd\u7559\u3002", "result": "MagicSkin\u5728\u76ee\u6807\u5206\u7c7b\u3001\u7eb9\u7406\u5206\u7c7b\u3001\u5207\u5411\u4f4d\u79fb\u8ffd\u8e2a\u548c\u529b\u9884\u6d4b\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5206\u522b\u8fbe\u5230\u4e8699.17%\u300193.51%\u300197%\u548c66%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u660e\u663e\u4f18\u4e8e\u4f20\u7edf\u8bbe\u8ba1\u3002", "conclusion": "MagicSkin\u901a\u8fc7\u5f15\u5165\u534a\u900f\u660e\u7684\u7740\u8272\u6807\u8bb0\uff0c\u6d88\u9664\u4e86\u4f20\u7edf\u6807\u8bb0\u548c\u65e0\u6807\u8bb0\u6a21\u5f0f\u4e4b\u95f4\u7684\u6027\u80fd\u6743\u8861\uff0c\u63d0\u5347\u4e86\u591a\u6a21\u6001\u89e6\u89c9\u4f20\u611f\u7684\u80fd\u529b\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u89e6\u89c9\u673a\u5668\u4eba\u3002"}}
{"id": "2512.06868", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06868", "abs": "https://arxiv.org/abs/2512.06868", "authors": ["Xingguang Zhong", "Liren Jin", "Marija Popovi\u0107", "Jens Behley", "Cyrill Stachniss"], "title": "Dynamic Visual SLAM using a General 3D Prior", "comment": "8 pages", "summary": "Reliable incremental estimation of camera poses and 3D reconstruction is key to enable various applications including robotics, interactive visualization, and augmented reality. However, this task is particularly challenging in dynamic natural environments, where scene dynamics can severely deteriorate camera pose estimation accuracy. In this work, we propose a novel monocular visual SLAM system that can robustly estimate camera poses in dynamic scenes. To this end, we leverage the complementary strengths of geometric patch-based online bundle adjustment and recent feed-forward reconstruction models. Specifically, we propose a feed-forward reconstruction model to precisely filter out dynamic regions, while also utilizing its depth prediction to enhance the robustness of the patch-based visual SLAM. By aligning depth prediction with estimated patches from bundle adjustment, we robustly handle the inherent scale ambiguities of the batch-wise application of the feed-forward reconstruction model.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u5355\u76ee\u89c6\u89c9SLAM\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u52a8\u6001\u573a\u666f\u4e2d\u7a33\u5065\u5730\u4f30\u8ba1\u76f8\u673a\u59ff\u6001\u548c\u8fdb\u884c\u4e09\u7ef4\u91cd\u5efa\u3002", "motivation": "\u5728\u52a8\u6001\u81ea\u7136\u73af\u5883\u4e2d\uff0c\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u548c\u4e09\u7ef4\u91cd\u5efa\u7684\u53ef\u9760\u589e\u91cf\u4f30\u8ba1\u662f\u5173\u952e\uff0c\u4f46\u573a\u666f\u52a8\u6001\u4f1a\u4e25\u91cd\u5f71\u54cd\u51c6\u786e\u6027\u3002", "method": "\u7ed3\u5408\u51e0\u4f55\u8865\u4e01\u5728\u7ebf\u675f\u8c03\u6574\u548c\u6700\u65b0\u7684\u524d\u9988\u91cd\u5efa\u6a21\u578b\uff0c\u63d0\u51fa\u4e00\u79cd\u524d\u9988\u91cd\u5efa\u6a21\u578b\u6765\u7cbe\u786e\u8fc7\u6ee4\u52a8\u6001\u533a\uff0c\u5e76\u5229\u7528\u6df1\u5ea6\u9884\u6d4b\u589e\u5f3a\u8865\u4e01\u89c6\u89c9SLAM\u7684\u7a33\u5065\u6027\u3002", "result": "\u901a\u8fc7\u5bf9\u9f50\u6df1\u5ea6\u9884\u6d4b\u4e0e\u4ece\u675f\u8c03\u6574\u4f30\u8ba1\u7684\u8865\u4e01\uff0c\u7a33\u5065\u5904\u7406\u524d\u9988\u91cd\u5efa\u6a21\u578b\u7684\u6279\u91cf\u5e94\u7528\u6240\u56fa\u6709\u7684\u89c4\u6a21\u6a21\u7cca\u6027\u3002", "conclusion": "\u6240\u63d0\u7cfb\u7edf\u5728\u52a8\u6001\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u6280\u672f\u3001\u4ea4\u4e92\u53ef\u89c6\u5316\u548c\u589e\u5f3a\u73b0\u5b9e\u7b49\u5e94\u7528\u3002"}}
{"id": "2512.06892", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06892", "abs": "https://arxiv.org/abs/2512.06892", "authors": ["Hassan Jardali", "Durgakant Pushp", "Youwei Yu", "Mahmoud Ali", "Ihab S. Mohamed", "Alejandro Murillo-Gonzalez", "Paul D. Coen", "Md. Al-Masrur Khan", "Reddy Charan Pulivendula", "Saeoul Park", "Lingchuan Zhou", "Lantao Liu"], "title": "From Zero to High-Speed Racing: An Autonomous Racing Stack", "comment": null, "summary": "High-speed, head-to-head autonomous racing presents substantial technical and logistical challenges, including precise localization, rapid perception, dynamic planning, and real-time control-compounded by limited track access and costly hardware. This paper introduces the Autonomous Race Stack (ARS), developed by the IU Luddy Autonomous Racing team for the Indy Autonomous Challenge (IAC). We present three iterations of our ARS, each validated on different tracks and achieving speeds up to 260 km/h. Our contributions include: (i) the modular architecture and evolution of the ARS across ARS1, ARS2, and ARS3; (ii) a detailed performance evaluation that contrasts control, perception, and estimation across oval and road-course environments; and (iii) the release of a high-speed, multi-sensor dataset collected from oval and road-course tracks. Our findings highlight the unique challenges and insights from real-world high-speed full-scale autonomous racing.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u81ea\u4e3b\u8d5b\u8f66\u5806\u6808\uff08ARS\uff09\u7684\u53d1\u5c55\u53ca\u5176\u5728\u9ad8\u901f\u81ea\u4e3b\u8d5b\u8f66\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u4e09\u6b21\u8fed\u4ee3\u548c\u8be6\u7ec6\u6027\u80fd\u8bc4\u4f30\u5c55\u793a\u5176\u6311\u6218\u4e0e\u6210\u679c\u3002", "motivation": "\u65e8\u5728\u5e94\u5bf9\u9ad8\u901f\u81ea\u4e3b\u8d5b\u8f66\u4e2d\u7684\u7cbe\u786e\u5b9a\u4f4d\u3001\u5feb\u901f\u611f\u77e5\u3001\u52a8\u6001\u89c4\u5212\u548c\u5b9e\u65f6\u63a7\u5236\u7b49\u6280\u672f\u4e0e\u540e\u52e4\u6311\u6218\u3002", "method": "\u4ecb\u7ecd\u4e86\u81ea\u4e3b\u8d5b\u8f66\u5806\u6808\uff08ARS\uff09\u7684\u4e09\u6b21\u8fed\u4ee3\u53ca\u5176\u5728\u4e0d\u540c\u8d5b\u9053\u4e0a\u7684\u9a8c\u8bc1\uff0c\u7279\u522b\u662f\u5728\u9ad8\u901f\u73af\u5883\u4e0b\u7684\u63a7\u5236\u3001\u611f\u77e5\u548c\u4f30\u8ba1\u6027\u80fd\u8bc4\u4f30\u3002", "result": "ARS\u5728\u4e0d\u540c\u8d5b\u9053\u4e0a\u7ecf\u8fc7\u9a8c\u8bc1\uff0c\u6700\u9ad8\u901f\u5ea6\u53ef\u8fbe260 km/h\uff0c\u5e76\u53d1\u5e03\u4e86\u9ad8\u901f\u591a\u4f20\u611f\u5668\u6570\u636e\u96c6\u3002", "conclusion": "\u672c\u8bba\u6587\u7684\u7814\u7a76\u63ed\u793a\u4e86\u9ad8\u901f\u5168\u89c4\u6a21\u81ea\u4e3b\u8d5b\u8f66\u9762\u4e34\u7684\u72ec\u7279\u6311\u6218\u4e0e\u89c1\u89e3\u3002"}}
{"id": "2512.06896", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.06896", "abs": "https://arxiv.org/abs/2512.06896", "authors": ["Chrysostomos Karakasis", "Camryn Scully", "Robert Salati", "Panagiotis Artemiadis"], "title": "Control of Powered Ankle-Foot Prostheses on Compliant Terrain: A Quantitative Approach to Stability Enhancement", "comment": null, "summary": "Walking on compliant terrain presents a substantial challenge for individuals with lower-limb amputation, further elevating their already high risk of falling. While powered ankle-foot prostheses have demonstrated adaptability across speeds and rigid terrains, control strategies optimized for soft or compliant surfaces remain underexplored. This work experimentally validates an admittance-based control strategy that dynamically adjusts the quasi-stiffness of powered prostheses to enhance gait stability on compliant ground. Human subject experiments were conducted with three healthy individuals walking on two bilaterally compliant surfaces with ground stiffness values of 63 and 25 kN/m, representative of real-world soft environments. Controller performance was quantified using phase portraits and two walking stability metrics, offering a direct assessment of fall risk. Compared to a standard phase-variable controller developed for rigid terrain, the proposed admittance controller consistently improved gait stability across all compliant conditions. These results demonstrate the potential of adaptive, stability-aware prosthesis control to reduce fall risk in real-world environments and advance the robustness of human-prosthesis interaction in rehabilitation robotics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9a8c\u8bc1\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u57fa\u4e8e\u63a5\u7eb3\u7684\u63a7\u5236\u7b56\u7565\uff0c\u5728\u67d4\u6027\u5730\u9762\u4e0a\u663e\u8457\u63d0\u9ad8\u4e0b\u80a2\u5047\u80a2\u7684\u6b65\u6001\u7a33\u5b9a\u6027\uff0c\u51cf\u5c11\u8dcc\u5012\u98ce\u9669\u3002", "motivation": "\u964d\u4f4e\u4e0b\u80a2\u622a\u80a2\u8005\u5728\u67d4\u6027\u5730\u9762\u4e0a\u7684\u8dcc\u5012\u98ce\u9669\uff0c\u63d0\u5347\u5047\u80a2\u7684\u63a7\u5236\u7b56\u7565\u9002\u5e94\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u63a5\u7eb3\u7684\u63a7\u5236\u7b56\u7565\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5047\u80a2\u7684\u51c6\u521a\u5ea6\uff0c\u6539\u5584\u5728\u67d4\u6027\u5730\u9762\u4e0a\u7684\u884c\u8d70\u7a33\u5b9a\u6027\uff0c\u5e76\u8fdb\u884c\u4e86\u4eba\u7c7b\u5b9e\u9a8c\u3002", "result": "\u4e0e\u6807\u51c6\u7684\u76f8\u4f4d\u53d8\u91cf\u63a7\u5236\u5668\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u63a5\u7eb3\u63a7\u5236\u5668\u5728\u6240\u6709\u67d4\u6027\u6761\u4ef6\u4e0b\u5747\u663e\u8457\u63d0\u9ad8\u4e86\u6b65\u6001\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u57fa\u4e8e\u63a5\u7eb3\u7684\u63a7\u5236\u7b56\u7565\u5728\u63d0\u9ad8\u4e0b\u80a2\u5047\u80a2\u5728\u67d4\u6027\u5730\u9762\u4e0a\u7684\u6b65\u6001\u7a33\u5b9a\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u663e\u8457\u964d\u4f4e\u8dcc\u5012\u98ce\u9669\uff0c\u5e76\u63a8\u52a8\u5eb7\u590d\u673a\u5668\u4eba\u4e2d\u4eba\u673a\u4ea4\u4e92\u7684\u5065\u58ee\u6027\u3002"}}
{"id": "2512.06897", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06897", "abs": "https://arxiv.org/abs/2512.06897", "authors": ["Bradley Hobbs", "Panagiotis Artemiadis"], "title": "Ground Compliance Improves Retention of Visual Feedback-Based Propulsion Training for Gait Rehabilitation", "comment": null, "summary": "This study investigates whether adding ground compliance to visual feedback (VF) gait training is more effective at increasing push-off force (POF) compared to using VF alone, with implications for gait rehabilitation. Ten healthy participants walked on a custom split-belt treadmill. All participants received real-time visual feedback of their ground reaction forces. One group also experienced changes in ground compliance, while a control group received only visual feedback. Intentional increases in propulsive ground reaction forces (POF) were successfully achieved and sustained post-intervention, especially in the group that experienced ground compliance. This group also demonstrated lasting after-effects in muscle activity and joint kinematics, indicating a more robust learning of natural strategies to increase propulsion. This work demonstrates how visual and proprioceptive systems coordinate during gait adaptation. It uniquely shows that combining ground compliance with visual feedback enhances the learning of propulsive forces, supporting the potential use of compliant terrain in long-term rehabilitation targeting propulsion deficits, such as those following a stroke.", "AI": {"tldr": "\u672c\u7814\u7a76\u663e\u793a\uff0c\u5c06\u5730\u9762\u987a\u5e94\u6027\u4e0e\u89c6\u89c9\u53cd\u9988\u7ed3\u5408\uff0c\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u63d0\u5347\u63a8\u529b\uff0c\u4fc3\u8fdb\u6b65\u6001\u5eb7\u590d\u3002", "motivation": "\u7814\u7a76\u6dfb\u52a0\u5730\u9762\u987a\u5e94\u6027\u662f\u5426\u80fd\u66f4\u6709\u6548\u5730\u63d0\u9ad8\u63a8\u529b\uff0c\u800c\u4e0d\u4ec5\u4ec5\u4f9d\u8d56\u89c6\u89c9\u53cd\u9988\uff0c\u8fd9\u5bf9\u6b65\u6001\u5eb7\u590d\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u7814\u7a76\u5728\u5b9a\u5236\u7684\u5206\u5e26\u8dd1\u6b65\u673a\u4e0a\uff0c\u670910\u540d\u5065\u5eb7\u53c2\u4e0e\u8005\u63a5\u53d7\u5b9e\u65f6\u7684\u5730\u9762\u53cd\u4f5c\u7528\u529b\u89c6\u89c9\u53cd\u9988\u3002\u4e00\u7ec4\u53c2\u4e0e\u8005\u4f53\u9a8c\u4e86\u5730\u9762\u987a\u5e94\u6027\u7684\u53d8\u5316\uff0c\u800c\u5bf9\u7167\u7ec4\u4ec5\u83b7\u5f97\u89c6\u89c9\u53cd\u9988\u3002", "result": "\u6709\u6548\u4e14\u6301\u7eed\u5730\u63d0\u9ad8\u4e86\u63a8\u52a8\u6027\u5730\u9762\u53cd\u4f5c\u7528\u529b\uff0c\u5c24\u5176\u662f\u5728\u4f53\u9a8c\u5730\u9762\u987a\u5e94\u6027\u7684\u7ec4\u522b\u4e2d\uff0c\u8be5\u7ec4\u663e\u793a\u51fa\u808c\u8089\u6d3b\u52a8\u548c\u5173\u8282\u8fd0\u52a8\u5b66\u7684\u6301\u4e45\u540e\u6548\uff0c\u4f53\u73b0\u4e86\u66f4\u5f3a\u7684\u5b66\u4e60\u80fd\u529b\u3002", "conclusion": "\u7ed3\u5408\u5730\u9762\u987a\u5e94\u6027\u548c\u89c6\u89c9\u53cd\u9988\u589e\u5f3a\u4e86\u5bf9\u63a8\u52a8\u529b\u7684\u5b66\u4e60\uff0c\u652f\u6301\u5c06\u987a\u5e94\u6027\u5730\u5f62\u7528\u4e8e\u957f\u671f\u5eb7\u590d\uff0c\u7279\u522b\u662f\u9488\u5bf9\u4e2d\u98ce\u540e\u7684\u63a8\u52a8\u529b\u7f3a\u9677\u3002"}}
{"id": "2512.06912", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06912", "abs": "https://arxiv.org/abs/2512.06912", "authors": ["Rushiraj Gadhvi", "Sandeep Manjanna"], "title": "Energy-Efficient Navigation for Surface Vehicles in Vortical Flow Fields", "comment": "Under Review for International Conference on Robotics and Automation (ICRA 2026)", "summary": "For centuries, khalasi have skillfully harnessed ocean currents to navigate vast waters with minimal effort. Emulating this intuition in autonomous systems remains a significant challenge, particularly for Autonomous Surface Vehicles tasked with long duration missions under strict energy budgets. In this work, we present a learning-based approach for energy-efficient surface vehicle navigation in vortical flow fields, where partial observability often undermines traditional path-planning methods. We present an end to end reinforcement learning framework based on Soft Actor Critic that learns flow-aware navigation policies using only local velocity measurements. Through extensive evaluation across diverse and dynamically rich scenarios, our method demonstrates substantial energy savings and robust generalization to previously unseen flow conditions, offering a promising path toward long term autonomy in ocean environments. The navigation paths generated by our proposed approach show an improvement in energy conservation 30 to 50 percent compared to the existing state of the art techniques.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u81ea\u4e3b\u8868\u9762\u8f66\u8f86\u5728\u6da1\u6d41\u6d41\u573a\u4e2d\u8fdb\u884c\u80fd\u6548\u5bfc\u822a\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u8282\u7701\u80fd\u6e90\uff0c\u5e76\u5728\u65b0\u6761\u4ef6\u4e0b\u5177\u5907\u826f\u597d\u7684\u901a\u7528\u6027\u3002", "motivation": "\u901a\u8fc7\u6a21\u4effkhalasi\u5229\u7528\u6d77\u6d41\u5bfc\u822a\u7684\u667a\u6167\uff0c\u89e3\u51b3\u81ea\u4e3b\u8868\u9762\u8f66\u8f86\u5728\u957f\u65f6\u95f4\u4efb\u52a1\u4e2d\u7684\u80fd\u6548\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eSoft Actor Critic\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4ec5\u4f7f\u7528\u5c40\u90e8\u901f\u5ea6\u6d4b\u91cf\u5b66\u4e60\u6d41\u52a8\u611f\u77e5\u5bfc\u822a\u7b56\u7565\u3002", "result": "\u5728\u591a\u79cd\u52a8\u6001\u573a\u666f\u4e0b\u8bc4\u4f30\uff0c\u65b9\u6cd5\u76f8\u6bd4\u4e8e\u73b0\u6709\u6280\u672f\u53ef\u8282\u770130%\u81f350%\u7684\u80fd\u6e90\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u81ea\u4e3b\u6d77\u6d0b\u73af\u5883\u4e2d\u7684\u957f\u671f\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u8def\u5f84\u3002"}}
{"id": "2512.06935", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06935", "abs": "https://arxiv.org/abs/2512.06935", "authors": ["Nicol\u00f2 Botteghi", "Owen Brook", "Urban Fasel", "Federico Califano"], "title": "Interconnection and Damping Assignment Passivity-Based Control using Sparse Neural ODEs", "comment": null, "summary": "Interconnection and Damping Assignment Passivity-Based Control (IDA-PBC) is a nonlinear control technique that assigns a port-Hamiltonian (pH) structure to a controlled system using a state-feedback law. While IDA-PBC has been extensively studied and applied to many systems, its practical implementation often remains confined to academic examples and, almost exclusively, to stabilization tasks. The main limitation of IDA-PBC stems from the complexity of analytically solving a set of partial differential equations (PDEs), referred to as the matching conditions, which enforce the pH structure of the closed-loop system. However, this is extremely challenging, especially for complex physical systems and tasks.\n  In this work, we propose a novel numerical approach for designing IDA-PBC controllers without solving the matching PDEs exactly. We cast the IDA-PBC problem as the learning of a neural ordinary differential equation. In particular, we rely on sparse dictionary learning to parametrize the desired closed-loop system as a sparse linear combination of nonlinear state-dependent functions. Optimization of the controller parameters is achieved by solving a multi-objective optimization problem whose cost function is composed of a generic task-dependent cost and a matching condition-dependent cost. Our numerical results show that the proposed method enables (i) IDA-PBC to be applicable to complex tasks beyond stabilization, such as the discovery of periodic oscillatory behaviors, (ii) the derivation of closed-form expressions of the controlled system, including residual terms", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6570\u503c\u65b9\u6cd5\uff0c\u4f7f\u5f97IDA-PBC\u80fd\u591f\u514b\u670d\u504f\u5fae\u5206\u65b9\u7a0b\u6c42\u89e3\u7684\u590d\u6742\u6027\uff0c\u6269\u5c55\u81f3\u590d\u6742\u4efb\u52a1\u5e76\u63a8\u5bfc\u63a7\u5236\u7cfb\u7edf\u7684\u5c01\u95ed\u5f0f\u8868\u8fbe\u3002", "motivation": "\u89e3\u51b3IDA-PBC\u5728\u5b9e\u9645\u5b9e\u65bd\u4e2d\u53d7\u9650\u4e8e\u590d\u6742\u7684\u504f\u5fae\u5206\u65b9\u7a0b\u6c42\u89e3\u7684\u95ee\u9898\uff0c\u4ece\u800c\u6269\u5c55\u5176\u5e94\u7528\u8303\u56f4\u3002", "method": "\u5c06IDA-PBC\u95ee\u9898\u8f6c\u5316\u4e3a\u5b66\u4e60\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\uff0c\u901a\u8fc7\u7a00\u758f\u5b57\u5178\u5b66\u4e60\u5bf9\u671f\u671b\u95ed\u73af\u7cfb\u7edf\u8fdb\u884c\u53c2\u6570\u5316\uff0c\u4f7f\u7528\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\u4f18\u5316\u63a7\u5236\u5668\u53c2\u6570\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u4f7fIDA-PBC\u9002\u7528\u4e8e\u8d85\u51fa\u7a33\u5b9a\u6027\u7684\u590d\u6742\u4efb\u52a1\uff0c\u5e76\u80fd\u591f\u63a8\u5bfc\u5c01\u95ed\u5f0f\u63a7\u5236\u7cfb\u7edf\u8868\u8fbe\u5f0f\uff0c\u5305\u62ec\u6b8b\u5dee\u9879\u3002", "conclusion": "\u63d0\u51fa\u7684\u6570\u503c\u65b9\u6cd5\u4f7fIDA-PBC\u80fd\u591f\u5e94\u7528\u4e8e\u590d\u6742\u4efb\u52a1\uff0c\u5e76\u80fd\u63a8\u5bfc\u51fa\u63a7\u5236\u7cfb\u7edf\u7684\u5c01\u95ed\u5f0f\u8868\u8fbe\u3002"}}
{"id": "2512.06951", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06951", "abs": "https://arxiv.org/abs/2512.06951", "authors": ["Ilia Larchenko", "Gleb Zarin", "Akash Karnatak"], "title": "Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge", "comment": "2025 NeurIPS Behavior Challenge 1st place solution", "summary": "We present a vision-action policy that won 1st place in the 2025 BEHAVIOR Challenge - a large-scale benchmark featuring 50 diverse long-horizon household tasks in photo-realistic simulation, requiring bimanual manipulation, navigation, and context-aware decision making.\n  Building on the Pi0.5 architecture, we introduce several innovations. Our primary contribution is correlated noise for flow matching, which improves training efficiency and enables correlation-aware inpainting for smooth action sequences. We also apply learnable mixed-layer attention and System 2 stage tracking for ambiguity resolution. Training employs multi-sample flow matching to reduce variance, while inference uses action compression and challenge-specific correction rules.\n  Our approach achieves 26% q-score across all 50 tasks on both public and private leaderboards.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57282025 BEHAVIOR\u6311\u6218\u4e2d\u83b7\u80dc\u7684\u89c6\u89c9-\u52a8\u4f5c\u7b56\u7565\uff0c\u4e13\u6ce8\u4e8e\u957f\u65f6\u95f4\u5bb6\u5ead\u4efb\u52a1\u7684\u5904\u7406\u3002", "motivation": "\u89e3\u51b3\u957f\u65f6\u95f4\u5bb6\u5ead\u4efb\u52a1\u4e2d\u7684\u53cc\u624b\u64cd\u63a7\u3001\u5bfc\u822a\u548c\u4e0a\u4e0b\u6587\u610f\u8bc6\u51b3\u7b56\u95ee\u9898\u3002", "method": "\u57fa\u4e8ePi0.5\u67b6\u6784\uff0c\u91c7\u7528\u76f8\u5173\u566a\u58f0\u8fdb\u884c\u6d41\u5339\u914d\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u6df7\u5408\u5c42\u6ce8\u610f\u529b\u548c\u7cfb\u7edf2\u9636\u6bb5\u8ffd\u8e2a\u7b49\u521b\u65b0\u65b9\u6cd5\u3002", "result": "\u5728\u6240\u670950\u4e2a\u4efb\u52a1\u4e0a\uff0c\u516c\u5171\u548c\u79c1\u6709\u6392\u884c\u699c\u7684q-score\u8fbe\u523026%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u5e76\u5728\u590d\u6742\u4efb\u52a1\u5904\u7406\u4e0a\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2512.06963", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06963", "abs": "https://arxiv.org/abs/2512.06963", "authors": ["Yichao Shen", "Fangyun Wei", "Zhiying Du", "Yaobo Liang", "Yan Lu", "Jiaolong Yang", "Nanning Zheng", "Baining Guo"], "title": "VideoVLA: Video Generators Can Be Generalizable Robot Manipulators", "comment": "Project page: https://videovla-nips2025.github.io", "summary": "Generalization in robot manipulation is essential for deploying robots in open-world environments and advancing toward artificial general intelligence. While recent Vision-Language-Action (VLA) models leverage large pre-trained understanding models for perception and instruction following, their ability to generalize to novel tasks, objects, and settings remains limited. In this work, we present VideoVLA, a simple approach that explores the potential of transforming large video generation models into robotic VLA manipulators. Given a language instruction and an image, VideoVLA predicts an action sequence as well as the future visual outcomes. Built on a multi-modal Diffusion Transformer, VideoVLA jointly models video, language, and action modalities, using pre-trained video generative models for joint visual and action forecasting. Our experiments show that high-quality imagined futures correlate with reliable action predictions and task success, highlighting the importance of visual imagination in manipulation. VideoVLA demonstrates strong generalization, including imitating other embodiments' skills and handling novel objects. This dual-prediction strategy - forecasting both actions and their visual consequences - explores a paradigm shift in robot learning and unlocks generalization capabilities in manipulation systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faVideoVLA\uff0c\u4e00\u79cd\u5229\u7528\u89c6\u9891\u751f\u6210\u6a21\u578b\u6539\u8fdb\u673a\u5668\u4eba\u64cd\u63a7\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u7ed3\u679c\u663e\u793a\u5176\u5728\u65b0\u4efb\u52a1\u548c\u5bf9\u8c61\u4e0a\u7684\u826f\u597d\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u63a8\u52a8\u673a\u5668\u4eba\u5728\u5f00\u653e\u4e16\u754c\u4e2d\u7684\u64cd\u63a7\u5e94\u7528\uff0c\u5e76\u671d\u5411\u4eba\u5de5\u901a\u7528\u667a\u80fd\u53d1\u5c55\u3002", "method": "VideoVLA\u57fa\u4e8e\u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u5171\u540c\u5efa\u6a21\u89c6\u9891\u3001\u8bed\u8a00\u548c\u52a8\u4f5c\u6a21\u6001\u3002", "result": "VideoVLA \u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u5c06\u5927\u578b\u89c6\u9891\u751f\u6210\u6a21\u578b\u8f6c\u5316\u4e3a\u673a\u5668\u4ebaVLA\u64cd\u63a7\u8005\u6765\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u5f00\u653e\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "VideoVLA\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5305\u62ec\u6a21\u4eff\u5176\u4ed6\u4f53\u7684\u6280\u80fd\u548c\u5904\u7406\u65b0\u5bf9\u8c61\uff0c\u8bc1\u660e\u4e86\u89c6\u89c9\u60f3\u8c61\u5728\u64cd\u63a7\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2512.06995", "categories": ["cs.RO", "physics.class-ph"], "pdf": "https://arxiv.org/pdf/2512.06995", "abs": "https://arxiv.org/abs/2512.06995", "authors": ["Maryam Seraj", "Mohammad Hossein Kamrava", "Carlo Tiseo"], "title": "Parametric Design of a Cable-Driven Coaxial Spherical Parallel Mechanism for Ultrasound Scans", "comment": null, "summary": "Haptic interfaces play a critical role in medical teleoperation by enabling surgeons to interact with remote environments through realistic force and motion feedback. Achieving high fidelity in such systems requires balancing performance trade-off among workspace, dexterity, stiffness, inertia, and bandwidth, particularly in applications demanding pure rotational motion. This paper presents the design methodology and kinematic analysis of a Cable-Driven Coaxial Spherical Parallel Mechanism (CDC-SPM) developed to address these challenges. The proposed cable-driven interface design allows for reducing the mass placed at the robot arm end-effector, thereby minimizing inertial loads, enhancing stiffness, and improving dynamic responsiveness. Through parallel and coaxial actuation, the mechanism achieves decoupled rotational degrees of freedom with isotropic force and torque transmission. Simulation and analysis demonstrate that the CDC-SPM provides accurate, responsive, and safe motion characteristics suitable for high-precision haptic applications. These results highlight the mechanism's potential for medical teleoperation tasks such as ultrasound imaging, where precise and intuitive manipulation is essential.", "AI": {"tldr": "\u672c\u6587\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7535\u7f06\u9a71\u52a8\u7684\u540c\u8f74\u7403\u5f62\u5e76\u8054\u673a\u5236\uff08CDC-SPM\uff09\u4ee5\u63d0\u5347\u533b\u7597\u9065\u64cd\u4f5c\u4e2d\u7684\u89e6\u89c9\u53cd\u9988\uff0c\u4eff\u771f\u7ed3\u679c\u8bc1\u660e\u5176\u9002\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u5e94\u7528\u3002", "motivation": "\u5728\u533b\u7597\u9065\u64cd\u4f5c\u4e2d\uff0c\u89e6\u89c9\u63a5\u53e3\u4f7f\u5916\u79d1\u533b\u751f\u80fd\u591f\u901a\u8fc7\u903c\u771f\u7684\u529b\u548c\u8fd0\u52a8\u53cd\u9988\u4e0e\u8fdc\u7a0b\u73af\u5883\u8fdb\u884c\u4e92\u52a8\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7535\u7f06\u9a71\u52a8\u540c\u8f74\u7403\u5f62\u5e76\u8054\u673a\u5236\uff08CDC-SPM\uff09\u7684\u8bbe\u8ba1\u65b9\u6cd5\u548c\u8fd0\u52a8\u5b66\u5206\u6790\uff0c\u4ee5\u89e3\u51b3\u5728\u7eaf\u65cb\u8f6c\u8fd0\u52a8\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u6298\u8877\u95ee\u9898\u3002", "result": "CDC-SPM\u901a\u8fc7\u7535\u7f06\u9a71\u52a8\u754c\u9762\u8bbe\u8ba1\uff0c\u51cf\u5c11\u4e86\u673a\u5668\u4eba\u624b\u81c2\u672b\u7aef\u6548\u5e94\u5668\u7684\u8d28\u91cf\uff0c\u4ece\u800c\u51cf\u5c0f\u4e86\u60ef\u6027\u8f7d\u8377\uff0c\u589e\u5f3a\u4e86\u521a\u6027\uff0c\u63d0\u9ad8\u4e86\u52a8\u6001\u54cd\u5e94\u6027\u3002\u901a\u8fc7\u5e76\u8054\u548c\u540c\u8f74\u9a71\u52a8\uff0c\u8be5\u673a\u5236\u5b9e\u73b0\u4e86\u53bb\u8026\u5408\u7684\u65cb\u8f6c\u81ea\u7531\u5ea6\u548c\u5404\u5411\u540c\u6027\u7684\u529b\u548c\u626d\u77e9\u4f20\u8f93\u3002", "conclusion": "\u4eff\u771f\u548c\u5206\u6790\u8868\u660e\uff0cCDC-SPM\u5177\u6709\u51c6\u786e\u3001\u7075\u654f\u548c\u5b89\u5168\u7684\u8fd0\u52a8\u7279\u6027\uff0c\u9002\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u89e6\u89c9\u5e94\u7528\uff0c\u5c24\u5176\u5728\u8d85\u58f0\u6210\u50cf\u7b49\u533b\u7597\u9065\u64cd\u4f5c\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u663e\u8457\u6f5c\u529b\u3002"}}
{"id": "2512.07032", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07032", "abs": "https://arxiv.org/abs/2512.07032", "authors": ["Runcong Wang", "Fengyi Wang", "Gordon Cheng"], "title": "A Hetero-Associative Sequential Memory Model Utilizing Neuromorphic Signals: Validated on a Mobile Manipulator", "comment": null, "summary": "This paper presents a hetero-associative sequential memory system for mobile manipulators that learns compact, neuromorphic bindings between robot joint states and tactile observations to produce step-wise action decisions with low compute and memory cost. The method encodes joint angles via population place coding and converts skin-measured forces into spike-rate features using an Izhikevich neuron model; both signals are transformed into bipolar binary vectors and bound element-wise to create associations stored in a large-capacity sequential memory. To improve separability in binary space and inject geometry from touch, we introduce 3D rotary positional embeddings that rotate subspaces as a function of sensed force direction, enabling fuzzy retrieval through a softmax weighted recall over temporally shifted action patterns. On a Toyota Human Support Robot covered by robot skin, the hetero-associative sequential memory system realizes a pseudocompliance controller that moves the link under touch in the direction and with speed correlating to the amplitude of applied force, and it retrieves multi-joint grasp sequences by continuing tactile input. The system sets up quickly, trains from synchronized streams of states and observations, and exhibits a degree of generalization while remaining economical. Results demonstrate single-joint and full-arm behaviors executed via associative recall, and suggest extensions to imitation learning, motion planning, and multi-modal integration.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u79fb\u52a8\u64cd\u7eb5\u5668\u7684\u5f02\u7c7b\u5173\u8054\u5e8f\u5217\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u901a\u8fc7\u795e\u7ecf\u5f62\u6001\u7ed1\u5b9a\u5b66\u4e60\u673a\u5668\u4eba\u5173\u8282\u72b6\u6001\u4e0e\u89e6\u89c9\u89c2\u5bdf\u4e4b\u95f4\u7684\u7d27\u51d1\u5173\u8054\uff0c\u5b9e\u73b0\u4f4e\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u7684\u9010\u6b65\u884c\u52a8\u51b3\u7b56\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u7684\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u4ee5\u652f\u6301\u79fb\u52a8\u673a\u5668\u4eba\u5728\u4f4e\u8d44\u6e90\u6d88\u8017\u7684\u60c5\u51b5\u4e0b\uff0c\u505a\u51fa\u57fa\u4e8e\u89e6\u89c9\u8f93\u5165\u7684\u5b9e\u65f6\u884c\u52a8\u51b3\u7b56\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7fa4\u4f53\u4f4d\u7f6e\u7f16\u7801\u5bf9\u5173\u8282\u89d2\u5ea6\u8fdb\u884c\u7f16\u7801\uff0c\u5e76\u4f7f\u7528Izhikevich\u795e\u7ecf\u5143\u6a21\u578b\u5c06\u76ae\u80a4\u6d4b\u91cf\u7684\u529b\u8f6c\u6362\u4e3a\u8109\u51b2\u7387\u7279\u5f81\uff0c\u5c06\u4e24\u4e2a\u4fe1\u53f7\u8f6c\u6362\u4e3a\u4e8c\u5143\u5411\u91cf\u5e76\u8fdb\u884c\u5143\u7d20\u7ea7\u7ed1\u5b9a\uff0c\u5b58\u50a8\u5728\u5927\u5bb9\u91cf\u5e8f\u5217\u8bb0\u5fc6\u4e2d\u3002\u540c\u65f6\uff0c\u5f15\u51653D\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\uff0c\u4ee5\u5e2e\u52a9\u63d0\u9ad8\u4e8c\u5143\u7a7a\u95f4\u4e2d\u7684\u53ef\u5206\u79bb\u6027\u3002", "result": "\u7cfb\u7edf\u5b9e\u73b0\u4e86\u4f2a\u987a\u5e94\u63a7\u5236\uff0c\u80fd\u591f\u6839\u636e\u65bd\u52a0\u529b\u91cf\u7684\u65b9\u5411\u548c\u5e45\u5ea6\u79fb\u52a8\u94fe\u63a5\uff0c\u5e76\u80fd\u901a\u8fc7\u89e6\u89c9\u8f93\u5165\u68c0\u7d22\u591a\u5173\u8282\u6293\u53d6\u5e8f\u5217\uff0c\u5c55\u73b0\u4e86\u5728\u5355\u5173\u8282\u548c\u5168\u81c2\u884c\u4e3a\u4e0a\u7684\u8054\u60f3\u56de\u5fc6\u80fd\u529b\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u5feb\u901f\u5efa\u7acb\u548c\u8bad\u7ec3\uff0c\u5e76\u5728\u4fdd\u6301\u7ecf\u6d4e\u6027\u7684\u540c\u65f6\u8868\u73b0\u51fa\u4e00\u5b9a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u6697\u793a\u5176\u5728\u6a21\u4eff\u5b66\u4e60\u3001\u8fd0\u52a8\u89c4\u5212\u548c\u591a\u6a21\u6001\u96c6\u6210\u65b9\u9762\u7684\u6269\u5c55\u6f5c\u529b\u3002"}}
{"id": "2512.07041", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07041", "abs": "https://arxiv.org/abs/2512.07041", "authors": ["Hiroki Sawada", "Alexandre Pitti", "Mathias Quoy"], "title": "CERNet: Class-Embedding Predictive-Coding RNN for Unified Robot Motion, Recognition, and Confidence Estimation", "comment": null, "summary": "Robots interacting with humans must not only generate learned movements in real-time, but also infer the intent behind observed behaviors and estimate the confidence of their own inferences. This paper proposes a unified model that achieves all three capabilities within a single hierarchical predictive-coding recurrent neural network (PC-RNN) equipped with a class embedding vector, CERNet, which leverages a dynamically updated class embedding vector to unify motor generation and recognition. The model operates in two modes: generation and inference. In the generation mode, the class embedding constrains the hidden state dynamics to a class-specific subspace; in the inference mode, it is optimized online to minimize prediction error, enabling real-time recognition. Validated on a humanoid robot across 26 kinesthetically taught alphabets, our hierarchical model achieves 76% lower trajectory reproduction error than a parameter-matched single-layer baseline, maintains motion fidelity under external perturbations, and infers the demonstrated trajectory class online with 68% Top-1 and 81% Top-2 accuracy. Furthermore, internal prediction errors naturally reflect the model's confidence in its recognition. This integration of robust generation, real-time recognition, and intrinsic uncertainty estimation within a compact PC-RNN framework offers a compact and extensible approach to motor memory in physical robots, with potential applications in intent-sensitive human-robot collaboration.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684PC-RNN\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u6574\u5408\u4e86\u8fd0\u52a8\u751f\u6210\u3001\u5b9e\u65f6\u8bc6\u522b\u548c\u5185\u5728\u4e0d\u786e\u5b9a\u6027\u8bc4\u4f30\uff0c\u9002\u7528\u4e8e\u610f\u56fe\u654f\u611f\u7684\u4eba\u673a\u534f\u4f5c\u3002", "motivation": "\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u7684\u4e92\u52a8\u4e0d\u4ec5\u9700\u8981\u5b9e\u65f6\u751f\u6210\u8fd0\u52a8\uff0c\u8fd8\u9700\u8981\u63a8\u65ad\u89c2\u5bdf\u5230\u7684\u884c\u4e3a\u80cc\u540e\u7684\u610f\u56fe\uff0c\u5e76\u8bc4\u4f30\u81ea\u8eab\u63a8\u65ad\u7684\u4fe1\u5fc3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u5c42\u6b21\u9884\u6d4b\u7f16\u7801\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\uff08PC-RNN\uff09\uff0c\u540d\u4e3aCERNet\uff0c\u7ed3\u5408\u4e86\u7c7b\u5d4c\u5165\u5411\u91cf\uff0c\u5904\u7406\u8fd0\u52a8\u751f\u6210\u548c\u8bc6\u522b\u3002", "result": "\u572826\u4e2a\u4ee5\u8fd0\u52a8\u65b9\u5f0f\u6559\u6388\u7684\u5b57\u6bcd\u4e0a\uff0c\u8be5\u6a21\u578b\u7684\u8f68\u8ff9\u518d\u73b0\u8bef\u5dee\u6bd4\u5355\u5c42\u57fa\u7ebf\u4f4e76%\uff0c\u5728\u5916\u90e8\u6270\u52a8\u4e0b\u4fdd\u6301\u8fd0\u52a8\u4fdd\u771f\u5ea6\uff0c\u5728\u7ebf\u63a8\u65ad\u8f68\u8ff9\u7c7b\u522b\u7684Top-1\u548cTop-2\u51c6\u786e\u7387\u5206\u522b\u4e3a68%\u548c81%\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u7269\u7406\u673a\u5668\u4eba\u4e2d\u7684\u5e94\u7528\u5177\u6709\u6f5c\u5728\u7684\u7528\u9014\uff0c\u5c24\u5176\u662f\u5728\u610f\u56fe\u654f\u611f\u7684\u4eba\u673a\u534f\u4f5c\u65b9\u9762\u3002"}}
{"id": "2512.07091", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07091", "abs": "https://arxiv.org/abs/2512.07091", "authors": ["Tomoya Takahashi", "Yusaku Nakajima", "Cristian Camilo Beltran-Hernandez", "Yuki Kuroda", "Kazutoshi Tanaka", "Masashi Hamaya", "Kanta Ono", "Yoshitaka Ushiku"], "title": "A Flexible Funnel-Shaped Robotic Hand with an Integrated Single-Sheet Valve for Milligram-Scale Powder Handling", "comment": "9 pages, 8 figures", "summary": "Laboratory Automation (LA) has the potential to accelerate solid-state materials discovery by enabling continuous robotic operation without human intervention. While robotic systems have been developed for tasks such as powder grinding and X-ray diffraction (XRD) analysis, fully automating powder handling at the milligram scale remains a significant challenge due to the complex flow dynamics of powders and the diversity of laboratory tasks. To address this challenge, this study proposes a novel, funnel-shaped, flexible robotic hand that preserves the softness and conical sheet designs in prior work while incorporating a controllable valve at the cone apex to enable precise, incremental dispensing of milligram-scale powder quantities. The hand is integrated with an external balance through a feedback control system based on a model of powder flow and online parameter identification. Experimental evaluations with glass beads, monosodium glutamate, and titanium dioxide demonstrated that 80% of the trials achieved an error within 2 mg, and the maximum error observed was approximately 20 mg across a target range of 20 mg to 3 g. In addition, by incorporating flow prediction models commonly used for hoppers and performing online parameter identification, the system is able to adapt to variations in powder dynamics. Compared to direct PID control, the proposed model-based control significantly improved both accuracy and convergence speed. These results highlight the potential of the proposed system to enable efficient and flexible powder weighing, with scalability toward larger quantities and applicability to a broad range of laboratory automation tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u7684\u67d4\u6027\u673a\u5668\u4eba\u624b\u5b9e\u73b0\u4e86\u5bf9\u5fae\u514b\u7ea7\u7c89\u672b\u7684\u9ad8\u7cbe\u5ea6\u81ea\u52a8\u5206\u914d\uff0c\u5177\u6709\u663e\u8457\u7684\u5b9e\u9a8c\u5ba4\u81ea\u52a8\u5316\u6f5c\u529b\u3002", "motivation": "\u968f\u7740\u56fa\u6001\u6750\u6599\u53d1\u73b0\u7684\u9700\u6c42\u589e\u52a0\uff0c\u4f20\u7edf\u4eba\u5de5\u64cd\u4f5c\u5728\u5904\u7406\u5fae\u514b\u7ea7\u7c89\u672b\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u56e0\u6b64\u8feb\u5207\u9700\u8981\u5f00\u53d1\u5168\u81ea\u52a8\u7684\u7c89\u672b\u5904\u7406\u7cfb\u7edf\u3002", "method": "\u672c\u7814\u7a76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u578b\u6f0f\u6597\u5f62\u67d4\u6027\u673a\u5668\u4eba\u624b\uff0c\u5e76\u901a\u8fc7\u5916\u90e8\u5929\u5e73\u4e0e\u7c89\u672b\u6d41\u52a8\u6a21\u578b\u7ed3\u5408\u8fdb\u884c\u53cd\u9988\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u7c89\u672b\u5206\u914d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c80%\u7684\u8bd5\u9a8c\u57282 mg\u8bef\u5dee\u8303\u56f4\u5185\uff0c\u6700\u5927\u8bef\u5dee\u7ea6\u4e3a20 mg\uff0c\u8868\u660e\u7cfb\u7edf\u5728\u7c89\u672b\u8ba1\u91cf\u4e0a\u7684\u9ad8\u7cbe\u5ea6\u548c\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u67d4\u6027\u673a\u5668\u4eba\u624b\u80fd\u591f\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u7c89\u672b\u8ba1\u91cf\uff0c\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u4e0e\u9002\u5e94\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5b9e\u9a8c\u5ba4\u81ea\u52a8\u5316\u4efb\u52a1\u3002"}}
{"id": "2512.07114", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.07114", "abs": "https://arxiv.org/abs/2512.07114", "authors": ["Jue Wang", "Mingsong Jiang", "Luis A. Ramirez", "Bilige Yang", "Mujun Zhang", "Esteban Figueroa", "Wenzhong Yan", "Rebecca Kramer-Bottiglio"], "title": "Surrogate compliance modeling enables reinforcement learned locomotion gaits for soft robots", "comment": null, "summary": "Adaptive morphogenetic robots adapt their morphology and control policies to meet changing tasks and environmental conditions. Many such systems leverage soft components, which enable shape morphing but also introduce simulation and control challenges. Soft-body simulators remain limited in accuracy and computational tractability, while rigid-body simulators cannot capture soft-material dynamics. Here, we present a surrogate compliance modeling approach: rather than explicitly modeling soft-body physics, we introduce indirect variables representing soft-material deformation within a rigid-body simulator. We validate this approach using our amphibious robotic turtle, a quadruped with soft morphing limbs designed for multi-environment locomotion. By capturing deformation effects as changes in effective limb length and limb center of mass, and by applying reinforcement learning with extensive randomization of these indirect variables, we achieve reliable policy learning entirely in a rigid-body simulation. The resulting gaits transfer directly to hardware, demonstrating high-fidelity sim-to-real performance on hard, flat substrates and robust, though lower-fidelity, transfer on rheologically complex terrains. The learned closed-loop gaits exhibit unprecedented terrestrial maneuverability and achieve an order-of-magnitude reduction in cost of transport compared to open-loop baselines. Field experiments with the robot further demonstrate stable, multi-gait locomotion across diverse natural terrains, including gravel, grass, and mud.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u95f4\u63a5\u53d8\u91cf\u5728\u521a\u4f53\u4eff\u771f\u4e2d\u5efa\u6a21\u8f6f\u6750\u6599\u53d8\u5f62\u7684\u65b9\u6cd5\uff0c\u4e3a\u81ea\u9002\u5e94\u5f62\u6001\u673a\u5668\u4eba\u63d0\u4f9b\u9ad8\u6548\u7684\u4eff\u771f\u4e0e\u63a7\u5236\u7b56\u7565\u3002", "motivation": "\u81ea\u9002\u5e94\u5f62\u6001\u53d1\u751f\u673a\u5668\u4eba\u5728\u9762\u5bf9\u4e0d\u65ad\u53d8\u5316\u7684\u4efb\u52a1\u548c\u73af\u5883\u6761\u4ef6\u65f6\uff0c\u9700\u8981\u9002\u5e94\u5176\u5f62\u6001\u548c\u63a7\u5236\u7b56\u7565\uff0c\u800c\u8f6f\u7ec4\u4ef6\u7684\u5e94\u7528\u589e\u52a0\u4e86\u5f62\u6001\u53d8\u6362\u7684\u80fd\u529b\uff0c\u4f46\u4e5f\u5e26\u6765\u4e86\u4eff\u771f\u548c\u63a7\u5236\u7684\u6311\u6218\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u66ff\u4ee3\u7684\u67d4\u6027\u5efa\u6a21\u65b9\u6cd5\uff0c\u4e0d\u662f\u663e\u5f0f\u5730\u5efa\u6a21\u8f6f\u4f53\u7269\u7406\uff0c\u800c\u662f\u5f15\u5165\u95f4\u63a5\u53d8\u91cf\u6765\u8868\u793a\u8f6f\u6750\u6599\u7684\u53d8\u5f62\uff0c\u8fd9\u4e9b\u53d8\u91cf\u5728\u521a\u4f53\u4eff\u771f\u4e2d\u8fdb\u884c\u5904\u7406\u3002", "result": "\u901a\u8fc7\u6355\u6349\u53d8\u5f62\u6548\u679c\u4f5c\u4e3a\u6709\u6548\u80a2\u4f53\u957f\u5ea6\u548c\u80a2\u4f53\u8d28\u5fc3\u7684\u53d8\u5316\uff0c\u5e76\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u8fd9\u4e9b\u95f4\u63a5\u53d8\u91cf\u7684\u968f\u673a\u5316\uff0c\u5b8c\u6210\u4e86\u5728\u521a\u4f53\u4eff\u771f\u4e2d\u53ef\u9760\u7684\u7b56\u7565\u5b66\u4e60\u3002", "conclusion": "\u6240\u5f97\u5230\u7684\u8fd0\u52a8\u6a21\u5f0f\u80fd\u591f\u76f4\u63a5\u8fc1\u79fb\u5230\u786c\u4ef6\u4e0a\uff0c\u5728\u786c\u8d28\u5e73\u5766\u8868\u9762\u4e0a\u663e\u793a\u51fa\u9ad8\u4fdd\u771f\u5ea6\u7684\u4eff\u771f-\u73b0\u5b9e\u6027\u80fd\uff0c\u5e76\u5728\u590d\u6742\u6d41\u53d8\u5730\u5f62\u4e0a\u8868\u73b0\u51fa\u7a33\u5b9a\u7684\u591a\u79cd\u8fd0\u52a8\u6a21\u5f0f\uff0c\u663e\u793a\u51fa\u5353\u8d8a\u7684\u5730\u9762\u673a\u52a8\u6027\uff0c\u5e76\u5728\u8fd0\u8f93\u6210\u672c\u4e0a\u76f8\u6bd4\u5f00\u653e\u73af\u8def\u57fa\u7ebf\u51cf\u5c11\u4e86\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002"}}
{"id": "2512.07130", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07130", "abs": "https://arxiv.org/abs/2512.07130", "authors": ["Zebin Xing", "Yupeng Zheng", "Qichao Zhang", "Zhixing Ding", "Pengxuan Yang", "Songen Gu", "Zhongpu Xia", "Dongbin Zhao"], "title": "Mimir: Hierarchical Goal-Driven Diffusion with Uncertainty Propagation for End-to-End Autonomous Driving", "comment": null, "summary": "End-to-end autonomous driving has emerged as a pivotal direction in the field of autonomous systems. Recent works have demonstrated impressive performance by incorporating high-level guidance signals to steer low-level trajectory planners. However, their potential is often constrained by inaccurate high-level guidance and the computational overhead of complex guidance modules. To address these limitations, we propose Mimir, a novel hierarchical dual-system framework capable of generating robust trajectories relying on goal points with uncertainty estimation: (1) Unlike previous approaches that deterministically model, we estimate goal point uncertainty with a Laplace distribution to enhance robustness; (2) To overcome the slow inference speed of the guidance system, we introduce a multi-rate guidance mechanism that predicts extended goal points in advance. Validated on challenging Navhard and Navtest benchmarks, Mimir surpasses previous state-of-the-art methods with a 20% improvement in the driving score EPDMS, while achieving 1.6 times improvement in high-level module inference speed without compromising accuracy. The code and models will be released soon to promote reproducibility and further development. The code is available at https://github.com/ZebinX/Mimir-Uncertainty-Driving", "AI": {"tldr": "\u63d0\u51faMimir\u6846\u67b6\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u591a\u901f\u7387\u5f15\u5bfc\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u8f68\u8ff9\u751f\u6210\u7684\u7a33\u5065\u6027\u548c\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u9762\u4e34\u9ad8\u5c42\u6307\u5bfc\u4fe1\u53f7\u4e0d\u51c6\u786e\u548c\u590d\u6742\u6307\u5bfc\u6a21\u5757\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u95ee\u9898\uff0c\u6709\u5fc5\u8981\u5f00\u53d1\u7a33\u5065\u4e14\u9ad8\u6548\u7684\u8f68\u8ff9\u751f\u6210\u65b9\u6cd5\u3002", "method": "Mimir \u5229\u7528\u62c9\u666e\u62c9\u65af\u5206\u5e03\u4f30\u8ba1\u76ee\u6807\u70b9\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5f15\u5165\u591a\u901f\u7387\u6307\u5bfc\u673a\u5236\u4ee5\u63d0\u9ad8\u63a8\u65ad\u901f\u5ea6\u3002", "result": "Mimir \u662f\u4e00\u4e2a\u521b\u65b0\u7684\u5206\u5c42\u53cc\u7cfb\u7edf\u6846\u67b6\uff0c\u80fd\u591f\u751f\u6210\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u7a33\u5065\u8f68\u8ff9\uff0c\u4ee5\u5e94\u5bf9\u81ea\u52a8\u9a7e\u9a76\u4e2d\u9ad8\u5c42\u6307\u5bfc\u4fe1\u53f7\u4e0d\u51c6\u786e\u548c\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u6311\u6218\u3002", "conclusion": "Mimir \u5728 Navhard \u548c Navtest \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86 20%\u7684\u9a7e\u9a76\u5206\u6570\u63d0\u5347\u548c 1.6 \u500d\u7684\u9ad8\u5c42\u6a21\u5757\u63a8\u65ad\u901f\u5ea6\u63d0\u5347\u3002"}}
{"id": "2512.07137", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.07137", "abs": "https://arxiv.org/abs/2512.07137", "authors": ["Kang Yijie", "Hao Yuqing", "Wang Qingyun", "Chen Guanrong"], "title": "Time-Varying Formation Tracking Control of Wheeled Mobile Robots With Region Constraint: A Generalized Udwadia-Kalaba Framework", "comment": "10 pages,9 figures", "summary": "In this paper, the time-varying formation tracking control of wheeled mobile robots with region constraint is investigated from a generalized Udwadia-Kalaba framework. The communication topology is directed, weighted and has a spanning tree with the leader being the root. By reformulating the time-varying formation tracking control objective as a constrained equation and transforming the region constraint by a diffeomorphism, the time-varying formation tracking controller with the region constraint is designed under the generalized Udwadia-Kalaba framework. Compared with the existing works on time-varying formation tracking control, the region constraint is takeninto account in this paper, which ensures the safety of the robots.Finally, some numerical simulations are presented to illustrate the effectiveness of the proposed control strategy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e7f\u4e49Udwadia-Kalaba\u6846\u67b6\u7684\u65f6\u95f4\u53d8 formation \u8ddf\u8e2a\u63a7\u5236\u7b56\u7565\uff0c\u8003\u8651\u4e86\u533a\u57df\u7ea6\u675f\u4ee5\u786e\u4fdd\u673a\u5668\u4eba\u5b89\u5168\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5e26\u6709\u533a\u57df\u7ea6\u675f\u7684\u8f6e\u5f0f\u79fb\u52a8\u673a\u5668\u4eba\u5728\u65f6\u95f4\u53d8 formation \u8ddf\u8e2a\u63a7\u5236\u65b9\u9762\u7684\u95ee\u9898\uff0c\u4ee5\u786e\u4fdd\u673a\u5668\u4eba\u7684\u5b89\u5168\u6027\u3002", "method": "\u57fa\u4e8e\u5e7f\u4e49\u7684Udwadia-Kalaba\u6846\u67b6\u8bbe\u8ba1\u4e86\u8003\u8651\u533a\u57df\u7ea6\u675f\u7684\u65f6\u95f4\u53d8 formation \u8ddf\u8e2a\u63a7\u5236\u5668\u3002", "result": "\u901a\u8fc7\u91cd\u65b0\u8868\u8ff0\u63a7\u5236\u76ee\u6807\u5e76\u4f7f\u7528\u5fae\u5206\u6620\u5c04\u8f6c\u6362\u533a\u57df\u7ea6\u675f\uff0c\u5b9e\u73b0\u4e86\u5e26\u533a\u57df\u7ea6\u675f\u7684\u65f6\u95f4\u53d8 formation \u8ddf\u8e2a\u63a7\u5236\u3002", "conclusion": "\u63d0\u51fa\u7684\u63a7\u5236\u7b56\u7565\u6709\u6548\u6027\u5f97\u5230\u4e86\u6570\u503c\u4eff\u771f\u7684\u9a8c\u8bc1\u3002"}}
{"id": "2512.07177", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07177", "abs": "https://arxiv.org/abs/2512.07177", "authors": ["Fanjun Bu", "Melina Tsai", "Audrey Tjokro", "Tapomayukh Bhattacharjee", "Jorge Ortiz", "Wendy Ju"], "title": "Using Vision-Language Models as Proxies for Social Intelligence in Human-Robot Interaction", "comment": null, "summary": "Robots operating in everyday environments must often decide when and whether to engage with people, yet such decisions often hinge on subtle nonverbal cues that unfold over time and are difficult to model explicitly. Drawing on a five-day Wizard-of-Oz deployment of a mobile service robot in a university cafe, we analyze how people signal interaction readiness through nonverbal behaviors and how expert wizards use these cues to guide engagement. Motivated by these observations, we propose a two-stage pipeline in which lightweight perceptual detectors (gaze shifts and proxemics) are used to selectively trigger heavier video-based vision-language model (VLM) queries at socially meaningful moments. We evaluate this pipeline on replayed field interactions and compare two prompting strategies. Our findings suggest that selectively using VLMs as proxies for social reasoning enables socially responsive robot behavior, allowing robots to act appropriately by attending to the cues people naturally provide in real-world interactions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u7684\u7ba1\u9053\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u611f\u77e5\u68c0\u6d4b\u5668\u5728\u793e\u4ea4\u610f\u4e49\u91cd\u5927\u7684\u65f6\u523b\u89e6\u53d1\u66f4\u590d\u6742\u7684\u89c6\u9891-\u8bed\u8a00\u6a21\u578b\u67e5\u8be2\uff0c\u4ece\u800c\u4f7f\u673a\u5668\u4eba\u80fd\u66f4\u597d\u5730\u7406\u89e3\u548c\u54cd\u5e94\u4eba\u7c7b\u975e\u8bed\u8a00\u4fe1\u53f7\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u975e\u8bed\u8a00\u884c\u4e3a\u4fe1\u53f7\u5206\u6790\u4eba\u4e0e\u673a\u5668\u4eba\u7684\u4e92\u52a8\uff0c\u4ee5\u6539\u8fdb\u673a\u5668\u4eba\u7684\u793e\u4ea4\u54cd\u5e94\u80fd\u529b\u3002", "method": "\u5728\u5927\u5b66\u5496\u5561\u9986\u8fdb\u884c\u4e94\u5929\u7684Wizard-of-Oz\u5b9e\u9a8c\uff0c\u901a\u8fc7\u5206\u6790\u4eba\u7c7b\u7684\u975e\u8bed\u8a00\u884c\u4e3a\u4fe1\u53f7\uff0c\u63d0\u51fa\u4e00\u5957\u4e24\u9636\u6bb5\u7684\u7ba1\u9053\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u611f\u77e5\u68c0\u6d4b\u5668\u548c\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u9009\u62e9\u6027\u5730\u4f7f\u7528\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u793e\u4f1a\u63a8\u7406\u7684\u4ee3\u7406\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u9002\u5f53\u5730\u54cd\u5e94\u4eba\u7c7b\u5728\u73b0\u5b9e\u4e16\u754c\u4e92\u52a8\u4e2d\u81ea\u7136\u63d0\u4f9b\u7684\u7ebf\u7d22\u3002", "conclusion": "\u901a\u8fc7\u6709\u9009\u62e9\u6027\u5730\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u673a\u5668\u4eba\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u548c\u54cd\u5e94\u4eba\u7c7b\u7684\u975e\u8bed\u8a00\u4ea4\u4e92\u4fe1\u53f7\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u52a0\u793e\u4ea4\u5316\u7684\u884c\u4e3a\u3002"}}
{"id": "2512.07221", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07221", "abs": "https://arxiv.org/abs/2512.07221", "authors": ["Zichao Shu", "Shitao Bei", "Lijun Li", "Zetao Chen"], "title": "Spatiotemporal Calibration and Ground Truth Estimation for High-Precision SLAM Benchmarking in Extended Reality", "comment": null, "summary": "Simultaneous localization and mapping (SLAM) plays a fundamental role in extended reality (XR) applications. As the standards for immersion in XR continue to increase, the demands for SLAM benchmarking have become more stringent. Trajectory accuracy is the key metric, and marker-based optical motion capture (MoCap) systems are widely used to generate ground truth (GT) because of their drift-free and relatively accurate measurements. However, the precision of MoCap-based GT is limited by two factors: the spatiotemporal calibration with the device under test (DUT) and the inherent jitter in the MoCap measurements. These limitations hinder accurate SLAM benchmarking, particularly for key metrics like rotation error and inter-frame jitter, which are critical for immersive XR experiences. This paper presents a novel continuous-time maximum likelihood estimator to address these challenges. The proposed method integrates auxiliary inertial measurement unit (IMU) data to compensate for MoCap jitter. Additionally, a variable time synchronization method and a pose residual based on screw congruence constraints are proposed, enabling precise spatiotemporal calibration across multiple sensors and the DUT. Experimental results demonstrate that our approach outperforms existing methods, achieving the precision necessary for comprehensive benchmarking of state-of-the-art SLAM algorithms in XR applications. Furthermore, we thoroughly validate the practicality of our method by benchmarking several leading XR devices and open-source SLAM algorithms. The code is publicly available at https://github.com/ylab-xrpg/xr-hpgt.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3MoCap\u5728SLAM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u5c40\u9650\uff0c\u901a\u8fc7\u6574\u5408IMU\u6570\u636e\u548c\u5148\u8fdb\u7684\u6821\u51c6\u6280\u672f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86SLAM\u7684\u8f68\u8ff9\u7cbe\u5ea6\u3002", "motivation": "\u968f\u7740XR\u5e94\u7528\u7684\u6c89\u6d78\u6807\u51c6\u4e0d\u65ad\u63d0\u5347\uff0c\u5bf9SLAM\u57fa\u51c6\u6d4b\u8bd5\u7684\u8981\u6c42\u53d8\u5f97\u66f4\u52a0\u4e25\u683c\uff0c\u5c24\u5176\u662f\u5728\u8f68\u8ff9\u7cbe\u5ea6\u7b49\u5173\u952e\u6307\u6807\u4e0a\uff0c\u5f53\u524dMoCap\u7cfb\u7edf\u5b58\u5728\u7684\u4e00\u4e9b\u9650\u5236\u4e9f\u9700\u89e3\u51b3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8fde\u7eed\u65f6\u95f4\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5668\uff0c\u7ed3\u5408\u8f85\u52a9\u7684\u60ef\u6027\u6d4b\u91cf\u5355\u5143\uff08IMU\uff09\u6570\u636e\u4ee5\u8865\u507fMoCap\u6296\u52a8\uff0c\u5e76\u63d0\u51fa\u4e86\u53ef\u53d8\u65f6\u95f4\u540c\u6b65\u65b9\u6cd5\u548c\u57fa\u4e8e\u87ba\u65cb\u4e00\u81f4\u6027\u7ea6\u675f\u7684\u59ff\u6001\u6b8b\u5dee\uff0c\u4ee5\u5b9e\u73b0\u591a\u4f20\u611f\u5668\u4e0e\u88ab\u6d4b\u8bbe\u5907\u4e4b\u95f4\u7684\u7cbe\u786e\u65f6\u7a7a\u6821\u51c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728SLAM\u7b97\u6cd5\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8fbe\u5230\u4e86\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\u6240\u9700\u7684\u7cbe\u786e\u5ea6\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u7cbe\u786e\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u5168\u9762\u8bc4\u4f30\u6700\u5148\u8fdb\u7684SLAM\u7b97\u6cd5\u5728XR\u5e94\u7528\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u901a\u8fc7\u5bf9\u591a\u4e2a\u9886\u5148XR\u8bbe\u5907\u548c\u5f00\u6e90SLAM\u7b97\u6cd5\u7684\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\u3002"}}
{"id": "2512.07266", "categories": ["cs.RO", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.07266", "abs": "https://arxiv.org/abs/2512.07266", "authors": ["Florian Tretter", "Daniel Fl\u00f6gel", "Alexandru Vasilache", "Max Grobbel", "J\u00fcrgen Becker", "S\u00f6ren Hohmann"], "title": "SINRL: Socially Integrated Navigation with Reinforcement Learning using Spiking Neural Networks", "comment": "8 pages, 6 figures", "summary": "Integrating autonomous mobile robots into human environments requires human-like decision-making and energy-efficient, event-based computation. Despite progress, neuromorphic methods are rarely applied to Deep Reinforcement Learning (DRL) navigation approaches due to unstable training. We address this gap with a hybrid socially integrated DRL actor-critic approach that combines Spiking Neural Networks (SNNs) in the actor with Artificial Neural Networks (ANNs) in the critic and a neuromorphic feature extractor to capture temporal crowd dynamics and human-robot interactions. Our approach enhances social navigation performance and reduces estimated energy consumption by approximately 1.69 orders of magnitude.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u4e0e\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u7ed3\u5408\u7684\u6df7\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u5728\u52a8\u6001\u4eba\u7c7b\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u8868\u73b0\u3002", "motivation": "\u5f00\u53d1\u80fd\u5728\u52a8\u6001\u4eba\u7c7b\u73af\u5883\u4e2d\u6709\u6548\u8fd0\u4f5c\u7684\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\uff0c\u9700\u5b9e\u73b0\u7c7b\u4eba\u51b3\u7b56\u548c\u8282\u80fd\u7684\u8ba1\u7b97\u65b9\u5f0f\u3002", "method": "\u4f7f\u7528\u4e00\u79cd\u6df7\u5408\u7684\u793e\u4f1a\u6574\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6f14\u5458-\u8bc4\u8bba\u5bb6\u65b9\u6cd5\uff0c\u5c06\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5e94\u7528\u4e8e\u6f14\u5458\u90e8\u5206\uff0c\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u5e94\u7528\u4e8e\u8bc4\u8bba\u5bb6\u90e8\u5206\uff0c\u5e76\u7ed3\u5408\u795e\u7ecf\u5f62\u6001\u7279\u5f81\u63d0\u53d6\u5668\u6355\u6349\u4eba\u7fa4\u52a8\u6001\u4e0e\u4eba\u673a\u4ea4\u4e92\u3002", "result": "\u793e\u4ea4\u5bfc\u822a\u6027\u80fd\u63d0\u5347\uff0c\u4f30\u8ba1\u80fd\u8017\u51cf\u5c11\u7ea61.69\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u5c55\u793a\u4e86\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7ed3\u5408\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.07303", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07303", "abs": "https://arxiv.org/abs/2512.07303", "authors": ["Gianpietro Battocletti", "Dimitris Boskos", "Bart De Schutter"], "title": "Efficient Computation of a Continuous Topological Model of the Configuration Space of Tethered Mobile Robots", "comment": "7 pages, 3 figures, submitted to IFAC World Congress 2026", "summary": "Despite the attention that the problem of path planning for tethered robots has garnered in the past few decades, the approaches proposed to solve it typically rely on a discrete representation of the configuration space and do not exploit a model that can simultaneously capture the topological information of the tether and the continuous location of the robot. In this work, we explicitly build a topological model of the configuration space of a tethered robot starting from a polygonal representation of the workspace where the robot moves. To do so, we first establish a link between the configuration space of the tethered robot and the universal covering space of the workspace, and then we exploit this link to develop an algorithm to compute a simplicial complex model of the configuration space. We show how this approach improves the performances of existing algorithms that build other types of representations of the configuration space. The proposed model can be computed in a fraction of the time required to build traditional homotopy-augmented graphs, and is continuous, allowing to solve the path planning task for tethered robots using a broad set of path planning algorithms.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u7684\u62d3\u6251\u6a21\u578b\u7528\u4e8e\u5e26\u7ebf\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u53ca\u6027\u80fd\uff0c\u652f\u6301\u591a\u79cd\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u8fc7\u53bb\u51e0\u5341\u5e74\u91cc\uff0c\u5e26\u7ebf\u673a\u5668\u4eba\u7684\u8def\u5f84\u89c4\u5212\u95ee\u9898\u53d7\u5230\u4e86\u5e7f\u6cdb\u5173\u6ce8\uff0c\u4f46\u73b0\u6709\u7684\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u4f9d\u8d56\u4e8e\u79bb\u6563\u7684\u914d\u7f6e\u7a7a\u95f4\u8868\u793a\uff0c\u65e0\u6cd5\u540c\u65f6\u6355\u6349\u5230\u7ebf\u7f06\u7684\u62d3\u6251\u4fe1\u606f\u548c\u673a\u5668\u4eba\u7684\u8fde\u7eed\u4f4d\u7f6e\u3002", "method": "\u901a\u8fc7\u5efa\u7acb\u5e26\u7ebf\u673a\u5668\u4eba\u914d\u7f6e\u7a7a\u95f4\u4e0e\u5de5\u4f5c\u7a7a\u95f4\u7684\u5168\u8986\u76d6\u7a7a\u95f4\u4e4b\u95f4\u7684\u8054\u7ed3\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u7b97\u6cd5\u6765\u8ba1\u7b97\u914d\u7f6e\u7a7a\u95f4\u7684\u5355\u7eaf\u590d\u5f62\u6a21\u578b\u3002", "result": "\u63d0\u51fa\u7684\u62d3\u6251\u6a21\u578b\u53ef\u4ee5\u5728\u4f20\u7edf\u540c\u4f26\u589e\u5f3a\u56fe\u6240\u9700\u65f6\u95f4\u7684\u6781\u5c0f\u4e00\u90e8\u5206\u5185\u88ab\u8ba1\u7b97\u51fa\u6765\uff0c\u4e14\u6a21\u578b\u672c\u8eab\u662f\u8fde\u7eed\u7684\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u62d3\u6251\u6a21\u578b\u5728\u6c42\u89e3\u5e26\u7ebf\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u95ee\u9898\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u73b0\u6709\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u80fd\u5feb\u901f\u8ba1\u7b97\uff0c\u5728\u89e3\u51b3\u8def\u5f84\u89c4\u5212\u4efb\u52a1\u65f6\u652f\u6301\u591a\u79cd\u7b97\u6cd5\u7684\u4f7f\u7528\u3002"}}
{"id": "2512.07316", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.07316", "abs": "https://arxiv.org/abs/2512.07316", "authors": ["Gianpietro Battocletti", "Dimitris Boskos", "Bart De Schutter"], "title": "Model Predictive Control for Cooperative Docking Between Autonomous Surface Vehicles with Disturbance Rejection", "comment": "7 pages, 4 figures, submitted to IFAC World Congress 2026", "summary": "Uncrewed Surface Vehicles (USVs) are a popular and efficient type of marine craft that find application in a large number of water-based tasks. When multiple USVs operate in the same area, they may be required to dock to each other to perform a shared task. Existing approaches for the docking between autonomous USVs generally consider one USV as a stationary target, while the second one is tasked to reach the required docking pose. In this work, we propose a cooperative approach for USV-USV docking, where two USVs work together to dock at an agreed location. We use a centralized Model Predictive Control (MPC) approach to solve the control problem, obtaining feasible trajectories that also guarantee constraint satisfaction. Owing to its model-based nature, this approach allows the rejection of disturbances, inclusive of exogenous inputs, by anticipating their effect on the USVs through the MPC prediction model. This is particularly effective in case of almost-stationary disturbances such as water currents. In simulations, we demonstrate how the proposed approach allows for a faster and more efficient docking with respect to existing approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5408\u4f5c\u6027\u81ea\u822a\u6c34\u9762\u8f66\u8f86\uff08USV\uff09\u505c\u9760\u65b9\u6cd5\uff0c\u4f7f\u7528\u96c6\u4e2d\u5f0f\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u89e3\u51b3\u63a7\u5236\u95ee\u9898\uff0c\u663e\u793a\u51fa\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u6548\u7387\u548c\u6297\u5e72\u6270\u80fd\u529b\u3002", "motivation": "\u5728\u591a\u4e2a\u65e0\u8239\u5458\u6c34\u9762\u8f66\u8f86\u5728\u540c\u4e00\u533a\u57df\u64cd\u4f5c\u65f6\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u534f\u540c\u505c\u9760\u3002", "method": "\u96c6\u4e2d\u5f0f\u6a21\u578b\u9884\u6d4b\u63a7\u5236 (MPC) \u65b9\u6cd5", "result": "\u901a\u8fc7\u8fd9\u4e2a\u65b9\u6cd5\u5f97\u5230\u4e86\u53ef\u884c\u7684\u8f68\u8ff9\uff0c\u80fd\u591f\u4fdd\u8bc1\u7ea6\u675f\u6ee1\u8db3\uff0c\u5e76\u80fd\u62b5\u6297\u5e72\u6270", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4eff\u771f\u4e2d\u663e\u793a\u51fa\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u5feb\u548c\u66f4\u9ad8\u6548\u7684\u505c\u9760\u6548\u679c"}}
{"id": "2512.07359", "categories": ["cs.RO", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.07359", "abs": "https://arxiv.org/abs/2512.07359", "authors": ["Bin Zhao", "Yiwen Lu", "Haohua Zhu", "Xiao Li", "Sheng Yi"], "title": "Multi-Rigid-Body Approximation of Human Hands with Application to Digital Twin", "comment": "10 pages, 4 figures. Accepted at ICBSR'25 (International Conference on Biomechanical Systems and Robotics)", "summary": "Human hand simulation plays a critical role in digital twin applications, requiring models that balance anatomical fidelity with computational efficiency. We present a complete pipeline for constructing multi-rigid-body approximations of human hands that preserve realistic appearance while enabling real-time physics simulation. Starting from optical motion capture of a specific human hand, we construct a personalized MANO (Multi-Abstracted hand model with Neural Operations) model and convert it to a URDF (Unified Robot Description Format) representation with anatomically consistent joint axes. The key technical challenge is projecting MANO's unconstrained SO(3) joint rotations onto the kinematically constrained joints of the rigid-body model. We derive closed-form solutions for single degree-of-freedom joints and introduce a Baker-Campbell-Hausdorff (BCH)-corrected iterative method for two degree-of-freedom joints that properly handles the non-commutativity of rotations. We validate our approach through digital twin experiments where reinforcement learning policies control the multi-rigid-body hand to replay captured human demonstrations. Quantitative evaluation shows sub-centimeter reconstruction error and successful grasp execution across diverse manipulation tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4eba\u624b\u6a21\u62df\u65b9\u6cd5\uff0c\u65e8\u5728\u5e73\u8861\u89e3\u5256\u5b66\u903c\u771f\u6027\u4e0e\u8ba1\u7b97\u6548\u7387\uff0c\u6784\u5efa\u53ef\u5b9e\u65f6\u7269\u7406\u6a21\u62df\u7684\u591a\u521a\u4f53\u4eba\u624b\u6a21\u578b\u3002", "motivation": "\u4eba\u624b\u6a21\u62df\u5728\u6570\u5b57\u53cc\u80de\u80ce\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u9ad8\u6548\u4e14\u771f\u5b9e\u7684\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u5149\u5b66\u52a8\u4f5c\u6355\u6349\u6784\u5efa\u4e2a\u6027\u5316\u7684MANO\u6a21\u578b\uff0c\u5e76\u5c06\u5176\u8f6c\u6362\u4e3a\u5177\u6709\u89e3\u5256\u4e00\u81f4\u5173\u8282\u8f74\u7684URDF\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u521a\u4f53\u6a21\u578b\u5173\u8282\u7684\u65cb\u8f6c\u7ea6\u675f\u95ee\u9898\u3002", "result": "\u5728\u6570\u5b57\u53cc\u80de\u80ce\u5b9e\u9a8c\u4e2d\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u624b\u90e8\u91cd\u653e\u4eba\u7c7b\u6f14\u793a\uff0c\u5b9e\u73b0\u4e9a\u5398\u7c73\u7ea7\u91cd\u5efa\u8bef\u5dee\u548c\u6210\u529f\u7684\u6293\u53d6\u6267\u884c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5730\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u903c\u771f\u7684\u4eba\u624b\u6a21\u62df\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u6a21\u578b\u5728\u5b9e\u65f6\u7269\u7406\u6a21\u62df\u4e2d\u7684\u9650\u5236\u3002"}}
{"id": "2512.07371", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07371", "abs": "https://arxiv.org/abs/2512.07371", "authors": ["Byungju Kim", "Jinu Pahk", "Chungwoo Lee", "Jaejoon Kim", "Jangha Lee", "Theo Taeyeong Kim", "Kyuhwan Shim", "Jun Ki Lee", "Byoung-Tak Zhang"], "title": "ESPADA: Execution Speedup via Semantics Aware Demonstration Data Downsampling for Imitation Learning", "comment": "project page: https://project-espada.github.io/espada/", "summary": "Behavior-cloning based visuomotor policies enable precise manipulation but often inherit the slow, cautious tempo of human demonstrations, limiting practical deployment. However, prior studies on acceleration methods mainly rely on statistical or heuristic cues that ignore task semantics and can fail across diverse manipulation settings. We present ESPADA, a semantic and spatially aware framework that segments demonstrations using a VLM-LLM pipeline with 3D gripper-object relations, enabling aggressive downsampling only in non-critical segments while preserving precision-critical phases, without requiring extra data or architectural modifications, or any form of retraining. To scale from a single annotated episode to the full dataset, ESPADA propagates segment labels via Dynamic Time Warping (DTW) on dynamics-only features. Across both simulation and real-world experiments with ACT and DP baselines, ESPADA achieves approximately a 2x speed-up while maintaining success rates, narrowing the gap between human demonstrations and efficient robot control.", "AI": {"tldr": "ESPADA\u662f\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u548c\u7a7a\u95f4\u611f\u77e5\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u793a\u8303\u8fdb\u884c\u667a\u80fd\u5206\u6bb5\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u63a7\u5236\u901f\u5ea6\u7684\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u64cd\u63a7\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u884c\u4e3a\u514b\u9686\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5728\u6267\u884c\u64cd\u63a7\u65f6\u5f80\u5f80\u56e0\u7ee7\u627f\u4eba\u7c7b\u6f14\u793a\u7684\u7f13\u6162\u548c\u8c28\u614e\u7684\u8282\u594f\u800c\u53d7\u5230\u9650\u5236\uff0c\u8feb\u5207\u9700\u8981\u63d0\u5347\u673a\u5668\u4eba\u63a7\u5236\u7684\u6548\u7387\u3002", "method": "ESPADA\u4f7f\u7528VLM-LLM\u7ba1\u9053\u5bf9\u6f14\u793a\u8fdb\u884c\u5206\u6bb5\uff0c\u901a\u8fc7\u52a8\u6001\u65f6\u95f4\u89c4\u6574(DTW)\u5728\u4ec5\u5305\u542b\u52a8\u6001\u7279\u5f81\u7684\u60c5\u51b5\u4e0b\u4f20\u64ad\u6bb5\u6807\u7b7e\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0cESPADA\u5728\u4e0eACT\u548cDP\u57fa\u7ebf\u5bf9\u6bd4\u4e2d\uff0c\u5b9e\u73b0\u4e86\u7ea62\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u800c\u6210\u529f\u7387\u5f97\u4ee5\u4fdd\u6301\u3002", "conclusion": "ESPADA\u65b9\u6cd5\u5728\u4fdd\u6301\u6210\u529f\u7387\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u7ea62\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u7f29\u5c0f\u4e86\u4eba\u7c7b\u793a\u8303\u4e0e\u9ad8\u6548\u673a\u5668\u4eba\u63a7\u5236\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2512.07464", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07464", "abs": "https://arxiv.org/abs/2512.07464", "authors": ["Haolin Song", "Hongbo Zhu", "Tao Yu", "Yan Liu", "Mingqi Yuan", "Wengang Zhou", "Hua Chen", "Houqiang Li"], "title": "Gait-Adaptive Perceptive Humanoid Locomotion with Real-Time Under-Base Terrain Reconstruction", "comment": null, "summary": "For full-size humanoid robots, even with recent advances in reinforcement learning-based control, achieving reliable locomotion on complex terrains, such as long staircases, remains challenging. In such settings, limited perception, ambiguous terrain cues, and insufficient adaptation of gait timing can cause even a single misplaced or mistimed step to result in rapid loss of balance. We introduce a perceptive locomotion framework that merges terrain sensing, gait regulation, and whole-body control into a single reinforcement learning policy. A downward-facing depth camera mounted under the base observes the support region around the feet, and a compact U-Net reconstructs a dense egocentric height map from each frame in real time, operating at the same frequency as the control loop. The perceptual height map, together with proprioceptive observations, is processed by a unified policy that produces joint commands and a global stepping-phase signal, allowing gait timing and whole-body posture to be adapted jointly to the commanded motion and local terrain geometry. We further adopt a single-stage successive teacher-student training scheme for efficient policy learning and knowledge transfer. Experiments conducted on a 31-DoF, 1.65 m humanoid robot demonstrate robust locomotion in both simulation and real-world settings, including forward and backward stair ascent and descent, as well as crossing a 46 cm gap. Project Page:https://ga-phl.github.io/", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7c7b\u4eba\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e0a\u884c\u8d70\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u611f\u77e5\u3001\u6b65\u6001\u548c\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u8fd0\u52a8\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u5168\u5c3a\u5bf8\u7c7b\u4eba\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e0a\uff08\u5982\u957f\u697c\u68af\uff09\u5b9e\u73b0\u53ef\u9760\u8fd0\u52a8\u7684\u6311\u6218\uff0c\u514b\u670d\u6709\u9650\u611f\u77e5\u548c\u6b65\u6001\u65f6\u673a\u4e0d\u9002\u5e94\u7684\u95ee\u9898\u3002", "method": "\u878d\u5408\u4e86\u5730\u5f62\u611f\u77e5\u3001\u6b65\u6001\u8c03\u8282\u548c\u6574\u4f53\u63a7\u5236\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5229\u7528\u5b89\u88c5\u5728\u673a\u5668\u4eba\u5e95\u90e8\u7684\u6df1\u5ea6\u76f8\u673a\u83b7\u53d6\u5730\u9762\u652f\u6301\u533a\u57df\u7684\u5b9e\u65f6\u4fe1\u606f\uff0c\u4f7f\u7528\u7d27\u51d1\u578bU-Net\u6784\u5efa\u81ea\u6211\u4e2d\u5fc3\u7684\u9ad8\u5ea6\u56fe\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u5747\u80fd\u5b9e\u73b0\u7a33\u5b9a\u7684\u8fd0\u52a8\uff0c\u6311\u6218\u5305\u62ec\u5411\u524d\u548c\u5411\u540e\u697c\u68af\u7684\u4e0a\u5347\u548c\u4e0b\u964d\uff0c\u4ee5\u53ca\u8de8\u8d8a46\u5398\u7c73\u7684\u95f4\u9699\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u6846\u67b6\u5728\u590d\u6742\u5730\u5f62\u4e0a\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u673a\u5668\u4eba\u884c\u8d70\u80fd\u529b\uff0c\u5305\u62ec\u697c\u68af\u7684\u4e0a\u4e0b\u548c\u6a2a\u8de8\u95f4\u9699\uff0c\u8868\u73b0\u51fa\u7a33\u5b9a\u7684\u8fd0\u52a8\u6027\u80fd\u3002"}}
{"id": "2512.07472", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07472", "abs": "https://arxiv.org/abs/2512.07472", "authors": ["Siyu Xu", "Zijian Wang", "Yunke Wang", "Chenghao Xia", "Tao Huang", "Chang Xu"], "title": "Affordance Field Intervention: Enabling VLAs to Escape Memory Traps in Robotic Manipulation", "comment": null, "summary": "Vision-Language-Action (VLA) models have shown great performance in robotic manipulation by mapping visual observations and language instructions directly to actions. However, they remain brittle under distribution shifts: when test scenarios change, VLAs often reproduce memorized trajectories instead of adapting to the updated scene, which is a failure mode we refer to as the \"Memory Trap\". This limitation stems from the end-to-end design, which lacks explicit 3D spatial reasoning and prevents reliable identification of actionable regions in unfamiliar environments. To compensate for this missing spatial understanding, 3D Spatial Affordance Fields (SAFs) can provide a geometric representation that highlights where interactions are physically feasible, offering explicit cues about regions the robot should approach or avoid. We therefore introduce Affordance Field Intervention (AFI), a lightweight hybrid framework that uses SAFs as an on-demand plug-in to guide VLA behavior. Our system detects memory traps through proprioception, repositions the robot to recent high-affordance regions, and proposes affordance-driven waypoints that anchor VLA-generated actions. A SAF-based scorer then selects trajectories with the highest cumulative affordance. Extensive experiments demonstrate that our method achieves an average improvement of 23.5% across different VLA backbones ($\u03c0_{0}$ and $\u03c0_{0.5}$) under out-of-distribution scenarios on real-world robotic platforms, and 20.2% on the LIBERO-Pro benchmark, validating its effectiveness in enhancing VLA robustness to distribution shifts.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u7684AFI\u6846\u67b6\u901a\u8fc7\u7ed3\u54083D\u7a7a\u95f4\u53ef\u4f9b\u6027\u573a\uff0c\u6709\u6548\u89e3\u51b3\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8\u6a21\u578b\u5728\u5206\u5e03\u8fc1\u79fb\u4e2d\u7684\u5185\u5b58\u9677\u9631\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u9c81\u68d2\u6027\u3002 ", "motivation": "\u968f\u7740\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u5e94\u7528\u589e\u591a\uff0c\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5206\u5e03\u8fc1\u79fb\u95ee\u9898\u66b4\u9732\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u6a21\u578b\u4e0d\u80fd\u9002\u5e94\u65b0\u7684\u73af\u5883\u800c\u9677\u5165\u2018\u5185\u5b58\u9677\u9631\u2019\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6df7\u5408\u6846\u67b6\uff0c\u5229\u75283D\u7a7a\u95f4\u53ef\u4f9b\u6027\u573a(SAFs)\u4f5c\u4e3a\u63d2\u4ef6\u5f15\u5bfcVLA\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u81ea\u6211\u611f\u77e5\u68c0\u6d4b\u5185\u5b58\u9677\u9631\uff0c\u8c03\u6574\u673a\u5668\u4eba\u4f4d\u7f6e\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u53ef\u4f9b\u6027\u7684\u8def\u5f84\u70b9\u3002", "result": "\u5728\u771f\u5b9e\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\uff0c\u672c\u65b9\u6cd5\u5728\u4e0d\u540c\u7684VLA\u57fa\u7840\u6a21\u578b\u4e0b\u53d6\u5f97\u4e8623.5%\u7684\u5e73\u5747\u63d0\u5347\uff0c\u5728LIBERO-Pro\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u63d0\u534720.2%\uff0c\u9a8c\u8bc1\u4e86\u5728\u5e94\u5bf9\u5206\u5e03\u8fc1\u79fb\u65f6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684Affordance Field Intervention (AFI)\u6846\u67b6\u6709\u6548\u63d0\u9ad8\u4e86\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8\u6a21\u578b\u5728\u5206\u5e03\u8fc1\u79fb\u60c5\u5883\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4e0d\u540c\u7684VLA\u57fa\u7840\u6a21\u578b\u4e0a\u5e73\u5747\u63d0\u534723.5%\u7684\u6027\u80fd\u3002"}}
{"id": "2512.07482", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07482", "abs": "https://arxiv.org/abs/2512.07482", "authors": ["Florian L\u00fcttner", "Nicole Neis", "Daniel Stadler", "Robin Moss", "Mirjam Fehling-Kaschek", "Matthias Pfriem", "Alexander Stolz", "Jens Ziehn"], "title": "From Real-World Traffic Data to Relevant Critical Scenarios", "comment": "8 pages, 8 figures", "summary": "The reliable operation of autonomous vehicles, automated driving functions, and advanced driver assistance systems across a wide range of relevant scenarios is critical for their development and deployment. Identifying a near-complete set of relevant driving scenarios for such functionalities is challenging due to numerous degrees of freedom involved, each affecting the outcomes of the driving scenario differently. Moreover, with increasing technical complexity of new functionalities, the number of potentially relevant, particularly \"unknown unsafe\" scenarios is increasing. To enhance validation efficiency, it is essential to identify relevant scenarios in advance, starting with simpler domains like highways before moving to more complex environments such as urban traffic. To address this, this paper focuses on analyzing lane change scenarios in highway traffic, which involve multiple degrees of freedom and present numerous safetyrelevant scenarios. We describe the process of data acquisition and processing of real-world data from public highway traffic, followed by the application of criticality measures on trajectory data to evaluate scenarios, as conducted within the AVEAS project (www.aveas.org). By linking the calculated measures to specific lane change driving scenarios and the conditions under which the data was collected, we facilitate the identification of safetyrelevant driving scenarios for various applications. Further, to tackle the extensive range of \"unknown unsafe\" scenarios, we propose a way to generate relevant scenarios by creating synthetic scenarios based on recorded ones. Consequently, we demonstrate and evaluate a processing chain that enables the identification of safety-relevant scenarios, the development of data-driven methods for extracting these scenarios, and the generation of synthetic critical scenarios via sampling on highways.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u9ad8\u901f\u516c\u8def\u8f66\u9053\u53d8\u6362\u573a\u666f\u7684\u91cd\u8981\u6027\uff0c\u901a\u8fc7\u73b0\u5b9e\u6570\u636e\u7684\u5904\u7406\u548c\u5173\u952e\u5ea6\u5ea6\u91cf\uff0c\u8bc6\u522b\u5b89\u5168\u76f8\u5173\u9a7e\u9a76\u573a\u666f\uff0c\u5e76\u5f15\u5165\u5408\u6210\u573a\u666f\u751f\u6210\u4ee5\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u529f\u80fd\u7684\u9a8c\u8bc1\u6548\u7387\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u7684\u590d\u6742\u6027\u589e\u52a0\uff0c\u8bc6\u522b\u548c\u9a8c\u8bc1\u76f8\u5173\u9a7e\u9a76\u573a\u666f\u53d8\u5f97\u6108\u52a0\u91cd\u8981\uff0c\u4ee5\u786e\u4fdd\u8f66\u8f86\u5728\u5404\u79cd\u60c5\u51b5\u4e0b\u7684\u5b89\u5168\u64cd\u4f5c\u3002", "method": "\u901a\u8fc7\u5bf9\u516c\u5171\u9ad8\u901f\u516c\u8def\u4ea4\u901a\u7684\u771f\u5b9e\u6570\u636e\u8fdb\u884c\u91c7\u96c6\u548c\u5904\u7406\uff0c\u5e94\u7528\u5173\u952e\u5ea6\u5ea6\u91cf\u5bf9\u8f66\u9053\u53d8\u6362\u573a\u666f\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u751f\u6210\u57fa\u4e8e\u5f55\u5236\u6570\u636e\u7684\u5408\u6210\u573a\u666f\u3002", "result": "\u6210\u529f\u6807\u8bc6\u51fa\u5b89\u5168\u76f8\u5173\u9a7e\u9a76\u573a\u666f\uff0c\u5e76\u5c55\u793a\u4e86\u4e00\u4e2a\u5904\u7406\u94fe\uff0c\u8be5\u94fe\u80fd\u591f\u8bc6\u522b\u548c\u751f\u6210\u5371\u6025\u573a\u666f\u7684\u6709\u6548\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc6\u522b\u548c\u751f\u6210\u5b89\u5168\u76f8\u5173\u9a7e\u9a76\u573a\u666f\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u9a8c\u8bc1\u6548\u7387\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u901f\u516c\u8def\u8f66\u9053\u53d8\u6362\u573a\u666f\u4e2d\uff0c\u5f3a\u8c03\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u53ca\u5176\u5728\u5408\u6210\u5371\u6025\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2512.07507", "categories": ["cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.07507", "abs": "https://arxiv.org/abs/2512.07507", "authors": ["Yiming Cui", "Shiyu Fang", "Jiarui Zhang", "Yan Huang", "Chengkai Xu", "Bing Zhu", "Hao Zhang", "Peng Hang", "Jian Sun"], "title": "VP-AutoTest: A Virtual-Physical Fusion Autonomous Driving Testing Platform", "comment": null, "summary": "The rapid development of autonomous vehicles has led to a surge in testing demand. Traditional testing methods, such as virtual simulation, closed-course, and public road testing, face several challenges, including unrealistic vehicle states, limited testing capabilities, and high costs. These issues have prompted increasing interest in virtual-physical fusion testing. However, despite its potential, virtual-physical fusion testing still faces challenges, such as limited element types, narrow testing scope, and fixed evaluation metrics. To address these challenges, we propose the Virtual-Physical Testing Platform for Autonomous Vehicles (VP-AutoTest), which integrates over ten types of virtual and physical elements, including vehicles, pedestrians, and roadside infrastructure, to replicate the diversity of real-world traffic participants. The platform also supports both single-vehicle interaction and multi-vehicle cooperation testing, employing adversarial testing and parallel deduction to accelerate fault detection and explore algorithmic limits, while OBU and Redis communication enable seamless vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) cooperation across all levels of cooperative automation. Furthermore, VP-AutoTest incorporates a multidimensional evaluation framework and AI-driven expert systems to conduct comprehensive performance assessment and defect diagnosis. Finally, by comparing virtual-physical fusion test results with real-world experiments, the platform performs credibility self-evaluation to ensure both the fidelity and efficiency of autonomous driving testing. Please refer to the website for the full testing functionalities on the autonomous driving public service platform OnSite:https://www.onsite.com.cn.", "AI": {"tldr": "VP-AutoTest\u5e73\u53f0\u6574\u5408\u591a\u79cd\u6d4b\u8bd5\u5143\u7d20\uff0c\u63d0\u5347\u81ea\u4e3b\u9a7e\u9a76\u6d4b\u8bd5\u7684\u6548\u7387\u548c\u53ef\u4fe1\u5ea6\uff0c\u89e3\u51b3\u4f20\u7edf\u6d4b\u8bd5\u4e2d\u7684\u591a\u9879\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u7684\u6d4b\u8bd5\u65b9\u6cd5\u9762\u4e34\u4e0d\u73b0\u5b9e\u7684\u8f66\u8f86\u72b6\u6001\u3001\u6709\u9650\u7684\u6d4b\u8bd5\u80fd\u529b\u548c\u9ad8\u6210\u672c\u7b49\u6311\u6218\uff0c\u865a\u62df-\u7269\u7406\u878d\u5408\u6d4b\u8bd5\u5728\u8fd9\u4e00\u80cc\u666f\u4e0b\u53d7\u5230\u5173\u6ce8\u3002", "method": "\u5f00\u53d1\u4e86VP-AutoTest\u5e73\u53f0\uff0c\u96c6\u6210\u4e86\u591a\u79cd\u865a\u62df\u4e0e\u7269\u7406\u5143\u7d20\uff0c\u652f\u6301\u5355\u8f66\u548c\u591a\u8f66\u7684\u4e92\u52a8\u8f66\u8f86\u6d4b\u8bd5\uff0c\u7ed3\u5408\u5bf9\u6297\u6d4b\u8bd5\u548c\u5e76\u884c\u63a8\u7406\u52a0\u5feb\u6545\u969c\u68c0\u6d4b\uff0c\u5e76\u901a\u8fc7\u591a\u7ef4\u8bc4\u4f30\u6846\u67b6\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\u3002", "result": "VP-AutoTest\u5e73\u53f0\u4e0d\u4ec5\u96c6\u6210\u4e86\u8d85\u8fc7\u5341\u79cd\u4ea4\u901a\u53c2\u4e0e\u8005\u5143\u7d20\uff0c\u8fd8\u5177\u5907\u57fa\u4e8e\u5b9e\u9645\u5b9e\u9a8c\u4e0e\u865a\u62df-\u7269\u7406\u6d4b\u8bd5\u7ed3\u679c\u5bf9\u6bd4\u7684\u53ef\u4fe1\u5ea6\u81ea\u6211\u8bc4\u4f30\u80fd\u529b\u3002", "conclusion": "VP-AutoTest\u5e73\u53f0\u901a\u8fc7\u865a\u62df\u4e0e\u7269\u7406\u5143\u7d20\u7684\u878d\u5408\uff0c\u63d0\u4f9b\u591a\u7ef4\u8bc4\u4f30\u6846\u67b6\u548cAI\u9a71\u52a8\u7684\u4e13\u5bb6\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u5bf9\u81ea\u4e3b\u9a7e\u9a76\u6d4b\u8bd5\u7684\u9ad8\u4fdd\u771f\u5ea6\u4e0e\u9ad8\u6548\u7387\u7684\u81ea\u6211\u8bc4\u4f30\u3002"}}
{"id": "2512.07582", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07582", "abs": "https://arxiv.org/abs/2512.07582", "authors": ["Guangyan Chen", "Meiling Wang", "Qi Shao", "Zichen Zhou", "Weixin Mao", "Te Cui", "Minzhao Zhu", "Yinan Deng", "Luojie Yang", "Zhanqi Zhang", "Yi Yang", "Hua Chen", "Yufeng Yue"], "title": "See Once, Then Act: Vision-Language-Action Model with Task Learning from One-Shot Video Demonstrations", "comment": null, "summary": "Developing robust and general-purpose manipulation policies represents a fundamental objective in robotics research. While Vision-Language-Action (VLA) models have demonstrated promising capabilities for end-to-end robot control, existing approaches still exhibit limited generalization to tasks beyond their training distributions. In contrast, humans possess remarkable proficiency in acquiring novel skills by simply observing others performing them once. Inspired by this capability, we propose ViVLA, a generalist robotic manipulation policy that achieves efficient task learning from a single expert demonstration video at test time. Our approach jointly processes an expert demonstration video alongside the robot's visual observations to predict both the demonstrated action sequences and subsequent robot actions, effectively distilling fine-grained manipulation knowledge from expert behavior and transferring it seamlessly to the agent. To enhance the performance of ViVLA, we develop a scalable expert-agent pair data generation pipeline capable of synthesizing paired trajectories from easily accessible human videos, further augmented by curated pairs from publicly available datasets. This pipeline produces a total of 892,911 expert-agent samples for training ViVLA. Experimental results demonstrate that our ViVLA is able to acquire novel manipulation skills from only a single expert demonstration video at test time. Our approach achieves over 30% improvement on unseen LIBERO tasks and maintains above 35% gains with cross-embodiment videos. Real-world experiments demonstrate effective learning from human videos, yielding more than 38% improvement on unseen tasks.", "AI": {"tldr": "ViVLA\u662f\u4e00\u79cd\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\uff0c\u4ece\u5355\u4e2a\u4e13\u5bb6\u89c6\u9891\u4e2d\u9ad8\u6548\u5b66\u4e60\u65b0\u6280\u80fd\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u673a\u5668\u4eba\u7814\u7a76\u7684\u4e00\u4e2a\u57fa\u672c\u76ee\u6807\u662f\u5f00\u53d1\u5f3a\u5927\u4e14\u901a\u7528\u7684\u64cd\u4f5c\u653f\u7b56\uff0c\u800c\u73b0\u6709VLA\u6a21\u578b\u5728\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u542f\u53d1\u4e8e\u662f\u5f00\u53d1ViVLA\u3002", "method": "\u63d0\u51faViVLA\uff0c\u4e00\u79cd\u4ece\u5355\u4e2a\u4e13\u5bb6\u6f14\u793a\u89c6\u9891\u5b66\u4e60\u4efb\u52a1\u7684\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\uff0c\u901a\u8fc7\u5904\u7406\u4e13\u5bb6\u89c6\u9891\u4e0e\u673a\u5668\u4eba\u89c6\u89c9\u89c2\u5bdf\u6765\u9884\u6d4b\u52a8\u4f5c\u5e8f\u5217\u548c\u540e\u7eed\u52a8\u4f5c\u3002", "result": "\u5728892,911\u4e2a\u4e13\u5bb6-\u4ee3\u7406\u6837\u672c\u7684\u8bad\u7ec3\u4e0b\uff0cViVLA\u80fd\u4ece\u5355\u4e2a\u4e13\u5bb6\u6f14\u793a\u4e2d\u9ad8\u6548\u5b66\u4e60\u65b0\u64cd\u4f5c\u6280\u80fd\u3002", "conclusion": "ViVLA\u5728\u672a\u89c1\u7684LIBERO\u4efb\u52a1\u4e0a\u63d0\u9ad8\u4e86\u8d85\u8fc730%\u7684\u8868\u73b0\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u6709\u6548\u7684\u5b66\u4e60\u80fd\u529b\u3002"}}
{"id": "2512.07673", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07673", "abs": "https://arxiv.org/abs/2512.07673", "authors": ["Matthias Heyrman", "Chenhao Li", "Victor Klemm", "Dongho Kang", "Stelian Coros", "Marco Hutter"], "title": "Multi-Domain Motion Embedding: Expressive Real-Time Mimicry for Legged Robots", "comment": "15 pages", "summary": "Effective motion representation is crucial for enabling robots to imitate expressive behaviors in real time, yet existing motion controllers often ignore inherent patterns in motion. Previous efforts in representation learning do not attempt to jointly capture structured periodic patterns and irregular variations in human and animal movement. To address this, we present Multi-Domain Motion Embedding (MDME), a motion representation that unifies the embedding of structured and unstructured features using a wavelet-based encoder and a probabilistic embedding in parallel. This produces a rich representation of reference motions from a minimal input set, enabling improved generalization across diverse motion styles and morphologies. We evaluate MDME on retargeting-free real-time motion imitation by conditioning robot control policies on the learned embeddings, demonstrating accurate reproduction of complex trajectories on both humanoid and quadruped platforms. Our comparative studies confirm that MDME outperforms prior approaches in reconstruction fidelity and generalizability to unseen motions. Furthermore, we demonstrate that MDME can reproduce novel motion styles in real-time through zero-shot deployment, eliminating the need for task-specific tuning or online retargeting. These results position MDME as a generalizable and structure-aware foundation for scalable real-time robot imitation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7684\u591a\u57df\u8fd0\u52a8\u5d4c\u5165\uff08MDME\uff09\u7ed3\u5408\u4e86\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u7279\u5f81\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u591a\u6837\u5316\u8fd0\u52a8\u98ce\u683c\u4e0b\u7684\u5b9e\u65f6\u6a21\u4eff\u80fd\u529b\uff0c\u5e76\u5728\u7cbe\u786e\u518d\u73b0\u590d\u6742\u8f68\u8ff9\u4e0a outperform \u4e86\u65e2\u5f80\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u8fd0\u52a8\u63a7\u5236\u5668\u5ffd\u89c6\u4e86\u8fd0\u52a8\u4e2d\u7684\u56fa\u6709\u6a21\u5f0f\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u4eba\u4e0e\u52a8\u7269\u8fd0\u52a8\u4e2d\u7684\u7ed3\u6784\u5316\u5468\u671f\u6a21\u5f0f\u548c\u4e0d\u89c4\u5219\u53d8\u5316\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u5f00\u53d1\u65b0\u7684\u8fd0\u52a8\u8868\u793a\u3002", "method": "MDME\u4f7f\u7528\u5c0f\u6ce2\u7f16\u7801\u5668\u548c\u5e76\u884c\u6982\u7387\u5d4c\u5165\uff0c\u7edf\u4e00\u7ed3\u6784\u5316\u7279\u5f81\u548c\u975e\u7ed3\u6784\u5316\u7279\u5f81\u7684\u5d4c\u5165\uff0c\u751f\u6210\u4e30\u5bcc\u7684\u53c2\u8003\u8fd0\u52a8\u8868\u793a\u3002", "result": "MDME\u5728\u65e0\u91cd\u5b9a\u5411\u7684\u5b9e\u65f6\u8fd0\u52a8\u6a21\u4eff\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u7cbe\u786e\u518d\u73b0\u590d\u6742\u8f68\u8ff9\uff0c\u5e76\u5728\u91cd\u5efa\u7cbe\u5ea6\u548c\u5bf9\u672a\u89c1\u8fd0\u52a8\u7684\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u4f18\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\u3002", "conclusion": "MDME\u4f5c\u4e3a\u4e00\u79cd\u901a\u7528\u4e14\u610f\u8bc6\u5230\u7ed3\u6784\u7684\u57fa\u7840\uff0c\u80fd\u591f\u5728\u65e0\u9700\u7279\u5b9a\u4efb\u52a1\u8c03\u6574\u6216\u5728\u7ebf\u91cd\u5b9a\u5411\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u5b9e\u65f6\u673a\u5668\u4eba\u6a21\u4eff\u4e2d\u6709\u6548\u518d\u73b0\u590d\u6742\u7684\u8fd0\u52a8\u98ce\u683c\u3002"}}
{"id": "2512.07680", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07680", "abs": "https://arxiv.org/abs/2512.07680", "authors": ["P. A. Wigner", "L. Romanello", "A. Hammad", "P. H. Nguyen", "T. Lan", "S. F. Armanini", "B. B. Kocer", "M. Kovac"], "title": "AMBER: Aerial deployable gripping crawler with compliant microspine for canopy manipulation", "comment": null, "summary": "This paper presents an aerially deployable crawler designed for adaptive locomotion and manipulation within tree canopies. The system combines compliant microspine-based tracks, a dual-track rotary gripper, and an elastic tail, enabling secure attachment and stable traversal across branches of varying curvature and inclination.\n  Experiments demonstrate reliable gripping up to 90 degrees of body roll and inclination, while effective climbing on branches inclined up to 67.5 degrees, achieving a maximum speed of 0.55 body lengths per second on horizontal branches. The compliant tracks allow yaw steering of up to 10 degrees, enhancing maneuverability on irregular surfaces.\n  Power measurements show efficient operation with a dimensionless cost of transport over an order of magnitude lower than typical hovering power consumption in aerial robots. Integrated within a drone-tether deployment system, the crawler provides a robust, low-power platform for environmental sampling and in-canopy sensing, bridging the gap between aerial and surface-based ecological robotics.", "AI": {"tldr": "\u672c\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7528\u4e8e\u6811\u51a0\u5185\u9002\u5e94\u6027\u8fd0\u52a8\u548c\u64cd\u4f5c\u7684\u7a7a\u4e2d\u90e8\u7f72\u722c\u884c\u5668\uff0c\u91c7\u7528\u5fae\u68f1\u8f68\u9053\u548c\u53cc\u8f68\u5939\u6301\u5668\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u5176\u5177\u6709\u5f88\u597d\u7684\u6293\u63e1\u548c\u6500\u722c\u80fd\u529b\uff0c\u4e14\u80fd\u9ad8\u6548\u5de5\u4f5c\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u9002\u5e94\u6027\u5f3a\u7684\u5730\u9762\u722c\u884c\u52a8\u7269\uff0c\u4ee5\u4fbf\u5728\u6811\u51a0\u5185\u8fdb\u884c\u5b9a\u4f4d\u548c\u64cd\u4f5c\u3002", "method": "\u91c7\u7528\u5408\u89c4\u7684\u5fae\u68f1\u8f68\u9053\u3001\u53cc\u8f68\u65cb\u8f6c\u5939\u6301\u5668\u548c\u5f39\u6027\u5c3e\uff0c\u8bbe\u8ba1\u51fa\u4e00\u79cd\u7a7a\u4e2d\u53ef\u90e8\u7f72\u7684\u722c\u884c\u5668\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u722c\u884c\u5668\u5728\u9ad8\u8fbe90\u5ea6\u7684\u8eab\u4f53\u7ffb\u6eda\u548c\u503e\u659c\u4e0b\u4fdd\u6301\u53ef\u9760\u7684\u6293\u63e1\u80fd\u529b\uff0c\u5e76\u80fd\u5728\u9ad8\u8fbe67.5\u5ea6\u7684\u659c\u679d\u4e0a\u6709\u6548\u6500\u722c\uff0c\u6c34\u5e73\u679d\u6761\u4e0a\u7684\u6700\u5927\u901f\u5ea6\u4e3a\u6bcf\u79d20.55\u4e2a\u8eab\u4f53\u957f\u5ea6\u3002", "conclusion": "\u8be5\u722c\u884c\u5668\u5728\u73af\u4fdd\u91c7\u6837\u548c\u6811\u51a0\u5185\u4f20\u611f\u4e2d\u5c55\u73b0\u51fa\u4e86\u9ad8\u6548\u7684\u64cd\u4f5c\u6027\u80fd\uff0c\u5e76\u6709\u6548\u964d\u4f4e\u4e86\u80fd\u6e90\u6d88\u8017\uff0c\u586b\u8865\u4e86\u7a7a\u4e2d\u4e0e\u5730\u9762\u751f\u6001\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u7a7a\u767d\u3002"}}
{"id": "2512.07697", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07697", "abs": "https://arxiv.org/abs/2512.07697", "authors": ["Aileen Liao", "Dong-Ki Kim", "Max Olan Smith", "Ali-akbar Agha-mohammadi", "Shayegan Omidshafiei"], "title": "Delay-Aware Diffusion Policy: Bridging the Observation-Execution Gap in Dynamic Tasks", "comment": null, "summary": "As a robot senses and selects actions, the world keeps changing. This inference delay creates a gap of tens to hundreds of milliseconds between the observed state and the state at execution. In this work, we take the natural generalization from zero delay to measured delay during training and inference. We introduce Delay-Aware Diffusion Policy (DA-DP), a framework for explicitly incorporating inference delays into policy learning. DA-DP corrects zero-delay trajectories to their delay-compensated counterparts, and augments the policy with delay conditioning. We empirically validate DA-DP on a variety of tasks, robots, and delays and find its success rate more robust to delay than delay-unaware methods. DA-DP is architecture agnostic and transfers beyond diffusion policies, offering a general pattern for delay-aware imitation learning. More broadly, DA-DP encourages evaluation protocols that report performance as a function of measured latency, not just task difficulty.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5ef6\u8fdf\u611f\u77e5\u6269\u6563\u7b56\u7565\uff08DA-DP\uff09\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u5b58\u5728\u63a8\u7406\u5ef6\u8fdf\u60c5\u51b5\u4e0b\u7684\u7b56\u7565\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u673a\u5668\u4eba\u5728\u611f\u77e5\u548c\u9009\u62e9\u884c\u52a8\u65f6\uff0c\u4e16\u754c\u4e0d\u65ad\u53d8\u5316\uff0c\u5bfc\u81f4\u89c2\u5bdf\u72b6\u6001\u4e0e\u6267\u884c\u72b6\u6001\u4e4b\u95f4\u5b58\u5728\u5ef6\u8fdf\u3002", "method": "\u63d0\u51fa\u5ef6\u8fdf\u611f\u77e5\u6269\u6563\u7b56\u7565\uff08DA-DP\uff09\uff0c\u5728\u7b56\u7565\u5b66\u4e60\u4e2d\u663e\u5f0f\u5f15\u5165\u63a8\u7406\u5ef6\u8fdf\u3002", "result": "DA-DP\u5728\u5404\u79cd\u4efb\u52a1\u3001\u673a\u5668\u4eba\u548c\u5ef6\u8fdf\u60c5\u51b5\u4e0b\u7684\u9a8c\u8bc1\u4e2d\uff0c\u5176\u6210\u529f\u7387\u5bf9\u5ef6\u8fdf\u7684\u9c81\u68d2\u6027\u66f4\u5f3a\u3002", "conclusion": "DA-DP\u662f\u4e00\u79cd\u67b6\u6784\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u8d85\u8d8a\u4e86\u6269\u6563\u7b56\u7565\uff0c\u4e3a\u5ef6\u8fdf\u611f\u77e5\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u6a21\u5f0f\uff0c\u4fc3\u8fdb\u4e86\u62a5\u544a\u57fa\u4e8e\u6d4b\u91cf\u5ef6\u8fdf\u7684\u6027\u80fd\u8bc4\u4f30\u534f\u8bae\u3002"}}
{"id": "2512.07765", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07765", "abs": "https://arxiv.org/abs/2512.07765", "authors": ["Gustavo A. Cardona", "Shubham S. Kumbhar", "Panagiotis Artemiadis"], "title": "Toward Seamless Physical Human-Humanoid Interaction: Insights from Control, Intent, and Modeling with a Vision for What Comes Next", "comment": "60 pages, 5 figures, 3 tables", "summary": "Physical Human-Humanoid Interaction (pHHI) is a rapidly advancing field with significant implications for deploying robots in unstructured, human-centric environments. In this review, we examine the current state of the art in pHHI through three core pillars: (i) humanoid modeling and control, (ii) human intent estimation, and (iii) computational human models. For each pillar, we survey representative approaches, identify open challenges, and analyze current limitations that hinder robust, scalable, and adaptive interaction. These include the need for whole-body control strategies capable of handling uncertain human dynamics, real-time intent inference under limited sensing, and modeling techniques that account for variability in human physical states. Although significant progress has been made within each domain, integration across pillars remains limited. We propose pathways for unifying methods across these areas to enable cohesive interaction frameworks. This structure enables us not only to map the current landscape but also to propose concrete directions for future research that aim to bridge these domains. Additionally, we introduce a unified taxonomy of interaction types based on modality, distinguishing between direct interactions (e.g., physical contact) and indirect interactions (e.g., object-mediated), and on the level of robot engagement, ranging from assistance to cooperation and collaboration. For each category in this taxonomy, we provide the three core pillars that highlight opportunities for cross-pillar unification. Our goal is to suggest avenues to advance robust, safe, and intuitive physical interaction, providing a roadmap for future research that will allow humanoid systems to effectively understand, anticipate, and collaborate with human partners in diverse real-world settings.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7efc\u8ff0\u4e86\u7269\u7406\u4eba-\u7c7b\u4eba\u4ea4\u4e92\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u63d0\u51fa\u4e86\u7ed3\u5408\u5efa\u6a21\u3001\u4eba\u7c7b\u610f\u56fe\u548c\u8ba1\u7b97\u6a21\u578b\u7684\u8de8\u9886\u57df\u6574\u5408\u65b9\u6cd5\uff0c\u4ee5\u4fc3\u8fdb\u4eba\u5f62\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u7684\u5b89\u5168\u3001\u9ad8\u6548\u4e92\u52a8\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u5728\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u73af\u5883\u4e2d\u7684\u5e94\u7528\u6108\u52a0\u5e7f\u6cdb\uff0c\u7814\u7a76\u7269\u7406\u4eba-\u7c7b\u4eba\u4ea4\u4e92(pHHI)\u7684\u6709\u6548\u6027\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u4e09\u4e2a\u6838\u5fc3\u652f\u67f1\uff1a\u4eba\u5f62\u5efa\u6a21\u4e0e\u63a7\u5236\u3001\u4eba\u7c7b\u610f\u56fe\u4f30\u8ba1\u548c\u8ba1\u7b97\u4eba\u7c7b\u6a21\u578b\uff0c\u5206\u6790\u73b0\u72b6\u3001\u6311\u6218\u548c\u5c40\u9650\u6027\u3002", "result": "\u5c3d\u7ba1\u5404\u9886\u57df\u5df2\u6709\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u67f1\u95f4\u6574\u5408\u6709\u9650\uff0c\u9700\u8981\u89e3\u51b3\u4e0d\u786e\u5b9a\u4eba\u7c7b\u52a8\u6001\u7684\u6574\u4f53\u63a7\u5236\u7b56\u7565\u3001\u5b9e\u65f6\u610f\u56fe\u63a8\u65ad\u548c\u8003\u8651\u4eba\u7c7b\u8eab\u4f53\u72b6\u6001\u53d8\u5f02\u6027\u7684\u5efa\u6a21\u6280\u672f\u3002", "conclusion": "\u8be5\u7814\u7a76\u5efa\u8bae\u4e86\u4e00\u6761\u8054\u5408\u8de8\u67f1\u7684\u65b9\u6cd5\uff0c\u4ee5\u4fc3\u8fdb\u4eba\u5f62\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u7684\u4e92\u52a8\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u63d0\u9ad8\u4ea4\u6d41\u7684\u9c81\u68d2\u6027\u3001\u5b89\u5168\u6027\u548c\u76f4\u89c2\u6027\u3002"}}
{"id": "2512.07775", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07775", "abs": "https://arxiv.org/abs/2512.07775", "authors": ["David Thorne", "Nathan Chan", "Christa S. Robison", "Philip R. Osteen", "Brett T. Lopez"], "title": "OptMap: Geometric Map Distillation via Submodular Maximization", "comment": null, "summary": "Autonomous robots rely on geometric maps to inform a diverse set of perception and decision-making algorithms. As autonomy requires reasoning and planning on multiple scales of the environment, each algorithm may require a different map for optimal performance. Light Detection And Ranging (LiDAR) sensors generate an abundance of geometric data to satisfy these diverse requirements, but selecting informative, size-constrained maps is computationally challenging as it requires solving an NP-hard combinatorial optimization. In this work we present OptMap: a geometric map distillation algorithm which achieves real-time, application-specific map generation via multiple theoretical and algorithmic innovations. A central feature is the maximization of set functions that exhibit diminishing returns, i.e., submodularity, using polynomial-time algorithms with provably near-optimal solutions. We formulate a novel submodular reward function which quantifies informativeness, reduces input set sizes, and minimizes bias in sequentially collected datasets. Further, we propose a dynamically reordered streaming submodular algorithm which improves empirical solution quality and addresses input order bias via an online approximation of the value of all scans. Testing was conducted on open-source and custom datasets with an emphasis on long-duration mapping sessions, highlighting OptMap's minimal computation requirements. Open-source ROS1 and ROS2 packages are available and can be used alongside any LiDAR SLAM algorithm.", "AI": {"tldr": "OptMap\u662f\u4e00\u79cd\u51e0\u4f55\u5730\u56fe\u63d0\u53d6\u7b97\u6cd5\uff0c\u901a\u8fc7\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u5b9e\u73b0\u5b9e\u65f6\u5e94\u7528\u7279\u5b9a\u5730\u56fe\u751f\u6210\uff0c\u4f18\u5316\u4e86LiDAR\u4f20\u611f\u5668\u751f\u6210\u7684\u51e0\u4f55\u6570\u636e\u6765\u6ee1\u8db3\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u611f\u77e5\u548c\u51b3\u7b56\u4e2d\u7684\u9700\u6c42\u3002", "motivation": "\u81ea\u4e3b\u673a\u5668\u4eba\u4f9d\u8d56\u51e0\u4f55\u5730\u56fe\u6765\u652f\u6301\u5176\u611f\u77e5\u548c\u51b3\u7b56\u7b97\u6cd5\uff0c\u9762\u4e34\u7684\u6311\u6218\u662f\u5982\u4f55\u9009\u62e9\u4fe1\u606f\u91cf\u4e30\u5bcc\u4e14\u53d7\u5927\u5c0f\u9650\u5236\u7684\u5730\u56fe\uff0c\u8fd9\u9700\u8981\u89e3\u51b3NP\u56f0\u96be\u7684\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b50\u6a21\u5956\u52b1\u51fd\u6570\u6765\u91cf\u5316\u4fe1\u606f\u91cf\uff0c\u51cf\u5c11\u8f93\u5165\u96c6\u5408\u7684\u5927\u5c0f\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u91cd\u6392\u5e8f\u6d41\u5f0f\u5b50\u6a21\u7b97\u6cd5\u6539\u8fdb\u4e86\u89e3\u51b3\u65b9\u6848\u7684\u8d28\u91cf\uff0c\u89e3\u51b3\u8f93\u5165\u987a\u5e8f\u504f\u5dee\u3002", "result": "\u5728\u516c\u5f00\u548c\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u4e0a\u7684\u6d4b\u8bd5\u4e2d\uff0cOptMap\u5c55\u793a\u4e86\u51fa\u8272\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u957f\u65f6\u95f4\u6620\u5c04\u4f1a\u8bdd\u4e2d\uff0c\u8868\u73b0\u51fa\u6700\u4f4e\u7684\u8ba1\u7b97\u9700\u6c42\u3002", "conclusion": "OptMap\u80fd\u591f\u5b9e\u65f6\u751f\u6210\u5e94\u7528\u7279\u5b9a\u5730\u56fe\uff0c\u5e76\u63d0\u4f9b\u5f00\u6e90ROS1\u548cROS2\u8f6f\u4ef6\u5305\uff0c\u53ef\u4e0e\u4efb\u4f55LiDAR SLAM\u7b97\u6cd5\u914d\u5408\u4f7f\u7528\u3002"}}
{"id": "2512.07813", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07813", "abs": "https://arxiv.org/abs/2512.07813", "authors": ["Hari Prakash Thanabalan", "Lars Bengtsson", "Ugo Lafont", "Giovanni Volpe"], "title": "Inchworm-Inspired Soft Robot with Groove-Guided Locomotion", "comment": null, "summary": "Soft robots require directional control to navigate complex terrains. However, achieving such control often requires multiple actuators, which increases mechanical complexity, complicates control systems, and raises energy consumption. Here, we introduce an inchworm-inspired soft robot whose locomotion direction is controlled passively by patterned substrates. The robot employs a single rolled dielectric elastomer actuator, while groove patterns on a 3D-printed substrate guide its alignment and trajectory. Through systematic experiments, we demonstrate that varying groove angles enables precise control of locomotion direction without the need for complex actuation strategies. This groove-guided approach reduces energy consumption, simplifies robot design, and expands the applicability of bio-inspired soft robots in fields such as search and rescue, pipe inspection, and planetary exploration.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u53d7\u86af\u8693\u542f\u53d1\u7684\u8f6f\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u6c9f\u69fd\u6a21\u5f0f\u4e3b\u52a8\u63a7\u5236\u8fd0\u52a8\u65b9\u5411\uff0c\u7b80\u5316\u8bbe\u8ba1\uff0c\u964d\u4f4e\u80fd\u8017\uff0c\u62d3\u5c55\u5e94\u7528\u9886\u57df\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8f6f\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u5bfc\u822a\u4e2d\u7684\u591a\u9a71\u52a8\u5668\u6240\u5e26\u6765\u7684\u673a\u68b0\u590d\u6742\u6027\u3001\u63a7\u5236\u7cfb\u7edf\u590d\u6742\u6027\u548c\u80fd\u8017\u589e\u52a0\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5355\u4e2a\u5377\u66f2\u7684\u4ecb\u7535\u5f39\u6027\u4f53\u6267\u884c\u5668\uff0c\u7ed3\u54083D\u6253\u5370\u7684\u5177\u6c9f\u69fd\u6a21\u5f0f\u7684\u57fa\u5e95\uff0c\u63a7\u5236\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u65b9\u5411\u3002", "result": "\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u6539\u53d8\u6c9f\u69fd\u89d2\u5ea6\u53ef\u4ee5\u5b9e\u73b0\u7cbe\u786e\u7684\u8fd0\u52a8\u65b9\u5411\u63a7\u5236\uff0c\u800c\u65e0\u9700\u590d\u6742\u7684\u9a71\u52a8\u7b56\u7565\u3002", "conclusion": "\u4f7f\u7528\u6c9f\u69fd\u5bfc\u5411\u7684\u65b9\u6cd5\u51cf\u5c11\u4e86\u80fd\u8017\uff0c\u7b80\u5316\u4e86\u673a\u5668\u4eba\u8bbe\u8ba1\uff0c\u5e76\u6269\u5927\u4e86\u4eff\u751f\u8f6f\u673a\u5668\u4eba\u5728\u5404\u79cd\u9886\u57df\u7684\u5e94\u7528\u3002"}}
{"id": "2512.07819", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07819", "abs": "https://arxiv.org/abs/2512.07819", "authors": ["Shubham S. Kumbhar", "Abhijeet M. Kulkarni", "Panagiotis Artemiadis"], "title": "Efficient and Compliant Control Framework for Versatile Human-Humanoid Collaborative Transportation", "comment": null, "summary": "We present a control framework that enables humanoid robots to perform collaborative transportation tasks with a human partner. The framework supports both translational and rotational motions, which are fundamental to co-transport scenarios. It comprises three components: a high-level planner, a low-level controller, and a stiffness modulation mechanism. At the planning level, we introduce the Interaction Linear Inverted Pendulum (I-LIP), which, combined with an admittance model and an MPC formulation, generates dynamically feasible footstep plans. These are executed by a QP-based whole-body controller that accounts for the coupled humanoid-object dynamics. Stiffness modulation regulates robot-object interaction, ensuring convergence to the desired relative configuration defined by the distance between the object and the robot's center of mass. We validate the effectiveness of the framework through real-world experiments conducted on the Digit humanoid platform. To quantify collaboration quality, we propose an efficiency metric that captures both task performance and inter-agent coordination. We show that this metric highlights the role of compliance in collaborative tasks and offers insights into desirable trajectory characteristics across both high- and low-level control layers. Finally, we showcase experimental results on collaborative behaviors, including translation, turning, and combined motions such as semi circular trajectories, representative of naturally occurring co-transportation tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u63a7\u5236\u6846\u67b6\uff0c\u4f7f\u4eba\u5f62\u673a\u5668\u4eba\u80fd\u591f\u4e0e\u4eba\u5408\u4f5c\u8fdb\u884c\u8fd0\u8f93\u4efb\u52a1\uff0c\u652f\u6301\u5e73\u79fb\u548c\u65cb\u8f6c\u52a8\u4f5c\uff0c\u5305\u542b\u89c4\u5212\u3001\u63a7\u5236\u548c\u8c03\u8282\u673a\u5236\u3002", "motivation": "\u5728\u4eba\u4e0e\u673a\u5668\u4eba\u7684\u534f\u4f5c\u8fd0\u8f93\u4efb\u52a1\u4e2d\u63d0\u5347\u6548\u7387\u548c\u534f\u8c03\u6027\u3002", "method": "\u901a\u8fc7\u9ad8\u5c42\u89c4\u5212\uff08\u4f7f\u7528I-LIP\uff09\u3001\u4f4e\u5c42\u63a7\u5236\uff08QP\u57fa\u7840\u7684\u5168\u8eab\u63a7\u5236\u5668\uff09\u548c\u521a\u5ea6\u8c03\u8282\u7b49\u4e09\u5927\u7ec4\u4ef6\uff0c\u5f62\u6210\u63a7\u5236\u6846\u67b6\u3002", "result": "\u901a\u8fc7\u771f\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u51fa\u6548\u7387\u6307\u6807\u91cf\u5316\u534f\u4f5c\u8d28\u91cf\u3002", "conclusion": "\u6846\u67b6\u5c55\u793a\u4e86\u5408\u9002\u7684\u521a\u5ea6\u5bf9\u534f\u4f5c\u4efb\u52a1\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5728\u591a\u79cd\u8fd0\u52a8\u573a\u666f\u4e2d\u8bc1\u660e\u4e86\u5176\u5b9e\u7528\u6027\u3002"}}
