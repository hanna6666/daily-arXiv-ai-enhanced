{"id": "2511.17507", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17507", "abs": "https://arxiv.org/abs/2511.17507", "authors": ["Arnaud Zeller", "Emmanuelle Chevry Pebayle"], "title": "The use of artificial intelligence in music creation: between interface and appropriation", "comment": "in French language", "summary": "By observing the activities and relationships of musicians and sound designers to the activities of creation, performance, publishing and dissemination with artificial intelligence (AI), from two specialized forums between 2022 and 2024, this article proposes a lexicometric analysis of the representations linked to their use. Indeed, the machine, now equipped with artificial intelligences requiring new appropriations and enabling new mediations, constitutes new challenges for artists. To study these confrontations and new mediations, our approach mobilizes the theoretical framework of the Human-AI Musicking Framework, based on a lexicometric analysis of content. The aim is to clarify the present and future uses of AI from the interfaces, in the creation of sound and musical content, and to identify the obstacles, obstacles, brakes and limits to appropriation ``in the fact of making the content one's own and integrating it as a part of oneself'' (Bachimont and Crozat, 2004) in the context of a collaboration between musician and machine.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u4eba\u5de5\u667a\u80fd\u5bf9\u97f3\u4e50\u521b\u4f5c\u548c\u8868\u6f14\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u8bcd\u5178\u8ba1\u91cf\u5206\u6790\u7684\u65b9\u6cd5\uff0c\u4ee5\u7406\u89e3\u97f3\u4e50\u5bb6\u4e0eAI\u7684\u5173\u7cfb\u53ca\u5176\u9762\u4e34\u7684\u65b0\u6311\u6218\u3002", "motivation": "\u63a2\u8ba8\u4eba\u5de5\u667a\u80fd\u5982\u4f55\u6539\u53d8\u97f3\u4e50\u521b\u4f5c\u3001\u8868\u6f14\u548c\u4f20\u64ad\u7684\u65b9\u5f0f\uff0c\u4ee5\u53ca\u5b83\u7ed9\u827a\u672f\u5bb6\u5e26\u6765\u7684\u65b0\u6311\u6218\u3002", "method": "\u8fd0\u7528\u4eba\u673a\u4ea4\u4e92\u97f3\u4e50\u6846\u67b6\uff0c\u7ed3\u5408\u8bcd\u5178\u8ba1\u91cf\u5206\u6790\u5bf92022\u81f32024\u5e74\u95f4\u4e24\u4e2a\u4e13\u4e1a\u8bba\u575b\u7684\u5185\u5bb9\u8fdb\u884c\u7814\u7a76\u3002", "result": "\u901a\u8fc7\u5206\u6790\uff0c\u8bc6\u522b\u51fa\u5728\u97f3\u4e50\u521b\u4f5c\u4e0eAI\u534f\u4f5c\u4e2d\u5b58\u5728\u7684\u969c\u788d\u4e0e\u9650\u5ea6\uff0c\u4e3a\u672a\u6765\u7684\u5b9e\u8df5\u63d0\u4f9b\u4e86\u53c2\u8003\u3002", "conclusion": "\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5bf9\u97f3\u4e50\u5bb6\u4e0e\u4eba\u5de5\u667a\u80fd\u4e4b\u95f4\u4e92\u52a8\u7684\u7814\u7a76\uff0c\u5398\u6e05AI\u5728\u97f3\u4e50\u521b\u4f5c\u4e2d\u7684\u672a\u6765\u5e94\u7528\uff0c\u5e76\u63ed\u793a\u5408\u4f5c\u8fc7\u7a0b\u4e2d\u7684\u5404\u79cd\u969c\u788d\u548c\u6311\u6218\u3002"}}
{"id": "2511.17508", "categories": ["cs.HC", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17508", "abs": "https://arxiv.org/abs/2511.17508", "authors": ["Alice Smith", "Bob Johnson", "Xiaoyu Zhu", "Carol Lee"], "title": "Deep Learning-based Lightweight RGB Object Tracking for Augmented Reality Devices", "comment": null, "summary": "Augmented Reality (AR) applications often require robust real-time tracking of objects in the user's environment to correctly overlay virtual content. Recent advances in computer vision have produced highly accurate deep learning-based object trackers, but these models are typically too heavy in computation and memory for wearable AR devices. In this paper, we present a lightweight RGB object tracking algorithm designed specifically for resource-constrained AR platforms. The proposed tracker employs a compact Siamese neural network architecture and incorporates optimization techniques such as model pruning, quantization, and knowledge distillation to drastically reduce model size and inference cost while maintaining high tracking accuracy. We train the tracker offline on large video datasets using deep convolutional neural networks and then deploy it on-device for real-time tracking. Experimental results on standard tracking benchmarks show that our approach achieves comparable accuracy to state-of-the-art trackers, yet runs in real-time on a mobile AR headset at around 30 FPS -- more than an order of magnitude faster than prior high-performance trackers on the same hardware. This work enables practical, robust object tracking for AR use-cases, opening the door to more interactive and dynamic AR experiences on lightweight devices.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684RGB\u7269\u4f53\u8ddf\u8e2a\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684AR\u8bbe\u5907\uff0c\u80fd\u591f\u5b9e\u73b0\u5b9e\u65f6\u8ddf\u8e2a\u5e76\u4fdd\u6301\u9ad8\u7cbe\u5ea6\uff0c\u5f00\u542f\u4e86\u5728\u8f7b\u91cf\u8bbe\u5907\u4e0a\u4ea4\u4e92\u6027\u548c\u52a8\u6001\u6027\u7684AR\u4f53\u9a8c\u3002", "motivation": "\u589e\u5f3a\u73b0\u5b9e\u5e94\u7528\u9700\u8981\u5b9e\u65f6\u3001\u53ef\u9760\u7684\u7269\u4f53\u8ddf\u8e2a\uff0c\u4f46\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u901a\u5e38\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u592a\u5927\uff0c\u96be\u4ee5\u5728\u53ef\u7a7f\u6234AR\u8bbe\u5907\u4e0a\u5b9e\u73b0\u3002", "method": "\u91c7\u7528\u7d27\u51d1\u7684\u5b6a\u751f\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5e76\u901a\u8fc7\u6a21\u578b\u526a\u679d\u3001\u91cf\u5316\u548c\u77e5\u8bc6\u84b8\u998f\u7b49\u4f18\u5316\u6280\u672f\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684RGB\u7269\u4f53\u8ddf\u8e2a\u7b97\u6cd5\uff0c\u4e13\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u5e73\u53f0\u8bbe\u8ba1\u3002\u8be5\u7b97\u6cd5\u4f7f\u7528\u7d27\u51d1\u7684\u5b6a\u751f\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5e76\u7ed3\u5408\u6a21\u578b\u526a\u679d\u3001\u91cf\u5316\u548c\u77e5\u8bc6\u84b8\u998f\u7b49\u4f18\u5316\u6280\u672f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6a21\u578b\u5927\u5c0f\u548c\u63a8\u7406\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8ddf\u8e2a\u7cbe\u5ea6\u3002\u5728\u6807\u51c6\u8ddf\u8e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u79fb\u52a8AR\u5934\u663e\u4e0a\u4ee5\u7ea630\u5e27\u6bcf\u79d2\u7684\u901f\u5ea6\u8fd0\u884c\uff0c\u901f\u5ea6\u662f\u4ee5\u524d\u9ad8\u6027\u80fd\u8ddf\u8e2a\u5668\u7684\u4e00\u4e2a\u6570\u91cf\u7ea7\u4ee5\u4e0a\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5b9e\u73b0\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u7684AR\u8bbe\u5907\u4e0a\u8fdb\u884c\u9ad8\u6548\u3001\u5b9e\u65f6\u7684\u7269\u4f53\u8ddf\u8e2a\uff0c\u4e3a\u589e\u5f3a\u73b0\u5b9e\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u5e7f\u9614\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2511.17509", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17509", "abs": "https://arxiv.org/abs/2511.17509", "authors": ["Federico Maria Cau", "Lucio Davide Spano"], "title": "Beyond Awareness: Investigating How AI and Psychological Factors Shape Human Self-Confidence Calibration", "comment": null, "summary": "Human-AI collaboration outcomes depend strongly on human self-confidence calibration, which drives reliance or resistance toward AI's suggestions. This work presents two studies examining whether calibration of self-confidence before decision tasks, low versus high levels of Need for Cognition (NFC), and Actively Open-Minded Thinking (AOT), leads to differences in decision accuracy, self-confidence appropriateness during the tasks, and metacognitive perceptions (global and affective). The first study presents strategies to identify well-calibrated users, also comparing decision accuracy and the appropriateness of self-confidence across NFC and AOT levels. The second study investigates the effects of calibrated self-confidence in AI-assisted decision-making (no AI, two-stage AI, and personalized AI), also considering different NFC and AOT levels. Our results show the importance of human self-confidence calibration and psychological traits when designing AI-assisted decision systems. We further propose design recommendations to address the challenge of calibrating self-confidence and supporting tailored, user-centric AI that accounts for individual traits.", "AI": {"tldr": "\u4eba\u7c7b\u4e0eAI\u534f\u4f5c\u7684\u7ed3\u679c\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u4eba\u7c7b\u7684\u81ea\u4fe1\u5fc3\u6821\u51c6\uff0c\u672c\u6587\u901a\u8fc7\u4e24\u9879\u7814\u7a76\u63a2\u8ba8\u81ea\u4fe1\u5fc3\u6821\u51c6\u3001\u8ba4\u77e5\u9700\u6c42\u548c\u79ef\u6781\u5f00\u653e\u601d\u7ef4\u5bf9\u51b3\u7b56\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u81ea\u4fe1\u5fc3\u7684\u9002\u5f53\u6821\u51c6\u5bf9\u4e8e\u6709\u6548\u7684AI\u8f85\u52a9\u51b3\u7b56\u7cfb\u7edf\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u63a2\u8ba8\u81ea\u4fe1\u5fc3\u6821\u51c6\u5982\u4f55\u5f71\u54cd\u4eba\u7c7b\u5bf9AI\u5efa\u8bae\u7684\u4f9d\u8d56\u7a0b\u5ea6\uff0c\u4ee5\u4f18\u5316\u4eba\u673a\u534f\u4f5c\u7ed3\u679c\u3002", "method": "\u901a\u8fc7\u4e24\u9879\u7814\u7a76\u8bc4\u4f30\u81ea\u4fe1\u5fc3\u6821\u51c6\u3001\u8ba4\u77e5\u9700\u6c42\u548c\u79ef\u6781\u5f00\u653e\u601d\u7ef4\u5bf9\u51b3\u7b56\u51c6\u786e\u6027\u548c\u81ea\u4fe1\u5fc3\u9002\u5f53\u6027\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u663e\u793a\uff0c\u81ea\u4fe1\u5fc3\u6821\u51c6\u548c\u5fc3\u7406\u7279\u5f81\u663e\u8457\u5f71\u54cd\u51b3\u7b56\u51c6\u786e\u6027\u548c\u81ea\u4fe1\u5fc3\u9002\u5f53\u6027\uff0c\u4ece\u800c\u5f71\u54cdAI\u8f85\u52a9\u51b3\u7b56\u7684\u6548\u679c\u3002", "conclusion": "\u81ea\u4fe1\u5fc3\u7684\u6821\u51c6\u548c\u5fc3\u7406\u7279\u5f81\u5728\u8bbe\u8ba1AI\u8f85\u52a9\u51b3\u7b56\u7cfb\u7edf\u65f6\u975e\u5e38\u91cd\u8981\uff0c\u9700\u9488\u5bf9\u4e2a\u4f53\u7279\u5f81\u63d0\u51fa\u8bbe\u8ba1\u5efa\u8bae\u3002"}}
{"id": "2511.17511", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17511", "abs": "https://arxiv.org/abs/2511.17511", "authors": ["Bingkun Guo", "Wentian Li", "Xiaojian Liu", "Jiaqi Luo", "Zibin Yu", "Dalong Dong", "Shuyou Zhang", "Yiming Zhang"], "title": "A Multidisciplinary Design and Optimization (MDO) Agent Driven by Large Language Models", "comment": null, "summary": "To accelerate mechanical design and enhance design quality and innovation, we present a Multidisciplinary Design and Optimization (MDO) Agent driven by Large Language Models (LLMs). The agent semi-automates the end-to-end workflow by orchestrating three core capabilities: (i) natural-language-driven parametric modeling, (ii) retrieval-augmented generation (RAG) for knowledge-grounded conceptualization, and (iii) intelligent orchestration of engineering software for performance verification and optimization. Working in tandem, these capabilities interpret high-level, unstructured intent, translate it into structured design representations, automatically construct parametric 3D CAD models, generate reliable concept variants using external knowledge bases, and conduct evaluation with iterative optimization via tool calls such as finite-element analysis (FEA). Validation on three representative cases - a gas-turbine blade, a machine-tool column, and a fractal heat sink - shows that the agent completes the pipeline from natural-language intent to verified and optimized designs with reduced manual scripting and setup effort, while promoting innovative design exploration. This work points to a practical path toward human-AI collaborative mechanical engineering and lays a foundation for more dependable, vertically customized MDO systems.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u5b66\u79d1\u8bbe\u8ba1\u4e0e\u4f18\u5316\u4ee3\u7406\uff0c\u5b9e\u73b0\u4e86\u4ece\u81ea\u7136\u8bed\u8a00\u610f\u56fe\u5230\u7ecf\u8fc7\u9a8c\u8bc1\u548c\u4f18\u5316\u8bbe\u8ba1\u7684\u81ea\u52a8\u5316\u6d41\u7a0b\u3002", "motivation": "\u52a0\u901f\u673a\u68b0\u8bbe\u8ba1\uff0c\u63d0\u9ad8\u8bbe\u8ba1\u8d28\u91cf\u548c\u521b\u65b0\u80fd\u529b", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7531\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u591a\u5b66\u79d1\u8bbe\u8ba1\u4e0e\u4f18\u5316\u4ee3\u7406", "result": "\u901a\u8fc7\u4e09\u9879\u6838\u5fc3\u80fd\u529b\u5b9e\u73b0\u7aef\u5230\u7aef\u5de5\u4f5c\u6d41\u7a0b\u7684\u534a\u81ea\u52a8\u5316\uff0c\u89e3\u51b3\u5b9e\u9645\u6848\u4f8b", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u4eba\u673a\u534f\u4f5c\u7684\u673a\u68b0\u5de5\u7a0b\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u8df5\u8def\u5f84\uff0c\u4e3a\u672a\u6765\u66f4\u53ef\u9760\u7684\u591a\u5b66\u79d1\u8bbe\u8ba1\u4e0e\u4f18\u5316\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.17540", "categories": ["cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.17540", "abs": "https://arxiv.org/abs/2511.17540", "authors": ["Ryudai Iwakami", "Bo Peng", "Hiroyuki Hanyu", "Tasuku Ishigooka", "Takuya Azumi"], "title": "AUTOSAR AP and ROS 2 Collaboration Framework", "comment": "9 pages. This version includes minor \\lstlisting configuration adjustments for successful compilation. The page count is now nine pages due to the addition of author information. There are no other significant changes to the content or layout. Originally published at Euromicro Conference DSD 2024", "summary": "The field of autonomous vehicle research is advancing rapidly, necessitating platforms that meet real-time performance, safety, and security requirements for practical deployment. AUTOSAR Adaptive Platform (AUTOSAR AP) is widely adopted in development to meet these criteria; however, licensing constraints and tool implementation challenges limit its use in research. Conversely, Robot Operating System 2 (ROS 2) is predominantly used in research within the autonomous driving domain, leading to a disparity between research and development platforms that hinders swift commercialization. This paper proposes a collaboration framework that enables AUTOSAR AP and ROS 2 to communicate with each other using a Data Distribution Service for Real-Time Systems (DDS). In contrast, AUTOSAR AP uses Scalable service-Oriented Middleware over IP (SOME/IP) for communication. The proposed framework bridges these protocol differences, ensuring seamless interaction between the two platforms. We validate the functionality and performance of our bridge converter through empirical analysis, demonstrating its efficiency in conversion time and ease of integration with ROS 2 tools. Furthermore, the availability of the proposed collaboration framework is improved by automatically generating a configuration file for the proposed bridge converter.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5408\u4f5c\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3AUTOSAR AP\u548cROS 2\u4e4b\u95f4\u7684\u901a\u4fe1\u5dee\u5f02\uff0c\u4fc3\u8fdb\u8fd9\u4e24\u4e2a\u5e73\u53f0\u7684\u4e92\u64cd\u4f5c\u6027\uff0c\u9a8c\u8bc1\u4e86\u6865\u63a5\u8f6c\u6362\u5668\u7684\u529f\u80fd\u548c\u6027\u80fd\uff0c\u63d0\u5347\u4e86\u81ea\u52a8\u751f\u6210\u914d\u7f6e\u6587\u4ef6\u7684\u53ef\u7528\u6027\u3002", "motivation": "\u5728\u81ea\u4e3b\u9a7e\u9a76\u9886\u57df\uff0cAUTOSAR AP\u548cROS 2\u4e4b\u95f4\u5b58\u5728\u7814\u7a76\u4e0e\u5f00\u53d1\u5e73\u53f0\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u9650\u5236\u4e86\u5546\u4e1a\u5316\u901f\u5ea6\uff0c\u8feb\u5207\u9700\u8981\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\u4ee5\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u9a8c\u8bc1\u4e86\u6865\u63a5\u8f6c\u6362\u5668\u7684\u529f\u80fd\u548c\u6027\u80fd\uff0c\u8bc4\u4f30\u4e86\u8f6c\u6362\u65f6\u95f4\u6548\u7387\u548c\u4e0eROS 2\u5de5\u5177\u7684\u96c6\u6210\u4fbf\u5229\u6027\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86AUTOSAR AP\u4e0eROS 2\u4e4b\u95f4\u7684\u65e0\u7f1d\u901a\u4fe1\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u914d\u7f6e\u6587\u4ef6\u63d0\u9ad8\u4e86\u5408\u4f5c\u6846\u67b6\u7684\u53ef\u7528\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u5408\u4f5c\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86AUTOSAR AP\u548cROS 2\u4e4b\u95f4\u7684\u901a\u4fe1\u969c\u788d\uff0c\u4fc3\u8fdb\u4e86\u8fd9\u4e24\u4e2a\u5e73\u53f0\u7684\u4e92\u64cd\u4f5c\u6027\u3002"}}
{"id": "2511.17512", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.17512", "abs": "https://arxiv.org/abs/2511.17512", "authors": ["Gloria Xiaodan Zhang", "Yijia Wang", "Taro Leo Nakajima", "Katie Seaborn"], "title": "First Contact with Dark Patterns and Deceptive Designs in Chinese and Japanese Free-to-Play Mobile Games", "comment": "CHI PLAY '25", "summary": "Mobile games have gained immense popularity due to their accessibility, allowing people to play anywhere, anytime. Dark patterns and deceptive designs (DPs) have been found in these and other gaming platforms within certain cultural contexts. Here, we explored DPs in the onboarding experiences of free-to-play mobile games from China and Japan. We identified several unique patterns and mapped their relative prevalence. We also found that game developers often employ combinations of DPs as a strategy (\"DP Combos\") and use elements that, while not inherently manipulative, can enhance the impact of known patterns (\"DP Enhancers\"). Guided by these findings, we then developed an enriched ontology for categorizing deceptive game design patterns into classes and subclasses. This research contributes to understanding deceptive game design patterns and offers insights for future studies on cultural dimensions and ethical game design in general.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u4e2d\u56fd\u548c\u65e5\u672c\u514d\u8d39\u79fb\u52a8\u6e38\u620f\u4e2d\u7684\u6b3a\u9a97\u6027\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u8fd9\u4e9b\u6a21\u5f0f\u7684\u6587\u5316\u80cc\u666f\u548c\u4f26\u7406\u8bbe\u8ba1\u7684\u5f71\u54cd\u3002", "motivation": "\u79fb\u52a8\u6e38\u620f\u7684\u666e\u53ca\u548c\u6587\u5316\u80cc\u666f\u4e0b\u7684\u9ed1\u6697\u8bbe\u8ba1\u6a21\u5f0f\u201d\u7b49\u95ee\u9898\u7684\u63a2\u7d22\u4e0e\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e2d\u56fd\u548c\u65e5\u672c\u7684\u79fb\u52a8\u6e38\u620f\u5728\u5f15\u5bfc\u4f53\u9a8c\u4e2d\u7684\u9ed1\u6697\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u8bc6\u522b\u51fa\u591a\u79cd\u72ec\u7279\u7684\u6a21\u5f0f\u53ca\u5176\u6d41\u884c\u7a0b\u5ea6\uff0c\u5e76\u5efa\u7acb\u4e86\u5206\u7c7b\u672c\u4f53\u3002", "result": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4e2d\u56fd\u548c\u65e5\u672c\u514d\u8d39\u79fb\u52a8\u6e38\u620f\u4e2d\u7684\u9ed1\u6697\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u53d1\u73b0\u4e86\u51e0\u79cd\u72ec\u7279\u7684\u8bbe\u8ba1\u6a21\u5f0f\u53ca\u5176\u76f8\u5bf9\u6d41\u884c\u7a0b\u5ea6\uff0c\u63d0\u51fa\u4e86\u5f00\u53d1\u8005\u5e38\u4f7f\u7528\u7684\u201cDP\u7ec4\u5408\u201d\u7b56\u7565\u4ee5\u53ca\u589e\u5f3a\u73b0\u6709\u6a21\u5f0f\u5f71\u54cd\u529b\u7684\u5143\u7d20\uff0c\u6700\u7ec8\u6784\u5efa\u4e86\u4e30\u5bcc\u7684\u672c\u4f53\u4ee5\u5206\u7c7b\u8fd9\u4e9b\u6b3a\u9a97\u6027\u8bbe\u8ba1\u6a21\u5f0f\u3002", "conclusion": "\u672c\u7814\u7a76\u5e2e\u52a9\u7406\u89e3\u6b3a\u9a97\u6027\u6e38\u620f\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u5e76\u4e3a\u672a\u6765\u6587\u5316\u7ef4\u5ea6\u548c\u4f26\u7406\u6e38\u620f\u8bbe\u8ba1\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2511.17578", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17578", "abs": "https://arxiv.org/abs/2511.17578", "authors": ["Neelotpal Dutta", "Tianyu Zhang", "Tao Liu", "Yongxue Chen", "Charlie C. L. Wang"], "title": "Implicit Neural Field-Based Process Planning for Multi-Axis Manufacturing: Direct Control over Collision Avoidance and Toolpath Geometry", "comment": null, "summary": "Existing curved-layer-based process planning methods for multi-axis manufacturing address collisions only indirectly and generate toolpaths in a post-processing step, leaving toolpath geometry uncontrolled during optimization. We present an implicit neural field-based framework for multi-axis process planning that overcomes these limitations by embedding both layer generation and toolpath design within a single differentiable pipeline. Using sinusoidally activated neural networks to represent layers and toolpaths as implicit fields, our method enables direct evaluation of field values and derivatives at any spatial point, thereby allowing explicit collision avoidance and joint optimization of manufacturing layers and toolpaths. We further investigate how network hyperparameters and objective definitions influence singularity behavior and topology transitions, offering built-in mechanisms for regularization and stability control. The proposed approach is demonstrated on examples in both additive and subtractive manufacturing, validating its generality and effectiveness.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u9690\u5f0f\u795e\u7ecf\u573a\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5c42\u751f\u6210\u548c\u5de5\u5177\u8def\u5f84\u8bbe\u8ba1\u7ed3\u5408\uff0c\u4f18\u5316\u591a\u8f74\u5236\u9020\u8fc7\u7a0b\uff0c\u76f4\u63a5\u5904\u7406\u78b0\u649e\u5e76\u6539\u8fdb\u5de5\u5177\u8def\u5f84\u51e0\u4f55\u8bbe\u8ba1\u3002", "motivation": "\u73b0\u6709\u7684\u5f2f\u66f2\u5c42\u57fa\u4e8e\u8fc7\u7a0b\u89c4\u5212\u65b9\u6cd5\u5728\u591a\u8f74\u5236\u9020\u4e2d\u4ec5\u95f4\u63a5\u5904\u7406\u78b0\u649e\uff0c\u5e76\u4e14\u5728\u540e\u5904\u7406\u6b65\u9aa4\u4e2d\u751f\u6210\u5de5\u5177\u8def\u5f84\uff0c\u5bfc\u81f4\u4f18\u5316\u8fc7\u7a0b\u4e2d\u7684\u5de5\u5177\u8def\u5f84\u51e0\u4f55\u5f62\u72b6\u65e0\u6cd5\u63a7\u5236\u3002", "method": "\u4f7f\u7528\u6b63\u5f26\u6fc0\u6d3b\u795e\u7ecf\u7f51\u7edc\u5c06\u5c42\u548c\u5de5\u5177\u8def\u5f84\u8868\u793a\u4e3a\u9690\u5f0f\u573a\uff0c\u652f\u6301\u5728\u4efb\u610f\u7a7a\u95f4\u70b9\u76f4\u63a5\u8bc4\u4f30\u573a\u503c\u548c\u5bfc\u6570\uff0c\u4ece\u800c\u5b9e\u73b0\u663e\u5f0f\u78b0\u649e\u907f\u514d\u548c\u5c42\u4e0e\u5de5\u5177\u8def\u5f84\u7684\u8054\u5408\u4f18\u5316\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u573a\u7684\u591a\u8f74\u8fc7\u7a0b\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5c42\u751f\u6210\u548c\u5de5\u5177\u8def\u5f84\u8bbe\u8ba1\u5d4c\u5165\u4e00\u4e2a\u53ef\u5fae\u5206\u7684\u7ba1\u9053\u4e2d\uff0c\u514b\u670d\u4e86\u8fd9\u4e9b\u9650\u5236\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5728\u589e\u6750\u548c\u51cf\u6750\u5236\u9020\u4e0a\u5f97\u5230\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u5176\u901a\u7528\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2511.17513", "categories": ["cs.HC", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.17513", "abs": "https://arxiv.org/abs/2511.17513", "authors": ["Omer Eldadi", "Yarin Dekimhi", "Gershon Tenenbaum"], "title": "Motivational Climate Effects on Communications, Emotional-Social States, and Performance in Collaborative Gaming Environment", "comment": "25 pages, 5 figures", "summary": "The study explores the effects of motivational climate on communication features, emotional states, collective efficacy, and performance in collaborative gaming environments. Forty participants with no prior gaming experience were randomly assigned to 20 gender-matched teams of three (including one confederate) across two motivational climates: positive-supportive (PS) or neutral-unsupported (NU) (10 teams per condition). Team members completed three progressively difficult levels of Overcooked! 2 during which communication contents, emotional responses, collective efficacy, and performance outcomes were observed and coded. Mixed-design MANOVAs and ANOVAs were employed to examine the effects of motivational climate and task difficulty on communication patterns, emotions, collective efficacy, and performance. Chi-square analyses were performed to test communication content differences between conditions. Results revealed that PS team members significantly outperformed NU teams at lower task difficulty level, but this advantage diminished as task complexity increased. Communication analysis revealed that PS team members utilized significantly more action-oriented, factual, and emotional/motivational statements, while NU team members used more statements of uncertainty and non-task-related communication. The percentage of the talk time increased with difficulty across both climate conditions. PS team members maintained more positive emotional profiles throughout, with higher excitement and happiness scores and lower anxiety, dejection, and anger compared to NU team members. Furthermore, PS team members reported consistently higher collective efficacy beliefs across all difficulty levels. These findings reveal that positive motivational climate enhances team communication effectiveness, emotional resilience, and performance outcomes in challenging collaborative environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u79ef\u6781\u7684\u6fc0\u52b1\u6c14\u5019\u80fd\u63d0\u5347\u56e2\u961f\u6c9f\u901a\u6548\u679c\u3001\u60c5\u7eea\u97e7\u6027\u548c\u8868\u73b0\uff0c\u5728\u5408\u4f5c\u6311\u6218\u4e2d\u53d1\u6325\u91cd\u8981\u4f5c\u7528\u3002", "motivation": "\u63a2\u8ba8\u6fc0\u52b1\u6c14\u5019\u5bf9\u5408\u4f5c\u6e38\u620f\u73af\u5883\u4e2d\u6c9f\u901a\u7279\u5f81\u3001\u60c5\u7eea\u72b6\u6001\u3001\u96c6\u4f53\u6548\u80fd\u548c\u8868\u73b0\u7684\u5f71\u54cd\u3002", "method": "40\u540d\u65e0\u5148\u524d\u6e38\u620f\u7ecf\u9a8c\u7684\u53c2\u4e0e\u8005\u88ab\u968f\u673a\u5206\u4e3a20\u4e2a\u6027\u522b\u5339\u914d\u7684\u56e2\u961f\uff0c\u5206\u522b\u5728\u79ef\u6781\u652f\u6301\u6027\u548c\u4e2d\u6027\u4e0d\u652f\u6301\u6027\u4e24\u79cd\u6fc0\u52b1\u6c14\u5019\u4e0b\u5b8c\u6210Overcooked! 2\u7684\u4e09\u79cd\u9010\u6e10\u56f0\u96be\u7684\u4efb\u52a1\uff0c\u4f7f\u7528\u591a\u5143\u65b9\u5dee\u5206\u6790\u548c\u5361\u65b9\u68c0\u9a8c\u7b49\u65b9\u6cd5\u5206\u6790\u6570\u636e\u3002", "result": "\u53d1\u73b0\u79ef\u6781\u652f\u6301\u6027\u56e2\u961f\u5728\u8f83\u4f4e\u4efb\u52a1\u96be\u5ea6\u4e0b\u7684\u8868\u73b0\u4f18\u4e8e\u4e2d\u6027\u4e0d\u652f\u6301\u56e2\u961f\uff0c\u4f46\u968f\u7740\u4efb\u52a1\u590d\u6742\u6027\u7684\u589e\u52a0\uff0c\u8fd9\u4e00\u4f18\u52bf\u51cf\u5f31\u3002", "conclusion": "\u79ef\u6781\u7684\u6fc0\u52b1\u6c14\u5019\u80fd\u591f\u589e\u5f3a\u56e2\u961f\u7684\u6c9f\u901a\u6548\u7387\u3001\u60c5\u7eea\u9002\u5e94\u80fd\u529b\u548c\u5728\u6311\u6218\u6027\u5408\u4f5c\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2511.17603", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.17603", "abs": "https://arxiv.org/abs/2511.17603", "authors": ["Chelsea-Xi Chen", "Zhe Zhang", "Aven-Le Zhou"], "title": "Translating Cultural Choreography from Humanoid Forms to Robotic Arm", "comment": null, "summary": "Robotic arm choreography often reproduces trajectories while missing cultural semantics. This study examines whether symbolic posture transfer with joint space compatible notation can preserve semantic fidelity on a six-degree-of-freedom arm and remain portable across morphologies. We implement ROPERA, a three-stage pipeline for encoding culturally codified postures, composing symbolic sequences, and decoding to servo commands. A scene from Kunqu opera, \\textit{The Peony Pavilion}, serves as the material for evaluation. The procedure includes corpus-based posture selection, symbolic scoring, direct joint angle execution, and a visual layer with light painting and costume-informed colors. Results indicate reproducible execution with intended timing and cultural legibility reported by experts and audiences. The study points to non-anthropocentric cultural preservation and portable authoring workflows. Future work will design dance-informed transition profiles, extend the notation to locomotion with haptic, musical, and spatial cues, and test portability across platforms.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u673a\u5668\u4eba\u624b\u81c2\u7f16\u6392\u4e2d\uff0c\u901a\u8fc7\u8c61\u5f81\u6027\u59ff\u52bf\u8f6c\u79fb\u4fdd\u6301\u6587\u5316\u8bed\u4e49\u7684\u6548\u679c\u3002", "motivation": "\u7814\u7a76\u673a\u5668\u4eba\u624b\u81c2\u7f16\u6392\u4e2d\u7684\u6587\u5316\u8bed\u4e49\u4fdd\u7559\u95ee\u9898\uff0c\u63a2\u7d22\u8c61\u5f81\u6027\u59ff\u52bf\u8f6c\u79fb\u7684\u6709\u6548\u6027\u3002", "method": "ROPERA\u4e09\u9636\u6bb5\u6d41\u7a0b\uff0c\u5305\u62ec\u7f16\u7801\u6587\u5316\u7279\u5b9a\u59ff\u52bf\u3001\u7ec4\u5408\u7b26\u53f7\u5e8f\u5217\u548c\u89e3\u7801\u4e3a\u4f3a\u670d\u547d\u4ee4\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u53ef\u91cd\u590d\u7684\u6267\u884c\uff0c\u5f97\u5230\u4e13\u5bb6\u548c\u89c2\u4f17\u5bf9\u65f6\u673a\u548c\u6587\u5316\u53ef\u8bfb\u6027\u7684\u79ef\u6781\u53cd\u9988\u3002", "conclusion": "\u672c\u7814\u7a76\u6307\u5411\u975e\u4eba\u7c7b\u4e2d\u5fc3\u7684\u6587\u5316\u4fdd\u62a4\u548c\u53ef\u79fb\u690d\u7684\u521b\u4f5c\u5de5\u4f5c\u6d41\u7a0b\uff0c\u672a\u6765\u5c06\u6539\u8fdb\u8fc7\u6e21\u7279\u5f81\u5e76\u6269\u5927\u7b26\u53f7\u8bb0\u8c31\u7684\u5e94\u7528\u3002"}}
{"id": "2511.17515", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17515", "abs": "https://arxiv.org/abs/2511.17515", "authors": ["Mahmoud Elkhodr", "Ergun Gide"], "title": "Embedding Generative AI into Systems Analysis and Design Curriculum: Framework, Case Study, and Cross-Campus Empirical Evidence", "comment": "~12,000 words; 4 figures; 6 tables; multi-site study (across 4 Australian campuses)", "summary": "Systems analysis students increasingly use Generative AI, yet current pedagogy lacks systematic approaches for teaching responsible AI orchestration that fosters critical thinking whilst meeting educational outcomes. Students risk accepting AI suggestions blindly or uncritically without assessing alignment with user needs or contextual appropriateness. SAGE (Structured AI-Guided Education) addresses this gap by embedding GenAI into curriculum design, training students when to accept, modify, or reject AI contributions. Implementation with 18 student groups across four Australian universities revealed how orchestration skills develop. Most groups (84\\%) moved beyond passive acceptance, showing selective judgment, yet none proactively identified gaps overlooked by both human and AI analysis, indicating a competency ceiling. Students strong at explaining decisions also performed well at integrating sources, and those with deep domain understanding consistently considered accessibility considerations. Accessibility awareness proved fragile. When writing requirements, 85\\% of groups explicitly considered elderly users and cultural needs. Notably, 55\\% of groups struggled identifying when AI misclassified system boundaries (what belongs inside versus outside the system), 45\\% missed data management errors (how information is stored and updated), and 55\\% overlooked missing exception handling. Three implications emerge for educators: (i) require students to document why they accepted, modified, or rejected each AI suggestion, making reasoning explicit; (ii) embed accessibility prompts at each development stage because awareness collapses without continuous scaffolding; and (iii) have students create their own specifications before using AI, then compare versions, and anchor to research or standards to identify gaps.", "AI": {"tldr": "SAGE\u8bfe\u7a0b\u901a\u8fc7\u7cfb\u7edf\u6027\u63d2\u5165\u751f\u6210AI\u4e8e\u6559\u80b2\u4e2d\uff0c\u63d0\u5347\u5b66\u751f\u5bf9AI\u5efa\u8bae\u7684\u6279\u5224\u6027\u5206\u6790\u548c\u51b3\u7b56\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u6559\u5b66\u6cd5\u7f3a\u4e4f\u7cfb\u7edf\u6027\uff0c\u5bfc\u81f4\u5b66\u751f\u5728\u4f7f\u7528\u751f\u6210AI\u65f6\u672a\u80fd\u6279\u5224\u6027\u5730\u8bc4\u4f30\u5176\u5efa\u8bae\u3002", "method": "SAGE (Structured AI-Guided Education)\u8bfe\u7a0b\u8bbe\u8ba1", "result": "\u5b9e\u65bd\u540e\uff0c\u5927\u591a\u6570\u5b66\u751f\uff0884%\uff09\u5c55\u73b0\u51fa\u9009\u62e9\u6027\u5224\u65ad\u80fd\u529b\uff0c\u4f46\u7686\u672a\u80fd\u4e3b\u52a8\u8bc6\u522bAI\u548c\u4eba\u7c7b\u5206\u6790\u7684\u76f2\u70b9\u3002", "conclusion": "\u6559\u80b2\u8005\u5e94\u5f15\u5bfc\u5b66\u751f\u8bb0\u5f55AI\u5efa\u8bae\u7684\u63a5\u53d7\u3001\u4fee\u6539\u6216\u62d2\u7edd\u539f\u56e0\uff0c\u5e76\u91cd\u89c6\u65e0\u969c\u788d\u8bbe\u8ba1\uff0c\u786e\u4fdd\u6279\u5224\u6027\u5206\u6790\u80fd\u529b\u7684\u63d0\u5347\u3002"}}
{"id": "2511.17608", "categories": ["cs.RO", "physics.optics"], "pdf": "https://arxiv.org/pdf/2511.17608", "abs": "https://arxiv.org/abs/2511.17608", "authors": ["Yunlong Guo", "John Canning", "Zenon Chaczko", "Gang-Ding Peng"], "title": "Robot joint characterisation and control using a magneto-optical rotary encoder", "comment": null, "summary": "A robust and compact magneto-optical rotary encoder for the characterisation of robotic rotary joints is demonstrated. The system employs magnetic field-induced optical attenuation in a double-pass configuration using rotating nonuniform magnets around an optical circulator operating in reflection. The encoder tracks continuous 360\u00b0 rotation with rotation sweep rates from \u03bd = 135 \u00b0/s to \u03bd = 370 \u00b0/s, and an angular resolution of \u0394\u03b8 = 0.3\u00b0. This offers a low-cost and reliable alternative to conventional robot rotation encoders while maintaining competitive performance.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u78c1\u5149\u65cb\u8f6c\u7f16\u7801\u5668\uff0c\u9002\u7528\u4e8e\u673a\u68b0\u624b\u81c2\u7684\u9ad8\u6548\u548c\u53ef\u9760\u7684\u4f4d\u7f6e\u8ffd\u8e2a\u3002", "motivation": "\u4e3a\u673a\u68b0\u624b\u81c2\u7684\u65cb\u8f6c\u5173\u8282\u63d0\u4f9b\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u53ef\u9760\u7684\u7f16\u7801\u5668\u89e3\u51b3\u65b9\u6848", "method": "\u4f7f\u7528\u78c1\u573a\u8bf1\u5bfc\u7684\u5149\u8870\u51cf\uff0c\u5728\u53cd\u5c04\u6a21\u5f0f\u4e0b\uff0c\u901a\u8fc7\u5149\u5b66\u5faa\u73af\u5668\u548c\u65cb\u8f6c\u7684\u975e\u5747\u5300\u78c1\u4f53\u8fdb\u884c\u53cc\u901a\u9053\u914d\u7f6e", "result": "\u5b9e\u73b0\u4e86360\u00b0\u8fde\u7eed\u65cb\u8f6c\uff0c\u65cb\u8f6c\u901f\u5ea6\u8303\u56f4\u4e3a135 \u00b0/s\u81f3370 \u00b0/s\uff0c\u89d2\u5206\u8fa8\u7387\u4e3a0.3\u00b0", "conclusion": "\u8be5\u7f16\u7801\u5668\u4e3a\u4f20\u7edf\u673a\u5668\u4eba\u65cb\u8f6c\u7f16\u7801\u5668\u63d0\u4f9b\u4e86\u4e00\u79cd\u5177\u6709\u7ade\u4e89\u529b\u7684\u4f4e\u6210\u672c\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2511.17516", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.17516", "abs": "https://arxiv.org/abs/2511.17516", "authors": ["Rohit Chouhan"], "title": "A Dynamic Take on Window Management", "comment": null, "summary": "On modern computers with graphical user interfaces, application windows are managed by a window manager, a core component of the desktop environment. Mainstream operating systems such as Microsoft Windows and Apple's macOS employ window managers, where users rely on a mouse or trackpad to manually resize, reposition, and switch between overlapping windows. This approach can become inefficient, particularly on smaller screens such as laptops, where frequent window adjustments disrupt workflow and increase task completion time. An alternative paradigm, dynamic window management, automatically arranges application windows into non-overlapping layouts. These systems reduce the need for manual manipulation by providing intelligent placement strategies and support for multiple workspaces. Despite their potential usability benefits, dynamic window managers remain niche, primarily available on Linux systems and rarely enabled by default. This study evaluates the usability of dynamic window managers in comparison to conventional floating window systems. We developed a prototype dynamic window manager that incorporates configurable layouts and workspace management, and we conducted both heuristic evaluation and statistical testing to assess its effectiveness. Our findings indicate that dynamic window managers significantly improve task completion time in multi-window workflows by 37.83%. By combining cognitive heuristics with empirical performance measures, this work highlights the potential of dynamic window management as a viable alternative to traditional floating window systems and contributes evidence-based insights to the broader field of human-computer interaction (HCI).", "AI": {"tldr": "\u52a8\u6001\u7a97\u53e3\u7ba1\u7406\u901a\u8fc7\u667a\u80fd\u5e03\u5c40\u548c\u591a\u5de5\u4f5c\u533a\u652f\u6301\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u7a97\u53e3\u73af\u5883\u7684\u5de5\u4f5c\u6548\u7387\u3002", "motivation": "\u5728\u73b0\u4ee3\u56fe\u5f62\u7528\u6237\u754c\u9762\u7684\u8ba1\u7b97\u673a\u4e0a\uff0c\u7a97\u53e3\u7ba1\u7406\u6548\u7387\u4f4e\u4e0b\uff0c\u5c24\u5176\u662f\u5728\u5c0f\u5c4f\u5e55\u4e0a\uff0c\u9891\u7e41\u8c03\u6574\u7a97\u53e3\u5f71\u54cd\u5de5\u4f5c\u6d41\u7a0b\u548c\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u3002", "method": "\u5f00\u53d1\u4e00\u4e2a\u539f\u578b\u52a8\u6001\u7a97\u53e3\u7ba1\u7406\u5668\uff0c\u8fdb\u884c\u542f\u53d1\u5f0f\u8bc4\u4f30\u548c\u7edf\u8ba1\u6d4b\u8bd5\uff0c\u6bd4\u8f83\u5176\u4e0e\u4f20\u7edf\u6d6e\u52a8\u7a97\u53e3\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "result": "\u52a8\u6001\u7a97\u53e3\u7ba1\u7406\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u591a\u7a97\u53e3\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\uff0c\u63d0\u5347\u6548\u7387\u3002", "conclusion": "\u52a8\u6001\u7a97\u53e3\u7ba1\u7406\u4f5c\u4e3a\u4f20\u7edf\u6d6e\u52a8\u7a97\u53e3\u7cfb\u7edf\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u5b8c\u6210\u6548\u7387\uff0c\u5e76\u4e3a\u4eba\u673a\u4ea4\u4e92\u9886\u57df\u63d0\u4f9b\u4e86\u57fa\u4e8e\u8bc1\u636e\u7684\u89c1\u89e3\u3002"}}
{"id": "2511.17720", "categories": ["cs.RO", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2511.17720", "abs": "https://arxiv.org/abs/2511.17720", "authors": ["Sean Cowan", "Pietro Fanti", "Leon B. S. Williams", "Chit Hong Yam", "Kaneyasu Asakuma", "Yuichiro Nada", "Dario Izzo"], "title": "Vision-Guided Optic Flow Navigation for Small Lunar Missions", "comment": null, "summary": "Private lunar missions are faced with the challenge of robust autonomous navigation while operating under stringent constraints on mass, power, and computational resources. This work proposes a motion-field inversion framework that uses optical flow and rangefinder-based depth estimation as a lightweight CPU-based solution for egomotion estimation during lunar descent. We extend classical optical flow formulations by integrating them with depth modeling strategies tailored to the geometry for lunar/planetary approach, descent, and landing, specifically, planar and spherical terrain approximations parameterized by a laser rangefinder. Motion field inversion is performed through a least-squares framework, using sparse optical flow features extracted via the pyramidal Lucas-Kanade algorithm. We verify our approach using synthetically generated lunar images over the challenging terrain of the lunar south pole, using CPU budgets compatible with small lunar landers. The results demonstrate accurate velocity estimation from approach to landing, with sub-10% error for complex terrain and on the order of 1% for more typical terrain, as well as performances suitable for real-time applications. This framework shows promise for enabling robust, lightweight on-board navigation for small lunar missions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u8fd0\u52a8\u573a\u53cd\u6f14\u6846\u67b6\uff0c\u5229\u7528\u5149\u6d41\u548c\u6fc0\u5149\u6d4b\u8ddd\u6df1\u5ea6\u4f30\u8ba1\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u7cbe\u51c6\u7684\u6708\u7403\u964d\u843d\u5bfc\u822a\u3002", "motivation": "\u79c1\u8425\u6708\u7403\u4efb\u52a1\u9700\u8981\u5728\u8d44\u6e90\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u7a33\u5065\u7684\u81ea\u4e3b\u5bfc\u822a\u3002", "method": "\u901a\u8fc7\u5c06\u5149\u6d41\u4e0e\u6fc0\u5149\u6d4b\u8ddd\u6df1\u5ea6\u5efa\u6a21\u7ed3\u5408\uff0c\u91c7\u7528\u6700\u5c0f\u4e8c\u4e58\u6846\u67b6\u8fdb\u884c\u8fd0\u52a8\u573a\u53cd\u6f14\uff0c\u4f7f\u7528\u91d1\u5b57\u5854Lucas-Kanade\u7b97\u6cd5\u63d0\u53d6\u7a00\u758f\u5149\u6d41\u7279\u5f81\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8fd0\u52a8\u573a\u53cd\u6f14\u6846\u67b6\uff0c\u80fd\u591f\u5728\u6708\u7403\u4e0b\u964d\u9636\u6bb5\u5b9e\u73b0\u7cbe\u786e\u7684\u81ea\u8fd0\u52a8\u4f30\u8ba1\uff0c\u5177\u6709\u5b9e\u65f6\u5e94\u7528\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5c0f\u578b\u6708\u7403\u4efb\u52a1\u63d0\u4f9b\u4e86\u7a33\u5065\u8f7b\u91cf\u7684\u822a\u884c\u89e3\u51b3\u65b9\u6848\uff0c\u6f14\u793a\u51fa\u4f18\u826f\u7684\u5b9e\u65f6\u5e94\u7528\u80fd\u529b\u3002"}}
{"id": "2511.17756", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.17756", "abs": "https://arxiv.org/abs/2511.17756", "authors": ["Aparup Khatua", "David Jurgens", "Ingmar Weber"], "title": "Digital Diasporas: How Origin Characteristics and Host-Native Distance Shape Immigrants' Online Cultural Retention", "comment": "This paper will appear at ICWSM 2026. Please cite the peer-reviewed version", "summary": "Immigrants bring unique cultural backgrounds to their host countries. Subsequent interplay of cultures can lead to either a melting pot, where immigrants adopt the dominant culture of the host country, or a mosaic, where distinct cultural identities coexist. The existing literature primarily focuses on the acculturation of immigrants, specifically the melting pot hypothesis. In contrast, we attempt to identify the antecedents of the mosaic hypothesis or factors that enhance (or diminish) the propensity for cultural retention among immigrants. Based on Facebook advertising data for immigrants from 8 countries residing in the USA, our findings suggest that greater host-native distance is linked to higher online cultural retention, and while origin country context is statistically significant, its impact is generally smaller.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u79fb\u6c11\u6587\u5316\u4fdd\u7559\u7684\u56e0\u7d20\uff0c\u53d1\u73b0\u5bbf\u4e3b\u56fd\u6587\u5316\u8ddd\u79bb\u504f\u8fdc\u53ef\u589e\u5f3a\u79fb\u6c11\u7684\u6587\u5316\u4fdd\u7559\u3002", "motivation": "\u7814\u7a76\u79fb\u6c11\u6587\u5316\u80cc\u666f\u4e0e\u5bbf\u4e3b\u56fd\u6587\u5316\u4e4b\u95f4\u7684\u4e92\u52a8\uff0c\u5c24\u5176\u662f\u5bfb\u627e\u652f\u6301\u6587\u5316\u4fdd\u7559\u7684\u56e0\u7d20\u3002", "method": "\u57fa\u4e8e\u6765\u81ea8\u4e2a\u56fd\u5bb6\u79fb\u6c11\u5728\u7f8e\u56fd\u7684Facebook\u5e7f\u544a\u6570\u636e\u8fdb\u884c\u5206\u6790\u3002", "result": "\u8f83\u5927\u7684\u5bbf\u4e3b\u56fd\u6587\u5316\u8ddd\u79bb\u4e0e\u8f83\u9ad8\u7684\u5728\u7ebf\u6587\u5316\u4fdd\u7559\u76f8\u5173\uff0c\u539f\u7c4d\u56fd\u7684\u80cc\u666f\u5bf9\u6587\u5316\u4fdd\u7559\u7684\u5f71\u54cd\u76f8\u5bf9\u8f83\u5c0f\u3002", "conclusion": "\u79fb\u6c11\u5728\u6587\u5316\u4fdd\u7559\u65b9\u9762\u7684\u503e\u5411\u4e0e\u4e3b\u673a\u56fd\u7684\u6587\u5316\u8ddd\u79bb\u76f8\u5173\uff0c\u4e14\u539f\u7c4d\u56fd\u7684\u56e0\u7d20\u867d\u663e\u8457\u4f46\u5f71\u54cd\u8f83\u5c0f\u3002"}}
{"id": "2511.17765", "categories": ["cs.RO", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.17765", "abs": "https://arxiv.org/abs/2511.17765", "authors": ["Darren Chiu", "Zhehui Huang", "Ruohai Ge", "Gaurav S. Sukhatme"], "title": "LEARN: Learning End-to-End Aerial Resource-Constrained Multi-Robot Navigation", "comment": "20 pages, 15 figures", "summary": "Nano-UAV teams offer great agility yet face severe navigation challenges due to constrained onboard sensing, communication, and computation. Existing approaches rely on high-resolution vision or compute-intensive planners, rendering them infeasible for these platforms. We introduce LEARN, a lightweight, two-stage safety-guided reinforcement learning (RL) framework for multi-UAV navigation in cluttered spaces. Our system combines low-resolution Time-of-Flight (ToF) sensors and a simple motion planner with a compact, attention-based RL policy. In simulation, LEARN outperforms two state-of-the-art planners by $10\\%$ while using substantially fewer resources. We demonstrate LEARN's viability on six Crazyflie quadrotors, achieving fully onboard flight in diverse indoor and outdoor environments at speeds up to $2.0 m/s$ and traversing $0.2 m$ gaps.", "AI": {"tldr": "LEARN\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4e13\u4e3a\u591a\u65e0\u4eba\u673a\u5728\u590d\u6742\u7a7a\u95f4\u4e2d\u7684\u5bfc\u822a\u8bbe\u8ba1\uff0c\u6027\u80fd\u4f18\u5f02\u4e14\u8d44\u6e90\u6d88\u8017\u4f4e\u3002", "motivation": "Nano-UAV\u56e2\u961f\u5728\u5bfc\u822a\u4e2d\u9762\u4e34\u4e25\u91cd\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u6216\u8ba1\u7b97\u5bc6\u96c6\u578b\u89c4\u5212\u5668\uff0c\u65e0\u6cd5\u9002\u5e94\u8fd9\u4e9b\u5e73\u53f0\u7684\u9700\u6c42\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u4e24\u9636\u6bb5\u5b89\u5168\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff08LEARN\uff09", "result": "LEARN\u5728\u4eff\u771f\u4e2d\u8868\u73b0\u4f18\u4e8e\u4e24\u79cd\u6700\u5148\u8fdb\u7684\u89c4\u5212\u5668\uff0c\u63d0\u5347\u4e8610%\uff0c\u540c\u65f6\u4f7f\u7528\u7684\u8d44\u6e90\u663e\u8457\u66f4\u5c11\u3002", "conclusion": "LEARN\u5728\u516d\u67b6Crazyflie\u56db\u65cb\u7ffc\u4e0a\u6210\u529f\u6f14\u793a\uff0c\u5b9e\u73b0\u4e86\u591a\u6837\u5316\u73af\u5883\u4e0b\u7684\u5b8c\u5168\u673a\u8f7d\u98de\u884c\uff0c\u901f\u5ea6\u53ef\u8fbe2.0 m/s\uff0c\u5e76\u6210\u529f\u7a7f\u8d8a0.2 m\u7684\u95f4\u9699\u3002"}}
{"id": "2511.17906", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17906", "abs": "https://arxiv.org/abs/2511.17906", "authors": ["Wen-Fan Wang", "Chien-Ting Lu", "Jin Ping Ng", "Yi-Ting Chiu", "Ting-Ying Lee", "Miaosen Wang", "Bing-Yu Chen", "Xiang 'Anthony' Chen"], "title": "AnimAgents: Coordinating Multi-Stage Animation Pre-Production with Human-Multi-Agent Collaboration", "comment": null, "summary": "Animation pre-production lays the foundation of an animated film by transforming initial concepts into a coherent blueprint across interdependent stages such as ideation, scripting, design, and storyboarding. While generative AI tools are increasingly adopted in this process, they remain isolated, requiring creators to juggle multiple systems without integrated workflow support. Our formative study with 12 professional creative directors and independent animators revealed key challenges in their current practice: Creators must manually coordinate fragmented outputs, manage large volumes of information, and struggle to maintain continuity and creative control between stages. Based on the insights, we present AnimAgents, a human-multi-agent collaborative system that coordinates complex, multi-stage workflows through a core agent and specialized agents, supported by dedicated boards for the four major stages of pre-production. AnimAgents enables stage-aware orchestration, stage-specific output management, and element-level refinement, providing an end-to-end workflow tailored to professional practice. In a within-subjects summative study with 16 professional creators, AnimAgents significantly outperformed a strong single-agent baseline that equipped with advanced parallel image generation in coordination, consistency, information management, and overall satisfaction (p < .01). A field deployment with 4 creators further demonstrated AnimAgents' effectiveness in real-world projects.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86AnimAgents\uff0c\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7cfb\u7edf\uff0c\u65e8\u5728\u4f18\u5316\u52a8\u753b\u524d\u671f\u5236\u4f5c\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u63d0\u9ad8\u521b\u4f5c\u8005\u7684\u534f\u540c\u6548\u7387\u548c\u6ee1\u610f\u5ea6\u3002", "motivation": "\u968f\u7740\u751f\u6210\u6027AI\u5de5\u5177\u5728\u52a8\u753b\u5236\u4f5c\u4e2d\u7684\u6108\u52a0\u666e\u53ca\uff0c\u521b\u4f5c\u8005\u9762\u4e34\u7740\u591a\u4e2a\u5b64\u7acb\u7cfb\u7edf\u95f4\u7684\u534f\u8c03\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u96c6\u6210\u7684\u5de5\u4f5c\u6d41\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u5347\u751f\u4ea7\u6548\u7387\u548c\u521b\u4f5c\u63a7\u5236\u529b\u3002", "method": "\u672c\u6587\u91c7\u7528\u4e86\u5f62\u6210\u6027\u7814\u7a76\u548c\u603b\u7ed3\u6027\u7814\u7a76\u7684\u65b9\u6cd5\uff0c\u9996\u5148\u901a\u8fc7\u5bf912\u4f4d\u4e13\u4e1a\u521b\u610f\u603b\u76d1\u548c\u72ec\u7acb\u52a8\u753b\u5e08\u7684\u8bbf\u8c08\u786e\u5b9a\u4e86\u73b0\u5b58\u6311\u6218\uff0c\u7136\u540e\u901a\u8fc7\u5bf916\u4f4d\u4e13\u4e1a\u521b\u4f5c\u8005\u7684\u5b9e\u8bc1\u5b9e\u9a8c\u8bc4\u4f30AnimAgents\u7684\u6709\u6548\u6027\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAnimAgents\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7cfb\u7edf\uff0c\u65e8\u5728\u89e3\u51b3\u52a8\u753b\u7535\u5f71\u524d\u671f\u5236\u4f5c\u4e2d\u521b\u4f5c\u8005\u9762\u4e34\u7684\u534f\u8c03\u548c\u7ba1\u7406\u6311\u6218\u3002\u8be5\u7cfb\u7edf\u901a\u8fc7\u6838\u5fc3\u667a\u80fd\u4f53\u548c\u4e13\u4e1a\u667a\u80fd\u4f53\uff0c\u63d0\u4f9b\u9488\u5bf9 ideation\u3001scripting\u3001design \u548c storyboarding \u56db\u4e2a\u4e3b\u8981\u9636\u6bb5\u7684\u652f\u6301\uff0c\u4ece\u800c\u5b9e\u73b0\u590d\u6742\u7684\u591a\u9636\u6bb5\u5de5\u4f5c\u6d41\u534f\u540c\u3002\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0cAnimAgents\u5728\u534f\u8c03\u6027\u3001\u4e00\u81f4\u6027\u3001\u4fe1\u606f\u7ba1\u7406\u548c\u6574\u4f53\u6ee1\u610f\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7684\u5355\u4e00\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u548c\u5b9e\u9645\u90e8\u7f72\uff0cAnimAgents\u88ab\u8bc1\u660e\u5728\u52a8\u753b\u524d\u671f\u5236\u4f5c\u4e2d\u5177\u6709\u663e\u8457\u7684\u6548\u7387\u548c\u6ee1\u610f\u5ea6\u63d0\u5347\uff0c\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u73b0\u5b58\u7684\u534f\u8c03\u548c\u7ba1\u7406\u95ee\u9898\u3002"}}
{"id": "2511.17774", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17774", "abs": "https://arxiv.org/abs/2511.17774", "authors": ["Salma Mozaffari", "Daniel Ruan", "William van den Bogert", "Nima Fazeli", "Sigrid Adriaenssens", "Arash Adel"], "title": "Learning Diffusion Policies for Robotic Manipulation of Timber Joinery under Fabrication Uncertainty", "comment": null, "summary": "Construction uncertainties such as fabrication inaccuracies and material imperfections pose a significant challenge to contact-rich robotic manipulation by hindering precise and robust assembly. In this paper, we explore the performance and robustness of diffusion policy learning as a promising solution for contact-sensitive robotic assembly at construction scale, using timber mortise and tenon joints as a case study. A two-phase study is conducted: first, to evaluate policy performance and applicability; second, to assess robustness in handling fabrication uncertainties simulated as randomized perturbations to the mortise position. The best-performing policy achieved a total average success rate of 75% with perturbations up to 10 mm, including 100% success in unperturbed cases. The results demonstrate the potential of sensory-motor diffusion policies to generalize to a wide range of complex, contact-rich assembly tasks across construction and manufacturing, advancing robotic construction under uncertainty and contributing to safer, more efficient building practices.", "AI": {"tldr": "\u672c\u8bba\u6587\u63a2\u8ba8\u4e86\u6269\u6563\u653f\u7b56\u5b66\u4e60\u5728\u5e94\u5bf9\u5efa\u7b51\u4e2d\u63a5\u89e6\u4e30\u5bcc\u7684\u673a\u5668\u4eba\u7ec4\u88c5\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u9762\u4e34\u65bd\u5de5\u4e0d\u786e\u5b9a\u6027\u65f6\u7684\u8868\u73b0\u4e0e\u9c81\u68d2\u6027\u3002", "motivation": "\u5efa\u7b51\u5de5\u7a0b\u4e2d\uff0c\u5236\u9020\u4e0d\u51c6\u786e\u548c\u6750\u6599\u7f3a\u9677\u5bf9\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\uff0c\u5f71\u54cd\u7ec4\u88c5\u7cbe\u5ea6\u4e0e\u7a33\u5065\u6027\u3002", "method": "\u901a\u8fc7\u4e24\u9636\u6bb5\u7814\u7a76\uff0c\u9996\u5148\u8bc4\u4f30\u653f\u7b56\u7684\u6027\u80fd\u548c\u9002\u7528\u6027\uff0c\u5176\u6b21\u8bc4\u4f30\u5176\u5728\u5e94\u5bf9\u65bd\u5de5\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u6a21\u62df\u7684\u968f\u673a\u504f\u5dee\u4e0b\uff0c\u6700\u4f73\u653f\u7b56\u5728\u504f\u5dee\u6700\u591a\u4e3a10\u6beb\u7c73\u7684\u60c5\u51b5\u4e0b\u53d6\u5f97\u4e8675%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u5728\u672a\u53d7\u6270\u52a8\u7684\u60c5\u51b5\u4e0b\u5219\u8fbe\u5230100%\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u6269\u6563\u653f\u7b56\u5b66\u4e60\u5c55\u793a\u4e86\u5904\u7406\u590d\u6742\u63a5\u89e6\u4e30\u5bcc\u7ec4\u88c5\u4efb\u52a1\u7684\u6f5c\u529b\uff0c\u63a8\u52a8\u4e86\u673a\u5668\u4eba\u65bd\u5de5\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u53d1\u5c55\uff0c\u5177\u6709\u66f4\u5b89\u5168\u9ad8\u6548\u7684\u5efa\u7b51\u5b9e\u8df5\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.17919", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.17919", "abs": "https://arxiv.org/abs/2511.17919", "authors": ["Tamzid Hossain", "Md. Fahimul Islam", "Farida Chowdhury"], "title": "Exploring Multiview UI Layouts and Placement Strategies for Collaborative Sensemaking in Virtual Reality", "comment": "12 pages, 9 figures", "summary": "Immersive technologies expand the potential for collaborative sense-making and visual analysis via head-worn displays (HWDs), offering customizable, high-resolution perspectives of a shared visualization space. In such an immersive environment, window/view management is crucial for collaborative sense-making tasks. However, the role of document types (graphs, images) and pair dynamics in collaborative layout formation has rarely been explored. We conducted a user study with 20 participants to explore how pair of users organize multiview windows in remote immersive workspaces during tasks such as search, comparison, and classification. Findings show that users often arrange windows in a semi-circular layout for pair collaboration. Image+text documents reduce mental and temporal demand in comparison tasks, while graphs lower task load for classification. Conflicts in window selection arise mainly in complex comparisons, with frequent discussion and reorganization during difficult tasks. Based on these insights, we propose design guidelines for multiview systems that support VR collaboration and brainstorming.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u7528\u6237\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u6c89\u6d78\u5f0f\u5de5\u4f5c\u7a7a\u95f4\u4e2d\uff0c\u7528\u6237\u5982\u4f55\u7ba1\u7406\u7a97\u53e3\u5e03\u5c40\uff0c\u5e76\u63d0\u51fa\u4e86\u8bbe\u8ba1\u6307\u5bfc\u539f\u5219\u3002", "motivation": "\u63a2\u8ba8\u6587\u6863\u7c7b\u578b\u548c\u7528\u6237\u52a8\u6001\u5728\u534f\u4f5c\u5e03\u5c40\u5f62\u6210\u4e2d\u7684\u5f71\u54cd\uff0c\u4ee5\u4f18\u5316\u865a\u62df\u73b0\u5b9e\u4e2d\u7684\u5408\u4f5c\u548c\u89c6\u89c9\u5206\u6790\u3002", "method": "\u901a\u8fc7\u5bf920\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u7528\u6237\u7814\u7a76\uff0c\u5206\u6790\u7528\u6237\u5728\u4efb\u52a1\u4e2d\u5982\u4f55\u7ec4\u7ec7\u591a\u89c6\u7a97\u3002", "result": "\u8fd9\u9879\u7814\u7a76\u63a2\u8ba8\u4e86\u8eab\u4e34\u5176\u5883\u7684\u6280\u672f\u5982\u4f55\u589e\u5f3a\u534f\u4f5c\u611f\u77e5\u548c\u89c6\u89c9\u5206\u6790\uff0c\u7279\u522b\u662f\u5728\u9065\u8fdc\u7684\u6c89\u6d78\u5f0f\u5de5\u4f5c\u7a7a\u95f4\u4e2d\uff0c\u7528\u6237\u5982\u4f55\u7ba1\u7406\u591a\u89c6\u7a97\u5e03\u5c40\u3002\u901a\u8fc720\u540d\u53c2\u4e0e\u8005\u7684\u7528\u6237\u7814\u7a76\uff0c\u53d1\u73b0\u534a\u5706\u5f62\u5e03\u5c40\u66f4\u9002\u5408\u5408\u4f5c\u8005\uff0c\u56fe\u50cf\u52a0\u6587\u672c\u7684\u6587\u6863\u5728\u6bd4\u8f83\u4efb\u52a1\u4e2d\u6709\u6548\u51cf\u5c11\u4e86\u5fc3\u7406\u548c\u65f6\u95f4\u9700\u6c42\uff0c\u800c\u56fe\u8868\u5219\u964d\u4f4e\u4e86\u5206\u7c7b\u4efb\u52a1\u7684\u8d1f\u62c5\u3002\u672c\u7814\u7a76\u4e5f\u6307\u51fa\uff0c\u5728\u590d\u6742\u6bd4\u8f83\u4e2d\u7a97\u53e3\u9009\u62e9\u7684\u51b2\u7a81\u5c24\u4e3a\u7a81\u51fa\uff0c\u5bfc\u81f4\u9891\u7e41\u8ba8\u8bba\u548c\u91cd\u7ec4\u3002\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u63d0\u51fa\u4e86\u9488\u5bf9\u865a\u62df\u73b0\u5b9e\u534f\u4f5c\u548c\u5934\u6234\u663e\u793a\u5668\u8bbe\u8ba1\u591a\u89c6\u7a97\u7cfb\u7edf\u7684\u6307\u5bfc\u539f\u5219\u3002", "conclusion": "\u7528\u6237\u5f80\u5f80\u5728\u534f\u4f5c\u4efb\u52a1\u4e2d\u9009\u62e9\u534a\u5706\u5f62\u5e03\u5c40\uff0c\u7279\u5b9a\u6587\u6863\u7c7b\u578b\u80fd\u6709\u6548\u51cf\u8f7b\u4efb\u52a1\u8d1f\u62c5\uff0c\u590d\u6742\u6bd4\u8f83\u4e2d\u51b2\u7a81\u9891\u7e41\u3002"}}
{"id": "2511.17777", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17777", "abs": "https://arxiv.org/abs/2511.17777", "authors": ["Ravi Prakash", "Vincent Y. Wang", "Arpit Mishra", "Devi Yuliarti", "Pei Zhong", "Ryan P. McNabb", "Patrick J. Codd", "Leila J. Bridgeman"], "title": "See, Plan, Cut: MPC-Based Autonomous Volumetric Robotic Laser Surgery with OCT Guidance", "comment": "9 pages, 8 figures", "summary": "Robotic laser systems offer the potential for sub-millimeter, non-contact, high-precision tissue resection, yet existing platforms lack volumetric planning and intraoperative feedback. We present RATS (Robot-Assisted Tissue Surgery), an intelligent opto-mechanical, optical coherence tomography (OCT)-guided robotic platform designed for autonomous volumetric soft tissue resection in surgical applications. RATS integrates macro-scale RGB-D imaging, micro-scale OCT, and a fiber-coupled surgical laser, calibrated through a novel multistage alignment pipeline that achieves OCT-to-laser calibration accuracy of 0.161+-0.031mm on tissue phantoms and ex vivo porcine tissue. A super-Gaussian laser-tissue interaction (LTI) model characterizes ablation crater morphology with an average RMSE of 0.231+-0.121mm, outperforming Gaussian baselines. A sampling-based model predictive control (MPC) framework operates directly on OCT voxel data to generate constraint-aware resection trajectories with closed-loop feedback, achieving 0.842mm RMSE and improving intersection-over-union agreement by 64.8% compared to feedforward execution. With OCT, RATS detects subsurface structures and modifies the planner's objective to preserve them, demonstrating clinical feasibility.", "AI": {"tldr": "RATS\u662f\u4e00\u4e2a\u65b0\u578b\u7684\u673a\u5668\u4eba\u8f85\u52a9\u7ec4\u7ec7\u624b\u672f\u5e73\u53f0\uff0c\u7ed3\u5408\u4e86OCT\u548c\u6fc0\u5149\u6280\u672f\uff0c\u80fd\u505a\u5230\u9ad8\u7cbe\u5ea6\u7684\u8f6f\u7ec4\u7ec7\u5207\u9664\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u4eba\u6fc0\u5149\u7cfb\u7edf\u7f3a\u4e4f\u4f53\u79ef\u89c4\u5212\u548c\u672f\u4e2d\u53cd\u9988\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u9ad8\u7cbe\u5ea6\u8f6f\u7ec4\u7ec7\u5207\u9664\u4e2d\u7684\u5e94\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8be5\u7814\u7a76\u91c7\u7528\u591a\u7ea7\u5bf9\u9f50\u6d41\u7a0b\u5b9e\u73b0OCT\u4e0e\u6fc0\u5149\u7684\u7cbe\u786e\u6821\u51c6\uff0c\u5e76\u5229\u7528\u57fa\u4e8e\u91c7\u6837\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\u751f\u6210\u7cbe\u786e\u7684\u5207\u9664\u8f68\u8ff9\u3002", "result": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aRATS\u7684\u673a\u5668\u4eba\u8f85\u52a9\u7ec4\u7ec7\u624b\u672f\u5e73\u53f0\uff0c\u65e8\u5728\u901a\u8fc7\u5149\u5b66\u76f8\u5e72\u6210\u50cf\u5f15\u5bfc\u5b9e\u73b0\u81ea\u4e3b\u7684\u4f53\u79ef\u8f6f\u7ec4\u7ec7\u5207\u9664\u3002\u8be5\u5e73\u53f0\u5c06\u5b8f\u89c2RGB-D\u6210\u50cf\u3001\u5fae\u89c2OCT\u548c\u5149\u7ea4\u8026\u5408\u7684\u5916\u79d1\u6fc0\u5149\u76f8\u7ed3\u5408\uff0c\u5177\u6709\u9ad8\u7cbe\u5ea6\u7684\u5207\u9664\u80fd\u529b\u3002", "conclusion": "RATS\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u7ec4\u7ec7\u5185\u90e8\u7ed3\u6784\uff0c\u5e76\u6839\u636e\u68c0\u6d4b\u7ed3\u679c\u8c03\u6574\u5207\u9664\u89c4\u5212\uff0c\u663e\u793a\u51fa\u5176\u4e34\u5e8a\u5e94\u7528\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2511.18213", "categories": ["cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18213", "abs": "https://arxiv.org/abs/2511.18213", "authors": ["Kunwoo Lee", "Dhivya Sreedhar", "Pushkar Saraf", "Chaeeun Lee", "Kateryna Shapovalenko"], "title": "Typing Reinvented: Towards Hands-Free Input via sEMG", "comment": null, "summary": "We explore surface electromyography (sEMG) as a non-invasive input modality for mapping muscle activity to keyboard inputs, targeting immersive typing in next-generation human-computer interaction (HCI). This is especially relevant for spatial computing and virtual reality (VR), where traditional keyboards are impractical. Using attention-based architectures, we significantly outperform the existing convolutional baselines, reducing online generic CER from 24.98% -> 20.34% and offline personalized CER from 10.86% -> 10.10%, while remaining fully causal. We further incorporate a lightweight decoding pipeline with language-model-based correction, demonstrating the feasibility of accurate, real-time muscle-driven text input for future wearable and spatial interfaces.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u8868\u9762\u808c\u7535\u56fe(sEMG)\u4f5c\u4e3a\u4e00\u79cd\u975e\u4fb5\u5165\u6027\u8f93\u5165\u65b9\u5f0f\uff0c\u5c06\u808c\u8089\u6d3b\u52a8\u6620\u5c04\u5230\u952e\u76d8\u8f93\u5165\uff0c\u4ee5\u5b9e\u73b0\u4e0b\u4e00\u4ee3\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u6c89\u6d78\u5f0f\u6253\u5b57\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u865a\u62df\u73b0\u5b9e(VR)\u548c\u7a7a\u95f4\u8ba1\u7b97\u573a\u666f\u3002", "motivation": "\u5728\u865a\u62df\u73b0\u5b9e\u548c\u7a7a\u95f4\u8ba1\u7b97\u4e2d\uff0c\u4f20\u7edf\u952e\u76d8\u7684\u4e0d\u4fbf\u4fc3\u4f7f\u63a2\u7d22\u65b0\u7684\u8f93\u5165\u65b9\u5f0f\uff0c\u4ee5\u589e\u5f3a\u4eba\u673a\u4ea4\u4e92\u4f53\u9a8c\u3002", "method": "\u5229\u7528\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u67b6\u6784\u548c\u8f7b\u91cf\u89e3\u7801\u7ba1\u9053\u4e0e\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u6821\u6b63\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8f93\u5165\u51c6\u786e\u6027\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u5377\u79ef\u57fa\u7ebf\uff0c\u5728\u7ebf\u901a\u7528\u5b57\u7b26\u9519\u8bef\u7387(CER)\u4ece24.98%\u964d\u4f4e\u81f320.34%\uff0c\u79bb\u7ebf\u4e2a\u6027\u5316CER\u4ece10.86%\u964d\u4f4e\u81f310.10%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0csEMG\u53ef\u4ee5\u4f5c\u4e3a\u6709\u6548\u7684\u5b9e\u65f6\u808c\u8089\u9a71\u52a8\u6587\u672c\u8f93\u5165\u65b9\u5f0f\uff0c\u9002\u7528\u4e8e\u53ef\u7a7f\u6234\u8bbe\u5907\u548c\u7a7a\u95f4\u754c\u9762\u3002"}}
{"id": "2511.17781", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17781", "abs": "https://arxiv.org/abs/2511.17781", "authors": ["Kristy Sakano", "Jianyu An", "Dinesh Manocha", "Huan Xu"], "title": "SAFE-SMART: Safety Analysis and Formal Evaluation using STL Metrics for Autonomous RoboTs", "comment": null, "summary": "We present a novel, regulator-driven approach for post hoc safety evaluation of learning-based, black-box autonomous mobile robots, ensuring ongoing compliance with evolving, human-defined safety rules. In our iterative workflow, human safety requirements are translated by regulators into Signal Temporal Logic (STL) specifications. Rollout traces from the black-box model are externally verified for compliance, yielding quantitative safety metrics, Total Robustness Value (TRV) and Largest Robustness Value (LRV), which measure average and worst-case specification adherence. These metrics inform targeted retraining and iterative improvement by model designers. We apply our method across two different applications: a virtual driving scenario and an autonomous mobile robot navigating a complex environment, and observe statistically significant improvements across both scenarios. In the virtual driving scenario, we see a 177% increase in traces adhering to the simulation speed limit, a 1138% increase in traces minimizing off-road driving, and a 16% increase in traces successfully reaching the goal within the time limit. In the autonomous navigation scenario, there is a 300% increase in traces avoiding sharp turns, a 200% increase in traces reaching the goal within the time limit, and a 49% increase in traces minimizing time spent near obstacles. Finally, we validate our approach on a TurtleBot3 robot in the real world, and demonstrate improved obstacle navigation with safety buffers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8c03\u8282\u9a71\u52a8\u65b9\u6cd5\u7528\u4e8e\u5b66\u4e60\u578b\u9ed1\u7bb1\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u7684\u540e\u671f\u5b89\u5168\u8bc4\u4f30\uff0c\u901a\u8fc7\u5c06\u4eba\u7c7b\u5b89\u5168\u8981\u6c42\u8f6c\u5316\u4e3aSTL\u89c4\u8303\uff0c\u5916\u90e8\u9a8c\u8bc1\u9ed1\u7bb1\u6a21\u578b\u7684\u56de\u653e\u8f68\u8ff9\uff0c\u5e76\u83b7\u5f97\u5b89\u5168\u5ea6\u91cf\u503c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e24\u79cd\u5e94\u7528\u573a\u666f\u7684\u5b89\u5168\u6027\u80fd\u3002", "motivation": "\u652f\u6301\u57fa\u4e8e\u5b66\u4e60\u7684\u9ed1\u7bb1\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u7684\u540e\u671f\u5b89\u5168\u8bc4\u4f30\uff0c\u4ee5\u786e\u4fdd\u5176\u6301\u7eed\u9075\u5b88\u4e0d\u65ad\u6f14\u53d8\u7684\u4eba\u7c7b\u5b9a\u4e49\u7684\u5b89\u5168\u89c4\u5219\u3002", "method": "\u901a\u8fc7\u5c06\u4eba\u7c7b\u5b89\u5168\u8981\u6c42\u8f6c\u6362\u4e3a\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91\uff08STL\uff09\u89c4\u8303\uff0c\u6211\u4eec\u63d0\u53d6\u9ed1\u7bb1\u6a21\u578b\u7684\u56de\u653e\u8f68\u8ff9\uff0c\u5e76\u8fdb\u884c\u5408\u89c4\u6027\u5916\u90e8\u9a8c\u8bc1\uff0c\u751f\u6210\u603b\u9c81\u68d2\u6027\u503c\uff08TRV\uff09\u548c\u6700\u5927\u9c81\u68d2\u6027\u503c\uff08LRV\uff09\u4f5c\u4e3a\u5b89\u5168\u5ea6\u91cf\u6307\u6807\u3002", "result": "\u5728\u865a\u62df\u9a7e\u9a76\u573a\u666f\u4e2d\uff0c\u9075\u5b88\u901f\u5ea6\u9650\u5236\u7684\u8f68\u8ff9\u589e\u52a0\u4e86177%\uff0c\u51cf\u5c11\u8d8a\u754c\u9a7e\u9a76\u7684\u8f68\u8ff9\u589e\u52a0\u4e861138%\uff0c\u6309\u65f6\u6210\u529f\u8fbe\u5230\u76ee\u6807\u7684\u8f68\u8ff9\u589e\u52a0\u4e8616%\uff1b\u5728\u81ea\u4e3b\u5bfc\u822a\u573a\u666f\u4e2d\uff0c\u907f\u514d\u6025\u8f6c\u5f2f\u7684\u8f68\u8ff9\u589e\u52a0\u4e86300%\uff0c\u6309\u65f6\u8fbe\u5230\u76ee\u6807\u7684\u8f68\u8ff9\u589e\u52a0\u4e86200%\uff0c\u51cf\u5c11\u5728\u969c\u788d\u7269\u9644\u8fd1\u9017\u7559\u65f6\u95f4\u7684\u8f68\u8ff9\u589e\u52a0\u4e8649%\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u865a\u62df\u9a7e\u9a76\u573a\u666f\u548c\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u5bfc\u822a\u573a\u666f\u4e2d\u5747\u663e\u793a\u51fa\u663e\u8457\u7684\u6539\u8fdb\uff0c\u4e14\u5728\u771f\u5b9e\u73af\u5883\u4e0b\u4e5f\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002"}}
{"id": "2511.18274", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18274", "abs": "https://arxiv.org/abs/2511.18274", "authors": ["Edward Kim", "Yuri Cho", "Jose Eduardo E. Lima", "Julie Muccini", "Jenelle Jindal", "Alison Scheid", "Erik Nelson", "Seong Hyun Park", "Yuchen Zeng", "Alton Sturgis", "Caesar Li", "Jackie Dai", "Sun Min Kim", "Yash Prakash", "Liwen Sun", "Isabella Hu", "Hongxuan Wu", "Daniel He", "Wiktor Rajca", "Cathra Halabi", "Maarten Lansberg", "Bjoern Hartmann", "Sanjit A. Seshia"], "title": "Clinician-Directed Large Language Model Software Generation for Therapeutic Interventions in Physical Rehabilitation", "comment": null, "summary": "Digital health interventions are increasingly used in physical and occupational therapy to deliver home exercise programs via sensor equipped devices such as smartphones, enabling remote monitoring of adherence and performance. However, digital interventions are typically programmed as software before clinical encounters as libraries of parametrized exercise modules targeting broad patient populations. At the point of care, clinicians can only select modules and adjust a narrow set of parameters like repetitions, so patient specific needs that emerge during encounters, such as distinct movement limitations, and home environments, are rarely reflected in the software. We evaluated a digital intervention paradigm that uses large language models (LLMs) to translate clinicians' exercise prescriptions into intervention software. In a prospective single arm feasibility study with 20 licensed physical and occupational therapists and a standardized patient, clinicians created 40 individualized upper extremity programs (398 instructions) that were automatically translated into executable software. Our results show a 45% increase in the proportion of personalized prescriptions that can be implemented as software compared with a template based benchmark, with unanimous consensus among therapists on ease of use. The LLM generated software correctly delivered 99.78% (397/398) of instructions as prescribed and monitored performance with 88.4% (352/398) accuracy, with 90% (18/20) of therapists judged it safe to interact with patients, and 75% (15/20) expressed willingness to adopt it. To our knowledge, this is the first prospective evaluation of clinician directed intervention software generation with LLMs in healthcare, demonstrating feasibility and motivating larger trials to assess clinical effectiveness and safety in real patient populations.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u8bc4\u4f30\u4e86\u4f7f\u7528LLMs\u751f\u6210\u4e34\u5e8a\u533b\u751f\u5bfc\u5411\u7684\u5e72\u9884\u8f6f\u4ef6\uff0c\u663e\u793a\u5176\u53ef\u884c\u6027\u5e76\u6fc0\u52b1\u8fdb\u4e00\u6b65\u7684\u4e34\u5e8a\u6709\u6548\u6027\u548c\u5b89\u5168\u6027\u7814\u7a76", "motivation": "\u89e3\u51b3\u73b0\u6709\u6570\u5b57\u5e72\u9884\u65e0\u6cd5\u5145\u5206\u6ee1\u8db3\u60a3\u8005\u4e2a\u4f53\u9700\u6c42\u7684\u95ee\u9898", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5c06\u4e34\u5e8a\u533b\u751f\u7684\u8fd0\u52a8\u5904\u65b9\u8f6c\u6362\u4e3a\u5e72\u9884\u8f6f\u4ef6", "result": "\u60a3\u8005\u4e2a\u6027\u5316\u5904\u65b9\u7684\u5b9e\u73b0\u6bd4\u4f8b\u63d0\u9ad8\u4e8645%\uff0c\u5e76\u4e14\u751f\u6210\u7684\u8f6f\u4ef6\u6307\u4ee4\u51c6\u786e\u6027\u4e3a99.78%", "conclusion": "\u672c\u7814\u7a76\u8868\u660eLLMs\u5728\u6570\u5b57\u5065\u5eb7\u5e72\u9884\u4e2d\u7684\u6f5c\u529b\uff0c\u9700\u8fdb\u884c\u66f4\u5927\u89c4\u6a21\u7684\u8bd5\u9a8c\u4ee5\u9a8c\u8bc1\u4e34\u5e8a\u6548\u679c\u3002"}}
{"id": "2511.17798", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17798", "abs": "https://arxiv.org/abs/2511.17798", "authors": ["Francesco D'Orazio", "Sepehr Samavi", "Xintong Du", "Siqi Zhou", "Giuseppe Oriolo", "Angela P. Schoellig"], "title": "SM$^2$ITH: Safe Mobile Manipulation with Interactive Human Prediction via Task-Hierarchical Bilevel Model Predictive Control", "comment": null, "summary": "Mobile manipulators are designed to perform complex sequences of navigation and manipulation tasks in human-centered environments. While recent optimization-based methods such as Hierarchical Task Model Predictive Control (HTMPC) enable efficient multitask execution with strict task priorities, they have so far been applied mainly to static or structured scenarios. Extending these approaches to dynamic human-centered environments requires predictive models that capture how humans react to the actions of the robot. This work introduces Safe Mobile Manipulation with Interactive Human Prediction via Task-Hierarchical Bilevel Model Predictive Control (SM$^2$ITH), a unified framework that combines HTMPC with interactive human motion prediction through bilevel optimization that jointly accounts for robot and human dynamics. The framework is validated on two different mobile manipulators, the Stretch 3 and the Ridgeback-UR10, across three experimental settings: (i) delivery tasks with different navigation and manipulation priorities, (ii) sequential pick-and-place tasks with different human motion prediction models, and (iii) interactions involving adversarial human behavior. Our results highlight how interactive prediction enables safe and efficient coordination, outperforming baselines that rely on weighted objectives or open-loop human models.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6SM$^2$ITH\uff0c\u5c06\u4efb\u52a1\u5206\u5c42\u53cc\u5c42\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u4e0e\u4ea4\u4e92\u5f0f\u4eba\u7c7b\u8fd0\u52a8\u9884\u6d4b\u76f8\u7ed3\u5408\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u79fb\u52a8\u64cd\u63a7\u5b89\u5168\u6027\u4e0e\u6548\u7387\u3002", "motivation": "\u79fb\u52a8\u64cd\u63a7\u5668\u5728\u4eba\u4e2d\u5fc3\u73af\u5883\u4e2d\u9700\u8981\u8fdb\u884c\u590d\u6742\u7684\u5bfc\u822a\u548c\u64cd\u63a7\u4efb\u52a1\uff0c\u4f20\u7edf\u65b9\u6cd5\u4e3b\u8981\u7528\u4e8e\u9759\u6001\u573a\u666f\uff0c\u56e0\u6b64\u9700\u8981\u6269\u5c55\u81f3\u52a8\u6001\u73af\u5883\uff0c\u4ee5\u6355\u6349\u4eba\u7c7b\u5bf9\u673a\u5668\u4eba\u884c\u4e3a\u7684\u53cd\u5e94\u3002", "method": "\u5b89\u5168\u79fb\u52a8\u64cd\u63a7\u4e0e\u4ea4\u4e92\u5f0f\u4eba\u7c7b\u9884\u6d4b\u7684\u4efb\u52a1\u5206\u5c42\u53cc\u5c42\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08SM$^2$ITH\uff09", "result": "\u6211\u4eec\u5728\u4e24\u4e2a\u79fb\u52a8\u64cd\u63a7\u5668\uff08Stretch 3\u548cRidgeback-UR10\uff09\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u7ed3\u679c\u663e\u793a\u4ea4\u4e92\u5f0f\u9884\u6d4b\u80fd\u591f\u5b89\u5168\u6709\u6548\u5730\u534f\u8c03\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u7684\u4e92\u52a8\uff0c\u4f18\u4e8e\u57fa\u4e8e\u52a0\u6743\u76ee\u6807\u6216\u5f00\u653e\u5f0f\u4eba\u7c7b\u6a21\u578b\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u4ea4\u4e92\u5f0f\u9884\u6d4b\uff0c\u672c\u6846\u67b6\u5728\u52a8\u6001\u4eba\u7c7b\u73af\u5883\u4e2d\u7684\u5b89\u5168\u534f\u8c03\u548c\u6548\u7387\u8868\u73b0\u4f18\u8d8a\uff0c\u9002\u5e94\u4e86\u4eba\u7c7b\u884c\u4e3a\u53d8\u5316\u3002"}}
{"id": "2511.18548", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.18548", "abs": "https://arxiv.org/abs/2511.18548", "authors": ["Maria Stella Albarelli"], "title": "Emotion-Aware Conversational Recommender Systems: a Case Study", "comment": null, "summary": "In recent years, online shopping has grown rapidly, especially during the COVID-19 period. However, it still lacks elements typical of physical stores, such as empathic support and personalised advice from a sales assistant. This study explores how an emotion-aware Conversational Agent (CA) can improve the online shopping experience by responding to user emotions in a more natural and human way. The project focuses on Gala, a virtual assistant developed for the Galeries Lafayette website, capable of recognising emotional states from voice messages and adapting its responses accordingly. User needs were first analysed through semi-structured interviews, which informed the design of Gala's UX and functionalities. Gala was implemented using the OpenAI API and the Galeries Lafayette API, adopting a Content-Based recommendation approach. Through Natural Language Processing, it interprets user requests and retrieves products aligned with specific attributes such as name, price, and brand, enabling fluid dialogue and tailored suggestions. Two user studies were conducted: a usability test and a comparative evaluation between a standard CA and Gala's emotion-aware version. The results highlight the potential of emotion-aware CAs to make online shopping faster, more engaging, and closer to an in-store guided experience.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u60c5\u611f\u611f\u77e5\u7684\u5bf9\u8bdd\u4ee3\u7406\u5982\u4f55\u6539\u5584\u5728\u7ebf\u8d2d\u7269\u4f53\u9a8c\uff0c\u5c55\u793a\u5176\u5728\u7528\u6237\u60c5\u611f\u54cd\u5e94\u4e2d\u7684\u5e94\u7528\u53ca\u6210\u6548\u3002", "motivation": "\u63d0\u5347\u5728\u7ebf\u8d2d\u7269\u4f53\u9a8c\uff0c\u5f25\u8865\u7f3a\u4e4f\u540c\u7406\u652f\u6301\u548c\u4e2a\u6027\u5316\u5efa\u8bae\u7684\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u8bed\u97f3\u6d88\u606f\u8bc6\u522b\u60c5\u611f\u72b6\u6001\uff0c\u5e94\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\uff0c\u63d0\u4f9b\u4e2a\u6027\u5316\u63a8\u8350\u548c\u5bf9\u8bdd\u3002", "result": "\u60c5\u611f\u611f\u77e5\u7684\u5bf9\u8bdd\u4ee3\u7406Gala\u4f7f\u5728\u7ebf\u8d2d\u7269\u66f4\u5feb\u3001\u66f4\u5177\u5438\u5f15\u529b\uff0c\u63a5\u8fd1\u5b9e\u4f53\u5e97\u7684\u5bfc\u8d2d\u4f53\u9a8c\u3002", "conclusion": "\u60c5\u611f\u611f\u77e5\u7684\u5bf9\u8bdd\u4ee3\u7406\u5c55\u793a\u4e86\u63d0\u9ad8\u5728\u7ebf\u8d2d\u7269\u4f53\u9a8c\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u901f\u5ea6\u548c\u4e92\u52a8\u6027\u65b9\u9762\u3002"}}
{"id": "2511.17889", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17889", "abs": "https://arxiv.org/abs/2511.17889", "authors": ["Ting Huang", "Dongjian Li", "Rui Yang", "Zeyu Zhang", "Zida Yang", "Hao Tang"], "title": "MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots", "comment": null, "summary": "Grounding natural-language instructions into continuous control for quadruped robots remains a fundamental challenge in vision language action. Existing methods struggle to bridge high-level semantic reasoning and low-level actuation, leading to unstable grounding and weak generalization in the real world. To address these issues, we present MobileVLA-R1, a unified vision-language-action framework that enables explicit reasoning and continuous control for quadruped robots. We construct MobileVLA-CoT, a large-scale dataset of multi-granularity chain-of-thought (CoT) for embodied trajectories, providing structured reasoning supervision for alignment. Built upon this foundation, we introduce a two-stage training paradigm that combines supervised CoT alignment with GRPO reinforcement learning to enhance reasoning consistency, control stability, and long-horizon execution. Extensive evaluations on VLN and VLA tasks demonstrate superior performance over strong baselines, with approximately a 5% improvement. Real-world deployment on a quadruped robot validates robust performance in complex environments. Code: https://github.com/AIGeeksGroup/MobileVLA-R1. Website: https://aigeeksgroup.github.io/MobileVLA-R1.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MobileVLA-R1\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8\u56db\u8db3\u673a\u5668\u4eba\u5728\u8fde\u7eed\u63a7\u5236\u4e2d\u7684\u8868\u73b0\uff0c\u89e3\u51b3\u4e86\u9ad8\u5c42\u8bed\u4e49\u63a8\u7406\u4e0e\u4f4e\u5c42\u6267\u884c\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5f53\u524d\u65b9\u6cd5\u5728\u8bed\u4e49\u63a8\u7406\u4e0e\u6267\u884c\u4e0a\u7684\u4e0d\u7a33\u5b9a\u6027\u548c\u5f31\u6cdb\u5316\u80fd\u529b\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u6709\u6548\u7684\u6846\u67b6\u6765\u6539\u5584\u56db\u8db3\u673a\u5668\u4eba\u7684\u63a7\u5236\u80fd\u529b\u3002", "method": "\u91c7\u7528\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff0c\u7ed3\u5408\u4e86\u6709\u76d1\u7763\u7684\u94fe\u5f0f\u63a8\u7406\u5bf9\u9f50\u4e0eGRPO\u5f3a\u5316\u5b66\u4e60\uff0c\u4ee5\u63d0\u9ad8\u63a8\u7406\u4e00\u81f4\u6027\u548c\u63a7\u5236\u7a33\u5b9a\u6027\u3002", "result": "\u5728VLN\u548cVLA\u4efb\u52a1\u4e0a\uff0cMobileVLA-R1\u6bd4\u5f3a\u57fa\u7ebf\u8868\u73b0\u63d0\u9ad8\u4e86\u7ea65%\u3002", "conclusion": "\u7ecf\u8fc7\u5b9e\u5730\u6d4b\u8bd5\uff0cMobileVLA-R1\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.18561", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.18561", "abs": "https://arxiv.org/abs/2511.18561", "authors": ["Zitian Peng", "Shiyao Zhang", "Shanliang Yao", "Xiaohui Zhu", "Mengjie Huang", "Prudence Wong", "Yong Yue"], "title": "The Evaluation for Usability Methods of Unmanned Surface Vehicles: Are Current Usability Methods Viable for Unmanned Surface Vehicles? Insights from a Multiple Case Study Approach to Human-Robot Interaction", "comment": null, "summary": "Unmanned Surface Vehicles (USVs) are increasingly utilised for diverse applications, ranging from environmental monitoring to security patrols. While USV technology is progressing, it remains clear that full autonomy is not achievable in all scenarios, and remote human intervention is still crucial, particularly in dynamic or complex environments. This continued reliance on human intervention highlights a range of Human-Robot Interaction (HRI) challenges that remain unresolved. Compared to the extensive body of HRI research in domains such as unmanned aerial vehicles and autonomous vehicles, HRI considerations specific to USVs remain significantly underexplored. Addressing this gap, our study investigates real-world usability challenges in USV operation through in-depth interviews with 9 engineers and users, supported by field observations. We focus especially on the difficulties beginner operators encounter and their coping strategies. Our findings reveal existing usability issues, mental models, and adaptation strategies of beginners that inform future user-centered design of USV systems, contributing new insights to the emerging field of maritime HRI. Based on these findings, we argue that current USV systems are poorly suited for beginner operation in dynamic inland and offshore environments, where operators must make timely decisions under uncertainty, manage complex spatial awareness, and adapt to changing environmental conditions. Furthermore, we identify key operational patterns in three representative use cases-harmful algal bloom detection, underwater concealed pipe inspection and post-construction hydrographic survey, and summarise key interaction constraints that should inform future maritime HRI design efforts.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86USV\u64cd\u4f5c\u4e2d\u7684\u53ef\u7528\u6027\u6311\u6218\uff0c\u7279\u522b\u662f\u521d\u5b66\u8005\u7684\u56f0\u96be\u4e0e\u5e94\u5bf9\u7b56\u7565\uff0c\u65e8\u5728\u4e3a\u672a\u6765\u7684\u6d77\u4e8bHRI\u8bbe\u8ba1\u63d0\u4f9b\u6307\u5bfc", "motivation": "USV\u6280\u672f\u4e0d\u65ad\u8fdb\u6b65\uff0c\u4f46\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f9d\u7136\u9700\u8981\u4eba\u7c7b\u5e72\u9884\uff0c\u7a81\u663eHRI\u6311\u6218\uff0c\u7279\u522b\u662f\u5728USV\u9886\u57df\u8f83\u5c11\u63a2\u7d22", "method": "\u8fdb\u884c\u6df1\u5ea6\u8bbf\u8c08\u548c\u5b9e\u5730\u89c2\u5bdf\u4ee5\u7814\u7a76USV\u64cd\u4f5c\u7684\u53ef\u7528\u6027\u6311\u6218", "result": "\u63ed\u793a\u4e86\u521d\u5b66\u8005\u5728USV\u64cd\u4f5c\u4e2d\u7684\u53ef\u7528\u6027\u95ee\u9898\u3001\u5fc3\u7406\u6a21\u578b\u548c\u9002\u5e94\u7b56\u7565\uff0c\u63d0\u4f9b\u4e86\u672a\u6765\u7528\u6237\u4e2d\u5fc3\u8bbe\u8ba1\u7684\u89c1\u89e3", "conclusion": "\u5f53\u524dUSV\u7cfb\u7edf\u4e0d\u9002\u5408\u521d\u5b66\u8005\u5728\u52a8\u6001\u73af\u5883\u4e2d\u64cd\u4f5c\uff0c\u9700\u9488\u5bf9\u4ea4\u4e92\u7ea6\u675f\u548c\u5b9e\u9645\u5e94\u7528\u6848\u4f8b\u4f18\u5316\u672a\u6765\u8bbe\u8ba1\u3002"}}
{"id": "2511.17898", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17898", "abs": "https://arxiv.org/abs/2511.17898", "authors": ["Weixi Song", "Zhetao Chen", "Tao Xu", "Xianchao Zeng", "Xinyu Zhou", "Lixin Yang", "Donglin Wang", "Cewu Lu", "Yong-Lu Li"], "title": "L1 Sample Flow for Efficient Visuomotor Learning", "comment": null, "summary": "Denoising-based models, such as diffusion and flow matching, have been a critical component of robotic manipulation for their strong distribution-fitting and scaling capacity. Concurrently, several works have demonstrated that simple learning objectives, such as L1 regression, can achieve performance comparable to denoising-based methods on certain tasks, while offering faster convergence and inference. In this paper, we focus on how to combine the advantages of these two paradigms: retaining the ability of denoising models to capture multi-modal distributions and avoid mode collapse while achieving the efficiency of the L1 regression objective. To achieve this vision, we reformulate the original v-prediction flow matching and transform it into sample-prediction with the L1 training objective. We empirically show that the multi-modality can be expressed via a single ODE step. Thus, we propose \\textbf{L1 Flow}, a two-step sampling schedule that generates a suboptimal action sequence via a single integration step and then reconstructs the precise action sequence through a single prediction. The proposed method largely retains the advantages of flow matching while reducing the iterative neural function evaluations to merely two and mitigating the potential performance degradation associated with direct sample regression. We evaluate our method with varying baselines and benchmarks, including 8 tasks in MimicGen, 5 tasks in RoboMimic \\& PushT Bench, and one task in the real-world scenario. The results show the advantages of the proposed method with regard to training efficiency, inference speed, and overall performance. \\href{https://song-wx.github.io/l1flow.github.io/}{Project Website.}", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5L1 Flow\uff0c\u901a\u8fc7\u7ed3\u5408\u53bb\u566a\u6a21\u578b\u548cL1\u56de\u5f52\u7684\u4f18\u70b9\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u63a7\u7684\u6548\u7387\u548c\u8868\u73b0\u3002", "motivation": "\u65e8\u5728\u7ed3\u5408\u53bb\u566a\u57fa\u6a21\u578b\u7684\u591a\u6a21\u6001\u6355\u83b7\u80fd\u529b\u548cL1\u56de\u5f52\u7684\u5feb\u901f\u6536\u655b\u6027\uff0c\u4ee5\u63d0\u5347\u590d\u6742\u4efb\u52a1\u7684\u5904\u7406\u6548\u679c\u3002", "method": "\u5c06\u539f\u59cb\u7684v-prediction\u6d41\u5339\u914d\u91cd\u6784\u4e3a\u57fa\u4e8eL1\u8bad\u7ec3\u76ee\u6807\u7684\u6837\u672c\u9884\u6d4b\uff0c\u91c7\u7528\u4e24\u6b65\u91c7\u6837\u7b56\u7565\u751f\u6210\u548c\u91cd\u5efa\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "\u4e0e\u591a\u79cd\u57fa\u7ebf\u548c\u57fa\u51c6\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u8bad\u7ec3\u6548\u7387\u3001\u63a8\u7406\u901f\u5ea6\u548c\u6574\u4f53\u6027\u80fd\u4e0a\u5177\u6709\u660e\u663e\u4f18\u52bf\u3002", "conclusion": "L1 Flow\u65b9\u6cd5\u6709\u6548\u6574\u5408\u4e86\u591a\u6a21\u6001\u5206\u5e03\u6355\u83b7\u80fd\u529b\u4e0eL1\u56de\u5f52\u7684\u9ad8\u6548\u6027\uff0c\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u5c55\u73b0\u4e86\u4f18\u8d8a\u7684\u8bad\u7ec3\u6548\u7387\u548c\u63a8\u7406\u901f\u5ea6\u3002"}}
{"id": "2511.18965", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.18965", "abs": "https://arxiv.org/abs/2511.18965", "authors": ["Susanne Hindennach", "Mayar Elfares", "C\u00e9line Gressel", "Andreas Bulling"], "title": "REFLECTing SPERET: Measuring and Promoting Ethics and Privacy Reflexivity in Eye-Tracking Research", "comment": "15 pages", "summary": "The proliferation of eye tracking in high-stakes domains - such as healthcare, marketing and surveillance - underscores the need for researchers to be ethically aware when employing this technology. Although privacy and ethical guidelines have emerged in recent years, empirical research on how scholars reflect on their own work remains scarce. To address this gap, we present two complementary instruments developed with input from more than 70 researchers: REFLECT, a qualitative questionnaire, and SPERET (Latin for \"hope\"), a quantitative psychometric scale that measures privacy and ethics reflexivity in eye tracking. Our findings reveal a research community that is concerned about user privacy, cognisant of methodological constraints, such as sample bias, and that possesses a nuanced sense of ethical responsibility evolving with project maturity. Together, these tools and our analyses offer a systematic examination and a hopeful outlook on reflexivity in eye-tracking research, promoting more privacy and ethics-conscious practice.", "AI": {"tldr": "\u7814\u7a76\u8005\u5728\u773c\u52a8\u8ffd\u8e2a\u6280\u672f\u5e94\u7528\u4e2d\u9700\u5173\u6ce8\u4f26\u7406\u548c\u9690\u79c1\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u5de5\u5177\u5e2e\u52a9\u53cd\u601d\uff0c\u5e76\u663e\u793a\u51fa\u7814\u7a76\u8005\u5bf9\u6b64\u7684\u654f\u611f\u6027\u3002", "motivation": "\u5728\u773c\u52a8\u8ffd\u8e2a\u6280\u672f\u5e7f\u6cdb\u5e94\u7528\u4e8e\u533b\u7597\u3001\u5e02\u573a\u8425\u9500\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u7684\u80cc\u666f\u4e0b\uff0c\u7814\u7a76\u8005\u5fc5\u987b\u63d0\u9ad8\u5bf9\u4f26\u7406\u548c\u9690\u79c1\u7684\u610f\u8bc6\uff0c\u4ee5\u4fdd\u969c\u7528\u6237\u6743\u76ca\u3002", "method": "\u901a\u8fc7\u4e0e70\u591a\u540d\u7814\u7a76\u8005\u7684\u4e92\u52a8\uff0c\u5f00\u53d1\u4e86REFLECT\uff08\u5b9a\u6027\u95ee\u5377\uff09\u548cSPERET\uff08\u5b9a\u91cf\u5fc3\u7406\u91cf\u8868\uff09\u6765\u8861\u91cf\u9690\u79c1\u548c\u4f26\u7406\u53cd\u601d\u3002", "result": "\u672c\u6587\u63a2\u8ba8\u4e86\u9ad8\u98ce\u9669\u9886\u57df\u4e2d\u773c\u52a8\u8ffd\u8e2a\u6280\u672f\u7684\u4f26\u7406\u4f7f\u7528\uff0c\u63d0\u51fa\u4e86\u4e24\u4e2a\u5de5\u5177\u4ee5\u5e2e\u52a9\u7814\u7a76\u8005\u53cd\u601d\u81ea\u5df1\u7684\u5de5\u4f5c\uff0c\u5e76\u5f3a\u8c03\u4e86\u7814\u7a76\u8005\u5bf9\u7528\u6237\u9690\u79c1\u7684\u5173\u6ce8\u548c\u4f26\u7406\u8d23\u4efb\u611f\u7684\u6f14\u53d8\u3002", "conclusion": "\u7814\u7a76\u8005\u5728\u8fdb\u884c\u773c\u52a8\u8ffd\u8e2a\u7814\u7a76\u65f6\u9700\u66f4\u52a0\u5173\u6ce8\u9690\u79c1\u4e0e\u4f26\u7406\u95ee\u9898\uff0c\u6240\u63d0\u4f9b\u7684\u5de5\u5177\u548c\u5206\u6790\u6709\u52a9\u4e8e\u63a8\u52a8\u8fd9\u4e00\u5b9e\u8df5\u7684\u53cd\u601d\u548c\u8fdb\u6b65\u3002"}}
{"id": "2511.17925", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17925", "abs": "https://arxiv.org/abs/2511.17925", "authors": ["Jeonghwan Kim", "Wontaek Kim", "Yidan Lu", "Jin Cheng", "Fatemeh Zargarbashi", "Zicheng Zeng", "Zekun Qi", "Zhiyang Dou", "Nitish Sontakke", "Donghoon Baek", "Sehoon Ha", "Tianyu Li"], "title": "Switch-JustDance: Benchmarking Whole Body Motion Tracking Policies Using a Commercial Console Game", "comment": null, "summary": "Recent advances in whole-body robot control have enabled humanoid and legged robots to perform increasingly agile and coordinated motions. However, standardized benchmarks for evaluating these capabilities in real-world settings, and in direct comparison to humans, remain scarce. Existing evaluations often rely on pre-collected human motion datasets or simulation-based experiments, which limit reproducibility, overlook hardware factors, and hinder fair human-robot comparisons. We present Switch-JustDance, a low-cost and reproducible benchmarking pipeline that leverages motion-sensing console games, Just Dance on the Nintendo Switch, to evaluate robot whole-body control. Using Just Dance on the Nintendo Switch as a representative platform, Switch-JustDance converts in-game choreography into robot-executable motions through streaming, motion reconstruction, and motion retargeting modules and enables users to evaluate controller performance through the game's built-in scoring system. We first validate the evaluation properties of Just Dance, analyzing its reliability, validity, sensitivity, and potential sources of bias. Our results show that the platform provides consistent and interpretable performance measures, making it a suitable tool for benchmarking embodied AI. Building on this foundation, we benchmark three state-of-the-art humanoid whole-body controllers on hardware and provide insights into their relative strengths and limitations.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faSwitch-JustDance\u4f5c\u4e3a\u8bc4\u4f30\u673a\u5668\u4eba\u63a7\u5236\u7684\u4f4e\u6210\u672c\u57fa\u51c6\uff0c\u901a\u8fc7\u4efb\u5929\u5802Switch\u7684\u300aJust Dance\u300b\u6e38\u620f\u5b9e\u73b0\u52a8\u4f5c\u8f6c\u6362\u548c\u6027\u80fd\u8bc4\u4f30\u3002", "motivation": "\u9488\u5bf9\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e00\u79cd\u65b0\u7684\u3001\u53ef\u91cd\u590d\u7684\u8bc4\u4f30\u673a\u5668\u4eba\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5c24\u5176\u662f\u5728\u4e0e\u4eba\u7c7b\u76f4\u63a5\u6bd4\u8f83\u65f6\u7684\u6807\u51c6\u5316\u57fa\u51c6\u3002", "method": "\u5229\u7528\u4efb\u5929\u5802Switch\u4e0a\u7684\u300aJust Dance\u300b\u6e38\u620f\uff0c\u5c06\u6e38\u620f\u4e2d\u7684\u7f16\u821e\u8f6c\u6362\u4e3a\u673a\u5668\u4eba\u53ef\u6267\u884c\u7684\u52a8\u4f5c\uff0c\u901a\u8fc7\u6d41\u5a92\u4f53\u3001\u52a8\u4f5c\u91cd\u5efa\u548c\u52a8\u4f5c\u91cd\u65b0\u5b9a\u4f4d\u6a21\u5757\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u9a8c\u8bc1\u4e86\u300aJust Dance\u300b\u7684\u8bc4\u4f30\u5c5e\u6027\uff0c\u5e76\u6210\u529f\u57fa\u51c6\u6d4b\u8bd5\u4e86\u4e09\u79cd\u5148\u8fdb\u7684\u7c7b\u4eba\u5168\u8eab\u63a7\u5236\u5668\uff0c\u63ed\u793a\u4e86\u5b83\u4eec\u7684\u76f8\u5bf9\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "conclusion": "Switch-JustDance\u662f\u4e00\u4e2a\u4f4e\u6210\u672c\u4e14\u53ef\u91cd\u590d\u7684\u57fa\u51c6\u6d4b\u8bd5\u7ba1\u9053\uff0c\u9002\u5408\u8bc4\u4f30\u673a\u5668\u4eba\u5168\u8eab\u63a7\u5236\u6027\u80fd\u3002"}}
{"id": "2511.18985", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.18985", "abs": "https://arxiv.org/abs/2511.18985", "authors": ["Manuel Valle Torre", "Marcus Specht", "Catharine Oertel"], "title": "LLM Chatbots in High School Programming: Exploring Behaviors and Interventions", "comment": null, "summary": "This study uses a Design-Based Research (DBR) cycle to refine the integration of Large Language Models (LLMs) in high school programming education. The initial problem was identified in an Intervention Group where, in an unguided setting, a higher proportion of executive, solution-seeking queries correlated strongly and negatively with exam performance. A contemporaneous Comparison Group demonstrated that without guidance, these unproductive help-seeking patterns do not self-correct, with engagement fluctuating and eventually declining. This insight prompted a mid-course pedagogical intervention in the first group, designed to teach instrumental help-seeking. The subsequent evaluation confirmed the intervention's success, revealing a decrease in executive queries, as well as a shift toward more productive learning workflows. However, this behavioral change did not translate into a statistically significant improvement in exam grades, suggesting that altering tool-use strategies alone may be insufficient to overcome foundational knowledge gaps. The DBR process thus yields a more nuanced principle: the educational value of an LLM depends on a pedagogy that scaffolds help-seeking, but this is only one part of the complex process of learning.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7DBR\u5faa\u73af\u6539\u8fdb\u9ad8\u4e2d\u7f16\u7a0b\u6559\u80b2\u4e2dLLM\u7684\u6574\u5408\uff0c\u53d1\u73b0\u65e0\u6307\u5bfc\u7684\u6c42\u52a9\u884c\u4e3a\u4e0e\u8003\u8bd5\u6210\u7ee9\u8d1f\u76f8\u5173\u3002\u5c3d\u7ba1\u5e72\u9884\u540e\u884c\u4e3a\u6709\u6240\u6539\u5584\uff0c\u4f46\u8003\u8bd5\u6210\u7ee9\u672a\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u8bc6\u522b\u51fa\u5728\u65e0\u6307\u5bfc\u60c5\u51b5\u4e0b\u6c42\u52a9\u6a21\u5f0f\u4e0d\u81ea\u6211\u4fee\u6b63\uff0c\u4e0e\u8003\u8bd5\u8868\u73b0\u8d1f\u76f8\u5173\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u8bbe\u8ba1\u57fa\u7840\u7814\u7a76\uff08DBR\uff09\u5faa\u73af\u6765\u6539\u8fdb\u9ad8\u4e2d\u7f16\u7a0b\u6559\u80b2\u4e2d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6574\u5408\u3002", "result": "\u5e72\u9884\u7ec4\u7684\u5e72\u9884\u6210\u529f\uff0c\u51cf\u5c11\u9ad8\u7ba1\u67e5\u8be2\uff0c\u8f6c\u5411\u66f4\u6709\u6210\u6548\u7684\u5b66\u4e60\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7136\u800c\u8003\u8bd5\u6210\u7ee9\u5e76\u672a\u663e\u8457\u6539\u5584\u3002", "conclusion": "\u6539\u53d8\u5de5\u5177\u4f7f\u7528\u7b56\u7565\u53ef\u80fd\u4e0d\u8db3\u4ee5\u514b\u670d\u57fa\u7840\u77e5\u8bc6\u7f3a\u53e3\uff0cLLM\u7684\u6559\u80b2\u4ef7\u503c\u4f9d\u8d56\u4e8e\u652f\u6491\u6c42\u52a9\u884c\u4e3a\u7684\u6559\u5b66\u6cd5\u3002"}}
{"id": "2511.17961", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17961", "abs": "https://arxiv.org/abs/2511.17961", "authors": ["Hao Wang", "Xiaobao Wei", "Ying Li", "Qingpo Wuwu", "Dongli Wu", "Jiajun Cao", "Ming Lu", "Wenzhao Zheng", "Shanghang Zhang"], "title": "RoboArmGS: High-Quality Robotic Arm Splatting via B\u00e9zier Curve Refinement", "comment": null, "summary": "Building high-quality digital assets of robotic arms is crucial yet challenging for the Real2Sim2Real pipeline. Current approaches naively bind static 3D Gaussians according to URDF links, forcing them to follow an URDF-rigged motion passively. However, real-world arm motion is noisy, and the idealized URDF-rigged motion cannot accurately model it, leading to severe rendering artifacts in 3D Gaussians. To address these challenges, we propose RoboArmGS, a novel hybrid representation that refines the URDF-rigged motion with learnable B\u00e9zier curves, enabling more accurate real-world motion modeling. To be more specific, we present a learnable B\u00e9zier Curve motion refiner that corrects per-joint residuals to address mismatches between real-world motion and URDF-rigged motion. RoboArmGS enables the learning of more accurate real-world motion while achieving a coherent binding of 3D Gaussians across arm parts. To support future research, we contribute a carefully collected dataset named RoboArm4D, which comprises several widely used robotic arms for evaluating the quality of building high-quality digital assets. We evaluate our approach on RoboArm4D, and RoboArmGS achieves state-of-the-art performance in real-world motion modeling and rendering quality. The code and dataset will be released.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faRoboArmGS\uff0c\u4e00\u4e2a\u6df7\u5408\u8868\u793a\uff0c\u7ed3\u5408\u53ef\u5b66\u4e60\u7684\u8d1d\u5179\u66f2\u7ebf\u4ee5\u6539\u8fdbURDF\u6a21\u62df\u8fd0\u52a8\uff0c\u89e3\u51b3\u5b9e\u9645\u8fd0\u52a8\u5efa\u6a21\u4e2d\u7684\u4e0d\u51c6\u786e\u6027\uff0c\u5e76\u8d21\u732e\u4e86RoboArm4D\u6570\u636e\u96c6\uff0c\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u3002", "motivation": "\u5728Real2Sim2Real\u6d41\u7a0b\u4e2d\uff0c\u9ad8\u8d28\u91cf\u7684\u673a\u5668\u4eba\u81c2\u6570\u5b57\u8d44\u4ea7\u6784\u5efa\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u73b0\u6709\u65b9\u6cd5\u5bf9URDF\u94fe\u63a5\u7684\u9759\u6001\u7ed1\u5b9a\uff0c\u5bfc\u81f4\u65e0\u6cd5\u51c6\u786e\u6a21\u62df\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5173\u8282\u8fd0\u52a8\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5b66\u4e60\u7684\u8d1d\u5179\u66f2\u7ebf\u8fd0\u52a8\u4fee\u6b63\u5668\uff0c\u901a\u8fc7\u7ea0\u6b63\u6bcf\u4e2a\u5173\u8282\u7684\u6b8b\u5dee\u6765\u5904\u7406\u771f\u5b9e\u4e0eURDF\u6a21\u62df\u8fd0\u52a8\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u3002", "result": "RoboArmGS\u901a\u8fc7\u7ed3\u5408\u53ef\u5b66\u4e60\u7684\u8d1d\u5179\u66f2\u7ebf\uff0c\u63d0\u9ad8\u8fd0\u52a8\u5efa\u6a21\u7684\u51c6\u786e\u6027\uff0c\u5c55\u793a\u4e86\u5728\u73b0\u5b9e\u4e16\u754c\u8fd0\u52a8\u5efa\u6a21\u548c\u6e32\u67d3\u8d28\u91cf\u4e0a\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "RoboArmGS\u5c55\u793a\u4e86\u901a\u8fc7\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u8d1d\u5179\u66f2\u7ebf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u81c2\u7684\u771f\u5b9e\u8fd0\u52a8\u5efa\u6a21\u7cbe\u5ea6\u548c\u6e32\u67d3\u8d28\u91cf\uff0c\u5e76\u9884\u8ba1\u5c06\u5bf9\u76f8\u5173\u9886\u57df\u4ea7\u751f\u79ef\u6781\u5f71\u54cd\u3002"}}
{"id": "2511.19123", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.19123", "abs": "https://arxiv.org/abs/2511.19123", "authors": ["R. Bermudez Schettino", "A. Dasmeh", "L. Brinkmann"], "title": "Facilitating the Integration of LLMs Into Online Experiments With Simple Chat", "comment": null, "summary": "As large language models (LLMs) become increasingly prevalent, understanding human-LLM interactions is emerging as a central priority in psychological research. Online experiments offer an efficient means to study human-LLM interactions, yet integrating LLMs into established survey platforms remains technically demanding, particularly when aiming for ecologically valid, real-time conversational experiences with strong experimental control. We introduce Simple Chat, an open-source, research-focused chat interface that streamlines LLM integration for platforms such as Qualtrics, oTree, and LimeSurvey, while presenting a unified participant experience across conditions. Simple Chat connects to both commercial providers and open-weights models, supports streaming responses to preserve conversational flow, and offers an administrative interface for fine-grained control of prompts and interface features. By reducing technical barriers, standardizing interfaces, and improving participant experience, Simple Chat helps advance the study of human-LLM interaction. In this article, we outline Simple Chat's key features, provide a step-by-step tutorial, and demonstrate its utility through two illustrative case studies.", "AI": {"tldr": "Simple Chat\u662f\u4e00\u4e2a\u5f00\u6e90\u804a\u5929\u63a5\u53e3\uff0c\u65e8\u5728\u964d\u4f4e\u5728\u5728\u7ebf\u5b9e\u9a8c\u4e2d\u96c6\u6210\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6280\u672f\u58c1\u5792\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u666e\u53ca\uff0c\u7406\u89e3\u4eba\u7c7b\u4e0eLLM\u7684\u4e92\u52a8\u6210\u4e3a\u5fc3\u7406\u5b66\u7814\u7a76\u7684\u91cd\u70b9\uff0c\u800c\u73b0\u6709\u6280\u672f\u96be\u4ee5\u6574\u5408LLM\u4e0e\u8c03\u67e5\u5e73\u53f0\u3002", "method": "\u672c\u6587\u901a\u8fc7\u4ecb\u7ecdSimple Chat\u7684\u4e3b\u8981\u529f\u80fd\uff0c\u63d0\u4f9b\u9010\u6b65\u6559\u7a0b\uff0c\u5e76\u901a\u8fc7\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u5176\u5b9e\u7528\u6027\u3002", "result": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5f00\u6e90\u7684\u804a\u5929\u63a5\u53e3Simple Chat\uff0c\u65e8\u5728\u7b80\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5728\u7ebf\u5b9e\u9a8c\u4e2d\u7684\u96c6\u6210\uff0c\u4fc3\u8fdb\u4eba\u7c7b\u4e0eLLM\u7684\u4e92\u52a8\u7814\u7a76\u3002", "conclusion": "Simple Chat\u901a\u8fc7\u6807\u51c6\u5316\u63a5\u53e3\u548c\u6539\u5584\u53c2\u4e0e\u8005\u4f53\u9a8c\uff0c\u63a8\u52a8\u4eba\u7c7b\u4e0eLLM\u4e92\u52a8\u7814\u7a76\u7684\u8fdb\u5c55\u3002"}}
{"id": "2511.17992", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17992", "abs": "https://arxiv.org/abs/2511.17992", "authors": ["Chungeng Tian", "Fenghua He", "Ning Hao"], "title": "Unobservable Subspace Evolution and Alignment for Consistent Visual-Inertial Navigation", "comment": "20 pages, 16 figures", "summary": "The inconsistency issue in the Visual-Inertial Navigation System (VINS) is a long-standing and fundamental challenge. While existing studies primarily attribute the inconsistency to observability mismatch, these analyses are often based on simplified theoretical formulations that consider only prediction and SLAM correction. Such formulations fail to cover the non-standard estimation steps, such as MSCKF correction and delayed initialization, which are critical for practical VINS estimators. Furthermore, the lack of a comprehensive understanding of how inconsistency dynamically emerges across estimation steps has hindered the development of precise and efficient solutions. As a result, current approaches often face a trade-off between estimator accuracy, consistency, and implementation complexity. To address these limitations, this paper proposes a novel analysis framework termed Unobservable Subspace Evolution (USE), which systematically characterizes how the unobservable subspace evolves throughout the entire estimation pipeline by explicitly tracking changes in its evaluation points. This perspective sheds new light on how individual estimation steps contribute to inconsistency. Our analysis reveals that observability misalignment induced by certain steps is the antecedent of observability mismatch. Guided by this insight, we propose a simple yet effective solution paradigm, Unobservable Subspace Alignment (USA), which eliminates inconsistency by selectively intervening only in those estimation steps that induce misalignment. We design two USA methods: transformation-based and re-evaluation-based, both offering accurate and computationally lightweight solutions. Extensive simulations and real-world experiments validate the effectiveness of the proposed methods.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u65b0\u7684\u5206\u6790\u6846\u67b6\u548c\u89e3\u51b3\u65b9\u6848\uff0c\u65e8\u5728\u6d88\u9664\u89c6\u89c9\u60ef\u6027\u5bfc\u822a\u7cfb\u7edf\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u5bf9\u4f30\u8ba1\u6b65\u9aa4\u7684\u9009\u62e9\u6027\u5e72\u9884\uff0c\u63d0\u4f9b\u4e86\u51c6\u786e\u4e14\u8ba1\u7b97\u8f7b\u91cf\u7684\u89e3\u51b3\u65b9\u6cd5\u3002", "motivation": "\u5206\u6790\u89c6\u89c9\u60ef\u6027\u5bfc\u822a\u7cfb\u7edf\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u8be5\u95ee\u9898\u4e3b\u8981\u5f52\u56e0\u4e8e\u89c2\u5bdf\u6027\u4e0d\u5339\u914d\uff0c\u76ee\u524d\u7684\u7814\u7a76\u5927\u591a\u57fa\u4e8e\u7b80\u5316\u7684\u7406\u8bba\uff0c\u7f3a\u4e4f\u5bf9\u975e\u6807\u51c6\u4f30\u8ba1\u6b65\u9aa4\u7684\u7efc\u5408\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8eUnobservable Subspace Alignment (USA)\u7684\u65b9\u6cd5\uff0c\u5206\u522b\u662f\u57fa\u4e8e\u53d8\u6362\u7684\u548c\u57fa\u4e8e\u91cd\u65b0\u8bc4\u4f30\u7684\uff0c\u5747\u4e3a\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u5206\u6790\u6846\u67b6Unobservable Subspace Evolution (USE)\uff0c\u7cfb\u7edf\u6027\u5730\u63cf\u7ed8\u4e86\u4e0d\u53ef\u89c2\u6d4b\u5b50\u7a7a\u95f4\u5728\u4f30\u8ba1\u8fc7\u7a0b\u4e2d\u7684\u6f14\u53d8\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u4ee5\u6d88\u9664\u4e0d\u4e00\u81f4\u6027\u3002", "conclusion": "\u901a\u8fc7\u9009\u62e9\u6027\u5e72\u9884\u5bfc\u81f4\u4e0d\u4e00\u81f4\u6027\u7684\u4f30\u8ba1\u6b65\u9aa4\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u6d88\u9664\u4e86\u89c6\u89c9\u60ef\u6027\u5bfc\u822a\u7cfb\u7edf\u4e2d\u7684\u4e0d\u4e00\u81f4\u95ee\u9898\u3002"}}
{"id": "2511.19312", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.19312", "abs": "https://arxiv.org/abs/2511.19312", "authors": ["Christopher Baker", "Stephen Hinton", "Akashdeep Nijjar", "Riccardo Poli", "Caterina Cinel", "Tom Reed", "Stephen Fairclough"], "title": "Human-AI Teaming Under Deception: An Implicit BCI Safeguards Drone Team Performance in Virtual Reality", "comment": "30 pages, 19 figures", "summary": "Human-AI teams can be vulnerable to catastrophic failure when feedback from the AI is incorrect, especially under high cognitive workload. Traditional team aggregation methods, such as voting, are susceptible to these AI errors, which can actively bias the behaviour of each individual and inflate the likelihood of an erroneous group decision. We hypothesised that a collaborative Brain-Computer Interface (cBCI) using neural activity collected before a behavioural decision is made can provide a source of information that is decoupled from this biased behaviour, thereby protecting the team from the deleterious influence of AI error. We tested this in a VR drone surveillance task where teams of operators faced high workload and systematically misleading AI cues, comparing traditional behaviour-based team strategies against a purely Neuro-Decoupled Team (NDT) that used only BCI confidence scores derived from pre-response EEG. Under AI deception, behaviour-based teams catastrophically failed, with Majority Vote accuracy collapsing to 44%. The NDT, however, maintained 98% accuracy, a statistically significant synergistic gain over even the team's best individual performer (p < .001). This was explained by a neuro-behavioural decoupling, where the BCI's predictions remained highly accurate while the operator's subjective confidence became an unreliable signal. We conclude that an implicit BCI provides resilience by learning to adapt its neural strategy, shifting from relying on signals of efficient, autopilot processing in simple conditions to interpreting signatures of effortful deliberation when confronted with cognitive conflict. This demonstrates a system that leverages the context of the neural signal to defend against AI-induced error in high-stakes environments.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8e\u795e\u7ecf\u6d3b\u52a8\u7684\u534f\u4f5c\u8111\u673a\u63a5\u53e3\u80fd\u591f\u5728\u9ad8\u8ba4\u77e5\u8d1f\u8f7d\u548cAI\u8bef\u5bfc\u7684\u60c5\u51b5\u4e0b\u4fdd\u62a4\u4eba\u673a\u56e2\u961f\uff0c\u907f\u514d\u6781\u7aef\u51b3\u7b56\u9519\u8bef\uff0c\u663e\u793a\u51fa\u5176\u5728\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u7531\u4e8e\u4f20\u7edf\u56e2\u961f\u805a\u5408\u65b9\u6cd5\u6613\u53d7AI\u9519\u8bef\u5f71\u54cd\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u5f0f\u4ee5\u589e\u5f3a\u4eba\u673a\u56e2\u961f\u5728\u9ad8\u8ba4\u77e5\u8d1f\u8f7d\u4e0b\u7684\u51b3\u7b56\u80fd\u529b\u3002", "method": "\u901a\u8fc7VR\u65e0\u4eba\u673a\u76d1\u89c6\u4efb\u52a1\uff0c\u6bd4\u8f83\u4f20\u7edf\u884c\u4e3a\u57fa\u7840\u7b56\u7565\u4e0e\u57fa\u4e8eBCI\u4fe1\u5fc3\u8bc4\u5206\u7684\u795e\u7ecf\u53bb\u8026\u5408\u56e2\u961f\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4f20\u7edf\u7684\u56e2\u961f\u805a\u5408\u65b9\u6cd5\u5728\u9762\u5bf9\u9519\u8bef\u7684AI\u53cd\u9988\u65f6\u5bb9\u6613\u5bfc\u81f4\u707e\u96be\u6027\u5931\u8d25\uff0c\u7279\u522b\u662f\u5728\u9ad8\u8ba4\u77e5\u8d1f\u8f7d\u4e0b\u3002\u800c\u57fa\u4e8e\u795e\u7ecf\u6d3b\u52a8\u7684\u534f\u4f5c\u8111\u673a\u63a5\u53e3\uff08cBCI\uff09\u80fd\u591f\u63d0\u4f9b\u53bb\u504f\u89c1\u7684\u4fe1\u606f\uff0c\u4ece\u800c\u4fdd\u62a4\u56e2\u961f\u514d\u906dAI\u9519\u8bef\u7684\u7834\u574f\u6027\u5f71\u54cd\u3002\u5b9e\u9a8c\u4e2d\uff0c\u91c7\u7528VR\u65e0\u4eba\u673a\u76d1\u89c6\u4efb\u52a1\uff0c\u6bd4\u8f83\u4e86\u57fa\u4e8e\u884c\u4e3a\u7684\u4f20\u7edf\u56e2\u961f\u7b56\u7565\u4e0e\u4ec5\u4f7f\u7528BCI\u4fe1\u5fc3\u8bc4\u5206\u7684\u795e\u7ecf\u53bb\u8026\u5408\u56e2\u961f\uff08NDT\uff09\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4f20\u7edf\u56e2\u961f\u5728AI\u6b3a\u9a97\u4e0b\u8868\u73b0\u7cdf\u7cd5\uff0c\u800cNDT\u5728\u51c6\u786e\u6027\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u9690\u5f0fBCI\u901a\u8fc7\u5b66\u4e60\u9002\u5e94\u5176\u795e\u7ecf\u7b56\u7565\uff0c\u5728\u9762\u4e34\u8ba4\u77e5\u51b2\u7a81\u65f6\u6709\u6548\u62b5\u5fa1AI\u5f15\u8d77\u7684\u9519\u8bef\uff0c\u5c55\u73b0\u51fa\u5176\u5728\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2511.18085", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18085", "abs": "https://arxiv.org/abs/2511.18085", "authors": ["Yuxuan Wu", "Guangming Wang", "Zhiheng Yang", "Maoqing Yao", "Brian Sheil", "Hesheng Wang"], "title": "Continually Evolving Skill Knowledge in Vision Language Action Model", "comment": null, "summary": "Developing general robot intelligence in open environments requires continual skill learning. Recent Vision-Language-Action (VLA) models leverage massive pretraining data to support diverse manipulation tasks, but they still depend heavily on task-specific fine-tuning, revealing a lack of continual learning capability. Existing continual learning methods are also resource-intensive to scale to VLA models. We propose Stellar VLA, a knowledge-driven continual learning framework with two variants: T-Stellar, modeling task-centric knowledge space, and TS-Stellar, capturing hierarchical task-skill structure. Stellar VLA enables self-supervised knowledge evolution through joint learning of task latent representation and the knowledge space, reducing annotation needs. Knowledge-guided expert routing provide task specialization without extra network parameters, lowering training overhead.Experiments on the LIBERO benchmark and real-world tasks show over 50 percentage average improvement in final success rates relative to baselines. TS-Stellar further excels in complex action inference, and in-depth analyses verify effective knowledge retention and discovery. Our code will be released soon.", "AI": {"tldr": "Stellar VLA\u901a\u8fc7\u77e5\u8bc6\u9a71\u52a8\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\u63d0\u5347\u673a\u5668\u4eba\u5728\u5f00\u653e\u73af\u5883\u4e2d\u7684\u6280\u80fd\u5b66\u4e60\u80fd\u529b\uff0c\u5c55\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u6539\u8fdb\u548c\u6709\u6548\u7684\u77e5\u8bc6\u4fdd\u7559\u3002", "motivation": "\u673a\u5668\u4eba\u667a\u80fd\u5316\u7684\u6301\u7eed\u6280\u80fd\u5b66\u4e60\u9700\u8981\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u5728\u5f00\u653e\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u80fd\u529b\uff0c\u800c\u73b0\u6709\u7684VLA\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u548c\u8d44\u6e90\u6d88\u8017\u4e0a\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86T-Stellar\u548cTS-Stellar\u4e24\u79cd\u53d8\u4f53\uff0c\u4ee5\u5b9e\u73b0\u4efb\u52a1\u77e5\u8bc6\u7684\u5efa\u6a21\u548c\u6280\u80fd\u7ed3\u6784\u7684\u6355\u6349\uff0c\u7ed3\u5408\u81ea\u76d1\u7763\u5b66\u4e60\u964d\u4f4e\u6ce8\u91ca\u9700\u6c42\u3002", "result": "Stellar VLA\u662f\u4e00\u4e2a\u77e5\u8bc6\u9a71\u52a8\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u9488\u5bf9\u673a\u5668\u4eba\u5728\u5f00\u653e\u73af\u5883\u4e2d\u7684\u6280\u80fd\u5b66\u4e60\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4efb\u52a1\u4e2d\u5fc3\u7684\u77e5\u8bc6\u7a7a\u95f4\u6a21\u578b\u548c\u5c42\u6b21\u4efb\u52a1-\u6280\u80fd\u7ed3\u6784\u6355\u6349\u3002", "conclusion": "Stellar VLA\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6210\u529f\u7387\uff0c\u5e76\u5728\u77e5\u8bc6\u4fdd\u7559\u548c\u53d1\u73b0\u65b9\u9762\u53d6\u5f97\u4e86\u6709\u6548\u7684\u8fdb\u5c55\u3002"}}
{"id": "2511.18086", "categories": ["cs.RO", "cs.NI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.18086", "abs": "https://arxiv.org/abs/2511.18086", "authors": ["Miguel Louren\u00e7o", "Ant\u00f3nio Grilo"], "title": "Anti-Jamming based on Null-Steering Antennas and Intelligent UAV Swarm Behavior", "comment": "10 pages", "summary": "Unmanned Aerial Vehicle (UAV) swarms represent a key advancement in autonomous systems, enabling coordinated missions through inter-UAV communication. However, their reliance on wireless links makes them vulnerable to jamming, which can disrupt coordination and mission success. This work investigates whether a UAV swarm can effectively overcome jamming while maintaining communication and mission efficiency.\n  To address this, a unified optimization framework combining Genetic Algorithms (GA), Supervised Learning (SL), and Reinforcement Learning (RL) is proposed. The mission model, structured into epochs and timeslots, allows dynamic path planning, antenna orientation, and swarm formation while progressively enforcing collision rules. Null-steering antennas enhance resilience by directing antenna nulls toward interference sources.\n  Results show that the GA achieved stable, collision-free trajectories but with high computational cost. SL models replicated GA-based configurations but struggled to generalize under dynamic or constrained settings. RL, trained via Proximal Policy Optimization (PPO), demonstrated adaptability and real-time decision-making with consistent communication and lower computational demand. Additionally, the Adaptive Movement Model generalized UAV motion to arbitrary directions through a rotation-based mechanism, validating the scalability of the proposed system.\n  Overall, UAV swarms equipped with null-steering antennas and guided by intelligent optimization algorithms effectively mitigate jamming while maintaining communication stability, formation cohesion, and collision safety. The proposed framework establishes a unified, flexible, and reproducible basis for future research on resilient swarm communication systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u6846\u67b6\uff0c\u805a\u7126\u4e8e\u5728\u5e72\u6270\u60c5\u51b5\u4e0b\u7ef4\u6301\u65e0\u4eba\u673a\u7fa4\u7684\u901a\u4fe1\u6548\u7387\u4e0e\u4efb\u52a1\u6267\u884c\u3002", "motivation": "\u63a2\u8ba8\u65e0\u4eba\u673a\u7fa4\u5728\u6267\u884c\u4efb\u52a1\u65f6\u5982\u4f55\u514b\u670d\u5e72\u6270\uff0c\u4ee5\u786e\u4fdd\u901a\u4fe1\u548c\u4efb\u52a1\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u9057\u4f20\u7b97\u6cd5\u3001\u76d1\u7763\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u7edf\u4e00\u4f18\u5316\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u89c4\u5212\u3001\u5929\u7ebf\u65b9\u5411\u8c03\u6574\u53ca\u7fa4\u4f53\u5f62\u6210\u7b56\u7565\u6765\u5e94\u5bf9\u5e72\u6270\u3002", "result": "\u65e0\u4eba\u673a\u7fa4\u5728\u534f\u8c03\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u4f46\u5176\u4f9d\u8d56\u65e0\u7ebf\u94fe\u63a5\u4f7f\u5176\u6613\u53d7\u5e72\u6270\u3002\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5728\u53d7\u5230\u5e72\u6270\u7684\u60c5\u51b5\u4e0b\uff0c\u4fdd\u6301\u65e0\u4eba\u673a\u95f4\u7684\u901a\u4fe1\u6548\u7387\u548c\u4efb\u52a1\u6267\u884c\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9057\u4f20\u7b97\u6cd5\u3001\u76d1\u7763\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u7edf\u4e00\u4f18\u5316\u6846\u67b6\uff0c\u4ee5\u5e94\u5bf9\u5e72\u6270\u95ee\u9898\u3002\u7ed3\u679c\u8868\u660e\uff0c\u9057\u4f20\u7b97\u6cd5\u53ef\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u7684\u65e0\u78b0\u649e\u8f68\u8ff9\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u8f83\u9ad8\uff1b\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u5728\u52a8\u6001\u6216\u8005\u53d7\u9650\u8bbe\u7f6e\u4e0b\u96be\u4ee5\u6cdb\u5316\uff1b\u5f3a\u5316\u5b66\u4e60\u901a\u8fc7\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u8bad\u7ec3\uff0c\u8868\u73b0\u51fa\u9002\u5e94\u6027\u548c\u5b9e\u65f6\u51b3\u7b56\u80fd\u529b\uff0c\u540c\u65f6\u5177\u6709\u8f83\u4f4e\u7684\u8ba1\u7b97\u9700\u6c42\u3002\u6700\u7ec8\uff0c\u88c5\u5907\u96f6\u6307\u5411\u5929\u7ebf\u7684\u65e0\u4eba\u673a\u7fa4\u5728\u667a\u80fd\u4f18\u5316\u7b97\u6cd5\u7684\u6307\u5bfc\u4e0b\uff0c\u80fd\u6709\u6548\u62b5\u5fa1\u5e72\u6270\uff0c\u5e76\u7ef4\u6301\u901a\u4fe1\u7a33\u5b9a\u6027\u3001\u961f\u5f62\u4e00\u81f4\u6027\u548c\u78b0\u649e\u5b89\u5168\u6027\u3002\u8be5\u6846\u67b6\u4e3a\u672a\u6765\u7684\u97e7\u6027\u7fa4\u4f53\u901a\u4fe1\u7cfb\u7edf\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u7ed3\u5408\u96f6\u6307\u5411\u5929\u7ebf\u548c\u667a\u80fd\u4f18\u5316\u7b97\u6cd5\u7684\u65e0\u4eba\u673a\u7fa4\u80fd\u6709\u6548\u62b5\u6297\u5e72\u6270\uff0c\u4fdd\u6301\u901a\u4fe1\u7a33\u5b9a\u6027\u53ca\u78b0\u649e\u5b89\u5168\u3002"}}
{"id": "2511.18088", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18088", "abs": "https://arxiv.org/abs/2511.18088", "authors": ["Ibrahim Alsarraj", "Yuhao Wang", "Abdalla Swikir", "Cesare Stefanini", "Dezhen Song", "Zhanchi Wang", "Ke Wu"], "title": "A Unified Multi-Dynamics Framework for Perception-Oriented Modeling in Tendon-Driven Continuum Robots", "comment": null, "summary": "Tendon-driven continuum robots offer intrinsically safe and contact-rich interactions owing to their kinematic redundancy and structural compliance. However, their perception often depends on external sensors, which increase hardware complexity and limit scalability. This work introduces a unified multi-dynamics modeling framework for tendon-driven continuum robotic systems, exemplified by a spiral-inspired robot named Spirob. The framework integrates motor electrical dynamics, motor-winch dynamics, and continuum robot dynamics into a coherent system model. Within this framework, motor signals such as current and angular displacement are modeled to expose the electromechanical signatures of external interactions, enabling perception grounded in intrinsic dynamics. The model captures and validates key physical behaviors of the real system, including actuation hysteresis and self-contact at motion limits. Building on this foundation, the framework is applied to environmental interaction: first for passive contact detection, verified experimentally against simulation data; then for active contact sensing, where control and perception strategies from simulation are successfully applied to the real robot; and finally for object size estimation, where a policy learned in simulation is directly deployed on hardware. The results demonstrate that the proposed framework provides a physically grounded way to interpret interaction signatures from intrinsic motor signals in tendon-driven continuum robots.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u5efa\u6a21\u6846\u67b6\uff0c\u80fd\u591f\u901a\u8fc7\u5185\u90e8\u52a8\u529b\u5b66\u5b9e\u73b0\u808c\u8171\u9a71\u52a8\u7684\u8fde\u7eed\u673a\u5668\u4eba\u81ea\u6211\u611f\u77e5\uff0c\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u4f20\u611f\u5668\u3002", "motivation": "\u4f20\u7edf\u5916\u90e8\u4f20\u611f\u5668\u589e\u52a0\u4e86\u786c\u4ef6\u590d\u6742\u6027\u5e76\u9650\u5236\u4e86\u6269\u5c55\u6027\uff0c\u800c\u8be5\u6846\u67b6\u65e8\u5728\u901a\u8fc7\u5185\u90e8\u4fe1\u53f7\u63d0\u9ad8\u673a\u5668\u4eba\u7684\u611f\u77e5\u80fd\u529b\uff0c\u7b80\u5316\u786c\u4ef6\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u6574\u5408\u7535\u52a8\u673a\u3001\u5377\u7ebf\u673a\u53ca\u8fde\u7eed\u673a\u5668\u4eba\u7684\u52a8\u529b\u5b66\u7279\u6027\uff0c\u6784\u5efa\u4e00\u81f4\u7684\u7cfb\u7edf\u6a21\u578b\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u9a8c\u8bc1\u4e86\u88ab\u52a8\u548c\u4e3b\u52a8\u63a5\u89e6\u68c0\u6d4b\u7684\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u591a\u52a8\u529b\u5b66\u5efa\u6a21\u6846\u67b6\uff0c\u7528\u4e8e\u808c\u8171\u9a71\u52a8\u7684\u8fde\u7eed\u673a\u5668\u4eba\uff0c\u7ed3\u5408\u4e86\u7535\u52a8\u673a\u7535\u6c14\u52a8\u6001\u3001\u5377\u7ebf\u673a\u52a8\u6001\u548c\u8fde\u7eed\u673a\u5668\u4eba\u52a8\u6001\uff0c\u4f7f\u5f97\u673a\u5668\u4eba\u80fd\u81ea\u4e3b\u611f\u77e5\u5916\u90e8\u4ea4\u4e92\u3002", "conclusion": "\u8fd9\u4e00\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u4ee5\u7269\u7406\u4e3a\u57fa\u7840\u7684\u89e3\u91ca\u65b9\u6cd5\uff0c\u80fd\u591f\u4ece\u808c\u8171\u9a71\u52a8\u673a\u5668\u4eba\u5185\u90e8\u7535\u673a\u4fe1\u53f7\u4e2d\u63d0\u53d6\u4ea4\u4e92\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u7684\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2511.18112", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18112", "abs": "https://arxiv.org/abs/2511.18112", "authors": ["Min Lin", "Xiwen Liang", "Bingqian Lin", "Liu Jingzhi", "Zijian Jiao", "Kehan Li", "Yuhan Ma", "Yuecheng Liu", "Shen Zhao", "Yuzheng Zhuang", "Xiaodan Liang"], "title": "EchoVLA: Robotic Vision-Language-Action Model with Synergistic Declarative Memory for Mobile Manipulation", "comment": null, "summary": "Recent progress in Vision-Language-Action (VLA) models has enabled embodied agents to interpret multimodal instructions and perform complex tasks. However, existing VLAs are mostly confined to short-horizon, table-top manipulation, lacking the memory and reasoning capability required for long-horizon mobile manipulation, where agents must coordinate navigation and manipulation under changing spatial contexts. In this work, we present EchoVLA, a memory-aware VLA model for long-horizon mobile manipulation. EchoVLA incorporates a synergistic declarative memory inspired by the human brain, consisting of a scene memory that maintains a collection of spatial-semantic maps and an episodic memory that stores task-level experiences with multimodal contextual features. During both training and inference, the two memories are individually stored, updated, and retrieved based on current observations, task history, and instructions, and their retrieved representations are fused via coarse- and fine-grained attention to guide mobile-arm diffusion policies. To support large-scale training and evaluation, we further introduce MoMani, an automated benchmark that generates expert-level long-horizon trajectories through multimodal large language model (MLLM)-guided planning and feedback-driven refinement, supplemented with real-robot demonstrations. Experiments in simulated and real-world settings show that EchoVLA improves long-horizon performance, reaching 0.52 SR on manipulation/navigation and 0.31 on mobile manipulation, exceeding $\u03c0_{0.5}$ by +0.08 and +0.11.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86EchoVLA\uff0c\u4e00\u79cd\u9002\u7528\u4e8e\u957f\u671f\u79fb\u52a8\u64cd\u63a7\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u5229\u7528\u4eba\u8111\u542f\u53d1\u7684\u8bb0\u5fc6\u7ed3\u6784\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u4e3b\u8981\u5c40\u9650\u4e8e\u77ed\u671f\u7684\u684c\u9762\u64cd\u63a7\uff0c\u7f3a\u4e4f\u5e94\u5bf9\u957f\u671f\u79fb\u52a8\u64cd\u63a7\u4e2d\u590d\u6742\u73af\u5883\u53d8\u5316\u7684\u8bb0\u5fc6\u548c\u63a8\u7406\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5904\u7406\u957f\u671f\u4efb\u52a1\u7684\u6a21\u578b\u3002", "method": "EchoVLA\u7ed3\u5408\u4e86\u573a\u666f\u8bb0\u5fc6\u548c\u60c5\u8282\u8bb0\u5fc6\uff0c\u901a\u8fc7\u7c97\u7ec6\u7c92\u5ea6\u7684\u6ce8\u610f\u529b\u673a\u5236\u6765\u5f15\u5bfc\u79fb\u52a8\u81c2\u6269\u6563\u7b56\u7565\uff0c\u540c\u65f6\u5f15\u5165MoMani\u57fa\u51c6\u8fdb\u884c\u5927\u89c4\u6a21\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "EchoVLA\u6a21\u578b\u5728\u957f\u671f\u79fb\u52a8\u64cd\u63a7\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u5c55\u793a\u4e86\u5f3a\u5316\u7684\u8bb0\u5fc6\u80fd\u529b\u548c\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cEchoVLA\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5747\u63d0\u5347\u4e86\u957f\u671f\u64cd\u63a7\u7684\u8868\u73b0\uff0c\u5c24\u5176\u5728\u4efb\u52a1\u6210\u529f\u7387\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002"}}
{"id": "2511.18140", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18140", "abs": "https://arxiv.org/abs/2511.18140", "authors": ["Yilong Wang", "Cheng Qian", "Ruomeng Fan", "Edward Johns"], "title": "Observer Actor: Active Vision Imitation Learning with Sparse View Gaussian Splatting", "comment": "Videos are available on our project webpage at https://obact.github.io", "summary": "We propose Observer Actor (ObAct), a novel framework for active vision imitation learning in which the observer moves to optimal visual observations for the actor. We study ObAct on a dual-arm robotic system equipped with wrist-mounted cameras. At test time, ObAct dynamically assigns observer and actor roles: the observer arm constructs a 3D Gaussian Splatting (3DGS) representation from three images, virtually explores this to find an optimal camera pose, then moves to this pose; the actor arm then executes a policy using the observer's observations. This formulation enhances the clarity and visibility of both the object and the gripper in the policy's observations. As a result, we enable the training of ambidextrous policies on observations that remain closer to the occlusion-free training distribution, leading to more robust policies. We study this formulation with two existing imitation learning methods -- trajectory transfer and behavior cloning -- and experiments show that ObAct significantly outperforms static-camera setups: trajectory transfer improves by 145% without occlusion and 233% with occlusion, while behavior cloning improves by 75% and 143%, respectively. Videos are available at https://obact.github.io.", "AI": {"tldr": "ObAct \u6846\u67b6\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u89c2\u5bdf\u8005\u4e0e\u6267\u884c\u8005\u7684\u89d2\u8272\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u53cc\u81c2\u673a\u5668\u4eba\u5728\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u5904\u7406\u906e\u6321\u7684\u60c5\u51b5\u4e0b\u4f18\u8d8a\u3002", "motivation": "\u65e8\u5728\u4f18\u5316\u673a\u5668\u4eba\u5728\u6267\u884c\u4efb\u52a1\u65f6\u7684\u89c6\u89c9\u89c2\u5bdf\uff0c\u4ee5\u63d0\u9ad8\u6a21\u4eff\u5b66\u4e60\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53cc\u81c2\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u5229\u7528\u624b\u8155\u5b89\u88c5\u7684\u76f8\u673a\u52a8\u6001\u5206\u914d\u89c2\u5bdf\u8005\u548c\u6267\u884c\u8005\u89d2\u8272\uff0c\u901a\u8fc73D \u9ad8\u65af\u55b7\u6d8c\u8868\u793a\u6784\u5efa\u6700\u4f73\u76f8\u673a\u4f4d\u59ff\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cObAct \u5728\u4e0e\u4f20\u7edf\u9759\u6001\u76f8\u673a\u8bbe\u7f6e\u76f8\u6bd4\uff0c\u8f68\u8ff9\u8f6c\u79fb\u548c\u884c\u4e3a\u514b\u9686\u65b9\u6cd5\u7684\u8868\u73b0\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "ObAct \u6846\u67b6\u5728\u6a21\u4eff\u5b66\u4e60\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u81ea\u4e3b\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6548\u679c\uff0c\u5c24\u5176\u5728\u5904\u7406\u906e\u6321\u7684\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2511.18153", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18153", "abs": "https://arxiv.org/abs/2511.18153", "authors": ["Shreyas Kumar", "Barat S", "Debojit Das", "Yug Desai", "Siddhi Jain", "Rajesh Kumar", "Harish J. Palanthandalam-Madapusi"], "title": "A Coordinated Dual-Arm Framework for Delicate Snap-Fit Assemblies", "comment": "10 pages, 9 figures", "summary": "Delicate snap-fit assemblies, such as inserting a lens into an eye-wear frame or during electronics assembly, demand timely engagement detection and rapid force attenuation to prevent overshoot-induced component damage or assembly failure. We address these challenges with two key contributions. First, we introduce SnapNet, a lightweight neural network that detects snap-fit engagement from joint-velocity transients in real-time, showing that reliable detection can be achieved using proprioceptive signals without external sensors. Second, we present a dynamical-systems-based dual-arm coordination framework that integrates SnapNet driven detection with an event-triggered impedance modulation, enabling accurate alignment and compliant insertion during delicate snap-fit assemblies. Experiments across diverse geometries on a heterogeneous bimanual platform demonstrate high detection accuracy (over 96% recall) and up to a 30% reduction in peak impact forces compared to standard impedance control.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86SnapNet\u795e\u7ecf\u7f51\u7edc\u548c\u57fa\u4e8e\u52a8\u6001\u7cfb\u7edf\u7684\u53cc\u81c2\u534f\u8c03\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8\u7cbe\u5bc6\u914d\u5408\u88c5\u914d\u7684\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u964d\u4f4e\u51b2\u51fb\u529b\u3002", "motivation": "\u89e3\u51b3\u7cbe\u5bc6\u914d\u5408\u88c5\u914d\u4e2d\u7684\u53c2\u4e0e\u68c0\u6d4b\u548c\u5feb\u901f\u529b\u8870\u51cf\u95ee\u9898\uff0c\u4ee5\u9632\u6b62\u56e0\u8fc7\u51b2\u5bfc\u81f4\u7684\u7ec4\u4ef6\u635f\u574f\u6216\u88c5\u914d\u5931\u8d25\u3002", "method": "\u4f7f\u7528\u5185\u53cd\u9988\u4fe1\u53f7\u5b9e\u73b0\u4e86SnapNet\u7684\u5feb\u901f\u5b9e\u65f6\u68c0\u6d4b\uff0c\u5e76\u901a\u8fc7\u4e8b\u4ef6\u89e6\u53d1\u7684\u963b\u6297\u8c03\u5236\u5b9e\u73b0\u53cc\u81c2\u534f\u8c03\u3002", "result": "\u63d0\u51fa\u4e86SnapNet\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\uff0c\u80fd\u5b9e\u65f6\u68c0\u6d4b\u63a5\u5408\u8fc7\u7a0b\u4e2d\u7684\u901f\u5ea6\u53d8\u5316\uff0c\u5e76\u4e14\u901a\u8fc7\u53cc\u624b\u534f\u8c03\u6846\u67b6\u5b9e\u73b0\u7cbe\u786e\u5bf9\u9f50\u548c\u987a\u5e94\u63d2\u5165\u3002", "conclusion": "\u5b9e\u9a8c\u8868\u660e\uff0cSnapNet\u63d0\u4f9b\u4e86\u8d85\u8fc796%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u5e76\u4e14\u4e0e\u4f20\u7edf\u963b\u6297\u63a7\u5236\u76f8\u6bd4\uff0c\u5cf0\u503c\u51b2\u51fb\u529b\u964d\u4f4e\u4e8630%\u3002"}}
{"id": "2511.18170", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18170", "abs": "https://arxiv.org/abs/2511.18170", "authors": ["Kaier Liang", "Licheng Luo", "Yixuan Wang", "Mingyu Cai", "Cristian Ioan Vasile"], "title": "Time-aware Motion Planning in Dynamic Environments with Conformal Prediction", "comment": null, "summary": "Safe navigation in dynamic environments remains challenging due to uncertain obstacle behaviors and the lack of formal prediction guarantees. We propose two motion planning frameworks that leverage conformal prediction (CP): a global planner that integrates Safe Interval Path Planning (SIPP) for uncertainty-aware trajectory generation, and a local planner that performs online reactive planning. The global planner offers distribution-free safety guarantees for long-horizon navigation, while the local planner mitigates inaccuracies in obstacle trajectory predictions through adaptive CP, enabling robust and responsive motion in dynamic environments. To further enhance trajectory feasibility, we introduce an adaptive quantile mechanism in the CP-based uncertainty quantification. Instead of using a fixed confidence level, the quantile is automatically tuned to the optimal value that preserves trajectory feasibility, allowing the planner to adaptively tighten safety margins in regions with higher uncertainty. We validate the proposed framework through numerical experiments conducted in dynamic and cluttered environments. The project page is available at https://time-aware-planning.github.io", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8e\u5171\u5f62\u9884\u6d4b\u7684\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u63d0\u4f9b\u5b89\u5168\u4fdd\u969c\u5e76\u589e\u5f3a\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8fd0\u52a8\u53cd\u5e94\u80fd\u529b\u3002", "motivation": "\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b89\u5168\u5bfc\u822a\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u56e0\u969c\u788d\u7269\u884c\u4e3a\u4e0d\u786e\u5b9a\u4e14\u7f3a\u4e4f\u6b63\u5f0f\u7684\u9884\u6d4b\u4fdd\u969c\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u5229\u7528\u5171\u5f62\u9884\u6d4b\u7684\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff1a\u5168\u5c40\u89c4\u5212\u5668\u548c\u5c40\u90e8\u89c4\u5212\u5668\u3002", "result": "\u5168\u5c40\u89c4\u5212\u5668\u4e3a\u957f\u8fdc\u5bfc\u822a\u63d0\u4f9b\u4e86\u65e0\u5206\u5e03\u5b89\u5168\u4fdd\u969c\uff0c\u800c\u5c40\u90e8\u89c4\u5212\u5668\u901a\u8fc7\u81ea\u9002\u5e94\u5171\u5f62\u9884\u6d4b\u51cf\u8f7b\u4e86\u969c\u788d\u7269\u8f68\u8ff9\u9884\u6d4b\u7684\u4e0d\u51c6\u786e\u6027\u3002", "conclusion": "\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u6240\u63d0\u6846\u67b6\u5728\u52a8\u6001\u548c\u62e5\u6324\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.18183", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18183", "abs": "https://arxiv.org/abs/2511.18183", "authors": ["Yixuan Jia", "Qingyuan Li", "Jonathan P. How"], "title": "Off-Road Navigation via Implicit Neural Representation of Terrain Traversability", "comment": "9 pages", "summary": "Autonomous off-road navigation requires robots to estimate terrain traversability from onboard sensors and plan accordingly. Conventional approaches typically rely on sampling-based planners such as MPPI to generate short-term control actions that aim to minimize traversal time and risk measures derived from the traversability estimates. These planners can react quickly but optimize only over a short look-ahead window, limiting their ability to reason about the full path geometry, which is important for navigating in challenging off-road environments. Moreover, they lack the ability to adjust speed based on the terrain bumpiness, which is important for smooth navigation on challenging terrains. In this paper, we introduce TRAIL (Traversability with an Implicit Learned Representation), an off-road navigation framework that leverages an implicit neural representation to continuously parameterize terrain properties. This representation yields spatial gradients that enable integration with a novel gradient-based trajectory optimization method that adapts the path geometry and speed profile based on terrain traversability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u8fdb\u884c\u8d8a\u91ce\u5bfc\u822a\u7684\u65b0\u6846\u67b6TRAIL\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u91c7\u6837\u89c4\u5212\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u80fd\u6839\u636e\u5730\u5f62\u53ef\u7a7f\u900f\u6027\u8c03\u6574\u8def\u5f84\u548c\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u6311\u6218\u6027\u8d8a\u91ce\u73af\u5883\u65f6\uff0c\u65e0\u6cd5\u5168\u9762\u8003\u8651\u8def\u5f84\u51e0\u4f55\u548c\u5730\u5f62\u5d0e\u5c96\u5ea6\u5bf9\u901f\u5ea6\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u5bf9\u5730\u5f62\u7279\u6027\u8fdb\u884c\u53c2\u6570\u5316\uff0c\u7ed3\u5408\u65b0\u7684\u57fa\u4e8e\u68af\u5ea6\u7684\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\uff0c\u8c03\u6574\u8def\u5f84\u51e0\u4f55\u548c\u901f\u5ea6\u5256\u9762\u3002", "result": "TRAIL\u6846\u67b6\u80fd\u591f\u5728\u8d8a\u91ce\u5bfc\u822a\u4e2d\u5b9e\u73b0\u66f4\u6d41\u7545\u7684\u901f\u5ea6\u8c03\u8282\u548c\u4f18\u5316\u7684\u8def\u5f84\u9009\u62e9\u3002", "conclusion": "TRAIL\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u548c\u57fa\u4e8e\u68af\u5ea6\u7684\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d8a\u91ce\u5bfc\u822a\u4e2d\u8def\u5f84\u51e0\u4f55\u548c\u901f\u5ea6\u8c03\u8282\u7684\u80fd\u529b\u3002"}}
{"id": "2511.18203", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18203", "abs": "https://arxiv.org/abs/2511.18203", "authors": ["Ziyi Yang", "Benned Hedegaard", "Ahmed Jaafar", "Yichen Wei", "Skye Thompson", "Shreyas S. Raman", "Haotian Fu", "Stefanie Tellex", "George Konidaris", "David Paulius", "Naman Shah"], "title": "SkillWrapper: Generative Predicate Invention for Skill Abstraction", "comment": null, "summary": "Generalizing from individual skill executions to solving long-horizon tasks remains a core challenge in building autonomous agents. A promising direction is learning high-level, symbolic abstractions of the low-level skills of the agents, enabling reasoning and planning independent of the low-level state space. Among possible high-level representations, object-centric skill abstraction with symbolic predicates has been proven to be efficient because of its compatibility with domain-independent planners. Recent advances in foundation models have made it possible to generate symbolic predicates that operate on raw sensory inputs, a process we call generative predicate invention, to facilitate downstream abstraction learning. However, it remains unclear which formal properties the learned representations must satisfy, and how they can be learned to guarantee these properties. In this paper, we address both questions by presenting a formal theory of generative predicate invention for skill abstraction, resulting in symbolic operators that can be used for provably sound and complete planning. Within this framework, we propose SkillWrapper, a method that leverages foundation models to actively collect robot data and learn human-interpretable, plannable representations of black-box skills, using only RGB image observations. Our extensive empirical evaluation in simulation and on real robots shows that SkillWrapper learns abstract representations that enable solving unseen, long-horizon tasks in the real world with black-box skills.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e00\u5957\u7406\u8bba\u548c\u65b9\u6cd5\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u901a\u8fc7\u9ad8\u5c42\u6b21\u7684\u7b26\u53f7\u62bd\u8c61\uff0c\u4ece\u800c\u6539\u5584\u5176\u5728\u957f\u671f\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4f7f\u7528RGB\u56fe\u50cf\u89c2\u5bdf\u8ba9\u6280\u80fd\u66f4\u5177\u53ef\u7406\u89e3\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4ece\u4e2a\u4f53\u6280\u80fd\u6267\u884c\u5230\u957f\u671f\u4efb\u52a1\u89e3\u51b3\u7684\u6311\u6218\uff0c\u901a\u8fc7\u751f\u6210\u5f0f\u8c13\u8bcd\u53d1\u660e\u5b66\u4e60\u9ad8\u5c42\u6b21\u6280\u80fd\u62bd\u8c61\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u5f0f\u8c13\u8bcd\u53d1\u660e\u7684\u6b63\u5f0f\u7406\u8bba\uff0c\u5e76\u5b9e\u65bd\u4e86SkillWrapper\uff0c\u901a\u8fc7\u6536\u96c6\u673a\u5668\u4eba\u7684\u6570\u636e\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u5b66\u4e60\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u53ef\u89c4\u5212\u8868\u793a\u3002", "result": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSkillWrapper\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5f0f\u8c13\u8bcd\u53d1\u660e\u5b9e\u73b0\u673a\u5668\u4eba\u6280\u80fd\u7684\u62bd\u8c61\uff0c\u4ece\u800c\u5728\u590d\u6742\u7684\u957f\u671f\u4efb\u52a1\u4e2d\u63d0\u9ad8\u81ea\u4e3b\u4ee3\u7406\u7684\u80fd\u529b\u3002", "conclusion": "SkillWrapper\u80fd\u591f\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u89e3\u51b3\u672a\u89c1\u8fc7\u7684\u957f\u671f\u4efb\u52a1\uff0c\u663e\u793a\u51fa\u5176\u5f3a\u5927\u7684\u80fd\u529b\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u6280\u80fd\u62bd\u8c61\u548c\u81ea\u4e3b\u5b66\u4e60\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2511.18215", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18215", "abs": "https://arxiv.org/abs/2511.18215", "authors": ["Shangyuan Yuan", "Preston Fairchild", "Yu Mei", "Xinyu Zhou", "Xiaobo Tan"], "title": "AFT: Appearance-Based Feature Tracking for Markerless and Training-Free Shape Reconstruction of Soft Robots", "comment": null, "summary": "Accurate shape reconstruction is essential for precise control and reliable operation of soft robots. Compared to sensor-based approaches, vision-based methods offer advantages in cost, simplicity, and ease of deployment. However, existing vision-based methods often rely on complex camera setups, specific backgrounds, or large-scale training datasets, limiting their practicality in real-world scenarios. In this work, we propose a vision-based, markerless, and training-free framework for soft robot shape reconstruction that directly leverages the robot's natural surface appearance. These surface features act as implicit visual markers, enabling a hierarchical matching strategy that decouples local partition alignment from global kinematic optimization. Requiring only an initial 3D reconstruction and kinematic alignment, our method achieves real-time shape tracking across diverse environments while maintaining robustness to occlusions and variations in camera viewpoints. Experimental validation on a continuum soft robot demonstrates an average tip error of 2.6% during real-time operation, as well as stable performance in practical closed-loop control tasks. These results highlight the potential of the proposed approach for reliable, low-cost deployment in dynamic real-world settings.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u89c9\u57fa\u7840\u8f6f\u673a\u5668\u4eba\u5f62\u72b6\u91cd\u5efa\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u53ef\u7528\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u73b0\u6709\u89c6\u89c9\u65b9\u6cd5\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8fd0\u7528\u7684\u5c40\u9650\u6027\uff0c\u5f00\u53d1\u4e00\u79cd\u7b80\u4fbf\u4e14\u9ad8\u6548\u7684\u8f6f\u673a\u5668\u4eba\u5f62\u72b6\u91cd\u5efa\u6280\u672f\u3002", "method": "\u8be5\u6846\u67b6\u4ec5\u9700\u521d\u6b65\u76843D\u91cd\u5efa\u548c\u8fd0\u52a8\u5b66\u5bf9\u9f50\uff0c\u91c7\u7528\u5206\u5c42\u5339\u914d\u7b56\u7565\uff0c\u5c06\u5c40\u90e8\u5206\u533a\u5bf9\u9f50\u4e0e\u5168\u5c40\u8fd0\u52a8\u5b66\u4f18\u5316\u89e3\u8026\uff0c\u4ece\u800c\u5b9e\u73b0\u5b9e\u65f6\u5f62\u72b6\u8ddf\u8e2a\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u3001\u65e0\u6807\u8bb0\u3001\u65e0\u8bad\u7ec3\u7684\u8f6f\u673a\u5668\u4eba\u5f62\u72b6\u91cd\u5efa\u6846\u67b6\uff0c\u5229\u7528\u673a\u5668\u4eba\u7684\u81ea\u7136\u8868\u9762\u7279\u5f81\u8fdb\u884c\u53ef\u9760\u7684\u5f62\u72b6\u8ddf\u8e2a\uff0c\u4e14\u5728\u5404\u79cd\u73af\u5883\u4e2d\u8868\u73b0\u7a33\u5b9a\u3002", "conclusion": "\u672c\u7814\u7a76\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u5b9e\u65f6\u64cd\u4f5c\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\uff0c\u5177\u6709\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u53ef\u9760\u3001\u4f4e\u6210\u672c\u90e8\u7f72\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.18236", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.18236", "abs": "https://arxiv.org/abs/2511.18236", "authors": ["Nuno Soares", "Ant\u00f3nio Grilo"], "title": "APULSE: A Scalable Hybrid Algorithm for the RCSPP on Large-Scale Dense Graphs", "comment": "9 pages", "summary": "The resource-constrained shortest path problem (RCSPP) is a fundamental NP-hard optimization challenge with broad applications, from network routing to autonomous navigation. This problem involves finding a path that minimizes a primary cost subject to a budget on a secondary resource. While various RCSPP solvers exist, they often face critical scalability limitations when applied to the large, dense graphs characteristic of complex, real-world scenarios, making them impractical for time-critical planning. This challenge is particularly acute in domains like mission planning for unmanned ground vehicles (UGVs), which demand solutions on large-scale terrain graphs. This paper introduces APULSE, a hybrid label-setting algorithm designed to efficiently solve the RCSPP on such challenging graphs. APULSE integrates a best-first search guided by an A* heuristic with aggressive, Pulse-style pruning mechanisms and a time-bucketing strategy for effective state-space reduction. A computational study, using a large-scale UGV planning scenario, benchmarks APULSE against state-of-the-art algorithms. The results demonstrate that APULSE consistently finds near-optimal solutions while being orders of magnitude faster and more robust, particularly on large problem instances where competing methods fail. This superior scalability establishes APULSE as an effective solution for RCSPP in complex, large-scale environments, enabling capabilities such as interactive decision support and dynamic replanning.", "AI": {"tldr": "APULSE\u7b97\u6cd5\u901a\u8fc7\u7ed3\u5408A*\u542f\u53d1\u5f0f\u641c\u7d22\u548c\u8109\u51b2\u5f0f\u4fee\u526a\u673a\u5236\uff0c\u89e3\u51b3\u5927\u89c4\u6a21\u8d44\u6e90\u7ea6\u675f\u6700\u77ed\u8def\u5f84\u95ee\u9898\uff0c\u5c24\u5176\u5728\u65e0\u4eba\u5730\u9762\u8f66\u8f86\u4efb\u52a1\u89c4\u5212\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u7684RCSPP\u6c42\u89e3\u5668\u5728\u5904\u7406\u590d\u6742\u771f\u5b9e\u573a\u666f\u7684\u5927\u89c4\u6a21\u7a20\u5bc6\u56fe\u65f6\u5b58\u5728\u4e25\u91cd\u7684\u53ef\u6269\u5c55\u6027\u9650\u5236\uff0c\u8feb\u5207\u9700\u8981\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5\u6765\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u6700\u4f73\u4f18\u5148\u641c\u7d22\u548c\u65f6\u95f4\u5206\u7bb1\u7b56\u7565\uff0c\u901a\u8fc7\u8109\u51b2\u5f0f\u4fee\u526a\u673a\u5236\u51cf\u5c11\u72b6\u6001\u7a7a\u95f4\uff0c\u6709\u6548\u63d0\u5347\u7b97\u6cd5\u6548\u7387\u3002", "result": "APULSE\u662f\u4e00\u79cd\u6df7\u5408\u6807\u7b7e\u8bbe\u7f6e\u7b97\u6cd5\uff0c\u65e8\u5728\u6709\u6548\u89e3\u51b3\u8d44\u6e90\u7ea6\u675f\u6700\u77ed\u8def\u5f84\u95ee\u9898\u3002", "conclusion": "APULSE\u5728\u5904\u7406\u590d\u6742\u548c\u5927\u89c4\u6a21\u73af\u5883\u4e0b\u7684\u8d44\u6e90\u7ea6\u675f\u6700\u77ed\u8def\u5f84\u95ee\u9898\u65f6\uff0c\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u53ef\u6269\u5c55\u6027\u548c\u4f18\u8d8a\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u5408\u4e8e\u52a8\u6001\u51b3\u7b56\u652f\u6301\u548c\u91cd\u89c4\u5212\u3002"}}
{"id": "2511.18243", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18243", "abs": "https://arxiv.org/abs/2511.18243", "authors": ["Eashan Vytla", "Bhavanishankar Kalavakolanu", "Andrew Perrault", "Matthew McCrink"], "title": "Dreaming Falcon: Physics-Informed Model-Based Reinforcement Learning for Quadcopters", "comment": null, "summary": "Current control algorithms for aerial robots struggle with robustness in dynamic environments and adverse conditions. Model-based reinforcement learning (RL) has shown strong potential in handling these challenges while remaining sample-efficient. Additionally, Dreamer has demonstrated that online model-based RL can be achieved using a recurrent world model trained on replay buffer data. However, applying Dreamer to aerial systems has been quite challenging due to its sample inefficiency and poor generalization of dynamics models. Our work explores a physics-informed approach to world model learning and improves policy performance. The world model treats the quadcopter as a free-body system and predicts the net forces and moments acting on it, which are then passed through a 6-DOF Runge-Kutta integrator (RK4) to predict future state rollouts. In this paper, we compare this physics-informed method to a standard RNN-based world model. Although both models perform well on the training data, we observed that they fail to generalize to new trajectories, leading to rapid divergence in state rollouts, preventing policy convergence.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7269\u7406\u4fe1\u606f\u9a71\u52a8\u7684\u56db\u65cb\u7ffc\u7cfb\u7edf\u4e16\u754c\u6a21\u578b\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u63a7\u5236\u6311\u6218\uff0c\u63d0\u5347\u4e86\u7b56\u7565\u7684\u8868\u73b0\u3002", "motivation": "\u901a\u8fc7\u589e\u5f3a\u73b0\u6709\u6a21\u578b\u7684\u6837\u672c\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u6311\u6218", "method": "\u7269\u7406\u4fe1\u606f\u9a71\u52a8\u7684\u4e16\u754c\u6a21\u578b\u5b66\u4e60", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u6a21\u578b\u7684\u4e16\u754c\u6a21\u578b\uff0c\u80fd\u591f\u63d0\u9ad8\u7b56\u7565\u8868\u73b0", "conclusion": "\u5c3d\u7ba1\u7269\u7406\u9a71\u52a8\u6a21\u578b\u5728\u8bad\u7ec3\u6570\u636e\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u65b0\u8f68\u8ff9\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u9700\u6539\u5584\uff0c\u4ee5\u5b9e\u73b0\u7b56\u7565\u7684\u6536\u655b\u3002"}}
{"id": "2511.18270", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18270", "abs": "https://arxiv.org/abs/2511.18270", "authors": ["Zhongkai Chen", "Yihao Sun", "Chao Yan", "Han Zhou", "Xiaojia Xiang", "Jie Jiang"], "title": "Skypilot: Fine-Tuning LLM with Physical Grounding for AAV Coverage Search", "comment": null, "summary": "Autonomous aerial vehicles (AAVs) have played a pivotal role in coverage operations and search missions. Recent advances in large language models (LLMs) offer promising opportunities to augment AAV intelligence. These advances help address complex challenges like area coverage optimization, dynamic path planning, and adaptive decision-making. However, the absence of physical grounding in LLMs leads to hallucination and reproducibility problems in spatial reasoning and decision-making. To tackle these issues, we present Skypilot, an LLM-enhanced two-stage framework that grounds language models in physical reality by integrating monte carlo tree search (MCTS). In the first stage, we introduce a diversified action space that encompasses generate, regenerate, fine-tune, and evaluate operations, coupled with physics-informed reward functions to ensure trajectory feasibility. In the second stage, we fine-tune Qwen3-4B on 23,000 MCTS-generated samples, achieving substantial inference acceleration while maintaining solution quality. Extensive numerical simulations and real-world flight experiments validate the efficiency and superiority of our proposed approach. Detailed information and experimental results are accessible at https://sky-pilot.top.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faSkypilot\u6846\u67b6\uff0c\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u7269\u7406\u73b0\u5b9e\u7ed3\u5408\uff0c\u4ee5\u63d0\u5347\u81ea\u4e3b\u98de\u884c\u5668\u7684\u667a\u80fd\u548c\u51b3\u7b56\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u589e\u5f3a\u81ea\u4e3b\u98de\u884c\u5668\uff08AAVs\uff09\u7684\u667a\u80fd\u4ee5\u5e94\u5bf9\u590d\u6742\u7684\u8986\u76d6\u4f18\u5316\u548c\u52a8\u6001\u8def\u5f84\u89c4\u5212\u6311\u6218\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7b2c\u4e00\u9636\u6bb5\u5f15\u5165\u591a\u6837\u5316\u884c\u52a8\u7a7a\u95f4\u548c\u7269\u7406\u5956\u52b1\uff0c\u7b2c\u4e8c\u9636\u6bb5\u5bf9Qwen3-4B\u8fdb\u884c\u5fae\u8c03\u4ee5\u63d0\u53d6MCTS\u751f\u6210\u7684\u6837\u672c\u3002", "result": "\u63d0\u51faSkypilot\uff0c\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u8499\u7279\u5361\u7f57\u6811\u641c\u7d22\uff08MCTS\uff09\u6765\u5c06\u8bed\u8a00\u6a21\u578b\u4e0e\u7269\u7406\u73b0\u5b9e\u76f8\u7ed3\u5408\u3002", "conclusion": "Skypilot\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u591a\u6837\u5316\u7684\u884c\u52a8\u7a7a\u95f4\u548c\u7269\u7406\u5956\u52b1\u51fd\u6570\uff0c\u5728\u4fdd\u7559\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u52a0\u901f\u4e86\u63a8\u7406\u8fc7\u7a0b\uff0c\u5c55\u73b0\u4e86\u4f18\u8d8a\u6027\u3002"}}
{"id": "2511.18293", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18293", "abs": "https://arxiv.org/abs/2511.18293", "authors": ["Shuai Zhang", "Jingsong Mu", "Cancan Zhao", "Leiqi Tian", "Zhijun Xing", "Bo Ouyang", "Xiang Li"], "title": "AIA-UltraNeRF:Acoustic-Impedance-Aware Neural Radiance Field with Hash Encodings for Robotic Ultrasound Reconstruction and Localization", "comment": null, "summary": "Neural radiance field (NeRF) is a promising approach for reconstruction and new view synthesis. However, previous NeRF-based reconstruction methods overlook the critical role of acoustic impedance in ultrasound imaging. Localization methods face challenges related to local minima due to the selection of initial poses. In this study, we design a robotic ultrasound system (RUSS) with an acoustic-impedance-aware ultrasound NeRF (AIA-UltraNeRF) to decouple the scanning and diagnostic processes. Specifically, AIA-UltraNeRF models a continuous function of hash-encoded spatial coordinates for the 3D ultrasound map, allowing for the storage of acoustic impedance without dense sampling. This approach accelerates both reconstruction and inference speeds. We then propose a dual-supervised network that leverages teacher and student models to hash-encode the rendered ultrasound images from the reconstructed map. AIA-UltraNeRF retrieves the most similar hash values without the need to render images again, providing an offline initial image position for localization. Moreover, we develop a RUSS with a spherical remote center of motion mechanism to hold the probe, implementing operator-independent scanning modes that separate image acquisition from diagnostic workflows. Experimental results on a phantom and human subjects demonstrate the effectiveness of acoustic impedance in implicitly characterizing the color of ultrasound images. AIAUltraNeRF achieves both reconstruction and localization with inference speeds that are 9.9 faster than those of vanilla NeRF.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u8bbe\u8ba1\u4e86\u4e00\u79cdAIA-UltraNeRF\u65b9\u6cd5\uff0c\u7ed3\u5408\u58f0\u963b\u6297\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8d85\u58f0\u56fe\u50cf\u7684\u91cd\u5efa\u548c\u5b9a\u4f4d\u6548\u7387\u3002", "motivation": "\u5728\u8d85\u58f0\u6210\u50cf\u4e2d\uff0c\u58f0\u963b\u6297\u7684\u4f5c\u7528\u88ab\u5ffd\u89c6\uff0c\u8fd9\u5bf9\u91cd\u5efa\u548c\u65b0\u89c6\u56fe\u5408\u6210\u7684\u65b9\u6cd5\u63d0\u51fa\u4e86\u6311\u6218\u3002", "method": "AIA-UltraNeRF\u6a21\u578b\u91c7\u7528\u54c8\u5e0c\u7f16\u7801\u7684\u7a7a\u95f4\u5750\u6807\u8fde\u7eed\u51fd\u6570\u4ee5\u6784\u5efa3D\u8d85\u58f0\u56fe\uff0c\u7ed3\u5408\u53cc\u76d1\u7763\u7f51\u7edc\u4ee5\u52a0\u901f\u91cd\u5efa\u548c\u5b9a\u4f4d\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u58f0\u963b\u6297\u610f\u8bc6\u7684\u8d85\u58f0NeRF\uff08AIA-UltraNeRF\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u673a\u5668\u4eba\u8d85\u58f0\u7cfb\u7edf\uff08RUSS\uff09\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u91cd\u5efa\u548c\u63a8\u65ad\u901f\u5ea6\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5f15\u5165\u58f0\u963b\u6297\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u9ad8\u4e86\u8d85\u58f0\u56fe\u50cf\u8d28\u91cf\uff0c\u5e76\u52a0\u5feb\u4e86\u5904\u7406\u901f\u5ea6\u3002"}}
{"id": "2511.18299", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18299", "abs": "https://arxiv.org/abs/2511.18299", "authors": ["Steven Oh", "Tai Inui", "Magdeline Kuan", "Jia-Yeu Lin"], "title": "MicCheck: Repurposing Off-the-Shelf Pin Microphones for Easy and Low-Cost Contact Sensing", "comment": null, "summary": "Robotic manipulation tasks are contact-rich, yet most imitation learning (IL) approaches rely primarily on vision, which struggles to capture stiffness, roughness, slip, and other fine interaction cues. Tactile signals can address this gap, but existing sensors often require expensive, delicate, or integration-heavy hardware. In this work, we introduce MicCheck, a plug-and-play acoustic sensing approach that repurposes an off-the-shelf Bluetooth pin microphone as a low-cost contact sensor. The microphone clips into a 3D-printed gripper insert and streams audio via a standard USB receiver, requiring no custom electronics or drivers. Despite its simplicity, the microphone provides signals informative enough for both perception and control. In material classification, it achieves 92.9% accuracy on a 10-class benchmark across four interaction types (tap, knock, slow press, drag). For manipulation, integrating pin microphone into an IL pipeline with open source hardware improves the success rate on picking and pouring task from 0.40 to 0.80 and enables reliable execution of contact-rich skills such as unplugging and sound-based sorting. Compared with high-resolution tactile sensors, pin microphones trade spatial detail for cost and ease of integration, offering a practical pathway for deploying acoustic contact sensing in low-cost robot setups.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faMicCheck\uff0c\u4e00\u79cd\u4f7f\u7528\u84dd\u7259\u9ea6\u514b\u98ce\u4f5c\u4e3a\u4f4e\u6210\u672c\u63a5\u89e6\u4f20\u611f\u5668\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u4eff\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u6210\u529f\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u89c6\u89c9\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u63a5\u89e6\u65f6\u7684\u7ec6\u5fae\u4fe1\u606f\uff0c\u9020\u6210\u673a\u5668\u4eba\u5728\u590d\u6742\u64cd\u4f5c\u4e2d\u7684\u5c40\u9650\u6027\u3002\u6211\u4eec\u5e0c\u671b\u901a\u8fc7\u4f4e\u6210\u672c\u7684\u58f0\u5b66\u4f20\u611f\u5668\u8865\u5145\u8fd9\u4e9b\u4fe1\u606f\uff0c\u4ece\u800c\u63d0\u9ad8\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6210\u529f\u7387\u3002", "method": "\u6211\u4eec\u4f7f\u7528\u73b0\u6210\u7684\u84dd\u7259\u9488\u5f62\u9ea6\u514b\u98ce\uff0c\u901a\u8fc73D\u6253\u5370\u5939\u5177\u8fdb\u884c\u96c6\u6210\uff0c\u5b9e\u65f6\u4f20\u8f93\u97f3\u9891\u4fe1\u53f7\uff0c\u5e76\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u4ee5\u8bc4\u4f30\u5176\u6027\u80fd\u3002", "result": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u7684\u63a5\u89e6\u4f20\u611f\u5668MicCheck\uff0c\u8be5\u4f20\u611f\u5668\u5229\u7528\u5e02\u9762\u4e0a\u73b0\u6210\u7684\u84dd\u7259\u9488\u5f62\u9ea6\u514b\u98ce\u6765\u89e3\u51b3\u673a\u5668\u4eba\u7684\u6a21\u4eff\u5b66\u4e60\u4e2d\u65e0\u6cd5\u6355\u6349\u7ec6\u5fae\u63a5\u89e6\u4fe1\u606f\u7684\u95ee\u9898\u3002\u8be5\u9ea6\u514b\u98ce\u53ef\u5939\u57283D\u6253\u5370\u7684\u5939\u722a\u63d2\u5165\u7269\u4e0a\uff0c\u901a\u8fc7\u6807\u51c6USB\u63a5\u6536\u5668\u5b9e\u65f6\u4f20\u8f93\u97f3\u9891\uff0c\u4e14\u4e0d\u9700\u8981\u5b9a\u5236\u7684\u7535\u5b50\u5143\u4ef6\u6216\u9a71\u52a8\u7a0b\u5e8f\u3002\u6d4b\u8bd5\u8868\u660e\uff0c\u5728\u7269\u6599\u5206\u7c7b\u4e0a\uff0c\u8be5\u9ea6\u514b\u98ce\u572810\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e8692.9%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u6210\u529f\u63d0\u9ad8\u4e86\u62fe\u53d6\u548c\u5012\u6db2\u4efb\u52a1\u7684\u6210\u529f\u7387\uff0c\u4ece0.40\u63d0\u5347\u52300.80\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u652f\u6301\u6267\u884c unplugging \u548c\u57fa\u4e8e\u58f0\u97f3\u7684\u6392\u5e8f\u7b49\u63a5\u89e6\u4e30\u5bcc\u6280\u80fd\u3002", "conclusion": "MicCheck\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ecf\u6d4e\u5b9e\u7528\u7684\u58f0\u5b66\u63a5\u89e6\u611f\u77e5\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u6210\u672c\u654f\u611f\u7684\u673a\u5668\u4eba\u5e94\u7528\uff0c\u5c3d\u7ba1\u5728\u7a7a\u95f4\u7ec6\u8282\u4e0a\u4e0d\u5982\u9ad8\u5206\u8fa8\u7387\u89e6\u89c9\u4f20\u611f\u5668\u3002"}}
{"id": "2511.18322", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18322", "abs": "https://arxiv.org/abs/2511.18322", "authors": ["Henrik Krauss", "Johann Licher", "Naoya Takeishi", "Annika Raatz", "Takehisa Yairi"], "title": "Learning Visually Interpretable Oscillator Networks for Soft Continuum Robots from Video", "comment": null, "summary": "Data-driven learning of soft continuum robot (SCR) dynamics from high-dimensional observations offers flexibility but often lacks physical interpretability, while model-based approaches require prior knowledge and can be computationally expensive. We bridge this gap by introducing (1) the Attention Broadcast Decoder (ABCD), a plug-and-play module for autoencoder-based latent dynamics learning that generates pixel-accurate attention maps localizing each latent dimension's contribution while filtering static backgrounds. (2) By coupling these attention maps to 2D oscillator networks, we enable direct on-image visualization of learned dynamics (masses, stiffness, and forces) without prior knowledge. We validate our approach on single- and double-segment SCRs, demonstrating that ABCD-based models significantly improve multi-step prediction accuracy: 5.7x error reduction for Koopman operators and 3.5x for oscillator networks on the two-segment robot. The learned oscillator network autonomously discovers a chain structure of oscillators. Unlike standard methods, ABCD models enable smooth latent space extrapolation beyond training data. This fully data-driven approach yields compact, physically interpretable models suitable for control applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u5b66\u4e60\u4e0e\u7269\u7406\u53ef\u89e3\u91ca\u6027\u7684\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f6f\u8fde\u7eed\u673a\u5668\u4eba\u52a8\u6001\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u9a71\u52a8\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u7269\u7406\u53ef\u89e3\u91ca\u6027\uff0c\u800c\u6a21\u578b\u57fa\u7840\u7684\u65b9\u6cd5\u9700\u8981\u5148\u9a8c\u77e5\u8bc6\u4e14\u8ba1\u7b97\u5f00\u9500\u5927\u3002", "method": "\u91c7\u7528ABCD\u6a21\u5757\u8fdb\u884c\u81ea\u7f16\u7801\u5668\u57fa\u7840\u7684\u6f5c\u5728\u52a8\u6001\u5b66\u4e60\uff0c\u7ed3\u54082D\u632f\u8361\u5668\u7f51\u7edc\u5b9e\u73b0\u52a8\u6001\u7684\u76f4\u63a5\u56fe\u50cf\u53ef\u89c6\u5316\u3002", "result": "\u5f15\u5165\u4e86\u6ce8\u610f\u529b\u5e7f\u64ad\u89e3\u7801\u5668(ABCD)\uff0c\u5b9e\u73b0\u66f4\u9ad8\u7cbe\u5ea6\u7684\u591a\u6b65\u9884\u6d4b\uff0c\u5e76\u5728\u65e0\u5148\u9a8c\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u52a8\u6001\u53ef\u89c6\u5316\u3002", "conclusion": "ABCD\u6a21\u578b\u63d0\u4f9b\u4e86\u7d27\u51d1\u7684\u3001\u7269\u7406\u53ef\u89e3\u91ca\u7684\u6a21\u578b\uff0c\u9002\u5408\u63a7\u5236\u5e94\u7528\u3002"}}
{"id": "2511.18353", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18353", "abs": "https://arxiv.org/abs/2511.18353", "authors": ["Sigrid Helene Strand", "Thomas Wiedemann", "Bram Burczek", "Dmitriy Shutin"], "title": "Enhancing UAV Search under Occlusion using Next Best View Planning", "comment": "Submitted to IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing", "summary": "Search and rescue missions are often critical following sudden natural disasters or in high-risk environmental situations. The most challenging search and rescue missions involve difficult-to-access terrains, such as dense forests with high occlusion. Deploying unmanned aerial vehicles for exploration can significantly enhance search effectiveness, facilitate access to challenging environments, and reduce search time. However, in dense forests, the effectiveness of unmanned aerial vehicles depends on their ability to capture clear views of the ground, necessitating a robust search strategy to optimize camera positioning and perspective. This work presents an optimized planning strategy and an efficient algorithm for the next best view problem in occluded environments. Two novel optimization heuristics, a geometry heuristic, and a visibility heuristic, are proposed to enhance search performance by selecting optimal camera viewpoints. Comparative evaluations in both simulated and real-world settings reveal that the visibility heuristic achieves greater performance, identifying over 90% of hidden objects in simulated forests and offering 10% better detection rates than the geometry heuristic. Additionally, real-world experiments demonstrate that the visibility heuristic provides better coverage under the canopy, highlighting its potential for improving search and rescue missions in occluded environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f18\u5316\u4e86\u65e0\u4eba\u673a\u5728\u5bc6\u6797\u73af\u5883\u4e2d\u7684\u641c\u7d22\u7b56\u7565\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u9896\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u63d0\u9ad8\u4e86\u4eba\u673a\u5408\u4f5c\u7684\u641c\u7d22\u6548\u7387\u3002", "motivation": "\u5728\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\uff0c\u5c24\u5176\u662f\u5bc6\u6797\u7b49\u96be\u4ee5\u8fdb\u5165\u7684\u5730\u5f62\uff0c\u641c\u7d22\u548c\u6551\u63f4\u4efb\u52a1\u8feb\u5207\u9700\u8981\u63d0\u9ad8\u6548\u679c\uff0c\u5c24\u5176\u662f\u5229\u7528\u65e0\u4eba\u673a\u7684\u4f18\u52bf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u7684\u89c4\u5212\u7b56\u7565\u548c\u9ad8\u6548\u7b97\u6cd5\uff0c\u4ee5\u89e3\u51b3\u906e\u6321\u73af\u5883\u4e2d\u7684\u4e0b\u4e00\u4e2a\u6700\u4f73\u89c6\u56fe\u95ee\u9898\u3002", "result": "\u89c6\u89c9\u542f\u53d1\u5f0f\u7b97\u6cd5\u76f8\u6bd4\u51e0\u4f55\u542f\u53d1\u5f0f\u7b97\u6cd5\u8bc6\u522b\u4e86\u8d85\u8fc790%\u7684\u9690\u85cf\u7269\u4f53\uff0c\u5e76\u5728\u5b9e\u65f6\u5b9e\u9a8c\u4e2d\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u8986\u76d6\u6548\u679c\u3002", "conclusion": "\u89c6\u89c9\u542f\u53d1\u5f0f\u7b97\u6cd5\u5728\u906e\u6321\u73af\u5883\u4e0b\u7684\u6551\u63f4\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u51fa\u8272\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u641c\u7d22\u6548\u7387\u548c\u8986\u76d6\u7387\u3002"}}
{"id": "2511.18374", "categories": ["cs.RO", "eess.SY", "math.DS"], "pdf": "https://arxiv.org/pdf/2511.18374", "abs": "https://arxiv.org/abs/2511.18374", "authors": ["Jiaxun Sun"], "title": "Explicit Bounds on the Hausdorff Distance for Truncated mRPI Sets via Norm-Dependent Contraction Rates", "comment": null, "summary": "This paper establishes the first explicit and closed-form upper bound on the Hausdorff distance between the truncated minimal robust positively invariant (mRPI) set and its infinite-horizon limit. While existing mRPI approximations guarantee asymptotic convergence through geometric or norm-based arguments, none provides a computable expression that quantifies the truncation error for a given horizon. We show that the error satisfies \\( d_H(\\mathcal{E}_N,\\mathcal{E}_\\infty) \\le r_W\\,\u03b3^{N+1}/(1-\u03b3), \\) where $\u03b3<1$ is the induced-norm contraction factor and $r_W$ depends only on the disturbance set. The bound is fully analytic, requires no iterative set computations, and directly characterizes the decay rate of the truncated Minkowski series. We further demonstrate that the choice of vector norm serves as a design parameter that accelerates convergence, enabling substantially tighter horizon selection for robust invariant-set computations and tube-based MPC. Numerical experiments validate the sharpness, scalability, and practical relevance of the proposed bound.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u63d0\u51fa\u4e86mRPI\u96c6\u4e0e\u5176\u65e0\u9650\u65f6\u9650\u6781\u9650\u4e4b\u95f4Hausdorff\u8ddd\u79bb\u7684\u4e0a\u754c\uff0c\u63d0\u4f9b\u4e86\u622a\u65ad\u8bef\u5dee\u7684\u53ef\u8ba1\u7b97\u8868\u8fbe\u5f0f\uff0c\u4e14\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709mRPI\u8fd1\u4f3c\u65b9\u6cd5\u53ea\u80fd\u4fdd\u8bc1\u6e10\u8fdb\u6536\u655b\u4f46\u65e0\u6cd5\u8ba1\u7b97\u622a\u65ad\u8bef\u5dee\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u63a8\u5bfcHausdorff\u8ddd\u79bb\u7684\u663e\u5f0f\u4e0a\u754c\uff0c\u5e76\u5229\u7528\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "result": "\u5efa\u7acb\u4e86mRPI\u96c6\u4e0e\u5176\u65e0\u9650\u65f6\u9650\u6781\u9650\u4e4b\u95f4\u7684Hausdorff\u8ddd\u79bb\u7684\u7b2c\u4e00\u4e2a\u663e\u5f0f\u4e0a\u754c\uff0c\u5e76\u7ed9\u51fa\u4e86\u4e00\u4e2a\u53ef\u8ba1\u7b97\u7684\u8868\u8fbe\u5f0f\u3002", "conclusion": "\u63d0\u51fa\u7684\u754c\u9650\u80fd\u591f\u6709\u6548\u6307\u5bfcmRPI\u96c6\u7684\u8ba1\u7b97\u4e0e\u9009\u62e9\uff0c\u66f4\u52a0\u9ad8\u6548\u7684\u8bbe\u8ba1\u63a7\u5236\u7b56\u7565\u3002"}}
{"id": "2511.18486", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.18486", "abs": "https://arxiv.org/abs/2511.18486", "authors": ["Jasan Zughaibi", "Denis von Arx", "Maurus Derungs", "Florian Heemeyer", "Luca A. Antonelli", "Quentin Boehler", "Michael Muehlebach", "Bradley J. Nelson"], "title": "Expanding the Workspace of Electromagnetic Navigation Systems Using Dynamic Feedback for Single- and Multi-agent Control", "comment": null, "summary": "Electromagnetic navigation systems (eMNS) enable a number of magnetically guided surgical procedures. A challenge in magnetically manipulating surgical tools is that the effective workspace of an eMNS is often severely constrained by power and thermal limits. We show that system-level control design significantly expands this workspace by reducing the currents needed to achieve a desired motion. We identified five key system approaches that enable this expansion: (i) motion-centric torque/force objectives, (ii) energy-optimal current allocation, (iii) real-time pose estimation, (iv) dynamic feedback, and (v) high-bandwidth eMNS components. As a result, we stabilize a 3D inverted pendulum on an eight-coil OctoMag eMNS with significantly lower currents (0.1-0.2 A vs. 8-14 A), by replacing a field-centric field-alignment strategy with a motion-centric torque/force-based approach. We generalize to multi-agent control by simultaneously stabilizing two inverted pendulums within a shared workspace, exploiting magnetic-field nonlinearity and coil redundancy for independent actuation. A structured analysis compares the electromagnetic workspaces of both paradigms and examines current-allocation strategies that map motion objectives to coil currents. Cross-platform evaluation of the clinically oriented Navion eMNS further demonstrates substantial workspace expansion by maintaining stable balancing at distances up to 50 cm from the coils. The results demonstrate that feedback is a practical path to scalable, efficient, and clinically relevant magnetic manipulation.", "AI": {"tldr": "\u901a\u8fc7\u7cfb\u7edf\u7ea7\u63a7\u5236\u8bbe\u8ba1\uff0c\u91c7\u7528\u8fd0\u52a8\u4e2d\u5fc3\u7684\u7535\u6d41\u5206\u914d\u65b9\u6cd5\uff0c\u663e\u8457\u6269\u5927\u4e86\u7535\u78c1\u5bfc\u822a\u7cfb\u7edf\u7684\u5de5\u4f5c\u7a7a\u95f4\uff0c\u5e76\u964d\u4f4e\u4e86\u64cd\u63a7\u7535\u6d41\u3002", "motivation": "\u5728\u78c1\u6027\u5f15\u5bfc\u5916\u79d1\u624b\u672f\u4e2d\uff0c\u7535\u78c1\u5bfc\u822a\u7cfb\u7edf\u7684\u6709\u6548\u5de5\u4f5c\u7a7a\u95f4\u53d7\u5230\u529f\u7387\u548c\u70ed\u91cf\u9650\u5236\uff0c\u6025\u9700\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\u6765\u6269\u5927\u5176\u5de5\u4f5c\u7a7a\u95f4\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u7ea7\u63a7\u5236\u8bbe\u8ba1\u6765\u964d\u4f4e\u5b9e\u73b0\u6240\u9700\u8fd0\u52a8\u7684\u7535\u6d41\uff0c\u4ece\u800c\u6269\u5c55\u6709\u6548\u5de5\u4f5c\u7a7a\u95f4\u3002", "result": "\u901a\u8fc7\u8fd0\u52a8\u4e2d\u5fc3\u7684\u626d\u77e9/\u529b\u63a7\u5236\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u7535\u6d41\u9700\u6c42\uff0c\u5e76\u6210\u529f\u5728\u5171\u4eab\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u7a33\u5b9a\u4e24\u4e2a\u5012\u7acb\u6446\u3002", "conclusion": "\u53cd\u9988\u662f\u4e00\u79cd\u53ef\u884c\u7684\u9014\u5f84\uff0c\u4f7f\u78c1\u6027\u64cd\u63a7\u5177\u6709\u53ef\u6269\u5c55\u6027\u3001\u9ad8\u6548\u6027\u548c\u4e34\u5e8a\u76f8\u5173\u6027\u3002"}}
{"id": "2511.18509", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18509", "abs": "https://arxiv.org/abs/2511.18509", "authors": ["Ziyu Meng", "Tengyu Liu", "Le Ma", "Yingying Wu", "Ran Song", "Wei Zhang", "Siyuan Huang"], "title": "SafeFall: Learning Protective Control for Humanoid Robots", "comment": null, "summary": "Bipedal locomotion makes humanoid robots inherently prone to falls, causing catastrophic damage to the expensive sensors, actuators, and structural components of full-scale robots. To address this critical barrier to real-world deployment, we present \\method, a framework that learns to predict imminent, unavoidable falls and execute protective maneuvers to minimize hardware damage. SafeFall is designed to operate seamlessly alongside existing nominal controller, ensuring no interference during normal operation. It combines two synergistic components: a lightweight, GRU-based fall predictor that continuously monitors the robot's state, and a reinforcement learning policy for damage mitigation. The protective policy remains dormant until the predictor identifies a fall as unavoidable, at which point it activates to take control and execute a damage-minimizing response. This policy is trained with a novel, damage-aware reward function that incorporates the robot's specific structural vulnerabilities, learning to shield critical components like the head and hands while absorbing energy with more robust parts of its body. Validated on a full-scale Unitree G1 humanoid, SafeFall demonstrated significant performance improvements over unprotected falls. It reduced peak contact forces by 68.3\\%, peak joint torques by 78.4\\%, and eliminated 99.3\\% of collisions with vulnerable components. By enabling humanoids to fail safely, SafeFall provides a crucial safety net that allows for more aggressive experiments and accelerates the deployment of these robots in complex, real-world environments.", "AI": {"tldr": "\u63d0\u51faSafeFall\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u4e0d\u53ef\u907f\u514d\u7684\u8dcc\u5012\u5e76\u6267\u884c\u4fdd\u62a4\u63aa\u65bd\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u8dcc\u5012\u65f6\u7684\u786c\u4ef6\u635f\u5bb3\uff0c\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\u3002", "motivation": "\u7531\u4e8e\u53cc\u8db3\u884c\u8d70\u4f7f\u4eba\u5f62\u673a\u5668\u4eba\u6613\u4e8e\u8dcc\u5012\uff0c\u4fdd\u62a4\u6602\u8d35\u7684\u4f20\u611f\u5668\u548c\u7ed3\u6784\u90e8\u4ef6\u662f\u5b9e\u9645\u5e94\u7528\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u8f7b\u91cf\u7ea7\u7684GRU\u57fa\u7840\u7684\u8dcc\u5012\u9884\u6d4b\u5668\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u635f\u5bb3\u7f13\u89e3\u7b56\u7565\uff0c\u80fd\u591f\u5728\u9884\u6d4b\u5230\u4e0d\u53ef\u907f\u514d\u8dcc\u5012\u65f6\u6267\u884c\u4fdd\u62a4\u63aa\u65bd\u3002", "result": "SafeFall\u5728Unitree G1\u5168\u5c3a\u5ea6\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5cf0\u503c\u63a5\u89e6\u529b68.3%\uff0c\u5cf0\u503c\u5173\u8282\u626d\u77e978.4%\uff0c\u5e76\u6d88\u9664\u4e8699.3%\u7684\u8106\u5f31\u90e8\u4ef6\u78b0\u649e\u3002", "conclusion": "SafeFall\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u5b89\u5168\u4fdd\u62a4\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e2d\u66f4\u5177\u653b\u51fb\u6027\u5730\u5b9e\u9a8c\u5e76\u52a0\u901f\u90e8\u7f72\u3002"}}
{"id": "2511.18525", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18525", "abs": "https://arxiv.org/abs/2511.18525", "authors": ["Samarth Chopra", "Jing Liang", "Gershom Seneviratne", "Yonghan Lee", "Jaehoon Choi", "Jianyu An", "Stephen Cheng", "Dinesh Manocha"], "title": "Splatblox: Traversability-Aware Gaussian Splatting for Outdoor Robot Navigation", "comment": "Submitted to ICRA 2026", "summary": "We present Splatblox, a real-time system for autonomous navigation in outdoor environments with dense vegetation, irregular obstacles, and complex terrain. Our method fuses segmented RGB images and LiDAR point clouds using Gaussian Splatting to construct a traversability-aware Euclidean Signed Distance Field (ESDF) that jointly encodes geometry and semantics. Updated online, this field enables semantic reasoning to distinguish traversable vegetation (e.g., tall grass) from rigid obstacles (e.g., trees), while LiDAR ensures 360-degree geometric coverage for extended planning horizons. We validate Splatblox on a quadruped robot and demonstrate transfer to a wheeled platform. In field trials across vegetation-rich scenarios, it outperforms state-of-the-art methods with over 50% higher success rate, 40% fewer freezing incidents, 5% shorter paths, and up to 13% faster time to goal, while supporting long-range missions up to 100 meters. Experiment videos and more details can be found on our project page: https://splatblox.github.io", "AI": {"tldr": "Splatblox \u662f\u4e00\u4e2a\u878d\u5408\u4e86 RGB \u56fe\u50cf\u548c LiDAR \u70b9\u4e91\u7684\u5b9e\u65f6\u81ea\u4e3b\u5bfc\u822a\u7cfb\u7edf\uff0c\u5728\u590d\u6742\u7684\u6237\u5916\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5728\u590d\u6742\u7684\u6237\u5916\u73af\u5883\u4e2d\u8fdb\u884c\u81ea\u4e3b\u5bfc\u822a\u9762\u4e34\u8bf8\u591a\u6311\u6218\uff0c\u5982\u6d53\u5bc6\u690d\u88ab\u3001 irregular \u969c\u788d\u7269\u548c\u590d\u6742\u5730\u5f62\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u9ad8\u65af Splatting \u5c06\u5206\u6bb5 RGB \u56fe\u50cf\u4e0e LiDAR \u70b9\u4e91\u878d\u5408\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u80fd\u591f\u611f\u77e5\u53ef\u901a\u884c\u6027\u7684\u6b27\u51e0\u91cc\u5f97\u7b26\u53f7\u8ddd\u79bb\u573a (ESDF)\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Splatblox \u7684\u5b9e\u65f6\u7cfb\u7edf\uff0c\u80fd\u591f\u5728 dense vegetation \u548c\u590d\u6742\u5730\u5f62\u4e2d\u5b9e\u73b0\u81ea\u4e3b\u5bfc\u822a\u3002", "conclusion": "\u5728\u690d\u88ab\u4e30\u5bcc\u7684\u5b9e\u9645\u6d4b\u8bd5\u4e2d\uff0cSplatblox \u7684\u6210\u529f\u7387\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u63d0\u9ad8\u8d85\u8fc7 50%\uff0c\u5e76\u5728\u591a\u9879\u6307\u6807\u4e0a\u8868\u73b0\u66f4\u4f73\u3002"}}
{"id": "2511.18563", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18563", "abs": "https://arxiv.org/abs/2511.18563", "authors": ["Cem Bilaloglu", "Tobias L\u00f6w", "Sylvain Calinon"], "title": "Object-centric Task Representation and Transfer using Diffused Orientation Fields", "comment": null, "summary": "Curved objects pose a fundamental challenge for skill transfer in robotics: unlike planar surfaces, they do not admit a global reference frame. As a result, task-relevant directions such as \"toward\" or \"along\" the surface vary with position and geometry, making object-centric tasks difficult to transfer across shapes. To address this, we introduce an approach using Diffused Orientation Fields (DOF), a smooth representation of local reference frames, for transfer learning of tasks across curved objects. By expressing manipulation tasks in these smoothly varying local frames, we reduce the problem of transferring tasks across curved objects to establishing sparse keypoint correspondences. DOF is computed online from raw point cloud data using diffusion processes governed by partial differential equations, conditioned on keypoints. We evaluate DOF under geometric, topological, and localization perturbations, and demonstrate successful transfer of tasks requiring continuous physical interaction such as inspection, slicing, and peeling across varied objects. We provide our open-source codes at our website https://github.com/idiap/diffused_fields_robotics", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4f7f\u7528\u6269\u6563\u65b9\u5411\u573a\uff08DOF\uff09\u6765\u7b80\u5316\u5f2f\u66f2\u7269\u4f53\u4e0a\u7684\u64cd\u4f5c\u4efb\u52a1\u8f6c\u79fb\u95ee\u9898\u3002", "motivation": "\u5f2f\u66f2\u7269\u4f53\u7f3a\u4e4f\u5168\u5c40\u53c2\u8003\u6846\u67b6\uff0c\u8fd9\u4f7f\u5f97\u5728\u4e0d\u540c\u5f62\u72b6\u4e0a\u6267\u884c\u4ee5\u7269\u4f53\u4e3a\u4e2d\u5fc3\u7684\u4efb\u52a1\u53d8\u5f97\u56f0\u96be\u3002", "method": "\u901a\u8fc7\u6269\u6563\u8fc7\u7a0b\u8ba1\u7b97\u5c40\u90e8\u53c2\u8003\u6846\u67b6\u7684\u5e73\u6ed1\u8868\u793a\uff08DOF\uff09\uff0c\u5e76\u5229\u7528\u504f\u5fae\u5206\u65b9\u7a0b\u5904\u7406\u70b9\u4e91\u6570\u636e\uff0c\u6761\u4ef6\u4e8e\u5173\u952e\u70b9\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8f6c\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u673a\u5668\u4eba\u5728\u5f2f\u66f2\u7269\u4f53\u4e0a\u8fdb\u884c\u6280\u80fd\u8f6c\u79fb\u7684\u6311\u6218\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u5efa\u7acb\u7a00\u758f\u7684\u5173\u952e\u70b9\u5bf9\u5e94\u5173\u7cfb\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u4e0d\u540c\u5f2f\u66f2\u7269\u4f53\u4e0a\u6267\u884c\u8fde\u7eed\u4ea4\u4e92\u64cd\u4f5c\u7684\u4efb\u52a1\u8f6c\u79fb\u3002"}}
{"id": "2511.18604", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.18604", "abs": "https://arxiv.org/abs/2511.18604", "authors": ["Hannah Lee", "James D. Motes", "Marco Morales", "Nancy M. Amato"], "title": "An Analysis of Constraint-Based Multi-Agent Pathfinding Algorithms", "comment": null, "summary": "This study informs the design of future multi-agent pathfinding (MAPF) and multi-robot motion planning (MRMP) algorithms by guiding choices based on constraint classification for constraint-based search algorithms. We categorize constraints as conservative or aggressive and provide insights into their search behavior, focusing specifically on vanilla Conflict-Based Search (CBS) and Conflict-Based Search with Priorities (CBSw/P). Under a hybrid grid-roadmap representation with varying resolution, we observe that aggressive (priority constraint) formulations tend to solve more instances as agent count or resolution increases, whereas conservative (motion constraint) formulations yield stronger solution quality when both succeed. Findings are synthesized in a decision flowchart, aiding users in selecting suitable constraints. Recommendations extend to Multi-Robot Motion Planning (MRMP), emphasizing the importance of considering topological features alongside problem, solution, and representation features. A comprehensive exploration of the study, including raw data and map performance, is available in our public GitHub Repository: https://GitHub.com/hannahjmlee/constraint-mapf-analysis", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5206\u7c7b\u7ea6\u675f\u6307\u5bfc\u672a\u6765\u591a\u4ee3\u7406\u8def\u5f84\u89c4\u5212\u548c\u591a\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\u7684\u8bbe\u8ba1\uff0c\u53d1\u73b0\u4fb5\u7565\u6027\u7ea6\u675f\u5728\u89e3\u9898\u6570\u91cf\u4e0a\u8868\u73b0\u8f83\u597d\uff0c\u800c\u4fdd\u5b88\u6027\u7ea6\u675f\u5728\u89e3\u7684\u8d28\u91cf\u4e0a\u66f4\u5f3a\uff0c\u63d0\u4f9b\u4e86\u51b3\u7b56\u6d41\u7a0b\u56fe\u3002", "motivation": "\u7814\u7a76\u672a\u6765\u591a\u4ee3\u7406\u8def\u5f84\u89c4\u5212\uff08MAPF\uff09\u548c\u591a\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\uff08MRMP\uff09\u7b97\u6cd5\u7684\u8bbe\u8ba1\uff0c\u7279\u522b\u662f\u57fa\u4e8e\u7ea6\u675f\u5206\u7c7b\u7684\u9009\u62e9\u3002", "method": "\u5c06\u7ea6\u675f\u5206\u4e3a\u4fdd\u5b88\u6027\u548c\u4fb5\u7565\u6027\uff0c\u7814\u7a76\u5176\u5728Conflict-Based Search (CBS)\u53ca\u5e26\u4f18\u5148\u7ea7\u7684Conflict-Based Search (CBSw/P)\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u901a\u8fc7\u6df7\u5408\u7f51\u683c-\u8def\u7ebf\u56fe\u8868\u793a\u6cd5\u5206\u6790\u4e0d\u540c\u5206\u8fa8\u7387\u7684\u5f71\u54cd\u3002", "result": "\u4e3a\u7528\u6237\u63d0\u4f9b\u9009\u62e9\u5408\u9002\u7ea6\u675f\u7684\u51b3\u7b56\u6d41\u7a0b\u56fe\uff0c\u5e76\u63d0\u51fa\u4e86\u5bf9\u591a\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u7684\u5efa\u8bae\u3002", "conclusion": "\u6709\u6548\u7684\u7ea6\u675f\u5206\u7c7b\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u591a\u4ee3\u7406\u8def\u5f84\u89c4\u5212\u548c\u591a\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\u7684\u51b3\u7b56\u6548\u7387\u548c\u95ee\u9898\u89e3\u51b3\u8d28\u91cf\u3002"}}
{"id": "2511.18606", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18606", "abs": "https://arxiv.org/abs/2511.18606", "authors": ["Kensuke Nakamura", "Arun L. Bishop", "Steven Man", "Aaron M. Johnson", "Zachary Manchester", "Andrea Bajcsy"], "title": "How to Train Your Latent Control Barrier Function: Smooth Safety Filtering Under Hard-to-Model Constraints", "comment": "3 figures, 10 tables, 22 pages", "summary": "Latent safety filters extend Hamilton-Jacobi (HJ) reachability to operate on latent state representations and dynamics learned directly from high-dimensional observations, enabling safe visuomotor control under hard-to-model constraints. However, existing methods implement \"least-restrictive\" filtering that discretely switch between nominal and safety policies, potentially undermining the task performance that makes modern visuomotor policies valuable. While reachability value functions can, in principle, be adapted to be control barrier functions (CBFs) for smooth optimization-based filtering, we theoretically and empirically show that current latent-space learning methods produce fundamentally incompatible value functions. We identify two sources of incompatibility: First, in HJ reachability, failures are encoded via a \"margin function\" in latent space, whose sign indicates whether or not a latent is in the constraint set. However, representing the margin function as a classifier yields saturated value functions that exhibit discontinuous jumps. We prove that the value function's Lipschitz constant scales linearly with the margin function's Lipschitz constant, revealing that smooth CBFs require smooth margins. Second, reinforcement learning (RL) approximations trained solely on safety policy data yield inaccurate value estimates for nominal policy actions, precisely where CBF filtering needs them. We propose the LatentCBF, which addresses both challenges through gradient penalties that lead to smooth margin functions without additional labeling, and a value-training procedure that mixes data from both nominal and safety policy distributions. Experiments on simulated benchmarks and hardware with a vision-based manipulation policy demonstrate that LatentCBF enables smooth safety filtering while doubling the task-completion rate over prior switching methods.", "AI": {"tldr": "LatentCBF\u901a\u8fc7\u5e73\u6ed1\u8fb9\u9645\u51fd\u6570\u548c\u6df7\u5408\u6570\u636e\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u9690\u72b6\u6001\u8868\u793a\u4e0b\u7684\u5b89\u5168\u8fc7\u6ee4\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u4efb\u52a1\u5b8c\u6210\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7684\u201c\u6700\u5c11\u9650\u5236\u201d\u8fc7\u6ee4\u53ef\u80fd\u4f1a\u5f71\u54cd\u73b0\u4ee3\u89c6\u52a8\u63a7\u5236\u7b56\u7565\u7684\u4efb\u52a1\u8868\u73b0\uff0c\u540c\u65f6\u9690\u542b\u7a7a\u95f4\u5b66\u4e60\u65b9\u6cd5\u4ea7\u751f\u7684\u4ef7\u503c\u51fd\u6570\u5b58\u5728\u4e0d\u517c\u5bb9\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aLatentCBF\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u60e9\u7f5a\u548c\u4ef7\u503c\u8bad\u7ec3\u7a0b\u5e8f\u6765\u89e3\u51b3\u5b89\u5168\u8fc7\u6ee4\u7684\u6311\u6218\u3002", "result": "LatentCBF\u80fd\u5728\u4fdd\u8bc1\u5e73\u6ed1\u5b89\u5168\u8fc7\u6ee4\u7684\u540c\u65f6\uff0c\u4efb\u52a1\u5b8c\u6210\u7387\u6bd4\u4e4b\u524d\u7684\u5207\u6362\u65b9\u6cd5\u63d0\u9ad8\u4e86\u4e00\u500d\u3002", "conclusion": "LatentCBF\u662f\u5bf9\u73b0\u6709\u5b89\u5168\u8fc7\u6ee4\u65b9\u6cd5\u7684\u6539\u8fdb\uff0c\u80fd\u6709\u6548\u63d0\u9ad8\u5728\u590d\u6742\u7ea6\u675f\u6761\u4ef6\u4e0b\u7684\u89c6\u52a8\u63a7\u5236\u6027\u80fd\u3002"}}
{"id": "2511.18617", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18617", "abs": "https://arxiv.org/abs/2511.18617", "authors": ["Litian Gong", "Fatemeh Bahrani", "Yutai Zhou", "Amin Banayeeanzade", "Jiachen Li", "Erdem Biyik"], "title": "AutoFocus-IL: VLM-based Saliency Maps for Data-Efficient Visual Imitation Learning without Extra Human Annotations", "comment": "8 pages, 6 figures. Code and datasets available at http://autofocus-il.github.io/", "summary": "AutoFocus-IL is a simple yet effective method to improve data efficiency and generalization in visual imitation learning by guiding policies to attend to task-relevant features rather than distractors and spurious correlations. Although saliency regularization has emerged as a promising way to achieve this, existing approaches typically require costly supervision such as human gaze data or manual saliency annotations. In contrast, AutoFocus-IL leverages vision-language models (VLMs) to automatically identify and track key objects in demonstrations, generating temporal saliency maps that highlight causal visual signals while suppressing distractors. These maps are then used to regularize behavior cloning policies, yielding stronger alignment between visual attention and task-relevant cues. Experiments in both the CARLA simulator and real-robot manipulation tasks demonstrate that AutoFocus-IL not only outperforms standard behavior cloning but also surpasses state-of-the-art baselines that assume privileged access to human supervision, such as gaze data. Code, datasets, and trained policy videos are available at https://AutoFocus-IL.github.io/.", "AI": {"tldr": "AutoFocus-IL\u901a\u8fc7\u4f7f\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u8bc6\u522b\u5173\u952e\u5bf9\u8c61\u6765\u6539\u8fdb\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u7684\u6570\u636e\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u63d0\u9ad8\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u907f\u514d\u5e72\u6270\u56e0\u7d20\u548c\u865a\u5047\u76f8\u5173\u6027\u3002", "method": "\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u751f\u6210\u65f6\u95f4\u663e\u8457\u6027\u56fe\uff0c\u901a\u8fc7\u5bf9\u884c\u4e3a\u514b\u9686\u7b56\u7565\u8fdb\u884c\u6b63\u5219\u5316\u6765\u5f15\u5bfc\u89c6\u89c9\u6ce8\u610f\u529b\u4e0e\u4efb\u52a1\u76f8\u5173\u7ebf\u7d22\u5bf9\u9f50\u3002", "result": "AutoFocus-IL\u65b9\u6cd5 outperform \u4f20\u7edf\u7684\u884c\u4e3a\u514b\u9686\u548c\u4e00\u4e9b\u4f9d\u8d56\u4eba\u7c7b\u76d1\u7763\u7684\u6700\u65b0\u65b9\u6cd5\u3002", "conclusion": "\u5b9e\u9a8c\u8868\u660e\uff0cAutoFocus-IL\u5728\u6c7d\u8f66\u6a21\u62df\u5668\u548c\u771f\u5b9e\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u79c0\u3002"}}
{"id": "2511.18683", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18683", "abs": "https://arxiv.org/abs/2511.18683", "authors": ["Yinan Dong", "Ziyu Xu", "Tsimafei Lazouski", "Sangli Teng", "Maani Ghaffari"], "title": "Online Learning-Enhanced Lie Algebraic MPC for Robust Trajectory Tracking of Autonomous Surface Vehicles", "comment": null, "summary": "Autonomous surface vehicles (ASVs) are easily influenced by environmental disturbances such as wind and waves, making accurate trajectory tracking a persistent challenge in dynamic marine conditions. In this paper, we propose an efficient controller for trajectory tracking of marine vehicles under unknown disturbances by combining a convex error-state MPC on the Lie group with an online learning module to compensate for these disturbances in real time. This design enables adaptive and robust control while maintaining computational efficiency. Extensive evaluations in numerical simulations, the Virtual RobotX (VRX) simulator, and real-world field experiments demonstrate that our method achieves superior tracking accuracy under various disturbance scenarios compared with existing approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u7ed3\u5408MPC\u548c\u5728\u7ebf\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86ASV\u5728\u52a8\u6001\u6d77\u6d0b\u6761\u4ef6\u4e0b\u7684\u9ad8\u6548\u8f68\u8ff9\u8ddf\u8e2a\uff0c\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u73af\u5883\u5e72\u6270\uff08\u5982\u98ce\u548c\u6ce2\u6d6a\uff09\u5bf9\u81ea\u4e3b\u6c34\u9762\u8f66\u8f86\uff08ASV\uff09\u7684\u5f71\u54cd\uff0c\u7cbe\u786e\u7684\u8f68\u8ff9\u8ddf\u8e2a\u5728\u52a8\u6001\u6d77\u6d0b\u6761\u4ef6\u4e0b\u59cb\u7ec8\u662f\u4e00\u9879\u6311\u6218\u3002", "method": "\u7ed3\u5408\u4e86Lie\u7fa4\u4e0a\u7684\u51f8\u8bef\u5dee\u72b6\u6001\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u548c\u5728\u7ebf\u5b66\u4e60\u6a21\u5757\uff0c\u4ee5\u5b9e\u65f6\u8865\u507f\u672a\u77e5\u7684\u73af\u5883\u5e72\u6270\u3002", "result": "\u5e7f\u6cdb\u7684\u6570\u503c\u6a21\u62df\u3001\u865a\u62dfRobotX\uff08VRX\uff09\u6a21\u62df\u5668\u4ee5\u53ca\u771f\u5b9e\u573a\u5730\u5b9e\u9a8c\u7684\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5404\u79cd\u5e72\u6270\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u8ddf\u8e2a\u7cbe\u5ea6\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u9762\u5bf9\u5404\u79cd\u5e72\u6270\u573a\u666f\u65f6\uff0c\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u8f68\u8ff9\u8ddf\u8e2a\u7cbe\u5ea6\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2511.18694", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18694", "abs": "https://arxiv.org/abs/2511.18694", "authors": ["Shuo Wen", "Edwin Meriaux", "Mariana Sosa Guzm\u00e1n", "Zhizun Wang", "Junming Shi", "Gregory Dudek"], "title": "Stable Multi-Drone GNSS Tracking System for Marine Robots", "comment": null, "summary": "Accurate localization is essential for marine robotics, yet Global Navigation Satellite System (GNSS) signals are unreliable or unavailable even at a very short distance below the water surface. Traditional alternatives, such as inertial navigation, Doppler Velocity Loggers (DVL), SLAM, and acoustic methods, suffer from error accumulation, high computational demands, or infrastructure dependence. In this work, we present a scalable multi-drone GNSS-based tracking system for surface and near-surface marine robots. Our approach combines efficient visual detection, lightweight multi-object tracking, GNSS-based triangulation, and a confidence-weighted Extended Kalman Filter (EKF) to provide stable GNSS estimation in real time. We further introduce a cross-drone tracking ID alignment algorithm that enforces global consistency across views, enabling robust multi-robot tracking with redundant aerial coverage. We validate our system in diversified complex settings to show the scalability and robustness of the proposed algorithm.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u591a\u65e0\u4eba\u673aGNSS\u8ddf\u8e2a\u7cfb\u7edf\uff0c\u7ed3\u5408\u89c6\u89c9\u68c0\u6d4b\u548c\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\uff0c\u5b9e\u73b0\u5b9e\u65f6\u7a33\u5b9a\u5b9a\u4f4d\u3002", "motivation": "\u6d77\u6d0b\u673a\u5668\u4eba\u5728\u6c34\u4e0b\u4fe1\u53f7\u4e0d\u53ef\u9760\uff0c\u9700\u8981\u51c6\u786e\u5b9a\u4f4d\uff0c\u4f20\u7edf\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027", "method": "\u591a\u65e0\u4eba\u673aGNSS\u8ddf\u8e2a\u7cfb\u7edf", "result": "\u5b9e\u73b0\u7a33\u5b9a\u7684GNSS\u4f30\u8ba1\uff0c\u5e76\u901a\u8fc7\u8de8\u65e0\u4eba\u673a\u8ddf\u8e2aID\u5bf9\u9f50\u7b97\u6cd5\u63d0\u9ad8\u4e86\u591a\u673a\u5668\u4eba\u8ddf\u8e2a\u7684\u9c81\u68d2\u6027", "conclusion": "\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5728\u591a\u6837\u590d\u6742\u73af\u5883\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.18702", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18702", "abs": "https://arxiv.org/abs/2511.18702", "authors": ["Xueyan Oh", "Leonard Loh", "Shaohui Foong", "Zhong Bao Andy Koh", "Kow Leong Ng", "Poh Kang Tan", "Pei Lin Pearlin Toh", "U-Xuan Tan"], "title": "CNN-Based Camera Pose Estimation and Localisation of Scan Images for Aircraft Visual Inspection", "comment": "12 pages, 12 figures", "summary": "General Visual Inspection is a manual inspection process regularly used to detect and localise obvious damage on the exterior of commercial aircraft. There has been increasing demand to perform this process at the boarding gate to minimise the downtime of the aircraft and automating this process is desired to reduce the reliance on human labour. Automating this typically requires estimating a camera's pose with respect to the aircraft for initialisation but most existing localisation methods require infrastructure, which is very challenging in uncontrolled outdoor environments and within the limited turnover time (approximately 2 hours) on an airport tarmac. Additionally, many airlines and airports do not allow contact with the aircraft's surface or using UAVs for inspection between flights, and restrict access to commercial aircraft. Hence, this paper proposes an on-site method that is infrastructure-free and easy to deploy for estimating a pan-tilt-zoom camera's pose and localising scan images. This method initialises using the same pan-tilt-zoom camera used for the inspection task by utilising a Deep Convolutional Neural Network fine-tuned on only synthetic images to predict its own pose. We apply domain randomisation to generate the dataset for fine-tuning the network and modify its loss function by leveraging aircraft geometry to improve accuracy. We also propose a workflow for initialisation, scan path planning, and precise localisation of images captured from a pan-tilt-zoom camera. We evaluate and demonstrate our approach through experiments with real aircraft, achieving root-mean-square camera pose estimation errors of less than 0.24 m and 2 degrees for all real scenes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65e0\u57fa\u7840\u8bbe\u65bd\u65b9\u6cd5\uff0c\u7528\u4e8e\u7cbe\u786e\u4f30\u8ba1\u822a\u62cd\u6444\u50cf\u673a\u59ff\u6001\uff0c\u4ece\u800c\u81ea\u52a8\u5316\u5546\u4e1a\u98de\u673a\u7684\u89c6\u89c9\u68c0\u67e5\uff0c\u51cf\u5c11\u4eba\u5de5\u4f9d\u8d56\uff0c\u63d0\u9ad8\u68c0\u4fee\u6548\u7387\u3002", "motivation": "\u51cf\u5c11\u5bf9\u4eba\u5de5\u7684\u4f9d\u8d56\u4ee5\u53ca\u98de\u673a\u68c0\u4fee\u7684\u505c\u673a\u65f6\u95f4\uff0c\u901a\u8fc7\u5728\u767b\u673a\u53e3\u81ea\u52a8\u5316\u89c6\u89c9\u68c0\u67e5\u8fc7\u7a0b\u6765\u63d0\u5347\u822a\u7a7a\u516c\u53f8\u548c\u673a\u573a\u7684\u6548\u7387\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u6444\u50cf\u673a\u59ff\u6001\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7\u57df\u968f\u673a\u5316\u751f\u6210\u5408\u6210\u56fe\u50cf\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\uff0c\u4fee\u6539\u635f\u5931\u51fd\u6570\u4ee5\u63d0\u5347\u7cbe\u5ea6\u3002", "result": "\u5728\u771f\u5b9e\u573a\u666f\u4e2d\uff0c\u6444\u50cf\u673a\u59ff\u6001\u4f30\u8ba1\u7684\u5747\u65b9\u6839\u8bef\u5dee\u5c0f\u4e8e0.24\u7c73\u548c2\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u57fa\u7840\u8bbe\u65bd\u3001\u6613\u4e8e\u90e8\u7f72\u7684\u73b0\u573a\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4f30\u8ba1\u6444\u50cf\u673a\u59ff\u6001\uff0c\u5b9e\u73b0\u5bf9\u5546\u4e1a\u98de\u673a\u7684\u89c6\u89c9\u68c0\u67e5\u548c\u626b\u63cf\u56fe\u50cf\u7684\u5b9a\u4f4d\u3002"}}
{"id": "2511.18703", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18703", "abs": "https://arxiv.org/abs/2511.18703", "authors": ["Ardalan Tajbakhsh", "Augustinos Saravanos", "James Zhu", "Evangelos A. Theodorou", "Lorenz T. Biegler", "Aaron M. Johnson"], "title": "Asynchronous Distributed Multi-Robot Motion Planning Under Imperfect Communication", "comment": "9 pages, 5 figures", "summary": "This paper addresses the challenge of coordinating multi-robot systems under realistic communication delays using distributed optimization. We focus on consensus ADMM as a scalable framework for generating collision-free, dynamically feasible motion plans in both trajectory optimization and receding-horizon control settings. In practice, however, these algorithms are sensitive to penalty tuning or adaptation schemes (e.g. residual balancing and adaptive parameter heuristics) that do not explicitly consider delays. To address this, we introduce a Delay-Aware ADMM (DA-ADMM) variant that adapts penalty parameters based on real-time delay statistics, allowing agents to down-weight stale information and prioritize recent updates during consensus and dual updates. Through extensive simulations in 2D and 3D environments with double-integrator, Dubins-car, and drone dynamics, we show that DA-ADMM significantly improves robustness, success rate, and solution quality compared to fixed-parameter, residual-balancing, and fixed-constraint baselines. Our results highlight that performance degradation is not solely determined by delay length or frequency, but by the optimizer's ability to contextually reason over delayed information. The proposed DA-ADMM achieves consistently better coordination performance across a wide range of delay conditions, offering a principled and efficient mechanism for resilient multi-robot motion planning under imperfect communication.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684DA-ADMM\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u5e94\u5bf9\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u5b58\u5728\u901a\u4fe1\u5ef6\u8fdf\u65f6\u7684\u534f\u8c03\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u548c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u5b9e\u9645\u901a\u4fe1\u5ef6\u8fdf\u4e0b\u7684\u534f\u8c03\u6311\u6218\uff0c\u63d0\u5347\u8fd0\u52a8\u89c4\u5212\u7684\u6210\u529f\u7387\u548c\u8d28\u91cf\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u5b9e\u65f6\u5ef6\u8fdf\u7edf\u8ba1\u7684\u81ea\u9002\u5e94\u60e9\u7f5a\u53c2\u6570\u8c03\u6574\u673a\u5236\uff0cDA-ADMM\u80fd\u4f18\u5148\u5904\u7406\u6700\u8fd1\u66f4\u65b0\u7684\u4fe1\u606f\uff0c\u4ece\u800c\u6539\u5584\u4e86\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u534f\u8c03\u80fd\u529b\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5ef6\u8fdf\u611f\u77e5\u7684\u81ea\u9002\u5e94\u4ea4\u66ff\u65b9\u5411\u4e58\u5b50\u6cd5\uff08DA-ADMM\uff09\uff0c\u7528\u4e8e\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u73b0\u5b9e\u901a\u4fe1\u5ef6\u8fdf\u6761\u4ef6\u4e0b\u7684\u534f\u8c03\u548c\u8fd0\u52a8\u89c4\u5212\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684DA-ADMM\u65b9\u6cd5\u5728\u5404\u79cd\u5ef6\u8fdf\u6761\u4ef6\u4e0b\u5747\u5c55\u73b0\u4e86\u4f18\u8d8a\u7684\u534f\u8c03\u6027\u80fd\uff0c\u4e3a\u591a\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.18708", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18708", "abs": "https://arxiv.org/abs/2511.18708", "authors": ["Yanbin Li", "Canran Xiao", "Shenghai Yuan", "Peilai Yu", "Ziruo Li", "Zhiguo Zhang", "Wenzheng Chi", "Wei Zhang"], "title": "GVD-TG: Topological Graph based on Fast Hierarchical GVD Sampling for Robot Exploration", "comment": "12 pages, 10 figures", "summary": "Topological maps are more suitable than metric maps for robotic exploration tasks. However, real-time updating of accurate and detail-rich environmental topological maps remains a challenge. This paper presents a topological map updating method based on the Generalized Voronoi Diagram (GVD). First, the newly observed areas are denoised to avoid low-efficiency GVD nodes misleading the topological structure. Subsequently, a multi-granularity hierarchical GVD generation method is designed to control the sampling granularity at both global and local levels. This not only ensures the accuracy of the topological structure but also enhances the ability to capture detail features, reduces the probability of path backtracking, and ensures no overlap between GVDs through the maintenance of a coverage map, thereby improving GVD utilization efficiency. Second, a node clustering method with connectivity constraints and a connectivity method based on a switching mechanism are designed to avoid the generation of unreachable nodes and erroneous nodes caused by obstacle attraction. A special cache structure is used to store all connectivity information, thereby improving exploration efficiency. Finally, to address the issue of frontiers misjudgment caused by obstacles within the scope of GVD units, a frontiers extraction method based on morphological dilation is designed to effectively ensure the reachability of frontiers. On this basis, a lightweight cost function is used to assess and switch to the next viewpoint in real time. This allows the robot to quickly adjust its strategy when signs of path backtracking appear, thereby escaping the predicament and increasing exploration flexibility. And the performance of system for exploration task is verified through comparative tests with SOTA methods.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u62d3\u6251\u5730\u56fe\u66f4\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u5c42\u6b21GVD\u751f\u6210\u3001\u8282\u70b9\u805a\u7c7b\u7ea6\u675f\u548c\u524d\u6cbf\u63d0\u53d6\uff0c\u63d0\u5347\u673a\u5668\u4eba\u63a2\u7d22\u80fd\u529b\u4e0e\u6548\u7387\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u63a2\u7d22\u4efb\u52a1\u4e2d\uff0c\u62d3\u6251\u5730\u56fe\u6bd4\u5ea6\u91cf\u5730\u56fe\u66f4\u4e3a\u9002\u7528\uff0c\u4f46\u5b9e\u65f6\u66f4\u65b0\u51c6\u786e\u4e14\u7ec6\u81f4\u7684\u73af\u5883\u62d3\u6251\u5730\u56fe\u4ecd\u7136\u9762\u4e34\u6311\u6218\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5bf9\u65b0\u89c2\u5bdf\u533a\u57df\u53bb\u566a\u3001\u591a\u5c42\u7ea7GVD\u751f\u6210\u3001\u8fde\u63a5\u6027\u7ea6\u675f\u7684\u8282\u70b9\u805a\u7c7b\u3001\u5f62\u6001\u81a8\u80c0\u7684\u524d\u6cbf\u63d0\u53d6\u4ee5\u53ca\u5b9e\u65f6\u6210\u672c\u51fd\u6570\u8bc4\u4f30\u4e0e\u5207\u6362\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e7f\u4e49Voronoi\u56fe\uff08GVD\uff09\u7684\u62d3\u6251\u5730\u56fe\u66f4\u65b0\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u63a2\u7d22\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6bd4\u8f83\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5176\u5728\u63a2\u7d22\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.18709", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18709", "abs": "https://arxiv.org/abs/2511.18709", "authors": ["Xueyan Oh", "Jonathan Her", "Zhixiang Ong", "Brandon Koh", "Yun Hann Tan", "U-Xuan Tan"], "title": "Autonomous Surface Selection For Manipulator-Based UV Disinfection In Hospitals Using Foundation Models", "comment": "7 pages, 7 figures; This paper has been accepted by IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)", "summary": "Ultraviolet (UV) germicidal radiation is an established non-contact method for surface disinfection in medical environments. Traditional approaches require substantial human intervention to define disinfection areas, complicating automation, while deep learning-based methods often need extensive fine-tuning and large datasets, which can be impractical for large-scale deployment. Additionally, these methods often do not address scene understanding for partial surface disinfection, which is crucial for avoiding unintended UV exposure. We propose a solution that leverages foundation models to simplify surface selection for manipulator-based UV disinfection, reducing human involvement and removing the need for model training. Additionally, we propose a VLM-assisted segmentation refinement to detect and exclude thin and small non-target objects, showing that this reduces mis-segmentation errors. Our approach achieves over 92\\% success rate in correctly segmenting target and non-target surfaces, and real-world experiments with a manipulator and simulated UV light demonstrate its practical potential for real-world applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u7d2b\u5916\u7ebf\u6d88\u6bd2\u8868\u9762\u9009\u62e9\u65b9\u6cd5\uff0c\u964d\u4f4e\u4eba\u7c7b\u5e72\u9884\u5e76\u6d88\u9664\u6a21\u578b\u8bad\u7ec3\u9700\u6c42\u3002", "motivation": "\u73b0\u6709\u7684\u7d2b\u5916\u7ebf\u6d88\u6bd2\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u4eba\u529b\u548c\u590d\u6742\u7684\u6a21\u578b\u8bad\u7ec3\uff0c\u96be\u4ee5\u5b9e\u73b0\u81ea\u52a8\u5316\u548c\u5927\u89c4\u6a21\u90e8\u7f72\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u90e8\u5206\u8868\u9762\u6d88\u6bd2\u7684\u573a\u666f\u7406\u89e3\u3002", "method": "\u5229\u7528\u57fa\u7840\u6a21\u578b\u7b80\u5316\u6d88\u6bd2\u8868\u9762\u9009\u62e9\uff0c\u5e76\u7ed3\u5408VLM\u8f85\u52a9\u7684\u7ec6\u5206\u4f18\u5316\u4ee5\u51cf\u5c11\u8bef\u5206\u5272\u9519\u8bef\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\uff0c\u8be5\u65b9\u6cd5\u5728\u76ee\u6807\u548c\u975e\u76ee\u6807\u8868\u9762\u5206\u5272\u4e2d\u8fbe\u621092%\u4ee5\u4e0a\u7684\u6210\u529f\u7387\uff0c\u5e76\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u5c55\u73b0\u5176\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u663e\u793a\u51fa\u826f\u597d\u7684\u6f5c\u529b\uff0c\u6210\u529f\u7387\u8d85\u8fc792%\u3002"}}
{"id": "2511.18712", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18712", "abs": "https://arxiv.org/abs/2511.18712", "authors": ["Tianyu Wang", "Chunxiang Yan", "Xuanhong Liao", "Tao Zhang", "Ping Wang", "Cong Wen", "Dingchuan Liu", "Haowen Yu", "Ximin Lyu"], "title": "Head Stabilization for Wheeled Bipedal Robots via Force-Estimation-Based Admittance Control", "comment": null, "summary": "Wheeled bipedal robots are emerging as flexible platforms for field exploration. However, head instability induced by uneven terrain can degrade the accuracy of onboard sensors or damage fragile payloads. Existing research primarily focuses on stabilizing the mobile platform but overlooks active stabilization of the head in the world frame, resulting in vertical oscillations that undermine overall stability. To address this challenge, we developed a model-based ground force estimation method for our 6-degree-of-freedom wheeled bipedal robot. Leveraging these force estimates, we implemented an admittance control algorithm to enhance terrain adaptability. Simulation experiments validated the real-time performance of the force estimator and the robot's robustness when traversing uneven terrain.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u63d0\u9ad8\u8f6e\u5f0f\u53cc\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e0a\u7684\u9002\u5e94\u80fd\u529b\uff0c\u91cd\u70b9\u89e3\u51b3\u4e86\u5934\u90e8\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u8f6e\u5f0f\u53cc\u8db3\u673a\u5668\u4eba\u5728\u4e0d\u5e73\u5766\u5730\u5f62\u4e0a\u5934\u90e8\u7684\u4e0d\u7a33\u5b9a\u6027\u5bf9\u4f20\u611f\u5668\u7cbe\u5ea6\u548c\u8f7d\u8377\u5b89\u5168\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6a21\u578b\u7684\u5730\u9762\u529b\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5e76\u5b9e\u65bd\u4e86\u5bb9\u8bb8\u63a7\u5236\u7b97\u6cd5\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u529b\u4f30\u8ba1\u5668\u7684\u5b9e\u65f6\u6027\u80fd\u53ca\u673a\u5668\u4eba\u5728\u4e0d\u5e73\u5766\u5730\u5f62\u4e0a\u7684\u7a33\u5065\u6027\u3002", "conclusion": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u6a21\u578b\u7684\u5730\u9762\u529b\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7ed3\u5408\u5bb9\u8bb8\u63a7\u5236\u7b97\u6cd5\uff0c\u63d0\u9ad8\u4e86\u8f6e\u5f0f\u53cc\u8db3\u673a\u5668\u4eba\u5728\u4e0d\u5e73\u5766\u5730\u5f62\u4e0a\u7684\u9002\u5e94\u6027\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2511.18718", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18718", "abs": "https://arxiv.org/abs/2511.18718", "authors": ["Omar Garib", "Jayaprakash D. Kambhampaty", "Olivia J. Pinon Fischer", "Dimitri N. Mavris"], "title": "AIRHILT: A Human-in-the-Loop Testbed for Multimodal Conflict Detection in Aviation", "comment": "9 pages, 4 figures, 1 table, 1 algorithm", "summary": "We introduce AIRHILT (Aviation Integrated Reasoning, Human-in-the-Loop Testbed), a modular and lightweight simulation environment designed to evaluate multimodal pilot and air traffic control (ATC) assistance systems for aviation conflict detection. Built on the open-source Godot engine, AIRHILT synchronizes pilot and ATC radio communications, visual scene understanding from camera streams, and ADS-B surveillance data within a unified, scalable platform. The environment supports pilot- and controller-in-the-loop interactions, providing a comprehensive scenario suite covering both terminal area and en route operational conflicts, including communication errors and procedural mistakes. AIRHILT offers standardized JSON-based interfaces that enable researchers to easily integrate, swap, and evaluate automatic speech recognition (ASR), visual detection, decision-making, and text-to-speech (TTS) models. We demonstrate AIRHILT through a reference pipeline incorporating fine-tuned Whisper ASR, YOLO-based visual detection, ADS-B-based conflict logic, and GPT-OSS-20B structured reasoning, and present preliminary results from representative runway-overlap scenarios, where the assistant achieves an average time-to-first-warning of approximately 7.7 s, with average ASR and vision latencies of approximately 5.9 s and 0.4 s, respectively. The AIRHILT environment and scenario suite are openly available, supporting reproducible research on multimodal situational awareness and conflict detection in aviation; code and scenarios are available at https://github.com/ogarib3/airhilt.", "AI": {"tldr": "AIRHILT\u662f\u4e00\u4e2a\u8f7b\u91cf\u5316\u7684\u822a\u7a7a\u6a21\u62df\u73af\u5883\uff0c\u7528\u4e8e\u8bc4\u4f30\u98de\u884c\u51b2\u7a81\u68c0\u6d4b\u7684\u591a\u6a21\u6001\u7cfb\u7edf\uff0c\u652f\u6301\u5404\u79cd\u4ea4\u4e92\u5e76\u63d0\u4f9b\u5f00\u653e\u4ee3\u7801\u4ee5\u4fc3\u8fdb\u7814\u7a76\u3002", "motivation": "\u968f\u7740\u822a\u7a7a\u4ea4\u901a\u7684\u589e\u52a0\uff0c\u4f18\u5316\u98de\u884c\u5458\u4e0e\u7a7a\u7ba1\u7684\u534f\u4f5c\u80fd\u529b\uff0c\u53ca\u65f6\u4e0e\u51c6\u786e\u7684\u51b2\u7a81\u68c0\u6d4b\u53d8\u5f97\u6108\u53d1\u91cd\u8981\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u6709\u6548\u7684\u6d4b\u8bd5\u5e73\u53f0\u6765\u8bc4\u4f30\u76f8\u5173\u6280\u672f\u3002", "method": "AIRHILT\u96c6\u6210\u4e86\u98de\u884c\u5458\u548cATC\u7684\u65e0\u7ebf\u7535\u901a\u4fe1\u3001\u89c6\u89c9\u573a\u666f\u7406\u89e3\u4ee5\u53caADS-B\u76d1\u89c6\u6570\u636e\uff0c\u652f\u6301\u591a\u6837\u5316\u7684\u4ea4\u4e92\uff0c\u63d0\u4f9b\u4e86\u4e00\u7cfb\u5217\u6807\u51c6\u5316\u7684JSON\u63a5\u53e3\u4ee5\u65b9\u4fbf\u7814\u7a76\u4eba\u5458\u548c\u6a21\u578b\u96c6\u6210\u3002", "result": "\u672c\u6587\u4ecb\u7ecd\u4e86AIRHILT\uff08\u822a\u7a7a\u7efc\u5408\u63a8\u7406\uff0c\u4eba\u673a\u534f\u540c\u6d4b\u8bd5\u5e73\u53f0\uff09\uff0c\u4e00\u4e2a\u6a21\u5757\u5316\u4e14\u8f7b\u91cf\u7684\u4eff\u771f\u73af\u5883\uff0c\u65e8\u5728\u8bc4\u4f30\u822a\u7a7a\u51b2\u7a81\u68c0\u6d4b\u4e2d\u7684\u591a\u6a21\u6001\u98de\u884c\u5458\u548c\u7a7a\u4e2d\u4ea4\u901a\u7ba1\u5236\uff08ATC\uff09\u8f85\u52a9\u7cfb\u7edf\u3002", "conclusion": "AIRHILT\u73af\u5883\u548c\u573a\u666f\u5957\u4ef6\u7684\u5f00\u653e\u6027\u652f\u6301\u4e86\u822a\u7a7a\u9886\u57df\u591a\u6a21\u6001\u6001\u52bf\u611f\u77e5\u548c\u51b2\u7a81\u68c0\u6d4b\u7684\u53ef\u91cd\u590d\u7814\u7a76\u3002"}}
{"id": "2511.18756", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18756", "abs": "https://arxiv.org/abs/2511.18756", "authors": ["Xueyu Du", "Lilian Zhang", "Fuan Duan", "Xincan Luo", "Maosong Wang", "Wenqi Wu", "JunMao"], "title": "SP-VINS: A Hybrid Stereo Visual Inertial Navigation System based on Implicit Environmental Map", "comment": null, "summary": "Filter-based visual inertial navigation system (VINS) has attracted mobile-robot researchers for the good balance between accuracy and efficiency, but its limited mapping quality hampers long-term high-accuracy state estimation. To this end, we first propose a novel filter-based stereo VINS, differing from traditional simultaneous localization and mapping (SLAM) systems based on 3D map, which performs efficient loop closure constraints with implicit environmental map composed of keyframes and 2D keypoints. Secondly, we proposed a hybrid residual filter framework that combines landmark reprojection and ray constraints to construct a unified Jacobian matrix for measurement updates. Finally, considering the degraded environment, we incorporated the camera-IMU extrinsic parameters into visual description to achieve online calibration. Benchmark experiments demonstrate that the proposed SP-VINS achieves high computational efficiency while maintaining long-term high-accuracy localization performance, and is superior to existing state-of-the-art (SOTA) methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u7acb\u4f53\u89c6\u89c9\u60ef\u6027\u5bfc\u822a\u7cfb\u7edf\uff0c\u589e\u5f3a\u4e86\u6620\u5c04\u8d28\u91cf\u548c\u957f\u671f\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u8fc7\u6ee4\u5668\u57fa\u7684\u89c6\u89c9\u60ef\u6027\u5bfc\u822a\u7cfb\u7edf\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5e73\u8861\uff0c\u4f46\u5176\u6620\u5c04\u8d28\u91cf\u6709\u9650\uff0c\u963b\u788d\u4e86\u9ad8\u7cbe\u5ea6\u7684\u957f\u671f\u72b6\u6001\u4f30\u8ba1\u3002", "method": "\u9996\u5148\u63d0\u51fa\u4e00\u79cd\u4e0d\u540c\u4e8e\u4f20\u7edfSLAM\u7cfb\u7edf\u7684\u7acb\u4f53VINS\uff0c\u5e94\u7528\u9690\u5f0f\u73af\u5883\u5730\u56fe\u8fdb\u884c\u6709\u6548\u95ed\u73af\u7ea6\u675f\u3002\u5176\u6b21\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u7ec4\u5408\u5730\u6807\u91cd\u6295\u5f71\u4e0e\u5149\u7ebf\u7ea6\u675f\u7684\u6df7\u5408\u6b8b\u5dee\u6ee4\u6ce2\u6846\u67b6\uff0c\u6700\u540e\u9488\u5bf9\u6076\u52a3\u73af\u5883\u8fdb\u884c\u5728\u7ebf\u6807\u5b9a\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8fc7\u6ee4\u5668\u57fa\u7840\u7684\u7acb\u4f53\u89c6\u89c9\u60ef\u6027\u5bfc\u822a\u7cfb\u7edf\uff0c\u5177\u6709\u5f88\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684SP-VINS\u5728\u8ba1\u7b97\u6548\u7387\u4e0e\u957f\u671f\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2511.18810", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18810", "abs": "https://arxiv.org/abs/2511.18810", "authors": ["Yuxia Fu", "Zhizhen Zhang", "Yuqi Zhang", "Zijian Wang", "Zi Huang", "Yadan Luo"], "title": "MergeVLA: Cross-Skill Model Merging Toward a Generalist Vision-Language-Action Agent", "comment": null, "summary": "Recent Vision-Language-Action (VLA) models reformulate vision-language models by tuning them with millions of robotic demonstrations. While they perform well when fine-tuned for a single embodiment or task family, extending them to multi-skill settings remains challenging: directly merging VLA experts trained on different tasks results in near-zero success rates. This raises a fundamental question: what prevents VLAs from mastering multiple skills within one model? With an empirical decomposition of learnable parameters during VLA fine-tuning, we identify two key sources of non-mergeability: (1) Finetuning drives LoRA adapters in the VLM backbone toward divergent, task-specific directions beyond the capacity of existing merging methods to unify. (2) Action experts develop inter-block dependencies through self-attention feedback, causing task information to spread across layers and preventing modular recombination. To address these challenges, we present MergeVLA, a merging-oriented VLA architecture that preserves mergeability by design. MergeVLA introduces sparsely activated LoRA adapters via task masks to retain consistent parameters and reduce irreconcilable conflicts in the VLM. Its action expert replaces self-attention with cross-attention-only blocks to keep specialization localized and composable. When the task is unknown, it uses a test-time task router to adaptively select the appropriate task mask and expert head from the initial observation, enabling unsupervised task inference. Across LIBERO, LIBERO-Plus, RoboTwin, and multi-task experiments on the real SO101 robotic arm, MergeVLA achieves performance comparable to or even exceeding individually finetuned experts, demonstrating robust generalization across tasks, embodiments, and environments.", "AI": {"tldr": "MergeVLA\u662f\u4e00\u79cd\u65b0\u578bVLA\u67b6\u6784\uff0c\u901a\u8fc7\u8bbe\u8ba1\u4fdd\u6301\u53ef\u5408\u5e76\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u6280\u80fd\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u76ee\u524d\u7684VLA\u6a21\u578b\u5728\u591a\u6280\u80fd\u8bbe\u7f6e\u4e2d\u7684\u5408\u5e76\u6548\u679c\u6781\u5dee\uff0c\u6fc0\u53d1\u4e86\u5bf9\u5355\u4e2a\u6a21\u578b\u638c\u63e1\u591a\u9879\u6280\u80fd\u7684\u7814\u7a76\u5174\u8da3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cdMergeVLA\u67b6\u6784\uff0c\u901a\u8fc7\u8bbe\u8ba1\u4fdd\u6301\u53ef\u5408\u5e76\u6027\uff0c\u5f15\u5165\u7a00\u758f\u6fc0\u6d3b\u7684LoRA\u9002\u914d\u5668\u548c\u6d4b\u8bd5\u65f6\u4efb\u52a1\u8def\u7531\u5668\u3002", "result": "MergeVLA\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c\u771f\u5b9e\u673a\u68b0\u81c2\u4e0a\u7684\u5b9e\u9a8c\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u5176\u67b6\u6784\u7684\u6709\u6548\u6027\u3002", "conclusion": "MergeVLA\u5728\u591a\u4e2a\u4efb\u52a1\u3001\u4f53\u6001\u548c\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5176\u6027\u80fd\u53ef\u4e0e\u5355\u72ec\u5fae\u8c03\u7684\u4e13\u5bb6\u76f8\u5ab2\u7f8e\u6216\u66f4\u4f73\u3002"}}
{"id": "2511.18857", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18857", "abs": "https://arxiv.org/abs/2511.18857", "authors": ["Changsheng Luo", "Yushi Wang", "Wenhan Cai", "Mingguo Zhao"], "title": "AutoOdom: Learning Auto-regressive Proprioceptive Odometry for Legged Locomotion", "comment": null, "summary": "Accurate proprioceptive odometry is fundamental for legged robot navigation in GPS-denied and visually degraded environments where conventional visual odometry systems fail. Current approaches face critical limitations: analytical filtering methods suffer from modeling uncertainties and cumulative drift, hybrid learning-filtering approaches remain constrained by their analytical components, while pure learning-based methods struggle with simulation-to-reality transfer and demand extensive real-world data collection. This paper introduces AutoOdom, a novel autoregressive proprioceptive odometry system that overcomes these challenges through an innovative two-stage training paradigm. Stage 1 employs large-scale simulation data to learn complex nonlinear dynamics and rapidly changing contact states inherent in legged locomotion, while Stage 2 introduces an autoregressive enhancement mechanism using limited real-world data to effectively bridge the sim-to-real gap. The key innovation lies in our autoregressive training approach, where the model learns from its own predictions to develop resilience against sensor noise and improve robustness in highly dynamic environments. Comprehensive experimental validation on the Booster T1 humanoid robot demonstrates that AutoOdom significantly outperforms state-of-the-art methods across all evaluation metrics, achieving 57.2% improvement in absolute trajectory error, 59.2% improvement in Umeyama-aligned error, and 36.2% improvement in relative pose error compared to the Legolas baseline. Extensive ablation studies provide critical insights into sensor modality selection and temporal modeling, revealing counterintuitive findings about IMU acceleration data and validating our systematic design choices for robust proprioceptive odometry in challenging locomotion scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a AutoOdom \u7684\u65b0\u578b\u81ea\u56de\u5f52\u672c\u4f53\u89c9\u91cc\u7a0b\u8ba1\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u548c\u81ea\u56de\u5f52\u589e\u5f3a\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u817f\u90e8\u673a\u5668\u4eba\u7684\u5bfc\u822a\u7cbe\u5ea6\uff0c\u5e76\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u52a8\u529b\u5b66\u5efa\u6a21\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u7d2f\u79ef\u6f02\u79fb\u662f\u73b0\u6709\u672c\u4f53\u89c9\u91cc\u7a0b\u8ba1\u65b9\u6cd5\u7684\u4e3b\u8981\u5c40\u9650\u6027\uff0c\u672c\u6587\u65e8\u5728\u6539\u5584\u817f\u90e8\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4e24\u9636\u6bb5\u8bad\u7ec3\uff0c\u7b2c\u4e00\u9636\u6bb5\u5229\u7528\u5927\u89c4\u6a21\u4eff\u771f\u6570\u636e\u5b66\u4e60\u590d\u6742\u7684\u975e\u7ebf\u6027\u52a8\u529b\u5b66\uff0c\u7b2c\u4e8c\u9636\u6bb5\u5219\u901a\u8fc7\u6709\u9650\u7684\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u5f15\u5165\u81ea\u56de\u5f52\u589e\u5f3a\u673a\u5236\uff0c\u5f25\u5408\u4e86\u4eff\u771f\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "result": "AutoOdom \u662f\u4e00\u79cd\u521b\u65b0\u7684\u81ea\u56de\u5f52\u672c\u4f53\u89c9\u91cc\u7a0b\u8ba1\u7cfb\u7edf\uff0c\u901a\u8fc7\u72ec\u7279\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\u89e3\u51b3\u4e86\u5f53\u524d\u65b9\u6cd5\u5728\u65e0GPS\u548c\u89c6\u89c9\u73af\u5883\u4e2d\u7684\u8bf8\u591a\u9650\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u817f\u90e8\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAutoOdom \u5728\u5404\u9879\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u660e\u663e\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7684\u7a33\u5b9a\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.18878", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18878", "abs": "https://arxiv.org/abs/2511.18878", "authors": ["Suzie Kim", "Hye-Bin Shin", "Hyo-Jeong Jang"], "title": "Accelerating Reinforcement Learning via Error-Related Human Brain Signals", "comment": null, "summary": "In this work, we investigate how implicit neural feed back can accelerate reinforcement learning in complex robotic manipulation settings. While prior electroencephalogram (EEG) guided reinforcement learning studies have primarily focused on navigation or low-dimensional locomotion tasks, we aim to understand whether such neural evaluative signals can improve policy learning in high-dimensional manipulation tasks involving obstacles and precise end-effector control. We integrate error related potentials decoded from offline-trained EEG classifiers into reward shaping and systematically evaluate the impact of human-feedback weighting. Experiments on a 7-DoF manipulator in an obstacle-rich reaching environment show that neural feedback accelerates reinforcement learning and, depending on the human-feedback weighting, can yield task success rates that at times exceed those of sparse-reward baselines. Moreover, when applying the best-performing feedback weighting across all sub jects, we observe consistent acceleration of reinforcement learning relative to the sparse-reward setting. Furthermore, leave-one subject-out evaluations confirm that the proposed framework remains robust despite the intrinsic inter-individual variability in EEG decodability. Our findings demonstrate that EEG-based reinforcement learning can scale beyond locomotion tasks and provide a viable pathway for human-aligned manipulation skill acquisition.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u9690\u5f0f\u795e\u7ecf\u53cd\u9988\u52a0\u901f\u590d\u6742\u673a\u5668\u4eba\u64cd\u63a7\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u96c6\u6210EEG\u89e3\u7801\u7684\u8bef\u5dee\u76f8\u5173\u7535\u4f4d\uff0c\u53d1\u73b0\u6b64\u65b9\u6cd5\u5728\u9ad8\u7ef4\u64cd\u63a7\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u65e8\u5728\u8bc4\u4f30\u795e\u7ecf\u53cd\u9988\u4fe1\u53f7\u662f\u5426\u80fd\u591f\u6539\u5584\u9ad8\u7ef4\u64cd\u63a7\u4efb\u52a1\u4e2d\u7684\u7b56\u7565\u5b66\u4e60\uff0c\u8d85\u8d8a\u4e4b\u524d\u5bf9\u5bfc\u822a\u6216\u4f4e\u7ef4\u8fd0\u52a8\u4efb\u52a1\u7684\u7814\u7a76\u3002", "method": "\u5c06\u79bb\u7ebf\u8bad\u7ec3\u7684EEG\u5206\u7c7b\u5668\u89e3\u7801\u7684\u8bef\u5dee\u76f8\u5173\u7535\u4f4d\u96c6\u6210\u5165\u5956\u52b1\u5851\u9020\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4eba\u7c7b\u53cd\u9988\u6743\u91cd\u7684\u5f71\u54cd\u3002", "result": "\u5728\u590d\u6742\u7684\u969c\u788d\u73af\u5883\u4e2d\uff0c\u795e\u7ecf\u53cd\u9988\u52a0\u901f\u4e86\u5f3a\u5316\u5b66\u4e60\uff0c\u4efb\u52a1\u6210\u529f\u7387\u6709\u65f6\u8d85\u8fc7\u7a00\u758f\u5956\u52b1\u57fa\u51c6\uff0c\u4e14\u91c7\u7528\u6700\u4f73\u53cd\u9988\u6743\u91cd\u7684\u4e00\u81f4\u6027\u52a0\u901f\u6548\u679c\u663e\u8457\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8eEEG\u7684\u5f3a\u5316\u5b66\u4e60\u5728\u673a\u5668\u4eba\u64cd\u63a7\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\uff0c\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5e76\u5728\u4e0d\u540c\u4e3b\u4f53\u95f4\u4fdd\u6301\u7a33\u5b9a\u6027\u3002"}}
{"id": "2511.18910", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18910", "abs": "https://arxiv.org/abs/2511.18910", "authors": ["Samuel Cerezo", "Seong Hun Lee", "Javier Civera"], "title": "An Efficient Closed-Form Solution to Full Visual-Inertial State Initialization", "comment": "8 pages, 2 figures, 10 tables. Submitted to RA-L", "summary": "In this letter, we present a closed-form initialization method that recovers the full visual-inertial state without nonlinear optimization. Unlike previous approaches that rely on iterative solvers, our formulation yields analytical, easy-to-implement, and numerically stable solutions for reliable start-up. Our method builds on small-rotation and constant-velocity approximations, which keep the formulation compact while preserving the essential coupling between motion and inertial measurements. We further propose an observability-driven, two-stage initialization scheme that balances accuracy with initialization latency. Extensive experiments on the EuRoC dataset validate our assumptions: our method achieves 10-20% lower initialization error than optimization-based approaches, while using 4x shorter initialization windows and reducing computational cost by 5x.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u95ed\u5f0f\u5f62\u5f0f\u7684\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u65e0\u975e\u7ebf\u6027\u4f18\u5316\u7684\u60c5\u51b5\u4e0b\u6062\u590d\u5b8c\u6574\u7684\u89c6\u89c9-\u60ef\u6027\u72b6\u6001\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u521d\u59cb\u5316\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u73b0\u6709\u8fed\u4ee3\u6c42\u89e3\u65b9\u6cd5\u7684\u7f3a\u9677\uff0c\u5c24\u5176\u662f\u5728\u521d\u59cb\u5316\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\uff0c\u5f00\u53d1\u4e00\u79cd\u66f4\u7a33\u5b9a\u548c\u5feb\u901f\u7684\u521d\u59cb\u5316\u6280\u672f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u95ed\u5f0f\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u4ee5\u53ca\u89c2\u5bdf\u6027\u9a71\u52a8\u7684\u4e24\u9636\u6bb5\u521d\u59cb\u5316\u65b9\u6848\uff0c\u7ed3\u5408\u5c0f\u65cb\u8f6c\u548c\u6052\u901f\u5047\u8bbe\uff0c\u786e\u4fdd\u4e86\u7cbe\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "result": "\u5728EuRoC\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u6bd4\u4f18\u5316\u57fa\u65b9\u6cd5\u964d\u4f4e\u4e8610-20%\u7684\u521d\u59cb\u5316\u8bef\u5dee\uff0c\u521d\u59cb\u5316\u7a97\u53e3\u7f29\u77ed\u4e864\u500d\uff0c\u8ba1\u7b97\u6210\u672c\u51cf\u5c11\u4e865\u500d\u3002", "conclusion": "\u672c\u6587\u7684\u65b9\u6cd5\u6bd4\u57fa\u4e8e\u4f18\u5316\u7684\u65b9\u6cd5\u5177\u6709\u66f4\u4f4e\u7684\u521d\u59cb\u5316\u8bef\u5dee\uff0c\u4ee5\u53ca\u66f4\u77ed\u7684\u521d\u59cb\u5316\u7a97\u53e3\u548c\u66f4\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u5177\u6709\u66f4\u597d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2511.18950", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18950", "abs": "https://arxiv.org/abs/2511.18950", "authors": ["Juntao Gao", "Feiyang Ye", "Jing Zhang", "Wenjing Qian"], "title": "Compressor-VLA: Instruction-Guided Visual Token Compression for Efficient Robotic Manipulation", "comment": "11 pages, 5 figures", "summary": "Vision-Language-Action (VLA) models have emerged as a powerful paradigm in Embodied AI. However, the significant computational overhead of processing redundant visual tokens remains a critical bottleneck for real-time robotic deployment. While standard token pruning techniques can alleviate this, these task-agnostic methods struggle to preserve task-critical visual information. To address this challenge, simultaneously preserving both the holistic context and fine-grained details for precise action, we propose Compressor-VLA, a novel hybrid instruction-conditioned token compression framework designed for efficient, task-oriented compression of visual information in VLA models. The proposed Compressor-VLA framework consists of two token compression modules: a Semantic Task Compressor (STC) that distills holistic, task-relevant context, and a Spatial Refinement Compressor (SRC) that preserves fine-grained spatial details. This compression is dynamically modulated by the natural language instruction, allowing for the adaptive condensation of task-relevant visual information. Experimentally, extensive evaluations demonstrate that Compressor-VLA achieves a competitive success rate on the LIBERO benchmark while reducing FLOPs by 59% and the visual token count by over 3x compared to its baseline. The real-robot deployments on a dual-arm robot platform validate the model's sim-to-real transferability and practical applicability. Moreover, qualitative analyses reveal that our instruction guidance dynamically steers the model's perceptual focus toward task-relevant objects, thereby validating the effectiveness of our approach.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCompressor-VLA\u7684\u9ad8\u6548\u89c6\u89c9\u4fe1\u606f\u538b\u7f29\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u4efb\u52a1\u76f8\u5173\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u7684\u4f18\u5316\u3002", "motivation": "\u8bbe\u8ba1\u9ad8\u6548\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u4f46\u5904\u7406\u5197\u4f59\u89c6\u89c9\u6807\u8bb0\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u5927\u3002", "method": "Compressor-VLA\u7531\u4e24\u4e2a\u538b\u7f29\u6a21\u5757\u7ec4\u6210\uff1a\u8bed\u4e49\u4efb\u52a1\u538b\u7f29\u5668(STC)\u548c\u7a7a\u95f4\u7ec6\u5316\u538b\u7f29\u5668(SRC)\uff0c\u52a8\u6001\u8c03\u8282\u4ee5\u9002\u5e94\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u3002", "result": "\u63d0\u51fa\u4e86Compressor-VLA\u6846\u67b6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4efb\u52a1\u76f8\u5173\u7684\u89c6\u89c9\u4fe1\u606f\u3002", "conclusion": "Compressor-VLA\u5728\u5b9e\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u8d85\u8fc7\u4e86\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2511.19011", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19011", "abs": "https://arxiv.org/abs/2511.19011", "authors": ["Jiale Zhang", "Yeqiang Qian", "Tong Qin", "Mingyang Jiang", "Siyuan Chen", "Ming Yang"], "title": "End-to-end Autonomous Vehicle Following System using Monocular Fisheye Camera", "comment": null, "summary": "The increase in vehicle ownership has led to increased traffic congestion, more accidents, and higher carbon emissions. Vehicle platooning is a promising solution to address these issues by improving road capacity and reducing fuel consumption. However, existing platooning systems face challenges such as reliance on lane markings and expensive high-precision sensors, which limits their general applicability. To address these issues, we propose a vehicle following framework that expands its capability from restricted scenarios to general scenario applications using only a camera. This is achieved through our newly proposed end-to-end method, which improves overall driving performance. The method incorporates a semantic mask to address causal confusion in multi-frame data fusion. Additionally, we introduce a dynamic sampling mechanism to precisely track the trajectories of preceding vehicles. Extensive closed-loop validation in real-world vehicle experiments demonstrates the system's ability to follow vehicles in various scenarios, outperforming traditional multi-stage algorithms. This makes it a promising solution for cost-effective autonomous vehicle platooning. A complete real-world vehicle experiment is available at https://youtu.be/zL1bcVb9kqQ.", "AI": {"tldr": "\u9488\u5bf9\u8f66\u8f86\u6240\u6709\u91cf\u589e\u52a0\u5e26\u6765\u7684\u4ea4\u901a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6444\u50cf\u5934\u7684\u8f66\u8f86\u8ddf\u968f\u6846\u67b6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u961f\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u8f66\u8f86\u62e5\u6709\u91cf\u589e\u52a0\uff0c\u4ea4\u901a\u62e5\u5835\u3001\u4e8b\u6545\u9891\u53d1\u548c\u78b3\u6392\u653e\u4e0a\u5347\u95ee\u9898\u65e5\u76ca\u4e25\u91cd\u3002", "method": "\u91c7\u7528\u65b0\u7684\u7aef\u5230\u7aef\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u63a9\u7801\u5904\u7406\u591a\u5e27\u6570\u636e\u878d\u5408\u4e2d\u7684\u56e0\u679c\u6df7\u6dc6\uff0c\u7ed3\u5408\u52a8\u6001\u91c7\u6837\u673a\u5236\u7cbe\u786e\u8ddf\u8e2a\u524d\u8f66\u8f68\u8ff9\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u5229\u7528\u6444\u50cf\u5934\u7684\u8f66\u8f86\u8ddf\u968f\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4e00\u822c\u573a\u666f\u4e2d\u5e94\u7528\uff0c\u6539\u8fdb\u4e86\u9a7e\u9a76\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u8f66\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u7cfb\u7edf\u5728\u591a\u79cd\u573a\u666f\u4e0b\u7684\u4f18\u8d8a\u6027\uff0c\u6210\u4e3a\u6210\u672c\u6548\u76ca\u9ad8\u7684\u81ea\u52a8\u9a7e\u9a76\u8f66\u961f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.19031", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19031", "abs": "https://arxiv.org/abs/2511.19031", "authors": ["Haihang Wu", "Yuchen Zhou"], "title": "Multi-Agent Monocular Dense SLAM With 3D Reconstruction Priors", "comment": null, "summary": "Monocular Simultaneous Localization and Mapping (SLAM) aims to estimate a robot's pose while simultaneously reconstructing an unknown 3D scene using a single camera. While existing monocular SLAM systems generate detailed 3D geometry through dense scene representations, they are computationally expensive due to the need for iterative optimization. To address this challenge, MASt3R-SLAM utilizes learned 3D reconstruction priors, enabling more efficient and accurate estimation of both 3D structures and camera poses. However, MASt3R-SLAM is limited to single-agent operation. In this paper, we extend MASt3R-SLAM to introduce the first multi-agent monocular dense SLAM system. Each agent performs local SLAM using a 3D reconstruction prior, and their individual maps are fused into a globally consistent map through a loop-closure-based map fusion mechanism. Our approach improves computational efficiency compared to state-of-the-art methods, while maintaining similar mapping accuracy when evaluated on real-world datasets.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u5355\u76ee\u7a20\u5bc6SLAM\u7cfb\u7edf\uff0c\u6269\u5c55\u4e86MASt3R-SLAM\uff0c\u5229\u7528\u5b66\u4e60\u76843D\u91cd\u5efa\u5148\u9a8c\u63d0\u9ad8\u4e86\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u76eeSLAM\u7cfb\u7edf\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u6539\u8fdb\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u6bcf\u4e2a\u667a\u80fd\u4f53\u5229\u75283D\u91cd\u5efa\u5148\u9a8c\u8fdb\u884c\u5c40\u90e8SLAM\uff0c\u5c06\u5404\u81ea\u7684\u5730\u56fe\u901a\u8fc7\u57fa\u4e8e\u56de\u73af\u95ed\u5408\u7684\u5730\u56fe\u878d\u5408\u673a\u5236\u6574\u5408\u4e3a\u5168\u5c40\u4e00\u81f4\u7684\u5730\u56fe\u3002", "result": "\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u4f3c\u7684\u6620\u5c04\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u663e\u793a\u51fa\u8f83\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\u4e0e\u76f8\u4f3c\u7684\u6620\u5c04\u7cbe\u5ea6\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.19094", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19094", "abs": "https://arxiv.org/abs/2511.19094", "authors": ["David Bricher", "Andreas Mueller"], "title": "Analysis of Deep-Learning Methods in an ISO/TS 15066-Compliant Human-Robot Safety Framework", "comment": "MDPI Sensors, published 22 November 2025", "summary": "Over the last years collaborative robots have gained great success in manufacturing applications where human and robot work together in close proximity. However, current ISO/TS-15066-compliant implementations often limit the efficiency of collaborative tasks due to conservative speed restrictions. For this reason, this paper introduces a deep-learning-based human-robot-safety framework (HRSF) that aims at a dynamical adaptation of robot velocities depending on the separation distance between human and robot while respecting maximum biomechanical force and pressure limits. The applicability of the framework was investigated for four different deep learning approaches that can be used for human body extraction: human body recognition, human body segmentation, human pose estimation, and human body part segmentation. Unlike conventional industrial safety systems, the proposed HRSF differentiates individual human body parts from other objects, enabling optimized robot process execution. Experiments demonstrated a quantitative reduction in cycle time of up to 15% compared to conventional safety technology.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4eba\u673a\u5b89\u5168\u6846\u67b6\uff0c\u65e8\u5728\u6839\u636e\u4eba\u673a\u95f4\u8ddd\u52a8\u6001\u8c03\u6574\u673a\u5668\u4eba\u901f\u5ea6\uff0c\u540c\u65f6\u9075\u5faa\u751f\u7269\u529b\u5b66\u5b89\u5168\u9650\u5236\uff0c\u63d0\u5347\u534f\u4f5c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684ISO\u6807\u51c6\u5b9e\u65bd\u5bf9\u673a\u5668\u4eba\u7684\u901f\u5ea6\u9650\u5236\u8fc7\u4e8e\u4fdd\u5b88\uff0c\u5f71\u54cd\u4e86\u534f\u4f5c\u4efb\u52a1\u7684\u6548\u7387\u3002", "method": "\u91c7\u7528\u56db\u79cd\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u4eba\u4f53\u63d0\u53d6\uff0c\u4ee5\u5b9e\u73b0\u52a8\u6001\u8c03\u6574\u673a\u5668\u4eba\u901f\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u5b89\u5168\u6280\u672f\u76f8\u6bd4\uff0c\u5468\u671f\u65f6\u95f4\u51cf\u5c11\u4e86\u9ad8\u8fbe15%\u3002", "conclusion": "HRSF\u663e\u8457\u63d0\u9ad8\u4e86\u4eba\u673a\u534f\u4f5c\u7684\u6548\u7387\uff0c\u51cf\u5c11\u4e86\u673a\u5668\u4eba\u5de5\u4f5c\u5468\u671f\u65f6\u95f4\u3002"}}
{"id": "2511.19135", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19135", "abs": "https://arxiv.org/abs/2511.19135", "authors": ["Pascal Goldschmid", "Aamir Ahmad"], "title": "Autonomous Docking of Multi-Rotor UAVs on Blimps under the Influence of Wind Gusts", "comment": "13 pages, 8 figures, 8 tables", "summary": "Multi-rotor UAVs face limited flight time due to battery constraints. Autonomous docking on blimps with onboard battery recharging and data offloading offers a promising solution for extended UAV missions. However, the vulnerability of blimps to wind gusts causes trajectory deviations, requiring precise, obstacle-aware docking strategies. To this end, this work introduces two key novelties: (i) a temporal convolutional network that predicts blimp responses to wind gusts, enabling rapid gust detection and estimation of points where the wind gust effect has subsided; (ii) a model predictive controller (MPC) that leverages these predictions to compute collision-free trajectories for docking, enabled by a novel obstacle avoidance method for close-range manoeuvres near the blimp. Simulation results show our method outperforms a baseline constant-velocity model of the blimp significantly across different scenarios. We further validate the approach in real-world experiments, demonstrating the first autonomous multi-rotor docking control strategy on blimps shown outside simulation. Source code is available here https://github.com/robot-perception-group/multi_rotor_airship_docking.", "AI": {"tldr": "\u65e0\u4eba\u673a\u901a\u8fc7\u81ea\u52a8\u5bf9\u63a5\u6c14\u56ca\u8fdb\u884c\u5145\u7535\u548c\u6570\u636e\u5378\u8f7d\uff0c\u4ee5\u5ef6\u957f\u98de\u884c\u65f6\u95f4\u3002", "motivation": "\u7531\u4e8e\u98ce\u7684\u5f71\u54cd\uff0c\u6c14\u56ca\u5728\u65e0\u4eba\u673a\u5bf9\u63a5\u65f6\u7684\u8f68\u8ff9\u53ef\u80fd\u504f\u79bb\uff0c\u56e0\u6b64\u9700\u8981\u7cbe\u51c6\u7684\u907f\u969c\u5bf9\u63a5\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u65f6\u95f4\u5377\u79ef\u7f51\u7edc\u9884\u6d4b\u6c14\u56ca\u5bf9\u98ce\u7684\u53cd\u5e94\uff0c\u5e76\u5e94\u7528\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\u8ba1\u7b97\u65e0\u969c\u788d\u5bf9\u63a5\u8f68\u8ff9\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u573a\u666f\u4e2d\u660e\u663e\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\uff0c\u5e76\u5728\u5b9e\u9645\u5b9e\u9a8c\u4e2d\u6210\u529f\u5b9e\u73b0\u81ea\u4e3b\u7ba1\u7406\u5bf9\u63a5\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u7684\u6a21\u578b\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u5e38\u901f\u6a21\u578b\uff0c\u5e76\u5728\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u81ea\u4e3b\u7ba1\u7406\u591a\u65cb\u7ffc\u65e0\u4eba\u673a\u5bf9\u63a5\u6c14\u56ca\u7684\u63a7\u5236\u7b56\u7565\u3002"}}
{"id": "2511.19201", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19201", "abs": "https://arxiv.org/abs/2511.19201", "authors": ["Ann-Sophia M\u00fcller", "Moonkwang Jeong", "Jiyuan Tian", "Meng Zhang", "Tian Qiu"], "title": "Efficient Optimization of a Permanent Magnet Array for a Stable 2D Trap", "comment": "6 pages, 6 figures, IEEE International Conference on Robotics and Automation (ICRA)", "summary": "Untethered magnetic manipulation of biomedical millirobots has a high potential for minimally invasive surgical applications. However, it is still challenging to exert high actuation forces on the small robots over a large distance. Permanent magnets offer stronger magnetic torques and forces than electromagnetic coils, however, feedback control is more difficult. As proven by Earnshaw's theorem, it is not possible to achieve a stable magnetic trap in 3D by static permanent magnets. Here, we report a stable 2D magnetic force trap by an array of permanent magnets to control a millirobot. The trap is located in an open space with a tunable distance to the magnet array in the range of 20 - 120mm, which is relevant to human anatomical scales. The design is achieved by a novel GPU-accelerated optimization algorithm that uses mean squared error (MSE) and Adam optimizer to efficiently compute the optimal angles for any number of magnets in the array. The algorithm is verified using numerical simulation and physical experiments with an array of two magnets. A millirobot is successfully trapped and controlled to follow a complex trajectory. The algorithm demonstrates high scalability by optimizing the angles for 100 magnets in under three seconds. Moreover, the optimization workflow can be adapted to optimize a permanent magnet array to achieve the desired force vector fields.", "AI": {"tldr": "\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u6c38\u4e45\u78c1\u94c1\u76842D\u78c1\u529b\u6355\u83b7\u88c5\u7f6e\uff0c\u4ee5\u63a7\u5236\u751f\u7269\u533b\u5b66\u6beb\u7c73\u673a\u5668\u4eba\uff0c\u9002\u7528\u4e8e\u5fae\u521b\u624b\u672f\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5728\u5927\u8ddd\u79bb\u4e0a\u5bf9\u5c0f\u578b\u673a\u5668\u4eba\u65bd\u52a0\u9ad8\u9a71\u52a8\u529b\u7684\u6311\u6218\uff0c\u63a8\u52a8\u5fae\u521b\u624b\u672f\u7684\u53d1\u5c55\u3002", "method": "\u5229\u7528GPU\u52a0\u901f\u7684\u4f18\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\u548cAdam\u4f18\u5316\u5668\u6765\u8ba1\u7b97\u6c38\u78c1\u4f53\u9635\u5217\u7684\u6700\u4f73\u89d2\u5ea6\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u80fd\u591f\u572820-120mm\u8303\u56f4\u5185\u8c03\u8282\u7684\u7a33\u5b9a2D\u78c1\u529b\u9677\u9631\uff0c\u5e76\u6210\u529f\u63a7\u5236\u4e86\u6beb\u7c73\u673a\u5668\u4eba\u6cbf\u590d\u6742\u8f68\u8ff9\u79fb\u52a8\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u5f00\u653e\u7a7a\u95f4\u5185\u6355\u6349\u548c\u63a7\u5236\u6beb\u7c73\u673a\u5668\u4eba\uff0c\u5e76\u5c55\u793a\u4e86\u4f18\u5316\u7b97\u6cd5\u7684\u9ad8\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2511.19204", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.19204", "abs": "https://arxiv.org/abs/2511.19204", "authors": ["Fabian Schramm", "Pierre Fabre", "Nicolas Perrin-Gilbert", "Justin Carpentier"], "title": "Reference-Free Sampling-Based Model Predictive Control", "comment": null, "summary": "We present a sampling-based model predictive control (MPC) framework that enables emergent locomotion without relying on handcrafted gait patterns or predefined contact sequences. Our method discovers diverse motion patterns, ranging from trotting to galloping, robust standing policies, jumping, and handstand balancing, purely through the optimization of high-level objectives. Building on model predictive path integral (MPPI), we propose a dual-space spline parameterization that operates on position and velocity control points. Our approach enables contact-making and contact-breaking strategies that adapt automatically to task requirements, requiring only a limited number of sampled trajectories. This sample efficiency allows us to achieve real-time control on standard CPU hardware, eliminating the need for GPU acceleration typically required by other state-of-the-art MPPI methods. We validate our approach on the Go2 quadrupedal robot, demonstrating various emergent gaits and basic jumping capabilities. In simulation, we further showcase more complex behaviors, such as backflips, dynamic handstand balancing and locomotion on a Humanoid, all without requiring reference tracking or offline pre-training.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u91c7\u6837\u57fa\u7840\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u53ef\u4ee5\u81ea\u53d1\u751f\u6210\u591a\u6837\u7684\u8fd0\u52a8\u6a21\u5f0f\uff0c\u800c\u4e0d\u9700\u624b\u52a8\u8bbe\u8ba1\uff0c\u9a8c\u8bc1\u4e86\u5728\u591a\u79cd\u673a\u5668\u4eba\u4e0a\u7684\u6709\u6548\u6027\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5728\u6ca1\u6709\u624b\u5de5\u8bbe\u8ba1\u7684\u6b65\u6001\u6a21\u5f0f\u6216\u63a5\u89e6\u5e8f\u5217\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u81ea\u4e3b\u7684\u590d\u6742\u8fd0\u52a8\u63a7\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91c7\u6837\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u53cc\u7a7a\u95f4\u6837\u6761\u53c2\u6570\u5316\uff0c\u4f18\u5316\u9ad8\u5c42\u76ee\u6807\u4ee5\u53d1\u73b0\u591a\u6837\u5316\u7684\u8fd0\u52a8\u6a21\u5f0f\u3002", "result": "\u5b9e\u73b0\u4e86\u591a\u79cd\u8fd0\u52a8\u6a21\u5f0f\uff0c\u5305\u62ec\u8df3\u8dc3\u548c\u52a8\u6001\u5e73\u8861\uff0c\u4e14\u5728\u6807\u51c6CPU\u4e0a\u5b9e\u73b0\u4e86\u5b9e\u65f6\u63a7\u5236\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u65e0\u9700\u9884\u5b9a\u4e49\u6b65\u6001\u6a21\u5f0f\u6216\u63a5\u89e6\u5e8f\u5217\u7684\u81ea\u53d1\u8fd0\u52a8\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u56db\u8db3\u673a\u5668\u4eba\u548c\u7c7b\u4eba\u673a\u5668\u4eba\u4e0a\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.19211", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19211", "abs": "https://arxiv.org/abs/2511.19211", "authors": ["Prabhat Kumar", "Chandra Prakash", "Josh Pinskier", "David Howard", "Matthijs Langelaar"], "title": "Soft pneumatic grippers: Topology optimization, 3D-printing and experimental validation", "comment": "9 Figures", "summary": "This paper presents a systematic topology optimization framework for designing a soft pneumatic gripper (SPG), explicitly considering the design-dependent nature of the actuating load. The load is modeled using Darcy's law with an added drainage term. A 2D soft arm unit is optimized by formulating it as a compliant mechanism design problem using the robust formulation. The problem is posed as a min-max optimization, where the output deformations of blueprint and eroded designs are considered. A volume constraint is imposed on the blueprint part, while a strain-energy constraint is enforced on the eroded part. The MMA is employed to solve the optimization problem and obtain the optimized soft unit. Finite element analysis with the Ogden material model confirms that the optimized 2D unit outperforms a conventional rectangular design under pneumatic loading. The optimized 2D unit is extruded to obtain a 3D module, and ten such units are assembled to create a soft arm. Deformation profiles of the optimized arm are analysed under different pressure loads. Four arms are 3D-printed and integrated with a supporting structure to realize the proposed SPG. The gripping performance of the SPG is demonstrated on objects with different weights, sizes, stiffness, and shapes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u7684\u62d3\u6251\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u8bbe\u8ba1\u8f6f\u6c14\u52a8\u5939\u624b\u5668\uff0c\u4f18\u5316\u6d41\u7a0b\u786e\u4fdd\u4e86\u8bbe\u8ba1\u6027\u80fd\u7684\u63d0\u5347\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u4e2a\u8f6f\u6c14\u52a8\u5939\u624b\u5668\uff0c\u8003\u8651\u9a71\u52a8\u8d1f\u8377\u7684\u8bbe\u8ba1\u4f9d\u8d56\u6027", "method": "\u7cfb\u7edf\u62d3\u6251\u4f18\u5316\u6846\u67b6", "result": "\u4f18\u5316\u7684\u4e8c\u7ef4\u5355\u5143\u5728\u6c14\u52a8\u52a0\u8f7d\u4e0b\u4f18\u4e8e\u4f20\u7edf\u77e9\u5f62\u8bbe\u8ba1\uff0c\u6700\u7ec8\u6784\u5efa\u51fa\u8f6f\u81c2\u5e76\u5c55\u793a\u4e86\u6293\u63e1\u6027\u80fd", "conclusion": "\u5b9e\u73b0\u4e86\u4e0d\u540c\u8d1f\u8377\u6761\u4ef6\u4e0b\u7684\u5939\u6301\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u4f18\u5316\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u548c\u53ef\u884c\u6027\u3002"}}
{"id": "2511.19236", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19236", "abs": "https://arxiv.org/abs/2511.19236", "authors": ["Yuxuan Wang", "Haobin Jiang", "Shiqing Yao", "Ziluo Ding", "Zongqing Lu"], "title": "SENTINEL: A Fully End-to-End Language-Action Model for Humanoid Whole Body Control", "comment": "23 pages, 8 figures, 11 tables", "summary": "Existing humanoid control systems often rely on teleoperation or modular generation pipelines that separate language understanding from physical execution. However, the former is entirely human-driven, and the latter lacks tight alignment between language commands and physical behaviors. In this paper, we present SENTINEL, a fully end-to-end language-action model for humanoid whole-body control. We construct a large-scale dataset by tracking human motions in simulation using a pretrained whole body controller, combined with their text annotations. The model directly maps language commands and proprioceptive inputs to low-level actions without any intermediate representation. The model generates action chunks using flow matching, which can be subsequently refined by a residual action head for real-world deployment. Our method exhibits strong semantic understanding and stable execution on humanoid robots in both simulation and real-world deployment, and also supports multi-modal extensions by converting inputs into texts.", "AI": {"tldr": "\u63d0\u51faSENTINEL\u6a21\u578b\uff0c\u91c7\u7528\u5168\u7aef\u5230\u7aef\u7684\u65b9\u6cd5\u6539\u5584\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\uff0c\u63d0\u5347\u8bed\u8a00\u4e0e\u52a8\u4f5c\u4e4b\u95f4\u7684\u76f4\u63a5\u6620\u5c04\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u5f62\u63a7\u5236\u7cfb\u7edf\u901a\u5e38\u4f9d\u8d56\u4e8e\u9065\u64cd\u4f5c\u6216\u6a21\u5757\u5316\u751f\u6210\u7ba1\u9053\uff0c\u8fd9\u4e24\u8005\u4e4b\u95f4\u7f3a\u4e4f\u8bed\u8a00\u4e0e\u7269\u7406\u884c\u4e3a\u7684\u7d27\u5bc6\u5bf9\u9f50\u3002", "method": "\u63d0\u51faSENTINEL\uff0c\u4e00\u4e2a\u5168\u7aef\u5230\u7aef\u7684\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7528\u4e8e\u4eba\u5f62\u5168\u8eab\u63a7\u5236\u3002", "result": "\u6a21\u578b\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u4e16\u754c\u7684\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u8bed\u4e49\u7406\u89e3\u548c\u7a33\u5b9a\u6267\u884c\u80fd\u529b\uff0c\u5e76\u652f\u6301\u591a\u6a21\u6001\u6269\u5c55\u3002", "conclusion": "\u901a\u8fc7\u8ddf\u8e2a\u4eba\u7c7b\u52a8\u4f5c\u5e76\u7ed3\u5408\u6587\u672c\u6ce8\u91ca\u6784\u5efa\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u6a21\u578b\u80fd\u591f\u76f4\u63a5\u5c06\u8bed\u8a00\u547d\u4ee4\u548c\u8f93\u5165\u6620\u5c04\u5230\u4f4e\u7ea7\u52a8\u4f5c\u3002"}}
{"id": "2511.19315", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19315", "abs": "https://arxiv.org/abs/2511.19315", "authors": ["Weiliang Tang", "Jialin Gao", "Jia-Hui Pan", "Gang Wang", "Li Erran Li", "Yunhui Liu", "Mingyu Ding", "Pheng-Ann Heng", "Chi-Wing Fu"], "title": "Rethinking Intermediate Representation for VLM-based Robot Manipulation", "comment": null, "summary": "Vision-Language Model (VLM) is an important component to enable robust robot manipulation. Yet, using it to translate human instructions into an action-resolvable intermediate representation often needs a tradeoff between VLM-comprehensibility and generalizability. Inspired by context-free grammar, we design the Semantic Assembly representation named SEAM, by decomposing the intermediate representation into vocabulary and grammar. Doing so leads us to a concise vocabulary of semantically-rich operations and a VLM-friendly grammar for handling diverse unseen tasks. In addition, we design a new open-vocabulary segmentation paradigm with a retrieval-augmented few-shot learning strategy to localize fine-grained object parts for manipulation, effectively with the shortest inference time over all state-of-the-art parallel works. Also, we formulate new metrics for action-generalizability and VLM-comprehensibility, demonstrating the compelling performance of SEAM over mainstream representations on both aspects. Extensive real-world experiments further manifest its SOTA performance under varying settings and tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bed\u4e49\u7ec4\u88c5\u8868\u793a\uff08SEAM\uff09\u6765\u63d0\u9ad8\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u7406\u89e3\u6027\u4e0e\u666e\u9002\u6027\uff0c\u5e76\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u8bbe\u8ba1\u5f3a\u5927\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4ee5\u5b9e\u73b0\u6709\u6548\u7684\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u89e3\u51b3\u5728VLM\u53ef\u7406\u89e3\u6027\u4e0e\u666e\u9002\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5256\u6790\u4e2d\u95f4\u8868\u793a\uff0c\u5c06\u5176\u5206\u89e3\u4e3a\u8bcd\u6c47\u548c\u8bed\u6cd5\uff0c\u7ed3\u5408\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u8303\u5f0f\u4e0e\u589e\u5f3a\u68c0\u7d22\u7684\u5c11\u6837\u672c\u5b66\u4e60\u7b56\u7565\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u7269\u4f53\u90e8\u4ef6\u5b9a\u4f4d\u3002", "result": "\u8bbe\u8ba1\u4e86\u8bed\u4e49\u7ec4\u88c5\u8868\u793a\uff08SEAM\uff09\uff0c\u5b9e\u73b0\u4e86\u8bed\u4e49\u4e30\u5bcc\u7684\u64cd\u4f5c\u8bcd\u6c47\u4e0e\u9002\u5408VLM\u7684\u8bed\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u5404\u79cd\u672a\u89c1\u4efb\u52a1\u3002", "conclusion": "SEAM\u5728\u53ef\u64cd\u4f5c\u6027\u4e0eVLM\u53ef\u7406\u89e3\u6027\u4e24\u65b9\u9762\u76f8\u6bd4\u4e3b\u6d41\u8868\u793a\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2511.19377", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19377", "abs": "https://arxiv.org/abs/2511.19377", "authors": ["Mamoon Aamir", "Mariyam Sattar", "Naveed Ur Rehman Junejo", "Aqsa Zafar Abbasi"], "title": "Deployment Dynamics and Optimization of Novel Space Antenna Deployable Mechanism", "comment": null, "summary": "Given the increasing need for large aperture antennas in space missions, the difficulty of fitting such structures into small launch vehicles has prompted the design of deployable antenna systems. The thesis introduces a new Triple Scissors Deployable Truss Mechanism (TSDTM) for space antenna missions. The new mechanism is to be stowed during launch and efficiently deploy in orbit, offering maximum aperture size while taking up minimal launch volume. The thesis covers the entire design process from geometric modeling, kinematic analysis with screw theory and Newtonian approaches, dynamic analysis by eigenvalue and simulation methods, and verification with SolidWorks. In addition, optimization routines were coded based on Support Vector Machines for material choice in LEO environments and machine learning method for geometric setup. The TSDTM presented has enhanced structural dynamics with good comparison between simulation and analytical predictions. The structure optimized proved highly accurate, with a deviation of just 1.94% between machine learning-predicted and simulated natural frequencies, demonstrating the potential of incorporating AI-based methods in space structural design.", "AI": {"tldr": "\u672c\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u5c55\u5f00\u4e09\u5200\u526a\u6846\u67b6\u673a\u5236\uff0c\u9002\u7528\u4e8e\u592a\u7a7a\u4efb\u52a1\u4e2d\u7684\u5927\u53e3\u5f84\u5929\u7ebf\uff0c\u4f18\u5316\u4e86\u53d1\u5c04\u4f53\u79ef\u5e76\u63d0\u9ad8\u4e86\u7ed3\u6784\u52a8\u6001\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u7a7a\u95f4\u4efb\u52a1\u5bf9\u5927\u53e3\u5f84\u5929\u7ebf\u9700\u6c42\u7684\u589e\u957f\uff0c\u4f20\u7edf\u7ed3\u6784\u5728\u53d1\u5c04\u5c3a\u5bf8\u4e0a\u7684\u9650\u5236\u63a8\u52a8\u4e86\u53ef\u5c55\u5f00\u5929\u7ebf\u7cfb\u7edf\u7684\u8bbe\u8ba1\u3002", "method": "\u5305\u542b\u51e0\u4f55\u5efa\u6a21\u3001\u8fd0\u52a8\u5b66\u5206\u6790\u3001\u52a8\u6001\u5206\u6790\u53ca\u9a8c\u8bc1\uff0c\u7ed3\u5408\u652f\u6301\u5411\u91cf\u673a\u4f18\u5316\u6750\u6599\u9009\u62e9\u548c\u673a\u5668\u5b66\u4e60\u4f18\u5316\u51e0\u4f55\u7ed3\u6784\u3002", "result": "TSDTM\u5728\u7ed3\u6784\u52a8\u6001\u5b66\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\uff0c\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u7684\u81ea\u7136\u9891\u7387\u4e0e\u6a21\u62df\u7ed3\u679c\u7684\u504f\u5dee\u4ec5\u4e3a1.94%\u3002", "conclusion": "\u5f15\u5165AI\u65b9\u6cd5\u53ef\u663e\u8457\u63d0\u5347\u7a7a\u95f4\u7ed3\u6784\u8bbe\u8ba1\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2511.19433", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19433", "abs": "https://arxiv.org/abs/2511.19433", "authors": ["Dong Jing", "Gang Wang", "Jiaqi Liu", "Weiliang Tang", "Zelong Sun", "Yunchao Yao", "Zhenyu Wei", "Yunhui Liu", "Zhiwu Lu", "Mingyu Ding"], "title": "Mixture of Horizons in Action Chunking", "comment": "15 pages, 14 figures", "summary": "Vision-language-action (VLA) models have shown remarkable capabilities in robotic manipulation, but their performance is sensitive to the $\\textbf{action chunk length}$ used during training, termed $\\textbf{horizon}$. Our empirical study reveals an inherent trade-off: longer horizons provide stronger global foresight but degrade fine-grained accuracy, while shorter ones sharpen local control yet struggle on long-term tasks, implying fixed choice of single horizons being suboptimal. To mitigate the trade-off, we propose a $\\textbf{mixture of horizons (MoH)}$ strategy. MoH rearranges the action chunk into several segments with different horizons, processes them in parallel with a shared action transformer, and fuses outputs with a light linear gate. It has three appealing benefits. 1) MoH exploits long-term foresight and short-term precision jointly within a single model, improving both performance and generalizability to complex tasks. 2) MoH is plug-and-play for full-attention action modules with minimal training or inference overhead. 3) MoH enables dynamic inference with adaptive horizons, which selects stable actions through cross-horizon consensus, achieving 2.5$\\times$ higher throughput than baselines while preserving superior performance. Extensive experiments over flow-based policies $\u03c0_0$, $\u03c0_{0.5}$, and one-step regression policy $\u03c0_{\\text{reg}}$ demonstrate that MoH yields consistent and significant gains on both simulations and real-world tasks. Notably, under mixed-task setting, $\u03c0_{0.5}$ with MoH reaches a new state-of-the-art with 99$\\%$ average success rate on LIBERO after only $30k$ training iterations. Project page: https://github.com/Timsty1/MixtureOfHorizons", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u89c6\u89d2\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u64cd\u63a7\u4e2d\u52a8\u4f5c\u5206\u5757\u957f\u5ea6\u5bf9\u6027\u80fd\u5f71\u54cd\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6210\u529f\u7387\u548c\u5904\u7406\u6548\u7387\u3002", "motivation": "\u7814\u7a76\u8868\u660e\uff0c\u8bad\u7ec3\u4e2d\u4f7f\u7528\u7684\u52a8\u4f5c\u5206\u5757\u957f\u5ea6\uff08\u89c6\u91ce\uff09\u5bf9VLA\u6a21\u578b\u7684\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5b58\u5728\u4e00\u4e2a\u56fa\u6709\u7684\u6743\u8861\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u89c6\u89d2\u7b56\u7565\uff08MoH\uff09\uff0c\u5c06\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u7684\u52a8\u4f5c\u6bb5\u91cd\u7ec4\u5e76\u5e76\u884c\u5904\u7406\u3002", "result": "MoH\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6027\u80fd\u548c\u590d\u6742\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u5728\u6df7\u5408\u4efb\u52a1\u6761\u4ef6\u4e0b\u8fbe\u5230\u4e8699%\u7684\u5e73\u5747\u6210\u529f\u7387\u3002", "conclusion": "MoH\u7b56\u7565\u5728\u6df7\u5408\u4efb\u52a1\u8bbe\u7f6e\u4e2d\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u9ad8\u72b6\u6001\uff0c\u5c55\u793a\u4e86\u5728\u673a\u5668\u4eba\u64cd\u63a7\u65b9\u9762\u7684\u663e\u8457\u63d0\u5347\u548c\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
