{"id": "2512.23859", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.23859", "abs": "https://arxiv.org/abs/2512.23859", "authors": ["Leah Hope Ajmani", "Arka Ghosh", "Benjamin Kaveladze", "Eugenia Kim", "Keertana Namuduri", "Theresa Nguyen", "Ebele Okoli", "Jessica Schleider", "Denae Ford", "Jina Suh"], "title": "Seeking Late Night Life Lines: Experiences of Conversational AI Use in Mental Health Crisis", "comment": null, "summary": "Online, people often recount their experiences turning to conversational AI agents (e.g., ChatGPT, Claude, Copilot) for mental health support -- going so far as to replace their therapists. These anecdotes suggest that AI agents have great potential to offer accessible mental health support. However, it's unclear how to meet this potential in extreme mental health crisis use cases. In this work, we explore the first-person experience of turning to a conversational AI agent in a mental health crisis. From a testimonial survey (n = 53) of lived experiences, we find that people use AI agents to fill the in-between spaces of human support; they turn to AI due to lack of access to mental health professionals or fears of burdening others. At the same time, our interviews with mental health experts (n = 16) suggest that human-human connection is an essential positive action when managing a mental health crisis. Using the stages of change model, our results suggest that a responsible AI crisis intervention is one that increases the user's preparedness to take a positive action while de-escalating any intended negative action. We discuss the implications of designing conversational AI agents as bridges towards human-human connection rather than ends in themselves.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4eba\u4eec\u5728\u5fc3\u7406\u5065\u5eb7\u5371\u673a\u4e2d\u5982\u4f55\u4f7f\u7528AI\u4ee3\u7406\uff0c\u5f3a\u8c03\u4e86\u4eba\u9645\u652f\u6301\u7684\u91cd\u8981\u6027\uff0c\u5efa\u8bae\u8bbe\u8ba1AI\u65f6\u8981\u4fc3\u8fdb\u4eba\u9645\u8fde\u63a5\u800c\u975e\u66ff\u4ee3\u3002", "motivation": "\u63a2\u7d22AI\u4ee3\u7406\u5728\u6781\u7aef\u5fc3\u7406\u5065\u5eb7\u5371\u673a\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u5bfb\u627e\u4fc3\u8fdb\u4eba\u9645\u652f\u6301\u7684\u8bbe\u8ba1\u65b9\u5411\u3002", "method": "\u901a\u8fc7\u7b2c\u4e00\u4eba\u79f0\u7ecf\u9a8c\u7684\u8c03\u67e5\u548c\u4e13\u5bb6\u8bbf\u8c08", "result": "\u8c03\u67e5\u53d1\u73b0\u4eba\u4eec\u5728\u5fc3\u7406\u5065\u5eb7\u5371\u673a\u4e2d\u4f7f\u7528AI\u4ee3\u7406\u8fdb\u884c\u652f\u6301\uff0c\u4e3b\u8981\u662f\u56e0\u4e3a\u7f3a\u4e4f\u5fc3\u7406\u5065\u5eb7\u4e13\u4e1a\u4eba\u58eb\u6216\u62c5\u5fe7\u7ed9\u4ed6\u4eba\u5e26\u6765\u8d1f\u62c5\uff1b\u4e13\u5bb6\u610f\u89c1\u6307\u51fa\u4eba\u9645\u8fde\u63a5\u5728\u5fc3\u7406\u5065\u5eb7\u5371\u673a\u7ba1\u7406\u4e2d\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8bbe\u8ba1\u4f1a\u8bddAI\u4ee3\u7406\u5e94\u4f5c\u4e3a\u4eba\u9645\u8fde\u63a5\u7684\u6865\u6881\uff0c\u800c\u975e\u5355\u72ec\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u63d0\u5347\u7528\u6237\u91c7\u53d6\u79ef\u6781\u884c\u52a8\u7684\u51c6\u5907\u7a0b\u5ea6\u3002"}}
{"id": "2512.23907", "categories": ["cs.HC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.23907", "abs": "https://arxiv.org/abs/2512.23907", "authors": ["Paul Englefield", "Russell Beale"], "title": "Deletion Considered Harmful", "comment": null, "summary": "In a world of information overload, understanding how we can most effectively manage information is crucial to success. We set out to understand how people view deletion, the removal of material no longer needed: does it help by reducing clutter and improving the signal to noise ratio, or does the effort required to decide to delete something make it not worthwhile? How does deletion relate to other strategies like filing; do people who spend extensive time in filing also prune their materials too? We studied the behaviour of 51 knowledge workers though a series of questionnaires and interviews to evaluate a range of tactics they used aimed at organizing, filing, and retrieving digital resources. Our study reveals that deletion is consistently under-adopted compared to other tactics such as Filing, Coverage, Ontology, and Timeliness. Moreover, the empirical data indicate that deletion is actually detrimental to retrieval success and satisfaction. In this paper, we examine the practice of deletion, review the related literature, and present detailed statistical results and clustering outcomes that underscore its adverse effects.", "AI": {"tldr": "\u672c\u7814\u7a76\u8c03\u67e5\u4e86\u5220\u9664\u5728\u4fe1\u606f\u7ba1\u7406\u4e2d\u7684\u6548\u679c\uff0c\u53d1\u73b0\u5176\u5b9e\u9645\u4f7f\u7528\u7387\u4f4e\u4e14\u5bf9\u68c0\u7d22\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u91cd\u65b0\u8bc4\u4f30\u5220\u9664\u7b56\u7565\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u5728\u4fe1\u606f\u6cdb\u6ee5\u7684\u4e16\u754c\u4e2d\uff0c\u7406\u89e3\u5982\u4f55\u6709\u6548\u7ba1\u7406\u4fe1\u606f\u5bf9\u6210\u529f\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u95ee\u5377\u548c\u8bbf\u8c08\u5bf951\u540d\u77e5\u8bc6\u5de5\u4f5c\u8005\u7684\u884c\u4e3a\u8fdb\u884c\u7814\u7a76\uff0c\u8bc4\u4f30\u4ed6\u4eec\u5728\u7ec4\u7ec7\u3001\u5f52\u6863\u548c\u68c0\u7d22\u6570\u5b57\u8d44\u6e90\u65f6\u4f7f\u7528\u7684\u7b56\u7565\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u76f8\u6bd4\u4e8e\u5176\u4ed6\u7b56\u7565\u5982\u5f52\u6863\u3001\u8986\u76d6\u3001Ontology\u548c\u65f6\u6548\u6027\uff0c\u5220\u9664\u7b56\u7565\u88ab\u4f4e\u4f30\uff0c\u5e76\u4e14\u5176\u5bf9\u68c0\u7d22\u6210\u529f\u4e0e\u6ee1\u610f\u5ea6\u6709\u8d1f\u9762\u5f71\u54cd\u3002", "conclusion": "\u5220\u9664\u5728\u4fe1\u606f\u7ba1\u7406\u4e2d\u88ab\u4f4e\u4f30\uff0c\u4e14\u5bf9\u68c0\u7d22\u6210\u529f\u548c\u6ee1\u610f\u5ea6\u6709\u8d1f\u9762\u5f71\u54cd\u3002"}}
{"id": "2512.24166", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.24166", "abs": "https://arxiv.org/abs/2512.24166", "authors": ["Boya Sun", "Haotian Shi", "Ying Ni", "Shaocheng Jia", "Haoyang Liang"], "title": "External Human-Machine Interface based on Intent Recognition: Framework Design and Experimental Validation", "comment": null, "summary": "Increasing autonomous vehicles (AVs) in transportation systems makes effective interactions between AVs and pedestrians indispensable. External human--machine interface (eHMI), which employs visual or auditory cues to explicitly convey vehicle behaviors can compensate for the loss of human-like interactions and enhance AV--pedestrian cooperation. To facilitate faster intent convergence between pedestrian and AVs, this study incorporates an adaptive interaction mechanism into eHMI based on pedestrian intent recognition, namely IR-eHMI. IR-eHMI dynamically detects and infers the behavioral intentions of both pedestrians and AVs through identifying their cooperation states. The proposed interaction framework is implemented and evaluated on a virtual reality (VR) experimental platform to demonstrate its effectiveness through statistical analysis. Experimental results show that IR-eHMI significantly improves crossing efficiency, reduces gaze distraction while maintaining interaction safety compared to traditional fixed-distance eHMI. This adaptive and explicit interaction mode introduces an innovative procedural paradigm for AV--pedestrian cooperation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u884c\u4eba\u610f\u56fe\u8bc6\u522b\u7684\u81ea\u9002\u5e94\u5916\u90e8\u4eba\u673a\u754c\u9762IR-eHMI\uff0c\u663e\u8457\u6539\u5584\u4e86\u81ea\u4e3b\u8f66\u8f86\u4e0e\u884c\u4eba\u4e4b\u95f4\u7684\u4e92\u52a8\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u968f\u7740\u81ea\u4e3b\u8f66\u8f86\uff08AVs\uff09\u5728\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u589e\u52a0\uff0c\u52a0\u5f3aAV\u4e0e\u884c\u4eba\u4e4b\u95f4\u7684\u6709\u6548\u4e92\u52a8\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u5c06\u81ea\u9002\u5e94\u4e92\u52a8\u673a\u5236\u7eb3\u5165eHMI\uff08\u5916\u90e8\u4eba\u673a\u754c\u9762\uff09\uff0c\u6839\u636e\u884c\u4eba\u610f\u56fe\u8bc6\u522b\u6765\u5b9e\u73b0\u79f0\u4e3aIR-eHMI\u7684\u7cfb\u7edf\u3002IR-eHMI\u52a8\u6001\u68c0\u6d4b\u5e76\u63a8\u65ad\u884c\u4eba\u548cAV\u7684\u884c\u4e3a\u610f\u56fe\uff0c\u901a\u8fc7\u8bc6\u522b\u5b83\u4eec\u7684\u5408\u4f5c\u72b6\u6001\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u7684\u56fa\u5b9a\u8ddd\u79bbeHMI\u76f8\u6bd4\uff0cIR-eHMI\u663e\u8457\u63d0\u9ad8\u4e86\u8fc7\u9a6c\u8def\u7684\u6548\u7387\uff0c\u51cf\u5c11\u4e86\u76ee\u5149\u5206\u6563\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e92\u52a8\u5b89\u5168\u3002", "conclusion": "\u8fd9\u79cd\u81ea\u9002\u5e94\u548c\u660e\u786e\u7684\u4e92\u52a8\u6a21\u5f0f\u4e3aAV\u4e0e\u884c\u4eba\u4e4b\u95f4\u7684\u5408\u4f5c\u5f15\u5165\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u7a0b\u5e8f\u8303\u5f0f\u3002"}}
{"id": "2512.24237", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.24237", "abs": "https://arxiv.org/abs/2512.24237", "authors": ["Guillaume Riviere"], "title": "A Framing and Analysis of Applicative Tangible Interfaces", "comment": "16 pages. Approximately 6300 words. 5 figures and 8 tables", "summary": "The investigation of tangible user interfaces commenced approximately thirty years ago. Questions on its commercial potential become more pressing as the field becomes mature. To take the field one step further -- as the emergence of components contributed to the commercial development of graphical user interfaces -- this article suggests that applicative tangible user interfaces could also be split into components. These components are composed of the aggregation, combination, or coupling of physical items and fulfil four roles that are described through a new interaction model. This article successfully distributed among these four components' roles all of the 159 physical items from a representative collection of 35 applications. Further examination of these applicative tangible interfaces coincides with four research phases in the field and identifies three main paths for future research to fully realize the potential of tangible user interfaces.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u6709\u5f62\u7528\u6237\u754c\u9762\u7684\u7ec4\u4ef6\u5316\uff0c\u63d0\u51fa\u5546\u4e1a\u6f5c\u529b\u5e76\u5b9a\u4e49\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u6709\u5f62\u7528\u6237\u754c\u9762\u7684\u9010\u6b65\u6210\u719f\uff0c\u63a2\u7d22\u5176\u5546\u4e1a\u6f5c\u529b\u53d8\u5f97\u66f4\u52a0\u91cd\u8981\u3002", "method": "\u7814\u7a76\u5c06159\u4e2a\u7269\u7406\u9879\u5206\u914d\u5230\u56db\u4e2a\u89d2\u8272\uff0c\u5e76\u4e0e\u6709\u5f62\u7528\u6237\u754c\u9762\u9886\u57df\u7684\u7814\u7a76\u9636\u6bb5\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u6587\u4e2d\u6210\u529f\u5730\u5c0635\u4e2a\u5e94\u7528\u4e2d\u7684159\u4e2a\u7269\u7406\u9879\u5206\u914d\u5230\u65b0\u7684\u4e92\u52a8\u6a21\u578b\u4e2d\u5b9a\u4e49\u7684\u56db\u4e2a\u89d2\u8272\uff0c\u5e76\u6307\u51fa\u672a\u6765\u7814\u7a76\u7684\u4e09\u6761\u4e3b\u8981\u8def\u5f84\u3002", "conclusion": "\u672c\u6587\u8bc6\u522b\u51fa\u9002\u7528\u7684\u6709\u5f62\u7528\u6237\u754c\u9762\u7684\u56db\u4e2a\u7ec4\u4ef6\u89d2\u8272\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u4e3b\u8981\u65b9\u5411\u3002"}}
{"id": "2512.23856", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.23856", "abs": "https://arxiv.org/abs/2512.23856", "authors": ["Mark Van der Merwe", "Kei Ota", "Dmitry Berenson", "Nima Fazeli", "Devesh K. Jha"], "title": "Simultaneous Extrinsic Contact and In-Hand Pose Estimation via Distributed Tactile Sensing", "comment": "8 pages. IEEE Robotics and Automation Letters, 2026", "summary": "Prehensile autonomous manipulation, such as peg insertion, tool use, or assembly, require precise in-hand understanding of the object pose and the extrinsic contacts made during interactions. Providing accurate estimation of pose and contacts is challenging. Tactile sensors can provide local geometry at the sensor and force information about the grasp, but the locality of sensing means resolving poses and contacts from tactile alone is often an ill-posed problem, as multiple configurations can be consistent with the observations. Adding visual feedback can help resolve ambiguities, but can suffer from noise and occlusions. In this work, we propose a method that pairs local observations from sensing with the physical constraints of contact. We propose a set of factors that ensure local consistency with tactile observations as well as enforcing physical plausibility, namely, that the estimated pose and contacts must respect the kinematic and force constraints of quasi-static rigid body interactions. We formalize our problem as a factor graph, allowing for efficient estimation. In our experiments, we demonstrate that our method outperforms existing geometric and contact-informed estimation pipelines, especially when only tactile information is available. Video results can be found at https://tacgraph.github.io/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u89e6\u89c9\u4f20\u611f\u548c\u7269\u7406\u7ea6\u675f\u7684\u5c40\u90e8\u89c2\u5bdf\u65b9\u6cd5\uff0c\u4f18\u5316\u4e86\u7269\u4f53\u59ff\u6001\u4e0e\u63a5\u89e6\u60c5\u51b5\u7684\u4f30\u8ba1\u3002", "motivation": "\u5728\u81ea\u9002\u5e94\u64cd\u63a7\u4e2d\uff0c\u9700\u8981\u7cbe\u786e\u7406\u89e3\u7269\u4f53\u7684\u59ff\u6001\u548c\u63a5\u89e6\uff0c\u800c\u4ec5\u4f9d\u8d56\u89e6\u89c9\u4f20\u611f\u53ef\u80fd\u5bfc\u81f4\u4e0d\u9002\u5b9a\u95ee\u9898\u3002", "method": "\u5c06\u5c40\u90e8\u89c2\u5bdf\u4e0e\u63a5\u89e6\u7684\u7269\u7406\u7ea6\u675f\u76f8\u7ed3\u5408", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u56e0\u5b50\u56fe\u7684\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u4f30\u8ba1\u7269\u4f53\u7684\u59ff\u6001\u548c\u63a5\u89e6\u60c5\u51b5", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4ec5\u4f7f\u7528\u89e6\u89c9\u4fe1\u606f\u65f6\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u51e0\u4f55\u548c\u63a5\u89e6\u4fe1\u606f\u4f30\u8ba1\u6d41\u7a0b"}}
{"id": "2512.24632", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.24632", "abs": "https://arxiv.org/abs/2512.24632", "authors": ["Md Nazmus Sakib", "Naga Manogna Rayasam", "Ishika Tarin", "Sanorita Dey"], "title": "ReflecToMeet: An AI-Assisted Reflection Based System to Enhance Collaborative Preparedness", "comment": null, "summary": "In collaborative settings, difficulties in sustaining a consistent pace and engagement often lead to task drift, reducing preparedness and overall effectiveness between meetings. To address this challenge, we conducted a formative study and developed ReflecToMeet, an AI assisted system that integrates theory driven reflective prompts with mechanisms for sharing teammates reflections. Informed by ten formative interviews, the system was evaluated in a mixed method study across three conditions: deeper reflection, regular reflection, and a control condition with unstructured reflection. Participants in the control condition demonstrated less deliberate thought and weaker collaboration, which led to stress and misalignment during team meetings. In contrast, structured reflection supported greater organization and steadier progress. The deeper reflection condition further facilitated confidence, teamwork, and idea generation, although it imposed a higher cognitive load. We conclude by discussing design implications for AI agents that facilitate reflection to enhance collaboration and broader considerations for AI assisted systems aimed at sustaining collaborative goals.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86ReflecToMeet\uff0c\u4e00\u4e2aAI\u8f85\u52a9\u7cfb\u7edf\uff0c\u65e8\u5728\u901a\u8fc7\u53cd\u601d\u63d0\u793a\u548c\u5206\u4eab\u673a\u5236\u89e3\u51b3\u534f\u4f5c\u73af\u5883\u4e2d\u4efb\u52a1\u504f\u79bb\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u534f\u4f5c\u73af\u5883\u4e2d\uff0c\u96be\u4ee5\u4fdd\u6301\u4e00\u81f4\u7684\u8282\u594f\u548c\u6295\u5165\u4f1a\u5bfc\u81f4\u4efb\u52a1\u504f\u79bb\uff0c\u964d\u4f4e\u4f1a\u8bae\u95f4\u7684\u51c6\u5907\u6027\u548c\u6574\u4f53\u6548\u80fd\u3002", "method": "\u8fdb\u884c\u4e86\u4e00\u4e2a\u5f62\u6210\u6027\u7814\u7a76\uff0c\u5e76\u5f00\u5c55\u4e86\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\uff0c\u5bf9\u4e09\u79cd\u6761\u4ef6\u4e0b\u7684\u7cfb\u7edf\u8fdb\u884c\u4e86\u8bc4\u4f30\uff1a\u6df1\u5165\u53cd\u601d\u3001\u5b9a\u671f\u53cd\u601d\u548c\u4e0d\u7ed3\u6784\u5316\u53cd\u601d\u7684\u5bf9\u7167\u7ec4\u3002", "result": "\u5bf9\u7167\u7ec4\u8868\u73b0\u51fa\u8f83\u5c11\u7684\u6df1\u601d\u719f\u8651\u548c\u8f83\u5f31\u7684\u534f\u4f5c\uff0c\u5bfc\u81f4\u4f1a\u8bae\u671f\u95f4\u7684\u538b\u529b\u548c\u4e0d\u5bf9\u9f50\uff0c\u800c\u7ed3\u6784\u5316\u53cd\u601d\u652f\u6301\u4e86\u66f4\u597d\u7684\u7ec4\u7ec7\u548c\u7a33\u5b9a\u7684\u8fdb\u5c55\u3002", "conclusion": "\u6211\u4eec\u8ba8\u8bba\u4e86AI\u4ee3\u7406\u4fc3\u8fdb\u53cd\u601d\u4ee5\u589e\u5f3a\u534f\u4f5c\u7684\u8bbe\u8ba1\u542f\u793a\uff0c\u4ee5\u53ca\u652f\u6301\u534f\u4f5c\u76ee\u6807\u7684AI\u8f85\u52a9\u7cfb\u7edf\u7684\u66f4\u5e7f\u6cdb\u8003\u8651\u3002"}}
{"id": "2512.23864", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.23864", "abs": "https://arxiv.org/abs/2512.23864", "authors": ["Guo Ye", "Zexi Zhang", "Xu Zhao", "Shang Wu", "Haoran Lu", "Shihan Lu", "Han Liu"], "title": "Learning to Feel the Future: DreamTacVLA for Contact-Rich Manipulation", "comment": null, "summary": "Vision-Language-Action (VLA) models have shown remarkable generalization by mapping web-scale knowledge to robotic control, yet they remain blind to physical contact. Consequently, they struggle with contact-rich manipulation tasks that require reasoning about force, texture, and slip. While some approaches incorporate low-dimensional tactile signals, they fail to capture the high-resolution dynamics essential for such interactions. To address this limitation, we introduce DreamTacVLA, a framework that grounds VLA models in contact physics by learning to feel the future. Our model adopts a hierarchical perception scheme in which high-resolution tactile images serve as micro-vision inputs coupled with wrist-camera local vision and third-person macro vision. To reconcile these multi-scale sensory streams, we first train a unified policy with a Hierarchical Spatial Alignment (HSA) loss that aligns tactile tokens with their spatial counterparts in the wrist and third-person views. To further deepen the model's understanding of fine-grained contact dynamics, we finetune the system with a tactile world model that predicts future tactile signals. To mitigate tactile data scarcity and the wear-prone nature of tactile sensors, we construct a hybrid large-scale dataset sourced from both high-fidelity digital twin and real-world experiments. By anticipating upcoming tactile states, DreamTacVLA acquires a rich model of contact physics and conditions its actions on both real observations and imagined consequences. Across contact-rich manipulation tasks, it outperforms state-of-the-art VLA baselines, achieving up to 95% success, highlighting the importance of understanding physical contact for robust, touch-aware robotic agents.", "AI": {"tldr": "DreamTacVLA\u901a\u8fc7\u6574\u5408\u89e6\u89c9\u4fe1\u53f7\u4e0e\u89c6\u89c9\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u63a5\u89e6\u4e30\u5bcc\u64cd\u4f5c\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u76ee\u524dVLA\u6a21\u578b\u5728\u7269\u7406\u63a5\u89e6\u65b9\u9762\u7684\u76f2\u70b9\uff0c\u63d0\u5347\u5176\u5728\u9700\u8981\u5224\u65ad\u529b\u3001\u7eb9\u7406\u548c\u6ed1\u79fb\u7684\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u5206\u5c42\u611f\u77e5\u65b9\u6848\uff0c\u4f7f\u7528\u9ad8\u5206\u8fa8\u7387\u89e6\u89c9\u56fe\u50cf\u3001\u624b\u8155\u6444\u50cf\u673a\u7684\u5c40\u90e8\u89c6\u89c9\u548c\u7b2c\u4e09\u4eba\u79f0\u5b8f\u89c2\u89c6\u89c9\u8fdb\u884c\u7edf\u4e00\u7b56\u7565\u8bad\u7ec3\uff0c\u5e76\u91c7\u7528\u89e6\u89c9\u4e16\u754c\u6a21\u578b\u7ec6\u5316\u7cfb\u7edf\u3002", "result": "DreamTacVLA\u901a\u8fc7\u9884\u6d4b\u672a\u6765\u89e6\u89c9\u4fe1\u53f7\uff0c\u83b7\u5f97\u4e30\u5bcc\u7684\u63a5\u89e6\u7269\u7406\u6a21\u578b\uff0c\u5e76\u5728\u64cd\u4f5c\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u663e\u8457\u7684\u6210\u529f\u7387\u63d0\u5347\u3002", "conclusion": "DreamTacVLA\u5728\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\u4e2d outperform \u73b0\u6709\u7684VLA\u57fa\u7ebf\u6a21\u578b\uff0c\u6210\u529f\u7387\u9ad8\u8fbe95%\u3002"}}
{"id": "2512.24939", "categories": ["cs.HC", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.24939", "abs": "https://arxiv.org/abs/2512.24939", "authors": ["Hongrui Jin"], "title": "Vibe Coding, Interface Flattening", "comment": "16 pages, 1 figure", "summary": "Large language models are reshaping programming by enabling 'vibe coding': the development of softwares through natural-language interaction with model-driven toolchains. This article argues that vibe coding is best understood as interface flattening, a reconfiguration in which previously distinct modalities (GUI, CLI, and API) appear to converge into a single conversational surface, even as the underlying chain of translation from intention to machinic effect lengthens and thickens. Drawing on Friedrich Kittler's materialist media theory and Alexander Galloway's account of interfaces as sites of protocol control, the paper situates programming as a historically localised interface arrangement rather than an essential relation to computation. Through a materialist reconstruction of the contemporary vibe-coding stack, it shows how remote compute infrastructures, latency and connectivity, structured outputs, function/tool calling, and interoperability standards such as the Model Context Protocol relocate control and meaning-making power to model and protocol providers. The apparent democratisation of technical capability therefore depends on new dependencies and new literacies. By foregrounding the tension between experiential flattening and infrastructural thickening, I demonstrate how LLM-mediated development redistributes symbolic labour/power, obscures responsibility, and privatises competencies previously dispersed across programming communities, contributing a critical lens on the political economy of AI-mediated human-computer interaction.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u91cd\u5851\u4e86\u7f16\u7a0b\uff0c\u4ea7\u751f\u4e86'vibe coding'\uff0c\u5373\u65b0\u578b\u7684\u8f6f\u4ef6\u5f00\u53d1\u6a21\u5f0f\uff0c\u5c3d\u7ba1\u6280\u672f\u80fd\u529b\u8868\u9762\u4e0a\u88ab\u6c11\u4e3b\u5316\uff0c\u4f46\u5b9e\u9645\u4e0a\u5374\u4f9d\u8d56\u4e8e\u6a21\u578b\u548c\u534f\u8bae\u63d0\u4f9b\u8005\uff0c\u5bfc\u81f4\u65b0\u5f62\u5f0f\u7684\u4f9d\u8d56\u4e0e\u7d20\u517b\u3002", "motivation": "\u63a2\u8ba8'vibe coding'\u4f5c\u4e3a\u754c\u9762\u6241\u5e73\u5316\u7684\u91cd\u6784\uff0c\u63ed\u793a\u5176\u5bf9\u7f16\u7a0b\u7684\u5f71\u54cd\u53ca\u5176\u80cc\u540e\u7684\u63a7\u5236\u4e0e\u610f\u4e49\u521b\u9020\u7684\u6743\u529b\u8f6c\u79fb\u3002", "method": "\u901a\u8fc7\u91cd\u65b0\u6784\u5efa\u5f53\u4ee3'vibe coding'\u5806\u6808\uff0c\u5206\u6790\u8fdc\u7a0b\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\u3001\u5ef6\u8fdf\u4e0e\u8fde\u63a5\u6027\u3001\u7ed3\u6784\u5316\u8f93\u51fa\u3001\u529f\u80fd/\u5de5\u5177\u8c03\u7528\u53ca\u4e92\u64cd\u4f5c\u6027\u6807\u51c6\u7684\u5f71\u54cd\u3002", "result": "\u8be5\u7814\u7a76\u6307\u51fa\uff0c'vibe coding'\u7684\u8868\u9762\u6c11\u4e3b\u5316\u4f9d\u8d56\u4e8e\u5bf9\u6a21\u578b\u548c\u534f\u8bae\u63d0\u4f9b\u8005\u7684\u65b0\u4f9d\u8d56\uff0c\u540c\u65f6\u4e5f\u63ed\u793a\u4e86\u6280\u672f\u80fd\u529b\u548c\u8d1f\u8d23\u6743\u7684\u52a8\u6001\u53d8\u5316\u3002", "conclusion": "LLM\u4ecb\u5bfc\u7684\u5f00\u53d1\u91cd\u65b0\u5206\u914d\u4e86\u7b26\u53f7\u52b3\u52a8\u548c\u6743\u529b\uff0c\u6a21\u7cca\u4e86\u8d23\u4efb\uff0c\u5e76\u4f7f\u5148\u524d\u5206\u6563\u5728\u7f16\u7a0b\u793e\u533a\u7684\u80fd\u529b\u79c1\u6709\u5316\uff0c\u4ece\u800c\u63d0\u4f9b\u4e86\u5bf9AI\u4ecb\u5bfc\u7684\u4eba\u673a\u4ea4\u4e92\u7684\u653f\u6cbb\u7ecf\u6d4e\u7684\u91cd\u8981\u89c6\u89d2\u3002"}}
{"id": "2512.23972", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.23972", "abs": "https://arxiv.org/abs/2512.23972", "authors": ["Liangtao Feng", "Zhenchang Liu", "Feng Zhang", "Xuefeng Ren"], "title": "SHIELD: Spherical-Projection Hybrid-Frontier Integration for Efficient LiDAR-based Drone Exploration", "comment": null, "summary": "This paper introduces SHIELD, a Spherical-Projection Hybrid-Frontier Integration for Efficient LiDAR-based Drone exploration method. Although laser LiDAR offers the advantage of a wide field of view, its application in UAV exploration still faces several challenges. The observation quality of LiDAR point clouds is generally inferior to that of depth cameras. Traditional frontier methods based on known and unknown regions impose a heavy computational burden, especially when handling the wide field of view of LiDAR. In addition, regions without point cloud are also difficult to classify as free space through raycasting. To address these problems, the SHIELD is proposed. It maintains an observation-quality occupancy map and performs ray-casting on this map to address the issue of inconsistent point-cloud quality during exploration. A hybrid frontier method is used to tackle both the computational burden and the limitations of point-cloud quality exploration. In addition, an outward spherical-projection ray-casting strategy is proposed to jointly ensure flight safety and exploration efficiency in open areas. Simulations and flight experiments prove the effectiveness of SHIELD. This work will be open-sourced to contribute to the research community.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faSHIELD\uff0c\u4e00\u79cd\u7528\u4e8eLiDAR\u65e0\u4eba\u673a\u63a2\u7d22\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u5584\u89c2\u5bdf\u8d28\u91cf\u548c\u964d\u4f4e\u8ba1\u7b97\u8d1f\u62c5\uff0c\u5df2\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3LiDAR\u5728\u65e0\u4eba\u673a\u63a2\u7d22\u4e2d\u7684\u89c2\u5bdf\u8d28\u91cf\u3001\u8ba1\u7b97\u8d1f\u62c5\u548c\u7a7a\u95f4\u5206\u7c7b\u7b49\u6311\u6218\u3002", "method": "\u4f7f\u7528\u6df7\u5408\u524d\u6cbf\u7b56\u7565\u548c\u7403\u9762\u6295\u5f71\u5c04\u7ebf\u6295\u5c04\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8LiDAR\u70b9\u4e91\u7684\u89c2\u5bdf\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u98de\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86SHIELD\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "SHIELD proves to be an effective method for LiDAR-based drone exploration, improving observation quality and computational efficiency."}}
{"id": "2512.24029", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.24029", "abs": "https://arxiv.org/abs/2512.24029", "authors": ["Takashi Yamamoto", "Hiroaki Yaguchi", "Shohei Kato", "Hiroyuki Okada"], "title": "Evaluation of Impression Difference of a Domestic Mobile Manipulator with Autonomous and/or Remote Control in Fetch-and-Carry Tasks", "comment": "Published in Advanced Robotics (2020). This paper defines an autonomous remote-control setting that makes the User-Robot-Operator triadic interaction explicit in physical tasks, and reports empirical differences in affinity and perceived security across autonomous, remote, and hybrid modes. Please cite: Advanced Robotics 34(20):1291-1308, 2020. DOI:10.1080/01691864.2020.1780152", "summary": "A single service robot can present two distinct agencies: its onboard autonomy and an operator-mediated agency, yet users experience them through one physical body. We formalize this dual-agency structure as a User-Robot-Operator triad in an autonomous remote-control setting that combines autonomous execution with remote human support. Prior to the recent surge of language-based and multimodal interfaces, we developed and evaluated an early-stage prototype in 2020 that combined natural-language text chat with freehand sketch annotations over the robot's live camera view to support remote intervention. We evaluated three modes - autonomous, remote, and hybrid - in controlled fetch-and-carry tasks using a domestic mobile manipulator (HSR) on a World Robot Summit 2020 rule-compliant test field. The results show systematic mode-dependent differences in user-rated affinity and additional insights on perceived security, indicating that switching or blending agency within one robot measurably shapes human impressions. These findings provide empirical guidance for designing human-in-the-loop mobile manipulation in domestic physical tasks.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u670d\u52a1\u673a\u5668\u4eba\u4e2d\u81ea\u4e3b\u4e0e\u64cd\u4f5c\u5458\u4ecb\u5165\u7684\u53cc\u91cd\u4ee3\u7406\uff0c\u5e76\u8bc4\u4f30\u4e0d\u540c\u4ea4\u4e92\u6a21\u5f0f\u5bf9\u7528\u6237\u4f53\u9a8c\u7684\u5f71\u54cd\uff0c\u7ed3\u679c\u8868\u660e\u6df7\u5408\u4ee3\u7406\u80fd\u663e\u8457\u5f71\u54cd\u4eba\u7c7b\u7684\u5370\u8c61\u3002", "motivation": "\u7814\u7a76\u670d\u52a1\u673a\u5668\u4eba\u5982\u4f55\u5728\u5176\u81ea\u4e3b\u6027\u4e0e\u64cd\u4f5c\u5458\u4ecb\u5165\u4e4b\u95f4\u5e73\u8861\uff0c\u63d0\u4f9b\u66f4\u597d\u7684\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u901a\u8fc7\u63a7\u5236\u7684\u53d6\u653e\u4efb\u52a1\u5728\u6a21\u62df\u6d4b\u8bd5\u573a\u8fdb\u884c\u8bc4\u4f30\uff0c\u6bd4\u8f83\u4e09\u79cd\u6a21\u5f0f\uff08\u81ea\u4e3b\u3001\u8fdc\u7a0b\u548c\u6df7\u5408\uff09\u7684\u8868\u73b0\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u7528\u6237\u5bf9\u4e0d\u540c\u6a21\u5f0f\u7684\u4eb2\u548c\u529b\u5b58\u5728\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u8fd9\u5f71\u54cd\u4e86\u4ed6\u4eec\u5bf9\u5b89\u5168\u611f\u7684\u611f\u77e5\u3002", "conclusion": "\u5207\u6362\u6216\u6df7\u5408\u673a\u5668\u4eba\u4e2d\u7684\u4ee3\u7406\u4f1a\u663e\u8457\u5f71\u54cd\u4eba\u7c7b\u7684\u5370\u8c61\uff0c\u4e3a\u5bb6\u5ead\u7269\u7406\u4efb\u52a1\u4e2d\u7684\u4eba\u673a\u534f\u4f5c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u6307\u5bfc\u3002"}}
{"id": "2512.24112", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24112", "abs": "https://arxiv.org/abs/2512.24112", "authors": ["Zonghan Li", "Tianwen Tao", "Rao Fu", "Liang Wang", "Dongyuan Zhang", "Quan Quan"], "title": "RflyUT-Sim: A Simulation Platform for Development and Testing of Complex Low-Altitude Traffic Control", "comment": null, "summary": "Significant challenges are posed by simulation and testing in the field of low-altitude unmanned aerial vehicle (UAV) traffic due to the high costs associated with large-scale UAV testing and the complexity of establishing low-altitude traffic test scenarios. Stringent safety requirements make high fidelity one of the key metrics for simulation platforms. Despite advancements in simulation platforms for low-altitude UAVs, there is still a shortage of platforms that feature rich traffic scenarios, high-precision UAV and scenario simulators, and comprehensive testing capabilities for low-altitude traffic. Therefore, this paper introduces an integrated high-fidelity simulation platform for low-altitude UAV traffic. This platform simulates all components of the UAV traffic network, including the control system, the traffic management system, the UAV system, the communication network , the anomaly and fault modules, etc. Furthermore, it integrates RflySim/AirSim and Unreal Engine 5 to develop full-state models of UAVs and 3D maps that model the real world using the oblique photogrammetry technique. Additionally, the platform offers a wide range of interfaces, and all models and scenarios can be customized with a high degree of flexibility. The platform's source code has been released, making it easier to conduct research related to low-altitude traffic.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u9ad8\u4fdd\u771f\u4f4e\u7a7a\u65e0\u4eba\u673a\u4ea4\u901a\u4eff\u771f\u5e73\u53f0\uff0c\u96c6\u6210\u591a\u79cd\u529f\u80fd\u5e76\u63d0\u4f9b\u7075\u6d3b\u5b9a\u5236\uff0c\u652f\u6301\u7814\u7a76\u3002", "motivation": "\u56e0\u4f4e\u7a7a\u65e0\u4eba\u673a\u4ea4\u901a\u4eff\u771f\u4e0e\u6d4b\u8bd5\u9762\u4e34\u9ad8\u6210\u672c\u548c\u590d\u6742\u573a\u666f\u5efa\u7acb\u7684\u6311\u6218\uff0c\u4e9f\u9700\u4e00\u4e2a\u9ad8\u4fdd\u771f\u4eff\u771f\u5e73\u53f0\u6765\u652f\u6301\u76f8\u5173\u7814\u7a76\u3002", "method": "\u4ecb\u7ecd\u4e00\u4e2a\u96c6\u6210\u7684\u9ad8\u4fdd\u771f\u4eff\u771f\u5e73\u53f0\uff0c\u6a21\u62df\u4f4e\u7a7a\u65e0\u4eba\u673a\u4ea4\u901a\u7684\u5404\u4e2a\u7ec4\u6210\u90e8\u5206\u3002", "result": "\u5e73\u53f0\u901a\u8fc7\u96c6\u6210RflySim/AirSim\u548cUnreal Engine 5\uff0c\u5f00\u53d1\u51fa\u5b8c\u6574\u72b6\u6001\u7684\u65e0\u4eba\u673a\u6a21\u578b\u548c\u4f7f\u7528\u503e\u659c\u6444\u5f71\u6d4b\u91cf\u6280\u672f\u5efa\u6a21\u76843D\u5730\u56fe\u3002", "conclusion": "\u8be5\u5e73\u53f0\u4e3a\u4f4e\u7a7a\u4ea4\u901a\u7814\u7a76\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u63a5\u53e3\u548c\u53ef\u5b9a\u5236\u7684\u6a21\u578b\u548c\u573a\u666f\uff0c\u5e76\u4e14\u6e90\u4ee3\u7801\u5df2\u53d1\u5e03\uff0c\u4fc3\u8fdb\u4e86\u76f8\u5173\u7814\u7a76\u7684\u5f00\u5c55\u3002"}}
{"id": "2512.24125", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24125", "abs": "https://arxiv.org/abs/2512.24125", "authors": ["Yi Liu", "Sukai Wang", "Dafeng Wei", "Xiaowei Cai", "Linqing Zhong", "Jiange Yang", "Guanghui Ren", "Jinyu Zhang", "Maoqing Yao", "Chuankang Li", "Xindong He", "Liliang Chen", "Jianlan Luo"], "title": "Unified Embodied VLM Reasoning with Robotic Action via Autoregressive Discretized Pre-training", "comment": null, "summary": "General-purpose robotic systems operating in open-world environments must achieve both broad generalization and high-precision action execution, a combination that remains challenging for existing Vision-Language-Action (VLA) models. While large Vision-Language Models (VLMs) improve semantic generalization, insufficient embodied reasoning leads to brittle behavior, and conversely, strong reasoning alone is inadequate without precise control. To provide a decoupled and quantitative assessment of this bottleneck, we introduce Embodied Reasoning Intelligence Quotient (ERIQ), a large-scale embodied reasoning benchmark in robotic manipulation, comprising 6K+ question-answer pairs across four reasoning dimensions. By decoupling reasoning from execution, ERIQ enables systematic evaluation and reveals a strong positive correlation between embodied reasoning capability and end-to-end VLA generalization. To bridge the gap from reasoning to precise execution, we propose FACT, a flow-matching-based action tokenizer that converts continuous control into discrete sequences while preserving high-fidelity trajectory reconstruction. The resulting GenieReasoner jointly optimizes reasoning and action in a unified space, outperforming both continuous-action and prior discrete-action baselines in real-world tasks. Together, ERIQ and FACT provide a principled framework for diagnosing and overcoming the reasoning-precision trade-off, advancing robust, general-purpose robotic manipulation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ERIQ\u57fa\u51c6\u4e0eFACT\u5de5\u5177\uff0c\u65e8\u5728\u89e3\u51b3\u901a\u7528\u673a\u5668\u4eba\u5728\u63a8\u7406\u4e0e\u7cbe\u51c6\u6267\u884c\u4e4b\u95f4\u7684\u6311\u6218\uff0c\u63d0\u5347\u673a\u5668\u4eba\u7684\u64cd\u4f5c\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6709\u6548\u6574\u5408\u5e7f\u6cdb\u6cdb\u5316\u4e0e\u9ad8\u7cbe\u5ea6\u884c\u52a8\u6267\u884c\u3002", "method": "\u5f15\u5165ERIQ\u8bc4\u4f30\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u91c7\u7528FACT\u5c06\u8fde\u7eed\u63a7\u5236\u8f6c\u5316\u4e3a\u79bb\u6563\u5e8f\u5217\uff0c\u5b9e\u73b0\u63a8\u7406\u4e0e\u884c\u52a8\u7684\u7edf\u4e00\u4f18\u5316\u3002", "result": "GenieReasoner\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4ee5\u5f80\u7684\u52a8\u4f5c\u57fa\u7ebf\uff0c\u8868\u660e\u63a8\u7406\u80fd\u529b\u4e0e\u6267\u884c\u7cbe\u5ea6\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u6b63\u76f8\u5173\u3002", "conclusion": "ERIQ\u4e0eFACT\u4e3a\u89e3\u51b3\u673a\u5668\u4eba\u7cbe\u51c6\u63a7\u5236\u4e0e\u63a8\u7406\u80fd\u529b\u4e4b\u95f4\u7684\u77db\u76fe\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6846\u67b6\uff0c\u63a8\u52a8\u4e86\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u8fdb\u6b65\u3002"}}
{"id": "2512.24129", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.24129", "abs": "https://arxiv.org/abs/2512.24129", "authors": ["Manuel Bied", "John Arockiasamy", "Andy Comeca", "Maximilian Schrapel", "Victoria Yang", "Alexey Rolich", "Barbara Bruno", "Maike Schwammberger", "Dieter Fiems", "Alexey Vinel"], "title": "ROBOPOL: Social Robotics Meets Vehicular Communications for Cooperative Automated Driving", "comment": null, "summary": "On the way towards full autonomy, sharing roads between automated vehicles and human actors in so-called mixed traffic is unavoidable. Moreover, even if all vehicles on the road were autonomous, pedestrians would still be crossing the streets. We propose social robots as moderators between autonomous vehicles and vulnerable road users (VRU). To this end, we identify four enablers requiring integration: (1) advanced perception, allowing the robot to see the environment; (2) vehicular communications allowing connected vehicles to share intentions and the robot to send guiding commands; (3) social human-robot interaction allowing the robot to effectively communicate with VRUs and drivers; (4) formal specification allowing the robot to understand traffic and plan accordingly. This paper presents an overview of the key enablers and report on a first proof-of-concept integration of the first three enablers envisioning a social robot advising pedestrians in scenarios with a cooperative automated e-bike.", "AI": {"tldr": "\u672c\u8bba\u6587\u63a2\u8ba8\u4e86\u793e\u4ea4\u673a\u5668\u4eba\u5982\u4f55\u5e2e\u52a9\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e0e\u884c\u4eba\u4e92\u52a8\uff0c\u63d0\u51fa\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u7684\u56db\u4e2a\u5173\u952e\u6280\u672f\uff0c\u5e76\u8fdb\u884c\u4e86\u521d\u6b65\u7684\u6982\u5ff5\u9a8c\u8bc1\u3002", "motivation": "\u5728\u5b9e\u73b0\u5b8c\u5168\u81ea\u4e3b\u9a7e\u9a76\u7684\u8fc7\u7a0b\u4e2d\uff0c\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e0e\u4eba\u7c7b\u4ea4\u901a\u53c2\u4e0e\u8005\u5171\u4eab\u9053\u8def\u662f\u4e0d\u53ef\u907f\u514d\u7684\uff0c\u5c24\u5176\u662f\u9762\u5bf9\u884c\u4eba\u7b49\u8106\u5f31\u9053\u8def\u7528\u6237\u7684\u60c5\u51b5\u3002", "method": "\u63d0\u51fa\u5c06\u793e\u4ea4\u673a\u5668\u4eba\u4f5c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e0e\u8106\u5f31\u9053\u8def\u7528\u6237\u4e4b\u95f4\u7684\u8c03\u89e3\u8005\uff0c\u8bc6\u522b\u51fa\u96c6\u6210\u7684\u56db\u4e2a\u5173\u952e\u56e0\u7d20\u3002", "result": "\u6982\u8ff0\u4e86\u56db\u4e2a\u5173\u952e\u56e0\u7d20\u5e76\u62a5\u544a\u4e86\u524d\u671f\u6982\u5ff5\u9a8c\u8bc1\uff0c\u96c6\u6210\u4e86\u524d\u4e09\u4e2a\u5173\u952e\u56e0\u7d20\uff0c\u8bbe\u60f3\u4e86\u793e\u4ea4\u673a\u5668\u4eba\u5728\u4e0e\u534f\u4f5c\u81ea\u52a8\u7535\u52a8\u81ea\u884c\u8f66\u7684\u573a\u666f\u4e2d\u6307\u5bfc\u884c\u4eba\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u793e\u4ea4\u673a\u5668\u4eba\u5728\u6df7\u5408\u4ea4\u901a\u4e2d\u4e0e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u534f\u4f5c\u63d0\u4f9b\u4e86\u521d\u6b65\u7684\u6982\u5ff5\u9a8c\u8bc1\uff0c\u663e\u793a\u4e86\u8fd9\u79cd\u96c6\u6210\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.24210", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24210", "abs": "https://arxiv.org/abs/2512.24210", "authors": ["Ruoshi Wen", "Guangzeng Chen", "Zhongren Cui", "Min Du", "Yang Gou", "Zhigang Han", "Liqun Huang", "Mingyu Lei", "Yunfei Li", "Zhuohang Li", "Wenlei Liu", "Yuxiao Liu", "Xiao Ma", "Hao Niu", "Yutao Ouyang", "Zeyu Ren", "Haixin Shi", "Wei Xu", "Haoxiang Zhang", "Jiajun Zhang", "Xiao Zhang", "Liwei Zheng", "Weiheng Zhong", "Yifei Zhou", "Zhengming Zhu", "Hang Li"], "title": "GR-Dexter Technical Report", "comment": null, "summary": "Vision-language-action (VLA) models have enabled language-conditioned, long-horizon robot manipulation, but most existing systems are limited to grippers. Scaling VLA policies to bimanual robots with high degree-of-freedom (DoF) dexterous hands remains challenging due to the expanded action space, frequent hand-object occlusions, and the cost of collecting real-robot data. We present GR-Dexter, a holistic hardware-model-data framework for VLA-based generalist manipulation on a bimanual dexterous-hand robot. Our approach combines the design of a compact 21-DoF robotic hand, an intuitive bimanual teleoperation system for real-robot data collection, and a training recipe that leverages teleoperated robot trajectories together with large-scale vision-language and carefully curated cross-embodiment datasets. Across real-world evaluations spanning long-horizon everyday manipulation and generalizable pick-and-place, GR-Dexter achieves strong in-domain performance and improved robustness to unseen objects and unseen instructions. We hope GR-Dexter serves as a practical step toward generalist dexterous-hand robotic manipulation.", "AI": {"tldr": "GR-Dexter\u6846\u67b6\u7ed3\u5408\u4e86\u786c\u4ef6\u3001\u6a21\u578b\u548c\u6570\u636e\uff0c\u6210\u529f\u5e94\u5bf9\u53cc\u624b\u7075\u5de7\u673a\u5668\u4eba\u7684\u64cd\u4f5c\u6311\u6218\uff0c\u5177\u6709\u826f\u597d\u7684\u6027\u80fd\u4e0e\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u4ee3\u7406\u673a\u6784\u5728\u53cc\u624b\u7075\u5de7\u64cd\u4f5c\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u6269\u5927\u884c\u52a8\u7a7a\u95f4\u548c\u6539\u8fdb\u6570\u636e\u6536\u96c6\u8fc7\u7a0b\u3002", "method": "\u5c06\u7d27\u51d1\u768421\u81ea\u7531\u5ea6\u673a\u68b0\u624b\u8bbe\u8ba1\u3001\u76f4\u89c2\u7684\u53cc\u624b\u9065\u64cd\u4f5c\u7cfb\u7edf\u4ee5\u53ca\u8bad\u7ec3\u65b9\u6848\u76f8\u7ed3\u5408\uff0c\u5229\u7528\u9065\u64cd\u4f5c\u673a\u5668\u4eba\u8f68\u8ff9\u548c\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6570\u636e\u96c6", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u4e2d\uff0cGR-Dexter\u5728\u957f\u65f6\u95f4\u65e5\u5e38\u64cd\u4f5c\u548c\u53ef\u63a8\u5e7f\u7684\u62fe\u653e\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u52b2\u7684\u9886\u57df\u5185\u6027\u80fd\uff0c\u5e76\u5bf9\u672a\u89c1\u7269\u4f53\u548c\u6307\u4ee4\u8868\u73b0\u51fa\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "GR-Dexter\u4f5c\u4e3a\u671d\u5411\u901a\u7528\u7075\u5de7\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u5b9e\u9645\u6b65\u9aa4\uff0c\u7b80\u5316\u4e86\u64cd\u4f5c\u548c\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2512.24212", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.24212", "abs": "https://arxiv.org/abs/2512.24212", "authors": ["Ming-Ming Yu", "Yi Chen", "B\u00f6rje F. Karlsson", "Wenjun Wu"], "title": "RANGER: A Monocular Zero-Shot Semantic Navigation Framework through Contextual Adaptation", "comment": null, "summary": "Efficiently finding targets in complex environments is fundamental to real-world embodied applications. While recent advances in multimodal foundation models have enabled zero-shot object goal navigation, allowing robots to search for arbitrary objects without fine-tuning, existing methods face two key limitations: (1) heavy reliance on precise depth and pose information provided by simulators, which restricts applicability in real-world scenarios; and (2) lack of in-context learning (ICL) capability, making it difficult to quickly adapt to new environments, as in leveraging short videos. To address these challenges, we propose RANGER, a novel zero-shot, open-vocabulary semantic navigation framework that operates using only a monocular camera. Leveraging powerful 3D foundation models, RANGER eliminates the dependency on depth and pose while exhibiting strong ICL capability. By simply observing a short video of a new environment, the system can also significantly improve task efficiency without requiring architectural modifications or fine-tuning. The framework integrates several key components: keyframe-based 3D reconstruction, semantic point cloud generation, vision-language model (VLM)-driven exploration value estimation, high-level adaptive waypoint selection, and low-level action execution. Experiments on the HM3D benchmark and real-world environments demonstrate that RANGER achieves competitive performance in terms of navigation success rate and exploration efficiency, while showing superior ICL adaptability, with no previous 3D mapping of the environment required.", "AI": {"tldr": "RANGER\u662f\u4e00\u79cd\u65b0\u9896\u7684\u96f6-shot\u8bed\u4e49\u5bfc\u822a\u6846\u67b6\uff0c\u5229\u7528\u5355\u76ee\u76f8\u673a\u9ad8\u6548\u5bfc\u822a\uff0c\u514b\u670d\u4e86\u5bf9\u6df1\u5ea6\u548c\u59ff\u6001\u4fe1\u606f\u7684\u4f9d\u8d56\uff0c\u5e76\u5177\u5907\u5b66\u4e60\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u5728\u590d\u6742\u73af\u5883\u4e2d\u6709\u6548\u5bfb\u627e\u76ee\u6807\u662f\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u7684\u5173\u952e\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7cbe\u786e\u7684\u6df1\u5ea6\u548c\u59ff\u6001\u4fe1\u606f\uff0c\u5e76\u4e14\u7f3a\u4e4f\u5feb\u901f\u9002\u5e94\u65b0\u73af\u5883\u7684\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u3002", "method": "RANGER\u7ed3\u5408\u4e86\u591a\u79cd\u5173\u952e\u7ec4\u4ef6\uff0c\u5305\u62ec\u57fa\u4e8e\u5173\u952e\u5e27\u76843D\u91cd\u5efa\u3001\u8bed\u4e49\u70b9\u4e91\u751f\u6210\u3001\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a2\u7d22\u4ef7\u503c\u4f30\u7b97\u3001\u9ad8\u7ea7\u81ea\u9002\u5e94\u822a\u6807\u9009\u62e9\u548c\u4f4e\u7ea7\u52a8\u4f5c\u6267\u884c\uff0c\u4ece\u800c\u5b9e\u73b0\u96f6-shot\u5bfc\u822a\u3002", "result": "RANGER\u662f\u4e00\u79cd\u65b0\u7684\u96f6-shot\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5bfc\u822a\u6846\u67b6\uff0c\u80fd\u591f\u5728\u6ca1\u6709\u6df1\u5ea6\u548c\u59ff\u6001\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u4ec5\u4f9d\u9760\u5355\u76ee\u76f8\u673a\u8fdb\u884c\u6709\u6548\u76ee\u6807\u5bfb\u627e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u3002", "conclusion": "\u901a\u8fc7\u5f3a\u5927\u76843D\u57fa\u7840\u6a21\u578b\u548c\u9ad8\u6c34\u5e73\u7684\u81ea\u9002\u5e94\u8def\u5f84\u9009\u62e9\uff0cRANGER\u5728HM3D\u57fa\u51c6\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5c55\u793a\u4e86\u7ade\u4e89\u529b\u7684\u5bfc\u822a\u6210\u529f\u7387\u548c\u63a2\u7d22\u6548\u7387\uff0c\u540c\u65f6\u5728\u65e0\u5148\u524d3D\u6620\u5c04\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u7684ICL\u9002\u5e94\u6027\u3002"}}
{"id": "2512.24249", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24249", "abs": "https://arxiv.org/abs/2512.24249", "authors": ["Fuqiang Gu", "Jiangshan Ai", "Xu Lu", "Xianlei Long", "Yan Li", "Tao Jiang", "Chao Chen", "Huidong Liu"], "title": "Heteroscedastic Bayesian Optimization-Based Dynamic PID Tuning for Accurate and Robust UAV Trajectory Tracking", "comment": "Accepted by IROS 2025 (2025 IEEE/RSJ International Conference on Intelligent Robots and Systems)", "summary": "Unmanned Aerial Vehicles (UAVs) play an important role in various applications, where precise trajectory tracking is crucial. However, conventional control algorithms for trajectory tracking often exhibit limited performance due to the underactuated, nonlinear, and highly coupled dynamics of quadrotor systems. To address these challenges, we propose HBO-PID, a novel control algorithm that integrates the Heteroscedastic Bayesian Optimization (HBO) framework with the classical PID controller to achieve accurate and robust trajectory tracking. By explicitly modeling input-dependent noise variance, the proposed method can better adapt to dynamic and complex environments, and therefore improve the accuracy and robustness of trajectory tracking. To accelerate the convergence of optimization, we adopt a two-stage optimization strategy that allow us to more efficiently find the optimal controller parameters. Through experiments in both simulation and real-world scenarios, we demonstrate that the proposed method significantly outperforms state-of-the-art (SOTA) methods. Compared to SOTA methods, it improves the position accuracy by 24.7% to 42.9%, and the angular accuracy by 40.9% to 78.4%.", "AI": {"tldr": "\u63d0\u51fa\u7684HBO-PID\u63a7\u5236\u7b97\u6cd5\u7ed3\u5408\u5f02\u65b9\u5dee\u8d1d\u53f6\u65af\u4f18\u5316\u548cPID\u63a7\u5236\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u7684\u8f68\u8ff9\u8ddf\u8e2a\u7cbe\u5ea6\u3002", "motivation": "\u65e0\u4eba\u673a\u5728\u591a\u79cd\u5e94\u7528\u4e2d\u626e\u6f14\u91cd\u8981\u89d2\u8272\uff0c\u4f46\u4f20\u7edf\u8f68\u8ff9\u8ddf\u8e2a\u63a7\u5236\u7b97\u6cd5\u56e0\u56db\u65cb\u7ffc\u7cfb\u7edf\u7684\u6b20\u9a71\u52a8\u3001\u975e\u7ebf\u6027\u53ca\u9ad8\u5ea6\u8026\u5408\u52a8\u6001\u800c\u8868\u73b0\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHBO-PID\u7684\u65b0\u578b\u63a7\u5236\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e86\u5f02\u65b9\u5dee\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\u4e0e\u7ecf\u5178PID\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u5efa\u6a21\u8f93\u5165\u76f8\u5173\u566a\u58f0\u65b9\u5dee\u4ee5\u63d0\u9ad8\u8ddf\u8e2a\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u548c\u5b9e\u9645\u573a\u666f\u5b9e\u9a8c\uff0cHBO-PID\u65b9\u6cd5\u5728\u4f4d\u7f6e\u7cbe\u5ea6\u4e0a\u6bd4\u5148\u8fdb\u6280\u672f\u63d0\u9ad8\u4e8624.7%\u81f342.9%\uff0c\u5728\u89d2\u5ea6\u7cbe\u5ea6\u4e0a\u63d0\u9ad8\u4e8640.9%\u81f378.4%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u7684\u5148\u8fdb\u6280\u672f\uff0c\u63d0\u9ad8\u4e86\u4f4d\u7f6e\u548c\u89d2\u5ea6\u7684\u8ddf\u8e2a\u7cbe\u5ea6\u3002"}}
{"id": "2512.24272", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24272", "abs": "https://arxiv.org/abs/2512.24272", "authors": ["Jiawei Zhang", "Chengchao Bai", "Wei Pan", "Tianhang Liu", "Jifeng Guo"], "title": "Local Path Optimization in The Latent Space Using Learned Distance Gradient", "comment": "This paper has been published in IROS 2025", "summary": "Constrained motion planning is a common but challenging problem in robotic manipulation. In recent years, data-driven constrained motion planning algorithms have shown impressive planning speed and success rate. Among them, the latent motion method based on manifold approximation is the most efficient planning algorithm. Due to errors in manifold approximation and the difficulty in accurately identifying collision conflicts within the latent space, time-consuming path validity checks and path replanning are required. In this paper, we propose a method that trains a neural network to predict the minimum distance between the robot and obstacles using latent vectors as inputs. The learned distance gradient is then used to calculate the direction of movement in the latent space to move the robot away from obstacles. Based on this, a local path optimization algorithm in the latent space is proposed, and it is integrated with the path validity checking process to reduce the time of replanning. The proposed method is compared with state-of-the-art algorithms in multiple planning scenarios, demonstrating the fastest planning speed", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u6700\u5c0f\u8ddd\u79bb\u9884\u6d4b\u65b9\u6cd5\uff0c\u7ed3\u5408\u5c40\u90e8\u8def\u5f84\u4f18\u5316\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u53d7\u9650\u8fd0\u52a8\u89c4\u5212\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edf\u7684\u53d7\u9650\u8fd0\u52a8\u89c4\u5212\u9762\u4e34\u8def\u5f84\u6709\u6548\u6027\u68c0\u67e5\u548c\u91cd\u89c4\u5212\u8017\u65f6\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u7531\u4e8e\u6d41\u5f62\u8fd1\u4f3c\u8bef\u5dee\u548c\u78b0\u649e\u51b2\u7a81\u8bc6\u522b\u56f0\u96be\u3002", "method": "\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u673a\u5668\u4eba\u4e0e\u969c\u788d\u7269\u4e4b\u95f4\u7684\u6700\u5c0f\u8ddd\u79bb\uff0c\u5e76\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u5c40\u90e8\u8def\u5f84\u4f18\u5316\u3002", "result": "\u4e0e\u6700\u5148\u8fdb\u7684\u7b97\u6cd5\u76f8\u6bd4\uff0c\u5728\u591a\u4e2a\u89c4\u5212\u573a\u666f\u4e2d\u5c55\u793a\u4e86\u6700\u5feb\u7684\u89c4\u5212\u901f\u5ea6\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u51cf\u5c11\u8def\u5f84\u91cd\u89c4\u5212\u7684\u65f6\u95f4\uff0c\u63d0\u9ad8\u8fd0\u52a8\u89c4\u5212\u901f\u5ea6\u3002"}}
{"id": "2512.24284", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24284", "abs": "https://arxiv.org/abs/2512.24284", "authors": ["Ruitong Li", "Lin Zhang", "Yuenan Zhao", "Chengxin Liu", "Ran Song", "Wei Zhang"], "title": "DRL-TH: Jointly Utilizing Temporal Graph Attention and Hierarchical Fusion for UGV Navigation in Crowded Environments", "comment": null, "summary": "Deep reinforcement learning (DRL) methods have demonstrated potential for autonomous navigation and obstacle avoidance of unmanned ground vehicles (UGVs) in crowded environments. Most existing approaches rely on single-frame observation and employ simple concatenation for multi-modal fusion, which limits their ability to capture temporal context and hinders dynamic adaptability. To address these challenges, we propose a DRL-based navigation framework, DRL-TH, which leverages temporal graph attention and hierarchical graph pooling to integrate historical observations and adaptively fuse multi-modal information. Specifically, we introduce a temporal-guided graph attention network (TG-GAT) that incorporates temporal weights into attention scores to capture correlations between consecutive frames, thereby enabling the implicit estimation of scene evolution. In addition, we design a graph hierarchical abstraction module (GHAM) that applies hierarchical pooling and learnable weighted fusion to dynamically integrate RGB and LiDAR features, achieving balanced representation across multiple scales. Extensive experiments demonstrate that our DRL-TH outperforms existing methods in various crowded environments. We also implemented DRL-TH control policy on a real UGV and showed that it performed well in real world scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u65b0\u578b\u5bfc\u822a\u6846\u67b6DRL-TH\uff0c\u901a\u8fc7\u65f6\u95f4\u56fe\u6ce8\u610f\u529b\u548c\u5206\u5c42\u56fe\u6c60\u5316\u63d0\u9ad8\u4e86\u65e0\u4eba\u5730\u9762\u8f66\u8f86\u5728\u62e5\u6324\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u548c\u969c\u788d\u7269\u89c4\u907f\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5355\u5e27\u89c2\u5bdf\u5e76\u91c7\u7528\u7b80\u5355\u7684\u591a\u6a21\u6001\u878d\u5408\u65b9\u5f0f\uff0c\u8fd9\u9650\u5236\u4e86\u6355\u6349\u65f6\u5e8f\u4e0a\u4e0b\u6587\u7684\u80fd\u529b\uff0c\u5f71\u54cd\u4e86\u52a8\u6001\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDRL\u7684\u5bfc\u822a\u6846\u67b6DRL-TH\uff0c\u5229\u7528\u65f6\u95f4\u56fe\u6ce8\u610f\u529b\u548c\u5206\u5c42\u56fe\u6c60\u5316\u6765\u6574\u5408\u5386\u53f2\u89c2\u5bdf\u548c\u9002\u5e94\u6027\u878d\u5408\u591a\u6a21\u6001\u4fe1\u606f\u3002", "result": "DRL-TH\u901a\u8fc7\u5f15\u5165\u65f6\u95f4\u52a0\u6743\u7684\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u548c\u56fe\u5206\u5c42\u62bd\u8c61\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u5bf9RGB\u548c\u6fc0\u5149\u96f7\u8fbe\u7279\u5f81\u7684\u52a8\u6001\u96c6\u6210\uff0c\u5e76\u5728\u5404\u79cd\u62e5\u6324\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u5b9e\u9a8c\u8868\u660e\uff0cDRL-TH\u5728\u5404\u79cd\u62e5\u6324\u73af\u5883\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u8868\u73b0\u826f\u597d\u3002"}}
{"id": "2512.24288", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24288", "abs": "https://arxiv.org/abs/2512.24288", "authors": ["Yinuo Zhao", "Huiqian Jin", "Lechun Jiang", "Xinyi Zhang", "Kun Wu", "Pei Ren", "Zhiyuan Xu", "Zhengping Che", "Lei Sun", "Dapeng Wu", "Chi Harold Liu", "Jian Tang"], "title": "Real-world Reinforcement Learning from Suboptimal Interventions", "comment": null, "summary": "Real-world reinforcement learning (RL) offers a promising approach to training precise and dexterous robotic manipulation policies in an online manner, enabling robots to learn from their own experience while gradually reducing human labor. However, prior real-world RL methods often assume that human interventions are optimal across the entire state space, overlooking the fact that even expert operators cannot consistently provide optimal actions in all states or completely avoid mistakes. Indiscriminately mixing intervention data with robot-collected data inherits the sample inefficiency of RL, while purely imitating intervention data can ultimately degrade the final performance achievable by RL. The question of how to leverage potentially suboptimal and noisy human interventions to accelerate learning without being constrained by them thus remains open. To address this challenge, we propose SiLRI, a state-wise Lagrangian reinforcement learning algorithm for real-world robot manipulation tasks. Specifically, we formulate the online manipulation problem as a constrained RL optimization, where the constraint bound at each state is determined by the uncertainty of human interventions. We then introduce a state-wise Lagrange multiplier and solve the problem via a min-max optimization, jointly optimizing the policy and the Lagrange multiplier to reach a saddle point. Built upon a human-as-copilot teleoperation system, our algorithm is evaluated through real-world experiments on diverse manipulation tasks. Experimental results show that SiLRI effectively exploits human suboptimal interventions, reducing the time required to reach a 90% success rate by at least 50% compared with the state-of-the-art RL method HIL-SERL, and achieving a 100% success rate on long-horizon manipulation tasks where other RL methods struggle to succeed. Project website: https://silri-rl.github.io/.", "AI": {"tldr": "SiLRI\u662f\u4e00\u79cd\u65b0\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5229\u7528\u4eba\u7c7b\u7684\u6b21\u4f18\u5e72\u9884\u6765\u52a0\u901f\u673a\u5668\u4eba\u64cd\u63a7\u4efb\u52a1\u7684\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u9ad8\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5f80\u5f80\u5047\u8bbe\u4eba\u7c7b\u7684\u5e72\u9884\u662f\u6700\u4f73\u7684\uff0c\u4f46\u5b9e\u9645\u4e0a\u5373\u4fbf\u662f\u4e13\u5bb6\u5728\u6240\u6709\u72b6\u6001\u4e0b\u4e5f\u65e0\u6cd5\u63d0\u4f9b\u6700\u4f73\u7684\u52a8\u4f5c\uff0c\u6b64\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u4ece\u6f5c\u5728\u6b21\u4f18\u5e76\u5e26\u6709\u566a\u58f0\u7684\u4eba\u7c7b\u5e72\u9884\u4e2d\u52a0\u901f\u5b66\u4e60\u3002", "method": "\u901a\u8fc7\u5c06\u5728\u7ebf\u64cd\u63a7\u95ee\u9898\u516c\u5f0f\u5316\u4e3a\u4e00\u4e2a\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u7528\u72b6\u6001\u76f8\u5173\u7684\u62c9\u683c\u6717\u65e5\u4e58\u5b50\u8fdb\u884c\u8054\u5408\u4f18\u5316\uff0c\u6700\u7ec8\u8fbe\u5230\u978d\u70b9\u3002", "result": "\u63d0\u51fa\u7684SiLRI\u7b97\u6cd5\u901a\u8fc7\u8003\u8651\u4eba\u7c7b\u5e72\u9884\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u5404\u79cd\u64cd\u63a7\u4efb\u52a1\u4e2d\u6709\u6548\u5229\u7528\u6f5c\u5728\u7684\u6b21\u4f18\u5e72\u9884\uff0c\u7f29\u77ed\u4e86\u8fbe\u5230\u9ad8\u6210\u529f\u7387\u6240\u9700\u7684\u65f6\u95f4\u3002", "conclusion": "SiLRI\u7b97\u6cd5\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5b83\u5bf9\u4eba\u7c7b\u7684\u6b21\u4f18\u5e72\u9884\u8fdb\u884c\u6709\u6548\u5229\u7528\uff0c\u4f7f\u5f97\u5728\u957f\u65f6\u95f4\u64cd\u4f5c\u4efb\u52a1\u4e2d\u6210\u529f\u7387\u8fbe\u5230\u4e86100%\uff0c\u800c\u5176\u4ed6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5219\u96be\u4ee5\u8fbe\u5230\u8fd9\u6837\u7684\u6548\u679c\u3002"}}
{"id": "2512.24310", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24310", "abs": "https://arxiv.org/abs/2512.24310", "authors": ["TARS Robotics", "Yuhang Zheng", "Jichao Peng", "Weize Li", "Yupeng Zheng", "Xiang Li", "Yujie Jin", "Julong Wei", "Guanhua Zhang", "Ruiling Zheng", "Ming Cao", "Songen Gu", "Zhenhong Zou", "Kaige Li", "Ke Wu", "Mingmin Yang", "Jiahao Liu", "Pengfei Li", "Hengjie Si", "Feiyu Zhu", "Wang Fu", "Likun Wang", "Ruiwen Yao", "Jieru Zhao", "Yilun Chen", "Wenchao Din"], "title": "World In Your Hands: A Large-Scale and Open-source Ecosystem for Learning Human-centric Manipulation in the Wild", "comment": null, "summary": "Large-scale pre-training is fundamental for generalization in language and vision models, but data for dexterous hand manipulation remains limited in scale and diversity, hindering policy generalization. Limited scenario diversity, misaligned modalities, and insufficient benchmarking constrain current human manipulation datasets. To address these gaps, we introduce World In Your Hands (WiYH), a large-scale open-source ecosystem for human-centric manipulation learning. WiYH includes (1) the Oracle Suite, a wearable data collection kit with an auto-labeling pipeline for accurate motion capture; (2) the WiYH Dataset, featuring over 1,000 hours of multi-modal manipulation data across hundreds of skills in diverse real-world scenarios; and (3) extensive annotations and benchmarks supporting tasks from perception to action. Furthermore, experiments based on the WiYH ecosystem show that integrating WiYH's human-centric data significantly enhances the generalization and robustness of dexterous hand policies in tabletop manipulation tasks. We believe that World In Your Hands will bring new insights into human-centric data collection and policy learning to the community.", "AI": {"tldr": "WiYH\u662f\u4e00\u4e2a\u5927\u578b\u5f00\u6e90\u751f\u6001\u7cfb\u7edf\uff0c\u65e8\u5728\u901a\u8fc7\u591a\u6a21\u6001\u7684\u6570\u636e\u548c\u6807\u6ce8\u96c6\u6765\u63d0\u5347\u4eba\u7c7b\u64cd\u63a7\u5b66\u4e60\uff0c\u7279\u522b\u662f\u5728\u684c\u9762\u64cd\u63a7\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6548\u679c\u663e\u8457\u63d0\u9ad8\u3002", "motivation": "\u5f53\u524d\u7684\u4eba\u7c7b\u64cd\u63a7\u6570\u636e\u96c6\u5728\u89c4\u6a21\u548c\u591a\u6837\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u8fd9\u9650\u5236\u4e86\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5f00\u53d1Oracle Suite\u53ef\u7a7f\u6234\u6570\u636e\u91c7\u96c6\u5de5\u5177\u53caWiYH\u6570\u636e\u96c6\uff0c\u6536\u96c6\u8d85\u8fc71000\u5c0f\u65f6\u7684\u591a\u6a21\u6001\u64cd\u63a7\u6570\u636e\uff0c\u5e76\u63d0\u4f9b\u5e7f\u6cdb\u7684\u6ce8\u91ca\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5229\u7528WiYH\u7684\u4eba\u7c7b\u4e2d\u5fc3\u6570\u636e\u663e\u8457\u589e\u5f3a\u4e86\u7075\u5de7\u624b\u653f\u7b56\u5728\u684c\u9762\u64cd\u63a7\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "World In Your Hands (WiYH) improves dexterous hand manipulation policy learning through extensive and diverse datasets, enhancing generalization and robustness."}}
{"id": "2512.24326", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24326", "abs": "https://arxiv.org/abs/2512.24326", "authors": ["Camron Alexander Hirst", "Chris Reale", "Eric Frew"], "title": "3D Path-Following Guidance via Nonlinear Model Predictive Control for Fixed-Wing Small UAS", "comment": null, "summary": "This paper presents the design, implementation, and flight test results of two novel 3D path-following guidance algorithms based on nonlinear model predictive control (MPC), with specific application to fixed-wing small uncrewed aircraft systems. To enable MPC, control-augmented modelling and system identification of the RAAVEN small uncrewed aircraft is presented. Two formulations of MPC are then showcased. The first schedules a static reference path rate over the MPC horizon, incentivizing a constant inertial speed. The second, with inspiration from model predictive contouring control, dynamically optimizes for the reference path rate over the controller horizon as the system operates. This allows for a weighted tradeoff between path progression and distance from path, two competing objectives in path-following guidance. Both controllers are formulated to operate over general smooth 3D arc-length parameterized curves. The MPC guidance algorithms are flown over several high-curvature test paths, with comparison to a baseline lookahead guidance law. The results showcase the real-world feasibility and superior performance of nonlinear MPC for 3D path-following guidance at ground speeds up to 36 meters per second.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c55\u793a\u4e86\u57fa\u4e8e\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u76843D\u8def\u5f84\u8ddf\u968f\u5f15\u5bfc\u7b97\u6cd5\uff0c\u5f3a\u8c03\u4e86\u5176\u5728\u5c0f\u578b\u65e0\u4eba\u673a\u5e94\u7528\u4e2d\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u8bbe\u8ba1\u548c\u5b9e\u73b0\u4e24\u79cd\u65b0\u578b\u76843D\u8def\u5f84\u8ddf\u968f\u5f15\u5bfc\u7b97\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5c0f\u578b\u65e0\u6e90\u98de\u884c\u5668\u5728\u590d\u6742\u8def\u5f84\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u57fa\u4e8e\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236(MPC)\u7684\u65b9\u6cd5\uff0c\u91c7\u7528\u63a7\u5236\u589e\u5f3a\u5efa\u6a21\u548c\u7cfb\u7edf\u8bc6\u522b\u6280\u672f\uff0c\u5bf9\u5c0f\u578b\u65e0\u4eba\u673a\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728\u9ad8\u66f2\u7387\u6d4b\u8bd5\u8def\u5f84\u4e0a\u98de\u884c\u5e76\u4e0e\u57fa\u7ebf\u524d\u89c6\u5f15\u5bfc\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff0c\u7ed3\u679c\u663e\u793a\u975e\u7ebf\u6027MPC\u57283D\u8def\u5f84\u8ddf\u968f\u5f15\u5bfc\u4e0a\u5177\u5907\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u975e\u7ebf\u6027MPC\u7b97\u6cd5\u5728\u4ee5\u9ad8\u8fbe36\u7c73\u6bcf\u79d2\u7684\u901f\u5ea6\u8fdb\u884c3D\u8def\u5f84\u8ddf\u968f\u65f6\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u5177\u6709\u53ef\u884c\u6027\u548c\u5353\u8d8a\u6027\u80fd\u3002"}}
{"id": "2512.24384", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.24384", "abs": "https://arxiv.org/abs/2512.24384", "authors": ["Yanlong Ma", "Nakul S. Joshi", "Christa S. Robison", "Philip R. Osteen", "Brett T. Lopez"], "title": "Geometric Multi-Session Map Merging with Learned Local Descriptors", "comment": null, "summary": "Multi-session map merging is crucial for extended autonomous operations in large-scale environments. In this paper, we present GMLD, a learning-based local descriptor framework for large-scale multi-session point cloud map merging that systematically aligns maps collected across different sessions with overlapping regions. The proposed framework employs a keypoint-aware encoder and a plane-based geometric transformer to extract discriminative features for loop closure detection and relative pose estimation. To further improve global consistency, we include inter-session scan matching cost factors in the factor-graph optimization stage. We evaluate our framework on the public datasets, as well as self-collected data from diverse environments. The results show accurate and robust map merging with low error, and the learned features deliver strong performance in both loop closure detection and relative pose estimation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGMLD\u6846\u67b6\uff0c\u9488\u5bf9\u5927\u89c4\u6a21\u591a\u4f1a\u8bdd\u70b9\u4e91\u5730\u56fe\u5408\u5e76\uff0c\u901a\u8fc7\u5b66\u4e60\u65b9\u6cd5\u5b9e\u73b0\u7cbe\u51c6\u5bf9\u9f50\u548c\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u5730\u56fe\u5408\u5e76\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5728\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u6269\u5c55\u81ea\u4e3b\u64cd\u4f5c\u65f6\u591a\u4f1a\u8bdd\u5730\u56fe\u5408\u5e76\u7684\u6311\u6218\uff0c\u786e\u4fdd\u4e0d\u540c\u4f1a\u8bdd\u6536\u96c6\u7684\u5730\u56fe\u5728\u91cd\u53e0\u533a\u57df\u8fdb\u884c\u7cfb\u7edf\u5bf9\u9f50\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5b66\u4e60\u7684\u672c\u5730\u63cf\u8ff0\u5b50\u6846\u67b6\uff0c\u901a\u8fc7\u5173\u952e\u70b9\u611f\u77e5\u7f16\u7801\u5668\u548c\u57fa\u4e8e\u5e73\u9762\u7684\u51e0\u4f55\u53d8\u6362\u5668\u6765\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u5728\u56e0\u5b50\u56fe\u4f18\u5316\u9636\u6bb5\u5305\u542b\u8de8\u4f1a\u8bdd\u626b\u63cf\u5339\u914d\u6210\u672c\u56e0\u5b50\uff0c\u4ee5\u63d0\u9ad8\u5168\u5c40\u4e00\u81f4\u6027\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u548c\u81ea\u6536\u96c6\u7684\u6570\u636e\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u51fa\u4f4e\u8bef\u5dee\u7684\u51c6\u786e\u548c\u9c81\u68d2\u7684\u5730\u56fe\u5408\u5e76\u6548\u679c\u3002", "conclusion": "\u901a\u8fc7\u5728\u591a\u79cd\u73af\u5883\u4e2d\u6d4b\u8bd5\uff0cGMLD\u5c55\u793a\u4e86\u5728\u5730\u56fe\u5408\u5e76\u65b9\u9762\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u5728\u73af\u95ed\u5408\u68c0\u6d4b\u548c\u76f8\u5bf9\u4f4d\u59ff\u4f30\u8ba1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2512.24402", "categories": ["cs.RO", "cs.AI", "cs.SE", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.24402", "abs": "https://arxiv.org/abs/2512.24402", "authors": ["Giovanni Lambertini", "Matteo Pini", "Eugenio Mascaro", "Francesco Moretti", "Ayoub Raji", "Marko Bertogna"], "title": "Fast and Realistic Automated Scenario Simulations and Reporting for an Autonomous Racing Stack", "comment": "Accepted to the 2026 IEEE/SICE International Symposium on System Integration (SII 2026)", "summary": "In this paper, we describe the automated simulation and reporting pipeline implemented for our autonomous racing stack, ur.autopilot. The backbone of the simulation is based on a high-fidelity model of the vehicle interfaced as a Functional Mockup Unit (FMU). The pipeline can execute the software stack and the simulation up to three times faster than real-time, locally or on GitHub for Continuous Integration/- Continuous Delivery (CI/CD). As the most important input of the pipeline, there is a set of running scenarios. Each scenario allows the initialization of the ego vehicle in different initial conditions (position and speed), as well as the initialization of any other configuration of the stack. This functionality is essential to validate efficiently critical modules, like the one responsible for high-speed overtaking maneuvers or localization, which are among the most challenging aspects of autonomous racing. Moreover, we describe how we implemented a fault injection module, capable of introducing sensor delays and perturbations as well as modifying outputs of any node of the stack. Finally, we describe the design of our automated reporting process, aimed at maximizing the effectiveness of the simulation analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u4eff\u771f\u548c\u62a5\u544a\u6d41\u6c34\u7ebf\uff0c\u65e8\u5728\u63d0\u9ad8\u81ea\u4e3b\u8d5b\u8f66\u6280\u672f\u7684\u9a8c\u8bc1\u6548\u7387\uff0c\u901a\u8fc7\u9ad8\u4fdd\u771f\u6a21\u578b\u548c\u6545\u969c\u6ce8\u5165\uff0c\u652f\u6301\u5173\u952e\u6a21\u5757\u7684\u6709\u6548\u9a8c\u8bc1\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u81ea\u52a8\u5316\u7684\u4eff\u771f\u548c\u62a5\u544a\u6d41\u6c34\u7ebf\uff0c\u4ee5\u63d0\u9ad8\u81ea\u4e3b\u8d5b\u8f66\u6280\u672f\u7684\u9a8c\u8bc1\u548c\u5206\u6790\u6548\u7387\u3002", "method": "\u57fa\u4e8e\u9ad8\u4fdd\u771f\u8f66\u8f86\u6a21\u578b\u6784\u5efa\u7684\u6d41\u6c34\u7ebf\uff0c\u652f\u6301\u8f6f\u4ef6\u5806\u6808\u7684\u5b9e\u65f6\u4eff\u771f\uff0c\u5e76\u901a\u8fc7\u8fd0\u884c\u4e0d\u540c\u573a\u666f\u521d\u59cb\u5316\u81ea\u6211\u8f66\u8f86\u53ca\u5806\u6808\u914d\u7f6e\uff0c\u7ed3\u5408\u6545\u969c\u6ce8\u5165\u6a21\u5757\u548c\u81ea\u52a8\u62a5\u544a\u6d41\u7a0b\u3002", "result": "\u8be5\u6d41\u6c34\u7ebf\u80fd\u591f\u4ee5\u6bd4\u5b9e\u65f6\u5feb\u4e09\u500d\u7684\u901f\u5ea6\u6267\u884c\u4eff\u771f\u548c\u8f6f\u4ef6\u5806\u6808\uff0c\u80fd\u591f\u6709\u6548\u9a8c\u8bc1\u5173\u952e\u6a21\u5757\uff0c\u5982\u9ad8\u901f\u8d85\u8f66\u548c\u5b9a\u4f4d\u3002", "conclusion": "\u901a\u8fc7\u8be5\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\uff0c\u53ef\u4ee5\u66f4\u9ad8\u6548\u5730\u9a8c\u8bc1\u548c\u5206\u6790\u81ea\u4e3b\u8d5b\u8f66\u7cfb\u7edf\uff0c\u4ece\u800c\u63d0\u9ad8\u5176\u53ef\u9760\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2512.24426", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24426", "abs": "https://arxiv.org/abs/2512.24426", "authors": ["Zhenghao \"Mark\" Peng", "Wenhao Ding", "Yurong You", "Yuxiao Chen", "Wenjie Luo", "Thomas Tian", "Yulong Cao", "Apoorva Sharma", "Danfei Xu", "Boris Ivanovic", "Boyi Li", "Bolei Zhou", "Yan Wang", "Marco Pavone"], "title": "Counterfactual VLA: Self-Reflective Vision-Language-Action Model with Adaptive Reasoning", "comment": null, "summary": "Recent reasoning-augmented Vision-Language-Action (VLA) models have improved the interpretability of end-to-end autonomous driving by generating intermediate reasoning traces. Yet these models primarily describe what they perceive and intend to do, rarely questioning whether their planned actions are safe or appropriate. This work introduces Counterfactual VLA (CF-VLA), a self-reflective VLA framework that enables the model to reason about and revise its planned actions before execution. CF-VLA first generates time-segmented meta-actions that summarize driving intent, and then performs counterfactual reasoning conditioned on both the meta-actions and the visual context. This step simulates potential outcomes, identifies unsafe behaviors, and outputs corrected meta-actions that guide the final trajectory generation. To efficiently obtain such self-reflective capabilities, we propose a rollout-filter-label pipeline that mines high-value scenes from a base (non-counterfactual) VLA's rollouts and labels counterfactual reasoning traces for subsequent training rounds. Experiments on large-scale driving datasets show that CF-VLA improves trajectory accuracy by up to 17.6%, enhances safety metrics by 20.5%, and exhibits adaptive thinking: it only enables counterfactual reasoning in challenging scenarios. By transforming reasoning traces from one-shot descriptions to causal self-correction signals, CF-VLA takes a step toward self-reflective autonomous driving agents that learn to think before they act.", "AI": {"tldr": "CF-VLA\u662f\u4e00\u79cd\u81ea\u6211\u53cd\u601d\u7684VLA\u6846\u67b6\uff0c\u901a\u8fc7\u53cd\u4e8b\u5b9e\u63a8\u7406\u4fee\u6b63\u9a7e\u9a76\u8ba1\u5212\uff0c\u663e\u8457\u63d0\u9ad8\u8f68\u8ff9\u7cbe\u5ea6\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u6709\u7684VLA\u6a21\u578b\u4f18\u5316\u4e86\u89e3\u91ca\u6027\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u8ba1\u5212\u884c\u52a8\u5b89\u5168\u6027\u7684\u53cd\u601d\u548c\u4fee\u6b63\u3002 CF-VLA\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u6211\u53cd\u601d\u7684VLA\u6846\u67b6CF-VLA\uff0c\u751f\u6210\u6570\u636e\u9a71\u52a8\u7684\u65f6\u95f4\u5206\u6bb5\u5143\u52a8\u4f5c\u5e76\u8fdb\u884c\u53cd\u4e8b\u5b9e\u63a8\u7406\uff0c\u4ee5\u4fee\u6b63\u9a71\u52a8\u610f\u56fe\u3002", "result": "\u5728\u5927\u89c4\u6a21\u9a7e\u9a76\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cCF-VLA\u7684\u8f68\u8ff9\u7cbe\u786e\u5ea6\u63d0\u9ad8\u4e8617.6%\uff0c\u5b89\u5168\u6027\u6307\u6807\u63d0\u534720.5%\u3002", "conclusion": "CF-VLA\u4fc3\u8fdb\u4e86\u81ea\u6211\u53cd\u601d\u7684\u81ea\u4e3b\u9a7e\u9a76\u4ee3\u7406\u7684\u53d1\u5c55\uff0c\u4f7f\u5176\u5728\u884c\u52a8\u524d\u8fdb\u884c\u601d\u8003\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u9a7e\u9a76\u5b89\u5168\u6027\u548c\u51b3\u7b56\u80fd\u529b\u3002"}}
{"id": "2512.24428", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24428", "abs": "https://arxiv.org/abs/2512.24428", "authors": ["Qian Wang", "Omar Abdellall", "Tony Gao", "Xiatao Sun", "Daniel Rakita"], "title": "Subsecond 3D Mesh Generation for Robot Manipulation", "comment": "In submission", "summary": "3D meshes are a fundamental representation widely used in computer science and engineering. In robotics, they are particularly valuable because they capture objects in a form that aligns directly with how robots interact with the physical world, enabling core capabilities such as predicting stable grasps, detecting collisions, and simulating dynamics. Although automatic 3D mesh generation methods have shown promising progress in recent years, potentially offering a path toward real-time robot perception, two critical challenges remain. First, generating high-fidelity meshes is prohibitively slow for real-time use, often requiring tens of seconds per object. Second, mesh generation by itself is insufficient. In robotics, a mesh must be contextually grounded, i.e., correctly segmented from the scene and registered with the proper scale and pose. Additionally, unless these contextual grounding steps remain efficient, they simply introduce new bottlenecks. In this work, we introduce an end-to-end system that addresses these challenges, producing a high-quality, contextually grounded 3D mesh from a single RGB-D image in under one second. Our pipeline integrates open-vocabulary object segmentation, accelerated diffusion-based mesh generation, and robust point cloud registration, each optimized for both speed and accuracy. We demonstrate its effectiveness in a real-world manipulation task, showing that it enables meshes to be used as a practical, on-demand representation for robotics perception and planning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u7cfb\u7edf\uff0c\u4ece\u5355\u4e2aRGB-D\u56fe\u50cf\u4e2d\u5728\u4e0d\u5230\u4e00\u79d2\u7684\u65f6\u95f4\u5185\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4e0a\u4e0b\u6587\u76f8\u51733D\u7f51\u683c\uff0c\u4ee5\u89e3\u51b3\u81ea\u52a83D\u7f51\u683c\u751f\u6210\u7684\u6548\u7387\u548c\u4e0a\u4e0b\u6587\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u8fd1\u5e74\u6765\u81ea\u52a83D\u7f51\u683c\u751f\u6210\u65b9\u6cd5\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u751f\u6210\u9ad8\u4fdd\u771f\u7f51\u683c\u4ecd\u7136\u7f13\u6162\uff0c\u5e76\u4e14\u9700\u8981\u4e0e\u573a\u666f\u4e0a\u4e0b\u6587\u76f8\u7ed3\u5408\u3002", "method": "\u91c7\u7528\u7aef\u5230\u7aef\u7cfb\u7edf\u96c6\u6210\u5f00\u653e\u8bcd\u6c47\u7269\u4f53\u5206\u5272\u3001\u52a0\u901f\u7684\u57fa\u4e8e\u6269\u6563\u7684\u7f51\u683c\u751f\u6210\u548c\u7a33\u5065\u7684\u70b9\u4e91\u6ce8\u518c\uff0c\u4f18\u5316\u901f\u5ea6\u548c\u51c6\u786e\u6027\u3002", "result": "\u8be5\u7cfb\u7edf\u80fd\u591f\u5728\u4e00\u79d2\u5185\u4ece\u5355\u4e2aRGB-D\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u76843D\u7f51\u683c\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u663e\u8457\u63d0\u9ad8\u4e863D\u7f51\u683c\u751f\u6210\u6548\u7387\uff0c\u4f7f\u5176\u5728\u673a\u5668\u4eba\u611f\u77e5\u548c\u89c4\u5212\u4e2d\u5177\u5907\u4e86\u5b9e\u7528\u6027\uff0c\u7279\u522b\u662f\u5728\u771f\u5b9e\u73af\u5883\u64cd\u4f5c\u4e2d\u5c55\u73b0\u4e86\u6709\u6548\u6027\u3002"}}
{"id": "2512.24470", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24470", "abs": "https://arxiv.org/abs/2512.24470", "authors": ["Kim Alexander Christensen", "Andreas Gudahl Tufte", "Alexey Gusev", "Rohan Sinha", "Milan Ganai", "Ole Andreas Alsos", "Marco Pavoned", "Martin Steinert"], "title": "Foundation models on the bridge: Semantic hazard detection and safety maneuvers for maritime autonomy with vision-language models", "comment": "17 pages without bibliography or appendix. The main paper has 16 figures", "summary": "The draft IMO MASS Code requires autonomous and remotely supervised maritime vessels to detect departures from their operational design domain, enter a predefined fallback that notifies the operator, permit immediate human override, and avoid changing the voyage plan without approval. Meeting these obligations in the alert-to-takeover gap calls for a short-horizon, human-overridable fallback maneuver. Classical maritime autonomy stacks struggle when the correct action depends on meaning (e.g., diver-down flag means people in the water, fire close by means hazard). We argue (i) that vision-language models (VLMs) provide semantic awareness for such out-of-distribution situations, and (ii) that a fast-slow anomaly pipeline with a short-horizon, human-overridable fallback maneuver makes this practical in the handover window. We introduce Semantic Lookout, a camera-only, candidate-constrained vision-language model (VLM) fallback maneuver selector that selects one cautious action (or station-keeping) from water-valid, world-anchored trajectories under continuous human authority. On 40 harbor scenes we measure per-call scene understanding and latency, alignment with human consensus (model majority-of-three voting), short-horizon risk-relief on fire hazard scenes, and an on-water alert->fallback maneuver->operator handover. Sub-10 s models retain most of the awareness of slower state-of-the-art models. The fallback maneuver selector outperforms geometry-only baselines and increases standoff distance on fire scenes. A field run verifies end-to-end operation. These results support VLMs as semantic fallback maneuver selectors compatible with the draft IMO MASS Code, within practical latency budgets, and motivate future work on domain-adapted, hybrid autonomy that pairs foundation-model semantics with multi-sensor bird's-eye-view perception and short-horizon replanning.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u7684Semantic Lookout\u6a21\u578b\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e3a\u81ea\u4e3b\u822a\u8fd0\u63d0\u4f9b\u8bed\u4e49\u610f\u8bc6\uff0c\u4ece\u800c\u589e\u5f3a\u77ed\u671f\u51b3\u7b56\u80fd\u529b\uff0c\u7b26\u5408IMO MASS\u4ee3\u7801\u7684\u8981\u6c42\uff0c\u5e76\u6709\u6548\u652f\u6301\u4eba\u7c7b\u64cd\u4f5c\u5e72\u9884\u3002", "motivation": "\u6ee1\u8db3\u56fd\u9645\u6d77\u4e8b\u7ec4\u7ec7(IMO) MASS\u4ee3\u7801\u5bf9\u81ea\u4e3b\u548c\u8fdc\u7a0b\u76d1\u7763\u822a\u8fd0\u8239\u53ea\u7684\u8981\u6c42\uff0c\u7279\u522b\u662f\u5728\u64cd\u4f5c\u8bbe\u8ba1\u57df\u5185\u68c0\u6d4b\u504f\u79bb\u60c5\u51b5\u5e76\u53ca\u65f6\u91c7\u53d6\u5e94\u5bf9\u63aa\u65bd\u3002", "method": "\u5f15\u5165\u8bed\u4e49 lookout\uff0c\u4e00\u4e2a\u57fa\u4e8e\u76f8\u673a\u7684\u5019\u9009\u7ea6\u675f\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b(VLM)\u540e\u5907\u673a\u52a8\u9009\u62e9\u5668\uff0c\u901a\u8fc7\u77ed\u671f\u4eba\u7c7b\u53ef\u8986\u76d6\u7684\u673a\u52a8\u6765\u9009\u62e9\u4ece\u6c34\u57df\u6709\u6548\u3001\u4e16\u754c\u951a\u5b9a\u8f68\u8ff9\u4e2d\u9009\u62e9\u4e00\u79cd\u8c28\u614e\u7684\u52a8\u4f5c\u6216\u9759\u6b62\u4fdd\u6301\u3002", "result": "\u572840\u4e2a\u6e2f\u53e3\u573a\u666f\u4e2d\u6d4b\u91cf\u6bcf\u6b21\u8c03\u7528\u7684\u573a\u666f\u7406\u89e3\u548c\u5ef6\u8fdf\uff0c\u6a21\u578b\u5728\u706b\u707e\u5371\u9669\u573a\u666f\u4e2d\u7684\u77ed\u671f\u98ce\u9669\u7f13\u89e3\u53ca\u4e0e\u4eba\u7c7b\u5171\u8bc6\u7684\u4e00\u81f4\u6027\u7b49\u8868\u73b0\u826f\u597d\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u540e\u5907\u673a\u52a8\u9009\u62e9\u5668\u4f18\u4e8e\u51e0\u4f55\u6a21\u578b\u57fa\u51c6\uff0c\u5e76\u5728\u706b\u707e\u573a\u666f\u4e2d\u63d0\u5347\u4e86\u5b89\u5168\u8ddd\u79bb\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u652f\u6301 VLM \u4f5c\u4e3a\u4e0e IMO MASS \u4ee3\u7801\u517c\u5bb9\u7684\u8bed\u4e49\u540e\u5907\u673a\u52a8\u9009\u62e9\u5668\uff0c\u5177\u6709\u5b9e\u7528\u5ef6\u8fdf\u9884\u7b97\uff0c\u5e76\u63a8\u52a8\u4e86\u672a\u6765\u7684\u591a\u4f20\u611f\u5668\u9e1f\u77b0\u611f\u89c9\u4e0e\u77ed\u671f\u91cd\u65b0\u89c4\u5212\u7684\u6df7\u5408\u81ea\u4e3b\u7814\u7a76\u3002"}}
{"id": "2512.24550", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24550", "abs": "https://arxiv.org/abs/2512.24550", "authors": ["Tomoya Yamanokuchi", "Alberto Bacchin", "Emilio Olivastri", "Ryotaro Arifuku", "Takamitsu Matsubara", "Emanuele Menegatti"], "title": "DISF: Disentangled Iterative Surface Fitting for Contact-stable Grasp Planning with Grasp Pose Alignment to the Object Center of Mass", "comment": "48 pages", "summary": "In this work, we address the limitation of surface fitting-based grasp planning algorithm, which primarily focuses on geometric alignment between the gripper and object surface while overlooking the stability of contact point distribution, often resulting in unstable grasps due to inadequate contact configurations. To overcome this limitation, we propose a novel surface fitting algorithm that integrates contact stability while preserving geometric compatibility. Inspired by human grasping behavior, our method disentangles the grasp pose optimization into three sequential steps: (1) rotation optimization to align contact normals, (2) translation refinement to improve the alignment between the gripper frame origin and the object Center of Mass (CoM), and (3) gripper aperture adjustment to optimize contact point distribution. We validate our approach in simulation across 15 objects under both Known-shape (with clean CAD-derived dataset) and Observed-shape (with YCB object dataset) settings, including cross-platform grasp execution on three robot--gripper platforms. We further validate the method in real-world grasp experiments on a UR3e robot. Overall, DISF reduces CoM misalignment while maintaining geometric compatibility, translating into higher grasp success in both simulation and real-world execution compared to baselines. Additional videos and supplementary results are available on our project page: https://tomoya-yamanokuchi.github.io/disf-ras-project-page/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u6293\u53d6\u59ff\u6001\u7684\u4e0d\u540c\u6b65\u9aa4\uff0c\u63d0\u9ad8\u4e86\u63a5\u89e6\u70b9\u7684\u7a33\u5b9a\u6027\u548c\u6293\u53d6\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u8868\u9762\u62df\u5408\u7684\u6293\u53d6\u89c4\u5212\u7b97\u6cd5\u5728\u63a5\u89e6\u70b9\u5206\u5e03\u7a33\u5b9a\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u8868\u9762\u62df\u5408\u7b97\u6cd5\uff0c\u96c6\u6210\u63a5\u89e6\u7a33\u5b9a\u6027\u4e0e\u51e0\u4f55\u517c\u5bb9\u6027\uff0c\u5206\u89e3\u6293\u53d6\u59ff\u6001\u4f18\u5316\u4e3a\u65cb\u8f6c\u4f18\u5316\u3001\u5e73\u79fb\u7ec6\u5316\u548c\u5939\u722a\u5f00\u53e3\u8c03\u6574\u4e09\u4e2a\u6b65\u9aa4\u3002", "result": "\u572815\u4e2a\u7269\u4f53\u4e0a\u8fdb\u884c\u4eff\u771f\u9a8c\u8bc1\uff0c\u5305\u62ec\u5df2\u77e5\u5f62\u72b6\u548c\u89c2\u5bdf\u5230\u7684\u5f62\u72b6\u6570\u636e\u96c6\uff0c\u5e76\u5728UR3e\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u5b9e\u7269\u6293\u53d6\u5b9e\u9a8c\u3002", "conclusion": "DISF\u5728\u4fdd\u6301\u51e0\u4f55\u517c\u5bb9\u6027\u7684\u540c\u65f6\u51cf\u5c11\u4e86\u8d28\u5fc3\u8bef\u5bf9\u9f50\uff0c\u63d0\u9ad8\u4e86\u4eff\u771f\u548c\u73b0\u5b9e\u4e16\u754c\u6267\u884c\u4e2d\u7684\u6293\u53d6\u6210\u529f\u7387\u3002"}}
{"id": "2512.24638", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24638", "abs": "https://arxiv.org/abs/2512.24638", "authors": ["Qingda Hu", "Ziheng Qiu", "Zijun Xu", "Kaizhao Zhang", "Xizhou Bu", "Zuolei Sun", "Bo Zhang", "Jieru Zhao", "Zhongxue Gan", "Wenchao Ding"], "title": "Resolving State Ambiguity in Robot Manipulation via Adaptive Working Memory Recoding", "comment": null, "summary": "State ambiguity is common in robotic manipulation. Identical observations may correspond to multiple valid behavior trajectories. The visuomotor policy must correctly extract the appropriate types and levels of information from the history to identify the current task phase. However, naively extending the history window is computationally expensive and may cause severe overfitting. Inspired by the continuous nature of human reasoning and the recoding of working memory, we introduce PAM, a novel visuomotor Policy equipped with Adaptive working Memory. With minimal additional training cost in a two-stage manner, PAM supports a 300-frame history window while maintaining high inference speed. Specifically, a hierarchical frame feature extractor yields two distinct representations for motion primitives and temporal disambiguation. For compact representation, a context router with range-specific queries is employed to produce compact context features across multiple history lengths. And an auxiliary objective of reconstructing historical information is introduced to ensure that the context router acts as an effective bottleneck. We meticulously design 7 tasks and verify that PAM can handle multiple scenarios of state ambiguity simultaneously. With a history window of approximately 10 seconds, PAM still supports stable training and maintains inference speeds above 20Hz. Project website: https://tinda24.github.io/pam/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565PAM\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5de5\u4f5c\u8bb0\u5fc6\u548c\u5206\u5c42\u6846\u67b6\u7279\u5f81\u63d0\u53d6\uff0c\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u72b6\u6001\u6a21\u7cca\u6027\u95ee\u9898\u3002", "motivation": "\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u5e38\u89c1\u7684\u72b6\u6001\u6a21\u7cca\u6027\u4f7f\u5f97\u76f8\u540c\u7684\u89c2\u5bdf\u7ed3\u679c\u53ef\u80fd\u5bf9\u5e94\u591a\u4e2a\u6709\u6548\u7684\u884c\u4e3a\u8f68\u8ff9\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u8fc7\u62df\u5408\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u7684\u8bad\u7ec3\u65b9\u5f0f\uff0c\u8bbe\u8ba1\u4e86\u5206\u5c42\u6846\u67b6\u7279\u5f81\u63d0\u53d6\u5668\u548c\u4e0a\u4e0b\u6587\u8def\u7531\u5668\uff0c\u4ee5\u652f\u6301300\u5e27\u7684\u5386\u53f2\u7a97\u53e3\u540c\u65f6\u7ef4\u6301\u9ad8\u63a8\u7406\u901f\u5ea6\u3002", "result": "\u5f15\u5165\u4e86PAM\uff0c\u4e00\u4e2a\u5177\u6709\u81ea\u9002\u5e94\u5de5\u4f5c\u8bb0\u5fc6\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\uff0c\u80fd\u591f\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u6709\u6548\u5904\u7406\u72b6\u6001\u6a21\u7cca\u6027\u3002", "conclusion": "PAM\u901a\u8fc7\u8bbe\u8ba1\u4e0a\u4e0b\u6587\u8def\u7531\u5668\u548c\u8f85\u52a9\u76ee\u6807\uff0c\u6709\u6548\u5730\u63d0\u53d6\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u8bad\u7ec3\u548c\u9ad8\u6548\u7684\u63a8\u7406\u901f\u5ea6\u3002"}}
{"id": "2512.24651", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.24651", "abs": "https://arxiv.org/abs/2512.24651", "authors": ["Yury Kolomeytsev", "Dmitry Golembiovsky"], "title": "Hybrid Motion Planning with Deep Reinforcement Learning for Mobile Robot Navigation", "comment": "22 pages, 4 figures", "summary": "Autonomous mobile robots operating in complex, dynamic environments face the dual challenge of navigating large-scale, structurally diverse spaces with static obstacles while safely interacting with various moving agents. Traditional graph-based planners excel at long-range pathfinding but lack reactivity, while Deep Reinforcement Learning (DRL) methods demonstrate strong collision avoidance but often fail to reach distant goals due to a lack of global context. We propose Hybrid Motion Planning with Deep Reinforcement Learning (HMP-DRL), a hybrid framework that bridges this gap. Our approach utilizes a graph-based global planner to generate a path, which is integrated into a local DRL policy via a sequence of checkpoints encoded in both the state space and reward function. To ensure social compliance, the local planner employs an entity-aware reward structure that dynamically adjusts safety margins and penalties based on the semantic type of surrounding agents. We validate the proposed method through extensive testing in a realistic simulation environment derived from real-world map data. Comprehensive experiments demonstrate that HMP-DRL consistently outperforms other methods, including state-of-the-art approaches, in terms of key metrics of robot navigation: success rate, collision rate, and time to reach the goal. Overall, these findings confirm that integrating long-term path guidance with semantically-aware local control significantly enhances both the safety and reliability of autonomous navigation in complex human-centric settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5HMP-DRL\uff0c\u901a\u8fc7\u56fe\u5f62\u7b97\u6cd5\u4e0e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\uff0c\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e2d\u4f18\u5316\u673a\u5668\u4eba\u5bfc\u822a\u3002", "motivation": "\u9762\u5bf9\u590d\u6742\u3001\u52a8\u6001\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u9700\u540c\u65f6\u5e94\u5bf9\u5927\u578b\u591a\u6837\u5316\u7a7a\u95f4\u7684\u5bfc\u822a\u4e0e\u4e0e\u79fb\u52a8\u4f53\u7684\u5b89\u5168\u4ea4\u4e92\u3002", "method": "HMP-DRL\u65b9\u6cd5\u5229\u7528\u56fe\u5f62\u7b97\u6cd5\u7684\u5168\u5c40\u89c4\u5212\u751f\u6210\u8def\u5f84\uff0c\u5e76\u901a\u8fc7\u7f16\u7801\u72b6\u6001\u7a7a\u95f4\u548c\u5956\u52b1\u51fd\u6570\u7684\u68c0\u67e5\u70b9\u5c06\u5176\u96c6\u6210\u5165\u5c40\u90e8DRL\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eHMP-DRL\u5728\u6210\u529f\u7387\u3001\u78b0\u649e\u7387\u548c\u5230\u8fbe\u76ee\u6807\u65f6\u95f4\u7b49\u5173\u952e\u6307\u6807\u4e0a\u4f18\u4e8e\u5176\u4ed6\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "HMP-DRL\u5728\u81ea\u4e3b\u5bfc\u822a\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u663e\u8457\u63d0\u5347\uff0c\u7ed3\u5408\u4e86\u957f\u8fdc\u8def\u5f84\u6307\u5bfc\u4e0e\u8bed\u4e49\u611f\u77e5\u7684\u5c40\u90e8\u63a7\u5236\u3002"}}
{"id": "2512.24653", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24653", "abs": "https://arxiv.org/abs/2512.24653", "authors": ["Chengkai Hou", "Kun Wu", "Jiaming Liu", "Zhengping Che", "Di Wu", "Fei Liao", "Guangrun Li", "Jingyang He", "Qiuxuan Feng", "Zhao Jin", "Chenyang Gu", "Zhuoyang Liu", "Nuowei Han", "Xiangju Mi", "Yaoxu Lv", "Yankai Fu", "Gaole Dai", "Langzhe Gu", "Tao Li", "Yuheng Zhang", "Yixue Zhang", "Xinhua Wang", "Shichao Fan", "Meng Li", "Zhen Zhao", "Ning Liu", "Zhiyuan Xu", "Pei Ren", "Junjie Ji", "Haonan Liu", "Kuan Cheng", "Shanghang Zhang", "Jian Tang"], "title": "RoboMIND 2.0: A Multimodal, Bimanual Mobile Manipulation Dataset for Generalizable Embodied Intelligence", "comment": null, "summary": "While data-driven imitation learning has revolutionized robotic manipulation, current approaches remain constrained by the scarcity of large-scale, diverse real-world demonstrations. Consequently, the ability of existing models to generalize across long-horizon bimanual tasks and mobile manipulation in unstructured environments remains limited. To bridge this gap, we present RoboMIND 2.0, a comprehensive real-world dataset comprising over 310K dual-arm manipulation trajectories collected across six distinct robot embodiments and 739 complex tasks. Crucially, to support research in contact-rich and spatially extended tasks, the dataset incorporates 12K tactile-enhanced episodes and 20K mobile manipulation trajectories. Complementing this physical data, we construct high-fidelity digital twins of our real-world environments, releasing an additional 20K-trajectory simulated dataset to facilitate robust sim-to-real transfer. To fully exploit the potential of RoboMIND 2.0, we propose MIND-2 system, a hierarchical dual-system frame-work optimized via offline reinforcement learning. MIND-2 integrates a high-level semantic planner (MIND-2-VLM) to decompose abstract natural language instructions into grounded subgoals, coupled with a low-level Vision-Language-Action executor (MIND-2-VLA), which generates precise, proprioception-aware motor actions.", "AI": {"tldr": "RoboMIND 2.0\u662f\u4e00\u4e2a\u5927\u578b\u6570\u636e\u96c6\uff0c\u65e8\u5728\u63d0\u9ad8\u673a\u5668\u4eba\u53cc\u81c2\u64cd\u4f5c\u548c\u79fb\u52a8\u64cd\u4f5c\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86MIND-2\u7cfb\u7edf\u4ee5\u4f18\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "motivation": "\u5f53\u524d\u6570\u636e\u9a71\u52a8\u7684\u6a21\u4eff\u5b66\u4e60\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u65b9\u9762\u53d7\u5230\u5b9e\u9645\u6f14\u793a\u532e\u4e4f\u7684\u9650\u5236\uff0c\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63a8\u51faRoboMIND 2.0\u6570\u636e\u96c6\uff0c\u5305\u542b310K\u4e2a\u53cc\u81c2\u64cd\u4f5c\u8f68\u8ff9\u548c\u9ad8\u4fdd\u771f\u6570\u5b57\u53cc\u80de\u80ce\u73af\u5883\uff0c\u7ed3\u5408MIND-2\u7cfb\u7edf\u8fdb\u884c\u4f18\u5316\u3002", "result": "RoboMIND 2.0\u6570\u636e\u96c6\u5305\u62ec\u591a\u79cd\u64cd\u4f5c\u8f68\u8ff9\uff0c\u5177\u6709\u8f83\u5f3a\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u540c\u65f6MIND-2\u7cfb\u7edf\u9ad8\u6548\u5730\u5c06\u6307\u4ee4\u8f6c\u5316\u4e3a\u5b9e\u9645\u52a8\u4f5c\u3002", "conclusion": "RoboMIND 2.0\u53caMIND-2\u7cfb\u7edf\u4e3a\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u57fa\u7840\uff0c\u4fc3\u8fdb\u4e86\u673a\u5668\u4eba\u5b66\u4e60\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2512.24657", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24657", "abs": "https://arxiv.org/abs/2512.24657", "authors": ["Sungjae Min", "Hyungjoo Kim", "David Hyunchul Shim"], "title": "Antagonistic Bowden-Cable Actuation of a Lightweight Robotic Hand: Toward Dexterous Manipulation for Payload Constrained Humanoids", "comment": "Preprint", "summary": "Humanoid robots toward human-level dexterity require robotic hands capable of simultaneously providing high grasping force, rapid actuation speeds, multiple degrees of freedom, and lightweight structures within human-like size constraints. Meeting these conflicting requirements remains challenging, as satisfying this combination typically necessitates heavier actuators and bulkier transmission systems, significantly restricting the payload capacity of robot arms. In this letter, we present a lightweight anthropomorphic hand actuated by Bowden cables, which uniquely combines rolling-contact joint optimization with antagonistic cable actuation, enabling single-motor-per-joint control with negligible cable-length deviation. By relocating the actuator module to the torso, the design substantially reduces distal mass while maintaining anthropomorphic scale and dexterity. Additionally, this antagonistic cable actuation eliminates the need for synchronization between motors. Using the proposed methods, the hand assembly with a distal mass of 236g (excluding remote actuators and Bowden sheaths) demonstrated reliable execution of dexterous tasks, exceeding 18N fingertip force and lifting payloads over one hundred times its own mass. Furthermore, robustness was validated through Cutkosky taxonomy grasps and trajectory consistency under perturbed actuator-hand transformations.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684Bowden\u7535\u7f06\u9a71\u52a8\u4eba\u5f62\u624b\uff0c\u901a\u8fc7\u4f18\u5316\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6293\u53d6\u80fd\u529b\uff0c\u6ee1\u8db3\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u7684\u591a\u91cd\u6027\u80fd\u9700\u6c42\u3002", "motivation": "\u5b9e\u73b0\u4e0e\u4eba\u7c7b\u624b\u76f8\u4f3c\u7684\u7075\u6d3b\u6027\u4e0e\u529b\u91cf\uff0c\u5bf9\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u5728\u5404\u79cd\u73af\u5883\u4e2d\u7684\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528Bowden\u7535\u7f06\u9a71\u52a8\u7684\u8f7b\u91cf\u578b\u4eba\u5f62\u624b\uff0c\u7ed3\u5408\u6eda\u52a8\u63a5\u89e6\u5173\u8282\u4f18\u5316\u4e0e\u5bf9\u6297\u6027\u7535\u7f06\u9a71\u52a8\uff0c\u5b9e\u73b0\u5355\u7535\u673a\u63a7\u5236\uff0c\u51cf\u5c11\u4e86\u7cfb\u7edf\u7684\u6574\u4f53\u8d28\u91cf\u3002", "result": "\u624b\u90e8\u7ec4\u4ef6\u5728\u4e0d\u4f7f\u7528\u8fdc\u7a0b\u9a71\u52a8\u5668\u548cBowden\u5916\u5957\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u4e86236\u514b\u7684\u8d28\u91cf\uff0c\u80fd\u591f\u5b8c\u6210\u8d85\u8fc718N\u7684\u6307\u5c16\u529b\u91cf\uff0c\u63d0\u5347\u7684\u8f7d\u8377\u8d85\u8fc7\u81ea\u8eab\u8d28\u91cf\u7684\u767e\u500d\uff0c\u663e\u793a\u51fa\u6781\u9ad8\u7684\u4efb\u52a1\u6267\u884c\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u8f7b\u91cf\u7ea7\u4eba\u5f62\u624b\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u6293\u63e1\u529b\u4e0e\u9ad8\u7075\u6d3b\u5ea6\uff0c\u5e76\u80fd\u591f\u5728\u4e0d\u589e\u52a0\u6574\u4f53\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\uff0c\u6267\u884c\u590d\u6742\u7684\u6293\u53d6\u4efb\u52a1\u3002"}}
{"id": "2512.24673", "categories": ["cs.RO", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.24673", "abs": "https://arxiv.org/abs/2512.24673", "authors": ["Yongsheng Zhao", "Lei Zhao", "Baoping Cheng", "Gongxin Yao", "Xuanzhang Wen", "Han Gao"], "title": "VLA-RAIL: A Real-Time Asynchronous Inference Linker for VLA Models and Robots", "comment": null, "summary": "Vision-Language-Action (VLA) models have achieved remarkable breakthroughs in robotics, with the action chunk playing a dominant role in these advances. Given the real-time and continuous nature of robotic motion control, the strategies for fusing a queue of successive action chunks have a profound impact on the overall performance of VLA models. Existing methods suffer from jitter, stalling, or even pauses in robotic action execution, which not only limits the achievable execution speed but also reduces the overall success rate of task completion. This paper introduces VLA-RAIL (A Real-Time Asynchronous Inference Linker), a novel framework designed to address these issues by conducting model inference and robot motion control asynchronously and guaranteeing smooth, continuous, and high-speed action execution. The core contributions of the paper are two fold: a Trajectory Smoother that effectively filters out the noise and jitter in the trajectory of one action chunk using polynomial fitting and a Chunk Fuser that seamlessly align the current executing trajectory and the newly arrived chunk, ensuring position, velocity, and acceleration continuity between two successive action chunks. We validate the effectiveness of VLA-RAIL on a benchmark of dynamic simulation tasks and several real-world manipulation tasks. Experimental results demonstrate that VLA-RAIL significantly reduces motion jitter, enhances execution speed, and improves task success rates, which will become a key infrastructure for the large-scale deployment of VLA models.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faVLA-RAIL\uff0c\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5f02\u6b65\u63a8\u7406\u548c\u8fd0\u52a8\u63a7\u5236\u89e3\u51b3\u8fd0\u52a8\u6296\u52a8\u548c\u901f\u5ea6\u95ee\u9898\uff0c\u63d0\u5347VLA\u6a21\u578b\u7684\u6574\u4f53\u6027\u80fd\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\u4e2d\uff0c\u5df2\u6709\u65b9\u6cd5\u5b58\u5728\u6296\u52a8\u3001\u505c\u987f\u7b49\u95ee\u9898\uff0c\u5f71\u54cd\u6267\u884c\u901f\u5ea6\u548c\u4efb\u52a1\u6210\u529f\u7387\u3002", "method": "\u63d0\u51fa\u4e86VLA-RAIL\u6846\u67b6\uff0c\u5176\u4e2d\u5305\u542b\u4e00\u4e2a\u91c7\u7528\u591a\u9879\u5f0f\u62df\u5408\u7684\u8f68\u8ff9\u5e73\u6ed1\u5668\u548c\u4e00\u4e2a\u5bf9\u9f50\u5f53\u524d\u6267\u884c\u8f68\u8ff9\u4e0e\u65b0\u5230\u8fbe\u52a8\u4f5c\u5757\u7684\u5757\u878d\u5408\u5668\u3002", "result": "VLA-RAIL\u5728\u52a8\u6001\u4eff\u771f\u4efb\u52a1\u548c\u5b9e\u9645\u64cd\u4f5c\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u8bc1\u660e\u5176\u80fd\u591f\u5b9e\u73b0\u5e73\u6ed1\u3001\u8fde\u7eed\u548c\u9ad8\u901f\u7684\u52a8\u4f5c\u6267\u884c\u3002", "conclusion": "VLA-RAIL\u663e\u8457\u51cf\u5c11\u4e86\u8fd0\u52a8\u6296\u52a8\uff0c\u63d0\u9ad8\u4e86\u6267\u884c\u901f\u5ea6\uff0c\u6539\u5584\u4e86\u4efb\u52a1\u6210\u529f\u7387\uff0c\u4e3aVLA\u6a21\u578b\u7684\u5927\u89c4\u6a21\u5e94\u7528\u5960\u5b9a\u4e86\u5173\u952e\u57fa\u7840\u3002"}}
{"id": "2512.24680", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24680", "abs": "https://arxiv.org/abs/2512.24680", "authors": ["Kangjie Zhou", "Zhaoyang Li", "Han Gao", "Yao Su", "Hangxin Liu", "Junzhi Yu", "Chang Liu"], "title": "ReSPIRe: Informative and Reusable Belief Tree Search for Robot Probabilistic Search and Tracking in Unknown Environments", "comment": "17 pages, 12 figures, accepted to IEEE Transactions on Systems, Man, and Cybernetics: Systems", "summary": "Target search and tracking (SAT) is a fundamental problem for various robotic applications such as search and rescue and environmental exploration. This paper proposes an informative trajectory planning approach, namely ReSPIRe, for SAT in unknown cluttered environments under considerably inaccurate prior target information and limited sensing field of view. We first develop a novel sigma point-based approximation approach to fast and accurately estimate mutual information reward under non-Gaussian belief distributions, utilizing informative sampling in state and observation spaces to mitigate the computational intractability of integral calculation. To tackle significant uncertainty associated with inadequate prior target information, we propose the hierarchical particle structure in ReSPIRe, which not only extracts critical particles for global route guidance, but also adjusts the particle number adaptively for planning efficiency. Building upon the hierarchical structure, we develop the reusable belief tree search approach to build a policy tree for online trajectory planning under uncertainty, which reuses rollout evaluation to improve planning efficiency. Extensive simulations and real-world experiments demonstrate that ReSPIRe outperforms representative benchmark methods with smaller MI approximation error, higher search efficiency, and more stable tracking performance, while maintaining outstanding computational efficiency.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faReSPIRe\uff0c\u4e00\u79cd\u9488\u5bf9\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u76ee\u6807\u641c\u7d22\u4e0e\u8ddf\u8e2a\u7684\u65b0\u9896\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u641c\u7d22\u6548\u7387\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u76ee\u6807\u641c\u7d22\u4e0e\u8ddf\u8e2a\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u4e0d\u51c6\u786e\u5148\u9a8c\u4fe1\u606f\u548c\u9650\u5236\u611f\u77e5\u89c6\u91ce\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u6b20\u4f73\u3002", "method": "\u8be5\u65b9\u6cd5\u4f7f\u7528\u4e86\u57fa\u4e8esigma\u70b9\u7684\u8fd1\u4f3c\u6280\u672f\u3001\u5206\u5c42\u7c92\u5b50\u7ed3\u6784\u548c\u53ef\u91cd\u7528\u4fe1\u5ff5\u6811\u641c\u7d22\u6765\u63d0\u9ad8\u89c4\u5212\u6548\u7387\u548c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "ReSPIRe\u65b9\u6cd5\u5728\u672a\u77e5\u6742\u4e71\u73af\u5883\u4e2d\u8fdb\u884c\u76ee\u6807\u641c\u7d22\u548c\u8ddf\u8e2a\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u4e0d\u51c6\u786e\u7684\u5148\u9a8c\u76ee\u6807\u4fe1\u606f\u548c\u6709\u9650\u7684\u611f\u77e5\u89c6\u91ce\u3002", "conclusion": "\u901a\u8fc7\u521b\u65b0\u7684\u65b9\u6cd5\uff0cReSPIRe\u5728\u76ee\u6807\u641c\u7d22\u548c\u8ddf\u8e2a\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6548\u7387\u548c\u66f4\u7a33\u5b9a\u7684\u8868\u73b0\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2512.24688", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24688", "abs": "https://arxiv.org/abs/2512.24688", "authors": ["Zhehan Li", "Zheng Wang", "Jiadong Lu", "Qi Liu", "Zhiren Xun", "Yue Wang", "Fei Gao", "Chao Xu", "Yanjun Cao"], "title": "CREPES-X: Hierarchical Bearing-Distance-Inertial Direct Cooperative Relative Pose Estimation System", "comment": "21 pages, 23 figures, journal", "summary": "Relative localization is critical for cooperation in autonomous multi-robot systems. Existing approaches either rely on shared environmental features or inertial assumptions or suffer from non-line-of-sight degradation and outliers in complex environments. Robust and efficient fusion of inter-robot measurements such as bearings, distances, and inertials for tens of robots remains challenging. We present CREPES-X (Cooperative RElative Pose Estimation System with multiple eXtended features), a hierarchical relative localization framework that enhances speed, accuracy, and robustness under challenging conditions, without requiring any global information. CREPES-X starts with a compact hardware design: InfraRed (IR) LEDs, an IR camera, an ultra-wideband module, and an IMU housed in a cube no larger than 6cm on each side. Then CREPES-X implements a two-stage hierarchical estimator to meet different requirements, considering speed, accuracy, and robustness. First, we propose a single-frame relative estimator that provides instant relative poses for multi-robot setups through a closed-form solution and robust bearing outlier rejection. Then a multi-frame relative estimator is designed to offer accurate and robust relative states by exploring IMU pre-integration via robocentric relative kinematics with loosely- and tightly-coupled optimization. Extensive simulations and real-world experiments validate the effectiveness of CREPES-X, showing robustness to up to 90% bearing outliers, proving resilience in challenging conditions, and achieving RMSE of 0.073m and 1.817\u00b0 in real-world datasets.", "AI": {"tldr": "CREPES-X\u662f\u4e00\u79cd\u65b0\u578b\u7684\u5c42\u6b21\u5316\u76f8\u5bf9\u5b9a\u4f4d\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u5347\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u901f\u5ea6\u3001\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e14\u4e0d\u4f9d\u8d56\u4e8e\u5168\u5c40\u4fe1\u606f\u3002", "motivation": "\u5728\u81ea\u4e3b\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\uff0c\u76f8\u5bf9\u5b9a\u4f4d\u5bf9\u4e8e\u5408\u4f5c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u5171\u4eab\u73af\u5883\u7279\u5f81\u3001\u60ef\u6027\u5047\u8bbe\u4ee5\u53ca\u5728\u590d\u6742\u73af\u5883\u4e2d\u906d\u9047\u7684\u975e\u89c6\u8ddd\u8870\u51cf\u548c\u5f02\u5e38\u503c\u7b49\u6311\u6218\u3002", "method": "CREPES-X\u91c7\u7528\u7d27\u51d1\u7684\u786c\u4ef6\u8bbe\u8ba1\u548c\u4e24\u9636\u6bb5\u7684\u5c42\u6b21\u4f30\u8ba1\u5668\uff0c\u9996\u5148\u901a\u8fc7\u5355\u5e27\u76f8\u5bf9\u4f30\u8ba1\u5668\u63d0\u4f9b\u5373\u65f6\u4f4d\u7f6e\uff0c\u5176\u6b21\u901a\u8fc7\u591a\u5e27\u76f8\u5bf9\u4f30\u8ba1\u5668\u7ed3\u5408IMU\u9884\u79ef\u5206\u6765\u4f18\u5316\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "result": "CREPES-X\u7ecf\u8fc7\u5e7f\u6cdb\u7684\u4eff\u771f\u548c\u5b9e\u5730\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5728\u9ad8\u8fbe90%\u7684\u5f02\u5e38\u503c\u4e0b\u4ecd\u80fd\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u5b9e\u73b0\u4e86\u771f\u5b9e\u6570\u636e\u96c6\u4e2d\u7684\u5747\u65b9\u6839\u8bef\u5dee\u4e3a0.073m\u548c\u89d2\u5ea6\u8bef\u5dee\u4e3a1.817\u00b0\u7684\u7ed3\u679c\u3002", "conclusion": "CREPES-X\u5728\u6ca1\u6709\u5168\u5c40\u4fe1\u606f\u7684\u590d\u6742\u6761\u4ef6\u4e0b\uff0c\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u5b9a\u4f4d\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5408\u4f5c\u4efb\u52a1\u3002"}}
{"id": "2512.24698", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24698", "abs": "https://arxiv.org/abs/2512.24698", "authors": ["Dongyun Kang", "Min-Gyu Kim", "Tae-Gyu Song", "Hajun Kim", "Sehoon Ha", "Hae-Won Park"], "title": "Dynamic Policy Learning for Legged Robot with Simplified Model Pretraining and Model Homotopy Transfer", "comment": "8 pages. Submitted to the IEEE for possible publication", "summary": "Generating dynamic motions for legged robots remains a challenging problem. While reinforcement learning has achieved notable success in various legged locomotion tasks, producing highly dynamic behaviors often requires extensive reward tuning or high-quality demonstrations. Leveraging reduced-order models can help mitigate these challenges. However, the model discrepancy poses a significant challenge when transferring policies to full-body dynamics environments. In this work, we introduce a continuation-based learning framework that combines simplified model pretraining and model homotopy transfer to efficiently generate and refine complex dynamic behaviors. First, we pretrain the policy using a single rigid body model to capture core motion patterns in a simplified environment. Next, we employ a continuation strategy to progressively transfer the policy to the full-body environment, minimizing performance loss. To define the continuation path, we introduce a model homotopy from the single rigid body model to the full-body model by gradually redistributing mass and inertia between the trunk and legs. The proposed method not only achieves faster convergence but also demonstrates superior stability during the transfer process compared to baseline methods. Our framework is validated on a range of dynamic tasks, including flips and wall-assisted maneuvers, and is successfully deployed on a real quadrupedal robot.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9884\u8bad\u7ec3\u548c\u6a21\u578b\u540c\u4f26\u8f6c\u79fb\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u9ad8\u6548\u751f\u6210\u56db\u8db3\u673a\u5668\u4eba\u7684\u52a8\u6001\u52a8\u4f5c\uff0c\u7ecf\u8fc7\u9a8c\u8bc1\u663e\u793a\u51fa\u4f18\u79c0\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u751f\u6210\u52a8\u6001\u52a8\u4f5c\u5bf9\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u6765\u8bf4\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u5f3a\u5316\u5b66\u4e60\u5728\u817f\u90e8\u8fd0\u52a8\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u751f\u6210\u9ad8\u5ea6\u52a8\u6001\u7684\u884c\u4e3a\u901a\u5e38\u9700\u8981\u5e7f\u6cdb\u7684\u5956\u52b1\u8c03\u6574\u6216\u9ad8\u8d28\u91cf\u7684\u6f14\u793a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6301\u7eed\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u7b80\u5316\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u548c\u6a21\u578b\u540c\u4f26\u8f6c\u79fb\uff0c\u4ee5\u6709\u6548\u751f\u6210\u548c\u7cbe\u70bc\u590d\u6742\u7684\u52a8\u6001\u884c\u4e3a\u3002\u9996\u5148\uff0c\u7528\u5355\u4e00\u521a\u4f53\u6a21\u578b\u5bf9\u7b56\u7565\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u4ee5\u6355\u6349\u7b80\u5316\u73af\u5883\u4e2d\u7684\u6838\u5fc3\u8fd0\u52a8\u6a21\u5f0f\u3002\u7136\u540e\uff0c\u91c7\u7528\u6301\u7eed\u7b56\u7565\u9010\u6b65\u5c06\u7b56\u7565\u8f6c\u79fb\u5230\u5168\u8eab\u73af\u5883\uff0c\u6700\u5c0f\u5316\u6027\u80fd\u635f\u5931\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u52a8\u6001\u4efb\u52a1\uff08\u5982\u7ffb\u8f6c\u548c\u5899\u8f85\u52a9\u64cd\u4f5c\uff09\u4e2d\u9a8c\u8bc1\uff0c\u4e14\u5728\u8f6c\u79fb\u8fc7\u7a0b\u4e2d\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5c55\u73b0\u51fa\u66f4\u5feb\u7684\u6536\u655b\u6027\u548c\u66f4\u597d\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5730\u5728\u5b9e\u9645\u7684\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u90e8\u7f72\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u52a8\u6001\u884c\u4e3a\u751f\u6210\u3002"}}
{"id": "2512.24712", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24712", "abs": "https://arxiv.org/abs/2512.24712", "authors": ["Qian Cheng", "Weitao Zhou", "Cheng Jing", "Nanshan Deng", "Junze Wen", "Zhaoyang Liu", "Kun Jiang", "Diange Yang"], "title": "LSRE: Latent Semantic Rule Encoding for Real-Time Semantic Risk Detection in Autonomous Driving", "comment": null, "summary": "Real-world autonomous driving must adhere to complex human social rules that extend beyond legally codified traffic regulations. Many of these semantic constraints, such as yielding to emergency vehicles, complying with traffic officers' gestures, or stopping for school buses, are intuitive for humans yet difficult to encode explicitly. Although large vision-language models (VLMs) can interpret such semantics, their inference cost makes them impractical for real-time deployment.This work proposes LSRE, a Latent Semantic Rule Encoding framework that converts sparsely sampled VLM judgments into decision boundaries within the latent space of a recurrent world model. By encoding language-defined safety semantics into a lightweight latent classifier, LSRE enables real-time semantic risk assessment at 10 Hz without per-frame VLM queries. Experiments on six semantic-failure scenarios in CARLA demonstrate that LSRE attains semantic risk detection accuracy comparable to a large VLM baseline, while providing substantially earlier hazard anticipation and maintaining low computational latency. LSRE further generalizes to rarely seen semantic-similar test cases, indicating that language-guided latent classification offers an effective and deployable mechanism for semantic safety monitoring in autonomous driving.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLSRE\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5728\u81ea\u4e3b\u9a7e\u9a76\u4e2d\u5b9e\u65f6\u8fdb\u884c\u8bed\u4e49\u98ce\u9669\u8bc4\u4f30\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u81ea\u4e3b\u9a7e\u9a76\u9700\u8981\u9075\u5faa\u590d\u6742\u7684\u4eba\u7c7b\u793e\u4f1a\u89c4\u5219\uff0c\u8fd9\u4e9b\u89c4\u5219\u8d85\u8d8a\u4e86\u6cd5\u5f8b\u6cd5\u89c4\uff0c\u4f20\u7edf\u7684\u65b9\u6cd5\u96be\u4ee5\u660e\u786e\u7f16\u7801\u8fd9\u4e9b\u8bed\u4e49\u7ea6\u675f\u3002", "method": "LSRE\u6846\u67b6\u5c06\u7a00\u758f\u91c7\u6837\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5224\u65ad\u8f6c\u6362\u4e3a\u9012\u5f52\u4e16\u754c\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u51b3\u7b56\u8fb9\u754c\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u7684\u6f5c\u5728\u5206\u7c7b\u5668\u8fdb\u884c\u5b9e\u65f6\u98ce\u9669\u8bc4\u4f30\u3002", "result": "\u5728CARLA\u73af\u5883\u4e0b\u8fdb\u884c\u7684\u5b9e\u9a8c\u663e\u793a\uff0cLSRE\u5728\u516d\u79cd\u8bed\u4e49\u6545\u969c\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u4e0e\u5927\u578bVLM\u57fa\u7ebf\u76f8\u5f53\u7684\u8bed\u4e49\u98ce\u9669\u68c0\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u524d\u9884\u6d4b\u5371\u9669\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u8ba1\u7b97\u5ef6\u8fdf\u3002", "conclusion": "LSRE\u4e3a\u81ea\u4e3b\u9a7e\u9a76\u4e2d\u7684\u8bed\u4e49\u5b89\u5168\u76d1\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u548c\u53ef\u90e8\u7f72\u7684\u673a\u5236\uff0c\u80fd\u591f\u5f88\u597d\u5730\u63a8\u5e7f\u5230\u7f55\u89c1\u7684\u8bed\u4e49\u76f8\u4f3c\u6d4b\u8bd5\u6848\u4f8b\u3002"}}
{"id": "2512.24740", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.24740", "abs": "https://arxiv.org/abs/2512.24740", "authors": ["Yichen Liu", "Kesava Viswanadha", "Zhongyu Li", "Nelson Lojo", "Kristofer S. J. Pister"], "title": "Control of Microrobots with Reinforcement Learning under On-Device Compute Constraints", "comment": "9 pages, 10 figures", "summary": "An important function of autonomous microrobots is the ability to perform robust movement over terrain. This paper explores an edge ML approach to microrobot locomotion, allowing for on-device, lower latency control under compute, memory, and power constraints. This paper explores the locomotion of a sub-centimeter quadrupedal microrobot via reinforcement learning (RL) and deploys the resulting controller on an ultra-small system-on-chip (SoC), SC$\u03bc$M-3C, featuring an ARM Cortex-M0 microcontroller running at 5 MHz. We train a compact FP32 multilayer perceptron (MLP) policy with two hidden layers ($[128, 64]$) in a massively parallel GPU simulation and enhance robustness by utilizing domain randomization over simulation parameters. We then study integer (Int8) quantization (per-tensor and per-feature) to allow for higher inference update rates on our resource-limited hardware, and we connect hardware power budgets to achievable update frequency via a cycles-per-update model for inference on our Cortex-M0. We propose a resource-aware gait scheduling viewpoint: given a device power budget, we can select the gait mode (trot/intermediate/gallop) that maximizes expected RL reward at a corresponding feasible update frequency. Finally, we deploy our MLP policy on a real-world large-scale robot on uneven terrain, qualitatively noting that domain-randomized training can improve out-of-distribution stability. We do not claim real-world large-robot empirical zero-shot transfer in this work.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u5728\u786c\u4ef6\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u63a7\u5236\u7b97\u6cd5\u4f18\u5316\u5fae\u578b\u56db\u8db3\u673a\u5668\u4eba\u7684\u8fd0\u52a8\uff0c\u63d0\u5347\u5176\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u7a33\u5b9a\u6027\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u786c\u4ef6\u9650\u5236\u4e0b\uff0c\u63d0\u9ad8\u5fae\u578b\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u63a7\u5236\u80fd\u529b\uff0c\u6ee1\u8db3\u4f4e\u5ef6\u8fdf\u548c\u4f4e\u80fd\u8017\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7d27\u51d1\u7684\u591a\u5c42\u611f\u77e5\u5668\u63a7\u5236\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u786c\u4ef6\u91cf\u5316\u548c\u9886\u57df\u968f\u673a\u5316\u63d0\u9ad8\u5176\u5728\u8d44\u6e90\u6709\u9650\u7684\u786c\u4ef6\u4e0a\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "\u6210\u529f\u5728\u8d44\u6e90\u53d7\u9650\u7684ARM Cortex-M0\u5fae\u63a7\u5236\u5668\u4e0a\u5b9e\u73b0\u4e86\u4f18\u5316\u7684MLP\u63a7\u5236\u7b56\u7565\uff0c\u5e76\u5728\u4e0d\u5e73\u5766\u5730\u5f62\u4e0a\u9a8c\u8bc1\u4e86\u5176\u7a33\u5b9a\u6027\u3002", "conclusion": "\u901a\u8fc7\u5bf9\u79f0\u8fd0\u52a8\u8c03\u5ea6\uff0c\u80fd\u5728\u6709\u9650\u7684\u786c\u4ef6\u8d44\u6e90\u4e0b\u6709\u6548\u63a7\u5236\u5fae\u578b\u56db\u8db3\u673a\u5668\u4eba\uff0c\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u5b9e\u73b0\u66f4\u7a33\u5b9a\u7684\u8f6c\u79fb\u5b66\u4e60\u3002"}}
{"id": "2512.24766", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.24766", "abs": "https://arxiv.org/abs/2512.24766", "authors": ["Karthik Dharmarajan", "Wenlong Huang", "Jiajun Wu", "Li Fei-Fei", "Ruohan Zhang"], "title": "Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow", "comment": "Project website: https://dream2flow.github.io/", "summary": "Generative video modeling has emerged as a compelling tool to zero-shot reason about plausible physical interactions for open-world manipulation. Yet, it remains a challenge to translate such human-led motions into the low-level actions demanded by robotic systems. We observe that given an initial image and task instruction, these models excel at synthesizing sensible object motions. Thus, we introduce Dream2Flow, a framework that bridges video generation and robotic control through 3D object flow as an intermediate representation. Our method reconstructs 3D object motions from generated videos and formulates manipulation as object trajectory tracking. By separating the state changes from the actuators that realize those changes, Dream2Flow overcomes the embodiment gap and enables zero-shot guidance from pre-trained video models to manipulate objects of diverse categories-including rigid, articulated, deformable, and granular. Through trajectory optimization or reinforcement learning, Dream2Flow converts reconstructed 3D object flow into executable low-level commands without task-specific demonstrations. Simulation and real-world experiments highlight 3D object flow as a general and scalable interface for adapting video generation models to open-world robotic manipulation. Videos and visualizations are available at https://dream2flow.github.io/.", "AI": {"tldr": "Dream2Flow\u662f\u4e00\u4e2a\u5c06\u89c6\u9891\u751f\u6210\u548c\u673a\u5668\u4eba\u63a7\u5236\u8fde\u63a5\u7684\u6846\u67b6\uff0c\u901a\u8fc73D\u7269\u4f53\u6d41\u5b9e\u73b0\u4f4e\u7ea7\u52a8\u4f5c\u7684\u751f\u6210\uff0c\u652f\u6301\u591a\u79cd\u7269\u4f53\u7684\u64cd\u63a7\u3002", "motivation": "\u89e3\u51b3\u751f\u6210\u6027\u89c6\u9891\u6a21\u578b\u4e0e\u673a\u5668\u4eba\u7684\u4f4e\u7ea7\u52a8\u4f5c\u4e4b\u95f4\u7684\u8f6c\u6362\u96be\u9898\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u5f00\u653e\u4e16\u754c\u64cd\u4f5c\u3002", "method": "\u901a\u8fc73D\u7269\u4f53\u6d41\u91cd\u6784\u7269\u4f53\u8fd0\u52a8\uff0c\u5e76\u5c06\u64cd\u4f5c\u8868\u8ff0\u4e3a\u7269\u4f53\u8f68\u8ff9\u8ddf\u8e2a\uff0c\u4ece\u800c\u5b9e\u73b0\u4ece\u751f\u6210\u89c6\u9891\u5230\u53ef\u6267\u884c\u4f4e\u7ea7\u547d\u4ee4\u7684\u8f6c\u5316\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6Dream2Flow\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5c06\u751f\u6210\u6027\u89c6\u9891\u6a21\u578b\u4e0e\u673a\u5668\u4eba\u63a7\u5236\u8fde\u63a5\u8d77\u6765\uff0c\u901a\u8fc73D\u7269\u4f53\u6d41\u7684\u4e2d\u4ecb\u8868\u793a\uff0c\u5b9e\u73b0\u5f00\u653e\u4e16\u754c\u4e0b\u7684\u7269\u4f53\u64cd\u4f5c\u3002", "conclusion": "Dream2Flow\u6210\u529f\u5730\u5c06\u4eba\u7c7b\u5f15\u5bfc\u7684\u52a8\u4f5c\u4e0e\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u4f4e\u7ea7\u52a8\u4f5c\u8fdb\u884c\u4e86\u6709\u6548\u8f6c\u6362\uff0c\u5c55\u73b0\u4e86\u5728\u6ca1\u6709\u4efb\u52a1\u7279\u5b9a\u6f14\u793a\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u5f00\u653e\u4e16\u754c\u64cd\u4f5c\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.24845", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24845", "abs": "https://arxiv.org/abs/2512.24845", "authors": ["Qiuyi Gu", "Yuze Sheng", "Jincheng Yu", "Jiahao Tang", "Xiaolong Shan", "Zhaoyang Shen", "Tinghao Yi", "Xiaodan Liang", "Xinlei Chen", "Yu Wang"], "title": "ArtiSG: Functional 3D Scene Graph Construction via Human-demonstrated Articulated Objects Manipulation", "comment": null, "summary": "3D scene graphs have empowered robots with semantic understanding for navigation and planning, yet they often lack the functional information required for physical manipulation, particularly regarding articulated objects. Existing approaches for inferring articulation mechanisms from static observations are prone to visual ambiguity, while methods that estimate parameters from state changes typically rely on constrained settings such as fixed cameras and unobstructed views. Furthermore, fine-grained functional elements like small handles are frequently missed by general object detectors. To bridge this gap, we present ArtiSG, a framework that constructs functional 3D scene graphs by encoding human demonstrations into structured robotic memory. Our approach leverages a robust articulation data collection pipeline utilizing a portable setup to accurately estimate 6-DoF articulation trajectories and axes even under camera ego-motion. We integrate these kinematic priors into a hierarchical and open-vocabulary graph while utilizing interaction data to discover inconspicuous functional elements missed by visual perception. Extensive real-world experiments demonstrate that ArtiSG significantly outperforms baselines in functional element recall and articulation estimation precision. Moreover, we show that the constructed graph serves as a reliable functional memory that effectively guides robots to perform language-directed manipulation tasks in real-world environments containing diverse articulated objects.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86ArtiSG\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4eba\u7c7b\u793a\u8303\u7f16\u7801\u4e3a\u7ed3\u6784\u5316\u7684\u673a\u5668\u4eba\u8bb0\u5fc6\uff0c\u6784\u5efa\u529f\u80fd\u60273D\u573a\u666f\u56fe\uff0c\u4ee5\u63d0\u5347\u673a\u5668\u4eba\u5bf9\u5173\u8282\u7269\u4f53\u7684\u64cd\u4f5c\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u76843D\u573a\u666f\u56fe\u7f3a\u4e4f\u5bf9\u7269\u7406\u64cd\u4f5c\u6240\u9700\u7684\u529f\u80fd\u4fe1\u606f\uff0c\u5c24\u5176\u662f\u5173\u8282\u7269\u4f53\u7684\u5904\u7406\u3002", "method": "\u6211\u4eec\u63d0\u51faArtiSG\u6846\u67b6\uff0c\u5229\u7528\u4fbf\u643a\u8bbe\u7f6e\u7684\u5f3a\u5927\u5173\u8282\u6570\u636e\u6536\u96c6\u7ba1\u9053\uff0c\u51c6\u786e\u4f30\u8ba1\u5173\u8282\u8f68\u8ff9\u548c\u8f74\u5fc3\uff0c\u5e76\u5c06\u8fd0\u52a8\u5b66\u5148\u9a8c\u6574\u5408\u5230\u5206\u5c42\u5f00\u53e3\u8bcd\u6c47\u56fe\u4e2d\uff0c\u540c\u65f6\u5229\u7528\u4ea4\u4e92\u6570\u636e\u53d1\u73b0\u88ab\u89c6\u89c9\u611f\u77e5\u9057\u6f0f\u7684\u529f\u80fd\u5143\u7d20\u3002", "result": "\u5927\u91cf\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8868\u660e\uff0cArtiSG\u5728\u529f\u80fd\u5143\u7d20\u53ec\u56de\u548c\u5173\u8282\u4f30\u8ba1\u7cbe\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "\u6784\u5efa\u7684\u56fe\u4f5c\u4e3a\u53ef\u9760\u7684\u529f\u80fd\u8bb0\u5fc6\uff0c\u6709\u6548\u6307\u5bfc\u673a\u5668\u4eba\u5728\u5305\u542b\u591a\u6837\u5316\u5173\u8282\u7269\u4f53\u7684\u73b0\u5b9e\u73af\u5883\u4e2d\u6267\u884c\u8bed\u8a00\u6307\u4ee4\u7684\u64cd\u4f5c\u4efb\u52a1\u3002"}}
{"id": "2512.24974", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.24974", "abs": "https://arxiv.org/abs/2512.24974", "authors": ["Yunxi Tang", "Tianqi Yang", "Jing Huang", "Xiangyu Chu", "Kwok Wai Samuel Au"], "title": "Hierarchical Deformation Planning and Neural Tracking for DLOs in Constrained Environments", "comment": null, "summary": "Deformable linear objects (DLOs) manipulation presents significant challenges due to DLOs' inherent high-dimensional state space and complex deformation dynamics. The wide-populated obstacles in realistic workspaces further complicate DLO manipulation, necessitating efficient deformation planning and robust deformation tracking. In this work, we propose a novel framework for DLO manipulation in constrained environments. This framework combines hierarchical deformation planning with neural tracking, ensuring reliable performance in both global deformation synthesis and local deformation tracking. Specifically, the deformation planner begins by generating a spatial path set that inherently satisfies the homotopic constraints associated with DLO keypoint paths. Next, a path-set-guided optimization method is applied to synthesize an optimal temporal deformation sequence for the DLO. In manipulation execution, a neural model predictive control approach, leveraging a data-driven deformation model, is designed to accurately track the planned DLO deformation sequence. The effectiveness of the proposed framework is validated in extensive constrained DLO manipulation tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\u7528\u4e8e\u53d7\u9650\u73af\u5883\u4e2d\u7684\u53d8\u5f62\u7ebf\u6027\u7269\u4f53(GLO)\u64cd\u4f5c\uff0c\u7ed3\u5408\u4e86\u5c42\u6b21\u5316\u53d8\u5f62\u89c4\u5212\u4e0e\u795e\u7ecf\u8ddf\u8e2a\uff0c\u89e3\u51b3\u4e86\u9ad8\u7ef4\u72b6\u6001\u7a7a\u95f4\u548c\u590d\u6742\u53d8\u5f62\u52a8\u6001\u5e26\u6765\u7684\u6311\u6218\u3002", "motivation": "\u53d8\u5f62\u7ebf\u6027\u7269\u4f53\u7684\u64cd\u4f5c\u9762\u4e34\u9ad8\u7ef4\u72b6\u6001\u7a7a\u95f4\u548c\u590d\u6742\u53d8\u5f62\u52a8\u6001\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u969c\u788d\u7269\u5bc6\u96c6\u7684\u73b0\u5b9e\u5de5\u4f5c\u73af\u5883\u4e2d\uff0c\u8feb\u5207\u9700\u8981\u9ad8\u6548\u7684\u53d8\u5f62\u89c4\u5212\u548c\u7a33\u5065\u7684\u53d8\u5f62\u8ddf\u8e2a\u65b9\u6cd5\u3002", "method": "\u6846\u67b6\u7ed3\u5408\u4e86\u5c42\u6b21\u5316\u53d8\u5f62\u89c4\u5212\u548c\u795e\u7ecf\u8ddf\u8e2a\u3002\u53d8\u5f62\u89c4\u5212\u5668\u751f\u6210\u6ee1\u8db3\u540c\u4f26\u7ea6\u675f\u7684\u7a7a\u95f4\u8def\u5f84\u96c6\uff0c\u5e76\u5e94\u7528\u8def\u5f84\u96c6\u5f15\u5bfc\u7684\u4f18\u5316\u65b9\u6cd5\u5408\u6210\u6700\u4f73\u65f6\u5e8f\u53d8\u5f62\u5e8f\u5217\u3002\u4f7f\u7528\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u53d8\u5f62\u6a21\u578b\u7684\u795e\u7ecf\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u65b9\u6cd5\u6765\u51c6\u786e\u8ddf\u8e2a\u8ba1\u5212\u7684DLO\u53d8\u5f62\u5e8f\u5217\u3002", "result": "\u901a\u8fc7\u5728\u591a\u79cd\u53d7\u9650DLO\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u6240\u63d0\u6846\u67b6\u5728\u53d8\u5f62\u5408\u6210\u548c\u8ddf\u8e2a\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u591a\u79cd\u53d7\u9650DLO\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5f97\u5230\u4e86\u6709\u6548\u9a8c\u8bc1\uff0c\u663e\u793a\u51fa\u5176\u5728\u5168\u5c40\u5f62\u53d8\u5408\u6210\u548c\u5c40\u90e8\u5f62\u53d8\u8ddf\u8e2a\u65b9\u9762\u7684\u53ef\u9760\u6027\u80fd\u3002"}}
{"id": "2512.25072", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.25072", "abs": "https://arxiv.org/abs/2512.25072", "authors": ["Haozhi Qi", "Yen-Jen Wang", "Toru Lin", "Brent Yi", "Yi Ma", "Koushil Sreenath", "Jitendra Malik"], "title": "Coordinated Humanoid Manipulation with Choice Policies", "comment": "Code and Website: https://choice-policy.github.io/", "summary": "Humanoid robots hold great promise for operating in human-centric environments, yet achieving robust whole-body coordination across the head, hands, and legs remains a major challenge. We present a system that combines a modular teleoperation interface with a scalable learning framework to address this problem. Our teleoperation design decomposes humanoid control into intuitive submodules, which include hand-eye coordination, grasp primitives, arm end-effector tracking, and locomotion. This modularity allows us to collect high-quality demonstrations efficiently. Building on this, we introduce Choice Policy, an imitation learning approach that generates multiple candidate actions and learns to score them. This architecture enables both fast inference and effective modeling of multimodal behaviors. We validate our approach on two real-world tasks: dishwasher loading and whole-body loco-manipulation for whiteboard wiping. Experiments show that Choice Policy significantly outperforms diffusion policies and standard behavior cloning. Furthermore, our results indicate that hand-eye coordination is critical for success in long-horizon tasks. Our work demonstrates a practical path toward scalable data collection and learning for coordinated humanoid manipulation in unstructured environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u9065\u64cd\u4f5c\u548c\u6a21\u4eff\u5b66\u4e60\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u7c7b\u4eba\u673a\u5668\u4eba\u7684\u6574\u4f53\u534f\u8c03\u80fd\u529b\u3002", "motivation": "\u5728\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u73af\u5883\u4e2d\uff0c\u7c7b\u4eba\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5b9e\u73b0\u5934\u90e8\u3001\u624b\u548c\u817f\u4e4b\u95f4\u7684\u6574\u4f53\u534f\u8c03\u4ecd\u7136\u662f\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u6a21\u5757\u5316\u9065\u64cd\u4f5c\u63a5\u53e3\u4e0e\u53ef\u6269\u5c55\u5b66\u4e60\u6846\u67b6\u7ed3\u5408\u7684\u7cfb\u7edf\uff0c\u5229\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\u5206\u89e3\u7c7b\u4eba\u63a7\u5236\u4e3a\u76f4\u89c2\u7684\u5b50\u6a21\u5757\uff0c\u5305\u62ec\u624b\u773c\u534f\u8c03\u3001\u6293\u53d6\u539f\u8bed\u3001\u81c2\u672b\u7aef\u6267\u884c\u5668\u8ddf\u8e2a\u548c locomotion\u3002\u5f15\u5165\u4e86Choice Policy\uff0c\u4e00\u79cd\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u751f\u6210\u591a\u4e2a\u5019\u9009\u52a8\u4f5c\u5e76\u5bf9\u5176\u8fdb\u884c\u8bc4\u5206\u3002", "result": "Choice Policy\u663e\u8457\u4f18\u4e8e\u6269\u6563\u7b56\u7565\u548c\u6807\u51c6\u884c\u4e3a\u514b\u9686\uff0c\u4e14\u624b\u773c\u534f\u8c03\u5bf9\u6210\u529f\u5b8c\u6210\u957f\u65f6\u95f4\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u6211\u4eec\u7684\u5de5\u4f5c\u5c55\u793a\u4e86\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\uff0c\u9762\u5411\u534f\u8c03\u7c7b\u4eba\u64cd\u4f5c\u7684\u6570\u636e\u6536\u96c6\u548c\u5b66\u4e60\u7684\u53ef\u884c\u8def\u5f84\u3002"}}
