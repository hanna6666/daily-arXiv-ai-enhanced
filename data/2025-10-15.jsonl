{"id": "2510.12090", "categories": ["cs.RO", "cs.ET"], "pdf": "https://arxiv.org/pdf/2510.12090", "abs": "https://arxiv.org/abs/2510.12090", "authors": ["Hakan Ceylan", "Edoardo Sinibaldi", "Sanjay Misra", "Pankaj J. Pasricha", "Dietmar W. Hutmacher"], "title": "Translating Milli/Microrobots with A Value-Centered Readiness Framework", "comment": null, "summary": "Untethered mobile milli/microrobots hold transformative potential for\ninterventional medicine by enabling more precise and entirely non-invasive\ndiagnosis and therapy. Realizing this promise requires bridging the gap between\ngroundbreaking laboratory demonstrations and successful clinical integration.\nDespite remarkable technical progress over the past two decades, most\nmillirobots and microrobots remain confined to laboratory proof-of-concept\ndemonstrations, with limited real-world feasibility. In this Review, we\nidentify key factors that slow translation from bench to bedside, focusing on\nthe disconnect between technical innovation and real-world application. We\nargue that the long-term impact and sustainability of the field depend on\naligning development with unmet medical needs, ensuring applied feasibility,\nand integrating seamlessly into existing clinical workflows, which are\nessential pillars for delivering meaningful patient outcomes. To support this\nshift, we introduce a strategic milli/microrobot Technology Readiness Level\nframework (mTRL), which maps system development from initial conceptualization\nto clinical adoption through clearly defined milestones and their associated\nstepwise activities. The mTRL model provides a structured gauge of\ntechnological maturity, a common language for cross-disciplinary collaboration\nand actionable guidance to accelerate translational development toward new,\nsafer and more efficient interventions."}
{"id": "2510.12101", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.12101", "abs": "https://arxiv.org/abs/2510.12101", "authors": ["Pengyu Yin", "Shenghai Yuan", "Haozhi Cao", "Xingyu Ji", "Ruofei Bai", "Siyu Chen", "Lihua Xie"], "title": "Gaussian Semantic Field for One-shot LiDAR Global Localization", "comment": null, "summary": "We present a one-shot LiDAR global localization algorithm featuring semantic\ndisambiguation ability based on a lightweight tri-layered scene graph. While\nlandmark semantic registration-based methods have shown promising performance\nimprovements in global localization compared with geometric-only methods,\nlandmarks can be repetitive and misleading for correspondence establishment. We\npropose to mitigate this problem by modeling semantic distributions with\ncontinuous functions learned from a population of Gaussian processes. Compared\nwith discrete semantic labels, the continuous functions capture finer-grained\ngeo-semantic information and also provide more detailed metric information for\ncorrespondence establishment. We insert this continuous function as the middle\nlayer between the object layer and the metric-semantic layer, forming a\ntri-layered 3D scene graph, serving as a light-weight yet performant backend\nfor one-shot localization. We term our global localization pipeline Outram-GSF\n(Gaussian semantic field) and conduct a wide range of experiments on publicly\navailable data sets, validating the superior performance against the current\nstate-of-the-art."}
{"id": "2510.12169", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.12169", "abs": "https://arxiv.org/abs/2510.12169", "authors": ["Akshay Naik", "William R. Norris", "Dustin Nottage", "Ahmet Soylemezoglu"], "title": "Hybrid Terrain-Aware Path Planning: Integrating VD--RRT\\(^{*}\\) Exploration and VD--D\\(^{*}\\) Lite Repair", "comment": null, "summary": "Autonomous ground vehicles operating off-road must plan curvature-feasible\npaths while accounting for spatially varying soil strength and slope hazards in\nreal time. We present a continuous state--cost metric that combines a Bekker\npressure--sinkage model with elevation-derived slope and attitude penalties.\nThe resulting terrain cost field is analytic, bounded, and monotonic in soil\nmodulus and slope, ensuring well-posed discretization and stable updates under\nsensor noise. This metric is evaluated on a lattice with exact steering\nprimitives: Dubins and Reeds--Shepp motions for differential drive and\ntime-parameterized bicycle arcs for Ackermann steering. Global exploration is\nperformed using Vehicle-Dynamics RRT\\(^{*}\\), while local repair is managed by\nVehicle-Dynamics D\\(^{*}\\) Lite, enabling millisecond-scale replanning without\nheuristic smoothing. By separating the terrain--vehicle model from the planner,\nthe framework provides a reusable basis for deterministic, sampling-based, or\nlearning-driven planning in deformable terrain. Hardware trials on an off-road\nplatform demonstrate real-time navigation across soft soil and slope\ntransitions, supporting reliable autonomy in unstructured environments."}
{"id": "2510.12206", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12206", "abs": "https://arxiv.org/abs/2510.12206", "authors": ["Pin-Lun Chen", "Chi-Hsi Kung", "Che-Han Chang", "Wei-Chen Chiu", "Yi-Ting Chen"], "title": "Controllable Collision Scenario Generation via Collision Pattern Prediction", "comment": "8 pages, 3 figures. Submitted to IEEE International Conference on\n  Robotics and Automation (ICRA) 2026", "summary": "Evaluating the safety of autonomous vehicles (AVs) requires diverse,\nsafety-critical scenarios, with collisions being especially important yet rare\nand unsafe to collect in the real world. Therefore, the community has been\nfocusing on generating safety-critical scenarios in simulation. However,\ncontrolling attributes such as collision type and time-to-accident (TTA)\nremains challenging. We introduce a new task called controllable collision\nscenario generation, where the goal is to produce trajectories that realize a\nuser-specified collision type and TTA, to investigate the feasibility of\nautomatically generating desired collision scenarios. To support this task, we\npresent COLLIDE, a large-scale collision scenario dataset constructed by\ntransforming real-world driving logs into diverse collisions, balanced across\nfive representative collision types and different TTA intervals. We propose a\nframework that predicts Collision Pattern, a compact and interpretable\nrepresentation that captures the spatial configuration of the ego and the\nadversarial vehicles at impact, before rolling out full adversarial\ntrajectories. Experiments show that our approach outperforms strong baselines\nin both collision rate and controllability. Furthermore, generated scenarios\nconsistently induce higher planner failure rates, revealing limitations of\nexisting planners. We demonstrate that these scenarios fine-tune planners for\nrobustness improvements, contributing to safer AV deployment in different\ncollision scenarios."}
{"id": "2510.11830", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.11830", "abs": "https://arxiv.org/abs/2510.11830", "authors": ["Yuyang Jiang", "Binzhu Xie", "Lina Xu", "Xiaokang Lei", "Shi Qiu", "Luwen Yu", "Pan Hui"], "title": "Generative Multi-Sensory Meditation: Exploring Immersive Depth and Activation in Virtual Reality", "comment": "Accepted in MM 2025", "summary": "Mindfulness meditation has seen increasing applications in diverse domains as\nan effective practice to improve mental health. However, the standardized\nframeworks adopted by most applications often fail to cater to users with\nvarious psychological states and health conditions. This limitation arises\nprimarily from the lack of personalization and adaptive content design. To\naddress this, we propose MindfulVerse, an AI-Generated Content (AIGC)-driven\napplication to create personalized and immersive mindfulness experiences. By\ndeveloping a novel agent, the system can dynamically adjust the meditation\ncontent based on the ideas of individual users. Furthermore, we conducted\nexploratory user studies and comparative evaluations to assess the application\nscenarios and performance of our novel generative meditation tool in VR\nenvironments. The results of this user study indicate that generative\nmeditation improves neural activation in self-regulation and shows a positive\nimpact on emotional regulation and participation. Our approach offers a\ngenerative meditation procedure that provides users with an application that\nbetter suits their preferences and states."}
{"id": "2510.12215", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12215", "abs": "https://arxiv.org/abs/2510.12215", "authors": ["Chanwoo Kim", "Jihwan Yoon", "Hyeonseong Kim", "Taemoon Jeong", "Changwoo Yoo", "Seungbeen Lee", "Soohwan Byeon", "Hoon Chung", "Matthew Pan", "Jean Oh", "Kyungjae Lee", "Sungjoon Choi"], "title": "Learning Social Navigation from Positive and Negative Demonstrations and Rule-Based Specifications", "comment": "For more videos, see https://chanwookim971024.github.io/PioneeR/", "summary": "Mobile robot navigation in dynamic human environments requires policies that\nbalance adaptability to diverse behaviors with compliance to safety\nconstraints. We hypothesize that integrating data-driven rewards with\nrule-based objectives enables navigation policies to achieve a more effective\nbalance of adaptability and safety. To this end, we develop a framework that\nlearns a density-based reward from positive and negative demonstrations and\naugments it with rule-based objectives for obstacle avoidance and goal\nreaching. A sampling-based lookahead controller produces supervisory actions\nthat are both safe and adaptive, which are subsequently distilled into a\ncompact student policy suitable for real-time operation with uncertainty\nestimates. Experiments in synthetic and elevator co-boarding simulations show\nconsistent gains in success rate and time efficiency over baselines, and\nreal-world demonstrations with human participants confirm the practicality of\ndeployment. A video illustrating this work can be found on our project page\nhttps://chanwookim971024.github.io/PioneeR/."}
{"id": "2510.11897", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.11897", "abs": "https://arxiv.org/abs/2510.11897", "authors": ["Sara Rosenthal", "Maeda Hanafi", "Yannis Katsis", "Lucian Popa", "Marina Danilevsky"], "title": "A Longitudinal Study on Different Annotator Feedback Loops in Complex RAG Tasks", "comment": "26 pages", "summary": "Grounding conversations in existing passages, known as Retrieval-Augmented\nGeneration (RAG), is an important aspect of Chat-Based Assistants powered by\nLarge Language Models (LLMs) to ensure they are faithful and don't provide\nmisinformation. Several benchmarks have been created to measure the performance\nof LLMs on this task. We present a longitudinal study comparing the feedback\nloop of an internal and external human annotator group for the complex\nannotation task of creating multi-turn RAG conversations for evaluating LLMs.\nWe analyze the conversations produced by both groups and provide results of a\nsurvey comparing their experiences. Our study highlights the advantages of each\nannotator population and the impact of the different feedback loops; a closer\nloop creates higher quality conversations with a decrease in quantity and\ndiversity. Further, we present guidance for how to best utilize two different\npopulation groups when performing annotation tasks, particularly when the task\nis complex."}
{"id": "2510.12276", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12276", "abs": "https://arxiv.org/abs/2510.12276", "authors": ["Fuhao Li", "Wenxuan Song", "Han Zhao", "Jingbo Wang", "Pengxiang Ding", "Donglin Wang", "Long Zeng", "Haoang Li"], "title": "Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model", "comment": null, "summary": "Vision-language-action (VLA) models have recently shown strong potential in\nenabling robots to follow language instructions and execute precise actions.\nHowever, most VLAs are built upon vision-language models pretrained solely on\n2D data, which lack accurate spatial awareness and hinder their ability to\noperate in the 3D physical world. Existing solutions attempt to incorporate\nexplicit 3D sensor inputs such as depth maps or point clouds, but these\napproaches face challenges due to sensor noise, hardware heterogeneity, and\nincomplete depth coverage in existing datasets. Alternative methods that\nestimate 3D cues from 2D images also suffer from the limited performance of\ndepth estimators.We propose Spatial Forcing (SF), a simple yet effective\nalignment strategy that implicitly forces VLA models to develop spatial\ncomprehension capabilities without relying on explicit 3D inputs or depth\nestimators. SF aligns intermediate visual embeddings of VLAs with geometric\nrepresentations produced by pretrained 3D foundation models. By enforcing\nalignment at intermediate layers, SF guides VLAs to encode richer spatial\nrepresentations that enhance action precision.Extensive experiments in\nsimulation and real-world environments demonstrate that SF achieves\nstate-of-the-art results, surpassing both 2D- and 3D-based VLAs. SF further\naccelerates training by up to 3.8x and improves data efficiency across diverse\nrobotic tasks. Project page is at https://spatial-forcing.github.io/"}
{"id": "2510.11912", "categories": ["cs.HC", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.11912", "abs": "https://arxiv.org/abs/2510.11912", "authors": ["Rifat Ara Proma", "Ghulam Jilani Quadri", "Paul Rosen"], "title": "Evaluating Line Chart Strategies for Mitigating Density of Temporal Data: The Impact on Trend, Prediction, and Decision-Making", "comment": null, "summary": "Overplotted line charts can obscure trends in temporal data and hinder\nprediction. We conduct a user study comparing three alternatives-aggregated,\ntrellis, and spiral line charts against standard line charts on tasks involving\ntrend identification, making predictions, and decision-making. We found\naggregated charts performed similarly to standard charts and support more\naccurate trend recognition and prediction; trellis and spiral charts generally\nlag. We also examined the impact on decision-making via a trust game. The\nresults showed similar trust in standard and aggregated charts, varied trust in\nspiral charts, and a lean toward distrust in trellis charts. These findings\nprovide guidance for practitioners choosing visualization strategies for dense\ntemporal data."}
{"id": "2510.12332", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12332", "abs": "https://arxiv.org/abs/2510.12332", "authors": ["Mohammadreza Kasaei", "Mostafa Ghobadi", "Mohsen Khadem"], "title": "Shape-Aware Whole-Body Control for Continuum Robots with Application in Endoluminal Surgical Robotics", "comment": null, "summary": "This paper presents a shape-aware whole-body control framework for\ntendon-driven continuum robots with direct application to endoluminal surgical\nnavigation. Endoluminal procedures, such as bronchoscopy, demand precise and\nsafe navigation through tortuous, patient-specific anatomy where conventional\ntip-only control often leads to wall contact, tissue trauma, or failure to\nreach distal targets. To address these challenges, our approach combines a\nphysics-informed backbone model with residual learning through an Augmented\nNeural ODE, enabling accurate shape estimation and efficient Jacobian\ncomputation. A sampling-based Model Predictive Path Integral (MPPI) controller\nleverages this representation to jointly optimize tip tracking, backbone\nconformance, and obstacle avoidance under actuation constraints. A task manager\nfurther enhances adaptability by allowing real-time adjustment of objectives,\nsuch as wall clearance or direct advancement, during tele-operation. Extensive\nsimulation studies demonstrate millimeter-level accuracy across diverse\nscenarios, including trajectory tracking, dynamic obstacle avoidance, and\nshape-constrained reaching. Real-robot experiments on a bronchoscopy phantom\nvalidate the framework, showing improved lumen-following accuracy, reduced wall\ncontacts, and enhanced adaptability compared to joystick-only navigation and\nexisting baselines. These results highlight the potential of the proposed\nframework to increase safety, reliability, and operator efficiency in minimally\ninvasive endoluminal surgery, with broader applicability to other confined and\nsafety-critical environments."}
{"id": "2510.11927", "categories": ["cs.HC", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.11927", "abs": "https://arxiv.org/abs/2510.11927", "authors": ["Rifat Ara Proma", "Michael Correll", "Ghulam Jilani Quadri", "Paul Rosen"], "title": "Visual Stenography: Feature Recreation and Preservation in Sketches of Noisy Line Charts", "comment": null, "summary": "Line charts surface many features in time series data, from trends to\nperiodicity to peaks and valleys. However, not every potentially important\nfeature in the data may correspond to a visual feature which readers can detect\nor prioritize. In this study, we conducted a visual stenography task, where\nparticipants re-drew line charts to solicit information about the visual\nfeatures they believed to be important. We systematically varied noise levels\n(SNR ~5-30 dB) across line charts to observe how visual clutter influences\nwhich features people prioritize in their sketches. We identified three key\nstrategies that correlated with the noise present in the stimuli: the\nReplicator attempted to retain all major features of the line chart including\nnoise; the Trend Keeper prioritized trends disregarding periodicity and peaks;\nand the De-noiser filtered out noise while preserving other features. Further,\nwe found that participants tended to faithfully retain trends and peaks and\nvalleys when these features were present, while periodicity and noise were\nrepresented in more qualitative or gestural ways: semantically rather than\naccurately. These results suggest a need to consider more flexible and\nhuman-centric ways of presenting, summarizing, pre-processing, or clustering\ntime series data."}
{"id": "2510.12340", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12340", "abs": "https://arxiv.org/abs/2510.12340", "authors": ["Nicky Mol", "Luka Peternel", "Alessandro Ianniello", "Denis Zatyagov", "Auke Nachenius", "Stephan Balvert", "J. Micah Prendergast", "Sara Muscolo", "Olger Siebinga", "Eva Verhoef", "Deborah Forster", "David A. Abbink"], "title": "Achieving Meaningful Collaboration: Worker-centered Design of a Physical Human-Robot Collaborative Blending Task", "comment": "3 pages, 1 figure, ICRA@40 (Extended abstract)", "summary": "The use of robots in industrial settings continues to grow, driven by the\nneed to address complex societal challenges such as labor shortages, aging\npopulations, and ever-increasing production demands. In this abstract, we\nadvocate for (and demonstrate) a transdisciplinary approach when considering\nrobotics in the workplace. Transdisciplinarity emphasizes the integration of\nacademic research with pragmatic expertise and embodied experiential knowledge,\nthat prioritize values such as worker wellbeing and job attractiveness. In the\nfollowing, we describe an ongoing multi-pronged effort to explore the potential\nof collaborative robots in the context of airplane engine repair and\nmaintenance operations."}
{"id": "2510.11941", "categories": ["cs.HC", "H.5.2; J.6"], "pdf": "https://arxiv.org/pdf/2510.11941", "abs": "https://arxiv.org/abs/2510.11941", "authors": ["Rebecca Lin", "Michal Lukáč", "Mackenzie Leake"], "title": "Refashion: Reconfigurable Garments via Modular Design", "comment": "18 pages, 28 figures", "summary": "While bodies change over time and trends vary, most store-bought clothing\ncomes in fixed sizes and styles and fails to adapt to these changes.\nAlterations can enable small changes to otherwise static garments, but these\nchanges often require sewing and are non-reversible. We propose a modular\napproach to garment design that considers resizing, restyling, and reuse\nearlier in the design process. Our contributions include a compact set of\nmodules and connectors that form the building blocks of modular garments, a\nmethod to decompose a garment into modules via integer linear programming, and\na digital design tool that supports modular garment design and simulation. Our\nuser evaluation suggests that our approach to modular design can support the\ncreation of a wide range of garments and can help users transform them across\nsizes and styles while reusing the same building blocks."}
{"id": "2510.12346", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12346", "abs": "https://arxiv.org/abs/2510.12346", "authors": ["Bingquan Li", "Ning Wang", "Tianwei Zhang", "Zhicheng He", "Yucong Wu"], "title": "PolygMap: A Perceptive Locomotion Framework for Humanoid Robot Stair Climbing", "comment": null, "summary": "Recently, biped robot walking technology has been significantly developed,\nmainly in the context of a bland walking scheme. To emulate human walking,\nrobots need to step on the positions they see in unknown spaces accurately. In\nthis paper, we present PolyMap, a perception-based locomotion planning\nframework for humanoid robots to climb stairs. Our core idea is to build a\nreal-time polygonal staircase plane semantic map, followed by a footstep planar\nusing these polygonal plane segments. These plane segmentation and visual\nodometry are done by multi-sensor fusion(LiDAR, RGB-D camera and IMUs). The\nproposed framework is deployed on a NVIDIA Orin, which performs 20-30 Hz\nwhole-body motion planning output. Both indoor and outdoor real-scene\nexperiments indicate that our method is efficient and robust for humanoid robot\nstair climbing."}
{"id": "2510.11954", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.11954", "abs": "https://arxiv.org/abs/2510.11954", "authors": ["Sam Yu-Te Lee", "Jingya Chen", "Albert Calzaretto", "Richard Lee", "Alice Ferng", "Mihaela Vorvoreanu"], "title": "VizCopilot: Fostering Appropriate Reliance on Enterprise Chatbots with Context Visualization", "comment": null, "summary": "Enterprise chatbots show promise in supporting knowledge workers in\ninformation synthesis tasks by retrieving context from large, heterogeneous\ndatabases before generating answers. However, when the retrieved context\nmisaligns with user intentions, the chatbot often produces \"irrelevantly right\"\nresponses that provide little value. In this work, we introduce VizCopilot, a\nprototype that incorporates visualization techniques to actively involve\nend-users in context alignment. By combining topic modeling with document\nvisualization, VizCopilot enables human oversight and modification of retrieved\ncontext while keeping cognitive overhead manageable. We used VizCopilot as a\ndesign probe in a Research-through-Design study to evaluate the role of\nvisualization in context alignment and to surface future design opportunities.\nOur findings show that visualization not only helps users detect and correct\nmisaligned context but also encourages them to adapt their prompting\nstrategies, enabling the system to retrieve more relevant context from the\noutset. At the same time, the study reveals limitations in verification support\nregarding close-reading and trust in AI summaries. We outline future directions\nfor visualization-enhanced chatbots, focusing on personalization, proactivity,\nand sustainable human-AI collaboration."}
{"id": "2510.12363", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12363", "abs": "https://arxiv.org/abs/2510.12363", "authors": ["Jiale Fan", "Andrei Cramariuc", "Tifanny Portela", "Marco Hutter"], "title": "Pretraining in Actor-Critic Reinforcement Learning for Robot Motion Control", "comment": "Submitted to ICLR 2026", "summary": "The pretraining-finetuning paradigm has facilitated numerous transformative\nadvancements in artificial intelligence research in recent years. However, in\nthe domain of reinforcement learning (RL) for robot motion control, individual\nskills are often learned from scratch despite the high likelihood that some\ngeneralizable knowledge is shared across all task-specific policies belonging\nto a single robot embodiment. This work aims to define a paradigm for\npretraining neural network models that encapsulate such knowledge and can\nsubsequently serve as a basis for warm-starting the RL process in classic\nactor-critic algorithms, such as Proximal Policy Optimization (PPO). We begin\nwith a task-agnostic exploration-based data collection algorithm to gather\ndiverse, dynamic transition data, which is then used to train a Proprioceptive\nInverse Dynamics Model (PIDM) through supervised learning. The pretrained\nweights are loaded into both the actor and critic networks to warm-start the\npolicy optimization of actual tasks. We systematically validated our proposed\nmethod on seven distinct robot motion control tasks, showing significant\nbenefits to this initialization strategy. Our proposed approach on average\nimproves sample efficiency by 40.1% and task performance by 7.5%, compared to\nrandom initialization. We further present key ablation studies and empirical\nanalyses that shed light on the mechanisms behind the effectiveness of our\nmethod."}
{"id": "2510.11999", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.11999", "abs": "https://arxiv.org/abs/2510.11999", "authors": ["Skyler Oakeson", "David H. Smith IV", "Jaxton Winder", "Seth Poulsen"], "title": "Choose Your Own Solution: Supporting Optional Blocks in Block Ordering Problems", "comment": null, "summary": "This paper extends the functionality of block ordering problems (such as\nParsons problems and Proof Blocks) to include optional blocks. We detail the\nalgorithms used to implement the optional block feature and present usage\nexperiences from instructors who have integrated it into their curriculum. The\noptional blocks feature enables instructors to create more complex Parsons\nproblems with multiple correct solutions utilizing omitted or optional blocks.\nThis affords students a method to engage with questions that have several valid\nsolutions composed of different answer components. Instructors can specify\nblocks with multiple mutually exclusive dependencies, which we represent using\na multigraph structure. This multigraph is then collapsed into multiple\ndirected acyclic graphs (DAGs), allowing us to reuse existing algorithms for\ngrading block ordering problems represented as a DAG. We present potential use\ncases for this feature across various domains, including helping students learn\nGit workflows, shell command sequences, mathematical proofs, and Python\nprogramming concepts."}
{"id": "2510.12370", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12370", "abs": "https://arxiv.org/abs/2510.12370", "authors": ["Wenli Shi", "Clemence Grislain", "Olivier Sigaud", "Mohamed Chetouani"], "title": "Controlling Intent Expressiveness in Robot Motion with Diffusion Models", "comment": "Using diffusion models trained on quality diversity datasets for\n  generating robot motions with adjustable legibility levels", "summary": "Legibility of robot motion is critical in human-robot interaction, as it\nallows humans to quickly infer a robot's intended goal. Although traditional\ntrajectory generation methods typically prioritize efficiency, they often fail\nto make the robot's intentions clear to humans. Meanwhile, existing approaches\nto legible motion usually produce only a single \"most legible\" trajectory,\noverlooking the need to modulate intent expressiveness in different contexts.\nIn this work, we propose a novel motion generation framework that enables\ncontrollable legibility across the full spectrum, from highly legible to highly\nambiguous motions. We introduce a modeling approach based on an Information\nPotential Field to assign continuous legibility scores to trajectories, and\nbuild upon it with a two-stage diffusion framework that first generates paths\nat specified legibility levels and then translates them into executable robot\nactions. Experiments in both 2D and 3D reaching tasks demonstrate that our\napproach produces diverse and controllable motions with varying degrees of\nlegibility, while achieving performance comparable to SOTA. Code and project\npage: https://legibility-modulator.github.io."}
{"id": "2510.12081", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.12081", "abs": "https://arxiv.org/abs/2510.12081", "authors": ["Anna Fang", "Jiayang Shi", "Hriday Chhabria", "Bosi Li", "Haiyi Zhu"], "title": "Social Simulation for Integrating Self-Care: Measuring the Effects of Contextual Environments in Augmented Reality for Mental Health Practice", "comment": null, "summary": "Despite growing interest in virtual and augmented reality (VR/AR) for mental\nwell-being, prior work using immersive interventions to teach mental health\nskills has largely focused on calming or abstract settings. As a result, little\nis known about how realistic social simulation may better support the transfer\nand application of skills to in-person environments. In this work, we present a\n14-day user study with 43-participants comparing an augmented reality\nintervention simulating a realistic contextual environment against a matched\nnon-contextual control, applied to the public speaking context. We found that\nparticipants who practice mental health skills in the contextual environment\nshowed significantly greater likelihood to apply self-care techniques and\ngreater physiological stress reduction when using skills in mock in-person\ntasks. Overall, our work provides empirical evidence for the effects of\nrealistic stressor simulation, and offers design implications for mental health\ntechnology that supports effective transfer of skills to the real-world."}
{"id": "2510.12392", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12392", "abs": "https://arxiv.org/abs/2510.12392", "authors": ["Junhyuk So", "Chiwoong Lee", "Shinyoung Lee", "Jungseul Ok", "Eunhyeok Park"], "title": "Improving Generative Behavior Cloning via Self-Guidance and Adaptive Chunking", "comment": "Accepted at NeurIPS25", "summary": "Generative Behavior Cloning (GBC) is a simple yet effective framework for\nrobot learning, particularly in multi-task settings. Recent GBC methods often\nemploy diffusion policies with open-loop (OL) control, where actions are\ngenerated via a diffusion process and executed in multi-step chunks without\nreplanning. While this approach has demonstrated strong success rates and\ngeneralization, its inherent stochasticity can result in erroneous action\nsampling, occasionally leading to unexpected task failures. Moreover, OL\ncontrol suffers from delayed responses, which can degrade performance in noisy\nor dynamic environments. To address these limitations, we propose two novel\ntechniques to enhance the consistency and reactivity of diffusion policies: (1)\nself-guidance, which improves action fidelity by leveraging past observations\nand implicitly promoting future-aware behavior; and (2) adaptive chunking,\nwhich selectively updates action sequences when the benefits of reactivity\noutweigh the need for temporal consistency. Extensive experiments show that our\napproach substantially improves GBC performance across a wide range of\nsimulated and real-world robotic manipulation tasks. Our code is available at\nhttps://github.com/junhyukso/SGAC"}
{"id": "2510.12113", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.12113", "abs": "https://arxiv.org/abs/2510.12113", "authors": ["Sangho Suh", "Rahul Hingorani", "Bryan Wang", "Tovi Grossman"], "title": "KnowledgeTrail: Generative Timeline for Exploration and Sensemaking of Historical Events and Knowledge Formation", "comment": null, "summary": "The landscape of interactive systems is shifting toward dynamic, generative\nexperiences that empower users to explore and construct knowledge in real time.\nYet, timelines -- a fundamental tool for representing historical and conceptual\ndevelopment -- remain largely static, limiting user agency and curiosity. We\nintroduce the concept of a generative timeline: an AI-powered timeline that\nadapts to users' evolving questions by expanding or contracting in response to\ninput. We instantiate this concept through KnowledgeTrail, a system that\nenables users to co-construct timelines of historical events and knowledge\nformation processes. Two user studies showed that KnowledgeTrail fosters\ncuriosity-driven exploration, serendipitous discovery, and the ability to trace\ncomplex relationships between ideas and events, while citation features\nsupported verification yet revealed fragile trust shaped by perceptions of\nsource credibility. We contribute a vision for generative timelines as a new\nclass of exploratory interface, along with design insights for balancing\nserendipity and credibility."}
{"id": "2510.12403", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12403", "abs": "https://arxiv.org/abs/2510.12403", "authors": ["Francesco Capuano", "Caroline Pascal", "Adil Zouitine", "Thomas Wolf", "Michel Aractingi"], "title": "Robot Learning: A Tutorial", "comment": "Tutorial on Robot Learning using LeRobot, the end-to-end robot\n  learning library developed by Hugging Face", "summary": "Robot learning is at an inflection point, driven by rapid advancements in\nmachine learning and the growing availability of large-scale robotics data.\nThis shift from classical, model-based methods to data-driven, learning-based\nparadigms is unlocking unprecedented capabilities in autonomous systems. This\ntutorial navigates the landscape of modern robot learning, charting a course\nfrom the foundational principles of Reinforcement Learning and Behavioral\nCloning to generalist, language-conditioned models capable of operating across\ndiverse tasks and even robot embodiments. This work is intended as a guide for\nresearchers and practitioners, and our goal is to equip the reader with the\nconceptual understanding and practical tools necessary to contribute to\ndevelopments in robot learning, with ready-to-use examples implemented in\n$\\texttt{lerobot}$."}
{"id": "2510.12146", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.12146", "abs": "https://arxiv.org/abs/2510.12146", "authors": ["Muhammad Talha", "Abdullah Mohiuddin", "Sehrish Javed", "Ahmed Jawad Qureshi"], "title": "Lowering Barriers to CAD Adoption: A Comparative Study of Augmented Reality-Based CAD (AR-CAD) and a Traditional CAD tool", "comment": "Accepted at ASME IDETC/CIE 2025 Conference", "summary": "The paper presents a comparative user study between an Augmented\nReality-based Computer-Aided Design (AR-CAD) system and a traditional\ncomputer-based CAD modeling software, SolidWorks. Twenty participants of\nvarying skill levels performed 3D modeling tasks using both systems. The\nresults showed that while the average task completion time is comparable for\nboth groups, novice designers had a higher completion rate in AR-CAD than in\nthe traditional CAD interface, and experienced designers had a similar\ncompletion rate in both systems. A statistical comparison of task completion\nrate, time, and NASA Task Load Index (TLX) showed that AR-CAD slightly reduced\ncognitive load while favoring a high task completion rate. Higher scores on the\nSystem Usability Scale (SUS) by novices indicated that AR-CAD was superior and\nworthwhile for reducing barriers to entering CAD. In contrast, the Traditional\nCAD interface was favored by experienced users for its advanced capabilities,\nwhile many viewed AR-CAD as a valid means for rapid concept development,\neducation, and an initial critique of designs. This opens up the need for\nfuture research on the needed refinement of AR-CAD with a focus on\nhigh-precision input tools and its evaluation of complex design processes. This\nresearch highlights the potential for immersive interfaces to enhance design\npractice, bridging the gap between novice and experienced CAD users."}
{"id": "2510.12419", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12419", "abs": "https://arxiv.org/abs/2510.12419", "authors": ["Shunnosuke Yoshimura", "Kento Kawaharazuka", "Kei Okada"], "title": "M3D-skin: Multi-material 3D-printed Tactile Sensor with Hierarchical Infill Structures for Pressure Sensing", "comment": "Accepted to IROS2025, Website:\n  https://ssk-yoshimura.github.io/M3D-skin/", "summary": "Tactile sensors have a wide range of applications, from utilization in\nrobotic grippers to human motion measurement. If tactile sensors could be\nfabricated and integrated more easily, their applicability would further\nexpand. In this study, we propose a tactile sensor-M3D-skin-that can be easily\nfabricated with high versatility by leveraging the infill patterns of a\nmulti-material fused deposition modeling (FDM) 3D printer as the sensing\nprinciple. This method employs conductive and non-conductive flexible filaments\nto create a hierarchical structure with a specific infill pattern. The flexible\nhierarchical structure deforms under pressure, leading to a change in\nelectrical resistance, enabling the acquisition of tactile information. We\nmeasure the changes in characteristics of the proposed tactile sensor caused by\nmodifications to the hierarchical structure. Additionally, we demonstrate the\nfabrication and use of a multi-tile sensor. Furthermore, as applications, we\nimplement motion pattern measurement on the sole of a foot, integration with a\nrobotic hand, and tactile-based robotic operations. Through these experiments,\nwe validate the effectiveness of the proposed tactile sensor."}
{"id": "2510.12156", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.12156", "abs": "https://arxiv.org/abs/2510.12156", "authors": ["Hyemi Song", "Matthew Johnson", "Kirsten Whitley", "Eric Krokos", "Amitabh Varshney"], "title": "Embodied Natural Language Interaction (NLI): Speech Input Patterns in Immersive Analytics", "comment": null, "summary": "Embodiment shapes how users verbally express intent when interacting with\ndata through speech interfaces in immersive analytics. Despite growing interest\nin Natural Language Interaction (NLI) for visual analytics in immersive\nenvironments, users' speech patterns and their use of embodiment cues in speech\nremain underexplored. Understanding their interplay is crucial to bridging the\ngap between users' intent and an immersive analytic system. To address this, we\nreport the results from 15 participants in a user study conducted using the\nWizard of Oz method. We performed axial coding on 1,280 speech acts derived\nfrom 734 utterances, examining how analysis tasks are carried out with\nembodiment and linguistic features. Next, we measured speech input uncertainty\nfor each analysis task using the semantic entropy of utterances, estimating how\nuncertain users' speech inputs appear to an analytic system. Through these\nanalyses, we identified five speech input patterns, showing that users\ndynamically blend embodied and non-embodied speech acts depending on data\nanalysis tasks, phases, and embodiment reliance driven by the counts and types\nof embodiment cues in each utterance. We then examined how these patterns align\nwith user reflections on factors that challenge speech interaction during the\nstudy. Finally, we propose design implications aligned with the five patterns."}
{"id": "2510.12477", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12477", "abs": "https://arxiv.org/abs/2510.12477", "authors": ["Gaoyuan Liu", "Joris de Winter", "Kelly Merckaert", "Denis Steckelmacher", "Ann Nowe", "Bram Vanderborght"], "title": "A Task-Efficient Reinforcement Learning Task-Motion Planner for Safe Human-Robot Cooperation", "comment": null, "summary": "In a Human-Robot Cooperation (HRC) environment, safety and efficiency are the\ntwo core properties to evaluate robot performance. However, safety mechanisms\nusually hinder task efficiency since human intervention will cause backup\nmotions and goal failures of the robot. Frequent motion replanning will\nincrease the computational load and the chance of failure. In this paper, we\npresent a hybrid Reinforcement Learning (RL) planning framework which is\ncomprised of an interactive motion planner and a RL task planner. The RL task\nplanner attempts to choose statistically safe and efficient task sequences\nbased on the feedback from the motion planner, while the motion planner keeps\nthe task execution process collision-free by detecting human arm motions and\ndeploying new paths when the previous path is not valid anymore. Intuitively,\nthe RL agent will learn to avoid dangerous tasks, while the motion planner\nensures that the chosen tasks are safe. The proposed framework is validated on\nthe cobot in both simulation and the real world, we compare the planner with\nhard-coded task motion planning methods. The results show that our planning\nframework can 1) react to uncertain human motions at both joint and task\nlevels; 2) reduce the times of repeating failed goal commands; 3) reduce the\ntotal number of replanning requests."}
{"id": "2510.12268", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.12268", "abs": "https://arxiv.org/abs/2510.12268", "authors": ["Jeanne Choi", "Dasom Choi", "Sejun Jeong", "Hwajung Hong", "Joseph Seering"], "title": "How Far I'll Go: Imagining Futures of Conversational AI with People with Visual Impairments Through Design Fiction", "comment": null, "summary": "People with visual impairments (PVI) use a variety of assistive technologies\nto navigate their daily lives, and conversational AI (CAI) tools are a growing\npart of this toolset. Much existing HCI research has focused on the technical\ncapabilities of current CAI tools, but in this paper, we instead examine how\nPVI themselves envision potential futures for living with CAI. We conducted a\nstudy with 14 participants with visual impairments using an audio-based Design\nFiction probe featuring speculative dialogues between participants and a future\nCAI. Participants imagined using CAI to expand their boundaries by exploring\nnew opportunities or places, but also voiced concerns about balancing reliance\non CAI with maintaining autonomy, the need to consider diverse levels of\nvision-loss, and enhancing visibility of PVI for greater inclusion. We discuss\nimplications for designing CAI that support genuine agency for PVI based on the\nfuture lives they envisioned."}
{"id": "2510.12483", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.12483", "abs": "https://arxiv.org/abs/2510.12483", "authors": ["Jingkai Jia", "Tong Yang", "Xueyao Chen", "Chenhuan Liu", "Wenqiang Zhang"], "title": "Fast Visuomotor Policy for Robotic Manipulation", "comment": null, "summary": "We present a fast and effective policy framework for robotic manipulation,\nnamed Energy Policy, designed for high-frequency robotic tasks and\nresource-constrained systems. Unlike existing robotic policies, Energy Policy\nnatively predicts multimodal actions in a single forward pass, enabling\nhigh-precision manipulation at high speed. The framework is built upon two core\ncomponents. First, we adopt the energy score as the learning objective to\nfacilitate multimodal action modeling. Second, we introduce an energy MLP to\nimplement the proposed objective while keeping the architecture simple and\nefficient. We conduct comprehensive experiments in both simulated environments\nand real-world robotic tasks to evaluate the effectiveness of Energy Policy.\nThe results show that Energy Policy matches or surpasses the performance of\nstate-of-the-art manipulation methods while significantly reducing\ncomputational overhead. Notably, on the MimicGen benchmark, Energy Policy\nachieves superior performance with at a faster inference compared to existing\napproaches."}
{"id": "2510.12386", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.12386", "abs": "https://arxiv.org/abs/2510.12386", "authors": ["Vaishali Dhanoa", "Gabriela Molina León", "Eve Hoggan", "Eduard Gröller", "Marc Streit", "Niklas Elmqvist"], "title": "Hey Dashboard!: Supporting Voice, Text, and Pointing Modalities in Dashboard Onboarding", "comment": null, "summary": "Visualization dashboards are regularly used for data exploration and\nanalysis, but their complex interactions and interlinked views often require\ntime-consuming onboarding sessions from dashboard authors. Preparing these\nonboarding materials is labor-intensive and requires manual updates when\ndashboards change. Recent advances in multimodal interaction powered by large\nlanguage models (LLMs) provide ways to support self-guided onboarding. We\npresent DIANA (Dashboard Interactive Assistant for Navigation and Analysis), a\nmultimodal dashboard assistant that helps users for navigation and guided\nanalysis through chat, audio, and mouse-based interactions. Users can choose\nany interaction modality or a combination of them to onboard themselves on the\ndashboard. Each modality highlights relevant dashboard features to support user\norientation. Unlike typical LLM systems that rely solely on text-based chat,\nDIANA combines multiple modalities to provide explanations directly in the\ndashboard interface. We conducted a qualitative user study to understand the\nuse of different modalities for different types of onboarding tasks and their\ncomplexities."}
{"id": "2510.12509", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12509", "abs": "https://arxiv.org/abs/2510.12509", "authors": ["Gaoyuan Liu", "Bas Boom", "Naftali Slob", "Yuri Durodié", "Ann Nowé", "Bram Vanderborght"], "title": "Automated Behavior Planning for Fruit Tree Pruning via Redundant Robot Manipulators: Addressing the Behavior Planning Challenge", "comment": null, "summary": "Pruning is an essential agricultural practice for orchards. Proper pruning\ncan promote healthier growth and optimize fruit production throughout the\norchard's lifespan. Robot manipulators have been developed as an automated\nsolution for this repetitive task, which typically requires seasonal labor with\nspecialized skills. While previous research has primarily focused on the\nchallenges of perception, the complexities of manipulation are often\noverlooked. These challenges involve planning and control in both joint and\nCartesian spaces to guide the end-effector through intricate, obstructive\nbranches. Our work addresses the behavior planning challenge for a robotic\npruning system, which entails a multi-level planning problem in environments\nwith complex collisions. In this paper, we formulate the planning problem for a\nhigh-dimensional robotic arm in a pruning scenario, investigate the system's\nintrinsic redundancies, and propose a comprehensive pruning workflow that\nintegrates perception, modeling, and holistic planning. In our experiments, we\ndemonstrate that more comprehensive planning methods can significantly enhance\nthe performance of the robotic manipulator. Finally, we implement the proposed\nworkflow on a real-world robot. As a result, this work complements previous\nefforts on robotic pruning and motivates future research and development in\nplanning for pruning applications."}
{"id": "2510.12590", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.12590", "abs": "https://arxiv.org/abs/2510.12590", "authors": ["Shi-Ting Ni", "Kairong Fang", "Yuyang Wang", "Pan Hui"], "title": "Gauging the Competition: Understanding Social Comparison and Anxiety through Eye-tracking in Virtual Reality Group Interview", "comment": null, "summary": "Virtual Reality (VR) is a promising tool for interview training, yet the\npsychological dynamics of group interviews, such as social comparison, remain\nunderexplored. We investigate this phenomenon by developing an immersive VR\ngroup interview system and conducting an eye-tracking study with 73\nparticipants. We manipulated peer performance using ambiguous behavioral cues\n(e.g., hand-raising) and objective information (public test scores) to measure\ntheir effect on participants' attention and self-concept. Our results\ndemonstrate a \"Big-Fish-Little-Pond Effect\" in VR: an increase in\nhigh-achieving peer behaviors heightened participants' processing of social\ncomparison information and significantly lowered their self-assessments. The\nintroduction of objective scores further intensified these comparative\nbehaviors. We also found that lower perceived realism of the VR environment\ncorrelated with higher anxiety. These findings offer key insights and design\nconsiderations for creating more effective and psychologically-aware virtual\ntraining environments that account for complex social dynamics."}
{"id": "2510.12528", "categories": ["cs.RO", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2510.12528", "abs": "https://arxiv.org/abs/2510.12528", "authors": ["Muxing Huang", "Zibin Chen", "Weiliang Xu", "Zilan Li", "Yuanzhi Zhou", "Guoyuan Zhou", "Wenjing Chen", "Xinming Li"], "title": "Two-stream network-driven vision-based tactile sensor for object feature extraction and fusion perception", "comment": null, "summary": "Tactile perception is crucial for embodied intelligent robots to recognize\nobjects. Vision-based tactile sensors extract object physical attributes\nmultidimensionally using high spatial resolution; however, this process\ngenerates abundant redundant information. Furthermore, single-dimensional\nextraction, lacking effective fusion, fails to fully characterize object\nattributes. These challenges hinder the improvement of recognition accuracy. To\naddress this issue, this study introduces a two-stream network feature\nextraction and fusion perception strategy for vision-based tactile systems.\nThis strategy employs a distributed approach to extract internal and external\nobject features. It obtains depth map information through three-dimensional\nreconstruction while simultaneously acquiring hardness information by measuring\ncontact force data. After extracting features with a convolutional neural\nnetwork (CNN), weighted fusion is applied to create a more informative and\neffective feature representation. In standard tests on objects of varying\nshapes and hardness, the force prediction error is 0.06 N (within a 12 N\nrange). Hardness recognition accuracy reaches 98.0%, and shape recognition\naccuracy reaches 93.75%. With fusion algorithms, object recognition accuracy in\nactual grasping scenarios exceeds 98.5%. Focused on object physical attributes\nperception, this method enhances the artificial tactile system ability to\ntransition from perception to cognition, enabling its use in embodied\nperception applications."}
{"id": "2510.12692", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12692", "abs": "https://arxiv.org/abs/2510.12692", "authors": ["Sarina Xi", "Orelia Pi", "Miaomiao Zhang", "Becca Xiong", "Jacqueline Ng Lane", "Nihar B. Shah"], "title": "Who is a Better Matchmaker? Human vs. Algorithmic Judge Assignment in a High-Stakes Startup Competition", "comment": "17 Pages, 2 figures", "summary": "There is growing interest in applying artificial intelligence (AI) to\nautomate and support complex decision-making tasks. However, it remains unclear\nhow algorithms compare to human judgment in contexts requiring semantic\nunderstanding and domain expertise. We examine this in the context of the judge\nassignment problem, matching submissions to suitably qualified judges.\nSpecifically, we tackled this problem at the Harvard President's Innovation\nChallenge, the university's premier venture competition awarding over \\$500,000\nto student and alumni startups. This represents a real-world environment where\nhigh-quality judge assignment is essential. We developed an AI-based\njudge-assignment algorithm, Hybrid Lexical-Semantic Similarity Ensemble (HLSE),\nand deployed it at the competition. We then evaluated its performance against\nhuman expert assignments using blinded match-quality scores from judges on\n$309$ judge-venture pairs. Using a Mann-Whitney U statistic based test, we\nfound no statistically significant difference in assignment quality between the\ntwo approaches ($AUC=0.48, p=0.40$); on average, algorithmic matches are rated\n$3.90$ and manual matches $3.94$ on a 5-point scale, where 5 indicates an\nexcellent match. Furthermore, manual assignments that previously required a\nfull week could be automated in several hours by the algorithm during\ndeployment. These results demonstrate that HLSE achieves human-expert-level\nmatching quality while offering greater scalability and efficiency,\nunderscoring the potential of AI-driven solutions to support and enhance human\ndecision-making for judge assignment in high-stakes settings."}
{"id": "2510.12611", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.12611", "abs": "https://arxiv.org/abs/2510.12611", "authors": ["Lukas Pries", "Markus Ryll"], "title": "Learning Robust Agile Flight Control with Stability Guarantees", "comment": null, "summary": "In the evolving landscape of high-speed agile quadrotor flight, achieving\nprecise trajectory tracking at the platform's operational limits is paramount.\nControllers must handle actuator constraints, exhibit robustness to\ndisturbances, and remain computationally efficient for safety-critical\napplications. In this work, we present a novel neural-augmented feedback\ncontroller for agile flight control. The controller addresses individual\nlimitations of existing state-of-the-art control paradigms and unifies their\nstrengths. We demonstrate the controller's capabilities, including the accurate\ntracking of highly aggressive trajectories that surpass the feasibility of the\nactuators. Notably, the controller provides universal stability guarantees,\nenhancing its robustness and tracking performance even in exceedingly\ndisturbance-prone settings. Its nonlinear feedback structure is highly\nefficient enabling fast computation at high update rates. Moreover, the\nlearning process in simulation is both fast and stable, and the controller's\ninherent robustness allows direct deployment to real-world platforms without\nthe need for training augmentations or fine-tuning."}
{"id": "2510.12728", "categories": ["cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12728", "abs": "https://arxiv.org/abs/2510.12728", "authors": ["Minjae Lee", "Minsuk Kahng"], "title": "Data-Model Co-Evolution: Growing Test Sets to Refine LLM Behavior", "comment": null, "summary": "A long-standing challenge in machine learning has been the rigid separation\nbetween data work and model refinement, enforced by slow fine-tuning cycles.\nThe rise of Large Language Models (LLMs) overcomes this historical barrier,\nallowing applications developers to instantly govern model behavior by editing\nprompt instructions. This shift enables a new paradigm: data-model\nco-evolution, where a living test set and a model's instructions evolve in\ntandem. We operationalize this paradigm in an interactive system designed to\naddress the critical challenge of encoding subtle, domain-specific policies\ninto prompt instructions. The system's structured workflow guides people to\ndiscover edge cases, articulate rationales for desired behavior, and\niteratively evaluate instruction revisions against a growing test set. A user\nstudy shows our workflow helps participants refine instructions systematically\nand specify ambiguous policies more concretely. This work points toward more\nrobust and responsible LLM applications through human-in-the-loop development\naligned with local preferences and policies."}
{"id": "2510.12630", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12630", "abs": "https://arxiv.org/abs/2510.12630", "authors": ["Ajith Anil Meera", "Abian Torres", "Pablo Lanillos"], "title": "Designing Tools with Control Confidence", "comment": null, "summary": "Prehistoric humans invented stone tools for specialized tasks by not just\nmaximizing the tool's immediate goal-completion accuracy, but also increasing\ntheir confidence in the tool for later use under similar settings. This factor\ncontributed to the increased robustness of the tool, i.e., the least\nperformance deviations under environmental uncertainties. However, the current\nautonomous tool design frameworks solely rely on performance optimization,\nwithout considering the agent's confidence in tool use for repeated use. Here,\nwe take a step towards filling this gap by i) defining an optimization\nframework for task-conditioned autonomous hand tool design for robots, where\nii) we introduce a neuro-inspired control confidence term into the optimization\nroutine that helps the agent to design tools with higher robustness. Through\nrigorous simulations using a robotic arm, we show that tools designed with\ncontrol confidence as the objective function are more robust to environmental\nuncertainties during tool use than a pure accuracy-driven objective. We further\nshow that adding control confidence to the objective function for tool design\nprovides a balance between the robustness and goal accuracy of the designed\ntools under control perturbations. Finally, we show that our CMAES-based\nevolutionary optimization strategy for autonomous tool design outperforms other\nstate-of-the-art optimizers by designing the optimal tool within the fewest\niterations. Code: https://github.com/ajitham123/Tool_design_control_confidence."}
{"id": "2510.12662", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12662", "abs": "https://arxiv.org/abs/2510.12662", "authors": ["Oz Gitelson", "Satya Prakash Nayak", "Ritam Raha", "Anne-Kathrin Schmuck"], "title": "Maximal Adaptation, Minimal Guidance: Permissive Reactive Robot Task Planning with Humans in the Loop", "comment": null, "summary": "We present a novel framework for human-robot \\emph{logical} interaction that\nenables robots to reliably satisfy (infinite horizon) temporal logic tasks\nwhile effectively collaborating with humans who pursue independent and unknown\ntasks. The framework combines two key capabilities: (i) \\emph{maximal\nadaptation} enables the robot to adjust its strategy \\emph{online} to exploit\nhuman behavior for cooperation whenever possible, and (ii) \\emph{minimal\ntunable feedback} enables the robot to request cooperation by the human online\nonly when necessary to guarantee progress. This balance minimizes human-robot\ninterference, preserves human autonomy, and ensures persistent robot task\nsatisfaction even under conflicting human goals. We validate the approach in a\nreal-world block-manipulation task with a Franka Emika Panda robotic arm and in\nthe Overcooked-AI benchmark, demonstrating that our method produces rich,\n\\emph{emergent} cooperative behaviors beyond the reach of existing approaches,\nwhile maintaining strong formal guarantees."}
{"id": "2510.12684", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.12684", "abs": "https://arxiv.org/abs/2510.12684", "authors": ["Alvaro Belmonte-Baeza", "Miguel Cazorla", "Gabriel J. García", "Carlos J. Pérez-Del-Pulgar", "Jorge Pomares"], "title": "Autonomous Legged Mobile Manipulation for Lunar Surface Operations via Constrained Reinforcement Learning", "comment": "This is the authors version of the paper accepted for publication in\n  The IEEE International Conference on Space Robotics 2025. The final version\n  link will be added here after conference proceedings are published", "summary": "Robotics plays a pivotal role in planetary science and exploration, where\nautonomous and reliable systems are crucial due to the risks and challenges\ninherent to space environments. The establishment of permanent lunar bases\ndemands robotic platforms capable of navigating and manipulating in the harsh\nlunar terrain. While wheeled rovers have been the mainstay for planetary\nexploration, their limitations in unstructured and steep terrains motivate the\nadoption of legged robots, which offer superior mobility and adaptability. This\npaper introduces a constrained reinforcement learning framework designed for\nautonomous quadrupedal mobile manipulators operating in lunar environments. The\nproposed framework integrates whole-body locomotion and manipulation\ncapabilities while explicitly addressing critical safety constraints, including\ncollision avoidance, dynamic stability, and power efficiency, in order to\nensure robust performance under lunar-specific conditions, such as reduced\ngravity and irregular terrain. Experimental results demonstrate the framework's\neffectiveness in achieving precise 6D task-space end-effector pose tracking,\nachieving an average positional accuracy of 4 cm and orientation accuracy of\n8.1 degrees. The system consistently respects both soft and hard constraints,\nexhibiting adaptive behaviors optimized for lunar gravity conditions. This work\neffectively bridges adaptive learning with essential mission-critical safety\nrequirements, paving the way for advanced autonomous robotic explorers for\nfuture lunar missions."}
{"id": "2510.12710", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12710", "abs": "https://arxiv.org/abs/2510.12710", "authors": ["Baicheng Li", "Dong Wu", "Zike Yan", "Xinchen Liu", "Zecui Zeng", "Lusong Li", "Hongbin Zha"], "title": "Reflection-Based Task Adaptation for Self-Improving VLA", "comment": null, "summary": "Pre-trained Vision-Language-Action (VLA) models represent a major leap\ntowards general-purpose robots, yet efficiently adapting them to novel,\nspecific tasks in-situ remains a significant hurdle. While reinforcement\nlearning (RL) is a promising avenue for such adaptation, the process often\nsuffers from low efficiency, hindering rapid task mastery. We introduce\nReflective Self-Adaptation, a framework for rapid, autonomous task adaptation\nwithout human intervention. Our framework establishes a self-improving loop\nwhere the agent learns from its own experience to enhance both strategy and\nexecution.\n  The core of our framework is a dual-pathway architecture that addresses the\nfull adaptation lifecycle. First, a Failure-Driven Reflective RL pathway\nenables rapid learning by using the VLM's causal reasoning to automatically\nsynthesize a targeted, dense reward function from failure analysis. This\nprovides a focused learning signal that significantly accelerates policy\nexploration. However, optimizing such proxy rewards introduces a potential risk\nof \"reward hacking,\" where the agent masters the reward function but fails the\nactual task. To counteract this, our second pathway, Success-Driven\nQuality-Guided SFT, grounds the policy in holistic success. It identifies and\nselectively imitates high-quality successful trajectories, ensuring the agent\nremains aligned with the ultimate task goal. This pathway is strengthened by a\nconditional curriculum mechanism to aid initial exploration.\n  We conduct experiments in challenging manipulation tasks. The results\ndemonstrate that our framework achieves faster convergence and higher final\nsuccess rates compared to representative baselines. Our work presents a robust\nsolution for creating self-improving agents that can efficiently and reliably\nadapt to new environments."}
{"id": "2510.12717", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12717", "abs": "https://arxiv.org/abs/2510.12717", "authors": ["Se Hwan Jeon", "Ho Jae Lee", "Seungwoo Hong", "Sangbae Kim"], "title": "Residual MPC: Blending Reinforcement Learning with GPU-Parallelized Model Predictive Control", "comment": "TRO submission preprint", "summary": "Model Predictive Control (MPC) provides interpretable, tunable locomotion\ncontrollers grounded in physical models, but its robustness depends on frequent\nreplanning and is limited by model mismatch and real-time computational\nconstraints. Reinforcement Learning (RL), by contrast, can produce highly\nrobust behaviors through stochastic training but often lacks interpretability,\nsuffers from out-of-distribution failures, and requires intensive reward\nengineering. This work presents a GPU-parallelized residual architecture that\ntightly integrates MPC and RL by blending their outputs at the torque-control\nlevel. We develop a kinodynamic whole-body MPC formulation evaluated across\nthousands of agents in parallel at 100 Hz for RL training. The residual policy\nlearns to make targeted corrections to the MPC outputs, combining the\ninterpretability and constraint handling of model-based control with the\nadaptability of RL. The model-based control prior acts as a strong bias,\ninitializing and guiding the policy towards desirable behavior with a simple\nset of rewards. Compared to standalone MPC or end-to-end RL, our approach\nachieves higher sample efficiency, converges to greater asymptotic rewards,\nexpands the range of trackable velocity commands, and enables zero-shot\nadaptation to unseen gaits and uneven terrain."}
{"id": "2510.12724", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12724", "abs": "https://arxiv.org/abs/2510.12724", "authors": ["Xin Fei", "Zhixuan Xu", "Huaicong Fang", "Tianrui Zhang", "Lin Shao"], "title": "T(R,O) Grasp: Efficient Graph Diffusion of Robot-Object Spatial Transformation for Cross-Embodiment Dexterous Grasping", "comment": "12 pages, 14 figures", "summary": "Dexterous grasping remains a central challenge in robotics due to the\ncomplexity of its high-dimensional state and action space. We introduce T(R,O)\nGrasp, a diffusion-based framework that efficiently generates accurate and\ndiverse grasps across multiple robotic hands. At its core is the T(R,O) Graph,\na unified representation that models spatial transformations between robotic\nhands and objects while encoding their geometric properties. A graph diffusion\nmodel, coupled with an efficient inverse kinematics solver, supports both\nunconditioned and conditioned grasp synthesis. Extensive experiments on a\ndiverse set of dexterous hands show that T(R,O) Grasp achieves average success\nrate of 94.83%, inference speed of 0.21s, and throughput of 41 grasps per\nsecond on an NVIDIA A100 40GB GPU, substantially outperforming existing\nbaselines. In addition, our approach is robust and generalizable across\nembodiments while significantly reducing memory consumption. More importantly,\nthe high inference speed enables closed-loop dexterous manipulation,\nunderscoring the potential of T(R,O) Grasp to scale into a foundation model for\ndexterous grasping."}
{"id": "2510.12733", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12733", "abs": "https://arxiv.org/abs/2510.12733", "authors": ["Hang Yu", "Julian Jordan", "Julian Schmidt", "Silvan Lindner", "Alessandro Canevaro", "Wilhelm Stork"], "title": "HYPE: Hybrid Planning with Ego Proposal-Conditioned Predictions", "comment": null, "summary": "Safe and interpretable motion planning in complex urban environments needs to\nreason about bidirectional multi-agent interactions. This reasoning requires to\nestimate the costs of potential ego driving maneuvers. Many existing planners\ngenerate initial trajectories with sampling-based methods and refine them by\noptimizing on learned predictions of future environment states, which requires\na cost function that encodes the desired vehicle behavior. Designing such a\ncost function can be very challenging, especially if a wide range of complex\nurban scenarios has to be considered. We propose HYPE: HYbrid Planning with Ego\nproposal-conditioned predictions, a planner that integrates multimodal\ntrajectory proposals from a learned proposal model as heuristic priors into a\nMonte Carlo Tree Search (MCTS) refinement. To model bidirectional interactions,\nwe introduce an ego-conditioned occupancy prediction model, enabling\nconsistent, scene-aware reasoning. Our design significantly simplifies cost\nfunction design in refinement by considering proposal-driven guidance,\nrequiring only minimalistic grid-based cost terms. Evaluations on large-scale\nreal-world benchmarks nuPlan and DeepUrban show that HYPE effectively achieves\nstate-of-the-art performance, especially in safety and adaptability."}
