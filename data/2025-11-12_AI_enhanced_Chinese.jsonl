{"id": "2511.07619", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.07619", "abs": "https://arxiv.org/abs/2511.07619", "authors": ["Luca Macesanu", "Boueny Folefack", "Samik Singh", "Ruchira Ray", "Ben Abbatematteo", "Roberto Mart\u00edn-Mart\u00edn"], "title": "CAVER: Curious Audiovisual Exploring Robot", "comment": "9 pages, 6 figures", "summary": "Multimodal audiovisual perception can enable new avenues for robotic manipulation, from better material classification to the imitation of demonstrations for which only audio signals are available (e.g., playing a tune by ear). However, to unlock such multimodal potential, robots need to learn the correlations between an object's visual appearance and the sound it generates when they interact with it. Such an active sensorimotor experience requires new interaction capabilities, representations, and exploration methods to guide the robot in efficiently building increasingly rich audiovisual knowledge. In this work, we present CAVER, a novel robot that builds and utilizes rich audiovisual representations of objects. CAVER includes three novel contributions: 1) a novel 3D printed end-effector, attachable to parallel grippers, that excites objects' audio responses, 2) an audiovisual representation that combines local and global appearance information with sound features, and 3) an exploration algorithm that uses and builds the audiovisual representation in a curiosity-driven manner that prioritizes interacting with high uncertainty objects to obtain good coverage of surprising audio with fewer interactions. We demonstrate that CAVER builds rich representations in different scenarios more efficiently than several exploration baselines, and that the learned audiovisual representation leads to significant improvements in material classification and the imitation of audio-only human demonstrations. https://caver-bot.github.io/", "AI": {"tldr": "CAVER\u673a\u5668\u4eba\u901a\u8fc7\u6709\u6548\u7684\u89c6\u542c\u611f\u77e5\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u80fd\u529b\uff0c\u8868\u73b0\u51fa\u5728\u6750\u6599\u5206\u7c7b\u4e0e\u6a21\u4eff\u4ec5\u901a\u8fc7\u97f3\u9891\u4fe1\u53f7\u7684\u4eba\u7c7b\u793a\u8303\u65b9\u9762\u7684\u663e\u8457\u8fdb\u6b65\u3002", "motivation": "\u5b9e\u73b0\u673a\u5668\u4eba\u80fd\u591f\u66f4\u597d\u5730\u5206\u7c7b\u6750\u6599\u5e76\u6a21\u4eff\u97f3\u9891\u4fe1\u53f7\uff0c\u901a\u8fc7\u89c6\u542c\u611f\u77e5\u7684\u6574\u5408\u63d0\u5347\u64cd\u4f5c\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd3D\u6253\u5370\u7684\u672b\u7aef\u6267\u884c\u5668\uff0c\u7ed3\u5408\u89c6\u89c9\u4e0e\u58f0\u97f3\u7684\u4fe1\u606f\uff0c\u5e76\u91c7\u7528\u597d\u5947\u9a71\u52a8\u7684\u63a2\u7d22\u7b97\u6cd5\u6784\u5efa\u89c6\u542c\u8868\u793a\u3002", "result": "CAVER\u673a\u5668\u4eba\u5728\u591a\u79cd\u573a\u666f\u4e2d\u6bd4\u591a\u4e2a\u57fa\u7ebf\u66f4\u9ad8\u6548\u5730\u6784\u5efa\u4e30\u5bcc\u7684\u89c6\u542c\u8868\u793a\uff0c\u5e76\u5728\u4efb\u52a1\u4e0a\u83b7\u5f97\u663e\u8457\u63d0\u5347\u3002", "conclusion": "CAVER\u7684\u89c6\u542c\u8868\u793a\u80fd\u529b\u548c\u63a2\u7d22\u7b97\u6cd5\u4f7f\u5176\u5728\u6750\u6599\u5206\u7c7b\u4e0e\u6a21\u4eff\u4eba\u7c7b\u793a\u8303\u4e0a\u6548\u679c\u663e\u8457\uff0c\u5c55\u793a\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u5728\u673a\u5668\u4eba\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.07654", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.07654", "abs": "https://arxiv.org/abs/2511.07654", "authors": ["Yinsen Jia", "Boyuan Chen"], "title": "Time-Aware Policy Learning for Adaptive and Punctual Robot Control", "comment": null, "summary": "Temporal awareness underlies intelligent behavior in both animals and humans, guiding how actions are sequenced, paced, and adapted to changing goals and environments. Yet most robot learning algorithms remain blind to time. We introduce time-aware policy learning, a reinforcement learning framework that enables robots to explicitly perceive and reason with time as a first-class variable. The framework augments conventional reinforcement policies with two complementary temporal signals, the remaining time and a time ratio, which allow a single policy to modulate its behavior continuously from rapid and dynamic to cautious and precise execution. By jointly optimizing punctuality and stability, the robot learns to balance efficiency, robustness, resiliency, and punctuality without re-training or reward adjustment. Across diverse manipulation domains from long-horizon pick and place, to granular-media pouring, articulated-object handling, and multi-agent object delivery, the time-aware policy produces adaptive behaviors that outperform standard reinforcement learning baselines by up to 48% in efficiency, 8 times more robust in sim-to-real transfer, and 90% in acoustic quietness while maintaining near-perfect success rates. Explicit temporal reasoning further enables real-time human-in-the-loop control and multi-agent coordination, allowing robots to recover from disturbances, re-synchronize after delays, and align motion tempo with human intent. By treating time not as a constraint but as a controllable dimension of behavior, time-aware policy learning provides a unified foundation for efficient, robust, resilient, and human-aligned robot autonomy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u95f4\u611f\u77e5\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u5c06\u65f6\u95f4\u4f5c\u4e3a\u91cd\u8981\u53d8\u91cf\uff0c\u4ece\u800c\u63d0\u9ad8\u5176\u6267\u884c\u4efb\u52a1\u7684\u80fd\u529b\u548c\u6548\u7387\u3002", "motivation": "\u667a\u80fd\u884c\u4e3a\u4f9d\u8d56\u4e8e\u65f6\u95f4\u610f\u8bc6\uff0c\u800c\u73b0\u6709\u673a\u5668\u4eba\u5b66\u4e60\u7b97\u6cd5\u7f3a\u4e4f\u65f6\u95f4\u611f\u77e5\u80fd\u529b\u3002", "method": "\u65f6\u95f4\u611f\u77e5\u7b56\u7565\u5b66\u4e60\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6", "result": "\u901a\u8fc7\u5f15\u5165\u5269\u4f59\u65f6\u95f4\u548c\u65f6\u95f4\u6bd4\u7387\u7684\u4fe1\u53f7\uff0c\u8be5\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u7684\u6548\u7387\u3001\u9c81\u68d2\u6027\u548c\u51c6\u786e\u5ea6\u3002", "conclusion": "\u65f6\u95f4\u611f\u77e5\u7b56\u7565\u5b66\u4e60\u4e3a\u673a\u5668\u4eba\u81ea\u4e3b\u884c\u4e3a\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u57fa\u7840\uff0c\u517c\u987e\u4e86\u6548\u7387\u3001\u9c81\u68d2\u6027\u548c\u4eba\u7c7b\u610f\u56fe\u5bf9\u9f50\u3002"}}
{"id": "2511.07687", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.07687", "abs": "https://arxiv.org/abs/2511.07687", "authors": ["Braden Meyers", "Joshua G. Mangelson"], "title": "Testing and Evaluation of Underwater Vehicle Using Hardware-In-The-Loop Simulation with HoloOcean", "comment": "Published in IEEE/MTS OCEANS Conference proceedings 2025 Great Lakes", "summary": "Testing marine robotics systems in controlled environments before field tests is challenging, especially when acoustic-based sensors and control surfaces only function properly underwater. Deploying robots in indoor tanks and pools often faces space constraints that complicate testing of control, navigation, and perception algorithms at scale. Recent developments of high-fidelity underwater simulation tools have the potential to address these problems. We demonstrate the utility of the recently released HoloOcean 2.0 simulator with improved dynamics for torpedo AUV vehicles and a new ROS 2 interface. We have successfully demonstrated a Hardware-in-the-Loop (HIL) and Software-in-the-Loop (SIL) setup for testing and evaluating a CougUV torpedo autonomous underwater vehicle (AUV) that was built and developed in our lab. With this HIL and SIL setup, simulations are run in HoloOcean using a ROS 2 bridge such that simulated sensor data is sent to the CougUV (mimicking sensor drivers) and control surface commands are sent back to the simulation, where vehicle dynamics and sensor data are calculated. We compare our simulated results to real-world field trial results.", "AI": {"tldr": "\u672c\u6587\u5c55\u793a\u4e86HoloOcean 2.0\u6a21\u62df\u5668\u5728\u6d77\u6d0b\u673a\u5668\u4eba\u6d4b\u8bd5\u4e2d\u7684\u5e94\u7528\uff0c\u89e3\u51b3\u4e86\u53d7\u63a7\u73af\u5883\u4e0b\u7684\u7a7a\u95f4\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u5728\u53d7\u63a7\u73af\u5883\u4e0b\u6d4b\u8bd5\u6d77\u6d0b\u673a\u5668\u4eba\u7cfb\u7edf\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u58f0\u5b66\u4f20\u611f\u5668\u548c\u63a7\u5236\u9762\u5728\u6c34\u4e0b\u624d\u80fd\u6b63\u5e38\u5de5\u4f5c\u3002\u63a7\u6c34\u6c60\u6d4b\u8bd5\u7a7a\u95f4\u6709\u9650\uff0c\u96be\u4ee5\u5927\u89c4\u6a21\u6d4b\u8bd5\u5bfc\u822a\u548c\u611f\u77e5\u7b97\u6cd5\u3002", "method": "\u4f7f\u7528HoloOcean 2.0\u6a21\u62df\u5668\u8fdb\u884c\u786c\u4ef6\u5728\u73af\uff08HIL\uff09\u548c\u8f6f\u4ef6\u5728\u73af\uff08SIL\uff09\u6d4b\u8bd5", "result": "\u6210\u529f\u5c55\u793a\u4e86CougUV\u9c7c\u96f7\u81ea\u4e3b\u6c34\u4e0b\u8f66\u8f86\u7684HIL\u548cSIL\u6d4b\u8bd5\u8bbe\u7f6e\uff0c\u6a21\u62df\u6570\u636e\u4e0e\u73b0\u5b9e\u4e16\u754c\u8bd5\u9a8c\u7ed3\u679c\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002", "conclusion": "\u901a\u8fc7HIL\u548cSIL\u8bbe\u7f6e\uff0c\u9a8c\u8bc1\u4e86\u6a21\u62df\u5668\u7684\u6709\u6548\u6027\uff0c\u4e3a\u81ea\u4e3b\u6c34\u4e0b\u8f66\u8f86\u7684\u5f00\u53d1\u548c\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2511.07717", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.07717", "abs": "https://arxiv.org/abs/2511.07717", "authors": ["Yifan Liu", "Fangneng Zhan", "Wanhua Li", "Haowen Sun", "Katerina Fragkiadaki", "Hanspeter Pfister"], "title": "RoboTAG: End-to-end Robot Configuration Estimation via Topological Alignment Graph", "comment": null, "summary": "Estimating robot pose from a monocular RGB image is a challenge in robotics and computer vision. Existing methods typically build networks on top of 2D visual backbones and depend heavily on labeled data for training, which is often scarce in real-world scenarios, causing a sim-to-real gap. Moreover, these approaches reduce the 3D-based problem to 2D domain, neglecting the 3D priors. To address these, we propose Robot Topological Alignment Graph (RoboTAG), which incorporates a 3D branch to inject 3D priors while enabling co-evolution of the 2D and 3D representations, alleviating the reliance on labels. Specifically, the RoboTAG consists of a 3D branch and a 2D branch, where nodes represent the states of the camera and robot system, and edges capture the dependencies between these variables or denote alignments between them. Closed loops are then defined in the graph, on which a consistency supervision across branches can be applied. This design allows us to utilize in-the-wild images as training data without annotations. Experimental results demonstrate that our method is effective across robot types, highlighting its potential to alleviate the data bottleneck in robotics.", "AI": {"tldr": "RoboTAG\u901a\u8fc7\u7ed3\u54083D\u4fe1\u606f\u4e0e2D\u8868\u793a\uff0c\u51cf\u5c11\u4e86\u5bf9\u771f\u5b9e\u4e16\u754c\u6807\u8bb0\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5e76\u5728\u591a\u79cd\u673a\u5668\u4eba\u7c7b\u578b\u4e0a\u5c55\u793a\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\uff0c\u4ece\u5355\u76eeRGB\u56fe\u50cf\u4f30\u8ba1\u673a\u5668\u4eba\u59ff\u6001\u662f\u4e00\u9879\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6807\u8bb0\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u800c\u8fd9\u79cd\u6570\u636e\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u5f80\u5f80\u7a00\u7f3a\uff0c\u5bfc\u81f4\u4e86\u4eff\u771f\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u540c\u65f6\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5c063D\u95ee\u9898\u7b80\u5316\u4e3a2D\u9886\u57df\uff0c\u5ffd\u7565\u4e863D\u5148\u9a8c\u4fe1\u606f\u3002", "method": "RoboTAG\u5305\u542b\u4e00\u4e2a3D\u5206\u652f\u548c\u4e00\u4e2a2D\u5206\u652f\uff0c\u8282\u70b9\u8868\u793a\u76f8\u673a\u548c\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u72b6\u6001\uff0c\u8fb9\u5219\u6355\u83b7\u53d8\u91cf\u95f4\u7684\u4f9d\u8d56\u6027\u6216\u5bf9\u5176\u8fdb\u884c\u5bf9\u9f50\u3002\u901a\u8fc7\u5728\u56fe\u4e2d\u5b9a\u4e49\u95ed\u73af\uff0c\u53ef\u4ee5\u5728\u5206\u652f\u4e4b\u95f4\u65bd\u52a0\u4e00\u81f4\u6027\u76d1\u7763\uff0c\u4ece\u800c\u5b9e\u73b0\u65e0\u6ce8\u91ca\u7684\u8bad\u7ec3\u3002", "result": "\u63d0\u51fa\u4e86\u673a\u5668\u4eba\u62d3\u6251\u5bf9\u9f50\u56fe\uff08RoboTAG\uff09\uff0c\u5b83\u6574\u5408\u4e863D\u5206\u652f\u4ee5\u6ce8\u51653D\u5148\u9a8c\uff0c\u540c\u65f6\u4fc3\u8fdb2D\u548c3D\u8868\u793a\u7684\u5171\u540c\u6f14\u5316\uff0c\u51cf\u5c11\u5bf9\u6807\u8bb0\u7684\u4f9d\u8d56\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRoboTAG\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u4e2d\u6570\u636e\u74f6\u9888\u95ee\u9898\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2511.07600", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.07600", "abs": "https://arxiv.org/abs/2511.07600", "authors": ["Swaroop Panda"], "title": "Look into your Heart - Prototypes for a Speculative Design Exploration of Personal Heart Rate Visualization", "comment": null, "summary": "Personal heart rate data from wearable devices contains rich information, yet current visualizations primarily focus on simple metrics, leaving complex temporal patterns largely unexplored. We present a speculative exploration of personal heart rate visualization possibilities through five prototype approaches derived from established visualization literature: pattern/variability heatmaps, recurrence plots, spectrograms, T-SNE, and Poincar\u00e9 plots. Using physiologically-informed synthetic datasets generated through large language models, we systematically explore how different visualization strategies might reveal distinct aspects of heart rate patterns across temporal scales and analytical complexity. We evaluate these prototypes using established visualization assessment scales from multiple literacy perspectives, then conduct reflective analysis on both the evaluation and the design of the prototypes. Our iterative process reveals recurring design tensions in visualizing complex physiological data. This work offers a speculative map of the personal heart rate visualization design space, providing insights into making heart rate data more visually accessible and meaningful.", "AI": {"tldr": "\u63d0\u51fa\u4e94\u79cd\u4e2a\u4eba\u5fc3\u7387\u53ef\u89c6\u5316\u539f\u578b\uff0c\u5229\u7528\u751f\u7406\u77e5\u8bc6\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u63a2\u8ba8\u590d\u6742\u65f6\u5e8f\u6a21\u5f0f\u7684\u53ef\u89c6\u5316\uff0c\u63ed\u793a\u8bbe\u8ba1\u51b2\u7a81\uff0c\u4e3a\u5fc3\u7387\u6570\u636e\u7684\u53ef\u89c6\u5316\u8bbe\u8ba1\u63d0\u4f9b\u65b0\u601d\u8def\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u5f53\u524d\u5fc3\u7387\u6570\u636e\u53ef\u89c6\u5316\u7684\u7a7a\u767d\uff0c\u63a2\u8ba8\u590d\u6742\u65f6\u5e8f\u6a21\u5f0f\u7684\u8868\u73b0\uff0c\u4ee5\u63d0\u9ad8\u6570\u636e\u7684\u89c6\u89c9\u53ef\u53ca\u6027\u548c\u89e3\u8bfb\u4ef7\u503c\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u4e94\u79cd\u53ef\u89c6\u5316\u539f\u578b\u5e76\u4f7f\u7528\u5408\u6210\u6570\u636e\u96c6\u6765\u68c0\u9a8c\u5b83\u4eec\u5728\u63ed\u793a\u5fc3\u7387\u6a21\u5f0f\u4e2d\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u53ef\u89c6\u5316\u8bc4\u4f30\u6807\u51c6\u548c\u53cd\u601d\u5206\u6790\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e94\u79cd\u4e0d\u540c\u7684\u4e2a\u4eba\u5fc3\u7387\u53ef\u89c6\u5316\u539f\u578b\uff0c\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u591a\u79cd\u89c6\u89c9\u5316\u7b56\u7565\u63ed\u793a\u5fc3\u7387\u6570\u636e\u7684\u590d\u6742\u65f6\u5e8f\u6a21\u5f0f\u3002\u8fd9\u4e9b\u539f\u578b\u57fa\u4e8e\u65e2\u6709\u7684\u53ef\u89c6\u5316\u6587\u732e\uff0c\u5177\u4f53\u5305\u62ec\u6a21\u5f0f/\u53d8\u5f02\u6027\u70ed\u56fe\u3001\u91cd\u590d\u56fe\u3001\u58f0\u8c31\u56fe\u3001T-SNE\u548cPoincar\u00e9\u56fe\u3002\u91c7\u7528\u751f\u7406\u5b66\u77e5\u8bc6\u5f15\u5bfc\u7684\u5408\u6210\u6570\u636e\u96c6\u8fdb\u884c\u7cfb\u7edf\u63a2\u7d22\uff0c\u7814\u7a76\u4e0d\u540c\u53ef\u89c6\u5316\u7b56\u7565\u5bf9\u5fc3\u7387\u6a21\u5f0f\u7684\u8868\u73b0\u3002\u901a\u8fc7\u591a\u5c42\u9762\u7684\u53ef\u89c6\u5316\u8bc4\u4f30\u6807\u51c6\u8fdb\u884c\u539f\u578b\u8bc4\u4f30\uff0c\u5e76\u5bf9\u8bc4\u4f30\u53ca\u539f\u578b\u8bbe\u8ba1\u8fdb\u884c\u53cd\u601d\u5206\u6790\uff0c\u63ed\u793a\u5728\u89c6\u89c9\u5316\u590d\u6742\u751f\u7406\u6570\u636e\u65f6\u7684\u8bbe\u8ba1\u51b2\u7a81\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u5f20\u4e2a\u4eba\u5fc3\u7387\u53ef\u89c6\u5316\u8bbe\u8ba1\u7a7a\u95f4\u7684\u63a2\u7d22\u5730\u56fe\uff0c\u65e8\u5728\u4f7f\u5fc3\u7387\u6570\u636e\u7684\u89c6\u89c9\u5448\u73b0\u66f4\u5177\u53ef\u53ca\u6027\u548c\u610f\u4e49\u3002"}}
{"id": "2511.07720", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.07720", "abs": "https://arxiv.org/abs/2511.07720", "authors": ["Yuxuan Zhao", "Yuanchen Tang", "Jindi Zhang", "Hongyu Yu"], "title": "A QP Framework for Improving Data Collection: Quantifying Device-Controller Performance in Robot Teleoperation", "comment": null, "summary": "Robot learning empowers the robot system with human brain-like intelligence to autonomously acquire and adapt skills through experience, enhancing flexibility and adaptability in various environments. Aimed at achieving a similar level of capability in large language models (LLMs) for embodied intelligence, data quality plays a crucial role in training a foundational model with diverse robot skills. In this study, we investigate the collection of data for manipulation tasks using teleoperation devices. Different devices yield varying effects when paired with corresponding controller strategies, including position-based inverse kinematics (IK) control, torque-based inverse dynamics (ID) control, and optimization-based compliance control. In this paper, we develop a teleoperation pipeline that is compatible with different teleoperation devices and manipulator controllers. Within the pipeline, we construct the optimal QP formulation with the dynamic nullspace and the impedance tracking as the novel optimal controller to achieve compliant pose tracking and singularity avoidance. Regarding the optimal controller, it adaptively adjusts the weights assignment depending on the robot joint manipulability that reflects the state of joint configuration for the pose tracking in the form of impedance control and singularity avoidance with nullspace tracking. Analysis of quantitative experimental results suggests the quality of the teleoperated trajectory data, including tracking error, occurrence of singularity, and the smoothness of the joints' trajectory, with different combinations of teleoperation interface and the motion controller.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u901a\u8fc7\u8fdc\u7a0b\u64cd\u4f5c\u8bbe\u5907\u6536\u96c6\u673a\u5668\u4eba\u64cd\u63a7\u6570\u636e\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u9002\u5e94\u6027\u5f3a\u7684\u8fdc\u7a0b\u64cd\u4f5c\u7ba1\u9053\uff0c\u4ee5\u63d0\u9ad8\u64cd\u63a7\u4efb\u52a1\u4e2d\u6570\u636e\u8d28\u91cf\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5177\u8eab\u667a\u80fd\u65b9\u9762\u7684\u80fd\u529b\uff0c\u6570\u636e\u8d28\u91cf\u5728\u8bad\u7ec3\u5177\u5907\u591a\u6837\u5316\u673a\u5668\u4eba\u6280\u80fd\u7684\u57fa\u7840\u6a21\u578b\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002", "method": "\u672c\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u517c\u5bb9\u4e0d\u540c\u8fdc\u7a0b\u64cd\u4f5c\u8bbe\u5907\u548c\u63a7\u5236\u5668\u7684\u8fdc\u7a0b\u64cd\u4f5c\u7ba1\u9053\uff0c\u91c7\u7528\u52a8\u6001\u96f6\u7a7a\u95f4\u548c\u963b\u6297\u8ddf\u8e2a\u7684\u65b0\u578b\u6700\u4f18\u63a7\u5236\u5668\u6765\u5b9e\u73b0\u5408\u89c4\u59ff\u6001\u8ddf\u8e2a\u548c\u5947\u5f02\u6027\u907f\u514d\u3002", "result": "\u901a\u8fc7\u4e0d\u540c\u8fdc\u7a0b\u64cd\u4f5c\u754c\u9762\u4e0e\u8fd0\u52a8\u63a7\u5236\u5668\u7684\u7ec4\u5408\u8fdb\u884c\u5b9a\u91cf\u5b9e\u9a8c\u5206\u6790\uff0c\u5efa\u8bae\u8fdc\u7a0b\u64cd\u4f5c\u8f68\u8ff9\u6570\u636e\u7684\u8d28\u91cf\uff0c\u5305\u62ec\u8ddf\u8e2a\u8bef\u5dee\u3001\u5947\u5f02\u6027\u53d1\u751f\u548c\u5173\u8282\u8f68\u8ff9\u7684\u5e73\u6ed1\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u5f00\u53d1\u7684\u8fdc\u7a0b\u64cd\u4f5c\u7ba1\u9053\u80fd\u591f\u6709\u6548\u5730\u9002\u5e94\u4e0d\u540c\u7684\u8fdc\u7a0b\u64cd\u4f5c\u8bbe\u5907\u548c\u64cd\u63a7\u5668\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5728\u64cd\u63a7\u4efb\u52a1\u4e2d\u7684\u7075\u6d3b\u6027\u4e0e\u9002\u5e94\u6027\u3002"}}
{"id": "2511.07634", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.07634", "abs": "https://arxiv.org/abs/2511.07634", "authors": ["Chadani Acharya"], "title": "Accessibility, Safety, and Accommodation Burden in U.S. Higher Education Syllabi for Blind and Low-Vision Students", "comment": "Preprint. LaTeX (acmart, nonacm). 13 pages. Includes 8 tables and 4 figures", "summary": "Course syllabi are often the first and sometimes only structured artifact that explains how a class will run: deadlines, grading rules, safety procedures, and how to request disability accommodations. For blind and low-vision (BLV) students who use screen readers, independent access depends on whether the syllabus is machine readable and navigable. We audited publicly posted syllabi and master syllabi from five U.S. institutions spanning an elite private R1 university, large public R1s (including a UC campus), a large community college, and a workforce focused technical college. We coded each document on five dimensions: (1) machine-readability of core logistics, (2) readability of safety critical procedures, (3) accommodation framing (rights based vs. burden based), (4) governance model (instructor-authored vs. centralized \"master syllabus\"), and (5) presence of proactive universal design language. Across the sample, logistics and many safety expectations are published as selectable text. Accommodation language, however, shifts by institution type: research universities more often use rights based wording (while still requiring advance letters), whereas community/technical colleges emphasize disclosure, documentation, and institutional discretion in master syllabi that replicate across sections. We argue that accessibility is not only a PDF tagging problem but also a question of governance and equity, and we outline implications for HCI, including an \"accessible master syllabus\" template as a high leverage intervention.", "AI": {"tldr": "\u672c\u7814\u7a76\u5ba1\u8ba1\u4e86\u4e0d\u540c\u7c7b\u578b\u5927\u5b66\u7684\u8bfe\u7a0b\u5927\u7eb2\uff0c\u53d1\u73b0\u5176\u65e0\u969c\u788d\u8bbe\u8ba1\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5f3a\u8c03\u6cbb\u7406\u4e0e\u516c\u5e73\u5728\u8bfe\u7a0b\u8bbe\u8ba1\u4e2d\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u4e3a\u4e86\u8bc4\u4f30\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u5b66\u751f\u662f\u5426\u80fd\u591f\u72ec\u7acb\u8bbf\u95ee\u8bfe\u7a0b\u5927\u7eb2\uff0c\u4e86\u89e3\u5176\u53ef\u8bfb\u6027\u4e0e\u53ef\u5bfc\u822a\u6027\u3002", "method": "\u901a\u8fc7\u5ba1\u8ba1\u6765\u81ea\u4e94\u5bb6\u7f8e\u56fd\u673a\u6784\u7684\u516c\u5f00\u8bfe\u7a0b\u5927\u7eb2\uff0c\u9488\u5bf9\u4e94\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u4e86\u7f16\u7801\u3002", "result": "\u5ba1\u8ba1\u7ed3\u679c\u663e\u793a\u79d1\u7814\u578b\u5927\u5b66\u548c\u793e\u533a/\u6280\u672f\u5b66\u9662\u5728\u8bfe\u7a0b\u5927\u7eb2\u4e2d\u7684\u65e0\u969c\u788d\u8bed\u8a00\u5dee\u5f02\uff0c\u524d\u8005\u503e\u5411\u4e8e\u6743\u76ca\u5bfc\u5411\uff0c\u800c\u540e\u8005\u5219\u4fa7\u91cd\u4e8e\u62ab\u9732\u548c\u6587\u4ef6\u8bc1\u660e\u3002", "conclusion": "\u8bfe\u7a0b\u5927\u7eb2\u7684\u53ef\u53ca\u6027\u4e0d\u4ec5\u662fPDF\u6807\u8bb0\u7684\u95ee\u9898\uff0c\u8fd8\u6d89\u53ca\u6cbb\u7406\u548c\u516c\u5e73\u95ee\u9898\u3002"}}
{"id": "2511.07727", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.07727", "abs": "https://arxiv.org/abs/2511.07727", "authors": ["Xiaohan Zhang", "Yan Ding", "Yohei Hayamizu", "Zainab Altaweel", "Yifeng Zhu", "Yuke Zhu", "Peter Stone", "Chris Paxton", "Shiqi Zhang"], "title": "LLM-GROP: Visually Grounded Robot Task and Motion Planning with Large Language Models", "comment": null, "summary": "Task planning and motion planning are two of the most important problems in robotics, where task planning methods help robots achieve high-level goals and motion planning methods maintain low-level feasibility. Task and motion planning (TAMP) methods interleave the two processes of task planning and motion planning to ensure goal achievement and motion feasibility. Within the TAMP context, we are concerned with the mobile manipulation (MoMa) of multiple objects, where it is necessary to interleave actions for navigation and manipulation.\n  In particular, we aim to compute where and how each object should be placed given underspecified goals, such as ``set up dinner table with a fork, knife and plate.'' We leverage the rich common sense knowledge from large language models (LLMs), e.g., about how tableware is organized, to facilitate both task-level and motion-level planning. In addition, we use computer vision methods to learn a strategy for selecting base positions to facilitate MoMa behaviors, where the base position corresponds to the robot's ``footprint'' and orientation in its operating space. Altogether, this article provides a principled TAMP framework for MoMa tasks that accounts for common sense about object rearrangement and is adaptive to novel situations that include many objects that need to be moved. We performed quantitative experiments in both real-world settings and simulated environments. We evaluated the success rate and efficiency in completing long-horizon object rearrangement tasks. While the robot completed 84.4\\% real-world object rearrangement trials, subjective human evaluations indicated that the robot's performance is still lower than experienced human waiters.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4efb\u52a1\u548c\u8fd0\u52a8\u89c4\u5212\u7684\u6846\u67b6\uff0c\u5e94\u7528\u4e8e\u591a\u7269\u4f53\u7684\u79fb\u52a8\u64cd\u63a7\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u63d0\u5347\u673a\u5668\u4eba\u5728\u79fb\u52a8\u64cd\u63a7\u4efb\u52a1\u4e2d\u7684\u81ea\u4e3b\u6027\u548c\u6548\u7387\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u7684\u591a\u7269\u4f53\u4ea4\u4e92\u573a\u666f\u4e2d\u3002", "method": "\u901a\u8fc7\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\u7684\u7efc\u5408\u65b9\u6cd5\uff0c\u786e\u5b9a\u76ee\u6807\u7269\u4f53\u7684\u4f4d\u7f6e\u548c\u64cd\u4f5c\u7b56\u7565\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u79fb\u52a8\u64cd\u63a7\uff08MoMa\uff09\u4efb\u52a1\u7684\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\uff08TAMP\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u4efb\u52a1\u89c4\u5212\u548c\u8fd0\u52a8\u89c4\u5212\u4ee5\u4fdd\u8bc1\u76ee\u6807\u5b9e\u73b0\u548c\u8fd0\u52a8\u53ef\u884c\u6027\u3002\u901a\u8fc7\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u6cd5\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u7b56\u7565\u6765\u9009\u62e9\u673a\u5668\u4eba\u5728\u64cd\u4f5c\u7a7a\u95f4\u4e2d\u7684\u57fa\u5730\u4f4d\u7f6e\uff0c\u4ee5\u4fc3\u8fdb\u591a\u7269\u4f53\u7684\u79fb\u52a8\u64cd\u63a7\u3002\u540c\u65f6\uff0c\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u548c\u4eff\u771f\u73af\u5883\u7684\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\u5728\u957f\u65f6\u95f4\u7269\u4f53\u91cd\u6392\u4efb\u52a1\u4e2d\u7684\u6210\u529f\u7387\u548c\u6548\u7387\uff0c\u5c3d\u7ba1\u673a\u5668\u4eba\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u5b8c\u6210\u4e8684.4%\u7684\u91cd\u6392\u8bd5\u9a8c\uff0c\u4f46\u4e0e\u7ecf\u9a8c\u4e30\u5bcc\u7684\u670d\u52a1\u5458\u76f8\u6bd4\uff0c\u5176\u8868\u73b0\u4ecd\u6709\u5dee\u8ddd\u3002", "conclusion": "\u8be5TAMP\u6846\u67b6\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u591a\u7269\u4f53\u7684\u79fb\u52a8\u64cd\u63a7\u4efb\u52a1\uff0c\u5c3d\u7ba1\u4e0e\u4eba\u7c7b\u76f8\u6bd4\uff0c\u673a\u5668\u4eba\u5728\u8868\u73b0\u4e0a\u4ecd\u9700\u6539\u8fdb\u3002"}}
{"id": "2511.07682", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07682", "abs": "https://arxiv.org/abs/2511.07682", "authors": ["Michael Hoffmann", "Jophin John", "Jan Fillies", "Adrian Paschke"], "title": "Designing and Evaluating Malinowski's Lens: An AI-Native Educational Game for Ethnographic Learning", "comment": "21 pages, 8 figures. Full preprint version; shorter version in preparation", "summary": "This study introduces 'Malinowski's Lens', the first AI-native educational game for anthropology that transforms Bronislaw Malinowski's 'Argonauts of the Western Pacific' (1922) into an interactive learning experience. The system combines Retrieval-Augmented Generation with DALL-E 3 text-to-image generation, creating consistent VGA-style visuals as players embody Malinowski during his Trobriand Islands fieldwork (1915-1918). To address ethical concerns, indigenous peoples appear as silhouettes while Malinowski is detailed, prompting reflection on anthropological representation. Two validation studies confirmed effectiveness: Study 1 with 10 non-specialists showed strong learning outcomes (average quiz score 7.5/10) and excellent usability (SUS: 83/100). Study 2 with 4 expert anthropologists confirmed pedagogical value, with one senior researcher discovering \"new aspects\" of Malinowski's work through gameplay. The findings demonstrate that AI-driven educational games can effectively convey complex anthropological concepts while sparking disciplinary curiosity. This study advances AI-native educational game design and provides a replicable model for transforming academic texts into engaging interactive experiences.", "AI": {"tldr": "\u8be5\u7814\u7a76\u521b\u9020\u4e86\u9996\u4e2a\u4eba\u5de5\u667a\u80fd\u4eba\u7c7b\u5b66\u6559\u80b2\u6e38\u620f\uff0c\u901a\u8fc7\u4e92\u52a8\u5b66\u4e60\u63d0\u5347\u4e86\u5bf9\u9a6c\u6797\u8bfa\u592b\u65af\u57fa\u5de5\u4f5c\u7684\u7406\u89e3\uff0c\u6709\u6548\u4f20\u8fbe\u590d\u6742\u4eba\u7c7b\u5b66\u6982\u5ff5\u3002", "motivation": "\u65e8\u5728\u5c06\u7ecf\u5178\u4eba\u7c7b\u5b66\u6587\u672c\u8f6c\u5316\u4e3a\u4e92\u52a8\u5b66\u4e60\u4f53\u9a8c\uff0c\u540c\u65f6\u89e3\u51b3\u4f26\u7406\u95ee\u9898\uff0c\u63d0\u5347\u5bf9\u4eba\u7c7b\u5b66\u8868\u73b0\u5f62\u5f0f\u7684\u601d\u8003\u3002", "method": "\u7ed3\u5408\u589e\u5f3a\u68c0\u7d22\u751f\u6210\u4e0eDALL-E 3\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff0c\u8fdb\u884c\u4e24\u9879\u9a8c\u8bc1\u7814\u7a76\u4ee5\u8bc4\u4f30\u6559\u80b2\u6548\u679c\u4e0e\u6559\u5b66\u4ef7\u503c\u3002", "result": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86'Malinowski's Lens'\uff0c\u8fd9\u662f\u9996\u4e2a\u4ee5\u4eba\u5de5\u667a\u80fd\u4e3a\u57fa\u7840\u7684\u4eba\u7c7b\u5b66\u6559\u80b2\u6e38\u620f\uff0c\u5c06\u5e03\u7f57\u5c3c\u65af\u52b3\u00b7\u9a6c\u6797\u8bfa\u592b\u65af\u57fa\u7684\u300a\u592a\u5e73\u6d0b\u7684\u7f8e\u6d32\u571f\u8457\u300b\uff081922\uff09\u8f6c\u53d8\u4e3a\u4e92\u52a8\u5b66\u4e60\u4f53\u9a8c\u3002\u8be5\u7cfb\u7edf\u7ed3\u5408\u4e86\u589e\u5f3a\u68c0\u7d22\u751f\u6210\u548cDALL-E 3\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff0c\u521b\u9020\u4e86\u73a9\u5bb6\u5728\u9a6c\u6797\u8bfa\u592b\u65af\u57fa1915-1918\u5e74\u95f4\u7684\u7279\u7f57\u5e03\u91cc\u6069\u7fa4\u5c9b\u5b9e\u5730\u7814\u7a76\u4e2d\u6240\u9700\u7684\u4e00\u81f4 VGA \u98ce\u683c\u89c6\u89c9\u6548\u679c\u3002\u4e3a\u89e3\u51b3\u4f26\u7406\u95ee\u9898\uff0c\u571f\u8457\u4eba\u6c11\u4ee5\u526a\u5f71\u5f62\u5f0f\u51fa\u73b0\uff0c\u800c\u9a6c\u6797\u8bfa\u592b\u65af\u57fa\u5219\u8be6\u7ec6\u5448\u73b0\uff0c\u4fc3\u4f7f\u4eba\u4eec\u53cd\u601d\u4eba\u7c7b\u5b66\u7684\u8868\u73b0\u5f62\u5f0f\u3002\u4e24\u9879\u9a8c\u8bc1\u7814\u7a76\u8bc1\u5b9e\u4e86\u5176\u6709\u6548\u6027\uff1a\u7b2c\u4e00\u9879\u7814\u7a76\u4e2d10\u540d\u975e\u4e13\u4e1a\u4eba\u58eb\u663e\u793a\u51fa\u5f3a\u52b2\u7684\u5b66\u4e60\u6210\u679c\uff08\u5e73\u5747\u6d4b\u9a8c\u5f97\u5206 7.5/10\uff09\u548c\u51fa\u8272\u7684\u53ef\u7528\u6027\uff08SUS\uff1a83/100\uff09\uff1b\u7b2c\u4e8c\u9879\u7814\u7a76\u4e2d4\u540d\u4e13\u5bb6\u4eba\u7c7b\u5b66\u5bb6\u8bc1\u5b9e\u4e86\u5176\u6559\u5b66\u4ef7\u503c\uff0c\u5176\u4e2d\u4e00\u4f4d\u9ad8\u7ea7\u7814\u7a76\u5458\u901a\u8fc7\u6e38\u620f\u73a9\u6cd5\u53d1\u73b0\u4e86\u9a6c\u6797\u8bfa\u592b\u65af\u57fa\u5de5\u4f5c\u4e2d\u7684\u201c\u65b0\u65b9\u9762\u201d\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u6559\u80b2\u6e38\u620f\u80fd\u591f\u6709\u6548\u4f20\u8fbe\u590d\u6742\u7684\u4eba\u7c7b\u5b66\u6982\u5ff5\uff0c\u540c\u65f6\u6fc0\u53d1\u5b66\u79d1\u597d\u5947\u5fc3\u3002\u8be5\u7814\u7a76\u63a8\u8fdb\u4e86\u4eba\u5de5\u667a\u80fd\u539f\u751f\u6559\u80b2\u6e38\u620f\u7684\u8bbe\u8ba1\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u590d\u5236\u7684\u6a21\u578b\uff0c\u5c06\u5b66\u672f\u6587\u672c\u8f6c\u5316\u4e3a\u5f15\u4eba\u5165\u80dc\u7684\u4e92\u52a8\u4f53\u9a8c\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u6559\u80b2\u6e38\u620f\u80fd\u591f\u6709\u6548\u4f20\u64ad\u4eba\u7c7b\u5b66\u77e5\u8bc6\uff0c\u6fc0\u53d1\u5b66\u4e60\u8005\u7684\u597d\u5947\u5fc3\uff0c\u540c\u65f6\u4fc3\u8fdb\u5b66\u672f\u5185\u5bb9\u7684\u4e92\u52a8\u8f6c\u5316\u3002"}}
{"id": "2511.07732", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.07732", "abs": "https://arxiv.org/abs/2511.07732", "authors": ["Sandeep Routray", "Hengkai Pan", "Unnat Jain", "Shikhar Bahl", "Deepak Pathak"], "title": "ViPRA: Video Prediction for Robot Actions", "comment": "Website: https://vipra-project.github.io", "summary": "Can we turn a video prediction model into a robot policy? Videos, including those of humans or teleoperated robots, capture rich physical interactions. However, most of them lack labeled actions, which limits their use in robot learning. We present Video Prediction for Robot Actions (ViPRA), a simple pretraining-finetuning framework that learns continuous robot control from these actionless videos. Instead of directly predicting actions, we train a video-language model to predict both future visual observations and motion-centric latent actions, which serve as intermediate representations of scene dynamics. We train these latent actions using perceptual losses and optical flow consistency to ensure they reflect physically grounded behavior. For downstream control, we introduce a chunked flow matching decoder that maps latent actions to robot-specific continuous action sequences, using only 100 to 200 teleoperated demonstrations. This approach avoids expensive action annotation, supports generalization across embodiments, and enables smooth, high-frequency continuous control upto 22 Hz via chunked action decoding. Unlike prior latent action works that treat pretraining as autoregressive policy learning, explicitly models both what changes and how. Our method outperforms strong baselines, with a 16% gain on the SIMPLER benchmark and a 13% improvement across real world manipulation tasks. We will release models and code at https://vipra-project.github.io", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9884\u8bad\u7ec3-\u5fae\u8c03\u6846\u67b6ViPRA\uff0c\u80fd\u591f\u4ece\u65e0\u6807\u6ce8\u89c6\u9891\u4e2d\u5b66\u4e60\u8fde\u7eed\u673a\u5668\u4eba\u63a7\u5236\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u89c6\u9891\u4e2d\u6355\u6349\u5230\u4e86\u4e30\u5bcc\u7684\u7269\u7406\u4ea4\u4e92\uff0c\u4f46\u5927\u591a\u6570\u89c6\u9891\u7f3a\u4e4f\u6807\u6ce8\u52a8\u4f5c\uff0c\u9650\u5236\u4e86\u5176\u5728\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u9884\u8bad\u7ec3-\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u8bad\u7ec3\u89c6\u9891-\u8bed\u8a00\u6a21\u578b\u6765\u9884\u6d4b\u672a\u6765\u7684\u89c6\u89c9\u89c2\u6d4b\u548c\u4ee5\u8fd0\u52a8\u4e3a\u4e2d\u5fc3\u7684\u6f5c\u5728\u52a8\u4f5c\uff0c\u5e76\u4f7f\u7528chunked flow matching\u89e3\u7801\u5668\u5c06\u6f5c\u5728\u52a8\u4f5c\u6620\u5c04\u5230\u7279\u5b9a\u4e8e\u673a\u5668\u4eba\u7684\u8fde\u7eed\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "\u6211\u4eec\u7684\u6a21\u578b\u5728\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u907f\u514d\u4e86\u6602\u8d35\u7684\u52a8\u4f5c\u6807\u6ce8\uff0c\u652f\u6301\u8de8\u5b9e\u73b0\u4f53\u7684\u6cdb\u5316\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe22 Hz\u7684\u5e73\u6ed1\u8fde\u7eed\u63a7\u5236\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728SIMPLER\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u5347\u4e8616%\u7684\u6027\u80fd\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u7684\u64cd\u63a7\u4efb\u52a1\u4e2d\u63d0\u9ad8\u4e8613%\u7684\u6548\u679c\u3002"}}
{"id": "2511.07729", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.07729", "abs": "https://arxiv.org/abs/2511.07729", "authors": ["Neil K. R. Sehgal", "Hita Kambhamettu", "Sai Preethi Matam", "Lyle Ungar", "Sharath Chandra Guntuku"], "title": "Designing Mental-Health Chatbots for Indian Adolescents: Mixed-Methods Evidence, a Boundary-Object Lens, and a Design-Tensions Framework", "comment": null, "summary": "Mental health challenges among Indian adolescents are shaped by unique cultural and systemic barriers, including high social stigma and limited professional support. We report a mixed-methods study of Indian adolescents (survey n=362; interviews n=14) examining how they navigate mental-health challenges and engage with digital tools. Quantitative results highlight low self-stigma but significant social stigma, a preference for text over voice interactions, and low utilization of mental health apps but high smartphone access. Our qualitative findings reveal that while adolescents value privacy, emotional support, and localized content in mental health tools, existing chatbots lack personalization and cultural relevance. We contribute (1) a Design-Tensions framework; (2) an artifact-level probe; and (3) a boundary-objects account that specifies how chatbots mediate adolescents, peers, families, and services. This work advances culturally sensitive chatbot design by centering on underrepresented populations, addressing critical gaps in accessibility and support for adolescents in India.", "AI": {"tldr": "\u7814\u7a76\u5370\u5ea6\u9752\u5c11\u5e74\u5728\u5fc3\u7406\u5065\u5eb7\u6311\u6218\u4e0b\u4f7f\u7528\u6570\u5b57\u5de5\u5177\u7684\u60c5\u51b5\uff0c\u53d1\u73b0\u793e\u4f1a\u6c61\u540d\u663e\u8457\uff0c\u73b0\u6709\u804a\u5929\u673a\u5668\u4eba\u7f3a\u4e4f\u4e2a\u6027\u5316\u548c\u6587\u5316\u76f8\u5173\u6027\u3002", "motivation": "\u5370\u5ea6\u9752\u5c11\u5e74\u9762\u4e34\u7279\u6709\u7684\u6587\u5316\u548c\u7cfb\u7edf\u6027\u969c\u788d\uff0c\u5f71\u54cd\u5fc3\u7406\u5065\u5eb7\u6311\u6218\u3002", "method": "\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\uff08\u8c03\u67e5\u548c\u8bbf\u8c08\uff09", "result": "\u53d1\u73b0\u4f4e\u81ea\u6211\u6c61\u540d\u4f46\u663e\u8457\u7684\u793e\u4f1a\u6c61\u540d\uff0c\u504f\u597d\u6587\u672c\u800c\u975e\u8bed\u97f3\u4ea4\u4e92\uff0c\u5fc3\u7406\u5065\u5eb7\u5e94\u7528\u5229\u7528\u7387\u4f4e\u4f46\u667a\u80fd\u624b\u673a\u666e\u53ca\u7387\u9ad8\u3002", "conclusion": "\u672c\u7814\u7a76\u63a8\u52a8\u4e86\u6587\u5316\u654f\u611f\u7684\u804a\u5929\u673a\u5668\u4eba\u8bbe\u8ba1\uff0c\u91cd\u70b9\u5173\u6ce8\u5370\u5ea6\u9752\u5c11\u5e74\u5728\u53ef\u53ca\u6027\u548c\u652f\u6301\u65b9\u9762\u7684\u5173\u952e\u7f3a\u53e3\u3002"}}
{"id": "2511.07750", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.07750", "abs": "https://arxiv.org/abs/2511.07750", "authors": ["Durgakant Pushp", "Weizhe Chen", "Zheng Chen", "Chaomin Luo", "Jason M. Gregory", "Lantao Liu"], "title": "Navigating the Wild: Pareto-Optimal Visual Decision-Making in Image Space", "comment": null, "summary": "Navigating complex real-world environments requires semantic understanding and adaptive decision-making. Traditional reactive methods without maps often fail in cluttered settings, map-based approaches demand heavy mapping effort, and learning-based solutions rely on large datasets with limited generalization. To address these challenges, we present Pareto-Optimal Visual Navigation, a lightweight image-space framework that combines data-driven semantics, Pareto-optimal decision-making, and visual servoing for real-time navigation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u56fe\u50cf\u7a7a\u95f4\u6846\u67b6\uff0c\u7528\u4e8e\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u5bfc\u822a\uff0c\u7ed3\u5408\u4e86\u6570\u636e\u9a71\u52a8\u7684\u8bed\u4e49\u3001\u5e15\u7d2f\u6258\u6700\u4f18\u51b3\u7b56\u548c\u89c6\u89c9\u4f3a\u670d\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u5e15\u7d2f\u6258\u6700\u4f18\u89c6\u89c9\u5bfc\u822a\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u6570\u636e\u9a71\u52a8\u7684\u8bed\u4e49\u5206\u6790\u3001\u5e15\u7d2f\u6258\u6700\u4f18\u51b3\u7b56\u548c\u89c6\u89c9\u4f3a\u670d\u6280\u672f\u3002", "result": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u5b9e\u65f6\u5bfc\u822a\uff0c\u5e76\u5728\u590d\u6742\u73af\u5883\u4e2d\u5c55\u73b0\u4e86\u66f4\u597d\u7684\u9002\u5e94\u6027\u548c\u51b3\u7b56\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u5728\u590d\u6742\u73af\u5883\u4e2d\u8fdb\u884c\u5bfc\u822a\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.07860", "categories": ["cs.HC", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.07860", "abs": "https://arxiv.org/abs/2511.07860", "authors": ["Geuntae Park", "Jiwon Yi", "Taehyun Rhee", "Kwanguk Kim", "Yoonsang Lee"], "title": "TouchWalker: Real-Time Avatar Locomotion from Touchscreen Finger Walking", "comment": "Accepted to ISMAR 2025", "summary": "We present TouchWalker, a real-time system for controlling full-body avatar locomotion using finger-walking gestures on a touchscreen. The system comprises two main components: TouchWalker-MotionNet, a neural motion generator that synthesizes full-body avatar motion on a per-frame basis from temporally sparse two-finger input, and TouchWalker-UI, a compact touch interface that interprets user touch input to avatar-relative foot positions. Unlike prior systems that rely on symbolic gesture triggers or predefined motion sequences, TouchWalker uses its neural component to generate continuous, context-aware full-body motion on a per-frame basis-including airborne phases such as running, even without input during mid-air steps-enabling more expressive and immediate interaction. To ensure accurate alignment between finger contacts and avatar motion, it employs a MoE-GRU architecture with a dedicated foot-alignment loss. We evaluate TouchWalker in a user study comparing it to a virtual joystick baseline with predefined motion across diverse locomotion tasks. Results show that TouchWalker improves users' sense of embodiment, enjoyment, and immersion.", "AI": {"tldr": "TouchWalker\u662f\u4e00\u4e2a\u5b9e\u65f6\u7cfb\u7edf\uff0c\u901a\u8fc7\u89e6\u6478\u5c4f\u7684\u624b\u6307\u6b65\u6001\u63a7\u5236\u5168\u8eab\u5316\u8eab\u8fd0\u52a8\uff0c\u91c7\u7528\u795e\u7ecf\u8fd0\u52a8\u751f\u6210\u5668\u751f\u6210\u6301\u7eed\u7684\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5168\u8eab\u8fd0\u52a8\uff0c\u6539\u8fdb\u4e86\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u63d0\u5347\u865a\u62df\u73af\u5883\u4e2d\u5168\u8eab\u5316\u8eab\u7684\u8fd0\u52a8\u63a7\u5236\u4f53\u9a8c\uff0c\u4f7f\u4e4b\u66f4\u52a0\u76f4\u89c2\u548c\u751f\u52a8\u3002", "method": "\u4f7f\u7528TouchWalker-MotionNet\u795e\u7ecf\u8fd0\u52a8\u751f\u6210\u5668\u548cTouchWalker-UI\u754c\u9762\uff0c\u4ee5\u4e24\u6307\u8f93\u5165\u751f\u6210\u6bcf\u5e27\u7684\u5168\u8eab\u8fd0\u52a8\uff0c\u5e76\u901a\u8fc7MoE-GRU\u67b6\u6784\u7cbe\u786e\u5bf9\u9f50\u624b\u6307\u63a5\u89e6\u548c\u5316\u8eab\u8fd0\u52a8\u3002", "result": "TouchWalker\u7cfb\u7edf\u4f18\u4e8e\u4f20\u7edf\u865a\u62df\u64cd\u7eb5\u6746\uff0c\u63d0\u5347\u4e86\u7528\u6237\u7684\u8eab\u4f53\u611f\u77e5\u3001\u4eab\u53d7\u5ea6\u548c\u6c89\u6d78\u611f\u3002", "conclusion": "TouchWalker\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4ea4\u4e92\u65b9\u5f0f\uff0c\u80fd\u591f\u63d0\u4f9b\u66f4\u5177\u8868\u73b0\u529b\u548c\u5373\u65f6\u6027\u7684\u5168\u8eab\u8fd0\u52a8\u63a7\u5236\uff0c\u6539\u5584\u7528\u6237\u7684\u6c89\u6d78\u4f53\u9a8c\u3002"}}
{"id": "2511.07761", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.07761", "abs": "https://arxiv.org/abs/2511.07761", "authors": ["Myles Pasetsky", "Jiawei Lin", "Bradley Guo", "Sarah Dean"], "title": "High-Altitude Balloon Station-Keeping with First Order Model Predictive Control", "comment": null, "summary": "High-altitude balloons (HABs) are common in scientific research due to their wide range of applications and low cost. Because of their nonlinear, underactuated dynamics and the partial observability of wind fields, prior work has largely relied on model-free reinforcement learning (RL) methods to design near-optimal control schemes for station-keeping. These methods often compare only against hand-crafted heuristics, dismissing model-based approaches as impractical given the system complexity and uncertain wind forecasts. We revisit this assumption about the efficacy of model-based control for station-keeping by developing First-Order Model Predictive Control (FOMPC). By implementing the wind and balloon dynamics as differentiable functions in JAX, we enable gradient-based trajectory optimization for online planning. FOMPC outperforms a state-of-the-art RL policy, achieving a 24% improvement in time-within-radius (TWR) without requiring offline training, though at the cost of greater online computation per control step. Through systematic ablations of modeling assumptions and control factors, we show that online planning is effective across many configurations, including under simplified wind and dynamics models.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b2c\u4e00\u9636\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u65b9\u6cd5\uff08FOMPC\uff09\uff0c\u5b83\u901a\u8fc7\u68af\u5ea6\u4f18\u5316\u5728\u9ad8\u7a7a\u6c14\u7403\u63a7\u5236\u4e2d\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5b9e\u73b0\u4e8624%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u91cd\u65b0\u5ba1\u89c6\u6a21\u578b\u57fa\u7840\u63a7\u5236\u5728\u5b9a\u4f4d\u4fdd\u6301\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4ee5\u5e94\u5bf9\u9ad8\u7a7a\u6c14\u7403\u7684\u975e\u7ebf\u6027\u3001\u6b20\u5145\u5206\u9a71\u52a8\u52a8\u6001\u6027\u548c\u98ce\u573a\u7684\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u3002", "method": "\u901a\u8fc7\u5728JAX\u4e2d\u5b9e\u73b0\u98ce\u548c\u6c14\u7403\u52a8\u529b\u5b66\u7684\u53ef\u5fae\u51fd\u6570\uff0c\u5b9e\u73b0\u57fa\u4e8e\u68af\u5ea6\u7684\u8f68\u8ff9\u4f18\u5316\u8fdb\u884c\u5728\u7ebf\u89c4\u5212\u3002", "result": "FOMPC outperform\u4e86\u6700\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5c55\u793a\u4e86\u5728\u7ebf\u89c4\u5212\u5728\u591a\u79cd\u914d\u7f6e\u4e0b\u7684\u6709\u6548\u6027\uff0c\u5305\u62ec\u5728\u7b80\u5316\u7684\u98ce\u548c\u52a8\u529b\u5b66\u6a21\u578b\u4e0b\u3002", "conclusion": "FOMPC\u5728\u4e0d\u9700\u8981\u79bb\u7ebf\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e8624%\u7684\u65f6\u95f4\u5728\u534a\u5f84\u5185\u7684\u63d0\u9ad8\uff0c\u5c3d\u7ba1\u6bcf\u6b21\u63a7\u5236\u6b65\u9aa4\u7684\u5728\u7ebf\u8ba1\u7b97\u91cf\u8f83\u5927\u3002"}}
{"id": "2511.07986", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.07986", "abs": "https://arxiv.org/abs/2511.07986", "authors": ["Yoichi Ochiai", "Takashi Shimizu"], "title": "Digital Nature Revisited: A Ten-Year Synthesis of Art, Technology, and the Evolution of \"Nature\": Reimagining Post-Truth Ecologies Through Art, Algorithm, and Animism", "comment": "9 pages", "summary": "This paper critically re-examines \"Digital Nature,\" a concept that has proliferated across various domains over the last ten years. By \"Digital Nature,\" we refer to an evolving view of nature as a dynamic process of circulating computation and matter, one that extends into the realms of AI, XR, indigenous perspectives, and post-human theory. Despite its popularity, \"Digital Nature\" remains ambiguously defined. This paper provides a genealogical and philosophical survey of how the idea has emerged, diverged, and overlapped in media art, bio-art, and generative art, alongside relevant Eastern, Islamic, and indigenous worldviews. We then introduce a multi-axis framework (from real/virtual to anthropocentric/object-oriented, with sub-axes of enchantment and materialization), illustrating how digital technologies have reconceptualized the question \"What is nature?\" in unexpected ways. Finally, we discuss how the field might evolve, particularly through the lens of large language models, AGI, and \"supernatural reality,\" while highlighting the ethical and political pitfalls of techno-occultism. Our ultimate goal is to re-situate \"Digital Nature\" as both an intellectual frontier and a collaborative platform that invites continuous dialogue between art, science, technology, and cultural philosophies.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u201c\u6570\u5b57\u81ea\u7136\u201d\u8fd9\u4e00\u6982\u5ff5\uff0c\u63a2\u8ba8\u5176\u5728\u591a\u4e2a\u9886\u57df\u7684\u6f14\u53d8\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u591a\u8f74\u6846\u67b6\u6765\u754c\u5b9a\u5176\u5728\u6570\u5b57\u6280\u672f\u4e0b\u7684\u91cd\u65b0\u6982\u5ff5\u5316\u3002", "motivation": "\u63a2\u8ba8\u201c\u6570\u5b57\u81ea\u7136\u201d\u7684\u6a21\u7cca\u5b9a\u4e49\u53ca\u5176\u5728\u5f53\u4eca\u6280\u672f\u73af\u5883\u4e2d\u7684\u610f\u4e49\uff0c\u7279\u522b\u662f\u5927\u8bed\u8a00\u6a21\u578b\u548c\u8d85\u81ea\u7136\u73b0\u5b9e\u7684\u89c6\u89d2\u3002", "method": "\u901a\u8fc7\u57fa\u56e0\u8c31\u5b66\u548c\u54f2\u5b66\u8c03\u67e5\uff0c\u5206\u6790\u201c\u6570\u5b57\u81ea\u7136\u201d\u5728\u5a92\u4f53\u827a\u672f\u3001\u751f\u7269\u827a\u672f\u548c\u751f\u6210\u827a\u672f\u4e2d\u7684\u51fa\u73b0\u53ca\u5176\u4e0e\u4e1c\u65b9\u3001\u4f0a\u65af\u5170\u548c\u571f\u8457\u4e16\u754c\u89c2\u7684\u4ea4\u6c47\u3002", "result": "\u63d0\u51fa\u4e00\u4e2a\u591a\u8f74\u6846\u67b6\uff0c\u91cd\u65b0\u5b9a\u4e49\u4e86\u201c\u4ec0\u4e48\u662f\u81ea\u7136\u201d\u7684\u95ee\u9898\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u79d1\u6280\u4e0e\u795e\u79d8\u4e3b\u4e49\u7684\u4f26\u7406\u548c\u653f\u6cbb\u9677\u9631\u3002", "conclusion": "\u201c\u6570\u5b57\u81ea\u7136\u201d\u5e94\u4f5c\u4e3a\u4e00\u4e2a\u667a\u529b\u524d\u6cbf\u548c\u5408\u4f5c\u5e73\u53f0\uff0c\u9f13\u52b1\u827a\u672f\u3001\u79d1\u5b66\u3001\u6280\u672f\u4e0e\u6587\u5316\u54f2\u5b66\u4e4b\u95f4\u7684\u6301\u7eed\u5bf9\u8bdd\u3002"}}
{"id": "2511.07797", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.07797", "abs": "https://arxiv.org/abs/2511.07797", "authors": ["Benjamin Davis", "Hannah Stuart"], "title": "Benchmarking Resilience and Sensitivity of Polyurethane-Based Vision-Based Tactile Sensors", "comment": null, "summary": "Vision-based tactile sensors (VBTSs) are a promising technology for robots, providing them with dense signals that can be translated into an understanding of normal and shear load, contact region, texture classification, and more. However, existing VBTS tactile surfaces make use of silicone gels, which provide high sensitivity but easily deteriorate from loading and surface wear. We propose that polyurethane rubber, used for high-load applications like shoe soles, rubber wheels, and industrial gaskets, may provide improved physical gel resilience, potentially at the cost of sensitivity. To compare the resilience and sensitivity of silicone and polyurethane VBTS gels, we propose a series of standard evaluation benchmarking protocols. Our resilience tests assess sensor durability across normal loading, shear loading, and abrasion. For sensitivity, we introduce model-free assessments of force and spatial sensitivity to directly measure the physical capabilities of each gel without effects introduced from data and model quality. Finally, we include a bottle cap loosening and tightening demonstration as an example where polyurethane gels provide an advantage over their silicone counterparts.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u91c7\u7528\u805a\u6c28\u916f\u6a61\u80f6\u66ff\u4ee3\u7845\u80f6\uff0c\u7528\u4e8e\u63d0\u9ad8\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u5668\u7684\u8010\u7528\u6027\uff0c\u540c\u65f6\u8bc4\u4f30\u4e24\u8005\u5728\u654f\u611f\u5ea6\u4e0a\u7684\u5dee\u5f02\u3002", "motivation": "\u5f53\u524d\u7684\u7845\u80f6\u6750\u6599\u5728\u8d1f\u8f7d\u548c\u8868\u9762\u78e8\u635f\u65b9\u9762\u6613\u4e8e\u6076\u5316\uff0c\u56e0\u6b64\u5bfb\u627e\u66f4\u8010\u7528\u7684\u6750\u6599\u662f\u5fc5\u8981\u7684\u3002", "method": "\u901a\u8fc7\u6807\u51c6\u5316\u8bc4\u4f30\u57fa\u51c6\u534f\u8bae\u5bf9\u7845\u80f6\u548c\u805a\u6c28\u916f\u7684\u8010\u7528\u6027\u548c\u654f\u611f\u6027\u8fdb\u884c\u6bd4\u8f83\uff0c\u8fdb\u884c\u8010\u4e45\u6027\u548c\u654f\u611f\u6027\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u805a\u6c28\u916f\u6a61\u80f6\u5728\u8010\u7528\u6027\u65b9\u9762\u4f18\u4e8e\u7845\u80f6\uff0c\u5e76\u901a\u8fc7\u74f6\u76d6\u677e\u7d27\u6f14\u793a\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4f18\u52bf\u3002", "conclusion": "\u805a\u6c28\u916f\u6a61\u80f6\u5728\u8010\u7528\u6027\u4e0a\u4f18\u4e8e\u7845\u80f6\uff0c\u5c3d\u7ba1\u53ef\u80fd\u5728\u654f\u611f\u5ea6\u4e0a\u6709\u6240\u59a5\u534f\u3002"}}
{"id": "2511.07993", "categories": ["cs.HC", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.07993", "abs": "https://arxiv.org/abs/2511.07993", "authors": ["Jiarui Chen", "Xinwei Loo", "Yien Hong", "Anand Bhojan"], "title": "Private Chat in a Public Space of Metaverse Systems", "comment": "7 pages, 5 figures", "summary": "With the proliferation of Virtual Reality (VR) technologies and the emergence of the Metaverse, social VR applications have become increasingly prevalent and accessible to the general user base. Serving as a novel form of social media, these platforms give users a unique opportunity to engage in social activities. However, there remains a significant limitation: the inability to engage in private conversations within public social VR environments. Current interactions are predominantly public, making it challenging for users to have confidential side discussions or whispers without disrupting ongoing conversations. To address this gap, we developed Hushhub, a private chat system integrated into the popular social VR platform VRChat. Our system enables users within a shared VR space to initiate private audio conversations selectively, allowing them to maintain awareness and engagement with the broader group discussions. To evaluate the system, we conducted user studies to gather insight and feedback on the efficacy and user experience of the implemented system. The results demonstrate the value and necessity of enabling private conversations within immersive social VR environments, paving the way for richer, more nuanced social interactions.", "AI": {"tldr": "\u672c\u8bba\u6587\u4ecb\u7ecd\u4e86 Hushhub\uff0c\u4e00\u4e2a\u96c6\u6210\u4e8e VRChat \u7684\u79c1\u804a\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u793e\u4ea4 VR \u73af\u5883\u4e2d\u65e0\u6cd5\u8fdb\u884c\u79c1\u5bc6\u5bf9\u8bdd\u7684\u96be\u9898\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u7528\u6237\u7684\u793e\u4ea4\u4f53\u9a8c\u3002", "motivation": "\u5728\u516c\u5171\u793e\u4ea4 VR \u73af\u5883\u4e2d\uff0c\u65e0\u6cd5\u8fdb\u884c\u79c1\u5bc6\u8c08\u8bdd\u9650\u5236\u4e86\u7528\u6237\u7684\u793e\u4ea4\u4e92\u52a8\uff0c\u5f71\u54cd\u4e86\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u901a\u8fc7\u5728 VRChat \u5e73\u53f0\u96c6\u6210 Hushhub \u79c1\u804a\u7cfb\u7edf\uff0c\u5141\u8bb8\u7528\u6237\u5728\u5171\u4eab VR \u7a7a\u95f4\u5185\u9009\u62e9\u6027\u5730\u53d1\u8d77\u79c1\u5bc6\u97f3\u9891\u5bf9\u8bdd\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cHushhub \u63d0\u5347\u4e86\u7528\u6237\u5728\u6c89\u6d78\u5f0f\u793e\u4ea4 VR \u73af\u5883\u4e2d\u7684\u79c1\u5bc6\u5bf9\u8bdd\u80fd\u529b\uff0c\u6ee1\u8db3\u4e86\u7528\u6237\u7684\u9700\u6c42\u3002", "conclusion": "Hushhub \u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u5f0f\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u5728\u793e\u4ea4 VR \u73af\u5883\u4e2d\u8fdb\u884c\u79c1\u5bc6\u5bf9\u8bdd\uff0c\u589e\u5f3a\u4e86\u793e\u4ea4\u4e92\u52a8\u7684\u4e30\u5bcc\u6027\u548c\u590d\u6742\u6027\u3002"}}
{"id": "2511.07811", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.07811", "abs": "https://arxiv.org/abs/2511.07811", "authors": ["Sagar Gupta", "Thanh Vinh Nguyen", "Thieu Long Phan", "Vidul Attri", "Archit Gupta", "Niroshinie Fernando", "Kevin Lee", "Seng W. Loke", "Ronny Kutadinata", "Benjamin Champion", "Akansel Cosgun"], "title": "Virtual Traffic Lights for Multi-Robot Navigation: Decentralized Planning with Centralized Conflict Resolution", "comment": null, "summary": "We present a hybrid multi-robot coordination framework that combines decentralized path planning with centralized conflict resolution. In our approach, each robot autonomously plans its path and shares this information with a centralized node. The centralized system detects potential conflicts and allows only one of the conflicting robots to proceed at a time, instructing others to stop outside the conflicting area to avoid deadlocks. Unlike traditional centralized planning methods, our system does not dictate robot paths but instead provides stop commands, functioning as a virtual traffic light. In simulation experiments with multiple robots, our approach increased the success rate of robots reaching their goals while reducing deadlocks. Furthermore, we successfully validated the system in real-world experiments with two quadruped robots and separately with wheeled Duckiebots.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u591a\u673a\u5668\u4eba\u534f\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6563\u7684\u8def\u5f84\u89c4\u5212\u4e0e\u96c6\u4e2d\u5f0f\u51b2\u7a81\u89e3\u51b3\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u6210\u529f\u5230\u8fbe\u76ee\u6807\u7684\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u8def\u5f84\u89c4\u5212\u548c\u51b2\u7a81\u7ba1\u7406\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u578b\u6846\u67b6\uff0c\u589e\u5f3a\u4e86\u673a\u5668\u4eba\u81ea\u4e3b\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u96c6\u4e2d\u63a7\u5236\u907f\u514d\u51b2\u7a81\u3002", "method": "\u6bcf\u4e2a\u673a\u5668\u4eba\u81ea\u4e3b\u89c4\u5212\u8def\u5f84\uff0c\u5e76\u5411\u96c6\u4e2d\u8282\u70b9\u5171\u4eab\u4fe1\u606f\uff0c\u96c6\u4e2d\u7cfb\u7edf\u68c0\u6d4b\u6f5c\u5728\u51b2\u7a81\uff0c\u5141\u8bb8\u4e00\u4e2a\u51b2\u7a81\u673a\u5668\u4eba\u5148\u884c\uff0c\u5176\u4f59\u673a\u5668\u4eba\u5728\u51b2\u7a81\u533a\u5916\u505c\u6b62\uff0c\u907f\u514d\u6b7b\u9501\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u591a\u673a\u5668\u4eba\u534f\u8c03\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u5206\u6563\u7684\u8def\u5f84\u89c4\u5212\u548c\u96c6\u4e2d\u5f0f\u7684\u51b2\u7a81\u89e3\u51b3\uff0c\u80fd\u6709\u6548\u63d0\u9ad8\u673a\u5668\u4eba\u5230\u8fbe\u76ee\u6807\u5730\u7684\u6210\u529f\u7387\u5e76\u51cf\u5c11\u6b7b\u9501\u3002", "conclusion": "\u901a\u8fc7\u5728\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\uff0c\u8fd9\u4e00\u6846\u67b6\u6210\u529f\u51cf\u5c11\u4e86\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u51b2\u7a81\u548c\u6b7b\u9501\u73b0\u8c61\uff0c\u63d0\u9ad8\u4e86\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u6548\u7387\u3002"}}
{"id": "2511.08177", "categories": ["cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.08177", "abs": "https://arxiv.org/abs/2511.08177", "authors": ["Yasmine Elfares", "G\u00fcl \u00c7alikli", "Mohamed Khamis"], "title": "GazeCopilot: Evaluating Novel Gaze-Informed Prompting for AI-Supported Code Comprehension and Readability", "comment": null, "summary": "AI-powered coding assistants, like GitHub Copilot, are increasingly used to boost developers' productivity. However, their output quality hinges on the contextual richness of the prompts. Meanwhile, gaze behaviour carries rich cognitive information, providing insights into how developers process code. We leverage this in Real-time GazeCopilot, a novel approach that refines prompts using real-time gaze data to improve code comprehension and readability by integrating gaze metrics, like fixation patterns and pupil dilation, into prompts to adapt suggestions to developers' cognitive states. In a controlled lab study with 25 developers, we evaluated Real-time GazeCopilot against two baselines: Standard Copilot, which relies on text prompts provided by developers, and Pre-set GazeCopilot, which uses a hard-coded prompt that assumes developers' gaze metrics indicate they are struggling with all aspects of the code, allowing us to assess the impact of leveraging the developer's personal real-time gaze data. Our results show that prompts dynamically generated using developers' real-time gaze data significantly improve code comprehension accuracy, reduce comprehension time, and improve perceived readability compared to Standard Copilot. Our Real-time GazeCopilot approach selectively refactors only code aspects where gaze data indicate difficulty, outperforming the overgeneralized refactoring done by Pre-set GazeCopilot by avoiding revising code the developer already understands.", "AI": {"tldr": "\u901a\u8fc7\u6574\u5408\u5f00\u53d1\u8005\u5b9e\u65f6\u6ce8\u89c6\u6570\u636e\uff0cGazeCopilot\u6709\u6548\u63d0\u5347\u4ee3\u7801\u7406\u89e3\u548c\u53ef\u8bfb\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u5347\u57fa\u4e8eAI\u7684\u7f16\u7801\u52a9\u624b\u7684\u8f93\u51fa\u8d28\u91cf\uff0c\u4ece\u800c\u5e2e\u52a9\u5f00\u53d1\u8005\u66f4\u597d\u5730\u7406\u89e3\u548c\u9605\u8bfb\u4ee3\u7801\u3002", "method": "\u8fdb\u884c\u4e86\u4e00\u9879\u63a7\u5236\u5b9e\u9a8c\uff0c\u8bc4\u4f30GazeCopilot\u4e0e\u6807\u51c6Copilot\u548c\u9884\u8bbeGazeCopilot\u7684\u6548\u679c\u5bf9\u6bd4\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b9e\u65f6\u6ce8\u89c6\u6570\u636e\u7684GazeCopilot\u65b9\u6cd5\uff0c\u80fd\u591f\u6839\u636e\u5f00\u53d1\u8005\u7684\u8ba4\u77e5\u72b6\u6001\u52a8\u6001\u8c03\u6574\u63d0\u793a\uff0c\u4ece\u800c\u63d0\u9ad8\u4ee3\u7801\u7684\u7406\u89e3\u529b\u548c\u53ef\u8bfb\u6027\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u65f6\u6ce8\u89c6\u6570\u636e\u751f\u6210\u7684\u63d0\u793a\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u7406\u89e3\u7684\u51c6\u786e\u6027\uff0c\u5e76\u51cf\u5c11\u4e86\u7406\u89e3\u65f6\u95f4\u3002"}}
{"id": "2511.07820", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.GR", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.07820", "abs": "https://arxiv.org/abs/2511.07820", "authors": ["Zhengyi Luo", "Ye Yuan", "Tingwu Wang", "Chenran Li", "Sirui Chen", "Fernando Casta\u00f1eda", "Zi-Ang Cao", "Jiefeng Li", "David Minor", "Qingwei Ben", "Xingye Da", "Runyu Ding", "Cyrus Hogg", "Lina Song", "Edy Lim", "Eugene Jeong", "Tairan He", "Haoru Xue", "Wenli Xiao", "Zi Wang", "Simon Yuen", "Jan Kautz", "Yan Chang", "Umar Iqbal", "Linxi \"Jim\" Fan", "Yuke Zhu"], "title": "SONIC: Supersizing Motion Tracking for Natural Humanoid Whole-Body Control", "comment": "Project page: https://nvlabs.github.io/SONIC/", "summary": "Despite the rise of billion-parameter foundation models trained across thousands of GPUs, similar scaling gains have not been shown for humanoid control. Current neural controllers for humanoids remain modest in size, target a limited behavior set, and are trained on a handful of GPUs over several days. We show that scaling up model capacity, data, and compute yields a generalist humanoid controller capable of creating natural and robust whole-body movements. Specifically, we posit motion tracking as a natural and scalable task for humanoid control, leverageing dense supervision from diverse motion-capture data to acquire human motion priors without manual reward engineering. We build a foundation model for motion tracking by scaling along three axes: network size (from 1.2M to 42M parameters), dataset volume (over 100M frames, 700 hours of high-quality motion data), and compute (9k GPU hours). Beyond demonstrating the benefits of scale, we show the practical utility of our model through two mechanisms: (1) a real-time universal kinematic planner that bridges motion tracking to downstream task execution, enabling natural and interactive control, and (2) a unified token space that supports various motion input interfaces, such as VR teleoperation devices, human videos, and vision-language-action (VLA) models, all using the same policy. Scaling motion tracking exhibits favorable properties: performance improves steadily with increased compute and data diversity, and learned representations generalize to unseen motions, establishing motion tracking at scale as a practical foundation for humanoid control.", "AI": {"tldr": "\u901a\u8fc7\u6269\u5c55\u4eba\u5f62\u63a7\u5236\u7684\u6a21\u578b\u89c4\u6a21\uff0c\u6570\u636e\u91cf\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u63a7\u5236\u5668\uff0c\u80fd\u591f\u5728\u591a\u79cd\u8f93\u5165\u754c\u9762\u4e0b\u5b9e\u73b0\u81ea\u7136\u7684\u8fd0\u52a8\u63a7\u5236\u3002", "motivation": "\u5f53\u524d\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u7684\u795e\u7ecf\u63a7\u5236\u5668\u89c4\u6a21\u8f83\u5c0f\u3001\u76ee\u6807\u884c\u4e3a\u53d7\u9650\uff0c\u8bad\u7ec3\u65f6\u4ec5\u5728\u5c11\u6570GPU\u4e0a\u8fdb\u884c\uff0c\u65e0\u6cd5\u5145\u5206\u53d1\u6325\u66f4\u5927\u6a21\u578b\u7684\u6f5c\u529b\u3002", "method": "\u6784\u5efa\u8fd0\u52a8\u8ddf\u8e2a\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u6269\u5927\u7f51\u7edc\u89c4\u6a21\uff08\u4ece120\u4e07\u52304200\u4e07\u53c2\u6570\uff09\u3001\u6570\u636e\u96c6\u89c4\u6a21\uff08\u8d85\u8fc71\u4ebf\u5e27\uff0c700\u5c0f\u65f6\u9ad8\u8d28\u91cf\u52a8\u4f5c\u6570\u636e\uff09\u548c\u8ba1\u7b97\u65f6\u95f4\uff089000 GPU\u5c0f\u65f6\uff09\u6765\u5b9e\u73b0\u3002", "result": "\u901a\u8fc7\u6269\u5927\u6a21\u578b\u5bb9\u91cf\u3001\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u5f00\u53d1\u51fa\u4e00\u79cd\u901a\u7528\u7684\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u5668\uff0c\u80fd\u591f\u751f\u6210\u81ea\u7136\u4e14\u7a33\u5065\u7684\u5168\u8eab\u8fd0\u52a8\u3002", "conclusion": "\u6269\u5c55\u8fd0\u52a8\u8ddf\u8e2a\u7684\u80fd\u529b\u4f7f\u6027\u80fd\u968f\u7740\u8ba1\u7b97\u548c\u6570\u636e\u7684\u589e\u591a\u800c\u7a33\u6b65\u63d0\u5347\uff0c\u6240\u5b66\u8868\u793a\u80fd\u591f\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u52a8\u4f5c\uff0c\u5960\u5b9a\u4e86\u5927\u89c4\u6a21\u8fd0\u52a8\u8ddf\u8e2a\u4f5c\u4e3a\u4eba\u5f62\u63a7\u5236\u7684\u5b9e\u7528\u57fa\u7840\u3002"}}
{"id": "2511.08279", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.08279", "abs": "https://arxiv.org/abs/2511.08279", "authors": ["Alan Magdaleno", "Pietro Bonazzi", "Tommaso Polonelli", "Michele Magno"], "title": "Evaluating Electric Charge Variation Sensors for Camera-free Eye Tracking on Smart Glasses", "comment": null, "summary": "Contactless Electrooculography (EOC) using electric charge variation (QVar) sensing has recently emerged as a promising eye-tracking technique for wearable devices. QVar enables low-power and unobtrusive interaction without requiring skin-contact electrodes. Previous work demonstrated that such systems can accurately classify eye movements using onboard TinyML under controlled laboratory conditions. However, the performance and robustness of contactless EOC in real-world scenarios, where environmental noise and user variability are significant, remain largely unexplored. In this paper, we present a field evaluation of a previously proposed QVar-based eye-tracking system, assessing its limitations in everyday usage contexts across 29 users and 100 recordings in everyday scenarios such as working in front of a laptop. Our results show that classification accuracy varies between 57% and 89% depending on the user, with an average of 74.5%, and degrades significantly in the presence of nearby electronic noise sources. These results show that contactless EOC remains viable under realistic conditions, though subject variability and environmental factors can significantly affect classification accuracy. The findings inform the future development of wearable gaze interfaces for human-computer interaction and augmented reality, supporting the transition of this technology from prototype to practice.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e00\u79cd\u63a5\u89e6\u5f0f\u7535\u773c\u52a8\u8ffd\u8e2a\u7cfb\u7edf\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u7528\u6237\u5dee\u5f02\u548c\u73af\u5883\u566a\u58f0\u5bf9\u5206\u7c7b\u51c6\u786e\u6027\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u63a5\u89e6\u5f0fEOC\u901a\u8fc7\u7535\u8377\u53d8\u5316\u611f\u5e94\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u529f\u8017\u4e14\u4e0d\u663e\u8457\u7684\u773c\u52a8\u8ffd\u8e2a\u6280\u672f\uff0c\u4f46\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u6027\u80fd\u548c\u7a33\u5065\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u8ba8\u3002", "method": "\u5bf9\u4e4b\u524d\u63d0\u51fa\u7684\u57fa\u4e8eQVar\u7684\u773c\u52a8\u8ffd\u8e2a\u7cfb\u7edf\u8fdb\u884c\u4e86\u73b0\u573a\u8bc4\u4f30\uff0c\u572829\u4e2a\u7528\u6237\u548c100\u4e2a\u5f55\u97f3\u7684\u65e5\u5e38\u573a\u666f\u4e2d\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u5206\u7c7b\u51c6\u786e\u7387\u572857%\u523089%\u4e4b\u95f4\u53d8\u5316\uff0c\u5e73\u5747\u4e3a74.5%\uff0c\u5728\u6709\u7535\u5b50\u566a\u58f0\u6e90\u7684\u60c5\u51b5\u4e0b\u51c6\u786e\u6027\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u5c3d\u7ba1\u63a5\u89e6\u5f0fEOC\u5728\u73b0\u5b9e\u6761\u4ef6\u4e0b\u4ecd\u7136\u53ef\u884c\uff0c\u4f46\u7528\u6237\u5dee\u5f02\u548c\u73af\u5883\u56e0\u7d20\u4f1a\u663e\u8457\u5f71\u54cd\u5206\u7c7b\u51c6\u786e\u6027\u3002"}}
{"id": "2511.07822", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.07822", "abs": "https://arxiv.org/abs/2511.07822", "authors": ["Collin Hague", "Artur Wolek"], "title": "Occlusion-Aware Ground Target Search by a UAV in an Urban Environment", "comment": "18 pages, 18 figures, 5 tables", "summary": "This paper considers the problem of searching for a point of interest (POI) moving along an urban road network with an uncrewed aerial vehicle (UAV). The UAV is modeled as a variable-speed Dubins vehicle with a line-of-sight sensor in an urban environment that may occlude the sensor's view of the POI. A search strategy is proposed that exploits a probabilistic visibility volume (VV) to plan its future motion with iterative deepening $A^\\ast$. The probabilistic VV is a time-varying three-dimensional representation of the sensing constraints for a particular distribution of the POI's state. To find the path most likely to view the POI, the planner uses a heuristic to optimistically estimate the probability of viewing the POI over a time horizon. The probabilistic VV is max-pooled to create a variable-timestep planner that reduces the search space and balances long-term and short-term planning. The proposed path planning method is compared to prior work with a Monte-Carlo simulation and is shown to outperform the baseline methods in cluttered environments when the UAV's sensor has a higher false alarm probability.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u53ef\u89c6\u4f53\u79ef\u7684\u65b9\u6cd5\uff0c\u4f18\u5316\u4e86\u65e0\u4eba\u673a\u5728\u57ce\u5e02\u73af\u5883\u4e2d\u8fc7\u6ee4\u5174\u8da3\u70b9\u7684\u641c\u7d22\u8def\u5f84\uff0c\u5e76\u5728\u6a21\u62df\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u6548\u679c\u3002", "motivation": "\u5728\u57ce\u5e02\u73af\u5883\u4e2d\uff0c\u5174\u8da3\u70b9\u7684\u906e\u6321\u548c\u4f20\u611f\u5668\u7684\u4e0d\u51c6\u786e\u6027\u4f7f\u5f97\u9ad8\u6548\u641c\u7d22\u53d8\u5f97\u590d\u6742\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u641c\u7d22\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u901a\u8fc7\u5efa\u7acb\u4e00\u4e2a\u57fa\u4e8e\u6982\u7387\u53ef\u89c6\u4f53\u79ef\u7684\u6a21\u578b\uff0c\u7ed3\u5408\u65f6\u95f4\u53d8\u5316\u7684\u4e09\u7ef4\u8868\u793a\u548c\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u4f7f\u7528\u8fed\u4ee3\u6df1\u5316A*\u89c4\u5212\u8def\u5f84\u3002", "result": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u65e0\u4eba\u673a(UAV)\u5728\u57ce\u5e02\u9053\u8def\u7f51\u7edc\u4e2d\u641c\u7d22\u79fb\u52a8\u5174\u8da3\u70b9(POI)\u7684\u7b56\u7565\u3002\u901a\u8fc7\u4f7f\u7528\u6982\u7387\u53ef\u89c6\u4f53\u79ef(VV)\u548c\u8fed\u4ee3\u6df1\u5316A*\u7b97\u6cd5\uff0c UAV\u80fd\u591f\u66f4\u597d\u5730\u89c4\u5212\u5176\u672a\u6765\u52a8\u4f5c\uff0c\u5c24\u5176\u662f\u5728\u4f20\u611f\u5668\u89c6\u91ce\u53ef\u80fd\u88ab\u906e\u6321\u7684\u57ce\u5e02\u73af\u5883\u4e2d\u3002", "conclusion": "\u8be5\u7814\u7a76\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5728\u590d\u6742\u7684\u57ce\u5e02\u73af\u5883\u4e2d\uff0c\u5f53\u65e0\u4eba\u673a\u7684\u4f20\u611f\u5668\u5177\u6709\u8f83\u9ad8\u7684\u8bef\u62a5\u6982\u7387\u65f6\uff0c\u6240\u63d0\u51fa\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u6bd4\u5148\u524d\u7684\u57fa\u51c6\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2511.08098", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.08098", "abs": "https://arxiv.org/abs/2511.08098", "authors": ["Sabrina Patania", "Luca Annese", "Anita Pellegrini", "Silvia Serino", "Anna Lambiase", "Luca Pallonetto", "Silvia Rossi", "Simone Colombani", "Tom Foulsham", "Azzurra Ruggeri", "Dimitri Ognibene"], "title": "PerspAct: Enhancing LLM Situated Collaboration Skills through Perspective Taking and Active Vision", "comment": "Accepted at IAS19", "summary": "Recent advances in Large Language Models (LLMs) and multimodal foundation models have significantly broadened their application in robotics and collaborative systems. However, effective multi-agent interaction necessitates robust perspective-taking capabilities, enabling models to interpret both physical and epistemic viewpoints. Current training paradigms often neglect these interactive contexts, resulting in challenges when models must reason about the subjectivity of individual perspectives or navigate environments with multiple observers. This study evaluates whether explicitly incorporating diverse points of view using the ReAct framework, an approach that integrates reasoning and acting, can enhance an LLM's ability to understand and ground the demands of other agents. We extend the classic Director task by introducing active visual exploration across a suite of seven scenarios of increasing perspective-taking complexity. These scenarios are designed to challenge the agent's capacity to resolve referential ambiguity based on visual access and interaction, under varying state representations and prompting strategies, including ReAct-style reasoning. Our results demonstrate that explicit perspective cues, combined with active exploration strategies, significantly improve the model's interpretative accuracy and collaborative effectiveness. These findings highlight the potential of integrating active perception with perspective-taking mechanisms in advancing LLMs' application in robotics and multi-agent systems, setting a foundation for future research into adaptive and context-aware AI systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u89c6\u89d2\u548c\u6d3b\u8dc3\u63a2\u7d22\u7b56\u7565\uff0c\u63a2\u8ba8\u4e86\u5982\u4f55\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u548c\u591a\u4ee3\u7406\u7cfb\u7edf\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u8bad\u7ec3\u6a21\u5f0f\u5ffd\u89c6\u4e92\u52a8\u4e0a\u4e0b\u6587\uff0c\u5bfc\u81f4\u5728\u591a\u89c6\u89d2\u73af\u5883\u4e2d\u63a8\u7406\u4e3b\u89c2\u6027\u65b9\u9762\u7684\u56f0\u96be\u3002", "method": "\u5728\u7814\u7a76\u4e2d\u5f15\u5165\u6d3b\u8dc3\u7684\u89c6\u89c9\u63a2\u7d22\u548c\u591a\u6837\u5316\u89c6\u89d2\uff0c\u5229\u7528ReAct\u6846\u67b6\u6765\u589e\u5f3a\u6a21\u578b\u7406\u89e3\u5176\u4ed6\u4ee3\u7406\u9700\u6c42\u7684\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u5229\u7528\u663e\u5f0f\u7684\u89c6\u89d2\u7ebf\u7d22\u548c\u6d3b\u8dc3\u63a2\u7d22\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u89e3\u8bfb\u51c6\u786e\u6027\u548c\u534f\u4f5c\u6709\u6548\u6027\u3002", "conclusion": "\u6574\u5408\u4e3b\u52a8\u611f\u77e5\u548c\u89c6\u89d2\u7406\u89e3\u673a\u5236\u80fd\u591f\u63a8\u52a8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u548c\u591a\u4ee3\u7406\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff0c\u4e3a\u672a\u6765\u7684\u9002\u5e94\u6027\u548c\u4e0a\u4e0b\u6587\u611f\u77e5AI\u7cfb\u7edf\u7684\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2511.07882", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.07882", "abs": "https://arxiv.org/abs/2511.07882", "authors": ["Jessica Gumowski", "Krishna Manaswi Digumarti", "David Howard"], "title": "A Comprehensive Experimental Characterization of Mechanical Layer Jamming Systems", "comment": "6 pages, 9 figures, RoboSoft 2026", "summary": "Organisms in nature, such as Cephalopods and Pachyderms, exploit stiffness modulation to achieve amazing dexterity in the control of their appendages. In this paper, we explore the phenomenon of layer jamming, which is a popular stiffness modulation mechanism that provides an equivalent capability for soft robots. More specifically, we focus on mechanical layer jamming, which we realise through two-layer multi material structure with tooth-like protrusions. We identify key design parameters for mechanical layer jamming systems, including the ability to modulate stiffness, and perform a variety of comprehensive tests placing the specimens under bending and torsional loads to understand the influence of our selected design parameters (mainly tooth geometry) on the performance of the jammed structures. We note the ability of these structures to produce a peak change in stiffness of 5 times in bending and 3.2 times in torsion. We also measure the force required to separate the two jammed layers, an often ignored parameter in the study of jamming-induced stiffness change. This study aims to shed light on the principled design of mechanical layer jammed systems and guide researchers in the selection of appropriate designs for their specific application domains.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u673a\u68b0\u5c42\u95f4\u624e\u5806\u73b0\u8c61\uff0c\u63ed\u793a\u5176\u5728\u8f6f\u673a\u5668\u4eba\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5f3a\u8c03\u4e86\u8bbe\u8ba1\u53c2\u6570\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5229\u7528\u81ea\u7136\u754c\u4e2d\u751f\u7269\u7684\u53d8\u5f62\u80fd\u529b\uff0c\u63a2\u7d22\u8f6f\u673a\u5668\u4eba\u5728\u63a7\u5236\u9644\u80a2\u65b9\u9762\u7684\u7075\u6d3b\u6027", "method": "\u7814\u7a76\u673a\u68b0\u5c42\u95f4\u624e\u5806\u73b0\u8c61\uff0c\u901a\u8fc7\u53cc\u5c42\u591a\u6750\u6599\u7ed3\u6784\u5b9e\u73b0\u786c\u5ea6\u8c03\u8282", "result": "\u786e\u5b9a\u4e86\u5173\u952e\u8bbe\u8ba1\u53c2\u6570\uff0c\u4f7f\u5f97\u7ed3\u6784\u5728\u5f2f\u66f2\u65f6\u786c\u5ea6\u5cf0\u503c\u53d8\u5316\u8fbe\u52305\u500d\uff0c\u5728\u626d\u8f6c\u65f6\u8fbe\u52303.2\u500d\uff0c\u5e76\u6d4b\u91cf\u4e86\u5206\u79bb\u4e24\u4e2a\u624e\u5806\u5c42\u6240\u9700\u7684\u529b", "conclusion": "\u672c\u7814\u7a76\u4e3a\u673a\u68b0\u5c42\u95f4\u624e\u5806\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u539f\u5219\u6307\u5bfc\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u9009\u62e9\u5408\u9002\u7684\u8bbe\u8ba1\u4ee5\u6ee1\u8db3\u7279\u5b9a\u5e94\u7528\u9700\u6c42\u3002"}}
{"id": "2511.08377", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.08377", "abs": "https://arxiv.org/abs/2511.08377", "authors": ["Michael Bowman", "Xiaoli Zhang"], "title": "Human Motion Intent Inferencing in Teleoperation Through a SINDy Paradigm", "comment": "Open source software and video examples here: https://github.com/namwob44/Psychic", "summary": "Intent inferencing in teleoperation has been instrumental in aligning operator goals and coordinating actions with robotic partners. However, current intent inference methods often ignore subtle motion that can be strong indicators for a sudden change in intent. Specifically, we aim to tackle 1) if we can detect sudden jumps in operator trajectories, 2) how we appropriately use these sudden jump motions to infer an operator's goal state, and 3) how to incorporate these discontinuous and continuous dynamics to infer operator motion. Our framework, called Psychic, models these small indicative motions through a jump-drift-diffusion stochastic differential equation to cover discontinuous and continuous dynamics. Kramers-Moyal (KM) coefficients allow us to detect jumps with a trajectory which we pair with a statistical outlier detection algorithm to nominate goal transitions. Through identifying jumps, we can perform early detection of existing goals and discover undefined goals in unstructured scenarios. Our framework then applies a Sparse Identification of Nonlinear Dynamics (SINDy) model using KM coefficients with the goal transitions as a control input to infer an operator's motion behavior in unstructured scenarios. We demonstrate Psychic can produce probabilistic reachability sets and compare our strategy to a negative log-likelihood model fit. We perform a retrospective study on 600 operator trajectories in a hands-free teleoperation task to evaluate the efficacy of our opensource package, Psychic, in both offline and online learning.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u6846\u67b6Psychic\uff0c\u4ee5\u8bc6\u522b\u64cd\u4f5c\u5458\u7684\u610f\u56fe\u53d8\u5316\uff0c\u901a\u8fc7\u7ed3\u5408\u5fae\u5c0f\u8fd0\u52a8\u548c\u7edf\u8ba1\u5206\u6790\uff0c\u589e\u5f3a\u4e86\u8fdc\u7a0b\u64cd\u4f5c\u4e2d\u7684\u610f\u56fe\u63a8\u65ad\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u7684\u610f\u56fe\u63a8\u65ad\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u4e86\u5fae\u5999\u7684\u8fd0\u52a8\uff0c\u8fd9\u4e9b\u8fd0\u52a8\u53ef\u80fd\u662f\u610f\u56fe\u7a81\u7136\u53d8\u5316\u7684\u5f3a\u6307\u793a\u3002", "method": "\u4f7f\u7528\u4e86\u8df3\u8dc3-\u6f02\u79fb-\u6269\u6563\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u548cKramers-Moyal\u7cfb\u6570\u6765\u68c0\u6d4b\u64cd\u4f5c\u5458\u8f68\u8ff9\u4e2d\u7684\u8df3\u8dc3\uff0c\u5e76\u7ed3\u5408\u7edf\u8ba1\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\u548cSINDy\u6a21\u578b\u8fdb\u884c\u610f\u56fe\u63a8\u65ad\u3002", "result": "Psychic\u80fd\u591f\u4ea7\u751f\u6982\u7387\u53ef\u8fbe\u96c6\uff0c\u5e76\u5728600\u4e2a\u64cd\u4f5c\u5458\u7684\u8f68\u8ff9\u4e0a\u8fdb\u884c\u4e86\u56de\u987e\u6027\u7814\u7a76\uff0c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u5f00\u6e90\u5305\u5728\u79bb\u7ebf\u548c\u5728\u7ebf\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6211\u4eec\u7684\u6846\u67b6Psychic\u80fd\u591f\u6709\u6548\u8bc6\u522b\u5e76\u7ed3\u5408\u64cd\u4f5c\u5458\u7684\u8df3\u8dc3\u52a8\u4f5c\uff0c\u4ece\u800c\u63a8\u65ad\u5176\u610f\u56fe\u5e76\u9002\u5e94\u975e\u7ed3\u6784\u5316\u573a\u666f\u4e2d\u7684\u52a8\u6001\u53d8\u5316\u3002"}}
{"id": "2511.07887", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.07887", "abs": "https://arxiv.org/abs/2511.07887", "authors": ["Yinglei Zhu", "Xuguang Dong", "Qiyao Wang", "Qi Shao", "Fugui Xie", "Xinjun Liu", "Huichan Zhao"], "title": "EquiMus: Energy-Equivalent Dynamic Modeling and Simulation of Musculoskeletal Robots Driven by Linear Elastic Actuators", "comment": null, "summary": "Dynamic modeling and control are critical for unleashing soft robots' potential, yet remain challenging due to their complex constitutive behaviors and real-world operating conditions. Bio-inspired musculoskeletal robots, which integrate rigid skeletons with soft actuators, combine high load-bearing capacity with inherent flexibility. Although actuation dynamics have been studied through experimental methods and surrogate models, accurate and effective modeling and simulation remain a significant challenge, especially for large-scale hybrid rigid--soft robots with continuously distributed mass, kinematic loops, and diverse motion modes. To address these challenges, we propose EquiMus, an energy-equivalent dynamic modeling framework and MuJoCo-based simulation for musculoskeletal rigid--soft hybrid robots with linear elastic actuators. The equivalence and effectiveness of the proposed approach are validated and examined through both simulations and real-world experiments on a bionic robotic leg. EquiMus further demonstrates its utility for downstream tasks, including controller design and learning-based control strategies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u52a8\u6001\u5efa\u6a21\u6846\u67b6EquiMus\uff0c\u4ee5\u5e94\u5bf9\u751f\u7269\u542f\u53d1\u7684\u808c\u8089\u9aa8\u9abc\u673a\u5668\u4eba\u5728\u52a8\u6001\u5efa\u6a21\u548c\u63a7\u5236\u4e0a\u7684\u6311\u6218\u3002", "motivation": "\u8f6f\u673a\u5668\u4eba\u5728\u52a8\u6001\u5efa\u6a21\u548c\u63a7\u5236\u65b9\u9762\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\u6df7\u5408\u521a\u67d4\u673a\u5668\u4eba\u4e2d\u9700\u8981\u5904\u7406\u590d\u6742\u7684\u8fd0\u52a8\u6a21\u5f0f\u548c\u5b9e\u65f6\u64cd\u4f5c\u6761\u4ef6\u3002", "method": "\u5efa\u7acb\u4e86\u4e00\u79cd\u57fa\u4e8e\u80fd\u91cf\u5e73\u8861\u7684\u52a8\u6001\u5efa\u6a21\u6846\u67b6\uff0c\u5e76\u4f7f\u7528MuJoCo\u8fdb\u884c\u4eff\u771f\u3002", "result": "\u901a\u8fc7\u5bf9\u4eff\u771f\u548c\u4eff\u751f\u673a\u5668\u817f\u7684\u771f\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1\u4e86EquiMus\u7684\u7b49\u6548\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "EquiMus\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u63a7\u5236\u8bbe\u8ba1\u548c\u5b66\u4e60\u63a7\u5236\u7b56\u7565\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2511.07921", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.07921", "abs": "https://arxiv.org/abs/2511.07921", "authors": ["Byeong-Il Ham", "Hyun-Bin Kim", "Jeonguk Kang", "Keun Ha Choi", "Kyung-Soo Kim"], "title": "Dual-MPC Footstep Planning for Robust Quadruped Locomotion", "comment": "9 pages, 9 figures", "summary": "In this paper, we propose a footstep planning strategy based on model predictive control (MPC) that enables robust regulation of body orientation against undesired body rotations by optimizing footstep placement. Model-based locomotion approaches typically adopt heuristic methods or planning based on the linear inverted pendulum model. These methods account for linear velocity in footstep planning, while excluding angular velocity, which leads to angular momentum being handled exclusively via ground reaction force (GRF). Footstep planning based on MPC that takes angular velocity into account recasts the angular momentum control problem as a dual-input approach that coordinates GRFs and footstep placement, instead of optimizing GRFs alone, thereby improving tracking performance. A mutual-feedback loop couples the footstep planner and the GRF MPC, with each using the other's solution to iteratively update footsteps and GRFs. The use of optimal solutions reduces body oscillation and enables extended stance and swing phases. The method is validated on a quadruped robot, demonstrating robust locomotion with reduced oscillations, longer stance and swing phases across various terrains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8eMPC\u7684\u6b65\u6001\u89c4\u5212\u65b9\u6cd5\uff0c\u4ee5\u8003\u8651\u89d2\u901f\u5ea6\u4e3a\u57fa\u7840\uff0c\u4f18\u5316\u8db3\u8ff9\u653e\u7f6e\uff0c\u63d0\u9ad8\u4e86\u8ddf\u8e2a\u6027\u80fd\u5e76\u51cf\u5c11\u4e86\u8eab\u4f53\u632f\u8361\u3002", "motivation": "\u4f20\u7edf\u7684\u6b65\u6001\u89c4\u5212\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u89d2\u901f\u5ea6\uff0c\u5bf9\u8eab\u4f53\u7684\u65cb\u8f6c\u63a7\u5236\u4e0d\u591f\u6709\u6548\uff0c\u5bfc\u81f4\u4ec5\u4f9d\u8d56\u5730\u9762\u53cd\u4f5c\u7528\u529b\u6765\u5904\u7406\u89d2\u52a8\u91cf\uff0c\u9650\u5236\u4e86\u6027\u80fd\u3002", "method": "\u91c7\u7528\u6a21\u578b\u9884\u6d4b\u63a7\u5236(MPC)\u7b56\u7565\u8fdb\u884c\u6b65\u6001\u89c4\u5212\uff0c\u6574\u5408\u4e86\u89d2\u901f\u5ea6\u548c\u5730\u9762\u53cd\u4f5c\u7528\u529b(GRF)\u7684\u53cc\u8f93\u5165\u63a7\u5236\u3002", "result": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u9884\u6d4b\u63a7\u5236(MPC)\u7684\u6b65\u6001\u89c4\u5212\u7b56\u7565\uff0c\u80fd\u591f\u901a\u8fc7\u4f18\u5316\u8db3\u8ff9\u653e\u7f6e\u6765\u589e\u5f3a\u8eab\u4f53\u671d\u5411\u7684\u7a33\u5065\u6027\uff0c\u62b5\u6297\u4e0d\u5fc5\u8981\u7684\u8eab\u4f53\u65cb\u8f6c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0c\u663e\u793a\u51fa\u7a33\u5065\u7684\u79fb\u52a8\u80fd\u529b\uff0c\u51cf\u5c11\u632f\u8361\uff0c\u5ef6\u957f\u4e86\u7ad9\u7acb\u548c\u6446\u52a8\u9636\u6bb5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5730\u5f62\u3002"}}
{"id": "2511.07927", "categories": ["cs.RO", "math.DS", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.07927", "abs": "https://arxiv.org/abs/2511.07927", "authors": ["Okan Arif Guvenkaya", "Selim Ahmet Iz", "Mustafa Unel"], "title": "Local Path Planning with Dynamic Obstacle Avoidance in Unstructured Environments", "comment": null, "summary": "Obstacle avoidance and path planning are essential for guiding unmanned ground vehicles (UGVs) through environments that are densely populated with dynamic obstacles. This paper develops a novel approach that combines tangentbased path planning and extrapolation methods to create a new decision-making algorithm for local path planning. In the assumed scenario, a UGV has a prior knowledge of its initial and target points within the dynamic environment. A global path has already been computed, and the robot is provided with waypoints along this path. As the UGV travels between these waypoints, the algorithm aims to avoid collisions with dynamic obstacles. These obstacles follow polynomial trajectories, with their initial positions randomized in the local map and velocities randomized between O and the allowable physical velocity limit of the robot, along with some random accelerations. The developed algorithm is tested in several scenarios where many dynamic obstacles move randomly in the environment. Simulation results show the effectiveness of the proposed local path planning strategy by gradually generating a collision free path which allows the robot to navigate safely between initial and the target locations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5c40\u90e8\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5207\u7ebf\u8def\u5f84\u89c4\u5212\u548c\u5916\u63a8\u65b9\u6cd5\uff0c\u5b9e\u73b0UGV\u5728\u52a8\u6001\u969c\u788d\u7269\u73af\u5883\u4e2d\u7684\u5b89\u5168\u5bfc\u822a\u3002", "motivation": "\u5728\u52a8\u6001\u969c\u788d\u7269\u5bc6\u96c6\u7684\u73af\u5883\u4e2d\uff0c\u4e3a\u65e0\u4eba\u5730\u9762\u8f66\u8f86\uff08UGV\uff09\u63d0\u4f9b\u6709\u6548\u7684\u969c\u788d\u7269\u907f\u514d\u548c\u8def\u5f84\u89c4\u5212\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u57fa\u4e8e\u5207\u7ebf\u7684\u8def\u5f84\u89c4\u5212\u548c\u5916\u63a8\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u51b3\u7b56\u7b97\u6cd5\u6765\u8fdb\u884c\u5c40\u90e8\u8def\u5f84\u89c4\u5212\u3002", "result": "\u901a\u8fc7\u5728\u591a\u4e2a\u573a\u666f\u4e2d\u6d4b\u8bd5\uff0c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u51fa\u7684\u5c40\u90e8\u8def\u5f84\u89c4\u5212\u7b56\u7565\u5728\u751f\u6210\u65e0\u78b0\u649e\u8def\u5f84\u65b9\u9762\u975e\u5e38\u6709\u6548\uff0cUGV\u80fd\u591f\u5b89\u5168\u5bfc\u822a\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5c40\u90e8\u8def\u5f84\u89c4\u5212\u7b56\u7565\u80fd\u591f\u6709\u6548\u5730\u751f\u6210\u65e0\u78b0\u649e\u8def\u5f84\uff0c\u4f7fUGV\u80fd\u591f\u5b89\u5168\u5730\u5728\u521d\u59cb\u4f4d\u7f6e\u548c\u76ee\u6807\u4f4d\u7f6e\u4e4b\u95f4\u5bfc\u822a\u3002"}}
{"id": "2511.07950", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07950", "abs": "https://arxiv.org/abs/2511.07950", "authors": ["Yara AlaaEldin", "Enrico Simetti", "Francesca Odone"], "title": "USV Obstacles Detection and Tracking in Marine Environments", "comment": null, "summary": "Developing a robust and effective obstacle detection and tracking system for Unmanned Surface Vehicle (USV) at marine environments is a challenging task. Research efforts have been made in this area during the past years by GRAAL lab at the university of Genova that resulted in a methodology for detecting and tracking obstacles on the image plane and, then, locating them in the 3D LiDAR point cloud. In this work, we continue on the developed system by, firstly, evaluating its performance on recently published marine datasets. Then, we integrate the different blocks of the system on ROS platform where we could test it in real-time on synchronized LiDAR and camera data collected in various marine conditions available in the MIT marine datasets. We present a thorough experimental analysis of the results obtained using two approaches; one that uses sensor fusion between the camera and LiDAR to detect and track the obstacles and the other uses only the LiDAR point cloud for the detection and tracking. In the end, we propose a hybrid approach that merges the advantages of both approaches to build an informative obstacles map of the surrounding environment to the USV.", "AI": {"tldr": "\u672c\u8bba\u6587\u7814\u7a76\u4e86\u65e0\u4eba\u6c34\u9762\u8f66\u8f86\u7684\u969c\u788d\u7269\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u7cfb\u7edf\uff0c\u6574\u5408\u4e86\u76f8\u673a\u548cLiDAR\u6570\u636e\uff0c\u5e76\u63d0\u51fa\u4e86\u878d\u5408\u4e24\u8005\u4f18\u70b9\u7684\u6df7\u5408\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u65e0\u4eba\u6c34\u9762\u8f66\u8f86\u5728\u6d77\u6d0b\u73af\u5883\u4e2d\u969c\u788d\u7269\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u7684\u6311\u6218\uff0c\u63d0\u5347\u5176\u6027\u80fd\u548c\u5b89\u5168\u6027\u3002", "method": "\u8bc4\u4f30\u73b0\u6709\u7cfb\u7edf\u5728\u6d77\u6d0b\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u5728ROS\u5e73\u53f0\u4e0a\u5b9e\u65f6\u6d4b\u8bd5\uff0c\u4f7f\u7528\u4e24\u79cd\u65b9\u6cd5\uff1a\u4f20\u611f\u5668\u878d\u5408\u548c\u5355\u4e00LiDAR\u70b9\u4e91\u68c0\u6d4b\u3002", "result": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u5065\u58ee\u6709\u6548\u7684\u65e0\u4eba\u6c34\u9762\u8f66\u8f86\uff08USV\uff09\u969c\u788d\u7269\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u7cfb\u7edf\uff0c\u7ecf\u8fc7\u4e0eMIT\u6d77\u6d0b\u6570\u636e\u96c6\u7684\u5b9e\u5730\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86\u8be5\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u7684\u6df7\u5408\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u878d\u5408\u76f8\u673a\u548cLiDAR\u7684\u6570\u636e\uff0c\u751f\u6210\u5468\u56f4\u73af\u5883\u7684\u969c\u788d\u7269\u5730\u56fe\uff0c\u63d0\u5347USV\u7684\u5bfc\u822a\u80fd\u529b\u3002"}}
{"id": "2511.08001", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.08001", "abs": "https://arxiv.org/abs/2511.08001", "authors": ["Avishav Engle", "Andrey Zhitnikov", "Oren Salzman", "Omer Ben-Porat", "Kiril Solovey"], "title": "Effective Game-Theoretic Motion Planning via Nested Search", "comment": null, "summary": "To facilitate effective, safe deployment in the real world, individual robots must reason about interactions with other agents, which often occur without explicit communication. Recent work has identified game theory, particularly the concept of Nash Equilibrium (NE), as a key enabler for behavior-aware decision-making. Yet, existing work falls short of fully unleashing the power of game-theoretic reasoning. Specifically, popular optimization-based methods require simplified robot dynamics and tend to get trapped in local minima due to convexification. Other works that rely on payoff matrices suffer from poor scalability due to the explicit enumeration of all possible trajectories. To bridge this gap, we introduce Game-Theoretic Nested Search (GTNS), a novel, scalable, and provably correct approach for computing NEs in general dynamical systems. GTNS efficiently searches the action space of all agents involved, while discarding trajectories that violate the NE constraint (no unilateral deviation) through an inner search over a lower-dimensional space. Our algorithm enables explicit selection among equilibria by utilizing a user-specified global objective, thereby capturing a rich set of realistic interactions. We demonstrate the approach on a variety of autonomous driving and racing scenarios where we achieve solutions in mere seconds on commodity hardware.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGTNS\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u52a8\u6001\u7cfb\u7edf\u4e2d\u9ad8\u6548\u8ba1\u7b97\u7eb3\u4ec0\u5747\u8861\uff0c\u514b\u670d\u73b0\u6709\u535a\u5f08\u7406\u8bba\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u673a\u5668\u4eba\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u5b89\u5168\u6709\u6548\u7684\u90e8\u7f72\uff0c\u673a\u5668\u4eba\u9700\u8981\u5728\u6ca1\u6709\u660e\u786e\u6c9f\u901a\u7684\u60c5\u51b5\u4e0b\u63a8\u7406\u4e0e\u5176\u4ed6\u4ee3\u7406\u7684\u4e92\u52a8\u3002", "method": "\u901a\u8fc7\u5728\u8f83\u4f4e\u7ef4\u7a7a\u95f4\u4e2d\u8fdb\u884c\u5185\u641c\u7d22\u6765\u6709\u6548\u5730\u63a2\u7d22\u6240\u6709\u4ee3\u7406\u7684\u884c\u52a8\u7a7a\u95f4\uff0c\u540c\u65f6\u6392\u9664\u4e0d\u6ee1\u8db3\u7eb3\u4ec0\u5747\u8861\u7ea6\u675f\u7684\u8f68\u8ff9\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u535a\u5f08\u8bba\u5d4c\u5957\u641c\u7d22\uff08GTNS\uff09\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u51b3\u7b56\u4e2d\u9047\u5230\u7684\u5c40\u9650\u6027\u3002GTNS\u80fd\u591f\u5728\u4e00\u822c\u52a8\u6001\u7cfb\u7edf\u4e2d\u6709\u6548\u641c\u7d22\u6240\u6709\u4ee3\u7406\u7684\u884c\u52a8\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u5728\u8f83\u4f4e\u7ef4\u5ea6\u7a7a\u95f4\u5185\u7684\u5185\u641c\u7d22\u6765\u6392\u9664\u8fdd\u53cd\u7eb3\u4ec0\u5747\u8861\uff08NE\uff09\u7ea6\u675f\u7684\u8f68\u8ff9\uff0c\u5229\u7528\u7528\u6237\u6307\u5b9a\u7684\u5168\u5c40\u76ee\u6807\u660e\u786e\u9009\u62e9\u5747\u8861\u3002", "conclusion": "GTNS\u65b9\u6cd5\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u5728\u52a8\u6001\u7cfb\u7edf\u4e2d\u53ef\u8bc1\u660e\u6b63\u786e\u7684\u7b97\u6cd5\uff0c\u80fd\u5feb\u901f\u9002\u5e94\u5404\u79cd\u81ea\u4e3b\u9a7e\u9a76\u548c\u8d5b\u8f66\u60c5\u666f\u3002"}}
{"id": "2511.08005", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.08005", "abs": "https://arxiv.org/abs/2511.08005", "authors": ["Huacen Wang", "Hongqiang Wang"], "title": "A Two-Layer Electrostatic Film Actuator with High Actuation Stress and Integrated Brake", "comment": null, "summary": "Robotic systems driven by conventional motors often suffer from challenges such as large mass, complex control algorithms, and the need for additional braking mechanisms, which limit their applications in lightweight and compact robotic platforms. Electrostatic film actuators offer several advantages, including thinness, flexibility, lightweight construction, and high open-loop positioning accuracy. However, the actuation stress exhibited by conventional actuators in air still needs improvement, particularly for the widely used three-phase electrode design. To enhance the output performance of actuators, this paper presents a two-layer electrostatic film actuator with an integrated brake. By alternately distributing electrodes on both the top and bottom layers, a smaller effective electrode pitch is achieved under the same fabrication constraints, resulting in an actuation stress of approximately 241~N/m$^2$, representing a 90.5\\% improvement over previous three-phase actuators operating in air. Furthermore, its integrated electrostatic adhesion mechanism enables load retention under braking mode. Several demonstrations, including a tug-of-war between a conventional single-layer actuator and the proposed two-layer actuator, a payload operation, a one-degree-of-freedom robotic arm, and a dual-mode gripper, were conducted to validate the actuator's advantageous capabilities in both actuation and braking modes.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u53cc\u5c42\u7535\u9759\u6001\u8584\u819c\u6267\u884c\u5668\uff0c\u5177\u6709\u96c6\u6210\u5236\u52a8\u529f\u80fd\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6267\u884c\u5668\u7684\u8f93\u51fa\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7535\u673a\u9a71\u52a8\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u5b58\u5728\u8d28\u91cf\u5927\u3001\u63a7\u5236\u590d\u6742\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u8f7b\u91cf\u5316\u548c\u7d27\u51d1\u578b\u673a\u5668\u4eba\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u5728\u4e0a\u4e0b\u4e24\u5c42\u4ea4\u66ff\u5206\u5e03\u7535\u6781\uff0c\u5b9e\u73b0\u4e86\u8f83\u5c0f\u7684\u6709\u6548\u7535\u6781\u95f4\u8ddd\uff0c\u63d0\u5347\u4e86\u6267\u884c\u5e94\u529b\u3002", "result": "\u53cc\u5c42\u6267\u884c\u5668\u7684\u8f93\u51fa\u6027\u80fd\u663e\u8457\u63d0\u9ad8\uff0c\u6267\u884c\u5e94\u529b\u8fbe\u5230\u7ea6241~N/m\u00b2\uff0c\u76f8\u8f83\u4e8e\u4ee5\u5f80\u4e09\u76f8\u6267\u884c\u5668\u63d0\u5347\u4e8690.5%\u3002", "conclusion": "\u53cc\u5c42\u7535\u9759\u6001\u8584\u819c\u6267\u884c\u5668\u5728\u9a71\u52a8\u548c\u5236\u52a8\u6a21\u5f0f\u4e0b\u5177\u6709\u4f18\u8d8a\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u8f7b\u91cf\u5316\u548c\u7d27\u51d1\u578b\u673a\u5668\u4eba\u5e73\u53f0\u3002"}}
{"id": "2511.08016", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.08016", "abs": "https://arxiv.org/abs/2511.08016", "authors": ["Adrian Sch\u00f6nnagel", "Michael Dub\u00e9", "Christoph Steup", "Felix Keppler", "Sanaz Mostaghim"], "title": "AVOID-JACK: Avoidance of Jackknifing for Swarms of Long Heavy Articulated Vehicles", "comment": "6+1 pages, 9 figures, accepted for publication in IEEE MRS 2025", "summary": "This paper presents a novel approach to avoiding jackknifing and mutual collisions in Heavy Articulated Vehicles (HAVs) by leveraging decentralized swarm intelligence. In contrast to typical swarm robotics research, our robots are elongated and exhibit complex kinematics, introducing unique challenges. Despite its relevance to real-world applications such as logistics automation, remote mining, airport baggage transport, and agricultural operations, this problem has not been addressed in the existing literature.\n  To tackle this new class of swarm robotics problems, we propose a purely reaction-based, decentralized swarm intelligence strategy tailored to automate elongated, articulated vehicles. The method presented in this paper prioritizes jackknifing avoidance and establishes a foundation for mutual collision avoidance. We validate our approach through extensive simulation experiments and provide a comprehensive analysis of its performance. For the experiments with a single HAV, we observe that for 99.8% jackknifing was successfully avoided and that 86.7% and 83.4% reach their first and second goals, respectively. With two HAVs interacting, we observe 98.9%, 79.4%, and 65.1%, respectively, while 99.7% of the HAVs do not experience mutual collisions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u53bb\u4e2d\u5fc3\u5316\u7fa4\u4f53\u667a\u80fd\u907f\u514d\u91cd\u578b\u5173\u8282\u8f66\u8f86\u7684\u6298\u53e0\u548c\u76f8\u4e92\u78b0\u649e\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7528\u4ee5\u89e3\u51b3\u91cd\u578b\u5173\u8282\u8f66\u8f86\u5728\u7269\u6d41\u81ea\u52a8\u5316\u3001\u8fdc\u7a0b\u91c7\u77ff\u3001\u673a\u573a\u884c\u674e\u8fd0\u8f93\u548c\u519c\u4e1a\u4f5c\u4e1a\u4e2d\u907f\u514d\u6298\u53e0\u548c\u76f8\u4e92\u78b0\u649e\u7684\u95ee\u9898\uff0c\u800c\u6b64\u95ee\u9898\u5728\u73b0\u6709\u6587\u732e\u4e2d\u5c1a\u672a\u89e3\u51b3\u3002", "method": "\u57fa\u4e8e\u53cd\u5e94\u7684\u53bb\u4e2d\u5fc3\u5316\u7fa4\u4f53\u667a\u80fd\u7b56\u7565", "result": "\u5728\u5355\u4e00\u91cd\u578b\u5173\u8282\u8f66\u8f86(HAV)\u5b9e\u9a8c\u4e2d\uff0c\u5b9e\u73b0\u4e8699.8%\u7684\u6298\u53e0\u907f\u514d\u7387\uff0c86.7%\u548c83.4%\u7684HAV\u6210\u529f\u8fbe\u5230\u5176\u7b2c\u4e00\u4e2a\u548c\u7b2c\u4e8c\u4e2a\u76ee\u6807\u3002\u5728\u4e24\u4e2aHAV\u76f8\u4e92\u4f5c\u7528\u7684\u5b9e\u9a8c\u4e2d\uff0c\u5b9e\u73b0\u4e8698.9%\u300179.4%\u548c65.1%\u7684\u76ee\u6807\u8fbe\u5230\u7387\uff0c\u4e1499.7%\u7684HAV\u672a\u53d1\u751f\u76f8\u4e92\u78b0\u649e\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u5730\u907f\u514d\u4e86\u91cd\u578b\u5173\u8282\u8f66\u8f86\u7684\u6298\u53e0\u548c\u78b0\u649e\uff0c\u4e3a\u81ea\u52a8\u5316\u5e94\u7528\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.08019", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.08019", "abs": "https://arxiv.org/abs/2511.08019", "authors": ["Kohei Honda"], "title": "Model Predictive Control via Probabilistic Inference: A Tutorial", "comment": "15 pages, 7 figures", "summary": "Model Predictive Control (MPC) is a fundamental framework for optimizing robot behavior over a finite future horizon. While conventional numerical optimization methods can efficiently handle simple dynamics and cost structures, they often become intractable for the nonlinear or non-differentiable systems commonly encountered in robotics. This article provides a tutorial on probabilistic inference-based MPC, presenting a unified theoretical foundation and a comprehensive overview of representative methods. Probabilistic inference-based MPC approaches, such as Model Predictive Path Integral (MPPI) control, have gained significant attention by reinterpreting optimal control as a problem of probabilistic inference. Rather than relying on gradient-based numerical optimization, these methods estimate optimal control distributions through sampling-based techniques, accommodating arbitrary cost functions and dynamics. We first derive the optimal control distribution from the standard optimal control problem, elucidating its probabilistic interpretation and key characteristics. The widely used MPPI algorithm is then derived as a practical example, followed by discussions on prior and variational distribution design, tuning principles, and theoretical aspects. This article aims to serve as a systematic guide for researchers and practitioners seeking to understand, implement, and extend these methods in robotics and beyond.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u57fa\u4e8e\u6982\u7387\u63a8\u65ad\u7684MPC\u65b9\u6cd5\uff0c\u8fd9\u79cd\u65b9\u6cd5\u901a\u8fc7\u91c7\u6837\u6765\u4f30\u8ba1\u6700\u4f18\u63a7\u5236\u5206\u5e03\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7684\u673a\u5668\u4eba\u52a8\u529b\u5b66\u548c\u6210\u672c\u51fd\u6570\u3002", "motivation": "\u4f20\u7edf\u7684\u6570\u503c\u4f18\u5316\u65b9\u6cd5\u5728\u9762\u5bf9\u590d\u6742\u7684\u673a\u5668\u4eba\u52a8\u6001\u548c\u6210\u672c\u7ed3\u6784\u65f6\u6548\u7387\u4e0b\u964d\uff0c\u6025\u9700\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u5904\u7406\u8fd9\u4e9b\u60c5\u51b5\u3002", "method": "\u901a\u8fc7\u63a8\u5bfc\u6700\u4f18\u63a7\u5236\u5206\u5e03\uff0c\u5e76\u5229\u7528\u91c7\u6837\u6280\u672f\u6765\u4f30\u8ba1\u6b64\u5206\u5e03\uff0c\u5177\u4f53\u4f8b\u5b50\u4e3aMPPI\u7b97\u6cd5\u3002", "result": "\u672c\u6587\u63d0\u4f9b\u4e86\u57fa\u4e8e\u6982\u7387\u63a8\u65ad\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u65b9\u6cd5\u7684\u7efc\u5408\u7406\u8bba\u57fa\u7840\u548c\u65b9\u6cd5\u6982\u8ff0\uff0c\u7279\u522b\u5173\u6ce8MPPI\u7b97\u6cd5\uff0c\u5f3a\u8c03\u5176\u5728\u673a\u5668\u4eba\u9886\u57df\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "\u57fa\u4e8e\u6982\u7387\u63a8\u65ad\u7684MPC\u4e3a\u89e3\u51b3\u975e\u7ebf\u6027\u6216\u4e0d\u53ef\u5fae\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5e76\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6307\u5357\u3002"}}
{"id": "2511.08214", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.08214", "abs": "https://arxiv.org/abs/2511.08214", "authors": ["Yi Huang", "Zhan Qu", "Lihui Jiang", "Bingbing Liu", "Hongbo Zhang"], "title": "Prioritizing Perception-Guided Self-Supervision: A New Paradigm for Causal Modeling in End-to-End Autonomous Driving", "comment": "Accepted at NeurIPS 2025", "summary": "End-to-end autonomous driving systems, predominantly trained through imitation learning, have demonstrated considerable effectiveness in leveraging large-scale expert driving data. Despite their success in open-loop evaluations, these systems often exhibit significant performance degradation in closed-loop scenarios due to causal confusion. This confusion is fundamentally exacerbated by the overreliance of the imitation learning paradigm on expert trajectories, which often contain unattributable noise and interfere with the modeling of causal relationships between environmental contexts and appropriate driving actions.\n  To address this fundamental limitation, we propose Perception-Guided Self-Supervision (PGS) - a simple yet effective training paradigm that leverages perception outputs as the primary supervisory signals, explicitly modeling causal relationships in decision-making. The proposed framework aligns both the inputs and outputs of the decision-making module with perception results, such as lane centerlines and the predicted motions of surrounding agents, by introducing positive and negative self-supervision for the ego trajectory. This alignment is specifically designed to mitigate causal confusion arising from the inherent noise in expert trajectories.\n  Equipped with perception-driven supervision, our method, built on a standard end-to-end architecture, achieves a Driving Score of 78.08 and a mean success rate of 48.64% on the challenging closed-loop Bench2Drive benchmark, significantly outperforming existing state-of-the-art methods, including those employing more complex network architectures and inference pipelines. These results underscore the effectiveness and robustness of the proposed PGS framework and point to a promising direction for addressing causal confusion and enhancing real-world generalization in autonomous driving.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u57f9\u8bad\u8303\u5f0fPGS\uff0c\u901a\u8fc7\u611f\u77e5\u8f93\u51fa\u8fdb\u884c\u81ea\u6211\u76d1\u7763\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u56e0\u679c\u6df7\u6dc6\uff0c\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5728\u95ed\u73af\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u7531\u4e8e\u5bf9\u4e13\u5bb6\u8f68\u8ff9\u7684\u8fc7\u5ea6\u4f9d\u8d56\u5bfc\u81f4\u56e0\u679c\u6df7\u6dc6\u3002", "method": "\u63d0\u51fa\u611f\u77e5\u5f15\u5bfc\u81ea\u6211\u76d1\u7763(PGS)\u8bad\u7ec3\u8303\u5f0f\uff0c\u4ee5\u611f\u77e5\u8f93\u51fa\u4f5c\u4e3a\u4e3b\u8981\u76d1\u7763\u4fe1\u53f7\uff0c\u5efa\u6a21\u51b3\u7b56\u4e2d\u7684\u56e0\u679c\u5173\u7cfb\u3002", "result": "\u5728\u95ed\u73afBench2Drive\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53d6\u5f97\u4e8678.08\u7684\u9a7e\u9a76\u8bc4\u5206\u548c48.64%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "PGS\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u56e0\u679c\u6df7\u6dc6\u7684\u95ee\u9898\uff0c\u4e3a\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u7684\u73b0\u5b9e\u4e16\u754c\u6cdb\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2511.08231", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.08231", "abs": "https://arxiv.org/abs/2511.08231", "authors": ["Devin Hunter", "Chinwendu Enyioha"], "title": "Real-Time Performance Analysis of Multi-Fidelity Residual Physics-Informed Neural Process-Based State Estimation for Robotic Systems", "comment": "8 pages, 5 figures", "summary": "Various neural network architectures are used in many of the state-of-the-art approaches for real-time nonlinear state estimation. With the ever-increasing incorporation of these data-driven models into the estimation domain, model predictions with reliable margins of error are a requirement -- especially for safety-critical applications. This paper discusses the application of a novel real-time, data-driven estimation approach based on the multi-fidelity residual physics-informed neural process (MFR-PINP) toward the real-time state estimation of a robotic system. Specifically, we address the model-mismatch issue of selecting an accurate kinematic model by tasking the MFR-PINP to also learn the residuals between simple, low-fidelity predictions and complex, high-fidelity ground-truth dynamics. To account for model uncertainty present in a physical implementation, robust uncertainty guarantees from the split conformal (SC) prediction framework are modeled in the training and inference paradigms. We provide implementation details of our MFR-PINP-based estimator for a hybrid online learning setting to validate our model's usage in real-time applications. Experimental results of our approach's performance in comparison to the state-of-the-art variants of the Kalman filter (i.e. unscented Kalman filter and deep Kalman filter) in estimation scenarios showed promising results for the MFR-PINP model as a viable option in real-time estimation tasks.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8eMFR-PINP\u7684\u5b9e\u65f6\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u9488\u5bf9\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u9ad8\u4fdd\u771f\u5ea6\u548c\u4f4e\u4fdd\u771f\u5ea6\u9884\u6d4b\u95f4\u7684\u6b8b\u5dee\u8fdb\u884c\u5b66\u4e60\uff0c\u5177\u6709\u826f\u597d\u7684\u53ef\u9760\u6027\u548c\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u6570\u636e\u9a71\u52a8\u6a21\u578b\u5728\u72b6\u6001\u4f30\u8ba1\u9886\u57df\u7684\u5e94\u7528\u65e5\u76ca\u589e\u52a0\uff0c\u5bf9\u4e8e\u5177\u6709\u53ef\u9760\u8bef\u5dee\u8303\u56f4\u7684\u6a21\u578b\u9884\u6d4b\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u663e\u5f97\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMFR-PINP\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u6a21\u578b\u7684\u6b8b\u5dee\uff0c\u5e76\u7ed3\u5408\u5206\u88c2\u4fdd\u5f62\u9884\u6d4b\u6846\u67b6\u7684\u7a33\u5065\u4e0d\u786e\u5b9a\u6027\u4fdd\u8bc1\uff0c\u5728\u8bad\u7ec3\u548c\u63a8\u65ad\u8fc7\u7a0b\u4e2d\u589e\u5f3a\u72b6\u6001\u4f30\u8ba1\u7684\u53ef\u9760\u6027\u3002", "result": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u4fdd\u771f\u5ea6\u6b8b\u5dee\u7269\u7406\u542f\u53d1\u795e\u7ecf\u8fc7\u7a0b\uff08MFR-PINP\uff09\u7684\u65b0\u578b\u5b9e\u65f6\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5e76\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u3002\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u578b\u5339\u914d\u95ee\u9898\uff0c\u901a\u8fc7\u5b66\u4e60\u7b80\u5355\u4f4e\u4fdd\u771f\u5ea6\u9884\u6d4b\u548c\u590d\u6742\u9ad8\u4fdd\u771f\u5ea6\u52a8\u6001\u4e4b\u95f4\u7684\u6b8b\u5dee\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMFR-PINP\u5728\u4e0e\u73b0\u6709\u7684\u5361\u5c14\u66fc\u6ee4\u6ce2\u53d8\u4f53\uff08\u5982\u65e0\u8ff9\u5361\u5c14\u66fc\u6ee4\u6ce2\u548c\u6df1\u5ea6\u5361\u5c14\u66fc\u6ee4\u6ce2\uff09\u6bd4\u8f83\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u662f\u5b9e\u65f6\u72b6\u6001\u4f30\u8ba1\u4efb\u52a1\u7684\u4e00\u4e2a\u53ef\u884c\u9009\u9879\u3002"}}
{"id": "2511.08277", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.08277", "abs": "https://arxiv.org/abs/2511.08277", "authors": ["Dehan Shen", "Changhao Chen"], "title": "X-IONet: Cross-Platform Inertial Odometry Network with Dual-Stage Attention", "comment": null, "summary": "Learning-based inertial odometry has achieved remarkable progress in pedestrian navigation. However, extending these methods to quadruped robots remains challenging due to their distinct and highly dynamic motion patterns. Models that perform well on pedestrian data often experience severe degradation when deployed on legged platforms. To tackle this challenge, we introduce X-IONet, a cross-platform inertial odometry framework that operates solely using a single Inertial Measurement Unit (IMU). X-IONet incorporates a rule-based expert selection module to classify motion platforms and route IMU sequences to platform-specific expert networks. The displacement prediction network features a dual-stage attention architecture that jointly models long-range temporal dependencies and inter-axis correlations, enabling accurate motion representation. It outputs both displacement and associated uncertainty, which are further fused through an Extended Kalman Filter (EKF) for robust state estimation. Extensive experiments on public pedestrian datasets and a self-collected quadruped robot dataset demonstrate that X-IONet achieves state-of-the-art performance, reducing Absolute Trajectory Error (ATE) by 14.3% and Relative Trajectory Error (RTE) by 11.4% on pedestrian data, and by 52.8% and 41.3% on quadruped robot data. These results highlight the effectiveness of X-IONet in advancing accurate and robust inertial navigation across both human and legged robot platforms.", "AI": {"tldr": "X-IONet\u662f\u4e00\u79cd\u521b\u65b0\u7684\u60ef\u6027\u5bfc\u822a\u6846\u67b6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4eba\u7c7b\u548c\u56db\u8db3\u673a\u5668\u4eba\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u7531\u4e8e\u6b65\u6001\u5dee\u5f02\uff0c\u73b0\u6709\u6b65\u884c\u8005\u6570\u636e\u4e0a\u7684\u6a21\u578b\u5728\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u5f00\u53d1\u901a\u7528\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5355\u4e2aIMU\u7684\u8de8\u5e73\u53f0\u60ef\u6027\u91cc\u7a0b\u8ba1\u6846\u67b6\uff0c\u5e76\u7ed3\u5408\u4e86\u4e13\u5bb6\u9009\u62e9\u6a21\u5757\u3001\u53cc\u9636\u6bb5\u6ce8\u610f\u529b\u67b6\u6784\u548c\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u3002", "result": "\u5728\u6b65\u884c\u8005\u6570\u636e\u96c6\u4e0a\uff0cATE\u964d\u4f4e14.3%\uff0cRTE\u964d\u4f4e11.4%\uff1b\u5728\u56db\u8db3\u673a\u5668\u4eba\u6570\u636e\u96c6\u4e0a\uff0cATE\u964d\u4f4e52.8%\uff0cRTE\u964d\u4f4e41.3%\u3002", "conclusion": "X-IONet\u5728\u6b65\u884c\u8005\u548c\u56db\u8db3\u673a\u5668\u4eba\u7684\u60ef\u6027\u5bfc\u822a\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4f4d\u7f6e\u8bef\u5dee\u3002"}}
{"id": "2511.08299", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.08299", "abs": "https://arxiv.org/abs/2511.08299", "authors": ["Zhiang Liu", "Yang Liu", "Yongchun Fang", "Xian Guo"], "title": "Learning Omnidirectional Locomotion for a Salamander-Like Quadruped Robot", "comment": null, "summary": "Salamander-like quadruped robots are designed inspired by the skeletal structure of their biological counterparts. However, existing controllers cannot fully exploit these morphological features and largely rely on predefined gait patterns or joint trajectories, which prevents the generation of diverse and flexible locomotion and limits their applicability in real-world scenarios. In this paper, we propose a learning framework that enables the robot to acquire a diverse repertoire of omnidirectional gaits without reference motions. Each body part is controlled by a phase variable capable of forward and backward evolution, with a phase coverage reward to promote the exploration of the leg phase space. Additionally, morphological symmetry of the robot is incorporated via data augmentation, improving sample efficiency and enforcing both motion-level and task-level symmetry in learned behaviors. Extensive experiments show that the robot successfully acquires 22 omnidirectional gaits exhibiting both dynamic and symmetric movements, demonstrating the effectiveness of the proposed learning framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u5f97\u706b\u8725\u8734\u98ce\u683c\u7684\u56db\u8db3\u673a\u5668\u4eba\u80fd\u591f\u65e0\u53c2\u8003\u8fd0\u52a8\u5730\u5b66\u4e60\u591a\u6837\u5316\u7684\u5168\u5411\u6b65\u6001\uff0c\u6210\u529f\u5c55\u793a\u4e86\u52a8\u6001\u548c\u5bf9\u79f0\u7684\u8fd0\u52a8\u3002", "motivation": "\u73b0\u6709\u7684\u63a7\u5236\u7cfb\u7edf\u65e0\u6cd5\u5145\u5206\u5229\u7528\u673a\u5668\u4eba\u7684\u5f62\u6001\u7279\u5f81\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u7684\u7075\u6d3b\u6027\u548c\u9002\u7528\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u81ea\u4e3b\u5b66\u4e60\u591a\u6837\u5316\u6b65\u6001\u7684\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u4f7f\u7528\u76f8\u4f4d\u53d8\u91cf\u63a7\u5236\u673a\u5668\u4eba\u7684\u5404\u4e2a\u8eab\u4f53\u90e8\u4f4d\uff0c\u5e76\u5f15\u5165\u76f8\u4f4d\u8986\u76d6\u5956\u52b1\u4fc3\u8fdb\u6b65\u6001\u7a7a\u95f4\u7684\u63a2\u7d22\uff0c\u540c\u65f6\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u6765\u7ed3\u5408\u673a\u5668\u4eba\u7684\u5f62\u6001\u5bf9\u79f0\u6027\u3002", "result": "\u7ecf\u8fc7\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u673a\u5668\u4eba\u6210\u529f\u5730\u83b7\u5f97\u4e8622\u79cd\u5168\u5411\u6b65\u6001\uff0c\u663e\u793a\u4e86\u63d0\u8bae\u5b66\u4e60\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5b66\u4e60\u6846\u67b6\u6709\u6548\u5730\u4f7f\u673a\u5668\u4eba\u83b7\u5f97\u4e8622\u79cd\u5168\u5411\u8fd0\u52a8\u6b65\u6001\uff0c\u5c55\u73b0\u4e86\u52a8\u6001\u548c\u5bf9\u79f0\u7684\u8fd0\u52a8\u7279\u70b9\u3002"}}
{"id": "2511.08354", "categories": ["cs.RO", "cs.ET", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.08354", "abs": "https://arxiv.org/abs/2511.08354", "authors": ["H. Zhu", "T. Samizadeh", "R. C. Sofia"], "title": "A CODECO Case Study and Initial Validation for Edge Orchestration of Autonomous Mobile Robots", "comment": null, "summary": "Autonomous Mobile Robots (AMRs) increasingly adopt containerized micro-services across the Edge-Cloud continuum. While Kubernetes is the de-facto orchestrator for such systems, its assumptions of stable networks, homogeneous resources, and ample compute capacity do not fully hold in mobile, resource-constrained robotic environments.\n  This paper describes a case study on smart-manufacturing AMRs and performs an initial comparison between CODECO orchestration and standard Kubernetes using a controlled KinD environment. Metrics include pod deployment and deletion times, CPU and memory usage, and inter-pod data rates. The observed results indicate that CODECO offers reduced CPU consumption and more stable communication patterns, at the cost of modest memory overhead (10-15%) and slightly increased pod lifecycle latency due to secure overlay initialization.", "AI": {"tldr": "\u5728\u79fb\u52a8\u673a\u5668\u4eba\u73af\u5883\u4e2d\uff0cCODECO\u7f16\u6392\u65b9\u6848\u6bd4Kubernetes\u66f4\u9ad8\u6548\uff0c\u4f46\u5b58\u5728\u4e00\u4e9b\u5185\u5b58\u548c\u5ef6\u8fdf\u7684\u589e\u52a0\u3002", "motivation": "\u7814\u7a76\u5728\u79fb\u52a8\u548c\u8d44\u6e90\u53d7\u9650\u7684\u673a\u5668\u4eba\u73af\u5883\u4e2d\uff0cKubernetes\u7684\u5047\u8bbe\u4e0d\u9002\u7528\uff0c\u56e0\u6b64\u9700\u8981\u8bc4\u4f30\u66f4\u9002\u5408\u7684\u7f16\u6392\u65b9\u6848\u3002", "method": "\u5f00\u5c55\u6848\u4f8b\u7814\u7a76\u548c\u63a7\u5236\u5b9e\u9a8c\uff0c\u6bd4\u8f83CODECO\u548cKubernetes\u7684\u6027\u80fd\u3002", "result": "CODECO\u5728CPU\u6d88\u8017\u548c\u901a\u4fe1\u6a21\u5f0f\u7684\u7a33\u5b9a\u6027\u4e0a\u4f18\u4e8eKubernetes\uff0c\u4f46\u5728\u5185\u5b58\u5f00\u9500\u548cPods\u751f\u547d\u5468\u671f\u5ef6\u8fdf\u65b9\u9762\u7565\u6709\u589e\u52a0\u3002", "conclusion": "\u9488\u5bf9\u667a\u80fd\u5236\u9020AMRs\u7684\u73af\u5883\uff0cCODECO\u63d0\u4f9b\u4e86\u6bd4Kubernetes\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u573a\u666f\u3002"}}
{"id": "2511.08454", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.08454", "abs": "https://arxiv.org/abs/2511.08454", "authors": ["Tianyu Jia", "Xingchen Yang", "Ciaran McGeady", "Yifeng Li", "Jinzhi Lin", "Kit San Ho", "Feiyu Pan", "Linhong Ji", "Chong Li", "Dario Farina"], "title": "Intuitive control of supernumerary robotic limbs through a tactile-encoded neural interface", "comment": null, "summary": "Brain-computer interfaces (BCIs) promise to extend human movement capabilities by enabling direct neural control of supernumerary effectors, yet integrating augmented commands with multiple degrees of freedom without disrupting natural movement remains a key challenge. Here, we propose a tactile-encoded BCI that leverages sensory afferents through a novel tactile-evoked P300 paradigm, allowing intuitive and reliable decoding of supernumerary motor intentions even when superimposed with voluntary actions. The interface was evaluated in a multi-day experiment comprising of a single motor recognition task to validate baseline BCI performance and a dual task paradigm to assess the potential influence between the BCI and natural human movement. The brain interface achieved real-time and reliable decoding of four supernumerary degrees of freedom, with significant performance improvements after only three days of training. Importantly, after training, performance did not differ significantly between the single- and dual-BCI task conditions, and natural movement remained unimpaired during concurrent supernumerary control. Lastly, the interface was deployed in a movement augmentation task, demonstrating its ability to command two supernumerary robotic arms for functional assistance during bimanual tasks. These results establish a new neural interface paradigm for movement augmentation through stimulation of sensory afferents, expanding motor degrees of freedom without impairing natural movement.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8111\u673a\u63a5\u53e3\uff0c\u901a\u8fc7\u523a\u6fc0\u611f\u89c9\u4f20\u5165\uff0c\u80fd\u5728\u4e0d\u5f71\u54cd\u81ea\u7136\u8fd0\u52a8\u7684\u60c5\u51b5\u4e0b\u63a7\u5236\u591a\u4e2a\u8d85\u989d\u8fd0\u52a8\uff0c\u589e\u5f3a\u4eba\u7c7b\u8fd0\u52a8\u80fd\u529b\u3002", "motivation": "\u8111\u673a\u63a5\u53e3(BCI)\u65e8\u5728\u589e\u5f3a\u4eba\u7c7b\u8fd0\u52a8\u80fd\u529b\uff0c\u4f46\u5728\u878d\u5408\u591a\u81ea\u7531\u5ea6\u7684\u589e\u5f3a\u547d\u4ee4\u65f6\uff0c\u4fdd\u6301\u81ea\u7136\u8fd0\u52a8\u7684\u6311\u6218\u4f9d\u7136\u5b58\u5728\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89e6\u89c9\u7f16\u7801\u7684\u8111\u673a\u63a5\u53e3\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u89e6\u89c9\u5f15\u53d1P300\u8303\u4f8b\uff0c\u5b9e\u73b0\u76f4\u89c2\u4e14\u53ef\u9760\u7684\u8d85\u989d\u8fd0\u52a8\u610f\u56fe\u89e3\u7801\uff0c\u5e76\u5728\u591a\u65e5\u5b9e\u9a8c\u4e2d\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u8be5\u63a5\u53e3\u5728\u591a\u65e5\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u4e86\u56db\u4e2a\u8d85\u989d\u81ea\u7531\u5ea6\u7684\u5b9e\u65f6\u53ef\u9760\u89e3\u7801\uff0c\u5728\u7ecf\u8fc7\u4e09\u5929\u8bad\u7ec3\u540e\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u5e76\u5728\u5355\u4efb\u52a1\u548c\u53cc\u4efb\u52a1\u6761\u4ef6\u4e0b\u8868\u73b0\u76f8\u8fd1\uff0c\u81ea\u7136\u8fd0\u52a8\u672a\u53d7\u5f71\u54cd\u3002", "conclusion": "\u8be5\u754c\u9762\u80fd\u591f\u5728\u4e0d\u5f71\u54cd\u81ea\u7136\u8fd0\u52a8\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u5bf9\u591a\u4e2a\u8d85\u989d\u81ea\u7531\u5ea6\u7684\u89e3\u7801\u548c\u63a7\u5236\uff0c\u5c55\u793a\u4e86\u57fa\u4e8e\u611f\u89c9\u4f20\u5165\u7684\u795e\u7ecf\u63a5\u53e3\u65b0\u8303\u5f0f\u3002"}}
{"id": "2511.08490", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.08490", "abs": "https://arxiv.org/abs/2511.08490", "authors": ["Mariana Smith", "Tanner Watts", "Susheela Sharma Stern", "Brendan Burkhart", "Hao Li", "Alejandro O. Chara", "Nithesh Kumar", "James Ferguson", "Ayberk Acar", "Jesse F. d'Almeida", "Lauren Branscombe", "Lauren Shepard", "Ahmed Ghazi", "Ipek Oguz", "Jie Ying Wu", "Robert J. Webster", "Axel Krieger", "Alan Kuntz"], "title": "A Supervised Autonomous Resection and Retraction Framework for Transurethral Enucleation of the Prostatic Median Lobe", "comment": "Submitted to International Symposium on Medial Robotics (ISMR) 2026. 7 pages, 8 figures", "summary": "Concentric tube robots (CTRs) offer dexterous motion at millimeter scales, enabling minimally invasive procedures through natural orifices. This work presents a coordinated model-based resection planner and learning-based retraction network that work together to enable semi-autonomous tissue resection using a dual-arm transurethral concentric tube robot (the Virtuoso). The resection planner operates directly on segmented CT volumes of prostate phantoms, automatically generating tool trajectories for a three-phase median lobe resection workflow: left/median trough resection, right/median trough resection, and median blunt dissection. The retraction network, PushCVAE, trained on surgeon demonstrations, generates retractions according to the procedural phase. The procedure is executed under Level-3 (supervised) autonomy on a prostate phantom composed of hydrogel materials that replicate the mechanical and cutting properties of tissue. As a feasibility study, we demonstrate that our combined autonomous system achieves a 97.1% resection of the targeted volume of the median lobe. Our study establishes a foundation for image-guided autonomy in transurethral robotic surgery and represents a first step toward fully automated minimally-invasive prostate enucleation.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u7ed3\u5408\u5207\u9664\u89c4\u5212\u548c\u5b66\u4e60\u7f51\u7edc\u7684\u534a\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u6a21\u62df\u7684\u524d\u5217\u817a\u7ec4\u7ec7\u4e0a\u8fdb\u884c\u7cbe\u51c6\u7684\u7ec4\u7ec7\u5207\u9664\u3002", "motivation": "\u5f00\u53d1\u7528\u4e8e\u7ecf\u5c3f\u9053\u624b\u672f\u7684\u66f4\u9ad8\u6548\u3001\u81ea\u52a8\u5316\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u4ee5\u5b9e\u73b0\u66f4\u7cbe\u51c6\u7684\u7ec4\u7ec7\u5207\u9664\u3002", "method": "\u901a\u8fc7\u5bf9\u524d\u5217\u817a\u6a21\u4f53\u7684CT\u56fe\u50cf\u8fdb\u884c\u5206\u5272\uff0c\u81ea\u52a8\u751f\u6210\u5207\u524a\u5de5\u5177\u8f68\u8ff9\uff0c\u5e76\u4f7f\u7528\u8bad\u7ec3\u81ea\u5916\u79d1\u533b\u751f\u793a\u8303\u7684\u56de\u64a4\u7f51\u7edc\u6765\u6307\u5bfc\u624b\u672f\u6b65\u9aa4\uff0c\u6700\u7ec8\u5728\u6a21\u62df\u6750\u6599\u4e0a\u5b9e\u65bd\u64cd\u4f5c\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e86\u57fa\u4e8e\u6a21\u578b\u7684\u5207\u9664\u89c4\u5212\u5668\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u56de\u64a4\u7f51\u7edc\u7684\u534a\u81ea\u4e3b\u5207\u9664\u7cfb\u7edf\uff0c\u80fd\u591f\u9ad8\u6548\u5730\u5207\u9664\u524d\u5217\u817a\u7ec4\u7ec7\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5728\u5047\u4f53\u4e0a\u5b9e\u73b0\u4e8697.1%\u7684\u76ee\u6807\u7ec4\u7ec7\u5207\u9664\uff0c\u4e3a\u56fe\u50cf\u5f15\u5bfc\u7684\u7ecf\u5c3f\u9053\u673a\u5668\u4eba\u624b\u672f\u7684\u81ea\u4e3b\u6027\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.08502", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.08502", "abs": "https://arxiv.org/abs/2511.08502", "authors": ["Ruya Karagulle", "Cristian-Ioan Vasile", "Necmiye Ozay"], "title": "Safe and Optimal Learning from Preferences via Weighted Temporal Logic with Applications in Robotics and Formula 1", "comment": "8 pages, 2 figures", "summary": "Autonomous systems increasingly rely on human feedback to align their behavior, expressed as pairwise comparisons, rankings, or demonstrations. While existing methods can adapt behaviors, they often fail to guarantee safety in safety-critical domains. We propose a safety-guaranteed, optimal, and efficient approach to solve the learning problem from preferences, rankings, or demonstrations using Weighted Signal Temporal Logic (WSTL). WSTL learning problems, when implemented naively, lead to multi-linear constraints in the weights to be learned. By introducing structural pruning and log-transform procedures, we reduce the problem size and recast the problem as a Mixed-Integer Linear Program while preserving safety guarantees. Experiments on robotic navigation and real-world Formula 1 data demonstrate that the method effectively captures nuanced preferences and models complex task objectives.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7WSTL\u5b66\u4e60\u4ee5\u4fdd\u8bc1\u5b89\u5168\uff0c\u540c\u65f6\u6709\u6548\u5904\u7406\u504f\u597d\u548c\u6392\u540d\u95ee\u9898\uff0c\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u4e2d\u65e0\u6cd5\u4fdd\u8bc1\u5b89\u5168\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "method": "\u901a\u8fc7\u7ed3\u6784\u6027\u4fee\u526a\u548c\u5bf9\u6570\u53d8\u6362\u5904\u7406\uff0c\u5c06\u5b66\u4e60\u95ee\u9898\u8f6c\u5316\u4e3a\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff0c\u540c\u65f6\u4fdd\u6301\u5b89\u5168\u4fdd\u8bc1\u3002", "result": "\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u771f\u5b9e\u4e16\u754c\u7684\u4e00\u7ea7\u65b9\u7a0b\u5f0f\u6570\u636e\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u6355\u6349\u4e86\u7ec6\u5fae\u7684\u504f\u597d\u5e76\u5efa\u6a21\u590d\u6742\u7684\u4efb\u52a1\u76ee\u6807\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a0\u6743\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91\uff08WSTL\uff09\u7684\u5b89\u5168\u4fdd\u8bc1\u3001\u6700\u4f18\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u504f\u597d\u3001\u6392\u540d\u6216\u793a\u8303\u5b66\u4e60\u95ee\u9898\u3002"}}
{"id": "2511.08583", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.08583", "abs": "https://arxiv.org/abs/2511.08583", "authors": ["Rong Xue", "Jiageng Mao", "Mingtong Zhang", "Yue Wang"], "title": "SeFA-Policy: Fast and Accurate Visuomotor Policy Learning with Selective Flow Alignment", "comment": null, "summary": "Developing efficient and accurate visuomotor policies poses a central challenge in robotic imitation learning. While recent rectified flow approaches have advanced visuomotor policy learning, they suffer from a key limitation: After iterative distillation, generated actions may deviate from the ground-truth actions corresponding to the current visual observation, leading to accumulated error as the reflow process repeats and unstable task execution. We present Selective Flow Alignment (SeFA), an efficient and accurate visuomotor policy learning framework. SeFA resolves this challenge by a selective flow alignment strategy, which leverages expert demonstrations to selectively correct generated actions and restore consistency with observations, while preserving multimodality. This design introduces a consistency correction mechanism that ensures generated actions remain observation-aligned without sacrificing the efficiency of one-step flow inference. Extensive experiments across both simulated and real-world manipulation tasks show that SeFA Policy surpasses state-of-the-art diffusion-based and flow-based policies, achieving superior accuracy and robustness while reducing inference latency by over 98%. By unifying rectified flow efficiency with observation-consistent action generation, SeFA provides a scalable and dependable solution for real-time visuomotor policy learning. Code is available on https://github.com/RongXueZoe/SeFA.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u9009\u62e9\u6027\u6d41\u5bf9\u9f50\uff08SeFA\uff09\uff0c\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89c6\u89c9\u8fd0\u52a8\u653f\u7b56\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u5bb6\u793a\u8303\u6765\u6821\u6b63\u751f\u6210\u7684\u52a8\u4f5c\uff0c\u4ee5\u4fdd\u6301\u4e0e\u89c2\u5bdf\u7684\u4e00\u81f4\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u653f\u7b56\u6027\u80fd\u5e76\u51cf\u5c11\u4e86\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u4e2d\uff0c\u5f00\u53d1\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89c6\u89c9\u8fd0\u52a8\u653f\u7b56\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u800c\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u751f\u6210\u52a8\u4f5c\u7684\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u9009\u62e9\u6027\u6d41\u5bf9\u9f50\u7b56\u7565\uff0c\u5229\u7528\u4e13\u5bb6\u6f14\u793a\u9009\u62e9\u6027\u4fee\u6b63\u751f\u6210\u7684\u52a8\u4f5c\uff0c\u4fdd\u6301\u4e0e\u89c6\u89c9\u89c2\u5bdf\u7684\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4fdd\u7559\u591a\u6a21\u6001\u7279\u6027\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9009\u62e9\u6027\u6d41\u5bf9\u9f50\uff08SeFA\uff09\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u9009\u62e9\u6027\u5730\u6821\u6b63\u751f\u6210\u52a8\u4f5c\u6765\u89e3\u51b3\u73b0\u6709\u7684\u89c6\u89c9Motor\u653f\u7b56\u5b66\u4e60\u4e2d\u7684\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u4ece\u800c\u6709\u6548\u964d\u4f4e\u4e86\u63a8\u7406\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "SeFA\u901a\u8fc7\u878d\u5408\u6d41\u7684\u9ad8\u6548\u6027\u4e0e\u89c2\u5bdf\u4e00\u81f4\u7684\u52a8\u4f5c\u751f\u6210\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u53ef\u9760\u7684\u5b9e\u65f6\u89c6\u89c9\u8fd0\u52a8\u653f\u7b56\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u3002"}}
