{"id": "2511.11577", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.11577", "abs": "https://arxiv.org/abs/2511.11577", "authors": ["Eduardo Augusto Monteiro de Almeida", "Guillaume Thomann", "Angelina Dias Le\u00e3o Costa"], "title": "From customer to product: design tools for the visually impaired", "comment": null, "summary": "Navigation in new or unknown environments is vital, especially for visually impaired individuals. While many solutions exist, few are tailored to specific disabilities, often due to limited collaboration with handicap users in the design process. This article examines 7 tools that enable visually impaired users to participate in design, selected through a systematic review and analyzed for affinities, differences, and applications. The study suggests correlations among the tools, offering a foundation for a methodology that enhances inclusive design and accessibility.", "AI": {"tldr": "\u7814\u7a76\u65e8\u5728\u63d0\u5347\u89c6\u969c\u4eba\u58eb\u5728\u8bbe\u8ba1\u4e2d\u7684\u53c2\u4e0e\uff0c\u5206\u67907\u79cd\u76f8\u5173\u5de5\u5177\u4ee5\u5b9e\u73b0\u66f4\u5177\u5305\u5bb9\u6027\u7684\u8bbe\u8ba1\u65b9\u6cd5", "motivation": "\u589e\u5f3a\u89c6\u969c\u4eba\u58eb\u5728\u8bbe\u8ba1\u8fc7\u7a0b\u4e2d\u7684\u53c2\u4e0e\u611f\u548c\u53ef\u8fbe\u6027", "method": "\u7cfb\u7edf\u6027\u56de\u987e\u4e0e\u5de5\u5177\u5206\u6790", "result": "\u8bc6\u522b\u548c\u5206\u67907\u79cd\u5de5\u5177\uff0c\u63ed\u793a\u5b83\u4eec\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u548c\u5e94\u7528", "conclusion": "\u7814\u7a76\u4e3a\u521b\u9020\u66f4\u5177\u5305\u5bb9\u6027\u548c\u53ef\u8fbe\u6027\u7684\u8bbe\u8ba1\u65b9\u6cd5\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u547c\u5401\u5728\u8bbe\u8ba1\u8fc7\u7a0b\u4e2d\u66f4\u597d\u5730\u4e0e\u6b8b\u75be\u7528\u6237\u5408\u4f5c\u3002"}}
{"id": "2511.11578", "categories": ["cs.HC", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.11578", "abs": "https://arxiv.org/abs/2511.11578", "authors": ["Botao Zhu", "Xianbin Wang"], "title": "Social and Physical Attributes-Defined Trust Evaluation for Effective Collaborator Selection in Human-Device Coexistence Systems", "comment": null, "summary": "In human-device coexistence systems, collaborations among devices are determined by not only physical attributes such as network topology but also social attributes among human users. Consequently, trust evaluation of potential collaborators based on these multifaceted attributes becomes critical for ensuring the eventual outcome. However, due to the high heterogeneity and complexity of physical and social attributes, efficiently integrating them for accurate trust evaluation remains challenging. To overcome this difficulty, a canonical correlation analysis-enhanced hypergraph self-supervised learning (HSLCCA) method is proposed in this research. First, by treating all attributes as relationships among connected devices, a relationship hypergraph is constructed to comprehensively capture inter-device relationships across three dimensions: spatial attribute-related, device attribute-related, and social attribute-related. Next, a self-supervised learning framework is developed to integrate these multi-dimensional relationships and generate device embeddings enriched with relational semantics. In this learning framework, the relationship hypergraph is augmented into two distinct views to enhance semantic information. A parameter-sharing hypergraph neural network is then utilized to learn device embeddings from both views. To further enhance embedding quality, a CCA approach is applied, allowing the comparison of data between the two views. Finally, the trustworthiness of devices is calculated based on the learned device embeddings. Extensive experiments demonstrate that the proposed HSLCCA method significantly outperforms the baseline algorithm in effectively identifying trusted devices.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHSLCCA\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d85\u56fe\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u6709\u6548\u6574\u5408\u591a\u7ef4\u5c5e\u6027\u4ee5\u8bc4\u4f30\u8bbe\u5907\u95f4\u4fe1\u4efb\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5728\u4eba\u673a\u5171\u5b58\u7cfb\u7edf\u4e2d\uff0c\u8bbe\u5907\u95f4\u7684\u5408\u4f5c\u4e0d\u4ec5\u53d7\u7269\u7406\u5c5e\u6027\u5f71\u54cd\uff0c\u4e5f\u53d7\u4eba\u7c7b\u7528\u6237\u7684\u793e\u4f1a\u5c5e\u6027\u5f71\u54cd\uff0c\u56e0\u6b64\u51c6\u786e\u7684\u4fe1\u4efb\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u5173\u7cfb\u8d85\u56fe\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53c2\u6570\u5171\u4eab\u7684\u8d85\u56fe\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u8bbe\u5907\u5d4c\u5165\uff0c\u5229\u7528CCA\u65b9\u6cd5\u63d0\u5347\u5d4c\u5165\u8d28\u91cf\u3002", "result": "\u901a\u8fc7\u6784\u5efa\u5173\u7cfb\u8d85\u56fe\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u6210\u529f\u6574\u5408\u591a\u7ef4\u5ea6\u5c5e\u6027\uff0c\u751f\u6210\u5bcc\u542b\u5173\u7cfb\u8bed\u4e49\u7684\u8bbe\u5907\u5d4c\u5165\uff0c\u5e76\u8ba1\u7b97\u8bbe\u5907\u7684\u53ef\u4fe1\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u7684HSLCCA\u65b9\u6cd5\u5728\u6709\u6548\u8bc6\u522b\u53ef\u4fe1\u8bbe\u5907\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u7b97\u6cd5\u3002"}}
{"id": "2511.11587", "categories": ["cs.HC", "cs.AI", "cs.CE", "cs.GR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.11587", "abs": "https://arxiv.org/abs/2511.11587", "authors": ["Yiming Zhang", "Yuejia Xu", "Ziyao Wang", "Xin Yan", "Xiaosai Hao"], "title": "MedBuild AI: An Agent-Based Hybrid Intelligence Framework for Reshaping Agency in Healthcare Infrastructure Planning through Generative Design for Medical Architecture", "comment": "25 pages, 16 figures. Submitted to the IJAC Special Issue \"Rebalance and Reciprocity\"", "summary": "Globally, disparities in healthcare infrastructure remain stark, leaving countless communities without access to even basic services. Traditional infrastructure planning is often slow and inaccessible, and although many architects are actively delivering humanitarian and aid-driven hospital projects worldwide, these vital efforts still fall far short of the sheer scale and urgency of demand. This paper introduces MedBuild AI, a hybrid-intelligence framework that integrates large language models (LLMs) with deterministic expert systems to rebalance the early design and conceptual planning stages. As a web-based platform, it enables any region with satellite internet access to obtain guidance on modular, low-tech, low-cost medical building designs. The system operates through three agents: the first gathers local health intelligence via conversational interaction; the second translates this input into an architectural functional program through rule-based computation; and the third generates layouts and 3D models. By embedding computational negotiation into the design process, MedBuild AI fosters a reciprocal, inclusive, and equitable approach to healthcare planning, empowering communities and redefining agency in global healthcare architecture.", "AI": {"tldr": "MedBuild AI\u662f\u4e00\u4e2a\u6df7\u5408\u667a\u80fd\u6846\u67b6\uff0c\u65e8\u5728\u4e3a\u7f3a\u4e4f\u533b\u7597\u57fa\u7840\u8bbe\u65bd\u7684\u5730\u533a\u63d0\u4f9b\u4f4e\u6210\u672c\u533b\u7597\u5efa\u7b51\u8bbe\u8ba1\u6307\u5bfc\uff0c\u5e76\u901a\u8fc7\u4e09\u4e2a\u4ee3\u7406\u5b9e\u73b0\u66f4\u516c\u5e73\u7684\u533b\u7597\u89c4\u5212\u3002", "motivation": "\u9762\u5bf9\u5168\u7403\u533b\u7597\u57fa\u7840\u8bbe\u65bd\u4e0d\u5e73\u7b49\u73b0\u8c61\uff0c\u6025\u9700\u4e00\u79cd\u5feb\u901f\u3001\u4f4e\u6210\u672c\u7684\u8bbe\u8ba1\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u5e94\u5bf9\u6ee1\u8db3\u793e\u533a\u57fa\u672c\u533b\u7597\u670d\u52a1\u7684\u8feb\u5207\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u4e09\u4e2a\u529f\u80fd\u4ee3\u7406\u8fdb\u884c\u64cd\u4f5c\uff1a\u6536\u96c6\u5f53\u5730\u5065\u5eb7\u60c5\u62a5\uff0c\u8f6c\u6362\u4e3a\u5efa\u7b51\u529f\u80fd\u7a0b\u5e8f\uff0c\u5e76\u751f\u6210\u8bbe\u8ba1\u6a21\u578b\uff0c\u5f62\u6210\u4e00\u4e2a\u534f\u4f5c\u8bbe\u8ba1\u8fc7\u7a0b\u3002", "result": "\u672c\u6587\u4ecb\u7ecd\u4e86MedBuild AI\uff0c\u8fd9\u662f\u4e00\u4e2a\u878d\u5408\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u786e\u5b9a\u6027\u4e13\u5bb6\u7cfb\u7edf\u7684\u6df7\u5408\u667a\u80fd\u6846\u67b6\uff0c\u65e8\u5728\u6539\u5584\u533b\u7597\u57fa\u7840\u8bbe\u65bd\u8bbe\u8ba1\u548c\u89c4\u5212\u3002\u8be5\u5e73\u53f0\u9762\u5411\u4efb\u4f55\u62e5\u6709\u536b\u661f\u4e92\u8054\u7f51\u7684\u5730\u533a\uff0c\u4e3a\u5176\u63d0\u4f9b\u6a21\u5757\u5316\u3001\u4f4e\u6280\u672f\u3001\u4f4e\u6210\u672c\u7684\u533b\u7597\u5efa\u7b51\u8bbe\u8ba1\u6307\u5bfc\u3002MedBuild AI\u901a\u8fc7\u4e09\u4e2a\u4ee3\u7406\u8fd0\u4f5c\uff0c\u5206\u522b\u6536\u96c6\u5f53\u5730\u5065\u5eb7\u4fe1\u606f\u3001\u5c06\u5176\u8f6c\u5316\u4e3a\u5efa\u7b51\u529f\u80fd\u7a0b\u5e8f\uff0c\u5e76\u751f\u6210\u5e03\u5c40\u548c3D\u6a21\u578b\uff0c\u4ece\u800c\u5b9e\u73b0\u533b\u7597\u89c4\u5212\u7684\u516c\u5e73\u6027\u548c\u5305\u5bb9\u6027\u3002", "conclusion": "MedBuild AI\u901a\u8fc7\u5f15\u5165\u8ba1\u7b97\u534f\u5546\uff0c\u91cd\u5851\u4e86\u533b\u7597\u5efa\u7b51\u8bbe\u8ba1\u7684\u6743\u5229\u4e0e\u80fd\u529b\uff0c\u4e3a\u5168\u7403\u533b\u7597\u4f53\u7cfb\u7684\u516c\u5e73\u6027\u548c\u53ef\u53ca\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.11610", "categories": ["cs.HC", "cs.CY", "cs.ET"], "pdf": "https://arxiv.org/pdf/2511.11610", "abs": "https://arxiv.org/abs/2511.11610", "authors": ["Angelica Urbanelli", "Marina Nadalin", "Mario Chiesa", "Rojin Bayat", "Massimo Migliorini", "Claudio Rossi"], "title": "ARise: an Augmented Reality Mobile Application to Improve Cultural Heritage Resilience", "comment": null, "summary": "The preservation of cultural heritage faces increasing threats from climate change effects and environmental hazards, demanding innovative solutions that can promote awareness and resilience. This paper presents ARise, an Augmented Reality mobile application designed to enhance public engagement with cultural sites while raising awareness about the local impacts of climate change. Based on a user-centered co-creative methodology involving stakeholders from five European regions, ARise integrates multiple data sourcess - a Crowdsourcing Chatbot, a Social Media Data Analysis tool, and an AI-based Artwork Generation module - to deliver immersive and emotionally engaging experiences. Although formal user testing is forthcoming, this prototype demonstrates the potential of AR to support education, cultural sustainability, and climate adaptation.", "AI": {"tldr": "ARise\u662f\u4e00\u6b3e\u589e\u5f3a\u73b0\u5b9e\u79fb\u52a8\u5e94\u7528\uff0c\u65e8\u5728\u901a\u8fc7\u4fc3\u8fdb\u516c\u4f17\u53c2\u4e0e\u4e0e\u63d0\u9ad8\u6c14\u5019\u53d8\u5316\u5f71\u54cd\u610f\u8bc6\u6765\u4fdd\u62a4\u6587\u5316\u9057\u4ea7\u3002", "motivation": "\u9762\u5bf9\u6c14\u5019\u53d8\u5316\u548c\u73af\u5883\u5a01\u80c1\uff0c\u5bf9\u6587\u5316\u9057\u4ea7\u7684\u4fdd\u62a4\u8feb\u5728\u7709\u776b\uff0c\u9700\u8981\u521b\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u9ad8\u516c\u4f17\u610f\u8bc6\u548c\u97e7\u6027\u3002", "method": "\u4f7f\u7528\u7528\u6237\u4e2d\u5fc3\u7684\u5171\u540c\u521b\u4f5c\u65b9\u6cd5\uff0c\u6574\u5408\u591a\u4e2a\u6570\u636e\u6e90\uff0c\u5305\u62ec\u4f17\u5305\u804a\u5929\u673a\u5668\u4eba\u3001\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u5206\u6790\u5de5\u5177\u548c\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\u7684\u827a\u672f\u4f5c\u54c1\u751f\u6210\u6a21\u5757\u3002", "result": "ARise\u5e94\u7528\u5c55\u793a\u4e86\u589e\u5f3a\u73b0\u5b9e\u6280\u672f\u5982\u4f55\u652f\u6301\u6559\u80b2\u548c\u589e\u5f3a\u5bf9\u6c14\u5019\u53d8\u5316\u5f71\u54cd\u7684\u8ba4\u8bc6\u3002", "conclusion": "ARise\u539f\u578b\u5c55\u793a\u4e86\u589e\u5f3a\u73b0\u5b9e\u5728\u6559\u80b2\u3001\u6587\u5316\u53ef\u6301\u7eed\u6027\u548c\u6c14\u5019\u9002\u5e94\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.11616", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.11616", "abs": "https://arxiv.org/abs/2511.11616", "authors": ["Rathin Chandra Shit", "Sharmila Subudhi"], "title": "Hierarchical Federated Graph Attention Networks for Scalable and Resilient UAV Collision Avoidance", "comment": "Accepted and scheduled for conference presentation", "summary": "The real-time performance, adversarial resiliency, and privacy preservation are the most important metrics that need to be balanced to practice collision avoidance in large-scale multi-UAV (Unmanned Aerial Vehicle) systems. Current frameworks tend to prescribe monolithic solutions that are not only prohibitively computationally complex with a scaling cost of $O(n^2)$ but simply do not offer Byzantine fault tolerance. The proposed hierarchical framework presented in this paper tries to eliminate such trade-offs by stratifying a three-layered architecture. We spread the intelligence into three layers: an immediate collision avoiding local layer running on dense graph attention with latency of $<10 ms$, a regional layer using sparse attention with $O(nk)$ computational complexity and asynchronous federated learning with coordinate-wise trimmed mean aggregation, and lastly, a global layer using a lightweight Hashgraph-inspired protocol. We have proposed an adaptive differential privacy mechanism, wherein the noise level $(\u03b5\\in [0.1, 1.0])$ is dynamically reduced based on an evaluation of the measured real-time threat that in turn maximized the privacy-utility tradeoff. Through the use of Distributed Hash Table (DHT)-based lightweight audit logging instead of heavyweight blockchain consensus, the median cost of getting a $95^{th}$ percentile decision within 50ms is observed across all tested swarm sizes. This architecture provides a scalable scenario of 500 UAVs with a collision rate of $< 2.0\\%$ and the Byzantine fault tolerance of $f < n/3$.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u5c42\u5206\u5c42\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5927\u89c4\u6a21\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7684\u78b0\u649e\u907f\u514d\uff0c\u6210\u529f\u5e73\u8861\u4e86\u5b9e\u65f6\u6027\u3001\u5bf9\u6297\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u3002", "motivation": "\u5728\u5927\u89c4\u6a21\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u4e2d\uff0c\u5b9e\u65f6\u6027\u80fd\u3001\u5bf9\u6297\u6062\u590d\u80fd\u529b\u548c\u9690\u79c1\u4fdd\u62a4\u662f\u78b0\u649e\u907f\u514d\u9700\u8981\u5e73\u8861\u7684\u91cd\u8981\u6307\u6807\u3002", "method": "\u5206\u5c42\u67b6\u6784\u5305\u62ec\u5c40\u90e8\u5c42\u3001\u533a\u57df\u5c42\u548c\u5168\u5c40\u5c42\uff0c\u5404\u81ea\u8fd0\u7528\u4e0d\u540c\u7684\u6ce8\u610f\u529b\u673a\u5236\u548c\u5b66\u4e60\u65b9\u5f0f\uff0c\u540c\u65f6\u5f15\u5165\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u5dee\u5206\u9690\u79c1\u673a\u5236\u3002", "result": "\u91c7\u7528\u8be5\u67b6\u6784\uff0c\u5728500\u67b6\u65e0\u4eba\u673a\u7684\u573a\u666f\u4e2d\uff0c\u78b0\u649e\u7387\u4fdd\u6301\u57282.0%\u4ee5\u4e0b\uff0c\u5e76\u5b9e\u73b0\u4e86\u62dc\u5360\u5ead\u5bb9\u9519\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e09\u5c42\u67b6\u6784\u53ef\u4ee5\u6709\u6548\u5e73\u8861\u5b9e\u65f6\u6027\u80fd\u3001\u5bf9\u6297\u6027\u548c\u9690\u79c1\u4fdd\u62a4\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u4e2d\u7684\u78b0\u649e\u907f\u514d\u3002"}}
{"id": "2511.11811", "categories": ["cs.HC", "eess.AS", "eess.IV", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.11811", "abs": "https://arxiv.org/abs/2511.11811", "authors": ["Yonatan Tussa", "Andy Heredia", "Nirupam Roy"], "title": "Lessons Learned from Developing a Privacy-Preserving Multimodal Wearable for Local Voice-and-Vision Inference", "comment": "7 pages, 5 figures", "summary": "Many promising applications of multimodal wearables require continuous sensing and heavy computation, yet users reject such devices due to privacy concerns. This paper shares our experiences building an ear-mounted voice-and-vision wearable that performs local AI inference using a paired smartphone as a trusted personal edge. We describe the hardware--software co-design of this privacy-preserving system, including challenges in integrating a camera, microphone, and speaker within a 30-gram form factor, enabling wake word-triggered capture, and running quantized vision-language and large-language models entirely offline. Through iterative prototyping, we identify key design hurdles in power budgeting, connectivity, latency, and social acceptability. Our initial evaluation shows that fully local multimodal inference is feasible on commodity mobile hardware with interactive latency. We conclude with design lessons for researchers developing embedded AI systems that balance privacy, responsiveness, and usability in everyday settings.", "AI": {"tldr": "\u672c\u8bba\u6587\u63a2\u8ba8\u4e86\u4e00\u79cd\u8033\u6302\u5f0f\u591a\u6a21\u6001\u53ef\u7a7f\u6234\u8bbe\u5907\uff0c\u89e3\u51b3\u9690\u79c1\u95ee\u9898\u5e76\u5b9e\u73b0\u672c\u5730AI\u63a8\u65ad\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u53ef\u7a7f\u6234\u8bbe\u5907\u6709\u6f5c\u5728\u5e94\u7528\uff0c\u4f46\u7528\u6237\u56e0\u9690\u79c1\u95ee\u9898\u800c\u62d2\u7edd\u4f7f\u7528\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u9690\u79c1\u4fdd\u62a4\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u786c\u4ef6\u548c\u8f6f\u4ef6\u7684\u5171\u540c\u8bbe\u8ba1\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u9690\u79c1\u4fdd\u62a4\u7684\u8033\u6302\u5f0f\u8bbe\u5907\uff0c\u5229\u7528\u914d\u5bf9\u667a\u80fd\u624b\u673a\u8fdb\u884c\u672c\u5730AI\u63a8\u65ad\u3002", "result": "\u521d\u6b65\u8bc4\u4f30\u8868\u660e\uff0c\u5728\u666e\u901a\u79fb\u52a8\u786c\u4ef6\u4e0a\u53ef\u4ee5\u5b9e\u73b0\u5b8c\u5168\u672c\u5730\u7684\u591a\u6a21\u6001\u63a8\u7406\uff0c\u4e14\u5177\u5907\u4e92\u52a8\u5ef6\u8fdf\u3002", "conclusion": "\u4e3a\u5f00\u53d1\u5e73\u8861\u9690\u79c1\u3001\u54cd\u5e94\u6027\u548c\u6613\u7528\u6027\u7684\u5d4c\u5165\u5f0fAI\u7cfb\u7edf\u7684\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u8bbe\u8ba1\u7ecf\u9a8c\u3002"}}
{"id": "2511.11634", "categories": ["cs.RO", "cs.CV", "cs.HC", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.11634", "abs": "https://arxiv.org/abs/2511.11634", "authors": ["Michikuni Eguchi", "Takekazu Kitagishi", "Yuichi Hiroi", "Takefumi Hiraki"], "title": "Tactile Data Recording System for Clothing with Motion-Controlled Robotic Sliding", "comment": "3 pages, 2 figures, 1 table. Presented at SIGGRAPH Asia 2025 Posters (SA Posters '25), December 15-18, 2025, Hong Kong, Hong Kong", "summary": "The tactile sensation of clothing is critical to wearer comfort. To reveal physical properties that make clothing comfortable, systematic collection of tactile data during sliding motion is required. We propose a robotic arm-based system for collecting tactile data from intact garments. The system performs stroking measurements with a simulated fingertip while precisely controlling speed and direction, enabling creation of motion-labeled, multimodal tactile databases. Machine learning evaluation showed that including motion-related parameters improved identification accuracy for audio and acceleration data, demonstrating the efficacy of motion-related labels for characterizing clothing tactile sensation. This system provides a scalable, non-destructive method for capturing tactile data of clothing, contributing to future studies on fabric perception and reproduction.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u4eba\u624b\u81c2\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u7cfb\u7edf\u6027\u5730\u6536\u96c6\u8863\u7269\u7684\u89e6\u89c9\u6570\u636e\uff0c\u4ee5\u63a2\u8ba8\u5f71\u54cd\u7a7f\u7740\u8212\u9002\u5ea6\u7684\u7269\u7406\u7279\u6027\u3002", "motivation": "\u4e3a\u4e86\u63ed\u793a\u5f71\u54cd\u8863\u7269\u8212\u9002\u5ea6\u7684\u7269\u7406\u7279\u6027\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u6536\u96c6\u5728\u6ed1\u52a8\u8fc7\u7a0b\u4e2d\u4ea7\u751f\u7684\u89e6\u89c9\u6570\u636e\u3002", "method": "\u91c7\u7528\u673a\u5668\u4eba\u624b\u81c2\u8fdb\u884c\u6ed1\u52a8\u89e6\u89c9\u6570\u636e\u91c7\u96c6\uff0c\u6a21\u62df\u6307\u5c16\u8fdb\u884c\u6d4b\u91cf\uff0c\u63a7\u5236\u901f\u5ea6\u548c\u65b9\u5411\uff0c\u521b\u9020\u8fd0\u52a8\u6807\u8bb0\u7684\u591a\u6a21\u6001\u89e6\u89c9\u6570\u636e\u5e93\u3002", "result": "\u673a\u5668\u5b66\u4e60\u8bc4\u4f30\u8868\u660e\uff0c\u5305\u542b\u8fd0\u52a8\u76f8\u5173\u53c2\u6570\u63d0\u9ad8\u4e86\u97f3\u9891\u548c\u52a0\u901f\u5ea6\u6570\u636e\u7684\u8bc6\u522b\u51c6\u786e\u6027\uff0c\u8bc1\u660e\u4e86\u8fd0\u52a8\u76f8\u5173\u6807\u7b7e\u5728\u8868\u5f81\u8863\u7269\u89e6\u89c9\u611f\u77e5\u4e2d\u7684\u6709\u6548\u6027.", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u83b7\u53d6\u8863\u7269\u89e6\u89c9\u6570\u636e\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u65e0\u635f\u7684\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u672a\u6765\u7684\u9762\u6599\u611f\u77e5\u548c\u518d\u73b0\u7814\u7a76\u3002"}}
{"id": "2511.11823", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.11823", "abs": "https://arxiv.org/abs/2511.11823", "authors": ["Salman Sayeed", "Bijoy Ahmed Saiem", "Al-Amin Sany", "Sadia Sharmin", "A. B. M. Alim Al Islam"], "title": "CollaClassroom: An AI-Augmented Collaborative Learning Platform with LLM Support in the Context of Bangladeshi University Students", "comment": null, "summary": "CollaClassroom is an AI-enhanced platform that embeds large language models (LLMs) into both individual and group study panels to support real-time collaboration. We evaluate CollaClassroom with Bangladeshi university students (N = 12) through a small-group study session and a pre-post survey. Participants have substantial prior experience with collaborative learning and LLMs and express strong receptivity to LLM-assisted study (92% agree/strongly agree). Usability ratings are positive, including high learnability(67% \"easy\"), strong reliability (83% \"reliable\"), and low frustration (83% \"not at all\"). Correlational analyses show that participants who perceive the LLM as supporting equal participation also view it as a meaningful contributor to discussions (r = 0.86). Moreover, their pre-use expectations of LLM value align with post-use assessments (r = 0.61). These findings suggest that LLMs can enhance engagement and perceived learning when designed to promote equitable turn-taking and transparency across individual and shared spaces. The paper contributes an empirically grounded account of AI-mediated collaboration in a Global South higher-education context, with design implications for fairness-aware orchestration of human-AI teamwork.", "AI": {"tldr": "CollaClassroom\u662f\u4e00\u4e2aAI\u5e73\u53f0\uff0c\u901a\u8fc7LLMs\u652f\u6301\u5b9e\u65f6\u5b66\u4e60\uff0c\u5b5f\u52a0\u62c9\u56fd\u5927\u5b66\u751f\u5bf9\u5176\u4f7f\u7528\u6001\u5ea6\u79ef\u6781\uff0c\u4f53\u9a8c\u826f\u597d\uff0cLLMs\u53ef\u63d0\u5347\u5b66\u4e60\u53c2\u4e0e\u5ea6\u4e0e\u6210\u679c\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u5982\u4f55\u5728\u5168\u7403\u5357\u65b9\u7684\u9ad8\u7b49\u6559\u80b2\u73af\u5883\u4e2d\uff0c\u901a\u8fc7AI\u4f18\u5316\u5b66\u4e60\u534f\u4f5c\uff0c\u63d0\u5347\u5b66\u4e60\u4f53\u9a8c\u3002", "method": "\u901a\u8fc7\u5bf912\u540d\u5b5f\u52a0\u62c9\u56fd\u5927\u5b66\u751f\u8fdb\u884c\u5c0f\u7ec4\u5b66\u4e60\u4f1a\u8bdd\u548c\u524d\u540e\u5bf9\u6bd4\u8c03\u67e5\uff0c\u8bc4\u4f30CollaClassroom\u7684\u6548\u679c\u3002", "result": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86CollaClassroom\uff0c\u4e00\u4e2a\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5d4c\u5165\u4e2a\u4eba\u4e0e\u5c0f\u7ec4\u5b66\u4e60\u9762\u677f\u4e2d\u7684\u4eba\u5de5\u667a\u80fd\u589e\u5f3a\u5e73\u53f0\uff0c\u4ee5\u652f\u6301\u5b9e\u65f6\u534f\u4f5c\u3002\u5728\u5b5f\u52a0\u62c9\u56fd\u7684\u5927\u5b66\u751f\u4e2d\u8fdb\u884c\u8bc4\u4f30\uff0c\u53d1\u73b0\u53c2\u4e0e\u8005\u5bf9LLMs\u8f85\u52a9\u5b66\u4e60\u6301\u79ef\u6781\u6001\u5ea6\uff0c95%\u7684\u53c2\u4e0e\u8005\u8868\u793a\u652f\u6301\u5176\u4f7f\u7528\uff0c\u5e76\u4e14\u5bf9\u53ef\u7528\u6027\u8bc4\u4f30\u7684\u8bc4\u5206\u4e5f\u5f88\u9ad8\u3002\u76f8\u5173\u5206\u6790\u8868\u660e\uff0c\u8ba4\u4e3aLLM\u652f\u6301\u5e73\u7b49\u53c2\u4e0e\u7684\u53c2\u4e0e\u8005\u4e5f\u8ba4\u4e3a\u5b83\u5bf9\u8ba8\u8bba\u6709\u610f\u4e49\u7684\u8d21\u732e\u3002\u8fd9\u8868\u660e\uff0cLLMs\u7684\u8bbe\u8ba1\u5e94\u4fc3\u8fdb\u516c\u5e73\u7684\u53d1\u8a00\u673a\u4f1a\uff0c\u4ee5\u63d0\u5347\u5b66\u4e60\u53c2\u4e0e\u611f\u548c\u77e5\u8bc6\u83b7\u53d6\u611f\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u8bc1\u6848\u4f8b\uff0c\u8868\u660e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5728\u9ad8\u7b49\u6559\u80b2\u4e2d\u4fc3\u8fdb\u516c\u5e73\u7684\u4eba\u5de5\u667a\u80fd\u8f85\u52a9\u534f\u4f5c\uff0c\u5e76\u7ed9\u51fa\u8bbe\u8ba1\u5efa\u8bae\u3002"}}
{"id": "2511.11639", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11639", "abs": "https://arxiv.org/abs/2511.11639", "authors": ["Jie Fan", "Francesco Visentin", "Barbara Mazzolai", "Emanuela Del Dottore"], "title": "Image-based Morphological Characterization of Filamentous Biological Structures with Non-constant Curvature Shape Feature", "comment": "This manuscript is a preprint version of the article currently under peer review at International Journal of Computer Vision (IJCV)", "summary": "Tendrils coil their shape to anchor the plant to supporting structures, allowing vertical growth toward light. Although climbing plants have been studied for a long time, extracting information regarding the relationship between the temporal shape change, the event that triggers it, and the contact location is still challenging. To help build this relation, we propose an image-based method by which it is possible to analyze shape changes over time in tendrils when mechano-stimulated in different portions of their body. We employ a geometric approach using a 3D Piece-Wise Clothoid-based model to reconstruct the configuration taken by a tendril after mechanical rubbing. The reconstruction shows high robustness and reliability with an accuracy of R2 > 0.99. This method demonstrates distinct advantages over deep learning-based approaches, including reduced data requirements, lower computational costs, and interpretability. Our analysis reveals higher responsiveness in the apical segment of tendrils, which might correspond to higher sensitivity and tissue flexibility in that region of the organs. Our study provides a methodology for gaining new insights into plant biomechanics and offers a foundation for designing and developing novel intelligent robotic systems inspired by climbing plants.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u50cf\u5206\u6790\u817a\u5377\u7684\u5f62\u6001\u53d8\u5316\uff0c\u63d0\u9ad8\u4e86\u690d\u7269\u751f\u7269\u529b\u5b66\u7814\u7a76\u7684\u7406\u89e3\uff0c\u5e76\u4e3a\u667a\u80fd\u673a\u5668\u4eba\u8bbe\u8ba1\u63d0\u4f9b\u4f9d\u636e\u3002", "motivation": "\u5c3d\u7ba1\u6500\u7f18\u690d\u7269\u7684\u7814\u7a76\u5386\u53f2\u60a0\u4e45\uff0c\u4f46\u63d0\u53d6\u5f62\u6001\u53d8\u5316\u3001\u89e6\u53d1\u4e8b\u4ef6\u548c\u63a5\u89e6\u4f4d\u7f6e\u4e4b\u95f4\u7684\u5173\u7cfb\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5229\u75283D\u5206\u6bb5Clothoid\u6a21\u578b\u91cd\u5efa\u817a\u5377\u5728\u673a\u68b0\u6469\u64e6\u540e\u7684\u914d\u7f6e\uff0c\u91c7\u7528\u51e0\u4f55\u65b9\u6cd5\u5206\u6790\u5f62\u6001\u53d8\u5316\u3002", "result": "\u901a\u8fc7\u672c\u65b9\u6cd5\u5206\u6790\u817a\u5377\u5728\u4e0d\u540c\u90e8\u4f4d\u53d7\u5230\u673a\u68b0\u523a\u6fc0\u540e\u7684\u5f62\u6001\u53d8\u5316\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5177\u6709\u9ad8\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\uff0c\u51c6\u786e\u5ea6\u8fbe\u5230R2 > 0.99\uff0c\u5e76\u63ed\u793a\u817a\u5377\u9876\u7aef\u6bb5\u7684\u54cd\u5e94\u6027\u66f4\u5f3a\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u7684\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u6500\u7f18\u690d\u7269\u7684\u817a\u5377\u5728\u53d7\u673a\u68b0\u523a\u6fc0\u65f6\u7684\u5f62\u6001\u53d8\u5316\u53ca\u5176\u54cd\u5e94\u7279\u70b9\uff0c\u4e3a\u690d\u7269\u751f\u7269\u529b\u5b66\u7814\u7a76\u548c\u667a\u80fd\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.11930", "categories": ["cs.HC", "cs.CV", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.11930", "abs": "https://arxiv.org/abs/2511.11930", "authors": ["Tianyu Xu", "Jihan Li", "Penghe Zu", "Pranav Sahay", "Maruchi Kim", "Jack Obeng-Marnu", "Farley Miller", "Xun Qian", "Katrina Passarella", "Mahitha Rachumalla", "Rajeev Nongpiur", "D. Shin"], "title": "Enhancing XR Auditory Realism via Multimodal Scene-Aware Acoustic Rendering", "comment": null, "summary": "In Extended Reality (XR), rendering sound that accurately simulates real-world acoustics is pivotal in creating lifelike and believable virtual experiences. However, existing XR spatial audio rendering methods often struggle with real-time adaptation to diverse physical scenes, causing a sensory mismatch between visual and auditory cues that disrupts user immersion. To address this, we introduce SAMOSA, a novel on-device system that renders spatially accurate sound by dynamically adapting to its physical environment. SAMOSA leverages a synergistic multimodal scene representation by fusing real-time estimations of room geometry, surface materials, and semantic-driven acoustic context. This rich representation then enables efficient acoustic calibration via scene priors, allowing the system to synthesize a highly realistic Room Impulse Response (RIR). We validate our system through technical evaluation using acoustic metrics for RIR synthesis across various room configurations and sound types, alongside an expert evaluation (N=12). Evaluation results demonstrate SAMOSA's feasibility and efficacy in enhancing XR auditory realism.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSAMOSA\u7684\u7cfb\u7edf\uff0c\u80fd\u591f\u5728XR\u4e2d\u5b9e\u65f6\u6e32\u67d3\u7a7a\u95f4\u97f3\u9891\uff0c\u63d0\u5347\u7528\u6237\u6c89\u6d78\u611f\u3002", "motivation": "\u63d0\u5347XR\u4e2d\u542c\u89c9\u4f53\u9a8c\u7684\u771f\u5b9e\u611f\uff0c\u514b\u670d\u73b0\u6709\u7a7a\u95f4\u97f3\u9891\u6e32\u67d3\u65b9\u6cd5\u5728\u5b9e\u65f6\u9002\u5e94\u7269\u7406\u73af\u5883\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u6574\u5408\u5b9e\u65f6\u7684\u623f\u95f4\u51e0\u4f55\u3001\u8868\u9762\u6750\u6599\u548c\u8bed\u4e49\u9a71\u52a8\u7684\u58f0\u5b66\u4e0a\u4e0b\u6587\uff0c\u4f7f\u7528\u573a\u666f\u5148\u9a8c\u8fdb\u884c\u9ad8\u6548\u7684\u58f0\u5b66\u6821\u51c6\uff0c\u5408\u6210\u771f\u5b9e\u7684\u623f\u95f4\u8109\u51b2\u54cd\u5e94\u3002", "result": "SAMOSA\u662f\u4e00\u79cd\u65b0\u578b\u7684\u8bbe\u5907\u4e0a\u7cfb\u7edf\uff0c\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u7269\u7406\u73af\u5883\u6765\u5b9e\u73b0\u7a7a\u95f4\u51c6\u786e\u7684\u58f0\u97f3\u6e32\u67d3\uff0c\u4ece\u800c\u589e\u5f3aXR\u73af\u5883\u4e2d\u7684\u542c\u89c9\u771f\u5b9e\u611f\u3002", "conclusion": "SAMOSA\u5c55\u793a\u4e86\u5728\u591a\u79cd\u623f\u95f4\u914d\u7f6e\u548c\u58f0\u97f3\u7c7b\u578b\u4e0b\uff0c\u5b9e\u73b0\u771f\u5b9e\u97f3\u54cd\u6548\u679c\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2511.11740", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11740", "abs": "https://arxiv.org/abs/2511.11740", "authors": ["Haowen Jiang", "Xinyu Huang", "You Lu", "Dingji Wang", "Yuheng Cao", "Chaofeng Sha", "Bihuan Chen", "Keyu Chen", "Xin Peng"], "title": "ExpertAD: Enhancing Autonomous Driving Systems with Mixture of Experts", "comment": "The paper has been accepted by the Fortieth AAAI Conference on Artificial Intelligence. AAAI 2026", "summary": "Recent advancements in end-to-end autonomous driving systems (ADSs) underscore their potential for perception and planning capabilities. However, challenges remain. Complex driving scenarios contain rich semantic information, yet ambiguous or noisy semantics can compromise decision reliability, while interference between multiple driving tasks may hinder optimal planning. Furthermore, prolonged inference latency slows decision-making, increasing the risk of unsafe driving behaviors. To address these challenges, we propose ExpertAD, a novel framework that enhances the performance of ADS with Mixture of Experts (MoE) architecture. We introduce a Perception Adapter (PA) to amplify task-critical features, ensuring contextually relevant scene understanding, and a Mixture of Sparse Experts (MoSE) to minimize task interference during prediction, allowing for effective and efficient planning. Our experiments show that ExpertAD reduces average collision rates by up to 20% and inference latency by 25% compared to prior methods. We further evaluate its multi-skill planning capabilities in rare scenarios (e.g., accidents, yielding to emergency vehicles) and demonstrate strong generalization to unseen urban environments. Additionally, we present a case study that illustrates its decision-making process in complex driving scenarios.", "AI": {"tldr": "\u63d0\u51faExpertAD\u6846\u67b6\uff0c\u5229\u7528Mixture of Experts\u67b6\u6784\u548cPerception Adapter\uff0c\u63d0\u9ad8\u81ea\u4e3b\u9a7e\u9a76\u7cfb\u7edf\u5728\u590d\u6742\u60c5\u5883\u4e2d\u7684\u6027\u80fd\uff0c\u663e\u8457\u964d\u4f4e\u78b0\u649e\u7387\u4e0e\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "\u6539\u5584\u81ea\u4e3b\u9a7e\u9a76\u7cfb\u7edf\u5728\u590d\u6742\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u611f\u77e5\u4e0e\u89c4\u5212\u80fd\u529b\uff0c\u89e3\u51b3\u8bed\u4e49\u6a21\u7cca\u3001\u4efb\u52a1\u5e72\u6270\u53ca\u63a8\u7406\u5ef6\u8fdf\u7b49\u95ee\u9898", "method": "ExpertAD\u6846\u67b6\uff0c\u7ed3\u5408Mixture of Experts (MoE)\u67b6\u6784\u548cPerception Adapter (PA)", "result": "ExpertAD\u6846\u67b6\u76f8\u6bd4\u4e8e\u4e4b\u524d\u7684\u65b9\u6cd5\uff0c\u5e73\u5747\u78b0\u649e\u7387\u964d\u4f4e\u81f320%\uff0c\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e25%\u3002\u5728\u7a00\u6709\u573a\u666f\u4e2d\u7684\u591a\u6280\u80fd\u89c4\u5212\u4e5f\u8868\u73b0\u826f\u597d\uff0c\u5c55\u73b0\u4e86\u5bf9\u672a\u77e5\u57ce\u5e02\u73af\u5883\u7684\u5f3a\u6cdb\u5316\u80fd\u529b", "conclusion": "ExpertAD\u5728\u590d\u6742\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u51b3\u7b56\u8fc7\u7a0b\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\uff0c\u63d0\u9ad8\u4e86\u51b3\u7b56\u7684\u53ef\u9760\u6027\uff0c\u589e\u5f3a\u4e86\u6574\u4f53\u7684\u9a7e\u9a76\u5b89\u5168\u6027\u3002"}}
{"id": "2511.11961", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.11961", "abs": "https://arxiv.org/abs/2511.11961", "authors": ["Shuning Zhang", "Jiaqi Bai", "Linzhi Wang", "Shixuan Li", "Xin Yi", "Hewu Li"], "title": "\"Power of Words\": Stealthy and Adaptive Private Information Elicitation via LLM Communication Strategies", "comment": null, "summary": "While communication strategies of Large Language Models (LLMs) are crucial for human-LLM interactions, they can also be weaponized to elicit private information, yet such stealthy attacks remain under-explored. This paper introduces the first adaptive attack framework for stealthy and targeted private information elicitation via communication strategies. Our framework operates in a dynamic closed-loop: it first performs real-time psychological profiling of the users' state, then adaptively selects an optimized communication strategy, and finally maintains stealthiness through prompt-based rewriting. We validated this framework through a user study (N=84), demonstrating its generalizability across 3 distinct LLMs and 3 scenarios. The targeted attacks achieved a 205.4% increase in eliciting specific targeted information compared to stealthy interactions without strategies. Even stealthy interactions without specific strategies successfully elicited private information in 54.8% cases. Notably, users not only failed to detect the manipulation but paradoxically rated the attacking chatbot as more empathetic and trustworthy. Finally, we advocate for mitigations, encouraging developers to integrate adaptive, just-in-time alerts, users to build literacy against specific manipulative tactics, and regulators to define clear ethical boundaries distinguishing benign persuasion from coercion.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9002\u5e94\u6027\u653b\u51fb\u6846\u67b6\uff0c\u80fd\u591f\u901a\u8fc7\u5b9e\u65f6\u5fc3\u7406\u753b\u50cf\u4e0e\u4f18\u5316\u6c9f\u901a\u7b56\u7565\u6709\u6548\u83b7\u53d6\u7528\u6237\u79c1\u4eba\u4fe1\u606f\uff0c\u4e14\u7528\u6237\u672a\u80fd\u5bdf\u89c9\u5176\u64cd\u63a7\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6c9f\u901a\u7b56\u7565\u53ef\u80fd\u88ab\u7528\u4e8e\u9690\u79d8\u653b\u51fb\u4ee5\u83b7\u53d6\u79c1\u4eba\u4fe1\u606f\u7684\u98ce\u9669\uff0c\u586b\u8865\u8fd9\u4e00\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7528\u6237\u7814\u7a76\uff08\u6837\u672c\u91cf\u4e3a84\uff09\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "result": "\u9488\u5bf9\u6027\u653b\u51fb\u6bd4\u6ca1\u6709\u7b56\u7565\u7684\u9690\u79d8\u4e92\u52a8\u6210\u529f\u63d0\u53d6\u76ee\u6807\u4fe1\u606f\u7684\u80fd\u529b\u63d0\u9ad8\u4e86205.4%\uff0c\u5373\u4f7f\u662f\u5728\u6ca1\u6709\u5177\u4f53\u7b56\u7565\u7684\u9690\u79d8\u4e92\u52a8\u4e2d\uff0c54.8%\u7684\u6848\u4f8b\u4e5f\u6210\u529f\u83b7\u53d6\u4e86\u79c1\u4eba\u4fe1\u606f\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u9002\u5e94\u6027\u653b\u51fb\u6846\u67b6\u5728\u591a\u4e2a\u573a\u666f\u4e0b\u6709\u6548\u63d0\u9ad8\u4e86\u4ece\u7528\u6237\u5904\u83b7\u53d6\u79c1\u5bc6\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u4e14\u7528\u6237\u96be\u4ee5\u5bdf\u89c9\u5230\u64cd\u63a7\u7684\u5b58\u5728\u3002"}}
{"id": "2511.11777", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11777", "abs": "https://arxiv.org/abs/2511.11777", "authors": ["Vinit Mehta", "Charu Sharma", "Karthick Thiyagarajan"], "title": "Large Language Models and 3D Vision for Intelligent Robotic Perception and Autonomy: A Review", "comment": "45 pages, 15 figures, MDPI Sensors Journal", "summary": "With the rapid advancement of artificial intelligence and robotics, the integration of Large Language Models (LLMs) with 3D vision is emerging as a transformative approach to enhancing robotic sensing technologies. This convergence enables machines to perceive, reason and interact with complex environments through natural language and spatial understanding, bridging the gap between linguistic intelligence and spatial perception. This review provides a comprehensive analysis of state-of-the-art methodologies, applications and challenges at the intersection of LLMs and 3D vision, with a focus on next-generation robotic sensing technologies. We first introduce the foundational principles of LLMs and 3D data representations, followed by an in-depth examination of 3D sensing technologies critical for robotics. The review then explores key advancements in scene understanding, text-to-3D generation, object grounding and embodied agents, highlighting cutting-edge techniques such as zero-shot 3D segmentation, dynamic scene synthesis and language-guided manipulation. Furthermore, we discuss multimodal LLMs that integrate 3D data with touch, auditory and thermal inputs, enhancing environmental comprehension and robotic decision-making. To support future research, we catalog benchmark datasets and evaluation metrics tailored for 3D-language and vision tasks. Finally, we identify key challenges and future research directions, including adaptive model architectures, enhanced cross-modal alignment and real-time processing capabilities, which pave the way for more intelligent, context-aware and autonomous robotic sensing systems.", "AI": {"tldr": "\u7efc\u8ff0LLMs\u4e0e3D\u89c6\u89c9\u7ed3\u5408\u7684\u6280\u672f\u8fdb\u5c55\u53ca\u672a\u6765\u6311\u6218\uff0c\u52a9\u529b\u667a\u80fd\u673a\u5668\u4eba\u611f\u77e5\u7cfb\u7edf\u7684\u53d1\u5c55", "motivation": "\u63a2\u8ba8\u4eba\u5de5\u667a\u80fd\u4e0e\u673a\u5668\u4eba\u6280\u672f\u7684\u6574\u5408\uff0c\u4ee5\u63d0\u5347\u673a\u5668\u7684\u611f\u77e5\u80fd\u529b", "method": "\u7efc\u8ff0\u73b0\u6709\u7684LLMs\u4e0e3D\u89c6\u89c9\u7ed3\u5408\u7684\u6280\u672f", "result": "\u5206\u6790\u4e86LLMs\u4e0e3D\u89c6\u89c9\u4ea4\u53c9\u9886\u57df\u7684\u5148\u8fdb\u65b9\u6cd5\u3001\u5e94\u7528\u53ca\u6311\u6218\uff0c\u63d0\u4f9b\u672a\u6765\u7814\u7a76\u65b9\u5411", "conclusion": "\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u5173\u952e\u6311\u6218\u548c\u65b9\u5411\uff0c\u5982\u6a21\u578b\u67b6\u6784\u7684\u9002\u5e94\u6027\u3001\u8de8\u6a21\u6001\u5bf9\u9f50\u7b49\uff0c\u4ee5\u4fc3\u8fdb\u667a\u80fd\u673a\u5668\u4eba\u611f\u77e5\u6280\u672f\u7684\u8fdb\u6b65\u3002"}}
{"id": "2511.11962", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.11962", "abs": "https://arxiv.org/abs/2511.11962", "authors": ["Carlos Mosquera", "Neven Elsayed", "Ernst Kruijff", "Joseph Newman", "Eduardo Veas"], "title": "A Study of Performance and Interaction Patterns in Hand and Tangible Interaction in Tabletop Mixed Reality", "comment": null, "summary": "This paper presents a comprehensive study of virtual 3D object manipulation along 4DoF on real surfaces in mixed reality (MR), using hand-based and tangible interactions. A custom cylindrical tangible proxy leverages affordances of physical knobs and tabletop support for stable input. We evaluate both modalities across isolated tasks (2DoF translation, 1DoF rotation scaling), semicombined (3DoF translation rotation), and full 4DoF compound manipulation.\n  We offer analyses of hand interactions, tangible interactions, and their comparison in MR tasks. For hand interactions, compound tasks required repetitive corrections, increasing completion times yet surprisingly, rotation errors were smaller in compound tasks than in rotation only tasks. Tangible interactions exhibited significantly larger errors in translation, rotation, and scaling during compound tasks compared to isolated tasks. Crucially, tangible interactions outperformed hand interactions in precision, likely due to tabletop support and constrained 4DoF design. These findings inform designers opting for hand-only interaction (highlighting tradeoffs in compound tasks) and those leveraging tangibles (emphasizing precision gains despite compound-task challenges).", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u6df7\u5408\u73b0\u5b9e\u4e2d\u57fa\u4e8e\u624b\u548c\u5b9e\u4f53\u4ea4\u4e92\u7684\u865a\u62df3D\u5bf9\u8c61 \u56db\u7ef4\u64cd\u4f5c\uff0c\u8bc4\u4f30\u5176\u5728\u4e0d\u540c\u4efb\u52a1\u4e0b\u7684\u6027\u80fd\u548c\u7cbe\u5ea6\u3002", "motivation": "\u7814\u7a76\u7684\u52a8\u673a\u662f\u4e86\u89e3\u4e0d\u540c\u4ea4\u4e92\u65b9\u5f0f\u5728\u6df7\u5408\u73b0\u5b9e\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u4ece\u800c\u4e3a\u8bbe\u8ba1\u8005\u63d0\u4f9b\u53c2\u8003\u3002", "method": "\u901a\u8fc7\u8bc4\u4f30\u57fa\u4e8e\u624b\u548c\u5b9e\u4f53\u7684\u4ea4\u4e92\u5728\u4e0d\u540c\u7ef4\u5ea6\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u7684\u51c6\u786e\u6027\u548c\u65f6\u95f4\u6548\u7387\u3002", "result": "\u624b\u90e8\u4ea4\u4e92\u5728\u590d\u5408\u4efb\u52a1\u4e2d\u9700\u8981\u91cd\u590d\u6821\u6b63\uff0c\u6574\u4f53\u5b8c\u6210\u65f6\u95f4\u8f83\u957f\uff0c\u800c\u5b9e\u4f53\u4ea4\u4e92\u5728\u7a33\u5b9a\u6027\u548c\u8f93\u5165\u65b9\u9762\u8868\u73b0\u4f18\u79c0\uff0c\u7279\u522b\u662f\u5728\u7cbe\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u624b\u90e8\u4ea4\u4e92\u3002", "conclusion": "\u57284DoF\u64cd\u4f5c\u4e2d\uff0c\u5b9e\u4f53\u4ea4\u4e92\u5728\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u624b\u90e8\u4ea4\u4e92\uff0c\u4f46\u5728\u590d\u5408\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u5927\u7684\u9519\u8bef\uff0c\u800c\u624b\u90e8\u4ea4\u4e92\u867d\u7136\u5b8c\u6210\u65f6\u95f4\u957f\uff0c\u4f46\u5728\u65cb\u8f6c\u9519\u8bef\u4e0a\u8868\u73b0\u66f4\u597d\u3002"}}
{"id": "2511.11840", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11840", "abs": "https://arxiv.org/abs/2511.11840", "authors": ["Shuangyu Xie", "Kaiyuan Chen", "Wenjing Chen", "Chengyuan Qian", "Christian Juette", "Liu Ren", "Dezhen Song", "Ken Goldberg"], "title": "LAVQA: A Latency-Aware Visual Question Answering Framework for Shared Autonomy in Self-Driving Vehicles", "comment": null, "summary": "When uncertainty is high, self-driving vehicles may halt for safety and benefit from the access to remote human operators who can provide high-level guidance. This paradigm, known as {shared autonomy}, enables autonomous vehicle and remote human operators to jointly formulate appropriate responses. To address critical decision timing with variable latency due to wireless network delays and human response time, we present LAVQA, a latency-aware shared autonomy framework that integrates Visual Question Answering (VQA) and spatiotemporal risk visualization. LAVQA augments visual queries with Latency-Induced COllision Map (LICOM), a dynamically evolving map that represents both temporal latency and spatial uncertainty. It enables remote operator to observe as the vehicle safety regions vary over time in the presence of dynamic obstacles and delayed responses. Closed-loop simulations in CARLA, the de-facto standard for autonomous vehicle simulator, suggest that that LAVQA can reduce collision rates by over 8x compared to latency-agnostic baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLAVQA\u7684\u5ef6\u8fdf\u611f\u77e5\u5171\u4eab\u81ea\u6cbb\u6846\u67b6\uff0c\u4ee5\u63d0\u5347\u81ea\u9a7e\u8f66\u5728\u9ad8\u4e0d\u786e\u5b9a\u6027\u60c5\u51b5\u4e0b\u7684\u5b89\u5168\u6027\uff0c\u80fd\u6709\u6548\u51cf\u5c11\u78b0\u649e\u7387\u3002", "motivation": "\u5728\u4e0d\u786e\u5b9a\u6027\u9ad8\u7684\u60c5\u51b5\u4e0b\uff0c\u81ea\u9a7e\u8f66\u9700\u8981\u786e\u4fdd\u5b89\u5168\uff0c\u5e76\u4f9d\u9760\u8fdc\u7a0b\u4eba\u7c7b\u64cd\u4f5c\u5458\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u7ed3\u5408\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u548c\u65f6\u7a7a\u98ce\u9669\u53ef\u89c6\u5316\uff0c\u4f7f\u7528\u5ef6\u8fdf\u8bf1\u5bfc\u78b0\u649e\u56fe\uff08LICOM\uff09\u6765\u8f85\u52a9\u51b3\u7b56\u3002", "result": "\u5728CARLA\u4eff\u771f\u4e2d\uff0cLAVQA\u7684\u78b0\u649e\u7387\u6bd4\u5ef6\u8fdf\u65e0\u5173\u57fa\u7ebf\u964d\u4f4e\u8d85\u8fc78\u500d\u3002", "conclusion": "LAVQA\u6846\u67b6\u80fd\u591f\u663e\u8457\u964d\u4f4e\u78b0\u649e\u7387\uff0c\u63d0\u5347\u81ea\u9a7e\u8f66\u5728\u590d\u6742\u60c5\u51b5\u4e0b\u7684\u51b3\u7b56\u80fd\u529b\u3002"}}
{"id": "2511.12068", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.12068", "abs": "https://arxiv.org/abs/2511.12068", "authors": ["Nana Tian", "Giorgio Colombo", "Victor Schinazi"], "title": "From Play to Detection: Mini-SPACE as a Serious Game for Unsupervised Cognitive Impairment Screening", "comment": "22 pages", "summary": "Early detection of Cognitive Impairment (CI) is critical for timely intervention, preservation of independence, and reducing the burden of dementia. Yet, most screening tools remain lengthy, clinic-based, and poorly suited for large-scale unsupervised deployment. This paper evaluates the test-retest reliability, validity, and usability of mini-SPACE, a short iPad-based serious game for detecting early signs of CI. Participants played mini-SPACE at home without supervision once a week for three weeks, with a longer version of the game in the final week. Mini-SPACE showed good test-retest reliability in unsupervised settings. While younger age was the primary predictor of performance, usability, and cognitive load, participants of all ages were able to complete the tasks and reported good usability and low cognitive load. Importantly, the prediction of scores in the Montreal Cognitive Assessment (MoCA) improved with repeated measures. These findings highlight mini-SPACE as a promising digital marker for scalable, age-sensitive screening and potential longitudinal tracking of CI.", "AI": {"tldr": "mini-SPACE\u662f\u4e00\u4e2a\u77ed\u5c0f\u7684iPad\u6e38\u620f\uff0c\u9002\u5408\u7528\u4e8e\u65e9\u671f\u8ba4\u77e5\u969c\u788d\u7684\u7b5b\u67e5\uff0c\u663e\u793a\u51fa\u826f\u597d\u7684\u53ef\u9760\u6027\u548c\u53ef\u7528\u6027\uff0c\u4e14\u9002\u5408\u5404\u5e74\u9f84\u6bb5\u4f7f\u7528\u3002", "motivation": "\u65e9\u671f\u68c0\u6d4b\u8ba4\u77e5\u969c\u788d\u5bf9\u4e8e\u53ca\u65f6\u5e72\u9884\u548c\u51cf\u8f7b\u75f4\u5446\u8d1f\u62c5\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7b5b\u67e5\u5de5\u5177\u8fc7\u4e8e\u6f2b\u957f\u4e14\u4e0d\u9002\u5408\u5927\u89c4\u6a21\u65e0\u76d1\u7763\u90e8\u7f72\u3002", "method": "\u8bc4\u4f30mini-SPACE\u5728\u65e9\u671f\u8ba4\u77e5\u969c\u788d\u68c0\u6d4b\u4e2d\u7684\u6d4b\u8bd5-\u91cd\u6d4b\u53ef\u9760\u6027\u3001\u6709\u6548\u6027\u548c\u53ef\u7528\u6027", "result": "mini-SPACE\u5728\u65e0\u76d1\u7763\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6d4b\u8bd5-\u91cd\u6d4b\u53ef\u9760\u6027\uff0c\u53c2\u4e0e\u8005\u5728\u6240\u6709\u5e74\u9f84\u6bb5\u5747\u80fd\u5b8c\u6210\u4efb\u52a1\uff0c\u4e14\u62a5\u544a\u7684\u53ef\u7528\u6027\u826f\u597d\u3001\u8ba4\u77e5\u8d1f\u8377\u4f4e\u3002", "conclusion": "mini-SPACE\u662f\u4e00\u79cd\u4fc3\u8fdb\u53ef\u6269\u5c55\u548c\u5e74\u9f84\u654f\u611f\u7b5b\u67e5\u7684\u6709\u524d\u666f\u7684\u6570\u5b57\u6807\u8bb0\uff0c\u80fd\u591f\u7528\u4e8e\u8ba4\u77e5\u969c\u788d\u7684\u6f5c\u5728\u957f\u671f\u8ddf\u8e2a\u3002"}}
{"id": "2511.11845", "categories": ["cs.RO", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.11845", "abs": "https://arxiv.org/abs/2511.11845", "authors": ["K. A. I. N Jayarathne", "R. M. N. M. Rathnayaka", "D. P. S. S. Peiris"], "title": "Autonomous Underwater Cognitive System for Adaptive Navigation: A SLAM-Integrated Cognitive Architecture", "comment": "6 pages, 2 figures", "summary": "Deep-sea exploration poses significant challenges, including disorientation, communication loss, and navigational failures in dynamic underwater environments. This paper presents an Autonomous Underwater Cognitive System (AUCS) that integrates Simultaneous Localization and Mapping (SLAM) with a Soar-based cognitive architecture to enable adaptive navigation in complex oceanic conditions. The system fuses multi-sensor data from SONAR, LiDAR, IMU, and DVL with cognitive reasoning modules for perception, attention, planning, and learning. Unlike conventional SLAM systems, AUCS incorporates semantic understanding, adaptive sensor management, and memory-based learning to differentiate between dynamic and static objects, reducing false loop closures and enhancing long-term map consistency. The proposed architecture demonstrates a complete perception-cognition-action-learning loop, allowing autonomous underwater vehicles to sense, reason, and adapt intelligently. This work lays a foundation for next-generation cognitive submersible systems, improving safety, reliability, and autonomy in deep-sea exploration.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u7684\u81ea\u4e3b\u6c34\u4e0b\u8ba4\u77e5\u7cfb\u7edf\uff08AUCS\uff09\u5229\u7528SLAM\u548c\u8ba4\u77e5\u67b6\u6784\u63d0\u9ad8\u4e86\u6df1\u6d77\u63a2\u7d22\u7684\u5bfc\u822a\u80fd\u529b\uff0c\u878d\u5408\u4e86\u591a\u79cd\u4f20\u611f\u5668\u6570\u636e\uff0c\u589e\u5f3a\u4e86\u5b89\u5168\u6027\u548c\u81ea\u4e3b\u6027\u3002", "motivation": "\u6df1\u6d77\u63a2\u7d22\u9762\u4e34\u5bfc\u822a\u5931\u7075\u3001\u901a\u4fe1\u4e2d\u65ad\u548c\u52a8\u6001\u73af\u5883\u7684\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u667a\u80fd\u9002\u5e94\u5404\u79cd\u6d77\u6d0b\u6761\u4ef6\u7684\u7cfb\u7edf\u3002", "method": "AUCS\u7ed3\u5408SLAM\u6280\u672f\u4e0e\u8ba4\u77e5\u6a21\u5757\uff0c\u901a\u8fc7\u591a\u4f20\u611f\u5668\u6570\u636e\u878d\u5408\u4e0e\u8bed\u4e49\u7406\u89e3\uff0c\u5b9e\u73b0\u4e86\u81ea\u9002\u5e94\u5bfc\u822a\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u4e3b\u6c34\u4e0b\u8ba4\u77e5\u7cfb\u7edf\uff08AUCS\uff09\u6765\u5e94\u5bf9\u6df1\u6d77\u52d8\u63a2\u4e2d\u9762\u4e34\u7684\u6311\u6218\uff0c\u7ed3\u5408\u4e86\u540c\u65f6\u5b9a\u4f4d\u4e0e\u5730\u56fe\u6784\u5efa\uff08SLAM\uff09\u4e0e\u57fa\u4e8eSoar\u7684\u8ba4\u77e5\u67b6\u6784\uff0c\u589e\u5f3a\u4e86\u5728\u590d\u6742\u6d77\u6d0b\u6761\u4ef6\u4e0b\u7684\u81ea\u9002\u5e94\u5bfc\u822a\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684AUCS\u67b6\u6784\u4e3a\u672a\u6765\u7684\u8ba4\u77e5\u6f5c\u6c34\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u6df1\u6d77\u63a2\u6d4b\u7684\u5b89\u5168\u6027\u3001\u53ef\u9760\u6027\u548c\u81ea\u4e3b\u6027\u3002"}}
{"id": "2511.12394", "categories": ["cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12394", "abs": "https://arxiv.org/abs/2511.12394", "authors": ["Prithila Angkan", "Amin Jalali", "Paul Hungler", "Ali Etemad"], "title": "Multi-Domain EEG Representation Learning with Orthogonal Mapping and Attention-based Fusion for Cognitive Load Classification", "comment": "This work has been submitted to the Transactions on Human Machine Systems for possible publication", "summary": "We propose a new representation learning solution for the classification of cognitive load based on Electroencephalogram (EEG). Our method integrates both time and frequency domains by first passing the raw EEG signals through the convolutional encoder to obtain the time domain representations. Next, we measure the Power Spectral Density (PSD) for all five EEG frequency bands and generate the channel power values as 2D images referred to as multi-spectral topography maps. These multi-spectral topography maps are then fed to a separate encoder to obtain the representations in frequency domain. Our solution employs a multi-domain attention module that maps these domain-specific embeddings onto a shared embedding space to emphasize more on important inter-domain relationships to enhance the representations for cognitive load classification. Additionally, we incorporate an orthogonal projection constraint during the training of our method to effectively increase the inter-class distances while improving intra-class clustering. This enhancement allows efficient discrimination between different cognitive states and aids in better grouping of similar states within the feature space. We validate the effectiveness of our model through extensive experiments on two public EEG datasets, CL-Drive and CLARE for cognitive load classification. Our results demonstrate the superiority of our multi-domain approach over the traditional single-domain techniques. Moreover, we conduct ablation and sensitivity analyses to assess the impact of various components of our method. Finally, robustness experiments on different amounts of added noise demonstrate the stability of our method compared to other state-of-the-art solutions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8ba4\u77e5\u8d1f\u8377\u5206\u7c7b\u65b9\u6cd5\uff0c\u7ed3\u5408\u65f6\u95f4\u548c\u9891\u7387\u57df\u7279\u5f81\uff0c\u901a\u8fc7\u591a\u57df\u6ce8\u610f\u529b\u6a21\u5757\u548c\u6b63\u4ea4\u6295\u5f71\u7ea6\u675f\uff0c\u63d0\u9ad8\u5206\u7c7b\u7cbe\u5ea6\u548c\u6297\u5e72\u6270\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u8ba4\u77e5\u8d1f\u8377\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u7814\u7a76\u8005\u7ed3\u5408\u65f6\u95f4\u548c\u9891\u7387\u57df\u7684\u4fe1\u606f\uff0c\u63a2\u7d22\u65b0\u7684\u7279\u5f81\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u79d1\u7814\u56e2\u961f\u91c7\u7528\u5377\u79ef\u7f16\u7801\u5668\u5904\u7406\u539f\u59cbEEG\u4fe1\u53f7\u4ee5\u63d0\u53d6\u65f6\u95f4\u57df\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u4e94\u4e2a\u9891\u7387\u5e26\u7684\u529f\u7387\u8c31\u5bc6\u5ea6\u751f\u6210\u7684\u591a\u5149\u8c31\u5730\u5f62\u56fe\uff0c\u4f7f\u7528\u591a\u57df\u6ce8\u610f\u529b\u6a21\u5757\u6765\u805a\u7126\u5728\u91cd\u8981\u7684\u9886\u57df\u95f4\u5173\u7cfb\uff0c\u4ee5\u53ca\u5f15\u5165\u6b63\u4ea4\u6295\u5f71\u7ea6\u675f\u6765\u4f18\u5316\u7c7b\u95f4\u8ddd\u79bb\u548c\u7c7b\u5185\u805a\u7c7b\u3002", "result": "\u7814\u7a76\u5728\u4e24\u4e2a\u516c\u5f00EEG\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u7ed3\u679c\u663e\u793a\u6240\u63d0\u6a21\u578b\u5728\u8ba4\u77e5\u8d1f\u8377\u5206\u7c7b\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u540c\u65f6\u901a\u8fc7\u6d88\u878d\u548c\u654f\u611f\u6027\u5206\u6790\u8bc4\u4f30\u4e86\u5404\u7ec4\u4ef6\u7684\u5f71\u54cd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u591a\u57df\u5b66\u4e60\u65b9\u6cd5\u5728\u8ba4\u77e5\u8d1f\u8377\u5206\u7c7b\u4e0a\u4f18\u4e8e\u4f20\u7edf\u5355\u57df\u6280\u672f\uff0c\u5e76\u5728\u591a\u6b21\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2511.11931", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11931", "abs": "https://arxiv.org/abs/2511.11931", "authors": ["Saida Liu", "Nikolay Atanasov", "Shumon Koga"], "title": "MATT-Diff: Multimodal Active Target Tracking by Diffusion Policy", "comment": "14 pages, 3 figures. Submitted to L4DC 2026", "summary": "This paper proposes MATT-Diff: Multi-Modal Active Target Tracking by Diffusion Policy, a control policy that captures multiple behavioral modes - exploration, dedicated tracking, and target reacquisition - for active multi-target tracking. The policy enables agent control without prior knowledge of target numbers, states, or dynamics. Effective target tracking demands balancing exploration for undetected or lost targets with following the motion of detected but uncertain ones. We generate a demonstration dataset from three expert planners including frontier-based exploration, an uncertainty-based hybrid planner switching between frontier-based exploration and RRT* tracking based on target uncertainty, and a time-based hybrid planner switching between exploration and tracking based on target detection time. We design a control policy utilizing a vision transformer for egocentric map tokenization and an attention mechanism to integrate variable target estimates represented by Gaussian densities. Trained as a diffusion model, the policy learns to generate multi-modal action sequences through a denoising process. Evaluations demonstrate MATT-Diff's superior tracking performance against expert and behavior cloning baselines across multiple target motions, empirically validating its advantages in target tracking.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u591a\u6a21\u6001\u4e3b\u52a8\u76ee\u6807\u8ddf\u8e2a\u7b56\u7565MATT-Diff\uff0c\u80fd\u591f\u5728\u65e0\u5148\u9a8c\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u8ddf\u8e2a\u591a\u4e2a\u76ee\u6807\u3002", "motivation": "\u5b9e\u73b0\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\uff0c\u5e73\u8861\u76ee\u6807\u63a2\u7d22\u4e0e\u8ddf\u8e2a\uff0c\u9002\u5e94\u590d\u6742\u73af\u5883\u4e2d\u7684\u52a8\u6001\u53d8\u5316\u3002", "method": "MATT-Diff\u63a7\u5236\u7b56\u7565", "result": "MATT-Diff\u5728\u591a\u79cd\u76ee\u6807\u8fd0\u52a8\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u8ddf\u8e2a\u6027\u80fd\uff0c\u76f8\u6bd4\u4e8e\u4e13\u5bb6\u548c\u884c\u4e3a\u514b\u9686\u57fa\u51c6\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u8bc4\u4f30\u7ed3\u679c\u9a8c\u8bc1\u4e86MATT-Diff\u5728\u76ee\u6807\u8ddf\u8e2a\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u5c24\u5176\u5728\u5e94\u5bf9\u590d\u6742\u52a8\u6001\u60c5\u51b5\u4e0b\u7684\u8868\u73b0\u3002"}}
{"id": "2511.12468", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.12468", "abs": "https://arxiv.org/abs/2511.12468", "authors": ["Atharva Mehta", "Rajesh Kumar", "Aman Singla", "Kartik Bisht", "Yaman Kumar Singla", "Rajiv Ratn Shah"], "title": "Detecting LLM-Assisted Academic Dishonesty using Keystroke Dynamics", "comment": "17 pages, 4 figures, 5 tables, extension of IJCB 2024 paper, and under review at IEEE TBIOM", "summary": "The rapid adoption of generative AI tools has intensified the challenge of maintaining academic integrity. Conventional plagiarism detectors, which rely on text-matching or text-intrinsic features, often fail to identify submissions that have been AI-assisted or paraphrased. To address this limitation, we introduce keystroke-dynamics-based detectors that analyze how, rather than what, a person writes to distinguish genuine from assisted writing. Building on our earlier study, which collected keystroke data from 40 participants and trained a modified TypeNet model to detect assisted text, we expanded the dataset by adding 90 new participants and introducing a paraphrasing-based plagiarism-detection mode. We then benchmarked two additional gradient-boosting classifiers, LightGBM and CatBoost, alongside TypeNet, and compared their performance with DetectGPT, LLaMA 3.3 70B Instruct, and the results of 44 human evaluators. To further assess and improve robustness, we proposed a deception-based threat model simulating forged keystrokes and applied adversarial training as a countermeasure. Results show that the machine learning models achieve F1 scores above 97% in structured settings, while TypeNet performs best in detecting paraphrasing, with an F1 score of 86.9%. In contrast, text-only detectors and human evaluators perform near-chance, demonstrating that keystroke dynamics provide a strong behavioral signal for identifying AI-assisted plagiarism and support the use of multimodal behavioral features for reliable academic integrity assessment.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u51fb\u952e\u52a8\u6001\u68c0\u6d4bAI\u8f85\u52a9\u5199\u4f5c\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8bc6\u522b\u5b66\u672f\u6284\u88ad\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u751f\u6210\u6027AI\u5de5\u5177\u7684\u666e\u53ca\uff0c\u4f20\u7edf\u7684\u6284\u88ad\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u8bc6\u522bAI\u8f85\u52a9\u5199\u4f5c\u7684\u6587\u672c\uff0c\u8feb\u5207\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u6765\u7ef4\u62a4\u5b66\u672f\u8bda\u4fe1\u3002", "method": "\u7814\u7a76\u57fa\u4e8e\u6269\u5c55\u7684\u51fb\u952e\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5305\u62ecTypeNet\u3001LightGBM\u548cCatBoost\uff0c\u5e76\u5229\u7528\u5bf9\u6297\u8bad\u7ec3\u589e\u5f3a\u68c0\u6d4b\u5668\u7684\u9c81\u68d2\u6027\u3002", "result": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51fb\u952e\u52a8\u6001\u7684\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u5206\u6790\u4e2a\u4f53\u5199\u4f5c\u65b9\u5f0f\u6765\u533a\u5206\u771f\u5b9e\u4e0e\u8f85\u52a9\u5199\u4f5c\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9AI\u8f85\u52a9\u6284\u88ad\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "conclusion": "\u51fb\u952e\u52a8\u6001\u68c0\u6d4b\u5668\u5728\u8bc6\u522bAI\u8f85\u52a9\u6284\u88ad\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u4f20\u7edf\u6587\u672c\u5339\u914d\u65b9\u6cd5\uff0c\u652f\u6301\u5b66\u672f\u8bda\u4fe1\u8bc4\u4f30\u7684\u591a\u6a21\u6001\u7279\u5f81\u4f7f\u7528\u3002"}}
{"id": "2511.11958", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11958", "abs": "https://arxiv.org/abs/2511.11958", "authors": ["Derek Chen", "Zoe Samuels", "Lizzie Peiros", "Sujaan Mukherjee", "Michael C. Yip"], "title": "Characterization and Evaluation of Screw-Based Locomotion Across Aquatic, Granular, and Transitional Media", "comment": null, "summary": "Screw-based propulsion systems offer promising capabilities for amphibious mobility, yet face significant challenges in optimizing locomotion across water, granular materials, and transitional environments. This study presents a systematic investigation into the locomotion performance of various screw configurations in media such as dry sand, wet sand, saturated sand, and water. Through a principles-first approach to analyze screw performance, it was found that certain parameters are dominant in their impact on performance. Depending on the media, derived parameters inspired from optimizing heat sink design help categorize performance within the dominant design parameters. Our results provide specific insights into screw shell design and adaptive locomotion strategies to enhance the performance of screw-based propulsion systems for versatile amphibious applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u87ba\u65cb\u63a8\u8fdb\u7cfb\u7edf\u5728\u6c34\u3001\u5e72\u6c99\u3001\u6e7f\u6c99\u548c\u9971\u548c\u6c99\u4e2d\u8fd0\u52a8\u6027\u80fd\uff0c\u53d1\u73b0\u5173\u952e\u53c2\u6570\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u5982\u4f55\u8bbe\u8ba1\u87ba\u65cb\u5916\u58f3\u4ee5\u6539\u5584\u5176\u5728\u591a\u79cd\u73af\u5883\u4e0b\u7684\u9002\u5e94\u6027\u8fd0\u52a8\u7b56\u7565\u3002", "motivation": "\u87ba\u65cb\u63a8\u8fdb\u7cfb\u7edf\u5728\u4e24\u6816\u79fb\u52a8\u8fc7\u7a0b\u4e2d\u9762\u4e34\u5728\u4e0d\u540c\u4ecb\u8d28\u4e2d\u4f18\u5316\u8fd0\u52a8\u7684\u6311\u6218\uff0c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u96be\u9898\u3002", "method": "\u901a\u8fc7\u539f\u5219\u4f18\u5148\u7684\u65b9\u6cd5\u7cfb\u7edf\u5206\u6790\u4e0d\u540c\u87ba\u65cb\u914d\u7f6e\u5728\u591a\u79cd\u4ecb\u8d28\u4e2d\uff08\u5982\u5e72\u6c99\u3001\u6e7f\u6c99\u3001\u9971\u548c\u6c99\u548c\u6c34\uff09\u7684\u8fd0\u52a8\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u67d0\u4e9b\u53c2\u6570\u5bf9\u87ba\u65cb\u63a8\u8fdb\u7cfb\u7edf\u7684\u6027\u80fd\u6709\u4e3b\u5bfc\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u70ed\u6c89\u8bbe\u8ba1\u4f18\u5316\u7684\u6d3e\u751f\u53c2\u6570\u4ee5\u5206\u7c7b\u4e0d\u540c\u4ecb\u8d28\u4e2d\u7684\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u87ba\u65cb\u63a8\u8fdb\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5177\u4f53\u7684\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u5176\u5728\u591a\u6837\u5316\u4e24\u6816\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2511.12521", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.12521", "abs": "https://arxiv.org/abs/2511.12521", "authors": ["Michal R. Wrobel"], "title": "A Proxy-Based Method for Mapping Discrete Emotions onto VAD model", "comment": null, "summary": "Mapping discrete and dimensional models of emotion remains a persistent challenge in affective science and computing. This incompatibility hinders the combination of valuable data sets, creating a significant bottleneck for training robust machine learning models. To bridge this gap, this paper presents a novel, human-centric, proxy-based approach that transcends purely computational or direct mapping techniques. Implemented through a web-based survey, the method utilizes simple, user-generated geometric animations as intermediary artifacts to establish a correspondence between discrete emotion labels and the continuous valence-arousal-dominance (VAD) space. The approach involves a two-phase process: first, each participant creates an animation to represent a given emotion label (encoding); then, they immediately assess their own creation on the three VAD dimensions. The method was empirically validated and refined through two iterative user studies. The results confirmed the method's robustness. Combining the data from both studies generated a final, comprehensive mapping between discrete and dimensional models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4eba\u672c\u5bfc\u5411\u7684\u4ee3\u7406\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u60c5\u611f\u5448\u73b0\u4e2d\u79bb\u6563\u6a21\u578b\u4e0e\u7ef4\u5ea6\u6a21\u578b\u4e4b\u95f4\u7684\u4e0d\u517c\u5bb9\uff0c\u901a\u8fc7\u7528\u6237\u751f\u6210\u7684\u51e0\u4f55\u52a8\u753b\u5728\u79bb\u6563\u60c5\u611f\u6807\u7b7e\u548c\u8fde\u7eed\u7684VAD\u7a7a\u95f4\u4e4b\u95f4\u5efa\u7acb\u5bf9\u5e94\u5173\u7cfb\u3002", "motivation": "\u89e3\u51b3\u60c5\u611f\u79d1\u5b66\u4e0e\u8ba1\u7b97\u4e2d\u79bb\u6563\u6a21\u578b\u4e0e\u7ef4\u5ea6\u6a21\u578b\u4e4b\u95f4\u7684\u4e0d\u517c\u5bb9\u6027\uff0c\u4ee5\u4fbf\u6574\u5408\u66f4\u6709\u4ef7\u503c\u7684\u6570\u636e\u96c6\uff0c\u63d0\u5347\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u679c\u3002", "method": "\u91c7\u7528\u7f51\u7edc\u8c03\u67e5\uff0c\u901a\u8fc7\u7528\u6237\u751f\u6210\u7684\u51e0\u4f55\u52a8\u753b\u5728\u79bb\u6563\u60c5\u611f\u548c\u8fde\u7eedVAD\u7a7a\u95f4\u4e4b\u95f4\u5efa\u7acb\u8054\u7cfb\uff0c\u5206\u4e3a\u7f16\u7801\u548c\u8bc4\u4f30\u4e24\u4e2a\u9636\u6bb5\u3002", "result": "\u7ecf\u8fc7\u4e24\u8f6e\u7528\u6237\u7814\u7a76\uff0c\u9a8c\u8bc1\u548c\u4f18\u5316\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6210\u529f\u6784\u5efa\u4e86\u79bb\u6563\u60c5\u611f\u6807\u7b7e\u4e0eVAD\u7ef4\u5ea6\u4e4b\u95f4\u7684\u6620\u5c04\u5173\u7cfb\u3002", "conclusion": "\u7ed3\u5408\u4e24\u4e2a\u7814\u7a76\u7684\u6570\u636e\uff0c\u6700\u7ec8\u5f62\u6210\u4e86\u4e00\u5957\u5b8c\u6574\u7684\u79bb\u6563\u4e0e\u7ef4\u5ea6\u6a21\u578b\u4e4b\u95f4\u7684\u6620\u5c04\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.11967", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11967", "abs": "https://arxiv.org/abs/2511.11967", "authors": ["Mani Amani", "Behrad Beheshti", "Reza Akhavian"], "title": "Bootstrapped LLM Semantics for Context-Aware Path Planning", "comment": null, "summary": "Prompting robots with natural language (NL) has largely been studied as what task to execute (goal selection, skill sequencing) rather than how to execute that task safely and efficiently in semantically rich, human-centric spaces. We address this gap with a framework that turns a large language model (LLM) into a stochastic semantic sensor whose outputs modulate a classical planner. Given a prompt and a semantic map, we draw multiple LLM \"danger\" judgments and apply a Bayesian bootstrap to approximate a posterior over per-class risk. Using statistics from the posterior, we create a potential cost to formulate a path planning problem. Across simulated environments and a BIM-backed digital twin, our method adapts how the robot moves in response to explicit prompts and implicit contextual information. We present qualitative and quantitative results.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u4f5c\u4e3a\u968f\u673a\u8bed\u4e49\u4f20\u611f\u5668\uff0c\u4ece\u800c\u5728\u590d\u6742\u7684\u4eba\u7c7b\u4e2d\u5fc3\u73af\u5883\u4e2d\u5b9e\u73b0\u5b89\u5168\u9ad8\u6548\u7684\u4efb\u52a1\u6267\u884c\u3002", "motivation": "\u7814\u7a76\u73b0\u6709\u7684\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u673a\u5668\u4eba\u4efb\u52a1\u6267\u884c\u7684\u5c40\u9650\u6027\uff0c\u5173\u6ce8\u5728\u590d\u6742\u73af\u5883\u4e2d\u5982\u4f55\u5b89\u5168\u9ad8\u6548\u5730\u6267\u884c\u4efb\u52a1\u3002", "method": "\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8f93\u51fa\u4e0e\u7ecf\u5178\u89c4\u5212\u5668\u7ed3\u5408\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u81ea\u52a9\u6cd5\u8fd1\u4f3c\u6bcf\u7c7b\u98ce\u9669\u7684\u540e\u9a8c\u5206\u5e03\uff0c\u8fdb\u800c\u5f62\u6210\u8def\u5f84\u89c4\u5212\u95ee\u9898\u3002", "result": "\u5728\u6a21\u62df\u73af\u5883\u548c\u6570\u5b57\u53cc\u80de\u80ce\u4e2d\u5c55\u793a\u4e86\u5b9a\u6027\u548c\u5b9a\u91cf\u7684\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u73af\u5883\u548c\u6570\u5b57\u53cc\u80de\u80ce\u4e2d\u6709\u6548\u5730\u8c03\u6574\u4e86\u673a\u5668\u4eba\u8fd0\u52a8\uff0c\u80fd\u591f\u54cd\u5e94\u660e\u786e\u63d0\u793a\u4e0e\u9690\u542b\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002"}}
{"id": "2511.12529", "categories": ["cs.HC", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12529", "abs": "https://arxiv.org/abs/2511.12529", "authors": ["Sanchaita Hazra", "Doeun Lee", "Bodhisattwa Prasad Majumder", "Sachin Kumar"], "title": "Accepted with Minor Revisions: Value of AI-Assisted Scientific Writing", "comment": null, "summary": "Large Language Models have seen expanding application across domains, yet their effectiveness as assistive tools for scientific writing -- an endeavor requiring precision, multimodal synthesis, and domain expertise -- remains insufficiently understood. We examine the potential of LLMs to support domain experts in scientific writing, with a focus on abstract composition. We design an incentivized randomized controlled trial with a hypothetical conference setup where participants with relevant expertise are split into an author and reviewer pool. Inspired by methods in behavioral science, our novel incentive structure encourages authors to edit the provided abstracts to an acceptable quality for a peer-reviewed submission. Our 2x2 between-subject design expands into two dimensions: the implicit source of the provided abstract and the disclosure of it. We find authors make most edits when editing human-written abstracts compared to AI-generated abstracts without source attribution, often guided by higher perceived readability in AI generation. Upon disclosure of source information, the volume of edits converges in both source treatments. Reviewer decisions remain unaffected by the source of the abstract, but bear a significant correlation with the number of edits made. Careful stylistic edits, especially in the case of AI-generated abstracts, in the presence of source information, improve the chance of acceptance. We find that AI-generated abstracts hold potential to reach comparable levels of acceptability to human-written ones with minimal revision, and that perceptions of AI authorship, rather than objective quality, drive much of the observed editing behavior. Our findings reverberate the significance of source disclosure in collaborative scientific writing.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u5199\u4f5c\u4e2d\u7684\u5e94\u7528\uff0c\u5c24\u5176\u662f\u6458\u8981\u64b0\u5199\u3002\u7814\u7a76\u8868\u660e\u4eba\u7c7b\u64b0\u5199\u6458\u8981\u7f16\u8f91\u66f4\u591a\uff0c\u6e90\u4fe1\u606f\u62ab\u9732\u540e\u7f16\u8f91\u6548\u679c\u8d8b\u540c\uff0cAI\u751f\u6210\u6458\u8981\u53ef\u8fbe\u5230\u76f8\u4f3c\u7684\u63a5\u53d7\u5ea6\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u5199\u4f5c\u4e2d\u4f5c\u4e3a\u8f85\u52a9\u5de5\u5177\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u5173\u6ce8\u6458\u8981\u64b0\u5199\u4ee5\u6307\u5bfc\u4f5c\u8005\u548c\u5ba1\u7a3f\u4eba\u66f4\u597d\u5730\u7406\u89e3\u5176\u5e94\u7528\u53ca\u5176\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u6fc0\u52b1\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\uff0c\u8bbe\u8ba1\u4e3a\u4e00\u4e2a\u865a\u62df\u4f1a\u8bae\u573a\u666f\uff0c\u53c2\u4e0e\u8005\u5206\u4e3a\u4f5c\u8005\u548c\u5ba1\u7a3f\u4eba\uff0c\u68c0\u9a8c\u7f16\u8f91\u884c\u4e3a\u4e0e\u4fe1\u606f\u62ab\u9732\u7684\u5173\u7cfb\u3002", "result": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u79d1\u5b66\u5199\u4f5c\u8f85\u52a9\u5de5\u5177\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u6458\u8981\u64b0\u5199\u65b9\u9762\u3002\u901a\u8fc7\u4e00\u4e2a\u6fc0\u52b1\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\uff0c\u53d1\u73b0\u4f5c\u8005\u5728\u7f16\u8f91\u4eba\u7c7b\u64b0\u5199\u7684\u6458\u8981\u65f6\u6bd4\u7f16\u8f91AI\u751f\u6210\u7684\u6458\u8981\u65f6\u8fdb\u884c\u66f4\u591a\u4fee\u6539\uff0c\u5e76\u4e14\u8fd9\u79cd\u60c5\u51b5\u4e0e\u9605\u8bfb\u6027\u6709\u5173\u3002\u4fe1\u606f\u62ab\u9732\u540e\uff0c\u7f16\u8f91\u7684\u6570\u91cf\u5728\u4e0d\u540c\u6765\u6e90\u7684\u6458\u8981\u4e4b\u95f4\u8d8b\u4e8e\u76f8\u7b49\uff0c\u5ba1\u6838\u4eba\u5458\u7684\u51b3\u5b9a\u4e0e\u7f16\u8f91\u6570\u91cf\u5177\u6709\u663e\u8457\u76f8\u5173\u6027\u3002\u4f5c\u8005\u7684\u7ec6\u81f4\u98ce\u683c\u7f16\u8f91\u5728AI\u751f\u6210\u6458\u8981\u4e2d\u80fd\u591f\u63d0\u9ad8\u88ab\u63a5\u53d7\u7684\u53ef\u80fd\u6027\u3002\u6700\u540e\uff0cAI\u751f\u6210\u7684\u6458\u8981\u5728\u7ecf\u8fc7\u6700\u5c0f\u4fee\u6539\u540e\u4e5f\u80fd\u591f\u8fbe\u5230\u4e0e\u4eba\u7c7b\u64b0\u5199\u6458\u8981\u76f8\u5f53\u7684\u63a5\u53d7\u6c34\u5e73\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u6e90\u4fe1\u606f\u62ab\u9732\u5728\u79d1\u5b66\u534f\u4f5c\u5199\u4f5c\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u663e\u793aAI\u751f\u6210\u6458\u8981\u5177\u6709\u6f5c\u529b\uff0c\u5728\u7ecf\u8fc7\u9002\u5ea6\u4fee\u8ba2\u540e\u53ef\u4ee5\u4e0e\u4eba\u7c7b\u64b0\u5199\u7684\u6458\u8981\u76f8\u5ab2\u7f8e\u3002"}}
{"id": "2511.11970", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11970", "abs": "https://arxiv.org/abs/2511.11970", "authors": ["Sara Wickenhiser", "Lizzie Peiros", "Calvin Joyce", "Peter Gavrilrov", "Sujaan Mukherjee", "Syler Sylvester", "Junrong Zhou", "Mandy Cheung", "Jason Lim", "Florian Richter", "Michael C. Yip"], "title": "ARCSnake V2: An Amphibious Multi-Domain Screw-Propelled Snake-Like Robot", "comment": "8 pages, 9 figures, ICRA", "summary": "Robotic exploration in extreme environments such as caves, oceans, and planetary surfaces pose significant challenges, particularly in locomotion across diverse terrains. Conventional wheeled or legged robots often struggle in these contexts due to surface variability. This paper presents ARCSnake V2, an amphibious, screw propelled, snake like robot designed for teleoperated or autonomous locomotion across land, granular media, and aquatic environments. ARCSnake V2 combines the high mobility of hyper redundant snake robots with the terrain versatility of Archimedean screw propulsion. Key contributions include a water sealed mechanical design with serially linked screw and joint actuation, an integrated buoyancy control system, and teleoperation via a kinematically matched handheld controller. The robots design and control architecture enable multiple locomotion modes screwing, wheeling, and sidewinding with smooth transitions between them. Extensive experiments validate its underwater maneuverability, communication robustness, and force regulated actuation. These capabilities position ARCSnake V2 as a versatile platform for exploration, search and rescue, and environmental monitoring in multi domain settings.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u7684ARCSnake V2\u673a\u5668\u4eba\u5177\u6709\u591a\u79cd\u8fd0\u52a8\u6a21\u5f0f\uff0c\u80fd\u5728\u6781\u7aef\u73af\u5883\u4e2d\u7075\u6d3b\u9002\u5e94\uff0c\u5e76\u6709\u671b\u5e7f\u6cdb\u5e94\u7528\u4e8e\u63a2\u7d22\u4e0e\u73af\u5883\u76d1\u6d4b\u3002", "motivation": "\u6781\u7aef\u73af\u5883\u4e0b\u7684\u673a\u5668\u4eba\u63a2\u7d22\u9762\u4e34\u6311\u6218\uff0c\u4f20\u7edf\u673a\u5668\u4eba\u96be\u4ee5\u9002\u5e94\u591a\u6837\u5730\u5f62\uff0c\u6025\u9700\u65b0\u578b\u673a\u5668\u4eba\u6ee1\u8db3\u591a\u57df\u63a2\u6d4b\u9700\u6c42", "method": "ARCSnake V2\u7684\u6c34\u5bc6\u673a\u68b0\u8bbe\u8ba1\u3001\u4e32\u8054\u87ba\u65cb\u548c\u5173\u8282\u9a71\u52a8\u3001\u96c6\u6210\u6d6e\u529b\u63a7\u5236\u7cfb\u7edf\uff0c\u4ee5\u53ca\u901a\u8fc7\u624b\u6301\u63a7\u5236\u5668\u7684\u9065\u63a7\u64cd\u4f5c", "result": "ARCSnake V2\u5c55\u793a\u4e86\u5148\u8fdb\u7684\u5728\u591a\u79cd\u73af\u5883\u4e2d\u81ea\u5982\u8fd0\u52a8\u7684\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u6c34\u4e0b\u7075\u6d3b\u6027\u3001\u901a\u4fe1\u53ef\u9760\u6027\u53ca\u529b\u63a7\u9a71\u52a8\u7684\u6709\u6548\u6027", "conclusion": "ARCSnake V2\u7684\u8bbe\u8ba1\u4e0e\u63a7\u5236\u67b6\u6784\u4f7f\u5176\u6210\u4e3a\u63a2\u7d22\u3001\u6551\u63f4\u53ca\u73af\u5883\u76d1\u6d4b\u7684\u591a\u529f\u80fd\u5e73\u53f0\uff0c\u5177\u6709\u663e\u8457\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2511.12533", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.12533", "abs": "https://arxiv.org/abs/2511.12533", "authors": ["Botao 'Amber' Hu", "Danlin Huang"], "title": "Designing-with More-than-Human Through Human Augmentation", "comment": "Submitted to DRS 2026", "summary": "The recent more-than-human turn in design calls for \"designing-with\" other species and ecologies beyond humans. Yet-as Thomas Nagel's famous \"What is it like to be a bat?\" thought experiment highlights-human experience is constrained by our own sensorium and an irreducible gap in phenomenal access to nonhuman lifeworlds. This paper proposes More-than-Human through Human Augmentation (MtHtHA, denoted \">HtH+\") as a design approach that repurposes human augmentation technologies-typically aimed at enhancing human capabilities-away from human optimization and exceptionalism but toward eco-phenomenological awareness. Grounded in somaesthetic design and eco-somatics, MtHtHA entails creating temporary, embodied experiences that modulate the human Umwelt to re-sensitize us to pluriversal more-than-human perceptions. We articulate seven design principles and report five design cases-EchoVision (bat-like echolocation), FeltSight (star-nosed-mole tactile navigation), FungiSync (fungal network attunement), TentacUs (octopus-like distributed agency), and City of Sparkles (urban data from AI's perspective). We demonstrate that such experiential \"designing-with\" can cultivate ecological awareness, empathy and obligations of care across species boundaries.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u4eba\u7c7b\u589e\u5f3a\u6280\u672f\u4fc3\u8fdb\u5bf9\u751f\u6001\u548c\u975e\u4eba\u7c7b\u7269\u79cd\u7684\u611f\u77e5\uff0c\u65e8\u5728\u589e\u5f3a\u751f\u6001\u610f\u8bc6\u3002", "motivation": "\u5728\u8bbe\u8ba1\u4e2d\u4e0e\u5176\u4ed6\u7269\u79cd\u548c\u751f\u6001\u7cfb\u7edf\u5171\u540c\u521b\u9020\uff0c\u514b\u670d\u4eba\u7c7b\u7ecf\u9a8c\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u66f4\u6df1\u5c42\u6b21\u7684\u751f\u6001\u7406\u89e3\u548c\u5171\u751f\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u4eba\u7c7b\u589e\u5f3a\u6280\u672f\uff0c\u521b\u5efa\u4f53\u611f\u7684\u4e34\u65f6\u4f53\u9a8c\uff0c\u4ee5\u8c03\u8282\u4eba\u7c7b\u7684\u73af\u5883\u610f\u8bc6\uff0c\u589e\u5f3a\u5bf9\u591a\u5143\u8d85\u4eba\u7c7b\u611f\u77e5\u7684\u654f\u611f\u6027\u3002", "result": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5373\u901a\u8fc7\u4eba\u7c7b\u589e\u5f3a\u6280\u672f\u6765\u5b9e\u73b0\u8d85\u4eba\u7c7b\u7684\u8bbe\u8ba1\uff0c\u8fd9\u79cd\u65b9\u6cd5\u65e8\u5728\u589e\u5f3a\u5bf9\u975e\u4eba\u7c7b\u7269\u79cd\u7684\u611f\u77e5\u548c\u7406\u89e3\uff0c\u5e76\u4fc3\u8fdb\u751f\u6001\u610f\u8bc6\u548c\u8de8\u7269\u79cd\u7684\u5173\u6000\u8d23\u4efb\u3002", "conclusion": "\u8fd9\u79cd\u4f53\u9a8c\u5f0f\u7684\u8bbe\u8ba1\u80fd\u591f\u589e\u5f3a\u751f\u6001\u610f\u8bc6\u3001\u540c\u7406\u5fc3\u4ee5\u53ca\u8de8\u7269\u79cd\u7684\u5173\u6000\u8d23\u4efb\u3002"}}
{"id": "2511.12022", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12022", "abs": "https://arxiv.org/abs/2511.12022", "authors": ["Anh-Quan Pham", "Kabir Ram Puri", "Shreyas Raorane"], "title": "SBAMP: Sampling Based Adaptive Motion Planning", "comment": "8 pages, 13 figures", "summary": "Autonomous robotic systems must navigate complex, dynamic environments in real time, often facing unpredictable obstacles and rapidly changing conditions. Traditional sampling-based methods, such as RRT*, excel at generating collision-free paths but struggle to adapt to sudden changes without extensive replanning. Conversely, learning-based dynamical systems, such as the Stable Estimator of Dynamical Systems (SEDS), offer smooth, adaptive trajectory tracking but typically rely on pre-collected demonstration data, limiting their generalization to novel scenarios. This paper introduces Sampling-Based Adaptive Motion Planning (SBAMP), a novel framework that overcomes these limitations by integrating RRT* for global path planning with a SEDS-based local controller for continuous, adaptive trajectory adjustment. Our approach requires no pre-trained datasets and ensures smooth transitions between planned waypoints, maintaining stability through Lyapunov-based guarantees. We validate SBAMP in both simulated environments and real hardware using the RoboRacer platform, demonstrating superior performance in dynamic obstacle scenarios, rapid recovery from perturbations, and robust handling of sharp turns. Experimental results highlight SBAMP's ability to adapt in real time without sacrificing global path optimality, providing a scalable solution for dynamic, unstructured environments.", "AI": {"tldr": "SBAMP\u7ed3\u5408\u4e86\u5168\u5c40\u8def\u5f84\u89c4\u5212\u548c\u5c40\u90e8\u63a7\u5236\uff0c\u80fd\u591f\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u65f6\u9002\u5e94\uff0c\u800c\u65e0\u9700\u9884\u5148\u8bad\u7ec3\u7684\u6570\u636e\u96c6\u3002", "motivation": "\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\u9700\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u65f6\u5bfc\u822a\uff0c\u9762\u5bf9\u4e0d\u53ef\u9884\u89c1\u7684\u969c\u788d\u548c\u5feb\u901f\u53d8\u5316\u7684\u6761\u4ef6\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u5c06\u4f20\u7edf\u7684RRT*\u4e0e\u57fa\u4e8eSEDS\u7684\u5c40\u90e8\u63a7\u5236\u5668\u76f8\u7ed3\u5408\uff0c\u786e\u4fdd\u8def\u5f84\u5e73\u6ed1\u8fc7\u6e21\u548c\u7a33\u5b9a\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6SBAMP\uff0c\u5c06RRT*\u4e0eSEDS\u7ed3\u5408\uff0c\u5b9e\u73b0\u52a8\u6001\u9002\u5e94\u6027\u8f68\u8ff9\u8c03\u6574\u3002", "conclusion": "SBAMP\u5728\u52a8\u6001\u969c\u788d\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u5feb\u901f\u6062\u590d\u5e76\u5904\u7406\u6025\u8f6c\u5f2f\uff0c\u63d0\u4f9b\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12645", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.12645", "abs": "https://arxiv.org/abs/2511.12645", "authors": ["Junwei Li", "Wenqing Wang", "Huiliu Mao", "Jiazhe Ni", "Zuyu Xiong"], "title": "BeautyGuard: Designing a Multi-Agent Roundtable System for Proactive Beauty Tech Compliance through Stakeholder Collaboration", "comment": "International Conference on Human-Engaged Computing (ICHEC 2025), Singapore", "summary": "As generative AI enters enterprise workflows, ensuring compliance with legal, ethical, and reputational standards becomes a pressing challenge. In beauty tech, where biometric and personal data are central, traditional reviews are often manual, fragmented, and reactive. To examine these challenges, we conducted a formative study with six experts (four IT managers, two legal managers) at a multinational beauty company. The study revealed pain points in rule checking, precedent use, and the lack of proactive guidance.\n  Motivated by these findings, we designed a multi-agent \"roundtable\" system powered by a large language model. The system assigns role-specialized agents for legal interpretation, checklist review, precedent search, and risk mitigation, synthesizing their perspectives into structured compliance advice.\n  We evaluated the prototype with the same experts using System Usability Scale(SUS), The Official NASA Task Load Index(NASA-TLX), and interviews. Results show exceptional usability (SUS: 77.5/100) and minimal cognitive workload, with three key findings: (1) multi-agent systems can preserve tacit knowledge into standardized workflows, (2) information augmentation achieves higher acceptance than decision automation, and (3) successful enterprise AI should mirror organizational structures. This work contributes design principles for human-AI collaboration in compliance review, with broader implications for regulated industries beyond beauty tech.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u65e8\u5728\u63d0\u5347\u7f8e\u5bb9\u79d1\u6280\u884c\u4e1a\u5408\u89c4\u6027\uff0c\u901a\u8fc7\u4e13\u5bb6\u8bc4\u4f30\u8fdb\u884c\u53ef\u7528\u6027\u6d4b\u8bd5\u3002", "motivation": "\u968f\u7740\u751f\u6210\u6027\u4eba\u5de5\u667a\u80fd\u5728\u4f01\u4e1a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u5e94\u7528\uff0c\u7f8e\u5bb9\u79d1\u6280\u884c\u4e1a\u9762\u4e34\u7740\u6cd5\u5f8b\u548c\u4f26\u7406\u5408\u89c4\u7684\u7d27\u8feb\u6311\u6218\uff0c\u4f20\u7edf\u68c0\u67e5\u65b9\u6cd5\u5b58\u5728\u624b\u52a8\u3001\u5206\u6563\u548c\u53cd\u5e94\u5f0f\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4e0e\u516d\u4f4d\u4e13\u5bb6\u7684\u5f62\u6210\u6027\u7814\u7a76\u63a2\u8ba8\u5408\u89c4\u5ba1\u67e5\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u4f7f\u7528\u7cfb\u7edf\u53ef\u7528\u6027\u91cf\u8868\u548cNASA\u4efb\u52a1\u8d1f\u8377\u6307\u6570\u5bf9\u539f\u578b\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u4ee3\u7406\u201c\u5706\u684c\u201d\u7cfb\u7edf\uff0c\u4ee5\u89e3\u51b3\u7f8e\u5bb9\u79d1\u6280\u884c\u4e1a\u4e2d\u5408\u89c4\u68c0\u67e5\u7684\u6311\u6218\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u591a\u4ee3\u7406\u7cfb\u7edf\u5177\u6709\u51fa\u8272\u7684\u53ef\u7528\u6027\u548c\u4f4e\u8ba4\u77e5\u8d1f\u8377\uff0c\u80fd\u591f\u6709\u6548\u5730\u652f\u6301\u6cd5\u5f8b\u5408\u89c4\u5ba1\u67e5\uff0c\u5e76\u4e3a\u5176\u4ed6\u53d7\u76d1\u7ba1\u884c\u4e1a\u63d0\u4f9b\u8bbe\u8ba1\u539f\u5219\u3002"}}
{"id": "2511.12101", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12101", "abs": "https://arxiv.org/abs/2511.12101", "authors": ["Jian Zhou", "Sihao Lin", "Shuai Fu", "Qi WU"], "title": "Decoupled Action Head: Confining Task Knowledge to Conditioning Layers", "comment": null, "summary": "Behavior Cloning (BC) is a data-driven supervised learning approach that has gained increasing attention with the success of scaling laws in language and vision domains. Among its implementations in robotic manipulation, Diffusion Policy (DP), with its two variants DP-CNN (DP-C) and DP-Transformer (DP-T), is one of the most effective and widely adopted models, demonstrating the advantages of predicting continuous action sequences. However, both DP and other BC methods remain constrained by the scarcity of paired training data, and the internal mechanisms underlying DP's effectiveness remain insufficiently understood, leading to limited generalization and a lack of principled design in model development. In this work, we propose a decoupled training recipe that leverages nearly cost-free kinematics-generated trajectories as observation-free data to pretrain a general action head (action generator). The pretrained action head is then frozen and adapted to novel tasks through feature modulation. Our experiments demonstrate the feasibility of this approach in both in-distribution and out-of-distribution scenarios. As an additional benefit, decoupling improves training efficiency; for instance, DP-C achieves up to a 41% speedup. Furthermore, the confinement of task-specific knowledge to the conditioning components under decoupling, combined with the near-identical performance of DP-C in both normal and decoupled training, indicates that the action generation backbone plays a limited role in robotic manipulation. Motivated by this observation, we introduce DP-MLP, which replaces the 244M-parameter U-Net backbone of DP-C with only 4M parameters of simple MLP blocks, achieving a 83.9% faster training speed under normal training and 89.1% under decoupling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u8026\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u884c\u4e3a\u514b\u9686\u6a21\u578b\u7684\u6548\u7387\u548c\u901f\u5ea6\uff0c\u4e14\u901a\u8fc7\u5f15\u5165\u66f4\u7b80\u5355\u7684\u6a21\u578b\u7ed3\u6784\uff0c\u964d\u4f4e\u4e86\u53c2\u6570\u6570\u91cf\u3002", "motivation": "\u53d7\u5230\u8bed\u8a00\u548c\u89c6\u89c9\u9886\u57df\u6269\u5c55\u6cd5\u5219\u6210\u529f\u7684\u542f\u53d1\uff0c\u65e8\u5728\u89e3\u51b3\u884c\u4e3a\u514b\u9686\uff08BC\uff09\u65b9\u6cd5\u4e2d\u6570\u636e\u7a00\u7f3a\u6027\u548c\u6a21\u578b\u900f\u660e\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5229\u7528\u51e0\u4e4e\u65e0\u6210\u672c\u7684\u8fd0\u52a8\u5b66\u751f\u6210\u8f68\u8ff9\u4f5c\u4e3a\u65e0\u89c2\u5bdf\u6570\u636e\u6765\u9884\u8bad\u7ec3\u901a\u7528\u52a8\u4f5c\u5934\uff08\u52a8\u4f5c\u751f\u6210\u5668\uff09\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u573a\u666f\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e14DP-C\u8bad\u7ec3\u901f\u5ea6\u63d0\u9ad8\u4e8641%\u3002\u5f15\u5165DP-MLP\u66ff\u4ee3DP-C\u7684U-Net\u540e\uff0c\u8bad\u7ec3\u901f\u5ea6\u5206\u522b\u63d0\u5347\u4e8683.9%\u548c89.1%\u3002", "conclusion": "\u89e3\u8026\u8bad\u7ec3\u65b9\u6cd5\u66f4\u6709\u6548\uff0c\u7b80\u5316\u7684DP-MLP\u7ed3\u6784\u5728\u8bad\u7ec3\u901f\u5ea6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8868\u660e\u52a8\u4f5c\u751f\u6210\u7684\u57fa\u7840\u7ed3\u6784\u5bf9\u673a\u5668\u4eba\u64cd\u63a7\u7684\u4f5c\u7528\u6709\u9650\u3002"}}
{"id": "2511.12796", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12796", "abs": "https://arxiv.org/abs/2511.12796", "authors": ["Andreas Chouliaras", "Dimitris Chatzopoulos"], "title": "Maximizing the efficiency of human feedback in AI alignment: a comparative analysis", "comment": "16 pages, 6 figures, 6 algorithms. AICS2025", "summary": "Reinforcement Learning from Human Feedback (RLHF) relies on preference modeling to align machine learning systems with human values, yet the popular approach of random pair sampling with Bradley-Terry modeling is statistically limited and inefficient under constrained annotation budgets. In this work, we explore alternative sampling and evaluation strategies for preference inference in RLHF, drawing inspiration from areas such as game theory, statistics, and social choice theory. Our best-performing method, Swiss InfoGain, employs a Swiss tournament system with a proxy mutual-information-gain pairing rule, which significantly outperforms all other methods in constrained annotation budgets while also being more sample-efficient. Even in high-resource settings, we can identify superior alternatives to the Bradley-Terry baseline. Our experiments demonstrate that adaptive, resource-aware strategies reduce redundancy, enhance robustness, and yield statistically significant improvements in preference learning, highlighting the importance of balancing alignment quality with human workload in RLHF pipelines.", "AI": {"tldr": "\u63d0\u51faSwiss InfoGain\u65b9\u6cd5\uff0c\u901a\u8fc7\u745e\u58eb\u9526\u6807\u8d5b\u7cfb\u7edf\u4f18\u5316\u504f\u597d\u63a8\u65ad\uff0c\u964d\u4f4e\u5197\u4f59\uff0c\u63d0\u9ad8\u7a33\u5065\u6027\uff0c\u663e\u8457\u6539\u5584\u6807\u6ce8\u6548\u7387\u3002", "motivation": "\u63d0\u9ad8RLHF\u4e2d\u504f\u597d\u5efa\u6a21\u7684\u6548\u7387\u548c\u6709\u6548\u6027\uff0c\u5c24\u5176\u662f\u5728\u6709\u9650\u7684\u6807\u6ce8\u9884\u7b97\u4e0b", "method": "\u63a2\u7d22\u66ff\u4ee3\u7684\u91c7\u6837\u548c\u8bc4\u4f30\u7b56\u7565\u4ee5\u8fdb\u884c\u504f\u597d\u63a8\u65ad", "result": "Swiss InfoGain\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5e76\u5728\u9ad8\u8d44\u6e90\u8bbe\u7f6e\u4e2d\u8bc6\u522b\u51fa\u6bd4Bradley-Terry\u57fa\u7ebf\u66f4\u597d\u7684\u66ff\u4ee3\u65b9\u6848", "conclusion": "\u5728RLHF\u7ba1\u9053\u4e2d\u5e73\u8861\u5bf9\u9f50\u8d28\u91cf\u548c\u4eba\u7c7b\u5de5\u4f5c\u8d1f\u8f7d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.12148", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12148", "abs": "https://arxiv.org/abs/2511.12148", "authors": ["Advik Sinha", "Akshay Arjun", "Abhijit Das", "Joyjit Mukherjee"], "title": "Towards Obstacle-Avoiding Control of Planar Snake Robots Exploring Neuro-Evolution of Augmenting Topologies", "comment": "9 pages, 6 figures", "summary": "This work aims to develop a resource-efficient solution for obstacle-avoiding tracking control of a planar snake robot in a densely cluttered environment with obstacles. Particularly, Neuro-Evolution of Augmenting Topologies (NEAT) has been employed to generate dynamic gait parameters for the serpenoid gait function, which is implemented on the joint angles of the snake robot, thus controlling the robot on a desired dynamic path. NEAT is a single neural-network based evolutionary algorithm that is known to work extremely well when the input layer is of significantly higher dimension and the output layer is of a smaller size. For the planar snake robot, the input layer consists of the joint angles, link positions, head link position as well as obstacle positions in the vicinity. However, the output layer consists of only the frequency and offset angle of the serpenoid gait that control the speed and heading of the robot, respectively. Obstacle data from a LiDAR and the robot data from various sensors, along with the location of the end goal and time, are employed to parametrize a reward function that is maximized over iterations by selective propagation of superior neural networks. The implementation and experimental results showcase that the proposed approach is computationally efficient, especially for large environments with many obstacles. The proposed framework has been verified through a physics engine simulation study on PyBullet. The approach shows superior results to existing state-of-the-art methodologies and comparable results to the very recent CBRL approach with significantly lower computational overhead. The video of the simulation can be found here: https://sites.google.com/view/neatsnakerobot", "AI": {"tldr": "\u672c\u7814\u7a76\u65e8\u5728\u4e3a\u5e73\u9762\u86c7\u5f62\u673a\u5668\u4eba\u5728\u969c\u788d\u7269\u5bc6\u96c6\u73af\u5883\u4e2d\u7684\u907f\u969c\u8ddf\u8e2a\u63a7\u5236\u5f00\u53d1\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f7f\u7528NEAT\u7b97\u6cd5\u751f\u6210\u52a8\u6001\u6b65\u6001\u53c2\u6570\uff0c\u5e76\u5728\u6a21\u62df\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5e94\u7528\u9700\u6c42\u589e\u52a0\uff0c\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u7684\u52a8\u6001\u63a7\u5236\u65b9\u6cd5\u663e\u5f97\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u4f7f\u7528NEAT\u7b97\u6cd5\u4e3a\u86c7\u5f62\u673a\u5668\u4eba\u7684\u5173\u8282\u89d2\u5ea6\u751f\u6210\u52a8\u6001\u6b65\u6001\u53c2\u6570\uff0c\u901a\u8fc7\u6700\u5927\u5316\u5956\u52b1\u51fd\u6570\u6765\u4f18\u5316\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u5728PyBullet\u6a21\u62df\u4e2d\u53d6\u5f97\u4e86\u826f\u597d\u6548\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u8ba1\u7b97\u6548\u7387\u548c\u9002\u5e94\u6027\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2511.12841", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.12841", "abs": "https://arxiv.org/abs/2511.12841", "authors": ["Shuning Zhang", "Yijing Liu", "Yuyu Liu", "Ying Ma", "Shixuan Li", "Xin Yi", "Qian Wu", "Hewu Li"], "title": "SoK: Synthesizing Smart Home Privacy Protection Mechanisms Across Academic Proposals and Commercial Documentations", "comment": null, "summary": "Pervasive data collection by Smart Home Devices (SHDs) demands robust Privacy Protection Mechanisms (PPMs). The effectiveness of many PPMs, particularly user-facing controls, depends on user awareness and adoption, which are shaped by manufacturers' public documentations. However, the landscape of academic proposals and commercial disclosures remains underexplored. To address this gap, we investigate: (1) What PPMs have academics proposed, and how are these PPMs evaluated? (2) What PPMs do manufacturers document and what factors affect these documentation? To address these questions, we conduct a two-phase study, synthesizing a systematic review of 117 academic papers with an empirical analysis of 86 SHDs' publicly disclosed documentations. Our review of academic literature reveals a strong focus on novel system- and algorithm-based PPMs. However, these proposals neglect deployment barriers (e.g., cost, interoperability), and lack real-world field validation and legal analysis. Concurrently, our analysis of commercial SHDs finds that advanced academic proposals are absent from public discourse. Industry postures are fundamentally reactive, prioritizing compliance via post-hoc data management (e.g., deletion options), rather than the preventative controls favored by academia. The documented protections correspondingly converge on a small set of practical mechanisms, such as physical buttons and localized processing. By synthesizing these findings, we advocate for research to analyze challenges, provide deployable frameworks, real-world field validation, and interoperability solutions to advance practical PPMs.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u667a\u80fd\u5bb6\u5c45\u8bbe\u5907\u7684\u9690\u79c1\u4fdd\u62a4\u673a\u5236\uff0c\u53d1\u73b0\u5b66\u672f\u754c\u5173\u6ce8\u65b0\u9896\u673a\u5236\u7684\u7406\u8bba\u4e0e\u7b97\u6cd5\uff0c\u4f46\u7f3a\u4e4f\u5b9e\u9645\u5e94\u7528\uff1b\u800c\u5546\u4e1a\u754c\u5219\u4ee5\u5408\u89c4\u4e3a\u91cd\uff0c\u63d0\u4f9b\u6709\u9650\u7684\u5b9e\u8df5\u63aa\u65bd\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u5bb6\u5c45\u8bbe\u5907\u6570\u636e\u6536\u96c6\u7684\u666e\u904d\u5316\uff0c\u4e9f\u9700\u5efa\u7acb\u6709\u6548\u7684\u9690\u79c1\u4fdd\u62a4\u673a\u5236\uff0c\u4f46\u5b66\u672f\u4e0e\u5546\u4e1a\u754c\u7684\u7814\u7a76\u548c\u6587\u6863\u5b58\u5728\u660e\u663e\u5dee\u5f02\uff0c\u9700\u6df1\u5165\u63a2\u8ba8\u3002", "method": "\u7814\u7a76\u5305\u62ec\u4e24\u9636\u6bb5\uff1a\u7cfb\u7edf\u7efc\u8ff0117\u7bc7\u5b66\u672f\u6587\u732e\u548c\u5bf986\u6b3eSHDs\u7684\u516c\u5f00\u6587\u6863\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\u3002", "result": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u667a\u80fd\u5bb6\u5c45\u8bbe\u5907\uff08SHDs\uff09\u7684\u9690\u79c1\u4fdd\u62a4\u673a\u5236\uff08PPMs\uff09\uff0c\u5206\u6790\u4e86\u5b66\u672f\u754c\u4e0e\u5546\u4e1a\u6587\u732e\u5bf9\u8fd9\u4e9b\u673a\u5236\u7684\u5173\u6ce8\u4e0e\u5dee\u5f02\u3002", "conclusion": "\u547c\u5401\u5f00\u5c55\u66f4\u591a\u9488\u5bf9\u5b9e\u9645\u6311\u6218\u7684\u7814\u7a76\uff0c\u4ee5\u521b\u9020\u53ef\u90e8\u7f72\u7684\u9690\u79c1\u4fdd\u62a4\u6846\u67b6\uff0c\u5e76\u9a8c\u8bc1\u5176\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.12160", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12160", "abs": "https://arxiv.org/abs/2511.12160", "authors": ["Wenbin Mai", "Minghui Liwang", "Xinlei Yi", "Xiaoyu Xia", "Seyyedali Hosseinalipour", "Xianbin Wang"], "title": "Game-Theoretic Safe Multi-Agent Motion Planning with Reachability Analysis for Dynamic and Uncertain Environments (Extended Version)", "comment": "12 pages, 9 figures", "summary": "Ensuring safe, robust, and scalable motion planning for multi-agent systems in dynamic and uncertain environments is a persistent challenge, driven by complex inter-agent interactions, stochastic disturbances, and model uncertainties. To overcome these challenges, particularly the computational complexity of coupled decision-making and the need for proactive safety guarantees, we propose a Reachability-Enhanced Dynamic Potential Game (RE-DPG) framework, which integrates game-theoretic coordination into reachability analysis. This approach formulates multi-agent coordination as a dynamic potential game, where the Nash equilibrium (NE) defines optimal control strategies across agents. To enable scalability and decentralized execution, we develop a Neighborhood-Dominated iterative Best Response (ND-iBR) scheme, built upon an iterated $\\varepsilon$-BR (i$\\varepsilon$-BR) process that guarantees finite-step convergence to an $\\varepsilon$-NE. This allows agents to compute strategies based on local interactions while ensuring theoretical convergence guarantees. Furthermore, to ensure safety under uncertainty, we integrate a Multi-Agent Forward Reachable Set (MA-FRS) mechanism into the cost function, explicitly modeling uncertainty propagation and enforcing collision avoidance constraints. Through both simulations and real-world experiments in 2D and 3D environments, we validate the effectiveness of RE-DPG across diverse operational scenarios.", "AI": {"tldr": "\u63d0\u51faRE-DPG\u6846\u67b6\uff0c\u7ed3\u5408\u6e38\u620f\u7406\u8bba\u548c\u53ef\u8fbe\u6027\u5206\u6790\uff0c\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u52a8\u6001\u548c\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u8fd0\u52a8\u89c4\u5212\u7684\u5b89\u5168\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898", "method": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u6e38\u620f\u7406\u8bba\u534f\u8c03\u4e0e\u53ef\u8fbe\u6027\u5206\u6790\u7684RE-DPG\u6846\u67b6", "result": "\u901a\u8fc7\u6a21\u62df\u548c\u5b9e\u9645\u5b9e\u9a8c\u9a8c\u8bc1\u4e86RE-DPG\u5728\u4e0d\u540c\u64cd\u4f5c\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027", "conclusion": "RE-DPG\u6846\u67b6\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u524d\u5411\u53ef\u8fbe\u96c6\u673a\u5236\u589e\u5f3a\u4e86\u5b89\u5168\u6027\uff0c\u540c\u65f6\u786e\u4fdd\u4e86\u8ba1\u7b97\u7684\u53ef\u6269\u5c55\u6027\u548c\u6536\u655b\u6027\u3002"}}
{"id": "2511.12952", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.12952", "abs": "https://arxiv.org/abs/2511.12952", "authors": ["Yibo Meng", "Zhiming Liu", "Xiaochen Qin"], "title": "Design and Evaluation of an AI-DrivenPersonalized Mobile App to Provide MultifacetedHealth Support for Type 2 Diabetes Patients inChina", "comment": null, "summary": "Type 2 diabetes patients in China face many significant challenges in patient-provider communication and self management In light of this, this work designed,implemented,and evaluated an AI-driven, personalized, multi-functional mobile app system named T2MD Health. The appintegrates real-time patient- provider conversation transcription,medical terminology interpretation, daily health tracking, and adata-driven feedback loop. We conducted qualitative interviewswith 40 participants to study key user needs before systemdevelopment and a mixed- method controlled experiment with 60participants after to evaluate the effectiveness and usability ofthe app. Evaluation results showed that the app was effective inimproving patient-provider communication efficiency, patientunderstanding and knowledge retention,and patient selfmanagement, Patient feedback also revealed that the app has thepotential to address the urban-rural gap in the access to medica!consultation services to some extent, Findings ofthis study couldinform future studies that seek to utilize mobile apps andartificial intelligence to support patients with chronic diseases.", "AI": {"tldr": "\u8bbe\u8ba1\u5e76\u8bc4\u4f30\u4e86\u4e00\u6b3e\u540d\u4e3aT2MD Health\u7684AI\u9a71\u52a8\u79fb\u52a8\u5e94\u7528\uff0c\u65e8\u5728\u6539\u5584\u4e2d\u56fd2\u578b\u7cd6\u5c3f\u75c5\u60a3\u8005\u7684\u6c9f\u901a\u53ca\u81ea\u6211\u7ba1\u7406\u3002", "motivation": "\u5e94\u5bf9\u4e2d\u56fd2\u578b\u7cd6\u5c3f\u75c5\u60a3\u8005\u5728\u60a3\u8005-\u533b\u751f\u6c9f\u901a\u548c\u81ea\u6211\u7ba1\u7406\u4e2d\u9762\u4e34\u7684\u91cd\u5927\u6311\u6218\u3002", "method": "\u8fdb\u884c\u5b9a\u6027\u8bbf\u8c08\u548c\u6df7\u5408\u65b9\u6cd5\u5bf9\u7167\u5b9e\u9a8c\uff0c\u5206\u522b\u91c7\u8bbf40\u540d\u53c2\u4e0e\u8005\u4ee5\u4e86\u89e3\u9700\u6c42\uff0c\u5e76\u5bf960\u540d\u53c2\u4e0e\u8005\u8bc4\u4f30\u5e94\u7528\u6548\u679c\u3002", "result": "\u5e94\u7528\u663e\u8457\u6539\u5584\u4e86\u60a3\u8005\u548c\u533b\u751f\u4e4b\u95f4\u7684\u6c9f\u901a\u6548\u7387\u548c\u60a3\u8005\u81ea\u6211\u7ba1\u7406\u80fd\u529b\uff0c\u4e14\u5728\u57ce\u4e61\u533b\u7597\u670d\u52a1\u63a5\u5165\u65b9\u9762\u5c55\u73b0\u51fa\u4e00\u5b9a\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u5e94\u7528\u6709\u6548\u63d0\u5347\u4e86\u60a3\u8005\u4e0e\u533b\u52a1\u4eba\u5458\u7684\u6c9f\u901a\u6548\u7387\u3001\u81ea\u6211\u7ba1\u7406\u80fd\u529b\u548c\u77e5\u8bc6\u4fdd\u7559\uff0c\u663e\u793a\u51fa\u7f29\u5c0f\u57ce\u4e61\u533b\u7597\u54a8\u8be2\u670d\u52a1\u5dee\u8ddd\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.12184", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12184", "abs": "https://arxiv.org/abs/2511.12184", "authors": ["Jun Huo", "Kehan Xu", "Chengyao Li", "Yu Cao", "Jie Zuo", "Xinxing Chen", "Jian Huang"], "title": "Variable Impedance Control for Floating-Base Supernumerary Robotic Leg in Walking Assistance", "comment": null, "summary": "In human-robot systems, ensuring safety during force control in the presence of both internal and external disturbances is crucial. As a typical loosely coupled floating-base robot system, the supernumerary robotic leg (SRL) system is particularly susceptible to strong internal disturbances. To address the challenge posed by floating base, we investigated the dynamics model of the loosely coupled SRL and designed a hybrid position/force impedance controller to fit dynamic torque input. An efficient variable impedance control (VIC) method is developed to enhance human-robot interaction, particularly in scenarios involving external force disturbances. By dynamically adjusting impedance parameters, VIC improves the dynamic switching between rigidity and flexibility, so that it can adapt to unknown environmental disturbances in different states. An efficient real-time stability guaranteed impedance parameters generating network is specifically designed for the proposed SRL, to achieve shock mitigation and high rigidity supporting. Simulations and experiments validate the system's effectiveness, demonstrating its ability to maintain smooth signal transitions in flexible states while providing strong support forces in rigid states. This approach provides a practical solution for accommodating individual gait variations in interaction, and significantly advances the safety and adaptability of human-robot systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u9488\u5bf9\u8d85\u4f59\u673a\u5668\u4eba\u817f(SRL)\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u6027\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u963b\u6297\u63a7\u5236\u65b9\u6848\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u963b\u6297\u53c2\u6570\uff0c\u63d0\u5347\u4e86\u5728\u5916\u90e8\u6270\u52a8\u60c5\u51b5\u4e0b\u7684\u9002\u5e94\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u5728\u4eba\u673a\u7cfb\u7edf\u4e2d\uff0c\u786e\u4fdd\u5728\u5185\u90e8\u548c\u5916\u90e8\u6270\u52a8\u4e0b\u7684\u5b89\u5168\u662f\u81f3\u5173\u91cd\u8981\u7684\uff0c\u5c24\u5176\u5bf9\u4e8e\u677e\u6563\u8026\u5408\u7684\u6d6e\u52a8\u57fa\u673a\u5668\u4eba\u7cfb\u7edf\u5982\u8d85\u4f59\u673a\u5668\u4eba\u817f(SRL)\u7cfb\u7edf\uff0c\u5bb9\u6613\u53d7\u5230\u5f3a\u70c8\u7684\u5185\u90e8\u6270\u52a8\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u4e86SRL\u7684\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6df7\u5408\u4f4d\u7f6e/\u529b\u963b\u6297\u63a7\u5236\u5668\uff0c\u5f00\u53d1\u4e86\u9ad8\u6548\u7684\u53d8\u963b\u6297\u63a7\u5236\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u4e0d\u540c\u72b6\u6001\u4e0b\u7684\u672a\u77e5\u73af\u5883\u6270\u52a8\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u4f4d\u7f6e/\u529b\u963b\u6297\u63a7\u5236\u5668\u548c\u9ad8\u6548\u7684\u53d8\u963b\u6297\u63a7\u5236(VIC)\u65b9\u6cd5\uff0c\u4ee5\u589e\u5f3a\u4eba\u673a\u4ea4\u4e92\u5e76\u9002\u5e94\u4e0d\u540c\u7684\u73af\u5883\u6270\u52a8\u3002", "conclusion": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u7cfb\u7edf\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u5176\u5728\u7075\u6d3b\u72b6\u6001\u4e0b\u4fdd\u6301\u5e73\u6ed1\u4fe1\u53f7\u8fc7\u6e21\u7684\u80fd\u529b\uff0c\u5e76\u5728\u521a\u6027\u72b6\u6001\u4e0b\u63d0\u4f9b\u5f3a\u6709\u529b\u7684\u652f\u6301\u3002\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u4eba\u673a\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2511.13046", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.13046", "abs": "https://arxiv.org/abs/2511.13046", "authors": ["Ichiro Matsuda", "Komichi Takezawa", "Katsuhito Muroi", "Kensuke Katori", "Ryosuke Hyakuta", "Jingjing Li", "Yoichi Ochiai"], "title": "Knowing Ourselves Through Others: Reflecting with AI in Digital Human Debates", "comment": null, "summary": "LLMs can act as an impartial other, drawing on vast knowledge, or as personalized self-reflecting user prompts. These personalized LLMs, or Digital Humans, occupy an intermediate position between self and other. This research explores the dynamic of self and other mediated by these Digital Humans. Using a Research Through Design approach, nine junior and senior high school students, working in teams, designed Digital Humans and had them debate. Each team built a unique Digital Human using prompt engineering and RAG, then observed their autonomous debates. Findings from generative AI literacy tests, interviews, and log analysis revealed that participants deepened their understanding of AI's capabilities. Furthermore, experiencing their own creations as others prompted a reflective attitude, enabling them to objectively view their own cognition and values. We propose \"Reflecting with AI\" - using AI to re-examine the self - as a new generative AI literacy, complementing the conventional understanding, applying, criticism and ethics.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4e2a\u6027\u5316\u6570\u5b57\u4eba\u7c7b\u5728\u81ea\u6211\u4e0e\u4ed6\u4eba\u4e4b\u95f4\u7684\u52a8\u6001\u5173\u7cfb\uff0c\u53d1\u73b0\u53c2\u4e0e\u8005\u5728\u8bbe\u8ba1\u548c\u4f7f\u7528\u6570\u5b57\u4eba\u7c7b\u65f6\u52a0\u6df1\u4e86\u5bf9AI\u7684\u7406\u89e3\uff0c\u5e76\u57f9\u517b\u4e86\u53cd\u601d\u81ea\u6211\u8ba4\u77e5\u7684\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u53d1\u5c55\uff0c\u63a2\u8ba8\u5176\u5728\u81ea\u6211\u4e0e\u4ed6\u4eba\u4e4b\u95f4\u7684\u89d2\u8272\u548c\u5f71\u54cd\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u4e2a\u6027\u5316\u7684\u6570\u5b57\u4eba\u7c7b\uff08Digital Humans\uff09\u5982\u4f55\u5e2e\u52a9\u7528\u6237\u53cd\u601d\u81ea\u6211\u8ba4\u77e5\u548c\u4ef7\u503c\u89c2\u3002", "method": "\u91c7\u7528\u8bbe\u8ba1\u9a71\u52a8\u7814\u7a76\u7684\u65b9\u6cd5\uff0c\u4e5d\u540d\u4e2d\u5b66\u751f\u4ee5\u56e2\u961f\u5f62\u5f0f\u8bbe\u8ba1\u6570\u5b57\u4eba\u7c7b\u5e76\u53c2\u4e0e\u8fa9\u8bba\uff0c\u901a\u8fc7\u751f\u6210\u6027\u4eba\u5de5\u667a\u80fd\u7d20\u517b\u6d4b\u8bd5\u3001\u8bbf\u8c08\u548c\u65e5\u5fd7\u5206\u6790\u8fdb\u884c\u6570\u636e\u6536\u96c6\u4e0e\u5206\u6790\u3002", "result": "\u7814\u7a76\u663e\u793a\uff0c\u53c2\u4e0e\u8005\u901a\u8fc7\u8bbe\u8ba1\u548c\u89c2\u5bdf\u81ea\u5df1\u7684\u6570\u5b57\u4eba\u7c7b\u8fdb\u884c\u8fa9\u8bba\uff0c\u589e\u5f3a\u4e86\u5bf9\u4eba\u5de5\u667a\u80fd\u80fd\u529b\u7684\u7406\u89e3\uff0c\u540c\u65f6\u57f9\u517b\u4e86\u53cd\u601d\u6001\u5ea6\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u5c06\u2018\u4e0eAI\u53cd\u601d\u2019\u4f5c\u4e3a\u4e00\u79cd\u65b0\u7684\u751f\u6210\u6027\u4eba\u5de5\u667a\u80fd\u7d20\u517b\uff0c\u8865\u5145\u4f20\u7edf\u7684\u7406\u89e3\u3001\u5e94\u7528\u3001\u6279\u8bc4\u4e0e\u4f26\u7406\u89c2\u5ff5\u3002"}}
{"id": "2511.12186", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12186", "abs": "https://arxiv.org/abs/2511.12186", "authors": ["Jun Huo", "Jian Huang", "Jie Zuo", "Bo Yang", "Zhongzheng Fu", "Xi Li", "Samer Mohammed"], "title": "Innovative Design of Multi-functional Supernumerary Robotic Limbs with Ellipsoid Workspace Optimization", "comment": null, "summary": "Supernumerary robotic limbs (SRLs) offer substantial potential in both the rehabilitation of hemiplegic patients and the enhancement of functional capabilities for healthy individuals. Designing a general-purpose SRL device is inherently challenging, particularly when developing a unified theoretical framework that meets the diverse functional requirements of both upper and lower limbs. In this paper, we propose a multi-objective optimization (MOO) design theory that integrates grasping workspace similarity, walking workspace similarity, braced force for sit-to-stand (STS) movements, and overall mass and inertia. A geometric vector quantification method is developed using an ellipsoid to represent the workspace, aiming to reduce computational complexity and address quantification challenges. The ellipsoid envelope transforms workspace points into ellipsoid attributes, providing a parametric description of the workspace. Furthermore, the STS static braced force assesses the effectiveness of force transmission. The overall mass and inertia restricts excessive link length. To facilitate rapid and stable convergence of the model to high-dimensional irregular Pareto fronts, we introduce a multi-subpopulation correction firefly algorithm. This algorithm incorporates a strategy involving attractive and repulsive domains to effectively handle the MOO task. The optimized solution is utilized to redesign the prototype for experimentation to meet specified requirements. Six healthy participants and two hemiplegia patients participated in real experiments. Compared to the pre-optimization results, the average grasp success rate improved by 7.2%, while the muscle activity during walking and STS tasks decreased by an average of 12.7% and 25.1%, respectively. The proposed design theory offers an efficient option for the design of multi-functional SRL mechanisms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u76ee\u6807\u4f18\u5316\u8bbe\u8ba1\u7406\u8bba\uff0c\u65e8\u5728\u89e3\u51b3\u8d85\u6570\u673a\u5668\u4eba\u80a2\u4f53\uff08SRL\uff09\u8bbe\u5907\u7684\u590d\u6742\u529f\u80fd\u8981\u6c42\uff0c\u901a\u8fc7\u51e0\u4f55\u5411\u91cf\u91cf\u5316\u65b9\u6cd5\u548c\u6539\u8fdb\u7684\u8424\u706b\u866b\u7b97\u6cd5\u4f18\u5316SRL\u7684\u8bbe\u8ba1\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4f18\u5316\u540e\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u8bbe\u8ba1\u901a\u7528\u7684\u8d85\u6570\u673a\u5668\u4eba\u80a2\u4f53\u8bbe\u5907\u9762\u4e34\u591a\u6837\u5316\u529f\u80fd\u8981\u6c42\u7684\u6311\u6218\uff0c\u9700\u5728\u8bbe\u8ba1\u8fc7\u7a0b\u4e2d\u8003\u8651\u6293\u53d6\u548c\u884c\u8d70\u7684\u5de5\u4f5c\u7a7a\u95f4\u7b49\u591a\u4e2a\u56e0\u7d20\u3002", "method": "\u4f7f\u7528\u591a\u76ee\u6807\u4f18\u5316\u8bbe\u8ba1\u7406\u8bba\uff0c\u7ed3\u5408\u51e0\u4f55\u5411\u91cf\u91cf\u5316\u548c\u591a\u5b50\u79cd\u7fa4\u4fee\u6b63\u8424\u706b\u866b\u7b97\u6cd5\uff0c\u4ee5\u4f18\u5316SRL\u7684\u529f\u80fd\u548c\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u4f18\u5316\u540e\u6293\u53d6\u6210\u529f\u7387\u63d0\u9ad87.2%\uff0c\u884c\u8d70\u548c\u5750\u7acb\u8f6c\u53d8\u4efb\u52a1\u4e2d\u7684\u808c\u8089\u6d3b\u52a8\u5206\u522b\u51cf\u5c1112.7%\u548c25.1%\u3002", "conclusion": "\u63d0\u51fa\u7684\u8bbe\u8ba1\u7406\u8bba\u4e3a\u8bbe\u8ba1\u591a\u529f\u80fd\u8d85\u6570\u673a\u5668\u4eba\u80a2\u4f53\u673a\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u4f18\u5316\u540e\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6027\u80fd\u6709\u663e\u8457\u6539\u5584\u3002"}}
{"id": "2511.13112", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.13112", "abs": "https://arxiv.org/abs/2511.13112", "authors": ["Wenya Wei", "Sipeng Yang", "Qixian Zhou", "Ruochen Liu", "Xuelei Zhang", "Yifu Yuan", "Yan Jiang", "Yongle Luo", "Hailong Wang", "Tianzhou Wang", "Peipei Jin", "Wangtong Liu", "Zhou Zhao", "Xiaogang Jin", "Elvis S. Liu"], "title": "F.A.C.U.L.: Language-Based Interaction with AI Companions in Gaming", "comment": "14 pages, 11 figures,", "summary": "In cooperative video games, traditional AI companions are deployed to assist players, who control them using hotkeys or command wheels to issue predefined commands such as ``attack'', ``defend'', or ``retreat''. Despite their simplicity, these methods, which lack target specificity, limit players' ability to give complex tactical instructions and hinder immersive gameplay experiences. To address this problem, we propose the FPS AI Companion who Understands Language (F.A.C.U.L.), the first real-time AI system that enables players to communicate and collaborate with AI companions using natural language. By integrating natural language processing with a confidence-based framework, F.A.C.U.L. efficiently decomposes complex commands and interprets player intent. It also employs a dynamic entity retrieval method for environmental awareness, aligning human intentions with decision-making. Unlike traditional rule-based systems, our method supports real-time language interactions, enabling players to issue complex commands such as ``clear the second floor'', ``take cover behind that tree'', or ``retreat to the river''. The system provides real-time behavioral responses and vocal feedback, ensuring seamless tactical collaboration. Using the popular FPS game \\textit{Arena Breakout: Infinite} as a case study, we present comparisons demonstrating the efficacy of our approach and discuss the advantages and limitations of AI companions based on real-world user feedback.", "AI": {"tldr": "F.A.C.U.L.\u662f\u4e00\u4e2a\u521b\u65b0\u7684AI\u7cfb\u7edf\uff0c\u5141\u8bb8\u73a9\u5bb6\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u6325AI\u4f19\u4f34\uff0c\u4ece\u800c\u6539\u5584\u5408\u4f5c\u89c6\u9891\u6e38\u620f\u7684\u6e38\u620f\u4f53\u9a8c\u3002", "motivation": "\u4f20\u7edf\u7684AI\u4f19\u4f34\u4ea4\u4e92\u65b9\u5f0f\u9650\u5236\u4e86\u73a9\u5bb6\u53d1\u51fa\u590d\u6742\u6218\u672f\u6307\u4ee4\u7684\u80fd\u529b\uff0c\u5f71\u54cd\u4e86\u6e38\u620f\u4f53\u9a8c\u3002", "method": "\u91c7\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u57fa\u4e8e\u4fe1\u5fc3\u7684\u6846\u67b6\uff0cF.A.C.U.L.\u89e3\u6790\u590d\u6742\u547d\u4ee4\uff0c\u5e76\u5b9e\u73b0\u52a8\u6001\u5b9e\u4f53\u68c0\u7d22\u4ee5\u589e\u5f3a\u73af\u5883\u610f\u8bc6\u3002", "result": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aF.A.C.U.L.\u7684\u5b9e\u65f6AI\u7cfb\u7edf\uff0c\u5141\u8bb8\u73a9\u5bb6\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u4e0eAI\u4f19\u4f34\u8fdb\u884c\u6c9f\u901a\u548c\u534f\u4f5c\u3002", "conclusion": "F.A.C.U.L.\u80fd\u591f\u6709\u6548\u5730\u63d0\u5347\u73a9\u5bb6\u4e0eAI\u4f19\u4f34\u95f4\u7684\u4e92\u52a8\u8d28\u91cf\uff0c\u589e\u5f3a\u6e38\u620f\u7684\u6c89\u6d78\u611f\u4e0e\u6218\u672f\u534f\u4f5c\u80fd\u529b\u3002"}}
{"id": "2511.12203", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12203", "abs": "https://arxiv.org/abs/2511.12203", "authors": ["Antony Thomas", "Fulvio Mastrogiovanni", "Marco Baglietto"], "title": "Locally Optimal Solutions to Constraint Displacement Problems via Path-Obstacle Overlaps", "comment": "Robotics and Autonomous Systems", "summary": "We present a unified approach for constraint displacement problems in which a robot finds a feasible path by displacing constraints or obstacles. To this end, we propose a two stage process that returns locally optimal obstacle displacements to enable a feasible path for the robot. The first stage proceeds by computing a trajectory through the obstacles while minimizing an appropriate objective function. In the second stage, these obstacles are displaced to make the computed robot trajectory feasible, that is, collision-free. Several examples are provided that successfully demonstrate our approach on two distinct classes of constraint displacement problems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7edf\u4e00\u7684\u65b9\u6cd5\u6765\u5904\u7406\u673a\u5668\u4eba\u7ea6\u675f\u4f4d\u79fb\u95ee\u9898\uff0c\u5305\u542b\u4e24\u9636\u6bb5\u7684\u9009\u62e9\u4ee5\u5b9e\u73b0\u53ef\u884c\u8def\u5f84\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u7ea6\u675f\u4f4d\u79fb\u95ee\u9898\uff0c\u4fdd\u8bc1\u673a\u5668\u4eba\u80fd\u591f\u627e\u5230\u53ef\u884c\u8def\u5f84\u3002", "method": "\u7b2c\u4e00\u9636\u6bb5\u8ba1\u7b97\u6700\u4f18\u8f68\u8ff9\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u4f4d\u79fb\u969c\u788d\u7269\u4f7f\u8f68\u8ff9\u53ef\u884c\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u7684\u8fc7\u7a0b\uff0c\u8fd4\u56de\u5c40\u90e8\u6700\u4f18\u7684\u969c\u788d\u7269\u4f4d\u79fb\uff0c\u4ee5\u4f7f\u673a\u5668\u4eba\u8def\u5f84\u53ef\u884c\u3002", "conclusion": "\u901a\u8fc7\u591a\u4e2a\u6848\u4f8b\u6210\u529f\u5c55\u793a\u4e86\u6240\u63d0\u65b9\u6cd5\u5728\u4e24\u7c7b\u7ea6\u675f\u4f4d\u79fb\u95ee\u9898\u4e0a\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.13158", "categories": ["cs.HC", "cs.MA", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.13158", "abs": "https://arxiv.org/abs/2511.13158", "authors": ["Samuele Burattini", "Alessandro Ricci", "Simon Mayer", "Danai Vachtsevanou", "Jeremy Lemee", "Andrei Ciortea", "Angelo Croatti"], "title": "Agent-Oriented Visual Programming for the Web of Things", "comment": "Accepted and presented at the 10th International Workshop on Engineering Multi-Agent Systems (EMAS 2022), 9-10 May 2022, Auckland, New Zealand", "summary": "In this paper we introduce and discuss an approach for multi-agent-oriented visual programming. This aims at enabling individuals without programming experience but with knowledge in specific target domains to design and (re)configure autonomous software. We argue that, compared to procedural programming, it should be simpler for users to create programs when agent abstractions are employed. The underlying rationale is that these abstractions, and specifically the belief-desire-intention architecture that is aligned with human practical reasoning, match more closely with people's everyday experience in interacting with other agents and artifacts in the real world. On top of this, we designed and implemented a visual programming system for agents that hides the technicalities of agent-oriented programming using a blocks-based visual development environment that is built on the JaCaMo platform. To further validate the proposed solution, we integrate the Web of Things (WoT) to let users create autonomous behaviour on top of physical mashups of devices, following the trends in industrial end-user programming. Finally, we report on a pilot user study where we verified that novice users are indeed able to make use of this development environment to create multi-agent systems to solve simple automation tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u591a\u667a\u80fd\u4f53\u7684\u89c6\u89c9\u7f16\u7a0b\u65b9\u6cd5\uff0c\u65e8\u5728\u5e2e\u52a9\u6ca1\u6709\u7f16\u7a0b\u7ecf\u9a8c\u7684\u7528\u6237\u8bbe\u8ba1\u81ea\u4e3b\u8f6f\u4ef6\uff0c\u901a\u8fc7\u57fa\u4e8e\u533a\u5757\u7684\u5f00\u53d1\u73af\u5883\u4f7f\u7f16\u7a0b\u53d8\u5f97\u7b80\u5355\u3002", "motivation": "\u4e3a\u4e86\u4f7f\u6ca1\u6709\u7f16\u7a0b\u7ecf\u9a8c\u4f46\u5177\u5907\u7279\u5b9a\u76ee\u6807\u9886\u57df\u77e5\u8bc6\u7684\u4e2a\u4eba\u80fd\u591f\u8bbe\u8ba1\u548c\u91cd\u65b0\u914d\u7f6e\u81ea\u4e3b\u8f6f\u4ef6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u591a\u667a\u80fd\u4f53\u7684\u89c6\u89c9\u7f16\u7a0b\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eJaCaMo\u5e73\u53f0\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u89c6\u89c9\u7f16\u7a0b\u7cfb\u7edf\uff0c\u96c6\u6210Web of Things\u4ee5\u652f\u6301\u7528\u6237\u5728\u7269\u7406\u8bbe\u5907\u7684\u7ec4\u5408\u4e0a\u521b\u5efa\u81ea\u4e3b\u884c\u4e3a\u3002", "result": "\u8be5\u89c6\u89c9\u7f16\u7a0b\u7cfb\u7edf\u80fd\u591f\u8ba9\u521d\u5b66\u8005\u521b\u5efa\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4ee5\u89e3\u51b3\u7b80\u5355\u7684\u81ea\u52a8\u5316\u4efb\u52a1\u3002", "conclusion": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u521d\u5b66\u8005\u53ef\u4ee5\u4f7f\u7528\u8fd9\u79cd\u5f00\u53d1\u73af\u5883\u521b\u5efa\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3002"}}
{"id": "2511.12232", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12232", "abs": "https://arxiv.org/abs/2511.12232", "authors": ["Lingfeng Zhang", "Erjia Xiao", "Xiaoshuai Hao", "Haoxiang Fu", "Zeying Gong", "Long Chen", "Xiaojun Liang", "Renjing Xu", "Hangjun Ye", "Wenbo Ding"], "title": "SocialNav-Map: Dynamic Mapping with Human Trajectory Prediction for Zero-Shot Social Navigation", "comment": null, "summary": "Social navigation in densely populated dynamic environments poses a significant challenge for autonomous mobile robots, requiring advanced strategies for safe interaction. Existing reinforcement learning (RL)-based methods require over 2000+ hours of extensive training and often struggle to generalize to unfamiliar environments without additional fine-tuning, limiting their practical application in real-world scenarios. To address these limitations, we propose SocialNav-Map, a novel zero-shot social navigation framework that combines dynamic human trajectory prediction with occupancy mapping, enabling safe and efficient navigation without the need for environment-specific training. Specifically, SocialNav-Map first transforms the task goal position into the constructed map coordinate system. Subsequently, it creates a dynamic occupancy map that incorporates predicted human movements as dynamic obstacles. The framework employs two complementary methods for human trajectory prediction: history prediction and orientation prediction. By integrating these predicted trajectories into the occupancy map, the robot can proactively avoid potential collisions with humans while efficiently navigating to its destination. Extensive experiments on the Social-HM3D and Social-MP3D datasets demonstrate that SocialNav-Map significantly outperforms state-of-the-art (SOTA) RL-based methods, which require 2,396 GPU hours of training. Notably, it reduces human collision rates by over 10% without necessitating any training in novel environments. By eliminating the need for environment-specific training, SocialNav-Map achieves superior navigation performance, paving the way for the deployment of social navigation systems in real-world environments characterized by diverse human behaviors. The code is available at: https://github.com/linglingxiansen/SocialNav-Map.", "AI": {"tldr": "SocialNav-Map\u662f\u4e00\u4e2a\u65b0\u7684\u793e\u4ea4\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u4eba\u7c7b\u8f68\u8ff9\u9884\u6d4b\u548c\u5360\u7528\u56fe\u6620\u5c04\uff0c\u5b9e\u73b0\u65e0\u9700\u73af\u5883\u7279\u5b9a\u8bad\u7ec3\u7684\u5b89\u5168\u5bfc\u822a\uff0c\u663e\u8457\u51cf\u5c11\u78b0\u649e\u7387\u3002", "motivation": "\u89e3\u51b3\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u5728\u5bc6\u96c6\u52a8\u6001\u73af\u5883\u4e2d\u5bfc\u822a\u96be\u9898\uff0c\u4ee5\u53ca\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7684\u8bad\u7ec3\u65f6\u95f4\u8fc7\u957f\u548c\u6cdb\u5316\u80fd\u529b\u5f31\u7684\u95ee\u9898\u3002", "method": "\u5c06\u4efb\u52a1\u76ee\u6807\u4f4d\u7f6e\u8f6c\u6362\u4e3a\u5730\u56fe\u5750\u6807\u7cfb\u7edf\uff0c\u521b\u5efa\u52a8\u6001\u5360\u7528\u56fe\u5e76\u4f7f\u7528\u5386\u53f2\u9884\u6d4b\u548c\u65b9\u5411\u9884\u6d4b\u7684\u4e24\u79cd\u65b9\u6cd5\u6765\u9884\u6d4b\u4eba\u7c7b\u8f68\u8ff9\uff0c\u4ece\u800c\u907f\u514d\u6f5c\u5728\u7684\u78b0\u649e\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSocialNav-Map\u7684\u96f6-shot\u793e\u4ea4\u5bfc\u822a\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4e0d\u8fdb\u884c\u73af\u5883\u7279\u5b9a\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5b89\u5168\u3001\u9ad8\u6548\u7684\u5bfc\u822a\u3002", "conclusion": "SocialNav-Map\u5728\u591a\u6837\u4eba\u7c7b\u884c\u4e3a\u73af\u5883\u4e2d\u5c55\u793a\u4e86\u51fa\u8272\u7684\u5bfc\u822a\u8868\u73b0\uff0c\u4e3a\u793e\u4ea4\u5bfc\u822a\u7cfb\u7edf\u7684\u5b9e\u9645\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.13458", "categories": ["cs.HC", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13458", "abs": "https://arxiv.org/abs/2511.13458", "authors": ["Agnese Chiatti", "Lara Piccolo", "Sara Bernardini", "Matteo Matteucci", "Viola Schiaffonati"], "title": "Trust in Vision-Language Models: Insights from a Participatory User Workshop", "comment": null, "summary": "With the growing deployment of Vision-Language Models (VLMs), pre-trained on large image-text and video-text datasets, it is critical to equip users with the tools to discern when to trust these systems. However, examining how user trust in VLMs builds and evolves remains an open problem. This problem is exacerbated by the increasing reliance on AI models as judges for experimental validation, to bypass the cost and implications of running participatory design studies directly with users. Following a user-centred approach, this paper presents preliminary results from a workshop with prospective VLM users. Insights from this pilot workshop inform future studies aimed at contextualising trust metrics and strategies for participants' engagement to fit the case of user-VLM interaction.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u7528\u6237\u5bf9\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u4fe1\u4efb\u5efa\u7acb\u8fc7\u7a0b\uff0c\u901a\u8fc7\u7528\u6237\u4e2d\u5fc3\u65b9\u6cd5\u7684\u4e00\u6b21\u5de5\u4f5c\u574a\u83b7\u5f97\u521d\u6b65\u7ed3\u679c\u3002", "motivation": "\u968f\u7740\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u5e7f\u6cdb\u90e8\u7f72\uff0c\u7528\u6237\u5bf9\u8fd9\u4e9b\u7cfb\u7edf\u7684\u4fe1\u4efb\u95ee\u9898\u65e5\u76ca\u91cd\u8981\u3002", "method": "\u91c7\u7528\u7528\u6237\u4e2d\u5fc3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0e\u6f5c\u5728 VLM \u7528\u6237\u7684\u5de5\u4f5c\u574a\u6536\u96c6\u521d\u6b65\u6570\u636e\u3002", "result": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u7528\u6237\u5728\u4f7f\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u8fc7\u7a0b\u4e2d\u7684\u4fe1\u4efb\u5efa\u7acb\u4e0e\u53d1\u5c55\uff0c\u5c24\u5176\u5173\u6ce8\u5982\u4f55\u786e\u4fdd\u7528\u6237\u80fd\u591f\u5224\u65ad\u4f55\u65f6\u53ef\u4ee5\u4fe1\u4efb\u8fd9\u4e9b\u7cfb\u7edf\u3002", "conclusion": "\u8c03\u7814\u7ed3\u679c\u5c06\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u57fa\u7840\uff0c\u4ee5\u66f4\u597d\u5730 contextualize\u4fe1\u4efb\u6d4b\u91cf\u548c\u53c2\u4e0e\u8005\u7684\u4e92\u52a8\u7b56\u7565\u3002"}}
{"id": "2511.12237", "categories": ["cs.RO", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.12237", "abs": "https://arxiv.org/abs/2511.12237", "authors": ["Alysson Ribeiro da Silva", "Luiz Chaimowicz"], "title": "Intermittent Rendezvous Plans with Mixed Integer Linear Program for Large-Scale Multi-Robot Exploration", "comment": "9 pages, 9 figures, International Conference on Advanced Robotics", "summary": "Multi-Robot Exploration (MRE) systems with communication constraints have proven efficient in accomplishing a variety of tasks, including search-and-rescue, stealth, and military operations. While some works focus on opportunistic approaches for efficiency, others concentrate on pre-planned trajectories or scheduling for increased interpretability. However, scheduling usually requires knowledge of the environment beforehand, which prevents its deployment in several domains due to related uncertainties (e.g., underwater exploration). In our previous work, we proposed an intermittent communications framework for MRE under communication constraints that uses scheduled rendezvous events to mitigate such limitations. However, the system was unable to generate optimal plans and had no mechanisms to follow the plan considering realistic trajectories, which is not suited for real-world deployments. In this work, we further investigate the problem by formulating the Multi-Robot Exploration with Communication Constraints and Intermittent Connectivity (MRE-CCIC) problem. We propose a Mixed-Integer Linear Program (MILP) formulation to generate rendezvous plans and a policy to follow them based on the Rendezvous Tracking for Unknown Scenarios (RTUS) mechanism. The RTUS is a simple rule to allow robots to follow the assigned plan, considering unknown conditions. Finally, we evaluated our method in a large-scale environment configured in Gazebo simulations. The results suggest that our method can follow the plan promptly and accomplish the task efficiently. We provide an open-source implementation of both the MILP plan generator and the large-scale MRE-CCIC.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u901a\u4fe1\u7ea6\u675f\u4e0b\u7684\u591a\u673a\u5668\u4eba\u63a2\u7d22\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212(MILP)\u6a21\u578b\u548c\u6839\u636eRTUS\u673a\u5236\u8ddf\u968f\u8ba1\u5212\u7684\u7b56\u7565\uff0c\u4ee5\u5e94\u5bf9\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u548c\u901a\u4fe1\u9650\u5236\u4e0b\uff0c\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u6709\u6548\u4efb\u52a1\u6267\u884c\u95ee\u9898\uff0c\u589e\u5f3a\u5176\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u91c7\u7528\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212(MILP)\u6a21\u578b\u751f\u6210\u4f1a\u5408\u8ba1\u5212\uff0c\u5e76\u57fa\u4e8eRTUS\u673a\u5236\u8ddf\u8e2a\u6267\u884c\u8ba1\u5212\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u673a\u5668\u4eba\u63a2\u7d22(MRE)\u6846\u67b6\uff0c\u80fd\u591f\u5728\u901a\u4fe1\u7ea6\u675f\u548c\u4e0d\u786e\u5b9a\u73af\u5883\u4e0b\u751f\u6210\u6709\u6548\u7684\u4f1a\u5408\u8ba1\u5212\uff0c\u5e76\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u8ddf\u968f\u8be5\u8ba1\u5212\u8fdb\u884c\u63a2\u7d22\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u663e\u793a\u51fa\u6709\u6548\u7684\u8ba1\u5212\u8ddf\u968f\u80fd\u529b\u548c\u4efb\u52a1\u5b8c\u6210\u6548\u7387\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2511.13466", "categories": ["cs.HC", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2511.13466", "abs": "https://arxiv.org/abs/2511.13466", "authors": ["Jaclyn Ocumpaugh", "Luc Paquette", "Ryan S. Baker", "Amanda Barany", "Jeff Ginger", "Nathan Casano", "Andres F. Zambrano", "Xiner Liu", "Zhanlan Wei", "Yiqui Zhou", "Qianhui Liu", "Stephen Hutt", "Alexandra M. A. Andres", "Nidhi Nasiar", "Camille Giordano", "Martin van Velsen", "Micheal Mogessi"], "title": "The Quick Red Fox gets the best Data Driven Classroom Interviews: A manual for an interview app and its associated methodology", "comment": null, "summary": "Data Driven Classroom Interviews (DDCIs) are an interviewing technique that is facilitated by recent technological developments in the learning analytics community. DDCIs are short, targeted interviews that allow researchers to contextualize students' interactions with a digital learning environment (e.g., intelligent tutoring systems or educational games) while minimizing the amount of time that the researcher interrupts that learning experience, and focusing researcher time on the events they most want to focus on DDCIs are facilitated by a research tool called the Quick Red Fox (QRF)--an open-source server-client Android app that optimizes researcher time by directing interviewers to users that have just displayed an interesting behavior (previously defined by the research team). QRF integrates with existing student modeling technologies (e.g., behavior-sensing, affect-sensing, detection of self-regulated learning) to alert researchers to key moments in a learner's experience. This manual documents the tech while providing training on the processes involved in developing triggers and interview techniques; it also suggests methods of analyses.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u8bbf\u8c08\u6280\u672f\uff0c\u5229\u7528\u5feb\u901f\u7ea2\u72d0\u5de5\u5177\u63d0\u5347\u7814\u7a76\u6548\u7387\u5e76\u51cf\u5c11\u5b66\u751f\u5b66\u4e60\u4e2d\u65ad\u3002", "motivation": "\u4fc3\u8fdb\u7814\u7a76\u8005\u5728\u5b66\u751f\u4e0e\u6570\u5b57\u5b66\u4e60\u73af\u5883\u4e92\u52a8\u65f6\uff0c\u5728\u6700\u5c0f\u5316\u5e72\u6270\u5b66\u4e60\u7684\u60c5\u51b5\u4e0b\uff0c\u83b7\u53d6\u5173\u952e\u4fe1\u606f\u3002", "method": "\u6570\u636e\u9a71\u52a8\u8bfe\u5802\u8bbf\u8c08\uff08DDCIs\uff09\u662f\u4e00\u79cd\u5229\u7528\u6280\u672f\u53d1\u5c55\u7684\u8bbf\u8c08\u6280\u672f\uff0c\u4f7f\u7528\u4e86\u540d\u4e3a\u5feb\u901f\u7ea2\u72d0\uff08QRF\uff09\u7684\u5de5\u5177\u52a9\u529b\u8fdb\u884c\u8bbf\u8c08\u3002", "result": "\u901a\u8fc7\u6574\u5408\u5b66\u751f\u5efa\u6a21\u6280\u672f\uff0cQRF\u80fd\u5728\u5b66\u751f\u8868\u73b0\u51fa\u6709\u8da3\u884c\u4e3a\u65f6\u63d0\u9192\u7814\u7a76\u8005\uff0c\u4ece\u800c\u4f18\u5316\u8bbf\u8c08\u8fc7\u7a0b\u3002", "conclusion": "\u672c\u6587\u6863\u4e0d\u4ec5\u63cf\u8ff0\u4e86\u76f8\u5173\u6280\u672f\uff0c\u8fd8\u63d0\u4f9b\u4e86\u5f00\u53d1\u89e6\u53d1\u5668\u548c\u8bbf\u8c08\u6280\u5de7\u7684\u57f9\u8bad\uff0c\u5e76\u5efa\u8bae\u4e86\u5206\u6790\u65b9\u6cd5\u3002"}}
{"id": "2511.12361", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12361", "abs": "https://arxiv.org/abs/2511.12361", "authors": ["Leroy D'Souza", "Akash Karthikeyan", "Yash Vardhan Pant", "Sebastian Fischmeister"], "title": "SAC-MoE: Reinforcement Learning with Mixture-of-Experts for Control of Hybrid Dynamical Systems with Uncertainty", "comment": null, "summary": "Hybrid dynamical systems result from the interaction of continuous-variable dynamics with discrete events and encompass various systems such as legged robots, vehicles and aircrafts. Challenges arise when the system's modes are characterized by unobservable (latent) parameters and the events that cause system dynamics to switch between different modes are also unobservable. Model-based control approaches typically do not account for such uncertainty in the hybrid dynamics, while standard model-free RL methods fail to account for abrupt mode switches, leading to poor generalization.\n  To overcome this, we propose SAC-MoE which models the actor of the Soft Actor-Critic (SAC) framework as a Mixture-of-Experts (MoE) with a learned router that adaptively selects among learned experts. To further improve robustness, we develop a curriculum-based training algorithm to prioritize data collection in challenging settings, allowing better generalization to unseen modes and switching locations. Simulation studies in hybrid autonomous racing and legged locomotion tasks show that SAC-MoE outperforms baselines (up to 6x) in zero-shot generalization to unseen environments. Our curriculum strategy consistently improves performance across all evaluated policies. Qualitative analysis shows that the interpretable MoE router activates different experts for distinct latent modes.", "AI": {"tldr": "SAC-MoE\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u548c\u8bfe\u7a0b\u8bad\u7ec3\u7b56\u7565\u63d0\u9ad8\u4e86\u6df7\u5408\u52a8\u529b\u7cfb\u7edf\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u6df7\u5408\u52a8\u529b\u7cfb\u7edf\u7684\u590d\u6742\u6027\u4e0e\u4e0d\u786e\u5b9a\u6027\uff0c\u5305\u62ec\u4e0d\u53ef\u89c2\u5bdf\u7684\u53c2\u6570\u548c\u4e8b\u4ef6\uff0c\u7ed9\u6a21\u578b\u63a7\u5236\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5c06Soft Actor-Critic\u6846\u67b6\u7684actor\u5efa\u6a21\u4e3a\u6df7\u5408\u4e13\u5bb6\uff0c\u540c\u65f6\u91c7\u7528\u81ea\u9002\u5e94\u9009\u62e9\u7684\u5b66\u4e60\u8def\u7531\u5668\uff0c\u7ed3\u5408\u8bfe\u7a0b\u8bad\u7ec3\u7b97\u6cd5\u4f18\u5148\u6570\u636e\u6536\u96c6\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSAC-MoE\u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u65b9\u6cd5\u89e3\u51b3\u53ef\u89c2\u5bdf\u6027\u548c\u6a21\u5f0f\u5207\u6362\u5e26\u6765\u7684\u95ee\u9898\u3002", "conclusion": "SAC-MoE\u5728\u6df7\u5408\u81ea\u4e3b\u8d5b\u8f66\u548c\u56db\u8db3\u6b65\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\uff0c\u663e\u793a\u51fa\u826f\u597d\u7684\u96f6-shot \u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u8bfe\u7a0b\u8bad\u7ec3\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u6240\u6709\u8bc4\u4f30\u7b56\u7565\u7684\u6027\u80fd\u3002"}}
{"id": "2511.13480", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13480", "abs": "https://arxiv.org/abs/2511.13480", "authors": ["Parisa Arbab", "Xiaowen Fang"], "title": "A Lexical Analysis of online Reviews on Human-AI Interactions", "comment": "10 pages, 1 table", "summary": "This study focuses on understanding the complex dynamics between humans and AI systems by analyzing user reviews. While previous research has explored various aspects of human-AI interaction, such as user perceptions and ethical considerations, there remains a gap in understanding the specific concerns and challenges users face. By using a lexical approach to analyze 55,968 online reviews from G2.com, Producthunt.com, and Trustpilot.com, this preliminary research aims to analyze human-AI interaction. Initial results from factor analysis reveal key factors influencing these interactions. The study aims to provide deeper insights into these factors through content analysis, contributing to the development of more user-centric AI systems. The findings are expected to enhance our understanding of human-AI interaction and inform future AI technology and user experience improvements.", "AI": {"tldr": "\u901a\u8fc7\u5bf9\u5728\u7ebf\u8bc4\u8bba\u7684\u8bcd\u6c47\u5206\u6790\uff0c\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u5f71\u54cd\u4eba\u673a\u4ea4\u4e92\u7684\u5173\u952e\u56e0\u7d20\uff0c\u65e8\u5728\u4e3a\u7528\u6237\u4e2d\u5fc3\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u66f4\u6df1\u5165\u7684\u89c1\u89e3\u3002", "motivation": "\u7406\u89e3\u4eba\u7c7b\u4e0e\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u4e4b\u95f4\u590d\u6742\u7684\u52a8\u6001\u5173\u7cfb\uff0c\u7279\u522b\u662f\u7528\u6237\u9762\u4e34\u7684\u5177\u4f53\u62c5\u5fe7\u548c\u6311\u6218\u3002", "method": "\u91c7\u7528\u8bcd\u6c47\u5206\u6790\u548c\u56e0\u5b50\u5206\u6790\u65b9\u6cd5\uff0c\u5bf9\u6765\u81eaG2.com\u3001Producthunt.com\u548cTrustpilot.com\u7684\u7528\u6237\u8bc4\u8bba\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5bf955,968\u6761\u5728\u7ebf\u8bc4\u8bba\u8fdb\u884c\u8bcd\u6c47\u5206\u6790\uff0c\u8bc6\u522b\u5f71\u54cd\u4eba\u673a\u4ea4\u4e92\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5c06\u589e\u5f3a\u6211\u4eec\u5bf9\u4eba\u673a\u4ea4\u4e92\u7684\u7406\u89e3\uff0c\u4fc3\u8fdb\u672a\u6765\u4eba\u5de5\u667a\u80fd\u6280\u672f\u548c\u7528\u6237\u4f53\u9a8c\u7684\u6539\u5584\u3002"}}
{"id": "2511.12380", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12380", "abs": "https://arxiv.org/abs/2511.12380", "authors": ["Nicholas Gunter", "Heiko Kabutz", "Kaushik Jayaram"], "title": "Multilaminate piezoelectric PVDF actuators to enhance performance of soft micro robots", "comment": null, "summary": "Multilayer piezoelectric polyvinylidene fluoride (PVDF) actuators are a promising approach to enhance performance of soft microrobotic systems. In this work, we develop and characterize multilayer PVDF actuators with parallel voltage distribution across each layer, bridging a unique design space between brittle high-force PZT stacks and compliant but lower-bandwidth soft polymer actuators. We show the effects of layer thickness and number of layers in actuator performance and their agreement with a first principles model. By varying these parameters, we demonstrate actuators capable of >3 mm of free deflection, >20 mN of blocked force, and >=500 Hz, while operating at voltages as low as 150 volts. To illustrate their potential for robotic integration, we integrate our actuators into a planar, translating microrobot that leverages resonance to achieve locomotion with robustness to large perturbations.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u591a\u5c42PVDF\u9a71\u52a8\u5668\uff0c\u63d0\u5347\u4e86\u8f6f\u5fae\u578b\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u8bbe\u8ba1\u7a7a\u95f4\u53ca\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u63d0\u5347\u8f6f\u5fae\u578b\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u586b\u8865\u8106\u6027\u9ad8\u5f3a\u5ea6PZT\u5806\u4e0e\u67d4\u6027\u4f4e\u5e26\u5bbd\u805a\u5408\u7269\u9a71\u52a8\u5668\u4e4b\u95f4\u7684\u8bbe\u8ba1\u7a7a\u95f4\u3002", "method": "\u901a\u8fc7\u6539\u53d8\u9a71\u52a8\u5668\u7684\u5c42\u539a\u548c\u5c42\u6570\uff0c\u4f7f\u7528\u7b2c\u4e00\u6027\u6a21\u578b\u5bf9\u5176\u6027\u80fd\u8fdb\u884c\u8868\u5f81\u3002", "result": "\u5b9e\u73b0\u4e86\u8d85\u8fc73\u6beb\u7c73\u7684\u81ea\u7531\u504f\u8f6c\u3001\u8d85\u8fc720\u6beb\u725b\u7684\u963b\u529b\u548c>=500\u8d6b\u5179\u7684\u9891\u7387\uff0c\u4e14\u5de5\u4f5c\u7535\u538b\u4ec5\u4e3a150\u4f0f\u7279\u3002", "conclusion": "\u591a\u5c42PVDF\u9a71\u52a8\u5668\u5728\u8f6f\u5fae\u578b\u673a\u5668\u4eba\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u5907\u9ad8\u81ea\u7531\u504f\u8f6c\u3001\u529b\u91cf\u53ca\u9891\u7387\uff0c\u540c\u65f6\u4f4e\u5de5\u4f5c\u7535\u538b\u3002"}}
{"id": "2511.13670", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13670", "abs": "https://arxiv.org/abs/2511.13670", "authors": ["Agnieszka Bie\u0144kowska", "Jacek Ma\u0142ecki", "Alexander Mathiesen-Ohman", "Katarzyna Tworek"], "title": "Person-AI Bidirectional Fit - A Proof-Of-Concept Case Study Of Augmented Human-Ai Symbiosis In Management Decision-Making Process", "comment": "30 pages, 2 figures", "summary": "This article develops the concept of Person-AI bidirectional fit, defined as the continuously evolving, context-sensitive alignment-primarily cognitive, but also emotional and behavioral-between a human decision-maker and an artificial intelligence system. Grounded in contingency theory and quality theory, the study examines the role of P-AI fit in managerial decision-making through a proof-of-concept case study involving a real hiring process for a Senior AI Lead. Three decision pathways are compared: (1) independent evaluations by a CEO, CTO, and CSO; (2) an evaluation produced by an augmented human-AI symbiotic intelligence system (H3LIX-LAIZA); and (3) an assessment generated by a general-purpose large language model. The results reveal substantial role-based divergence in human judgments, high alignment between H3LIX-LAIZA and the CEOs implicit decision model-including ethical disqualification of a high-risk candidate and a critical false-positive recommendation from the LLMr. The findings demonstrate that higher P-AI fit, exemplified by the CEO H3LIX-LAIZA relationship, functions as a mechanism linking augmented symbiotic intelligence to accurate, trustworthy, and context-sensitive decisions. The study provides an initial verification of the P-AI fit construct and a proof-of-concept for H3LIX-LAIZA as an augmented human-AI symbiotic intelligence system.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4eba-AI\u53cc\u5411\u5339\u914d\u7684\u6982\u5ff5\uff0c\u7814\u7a76\u5176\u5728\u7ba1\u7406\u51b3\u7b56\u4e2d\u7684\u4f5c\u7528\uff0c\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u6bd4\u8f83\u4f20\u7edf\u8bc4\u4f30\u4e0e\u589e\u5f3a\u4eba-AI\u667a\u80fd\u7cfb\u7edf\u7684\u6548\u679c\u3002", "motivation": "\u65e8\u5728\u53d1\u5c55\u4eba-AI\u53cc\u5411\u5339\u914d\u7684\u6982\u5ff5\uff0c\u5e76\u63a2\u8ba8\u5176\u5728\u7ba1\u7406\u51b3\u7b56\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u771f\u5b9e\u7684\u9ad8\u7ea7AI\u9886\u5bfc\u8005\u62db\u8058\u8fc7\u7a0b\u6848\u4f8b\u7814\u7a76\uff0c\u6bd4\u8f83\u4e86\u4e09\u6761\u51b3\u7b56\u8def\u5f84\u7684\u6548\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4eba\u7c7b\u5224\u65ad\u5b58\u5728\u663e\u8457\u7684\u89d2\u8272\u57fa\u7840\u5206\u6b67\uff0cH3LIX-LAIZA\u4e0eCEO\u9690\u6027\u51b3\u7b56\u6a21\u578b\u4e4b\u95f4\u5177\u6709\u9ad8\u5ea6\u5bf9\u9f50\uff0c\u800c\u901a\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5219\u5b58\u5728\u5173\u952e\u7684\u5047\u9633\u6027\u63a8\u8350\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8f83\u9ad8\u7684\u4eba-AI\u5339\u914d\u5ea6\uff08\u4ee5CEO\u4e0eH3LIX-LAIZA\u7684\u5173\u7cfb\u4e3a\u4f8b\uff09\u4f5c\u4e3a\u73af\u8282\uff0c\u5c06\u589e\u5f3a\u7684\u5171\u751f\u667a\u80fd\u4e0e\u51c6\u786e\u3001\u53ef\u9760\u548c\u5177\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u51b3\u7b56\u8054\u7cfb\u8d77\u6765\u3002"}}
{"id": "2511.12383", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12383", "abs": "https://arxiv.org/abs/2511.12383", "authors": ["Sanjar Atamuradov"], "title": "Evaluating Model-Agnostic Meta-Learning on MetaWorld ML10 Benchmark: Fast Adaptation in Robotic Manipulation Tasks", "comment": "7 pages, 5 figures", "summary": "Meta-learning algorithms enable rapid adaptation to new tasks with minimal data, a critical capability for real-world robotic systems. This paper evaluates Model-Agnostic Meta-Learning (MAML) combined with Trust Region Policy Optimization (TRPO) on the MetaWorld ML10 benchmark, a challenging suite of ten diverse robotic manipulation tasks. We implement and analyze MAML-TRPO's ability to learn a universal initialization that facilitates few-shot adaptation across semantically different manipulation behaviors including pushing, picking, and drawer manipulation. Our experiments demonstrate that MAML achieves effective one-shot adaptation with clear performance improvements after a single gradient update, reaching final success rates of 21.0% on training tasks and 13.2% on held-out test tasks. However, we observe a generalization gap that emerges during meta-training, where performance on test tasks plateaus while training task performance continues to improve. Task-level analysis reveals high variance in adaptation effectiveness, with success rates ranging from 0% to 80% across different manipulation skills. These findings highlight both the promise and current limitations of gradient-based meta-learning for diverse robotic manipulation, and suggest directions for future work in task-aware adaptation and structured policy architectures.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86MAML\u4e0eTRPO\u7ed3\u5408\u7684\u7b97\u6cd5\u5728\u591a\u6837\u5316\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u8be5\u65b9\u6cd5\u5728\u4e00\u6b21\u6027\u9002\u5e94\u4e0a\u5b58\u5728\u663e\u8457\u8fdb\u6b65\uff0c\u4f46\u5728\u4e0d\u540c\u4efb\u52a1\u4e4b\u95f4\u7684\u6709\u6548\u6027\u5b58\u5728\u8f83\u5927\u5dee\u5f02\u3002", "motivation": "\u5feb\u901f\u9002\u5e94\u65b0\u4efb\u52a1\u7684\u80fd\u529b\u662f\u5b9e\u9645\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5173\u952e\u9700\u6c42", "method": "\u7ed3\u5408\u6a21\u578b\u65e0\u5173\u5143\u5b66\u4e60\uff08MAML\uff09\u548c\u4fe1\u4efb\u533a\u57df\u7b56\u7565\u4f18\u5316\uff08TRPO\uff09\u8bc4\u4f30\u5143\u4e16\u754cML10\u57fa\u51c6\u4efb\u52a1", "result": "MAML-TRPO\u5728\u73b0\u6709\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u4e00\u6b21\u6027\u9002\u5e94\uff0c\u5e76\u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u4efb\u52a1\u4e2d\u663e\u793a\u4e86\u4e0d\u540c\u7a0b\u5ea6\u7684\u6210\u529f\u7387", "conclusion": "\u867d\u7136MAML-TRPO\u5c55\u793a\u4e86\u826f\u597d\u7684\u4e00\u6b21\u6027\u9002\u5e94\u80fd\u529b\uff0c\u4f46\u5143\u8bad\u7ec3\u671f\u95f4\u7684\u6cdb\u5316\u5dee\u8ddd\u63ed\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u591a\u6837\u5316\u64cd\u4f5c\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u4efb\u52a1\u611f\u77e5\u9002\u5e94\u548c\u7ed3\u6784\u5316\u7b56\u7565\u67b6\u6784\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u5efa\u8bae\u3002"}}
{"id": "2511.12390", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12390", "abs": "https://arxiv.org/abs/2511.12390", "authors": ["Sanjar Atamuradov"], "title": "Learning Adaptive Neural Teleoperation for Humanoid Robots: From Inverse Kinematics to End-to-End Control", "comment": "9 pages, 5 figures", "summary": "Virtual reality (VR) teleoperation has emerged as a promising approach for controlling humanoid robots in complex manipulation tasks. However, traditional teleoperation systems rely on inverse kinematics (IK) solvers and hand-tuned PD controllers, which struggle to handle external forces, adapt to different users, and produce natural motions under dynamic conditions. In this work, we propose a learning-based neural teleoperation framework that replaces the conventional IK+PD pipeline with learned policies trained via reinforcement learning. Our approach learns to directly map VR controller inputs to robot joint commands while implicitly handling force disturbances, producing smooth trajectories, and adapting to user preferences. We train our policies in simulation using demonstrations collected from IK-based teleoperation as initialization, then fine-tune them with force randomization and trajectory smoothness rewards. Experiments on the Unitree G1 humanoid robot demonstrate that our learned policies achieve 34% lower tracking error, 45% smoother motions, and superior force adaptation compared to the IK baseline, while maintaining real-time performance (50Hz control frequency). We validate our approach on manipulation tasks including object pick-and-place, door opening, and bimanual coordination. These results suggest that learning-based approaches can significantly improve the naturalness and robustness of humanoid teleoperation systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u795e\u7ecf\u9065\u64cd\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u66ff\u4ee3\u4f20\u7edf\u7684\u9006\u8fd0\u52a8\u5b66\u548cPD\u63a7\u5236\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u9065\u64cd\u4f5c\u7684\u81ea\u7136\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u9065\u64cd\u4f5c\u7cfb\u7edf\u5728\u5e94\u5bf9\u5916\u90e8\u529b\u91cf\u548c\u7528\u6237\u9002\u5e94\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u6b64\u4e9f\u9700\u6539\u8fdb\u4ee5\u63d0\u5347\u64cd\u63a7\u6548\u679c\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u5b66\u4e60\u7b56\u7565\uff0c\u76f4\u63a5\u5c06VR\u63a7\u5236\u5668\u8f93\u5165\u6620\u5c04\u5230\u673a\u5668\u4eba\u5173\u8282\u547d\u4ee4\uff0c\u540c\u65f6\u5904\u7406\u5916\u90e8\u6270\u52a8\uff0c\u4fc3\u8fdb\u5e73\u6ed1\u8f68\u8ff9\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5b66\u4e60\u7b56\u7565\u5728\u8ddf\u8e2a\u7cbe\u5ea6\u3001\u8fd0\u52a8\u5e73\u6ed1\u6027\u548c\u529b\u91cf\u9002\u5e94\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edfIK\u57fa\u7ebf\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u9065\u64cd\u4f5c\u7cfb\u7edf\u7684\u81ea\u7136\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.12436", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12436", "abs": "https://arxiv.org/abs/2511.12436", "authors": ["Xiaoshuai Hao", "Yingbo Tang", "Lingfeng Zhang", "Yanbiao Ma", "Yunfeng Diao", "Ziyu Jia", "Wenbo Ding", "Hangjun Ye", "Long Chen"], "title": "RoboAfford++: A Generative AI-Enhanced Dataset for Multimodal Affordance Learning in Robotic Manipulation and Navigation", "comment": null, "summary": "Robotic manipulation and navigation are fundamental capabilities of embodied intelligence, enabling effective robot interactions with the physical world. Achieving these capabilities requires a cohesive understanding of the environment, including object recognition to localize target objects, object affordances to identify potential interaction areas and spatial affordances to discern optimal areas for both object placement and robot movement. While Vision-Language Models (VLMs) excel at high-level task planning and scene understanding, they often struggle to infer actionable positions for physical interaction, such as functional grasping points and permissible placement regions. This limitation stems from the lack of fine-grained annotations for object and spatial affordances in their training datasets. To tackle this challenge, we introduce RoboAfford++, a generative AI-enhanced dataset for multimodal affordance learning for both robotic manipulation and navigation. Our dataset comprises 869,987 images paired with 2.0 million question answering (QA) annotations, covering three critical tasks: object affordance recognition to identify target objects based on attributes and spatial relationships, object affordance prediction to pinpoint functional parts for manipulation, and spatial affordance localization to identify free space for object placement and robot navigation. Complementing this dataset, we propose RoboAfford-Eval, a comprehensive benchmark for assessing affordance-aware prediction in real-world scenarios, featuring 338 meticulously annotated samples across the same three tasks. Extensive experimental results reveal the deficiencies of existing VLMs in affordance learning, while fine-tuning on the RoboAfford++ dataset significantly enhances their ability to reason about object and spatial affordances, validating the dataset's effectiveness.", "AI": {"tldr": "RoboAfford++\u662f\u4e00\u4e2a\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u5bfc\u822a\u7684\u591a\u6a21\u6001\u53ef\u7528\u6027\u5b66\u4e60\u7684\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7269\u4f53\u53ca\u7a7a\u95f4\u53ef\u7528\u6027\u63a8\u65ad\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7269\u4f53\u548c\u7a7a\u95f4\u53ef\u7528\u6027\u7684\u63a8\u65ad\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u7ec6\u81f4\u7684\u6807\u6ce8\u6570\u636e", "method": "\u63d0\u51faRoboAfford++\u6570\u636e\u96c6\u5e76\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5", "result": "RoboAfford++\u6570\u636e\u96c6\u53caRoboAfford-Eval\u57fa\u51c6\u5927\u5e45\u63d0\u9ad8VLM\u5728\u53ef\u7528\u6027\u5b66\u4e60\u65b9\u9762\u7684\u8868\u73b0", "conclusion": "RoboAfford++\u7684\u6570\u636e\u96c6\u6709\u6548\u63d0\u5347\u4e86\u73b0\u6709\u6a21\u578b\u5bf9\u7269\u4f53\u548c\u7a7a\u95f4\u53ef\u7528\u6027\u7684\u63a8\u7406\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6570\u636e\u96c6\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.12479", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12479", "abs": "https://arxiv.org/abs/2511.12479", "authors": ["Navin Sriram Ravie", "Keerthi Vasan M", "Bijo Sebastian"], "title": "ClutterNav: Gradient-Guided Search for Efficient 3D Clutter Removal with Learned Costmaps", "comment": null, "summary": "Dense clutter removal for target object retrieval presents a challenging problem, especially when targets are embedded deep within densely-packed configurations. It requires foresight to minimize overall changes to the clutter configuration while accessing target objects, avoiding stack destabilization and reducing the number of object removals required. Rule-based planners when applied to this problem, rely on rigid heuristics, leading to high computational overhead. End-to-end reinforcement learning approaches struggle with interpretability and generalizability over different conditions. To address these issues, we present ClutterNav, a novel decision-making framework that can identify the next best object to be removed so as to access a target object in a given clutter, while minimising stack disturbances. ClutterNav formulates the problem as a continuous reinforcement learning task, where each object removal dynamically updates the understanding of the scene. A removability critic, trained from demonstrations, estimates the cost of removing any given object based on geometric and spatial features. This learned cost is complemented by integrated gradients that assess how the presence or removal of surrounding objects influences the accessibility of the target. By dynamically prioritizing actions that balance immediate removability against long-term target exposure, ClutterNav achieves near human-like strategic sequencing, without predefined heuristics. The proposed approach is validated extensively in simulation and over real-world experiments. The results demonstrate real-time, occlusion-aware decision-making in partially observable environments.", "AI": {"tldr": "ClutterNav\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u51b3\u7b56\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u53bb\u9664\u7269\u4f53\u7684\u95ee\u9898\u89c6\u4e3a\u8fde\u7eed\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\uff0c\u80fd\u591f\u5728\u6709\u6548\u79fb\u9664\u7269\u4f53\u7684\u540c\u65f6\u5b9e\u73b0\u76ee\u6807\u7269\u4f53\u7684\u9ad8\u6548\u8bbf\u95ee\u3002", "motivation": "\u5bc6\u96c6\u7684\u6742\u4e71\u7269\u54c1\u53bb\u9664\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u5c24\u5176\u662f\u5f53\u76ee\u6807\u5d4c\u5165\u5728\u5bc6\u96c6\u914d\u7f6e\u4e2d\u65f6\u3002\u56e0\u6b64\uff0c\u5728\u5c3d\u91cf\u51cf\u5c11\u6742\u4e71\u914d\u7f6e\u53d8\u5316\u7684\u540c\u65f6\u8bbf\u95ee\u76ee\u6807\u7269\u54c1\u663e\u5f97\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e00\u4e2a\u79fb\u9664\u6027\u6279\u8bc4\u8005\u8fdb\u884c\u8bad\u7ec3\uff0c\u6b64\u6279\u8bc4\u8005\u6839\u636e\u51e0\u4f55\u548c\u7a7a\u95f4\u7279\u5f81\u8bc4\u4f30\u53bb\u9664\u4efb\u4e00\u7269\u4f53\u7684\u6210\u672c\uff0c\u5e76\u4f7f\u7528\u96c6\u6210\u68af\u5ea6\u6765\u8bc4\u4f30\u5468\u56f4\u7269\u4f53\u7684\u5b58\u5728\u6216\u53bb\u9664\u5bf9\u76ee\u6807\u53ef\u8fbe\u6027\u7684\u5f71\u54cd\u3002", "result": "ClutterNav\u5c55\u793a\u4e86\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u73af\u5883\u4e2d\u5b9e\u65f6\u4e14\u8003\u8651\u906e\u6321\u7684\u51b3\u7b56\u80fd\u529b\uff0c\u80fd\u591f\u6709\u6548\u5730\u8bc6\u522b\u4e0b\u4e00\u4e2a\u6700\u4f73\u79fb\u9664\u7269\u4f53\u4ee5\u8bbf\u95ee\u76ee\u6807\u7269\u4f53\u3002", "conclusion": "\u901a\u8fc7\u5728\u6a21\u62df\u548c\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u7684\u5e7f\u6cdb\u9a8c\u8bc1\uff0cClutterNav\u5b9e\u73b0\u4e86\u4eba\u7c7b\u822c\u7684\u6218\u7565\u5e8f\u5217\u5316\uff0c\u80fd\u5728\u590d\u6742\u73af\u5883\u4e2d\u9ad8\u6548\u51b3\u7b56\u3002"}}
{"id": "2511.12526", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12526", "abs": "https://arxiv.org/abs/2511.12526", "authors": ["Davide De Benedittis", "Giovanni Di Lorenzo", "Franco Angelini", "Barbara Valle", "Marina Serena Borgatti", "Paolo Remagnino", "Marco Caccianiga", "Manolo Garabini"], "title": "Botany Meets Robotics in Alpine Scree Monitoring", "comment": "Published as Early Access in IEEE Transactions on Field Robotics. 19 pages, 13 figures", "summary": "According to the European Union's Habitat Directive, habitat monitoring plays a critical role in response to the escalating problems posed by biodiversity loss and environmental degradation. Scree habitats, hosting unique and often endangered species, face severe threats from climate change due to their high-altitude nature. Traditionally, their monitoring has required highly skilled scientists to conduct extensive fieldwork in remote, potentially hazardous locations, making the process resource-intensive and time-consuming. This paper presents a novel approach for scree habitat monitoring using a legged robot to assist botanists in data collection and species identification. Specifically, we deployed the ANYmal C robot in the Italian Alpine bio-region in two field campaigns spanning two years and leveraged deep learning to detect and classify key plant species of interest. Our results demonstrate that agile legged robots can navigate challenging terrains and increase the frequency and efficiency of scree monitoring. When paired with traditional phytosociological surveys performed by botanists, this robotics-assisted protocol not only streamlines field operations but also enhances data acquisition, storage, and usage. The outcomes of this research contribute to the evolving landscape of robotics in environmental science, paving the way for a more comprehensive and sustainable approach to habitat monitoring and preservation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u817f\u90e8\u673a\u5668\u4eba\u8f85\u52a9\u690d\u7269\u5b66\u5bb6\u8fdb\u884c\u77f3\u783e\u6816\u606f\u5730\u76d1\u6d4b\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u63d0\u9ad8\u690d\u7269\u7269\u79cd\u8bc6\u522b\u7684\u6548\u7387\uff0c\u63a8\u52a8\u53ef\u6301\u7eed\u751f\u6001\u76d1\u6d4b\u3002", "motivation": "\u9274\u4e8e\u751f\u7269\u591a\u6837\u6027\u4e27\u5931\u548c\u73af\u5883\u9000\u5316\u95ee\u9898\u7684\u52a0\u5267\uff0c\u6816\u606f\u5730\u76d1\u6d4b\u5728\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u800c\u4f20\u7edf\u76d1\u6d4b\u65b9\u6cd5\u8d44\u6e90\u5bc6\u96c6\u4e14\u8017\u65f6\u3002", "method": "\u901a\u8fc7\u5728\u610f\u5927\u5229\u963f\u5c14\u5351\u65af\u751f\u7269\u533a\u7684\u4e24\u6b21\u5b9e\u5730\u8003\u5bdf\u4e2d\uff0c\u4f7f\u7528ANYmal C\u673a\u5668\u4eba\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u6536\u96c6\u548c\u8bc6\u522b\u5173\u952e\u690d\u7269\u7269\u79cd\u7684\u6570\u636e\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\u7075\u6d3b\u7684\u817f\u90e8\u673a\u5668\u4eba\u80fd\u591f\u5728\u56f0\u96be\u5730\u5f62\u4e2d\u5bfc\u822a\uff0c\u63d0\u9ad8\u77f3\u783e\u6816\u606f\u5730\u76d1\u6d4b\u7684\u9891\u7387\u548c\u6548\u7387\uff0c\u5e76\u4e0e\u690d\u7269\u5b66\u5bb6\u8fdb\u884c\u4f20\u7edf\u7684\u690d\u7269\u7fa4\u843d\u8c03\u67e5\u76f8\u7ed3\u5408\uff0c\u4f18\u5316\u73b0\u573a\u64cd\u4f5c\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u817f\u90e8\u673a\u5668\u4eba\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u77f3\u783e\u6816\u606f\u5730\u76d1\u6d4b\u7684\u6548\u7387\u548c\u9891\u7387\uff0c\u4fc3\u8fdb\u6570\u636e\u7684\u83b7\u53d6\u548c\u4f7f\u7528\uff0c\u63a8\u52a8\u751f\u6001\u76d1\u6d4b\u7684\u53ef\u6301\u7eed\u53d1\u5c55\u3002"}}
{"id": "2511.12618", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12618", "abs": "https://arxiv.org/abs/2511.12618", "authors": ["Jordan Leyva", "Nahim J. Moran Vera", "Yihan Xu", "Adrien Durasno", "Christopher U. Romero", "Tendai Chimuka", "Gabriel O. Huezo Ramirez", "Ziqian Dong", "Roberto Rojas-Cessa"], "title": "EcoFlight: Finding Low-Energy Paths Through Obstacles for Autonomous Sensing Drones", "comment": "Autonomous drone, A* algorithm, 3D environments, path planning, obstacle avoidance, energy efficiency, MIT Conference", "summary": "Obstacle avoidance path planning for uncrewed aerial vehicles (UAVs), or drones, is rarely addressed in most flight path planning schemes, despite obstacles being a realistic condition. Obstacle avoidance can also be energy-intensive, making it a critical factor in efficient point-to-point drone flights. To address these gaps, we propose EcoFlight, an energy-efficient pathfinding algorithm that determines the lowest-energy route in 3D space with obstacles. The algorithm models energy consumption based on the drone propulsion system and flight dynamics. We conduct extensive evaluations, comparing EcoFlight with direct-flight and shortest-distance schemes. The simulation results across various obstacle densities show that EcoFlight consistently finds paths with lower energy consumption than comparable algorithms, particularly in high-density environments. We also demonstrate that a suitable flying speed can further enhance energy savings.", "AI": {"tldr": "EcoFlight\u662f\u4e00\u79cd\u65b0\u578b\u7684\u80fd\u6548\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\uff0c\u80fd\u591f\u5728\u590d\u6742\u969c\u788d\u73af\u5883\u4e2d\u63d0\u4f9b\u4f4e\u80fd\u8017\u7684\u98de\u884c\u8def\u5f84\uff0c\u76f8\u8f83\u4e8e\u4f20\u7edf\u7b97\u6cd5\u8868\u73b0\u66f4\u4f73\u3002", "motivation": "\u76ee\u524d\u5927\u591a\u6570\u98de\u884c\u8def\u5f84\u89c4\u5212\u65b9\u6848\u672a\u5145\u5206\u8003\u8651\u969c\u788d\u7269\u7684\u5f71\u54cd\uff0c\u800c\u969c\u788d\u7269\u4f1a\u663e\u8457\u589e\u52a0\u80fd\u8017\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\u4ee5\u63d0\u9ad8\u80fd\u6548\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684EcoFlight\u7b97\u6cd5\uff0c\u901a\u8fc7\u6a21\u578b\u5316\u65e0\u4eba\u673a\u7684\u63a8\u8fdb\u7cfb\u7edf\u548c\u98de\u884c\u52a8\u529b\u5b66\uff0c\u6765\u786e\u5b9a\u57283D\u7a7a\u95f4\u4e2d\u6700\u4f4e\u80fd\u91cf\u6d88\u8017\u7684\u98de\u884c\u8def\u5f84\u3002", "result": "\u7ecf\u8fc7\u5e7f\u6cdb\u8bc4\u4f30\uff0cEcoFlight\u5728\u4e0d\u540c\u969c\u788d\u5bc6\u5ea6\u4e0b\u4e0e\u76f4\u63a5\u98de\u884c\u548c\u6700\u77ed\u8ddd\u79bb\u65b9\u6848\u8fdb\u884c\u6bd4\u8f83\uff0c\u7ed3\u679c\u663e\u793aEcoFlight\u5728\u80fd\u8017\u4e0a\u5177\u6709\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u969c\u788d\u7269\u5bc6\u5ea6\u9ad8\u7684\u73af\u5883\u4e2d\u3002\u8fd8\u53d1\u73b0\u5408\u9002\u7684\u98de\u884c\u901f\u5ea6\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u80fd\u91cf\u8282\u7701\u3002", "conclusion": "EcoFlight\u7b97\u6cd5\u5728\u5b58\u5728\u969c\u788d\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u6709\u6548\u627e\u5230\u80fd\u8017\u66f4\u4f4e\u7684\u98de\u884c\u8def\u5f84\uff0c\u5c24\u5176\u5728\u969c\u788d\u5bc6\u96c6\u7684\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2511.12650", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12650", "abs": "https://arxiv.org/abs/2511.12650", "authors": ["Arvind Kumar Mishra", "Sohom Chakrabarty"], "title": "Task-Aware Morphology Optimization of Planar Manipulators via Reinforcement Learning", "comment": "10 pages, 11 figures, It is submitted as a journal option paper associated with the IFAC World Congress 2026", "summary": "In this work, Yoshikawa's manipulability index is used to investigate reinforcement learning (RL) as a framework for morphology optimization in planar robotic manipulators. A 2R manipulator tracking a circular end-effector path is first examined because this case has a known analytical optimum: equal link lengths and the second joint orthogonal to the first. This serves as a validation step to test whether RL can rediscover the optimum using reward feedback alone, without access to the manipulability expression or the Jacobian. Three RL algorithms (SAC, DDPG, and PPO) are compared with grid search and black-box optimizers, with morphology represented by a single action parameter phi that maps to the link lengths. All methods converge to the analytical solution, showing that numerical recovery of the optimum is possible without supplying analytical structure.\n  Most morphology design tasks have no closed-form solutions, and grid or heuristic search becomes expensive as dimensionality increases. RL is therefore explored as a scalable alternative. The formulation used for the circular path is extended to elliptical and rectangular paths by expanding the action space to the full morphology vector (L1, L2, theta2). In these non-analytical settings, RL continues to converge reliably, whereas grid and black-box methods require far larger evaluation budgets. These results indicate that RL is effective for both recovering known optima and solving morphology optimization problems without analytical solutions.", "AI": {"tldr": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u6709\u6548\u5730\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u63a7\u5668\u7684\u5f62\u6001\u4f18\u5316\uff0c\u65e0\u9700\u89e3\u6790\u89e3\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9ad8\u7ef4\u7a7a\u95f4\u3002", "motivation": "\u5927\u591a\u6570\u5f62\u6001\u8bbe\u8ba1\u4efb\u52a1\u6ca1\u6709\u5c01\u95ed\u89e3\u6790\u89e3\uff0c\u968f\u7740\u7ef4\u5ea6\u589e\u52a0\uff0c\u7f51\u683c\u641c\u7d22\u6216\u542f\u53d1\u5f0f\u641c\u7d22\u4ee3\u4ef7\u6602\u8d35\uff0c\u56e0\u6b64\u63a2\u7d22\u5f3a\u5316\u5b66\u4e60\u4f5c\u4e3a\u53ef\u6269\u5c55\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u5229\u7528Yoshikawa\u7684\u53ef\u64cd\u63a7\u6027\u6307\u6570\uff0c\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f18\u5316\u5e73\u9762\u673a\u5668\u4eba\u64cd\u63a7\u5668\u7684\u5f62\u6001\uff0c\u6bd4\u8f83\u4e09\u79cdRL\u7b97\u6cd5\u4e0e\u7f51\u683c\u641c\u7d22\u53ca\u9ed1\u7bb1\u4f18\u5316\u5668\u7684\u8868\u73b0\u3002", "result": "\u6240\u6709\u65b9\u6cd5\u90fd\u6536\u655b\u81f3\u89e3\u6790\u89e3\uff0c\u5728\u65e0\u89e3\u6790\u7ed3\u6784\u60c5\u51b5\u4e0b\uff0c\u6570\u503c\u6062\u590d\u6700\u4f18\u89e3\u662f\u53ef\u80fd\u7684\uff0c\u800c\u5728\u975e\u89e3\u6790\u8bbe\u7f6e\u4e0b\uff0c\u5f3a\u5316\u5b66\u4e60\u4ecd\u7136\u53ef\u9760\u6536\u655b\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u5728\u5df2\u77e5\u6700\u4f18\u89e3\u7684\u6062\u590d\u548c\u65e0\u89e3\u6790\u89e3\u7684\u5f62\u6001\u4f18\u5316\u95ee\u9898\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002"}}
{"id": "2511.12755", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12755", "abs": "https://arxiv.org/abs/2511.12755", "authors": ["Aleesha Khurram", "Amir Moeini", "Shangtong Zhang", "Rohan Chandra"], "title": "Prompt-Driven Domain Adaptation for End-to-End Autonomous Driving via In-Context RL", "comment": null, "summary": "Despite significant progress and advances in autonomous driving, many end-to-end systems still struggle with domain adaptation (DA), such as transferring a policy trained under clear weather to adverse weather conditions. Typical DA strategies in the literature include collecting additional data in the target domain or re-training the model, or both. Both these strategies quickly become impractical as we increase scale and complexity of driving. These limitations have encouraged investigation into few-shot and zero-shot prompt-driven DA at inference time involving LLMs and VLMs. These methods work by adding a few state-action trajectories during inference to the prompt (similar to in-context learning). However, there are two limitations of such an approach: $(i)$ prompt-driven DA methods are currently restricted to perception tasks such as detection and segmentation and $(ii)$ they require expert few-shot data. In this work, we present a new approach to inference-time few-shot prompt-driven DA for closed-loop autonomous driving in adverse weather condition using in-context reinforcement learning (ICRL). Similar to other prompt-driven DA methods, our approach does not require any updates to the model parameters nor does it require additional data collection in adversarial weather regime. Furthermore, our approach advances the state-of-the-art in prompt-driven DA by extending to closed driving using general trajectories observed during inference. Our experiments using the CARLA simulator show that ICRL results in safer, more efficient, and more comfortable driving policies in the target domain compared to state-of-the-art prompt-driven DA baselines.", "AI": {"tldr": " \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a8\u7406\u65f6\u5c11-shot \u63d0\u793a\u9a71\u52a8\u9886\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u81ea\u4e3b\u9a7e\u9a76\u95ee\u9898\uff0c\u91c7\u7528\u4e86\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\uff0c\u5b9e\u9a8c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": " \u89e3\u51b3\u81ea\u4e3b\u9a7e\u9a76\u7cfb\u7edf\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u7684\u9886\u57df\u9002\u5e94\u95ee\u9898\uff0c\u514b\u670d\u4f20\u7edf\u9886\u57df\u9002\u5e94\u7b56\u7565\u7684\u5c40\u9650\u6027\u3002", "method": " inference-time few-shot prompt-driven domain adaptation using in-context reinforcement learning (ICRL)", "result": " \u5b9e\u9a8c\u8868\u660e\uff0cICRL\u5728CARLA\u6a21\u62df\u5668\u4e2d\u4f18\u4e8e\u73b0\u6709\u7684\u63d0\u793a\u9a71\u52a8\u9886\u57df\u9002\u5e94\u57fa\u7ebf\uff0c\u63d0\u4f9b\u66f4\u5b89\u5168\u3001\u9ad8\u6548\u548c\u8212\u9002\u7684\u9a7e\u9a76\u7b56\u7565\u3002", "conclusion": " \u901a\u8fc7\u6269\u5c55\u63d0\u793a\u9a71\u52a8\u9886\u57df\u9002\u5e94\u5230\u5c01\u95ed\u9a7e\u9a76\u5e76\u4f7f\u7528\u4e00\u822c\u8f68\u8ff9\uff0cICRL\u5728\u5e94\u5bf9\u6311\u6218\u6027\u9a7e\u9a76\u73af\u5883\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002"}}
{"id": "2511.12778", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12778", "abs": "https://arxiv.org/abs/2511.12778", "authors": ["Vignesh Rajagopal", "Kasun Weerakoon Kulathun Mudiyanselage", "Gershom Devake Seneviratne", "Pon Aswin Sankaralingam", "Mohamed Elnoor", "Jing Liang", "Rohan Chandra", "Dinesh Manocha"], "title": "DR. Nav: Semantic-Geometric Representations for Proactive Dead-End Recovery and Navigation", "comment": null, "summary": "We present DR. Nav (Dead-End Recovery-aware Navigation), a novel approach to autonomous navigation in scenarios where dead-end detection and recovery are critical, particularly in unstructured environments where robots must handle corners, vegetation occlusions, and blocked junctions. DR. Nav introduces a proactive strategy for navigation in unmapped environments without prior assumptions. Our method unifies dead-end prediction and recovery by generating a single, continuous, real-time semantic cost map. Specifically, DR. Nav leverages cross-modal RGB-LiDAR fusion with attention-based filtering to estimate per-cell dead-end likelihoods and recovery points, which are continuously updated through Bayesian inference to enhance robustness. Unlike prior mapping methods that only encode traversability, DR. Nav explicitly incorporates recovery-aware risk into the navigation cost map, enabling robots to anticipate unsafe regions and plan safer alternative trajectories. We evaluate DR. Nav across multiple dense indoor and outdoor scenarios and demonstrate an increase of 83.33% in accuracy in detection, a 52.4% reduction in time-to-goal (path efficiency), compared to state-of-the-art planners such as DWA, MPPI, and Nav2 DWB. Furthermore, the dead-end classifier functions", "AI": {"tldr": "DR. Nav \u662f\u4e00\u79cd\u65b0\u578b\u81ea\u4e3b\u5bfc\u822a\u65b9\u6cd5\uff0c\u7ed3\u5408\u6b7b\u80e1\u540c\u68c0\u6d4b\u4e0e\u6062\u590d\uff0c\u901a\u8fc7\u5b9e\u65f6\u8bed\u4e49\u6210\u672c\u5730\u56fe\u548c RGB-LiDAR \u878d\u5408\u6765\u63d0\u5347\u5bfc\u822a\u6027\u80fd\u3002", "motivation": "\u5728\u590d\u6742\u548c\u672a\u6620\u5c04\u7684\u73af\u5883\u4e2d\uff0c\u673a\u5668\u4eba\u9700\u8981\u9ad8\u6548\u7684\u6b7b\u80e1\u540c\u68c0\u6d4b\u4e0e\u6062\u590d\u7b56\u7565\uff0c\u4ee5\u4fdd\u8bc1\u5bfc\u822a\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "method": "DR. Nav \u7ed3\u5408 RGB-LiDAR \u878d\u5408\u4e0e\u8d1d\u53f6\u65af\u63a8\u65ad\uff0c\u751f\u6210\u5b9e\u65f6\u66f4\u65b0\u7684\u8bed\u4e49\u6210\u672c\u5730\u56fe\uff0c\u9884\u6d4b\u6b7b\u80e1\u540c\u53ca\u6062\u590d\u70b9\u3002", "result": "DR. Nav \u662f\u4e00\u79cd\u521b\u65b0\u7684\u81ea\u4e3b\u5bfc\u822a\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u6b7b\u80e1\u540c\u68c0\u6d4b\u548c\u6062\u590d\uff0c\u5c24\u5176\u9002\u5408\u590d\u6742\u73af\u5883\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u5b9e\u65f6\u8bed\u4e49\u6210\u672c\u5730\u56fe\uff0c\u7ed3\u5408 RGB-LiDAR \u878d\u5408\u6280\u672f\uff0c\u5e76\u901a\u8fc7\u8d1d\u53f6\u65af\u63a8\u65ad\u4e0d\u65ad\u66f4\u65b0\uff0c\u63d0\u9ad8\u4e86\u5bfc\u822a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "DR. Nav \u5728\u591a\u4e2a\u5ba4\u5185\u5916\u573a\u666f\u4e2d\u7684\u6d4b\u8bd5\u7ed3\u679c\u8868\u660e\uff0c\u5176\u6709\u6548\u6027\u663e\u8457\u9ad8\u4e8e\u73b0\u6709\u7684\u89c4\u5212\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u63d0\u9ad8 83.33%\uff0c\u8def\u5f84\u6548\u7387\u63d0\u9ad8 52.4%\u3002"}}
{"id": "2511.12795", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12795", "abs": "https://arxiv.org/abs/2511.12795", "authors": ["Boshu Lei", "Wen Jiang", "Kostas Daniilidis"], "title": "ActiveGrasp: Information-Guided Active Grasping with Calibrated Energy-based Model", "comment": "under review", "summary": "Grasping in a densely cluttered environment is a challenging task for robots. Previous methods tried to solve this problem by actively gathering multiple views before grasp pose generation. However, they either overlooked the importance of the grasp distribution for information gain estimation or relied on the projection of the grasp distribution, which ignores the structure of grasp poses on the SE(3) manifold. To tackle these challenges, we propose a calibrated energy-based model for grasp pose generation and an active view selection method that estimates information gain from grasp distribution. Our energy-based model captures the multi-modality nature of grasp distribution on the SE(3) manifold. The energy level is calibrated to the success rate of grasps so that the predicted distribution aligns with the real distribution. The next best view is selected by estimating the information gain for grasp from the calibrated distribution conditioned on the reconstructed environment, which could efficiently drive the robot to explore affordable parts of the target object. Experiments on simulated environments and real robot setups demonstrate that our model could successfully grasp objects in a cluttered environment with limited view budgets compared to previous state-of-the-art models. Our simulated environment can serve as a reproducible platform for future research on active grasping. The source code of our paper will be made public when the paper is released to the public.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6a21\u578b\uff0c\u901a\u8fc7\u4f18\u5316\u6293\u53d6\u59ff\u6001\u751f\u6210\u548c\u4e3b\u52a8\u89c6\u89d2\u9009\u62e9\uff0c\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u5728\u62e5\u6324\u73af\u5883\u4e2d\u6293\u53d6\u7269\u4f53\u7684\u96be\u9898\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u9488\u5bf9\u673a\u5668\u4eba\u5728\u62e5\u6324\u73af\u5883\u4e2d\u6293\u53d6\u7269\u4f53\u7684\u6311\u6218\uff0c\u6539\u5584\u4fe1\u606f\u589e\u76ca\u4f30\u8ba1\u4e0e\u6293\u53d6\u5206\u5e03\u7684\u5173\u8054\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u91cf\u57fa\u7840\u6a21\u578b\u7528\u4e8e\u6293\u53d6\u59ff\u6001\u751f\u6210\uff0c\u5e76\u901a\u8fc7\u4f30\u8ba1\u6293\u53d6\u5206\u5e03\u7684\u4fe1\u606f\u589e\u76ca\u9009\u62e9\u4e3b\u52a8\u89c6\u89d2\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u578b\u5728\u6a21\u62df\u73af\u5883\u548c\u771f\u5b9e\u673a\u5668\u4eba\u8bbe\u7f6e\u4e2d\u80fd\u591f\u9ad8\u6548\u5730\u6293\u53d6\u7269\u4f53\uff0c\u4e14\u63d0\u4f9b\u4e86\u53ef\u518d\u751f\u7684\u5b9e\u9a8c\u5e73\u53f0\u3002", "conclusion": "\u672c\u6a21\u578b\u80fd\u591f\u5728\u6709\u9650\u89c6\u89d2\u9884\u7b97\u4e0b\uff0c\u5728\u62e5\u6324\u73af\u5883\u4e2d\u6210\u529f\u6293\u53d6\u7269\u4f53\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u6a21\u578b\u3002"}}
{"id": "2511.12848", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12848", "abs": "https://arxiv.org/abs/2511.12848", "authors": ["Max M. Sun", "Todd Murphey"], "title": "Structured Imitation Learning of Interactive Policies through Inverse Games", "comment": "Presented at the \"Workshop on Generative Modeling Meets Human-Robot Interaction\" at Robotics: Science and Systems 2025. Workshop website: https://sites.google.com/view/gai-hri/", "summary": "Generative model-based imitation learning methods have recently achieved strong results in learning high-complexity motor skills from human demonstrations. However, imitation learning of interactive policies that coordinate with humans in shared spaces without explicit communication remains challenging, due to the significantly higher behavioral complexity in multi-agent interactions compared to non-interactive tasks. In this work, we introduce a structured imitation learning framework for interactive policies by combining generative single-agent policy learning with a flexible yet expressive game-theoretic structure. Our method explicitly separates learning into two steps: first, we learn individual behavioral patterns from multi-agent demonstrations using standard imitation learning; then, we structurally learn inter-agent dependencies by solving an inverse game problem. Preliminary results in a synthetic 5-agent social navigation task show that our method significantly improves non-interactive policies and performs comparably to the ground truth interactive policy using only 50 demonstrations. These results highlight the potential of structured imitation learning in interactive settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4e2a\u4f53\u884c\u4e3a\u6a21\u5f0f\u5b66\u4e60\u4e0e\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u4f9d\u8d56\u7ed3\u6784\u5b66\u4e60\u7ed3\u5408\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5728\u4ea4\u4e92\u73af\u5883\u4e0b\u7684\u7b56\u7565\u5b66\u4e60\u3002", "motivation": "\u5728\u5171\u4eab\u7a7a\u95f4\u4e2d\u4e0e\u4eba\u7c7b\u534f\u8c03\u7684\u4ea4\u4e92\u7b56\u7565\u7684\u6a21\u4eff\u5b66\u4e60\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u5728\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u4e2d\u7684\u884c\u4e3a\u590d\u6742\u6027\u9ad8\u4e8e\u975e\u4ea4\u4e92\u4efb\u52a1\u3002", "method": "\u7ed3\u5408\u751f\u6210\u5355\u667a\u80fd\u4f53\u7b56\u7565\u5b66\u4e60\u4e0e\u7075\u6d3b\u8868\u73b0\u7684\u535a\u5f08\u8bba\u7ed3\u6784\u7684\u6709\u7ed3\u6784\u7684\u6a21\u4eff\u5b66\u4e60\u6846\u67b6", "result": "\u5728\u4e00\u4e2a\u5408\u6210\u76845\u667a\u80fd\u4f53\u793e\u4ea4\u5bfc\u822a\u4efb\u52a1\u4e2d\uff0c\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u975e\u4ea4\u4e92\u7b56\u7565\uff0c\u5e76\u4e14\u5728\u4ec5\u4f7f\u752850\u4e2a\u6f14\u793a\u7684\u60c5\u51b5\u4e0b\uff0c\u8868\u73b0\u4e0e\u771f\u5b9e\u7684\u4ea4\u4e92\u7b56\u7565\u76f8\u5f53\u3002", "conclusion": "\u7ed3\u6784\u5316\u6a21\u4eff\u5b66\u4e60\u5728\u4ea4\u4e92\u73af\u5883\u4e0b\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u80fd\u591f\u663e\u8457\u6539\u5584\u7b56\u7565\u5b66\u4e60\u7684\u6548\u679c\u3002"}}
{"id": "2511.12882", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12882", "abs": "https://arxiv.org/abs/2511.12882", "authors": ["Taiyi Su", "Jian Zhu", "Yaxuan Li", "Chong Ma", "Zitai Huang", "Yichen Zhu", "Hanli Wang", "Yi Xu"], "title": "Towards High-Consistency Embodied World Model with Multi-View Trajectory Videos", "comment": "11 pages, 5 figures", "summary": "Embodied world models aim to predict and interact with the physical world through visual observations and actions. However, existing models struggle to accurately translate low-level actions (e.g., joint positions) into precise robotic movements in predicted frames, leading to inconsistencies with real-world physical interactions. To address these limitations, we propose MTV-World, an embodied world model that introduces Multi-view Trajectory-Video control for precise visuomotor prediction. Specifically, instead of directly using low-level actions for control, we employ trajectory videos obtained through camera intrinsic and extrinsic parameters and Cartesian-space transformation as control signals. However, projecting 3D raw actions onto 2D images inevitably causes a loss of spatial information, making a single view insufficient for accurate interaction modeling. To overcome this, we introduce a multi-view framework that compensates for spatial information loss and ensures high-consistency with physical world. MTV-World forecasts future frames based on multi-view trajectory videos as input and conditioning on an initial frame per view. Furthermore, to systematically evaluate both robotic motion precision and object interaction accuracy, we develop an auto-evaluation pipeline leveraging multimodal large models and referring video object segmentation models. To measure spatial consistency, we formulate it as an object location matching problem and adopt the Jaccard Index as the evaluation metric. Extensive experiments demonstrate that MTV-World achieves precise control execution and accurate physical interaction modeling in complex dual-arm scenarios.", "AI": {"tldr": "MTV-World\u901a\u8fc7\u591a\u89c6\u89d2\u8f68\u8ff9\u89c6\u9891\u63a7\u5236\u4f18\u5316\u673a\u5668\u4eba\u5728\u7269\u7406\u4e16\u754c\u4e2d\u7684\u89c6\u89c9\u8fd0\u52a8\u9884\u6d4b\uff0c\u63d0\u9ad8\u4e86\u4ea4\u4e92\u7684\u4e00\u81f4\u6027\u548c\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u8eab\u4f53\u4e16\u754c\u6a21\u578b\u5728\u5c06\u4f4e\u7ea7\u52a8\u4f5c\u8f6c\u5316\u4e3a\u673a\u5668\u4eba\u8fd0\u52a8\u65f6\u5b58\u5728\u4e0d\u4e00\u81f4\uff0c\u65e8\u5728\u901a\u8fc7MTV-World\u89e3\u51b3\u7cbe\u786e\u4ea4\u4e92\u4e2d\u7684\u9650\u5236\u3002", "method": "\u8be5\u65b9\u6cd5\u4f7f\u7528\u901a\u8fc7\u76f8\u673a\u5185\u5916\u53c2\u6570\u548c\u7b1b\u5361\u5c14\u7a7a\u95f4\u53d8\u6362\u83b7\u5f97\u7684\u8f68\u8ff9\u89c6\u9891\u4f5c\u4e3a\u63a7\u5236\u4fe1\u53f7\uff0c\u91c7\u7528\u591a\u89c6\u56fe\u6846\u67b6\u6765\u5f25\u8865\u7a7a\u95f4\u4fe1\u606f\u635f\u5931\uff0c\u786e\u4fdd\u4e0e\u7269\u7406\u4e16\u754c\u7684\u9ad8\u5ea6\u4e00\u81f4\u6027\u3002", "result": "MTV-World\u6a21\u578b\u5f15\u5165\u4e86\u591a\u89c6\u89d2\u8f68\u8ff9\u89c6\u9891\u63a7\u5236\uff0c\u6539\u8fdb\u4e86\u8eab\u4f53\u4e16\u754c\u6a21\u578b\u5728\u7cbe\u786e\u89c6\u89c9\u8fd0\u52a8\u9884\u6d4b\u548c\u7269\u7406\u4ea4\u4e92\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u7684\u53cc\u81c2\u673a\u5668\u4eba\u573a\u666f\u4e2d\uff0c\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u7684\u63a7\u5236\u6267\u884c\u548c\u4ea4\u4e92\u5efa\u6a21\u80fd\u529b\u3002", "conclusion": "MTV-World\u5728\u590d\u6742\u7684\u673a\u5668\u4eba\u7684\u53cc\u81c2\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u63a7\u5236\u548c\u7269\u7406\u4ea4\u4e92\u5efa\u6a21\u3002"}}
{"id": "2511.12896", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12896", "abs": "https://arxiv.org/abs/2511.12896", "authors": ["Jun Huo", "Hongge Ru", "Bo Yang", "Xingjian Chen", "Xi Li", "Jian Huang"], "title": "Air-Chamber Based Soft Six-Axis Force/Torque Sensor for Human-Robot Interaction", "comment": null, "summary": "Soft multi-axis force/torque sensors provide safe and precise force interaction. Capturing the complete degree-of-freedom of force is imperative for accurate force measurement with six-axis force/torque sensors. However, cross-axis coupling can lead to calibration issues and decreased accuracy. In this instance, developing a soft and accurate six-axis sensor is a challenging task. In this paper, a soft air-chamber type six-axis force/torque sensor with 16-channel barometers is introduced, which housed in hyper-elastic air chambers made of silicone rubber. Additionally, an effective decoupling method is proposed, based on a rigid-soft hierarchical structure, which reduces the six-axis decoupling problem to two three-axis decoupling problems. Finite element model simulation and experiments demonstrate the compatibility of the proposed approach with reality. The prototype's sensing performance is quantitatively measured in terms of static load response, dynamic load response and dynamic response characteristic. It possesses a measuring range of 50 N force and 1 Nm torque, and the average deviation, repeatability, non-linearity and hysteresis are 4.9$\\%$, 2.7$\\%$, 5.8$\\%$ and 6.7$\\%$, respectively. The results indicate that the prototype exhibits satisfactory sensing performance while maintaining its softness due to the presence of soft air chambers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f6f\u6c14\u5ba4\u516d\u8f74\u529b/\u626d\u77e9\u4f20\u611f\u5668\uff0c\u901a\u8fc7\u6709\u6548\u89e3\u8026\u65b9\u6cd5\u89e3\u51b3\u4e86\u516d\u8f74\u8026\u5408\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u4fdd\u6301\u67d4\u6027\u7684\u540c\u65f6\u5177\u6709\u826f\u597d\u7684\u4f20\u611f\u6027\u80fd\u3002", "motivation": "\u57fa\u4e8e\u8f6f\u591a\u8f74\u529b/\u626d\u77e9\u4f20\u611f\u5668\u7684\u5e94\u7528\u9700\u6c42\uff0c\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5b9e\u73b0\u51c6\u786e\u516d\u8f74\u529b\u6d4b\u91cf\u7684\u4f20\u611f\u5668\uff0c\u514b\u670d\u4ea4\u53c9\u8f74\u8026\u5408\u5e26\u6765\u7684\u6821\u51c6\u95ee\u9898\u548c\u7cbe\u5ea6\u4e0b\u964d\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u5e26\u670916\u901a\u9053\u6c14\u538b\u8ba1\u7684\u8f6f\u6c14\u5ba4\u516d\u8f74\u529b/\u626d\u77e9\u4f20\u611f\u5668\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u521a\u67d4\u5206\u7ea7\u7ed3\u6784\u7684\u6709\u6548\u89e3\u8026\u65b9\u6cd5\u3002", "result": "\u539f\u578b\u4f20\u611f\u5668\u7684\u9759\u6001\u8d1f\u8f7d\u54cd\u5e94\u3001\u52a8\u6001\u8d1f\u8f7d\u54cd\u5e94\u548c\u52a8\u6001\u54cd\u5e94\u7279\u6027\u7ecf\u8fc7\u91cf\u5316\u6d4b\u91cf\uff0c\u5177\u670950 N\u7684\u6d4b\u91cf\u8303\u56f4\u548c1 Nm\u7684\u626d\u77e9\uff0c\u5e73\u5747\u504f\u5dee\u3001\u91cd\u590d\u6027\u3001\u4e0d\u7ebf\u6027\u548c\u6ede\u540e\u5206\u522b\u4e3a4.9%\u30012.7%\u30015.8%\u548c6.7%\u3002", "conclusion": "\u539f\u578b\u4f20\u611f\u5668\u5728\u4fdd\u6301\u8f6f\u6027\u7684\u540c\u65f6\u8868\u73b0\u51fa\u4ee4\u4eba\u6ee1\u610f\u7684\u4f20\u611f\u6027\u80fd\u3002"}}
{"id": "2511.12910", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12910", "abs": "https://arxiv.org/abs/2511.12910", "authors": ["Yong Li", "Yujun Huang", "Yi Chen", "Hui Cheng"], "title": "TOPP-DWR: Time-Optimal Path Parameterization of Differential-Driven Wheeled Robots Considering Piecewise-Constant Angular Velocity Constraints", "comment": null, "summary": "Differential-driven wheeled robots (DWR) represent the quintessential type of mobile robots and find extensive appli- cations across the robotic field. Most high-performance control approaches for DWR explicitly utilize the linear and angular velocities of the trajectory as control references. However, existing research on time-optimal path parameterization (TOPP) for mobile robots usually neglects the angular velocity and joint vel- ocity constraints, which can result in degraded control perfor- mance in practical applications. In this article, a systematic and practical TOPP algorithm named TOPP-DWR is proposed for DWR and other mobile robots. First, the non-uniform B-spline is adopted to represent the initial trajectory in the task space. Second, the piecewise-constant angular velocity, as well as joint velocity, linear velocity, and linear acceleration constraints, are incorporated into the TOPP problem. During the construction of the optimization problem, the aforementioned constraints are uniformly represented as linear velocity constraints. To boost the numerical computational efficiency, we introduce a slack variable to reformulate the problem into second-order-cone programming (SOCP). Subsequently, comparative experiments are conducted to validate the superiority of the proposed method. Quantitative performance indexes show that TOPP-DWR achieves TOPP while adhering to all constraints. Finally, field autonomous navigation experiments are carried out to validate the practicability of TOPP-DWR in real-world applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTOPP-DWR\u7684\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u79fb\u52a8\u673a\u5668\u4eba\u8def\u5f84\u53c2\u6570\u5316\u4e2d\u7684\u7ea6\u675f\u95ee\u9898\uff0c\u5b9e\u73b0\u65f6\u95f4\u6700\u4f18\u8def\u5f84\uff0c\u540c\u65f6\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u79fb\u52a8\u673a\u5668\u4eba\u4efb\u52a1\u65f6\u95f4\u6700\u4f18\u8def\u5f84\u53c2\u6570\u5316\u7814\u7a76\u901a\u5e38\u5ffd\u89c6\u4e86\u89d2\u901f\u5ea6\u548c\u5173\u8282\u901f\u5ea6\u7ea6\u675f\uff0c\u8fd9\u4f1a\u964d\u4f4e\u63a7\u5236\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u7cfb\u7edf\u7684\u3001\u5b9e\u7528\u7684\u7b97\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u975e\u5747\u5300B\u6837\u6761\u8868\u793a\u521d\u59cb\u8f68\u8ff9\uff0c\u6574\u5408\u89d2\u901f\u5ea6\u3001\u5173\u8282\u901f\u5ea6\u3001\u7ebf\u901f\u5ea6\u548c\u7ebf\u52a0\u901f\u5ea6\u7ea6\u675f\uff0c\u5e76\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u4e8c\u9636\u9525\u89c4\u5212(SOCP)\uff0c\u4ee5\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u901a\u8fc7\u6bd4\u8f83\u5b9e\u9a8c\u548c\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u5b9e\u9a8c\uff0c\u8868\u660eTOPP-DWR\u76f8\u6bd4\u4e8e\u4f20\u7edf\u65b9\u6cd5\u5177\u6709\u66f4\u5f3a\u7684\u6027\u80fd\uff0c\u80fd\u5728\u6ee1\u8db3\u7ea6\u675f\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u65f6\u95f4\u6700\u4f18\u8def\u5f84\u3002", "conclusion": "TOPP-DWR \u7b97\u6cd5\u5728\u9075\u5faa\u6240\u6709\u7ea6\u675f\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u65f6\u95f4\u6700\u4f18\u8def\u5f84\u53c2\u6570\u5316\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2511.12912", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12912", "abs": "https://arxiv.org/abs/2511.12912", "authors": ["Yingting Zhou", "Wenbo Cui", "Weiheng Liu", "Guixing Chen", "Haoran Li", "Dongbin Zhao"], "title": "DiffuDepGrasp: Diffusion-based Depth Noise Modeling Empowers Sim2Real Robotic Grasping", "comment": null, "summary": "Transferring the depth-based end-to-end policy trained in simulation to physical robots can yield an efficient and robust grasping policy, yet sensor artifacts in real depth maps like voids and noise establish a significant sim2real gap that critically impedes policy transfer. Training-time strategies like procedural noise injection or learned mappings suffer from data inefficiency due to unrealistic noise simulation, which is often ineffective for grasping tasks that require fine manipulation or dependency on paired datasets heavily. Furthermore, leveraging foundation models to reduce the sim2real gap via intermediate representations fails to mitigate the domain shift fully and adds computational overhead during deployment. This work confronts dual challenges of data inefficiency and deployment complexity. We propose DiffuDepGrasp, a deploy-efficient sim2real framework enabling zero-shot transfer through simulation-exclusive policy training. Its core innovation, the Diffusion Depth Generator, synthesizes geometrically pristine simulation depth with learned sensor-realistic noise via two synergistic modules. The first Diffusion Depth Module leverages temporal geometric priors to enable sample-efficient training of a conditional diffusion model that captures complex sensor noise distributions, while the second Noise Grafting Module preserves metric accuracy during perceptual artifact injection. With only raw depth inputs during deployment, DiffuDepGrasp eliminates computational overhead and achieves a 95.7% average success rate on 12-object grasping with zero-shot transfer and strong generalization to unseen objects.Project website: https://diffudepgrasp.github.io/.", "AI": {"tldr": "DiffuDepGrasp\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u4eff\u771f\u8bad\u7ec3\u6765\u96f6-shot \u8f6c\u79fb\u81f3\u7269\u7406\u673a\u5668\u4eba\u7684\u6293\u53d6\u7b56\u7565\uff0c\u514b\u670d\u6570\u636e\u6548\u7387\u4f4e\u4e0b\u548c\u90e8\u7f72\u590d\u6742\u6027\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5728\u5c06\u6df1\u5ea6\u57fa\u7840\u7684\u7b56\u7565\u4ece\u6a21\u62df\u8f6c\u79fb\u5230\u7269\u7406\u673a\u5668\u4eba\u4e2d\u9762\u4e34\u7684\u91cd\u5927sim2real\u5dee\u8ddd\u548c\u6570\u636e\u6548\u7387\u53ca\u90e8\u7f72\u590d\u6742\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faDiffuDepGrasp\u6846\u67b6\uff0c\u5305\u62ecDiffusion Depth Generator\u7684\u4e24\u4e2a\u6a21\u5757\uff0c\u5206\u522b\u662f\u80fd\u6355\u6349\u590d\u6742\u4f20\u611f\u5668\u566a\u58f0\u5206\u5e03\u7684Diffusion Depth Module\u548c\u4fdd\u6301\u5ea6\u91cf\u51c6\u786e\u6027\u7684Noise Grafting Module\u3002", "result": "DiffuDepGrasp\u662f\u4e00\u79cd\u6709\u6548\u4e14\u9c81\u68d2\u7684\u6293\u53d6\u7b56\u7565\uff0c\u4f7f\u5f97\u5728\u6ca1\u6709\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5c06\u6a21\u62df\u4e2d\u7684\u7b56\u7565\u8f6c\u79fb\u5230\u7269\u7406\u673a\u5668\u4eba\u4e0a\u6210\u4e3a\u53ef\u80fd\uff0c\u901a\u8fc7\u5c06\u51e0\u4f55\u4e0a\u5b8c\u7f8e\u7684\u6a21\u62df\u6df1\u5ea6\u4e0e\u5b66\u4e60\u5f97\u5230\u7684\u4f20\u611f\u5668\u771f\u5b9e\u566a\u58f0\u7ed3\u5408\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u6570\u636e\u6548\u7387\u548c\u90e8\u7f72\u590d\u6742\u6027\u4e24\u5927\u6311\u6218\u3002", "conclusion": "DiffuDepGrasp\u67b6\u6784\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u96f6-shot \u7b56\u7565\u4f20\u9012\uff0c\u5177\u6709\u663e\u8457\u7684\u6210\u529f\u7387\u548c\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u591f\u5728\u90e8\u7f72\u65f6\u4ec5\u4f7f\u7528\u539f\u59cb\u6df1\u5ea6\u8f93\u5165\uff0c\u6d88\u9664\u8ba1\u7b97\u5f00\u9500\u3002"}}
{"id": "2511.12941", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12941", "abs": "https://arxiv.org/abs/2511.12941", "authors": ["Chunyong Hu", "Qi Luo", "Jianyun Xu", "Song Wang", "Qiang Li", "Sheng Yang"], "title": "GUIDE: Gaussian Unified Instance Detection for Enhanced Obstacle Perception in Autonomous Driving", "comment": null, "summary": "In the realm of autonomous driving, accurately detecting surrounding obstacles is crucial for effective decision-making. Traditional methods primarily rely on 3D bounding boxes to represent these obstacles, which often fail to capture the complexity of irregularly shaped, real-world objects. To overcome these limitations, we present GUIDE, a novel framework that utilizes 3D Gaussians for instance detection and occupancy prediction. Unlike conventional occupancy prediction methods, GUIDE also offers robust tracking capabilities. Our framework employs a sparse representation strategy, using Gaussian-to-Voxel Splatting to provide fine-grained, instance-level occupancy data without the computational demands associated with dense voxel grids. Experimental validation on the nuScenes dataset demonstrates GUIDE's performance, with an instance occupancy mAP of 21.61, marking a 50\\% improvement over existing methods, alongside competitive tracking capabilities. GUIDE establishes a new benchmark in autonomous perception systems, effectively combining precision with computational efficiency to better address the complexities of real-world driving environments.", "AI": {"tldr": "GUIDE\u662f\u4e00\u4e2a\u65b0\u7684\u81ea\u4e3b\u9a7e\u9a76\u6846\u67b6\uff0c\u4f7f\u75283D\u9ad8\u65af\u65b9\u6cd5\u8fdb\u884c\u969c\u788d\u7269\u68c0\u6d4b\u548c\u5360\u7528\u9884\u6d4b\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406\u4e0d\u89c4\u5219\u5f62\u72b6\u7684\u969c\u788d\u7269\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u68c0\u6d4b\u548c\u8ffd\u8e2a\u80fd\u529b\u3002", "method": "GUIDE\u6846\u67b6\u5229\u75283D\u9ad8\u65af\u8fdb\u884c\u5b9e\u4f8b\u68c0\u6d4b\u548c\u5360\u7528\u9884\u6d4b\uff0c\u91c7\u7528\u7a00\u758f\u8868\u793a\u7b56\u7565\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u573a\u666f\u7406\u89e3\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\uff0cGUIDE\u7684\u5b9e\u4f8b\u5360\u7528mAP\u8fbe\u523021.61\uff0c\u6027\u80fd\u63d0\u534750%\uff0c\u4e14\u8ffd\u8e2a\u80fd\u529b\u5177\u7ade\u4e89\u529b\u3002", "conclusion": "GUIDE\u4e3a\u81ea\u4e3b\u611f\u77e5\u7cfb\u7edf\u8bbe\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u5904\u7406\u590d\u6742\u7684\u771f\u5b9e\u4e16\u754c\u9a7e\u9a76\u73af\u5883\u3002"}}
{"id": "2511.12972", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12972", "abs": "https://arxiv.org/abs/2511.12972", "authors": ["Siddarth Narasimhan", "Matthew Lisondra", "Haitong Wang", "Goldie Nejat"], "title": "SplatSearch: Instance Image Goal Navigation for Mobile Robots using 3D Gaussian Splatting and Diffusion Models", "comment": "Project Page: https://splat-search.github.io/", "summary": "The Instance Image Goal Navigation (IIN) problem requires mobile robots deployed in unknown environments to search for specific objects or people of interest using only a single reference goal image of the target. This problem can be especially challenging when: 1) the reference image is captured from an arbitrary viewpoint, and 2) the robot must operate with sparse-view scene reconstructions. In this paper, we address the IIN problem, by introducing SplatSearch, a novel architecture that leverages sparse-view 3D Gaussian Splatting (3DGS) reconstructions. SplatSearch renders multiple viewpoints around candidate objects using a sparse online 3DGS map, and uses a multi-view diffusion model to complete missing regions of the rendered images, enabling robust feature matching against the goal image. A novel frontier exploration policy is introduced which uses visual context from the synthesized viewpoints with semantic context from the goal image to evaluate frontier locations, allowing the robot to prioritize frontiers that are semantically and visually relevant to the goal image. Extensive experiments in photorealistic home and real-world environments validate the higher performance of SplatSearch against current state-of-the-art methods in terms of Success Rate and Success Path Length. An ablation study confirms the design choices of SplatSearch.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSplatSearch\uff0c\u89e3\u51b3\u79fb\u52a8\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u4ec5\u4f9d\u8d56\u5355\u4e2a\u53c2\u8003\u76ee\u6807\u56fe\u50cf\u8fdb\u884c\u5bfc\u822a\u7684\u6311\u6218\uff0c\u901a\u8fc7\u7a00\u758f\u89c6\u56fe3D\u91cd\u5efa\u548c\u591a\u89c6\u89d2\u6269\u6563\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u79fb\u52a8\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u4ec5\u9760\u5355\u4e2a\u53c2\u8003\u76ee\u6807\u56fe\u50cf\u641c\u7d22\u7279\u5b9a\u7269\u4f53\u6216\u4eba\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u53c2\u8003\u56fe\u50cf\u53d6\u81ea\u4efb\u610f\u89c6\u89d2\u4e14\u4f7f\u7528\u7a00\u758f\u89c6\u56fe\u573a\u666f\u91cd\u5efa\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u67b6\u6784SplatSearch\uff0c\u7ed3\u5408\u7a00\u758f\u89c6\u56fe\u76843D\u9ad8\u65af\u70b9\u4e91\u91cd\u5efa\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u6269\u6563\u6a21\u578b\u5b8c\u6210\u56fe\u50cf\u7f3a\u5931\u533a\u57df\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u578b\u524d\u6cbf\u63a2\u7d22\u7b56\u7565\u3002", "result": "\u901a\u8fc7SplatSearch\u67b6\u6784\uff0c\u673a\u5668\u4eba\u80fd\u591f\u66f4\u6709\u6548\u5730\u7ed3\u5408\u89c6\u89c9\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u4f18\u5148\u63a2\u7d22\u4e0e\u76ee\u6807\u56fe\u50cf\u76f8\u5173\u7684\u524d\u6cbf\u4f4d\u7f6e\uff0c\u63d0\u5347\u4e86\u641c\u7d22\u4efb\u52a1\u7684\u6210\u529f\u7387\u548c\u6548\u7387\u3002", "conclusion": "SplatSearch\u5728\u6210\u529f\u7387\u548c\u6210\u529f\u8def\u5f84\u957f\u5ea6\u4e0a\u4f18\u4e8e\u5f53\u524d\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2511.12984", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12984", "abs": "https://arxiv.org/abs/2511.12984", "authors": ["Miryeong Park", "Dongjin Cho", "Sanghyun Kim", "Younggun Cho"], "title": "CUTE-Planner: Confidence-aware Uneven Terrain Exploration Planner", "comment": "Accepted in International Conference on Space Robotics 2025", "summary": "Planetary exploration robots must navigate uneven terrain while building reliable maps for space missions. However, most existing methods incorporate traversability constraints but may not handle high uncertainty in elevation estimates near complex features like craters, do not consider exploration strategies for uncertainty reduction, and typically fail to address how elevation uncertainty affects navigation safety and map quality. To address the problems, we propose a framework integrating safe path generation, adaptive confidence updates, and confidence-aware exploration strategies. Using Kalman-based elevation estimation, our approach generates terrain traversability and confidence scores, then incorporates them into Graph-Based exploration Planner (GBP) to prioritize exploration of traversable low-confidence regions. We evaluate our framework through simulated lunar experiments using a novel low-confidence region ratio metric, achieving 69% uncertainty reduction compared to baseline GBP. In terms of mission success rate, our method achieves 100% while baseline GBP achieves 0%, demonstrating improvements in exploration safety and map reliability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u6765\u89e3\u51b3\u884c\u661f\u63a2\u6d4b\u673a\u5668\u4eba\u7684\u9ad8\u5ea6\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u4efb\u52a1\u6210\u529f\u7387\u548c\u4e0d\u786e\u5b9a\u6027\u51cf\u5c11\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u5904\u7406\u590d\u6742\u7279\u5f81\u9644\u8fd1\u7684\u9ad8\u5ea6\u4e0d\u786e\u5b9a\u6027\uff0c\u4e14\u672a\u8003\u8651\u964d\u4f4e\u4e0d\u786e\u5b9a\u6027\u7684\u63a2\u7d22\u7b56\u7565\uff0c\u5f71\u54cd\u5bfc\u822a\u5b89\u5168\u6027\u548c\u5730\u56fe\u8d28\u91cf", "method": "\u63d0\u51fa\u4e00\u4e2a\u96c6\u6210\u5b89\u5168\u8def\u5f84\u751f\u6210\u3001\u81ea\u9002\u5e94\u7f6e\u4fe1\u5ea6\u66f4\u65b0\u548c\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u63a2\u7d22\u7b56\u7565\u7684\u6846\u67b6", "result": "\u901a\u8fc7\u6a21\u62df\u6708\u7403\u5b9e\u9a8c\uff0c\u4f7f\u7528\u65b0\u9896\u7684\u4f4e\u7f6e\u4fe1\u533a\u57df\u6bd4\u4f8b\u6307\u6807\uff0c\u53d6\u5f9769%\u7684\u4e0d\u786e\u5b9a\u6027\u51cf\u5c11\uff0c\u4efb\u52a1\u6210\u529f\u7387\u8fbe\u5230100%\uff1b\u800c\u57fa\u7ebfGBP\u4e3a0%\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u5728\u63a2\u7d22\u5b89\u5168\u6027\u548c\u5730\u56fe\u53ef\u9760\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6539\u5584\uff0c\u9002\u7528\u4e8e\u672a\u6765\u7684\u822a\u5929\u4efb\u52a1\u3002"}}
{"id": "2511.13042", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13042", "abs": "https://arxiv.org/abs/2511.13042", "authors": ["Yong Li", "Hui Cheng"], "title": "APP: A* Post-Processing Algorithm for Robots with Bidirectional Shortcut and Path Perturbation", "comment": null, "summary": "Paths generated by A* and other graph-search-based planners are widely used in the robotic field. Due to the restricted node-expansion directions, the resulting paths are usually not the shortest. Besides, unnecessary heading changes, or zig-zag patterns, exist even when no obstacle is nearby, which is inconsistent with the human intuition that the path segments should be straight in wide-open space due to the absence of obstacles. This article puts forward a general and systematic post-processing algorithm for A* and other graph-search-based planners. The A* post-processing algorithm, called APP, is developed based on the costmap, which is widely used in commercial service robots. First, a bidirectional vertices reduction algorithm is proposed to tackle the asymm- etry of the path and the environments. During the forward and backward vertices reduction, a thorough shortcut strategy is put forward to improve the path-shortening performance and avoid unnecessary heading changes. Second, an iterative path perturbation algorithm is adopted to locally reduce the number of unnecessary heading changes and improve the path smooth- ness. Comparative experiments are then carried out to validate the superiority of the proposed method. Quantitative performance indexes show that APP outperforms the existing methods in planning time, path length as well as the number of unnecessary heading changes. Finally, field navigation experiments are carried out to verify the practicability of APP.", "AI": {"tldr": "\u4e3aA*\u53ca\u5176\u4ed6\u56fe\u641c\u7d22\u89c4\u5212\u5668\u63d0\u51fa\u4e86APP\u540e\u5904\u7406\u7b97\u6cd5\uff0c\u663e\u8457\u4f18\u5316\u4e86\u8def\u5f84\u89c4\u5212\u6548\u679c", "motivation": "\u73b0\u6709\u7684A*\u53ca\u56fe\u641c\u7d22\u89c4\u5212\u5668\u751f\u6210\u7684\u8def\u5f84\u901a\u5e38\u4e0d\u591f\u77ed\uff0c\u4e14\u5b58\u5728\u4e0d\u5fc5\u8981\u7684\u8f6c\u5411\uff0c\u7f3a\u4e4f\u4eba\u7c7b\u76f4\u89c9\u4e2d\u7684\u76f4\u7ebf\u8def\u5f84\u7279\u5f81", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u540e\u5904\u7406\u7b97\u6cd5APP\uff0c\u57fa\u4e8e\u6210\u672c\u5730\u56fe\uff0c\u7ed3\u5408\u53cc\u5411\u9876\u70b9\u51cf\u5c11\u548c\u8fed\u4ee3\u8def\u5f84\u6270\u52a8\u7b97\u6cd5", "result": "APP\u5728\u89c4\u5212\u65f6\u95f4\u3001\u8def\u5f84\u957f\u5ea6\u548c\u4e0d\u5fc5\u8981\u7684\u8f6c\u5411\u6b21\u6570\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u5730\u5bfc\u822a\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5b9e\u7528\u6027", "conclusion": "APP\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u51cf\u5c11\u8def\u5f84\u957f\u5ea6\u548c\u4e0d\u5fc5\u8981\u7684\u8f6c\u5411\uff0c\u63d0\u5347\u8def\u5f84\u5e73\u6ed1\u6027\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6548\u679c"}}
{"id": "2511.13048", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13048", "abs": "https://arxiv.org/abs/2511.13048", "authors": ["Yong Li", "Hui Cheng"], "title": "Unidirectional-Road-Network-Based Global Path Planning for Cleaning Robots in Semi-Structured Environments", "comment": "2023 IEEE International Conference on Robotics and Automation (ICRA)", "summary": "Practical global path planning is critical for commercializing cleaning robots working in semi-structured environments. In the literature, global path planning methods for free space usually focus on path length and neglect the traffic rule constraints of the environments, which leads to high-frequency re-planning and increases collision risks. In contrast, those for structured environments are developed mainly by strictly complying with the road network representing the traffic rule constraints, which may result in an overlong path that hinders the overall navigation efficiency. This article proposes a general and systematic approach to improve global path planning performance in semi-structured environments. A unidirectional road network is built to represent the traffic constraints in semi-structured environments and a hybrid strategy is proposed to achieve a guaranteed planning result.Cutting across the road at the starting and the goal points are allowed to achieve a shorter path. Especially, a two-layer potential map is proposed to achieve a guaranteed performance when the starting and the goal points are in complex intersections. Comparative experiments are carried out to validate the effectiveness of the proposed method. Quantitative experimental results show that, compared with the state-of-art, the proposed method guarantees a much better balance between path length and the consistency with the road network.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u534a\u7ed3\u6784\u5316\u73af\u5883\u7684\u5168\u7403\u8def\u5f84\u89c4\u5212\u65b0\u65b9\u6cd5\uff0c\u5e73\u8861\u8def\u5f84\u957f\u5ea6\u4e0e\u4ea4\u901a\u89c4\u5219\uff0c\u63d0\u9ad8\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8def\u5f84\u957f\u5ea6\u800c\u5ffd\u89c6\u4ea4\u901a\u89c4\u5219\uff0c\u5bfc\u81f4\u9ad8\u9891\u91cd\u89c4\u5212\u548c\u78b0\u649e\u98ce\u9669\uff0c\u8feb\u5207\u9700\u8981\u6539\u8fdb\u3002", "method": "\u6784\u5efa\u5355\u5411\u9053\u8def\u7f51\u7edc\u8868\u793a\u4ea4\u901a\u7ea6\u675f\uff0c\u901a\u8fc7\u6df7\u5408\u7b56\u7565\u548c\u53cc\u5c42\u52bf\u80fd\u56fe\u6765\u4fdd\u8bc1\u89c4\u5212\u7ed3\u679c\uff0c\u5b9e\u73b0\u8def\u5f84\u4f18\u5316\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u534a\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u6539\u8fdb\u5168\u7403\u8def\u5f84\u89c4\u5212\u6027\u80fd\u7684\u7cfb\u7edf\u65b9\u6cd5\u3002\u901a\u8fc7\u6784\u5efa\u5355\u5411\u9053\u8def\u7f51\u7edc\u8868\u793a\u4ea4\u901a\u7ea6\u675f\uff0c\u5e76\u91c7\u7528\u6df7\u5408\u7b56\u7565\u5b9e\u73b0\u89c4\u5212\u7ed3\u679c\u7684\u4fdd\u8bc1\u3002", "conclusion": "\u6bd4\u8f83\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u8def\u5f84\u957f\u5ea6\u4e0e\u9053\u8def\u7f51\u7edc\u4e00\u81f4\u6027\u4e4b\u95f4\u5177\u6709\u66f4\u597d\u7684\u5e73\u8861\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2511.13071", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13071", "abs": "https://arxiv.org/abs/2511.13071", "authors": ["Michal Levin", "Itzik Klein"], "title": "Orientation-Free Neural Network-Based Bias Estimation for Low-Cost Stationary Accelerometers", "comment": "22 pages, 10 figures", "summary": "Low-cost micro-electromechanical accelerometers are widely used in navigation, robotics, and consumer devices for motion sensing and position estimation. However, their performance is often degraded by bias errors. To eliminate deterministic bias terms a calibration procedure is applied under stationary conditions. It requires accelerom- eter leveling or complex orientation-dependent calibration procedures. To overcome those requirements, in this paper we present a model-free learning-based calibration method that estimates accelerometer bias under stationary conditions, without requiring knowledge of the sensor orientation and without the need to rotate the sensors. The proposed approach provides a fast, practical, and scalable solution suitable for rapid field deployment. Experimental validation on a 13.39-hour dataset collected from six accelerometers shows that the proposed method consistently achieves error levels more than 52% lower than traditional techniques. On a broader scale, this work contributes to the advancement of accurate calibration methods in orientation-free scenarios. As a consequence, it improves the reliability of low-cost inertial sensors in diverse scientific and industrial applications and eliminates the need for leveled calibration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u4f20\u611f\u5668\u65b9\u5411\u548c\u65cb\u8f6c\u7684\u5b66\u4e60\u578b\u6821\u51c6\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u4f4e\u6210\u672c\u52a0\u901f\u5ea6\u8ba1\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5feb\u901f\u73b0\u573a\u90e8\u7f72\u3002", "motivation": "\u4f4e\u6210\u672c\u5fae\u7535\u673a\u68b0\u52a0\u901f\u5ea6\u8ba1\u5728\u5bfc\u822a\u3001\u673a\u5668\u4eba\u548c\u6d88\u8d39\u8bbe\u5907\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u6027\u80fd\u5e38\u56e0\u504f\u5dee\u8bef\u5dee\u800c\u964d\u4f4e\uff0c\u4f20\u7edf\u6821\u51c6\u65b9\u6cd5\u5f80\u5f80\u8981\u6c42\u7279\u6b8a\u7684\u6761\u4ef6\u3002", "method": "\u4e00\u79cd\u57fa\u4e8e\u65e0\u6a21\u578b\u5b66\u4e60\u7684\u6821\u51c6\u65b9\u6cd5\uff0c\u65e0\u9700\u4f20\u611f\u5668\u65b9\u5411\u77e5\u8bc6\u6216\u65cb\u8f6c\u4f20\u611f\u5668\u5373\u53ef\u5728\u9759\u6001\u6761\u4ef6\u4e0b\u4f30\u8ba1\u52a0\u901f\u5ea6\u8ba1\u504f\u5dee\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u516d\u4e2a\u52a0\u901f\u5ea6\u8ba1\u6536\u96c6\u768413.39\u5c0f\u65f6\u6570\u636e\u96c6\u4e0a\uff0c\u8bef\u5dee\u6c34\u5e73\u6bd4\u4f20\u7edf\u6280\u672f\u4f4e52%\u4ee5\u4e0a\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u6210\u672c\u60ef\u6027\u4f20\u611f\u5668\u7684\u53ef\u9760\u6027\uff0c\u5e76\u6d88\u9664\u4e86\u5bf9\u6c34\u5e73\u6821\u51c6\u7684\u9700\u6c42\u3002"}}
{"id": "2511.13096", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13096", "abs": "https://arxiv.org/abs/2511.13096", "authors": ["Guy Damari", "Itzik Klein"], "title": "ResAlignNet: A Data-Driven Approach for INS/DVL Alignment", "comment": null, "summary": "Autonomous underwater vehicles rely on precise navigation systems that combine the inertial navigation system and the Doppler velocity log for successful missions in challenging environments where satellite navigation is unavailable. The effectiveness of this integration critically depends on accurate alignment between the sensor reference frames. Standard model-based alignment methods between these sensor systems suffer from lengthy convergence times, dependence on prescribed motion patterns, and reliance on external aiding sensors, significantly limiting operational flexibility. To address these limitations, this paper presents ResAlignNet, a data-driven approach using the 1D ResNet-18 architecture that transforms the alignment problem into deep neural network optimization, operating as an in-situ solution that requires only sensors on board without external positioning aids or complex vehicle maneuvers, while achieving rapid convergence in seconds. Additionally, the approach demonstrates the learning capabilities of Sim2Real transfer, enabling training in synthetic data while deploying in operational sensor measurements. Experimental validation using the Snapir autonomous underwater vehicle demonstrates that ResAlignNet achieves alignment accuracy within 0.8\u00b0 using only 25 seconds of data collection, representing a 65\\% reduction in convergence time compared to standard velocity-based methods. The trajectory-independent solution eliminates motion pattern requirements and enables immediate vehicle deployment without lengthy pre-mission procedures, advancing underwater navigation capabilities through robust sensor-agnostic alignment that scales across different operational scenarios and sensor specifications.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faResAlignNet\uff0c\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u901a\u8fc71D ResNet-18\u67b6\u6784\u5feb\u901f\u5b9e\u73b0\u6c34\u4e0b\u4f20\u611f\u5668\u7684\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u9ad8\u5bfc\u822a\u51c6\u786e\u6027\u548c\u7075\u6d3b\u6027\u3002", "motivation": "Overcome limitations of standard model-based sensor alignment methods, such as lengthy convergence times and dependence on external aids.", "method": "ResAlignNet, a data-driven approach using 1D ResNet-18 architecture for sensor alignment", "result": "Achieves alignment accuracy within 0.8\u00b0 in 25 seconds, with a 65% reduction in convergence time compared to standard methods.", "conclusion": "ResAlignNet\u663e\u8457\u6539\u5584\u4e86\u6c34\u4e0b\u5bfc\u822a\u80fd\u529b\uff0c\u901a\u8fc7\u5feb\u901f\u3001\u4f20\u611f\u5668\u65e0\u5173\u7684\u5bf9\u9f50\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7684\u64cd\u4f5c\u573a\u666f\u548c\u4f20\u611f\u5668\u89c4\u683c\u3002"}}
{"id": "2511.13100", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13100", "abs": "https://arxiv.org/abs/2511.13100", "authors": ["Xuecheng Chen", "Jingao Xu", "Wenhua Ding", "Haoyang Wang", "Xinyu Luo", "Ruiyang Duan", "Jialong Chen", "Xueqian Wang", "Yunhao Liu", "Xinlei Chen"], "title": "Count Every Rotation and Every Rotation Counts: Exploring Drone Dynamics via Propeller Sensing", "comment": null, "summary": "As drone-based applications proliferate, paramount contactless sensing of airborne drones from the ground becomes indispensable. This work demonstrates concentrating on propeller rotational speed will substantially improve drone sensing performance and proposes an event-camera-based solution, \\sysname. \\sysname features two components: \\textit{Count Every Rotation} achieves accurate, real-time propeller speed estimation by mitigating ultra-high sensitivity of event cameras to environmental noise. \\textit{Every Rotation Counts} leverages these speeds to infer both internal and external drone dynamics. Extensive evaluations in real-world drone delivery scenarios show that \\sysname achieves a sensing latency of 3$ms$ and a rotational speed estimation error of merely 0.23\\%. Additionally, \\sysname infers drone flight commands with 96.5\\% precision and improves drone tracking accuracy by over 22\\% when combined with other sensing modalities. \\textit{ Demo: {\\color{blue}https://eventpro25.github.io/EventPro/.} }", "AI": {"tldr": "\u672c\u7814\u7a76\u4e3b\u8981\u901a\u8fc7\u4e8b\u4ef6\u76f8\u673a\u548c\u7b97\u6cd5\u6539\u8fdb\u65e0\u4eba\u673a\u7684\u975e\u63a5\u89e6\u611f\u77e5\uff0c\u63d0\u5347\u4e86\u87ba\u65cb\u6868\u8f6c\u901f\u4f30\u8ba1\u548c\u52a8\u6001\u63a8\u65ad\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u5e94\u7528\u7684\u4e0d\u65ad\u589e\u52a0\uff0c\u5730\u9762\u5bf9\u65e0\u4eba\u673a\u7684\u975e\u63a5\u89e6\u5f0f\u611f\u77e5\u53d8\u5f97\u6108\u53d1\u91cd\u8981\u3002", "method": "\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u548c\u65b0\u63d0\u51fa\u7684\u7b97\u6cd5\u6765\u4f30\u8ba1\u65e0\u4eba\u673a\u7684\u87ba\u65cb\u6868\u8f6c\u901f\u548c\u63a8\u65ad\u65e0\u4eba\u673a\u5185\u90e8\u53ca\u5916\u90e8\u52a8\u6001\u3002", "result": "\textit{sysname} \u5728\u5b9e\u9645\u65e0\u4eba\u673a\u9001\u8d27\u573a\u666f\u4e2d\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u611f\u77e5\u5ef6\u8fdf\u4e3a 3 \u6beb\u79d2\uff0c\u8f6c\u901f\u4f30\u8ba1\u8bef\u5dee\u4ec5\u4e3a 0.23%\uff0c\u6307\u4ee4\u63a8\u65ad\u7684\u7cbe\u51c6\u5ea6\u4e3a 96.5%\uff0c\u4e14\u4e0e\u5176\u4ed6\u4f20\u611f\u624b\u6bb5\u7ed3\u5408\u540e\u8ddf\u8e2a\u7cbe\u5ea6\u63d0\u9ad8\u4e86 22%\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684 \textit{Count Every Rotation} \u53ca \textit{Every Rotation Counts} \u65b9\u6cd5\u5728\u65e0\u4eba\u673a\u4f20\u611f\u6027\u80fd\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u63d0\u5347\uff0c\u5177\u6709\u6781\u9ad8\u7684\u7cbe\u51c6\u5ea6\u548c\u4f4e\u5ef6\u8fdf\u3002"}}
{"id": "2511.13120", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13120", "abs": "https://arxiv.org/abs/2511.13120", "authors": ["Trevor Exley", "Anderson Brazil Nardin", "Petr Trunin", "Diana Cafiso", "Lucia Beccai"], "title": "Monolithic Units: Actuation, Sensing, and Simulation for Integrated Soft Robot Design", "comment": "8 pages, 6 figures, 1 algorithm, 1 table", "summary": "This work introduces the Monolithic Unit (MU), an actuator-lattice-sensor building block for soft robotics. The MU integrates pneumatic actuation, a compliant lattice envelope, and candidate sites for optical waveguide sensing into a single printed body. In order to study reproducibility and scalability, a parametric design framework establishes deterministic rules linking actuator chamber dimensions to lattice unit cell size. Experimental homogenization of lattice specimens provides effective material properties for finite element simulation. Within this simulation environment, sensor placement is treated as a discrete optimization problem, where a finite set of candidate waveguide paths derived from lattice nodes is evaluated by introducing local stiffening, and the configuration minimizing deviation from baseline mechanical response is selected. Optimized models are fabricated and experimentally characterized, validating the preservation of mechanical performance while enabling embedded sensing. The workflow is further extended to scaled units and a two-finger gripper, demonstrating generality of the MU concept. This approach advances monolithic soft robotic design by combining reproducible co-design rules with simulation-informed sensor integration.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5355\u7247\u5355\u5143\uff08MU\uff09\uff0c\u4f5c\u4e3a\u8f6f\u673a\u5668\u4eba\u4e2d\u7684\u65b0\u578b\u6267\u884c\u5668-\u683c\u5b50-\u4f20\u611f\u5668\u6784\u5efa\u5757\u3002", "motivation": "\u5728\u8f6f\u673a\u5668\u4eba\u9886\u57df\uff0c\u9700\u8981\u5c06\u6267\u884c\u3001\u611f\u77e5\u548c\u7ed3\u6784\u96c6\u6210\u5230\u4e00\u4e2a\u53ef\u91cd\u590d\u548c\u53ef\u6269\u5c55\u7684\u8bbe\u8ba1\u4e2d\u3002", "method": "\u5efa\u7acb\u4e86\u53c2\u6570\u5316\u8bbe\u8ba1\u6846\u67b6\uff0c\u4ee5\u786e\u5b9a\u6267\u884c\u5668\u8154\u4f53\u5c3a\u5bf8\u4e0e\u683c\u5b50\u5355\u5143\u5c3a\u5bf8\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u6709\u9650\u5143\u6a21\u62df\u4f18\u5316\u4f20\u611f\u5668\u5e03\u7f6e\u3002", "result": "\u4f18\u5316\u6a21\u578b\u7ecf\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u786e\u8ba4\u5176\u673a\u68b0\u6027\u80fd\u4fdd\u6301\u4e0d\u53d8\uff0c\u540c\u65f6\u5b9e\u73b0\u5d4c\u5165\u5f0f\u4f20\u611f\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u52a8\u4e86\u5355\u7247\u8f6f\u673a\u5668\u4eba\u7684\u8bbe\u8ba1\uff0c\u7ed3\u5408\u4e86\u53ef\u91cd\u590d\u7684\u5171\u8bbe\u8ba1\u89c4\u5219\u4e0e\u57fa\u4e8e\u6a21\u62df\u7684\u4f20\u611f\u5668\u96c6\u6210\u3002"}}
{"id": "2511.13188", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13188", "abs": "https://arxiv.org/abs/2511.13188", "authors": ["Osama Al Sheikh Ali", "Sotiris Koutsoftas", "Ze Zhang", "Knut Akesson", "Emmanuel Dean"], "title": "Collision-Free Navigation of Mobile Robots via Quadtree-Based Model Predictive Control", "comment": "This paper has been accepted by IEEE SII 2026", "summary": "This paper presents an integrated navigation framework for Autonomous Mobile Robots (AMRs) that unifies environment representation, trajectory generation, and Model Predictive Control (MPC). The proposed approach incorporates a quadtree-based method to generate structured, axis-aligned collision-free regions from occupancy maps. These regions serve as both a basis for developing safe corridors and as linear constraints within the MPC formulation, enabling efficient and reliable navigation without requiring direct obstacle encoding. The complete pipeline combines safe-area extraction, connectivity graph construction, trajectory generation, and B-spline smoothing into one coherent system. Experimental results demonstrate consistent success and superior performance compared to baseline approaches across complex environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u96c6\u6210\u5bfc\u822a\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u56db\u53c9\u6811\u751f\u6210\u65e0\u78b0\u649e\u533a\u57df\uff0c\u63d0\u5347\u5bfc\u822a\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u79fb\u52a8\u673a\u5668\u4eba\uff08AMRs\uff09\u7684\u5bfc\u822a\u9700\u6c42\u65e5\u76ca\u589e\u52a0\uff0c\u76ee\u524d\u7684\u6280\u672f\u5f80\u5f80\u65e0\u6cd5\u6709\u6548\u6574\u5408\u73af\u5883\u8868\u793a\u3001\u8f68\u8ff9\u751f\u6210\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u3002", "method": "\u901a\u8fc7\u56db\u53c9\u6811\u65b9\u6cd5\uff0c\u4ece\u5360\u7528\u5730\u56fe\u751f\u6210\u5b89\u5168\u533a\u57df\uff0c\u5e76\u5c06\u5176\u7528\u4e8eMPC\u516c\u5f0f\u4e2d\u7684\u7ebf\u6027\u7ea6\u675f\uff0c\u6574\u5408\u5b89\u5168\u533a\u57df\u63d0\u53d6\u3001\u8fde\u901a\u6027\u56fe\u6784\u5efa\u3001\u8f68\u8ff9\u751f\u6210\u548cB\u6837\u6761\u5e73\u6ed1\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u53c9\u6811\u65b9\u6cd5\u751f\u6210\u7ed3\u6784\u5316\u4e14\u8f74\u5bf9\u9f50\u7684\u65e0\u78b0\u649e\u533a\u57df\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5bfc\u822a\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u6210\u529f\u5ea6\u9ad8\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2511.13207", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13207", "abs": "https://arxiv.org/abs/2511.13207", "authors": ["Cheng Peng", "Zhenzhe Zhang", "Cheng Chi", "Xiaobao Wei", "Yanhao Zhang", "Heng Wang", "Pengwei Wang", "Zhongyuan Wang", "Jing Liu", "Shanghang Zhang"], "title": "PIGEON: VLM-Driven Object Navigation via Points of Interest Selection", "comment": null, "summary": "Navigating to a specified object in an unknown environment is a fundamental yet challenging capability of embodied intelligence. However, current methods struggle to balance decision frequency with intelligence, resulting in decisions lacking foresight or discontinuous actions. In this work, we propose PIGEON: Point of Interest Guided Exploration for Object Navigation with VLM, maintaining a lightweight and semantically aligned snapshot memory during exploration as semantic input for the exploration strategy. We use a large Visual-Language Model (VLM), named PIGEON-VL, to select Points of Interest (PoI) formed during exploration and then employ a lower-level planner for action output, increasing the decision frequency. Additionally, this PoI-based decision-making enables the generation of Reinforcement Learning with Verifiable Reward (RLVR) data suitable for simulators. Experiments on classic object navigation benchmarks demonstrate that our zero-shot transfer method achieves state-of-the-art performance, while RLVR further enhances the model's semantic guidance capabilities, enabling deep reasoning during real-time navigation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faPIGEON\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6539\u8fdb\u5bf9\u8c61\u5bfc\u822a\uff0c\u8868\u73b0\u51fa\u8272\u4e14\u5177\u5907\u6df1\u5ea6\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u51b3\u7b56\u9891\u7387\u4e0e\u667a\u80fd\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u56f0\u96be\uff0c\u65e8\u5728\u63d0\u5347\u76ee\u6807\u5bfc\u822a\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faPIGEON\u65b9\u6cd5\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5bf9\u8c61\u5bfc\u822a\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7684\u8bed\u4e49\u5feb\u7167\u8bb0\u5fc6\u63d0\u9ad8\u63a2\u7d22\u7b56\u7565\u3002", "result": "\u5728\u7ecf\u5178\u5bf9\u8c61\u5bfc\u822a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u96f6\u6837\u672c\u8fc1\u79fb\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u65b0\u7684\u6027\u80fd\uff0cRLVR\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u8bed\u4e49\u5f15\u5bfc\u80fd\u529b\u3002", "conclusion": "PIGEON\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5bf9\u8c61\u5bfc\u822a\u7b56\u7565\uff0c\u5c55\u793a\u4e86\u5728\u672a\u77e5\u73af\u5883\u4e2d\u9ad8\u6548\u51b3\u7b56\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.13216", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13216", "abs": "https://arxiv.org/abs/2511.13216", "authors": ["Chiyun Noh", "Sangwoo Jung", "Hanjun Kim", "Yafei Hu", "Laura Herlant", "Ayoung Kim"], "title": "GaRLILEO: Gravity-aligned Radar-Leg-Inertial Enhanced Odometry", "comment": null, "summary": "Deployment of legged robots for navigating challenging terrains (e.g., stairs, slopes, and unstructured environments) has gained increasing preference over wheel-based platforms. In such scenarios, accurate odometry estimation is a preliminary requirement for stable locomotion, localization, and mapping. Traditional proprioceptive approaches, which rely on leg kinematics sensor modalities and inertial sensing, suffer from irrepressible vertical drift caused by frequent contact impacts, foot slippage, and vibrations, particularly affected by inaccurate roll and pitch estimation. Existing methods incorporate exteroceptive sensors such as LiDAR or cameras. Further enhancement has been introduced by leveraging gravity vector estimation to add additional observations on roll and pitch, thereby increasing the accuracy of vertical pose estimation. However, these approaches tend to degrade in feature-sparse or repetitive scenes and are prone to errors from double-integrated IMU acceleration. To address these challenges, we propose GaRLILEO, a novel gravity-aligned continuous-time radar-leg-inertial odometry framework. GaRLILEO decouples velocity from the IMU by building a continuous-time ego-velocity spline from SoC radar Doppler and leg kinematics information, enabling seamless sensor fusion which mitigates odometry distortion. In addition, GaRLILEO can reliably capture accurate gravity vectors leveraging a novel soft S2-constrained gravity factor, improving vertical pose accuracy without relying on LiDAR or cameras. Evaluated on a self-collected real-world dataset with diverse indoor-outdoor trajectories, GaRLILEO demonstrates state-of-the-art accuracy, particularly in vertical odometry estimation on stairs and slopes. We open-source both our dataset and algorithm to foster further research in legged robot odometry and SLAM. https://garlileo.github.io/GaRLILEO", "AI": {"tldr": "GaRLILEO\u662f\u4e00\u79cd\u65b0\u5174\u7684\u96f7\u8fbe-\u817f-\u60ef\u6027\u91cc\u7a0b\u8ba1\u6846\u67b6\uff0c\u80fd\u591f\u51c6\u786e\u4f30\u8ba1\u91cd\u529b\u5411\u91cf\u5e76\u6539\u5584\u5782\u76f4\u59ff\u6001\u51c6\u786e\u6027\uff0c\u7279\u522b\u9002\u7528\u4e8e\u817f\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e0b\u7684\u5bfc\u822a\u3002", "motivation": "\u9488\u5bf9\u817f\u8db3\u673a\u5668\u4eba\u5728\u6311\u6218\u6027\u5730\u5f62\u4e2d\u5bfc\u822a\u65f6\u7684\u7cbe\u786e\u91cc\u7a0b\u8ba1\u4f30\u8ba1\u9700\u6c42\uff0c\u5c24\u5176\u662f\u5728\u5e38\u89c4\u65b9\u6cd5\u9762\u4e34\u7684\u5782\u76f4\u6f02\u79fb\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u91cd\u529b\u5bf9\u9f50\u8fde\u7eed\u65f6\u95f4\u96f7\u8fbe-\u817f-\u60ef\u6027\u91cc\u7a0b\u8ba1\u6846\u67b6\uff0c\u7ed3\u5408\u4e86SoC\u96f7\u8fbe\u591a\u666e\u52d2\u548c\u817f\u8fd0\u52a8\u5b66\u4fe1\u606f\uff0c\u6784\u5efa\u8fde\u7eed\u65f6\u95f4\u81ea\u6211\u901f\u5ea6\u6837\u6761\uff0c\u4ee5\u5b9e\u73b0\u65e0\u7f1d\u4f20\u611f\u5668\u878d\u5408\u3002", "result": "\u5728\u81ea\u6536\u96c6\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cGaRLILEO\u5c55\u793a\u51fa\u5148\u8fdb\u7684\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u5782\u76f4\u91cc\u7a0b\u8ba1\u4f30\u8ba1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u697c\u68af\u548c\u659c\u5761\u4e0a\u3002", "conclusion": "GaRLILEO\u5728\u5782\u76f4\u91cc\u7a0b\u8ba1\u4f30\u8ba1\u4e0a\u663e\u793a\u51fa\u5148\u8fdb\u7684\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u697c\u68af\u548c\u659c\u5761\u4e0a\uff0c\u5e76\u4e14\u5f00\u6e90\u6570\u636e\u96c6\u4e0e\u7b97\u6cd5\u4ee5\u4fc3\u8fdb\u540e\u7eed\u7684\u7814\u7a76\u3002"}}
{"id": "2511.13312", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13312", "abs": "https://arxiv.org/abs/2511.13312", "authors": ["Jonas Bode", "Raphael Memmesheimer", "Sven Behnke"], "title": "EL3DD: Extended Latent 3D Diffusion for Language Conditioned Multitask Manipulation", "comment": "10 pages; 2 figures; 1 table. Prprint submitted to the European Robotics Forum 2026", "summary": "Acting in human environments is a crucial capability for general-purpose robots, necessitating a robust understanding of natural language and its application to physical tasks. This paper seeks to harness the capabilities of diffusion models within a visuomotor policy framework that merges visual and textual inputs to generate precise robotic trajectories. By employing reference demonstrations during training, the model learns to execute manipulation tasks specified through textual commands within the robot's immediate environment. The proposed research aims to extend an existing model by leveraging improved embeddings, and adapting techniques from diffusion models for image generation. We evaluate our methods on the CALVIN dataset, proving enhanced performance on various manipulation tasks and an increased long-horizon success rate when multiple tasks are executed in sequence. Our approach reinforces the usefulness of diffusion models and contributes towards general multitask manipulation.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u6269\u6563\u6a21\u578b\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u8f93\u5165\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u6267\u884c\u64cd\u4f5c\u4efb\u52a1\u7684\u80fd\u529b\u3002", "motivation": "\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4e0b\u6267\u884c\u7269\u7406\u4efb\u52a1\u7684\u80fd\u529b", "method": "\u91c7\u7528\u6269\u6563\u6a21\u578b\u878d\u5165\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u6846\u67b6", "result": "\u5728CALVIN\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u5347\u4e86\u591a\u79cd\u64cd\u4f5c\u4efb\u52a1\u7684\u6210\u529f\u7387\uff0c\u5c24\u5176\u662f\u5728\u987a\u5e8f\u6267\u884c\u591a\u4e2a\u4efb\u52a1\u65f6", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u6269\u6563\u6a21\u578b\u5728\u591a\u4efb\u52a1\u64cd\u4f5c\u4e2d\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u589e\u5f3a\u5d4c\u5165\u548c\u9002\u5e94\u56fe\u50cf\u751f\u6210\u6280\u672f\uff0c\u63a8\u52a8\u4e86\u673a\u5668\u4eba\u7684\u53d1\u5c55\u3002"}}
{"id": "2511.13327", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13327", "abs": "https://arxiv.org/abs/2511.13327", "authors": ["Juntao Jian", "Yi-Lin Wei", "Chengjie Mou", "Yuhao Lin", "Xing Zhu", "Yujun Shen", "Wei-Shi Zheng", "Ruizhen Hu"], "title": "ZeroDexGrasp: Zero-Shot Task-Oriented Dexterous Grasp Synthesis with Prompt-Based Multi-Stage Semantic Reasoning", "comment": null, "summary": "Task-oriented dexterous grasping holds broad application prospects in robotic manipulation and human-object interaction. However, most existing methods still struggle to generalize across diverse objects and task instructions, as they heavily rely on costly labeled data to ensure task-specific semantic alignment. In this study, we propose \\textbf{ZeroDexGrasp}, a zero-shot task-oriented dexterous grasp synthesis framework integrating Multimodal Large Language Models with grasp refinement to generate human-like grasp poses that are well aligned with specific task objectives and object affordances. Specifically, ZeroDexGrasp employs prompt-based multi-stage semantic reasoning to infer initial grasp configurations and object contact information from task and object semantics, then exploits contact-guided grasp optimization to refine these poses for physical feasibility and task alignment. Experimental results demonstrate that ZeroDexGrasp enables high-quality zero-shot dexterous grasping on diverse unseen object categories and complex task requirements, advancing toward more generalizable and intelligent robotic grasping.", "AI": {"tldr": "ZeroDexGrasp\u662f\u4e00\u4e2a\u96f6-shot\u7684\u6293\u53d6\u6846\u67b6\uff0c\u80fd\u591f\u5e94\u5bf9\u591a\u6837\u5316\u7684\u5bf9\u8c61\u548c\u4efb\u52a1\u6307\u4ee4\uff0c\u663e\u8457\u63d0\u9ad8\u673a\u5668\u4eba\u6293\u53d6\u7684\u901a\u7528\u6027\u548c\u667a\u80fd\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u6807\u6ce8\u6570\u636e\uff0c\u5bfc\u81f4\u5176\u5728\u4e0d\u540c\u5bf9\u8c61\u548c\u4efb\u52a1\u6307\u4ee4\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u901a\u7528\u3001\u667a\u80fd\u7684\u6293\u53d6\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u63d0\u793a\u7684\u591a\u9636\u6bb5\u8bed\u4e49\u63a8\u7406\u63a8\u5bfc\u521d\u59cb\u6293\u53d6\u914d\u7f6e\uff0c\u518d\u901a\u8fc7\u63a5\u89e6\u5f15\u5bfc\u7684\u6293\u53d6\u4f18\u5316\u8c03\u6574\u4ee5\u786e\u4fdd\u7269\u7406\u53ef\u884c\u6027\u548c\u4efb\u52a1\u4e00\u81f4\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u96f6-shot\u4efb\u52a1\u5bfc\u5411\u7075\u5de7\u6293\u53d6\u6846\u67b6ZeroDexGrasp\uff0c\u5b83\u5c06\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u6293\u53d6\u4f18\u5316\u7ed3\u5408\uff0c\u901a\u8fc7\u521d\u6b65\u6293\u53d6\u914d\u7f6e\u63a8\u7406\u548c\u63a5\u89e6\u5f15\u5bfc\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u5bf9\u672a\u89c1\u5bf9\u8c61\u7c7b\u522b\u548c\u590d\u6742\u4efb\u52a1\u8981\u6c42\u7684\u9ad8\u8d28\u91cf\u6293\u53d6\u3002", "conclusion": "ZeroDexGrasp\u901a\u8fc7\u7ed3\u5408\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u548c\u6293\u53d6\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u4efb\u52a1\u548c\u591a\u6837\u76ee\u6807\u4e0a\u7684\u6293\u53d6\u80fd\u529b\uff0c\u671d\u7740\u66f4\u667a\u80fd\u5316\u7684\u6293\u53d6\u65b9\u5411\u8fc8\u8fdb\u3002"}}
{"id": "2511.13459", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13459", "abs": "https://arxiv.org/abs/2511.13459", "authors": ["Bingkun Huang", "Yuhe Gong", "Zewen Yang", "Tianyu Ren", "Luis Figueredo"], "title": "Contact-Safe Reinforcement Learning with ProMP Reparameterization and Energy Awareness", "comment": null, "summary": "Reinforcement learning (RL) approaches based on Markov Decision Processes (MDPs) are predominantly applied in the robot joint space, often relying on limited task-specific information and partial awareness of the 3D environment. In contrast, episodic RL has demonstrated advantages over traditional MDP-based methods in terms of trajectory consistency, task awareness, and overall performance in complex robotic tasks. Moreover, traditional step-wise and episodic RL methods often neglect the contact-rich information inherent in task-space manipulation, especially considering the contact-safety and robustness. In this work, contact-rich manipulation tasks are tackled using a task-space, energy-safe framework, where reliable and safe task-space trajectories are generated through the combination of Proximal Policy Optimization (PPO) and movement primitives. Furthermore, an energy-aware Cartesian Impedance Controller objective is incorporated within the proposed framework to ensure safe interactions between the robot and the environment. Our experimental results demonstrate that the proposed framework outperforms existing methods in handling tasks on various types of surfaces in 3D environments, achieving high success rates as well as smooth trajectories and energy-safe interactions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4efb\u52a1\u7a7a\u95f4\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u63a7\u4efb\u52a1\uff0c\u8868\u73b0\u51fa\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u4f18\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u74063D\u73af\u5883\u4e2d\u7684\u63a5\u89e6\u4e30\u5bcc\u64cd\u63a7\u4efb\u52a1\u65f6\uff0c\u5f80\u5f80\u5ffd\u7565\u4e86\u63a5\u89e6\u4fe1\u606f\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u548c\u8fd0\u52a8\u539f\u8bed\uff0c\u751f\u6210\u53ef\u9760\u4e14\u5b89\u5168\u7684\u4efb\u52a1\u7a7a\u95f4\u8f68\u8ff9\uff0c\u5e76\u5f15\u5165\u80fd\u91cf\u610f\u8bc6\u7684\u7b1b\u5361\u5c14\u963b\u6297\u63a7\u5236\u5668\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u5404\u79cd3D\u73af\u5883\u7684\u4efb\u52a1\u6267\u884c\u4e2d\uff0c\u8868\u73b0\u51fa\u4e86\u8f83\u9ad8\u7684\u6210\u529f\u7387\u548c\u5e73\u6ed1\u7684\u8f68\u8ff9\uff0c\u786e\u4fdd\u4e86\u4e0e\u73af\u5883\u7684\u5b89\u5168\u4ea4\u4e92\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u5904\u7406\u590d\u6742\u7684\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u65f6\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fbe\u5230\u9ad8\u6210\u529f\u7387\u548c\u5b89\u5168\u7684\u5e73\u6ed1\u8f68\u8ff9\u3002"}}
{"id": "2511.13530", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13530", "abs": "https://arxiv.org/abs/2511.13530", "authors": ["Vesna Poprcova", "Iulia Lefter", "Matthias Wieser", "Martijn Warnier", "Frances Brazier"], "title": "Towards Affect-Adaptive Human-Robot Interaction: A Protocol for Multimodal Dataset Collection on Social Anxiety", "comment": "Accepted at the Workshop on Benefits of pErsonalization and behAvioral adaptation in assistive Robots (BEAR 2025), held at the IEEE RO-MAN Conference 2025", "summary": "Social anxiety is a prevalent condition that affects interpersonal interactions and social functioning. Recent advances in artificial intelligence and social robotics offer new opportunities to examine social anxiety in the human-robot interaction context. Accurate detection of affective states and behaviours associated with social anxiety requires multimodal datasets, where each signal modality provides complementary insights into its manifestations. However, such datasets remain scarce, limiting progress in both research and applications. To address this, this paper presents a protocol for multimodal dataset collection designed to reflect social anxiety in a human-robot interaction context. The dataset will consist of synchronised audio, video, and physiological recordings acquired from at least 70 participants, grouped according to their level of social anxiety, as they engage in approximately 10-minute interactive Wizard-of-Oz role-play scenarios with the Furhat social robot under controlled experimental conditions. In addition to multimodal data, the dataset will be enriched with contextual data providing deeper insight into individual variability in social anxiety responses. This work can contribute to research on affect-adaptive human-robot interaction by providing support for robust multimodal detection of social anxiety.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u534f\u8bae\uff0c\u65e8\u5728\u6536\u96c6\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u4ee5\u7814\u7a76\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u793e\u4f1a\u7126\u8651\uff0c\u4e3a\u60c5\u611f\u9002\u5e94\u7684\u4eba\u673a\u4ea4\u4e92\u7814\u7a76\u63d0\u4f9b\u652f\u6301\u3002", "motivation": "\u793e\u4f1a\u7126\u8651\u5f71\u54cd\u4eba\u9645\u4e92\u52a8\u548c\u793e\u4ea4\u529f\u80fd\uff0c\u4eba\u5de5\u667a\u80fd\u548c\u793e\u4ea4\u673a\u5668\u4eba\u4e3a\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u673a\u9047\uff0c\u4f46\u7f3a\u4e4f\u591a\u6a21\u6001\u6570\u636e\u96c6\u9650\u5236\u4e86\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u8bbe\u8ba1\u7528\u4e8e\u53cd\u6620\u793e\u4f1a\u7126\u8651\u7684\u4eba\u673a\u4ea4\u4e92\u80cc\u666f\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u6536\u96c6\u534f\u8bae", "result": "\u6784\u5efa\u4e00\u4e2a\u5305\u542b\u81f3\u5c1170\u4e2a\u53c2\u4e0e\u8005\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u540c\u6b65\u97f3\u9891\u3001\u89c6\u9891\u548c\u751f\u7406\u8bb0\u5f55\uff0c\u53ca\u60c5\u5883\u6570\u636e\uff0c\u652f\u6301\u793e\u4f1a\u7126\u8651\u7684\u591a\u6a21\u6001\u68c0\u6d4b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5bf9\u60c5\u611f\u9002\u5e94\u7684\u4eba\u673a\u4ea4\u4e92\u7814\u7a76\u6709\u91cd\u8981\u8d21\u732e\uff0c\u901a\u8fc7\u5efa\u7acb\u591a\u6a21\u6001\u6570\u636e\u96c6\u4fc3\u8fdb\u793e\u4f1a\u7126\u8651\u7684\u51c6\u786e\u68c0\u6d4b\u3002"}}
{"id": "2511.13707", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13707", "abs": "https://arxiv.org/abs/2511.13707", "authors": ["Xiaoyu Liang", "Ziang Liu", "Kelvin Lin", "Edward Gu", "Ruolin Ye", "Tam Nguyen", "Cynthia Hsu", "Zhanxin Wu", "Xiaoman Yang", "Christy Sum Yu Cheung", "Harold Soh", "Katherine Dimitropoulou", "Tapomayukh Bhattacharjee"], "title": "OpenRoboCare: A Multimodal Multi-Task Expert Demonstration Dataset for Robot Caregiving", "comment": "IROS 2025", "summary": "We present OpenRoboCare, a multimodal dataset for robot caregiving, capturing expert occupational therapist demonstrations of Activities of Daily Living (ADLs). Caregiving tasks involve complex physical human-robot interactions, requiring precise perception under occlusions, safe physical contact, and long-horizon planning. While recent advances in robot learning from demonstrations have shown promise, there is a lack of a large-scale, diverse, and expert-driven dataset that captures real-world caregiving routines. To address this gap, we collect data from 21 occupational therapists performing 15 ADL tasks on two manikins. The dataset spans five modalities: RGB-D video, pose tracking, eye-gaze tracking, task and action annotations, and tactile sensing, providing rich multimodal insights into caregiver movement, attention, force application, and task execution strategies. We further analyze expert caregiving principles and strategies, offering insights to improve robot efficiency and task feasibility. Additionally, our evaluations demonstrate that OpenRoboCare presents challenges for state-of-the-art robot perception and human activity recognition methods, both critical for developing safe and adaptive assistive robots, highlighting the value of our contribution. See our website for additional visualizations: https://emprise.cs.cornell.edu/robo-care/.", "AI": {"tldr": "OpenRoboCare\u662f\u4e00\u4e2a\u9488\u5bf9\u673a\u5668\u4eba\u62a4\u7406\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b\u62a4\u7406\u4e13\u5bb6\u768415\u9879\u65e5\u5e38\u751f\u6d3b\u6d3b\u52a8\u6570\u636e\uff0c\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u591a\u6a21\u6001\u4fe1\u606f\u5e76\u5206\u6790\u4e86\u62a4\u7406\u7b56\u7565\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5927\u578b\u3001\u591a\u6837\u4e14\u7531\u4e13\u5bb6\u9a71\u52a8\u7684\u62a4\u7406\u6570\u636e\u96c6\uff0c\u4ee5\u652f\u6301\u673a\u5668\u4eba\u5b66\u4e60\u548c\u4eba\u673a\u4ea4\u4e92\u7814\u7a76\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u4e2a\u80fd\u591f\u6355\u6349\u771f\u5b9e\u62a4\u7406\u573a\u666f\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u3002", "method": "\u901a\u8fc7\u8bb0\u5f5521\u540d\u804c\u4e1a\u6cbb\u7597\u5e08\u5728\u4e24\u4e2a\u4eba\u5f62\u6a21\u578b\u4e0a\u6267\u884c15\u9879\u65e5\u5e38\u751f\u6d3b\u6d3b\u52a8\uff0c\u6536\u96c6\u4e86RGB-D\u89c6\u9891\u3001\u59ff\u6001\u8ddf\u8e2a\u3001\u773c\u52a8\u8ddf\u8e2a\u3001\u4efb\u52a1\u548c\u52a8\u4f5c\u6ce8\u91ca\u4ee5\u53ca\u89e6\u89c9\u611f\u77e5\u7b49\u4e94\u79cd\u6a21\u6001\u7684\u6570\u636e\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aOpenRoboCare\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e86\u62a4\u7406\u4e13\u4e1a\u4eba\u5458\u5bf9\u65e5\u5e38\u751f\u6d3b\u6d3b\u52a8\u7684\u8868\u6f14\uff0c\u5e76\u5206\u6790\u4e86\u62a4\u7406\u539f\u5219\u4e0e\u7b56\u7565\u3002", "conclusion": "OpenRoboCare\u5bf9\u73b0\u6709\u673a\u5668\u89c6\u89c9\u548c\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u65b9\u6cd5\u63d0\u51fa\u6311\u6218\uff0c\u5f3a\u8c03\u4e86\u5176\u5728\u53d1\u5c55\u5b89\u5168\u3001\u81ea\u9002\u5e94\u52a9\u6b8b\u673a\u5668\u4eba\u7684\u91cd\u8981\u6027\u548c\u4ef7\u503c\u3002"}}
{"id": "2511.13710", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13710", "abs": "https://arxiv.org/abs/2511.13710", "authors": ["Jianglong Ye", "Lai Wei", "Guangqi Jiang", "Changwei Jing", "Xueyan Zou", "Xiaolong Wang"], "title": "From Power to Precision: Learning Fine-grained Dexterity for Multi-fingered Robotic Hands", "comment": "Project page: https://jianglongye.com/power-to-precision", "summary": "Human grasps can be roughly categorized into two types: power grasps and precision grasps. Precision grasping enables tool use and is believed to have influenced human evolution. Today's multi-fingered robotic hands are effective in power grasps, but for tasks requiring precision, parallel grippers are still more widely adopted. This contrast highlights a key limitation in current robotic hand design: the difficulty of achieving both stable power grasps and precise, fine-grained manipulation within a single, versatile system. In this work, we bridge this gap by jointly optimizing the control and hardware design of a multi-fingered dexterous hand, enabling both power and precision manipulation. Rather than redesigning the entire hand, we introduce a lightweight fingertip geometry modification, represent it as a contact plane, and jointly optimize its parameters along with the corresponding control. Our control strategy dynamically switches between power and precision manipulation and simplifies precision control into parallel thumb-index motions, which proves robust for sim-to-real transfer. On the design side, we leverage large-scale simulation to optimize the fingertip geometry using a differentiable neural-physics surrogate model. We validate our approach through extensive experiments in both sim-to-real and real-to-real settings. Our method achieves an 82.5% zero-shot success rate on unseen objects in sim-to-real precision grasping, and a 93.3% success rate in challenging real-world tasks involving bread pinching. These results demonstrate that our co-design framework can significantly enhance the fine-grained manipulation ability of multi-fingered hands without reducing their ability for power grasps. Our project page is at https://jianglongye.com/power-to-precision", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u8054\u5408\u4f18\u5316\u591a\u6307\u7075\u5de7\u624b\u7684\u65b9\u6cd5\uff0c\u4f7f\u5176\u80fd\u591f\u540c\u65f6\u5b9e\u73b0\u529b\u91cf\u6293\u63e1\u548c\u7cbe\u786e\u64cd\u4f5c\uff0c\u901a\u8fc7\u8f7b\u91cf\u5316\u6307\u5c16\u51e0\u4f55\u4fee\u6539\u548c\u52a8\u6001\u63a7\u5236\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u6293\u63e1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u624b\u5728\u529b\u91cf\u6293\u63e1\u4e0a\u6548\u679c\u663e\u8457\uff0c\u4f46\u5728\u9700\u8981\u7cbe\u786e\u64cd\u4f5c\u7684\u4efb\u52a1\u4e2d\u4ecd\u4ee5\u5e73\u884c\u6293\u53d6\u5668\u4e3a\u4e3b\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u8bbe\u8ba1\u7684\u4e3b\u8981\u5c40\u9650\u6027\u3002", "method": "\u8054\u5408\u4f18\u5316\u591a\u6307\u7075\u5de7\u624b\u7684\u63a7\u5236\u4e0e\u786c\u4ef6\u8bbe\u8ba1\uff0c\u4ee5\u5b9e\u73b0\u529b\u91cf\u548c\u7cbe\u786e\u64cd\u4f5c\u3002", "result": "\u672c\u65b9\u6cd5\u5728\u770b\u4e0d\u89c1\u7684\u7269\u4f53\u7cbe\u786e\u6293\u53d6\u4e2d\u5b9e\u73b082.5%\u7684\u96f6-shot\u6210\u529f\u7387\uff0c\u5e76\u5728\u6d89\u53ca\u9762\u5305\u5939\u62fe\u7684\u590d\u6742\u4efb\u52a1\u4e2d\u53d6\u5f9793.3%\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u6211\u4eec\u7684\u5171\u540c\u8bbe\u8ba1\u6846\u67b6\u5728\u4e0d\u964d\u4f4e\u529b\u91cf\u6293\u63e1\u80fd\u529b\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6307\u624b\u7684\u7cbe\u7ec6\u64cd\u4f5c\u80fd\u529b\u3002"}}
