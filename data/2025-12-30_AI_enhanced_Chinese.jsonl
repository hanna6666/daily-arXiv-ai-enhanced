{"id": "2512.22187", "categories": ["cs.RO", "cs.ET", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.22187", "abs": "https://arxiv.org/abs/2512.22187", "authors": ["Ndagijimana Cyprien", "Mehdi Sookhak", "Hosein Zarini", "Chandra N Sekharan", "Mohammed Atiquzzaman"], "title": "Joint UAV-UGV Positioning and Trajectory Planning via Meta A3C for Reliable Emergency Communications", "comment": null, "summary": "Joint deployment of unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) has been shown to be an effective method to establish communications in areas affected by disasters. However, ensuring good Quality of Services (QoS) while using as few UAVs as possible also requires optimal positioning and trajectory planning for UAVs and UGVs. This paper proposes a joint UAV-UGV-based positioning and trajectory planning framework for UAVs and UGVs deployment that guarantees optimal QoS for ground users. To model the UGVs' mobility, we introduce a road graph, which directs their movement along valid road segments and adheres to the road network constraints. To solve the sum rate optimization problem, we reformulate the problem as a Markov Decision Process (MDP) and propose a novel asynchronous Advantage Actor Critic (A3C) incorporated with meta-learning for rapid adaptation to new environments and dynamic conditions. Numerical results demonstrate that our proposed Meta-A3C approach outperforms A3C and DDPG, delivering 13.1\\% higher throughput and 49\\% faster execution while meeting the QoS requirements.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u8054\u5408\u90e8\u7f72UAV\u548cUGV\u786e\u4fdd\u6700\u4f73QoS\u7684\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u65b0\u7684\u7b97\u6cd5\u5b9e\u73b0\u6027\u80fd\u4f18\u5316\u3002", "motivation": "\u9762\u5bf9\u707e\u540e\u901a\u8baf\u9700\u6c42\uff0c\u5982\u4f55\u5728\u4fdd\u8bc1\u826f\u597d\u670d\u52a1\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u51cf\u5c11UAV\u6570\u91cf\u53ca\u5176\u5408\u7406\u90e8\u7f72\u548c\u89c4\u5212\u8f68\u8ff9\u3002", "method": "\u5c06\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\uff0c\u7ed3\u5408\u5f02\u6b65\u4f18\u52bf\u6f14\u5458\u8bc4\u8bba\u5bb6\uff08A3C\uff09\u548c\u5143\u5b66\u4e60\uff0c\u4ee5\u5b9e\u73b0\u5feb\u901f\u9002\u5e94\u65b0\u73af\u5883\u548c\u52a8\u6001\u6761\u4ef6\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u4eba\u673a\uff08UAV\uff09\u548c\u65e0\u4eba\u5730\u9762\u8f66\u8f86\uff08UGV\uff09\u7684\u8054\u5408\u5b9a\u4f4d\u4e0e\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u786e\u4fdd\u5730\u9762\u7528\u6237\u7684\u6700\u4f73\u670d\u52a1\u8d28\u91cf\uff08QoS\uff09\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMeta-A3C\u65b9\u6cd5\u7684\u541e\u5410\u91cf\u6bd4\u4f20\u7edf\u65b9\u6cd5\u9ad8\u51fa13.1%\uff0c\u6267\u884c\u901f\u5ea6\u5feb49%\u3002"}}
{"id": "2512.22342", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.22342", "abs": "https://arxiv.org/abs/2512.22342", "authors": ["Wensi Huang", "Shaohao Zhu", "Meng Wei", "Jinming Xu", "Xihui Liu", "Hanqing Wang", "Tai Wang", "Feng Zhao", "Jiangmiao Pang"], "title": "VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs", "comment": null, "summary": "In most existing embodied navigation tasks, instructions are well-defined and unambiguous, such as instruction following and object searching. Under this idealized setting, agents are required solely to produce effective navigation outputs conditioned on vision and language inputs. However, real-world navigation instructions are often vague and ambiguous, requiring the agent to resolve uncertainty and infer user intent through active dialog. To address this gap, we propose Interactive Instance Object Navigation (IION), a task that requires agents not only to generate navigation actions but also to produce language outputs via active dialog, thereby aligning more closely with practical settings. IION extends Instance Object Navigation (ION) by allowing agents to freely consult an oracle in natural language while navigating. Building on this task, we present the Vision Language-Language Navigation (VL-LN) benchmark, which provides a large-scale, automatically generated dataset and a comprehensive evaluation protocol for training and assessing dialog-enabled navigation models. VL-LN comprises over 41k long-horizon dialog-augmented trajectories for training and an automatic evaluation protocol with an oracle capable of responding to agent queries. Using this benchmark, we train a navigation model equipped with dialog capabilities and show that it achieves significant improvements over the baselines. Extensive experiments and analyses further demonstrate the effectiveness and reliability of VL-LN for advancing research on dialog-enabled embodied navigation. Code and dataset: https://0309hws.github.io/VL-LN.github.io/", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bfc\u822a\u4efb\u52a1\u548c\u57fa\u51c6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u5bfc\u822a\u4efb\u52a1\u5bf9\u6a21\u7cca\u6307\u4ee4\u7684\u4e0d\u8db3\uff0c\u901a\u8fc7\u5bf9\u8bdd\u589e\u5f3a\u4ee3\u7406\u7684\u5bfc\u822a\u80fd\u529b\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5bfc\u822a\u6307\u4ee4\u901a\u5e38\u6a21\u7cca\u4e14\u542b\u7cca\u4e0d\u6e05\uff0c\u9700\u8981\u4ee3\u7406\u901a\u8fc7\u4e3b\u52a8\u5bf9\u8bdd\u6765\u89e3\u6790\u4e0d\u786e\u5b9a\u6027\u548c\u63a8\u65ad\u7528\u6237\u610f\u56fe\uff0c\u800c\u73b0\u6709\u7684\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u660e\u786e\u6307\u4ee4\u4e0a\u3002", "method": "\u5728\u4e92\u52a8\u5b9e\u4f8b\u5bf9\u8c61\u5bfc\u822a\u4efb\u52a1\u7684\u57fa\u7840\u4e0a\uff0c\u5141\u8bb8\u4ee3\u7406\u5728\u5bfc\u822a\u8fc7\u7a0b\u4e2d\u81ea\u7531\u54a8\u8be2\u8bed\u8a00\u5929\u7ebf\uff0c\u5e76\u5229\u7528\u8be5\u4efb\u52a1\u521b\u5efa\u4e86\u5305\u542b\u8d85\u8fc741k\u6761\u957f\u65f6\u95f4\u5bf9\u8bdd\u589e\u5f3a\u8f68\u8ff9\u7684\u6570\u636e\u96c6\u3002", "result": "\u63d0\u51fa\u4e86\u4e92\u52a8\u5b9e\u4f8b\u5bf9\u8c61\u5bfc\u822a\uff08IION\uff09\u4efb\u52a1\uff0c\u5e76\u5efa\u7acb\u4e86\u89c6\u89c9\u8bed\u8a00-\u8bed\u8a00\u5bfc\u822a\uff08VL-LN\uff09\u57fa\u51c6\uff0c\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21\u81ea\u52a8\u751f\u6210\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u534f\u8bae\u3002", "conclusion": "\u901a\u8fc7\u5bf9\u8bdd\u80fd\u529b\u8bad\u7ec3\u7684\u5bfc\u822a\u6a21\u578b\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\uff0c\u8bc1\u660e\u4e86VL-LN\u5728\u63a8\u52a8\u5bf9\u8bdd\u589e\u5f3a\u7684\u5177\u8eab\u5bfc\u822a\u7814\u7a76\u4e2d\u7684\u6709\u6548\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2512.22408", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22408", "abs": "https://arxiv.org/abs/2512.22408", "authors": ["Amro Gamar", "Ahmed Abduljalil", "Alargam Mohammed", "Ali Elhenidy", "Abeer Tawakol"], "title": "A Unified AI, Embedded, Simulation, and Mechanical Design Approach to an Autonomous Delivery Robot", "comment": null, "summary": "This paper presents the development of a fully autonomous delivery robot integrating mechanical engineering, embedded systems, and artificial intelligence. The platform employs a heterogeneous computing architecture, with RPi 5 and ROS 2 handling AI-based perception and path planning, while ESP32 running FreeRTOS ensures real-time motor control. The mechanical design was optimized for payload capacity and mobility through precise motor selection and material engineering. Key technical challenges addressed include optimizing computationally intensive AI algorithms on a resource-constrained platform and implementing a low-latency, reliable communication link between the ROS 2 host and embedded controller. Results demonstrate deterministic, PID-based motor control through rigorous memory and task management, and enhanced system reliability via AWS IoT monitoring and a firmware-level motor shutdown failsafe. This work highlights a unified, multi-disciplinary methodology, resulting in a robust and operational autonomous delivery system capable of real-world deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5168\u81ea\u52a8\u9001\u8d27\u673a\u5668\u4eba\uff0c\u7ed3\u5408\u4e86\u591a\u5b66\u79d1\u7684\u65b9\u6cd5\uff0c\u6210\u529f\u8fdb\u884cAI\u611f\u77e5\u548c\u8def\u5f84\u89c4\u5212\uff0c\u63d0\u5347\u4e86\u7cfb\u7edf\u53ef\u9760\u6027\u548c\u5b9e\u65f6\u63a7\u5236\u80fd\u529b\u3002", "motivation": "\u6574\u5408\u673a\u68b0\u5de5\u7a0b\u3001\u5d4c\u5165\u5f0f\u7cfb\u7edf\u548c\u4eba\u5de5\u667a\u80fd\uff0c\u6ee1\u8db3\u73b0\u5b9e\u4e16\u754c\u7684\u9001\u8d27\u9700\u6c42", "method": "\u5f00\u53d1\u4e00\u4e2a\u5168\u81ea\u52a8\u9001\u8d27\u673a\u5668\u4eba", "result": "\u5c55\u793a\u4e86\u4e00\u79cd\u53ef\u9760\u7684\u81ea\u9002\u5e94\u7cfb\u7edf\uff0c\u4f18\u5316\u4e86\u8ba1\u7b97\u5bc6\u96c6\u578bAI\u7b97\u6cd5\u548c\u4f4e\u5ef6\u8fdf\u901a\u4fe1\u94fe\u8def\uff0c\u8fdb\u884cPID motor\u63a7\u5236\u5e76\u5b9e\u65bd\u4e86\u6545\u969c\u5b89\u5168\u673a\u5236\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u4e00\u79cd\u5728\u8d44\u6e90\u6709\u9650\u7684\u5d4c\u5165\u5f0f\u5e73\u53f0\u4e0a\u8fd0\u884c\u7684\u81ea\u4e3b\u9001\u8d27\u7cfb\u7edf\uff0c\u5177\u5907\u5b9e\u7528\u6027\u548c\u53ef\u90e8\u7f72\u6027\u3002"}}
{"id": "2512.22414", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22414", "abs": "https://arxiv.org/abs/2512.22414", "authors": ["Simar Kareer", "Karl Pertsch", "James Darpinian", "Judy Hoffman", "Danfei Xu", "Sergey Levine", "Chelsea Finn", "Suraj Nair"], "title": "Emergence of Human to Robot Transfer in Vision-Language-Action Models", "comment": null, "summary": "Vision-language-action (VLA) models can enable broad open world generalization, but require large and diverse datasets. It is appealing to consider whether some of this data can come from human videos, which cover diverse real-world situations and are easy to obtain. However, it is difficult to train VLAs with human videos alone, and establishing a mapping between humans and robots requires manual engineering and presents a major research challenge. Drawing inspiration from advances in large language models, where the ability to learn from diverse supervision emerges with scale, we ask whether a similar phenomenon holds for VLAs that incorporate human video data. We introduce a simple co-training recipe, and find that human-to-robot transfer emerges once the VLA is pre-trained on sufficient scenes, tasks, and embodiments. Our analysis suggests that this emergent capability arises because diverse pretraining produces embodiment-agnostic representations for human and robot data. We validate these findings through a series of experiments probing human to robot skill transfer and find that with sufficiently diverse robot pre-training our method can nearly double the performance on generalization settings seen only in human data.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u4eba\u7c7b\u89c6\u9891\u6570\u636e\u6765\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5171\u8bad\u7ec3\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524dVLA\u6a21\u578b\u4f9d\u8d56\u4e8e\u5927\u91cf\u591a\u6837\u7684\u6570\u636e\uff0c\u800c\u4eba\u7c7b\u89c6\u9891\u63d0\u4f9b\u4e86\u4e00\u79cd\u6613\u4e8e\u83b7\u53d6\u7684\u4e30\u5bcc\u6570\u636e\u6e90\uff0c\u7814\u7a76\u662f\u5426\u80fd\u591f\u5229\u7528\u8fd9\u4e9b\u6570\u636e\u63d0\u5347\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u5171\u8bad\u7ec3\u65b9\u6cd5\uff0c\u9884\u8bad\u7ec3VLA\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u4e00\u7cfb\u5217\u5b9e\u9a8c\u4ee5\u9a8c\u8bc1\u4eba\u7c7b\u5230\u673a\u5668\u4eba\u6280\u80fd\u7684\u8fc1\u79fb\u6548\u679c\u3002", "result": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u4eba\u7c7b\u89c6\u9891\u6570\u636e\u6765\u589e\u5f3a\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c(VLA)\u6a21\u578b\u7684\u5f00\u653e\u4e16\u754c\u6cdb\u5316\u80fd\u529b\u3002\u901a\u8fc7\u5145\u5206\u7684\u573a\u666f\u3001\u4efb\u52a1\u548c\u4f53\u73b0\uff08embodiment\uff09\u7684\u9884\u8bad\u7ec3\uff0cVLA\u80fd\u591f\u5b9e\u73b0\u4eba\u7c7b\u5230\u673a\u5668\u4eba\u6280\u80fd\u7684\u8fc1\u79fb\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u8db3\u591f\u591a\u6837\u7684\u673a\u5668\u4eba\u9884\u8bad\u7ec3\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u4ec5\u89c1\u4e8e\u4eba\u7c7b\u6570\u636e\u7684\u6cdb\u5316\u4efb\u52a1\u4e2d\uff0c\u51e0\u4e4e\u53ef\u4ee5\u5c06\u6027\u80fd\u63d0\u9ad8\u4e00\u500d\u3002"}}
{"id": "2512.22333", "categories": ["cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22333", "abs": "https://arxiv.org/abs/2512.22333", "authors": ["Ricardo Vasquez", "Diego Riofr\u00edo-Luzcando", "Joe Carrion-Jumbo", "Cesar Guevara"], "title": "Emotion classification using EEG headset signals and Random Forest", "comment": "Preprint. Published version: https://doi.org/10.23919/CISTI58278.2023.10211789", "summary": "Emotions are one of the important components of the human being, thus they are a valuable part of daily activities such as interaction with people, decision making and learning. For this reason, it is important to detect, recognize and understand emotions using computational systems to improve communication between people and machines, which would facilitate the ability of computers to understand the communication between humans. This study proposes the creation of a model that allows the classification of people's emotions based on their EEG signals, for which the brain-computer interface EMOTIV EPOC was used. This allowed the collection of electroencephalographic information from 50 people, all of whom were shown audiovisual resources that helped to provoke the desired mood. The information obtained was stored in a database for the generation of the model and the corresponding classification analysis. Random Forest model was created for emotion prediction (happiness, sadness and relaxation), based on the signals of any person. The results obtained were 97.21% accurate for happiness, 76% for relaxation and 76% for sadness. Finally, the model was used to generate a real-time emotion prediction algorithm; it captures the person's EEG signals, executes the generated algorithm and displays the result on the screen with the help of images representative of each emotion.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eEEG\u4fe1\u53f7\u7684\u60c5\u7eea\u5206\u7c7b\u6a21\u578b\uff0c\u5229\u7528EMOTIV EPOC\u8111\u673a\u63a5\u53e3\u6536\u96c6\u6570\u636e\uff0c\u5e76\u5b9e\u73b0\u5b9e\u65f6\u60c5\u7eea\u9884\u6d4b\u3002", "motivation": "\u60c5\u7eea\u5728\u65e5\u5e38\u6d3b\u52a8\u4e2d\u626e\u6f14\u7740\u91cd\u8981\u89d2\u8272\uff0c\u56e0\u6b64\u901a\u8fc7\u8ba1\u7b97\u673a\u7cfb\u7edf\u68c0\u6d4b\u548c\u7406\u89e3\u60c5\u7eea\u662f\u6539\u5584\u4eba\u673a\u6c9f\u901a\u7684\u5173\u952e\u3002", "method": "\u4f7f\u7528EMOTIV EPOC\u8111\u673a\u63a5\u53e3\u6536\u96c650\u540d\u88ab\u8bd5\u8005\u7684EEG\u6570\u636e\uff0c\u4ee5\u521b\u9020\u968f\u673a\u68ee\u6797\u6a21\u578b\u8fdb\u884c\u60c5\u7eea\u9884\u6d4b\u3002", "result": "\u6784\u5efa\u7684\u6a21\u578b\u5b9e\u73b0\u4e86\u5bf9\u5e78\u798f\u3001\u653e\u677e\u548c\u60b2\u4f24\u60c5\u7eea\u7684\u5206\u7c7b\uff0c\u51c6\u786e\u7387\u5206\u522b\u4e3a97.21%\u300176%\u548c76%\uff0c\u5e76\u5f00\u53d1\u4e86\u5b9e\u65f6\u60c5\u7eea\u9884\u6d4b\u7b97\u6cd5\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u7684\u6a21\u578b\u80fd\u591f\u6709\u6548\u5730\u8bc6\u522b\u5e76\u5206\u7c7b\u5e78\u798f\u3001\u653e\u677e\u548c\u60b2\u4f24\u7b49\u60c5\u7eea\uff0c\u51c6\u786e\u7387\u5206\u522b\u4e3a97.21%\u300176%\u548c76%\u3002"}}
{"id": "2512.22448", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.22448", "abs": "https://arxiv.org/abs/2512.22448", "authors": ["Peleg Shefi", "Amir Ayali", "Gal A. Kaminka"], "title": "Bugs with Features: Vision-Based Fault-Tolerant Collective Motion Inspired by Nature", "comment": null, "summary": "In collective motion, perceptually-limited individuals move in an ordered manner, without centralized control. The perception of each individual is highly localized, as is its ability to interact with others. While natural collective motion is robust, most artificial swarms are brittle. This particularly occurs when vision is used as the sensing modality, due to ambiguities and information-loss inherent in visual perception. This paper presents mechanisms for robust collective motion inspired by studies of locusts. First, we develop a robust distance estimation method that combines visually perceived horizontal and vertical sizes of neighbors. Second, we introduce intermittent locomotion as a mechanism that allows robots to reliably detect peers that fail to keep up, and disrupt the motion of the swarm. We show how such faulty robots can be avoided in a manner that is robust to errors in classifying them as faulty. Through extensive physics-based simulation experiments, we show dramatic improvements to swarm resilience when using these techniques. We show these are relevant to both distance-based Avoid-Attract models, as well as to models relying on Alignment, in a wide range of experiment settings.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u53d7\u8757\u866b\u542f\u53d1\u7684\u673a\u5236\uff0c\u4ee5\u63d0\u9ad8\u4eba\u5de5\u7fa4\u4f53\u5728\u89c6\u89c9\u611f\u77e5\u4e0b\u7684\u8fd0\u52a8\u97e7\u6027\uff0c\u5305\u62ec\u4e00\u79cd\u65b0\u7684\u8ddd\u79bb\u4f30\u8ba1\u65b9\u6cd5\u548c\u95f4\u6b47\u6027\u8fd0\u52a8\u673a\u5236\u3002", "motivation": "\u89e3\u51b3\u4eba\u5de5\u7fa4\u4f53\u8fd0\u52a8\u5728\u89c6\u89c9\u611f\u77e5\u4e0b\u7684\u8106\u5f31\u6027\u95ee\u9898", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u90bb\u5c45\u89c6\u89c9\u611f\u77e5\u6c34\u5e73\u548c\u5782\u76f4\u5c3a\u5bf8\u7684\u8ddd\u79bb\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u95f4\u6b47\u6027\u8fd0\u52a8\u673a\u5236", "result": "\u901a\u8fc7\u7269\u7406\u6a21\u62df\u5b9e\u9a8c\u5c55\u793a\u4e86\u4f7f\u7528\u8fd9\u4e9b\u6280\u672f\u540e\u7fa4\u4f53\u97e7\u6027\u7684\u663e\u8457\u6539\u5584", "conclusion": "\u8fd9\u9879\u7814\u7a76\u7684\u7ed3\u679c\u63d0\u4f9b\u4e86\u5bf9\u5404\u79cd\u5b9e\u9a8c\u8bbe\u7f6e\u4e2d\u8ddd\u79bb\u57fa\u7840\u907f\u8ba9-\u5438\u5f15\u6a21\u578b\u548c\u4f9d\u8d56\u4e8e\u5bf9\u9f50\u6a21\u578b\u7684\u6539\u8fdb\u3002"}}
{"id": "2512.22404", "categories": ["cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.22404", "abs": "https://arxiv.org/abs/2512.22404", "authors": ["Quanzhi Fu", "Qiyu Wu", "Dan Williams"], "title": "Mining the Gold: Student-AI Chat Logs as Rich Sources for Automated Knowledge Gap Detection", "comment": "6 pages, 2 figures", "summary": "With the significant increase in enrollment in computing-related programs over the past 20 years, lecture sizes have grown correspondingly. In large lectures, instructors face challenges on identifying students' knowledge gaps timely, which is critical for effective teaching. Existing classroom response systems rely on instructor-initiated interactions, which limits their ability to capture the spontaneous knowledge gaps that naturally emerge during lectures. With the widespread adoption of LLMs among students, we recognize these student-AI dialogues as a valuable, student-centered data source for identifying knowledge gaps. In this idea paper, we propose QueryQuilt, a multi-agent LLM framework that automatically detects common knowledge gaps in large-scale lectures by analyzing students' chat logs with AI assistants. QueryQuilt consists of two key components: (1) a Dialogue Agent that responds to student questions while employing probing questions to reveal underlying knowledge gaps, and (2) a Knowledge Gap Identification Agent that systematically analyzes these dialogues to identify knowledge gaps across the student population. By generating frequency distributions of identified gaps, instructors can gain comprehensive insights into class-wide understanding. Our evaluation demonstrates promising results, with QueryQuilt achieving 100% accuracy in identifying knowledge gaps among simulated students and 95% completeness when tested on real student-AI dialogue data. These initial findings indicate the system's potential for facilitate teaching in authentic learning environments. We plan to deploy QueryQuilt in actual classroom settings for comprehensive evaluation, measuring its detection accuracy and impact on instruction.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u540d\u4e3aQueryQuilt\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u6790\u5b66\u751f\u4e0eAI\u5bf9\u8bdd\uff0c\u81ea\u52a8\u8bc6\u522b\u77e5\u8bc6\u7a7a\u767d\uff0c\u5e2e\u52a9\u6559\u5e08\u6539\u5584\u6559\u5b66\u3002", "motivation": "\u968f\u7740\u8ba1\u7b97\u673a\u76f8\u5173\u4e13\u4e1a\u5165\u5b66\u4eba\u6570\u7684\u5927\u5e45\u589e\u52a0\uff0c\u6559\u5e08\u96be\u4ee5\u53ca\u65f6\u8bc6\u522b\u5b66\u751f\u7684\u77e5\u8bc6\u7a7a\u767d\uff0c\u672c\u6587\u65e8\u5728\u5229\u7528\u5b66\u751f\u4e0eAI\u7684\u5bf9\u8bdd\u6570\u636e\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8be5\u7cfb\u7edf\u5305\u62ec\u4e24\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a\u5bf9\u8bdd\u4ee3\u7406\u548c\u77e5\u8bc6\u7a7a\u767d\u8bc6\u522b\u4ee3\u7406\uff0c\u524d\u8005\u901a\u8fc7\u4e0e\u5b66\u751f\u7684\u4e92\u52a8\u63ed\u793a\u77e5\u8bc6\u7a7a\u767d\uff0c\u540e\u8005\u5206\u6790\u5bf9\u8bdd\u5185\u5bb9\u8bc6\u522b\u77e5\u8bc6\u7a7a\u767d\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aQueryQuilt\u7684\u591a\u667a\u80fd\u4f53\u5927\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u5206\u6790\u5b66\u751f\u4e0eAI\u52a9\u624b\u7684\u804a\u5929\u8bb0\u5f55\uff0c\u81ea\u52a8\u8bc6\u522b\u5927\u89c4\u6a21\u8bfe\u5802\u4e2d\u7684\u77e5\u8bc6\u7a7a\u767d\uff0c\u4ece\u800c\u63d0\u9ad8\u6559\u5b66\u6548\u679c\u3002", "conclusion": "QueryQuilt\u5c55\u73b0\u4e86\u5728\u81ea\u52a8\u8bc6\u522b\u77e5\u8bc6\u7a7a\u767d\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u8ba1\u5212\u5728\u5b9e\u9645\u8bfe\u5802\u4e2d\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\uff0c\u4ee5\u9a8c\u8bc1\u5176\u5bf9\u6559\u5b66\u7684\u5f71\u54cd\u3002"}}
{"id": "2512.22484", "categories": ["cs.RO", "math.DG"], "pdf": "https://arxiv.org/pdf/2512.22484", "abs": "https://arxiv.org/abs/2512.22484", "authors": ["Ross L. Hatton", "Yousef Salaman", "Shai Revzen"], "title": "Asymmetric Friction in Geometric Locomotion", "comment": "23 pages, 15 figures", "summary": "Geometric mechanics models of locomotion have provided insight into how robots and animals use environmental interactions to convert internal shape changes into displacement through the world, encoding this relationship in a ``motility map''. A key class of such motility maps arises from (possibly anisotropic) linear drag acting on the system's individual body parts, formally described via Riemannian metrics on the motions of the system's individual body parts. The motility map can then be generated by invoking a sub-Riemannian constraint on the aggregate system motion under which the position velocity induced by a given shape velocity is that which minimizes the power dissipated via friction. The locomotion of such systems is ``geometric'' in the sense that the final position reached by the system depends only on the sequence of shapes that the system passes through, but not on the rate with which the shape changes are made.\n  In this paper, we consider a far more general class of systems in which the drag may be not only anisotropic (with different coefficients for forward/backward and left/right motions), but also asymmetric (with different coefficients for forward and backward motions). Formally, including asymmetry in the friction replaces the Riemannian metrics on the body parts with Finsler metrics. We demonstrate that the sub-Riemannian approach to constructing the system motility map extends naturally to a sub-Finslerian approach and identify system properties analogous to the constraint curvature of sub-Riemannian systems that allow for the characterization of the system motion capabilities.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86\u51e0\u4f55\u529b\u5b66\u6a21\u578b\uff0c\u8003\u8651\u66f4\u5e7f\u6cdb\u7684\u8fd0\u52a8\u7cfb\u7edf\uff0c\u91cd\u70b9\u5206\u6790\u4e0d\u5bf9\u79f0\u6469\u64e6\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e9a\u82ac\u65af\u52d2\u65b9\u6cd5\u3002", "motivation": "\u7406\u89e3\u590d\u6742\u8fd0\u52a8\u7cfb\u7edf\u4e2d\u5f62\u72b6\u53d8\u5316\u4e0e\u4f4d\u79fb\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u4f18\u5316\u5176\u5728\u591a\u6837\u5316\u6469\u64e6\u6761\u4ef6\u4e0b\u7684\u8fd0\u52a8\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u4e9a\u82ac\u65af\u52d2\u5ea6\u91cf\uff0c\u5c06\u4e0d\u5bf9\u79f0\u548c\u5404\u5411\u5f02\u6027\u6469\u64e6\u7eb3\u5165\u5206\u6790\uff0c\u53d1\u5c55\u4e9a\u82ac\u65af\u52d2\u7ea6\u675f\u4ee5\u6784\u5efa\u8fd0\u52a8\u56fe\u3002", "result": "\u672c\u6587\u63a2\u8ba8\u4e86\u51e0\u4f55\u529b\u5b66\u6a21\u578b\u5728\u8fd0\u52a8\u5b66\u4e2d\u7684\u5e94\u7528\uff0c\u5c24\u5176\u662f\u52a8\u7269\u548c\u673a\u5668\u4eba\u5982\u4f55\u901a\u8fc7\u4e0e\u73af\u5883\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5c06\u5185\u90e8\u5f62\u72b6\u53d8\u5316\u8f6c\u5316\u4e3a\u4f4d\u79fb\u3002\u901a\u8fc7\u5f15\u5165\u4e9a\u9ece\u66fc\u7ea6\u675f\uff0c\u4f5c\u8005\u6269\u5c55\u4e86\u4f20\u7edf\u6a21\u578b\u4ee5\u5305\u62ec\u4e0d\u5bf9\u79f0\u548c\u5404\u5411\u5f02\u6027\u7684\u6469\u64e6\uff0c\u63d0\u51fa\u4e86\u4e9a\u82ac\u65af\u52d2\u65b9\u6cd5\u6765\u6784\u5efa\u8fd0\u52a8\u56fe\uff0c\u5e76\u8bc6\u522b\u4e86\u4e0e\u7ea6\u675f\u66f2\u7387\u76f8\u4f3c\u7684\u7cfb\u7edf\u5c5e\u6027\uff0c\u4ee5\u8868\u5f81\u7cfb\u7edf\u7684\u8fd0\u52a8\u80fd\u529b\u3002", "conclusion": "\u9ad8\u6548\u8fd0\u52a8\u7cfb\u7edf\u7684\u6784\u5efa\u9700\u8981\u8003\u8651\u6469\u64e6\u7684\u4e0d\u5bf9\u79f0\u6027\u548c\u5404\u5411\u5f02\u6027\uff0c\u4e9a\u82ac\u65af\u52d2\u65b9\u6cd5\u4e3a\u6b64\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6846\u67b6\u3002"}}
{"id": "2512.22407", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.22407", "abs": "https://arxiv.org/abs/2512.22407", "authors": ["Carl Christopher Haynes-Magyar"], "title": "Learning to Program != \"One-Size-Fits-All\": Exploring Variations of Parsons Problems as Scaffolding", "comment": null, "summary": "Lowering the barriers to computer programming requires understanding how to scaffold learning. Parsons problems, which require learners to drag-and-drop blocks of code into the correct order and indentation, are proving to be beneficial for scaffolding learning how to write code from scratch. But little is known about the ability of other problem types to do so. This study explores learners' perceptions of a new programming environment called Codespec, which was developed to make computer programming more accessible and equitable by offering multiple means of engagement. Retrospective think-aloud interviews were conducted with nine programmers who were given the choice between Faded Parsons and Pseudocode Parsons problems as optional scaffolding toward solving write-code problems. The results showed that offering Faded and Pseudocode Parsons problems as optional scaffolds supported comprehension monitoring, strategy formation, and refinement of prior knowledge. Learners selectively used Faded Parsons problems for syntax/structure and Pseudocode Parsons problems for high-level reasoning. The costs noted included the time it takes to drag-and-drop the blocks and the confusion experienced when a solution diverges from a learners' mental model. Faded Parsons problems were also perceived as a desirable challenge. This study contributes to the field of computing education and human-computer interaction by extending the functionality of problem spaces that support Parsons problems and by providing empirical evidence of the effectiveness of using other problem types as scaffolding techniques.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4e0d\u540c\u7c7b\u578b\u95ee\u9898\uff08Faded\u4e0ePseudocode Parsons\uff09\u4f5c\u4e3a\u7f16\u7a0b\u5b66\u4e60\u652f\u67b6\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u5b83\u4eec\u80fd\u652f\u6301\u5b66\u4e60\u8005\u7684\u7406\u89e3\u548c\u7b56\u7565\u5f62\u6210\u3002", "motivation": "\u964d\u4f4e\u8ba1\u7b97\u673a\u7f16\u7a0b\u7684\u5b66\u4e60\u95e8\u69db\uff0c\u63a2\u7d22\u4e0d\u540c\u95ee\u9898\u7c7b\u578b\uff08\u5982Faded Parsons\u4e0ePseudocode Parsons\uff09\u5bf9\u5b66\u4e60\u8005\u7684\u652f\u6301\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u5bf9\u4e5d\u4f4d\u7a0b\u5e8f\u5458\u8fdb\u884c\u56de\u987e\u6027\u601d\u7ef4\u5927\u58f0\u8868\u8fbe\u8bbf\u8c08\uff0c\u63a2\u8ba8\u4e0d\u540c\u95ee\u9898\u7c7b\u578b\u5bf9\u5b66\u4e60\u8005\u7684\u5f71\u54cd\u3002", "result": "\u5b66\u4e60\u8005\u9009\u62e9\u6027\u4f7f\u7528Faded Parsons\u95ee\u9898\u6765\u89e3\u51b3\u8bed\u6cd5/\u7ed3\u6784\u95ee\u9898\uff0c\u4f7f\u7528Pseudocode Parsons\u95ee\u9898\u8fdb\u884c\u9ad8\u5c42\u6b21\u63a8\u7406\uff0c\u4e14\u5728\u89e3\u51b3\u8fc7\u7a0b\u4e2d\u53d1\u73b0\u4e86\u65f6\u95f4\u6d88\u8017\u548c\u8ba4\u77e5\u6df7\u4e71\u7b49\u95ee\u9898\u3002", "conclusion": "\u4f7f\u7528Faded\u548cPseudocode Parsons\u95ee\u9898\u4f5c\u4e3a\u53ef\u9009\u7684\u652f\u67b6\u80fd\u591f\u6709\u6548\u652f\u6301\u5b66\u4e60\u8005\u7684\u7406\u89e3\u76d1\u63a7\u3001\u7b56\u7565\u5f62\u6210\u548c\u77e5\u8bc6\u7684\u7ec6\u5316\uff0c\u8bc1\u660e\u5176\u5728\u8ba1\u7b97\u673a\u6559\u80b2\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2512.22502", "categories": ["cs.RO", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.22502", "abs": "https://arxiv.org/abs/2512.22502", "authors": ["Shen Changqing", "Xu Bingzhou", "Qi Bosong", "Zhang Xiaojian", "Yan Sijie", "Ding Han"], "title": "Topology-Preserving Scalar Field Optimization for Boundary-Conforming Spiral Toolpaths on Multiply Connected Freeform Surfaces", "comment": "24Pages,12Figures", "summary": "Ball-end milling path planning on multiply connected freeform surfaces is pivotal for high-quality and efficient machining of components in automotive and aerospace manufacturing. Although scalar-field-based optimization provides a unified framework for multi-objective toolpath generation, maintaining boundary conformity while eliminating zero-gradient singularities that cause iso-curve branching or termination and disrupt toolpath continuity remains challenging on multiply connected surfaces. We propose an efficient strategy to robustly enforce these constraints throughout optimization. Conformal slit mapping is employed to construct a feasible, singularity-free initial scalar field. The optimization is reformulated as a topology-preserving mesh deformation governed by boundary-synchronous updates, enabling globally optimized spacing, scallop-height uniformity, and smooth trajectory transitions. Consequently, the toolpaths are continuous, boundary-conforming, and free of self-intersections. Milling experiments demonstrate that, compared with a state-of-the-art conformal slit mapping-based method, the proposed approach increases machining efficiency by 14.24%, improves scallop-height uniformity by 5.70%, and reduces milling impact-induced vibrations by over 10%. The strategy offers broad applicability in high-performance machining scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u591a\u8fde\u901a\u81ea\u7531\u8868\u9762\u4e0a\u6709\u6548\u7684\u7403\u7aef\u94e3\u524a\u8def\u5f84\u89c4\u5212\u7b56\u7565\uff0c\u901a\u8fc7\u4f7f\u7528\u8d34\u5408\u7f1d\u6620\u5c04\u548c\u62d3\u6251\u4fdd\u6301\u7684\u7f51\u683c\u53d8\u5f62\uff0c\u4f18\u5316\u94e3\u524a\u8def\u5f84\u7684\u8fde\u7eed\u6027\u4e0e\u8fb9\u754c\u4e00\u81f4\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u52a0\u5de5\u6548\u7387\u53ca\u8d28\u91cf\u3002", "motivation": "\u5728\u6c7d\u8f66\u548c\u822a\u7a7a\u822a\u5929\u5236\u9020\u4e2d\uff0c\u9ad8\u8d28\u91cf\u548c\u9ad8\u6548\u7684\u7ec4\u4ef6\u52a0\u5de5\u81f3\u5173\u91cd\u8981\uff0c\u800c\u591a\u8fde\u901a\u81ea\u7531\u8868\u9762\u7684\u94e3\u524a\u8def\u5f84\u89c4\u5212\u662f\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u7684\u5173\u952e\u3002", "method": "\u7403\u7aef\u94e3\u524a\u8def\u5f84\u89c4\u5212", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u76f8\u8f83\u4e8e\u5148\u8fdb\u7684\u8d34\u5408\u7f1d\u6620\u5c04\u65b9\u6cd5\u5728\u52a0\u5de5\u6548\u7387\u4e0a\u63d0\u9ad8\u4e8614.24%\uff0c\u5728\u51f9\u69fd\u9ad8\u5ea6\u5747\u5300\u6027\u4e0a\u63d0\u9ad8\u4e865.70%\uff0c\u5e76\u4e14\u51cf\u5c11\u4e86\u8d85\u8fc710%\u7684\u94e3\u524a\u5f71\u54cd\u5f15\u8d77\u7684\u632f\u52a8\u3002", "conclusion": "\u8be5\u7b56\u7565\u5728\u9ad8\u6027\u80fd\u52a0\u5de5\u5e94\u7528\u4e2d\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2512.22462", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.22462", "abs": "https://arxiv.org/abs/2512.22462", "authors": ["Jiatao Quan", "Ziyue Li", "Tian Qi Zhu", "Yuxuan Li", "Baoying Wang", "Wanda Pratt", "Nan Gao"], "title": "Relational Mediators: LLM Chatbots as Boundary Objects in Psychotherapy", "comment": "To be published in CSCW 2026, Issue 2 of PACMHCI, 35 pages, 6 figures", "summary": "As large language models (LLMs) are embedded into mental health technologies, they are often framed either as tools assisting therapists or autonomous therapeutic systems. Such perspectives overlook their potential to mediate relational complexities in therapy, particularly for systemically marginalized clients. Drawing on in-depth interviews with 12 therapists and 12 marginalized clients in China, including LGBTQ+ individuals or those from other marginalized backgrounds, we identify enduring relational challenges: difficulties building trust amid institutional barriers, the burden clients carry in educating therapists about marginalized identities, and challenges sustaining authentic self-disclosure across therapy and daily life. We argue that addressing these challenges requires AI systems capable of actively mediating underlying knowledge gaps, power asymmetries, and contextual disconnects. To this end, we propose the Dynamic Boundary Mediation Framework, which reconceptualizes LLM-enhanced systems as adaptive boundary objects that shift mediating roles across therapeutic stages. The framework delineates three forms of mediation: Epistemic (reducing knowledge asymmetries), Relational (rebalancing power dynamics), and Contextual (bridging therapy-life discontinuities). This framework offers a pathway toward designing relationally accountable AI systems that center the lived realities of marginalized users and more effectively support therapeutic relationships.", "AI": {"tldr": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5fc3\u7406\u5065\u5eb7\u6280\u672f\u4e2d\u7684\u5e94\u7528\uff0c\u9488\u5bf9\u8fb9\u7f18\u5316\u7fa4\u4f53\u7684\u6cbb\u7597\u6311\u6218\uff0c\u63d0\u51fa\u52a8\u6001\u8fb9\u754c\u4e2d\u4ecb\u6846\u67b6\u4ee5\u6539\u5584\u7597\u6548\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u88ab\u5f15\u5165\u5fc3\u7406\u5065\u5eb7\u9886\u57df\uff0c\u73b0\u6709\u89c6\u89d2\u672a\u80fd\u5145\u5206\u8003\u8651\u5176\u5728\u6cbb\u7597\u5173\u7cfb\u4e2d\u7684\u590d\u6742\u6027\uff0c\u7279\u522b\u662f\u5bf9\u5236\u5ea6\u6027\u8fb9\u7f18\u5316\u5ba2\u6237\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5bf912\u540d\u6cbb\u7597\u5e08\u548c12\u540d\u8fb9\u7f18\u5316\u5ba2\u6237\u7684\u6df1\u5165\u8bbf\u8c08\u6536\u96c6\u6570\u636e\uff0c\u5206\u6790\u5173\u7cfb\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u52a8\u6001\u8fb9\u754c\u4e2d\u4ecb\u6846\u67b6\u3002", "result": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5fc3\u7406\u5065\u5eb7\u6280\u672f\u4e2d\u7684\u5e94\u7528\uff0c\u5c24\u5176\u662f\u5176\u5728\u6cbb\u7597\u4e2d\u7684\u4e2d\u4ecb\u4f5c\u7528\uff0c\u7279\u522b\u662f\u9488\u5bf9\u8fb9\u7f18\u5316\u7fa4\u4f53\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u5bf912\u540d\u6cbb\u7597\u5e08\u548c12\u540d\u8fb9\u7f18\u5316\u5ba2\u6237\u7684\u6df1\u5165\u8bbf\u8c08\uff0c\u8bc6\u522b\u51fa\u5efa\u7acb\u4fe1\u4efb\u3001\u5ba2\u6237\u6559\u80b2\u6cbb\u7597\u5e08\u7684\u8d1f\u62c5\u4ee5\u53ca\u5728\u6cbb\u7597\u4e0e\u65e5\u5e38\u751f\u6d3b\u4e4b\u95f4\u4fdd\u6301\u771f\u5b9e\u81ea\u6211\u62ab\u9732\u7684\u6311\u6218\u3002\u63d0\u51fa\u52a8\u6001\u8fb9\u754c\u4e2d\u4ecb\u6846\u67b6\uff0c\u4ee5\u5e2e\u52a9\u8bbe\u8ba1\u66f4\u5177\u8d23\u4efb\u611f\u7684AI\u7cfb\u7edf\uff0c\u652f\u6301\u8fb9\u7f18\u5316\u7528\u6237\u7684\u6cbb\u7597\u5173\u7cfb\u3002", "conclusion": "\u672c\u6587\u5efa\u8baeAI\u7cfb\u7edf\u5e94\u79ef\u6781\u4ecb\u5165\u77e5\u8bc6\u5dee\u8ddd\u3001\u6743\u529b\u4e0d\u5bf9\u79f0\u548c\u60c5\u5883\u65ad\u88c2\u7b49\u95ee\u9898\uff0c\u4ee5\u66f4\u597d\u5730\u670d\u52a1\u4e8e\u8fb9\u7f18\u5316\u7528\u6237\uff0c\u5b9e\u73b0\u6709\u6548\u7684\u6cbb\u7597\u5173\u7cfb\u3002"}}
{"id": "2512.22519", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.22519", "abs": "https://arxiv.org/abs/2512.22519", "authors": ["Khoa Vo", "Taisei Hanyu", "Yuki Ikebe", "Trong Thang Pham", "Nhat Chung", "Minh Nhat Vu", "Duy Nguyen Ho Minh", "Anh Nguyen", "Anthony Gunderman", "Chase Rainwater", "Ngan Le"], "title": "Clutter-Resistant Vision-Language-Action Models through Object-Centric and Geometry Grounding", "comment": "Under review. Project website: https://uark-aicv.github.io/OBEYED_VLA", "summary": "Recent Vision-Language-Action (VLA) models have made impressive progress toward general-purpose robotic manipulation by post-training large Vision-Language Models (VLMs) for action prediction. Yet most VLAs entangle perception and control in a monolithic pipeline optimized purely for action, which can erode language-conditioned grounding. In our real-world tabletop tests, policies over-grasp when the target is absent, are distracted by clutter, and overfit to background appearance.\n  To address these issues, we propose OBEYED-VLA (OBject-centric and gEometrY groundED VLA), a framework that explicitly disentangles perceptual grounding from action reasoning. Instead of operating directly on raw RGB, OBEYED-VLA augments VLAs with a perception module that grounds multi-view inputs into task-conditioned, object-centric, and geometry-aware observations. This module includes a VLM-based object-centric grounding stage that selects task-relevant object regions across camera views, along with a complementary geometric grounding stage that emphasizes the 3D structure of these objects over their appearance. The resulting grounded views are then fed to a pretrained VLA policy, which we fine-tune exclusively on single-object demonstrations collected without environmental clutter or non-target objects.\n  On a real-world UR10e tabletop setup, OBEYED-VLA substantially improves robustness over strong VLA baselines across four challenging regimes and multiple difficulty levels: distractor objects, absent-target rejection, background appearance changes, and cluttered manipulation of unseen objects. Ablation studies confirm that both semantic grounding and geometry-aware grounding are critical to these gains. Overall, the results indicate that making perception an explicit, object-centric component is an effective way to strengthen and generalize VLA-based robotic manipulation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faOBEYED-VLA\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u4e2d\u7684\u611f\u77e5\u548c\u63a7\u5236\u7ea0\u7f20\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u64cd\u63a7\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u9488\u5bf9\u4f20\u7edfVLA\u6a21\u578b\u5728\u611f\u77e5\u4e0e\u63a7\u5236\u4e0a\u5b58\u5728\u7684\u7ea0\u7f20\u95ee\u9898\uff0c\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u660e\u786e\u5206\u79bb\u611f\u77e5 grounding \u548c\u52a8\u4f5c\u63a8\u7406\u7684\u6846\u67b6\uff0c\u4ee5\u63d0\u5347\u673a\u5668\u4eba\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "method": "OBEYED-VLA\u901a\u8fc7\u4e00\u4e2a\u611f\u77e5\u6a21\u5757\uff0c\u5c06\u591a\u89c6\u89d2\u8f93\u5165\u8f6c\u5316\u4e3a\u4efb\u52a1\u76f8\u5173\u7684\u3001\u5bf9\u8c61\u4e2d\u5fc3\u548c\u51e0\u4f55\u611f\u77e5\u7684\u89c2\u5bdf\u3002\u540c\u65f6\uff0c\u8be5\u6846\u67b6\u501f\u52a9VLM\u8fdb\u884c\u5bf9\u8c61\u4e2d\u5fc3\u7684\u8bed\u4e49\u57fa\u7840\uff0c\u4f7f\u5f97\u673a\u5668\u4eba\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e2d\u8fdb\u884c\u6709\u6548\u64cd\u63a7\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u663e\u8457\u6539\u8fdb\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6846\u67b6OBEYED-VLA\uff0c\u901a\u8fc7\u5c06\u611f\u77e5\u548c\u52a8\u4f5c\u63a8\u7406\u660e\u786e\u5206\u79bb\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u5b58\u5728\u7684\u95ee\u9898\uff0c\u4f7f\u5f97\u673a\u5668\u4eba\u64cd\u63a7\u66f4\u52a0\u7a33\u5065\u4e14\u66f4\u52a0\u901a\u7528\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u5bf9\u8c61\u4e2d\u5fc3\u548c\u51e0\u4f55\u611f\u77e5\u7684\u6a21\u5757\uff0cOBEYED-VLA\u5728\u591a\u79cd\u590d\u6742\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u663e\u8457\u7684\u9c81\u68d2\u6027\uff0c\u8868\u660e\u8fd9\u79cd\u65b9\u6cd5\u6709\u6548\u589e\u5f3a\u4e86VLA\u57fa\u7840\u4e0a\u7684\u673a\u5668\u4eba\u64cd\u63a7\u80fd\u529b\u3002"}}
{"id": "2512.22481", "categories": ["cs.HC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22481", "abs": "https://arxiv.org/abs/2512.22481", "authors": ["Zihan Weng", "Chanlin Yi", "Pouya Bashivan", "Jing Lu", "Fali Li", "Dezhong Yao", "Jingming Hou", "Yangsong Zhang", "Peng Xu"], "title": "SPECTRE: Spectral Pre-training Embeddings with Cylindrical Temporal Rotary Position Encoding for Fine-Grained sEMG-Based Movement Decoding", "comment": null, "summary": "Decoding fine-grained movement from non-invasive surface Electromyography (sEMG) is a challenge for prosthetic control due to signal non-stationarity and low signal-to-noise ratios. Generic self-supervised learning (SSL) frameworks often yield suboptimal results on sEMG as they attempt to reconstruct noisy raw signals and lack the inductive bias to model the cylindrical topology of electrode arrays. To overcome these limitations, we introduce SPECTRE, a domain-specific SSL framework. SPECTRE features two primary contributions: a physiologically-grounded pre-training task and a novel positional encoding. The pre-training involves masked prediction of discrete pseudo-labels from clustered Short-Time Fourier Transform (STFT) representations, compelling the model to learn robust, physiologically relevant frequency patterns. Additionally, our Cylindrical Rotary Position Embedding (CyRoPE) factorizes embeddings along linear temporal and annular spatial dimensions, explicitly modeling the forearm sensor topology to capture muscle synergies. Evaluations on multiple datasets, including challenging data from individuals with amputation, demonstrate that SPECTRE establishes a new state-of-the-art for movement decoding, significantly outperforming both supervised baselines and generic SSL approaches. Ablation studies validate the critical roles of both spectral pre-training and CyRoPE. SPECTRE provides a robust foundation for practical myoelectric interfaces capable of handling real-world sEMG complexities.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411sEMG\u7684\u81ea\u6211\u76d1\u7763\u5b66\u4e60\u6846\u67b6SPECTRE\uff0c\u901a\u8fc7\u751f\u7406\u5b66\u57fa\u7840\u7684\u9884\u8bad\u7ec3\u4efb\u52a1\u548c\u65b0\u9896\u7684\u4f4d\u7f6e\u4fe1\u606f\u7f16\u7801\u6765\u89e3\u51b3\u8fd0\u52a8\u89e3\u7801\u7684\u6311\u6218\u3002", "motivation": "\u514b\u670dsEMG\u4fe1\u53f7\u7684\u975e\u5e73\u7a33\u6027\u548c\u4f4e\u4fe1\u566a\u6bd4\u5bf9\u5047\u80a2\u63a7\u5236\u7684\u6311\u6218", "method": "SPECTRE: \u4e00\u4e2a\u9886\u57df\u7279\u5b9a\u7684\u81ea\u6211\u76d1\u7763\u5b66\u4e60\u6846\u67b6", "result": "SPECTRE\u5728\u8fd0\u52a8\u89e3\u7801\u4e0a\u521b\u5efa\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u663e\u8457\u4f18\u4e8e\u76d1\u7763\u57fa\u7ebf\u548c\u901a\u7528\u81ea\u6211\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5", "conclusion": "SPECTRE\u4e3a\u80fd\u591f\u5904\u7406\u771f\u5b9e\u4e16\u754csEMG\u590d\u6742\u6027\u7684\u808c\u7535\u63a5\u53e3\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u57fa\u7840\u3002"}}
{"id": "2512.22539", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.22539", "abs": "https://arxiv.org/abs/2512.22539", "authors": ["Borong Zhang", "Jiahao Li", "Jiachen Shen", "Yishuai Cai", "Yuhao Zhang", "Yuanpei Chen", "Juntao Dai", "Jiaming Ji", "Yaodong Yang"], "title": "VLA-Arena: An Open-Source Framework for Benchmarking Vision-Language-Action Models", "comment": null, "summary": "While Vision-Language-Action models (VLAs) are rapidly advancing towards generalist robot policies, it remains difficult to quantitatively understand their limits and failure modes. To address this, we introduce a comprehensive benchmark called VLA-Arena. We propose a novel structured task design framework to quantify difficulty across three orthogonal axes: (1) Task Structure, (2) Language Command, and (3) Visual Observation. This allows us to systematically design tasks with fine-grained difficulty levels, enabling a precise measurement of model capability frontiers. For Task Structure, VLA-Arena's 170 tasks are grouped into four dimensions: Safety, Distractor, Extrapolation, and Long Horizon. Each task is designed with three difficulty levels (L0-L2), with fine-tuning performed exclusively on L0 to assess general capability. Orthogonal to this, language (W0-W4) and visual (V0-V4) perturbations can be applied to any task to enable a decoupled analysis of robustness. Our extensive evaluation of state-of-the-art VLAs reveals several critical limitations, including a strong tendency toward memorization over generalization, asymmetric robustness, a lack of consideration for safety constraints, and an inability to compose learned skills for long-horizon tasks. To foster research addressing these challenges and ensure reproducibility, we provide the complete VLA-Arena framework, including an end-to-end toolchain from task definition to automated evaluation and the VLA-Arena-S/M/L datasets for fine-tuning. Our benchmark, data, models, and leaderboard are available at https://vla-arena.github.io.", "AI": {"tldr": "VLA-Arena\u57fa\u51c6\u901a\u8fc7\u7cfb\u7edf\u8bbe\u8ba1\u4efb\u52a1\u96be\u5ea6\uff0c\u63ed\u793a\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u5de5\u5177\u548c\u6570\u636e\u652f\u6301\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u667a\u80fd\u673a\u5668\u4eba\u653f\u7b56\u65b9\u9762\u53d6\u5f97\u5feb\u901f\u8fdb\u5c55\uff0c\u4f46\u5176\u5c40\u9650\u6027\u548c\u5931\u8d25\u6a21\u5f0f\u7684\u5b9a\u91cf\u7406\u89e3\u4ecd\u7136\u56f0\u96be\u3002", "method": "\u8bbe\u8ba1\u4e86VLA-Arena\u57fa\u51c6\uff0c\u901a\u8fc7\u4efb\u52a1\u7ed3\u6784\u3001\u8bed\u8a00\u547d\u4ee4\u548c\u89c6\u89c9\u89c2\u5bdf\u4e09\u4e2a\u7ef4\u5ea6\u7cfb\u7edf\u6027\u5730\u91cf\u5316\u4efb\u52a1\u96be\u5ea6\uff0c\u5e76\u8fdb\u884c\u5e7f\u6cdb\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u8868\u660e\u73b0\u6709VLA\u7cfb\u7edf\u5728\u6cdb\u5316\u3001\u7a33\u5065\u6027\u3001\u5b89\u5168\u6027\u548c\u957f\u65f6\u7a0b\u4efb\u52a1\u7ec4\u6210\u7b49\u65b9\u9762\u5b58\u5728\u660e\u663e\u7f3a\u9677\u3002", "conclusion": "VLA-Arena\u57fa\u51c6\u63d0\u4f9b\u4e86\u4efb\u52a1\u8bbe\u8ba1\u6846\u67b6\u53ca\u8bc4\u4f30\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u5173\u952e\u5c40\u9650\u6027\uff0c\u4fc3\u8fdb\u9488\u5bf9\u8fd9\u4e9b\u6311\u6218\u7684\u7814\u7a76\u4e0e\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2512.22656", "categories": ["cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22656", "abs": "https://arxiv.org/abs/2512.22656", "authors": ["Argha Kamal Samanta", "Deepak Mewada", "Monalisa Sarma", "Debasis Samanta"], "title": "Clinically Calibrated Machine Learning Benchmarks for Large-Scale Multi-Disorder EEG Classification", "comment": null, "summary": "Clinical electroencephalography is routinely used to evaluate patients with diverse and often overlapping neurological conditions, yet interpretation remains manual, time-intensive, and variable across experts. While automated EEG analysis has been widely studied, most existing methods target isolated diagnostic problems, particularly seizure detection, and provide limited support for multi-disorder clinical screening.\n  This study examines automated EEG-based classification across eleven clinically relevant neurological disorder categories, encompassing acute time-critical conditions, chronic neurocognitive and developmental disorders, and disorders with indirect or weak electrophysiological signatures. EEG recordings are processed using a standard longitudinal bipolar montage and represented through a multi-domain feature set capturing temporal statistics, spectral structure, signal complexity, and inter-channel relationships. Disorder-aware machine learning models are trained under severe class imbalance, with decision thresholds explicitly calibrated to prioritize diagnostic sensitivity.\n  Evaluation on a large, heterogeneous clinical EEG dataset demonstrates that sensitivity-oriented modeling achieves recall exceeding 80% for the majority of disorder categories, with several low-prevalence conditions showing absolute recall gains of 15-30% after threshold calibration compared to default operating points. Feature importance analysis reveals physiologically plausible patterns consistent with established clinical EEG markers.\n  These results establish realistic performance baselines for multi-disorder EEG classification and provide quantitative evidence that sensitivity-prioritized automated analysis can support scalable EEG screening and triage in real-world clinical settings.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf911\u79cd\u795e\u7ecf\u75be\u75c5\u7684\u81ea\u52a8\u5316EEG\u5206\u7c7b\u65b9\u6cd5\uff0c\u652f\u6301\u591a\u75be\u75c5\u7684\u4e34\u5e8a\u7b5b\u67e5\uff0c\u4e14\u5728\u7075\u654f\u5ea6\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u81ea\u52a8\u5316EEG\u5206\u6790\u5728\u4e34\u5e8a\u7b5b\u67e5\u591a\u79cd\u91cd\u53e0\u795e\u7ecf\u75be\u75c5\u4e2d\u7684\u5e94\u7528\u9700\u6c42", "method": "\u4f7f\u7528\u6807\u51c6\u7684\u7eb5\u5411\u53cc\u6781\u5bfc\u8054\u5904\u7406EEG\u8bb0\u5f55\uff0c\u5e76\u901a\u8fc7\u591a\u9886\u57df\u7279\u5f81\u96c6\u8fdb\u884c\u8868\u793a", "result": "\u5bf9\u4e8e\u5927\u591a\u6570\u75be\u75c5\u7c7b\u522b\uff0c\u654f\u611f\u6027\u5bfc\u5411\u5efa\u6a21\u7684\u53ec\u56de\u7387\u8d85\u8fc780%\uff0c\u9608\u503c\u6821\u51c6\u540e\u4f4e\u6d41\u884c\u75c5\u72b6\u51b5\u7684\u7edd\u5bf9\u53ec\u56de\u7387\u63d0\u9ad8\u4e8615-30%", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u591a\u75be\u75c5EEG\u5206\u7c7b\u5efa\u7acb\u4e86\u73b0\u5b9e\u7684\u6027\u80fd\u57fa\u7ebf\uff0c\u8bc1\u660e\u4e86\u654f\u611f\u6027\u4f18\u5148\u7684\u81ea\u52a8\u5316\u5206\u6790\u53ef\u4ee5\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u652f\u6301\u53ef\u6269\u5c55\u7684EEG\u7b5b\u67e5\u548c\u5206\u8bca\u3002"}}
{"id": "2512.22575", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.22575", "abs": "https://arxiv.org/abs/2512.22575", "authors": ["Xuewei Zhang", "Bailing Tian", "Kai Zheng", "Yulin Hui", "Junjie Lu", "Zhiyu Li"], "title": "ParaMaP: Parallel Mapping and Collision-free Motion Planning for Reactive Robot Manipulation", "comment": null, "summary": "Real-time and collision-free motion planning remains challenging for robotic manipulation in unknown environments due to continuous perception updates and the need for frequent online replanning. To address these challenges, we propose a parallel mapping and motion planning framework that tightly integrates Euclidean Distance Transform (EDT)-based environment representation with a sampling-based model predictive control (SMPC) planner. On the mapping side, a dense distance-field-based representation is constructed using a GPU-based EDT and augmented with a robot-masked update mechanism to prevent false self-collision detections during online perception. On the planning side, motion generation is formulated as a stochastic optimization problem with a unified objective function and efficiently solved by evaluating large batches of candidate rollouts in parallel within a SMPC framework, in which a geometrically consistent pose tracking metric defined on SE(3) is incorporated to ensure fast and accurate convergence to the target pose. The entire mapping and planning pipeline is implemented on the GPU to support high-frequency replanning. The effectiveness of the proposed framework is validated through extensive simulations and real-world experiments on a 7-DoF robotic manipulator. More details are available at: https://zxw610.github.io/ParaMaP.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408EDT\u8868\u793a\u548cSMPC\u7b97\u6cd5\u7684\u5e76\u884c\u89c4\u5212\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u672a\u77e5\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u65e0\u78b0\u649e\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\uff0c\u7ecf\u8fc7\u591a\u6b21\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u5728\u672a\u77e5\u73af\u5883\u4e2d\u5b9e\u65f6\u8fd0\u52a8\u89c4\u5212\u548c\u78b0\u649e\u907f\u514d\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u73af\u5883\u611f\u77e5\u66f4\u65b0\u548c\u5728\u7ebf\u91cd\u89c4\u5212\u9891\u7e41\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u7ed3\u5408\u57fa\u4e8e\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u53d8\u6362\u7684\u73af\u5883\u8868\u793a\u4e0e\u91c7\u6837\u57fa\u7840\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u89c4\u5212\u5668\uff0c\u6784\u5efa\u5bc6\u96c6\u8ddd\u79bb\u573a\u5e76\u5b9e\u65bd\u9ad8\u9891\u91cd\u89c4\u5212\uff0c\u4f7f\u7528GPU\u52a0\u901f\u3002", "result": "\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u57287\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u4e0a\u7684\u5927\u91cf\u6a21\u62df\u548c\u5b9e\u9645\u5b9e\u9a8c\u5c55\u793a\u4e86\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u5e76\u884c\u5236\u56fe\u548c\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\u6709\u6548\u5b9e\u73b0\u4e86\u5b9e\u65f6\u65e0\u78b0\u649e\u7684\u8fd0\u52a8\u89c4\u5212\uff0c\u7ecf\u8fc7\u5e7f\u6cdb\u7684\u6a21\u62df\u548c\u5b9e\u9645\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2512.22747", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.22747", "abs": "https://arxiv.org/abs/2512.22747", "authors": ["Jaclyn Ocumpaugh", "Zhanlan Wei", "Amanda Barany", "Xiner Liu", "Andres Felipe Zambrano", "Ryan Baker", "Camille Gioradno"], "title": "What do you say? A pilot study investigating student responses in Data Driven Classroom Interviews", "comment": "White Paper published at the University of Houston", "summary": "Data that contextualizes student interactions with online learning systems can be challenging to obtain. This study looks at the rhetorical strategies of a novel method for conducting in-the-moment Data-Driven Classroom Interviews (DDCIs). By using Ordered Network Analysis (ONA) to reanalyze data from Wei et al.'s (2025) Epistemic Network Analysis, we better account for the sequences in which these rhetorical strategies emerge during the interview process. Specifically, we examine how five rhetorical strategies by interviewers relate to five possible rhetorical strategies used in student responses. As with the previous study, results demonstrate minor differences in how students with high and low situational interest respond. Namely, whereas students with high situational interest show moderately higher levels of enthusiasm, students with low situational interest are more likely to respond to interviewers with an explanation. However, overall this study confirms that there are few interviewer-driven differences in these interviews, and it documents that interviewers are following guidelines to rely upon open-ended questions", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u8bfe\u5802\u8bbf\u8c08\u4e2d\u4f7f\u7528\u7684\u4fee\u8f9e\u7b56\u7565\uff0c\u5206\u6790\u4e86\u8bbf\u8c08\u8005\u548c\u5b66\u751f\u4e4b\u95f4\u7684\u4e92\u52a8\u6a21\u5f0f\uff0c\u53d1\u73b0\u9ad8\u4f4e\u60c5\u5883\u5174\u8da3\u7684\u5b66\u751f\u5728\u56de\u5e94\u4e2d\u7684\u8868\u73b0\u5b58\u5728\u7ec6\u5fae\u5dee\u522b\u3002", "motivation": "\u83b7\u53d6\u5b66\u751f\u4e0e\u5728\u7ebf\u5b66\u4e60\u7cfb\u7edf\u4e92\u52a8\u7684\u4e0a\u4e0b\u6587\u6570\u636e\u8f83\u4e3a\u56f0\u96be\uff0c\u56e0\u6b64\u63a2\u7d22\u4fee\u8f9e\u7b56\u7565\u4ee5\u63d0\u5347\u8bbf\u8c08\u8d28\u91cf\u5177\u6709\u91cd\u8981\u6027\u3002", "method": "\u4f7f\u7528\u6709\u5e8f\u7f51\u7edc\u5206\u6790\uff08ONA\uff09\u91cd\u5206\u6790Wei\u7b49\u4eba\uff082025\uff09\u7684\u77e5\u8bc6\u7f51\u7edc\u5206\u6790\u6570\u636e\uff0c\u7814\u7a76\u8bbf\u8c08\u8fc7\u7a0b\u4e2d\u7684\u4fee\u8f9e\u7b56\u7565\u5e8f\u5217\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\u9ad8\u60c5\u5883\u5174\u8da3\u7684\u5b66\u751f\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u70ed\u60c5\uff0c\u800c\u4f4e\u60c5\u5883\u5174\u8da3\u7684\u5b66\u751f\u66f4\u503e\u5411\u4e8e\u63d0\u4f9b\u89e3\u91ca\uff0c\u4f46\u603b\u4f53\u8bbf\u8c08\u8005\u9a71\u52a8\u7684\u5dee\u5f02\u5f88\u5c0f\u3002", "conclusion": "\u672c\u7814\u7a76\u786e\u8ba4\u4e86\u8bbf\u8c08\u8005\u5728\u8bbf\u8c08\u8fc7\u7a0b\u4e2d\u9075\u5faa\u5f00\u653e\u5f0f\u63d0\u95ee\u7684\u6307\u5bfc\u65b9\u9488\uff0c\u5e76\u8bb0\u5f55\u4e86\u8bbf\u8c08\u8005\u9a71\u52a8\u5dee\u5f02\u8f83\u5c11\u7684\u73b0\u8c61\u3002"}}
{"id": "2512.22588", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.22588", "abs": "https://arxiv.org/abs/2512.22588", "authors": ["Max Beffert", "Andreas Zell"], "title": "Modeling of UAV Tether Aerodynamics for Real-Time Simulation", "comment": null, "summary": "One of the main limitations of multirotor UAVs is their short flight time due to battery constraints. A practical solution for continuous operation is to power the drone from the ground via a tether. While this approach has been demonstrated for stationary systems, scenarios with a fast-moving base vehicle or strong wind conditions require modeling the tether forces, including aerodynamic effects. In this work, we propose two complementary approaches for real-time quasi-static tether modeling with aerodynamics. The first is an analytical method based on catenary theory with a uniform drag assumption, achieving very fast solve times below 1ms. The second is a numerical method that discretizes the tether into segments and lumped masses, solving the equilibrium equations using CasADi and IPOPT. By leveraging initialization strategies, such as warm starting and analytical initialization, real-time performance was achieved with a solve time of 5ms, while allowing for flexible force formulations. Both approaches were validated in real-world tests using a load cell to measure the tether force. The results show that the analytical method provides sufficient accuracy for most tethered UAV applications with minimal computational cost, while the numerical method offers higher flexibility and physical accuracy when required. These approaches form a lightweight and extensible framework for real-time tether simulation, applicable to both offline optimization and online tasks such as simulation, control, and trajectory planning.", "AI": {"tldr": "\u9488\u5bf9\u591a\u65cb\u7ffc\u65e0\u4eba\u673a\u7684\u7eed\u822a\u9650\u5236\uff0c\u63d0\u51fa\u4e24\u79cd\u8003\u8651\u6c14\u52a8\u6548\u5e94\u7684\u5b9e\u65f6\u7cfb\u7ef3\u5efa\u6a21\u65b9\u6cd5\uff0c\u5206\u522b\u4e3a\u5feb\u901f\u5206\u6790\u6cd5\u548c\u7075\u6d3b\u7684\u6570\u503c\u6cd5\uff0c\u5747\u901a\u8fc7\u5b9e\u9645\u6d4b\u8bd5\u9a8c\u8bc1\u3002", "motivation": "\u56e0\u65e0\u4eba\u673a\u7535\u6c60\u9650\u5236\uff0c\u77ed\u7eed\u822a\u65f6\u95f4\u662f\u591a\u65cb\u7ffc\u65e0\u4eba\u673a\u7684\u4e00\u5927\u5c40\u9650\uff0c\u901a\u8fc7\u5730\u9762\u7cfb\u7ef3\u4f9b\u7535\u53ef\u5b9e\u73b0\u6301\u7eed\u64cd\u4f5c\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u5b9e\u65f6\u51c6\u9759\u6001\u7684\u7cfb\u7ef3\u5efa\u6a21\u65b9\u6cd5\uff0c\u4ee5\u8003\u8651\u6c14\u52a8\u6548\u5e94\u3002", "result": "\u4e24\u79cd\u65b9\u6cd5\u90fd\u5728\u5b9e\u9645\u6d4b\u8bd5\u4e2d\u5f97\u5230\u9a8c\u8bc1\uff0c\u8868\u660e\u5206\u6790\u65b9\u6cd5\u5728\u5927\u591a\u6570\u7cfb\u7ef3\u65e0\u4eba\u673a\u5e94\u7528\u4e2d\u8db3\u591f\u51c6\u786e\uff0c\u800c\u6570\u503c\u65b9\u6cd5\u5728\u9700\u8981\u65f6\u63d0\u4f9b\u66f4\u9ad8\u7684\u7075\u6d3b\u6027\u548c\u7269\u7406\u51c6\u786e\u6027\u3002", "conclusion": "\u8fd9\u4e24\u79cd\u65b9\u6cd5\u5f62\u6210\u4e86\u4e00\u4e2a\u8f7b\u91cf\u4e14\u53ef\u6269\u5c55\u7684\u5b9e\u65f6\u7cfb\u7ef3\u4eff\u771f\u6846\u67b6\uff0c\u53ef\u5e94\u7528\u4e8e\u79bb\u7ebf\u4f18\u5316\u548c\u5728\u7ebf\u4efb\u52a1\u3002"}}
{"id": "2512.22790", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.22790", "abs": "https://arxiv.org/abs/2512.22790", "authors": ["Geoff Kimm", "Linus Tan"], "title": "ChatGraPhT: A Visual Conversation Interface for Multi-Path Reflection with Agentic LLM Support", "comment": "Early-stage research prototype; exploratory study", "summary": "Large Language Models (LLMs) are increasingly used in complex knowledge work, yet linear transcript interfaces limit support for reflection. Schon's Reflective Practice distinguishes between reflection-in-action (during a task) and reflection-on-action (after a task), both benefiting from non-linear, revisitable representations of dialogue. ChatGraPhT is an interactive tool that shows dialogue as a visual map, allowing users to branch and merge ideas, edit past messages, and receive guidance that prompts deeper reflection. It supports non-linear, multi-path dialogue, while two agentic LLM assistants provide moment-to-moment and higher-level guidance. Our inquiry suggests that keeping the conversation structure visible, allowing branching and merging, and suggesting patterns or ways to combine ideas deepened user reflective engagement. Contributions are: (1) the design of a node-link, agentic LLM interface for reflective dialogue, and (2) transferable design knowledge on balancing structure and AI support to sustain reflection in complex, open-ended tasks.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aChatGraPhT\u7684\u4e92\u52a8\u5de5\u5177\uff0c\u901a\u8fc7\u53ef\u89c6\u5316\u5bf9\u8bdd\u5730\u56fe\u4fc3\u8fdb\u7528\u6237\u7684\u53cd\u601d\u548c\u6df1\u5ea6\u601d\u8003\u3002", "motivation": "\u5f53\u524d\u7684\u7ebf\u6027\u8f6c\u5f55\u754c\u9762\u9650\u5236\u4e86\u53cd\u601d\u7684\u652f\u6301\uff0c\u56e0\u6b64\u9700\u8981\u975e\u7ebf\u6027\u3001\u53ef\u91cd\u8bbf\u7684\u5bf9\u8bdd\u8868\u793a\u3002", "method": "\u91c7\u7528\u53ef\u89c6\u5316\u7684\u8282\u70b9-\u94fe\u63a5\u63a5\u53e3\uff0c\u5e76\u7531\u4e24\u4e2a\u8bed\u8a00\u6a21\u578b\u52a9\u624b\u63d0\u4f9b\u6301\u7eed\u7684\u6307\u5bfc\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u4fdd\u6301\u5bf9\u8bdd\u7ed3\u6784\u53ef\u89c1\u3001\u5141\u8bb8\u5206\u652f\u548c\u5408\u5e76\u3001\u4ee5\u53ca\u5efa\u8bae\u601d\u7ef4\u6a21\u5f0f\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u7684\u53cd\u601d\u53c2\u4e0e\u5ea6\u3002", "conclusion": "ChatGraPhT\u5de5\u5177\u901a\u8fc7\u53ef\u89c6\u5316\u5bf9\u8bdd\u7ed3\u6784\u548c\u63d0\u4f9b\u667a\u80fd\u652f\u6301\uff0c\u663e\u8457\u589e\u5f3a\u7528\u6237\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u53cd\u601d\u80fd\u529b\u3002"}}
{"id": "2512.22734", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.22734", "abs": "https://arxiv.org/abs/2512.22734", "authors": ["Michelle Valenzuela", "Francisco Leiva", "Javier Ruiz-del-Solar"], "title": "Sistema de navegaci\u00f3n de cobertura para veh\u00edculos no holon\u00f3micos en ambientes de exterior", "comment": "13 pages, in Spanish language, 12 figures, accepted at Tercer Congreso Iberoamericano de Miner\u00eda Subterranea y a Cielo Abierto, UMining 2024", "summary": "In mobile robotics, coverage navigation refers to the deliberate movement of a robot with the purpose of covering a certain area or volume. Performing this task properly is fundamental for the execution of several activities, for instance, cleaning a facility with a robotic vacuum cleaner. In the mining industry, it is required to perform coverage in several unit processes related with material movement using industrial machinery, for example, in cleaning tasks, in dumps, and in the construction of tailings dam walls. The automation of these processes is fundamental to enhance the security associated with their execution. In this work, a coverage navigation system for a non-holonomic robot is presented. This work is intended to be a proof of concept for the potential automation of various unit processes that require coverage navigation like the ones mentioned before. The developed system includes the calculation of routes that allow a mobile platform to cover a specific area, and incorporates recovery behaviors in case that an unforeseen event occurs, such as the arising of dynamic or previously unmapped obstacles in the terrain to be covered, e.g., other machines or pedestrians passing through the area, being able to perform evasive maneuvers and post-recovery to ensure a complete coverage of the terrain. The system was tested in different simulated and real outdoor environments, obtaining results near 90% of coverage in the majority of experiments. The next step of development is to scale up the utilized robot to a mining machine/vehicle whose operation will be validated in a real environment. The result of one of the tests performed in the real world can be seen in the video available in https://youtu.be/gK7_3bK1P5g.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u975e\u5168\u5411\u673a\u5668\u4eba\u7684\u8986\u76d6\u5bfc\u822a\u7cfb\u7edf\uff0c\u65e8\u5728\u63d0\u9ad8\u77ff\u4e1a\u7b49\u9886\u57df\u7684\u81ea\u52a8\u5316\uff0c\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\u7cfb\u7edf\u6709\u6548\u3002", "motivation": "\u5728\u79fb\u52a8\u673a\u5668\u4eba\u9886\u57df\uff0c\u5b9e\u73b0\u533a\u57df\u6216\u4f53\u79ef\u7684\u8986\u76d6\u5bfc\u822a\u662f\u591a\u4e2a\u6d3b\u52a8\uff08\u5982\u5de5\u4e1a\u6e05\u626b\uff09\u7684\u57fa\u7840\uff0c\u5c24\u5176\u662f\u5728\u77ff\u4e1a\u4e2d\u5982\u6b64\uff0c\u81ea\u52a8\u5316\u8fd9\u4e00\u8fc7\u7a0b\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u9488\u5bf9\u975e\u5168\u5411\u673a\u5668\u4eba\u7684\u8986\u76d6\u5bfc\u822a\u7cfb\u7edf", "result": "\u5728\u591a\u6b21\u6a21\u62df\u548c\u771f\u5b9e\u6237\u5916\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u53d6\u5f97\u4e86\u63a5\u8fd190%\u7684\u8986\u76d6\u7387\u3002", "conclusion": "\u4e0b\u4e00\u6b65\u5c06\u673a\u5668\u4eba\u89c4\u6a21\u6269\u5927\u5230\u77ff\u7528\u673a\u68b0\uff0c\u4ee5\u9a8c\u8bc1\u5176\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u3002"}}
{"id": "2512.23054", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.23054", "abs": "https://arxiv.org/abs/2512.23054", "authors": ["Shuntian Zheng", "Guangming Wang", "Jiaqi Li", "Minzhe Ni", "Yu Guan"], "title": "Differentiable Physics-Driven Human Representation for Millimeter-Wave Based Pose Estimation", "comment": null, "summary": "While millimeter-wave (mmWave) presents advantages for Human Pose Estimation (HPE) through its non-intrusive sensing capabilities, current mmWave-based HPE methods face limitations in two predominant input paradigms: Heatmap and Point Cloud (PC). Heatmap represents dense multi-dimensional features derived from mmWave, but is significantly affected by multipath propagation and hardware modulation noise. PC, a set of 3D points, is obtained by applying the Constant False Alarm Rate algorithm to the Heatmap, which suppresses noise but results in sparse human-related features. To address these limitations, we study the feasibility of providing an alternative input paradigm: Differentiable Physics-driven Human Representation (DIPR), which represents humans as an ensemble of Gaussian distributions with kinematic and electromagnetic parameters. Inspired by Gaussian Splatting, DIPR leverages human kinematic priors and mmWave propagation physics to enhance human features while mitigating non-human noise through two strategies: 1) We incorporate prior kinematic knowledge to initialize DIPR based on the Heatmap and establish multi-faceted optimization objectives, ensuring biomechanical validity and enhancing motion features. 2) We simulate complete mmWave processing pipelines, re-render a new Heatmap from DIPR, and compare it with the original Heatmap, avoiding spurious noise generation due to kinematic constraints overfitting. Experimental results on three datasets with four methods demonstrate that existing mmWave-based HPE methods can easily integrate DIPR and achieve superior performance.", "AI": {"tldr": "DIPR\u662f\u4e00\u79cd\u9488\u5bf9\u6beb\u7c73\u6ce2\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u65b0\u8f93\u5165\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u63d0\u9ad8\u59ff\u6001\u8bc6\u522b\u7cbe\u5ea6\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u76ee\u524dmmWave HPE\u65b9\u6cd5\u5728\u70ed\u56fe\u548c\u70b9\u4e91\u8f93\u5165\u6a21\u5f0f\u4e0b\u7684\u7f3a\u9677\uff0c\u63d0\u9ad8\u4eba\u4f53\u7279\u5f81\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u9ad8\u65af\u5206\u5e03\u7684DIPR\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u8fd0\u52a8\u5b66\u5148\u9a8c\u548c\u6beb\u7c73\u6ce2\u4f20\u64ad\u7269\u7406\u5b66\uff0c\u521b\u9020\u51fa\u65b0\u7684\u70ed\u56fe\u4fe1\u606f\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4eba\u4f53\u8868\u793a\u65b9\u6cd5DIPR\uff0c\u65e8\u5728\u63d0\u9ad8\u6beb\u7c73\u6ce2\uff08mmWave\uff09\u7684\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\uff08HPE\uff09\u6027\u80fd\uff0c\u514b\u670d\u4f20\u7edf\u8f93\u5165\u65b9\u5f0f\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDIPR\u4e0e\u73b0\u6709\u6beb\u7c73\u6ce2HPE\u65b9\u6cd5\u7ed3\u5408\u540e\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2512.22757", "categories": ["cs.RO", "cs.AI", "cs.LG", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.22757", "abs": "https://arxiv.org/abs/2512.22757", "authors": ["Zheng Qiu", "Chih-Yuan Chiu", "Glen Chou"], "title": "Active Constraint Learning in High Dimensions from Demonstrations", "comment": "Under review, 25 pages, 11 figures", "summary": "We present an iterative active constraint learning (ACL) algorithm, within the learning from demonstrations (LfD) paradigm, which intelligently solicits informative demonstration trajectories for inferring an unknown constraint in the demonstrator's environment. Our approach iteratively trains a Gaussian process (GP) on the available demonstration dataset to represent the unknown constraints, uses the resulting GP posterior to query start/goal states, and generates informative demonstrations which are added to the dataset. Across simulation and hardware experiments using high-dimensional nonlinear dynamics and unknown nonlinear constraints, our method outperforms a baseline, random-sampling based method at accurately performing constraint inference from an iteratively generated set of sparse but informative demonstrations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8fed\u4ee3ACL\u7b97\u6cd5\uff0c\u901a\u8fc7\u667a\u80fd\u8bf7\u6c42\u793a\u8303\u8f68\u8ff9\u6765\u63a8\u65ad\u73af\u5883\u4e2d\u7684\u672a\u77e5\u7ea6\u675f\u3002", "motivation": "\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u6709\u6548\u63a8\u65ad\u73af\u5883\u4e2d\u7684\u672a\u77e5\u7ea6\u675f\uff0c\u4ee5\u63d0\u9ad8\u81ea\u52a8\u5b66\u4e60\u7cfb\u7edf\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u53d8\u6362\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\u5728\u793a\u8303\u6570\u636e\u96c6\u4e0a\u8fed\u4ee3\u8bad\u7ec3\uff0c\u5b9e\u73b0\u5bf9\u672a\u77e5\u7ea6\u675f\u7684\u5efa\u6a21\u548c\u63a8\u65ad\uff0c\u751f\u6210\u65b0\u7684\u793a\u8303\u4ee5\u4e30\u5bcc\u6570\u636e\u96c6\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8fed\u4ee3\u4e3b\u52a8\u7ea6\u675f\u5b66\u4e60\uff08ACL\uff09\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5728\u4ece\u793a\u8303\u5b66\u4e60\uff08LfD\uff09\u8303\u5f0f\u4e0b\uff0c\u667a\u80fd\u5730\u8bf7\u6c42\u6709\u4fe1\u606f\u91cf\u7684\u793a\u8303\u8f68\u8ff9\uff0c\u4ee5\u63a8\u65ad\u6f14\u793a\u8005\u73af\u5883\u4e2d\u7684\u672a\u77e5\u7ea6\u675f\u3002", "conclusion": "\u5728\u9ad8\u7ef4\u975e\u7ebf\u6027\u52a8\u6001\u548c\u672a\u77e5\u975e\u7ebf\u6027\u7ea6\u675f\u4e0b\uff0c\u672c\u6587\u7684\u65b9\u6cd5\u5728\u51c6\u786e\u6267\u884c\u7ea6\u675f\u63a8\u65ad\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u968f\u673a\u91c7\u6837\u65b9\u6cd5\u3002"}}
{"id": "2512.23055", "categories": ["cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.23055", "abs": "https://arxiv.org/abs/2512.23055", "authors": ["Jamie J. Alnasir"], "title": "Reimagining the Traditional Flight Computer: E6BJA as a Modern, Multi-Platform Tool for Flight Calculations and Training", "comment": "Pre-print. 23 pages, 6 figures, 2 tables", "summary": "Traditional flight computers -- including mechanical \"whiz-wheels\" (e.g. E6B, CRP series) and electronic flight calculators (e.g. ASA CX-3, Sportys E6-B) -- have long played a central role in flight planning and training within general aviation (GA). While these tools remain pedagogically valuable, their fixed form factors, constrained interaction models, and limited extensibility are increasingly misaligned with the expectations and workflows of pilots operating in modern digital environments.\n  This paper presents E6BJA (Jamies Flight Computer), a fully featured, multi-platform, software-based flight computer designed natively for Apple iOS, Android, and Microsoft Windows devices, with a complementary web-based implementation. E6BJA reproduces the core calculations of traditional flight computers while extending them through enhanced modelling capabilities such as the 1976 International Standard Atmosphere, carburettor icing risk estimation, and aircraft-specific weight and balance calculators. Each calculator is accompanied by embedded educational monographs that explain underlying assumptions, variables, and equations.\n  We compare E6BJA with mechanical and electronic flight computers across functional, cognitive, and technical dimensions, demonstrating improvements in accuracy, error reduction, discoverability, and educational value. We also discuss design trade-offs associated with native multi-platform development and examine how contemporary mobile computing environments can support safer and more intuitive pre-flight planning for pilots, trainees, instructors, and flight planning personnel. By combining the conceptual rigour of traditional flight planning methods with modern human-computer interaction design, E6BJA represents a meaningful evolution in pilot-facing flight tools, supporting both computation and instruction in aviation training contexts.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86E6BJA\u98de\u884c\u8ba1\u7b97\u673a\uff0c\u65e8\u5728\u7ed3\u5408\u4f20\u7edf\u65b9\u6cd5\u4e0e\u73b0\u4ee3\u4ea4\u4e92\u8bbe\u8ba1\uff0c\u4e3a\u98de\u884c\u5458\u63d0\u4f9b\u66f4\u5b89\u5168\u76f4\u89c2\u7684\u9884\u98de\u884c\u89c4\u5212\u5de5\u5177\u3002", "motivation": "\u4f20\u7edf\u7684\u98de\u884c\u8ba1\u7b97\u5de5\u5177\u7684\u5f62\u5f0f\u548c\u4ea4\u4e92\u6a21\u578b\u4e0e\u73b0\u4ee3\u6570\u5b57\u73af\u5883\u4e2d\u7684\u98de\u884c\u5458\u9700\u6c42\u4e0d\u518d\u5339\u914d\u3002", "method": "\u901a\u8fc7\u529f\u80fd\u3001\u8ba4\u77e5\u548c\u6280\u672f\u7ef4\u5ea6\u6bd4\u8f83E6BJA\u4e0e\u4f20\u7edf\u98de\u884c\u8ba1\u7b97\u673a\u7684\u6027\u80fd\u3002", "result": "E6BJA\u5728\u51c6\u786e\u6027\u3001\u964d\u4f4e\u9519\u8bef\u7387\u3001\u53ef\u53d1\u73b0\u6027\u548c\u6559\u80b2\u4ef7\u503c\u7b49\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u98de\u884c\u8ba1\u7b97\u673a\u3002", "conclusion": "E6BJA\u98de\u884c\u8ba1\u7b97\u673a\u662f\u5bf9\u4f20\u7edf\u98de\u884c\u8ba1\u7b97\u5de5\u5177\u7684\u6709\u610f\u4e49\u7684\u6f14\u8fdb\uff0c\u652f\u6301\u73b0\u4ee3\u822a\u7a7a\u57f9\u8bad\u7684\u8ba1\u7b97\u548c\u6559\u80b2\u3002"}}
{"id": "2512.22770", "categories": ["cs.RO", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.22770", "abs": "https://arxiv.org/abs/2512.22770", "authors": ["Naoki Kitamura", "Yuichi Sudo", "Koichi Wada"], "title": "Two-Robot Computational Landscape: A Complete Characterization of Model Power in Minimal Mobile Robot Systems", "comment": "23 pages, 3 figures", "summary": "The computational power of autonomous mobile robots under the Look-Compute-Move (LCM) model has been widely studied through an extensive hierarchy of robot models defined by the presence of memory, communication, and synchrony assumptions. While the general n-robot landscape has been largely established, the exact structure for two robots has remained unresolved. This paper presents the first complete characterization of the computational power of two autonomous robots across all major models, namely OBLOT, FSTA, FCOM, and LUMI, under the full spectrum of schedulers (FSYNCH, SSYNCH, ASYNCH, and their atomic variants). Our results reveal a landscape that fundamentally differs from the general case. Most notably, we prove that FSTA^F and LUMI^F coincide under full synchrony, a surprising collapse indicating that perfect synchrony can substitute both memory and communication when only two robots exist. We also show that FSTA and FCOM are orthogonal: there exists a problem solvable in the weakest communication model but impossible even in the strongest finite-state model, completing the bidirectional incomparability. All equivalence and separation results are derived through a novel simulation-free method, providing a unified and constructive view of the two-robot hierarchy. This yields the first complete and exact computational landscape for two robots, highlighting the intrinsic challenges of coordination at the minimal scale.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5b8c\u6574\u63cf\u8ff0\u4e86\u53cc\u673a\u5668\u4eba\u5728\u591a\u79cd\u6a21\u578b\u4e0b\u7684\u8ba1\u7b97\u80fd\u529b\uff0c\u6311\u6218\u534f\u8c03\u7684\u57fa\u672c\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u53cc\u673a\u5668\u4eba\u6a21\u578b\u672a\u51b3\u7684\u8ba1\u7b97\u80fd\u529b\u7ed3\u6784\u95ee\u9898", "method": "\u5206\u6790\u4e0d\u540c\u8c03\u5ea6\u7b56\u7565\u4e0b\u7684\u53cc\u673a\u5668\u4eba\u8ba1\u7b97\u6a21\u578b", "result": "\u9996\u6b21\u5b8c\u6574\u7279\u5f81\u5316\u53cc\u673a\u5668\u4eba\u7684\u8ba1\u7b97\u80fd\u529b\uff0c\u63ed\u793a\u8c03\u5ea6\u7b56\u7565\u5bf9\u8ba1\u7b97\u80fd\u529b\u5f71\u54cd", "conclusion": "\u5b8c\u7f8e\u540c\u6b65\u5728\u53cc\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u53ef\u4ee5\u66ff\u4ee3\u5185\u5b58\u548c\u901a\u4fe1\uff0cFSTA\u4e0eFCOM\u6a21\u578b\u95f4\u5b58\u5728\u95ee\u9898\u4e0d\u7b49\u4ef7\u6027\u3002"}}
{"id": "2512.23093", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.23093", "abs": "https://arxiv.org/abs/2512.23093", "authors": ["Ananya Drishti", "Mahfuza Farooque"], "title": "Cogniscope: Modeling Social Media Interactions as Digital Biomarkers for Early Detection of Cognitive Decline", "comment": null, "summary": "Alzheimer's disease (AD) and its prodromal stage, Mild Cognitive Impairment (MCI), are associated with subtle declines in memory, attention, and language that often go undetected until late in progression. Traditional diagnostic tools such as MRI and neuropsychological testing are invasive, costly, and poorly suited for population-scale monitoring. Social platforms, by contrast, produce continuous multimodal traces that can serve as ecologically valid indicators of cognition. In this paper, we introduce Cogniscope, a simulation framework that generates social-media-style interaction data for studying digital biomarkers of cognitive health. The framework models synthetic users with heterogeneous trajectories, embedding micro-tasks such as video summarization and lightweight question answering into content consumption streams. These interactions yield linguistic markers (semantic drift, disfluency) and behavioral signals (watch time, pausing, sharing), which can be fused to evaluate early detection models. We demonstrate the framework's use through ablation and sensitivity analyses, showing how detection performance varies across modalities, noise levels, and temporal windows. To support reproducibility, we release the generator code, parameter configurations, and synthetic datasets. By providing a controllable and ethically safe testbed, Cogniscope enables systematic investigation of multimodal cognitive markers and offers the community a benchmark resource that complements real-world validation studies.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faCogniscope\uff0c\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u793e\u4ea4\u5a92\u4f53\u4ea4\u4e92\u6570\u636e\u7684\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u591a\u6a21\u6001\u8bc6\u522b\u8ba4\u77e5\u5065\u5eb7\u7684\u6570\u5b57\u6807\u5fd7\u7269\uff0c\u4ece\u800c\u6539\u5584\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u65e9\u671f\u68c0\u6d4b\u3002", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u53ca\u5176\u524d\u9a71\u9636\u6bb5\u7684\u8ba4\u77e5\u4e0b\u964d\u901a\u5e38\u96be\u4ee5\u65e9\u671f\u68c0\u6d4b\uff0c\u4f20\u7edf\u7684\u8bca\u65ad\u5de5\u5177\u6210\u672c\u9ad8\u4e14\u4e0d\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u76d1\u6d4b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u76d1\u6d4b\u65b9\u5f0f\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u6846\u67b6\u751f\u6210\u793e\u4ea4\u5a92\u4f53\u98ce\u683c\u7684\u4ea4\u4e92\u6570\u636e\uff0c\u6a21\u578b\u5316\u5408\u6210\u7528\u6237\u5e76\u5d4c\u5165\u5fae\u4efb\u52a1\uff0c\u4ee5\u83b7\u5f97\u8bed\u8a00\u6807\u8bb0\u548c\u884c\u4e3a\u4fe1\u53f7\uff0c\u8fdb\u800c\u8bc4\u4f30\u65e9\u671f\u68c0\u6d4b\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u6d88\u878d\u548c\u654f\u611f\u6027\u5206\u6790\uff0c\u5c55\u793a\u4e86\u68c0\u6d4b\u6027\u80fd\u5982\u4f55\u968f\u6a21\u6001\u3001\u566a\u58f0\u6c34\u5e73\u548c\u65f6\u95f4\u7a97\u53e3\u7684\u53d8\u5316\u800c\u53d8\u5316\u3002", "conclusion": "Cogniscope\u4e3a\u7814\u7a76\u8ba4\u77e5\u5065\u5eb7\u7684\u6570\u5b57\u751f\u7269\u6807\u5fd7\u7269\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u63a7\u4e14\u5b89\u5168\u7684\u6d4b\u8bd5\u73af\u5883\uff0c\u5e76\u53d1\u5e03\u4e86\u57fa\u51c6\u8d44\u6e90\uff0c\u5e2e\u52a9\u793e\u533a\u8fdb\u884c\u66f4\u7cfb\u7edf\u7684\u591a\u6a21\u6001\u8ba4\u77e5\u6807\u8bb0\u7814\u7a76\u3002"}}
{"id": "2512.22868", "categories": ["cs.RO", "cs.AI", "q-bio.NC", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2512.22868", "abs": "https://arxiv.org/abs/2512.22868", "authors": ["Matej Hoffmann"], "title": "The body is not there to compute: Comment on \"Informational embodiment: Computational role of information structure in codes and robots\" by Pitti et al", "comment": "Comment on Pitti, A., Austin, M., Nakajima, K., & Kuniyoshi, Y. (2025). Informational Embodiment: Computational role of information structure in codes and robots. Physics of Life Reviews 53, 262-276. https://doi.org/10.1016/j.plrev.2025.03.018. Also available as arXiv:2408.12950", "summary": "Applying the lens of computation and information has been instrumental in driving the technological progress of our civilization as well as in empowering our understanding of the world around us. The digital computer was and for many still is the leading metaphor for how our mind operates. Information theory (IT) has also been important in our understanding of how nervous systems encode and process information. The target article deploys information and computation to bodies: to understand why they have evolved in particular ways (animal bodies) and to design optimal bodies (robots). In this commentary, I argue that the main role of bodies is not to compute.", "AI": {"tldr": "\u8fd9\u7bc7\u6587\u7ae0\u63a2\u8ba8\u4e86\u4fe1\u606f\u548c\u8ba1\u7b97\u5728\u52a8\u7269\u8eab\u4f53\u8fdb\u5316\u4e0e\u673a\u5668\u4eba\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u8eab\u4f53\u7684\u4e3b\u8981\u4f5c\u7528\u5e76\u975e\u8ba1\u7b97\u3002", "motivation": "\u63a2\u8ba8\u8eab\u4f53\u5982\u4f55\u8fdb\u5316\u4ee5\u53ca\u5982\u4f55\u8bbe\u8ba1\u6700\u4f73\u8eab\u4f53\uff08\u673a\u5668\u4eba\uff09\u3002", "method": "\u5e94\u7528\u8ba1\u7b97\u548c\u4fe1\u606f\u7684\u89c6\u89d2\u6765\u7406\u89e3\u8eab\u4f53\u7684\u6f14\u53d8\u548c\u8bbe\u8ba1\u3002", "result": "\u4fe1\u606f\u548c\u8ba1\u7b97\u5728\u7406\u89e3\u52a8\u7269\u8eab\u4f53\u53ca\u673a\u5668\u4eba\u8bbe\u8ba1\u4e2d\u53d1\u6325\u4e86\u91cd\u8981\u4f5c\u7528\u3002", "conclusion": "\u8eab\u4f53\u7684\u4e3b\u8981\u4f5c\u7528\u4e0d\u662f\u8fdb\u884c\u8ba1\u7b97\u3002"}}
{"id": "2512.23118", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.23118", "abs": "https://arxiv.org/abs/2512.23118", "authors": ["Mengyao Guo", "Kexin Nie", "Jinda Han", "Guanyou Li", "Adrian Wong"], "title": "ReHome Earth: A VR-Based Concept Validation for AI-Driven Space Homesickness Interventions", "comment": "11 pages, 1figure", "summary": "Space exploration has advanced rapidly, but the emotional needs of astronauts on long-duration missions remain underexplored. We present ReHome Earth, a dual-component design approach addressing space homesickness: 1) a future-oriented installation concept integrating transparent OLED displays with spaceship windows for real-time Earth connectivity, and 2) a functional VR prototype simulating astronaut isolation for testing AI-generated content effectiveness. Since accessing astronauts during missions is impossible, we conducted concept validation with terrestrial participants experiencing geographic displacement. Through evaluation with 84 proxy participants and 6 HCI experts, we demonstrate strong emotional resonance and validate three design implications: emotional pacing mechanisms, explainable biophysical feedback systems, and evolution from individual tools to collective affective infrastructure. Our contributions include a technically feasible space installation concept, a functional VR prototype for space HCI research, and empirical insights into the design of AI-driven emotional support systems for extreme isolation environments.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u9488\u5bf9\u5b87\u822a\u5458\u5728\u957f\u65f6\u95f4\u4efb\u52a1\u4e2d\u60c5\u611f\u9700\u6c42\u7684\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5305\u62ec\u900f\u660eOLED\u663e\u793a\u5c4f\u548cVR\u539f\u578b\uff0c\u9a8c\u8bc1\u4e86\u60c5\u611f\u5171\u9e23\u548c\u8bbe\u8ba1\u542f\u793a\u3002", "motivation": "\u5728\u957f\u65f6\u95f4\u7684\u592a\u7a7a\u4efb\u52a1\u4e2d\uff0c\u5b87\u822a\u5458\u7684\u60c5\u611f\u9700\u6c42\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u8ba8\u3002", "method": "\u901a\u8fc7\u4e0e\u5730\u9762\u53c2\u4e0e\u8005\u8fdb\u884c\u6982\u5ff5\u9a8c\u8bc1\uff0c\u8bc4\u4f30\u4e8684\u4f4d\u4ee3\u7406\u53c2\u4e0e\u8005\u548c6\u4f4dHCI\u4e13\u5bb6\u7684\u53cd\u9988\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u7ec4\u4ef6\u8bbe\u8ba1\u65b9\u6cd5ReHome Earth\uff0c\u65e8\u5728\u5e94\u5bf9\u592a\u7a7a\u601d\u4e61\u60c5\u7eea\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u8be5\u8bbe\u8ba1\u65b9\u6cd5\u5728\u5e94\u5bf9\u6781\u7aef\u5b64\u7acb\u73af\u5883\u4e2d\u5177\u6709\u53ef\u884c\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u60c5\u611f\u652f\u6301\u7cfb\u7edf\u7684\u8bbe\u8ba1\u542f\u793a\u3002"}}
{"id": "2512.22927", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.22927", "abs": "https://arxiv.org/abs/2512.22927", "authors": ["Daqian Cao", "Quan Yuan", "Weibang Bai"], "title": "P-FABRIK: A General Intuitive and Robust Inverse Kinematics Method for Parallel Mechanisms Using FABRIK Approach", "comment": "7 pages, 8 figures, and 2 tables", "summary": "Traditional geometric inverse kinematics methods for parallel mechanisms rely on specific spatial geometry constraints. However, their application to redundant parallel mechanisms is challenged due to the increased constraint complexity. Moreover, it will output no solutions and cause unpredictable control problems when the target pose lies outside its workspace. To tackle these challenging issues, this work proposes P-FABRIK, a general, intuitive, and robust inverse kinematics method to find one feasible solution for diverse parallel mechanisms based on the FABRIK algorithm. By decomposing the general parallel mechanism into multiple serial sub-chains using a new topological decomposition strategy, the end targets of each sub-chain can be subsequently revised to calculate the inverse kinematics solutions iteratively. Multiple case studies involving planar, standard, and redundant parallel mechanisms demonstrated the proposed method's generality across diverse parallel mechanisms. Furthermore, numerical simulation studies verified its efficacy and computational efficiency, as well as its robustness ability to handle out-of-workspace targets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9006\u5411\u8fd0\u52a8\u5b66\u65b9\u6cd5P-FABRIK\uff0c\u65e8\u5728\u4e3a\u591a\u79cd\u5e76\u8054\u673a\u5236\u627e\u51fa\u53ef\u884c\u89e3\uff0c\u89e3\u51b3\u5197\u4f59\u673a\u5236\u7684\u590d\u6742\u7ea6\u675f\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5197\u4f59\u5e76\u8054\u673a\u5236\u4e2d\u9762\u4e34\u7ea6\u675f\u590d\u6742\u6027\u548c\u8d85\u51fa\u5de5\u4f5c\u7a7a\u95f4\u7684\u63a7\u5236\u95ee\u9898\uff0c\u8feb\u5207\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u548c\u5f3a\u5927\u7684\u9006\u5411\u8fd0\u52a8\u5b66\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u65b0\u7684\u62d3\u6251\u5206\u89e3\u7b56\u7565\u5c06\u4e00\u822c\u5e76\u8054\u673a\u5236\u5206\u89e3\u4e3a\u591a\u4e2a\u4e32\u8054\u5b50\u94fe\uff0c\u8fed\u4ee3\u8ba1\u7b97\u6bcf\u4e2a\u5b50\u94fe\u7684\u9006\u5411\u8fd0\u52a8\u5b66\u89e3\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aP-FABRIK\u7684\u65b0\u578b\u9006\u5411\u8fd0\u52a8\u5b66\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u51e0\u4f55\u9006\u5411\u8fd0\u52a8\u5b66\u5728\u5904\u7406\u5197\u4f59\u5e76\u8054\u673a\u5236\u65f6\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u76ee\u6807\u59ff\u6001\u8d85\u51fa\u5de5\u4f5c\u7a7a\u95f4\u65f6\u7684\u95ee\u9898\u3002", "conclusion": "\u901a\u8fc7\u6570\u503c\u4eff\u771f\u9a8c\u8bc1\u4e86P-FABRIK\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3001\u8ba1\u7b97\u6548\u7387\u548c\u5728\u8d85\u51fa\u5de5\u4f5c\u7a7a\u95f4\u65f6\u7684\u7a33\u5065\u6027\u3002"}}
{"id": "2512.23128", "categories": ["cs.HC", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.23128", "abs": "https://arxiv.org/abs/2512.23128", "authors": ["Karolina Korgul", "Yushi Yang", "Arkadiusz Drohomirecki", "Piotr B\u0142aszczyk", "Will Howard", "Lukas Aichberger", "Chris Russell", "Philip H. S. Torr", "Adam Mahdi", "Adel Bibi"], "title": "It's a TRAP! Task-Redirecting Agent Persuasion Benchmark for Web Agents", "comment": null, "summary": "Web-based agents powered by large language models are increasingly used for tasks such as email management or professional networking. Their reliance on dynamic web content, however, makes them vulnerable to prompt injection attacks: adversarial instructions hidden in interface elements that persuade the agent to divert from its original task. We introduce the Task-Redirecting Agent Persuasion Benchmark (TRAP), an evaluation for studying how persuasion techniques misguide autonomous web agents on realistic tasks. Across six frontier models, agents are susceptible to prompt injection in 25\\% of tasks on average (13\\% for GPT-5 to 43\\% for DeepSeek-R1), with small interface or contextual changes often doubling success rates and revealing systemic, psychologically driven vulnerabilities in web-based agents. We also provide a modular social-engineering injection framework with controlled experiments on high-fidelity website clones, allowing for further benchmark expansion.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86TRAP\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u7f51\u7edc\u4ee3\u7406\u5728\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u4e0b\u7684\u8106\u5f31\u6027\uff0c\u53d1\u73b0\u5927\u7ea625%\u7684\u4efb\u52a1\u53d7\u5230\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u53ef\u6269\u5c55\u7684\u793e\u4f1a\u5de5\u7a0b\u6ce8\u5165\u6846\u67b6\u3002", "motivation": "\u7814\u7a76\u7f51\u7edc\u4ee3\u7406\u5728\u6267\u884c\u4efb\u52a1\u65f6\u7531\u4e8e\u4f9d\u8d56\u52a8\u6001\u7f51\u7edc\u5185\u5bb9\u800c\u9762\u4e34\u7684\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u5bf9\u516d\u4e2a\u524d\u6cbf\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\uff0c\u91cf\u5316\u4ee3\u7406\u5728\u9762\u5bf9\u63d0\u793a\u6ce8\u5165\u65f6\u7684\u8106\u5f31\u6027\uff0c\u5e76\u63d0\u4f9b\u6a21\u5757\u5316\u7684\u6ce8\u5165\u6846\u67b6\u8fdb\u884c\u53d7\u63a7\u5b9e\u9a8c\u3002", "result": "\u63d0\u51fa\u4e86\u4efb\u52a1\u91cd\u5b9a\u5411\u4ee3\u7406\u8bf4\u670d\u57fa\u51c6(TRAP)\uff0c\u65e8\u5728\u8bc4\u4f30\u8bf4\u670d\u6280\u672f\u5982\u4f55\u8bef\u5bfc\u81ea\u6cbb\u7f51\u7edc\u4ee3\u7406\u5728\u5b9e\u9645\u4efb\u52a1\u4e0a\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u7f51\u7edc\u4ee3\u7406\u5728\u6267\u884c\u4efb\u52a1\u65f6\u5b58\u5728\u7cfb\u7edf\u6027\u7684\u5fc3\u7406\u8106\u5f31\u6027\uff0c\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u6210\u4e3a\u4e3b\u8981\u98ce\u9669\u3002"}}
{"id": "2512.22957", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.22957", "abs": "https://arxiv.org/abs/2512.22957", "authors": ["Mengyu Ji", "Shiliang Guo", "Zhengzhen Li", "Jiahao Shen", "Huazi Cao", "Shiyu Zhao"], "title": "PreGME: Prescribed Performance Control of Aerial Manipulators based on Variable-Gain ESO", "comment": "12 pages, 6 figures", "summary": "An aerial manipulator, comprising a multirotor base and a robotic arm, is subject to significant dynamic coupling between these two components. Therefore, achieving precise and robust motion control is a challenging yet important objective. Here, we propose a novel prescribed performance motion control framework based on variable-gain extended state observers (ESOs), referred to as PreGME. The method includes variable-gain ESOs for real-time estimation of dynamic coupling and a prescribed performance flight control that incorporates error trajectory constraints. Compared with existing methods, the proposed approach exhibits the following two characteristics. First, the adopted variable-gain ESOs can accurately estimate rapidly varying dynamic coupling. This enables the proposed method to handle manipulation tasks that require aggressive motion of the robotic arm. Second, by prescribing the performance, a preset error trajectory is generated to guide the system evolution along this trajectory. This strategy allows the proposed method to ensure the tracking error remains within the prescribed performance envelope, thereby achieving high-precision control. Experiments on a real platform, including aerial staff twirling, aerial mixology, and aerial cart-pulling experiments, are conducted to validate the effectiveness of the proposed method.\n  Experimental results demonstrate that even under the dynamic coupling caused by rapid robotic arm motion (end-effector velocity: 1.02 m/s, acceleration: 5.10 m/s$^2$), the proposed method achieves high tracking performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u53ef\u53d8\u589e\u76ca\u6269\u5c55\u72b6\u6001\u89c2\u6d4b\u5668\u7684\u8fd0\u52a8\u63a7\u5236\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u5e94\u5bf9\u52a8\u6001\u8026\u5408\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u65e0\u4eba\u673a\u64cd\u63a7\u3002", "motivation": "\u65e0\u4eba\u673a\u548c\u673a\u68b0\u81c2\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u52a8\u6001\u8026\u5408\uff0c\u56e0\u6b64\u5b9e\u73b0\u7cbe\u786e\u548c\u9c81\u68d2\u7684\u8fd0\u52a8\u63a7\u5236\u662f\u4e00\u4e2a\u6311\u6218\u4e14\u91cd\u8981\u7684\u76ee\u6807\u3002", "method": "\u91c7\u7528\u53ef\u53d8\u589e\u76ca\u6269\u5c55\u72b6\u6001\u89c2\u6d4b\u5668\u8fdb\u884c\u52a8\u6001\u8026\u5408\u7684\u5b9e\u65f6\u4f30\u8ba1\uff0c\u5e76\u7ed3\u5408\u8bef\u5dee\u8f68\u8ff9\u7ea6\u675f\u7684\u6307\u5b9a\u6027\u80fd\u98de\u884c\u63a7\u5236\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u53d8\u589e\u76ca\u6269\u5c55\u72b6\u6001\u89c2\u6d4b\u5668\u7684\u6307\u5b9a\u6027\u80fd\u8fd0\u52a8\u63a7\u5236\u6846\u67b6\uff0c\u79f0\u4e3aPreGME\uff0c\u80fd\u591f\u9ad8\u7cbe\u5ea6\u63a7\u5236\u673a\u68b0\u81c2\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\uff0c\u5728\u5feb\u901f\u673a\u5668\u4eba\u81c2\u8fd0\u52a8\u9020\u6210\u7684\u52a8\u6001\u8026\u5408\u4e0b\uff0c\u8be5\u65b9\u6cd5\u4ecd\u80fd\u5b9e\u73b0\u9ad8\u8ddf\u8e2a\u6027\u80fd\u3002"}}
{"id": "2512.23136", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.23136", "abs": "https://arxiv.org/abs/2512.23136", "authors": ["Junyeong Park", "Jieun Han", "Yeon Su Park", "Youngbin Lee", "Suin Kim", "Juho Kim", "Alice Oh", "So-Yeon Ahn"], "title": "Understanding EFL Learners' Code-Switching and Teachers' Pedagogical Approaches in LLM-Supported Speaking Practice", "comment": null, "summary": "For English as a Foreign Language (EFL) learners, code-switching (CSW), or alternating between their native language and the target language (English), can lower anxiety and ease communication barriers. Large language models (LLMs), with their multilingual abilities, offer new opportunities to support CSW in speaking practice. Yet, the pedagogical design of LLM-based tutors remains underexplored. To this end, we conducted a six-week study of LLM-mediated speaking practice with 20 Korean EFL learners, alongside a qualitative study with nine English teachers who designed and refined responses to learner CSW. Findings show that learners used CSW not only to bridge lexical gaps but also to express cultural and emotional nuance, prompting teachers to employ selective interventions and dynamic scaffolding strategies. We conclude with design implications for bilingual LLM-powered tutors that leverage teachers' expertise to transform CSW into meaningful learning opportunities.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u5728EFL\u5b66\u4e60\u8005\u53e3\u8bed\u7ec3\u4e60\u4e2d\u7684\u4f5c\u7528\uff0c\u53d1\u73b0\u4ee3\u7801\u8f6c\u6362\u53ef\u4ee5\u964d\u4f4e\u7126\u8651\u5e76\u4fc3\u8fdb\u4ea4\u6d41\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u6559\u5e08\u5728\u6b64\u8fc7\u7a0b\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u652f\u6301EFL\u5b66\u4e60\u8005\u5728\u8bed\u8a00\u4ea4\u6d41\u4e2d\u7684\u4ee3\u7801\u8f6c\u6362\u5b9e\u8df5\uff0c\u6539\u5584\u5b66\u4e60\u4f53\u9a8c\u548c\u6548\u679c\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u9488\u5bf920\u540d\u97e9\u56fdEFL\u5b66\u4e60\u8005\u4e3a\u671f\u516d\u5468\u7684LLM\u4ecb\u5bfc\u53e3\u8bed\u7ec3\u4e60\u8fdb\u884c\u91cf\u5316\u7814\u7a76\uff0c\u5e76\u4e0e9\u540d\u82f1\u8bed\u6559\u5e08\u8fdb\u884c\u5b9a\u6027\u7814\u7a76\uff0c\u4ee5\u8bbe\u8ba1\u548c\u4f18\u5316\u5bf9\u5b66\u4e60\u8005CSW\u7684\u56de\u5e94\u3002", "result": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u82f1\u8bed\u4f5c\u4e3a\u5916\u8bed(EFL)\u5b66\u4e60\u8005\u5728\u53e3\u8bed\u7ec3\u4e60\u4e2d\u4f7f\u7528\u4ee3\u7801\u8f6c\u6362(CSW)\u7684\u80fd\u529b\uff0c\u5bf9\u4e8e\u514b\u670d\u8bed\u8a00\u969c\u788d\u548c\u589e\u5f3a\u4ea4\u6d41\u6709\u79ef\u6781\u4f5c\u7528\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5b66\u4e60\u8005\u5229\u7528CSW\u4e0d\u4ec5\u53ef\u4ee5\u5f25\u8865\u8bcd\u6c47\u4e0d\u8db3\uff0c\u8fd8\u80fd\u591f\u8868\u8fbe\u6587\u5316\u548c\u60c5\u611f\u7684\u7ec6\u5fae\u5dee\u522b\u3002\u6559\u5e08\u6839\u636e\u5b66\u4e60\u8005\u7684CSW\u505a\u51fa\u4e86\u6709\u9488\u5bf9\u6027\u7684\u5e72\u9884\uff0c\u4f7f\u7528\u52a8\u6001\u652f\u67b6\u7b56\u7565\u6765\u652f\u6301\u5b66\u751f\u3002", "conclusion": "\u6211\u4eec\u5efa\u8bae\u8bbe\u8ba1\u53cc\u8bed\u7684LLM\u652f\u6301\u7684\u8f85\u5bfc\u5de5\u5177\uff0c\u5145\u5206\u5229\u7528\u6559\u5e08\u7684\u4e13\u4e1a\u77e5\u8bc6\uff0c\u5c06\u4ee3\u7801\u8f6c\u6362\u8f6c\u53d8\u4e3a\u6709\u610f\u4e49\u7684\u5b66\u4e60\u673a\u4f1a\u3002"}}
{"id": "2512.22983", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.22983", "abs": "https://arxiv.org/abs/2512.22983", "authors": ["Shuanghao Bai", "Wenxuan Song", "Jiayi Chen", "Yuheng Ji", "Zhide Zhong", "Jin Yang", "Han Zhao", "Wanqi Zhou", "Zhe Li", "Pengxiang Ding", "Cheng Chi", "Chang Xu", "Xiaolong Zheng", "Donglin Wang", "Haoang Li", "Shanghang Zhang", "Badong Chen"], "title": "Embodied Robot Manipulation in the Era of Foundation Models: Planning and Learning Perspectives", "comment": "This work is a re-architected core derived from the full survey (arXiv:2510.10903) , refined to highlight the most central themes and representative studies", "summary": "Recent advances in vision, language, and multimodal learning have substantially accelerated progress in robotic foundation models, with robot manipulation remaining a central and challenging problem. This survey examines robot manipulation from an algorithmic perspective and organizes recent learning-based approaches within a unified abstraction of high-level planning and low-level control. At the high level, we extend the classical notion of task planning to include reasoning over language, code, motion, affordances, and 3D representations, emphasizing their role in structured and long-horizon decision making. At the low level, we propose a training-paradigm-oriented taxonomy for learning-based control, organizing existing methods along input modeling, latent representation learning, and policy learning. Finally, we identify open challenges and prospective research directions related to scalability, data efficiency, multimodal physical interaction, and safety. Together, these analyses aim to clarify the design space of modern foundation models for robotic manipulation.", "AI": {"tldr": "\u672c\u8c03\u67e5\u5206\u6790\u4e86\u673a\u5668\u4eba\u64cd\u63a7\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c06\u5176\u5206\u4e3a\u9ad8\u5c42\u89c4\u5212\u548c\u4f4e\u5c42\u63a7\u5236\uff0c\u6307\u51fa\u4e86\u76f8\u5173\u7684\u7814\u7a76\u6311\u6218\u4e0e\u65b9\u5411\u3002", "motivation": "\u5206\u6790\u673a\u5668\u4eba\u64cd\u63a7\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u63a8\u52a8\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u7684\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u4ece\u7b97\u6cd5\u7684\u89d2\u5ea6\u8c03\u67e5\u673a\u5668\u4eba\u64cd\u63a7\uff0c\u7ec4\u7ec7\u5b66\u4e60\u65b9\u6cd5\u4e3a\u9ad8\u5c42\u89c4\u5212\u4e0e\u4f4e\u5c42\u63a7\u5236\u7684\u7edf\u4e00\u62bd\u8c61\u3002", "result": "\u63d0\u51fa\u6269\u5c55\u7ecf\u5178\u4efb\u52a1\u89c4\u5212\u7684\u9ad8\u5c42\u6982\u5ff5\uff0c\u5e76\u5bf9\u5b66\u4e60\u63a7\u5236\u65b9\u6cd5\u8fdb\u884c\u5206\u7c7b\uff0c\u8bc6\u522b\u51fa\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u672c\u8c03\u67e5\u660e\u786e\u4e86\u73b0\u4ee3\u57fa\u7840\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u63a7\u4e2d\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u5e76\u63a2\u8ba8\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u548c\u6311\u6218\u3002"}}
{"id": "2512.23372", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.23372", "abs": "https://arxiv.org/abs/2512.23372", "authors": ["Tobias St\u00e4hle", "Matthijs Jansen op de Haar", "Sophia Boyer", "Rita Sevastjanova", "Arpit Narechania", "Mennatallah El-Assady"], "title": "A Design Space for Intelligent Agents in Mixed-Initiative Visual Analytics", "comment": null, "summary": "Mixed-initiative visual analytics (VA) systems, where human and artificial intelligence (AI) agents collaborate as equal partners during analysis, represented a paradigm shift in human-computer interaction. With recent advances in AI, these systems have seen an increase in sophisticated software agents that have improved task planning, reasoning, and completion capabilities. However, while existing work characterizes agent interplay and communication strategies, there is a limited understanding of the overarching design principles for intelligent agents. Through a systematic review of 90 systems (and 207 unique agents), we propose a design space of intelligent agents comprising six dimensions that collectively characterize an agent's perception, environmental understanding, action capability, and communication strategies. We contribute a novel framework for researchers and designers to explore various design choices for new systems and to situate a system in the current landscape. We conclude with future research opportunities for intelligent agents in mixed-initiative VA systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u8bc4\u5ba1\u63d0\u51fa\u4e86\u667a\u80fd\u4ee3\u7406\u8bbe\u8ba1\u7684\u516d\u4e2a\u7ef4\u5ea6\uff0c\u65e8\u5728\u6307\u5bfc\u672a\u6765\u7684\u6df7\u5408\u4e3b\u52a8\u89c6\u89c9\u5206\u6790\u7cfb\u7edf\u7684\u8bbe\u8ba1\u3002", "motivation": "\u7814\u7a76\u667a\u80fd\u4ee3\u7406\u7684\u8bbe\u8ba1\u539f\u5219\u548c\u7cfb\u7edf\u95f4\u7684\u76f8\u4e92\u4f5c\u7528", "method": "\u7cfb\u7edf\u8bc4\u5ba190\u4e2a\u7cfb\u7edf\u548c207\u4e2a\u72ec\u7279\u4ee3\u7406\u7684\u8bbe\u8ba1", "result": "\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u516d\u4e2a\u7ef4\u5ea6\u7684\u667a\u80fd\u4ee3\u7406\u8bbe\u8ba1\u7a7a\u95f4", "conclusion": "\u4e3a\u672a\u6765\u667a\u80fd\u4ee3\u7406\u5728\u6df7\u5408\u4e3b\u52a8\u89c6\u89c9\u5206\u6790\u7cfb\u7edf\u4e2d\u7684\u7814\u7a76\u673a\u4f1a\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2512.23077", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.23077", "abs": "https://arxiv.org/abs/2512.23077", "authors": ["Saraswati Soedarmadji", "Yunyue Wei", "Chen Zhang", "Yisong Yue", "Yanan Sui"], "title": "Embodied Learning of Reward for Musculoskeletal Control with Vision Language Models", "comment": "18 pages, 8 figures", "summary": "Discovering effective reward functions remains a fundamental challenge in motor control of high-dimensional musculoskeletal systems. While humans can describe movement goals explicitly such as \"walking forward with an upright posture,\" the underlying control strategies that realize these goals are largely implicit, making it difficult to directly design rewards from high-level goals and natural language descriptions. We introduce Motion from Vision-Language Representation (MoVLR), a framework that leverages vision-language models (VLMs) to bridge the gap between goal specification and movement control. Rather than relying on handcrafted rewards, MoVLR iteratively explores the reward space through iterative interaction between control optimization and VLM feedback, aligning control policies with physically coordinated behaviors. Our approach transforms language and vision-based assessments into structured guidance for embodied learning, enabling the discovery and refinement of reward functions for high-dimensional musculoskeletal locomotion and manipulation. These results suggest that VLMs can effectively ground abstract motion descriptions in the implicit principles governing physiological motor control.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cdMoVLR\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u63a2\u7d22\u9ad8\u7ef4\u808c\u8089\u9aa8\u9abc\u7cfb\u7edf\u7684\u5956\u52b1\u51fd\u6570\uff0c bridging the gap between high-level\u8fd0\u52a8\u76ee\u6807\u4e0e\u63a7\u5236\u7b56\u7565\u3002", "motivation": "\u6709\u6548\u5956\u52b1\u51fd\u6570\u7684\u53d1\u73b0\u5bf9\u9ad8\u7ef4\u808c\u8089\u9aa8\u9abc\u7cfb\u7edf\u7684\u8fd0\u52a8\u63a7\u5236\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7MoVLR\u6846\u67b6\uff0c\u8fed\u4ee3\u63a2\u7d22\u5956\u52b1\u7a7a\u95f4\uff0c\u7ed3\u5408\u63a7\u5236\u4f18\u5316\u4e0eVLM\u53cd\u9988\u6765\u8c03\u6574\u63a7\u5236\u7b56\u7565\u3002", "result": "MoVLR\u6846\u67b6\u5b9e\u73b0\u4e86\u9ad8\u7ef4\u808c\u8089\u9aa8\u9abc\u8fd0\u52a8\u548c\u64cd\u4f5c\u4e2d\u5956\u52b1\u51fd\u6570\u7684\u53d1\u73b0\u4e0e\u5b8c\u5584\u3002", "conclusion": "VLM\u80fd\u591f\u6709\u6548\u5c06\u62bd\u8c61\u7684\u8fd0\u52a8\u63cf\u8ff0\u4e0e\u751f\u7406\u8fd0\u52a8\u63a7\u5236\u7684\u9690\u542b\u539f\u5219\u7ed3\u5408\u3002"}}
{"id": "2512.23570", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.23570", "abs": "https://arxiv.org/abs/2512.23570", "authors": ["Amy Ingold", "Loong Yi Lee", "Richard Suphapol Diteesawat", "Ajmal Roshan", "Yael Zekaria", "Edith-Clare Hall", "Enrico Werner", "Nahian Rahman", "Elaine Czech", "Jonathan Rossiter"], "title": "Soft Robotic Technological Probe for Speculative Fashion Futures", "comment": null, "summary": "Emerging wearable robotics demand design approaches that address not only function, but also social meaning. In response, we present Sumbrella, a soft robotic garment developed as a speculative fashion probe. We first detail the design and fabrication of the Sumbrella, including sequenced origami-inspired bistable units, fabric pneumatic actuation chambers, cable driven shape morphing mechanisms, computer vision components, and an integrated wearable system comprising a hat and bolero jacket housing power and control electronics. Through a focus group with twelve creative technologists, we then used Sumbrella as a technological probe to explore how people interpreted, interacted, and imagined future relationships with soft robotic wearables. While Sumbrella allowed our participants to engage in rich discussion around speculative futures and expressive potential, it also surfaced concerns about exploitation, surveillance, and the personal risks and societal ethics of embedding biosensing technologies in public life. We contribute to the Human-Robot Interaction (HRI) field key considerations and recommendations for designing soft robotic garments, including the potential for kinesic communication, the impact of such technologies on social dynamics, and the importance of ethical guidelines. Finally, we provide a reflection on our application of speculative design; proposing that it allows HRI researchers to not only consider functionality, but also how wearable robots influence definitions of what is considered acceptable or desirable in public settings.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86Sumbrella\uff0c\u4e00\u79cd\u8f6f\u673a\u5668\u4eba\u670d\u88c5\uff0c\u65e8\u5728\u63a2\u7d22\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u5728\u793e\u4f1a\u610f\u4e49\u548c\u4f26\u7406\u65b9\u9762\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u5728\u529f\u80fd\u4e4b\u5916\u7684\u793e\u4f1a\u610f\u4e49\uff0c\u5173\u6ce8\u5728\u672a\u6765\u6280\u672f\u5e94\u7528\u4e2d\u7684\u4f26\u7406\u548c\u793e\u4f1a\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u548c\u5236\u4f5cSumbrella\uff0c\u4ee5\u53ca\u4e0e12\u540d\u521b\u610f\u6280\u672f\u4e13\u5bb6\u8fdb\u884c\u7126\u70b9\u5c0f\u7ec4\u8ba8\u8bba\uff0c\u63a2\u7d22\u4eba\u4eec\u5bf9\u8f6f\u673a\u5668\u4eba\u670d\u88c5\u7684\u7406\u89e3\u548c\u4e92\u52a8\u3002", "result": "Sumbrella\u6fc0\u53d1\u53c2\u4e0e\u8005\u5173\u4e8e\u672a\u6765\u60f3\u8c61\u4e0e\u793e\u4ea4\u6f5c\u529b\u7684\u8ba8\u8bba\uff0c\u540c\u65f6\u66b4\u9732\u51fa\u5bf9\u76d1\u89c6\u548c\u4e2a\u4eba\u98ce\u9669\u7684\u62c5\u5fe7\u3002", "conclusion": "Sumbrella\u7684\u8bbe\u8ba1\u4fc3\u4f7fHRI\u9886\u57df\u5173\u6ce8\u8f6f\u673a\u5668\u4eba\u670d\u88c5\u7684\u4f26\u7406\u3001\u793e\u4ea4\u52a8\u6001\u4e0e\u529f\u80fd\u6027\u3002"}}
{"id": "2512.23103", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.23103", "abs": "https://arxiv.org/abs/2512.23103", "authors": ["Peter Messina", "Daniel Rakita"], "title": "APOLLO Blender: A Robotics Library for Visualization and Animation in Blender", "comment": null, "summary": "High-quality visualizations are an essential part of robotics research, enabling clear communication of results through figures, animations, and demonstration videos. While Blender is a powerful and freely available 3D graphics platform, its steep learning curve and lack of robotics-focused integrations make it difficult and time-consuming for researchers to use effectively. In this work, we introduce a lightweight software library that bridges this gap by providing simple scripting interfaces for common robotics visualization tasks. The library offers three primary capabilities: (1) importing robots and environments directly from standardized descriptions such as URDF; (2) Python-based scripting tools for keyframing robot states and visual attributes; and (3) convenient generation of primitive 3D shapes for schematic figures and animations. Together, these features allow robotics researchers to rapidly create publication-ready images, animations, and explanatory schematics without needing extensive Blender expertise. We demonstrate the library through a series of proof-of-concept examples and conclude with a discussion of current limitations and opportunities for future extensions.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u8f6f\u4ef6\u5e93\uff0c\u65e8\u5728\u7b80\u5316\u673a\u5668\u4eba\u7814\u7a76\u4e2d\u7684\u53ef\u89c6\u5316\u4efb\u52a1\uff0c\u964d\u4f4e\u4f7f\u7528Blender\u7684\u95e8\u69db\u3002", "motivation": "\u9ad8\u8d28\u91cf\u53ef\u89c6\u5316\u662f\u673a\u5668\u4eba\u7814\u7a76\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u80fd\u591f\u901a\u8fc7\u56fe\u5f62\u3001\u52a8\u753b\u548c\u6f14\u793a\u89c6\u9891\u6e05\u6670\u5730\u4f20\u8fbe\u7ed3\u679c\u3002", "method": "\u8be5\u5e93\u63d0\u4f9b\u4e86\u4e09\u4e2a\u4e3b\u8981\u529f\u80fd\uff1a\u5bfc\u5165\u7b26\u5408\u6807\u51c6\u63cf\u8ff0\u7684\u673a\u5668\u4eba\u548c\u73af\u5883\uff1b\u57fa\u4e8ePython\u7684\u811a\u672c\u5de5\u5177\u7528\u4e8e\u5173\u952e\u5e27\u52a8\u753b\u548c\u89c6\u89c9\u5c5e\u6027\uff1b\u751f\u6210\u539f\u59cb3D\u5f62\u72b6\u4ee5\u7528\u4e8e\u793a\u610f\u56fe\u548c\u52a8\u753b\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u8f6f\u4ef6\u5e93\uff0c\u63d0\u4f9b\u7b80\u5355\u7684\u811a\u672c\u63a5\u53e3\uff0c\u4fbf\u4e8e\u5b8c\u6210\u5e38\u89c1\u7684\u673a\u5668\u4eba\u53ef\u89c6\u5316\u4efb\u52a1\u3002", "conclusion": "\u901a\u8fc7\u4e00\u7cfb\u5217\u6982\u5ff5\u9a8c\u8bc1\u793a\u4f8b\u5c55\u793a\u4e86\u8be5\u5e93\u7684\u529f\u80fd\uff0c\u5e76\u8ba8\u8bba\u4e86\u5f53\u524d\u7684\u5c40\u9650\u6027\u53ca\u672a\u6765\u6269\u5c55\u7684\u673a\u4f1a\u3002"}}
{"id": "2512.23135", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.23135", "abs": "https://arxiv.org/abs/2512.23135", "authors": ["Roshan Klein-Seetharaman", "Daniel Rakita"], "title": "Beyond URDF: The Universal Robot Description Directory for Shared, Extensible, and Standardized Robot Models", "comment": null, "summary": "Robots are typically described in software by specification files (e.g., URDF, SDF, MJCF, USD) that encode only basic kinematic, dynamic, and geometric information. As a result, downstream applications such as simulation, planning, and control must repeatedly re-derive richer data, leading to redundant computations, fragmented implementations, and limited standardization. In this work, we introduce the Universal Robot Description Directory (URDD), a modular representation that organizes derived robot information into structured, easy-to-parse JSON and YAML modules. Our open-source toolkit automatically generates URDDs from URDFs, with a Rust implementation supporting Bevy-based visualization. Additionally, we provide a JavaScript/Three.js viewer for web-based inspection of URDDs. Experiments on multiple robot platforms show that URDDs can be generated efficiently, encapsulate substantially richer information than standard specification files, and directly enable the construction of core robotics subroutines. URDD provides a unified, extensible resource for reducing redundancy and establishing shared standards across robotics frameworks. We conclude with a discussion on the limitations and implications of our work.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faURDD\uff0c\u65e8\u5728\u901a\u8fc7\u6a21\u5757\u5316\u7684JSON\u548cYAML\u683c\u5f0f\u7ec4\u7ec7\u673a\u5668\u4eba\u4fe1\u606f\uff0c\u4ece\u800c\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u6807\u51c6\u5316\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u4eba\u63cf\u8ff0\u65b9\u6cd5\u4ec5\u7f16\u7801\u57fa\u672c\u7684\u8fd0\u52a8\u5b66\u3001\u52a8\u529b\u5b66\u548c\u51e0\u4f55\u4fe1\u606f\uff0c\u5bfc\u81f4\u4e0b\u6e38\u5e94\u7528\u9700\u91cd\u590d\u63a8\u5bfc\u66f4\u4e30\u5bcc\u7684\u6570\u636e\uff0c\u9020\u6210\u5197\u4f59\u8ba1\u7b97\u548c\u6807\u51c6\u5316\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\u5305\uff0c\u81ea\u52a8\u4eceURDF\u751f\u6210URDD\uff0c\u652f\u6301\u591a\u79cd\u5e73\u53f0\u548c\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u5305\u62ecRust\u548cJavaScript/Three.js\u5b9e\u73b0\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u7684\u673a\u5668\u4eba\u4fe1\u606f\u8868\u793a\u6cd5\uff0c\u5373\u901a\u7528\u673a\u5668\u4eba\u63cf\u8ff0\u76ee\u5f55\uff08URDD\uff09\uff0c\u80fd\u6709\u6548\u7ec4\u7ec7\u66f4\u52a0\u4e30\u5bcc\u7684\u673a\u5668\u4eba\u4fe1\u606f\uff0c\u51cf\u5c11\u5197\u4f59\u548c\u63d0\u9ad8\u6807\u51c6\u5316\u3002", "conclusion": "URDD\u4e3a\u673a\u5668\u4eba\u6846\u67b6\u95f4\u7684\u6807\u51c6\u5316\u63d0\u4f9b\u4e86\u7edf\u4e00\u3001\u53ef\u6269\u5c55\u7684\u8d44\u6e90\uff0c\u5e76\u8ba8\u8bba\u4e86\u8be5\u5de5\u5177\u7684\u5c40\u9650\u6027\u548c\u5f71\u54cd\u3002"}}
{"id": "2512.23140", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.23140", "abs": "https://arxiv.org/abs/2512.23140", "authors": ["Roshan Klein-Seetharama", "Daniel Rakita"], "title": "A New Software Tool for Generating and Visualizing Robot Self-Collision Matrices", "comment": null, "summary": "In robotics, it is common to check whether a given robot state results in self-intersection (i.e., a self-collision query) or to assess its distance from such an intersection (i.e., a self-proximity query). These checks are typically performed between pairs of shapes attached to different robot links. However, many of these shape pairs can be excluded in advance, as their configurations are known to always or never result in contact. This information is typically encoded in a self-collision matrix, where each entry (i, j) indicates whether a check should be performed between shape i and shape j. While the MoveIt Setup Assistant is widely used to generate such matrices, current tools are limited by static visualization, lack of proximity support, rigid single-geometry assumptions, and tedious refinement workflows, hindering flexibility and reuse in downstream robotics applications. In this work, we introduce an interactive tool that overcomes these limitations by generating and visualizing self-collision matrices across multiple shape representations, enabling dynamic inspection, filtering, and refinement of shape pairs. Outputs are provided in both JSON and YAML for easy integration. The system is implemented in Rust and uses the Bevy game engine to deliver high-quality visualizations. We demonstrate its effectiveness on multiple robot platforms, showing that matrices generated using diverse shape types yield faster and more accurate self-collision and self-proximity queries.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u5de5\u5177\uff0c\u80fd\u6709\u6548\u751f\u6210\u548c\u53ef\u89c6\u5316\u673a\u5668\u4eba\u81ea\u78b0\u649e\u77e9\u9635\uff0c\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u7684\u7f3a\u9677\u3002", "motivation": "\u4f20\u7edf\u7684\u81ea\u78b0\u649e\u68c0\u6d4b\u5de5\u5177\u5b58\u5728\u53ef\u89c6\u5316\u9759\u6001\u3001\u65e0\u90bb\u8fd1\u652f\u6301\u3001\u5047\u8bbe\u5355\u4e00\u51e0\u4f55\u4f53\u548c\u7e41\u7410\u7684\u7cbe\u70bc\u5de5\u4f5c\u6d41\u7a0b\u7b49\u5c40\u9650\uff0c\u5f71\u54cd\u4e86\u673a\u5668\u4eba\u5e94\u7528\u7684\u7075\u6d3b\u6027\u548c\u91cd\u7528\u6027\u3002", "method": "\u5229\u7528Rust\u8bed\u8a00\u548cBevy\u6e38\u620f\u5f15\u64ce\u5f00\u53d1\u4e86\u8be5\u7cfb\u7edf\uff0c\u63d0\u4f9bJSON\u548cYAML\u683c\u5f0f\u7684\u8f93\u51fa\uff0c\u65b9\u4fbf\u96c6\u6210\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ea4\u4e92\u5f0f\u5de5\u5177\uff0c\u80fd\u591f\u751f\u6210\u548c\u53ef\u89c6\u5316\u591a\u79cd\u5f62\u72b6\u8868\u793a\u7684\u81ea\u78b0\u649e\u77e9\u9635\uff0c\u652f\u6301\u52a8\u6001\u68c0\u67e5\u3001\u8fc7\u6ee4\u548c\u7cbe\u70bc\u5f62\u72b6\u5bf9\u3002", "conclusion": "\u5b9e\u9a8c\u8868\u660e\uff0c\u91c7\u7528\u591a\u6837\u5316\u5f62\u72b6\u7c7b\u578b\u751f\u6210\u7684\u77e9\u9635\u80fd\u591f\u52a0\u901f\u5e76\u63d0\u9ad8\u81ea\u78b0\u649e\u548c\u81ea\u90bb\u8fd1\u67e5\u8be2\u7684\u7cbe\u5ea6\u3002"}}
{"id": "2512.23141", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.23141", "abs": "https://arxiv.org/abs/2512.23141", "authors": ["Wuhao Xie", "Kanji Tanaka"], "title": "Pole-centric Descriptors for Robust Robot Localization: Evaluation under Pole-at-Distance (PaD) Observations using the Small Pole Landmark (SPL) Dataset", "comment": "3 pages, technical report", "summary": "While pole-like structures are widely recognized as stable geometric anchors for long-term robot localization, their identification reliability degrades significantly under Pole-at-Distance (Pad) observations typical of large-scale urban environments. This paper shifts the focus from descriptor design to a systematic investigation of descriptor robustness. Our primary contribution is the establishment of a specialized evaluation framework centered on the Small Pole Landmark (SPL) dataset. This dataset is constructed via an automated tracking-based association pipeline that captures multi-view, multi-distance observations of the same physical landmarks without manual annotation. Using this framework, we present a comparative analysis of Contrastive Learning (CL) and Supervised Learning (SL) paradigms. Our findings reveal that CL induces a more robust feature space for sparse geometry, achieving superior retrieval performance particularly in the 5--10m range. This work provides an empirical foundation and a scalable methodology for evaluating landmark distinctiveness in challenging real-world scenarios.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5efa\u7acb\u5c0f\u6746\u5730\u6807\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u5bf9\u6bd4\u5b66\u4e60\u76f8\u8f83\u4e8e\u76d1\u7763\u5b66\u4e60\u5728\u7a00\u758f\u51e0\u4f55\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u7ed3\u679c\u663e\u793aCL\u65b9\u6cd5\u5728\u7279\u5b9a\u8ddd\u79bb\u5185\u663e\u8457\u63d0\u5347\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u5728\u5927\u578b\u57ce\u5e02\u73af\u5883\u4e2d\uff0c\u4ee5\u8fdc\u8ddd\u79bb\u89c2\u6d4b\u5230\u7684\u6746\u72b6\u7ed3\u6784\u7684\u8bc6\u522b\u53ef\u9760\u6027\u663e\u8457\u4e0b\u964d\uff0c\u9700\u7cfb\u7edf\u7814\u7a76\u63cf\u8ff0\u7b26\u7684\u9c81\u68d2\u6027\u3002", "method": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u4e13\u95e8\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4e2d\u5fc3\u662f\u901a\u8fc7\u81ea\u52a8\u8ddf\u8e2a\u7684\u5173\u8054\u7ba1\u9053\u6784\u5efa\u7684\u5c0f\u6746\u5730\u6807(SPL)\u6570\u636e\u96c6\u3002", "result": "\u5bf9\u6bd4\u5b66\u4e60\u548c\u76d1\u7763\u5b66\u4e60\u7684\u6bd4\u8f83\u5206\u6790\u663e\u793a\uff0cCL\u65b9\u6cd5\u57285-10\u7c73\u8303\u56f4\u5185\u5177\u6709\u66f4\u597d\u7684\u68c0\u7d22\u6027\u80fd\u3002", "conclusion": "\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u5728\u7a00\u758f\u51e0\u4f55\u4e2d\u751f\u6210\u4e86\u66f4\u53ef\u9760\u7684\u7279\u5f81\u7a7a\u95f4\uff0c\u7279\u522b\u662f\u57285-10\u7c73\u7684\u8ddd\u79bb\u8303\u56f4\u5185\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2512.23153", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.23153", "abs": "https://arxiv.org/abs/2512.23153", "authors": ["Seiko Piotr Yamaguchi", "Kentaro Uno", "Yasumaru Fujii", "Masazumi Imai", "Kazuki Takada", "Taku Okawara", "Kazuya Yoshida"], "title": "Towards the Automation in the Space Station: Feasibility Study and Ground Tests of a Multi-Limbed Intra-Vehicular Robot", "comment": "Author's version of a manuscript accepted at the 2025 IEEE/SICE International Symposium on System Integration (SII). (c) IEEE. The final published version is available at https://doi.org/10.1109/SII59315.2025.10870890", "summary": "This paper presents a feasibility study, including simulations and prototype tests, on the autonomous operation of a multi-limbed intra-vehicular robot (mobile manipulator), shortly MLIVR, designed to assist astronauts with logistical tasks on the International Space Station (ISS). Astronauts spend significant time on tasks such as preparation, close-out, and the collection and transportation of goods, reducing the time available for critical mission activities. Our study explores the potential for a mobile manipulator to support these operations, emphasizing the need for autonomous functionality to minimize crew and ground operator effort while enabling real-time task execution. We focused on the robot's transportation capabilities, simulating its motion planning in 3D space. The actual motion execution was tested with a prototype on a 2D table to mimic a microgravity environment. The results demonstrate the feasibility of performing these tasks with minimal human intervention, offering a promising solution to enhance operational efficiency on the ISS.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4e00\u79cd\u591a\u80a2\u4f53\u592a\u7a7a\u5185\u673a\u5668\u4eba\u5728\u56fd\u9645\u7a7a\u95f4\u7ad9\u4e0a\u81ea\u4e3b\u64cd\u4f5c\u7684\u53ef\u884c\u6027\uff0c\u6d4b\u8bd5\u8868\u660e\u5176\u80fd\u591f\u5728\u6700\u5c11\u4eba\u7c7b\u5e72\u9884\u4e0b\u6267\u884c\u540e\u52e4\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u9ad8\u64cd\u4f5c\u6548\u7387\u3002", "motivation": "\u5e2e\u52a9\u5b87\u822a\u5458\u51cf\u5c11\u5728\u56fd\u9645\u7a7a\u95f4\u7ad9\u7684\u540e\u52e4\u4efb\u52a1\u4e2d\u7684\u65f6\u95f4\uff0c\u91ca\u653e\u66f4\u591a\u65f6\u95f4\u7528\u4e8e\u5173\u952e\u4efb\u52a1", "method": "\u901a\u8fc7\u6a21\u62df\u548c\u539f\u578b\u6d4b\u8bd5\u6765\u7814\u7a76\u591a\u80a2\u4f53\u592a\u7a7a\u5185\u673a\u5668\u4eba\uff08MLIVR\uff09\u7684\u81ea\u4e3b\u64cd\u4f5c\u7684\u53ef\u884c\u6027", "result": "\u5c55\u793a\u4e86\u5728\u6700\u5c11\u4eba\u7c7b\u5e72\u9884\u4e0b\u6267\u884c\u4efb\u52a1\u7684\u53ef\u884c\u6027\uff0c\u63d0\u4f9b\u4e86\u63d0\u5347\u56fd\u9645\u7a7a\u95f4\u7ad9\u64cd\u4f5c\u6548\u7387\u7684\u89e3\u51b3\u65b9\u6848", "conclusion": "\u8be5\u79fb\u52a8\u64cd\u63a7\u5668\u5177\u6709\u5728\u56fd\u9645\u7a7a\u95f4\u7ad9\u652f\u6301\u5b87\u822a\u5458\u64cd\u4f5c\u7684\u6f5c\u529b\uff0c\u503c\u5f97\u8fdb\u4e00\u6b65\u7814\u53d1\u548c\u5e94\u7528\u3002"}}
{"id": "2512.23154", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.23154", "abs": "https://arxiv.org/abs/2512.23154", "authors": ["Keigo Torii", "Kentaro Uno", "Shreya Santra", "Kazuya Yoshida"], "title": "A Sequential Hermaphrodite Coupling Mechanism for Lattice-based Modular Robots", "comment": "Author's version of a manuscript accepted at the 2025 IEEE International Conference on Mechatronics (ICM). (c) IEEE. The final published version is available at https://doi.org/10.1109/ICM62621.2025.10934866", "summary": "Lattice-based modular robot systems are envisioned for large-scale construction in extreme environments, such as space. Coupling mechanisms for heterogeneous structural modules should meet all of the following requirements: single-sided coupling and decoupling, flat surfaces when uncoupled, and coupling to passive coupling interfaces as well as coupling behavior between coupling mechanisms. The design requirements for such a coupling mechanism are complex. We propose a novel shape-matching mechanical coupling mechanism that satisfies these design requirements. This mechanism enables controlled, sequential transitions between male and female states. When uncoupled, all mechanisms are in the female state. To enable single-sided coupling, one side of the mechanisms switches to the male state during the coupling process. Single-sided decoupling is possible not only from the male side but also from the female side by forcibly switching the opposite mechanism's male state to the female state. This coupling mechanism can be applied to various modular robot systems and robot arm tool changers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6ee1\u8db3\u590d\u6742\u8bbe\u8ba1\u8981\u6c42\u7684\u5f62\u72b6\u5339\u914d\u673a\u68b0\u8054\u63a5\u673a\u5236\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u6a21\u5757\u673a\u5668\u4eba\u7cfb\u7edf", "motivation": "\u89e3\u51b3\u5f02\u6784\u7ed3\u6784\u6a21\u5757\u7684\u8026\u5408\u673a\u5236\u8bbe\u8ba1\u590d\u6742\u6027", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5f62\u72b6\u5339\u914d\u673a\u68b0\u8054\u63a5\u673a\u5236", "result": "\u8be5\u673a\u5236\u6ee1\u8db3\u5355\u4fa7\u8026\u5408\u4e0e\u89e3\u8026\uff0c\u5e76\u5728\u89e3\u8026\u65f6\u4fdd\u6301\u5e73\u5766\u8868\u9762", "conclusion": "\u8be5\u8026\u5408\u673a\u5236\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6a21\u5757\u673a\u5668\u4eba\u7cfb\u7edf\u53ca\u673a\u5668\u4eba\u624b\u81c2\u5de5\u5177\u66f4\u6362\u5668\u4e2d\u3002"}}
{"id": "2512.23162", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.23162", "abs": "https://arxiv.org/abs/2512.23162", "authors": ["Yufan He", "Pengfei Guo", "Mengya Xu", "Zhaoshuo Li", "Andriy Myronenko", "Dillan Imans", "Bingjie Liu", "Dongren Yang", "Mingxue Gu", "Yongnan Ji", "Yueming Jin", "Ren Zhao", "Baiyong Shen", "Daguang Xu"], "title": "SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling", "comment": null, "summary": "Data scarcity remains a fundamental barrier to achieving fully autonomous surgical robots. While large scale vision language action (VLA) models have shown impressive generalization in household and industrial manipulation by leveraging paired video action data from diverse domains, surgical robotics suffers from the paucity of datasets that include both visual observations and accurate robot kinematics. In contrast, vast corpora of surgical videos exist, but they lack corresponding action labels, preventing direct application of imitation learning or VLA training. In this work, we aim to alleviate this problem by learning policy models from SurgWorld, a world model designed for surgical physical AI. We curated the Surgical Action Text Alignment (SATA) dataset with detailed action description specifically for surgical robots. Then we built SurgeWorld based on the most advanced physical AI world model and SATA. It's able to generate diverse, generalizable and realistic surgery videos. We are also the first to use an inverse dynamics model to infer pseudokinematics from synthetic surgical videos, producing synthetic paired video action data. We demonstrate that a surgical VLA policy trained with these augmented data significantly outperforms models trained only on real demonstrations on a real surgical robot platform. Our approach offers a scalable path toward autonomous surgical skill acquisition by leveraging the abundance of unlabeled surgical video and generative world modeling, thus opening the door to generalizable and data efficient surgical robot policies.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u6784\u5efaSurgWorld\u548cSATA\u6570\u636e\u96c6\uff0c\u514b\u670d\u4e86\u624b\u672f\u673a\u5668\u4eba\u6570\u636e\u532e\u4e4f\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u81ea\u4e3b\u624b\u672f\u6280\u80fd\u5b66\u4e60\u3002", "motivation": "\u624b\u672f\u673a\u5668\u4eba\u9762\u4e34\u6570\u636e\u532e\u4e4f\u7684\u6311\u6218\uff0c\u4f20\u7edf\u7684\u6a21\u4eff\u5b66\u4e60\u548cVLA\u8bad\u7ec3\u65e0\u6cd5\u6709\u6548\u5e94\u7528\u3002", "method": "\u7814\u7a76\u5229\u7528SurgWorld\u548cSATA\u6570\u636e\u96c6\uff0c\u5efa\u7acb\u4e86\u751f\u6210\u7684\u624b\u672f\u89c6\u9891\uff0c\u5e76\u901a\u8fc7\u9006\u52a8\u529b\u5b66\u6a21\u578b\u63a8\u65ad\u4f2a\u8fd0\u52a8\u5b66\u3002", "result": "\u901a\u8fc7\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u624b\u672fVLA\u653f\u7b56\u5728\u771f\u5b9e\u624b\u672f\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u8868\u73b0\u4f18\u4e8e\u4ec5\u4f7f\u7528\u771f\u5b9e\u6f14\u793a\u7684\u6570\u636e\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u5408\u6210\u89c6\u9891\u52a8\u4f5c\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u624b\u672fVLA\u653f\u7b56\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5bf9\u81ea\u4e3b\u624b\u672f\u6280\u80fd\u83b7\u53d6\u7684\u53ef\u6269\u5c55\u8def\u5f84\u3002"}}
{"id": "2512.23220", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.23220", "abs": "https://arxiv.org/abs/2512.23220", "authors": ["Qin Wang", "Shanmin Pang", "Jianwu Fang", "Shengye Dong", "Fuhao Liu", "Jianru Xue", "Chen Lv"], "title": "A Human-Oriented Cooperative Driving Approach: Integrating Driving Intention, State, and Conflict", "comment": null, "summary": "Human-vehicle cooperative driving serves as a vital bridge to fully autonomous driving by improving driving flexibility and gradually building driver trust and acceptance of autonomous technology. To establish more natural and effective human-vehicle interaction, we propose a Human-Oriented Cooperative Driving (HOCD) approach that primarily minimizes human-machine conflict by prioritizing driver intention and state. In implementation, we take both tactical and operational levels into account to ensure seamless human-vehicle cooperation. At the tactical level, we design an intention-aware trajectory planning method, using intention consistency cost as the core metric to evaluate the trajectory and align it with driver intention. At the operational level, we develop a control authority allocation strategy based on reinforcement learning, optimizing the policy through a designed reward function to achieve consistency between driver state and authority allocation. The results of simulation and human-in-the-loop experiments demonstrate that our proposed approach not only aligns with driver intention in trajectory planning but also ensures a reasonable authority allocation. Compared to other cooperative driving approaches, the proposed HOCD approach significantly enhances driving performance and mitigates human-machine conflict.The code is available at https://github.com/i-Qin/HOCD.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ee5\u4eba\u4e3a\u672c\u7684\u5408\u4f5c\u9a7e\u9a76\u65b9\u6cd5\uff0c\u5173\u6ce8\u9a7e\u9a76\u8005\u610f\u56fe\uff0c\u4f18\u5316\u9a7e\u9a76\u6027\u80fd\uff0c\u51cf\u8f7b\u4eba\u673a\u51b2\u7a81\u3002", "motivation": "\u4e3a\u5b9e\u73b0\u66f4\u81ea\u7136\u6709\u6548\u7684\u4eba\u8f66\u4e92\u52a8\uff0c\u4fc3\u8fdb\u4eba\u8f66\u5408\u4f5c\uff0c\u63d0\u5347\u9a7e\u9a76\u7075\u6d3b\u6027\uff0c\u589e\u5f3a\u9a7e\u9a76\u8005\u5bf9\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u7684\u4fe1\u4efb\u548c\u63a5\u53d7\u5ea6\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u4ee5\u4eba\u4e3a\u672c\u7684\u5408\u4f5c\u9a7e\u9a76(HOCD)\u65b9\u6cd5\uff0c\u5173\u6ce8\u9a7e\u9a76\u8005\u610f\u56fe\u548c\u72b6\u6001\uff0c\u51cf\u5c11\u4eba\u673a\u51b2\u7a81\u3002\u901a\u8fc7\u610f\u56fe\u611f\u77e5\u7684\u8f68\u8ff9\u89c4\u5212\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63a7\u5236\u6743\u9650\u5206\u914d\u6765\u5b9e\u65bd\u3002", "result": "\u4eff\u771f\u548c\u4eba\u673a\u4e92\u52a8\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cHOCD\u65b9\u6cd5\u5728\u8f68\u8ff9\u89c4\u5212\u4e2d\u4e0e\u9a7e\u9a76\u8005\u610f\u56fe\u4fdd\u6301\u4e00\u81f4\uff0c\u5e76\u786e\u4fdd\u5408\u7406\u7684\u6743\u9650\u5206\u914d\uff0c\u663e\u8457\u63d0\u9ad8\u9a7e\u9a76\u6027\u80fd\uff0c\u51cf\u5c11\u4eba\u673a\u51b2\u7a81\u3002", "conclusion": "HOCD\u65b9\u6cd5\u4f18\u4e8e\u5176\u4ed6\u5408\u4f5c\u9a7e\u9a76\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9a7e\u9a76\u8868\u73b0\u5e76\u7f13\u89e3\u4e86\u4eba\u673a\u51b2\u7a81\u3002"}}
{"id": "2512.23257", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.23257", "abs": "https://arxiv.org/abs/2512.23257", "authors": ["Socratis Gkelios", "Savvas D. Apostolidis", "Pavlos Ch. Kapoutsis", "Elias B. Kosmatopoulos", "Athanasios Ch. Kapoutsis"], "title": "Beyond Coverage Path Planning: Can UAV Swarms Perfect Scattered Regions Inspections?", "comment": null, "summary": "Unmanned Aerial Vehicles (UAVs) have revolutionized inspection tasks by offering a safer, more efficient, and flexible alternative to traditional methods. However, battery limitations often constrain their effectiveness, necessitating the development of optimized flight paths and data collection techniques. While existing approaches like coverage path planning (CPP) ensure comprehensive data collection, they can be inefficient, especially when inspecting multiple non connected Regions of Interest (ROIs). This paper introduces the Fast Inspection of Scattered Regions (FISR) problem and proposes a novel solution, the multi UAV Disjoint Areas Inspection (mUDAI) method. The introduced approach implements a two fold optimization procedure, for calculating the best image capturing positions and the most efficient UAV trajectories, balancing data resolution and operational time, minimizing redundant data collection and resource consumption. The mUDAI method is designed to enable rapid, efficient inspections of scattered ROIs, making it ideal for applications such as security infrastructure assessments, agricultural inspections, and emergency site evaluations. A combination of simulated evaluations and real world deployments is used to validate and quantify the method's ability to improve operational efficiency while preserving high quality data capture, demonstrating its effectiveness in real world operations. An open source Python implementation of the mUDAI method can be found on GitHub (https://github.com/soc12/mUDAI) and the collected and processed data from the real world experiments are all hosted on Zenodo (https://zenodo.org/records/13866483). Finally, this online platform (https://sites.google.com/view/mudai-platform/) allows interested readers to interact with the mUDAI method and generate their own multi UAV FISR missions.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u65e0\u4eba\u673a\u4f18\u5316\u68c0\u67e5\u6563\u5e03\u533a\u57df\uff0c\u63d0\u9ad8\u6570\u636e\u6536\u96c6\u6548\u7387\uff0c\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u65e0\u4eba\u673a\u5728\u68c0\u67e5\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u53d7\u5230\u7535\u6c60\u9650\u5236\uff0c\u9700\u8981\u4f18\u5316\u98de\u884c\u8def\u5f84\u548c\u6570\u636e\u6536\u96c6\u6280\u672f\uff0c\u5c24\u5176\u662f\u5728\u68c0\u67e5\u591a\u4e2a\u975e\u8fde\u63a5\u5174\u8da3\u533a\u57df\u65f6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u91cd\u4f18\u5316\u7a0b\u5e8f\uff0c\u8ba1\u7b97\u6700\u4f73\u56fe\u50cf\u91c7\u96c6\u4f4d\u7f6e\u548c\u65e0\u4eba\u673a\u6700\u6709\u6548\u7684\u822a\u8ff9\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u65e0\u4eba\u673a\u65e0\u8fde\u63a5\u533a\u57df\u68c0\u67e5\u65b9\u6cd5\uff08mUDAI\uff09\uff0c\u65e8\u5728\u4f18\u5316\u65e0\u4eba\u673a\u5728\u6563\u5e03\u533a\u57df\u5185\u7684\u68c0\u67e5\u6548\u7387\u3002", "conclusion": "mUDAI\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u64cd\u4f5c\u6548\u7387\uff0c\u5e76\u4fdd\u6301\u4e86\u6570\u636e\u6355\u83b7\u7684\u9ad8\u8d28\u91cf\u3002"}}
{"id": "2512.23312", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23312", "abs": "https://arxiv.org/abs/2512.23312", "authors": ["Sheng-Kai Chen", "Yi-Ling Tsai", "Chun-Chih Chang", "Yan-Chen Chen", "Po-Chiang Lin"], "title": "Explainable Neural Inverse Kinematics for Obstacle-Aware Robotic Manipulation: A Comparative Analysis of IKNet Variants", "comment": "27 pages, 16 figures", "summary": "Deep neural networks have accelerated inverse-kinematics (IK) inference to the point where low cost manipulators can execute complex trajectories in real time, yet the opaque nature of these models contradicts the transparency and safety requirements emerging in responsible AI regulation. This study proposes an explainability centered workflow that integrates Shapley-value attribution with physics-based obstacle avoidance evaluation for the ROBOTIS OpenManipulator-X. Building upon the original IKNet, two lightweight variants-Improved IKNet with residual connections and Focused IKNet with position-orientation decoupling are trained on a large, synthetically generated pose-joint dataset. SHAP is employed to derive both global and local importance rankings, while the InterpretML toolkit visualizes partial-dependence patterns that expose non-linear couplings between Cartesian poses and joint angles. To bridge algorithmic insight and robotic safety, each network is embedded in a simulator that subjects the arm to randomized single and multi-obstacle scenes; forward kinematics, capsule-based collision checks, and trajectory metrics quantify the relationship between attribution balance and physical clearance. Qualitative heat maps reveal that architectures distributing importance more evenly across pose dimensions tend to maintain wider safety margins without compromising positional accuracy. The combined analysis demonstrates that explainable AI(XAI) techniques can illuminate hidden failure modes, guide architectural refinements, and inform obstacle aware deployment strategies for learning based IK. The proposed methodology thus contributes a concrete path toward trustworthy, data-driven manipulation that aligns with emerging responsible-AI standards.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210Shapley\u503c\u5f52\u56e0\u548c\u57fa\u4e8e\u7269\u7406\u7684\u969c\u788d\u7269\u907f\u514d\u8bc4\u4f30\u7684\u53ef\u89e3\u91ca\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4ee5\u63d0\u9ad8\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u9006\u8fd0\u52a8\u5b66\u7684\u900f\u660e\u5ea6\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u968f\u7740\u8d1f\u8d23\u4eba\u5de5\u667a\u80fd\u6cd5\u89c4\u7684\u5174\u8d77\uff0c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u4e0d\u53ef\u89e3\u91ca\u6027\u4e0e\u900f\u660e\u5ea6\u548c\u5b89\u5168\u8981\u6c42\u76f8\u77db\u76fe\u3002", "method": "\u4f7f\u7528Shapley\u503c\u5f52\u56e0\u4e0e\u7269\u7406\u969c\u788d\u7269\u907f\u514d\u6280\u672f\u76f8\u7ed3\u5408\uff0c\u8bad\u7ec3\u4e86\u6539\u8fdb\u7684\u548c\u4e13\u6ce8\u7684\u8f7b\u91cf\u5316\u7f51\u7edc\uff0c\u5e76\u5728\u4eff\u771f\u4e2d\u8fdb\u884c\u6570\u636e\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u66f4\u5747\u5300\u5206\u914d\u91cd\u8981\u6027\u7684\u67b6\u6784\u5728\u4e0d\u5f71\u54cd\u4f4d\u7f6e\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\uff0c\u5f80\u5f80\u80fd\u591f\u7ef4\u6301\u66f4\u5bbd\u7684\u5b89\u5168\u8fb9\u9645\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u53ef\u4fe1\u8d56\u7684\u6570\u636e\u9a71\u52a8\u64cd\u63a7\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u8def\u5f84\uff0c\u7b26\u5408\u65b0\u5174\u7684\u8d1f\u8d23\u4efb\u4eba\u5de5\u667a\u80fd\u6807\u51c6\u3002"}}
{"id": "2512.23318", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.23318", "abs": "https://arxiv.org/abs/2512.23318", "authors": ["Sheng-Kai Chen", "Jie-Yu Chao", "Jr-Yu Chang", "Po-Lien Wu", "Po-Chiang Lin"], "title": "PCR-ORB: Enhanced ORB-SLAM3 with Point Cloud Refinement Using Deep Learning-Based Dynamic Object Filtering", "comment": "17 pages, 2 figures, 1 table", "summary": "Visual Simultaneous Localization and Mapping (vSLAM) systems encounter substantial challenges in dynamic environments where moving objects compromise tracking accuracy and map consistency. This paper introduces PCR-ORB (Point Cloud Refinement ORB), an enhanced ORB-SLAM3 framework that integrates deep learning-based point cloud refinement to mitigate dynamic object interference. Our approach employs YOLOv8 for semantic segmentation combined with CUDA-accelerated processing to achieve real-time performance. The system implements a multi-stage filtering strategy encompassing ground plane estimation, sky region removal, edge filtering, and temporal consistency validation. Comprehensive evaluation on the KITTI dataset (sequences 00-09) demonstrates performance characteristics across different environmental conditions and scene types. Notable improvements are observed in specific sequences, with sequence 04 achieving 25.9% improvement in ATE RMSE and 30.4% improvement in ATE median. However, results show mixed performance across sequences, indicating scenario-dependent effectiveness. The implementation provides insights into dynamic object filtering challenges and opportunities for robust navigation in complex environments.", "AI": {"tldr": "\u672c\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u589e\u5f3a\u7684ORB-SLAM3\u6846\u67b6PCR-ORB\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u6539\u5584\u52a8\u6001\u73af\u5883\u4e0b\u7684\u89c6\u89c9\u540c\u6b65\u5b9a\u4f4d\u4e0e\u5efa\u56fe\uff0c\u53d6\u5f97\u4e86\u5728KITTI\u6570\u636e\u96c6\u4e0a\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u89c6\u89c9\u540c\u6b65\u5b9a\u4f4d\u4e0e\u5efa\u56fe\u7cfb\u7edf\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b58\u5728\u8f83\u5927\u6311\u6218\uff0c\u79fb\u52a8\u7269\u4f53\u4f1a\u5f71\u54cd\u7cfb\u7edf\u7684\u8ffd\u8e2a\u51c6\u786e\u6027\u548c\u5730\u56fe\u4e00\u81f4\u6027\uff0c\u7814\u7a76\u76ee\u7684\u662f\u63d0\u9ad8\u8fd9\u4e9b\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "method": "PCR-ORB\u96c6\u6210\u4e86YOLOv8\u8fdb\u884c\u8bed\u4e49\u5206\u5272\u53caCUDA\u52a0\u901f\u5904\u7406\uff0c\u91c7\u7528\u591a\u9636\u6bb5\u8fc7\u6ee4\u7b56\u7565\uff0c\u5305\u62ec\u5730\u9762\u5e73\u9762\u4f30\u8ba1\u3001\u5929\u7a7a\u533a\u57df\u53bb\u9664\u3001\u8fb9\u7f18\u8fc7\u6ee4\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u9a8c\u8bc1\u3002", "result": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86PCR-ORB\uff08\u70b9\u4e91\u7cbe\u70bcORB\uff09\uff0c\u662f\u4e00\u4e2a\u589e\u5f3a\u7684ORB-SLAM3\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u52a8\u6001\u73af\u5883\u4e2d\u79fb\u52a8\u7269\u4f53\u5bf9\u5b9a\u4f4d\u51c6\u786e\u5ea6\u548c\u5730\u56fe\u4e00\u81f4\u6027\u7684\u5f71\u54cd\uff0c\u96c6\u6210\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u70b9\u4e91\u7cbe\u70bc\u6280\u672f\u3002", "conclusion": "\u5c3d\u7ba1\u5728\u5927\u591a\u6570\u5e8f\u5217\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4f46\u7ed3\u679c\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4e00\uff0c\u663e\u793a\u51fa\u8be5\u65b9\u6cd5\u5728\u573a\u666f\u9002\u5e94\u6027\u4e0a\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2512.23431", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.23431", "abs": "https://arxiv.org/abs/2512.23431", "authors": ["Simay Atasoy Bing\u00f6l", "Tobias T\u00f6pfer", "Sven Kosub", "Heiko Hamann", "Andreagiovanni Reina"], "title": "Optimal Scalability-Aware Allocation of Swarm Robots: From Linear to Retrograde Performance via Marginal Gains", "comment": "14 pages, 11 figures, Accepted for publication in IEEE Transactions on Systems, Man, and Cybernetics: Systems", "summary": "In collective systems, the available agents are a limited resource that must be allocated among tasks to maximize collective performance. Computing the optimal allocation of several agents to numerous tasks through a brute-force approach can be infeasible, especially when each task's performance scales differently with the increase of agents. For example, difficult tasks may require more agents to achieve similar performances compared to simpler tasks, but performance may saturate nonlinearly as the number of allocated agents increases. We propose a computationally efficient algorithm, based on marginal performance gains, for optimally allocating agents to tasks with concave scalability functions, including linear, saturating, and retrograde scaling, to achieve maximum collective performance. We test the algorithm by allocating a simulated robot swarm among collective decision-making tasks, where embodied agents sample their environment and exchange information to reach a consensus on spatially distributed environmental features. We vary task difficulties by different geometrical arrangements of environmental features in space (patchiness). In this scenario, decision performance in each task scales either as a saturating curve (following the Condorcet's Jury Theorem in an interference-free setup) or as a retrograde curve (when physical interference among robots restricts their movement). Using simple robot simulations, we show that our algorithm can be useful in allocating robots among tasks. Our approach aims to advance the deployment of future real-world multi-robot systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u96c6\u4f53\u7cfb\u7edf\u4e2d\u4f18\u5316\u4ee3\u7406\u4eba\u5206\u914d\uff0c\u4ee5\u5b9e\u73b0\u6700\u5927\u5316\u7684\u96c6\u4f53\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u4efb\u52a1\u6027\u80fd\u5bf9\u4ee3\u7406\u4eba\u6570\u91cf\u7684\u975e\u7ebf\u6027\u54cd\u5e94\u4e0b\u3002", "motivation": "\u5728\u96c6\u4f53\u7cfb\u7edf\u4e2d\uff0c\u6709\u9650\u7684\u4ee3\u7406\u4eba\u8d44\u6e90\u9700\u8981\u4f18\u5316\u5206\u914d\uff0c\u4ee5\u6700\u5927\u5316\u96c6\u4f53\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u4efb\u52a1\u7684\u6027\u80fd\u968f\u4ee3\u7406\u4eba\u6570\u91cf\u53d8\u5316\u65f6\uff0c\u8ba1\u7b97\u6700\u4f18\u5206\u914d\u53d8\u5f97\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fb9\u9645\u6027\u80fd\u589e\u76ca\u7684\u7b97\u6cd5\uff0c\u9488\u5bf9\u5177\u6709\u51f9\u5f62\u53ef\u6269\u5c55\u6027\u7684\u4efb\u52a1\u8fdb\u884c\u4ee3\u7406\u4eba\u5206\u914d\u3002", "result": "\u901a\u8fc7\u5728\u96c6\u4f53\u51b3\u7b56\u4efb\u52a1\u4e2d\u5206\u914d\u6a21\u62df\u673a\u5668\u4eba\u7fa4\uff0c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4efb\u52a1\u96be\u5ea6\u901a\u8fc7\u73af\u5883\u7279\u5f81\u7684\u51e0\u4f55\u6392\u5217\u53d8\u5316\u3002", "conclusion": "\u6211\u4eec\u7684\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5728\u4eba\u7fa4\u4efb\u52a1\u4e2d\u5206\u914d\u673a\u5668\u4eba\uff0c\u65e8\u5728\u63d0\u5347\u672a\u6765\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u90e8\u7f72\u6548\u7387\u3002"}}
{"id": "2512.23482", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23482", "abs": "https://arxiv.org/abs/2512.23482", "authors": ["Marie Bauer", "Julia Gachot", "Matthias Kerzel", "Cornelius Weber", "Stefan Wermter"], "title": "Theory of Mind for Explainable Human-Robot Interaction", "comment": null, "summary": "Within the context of human-robot interaction (HRI), Theory of Mind (ToM) is intended to serve as a user-friendly backend to the interface of robotic systems, enabling robots to infer and respond to human mental states. When integrated into robots, ToM allows them to adapt their internal models to users' behaviors, enhancing the interpretability and predictability of their actions. Similarly, Explainable Artificial Intelligence (XAI) aims to make AI systems transparent and interpretable, allowing humans to understand and interact with them effectively. Since ToM in HRI serves related purposes, we propose to consider ToM as a form of XAI and evaluate it through the eValuation XAI (VXAI) framework and its seven desiderata. This paper identifies a critical gap in the application of ToM within HRI, as existing methods rarely assess the extent to which explanations correspond to the robot's actual internal reasoning. To address this limitation, we propose to integrate ToM within XAI frameworks. By embedding ToM principles inside XAI, we argue for a shift in perspective, as current XAI research focuses predominantly on the AI system itself and often lacks user-centered explanations. Incorporating ToM would enable a change in focus, prioritizing the user's informational needs and perspective.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5c06\u5fc3\u667a\u7406\u8bba\uff08ToM\uff09\u4f5c\u4e3a\u4e00\u79cd\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u5f62\u5f0f\u5f15\u5165\u4eba\u673a\u4ea4\u4e92\uff08HRI\uff09\uff0c\u5e76\u901a\u8fc7VXAI\u6846\u67b6\u8bc4\u4f30\u5176\u6709\u6548\u6027\uff0c\u5f3a\u8c03\u7528\u6237\u4e2d\u5fc3\u7684\u89e3\u91ca\u91cd\u8981\u6027\u3002", "motivation": "\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u7406\u89e3\u548c\u56de\u5e94\u4eba\u7c7b\u7684\u5fc3\u7406\u72b6\u6001\uff0c\u63d0\u9ad8\u4eba\u673a\u4ea4\u4e92\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9884\u6d4b\u6027\u3002", "method": "\u5c06\u5fc3\u667a\u7406\u8bba\uff08ToM\uff09\u4e0e\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u6846\u67b6\u96c6\u6210\uff0c\u5e76\u901a\u8fc7eValuation XAI\uff08VXAI\uff09\u6846\u67b6\u8bc4\u4f30ToM\u3002", "result": "\u8bc6\u522b\u51faToM\u5728HRI\u4e2d\u7684\u5e94\u7528\u5b58\u5728\u7f3a\u53e3\uff0c\u63d0\u51fa\u5c06ToM\u539f\u5219\u5d4c\u5165XAI\u6846\u67b6\uff0c\u4ee5\u5173\u6ce8\u7528\u6237\u7684\u4fe1\u606f\u9700\u6c42\u548c\u89c2\u70b9\u3002", "conclusion": "\u5c06ToM\u96c6\u6210\u5230XAI\u4e2d\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u7814\u7a76\u65b9\u5411\u7684\u8f6c\u53d8\uff0c\u66f4\u52a0\u5173\u6ce8\u7528\u6237\u7684\u9700\u6c42\u4e0e\u89c6\u89d2\u3002"}}
{"id": "2512.23505", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.23505", "abs": "https://arxiv.org/abs/2512.23505", "authors": ["Mehdi Heydari Shahna"], "title": "Robust Deep Learning Control with Guaranteed Performance for Safe and Reliable Robotization in Heavy-Duty Machinery", "comment": "Doctoral Dissertation, Tampere University", "summary": "Today's heavy-duty mobile machines (HDMMs) face two transitions: from diesel-hydraulic actuation to clean electric systems driven by climate goals, and from human supervision toward greater autonomy. Diesel-hydraulic systems have long dominated, so full electrification, via direct replacement or redesign, raises major technical and economic challenges. Although advanced artificial intelligence (AI) could enable higher autonomy, adoption in HDMMs is limited by strict safety requirements, and these machines still rely heavily on human supervision.\n  This dissertation develops a control framework that (1) simplifies control design for electrified HDMMs through a generic modular approach that is energy-source independent and supports future modifications, and (2) defines hierarchical control policies that partially integrate AI while guaranteeing safety-defined performance and stability.\n  Five research questions align with three lines of investigation: a generic robust control strategy for multi-body HDMMs with strong stability across actuation types and energy sources; control solutions that keep strict performance under uncertainty and faults while balancing robustness and responsiveness; and methods to interpret and trust black-box learning strategies so they can be integrated stably and verified against international safety standards.\n  The framework is validated in three case studies spanning different actuators and conditions, covering heavy-duty mobile robots and robotic manipulators. Results appear in five peer-reviewed publications and one unpublished manuscript, advancing nonlinear control and robotics and supporting both transitions.", "AI": {"tldr": "\u672c\u8bba\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u63a7\u5236\u6846\u67b6\uff0c\u65e8\u5728\u7b80\u5316\u91cd\u578b\u79fb\u52a8\u673a\u5668\uff08HDMMs\uff09\u7684\u7535\u6c14\u5316\u63a7\u5236\u8bbe\u8ba1\uff0c\u90e8\u5206\u6574\u5408\u4eba\u5de5\u667a\u80fd\uff0c\u540c\u65f6\u4fdd\u8bc1\u5b89\u5168\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u5e94\u5bf9\u91cd\u578b\u79fb\u52a8\u673a\u5668\u5411\u6d01\u51c0\u7535\u6c14\u7cfb\u7edf\u548c\u66f4\u9ad8\u81ea\u4e3b\u6027\u7684\u8f6c\u578b\uff0c\u4ee5\u53ca\u514b\u670d\u4f20\u7edf\u67f4\u6cb9\u6db2\u538b\u7cfb\u7edf\u5e26\u6765\u7684\u6280\u672f\u548c\u7ecf\u6d4e\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u901a\u7528\u7684\u6a21\u5757\u5316\u63a7\u5236\u65b9\u6cd5\uff0c\u5e76\u5b9a\u4e49\u5206\u5c42\u63a7\u5236\u7b56\u7565\u4ee5\u90e8\u5206\u96c6\u6210\u4eba\u5de5\u667a\u80fd\uff0c\u786e\u4fdd\u7cfb\u7edf\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002", "result": "\u6846\u67b6\u5728\u4e0d\u540c\u9a71\u52a8\u548c\u6761\u4ef6\u4e0b\u7684\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u4e2d\u5f97\u5230\u9a8c\u8bc1\uff0c\u5e76\u5728\u4e94\u7bc7\u7ecf\u8fc7\u540c\u884c\u8bc4\u5ba1\u7684\u51fa\u7248\u7269\u548c\u4e00\u7bc7\u672a\u53d1\u8868\u7684\u624b\u7a3f\u4e2d\u5c55\u793a\u4e86\u7814\u7a76\u6210\u679c\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5bf9\u4e0d\u540c\u9a71\u52a8\u7c7b\u578b\u548c\u80fd\u91cf\u6765\u6e90\u7684\u6709\u6548\u63a7\u5236\uff0c\u652f\u6301\u91cd\u578b\u79fb\u52a8\u673a\u5668\u7684\u7535\u6c14\u5316\u548c\u81ea\u4e3b\u5316\u8f6c\u578b\u3002"}}
{"id": "2512.23541", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23541", "abs": "https://arxiv.org/abs/2512.23541", "authors": ["Pengfei Zhou", "Liliang Chen", "Shengcong Chen", "Di Chen", "Wenzhi Zhao", "Rongjun Jin", "Guanghui Ren", "Jianlan Luo"], "title": "Act2Goal: From World Model To General Goal-conditioned Policy", "comment": null, "summary": "Specifying robotic manipulation tasks in a manner that is both expressive and precise remains a central challenge. While visual goals provide a compact and unambiguous task specification, existing goal-conditioned policies often struggle with long-horizon manipulation due to their reliance on single-step action prediction without explicit modeling of task progress. We propose Act2Goal, a general goal-conditioned manipulation policy that integrates a goal-conditioned visual world model with multi-scale temporal control. Given a current observation and a target visual goal, the world model generates a plausible sequence of intermediate visual states that captures long-horizon structure. To translate this visual plan into robust execution, we introduce Multi-Scale Temporal Hashing (MSTH), which decomposes the imagined trajectory into dense proximal frames for fine-grained closed-loop control and sparse distal frames that anchor global task consistency. The policy couples these representations with motor control through end-to-end cross-attention, enabling coherent long-horizon behavior while remaining reactive to local disturbances. Act2Goal achieves strong zero-shot generalization to novel objects, spatial layouts, and environments. We further enable reward-free online adaptation through hindsight goal relabeling with LoRA-based finetuning, allowing rapid autonomous improvement without external supervision. Real-robot experiments demonstrate that Act2Goal improves success rates from 30% to 90% on challenging out-of-distribution tasks within minutes of autonomous interaction, validating that goal-conditioned world models with multi-scale temporal control provide structured guidance necessary for robust long-horizon manipulation. Project page: https://act2goal.github.io/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u673a\u5668\u4eba\u64cd\u7eb5\u7b56\u7565Act2Goal\uff0c\u901a\u8fc7\u76ee\u6807\u6761\u4ef6\u89c6\u89c9\u4e16\u754c\u6a21\u578b\u548c\u591a\u5c3a\u5ea6\u65f6\u95f4\u63a7\u5236\u663e\u8457\u63d0\u5347\u4e86\u64cd\u63a7\u6027\u80fd\u3002", "motivation": "\u9700\u8981\u4e00\u79cd\u65e2\u8868\u8fbe\u53c8\u7cbe\u786e\u7684\u673a\u5668\u4eba\u64cd\u7eb5\u4efb\u52a1\u89c4\u683c\u65b9\u6cd5\uff0c\u73b0\u6709\u7b56\u7565\u5728\u957f\u65f6\u95f4\u64cd\u63a7\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "Act2Goal\uff0c\u4e00\u4e2a\u6574\u5408\u76ee\u6807\u6761\u4ef6\u89c6\u89c9\u4e16\u754c\u6a21\u578b\u4e0e\u591a\u5c3a\u5ea6\u65f6\u95f4\u63a7\u5236\u7684\u64cd\u63a7\u7b56\u7565", "result": "Act2Goal\u5728\u957f\u65f6\u95f4\u64cd\u4f5c\u4e2d\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u6210\u529f\u7387\u4ece30%\u63d0\u5347\u81f390%\u3002", "conclusion": "\u76ee\u6807\u6761\u4ef6\u7684\u4e16\u754c\u6a21\u578b\u4e0e\u591a\u5c3a\u5ea6\u65f6\u95f4\u63a7\u5236\u4e3a\u7a33\u5065\u7684\u957f\u65f6\u95f4\u64cd\u63a7\u63d0\u4f9b\u4e86\u5fc5\u8981\u7684\u7ed3\u6784\u6027\u6307\u5bfc\u3002"}}
{"id": "2512.23585", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.23585", "abs": "https://arxiv.org/abs/2512.23585", "authors": ["Dat Le", "Thomas Manhardt", "Moritz Venator", "Johannes Betz"], "title": "Unsupervised Learning for Detection of Rare Driving Scenarios", "comment": null, "summary": "The detection of rare and hazardous driving scenarios is a critical challenge for ensuring the safety and reliability of autonomous systems. This research explores an unsupervised learning framework for detecting rare and extreme driving scenarios using naturalistic driving data (NDD). We leverage the recently proposed Deep Isolation Forest (DIF), an anomaly detection algorithm that combines neural network-based feature representations with Isolation Forests (IFs), to identify non-linear and complex anomalies. Data from perception modules, capturing vehicle dynamics and environmental conditions, is preprocessed into structured statistical features extracted from sliding windows. The framework incorporates t-distributed stochastic neighbor embedding (t-SNE) for dimensionality reduction and visualization, enabling better interpretability of detected anomalies. Evaluation is conducted using a proxy ground truth, combining quantitative metrics with qualitative video frame inspection. Our results demonstrate that the proposed approach effectively identifies rare and hazardous driving scenarios, providing a scalable solution for anomaly detection in autonomous driving systems. Given the study's methodology, it was unavoidable to depend on proxy ground truth and manually defined feature combinations, which do not encompass the full range of real-world driving anomalies or their nuanced contextual dependencies.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65e0\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b64\u7acb\u68ee\u6797\u7b97\u6cd5\u8bc6\u522b\u7a00\u6709\u9a7e\u9a76\u573a\u666f\uff0c\u6709\u6548\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5b89\u5168\u6027\u3002", "motivation": "\u786e\u4fdd\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\uff0c\u68c0\u6d4b\u7a00\u6709\u548c\u5371\u9669\u7684\u9a7e\u9a76\u573a\u666f", "method": "\u65e0\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u7ed3\u5408\u6df1\u5ea6\u5b64\u7acb\u68ee\u6797\uff08DIF\uff09\u7b97\u6cd5", "result": "\u6709\u6548\u8bc6\u522b\u7a00\u6709\u548c\u5371\u9669\u7684\u9a7e\u9a76\u573a\u666f\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848", "conclusion": "\u672c\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u4ee3\u7406\u771f\u5b9e\u60c5\u51b5\u548c\u624b\u52a8\u5b9a\u4e49\u7684\u7279\u5f81\u7ec4\u5408\uff0c\u8fd9\u4e9b\u7ec4\u5408\u65e0\u6cd5\u6db5\u76d6\u771f\u5b9e\u4e16\u754c\u9a7e\u9a76\u5f02\u5e38\u7684\u5168\u8303\u56f4\u53ca\u5176\u7ec6\u5fae\u7684\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\u3002"}}
{"id": "2512.23593", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.23593", "abs": "https://arxiv.org/abs/2512.23593", "authors": ["Nikolai Beving", "Jonas Marxen", "Steffen Mueller", "Johannes Betz"], "title": "A Kalman Filter-Based Disturbance Observer for Steer-by-Wire Systems", "comment": null, "summary": "Steer-by-Wire systems replace mechanical linkages, which provide benefits like weight reduction, design flexibility, and compatibility with autonomous driving. However, they are susceptible to high-frequency disturbances from unintentional driver torque, known as driver impedance, which can degrade steering performance. Existing approaches either rely on direct torque sensors, which are costly and impractical, or lack the temporal resolution to capture rapid, high-frequency driver-induced disturbances. We address this limitation by designing a Kalman filter-based disturbance observer that estimates high-frequency driver torque using only motor state measurements. We model the drivers passive torque as an extended state using a PT1-lag approximation and integrate it into both linear and nonlinear Steer-by-Wire system models. In this paper, we present the design, implementation and simulation of this disturbance observer with an evaluation of different Kalman filter variants. Our findings indicate that the proposed disturbance observer accurately reconstructs driver-induced disturbances with only minimal delay 14ms. We show that a nonlinear extended Kalman Filter outperforms its linear counterpart in handling frictional nonlinearities, improving estimation during transitions from static to dynamic friction. Given the study's methodology, it was unavoidable to rely on simulation-based validation rather than real-world experimentation. Further studies are needed to investigate the robustness of the observers under real-world driving conditions.", "AI": {"tldr": "\u672c\u6587\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5361\u5c14\u66fc\u6ee4\u6ce2\u5e72\u6270\u89c2\u5bdf\u5668\uff0c\u901a\u8fc7\u7535\u673a\u72b6\u6001\u6d4b\u91cf\u6765\u4f30\u8ba1\u9a7e\u9a76\u5458\u626d\u77e9\uff0c\u4e14\u975e\u7ebf\u6027\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u5728\u5904\u7406\u6469\u64e6\u975e\u7ebf\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "Steer-by-Wire\u7cfb\u7edf\u7684\u5f15\u5165\u65e8\u5728\u51cf\u5c11\u673a\u68b0\u8fde\u63a5\u548c\u63d0\u9ad8\u8bbe\u8ba1\u7075\u6d3b\u6027\uff0c\u4f46\u5176\u53d7\u5230\u6765\u81ea\u9a7e\u9a76\u5458\u65e0\u610f\u626d\u77e9\u7684\u9ad8\u9891\u5e72\u6270\u5f71\u54cd\uff0c\u5f71\u54cd\u65b9\u5411\u64cd\u63a7\u6027\u80fd\uff0c\u56e0\u800c\u4e9f\u9700\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5b9e\u65f6\u76d1\u6d4b\u548c\u8865\u507f\u8fd9\u4e9b\u5e72\u6270\u3002", "method": "\u6211\u4eec\u5c06\u9a7e\u9a76\u5458\u7684\u88ab\u52a8\u626d\u77e9\u5efa\u6a21\u4e3a\u6269\u5c55\u72b6\u6001\uff0c\u5e76\u7ed3\u5408PT1-lag\u8fd1\u4f3c\uff0c\u6269\u5c55\u4e86\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u7684Steer-by-Wire\u7cfb\u7edf\u6a21\u578b\u3002\u8bbe\u8ba1\u548c\u5b9e\u73b0\u4e86\u5e72\u6270\u89c2\u5bdf\u5668\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e0d\u540c\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u7684\u6548\u679c\u3002", "result": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u5e72\u6270\u89c2\u5bdf\u5668\uff0c\u7528\u4e8e\u51c6\u786e\u4f30\u8ba1\u9ad8\u7ea7\u522b\u9a7e\u9a76\u5458\u626d\u77e9\uff0c\u4ee5\u6539\u5584Steer-by-Wire\u7cfb\u7edf\u7684\u64cd\u63a7\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u7535\u673a\u72b6\u6001\u6d4b\u91cf\uff0c\u514b\u670d\u4e86\u6210\u672c\u9ad8\u548c\u65f6\u95f4\u5206\u8fa8\u7387\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "conclusion": "\u867d\u7136\u672c\u7814\u7a76\u91c7\u7528\u6a21\u62df\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u4f46\u63d0\u51fa\u7684\u5e72\u6270\u89c2\u5bdf\u5668\u80fd\u591f\u5728\u5ef6\u8fdf\u4ec514\u6beb\u79d2\u7684\u60c5\u51b5\u4e0b\uff0c\u51c6\u786e\u91cd\u5efa\u9a7e\u9a76\u5458\u5f15\u8d77\u7684\u5e72\u6270\uff0c\u4e3aSteer-by-Wire\u7cfb\u7edf\u7684\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2512.23616", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.23616", "abs": "https://arxiv.org/abs/2512.23616", "authors": ["Christoph Willibald", "Lugh Martensen", "Thomas Eiband", "Dongheui Lee"], "title": "Interactive Robot Programming for Surface Finishing via Task-Centric Mixed Reality Interfaces", "comment": "Currently under review at Intelligent Service Robotics", "summary": "Lengthy setup processes that require robotics expertise remain a major barrier to deploying robots for tasks involving high product variability and small batch sizes. As a result, collaborative robots, despite their advanced sensing and control capabilities, are rarely used for surface finishing in small-scale craft and manufacturing settings. To address this gap, we propose a novel robot programming approach that enables non-experts to intuitively program robots through interactive, task-focused workflows. For that, we developed a new surface segmentation algorithm that incorporates human input to identify and refine workpiece regions for processing. Throughout the programming process, users receive continuous visual feedback on the robot's learned model, enabling them to iteratively refine the segmentation result. Based on the segmented surface model, a robot trajectory is generated to cover the desired processing area. We evaluated multiple interaction designs across two comprehensive user studies to derive an optimal interface that significantly reduces user workload, improves usability and enables effective task programming even for users with limited practical experience.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u4f7f\u975e\u4e13\u5bb6\u80fd\u591f\u8f7b\u677e\u7f16\u7a0b\u673a\u5668\u4eba\u8fdb\u884c\u5c0f\u6279\u91cf\u751f\u4ea7\u4e2d\u7684\u8868\u9762\u5904\u7406\u3002", "motivation": "\u89e3\u51b3\u9ad8\u4ea7\u54c1\u53d8\u5f02\u6027\u548c\u5c0f\u6279\u91cf\u751f\u4ea7\u4e2d\u673a\u5668\u4eba\u90e8\u7f72\u7684\u969c\u788d\uff0c\u7279\u522b\u662f\u8868\u9762\u7cbe\u6574\u4efb\u52a1\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u673a\u5668\u4eba\u7f16\u7a0b\u65b9\u6cd5\uff0c\u4f7f\u975e\u4e13\u5bb6\u80fd\u591f\u901a\u8fc7\u4ea4\u4e92\u5f0f\u3001\u4ee5\u4efb\u52a1\u4e3a\u4e2d\u5fc3\u7684\u5de5\u4f5c\u6d41\u7a0b\u76f4\u89c2\u5730\u7f16\u7a0b\u673a\u5668\u4eba\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u8868\u9762\u5206\u5272\u7b97\u6cd5\uff0c\u7ed3\u5408\u4eba\u7c7b\u8f93\u5165\u6765\u8bc6\u522b\u548c\u4f18\u5316\u52a0\u5de5\u5de5\u4ef6\u533a\u57df\uff0c\u5e76\u5728\u7f16\u7a0b\u8fc7\u7a0b\u4e2d\u63d0\u4f9b\u8fde\u7eed\u7684\u89c6\u89c9\u53cd\u9988\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u754c\u9762\u8bbe\u8ba1\uff0c\u663e\u8457\u964d\u4f4e\u7528\u6237\u5de5\u4f5c\u8d1f\u62c5\uff0c\u63d0\u9ad8\u53ef\u7528\u6027\uff0c\u4f7f\u5f97\u6709\u9650\u5b9e\u8df5\u7ecf\u9a8c\u7684\u7528\u6237\u4e5f\u80fd\u6709\u6548\u7f16\u7a0b\u3002"}}
{"id": "2512.23619", "categories": ["cs.RO", "math.GT", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.23619", "abs": "https://arxiv.org/abs/2512.23619", "authors": ["Antonio Franchi"], "title": "The N-5 Scaling Law: Topological Dimensionality Reduction in the Optimal Design of Fully-actuated Multirotors", "comment": null, "summary": "The geometric design of fully-actuated and omnidirectional N-rotor aerial vehicles is conventionally formulated as a parametric optimization problem, seeking a single optimal set of N orientations within a fixed architectural family. This work departs from that paradigm to investigate the intrinsic topological structure of the optimization landscape itself. We formulate the design problem on the product manifold of Projective Lines \\RP^2^N, fixing the rotor positions to the vertices of polyhedral chassis while varying their lines of action. By minimizing a coordinate-invariant Log-Volume isotropy metric, we reveal that the topology of the global optima is governed strictly by the symmetry of the chassis. For generic (irregular) vertex arrangements, the solutions appear as a discrete set of isolated points. However, as the chassis geometry approaches regularity, the solution space undergoes a critical phase transition, collapsing onto an N-dimensional Torus of the lines tangent at the vertexes to the circumscribing sphere of the chassis, and subsequently reducing to continuous 1-dimensional curves driven by Affine Phase Locking. We synthesize these observations into the N-5 Scaling Law: an empirical relationship holding for all examined regular planar polygons and Platonic solids (N <= 10), where the space of optimal configurations consists of K=N-5 disconnected 1D topological branches. We demonstrate that these locking patterns correspond to a sequence of admissible Star Polygons {N/q}, allowing for the exact prediction of optimal phases for arbitrary N. Crucially, this topology reveals a design redundancy that enables optimality-preserving morphing: the vehicle can continuously reconfigure along these branches while preserving optimal isotropic control authority.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u5168\u9a71\u52a8\u548c\u5168\u5411N-\u8f6c\u5b50\u98de\u884c\u5668\u7684\u8bbe\u8ba1\u95ee\u9898\u7684\u5185\u5728\u62d3\u6251\u7ed3\u6784\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u5bf9\u6570\u4f53\u79ef\u5404\u5411\u540c\u6027\u5ea6\u91cf\u53d1\u73b0\u4e86\u8bbe\u8ba1\u5197\u4f59\uff0c\u5b9e\u73b0\u4e86\u4f18\u5316\u4fdd\u7559\u7684\u53d8\u5f62\u3002", "motivation": "\u7814\u7a76\u4f18\u5316\u666f\u89c2\u672c\u8eab\u7684\u5185\u5728\u62d3\u6251\u7ed3\u6784\uff0c\u8d85\u8d8a\u4f20\u7edf\u7684\u53c2\u6570\u4f18\u5316\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5728\u6295\u5f71\u7ebf\u7684\u4e58\u79ef\u6d41\u5f62\u4e0a\u8bbe\u5b9a\u8bbe\u8ba1\u95ee\u9898\uff0c\u56fa\u5b9a\u8f6c\u5b50\u4f4d\u7f6e\uff0c\u6539\u53d8\u5176\u4f5c\u7528\u7ebf\uff0c\u5229\u7528\u5750\u6807\u4e0d\u53d8\u7684\u5bf9\u6570\u4f53\u79ef\u5404\u5411\u540c\u6027\u5ea6\u91cf\u6765\u6700\u5c0f\u5316\u3002", "result": "\u53d1\u73b0\u5168\u5c40\u6700\u4f18\u89e3\u7684\u62d3\u6251\u7ed3\u6784\u4e25\u683c\u53d7\u5e95\u76d8\u7684\u5bf9\u79f0\u6027\u652f\u914d\uff0c\u5e76\u63ed\u793a\u4e86\u5728\u5e95\u67b6\u51e0\u4f55\u63a5\u8fd1\u89c4\u5219\u65f6\uff0c\u89e3\u7a7a\u95f4\u7ecf\u5386\u4e34\u754c\u76f8\u53d8\u3002", "conclusion": "\u8f66\u8f86\u53ef\u4ee5\u5728\u8fd9\u4e9b\u5206\u652f\u4e0a\u6301\u7eed\u91cd\u6784\uff0c\u540c\u65f6\u4fdd\u6301\u6700\u4f18\u7684\u5404\u5411\u540c\u6027\u63a7\u5236\u80fd\u529b\u3002"}}
{"id": "2512.23649", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.23649", "abs": "https://arxiv.org/abs/2512.23649", "authors": ["Zhe Li", "Cheng Chi", "Yangyang Wei", "Boan Zhu", "Tao Huang", "Zhenguo Sun", "Yibo Peng", "Pengwei Wang", "Zhongyuan Wang", "Fangzhou Liu", "Chang Xu", "Shanghang Zhang"], "title": "RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion", "comment": null, "summary": "Humans learn locomotion through visual observation, interpreting visual content first before imitating actions. However, state-of-the-art humanoid locomotion systems rely on either curated motion capture trajectories or sparse text commands, leaving a critical gap between visual understanding and control. Text-to-motion methods suffer from semantic sparsity and staged pipeline errors, while video-based approaches only perform mechanical pose mimicry without genuine visual understanding. We propose RoboMirror, the first retargeting-free video-to-locomotion framework embodying \"understand before you imitate\". Leveraging VLMs, it distills raw egocentric/third-person videos into visual motion intents, which directly condition a diffusion-based policy to generate physically plausible, semantically aligned locomotion without explicit pose reconstruction or retargeting. Extensive experiments validate the effectiveness of RoboMirror, it enables telepresence via egocentric videos, drastically reduces third-person control latency by 80%, and achieves a 3.7% higher task success rate than baselines. By reframing humanoid control around video understanding, we bridge the visual understanding and action gap.", "AI": {"tldr": "\u63d0\u51faRoboMirror\uff0c\u8fd9\u662f\u4e00\u79cd\u65e0\u91cd\u5b9a\u5411\u7684\u89c6\u9891\u5230\u8fd0\u52a8\u751f\u6210\u6846\u67b6\uff0c\u57fa\u4e8e\u89c6\u9891\u7406\u89e3\u5b9e\u73b0\u6709\u6548\u7684\u8fd0\u52a8\u63a7\u5236\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u89c9\u7406\u89e3\u4e0e\u63a7\u5236\u4e4b\u95f4\u7684\u5dee\u8ddd", "method": "RoboMirror\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u9891\u7406\u89e3\u9a71\u52a8\u4eba\u5f62\u8fd0\u52a8\u751f\u6210", "result": "RoboMirror\u80fd\u6709\u6548\u5b9e\u73b0\u901a\u8fc7\u89c6\u9891\u8fdb\u884c\u9065\u63a7\uff0c\u964d\u4f4e\u63a7\u5236\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387", "conclusion": "\u901a\u8fc7\u89c6\u9891\u7406\u89e3\u91cd\u5851\u4eba\u5f62\u63a7\u5236\uff0c\u5f25\u5408\u89c6\u89c9\u7406\u89e3\u4e0e\u884c\u52a8\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002"}}
{"id": "2512.23650", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.23650", "abs": "https://arxiv.org/abs/2512.23650", "authors": ["Zhe Li", "Cheng Chi", "Yangyang Wei", "Boan Zhu", "Tao Huang", "Zhenguo Sun", "Yibo Peng", "Pengwei Wang", "Zhongyuan Wang", "Fangzhou Liu", "Chang Xu", "Shanghang Zhang"], "title": "Do You Have Freestyle? Expressive Humanoid Locomotion via Audio Control", "comment": null, "summary": "Humans intuitively move to sound, but current humanoid robots lack expressive improvisational capabilities, confined to predefined motions or sparse commands. Generating motion from audio and then retargeting it to robots relies on explicit motion reconstruction, leading to cascaded errors, high latency, and disjointed acoustic-actuation mapping. We propose RoboPerform, the first unified audio-to-locomotion framework that can directly generate music-driven dance and speech-driven co-speech gestures from audio. Guided by the core principle of \"motion = content + style\", the framework treats audio as implicit style signals and eliminates the need for explicit motion reconstruction. RoboPerform integrates a ResMoE teacher policy for adapting to diverse motion patterns and a diffusion-based student policy for audio style injection. This retargeting-free design ensures low latency and high fidelity. Experimental validation shows that RoboPerform achieves promising results in physical plausibility and audio alignment, successfully transforming robots into responsive performers capable of reacting to audio.", "AI": {"tldr": "RoboPerform\u662f\u4e00\u4e2a\u5c06\u97f3\u9891\u8f6c\u4e3a\u673a\u5668\u52a8\u4f5c\u7684\u6846\u67b6\uff0c\u80fd\u6709\u6548\u5e94\u5bf9\u5f53\u524d\u7c7b\u4eba\u673a\u5668\u4eba\u5728\u5373\u5174\u8868\u6f14\u4e0a\u7684\u5c40\u9650\uff0c\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u4fdd\u771f\u7684\u97f3\u9891\u54cd\u5e94\u3002", "motivation": "\u5f53\u524d\u7684\u7c7b\u4eba\u673a\u5668\u4eba\u7f3a\u4e4f\u5373\u5174\u8868\u6f14\u80fd\u529b\uff0c\u65e0\u6cd5\u6709\u6548\u5730\u5c06\u97f3\u9891\u8f6c\u5316\u4e3a\u8fd0\u52a8\uff0c\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\u548c\u4e0d\u8fde\u8d2f\u7684\u52a8\u4f5c\u6620\u5c04\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u97f3\u9891\u5230\u8fd0\u52a8\u6846\u67b6RoboPerform\uff0c\u91c7\u7528\u9690\u5f0f\u6837\u5f0f\u4fe1\u53f7\uff0c\u6574\u5408\u4e86ResMoE\u6559\u5e08\u7b56\u7565\u548c\u57fa\u4e8e\u6269\u6563\u7684\u5b66\u751f\u7b56\u7565\u3002", "result": "RoboPerform\u5728\u7269\u7406\u53ef\u884c\u6027\u548c\u97f3\u9891\u5bf9\u9f50\u65b9\u9762\u53d6\u5f97\u4e86\u826f\u597d\u7ed3\u679c\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u97f3\u9891\u9a71\u52a8\u7684\u673a\u5668\u4eba\u4f53\u6001\u53d8\u5316\u3002", "conclusion": "RoboPerform\u80fd\u6709\u6548\u5730\u5c06\u97f3\u9891\u8f6c\u6362\u4e3a\u673a\u5668\u4eba\u7684\u821e\u8e48\u548c\u624b\u52bf\u52a8\u4f5c\uff0c\u5c55\u73b0\u51fa\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u4fdd\u771f\u5ea6\uff0c\u6210\u529f\u5b9e\u73b0\u673a\u5668\u4eba\u7684\u54cd\u5e94\u8868\u6f14\u80fd\u529b\u3002"}}
{"id": "2512.23672", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.23672", "abs": "https://arxiv.org/abs/2512.23672", "authors": ["Mohammed Baziyad", "Manal Al Shohna", "Tamer Rabie"], "title": "The Bulldozer Technique: Efficient Elimination of Local Minima Traps for APF-Based Robot Navigation", "comment": null, "summary": "Path planning is a fundamental component in autonomous mobile robotics, enabling a robot to navigate from its current location to a desired goal while avoiding obstacles. Among the various techniques, Artificial Potential Field (APF) methods have gained popularity due to their simplicity, real-time responsiveness, and low computational requirements. However, a major limitation of conventional APF approaches is the local minima trap problem, where the robot becomes stuck in a position with no clear direction toward the goal. This paper proposes a novel path planning technique, termed the Bulldozer, which addresses the local minima issue while preserving the inherent advantages of APF. The Bulldozer technique introduces a backfilling mechanism that systematically identifies and eliminates local minima regions by increasing their potential values, analogous to a bulldozer filling potholes in a road. Additionally, a ramp-based enhancement is incorporated to assist the robot in escaping trap areas when starting within a local minimum. The proposed technique is experimentally validated using a physical mobile robot across various maps with increasing complexity. Comparative analyses are conducted against standard APF, adaptive APF, and well-established planning algorithms such as A*, PRM, and RRT. Results demonstrate that the Bulldozer technique effectively resolves the local minima problem while achieving superior execution speed and competitive path quality. Furthermore, a kinematic tracking controller is employed to assess the smoothness and traceability of the planned paths, confirming their suitability for real-world execution.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8def\u5f84\u89c4\u5212\u6280\u672f-Bulldozer\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4eba\u5de5\u52bf\u573a\u65b9\u6cd5\u5728\u5c40\u90e8\u6781\u5c0f\u503c\u95ee\u9898\u4e0a\u7684\u5c40\u9650\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5176\u4f18\u52bf\u3002\u7ecf\u8fc7\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u590d\u6742\u5730\u56fe\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u4eba\u5de5\u52bf\u573a\u65b9\u6cd5\u4e2d\u7684\u5c40\u90e8\u6781\u5c0f\u503c\u9677\u9631\u95ee\u9898\uff0c\u4fdd\u6301\u7b80\u5355\u6027\u548c\u5b9e\u65f6\u54cd\u5e94\u80fd\u529b\u3002", "method": "Bulldozer path planning technique", "result": "Bulldozer\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e86\u5c40\u90e8\u6781\u5c0f\u503c\u95ee\u9898\uff0c\u5177\u6709\u66f4\u5feb\u7684\u6267\u884c\u901f\u5ea6\u548c\u7ade\u4e89\u529b\u7684\u8def\u5f84\u8d28\u91cf\u3002", "conclusion": "Bulldozer\u6280\u672f\u80fd\u591f\u6709\u6548\u63d0\u5347\u8def\u5f84\u89c4\u5212\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u6267\u884c\u7684\u5e73\u6ed1\u6027\u548c\u53ef\u8ffd\u6eaf\u6027\u3002"}}
{"id": "2512.23703", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.23703", "abs": "https://arxiv.org/abs/2512.23703", "authors": ["Huajie Tan", "Sixiang Chen", "Yijie Xu", "Zixiao Wang", "Yuheng Ji", "Cheng Chi", "Yaoxu Lyu", "Zhongxia Zhao", "Xiansheng Chen", "Peterson Co", "Shaoxuan Xie", "Guocai Yao", "Pengwei Wang", "Zhongyuan Wang", "Shanghang Zhang"], "title": "Robo-Dopamine: General Process Reward Modeling for High-Precision Robotic Manipulation", "comment": "27 pages, 11 figures", "summary": "The primary obstacle for applying reinforcement learning (RL) to real-world robotics is the design of effective reward functions. While recently learning-based Process Reward Models (PRMs) are a promising direction, they are often hindered by two fundamental limitations: their reward models lack step-aware understanding and rely on single-view perception, leading to unreliable assessments of fine-grained manipulation progress; and their reward shaping procedures are theoretically unsound, often inducing a semantic trap that misguides policy optimization. To address these, we introduce Dopamine-Reward, a novel reward modeling method for learning a general-purpose, step-aware process reward model from multi-view inputs. At its core is our General Reward Model (GRM), trained on a vast 3,400+ hour dataset, which leverages Step-wise Reward Discretization for structural understanding and Multi-Perspective Reward Fusion to overcome perceptual limitations. Building upon Dopamine-Reward, we propose Dopamine-RL, a robust policy learning framework that employs a theoretically-sound Policy-Invariant Reward Shaping method, which enables the agent to leverage dense rewards for efficient self-improvement without altering the optimal policy, thereby fundamentally avoiding the semantic trap. Extensive experiments across diverse simulated and real-world tasks validate our approach. GRM achieves state-of-the-art accuracy in reward assessment, and Dopamine-RL built on GRM significantly improves policy learning efficiency. For instance, after GRM is adapted to a new task in a one-shot manner from a single expert trajectory, the resulting reward model enables Dopamine-RL to improve the policy from near-zero to 95% success with only 150 online rollouts (approximately 1 hour of real robot interaction), while retaining strong generalization across tasks. Project website: https://robo-dopamine.github.io", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5Dopamine-Reward\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u8f93\u5165\u5b66\u4e60\u4e00\u4e2a\u901a\u7528\u7684\u3001\u5177\u6709\u6b65\u9aa4\u610f\u8bc6\u7684\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u96be\u9898\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5b66\u4e60\u578b\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u5728\u5956\u52b1\u8bc4\u4f30\u548c\u653f\u7b56\u4f18\u5316\u8fc7\u7a0b\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u5b66\u4e60\u6548\u7387\u3002", "method": "\u91c7\u7528\u4e86\u6b65\u9aa4\u5956\u52b1\u79bb\u6563\u548c\u591a\u89c6\u89d2\u5956\u52b1\u878d\u5408\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a\u901a\u7528\u5956\u52b1\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u4e00\u79cd\u7406\u8bba\u4e0a\u53ef\u9760\u7684\u653f\u7b56\u4e0d\u53d8\u5956\u52b1\u5851\u9020\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0cGRM\u5728\u5956\u52b1\u8bc4\u4f30\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\uff0cDopamine-RL\u663e\u8457\u63d0\u9ad8\u4e86\u7b56\u7565\u5b66\u4e60\u6548\u7387\uff0c\u5c55\u793a\u4e86\u4e00\u6b21\u6027\u9002\u5e94\u65b0\u4efb\u52a1\u7684\u6709\u6548\u6027\u3002", "conclusion": "Dopamine-Reward\u548cDopamine-RL\u6846\u67b6\u663e\u8457\u589e\u5f3a\u4e86\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u7684\u6548\u7387\uff0c\u907f\u514d\u4e86\u8bed\u4e49\u9677\u9631\uff0c\u5c55\u793a\u4e86\u5148\u8fdb\u7684\u5956\u52b1\u8bc4\u4f30\u51c6\u786e\u6027\u548c\u5f3a\u5927\u7684\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002"}}
