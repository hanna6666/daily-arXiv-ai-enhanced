{"id": "2510.00154", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00154", "abs": "https://arxiv.org/abs/2510.00154", "authors": ["Xinyi Liu", "Mohammadreza Fani Sani", "Zewei Zhou", "Julius Wirbel", "Bahram Zarrin", "Roberto Galeazzi"], "title": "RoboPilot: Generalizable Dynamic Robotic Manipulation with Dual-thinking Modes", "comment": null, "summary": "Despite rapid progress in autonomous robotics, executing complex or\nlong-horizon tasks remains a fundamental challenge. Most current approaches\nfollow an open-loop paradigm with limited reasoning and no feedback, resulting\nin poor robustness to environmental changes and severe error accumulation. We\npresent RoboPilot, a dual-thinking closed-loop framework for robotic\nmanipulation that supports adaptive reasoning for complex tasks in real-world\ndynamic environments. RoboPilot leverages primitive actions for structured task\nplanning and flexible action generation, while introducing feedback to enable\nreplanning from dynamic changes and execution errors. Chain-of-Thought\nreasoning further enhances high-level task planning and guides low-level action\ngeneration. The system dynamically switches between fast and slow thinking to\nbalance efficiency and accuracy. To systematically evaluate the robustness of\nRoboPilot in diverse robot manipulation scenarios, we introduce\nRoboPilot-Bench, a benchmark spanning 21 tasks across 10 categories, including\ninfeasible-task recognition and failure recovery. Experiments show that\nRoboPilot outperforms state-of-the-art baselines by 25.9\\% in task success\nrate, and the real-world deployment on an industrial robot further demonstrates\nits robustness in real-world settings.", "AI": {"tldr": "RoboPilot\u662f\u4e00\u4e2a\u65b0\u63d0\u51fa\u7684\u95ed\u73af\u6846\u67b6\uff0c\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u5904\u7406\u590d\u6742\u548c\u957f\u65f6\u4efb\u52a1\u65f6\u7684\u5f00\u653e\u5f0f\u5faa\u73af\u548c\u53cd\u9988\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u53cc\u91cd\u601d\u7ef4\u95ed\u73af\u6846\u67b6\uff0c\u7ed3\u5408\u53cd\u9988\u673a\u5236\u8fdb\u884c\u52a8\u6001\u91cd\u89c4\u5212\uff0c\u652f\u6301\u590d\u6742\u4efb\u52a1\u7684\u81ea\u9002\u5e94\u63a8\u7406\u3002", "result": "RoboPilot\u572821\u4e2a\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e8625.9%\u7684\u4efb\u52a1\u6210\u529f\u7387\u63d0\u5347\uff0c\u5e76\u5728\u5de5\u4e1a\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u5b9e\u9645\u5e94\u7528\u9a8c\u8bc1\u3002", "conclusion": "RoboPilot\u5728\u590d\u6742\u4efb\u52a1\u7684\u6267\u884c\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\uff0c\u5e76\u4e14\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u663e\u793a\u51fa\u8f83\u5f3a\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.00182", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00182", "abs": "https://arxiv.org/abs/2510.00182", "authors": ["Jorge Mendez-Mendez"], "title": "A Systematic Study of Large Language Models for Task and Motion Planning With PDDLStream", "comment": null, "summary": "Using large language models (LLMs) to solve complex robotics problems\nrequires understanding their planning capabilities. Yet while we know that LLMs\ncan plan on some problems, the extent to which these planning capabilities\ncover the space of robotics tasks is unclear. One promising direction is to\nintegrate the semantic knowledge of LLMs with the formal reasoning of task and\nmotion planning (TAMP). However, the myriad of choices for how to integrate\nLLMs within TAMP complicates the design of such systems. We develop 16\nalgorithms that use Gemini 2.5 Flash to substitute key TAMP components. Our\nzero-shot experiments across 4,950 problems and three domains reveal that the\nGemini-based planners exhibit lower success rates and higher planning times\nthan their engineered counterparts. We show that providing geometric details\nincreases the number of task-planning errors compared to pure PDDL\ndescriptions, and that (faster) non-reasoning LLM variants outperform (slower)\nreasoning variants in most cases, since the TAMP system can direct the LLM to\ncorrect its mistakes.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u57fa\u4e8eGemini\u7684\u89c4\u5212\u5668\uff0c\u53d1\u73b0\u5176\u5728\u6210\u529f\u7387\u548c\u6548\u7387\u4e0a\u4e0d\u5982\u5de5\u7a0b\u8bbe\u8ba1\u7684\u89c4\u5212\u5668\uff0c\u540c\u65f6\u6307\u51fa\u975e\u63a8\u7406L\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u52bf\u3002", "motivation": "\u7406\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u5728\u590d\u6742\u673a\u5668\u4eba\u95ee\u9898\u4e2d\u7684\u89c4\u5212\u80fd\u529b\uff0c\u4ee5\u53ca\u5982\u4f55\u5c06\u5176\u4e0e\u4efb\u52a1\u548c\u8fd0\u52a8\u89c4\u5212\u7684\u6b63\u5f0f\u63a8\u7406\u76f8\u7ed3\u5408\u3002", "method": "\u5f00\u53d1\u4e8616\u79cd\u7b97\u6cd5\uff0c\u4f7f\u7528Gemini 2.5 Flash\u66ff\u6362\u5173\u952e\u7684\u4efb\u52a1\u548c\u8fd0\u52a8\u89c4\u5212\u7ec4\u4ef6\uff0c\u5e76\u8fdb\u884c\u4e86\u96f6-shot\u5b9e\u9a8c\u3002", "result": "\u57284950\u4e2a\u95ee\u9898\u548c\u4e09\u4e2a\u9886\u57df\u7684\u5b9e\u9a8c\u4e2d\uff0cGemini-based\u89c4\u5212\u5668\u7684\u6210\u529f\u7387\u8f83\u4f4e\uff0c\u89c4\u5212\u65f6\u95f4\u8f83\u957f\uff0c\u5e76\u4e14\u63d0\u4f9b\u51e0\u4f55\u7ec6\u8282\u4f1a\u589e\u52a0\u4efb\u52a1\u89c4\u5212\u9519\u8bef\u3002", "conclusion": "Gemini-based\u89c4\u5212\u5668\u5728\u6210\u529f\u7387\u548c\u89c4\u5212\u65f6\u95f4\u4e0a\u5747\u900a\u8272\u4e8e\u5de5\u7a0b\u8bbe\u8ba1\u7684\u5bf9\u624b\uff0c\u975e\u63a8\u7406LLM\u53d8\u79cd\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u597d\u3002"}}
{"id": "2510.00188", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.00188", "abs": "https://arxiv.org/abs/2510.00188", "authors": ["Alireza Aliyari", "Gholamreza Vossoughi"], "title": "A Novel Robust Control Method Combining DNN-Based NMPC Approximation and PI Control: Application to Exoskeleton Squat Movements", "comment": null, "summary": "Nonlinear Model Predictive Control (NMPC) is a precise controller, but its\nheavy computational load often prevents application in robotic systems. Some\nstudies have attempted to approximate NMPC using deep neural networks\n(NMPC-DNN). However, in the presence of unexpected disturbances or when\noperating conditions differ from training data, this approach lacks robustness,\nleading to large tracking errors. To address this issue, for the first time,\nthe NMPC-DNN output is combined with a PI controller (Hybrid NMPC-DNN-PI). The\nproposed controller is validated by applying it to an exoskeleton robot during\nsquat movement, which has a complex dynamic model and has received limited\nattention regarding robust nonlinear control design. A human-robot dynamic\nmodel with three active joints (ankle, knee, hip) is developed, and more than\n5.3 million training samples are used to train the DNN. The results show that,\nunder unseen conditions for the DNN, the tracking error in Hybrid NMPC-DNN-PI\nis significantly lower compared to NMPC-DNN. Moreover, human joint torques are\ngreatly reduced with the use of the exoskeleton, with RMS values for the\nstudied case reduced by 30.9%, 41.8%, and 29.7% at the ankle, knee, and hip,\nrespectively. In addition, the computational cost of Hybrid NMPC-DNN-PI is\n99.93% lower than that of NMPC.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u6df7\u5408NMPC-DNN-PI\u63a7\u5236\u5668\uff0c\u4ee5\u63d0\u9ad8\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u9c81\u68d2\u6027\uff0c\u7ed3\u679c\u8868\u660e\u5176\u5728\u5916\u9aa8\u9abc\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ddf\u8e2a\u8bef\u5dee\u548c\u5173\u8282\u626d\u77e9\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u63d0\u9ad8NMPC\u5728\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u6548\u679c\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u610f\u5916\u5e72\u6270\u548c\u8bad\u7ec3\u6570\u636e\u4e0e\u5b9e\u9645\u60c5\u51b5\u4e0d\u7b26\u65f6\u7684\u9c81\u68d2\u6027\u3002", "method": "\u5c06NMPC-DNN\u8f93\u51fa\u4e0ePI\u63a7\u5236\u5668\u7ed3\u5408\uff0c\u5f62\u6210\u6df7\u5408\u63a7\u5236\u5668", "result": "\u6df7\u5408NMPC-DNN-PI\u5728\u590d\u6742\u52a8\u6001\u6a21\u578b\u7684\u5916\u9aa8\u9abc\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0c\u8ddf\u8e2a\u8bef\u5dee\u964d\u4f4e\uff0c\u5173\u8282\u626d\u77e9\u51cf\u5c11\uff0c\u8ba1\u7b97\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u6df7\u5408NMPC-DNN-PI\u63a7\u5236\u5668\u5728\u672a\u77e5\u6761\u4ef6\u4e0b\u7684\u8ddf\u8e2a\u8bef\u5dee\u663e\u8457\u4f4e\u4e8eNMPC-DNN\uff0c\u4e14\u964d\u4f4e\u4e86\u4eba\u7c7b\u5173\u8282\u626d\u77e9\uff0c\u540c\u65f6\u8ba1\u7b97\u6210\u672c\u5927\u5e45\u51cf\u5c11\u3002"}}
{"id": "2510.00225", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.00225", "abs": "https://arxiv.org/abs/2510.00225", "authors": ["Yue Meng", "Fei Chen", "Chuchu Fan"], "title": "TGPO: Temporal Grounded Policy Optimization for Signal Temporal Logic Tasks", "comment": null, "summary": "Learning control policies for complex, long-horizon tasks is a central\nchallenge in robotics and autonomous systems. Signal Temporal Logic (STL)\noffers a powerful and expressive language for specifying such tasks, but its\nnon-Markovian nature and inherent sparse reward make it difficult to be solved\nvia standard Reinforcement Learning (RL) algorithms. Prior RL approaches focus\nonly on limited STL fragments or use STL robustness scores as sparse terminal\nrewards. In this paper, we propose TGPO, Temporal Grounded Policy Optimization,\nto solve general STL tasks. TGPO decomposes STL into timed subgoals and\ninvariant constraints and provides a hierarchical framework to tackle the\nproblem. The high-level component of TGPO proposes concrete time allocations\nfor these subgoals, and the low-level time-conditioned policy learns to achieve\nthe sequenced subgoals using a dense, stage-wise reward signal. During\ninference, we sample various time allocations and select the most promising\nassignment for the policy network to rollout the solution trajectory. To foster\nefficient policy learning for complex STL with multiple subgoals, we leverage\nthe learned critic to guide the high-level temporal search via\nMetropolis-Hastings sampling, focusing exploration on temporally feasible\nsolutions. We conduct experiments on five environments, ranging from\nlow-dimensional navigation to manipulation, drone, and quadrupedal locomotion.\nUnder a wide range of STL tasks, TGPO significantly outperforms\nstate-of-the-art baselines (especially for high-dimensional and long-horizon\ncases), with an average of 31.6% improvement in task success rate compared to\nthe best baseline. The code will be available at\nhttps://github.com/mengyuest/TGPO", "AI": {"tldr": "TGPO\u662f\u4e3a\u4e86\u89e3\u51b3\u590d\u6742STL\u4efb\u52a1\u800c\u63d0\u51fa\u7684\u65b0\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u6539\u5584\u7b56\u7565\u5b66\u4e60\u5e76\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u4e3a\u4e86\u514b\u670dSTL\u7684\u975e\u9a6c\u5c14\u53ef\u592b\u6027\u548c\u7a00\u758f\u5956\u52b1\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u4e2a\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u4e00\u822cSTL\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86TGPO\uff0c\u5373\u65f6\u95f4\u5f15\u5bfc\u7b56\u7565\u4f18\u5316\uff0c\u4f7f\u7528\u5206\u5c42\u6846\u67b6\u5206\u89e3STL\u4e3a\u65f6\u5e8f\u5b50\u76ee\u6807\u548c\u4e0d\u53d8\u7ea6\u675f\uff0c\u901a\u8fc7\u9ad8\u5c42\u548c\u4f4e\u5c42\u7ec4\u4ef6\u6765\u5904\u7406\u95ee\u9898\uff0c\u5e76\u5229\u7528Metropolis-Hastings\u91c7\u6837\u8fdb\u884c\u9ad8\u6548\u7684\u7b56\u7565\u5b66\u4e60\u3002", "result": "TGPO\u5728\u4e94\u4e2a\u4e0d\u540c\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u4ece\u4f4e\u7ef4\u5bfc\u822a\u5230\u64cd\u4f5c\u3001\u65e0\u4eba\u673a\u548c\u56db\u8db3\u884c\u8d70\uff0c\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "TGPO\u5728\u5404\u79cdSTL\u4efb\u52a1\u4e2d\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7b97\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u7ef4\u548c\u957f\u65f6\u95f4\u8303\u56f4\u7684\u4efb\u52a1\u4e2d\uff0c\u4efb\u52a1\u6210\u529f\u7387\u5e73\u5747\u63d0\u9ad8\u4e8631.6%\u3002"}}
{"id": "2510.00120", "categories": ["cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.00120", "abs": "https://arxiv.org/abs/2510.00120", "authors": ["Xiang Chang", "Zhijie Yi", "Yichang Liu", "Hongling Sheng", "Dengbo He"], "title": "The Formation of Trust in Autonomous Vehicles after Interacting with Robotaxis on Public Roads", "comment": "Proceedings of the 69th HFES International Annual Meeting", "summary": "This study investigates how pedestrian trust, receptivity, and behavior\nevolve during interactions with Level-4 autonomous vehicles (AVs) at\nuncontrolled urban intersections in a naturalistic setting. While public\nacceptance is critical for AV adoption, most prior studies relied on simplified\nsimulations or field tests. We conducted a real-world experiment in a\ncommercial Robotaxi operation zone, where 33 participants repeatedly crossed an\nuncontrolled intersection with frequent Level-4 Robotaxi traffic. Participants\ncompleted the Pedestrian Behavior Questionnaire (PBQ), Pedestrian Receptivity\nQuestionnaire for Fully AVs (PRQF), pre- and post-experiment Trust in AVs\nScale, and Personal Innovativeness Scale (PIS). Results showed that trust in\nAVs significantly increased post-experiment, with the increase positively\nassociated with the Interaction component of PRQF. Additionally, both the\nPositive and Error subscales of the PBQ significantly influenced trust change.\nThis study reveals how trust forms in real-world pedestrian-AV encounters,\noffering insights beyond lab-based research by accounting for population\nheterogeneity.", "AI": {"tldr": "\u672c\u7814\u7a76\u57fa\u4e8e\u771f\u5b9e\u573a\u666f\u8003\u5bdf\u4e86\u884c\u4eba\u4e0e\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u4e4b\u95f4\u7684\u4fe1\u4efb\u3001\u63a5\u53d7\u5ea6\u548c\u884c\u4e3a\uff0c\u53d1\u73b0\u4fe1\u4efb\u53ef\u4ee5\u901a\u8fc7\u771f\u5b9e\u4e92\u52a8\u663e\u8457\u63d0\u9ad8\uff0c\u4e14\u53d7\u591a\u4e2a\u53d8\u91cf\u5f71\u54cd\u3002", "motivation": "\u516c\u4f17\u5bf9\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u7684\u63a5\u53d7\u5ea6\u5bf9\u5176\u63a8\u5e7f\u81f3\u5173\u91cd\u8981\uff0c\u800c\u4ee5\u5f80\u7684\u7814\u7a76\u591a\u57fa\u4e8e\u7b80\u5316\u6a21\u62df\u6216\u5b9e\u9a8c\u5ba4\u6d4b\u8bd5\uff0c\u7f3a\u4e4f\u5bf9\u771f\u5b9e\u573a\u666f\u7684\u63a2\u7d22\u3002", "method": "\u672c\u7814\u7a76\u5728\u5546\u4e1aRobotaxi\u8fd0\u8425\u533a\u57df\u8fdb\u884c\u771f\u5b9e\u5b9e\u9a8c\uff0c\u53c2\u4e0e\u8005\u5728\u65e0\u63a7\u5236\u7684\u57ce\u5e02\u4ea4\u53c9\u53e3\u591a\u6b21\u7a7f\u8d8a\uff0c\u5229\u7528\u591a\u79cd\u95ee\u5377\u8bc4\u4f30\u53c2\u4e0e\u8005\u7684\u884c\u4e3a\u3001\u63a5\u53d7\u5ea6\u548c\u4fe1\u4efb\u611f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u53c2\u4e0e\u8005\u5bf9\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u7684\u4fe1\u4efb\u611f\u5728\u5b9e\u9a8c\u540e\u663e\u8457\u589e\u52a0\uff0c\u5e76\u4e14\u8fd9\u79cd\u589e\u52a0\u4e0e\u53c2\u4e0e\u8005\u7684\u63a5\u53d7\u5ea6\u6709\u6b63\u76f8\u5173\u5173\u7cfb\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u884c\u4eba\u4e0e\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\uff08AV\uff09\u4e92\u52a8\u4e2d\u7684\u4fe1\u4efb\u662f\u53ef\u4ee5\u901a\u8fc7\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u591a\u6b21\u4ea4\u4e92\u663e\u8457\u63d0\u5347\u7684\uff0c\u540c\u65f6\u8fd9\u4e00\u4fe1\u4efb\u7684\u5f62\u6210\u53d7\u591a\u4e2a\u56e0\u7d20\u7684\u5f71\u54cd\u3002"}}
{"id": "2510.00272", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.00272", "abs": "https://arxiv.org/abs/2510.00272", "authors": ["Odichimnma Ezeji", "Michael Ziegltrum", "Giulio Turrisi", "Tommaso Belvedere", "Valerio Modugno"], "title": "BC-MPPI: A Probabilistic Constraint Layer for Safe Model-Predictive Path-Integral Control", "comment": null, "summary": "Model Predictive Path Integral (MPPI) control has recently emerged as a fast,\ngradient-free alternative to model-predictive control in highly non-linear\nrobotic tasks, yet it offers no hard guarantees on constraint satisfaction. We\nintroduce Bayesian-Constraints MPPI (BC-MPPI), a lightweight safety layer that\nattaches a probabilistic surrogate to every state and input constraint. At each\nre-planning step the surrogate returns the probability that a candidate\ntrajectory is feasible; this joint probability scales the weight given to a\ncandidate, automatically down-weighting rollouts likely to collide or exceed\nlimits and pushing the sampling distribution toward the safe subset; no\nhand-tuned penalty costs or explicit sample rejection required. We train the\nsurrogate from 1000 offline simulations and deploy the controller on a\nquadrotor in MuJoCo with both static and moving obstacles. Across K in\n[100,1500] rollouts BC-MPPI preserves safety margins while satisfying the\nprescribed probability of violation. Because the surrogate is a stand-alone,\nversion-controlled artefact and the runtime safety score is a single scalar,\nthe approach integrates naturally with verification-and-validation pipelines\nfor certifiable autonomous systems.", "AI": {"tldr": "\u63d0\u51faBC-MPPI\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u7ea6\u675f\u63d0\u9ad8MPPI\u7684\u5b89\u5168\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7684\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\uff0c\u80fd\u6709\u6548\u907f\u514d\u78b0\u649e\u3002", "motivation": "MPPI\u63a7\u5236\u65b9\u6cd5\u5728\u975e\u7ebf\u6027\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5feb\u901f\u4e14\u65e0\u68af\u5ea6\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u7ea6\u675f\u6ee1\u8db3\u7684\u4fdd\u8bc1\uff0c\u56e0\u6b64\u9700\u8981\u5f15\u5165\u4e00\u79cd\u5b89\u5168\u673a\u5236\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u7ea6\u675f\u4e3aMPPI\u63a7\u5236\u63d0\u4f9b\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u5b89\u5168\u5c42\uff0c\u901a\u8fc7\u6bcf\u4e2a\u72b6\u6001\u548c\u8f93\u5165\u7ea6\u675f\u9644\u52a0\u6982\u7387\u4ee3\u7406\uff0c\u8c03\u6574\u5019\u9009\u8f68\u8ff9\u7684\u6743\u91cd\u4ee5\u964d\u4f4e\u78b0\u649e\u98ce\u9669\u3002", "result": "\u5728MuJoCo\u7684\u56db\u65cb\u7ffc\u5b9e\u9a8c\u4e2d\uff0cBC-MPPI\u80fd\u591f\u5728\u4e0d\u9700\u8981\u624b\u52a8\u8c03\u8282\u60e9\u7f5a\u6210\u672c\u6216\u660e\u786e\u6837\u672c\u62d2\u7edd\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u5bf9\u5019\u9009\u8f68\u8ff9\u7684\u6982\u7387\u8bc4\u4f30\uff0c\u63a8\u52a8\u91c7\u6837\u5206\u5e03\u5411\u5b89\u5168\u5b50\u96c6\u503e\u659c\u3002", "conclusion": "BC-MPPI\u65b9\u6cd5\u5728\u4fdd\u8bc1\u5b89\u5168\u8fb9\u9645\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u540c\u65f6\u6ee1\u8db3\u89c4\u5b9a\u7684\u8fdd\u89c4\u6982\u7387\uff0c\u80fd\u591f\u4e0e\u9a8c\u8bc1\u548c\u786e\u8ba4\u6d41\u7a0b\u81ea\u7136\u7ed3\u5408\uff0c\u9002\u7528\u4e8e\u53ef\u8ba4\u8bc1\u7684\u81ea\u52a8\u7cfb\u7edf\u3002"}}
{"id": "2510.00191", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.00191", "abs": "https://arxiv.org/abs/2510.00191", "authors": ["Satoshi Hashiguchi", "Yuta Kataoka", "Asako Kimura", "Shohei Mori"], "title": "Perceived Weight of Mediated Reality Sticks", "comment": "This work is a pre-print version of a paper that has been accepted to\n  IEEE TVCG. DOI: 10.1109/TVCG.2025.3591181. Project Page:\n  https://mediated-reality.github.io/projects/hashiguchi_tvcg25/", "summary": "Mediated reality, where augmented reality (AR) and diminished reality (DR)\nmeet, enables visual modifications to real-world objects. A physical object\nwith a mediated reality visual change retains its original physical properties.\nHowever, it is perceived differently from the original when interacted with. We\npresent such a mediated reality object, a stick with different lengths or a\nstick with a missing portion in the middle, to investigate how users perceive\nits weight and center of gravity. We conducted two user studies (N=10), each of\nwhich consisted of two substudies. We found that the length of mediated reality\nsticks influences the perceived weight. A longer stick is perceived as lighter,\nand vice versa. The stick with a missing portion tends to be recognized as one\ncontinuous stick. Thus, its weight and center of gravity (COG) remain the same.\nWe formulated the relationship between inertia based on the reported COG and\nperceived weight in the context of dynamic touch.", "AI": {"tldr": "\u4e2d\u4ecb\u73b0\u5b9e\u68d2\u7684\u957f\u5ea6\u548c\u7f3a\u5931\u5bf9\u7528\u6237\u7684\u91cd\u91cf\u548c\u91cd\u5fc3\u611f\u77e5\u6709\u663e\u8457\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u89e6\u6478\u60c5\u5883\u4e0b\u3002", "motivation": "\u63a2\u8ba8\u4e2d\u4ecb\u73b0\u5b9e\uff08AR\u548cDR\u7ed3\u5408\uff09\u5bf9\u7269\u7406\u7269\u4f53\u89c6\u89c9\u4fee\u6539\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u7528\u6237\u5bf9\u7269\u4f53\u91cd\u91cf\u548c\u91cd\u5fc3\u7684\u611f\u77e5\u3002", "method": "\u8fdb\u884c\u4e86\u4e24\u4e2a\u7528\u6237\u7814\u7a76\uff08N=10\uff09\uff0c\u6bcf\u4e2a\u7814\u7a76\u5305\u542b\u4e24\u4e2a\u5b50\u7814\u7a76\uff0c\u901a\u8fc7\u7528\u6237\u4ea4\u4e92\u8bc4\u4f30\u7528\u6237\u5bf9\u4e2d\u4ecb\u73b0\u5b9e\u68d2\u7684\u611f\u77e5\u3002", "result": "\u53d1\u73b0\u4e2d\u4ecb\u73b0\u5b9e\u68d2\u7684\u957f\u5ea6\u5f71\u54cd\u7528\u6237\u5bf9\u91cd\u91cf\u7684\u611f\u77e5\uff0c\u800c\u7f3a\u5931\u90e8\u5206\u7684\u68d2\u88ab\u8ba4\u4e3a\u662f\u8fde\u7eed\u7684\uff0c\u5176\u91cd\u91cf\u548c\u91cd\u5fc3\u4fdd\u6301\u4e0d\u53d8\u3002", "conclusion": "\u4e2d\u4ecb\u73b0\u5b9e\u7269\u4f53\u7684\u7279\u6027\u5f71\u54cd\u7528\u6237\u5bf9\u5176\u91cd\u91cf\u548c\u91cd\u5fc3\u7684\u611f\u77e5\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u89e6\u6478\u7684\u60c5\u5883\u4e0b\u3002"}}
{"id": "2510.00329", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.00329", "abs": "https://arxiv.org/abs/2510.00329", "authors": ["Sarmad Mehrdad", "Maxime Sabbah", "Vincent Bonnet", "Ludovic Righetti"], "title": "Learning Human Reaching Optimality Principles from Minimal Observation Inverse Reinforcement Learning", "comment": "8 pages, 4 figures", "summary": "This paper investigates the application of Minimal Observation Inverse\nReinforcement Learning (MO-IRL) to model and predict human arm-reaching\nmovements with time-varying cost weights. Using a planar two-link biomechanical\nmodel and high-resolution motion-capture data from subjects performing a\npointing task, we segment each trajectory into multiple phases and learn\nphase-specific combinations of seven candidate cost functions. MO-IRL\niteratively refines cost weights by scaling observed and generated trajectories\nin the maximum entropy IRL formulation, greatly reducing the number of required\ndemonstrations and convergence time compared to classical IRL approaches.\nTraining on ten trials per posture yields average joint-angle Root Mean Squared\nErrors (RMSE) of 6.4 deg and 5.6 deg for six- and eight-segment weight\ndivisions, respectively, versus 10.4 deg using a single static weight.\nCross-validation on remaining trials and, for the first time, inter-subject\nvalidation on an unseen subject's 20 trials, demonstrates comparable predictive\naccuracy, around 8 deg RMSE, indicating robust generalization. Learned weights\nemphasize joint acceleration minimization during movement onset and\ntermination, aligning with smoothness principles observed in biological motion.\nThese results suggest that MO-IRL can efficiently uncover dynamic,\nsubject-independent cost structures underlying human motor control, with\npotential applications for humanoid robots.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u5c55\u793a\u4e86\u6700\u5c0f\u89c2\u5bdf\u9006\u5f3a\u5316\u5b66\u4e60\u5728\u5efa\u6a21\u548c\u9884\u6d4b\u4eba\u7c7b\u624b\u81c2\u8fd0\u52a8\u4e2d\u7684\u5e94\u7528\uff0c\u5176\u6210\u672c\u7ed3\u6784\u4e0d\u4ec5\u52a8\u6001\u4e14\u4e0d\u4f9d\u8d56\u4e8e\u7279\u5b9a\u4e3b\u4f53\uff0c\u5177\u6709\u4fc3\u8fdb\u4eba\u5f62\u673a\u5668\u4eba\u53d1\u5c55\u7684\u6f5c\u529b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edf\u9006\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6240\u9700\u6f14\u793a\u6b21\u6570\u591a\u3001\u6536\u655b\u65f6\u95f4\u957f\u7684\u95ee\u9898\uff0c\u7814\u7a76\u6700\u5c0f\u89c2\u5bdf\u9006\u5f3a\u5316\u5b66\u4e60(MO-IRL)\u5728\u63cf\u8ff0\u548c\u9884\u6d4b\u4eba\u7c7b\u624b\u81c2\u8fd0\u52a8\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u5e73\u9762\u53cc\u94fe\u751f\u7269\u529b\u5b66\u6a21\u578b\u548c\u9ad8\u5206\u8fa8\u7387\u8fd0\u52a8\u6355\u6349\u6570\u636e\uff0c\u901a\u8fc7\u5206\u6bb5\u6bcf\u4e2a\u8f68\u8ff9\u5b66\u4e60\u7279\u5b9a\u9636\u6bb5\u7684\u6210\u672c\u51fd\u6570\u7ec4\u5408\uff0c\u5229\u7528MO-IRL\u7b97\u6cd5\u8fed\u4ee3\u4f18\u5316\u6210\u672c\u6743\u91cd\u3002", "result": "\u5728\u5341\u4e2a\u8bd5\u9a8c\u4e2d\uff0c\u9488\u5bf9\u4e0d\u540c\u6743\u91cd\u5212\u5206\u7684\u5173\u8282\u89d2\u5747\u65b9\u6839\u8bef\u5dee(RMSE)\u5206\u522b\u4e3a6.4\u5ea6\u548c5.6\u5ea6\uff0c\u800c\u4f7f\u7528\u5355\u4e00\u9759\u6001\u6743\u91cd\u65f6\u4e3a10.4\u5ea6\u3002\u4ea4\u53c9\u9a8c\u8bc1\u548c\u9996\u6b21\u7684\u8de8\u4e3b\u4f53\u9a8c\u8bc1\u8868\u660e\uff0c\u9884\u6d4b\u51c6\u786e\u5ea6\u7ea6\u4e3a8\u5ea6RMSE\uff0c\u5c55\u793a\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MO-IRL\u80fd\u591f\u9ad8\u6548\u63ed\u793a\u4eba\u7c7b\u8fd0\u52a8\u63a7\u5236\u7684\u52a8\u6001\u3001\u4e3b\u4f53\u65e0\u5173\u6210\u672c\u7ed3\u6784\uff0c\u5177\u6709\u5bf9\u4eba\u5f62\u673a\u5668\u4eba\u6f5c\u5728\u5e94\u7528\u7684\u4ef7\u503c\u3002"}}
{"id": "2510.00222", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.00222", "abs": "https://arxiv.org/abs/2510.00222", "authors": ["Ke Er Amy Zhang", "David Grellscheid", "Laura Garrison"], "title": "Data Melodification FM: Where Musical Rhetoric Meets Sonification", "comment": "5 pages, 5 figures, accepted to alt.VIS 2025", "summary": "We propose a design space for data melodification, where standard\nvisualization idioms and fundamental data characteristics map to rhetorical\ndevices of music for a more affective experience of data. Traditional data\nsonification transforms data into sound by mapping it to different parameters\nsuch as pitch, volume, and duration. Often and regrettably, this mapping leaves\nbehind melody, harmony, rhythm and other musical devices that compose the\ncenturies-long persuasive and expressive power of music. What results is the\noccasional, unintentional sense of tinnitus and horror film-like impending doom\ncaused by a disconnect between the semantics of data and sound. Through this\nwork we ask, can the aestheticization of sonification through (classical) music\ntheory make data simultaneously accessible, meaningful, and pleasing to one's\nears?", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u97f3\u4e50\u7406\u8bba\u63d0\u5347\u6570\u636e\u58f0\u5316\u7684\u7f8e\u5b66\uff0c\u4f7f\u6570\u636e\u66f4\u6613\u63a5\u8fd1\u4e14\u6109\u60a6\u3002", "motivation": "\u5e0c\u671b\u901a\u8fc7\u58f0\u5316\u5c06\u6570\u636e\u7684\u8868\u8fbe\u548c\u611f\u77e5\u63d0\u5347\uff0c\u907f\u514d\u4f20\u7edf\u58f0\u5316\u4e2d\u5e38\u89c1\u7684\u4e0e\u8bed\u4e49\u4e0d\u7b26\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u6570\u636e\u58f0\u5316\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u5c06\u53ef\u89c6\u5316\u60ef\u4f8b\u548c\u6570\u636e\u7279\u5f81\u6620\u5c04\u5230\u97f3\u4e50\u7684\u4fee\u8f9e\u624b\u6cd5\u3002", "result": "\u5b9e\u73b0\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u58f0\u5316\u65b9\u5f0f\uff0c\u4f7f\u6570\u636e\u7684\u58f0\u5316\u4f53\u9a8c\u66f4\u52a0\u4e30\u5bcc\u548c\u5438\u5f15\u4eba\u3002", "conclusion": "\u901a\u8fc7\u8fd0\u7528\u7ecf\u5178\u97f3\u4e50\u7406\u8bba\u7684\u7f8e\u5b66\uff0c\u4f7f\u6570\u636e\u7684\u58f0\u5316\u65e2\u53ef\u7406\u89e3\u53c8\u6109\u60a6\u3002"}}
{"id": "2510.00358", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00358", "abs": "https://arxiv.org/abs/2510.00358", "authors": ["Linjin He", "Xinda Qi", "Dong Chen", "Zhaojian Li", "Xiaobo Tan"], "title": "DiSA-IQL: Offline Reinforcement Learning for Robust Soft Robot Control under Distribution Shifts", "comment": null, "summary": "Soft snake robots offer remarkable flexibility and adaptability in complex\nenvironments, yet their control remains challenging due to highly nonlinear\ndynamics. Existing model-based and bio-inspired controllers rely on simplified\nassumptions that limit performance. Deep reinforcement learning (DRL) has\nrecently emerged as a promising alternative, but online training is often\nimpractical because of costly and potentially damaging real-world interactions.\nOffline RL provides a safer option by leveraging pre-collected datasets, but it\nsuffers from distribution shift, which degrades generalization to unseen\nscenarios. To overcome this challenge, we propose DiSA-IQL\n(Distribution-Shift-Aware Implicit Q-Learning), an extension of IQL that\nincorporates robustness modulation by penalizing unreliable state-action pairs\nto mitigate distribution shift. We evaluate DiSA-IQL on goal-reaching tasks\nacross two settings: in-distribution and out-of-distribution evaluation.\nSimulation results show that DiSA-IQL consistently outperforms baseline models,\nincluding Behavior Cloning (BC), Conservative Q-Learning (CQL), and vanilla\nIQL, achieving higher success rates, smoother trajectories, and improved\nrobustness. The codes are open-sourced to support reproducibility and to\nfacilitate further research in offline RL for soft robot control.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faDiSA-IQL\u7b97\u6cd5\uff0c\u901a\u8fc7\u60e9\u7f5a\u4e0d\u53ef\u9760\u7684\u72b6\u6001-\u52a8\u4f5c\u5bf9\u6539\u5584\u8f6f\u86c7\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u6210\u529f\u964d\u4f4e\u5206\u5e03\u504f\u79fb\uff0c\u63d0\u5347\u4e86\u4efb\u52a1\u6210\u529f\u7387\u548c\u8f68\u8ff9\u5e73\u6ed1\u5ea6\u3002", "motivation": "\u7531\u4e8e\u73b0\u6709\u63a7\u5236\u65b9\u6cd5\u5728\u5904\u7406\u975e\u7ebf\u6027\u52a8\u6001\u65f6\u7684\u6027\u80fd\u9650\u5236\uff0c\u4e14\u5728\u7ebf\u8bad\u7ec3\u5f80\u5f80\u4e0d\u5207\u5b9e\u9645\uff0c\u56e0\u6b64\u7814\u7a76\u8005\u5bfb\u627e\u66f4\u5b89\u5168\u6709\u6548\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDiSA-IQL\u7684\u7b97\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u60e9\u7f5a\u4e0d\u53ef\u9760\u7684\u72b6\u6001-\u52a8\u4f5c\u5bf9\u6765\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u65e8\u5728\u51cf\u8f7b\u5206\u5e03\u504f\u79fb\u95ee\u9898\u3002", "result": "\u5728\u4e24\u79cd\u73af\u5883\u4e0b\u7684\u76ee\u6807\u5230\u8fbe\u4efb\u52a1\u4e2d\uff0cDiSA-IQL\u8868\u73b0\u4f18\u4e8e\u884c\u4e3a\u514b\u9686\u3001\u4fdd\u5b88Q\u5b66\u4e60\u53ca\u4f20\u7edfIQL\u6a21\u578b\u3002", "conclusion": "DiSA-IQL\u5728\u89e3\u51b3\u8f6f\u86c7\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u6210\u529f\u7387\u3001\u5e73\u6ed1\u7684\u8f68\u8ff9\u548c\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.00245", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00245", "abs": "https://arxiv.org/abs/2510.00245", "authors": ["Rizul Sharma", "Tianyu Jiang", "Seokki Lee", "Jillian Aurisano"], "title": "Can AI agents understand spoken conversations about data visualizations in online meetings?", "comment": null, "summary": "In this short paper, we present work evaluating an AI agent's understanding\nof spoken conversations about data visualizations in an online meeting\nscenario. There is growing interest in the development of AI-assistants that\nsupport meetings, such as by providing assistance with tasks or summarizing a\ndiscussion. The quality of this support depends on a model that understands the\nconversational dialogue. To evaluate this understanding, we introduce a\ndual-axis testing framework for diagnosing the AI agent's comprehension of\nspoken conversations about data. Using this framework, we designed a series of\ntests to evaluate understanding of a novel corpus of 72 spoken conversational\ndialogues about data visualizations. We examine diverse pipelines and model\narchitectures, LLM vs VLM, and diverse input formats for visualizations (the\nchart image, its underlying source code, or a hybrid of both) to see how this\naffects model performance on our tests. Using our evaluation methods, we found\nthat text-only input modalities achieved the best performance (96%) in\nunderstanding discussions of visualizations in online meetings.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u4e00\u79cdAI\u4ee3\u7406\u5728\u5728\u7ebf\u4f1a\u8bae\u4e2d\u7406\u89e3\u5173\u4e8e\u6570\u636e\u53ef\u89c6\u5316\u7684\u5bf9\u8bdd\u80fd\u529b\uff0c\u91c7\u7528\u53cc\u8f74\u6d4b\u8bd5\u6846\u67b6\uff0c\u6d4b\u8bd5\u53d1\u73b0\u6587\u672c\u8f93\u5165\u6a21\u5f0f\u7684\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u968f\u7740\u5bf9\u80fd\u591f\u652f\u6301\u4f1a\u8bae\u7684AI\u52a9\u624b\u7684\u5174\u8da3\u65e5\u76ca\u589e\u957f\uff0c\u5c24\u5176\u662f\u5728\u63d0\u4f9b\u4efb\u52a1\u534f\u52a9\u6216\u603b\u7ed3\u8ba8\u8bba\u65b9\u9762\uff0c\u7406\u89e3\u4f1a\u8bdd\u5bf9\u8bdd\u7684\u6a21\u578b\u8d28\u91cf\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165\u53cc\u8f74\u6d4b\u8bd5\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e86\u4e00\u7cfb\u5217\u6d4b\u8bd5\u6765\u8bc4\u4f30AI\u4ee3\u7406\u5bf972\u6bb5\u5173\u4e8e\u6570\u636e\u53ef\u89c6\u5316\u7684\u5bf9\u8bdd\u7684\u7406\u89e3\u3002", "result": "\u6211\u4eec\u7814\u7a76\u4e86\u591a\u79cd\u7ba1\u9053\u548c\u6a21\u578b\u67b6\u6784\uff08LLM\u4e0eVLM\uff09\uff0c\u4ee5\u53ca\u4e0d\u540c\u7684\u53ef\u89c6\u5316\u8f93\u5165\u683c\u5f0f\uff0c\u53d1\u73b0\u6587\u672c\u8f93\u5165\u6a21\u5f0f\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u901a\u8fc7\u6211\u4eec\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u53d1\u73b0\u4ec5\u6587\u672c\u8f93\u5165\u6a21\u5f0f\u5728\u7406\u89e3\u5728\u7ebf\u4f1a\u8bae\u4e2d\u5bf9\u53ef\u89c6\u5316\u7684\u8ba8\u8bba\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff08\u8fbe\u523096%\u7684\u51c6\u786e\u7387\uff09\u3002"}}
{"id": "2510.00401", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.00401", "abs": "https://arxiv.org/abs/2510.00401", "authors": ["Shounak Sural", "Charles Kekeh", "Wenliang Liu", "Federico Pecora", "Mouhacine Benosman"], "title": "Physics-Informed Neural Controlled Differential Equations for Scalable Long Horizon Multi-Agent Motion Forecasting", "comment": null, "summary": "Long-horizon motion forecasting for multiple autonomous robots is challenging\ndue to non-linear agent interactions, compounding prediction errors, and\ncontinuous-time evolution of dynamics. Learned dynamics of such a system can be\nuseful in various applications such as travel time prediction,\nprediction-guided planning and generative simulation. In this work, we aim to\ndevelop an efficient trajectory forecasting model conditioned on multi-agent\ngoals. Motivated by the recent success of physics-guided deep learning for\npartially known dynamical systems, we develop a model based on neural\nControlled Differential Equations (CDEs) for long-horizon motion forecasting.\nUnlike discrete-time methods such as RNNs and transformers, neural CDEs operate\nin continuous time, allowing us to combine physics-informed constraints and\nbiases to jointly model multi-robot dynamics. Our approach, named PINCoDE\n(Physics-Informed Neural Controlled Differential Equations), learns\ndifferential equation parameters that can be used to predict the trajectories\nof a multi-agent system starting from an initial condition. PINCoDE is\nconditioned on future goals and enforces physics constraints for robot motion\nover extended periods of time. We adopt a strategy that scales our model from\n10 robots to 100 robots without the need for additional model parameters, while\nproducing predictions with an average ADE below 0.5 m for a 1-minute horizon.\nFurthermore, progressive training with curriculum learning for our PINCoDE\nmodel results in a 2.7X reduction of forecasted pose error over 4 minute\nhorizons compared to analytical models.", "AI": {"tldr": "PINCoDE\u662f\u4e00\u4e2a\u57fa\u4e8e\u795e\u7ecf\u63a7\u5236\u5fae\u5206\u65b9\u7a0b\u7684\u8fd0\u52a8\u9884\u6d4b\u6a21\u578b\uff0c\u80fd\u5728\u591a\u76ee\u6807\u6761\u4ef6\u4e0b\u9ad8\u6548\u9884\u6d4b\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u8fd0\u52a8\uff0c\u4f53\u73b0\u4e86\u7269\u7406\u7ea6\u675f\u7684\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u957f\u671f\u8fd0\u52a8\u9884\u6d4b\u4e2d\u7684\u975e\u7ebf\u6027\u4ea4\u4e92\u3001\u9884\u6d4b\u8bef\u5dee\u7d2f\u79ef\u548c\u52a8\u6001\u6f14\u53d8\u7b49\u6311\u6218\uff0c\u63d0\u5347\u65c5\u884c\u65f6\u95f4\u9884\u6d4b\u548c\u89c4\u5212\u7b49\u5e94\u7528\u7684\u51c6\u786e\u6027\u3002", "method": "\u57fa\u4e8e\u795e\u7ecf\u63a7\u5236\u5fae\u5206\u65b9\u7a0b\uff08CDEs\uff09\u7684\u6a21\u578b\uff0c\u7ed3\u5408\u7269\u7406\u6307\u5bfc\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u8fdb\u884c\u591a\u673a\u5668\u4eba\u52a8\u6001\u5efa\u6a21\u3002", "result": "\u57281\u5206\u949f\u7684\u9884\u6d4b\u8303\u56f4\u5185\uff0cPINCoDE\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08ADE\uff09\u4f4e\u4e8e0.5\u7c73\uff0c\u4e14\u901a\u8fc7\u9010\u6b65\u5b66\u4e60\u7b56\u7565\uff0c\u9884\u6d4b\u4f4d\u7f6e\u8bef\u5dee\u964d\u4f4e\u4e862.7\u500d\uff0c\u5177\u6709\u826f\u597d\u7684\u6269\u5c55\u6027\u3002", "conclusion": "PINCoDE\u6a21\u578b\u5728\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u957f\u671f\u8fd0\u52a8\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u80fd\u591f\u5728\u4e0d\u589e\u52a0\u6a21\u578b\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u6269\u5c55\uff0c\u4e14\u5728\u672a\u6765\u76ee\u6807\u7684\u6761\u4ef6\u4e0b\u6709\u6548\u5730\u8003\u8651\u7269\u7406\u7ea6\u675f\uff0c\u51cf\u5c11\u9884\u6d4b\u8bef\u5dee\u3002"}}
{"id": "2510.00266", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.00266", "abs": "https://arxiv.org/abs/2510.00266", "authors": ["Paul C. Parsons"], "title": "Visualization Was Here: Reorienting Research When Visualizations Fade into the Background", "comment": "accepted to alt.VIS 2025 at IEEE VIS", "summary": "Visualization research often centers on how visual representations generate\ninsight, guide interpretation, or support decision-making. But in many\nreal-world domains, visualizations do not stand out--they recede into the\nbackground, stabilized and trusted as part of the everyday infrastructure of\nwork. This paper explores what it means to take such quiet roles seriously.\nDrawing on theoretical traditions from joint cognitive systems, naturalistic\ndecision making, and infrastructure studies, I examine how visualization can\nbecome embedded in the rhythms of expert practice--less a site of intervention\nthan a scaffold for attention, coordination, and judgment. I illustrate this\nreorientation with examples from mission control operations at NASA, where\nvisualizations are deeply integrated but rarely interrogated. Rather than treat\ninvisibility as a failure of design or innovation, I argue that visualization's\ninfrastructural presence demands new concepts, methods, and critical\nsensibilities. The goal is not to diminish visualization's importance, but to\nbroaden the field's theoretical repertoire--to recognize and support\nvisualization-in-use even when it fades from view.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u89c6\u89c9\u5316\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u4f5c\u4e3a\u57fa\u7840\u8bbe\u65bd\u7684\u9690\u6027\u89d2\u8272\uff0c\u5f3a\u8c03\u5728\u8bbe\u8ba1\u4e2d\u7406\u89e3\u548c\u652f\u6301\u8fd9\u79cd\u89d2\u8272\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u63a2\u8ba8\u5728\u8bb8\u591a\u771f\u5b9e\u4e16\u754c\u9886\u57df\u4e2d\uff0c\u89c6\u89c9\u5316\u7684\u9ed8\u9ed8\u89d2\u8272\u5982\u4f55\u5f71\u54cd\u5de5\u4f5c\u5b9e\u8df5\uff0c\u5e76\u5c06\u5176\u89c6\u4e3a\u4e00\u4e2a\u91cd\u8981\u65b9\u9762\uff0c\u800c\u4e0d\u662f\u4e00\u9879\u8bbe\u8ba1\u6216\u521b\u65b0\u7684\u5931\u8d25\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u8054\u5408\u8ba4\u77e5\u7cfb\u7edf\u3001\u81ea\u7136\u51b3\u7b56\u548c\u57fa\u7840\u8bbe\u65bd\u7814\u7a76\u7684\u7406\u8bba\u4f20\u7edf\uff0c\u7814\u7a76\u89c6\u89c9\u5316\u5982\u4f55\u878d\u5165\u4e13\u5bb6\u5b9e\u8df5\u7684\u8282\u594f\u3002", "result": "\u901a\u8fc7\u5bf9NASA\u4efb\u52a1\u63a7\u5236\u64cd\u4f5c\u7684\u4f8b\u5b50\u8fdb\u884c\u8bf4\u660e\uff0c\u5c55\u793a\u4e86\u89c6\u89c9\u5316\u5728\u4e13\u5bb6\u5b9e\u8df5\u4e2d\u7684\u6df1\u5ea6\u6574\u5408\u4e0e\u4f7f\u7528\u3002", "conclusion": "\u89c6\u89c9\u5316\u7684\u57fa\u7840\u6027\u5b58\u5728\u9700\u8981\u65b0\u7684\u6982\u5ff5\u3001\u65b9\u6cd5\u548c\u6279\u5224\u6027\u654f\u611f\u6027\uff0c\u4ee5\u652f\u6301\u89c6\u89c9\u5316\u5728\u4f7f\u7528\u4e2d\u7684\u89d2\u8272\uff0c\u5373\u4f7f\u5b83\u5728\u89c6\u91ce\u4e2d\u6de1\u5316\u3002"}}
{"id": "2510.00406", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.00406", "abs": "https://arxiv.org/abs/2510.00406", "authors": ["Hengtao Li", "Pengxiang Ding", "Runze Suo", "Yihao Wang", "Zirui Ge", "Dongyuan Zang", "Kexian Yu", "Mingyang Sun", "Hongyin Zhang", "Donglin Wang", "Weihua Su"], "title": "VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators", "comment": null, "summary": "Vision-Language-Action (VLA) models enable embodied decision-making but rely\nheavily on imitation learning, leading to compounding errors and poor\nrobustness under distribution shift. Reinforcement learning (RL) can mitigate\nthese issues yet typically demands costly real-world interactions or suffers\nfrom sim-to-real gaps. We introduce VLA-RFT, a reinforcement fine-tuning\nframework that leverages a data-driven world model as a controllable simulator.\nTrained from real interaction data, the simulator predicts future visual\nobservations conditioned on actions, allowing policy rollouts with dense,\ntrajectory-level rewards derived from goal-achieving references. This design\ndelivers an efficient and action-aligned learning signal, drastically lowering\nsample requirements. With fewer than 400 fine-tuning steps, VLA-RFT surpasses\nstrong supervised baselines and achieves greater efficiency than\nsimulator-based RL. Moreover, it exhibits strong robustness under perturbed\nconditions, sustaining stable task execution. Our results establish\nworld-model-based RFT as a practical post-training paradigm to enhance the\ngeneralization and robustness of VLA models. For more details, please refer to\nhttps://vla-rft.github.io/.", "AI": {"tldr": "VLA-RFT\u662f\u4e00\u79cd\u57fa\u4e8e\u4e16\u754c\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86VLA\u6a21\u578b\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u51b3\u7b56\u80fd\u529b\u4e0e\u9c81\u68d2\u6027\uff0c\u62e5\u6709\u9ad8\u6548\u7684\u5b66\u4e60\u4fe1\u53f7\u548c\u8f83\u4f4e\u7684\u6837\u672c\u9700\u6c42\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709VLA\u6a21\u578b\u5728\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u7531\u4e8e\u5206\u5e03\u8f6c\u53d8\u5f15\u8d77\u7684\u9519\u8bef\u7d2f\u79ef\u53ca\u9c81\u68d2\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4e86\u57fa\u4e8e\u4e16\u754c\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u6846\u67b6\uff0c\u5229\u7528\u5b9e\u9645\u4ea4\u4e92\u6570\u636e\u8bad\u7ec3\u6a21\u62df\u5668\uff0c\u4ee5\u9884\u6d4b\u672a\u6765\u89c6\u89c9\u89c2\u6d4b\u5e76\u63d0\u4f9b\u5bc6\u96c6\u7684\u3001\u57fa\u4e8e\u8f68\u8ff9\u7684\u5956\u52b1\u3002", "result": "\u5728\u4e0d\u5230400\u6b21\u5fae\u8c03\u6b65\u9aa4\u4e2d\uff0cVLA-RFT\u8d85\u8d8a\u4e86\u5f3a\u5927\u7684\u76d1\u7763\u57fa\u7ebf\uff0c\u5e76\u5728\u6270\u52a8\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u8f83\u5f3a\u7684\u9c81\u68d2\u6027\uff0c\u7a33\u5b9a\u6267\u884c\u4efb\u52a1\u3002", "conclusion": "VLA-RFT\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u4e16\u754c\u6a21\u578b\u5b9e\u73b0\u4e86\u53ef\u63a7\u7684\u4eff\u771f\uff0c\u63d0\u9ad8\u4e86VLA\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.00339", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG", "I.2.7; H.5.2"], "pdf": "https://arxiv.org/pdf/2510.00339", "abs": "https://arxiv.org/abs/2510.00339", "authors": ["T. James Brandt"], "title": "Navigating the Synchrony-Stability Frontier in Adaptive Chatbots", "comment": "pages; 9 tables; 7 figures; code & analysis artifact:\n  https://doi.org/10.5281/zenodo.17238269; under review at ACM IUI 2026", "summary": "Adaptive chatbots that mimic a user's linguistic style can build rapport and\nengagement, yet unconstrained mimicry risks an agent that feels unstable or\nsycophantic. We present a computational evaluation framework that makes the\ncore design tension explicit: balancing moment-to-moment linguistic synchrony\nagainst long-term persona stability. Using an 8-dimensional style vector and a\nclosed-loop \"base+delta\" prompting architecture, we simulate and compare\nexplicit adaptation policies - Uncapped, Cap, Exponential Moving Average (EMA),\nDead-Band, and Hybrids - on a human-log dataset. Our analysis maps a clear\nPareto frontier: bounded policies achieve substantial gains in stability at a\nmodest cost to synchrony. For example, a Hybrid (EMA+Cap) raises stability from\n0.542 to 0.878 (+62%) while reducing synchrony by only 17%. We confirm this\ntrade-off through large-scale replications on three public corpora\n(DailyDialog, Persona-Chat, EmpatheticDialogues) and LLM-in-the-loop validation\nacross two model families. Furthermore, we quantify \"prompt legibility,\"\nshowing that frontier policies reduce instruction churn and cut jarring\nregister flips (major tone changes) from 0.254 to 0.092, yielding systems that\nare easier to reason about and maintain. Taken together, our framework provides\na general evaluation harness for style adaptation; a systematic ablation that\nidentifies Pareto-efficient policies; robust validation across diverse datasets\nand models; and novel legibility metrics linking policy choices to system\nmaintainability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u8bc4\u4f30\u6846\u67b6\uff0c\u5f3a\u8c03\u5728\u8bed\u8a00\u540c\u6b65\u4e0e\u4e2a\u6027\u7a33\u5b9a\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u901a\u8fc7\u4e00\u7cfb\u5217\u653f\u7b56\u7684\u6bd4\u8f83\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u53ef\u63d0\u9ad8\u7a33\u5b9a\u6027\u7684\u6709\u6548\u9002\u5e94\u7b56\u7565\u3002", "motivation": "\u9002\u5e94\u6027\u804a\u5929\u673a\u5668\u4eba\u901a\u8fc7\u6a21\u4eff\u7528\u6237\u7684\u8bed\u8a00\u98ce\u683c\u6765\u5efa\u7acb\u5173\u7cfb\u548c\u53c2\u4e0e\u611f\uff0c\u4f46\u65e0\u8282\u5236\u7684\u6a21\u4eff\u53ef\u80fd\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u6216\u8c04\u5a9a\u7684\u4ee3\u7406\u4f53\u9a8c\u3002", "method": "\u4f7f\u75288\u7ef4\u98ce\u683c\u5411\u91cf\u548c\u5c01\u95ed\u5faa\u73af\u7684\u201c\u57fa\u7840+\u589e\u91cf\u201d\u63d0\u793a\u67b6\u6784\uff0c\u6a21\u62df\u548c\u6bd4\u8f83\u4e0d\u540c\u7684\u9002\u5e94\u653f\u7b56\u3002", "result": "\u8fb9\u754c\u653f\u7b56\u663e\u8457\u63d0\u9ad8\u4e86\u7a33\u5b9a\u6027\uff0c\u540c\u65f6\u4ec5\u5c0f\u5e45\u964d\u4f4e\u4e86\u540c\u6b65\u6027\uff0c\u4e14\u65b0\u63d0\u5021\u7684\u653f\u7b56\u80fd\u591f\u51cf\u5c11\u6307\u4ee4\u53d8\u5316\u5e76\u964d\u4f4e\u660e\u663e\u8bed\u6c14\u53d8\u5316\u7684\u53d1\u751f\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u7684\u98ce\u683c\u9002\u5e94\u8bc4\u4f30\u5de5\u5177\uff0c\u80fd\u591f\u8bc6\u522b\u5e15\u7d2f\u6258\u6709\u6548\u653f\u7b56\uff0c\u5e76\u901a\u8fc7\u591a\u6837\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\u8fdb\u884c\u7a33\u5065\u9a8c\u8bc1\uff0c\u540c\u65f6\u5f15\u5165\u65b0\u9896\u7684\u53ef\u8bfb\u6027\u6307\u6807\u4ee5\u8fde\u63a5\u653f\u7b56\u9009\u62e9\u4e0e\u7cfb\u7edf\u7684\u7ef4\u62a4\u6027\u3002"}}
{"id": "2510.00441", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.00441", "abs": "https://arxiv.org/abs/2510.00441", "authors": ["Yiyuan Pan", "Yunzhe Xu", "Zhe Liu", "Hesheng Wang"], "title": "Seeing through Uncertainty: Robust Task-Oriented Optimization in Visual Navigation", "comment": null, "summary": "Visual navigation is a fundamental problem in embodied AI, yet practical\ndeployments demand long-horizon planning capabilities to address\nmulti-objective tasks. A major bottleneck is data scarcity: policies learned\nfrom limited data often overfit and fail to generalize OOD. Existing neural\nnetwork-based agents typically increase architectural complexity that\nparadoxically become counterproductive in the small-sample regime. This paper\nintroduce NeuRO, a integrated learning-to-optimize framework that tightly\ncouples perception networks with downstream task-level robust optimization.\nSpecifically, NeuRO addresses core difficulties in this integration: (i) it\ntransforms noisy visual predictions under data scarcity into convex uncertainty\nsets using Partially Input Convex Neural Networks (PICNNs) with conformal\ncalibration, which directly parameterize the optimization constraints; and (ii)\nit reformulates planning under partial observability as a robust optimization\nproblem, enabling uncertainty-aware policies that transfer across environments.\nExtensive experiments on both unordered and sequential multi-object navigation\ntasks demonstrate that NeuRO establishes SoTA performance, particularly in\ngeneralization to unseen environments. Our work thus presents a significant\nadvancement for developing robust, generalizable autonomous agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7684NeuRO\u6846\u67b6\u6709\u6548\u7ed3\u5408\u611f\u77e5\u4e0e\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u591a\u76ee\u6807\u5bfc\u822a\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5728\u65b0\u73af\u5883\u4e2d\u7684\u4f18\u5f02\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u5bfc\u822a\u4ee3\u7406\u5f80\u5f80\u56e0\u6570\u636e\u7a00\u7f3a\u800c\u8fc7\u62df\u5408\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u65b0\u7684\u73af\u5883\uff0c\u56e0\u6b64\u6025\u9700\u589e\u5f3a\u957f\u65f6\u95f4\u89c4\u5212\u548c\u591a\u76ee\u6807\u4efb\u52a1\u7684\u80fd\u529b\u3002", "method": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aNeuRO\u7684\u7efc\u5408\u5b66\u4e60\u4f18\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u611f\u77e5\u7f51\u7edc\u4e0e\u4efb\u52a1\u5c42\u6b21\u7684\u9c81\u68d2\u4f18\u5316\uff0c\u901a\u8fc7\u90e8\u5206\u8f93\u5165\u51f8\u795e\u7ecf\u7f51\u7edc\uff08PICNNs\uff09\u5904\u7406\u6570\u636e\u7a00\u7f3a\u4e0b\u7684\u89c6\u89c9\u9884\u6d4b\uff0c\u5e76\u5c06\u90e8\u5206\u53ef\u89c2\u5bdf\u6027\u4e0b\u7684\u89c4\u5212\u91cd\u6784\u4e3a\u9c81\u68d2\u4f18\u5316\u95ee\u9898\u3002", "result": "\u5728\u65e0\u5e8f\u548c\u987a\u5e8f\u591a\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\u4e2d\uff0cNeuRO\u7684\u8868\u73b0\u8fbe\u5230\u4e86\u6700\u65b0\u7684\u6280\u672f\u6c34\u5e73\uff08SoTA\uff09\uff0c\u7279\u522b\u662f\u5728\u5bf9\u672a\u89c1\u73af\u5883\u7684\u6cdb\u5316\u80fd\u529b\u4e0a\u3002", "conclusion": "NeuRO\u5728\u591a\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u65b0\u73af\u5883\u7684\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u63a8\u52a8\u4e86\u9c81\u68d2\u6027\u548c\u53ef\u6cdb\u5316\u81ea\u6cbb\u4f53\u7684\u5f00\u53d1\u3002"}}
{"id": "2510.00344", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.00344", "abs": "https://arxiv.org/abs/2510.00344", "authors": ["Chang Han", "Andrew Mcnutt"], "title": "The Feng Shui of Visualization: Design the Path to SUCCESS and GOOD FORTUNE", "comment": "6 pages, 3 figures; alt.VIS", "summary": "Superstition and religious belief system have historically shaped human\nbehavior, offering powerful psychological motivations and persuasive frameworks\nto guide actions. Inspired by Feng Shui -- an ancient Chinese superstition --\nthis paper proposes a pseudo-theoretical framework that integrates\nsuperstition-like heuristics into visualization design. Rather than seeking\nempirical truth, this framework leverages culturally resonant (superstitious)\nnarratives and symbolic metaphors as persuasive tools to encourage desirable\ndesign practices, such as clarity, accessibility, and audience-centered\nthinking. We articulate a set of visualization designs into a Feng Shui\ncompass, reframing empirical design principles and guidelines within an engaing\nmythology. We present how visualization design principles can be intepreted in\nFeng Shui narratives, discussing the potential of these metaphorical principles\nin reducing designer anxiety, fostering community norms, and enhancing the\nmemorability and internalization of visualization design guidelines. Finally,\nwe discuss Feng Shui visualization theory as a set of cognitive shortcuts that\ncan exert persuasive power through playful, belief-like activities.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u98ce\u6c34\u7684\u53ef\u89c6\u5316\u8bbe\u8ba1\u6846\u67b6\uff0c\u5229\u7528\u8ff7\u4fe1\u53d9\u4e8b\u589e\u5f3a\u8bbe\u8ba1\u6548\u679c\uff0c\u51cf\u8f7b\u8bbe\u8ba1\u5e08\u538b\u529b\u5e76\u4fc3\u8fdb\u793e\u533a\u8ba4\u540c\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u8ff7\u4fe1\u4e0e\u5b97\u6559\u4fe1\u4ef0\u5728\u5851\u9020\u4eba\u7c7b\u884c\u4e3a\u4e2d\u7684\u5f71\u54cd\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u53ef\u89c6\u5316\u8bbe\u8ba1\uff0c\u4ee5\u63d0\u9ad8\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u4e0e\u63a5\u53d7\u5ea6\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u79cd\u4f2a\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u8ff7\u4fe1\u5fc3\u7406\u4e0e\u53ef\u89c6\u5316\u8bbe\u8ba1\u539f\u5219\u7ed3\u5408\uff0c\u4ee5\u63d0\u5347\u8bbe\u8ba1\u5b9e\u8df5\u3002", "result": "\u901a\u8fc7\u5c06\u53ef\u89c6\u5316\u8bbe\u8ba1\u539f\u5219\u4e0e\u98ce\u6c34\u53d9\u4e8b\u7ed3\u5408\uff0c\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u8bbe\u8ba1\u5b9e\u8df5\u6307\u5357\uff0c\u51cf\u8f7b\u8bbe\u8ba1\u5e08\u7126\u8651\uff0c\u589e\u5f3a\u793e\u533a\u89c4\u8303\u548c\u8bb0\u5fc6\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8ff7\u4fe1\u4e0e\u53ef\u89c6\u5316\u8bbe\u8ba1\u7684\u65b0\u6846\u67b6\uff0c\u5f3a\u8c03\u4e86\u6587\u5316\u53d9\u4e8b\u5728\u8bbe\u8ba1\u4e2d\u7684\u5fc3\u7406\u5f71\u54cd\u529b\u3002"}}
{"id": "2510.00466", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00466", "abs": "https://arxiv.org/abs/2510.00466", "authors": ["Run Su", "Hao Fu", "Shuai Zhou", "Yingao Fu"], "title": "Integrating Offline Pre-Training with Online Fine-Tuning: A Reinforcement Learning Approach for Robot Social Navigation", "comment": null, "summary": "Offline reinforcement learning (RL) has emerged as a promising framework for\naddressing robot social navigation challenges. However, inherent uncertainties\nin pedestrian behavior and limited environmental interaction during training\noften lead to suboptimal exploration and distributional shifts between offline\ntraining and online deployment. To overcome these limitations, this paper\nproposes a novel offline-to-online fine-tuning RL algorithm for robot social\nnavigation by integrating Return-to-Go (RTG) prediction into a causal\nTransformer architecture. Our algorithm features a spatiotem-poral fusion model\ndesigned to precisely estimate RTG values in real-time by jointly encoding\ntemporal pedestrian motion patterns and spatial crowd dynamics. This RTG\nprediction framework mitigates distribution shift by aligning offline policy\ntraining with online environmental interactions. Furthermore, a hybrid\noffline-online experience sampling mechanism is built to stabilize policy\nupdates during fine-tuning, ensuring balanced integration of pre-trained\nknowledge and real-time adaptation. Extensive experiments in simulated social\nnavigation environments demonstrate that our method achieves a higher success\nrate and lower collision rate compared to state-of-the-art baselines. These\nresults underscore the efficacy of our algorithm in enhancing navigation policy\nrobustness and adaptability. This work paves the way for more reliable and\nadaptive robotic navigation systems in real-world applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u56de\u62a5\u9884\u6d4b\u96c6\u6210\u5728\u56e0\u679c\u53d8\u6362\u5668\u4e2d\uff0c\u89e3\u51b3\u673a\u5668\u4eba\u793e\u4ea4\u5bfc\u822a\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u6548\u3002", "motivation": "\u5e94\u5bf9\u673a\u5668\u4eba\u793e\u4ea4\u5bfc\u822a\u4e2d\u7684\u884c\u4eba\u884c\u4e3a\u4e0d\u786e\u5b9a\u6027\u548c\u6709\u9650\u73af\u5883\u4ea4\u4e92\u5bfc\u81f4\u7684\u6b21\u4f18\u63a2\u7d22\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u79bb\u7ebf\u5230\u5728\u7ebf\u5fae\u8c03\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e86\u56de\u62a5\u9884\u6d4b\u548c\u56e0\u679c\u53d8\u6362\u5668\u67b6\u6784\u3002", "result": "\u5728\u6a21\u62df\u793e\u4ea4\u5bfc\u822a\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u7387\u66f4\u9ad8\u4e14\u78b0\u649e\u7387\u66f4\u4f4e\uff0c\u8d85\u8fc7\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u3002", "conclusion": "\u672c\u7814\u7a76\u5f00\u53d1\u7684\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u673a\u5668\u4eba\u793e\u4ea4\u5bfc\u822a\u7684\u6210\u529f\u7387\u548c\u9002\u5e94\u6027\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u5bfc\u822a\u7cfb\u7edf\u3002"}}
{"id": "2510.00361", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00361", "abs": "https://arxiv.org/abs/2510.00361", "authors": ["Hita Kambhamettu", "Alyssa Hwang", "Philippe Laban", "Andrew Head"], "title": "Attribution Gradients: Incrementally Unfolding Citations for Critical Examination of Attributed AI Answers", "comment": null, "summary": "AI question answering systems increasingly generate responses with\nattributions to sources. However, the task of verifying the actual content of\nthese attributions is in most cases impractical. In this paper, we present\nattribution gradients as a solution. Attribution gradients provide integrated,\nincremental affordances for diving into an attributed passage. A user can\ndecompose a sentence of an answer into its claims. For each claim, the user can\nview supporting and contradictory excerpts mined from sources. Those excerpts\nserve as clickable conduits into the source (in our application, scientific\npapers). When evidence itself contains more citations, the UI unpacks the\nevidence into excerpts from the cited sources. These features of attribution\ngradients facilitate concurrent interconnections among answer, claim, excerpt,\nand context. In a usability study, we observed greater engagement with sources\nand richer revision in a task where participants revised an attributed AI\nanswer with attribution gradients and a baseline.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u5f52\u56e0\u68af\u5ea6\u4f5c\u4e3a\u89e3\u51b3AI\u95ee\u7b54\u7cfb\u7edf\u6e90\u5f15\u8bc1\u9a8c\u8bc1\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u8fdb\u7528\u6237\u4e0e\u6765\u6e90\u5185\u5bb9\u95f4\u7684\u4e92\u52a8\uff0c\u63d0\u9ad8\u4e86\u4fee\u8ba2\u4efb\u52a1\u7684\u8d28\u91cf\u3002", "motivation": "\u968f\u7740AI\u95ee\u7b54\u7cfb\u7edf\u9010\u6e10\u751f\u6210\u5e26\u6709\u6e90\u5f15\u8bc1\u7684\u56de\u7b54\uff0c\u9a8c\u8bc1\u8fd9\u4e9b\u5f15\u8bc1\u7684\u5b9e\u9645\u5185\u5bb9\u53d8\u5f97\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u8fd0\u7528\u5f52\u56e0\u68af\u5ea6\u6280\u672f\uff0c\u7528\u6237\u53ef\u5206\u89e3AI\u7b54\u6848\u4e2d\u7684\u53e5\u5b50\uff0c\u67e5\u770b\u652f\u6301\u548c\u53cd\u5bf9\u7684\u5f15\u6587\u3002", "result": "\u7528\u6237\u5728\u4f7f\u7528\u5f52\u56e0\u68af\u5ea6\u8fdb\u884c\u4fee\u8ba2\u65f6\uff0c\u5bf9\u6e90\u6750\u6599\u7684\u53c2\u4e0e\u5ea6\u66f4\u9ad8\uff0c\u4fee\u8ba2\u4efb\u52a1\u7684\u4e30\u5bcc\u6027\u4e5f\u66f4\u5f3a\u3002", "conclusion": "\u4f7f\u7528\u5f52\u56e0\u68af\u5ea6\u53ef\u4ee5\u63d0\u9ad8\u7528\u6237\u5bf9\u6e90\u6750\u6599\u7684\u53c2\u4e0e\u5ea6\u548c\u4efb\u52a1\u4fee\u8ba2\u7684\u4e30\u5bcc\u6027\u3002"}}
{"id": "2510.00491", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00491", "abs": "https://arxiv.org/abs/2510.00491", "authors": ["Han Zhou", "Jinjin Cao", "Liyuan Ma", "Xueji Fang", "Guo-jun Qi"], "title": "From Human Hands to Robot Arms: Manipulation Skills Transfer via Trajectory Alignment", "comment": null, "summary": "Learning diverse manipulation skills for real-world robots is severely\nbottlenecked by the reliance on costly and hard-to-scale teleoperated\ndemonstrations. While human videos offer a scalable alternative, effectively\ntransferring manipulation knowledge is fundamentally hindered by the\nsignificant morphological gap between human and robotic embodiments. To address\nthis challenge and facilitate skill transfer from human to robot, we introduce\nTraj2Action,a novel framework that bridges this embodiment gap by using the 3D\ntrajectory of the operational endpoint as a unified intermediate\nrepresentation, and then transfers the manipulation knowledge embedded in this\ntrajectory to the robot's actions. Our policy first learns to generate a coarse\ntrajectory, which forms an high-level motion plan by leveraging both human and\nrobot data. This plan then conditions the synthesis of precise, robot-specific\nactions (e.g., orientation and gripper state) within a co-denoising framework.\nExtensive real-world experiments on a Franka robot demonstrate that Traj2Action\nboosts the performance by up to 27% and 22.25% over $\\pi_0$ baseline on short-\nand long-horizon real-world tasks, and achieves significant gains as human data\nscales in robot policy learning. Our project website, featuring code and video\ndemonstrations, is available at\nhttps://anonymous.4open.science/w/Traj2Action-4A45/.", "AI": {"tldr": "Traj2Action\u662f\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc73D\u8f68\u8ff9\u4fc3\u8fdb\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u6280\u80fd\u8f6c\u79fb\uff0c\u4ece\u800c\u5728\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u9ad8\u673a\u5668\u4eba\u7684\u64cd\u4f5c\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u901a\u8fc7\u65e0\u4eba\u673a\u6f14\u793a\u5b66\u4e60\u591a\u6837\u5316\u64cd\u4f5c\u6280\u80fd\u65f6\u7684\u6210\u672c\u548c\u6269\u5c55\u6027\u95ee\u9898\uff0c\u4ee5\u53ca\u4eba\u7c7b\u548c\u673a\u5668\u4eba\u5f62\u6001\u4e4b\u95f4\u7684\u663e\u8457\u5dee\u8ddd\u3002", "method": "\u4f7f\u75283D\u64cd\u4f5c\u7aef\u70b9\u8f68\u8ff9\u4f5c\u4e3a\u7edf\u4e00\u7684\u4e2d\u4ecb\u8868\u793a\uff0c\u901a\u8fc7\u6761\u4ef6\u751f\u6210\u6765\u5b9e\u73b0\u4eba\u4e0e\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u77e5\u8bc6\u8f6c\u79fb\u3002", "result": "\u5728Franka\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cTraj2Action\u5728\u77ed\u671f\u548c\u957f\u671f\u771f\u5b9e\u4efb\u52a1\u4e2d\uff0c\u76f8\u8f83\u4e8e$\text{\u03c0}_0$\u57fa\u51c6\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe27%\u548c22.25%\u3002", "conclusion": "Traj2Action\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u7684\u64cd\u4f5c\u6027\u80fd\uff0c\u4f7f\u5f97\u673a\u5668\u4eba\u80fd\u66f4\u6709\u6548\u5730\u4ece\u4eba\u7c7b\u89c6\u9891\u4e2d\u5b66\u4e60\u591a\u6837\u5316\u7684\u64cd\u4f5c\u6280\u80fd\u3002"}}
{"id": "2510.00407", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.00407", "abs": "https://arxiv.org/abs/2510.00407", "authors": ["Jade Kandel", "Sriya Kasumarthi", "Spiros Tsalikis", "Chelsea Duppen", "Daniel Szafir", "Michael Lewek", "Henry Fuchs", "Danielle Szafir"], "title": "Investigating Encoding and Perspective for Augmented Reality", "comment": null, "summary": "Augmented reality (AR) offers promising opportunities to support\nmovement-based activities, such as personal training or physical therapy, with\nreal-time, spatially-situated visual cues. While many approaches leverage AR to\nguide motion, existing design guidelines focus on simple, upper-body movements\nwithin the user's field of view. We lack evidence-based design recommendations\nfor guiding more diverse scenarios involving movements with varying levels of\nvisibility and direction. We conducted an experiment to investigate how\ndifferent visual encodings and perspectives affect motion guidance performance\nand usability, using three exercises that varied in visibility and planes of\nmotion. Our findings reveal significant differences in preference and\nperformance across designs. Notably, the best perspective varied depending on\nmotion visibility and showing more information about the overall motion did not\nnecessarily improve motion execution. We provide empirically-grounded\nguidelines for designing immersive, interactive visualizations for motion\nguidance to support more effective AR systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u8c03\u67e5\u4e86\u589e\u5f3a\u73b0\u5b9e\u4e2d\u600e\u6837\u7684\u53ef\u89c6\u7f16\u7801\u548c\u89c6\u89d2\u80fd\u6709\u6548\u6307\u5bfc\u8fd0\u52a8\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u5b9e\u8bc1\u7684\u8bbe\u8ba1\u6307\u5357\uff0c\u4ee5\u63d0\u9ad8\u589e\u5f3a\u73b0\u5b9e\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "motivation": "\u867d\u7136\u8bb8\u591a\u65b9\u6cd5\u5229\u7528\u589e\u5f3a\u73b0\u5b9e\u6765\u6307\u5bfc\u8fd0\u52a8\uff0c\u4f46\u73b0\u6709\u7684\u8bbe\u8ba1\u6307\u5357\u4e3b\u8981\u96c6\u4e2d\u5728\u7528\u6237\u89c6\u91ce\u5185\u7684\u7b80\u5355\u4e0a\u534a\u8eab\u8fd0\u52a8\u4e0a\uff0c\u56e0\u6b64\u7f3a\u4e4f\u5bf9\u591a\u6837\u5316\u573a\u666f\u7684\u8bc1\u636e\u57fa\u7840\u8bbe\u8ba1\u5efa\u8bae\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u7814\u7a76\u4e0d\u540c\u7684\u53ef\u89c6\u7f16\u7801\u548c\u89c6\u89d2\u5982\u4f55\u5f71\u54cd\u8fd0\u52a8\u5f15\u5bfc\u7684\u8868\u73b0\u548c\u7528\u6237\u4f53\u9a8c\uff0c\u9009\u53d6\u4e86\u4e09\u79cd\u53ef\u89c1\u6027\u548c\u8fd0\u52a8\u5e73\u9762\u5404\u5f02\u7684\u7ec3\u4e60\u3002", "result": "\u6211\u4eec\u7684\u7814\u7a76\u53d1\u73b0\uff0c\u4e0d\u540c\u8bbe\u8ba1\u7684\u504f\u597d\u548c\u8868\u73b0\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff1b\u6700\u4f73\u89c6\u89d2\u56e0\u8fd0\u52a8\u53ef\u89c1\u6027\u800c\u5f02\uff0c\u5e76\u4e14\u63d0\u4f9b\u66f4\u591a\u5173\u4e8e\u6574\u4f53\u8fd0\u52a8\u7684\u4fe1\u606f\u5e76\u4e0d\u4e00\u5b9a\u6539\u5584\u8fd0\u52a8\u6267\u884c\u3002", "conclusion": "\u6211\u4eec\u63d0\u4f9b\u4e86\u57fa\u4e8e\u5b9e\u8bc1\u7684\u8bbe\u8ba1\u6307\u5357\uff0c\u4ee5\u652f\u6301\u66f4\u6709\u6548\u7684\u589e\u5f3a\u73b0\u5b9e\u7cfb\u7edf\uff0c\u4e13\u6ce8\u4e8e\u8fd0\u52a8\u5f15\u5bfc\u7684\u6c89\u6d78\u5f0f\u4e92\u52a8\u53ef\u89c6\u5316\u8bbe\u8ba1\u3002"}}
{"id": "2510.00524", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.00524", "abs": "https://arxiv.org/abs/2510.00524", "authors": ["Baoshan Song", "Penggao Yan", "Xiao Xia", "Yihan Zhong", "Weisong Wen", "Li-Ta Hsu"], "title": "Two stage GNSS outlier detection for factor graph optimization based GNSS-RTK/INS/odometer fusion", "comment": null, "summary": "Reliable GNSS positioning in complex environments remains a critical\nchallenge due to non-line-of-sight (NLOS) propagation, multipath effects, and\nfrequent signal blockages. These effects can easily introduce large outliers\ninto the raw pseudo-range measurements, which significantly degrade the\nperformance of global navigation satellite system (GNSS) real-time kinematic\n(RTK) positioning and limit the effectiveness of tightly coupled GNSS-based\nintegrated navigation system. To address this issue, we propose a two-stage\noutlier detection method and apply the method in a tightly coupled GNSS-RTK,\ninertial navigation system (INS), and odometer integration based on factor\ngraph optimization (FGO). In the first stage, Doppler measurements are employed\nto detect pseudo-range outliers in a GNSS-only manner, since Doppler is less\nsensitive to multipath and NLOS effects compared with pseudo-range, making it a\nmore stable reference for detecting sudden inconsistencies. In the second\nstage, pre-integrated inertial measurement units (IMU) and odometer constraints\nare used to generate predicted double-difference pseudo-range measurements,\nwhich enable a more refined identification and rejection of remaining outliers.\nBy combining these two complementary stages, the system achieves improved\nrobustness against both gross pseudo-range errors and degraded satellite\nmeasuring quality. The experimental results demonstrate that the two-stage\ndetection framework significantly reduces the impact of pseudo-range outliers,\nand leads to improved positioning accuracy and consistency compared with\nrepresentative baseline approaches. In the deep urban canyon test, the outlier\nmitigation method has limits the RMSE of GNSS-RTK/INS/odometer fusion from 0.52\nm to 0.30 m, with 42.3% improvement.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u7684\u4f2a\u8ddd\u5f02\u5e38\u503c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u6709\u6548\u7f13\u89e3\u4e86GNSS\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u95ee\u9898\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u9ad8\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u5728\u590d\u6742\u73af\u5883\u4e2d\uff0c\u7531\u4e8e\u975e\u89c6\u8ddd\u4f20\u64ad\u3001\u591a\u8def\u5f84\u6548\u5e94\u548c\u4fe1\u53f7\u963b\u585e\uff0cGNSS\u5b9a\u4f4d\u9762\u4e34\u7740\u4e25\u91cd\u6311\u6218\uff0c\u5c24\u5176\u662f\u5f02\u5e38\u503c\u5bf9\u4f2a\u8ddd\u6d4b\u91cf\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u4e86\u4e24\u9636\u6bb5\u7684\u5f02\u5e38\u503c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u9996\u5148\u4f7f\u7528Doppler\u6d4b\u91cf\u8fdb\u884cGNSS-only\u7684\u4f2a\u8ddd\u5f02\u5e38\u503c\u68c0\u6d4b\uff0c\u5176\u6b21\u7ed3\u5408\u9884\u79ef\u5206\u7684IMU\u548c\u91cc\u7a0b\u8ba1\u7ea6\u675f\u751f\u6210\u9884\u6d4b\u7684\u53cc\u5dee\u4f2a\u8ddd\u6d4b\u91cf\u8fdb\u884c\u8fdb\u4e00\u6b65\u5904\u7406\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4f2a\u8ddd\u5f02\u5e38\u503c\u7684\u5f71\u54cd\uff0c\u5c06GNSS-RTK/INS/\u91cc\u7a0b\u8ba1\u878d\u5408\u7684\u5747\u65b9\u6839\u8bef\u5dee\u4ece0.52\u7c73\u964d\u81f30.30\u7c73\uff0c\u63d0\u5347\u4e8642.3%\u7684\u7cbe\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u5f02\u5e38\u503c\u68c0\u6d4b\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86GNSS-RTK\u4e0eINS\u53ca\u91cc\u7a0b\u8ba1\u878d\u5408\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2510.00414", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.00414", "abs": "https://arxiv.org/abs/2510.00414", "authors": ["Matthew Yue", "Zhikun Xu", "Vivek Gupta", "Thao Ha", "Liesal Sharabi", "Ben Zhou"], "title": "RELATE-Sim: Leveraging Turning Point Theory and LLM Agents to Predict and Understand Long-Term Relationship Dynamics through Interactive Narrative Simulations", "comment": "10 pages, 3 figures, Submitted to CHI 2026 Conference", "summary": "Most dating technologies optimize for getting together, not staying together.\nWe present RELATE-Sim, a theory-grounded simulator that models how couples\nbehave at consequential turning points-exclusivity talks, conflict-and-repair\nepisodes, relocations-rather than static traits. Two persona-aligned LLM agents\n(one per partner) interact under a centralized Scene Master that frames each\nturning point as a compact set of realistic options, advances the narrative,\nand infers interpretable state changes and an auditable commitment estimate\nafter each scene. On a longitudinal dataset of 71 couples with two-year\nfollow-ups, simulation-aware predictions outperform a personas-only baseline\nwhile surfacing actionable markers (e.g., repair attempts acknowledged, clarity\nshifts) that explain why trajectories diverge. RELATE-Sim pushes the\nrelationship research's focus from matchmaking to maintenance, providing a\ntransparent, extensible platform for understanding and forecasting long-term\nrelationship dynamics.", "AI": {"tldr": "RELATE-Sim\u662f\u4e00\u4e2a\u6a21\u62df\u5668\uff0c\u91cd\u70b9\u7814\u7a76\u957f\u671f\u60c5\u4fa3\u5173\u7cfb\u4e2d\u7684\u7ef4\u62a4\uff0c\u800c\u975e\u4ec5\u4ec5\u662f\u76f8\u805a\u3002", "motivation": "\u5927\u591a\u6570\u7ea6\u4f1a\u6280\u672f\u4f18\u5316\u7684\u662f\u60c5\u4fa3\u76f8\u805a\uff0c\u800c\u975e\u7ef4\u7cfb\u5173\u7cfb\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u7406\u89e3\u60c5\u4fa3\u5728\u5173\u952e\u65f6\u523b\u7684\u52a8\u6001\u884c\u4e3a\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u9a71\u52a8\u7684\u6a21\u62df\uff0cRELATE-Sim\u5efa\u6a21\u60c5\u4fa3\u5728\u5173\u952e\u8f6c\u6298\u70b9\u7684\u884c\u4e3a\uff0c\u5229\u7528\u4e24\u4e2a\u5bf9\u9f50\u7684LLM\u4ee3\u7406\u4eba\uff0c\u5728\u96c6\u4e2d\u573a\u666f\u7ba1\u7406\u4e0b\u4e92\u52a8\u3002", "result": "\u5728\u5bf971\u5bf9\u60c5\u4fa3\u8fdb\u884c\u4e24\u5e74\u8ddf\u8e2a\u7684\u7eb5\u5411\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u62df\u611f\u77e5\u9884\u6d4b\u4f18\u4e8e\u4ec5\u57fa\u4e8e\u89d2\u8272\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u53ef\u64cd\u4f5c\u7684\u6807\u8bb0\uff0c\u89e3\u91ca\u4e86\u5173\u7cfb\u8f68\u8ff9\u7684\u504f\u79bb\u3002", "conclusion": "RELATE-Sim\u4e3a\u7406\u89e3\u548c\u9884\u6d4b\u957f\u671f\u5173\u7cfb\u52a8\u6001\u63d0\u4f9b\u4e86\u4e00\u4e2a\u900f\u660e\u3001\u53ef\u6269\u5c55\u7684\u5e73\u53f0\uff0c\u63a8\u52a8\u4e86\u5173\u7cfb\u7814\u7a76\u4ece\u5339\u914d\u8f6c\u5411\u7ef4\u62a4\u3002"}}
{"id": "2510.00573", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.00573", "abs": "https://arxiv.org/abs/2510.00573", "authors": ["Yen-Ling Tai", "Yi-Ru Yang", "Kuan-Ting Yu", "Yu-Wei Chao", "Yi-Ting Chen"], "title": "GRITS: A Spillage-Aware Guided Diffusion Policy for Robot Food Scooping Tasks", "comment": null, "summary": "Robotic food scooping is a critical manipulation skill for food preparation\nand service robots. However, existing robot learning algorithms, especially\nlearn-from-demonstration methods, still struggle to handle diverse and dynamic\nfood states, which often results in spillage and reduced reliability. In this\nwork, we introduce GRITS: A Spillage-Aware Guided Diffusion Policy for Robot\nFood Scooping Tasks. This framework leverages guided diffusion policy to\nminimize food spillage during scooping and to ensure reliable transfer of food\nitems from the initial to the target location. Specifically, we design a\nspillage predictor that estimates the probability of spillage given current\nobservation and action rollout. The predictor is trained on a simulated dataset\nwith food spillage scenarios, constructed from four primitive shapes (spheres,\ncubes, cones, and cylinders) with varied physical properties such as mass,\nfriction, and particle size. At inference time, the predictor serves as a\ndifferentiable guidance signal, steering the diffusion sampling process toward\nsafer trajectories while preserving task success. We validate GRITS on a\nreal-world robotic food scooping platform. GRITS is trained on six food\ncategories and evaluated on ten unseen categories with different shapes and\nquantities. GRITS achieves an 82% task success rate and a 4% spillage rate,\nreducing spillage by over 40% compared to baselines without guidance, thereby\ndemonstrating its effectiveness.", "AI": {"tldr": "GRITS\u901a\u8fc7\u5f15\u5bfc\u6269\u6563\u7b56\u7565\u548c\u6ea2\u51fa\u9884\u6d4b\u5668\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u98df\u54c1\u94f2\u53d6\u4efb\u52a1\u7684\u6210\u529f\u7387\u5e76\u964d\u4f4e\u4e86\u6ea2\u51fa\u7387\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u5b66\u4e60\u7b97\u6cd5\u5728\u5e94\u5bf9\u591a\u6837\u5316\u548c\u52a8\u6001\u98df\u54c1\u72b6\u6001\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u5bfc\u81f4\u6ea2\u51fa\u548c\u53ef\u9760\u6027\u964d\u4f4e\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u66f4\u52a0\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faGRITS\u6846\u67b6\uff0c\u5229\u7528\u5f15\u5bfc\u6269\u6563\u7b56\u7565\u8fdb\u884c\u98df\u54c1\u94f2\u53d6\u4efb\u52a1\uff0c\u5e76\u8bbe\u8ba1\u4e86\u6ea2\u51fa\u9884\u6d4b\u5668\u6765\u4f30\u8ba1\u6ea2\u51fa\u7684\u6982\u7387\uff0c\u6b64\u9884\u6d4b\u5668\u5728\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "GRITS\u5728\u516d\u79cd\u98df\u54c1\u7c7b\u522b\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5728\u5341\u79cd\u672a\u89c1\u8fc7\u7684\u98df\u54c1\u7c7b\u522b\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u53d6\u5f97\u4e8682%\u7684\u4efb\u52a1\u6210\u529f\u7387\u548c4%\u7684\u6ea2\u51fa\u7387\u3002", "conclusion": "GRITS\u5728\u673a\u5668\u4eba\u98df\u54c1\u94f2\u53d6\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6210\u529f\u7387\u8fbe\u523082%\uff0c\u6ea2\u51fa\u7387\u964d\u4f4e\u81f34%\uff0c\u76f8\u6bd4\u4e8e\u65e0\u6307\u5bfc\u7684\u57fa\u7ebf\u51cf\u5c11\u4e8640%\u7684\u6ea2\u51fa\u60c5\u51b5\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.00489", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.00489", "abs": "https://arxiv.org/abs/2510.00489", "authors": ["Ismail Alihan Hadimlioglu", "Siddharth Linga"], "title": "Face2Feel: Emotion-Aware Adaptive User Interface", "comment": "8 pages, 8 figures", "summary": "This paper presents Face2Feel, a novel user interface (UI) model that\ndynamically adapts to user emotions and preferences captured through computer\nvision. This adaptive UI framework addresses the limitations of traditional\nstatic interfaces by integrating digital image processing, face recognition,\nand emotion detection techniques. Face2Feel analyzes user expressions utilizing\na webcam or pre-installed camera as the primary data source to personalize the\nUI in real-time. Although dynamically changing user interfaces based on\nemotional states are not yet widely implemented, their advantages and the\ndemand for such systems are evident. This research contributes to the\ndevelopment of emotion-aware applications, particularly in recommendation\nsystems and feedback mechanisms. A case study, \"Shresta: Emotion-Based Book\nRecommendation System,\" demonstrates the practical implementation of this\nframework, the technologies employed, and the system's usefulness. Furthermore,\na user survey conducted after presenting the working model reveals a strong\ndemand for such adaptive interfaces, emphasizing the importance of user\nsatisfaction and comfort in human-computer interaction. The results showed that\nnearly 85.7\\% of the users found these systems to be very engaging and\nuser-friendly. This study underscores the potential for emotion-driven UI\nadaptation to improve user experiences across various applications.", "AI": {"tldr": "Face2Feel\u662f\u4e00\u4e2a\u57fa\u4e8e\u7528\u6237\u60c5\u7eea\u548c\u504f\u597d\u7684\u52a8\u6001\u81ea\u9002\u5e94\u7528\u6237\u754c\u9762\u6a21\u578b\uff0c\u901a\u8fc7\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u5206\u6790\u7528\u6237\u8868\u60c5\uff0c\u5c55\u793a\u4e86\u60c5\u7eea\u9a71\u52a8\u754c\u9762\u5728\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u9759\u6001\u754c\u9762\u7684\u5c40\u9650\u6027\uff0c\u6ee1\u8db3\u5bf9\u60c5\u7eea\u611f\u77e5\u7cfb\u7edf\u7684\u9700\u6c42\u3002", "method": "\u5229\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u8fdb\u884c\u7528\u6237\u60c5\u7eea\u548c\u504f\u597d\u7684\u52a8\u6001\u9002\u5e94\u3002", "result": "\u7528\u6237\u8c03\u67e5\u663e\u793a85.7%\u7684\u7528\u6237\u89c9\u5f97\u8fd9\u4e9b\u7cfb\u7edf\u975e\u5e38\u5438\u5f15\u4eba\u548c\u7528\u6237\u53cb\u597d\uff1b\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5b9e\u7528\u6027\u3002", "conclusion": "\u60c5\u7eea\u9a71\u52a8\u7684\u7528\u6237\u754c\u9762\u9002\u5e94\u6027\u5177\u6709\u6539\u5584\u7528\u6237\u4f53\u9a8c\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u63a8\u8350\u7cfb\u7edf\u548c\u53cd\u9988\u673a\u5236\u4e2d\u3002"}}
{"id": "2510.00600", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00600", "abs": "https://arxiv.org/abs/2510.00600", "authors": ["Pietro Mazzaglia", "Cansu Sancaktar", "Markus Peschl", "Daniel Dijkman"], "title": "Hybrid Training for Vision-Language-Action Models", "comment": null, "summary": "Using Large Language Models to produce intermediate thoughts, a.k.a.\nChain-of-thought (CoT), before providing an answer has been a successful recipe\nfor solving complex language tasks. In robotics, similar embodied CoT\nstrategies, generating thoughts before actions, have also been shown to lead to\nimproved performance when using Vision-Language-Action models (VLAs). As these\ntechniques increase the length of the model's generated outputs to include the\nthoughts, the inference time is negatively affected. Delaying an agent's\nactions in real-world executions, as in robotic manipulation settings, strongly\naffects the usability of a method, as tasks require long sequences of actions.\nHowever, is the generation of long chains-of-thought a strong prerequisite for\nachieving performance improvements? In this work, we explore the idea of Hybrid\nTraining (HyT), a framework that enables VLAs to learn from thoughts and\nbenefit from the associated performance gains, while enabling the possibility\nto leave out CoT generation during inference. Furthermore, by learning to\nconditionally predict a diverse set of outputs, HyT supports flexibility at\ninference time, enabling the model to either predict actions directly, generate\nthoughts or follow instructions. We evaluate the proposed method in a series of\nsimulated benchmarks and real-world experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u8bad\u7ec3\u6846\u67b6\uff08HyT\uff09\uff0c\u7528\u4e8e\u63d0\u9ad8\u89c6\u89c9\u8bed\u8a00\u884c\u52a8\u6a21\u578b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u907f\u514d\u5728\u63a8\u7406\u65f6\u751f\u6210\u5197\u957f\u7684\u601d\u7ef4\u94fe\uff0c\u4ece\u800c\u589e\u5f3a\u5b9e\u9645\u5e94\u7528\u7684\u53ef\u7528\u6027\u3002", "motivation": "\u5728\u590d\u6742\u8bed\u8a00\u4efb\u52a1\u4e2d\uff0c\u751f\u6210\u4e2d\u95f4\u601d\u7ef4\uff08\u601d\u7ef4\u94fe\uff09\u88ab\u8bc1\u660e\u6709\u52a9\u4e8e\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u5728\u673a\u5668\u4eba\u8bbe\u7f6e\u4e2d\uff0c\u751f\u6210\u5197\u957f\u601d\u7ef4\u94fe\u53ef\u80fd\u4f1a\u5f71\u54cd\u5b9e\u9645\u5e94\u7528\u7684\u53ef\u7528\u6027\u3002", "method": "\u4ecb\u7ecd\u4e86\u6df7\u5408\u8bad\u7ec3\uff08HyT\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5141\u8bb8\u89c6\u89c9\u8bed\u8a00\u884c\u52a8\u6a21\u578b\u4ece\u601d\u7ef4\u4e2d\u5b66\u4e60\uff0c\u4ee5\u83b7\u5f97\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u53ef\u4ee5\u9009\u62e9\u4e0d\u751f\u6210\u601d\u7ef4\u94fe\u3002", "result": "\u5728\u6a21\u62df\u57fa\u51c6\u6d4b\u8bd5\u548c\u73b0\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86HyT\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u63d0\u9ad8\u6027\u80fd\u5e76\u63d0\u4f9b\u63a8\u7406\u7075\u6d3b\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u8bad\u7ec3\uff08HyT\uff09\u6846\u67b6\u80fd\u591f\u6539\u5584\u89c6\u89c9\u8bed\u8a00\u884c\u52a8\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5728\u63a8\u7406\u65f6\u907f\u514d\u751f\u6210\u5197\u957f\u7684\u601d\u7ef4\u94fe\uff0c\u4ece\u800c\u63d0\u9ad8\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.00555", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00555", "abs": "https://arxiv.org/abs/2510.00555", "authors": ["Niklas Gutheil", "Valentin Mayer", "Leopold M\u00fcller", "J\u00f6rg Rommelt", "Niklas K\u00fchl"], "title": "PromptPilot: Improving Human-AI Collaboration Through LLM-Enhanced Prompt Engineering", "comment": "Preprint version. Accepted for presentation at the International\n  Conference on Information Systems (ICIS 2025). Please cite the published\n  version when available", "summary": "Effective prompt engineering is critical to realizing the promised\nproductivity gains of large language models (LLMs) in knowledge-intensive\ntasks. Yet, many users struggle to craft prompts that yield high-quality\noutputs, limiting the practical benefits of LLMs. Existing approaches, such as\nprompt handbooks or automated optimization pipelines, either require\nsubstantial effort, expert knowledge, or lack interactive guidance. To address\nthis gap, we design and evaluate PromptPilot, an interactive prompting\nassistant grounded in four empirically derived design objectives for\nLLM-enhanced prompt engineering. We conducted a randomized controlled\nexperiment with 80 participants completing three realistic, work-related\nwriting tasks. Participants supported by PromptPilot achieved significantly\nhigher performance (median: 78.3 vs. 61.7; p = .045, d = 0.56), and reported\nenhanced efficiency, ease-of-use, and autonomy during interaction. These\nfindings empirically validate the effectiveness of our proposed design\nobjectives, establishing LLM-enhanced prompt engineering as a viable technique\nfor improving human-AI collaboration.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faPromptPilot\uff0c\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u63d0\u793a\u52a9\u624b\uff0c\u65e8\u5728\u6539\u5584\u7528\u6237\u7684\u63d0\u793a\u5de5\u7a0b\u80fd\u529b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u63d0\u5347\u4efb\u52a1\u8868\u73b0\u548c\u7528\u6237\u4f53\u9a8c\u65b9\u9762\u6709\u6548\u3002", "motivation": "\u8bb8\u591a\u7528\u6237\u5728\u7f16\u5199\u6709\u6548\u63d0\u793a\u65f6\u9762\u4e34\u56f0\u96be\uff0c\u9650\u5236\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u9645\u6548\u76ca\u3002", "method": "\u901a\u8fc7\u968f\u673a\u5bf9\u7167\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u53c2\u4e0e\u8005\u5728\u4f7f\u7528PromptPilot\u65f6\u7684\u8868\u73b0\u3002", "result": "\u4f7f\u7528PromptPilot\u7684\u53c2\u4e0e\u8005\u8868\u73b0\u663e\u8457\u63d0\u9ad8\uff0c\u4e14\u53cd\u9988\u6548\u7387\u9ad8\u3001\u6613\u4e8e\u4f7f\u7528\u548c\u81ea\u4e3b\u6027\u5f3a\u3002", "conclusion": "\u57fa\u4e8eLLM\u7684\u63d0\u793a\u5de5\u7a0b\u88ab\u786e\u7acb\u4e3a\u6539\u5584\u4eba\u7c7b\u4e0eAI\u5408\u4f5c\u7684\u53ef\u884c\u6280\u672f\u3002"}}
{"id": "2510.00619", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00619", "abs": "https://arxiv.org/abs/2510.00619", "authors": ["Michiel Braat", "Maren Buermann", "Marijke van Weperen", "Jan-Pieter Paardekooper"], "title": "What Did I Learn? Operational Competence Assessment for AI-Based Trajectory Planners", "comment": "Accepted for publication in proceedings of the 2025 IEEE\n  International Automated Vehicle Validation Conference", "summary": "Automated driving functions increasingly rely on machine learning for tasks\nlike perception and trajectory planning, requiring large, relevant datasets.\nThe performance of these algorithms depends on how closely the training data\nmatches the task. To ensure reliable functioning, it is crucial to know what is\nincluded in the dataset to assess the trained model's operational risk. We aim\nto enhance the safe use of machine learning in automated driving by developing\na method to recognize situations that an automated vehicle has not been\nsufficiently trained on. This method also improves explainability by describing\nthe dataset at a human-understandable level. We propose modeling driving data\nas knowledge graphs, representing driving scenes with entities and their\nrelationships. These graphs are queried for specific sub-scene configurations\nto check their occurrence in the dataset. We estimate a vehicle's competence in\na driving scene by considering the coverage and complexity of sub-scene\nconfigurations in the training set. Higher complexity scenes require greater\ncoverage for high competence. We apply this method to the NuPlan dataset,\nmodeling it with knowledge graphs and analyzing the coverage of specific\ndriving scenes. This approach helps monitor the competence of machine learning\nmodels trained on the dataset, which is essential for trustworthy AI to be\ndeployed in automated driving.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u8bc6\u522b\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u672a\u5145\u5206\u8bad\u7ec3\u573a\u666f\u7684\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u5b66\u4e60\u7684\u5b89\u5168\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u786e\u4fdd\u81ea\u52a8\u9a7e\u9a76\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u53ef\u9760\u6027\u9700\u8981\u4e86\u89e3\u6570\u636e\u96c6\u4e2d\u5305\u542b\u7684\u5185\u5bb9\uff0c\u4ee5\u8bc4\u4f30\u8bad\u7ec3\u6a21\u578b\u7684\u64cd\u4f5c\u98ce\u9669\u3002", "method": "\u5c06\u9a7e\u9a76\u6570\u636e\u5efa\u6a21\u4e3a\u77e5\u8bc6\u56fe\u8c31\uff0c\u4ee5\u5b9e\u4f53\u53ca\u5176\u5173\u7cfb\u8868\u793a\u9a7e\u9a76\u573a\u666f\uff0c\u5e76\u67e5\u8be2\u7279\u5b9a\u5b50\u573a\u666f\u914d\u7f6e\u4ee5\u68c0\u67e5\u5176\u5728\u6570\u636e\u96c6\u4e2d\u7684\u51fa\u73b0\u60c5\u51b5\u3002", "result": "\u901a\u8fc7\u5728NuPlan\u6570\u636e\u96c6\u4e0a\u5e94\u7528\u8be5\u65b9\u6cd5\uff0c\u6211\u4eec\u5efa\u6a21\u4e86\u77e5\u8bc6\u56fe\u8c31\u5e76\u5206\u6790\u4e86\u7279\u5b9a\u9a7e\u9a76\u573a\u666f\u7684\u8986\u76d6\u60c5\u51b5\uff0c\u4ece\u800c\u76d1\u63a7\u8bad\u7ec3\u6a21\u578b\u7684\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u5efa\u6a21\u9a7e\u9a76\u6570\u636e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u9ad8\u4e86\u5bf9\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u672a\u5145\u5206\u8bad\u7ec3\u573a\u666f\u4e0b\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u673a\u5668\u5b66\u4e60\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5b89\u5168\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2510.00583", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.00583", "abs": "https://arxiv.org/abs/2510.00583", "authors": ["Xinyang Shan", "Yuanyuan Xu", "Tian Xia", "Yinshan Lin"], "title": "Rethinking Wine Tasting for Chinese Consumers: A Service Design Approach Enhanced by Multimodal Personalization", "comment": null, "summary": "Wine tasting is a multimodal and culturally embedded activity that presents\nunique challenges when adapted to non-Western contexts. This paper proposes a\nservice design approach rooted in contextual co-creation to reimagine wine\ntasting experiences for Chinese consumers. Drawing on 26 in-situ interviews and\nfollow-up validation sessions, we identify three distinct user archetypes:\nCurious Tasters, Experience Seekers, and Knowledge Builders, each exhibiting\ndifferent needs in vocabulary, interaction, and emotional pacing. Our findings\nreveal that traditional wine descriptors lack cultural resonance and that\ncross-modal metaphors grounded in local gastronomy (e.g., green mango for\nacidity) significantly improve cognitive and emotional engagement. These\ninsights informed a partially implemented prototype, featuring AI-driven\nmetaphor-to-flavour mappings and real-time affective feedback visualisation. A\nsmall-scale usability evaluation confirmed improvements in engagement and\ncomprehension. Our comparative analysis shows alignment with and\ndifferentiation from prior multimodal and affect-aware tasting systems. This\nresearch contributes to CBMI by demonstrating how culturally adaptive\ninteraction systems can enhance embodied consumption experiences in physical\ntourism and beyond.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u5982\u4f55\u901a\u8fc7\u6587\u5316\u9002\u5e94\u6027\u7684\u670d\u52a1\u8bbe\u8ba1\u589e\u5f3a\u4e2d\u56fd\u6d88\u8d39\u8005\u7684\u8461\u8404\u9152\u54c1\u5c1d\u4f53\u9a8c\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u4ea4\u4e92\u7cfb\u7edf\u4e0e\u7528\u6237\u539f\u578b\u3002", "motivation": "\u5728\u975e\u897f\u65b9\u6587\u5316\u80cc\u666f\u4e0b\uff0c\u8461\u8404\u9152\u54c1\u5c1d\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u9700\u8981\u91cd\u65b0\u6784\u60f3\u4f53\u9a8c\u4ee5\u6ee1\u8db3\u4e2d\u56fd\u6d88\u8d39\u8005\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u670d\u52a1\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u57fa\u4e8e\u4e0a\u4e0b\u6587\u5171\u540c\u521b\u9020\uff0c\u8fdb\u884c26\u6b21\u73b0\u573a\u8bbf\u8c08\u548c\u540e\u7eed\u9a8c\u8bc1\u3002", "result": "\u786e\u5b9a\u4e86\u4e09\u79cd\u7528\u6237\u539f\u578b\uff0c\u63ed\u793a\u4e86\u4f20\u7edf\u8461\u8404\u9152\u63cf\u8ff0\u7684\u6587\u5316\u4e0d\u9002\u5b9c\u6027\uff0c\u4ee5\u53ca\u57fa\u4e8e\u5730\u65b9\u7f8e\u98df\u7684\u4ea4\u53c9\u9690\u55bb\u663e\u8457\u63d0\u5347\u4e86\u8ba4\u77e5\u4e0e\u60c5\u611f\u53c2\u4e0e\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u6587\u5316\u9002\u5e94\u7684\u4ea4\u4e92\u7cfb\u7edf\u589e\u5f3a\u8eab\u4e34\u5176\u5883\u7684\u6d88\u8d39\u4f53\u9a8c\uff0c\u5c24\u5176\u662f\u5728\u975e\u897f\u65b9\u80cc\u666f\u7684\u8461\u8404\u9152\u54c1\u5c1d\u4e2d\u3002"}}
{"id": "2510.00630", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.00630", "abs": "https://arxiv.org/abs/2510.00630", "authors": ["Federico Oliva", "Tom Shaked", "Daniele Carnevale", "Amir Degani"], "title": "Trajectory Based Observer Design: A Framework for Lightweight Sensor Fusion", "comment": null, "summary": "Efficient observer design and accurate sensor fusion are key in state\nestimation. This work proposes an optimization-based methodology, termed\nTrajectory Based Optimization Design (TBOD), allowing the user to easily design\nobservers for general nonlinear systems and multi-sensor setups. Starting from\nparametrized observer dynamics, the proposed method considers a finite set of\npre-recorded measurement trajectories from the nominal plant and exploits them\nto tune the observer parameters through numerical optimization. This research\nhinges on the classic observer's theory and Moving Horizon Estimators\nmethodology. Optimization is exploited to ease the observer's design, providing\nthe user with a lightweight, general-purpose sensor fusion methodology. TBOD's\nmain characteristics are the capability to handle general sensors efficiently\nand in a modular way and, most importantly, its straightforward tuning\nprocedure. The TBOD's performance is tested on a terrestrial rover localization\nproblem, combining IMU and ranging sensors provided by Ultra Wide Band\nantennas, and validated through a motion-capture system. Comparison with an\nExtended Kalman Filter is also provided, matching its position estimation\naccuracy and significantly improving in the orientation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u4f18\u5316\u7684\u89c2\u5bdf\u8005\u8bbe\u8ba1\u65b9\u6cd5TBOD\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u975e\u7ebf\u6027\u7cfb\u7edf\u548c\u591a\u4f20\u611f\u5668\u73af\u5883\uff0c\u6027\u80fd\u5728\u5730\u9762\u673a\u5668\u4eba\u5b9a\u4f4d\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u63d0\u9ad8\u72b6\u6001\u4f30\u8ba1\u4e2d\u89c2\u5bdf\u8005\u8bbe\u8ba1\u7684\u6548\u7387\u548c\u4f20\u611f\u5668\u878d\u5408\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u590d\u6742\u7cfb\u7edf\u4e2d\u3002", "method": "\u901a\u8fc7\u4f18\u5316\u8bbe\u8ba1\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u9884\u5148\u8bb0\u5f55\u7684\u6d4b\u91cf\u8f68\u8ff9\u8c03\u6574\u89c2\u5bdf\u8005\u53c2\u6570\uff0c\u9002\u7528\u4e8e\u975e\u7ebf\u6027\u7cfb\u7edf\u548c\u591a\u4f20\u611f\u5668\u8bbe\u7f6e\u3002", "result": "TBOD\u5728\u7ed3\u5408IMU\u548c\u8d85\u5bbd\u5e26\u5929\u7ebf\u4f20\u611f\u5668\u7684\u5730\u9762\u673a\u5668\u4eba\u5b9a\u4f4d\u95ee\u9898\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\uff0c\u4e14\u5728\u65b9\u5411\u4f30\u8ba1\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "TBOD\u65b9\u6cd5\u5728\u5730\u9762\u673a\u5668\u4eba\u5b9a\u4f4d\u4e2d\u7684\u8868\u73b0\u4f18\u4e8e\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff0c\u5c24\u5176\u5728\u65b9\u5411\u4f30\u8ba1\u4e0a\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2510.00607", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.00607", "abs": "https://arxiv.org/abs/2510.00607", "authors": ["Xinyang Shan", "Yuanyuan Xu", "Yuqing Wang", "Tian Xia", "Yinshan Lin"], "title": "Designing Wine Tasting Experiences for All: The role of Human Diversity and Personal food memory", "comment": null, "summary": "This study investigates the design of inclusive wine-tasting experiences by\nexamining the roles of human diversity and personal food memory. Through field\nstudies conducted in various wine regions, we explored how Chinese visitors\nengage with wine-tasting activities during winery tours, highlighting the\ncross-cultural challenges they face. Our findings underscore the importance of\nexperiencers' abilities, necessities, and aspirations (ANAs), the authenticity\nof wine tasting within the context of winery tours, and the use of personal\nfood memories as a wine-tasting tool accessible to all. These insights lay the\ngroundwork for developing more inclusive and engaging wine-tasting services,\noffering new perspectives for cultural exchange and sustainable wine business\npractices in China.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u4eba\u7c7b\u591a\u6837\u6027\u548c\u4e2a\u4eba\u98df\u54c1\u8bb0\u5fc6\u8bbe\u8ba1\u66f4\u5177\u5305\u5bb9\u6027\u7684\u9152\u5e84\u54c1\u9152\u4f53\u9a8c\uff0c\u7279\u522b\u5173\u6ce8\u4e2d\u56fd\u6e38\u5ba2\u7684\u6587\u5316\u4ea4\u6d41\u4e0e\u53ef\u6301\u7eed\u53d1\u5c55\u3002", "motivation": "\u63a2\u7d22\u4eba\u7c7b\u591a\u6837\u6027\u548c\u4e2a\u4eba\u98df\u54c1\u8bb0\u5fc6\u5bf9\u9152\u5e84\u54c1\u9152\u4f53\u9a8c\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u5173\u6ce8\u4e2d\u56fd\u6e38\u5ba2\u7684\u8de8\u6587\u5316\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5728\u591a\u4e2a\u8461\u8404\u9152\u4ea7\u533a\u8fdb\u884c\u5b9e\u5730\u7814\u7a76\uff0c\u8c03\u67e5\u4e2d\u56fd\u6e38\u5ba2\u5728\u9152\u5e84\u6e38\u89c8\u4e2d\u7684\u54c1\u9152\u4f53\u9a8c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4f53\u9a8c\u8005\u7684\u80fd\u529b\u3001\u9700\u6c42\u548c\u613f\u671b\uff08ANAs\uff09\u3001\u9152\u5e84\u6e38\u89c8\u4e2d\u7684\u54c1\u9152\u771f\u5b9e\u6027\uff0c\u4ee5\u53ca\u4e2a\u4eba\u98df\u54c1\u8bb0\u5fc6\u4f5c\u4e3a\u54c1\u9152\u5de5\u5177\u7684\u4ef7\u503c\u90fd\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u4e2d\u56fd\u7684\u9152\u5e84\u4f53\u9a8c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u66f4\u5177\u5305\u5bb9\u6027\u548c\u53c2\u4e0e\u6027\u7684\u89c6\u89d2\uff0c\u5f3a\u8c03\u4e86\u4eba\u7c7b\u591a\u6837\u6027\u548c\u4e2a\u4eba\u98df\u54c1\u8bb0\u5fc6\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.00646", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.00646", "abs": "https://arxiv.org/abs/2510.00646", "authors": ["Haoyang Wang", "Xinyu Luo", "Wenhua Ding", "Jingao Xu", "Xuecheng Chen", "Ruiyang Duan", "Jialong Chen", "Haitao Zhang", "Yunhao Liu", "Xinlei Chen"], "title": "Enabling High-Frequency Cross-Modality Visual Positioning Service for Accurate Drone Landing", "comment": "15 pages, 23 figures", "summary": "After years of growth, drone-based delivery is transforming logistics. At its\ncore, real-time 6-DoF drone pose tracking enables precise flight control and\naccurate drone landing. With the widespread availability of urban 3D maps, the\nVisual Positioning Service (VPS), a mobile pose estimation system, has been\nadapted to enhance drone pose tracking during the landing phase, as\nconventional systems like GPS are unreliable in urban environments due to\nsignal attenuation and multi-path propagation. However, deploying the current\nVPS on drones faces limitations in both estimation accuracy and efficiency. In\nthis work, we redesign drone-oriented VPS with the event camera and introduce\nEV-Pose to enable accurate, high-frequency 6-DoF pose tracking for accurate\ndrone landing. EV-Pose introduces a spatio-temporal feature-instructed pose\nestimation module that extracts a temporal distance field to enable 3D point\nmap matching for pose estimation; and a motion-aware hierarchical fusion and\noptimization scheme to enhance the above estimation in accuracy and efficiency,\nby utilizing drone motion in the \\textit{early stage} of event filtering and\nthe \\textit{later stage} of pose optimization. Evaluation shows that EV-Pose\nachieves a rotation accuracy of 1.34$\\degree$ and a translation accuracy of\n6.9$mm$ with a tracking latency of 10.08$ms$, outperforming baselines by\n$>$50\\%, \\tmcrevise{thus enabling accurate drone landings.} Demo:\nhttps://ev-pose.github.io/", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faEV-Pose\uff0c\u901a\u8fc7\u7ed3\u5408\u4e8b\u4ef6\u76f8\u673a\u548c\u65b0\u7684\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u65e0\u4eba\u673a\u5728\u57ce\u5e02\u73af\u5883\u4e0b\u7684\u7740\u9646\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u65e0\u4eba\u673a\u5728\u57ce\u5e02\u73af\u5883\u4e0b\u7684\u59ff\u6001\u8ddf\u8e2a\u4f20\u7edf\u7cfb\u7edf\u4e0d\u53ef\u9760\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u65e0\u4eba\u673a\u7740\u9646\u65f6\u7684\u59ff\u6001\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "method": "\u91cd\u8bbe\u8ba1\u89c6\u89c9\u5b9a\u4f4d\u670d\u52a1\uff0c\u7ed3\u5408\u4e8b\u4ef6\u76f8\u673a\uff0c\u63d0\u51faEV-Pose\uff0c\u5305\u62ec\u65f6\u7a7a\u7279\u5f81\u6307\u5bfc\u7684\u59ff\u6001\u4f30\u8ba1\u6a21\u5757\u548c\u8fd0\u52a8\u611f\u77e5\u7684\u5206\u5c42\u878d\u5408\u4f18\u5316\u65b9\u6848\u3002", "result": "\u901a\u8fc7\u91c7\u7528EV-Pose\u7684\u521b\u65b0\uff0c\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u5728\u7740\u9646\u9636\u6bb5\u7684\u59ff\u6001\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "conclusion": "EV-Pose\u663e\u8457\u63d0\u9ad8\u4e86\u65e0\u4eba\u673a\u7740\u9646\u7684\u51c6\u786e\u6027\uff0c\u65cb\u8f6c\u7cbe\u5ea6\u4e3a1.34\u5ea6\uff0c\u5e73\u79fb\u7cbe\u5ea6\u4e3a6.9\u6beb\u7c73\uff0c\u8ddf\u8e2a\u5ef6\u8fdf\u4e3a10.08\u6beb\u79d2\uff0c\u4f18\u4e8e\u57fa\u7ebf\u7cfb\u7edf\u8d85\u8fc750%\u3002"}}
{"id": "2510.00738", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.00738", "abs": "https://arxiv.org/abs/2510.00738", "authors": ["Helen Schneider", "Svetlana Pavlitska", "Helen Gremmelmaier", "J. Marius Z\u00f6llner"], "title": "Datasets for Valence and Arousal Inference: A Survey", "comment": "Accepted for publication at ABAW Workshop at CVPR2025", "summary": "Understanding human affect can be used in robotics, marketing, education,\nhuman-computer interaction, healthcare, entertainment, autonomous driving, and\npsychology to enhance decision-making, personalize experiences, and improve\nemotional well-being. This work presents a comprehensive overview of affect\ninference datasets that utilize continuous valence and arousal labels. We\nreviewed 25 datasets published between 2008 and 2024, examining key factors\nsuch as dataset size, subject distribution, sensor configurations, annotation\nscales, and data formats for valence and arousal values. While camera-based\ndatasets dominate the field, we also identified several widely used multimodal\ncombinations. Additionally, we explored the most common approaches to affect\ndetection applied to these datasets, providing insights into the prevailing\nmethodologies in the field. Our overview of sensor fusion approaches shows\npromising advancements in model improvement for valence and arousal inference.", "AI": {"tldr": "\u672c\u7814\u7a76\u6982\u8ff0\u4e8625\u4e2a\u60c5\u611f\u63a8\u65ad\u6570\u636e\u96c6\u7684\u7279\u5f81\uff0c\u5e76\u5206\u6790\u4e86\u60c5\u611f\u68c0\u6d4b\u7684\u65b9\u6cd5\u548c\u6280\u672f\u8fdb\u5c55\u3002", "motivation": "\u7406\u89e3\u4eba\u7c7b\u60c5\u611f\u53ef\u4ee5\u5728\u591a\u4e2a\u9886\u57df\u63d0\u5347\u51b3\u7b56\u3001\u4e2a\u6027\u5316\u4f53\u9a8c\u548c\u6539\u5584\u60c5\u611f\u5065\u5eb7\u3002", "method": "\u5bf92008\u81f32024\u5e74\u95f4\u53d1\u5e03\u768425\u4e2a\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u5ba1\u67e5\uff0c\u5206\u6790\u4e86\u6570\u636e\u96c6\u5927\u5c0f\u3001\u88ab\u8bd5\u5206\u5e03\u3001\u4f20\u611f\u5668\u914d\u7f6e\u3001\u6ce8\u91ca\u89c4\u6a21\u548c\u6570\u636e\u683c\u5f0f\u7b49\u5173\u952e\u56e0\u7d20\u3002", "result": "\u6211\u4eec\u603b\u7ed3\u4e86\u60c5\u611f\u68c0\u6d4b\u7684\u4e3b\u8981\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u4f20\u611f\u5668\u878d\u5408\u65b9\u6cd5\u5728\u60c5\u611f\u63a8\u65ad\u4e0a\u7684\u8fdb\u5c55\u3002", "conclusion": "\u60c5\u611f\u63a8\u65ad\u6570\u636e\u96c6\u7684\u5168\u9762\u89c6\u89d2\u80fd\u591f\u4e3a\u60c5\u611f\u68c0\u6d4b\u9886\u57df\u7684\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u6307\u5bfc\u3002"}}
{"id": "2510.00682", "categories": ["cs.RO", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.00682", "abs": "https://arxiv.org/abs/2510.00682", "authors": ["Shengzhi Wang", "Niels Dehio", "Xuanqi Zeng", "Xian Yang", "Lingwei Zhang", "Yun-Hui Liu", "K. W. Samuel Au"], "title": "Shared Object Manipulation with a Team of Collaborative Quadrupeds", "comment": "8 pages, 9 figures, submitted to The 2026 American Control Conference", "summary": "Utilizing teams of multiple robots is advantageous for handling bulky\nobjects. Many related works focus on multi-manipulator systems, which are\nlimited by workspace constraints. In this paper, we extend a classical hybrid\nmotion-force controller to a team of legged manipulator systems, enabling\ncollaborative loco-manipulation of rigid objects with a force-closed grasp. Our\nnovel approach allows the robots to flexibly coordinate their movements,\nachieving efficient and stable object co-manipulation and transport, validated\nthrough extensive simulations and real-world experiments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a7\u5236\u65b9\u6cd5\uff0c\u5141\u8bb8\u591a\u8db3\u673a\u5668\u4eba\u56e2\u961f\u9ad8\u6548\u3001\u7a33\u5b9a\u5730\u534f\u4f5c\u5904\u7406\u548c\u8fd0\u8f93\u786c\u7269\u4f53\uff0c\u514b\u670d\u4e86\u73b0\u6709\u591a\u64cd\u7eb5\u7cfb\u7edf\u7684\u5de5\u4f5c\u7a7a\u95f4\u9650\u5236\u3002", "motivation": "\u5229\u7528\u591a\u4e2a\u673a\u5668\u4eba\u7684\u56e2\u961f\u5bf9\u5927\u578b\u7269\u4f53\u8fdb\u884c\u5904\u7406\uff0c\u907f\u514d\u4e86\u591a\u64cd\u7eb5\u7cfb\u7edf\u5728\u5de5\u4f5c\u7a7a\u95f4\u4e0a\u7684\u9650\u5236\u3002", "method": "\u6269\u5c55\u7ecf\u5178\u7684\u6df7\u5408\u8fd0\u52a8-\u529b\u63a7\u5236\u5668\u81f3\u591a\u8db3\u64cd\u7eb5\u7cfb\u7edf\uff0c\u8fdb\u884c\u534f\u4f5c\u7684\u6d3b\u52a8-\u64cd\u63a7\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u7684\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u673a\u5668\u4eba\u7684\u534f\u8c03\u8fd0\u52a8\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u6211\u4eec\u7684\u65b0\u65b9\u6cd5\uff0c\u591a\u4e2a\u673a\u5668\u4eba\u80fd\u591f\u9ad8\u6548\u3001\u7a33\u5b9a\u5730\u5171\u540c\u64cd\u63a7\u548c\u8fd0\u8f93\u521a\u6027\u7269\u4f53\u3002"}}
{"id": "2510.00824", "categories": ["cs.HC", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.00824", "abs": "https://arxiv.org/abs/2510.00824", "authors": ["Xiaoye Michael Wang", "Ali Mazalek", "Catherine M. Sabiston", "Timothy N. Welsh"], "title": "Virtual Reality Alters Perceived Functional Body Size", "comment": null, "summary": "Virtual reality (VR) introduces sensory perturbations that may impact\nperception and action. The current study was designed to investigate how\nimmersive VR presented through a head-mounted display (HMD) affects perceived\nfunctional body size using a passable aperture paradigm. Participants (n=60)\nperformed an action task (sidle through apertures) and a perception task\n(adjust aperture width until passable without contact) in both physical,\nunmediated reality (UR) and VR. Results revealed significantly higher action\nand perceptual thresholds in VR compared to UR. Affordance ratios (perceptual\nthreshold over action threshold) were also higher in VR, indicating that the\nincrease in perceptual thresholds in VR was driven partly by sensorimotor\nuncertainty, as reflected in the increase in the action thresholds, and partly\nby perceptual distortions imposed by VR. This perceptual overestimation in VR\nalso persisted as an aftereffect in UR following VR exposure. Geometrical\nmodelling attributed the disproportionate increase in the perceptual threshold\nin VR primarily to depth compression. This compression, stemming from the\nvergence-accommodation conflict (VAC), caused the virtual aperture to be\nperceived as narrower than depicted, thus requiring a wider adjusted aperture.\nCritically, after mathematically correcting for the VAC's impact on perceived\naperture width, the affordance ratios in VR became equivalent to those in UR.\nThese outcomes demonstrate a recovered invariant geometrical scaling,\nsuggesting that perception remained functionally attuned to action capabilities\nonce VAC-induced distortions were accounted for. These findings highlight that\nVR-induced depth compression systematically alters perceived body-environment\nrelationships, leading to an altered sense of one's functional body size.", "AI": {"tldr": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u865a\u62df\u73b0\u5b9e\u5982\u4f55\u901a\u8fc7\u6df1\u5ea6\u538b\u7f29\u5f71\u54cd\u4e2a\u4f53\u5bf9\u529f\u80fd\u8eab\u4f53\u5927\u5c0f\u7684\u611f\u77e5\uff0c\u5bfc\u81f4\u611f\u77e5\u548c\u52a8\u4f5c\u9608\u503c\u7684\u589e\u52a0\u3002", "motivation": "\u63a2\u8ba8\u6c89\u6d78\u5f0f\u865a\u62df\u73b0\u5b9e\u5982\u4f55\u901a\u8fc7\u611f\u89c9\u5e72\u6270\u5f71\u54cd\u4e2a\u4f53\u5bf9\u4e8e\u81ea\u5df1\u8eab\u4f53\u5927\u5c0f\u7684\u8ba4\u77e5\u3002", "method": "\u91c7\u7528\u4f20\u9012\u5b54\u5f84\u8303\u5f0f\uff0c\u901a\u8fc7\u5934\u6234\u663e\u793a\u5668(HMD)\u8fdb\u884c\u6c89\u6d78\u5f0f\u865a\u62df\u73b0\u5b9e\u4f53\u9a8c\uff0c\u6d4b\u8bd5\u53c2\u4e0e\u8005\u5728\u4f53\u80b2\u548c\u865a\u62df\u73b0\u5b9e\u4e2d\u7684\u52a8\u4f5c\u4efb\u52a1\u548c\u611f\u77e5\u4efb\u52a1\u3002", "result": "\u5728\u865a\u62df\u73b0\u5b9e\u4e2d\uff0c\u53c2\u4e0e\u8005\u7684\u52a8\u4f5c\u548c\u611f\u77e5\u9608\u503c\u663e\u8457\u9ad8\u4e8e\u7269\u7406\u73b0\u5b9e\uff0c\u8868\u660e\u611f\u77e5\u548c\u52a8\u4f5c\u4e4b\u95f4\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u548c\u89c6\u89c9\u5931\u771f\u3002\u540c\u65f6\uff0c\u5728\u865a\u62df\u73b0\u5b9e\u4e2d\uff0c\u7ecf\u8fc7\u6570\u5b66\u4fee\u6b63\u540e\uff0c\u529f\u80fd\u6027\u6bd4\u4f8b\u4e0e\u7269\u7406\u73b0\u5b9e\u4e00\u81f4\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u865a\u62df\u73b0\u5b9e\u5bfc\u81f4\u7684\u6df1\u5ea6\u538b\u7f29\u7cfb\u7edf\u6027\u5730\u6539\u53d8\u4e86\u611f\u77e5\u7684\u8eab\u4f53-\u73af\u5883\u5173\u7cfb\uff0c\u4ece\u800c\u6539\u53d8\u4e86\u4e2a\u4f53\u7684\u529f\u80fd\u8eab\u4f53\u5927\u5c0f\u611f\u77e5\u3002"}}
{"id": "2510.00695", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.00695", "abs": "https://arxiv.org/abs/2510.00695", "authors": ["Myungkyu Koo", "Daewon Choi", "Taeyoung Kim", "Kyungmin Lee", "Changyeon Kim", "Youngyo Seo", "Jinwoo Shin"], "title": "HAMLET: Switch your Vision-Language-Action Model into a History-Aware Policy", "comment": "Project page: https://myungkyukoo.github.io/hamlet/", "summary": "Inherently, robotic manipulation tasks are history-dependent: leveraging past\ncontext could be beneficial. However, most existing Vision-Language-Action\nmodels (VLAs) have been designed without considering this aspect, i.e., they\nrely solely on the current observation, ignoring preceding context. In this\npaper, we propose HAMLET, a scalable framework to adapt VLAs to attend to the\nhistorical context during action prediction. Specifically, we introduce moment\ntokens that compactly encode perceptual information at each timestep. Their\nrepresentations are initialized with time-contrastive learning, allowing them\nto better capture temporally distinctive aspects. Next, we employ a lightweight\nmemory module that integrates the moment tokens across past timesteps into\nmemory features, which are then leveraged for action prediction. Through\nempirical evaluation, we show that HAMLET successfully transforms a\nstate-of-the-art VLA into a history-aware policy, especially demonstrating\nsignificant improvements on long-horizon tasks that require historical context.\nIn particular, on top of GR00T N1.5, HAMLET achieves an average success rate of\n76.4% on history-dependent real-world tasks, surpassing the baseline\nperformance by 47.2%. Furthermore, HAMLET pushes prior art performance from\n64.1% to 66.4% on RoboCasa Kitchen (100-demo setup) and from 95.6% to 97.7% on\nLIBERO, highlighting its effectiveness even under generic robot-manipulation\nbenchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86HAMLET\u6846\u67b6\uff0c\u6210\u529f\u63d0\u5347\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5bf9\u5386\u53f2\u4e0a\u4e0b\u6587\u7684\u5229\u7528\uff0c\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u672a\u8003\u8651\u5386\u53f2\u4e0a\u4e0b\u6587\u7684\u4f9d\u8d56\u6027\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u5f15\u5165\u4e86\u65f6\u523b\u4ee4\u724c\u548c\u8f7b\u91cf\u7ea7\u5185\u5b58\u6a21\u5757\uff0c\u901a\u8fc7\u65f6\u95f4\u5bf9\u6bd4\u5b66\u4e60\u53ca\u8bb0\u5fc6\u7279\u5f81\u6574\u5408\u5386\u53f2\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4ee5\u8fdb\u884c\u52a8\u4f5c\u9884\u6d4b\u3002", "result": "HAMLET\u5728\u5386\u53f2\u4f9d\u8d56\u5b9e\u7528\u4efb\u52a1\u4e0a\u6210\u529f\u5b9e\u73b076.4%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u8d85\u8d8a\u4e86\u57fa\u7ebf\u6027\u80fd47.2%\u3002\u5728RoboCasa Kitchen\u548cLIBERO\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u4e5f\u8868\u73b0\u51fa\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "HAMLET\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u5386\u53f2\u4f9d\u8d56\u6027\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u5386\u53f2\u4e0a\u4e0b\u6587\u7684\u957f\u65f6\u95f4\u8de8\u5ea6\u4efb\u52a1\u4e0a\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u3002"}}
{"id": "2510.00909", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00909", "abs": "https://arxiv.org/abs/2510.00909", "authors": ["Alexandra Klymenko", "Stephen Meisenbacher", "Patrick Gage Kelley", "Sai Teja Peddinti", "Kurt Thomas", "Florian Matthes"], "title": "\"We are not Future-ready\": Understanding AI Privacy Risks and Existing Mitigation Strategies from the Perspective of AI Developers in Europe", "comment": "20 pages, 1 figure, 4 tables. Accepted to SOUPS 2025", "summary": "The proliferation of AI has sparked privacy concerns related to training\ndata, model interfaces, downstream applications, and more. We interviewed 25 AI\ndevelopers based in Europe to understand which privacy threats they believe\npose the greatest risk to users, developers, and businesses and what protective\nstrategies, if any, would help to mitigate them. We find that there is little\nconsensus among AI developers on the relative ranking of privacy risks. These\ndifferences stem from salient reasoning patterns that often relate to human\nrather than purely technical factors. Furthermore, while AI developers are\naware of proposed mitigation strategies for addressing these risks, they\nreported minimal real-world adoption. Our findings highlight both gaps and\nopportunities for empowering AI developers to better address privacy risks in\nAI.", "AI": {"tldr": "\u7814\u7a76\u663e\u793aAI\u5f00\u53d1\u8005\u5bf9\u9690\u79c1\u98ce\u9669\u7684\u8ba4\u8bc6\u4e0d\u4e00\u81f4\uff0c\u4e14\u5df2\u6709\u7684\u51cf\u8f7b\u7b56\u7565\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5f88\u5c11\u3002", "motivation": "\u968f\u7740AI\u7684\u53d1\u5c55\uff0c\u9690\u79c1\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u56e0\u6b64\u9700\u8981\u4e86\u89e3\u5f00\u53d1\u8005\u5bf9\u9690\u79c1\u5a01\u80c1\u7684\u770b\u6cd5\u53ca\u5e94\u5bf9\u63aa\u65bd\u3002", "method": "\u901a\u8fc7\u5bf925\u4f4d\u6b27\u6d32AI\u5f00\u53d1\u8005\u7684\u8bbf\u8c08\u6765\u7814\u7a76\u9690\u79c1\u5a01\u80c1\u53ca\u5176\u5e94\u5bf9\u7b56\u7565\u3002", "result": "AI\u5f00\u53d1\u8005\u5bf9\u9690\u79c1\u98ce\u9669\u7684\u76f8\u5bf9\u6392\u540d\u7f3a\u4e4f\u5171\u8bc6\uff0c\u4e14\u73b0\u5b9e\u4e2d\u5f88\u5c11\u91c7\u7528\u5df2\u6709\u7684\u51cf\u8f7b\u7b56\u7565\u3002", "conclusion": "AI\u5f00\u53d1\u8005\u5bf9\u9690\u79c1\u98ce\u9669\u7684\u8ba4\u8bc6\u4e0d\u4e00\u81f4\uff0c\u5e76\u4e14\u5c3d\u7ba1\u4e86\u89e3\u51cf\u8f7b\u7b56\u7565\uff0c\u5b9e\u9645\u5e94\u7528\u4ecd\u7136\u5f88\u5c11\u3002"}}
{"id": "2510.00703", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.00703", "abs": "https://arxiv.org/abs/2510.00703", "authors": ["Andrea Bussolan", "Stefano Baraldo", "Oliver Avram", "Pablo Urcola", "Luis Montesano", "Luca Maria Gambardella", "Anna Valente"], "title": "MultiPhysio-HRC: Multimodal Physiological Signals Dataset for industrial Human-Robot Collaboration", "comment": null, "summary": "Human-robot collaboration (HRC) is a key focus of Industry 5.0, aiming to\nenhance worker productivity while ensuring well-being. The ability to perceive\nhuman psycho-physical states, such as stress and cognitive load, is crucial for\nadaptive and human-aware robotics. This paper introduces MultiPhysio-HRC, a\nmultimodal dataset containing physiological, audio, and facial data collected\nduring real-world HRC scenarios. The dataset includes electroencephalography\n(EEG), electrocardiography (ECG), electrodermal activity (EDA), respiration\n(RESP), electromyography (EMG), voice recordings, and facial action units. The\ndataset integrates controlled cognitive tasks, immersive virtual reality\nexperiences, and industrial disassembly activities performed manually and with\nrobotic assistance, to capture a holistic view of the participants' mental\nstates. Rich ground truth annotations were obtained using validated\npsychological self-assessment questionnaires. Baseline models were evaluated\nfor stress and cognitive load classification, demonstrating the dataset's\npotential for affective computing and human-aware robotics research.\nMultiPhysio-HRC is publicly available to support research in human-centered\nautomation, workplace well-being, and intelligent robotic systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a8\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aMultiPhysio-HRC\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u65e8\u5728\u63d0\u9ad8\u4eba\u673a\u534f\u8c03\u6548\u7387\u5e76\u5173\u6ce8\u4eba\u7c7b\u798f\u7949\uff0c\u9002\u7528\u4e8e\u4eba\u673a\u534f\u4f5c\u548c\u667a\u80fd\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u7814\u7a76\u3002", "motivation": "\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u5de5\u4f5c\u6548\u7387\u548c\u4fdd\u969c\u4eba\u7c7b\u798f\u7949\u3002", "method": "\u901a\u8fc7\u6536\u96c6\u5728\u771f\u5b9eHRC\u573a\u666f\u4e2d\u7684\u751f\u7406\u3001\u97f3\u9891\u548c\u9762\u90e8\u6570\u636e\uff0c\u5c06\u6570\u636e\u96c6\u6784\u5efa\u4e3a\u591a\u6a21\u6001\u7684\u5f62\u5f0f\u3002", "result": "\u8be5\u6570\u636e\u96c6\u5c55\u793a\u4e86\u5176\u5728\u60c5\u611f\u8ba1\u7b97\u548c\u4eba\u7c7b\u611f\u77e5\u673a\u5668\u4eba\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u901a\u8fc7\u57fa\u7ebf\u6a21\u578b\u8bc4\u4f30\u4e86\u538b\u529b\u548c\u8ba4\u77e5\u8d1f\u8377\u5206\u7c7b\u3002", "conclusion": "MultiPhysio-HRC\u6570\u636e\u96c6\u4e3a\u7814\u7a76\u4eba\u673a\u4ea4\u4e92\u548c\u673a\u5668\u4eba\u667a\u80fd\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u652f\u6301\u3002"}}
{"id": "2510.00964", "categories": ["cs.HC", "cs.CY", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.00964", "abs": "https://arxiv.org/abs/2510.00964", "authors": ["Aakash Gautam", "Chandani Shrestha", "Deborah Tatar", "Steve Harrison"], "title": "Social Photo-Elicitation: The Use of Communal Production of Meaning to Hear a Vulnerable Population", "comment": "20 pages, 3 figures, published at CSCW 2018", "summary": "We report on an initial ethnographic exploration of the situation of\nsex-trafficking survivors in Nepal. In the course of studying trafficking\nsurvivors in a protected-living situation created by a non-governmental\norganization in Nepal, we adapted photo-elicitation to hear the voices of the\nsurvivors by making the technique more communal. Bringing sociality to the\nforefront of the method reduced the pressure on survivors to assert voices as\nindividuals, allowing them to speak. We make three contributions to research.\nFirst, we propose a communal form of photo-elicitation as a method to elicit\nvalues in sensitive settings. Second, we present the complex circumstances of\nthe survivors as they undergo rehabilitation and move towards life with a ``new\nnormal''. Third, our work adds to HCI and CSCW literature on understanding\nspecific concerns of trafficking survivors and aims to inform designs that can\nsupport reintegration of survivors in society. The values that the survivors\nhold and their notion of future opportunities suggest possession of limited but\nimportant social capital in some domains that could be leveraged to aid\nreintegration.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5171 communal \u7167\u7247\u5f15\u5bfc\u6cd5\uff0c\u63a2\u7d22\u5c3c\u6cca\u5c14\u6027\u522b\u8d29\u8fd0\u5e78\u5b58\u8005\u7684\u5904\u5883\u4e0e\u518d\u878d\u5408\uff0c\u4e3a\u76f8\u5173\u6587\u732e\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002", "motivation": "\u63a2\u7d22\u5c3c\u6cca\u5c14\u6027\u522b\u8d29\u8fd0\u5e78\u5b58\u8005\u7684\u5904\u5883\uff0c\u4fc3\u8fdb\u4ed6\u4eec\u5728\u5eb7\u590d\u8fc7\u7a0b\u4e2d\u53d1\u58f0\u53ca\u6574\u5408\u3002", "method": "\u4f7f\u7528\u5171 communal \u7684\u7167\u7247\u5f15\u5bfc\u6280\u672f\uff0c\u4ee5\u4fbf\u66f4\u597d\u5730\u503e\u542c\u5e78\u5b58\u8005\u7684\u58f0\u97f3\u3002", "result": "\u63ed\u793a\u4e86\u5e78\u5b58\u8005\u590d\u6742\u7684\u5eb7\u590d\u60c5\u51b5\uff0c\u5f3a\u8c03\u4e86\u4ed6\u4eec\u5728\u793e\u4f1a\u4e2d\u6709\u9650\u4f46\u91cd\u8981\u7684\u793e\u4f1a\u8d44\u672c\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5171 communal \u65b9\u6cd5\u7684\u7167\u7247\u5f15\u5bfc\u6280\u672f\uff0c\u4ee5\u652f\u6301\u6027\u522b\u8d29\u8fd0\u5e78\u5b58\u8005\u7684\u518d\u878d\u5408\u3002"}}
{"id": "2510.00726", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00726", "abs": "https://arxiv.org/abs/2510.00726", "authors": ["Giovanni Minelli", "Giulio Turrisi", "Victor Barasuol", "Claudio Semini"], "title": "CroSTAta: Cross-State Transition Attention Transformer for Robotic Manipulation", "comment": "Code and data available at https://github.com/iit-DLSLab/croSTAta", "summary": "Learning robotic manipulation policies through supervised learning from\ndemonstrations remains challenging when policies encounter execution variations\nnot explicitly covered during training. While incorporating historical context\nthrough attention mechanisms can improve robustness, standard approaches\nprocess all past states in a sequence without explicitly modeling the temporal\nstructure that demonstrations may include, such as failure and recovery\npatterns. We propose a Cross-State Transition Attention Transformer that\nemploys a novel State Transition Attention (STA) mechanism to modulate standard\nattention weights based on learned state evolution patterns, enabling policies\nto better adapt their behavior based on execution history. Our approach\ncombines this structured attention with temporal masking during training, where\nvisual information is randomly removed from recent timesteps to encourage\ntemporal reasoning from historical context. Evaluation in simulation shows that\nSTA consistently outperforms standard cross-attention and temporal modeling\napproaches like TCN and LSTM networks across all tasks, achieving more than 2x\nimprovement over cross-attention on precision-critical tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6ce8\u610f\u529b\u673a\u5236\u6765\u589e\u5f3a\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u9002\u5e94\u6027\uff0c\u65b9\u6cd5\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u673a\u5668\u4eba\u64cd\u4f5c\u653f\u7b56\u8bad\u7ec3\u4e2d\uff0c\u6267\u884c\u8fc7\u7a0b\u4e2d\u672a\u6db5\u76d6\u7684\u53d8\u5316\u4f7f\u5f97\u6807\u51c6\u7684\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u9762\u4e34\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u5efa\u6a21\u6267\u884c\u5386\u53f2\u4e2d\u7684\u65f6\u95f4\u7ed3\u6784\u3002", "method": "\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u7684\u72b6\u6001\u8f6c\u79fb\u6ce8\u610f\u529b\u673a\u5236\uff08STA\uff09\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u6ce8\u610f\u529b\u548c\u8bad\u7ec3\u65f6\u7684\u65f6\u95f4\u63a9\u853d\u6280\u672f\uff0c\u4ece\u800c\u6539\u5584\u7b97\u6cd5\u5bf9\u5386\u53f2\u4e0a\u4e0b\u6587\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u4eff\u771f\u8bc4\u4f30\u4e2d\uff0cSTA\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u6807\u51c6\u4ea4\u53c9\u6ce8\u610f\u529b\u53ca\u91c7\u7528\u65f6\u95f4\u5efa\u6a21\u7684\u65b9\u6cd5\uff08\u5982TCN\u548cLSTM\u7f51\u7edc\uff09\uff0c\u5728\u7cbe\u786e\u5173\u952e\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8d85\u8fc72\u500d\u7684\u63d0\u5347\u3002", "conclusion": "\u63d0\u51fa\u7684Cross-State Transition Attention Transformer\u901a\u8fc7\u5f15\u5165\u72b6\u6001\u8f6c\u79fb\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u653f\u7b56\u5bf9\u6267\u884c\u5386\u53f2\u7684\u9002\u5e94\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u5728\u7cbe\u786e\u5173\u952e\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2510.00770", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.00770", "abs": "https://arxiv.org/abs/2510.00770", "authors": ["Tianle Ni", "Xiao Chen", "Hamid Sadeghian", "Sami Haddadin"], "title": "Tele-rehabilitation with online skill transfer and adaptation in $\\mathbb{R}^3 \\times \\mathit{S}^3$", "comment": null, "summary": "This paper proposes a tele-teaching framework for the domain of\nrobot-assisted tele-rehabilitation. The system connects two robotic\nmanipulators on therapist and patient side via bilateral teleoperation,\nenabling a therapist to remotely demonstrate rehabilitation exercises that are\nexecuted by the patient-side robot. A 6-DoF Dynamical Movement Primitives\nformulation is employed to jointly encode translational and rotational motions\nin $\\mathbb{R}^3 \\times \\mathit{S}^3$ space, ensuring accurate trajectory\nreproduction. The framework supports smooth transitions between therapist-led\nguidance and patient passive training, while allowing adaptive adjustment of\nmotion. Experiments with 7-DoF manipulators demonstrate the feasibility of the\napproach, highlighting its potential for personalized and remotely supervised\nrehabilitation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u673a\u5668\u4eba\u8f85\u52a9\u7684\u8fdc\u7a0b\u5eb7\u590d\u6559\u5b66\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5411\u8fdc\u7a0b\u64cd\u4f5c\u4f7f\u6cbb\u7597\u5e08\u80fd\u591f\u8fdc\u7a0b\u6307\u5bfc\u60a3\u8005\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u5728\u8fdc\u7a0b\u76d1\u7763\u5eb7\u590d\u4e2d\u5177\u6709\u826f\u597d\u9002\u5e94\u6027\u548c\u4e2a\u6027\u5316\u6f5c\u529b\u3002", "motivation": "\u63d0\u51fa\u4e00\u4e2a\u673a\u5668\u4eba\u8f85\u52a9\u7684\u8fdc\u7a0b\u5eb7\u590d\u6559\u5b66\u6846\u67b6\uff0c\u4ee5\u6539\u5584\u5eb7\u590d\u6cbb\u7597\u7684\u7075\u6d3b\u6027\u548c\u6709\u6548\u6027\u3002", "method": "\u5229\u75286\u81ea\u7531\u5ea6\u52a8\u6001\u8fd0\u52a8\u539f\u8bed\u6765\u7f16\u7801\u8fd0\u52a8\u8f68\u8ff9\uff0c\u5e76\u5b9e\u73b0\u53cc\u5411\u8fdc\u7a0b\u64cd\u4f5c\u3002", "result": "\u901a\u8fc77\u81ea\u7531\u5ea6\u673a\u68b0\u624b\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4e2a\u6027\u5316\u548c\u8fdc\u7a0b\u76d1\u7763\u5eb7\u590d\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2510.00783", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.00783", "abs": "https://arxiv.org/abs/2510.00783", "authors": ["Thanh Nguyen Canh", "Haolan Zhang", "Xiem HoangVan", "Nak Young Chong"], "title": "Semantic Visual Simultaneous Localization and Mapping: A Survey on State of the Art, Challenges, and Future Directions", "comment": null, "summary": "Semantic Simultaneous Localization and Mapping (SLAM) is a critical area of\nresearch within robotics and computer vision, focusing on the simultaneous\nlocalization of robotic systems and associating semantic information to\nconstruct the most accurate and complete comprehensive model of the surrounding\nenvironment. Since the first foundational work in Semantic SLAM appeared more\nthan two decades ago, this field has received increasing attention across\nvarious scientific communities. Despite its significance, the field lacks\ncomprehensive surveys encompassing recent advances and persistent challenges.\nIn response, this study provides a thorough examination of the state-of-the-art\nof Semantic SLAM techniques, with the aim of illuminating current trends and\nkey obstacles. Beginning with an in-depth exploration of the evolution of\nvisual SLAM, this study outlines its strengths and unique characteristics,\nwhile also critically assessing previous survey literature. Subsequently, a\nunified problem formulation and evaluation of the modular solution framework is\nproposed, which divides the problem into discrete stages, including visual\nlocalization, semantic feature extraction, mapping, data association, and loop\nclosure optimization. Moreover, this study investigates alternative\nmethodologies such as deep learning and the utilization of large language\nmodels, alongside a review of relevant research about contemporary SLAM\ndatasets. Concluding with a discussion on potential future research directions,\nthis study serves as a comprehensive resource for researchers seeking to\nnavigate the complex landscape of Semantic SLAM.", "AI": {"tldr": "\u672c\u7814\u7a76\u6df1\u5165\u63a2\u8ba8\u4e86\u8bed\u4e49SLAM\u7684\u73b0\u72b6\uff0c\u63d0\u51fa\u4e86\u7cfb\u7edf\u5316\u7684\u89e3\u51b3\u65b9\u6848\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u89e3\u51b3\u5f53\u524d\u9762\u4e34\u7684\u6311\u6218\u3002", "motivation": "\u5728\u8bed\u4e49SLAM\u9886\u57df\uff0c\u5c3d\u7ba1\u5176\u91cd\u8981\u6027\u4e0d\u65ad\u589e\u52a0\uff0c\u4f46\u7f3a\u4e4f\u5168\u9762\u7684\u8c03\u67e5\u7814\u7a76\u6765\u6db5\u76d6\u8fd1\u671f\u7684\u8fdb\u5c55\u548c\u6301\u7eed\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u95ee\u9898\u8868\u8ff0\u548c\u6a21\u5757\u5316\u89e3\u51b3\u65b9\u6848\u6846\u67b6\uff0c\u5c06SLAM\u95ee\u9898\u5206\u4e3a\u591a\u4e2a\u79bb\u6563\u9636\u6bb5\uff0c\u5305\u62ec\u89c6\u89c9\u5b9a\u4f4d\u3001\u8bed\u4e49\u7279\u5f81\u63d0\u53d6\u3001\u5efa\u56fe\u3001\u6570\u636e\u5173\u8054\u548c\u56de\u73af\u95ed\u5408\u4f18\u5316\u3002", "result": "\u7efc\u5408\u8003\u5bdf\u4e86\u8bed\u4e49SLAM\u6280\u672f\u7684\u6700\u65b0\u53d1\u5c55\uff0c\u8bc4\u4f30\u4e86\u89c6\u89c9SLAM\u7684\u6f14\u53d8\u3001\u7279\u70b9\u5e76\u5bf9\u524d\u8ff0\u8c03\u67e5\u6587\u732e\u8fdb\u884c\u4e86\u6279\u5224\u6027\u5206\u6790\uff0c\u540c\u65f6\u63a2\u7d22\u4e86\u6df1\u5ea6\u5b66\u4e60\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7b49\u66ff\u4ee3\u65b9\u6cd5\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u5173\u4e8e\u8bed\u4e49SLAM\u7684\u6700\u65b0\u8fdb\u5c55\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u7684\u5168\u9762\u8d44\u6e90\u3002"}}
{"id": "2510.00814", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.00814", "abs": "https://arxiv.org/abs/2510.00814", "authors": ["Kai Tang", "Dipankar Bhattacharya", "Hang Xu", "Fuyuki Tokuda", "Norman C. Tien", "Kazuhiro Kosuge"], "title": "RTFF: Random-to-Target Fabric Flattening Policy using Dual-Arm Manipulator", "comment": "9 pages, 6 figures, conference", "summary": "Robotic fabric manipulation in garment production for sewing, cutting, and\nironing requires reliable flattening and alignment, yet remains challenging due\nto fabric deformability, effectively infinite degrees of freedom, and frequent\nocclusions from wrinkles, folds, and the manipulator's End-Effector (EE) and\narm. To address these issues, this paper proposes the first Random-to-Target\nFabric Flattening (RTFF) policy, which aligns a random wrinkled fabric state to\nan arbitrary wrinkle-free target state. The proposed policy adopts a hybrid\nImitation Learning-Visual Servoing (IL-VS) framework, where IL learns with\nexplicit fabric models for coarse alignment of the wrinkled fabric toward a\nwrinkle-free state near the target, and VS ensures fine alignment to the\ntarget. Central to this framework is a template-based mesh that offers precise\ntarget state representation, wrinkle-aware geometry prediction, and consistent\nvertex correspondence across RTFF manipulation steps, enabling robust\nmanipulation and seamless IL-VS switching. Leveraging the power of mesh, a\nnovel IL solution for RTFF-Mesh Action Chunking Transformer (MACT)-is then\nproposed by conditioning the mesh information into a Transformer-based policy.\nThe RTFF policy is validated on a real dual-arm tele-operation system, showing\nzero-shot alignment to different targets, high accuracy, and strong\ngeneralization across fabrics and scales. Project website:\nhttps://kaitang98.github.io/RTFF_Policy/", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u968f\u673a\u5230\u76ee\u6807\u9762\u6599\u5e73\u6574\u7b56\u7565\uff08RTFF\uff09\uff0c\u901a\u8fc7\u6df7\u5408\u6a21\u4eff\u5b66\u4e60\u548c\u89c6\u89c9\u4f3a\u670d\u6280\u672f\uff0c\u89e3\u51b3\u9762\u6599\u5bf9\u9f50\u4e2d\u7684\u6311\u6218\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u9002\u5e94\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u670d\u88c5\u751f\u4ea7\u4e2d\uff0c\u673a\u5668\u4eba\u5bf9\u9762\u6599\u7684\u64cd\u4f5c\u9700\u8981\u53ef\u9760\u7684\u5e73\u6574\u548c\u5bf9\u9f50\uff0c\u4f46\u7531\u4e8e\u9762\u6599\u7684\u53ef\u53d8\u5f62\u6027\u548c\u590d\u6742\u5ea6\uff0c\u8fd9\u4e00\u8fc7\u7a0b\u5145\u6ee1\u6311\u6218\u3002", "method": "\u91c7\u7528\u6df7\u5408\u6a21\u4eff\u5b66\u4e60-\u89c6\u89c9\u4f3a\u670d\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u677f\u57fa\u7840\u7f51\u683c\u63d0\u4f9b\u76ee\u6807\u72b6\u6001\u8868\u793a\uff0c\u8fdb\u884c\u7c97\u7565\u4e0e\u7cbe\u786e\u7684\u9762\u6599\u5bf9\u9f50\u3002", "result": "RTFF\u653f\u7b56\u5728\u771f\u5b9e\u53cc\u81c2\u9065\u64cd\u4f5c\u7cfb\u7edf\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u663e\u793a\u4e86\u5728\u4e0d\u540c\u76ee\u6807\u4e0a\u7684\u96f6\u6837\u672c\u5bf9\u9f50\u548c\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684RTFF\u7b56\u7565\u5728\u4e0d\u540c\u76ee\u6807\u4e0a\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u5bf9\u9f50\uff0c\u5c55\u73b0\u4e86\u9ad8\u51c6\u786e\u6027\u548c\u5f3a\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u9002\u5e94\u591a\u79cd\u9762\u6599\u548c\u89c4\u6a21\u3002"}}
{"id": "2510.00933", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.00933", "abs": "https://arxiv.org/abs/2510.00933", "authors": ["Sara Strakosova", "Petr Novak", "Petr Kadera"], "title": "Product-oriented Product-Process-Resource Asset Network and its Representation in AutomationML for Asset Administration Shell", "comment": "This work has been submitted to the IEEE for possible publication. 8\n  pages, 6 figures", "summary": "Current products, especially in the automotive sector, pose complex technical\nsystems having a multi-disciplinary mechatronic nature. Industrial standards\nsupporting system engineering and production typically (i) address the\nproduction phase only, but do not cover the complete product life cycle, and\n(ii) focus on production processes and resources rather than the products\nthemselves. The presented approach is motivated by incorporating impacts of\nend-of-life phase of the product life cycle into the engineering phase. This\npaper proposes a modelling approach coming up from the Product-Process-Resource\n(PPR) modeling paradigm. It combines requirements on (i) respecting the product\nstructure as a basis for the model, and (ii) it incorporates repairing,\nremanufacturing, or upcycling within cyber-physical production systems. The\nproposed model called PoPAN should accompany the product during the entire life\ncycle as a digital shadow encapsulated within the Asset Administration Shell of\na product. To facilitate the adoption of the proposed paradigm, the paper also\nproposes serialization of the model in the AutomationML data format. The model\nis demonstrated on a use-case for disassembling electric vehicle batteries to\nsupport their remanufacturing for stationary battery applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6a21\u578bPoPAN\uff0c\u4ee5\u652f\u6301\u6c7d\u8f66\u7b49\u590d\u6742\u4ea7\u54c1\u5728\u6574\u4e2a\u751f\u547d\u5468\u671f\u4e2d\u7684\u4fee\u590d\u4e0e\u518d\u5236\u9020\uff0c\u7279\u522b\u805a\u7126\u7535\u52a8\u6c7d\u8f66\u7535\u6c60\u7684\u5e94\u7528\u3002", "motivation": "\u5e0c\u671b\u901a\u8fc7\u5c06\u4ea7\u54c1\u751f\u547d\u5468\u671f\u7684\u672b\u7aef\u5f71\u54cd\u7eb3\u5165\u5de5\u7a0b\u9636\u6bb5\uff0c\u6765\u6539\u5584\u5f53\u524d\u4ea7\u54c1\u5de5\u7a0b\u548c\u751f\u4ea7\u8fc7\u7a0b\u4e2d\u5b58\u5728\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6c7d\u8f66\u884c\u4e1a\u4e2d\u7684\u590d\u6742\u6280\u672f\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u4ea7\u54c1-\u8fc7\u7a0b-\u8d44\u6e90\uff08PPR\uff09\u5efa\u6a21\u8303\u5f0f\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5efa\u6a21\u65b9\u6cd5\uff0c\u5e76\u4f7f\u7528AutomationML\u6570\u636e\u683c\u5f0f\u8fdb\u884c\u5e8f\u5217\u5316\u3002", "result": "\u6a21\u578b\u7ed3\u5408\u4e86\u4ea7\u54c1\u7ed3\u6784\u548c\u518d\u5236\u9020\u7b49\u8981\u6c42\uff0c\u80fd\u591f\u5728\u667a\u80fd\u7269\u7406\u751f\u4ea7\u7cfb\u7edf\u4e2d\u5e94\u7528\uff0c\u5e76\u63a8\u5e7f\u81f3\u6574\u4e2a\u4ea7\u54c1\u751f\u547d\u5468\u671f\u3002", "conclusion": "\u63d0\u51fa\u7684PoPAN\u6a21\u578b\u80fd\u591f\u5728\u6574\u4e2a\u4ea7\u54c1\u751f\u547d\u5468\u671f\u5185\u4f34\u968f\u4ea7\u54c1\uff0c\u52a9\u529b\u4fee\u590d\u3001\u518d\u5236\u9020\u548c\u5347\u7ea7\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u7535\u52a8\u6c7d\u8f66\u7535\u6c60\u7684\u62c6\u89e3\u4e0e\u518d\u5236\u9020\u3002"}}
{"id": "2510.00942", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.00942", "abs": "https://arxiv.org/abs/2510.00942", "authors": ["Reza Vafaee", "Kian Behzad", "Milad Siami", "Luca Carlone", "Ali Jadbabaie"], "title": "Non-submodular Visual Attention for Robot Navigation", "comment": "22 pages; Accepted to appear in IEEE Transactions on Robotics (T-RO)", "summary": "This paper presents a task-oriented computational framework to enhance\nVisual-Inertial Navigation (VIN) in robots, addressing challenges such as\nlimited time and energy resources. The framework strategically selects visual\nfeatures using a Mean Squared Error (MSE)-based, non-submodular objective\nfunction and a simplified dynamic anticipation model. To address the\nNP-hardness of this problem, we introduce four polynomial-time approximation\nalgorithms: a classic greedy method with constant-factor guarantees; a low-rank\ngreedy variant that significantly reduces computational complexity; a\nrandomized greedy sampler that balances efficiency and solution quality; and a\nlinearization-based selector based on a first-order Taylor expansion for\nnear-constant-time execution. We establish rigorous performance bounds by\nleveraging submodularity ratios, curvature, and element-wise curvature\nanalyses. Extensive experiments on both standardized benchmarks and a custom\ncontrol-aware platform validate our theoretical results, demonstrating that\nthese methods achieve strong approximation guarantees while enabling real-time\ndeployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\u4ee5\u589e\u5f3a\u673a\u5668\u4eba\u7684\u89c6\u89c9\u60ef\u6027\u5bfc\u822a\uff0c\u5305\u542b\u591a\u79cd\u9ad8\u6548\u7684\u7279\u5f81\u9009\u62e9\u7b97\u6cd5\uff0c\u7ecf\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u9488\u5bf9\u89c6\u89c9\u60ef\u6027\u5bfc\u822a\u4e2d\u65f6\u95f4\u548c\u80fd\u91cf\u8d44\u6e90\u6709\u9650\u7684\u6311\u6218\uff0c\u63d0\u51fa\u6846\u67b6\u4ee5\u4f18\u5316\u89c6\u89c9\u7279\u5f81\u9009\u62e9\u3002", "method": "\u5f15\u5165\u4e86\u56db\u4e2a\u591a\u9879\u5f0f\u65f6\u95f4\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u5305\u62ec\u7ecf\u5178\u8d2a\u5a6a\u6cd5\u3001\u4f4e\u79e9\u8d2a\u5a6a\u53d8\u4f53\u3001\u968f\u673a\u8d2a\u5a6a\u91c7\u6837\u5668\u548c\u57fa\u4e8e\u7ebf\u6027\u5316\u9009\u62e9\u5668\uff0c\u7ed3\u5408\u4e86\u5b50\u6a21\u6027\u6bd4\u7387\u7b49\u5206\u6790\u65b9\u6cd5\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u548c\u81ea\u5b9a\u4e49\u63a7\u5236\u611f\u77e5\u5e73\u53f0\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u7684\u6709\u6548\u6027\u4e0e\u5f3a\u8fd1\u4f3c\u4fdd\u8bc1\u3002", "conclusion": "\u63d0\u51fa\u7684\u4efb\u52a1\u5bfc\u5411\u8ba1\u7b97\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7684\u89c6\u89c9\u60ef\u6027\u5bfc\u822a\u6027\u80fd\uff0c\u5e76\u5b9e\u73b0\u4e86\u5b9e\u65f6\u90e8\u7f72\u3002"}}
{"id": "2510.00995", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.00995", "abs": "https://arxiv.org/abs/2510.00995", "authors": ["Jacob Moore", "Phil Tokumaru", "Ian Reid", "Brandon Sutherland", "Joseph Ritchie", "Gabe Snow", "Tim McLain"], "title": "ROSflight 2.0: Lean ROS 2-Based Autopilot for Unmanned Aerial Vehicles", "comment": "To be submitted to the 2026 IEEE International Conference on Robotics\n  and Automation in Vienna, Austria", "summary": "ROSflight is a lean, open-source autopilot ecosystem for unmanned aerial\nvehicles (UAVs). Designed by researchers for researchers, it is built to lower\nthe barrier to entry to UAV research and accelerate the transition from\nsimulation to hardware experiments by maintaining a lean (not full-featured),\nwell-documented, and modular codebase. This publication builds on previous\ntreatments and describes significant additions to the architecture that improve\nthe modularity and usability of ROSflight, including the transition from ROS 1\nto ROS 2, supported hardware, low-level actuator mixing, and the simulation\nenvironment. We believe that these changes improve the usability of ROSflight\nand enable ROSflight to accelerate research in areas like advanced-air\nmobility. Hardware results are provided, showing that ROSflight is able to\ncontrol a multirotor over a serial connection at 400 Hz while closing all\ncontrol loops on the companion computer.", "AI": {"tldr": "ROSflight\u662f\u4e00\u4e2a\u5f00\u6e90\u65e0\u4eba\u673a\u81ea\u4e3b\u9a7e\u9a76\u5e73\u53f0\uff0c\u901a\u8fc7\u63d0\u9ad8\u6a21\u5757\u5316\u548c\u53ef\u7528\u6027\uff0c\u4fc3\u8fdb\u65e0\u4eba\u673a\u7814\u7a76\uff0c\u652f\u6301\u9ad8\u9891\u63a7\u5236\u3002", "motivation": "\u65e8\u5728\u964d\u4f4e\u65e0\u4eba\u673a\u7814\u7a76\u7684\u5165\u95e8\u95e8\u69db\uff0c\u5e76\u52a0\u901f\u4ece\u4eff\u771f\u5230\u786c\u4ef6\u5b9e\u9a8c\u7684\u8fc7\u6e21\u3002", "method": "\u5728ROSflight\u67b6\u6784\u4e2d\u8fdb\u884c\u91cd\u8981\u66f4\u65b0\uff0c\u5305\u62ec\u4eceROS 1\u8fc1\u79fb\u5230ROS 2\u3001\u652f\u6301\u7684\u786c\u4ef6\u3001\u4f4e\u7ea7\u6267\u884c\u5668\u6df7\u5408\u548c\u4eff\u771f\u73af\u5883\u7684\u6539\u8fdb\u3002", "result": "\u663e\u793aROSflight\u80fd\u591f\u901a\u8fc7\u4e32\u884c\u8fde\u63a5\u5728400 Hz\u4e0b\u63a7\u5236\u591a\u65cb\u7ffc\uff0c\u540c\u65f6\u5728\u4f34\u968f\u8ba1\u7b97\u673a\u4e0a\u95ed\u5408\u6240\u6709\u63a7\u5236\u56de\u8def\u3002", "conclusion": "ROSflight \u901a\u8fc7\u6539\u8fdb\u6a21\u5757\u5316\u548c\u53ef\u7528\u6027\uff0c\u52a0\u901f\u4e86\u65e0\u4eba\u673a\u7814\u7a76\uff0c\u652f\u6301 400 Hz \u7684\u591a\u65cb\u7ffc\u63a7\u5236\u3002"}}
{"id": "2510.01023", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01023", "abs": "https://arxiv.org/abs/2510.01023", "authors": ["S. Satsevich", "A. Bazhenov", "S. Egorov", "A. Erkhov", "M. Gromakov", "A. Fedoseev", "D. Tsetserukou"], "title": "Prometheus: Universal, Open-Source Mocap-Based Teleoperation System with Force Feedback for Dataset Collection in Robot Learning", "comment": null, "summary": "This paper presents a novel teleoperation system with force feedback,\nutilizing consumer-grade HTC Vive Trackers 2.0. The system integrates a\ncustom-built controller, a UR3 robotic arm, and a Robotiq gripper equipped with\ncustom-designed fingers to ensure uniform pressure distribution on an embedded\nforce sensor. Real-time compression force data is transmitted to the\ncontroller, enabling operators to perceive the gripping force applied to\nobjects. Experimental results demonstrate that the system enhances task success\nrates and provides a low-cost solution for large-scale imitation learning data\ncollection without compromising affordability.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4f4e\u6210\u672c\u8fdc\u7a0b\u64cd\u4f5c\u7cfb\u7edf\uff0c\u7ed3\u5408\u529b\u53cd\u9988\u548c\u5b9a\u5236\u8bbe\u5907\uff0c\u663e\u8457\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u6539\u5584\u8fdc\u7a0b\u64cd\u4f5c\u7cfb\u7edf\u7684\u53cd\u9988\u673a\u5236\uff0c\u5141\u8bb8\u64cd\u4f5c\u8005\u51c6\u786e\u611f\u77e5\u65bd\u52a0\u4e8e\u7269\u4f53\u7684\u6293\u53d6\u529b\u91cf\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210HTC Vive Trackers 2.0\u7684\u8fdc\u7a0b\u64cd\u4f5c\u7cfb\u7edf\uff0c\u7ed3\u5408\u5b9a\u5236\u63a7\u5236\u5668\u3001UR3\u673a\u68b0\u81c2\u53caRobotiq\u6293\u624b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u5728\u4efb\u52a1\u6210\u529f\u7387\u4e0a\u6709\u663e\u8457\u63d0\u5347\uff0c\u4e14\u7ecf\u6d4e\u6027\u8f83\u9ad8\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ecf\u6d4e\u5b9e\u60e0\u7684\u5927\u89c4\u6a21\u6a21\u4eff\u5b66\u4e60\u6570\u636e\u6536\u96c6\u65b9\u6848\u3002"}}
{"id": "2510.01041", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.01041", "abs": "https://arxiv.org/abs/2510.01041", "authors": ["Ian Reid", "Joseph Ritchie", "Jacob Moore", "Brandon Sutherland", "Gabe Snow", "Phillip Tokumaru", "Tim McLain"], "title": "ROSplane 2.0: A Fixed-Wing Autopilot for Research", "comment": null, "summary": "Unmanned aerial vehicle (UAV) research requires the integration of\ncutting-edge technology into existing autopilot frameworks. This process can be\narduous, requiring extensive resources, time, and detailed knowledge of the\nexisting system. ROSplane is a lean, open-source fixed-wing autonomy stack\nbuilt by researchers for researchers. It is designed to accelerate research by\nproviding clearly defined interfaces with an easily modifiable framework.\nPowered by ROS 2, ROSplane allows for rapid integration of low or high-level\ncontrol, path planning, or estimation algorithms. A focus on lean, easily\nunderstood code and extensive documentation lowers the barrier to entry for\nresearchers. Recent developments to ROSplane improve its capacity to accelerate\nUAV research, including the transition from ROS 1 to ROS 2, enhanced estimation\nand control algorithms, increased modularity, and an improved aerodynamic\nmodeling pipeline. This aerodynamic modeling pipeline significantly reduces the\neffort of transitioning from simulation to real-world testing without requiring\nexpensive system identification or computational fluid dynamics tools.\nROSplane's architecture reduces the effort required to integrate new research\ntools and methods, expediting hardware experimentation.", "AI": {"tldr": "ROSplane \u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u56fa\u5b9a\u7ffc\u81ea\u4e3b\u63a7\u5236\u6808\uff0c\u65e8\u5728\u52a0\u901f\u65e0\u4eba\u673a\u7814\u7a76\uff0c\u901a\u8fc7\u6613\u4e8e\u7406\u89e3\u7684\u4ee3\u7801\u548c\u6587\u6863\u964d\u4f4e\u7814\u7a76\u95e8\u69db\uff0c\u5e76\u7b80\u5316\u65b0\u5de5\u5177\u548c\u65b9\u6cd5\u7684\u96c6\u6210\u3002", "motivation": "\u63a8\u52a8\u65e0\u4eba\u673a\u7814\u7a76\u7684\u53d1\u5c55\uff0c\u964d\u4f4e\u7814\u7a76\u4eba\u5458\u5728\u6574\u5408\u5148\u8fdb\u6280\u672f\u4e0e\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u65f6\u7684\u8d44\u6e90\u4e0e\u65f6\u95f4\u6210\u672c\u3002", "method": "\u91c7\u7528 ROS 2 \u5e73\u53f0\uff0c\u63d0\u4f9b\u660e\u786e\u7684\u63a5\u53e3\u548c\u53ef\u4fee\u6539\u7684\u6846\u67b6\uff0c\u4ee5\u652f\u6301\u63a7\u5236\u3001\u8def\u5f84\u89c4\u5212\u548c\u4f30\u8ba1\u7b97\u6cd5\u7684\u5feb\u901f\u96c6\u6210\u3002", "result": "\u6700\u65b0\u7684 ROSplane \u7248\u672c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u7814\u7a76\u7684\u80fd\u529b\uff0c\u5305\u62ec\u4ece ROS 1 \u8fc1\u79fb\u5230 ROS 2\u3001\u589e\u5f3a\u7684\u4f30\u8ba1\u4e0e\u63a7\u5236\u7b97\u6cd5\u3001\u66f4\u9ad8\u7684\u6a21\u5757\u5316\u4ee5\u53ca\u6539\u8fdb\u7684\u7a7a\u6c14\u52a8\u529b\u5b66\u5efa\u6a21\u6d41\u7a0b\u3002", "conclusion": "ROSplane \u7684\u4f53\u7cfb\u7ed3\u6784\u7b80\u5316\u4e86\u65b0\u7814\u7a76\u5de5\u5177\u548c\u65b9\u6cd5\u7684\u96c6\u6210\uff0c\u4fc3\u8fdb\u4e86\u786c\u4ef6\u5b9e\u9a8c\u7684\u5feb\u901f\u8fdb\u884c\u3002"}}
{"id": "2510.01068", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01068", "abs": "https://arxiv.org/abs/2510.01068", "authors": ["Jiahang Cao", "Yize Huang", "Hanzhong Guo", "Rui Zhang", "Mu Nan", "Weijian Mai", "Jiaxu Wang", "Hao Cheng", "Jingkai Sun", "Gang Han", "Wen Zhao", "Qiang Zhang", "Yijie Guo", "Qihao Zheng", "Chunfeng Song", "Xiao Li", "Ping Luo", "Andrew F. Luo"], "title": "Compose Your Policies! Improving Diffusion-based or Flow-based Robot Policies via Test-time Distribution-level Composition", "comment": "Project Page: https://sagecao1125.github.io/GPC-Site/", "summary": "Diffusion-based models for robotic control, including vision-language-action\n(VLA) and vision-action (VA) policies, have demonstrated significant\ncapabilities. Yet their advancement is constrained by the high cost of\nacquiring large-scale interaction datasets. This work introduces an alternative\nparadigm for enhancing policy performance without additional model training.\nPerhaps surprisingly, we demonstrate that the composed policies can exceed the\nperformance of either parent policy. Our contribution is threefold. First, we\nestablish a theoretical foundation showing that the convex composition of\ndistributional scores from multiple diffusion models can yield a superior\none-step functional objective compared to any individual score. A\nGr\\\"onwall-type bound is then used to show that this single-step improvement\npropagates through entire generation trajectories, leading to systemic\nperformance gains. Second, motivated by these results, we propose General\nPolicy Composition (GPC), a training-free method that enhances performance by\ncombining the distributional scores of multiple pre-trained policies via a\nconvex combination and test-time search. GPC is versatile, allowing for the\nplug-and-play composition of heterogeneous policies, including VA and VLA\nmodels, as well as those based on diffusion or flow-matching, irrespective of\ntheir input visual modalities. Third, we provide extensive empirical\nvalidation. Experiments on Robomimic, PushT, and RoboTwin benchmarks, alongside\nreal-world robotic evaluations, confirm that GPC consistently improves\nperformance and adaptability across a diverse set of tasks. Further analysis of\nalternative composition operators and weighting strategies offers insights into\nthe mechanisms underlying the success of GPC. These results establish GPC as a\nsimple yet effective method for improving control performance by leveraging\nexisting policies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u653f\u7b56\u7ec4\u5408\u65b9\u6cd5\uff0cGPC\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u4e2a\u9884\u8bad\u7ec3\u653f\u7b56\u7684\u5206\u5e03\u5f97\u5206\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u4ece\u800c\u63d0\u9ad8\u673a\u5668\u4eba\u63a7\u5236\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u6269\u6563\u6a21\u578b\u5728\u673a\u5668\u63a7\u5236\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u5c31\uff0c\u4f46\u7531\u4e8e\u83b7\u53d6\u5927\u89c4\u6a21\u4ea4\u4e92\u6570\u636e\u96c6\u7684\u9ad8\u6210\u672c\uff0c\u5176\u53d1\u5c55\u53d7\u5230\u4e86\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u8bad\u7ec3\u7684\u65b9\u6cd5\u2014\u2014General Policy Composition (GPC)\uff0c\u901a\u8fc7\u5bf9\u591a\u4e2a\u9884\u8bad\u7ec3\u653f\u7b56\u7684\u5206\u5e03\u5f97\u5206\u8fdb\u884c\u51f8\u7ec4\u5408\u548c\u6d4b\u8bd5\u65f6\u641c\u7d22\u6765\u589e\u5f3a\u6027\u80fd\u3002", "result": "\u5728Robomimic\u3001PushT\u548cRoboTwin\u57fa\u51c6\u6d4b\u8bd5\u4ee5\u53ca\u771f\u5b9e\u673a\u5668\u4eba\u8bc4\u4f30\u4e2d\uff0cGPC\u4e00\u81f4\u6027\u5730\u63d0\u9ad8\u4e86\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002", "conclusion": "GPC\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u73b0\u6709\u653f\u7b56\u6765\u6539\u5584\u63a7\u5236\u6027\u80fd\u3002"}}
{"id": "2510.01138", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01138", "abs": "https://arxiv.org/abs/2510.01138", "authors": ["Matthew Woodward"], "title": "Real-Time Trajectory Generation and Hybrid Lyapunov-Based Control for Hopping Robots", "comment": "7 pages, 4 figures, 4 tables", "summary": "The advent of rotor-based hopping robots has created very capable hopping\nplatforms with high agility and efficiency, and similar controllability, as\ncompared to their purely flying quadrotor counterparts. Advances in robot\nperformance have increased the hopping height to greater than 4 meters and\nopened up the possibility for more complex aerial trajectories (i.e.,\nbehaviors). However, currently hopping robots do not directly control their\naerial trajectory or transition to flight, eliminating the efficiency benefits\nof a hopping system. Here we show a real-time, computationally efficiency,\nnon-linear drag compensated, trajectory generation methodology and accompanying\nLyapunov-based controller. The combined system can create and follow complex\naerial trajectories from liftoff to touchdown on horizontal and vertical\nsurfaces, while maintaining strick control over the orientation at touchdown.\nThe computational efficiency provides broad applicability across all size\nscales of hopping robots while maintaining applicability to quadrotors in\ngeneral.", "AI": {"tldr": "\u672c\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u4f7f\u8df3\u8dc3\u673a\u5668\u4eba\u80fd\u591f\u9ad8\u6548\u5730\u751f\u6210\u5e76\u63a7\u5236\u590d\u6742\u7684\u7a7a\u4e2d\u8f68\u8ff9\uff0c\u63d0\u5347\u4e86\u5176\u5728\u5404\u79cd\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\u3002", "motivation": "\u5e94\u5bf9\u5f53\u524d\u8df3\u8dc3\u673a\u5668\u4eba\u5728\u7a7a\u4e2d\u8f68\u8ff9\u63a7\u5236\u548c\u98de\u884c\u52a8\u4f5c\u8f6c\u6362\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u63d0\u9ad8\u5176\u6574\u4f53\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "\u91c7\u7528\u975e\u7ebf\u6027\u62d6\u66f3\u8865\u507f\u7684\u8f68\u8ff9\u751f\u6210\u65b9\u6cd5\u548c\u674e\u4e9a\u666e\u8bfa\u592b\u63a7\u5236\u5668\u6765\u8fdb\u884c\u5b9e\u65f6\u63a7\u5236\u3002", "result": "\u7ed3\u5408\u7684\u7cfb\u7edf\u53ef\u4ee5\u4ece\u8d77\u98de\u5230\u7740\u9646\u521b\u5efa\u5e76\u9075\u5faa\u590d\u6742\u7684\u7a7a\u4e2d\u8f68\u8ff9\uff0c\u5e76\u4fdd\u6301\u5728\u63a5\u89e6\u65f6\u7684\u59ff\u6001\u63a7\u5236\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u7684\u8f68\u8ff9\u751f\u6210\u65b9\u6cd5\u4e0e\u63a7\u5236\u5668\uff0c\u80fd\u591f\u6709\u6548\u63a7\u5236\u8df3\u8dc3\u673a\u5668\u4eba\u5728\u590d\u6742\u7a7a\u4e2d\u8f68\u8ff9\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u5728\u7740\u9646\u65f6\u7684\u59ff\u6001\u4fdd\u6301\u65b9\u9762\u3002"}}
