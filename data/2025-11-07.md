<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 21]
- [cs.HC](#cs.HC) [Total: 20]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Dynamic Shape Control of Soft Robots Enabled by Data-Driven Model Reduction](https://arxiv.org/abs/2511.03931)
*Iman Adibnazari,Harsh Sharma,Myungsun Park,Jacobo Cervera-Torralba,Boris Kramer,Michael T. Tolley*

Main category: cs.RO

TL;DR: 本研究通过对比三种模型降维技术，显示LOpInf在软机器人动态形状控制中的优越性。


<details>
  <summary>Details</summary>
Motivation: 解决软机器人动态控制中的高维动态建模难题，探索有效的控制工具。

Method: 比较三种数据驱动模型降维技术：特征系统实现算法、带控制的动态模态分解和Lagrangian算子推断法。

Result: 在模拟的软机器人实验中，LOpInf方法在跟踪误差方面优于其他两种模型。

Conclusion: LOpInf方法在动态形状控制中表现优于其他模型，能够有效降低跟踪误差。

Abstract: Soft robots have shown immense promise in settings where they can leverage
dynamic control of their entire bodies. However, effective dynamic shape
control requires a controller that accounts for the robot's high-dimensional
dynamics--a challenge exacerbated by a lack of general-purpose tools for
modeling soft robots amenably for control. In this work, we conduct a
comparative study of data-driven model reduction techniques for generating
linear models amendable to dynamic shape control. We focus on three
methods--the eigensystem realization algorithm, dynamic mode decomposition with
control, and the Lagrangian operator inference (LOpInf) method. Using each
class of model, we explored their efficacy in model predictive control policies
for the dynamic shape control of a simulated eel-inspired soft robot in three
experiments: 1) tracking simulated reference trajectories guaranteed to be
feasible, 2) tracking reference trajectories generated from a biological model
of eel kinematics, and 3) tracking reference trajectories generated by a
reduced-scale physical analog. In all experiments, the LOpInf-based policies
generated lower tracking errors than policies based on other models.

</details>


### [2] [Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots](https://arxiv.org/abs/2511.03996)
*Yushi Wang,Changsheng Luo,Penghui Chen,Jianran Liu,Weijian Sun,Tong Guo,Kechang Yang,Biao Hu,Yangang Zhang,Mingguo Zhao*

Main category: cs.RO

TL;DR: 本研究提出了一种集成视觉感知与运动控制的强化学习控制器，成功应对人形足球挑战。


<details>
  <summary>Details</summary>
Motivation: 人形足球对具身智能构成了挑战，旨在解决现有系统模块耦合不良、反应迟缓和行为不一致的问题。

Method: 基于统一的强化学习控制器，直接整合视觉感知和运动控制，并扩展对抗运动先验至现实世界动态环境中的感知设置。

Result: 该控制器在多种场景中展现出强大的反应性，能够高效执行机器人足球运动。

Conclusion: 所提出的控制器在各种场景中表现出强大的反应能力，能够一致地执行连贯且稳健的足球行为，特别是在真实的RoboCup比赛中。

Abstract: Humanoid soccer poses a representative challenge for embodied intelligence,
requiring robots to operate within a tightly coupled perception-action loop.
However, existing systems typically rely on decoupled modules, resulting in
delayed responses and incoherent behaviors in dynamic environments, while
real-world perceptual limitations further exacerbate these issues. In this
work, we present a unified reinforcement learning-based controller that enables
humanoid robots to acquire reactive soccer skills through the direct
integration of visual perception and motion control. Our approach extends
Adversarial Motion Priors to perceptual settings in real-world dynamic
environments, bridging motion imitation and visually grounded dynamic control.
We introduce an encoder-decoder architecture combined with a virtual perception
system that models real-world visual characteristics, allowing the policy to
recover privileged states from imperfect observations and establish active
coordination between perception and action. The resulting controller
demonstrates strong reactivity, consistently executing coherent and robust
soccer behaviors across various scenarios, including real RoboCup matches.

</details>


### [3] [Integrating Ergonomics and Manipulability for Upper Limb Postural Optimization in Bimanual Human-Robot Collaboration](https://arxiv.org/abs/2511.04009)
*Chenzui Li,Yiming Chen,Xi Wu,Giacinto Barresi,Fei Chen*

Main category: cs.RO

TL;DR: 本研究提出了一种上肢姿势优化方法，以增强人机协作过程中的身体工效学和操控能力，经过实验验证显示了良好的效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注人类安全或操作效率，而本研究提出的方法同时整合了这两个方面，以促进人机协作。

Method: 对简化人类骨骼模型的关节角度进行优化，使用预测阻抗控制器调节机器人的末端执行器姿态。

Result: 实验结果显示优化后目标肌肉的激活情况显著改善。

Conclusion: 该方法有效提高了肌肉状态，并在不同合作情境下表现出良好的优化效果。

Abstract: This paper introduces an upper limb postural optimization method for
enhancing physical ergonomics and force manipulability during bimanual
human-robot co-carrying tasks. Existing research typically emphasizes human
safety or manipulative efficiency, whereas our proposed method uniquely
integrates both aspects to strengthen collaboration across diverse conditions
(e.g., different grasping postures of humans, and different shapes of objects).
Specifically, the joint angles of a simplified human skeleton model are
optimized by minimizing the cost function to prioritize safety and manipulative
capability. To guide humans towards the optimized posture, the reference
end-effector poses of the robot are generated through a transformation module.
A bimanual model predictive impedance controller (MPIC) is proposed for our
human-like robot, CURI, to recalibrate the end effector poses through planned
trajectories. The proposed method has been validated through various subjects
and objects during human-human collaboration (HHC) and human-robot
collaboration (HRC). The experimental results demonstrate significant
improvement in muscle conditions by comparing the activation of target muscles
before and after optimization.

</details>


### [4] [An LLM-based Framework for Human-Swarm Teaming Cognition in Disaster Search and Rescue](https://arxiv.org/abs/2511.04042)
*Kailun Ji,Xiaoyu Hu,Xinyu Zhang,Jun Chen*

Main category: cs.RO

TL;DR: 本研究提出了一种基于大语言模型的系统，旨在改善复杂地形下的无人机群体在人机协作中的效率和效果，通过自然交互捕捉操作者意图，实现更快速的任务完成和较低的认知负担。


<details>
  <summary>Details</summary>
Motivation: 希望解决在复杂地形和通信阻断下，大规模救援操作中人机合作存在的意图与行动之间的沟通瓶颈。

Method: 提出LLM-CRF系统，通过自然多模式交互捕捉操作者意图，并利用大语言模型进行任务分解和规划。

Result: 在模拟SAR场景中，LLM驱动的方法比传统指令界面减少了约64.2%的任务完成时间，任务成功率提高了7%，主观认知负担显著降低。

Conclusion: 该研究证明了大规模救援操作中使用大语言模型（LLM）可显著改善人机协作，提高效率，并降低操作负担。

Abstract: Large-scale disaster Search And Rescue (SAR) operations are persistently
challenged by complex terrain and disrupted communications. While Unmanned
Aerial Vehicle (UAV) swarms offer a promising solution for tasks like wide-area
search and supply delivery, yet their effective coordination places a
significant cognitive burden on human operators. The core human-machine
collaboration bottleneck lies in the ``intention-to-action gap'', which is an
error-prone process of translating a high-level rescue objective into a
low-level swarm command under high intensity and pressure. To bridge this gap,
this study proposes a novel LLM-CRF system that leverages Large Language Models
(LLMs) to model and augment human-swarm teaming cognition. The proposed
framework initially captures the operator's intention through natural and
multi-modal interactions with the device via voice or graphical annotations. It
then employs the LLM as a cognitive engine to perform intention comprehension,
hierarchical task decomposition, and mission planning for the UAV swarm. This
closed-loop framework enables the swarm to act as a proactive partner,
providing active feedback in real-time while reducing the need for manual
monitoring and control, which considerably advances the efficacy of the SAR
task. We evaluate the proposed framework in a simulated SAR scenario.
Experimental results demonstrate that, compared to traditional order and
command-based interfaces, the proposed LLM-driven approach reduced task
completion time by approximately $64.2\%$ and improved task success rate by
$7\%$. It also leads to a considerable reduction in subjective cognitive
workload, with NASA-TLX scores dropping by $42.9\%$. This work establishes the
potential of LLMs to create more intuitive and effective human-swarm
collaborations in high-stakes scenarios.

</details>


### [5] [Enhancing Fault-Tolerant Space Computing: Guidance Navigation and Control (GNC) and Landing Vision System (LVS) Implementations on Next-Gen Multi-Core Processors](https://arxiv.org/abs/2511.04052)
*Kyongsik Yun,David Bayard,Gerik Kubiak,Austin Owens,Andrew Johnson,Ryan Johnson,Dan Scharf,Thomas Lu*

Main category: cs.RO

TL;DR: 本论文研究多核处理器在未来行星探索中的应用，提出ARBITER机制以增强计算的容错能力，并取得显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 未来的行星探索任务迫切需要高性能和容错计算，以支持自主导航与控制。

Method: 评估了多核处理器上GNC和LVS算法的部署，并提出ARBITER机制实现实时故障检测与恢复。

Result: 在与传统航天硬件相比，LVS图像处理速度提升达15倍，LFOLD轨迹优化的速度提升超过250倍，ARBITER机制有效提高了计算的可靠性。

Conclusion: 本研究为未来行星探索任务建立了一种可扩展且高效能的计算架构，特别是在自主性和故障恢复方面具有重要意义。

Abstract: Future planetary exploration missions demand high-performance, fault-tolerant
computing to enable autonomous Guidance, Navigation, and Control (GNC) and
Lander Vision System (LVS) operations during Entry, Descent, and Landing (EDL).
This paper evaluates the deployment of GNC and LVS algorithms on
next-generation multi-core processors--HPSC, Snapdragon VOXL2, and AMD Xilinx
Versal--demonstrating up to 15x speedup for LVS image processing and over 250x
speedup for Guidance for Fuel-Optimal Large Divert (GFOLD) trajectory
optimization compared to legacy spaceflight hardware. To ensure computational
reliability, we present ARBITER (Asynchronous Redundant Behavior Inspection for
Trusted Execution and Recovery), a Multi-Core Voting (MV) mechanism that
performs real-time fault detection and correction across redundant cores.
ARBITER is validated in both static optimization tasks (GFOLD) and dynamic
closed-loop control (Attitude Control System). A fault injection study further
identifies the gradient computation stage in GFOLD as the most sensitive to
bit-level errors, motivating selective protection strategies and vector-based
output arbitration. This work establishes a scalable and energy-efficient
architecture for future missions, including Mars Sample Return, Enceladus
Orbilander, and Ceres Sample Return, where onboard autonomy, low latency, and
fault resilience are critical.

</details>


### [6] [CBMC-V3: A CNS-inspired Control Framework Towards Manipulation Agility with SNN](https://arxiv.org/abs/2511.04109)
*Yanbo Pang,Qingkai Li,Mingguo Zhao*

Main category: cs.RO

TL;DR: 本文提出一种基于尖峰神经网络的生物仿生控制框架，解决现有控制算法在动态环境中的限制，经过验证的方法在柔性操控上优于工业标准。


<details>
  <summary>Details</summary>
Motivation: 随着机器人臂应用不断扩展至医疗、服务和日常生活，现有控制算法在复杂和动态环境中难以实现灵活操作。

Method: 利用基于尖峰神经网络（SNN）的控制模块，该框架分为五个模块和三个层次，通过反馈控制和强化学习动态调整参数。

Result: 通过仿真和实际机器人臂平台的验证，结果显示该方法在操作灵活性上超越了传统的工业级控制方法。

Conclusion: 该研究提出的生物仿生控制框架在灵活控制方面优于工业标准的位置控制，增强了机器人臂在动态环境中的操作能力。

Abstract: As robotic arm applications extend beyond industrial settings into
healthcare, service, and daily life, existing control algorithms struggle to
achieve the agile manipulation required for complex environments with dynamic
trajectories, unpredictable interactions, and diverse objects. This paper
presents a biomimetic control framework based on Spiking Neural Networks (SNN),
inspired by the human Central Nervous System (CNS), to achieve agile control in
such environments. The proposed framework features five control modules
(cerebral cortex, cerebellum, thalamus, brainstem, spinal cord), three
hierarchical control levels (first-order, second-order, third-order), and two
information pathways (ascending, descending). Each module is fully implemented
using SNN. The spinal cord module uses spike encoding and Leaky
Integrate-and-Fire (LIF) neurons for feedback control. The brainstem module
employs a network of LIF and non-spiking LIF neurons to dynamically adjust
spinal cord parameters via reinforcement learning. The thalamus module
similarly adjusts the cerebellum's torque outputs. The cerebellum module uses a
recurrent SNN to learn the robotic arm's dynamics through regression, providing
feedforward gravity compensation torques. The framework is validated both in
simulation and on real-world robotic arm platform under various loads and
trajectories. Results demonstrate that our method outperforms the
industrial-grade position control in manipulation agility.

</details>


### [7] [BFM-Zero: A Promptable Behavioral Foundation Model for Humanoid Control Using Unsupervised Reinforcement Learning](https://arxiv.org/abs/2511.04131)
*Yitang Li,Zhengyi Luo,Tonghe Zhang,Cunxi Dai,Anssi Kanervisto,Andrea Tirinzoni,Haoyang Weng,Kris Kitani,Mateusz Guzek,Ahmed Touati,Alessandro Lazaric,Matteo Pirotta,Guanya Shi*

Main category: cs.RO

TL;DR: BFM-Zero是一种新框架，通过共享潜在表示实现在真实环境中对多种人形机器人任务的有效控制，推动了可扩展行为基础模型的发展。


<details>
  <summary>Details</summary>
Motivation: 构建人形机器人行为基础模型可以统一多样的控制任务，但现有方法局限于模拟环境或特定任务。

Method: BFM-Zero框架通过共享潜在表示，将动作、目标和奖励嵌入共同空间，支持多种下游任务的提示，而无需重新训练。

Result: BFM-Zero在Unitree G1人形机器人上实现了多种下游任务的鲁棒全身技能，展示了其在现实世界中的有效性。

Conclusion: BFM-Zero展示了可扩展的通用行为基础模型在实际人形机器人控制中的应用潜力。

Abstract: Building Behavioral Foundation Models (BFMs) for humanoid robots has the
potential to unify diverse control tasks under a single, promptable generalist
policy. However, existing approaches are either exclusively deployed on
simulated humanoid characters, or specialized to specific tasks such as
tracking. We propose BFM-Zero, a framework that learns an effective shared
latent representation that embeds motions, goals, and rewards into a common
space, enabling a single policy to be prompted for multiple downstream tasks
without retraining. This well-structured latent space in BFM-Zero enables
versatile and robust whole-body skills on a Unitree G1 humanoid in the real
world, via diverse inference methods, including zero-shot motion tracking, goal
reaching, and reward optimization, and few-shot optimization-based adaptation.
Unlike prior on-policy reinforcement learning (RL) frameworks, BFM-Zero builds
upon recent advancements in unsupervised RL and Forward-Backward (FB) models,
which offer an objective-centric, explainable, and smooth latent representation
of whole-body motions. We further extend BFM-Zero with critical reward shaping,
domain randomization, and history-dependent asymmetric learning to bridge the
sim-to-real gap. Those key design choices are quantitatively ablated in
simulation. A first-of-its-kind model, BFM-Zero establishes a step toward
scalable, promptable behavioral foundation models for whole-body humanoid
control.

</details>


### [8] [PUL-SLAM: Path-Uncertainty Co-Optimization with Lightweight Stagnation Detection for Efficient Robotic Exploration](https://arxiv.org/abs/2511.04180)
*Yizhen Yin,Dapeng Feng,Hongbo Chen,Yuhua Qi*

Main category: cs.RO

TL;DR: 本文提出了一种混合框架，结合深度强化学习与轻量级检测机制，显著提高了主动SLAM的探索效率。


<details>
  <summary>Details</summary>
Motivation: 针对现有主动SLAM方法在探索速度慢和路径优化不足等问题进行改进。

Method: 采用路径不确定性协同优化深度强化学习框架和轻量级停滞检测机制的混合方法。

Result: 探索时间缩短最多达65%，路径距离减少最多达42%。

Conclusion: 该混合框架显著提高了复杂环境中的探索效率，同时保持了可靠的地图完整性。

Abstract: Existing Active SLAM methodologies face issues such as slow exploration speed
and suboptimal paths. To address these limitations, we propose a hybrid
framework combining a Path-Uncertainty Co-Optimization Deep Reinforcement
Learning framework and a Lightweight Stagnation Detection mechanism. The
Path-Uncertainty Co-Optimization framework jointly optimizes travel distance
and map uncertainty through a dual-objective reward function, balancing
exploration and exploitation. The Lightweight Stagnation Detection reduces
redundant exploration through Lidar Static Anomaly Detection and Map Update
Stagnation Detection, terminating episodes on low expansion rates. Experimental
results show that compared with the frontier-based method and RRT method, our
approach shortens exploration time by up to 65% and reduces path distance by up
to 42%, significantly improving exploration efficiency in complex environments
while maintaining reliable map completeness. Ablation studies confirm that the
collaborative mechanism accelerates training convergence. Empirical validation
on a physical robotic platform demonstrates the algorithm's practical
applicability and its successful transferability from simulation to real-world
environments.

</details>


### [9] [GraspView: Active Perception Scoring and Best-View Optimization for Robotic Grasping in Cluttered Environments](https://arxiv.org/abs/2511.04199)
*Shenglin Wang,Mingtong Dai,Jingxuan Su,Lingbo Liu,Chunjie Chen,Xinyu Wu,Liang Lin*

Main category: cs.RO

TL;DR: GraspView是一个无深度传感器的RGB单一抓取体系，结合多项技术，显示出在复杂环境中相比传统RGB-D系统具有更高的抓取可靠性和和适应性。


<details>
  <summary>Details</summary>
Motivation: 在杂乱环境中，传统的RGB-D摄像头在处理透明或光滑物体时表现不佳，无法提供稳定的抓取；因此，需要开发一种新的方法来实现精准的操作。

Method: GraspView集成了三大关键组件：全局感知场景重建、渲染与评分的主动感知策略、在线度量对齐模块。

Result: GraspView在各种桌面物体实验中，表现显著优于RGB-D和单视图RGB基线，尤其在重度遮挡、近场传感和透明物体的情况下。

Conclusion: GraspView是一个有效且多功能的RGB-only抓取管道，能够在无结构的真实环境中实现可靠的抓取。

Abstract: Robotic grasping is a fundamental capability for autonomous manipulation, yet
remains highly challenging in cluttered environments where occlusion, poor
perception quality, and inconsistent 3D reconstructions often lead to unstable
or failed grasps. Conventional pipelines have widely relied on RGB-D cameras to
provide geometric information, which fail on transparent or glossy objects and
degrade at close range. We present GraspView, an RGB-only robotic grasping
pipeline that achieves accurate manipulation in cluttered environments without
depth sensors. Our framework integrates three key components: (i) global
perception scene reconstruction, which provides locally consistent, up-to-scale
geometry from a single RGB view and fuses multi-view projections into a
coherent global 3D scene; (ii) a render-and-score active perception strategy,
which dynamically selects next-best-views to reveal occluded regions; and (iii)
an online metric alignment module that calibrates VGGT predictions against
robot kinematics to ensure physical scale consistency. Building on these
tailor-designed modules, GraspView performs best-view global grasping, fusing
multi-view reconstructions and leveraging GraspNet for robust execution.
Experiments on diverse tabletop objects demonstrate that GraspView
significantly outperforms both RGB-D and single-view RGB baselines, especially
under heavy occlusion, near-field sensing, and with transparent objects. These
results highlight GraspView as a practical and versatile alternative to RGB-D
pipelines, enabling reliable grasping in unstructured real-world environments.

</details>


### [10] [Can Context Bridge the Reality Gap? Sim-to-Real Transfer of Context-Aware Policies](https://arxiv.org/abs/2511.04249)
*Marco Iannotta,Yuxuan Yang,Johannes A. Stork,Erik Schaffernicht,Todor Stoyanov*

Main category: cs.RO

TL;DR: 通过上下文估计模块和领域随机化，提出了一种改进的强化学习策略，能在仿真到现实的转移中提高政策泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决在强化学习中进行的仿真到现实转移的挑战，特别是训练在仿真中表现良好，但在现实世界中效果不佳的问题。

Method: 将上下文估计模块集成到基于领域随机化的强化学习框架中，并系统性比较最先进的监督策略。

Result: 上下文感知策略在典型控制基准测试和真实世界推挤任务中表现出色，超越了无上下文策略的基线。

Conclusion: 上下文感知策略在所有设置中均优于上下文无关的基线，尽管最佳监督策略因任务而异。

Abstract: Sim-to-real transfer remains a major challenge in reinforcement learning (RL)
for robotics, as policies trained in simulation often fail to generalize to the
real world due to discrepancies in environment dynamics. Domain Randomization
(DR) mitigates this issue by exposing the policy to a wide range of randomized
dynamics during training, yet leading to a reduction in performance. While
standard approaches typically train policies agnostic to these variations, we
investigate whether sim-to-real transfer can be improved by conditioning the
policy on an estimate of the dynamics parameters -- referred to as context. To
this end, we integrate a context estimation module into a DR-based RL framework
and systematically compare SOTA supervision strategies. We evaluate the
resulting context-aware policies in both a canonical control benchmark and a
real-world pushing task using a Franka Emika Panda robot. Results show that
context-aware policies outperform the context-agnostic baseline across all
settings, although the best supervision strategy depends on the task.

</details>


### [11] [Design and Control of a Coaxial Dual-rotor Reconfigurable Tailsitter UAV Based on Swashplateless Mechanism](https://arxiv.org/abs/2511.04251)
*Jinfeng Liang,Haocheng Guo,Ximin Lyu*

Main category: cs.RO

TL;DR: 本文提出一种改进的尾摆机 UAV 设计，采用可重配置机翼和同轴双旋翼配置，以应对风干扰并提高能效，最终通过飞行测试验证其性能。


<details>
  <summary>Details</summary>
Motivation: 针对尾摆机 UAV 在多旋翼模式下易受风干扰的问题，提出了一种改进的设计以提高其动力效率和结构简化。

Method: 采用可重配置的机翼设计和同轴异构双旋翼配置，优化了无摇杆机构，并通过增加拍打铰链来减少振动。

Result: 通过新设计的舵机机制和减少的总体功耗，实现了尾摆机在固定翼和多旋翼模式下的有效转换和稳定飞行。

Conclusion: 对尾摆机 UAV 的飞行性能进行了全面的过渡飞行测试，验证了其在整个飞行动作范围内的稳定飞行性能。

Abstract: The tailsitter vertical takeoff and landing (VTOL) UAV is widely used due to
its lower dead weight, which eliminates the actuators and mechanisms for
tilting. However, the tailsitter UAV is susceptible to wind disturbances in
multi-rotor mode, as it exposes a large frontal fuselage area. To address this
issue, our tailsitter UAV features a reconfigurable wing design, allowing wings
to retract in multi-rotor mode and extend in fixed- wing mode. Considering
power efficiency, we design a coaxial heterogeneous dual-rotor configuration,
which significantly re- duces the total power consumption. To reduce structural
weight and simplify structural complexity, we employ a swashplateless mechanism
with an improved design to control pitch and roll in multi-rotor mode. We
optimize the structure of the swashplateless mechanism by adding flapping
hinges, which reduces vibration during cyclic acceleration and deceleration.
Finally, we perform comprehensive transition flight tests to validate stable
flight performance across the entire flight envelope of the tailsitter UAV.

</details>


### [12] [MacroNav: Multi-Task Context Representation Learning Enables Efficient Navigation in Unknown Environments](https://arxiv.org/abs/2511.04320)
*Kuankuan Sima,Longbin Tang,Haozhe Ma,Lin Zhao*

Main category: cs.RO

TL;DR: MacroNav是一个学习驱动的导航框架，通过上下文编码器和强化学习，解决了未知环境中导航效率和空间理解的挑战，在实验中表现优越。


<details>
  <summary>Details</summary>
Motivation: 在未知环境中自主导航需在部分可观察状态下，实现紧凑而富有表现力的空间理解，以支持高层决策。

Method: 采用学习-based方法，结合轻量级的上下文编码器和强化学习策略，通过图推理实现高效的动作选择。

Result: 通过大量实验和实际部署，MacroNav显示出了其环境理解的高效性与稳健性，成功率和路径长度加权成功率均显著提高。

Conclusion: MacroNav在导航效率和环境理解方面大大优于现有方法，实现了更高的成功率和路径长度加权成功率，且计算成本低。

Abstract: Autonomous navigation in unknown environments requires compact yet expressive
spatial understanding under partial observability to support high-level
decision making. Existing approaches struggle to balance rich contextual
representation with navigation efficiency. We present MacroNav, a
learning-based navigation framework featuring two key components: (1) a
lightweight context encoder trained via multi-task self-supervised learning to
capture multi-scale, navigation-centric spatial representations; and (2) a
reinforcement learning policy that seamlessly integrates these representations
with graph-based reasoning for efficient action selection. Extensive
experiments demonstrate the context encoder's efficient and robust
environmental understanding. Real-world deployments further validate MacroNav's
effectiveness, yielding significant gains over state-of-the-art navigation
methods in both Success Rate (SR) and Success weighted by Path Length (SPL),
while maintaining low computational cost. Code will be released upon
acceptance.

</details>


### [13] [GraSP-VLA: Graph-based Symbolic Action Representation for Long-Horizon Planning with VLA Policies](https://arxiv.org/abs/2511.04357)
*Maëlic Neau,Zoe Falomir,Paulo E. Santos,Anne-Gwenn Bosser,Cédric Buche*

Main category: cs.RO

TL;DR: 本研究提出GraSP-VLA，这是一种新的神经符号方法，可以有效进行计划域生成，并在长时间任务中展示了良好的协调能力。


<details>
  <summary>Details</summary>
Motivation: 现代机器人面临从演示中学习新技能的挑战，现有的解决方案在高水平符号规划和可扩展性上存在不足。

Method: 提出了一种新的神经符号方法GraSP-VLA，通过连续场景图表示生成符号表示，并用于推理期间的新规划领域生成。

Result: GraSP-VLA在自动规划领域生成及在长时间任务中协调低级VLA策略方面取得了显著效果。

Conclusion: GraSP-VLA在自动规划领域生成方面表现有效，并显示出在长时间任务中协调低级VLA策略的潜力。

Abstract: Deploying autonomous robots that can learn new skills from demonstrations is
an important challenge of modern robotics. Existing solutions often apply
end-to-end imitation learning with Vision-Language Action (VLA) models or
symbolic approaches with Action Model Learning (AML). On the one hand, current
VLA models are limited by the lack of high-level symbolic planning, which
hinders their abilities in long-horizon tasks. On the other hand, symbolic
approaches in AML lack generalization and scalability perspectives. In this
paper we present a new neuro-symbolic approach, GraSP-VLA, a framework that
uses a Continuous Scene Graph representation to generate a symbolic
representation of human demonstrations. This representation is used to generate
new planning domains during inference and serves as an orchestrator for
low-level VLA policies, scaling up the number of actions that can be reproduced
in a row. Our results show that GraSP-VLA is effective for modeling symbolic
representations on the task of automatic planning domain generation from
observations. In addition, results on real-world experiments show the potential
of our Continuous Scene Graph representation to orchestrate low-level VLA
policies in long-horizon tasks.

</details>


### [14] [Studying the Effect of Explicit Interaction Representations on Learning Scene-level Distributions of Human Trajectories](https://arxiv.org/abs/2511.04375)
*Anna Mészáros,Javier Alonso-Mora,Jens Kober*

Main category: cs.RO

TL;DR: 本文研究了代理间交互的表示对联合分布学习的影响，发现明确定义的交互关系能提升模型性能，而依赖数据学习的交互则常导致性能下降。


<details>
  <summary>Details</summary>
Motivation: 尽管近年来为此目的开发了新的模型，但如何最佳地表示联合分布，尤其是从代理间的交互角度，仍不清楚。

Method: 本研究探讨了在同一网络结构中描述代理之间交互的多种方式及其对最终学习的联合分布的影响。

Result: 研究表明，明确定义的交互（例如交叉口代理对中哪个代理先通过）通常能显著提升性能，而让网络根据数据建立交互连接则会对性能产生负面影响。

Conclusion: 在场景中有效捕获所有代理的联合分布对于预测场景的真实演变以及提供更准确的信息给自动驾驶决策过程至关重要。通过明确定义互动关系可以显著提升性能，而仅依赖网络基于数据建立互动连接通常会导致性能下降。

Abstract: Effectively capturing the joint distribution of all agents in a scene is
relevant for predicting the true evolution of the scene and in turn providing
more accurate information to the decision processes of autonomous vehicles.
While new models have been developed for this purpose in recent years, it
remains unclear how to best represent the joint distributions particularly from
the perspective of the interactions between agents. Thus far there is no clear
consensus on how best to represent interactions between agents; whether they
should be learned implicitly from data by neural networks, or explicitly
modeled using the spatial and temporal relations that are more grounded in
human decision-making. This paper aims to study various means of describing
interactions within the same network structure and their effect on the final
learned joint distributions. Our findings show that more often than not, simply
allowing a network to establish interactive connections between agents based on
data has a detrimental effect on performance. Instead, having well defined
interactions (such as which agent of an agent pair passes first at an
intersection) can often bring about a clear boost in performance.

</details>


### [15] [ForeRobo: Unlocking Infinite Simulation Data for 3D Goal-driven Robotic Manipulation](https://arxiv.org/abs/2511.04381)
*Dexin wang,Faliang Chang,Chunsheng Liu*

Main category: cs.RO

TL;DR: ForeRobo通过生成仿真结合经典控制，实现了高效的操作技能学习，展示出在现实世界任务中的强大推广能力和成功率。


<details>
  <summary>Details</summary>
Motivation: 高效利用仿真技术获取高级操作技能是一项重要且有挑战性的任务。

Method: 介绍了一种生成型机器人代理ForeRobo，结合生成仿真与经典控制，通过自我指导的	extit{propose-generate-learn-actuate}循环获取操作技能。

Result: ForeRobo在各种操作任务中相较于最先进的状态生成模型平均提高了56.32%。

Conclusion: ForeRobo展示了卓越的推广能力和在现实世界中的成功表现，达到了79.28%的平均成功率。

Abstract: Efficiently leveraging simulation to acquire advanced manipulation skills is
both challenging and highly significant. We introduce \textit{ForeRobo}, a
generative robotic agent that utilizes generative simulations to autonomously
acquire manipulation skills driven by envisioned goal states. Instead of
directly learning low-level policies, we advocate integrating generative
paradigms with classical control. Our approach equips a robotic agent with a
self-guided \textit{propose-generate-learn-actuate} cycle. The agent first
proposes the skills to be acquired and constructs the corresponding simulation
environments; it then configures objects into appropriate arrangements to
generate skill-consistent goal states (\textit{ForeGen}). Subsequently, the
virtually infinite data produced by ForeGen are used to train the proposed
state generation model (\textit{ForeFormer}), which establishes point-wise
correspondences by predicting the 3D goal position of every point in the
current state, based on the scene state and task instructions. Finally,
classical control algorithms are employed to drive the robot in real-world
environments to execute actions based on the envisioned goal states. Compared
with end-to-end policy learning methods, ForeFormer offers superior
interpretability and execution efficiency. We train and benchmark ForeFormer
across a variety of rigid-body and articulated-object manipulation tasks, and
observe an average improvement of 56.32\% over the state-of-the-art state
generation models, demonstrating strong generality across different
manipulation patterns. Moreover, in real-world evaluations involving more than
20 robotic tasks, ForeRobo achieves zero-shot sim-to-real transfer and exhibits
remarkable generalization capabilities, attaining an average success rate of
79.28\%.

</details>


### [16] [Temporal Action Selection for Action Chunking](https://arxiv.org/abs/2511.04421)
*Yueyang Weng,Xiaopeng Zhang,Yongjin Mu,Yingcong Zhu,Yanjie Li,Qi Liu*

Main category: cs.RO

TL;DR: 本文提出了一种新算法TAS，有效解决了动作分块在反应性与决策一致性之间的权衡问题，实现了成功率显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决现有动作分块方法中反应性不足的问题，尤其是在传感器噪声和动态环境变化下的适应性差。

Method: 提出了一种新算法TAS，通过缓存来自多个时间步的预测动作块，并通过轻量化的选择网络动态选择最佳动作。

Result: 在多项任务的实验中，TAS的成功率提升了高达73.3%。在集成TAS和残差强化学习后，训练效率和性能平台都有显著提高。

Conclusion: TAS显著提高了成功率，并在训练效率和性能上带来了显著提升。

Abstract: Action chunking is a widely adopted approach in Learning from Demonstration
(LfD). By modeling multi-step action chunks rather than single-step actions,
action chunking significantly enhances modeling capabilities for human expert
policies. However, the reduced decision frequency restricts the utilization of
recent observations, degrading reactivity - particularly evident in the
inadequate adaptation to sensor noise and dynamic environmental changes.
Existing efforts to address this issue have primarily resorted to trading off
reactivity against decision consistency, without achieving both. To address
this limitation, we propose a novel algorithm, Temporal Action Selector (TAS),
which caches predicted action chunks from multiple timesteps and dynamically
selects the optimal action through a lightweight selector network. TAS achieves
balanced optimization across three critical dimensions: reactivity, decision
consistency, and motion coherence. Experiments across multiple tasks with
diverse base policies show that TAS significantly improves success rates -
yielding an absolute gain of up to 73.3%. Furthermore, integrating TAS as a
base policy with residual reinforcement learning (RL) substantially enhances
training efficiency and elevates the performance plateau. Experiments in both
simulation and physical robots confirm the method's efficacy.

</details>


### [17] [Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment](https://arxiv.org/abs/2511.04555)
*Tao Lin,Yilei Zhong,Yuxin Du,Jingjing Zhang,Jiting Liu,Yinxinyu Chen,Encheng Gu,Ziyan Liu,Hongyi Cai,Yanwen Zou,Lixing Zou,Zhaoye Zhou,Gen Li,Bo Zhao*

Main category: cs.RO

TL;DR: Evo-1是一个高效的轻量化VLA模型，降低了计算开销，提升了部署效率，在多个基准测试中表现优异，无需依赖机器人数据预训练。


<details>
  <summary>Details</summary>
Motivation: 当前的VLA模型参数庞大，对大规模机器人数据预训练依赖强，导致训练时计算成本高，实时推理时可部署性差，同时也降低了感知表示的质量，出现过拟合和泛化能力差的问题。

Method: Evo-1采用了交叉调制扩散变换器和优化的集成模块，并引入了分阶段的训练范式，使动作与感知逐步对齐，保持视觉-语言模型的表现。

Result: Evo-1仅使用0.77亿参数，在Meta-World和RoboTwin测试集中分别超越了之前最佳模型12.4%和6.9%，在LIBERO上的成绩也达到了94.8%。在实际评估中，Evo-1的成功率达到78%，推理频率高且内存开销小，优于所有基线方法。

Conclusion: Evo-1是一个轻量级的视觉-语言-动作（VLA）模型，具有较低的计算需求和高效的部署能力，能够在不依赖机器人数据预训练的情况下保持强大的性能。

Abstract: Vision-Language-Action (VLA) models have emerged as a powerful framework that
unifies perception, language, and control, enabling robots to perform diverse
tasks through multimodal understanding. However, current VLA models typically
contain massive parameters and rely heavily on large-scale robot data
pretraining, leading to high computational costs during training, as well as
limited deployability for real-time inference. Moreover, most training
paradigms often degrade the perceptual representations of the vision-language
backbone, resulting in overfitting and poor generalization to downstream tasks.
In this work, we present Evo-1, a lightweight VLA model that reduces
computation and improves deployment efficiency, while maintaining strong
performance without pretraining on robot data. Evo-1 builds on a native
multimodal Vision-Language model (VLM), incorporating a novel cross-modulated
diffusion transformer along with an optimized integration module, together
forming an effective architecture. We further introduce a two-stage training
paradigm that progressively aligns action with perception, preserving the
representations of the VLM. Notably, with only 0.77 billion parameters, Evo-1
achieves state-of-the-art results on the Meta-World and RoboTwin suite,
surpassing the previous best models by 12.4% and 6.9%, respectively, and also
attains a competitive result of 94.8% on LIBERO. In real-world evaluations,
Evo-1 attains a 78% success rate with high inference frequency and low memory
overhead, outperforming all baseline methods. We release code, data, and model
weights to facilitate future research on lightweight and efficient VLA models.

</details>


### [18] [GentleHumanoid: Learning Upper-body Compliance for Contact-rich Human and Object Interaction](https://arxiv.org/abs/2511.04679)
*Qingzhou Lu,Yao Feng,Baiyu Shi,Michael Piseno,Zhenan Bao,C. Karen Liu*

Main category: cs.RO

TL;DR: GentleHumanoid框架通过集成阻抗控制与运动追踪，实现了人形机器人在与人类互动时的顺应性和安全性。


<details>
  <summary>Details</summary>
Motivation: 针对目前强化学习政策在与外部力量互动时过于刚性的问题，提出了一种新的框架来实现更自然和安全的互动。

Method: 引入了一种统一的基于弹簧的控制机制，集成阻抗控制与全身运动跟踪策略，着重于上半身的顺应性。

Result: GentleHumanoid在模拟和Unitree G1人形机器人上进行评估，展示了在温和拥抱、起坐辅助和安全物体操作等任务中，峰值接触力显著降低，同时任务成功率维持在高水平。

Conclusion: 该研究的结果表明，GentleHumanoid框架能够有效地提高人形机器人在与人类互动时的安全性和自然性，从而推动人形机器人在真实环境中的应用。

Abstract: Humanoid robots are expected to operate in human-centered environments where
safe and natural physical interaction is essential. However, most recent
reinforcement learning (RL) policies emphasize rigid tracking and suppress
external forces. Existing impedance-augmented approaches are typically
restricted to base or end-effector control and focus on resisting extreme
forces rather than enabling compliance. We introduce GentleHumanoid, a
framework that integrates impedance control into a whole-body motion tracking
policy to achieve upper-body compliance. At its core is a unified spring-based
formulation that models both resistive contacts (restoring forces when pressing
against surfaces) and guiding contacts (pushes or pulls sampled from human
motion data). This formulation ensures kinematically consistent forces across
the shoulder, elbow, and wrist, while exposing the policy to diverse
interaction scenarios. Safety is further supported through task-adjustable
force thresholds. We evaluate our approach in both simulation and on the
Unitree G1 humanoid across tasks requiring different levels of compliance,
including gentle hugging, sit-to-stand assistance, and safe object
manipulation. Compared to baselines, our policy consistently reduces peak
contact forces while maintaining task success, resulting in smoother and more
natural interactions. These results highlight a step toward humanoid robots
that can safely and effectively collaborate with humans and handle objects in
real-world environments.

</details>


### [19] [SAFe-Copilot: Unified Shared Autonomy Framework](https://arxiv.org/abs/2511.04664)
*Phat Nguyen,Erfan Aasi,Shiva Sreeram,Guy Rosman,Andrew Silva,Sertac Karaman,Daniela Rus*

Main category: cs.RO

TL;DR: 本研究提出了一种高级别的共享自主框架，利用视觉语言模型推断驾驶意图，显著提高自主驾驶系统在复杂场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 针对现有方法只限于低层次轨迹，无法捕捉驾驶意图的问题，提出现有共享自主方法在复杂以及不确定情况下的局限性。

Method: 利用视觉语言模型（VLM）从多模态线索中推断驾驶意图，并综合人类和自主控制的策略。

Result: 在Mock-Human设置下取得完美的回忆率和高准确率；在97%的情况下，参与者与裁决结果高度一致，并在Bench2Drive基准测试中表现出优越的性能和降低的碰撞率。

Conclusion: 提出了一种统一的共享自主框架，该框架通过语义和语言基元的表示实现人类输入和自主规划的整合，显著提升了系统在复杂场景下的表现。

Abstract: Autonomous driving systems remain brittle in rare, ambiguous, and
out-of-distribution scenarios, where human driver succeed through contextual
reasoning. Shared autonomy has emerged as a promising approach to mitigate such
failures by incorporating human input when autonomy is uncertain. However, most
existing methods restrict arbitration to low-level trajectories, which
represent only geometric paths and therefore fail to preserve the underlying
driving intent. We propose a unified shared autonomy framework that integrates
human input and autonomous planners at a higher level of abstraction. Our
method leverages Vision Language Models (VLMs) to infer driver intent from
multi-modal cues -- such as driver actions and environmental context -- and to
synthesize coherent strategies that mediate between human and autonomous
control. We first study the framework in a mock-human setting, where it
achieves perfect recall alongside high accuracy and precision. A human-subject
survey further shows strong alignment, with participants agreeing with
arbitration outcomes in 92% of cases. Finally, evaluation on the Bench2Drive
benchmark demonstrates a substantial reduction in collision rate and
improvement in overall performance compared to pure autonomy. Arbitration at
the level of semantic, language-based representations emerges as a design
principle for shared autonomy, enabling systems to exercise common-sense
reasoning and maintain continuity with human intent.

</details>


### [20] [Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation of Soft-Body Interactions](https://arxiv.org/abs/2511.04665)
*Kaifeng Zhang,Shuo Sha,Hanxiao Jiang,Matthew Loper,Hyunjong Song,Guangyan Cai,Zhuo Xu,Xiaochen Hu,Changxi Zheng,Yunzhu Li*

Main category: cs.RO

TL;DR: 本文提出了一种从现实视频生成软体数字双胞胎的框架，能够以高保真度评估机器人操控策略，验证了其在变形操控任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 机器人操控策略的快速发展使得直接在现实中评估变得成本高昂且难以重现，特别是在处理变形物体的任务中。

Method: 构建从现实世界视频生成的软体数字双胞胎，并使用3D高斯点云渲染出具有照片真实感的机器人、物体和环境。

Result: 模拟结果与现实执行性能强相关，并揭示了学习策略的关键行为模式。

Conclusion: 通过将物理信息重建与高质量渲染相结合，能够实现机器人操控策略的可重复、可扩展和准确的评估。

Abstract: Robotic manipulation policies are advancing rapidly, but their direct
evaluation in the real world remains costly, time-consuming, and difficult to
reproduce, particularly for tasks involving deformable objects. Simulation
provides a scalable and systematic alternative, yet existing simulators often
fail to capture the coupled visual and physical complexity of soft-body
interactions. We present a real-to-sim policy evaluation framework that
constructs soft-body digital twins from real-world videos and renders robots,
objects, and environments with photorealistic fidelity using 3D Gaussian
Splatting. We validate our approach on representative deformable manipulation
tasks, including plush toy packing, rope routing, and T-block pushing,
demonstrating that simulated rollouts correlate strongly with real-world
execution performance and reveal key behavioral patterns of learned policies.
Our results suggest that combining physics-informed reconstruction with
high-quality rendering enables reproducible, scalable, and accurate evaluation
of robotic manipulation policies. Website: https://real2sim-eval.github.io/

</details>


### [21] [X-Diffusion: Training Diffusion Policies on Cross-Embodiment Human Demonstrations](https://arxiv.org/abs/2511.04671)
*Maximus A. Pace,Prithwish Dan,Chuanruo Ning,Atiksh Bhardwaj,Audrey Du,Edward W. Duan,Wei-Chiu Ma,Kushal Kedia*

Main category: cs.RO

TL;DR: 本研究提出的X-Diffusion框架有效利用人类视频数据，通过扩散过程处理，实现机器人操作成功率的显著提升。


<details>
  <summary>Details</summary>
Motivation: 利用人类录像快速生成训练数据，同时克服人类与机器人之间的动作执行差异，以提高机器人学习的效果。

Method: 使用扩散过程作为框架，通过加噪声处理来从人类演示中抽取可行的机器人动作。

Result: X-Diffusion在五个操作任务中实现了比最佳基线高出16%的平均成功率，展示了其优越性。

Conclusion: X-Diffusion在执行机器人的操作时有效利用了人类的视频数据，克服了人类与机器人之间的执行差异，取得了显著的成功率提升。

Abstract: Human videos can be recorded quickly and at scale, making them an appealing
source of training data for robot learning. However, humans and robots differ
fundamentally in embodiment, resulting in mismatched action execution. Direct
kinematic retargeting of human hand motion can therefore produce actions that
are physically infeasible for robots. Despite these low-level differences,
human demonstrations provide valuable motion cues about how to manipulate and
interact with objects. Our key idea is to exploit the forward diffusion
process: as noise is added to actions, low-level execution differences fade
while high-level task guidance is preserved. We present X-Diffusion, a
principled framework for training diffusion policies that maximally leverages
human data without learning dynamically infeasible motions. X-Diffusion first
trains a classifier to predict whether a noisy action is executed by a human or
robot. Then, a human action is incorporated into policy training only after
adding sufficient noise such that the classifier cannot discern its embodiment.
Actions consistent with robot execution supervise fine-grained denoising at low
noise levels, while mismatched human actions provide only coarse guidance at
higher noise levels. Our experiments show that naive co-training under
execution mismatches degrades policy performance, while X-Diffusion
consistently improves it. Across five manipulation tasks, X-Diffusion achieves
a 16% higher average success rate than the best baseline. The project website
is available at https://portal-cornell.github.io/X-Diffusion/.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [22] [MazeMate: An LLM-Powered Chatbot to Support Computational Thinking in Gamified Programming Learning](https://arxiv.org/abs/2511.03727)
*Chenyu Hou,Hua Yu,Gaoxia Zhu,John Derek Anas,Jiao Liu,Yew Soon Ong*

Main category: cs.HC

TL;DR: 本研究介绍了MazeMate，一个基于大型语言模型的聊天机器人，旨在提升计算思维，在迷宫求解和设计中提供支持，初步实验结果表明有助于计算思维的某些过程，但在设计指导方面存在显著局限。


<details>
  <summary>Details</summary>
Motivation: 随着编程环境的游戏化，当前应用尚未充分促进计算思维的发展，亟需优化。

Method: 通过在247名本科生中实施MazeMate进行课堂实验，收集学生对其帮助程度的反馈。

Result: 学生对MazeMate的评价中等，迷宫求解的感知效用高于迷宫设计，并在主题分析中发现支撑计算思维的过程，存在建议不匹配和虚构算法解决方案的局限。

Conclusion: LLM驱动的支架在支持计算思维中具有潜力，但在迷宫设计方面存在改进空间。

Abstract: Computational Thinking (CT) is a foundational problem-solving skill, and
gamified programming environments are a widely adopted approach to cultivating
it. While large language models (LLMs) provide on-demand programming support,
current applications rarely foster CT development. We present MazeMate, an
LLM-powered chatbot embedded in a 3D Maze programming game, designed to deliver
adaptive, context-sensitive scaffolds aligned with CT processes in maze solving
and maze design. We report on the first classroom implementation with 247
undergraduates. Students rated MazeMate as moderately helpful, with higher
perceived usefulness for maze solving than for maze design. Thematic analysis
confirmed support for CT processes such as decomposition, abstraction, and
algorithmic thinking, while also revealing limitations in supporting maze
design, including mismatched suggestions and fabricated algorithmic solutions.
These findings demonstrate the potential of LLM-based scaffolding to support CT
and underscore directions for design refinement to enhance MazeMate usability
in authentic classrooms.

</details>


### [23] [Efficient On-Device Agents via Adaptive Context Management](https://arxiv.org/abs/2511.03728)
*Sanidhya Vijayvargiya,Rahul Lokesh*

Main category: cs.HC

TL;DR: 本研究提出了一种新框架，通过优化内存和上下文管理，提高了设备端AI代理的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 设备端AI代理潜在的个性化和低延迟辅助功能受到内存容量限制的制约，导致可用上下文受限。

Method: 提出了一种基于动态内存系统、最简化序列化格式和即时模式传递机制的框架，以优化设备端代理的上下文效率。

Result: 该代理的表现与常规基准相匹配或超越，同时显著压缩上下文，实现了超过6倍的初始系统提示上下文压缩和10到25倍的上下文增长率减少。

Conclusion: 通过战略性上下文管理，可以释放具备能力和持久性的设备端AI的潜力。

Abstract: On-device AI agents offer the potential for personalized, low-latency
assistance, but their deployment is fundamentally constrained by limited memory
capacity, which restricts usable context. This reduced practical context window
creates a trade-off between supporting rich, stateful interactions with complex
tool capabilities and maintaining on-device feasibility. We break this
trade-off with a framework for context-efficient on-device agents, driven by
three synergistic optimizations (1) a dynamic memory system using specialized
LoRA adapters to distill conversational history into a compressed, and
structured Context State Object; (2) a minimalist serialization format for tool
schemas to minimize token overhead per tool; and (3) a just-in-time
schema-passing mechanism that loads full tool definitions only upon tool
selection. We instantiate this framework by adapting a 3B parameter SLM to
context-efficient trajectories and rigorously evaluate it against a
conventional baseline on complex user tasks. Our agent matches, or exceeds, the
performance of a conventional baseline while dramatically compressing context,
achieving more than a 6-fold reduction in initial system prompt context and a
10- to 25-fold reduction in context growth rate based on the interaction
verbosity, demonstrating that strategic context management is key to unlocking
capable and persistent on-device AI.

</details>


### [24] [Beyond Chat: a Framework for LLMs as Human-Centered Support Systems](https://arxiv.org/abs/2511.03729)
*Zhiyin Zhou*

Main category: cs.HC

TL;DR: 本论文探讨了如何将大型语言模型作为人类中心的支持系统，提出了设计原则及评估方法，并分析了潜在风险和未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 为了支持人类的成长、决策和福祉，推动大型语言模型超越传统的问答角色。

Method: 通过提出基于角色的框架和评估跨领域的实际应用，分析设计原则和评价指标。

Result: 提出了跨领域的设计原则、评估指标，并分析了潜在风险和未来发展方向。

Conclusion: 本论文强调了在敏感环境中引入大型语言模型时，需要负责任地整合以确保适当的陪伴和指导。

Abstract: Large language models are moving beyond transactional question answering to
act as companions, coaches, mediators, and curators that scaffold human growth,
decision-making, and well-being. This paper proposes a role-based framework for
human-centered LLM support systems, compares real deployments across domains,
and identifies cross-cutting design principles: transparency, personalization,
guardrails, memory with privacy, and a balance of empathy and reliability. It
outlines evaluation metrics that extend beyond accuracy to trust, engagement,
and longitudinal outcomes. It also analyzes risks including over-reliance,
hallucination, bias, privacy exposure, and unequal access, and proposes future
directions spanning unified evaluation, hybrid human-AI models, memory
architectures, cross-domain benchmarking, and governance. The goal is to
support responsible integration of LLMs in sensitive settings where people need
accompaniment and guidance, not only answers.

</details>


### [25] [Not All Explanations are Created Equal: Investigating the Pitfalls of Current XAI Evaluation](https://arxiv.org/abs/2511.03730)
*Joe Shymanski,Jacob Brue,Sandip Sen*

Main category: cs.HC

TL;DR: 本研究指出当前XAI模型评估的不足之处，强调必须开发更全面的评估技术，以验证解释质量，而不仅仅依赖于用户满意度。


<details>
  <summary>Details</summary>
Motivation: 当前XAI评估技术存在临时性和局限性，大多数研究依赖简单的用户调查来比较有无解释的效果，这样的评估不足以证明解释的质量。

Method: 使用代理助手教授用户国际象棋概念的实验方法。

Result: 通过研究，发现大多数解释无论质量如何，都能提升用户满意度，并强调采取有行动性的解释的重要性。

Conclusion: 未来的XAI研究需要建立更全面的评估技术，以便能够评判解释质量，而不仅仅是用户满意度。

Abstract: Explainable Artificial Intelligence (XAI) aims to create transparency in
modern AI models by offering explanations of the models to human users. There
are many ways in which researchers have attempted to evaluate the quality of
these XAI models, such as user studies or proposed objective metrics like
"fidelity". However, these current XAI evaluation techniques are ad hoc at best
and not generalizable. Thus, most studies done within this field conduct simple
user surveys to analyze the difference between no explanations and those
generated by their proposed solution. We do not find this to provide adequate
evidence that the explanations generated are of good quality since we believe
any kind of explanation will be "better" in most metrics when compared to none
at all. Thus, our study looks to highlight this pitfall: most explanations,
regardless of quality or correctness, will increase user satisfaction. We also
propose that emphasis should be placed on actionable explanations. We
demonstrate the validity of both of our claims using an agent assistant to
teach chess concepts to users. The results of this chapter will act as a call
to action in the field of XAI for more comprehensive evaluation techniques for
future research in order to prove explanation quality beyond user satisfaction.
Additionally, we present an analysis of the scenarios in which placebic or
actionable explanations would be most useful.

</details>


### [26] [MimiTalk: Revolutionizing Qualitative Research with Dual-Agent AI](https://arxiv.org/abs/2511.03731)
*Fengming Liu,Shubin Yu*

Main category: cs.HC

TL;DR: MimiTalk是一个双代理宪法AI框架，优化了定性研究中的人机互动，提高了访谈的质量和有效性。


<details>
  <summary>Details</summary>
Motivation: 开发一个可扩展和伦理的对话数据收集框架，以支持社会科学研究。

Method: 通过三项研究评估MimiTalk框架：1) 评估20名参与者的可用性；2) 对比121个AI访谈和1271个人类访谈；3) 10名跨学科研究者进行人机访谈后进行盲目主题分析。

Result: MimiTalk在减轻访谈焦虑、保持对话连贯性方面表现良好，且在信息丰富性、连贯性和稳定性上超过人类访谈。AI访谈能引出技术见解与敏感话题的坦诚看法，而人类访谈则更能捕捉文化和情感的细微差别。

Conclusion: 双代理宪法AI支持有效的人机合作，能够实现可复制、可扩展和质量控制的定性研究。

Abstract: We present MimiTalk, a dual-agent constitutional AI framework designed for
scalable and ethical conversational data collection in social science research.
The framework integrates a supervisor model for strategic oversight and a
conversational model for question generation. We conducted three studies: Study
1 evaluated usability with 20 participants; Study 2 compared 121 AI interviews
to 1,271 human interviews from the MediaSum dataset using NLP metrics and
propensity score matching; Study 3 involved 10 interdisciplinary researchers
conducting both human and AI interviews, followed by blind thematic analysis.
Results across studies indicate that MimiTalk reduces interview anxiety,
maintains conversational coherence, and outperforms human interviews in
information richness, coherence, and stability. AI interviews elicit technical
insights and candid views on sensitive topics, while human interviews better
capture cultural and emotional nuances. These findings suggest that dual-agent
constitutional AI supports effective human-AI collaboration, enabling
replicable, scalable and quality-controlled qualitative research.

</details>


### [27] [Conversational Collective Intelligence (CCI) using Hyperchat AI in an Authentic Forecasting Task](https://arxiv.org/abs/2511.03732)
*Hans Schumann,Louis Rosenberg,Ganesh Mani,Gregg Willcox*

Main category: cs.HC

TL;DR: Hyperchat AI通过促进实时对话，帮助人群更准确地预测MLB比赛结果。


<details>
  <summary>Details</summary>
Motivation: 开发一种能在大型人群中促进深入讨论、协作和优化决策的智能工具。

Method: 进行了为期8周的研究，28名体育迷通过Hyperchat AI进行59场MLB比赛的预测。

Result: 使用Hyperchat AI的组在预测准确率上达到78%，显著超过了博彩市场的57%。

Conclusion: 通过实时对话，Hyperchat AI显著提高了人群的预测准确性。

Abstract: Hyperchat AI is a novel agentic technology that enables thoughtful
conversations among networked human groups of potentially unlimited size. It
allows large teams to discuss complex issues, brainstorm ideas, surface risks,
assess alternatives and efficiently converge on optimized solutions that
amplify the group's Collective Intelligence (CI). A formal study was conducted
to quantify the forecasting accuracy of human groups using Hyperchat AI to
conversationally predict the outcome of Major League Baseball (MLB) games.
During an 8-week period, networked groups of approximately 24 sports fans were
tasked with collaboratively forecasting the winners of 59 baseball games
through real-time conversation facilitated by AI agents. The results showed
that when debating the games using Hyperchat AI technology, the groups
converged on High Confidence predictions that significantly outperformed Vegas
betting markets. Specifically, groups were 78% accurate in their High
Confidence picks, a statistically strong result vs the Vegas odds of 57%
(p=0.020). Had the groups bet against the spread (ATS) on these games, they
would have achieved a 46% ROI against Vegas betting markets. In addition, High
Confidence forecasts that were generated through above-average conversation
rates were 88% accurate, suggesting that real-time interactive deliberation is
central to amplified accuracy.

</details>


### [28] [HACI: A Haptic-Audio Code Interface to Improve Educational Outcomes for Visually Impaired Introductory Programming Students](https://arxiv.org/abs/2511.03733)
*Pratham Gandhi*

Main category: cs.HC

TL;DR: 本论文介绍的HACI是一种通过触觉和音频反馈帮助视觉障碍学生学习编程的教育工具，初步评估显示其有助于非视觉的编程体验，但仍需改进反馈机制。


<details>
  <summary>Details</summary>
Motivation: 为了给视觉障碍学生提供公平的计算机科学教育机会，降低认知负担。

Method: 设计并实现一个集成触觉和音频反馈的web应用程序，以及一个配有六个触觉电机的Arduino手套，进行初步评估。

Result: HACI促进了编程结构的非视觉导航和理解，但在反馈机制和功能扩展上仍面临挑战。

Conclusion: HACI在编程教育中对视觉障碍学生具有变革潜力，通过触觉与音频反馈的整合，提升了可访问性。

Abstract: This thesis introduces the Haptic-Audio Code Interface (HACI), an educational
tool designed to enhance programming education for visually impaired (VI)
students by integrating haptic and audio feedback to compensate for the absence
of visual cues. HACI consists of a non-resource-intensive web application
supporting JavaScript program development, execution, and debugging, connected
via a cable to an Arduino-powered glove with six integrated haptic motors to
provide physical feedback to VI programmers. Motivated by the need to provide
equitable educational opportunities in computer science, HACI aims to improve
non-visual code navigation, comprehension, summarizing, editing, and debugging
for students with visual impairments while minimizing cognitive load. This work
details HACI's design principles, technical implementation, and a preliminary
evaluation through a pilot study conducted with undergraduate Computer Science
students. Findings indicate that HACI aids in the non-visual navigation and
understanding of programming constructs, although challenges remain in refining
feedback mechanisms to ensure consistency and reliability, as well as
supplementing the current functionality with a more feature-reach and
customizable accessible learning experience which will allow visually impaired
students to fully utilize interleaved haptic and audio feedback. The study
underscores the transformative potential of haptic and audio feedback in
educational practices for the visually impaired, setting a foundation for
future research and development in accessible programming education. This
thesis contributes to the field of accessible technology by demonstrating how
tactile and auditory feedback can be effectively integrated into educational
tools, thereby broadening accessibility in STEM education.

</details>


### [29] [SnappyMeal: Design and Longitudinal Evaluation of a Multimodal AI Food Logging Application](https://arxiv.org/abs/2511.03907)
*Liam Bakar,Zachary Englhardt,Vidya Srinivas,Girish Narayanswamy,Dilini Nissanka,Shwetak Patel,Vikram Iyer*

Main category: cs.HC

TL;DR: 研究提出SnappyMeal，一个利用AI和多模态输入的饮食追踪系统，旨在改善饮食记录的灵活性与准确性，并获得用户积极反馈。


<details>
  <summary>Details</summary>
Motivation: 当前饮食记录方法缺乏灵活性，导致低依从性和潜在的不准确性，迫切需要改善饮食记录方法。

Method: 通过对营养专家和用户的访谈，结合AI技术和多模态输入，SnappyMeal系统进行评估。

Result: SnappyMeal系统通过智能化的跟进问题和信息检索，提高了饮食记录的准确性，用户对其多个输入方式表示高度认可。

Conclusion: 多模态AI系统能显著提高饮食记录的灵活性和上下文意识，奠定了智能自我追踪应用的新基础。

Abstract: Food logging, both self-directed and prescribed, plays a critical role in
uncovering correlations between diet, medical, fitness, and health outcomes.
Through conversations with nutritional experts and individuals who practice
dietary tracking, we find current logging methods, such as handwritten and
app-based journaling, are inflexible and result in low adherence and
potentially inaccurate nutritional summaries. These findings, corroborated by
prior literature, emphasize the urgent need for improved food logging methods.
In response, we propose SnappyMeal, an AI-powered dietary tracking system that
leverages multimodal inputs to enable users to more flexibly log their food
intake. SnappyMeal introduces goal-dependent follow-up questions to
intelligently seek missing context from the user and information retrieval from
user grocery receipts and nutritional databases to improve accuracy. We
evaluate SnappyMeal through publicly available nutrition benchmarks and a
multi-user, 3-week, in-the-wild deployment capturing over 500 logged food
instances. Users strongly praised the multiple available input methods and
reported a strong perceived accuracy. These insights suggest that multimodal AI
systems can be leveraged to significantly improve dietary tracking flexibility
and context-awareness, laying the groundwork for a new class of intelligent
self-tracking applications.

</details>


### [30] [Human Resource Management and AI: A Contextual Transparency Database](https://arxiv.org/abs/2511.03916)
*Ellen Simpson,Ryan Ermovick,Mona Sloane*

Main category: cs.HC

TL;DR: 随着AI工具在招聘中的普及，论文提出了TARAI指数，强调AI透明度需在实践中理解，并不是固定属性，而是受专业实践和互动影响的动态结果。


<details>
  <summary>Details</summary>
Motivation: 随着AI工具在招聘和人力资源管理中的广泛应用，针对这些黑箱系统的透明度需求日益突出，特别是在各个职业特定领域。

Method: 运用迭代的混合方法过程，构建了一个数据库，该数据库评估了AI系统在招聘领域的功能性、声明、假设及其清晰度。

Result: 该论文展示了透明度的动态生成过程，强调透明度是一个社会性项目，而非单纯的技术或抽象概念。

Conclusion: 本文通过建立人才获取与招聘AI指数（TARAI），强调AI透明度不仅是一个固定属性，而是一个由专业实践、互动和能力动态形成的结果，为人力资源管理中的AI透明度提供了实践性和可操作性的方法。

Abstract: AI tools are proliferating in human resources management (HRM) and
recruiting, helping to mediate access to the labor market. As these systems
spread, profession-specific transparency needs emerging from black-boxed
systems in HRM move into focus. Prior work often frames transparency
technically or abstractly, but we contend AI transparency is a social project
shaped by materials, meanings, and competencies of practice. This paper
introduces the Talent Acquisition and Recruiting AI (TARAI) Index, situating AI
systems within the social practice of recruiting by examining product
functionality, claims, assumptions, and AI clarity. Built through an iterative,
mixed-methods process, the database demonstrates how transparency emerges: not
as a fixed property, but as a dynamic outcome shaped by professional practices,
interactions, and competencies. By centering social practice, our work offers a
grounded, actionable approach to understanding and articulating AI transparency
in HR and provides a blueprint for participatory database design for contextual
transparency in professional practice.

</details>


### [31] [Revealing AI Reasoning Increases Trust but Crowds Out Unique Human Knowledge](https://arxiv.org/abs/2511.04050)
*Zenan Chen,Ruijiang Gao,Yingzhi Liang*

Main category: cs.HC

TL;DR: 显示AI推理可以增进信任，但可能导致过度信任，损害人类在决策中的独特知识利用。


<details>
  <summary>Details</summary>
Motivation: 探讨人类如何评估AI能力并根据此调整信任，特别关注独特人类知识对决策的影响。

Method: 通过一次预注册的激励兼容实验（N=752）进行研究。

Result: 显示AI推理显著提高了对AI建议的信任和一致性，但导致了过度信任，抑制了UHK的使用。

Conclusion: 在显示AI推理的情况下，过度信任AI会抑制独特人类知识（UHK）的利用。

Abstract: Effective human-AI collaboration requires humans to accurately gauge AI
capabilities and calibrate their trust accordingly. Humans often have
context-dependent private information, referred to as Unique Human Knowledge
(UHK), that is crucial for deciding whether to accept or override AI's
recommendations. We examine how displaying AI reasoning affects trust and UHK
utilization through a pre-registered, incentive-compatible experiment (N =
752). We find that revealing AI reasoning, whether brief or extensive, acts as
a powerful persuasive heuristic that significantly increases trust and
agreement with AI recommendations. Rather than helping participants
appropriately calibrate their trust, this transparency induces over-trust that
crowds out UHK utilization. Our results highlight the need for careful
consideration when revealing AI reasoning and call for better information
design in human-AI collaboration systems.

</details>


### [32] ["Everyone Else Does It": The Rise of Preprinting Culture in Computing Disciplines](https://arxiv.org/abs/2511.04081)
*Kyrie Zhixuan Zhou,Justin Eric Chen,Xiang Zheng,Yaoyao Qian,Yunpeng Xiao,Kai Shu*

Main category: cs.HC

TL;DR: 本研究通过访谈15位学者，探讨了预印本在AI和HCI领域的动机与影响，发现其在传统出版模式中扮演了颠覆角色。


<details>
  <summary>Details</summary>
Motivation: 随着计算领域的快速发展，预印本已成为一种常态，本研究旨在揭示学者们对此的动机及看法。

Method: 通过对15位相关领域学者进行半结构化访谈，调查他们对预印本的动机和看法。

Result: 研究发现，预印本与领域特征密切相关，包括论文数量庞大、职业竞争激烈、抢先发表现象普遍，以及不完善的同行评审系统，预印本在这些方面为参与者提供了帮助。

Conclusion: 这项研究探讨了预印本在人工智能和人机交互领域中的意义，并反思了它对传统出版模式的颠覆作用。

Abstract: Preprinting has become a norm in fast-paced computing fields such as
artificial intelligence (AI) and human-computer interaction (HCI). In this
paper, we conducted semistructured interviews with 15 academics in these fields
to reveal their motivations and perceptions of preprinting. The results found a
close relationship between preprinting and characteristics of the fields,
including the huge number of papers, competitiveness in career advancement,
prevalence of scooping, and imperfect peer review system - preprinting comes to
the rescue in one way or another for the participants. Based on the results, we
reflect on the role of preprinting in subverting the traditional publication
mode and outline possibilities of a better publication ecosystem. Our study
contributes by inspecting the community aspects of preprinting practices
through talking to academics.

</details>


### [33] [Scaffolding Metacognition in Programming Education: Understanding Student-AI Interactions and Design Implications](https://arxiv.org/abs/2511.04144)
*Boxuan Ma,Huiyong Li,Gen Li,Li Chen,Cheng Tang,Yinjie Xie,Chenghao Gu,Atsushi Shimada,Shin'ichi Konomi*

Main category: cs.HC

TL;DR: 本研究探讨了生成式AI工具对大学编程课程中学生元认知过程的影响，并提供了增强教育AI工具的设计建议。


<details>
  <summary>Details</summary>
Motivation: 虽然生成式AI工具如ChatGPT为初学程序员提供了前所未有的即时个性化支持，但其对学生元认知过程的影响仍未被充分探索。

Method: 分析大学编程课程中超过10,000个对话日志，并结合学生和教师的调查数据，关注提示和回应与元认知阶段和策略的对齐情况。

Result: 通过多种数据源综合分析，提炼出旨在支持而非取代元认知参与的AI编程助手的设计考虑。

Conclusion: 本研究为教育AI工具的开发提供了指导，以增强学生在编程教育中的学习过程。

Abstract: Generative AI tools such as ChatGPT now provide novice programmers with
unprecedented access to instant, personalized support. While this holds clear
promise, their influence on students' metacognitive processes remains
underexplored. Existing work has largely focused on correctness and usability,
with limited attention to whether and how students' use of AI assistants
supports or bypasses key metacognitive processes. This study addresses that gap
by analyzing student-AI interactions through a metacognitive lens in
university-level programming courses. We examined more than 10,000 dialogue
logs collected over three years, complemented by surveys of students and
educators. Our analysis focused on how prompts and responses aligned with
metacognitive phases and strategies. Synthesizing these findings across data
sources, we distill design considerations for AI-powered coding assistants that
aim to support rather than supplant metacognitive engagement. Our findings
provide guidance for developing educational AI tools that strengthen students'
learning processes in programming education.

</details>


### [34] [Graph Neural Networks for User Satisfaction Classification in Human-Computer Interaction](https://arxiv.org/abs/2511.04166)
*Rui Liu,Runsheng Zhang,Shixiao Wang*

Main category: cs.HC

TL;DR: 本研究提出一种基于图神经网络的用户满意度分类框架，克服传统方法的局限性，在多项性能指标上表现优越，提升人机交互体验。


<details>
  <summary>Details</summary>
Motivation: 针对传统方法在处理复杂交互关系和多维特征方面的局限性，提出一种新颖框架以提高用户满意度分类的准确性和适应性。

Method: 使用图卷积和注意力机制融合局部特征和全局上下文，通过全局池化和分类层实现自动化满意度分类。

Result: 该方法在Kaggle的公共用户满意度调查数据集上进行验证，与多个基准模型进行比较，实验结果表明该方法在各项性能指标上均优于现有方法。

Conclusion: 该研究提出的基于图神经网络的用户满意度分类框架在准确性、F1分数、AUC和精确度等多个性能指标上优于现有方法，展示了图模型在满意度预测任务中的优势。

Abstract: This study focuses on the problem of user satisfaction classification and
proposes a framework based on graph neural networks to address the limitations
of traditional methods in handling complex interaction relationships and
multidimensional features. User behaviors, interface elements, and their
potential connections are abstracted into a graph structure, and joint modeling
of nodes and edges is used to capture semantics and dependencies in the
interaction process. Graph convolution and attention mechanisms are introduced
to fuse local features and global context, and global pooling with a
classification layer is applied to achieve automated satisfaction
classification. The method extracts deep patterns from structured data and
improves adaptability and robustness in multi-source heterogeneous and dynamic
environments. To verify effectiveness, a public user satisfaction survey
dataset from Kaggle is used, and results are compared with multiple baseline
models across several performance metrics. Experiments show that the method
outperforms existing approaches in accuracy, F1-Score, AUC, and Precision,
demonstrating the advantage of graph-based modeling in satisfaction prediction
tasks. The study not only enriches the theoretical framework of user modeling
but also highlights its practical value in optimizing human-computer
interaction experience.

</details>


### [35] [Active Domain Adaptation for mmWave-based HAR via Renyi Entropy-based Uncertainty Estimation](https://arxiv.org/abs/2511.04219)
*Mingzhi Lin,Teng Huang,Han Ding,Cui Zhao,Fei Wang,Ge Wang,Wei Xi*

Main category: cs.HC

TL;DR: mmADA是一个有效的主动领域适应框架，通过最小标记数据适应mmWave雷达的人体活动识别，从而提高模型在新用户和环境下的性能。


<details>
  <summary>Details</summary>
Motivation: 旨在解决mmWave雷达在人体活动识别中的领域移动问题，尤其是在新用户、位置和环境下模型性能下降的问题。

Method: 通过引入Renyi熵的基于不确定性的估计、对比学习和伪标记等技术，mmADA实现了高效的主动领域适应。

Result: mmADA在多个用户、位置和环境下进行了评估，展示了其在各种领域间设置下超过90%的准确率，并在与五个基线的比较中展现了优越的适应性能。

Conclusion: mmADA框架在不同用户和环境下表现出超过90%的识别准确率，展示了其强大的适应性和鲁棒性。

Abstract: Human Activity Recognition (HAR) using mmWave radar provides a non-invasive
alternative to traditional sensor-based methods but suffers from domain shift,
where model performance declines in new users, positions, or environments. To
address this, we propose mmADA, an Active Domain Adaptation (ADA) framework
that efficiently adapts mmWave-based HAR models with minimal labeled data.
mmADA enhances adaptation by introducing Renyi Entropy-based uncertainty
estimation to identify and label the most informative target samples.
Additionally, it leverages contrastive learning and pseudo-labeling to refine
feature alignment using unlabeled data. Evaluations with a TI IWR1443BOOST
radar across multiple users, positions, and environments show that mmADA
achieves over 90% accuracy in various cross-domain settings. Comparisons with
five baselines confirm its superior adaptation performance, while further tests
on unseen users, environments, and two additional open-source datasets validate
its robustness and generalization.

</details>


### [36] [Vitessce Link: A Mixed Reality and 2D Display Hybrid Approach for Visual Analysis of 3D Tissue Maps](https://arxiv.org/abs/2511.04262)
*Eric Mörth,Morgan L. Turner,Cydney Nielsen,Xianhao Carton Liu,Mark Keller,Lisa Choy,John Conroy,Tabassum Kakar,Clarence Yapp,Alex Wong,Peter Sorger,Liam McLaughlin,Sanjay Jain,Johanna Beyer,Hanspeter Pfister,Chen Zhu-Tian,Nils Gehlenborg*

Main category: cs.HC

TL;DR: Vitessce Link是一个网络原生的混合框架，通过3D立体视图和同步的2D展示，促进了对3D组织图谱的分析。


<details>
  <summary>Details</summary>
Motivation: 随着空间组学和高分辨率成像技术的进步，需要新的工具来全面探索3D组织图谱数据，而现有工具通常局限于2D显示。

Method: 通过WebSocket架构实现2D和3D混合现实的同步显示与交互，支持用户通过手势操作导航体积数据。

Result: 在肾脏病学和肿瘤学的案例研究中，展示了该混合方法如何提升分割评估、距离测量和空间关系的解析。

Conclusion: Vitessce Link建立了一个用于3D组织图谱集成分析的网络原生范式。

Abstract: Advances in spatial omics and high-resolution imaging enable the creation of
three-dimensional (3D) tissue maps that capture cellular organization and
interactions in situ. While these data provide critical insights into tissue
function and disease, their exploration is often constrained by tools limited
to 2D displays or stereoscopic rendering without analytical integration. We
present Vitessce Link, a web-based hybrid framework that unites a 3D
stereoscopic view in mixed reality with a synchronized 2D display environment.
Users can navigate volumetric data with intuitive hand gestures while
controlling channels, filters, and derived data views through the Vitessce
platform. Built on open standards and running entirely in the browser, Vitessce
Link minimizes friction, supports integration with computational notebooks, and
synchronizes interactions across devices via a lightweight WebSocket
architecture. Case studies in nephrology and oncology demonstrate how the
hybrid approach enhances segmentation evaluation, distance measurement, and
interpretation of spatial relationships. Vitessce Link establishes a paradigm
for integrative, web-native analysis of 3D tissue maps.

</details>


### [37] [Towards Aligning Multimodal LLMs with Human Experts: A Focus on Parent-Child Interaction](https://arxiv.org/abs/2511.04366)
*Weiyan Shi,Kenny Tsu Wei Choo*

Main category: cs.HC

TL;DR: 本研究探讨了多模态大语言模型(MMLMs)在分析父母与儿童互动中的应用，发现其在观察层面的对齐更为有效，但在判断层面存在专家解读标准的差异。


<details>
  <summary>Details</summary>
Motivation: 探讨多模态大语言模型在理解复杂社交互动中的能力，特别是在早期社交传播发展中的角色。

Method: 通过与三位言语语言 патологов (SLPs) 的访谈和视频标注，分析了儿童与父母互动中的共同注意力。

Result: 发现MLLM在观察层面能更好地模拟专家的工作流程，而在判断层面存在较大差异。

Conclusion: 研究显示，专家与AI在观察阶段的对齐更为稳健，而在判断层面则存在解读标准的差异，强调了在复杂社交行为分析中应用MLLMs的可行性与挑战。

Abstract: While multimodal large language models (MLLMs) are increasingly applied in
human-centred AI systems, their ability to understand complex social
interactions remains uncertain. We present an exploratory study on aligning
MLLMs with speech-language pathologists (SLPs) in analysing joint attention in
parent-child interactions, a key construct in early social-communicative
development. Drawing on interviews and video annotations with three SLPs, we
characterise how observational cues of gaze, action, and vocalisation inform
their reasoning processes. We then test whether an MLLM can approximate this
workflow through a two-stage prompting, separating observation from judgment.
Our findings reveal that alignment is more robust at the observation layer,
where experts share common descriptors, than at the judgement layer, where
interpretive criteria diverge. We position this work as a case-based probe into
expert-AI alignment in complex social behaviour, highlighting both the
feasibility and the challenges of applying MLLMs to socially situated
interaction analysis.

</details>


### [38] [HPC-Vis: A Visual Analytic System for Interactive Exploration of Historical Painter Cohorts](https://arxiv.org/abs/2511.04383)
*Yingping Yang,Guangtao You,Jiayi Chen,Jiazhou Chen*

Main category: cs.HC

TL;DR: 本文提出HPC-Vis，一个用于历史画家群体分析的可视化系统，利用多种技术重构复杂关系，显著提升研究效率。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在处理历史画家复杂关系和多样艺术风格时效率低、成本高的问题。

Method: 提出了HPC-Vis系统，包括三阶段重构算法、统一的艺术风格标签系统和人机协作交互探索机制。

Result: 成功重构历史画家之间的继承关系，并通过案例研究验证了HPC-Vis在画家群体分析中的优势。

Conclusion: HPC-Vis为历史画家分析提供了一种新颖的可视化交互探索工具，显著提升了研究者发现和验证历史画家群体的能力。

Abstract: More than ten thousand Chinese historical painters are recorded in the
literature; their cohort analysis has always been a key area of research on
Chinese painting history for both professional historians and amateur
enthusiasts. However, these painters have very diverse artistic styles and an
extremely complex network of inheritance relationships (e.g., master-apprentice
or style imitation relationships); traditional cohort analysis methods not only
heavily rely on field experience, but also cost a lot of time and effort with
numerous but scattered historical documents. In this paper, we propose HPC-Vis,
a visual analytical system for interactive exploration of historical painter
cohorts. Firstly, a three-stage reconstruction algorithm for inheritance
relationships of painters is proposed, which automatically converts the complex
relationship graph of historical painters into a forest structure that contains
multiple trees with clear inheriting chains, and we visually encoded this
forest as a mountain map to intuitively show potential cohorts of historical
painters. Secondly, a unified artistic style label system with three levels
(i.e., subjects, techniques, and emotions) is established by using large
language models, and it is further visually encoded as a new foldable nested
doughnut chart. Finally, a visually guided human-computer collaborative
interactive exploration mechanism is constructed, in which a painter cohort
recommendation model is designed by integrating style, identity, time, space,
and relationships. Two case studies and a user study demonstrate the advantage
of HPC-Vis on assisting historians in discovering, defining, and validating
cohorts of historical painters.

</details>


### [39] [Generate, Evaluate, Iterate: Synthetic Data for Human-in-the-Loop Refinement of LLM Judges](https://arxiv.org/abs/2511.04478)
*Hyo Jin Do,Zahra Ashktorab,Jasmina Gajcin,Erik Miehling,Martín Santillán Cooper,Qian Pan,Elizabeth M. Daly,Werner Geyer*

Main category: cs.HC

TL;DR: 引入合成数据生成工具以增强LLM-as-a-judge的灵活性，用户可快速创建多样化的测试用例，结果显示合成数据在评估中同样有效。


<details>
  <summary>Details</summary>
Motivation: 解决LLM-as-a-judge范式中数据稀缺和评估标准多样性不足的问题。

Method: 结合合成数据生成工具到LLM-as-a-judge工作流程中，支持用户创建可定制的测试用例，并进行AI辅助编辑。

Result: 用户研究显示83%的参与者偏好使用工具生成合成数据，且生成的合成数据与手工制作的数据在效果上相当。

Conclusion: 合成数据作为评估标准精炼和人类偏好对齐的有效替代方案，特别在效率和可扩展性至关重要的环境中。

Abstract: The LLM-as-a-judge paradigm enables flexible, user-defined evaluation, but
its effectiveness is often limited by the scarcity of diverse, representative
data for refining criteria. We present a tool that integrates synthetic data
generation into the LLM-as-a-judge workflow, empowering users to create
tailored and challenging test cases with configurable domains, personas,
lengths, and desired outcomes, including borderline cases. The tool also
supports AI-assisted inline editing of existing test cases. To enhance
transparency and interpretability, it reveals the prompts and explanations
behind each generation. In a user study (N=24), 83% of participants preferred
the tool over manually creating or selecting test cases, as it allowed them to
rapidly generate diverse synthetic data without additional workload. The
generated synthetic data proved as effective as hand-crafted data for both
refining evaluation criteria and aligning with human preferences. These
findings highlight synthetic data as a promising alternative, particularly in
contexts where efficiency and scalability are critical.

</details>


### [40] [Perceptions of AI Bad Behavior: Variations on Discordant Non-Performance](https://arxiv.org/abs/2511.04487)
*Jaime Banks*

Main category: cs.HC

TL;DR: 研究探讨非专家对AI不当行为的看法，提出了一种初步框架，强调道德基础、推理水平和道德二元论对AI行为评估的重要性。


<details>
  <summary>Details</summary>
Motivation: 填补对非专家如何看待AI不当行为理解的空白，探索人们如何讨论和评估AI的不当行为。

Method: 通过对28名非专家的访谈进行归纳分析，聚焦于大型语言模型及其不当行为。

Result: 研究发现，当讨论AI时，不当行为并不特别显著，但在特定情况下很容易被提及，并且在评估具体行为时变得更为显著。

Conclusion: 提出了一个考虑AI不当行为的初步框架，强调道德基础理论、推理水平理论和道德二元论的交汇点。

Abstract: Popular discourses are thick with narratives of generative AI's problematic
functions and outcomes, yet there is little understanding of how non-experts
consider AI activities to constitute bad behavior. This study starts to bridge
that gap through inductive analysis of interviews with non-experts (N = 28)
focusing on large-language models in general and their bad behavior,
specifically. Results suggest bad behaviors are not especially salient when
people discuss AI generally but the notion of AI behaving badly is easily
engaged when prompted, and bad behavior becomes even more salient when
evaluating specific AI behaviors. Types of observed behaviors considered bad
mostly align with their inspiring moral foundations; across all observed
behaviors, some variations on non-performance and social discordance were
present. By scaffolding findings at the intersections of moral foundations
theory, construal level theory, and moral dyadism, a tentative framework for
considering AI bad behavior is proposed.

</details>


### [41] [Students' Acceptance of Arduino Technology Integration in Student-Led Science Inquiry: Insights from the Technology Acceptance Model](https://arxiv.org/abs/2511.04614)
*Seok-Hyun Ga,Chun-Yen Chang,Sonya Martin*

Main category: cs.HC

TL;DR: 本研究探讨了高中生在科学课堂上对Arduino技术的接受度，发现社会文化因素和技术设计对接受度有重要影响。


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解高中学生在基于询问的科学课堂上对Arduino技术的接受度。

Method: 通过定性分析访谈和课堂观察，探讨学生对Arduino的感知。

Result: 关键发现表明，接受度不仅受到工作相关性和输出质量等工具性因素的驱动，也受到韩国教育系统独特的社会文化背景影响。

Conclusion: 研究表明，技术设计和教学支持对学生体验具有重要影响，必须考虑社会文化因素。

Abstract: This study examines high school students' acceptance of Arduino technology in
a student-led, inquiry-based science class, using the extended Technology
Acceptance Model (TAM2) as a guiding framework. Through qualitative analysis of
interviews and classroom observations, we explored how students perceived
Arduino's usefulness and ease of use. Going beyond traditional quantitative TAM
studies, this qualitative TAM research provides a nuanced, in-depth
understanding of the contextual factors shaping technology acceptance. Key
findings reveal that acceptance was driven not only by instrumental factors
like job relevance and output quality but also by the unique sociocultural
context of the Korean education system, where technology use was perceived as
valuable for university admissions (subjective norm and image). Critically,
unlike earlier research that emphasized programming challenges, participants in
this study found Arduino accessible and intuitive, thanks to integrated visual
block-coding tools. These findings highlight the importance of both
technological design and pedagogical support in shaping students' experiences.
Implications for science curriculum design, teacher preparation, and equitable
technology integration in secondary education are discussed.

</details>
