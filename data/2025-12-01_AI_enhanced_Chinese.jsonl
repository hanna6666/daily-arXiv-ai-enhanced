{"id": "2511.21886", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21886", "abs": "https://arxiv.org/abs/2511.21886", "authors": ["Jingtian Yan", "Shuai Zhou", "Stephen F. Smith", "Jiaoyang Li"], "title": "Bridging Planning and Execution: Multi-Agent Path Finding Under Real-World Deadlines", "comment": null, "summary": "The Multi-Agent Path Finding (MAPF) problem aims to find collision-free paths for multiple agents while optimizing objectives such as the sum of costs or makespan. MAPF has wide applications in domains like automated warehouses, manufacturing systems, and airport logistics. However, most MAPF formulations assume a simplified robot model for planning, which overlooks execution-time factors such as kinodynamic constraints, communication latency, and controller variability. This gap between planning and execution is problematic for time-sensitive applications. To bridge this gap, we propose REMAP, an execution-informed MAPF planning framework that can be combined with leading search-based MAPF planners with minor changes. Our framework integrates the proposed ExecTimeNet to accurately estimate execution time based on planned paths. We demonstrate our method for solving MAPF with Real-world Deadlines (MAPF-RD) problem, where agents must reach their goals before a predefined wall-clock time. We integrate our framework with two popular MAPF methods, MAPF-LNS and CBS. Experiments show that REMAP achieves up to 20% improvement in solution quality over baseline methods (e.g., constant execution speed estimators) on benchmark maps with up to 300 agents.", "AI": {"tldr": "REMAP\u662f\u4e00\u4e2a\u7ed3\u5408\u6267\u884c\u65f6\u95f4\u4f30\u8ba1\u7684\u591a\u4ee3\u7406\u8def\u5f84\u67e5\u627e\u6846\u67b6\uff0c\u901a\u8fc7\u4e0e\u73b0\u6709MAPF\u65b9\u6cd5\u96c6\u6210\uff0c\u63d0\u9ad8\u4e86\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5bf9\u65f6\u95f4\u654f\u611f\u7684\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u7684MAPF\u6a21\u578b\u5ffd\u7565\u4e86\u6267\u884c\u65f6\u95f4\u76f8\u5173\u7684\u56e0\u7d20\uff0c\u5982\u52a8\u9759\u6001\u7ea6\u675f\u3001\u901a\u4fe1\u5ef6\u8fdf\u548c\u63a7\u5236\u5668\u53d8\u5f02\u6027\uff0c\u8fd9\u5bf9\u65f6\u95f4\u654f\u611f\u7684\u5e94\u7528\u9020\u6210\u4e86\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210ExecTimeNet\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4f30\u8ba1\u57fa\u4e8e\u89c4\u5212\u8def\u5f84\u7684\u6267\u884c\u65f6\u95f4\uff0c\u5e76\u4e0e\u73b0\u6709\u7684MAPF\u65b9\u6cd5\uff08MAPF-LNS\u548cCBS\uff09\u7ed3\u5408\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u5730\u56fe\u4e0a\uff0cREMAP\u5728\u89e3\u51b3\u5177\u6709\u73b0\u5b9e\u4e16\u754c\u622a\u6b62\u65f6\u95f4\u7684MAPF\u95ee\u9898\u65f6\uff0c\u8f83\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u4e86\u9ad8\u8fbe20%\u7684\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u3002", "conclusion": "REMAP\u5728\u5904\u7406\u591a\u4ee3\u7406\u8def\u5f84\u67e5\u627e\u95ee\u9898\u65f6\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u89e3\u51b3\u65b9\u6848\u7684\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5728\u65f6\u95f4\u654f\u611f\u7684\u5e94\u7528\u4e2d\u3002"}}
{"id": "2511.21925", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.21925", "abs": "https://arxiv.org/abs/2511.21925", "authors": ["Alex Richardson", "Jonathan Sprinkle"], "title": "OpenTwinMap: An Open-Source Digital Twin Generator for Urban Autonomous Driving", "comment": null, "summary": "Digital twins of urban environments play a critical role in advancing autonomous vehicle (AV) research by enabling simulation, validation, and integration with emerging generative world models. While existing tools have demonstrated value, many publicly available solutions are tightly coupled to specific simulators, difficult to extend, or introduce significant technical overhead. For example, CARLA-the most widely used open-source AV simulator-provides a digital twin framework implemented entirely as an Unreal Engine C++ plugin, limiting flexibility and rapid prototyping. In this work, we propose OpenTwinMap, an open-source, Python-based framework for generating high-fidelity 3D urban digital twins. The completed framework will ingest LiDAR scans and OpenStreetMap (OSM) data to produce semantically segmented static environment assets, including road networks, terrain, and urban structures, which can be exported into Unreal Engine for AV simulation. OpenTwinMap emphasizes extensibility and parallelization, lowering the barrier for researchers to adapt and scale the pipeline to diverse urban contexts. We describe the current capabilities of the OpenTwinMap, which includes preprocessing of OSM and LiDAR data, basic road mesh and terrain generation, and preliminary support for CARLA integration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f00\u6e90\u7684Python\u6846\u67b6OpenTwinMap\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u76843D\u57ce\u5e02\u6570\u5b57\u53cc\u80de\u80ce\uff0c\u4ee5\u652f\u6301\u81ea\u52a8\u9a7e\u9a76\u7814\u7a76\u3002", "motivation": "\u5f53\u524d\u7684\u6570\u5b57\u53cc\u80de\u80ce\u5de5\u5177\u591a\u4e0e\u7279\u5b9a\u6a21\u62df\u5668\u7d27\u5bc6\u8026\u5408\uff0c\u5e76\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u9650\u5236\u4e86\u5feb\u901f\u539f\u578b\u7684\u80fd\u529b\u3002", "method": "\u5f00\u53d1OpenTwinMap\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406LiDAR\u626b\u63cf\u548cOpenStreetMap\u6570\u636e\uff0c\u751f\u6210\u57ce\u5e02\u73af\u5883\u7684\u9759\u6001\u8d44\u4ea7\uff0c\u5e76\u652f\u6301\u4e0eUnreal Engine\u7684\u96c6\u6210\u3002", "result": "OpenTwinMap\u80fd\u8fdb\u884cOSM\u548cLiDAR\u6570\u636e\u7684\u9884\u5904\u7406\uff0c\u751f\u6210\u9053\u8def\u7f51\u548c\u5730\u5f62\uff0c\u5e76\u521d\u6b65\u652f\u6301CARLA\u96c6\u6210\u3002", "conclusion": "\u6b64\u6846\u67b6\u5f3a\u8c03\u53ef\u6269\u5c55\u6027\u548c\u5e76\u884c\u5316\uff0c\u4f7f\u7814\u7a76\u4eba\u5458\u80fd\u66f4\u8f7b\u677e\u5730\u9002\u5e94\u548c\u6269\u5c55\u57ce\u5e02\u6570\u5b57\u53cc\u80de\u80ce\u7684\u751f\u6210\u3002"}}
{"id": "2511.21957", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.21957", "abs": "https://arxiv.org/abs/2511.21957", "authors": ["Cahit Ikbal Er", "Amin Kashiri", "Yasin Yazicioglu"], "title": "RSPECT: Robust and Scalable Planner for Energy-Aware Coordination of UAV-UGV Teams in Aerial Monitoring", "comment": null, "summary": "We consider the robust planning of energy-constrained unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs), which act as mobile charging stations, to perform long-horizon aerial monitoring missions. More specifically, given a set of points to be visited by the UAVs and desired final positions of the UAV-UGV teams, the objective is to find a robust plan (the vehicle trajectories) that can be realized without a major revision in the face of uncertainty (e.g., unknown obstacles/terrain, wind) to complete this mission in minimum time. We provide a formal description of this problem as a mixed-integer program (MIP), which is NP-hard. Since exact solution methods are computationally intractable for such problems, we propose RSPECT, a scalable and efficient heuristic. We provide theoretical results on the complexity of our algorithm and the feasibility and robustness of resulting plans. We also demonstrate the performance of our method via simulations and experiments.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u80fd\u91cf\u53d7\u9650\u7684\u65e0\u4eba\u673a\u548c\u65e0\u4eba\u5730\u9762\u8f66\u8f86\u7684\u7a33\u5065\u89c4\u5212\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5RSPECT\uff0c\u4ee5\u89e3\u51b3NP\u96be\u5ea6\u7684\u6df7\u5408\u6574\u6570\u89c4\u5212\u95ee\u9898\u3002", "motivation": "\u65e0\u4eba\u673a\u548c\u65e0\u4eba\u5730\u9762\u8f66\u8f86\u4f5c\u4e3a\u79fb\u52a8\u5145\u7535\u7ad9\u6267\u884c\u957f\u65f6\u95f4\u76d1\u6d4b\u4efb\u52a1\uff0c\u9762\u4e34\u73af\u5883\u4e0d\u786e\u5b9a\u6027\uff0c\u6025\u9700\u6709\u6548\u7684\u89c4\u5212\u7b56\u7565\u3002", "method": "\u63d0\u4f9b\u4e00\u79cd\u6df7\u5408\u6574\u6570\u89c4\u5212(MIP)\u7684\u5f62\u5f0f\u63cf\u8ff0\uff0c\u4ee5\u53ca\u4e00\u79cd\u540d\u4e3aRSPECT\u7684\u53ef\u6269\u5c55\u6709\u6548\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u7ed3\u679c\u548c\u4eff\u771f\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u7b97\u6cd5\u7684\u590d\u6742\u6027\u3001\u53ef\u884c\u6027\u548c\u7a33\u5065\u6027\u3002", "conclusion": "RSPECT\u80fd\u591f\u6709\u6548\u5730\u4ea7\u751f\u7a33\u5065\u7684\u65e0\u4eba\u673a\u548c\u65e0\u4eba\u5730\u9762\u8f66\u8f86\u8def\u5f84\uff0c\u4ee5\u5e94\u5bf9\u4e0d\u786e\u5b9a\u6027\u5e76\u6700\u5c0f\u5316\u5b8c\u6210\u4efb\u52a1\u7684\u65f6\u95f4\u3002"}}
{"id": "2511.22042", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22042", "abs": "https://arxiv.org/abs/2511.22042", "authors": ["Lei Li", "Jiale Gong", "Ziyang Li", "Hong Wang"], "title": "Constant-Volume Deformation Manufacturing for Material-Efficient Shaping", "comment": "46 pages, 27 figures", "summary": "Additive and subtractive manufacturing enable complex geometries but rely on discrete stacking or local removal, limiting continuous and controllable deformation and causing volume loss and shape deviations. We present a volumepreserving digital-mold paradigm that integrates real-time volume-consistency modeling with geometry-informed deformation prediction and an error-compensation strategy to achieve highly predictable shaping of plastic materials. By analyzing deformation patterns and error trends from post-formed point clouds, our method corrects elastic rebound and accumulation errors, maintaining volume consistency and surface continuity. Experiments on five representative geometries demonstrate that the system reproduces target shapes with high fidelity while achieving over 98% material utilization. This approach establishes a digitally driven, reproducible pathway for sustainable, zero-waste shaping of user-defined designs, bridging digital modeling, real-time sensing, and adaptive forming, and advancing next-generation sustainable and customizable manufacturing.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4f53\u79ef\u4fdd\u6301\u7684\u6570\u5b57\u6a21\u5177\u8303\u5f0f\uff0c\u901a\u8fc7\u5b9e\u65f6\u5efa\u6a21\u548c\u8bef\u5dee\u8865\u507f\uff0c\u5b9e\u73b0\u53ef\u9884\u6d4b\u7684\u5851\u6599\u6210\u578b\uff0c\u4fc3\u8fdb\u53ef\u6301\u7eed\u548c\u53ef\u5b9a\u5236\u7684\u5236\u9020\u3002", "motivation": "\u73b0\u6709\u7684\u589e\u51cf\u5236\u9020\u65b9\u6cd5\u9650\u5236\u4e86\u8fde\u7eed\u53ef\u63a7\u7684\u5f62\u53d8\uff0c\u5e76\u5bfc\u81f4\u4f53\u79ef\u635f\u5931\u548c\u5f62\u72b6\u504f\u5dee\uff0c\u8feb\u5207\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u6750\u6599\u5229\u7528\u3002", "method": "\u901a\u8fc7\u5b9e\u65f6\u4f53\u79ef\u4e00\u81f4\u6027\u5efa\u6a21\u3001\u51e0\u4f55\u4fe1\u606f\u9a71\u52a8\u7684\u5f62\u53d8\u9884\u6d4b\u4ee5\u53ca\u8bef\u5dee\u8865\u507f\u7b56\u7565\u6765\u5b9e\u73b0\u53ef\u9884\u6d4b\u7684\u5851\u6599\u6750\u6599\u6210\u578b\u3002", "result": "\u5728\u4e94\u79cd\u4ee3\u8868\u6027\u51e0\u4f55\u5f62\u72b6\u7684\u5b9e\u9a8c\u4e2d\uff0c\u7cfb\u7edf\u80fd\u591f\u4ee5\u9ad8\u4fdd\u771f\u5ea6\u518d\u73b0\u76ee\u6807\u5f62\u72b6\uff0c\u540c\u65f6\u5b9e\u73b0\u8d85\u8fc798%\u7684\u6750\u6599\u5229\u7528\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u7684\u76ee\u6807\u5f62\u72b6\u518d\u73b0\uff0c\u5e76\u4e14\u6750\u6599\u5229\u7528\u7387\u8d85\u8fc798%\u3002"}}
{"id": "2511.21994", "categories": ["cs.HC", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21994", "abs": "https://arxiv.org/abs/2511.21994", "authors": ["Megan Zheng", "Will Crichton", "Akshay Narayan", "Deepti Raghavan", "Nikos Vasilakis"], "title": "When Are Reactive Notebooks Not Reactive?", "comment": null, "summary": "Computational notebooks are convenient for programmers, but can easily become confusing and inconsistent due to the ability to incrementally edit a program that is running. Recent reactive notebook systems, such as Ipyflow, Marimo and Observable, strive to keep notebook state in sync with the current cell code by re-executing a minimal set of cells upon modification. However, each system defines reactivity a different way. Additionally, within any definition, we find simple notebook modifications that can break each system. Overall, these inconsistencies make it difficult for users to construct a mental model of their reactive notebook's implementation. This paper proposes Rex, a fine-grained test suite to discuss and assess reactivity capabilities within reactive notebook systems. We evaluate Rex on three existing reactive notebook systems and classify their failures with the aims of (i) helping programmers understand when reactivity fails and (ii) helping notebook implementations improve.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRex\u6d4b\u8bd5\u5957\u4ef6\uff0c\u4ee5\u8bc4\u4f30\u548c\u8ba8\u8bba\u53cd\u5e94\u6027\u7b14\u8bb0\u672c\u7cfb\u7edf\u7684\u80fd\u529b\uff0c\u5e76\u5e2e\u52a9\u7a0b\u5e8f\u5458\u7406\u89e3\u53cd\u5e94\u6027\u5931\u6548\u7684\u539f\u56e0\u3002", "motivation": "\u8ba1\u7b97\u7b14\u8bb0\u672c\u975e\u5e38\u65b9\u4fbf\uff0c\u4f46\u7531\u4e8e\u589e\u91cf\u7f16\u8f91\u529f\u80fd\uff0c\u53ef\u80fd\u4f1a\u5bfc\u81f4\u6df7\u6dc6\u548c\u4e0d\u4e00\u81f4\u3002", "method": "\u63d0\u51faRex\uff0c\u4e00\u4e2a\u7ec6\u7c92\u5ea6\u7684\u6d4b\u8bd5\u5957\u4ef6\uff0c\u7528\u4e8e\u8ba8\u8bba\u548c\u8bc4\u4f30\u53cd\u5e94\u6027\u7b14\u8bb0\u672c\u7cfb\u7edf\u7684\u53cd\u5e94\u6027\u80fd\u529b\uff0c\u5e76\u5728\u4e09\u4e2a\u73b0\u6709\u7684\u53cd\u5e94\u6027\u7b14\u8bb0\u672c\u7cfb\u7edf\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u5206\u7c7b\u4e86\u5404\u7cfb\u7edf\u5728\u53cd\u5e94\u6027\u65b9\u9762\u7684\u5931\u8d25\u3002", "conclusion": "\u8be5\u7814\u7a76\u65e8\u5728\u5e2e\u52a9\u7a0b\u5e8f\u5458\u7406\u89e3\u53cd\u5e94\u6027\u5931\u6548\u7684\u60c5\u51b5\u5e76\u4fc3\u8fdb\u7b14\u8bb0\u672c\u5b9e\u73b0\u7684\u6539\u8fdb\u3002"}}
{"id": "2511.22043", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.22043", "abs": "https://arxiv.org/abs/2511.22043", "authors": ["Xuchen Liu", "Ruocheng Li", "Bin Xin", "Weijia Yao", "Qigeng Duan", "Jinqiang Cui", "Ben M. Chen", "Jie Chen"], "title": "SwordRiding: A Unified Navigation Framework for Quadrotors in Unknown Complex Environments via Online Guiding Vector Fields", "comment": "For an experimental demo, see https://www.youtube.com/watch?v=tKYCg266c4o. For the lemma proof, see https://github.com/SmartGroupSystems/GVF_close_loop_planning/blob/main/proofs.md", "summary": "Although quadrotor navigation has achieved high performance in trajectory planning and control, real-time adaptability in unknown complex environments remains a core challenge. This difficulty mainly arises because most existing planning frameworks operate in an open-loop manner, making it hard to cope with environmental uncertainties such as wind disturbances or external perturbations. This paper presents a unified real-time navigation framework for quadrotors in unknown complex environments, based on the online construction of guiding vector fields (GVFs) from discrete reference path points. In the framework, onboard perception modules build a Euclidean Signed Distance Field (ESDF) representation of the environment, which enables obstacle awareness and path distance evaluation. The system first generates discrete, collision-free path points using a global planner, and then parameterizes them via uniform B-splines to produce a smooth and physically feasible reference trajectory. An adaptive GVF is then synthesized from the ESDF and the optimized B-spline trajectory. Unlike conventional approaches, the method adopts a closed-loop navigation paradigm, which significantly enhances robustness under external disturbances. Compared with conventional GVF methods, the proposed approach directly accommodates discretized paths and maintains compatibility with standard planning algorithms. Extensive simulations and real-world experiments demonstrate improved robustness against external disturbances and superior real-time performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u56db\u65cb\u7ffc\u5b9e\u65f6\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u6784\u5efa\u5f15\u5bfc\u5411\u91cf\u573a\uff0c\u6709\u6548\u5e94\u5bf9\u672a\u77e5\u590d\u6742\u73af\u5883\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u73b0\u6709\u7684\u89c4\u5212\u6846\u67b6\u4e3b\u8981\u5728\u5f00\u73af\u6a21\u5f0f\u4e0b\u8fd0\u884c\uff0c\u56e0\u6b64\u96be\u4ee5\u5e94\u5bf9\u672a\u77e5\u590d\u6742\u73af\u5883\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5c24\u5176\u662f\u98ce\u6270\u52a8\u6216\u5916\u90e8\u6270\u52a8\u3002", "method": "\u57fa\u4e8e\u79bb\u6563\u53c2\u8003\u8def\u5f84\u70b9\u5728\u7ebf\u6784\u5efa\u5f15\u5bfc\u5411\u91cf\u573a\uff08GVFs\uff09\uff0c\u5e76\u4f7f\u7528\u6b27\u51e0\u91cc\u5f97\u6709\u7b26\u53f7\u8ddd\u79bb\u573a\uff08ESDF\uff09\u8868\u793a\u73af\u5883\u3002", "result": "\u5728\u5e7f\u6cdb\u7684\u4eff\u771f\u548c\u73b0\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u663e\u793a\u51fa\u8f83\u5f3a\u7684\u6297\u5e72\u6270\u80fd\u529b\u548c\u4f18\u8d8a\u7684\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5b9e\u65f6\u6027\u80fd\u548c\u9c81\u68d2\u6027\u65b9\u9762\u8d85\u8fc7\u4e86\u4f20\u7edf\u7684\u5f15\u5bfc\u5411\u91cf\u573a\u65b9\u6cd5\uff0c\u5e76\u80fd\u6709\u6548\u5e94\u5bf9\u5916\u90e8\u5e72\u6270\u3002"}}
{"id": "2511.22056", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.22056", "abs": "https://arxiv.org/abs/2511.22056", "authors": ["Fengze Li", "Jieming Ma", "Kan Liu", "Xiaohan Zhang", "Yangle Liu", "Yue Li"], "title": "EAST: Environment-Aware Stylized Transition Along the Reality-Virtuality Continuum", "comment": null, "summary": "In the Virtual Reality (VR) gaming industry, maintaining immersion during real-world interruptions remains a challenge, particularly during transitions along the reality-virtuality continuum (RVC). Existing methods tend to rely on digital replicas or simple visual transitions, neglecting to address the aesthetic discontinuities between real and virtual environments, especially in highly stylized VR games. This paper introduces the Environment-Aware Stylized Transition (EAST) framework, which employs a novel style-transferred 3D Gaussian Splatting (3DGS) technique to transfer real-world interruptions into the virtual environment with seamless aesthetic consistency. Rather than merely transforming the real world into game-like visuals, EAST minimizes the disruptive impact of interruptions by integrating real-world elements within the framework. Qualitative user studies demonstrate significant enhancements in cognitive comfort and emotional continuity during transitions, while quantitative experiments highlight EAST's ability to maintain visual coherence across diverse VR styles.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faEAST\u6846\u67b6\uff0c\u5229\u7528\u98ce\u683c\u8f6c\u79fb\u76843D\u9ad8\u65af\u70b9\u9635\u6280\u672f\uff0c\u5b9e\u73b0\u771f\u5b9e\u5e72\u6270\u4e0e\u865a\u62df\u73af\u5883\u7684\u7f8e\u5b66\u4e00\u81f4\u6027\uff0c\u4ece\u800c\u63d0\u5347\u8ba4\u77e5\u548c\u60c5\u611f\u4f53\u9a8c\u3002", "motivation": "\u89e3\u51b3\u865a\u62df\u73b0\u5b9e\u6e38\u620f\u4e2d\u5728\u73b0\u5b9e\u548c\u865a\u62df\u73af\u5883\u4e4b\u95f4\u8fc7\u6e21\u65f6\u7684\u6c89\u6d78\u611f\u6311\u6218\uff0c\u7279\u522b\u662f\u5904\u7406\u9ad8\u5ea6\u98ce\u683c\u5316\u7684VR\u6e38\u620f\u4e2d\u7684\u7f8e\u5b66\u4e0d\u8fde\u7eed\u6027\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u98ce\u683c\u8f6c\u79fb\u76843D\u9ad8\u65af\u70b9\u9635\u6280\u672f\uff0c\u5c06\u771f\u5b9e\u4e16\u754c\u7684\u5e72\u6270\u6574\u5408\u5230\u865a\u62df\u73af\u5883\u4e2d\uff0c\u4fdd\u6301\u89c6\u89c9\u4e00\u81f4\u6027\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cEAST\u6846\u67b6\u63d0\u5347\u4e86\u8ba4\u77e5\u8212\u9002\u5ea6\u548c\u60c5\u611f\u8fde\u7eed\u6027\uff0c\u5b9a\u91cf\u5b9e\u9a8c\u4e5f\u663e\u793a\u51fa\u5176\u5728\u4e0d\u540cVR\u98ce\u683c\u4e0b\u4fdd\u6301\u89c6\u89c9\u4e00\u81f4\u6027\u7684\u80fd\u529b\u3002", "conclusion": "EAST\u6846\u67b6\u901a\u8fc7\u98ce\u683c\u8f6c\u79fb\u76843D\u9ad8\u65af\u70b9\u9635\u6280\u672f\u6210\u529f\u5b9e\u73b0\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u5e72\u6270\u4e0e\u865a\u62df\u73af\u5883\u4e4b\u95f4\u7684\u65e0\u7f1d\u7f8e\u5b66\u4e00\u81f4\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u7684\u8ba4\u77e5\u8212\u9002\u5ea6\u548c\u60c5\u611f\u8fde\u7eed\u6027\u3002"}}
{"id": "2511.22087", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.22087", "abs": "https://arxiv.org/abs/2511.22087", "authors": ["Tai Inui", "Jee-Hwan Ryu"], "title": "SoftNash: Entropy-Regularized Nash Games for Non-Fighting Virtual Fixtures", "comment": null, "summary": "Virtual fixtures (VFs) improve precision in teleoperation but often ``fight'' the user, inflating mental workload and eroding the sense of agency. We propose Soft-Nash Virtual Fixtures, a game-theoretic shared-control policy that softens the classic two-player linear-quadratic (LQ) Nash solution by inflating the fixture's effort weight with a single, interpretable scalar parameter $\u03c4$. This yields a continuous dial on controller assertiveness: $\u03c4=0$ recovers a hard, performance-focused Nash / virtual fixture controller, while larger $\u03c4$ reduce gains and pushback, yet preserve the equilibrium structure and continuity of closed-loop stability. We derive Soft-Nash from both a KL-regularized trust-region and a maximum-entropy viewpoint, obtaining a closed-form robot best response that shrinks authority and aligns the fixture with the operator's input as $\u03c4$ grows. We implement Soft-Nash on a 6-DoF haptic device in 3D tracking task ($n=12$). Moderate softness ($\u03c4\\approx 1-3$, especially $\u03c4=2$) maintains tracking error statistically indistinguishable from a tuned classic VF while sharply reducing controller-user conflict, lowering NASA-TLX workload, and increasing Sense of Agency (SoAS). A composite BalancedScore that combines normalized accuracy and non-fighting behavior peaks near $\u03c4=2-3$. These results show that a one-parameter Soft-Nash policy can preserve accuracy while improving comfort and perceived agency, providing a practical and interpretable pathway to personalized shared control in haptics and teleoperation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u865a\u62df\u5939\u5177\u7b56\u7565Soft-Nash\uff0c\u5229\u7528\u4e00\u4e2a\u53ef\u8c03\u53c2\u6570\u6539\u5584\u8fdc\u7a0b\u64cd\u4f5c\u4e2d\u7684\u7cbe\u786e\u5ea6\u4e0e\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u865a\u62df\u5939\u5177\u5728\u8fdc\u7a0b\u64cd\u4f5c\u4e2d\u63d0\u9ad8\u7cbe\u786e\u5ea6\uff0c\u4f46\u4f1a\u589e\u52a0\u5fc3\u7406\u8d1f\u62c5\uff0c\u524a\u5f31\u64cd\u4f5c\u611f\u3002", "method": "Soft-Nash\u662f\u4e00\u4e2a\u57fa\u4e8e\u535a\u5f08\u8bba\u7684\u5171\u4eab\u63a7\u5236\u7b56\u7565\uff0c\u901a\u8fc7\u8c03\u8282\u4e00\u4e2a\u53c2\u6570\u03c4\u6765\u63a7\u5236\u865a\u62df\u5939\u5177\u7684\u4f5c\u7528\u529b\u3002", "result": "\u5b9e\u65bd\u5728\u516d\u81ea\u7531\u5ea6\u89e6\u89c9\u8bbe\u5907\u4e0a\uff0c\u9002\u5f53\u7684\u53c2\u6570\u03c4(\u7ea61-3)\u80fd\u964d\u4f4e\u7528\u6237\u8d1f\u62c5\u548c\u51b2\u7a81\uff0c\u540c\u65f6\u8ddf\u8e2a\u8bef\u5dee\u4e0e\u4f20\u7edf\u865a\u62df\u5939\u5177\u76f8\u5f53\u3002", "conclusion": "Soft-Nash\u7b56\u7565\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e2a\u6027\u5316\u7684\u5171\u4eab\u63a7\u5236\u65b9\u5f0f\uff0c\u65e2\u80fd\u4fdd\u6301\u7cbe\u5ea6\uff0c\u53c8\u63d0\u5347\u8212\u9002\u611f\u548c\u64cd\u4f5c\u611f\u3002"}}
{"id": "2511.22269", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.22269", "abs": "https://arxiv.org/abs/2511.22269", "authors": ["Ruoyu Wen", "Xiaoli Wu", "Kunal Gupta", "Simon Hoermann", "Mark Billinghurst", "Alaeddin Nassani", "Dwain Allan", "Thammathip Piumsomboon"], "title": "Investigating AI in Peer Support via Multi-Module System-Driven Embodied Conversational Agents", "comment": null, "summary": "Young people's mental well-being is a global concern, with peer support playing a key role in daily emotional regulation. Conversational agents are increasingly viewed as promising tools for delivering accessible, personalised peer support, particularly where professional counselling is limited. However, existing systems often suffer from rigid input formats, scripted responses, and limited emotional sensitivity. The emergence of large language models introduces new possibilities for generating flexible, context-aware, and empathetic responses. To explore how individuals with psychological training perceive such systems in peer support contexts, we developed an LLM-based multi-module system to drive embodied conversational agents informed by Cognitive Behavioral Therapy (CBT). In a user study (N=10), we qualitatively examined participants' perceptions, focusing on trust, response quality, workflow integration, and design opportunities for future mental well-being support systems.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u8bdd\u4ee3\u7406\u5728\u652f\u6301\u5e74\u8f7b\u4eba\u5fc3\u7406\u5065\u5eb7\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u5b9a\u6027\u7528\u6237\u7814\u7a76\u5206\u6790\u4e86\u7528\u6237\u5bf9\u8fd9\u4e9b\u7cfb\u7edf\u7684\u770b\u6cd5\u3002", "motivation": "\u5e74\u8f7b\u4eba\u7684\u5fc3\u7406\u5065\u5eb7\u95ee\u9898\u65e5\u76ca\u53d7\u5230\u5173\u6ce8\uff0c\u800c\u540c\u4f34\u652f\u6301\u5728\u60c5\u611f\u8c03\u8282\u4e2d\u53d1\u6325\u7740\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u73b0\u6709\u652f\u6301\u7cfb\u7edf\u5728\u60c5\u611f\u654f\u611f\u6027\u548c\u7075\u6d3b\u6027\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u56e0\u6b64\u4e9f\u9700\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6a21\u5757\u7cfb\u7edf\uff0c\u7ed3\u5408\u8ba4\u77e5\u884c\u4e3a\u7597\u6cd5\uff08CBT\uff09\uff0c\u5e76\u8fdb\u884c\u4e86\u4e00\u9879\u7528\u6237\u7814\u7a76\uff0c\u53c2\u4e0e\u8005\u53cd\u9988\u5bf9\u7cfb\u7edf\u7684\u4fe1\u4efb\u3001\u54cd\u5e94\u8d28\u91cf\u3001\u5de5\u4f5c\u6d41\u7a0b\u7684\u6574\u5408\u4ee5\u53ca\u672a\u6765\u8bbe\u8ba1\u7684\u673a\u4f1a\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u53c2\u4e0e\u8005\u5bf9\u8fd9\u7c7b\u7cfb\u7edf\u7684\u4fe1\u4efb\u548c\u54cd\u5e94\u8d28\u91cf\u8868\u73b0\u51fa\u79ef\u6781\u770b\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u5c06\u8fd9\u4e9b\u7cfb\u7edf\u4e0e\u73b0\u6709\u5de5\u4f5c\u6d41\u7a0b\u6574\u5408\u7684\u6f5c\u5728\u8bbe\u8ba1\u673a\u4f1a\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684\u5bf9\u8bdd\u4ee3\u7406\u53ef\u4ee5\u5728\u540c\u4f34\u652f\u6301\u4e2d\u63d0\u4f9b\u7075\u6d3b\u3001\u654f\u611f\u548c\u9ad8\u8d28\u91cf\u7684\u60c5\u611f\u56de\u5e94\uff0c\u672a\u6765\u5e94\u8fdb\u4e00\u6b65\u4f18\u5316\u8bbe\u8ba1\u4ee5\u589e\u5f3a\u4fe1\u4efb\u548c\u5de5\u4f5c\u6d41\u7a0b\u6574\u5408\u3002"}}
{"id": "2511.22100", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22100", "abs": "https://arxiv.org/abs/2511.22100", "authors": ["Zelong Zhou", "Wenrui Chen", "Zeyun Hu", "Qiang Diao", "Qixin Gao", "Yaonan Wang"], "title": "Design of an Adaptive Modular Anthropomorphic Dexterous Hand for Human-like Manipulation", "comment": "7 pages, 8 figures", "summary": "Biological synergies have emerged as a widely adopted paradigm for dexterous hand design, enabling human-like manipulation with a small number of actuators. Nonetheless, excessive coupling tends to diminish the dexterity of hands. This paper tackles the trade-off between actuation complexity and dexterity by proposing an anthropomorphic finger topology with 4 DoFs driven by 2 actuators, and by developing an adaptive, modular dexterous hand based on this finger topology. We explore the biological basis of hand synergies and human gesture analysis, translating joint-level coordination and structural attributes into a modular finger architecture. Leveraging these biomimetic mappings, we design a five-finger modular hand and establish its kinematic model to analyze adaptive grasping and in-hand manipulation. Finally, we construct a physical prototype and conduct preliminary experiments, which validate the effectiveness of the proposed design and analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u4eba\u5f62\u624b\u6307\u62d3\u6251\u53ca\u6a21\u5757\u5316\u7075\u5de7\u624b\uff0c\u4ee5\u5e73\u8861\u9a71\u52a8\u590d\u6742\u6027\u4e0e\u7075\u6d3b\u6027\uff0c\u9a8c\u8bc1\u4e86\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u3002", "motivation": "\u63a2\u7d22\u751f\u7269\u624b\u90e8\u534f\u540c\u4f5c\u7528\u548c\u4eba\u7c7b\u624b\u52bf\u5206\u6790\uff0c\u4ece\u800c\u8bbe\u8ba1\u51fa\u66f4\u7075\u5de7\u7684\u4eba\u5f62\u624b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e4\u4e2a\u81ea\u7531\u5ea6\u548c2\u4e2a\u9a71\u52a8\u5668\u7684\u4eba\u5f62\u624b\u6307\u62d3\u6251\uff0c\u5e76\u5f00\u53d1\u4e00\u79cd\u81ea\u9002\u5e94\u7684\u6a21\u5757\u5316\u7075\u5de7\u624b", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u4e94\u6307\u6a21\u5757\u5316\u624b\uff0c\u5e76\u5efa\u7acb\u4e86\u5176\u8fd0\u52a8\u5b66\u6a21\u578b\uff0c\u4ee5\u5206\u6790\u81ea\u9002\u5e94\u6293\u63e1\u548c\u624b\u5185\u64cd\u63a7", "conclusion": "\u901a\u8fc7\u521d\u6b65\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u8bbe\u8ba1\u548c\u5206\u6790\u7684\u6709\u6548\u6027"}}
{"id": "2511.22337", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.22337", "abs": "https://arxiv.org/abs/2511.22337", "authors": ["Sachin Kumar Singh", "Ko Watanabe", "Brian Moser", "Andreas Dengel"], "title": "HandyLabel: Towards Post-Processing to Real-Time Annotation Using Skeleton Based Hand Gesture Recognition", "comment": null, "summary": "The success of machine learning is deeply linked to the availability of high-quality training data, yet retrieving and manually labeling new data remains a time-consuming and error-prone process. Traditional annotation tools, such as Label Studio, often require post-processing, where users label data after it has been recorded. Post-processing is highly time-consuming and labor-intensive, especially with large datasets, and may lead to erroneous annotations due to the difficulty of subjects' memory tasks when labeling cognitive activities such as emotions or comprehension levels. In this work, we introduce HandyLabel, a real-time annotation tool that leverages hand gesture recognition to map hand signs for labeling. The application enables users to customize gesture mappings through a web-based interface, allowing for real-time annotations. To ensure the performance of HandyLabel, we evaluate several hand gesture recognition models on an open-source hand sign (HaGRID) dataset, with and without skeleton-based preprocessing. We discovered that ResNet50 with preprocessed skeleton-based images performs an F1-score of 0.923. To validate the usability of HandyLabel, a user study was conducted with 46 participants. The results suggest that 88.9% of participants preferred HandyLabel over traditional annotation tools.", "AI": {"tldr": "HandyLabel\u662f\u4e00\u79cd\u5b9e\u65f6\u624b\u52bf\u8bc6\u522b\u6807\u6ce8\u5de5\u5177\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6807\u6ce8\u6548\u7387\uff0c\u4f18\u4e8e\u4f20\u7edf\u5de5\u5177\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6807\u6ce8\u5de5\u5177\u8017\u65f6\u957f\u3001\u5bb9\u6613\u51fa\u9519\u7684\u95ee\u9898\uff0c\u63d0\u5347\u673a\u5668\u5b66\u4e60\u6570\u636e\u6807\u6ce8\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u901a\u8fc7\u624b\u52bf\u8bc6\u522b\u6280\u672f\uff0c\u5141\u8bb8\u7528\u6237\u5b9e\u65f6\u6807\u8bb0\u624b\u52bf\uff0c\u5e76\u5728HaGRID\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u624b\u52bf\u8bc6\u522b\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "ResNet50\u5728\u7ecf\u8fc7\u9aa8\u67b6\u9884\u5904\u7406\u7684\u56fe\u50cf\u4e0a\uff0cF1\u5206\u6570\u4e3a0.923\uff1b88.9%\u7684\u53c2\u4e0e\u8005\u66f4\u559c\u6b22\u4f7f\u7528HandyLabel\u3002", "conclusion": "HandyLabel\u662f\u4e00\u6b3e\u5b9e\u65f6\u6ce8\u91ca\u5de5\u5177\uff0c\u7528\u6237\u666e\u904d\u504f\u7231\u5176\u76f8\u8f83\u4e8e\u4f20\u7edf\u6ce8\u91ca\u5de5\u5177\u7684\u4fbf\u6377\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2511.22195", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22195", "abs": "https://arxiv.org/abs/2511.22195", "authors": ["Zhiyang Liu", "Ruiteng Zhao", "Lei Zhou", "Chengran Yuan", "Yuwei Wu", "Sheng Guo", "Zhengshen Zhang", "Chenchen Liu", "Marcelo H Ang"], "title": "3D Affordance Keypoint Detection for Robotic Manipulation", "comment": "Accepted to IROS 2024", "summary": "This paper presents a novel approach for affordance-informed robotic manipulation by introducing 3D keypoints to enhance the understanding of object parts' functionality. The proposed approach provides direct information about what the potential use of objects is, as well as guidance on where and how a manipulator should engage, whereas conventional methods treat affordance detection as a semantic segmentation task, focusing solely on answering the what question. To address this gap, we propose a Fusion-based Affordance Keypoint Network (FAKP-Net) by introducing 3D keypoint quadruplet that harnesses the synergistic potential of RGB and Depth image to provide information on execution position, direction, and extent. Benchmark testing demonstrates that FAKP-Net outperforms existing models by significant margins in affordance segmentation task and keypoint detection task. Real-world experiments also showcase the reliability of our method in accomplishing manipulation tasks with previously unseen objects.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u5173\u952e\u70b9\u7684\u65b0\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u65b9\u6cd5FAKP-Net\uff0c\u901a\u8fc7\u878d\u5408RGB\u548c\u6df1\u5ea6\u56fe\u50cf\u4fe1\u606f\uff0c\u63d0\u9ad8\u4e86\u5bf9\u7269\u4f53\u529f\u80fd\u6027\u90e8\u4f4d\u7684\u7406\u89e3\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4ec5\u5173\u6ce8\u7269\u4f53\u7684\u8bed\u4e49\u5206\u5272\uff0c\u672a\u80fd\u5145\u5206\u8bc6\u522b\u7269\u4f53\u7684\u53ef\u7528\u6027\u548c\u64cd\u63a7\u65b9\u5f0f\u3002", "method": "\u91c7\u7528\u878d\u5408\u7684\u53ef\u7528\u6027\u5173\u952e\u70b9\u7f51\u7edcFAKP-Net\uff0c\u901a\u8fc7\u5f15\u51653D\u5173\u952e\u70b9\u56db\u91cd\u4f53\u7ed3\u5408RGB\u4e0e\u6df1\u5ea6\u56fe\u50cf\u7684\u4fe1\u606f\u3002", "result": "FAKP-Net\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u8d85\u8d8a\u4e86\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u5728\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u5728\u5904\u7406\u65b0\u7269\u4f53\u65f6\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684FAKP-Net\u5728\u53ef\u7528\u6027\u5206\u5272\u4efb\u52a1\u548c\u5173\u952e\u70b9\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u5728\u5b9e\u9645\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u53ef\u9760\u6027\u3002"}}
{"id": "2511.22352", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.22352", "abs": "https://arxiv.org/abs/2511.22352", "authors": ["Jarne Thys", "Davy Vanacken", "Gustavo Rovelo Ruiz"], "title": "Engineering Trustworthy Automation: Design Principles and Evaluation for AutoML Tools for Novices", "comment": "Submitted version accepted for publication in an LNCS Volume \"Engineering Interactive Computer Systems - EICS 2025 - International Workshops and Doctoral Consortium\"", "summary": "AutoML systems targeting novices often prioritize algorithmic automation over usability, leaving gaps in users' understanding, trust, and end-to-end workflow support. To address these issues, we propose an abstract pipeline that covers data intake, guided configuration, training, evaluation, and inference. To examine the abstract pipeline, we report a user study where we assess trust, understandability, and UX of a prototype implementation. In a 24-participant study, all participants successfully built their own models, UEQ ratings were positive, yet experienced users reported higher trust and understanding than novices. Based on this study, we propose four design principles to improve the design of AutoML systems targeting novices: (P1) support first-model success to enhance user self-efficacy, (P2) provide explanations to help users form correct mental models and develop appropriate levels of reliance, (P3) provide abstractions and context-aware assistance to keep users in their zone of proximal development, and (P4) ensure predictability and safeguards to strengthen users' sense of control.", "AI": {"tldr": "\u9488\u5bf9\u521d\u5b66\u8005\u7684AutoML\u7cfb\u7edf\u5e94\u6ce8\u91cd\u63d0\u9ad8\u7528\u6237\u7684\u6210\u529f\u611f\u548c\u4fe1\u4efb\uff0c\u63d0\u51fa\u56db\u4e2a\u8bbe\u8ba1\u539f\u5219\u4ee5\u63d0\u5347\u7cfb\u7edf\u8bbe\u8ba1\u7684\u53ef\u7528\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709AutoML\u7cfb\u7edf\u5728\u7b97\u6cd5\u81ea\u52a8\u5316\u4e0e\u7528\u6237\u6613\u7528\u6027\u4e4b\u95f4\u7684\u7f3a\u53e3\uff0c\u4ece\u800c\u63d0\u5347\u7528\u6237\u7406\u89e3\u3001\u4fe1\u4efb\u548c\u5168\u6d41\u7a0b\u652f\u6301\u3002", "method": "\u901a\u8fc7\u7528\u6237\u7814\u7a76\u8bc4\u4f30\u539f\u578b\u5b9e\u73b0\u7684\u4fe1\u4efb\u5ea6\u3001\u53ef\u7406\u89e3\u6027\u548c\u7528\u6237\u4f53\u9a8c\uff0c\u5e76\u572824\u540d\u53c2\u4e0e\u8005\u4e2d\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u53c2\u4e0e\u8005\u6210\u529f\u6784\u5efa\u4e86\u4ed6\u4eec\u81ea\u5df1\u7684\u6a21\u578b\uff0c\u7528\u6237\u4f53\u9a8c\u95ee\u5377\uff08UEQ\uff09\u8bc4\u5206\u79ef\u6781\uff0c\u5c3d\u7ba1\u7ecf\u9a8c\u4e30\u5bcc\u7684\u7528\u6237\u6bd4\u65b0\u624b\u62a5\u544a\u4e86\u66f4\u9ad8\u7684\u4fe1\u4efb\u548c\u7406\u89e3\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u4e86\u56db\u4e2a\u8bbe\u8ba1\u539f\u5219\u4ee5\u6539\u5584\u9488\u5bf9\u521d\u5b66\u8005\u7684AutoML\u7cfb\u7edf\u7684\u8bbe\u8ba1\u3002"}}
{"id": "2511.22225", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.22225", "abs": "https://arxiv.org/abs/2511.22225", "authors": ["Gabriel Aguirre", "Simay Atasoy Bing\u00f6l", "Heiko Hamann", "Jonas Kuckling"], "title": "Bayesian Decentralized Decision-making for Multi-Robot Systems: Sample-efficient Estimation of Event Rates", "comment": "7 pages, 3 figures, submitted to IEEE MRS 2025", "summary": "Effective collective decision-making in swarm robotics often requires balancing exploration, communication and individual uncertainty estimation, especially in hazardous environments where direct measurements are limited or costly. We propose a decentralized Bayesian framework that enables a swarm of simple robots to identify the safer of two areas, each characterized by an unknown rate of hazardous events governed by a Poisson process. Robots employ a conjugate prior to gradually predict the times between events and derive confidence estimates to adapt their behavior. Our simulation results show that the robot swarm consistently chooses the correct area while reducing exposure to hazardous events by being sample-efficient. Compared to baseline heuristics, our proposed approach shows better performance in terms of safety and speed of convergence. The proposed scenario has potential to extend the current set of benchmarks in collective decision-making and our method has applications in adaptive risk-aware sampling and exploration in hazardous, dynamic environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u7684\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u7fa4\u4f53\u673a\u5668\u4eba\u5728\u5371\u9669\u73af\u5883\u4e2d\u7684\u96c6\u4f53\u51b3\u7b56\u80fd\u529b\u3002", "motivation": "\u5728\u5371\u9669\u73af\u5883\u4e2d\uff0c\u7fa4\u4f53\u673a\u5668\u4eba\u9762\u4e34\u63a2\u7d22\u3001\u6c9f\u901a\u548c\u4e2a\u4f53\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u5e73\u8861\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5171\u8f6d\u5148\u9a8c\uff0c\u4ee5\u9884\u6d4b\u672a\u77e5\u7684\u5371\u9669\u4e8b\u4ef6\u53d1\u751f\u65f6\u95f4\u5e76\u63a8\u5bfc\u4fe1\u5fc3\u4f30\u8ba1\uff0c\u4ece\u800c\u8c03\u6574\u884c\u4e3a\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u8868\u660e\uff0c\u673a\u5668\u4eba\u7fa4\u4f53\u80fd\u591f\u6709\u6548\u9009\u62e9\u66f4\u5b89\u5168\u7684\u533a\u57df\uff0c\u5e76\u51cf\u5c11\u66b4\u9732\u4e8e\u5371\u9669\u4e8b\u4ef6\u7684\u98ce\u9669\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u548c\u6536\u655b\u901f\u5ea6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5e76\u53ef\u7528\u4e8e\u5371\u9669\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u98ce\u9669\u611f\u77e5\u91c7\u6837\u548c\u63a2\u7d22\u3002"}}
{"id": "2511.22420", "categories": ["cs.HC", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.22420", "abs": "https://arxiv.org/abs/2511.22420", "authors": ["Sebe Vanbrabant", "Gustavo Rovelo Ruiz", "Davy Vanacken"], "title": "MATCH: Engineering Transparent and Controllable Conversational XAI Systems through Composable Building Blocks", "comment": "Submitted Version accepted for publication in an LNCS Volume \"Engineering Interactive Computer Systems - EICS 2025 - International Workshops and Doctoral Consortium\"", "summary": "While the increased integration of AI technologies into interactive systems enables them to solve an increasing number of tasks, the black-box problem of AI models continues to spread throughout the interactive system as a whole. Explainable AI (XAI) techniques can make AI models more accessible by employing post-hoc methods or transitioning to inherently interpretable models. While this makes individual AI models clearer, the overarching system architecture remains opaque. This challenge not only pertains to standard XAI techniques but also to human examination and conversational XAI approaches that need access to model internals to interpret them correctly and completely. To this end, we propose conceptually representing such interactive systems as sequences of structural building blocks. These include the AI models themselves, as well as control mechanisms grounded in literature. The structural building blocks can then be explained through complementary explanatory building blocks, such as established XAI techniques like LIME and SHAP. The flow and APIs of the structural building blocks form an unambiguous overview of the underlying system, serving as a communication basis for both human and automated agents, thus aligning human and machine interpretability of the embedded AI models. In this paper, we present our flow-based approach and a selection of building blocks as MATCH: a framework for engineering Multi-Agent Transparent and Controllable Human-centered systems. This research contributes to the field of (conversational) XAI by facilitating the integration of interpretability into existing interactive systems.", "AI": {"tldr": "\u968f\u7740AI\u7684\u4e0d\u65ad\u878d\u5165\uff0c\u4ea4\u4e92\u7cfb\u7edf\u7684\u9ed1\u7bb1\u95ee\u9898\u65e5\u76ca\u4e25\u91cd\u3002\u672c\u6587\u63d0\u51faMATCH\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6784\u5efa\u5757\u548c\u73b0\u6709\u53ef\u89e3\u91caAI\u6280\u672f\uff0c\u63d0\u9ad8AI\u6a21\u578b\u53ca\u5176\u7cfb\u7edf\u6574\u4f53\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u968f\u7740AI\u6280\u672f\u7684\u666e\u53ca\uff0c\u4ea4\u4e92\u7cfb\u7edf\u7684\u9ed1\u7bb1\u95ee\u9898\u65e5\u76ca\u4e25\u91cd\uff0c\u4e9f\u9700\u63d0\u5347AI\u6a21\u578b\u53ca\u5176\u7cfb\u7edf\u67b6\u6784\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6d41\u7a0b\u7684\u6784\u5efa\u5757\u65b9\u6cd5\uff0c\u5229\u7528\u73b0\u6709\u7684\u53ef\u89e3\u91caAI\u6280\u672f\uff08\u5982LIME\u548cSHAP\uff09\u89e3\u91ca\u4ea4\u4e92\u7cfb\u7edf\u7684\u7ed3\u6784\u6027\u6784\u5efa\u5757\u3002", "result": "\u901a\u8fc7MATCH\u6846\u67b6\uff0c\u4e3a\u4eba\u673a\u5171\u540c\u89e3\u91ca\u5d4c\u5165\u7684AI\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6e05\u6670\u7684\u7cfb\u7edf\u6982\u89c8\uff0c\u589e\u5f3a\u4e86\u4eba\u7c7b\u548c\u673a\u5668\u5bf9\u4e8e\u6a21\u578b\u7684\u7406\u89e3\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86MATCH\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6784\u5efa\u5757\u4f7f\u4ea4\u4e92\u7cfb\u7edf\u900f\u660e\u5316\u5e76\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\uff0c\u4ece\u800c\u4fc3\u8fdb\u73b0\u6709\u4e92\u52a8\u7cfb\u7edf\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u96c6\u6210\u3002"}}
{"id": "2511.22238", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22238", "abs": "https://arxiv.org/abs/2511.22238", "authors": ["Ryosuke Ofuchi", "Yuichiro Toda", "Naoki Masuyama", "Takayuki Matsuno"], "title": "MLATC: Fast Hierarchical Topological Mapping from 3D LiDAR Point Clouds Based on Adaptive Resonance Theory", "comment": null, "summary": "This paper addresses the problem of building global topological maps from 3D LiDAR point clouds for autonomous mobile robots operating in large-scale, dynamic, and unknown environments. Adaptive Resonance Theory-based Topological Clustering with Different Topologies (ATC-DT) builds global topological maps represented as graphs while mitigating catastrophic forgetting during sequential processing. However, its winner selection mechanism relies on an exhaustive nearest-neighbor search over all existing nodes, leading to scalability limitations as the map grows. To address this challenge, we propose a hierarchical extension called Multi-Layer ATC (MLATC). MLATC organizes nodes into a hierarchy, enabling the nearest-neighbor search to proceed from coarse to fine resolutions, thereby drastically reducing the number of distance evaluations per query. The number of layers is not fixed in advance. MLATC employs an adaptive layer addition mechanism that automatically deepens the hierarchy when lower layers become saturated, keeping the number of user-defined hyperparameters low. Simulation experiments on synthetic large-scale environments show that MLATC accelerates topological map building compared to the original ATC-DT and exhibits a sublinear, approximately logarithmic scaling of search time with respect to the number of nodes. Experiments on campus-scale real-world LiDAR datasets confirm that MLATC maintains a millisecond-level per-frame runtime and enables real-time global topological map building in large-scale environments, significantly outperforming the original ATC-DT in terms of computational efficiency.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u5c42\u81ea\u9002\u5e94\u805a\u7c7b\u65b9\u6cd5MLATC\uff0c\u7528\u4e8e\u89e3\u51b3\u5728\u5927\u89c4\u6a21\u52a8\u6001\u73af\u5883\u4e2d\u81ea\u9002\u5e94\u79fb\u52a8\u673a\u5668\u4eba\u6784\u5efa\u5168\u7403\u62d3\u6251\u5730\u56fe\u65f6\u7684\u6548\u7387\u548c\u6269\u5c55\u6027\u95ee\u9898\u3002", "motivation": "\u5728\u52a8\u6001\u548c\u672a\u77e5\u7684\u73af\u5883\u4e2d\uff0c\u6784\u5efa\u6709\u6548\u7684\u5168\u7403\u62d3\u6251\u5730\u56fe\u662f\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u7684\u4e00\u9879\u91cd\u8981\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c42\u6b21\u5316\u7684\u591a\u5c42ATC\uff0c\u652f\u6301\u4ece\u7c97\u5230\u7ec6\u7684\u90bb\u8fd1\u641c\u7d22\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u5c42\u7ea7\u589e\u52a0\u673a\u5236\uff0c\u964d\u4f4e\u4e86\u8d85\u53c2\u6570\u7684\u9700\u6c42\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u591a\u5c42\u81ea\u9002\u5e94\u5171\u632f\u7406\u8bba\u62d3\u6251\u805a\u7c7b\uff08MLATC\uff09\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u5927\u89c4\u6a21\u52a8\u6001\u73af\u5883\u4e2d\u9ad8\u6548\u6784\u5efa\u5168\u7403\u62d3\u6251\u5730\u56fe\u3002", "conclusion": "MLATC\u663e\u8457\u63d0\u9ad8\u4e86\u62d3\u6251\u5730\u56fe\u6784\u5efa\u7684\u6548\u7387\uff0c\u5e76\u786e\u4fdd\u4e86\u5b9e\u65f6\u6027\u80fd\uff0c\u514b\u670d\u4e86\u539fATC-DT\u5728\u6269\u5c55\u6027\u4e0a\u7684\u9650\u5236\u3002"}}
{"id": "2511.22617", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.22617", "abs": "https://arxiv.org/abs/2511.22617", "authors": ["Johan Sebasti\u00e1n Galindez-Acosta", "Juan Jos\u00e9 Giraldo-Huertas"], "title": "A race to belief: How Evidence Accumulation shapes trust in AI and Human informants", "comment": "24 pages, 5 figures", "summary": "The integration of artificial intelligence into everyday decision-making has reshaped patterns of selective trust, yet the cognitive mechanisms behind context-dependent preferences for AI versus human informants remain unclear. We applied a Bayesian Hierarchical Sequential Sampling Model (HSSM) to analyze how 102 Colombian university students made trust decisions across 30 epistemic (factual) and social (interpersonal) scenarios.\n  Results show that context-dependent trust is primarily driven by differences in drift rate (v), the rate of evidence accumulation, rather than initial bias (z) or response caution (a). Epistemic scenarios produced strong negative drift rates (mean v = -1.26), indicating rapid evidence accumulation favoring AI, whereas social scenarios yielded positive drift rates (mean v = 0.70) favoring humans. Starting points were near neutral (z = 0.52), indicating minimal prior bias.\n  Drift rate showed a strong within-subject association with signed confidence (Fisher-z-averaged r = 0.736; 95 percent bootstrap CI 0.699 to 0.766; 97.8 percent of individual correlations positive, N = 93), suggesting that model-derived evidence accumulation closely mirrors participants' moment-to-moment confidence. These dynamics may help explain the fragility of AI trust: in epistemic domains, rapid but low-vigilance evidence processing may promote uncalibrated reliance on AI that collapses quickly after errors.\n  Interpreted through epistemic vigilance theory, the results indicate that domain-specific vigilance mechanisms modulate evidence accumulation. The findings inform AI governance by highlighting the need for transparency features that sustain vigilance without sacrificing efficiency, offering a mechanistic account of selective trust in human-AI collaboration.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u54e5\u4f26\u6bd4\u4e9a\u5927\u5b66\u751f\u5728\u4e0d\u540c\u60c5\u5883\u4e0b\u5bf9AI\u4e0e\u4eba\u7c7b\u7684\u4fe1\u4efb\u51b3\u7b56\uff0c\u53d1\u73b0\u4fe1\u4efb\u4e3b\u8981\u53d7\u8bc1\u636e\u7d2f\u79ef\u901f\u7387\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u5728AI\u6cbb\u7406\u4e2d\u9700\u8981\u4fdd\u6301\u900f\u660e\u5ea6\u4ee5\u652f\u6301\u51b3\u7b56\u8b66\u89c9\u6027\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u5728\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u878d\u5165\u65e5\u5e38\u51b3\u7b56\u7684\u8fc7\u7a0b\u4e2d\uff0c\u4eba\u4eec\u5bf9AI\u4e0e\u4eba\u7c7b\u4fe1\u606f\u6e90\u7684\u9009\u62e9\u6027\u4fe1\u4efb\u7684\u8ba4\u77e5\u673a\u5236\uff0c\u7279\u522b\u662f\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7684\u504f\u597d\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u5c42\u6b21\u5e8f\u5217\u91c7\u6837\u6a21\u578b\uff08HSSM\uff09\u5206\u6790102\u540d\u54e5\u4f26\u6bd4\u4e9a\u5927\u5b66\u751f\u572830\u4e2a\u4e8b\u5b9e\u548c\u793e\u4ea4\u60c5\u5883\u4e0b\u7684\u4fe1\u4efb\u51b3\u7b56\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7684\u4fe1\u4efb\u4e3b\u8981\u53d7\u8bc1\u636e\u7d2f\u79ef\u901f\u7387\uff08drift rate v\uff09\u7684\u5f71\u54cd\uff0c\u800c\u975e\u521d\u59cb\u504f\u89c1\uff08z\uff09\u6216\u53cd\u5e94\u8c28\u614e\u6027\uff08a\uff09\u3002\u5728\u4e8b\u5b9e\u60c5\u5883\u4e2d\uff0cdrift rate\u8868\u73b0\u4e3a\u5f3a\u70c8\u7684\u8d1f\u503c\uff0c\u8868\u660e\u5feb\u901f\u7684\u8bc1\u636e\u7d2f\u79ef\u503e\u5411\u4e8e\u4fe1\u4efbAI\uff0c\u800c\u5728\u793e\u4ea4\u60c5\u5883\u4e2d\u5219\u51fa\u73b0\u6b63\u503c\uff0c\u503e\u5411\u4e8e\u4fe1\u4efb\u4eba\u7c7b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4f9d\u636e\u8ba4\u77e5\u8b66\u89c9\u7406\u8bba\u89e3\u91ca\uff0c\u57df\u7279\u5b9a\u7684\u8b66\u89c9\u673a\u5236\u8c03\u8282\u8bc1\u636e\u7d2f\u79ef\uff0c\u8868\u660e\u5728AI\u6cbb\u7406\u4e2d\uff0c\u9700\u8981\u900f\u660e\u5ea6\u7279\u5f81\u6765\u7ef4\u6301\u8b66\u89c9\u6027\uff0c\u800c\u4e0d\u5f71\u54cd\u6548\u7387\uff0c\u4ece\u800c\u4e3a\u4eba\u7c7b\u4e0eAI\u534f\u4f5c\u4e2d\u7684\u9009\u62e9\u6027\u4fe1\u4efb\u63d0\u4f9b\u4e86\u673a\u5236\u6027\u89e3\u91ca\u3002"}}
{"id": "2511.22318", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22318", "abs": "https://arxiv.org/abs/2511.22318", "authors": ["Yuki Origane", "Koya Cho", "Hideyuki Tsukagoshi"], "title": "Soft Fluidic Sheet Transistor for Soft Robotic System Enabling Fluid Logic Operations", "comment": "7 pages, 16 figures", "summary": "Aiming to achieve both high functionality and flexibility in soft robot system, this paper presents a soft urethane sheet-like valve with an amplifier that can perform logical operations using only pneumatic signals. When the control chamber in the valve is pressurized, the main path is compressed along its central axis, buckling and being pressed,resulting in blockage. This allows control by a pressure signal smaller than that within the main channel. Furthermore, similar to transistors in electrical circuits, when combined, the proposed valve can perform a variety of logical operations. The basic type operates as a NOT logic element, which is named the fluidic sheet transistor (FST). By integrating multiple FSTs, logical operations such as positive logic, NAND, and NOR can be performed on a single sheet. This paper describes the operating principle, fabrication method, and characteristics of the FST,followed by a method for configuring logical operations.Moreover, we demonstrate the construction of a latch circuit(self-holding logic circuit) using FST, introducing a prototype of a fluid robot system that combines a tactile tube as a fluidic detector and fluid actuators. This demonstrates that it is possible to generate behavior that actively changes posture when hitting an obstacle using only air pressure from a single pipe, which verifies the effectiveness of the proposed methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u6c14\u52a8\u4fe1\u53f7\u8fdb\u884c\u903b\u8f91\u8fd0\u7b97\u7684\u8f6f\u805a\u6c28\u916f\u8584\u819c\u9600\uff0c\u521b\u9020\u51fa\u591a\u79cd\u903b\u8f91\u529f\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u8f6f\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u8f6f\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u529f\u80fd\u548c\u7075\u6d3b\u6027\u65b9\u9762\u7684\u9ad8\u6548\u6027\uff0c\u63a2\u7d22\u4e86\u5229\u7528\u6c14\u52a8\u4fe1\u53f7\u8fdb\u884c\u903b\u8f91\u64cd\u4f5c\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u548c\u5236\u9020\u8f6f\u805a\u6c28\u916f\u8584\u819c\u9600\uff0c\u5229\u7528\u6c14\u52a8\u4fe1\u53f7\u6267\u884c\u903b\u8f91\u64cd\u4f5c\uff0c\u5e76\u7ed3\u5408\u591a\u4e2aFST\u88c5\u7f6e\u8fdb\u884c\u590d\u6742\u903b\u8f91\u8fd0\u7b97\u3002", "result": "\u5f00\u53d1\u4e86\u4f7f\u7528\u6d41\u4f53\u8584\u819c\u6676\u4f53\u7ba1\u7684\u903b\u8f91\u7535\u8def\u793a\u4f8b\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u81ea\u4fdd\u6301\u903b\u8f91\u7535\u8def\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u53ca\u901a\u8fc7\u5355\u6839\u7ba1\u9053\u5b9e\u73b0\u673a\u5668\u4eba\u8fd0\u52a8\u7684\u80fd\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u6d41\u4f53\u8584\u819c\u6676\u4f53\u7ba1\uff08FST\uff09\u53ef\u4ee5\u901a\u8fc7\u6c14\u52a8\u4fe1\u53f7\u5b9e\u73b0\u591a\u79cd\u903b\u8f91\u64cd\u4f5c\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8f6f\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u6709\u6548\u6027\u548c\u6f5c\u529b\u3002"}}
{"id": "2511.22746", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.22746", "abs": "https://arxiv.org/abs/2511.22746", "authors": ["Sekoul Krastev", "Hilary Sweatman", "Anni Sternisko", "Steve Rathje"], "title": "Epistemic Fragility in Large Language Models: Prompt Framing Systematically Modulates Misinformation Correction", "comment": null, "summary": "As large language models (LLMs) rapidly displace traditional expertise, their capacity to correct misinformation has become a core concern. We investigate the idea that prompt framing systematically modulates misinformation correction - something we term 'epistemic fragility'. We manipulated prompts by open-mindedness, user intent, user role, and complexity. Across ten misinformation domains, we generated 320 prompts and elicited 2,560 responses from four frontier LLMs, which were coded for strength of misinformation correction and rectification strategy use. Analyses showed that creative intent, expert role, and closed framing led to a significant reduction in correction likelihood and effectiveness of used strategy. We also found striking model differences: Gemini 2.5 Pro had 74% lower odds of strong correction than Claude Sonnet 4.5. These findings highlight epistemic fragility as an important structural property of LLMs, challenging current guardrails and underscoring the need for alignment strategies that prioritize epistemic integrity over conversational compliance.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u63d0\u793a\u7684\u6784\u5efa\u5982\u4f55\u5f71\u54cd\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9519\u8bef\u4fe1\u606f\u7684\u7ea0\u6b63\u80fd\u529b\uff0c\u53d1\u73b0\u63d0\u793a\u7684\u7ed3\u6784\u4e0e\u6a21\u578b\u7279\u6027\u5728\u6b64\u8fc7\u7a0b\u4e2d\u8d77\u5230\u5173\u952e\u4f5c\u7528\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u666e\u53ca\uff0c\u5982\u4f55\u6709\u6548\u4fee\u6b63\u9519\u8bef\u4fe1\u606f\u6210\u4e3a\u5173\u952e\u95ee\u9898\uff0c\u5c24\u5176\u662f\u63d0\u793a\u7684\u6784\u5efa\u53ef\u80fd\u4f1a\u5f71\u54cd\u7ea0\u6b63\u7684\u6548\u679c\u3002", "method": "\u7814\u7a76\u8005\u901a\u8fc7\u64cd\u63a7\u4e0d\u540c\u7684\u63d0\u793a\u53d8\u91cf\uff08\u5982\u5f00\u653e\u6027\u3001\u7528\u6237\u610f\u56fe\u3001\u89d2\u8272\u548c\u590d\u6742\u6027\uff09\uff0c\u751f\u6210320\u4e2a\u63d0\u793a\uff0c\u4ece\u56db\u79cd\u524d\u6cbf\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u83b7\u53d62560\u4e2a\u54cd\u5e94\uff0c\u5e76\u5bf9\u5176\u8fdb\u884c\u7f16\u7801\u5206\u6790\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u521b\u9020\u6027\u610f\u56fe\u3001\u4e13\u5bb6\u89d2\u8272\u4ee5\u53ca\u5c01\u95ed\u6027\u63d0\u793a\u663e\u8457\u964d\u4f4e\u4e86\u4fe1\u606f\u7ea0\u6b63\u7684\u53ef\u80fd\u6027\u548c\u7b56\u7565\u7684\u6709\u6548\u6027\uff0c\u540c\u65f6\u4e0d\u540c\u6a21\u578b\u4e4b\u95f4\u7684\u8868\u73b0\u5dee\u5f02\u660e\u663e\u3002", "conclusion": "\u4f5c\u8005\u5efa\u8bae\uff0c\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u7b56\uff0c\u9700\u8981\u805a\u7126\u4e8e\u7ef4\u62a4\u77e5\u8bc6\u5b8c\u6574\u6027\uff0c\u800c\u975e\u4ec5\u4ec5\u4fdd\u8bc1\u5bf9\u8bdd\u7684\u5408\u89c4\u6027\u3002"}}
{"id": "2511.22338", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.22338", "abs": "https://arxiv.org/abs/2511.22338", "authors": ["Denghan Xiong", "Yanzhe Zhao", "Yutong Chen", "Zichun Wang"], "title": "Nonholonomic Narrow Dead-End Escape with Deep Reinforcement Learning", "comment": "14 pages, 5 figures, 1 table, submitted to arXiv", "summary": "Nonholonomic constraints restrict feasible velocities without reducing configuration-space dimension, which makes collision-free geometric paths generally non-executable for car-like robots. Ackermann steering further imposes curvature bounds and forbids in-place rotation, so escaping from narrow dead ends typically requires tightly sequenced forward and reverse maneuvers. Classical planners that decouple global search and local steering struggle in these settings because narrow passages occupy low-measure regions and nonholonomic reachability shrinks the set of valid connections, which degrades sampling efficiency and increases sensitivity to clearances. We study nonholonomic narrow dead-end escape for Ackermann vehicles and contribute three components. First, we construct a generator that samples multi-phase forward-reverse trajectories compatible with Ackermann kinematics and inflates their envelopes to synthesize families of narrow dead ends that are guaranteed to admit at least one feasible escape. Second, we construct a training environment that enforces kinematic constraints and train a policy using the soft actor-critic algorithm. Third, we evaluate against representative classical planners that combine global search with nonholonomic steering. Across parameterized dead-end families, the learned policy solves a larger fraction of instances, reduces maneuver count, and maintains comparable path length and planning time while under the same sensing and control limits. We provide our project as open source at https://github.com/gitagitty/cisDRL-RobotNav.git", "AI": {"tldr": "\u672c\u7814\u7a76\u9488\u5bf9Ackermann\u8f66\u8f86\u5728\u975e\u5b8c\u6574\u7ea6\u675f\u4e0b\u4ece\u7a84\u6b7b\u80e1\u540c\u9003\u751f\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u591a\u9636\u6bb5\u8f68\u8ff9\u751f\u6210\u5668\u3001\u8bad\u7ec3\u73af\u5883\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u7b56\u7565\u8bc4\u4f30\uff0c\u8868\u73b0\u8d85\u8fc7\u4f20\u7edf\u89c4\u5212\u65b9\u6cd5\u3002", "motivation": "\u975e\u5b8c\u6574\u7ea6\u675f\u4f7f\u5f97\u8f66\u7c7b\u673a\u5668\u4eba\u5728\u78b0\u649e\u81ea\u7531\u7684\u51e0\u4f55\u8def\u5f84\u4e0a\u65e0\u6cd5\u6267\u884c\uff0c\u5c24\u5176\u662f\u5728\u72ed\u7a84\u7684\u6b7b\u80e1\u540c\u4e2d\uff0c\u8fd9\u5bfc\u81f4\u79fb\u52a8\u7b56\u7565\u590d\u6742\u4e14\u96be\u4ee5\u5b9e\u73b0\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u751f\u6210\u5668\u7528\u4e8e\u91c7\u6837\u7b26\u5408Ackermann\u8fd0\u52a8\u5b66\u7684\u591a\u76f8\u4f4d\u524d\u540e\u8f68\u8ff9\uff0c\u5e76\u5229\u7528\u8f6f\u6f14\u5458-\u8bc4\u8bba\u5458\u7b97\u6cd5\u8bad\u7ec3\u4e00\u4e2a\u653f\u7b56\uff0c\u540c\u65f6\u8bc4\u4f30\u4e0e\u7ecf\u5178\u89c4\u5212\u8005\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u5b66\u4e60\u5f97\u5230\u7684\u653f\u7b56\u5728\u89e3\u51b3\u5b9e\u4f8b\u7684\u6570\u91cf\u4e0a\u8d85\u8fc7\u4f20\u7edf\u89c4\u5212\u65b9\u6cd5\uff0c\u51cf\u5c11\u4e86\u673a\u52a8\u6b21\u6570\uff0c\u5e76\u5728\u76f8\u540c\u7684\u4f20\u611f\u548c\u63a7\u5236\u9650\u5236\u4e0b\u4fdd\u6301\u7c7b\u4f3c\u7684\u8def\u5f84\u957f\u5ea6\u548c\u89c4\u5212\u65f6\u95f4\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3aAckermann\u8f66\u8f86\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u975e\u5b8c\u6574\u7ea6\u675f\u7a84\u6b7b\u80e1\u540c\u9003\u751f\u7b56\u7565\uff0c\u5c55\u793a\u4e86\u5b66\u4e60\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u4f18\u52bf\uff0c\u9879\u76ee\u5f00\u6e90\u53ef\u4f9b\u4f7f\u7528\u3002"}}
{"id": "2511.22789", "categories": ["cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.22789", "abs": "https://arxiv.org/abs/2511.22789", "authors": ["Alif Al Hasan", "Subarna Saha", "Mia Mohammad Imran"], "title": "Learning Programming in Informal Spaces: Using Emotion as a Lens to Understand Novice Struggles on r/learnprogramming", "comment": null, "summary": "Novice programmers experience emotional difficulties in informal online learning environments, where confusion and frustration can hinder motivation and learning outcomes. This study investigates novice programmers' emotional experiences in informal settings, identifies the causes of emotional struggle, and explores design opportunities for affect-aware support systems. We manually annotated 1,500 posts from r/learnprogramming using the Learning-Centered Emotions framework and conducted clustering and axial coding. Confusion, curiosity, and frustration were the most common emotions, often co-occurring and associated with early learning stages. Positive emotions were relatively rare. The primary emotional triggers included ambiguous errors, unclear learning pathways, and misaligned learning resources. We identify five key areas where novice programmers need support in informal learning spaces: stress relief and resilient motivation, topic explanation and resource recommendation, strategic decision-making and learning guidance, technical support, and acknowledgment of their challenges. Our findings highlight the need for intelligent, affect-sensitive mechanisms that provide timely support aligned with learners' emotional states.", "AI": {"tldr": "\u672c\u7814\u7a76\u5173\u6ce8\u65b0\u624b\u7a0b\u5e8f\u5458\u5728\u975e\u6b63\u5f0f\u5728\u7ebf\u5b66\u4e60\u4e2d\u7684\u60c5\u611f\u7ecf\u5386\uff0c\u8bc6\u522b\u4e86\u60c5\u611f\u56f0\u6270\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e94\u4e2a\u652f\u6301\u9886\u57df\uff0c\u4ee5\u4fc3\u8fdb\u66f4\u597d\u7684\u5b66\u4e60\u4f53\u9a8c\u3002", "motivation": "\u7814\u7a76\u65b0\u624b\u7a0b\u5e8f\u5458\u5728\u975e\u6b63\u5f0f\u5728\u7ebf\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u60c5\u611f\u7ecf\u5386\uff0c\u8bc6\u522b\u60c5\u611f\u56f0\u6270\u7684\u539f\u56e0\uff0c\u63a2\u7d22\u60c5\u611f\u611f\u77e5\u652f\u6301\u7cfb\u7edf\u7684\u8bbe\u8ba1\u673a\u4f1a\u3002", "method": "\u624b\u52a8\u6ce8\u91ca\u6765\u81ear/learnprogramming\u76841500\u4e2a\u5e16\u5b50\uff0c\u4f7f\u7528\u5b66\u4e60\u5bfc\u5411\u60c5\u611f\u6846\u67b6\uff0c\u8fdb\u884c\u805a\u7c7b\u548c\u8f74\u5fc3\u7f16\u7801\u5206\u6790\u3002", "result": "\u65b0\u624b\u7a0b\u5e8f\u5458\u5e38\u89c1\u7684\u60c5\u611f\u5305\u62ec\u56f0\u60d1\u3001\u597d\u5947\u548c\u6cae\u4e27\uff0c\u4e3b\u8981\u7684\u60c5\u611f\u89e6\u53d1\u56e0\u7d20\u6709\u6a21\u7cca\u9519\u8bef\u3001\u4e0d\u6e05\u6670\u7684\u5b66\u4e60\u8def\u5f84\u548c\u4e0d\u5339\u914d\u7684\u5b66\u4e60\u8d44\u6e90\u3002", "conclusion": "\u9700\u8981\u667a\u80fd\u5316\u7684\u60c5\u611f\u654f\u611f\u673a\u5236\uff0c\u53ca\u65f6\u652f\u6301\u4e0e\u5b66\u4e60\u8005\u60c5\u611f\u72b6\u6001\u76f8\u7b26\u7684\u5e2e\u52a9\u3002"}}
{"id": "2511.22354", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22354", "abs": "https://arxiv.org/abs/2511.22354", "authors": ["Suraj Borate", "Bhavish Rai B", "Vipul Pardeshi", "Madhu Vadali"], "title": "LLM-Based Generalizable Hierarchical Task Planning and Execution for Heterogeneous Robot Teams with Event-Driven Replanning", "comment": "submitted to ICRA 2026", "summary": "This paper introduces CoMuRoS (Collaborative Multi-Robot System), a generalizable hierarchical architecture for heterogeneous robot teams that unifies centralized deliberation with decentralized execution, and supports event-driven replanning. A Task Manager LLM interprets natural-language goals, classifies tasks, and allocates subtasks using static rules plus dynamic contexts (task, history, robot and task status, and events).Each robot runs a local LLM that composes executable Python code from primitive skills (ROS2 nodes, policies), while onboard perception (VLMs/image processing) continuously monitors events and classifies them into relevant or irrelevant to the task. Task failures or user intent changes trigger replanning, allowing robots to assist teammates, resume tasks, or request human help. Hardware studies demonstrate autonomous recovery from disruptive events, filtering of irrelevant distractions, and tightly coordinated transport with emergent human-robot cooperation (e.g., multirobot collaborative object recovery success rate: 9/10, coordinated transport: 8/8, human-assisted recovery: 5/5).Simulation studies show intention-aware replanning. A curated textual benchmark spanning 22 scenarios (3 tasks each, around 20 robots) evaluates task allocation, classification, IoU, executability, and correctness, with high average scores (e.g., correctness up to 0.91) across multiple LLMs, a separate replanning set (5 scenarios) achieves 1.0 correctness. Compared with prior LLM-based systems, CoMuRoS uniquely demonstrates runtime, event-driven replanning on physical robots, delivering robust, flexible multi-robot and human-robot collaboration.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a CoMuRoS \u7684\u534f\u4f5c\u591a\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u63d0\u4f9b\u4e86\u7ed3\u5408\u96c6\u4e2d\u51b3\u7b56\u4e0e\u5206\u6563\u6267\u884c\u7684\u901a\u7528\u5c42\u6b21\u67b6\u6784\uff0c\u80fd\u591f\u5b9e\u65f6\u5e94\u5bf9\u4efb\u52a1\u8c03 replanning \u548c\u4eba\u673a\u534f\u4f5c\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u65e8\u5728\u4e3a\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u63d0\u4f9b\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u67b6\u6784\uff0c\u652f\u6301\u81ea\u7136\u8bed\u8a00\u76ee\u6807\u89e3\u91ca\u4e0e\u5b50\u4efb\u52a1\u5206\u914d\uff0c\u540c\u65f6\u5e94\u5bf9\u4efb\u52a1\u5931\u8d25\u548c\u7528\u6237\u610f\u56fe\u53d8\u5316\u3002", "method": "\u4f7f\u7528\u5206\u5c42\u7ed3\u6784\u7ed3\u5408\u96c6\u4e2d\u5f0f\u51b3\u7b56\u4e0e\u5206\u6563\u5f0f\u6267\u884c\uff0c\u5229\u7528\u4efb\u52a1\u7ba1\u7406 LLM \u548c\u5c40\u90e8 LLM \u6765\u6267\u884c\u4efb\u52a1\u5206\u914d\u548c\u4e8b\u4ef6\u76d1\u63a7\u3002", "result": "\u786c\u4ef6\u7814\u7a76\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u4ece\u5e72\u6270\u4e8b\u4ef6\u4e2d\u81ea\u4e3b\u6062\u590d\uff0c\u8fc7\u6ee4\u65e0\u5173\u5e72\u6270\uff0c\u5e76\u5b9e\u73b0\u7d27\u5bc6\u534f\u8c03\u7684\u642c\u8fd0\u80fd\u529b\uff0c\u4e14\u5728\u591a\u673a\u5668\u4eba\u534f\u4f5c\u7269\u4f53\u56de\u6536\u3001\u4eba\u673a\u534f\u4f5c recovery \u7b49\u65b9\u9762\u53d6\u5f97\u663e\u8457\u6210\u529f\u3002", "conclusion": "CoMuRoS \u5728\u7269\u7406\u673a\u5668\u4eba\u4e0a\u5c55\u793a\u4e86\u72ec\u7279\u7684\u5b9e\u65f6\u4e8b\u4ef6\u9a71\u52a8\u91cd\u65b0\u89c4\u5212\uff0c\u63a8\u52a8\u4e86\u591a\u673a\u5668\u4eba\u548c\u4eba\u673a\u534f\u4f5c\u7684\u53ef\u9760\u6027\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2511.22809", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.22809", "abs": "https://arxiv.org/abs/2511.22809", "authors": ["Yiwei Xu", "Saloni Dash", "Sungha Kang", "Wang Liao", "Emma S. Spiro"], "title": "AI summaries in online search influence users' attitudes", "comment": null, "summary": "This study examined how AI-generated summaries, which have become visually prominent in online search results, affect how users think about different issues. In a preregistered randomized controlled experiment, participants (N = 2,004) viewed mock search result pages varying in the presence (vs. absence), placement (top vs. middle), and stance (benefit-framed vs. harm-framed) of AI-generated summaries across four publicly debated topics. Compared to a no-summary control group, participants exposed to AI-generated summaries reported issue attitudes, behavioral intentions, and policy support that aligned more closely with the AI summary stance. The summaries placed at the top of the page produced stronger shifts in users' issue attitudes (but not behavioral intentions or policy support) than those placed at the middle of the page. We also observed moderating effects from issue familiarity and general trust toward AI. In addition, users perceived the AI summaries more useful when it emphasized health harms versus benefits. These findings suggest that AI-generated search summaries can significantly shape public perceptions, raising important implications for the design and regulation of AI-integrated information ecosystems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8AI\u751f\u6210\u6458\u8981\u5982\u4f55\u5f71\u54cd\u7528\u6237\u5728\u4e0d\u540c\u4e89\u8bae\u95ee\u9898\u4e0a\u7684\u6001\u5ea6\uff0c\u53d1\u73b0\u5176\u5728\u516c\u5171\u8ba4\u77e5\u4e2d\u5177\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76AI\u751f\u6210\u6458\u8981\u5bf9\u7528\u6237\u601d\u7ef4\u65b9\u5f0f\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5176\u5728\u5728\u7ebf\u641c\u7d22\u7ed3\u679c\u4e2d\u7684\u7a81\u51fa\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u4e00\u4e2a\u9884\u5148\u6ce8\u518c\u7684\u968f\u673a\u5bf9\u7167\u5b9e\u9a8c\uff0c\u6d89\u53ca2004\u540d\u53c2\u4e0e\u8005\uff0c\u6bd4\u8f83\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684AI\u751f\u6210\u6458\u8981\u5bf9\u7528\u6237\u6001\u5ea6\u7684\u5f71\u54cd\u3002", "result": "\u76f8\u8f83\u4e8e\u6ca1\u6709\u6458\u8981\u7684\u5bf9\u7167\u7ec4\uff0c\u66b4\u9732\u4e8eAI\u751f\u6210\u6458\u8981\u7684\u53c2\u4e0e\u8005\u5728\u95ee\u9898\u6001\u5ea6\u3001\u884c\u4e3a\u610f\u56fe\u548c\u653f\u7b56\u652f\u6301\u4e0a\u66f4\u63a5\u8fd1\u6458\u8981\u7684\u7acb\u573a\uff0c\u5c24\u5176\u662f\u9876\u90e8\u6458\u8981\u5bf9\u7528\u6237\u6001\u5ea6\u7684\u53d8\u5316\u66f4\u660e\u663e\u3002", "conclusion": "AI\u751f\u6210\u7684\u6458\u8981\u663e\u8457\u5f71\u54cd\u516c\u4f17\u5bf9\u4e0d\u540c\u95ee\u9898\u7684\u770b\u6cd5\uff0c\u5177\u6709\u8bbe\u8ba1\u548c\u76d1\u7ba1\u7684\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2511.22364", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.22364", "abs": "https://arxiv.org/abs/2511.22364", "authors": ["Seongwon Cho", "Daechul Ahn", "Donghyun Shin", "Hyeonbeom Choi", "San Kim", "Jonghyun Choi"], "title": "BINDER: Instantly Adaptive Mobile Manipulation with Open-Vocabulary Commands", "comment": "12 pages, 8 figures", "summary": "Open-vocabulary mobile manipulation (OVMM) requires robots to follow language instructions, navigate, and manipulate while updating their world representation under dynamic environmental changes. However, most prior approaches update their world representation only at discrete update points such as navigation targets, waypoints, or the end of an action step, leaving robots blind between updates and causing cascading failures: overlooked objects, late error detection, and delayed replanning. To address this limitation, we propose BINDER (Bridging INstant and DEliberative Reasoning), a dual process framework that decouples strategic planning from continuous environment monitoring. Specifically, BINDER integrates a Deliberative Response Module (DRM, a multimodal LLM for task planning) with an Instant Response Module (IRM, a VideoLLM for continuous monitoring). The two modules play complementary roles: the DRM performs strategic planning with structured 3D scene updates and guides what the IRM attends to, while the IRM analyzes video streams to update memory, correct ongoing actions, and trigger replanning when necessary. Through this bidirectional coordination, the modules address the trade off between maintaining awareness and avoiding costly updates, enabling robust adaptation under dynamic conditions. Evaluated in three real world environments with dynamic object placement, BINDER achieves substantially higher success and efficiency than SoTA baselines, demonstrating its effectiveness for real world deployment.", "AI": {"tldr": "BINDER\u662f\u4e00\u4e2a\u53cc\u91cd\u8fc7\u7a0b\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u66f4\u65b0\u4e16\u754c\u8868\u793a\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u5f00\u653e\u8bcd\u6c47\u79fb\u52a8\u64cd\u63a7\u4e2d\u673a\u5668\u4eba\u5bf9\u8bed\u8a00\u6307\u4ee4\u7684\u6267\u884c\u80fd\u529b\uff0c\u4ee5\u53ca\u5728\u52a8\u6001\u73af\u5883\u53d8\u5316\u4e0b\u7684\u4e16\u754c\u8868\u793a\u66f4\u65b0\u95ee\u9898\u3002", "method": "\u63d0\u51faBINDER\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6218\u7565\u89c4\u5212\u548c\u8fde\u7eed\u73af\u5883\u76d1\u63a7\u89e3\u8026\uff0c\u5206\u522b\u4f7f\u7528Deliberative Response Module\u548cInstant Response Module\u8fdb\u884c\u4efb\u52a1\u89c4\u5212\u548c\u89c6\u9891\u6d41\u5206\u6790\u3002", "result": "\u5728\u4e09\u79cd\u52a8\u6001\u7269\u4f53\u653e\u7f6e\u7684\u771f\u5b9e\u73af\u5883\u4e2d\u8bc4\u4f30\uff0cBINDER\u5c55\u73b0\u51fa\u663e\u8457\u9ad8\u4e8e\u73b0\u6709\u57fa\u51c6\u7684\u6210\u529f\u7387\u548c\u6548\u7387\u3002", "conclusion": "BINDER\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u6761\u4ef6\u4e0b\u7684\u9002\u5e94\u6027\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.22942", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.22942", "abs": "https://arxiv.org/abs/2511.22942", "authors": ["Na Li", "Chuhao Wu", "Hongyang Zhou", "Huiran Yi", "Xuefei Wang", "Jie Cai", "Xinyi Fu", "John Carroll"], "title": "Body Management Information Practices on a Female-dominant Platform", "comment": "Preprint accepted at ICHEC 2025", "summary": "With growing awareness of long-term health and wellness, everyday body management has become a widespread practice. Social media platforms and health-related applications offer abundant information for those pursuing healthier lifestyles and more positive body images. While prior Human-Computer Interaction research has focused extensively on technology-mediated health interventions, the user-initiated practices of browsing and evaluating body management information remain underexplored. In this paper, we study a female-dominant social media platform in China to examine how users seek such information and how it shapes their lifestyle choices. Through semi-structured interviews with 18 users, we identify factors including consumerism, poster popularity, and perceived authenticity that influence decision-making, alongside challenges such as discerning reliable methods and managing body anxiety triggered by social media. We contribute insights into how content and media formats interact to shape users' information evaluation, and we outline design implications for supporting more reliable and healthy engagements with body management information.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u4e0a\u7528\u6237\u5982\u4f55\u5bfb\u627e\u8eab\u4f53\u7ba1\u7406\u4fe1\u606f\u53ca\u5176\u5bf9\u751f\u6d3b\u65b9\u5f0f\u9009\u62e9\u7684\u5f71\u54cd\uff0c\u8bc6\u522b\u4e86\u5f71\u54cd\u51b3\u7b56\u7684\u591a\u79cd\u56e0\u7d20\uff0c\u5e76\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u968f\u7740\u4eba\u4eec\u5bf9\u957f\u671f\u5065\u5eb7\u548c\u798f\u7949\u7684\u5173\u6ce8\uff0c\u65e5\u5e38\u8eab\u4f53\u7ba1\u7406\u6210\u4e3a\u4e00\u79cd\u5e7f\u6cdb\u7684\u5b9e\u8df5\uff0c\u4f46\u7528\u6237\u4e3b\u52a8\u6d4f\u89c8\u548c\u8bc4\u4f30\u8eab\u4f53\u7ba1\u7406\u4fe1\u606f\u7684\u884c\u4e3a\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u8ba8\u3002", "method": "\u901a\u8fc7\u5bf918\u540d\u7528\u6237\u8fdb\u884c\u534a\u7ed3\u6784\u5316\u7684\u8bbf\u8c08\u6765\u7814\u7a76\u7528\u6237\u4fe1\u606f\u641c\u7d22\u7684\u884c\u4e3a\u3002", "result": "\u8bc6\u522b\u4e86\u5f71\u54cd\u51b3\u7b56\u7684\u56e0\u7d20\uff0c\u5982\u6d88\u8d39\u4e3b\u4e49\u3001\u53d1\u5e16\u4eba\u53d7\u6b22\u8fce\u5ea6\u548c\u88ab\u611f\u77e5\u7684\u771f\u5b9e\u6027\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u8bc6\u522b\u53ef\u9760\u65b9\u6cd5\u548c\u7ba1\u7406\u793e\u4ea4\u5a92\u4f53\u5f15\u53d1\u7684\u8eab\u4f53\u7126\u8651\u7684\u6311\u6218\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7528\u6237\u5982\u4f55\u5bfb\u627e\u548c\u8bc4\u4f30\u4e0e\u8eab\u4f53\u7ba1\u7406\u76f8\u5173\u7684\u4fe1\u606f\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u7528\u4e8e\u8bbe\u8ba1\u66f4\u53ef\u9760\u548c\u5065\u5eb7\u7684\u53c2\u4e0e\u65b9\u5f0f\u7684\u8bbe\u8ba1\u5efa\u8bae\u3002"}}
{"id": "2511.22445", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22445", "abs": "https://arxiv.org/abs/2511.22445", "authors": ["Yikai Tang", "Haoran Geng", "Sheng Zang", "Pieter Abbeel", "Jitendra Malik"], "title": "Visual-Geometry Diffusion Policy: Robust Generalization via Complementarity-Aware Multimodal Fusion", "comment": null, "summary": "Imitation learning has emerged as a crucial ap proach for acquiring visuomotor skills from demonstrations, where designing effective observation encoders is essential for policy generalization. However, existing methods often struggle to generalize under spatial and visual randomizations, instead tending to overfit. To address this challenge, we propose Visual Geometry Diffusion Policy (VGDP), a multimodal imitation learning framework built around a Complementarity-Aware Fusion Module where modality-wise dropout enforces balanced use of RGB and point-cloud cues, with cross-attention serving only as a lightweight interaction layer. Our experiments show that the expressiveness of the fused latent space is largely induced by the enforced complementarity from modality-wise dropout, with cross-attention serving primarily as a lightweight interaction mechanism rather than the main source of robustness. Across a benchmark of 18 simulated tasks and 4 real-world tasks, VGDP outperforms seven baseline policies with an average performance improvement of 39.1%. More importantly, VGDP demonstrates strong robustness under visual and spatial per turbations, surpassing baselines with an average improvement of 41.5% in different visual conditions and 15.2% in different spatial settings.", "AI": {"tldr": "VGDP\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e92\u8865\u6027\u611f\u77e5\u878d\u5408\u6a21\u5757\u548c\u6a21\u6001\u4e22\u5f03\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86\u89c6\u89c9\u548c\u7a7a\u95f4\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u9274\u4e8e\u73b0\u6709\u65b9\u6cd5\u5728\u7a7a\u95f4\u548c\u89c6\u89c9\u968f\u673a\u5316\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u5b58\u5728\u8fc7\u62df\u5408\u73b0\u8c61\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u65b0\u7684\u65b9\u6cd5\u63d0\u9ad8\u6a21\u4eff\u5b66\u4e60\u7684\u6548\u679c\u5e76\u6539\u5584\u653f\u7b56\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVisual Geometry Diffusion Policy (VGDP) \u7684\u591a\u6a21\u6001\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u4e92\u8865\u6027\u611f\u77e5\u878d\u5408\u6a21\u5757\u4ee5\u53ca\u6a21\u6001\u4e22\u5f03\u7b56\u7565\u4ee5\u589e\u5f3aRGB\u548c\u70b9\u4e91\u6570\u636e\u7684\u5e73\u8861\u5229\u7528\uff0c\u5e76\u5229\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u4ea4\u4e92\u5c42\u3002", "result": "VGDP\u572818\u4e2a\u6a21\u62df\u4efb\u52a1\u548c4\u4e2a\u771f\u5b9e\u4efb\u52a1\u4e2d\u7684\u5e73\u5747\u6027\u80fd\u63d0\u5347\u4e3a39.1%\uff1b\u5728\u4e0d\u540c\u89c6\u89c9\u6761\u4ef6\u4e0b\u5e73\u5747\u63d0\u534741.5%\uff0c\u5728\u4e0d\u540c\u7a7a\u95f4\u8bbe\u7f6e\u4e0b\u5e73\u5747\u63d0\u534715.2%\u3002", "conclusion": "VGDP\u572818\u4e2a\u6a21\u62df\u4efb\u52a1\u548c4\u4e2a\u771f\u5b9e\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u5747\u4f18\u4e8e\u4e03\u4e2a\u57fa\u7ebf\u7b56\u7565\uff0c\u5177\u6709\u5f3a\u5927\u7684\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u5728\u4e0d\u540c\u89c6\u89c9\u548c\u7a7a\u95f4\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u663e\u8457\u7684\u6539\u5584\u3002"}}
{"id": "2511.23173", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.23173", "abs": "https://arxiv.org/abs/2511.23173", "authors": ["Hoang Khang Phan", "Khang Le", "Tu Nhat Khang Nguyen", "Anh Van Dao", "Nhat Tan Le"], "title": "Robust In-the-Wild Exercise Recognition from a Single Wearable: Data-Side Fusion, Sensor Rotation, and Feature Engineering", "comment": null, "summary": "Monitoring physical exercises is vital for health promotion, with automated systems becoming standard in personal health surveillance. However, sensor placement variability and unconstrained movements limit their effectiveness. This study proposes the team \"3KA\"'s one-sensor workout activity recognition method using feature extraction and data augmentation in 2ndWEAR Dataset Challenge. From raw acceleration, angle and signal magnitude vector features were derived, followed by extraction of statistical, fractal/spectral, and higher-order differential features. A fused dataset combining left/right limb data was created, and augmented via sensor rotation and axis inversion. We utilized a soft voting model combining Hist Gradient Boosting with balanced weights and Extreme Gradient Boosting without. Under group 5-fold evaluation, the model achieved 58.83\\% macro F1 overall (61.72% arm, 55.95% leg). ANOVA F-score showed fractal/spectral features were most important for arm-based recognition but least for leg-based. The code to reproduce the experiments is publicly available via: https://github.com/Khanghcmut/WEAR\\_3K", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e863KA\u56e2\u961f\u7684\u5355\u4f20\u611f\u5668\u953b\u70bc\u6d3b\u52a8\u8bc6\u522b\u65b9\u6cd5\uff0c\u7ed3\u5408\u7279\u5f81\u63d0\u53d6\u548c\u6570\u636e\u589e\u5f3a\uff0c\u57282ndWEAR\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u8fbe\u523058.83%\u7684\u5b8fF1\uff0c\u603b\u7ed3\u4e86\u4e0d\u540c\u7279\u5f81\u5bf9\u8bc6\u522b\u7684\u5f71\u54cd\u3002", "motivation": "\u76d1\u6d4b\u8eab\u4f53\u953b\u70bc\u5bf9\u5065\u5eb7\u4fc3\u8fdb\u81f3\u5173\u91cd\u8981\uff0c\u800c\u81ea\u52a8\u5316\u7cfb\u7edf\u5728\u4e2a\u4eba\u5065\u5eb7\u76d1\u6d4b\u4e2d\u9010\u6e10\u6210\u4e3a\u6807\u51c6\u3002\u7136\u800c\uff0c\u4f20\u611f\u5668\u653e\u7f6e\u7684\u53d8\u5f02\u6027\u548c\u4e0d\u53d7\u9650\u5236\u7684\u52a8\u4f5c\u9650\u5236\u4e86\u5176\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\"3KA\"\u7684\u56e2\u961f\u5355\u4f20\u611f\u5668\u953b\u70bc\u6d3b\u52a8\u8bc6\u522b\u65b9\u6cd5\uff0c\u5229\u7528\u7279\u5f81\u63d0\u53d6\u548c\u6570\u636e\u589e\u5f3a\uff0c\u57282ndWEAR\u6570\u636e\u96c6\u6311\u6218\u4e2d\u8fdb\u884c\u3002\u901a\u8fc7\u539f\u59cb\u52a0\u901f\u5ea6\u3001\u89d2\u5ea6\u548c\u4fe1\u53f7\u5e45\u5ea6\u5411\u91cf\u7279\u5f81\u7684\u63d0\u53d6\uff0c\u8fdb\u4e00\u6b65\u63d0\u53d6\u7edf\u8ba1\u3001\u5206\u5f62/\u9891\u8c31\u548c\u9ad8\u9636\u5fae\u5206\u7279\u5f81\uff0c\u5e76\u521b\u5efa\u4e86\u4e00\u4e2a\u7ed3\u5408\u5de6\u53f3\u80a2\u4f53\u6570\u636e\u7684\u878d\u5408\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u4f20\u611f\u5668\u65cb\u8f6c\u548c\u8f74\u7ffb\u8f6c\u8fdb\u884c\u4e86\u589e\u5f3a\u3002\u91c7\u7528\u4e86\u8f6f\u6295\u7968\u6a21\u578b\uff0c\u5c06Hist Gradient Boosting\u4e0e\u5e73\u8861\u6743\u91cd\u7684\u6781\u9650\u68af\u5ea6\u63d0\u5347\u76f8\u7ed3\u5408\u3002", "result": "\u57285-fold\u8bc4\u4f30\u4e0b\uff0c\u8be5\u6a21\u578b\u603b\u4f53\u8fbe\u5230\u4e8658.83%\u5b8fF1\uff0c\u624b\u81c2\u4e3a61.72%\uff0c\u817f\u90e8\u4e3a55.95%\u3002ANOVA F-score\u663e\u793a\uff0c\u5206\u5f62/\u9891\u8c31\u7279\u5f81\u5bf9\u57fa\u4e8e\u624b\u81c2\u7684\u8bc6\u522b\u6700\u91cd\u8981\uff0c\u4f46\u5bf9\u57fa\u4e8e\u817f\u90e8\u7684\u8bc6\u522b\u6700\u4e0d\u91cd\u8981\u3002", "conclusion": "\u5b9e\u9a8c\u8fc7\u7a0b\u4e2d\u7684\u4ee3\u7801\u5df2\u516c\u5f00\uff0c\u4fbf\u4e8e\u5176\u4ed6\u7814\u7a76\u8005\u590d\u5236\u5b9e\u9a8c\u3002"}}
{"id": "2511.22505", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.22505", "abs": "https://arxiv.org/abs/2511.22505", "authors": ["Xiujian Liang", "Jiacheng Liu", "Mingyang Sun", "Qichen He", "Cewu Lu", "Jianhua Sun"], "title": "RealD$^2$iff: Bridging Real-World Gap in Robot Manipulation via Depth Diffusion", "comment": null, "summary": "Robot manipulation in the real world is fundamentally constrained by the visual sim2real gap, where depth observations collected in simulation fail to reflect the complex noise patterns inherent to real sensors. In this work, inspired by the denoising capability of diffusion models, we invert the conventional perspective and propose a clean-to-noisy paradigm that learns to synthesize noisy depth, thereby bridging the visual sim2real gap through purely simulation-driven robotic learning. Building on this idea, we introduce RealD$^2$iff, a hierarchical coarse-to-fine diffusion framework that decomposes depth noise into global structural distortions and fine-grained local perturbations. To enable progressive learning of these components, we further develop two complementary strategies: Frequency-Guided Supervision (FGS) for global structure modeling and Discrepancy-Guided Optimization (DGO) for localized refinement. To integrate RealD$^2$iff seamlessly into imitation learning, we construct a pipeline that spans six stages. We provide comprehensive empirical and experimental validation demonstrating the effectiveness of this paradigm. RealD$^2$iff enables two key applications: (1) generating real-world-like depth to construct clean-noisy paired datasets without manual sensor data collection. (2) Achieving zero-shot sim2real robot manipulation, substantially improving real-world performance without additional fine-tuning.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faRealD\u00b2iff\uff0c\u4e00\u4e2a\u901a\u8fc7\u751f\u6210\u566a\u58f0\u6df1\u5ea6\u6765\u7f29\u5c0f\u89c6\u89c9sim2real\u5dee\u8ddd\u7684\u6846\u67b6\uff0c\u5177\u6709\u9ad8\u6548\u6027\uff0c\u5e76\u80fd\u7528\u4e8e\u751f\u6210\u771f\u5b9e\u6570\u636e\u4e0e\u5b9e\u73b0\u65e0\u7f1d\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u3002", "motivation": "\u89e3\u51b3\u4eff\u771f\u4e0e\u73b0\u5b9e\u4e2d\u7684\u89c6\u89c9\u5dee\u8ddd\uff0c\u6cbf\u7528\u6269\u6563\u6a21\u578b\u7684\u53bb\u566a\u80fd\u529b\uff0c\u63d0\u51fa\u4e00\u79cd\u4ece\u5e72\u51c0\u5230\u5608\u6742\u7684\u5b66\u4e60\u6a21\u5f0f\u3002", "method": "\u6784\u5efaRealD\u00b2iff\u6846\u67b6\uff0c\u7ed3\u5408\u5168\u5c40\u7ed3\u6784\u548c\u5c40\u90e8\u6270\u52a8\u7684\u6df1\u5ea6\u566a\u58f0\u5efa\u6a21\uff0c\u5e76\u901a\u8fc7\u9891\u7387\u5f15\u5bfc\u548c\u8bef\u5dee\u5f15\u5bfc\u7684\u4f18\u5316\u7b56\u7565\u5b9e\u73b0\u6a21\u578b\u7684\u9010\u6b65\u5b66\u4e60\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b56\u7565\uff0c\u901a\u8fc7\u751f\u6210\u566a\u58f0\u6df1\u5ea6\u6570\u636e\u6765\u5f25\u8865\u89c6\u89c9sim2real\u5dee\u8ddd\u3002", "conclusion": "RealD\u00b2iff\u65b9\u6cd5\u5229\u7528\u5206\u5c42\u7684\u6269\u6563\u6846\u67b6\u6709\u6548\u5730\u5728\u6a21\u62df\u73af\u5883\u4e2d\u751f\u6210\u4e0e\u771f\u5b9e\u4e16\u754c\u76f8\u4f3c\u7684\u566a\u58f0\u6df1\u5ea6\u6570\u636e\uff0c\u8fdb\u800c\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2511.23188", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.23188", "abs": "https://arxiv.org/abs/2511.23188", "authors": ["Yibo Meng", "Lyumanshan Ye", "Eve He", "Zhe Yan", "Zhiming Liu", "Yipeng Yu", "Yan Guan", "Xiaolan Ding"], "title": "Can Intelligent User Interfaces Engage in Philosophical Discussions? A Longitudinal Study of Philosophers' Evolving Perceptions", "comment": null, "summary": "This study investigates the evolving attitudes of philosophy scholars towards the participation of generative AI based Intelligent User Interfaces (IUIs) in philosophical discourse. We conducted a three year (2023--2025) mixed methods longitudinal study with 16 philosophy scholars and students. Qualitative data from annual interviews reveal a three stage evolution in attitude: from initial resistance and unfamiliarity, to instrumental acceptance of the IUI as a tool, and finally to a deep principled questioning of the IUI's fundamental capacity for genuine philosophical thought. Quantitative data from blind assessments, where participants rated anonymized philosophical answers from both humans and an IUI, complement these findings. While participants acknowledged the IUI's proficiency in tasks requiring formal logic and knowledge reproduction, they consistently identified significant shortcomings in areas demanding dialectical reasoning, originality and embodied understanding. The study concludes that participants do not see the IUI as a peer but rather as a sophisticated mirror whose capabilities and limitations provoke a deeper reflection on the unique and irreplaceable human dimensions of philosophical inquiry, such as intuition, value laden commitment and the courage to question fundamental premises.", "AI": {"tldr": "\u672c\u7814\u7a76\u8c03\u67e5\u4e86\u54f2\u5b66\u5b66\u8005\u5bf9\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u667a\u80fd\u7528\u6237\u754c\u9762\u5728\u54f2\u5b66\u8bdd\u8bed\u4e2d\u53c2\u4e0e\u7684\u6001\u5ea6\u53d8\u5316\uff0c\u7ed3\u679c\u663e\u793a\u6001\u5ea6\u7ecf\u5386\u4e86\u4e09\u9636\u6bb5\u6f14\u53d8\uff0c\u53c2\u4e0e\u8005\u6700\u7ec8\u8ba4\u4e3aIUI\u4e0d\u662f\u5e73\u7b49\u4f19\u4f34\uff0c\u800c\u662f\u53cd\u6620\u4eba\u7c7b\u54f2\u5b66\u63a2\u7a76\u72ec\u7279\u6027\u7684\u5de5\u5177\u3002", "motivation": "\u7814\u7a76\u54f2\u5b66\u5b66\u8005\u5bf9\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u667a\u80fd\u7528\u6237\u754c\u9762\u5728\u54f2\u5b66\u8ba8\u8bba\u4e2d\u53c2\u4e0e\u7684\u6001\u5ea6\u53d8\u5316\u3002", "method": "\u9488\u5bf916\u540d\u54f2\u5b66\u5b66\u8005\u548c\u5b66\u751f\u8fdb\u884c\u4e86\u4e3a\u671f\u4e09\u5e74\u7684\u6df7\u5408\u65b9\u6cd5\u7eb5\u5411\u7814\u7a76\uff0c\u5305\u62ec\u5e74\u5ea6\u8bbf\u8c08\u548c\u76f2\u8bc4\u91cf\u5316\u6570\u636e\u6536\u96c6\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u53c2\u4e0e\u8005\u7684\u6001\u5ea6\u7ecf\u5386\u4e86\u4ece\u6700\u521d\u7684\u62b5\u5236\u548c\u964c\u751f\u611f\uff0c\u5230\u5c06IUI\u89c6\u4e3a\u5de5\u5177\u7684\u5de5\u5177\u6027\u63a5\u53d7\uff0c\u6700\u7ec8\u6df1\u5165\u8d28\u7591IUI\u5bf9\u771f\u6b63\u54f2\u5b66\u601d\u8003\u7684\u6839\u672c\u80fd\u529b\u7684\u4e09\u9636\u6bb5\u6f14\u53d8\u3002\u867d\u7136\u53c2\u4e0e\u8005\u8ba4\u53efIUI\u5728\u5f62\u5f0f\u903b\u8f91\u548c\u77e5\u8bc6\u518d\u73b0\u4e0a\u7684\u80fd\u529b\uff0c\u4f46\u5728\u9700\u8981\u8fa9\u8bc1\u63a8\u7406\u3001\u539f\u521b\u6027\u548c\u5177\u8eab\u7406\u89e3\u7684\u9886\u57df\uff0c\u4ed6\u4eec\u8bc6\u522b\u51fa\u663e\u8457\u4e0d\u8db3\u3002", "conclusion": "\u53c2\u4e0e\u8005\u5e76\u4e0d\u5c06\u667a\u80fd\u7528\u6237\u754c\u9762\u89c6\u4e3a\u5e73\u7b49\u7684\u4f19\u4f34\uff0c\u800c\u662f\u4e00\u9762\u590d\u6742\u7684\u955c\u5b50\uff0c\u5176\u80fd\u529b\u548c\u5c40\u9650\u6027\u5f15\u53d1\u5bf9\u54f2\u5b66\u63a2\u7a76\u4e2d\u72ec\u7279\u4e14\u4e0d\u53ef\u66ff\u4ee3\u7684\u4eba\u7c7b\u7ef4\u5ea6\u7684\u66f4\u6df1\u601d\u8003\u3002"}}
{"id": "2511.22541", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.22541", "abs": "https://arxiv.org/abs/2511.22541", "authors": ["Jinyang Li", "Marcello Farina", "Luca Mozzarelli", "Luca Cattaneo", "Panita Rattamasanaprapai", "Eleonora A. Tagarelli", "Matteo Corno", "Paolo Perego", "Giuseppe Andreoni", "Emanuele Lettieri"], "title": "BUDD-e: an autonomous robotic guide for visually impaired users", "comment": "14 pages", "summary": "This paper describes the design and the realization of a prototype of the novel guide robot BUDD-e for visually impaired users. The robot has been tested in a real scenario with the help of visually disabled volunteers at ASST Grande Ospedale Metropolitano Niguarda, in Milan. The results of the experimental campaign are throughly described in the paper, displaying its remarkable performance and user-acceptance.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e3a\u89c6\u969c\u7528\u6237\u8bbe\u8ba1\u7684\u5f15\u5bfc\u673a\u5668\u4ebaBUDD-e\u539f\u578b\u7684\u5b9e\u73b0\u4e0e\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86\u5176\u4f18\u5f02\u6027\u80fd\u4e0e\u7528\u6237\u63a5\u53d7\u5ea6\u3002", "motivation": "\u4e3a\u63d0\u9ad8\u89c6\u969c\u7528\u6237\u7684\u72ec\u7acb\u6027\u4e0e\u751f\u6d3b\u8d28\u91cf\uff0c\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u6709\u6548\u5f15\u5bfc\u4ed6\u4eec\u7684\u673a\u5668\u4eba\u3002", "method": "\u8bbe\u8ba1\u548c\u5b9e\u73b0\u65b0\u578b\u76f2\u4eba\u5f15\u5bfc\u673a\u5668\u4ebaBUDD-e\u7684\u539f\u578b\uff0c\u5e76\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8fdb\u884c\u6d4b\u8bd5", "result": "\u7ecf\u8fc7\u89c6\u89c9\u969c\u788d\u5fd7\u613f\u8005\u7684\u53c2\u4e0e\uff0c\u8fd9\u6b3e\u673a\u5668\u4eba\u5728\u7c73\u5170\u7684ASST Grande Ospedale Metropolitano Niguarda\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u5c55\u793a\u4e86\u5176\u663e\u8457\u7684\u6027\u80fd\u548c\u7528\u6237\u63a5\u53d7\u5ea6\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cBUDD-e\u673a\u5668\u4eba\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5f97\u5230\u4e86\u7528\u6237\u7684\u9ad8\u5ea6\u8ba4\u53ef\u3002"}}
{"id": "2511.23376", "categories": ["cs.HC", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.23376", "abs": "https://arxiv.org/abs/2511.23376", "authors": ["Li Siyan", "Jason Zhang", "Akash Maharaj", "Yuanming Shi", "Yunyao Li"], "title": "Is Passive Expertise-Based Personalization Enough? A Case Study in AI-Assisted Test-Taking", "comment": "Accepted into Tailoring AI: Exploring Active and Passive LLM Personalization (PALS) workshop at EMNLP 2025", "summary": "Novice and expert users have different systematic preferences in task-oriented dialogues. However, whether catering to these preferences actually improves user experience and task performance remains understudied. To investigate the effects of expertise-based personalization, we first built a version of an enterprise AI assistant with passive personalization. We then conducted a user study where participants completed timed exams, aided by the two versions of the AI assistant. Preliminary results indicate that passive personalization helps reduce task load and improve assistant perception, but reveal task-specific limitations that can be addressed through providing more user agency. These findings underscore the importance of combining active and passive personalization to optimize user experience and effectiveness in enterprise task-oriented environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u9488\u5bf9\u65b0\u624b\u548c\u4e13\u5bb6\u4e0d\u540c\u504f\u597d\u7684\u4e2a\u6027\u5316\u5bf9\u7528\u6237\u4f53\u9a8c\u548c\u4efb\u52a1\u8868\u73b0\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u88ab\u52a8\u4e2a\u6027\u5316\u6709\u6548\uff0c\u4f46\u9700\u7ed3\u5408\u4e3b\u52a8\u4e2a\u6027\u5316\u4ee5\u4f18\u5316\u7ed3\u679c\u3002", "motivation": "\u7814\u7a76\u65b0\u624b\u548c\u4e13\u5bb6\u7528\u6237\u5728\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u4e2d\u5b58\u5728\u7684\u7cfb\u7edf\u6027\u504f\u597d\u5dee\u5f02\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u504f\u597d\u5bf9\u7528\u6237\u4f53\u9a8c\u548c\u4efb\u52a1\u8868\u73b0\u7684\u5f71\u54cd\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5177\u6709\u88ab\u52a8\u4e2a\u6027\u5316\u7684\u4f01\u4e1aAI\u52a9\u624b\uff0c\u5e76\u8fdb\u884c\u7528\u6237\u7814\u7a76\uff0c\u53c2\u4e0e\u8005\u5728\u4e24\u4e2a\u7248\u672c\u7684AI\u52a9\u624b\u5e2e\u52a9\u4e0b\u5b8c\u6210\u5b9a\u65f6\u8003\u8bd5\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u8868\u660e\uff0c\u88ab\u52a8\u4e2a\u6027\u5316\u6709\u52a9\u4e8e\u51cf\u5c11\u4efb\u52a1\u8d1f\u62c5\u5e76\u6539\u5584\u52a9\u624b\u611f\u77e5\uff0c\u4f46\u5728\u4efb\u52a1\u7279\u5b9a\u65b9\u9762\u5b58\u5728\u9650\u5236\uff0c\u8fd9\u53ef\u4ee5\u901a\u8fc7\u63d0\u4f9b\u66f4\u591a\u7528\u6237\u4e3b\u52a8\u6027\u6765\u89e3\u51b3\u3002", "conclusion": "\u7ed3\u5408\u4e3b\u52a8\u548c\u88ab\u52a8\u4e2a\u6027\u5316\u662f\u4f18\u5316\u7528\u6237\u4f53\u9a8c\u548c\u6709\u6548\u6027\u7684\u5173\u952e\u3002"}}
{"id": "2511.22555", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22555", "abs": "https://arxiv.org/abs/2511.22555", "authors": ["Yanbo Mao", "Jianlong Fu", "Ruoxuan Zhang", "Hongxia Xie", "Meibao Yao"], "title": "Beyond Success: Refining Elegant Robot Manipulation from Mixed-Quality Data via Just-in-Time Intervention", "comment": null, "summary": "Vision-Language-Action (VLA) models have enabled notable progress in general-purpose robotic manipulation, yet their learned policies often exhibit variable execution quality. We attribute this variability to the mixed-quality nature of human demonstrations, where the implicit principles that govern how actions should be carried out are only partially satisfied. To address this challenge, we introduce the LIBERO-Elegant benchmark with explicit criteria for evaluating execution quality. Using these criteria, we develop a decoupled refinement framework that improves execution quality without modifying or retraining the base VLA policy. We formalize Elegant Execution as the satisfaction of Implicit Task Constraints (ITCs) and train an Elegance Critic via offline Calibrated Q-Learning to estimate the expected quality of candidate actions. At inference time, a Just-in-Time Intervention (JITI) mechanism monitors critic confidence and intervenes only at decision-critical moments, providing selective, on-demand refinement. Experiments on LIBERO-Elegant and real-world manipulation tasks show that the learned Elegance Critic substantially improves execution quality, even on unseen tasks. The proposed model enables robotic control that values not only whether tasks succeed, but also how they are performed.", "AI": {"tldr": "LIBERO-Elegant\u57fa\u51c6\u901a\u8fc7\u7cbe\u70bc\u6846\u67b6\u548cElegance Critic\u663e\u8457\u63d0\u5347VLA\u6a21\u578b\u7684\u6267\u884c\u8d28\u91cf\uff0c\u5373\u4f7f\u5728\u65b0\u4efb\u52a1\u4e0a\u3002", "motivation": "VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u6267\u884c\u8d28\u91cf\u4e0d\u5747\uff0c\u5f52\u56e0\u4e8e\u4eba\u7c7b\u793a\u8303\u7684\u8d28\u91cf\u4e0d\u4e00\u3002", "method": "\u63d0\u51faLIBERO-Elegant\u57fa\u51c6\uff0c\u4f7f\u7528\u660e\u786e\u7684\u6807\u51c6\u8bc4\u4f30\u6267\u884c\u8d28\u91cf\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u89e3\u8026\u7684\u7cbe\u70bc\u6846\u67b6\uff0c\u6539\u5584\u6267\u884c\u8d28\u91cf\uff0c\u4f7f\u7528\u79bb\u7ebf\u6821\u51c6Q\u5b66\u4e60\u8bad\u7ec3Elegance Critic\u6765\u4f30\u8ba1\u5019\u9009\u52a8\u4f5c\u7684\u9884\u671f\u8d28\u91cf\u3002", "result": "\u5728LIBERO-Elegant\u53ca\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5b66\u4e60\u7684Elegance Critic\u663e\u8457\u63d0\u9ad8\u4e86\u6267\u884c\u8d28\u91cf\uff0c\u5373\u4f7f\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u4e5f\u6709\u6539\u5584\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e0d\u4ec5\u5173\u6ce8\u4efb\u52a1\u7684\u6210\u529f\u4e0e\u5426\uff0c\u540c\u65f6\u5173\u6ce8\u4efb\u52a1\u7684\u6267\u884c\u65b9\u5f0f\uff0c\u4ece\u800c\u63d0\u5347\u673a\u5668\u4eba\u63a7\u5236\u7684\u8d28\u91cf\u3002"}}
{"id": "2511.23379", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.23379", "abs": "https://arxiv.org/abs/2511.23379", "authors": ["Yimeng Liu", "Misha Sra"], "title": "AugGen: Augmenting Task-Based Learning in Professional Creative Software with LLM-Generated Scaffolded UIs", "comment": "arXiv admin note: substantial text overlap with arXiv:2505.12101", "summary": "Professional creative software often presents steep learning curves due to complex interfaces, lack of structured task-aware guidance, and unfamiliar domain terminology. To address these challenges and augment user learning experience, we introduce AugGen, a method for generating scaffolded user interfaces that simplify interface complexity and support task-based learning. With the user's task, our method surfaces task-relevant tools to reduce distracting features, organizes the tools around task workflow stages to offer execution guidance, connects tools with domain concepts to foster learning engagement, and progressively discloses advanced features to manage learning progress. To evaluate the method, we used our LLM-assisted pipeline to generate two task-specific scaffolded UIs and deployed them in Blender, our professional 3D modeling testbed. We invited both beginner (N=32) and expert (N=8) users to evaluate our implemented interfaces. Results show that the scaffolded interfaces significantly reduced user-perceived task load, enhanced task performance via embedded guidance, and augmented concept learning during task execution.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86AugGen\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u652f\u67b6\u5316\u7528\u6237\u754c\u9762\u6765\u7b80\u5316\u590d\u6742\u754c\u9762\uff0c\u652f\u6301\u7528\u6237\u7684\u4efb\u52a1\u5b66\u4e60\uff0c\u5e76\u5728Blender\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u663e\u8457\u6539\u5584\u4e86\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u4e13\u4e1a\u521b\u610f\u8f6f\u4ef6\u901a\u5e38\u56e0\u590d\u6742\u754c\u9762\u548c\u7f3a\u4e4f\u7ed3\u6784\u5316\u6307\u5bfc\u800c\u5bfc\u81f4\u5b66\u4e60\u66f2\u7ebf\u9661\u5ced\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u589e\u5f3a\u7528\u6237\u5b66\u4e60\u4f53\u9a8c\u3002", "method": "\u4ecb\u7ecd\u4e86AugGen\u65b9\u6cd5\uff0c\u5b83\u751f\u6210\u652f\u67b6\u5316\u7684\u7528\u6237\u754c\u9762\uff0c\u7b80\u5316\u754c\u9762\u590d\u6742\u6027\uff0c\u652f\u6301\u4efb\u52a1\u4e3a\u5bfc\u5411\u7684\u5b66\u4e60\uff0c\u5e76\u5728Blender\u4e2d\u8fdb\u884c\u4e86\u4e24\u4e2a\u7279\u5b9a\u4efb\u52a1\u7684\u8bc4\u4f30\u3002", "result": "AugGen\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u652f\u67b6\u5316\u7528\u6237\u754c\u9762\u6765\u7b80\u5316\u590d\u6742\u63a5\u53e3\uff0c\u652f\u6301\u57fa\u4e8e\u4efb\u52a1\u7684\u5b66\u4e60\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u652f\u67b6\u5316\u754c\u9762\u663e\u8457\u964d\u4f4e\u7528\u6237\u611f\u77e5\u7684\u4efb\u52a1\u8d1f\u62c5\uff0c\u63d0\u9ad8\u4efb\u52a1\u6267\u884c\u7684\u6027\u80fd\uff0c\u5e76\u589e\u5f3a\u4e86\u6982\u5ff5\u5b66\u4e60\u3002"}}
{"id": "2511.22685", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22685", "abs": "https://arxiv.org/abs/2511.22685", "authors": ["Haoyi Wang", "Licheng Luo", "Yiannis Kantaros", "Bruno Sinopoli", "Mingyu Cai"], "title": "Deadlock-Free Hybrid RL-MAPF Framework for Zero-Shot Multi-Robot Navigation", "comment": null, "summary": "Multi-robot navigation in cluttered environments presents fundamental challenges in balancing reactive collision avoidance with long-range goal achievement. When navigating through narrow passages\n  or confined spaces, deadlocks frequently emerge that prevent agents from reaching their destinations, particularly when Reinforcement Learning (RL) control policies encounter novel configurations out of learning distribution. Existing RL-based approaches suffer from limited generalization capability in unseen environments. We propose a hybrid framework that seamlessly integrates RL-based reactive navigation with on-demand Multi-Agent Path Finding (MAPF) to explicitly resolve topological deadlocks. Our approach integrates a safety layer that monitors agent progress to detect deadlocks and, when detected, triggers a coordination controller for affected agents. The framework constructs globally feasible trajectories via MAPF and regulates waypoint progression to reduce inter-agent conflicts during navigation.\n  Extensive evaluation on dense multi-agent benchmarks shows that our method boosts task completion from marginal to near-universal success, markedly reducing deadlocks and collisions. When integrated with hierarchical task planning, it enables coordinated navigation for heterogeneous robots, demonstrating that coupling reactive RL navigation with selective MAPF intervention yields a robust, zero-shot performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u4e0e\u591a\u4ee3\u7406\u8def\u5f84\u89c4\u5212\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u591a\u673a\u5668\u4eba\u5728\u62e5\u6324\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6210\u529f\u7387\u5e76\u51cf\u5c11\u4e86\u51b2\u7a81\u3002", "motivation": "\u5728\u62e5\u6324\u73af\u5883\u4e2d\u8fdb\u884c\u591a\u673a\u5668\u4eba\u5bfc\u822a\u65f6\uff0c\u5982\u4f55\u5e73\u8861\u53cd\u5e94\u6027\u907f\u514d\u78b0\u649e\u4e0e\u957f\u8ddd\u79bb\u76ee\u6807\u5b9e\u73b0\u4e4b\u95f4\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u548c\u591a\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\uff08MAPF\uff09\u7684\u6df7\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u76d1\u6d4b\u4ee3\u7406\u8fdb\u5c55\u4ee5\u53ca\u5728\u68c0\u6d4b\u5230\u6b7b\u9501\u65f6\u542f\u52a8\u534f\u8c03\u63a7\u5236\u5668\uff0c\u6784\u5efa\u5168\u5c40\u53ef\u884c\u7684\u8f68\u8ff9\u3002", "result": "\u5728\u591a\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u672c\u65b9\u6cd5\u5c06\u4efb\u52a1\u5b8c\u6210\u7387\u4ece\u5fae\u5f31\u63d0\u5347\u81f3\u63a5\u8fd1\u666e\u904d\u6210\u529f\uff0c\u540c\u65f6\u5728\u5f02\u6784\u673a\u5668\u4eba\u95f4\u5b9e\u73b0\u4e86\u534f\u8c03\u5bfc\u822a\u3002", "conclusion": "\u8be5\u6df7\u5408\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u5b8c\u6210\u7387\uff0c\u5e76\u6709\u6548\u51cf\u5c11\u4e86\u6b7b\u9501\u548c\u78b0\u649e\u4e8b\u4ef6\u3002"}}
{"id": "2511.23384", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.23384", "abs": "https://arxiv.org/abs/2511.23384", "authors": ["Isabel Whiteley Tscherniak", "Niels Christopher Thiemann", "Ana McWhinney-Fern\u00e1ndez", "Iustin Curcean", "Leon Jokinen", "Sadat Hodzic", "Thomas E. Huber", "Daniel Pavlov", "Manuel Methasani", "Pietro Marcolongo", "Glenn Viktor Krafczyk", "Oscar Osvaldo Soto Rivera", "Thien Le", "Flaminia Pallotti", "Enrico A. Fazzi", "neuroTUM e."], "title": "Improving motor imagery decoding methods for an EEG-based mobile brain-computer interface in the context of the 2024 Cybathlon", "comment": null, "summary": "Motivated by the Cybathlon 2024 competition, we developed a modular, online EEG-based brain-computer interface to address these challenges, increasing accessibility for individuals with severe mobility impairments. Our system uses three mental and motor imagery classes to control up to five control signals. The pipeline consists of four modules: data acquisition, preprocessing, classification, and the transfer function to map classification output to control dimensions. We use three diagonalized structured state-space sequence layers as a deep learning classifier. We developed a training game for our pilot where the mental tasks control the game during quick-time events. We implemented a mobile web application for live user feedback. The components were designed with a human-centred approach in collaboration with the tetraplegic user. We achieve up to 84% classification accuracy in offline analysis using an S4D-layer-based model. In a competition setting, our pilot successfully completed one task; we attribute the reduced performance in this context primarily to factors such as stress and the challenging competition environment. Following the Cybathlon, we further validated our pipeline with the original pilot and an additional participant, achieving a success rate of 73% in real-time gameplay. We also compare our model to the EEGEncoder, which is slower in training but has a higher performance. The S4D model outperforms the reference machine learning models. We provide insights into developing a framework for portable BCIs, bridging the gap between the laboratory and daily life. Specifically, our framework integrates modular design, real-time data processing, user-centred feedback, and low-cost hardware to deliver an accessible and adaptable BCI solution, addressing critical gaps in current BCI applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u9488\u5bf9\u4e25\u91cd\u8fd0\u52a8\u969c\u788d\u8005\u7684\u8111-\u673a\u63a5\u53e3\uff0c\u5e94\u7528\u4e8eCybathlon 2024\u6bd4\u8d5b\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u5b9e\u65f6\u5904\u7406\uff0c\u63d0\u5347\u4e86\u53ef\u53ca\u6027\u4e0e\u5b9e\u7528\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4e25\u91cd\u8fd0\u52a8\u969c\u788d\u8005\u5728Cybathlon 2024\u6bd4\u8d5b\u4e2d\u9762\u4e34\u7684\u6311\u6218\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u589e\u5f3a\u53ef\u53ca\u6027\u7684BCI\u7cfb\u7edf\u3002", "method": "\u7ed3\u5408\u591a\u79cd\u529f\u80fd\u6a21\u5757\uff0c\u5305\u62ec\u6570\u636e\u91c7\u96c6\u3001\u9884\u5904\u7406\u3001\u5206\u7c7b\u548c\u4f20\u8f93\u51fd\u6570\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u5206\u7c7b\u5668\u8fdb\u884c\u8111\u7535\u56fe\u4fe1\u53f7\u7684\u5b9e\u65f6\u5904\u7406\u3002", "result": "\u79bb\u7ebf\u5206\u6790\u4e2d\u8fbe\u5230\u4e86\u9ad8\u8fbe84%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u5728\u6bd4\u8d5b\u73af\u5883\u4e2d\u6210\u529f\u5b8c\u6210\u4e86\u4e00\u9879\u4efb\u52a1\uff0c\u5e76\u4e14\u5728\u540e\u7eed\u9a8c\u8bc1\u4e2d\u5b9e\u73b0\u4e8673%\u7684\u5b9e\u65f6\u6e38\u620f\u6210\u529f\u7387\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u3001\u5728\u7ebf\u7684\u8111-\u673a\u63a5\u53e3 (BCI)\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9488\u5bf9\u4e25\u91cd\u8fd0\u52a8\u969c\u788d\u8005\u7684\u53ef\u53ca\u6027\uff0c\u83b7\u5f97\u4e8673%\u7684\u5b9e\u65f6\u6e38\u620f\u6210\u529f\u7387\uff0c\u5e76\u4e3a\u4fbf\u643a\u5f0fBCI\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6846\u67b6\u548c\u89c1\u89e3\u3002"}}
{"id": "2511.22697", "categories": ["cs.RO", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.22697", "abs": "https://arxiv.org/abs/2511.22697", "authors": ["Chancharik Mitra", "Yusen Luo", "Raj Saravanan", "Dantong Niu", "Anirudh Pai", "Jesse Thomason", "Trevor Darrell", "Abrar Anwar", "Deva Ramanan", "Roei Herzig"], "title": "Mechanistic Finetuning of Vision-Language-Action Models via Few-Shot Demonstrations", "comment": null, "summary": "Vision-Language Action (VLAs) models promise to extend the remarkable success of vision-language models (VLMs) to robotics. Yet, unlike VLMs in the vision-language domain, VLAs for robotics require finetuning to contend with varying physical factors like robot embodiment, environment characteristics, and spatial relationships of each task. Existing fine-tuning methods lack specificity, adapting the same set of parameters regardless of a task's visual, linguistic, and physical characteristics. Inspired by functional specificity in neuroscience, we hypothesize that it is more effective to finetune sparse model representations specific to a given task. In this work, we introduce Robotic Steering, a finetuning approach grounded in mechanistic interpretability that leverages few-shot demonstrations to identify and selectively finetune task-specific attention heads aligned with the physical, visual, and linguistic requirements of robotic tasks. Through comprehensive on-robot evaluations with a Franka Emika robot arm, we demonstrate that Robotic Steering outperforms LoRA while achieving superior robustness under task variation, reduced computational cost, and enhanced interpretability for adapting VLAs to diverse robotic tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRobotic Steering\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u9488\u5bf9\u5177\u4f53\u4efb\u52a1\u7684\u6ce8\u610f\u5934\u5fae\u8c03\uff0c\u6539\u8fdb\u4e86\u673a\u5668\u4eba\u9886\u57df\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5fae\u8c03\u65b9\u6cd5\u7f3a\u4e4f\u7279\u5f02\u6027\uff0c\u4e0d\u80fd\u6709\u6548\u9002\u5e94\u4efb\u52a1\u7684\u89c6\u89c9\u3001\u8bed\u8a00\u548c\u7269\u7406\u7279\u5f81\u3002", "method": "Robotic Steering\u91c7\u7528\u4ee5\u673a\u5236\u53ef\u89e3\u91ca\u6027\u4e3a\u57fa\u7840\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u5229\u7528\u5c11\u91cf\u793a\u8303\u6765\u8bc6\u522b\u548c\u5fae\u8c03\u4efb\u52a1\u7279\u5b9a\u7684\u6ce8\u610f\u5934\u3002", "result": "\u5728\u4f7f\u7528Franka Emika\u673a\u5668\u4eba\u81c2\u8fdb\u884c\u7684\u7efc\u5408\u8bc4\u4f30\u4e2d\uff0cRobotic Steering\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u8f83\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u548c\u66f4\u9ad8\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "Robotic Steering\u6bd4LoRA\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u7684\u673a\u5668\u4eba\u4efb\u52a1\u3002"}}
{"id": "2511.22705", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.22705", "abs": "https://arxiv.org/abs/2511.22705", "authors": ["Ian Lalonde", "Jeff Denis", "Mathieu Lamy", "Camille Martin", "Karina Lebel", "Alexandre Girard"], "title": "A Two Degrees-of-Freedom Floor-Based Robot for Transfer and Rehabilitation Applications", "comment": "13 pages, 16 figures", "summary": "The ability to accomplish a sit-to-stand (STS) motion is key to increase functional mobility and reduce rehospitalization risks. While raising aid (transfer) devices and partial bodyweight support (rehabilitation) devices exist, both are unable to adjust the STS training to different mobility levels. Therefore, We have developed an STS training device that allows various configurations of impedance and vertical/forward forces to adapt to many training needs while maintaining commercial raising aid transfer capabilities. Experiments with healthy adults (both men and women) of various heights and weights show that the device 1) has a low impact on the natural STS kinematics, 2) can provide precise weight unloading at the patient's center of mass and 3) can add a forward virtual spring to assist the transfer of the bodyweight to the feet for seat-off, at the start of the STS motion.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u578bSTS\u8bad\u7ec3\u8bbe\u5907\uff0c\u80fd\u9002\u914d\u591a\u79cd\u8bad\u7ec3\u9700\u6c42\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u8fd0\u52a8\u5b66\u5f71\u54cd\u3001\u91cd\u91cf\u5378\u8f7d\u53ca\u4f53\u91cd\u8f6c\u79fb\u8f85\u52a9\u7b49\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u9274\u4e8e\u73b0\u6709\u8f6c\u79fb\u8bbe\u5907\u548c\u90e8\u5206\u8eab\u4f53\u652f\u6301\u8bbe\u5907\u65e0\u6cd5\u9488\u5bf9\u4e0d\u540c\u79fb\u52a8\u6027\u6c34\u5e73\u8c03\u6574STS\u8bad\u7ec3\uff0c\u56e0\u6b64\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u578b\u7684STS\u8bad\u7ec3\u8bbe\u5907\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u4e0e\u4e0d\u540c\u8eab\u9ad8\u548c\u4f53\u91cd\u7684\u5065\u5eb7\u6210\u5e74\u4eba\u8fdb\u884c\u6d4b\u8bd5\uff0c\u4ee5\u8bc4\u4f30\u8bbe\u5907\u5bf9\u81ea\u7136STS\u8fd0\u52a8\u5b66\u7684\u5f71\u54cd\u53ca\u5176\u7cbe\u51c6\u7684\u91cd\u91cf\u5378\u8f7d\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u8bbe\u5907\u5bf9\u81ea\u7136STS\u8fd0\u52a8\u5b66\u7684\u5f71\u54cd\u8f83\u5c0f\uff0c\u80fd\u591f\u5728\u60a3\u8005\u91cd\u5fc3\u5904\u63d0\u4f9b\u7cbe\u51c6\u7684\u91cd\u91cf\u5378\u8f7d\uff0c\u540c\u65f6\u5728STS\u8fd0\u52a8\u5f00\u59cb\u65f6\u589e\u52a0\u4e00\u4e2a\u524d\u5411\u865a\u62df\u5f39\u7c27\u4ee5\u5e2e\u52a9\u4f53\u91cd\u8f6c\u79fb\u3002", "conclusion": "\u6240\u5f00\u53d1\u7684STS\u8bad\u7ec3\u8bbe\u5907\u53ef\u4ee5\u9002\u5e94\u591a\u79cd\u8bad\u7ec3\u9700\u6c42\uff0c\u5e76\u5728\u4fdd\u6301\u5e38\u89c4\u63d0\u5347\u8f6c\u79fb\u80fd\u529b\u7684\u540c\u65f6\uff0c\u4f4e\u5f71\u54cd\u5730\u652f\u6301\u5750\u7ad9\u8fd0\u52a8\u3002"}}
{"id": "2511.22829", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.22829", "abs": "https://arxiv.org/abs/2511.22829", "authors": ["Zhen Tian", "Zhihao Lin"], "title": "Safe Autonomous Lane Changing: Planning with Dynamic Risk Fields and Time-Varying Convex Space Generation", "comment": null, "summary": "This paper presents a novel trajectory planning pipeline for complex driving scenarios like autonomous lane changing, by integrating risk-aware planning with guaranteed collision avoidance into a unified optimization framework. We first construct a dynamic risk fields (DRF) that captures both the static and dynamic collision risks from surrounding vehicles. Then, we develop a rigorous strategy for generating time-varying convex feasible spaces that ensure kinematic feasibility and safety requirements. The trajectory planning problem is formulated as a finite-horizon optimal control problem and solved using a constrained iterative Linear Quadratic Regulator (iLQR) algorithm that jointly optimizes trajectory smoothness, control effort, and risk exposure while maintaining strict feasibility. Extensive simulations demonstrate that our method outperforms traditional approaches in terms of safety and efficiency, achieving collision-free trajectories with shorter lane-changing distances (28.59 m) and times (2.84 s) while maintaining smooth and comfortable acceleration patterns. In dense roundabout environments the planner further demonstrates robust adaptability, producing larger safety margins, lower jerk, and superior curvature smoothness compared with APF, MPC, and RRT based baselines. These results confirm that the integrated DRF with convex feasible space and constrained iLQR solver provides a balanced solution for safe, efficient, and comfortable trajectory generation in dynamic and interactive traffic scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8f68\u8ff9\u89c4\u5212\u7ba1\u9053\uff0c\u901a\u8fc7\u6574\u5408\u98ce\u9669\u611f\u77e5\u89c4\u5212\u548c\u78b0\u649e\u907f\u514d\uff0c\u5c55\u793a\u4e86\u5728\u590d\u6742\u9a7e\u9a76\u573a\u666f\u4e2d\u5b89\u5168\u3001\u6709\u6548\u7684\u81ea\u52a8\u53d8\u9053\u8f68\u8ff9\u89c4\u5212\u3002", "motivation": "\u5e94\u5bf9\u590d\u6742\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u78b0\u649e\u98ce\u9669\u548c\u5b89\u5168\u9700\u6c42\uff0c\u5b9e\u73b0\u9ad8\u6548\u5b89\u5168\u7684\u81ea\u52a8\u9a7e\u9a76\u5b9e\u65bd\u3002", "method": "\u6574\u5408\u98ce\u9669\u611f\u77e5\u89c4\u5212\u548c\u786e\u4fdd\u78b0\u649e\u907f\u514d\u7684\u4f18\u5316\u6846\u67b6\uff0c\u4f7f\u7528\u52a8\u6001\u98ce\u9669\u573a\u6355\u6349\u5468\u56f4\u8f66\u8f86\u7684\u9759\u6001\u548c\u52a8\u6001\u78b0\u649e\u98ce\u9669\uff0c\u751f\u6210\u65f6\u95f4\u53d8\u5316\u7684\u51f8\u53ef\u884c\u7a7a\u95f4\uff0c\u5236\u5b9a\u8f68\u8ff9\u89c4\u5212\u7684\u6709\u9650\u65f6\u57df\u4f18\u5316\u95ee\u9898\u5e76\u7528\u7ea6\u675f\u8fed\u4ee3\u7ebf\u6027\u4e8c\u6b21\u8c03\u8282\u5668(iLQR)\u89e3\u51b3\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u548c\u6548\u7387\u65b9\u9762\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u572828.59\u7c73\u548c2.84\u79d2\u5185\u7684\u65e0\u78b0\u649e\u8f68\u8ff9\uff0c\u540c\u65f6\u4fdd\u6301\u5e73\u6ed1\u8212\u9002\u7684\u52a0\u901f\u6a21\u5f0f\u3002\u5728\u590d\u6742\u7684\u73af\u5f62\u4ea4\u53c9\u53e3\u73af\u5883\u4e2d\uff0c\u89c4\u5212\u5668\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u9002\u5e94\u6027\uff0c\u4ea7\u751f\u66f4\u5927\u7684\u5b89\u5168\u8fb9\u9645\u3001\u66f4\u4f4e\u7684\u51b2\u51fb\u548c\u4f18\u8d8a\u7684\u66f2\u7387\u5e73\u6ed1\u6027\u3002", "conclusion": "\u7efc\u5408DRF\u3001\u51f8\u53ef\u884c\u7a7a\u95f4\u548c\u7ea6\u675fiLQR\u6c42\u89e3\u5668\u63d0\u4f9b\u4e86\u5728\u52a8\u6001\u548c\u4ea4\u4e92\u4ea4\u901a\u573a\u666f\u4e2d\u5b89\u5168\u3001\u9ad8\u6548\u3001\u8212\u9002\u7684\u8f68\u8ff9\u751f\u6210\u65b9\u6848\u3002"}}
{"id": "2511.22744", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22744", "abs": "https://arxiv.org/abs/2511.22744", "authors": ["R\u00e9my Rahem", "Wael Suleiman"], "title": "Beyond Egocentric Limits: Multi-View Depth-Based Learning for Robust Quadrupedal Locomotion", "comment": "12 pages, 6 figures, code available at https://anonymous.4open.science/r/multiview-parkour-6FB8", "summary": "Recent progress in legged locomotion has allowed highly dynamic and parkour-like behaviors for robots, similar to their biological counterparts. Yet, these methods mostly rely on egocentric (first-person) perception, limiting their performance, especially when the viewpoint of the robot is occluded. A promising solution would be to enhance the robot's environmental awareness by using complementary viewpoints, such as multiple actors exchanging perceptual information. Inspired by this idea, this work proposes a multi-view depth-based locomotion framework that combines egocentric and exocentric observations to provide richer environmental context during agile locomotion. Using a teacher-student distillation approach, the student policy learns to fuse proprioception with dual depth streams while remaining robust to real-world sensing imperfections. To further improve robustness, we introduce extensive domain randomization, including stochastic remote-camera dropouts and 3D positional perturbations that emulate aerial-ground cooperative sensing. Simulation results show that multi-viewpoints policies outperform single-viewpoint baseline in gap crossing, step descent, and other dynamic maneuvers, while maintaining stability when the exocentric camera is partially or completely unavailable. Additional experiments show that moderate viewpoint misalignment is well tolerated when incorporated during training. This study demonstrates that heterogeneous visual feedback improves robustness and agility in quadrupedal locomotion. Furthermore, to support reproducibility, the implementation accompanying this work is publicly available at https://anonymous.4open.science/r/multiview-parkour-6FB8", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u89c6\u89d2\u6df1\u5ea6\u57fa\u7840\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u6846\u67b6\uff0c\u589e\u5f3a\u4e86\u673a\u5668\u4eba\u5728\u52a8\u6001\u8fd0\u52a8\u4e2d\u7684\u73af\u5883\u610f\u8bc6\uff0c\u7279\u522b\u662f\u5728\u89c6\u7ebf\u53d7\u963b\u65f6\u3002", "motivation": "\u673a\u5668\u4eba\u8fd0\u52a8\u7684\u6700\u65b0\u8fdb\u5c55\u4f7f\u5f97\u5176\u80fd\u591f\u8fdb\u884c\u50cf\u751f\u7269\u4e00\u6837\u7684\u52a8\u6001\u548c\u8dd1\u9177\u884c\u4e3a\uff0c\u4f46\u5927\u591a\u4f9d\u8d56\u4e8e\u7b2c\u4e00\u4eba\u79f0\u7684\u611f\u77e5\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u6211\u611f\u77e5\u548c\u5916\u90e8\u89c2\u5bdf\u7684\u591a\u89c6\u89d2\u6df1\u5ea6\u8fd0\u52a8\u6846\u67b6\uff0c\u4f7f\u7528\u6559\u5e08-\u5b66\u751f\u84b8\u998f\u65b9\u6cd5\uff0c\u5b66\u751f\u7b56\u7565\u878d\u5408\u4e86\u81ea\u6211\u611f\u77e5\u548c\u53cc\u6df1\u5ea6\u6d41\uff0c\u540c\u65f6\u5f15\u5165\u4e86\u5e7f\u6cdb\u7684\u9886\u57df\u968f\u673a\u5316\u4ee5\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u5728\u8de8\u8d8a\u7f1d\u9699\u3001\u4e0b\u5761\u4ee5\u53ca\u5176\u4ed6\u52a8\u6001\u52a8\u4f5c\u4e2d\uff0c\u591a\u89c6\u89d2\u7b56\u7565\u7684\u6027\u80fd\u4f18\u4e8e\u5355\u4e00\u89c6\u89d2\u57fa\u7ebf\uff0c\u5e76\u4e14\u5728\u5916\u90e8\u76f8\u673a\u90e8\u5206\u6216\u5b8c\u5168\u4e0d\u53ef\u7528\u65f6\u4ecd\u4fdd\u6301\u7a33\u5b9a\u3002", "conclusion": "\u5f02\u6784\u89c6\u89c9\u53cd\u9988\u63d0\u5347\u4e86\u56db\u8db3 locomotion \u7684\u9c81\u68d2\u6027\u548c\u7075\u6d3b\u6027\uff0c\u7814\u7a76\u7ed3\u679c\u53ca\u5b9e\u73b0\u53ef\u5728\u516c\u5f00\u5e73\u53f0\u4e0a\u83b7\u53d6\u3002"}}
{"id": "2511.22773", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.22773", "abs": "https://arxiv.org/abs/2511.22773", "authors": ["Rui Heng Yang", "Xuan Zhao", "Leo Maxime Brunswic", "Montgomery Alban", "Mateo Clemente", "Tongtong Cao", "Jun Jin", "Amir Rasouli"], "title": "CAPE: Context-Aware Diffusion Policy Via Proximal Mode Expansion for Collision Avoidance", "comment": "4 tables, 9 figures", "summary": "In robotics, diffusion models can capture multi-modal trajectories from demonstrations, making them a transformative approach in imitation learning. However, achieving optimal performance following this regiment requires a large-scale dataset, which is costly to obtain, especially for challenging tasks, such as collision avoidance. In those tasks, generalization at test time demands coverage of many obstacles types and their spatial configurations, which are impractical to acquire purely via data. To remedy this problem, we propose Context-Aware diffusion policy via Proximal mode Expansion (CAPE), a framework that expands trajectory distribution modes with context-aware prior and guidance at inference via a novel prior-seeded iterative guided refinement procedure. The framework generates an initial trajectory plan and executes a short prefix trajectory, and then the remaining trajectory segment is perturbed to an intermediate noise level, forming a trajectory prior. Such a prior is context-aware and preserves task intent. Repeating the process with context-aware guided denoising iteratively expands mode support to allow finding smoother, less collision-prone trajectories. For collision avoidance, CAPE expands trajectory distribution modes with collision-aware context, enabling the sampling of collision-free trajectories in previously unseen environments while maintaining goal consistency. We evaluate CAPE on diverse manipulation tasks in cluttered unseen simulated and real-world settings and show up to 26% and 80% higher success rates respectively compared to SOTA methods, demonstrating better generalization to unseen environments.", "AI": {"tldr": "CAPE\u6846\u67b6\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u65b9\u6cd5\u6269\u5c55\u8f68\u8ff9\u6a21\u5f0f\uff0c\u63d0\u9ad8\u4e86\u5728\u672a\u89c1\u73af\u5883\u4e2d\u907f\u78b0\u4efb\u52a1\u7684\u6210\u529f\u7387\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5728\u590d\u6742\u4efb\u52a1\uff08\u5982\u907f\u78b0\uff09\u4e2d\u83b7\u53d6\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u56f0\u96be\uff0c\u5e76\u5b9e\u73b0\u6700\u4f73\u8868\u73b0\u7684\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6269\u5c55\u6a21\u5f0f\u548c\u8fed\u4ee3\u5f15\u5bfc\u7cbe\u70bc\u7a0b\u5e8f\uff0c\u751f\u6210\u521d\u59cb\u8f68\u8ff9\u8ba1\u5212\uff0c\u5e76\u5728\u6267\u884c\u77ed\u524d\u7f00\u8f68\u8ff9\u540e\uff0c\u901a\u8fc7\u5c06\u5269\u4f59\u8f68\u8ff9\u6bb5\u6270\u52a8\u5230\u4e2d\u95f4\u566a\u58f0\u6c34\u5e73\uff0c\u5f62\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u8f68\u8ff9\u5148\u9a8c\u3002", "result": "\u76f8\u8f83\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0cCAPE\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u8bbe\u7f6e\u4e0b\u5206\u522b\u63d0\u9ad8\u4e86\u6700\u591a26%\u548c80%\u7684\u6210\u529f\u7387\uff0c\u8868\u660e\u5176\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u66f4\u597d\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CAPE\u6846\u67b6\u5728\u591a\u79cd\u64cd\u63a7\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6210\u529f\u7387\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u9762\u5bf9\u672a\u77e5\u73af\u5883\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.22777", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.22777", "abs": "https://arxiv.org/abs/2511.22777", "authors": ["Sajjad Pakdamansavoji", "Mozhgan Pourkeshavarz", "Adam Sigal", "Zhiyuan Li", "Rui Heng Yang", "Amir Rasouli"], "title": "Improving Robotic Manipulation Robustness via NICE Scene Surgery", "comment": "11 figures, 3 tables", "summary": "Learning robust visuomotor policies for robotic manipulation remains a challenge in real-world settings, where visual distractors can significantly degrade performance and safety. In this work, we propose an effective and scalable framework, Naturalistic Inpainting for Context Enhancement (NICE). Our method minimizes out-of-distribution (OOD) gap in imitation learning by increasing visual diversity through construction of new experiences using existing demonstrations. By utilizing image generative frameworks and large language models, NICE performs three editing operations, object replacement, restyling, and removal of distracting (non-target) objects. These changes preserve spatial relationships without obstructing target objects and maintain action-label consistency. Unlike previous approaches, NICE requires no additional robot data collection, simulator access, or custom model training, making it readily applicable to existing robotic datasets.\n  Using real-world scenes, we showcase the capability of our framework in producing photo-realistic scene enhancement. For downstream tasks, we use NICE data to finetune a vision-language model (VLM) for spatial affordance prediction and a vision-language-action (VLA) policy for object manipulation. Our evaluations show that NICE successfully minimizes OOD gaps, resulting in over 20% improvement in accuracy for affordance prediction in highly cluttered scenes. For manipulation tasks, success rate increases on average by 11% when testing in environments populated with distractors in different quantities. Furthermore, we show that our method improves visual robustness, lowering target confusion by 6%, and enhances safety by reducing collision rate by 7%.", "AI": {"tldr": "\u63d0\u51fa\u4e86NICE\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u89c6\u89c9\u591a\u6837\u6027\u548c\u65e0\u9700\u989d\u5916\u6570\u636e\u96c6\u8bad\u7ec3\u6765\u5b9e\u73b0\u66f4\u597d\u7684\u673a\u5668\u4eba\u64cd\u63a7\uff0c\u663e\u8457\u63d0\u9ad8\u591a\u9879\u4efb\u52a1\u7684\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u5e94\u5bf9\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u89c6\u89c9\u5e72\u6270\u5bf9\u673a\u5668\u4eba\u64cd\u4f5c\u6027\u80fd\u548c\u5b89\u5168\u6027\u9020\u6210\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5df2\u6709\u793a\u4f8b\u6784\u5efa\u65b0\u4f53\u9a8c\uff0c\u8fd0\u7528\u56fe\u50cf\u751f\u6210\u548c\u8bed\u8a00\u6a21\u578b\u6280\u672f\u8fdb\u884c\u64cd\u4f5c\uff0c\u5982\u7269\u4f53\u66ff\u6362\u548c\u53bb\u5e72\u6270\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u540d\u4e3aNICE\uff0c\u901a\u8fc7\u589e\u52a0\u89c6\u89c9\u591a\u6837\u6027\u6765\u51cf\u5c11\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684OOD\u5dee\u8ddd\u3002\u8be5\u65b9\u6cd5\u6d89\u53ca\u56fe\u50cf\u751f\u6210\u6846\u67b6\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u8fdb\u884c\u4e09\u79cd\u7f16\u8f91\u64cd\u4f5c\uff1a\u7269\u4f53\u66ff\u6362\u3001\u98ce\u683c\u91cd\u5851\u548c\u53bb\u9664\u5e72\u6270\u7269\u4f53\u3002NICE\u5728\u4fdd\u6301\u7a7a\u95f4\u5173\u7cfb\u548c\u52a8\u4f5c\u6807\u7b7e\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u65e0\u9700\u989d\u5916\u7684\u673a\u5668\u4eba\u6570\u636e\u6536\u96c6\u6216\u81ea\u5b9a\u4e49\u6a21\u578b\u8bad\u7ec3\uff0c\u9002\u7528\u4e8e\u73b0\u6709\u673a\u5668\u4eba\u6570\u636e\u96c6\u3002", "conclusion": "NICE\u6846\u67b6\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u7167\u7247\u7ea7\u771f\u5b9e\u611f\u7684\u573a\u666f\u589e\u5f3a\uff0c\u51cf\u5c11OOD\u5dee\u8ddd\uff0c\u9884\u6d4b\u7cbe\u5ea6\u63d0\u9ad8\u8d85\u8fc720%\uff0c\u64cd\u4f5c\u6210\u529f\u7387\u63d0\u9ad8\u5e73\u574711%\u3002\u6b64\u5916\uff0c\u89c6\u89c9\u9c81\u68d2\u6027\u63d0\u9ad8\uff0c\u76ee\u6807\u6df7\u6dc6\u7387\u964d\u4f4e6%\uff0c\u78b0\u649e\u7387\u964d\u4f4e7%\u3002"}}
{"id": "2511.22780", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.22780", "abs": "https://arxiv.org/abs/2511.22780", "authors": ["Amir Rasouli", "Montgomery Alban", "Sajjad Pakdamansavoji", "Zhiyuan Li", "Zhanguang Zhang", "Aaron Wu", "Xuan Zhao"], "title": "Distracted Robot: How Visual Clutter Undermine Robotic Manipulation", "comment": "12 figures, 2 tables", "summary": "In this work, we propose an evaluation protocol for examining the performance of robotic manipulation policies in cluttered scenes. Contrary to prior works, we approach evaluation from a psychophysical perspective, therefore we use a unified measure of clutter that accounts for environmental factors as well as the distractors quantity, characteristics, and arrangement. Using this measure, we systematically construct evaluation scenarios in both hyper-realistic simulation and real-world and conduct extensive experimentation on manipulation policies, in particular vision-language-action (VLA) models. Our experiments highlight the significant impact of scene clutter, lowering the performance of the policies, by as much as 34% and show that despite achieving similar average performance across the tasks, different VLA policies have unique vulnerabilities and a relatively low agreement on success scenarios. We further show that our clutter measure is an effective indicator of performance degradation and analyze the impact of distractors in terms of their quantity and occluding influence. At the end, we show that finetuning on enhanced data, although effective, does not equally remedy all negative impacts of clutter on performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u5728\u6df7\u4e71\u573a\u666f\u4e2d\u7684\u8868\u73b0\u7684\u534f\u8bae\uff0c\u5e76\u53d1\u73b0\u573a\u666f\u6df7\u4e71\u5bf9\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u4ece\u5fc3\u7406\u7269\u7406\u5b66\u89d2\u5ea6\u51fa\u53d1\uff0c\u7cfb\u7edf\u5730\u8bc4\u4f30\u673a\u5668\u4eba\u64cd\u63a7\u653f\u7b56\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u4e86\u89e3\u6df7\u4e71\u5bf9\u64cd\u4f5c\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u5efa\u7acb\u4e00\u4e2a\u8bc4\u4f30\u534f\u8bae\u6765\u6d4b\u8bd5\u673a\u5668\u4eba\u64cd\u63a7\u653f\u7b56\u5728\u6df7\u4e71\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u4f7f\u7528\u7edf\u4e00\u7684\u6df7\u4e71\u91cf\u5ea6\u4f5c\u4e3a\u8bc4\u4f30\u6807\u51c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u573a\u666f\u6df7\u4e71\u663e\u8457\u964d\u4f4e\u4e86\u7b56\u7565\u7684\u6027\u80fd\uff0c\u6700\u591a\u53ef\u8fbe34%\uff0c\u5e76\u4e14\u4e0d\u540c\u7684VLA\u653f\u7b56\u5728\u6210\u529f\u573a\u666f\u4e0a\u8868\u73b0\u51fa\u72ec\u7279\u7684\u8106\u5f31\u6027\u548c\u8f83\u4f4e\u7684\u5171\u8bc6\u3002", "conclusion": "\u867d\u7136\u901a\u8fc7\u589e\u5f3a\u6570\u636e\u7684\u5fae\u8c03\u662f\u6709\u6548\u7684\uff0c\u4f46\u5bf9\u4e0d\u540c\u7684\u6df7\u4e71\u5f71\u54cd\u5e76\u4e0d\u4e00\u6837\uff0c\u56e0\u6b64\u5e76\u4e0d\u80fd\u5b8c\u5168\u5f25\u8865\u6df7\u4e71\u5bf9\u6027\u80fd\u7684\u8d1f\u9762\u5f71\u54cd\u3002"}}
{"id": "2511.22847", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22847", "abs": "https://arxiv.org/abs/2511.22847", "authors": ["Yuying Zhang", "Na Fan", "Haowen Zheng", "Junning Liang", "Zongliang Pan", "Qifeng Chen", "Ximin Lyu"], "title": "Threat-Aware UAV Dodging of Human-Thrown Projectiles with an RGB-D Camera", "comment": null, "summary": "Uncrewed aerial vehicles (UAVs) performing tasks such as transportation and aerial photography are vulnerable to intentional projectile attacks from humans. Dodging such a sudden and fast projectile poses a significant challenge for UAVs, requiring ultra-low latency responses and agile maneuvers. Drawing inspiration from baseball, in which pitchers' body movements are analyzed to predict the ball's trajectory, we propose a novel real-time dodging system that leverages an RGB-D camera. Our approach integrates human pose estimation with depth information to predict the attacker's motion trajectory and the subsequent projectile trajectory. Additionally, we introduce an uncertainty-aware dodging strategy to enable the UAV to dodge incoming projectiles efficiently. Our perception system achieves high prediction accuracy and outperforms the baseline in effective distance and latency. The dodging strategy addresses temporal and spatial uncertainties to ensure UAV safety. Extensive real-world experiments demonstrate the framework's reliable dodging capabilities against sudden attacks and its outstanding robustness across diverse scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRGB-D\u76f8\u673a\u7684\u5b9e\u65f6\u8eb2\u907f\u7cfb\u7edf\uff0c\u6574\u5408\u4e86\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u4e0e\u6df1\u5ea6\u4fe1\u606f\uff0c\u63d0\u5347\u65e0\u4eba\u673a\u5bf9 projectile \u653b\u51fb\u7684\u8eb2\u907f\u80fd\u529b\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u5728\u5b9e\u65f6\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u65e0\u4eba\u673a\u5728\u6267\u884c\u8fd0\u8f93\u548c\u822a\u62cd\u4efb\u52a1\u65f6\u6613\u53d7\u4eba\u7c7b\u7684\u6545\u610f\u653b\u51fb\uff0c\u56e0\u6b64\u9700\u8981\u5feb\u901f\u53cd\u5e94\u548c\u654f\u6377\u7684\u8eb2\u907f\u80fd\u529b\u3002", "method": "\u901a\u8fc7RGB-D\u76f8\u673a\u96c6\u6210\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u4e0e\u6df1\u5ea6\u4fe1\u606f\u6765\u9884\u6d4b\u653b\u51fb\u8005\u7684\u52a8\u4f5c\u8f68\u8ff9\u548c\u968f\u540e\u7684\u5f39\u9053\uff1b\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8003\u8651\u4e0d\u786e\u5b9a\u6027\u7684\u8eb2\u907f\u7b56\u7565\u3002", "result": "\u611f\u77e5\u7cfb\u7edf\u5b9e\u73b0\u4e86\u9ad8\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5e76\u5728\u6709\u6548\u8ddd\u79bb\u548c\u5ef6\u8fdf\u65b9\u9762\u8d85\u8fc7\u57fa\u7ebf\uff1b\u771f\u5b9e\u573a\u666f\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u9762\u5bf9\u7a81\u53d1\u653b\u51fb\u65f6\u8868\u73b0\u51fa\u53ef\u9760\u7684\u8eb2\u907f\u80fd\u529b\u548c\u5353\u8d8a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5b9e\u65f6\u8eb2\u907f\u7cfb\u7edf\uff0c\u80fd\u6709\u6548\u589e\u5f3a\u65e0\u4eba\u673a\u5728\u53d7\u5230\u653b\u51fb\u65f6\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2511.22860", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.22860", "abs": "https://arxiv.org/abs/2511.22860", "authors": ["Sacchin Sundar", "Atman Kikani", "Aaliya Alam", "Sumukh Shrote", "A. Nayeemulla Khan", "A. Shahina"], "title": "MARVO: Marine-Adaptive Radiance-aware Visual Odometry", "comment": "10 pages, 5 figures, 3 tables, Submitted to CVPR2026", "summary": "Underwater visual localization remains challenging due to wavelength-dependent attenuation, poor texture, and non-Gaussian sensor noise. We introduce MARVO, a physics-aware, learning-integrated odometry framework that fuses underwater image formation modeling, differentiable matching, and reinforcement-learning optimization. At the front-end, we extend transformer-based feature matcher with a Physics Aware Radiance Adapter that compensates for color channel attenuation and contrast loss, yielding geometrically consistent feature correspondences under turbidity. These semi dense matches are combined with inertial and pressure measurements inside a factor-graph backend, where we formulate a keyframe-based visual-inertial-barometric estimator using GTSAM library. Each keyframe introduces (i) Pre-integrated IMU motion factors, (ii) MARVO-derived visual pose factors, and (iii) barometric depth priors, giving a full-state MAP estimate in real time. Lastly, we introduce a Reinforcement-Learningbased Pose-Graph Optimizer that refines global trajectories beyond local minima of classical least-squares solvers by learning optimal retraction actions on SE(2).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aMARVO\u7684\u6c34\u4e0b\u89c6\u89c9\u5b9a\u4f4d\u6846\u67b6\uff0c\u7ed3\u5408\u7269\u7406\u5efa\u6a21\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u4ee5\u5e94\u5bf9\u6c34\u4e0b\u73af\u5883\u4e2d\u7684\u89c6\u89c9\u5b9a\u4f4d\u6311\u6218\u3002", "motivation": "\u6c34\u4e0b\u89c6\u89c9\u5b9a\u4f4d\u56e0\u6ce2\u957f\u4f9d\u8d56\u8870\u51cf\u3001\u7eb9\u7406\u5dee\u5dee\u548c\u975e\u9ad8\u65af\u4f20\u611f\u5668\u566a\u58f0\u800c\u9762\u4e34\u6311\u6218\u3002", "method": "\u5f15\u5165MARVO\uff0c\u4e00\u4e2a\u4ee5\u7269\u7406\u4e3a\u57fa\u7840\u548c\u5b66\u4e60\u96c6\u6210\u7684\u91cc\u7a0b\u8ba1\u6846\u67b6\uff0c\u878d\u5408\u6c34\u4e0b\u56fe\u50cf\u5f62\u6210\u5efa\u6a21\u3001\u53ef\u5fae\u5339\u914d\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u3002", "result": "\u5728\u524d\u7aef\uff0c\u6269\u5c55\u4e86\u57fa\u4e8e\u53d8\u6362\u5668\u7684\u7279\u5f81\u5339\u914d\u5668\uff0c\u4f7f\u7528\u7269\u7406\u611f\u77e5\u8f90\u5c04\u9002\u914d\u5668\u6765\u8865\u507f\u989c\u8272\u901a\u9053\u8870\u51cf\u548c\u5bf9\u6bd4\u5ea6\u635f\u5931\uff0c\u751f\u6210\u5728\u6d51\u6d4a\u73af\u5883\u4e2d\u51e0\u4f55\u4e00\u81f4\u7684\u7279\u5f81\u5bf9\u5e94\u3002", "conclusion": "\u6700\u540e\uff0c\u5f15\u5165\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u59ff\u6001\u56fe\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u5728SE(2)\u4e0a\u5b66\u4e60\u6700\u4f73\u91cd\u7275\u5f15\u52a8\u4f5c\uff0c\u4ee5\u8d85\u8d8a\u7ecf\u5178\u6700\u5c0f\u4e8c\u4e58\u6c42\u89e3\u5668\u7684\u5c40\u90e8\u6700\u5c0f\u503c\uff0c\u7cbe\u7ec6\u5316\u5168\u5c40\u8f68\u8ff9\u3002"}}
{"id": "2511.22865", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.22865", "abs": "https://arxiv.org/abs/2511.22865", "authors": ["Wonjeong Ryu", "Seungjun Yu", "Seokha Moon", "Hojun Choi", "Junsung Park", "Jinkyu Kim", "Hyunjung Shim"], "title": "SUPER-AD: Semantic Uncertainty-aware Planning for End-to-End Robust Autonomous Driving", "comment": null, "summary": "End-to-End (E2E) planning has become a powerful paradigm for autonomous driving, yet current systems remain fundamentally uncertainty-blind. They assume perception outputs are fully reliable, even in ambiguous or poorly observed scenes, leaving the planner without an explicit measure of uncertainty. To address this limitation, we propose a camera-only E2E framework that estimates aleatoric uncertainty directly in BEV space and incorporates it into planning. Our method produces a dense, uncertainty-aware drivability map that captures both semantic structure and geometric layout at pixel-level resolution. To further promote safe and rule-compliant behavior, we introduce a lane-following regularization that encodes lane structure and traffic norms. This prior stabilizes trajectory planning under normal conditions while preserving the flexibility needed for maneuvers such as overtaking or lane changes. Together, these components enable robust and interpretable trajectory planning, even under challenging uncertainty conditions. Evaluated on the NAVSIM benchmark, our method achieves state-of-the-art performance, delivering substantial gains on both the challenging NAVHARD and NAVSAFE subsets. These results demonstrate that our principled aleatoric uncertainty modeling combined with driving priors significantly advances the safety and reliability of camera-only E2E autonomous driving.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6444\u50cf\u5934\u7684E2E\u6846\u67b6\uff0c\u76f4\u63a5\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\u5e76\u7ed3\u5408\u8def\u51b5\u7ea6\u675f\uff0c\u5b9e\u73b0\u4e86\u81ea\u4e3b\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u5728\u81ea\u4e3b\u9a7e\u9a76\u4e2d\u5047\u8bbe\u611f\u77e5\u8f93\u51fa\u5b8c\u5168\u53ef\u9760\uff0c\u800c\u6ca1\u6709\u660e\u786e\u7684\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\uff0c\u5bfc\u81f4\u89c4\u5212\u8fc7\u7a0b\u7f3a\u4e4f\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f9d\u8d56\u6444\u50cf\u5934\u7684E2E\u6846\u67b6\uff0c\u76f4\u63a5\u5728BEV\u7a7a\u95f4\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u7ed3\u5408\u8def\u51b5\u9075\u4ece\u6027\u7ea6\u675f\u8fdb\u884c\u89c4\u5212\u3002", "result": "\u8be5\u65b9\u6cd5\u751f\u6210\u4e86\u5bc6\u96c6\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u53ef\u9a7e\u9a76\u6027\u5730\u56fe\uff0c\u5e76\u5728NAVSIM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u5347\u4e86\u8868\u73b0\uff0c\u5c24\u5176\u662f\u5728NAVHARD\u548cNAVSAFE\u5b50\u96c6\u4e0a\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728NAVSIM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u9886\u5148\u6027\u80fd\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6444\u50cf\u5934\u4ec5E2E\u81ea\u4e3b\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2511.22928", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22928", "abs": "https://arxiv.org/abs/2511.22928", "authors": ["Jiaxin Liu", "Xiangyu Yan", "Liang Peng", "Lei Yang", "Lingjun Zhang", "Yuechen Luo", "Yueming Tao", "Ashton Yu Xuan Tan", "Mu Li", "Lei Zhang", "Ziqi Zhan", "Sai Guo", "Hong Wang", "Jun Li"], "title": "Seeing before Observable: Potential Risk Reasoning in Autonomous Driving via Vision Language Models", "comment": null, "summary": "Ensuring safety remains a key challenge for autonomous vehicles (AVs), especially in rare and complex scenarios. One critical but understudied aspect is the \\textbf{potential risk} situations, where the risk is \\textbf{not yet observable} but can be inferred from subtle precursors, such as anomalous behaviors or commonsense violations. Recognizing these precursors requires strong semantic understanding and reasoning capabilities, which are often absent in current AV systems due to the scarcity of such cases in existing driving or risk-centric datasets. Moreover, current autonomous driving accident datasets often lack annotations of the causal reasoning chains behind incidents, which are essential for identifying potential risks before they become observable. To address these gaps, we introduce PotentialRiskQA, a novel vision-language dataset designed for reasoning about potential risks prior to observation. Each sample is annotated with structured scene descriptions, semantic precursors, and inferred risk outcomes. Based on this dataset, we further propose PR-Reasoner, a vision-language-model-based framework tailored for onboard potential risk reasoning. Experimental results show that fine-tuning on PotentialRiskQA enables PR-Reasoner to significantly enhance its performance on the potential risk reasoning task compared to baseline VLMs. Together, our dataset and model provide a foundation for developing autonomous systems with improved foresight and proactive safety capabilities, moving toward more intelligent and resilient AVs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PotentialRiskQA\u6570\u636e\u96c6\u548cPR-Reasoner\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u9762\u4e34\u7684\u6f5c\u5728\u98ce\u9669\u63a8\u7406\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u786e\u4fdd\u5b89\u5168\u662f\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u9762\u4e34\u7684\u5173\u952e\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u7a00\u6709\u548c\u590d\u6742\u573a\u666f\u4e2d\u3002\u8bc6\u522b\u6f5c\u5728\u98ce\u9669\u7684\u524d\u5146\u9700\u8981\u5f3a\u5927\u7684\u8bed\u4e49\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u6846\u67b6PR-Reasoner\uff0c\u4e13\u95e8\u7528\u4e8e\u6f5c\u5728\u98ce\u9669\u7684\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728PotentialRiskQA\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u4f7fPR-Reasoner\u5728\u6f5c\u5728\u98ce\u9669\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u76f8\u8f83\u4e8e\u57fa\u7ebf\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "\u6211\u4eec\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e3a\u5f00\u53d1\u5177\u6709\u66f4\u597d\u524d\u77bb\u6027\u548c\u4e3b\u52a8\u5b89\u5168\u80fd\u529b\u7684\u81ea\u4e3b\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u671d\u7740\u66f4\u667a\u80fd\u548c\u66f4\u5177\u97e7\u6027\u7684\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u8fc8\u8fdb\u3002"}}
{"id": "2511.22963", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.22963", "abs": "https://arxiv.org/abs/2511.22963", "authors": ["Zhirui Liu", "Kaiyang Ji", "Ke Yang", "Jingyi Yu", "Ye Shi", "Jingya Wang"], "title": "Commanding Humanoid by Free-form Language: A Large Language Action Model with Unified Motion Vocabulary", "comment": "Project page: https://humanoidlla.github.io/", "summary": "Enabling humanoid robots to follow free-form language commands is critical for seamless human-robot interaction, collaborative task execution, and general-purpose embodied intelligence. While recent advances have improved low-level humanoid locomotion and robot manipulation, language-conditioned whole-body control remains a significant challenge. Existing methods are often limited to simple instructions and sacrifice either motion diversity or physical plausibility. To address this, we introduce Humanoid-LLA, a Large Language Action Model that maps expressive language commands to physically executable whole-body actions for humanoid robots. Our approach integrates three core components: a unified motion vocabulary that aligns human and humanoid motion primitives into a shared discrete space; a vocabulary-directed controller distilled from a privileged policy to ensure physical feasibility; and a physics-informed fine-tuning stage using reinforcement learning with dynamics-aware rewards to enhance robustness and stability. Extensive evaluations in simulation and on a real-world Unitree G1 humanoid show that Humanoid-LLA delivers strong language generalization while maintaining high physical fidelity, outperforming existing language-conditioned controllers in motion naturalness, stability, and execution success rate.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Humanoid-LLA\uff0c\u4e00\u4e2a\u5927\u578b\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\uff0c\u80fd\u591f\u5c06\u5bcc\u6709\u8868\u73b0\u529b\u7684\u8bed\u8a00\u6307\u4ee4\u6620\u5c04\u5230\u53ef\u6267\u884c\u7684\u5168\u8eab\u52a8\u4f5c\uff0c\u63d0\u5347\u4eba\u5f62\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u7684\u4e92\u52a8\u3002", "motivation": "\u63a8\u52a8\u4eba\u5f62\u673a\u5668\u4eba\u80fd\u591f\u7406\u89e3\u5e76\u6267\u884c\u590d\u6742\u7684\u8bed\u8a00\u6307\u4ee4\uff0c\u4ee5\u5b9e\u73b0\u66f4\u81ea\u7136\u7684\u4eba\u4e0e\u673a\u5668\u4eba\u4ea4\u4e92\u548c\u534f\u4f5c\u3002", "method": "\u63d0\u51faHumanoid-LLA\u6a21\u578b\uff0c\u6574\u5408\u7edf\u4e00\u7684\u8fd0\u52a8\u8bcd\u6c47\u3001\u4ece\u7279\u6743\u7b56\u7565\u63d0\u70bc\u7684\u63a7\u5236\u5668\u53ca\u57fa\u4e8e\u52a8\u6001\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u8c03\u4f18\u9636\u6bb5\u3002", "result": "\u5728\u6a21\u62df\u73af\u5883\u548c\u5b9e\u9645\u7684Unitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cHumanoid-LLA\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u8bed\u8a00\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u7269\u7406\u771f\u5b9e\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u8bed\u8a00\u6761\u4ef6\u63a7\u5236\u5668\u3002", "conclusion": "Humanoid-LLA\u663e\u8457\u63d0\u5347\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u9075\u5faa\u81ea\u7531\u5f62\u5f0f\u8bed\u8a00\u6307\u4ee4\u65b9\u9762\u7684\u80fd\u529b\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u8fd0\u52a8\u63a7\u5236\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u4e86\u66f4\u5f3a\u7684\u8bed\u8a00\u6cdb\u5316\u80fd\u529b\u3001\u8fd0\u52a8\u81ea\u7136\u6027\u3001\u7a33\u5b9a\u6027\u548c\u6267\u884c\u6210\u529f\u7387\u3002"}}
{"id": "2511.22996", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22996", "abs": "https://arxiv.org/abs/2511.22996", "authors": ["Ke Chen"], "title": "Analytical Inverse Kinematic Solution for \"Moz1\" NonSRS 7-DOF Robot arm with novel arm angle", "comment": null, "summary": "This paper presents an analytical solution to the inverse kinematic problem(IKP) for the seven degree-of-freedom (7-DOF) Moz1 Robot Arm with offsets on wrist. We provide closed-form solutions with the novel arm angle . it allow fully self-motion and solve the problem of algorithmic singularities within the workspace. It also provides information on how the redundancy is resolved in a new arm angle representation where traditional SEW angle faied to be defined and how singularities are handled. The solution is simple, fast and exact, providing full solution space (i.e. all 16 solutions) per pose.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u4e03\u81ea\u7531\u5ea6Moz1\u673a\u5668\u4eba\u624b\u81c2\u7684\u9006\u8fd0\u52a8\u5b66\u95ee\u9898\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89e3\u6790\u89e3\uff0c\u89e3\u51b3\u4e86\u7b97\u6cd5\u5947\u5f02\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u6bcf\u4e2a\u59ff\u6001\u7684\u5168\u90e8\u89e3\u3002", "motivation": "\u9488\u5bf9\u673a\u5668\u4eba\u624b\u81c2\u5728\u5de5\u4f5c\u7a7a\u95f4\u5185\u5b58\u5728\u7684\u9006\u8fd0\u52a8\u5b66\u95ee\u9898\uff0c\u76ee\u7684\u662f\u63d0\u4f9b\u4e00\u79cd\u9ad8\u6548\u3001\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u63d0\u9ad8\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u4e03\u81ea\u7531\u5ea6 (7-DOF) Moz1\u673a\u5668\u4eba\u624b\u81c2\u7684\u9006\u8fd0\u52a8\u5b66\u95ee\u9898\u7684\u89e3\u6790\u89e3\uff0c\u91c7\u7528\u4e86\u65b0\u9896\u7684\u81c2\u89d2\u8868\u793a\u6cd5\u3002", "result": "\u63d0\u4f9b\u4e86\u5c01\u95ed\u5f62\u5f0f\u7684\u89e3\u6cd5\uff0c\u5141\u8bb8\u5b8c\u5168\u81ea\u6211\u8fd0\u52a8\uff0c\u89e3\u51b3\u4e86\u5de5\u4f5c\u7a7a\u95f4\u5185\u7684\u7b97\u6cd5\u5947\u5f02\u6027\u95ee\u9898\u3002", "conclusion": "\u63d0\u51fa\u7684\u89e3\u51b3\u65b9\u6848\u7b80\u5355\u3001\u5feb\u901f\u4e14\u7cbe\u786e\uff0c\u4e3a\u6bcf\u4e2a\u59ff\u6001\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u89e3\u7a7a\u95f4\uff08\u5373\u6240\u670916\u79cd\u89e3\uff09\u3002"}}
{"id": "2511.23017", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.23017", "abs": "https://arxiv.org/abs/2511.23017", "authors": ["Elham Ahmadi", "Alireza Olama", "Petri V\u00e4lisuo", "Heidi Kuusniemi"], "title": "Adaptive Factor Graph-Based Tightly Coupled GNSS/IMU Fusion for Robust Positionin", "comment": null, "summary": "Reliable positioning in GNSS-challenged environments remains a critical challenge for navigation systems. Tightly coupled GNSS/IMU fusion improves robustness but remains vulnerable to non-Gaussian noise and outliers. We present a robust and adaptive factor graph-based fusion framework that directly integrates GNSS pseudorange measurements with IMU preintegration factors and incorporates the Barron loss, a general robust loss function that unifies several m-estimators through a single tunable parameter. By adaptively down weighting unreliable GNSS measurements, our approach improves resilience positioning. The method is implemented in an extended GTSAM framework and evaluated on the UrbanNav dataset. The proposed solution reduces positioning errors by up to 41% relative to standard FGO, and achieves even larger improvements over extended Kalman filter (EKF) baselines in urban canyon environments. These results highlight the benefits of Barron loss in enhancing the resilience of GNSS/IMU-based navigation in urban and signal-compromised environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u5b50\u56fe\u7684\u7a33\u5065\u81ea\u9002\u5e94GNSS/IMU\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u52a0\u6743\u4e0d\u53ef\u9760\u7684GNSS\u6d4b\u91cf\uff0c\u63d0\u9ad8\u57ce\u5e02\u53ca\u4fe1\u53f7\u53d7\u635f\u73af\u5883\u4e0b\u7684\u5bfc\u822a\u97e7\u6027\u3002", "motivation": "\u5728GNSS\u4fe1\u53f7\u4e0d\u7a33\u5b9a\u7684\u73af\u5883\u4e2d\uff0c\u63d0\u9ad8\u5bfc\u822a\u7cfb\u7edf\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u6297\u5e72\u6270\u80fd\u529b\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u56e0\u5b50\u56fe\u7684\u878d\u5408\u6846\u67b6\uff0c\u5c06GNSS\u4f2a\u8ddd\u6d4b\u91cf\u4e0eIMU\u9884\u79ef\u5206\u56e0\u5b50\u76f8\u7ed3\u5408\uff0c\u4f7f\u7528Barron\u635f\u5931\u4f5c\u4e3a\u7a33\u5065\u635f\u5931\u51fd\u6570\u3002", "result": "\u76f8\u8f83\u4e8e\u6807\u51c6\u7684FGO\uff0c\u6240\u63d0\u65b9\u6cd5\u5c06\u5b9a\u4f4d\u8bef\u5dee\u51cf\u5c11\u4e86\u9ad8\u8fbe41%\uff0c\u5728\u57ce\u5e02\u5ce1\u8c37\u73af\u5883\u4e2d\u76f8\u8f83\u4e8e\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u57fa\u51c6\u4e5f\u6709\u66f4\u5927\u7684\u6539\u8fdb\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u57ce\u5e02\u5ce1\u8c37\u73af\u5883\u4e2d\u51cf\u5c11\u4e86\u9ad8\u8fbe41%\u7684\u5b9a\u4f4d\u8bef\u5dee\uff0c\u5c55\u793a\u4e86Barron\u635f\u5931\u5728\u589e\u5f3aGNSS/IMU\u5bfc\u822a\u7cfb\u7edf\u97e7\u6027\u65b9\u9762\u7684\u4f18\u52bf\u3002"}}
{"id": "2511.23030", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.23030", "abs": "https://arxiv.org/abs/2511.23030", "authors": ["Casimir Feldmann", "Maximum Wilder-Smith", "Vaishakh Patil", "Michael Oechsle", "Michael Niemeyer", "Keisuke Tateno", "Marco Hutter"], "title": "DiskChunGS: Large-Scale 3D Gaussian SLAM Through Chunk-Based Memory Management", "comment": null, "summary": "Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated impressive results for novel view synthesis with real-time rendering capabilities. However, integrating 3DGS with SLAM systems faces a fundamental scalability limitation: methods are constrained by GPU memory capacity, restricting reconstruction to small-scale environments. We present DiskChunGS, a scalable 3DGS SLAM system that overcomes this bottleneck through an out-of-core approach that partitions scenes into spatial chunks and maintains only active regions in GPU memory while storing inactive areas on disk. Our architecture integrates seamlessly with existing SLAM frameworks for pose estimation and loop closure, enabling globally consistent reconstruction at scale. We validate DiskChunGS on indoor scenes (Replica, TUM-RGBD), urban driving scenarios (KITTI), and resource-constrained Nvidia Jetson platforms. Our method uniquely completes all 11 KITTI sequences without memory failures while achieving superior visual quality, demonstrating that algorithmic innovation can overcome the memory constraints that have limited previous 3DGS SLAM methods.", "AI": {"tldr": "DiskChunGS \u662f\u4e00\u79cd\u65b0\u578b\u7684\u53ef\u6269\u5c553DGS SLAM\u7cfb\u7edf\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u51fa\u6838\u7b56\u7565\u7a81\u7834\u4e86GPU\u5185\u5b58\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u5728\u66f4\u5927\u573a\u666f\u4e2d\u7684\u4e00\u81f4\u6027\u91cd\u5efa\uff0c\u4e14\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u4f20\u7edf\u76843DGS SLAM\u7cfb\u7edf\u53d7\u9650\u4e8eGPU\u5185\u5b58\u5bb9\u91cf\uff0c\u4f7f\u5f97\u91cd\u5efa\u4ec5\u9650\u4e8e\u5c0f\u89c4\u6a21\u73af\u5883\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u514b\u670d\u8fd9\u4e00\u74f6\u9888\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u73af\u5883\u7684\u5b9e\u65f6\u91cd\u5efa\u3002", "method": "\u63d0\u51fa\u4e86DiskChunGS\uff0c\u4e00\u4e2a\u53ef\u6269\u5c55\u76843DGS SLAM\u7cfb\u7edf\uff0c\u91c7\u7528\u51fa\u6838\u65b9\u6cd5\uff0c\u5c06\u573a\u666f\u5212\u5206\u4e3a\u7a7a\u95f4\u5757\uff0c\u4ec5\u5728GPU\u5185\u5b58\u4e2d\u7ef4\u62a4\u6d3b\u52a8\u533a\u57df\uff0c\u5c06\u975e\u6d3b\u52a8\u533a\u57df\u5b58\u50a8\u5728\u78c1\u76d8\u4e0a\u3002", "result": "\u5728\u591a\u4e2a indoor\u573a\u666f\u548c\u57ce\u5e02\u9a7e\u9a76\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86DiskChunGS\u7684\u6709\u6548\u6027\uff0c\u72ec\u7279\u5730\u5b8c\u6210\u4e86\u6240\u670911\u4e2aKITTI\u5e8f\u5217\uff0c\u4e14\u6ca1\u6709\u5185\u5b58\u6545\u969c\uff0c\u540c\u65f6\u89c6\u89c9\u8d28\u91cf\u4f18\u8d8a\uff0c\u5c55\u793a\u4e86\u7b97\u6cd5\u521b\u65b0\u80fd\u591f\u514b\u670d\u4ee5\u524d3DGS SLAM\u65b9\u6cd5\u7684\u5185\u5b58\u9650\u5236\u3002", "conclusion": "DiskChunGS\u901a\u8fc7\u4e00\u4e2a\u51fa\u6838\u65b9\u6cd5\u89e3\u51b3\u4e86GPU\u5185\u5b58\u5bb9\u91cf\u9650\u5236\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5728\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u7684\u4e00\u81f4\u91cd\u5efa\uff0c\u540c\u65f6\u5728\u5185\u5b58\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.23034", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.23034", "abs": "https://arxiv.org/abs/2511.23034", "authors": ["Zuolei Li", "Xingyu Gao", "Xiaofan Wang", "Jianlong Fu"], "title": "LatBot: Distilling Universal Latent Actions for Vision-Language-Action Models", "comment": "Project Page: https://mm-robot.github.io/distill_latent_action/", "summary": "Learning transferable latent actions from large-scale object manipulation videos can significantly enhance generalization in downstream robotics tasks, as such representations are agnostic to different robot embodiments. Existing approaches primarily rely on visual reconstruction objectives while neglecting physical priors, leading to sub-optimal performance in learning universal representations. To address these challenges, we propose a Universal Latent Action Learning framework that takes task instructions and multiple frames as inputs, and optimizes both future frame reconstruction and action sequence prediction. Unlike prior works, incorporating action predictions (e.g., gripper or hand trajectories and orientations) allows the model to capture richer physical priors such as real-world distances and orientations, thereby enabling seamless transferability to downstream tasks. We further decompose the latent actions into learnable motion and scene tokens to distinguish the robot's active movements from environmental changes, thus filtering out irrelevant dynamics. By distilling the learned latent actions into the latest VLA models, we achieve strong performance across both simulated (SIMPLER and LIBERO) and real-world robot settings. Notably, with only 10 real-world trajectories per task collected on a Franka robot, our approach successfully completes all five challenging tasks, demonstrating strong few-shot transferability in robotic manipulation.", "AI": {"tldr": "\u8be5\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u5c11\u91cf\u793a\u4f8b\u4e0b\u7684\u64cd\u4f5c\u6cdb\u5316\u80fd\u529b\uff0c\u901a\u8fc7\u4f18\u5316\u672a\u6765\u91cd\u6784\u548c\u52a8\u4f5c\u9884\u6d4b\u6765\u5305\u542b\u7269\u7406\u5148\u9a8c\uff0c\u53d6\u5f97\u4e86\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u7684\u6210\u529f\u3002", "motivation": "\u4ece\u5927\u89c4\u6a21\u7269\u4f53\u64cd\u4f5c\u89c6\u9891\u4e2d\u5b66\u4e60\u53ef\u8f6c\u79fb\u7684\u6f5c\u5728\u52a8\u4f5c\uff0c\u4ee5\u63d0\u9ad8\u4e0b\u6e38\u673a\u5668\u4eba\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cdUniversal Latent Action Learning\u6846\u67b6\uff0c\u901a\u8fc7\u8f93\u5165\u4efb\u52a1\u6307\u4ee4\u548c\u591a\u4e2a\u5e27\uff0c\u4f18\u5316\u672a\u6765\u5e27\u91cd\u6784\u548c\u52a8\u4f5c\u5e8f\u5217\u9884\u6d4b\uff0c\u540c\u65f6\u5f15\u5165\u52a8\u4f5c\u9884\u6d4b\u4ee5\u589e\u5f3a\u7269\u7406\u5148\u9a8c\u7684\u5b66\u4e60\u3002", "result": "\u901a\u8fc7\u5c06\u5b66\u4e60\u5230\u7684\u6f5c\u5728\u52a8\u4f5c\u84b8\u998f\u5230\u6700\u65b0\u7684VLA\u6a21\u578b\u4e2d\uff0c\u5b9e\u73b0\u4e86\u5728\u4eff\u771f\uff08SIMPLER\u548cLIBERO\uff09\u548c\u771f\u5b9e\u673a\u5668\u4eba\u73af\u5883\u4e2d\u7684\u5f3a\u5927\u8868\u73b0\uff0c\u4e14\u4ec5\u752810\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u8f68\u8ff9\u5c31\u6210\u529f\u5b8c\u6210\u591a\u9879\u6311\u6218\u6027\u4efb\u52a1\u3002", "conclusion": "\u63d0\u51fa\u7684Universal Latent Action Learning\u6846\u67b6\u80fd\u591f\u5728\u5c11\u91cf\u6837\u672c\u7684\u60c5\u51b5\u4e0b\u6210\u529f\u5b8c\u6210\u590d\u6742\u7684\u673a\u5668\u4eba\u4efb\u52a1\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u64cd\u4f5c\u8f6c\u79fb\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.23143", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.23143", "abs": "https://arxiv.org/abs/2511.23143", "authors": ["Enrico Saccon", "Davide De Martini", "Matteo Saveriano", "Edoardo Lamon", "Luigi Palopoli", "Marco Roveri"], "title": "Automated Generation of MDPs Using Logic Programming and LLMs for Robotic Applications", "comment": "9 pages, 11 figures, 2 tables, 2 algorithms, accepted for publication in IEEE Robotics and Automation Letters", "summary": "We present a novel framework that integrates Large Language Models (LLMs) with automated planning and formal verification to streamline the creation and use of Markov Decision Processes (MDP). Our system leverages LLMs to extract structured knowledge in the form of a Prolog knowledge base from natural language (NL) descriptions. It then automatically constructs an MDP through reachability analysis, and synthesises optimal policies using the Storm model checker. The resulting policy is exported as a state-action table for execution. We validate the framework in three human-robot interaction scenarios, demonstrating its ability to produce executable policies with minimal manual effort. This work highlights the potential of combining language models with formal methods to enable more accessible and scalable probabilistic planning in robotics.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u81ea\u52a8\u89c4\u5212\u548c\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7ed3\u5408\uff0c\u7b80\u5316\u4e86\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u7684\u521b\u5efa\uff0c\u5c55\u793a\u4e86\u5728\u673a\u5668\u4eba\u89c4\u5212\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u65e8\u5728\u7b80\u5316\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u7684\u521b\u5efa\u548c\u4f7f\u7528\uff0c\u4ee5\u63d0\u9ad8\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u7b56\u7565\u751f\u6210\u6548\u7387\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u6846\u67b6\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4e2d\u7684\u7ed3\u6784\u5316\u77e5\u8bc6\uff0c\u81ea\u52a8\u6784\u5efa\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e76\u4f7f\u7528Storm\u6a21\u578b\u68c0\u67e5\u5668\u5408\u6210\u6700\u4f73\u7b56\u7565\u3002", "result": "\u901a\u8fc7\u5728\u4e09\u4e2a\u4eba\u673a\u4ea4\u4e92\u573a\u666f\u4e2d\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u80fd\u591f\u4ee5\u6700\u5c0f\u7684\u624b\u52a8\u52aa\u529b\u751f\u6210\u53ef\u6267\u884c\u7b56\u7565\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u5c06\u8bed\u8a00\u6a21\u578b\u4e0e\u5f62\u5f0f\u5316\u65b9\u6cd5\u7ed3\u5408\u8d77\u6765\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u6613\u4e8e\u8bbf\u95ee\u548c\u53ef\u6269\u5c55\u7684\u673a\u5668\u4eba\u6982\u7387\u89c4\u5212\u3002"}}
{"id": "2511.23186", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.23186", "abs": "https://arxiv.org/abs/2511.23186", "authors": ["Runyu Jiao", "Matteo Bortolon", "Francesco Giuliari", "Alice Fasoli", "Sergio Povoli", "Guofeng Mei", "Yiming Wang", "Fabio Poiesi"], "title": "Obstruction reasoning for robotic grasping", "comment": null, "summary": "Successful robotic grasping in cluttered environments not only requires a model to visually ground a target object but also to reason about obstructions that must be cleared beforehand. While current vision-language embodied reasoning models show emergent spatial understanding, they remain limited in terms of obstruction reasoning and accessibility planning. To bridge this gap, we present UNOGrasp, a learning-based vision-language model capable of performing visually-grounded obstruction reasoning to infer the sequence of actions needed to unobstruct the path and grasp the target object. We devise a novel multi-step reasoning process based on obstruction paths originated by the target object. We anchor each reasoning step with obstruction-aware visual cues to incentivize reasoning capability. UNOGrasp combines supervised and reinforcement finetuning through verifiable reasoning rewards. Moreover, we construct UNOBench, a large-scale dataset for both training and benchmarking, based on MetaGraspNetV2, with over 100k obstruction paths annotated by humans with obstruction ratios, contact points, and natural-language instructions. Extensive experiments and real-robot evaluations show that UNOGrasp significantly improves obstruction reasoning and grasp success across both synthetic and real-world environments, outperforming generalist and proprietary alternatives. Project website: https://tev-fbk.github.io/UnoGrasp/.", "AI": {"tldr": "UNOGrasp\u662f\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u969c\u788d\u63a8\u7406\u4f18\u5316\u6293\u53d6\u673a\u5236\uff0c\u663e\u8457\u6539\u5584\u6293\u53d6\u6210\u529f\u7387\uff0c\u5177\u5907\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b", "motivation": "\u63d0\u9ad8\u5728\u6742\u4e71\u73af\u5883\u4e2d\u673a\u5668\u4eba\u6293\u53d6\u80fd\u529b\uff0c\u89e3\u51b3\u5f53\u524d\u6a21\u578b\u5728\u969c\u788d\u63a8\u7406\u548c\u53ef\u8fbe\u6027\u89c4\u5212\u65b9\u9762\u7684\u5c40\u9650\u6027", "method": "\u63d0\u51faUNOGrasp\u6a21\u578b\uff0c\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00\u7406\u89e3\u548c\u963b\u788d\u63a8\u7406\u6765\u63a8\u65ad\u6293\u53d6\u7269\u4f53\u7684\u52a8\u4f5c\u5e8f\u5217", "result": "UNOGrasp\u5728\u5408\u6210\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u969c\u788d\u63a8\u7406\u548c\u6293\u53d6\u6210\u529f\u7387\u663e\u8457\u63d0\u5347\uff0c\u8d85\u8d8a\u4e86\u901a\u7528\u548c\u4e13\u6709\u7684\u9009\u62e9\u65b9\u6848", "conclusion": "UNOGrasp\u901a\u8fc7\u521b\u65b0\u7684\u591a\u6b65\u63a8\u7406\u8fc7\u7a0b\uff0c\u589e\u5f3a\u4e86\u6293\u53d6\u4efb\u52a1\u4e2d\u7684\u969c\u788d\u63a8\u7406\u80fd\u529b\uff0c\u662f\u6293\u53d6\u673a\u5668\u4eba\u5728\u6742\u4e71\u73af\u5883\u4e0b\u7684\u91cd\u8981\u8fdb\u5c55"}}
{"id": "2511.23193", "categories": ["cs.RO", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.23193", "abs": "https://arxiv.org/abs/2511.23193", "authors": ["Yuchen Shi", "Huaxin Pei", "Yi Zhang", "Danya Yao"], "title": "Fault-Tolerant MARL for CAVs under Observation Perturbations for Highway On-Ramp Merging", "comment": null, "summary": "Multi-Agent Reinforcement Learning (MARL) holds significant promise for enabling cooperative driving among Connected and Automated Vehicles (CAVs). However, its practical application is hindered by a critical limitation, i.e., insufficient fault tolerance against observational faults. Such faults, which appear as perturbations in the vehicles' perceived data, can substantially compromise the performance of MARL-based driving systems. Addressing this problem presents two primary challenges. One is to generate adversarial perturbations that effectively stress the policy during training, and the other is to equip vehicles with the capability to mitigate the impact of corrupted observations. To overcome the challenges, we propose a fault-tolerant MARL method for cooperative on-ramp vehicles incorporating two key agents. First, an adversarial fault injection agent is co-trained to generate perturbations that actively challenge and harden the vehicle policies. Second, we design a novel fault-tolerant vehicle agent equipped with a self-diagnosis capability, which leverages the inherent spatio-temporal correlations in vehicle state sequences to detect faults and reconstruct credible observations, thereby shielding the policy from misleading inputs. Experiments in a simulated highway merging scenario demonstrate that our method significantly outperforms baseline MARL approaches, achieving near-fault-free levels of safety and efficiency under various observation fault patterns.", "AI": {"tldr": "\u9488\u5bf9\u591a\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u5728\u8054\u7f51\u548c\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u5bb9\u9519\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u8bca\u65ad\u548c\u5bf9\u6297\u6027\u6545\u969c\u6ce8\u5165\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u5728\u89c2\u6d4b\u6545\u969c\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3MARL\u5728CAVs\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\u7531\u4e8e\u89c2\u5bdf\u6545\u969c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u7cfb\u7edf\u7684\u5bb9\u9519\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5bb9\u9519MARL\u65b9\u6cd5\uff0c\u7ed3\u5408\u5bf9\u6297\u6027\u6545\u969c\u6ce8\u5165\u4ee3\u7406\u548c\u5177\u5907\u81ea\u8bca\u65ad\u80fd\u529b\u7684\u8f66\u8f86\u4ee3\u7406\uff0c\u4ee5\u68c0\u6d4b\u6545\u969c\u5e76\u91cd\u5efa\u53ef\u4fe1\u89c2\u5bdf\u3002", "result": "\u5728\u6a21\u62df\u9ad8\u901f\u516c\u8def\u5408\u6d41\u573a\u666f\u4e2d\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9762\u5bf9\u4e0d\u540c\u7684\u89c2\u6d4b\u6545\u969c\u6a21\u5f0f\u65f6\uff0c\u5b89\u5168\u6027\u548c\u6548\u7387\u663e\u8457\u6539\u5584\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u9ad8\u901f\u516c\u8def\u5408\u6d41\u573a\u666f\u4e2d\u660e\u663e\u4f18\u4e8e\u57fa\u7ebfMARL\u65b9\u6cd5\uff0c\u786e\u4fdd\u4e86\u5728\u5404\u79cd\u89c2\u6d4b\u6545\u969c\u6a21\u5f0f\u4e0b\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u63a5\u8fd1\u65e0\u6545\u969c\u6c34\u5e73\u3002"}}
{"id": "2511.23215", "categories": ["cs.RO", "cond-mat.other", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2511.23215", "abs": "https://arxiv.org/abs/2511.23215", "authors": ["Eduardo Sergio Oliveros-Mata", "Oleksandr V. Pylypovskyi", "Eleonora Raimondo", "Rico Illing", "Yevhen Zabila", "Lin Guo", "Guannan Mu", "M\u00f3nica Navarro L\u00f3pez", "Xu Wang", "Georgios Tzortzinis", "Angelos Filippatos", "Gilbert Santiago Ca\u00f1\u00f3n Berm\u00fadez", "Francesca Garesc\u00ec", "Giovanni Finocchio", "Denys Makarov"], "title": "Field-programmable dynamics in a soft magnetic actuator enabling true random number generation and reservoir computing", "comment": null, "summary": "Complex and even chaotic dynamics, though prevalent in many natural and engineered systems, has been largely avoided in the design of electromechanical systems due to concerns about wear and controlability. Here, we demonstrate that complex dynamics might be particularly advantageous in soft robotics, offering new functionalities beyond motion not easily achievable with traditional actuation methods. We designed and realized resilient magnetic soft actuators capable of operating in a tunable dynamic regime for tens of thousands cycles without fatigue. We experimentally demonstrated the application of these actuators for true random number generation and stochastic computing. {W}e validate soft robots as physical reservoirs capable of performing Mackey--Glass time series prediction. These findings show that exploring the complex dynamics in soft robotics would extend the application scenarios in soft computing, human-robot interaction and collaborative robots as we demonstrate with biomimetic blinking and randomized voice modulation.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u590d\u6742\u52a8\u6001\u5728\u8f6f\u673a\u5668\u4eba\u4e2d\u7684\u4f18\u8d8a\u6027\uff0c\u8868\u660e\u5176\u80fd\u591f\u6269\u5c55\u8f6f\u8ba1\u7b97\u53ca\u4eba\u673a\u534f\u4f5c\u7684\u5e94\u7528\u3002", "motivation": "\u590d\u6742\u548c\u6df7\u6c8c\u52a8\u6001\u5728\u8bb8\u591a\u81ea\u7136\u548c\u5de5\u7a0b\u7cfb\u7edf\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u4f46\u5728\u7535\u673a\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u5e38\u88ab\u89c4\u907f\uff0c\u672c\u6587\u63a2\u8ba8\u4e86\u5176\u5728\u8f6f\u673a\u5668\u4eba\u4e2d\u7684\u6f5c\u5728\u4f18\u52bf\u3002", "method": "\u8bbe\u8ba1\u548c\u5b9e\u73b0\u4e86\u5177\u6709\u53ef\u8c03\u52a8\u6001\u8303\u56f4\u7684\u6297\u75b2\u52b3\u78c1\u6027\u8f6f\u6267\u884c\u5668\uff0c\u5e76\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u6210\u529f\u5730\u5f00\u53d1\u4e86\u53ef\u8fdb\u884c\u771f\u5b9e\u968f\u673a\u6570\u751f\u6210\u548c\u968f\u673a\u8ba1\u7b97\u7684\u8f6f\u6267\u884c\u5668\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u4f5c\u4e3a\u7269\u7406\u5e93\u7684\u80fd\u529b\u3002", "conclusion": "\u590d\u6742\u52a8\u6001\u5728\u8f6f\u673a\u5668\u4eba\u4e2d\u7684\u5e94\u7528\u5e7f\u6cdb\u4e14\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u4ee5\u6269\u5c55\u5176\u5728\u8f6f\u8ba1\u7b97\u3001\u4eba\u673a\u4ea4\u4e92\u548c\u534f\u4f5c\u673a\u5668\u4eba\u4e2d\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2511.23236", "categories": ["cs.RO", "cs.ET"], "pdf": "https://arxiv.org/pdf/2511.23236", "abs": "https://arxiv.org/abs/2511.23236", "authors": ["Alex Richardson", "Azhar Hasan", "Gabor Karsai", "Jonathan Sprinkle"], "title": "Incorporating Ephemeral Traffic Waves in A Data-Driven Framework for Microsimulation in CARLA", "comment": "Submitted to IEEE IV 2026", "summary": "This paper introduces a data-driven traffic microsimulation framework in CARLA that reconstructs real-world wave dynamics using high-fidelity time-space data from the I-24 MOTION testbed. Calibration of road networks in microsimulators to reproduce ephemeral phenomena such as traffic waves for large-scale simulation is a process that is fraught with challenges. This work reconsiders the existence of the traffic state data as boundary conditions on an ego vehicle moving through previously recorded traffic data, rather than reproducing those traffic phenomena in a calibrated microsim. Our approach is to autogenerate a 1 mile highway segment corresponding to I-24, and use the I-24 data to power a cosimulation module that injects traffic information into the simulation. The CARLA and cosimulation simulations are centered around an ego vehicle sampled from the empirical data, with autogeneration of \"visible\" traffic within the longitudinal range of the ego vehicle. Boundary control beyond these visible ranges is achieved using ghost cells behind (upstream) and ahead (downstream) of the ego vehicle. Unlike prior simulation work that focuses on local car-following behavior or abstract geometries, our framework targets full time-space diagram fidelity as the validation objective. Leveraging CARLA's rich sensor suite and configurable vehicle dynamics, we simulate wave formation and dissipation in both low-congestion and high-congestion scenarios for qualitative analysis. The resulting emergent behavior closely mirrors that of real traffic, providing a novel cosimulation framework for evaluating traffic control strategies, perception-driven autonomy, and future deployment of wave mitigation solutions. Our work bridges microscopic modeling with physical experimental data, enabling the first perceptually realistic, boundary-driven simulation of empirical traffic wave phenomena in CARLA.", "AI": {"tldr": "\u672c\u8bba\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u6570\u636e\u9a71\u52a8\u7684\u4ea4\u901a\u5fae\u89c2\u6a21\u62df\u6846\u67b6\uff0c\u80fd\u6709\u6548\u91cd\u5efa\u771f\u5b9e\u4ea4\u901a\u6ce2\u52a8\u6001\uff0c\u6865\u63a5\u5fae\u89c2\u5efa\u6a21\u4e0e\u7269\u7406\u5b9e\u9a8c\u6570\u636e\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5728\u5927\u89c4\u6a21\u6a21\u62df\u4e2d\uff0c\u5fae\u89c2\u6a21\u62df\u5668\u6821\u51c6\u4ee5\u518d\u73b0\u4ea4\u901a\u6ce2\u52a8\u7b49\u77ac\u6001\u73b0\u8c61\u6240\u9762\u4e34\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5728CARLA\u73af\u5883\u4e2d\u5b9e\u73b0\u5171\u6a21\u62df\u6a21\u5757\uff0c\u5229\u7528I-24 MOTION\u6d4b\u8bd5\u5e73\u53f0\u7684\u9ad8\u4fdd\u771f\u65f6\u7a7a\u6570\u636e\uff0c\u81ea\u4e3b\u751f\u62101\u82f1\u91cc\u9ad8\u901f\u516c\u8def\u6bb5\u6765\u6ce8\u5165\u4ea4\u901a\u4fe1\u606f\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u5728\u4f4e\u62e5\u5835\u548c\u9ad8\u62e5\u5835\u573a\u666f\u4e2d\u6210\u529f\u6a21\u62df\u4ea4\u901a\u6ce2\u7684\u5f62\u6210\u4e0e\u6d88\u6563\uff0c\u751f\u6210\u7684\u884c\u4e3a\u4e0e\u771f\u5b9e\u4ea4\u901a\u5bc6\u5207\u76f8\u4f3c\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6570\u636e\u9a71\u52a8\u7684\u4ea4\u901a\u5fae\u89c2\u6a21\u62df\u6846\u67b6\uff0c\u80fd\u591f\u5728CARLA\u4e2d\u771f\u5b9e\u91cd\u5efa\u4ea4\u901a\u6ce2\u52a8\u73b0\u8c61\uff0c\u4fc3\u8fdb\u4ea4\u901a\u63a7\u5236\u7b56\u7565\u548c\u81ea\u4e3b\u611f\u77e5\u7cfb\u7edf\u7684\u8bc4\u4f30\u3002"}}
{"id": "2511.23300", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.23300", "abs": "https://arxiv.org/abs/2511.23300", "authors": ["Yara Mahmoud", "Jeffrin Sam", "Nguyen Khang", "Marcelino Fernando", "Issatay Tokmurziyev", "Miguel Altamirano Cabrera", "Muhammad Haris Khan", "Artem Lykov", "Dzmitry Tsetserukou"], "title": "SafeHumanoid: VLM-RAG-driven Control of Upper Body Impedance for Humanoid Robot", "comment": null, "summary": "Safe and trustworthy Human Robot Interaction (HRI) requires robots not only to complete tasks but also to regulate impedance and speed according to scene context and human proximity. We present SafeHumanoid, an egocentric vision pipeline that links Vision Language Models (VLMs) with Retrieval-Augmented Generation (RAG) to schedule impedance and velocity parameters for a humanoid robot. Egocentric frames are processed by a structured VLM prompt, embedded and matched against a curated database of validated scenarios, and mapped to joint-level impedance commands via inverse kinematics. We evaluate the system on tabletop manipulation tasks with and without human presence, including wiping, object handovers, and liquid pouring. The results show that the pipeline adapts stiffness, damping, and speed profiles in a context-aware manner, maintaining task success while improving safety. Although current inference latency (up to 1.4 s) limits responsiveness in highly dynamic settings, SafeHumanoid demonstrates that semantic grounding of impedance control is a viable path toward safer, standard-compliant humanoid collaboration.", "AI": {"tldr": "SafeHumanoid\u7cfb\u7edf\u5229\u7528\u89c6\u89d2\u8bed\u8a00\u6a21\u578b\u548c\u589e\u5f3a\u68c0\u7d22\u751f\u6210\u6280\u672f\uff0c\u4f7f\u4eba\u5f62\u673a\u5668\u4eba\u80fd\u591f\u6839\u636e\u573a\u666f\u4e0a\u4e0b\u6587\u667a\u80fd\u8c03\u8282\u5de5\u4f5c\u53c2\u6570\uff0c\u589e\u5f3a\u4e86\u5b89\u5168\u6027\u548c\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u5728\u4e0e\u4eba\u7c7b\u4ea4\u4e92\u65f6\uff0c\u673a\u5668\u4eba\u9700\u8981\u6839\u636e\u573a\u666f\u4e0a\u4e0b\u6587\u548c\u4eba\u7c7b\u8ddd\u79bb\u8c03\u6574\u963b\u6297\u548c\u901f\u5ea6\uff0c\u4ee5\u5b9e\u73b0\u5b89\u5168\u548c\u53ef\u4fe1\u8d56\u7684\u4e92\u52a8\u3002", "method": "\u7ed3\u5408\u89c6\u89d2\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u548c\u589e\u5f3a\u68c0\u7d22\u751f\u6210\uff08RAG\uff09\u7684\u65b9\u6cd5\uff0c\u5904\u7406\u81ea\u6211\u89c6\u89d2\u56fe\u50cf\u5e27\u5e76\u751f\u6210\u5173\u8282\u7ea7\u963b\u6297\u6307\u4ee4\u3002", "result": "\u5728\u6709\u65e0\u4eba\u7684\u6761\u4ef6\u4e0b\u8fdb\u884c\u684c\u9762\u64cd\u4f5c\u4efb\u52a1\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u7cfb\u7edf\u80fd\u591f\u4ee5\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u65b9\u5f0f\u8c03\u6574\u521a\u5ea6\u3001\u963b\u5c3c\u548c\u901f\u5ea6\u7279\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u6210\u529f\u7387\uff0c\u63d0\u9ad8\u5b89\u5168\u6027\u3002", "conclusion": "SafeHumanoid\u7cfb\u7edf\u5c55\u793a\u4e86\u5c06\u8bed\u4e49\u63a7\u5236\u5e94\u7528\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u7684\u6709\u6548\u6027\uff0c\u589e\u5f3a\u4e86\u5b89\u5168\u6027\u548c\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5c3d\u7ba1\u5176\u54cd\u5e94\u901f\u5ea6\u5728\u52a8\u6001\u73af\u5883\u4e2d\u4ecd\u9700\u6539\u8fdb\u3002"}}
{"id": "2511.23372", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.23372", "abs": "https://arxiv.org/abs/2511.23372", "authors": ["Kanhaiya Lal Chaurasiya", "Ruchira Kumar Pradhan", "Yashaswi Sinha", "Shivam Gupta", "Ujjain Kumar Bidila", "Digambar Killedar", "Kapil Das Sahu", "Bishakh Bhattacharya"], "title": "Design, modelling and experimental validation of bipenniform shape memory alloy-based linear actuator integrable with hydraulic stroke amplification mechanism", "comment": null, "summary": "The increasing industrial demand for alternative actuators over conventional electromagnetism-based systems having limited efficiency, bulky size, complex design due to in-built gear-train mechanisms, and high production and amortization costs necessitates the innovation in new actuator development. Integrating bio-inspired design principles into linear actuators could bring forth the next generation of adaptive and energy efficient smart material-based actuation systems. The present study amalgamates the advantages of bipenniform architecture, which generates high force in the given physiological region and a high power-to-weight ratio of shape memory alloy (SMA), into a novel bio-inspired SMA-based linear actuator. A mathematical model of a multi-layered bipenniform configuration-based SMA actuator was developed and validated experimentally. The current research also caters to the incorporation of failure mitigation strategies using design failure mode and effects analysis along with the experimental assessment of the performance of the developed actuator. The system has been benchmarked against an industry-developed stepper motor-driven actuator. It has shown promising results generating an actuation force of 257 N with 15 V input voltage, meeting the acceptable range for actuation operation. It further exhibits about 67% reduction in the weight of the drive mechanism, with 80% lesser component, 32% cost reduction, and 19% energy savings and similar envelope dimensions for assembly compatibility with dampers and louvers for easy onsite deployment. The study introduces SMA coil-based actuator as an advanced design that can be deployed for high force-high stroke applications. The bio-inspired SMA-based linear actuator has applications ranging from building automation controls to lightweight actuation systems for space robotics and medical prosthesis.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u7269\u542f\u53d1\u8bbe\u8ba1\u7684SMA\u7ebf\u6027\u9a71\u52a8\u5668\uff0c\u5177\u6709\u9ad8\u6548\u80fd\u3001\u8f7b\u91cf\u5316\u548c\u6210\u672c\u6548\u76ca\uff0c\u53ef\u7528\u4e8e\u591a\u79cd\u5e94\u7528\uff0c\u663e\u793a\u51fa\u4f18\u8d8a\u7684\u529b\u5b66\u6027\u80fd\u548c\u80fd\u6e90\u8282\u7ea6\u3002", "motivation": "\u5de5\u4e1a\u5bf9\u66ff\u4ee3\u4f20\u7edf\u7535\u78c1\u9a71\u52a8\u7cfb\u7edf\u7684\u9700\u6c42\u4e0d\u65ad\u589e\u957f\uff0c\u56e0\u6b64\u4e9f\u9700\u5f00\u53d1\u66f4\u9ad8\u6548\u3001\u4f53\u79ef\u66f4\u5c0f\u3001\u8bbe\u8ba1\u66f4\u7b80\u7ea6\u7684\u9a71\u52a8\u5668\u3002", "method": "\u672c\u7814\u7a76\u7ed3\u5408\u4e86\u53cc\u7fbd\u578b\u67b6\u6784\u548c\u5f62\u72b6\u8bb0\u5fc6\u5408\u91d1\u7684\u4f18\u70b9\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u751f\u7269\u542f\u53d1SMA\u7ebf\u6027\u9a71\u52a8\u5668\uff0c\u5e76\u5efa\u7acb\u4e86\u591a\u5c42\u53cc\u7fbd\u578b\u914d\u7f6e\u7684\u6570\u5b66\u6a21\u578b\uff0c\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u8be5\u9a71\u52a8\u5668\u572815V\u7535\u538b\u4e0b\u4ea7\u751f\u4e86257N\u7684\u9a71\u52a8\u529b\uff0c\u663e\u793a\u51fa67%\u7684\u91cd\u91cf\u51cf\u5c11\uff0c80%\u7684\u7ec4\u4ef6\u51cf\u5c11\uff0c32%\u7684\u6210\u672c\u964d\u4f4e\u548c19%\u7684\u80fd\u6e90\u8282\u7ea6\uff0c\u4e14\u5728\u7ec4\u88c5\u5c3a\u5bf8\u4e0a\u517c\u5bb9\u51cf\u9707\u5668\u548c\u767e\u53f6\u7a97\uff0c\u4fbf\u4e8e\u73b0\u573a\u90e8\u7f72\u3002", "conclusion": "\u751f\u7269\u542f\u53d1\u7684\u5f62\u72b6\u8bb0\u5fc6\u5408\u91d1\uff08SMA\uff09\u7ebf\u6027\u9a71\u52a8\u5668\u5728\u9ad8\u529b-\u9ad8\u884c\u7a0b\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u5177\u6709\u5e7f\u9614\u7684\u5e94\u7528\u524d\u666f\uff0c\u5305\u62ec\u5efa\u7b51\u81ea\u52a8\u5316\u3001\u8f7b\u91cf\u5316\u9a71\u52a8\u7cfb\u7edf\u3001\u592a\u7a7a\u673a\u5668\u4eba\u53ca\u533b\u7597\u5047\u80a2\u7b49\u9886\u57df\u3002"}}
{"id": "2511.23407", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.23407", "abs": "https://arxiv.org/abs/2511.23407", "authors": ["Jan Baumg\u00e4rtner", "Malte Hansjosten", "David Hald", "Adrian Hauptmannl", "Alexander Puchta", "J\u00fcrgen Fleischer"], "title": "From CAD to POMDP: Probabilistic Planning for Robotic Disassembly of End-of-Life Products", "comment": null, "summary": "To support the circular economy, robotic systems must not only assemble new products but also disassemble end-of-life (EOL) ones for reuse, recycling, or safe disposal. Existing approaches to disassembly sequence planning often assume deterministic and fully observable product models, yet real EOL products frequently deviate from their initial designs due to wear, corrosion, or undocumented repairs. We argue that disassembly should therefore be formulated as a Partially Observable Markov Decision Process (POMDP), which naturally captures uncertainty about the product's internal state. We present a mathematical formulation of disassembly as a POMDP, in which hidden variables represent uncertain structural or physical properties. Building on this formulation, we propose a task and motion planning framework that automatically derives specific POMDP models from CAD data, robot capabilities, and inspection results. To obtain tractable policies, we approximate this formulation with a reinforcement-learning approach that operates on stochastic action outcomes informed by inspection priors, while a Bayesian filter continuously maintains beliefs over latent EOL conditions during execution. Using three products on two robotic systems, we demonstrate that this probabilistic planning framework outperforms deterministic baselines in terms of average disassembly time and variance, generalizes across different robot setups, and successfully adapts to deviations from the CAD model, such as missing or stuck parts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8ePOMDP\u7684\u62c6\u89e3\u5e8f\u5217\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u8d1d\u53f6\u65af\u6ee4\u6ce2\u5668\u5e94\u5bf9EOL\u4ea7\u54c1\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u663e\u8457\u6539\u5584\u62c6\u89e3\u6548\u7387\u4e0e\u9002\u5e94\u6027\u3002", "motivation": "\u652f\u6301\u5faa\u73af\u7ecf\u6d4e\uff0c\u4e0d\u4ec5\u9700\u8981\u673a\u5668\u4eba\u7cfb\u7edf\u7ec4\u88c5\u65b0\u4ea7\u54c1\uff0c\u8fd8\u9700\u62c6\u89e3\u751f\u547d\u5468\u671f\u7ed3\u675f\u7684\u4ea7\u54c1\uff0c\u4ee5\u4fbf\u91cd\u65b0\u4f7f\u7528\u3001\u56de\u6536\u6216\u5b89\u5168\u5904\u7f6e\u3002", "method": "\u5c06\u62c6\u89e3\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u90e8\u5206\u53ef\u89c2\u5bdf\u7684\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08POMDP\uff09\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6765\u83b7\u5f97\u53ef\u5904\u7406\u7684\u7b56\u7565\uff0c\u540c\u65f6\u4f7f\u7528\u8d1d\u53f6\u65af\u6ee4\u6ce2\u5668\u5728\u6267\u884c\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u5bf9\u6f5c\u5728EOL\u6761\u4ef6\u7684\u4fe1\u5ff5\u3002", "result": "\u901a\u8fc7\u4e09\u79cd\u4ea7\u54c1\u5728\u4e24\u79cd\u673a\u5668\u4eba\u7cfb\u7edf\u4e0a\u7684\u5b9e\u9a8c\uff0c\u6f14\u793a\u8be5\u6846\u67b6\u5728\u5904\u7406\u5e73\u5747\u62c6\u89e3\u65f6\u95f4\u3001\u65b9\u5dee\u4e0e\u9002\u5e94CAD\u6a21\u578b\u504f\u5dee\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u6982\u7387\u89c4\u5212\u6846\u67b6\u5728\u5e73\u5747\u62c6\u89e3\u65f6\u95f4\u548c\u65b9\u5dee\u65b9\u9762\u4f18\u4e8e\u786e\u5b9a\u6027\u57fa\u7ebf\uff0c\u4e14\u80fd\u591f\u9002\u5e94CAD\u6a21\u578b\u7684\u504f\u5dee\uff0c\u5982\u7f3a\u5931\u6216\u5361\u4f4f\u7684\u90e8\u4ef6\u3002"}}
