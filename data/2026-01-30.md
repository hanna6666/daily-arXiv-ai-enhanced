<div id=toc></div>

# Table of Contents

- [cs.HC](#cs.HC) [Total: 19]
- [cs.RO](#cs.RO) [Total: 36]


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [1] [Designing the Interactive Memory Archive (IMA): A Socio-Technical Framework for AI-Mediated Reminiscence and Cultural Memory Preservation](https://arxiv.org/abs/2601.21001)
*Ron Fulbright*

Main category: cs.HC

TL;DR: 该论文介绍了一种名为互动记忆档案（IMA）的框架，旨在通过AI支持老年人的回忆互动，帮助他们应对记忆丧失。


<details>
  <summary>Details</summary>
Motivation: 随着人口老龄化，越来越多的老年人面临记忆退化问题，因此需要新的工具和方法来促进他们的认知参与。

Method: IMA框架结合多模态传感、自然语言对话支撑和基于云的存档，呈现为一种大型历史图册的形式。

Result: 论文定位IMA为理论贡献，提出可测试的假设，并为未来的实证、技术和伦理探索制定了研究议程。

Conclusion: 该论文提出的互动记忆档案（IMA）为辅助老年人记忆机制提供了一个创新的框架，并为未来的研究指明了方向。

Abstract: This paper introduces the Interactive Memory Archive (IMA), a conceptual framework for AI-mediated reminiscence designed to support cognitive en-gagement among older adults experiencing memory loss. IMA integrates multimodal sensing, natural language conversational scaffolding, and cloud-based archiving within the familiar form of a large format historical picture book. The model theorizes reminiscence as a guided, context-aware interaction eliciting autobiographical memories and preserving them as cul-tural artifacts. The paper positions IMA as a theoretical contribution, articu-lates testable propositions, and outlines a research agenda for future empiri-cal, technical, and ethical inquiry.

</details>


### [2] [Log2Motion: Biomechanical Motion Synthesis from Touch Logs](https://arxiv.org/abs/2601.21043)
*Michał Patryk Miazga,Hannah Bussmann,Antti Oulasvirta,Patrick Ebel*

Main category: cs.HC

TL;DR: 提出了一种基于强化学习的新方法，通过Log2Motion从触摸日志合成生物力学运动，为理解触摸交互提供新见解。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模收集了移动设备的触摸数据，但对其产生的交互却知之甚少，需通过新的计算问题来缩小这一差距。

Method: 强化学习驱动的肌肉骨骼前向仿真，结合软件仿真器与物理仿真器，生成符合触摸日志事件的生物力学运动序列。

Result: 开发了Log2Motion系统，能够从触摸日志中合成用户运动的丰富合成数据，包括运动、速度、准确性和努力等估计值。

Conclusion: Log2Motion为理解日志数据提供了新的视角，揭示了触摸交互背后的生物力学和运动控制。

Abstract: Touch data from mobile devices are collected at scale but reveal little about the interactions that produce them. While biomechanical simulations can illuminate motor control processes, they have not yet been developed for touch interactions. To close this gap, we propose a novel computational problem: synthesizing plausible motion directly from logs. Our key insight is a reinforcement learning-driven musculoskeletal forward simulation that generates biomechanically plausible motion sequences consistent with events recorded in touch logs. We achieve this by integrating a software emulator into a physics simulator, allowing biomechanical models to manipulate real applications in real-time. Log2Motion produces rich syntheses of user movements from touch logs, including estimates of motion, speed, accuracy, and effort. We assess the plausibility of generated movements by comparing against human data from a motion capture study and prior findings, and demonstrate Log2Motion in a large-scale dataset. Biomechanical motion synthesis provides a new way to understand log data, illuminating the ergonomics and motor control underlying touch interactions.

</details>


### [3] [Eye Feel You: A DenseNet-driven User State Prediction Approach](https://arxiv.org/abs/2601.21045)
*Kamrul Hasan,Oleg V. Komogortsev*

Main category: cs.HC

TL;DR: 利用DenseNet模型通过目光动力学数据准确预测主观状态，减少对自我报告的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有的主观自我报告收集成本高且解释难度大，急需一种客观的预测方法。

Method: 将主观报告预测问题视为监督回归问题，提出DenseNet深度学习回归器，并进行跨轮次和跨个体的泛化实验。

Result: 本文提出了一种基于DenseNet的深度学习回归器，通过目光动力学数据预测主观自我报告的感知状态，如疲劳、努力和任务难度。

Conclusion: 通过两项实验验证了模型在不同时间段及不同个体上的泛化能力，表明目光动态能够可靠地预测主观感受。

Abstract: Subjective self-reports, collected with eye-tracking data, reveal perceived states like fatigue, effort, and task difficulty. However, these reports are costly to collect and challenging to interpret consistently in longitudinal studies. In this work, we focus on determining whether objective gaze dynamics can reliably predict subjective reports across repeated recording rounds in the eye-tracking dataset. We formulate subjective-report prediction as a supervised regression problem and propose a DenseNet-based deep learning regressor that learns predictive representations from gaze velocity signals. We conduct two complementary experiments to clarify our aims. First, the cross-round generalization experiment tests whether models trained on earlier rounds transfer to later rounds, evaluating the models' ability to capture longitudinal changes. Second, cross-subject generalization tests models' robustness by predicting subjective outcomes for new individuals. These experiments aim to reduce reliance on hand-crafted feature designs and clarify which states of subjective experience systematically appear in oculomotor behavior over time.

</details>


### [4] [Privatization of Synthetic Gaze: Attenuating State Signatures in Diffusion-Generated Eye Movements](https://arxiv.org/abs/2601.21057)
*Kamrul Hasan,Oleg V. Komogortsev*

Main category: cs.HC

TL;DR: 本研究探讨了一种扩散式眼动合成方法，发现其生成的眼动数据与主观状态的相关性微弱，表明有效抑制了与状态相关的私密属性，并保留了必要的信号特性。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习的成功，生成高质量合成眼动数据成为可能，但这些数据引发了隐私问题。本研究旨在探讨如何在产生眼动数据时保留信号质量同时去除状态相关的敏感属性。

Method: 本研究采用了基于扩散的眼动合成方法，分析合成眼动特征与主观报告（如疲劳）之间的相关性。

Result: 本研究表明，所采用的扩散式眼动合成方法使得合成的眼动数据与主观报告（如疲劳）之间的相关性微不足道，表明该方法有效抑制了与状态相关的特征。同时，合成眼动数据保留了与真实数据相似的必要信号特性，支持其在隐私保护的眼动应用中的使用。

Conclusion: 该研究表明，扩散式眼动合成有效抑制了个体状态相关的特征，同时保留了与真实数据相似的信号特性，适合用于隐私保护的应用。

Abstract: The recent success of deep learning (DL) has enabled the generation of high-quality synthetic gaze data. However, such data also raises privacy concerns because gaze sequences can encode subjects' internal states, like fatigue, emotional load, or stress. Ideally, synthetic gaze should preserve the signal quality of real recordings and remove or attenuate state-related, privacy-sensitive attributes. Many recent DL-based generative models focus on replicating real gaze trajectories and do not explicitly consider subjective reports or the privatization of internal states. However, in this work, we consider a recent diffusion-based gaze synthesis approach and examine correlations between synthetic gaze features and subjective reports (e.g., fatigue and related self-reported states). Our result shows that these correlations are trivial, which suggests the generative approach suppresses state-related features. Moreover, synthetic gaze preserves necessary signal characteristics similar to those of real data, which supports its use for privacy-preserving gaze-based applications.

</details>


### [5] [Optimization and Mobile Deployment for Anthropocene Neural Style Transfer](https://arxiv.org/abs/2601.21141)
*Po-Hsun Chen,Ivan C. H. Liu*

Main category: cs.HC

TL;DR: 本文介绍了AnthropoCam，一个专为人类世环境的视觉合成优化的移动神经风格迁移系统，探讨了其参数配置对视觉转换的影响，并在移动设备上实现了低延迟的实时处理。


<details>
  <summary>Details</summary>
Motivation: 为了应对人类世景观的复杂性，提出了一种集成具体领域优化的神经风格迁移方法，以便在移动设备上进行实时的视觉干预。

Method: 通过系统地研究NST参数配置（如特征层选择、损失权重、训练稳定性等），开展了控制实验，识别了最大化风格表达的最佳参数组合，并实现了低延迟的前馈管道。

Result: 通过合适的卷积深度、损失比例和分辨率缩放的组合，成功实现了人类对材料特性的保真转换，并在一般移动硬件上实现了每3-5秒的高分辨率推断。

Conclusion: AnthropoCam是一个有效的移动神经风格迁移系统，能够在真实时间内实现对人类改造环境的视觉合成，促进对人类世景观的参与性互动。

Abstract: This paper presents AnthropoCam, a mobile-based neural style transfer (NST) system optimized for the visual synthesis of Anthropocene environments. Unlike conventional artistic NST, which prioritizes painterly abstraction, stylizing human-altered landscapes demands a careful balance between amplifying material textures and preserving semantic legibility. Industrial infrastructures, waste accumulations, and modified ecosystems contain dense, repetitive patterns that are visually expressive yet highly susceptible to semantic erosion under aggressive style transfer.
  To address this challenge, we systematically investigate the impact of NST parameter configurations on the visual translation of Anthropocene textures, including feature layer selection, style and content loss weighting, training stability, and output resolution. Through controlled experiments, we identify an optimal parameter manifold that maximizes stylistic expression while preventing semantic erasure. Our results demonstrate that appropriate combinations of convolutional depth, loss ratios, and resolution scaling enable the faithful transformation of anthropogenic material properties into a coherent visual language.
  Building on these findings, we implement a low-latency, feed-forward NST pipeline deployed on mobile devices. The system integrates a React Native frontend with a Flask-based GPU backend, achieving high-resolution inference within 3-5 seconds on general mobile hardware. This enables real-time, in-situ visual intervention at the site of image capture, supporting participatory engagement with Anthropocene landscapes.
  By coupling domain-specific NST optimization with mobile deployment, AnthropoCam reframes neural style transfer as a practical and expressive tool for real-time environmental visualization in the Anthropocene.

</details>


### [6] [Evaluating Spatialized Auditory Cues for Rapid Attention Capture in XR](https://arxiv.org/abs/2601.21264)
*Yoonsang Kim,Swapnil Dey,Arie Kaufman*

Main category: cs.HC

TL;DR: 本研究探讨了在时间紧迫的XR场景中，短时间的空间音频定位对用户的引导效果，并提出了设计建议。


<details>
  <summary>Details</summary>
Motivation: 在时间紧迫的XR场景中，用户需要迅速转移注意力，空间音频可以作为一种有效的提示方式。

Method: 通过HRTF渲染的宽带刺激，从多个方向向听众呈现，量化用户对短暂音频的方向感知能力，并测试短期视听反馈训练的效果。

Result: 研究发现，简短的空间音频线索可以传达粗略的方向信息，短期校准可以提高用户对声音信号的感知。

Conclusion: 空间音频可以快速引导注意力，但仅依赖听觉信号的精度不足，需结合其他感官信息。

Abstract: In time-critical eXtended reality (XR) scenarios where users must rapidly reorient their attention to hazards, alerts, or instructions while engaged in a primary task, spatial audio can provide an immediate directional cue without occupying visual bandwidth. However, such scenarios can afford only a brief auditory exposure, requiring users to interpret sound direction quickly and without extended listening or head-driven refinement. This paper reports a controlled exploratory study of rapid spatial-audio localization in XR. Using HRTF-rendered broadband stimuli presented from a semi-dense set of directions around the listener, we quantify how accurately users can infer coarse direction from brief audio alone. We further examine the effects of short-term visuo-auditory feedback training as a lightweight calibration mechanism. Our findings show that brief spatial cues can convey coarse directional information, and that even short calibration can improve users' perception of aural signals. While these results highlight the potential of spatial audio for rapid attention guidance, they also show that auditory cues alone may not provide sufficient precision for complex or high-stakes tasks, and that spatial audio may be most effective when complemented by other sensory modalities or visual cues, without relying on head-driven refinement. We leverage this study on spatial audio as a preliminary investigation into a first-stage attention-guidance channel for wearable XR (e.g., VR head-mounted displays and AR smart glasses), and provide design insights on stimulus selection and calibration for time-critical use.

</details>


### [7] [Envisioning Audio Augmented Reality in Everyday Life](https://arxiv.org/abs/2601.21271)
*Tram Thi Minh Tran,Soojeong Yoo,Oliver Weidlich,Yidan Cao,Xinyan Yu,Xin Cheng,Yin Ye,Natalia Gulbransen-Diaz,Callum Parker*

Main category: cs.HC

TL;DR: 研究探讨了音频增强现实(AAR)在日常生活中的潜力，识别了十种角色，并强调了上下文感知设计的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉增强技术已主导增强现实领域，但音频增强现实(AAR)的潜力尚未得到充分探索。本研究旨在填补这一空白。

Method: 通过协作自我民族志(N=5, 作者)和在线调查(N=74)的方法进行研究。

Result: 识别了AAR的十种角色，这些角色分为任务和效用导向、情感与社会、感知协作者三类，并映射到日常生活的微、中、宏节律。

Conclusion: 本文为日常生活中的音频增强现实(AAR)提供了基础性和前瞻性的框架，并为适应日常生活、感官参与和社会期望的系统设计奠定了基础。

Abstract: While visual augmentation dominates the augmented reality landscape, devices like Meta Ray-Ban audio smart glasses signal growing industry movement toward audio augmented reality (AAR). Hearing is a primary channel for sensing context, anticipating change, and navigating social space, yet AAR's everyday potential remains underexplored. We address this gap through a collaborative autoethnography (N=5, authoring) and an online survey (N=74). We identify ten roles for AAR, grouped into three categories: task- and utility-oriented, emotional and social, and perceptual collaborator. These roles are further layered with a rhythmic and embodied collaborator framing, mapping them onto micro-, meso-, and macro-rhythms of everyday life. Our analysis surfaces nuanced tensions, such as blocking distractions without erasing social presence, highlighting the need for context-aware design. This paper contributes a foundational and forward-looking framework for AAR in everyday life, providing design groundwork for systems attuned to daily routines, sensory engagement, and social expectations.

</details>


### [8] [Tell Me What I Missed: Tell Me What I Missed: Interacting with GPT during Recalling of One-Time Witnessed Events](https://arxiv.org/abs/2601.21460)
*Suifang Zhou,Qi Gong,Ximing Shen,RAY LC*

Main category: cs.HC

TL;DR: 研究表明，使用GPT辅助的技术可以影响记忆回忆的信任度和策略选择。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM辅助技术在记忆回忆中的作用及其使用者互动方式。

Method: 参与者观看短视频并使用默认GPT或引导GPT构建回忆陈述。

Result: 发现默认条件下，参与者对事件理解的主观信心影响他们对GPT输出的信任；而在引导条件下，主观清晰度与实际回忆之间的对齐更强，同时不同条件下参与者对事件的评价也有所不同。

Conclusion: GPT与用户的互动可能会潜在地影响他们对记忆事件的信念和感知。

Abstract: LLM-assisted technologies are increasingly used to support cognitive processing and information interpretation, yet their role in aiding memory recall, and how people choose to engage with them, remains underexplored. We studied participants who watched a short robbery video (approximating a one-time eyewitness scenario) and composed recall statements using either a default GPT or a guided GPT prompted with a standardized eyewitness protocol. Results show that, in the default condition, participants who believed they had a clearer understanding of the event were more likely to trust GPT's output, whereas in the guided condition, participants showed stronger alignment between subjective clarity and actual recall. Additionally, participants evaluated the legitimacy of the individuals in the incident differently across conditions. Interaction analysis further revealed that default-GPT users spontaneously developed diverse strategies, including building on existing recollections, requesting potentially missing details, and treating GPT as a recall coach. This work shows how GPT-user interplay can subconsciously shape beliefs and perceptions of remembered events.

</details>


### [9] [Are they just delegating? Cross-Sample Predictions on University Students' & Teachers' Use of AI](https://arxiv.org/abs/2601.21490)
*Fabian Albers,Sebastian Strauß,Nikol Rummel,Nils Köbis*

Main category: cs.HC

TL;DR: 研究发现，教师和学生在使用生成性人工智能方面存在认知差距，学生报告的使用频率和委托程度高于教师，双方过高估计了对方的AI使用情况。


<details>
  <summary>Details</summary>
Motivation: 在高等教育中，教师与学生之间的相互信任是有效教学、学习和评估的先决条件。

Method: 本研究对113名德国大学教师和123名学生进行了调查，涵盖六个相同的学术任务，考察AI使用的频率及委托程度。

Result: 结果显示，学生的AI使用报告和委托程度高于教师，双方均显著高估了对方的AI使用情况。

Conclusion: 教师和学生之间对人工智能(AI)使用的认知差距可能会阻碍信任和有效合作，因此需要在学术界进行开放的对话，并制定支持公平透明整合AI工具的政策。

Abstract: Mutual trust between teachers and students is a prerequisite for effective teaching, learning, and assessment in higher education. Accurate predictions about the other group's use of generative artificial intelligence (AI) are fundamental for such trust. However, the disruptive rise of AI has transformed academic work practices, raising important questions about how teachers and students use these tools and how well they can estimate each other's usage. While the frequency of use is well studied, little is known about how AI is used, and comparisons with similar practices are rare. This study surveyed German university teachers (N = 113) and students (N = 123) on the frequency of AI use and the degree of delegation across six identical academic tasks. Participants also provided incentivized cross-sample predictions of the other group's AI use to assess the accuracy of their predictions. We find that students reported higher use of AI and greater delegation than teachers. Both groups significantly overestimated the other group's use, with teachers predicting very frequent use and high delegation by students, and students assuming teachers use AI similarly to themselves. These findings reveal a perception gap between teachers' and students' expectations and actual AI use. Such gaps may hinder trust and effective collaboration, underscoring the need for open dialogue about AI practices in academia and for policies that support the equitable and transparent integration of AI tools in higher education.

</details>


### [10] [Organizational Practices and Socio-Technical Design of Human-Centered AI](https://arxiv.org/abs/2601.21492)
*Thomas Herrmann*

Main category: cs.HC

TL;DR: 本文探讨了如何通过社会技术视角有效整合AI，以支持人中心的人工智能，强调组织学习与AI输出的共同解释和工作流程的适应性。


<details>
  <summary>Details</summary>
Motivation: 探讨如何从人本视角有效整合AI以满足HCAI的需求。

Method: 通过案例分析探讨AI在组织实践中的社会技术整合。

Result: 展示了AI支持预测维护的十种案例模式，强调了其在质量保障与持续改进中的组织作用。

Conclusion: AI的采用在组织中常常需要并促进新形式的组织学习，以支持人中心的人工智能(HCAI)。

Abstract: This contribution explores how the integration of Artificial Intelligence (AI) into organizational practices can be effectively framed through a socio-technical perspective to comply with the requirements of Human-centered AI (HCAI). Instead of viewing AI merely as a technical tool, the analysis emphasizes the importance of embedding AI into communication, collaboration, and decision-making processes within organizations from a human-centered perspective. Ten case-based patterns illustrate how AI support of predictive maintenance can be organized to address quality assurance and continuous improvement and to provide different types of sup-port for HCAI. The analysis shows that AI adoption often requires and enables new forms of organizational learning, where specialists jointly interpret AI output, adapt workflows, and refine rules for system improve-ment. Different dimensions and levels of socio-technical integration of AI are considered to reflect the effort and benefits of keeping the organization in the loop.

</details>


### [11] [From Vulnerable to Resilient: Examining Parent and Teen Perceptions on How to Respond to Unwanted Cybergrooming Advances](https://arxiv.org/abs/2601.21518)
*Xinyi Zhang,Mamtaj Akter,Heajun An,Minqian Liu,Qi Zhang,Lifu Huang,Jin-Hee Cho,Pamela J. Wisniewski,Sang Won Lee*

Main category: cs.HC

TL;DR: 本研究关注青少年在网络 grooming 情境下的反应，识别了脆弱反应及保护策略，并提出相应的教育和干预建议。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在填补以往研究中对青少年在面对 cybergrooming 时反应的理解空白，以帮助实施有效的教育与干预。

Method: 通过对74名参与者的在线调查，模拟 cybergrooming 情景，并分析他们的回应，采用混合方法的分析方式。

Result: 该研究通过调查发现青少年对网络 grooming 的反应，识别出脆弱反应与保护策略的不同类型，并提出了青少年在面对网络 grooming 时的应对机制，及教育和干预的建议。

Conclusion: 该研究为青少年如何应对网络 grooming 提供了一种更为集中视角的理解，并形成了标签数据集与分阶段保护策略分类，以促进教育项目和社会技术干预。

Abstract: Cybergrooming is a form of online abuse that threatens teens' mental health and physical safety. Yet, most prior work has focused on detecting perpetrators' behaviors, leaving a limited understanding of how teens might respond to such unwanted advances. To address this gap, we conducted an online survey with 74 participants -- 51 parents and 23 teens -- who responded to simulated cybergrooming scenarios in two ways: responses that they think would make teens more vulnerable or resilient to unwanted sexual advances. Through a mixed-methods analysis, we identified four types of vulnerable responses (encouraging escalation, accepting an advance, displaying vulnerability, and negating risk concern) and four types of protective strategies (setting boundaries, directly declining, signaling risk awareness, and leveraging avoidance techniques). As the cybergrooming risk escalated, both vulnerable responses and protective strategies showed a corresponding progression. This study contributes a teen-centered understanding of cybergrooming, a labeled dataset, and a stage-based taxonomy of perceived protective strategies, while offering implications for educational programs and sociotechnical interventions.

</details>


### [12] [When Life Gives You AI, Will You Turn It Into A Market for Lemons? Understanding How Information Asymmetries About AI System Capabilities Affect Market Outcomes and Adoption](https://arxiv.org/abs/2601.21650)
*Alexander Erlei,Federico Cau,Radoslav Georgiev,Sagar Kumar,Kilian Bizer,Ujwal Gadiraju*

Main category: cs.HC

TL;DR: 该研究探讨了信息不对称及信息披露设计对AI系统采用的影响，发现部分信息披露有助于改善人类决策效率。


<details>
  <summary>Details</summary>
Motivation: AI消费市场的买卖双方存在显著的信息不对称，尽管已有监管措施，但信息缺口依然存在。

Method: 通过系统变化低质量AI系统的密度和信息披露深度，在模拟AI产品市场中进行实验，观察参与者对低质量AI系统的反应。

Result: 参与者的选择与理性贝叶斯模型的比较显示，部分信息披露可以提高AI系统的采用率。

Conclusion: 信息不对称对AI的采用有负面影响，但缺乏透明度的信息披露设计可以改善人类决策效率。

Abstract: AI consumer markets are characterized by severe buyer-supplier market asymmetries. Complex AI systems can appear highly accurate while making costly errors or embedding hidden defects. While there have been regulatory efforts surrounding different forms of disclosure, large information gaps remain. This paper provides the first experimental evidence on the important role of information asymmetries and disclosure designs in shaping user adoption of AI systems. We systematically vary the density of low-quality AI systems and the depth of disclosure requirements in a simulated AI product market to gauge how people react to the risk of accidentally relying on a low-quality AI system. Then, we compare participants' choices to a rational Bayesian model, analyzing the degree to which partial information disclosure can improve AI adoption. Our results underscore the deleterious effects of information asymmetries on AI adoption, but also highlight the potential of partial disclosure designs to improve the overall efficiency of human decision-making.

</details>


### [13] [Preliminary Results of a Scoping Review on Assistive Technologies for Adults with ADHD](https://arxiv.org/abs/2601.21791)
*Valerie Tan,Luisa Jost,Jens Gerken,Max Pascher*

Main category: cs.HC

TL;DR: 该论文通过文献综述揭示成人ADHD的研究现状，指出目前大多数学术工作集中于治疗而非支持，未来需强调技术性解决方案的发展。


<details>
  <summary>Details</summary>
Motivation: 成人ADHD虽然长久以来被视为儿童疾病，但在职场和高等教育中对成年人产生了独特挑战，因此需要研究针对成人的支持技术。

Method: 通过PRISMA-ScR方法进行文献检索和筛选，最终分析了46篇相关论文。

Result: 初步发现表明，有关成人ADHD的研究正在增加，但仍需更多关注积极支持的技术性解决方案。

Conclusion: 本研究发现，针对成人ADHD的技术支持研究在数量上显著增加，但大多数文献侧重于治疗和干预，而非积极支持的方法。

Abstract: Attention Deficit Hyperactivity Disorder (ADHD), characterized by inattention, hyperactivity, and impulsivity, is prevalent in the adult population. Long perceived and treated as a childhood condition, ADHD and its characteristics nonetheless impact a significant portion of adults today. In contrast to children with ADHD, adults with ADHD face unique challenges in the workplace and in higher education. In this work-in-progress paper, we present a scoping review as a foundation to understand and explore existing technology-based approaches to support adults with ADHD. In total, our search returned 3,538 papers upon which we selected, based on PRISMA-ScR, a total of 46 papers for in-depth analysis. Our initial findings highlight that most papers take on a therapeutic or intervention perspective instead of a more positive support perspective. Our analysis also found a tremendous increase in recent papers on the topic, which highlights that more and more researchers are becoming aware of the need to address ADHD with adults. For the future, we aim to further analyze the corpus and identify research gaps and potentials for further development of ADHD assistive technologies.

</details>


### [14] [From Future of Work to Future of Workers: Addressing Asymptomatic AI Harms for Dignified Human-AI Interaction](https://arxiv.org/abs/2601.21920)
*Upol Ehsan,Samir Passi,Koustuv Saha,Todd McNutt,Mark O. Riedl,Sara Alcorn*

Main category: cs.HC

TL;DR: 本文探讨AI在工作场所的双重影响，提出框架以促进尊严的人机互动，旨在平衡生产力与专业知识的保存。


<details>
  <summary>Details</summary>
Motivation: 探讨AI的双重角色：增强和侵蚀，同时提升表现却削弱基本专业知识。

Method: 进行了一年的研究，观察癌症专家在高风险工作环境中AI的长期使用。

Result: 发现初期的操作增益掩盖了专家判断的逐渐减弱，长期后果表现为技能萎缩和身份商品化。

Conclusion: 本研究提供了一个框架，促进尊严的人机互动，平衡生产力与人类专业知识的保存。

Abstract: In the future of work discourse, AI is touted as the ultimate productivity amplifier. Yet, beneath the efficiency gains lie subtle erosions of human expertise and agency. This paper shifts focus from the future of work to the future of workers by navigating the AI-as-Amplifier Paradox: AI's dual role as enhancer and eroder, simultaneously strengthening performance while eroding underlying expertise. We present a year-long study on the longitudinal use of AI in a high-stakes workplace among cancer specialists. Initial operational gains hid ``intuition rust'': the gradual dulling of expert judgment. These asymptomatic effects evolved into chronic harms, such as skill atrophy and identity commoditization. Building on these findings, we offer a framework for dignified Human-AI interaction co-constructed with professional knowledge workers facing AI-induced skill erosion without traditional labor protections. The framework operationalizes sociotechnical immunity through dual-purpose mechanisms that serve institutional quality goals while building worker power to detect, contain, and recover from skill erosion, and preserve human identity. Evaluated across healthcare and software engineering, our work takes a foundational step toward dignified human-AI interaction futures by balancing productivity with the preservation of human expertise.

</details>


### [15] [Cognitive Load Estimation Using Brain Foundation Models and Interpretability for BCIs](https://arxiv.org/abs/2601.21965)
*Deeksha M. Shama,Dimitra Emmanouilidou,Ivan J. Tashev*

Main category: cs.HC

TL;DR: 本研究提出将大规模预训练模型（BFMs）应用于脑电图（EEG）数据，以提高认知负荷估计的准确性，克服传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 准确监测认知负荷对适应用户参与度的脑机接口（BCIs）和个性化学习至关重要。

Method: 利用大规模预训练的脑模型（BFMs），对EEG数据进行特征提取，并对少量层进行微调，以提升认知负荷估计的准确性。

Result: BFMs在实时推理中表现出长上下文窗口，并揭示了与认知控制相关的前额叶区域的重要性及学习进展的长期趋势。

Conclusion: BFMs为实时认知负荷监测提供了有效和可解释的工具，适用于实际应用中。

Abstract: Accurately monitoring cognitive load in real time is critical for Brain-Computer Interfaces (BCIs) that adapt to user engagement and support personalized learning. Electroencephalography (EEG) offers a non-invasive, cost-effective modality for capturing neural activity, though traditional methods often struggle with cross-subject variability and task-specific preprocessing. We propose leveraging Brain Foundation Models (BFMs), large pre-trained neural networks, to extract generalizable EEG features for cognitive load estimation. We adapt BFMs for long-term EEG monitoring and show that fine-tuning a small subset of layers yields improved accuracy over the state-of-the-art. Despite their scale, BFMs allow for real-time inference with a longer context window. To address often-overlooked interpretability challenges, we apply Partition SHAP (SHapley Additive exPlanations) to quantify feature importance. Our findings reveal consistent emphasis on prefrontal regions linked to cognitive control, while longitudinal trends suggest learning progression. These results position BFMs as efficient and interpretable tools for continuous cognitive load monitoring in real-world BCIs.

</details>


### [16] [From Particles to Agents: Hallucination as a Metric for Cognitive Friction in Spatial Simulation](https://arxiv.org/abs/2601.21977)
*Javier Argota Sánchez-Vaquerizo,Luis Borunda Monsivais*

Main category: cs.HC

TL;DR: 提出Agentic Environmental Simulations，结合大型多模态生成模型与情景空间推理，构建以人类为中心的AI驱动仿真框架，以增加空间环境的互动性和认知性。


<details>
  <summary>Details</summary>
Motivation: 旨在将传统的物理基础仿真转变为以代理为基础的模拟，提升空间环境的互动性与认知性。

Method: 通过引入Agentic Environmental Simulations，利用大型多模态生成模型进行空间环境的下一状态预测，采用事件驱动的情景空间推理方法。

Result: 通过形式化认知摩擦(C_f)以揭示建筑空间中的“幻影可供性”，并探索将环境视为动态认知伙伴的可能性。

Conclusion: 提出了一种以人类为中心的认知编排框架，用于设计保留自主性、情感清晰性和认知完整性的AI驱动仿真。

Abstract: Traditional architectural simulations (e.g. Computational Fluid Dynamics, evacuation, structural analysis) model elements as deterministic physics-based "particles" rather than cognitive "agents". To bridge this, we introduce \textbf{Agentic Environmental Simulations}, where Large Multimodal generative models actively predict the next state of spatial environments based on semantic expectation. Drawing on examples from accessibility-oriented AR pipelines and multimodal digital twins, we propose a shift from chronological time-steps to Episodic Spatial Reasoning, where simulations advance through meaningful, surprisal-triggered events. Within this framework we posit AI hallucinations as diagnostic tools. By formalizing the \textbf{Cognitive Friction} ($C_f$) it is possible to reveal "Phantom Affordances", i.e. semiotic ambiguities in built space. Finally, we challenge current HCI paradigms by treating environments as dynamic cognitive partners and propose a human-centered framework of cognitive orchestration for designing AI-driven simulations that preserve autonomy, affective clarity, and cognitive integrity.

</details>


### [17] [Vidmento: Creating Video Stories Through Context-Aware Expansion With Generative Video](https://arxiv.org/abs/2601.22013)
*Catherine Yeh,Anh Truong,Mira Dontcheva,Bryan Wang*

Main category: cs.HC

TL;DR: 这篇论文探讨了生成视频如何增强视频故事讲述，介绍了Vidmento工具的开发及其对创作者的支持。


<details>
  <summary>Details</summary>
Motivation: 解决视频叙事中由材料限制导致的创造性表达不足和叙事空白的问题。

Method: 采访视频创作者，识别生成视频整合的机会与挑战，开发Vidmento工具

Result: Vidmento支持通过生成媒体系统性地扩展初始材料，促进叙事发展和探索，增强视频叙事的表现力。

Conclusion: 创造者利用生成内容弥补故事漏洞，并发现这一融合能力在特定情况下最具价值。

Abstract: Video storytelling is often constrained by available material, limiting creative expression and leaving undesired narrative gaps. Generative video offers a new way to address these limitations by augmenting captured media with tailored visuals. To explore this potential, we interviewed eight video creators to identify opportunities and challenges in integrating generative video into their workflows. Building on these insights and established filmmaking principles, we developed Vidmento, a tool for authoring hybrid video stories that combine captured and generated media through context-aware expansion. Vidmento surfaces opportunities for story development, generates clips that blend stylistically and narratively with surrounding media, and provides controls for refinement. In a study with 12 creators, Vidmento supported narrative development and exploration by systematically expanding initial materials with generative media, enabling expressive video storytelling aligned with creative intent. We highlight how creators bridge story gaps with generative content and where they find this blending capability most valuable.

</details>


### [18] [Accessibility-Driven Information Transformations in Mixed-Visual Ability Work Teams](https://arxiv.org/abs/2601.22081)
*Yichun Zhao,Miguel A. Nacenta,Mahadeo A. Sukhai,Sowmya Somanath*

Main category: cs.HC

TL;DR: 本研究探讨了盲人与低视力员工在混合视觉能力团队中的信息可访问性问题，并分析了与之相关的转化过程和协调模式。


<details>
  <summary>Details</summary>
Motivation: 盲人与低视力员工在混合视觉能力团队中经常遇到信息（例如 PDF 和图表）无法访问的问题。

Method: 进行了为期一周的日记研究，并对来自五个法律、非营利和咨询团队的23名盲人与低视力以及视力正常的专业人员进行了跟进访谈，记录了36个转化案例。

Result: 我们的分析描绘了团队如何进行可访问性的表征转化，包括转化的触发方式（主动或被动）、复杂性或增强性，以及员工在解决表征不兼容时的四种常见协调模式。

Conclusion: 研究结果揭示了更好地支持混合视觉能力工作的系统设计机会。

Abstract: Blind and low-vision (BLV) employees in mixed-visual ability teams often encounter information (e.g., PDFs, diagrams) in inaccessible formats. To enable teamwork, teams must transform these representations by modifying or re-creating them into accessible forms. However, these transformations are frequently overlooked, lack infrastructural support, and cause additional labour. To design systems that move beyond one-off accommodations to effective mixed-ability collaboration, we need a deeper understanding of the representations, their transformations and how they occur. We conducted a week-long diary study with follow-up interviews with 23 BLV and sighted professionals from five legal, non-profit, and consulting teams, documenting 36 transformation cases. Our analysis characterizes how teams perform representational transformations for accessibility: how they are triggered proactively or reactively, how they simplify or enhance, and four common patterns in which workers coordinate with each other to address representational incompatibility. Our findings uncover opportunities for designing systems that can better support mixed-visual ability work.

</details>


### [19] [Auditorily Embodied Conversational Agents: Effects of Spatialization and Situated Audio Cues on Presence and Social Perception](https://arxiv.org/abs/2601.22082)
*Yi Fei Cheng,Jarod Bloch,Alexander Wang,Andrea Bianchi,Anusha Withana,Anhong Guo,Laurie M. Heller,David Lindlbauer*

Main category: cs.HC

TL;DR: 通过引入空间化声音和Foley音效，听觉化身可以提升会话代理的存在感，但可能会削弱用户对代理的社交属性印象。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在寻找在缺乏视觉呈现的情况下增强会话代理感觉存在感的方法，特别是在使用耳机或无显示器眼镜时。

Method: 进行了一个2x2的在体内研究，比较了空间化声音与普通声音，以及有无Foley音效对会话代理感知的影响，参与者实际进行对话。

Result: 本研究探讨了听觉化身如何增强会话代理的存在感，特别是在用户使用耳机或无显示器眼镜时。实验发现，空间化的声音和环境交互的Foley音效增强了共存感，但降低了用户对代理关注和其他社会属性的认知。

Conclusion: 听觉化身通过空间化声和Foley音效可以增强用户的共存感，尽管这可能会影响他们对代理社交属性的感知。

Abstract: Embodiment can enhance conversational agents, such as increasing their perceived presence. This is typically achieved through visual representations of a virtual body; however, visual modalities are not always available, such as when users interact with agents using headphones or display-less glasses. In this work, we explore auditory embodiment. By introducing auditory cues of bodily presence - through spatially localized voice and situated Foley audio from environmental interactions - we investigate how audio alone can convey embodiment and influence perceptions of a conversational agent. We conducted a 2 (spatialization: monaural vs. spatialized) x 2 (Foley: none vs. Foley) within-subjects study, where participants (n=24) engaged in conversations with agents. Our results show that spatialization and Foley increase co-presence, but reduce users' perceptions of the agent's attention and other social attributes.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [20] [Quick Heuristic Validation of Edges in Dynamic Roadmap Graphs](https://arxiv.org/abs/2601.20968)
*Yulie Arad,Stav Ashur,Nancy M. Amato*

Main category: cs.RO

TL;DR: 本文介绍了一种新的方法，用于在动态环境中调整机器人运动规划的路线图，通过红-绿-灰的分类方法提升有效性标记的准确性。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中调整机器人运动规划的路线图，解决传统方法的局限性。

Method: 提出了"红-绿-灰"范式，通过低成本的启发式检查来分类节点和边的有效性状态，进行快速的半惰性路线图更新。

Result: 与Leven和Hutchinson的技术相比，我们的方法提高了准确性，同时在更新运行时间上保持了可比较的水平。

Conclusion: 我们的方法在保障相似更新运行时间的同时，准确性显著提升，能够正确将边标记为无效。

Abstract: In this paper we tackle the problem of adjusting roadmap graphs for robot motion planning to non-static environments. We introduce the "Red-Green-Gray" paradigm, a modification of the SPITE method, capable of classifying the validity status of nodes and edges using cheap heuristic checks, allowing fast semi-lazy roadmap updates. Given a roadmap, we use simple computational geometry methods to approximate the swept volumes of robots and perform lazy collision checks, and label a subset of the edges as invalid (red), valid (green), or unknown (gray). We present preliminary experimental results comparing our method to the well-established technique of Leven and Hutchinson, and showing increased accuracy as well as the ability to correctly label edges as invalid while maintaining comparable update runtimes.

</details>


### [21] [Meta-ROS: A Next-Generation Middleware Architecture for Adaptive and Scalable Robotic Systems](https://arxiv.org/abs/2601.21011)
*Anshul Ranjan,Anoosh Damodar,Neha Chougule,Dhruva S Nayak,Anantharaman P. N,Shylaja S S*

Main category: cs.RO

TL;DR: Meta-ROS是一种新中间件解决方案，通过简化集成和提升性能，解决了ROS2的复杂性和互操作性问题，并在多项性能测试中显著优于ROS2。


<details>
  <summary>Details</summary>
Motivation: 应对现有中间件框架如ROS2在复杂性和互操作性上带来的挑战，尤其是为了帮助新开发者更容易上手。

Method: 通过与现有中间件框架如ROS1和ROS2的比较测试，评估Meta-ROS的性能。

Result: Meta-ROS达到了最高30%的吞吐量提升，显著降低了消息延迟，并优化了资源使用。

Conclusion: Meta-ROS在处理性能、延迟和资源使用上显著优于ROS2，是现代实时机器人AI应用的理想解决方案。

Abstract: The field of robotics faces significant challenges related to the complexity and interoperability of existing middleware frameworks, like ROS2, which can be difficult for new developers to adopt. To address these issues, we propose Meta-ROS, a novel middleware solution designed to streamline robotics development by simplifying integration, enhancing performance, and ensuring cross-platform compatibility. Meta-ROS leverages modern communication protocols, such as Zenoh and ZeroMQ, to enable efficient and low-latency communication across diverse hardware platforms, while also supporting various data types like audio, images, and video. We evaluated Meta-ROS's performance through comprehensive testing, comparing it with existing middleware frameworks like ROS1 and ROS2. The results demonstrated that Meta-ROS outperforms ROS2, achieving up to 30% higher throughput, significantly reducing message latency, and optimizing resource usage. Additionally, its robust hardware support and developer-centric design facilitate seamless integration and ease of use, positioning Meta-ROS as an ideal solution for modern, real-time robotics AI applications.

</details>


### [22] [Track-centric Iterative Learning for Global Trajectory Optimization in Autonomous Racing](https://arxiv.org/abs/2601.21027)
*Youngim Nam,Jungbin Kim,Kyungtae Kang,Cheolhyeon Kwon*

Main category: cs.RO

TL;DR: 本文提出了一种全球轨迹优化框架，通过基于贝叶斯优化的方法结合实时数据更新，实现了自主赛车在不确定动态下的轨迹优化，显著减少了圈速，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在跟踪水平的动态学习，而未能更新轨迹以考虑所学习的动态。因此，迫切需要一个全局轨迹优化框架来解决这个问题。

Method: 提出了一种基于跟踪水平的全景轨迹优化框架，使用贝叶斯优化方法探索参数化空间，结合实时数据更新动态，以实现轨迹的逐步优化。

Result: 经过迭代学习，优化后的轨迹在仿真和实験中表现优异，超越了最先进的方法，实现了高达20.7%的时间减少。

Conclusion: 该框架在多个迭代中优化轨迹，并在模拟和真实实验中验证了其有效性，取得了最高20.7%的时间改善。

Abstract: This paper presents a global trajectory optimization framework for minimizing lap time in autonomous racing under uncertain vehicle dynamics. Optimizing the trajectory over the full racing horizon is computationally expensive, and tracking such a trajectory in the real world hardly assures global optimality due to uncertain dynamics. Yet, existing work mostly focuses on dynamics learning at the tracking level, without updating the trajectory itself to account for the learned dynamics. To address these challenges, we propose a track-centric approach that directly learns and optimizes the full-horizon trajectory. We first represent trajectories through a track-agnostic parametric space in light of the wavelet transform. This space is then efficiently explored using Bayesian optimization, where the lap time of each candidate is evaluated by running simulations with the learned dynamics. This optimization is embedded in an iterative learning framework, where the optimized trajectory is deployed to collect real-world data for updating the dynamics, progressively refining the trajectory over the iterations. The effectiveness of the proposed framework is validated through simulations and real-world experiments, demonstrating lap time improvement of up to 20.7% over a nominal baseline and consistently outperforming state-of-the-art methods.

</details>


### [23] [Multi-Robot Decentralized Collaborative SLAM in Planetary Analogue Environments: Dataset, Challenges, and Lessons Learned](https://arxiv.org/abs/2601.21063)
*Pierre-Yves Lajoie,Karthik Soma,Haechan Mark Bong,Alice Lemieux-Bourque,Rongge Zhang,Vivek Shankar Varadharajan,Giovanni Beltrame*

Main category: cs.RO

TL;DR: 本研究探讨了多机器人在去中心化协作SLAM中面临的通信限制和定位挑战，并提供了一个新的数据集以促进相关领域的研究。


<details>
  <summary>Details</summary>
Motivation: 在未知环境中实现多机器人任务的去中心化协作SLAM是探索月球、火星和其他行星的重要技术。

Method: 通过对三台机器人在火星类地形上的实验，评估去中心化C-SLAM在有限和间歇性通信下的性能表现。

Result: 研究结果表明，有限的通信条件对C-SLAM的性能有显著影响，并且提出了一个新的数据集，包含了机器人之间的实时通信吞吐量和延迟测量。

Conclusion: 本研究提供了有关多机器人在类似行星环境中进行去中心化协作同时定位与地图构建（C-SLAM）的实用见解，强调了通信限制对C-SLAM性能的影响，并提供了创新的数据集以支持未来的研究。

Abstract: Decentralized collaborative simultaneous localization and mapping (C-SLAM) is essential to enable multirobot missions in unknown environments without relying on preexisting localization and communication infrastructure. This technology is anticipated to play a key role in the exploration of the Moon, Mars, and other planets. In this article, we share insights and lessons learned from C-SLAM experiments involving three robots operating on a Mars analogue terrain and communicating over an ad hoc network. We examine the impact of limited and intermittent communication on C-SLAM performance, as well as the unique localization challenges posed by planetary-like environments. Additionally, we introduce a novel dataset collected during our experiments, which includes real-time peer-to-peer inter-robot throughput and latency measurements. This dataset aims to support future research on communication-constrained, decentralized multirobot operations.

</details>


### [24] [WheelArm-Sim: A Manipulation and Navigation Combined Multimodal Synthetic Data Generation Simulator for Unified Control in Assistive Robotics](https://arxiv.org/abs/2601.21129)
*Guangping Liu,Tipu Sultan,Vittorio Di Giorgio,Nick Hawkins,Flavio Esposito,Madi Babaiasl*

Main category: cs.RO

TL;DR: 本文提出了WheelArm，一个集成了轮椅和机械臂控制的网络物理系统，开发了WheelArm-Sim仿真框架以收集多模态数据，结果表明其对机器学习模型的集成控制是可行的。


<details>
  <summary>Details</summary>
Motivation: 为了实现轮椅和机械臂的集成和统一控制，填补当前研究中的空白。

Method: 使用Isaac Sim开发了WheelArm-Sim仿真框架，以进行合成数据收集。

Result: 收集了一个包含13个任务、232条轨迹和67,783个样本的多模态数据集，并在芥菜采摘任务中实现了基准模型的动作预测。

Conclusion: WheelArm-Sim所收集的数据为基于数据的机器学习模型进行集成控制提供了可行性。

Abstract: Wheelchairs and robotic arms enhance independent living by assisting individuals with upper-body and mobility limitations in their activities of daily living (ADLs). Although recent advancements in assistive robotics have focused on Wheelchair-Mounted Robotic Arms (WMRAs) and wheelchairs separately, integrated and unified control of the combination using machine learning models remains largely underexplored. To fill this gap, we introduce the concept of WheelArm, an integrated cyber-physical system (CPS) that combines wheelchair and robotic arm controls. Data collection is the first step toward developing WheelArm models. In this paper, we present WheelArm-Sim, a simulation framework developed in Isaac Sim for synthetic data collection. We evaluate its capability by collecting a manipulation and navigation combined multimodal dataset, comprising 13 tasks, 232 trajectories, and 67,783 samples. To demonstrate the potential of the WheelArm dataset, we implement a baseline model for action prediction in the mustard-picking task. The results illustrate that data collected from WheelArm-Sim is feasible for a data-driven machine learning model for integrated control.

</details>


### [25] [InspecSafe-V1: A Multimodal Benchmark for Safety Assessment in Industrial Inspection Scenarios](https://arxiv.org/abs/2601.21173)
*Zeyi Liu,Shuang Liu,Jihai Min,Zhaoheng Zhang,Jun Cen,Pengyu Han,Songqiao Hu,Zihan Meng,Xiao He,Donghua Zhou*

Main category: cs.RO

TL;DR: 该论文介绍了一个新的多模态数据集 InspecSafe-V1，旨在改善 AI 系统在复杂工业环境中的可靠感知和安全评估，涵盖多个工业场景，并提供多种传感器数据。


<details>
  <summary>Details</summary>
Motivation: 随着工业智能和无人检验的迅速发展，可靠的感知与安全评估成为部署预测性维护和自主检查的关键瓶颈，而现有数据集存在局限性，这促使研究者开发 InspecSafe-V1。

Method: InspecSafe-V1 数据集收集自真实工业环境中，融合多种传感器数据，包括红外视频、音频、深度点云等，提供像素级分割标注和语义场景描述。

Result: 数据集涵盖五种典型工业场景，包含来自 2,239 个有效检查站点的 5,013 次检查实例，支持了多模态异常识别和全面安全评估的研究。

Conclusion: InspecSafe-V1 数据集为工业 inspection 安全评估提供了多模态基准，促进了复杂工业环境中的可靠感知和安全评估，支持了先进的异常识别和安全评估研究。

Abstract: With the rapid development of industrial intelligence and unmanned inspection, reliable perception and safety assessment for AI systems in complex and dynamic industrial sites has become a key bottleneck for deploying predictive maintenance and autonomous inspection. Most public datasets remain limited by simulated data sources, single-modality sensing, or the absence of fine-grained object-level annotations, which prevents robust scene understanding and multimodal safety reasoning for industrial foundation models. To address these limitations, InspecSafe-V1 is released as the first multimodal benchmark dataset for industrial inspection safety assessment that is collected from routine operations of real inspection robots in real-world environments. InspecSafe-V1 covers five representative industrial scenarios, including tunnels, power facilities, sintering equipment, oil and gas petrochemical plants, and coal conveyor trestles. The dataset is constructed from 41 wheeled and rail-mounted inspection robots operating at 2,239 valid inspection sites, yielding 5,013 inspection instances. For each instance, pixel-level segmentation annotations are provided for key objects in visible-spectrum images. In addition, a semantic scene description and a corresponding safety level label are provided according to practical inspection tasks. Seven synchronized sensing modalities are further included, including infrared video, audio, depth point clouds, radar point clouds, gas measurements, temperature, and humidity, to support multimodal anomaly recognition, cross-modal fusion, and comprehensive safety assessment in industrial environments.

</details>


### [26] [Disturbance-Aware Flight Control of Robotic Gliding Blimp via Moving Mass Actuation](https://arxiv.org/abs/2601.21188)
*Hao Cheng,Feitian Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种基于移动视野估计和模型预测控制的控制框架，以增强机器人飞艇在风干扰下的飞行稳定性，并通过实验证明了其优越性。


<details>
  <summary>Details</summary>
Motivation: 致力于解决现有轻于 air (LTA) 平台缺乏对风扰动的控制框架的问题。

Method: 使用移动视野估计器（MHE）推断实时风扰动，并将这些估计值提供给模型预测控制器（MPC）。

Result: 通过提出的MHE-MPC框架，在不同风况下实现了稳健的轨迹和航向调节。

Conclusion: 采用2自由度移动质心机制生成惯性和气动力矩，提高了在干扰环境中的飞行稳定性。

Abstract: Robotic blimps, as lighter-than-air (LTA) aerial systems, offer long endurance and inherently safe operation but remain highly susceptible to wind disturbances. Building on recent advances in moving mass actuation, this paper addresses the lack of disturbance-aware control frameworks for LTA platforms by explicitly modeling and compensating for wind-induced effects. A moving horizon estimator (MHE) infers real-time wind perturbations and provides these estimates to a model predictive controller (MPC), enabling robust trajectory and heading regulation under varying wind conditions. The proposed approach leverages a two-degree-of-freedom (2-DoF) moving-mass mechanism to generate both inertial and aerodynamic moments for attitude and heading control, thereby enhancing flight stability in disturbance-prone environments. Extensive flight experiments under headwind and crosswind conditions show that the integrated MHE-MPC framework significantly outperforms baseline PID control, demonstrating its effectiveness for disturbance-aware LTA flight.

</details>


### [27] [Abstracting Robot Manipulation Skills via Mixture-of-Experts Diffusion Policies](https://arxiv.org/abs/2601.21251)
*Ce Hao,Xuanran Zhai,Yaohua Liu,Harold Soh*

Main category: cs.RO

TL;DR: SMP是一种新型扩散基混合专家策略，通过学习紧凑的技能基础和在推理时灵活激活专家，实现了多任务 Robot 操作的高效解决方案。


<details>
  <summary>Details</summary>
Motivation: 扩散基政策在机器人操作中表现良好，但在多任务场景中的扩展受限于模型规模和演示的高成本。

Method: 引入了一种基于扩散的混合专家策略（SMP），学习压缩的正交技能基础，并在每个步骤中使用粘性路由从一个小的、与任务相关的专家子集中组合动作。

Result: 在仿真和真实双臂平台上验证SMP，显示其在多任务学习和迁移学习任务中比大型扩散基线有更高的成功率和明显较低的推理成本。

Conclusion: SMP在多任务学习和迁移学习任务中表现出更高的成功率和显著较低的推理成本，显示了可扩展和可转移的多任务操作的实用路径。

Abstract: Diffusion-based policies have recently shown strong results in robot manipulation, but their extension to multi-task scenarios is hindered by the high cost of scaling model size and demonstrations. We introduce Skill Mixture-of-Experts Policy (SMP), a diffusion-based mixture-of-experts policy that learns a compact orthogonal skill basis and uses sticky routing to compose actions from a small, task-relevant subset of experts at each step. A variational training objective supports this design, and adaptive expert activation at inference yields fast sampling without oversized backbones. We validate SMP in simulation and on a real dual-arm platform with multi-task learning and transfer learning tasks, where SMP achieves higher success rates and markedly lower inference cost than large diffusion baselines. These results indicate a practical path toward scalable, transferable multi-task manipulation: learn reusable skills once, activate only what is needed, and adapt quickly when tasks change.

</details>


### [28] [Deep QP Safety Filter: Model-free Learning for Reachability-based Safety Filter](https://arxiv.org/abs/2601.21297)
*Byeongjun Kim,H. Jin Kim*

Main category: cs.RO

TL;DR: 提出Deep QP安全过滤器，通过无模型学习和H-J可达性相结合，实现黑盒动态系统的安全控制，减少失败并加快学习。


<details>
  <summary>Details</summary>
Motivation: 在无模型动态系统中实现安全控制，减少学习过程中的风险和失败。

Method: 通过结合哈密顿-雅可比(HJ)可达性与无模型学习，学习二次规划(QP)安全过滤器，构建基于收缩的损失函数，分别训练两个神经网络。

Result: 在各种动态系统（包括混合系统）和多个强化学习任务中，Deep QP安全过滤器显著减少了预收敛失败，同时加快了学习过程，达到了比较强基线更高的收益。

Conclusion: Deep QP安全过滤器为黑盒动态系统提供了一种有效且实用的安全控制方法，能够实现安全和高效的无模型控制。

Abstract: We introduce Deep QP Safety Filter, a fully data-driven safety layer for black-box dynamical systems. Our method learns a Quadratic-Program (QP) safety filter without model knowledge by combining Hamilton-Jacobi (HJ) reachability with model-free learning. We construct contraction-based losses for both the safety value and its derivatives, and train two neural networks accordingly. In the exact setting, the learned critic converges to the viscosity solution (and its derivative), even for non-smooth values. Across diverse dynamical systems -- even including a hybrid system -- and multiple RL tasks, Deep QP Safety Filter substantially reduces pre-convergence failures while accelerating learning toward higher returns than strong baselines, offering a principled and practical route to safe, model-free control.

</details>


### [29] [HPTune: Hierarchical Proactive Tuning for Collision-Free Model Predictive Control](https://arxiv.org/abs/2601.21346)
*Wei Zuo,Chengyang Li,Yikun Wang,Bingyang Cheng,Zeyi Ren,Shuai Wang,Derrick Wing Kwan Ng,Yik-Chung Wu*

Main category: cs.RO

TL;DR: 提出了一种新的HPTune框架，通过扩展评估未执行的动作，提高了模型预测控制中的参数调优效率。


<details>
  <summary>Details</summary>
Motivation: 现有参数调优方法在运动规划中往往只评估执行过的动作，导致因失败事件稀少而产生的效率低下。

Method: 提出了一种分层的主动调优框架（HPTune），结合快速和慢速调优，其中快速调优采用风险指标，慢速调优利用闭环反向传播的扩展评估损失。

Result: HPTune在高保真模拟器上的实验表明，其在复杂环境中有效地实现了MPC调优，并超越了各种基准方案。

Conclusion: HPTune能够实现面向情境的运动规划，为安全、灵活的避障策略提供支持。

Abstract: Parameter tuning is a powerful approach to enhance adaptability in model predictive control (MPC) motion planners. However, existing methods typically operate in a myopic fashion that only evaluates executed actions, leading to inefficient parameter updates due to the sparsity of failure events (e.g., obstacle nearness or collision). To cope with this issue, we propose to extend evaluation from executed to non-executed actions, yielding a hierarchical proactive tuning (HPTune) framework that combines both a fast-level tuning and a slow-level tuning. The fast one adopts risk indicators of predictive closing speed and predictive proximity distance, and the slow one leverages an extended evaluation loss for closed-loop backpropagation. Additionally, we integrate HPTune with the Doppler LiDAR that provides obstacle velocities apart from position-only measurements for enhanced motion predictions, thus facilitating the implementation of HPTune. Extensive experiments on high-fidelity simulator demonstrate that HPTune achieves efficient MPC tuning and outperforms various baseline schemes in complex environments. It is found that HPTune enables situation-tailored motion planning by formulating a safe, agile collision avoidance strategy.

</details>


### [30] [Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control](https://arxiv.org/abs/2601.21363)
*Weidong Huang,Zhehan Li,Hangxin Liu,Biao Hou,Yao Su,Jingwen Zhang*

Main category: cs.RO

TL;DR: 本研究通过结合SAC的高效预训练和基于模型的微调方法，显著提升了人形机器人在新环境中的适应能力，降低了探索风险，从而提高了整体效率。


<details>
  <summary>Details</summary>
Motivation: 在现有的强化学习算法中，传统的强化学习方法在新的环境中适应时样本效率低，同时希望提高这种适应能力。

Method: 使用基于偏差的软演员-批评算法（SAC）进行大规模预训练，并结合基于模型的方法进行适应微调。

Result: 通过采用大批量更新和高数据更新比（UTD），我们成功实现了人形机器人策略的零-shot 部署，并在新环境中有效微调。

Conclusion: 理论与实证相结合的方法增强了人形机器人适应新环境的能力，同时还优化了大规模预训练和高效微调过程的结合。

Abstract: Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning.

</details>


### [31] [Towards Space-Based Environmentally-Adaptive Grasping](https://arxiv.org/abs/2601.21394)
*Leonidas Askianakis,Aleksandr Artemov*

Main category: cs.RO

TL;DR: 本研究通过在潜在流形中学习抓取控制策略，克服了高维动作空间和稀疏奖励的挑战，实现了在空间环境中高效的单次抓取。


<details>
  <summary>Details</summary>
Motivation: 针对传统机器人操作在高维动作空间、稀疏奖励和未经过精心设置的训练情景下缺乏可靠执行的问题进行研究。

Method: 在学习的潜在流形中直接学习控制策略，与基于SAC的强化学习结合进行单次抓取任务的模拟，取得超过95%的任务成功率。

Result: 在不断变化的抓取条件下，通过GPU加速的物理仿真和单次操作任务，成功实现了较快的收敛性，优于现有的视觉基线。

Conclusion: 明确推理在潜在空间中可以提供更有效的样本学习，并提高对新物体和夹具几何形状、环境混乱以及传感器配置的鲁棒性。

Abstract: Robotic manipulation in unstructured environments requires reliable execution under diverse conditions, yet many state-of-the-art systems still struggle with high-dimensional action spaces, sparse rewards, and slow generalization beyond carefully curated training scenarios. We study these limitations through the example of grasping in space environments. We learn control policies directly in a learned latent manifold that fuses (grammarizes) multiple modalities into a structured representation for policy decision-making. Building on GPU-accelerated physics simulation, we instantiate a set of single-shot manipulation tasks and achieve over 95% task success with Soft Actor-Critic (SAC)-based reinforcement learning in less than 1M environment steps, under continuously varying grasping conditions from step 1. This empirically shows faster convergence than representative state-of-the-art visual baselines under the same open-loop single-shot conditions. Our analysis indicates that explicitly reasoning in latent space yields more sample-efficient learning and improved robustness to novel object and gripper geometries, environmental clutter, and sensor configurations compared to standard baselines. We identify remaining limitations and outline directions toward fully adaptive and generalizable grasping in the extreme conditions of space.

</details>


### [32] [DSCD-Nav: Dual-Stance Cooperative Debate for Object Navigation](https://arxiv.org/abs/2601.21409)
*Weitao An,Qi Liu,Chenghao Xu,Jiayi Chai,Xu Yang,Kun Wei,Cheng Deng*

Main category: cs.RO

TL;DR: 本文提出DSCD-Nav，通过双立场合作辩论机制改善家庭服务机器人在未知室内环境中的导航能力，实验证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有导航系统在决策层面依赖单次评分，导致过于自信的长时间错误和冗余的探索，因此需要一种新机制来改善这些问题。

Method: 提出了一种双立场合作辩论导航机制 (DSCD-Nav)，取代了一般的单次评分方法，以立场为基础的交叉检查和基于证据的仲裁，提高在部分可观测情况下的动作可靠性。

Result: 在HM3Dv1、HM3Dv2和MP3D上的实验展示了成功率和路径效率的一致性改善，同时减少了探索的冗余。

Conclusion: 在不熟悉的室内环境中应用的 Adaptive navigation 技术确实有效提升了家庭服务机器人在决策与路径规划中的能力。

Abstract: Adaptive navigation in unfamiliar indoor environments is crucial for household service robots. Despite advances in zero-shot perception and reasoning from vision-language models, existing navigation systems still rely on single-pass scoring at the decision layer, leading to overconfident long-horizon errors and redundant exploration. To tackle these problems, we propose Dual-Stance Cooperative Debate Navigation (DSCD-Nav), a decision mechanism that replaces one-shot scoring with stance-based cross-checking and evidence-aware arbitration to improve action reliability under partial observability. Specifically, given the same observation and candidate action set, we explicitly construct two stances by conditioning the evaluation on diverse and complementary objectives: a Task-Scene Understanding (TSU) stance that prioritizes goal progress from scene-layout cues, and a Safety-Information Balancing (SIB) stance that emphasizes risk and information value. The stances conduct a cooperative debate and make policy by cross-checking their top candidates with cue-grounded arguments. Then, a Navigation Consensus Arbitration (NCA) agent is employed to consolidate both sides' reasons and evidence, optionally triggering lightweight micro-probing to verify uncertain choices, preserving NCA's primary intent while disambiguating. Experiments on HM3Dv1, HM3Dv2, and MP3D demonstrate consistent improvements in success and path efficiency while reducing exploration redundancy.

</details>


### [33] [Singularity-Free Lie Group Integration and Geometrically Consistent Evaluation of Multibody System Models Described in Terms of Standard Absolute Coordinates](https://arxiv.org/abs/2601.21413)
*Andreas Mueller*

Main category: cs.RO

TL;DR: 本文提出了一种新的多体系统建模框架，结合了Lie群积分和绝对坐标，解决了时间积分中的奇异性问题，并维护了运动的几何特性。


<details>
  <summary>Details</summary>
Motivation: 针对多体系统建模过程中的奇异性问题，尝试使用Lie群积分方法避免常规时间积分中的奇异性，同时保持空间运动的几何特性。

Method: 提出了一种框架，将Lie群积分器与标准EOM公式接口相连，并引入局部-全球转移（LGT）映射，以便在绝对坐标与Lie群局部坐标之间进行有效更新。

Result: 成功建立了一种新的框架，结合了绝对坐标和Lie群积分，使得多体系统可以在标准计算代码中实现，无需重大重构。

Conclusion: 本文提出的框架能够有效地将Lie群积分器与标准EOM公式连接，从而在保持几何性质的同时，实现多体系统的建模与模拟。

Abstract: A classical approach to the multibody systems (MBS) modeling is to use absolute coordinates, i.e., a set of (possibly redundant) coordinates that describe the absolute position and orientation of the individual bodies with respect to an inertial frame (IFR). A well-known problem for the time integration of the equations of motion (EOM) is the lack of a singularity-free parameterization of spatial motions, which is usually tackled by using unit quaternions. Lie group integration methods were proposed as an alternative approach to the singularity-free time integration. At the same time, Lie group formulations of EOM naturally respect the geometry of spatial motions during integration. Lie group integration methods, operating directly on the configuration space Lie group, are incompatible with standard formulations of the EOM, and cannot be implemented in existing MBS simulation codes without a major restructuring. The contribution of this paper is twofold: (1) A framework for interfacing Lie group integrators to standard EOM formulations is presented. It allows describing MBS in terms of various absolute coordinates and at the same using Lie group integration schemes. (2) A method for consistently incorporating the geometry of rigid body motions into the evaluation of EOM in absolute coordinates integrated with standard vector space integration schemes. The direct product group and the semidirect product group SO(3)xR3 and the semidirect product group SE(3) are used for representing rigid body motions. The key element is the local-global transitions (LGT) transition map, which facilitates the update of (global) absolute coordinates in terms of the (local) coordinates on the Lie group. This LGT map is specific to the absolute coordinates, the local coordinates on the Lie group, and the Lie group used to represent rigid body configurations.

</details>


### [34] [Spotlighting Task-Relevant Features: Object-Centric Representations for Better Generalization in Robotic Manipulation](https://arxiv.org/abs/2601.21416)
*Alexandre Chapin,Bruno Machado,Emmanuel Dellandréa,Liming Chen*

Main category: cs.RO

TL;DR: 本研究探讨中间插槽基特征表示(SBOCR)在机器人操作任务中的优势，发现其能有效增强模型在多变视觉条件下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法的特征类型将与任务无关的信息混合，导致在分布变化下的泛化能力差，因此亟需探索中间结构的特征表示。

Method: 通过对比不同的全局和稠密特征表示，评估中间插槽基表示的有效性，涉及简单到复杂的模拟和现实操作任务。

Result: SBOCR的策略在多种视觉条件下表现出更好的泛化能力，即使没有任务特定的预训练。

Conclusion: SBOCR基于的策略在不同视觉条件下的泛化能力优于基于稠密和全局特征的策略，表明SBOCR是一种有效的视觉系统设计方向。

Abstract: The generalization capabilities of robotic manipulation policies are heavily influenced by the choice of visual representations. Existing approaches typically rely on representations extracted from pre-trained encoders, using two dominant types of features: global features, which summarize an entire image via a single pooled vector, and dense features, which preserve a patch-wise embedding from the final encoder layer. While widely used, both feature types mix task-relevant and irrelevant information, leading to poor generalization under distribution shifts, such as changes in lighting, textures, or the presence of distractors. In this work, we explore an intermediate structured alternative: Slot-Based Object-Centric Representations (SBOCR), which group dense features into a finite set of object-like entities. This representation permits to naturally reduce the noise provided to the robotic manipulation policy while keeping enough information to efficiently perform the task. We benchmark a range of global and dense representations against intermediate slot-based representations, across a suite of simulated and real-world manipulation tasks ranging from simple to complex. We evaluate their generalization under diverse visual conditions, including changes in lighting, texture, and the presence of distractors. Our findings reveal that SBOCR-based policies outperform dense and global representation-based policies in generalization settings, even without task-specific pretraining. These insights suggest that SBOCR is a promising direction for designing visual systems that generalize effectively in dynamic, real-world robotic environments.

</details>


### [35] [Nimbus: A Unified Embodied Synthetic Data Generation Framework](https://arxiv.org/abs/2601.21449)
*Zeyu He,Yuchang Zhang,Yuanzhen Zhou,Miao Tao,Hengjie Li,Yang Tian,Jia Zeng,Tai Wang,Wenzhe Cai,Yilun Chen,Ning Gao,Jiangmiao Pang*

Main category: cs.RO

TL;DR: Nimbus是一个统一的合成数据生成框架，通过模组化设计和优化，提高了数据生成效率，支持大规模数据合成。


<details>
  <summary>Details</summary>
Motivation: 随着数据量和多样性的增加，合成数据生成成为克服昂贵物理数据获取的关键，但现有的合成数据生成流程过于分散和特定任务，导致工程效率低下。

Method: Nimbus采用了统一的合成数据生成框架，采用模组化四层架构，分离了轨迹规划、渲染和存储等流程，同时实现了动态管道调度和全球负载平衡等优化。

Result: Nimbus相较于未优化的基线实现了2-3倍的端到端吞吐量提升，同时在大规模分布式环境中保持了系统的稳定性。

Conclusion: Nimbus框架显著提高了数据生成效率，并且在大规模分布式环境中表现稳健，为跨领域数据合成提供了强有力支持。

Abstract: Scaling data volume and diversity is critical for generalizing embodied intelligence. While synthetic data generation offers a scalable alternative to expensive physical data acquisition, existing pipelines remain fragmented and task-specific. This isolation leads to significant engineering inefficiency and system instability, failing to support the sustained, high-throughput data generation required for foundation model training. To address these challenges, we present Nimbus, a unified synthetic data generation framework designed to integrate heterogeneous navigation and manipulation pipelines. Nimbus introduces a modular four-layer architecture featuring a decoupled execution model that separates trajectory planning, rendering, and storage into asynchronous stages. By implementing dynamic pipeline scheduling, global load balancing, distributed fault tolerance, and backend-specific rendering optimizations, the system maximizes resource utilization across CPU, GPU, and I/O resources. Our evaluation demonstrates that Nimbus achieves a 2-3X improvement in end-to-end throughput compared to unoptimized baselines and ensuring robust, long-term operation in large-scale distributed environments. This framework serves as the production backbone for the InternData suite, enabling seamless cross-domain data synthesis.

</details>


### [36] [4D-CAAL: 4D Radar-Camera Calibration and Auto-Labeling for Autonomous Driving](https://arxiv.org/abs/2601.21454)
*Shanliang Yao,Zhuoxiao Li,Runwei Guan,Kebin Cao,Meng Xia,Fuping Hu,Sen Xu,Yong Yue,Xiaohui Zhu,Weiping Ding,Ryan Wen Liu*

Main category: cs.RO

TL;DR: 本文提出4D-CAAL框架，解决雷达与摄像头的标定与自动标注问题，显著提高标定精度，降低人工标注工作量，推动自主驾驶技术的发展。


<details>
  <summary>Details</summary>
Motivation: 解决现有标定方法中光学和雷达模态之间的目标分离，简化对应关系的建立，同时降低稀疏雷达数据手动标注的劳动强度和不可靠性。

Method: 提出了一种统一的4D雷达摄像头标定和自动标注框架，采用双重用途标定目标设计，开发了稳健的对应匹配算法和自动标注 pipeline。

Result: 通过实验验证，该方法在标定精度上表现优异，减少了人工注释的需求，为多模态感知系统的开发提供了加速。

Conclusion: 4D-CAAL框架实现了高精度的4D雷达与摄像头标定，显著减少了人工标注的工作量，促进了自主驾驶多模态感知系统的开发。

Abstract: 4D radar has emerged as a critical sensor for autonomous driving, primarily due to its enhanced capabilities in elevation measurement and higher resolution compared to traditional 3D radar. Effective integration of 4D radar with cameras requires accurate extrinsic calibration, and the development of radar-based perception algorithms demands large-scale annotated datasets. However, existing calibration methods often employ separate targets optimized for either visual or radar modalities, complicating correspondence establishment. Furthermore, manually labeling sparse radar data is labor-intensive and unreliable. To address these challenges, we propose 4D-CAAL, a unified framework for 4D radar-camera calibration and auto-labeling. Our approach introduces a novel dual-purpose calibration target design, integrating a checkerboard pattern on the front surface for camera detection and a corner reflector at the center of the back surface for radar detection. We develop a robust correspondence matching algorithm that aligns the checkerboard center with the strongest radar reflection point, enabling accurate extrinsic calibration. Subsequently, we present an auto-labeling pipeline that leverages the calibrated sensor relationship to transfer annotations from camera-based segmentations to radar point clouds through geometric projection and multi-feature optimization. Extensive experiments demonstrate that our method achieves high calibration accuracy while significantly reducing manual annotation effort, thereby accelerating the development of robust multi-modal perception systems for autonomous driving.

</details>


### [37] [DexTac: Learning Contact-aware Visuotactile Policies via Hand-by-hand Teaching](https://arxiv.org/abs/2601.21474)
*Xingyu Zhang,Chaofan Zhang,Boyue Zhang,Zhinan Peng,Shaowei Cui,Shuo Wang*

Main category: cs.RO

TL;DR: DexTac是一个基于动觉教学的视觉-触觉操控学习框架，提升了触觉信息的捕获及在复杂互动中的自我优化接触区域能力。


<details>
  <summary>Details</summary>
Motivation: 现有的灵巧操作数据收集和技能学习系统存在触觉信息维度低的问题，影响了有效的触摸感知运动生成。

Method: DexTac框架通过动觉教学直接从人类演示捕获多维触觉数据，并将这些数据整合进策略网络中。

Result: DexTac在单手注射任务中的成功率达到91.67%，且在高精度小规模注射器场景下，优于仅依赖力的信息方案31.67%。

Conclusion: 从人类演示中学习多维触觉优先级对于在接触丰富环境中实现稳健人类般灵巧操作至关重要。

Abstract: For contact-intensive tasks, the ability to generate policies that produce comprehensive tactile-aware motions is essential. However, existing data collection and skill learning systems for dexterous manipulation often suffer from low-dimensional tactile information. To address this limitation, we propose DexTac, a visuo-tactile manipulation learning framework based on kinesthetic teaching. DexTac captures multi-dimensional tactile data-including contact force distributions and spatial contact regions-directly from human demonstrations. By integrating these rich tactile modalities into a policy network, the resulting contact-aware agent enables a dexterous hand to autonomously select and maintain optimal contact regions during complex interactions. We evaluate our framework on a challenging unimanual injection task. Experimental results demonstrate that DexTac achieves a 91.67% success rate. Notably, in high-precision scenarios involving small-scale syringes, our approach outperforms force-only baselines by 31.67%. These results underscore that learning multi-dimensional tactile priors from human demonstrations is critical for achieving robust, human-like dexterous manipulation in contact-rich environments.

</details>


### [38] [Don't double it: Efficient Agent Prediction in Occlusions](https://arxiv.org/abs/2601.21504)
*Anna Rothenhäusler,Markus Mazzola,Andreas Look,Raghu Rajan,Joschka Bödecker*

Main category: cs.RO

TL;DR: 本文提出的MatchInformer方法，通过整合匈牙利匹配和解耦代理朝向与运动，能改善被遮挡交通参与者的识别和轨迹预测，实验结果显示性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 为了解决自动驾驶车辆在识别被遮挡的行人或车辆时所面临的挑战，减少冗余占用预测以降低计算负担。

Method: 使用基于变压器的MatchInformer方法，结合匈牙利匹配算法进行训练，减少冗余，并将代理的朝向与运动解耦以改进轨迹预测。

Result: 在Waymo开放运动数据集上的实验表明，MatchInformer在推理被遮挡区域和轨迹预测的准确性方面优于现有方法。

Conclusion: MatchInformer方法通过整合匈牙利匹配，并提高轨迹预测的准确性和可解释性，能够有效解决自动驾驶车辆在处理被遮挡交通参与者时的挑战。

Abstract: Occluded traffic agents pose a significant challenge for autonomous vehicles, as hidden pedestrians or vehicles can appear unexpectedly, yet this problem remains understudied. Existing learning-based methods, while capable of inferring the presence of hidden agents, often produce redundant occupancy predictions where a single agent is identified multiple times. This issue complicates downstream planning and increases computational load. To address this, we introduce MatchInformer, a novel transformer-based approach that builds on the state-of-the-art SceneInformer architecture. Our method improves upon prior work by integrating Hungarian Matching, a state-of-the-art object matching algorithm from object detection, into the training process to enforce a one-to-one correspondence between predictions and ground truth, thereby reducing redundancy. We further refine trajectory forecasts by decoupling an agent's heading from its motion, a strategy that improves the accuracy and interpretability of predicted paths. To better handle class imbalances, we propose using the Matthews Correlation Coefficient (MCC) to evaluate occupancy predictions. By considering all entries in the confusion matrix, MCC provides a robust measure even in sparse or imbalanced scenarios. Experiments on the Waymo Open Motion Dataset demonstrate that our approach improves reasoning about occluded regions and produces more accurate trajectory forecasts than prior methods.

</details>


### [39] [IROS: A Dual-Process Architecture for Real-Time VLM-Based Indoor Navigation](https://arxiv.org/abs/2601.21506)
*Joonhee Lee,Hyunseung Shin,Jeonggil Ko*

Main category: cs.RO

TL;DR: IROS是一个结合视觉语言模型和轻量感知模块的实时室内导航框架，能够高效处理复杂的语义理解与决策。


<details>
  <summary>Details</summary>
Motivation: 现有室内移动机器人导航方法在反应速度和语义理解上存在不足，需要有效的解决方案来平衡这两者。

Method: IROS框架基于双过程理论，将快速反应决策与慢速深思熟虑推理分开，仅在必要时调用视觉语言模型，同时增强紧凑视觉语言模型与空间及文本线索的结合，实现低延迟的导航。

Result: IROS框架结合了视觉语言模型的上下文推理与轻量感知模块的效率，旨在在低成本硬件上实现实时室内导航。

Conclusion: IROS在五个真实建筑中相比于基于持续视觉语言模型的导航提高了决策准确性，并将延迟减少了66%。

Abstract: Indoor mobile robot navigation requires fast responsiveness and robust semantic understanding, yet existing methods struggle to provide both. Classical geometric approaches such as SLAM offer reliable localization but depend on detailed maps and cannot interpret human-targeted cues (e.g., signs, room numbers) essential for indoor reasoning. Vision-Language-Action (VLA) models introduce semantic grounding but remain strictly reactive, basing decisions only on visible frames and failing to anticipate unseen intersections or reason about distant textual cues. Vision-Language Models (VLMs) provide richer contextual inference but suffer from high computational latency, making them unsuitable for real-time operation on embedded platforms. In this work, we present IROS, a real-time navigation framework that combines VLM-level contextual reasoning with the efficiency of lightweight perceptual modules on low-cost, on-device hardware. Inspired by Dual Process Theory, IROS separates fast reflexive decisions (System One) from slow deliberative reasoning (System Two), invoking the VLM only when necessary. Furthermore, by augmenting compact VLMs with spatial and textual cues, IROS delivers robust, human-like navigation with minimal latency. Across five real-world buildings, IROS improves decision accuracy and reduces latency by 66% compared to continuous VLM-based navigation.

</details>


### [40] [Training slow silicon neurons to control extremely fast robots with spiking reinforcement learning](https://arxiv.org/abs/2601.21548)
*Irene Ambrosini,Ingo Blakowski,Dmitrii Zendrikov,Cristiano Capone,Luna Gava,Giacomo Indiveri,Chiara De Luca,Chiara Bartolozzi*

Main category: cs.RO

TL;DR: 本研究开发了一种紧凑的脉冲神经元网络，通过强化学习在极少的实验中实现了高效的空气曲棍球控制，展示了神经倾向硬件在机器人系统中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 空气曲棍球需要在高速度下进行快速决策，这对传统系统提出了挑战，因此开发一个高效的学习系统对于机器人自主控制至关重要。

Method: 设计了一种在混合信号模拟/数字神经形态处理器上运行的脉冲神经元网络，结合强化学习和事件驱动活动来进行快速有效的学习。

Result: 系统在真实时间学习中表现出色，通过计算机与神经形态芯片的联合，实现了对脉冲神经网络的有效训练。

Conclusion: 本研究展示了神经科学启发的硬件与现实世界机器人控制的结合，证明了基于大脑的解决方案能够应对快速互动任务，并支持智能机器的持续学习。

Abstract: Air hockey demands split-second decisions at high puck velocities, a challenge we address with a compact network of spiking neurons running on a mixed-signal analog/digital neuromorphic processor. By co-designing hardware and learning algorithms, we train the system to achieve successful puck interactions through reinforcement learning in a remarkably small number of trials. The network leverages fixed random connectivity to capture the task's temporal structure and adopts a local e-prop learning rule in the readout layer to exploit event-driven activity for fast and efficient learning. The result is real-time learning with a setup comprising a computer and the neuromorphic chip in-the-loop, enabling practical training of spiking neural networks for robotic autonomous systems. This work bridges neuroscience-inspired hardware with real-world robotic control, showing that brain-inspired approaches can tackle fast-paced interaction tasks while supporting always-on learning in intelligent machines.

</details>


### [41] [AIR-VLA: Vision-Language-Action Systems for Aerial Manipulation](https://arxiv.org/abs/2601.21602)
*Jianli Sun,Bin Tian,Qiyao Zhang,Chengxiang Li,Zihan Song,Zhiyong Cui,Yisheng Lv,Yonglin Tian*

Main category: cs.RO

TL;DR: 提出了AIR-VLA基准，针对航空操控系统，验证了现有VLA模型在此领域的可行性和局限性。


<details>
  <summary>Details</summary>
Motivation: 探索Vision-Language-Action模型在航空操控系统中的适用性，填补当前研究的空白。

Method: 建立一个物理基础的模拟环境并发布包含3000个高质量多模态数据集的AIR-VLA基准，专门用于航空操控。

Result: 成功验证了将VLA范式转移到航空系统的可行性，并揭示了当前模型在UAV机动性、操纵器控制和高层次规划方面的能力和局限性。

Conclusion: AIR-VLA为未来航天机器人研究建立了标准化的测试平台和数据基础，促进了航空操控系统的研究。

Abstract: While Vision-Language-Action (VLA) models have achieved remarkable success in ground-based embodied intelligence, their application to Aerial Manipulation Systems (AMS) remains a largely unexplored frontier. The inherent characteristics of AMS, including floating-base dynamics, strong coupling between the UAV and the manipulator, and the multi-step, long-horizon nature of operational tasks, pose severe challenges to existing VLA paradigms designed for static or 2D mobile bases. To bridge this gap, we propose AIR-VLA, the first VLA benchmark specifically tailored for aerial manipulation. We construct a physics-based simulation environment and release a high-quality multimodal dataset comprising 3000 manually teleoperated demonstrations, covering base manipulation, object & spatial understanding, semantic reasoning, and long-horizon planning. Leveraging this platform, we systematically evaluate mainstream VLA models and state-of-the-art VLM models. Our experiments not only validate the feasibility of transferring VLA paradigms to aerial systems but also, through multi-dimensional metrics tailored to aerial tasks, reveal the capabilities and boundaries of current models regarding UAV mobility, manipulator control, and high-level planning. AIR-VLA establishes a standardized testbed and data foundation for future research in general-purpose aerial robotics. The resource of AIR-VLA will be available at https://anonymous.4open.science/r/AIR-VLA-dataset-B5CC/.

</details>


### [42] [From Instruction to Event: Sound-Triggered Mobile Manipulation](https://arxiv.org/abs/2601.21667)
*Hao Ju,Shaofei Huang,Hongyu Li,Zihan Ding,Si Liu,Meng Wang,Zhedong Zheng*

Main category: cs.RO

TL;DR: 引入声触发的移动操控概念，改善传统指令驱动的机器人性能。


<details>
  <summary>Details</summary>
Motivation: 当前移动操控研究受限于指令驱动范式，限制了代理的自主性和对动态事件的反应能力。

Method: 开发Habitat-Echo数据平台，集成声学渲染与物理交互，提出高层任务规划与低层策略模型的基线。

Result: 实验表明，该基线使代理能够主动探测和响应听觉事件，在动态双源场景中成功隔离主要声源并执行交互。

Conclusion: 声触发移动操控验证了基线的鲁棒性，减轻了对逐个指令的依赖。

Abstract: Current mobile manipulation research predominantly follows an instruction-driven paradigm, where agents rely on predefined textual commands to execute tasks. However, this setting confines agents to a passive role, limiting their autonomy and ability to react to dynamic environmental events. To address these limitations, we introduce sound-triggered mobile manipulation, where agents must actively perceive and interact with sound-emitting objects without explicit action instructions. To support these tasks, we develop Habitat-Echo, a data platform that integrates acoustic rendering with physical interaction. We further propose a baseline comprising a high-level task planner and low-level policy models to complete these tasks. Extensive experiments show that the proposed baseline empowers agents to actively detect and respond to auditory events, eliminating the need for case-by-case instructions. Notably, in the challenging dual-source scenario, the agent successfully isolates the primary source from overlapping acoustic interference to execute the first interaction, and subsequently proceeds to manipulate the secondary object, verifying the robustness of the baseline.

</details>


### [43] [CoFreeVLA: Collision-Free Dual-Arm Manipulation via Vision-Language-Action Model and Risk Estimation](https://arxiv.org/abs/2601.21712)
*Xuanran Zhai,Binkai Ou,Yemin Wang,Hui Yi Leong,Qiaojun Yu,Ce Hao,Yaohua Liu*

Main category: cs.RO

TL;DR: CoFreeVLA通过自我碰撞风险估计提升了双臂操作的安全性和成功率。


<details>
  <summary>Details</summary>
Motivation: 双臂操控在遵循指令执行时面临自我碰撞的问题，现有模型未能充分考虑这一点。

Method: 通过增添短期自我碰撞风险估计器，利用本体感知、视觉嵌入和规划动作预测碰撞可能性，并在五个双臂任务中应用于PiPER机器人手臂。

Result: CoFreeVLA减少了自我碰撞，改善了在五个双臂任务中的成功率。

Conclusion: CoFreeVLA在减少自我碰撞并提高成功率方面优于RDT和APEX。

Abstract: Vision Language Action (VLA) models enable instruction following manipulation, yet dualarm deployment remains unsafe due to under modeled selfcollisions between arms and grasped objects. We introduce CoFreeVLA, which augments an endtoend VLA with a short horizon selfcollision risk estimator that predicts collision likelihood from proprioception, visual embeddings, and planned actions. The estimator gates risky commands, recovers to safe states via risk-guided adjustments, and shapes policy refinement for safer rollouts. It is pre-trained with model-based collision labels and posttrained on real robot rollouts for calibration. On five bimanual tasks with the PiPER robot arm, CoFreeVLA reduces selfcollisions and improves success rates versus RDT and APEX.

</details>


### [44] [Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations](https://arxiv.org/abs/2601.21713)
*Donatien Delehelle,Fei Chen,Darwin Caldwell*

Main category: cs.RO

TL;DR: 本文提出了一种高效的增强学习方法用于布料操控，优化设计选择使模型更小、训练时间更短，同时在SoftGym基准上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 布料操控在日常生活中普遍存在，但由于状态空间维度高、动态复杂及自遮挡问题，机器人操控面临挑战。

Method: 本研究提出了一种模块化的增强学习方法，通过在模拟环境中进行有效设计，减少模型规模和训练时间，同时确保在现实世界中的有效迁移。

Result: 本文探讨了布料操控在机器人领域的挑战，并提出了一种有效的增强学习(RL)方法来解决此问题。通过优化模型设计和选择，显著降低了模型的规模和训练时间，同时在SoftGym基准上取得了显著的性能提升。

Conclusion: 通过高效的设计选择，作者提出的方法在模拟学习中显著提升了操作性能，并在真实环境中实现了良好的迁移效果，表明了璃模拟学习的潜力。

Abstract: Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model.

</details>


### [45] [Flocking behavior for dynamic and complex swarm structures](https://arxiv.org/abs/2601.21772)
*Carmen D. R. Pita-Romero,Pedro Arias-Perez,Miguel Fernandez-Cortizas,Rafael Perez-Segui,Pascual Campoy*

Main category: cs.RO

TL;DR: 提出了一种基于虚拟质心的无人机编队算法，能有效维护复杂结构和轨迹，经过实验验证其简单有效。


<details>
  <summary>Details</summary>
Motivation: 维护多无人机的复杂结构并实现复杂轨迹的控制是一个主要挑战，本文旨在解决该问题。

Method: 基于虚拟质心的算法，结合经典的虚拟行为，对无人机编队的数量和形式进行动态控制。

Result: 通过仿真测试和实际实验，证明了该算法在处理复杂队形和轨迹时的有效性。

Conclusion: 该算法在实现复杂队形和轨迹控制方面表现出色，证明了其有效性和简便性。

Abstract: Maintaining the formation of complex structures with multiple UAVs and achieving complex trajectories remains a major challenge. This work presents an algorithm for implementing the flocking behavior of UAVs based on the concept of Virtual Centroid to easily develop a structure for the flock. The approach builds on the classical virtual-based behavior, providing a theoretical framework for incorporating enhancements to dynamically control both the number of agents and the formation of the structure. Simulation tests and real-world experiments were conducted, demonstrating its simplicity even with complex formations and complex trajectories.

</details>


### [46] [GAZELOAD A Multimodal Eye-Tracking Dataset for Mental Workload in Industrial Human-Robot Collaboration](https://arxiv.org/abs/2601.21829)
*Bsher Karbouj,Baha Eddin Gaaloul,Jorg Kruger*

Main category: cs.RO

TL;DR: 本研究提供了 GAZELOAD 数据集，用于心理负荷估计，包含时间同步的眼动与环境数据，适用于工业人机协作场景。


<details>
  <summary>Details</summary>
Motivation: 为了量化工业人机协作中的心理负荷，并研究环境因素对眼动标记的影响。

Method: 通过在实验室装配测试平台上收集参与者与协作机器人互动时的眼动追踪数据与环境实时测量，时间同步进行。

Result: 生成的 GALELOAD 数据集包含眼动指标、环境记录和自我报告的心理负荷评分，可用于算法基准测试。

Conclusion: GAZELOAD 数据集为工业人机协作中的心理负荷估计提供了多模态数据，适用于算法开发与环境因素研究。

Abstract: This article describes GAZELOAD, a multimodal dataset for mental workload estimation in industrial human-robot collaboration. The data were collected in a laboratory assembly testbed where 26 participants interacted with two collaborative robots (UR5 and Franka Emika Panda) while wearing Meta ARIA smart glasses. The dataset time-synchronizes eye-tracking signals (pupil diameter, fixations, saccades, eye gaze, gaze transition entropy, fixation dispersion index) with environmental real-time and continuous measurements (illuminance) and task and robot context (bench, task block, induced faults), under controlled manipulations of task difficulty and ambient conditions. For each participant and workload-graded task block, we provide CSV files with ocular metrics aggregated into 250 ms windows, environmental logs, and self-reported mental workload ratings on a 1-10 Likert scale, organized in participant-specific folders alongside documentation. These data can be used to develop and benchmark algorithms for mental workload estimation, feature extraction, and temporal modeling in realistic industrial HRC scenarios, and to investigate the influence of environmental factors such as lighting on eye-based workload markers.

</details>


### [47] [LLM-Driven Scenario-Aware Planning for Autonomous Driving](https://arxiv.org/abs/2601.21876)
*He Li,Zhaowei Chen,Rui Gao,Guoliang Li,Qi Hao,Shuai Wang,Chengzhong Xu*

Main category: cs.RO

TL;DR: 本文提出了一种大型语言模型驱动的自适应规划方法（LAP），能在不同复杂度场景中高效切换驾驶模式，从而提升自动驾驶的效率与安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的混合规划器切换框架在密集交通中难以实现高效的驾驶与安全的机动，亟需改进。

Method: 通过树搜索模型预测控制和交替最小化解决联合优化问题，结合大型语言模型进行场景理解和模式配置、运动规划的联合优化。

Result: 高保真模拟结果显示，LAP在行驶时间和成功率上都超过了其他基准。

Conclusion: LAP在行驶时间和成功率上优于其他基准，证明了其在自动驾驶中的有效性。

Abstract: Hybrid planner switching framework (HPSF) for autonomous driving needs to reconcile high-speed driving efficiency with safe maneuvering in dense traffic. Existing HPSF methods often fail to make reliable mode transitions or sustain efficient driving in congested environments, owing to heuristic scene recognition and low-frequency control updates. To address the limitation, this paper proposes LAP, a large language model (LLM) driven, adaptive planning method, which switches between high-speed driving in low-complexity scenes and precise driving in high-complexity scenes, enabling high qualities of trajectory generation through confined gaps. This is achieved by leveraging LLM for scene understanding and integrating its inference into the joint optimization of mode configuration and motion planning. The joint optimization is solved using tree-search model predictive control and alternating minimization. We implement LAP by Python in Robot Operating System (ROS). High-fidelity simulation results show that the proposed LAP outperforms other benchmarks in terms of both driving time and success rate.

</details>


### [48] [Multi-Modular MANTA-RAY: A Modular Soft Surface Platform for Distributed Multi-Object Manipulation](https://arxiv.org/abs/2601.21884)
*Pratik Ingle,Jørn Lambertsen,Kasper Støy,Andres Faina*

Main category: cs.RO

TL;DR: MANTA-RAY平台通过分布式模块降低致动器密度，提高可扩展性，实现有效操控多种对象，具有实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 虽然密集的致动器阵列可以产生复杂的变形，但它们也带来了高自由度，增加了系统复杂性并限制了可扩展性。

Method: 通过分布式、模块化和可扩展的MANTA-RAY平台，使用物体在模块之间传递，并结合几何变换驱动的PID控制器，直接将倾斜角度控制输出映射到致动器指令。

Result: 该系统成功操控不同几何形状、质量和纹理的物体，包括脆弱物品（如鸡蛋和苹果），同时实现并行操作。

Conclusion: 多模块MANTA-RAY平台提高了可扩展性，并能够在更广泛的区域内协调操作多个物体，展示了其在实际应用中的潜力。

Abstract: Manipulation surfaces control objects by actively deforming their shape rather than directly grasping them. While dense actuator arrays can generate complex deformations, they also introduce high degrees of freedom (DOF), increasing system complexity and limiting scalability. The MANTA-RAY (Manipulation with Adaptive Non-rigid Textile Actuation with Reduced Actuation densitY) platform addresses these challenges by leveraging a soft, fabric-based surface with reduced actuator density to manipulate fragile and heterogeneous objects. Previous studies focused on single-module implementations supported by four actuators, whereas the feasibility and benefits of a scalable, multi-module configuration remain unexplored. In this work, we present a distributed, modular, and scalable variant of the MANTA-RAY platform that maintains manipulation performance with a reduced actuator density. The proposed multi-module MANTA-RAY platform and control strategy employs object passing between modules and a geometric transformation driven PID controller that directly maps tilt-angle control outputs to actuator commands, eliminating the need for extensive data-driven or black-box training. We evaluate system performance in simulation across surface configurations of varying modules (3x3 and 4x4) and validate its feasibility through experiments on a physical 2x2 hardware prototype. The system successfully manipulates objects with diverse geometries, masses, and textures including fragile items such as eggs and apples as well as enabling parallel manipulation. The results demonstrate that the multi-module MANTA-RAY improves scalability and enables coordinated manipulation of multiple objects across larger areas, highlighting its potential for practical, real-world applications.

</details>


### [49] [Information Filtering via Variational Regularization for Robot Manipulation](https://arxiv.org/abs/2601.21926)
*Jinhao Zhang,Wenlong Xia,Yaojia Wang,Zhexuan Zhou,Huizhe Li,Yichen Lai,Haoming Song,Youmin Gong,Jie Me*

Main category: cs.RO

TL;DR: 采用变分正则化(VR)模块改善了去噪效果，在多个仿真基准上达到了新的最先进成果。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用过大的去噪解码器，虽然增加模型容量可以改善去噪，但也会引入冗余和噪声。我们发现随机掩码在推断时可以改善性能。

Method: 提出了变分正则化(VR)，这是一个轻量模块，施加时间步条件的高斯分布于主干特征，并应用KL散度正则化，形成自适应信息瓶颈。

Result: 在三个仿真基准测试上进行了广泛实验，证明了我们的方法优于基线DP3，并在实际部署中表现良好。

Conclusion: 我们的方案在RoboTwin2.0上改善了6.1%的成功率，在Adroit和MetaWorld上改善了4.1%，达到了新的最先进结果。

Abstract: Diffusion-based visuomotor policies built on 3D visual representations have achieved strong performance in learning complex robotic skills. However, most existing methods employ an oversized denoising decoder. While increasing model capacity can improve denoising, empirical evidence suggests that it also introduces redundancy and noise in intermediate feature blocks. Crucially, we find that randomly masking backbone features at inference time (without changing training) can improve performance, confirming the presence of task-irrelevant noise in intermediate features. To this end, we propose Variational Regularization (VR), a lightweight module that imposes a timestep-conditioned Gaussian over backbone features and applies a KL-divergence regularizer, forming an adaptive information bottleneck. Extensive experiments on three simulation benchmarks (RoboTwin2.0, Adroit, and MetaWorld) show that, compared to the baseline DP3, our approach improves the success rate by 6.1% on RoboTwin2.0 and by 4.1% on Adroit and MetaWorld, achieving new state-of-the-art results. Real-world experiments further demonstrate that our method performs well in practical deployments. Code will released.

</details>


### [50] [MoE-ACT: Improving Surgical Imitation Learning Policies through Supervised Mixture-of-Experts](https://arxiv.org/abs/2601.21971)
*Lorenzo Mazza,Ariel Rodriguez,Rayan Younis,Martin Lelis,Ortrun Hellig,Chenpan Li,Sebastian Bodenstedt,Martin Wagner,Stefanie Speidel*

Main category: cs.RO

TL;DR: 提出监督的MoE架构，能使轻量级行动编码器在外科操作任务中从少量演示中学习，显著提高成功率并具备在实际应用中的潜力。


<details>
  <summary>Details</summary>
Motivation: 仿效学习在机器人操作领域取得成功，但在外科机器人应用上由于数据匮乏和安全性要求等挑战，需探索新的解决方案。

Method: 提出了一种监督的专家混合架构（MoE）用于相位结构的外科操作任务，该架构可在任何自主策略之上进行添加。

Result: 使用轻量级的动作编码器策略（如动作分块变换器ACT），可以从少于150个演示中仅依靠立体内窥镜图像学习复杂的长时间操作。

Conclusion: 该方法在外科助手机器人上表现优越，具有更高的成功率和更强的鲁棒性，能够执行复杂的外科任务，并成功转移到未见测试视角和无额外训练的猪体组织上。

Abstract: Imitation learning has achieved remarkable success in robotic manipulation, yet its application to surgical robotics remains challenging due to data scarcity, constrained workspaces, and the need for an exceptional level of safety and predictability. We present a supervised Mixture-of-Experts (MoE) architecture designed for phase-structured surgical manipulation tasks, which can be added on top of any autonomous policy. Unlike prior surgical robot learning approaches that rely on multi-camera setups or thousands of demonstrations, we show that a lightweight action decoder policy like Action Chunking Transformer (ACT) can learn complex, long-horizon manipulation from less than 150 demonstrations using solely stereo endoscopic images, when equipped with our architecture. We evaluate our approach on the collaborative surgical task of bowel grasping and retraction, where a robot assistant interprets visual cues from a human surgeon, executes targeted grasping on deformable tissue, and performs sustained retraction. We benchmark our method against state-of-the-art Vision-Language-Action (VLA) models and the standard ACT baseline. Our results show that generalist VLAs fail to acquire the task entirely, even under standard in-distribution conditions. Furthermore, while standard ACT achieves moderate success in-distribution, adopting a supervised MoE architecture significantly boosts its performance, yielding higher success rates in-distribution and demonstrating superior robustness in out-of-distribution scenarios, including novel grasp locations, reduced illumination, and partial occlusions. Notably, it generalizes to unseen testing viewpoints and also transfers zero-shot to ex vivo porcine tissue without additional training, offering a promising pathway toward in vivo deployment. To support this, we present qualitative preliminary results of policy roll-outs during in vivo porcine surgery.

</details>


### [51] [Macro-Scale Electrostatic Origami Motor](https://arxiv.org/abs/2601.21976)
*Alex S. Miller,Leo McElroy,Jeffrey H. Lang*

Main category: cs.RO

TL;DR: 本研究开发了首个宏观折叠连续旋转电机，能够折叠并展开操作，使用电晕放电产生扭矩，展现出优异性能。


<details>
  <summary>Details</summary>
Motivation: 折叠机器人在体积与质量比、便携性和形状适应性方面具有优势，然而目前缺乏宏观折叠连续旋转驱动器，限制了其应用。

Method: 开发了一种宏观规模的折叠式旋转电机，采用电晕放电作为扭矩源，能够折叠平放并展开运行。

Result: 原型电机达到2.5:1的扩展比，在-29 kV驱动下达到1440 rpm的最高转速，并展现超0.15 mN m的最大输出扭矩和0.04 Nm/kg的扭矩密度。

Conclusion: 本研究的折叠旋转电机展示了在折叠机器人中的潜在应用，具有实际操作的可行性和高效性。

Abstract: Foldable robots have been an active area of robotics research due to their high volume-to-mass ratio, easy packability, and shape adaptability. For locomotion, previously developed foldable robots have either embedded linear actuators in, or attached non-folding rotary motors to, their structure. Further, those actuators directly embedded in the structure of the folding medium all contributed to linear or folding motion, not to continuous rotary motion. On the macro-scale there has not yet been a folding continuous rotary actuator. This paper details the development and testing of the first macro-scale origami rotary motor that can be folded flat, and then unfurled to operate. Using corona discharge for torque production, the prototype motor achieved an expansion ratio of 2.5:1, reached a top speed of 1440 rpm when driven at -29 kV, and exhibited a maximum output torque over 0.15 mN m with an active component torque density of 0.04 Nm/kg.

</details>


### [52] [PocketDP3: Efficient Pocket-Scale 3D Visuomotor Policy](https://arxiv.org/abs/2601.22018)
*Jinhao Zhang,Zhexuan Zhou,Huizhe Li,Yichen Lai,Wenlong Xia,Haoming Song,Youmin Gong,Jie Me*

Main category: cs.RO

TL;DR: PocketDP3是一种新的轻量级3D扩散策略，通过采用Diffusion Mixer大幅减少参数，并在多个仿真基准测试中实现了领先性能。


<details>
  <summary>Details</summary>
Motivation: 观察到现有3D视觉扩散策略中的解码器参数浪费，提出了一种适用于紧凑场景表示的新架构。

Method: 提出了PocketDP3，一种基于MLP-Mixer块的轻量级Diffusion Mixer（DiM），取代了之前方法中重型条件U-Net解码器，实现了参数高效融合。

Result: PocketDP3在RoboTwin2.0、Adroit和MetaWorld等三项基准测试中表现优异，支持双步推理且不损失性能。

Conclusion: PocketDP3在三个仿真基准上达到了最先进的性能，参数量不足1%，同时加快了推理速度。

Abstract: Recently, 3D vision-based diffusion policies have shown strong capability in learning complex robotic manipulation skills. However, a common architectural mismatch exists in these models: a tiny yet efficient point-cloud encoder is often paired with a massive decoder. Given a compact scene representation, we argue that this may lead to substantial parameter waste in the decoder. Motivated by this observation, we propose PocketDP3, a pocket-scale 3D diffusion policy that replaces the heavy conditional U-Net decoder used in prior methods with a lightweight Diffusion Mixer (DiM) built on MLP-Mixer blocks. This architecture enables efficient fusion across temporal and channel dimensions, significantly reducing model size. Notably, without any additional consistency distillation techniques, our method supports two-step inference without sacrificing performance, improving practicality for real-time deployment. Across three simulation benchmarks--RoboTwin2.0, Adroit, and MetaWorld--PocketDP3 achieves state-of-the-art performance with fewer than 1% of the parameters of prior methods, while also accelerating inference. Real-world experiments further demonstrate the practicality and transferability of our method in real-world settings. Code will be released.

</details>


### [53] [mjlab: A Lightweight Framework for GPU-Accelerated Robot Learning](https://arxiv.org/abs/2601.22074)
*Kevin Zakka,Qiayuan Liao,Brent Yi,Louis Le Lay,Koushil Sreenath,Pieter Abbeel*

Main category: cs.RO

TL;DR: mjlab是一个轻量级、易于安装且具备GPU加速的开源机器人学习框架，提供组合化环境和多种参考实现。


<details>
  <summary>Details</summary>
Motivation: 旨在降低机器人学习框架的安装和使用复杂性，提供快速访问MuJoCo数据结构的能力。

Method: 使用基于管理器的API，结合MuJoCo Warp进行物理加速，用户可以轻松组合观察、奖励和事件的模块化构建块。

Result: mjlab允许用户通过单一命令安装，并附带了多种参考实现，包括速度跟踪、动作模仿和操作任务。

Conclusion: mjlab提供了一种轻量级、开源的机器人学习框架，具备GPU加速的模拟和模块化环境，极大地简化了设置过程。

Abstract: We present mjlab, a lightweight, open-source framework for robot learning that combines GPU-accelerated simulation with composable environments and minimal setup friction. mjlab adopts the manager-based API introduced by Isaac Lab, where users compose modular building blocks for observations, rewards, and events, and pairs it with MuJoCo Warp for GPU-accelerated physics. The result is a framework installable with a single command, requiring minimal dependencies, and providing direct access to native MuJoCo data structures. mjlab ships with reference implementations of velocity tracking, motion imitation, and manipulation tasks.

</details>


### [54] [ReactEMG Stroke: Healthy-to-Stroke Few-shot Adaptation for sEMG-Based Intent Detection](https://arxiv.org/abs/2601.22090)
*Runsheng Wang,Katelyn Lee,Xinyue Zhu,Lauren Winterbottom,Dawn M. Nilsen,Joel Stein,Matei Ciocarlie*

Main category: cs.RO

TL;DR: 本研究提出了一个健康到中风的适应管道，通过健康人群的大规模sEMG模型初始化意图检测器，并仅使用少量个体数据进行微调，从而提高中风后康复的意图检测准确性。


<details>
  <summary>Details</summary>
Motivation: 研究希望通过利用健康人群的sEMG模型来提高中风后康复中对于意图检测的准确性，并减少传统方法中所需的个体校准时间。

Method: 通过将大规模的健康人群sEMG预训练模型初始化，并使用少量的中风参与者的特定数据进行微调。比较了不同的适应策略，包括头部调优、参数高效LoRA适配器以及全端到端的微调。

Result: 最好的适应方法使平均转换准确率从0.42提高至0.61，原始准确率从0.69提高至0.78，展示了健康领域EMG表征在中风意图检测中的有效性。

Conclusion: 健康预训练的适应方法显著提高了中风意图检测的准确性，降低了校准负担，并增强了实时检测的鲁棒性。

Abstract: Surface electromyography (sEMG) is a promising control signal for assist-as-needed hand rehabilitation after stroke, but detecting intent from paretic muscles often requires lengthy, subject-specific calibration and remains brittle to variability. We propose a healthy-to-stroke adaptation pipeline that initializes an intent detector from a model pretrained on large-scale able-bodied sEMG, then fine-tunes it for each stroke participant using only a small amount of subject-specific data. Using a newly collected dataset from three individuals with chronic stroke, we compare adaptation strategies (head-only tuning, parameter-efficient LoRA adapters, and full end-to-end fine-tuning) and evaluate on held-out test sets that include realistic distribution shifts such as within-session drift, posture changes, and armband repositioning. Across conditions, healthy-pretrained adaptation consistently improves stroke intent detection relative to both zero-shot transfer and stroke-only training under the same data budget; the best adaptation methods improve average transition accuracy from 0.42 to 0.61 and raw accuracy from 0.69 to 0.78. These results suggest that transferring a reusable healthy-domain EMG representation can reduce calibration burden while improving robustness for real-time post-stroke intent detection.

</details>


### [55] [DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation](https://arxiv.org/abs/2601.22153)
*Haozhe Xie,Beichen Wen,Jiarui Zheng,Zhaoxi Chen,Fangzhou Hong,Haiwen Diao,Ziwei Liu*

Main category: cs.RO

TL;DR: 动态操作仍然是VLA模型的挑战，DynamicVLA通过新框架和DOM基准提升了动态物体操作的速度与精准度。


<details>
  <summary>Details</summary>
Motivation: 尽管VLA模型在静态操作上表现良好，但在动态场景中却面临快速感知、时间预判和持续控制的挑战，因此需要一个新的框架来应对动态物体操作。

Method: 该框架采用了一种紧凑的0.4B VLA，利用卷积视觉编码器进行空间高效、结构上真实的编码，支持快速的多模态推理，同时实现重叠推理和执行，减少延迟，及时适应物体运动，以及通过时间对齐的行动执行弥合感知与执行之间的差距。

Result: 通过建立动态物体操作（DOM）基准，收集了200K个合成的动态操作场景和对象，进行了广泛的评估，表明DynamicVLA在多个方面表现出色。

Conclusion: DynamicVLA系统在动态物体操作方面表现出显著的反应速度、感知和泛化能力的提升，成为动态物体操作的统一框架。

Abstract: Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.

</details>
