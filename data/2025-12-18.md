<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 16]
- [cs.HC](#cs.HC) [Total: 9]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Breathe with Me: Synchronizing Biosignals for User Embodiment in Robots](https://arxiv.org/abs/2512.14952)
*Iddo Yehoshua Wald,Amber Maimon,Shiyao Zhang,Dennis Küster,Robert Porzel,Tanja Schultz,Rainer Malaka*

Main category: cs.RO

TL;DR: 本研究探讨了通过实时呼吸反馈增强用户在机器人系统中的身体认同感，发现与自身呼吸同步的机器人运动显著增加了身体所有权感。


<details>
  <summary>Details</summary>
Motivation: 近年来关于用户在机器人系统中的体现感研究逐渐增多，本研究旨在探讨如何通过呼吸反馈来增强这一体验。

Method: 进行了一项被试间实验，参与者在控制机器人手臂时，手臂的运动要么与他们的呼吸同步，要么不同步。

Result: 实验发现，呼吸同步显著增加了参与者的身体所有权感，并且大多数参与者更喜欢同步的体验。

Conclusion: 我们提出生理信号的表现作为人机交互的新内感知途径，探讨了其在远程存在、假肢、与机器人协作及共享自主性方面的应用潜力。

Abstract: Embodiment of users within robotic systems has been explored in human-robot interaction, most often in telepresence and teleoperation. In these applications, synchronized visuomotor feedback can evoke a sense of body ownership and agency, contributing to the experience of embodiment. We extend this work by employing embreathment, the representation of the user's own breath in real time, as a means for enhancing user embodiment experience in robots. In a within-subjects experiment, participants controlled a robotic arm, while its movements were either synchronized or non-synchronized with their own breath. Synchrony was shown to significantly increase body ownership, and was preferred by most participants. We propose the representation of physiological signals as a novel interoceptive pathway for human-robot interaction, and discuss implications for telepresence, prosthetics, collaboration with robots, and shared autonomy.

</details>


### [2] [ISS Policy : Scalable Diffusion Policy with Implicit Scene Supervision](https://arxiv.org/abs/2512.15020)
*Wenlong Xia,Jinhao Zhang,Ce Zhang,Yaojia Wang,Youmin Gong,Jie Mei*

Main category: cs.RO

TL;DR: 提出了一种新型的隐式场景监督策略，显著提高了基于视觉的模仿学习的训练效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决基于视觉的模仿学习中对物体外观的依赖，提升训练效率和泛化能力。

Method: 引入隐式场景监督（ISS）策略，基于3D视觉运动的DiT扩展，利用点云观察预测连续动作序列。

Result: 在单臂操控任务（MetaWorld）和灵巧手操控（Adroit）上实现了最先进的性能，并在真实实验中展现强大的泛化与鲁棒性。

Conclusion: 方法在数据和参数上具备良好的扩展性，代码和视频将会发布。

Abstract: Vision-based imitation learning has enabled impressive robotic manipulation skills, but its reliance on object appearance while ignoring the underlying 3D scene structure leads to low training efficiency and poor generalization. To address these challenges, we introduce \emph{Implicit Scene Supervision (ISS) Policy}, a 3D visuomotor DiT-based diffusion policy that predicts sequences of continuous actions from point cloud observations. We extend DiT with a novel implicit scene supervision module that encourages the model to produce outputs consistent with the scene's geometric evolution, thereby improving the performance and robustness of the policy. Notably, ISS Policy achieves state-of-the-art performance on both single-arm manipulation tasks (MetaWorld) and dexterous hand manipulation (Adroit). In real-world experiments, it also demonstrates strong generalization and robustness. Additional ablation studies show that our method scales effectively with both data and parameters. Code and videos will be released.

</details>


### [3] [HERO: Hierarchical Traversable 3D Scene Graphs for Embodied Navigation Among Movable Obstacles](https://arxiv.org/abs/2512.15047)
*Yunheng Wang,Yixiao Feng,Yuetong Fang,Shuning Zhang,Tan Jing,Jian Li,Xiangrui Jiang,Renjing Xu*

Main category: cs.RO

TL;DR: HERO提出了一种新的分层可行3D场景图框架，通过将可操作障碍建模为路径来解决传统方法的局限性，显著提高了导航效率和可达性。


<details>
  <summary>Details</summary>
Motivation: 传统的3D场景图方法基于静态世界假设，无法有效处理交互障碍，限制了在复杂环境中的导航能力。

Method: 提出HERO框架，通过构建层次化的可行3D场景图，将可操作障碍视为可通行的路径，重新定义可通行性。

Result: HERO在部分障碍环境中减少了35.1%的PL，在完全障碍环境中提高了79.4%的SR，表明其在效率和可达性方面显著优于基线。

Conclusion: HERO提供了一种新的方式来理解和实现智能导航，适应复杂和动态环境中的操作需求。

Abstract: 3D Scene Graphs (3DSGs) constitute a powerful representation of the physical world, distinguished by their abilities to explicitly model the complex spatial, semantic, and functional relationships between entities, rendering a foundational understanding that enables agents to interact intelligently with their environment and execute versatile behaviors. Embodied navigation, as a crucial component of such capabilities, leverages the compact and expressive nature of 3DSGs to enable long-horizon reasoning and planning in complex, large-scale environments. However, prior works rely on a static-world assumption, defining traversable space solely based on static spatial layouts and thereby treating interactable obstacles as non-traversable. This fundamental limitation severely undermines their effectiveness in real-world scenarios, leading to limited reachability, low efficiency, and inferior extensibility. To address these issues, we propose HERO, a novel framework for constructing Hierarchical Traversable 3DSGs, that redefines traversability by modeling operable obstacles as pathways, capturing their physical interactivity, functional semantics, and the scene's relational hierarchy. The results show that, relative to its baseline, HERO reduces PL by 35.1% in partially obstructed environments and increases SR by 79.4% in fully obstructed ones, demonstrating substantially higher efficiency and reachability.

</details>


### [4] [NAP3D: NeRF Assisted 3D-3D Pose Alignment for Autonomous Vehicles](https://arxiv.org/abs/2512.15080)
*Gaurav Bansal*

Main category: cs.RO

TL;DR: NAP3D 是一种新的 3D-3D 位姿对齐方法，利用神经辐射场（NeRF）来优化相机位姿，相比传统方法具有更好的几何一致性，且不依赖于回访已观察地点。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶的需求增加，准确定位变得尤为重要，而现有的定位方法在面对传感器噪声和长时间漂移时表现不佳。

Method: 引入 NeRF-Assisted 3D-3D Pose Alignment (NAP3D)，通过直接对齐观察场景的 3D 点与 NeRF 合成的点，来优化相机位姿。

Result: 引入了一种新的 3D-3D 位姿对齐方法 NAP3D，该方法使用深度图与预训练的神经辐射场（NeRF）之间的 3D-3D 对应关系来优化相机位姿。

Conclusion: NAP3D 在各种噪声条件下提升了 3D 对齐的准确性，为现有SLAM和定位管道提供了一种轻量级的补充工具。

Abstract: Accurate localization is essential for autonomous vehicles, yet sensor noise and drift over time can lead to significant pose estimation errors, particularly in long-horizon environments. A common strategy for correcting accumulated error is visual loop closure in SLAM, which adjusts the pose graph when the agent revisits previously mapped locations. These techniques typically rely on identifying visual mappings between the current view and previously observed scenes and often require fusing data from multiple sensors.
  In contrast, this work introduces NeRF-Assisted 3D-3D Pose Alignment (NAP3D), a complementary approach that leverages 3D-3D correspondences between the agent's current depth image and a pre-trained Neural Radiance Field (NeRF). By directly aligning 3D points from the observed scene with synthesized points from the NeRF, NAP3D refines the estimated pose even from novel viewpoints, without relying on revisiting previously observed locations.
  This robust 3D-3D formulation provides advantages over conventional 2D-3D localization methods while remaining comparable in accuracy and applicability. Experiments demonstrate that NAP3D achieves camera pose correction within 5 cm on a custom dataset, robustly outperforming a 2D-3D Perspective-N-Point baseline. On TUM RGB-D, NAP3D consistently improves 3D alignment RMSE by approximately 6 cm compared to this baseline given varying noise, despite PnP achieving lower raw rotation and translation parameter error in some regimes, highlighting NAP3D's improved geometric consistency in 3D space. By providing a lightweight, dataset-agnostic tool, NAP3D complements existing SLAM and localization pipelines when traditional loop closure is unavailable.

</details>


### [5] [BEV-Patch-PF: Particle Filtering with BEV-Aerial Feature Matching for Off-Road Geo-Localization](https://arxiv.org/abs/2512.15111)
*Dongmyeong Lee,Jesse Quattrociocchi,Christian Ellis,Rwik Rana,Amanda Adkins,Adam Uccello,Garrett Warnell,Joydeep Biswas*

Main category: cs.RO

TL;DR: 提出一种无GPS的顺序地理定位系统BEV-Patch-PF，结合了粒子滤波器与学习的鸟瞰视图和航空特征图，性能显著提高。


<details>
  <summary>Details</summary>
Motivation: 在无GPS环境下的精确地理定位需求，特别是对机器人在复杂环境中的导航支持。

Method: 通过基于RGB和深度图像构建鸟瞰特征图，结合粒子滤波算法，更好地匹配特征并计算每个粒子的对数似然。

Result: 在两个真实世界的越野数据集上，方法在已见和未见路径上分别实现7.5倍和7.0倍较低的绝对轨迹误差，同时在密集树冠和阴影下保持准确性。

Conclusion: 该系统在NVIDIA Tesla T4上以10 Hz的实时速度运行，适用于实际机器人部署。

Abstract: We propose BEV-Patch-PF, a GPS-free sequential geo-localization system that integrates a particle filter with learned bird's-eye-view (BEV) and aerial feature maps. From onboard RGB and depth images, we construct a BEV feature map. For each 3-DoF particle pose hypothesis, we crop the corresponding patch from an aerial feature map computed from a local aerial image queried around the approximate location. BEV-Patch-PF computes a per-particle log-likelihood by matching the BEV feature to the aerial patch feature. On two real-world off-road datasets, our method achieves 7.5x lower absolute trajectory error (ATE) on seen routes and 7.0x lower ATE on unseen routes than a retrieval-based baseline, while maintaining accuracy under dense canopy and shadow. The system runs in real time at 10 Hz on an NVIDIA Tesla T4, enabling practical robot deployment.

</details>


### [6] [EPSM: A Novel Metric to Evaluate the Safety of Environmental Perception in Autonomous Driving](https://arxiv.org/abs/2512.15195)
*Jörg Gamerdinger,Sven Teufel,Stephan Amann,Lukas Marc Listl,Oliver Bringmann*

Main category: cs.RO

TL;DR: 本研究提出了一种新的安全指标框架，以集成和评估智能车辆感知系统的安全性，强调传统评估方法的不足。


<details>
  <summary>Details</summary>
Motivation: 确保智能车辆在复杂驾驶场景中的安全性，评估感知系统的重要性。

Method: 提出一种新颖的安全指标，联合评估关键的感知任务——物体和车道检测，并集成轻量级物体安全指标和车道安全指标。

Result: 通过DeepAccident数据集，展示该方法能够识别传统性能指标无法捕捉的安全关键感知错误。

Conclusion: 强调以安全为中心的评估方法在自动驾驶感知系统中的重要性。

Abstract: Extensive evaluation of perception systems is crucial for ensuring the safety of intelligent vehicles in complex driving scenarios. Conventional performance metrics such as precision, recall and the F1-score assess the overall detection accuracy, but they do not consider the safety-relevant aspects of perception. Consequently, perception systems that achieve high scores in these metrics may still cause misdetections that could lead to severe accidents. Therefore, it is important to evaluate not only the overall performance of perception systems, but also their safety. We therefore introduce a novel safety metric for jointly evaluating the most critical perception tasks, object and lane detection. Our proposed framework integrates a new, lightweight object safety metric that quantifies the potential risk associated with object detection errors, as well as an lane safety metric including the interdependence between both tasks that can occur in safety evaluation. The resulting combined safety score provides a unified, interpretable measure of perception safety performance. Using the DeepAccident dataset, we demonstrate that our approach identifies safety critical perception errors that conventional performance metrics fail to capture. Our findings emphasize the importance of safety-centric evaluation methods for perception systems in autonomous driving.

</details>


### [7] [Infrastructure-based Autonomous Mobile Robots for Internal Logistics -- Challenges and Future Perspectives](https://arxiv.org/abs/2512.15215)
*Erik Brorsson,Kristian Ceder,Ze Zhang,Sabino Francesco Roselli,Endre Erős,Martin Dahl,Beatrice Alenljung,Jessica Lindblom,Thanh Bui,Emmanuel Dean,Lennart Svensson,Kristofer Bengtsson,Per-Lage Götvall,Knut Åkesson*

Main category: cs.RO

TL;DR: 本文综述了基础设施驱动的自主移动机器人系统，探讨技术及用户体验，并提出架构以推动未来发展


<details>
  <summary>Details</summary>
Motivation: 研究AMR系统在复杂工业环境中的应用，强调基础设施支持的重要性

Method: 提供一个基础架构结合基础设施传感、现场云计算和车载自主性

Result: 提出了基于基础设施的AMR系统的全面概述，总结了关键技术及用户体验评估

Conclusion: 为未来大型、稳健且人性化的AMR系统的发展奠定了基础

Abstract: The adoption of Autonomous Mobile Robots (AMRs) for internal logistics is accelerating, with most solutions emphasizing decentralized, onboard intelligence. While AMRs in indoor environments like factories can be supported by infrastructure, involving external sensors and computational resources, such systems remain underexplored in the literature. This paper presents a comprehensive overview of infrastructure-based AMR systems, outlining key opportunities and challenges. To support this, we introduce a reference architecture combining infrastructure-based sensing, on-premise cloud computing, and onboard autonomy. Based on the architecture, we review core technologies for localization, perception, and planning. We demonstrate the approach in a real-world deployment in a heavy-vehicle manufacturing environment and summarize findings from a user experience (UX) evaluation. Our aim is to provide a holistic foundation for future development of scalable, robust, and human-compatible AMR systems in complex industrial environments.

</details>


### [8] [VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments](https://arxiv.org/abs/2512.15258)
*Yuze Wu,Mo Zhu,Xingxing Li,Yuheng Du,Yuxin Fan,Wenjun Li,Xin Zhou,Fei Gao*

Main category: cs.RO

TL;DR: 本论文提出的VLA-AN框架有效提升无人机导航能力，解决现有模型的主要限制，具有较高的成功率和实时性能。


<details>
  <summary>Details</summary>
Motivation: 提出一种高效的视觉-语言-动作（VLA）框架，以解决现有大规模无人机导航模型的限制

Method: 构建高保真数据集，并引入渐进式三阶段训练框架和轻量级实时动作模块

Result: 在资源受限的无人机上实现了实时推理吞吐量的8.3倍提升，单任务成功率达到98.1%

Conclusion: VLA-AN显著改善空间定位、场景推理与长距离导航，提供轻量级无人机全链闭环自主的高效解决方案

Abstract: This paper proposes VLA-AN, an efficient and onboard Vision-Language-Action (VLA) framework dedicated to autonomous drone navigation in complex environments. VLA-AN addresses four major limitations of existing large aerial navigation models: the data domain gap, insufficient temporal navigation with reasoning, safety issues with generative action policies, and onboard deployment constraints. First, we construct a high-fidelity dataset utilizing 3D Gaussian Splatting (3D-GS) to effectively bridge the domain gap. Second, we introduce a progressive three-stage training framework that sequentially reinforces scene comprehension, core flight skills, and complex navigation capabilities. Third, we design a lightweight, real-time action module coupled with geometric safety correction. This module ensures fast, collision-free, and stable command generation, mitigating the safety risks inherent in stochastic generative policies. Finally, through deep optimization of the onboard deployment pipeline, VLA-AN achieves a robust real-time 8.3x improvement in inference throughput on resource-constrained UAVs. Extensive experiments demonstrate that VLA-AN significantly improves spatial grounding, scene reasoning, and long-horizon navigation, achieving a maximum single-task success rate of 98.1%, and providing an efficient, practical solution for realizing full-chain closed-loop autonomy in lightweight aerial robots.

</details>


### [9] [A Network-Based Framework for Modeling and Analyzing Human-Robot Coordination Strategies](https://arxiv.org/abs/2512.15282)
*Martijn IJtsma,Salvatore Hargis*

Main category: cs.RO

TL;DR: 提出一种新框架分析人机系统中的联合工作策略，强调协调需求的演变，支持设计阶段的合作能力要求推理。


<details>
  <summary>Details</summary>
Motivation: 随着更先进的机器人能力的部署，人机合作能力的需求增加，因此需要设计能够支持这些需求的人机系统，并明确理解工作功能及其约束。

Method: 提出一种新的计算框架，集成功能建模与图论表示，分析人机系统中的联合工作策略。

Result: 该框架通过系统功能之间的关系以及工作环境的物理和信息结构来表征集体工作，并明确捕捉协调需求如何随时间演变。

Conclusion: 该框架在灾难机器人领域的案例研究中得到了验证，能够支持早期的人机协调策略的探索，并识别支持灵活管理协调开销的合作能力。

Abstract: Studies of human-robot interaction in dynamic and unstructured environments show that as more advanced robotic capabilities are deployed, the need for cooperative competencies to support collaboration with human problem-holders increases. Designing human-robot systems to meet these demands requires an explicit understanding of the work functions and constraints that shape the feasibility of alternative joint work strategies. Yet existing human-robot interaction frameworks either emphasize computational support for real-time execution or rely on static representations for design, offering limited support for reasoning about coordination dynamics during early-stage conceptual design. To address this gap, this article presents a novel computational framework for analyzing joint work strategies in human-robot systems by integrating techniques from functional modeling with graph-theoretic representations. The framework characterizes collective work in terms of the relationships among system functions and the physical and informational structure of the work environment, while explicitly capturing how coordination demands evolve over time. Its use during conceptual design is demonstrated through a case study in disaster robotics, which shows how the framework can be used to support early trade-space exploration of human-robot coordination strategies and to identify cooperative competencies that support flexible management of coordination overhead. These results show how the framework makes coordination demands and their temporal evolution explicit, supporting design-time reasoning about cooperative competency requirements and work demands prior to implementation.

</details>


### [10] [GuangMing-Explorer: A Four-Legged Robot Platform for Autonomous Exploration in General Environments](https://arxiv.org/abs/2512.15309)
*Kai Zhang,Shoubin Chen,Dong Li,Baiyang Zhang,Tao Huang,Zehao Wu,Jiasheng Chen,Bo Zhang*

Main category: cs.RO

TL;DR: 本文介绍了GuangMing-Explorer，一个全面自主探索平台，展示其在复杂环境中的有效性。


<details>
  <summary>Details</summary>
Motivation: 目前关于完全自动化探测系统的整体和实用描述较少，而自主探索是集感知、规划、控制和运动执行于一体的基本能力，具有重要应用价值。

Method: 提出了GuangMing-Explorer，一个全面集成的自主探索平台，涵盖硬件设计、软件堆栈、算法部署和实验配置的系统架构。

Result: 通过广泛的实际实验，展示了该平台在执行自主探索任务中的有效性和效率，证明其在复杂和非结构化环境中的潜在应用。

Conclusion: GuangMing-Explorer平台展示了在多样化环境中强大的操作能力，并为实际部署提供了可能。

Abstract: Autonomous exploration is a fundamental capability that tightly integrates perception, planning, control, and motion execution. It plays a critical role in a wide range of applications, including indoor target search, mapping of extreme environments, resource exploration, etc. Despite significant progress in individual components, a holistic and practical description of a completely autonomous exploration system, encompassing both hardware and software, remains scarce. In this paper, we present GuangMing-Explorer, a fully integrated autonomous exploration platform designed for robust operation across diverse environments. We provide a comprehensive overview of the system architecture, including hardware design, software stack, algorithm deployment, and experimental configuration. Extensive real-world experiments demonstrate the platform's effectiveness and efficiency in executing autonomous exploration tasks, highlighting its potential for practical deployment in complex and unstructured environments.

</details>


### [11] [Remotely Detectable Robot Policy Watermarking](https://arxiv.org/abs/2512.15379)
*Michael Amir,Manon Flageat,Amanda Prorok*

Main category: cs.RO

TL;DR: 本文提出了一种名为Colored Noise Coherency (CoNoCo) 的水印策略，旨在通过远程检测方法保护机器人工智能的知识产权。


<details>
  <summary>Details</summary>
Motivation: 机器学习在真实世界机器人系统中的成功催生了新的知识产权形式：训练策略，因此需要新的方法来验证所有权并检测未经授权的使用。

Method: 使用一种称为Colored Noise Coherency (CoNoCo) 的水印策略，该策略通过嵌入光谱信号到机器人的动作中，利用策略的固有随机性来实现远程检测。

Result: 实验结果表明，CoNoCo在模拟和真实世界的机器人实验中，能够在各种远程检测方式下（包括动作捕捉和视频监控）实现强健的水印检测。

Conclusion: 本研究提供了一种新方法CoNoCo，用于通过远程观察验证机器学习训练策略的所有权，这在机器人领域中具有重要意义。

Abstract: The success of machine learning for real-world robotic systems has created a new form of intellectual property: the trained policy. This raises a critical need for novel methods that verify ownership and detect unauthorized, possibly unsafe misuse. While watermarking is established in other domains, physical policies present a unique challenge: remote detection. Existing methods assume access to the robot's internal state, but auditors are often limited to external observations (e.g., video footage). This ``Physical Observation Gap'' means the watermark must be detected from signals that are noisy, asynchronous, and filtered by unknown system dynamics. We formalize this challenge using the concept of a \textit{glimpse sequence}, and introduce Colored Noise Coherency (CoNoCo), the first watermarking strategy designed for remote detection. CoNoCo embeds a spectral signal into the robot's motions by leveraging the policy's inherent stochasticity. To show it does not degrade performance, we prove CoNoCo preserves the marginal action distribution. Our experiments demonstrate strong, robust detection across various remote modalities, including motion capture and side-way/top-down video footage, in both simulated and real-world robot experiments. This work provides a necessary step toward protecting intellectual property in robotics, offering the first method for validating the provenance of physical policies non-invasively, using purely remote observations.

</details>


### [12] [MiVLA: Towards Generalizable Vision-Language-Action Model with Human-Robot Mutual Imitation Pre-training](https://arxiv.org/abs/2512.15411)
*Zhenhan Yin,Xuanhan Wang,Jiahao Jiang,Kaiyuan Deng,Pengqi Chen,Shuangle Li,Chong Liu,Xing Xu,ingkuan Song,Lianli Gao,Heng Tao Shen*

Main category: cs.RO

TL;DR: 提出了一种名为MiVLA的模型，通过人机互模仿的预训练，提升了视觉-语言-行动模型的泛化能力，克服了视角、视觉外观和形态匹配的限制，显著提高了机器人控制任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-行动模型在泛化能力上受到相机视角、视觉外观和机器人形态不匹配的限制，因此需要有效的解决方案来提高其适应性。

Method: 提出MiVLA模型，利用人手与机器人手臂间的行为相似性进行互模仿预训练，采用运动学规则实现人类与机器人动作空间的双向对齐，并基于人类或模拟机器人演示训练行为轨迹预测和模仿。

Result: 在仿真平台上，MiVLA在机器人控制任务中提升了25%的表现，在真实世界任务中提高了14%，超越了现有的最先进视觉-语言-行动模型。

Conclusion: MiVLA模型有效整合了真实世界人数据的行为保真度与模拟机器人数据的操作多样性，从而增强了模型的泛化能力。

Abstract: While leveraging abundant human videos and simulated robot data poses a scalable solution to the scarcity of real-world robot data, the generalization capability of existing vision-language-action models (VLAs) remains limited by mismatches in camera views, visual appearance, and embodiment morphologies. To overcome this limitation, we propose MiVLA, a generalizable VLA empowered by human-robot mutual imitation pre-training, which leverages inherent behavioral similarity between human hands and robotic arms to build a foundation of strong behavioral priors for both human actions and robotic control. Specifically, our method utilizes kinematic rules with left/right hand coordinate systems for bidirectional alignment between human and robot action spaces. Given human or simulated robot demonstrations, MiVLA is trained to forecast behavior trajectories for one embodiment, and imitate behaviors for another one unseen in the demonstration. Based on this mutual imitation, it integrates the behavioral fidelity of real-world human data with the manipulative diversity of simulated robot data into a unified model, thereby enhancing the generalization capability for downstream tasks. Extensive experiments conducted on both simulation and real-world platforms with three robots (ARX, PiPer and LocoMan), demonstrate that MiVLA achieves strong improved generalization capability, outperforming state-of-the-art VLAs (e.g., $\boldsymbolπ_{0}$, $\boldsymbolπ_{0.5}$ and H-RDT) by 25% in simulation, and 14% in real-world robot control tasks.

</details>


### [13] [Load-Based Variable Transmission Mechanism for Robotic Applications](https://arxiv.org/abs/2512.15448)
*Sinan Emre,Victor Barasuol,Matteo Villa,Claudio Semini*

Main category: cs.RO

TL;DR: 提出了一种基于负载的可变传动机制，通过动态调整传动比来增强机器人驱动，能在不增加执行器的情况下有效放大关节扭矩。


<details>
  <summary>Details</summary>
Motivation: 旨在减少机器人关节驱动系统的复杂性，提高其适应性和效率。

Method: 通过预张紧弹簧和四杆联动机构被动地调节传动比，而不是需要额外的执行器进行主动控制。

Result: 在预设的扭矩阈值下，传动比提高了40%，并且在施加力超过18N时触发了扭矩放大效果。

Conclusion: 该研究为轻量、高效、自适应的机器人传动系统的发展做出了贡献，特别是在动态扭矩适应性至关重要的四足机器人应用中。

Abstract: This paper presents a Load-Based Variable Transmission (LBVT) mechanism designed to enhance robotic actuation by dynamically adjusting the transmission ratio in response to external torque demands. Unlike existing variable transmission systems that require additional actuators for active control, the proposed LBVT mechanism leverages a pre-tensioned spring and a four-bar linkage to passively modify the transmission ratio, thereby reducing the complexity of robot joint actuation systems. The effectiveness of the LBVT mechanism is evaluated through simulation-based analyses. The results confirm that the system achieves up to a 40 percent increase in transmission ratio upon reaching a predefined torque threshold, effectively amplifying joint torque when required without additional actuation. Furthermore, the simulations demonstrate a torque amplification effect triggered when the applied force exceeds 18 N, highlighting the system ability to autonomously respond to varying load conditions. This research contributes to the development of lightweight, efficient, and adaptive transmission systems for robotic applications, particularly in legged robots where dynamic torque adaptation is critical.

</details>


### [14] [OMCL: Open-vocabulary Monte Carlo Localization](https://arxiv.org/abs/2512.15557)
*Evgenii Kruzhkov,Raphael Memmesheimer,Sven Behnke*

Main category: cs.RO

TL;DR: 本文提出了一种新的机器人定位方法，通过视觉-语言特征结合多种传感器数据，增强了不同环境下的定位精度。


<details>
  <summary>Details</summary>
Motivation: 在不同传感器创建的环境地图中，机器人测量需要稳健地与地图特征关联，以实现有效的导航规划。

Method: 通过扩展蒙特卡罗定位，利用视觉-语言特征来计算视觉观测的可能性，并利用自然语言描述进行全局定位初始化。

Result: 扩展了蒙特卡罗定位，通过引入视觉-语言特征，提高了机器人在导航计划中的定位能力。

Conclusion: 该方法在室内场景（Matterport3D和Replica）和室外场景（SemanticKITTI）上进行了评估，并显示出良好的泛化能力。

Abstract: Robust robot localization is an important prerequisite for navigation planning. If the environment map was created from different sensors, robot measurements must be robustly associated with map features. In this work, we extend Monte Carlo Localization using vision-language features. These open-vocabulary features enable to robustly compute the likelihood of visual observations, given a camera pose and a 3D map created from posed RGB-D images or aligned point clouds. The abstract vision-language features enable to associate observations and map elements from different modalities. Global localization can be initialized by natural language descriptions of the objects present in the vicinity of locations. We evaluate our approach using Matterport3D and Replica for indoor scenes and demonstrate generalization on SemanticKITTI for outdoor scenes.

</details>


### [15] [An Open Toolkit for Underwater Field Robotics](https://arxiv.org/abs/2512.15597)
*Giacomo Picardi,Saverio Iacoponi,Matias Carandell,Jorge Aguirregomezcorta,Mrudul Chellapurath,Joaquin del Rio,Marcello Calisti,Iacopo Aguzzi*

Main category: cs.RO

TL;DR: 本文介绍了一种开放的水下操作研究工具包，旨在减少开发成本，并促进行业创新，经过广泛测试以验证其可靠性和多用途性。


<details>
  <summary>Details</summary>
Motivation: 随着水下机器人在海洋科学、环境监测和海底工业作业中的重要性日益增加，现有的水下操作和驱动系统的发展受高成本、专有设计及模块化研究硬件的有限获取影响。

Method: 开发一个包括水下机器人关节（URJ）、控制和电源管理电子元件及基于ROS2的软件栈在内的工具包，并公开所有设计文件，实现本地制造和社区驱动的改进。

Result: 提出了一种开放、经济实惠的水下操作研究工具包，包括深度评估的水下机器人关节（URJ），有早期泄漏检测功能，以及基于ROS2的软件栈，成功经过多项实验和现场测试，适用于多种水下开发应用。

Conclusion: 这个工作通过提供一个完全开放和经过现场测试的平台，旨在降低水下操作研究的进入门槛，提高可重复性，加速水下机器人领域的创新。

Abstract: Underwater robotics is becoming increasingly important for marine science, environmental monitoring, and subsea industrial operations, yet the development of underwater manipulation and actuation systems remains restricted by high costs, proprietary designs, and limited access to modular, research-oriented hardware. While open-source initiatives have democratized vehicle construction and control software, a substantial gap persists for joint-actuated systems-particularly those requiring waterproof, feedback-enabled actuation suitable for manipulators, grippers, and bioinspired devices. As a result, many research groups face lengthy development cycles, limited reproducibility, and difficulty transitioning laboratory prototypes to field-ready platforms.
  To address this gap, we introduce an open, cost-effective hardware and software toolkit for underwater manipulation research. The toolkit includes a depth-rated Underwater Robotic Joint (URJ) with early leakage detection, compact control and power management electronics, and a ROS2-based software stack for sensing and multi-mode actuation. All CAD models, fabrication files, PCB sources, firmware, and ROS2 packages are openly released, enabling local manufacturing, modification, and community-driven improvement.
  The toolkit has undergone extensive laboratory testing and multiple field deployments, demonstrating reliable operation up to 40 m depth across diverse applications, including a 3-DoF underwater manipulator, a tendon-driven soft gripper, and an underactuated sediment sampler. These results validate the robustness, versatility, and reusability of the toolkit for real marine environments.
  By providing a fully open, field-tested platform, this work aims to lower the barrier to entry for underwater manipulation research, improve reproducibility, and accelerate innovation in underwater field robotics.

</details>


### [16] [mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs](https://arxiv.org/abs/2512.15692)
*Jonas Pai,Liam Achenbach,Victoriano Montesinos,Benedek Forrai,Oier Mees,Elvis Nava*

Main category: cs.RO

TL;DR: 本文提出新的视频-动作模型（VAM），通过视频预训练改善机器人操作性能，样本效率提升10倍。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型（VLA）依赖于与物理动态和时间依赖性无关的静态网页数据进行预训练，导致机器人在执行操作时需推断复杂的物理关系。

Method: 提出了一个新的视频-动作模型（VAM），结合了预训练的大规模视频模型和基于流匹配的动作解码器，利用潜在表示生成低级机器人动作。

Result: 在模拟和真实世界的机器人操作任务中，该方法在样本效率上提升了10倍，收敛速度提升了2倍，达到了最新的性能水平。

Conclusion: 该研究表明，视频模型可以更有效地捕捉语义和视觉动态，相比传统的VLA架构显著改善了机器人的操作能力。

Abstract: Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data. As a result, despite improved semantic generalization, the policy must implicitly infer complex physical dynamics and temporal dependencies solely from robot trajectories. This reliance creates an unsustainable data burden, necessitating continuous, large-scale expert data collection to compensate for the lack of innate physical understanding. We contend that while vision-language pretraining effectively captures semantic priors, it remains blind to physical causality. A more effective paradigm leverages video to jointly capture semantics and visual dynamics during pretraining, thereby isolating the remaining task of low-level control. To this end, we introduce \model, a novel Video-Action Model (VAM) that pairs a pretrained Internet-scale video model with a flow matching-based action decoder conditioned on its latent representations. The decoder serves as an Inverse Dynamics Model (IDM), generating low-level robot actions from the latent representation of video-space action plans. Our extensive evaluation shows that our approach achieves state-of-the-art performance on simulated and real-world robotic manipulation tasks, improving sample efficiency by 10x and convergence speed by 2x compared to traditional VLA architectures.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [17] [Analyzing Social Media Claims regarding Youth Online Safety Features to Identify Problem Areas and Communication Gaps](https://arxiv.org/abs/2512.14965)
*Renkai Ma,Dominique Geissler,Stefan Feuerriegel,Tobias Lauinger,Damon McCoy,Pamela Wisniewski*

Main category: cs.HC

TL;DR: 社交媒体平台在青少年在线安全方面面临审查，尽管已有研究揭示了对儿童的在线风险，但平台如何沟通这些风险及其安全措施尚未被全面审视。通过分析四个平台的352篇新闻稿和安全相关博客，识别出七个风险领域和安全特征，同时发现平台在沟通中存在不平衡和问题性做法。


<details>
  <summary>Details</summary>
Motivation: 研究社交媒体平台在提高青少年安全方面的沟通方式和效果，填补现有研究的空白。

Method: 分析2019至2024年间，在YouTube、TikTok、Meta和Snapchat等四个平台发布的352篇新闻稿和安全相关博客，采用归纳和演绎的定性分析方法。

Result: 识别出七个风险领域以及平台声称的安全特征，在沟通上发现对内容曝光和人际交流的侧重，而对内容创作、数据访问和平台访问的关注较少；同时揭示三种沟通问题，包括实施与可用性之间的差异、安全特征操作说明不清晰及缺乏有效性的证据。

Conclusion: 此分析提供了平台沟通中的风险与安全特征之间的沟通差距和透明度问题的见解，为负责任地沟通青少年安全特征提供了指导。

Abstract: Social media platforms have faced increasing scrutiny over whether and how they protect youth online. While online risks to children have been well-documented by prior research, how social media platforms communicate about these risks and their efforts to improve youth safety have not been holistically examined. To fill this gap, we analyzed N=352 press releases and safety-related blogs published between 2019 and 2024 by four platforms popular among youth: YouTube, TikTok, Meta (Facebook and Instagram), and Snapchat. Leveraging both inductive and deductive qualitative approaches, we developed a comprehensive framework of seven problem areas where risks arise, and a taxonomy of safety features that social media platforms claim address these risks. Our analysis revealed uneven emphasis across problem areas, with most communications focused on Content Exposure and Interpersonal Communication, whereas less emphasis was placed on Content Creation, Data Access, and Platform Access. Additionally, we identified three problematic communication practices related to their described safety features, including discrepancies between feature implementation and availability, unclear or inconsistent explanations of safety feature operation, and a lack of evidence regarding the effectiveness of safety features in mitigating risks once implemented. Based on these findings, we discuss the communication gaps between risks and the described safety features, as well as the tensions in achieving transparency in platform communication. Our analysis of platform communication informs guidelines for responsibly communicating about youth safety features.

</details>


### [18] [Human-Centered AI Maturity Model (HCAI-MM): An Organizational Design Perspective](https://arxiv.org/abs/2512.14977)
*Stuart Winby,Wei Xu*

Main category: cs.HC

TL;DR: 人本人工智能（HCAI）优先考虑人类需求和价值，但缺乏方法论指导。本章节介绍了人本人工智能成熟度模型（HCAI-MM），为组织设计和实施HCAI解决方案提供了系统化框架。


<details>
  <summary>Details</summary>
Motivation: 推动人本AI的设计和实施，强调人类需求与价值的重要性，同时填补HCAI与组织设计之间的缺口。

Method: 通过建立人本人工智能成熟度模型，结构化地评估组织的人本AI能力，并提供相关的阶段、指标、工具及治理机制。

Result: 提出了人本人工智能成熟度模型 (HCAI-MM)，该模型为组织评估和促进设计和实施HCAI解决方案的能力提供了结构化框架。

Conclusion: HCAI-MM作为一个支持组织人本AI实施的工具，帮助组织制定最佳实践并提升其在人本AI方面的成熟度，以更好地满足人类的需要和价值。

Abstract: Human-centered artificial intelligence (HCAI) is an approach to AI design, development, and deployment that prioritizes human needs, values, and experiences, ensuring that technology enhances human capabilities, well-being, and workforce empowerment. While HCAI has gained prominence in academic discourse and organizational practice, its implementation remains constrained by the absence of methodological guidance and structured frameworks. In particular, HCAI and organizational design practices are often treated separately, despite their interdependence in shaping effective socio-technical systems. This chapter addresses this gap by introducing the Human-Centered AI Maturity Model (HCAI-MM), a structured framework that enables organizations to evaluate, monitor, and advance their capacity to design and implement HCAI solutions. The model specifies stages of maturity, metrics, tools, governance mechanisms, and best practices, supported by case studies, while also incorporating an organizational design methodology that operationalizes maturity progression. Encompassing dimensions such as human-AI collaboration, explainability, fairness, and user experience, the HCAI-MM provides a roadmap for organizations to move from novice to advanced levels of maturity, aligning AI technologies with human values and organizational design principles.

</details>


### [19] [I am here for you": How relational conversational AI appeals to adolescents, especially those who are socially and emotionally vulnerable](https://arxiv.org/abs/2512.15117)
*Pilyoung Kim,Yun Xie,Sujin Yang*

Main category: cs.HC

TL;DR: 本研究探讨了对话风格对青少年特定情感支持的影响，结果显示关系风格更受青少年青睐，且提升了对聊天机器人的人性化认知与信任。


<details>
  <summary>Details</summary>
Motivation: 探讨对话风格如何影响青少年对聊天机器人的人性化和情感依赖，以改善AI对青少年的情感支持。

Method: 通过在线实验，对284对青少年和父母进行研究，比较了关系风格与透明风格的聊天记录。

Result: 研究发现，使用关系风格的聊天机器人更能吸引青少年，增强人性化、信任感和情感亲密感。

Conclusion: 关系风格的对话设计能够提高青少年对聊天机器人的信任和依赖性，这对情感脆弱的青少年有特殊影响。

Abstract: General-purpose conversational AI chatbots and AI companions increasingly provide young adolescents with emotionally supportive conversations, raising questions about how conversational style shapes anthropomorphism and emotional reliance. In a preregistered online experiment with 284 adolescent-parent dyads, youth aged 11-15 and their parents read two matched transcripts in which a chatbot responded to an everyday social problem using either a relational style (first-person, affiliative, commitment language) or a transparent style (explicit nonhumanness, informational tone). Adolescents more often preferred the relational than the transparent style, whereas parents were more likely to prefer transparent style than adolescents. Adolescents rated the relational chatbot as more human-like, likable, trustworthy and emotionally close, while perceiving both styles as similarly helpful. Adolescents who preferred relational style had lower family and peer relationship quality and higher stress and anxiety than those preferring transparent style or both chatbots. These findings identify conversational style as a key design lever for youth AI safety, showing that relational framing heightens anthropomorphism, trust and emotional closeness and can be especially appealing to socially and emotionally vulnerable adolescents, who may be at increased risk for emotional reliance on conversational AI.

</details>


### [20] [Lessons Learnt from Expert-Centred Studies Exploring Opportunities and Challenges for Immersive Forensic Investigation](https://arxiv.org/abs/2512.15220)
*Vahid Pooryousef,Tim Dwyer,Richard Bassed,Maxime Cordeil,Lonni Besançon*

Main category: cs.HC

TL;DR: 本研究探讨了在进行专家中心研究时遇到的多种挑战和解决方案，强调了与专家紧密合作的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着研究经费日益基于实践性和现实问题，研究者需要深入了解与专家合作的难点和应对策略。

Method: 反思并讨论专家中心研究中的挑战和解决方案

Result: 识别并应对在进行有关沉浸式法医调查的专家中心研究时所遇到的挑战和具体要求

Conclusion: 专家中心研究的复杂性要求研究者具备灵活应对策略，以确保研究的成功实施。

Abstract: Research studies involving human participants present challenges, including strict ethical considerations, participant recruitment, costs, and many human factors. While human-computer interaction researchers are familiar with these challenges and current solutions, expert-centred studies can be even more challenging in ways that researchers may not anticipate. This issue is particularly important as research grants are increasingly based on practical and real-world problems, which necessitate close collaboration with experts. In this paper, we reflect on and discuss the challenges, solutions, and specific requirements that arose during our expert-centred studies conducted over three years of a PhD study exploring immersive forensic investigation.

</details>


### [21] [Development of Immersive Virtual and Augmented Reality-Based Joint Attention Training Platform for Children with Autism](https://arxiv.org/abs/2512.15263)
*Ashirbad Samantaray,Taranjit Kaur,Sapna S Mishra,Kritika Lohia,Chayan Majumder,Sheffali Gulati,Tapan Kumar Gandhi*

Main category: cs.HC

TL;DR: 本研究开发了一个结合AR和VR的共同注意力培训平台，结果表明AR在训练自闭症儿童的共同注意力方面具有更高的效果，强调了沉浸式技术在干预中的潜力。


<details>
  <summary>Details</summary>
Motivation: 自闭症谱系障碍儿童在共同注意力方面存在缺陷，影响社交沟通，强调了早期干预的必要性。

Method: 开发了一个使用增强现实（AR）和虚拟现实（VR）设备的新的共同注意力培训平台，集成了基于眼动的互动以确保参与者的专注。通过实验验证该平台，在临床神经学家监督下对自闭症（N=19）和神经典型（N=13）参与者进行测试。

Result: AR实验中，神经典型参与者的反应速度显著快于自闭症参与者（p<0.00001）。同时发现自闭症参与者的反应时间与CARS评分之间存在相关性（Spearman系数=0.57, p=0.03）。在对比自闭症参与者在不同平台上的反应准确度时，AR的正确率（92.30%）高于VR（69.49%）。

Conclusion: 这些发现表明，沉浸式技术可以有效帮助自闭症的共同注意力训练，未来的研究应探讨其长期效果和现实应用。

Abstract: Joint Attention (JA), a crucial social skill for developing shared focus, is often impaired in children with Autism Spectrum Disorder (ASD), affecting social communication and highlighting the need for early intervention. Addressing gaps in prior research, such as limited use of immersive technology and reliance on distracting peripherals, we developed a novel JA training platform using Augmented Reality (AR) and Virtual Reality (VR) devices. The platform integrates eye gaze-based interactions to ensure participants undivided attention. To validate the platform, we conducted experiments on ASD (N=19) and Neurotypical (NT) (N=13) participants under a trained pediatric neurologist's supervision. For quantitative analysis, we employed key measures such as the number of correct responses, the duration of establishing eye contact (s), and the duration of registering a response (s), along with correlations to CARS scores and age. Results from AR-based experiments showed NT participants registered responses significantly faster (<0.00001) than ASD participants. A correlation (Spearman coefficient=0.57, p=0.03) was found between ASD participants response time and CARS scores. A similar trend was observed in VR-based experiments. When comparing response accuracy in ASD participants across platforms, AR yielded a higher correctness rate (92.30%) than VR (69.49%), indicating AR's greater effectiveness. These findings suggest that immersive technology can aid JA training in ASD. Future studies should explore long-term benefits and real-world applicability.

</details>


### [22] [Managing Ambiguity: A Proof of Concept of Human-AI Symbiotic Sense-making based on Quantum-Inspired Cognitive Mechanism of Rogue Variable Detection](https://arxiv.org/abs/2512.15325)
*Agnieszka Bienkowska,Jacek Malecki,Alexander Mathiesen-Ohman,Katarzyna Tworek*

Main category: cs.HC

TL;DR: 本研究探讨了在VUCA环境中如何通过LAIZA人机共生系统管理模糊性，以增强组织韧性。


<details>
  <summary>Details</summary>
Motivation: 在VUCA环境中，组织需要有效管理模糊性以避免决策错误，而现有的AI系统在高度模糊的条件下容易导致解释提前闭合。

Method: 研究提出了LAIZA人机增强共生智能系统及其专利流程，通过将模糊性视为非崩塌的认知状态，及时检测到解释性破裂并进行人类介入澄清。

Result: 通过三个月的案例研究，发现保持解释的多样性有助于提前准备情景，包括主动进行专利保护，从而在模糊性崩溃后能够果断行动。

Conclusion: 本研究通过提出LAIZA人机增强共生智能系统的概念证明，强调将模糊性作为重要构建的必要性，并展示了人机共生在VUCA环境中提高组织韧性的实际价值。

Abstract: Organizations increasingly operate in environments characterized by volatility, uncertainty, complexity, and ambiguity (VUCA), where early indicators of change often emerge as weak, fragmented signals. Although artificial intelligence (AI) is widely used to support managerial decision-making, most AI-based systems remain optimized for prediction and resolution, leading to premature interpretive closure under conditions of high ambiguity. This creates a gap in management science regarding how human-AI systems can responsibly manage ambiguity before it crystallizes into error or crisis. This study addresses this gap by presenting a proof of concept (PoC) of the LAIZA human-AI augmented symbiotic intelligence system and its patented process: Systems and Methods for Quantum-Inspired Rogue Variable Modeling (QRVM), Human-in-the-Loop Decoherence, and Collective Cognitive Inference. The mechanism operationalizes ambiguity as a non-collapsed cognitive state, detects persistent interpretive breakdowns (rogue variables), and activates structured human-in-the-loop clarification when autonomous inference becomes unreliable. Empirically, the article draws on a three-month case study conducted in 2025 within the AI development, involving prolonged ambiguity surrounding employee intentions and intellectual property boundaries. The findings show that preserving interpretive plurality enabled early scenario-based preparation, including proactive patent protection, allowing decisive and disruption-free action once ambiguity collapsed. The study contributes to management theory by reframing ambiguity as a first-class construct and demonstrates the practical value of human-AI symbiosis for organizational resilience in VUCA environments.

</details>


### [23] [Exploring User Acceptance and Concerns toward LLM-powered Conversational Agents in Immersive Extended Reality](https://arxiv.org/abs/2512.15343)
*Efe Bozkir,Enkelejda Kasneci*

Main category: cs.HC

TL;DR: 随着生成性人工智能和大语言模型的快速发展，用户在扩展现实中接受这些技术，但对安全、隐私等方面存在担忧，尤其对于数据敏感性有明显差异。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在通过了解用户对大语言模型在扩展现实中应用的接受度和担忧，探讨用户决策过程在技术使用中的影响。

Method: 我们进行了大规模的众包研究，涉及1036名参与者，重点分析了不同扩展现实环境、语音交互类型及数据处理地点对用户决策的影响。

Result: 该研究展示了1036名参与者在扩展现实环境中对大语言模型驱动的对话代理的接受度及其担忧。整体上，用户接受此技术，但在安全性、隐私、社会影响和信任方面表现出顾虑。

Conclusion: 为了促进用户对大语言模型驱动的扩展现实技术的接受，相关从业者需要有效地与用户沟通技术使用措施，以减少用户的不信任感。

Abstract: The rapid development of generative artificial intelligence (AI) and large language models (LLMs), and the availability of services that make them accessible, have led the general public to begin incorporating them into everyday life. The extended reality (XR) community has also sought to integrate LLMs, particularly in the form of conversational agents, to enhance user experience and task efficiency. When interacting with such conversational agents, users may easily disclose sensitive information due to the naturalistic flow of the conversations, and combining such conversational data with fine-grained sensor data may lead to novel privacy issues. To address these issues, a user-centric understanding of technology acceptance and concerns is essential. Therefore, to this end, we conducted a large-scale crowdsourcing study with 1036 participants, examining user decision-making processes regarding LLM-powered conversational agents in XR, across factors of XR setting type, speech interaction type, and data processing location. We found that while users generally accept these technologies, they express concerns related to security, privacy, social implications, and trust. Our results suggest that familiarity plays a crucial role, as daily generative AI use is associated with greater acceptance. In contrast, previous ownership of XR devices is linked to less acceptance, possibly due to existing familiarity with the settings. We also found that men report higher acceptance with fewer concerns than women. Regarding data type sensitivity, location data elicited the most significant concern, while body temperature and virtual object states were considered least sensitive. Overall, our study highlights the importance of practitioners effectively communicating their measures to users, who may remain distrustful. We conclude with implications and recommendations for LLM-powered XR.

</details>


### [24] [GazeBlend: Exploring Paired Gaze-Based Input Techniques for Navigation and Selection Tasks on Mobile Devices](https://arxiv.org/abs/2512.15491)
*Omar Namnakani,Yasmeen Abdrabou,Jonathan Grizou,Mohamed Khamis*

Main category: cs.HC

TL;DR: 本研究探讨了三种注视输入技术的结合在移动交互中的应用，发现这种结合可以提高任务完成效率。


<details>
  <summary>Details</summary>
Motivation: 研究表明，注视在无手移动交互中具有潜力，而组合不同的注视输入技术可以放大优势并减轻挑战。

Method: 通过用户研究，对比了三种注视输入技术（Dwell Time、Pursuits和Gaze Gestures）的可用性和表现，涉及坐着和走动两种情况下的导航和选择任务。

Result: 结果显示，将导航手势与Dwell Time或Pursuits结合使用能提高任务完成时间和成功率，与单独使用相比优势明显。

Conclusion: 研究提供了关于有效注视驱动界面的见解，探讨了不同输入技术之间的相互影响及其对任务性能的影响。

Abstract: The potential of gaze for hands-free mobile interaction is increasingly evident. While each gaze input technique presents distinct advantages and limitations, a combination can amplify strengths and mitigate challenges. We report on the results of a user study (N=24), in which we compared the usability and performance of pairing three popular gaze input techniques: Dwell Time, Pursuits, and Gaze Gestures, for navigation and selection tasks while sitting and walking. Results show that pairing gestures for navigation with either Dwell time or Pursuits for selection improves task completion time and rate compared to using either individually. We discuss the implications of pairing gaze input techniques, such as how Pursuits may negatively impact other techniques, likely due to the visual clutter it adds, how integrating gestures for navigation reduces the chances of unintentional selections, and the impact of motor activity on performance. Our findings provide insights for effective gaze-enabled interfaces.

</details>


### [25] [A Constructive Scientific Methodology to Improve Climate Figures from IPCC](https://arxiv.org/abs/2512.15514)
*Lu Ying,Junxiu Tang,Tingying He,Jean-Daniel Fekete*

Main category: cs.HC

TL;DR: 本文提出了一种改进气候变化领域IPCC图表的方法，以确保所有修改仍然科学严谨，并通过学习目标和阅读者评分评估其效果。


<details>
  <summary>Details</summary>
Motivation: IPCC图表难以理解，现有设计缺乏科学验证，容易被怀疑。

Method: 从官方IPCC图表开始，收集学习目标，制定测试以评估阅读者的学习效果，并通过评分比较原始和改进后图表的效果。

Result: 改进后图表的阅读者得分显著提高，证明了该方法的有效性和设计自由度。

Conclusion: 本文的方法不仅提升了图表的可理解性，还为设计者提供了在保持科学严谨性的基础上进行创造性修改的自由。

Abstract: We propose a methodology to improve figures from the Intergovernmental Panel on Climate Change (IPCC), ensuring that all modifications remain scientifically rigorous. IPCC figures are notoriously difficult to understand, and although designers have proposed alternatives, these lack formal IPCC validation and can be dismissed by skeptics. To address this gap, our approach starts from official IPCC figures. We gather their associated learning objectives and devise tests to score a pool of figure readers to assess how well they learn the objectives.We define improvement as higher scores obtained by a comparable reader pool after viewing a revised figure, where all modifications undergo review to ensure scientific validity. This assessment gives freedom to designers, who can deviate from the original design while making sure the objectives are still met and improved. We demonstrate the methodology through a case study and describe unexpected challenges encountered during the process.

</details>
