{"id": "2601.00969", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00969", "abs": "https://arxiv.org/abs/2601.00969", "authors": ["Ali Salamatian", "Ke", "Ren", "Kieran Pattison", "Cyrus Neary"], "title": "Value Vision-Language-Action Planning & Search", "comment": "10 pages, 3 figures", "summary": "Vision-Language-Action (VLA) models have emerged as powerful generalist policies for robotic manipulation, yet they remain fundamentally limited by their reliance on behavior cloning, leading to brittleness under distribution shift. While augmenting pretrained models with test-time search algorithms like Monte Carlo Tree Search (MCTS) can mitigate these failures, existing formulations rely solely on the VLA prior for guidance, lacking a grounded estimate of expected future return. Consequently, when the prior is inaccurate, the planner can only correct action selection via the exploration term, which requires extensive simulation to become effective. To address this limitation, we introduce Value Vision-Language-Action Planning and Search (V-VLAPS), a framework that augments MCTS with a lightweight, learnable value function. By training a simple multilayer perceptron (MLP) on the latent representations of a fixed VLA backbone (Octo), we provide the search with an explicit success signal that biases action selection toward high-value regions. We evaluate V-VLAPS on the LIBERO robotic manipulation suite, demonstrating that our value-guided search improves success rates by over 5 percentage points while reducing the average number of MCTS simulations by 5-15 percent compared to baselines that rely only on the VLA prior.", "AI": {"tldr": "V-VLAPS\u6846\u67b6\u901a\u8fc7\u589e\u5f3aMCTS\u7684\u641c\u7d22\u80fd\u529b\uff0c\u6539\u8fdb\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6210\u529f\u7387\u4e0e\u6548\u7387\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u5728\u884c\u4e3a\u514b\u9686\u8fc7\u7a0b\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5bf9\u5206\u5e03\u8f6c\u79fb\u654f\u611f\uff0c\u56e0\u6b64\u9700\u8981\u5f15\u5165\u57fa\u4e8e\u503c\u7684\u89c4\u5212\u4e0e\u641c\u7d22\u65b9\u5f0f\u3002", "method": "V-VLAPS\u7ed3\u5408\u4e86\u8d1f\u8f7d\u56fa\u5b9a\u7684VLA\u9aa8\u67b6\u7684\u6f5c\u5728\u8868\u793a\uff0c\u901a\u8fc7\u8bad\u7ec3\u591a\u5c42\u611f\u77e5\u5668(MLP)\u5efa\u7acb\u53ef\u5b66\u4e60\u7684\u503c\u51fd\u6570\u6765\u6307\u5bfc\u641c\u7d22\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aV-VLAPS\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u8f7b\u91cf\u7ea7\u7684\u53ef\u5b66\u4e60\u503c\u51fd\u6570\u6765\u589e\u5f3aMCTS\uff0c\u4ece\u800c\u89e3\u51b3VLA\u653f\u7b56\u5728\u5206\u5e03\u8f6c\u79fb\u4e0b\u7684\u8106\u5f31\u6027\u3002", "conclusion": "V-VLAPS\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u7684\u51b3\u7b56\u7b56\u7565\uff0c\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u5e76\u51cf\u5c11\u4e86\u6a21\u62df\u6b21\u6570\u3002"}}
{"id": "2601.00978", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00978", "abs": "https://arxiv.org/abs/2601.00978", "authors": ["Yanyi Chen", "Min Deng"], "title": "From Perception to Symbolic Task Planning: Vision-Language Guided Human-Robot Collaborative Structured Assembly", "comment": null, "summary": "Human-robot collaboration (HRC) in structured assembly requires reliable state estimation and adaptive task planning under noisy perception and human interventions. To address these challenges, we introduce a design-grounded human-aware planning framework for human-robot collaborative structured assembly. The framework comprises two coupled modules. Module I, Perception-to-Symbolic State (PSS), employs vision-language models (VLMs) based agents to align RGB-D observations with design specifications and domain knowledge, synthesizing verifiable symbolic assembly states. It outputs validated installed and uninstalled component sets for online state tracking. Module II, Human-Aware Planning and Replanning (HPR), performs task-level multi-robot assignment and updates the plan only when the observed state deviates from the expected execution outcome. It applies a minimal-change replanning rule to selectively revise task assignments and preserve plan stability even under human interventions. We validate the framework on a 27-component timber-frame assembly. The PSS module achieves 97% state synthesis accuracy, and the HPR module maintains feasible task progression across diverse HRC scenarios. Results indicate that integrating VLM-based perception with knowledge-driven planning improves robustness of state estimation and task planning under dynamic conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9762\u5411\u4eba\u7c7b\u7684\u8ba1\u5212\u6846\u67b6\u4ee5\u589e\u5f3a\u4eba\u673a\u534f\u4f5c\u7684\u72b6\u6001\u4f30\u8ba1\u548c\u4efb\u52a1\u89c4\u5212\u3002", "motivation": "\u89e3\u51b3\u7ed3\u6784\u88c5\u914d\u4e2d\u56e0\u566a\u58f0\u611f\u77e5\u548c\u4eba\u7c7b\u5e72\u9884\u5bfc\u81f4\u7684\u53ef\u9760\u72b6\u6001\u4f30\u8ba1\u548c\u81ea\u9002\u5e94\u4efb\u52a1\u89c4\u5212\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u6a21\u5757\u8fdb\u884c\u8bbe\u8ba1\uff1aPSS\u6a21\u5757\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5904\u7406\u89c2\u6d4b\u5e76\u751f\u6210\u7b26\u53f7\u72b6\u6001\uff0cHPR\u6a21\u5757\u8fdb\u884c\u591a\u673a\u5668\u4eba\u4efb\u52a1\u5206\u914d\u548c\u52a8\u6001\u8ba1\u5212\u66f4\u65b0\u3002", "result": "PSS\u6a21\u5757\u8fbe\u523097%\u7684\u72b6\u6001\u5408\u6210\u51c6\u786e\u7387\uff0cHPR\u6a21\u5757\u5728\u591a\u79cdHRC\u573a\u666f\u4e2d\u4fdd\u6301\u4efb\u52a1\u53ef\u884c\u6027\u3002", "conclusion": "VLM\u4e0e\u77e5\u8bc6\u9a71\u52a8\u7684\u89c4\u5212\u76f8\u7ed3\u5408\uff0c\u63d0\u9ad8\u4e86\u52a8\u6001\u6761\u4ef6\u4e0b\u7684\u72b6\u6001\u4f30\u8ba1\u548c\u4efb\u52a1\u89c4\u5212\u7684\u7a33\u5065\u6027\u3002"}}
{"id": "2601.00981", "categories": ["cs.RO", "cs.CV", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.00981", "abs": "https://arxiv.org/abs/2601.00981", "authors": ["Wenhui Chu", "Khang Tran", "Nikolaos V. Tsekos"], "title": "Simulations of MRI Guided and Powered Ferric Applicators for Tetherless Delivery of Therapeutic Interventions", "comment": "9 pages, 8 figures, published in ICBBB 2022", "summary": "Magnetic Resonance Imaging (MRI) is a well-established modality for pre-operative planning and is also explored for intra-operative guidance of procedures such as intravascular interventions. Among the experimental robot-assisted technologies, the magnetic field gradients of the MRI scanner are used to power and maneuver ferromagnetic applicators for accessing sites in the patient's body via the vascular network. In this work, we propose a computational platform for preoperative planning and modeling of MRI-powered applicators inside blood vessels. This platform was implemented as a two-way data and command pipeline that links the MRI scanner, the computational core, and the operator. The platform first processes multi-slice MR data to extract the vascular bed and then fits a virtual corridor inside the vessel. This corridor serves as a virtual fixture (VF), a forbidden region for the applicators to avoid vessel perforation or collision. The geometric features of the vessel centerline, the VF, and MRI safety compliance (dB/dt, max available gradient) are then used to generate magnetic field gradient waveforms. Different blood flow profiles can be user-selected, and those parameters are used for modeling the applicator's maneuvering. The modeling module further generates cues about whether the selected vascular path can be safely maneuvered. Given future experimental studies that require a real-time operation, the platform was implemented on the Qt framework (C/C++) with software modules performing specific tasks running on dedicated threads: PID controller, generation of VF, generation of MR gradient waveforms.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8eMRI\u9a71\u52a8\u7684\u5e94\u7528\u88c5\u7f6e\u5728\u8840\u7ba1\u5185\u5b89\u5168\u64cd\u4f5c\u7684\u8ba1\u7b97\u5e73\u53f0\uff0c\u7ed3\u5408\u4e86MRI\u626b\u63cf\u3001\u6570\u636e\u5904\u7406\u548c\u865a\u62df\u56fa\u5b9a\u533a\u57df\u751f\u6210\u3002", "motivation": "\u63a2\u7d22MRI\u5728\u672f\u524d\u89c4\u5212\u548c\u672f\u4e2d\u6307\u5bfc\u4e2d\u7684\u5e94\u7528\uff0c\u5c24\u5176\u662f\u5728\u8840\u7ba1\u5185\u5e72\u9884\u4e2d\u4f7f\u7528MRI\u673a\u5668\u4eba\u6280\u672f\u3002", "method": "\u57fa\u4e8eQt\u6846\u67b6\uff0c\u6784\u5efa\u4e00\u4e2a\u53cc\u5411\u6570\u636e\u548c\u6307\u4ee4\u7ba1\u9053\uff0c\u5904\u7406\u591a\u5207\u7247MRI\u6570\u636e\uff0c\u751f\u6210\u865a\u62df\u901a\u9053\uff0c\u8ba1\u7b97\u78c1\u573a\u68af\u5ea6\u6ce2\u5f62\uff0c\u5e76\u8bc4\u4f30\u8840\u7ba1\u8def\u5f84\u7684\u5b89\u5168\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8ba1\u7b97\u5e73\u53f0\uff0c\u7528\u4e8e\u672f\u524d\u89c4\u5212\u548cMRI\u9a71\u52a8\u7684\u5bfc\u7ba1\u5728\u8840\u7ba1\u5185\u7684\u5efa\u6a21\uff0c\u63d0\u4f9b\u4e86\u5b89\u5168\u64cd\u4f5c\u7684\u6307\u5bfc\u3002", "conclusion": "\u8be5\u5e73\u53f0\u4e3a\u672a\u6765\u5b9e\u9a8c\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u65f6\u64cd\u4f5c\u7684\u53ef\u80fd\u6027\uff0c\u589e\u5f3a\u4e86MRI\u673a\u5668\u4eba\u6280\u672f\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2601.01067", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01067", "abs": "https://arxiv.org/abs/2601.01067", "authors": ["Wenzheng Zhang", "Yoshitaka Hara", "Sousuke Nakamura"], "title": "Topological Mapping and Navigation using a Monocular Camera based on AnyLoc", "comment": "Published in Proc. IEEE CASE 2025. 7 pages, 11 figures", "summary": "This paper proposes a method for topological mapping and navigation using a monocular camera. Based on AnyLoc, keyframes are converted into descriptors to construct topological relationships, enabling loop detection and map building. Unlike metric maps, topological maps simplify path planning and navigation by representing environments with key nodes instead of precise coordinates. Actions for visual navigation are determined by comparing segmented images with the image associated with target nodes. The system relies solely on a monocular camera, ensuring fast map building and navigation using key nodes. Experiments show effective loop detection and navigation in real and simulation environments without pre-training. Compared to a ResNet-based method, this approach improves success rates by 60.2% on average while reducing time and space costs, offering a lightweight solution for robot and human navigation in various scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u76ee\u76f8\u673a\u7684\u62d3\u6251\u5730\u56fe\u6784\u5efa\u548c\u5bfc\u822a\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u6210\u529f\u7387\u5e76\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u7b80\u5316\u8def\u5f84\u89c4\u5212\u548c\u5bfc\u822a\uff0c\u901a\u8fc7\u7528\u5173\u952e\u8282\u70b9\u800c\u975e\u7cbe\u786e\u5750\u6807\u8868\u793a\u73af\u5883\uff0c\u63d0\u4f9b\u5feb\u901f\u7684\u5730\u56fe\u6784\u5efa\u548c\u5bfc\u822a\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u5355\u76ee\u76f8\u673a\u8fdb\u884c\u62d3\u6251\u5730\u56fe\u6784\u5efa\u548c\u5bfc\u822a\u7684\u65b9\u6cd5", "result": "\u5728\u771f\u5b9e\u548c\u4eff\u771f\u73af\u5883\u4e2d\uff0c\u6210\u529f\u7684\u95ed\u73af\u68c0\u6d4b\u548c\u5bfc\u822a\u5b9e\u9a8c\uff0c\u4e14\u8f83ResNet\u65b9\u6cd5\u63d0\u9ad8\u4e8660.2%\u7684\u6210\u529f\u7387\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u65f6\u95f4\u548c\u7a7a\u95f4\u6210\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u673a\u5668\u4eba\u548c\u4eba\u7c7b\u5728\u5404\u79cd\u573a\u666f\u4e2d\u7684\u5bfc\u822a\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01027", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.01027", "abs": "https://arxiv.org/abs/2601.01027", "authors": ["Rafael Wampfler", "Chen Yang", "Dillon Elste", "Nikola Kovacevic", "Philine Witzig", "Markus Gross"], "title": "A Platform for Interactive AI Character Experiences", "comment": null, "summary": "From movie characters to modern science fiction - bringing characters into interactive, story-driven conversations has captured imaginations across generations. Achieving this vision is highly challenging and requires much more than just language modeling. It involves numerous complex AI challenges, such as conversational AI, maintaining character integrity, managing personality and emotions, handling knowledge and memory, synthesizing voice, generating animations, enabling real-world interactions, and integration with physical environments. Recent advancements in the development of foundation models, prompt engineering, and fine-tuning for downstream tasks have enabled researchers to address these individual challenges. However, combining these technologies for interactive characters remains an open problem. We present a system and platform for conveniently designing believable digital characters, enabling a conversational and story-driven experience while providing solutions to all of the technical challenges. As a proof-of-concept, we introduce Digital Einstein, which allows users to engage in conversations with a digital representation of Albert Einstein about his life, research, and persona. While Digital Einstein exemplifies our methods for a specific character, our system is flexible and generalizes to any story-driven or conversational character. By unifying these diverse AI components into a single, easy-to-adapt platform, our work paves the way for immersive character experiences, turning the dream of lifelike, story-based interactions into a reality.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5e73\u53f0\uff0c\u65e8\u5728\u521b\u5efa\u903c\u771f\u7684\u4e92\u52a8\u6570\u5b57\u89d2\u8272\uff0c\u5c24\u5176\u662f\u4ee5\u7231\u56e0\u65af\u5766\u4e3a\u4f8b\uff0c\u5c55\u793a\u5982\u4f55\u5b9e\u73b0\u6545\u4e8b\u9a71\u52a8\u7684\u5bf9\u8bdd\u4f53\u9a8c\u3002", "motivation": "\u5c06\u4eba\u7269\u89d2\u8272\u878d\u5165\u4e92\u52a8\u6545\u4e8b\u9a71\u52a8\u7684\u5bf9\u8bdd\u4e2d\uff0c\u5438\u5f15\u4e86\u5404\u4ee3\u4eba\u7684\u60f3\u8c61\uff0c\u4f46\u5b9e\u73b0\u8fd9\u4e00\u613f\u666f\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002", "method": "\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u3001\u63d0\u793a\u5de5\u7a0b\u548c\u4e0b\u6e38\u4efb\u52a1\u5fae\u8c03\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6574\u5408\u591a\u79cdAI\u6280\u672f\u7684\u7cfb\u7edf\uff0c\u5141\u8bb8\u7528\u6237\u4e0e\u6570\u5b57\u5316\u7684\u7231\u56e0\u65af\u5766\u8fdb\u884c\u4e92\u52a8\u5bf9\u8bdd\u3002", "result": "\u63d0\u51fa\u4e00\u4e2a\u7cfb\u7edf\u548c\u5e73\u53f0\uff0c\u65b9\u4fbf\u8bbe\u8ba1\u53ef\u4fe1\u7684\u6570\u5b57\u89d2\u8272\uff0c\u63d0\u4f9b\u5bf9\u8bdd\u548c\u6545\u4e8b\u9a71\u52a8\u7684\u4f53\u9a8c\uff0c\u540c\u65f6\u89e3\u51b3\u6240\u6709\u6280\u672f\u6311\u6218\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5404\u79cdAI\u7ec4\u4ef6\u7edf\u4e00\u5230\u4e00\u4e2a\u6613\u4e8e\u9002\u5e94\u7684\u5e73\u53f0\u4e0a\uff0c\u6211\u4eec\u7684\u5de5\u4f5c\u4e3a\u6c89\u6d78\u5f0f\u89d2\u8272\u4f53\u9a8c\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u4f7f\u6829\u6829\u5982\u751f\u7684\u6545\u4e8b\u4e92\u52a8\u6210\u4e3a\u73b0\u5b9e\u3002"}}
{"id": "2601.01106", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01106", "abs": "https://arxiv.org/abs/2601.01106", "authors": ["Michele Grimaldi", "Yosaku Maeda", "Hitoshi Kakami", "Ignacio Carlucho", "Yvan Petillot", "Tomoya Inoue"], "title": "Towards reliable subsea object recovery: a simulation study of an auv with a suction-actuated end effector", "comment": null, "summary": "Autonomous object recovery in the hadal zone is challenging due to extreme hydrostatic pressure, limited visibility and currents, and the need for precise manipulation at full ocean depth. Field experimentation in such environments is costly, high-risk, and constrained by limited vehicle availability, making early validation of autonomous behaviors difficult. This paper presents a simulation-based study of a complete autonomous subsea object recovery mission using a Hadal Small Vehicle (HSV) equipped with a three-degree-of-freedom robotic arm and a suction-actuated end effector. The Stonefish simulator is used to model realistic vehicle dynamics, hydrodynamic disturbances, sensing, and interaction with a target object under hadal-like conditions. The control framework combines a world-frame PID controller for vehicle navigation and stabilization with an inverse-kinematics-based manipulator controller augmented by acceleration feed-forward, enabling coordinated vehicle - manipulator operation. In simulation, the HSV autonomously descends from the sea surface to 6,000 m, performs structured seafloor coverage, detects a target object, and executes a suction-based recovery. The results demonstrate that high-fidelity simulation provides an effective and low-risk means of evaluating autonomous deep-sea intervention behaviors prior to field deployment.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528Stonefish\u6a21\u62df\u5668\uff0c\u5c55\u793a\u4e86\u5728\u6781\u7aef\u6df1\u6d77\u6761\u4ef6\u4e0b\uff0cHSV\u5982\u4f55\u81ea\u4e3b\u6267\u884c\u4ece\u6c34\u9762\u52306000\u7c73\u7684\u4e0b\u6f5c\u3001\u76ee\u6807\u68c0\u6d4b\u548c\u7269\u4f53\u56de\u6536\u3002", "motivation": "\u81ea\u52a8\u5316\u7269\u4f53\u56de\u6536\u5728\u6781\u7aef\u6df1\u6d77\u73af\u5883\u4e2d\u9762\u4e34\u5de8\u5927\u7684\u6311\u6218\uff0c\u4f8b\u5982\u9ad8\u6c34\u538b\u3001\u80fd\u89c1\u5ea6\u4f4e\u548c\u6c34\u6d41\u590d\u6742\uff0c\u7cbe\u786e\u64cd\u4f5c\u53d8\u5f97\u5c24\u4e3a\u5173\u952e\uff0c\u800c\u5b9e\u9645\u73b0\u573a\u5b9e\u9a8c\u6210\u672c\u9ad8\u4e14\u98ce\u9669\u5927\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528\u4e86Stonefish\u6a21\u62df\u5668\uff0c\u7ed3\u5408PID\u63a7\u5236\u5668\u4e0e\u57fa\u4e8e\u9006\u8fd0\u52a8\u5b66\u7684\u673a\u68b0\u81c2\u63a7\u5236\u5668\uff0c\u5e76\u589e\u5f3a\u4e86\u52a0\u901f\u5ea6\u524d\u9988\uff0c\u8fdb\u884c\u534f\u8c03\u63a7\u5236\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eff\u771f\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u914d\u5907\u6709\u4e09\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u548c\u5438\u529b\u672b\u7aef\u6267\u884c\u5668\u7684\u6df1\u6d77\u5c0f\u578b\u8f66\u8f86\uff08HSV\uff09\uff0c\u6210\u529f\u8fdb\u884c\u4e86\u5b8c\u6574\u7684\u81ea\u52a8\u5316\u6c34\u4e0b\u7269\u4f53\u56de\u6536\u4efb\u52a1\u3002", "conclusion": "\u9ad8\u4fdd\u771f\u4eff\u771f\u4e3a\u8bc4\u4f30\u6df1\u6d77\u5e72\u9884\u884c\u4e3a\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u4f4e\u98ce\u9669\u7684\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u5728\u5b9e\u9645\u90e8\u7f72\u524d\u9a8c\u8bc1\u81ea\u52a8\u5316\u884c\u4e3a\u3002"}}
{"id": "2601.01094", "categories": ["cs.HC", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.01094", "abs": "https://arxiv.org/abs/2601.01094", "authors": ["Yubo Shu", "Peng Zhang", "Meng Wu", "Yan Chen", "Haoxuan Zhou", "Guanming Liu", "Yu Zhang", "Liuxin Zhang", "Qianying Wang", "Tun Lu", "Ning Gu"], "title": "SoulSeek: Exploring the Use of Social Cues in LLM-based Information Seeking", "comment": null, "summary": "Social cues, which convey others' presence, behaviors, or identities, play a crucial role in human information seeking by helping individuals judge relevance and trustworthiness. However, existing LLM-based search systems primarily rely on semantic features, creating a misalignment with the socialized cognition underlying natural information seeking. To address this gap, we explore how the integration of social cues into LLM-based search influences users' perceptions, experiences, and behaviors. Focusing on social media platforms that are beginning to adopt LLM-based search, we integrate design workshops, the implementation of the prototype system (SoulSeek), a between-subjects study, and mixed-method analyses to examine both outcome- and process-level findings. The workshop informs the prototype's cue-integrated design. The study shows that social cues improve perceived outcomes and experiences, promote reflective information behaviors, and reveal limits of current LLM-based search. We propose design implications emphasizing better social-knowledge understanding, personalized cue settings, and controllable interactions.", "AI": {"tldr": "\u7814\u7a76\u793e\u4ea4\u7ebf\u7d22\u5728LLM\u57fa\u7840\u641c\u7d22\u4e2d\u7684\u4f5c\u7528\uff0c\u7ed3\u679c\u663e\u793a\u793e\u4ea4\u7ebf\u7d22\u6539\u5584\u4e86\u7528\u6237\u4f53\u9a8c\u548c\u4fe1\u606f\u884c\u4e3a", "motivation": "\u63a2\u8ba8\u73b0\u6709LLM\u57fa\u7840\u641c\u7d22\u7cfb\u7edf\u4e0e\u4eba\u7c7b\u81ea\u7136\u4fe1\u606f\u641c\u7d22\u4e2d\u7684\u793e\u4f1a\u8ba4\u77e5\u4e4b\u95f4\u7684\u9519\u4f4d", "method": "\u901a\u8fc7\u8bbe\u8ba1\u7814\u8ba8\u4f1a\u3001\u539f\u578b\u7cfb\u7edf\u5b9e\u73b0\u548c\u88ab\u8bd5\u95f4\u7814\u7a76\uff0c\u4ee5\u53ca\u6df7\u5408\u65b9\u6cd5\u5206\u6790\uff0c\u7814\u7a76\u793e\u4ea4\u7ebf\u7d22\u5728LLM\u57fa\u7840\u641c\u7d22\u4e2d\u7684\u5f71\u54cd", "result": "\u793e\u4ea4\u7ebf\u7d22\u63d0\u5347\u4e86\u7528\u6237\u7684\u611f\u77e5\u7ed3\u679c\u548c\u4f53\u9a8c\uff0c\u4fc3\u8fdb\u53cd\u601d\u6027\u4fe1\u606f\u884c\u4e3a\uff0c\u5e76\u63ed\u793a\u4e86\u5f53\u524dLLM\u57fa\u7840\u641c\u7d22\u7684\u5c40\u9650\u6027", "conclusion": "\u63d0\u51fa\u6539\u5584\u793e\u4ea4\u77e5\u8bc6\u7406\u89e3\u3001\u4e2a\u6027\u5316\u7ebf\u7d22\u8bbe\u7f6e\u548c\u53ef\u63a7\u4ea4\u4e92\u7684\u8bbe\u8ba1\u5efa\u8bae\u3002"}}
{"id": "2601.01139", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.01139", "abs": "https://arxiv.org/abs/2601.01139", "authors": ["Sriram Rajasekar", "Ashwini Ratnoo"], "title": "Latent Space Reinforcement Learning for Multi-Robot Exploration", "comment": null, "summary": "Autonomous mapping of unknown environments is a critical challenge, particularly in scenarios where time is limited. Multi-agent systems can enhance efficiency through collaboration, but the scalability of motion-planning algorithms remains a key limitation. Reinforcement learning has been explored as a solution, but existing approaches are constrained by the limited input size required for effective learning, restricting their applicability to discrete environments. This work addresses that limitation by leveraging autoencoders to perform dimensionality reduction, compressing high-fidelity occupancy maps into latent state vectors while preserving essential spatial information. Additionally, we introduce a novel procedural generation algorithm based on Perlin noise, designed to generate topologically complex training environments that simulate asteroid fields, caves and forests. These environments are used for training the autoencoder and the navigation algorithm using a hierarchical deep reinforcement learning framework for decentralized coordination. We introduce a weighted consensus mechanism that modulates reliance on shared data via a tuneable trust parameter, ensuring robustness to accumulation of errors. Experimental results demonstrate that the proposed system scales effectively with number of agents and generalizes well to unfamiliar, structurally distinct environments and is resilient in communication-constrained settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u52a8\u7f16\u7801\u5668\u548c\u65b0\u9896\u7a0b\u5e8f\u751f\u6210\u7b97\u6cd5\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u591a\u4ee3\u7406\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u6620\u5c04\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u591a\u4ee3\u7406\u7cfb\u7edf\u5728\u672a\u77e5\u73af\u5883\u6620\u5c04\u4e2d\u9ad8\u6548\u6027\u4e0e\u53ef\u6269\u5c55\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u6709\u9650\u65f6\u95f4\u5185\u7684\u5e94\u7528\u573a\u666f\u3002", "method": "\u5229\u7528\u81ea\u52a8\u7f16\u7801\u5668\u8fdb\u884c\u7ef4\u5ea6\u51cf\u5c11\u548c\u63d0\u51fa\u57fa\u4e8ePerlin\u566a\u58f0\u7684\u65b0\u9896\u7a0b\u5e8f\u751f\u6210\u7b97\u6cd5\uff0c\u7ed3\u5408\u5206\u5c42\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u8fdb\u884c\u53bb\u4e2d\u5fc3\u5316\u534f\u8c03\u4e0e\u52a0\u6743\u5171\u8bc6\u673a\u5236\u3002", "result": "\u63d0\u51fa\u7684\u7cfb\u7edf\u5728\u4ee3\u7406\u6570\u91cf\u589e\u52a0\u65f6\u80fd\u591f\u6709\u6548\u6269\u5c55\uff0c\u5e76\u4e14\u80fd\u591f\u826f\u597d\u5730\u6cdb\u5316\u5230\u4e0d\u719f\u6089\u4e14\u7ed3\u6784\u4e0a\u4e0d\u540c\u7684\u73af\u5883\uff0c\u5728\u901a\u4fe1\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u4e5f\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u97e7\u6027\u3002", "conclusion": "\u672c\u6587\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u4e86\u591a\u4ee3\u7406\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2601.01218", "categories": ["cs.HC", "cs.MM", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01218", "abs": "https://arxiv.org/abs/2601.01218", "authors": ["Ka Yan Fung", "Tze Leung Rick Lui", "Yuxing Tao", "Kuen Fung Sin"], "title": "MotiBo: The Impact of Interactive Digital Storytelling Robots on Student Motivation through Self-Determination Theory", "comment": null, "summary": "Creativity is increasingly recognized as an important skill in education, and storytelling can enhance motivation and engagement among students. However, conventional storytelling methods often lack the interactive elements necessary to engage students. To this end, this study examines the impact of an interactive digital storytelling system incorporating a human-like robot on student engagement and creativity. The study aims to compare engagement levels across three modalities: paper-based, PowerPoint, and robot-assisted storytelling, MotiBo. Utilizing a quasi-experimental design, this work involves three groups of students who interact with the storytelling system over a five-day learning. Findings reveal that students using MotiBo exhibit statistically significant improvement in behavioural and cognitive engagement compared to those using traditional methods. These results suggest that the integration of novel technologies can effectively enhance the learning experience, ultimately promoting creativity and self-learning ability in educational settings. Future research will investigate the long-term effects of these technologies on learning outcomes and explore their potential for broader applications in diverse educational contexts.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540c\u53d9\u4e8b\u65b9\u5f0f\uff0c\u63a2\u8ba8\u4e86\u4e92\u52a8\u6570\u5b57\u53d9\u4e8b\u7cfb\u7edf\u5bf9\u5b66\u751f\u53c2\u4e0e\u5ea6\u548c\u521b\u9020\u529b\u7684\u5f71\u54cd\uff0c\u7ed3\u679c\u53d1\u73b0\u673a\u5668\u4eba\u8f85\u52a9\u53d9\u4e8b\u80fd\u663e\u8457\u63d0\u5347\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u5728\u6559\u80b2\u4e2d\u8d8a\u6765\u8d8a\u91cd\u89c6\u521b\u9020\u529b\uff0c\u800c\u4f20\u7edf\u53d9\u4e8b\u65b9\u6cd5\u7f3a\u4e4f\u8db3\u591f\u7684\u4e92\u52a8\u5143\u7d20\u6765\u5438\u5f15\u5b66\u751f\u3002", "method": "\u91c7\u7528\u51c6\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u6bd4\u8f83\u4e09\u7c7b\u53d9\u4e8b\u65b9\u6cd5\uff08\u7eb8\u8d28\u3001PowerPoint\u548c\u673a\u5668\u4eba\u8f85\u52a9\u53d9\u4e8bMotiBo\uff09\u5bf9\u5b66\u751f\u53c2\u4e0e\u5ea6\u7684\u5f71\u54cd\u3002", "result": "\u4f7f\u7528MotiBo\u7684\u5b66\u751f\u5728\u884c\u4e3a\u548c\u8ba4\u77e5\u53c2\u4e0e\u5ea6\u4e0a\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u6709\u663e\u8457\u6539\u5584\u3002", "conclusion": "\u6574\u5408\u65b0\u6280\u672f\u80fd\u591f\u6709\u6548\u63d0\u5347\u5b66\u4e60\u4f53\u9a8c\uff0c\u4fc3\u8fdb\u6559\u80b2\u73af\u5883\u4e2d\u7684\u521b\u9020\u529b\u548c\u81ea\u6211\u5b66\u4e60\u80fd\u529b\u3002"}}
{"id": "2601.01144", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01144", "abs": "https://arxiv.org/abs/2601.01144", "authors": ["Shu Pan", "Simon Archieri", "Ahmet Cinar", "Jonatan Scharff Willners", "Ignacio Carlucho", "Yvan Petillot"], "title": "VISO: Robust Underwater Visual-Inertial-Sonar SLAM with Photometric Rendering for Dense 3D Reconstruction", "comment": null, "summary": "Visual challenges in underwater environments significantly hinder the accuracy of vision-based localisation and the high-fidelity dense reconstruction. In this paper, we propose VISO, a robust underwater SLAM system that fuses a stereo camera, an inertial measurement unit (IMU), and a 3D sonar to achieve accurate 6-DoF localisation and enable efficient dense 3D reconstruction with high photometric fidelity. We introduce a coarse-to-fine online calibration approach for extrinsic parameters estimation between the 3D sonar and the camera. Additionally, a photometric rendering strategy is proposed for the 3D sonar point cloud to enrich the sonar map with visual information. Extensive experiments in a laboratory tank and an open lake demonstrate that VISO surpasses current state-of-the-art underwater and visual-based SLAM algorithms in terms of localisation robustness and accuracy, while also exhibiting real-time dense 3D reconstruction performance comparable to the offline dense mapping method.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6c34\u4e0bSLAM\u7cfb\u7edfVISO\uff0c\u7ed3\u5408\u4e86\u7acb\u4f53\u76f8\u673a\u3001IMU\u548c3D\u58f0\u7eb3\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u548c\u9ad8\u4fdd\u771f\u5bc6\u96c6\u91cd\u5efa\u3002", "motivation": "\u5e94\u5bf9\u6c34\u4e0b\u73af\u5883\u7684\u89c6\u89c9\u6311\u6218\uff0c\u63d0\u9ad8\u57fa\u4e8e\u89c6\u89c9\u7684\u5b9a\u4f4d\u548c\u91cd\u5efa\u7684\u51c6\u786e\u6027\u3002", "method": "\u5229\u7528\u7acb\u4f53\u76f8\u673a\u3001\u60ef\u6027\u6d4b\u91cf\u5355\u5143\u548c3D\u58f0\u7eb3\u878d\u5408\uff0c\u91c7\u7528\u7c97\u5230\u7ec6\u7684\u5728\u7ebf\u6807\u5b9a\u548c\u5149\u5ea6\u6e32\u67d3\u7b56\u7565\u3002", "result": "\u5728\u5b9e\u9a8c\u4e2d\uff0cVISO\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u5b9a\u4f4d\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\uff0c\u4ee5\u53ca\u4e0e\u79bb\u7ebf\u65b9\u6cd5\u53ef\u6bd4\u7684\u5b9e\u65f63D\u91cd\u5efa\u8868\u73b0\u3002", "conclusion": "VISO\u7cfb\u7edf\u5728\u6c34\u4e0b\u5b9a\u4f4d\u548c3D\u91cd\u5efa\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\uff0c\u5177\u6709\u5b9e\u65f6\u6027\u80fd\u3002"}}
{"id": "2601.01227", "categories": ["cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01227", "abs": "https://arxiv.org/abs/2601.01227", "authors": ["Ka Yan Fung", "Kwong Chiu Fung", "Yuxing Tao", "Tze Leung Rick Lui", "Kuen Fung Sin"], "title": "LiveBo: Empowering Non-Chinese Speaking Students through AI-Driven Real-Life Scenarios in Cantonese", "comment": null, "summary": "Language learning is a multifaceted process. Insufficient vocabulary can hinder communication and lead to demotivation. For non-Chinese speaking (NCS) students, learning Traditional Chinese (Cantonese) poses distinct challenges, particularly due to the complexity of converting spoken and written forms. To address this issue, this study examines the effectiveness of real-life scenario simulations integrated with interactive social robots in enhancing NCS student engagement and language acquisition. The research employs a quasi-experimental design involving NCS students who interact with an AI-driven, robot-assisted language learning system, LiveBo. The study aims to assess the impact of this innovative approach on active participation and motivation. Data are collected through proficiency tests, questionnaires and semi-structured interviews. Findings indicate that NCS students experience positive improvements in behavioural and emotional engagement, motivation and learning outcomes, highlighting the potential of integrating novel technologies in language education. We plan to compare with the control group in the future. This study highlights the significance of interactive and immersive learning experiences in promoting motivation and enhancing language acquisition among NCS students.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5c06\u73b0\u5b9e\u573a\u666f\u6a21\u62df\u4e0e\u4e92\u52a8\u793e\u4ea4\u673a\u5668\u4eba\u76f8\u7ed3\u5408\u7684\u8bed\u8a00\u5b66\u4e60\u65b9\u6cd5\u5bf9\u975e\u4e2d\u6587\u6bcd\u8bed\u5b66\u751f\u7684\u53c2\u4e0e\u5ea6\u548c\u5b66\u4e60\u6210\u5c31\u7684\u5f71\u54cd\uff0c\u53d6\u5f97\u79ef\u6781\u6548\u679c\u3002", "motivation": "\u7814\u7a76\u6307\u51fa\uff0c\u8bed\u8a00\u5b66\u4e60\u8fc7\u7a0b\u590d\u6742\uff0c\u4e0d\u5145\u8db3\u7684\u8bcd\u6c47\u91cf\u4f1a\u5bfc\u81f4\u6c9f\u901a\u969c\u788d\u548c\u5b66\u4e60\u52a8\u673a\u4e0b\u964d\u3002\u7279\u522b\u662f\u5bf9\u4e8e\u975e\u4e2d\u6587\u6bcd\u8bed(NCS)\u5b66\u751f\uff0c\u5b66\u4e60\u7ca4\u8bed\u9762\u4e34\u8bf8\u591a\u6311\u6218\u3002", "method": "\u91c7\u7528\u51c6\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u8ba9NCS\u5b66\u751f\u4e0e\u673a\u5668\u4eba\u8f85\u52a9\u7684\u8bed\u8a00\u5b66\u4e60\u7cfb\u7edf\u8fdb\u884c\u4ea4\u4e92\uff0c\u901a\u8fc7\u8bc4\u4f30\u6d4b\u8bd5\u3001\u95ee\u5377\u548c\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u6536\u96c6\u6570\u636e\u3002", "result": "\u901a\u8fc7\u4f7f\u7528\u57fa\u4e8eAI\u7684\u673a\u5668\u4eba\u8f85\u52a9\u8bed\u8a00\u5b66\u4e60\u7cfb\u7edfLiveBo\uff0cNCS\u5b66\u751f\u5728\u884c\u4e3a\u548c\u60c5\u611f\u53c2\u4e0e\u3001\u52a8\u673a\u4e0e\u5b66\u4e60\u6210\u679c\u4e0a\u83b7\u5f97\u79ef\u6781\u6539\u5584\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u4e92\u52a8\u548c\u6c89\u6d78\u5f0f\u5b66\u4e60\u4f53\u9a8c\u5728\u63d0\u5347NCS\u5b66\u751f\u7684\u52a8\u673a\u548c\u8bed\u8a00\u5b66\u4e60\u6548\u679c\u65b9\u9762\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2601.01155", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01155", "abs": "https://arxiv.org/abs/2601.01155", "authors": ["Zhang Shizhe", "Liang Jingsong", "Zhou Zhitao", "Ye Shuhan", "Wang Yizhuo", "Tan Ming Siang Derek", "Chiun Jimmy", "Cao Yuhong", "Sartoretti Guillaume"], "title": "ORION: Option-Regularized Deep Reinforcement Learning for Cooperative Multi-Agent Online Navigation", "comment": null, "summary": "Existing methods for multi-agent navigation typically assume fully known environments, offering limited support for partially known scenarios such as warehouses or factory floors. There, agents may need to plan trajectories that balance their own path optimality with their ability to collect and share information about the environment that can help their teammates reach their own goals. To these ends, we propose ORION, a novel deep reinforcement learning framework for cooperative multi-agent online navigation in partially known environments. Starting from an imperfect prior map, ORION trains agents to make decentralized decisions, coordinate to reach their individual targets, and actively reduce map uncertainty by sharing online observations in a closed perception-action loop. We first design a shared graph encoder that fuses prior map with online perception into a unified representation, providing robust state embeddings under dynamic map discrepancies. At the core of ORION is an option-critic framework that learns to reason about a set of high-level cooperative modes that translate into sequences of low-level actions, allowing agents to switch between individual navigation and team-level exploration adaptively. We further introduce a dual-stage cooperation strategy that enables agents to assist teammates under map uncertainty, thereby reducing the overall makespan. Across extensive maze-like maps and large-scale warehouse environments, our simulation results show that ORION achieves high-quality, real-time decentralized cooperation over varying team sizes, outperforming state-of-the-art classical and learning-based baselines. Finally, we validate ORION on physical robot teams, demonstrating its robustness and practicality for real-world cooperative navigation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ORION\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u9ad8\u591a\u667a\u80fd\u4f53\u5728\u90e8\u5206\u5df2\u77e5\u73af\u5883\u4e2d\u7684\u5408\u4f5c\u5bfc\u822a\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u5730\u56fe\u73af\u5883\u4e2d\uff0cORION\u80fd\u81ea\u9002\u5e94\u5730\u8c03\u6574\u667a\u80fd\u4f53\u7684\u5bfc\u822a\u7b56\u7565\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u534f\u4f5c\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u667a\u80fd\u4f53\u5bfc\u822a\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u73af\u5883\u5b8c\u5168\u5df2\u77e5\uff0c\u4f46\u5bf9\u90e8\u5206\u5df2\u77e5\u573a\u666f\uff08\u5982\u4ed3\u5e93\u6216\u5de5\u5382\u5730\u677f\uff09\u7684\u652f\u6301\u6709\u9650\uff0c\u667a\u80fd\u4f53\u9700\u8981\u5728\u81ea\u5df1\u8def\u5f84\u7684\u6700\u4f18\u6027\u4e0e\u6536\u96c6\u53ca\u5171\u4eab\u73af\u5883\u4fe1\u606f\u4e4b\u95f4\u8fdb\u884c\u5e73\u8861\u3002", "method": "ORION\u5229\u7528\u5171\u4eab\u56fe\u7f16\u7801\u5668\u6765\u7ed3\u5408\u5148\u9a8c\u5730\u56fe\u4e0e\u5728\u7ebf\u611f\u77e5\uff0c\u901a\u8fc7\u9009\u9879-\u8bc4\u8bba\u5bb6\u6846\u67b6\u5b66\u4e60\u9ad8\u5c42\u6b21\u5408\u4f5c\u6a21\u5f0f\uff0c\u5e76\u91c7\u7528\u53cc\u9636\u6bb5\u5408\u4f5c\u7b56\u7565\u6765\u652f\u63f4\u961f\u53cb\uff0c\u4ece\u800c\u964d\u4f4e\u6574\u4f53\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u3002", "result": "ORION\u6846\u67b6\u5728\u90e8\u5206\u5df2\u77e5\u73af\u5883\u4e2d\u7684\u5408\u4f5c\u591a\u667a\u80fd\u4f53\u5728\u7ebf\u5bfc\u822a\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u5728\u52a8\u6001\u5730\u56fe\u5dee\u5f02\u4e0b\u63d0\u4f9b\u9c81\u68d2\u72b6\u6001\u5d4c\u5165\uff0c\u5e76\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u5b9e\u65f6\u7684\u53bb\u4e2d\u5fc3\u5316\u5408\u4f5c\u3002", "conclusion": "\u901a\u8fc7\u5927\u91cf\u6a21\u62df\u548c\u5b9e\u9645\u673a\u5668\u4eba\u56e2\u961f\u9a8c\u8bc1\uff0cORION\u5c55\u793a\u4e86\u5176\u5728\u771f\u5b9e\u4e16\u754c\u5408\u4f5c\u5bfc\u822a\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2601.01247", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.01247", "abs": "https://arxiv.org/abs/2601.01247", "authors": ["Wei Xu"], "title": "Human-Centered Artificial Intelligence (HCAI): Foundations and Approaches", "comment": null, "summary": "Artificial Intelligence (AI) is a transformative yet double-edged technology that can advance human welfare while also posing risks to humans and society. In response, the Human-Centered Artificial Intelligence (HCAI) approach has emerged as both a design philosophy and a methodological complement to prevailing technology-centered AI paradigms. Placing humans at the core, HCAI seeks to ensure that AI systems serve, augment, and empower humans rather than harm or replace them. This chapter establishes the conceptual and methodological foundations of HCAI by tracing its evolution and recent advancements. It introduces key HCAI concepts, frameworks, guiding principles, methodologies, and practical strategies that bridge philosophical HCAI principles with operational implementation. Through an analytical review of the emerging characteristics and challenges of AI technologies, the chapter positions HCAI as a holistic paradigm for aligning AI innovation with human values, societal well-being, and sustainable progress. Finally, this chapter outlines the structure and contributions of the Handbook of Human-Centered Artificial Intelligence. The purpose of this chapter is to provide an integrated foundation that connects HCAI conceptual frameworks, principles, methodology, and practices for this handbook, thereby paving the way for the content of subsequent chapters.", "AI": {"tldr": "\u4eba\u7c7b\u4e2d\u5fc3\u7684\u4eba\u5de5\u667a\u80fd\uff08HCAI\uff09\u4f5c\u4e3a\u4e00\u79cd\u8bbe\u8ba1\u7406\u5ff5\u548c\u65b9\u6cd5\u8bba\u5e94\u8fd0\u800c\u751f\uff0c\u805a\u7126\u4eba\u7c7b\u5229\u76ca\uff0c\u529b\u6c42\u4f7f\u4eba\u5de5\u667a\u80fd\u6280\u672f\u670d\u52a1\u4e8e\u4eba\u7c7b\u798f\u7949\uff0c\u63d0\u4f9b\u8fd9\u4e00\u9886\u57df\u7684\u7efc\u5408\u6846\u67b6\u548c\u65b9\u6cd5\u652f\u6301\u3002", "motivation": "\u56de\u5e94\u4eba\u5de5\u667a\u80fd\u5e26\u6765\u7684\u53cc\u91cd\u6311\u6218\uff0c\u5f3a\u8c03\u4eba\u7c7b\u7684\u4e2d\u5fc3\u5730\u4f4d\uff0c\u4ee5\u786e\u4fddAI\u6280\u672f\u4e3a\u4eba\u7c7b\u670d\u52a1\uff0c\u589e\u8fdb\u793e\u4f1a\u798f\u7949\u3002", "method": "\u901a\u8fc7\u5206\u6790HCAI\u7684\u6f14\u53d8\uff0c\u4ecb\u7ecd\u5176\u5173\u952e\u6982\u5ff5\u3001\u539f\u5219\u3001\u65b9\u6cd5\u548c\u5b9e\u8df5\u7b56\u7565\uff0c\u63a2\u8ba8\u4eba\u5de5\u667a\u80fd\u6280\u672f\u7684\u7279\u5f81\u4e0e\u6311\u6218\u3002", "result": "\u672c\u7ae0\u786e\u7acb\u4e86\u4ee5\u4eba\u7c7b\u4e3a\u4e2d\u5fc3\u7684\u4eba\u5de5\u667a\u80fd\uff08HCAI\uff09\u7684\u6982\u5ff5\u548c\u65b9\u6cd5\u57fa\u7840\uff0c\u8ffd\u6eaf\u5176\u6f14\u53d8\u4e0e\u6700\u65b0\u8fdb\u5c55\uff0c\u4ecb\u7ecd\u4e86\u5173\u952e\u6982\u5ff5\u3001\u6846\u67b6\u548c\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u4eba\u5de5\u667a\u80fd\u6280\u672f\u9762\u4e34\u7684\u6311\u6218\uff0c\u5c06HCAI\u5b9a\u4f4d\u4e3a\u4e00\u79cd\u6574\u4f53\u8303\u5f0f\uff0c\u65e8\u5728\u5c06\u4eba\u5de5\u667a\u80fd\u521b\u65b0\u4e0e\u4eba\u7c7b\u4ef7\u503c\u3001\u793e\u4f1a\u798f\u7949\u53ca\u53ef\u6301\u7eed\u53d1\u5c55\u76f8\u7ed3\u5408\u3002", "conclusion": "\u672c\u7ae0\u4e3a\u300a\u4eba\u7c7b\u4e2d\u5fc3\u7684\u4eba\u5de5\u667a\u80fd\u624b\u518c\u300b\u7684\u5185\u5bb9\u5960\u5b9a\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u57fa\u7840\uff0c\u4fc3\u8fdbHCAI\u539f\u5219\u4e0e\u64cd\u4f5c\u5b9e\u65bd\u4e4b\u95f4\u7684\u8054\u7cfb\u3002"}}
{"id": "2601.01188", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01188", "abs": "https://arxiv.org/abs/2601.01188", "authors": ["Zhiwei Huang", "Yanwei Fu", "Yi Zhou", "Xieyuanli Chen", "Qijun Chen", "Rui Fan"], "title": "DST-Calib: A Dual-Path, Self-Supervised, Target-Free LiDAR-Camera Extrinsic Calibration Network", "comment": null, "summary": "LiDAR-camera extrinsic calibration is essential for multi-modal data fusion in robotic perception systems. However, existing approaches typically rely on handcrafted calibration targets (e.g., checkerboards) or specific, static scene types, limiting their adaptability and deployment in real-world autonomous and robotic applications. This article presents the first self-supervised LiDAR-camera extrinsic calibration network that operates in an online fashion and eliminates the need for specific calibration targets. We first identify a significant generalization degradation problem in prior methods, caused by the conventional single-sided data augmentation strategy. To overcome this limitation, we propose a novel double-sided data augmentation technique that generates multi-perspective camera views using estimated depth maps, thereby enhancing robustness and diversity during training. Built upon this augmentation strategy, we design a dual-path, self-supervised calibration framework that reduces the dependence on high-precision ground truth labels and supports fully adaptive online calibration. Furthermore, to improve cross-modal feature association, we replace the traditional dual-branch feature extraction design with a difference map construction process that explicitly correlates LiDAR and camera features. This not only enhances calibration accuracy but also reduces model complexity. Extensive experiments conducted on five public benchmark datasets, as well as our own recorded dataset, demonstrate that the proposed method significantly outperforms existing approaches in terms of generalizability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u7684LiDAR-\u76f8\u673a\u5916\u90e8\u6821\u51c6\u7f51\u7edc\uff0c\u91c7\u7528\u53cc\u9762\u6570\u636e\u589e\u5f3a\uff0c\u5728\u7ebf\u6821\u51c6\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u901a\u7528\u6027\u4e0e\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684LiDAR-\u76f8\u673a\u5916\u90e8\u6821\u51c6\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u5236\u4f5c\u7684\u6821\u51c6\u76ee\u6807\u6216\u7279\u5b9a\u9759\u6001\u573a\u666f\uff0c\u9650\u5236\u4e86\u5176\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u9002\u5e94\u6027\u548c\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u8def\u5f84\u81ea\u76d1\u7763\u6821\u51c6\u6846\u67b6\uff0c\u7ed3\u5408\u53cc\u9762\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u6539\u8fdb\u4e86\u8de8\u6a21\u6001\u7279\u5f81\u5173\u8054\uff0c\u4ece\u800c\u51cf\u5c11\u4e86\u5bf9\u9ad8\u7cbe\u5ea6\u5730\u9762\u771f\u503c\u6807\u7b7e\u7684\u4f9d\u8d56\u3002", "result": "\u63d0\u51fa\u4e86\u7b2c\u4e00\u79cd\u81ea\u76d1\u7763\u7684LiDAR-\u76f8\u673a\u5916\u90e8\u6821\u51c6\u7f51\u7edc\uff0c\u80fd\u591f\u5728\u7ebf\u5de5\u4f5c\uff0c\u6d88\u9664\u5bf9\u7279\u5b9a\u6821\u51c6\u76ee\u6807\u7684\u9700\u6c42\uff0c\u5e76\u901a\u8fc7\u53cc\u9762\u6570\u636e\u589e\u5f3a\u6280\u672f\u63d0\u9ad8\u4e86\u6821\u51c6\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u901a\u7528\u6027\u548c\u51c6\u786e\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u53ef\u5b9e\u73b0\u5b8c\u5168\u81ea\u9002\u5e94\u7684\u5728\u7ebf\u6821\u51c6\u3002"}}
{"id": "2601.01539", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.01539", "abs": "https://arxiv.org/abs/2601.01539", "authors": ["Mohammad Mahdi Habibi Bina", "Sepideh Baghernezhad", "Mohammad Reza Daliri", "Mohammad Hassan Moradi"], "title": "Neural Digital Twins: Toward Next-Generation Brain-Computer Interfaces", "comment": null, "summary": "Current neural interfaces such as brain-computer interfaces (BCIs) face several fundamental challenges, including frequent recalibration due to neuroplasticity and session-to-session variability, real-time processing latency, limited personalization and generalization across subjects, hardware constraints, surgical risks in invasive systems, and cognitive burden in patients with neurological impairments. These limitations significantly affect the accuracy, stability, and long-term usability of BCIs. This article introduces the concept of the Neural Digital Twin (NDT) as an advanced solution to overcome these barriers. NDT represents a dynamic, personalized computational model of the brain-BCI system that is continuously updated with real-time neural data, enabling prediction of brain states, optimization of control commands, and adaptive tuning of decoding algorithms. The design of NDT draws inspiration from the application of Digital Twin technology in advanced industries such as aerospace and autonomous vehicles, and leverages recent advances in artificial intelligence and neuroscience data acquisition technologies. In this work, we discuss the structure and implementation of NDT and explore its potential applications in next-generation BCIs and neural decoding, highlighting its ability to enhance precision, robustness, and individualized control in neurotechnology.", "AI": {"tldr": "\u795e\u7ecf\u6570\u5b57\u53cc\u80de\u80ce\uff08NDT\uff09\u662f\u4e00\u79cd\u52a8\u6001\u3001\u4e2a\u6027\u5316\u7684\u8ba1\u7b97\u6a21\u578b\uff0c\u53ef\u4ee5\u5b9e\u65f6\u66f4\u65b0\u795e\u7ecf\u6570\u636e\uff0c\u4ee5\u4f18\u5316\u8111\u673a\u63a5\u53e3\u7684\u63a7\u5236\u547d\u4ee4\u548c\u89e3\u7801\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3BCI\u7684\u9891\u7e41\u91cd\u6821\u51c6\u3001\u5b9e\u65f6\u5904\u7406\u5ef6\u8fdf\u3001\u4e2a\u6027\u5316\u4e0d\u8db3\u7b49\u6311\u6218\uff0c\u4ece\u800c\u63d0\u5347\u5176\u957f\u671f\u53ef\u7528\u6027\u548c\u7528\u6237\u4f53\u9a8c\u3002", "method": "NDT\u901a\u8fc7\u5b9e\u65f6\u795e\u7ecf\u6570\u636e\u66f4\u65b0\uff0c\u5229\u7528\u4eba\u5de5\u667a\u80fd\u548c\u795e\u7ecf\u79d1\u5b66\u6570\u636e\u91c7\u96c6\u6280\u672f\uff0c\u5b9e\u73b0\u8111\u72b6\u6001\u9884\u6d4b\u548c\u63a7\u5236\u547d\u4ee4\u4f18\u5316\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u795e\u7ecf\u6570\u5b57\u53cc\u80de\u80ce\uff08NDT\uff09\u7684\u6982\u5ff5\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u8111\u673a\u63a5\u53e3\uff08BCI\uff09\u9762\u4e34\u7684\u591a\u4e2a\u6839\u672c\u6027\u6311\u6218\uff0c\u5e76\u63d0\u9ad8BCI\u7684\u51c6\u786e\u6027\u3001\u7a33\u5b9a\u6027\u548c\u957f\u671f\u53ef\u7528\u6027\u3002", "conclusion": "NDT\u6709\u671b\u901a\u8fc7\u63d0\u9ad8\u795e\u7ecf\u6280\u672f\u7684\u7cbe\u786e\u6027\u3001\u7a33\u5065\u6027\u548c\u4e2a\u6027\u5316\u63a7\u5236\uff0c\u63a8\u52a8\u4e0b\u4e00\u4ee3\u8111\u673a\u63a5\u53e3\u548c\u795e\u7ecf\u89e3\u7801\u7684\u5e94\u7528\u3002"}}
{"id": "2601.01196", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01196", "abs": "https://arxiv.org/abs/2601.01196", "authors": ["Shenqi Lu", "Liangwei Zhang"], "title": "EduSim-LLM: An Educational Platform Integrating Large Language Models and Robotic Simulation for Beginners", "comment": null, "summary": "In recent years, the rapid development of Large Language Models (LLMs) has significantly enhanced natural language understanding and human-computer interaction, creating new opportunities in the field of robotics. However, the integration of natural language understanding into robotic control is an important challenge in the rapid development of human-robot interaction and intelligent automation industries. This challenge hinders intuitive human control over complex robotic systems, limiting their educational and practical accessibility. To address this, we present the EduSim-LLM, an educational platform that integrates LLMs with robot simulation and constructs a language-drive control model that translates natural language instructions into executable robot behavior sequences in CoppeliaSim. We design two human-robot interaction models: direct control and autonomous control, conduct systematic simulations based on multiple language models, and evaluate multi-robot collaboration, motion planning, and manipulation capabilities. Experiential results show that LLMs can reliably convert natural language into structured robot actions; after applying prompt-engineering templates instruction-parsing accuracy improves significantly; as task complexity increases, overall accuracy rate exceeds 88.9% in the highest complexity tests.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faEduSim-LLM\uff0c\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u63a7\u5236\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5b9e\u73b0\u673a\u5668\u4eba\u884c\u4e3a\u8f6c\u6362\uff0c\u89e3\u51b3\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u6311\u6218\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u7684\u53d1\u5c55\uff0c\u63d0\u9ad8\u4eba\u673a\u4ea4\u4e92\u7684\u76f4\u89c2\u6027\u548c\u53ef\u7528\u6027\u6210\u4e3a\u91cd\u8981\u6311\u6218\uff0cEduSim-LLM\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u4fc3\u8fdb\u673a\u5668\u4eba\u6559\u80b2\u548c\u5b9e\u8df5\u7684\u53ef\u53ca\u6027\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u76f4\u63a5\u63a7\u5236\u548c\u81ea\u4e3b\u63a7\u5236\u7684\u4eba\u673a\u4ea4\u4e92\u6a21\u578b\uff0c\u8fdb\u884c\u7cfb\u7edf\u7684\u4eff\u771f\uff0c\u8bc4\u4f30LLMs\u5728\u591a\u673a\u5668\u4eba\u534f\u4f5c\u3001\u8fd0\u52a8\u89c4\u5212\u548c\u64cd\u4f5c\u80fd\u529b\u4e0a\u7684\u5e94\u7528\u6548\u679c\u3002", "result": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86EduSim-LLM\uff0c\u8fd9\u662f\u4e00\u79cd\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u4e0e\u673a\u5668\u4eba\u4eff\u771f\u7ed3\u5408\u7684\u6559\u80b2\u5e73\u53f0\u3002EduSim-LLM\u65e8\u5728\u89e3\u51b3\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4e0e\u673a\u5668\u4eba\u63a7\u5236\u96c6\u6210\u4e2d\u7684\u6311\u6218\uff0c\u901a\u8fc7\u6784\u5efa\u8bed\u8a00\u9a71\u52a8\u63a7\u5236\u6a21\u578b\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7684\u673a\u5668\u4eba\u884c\u4e3a\u5e8f\u5217\u3002\u672c\u6587\u8bbe\u8ba1\u4e86\u76f4\u63a5\u63a7\u5236\u548c\u81ea\u4e3b\u63a7\u5236\u4e24\u79cd\u4eba\u673a\u4ea4\u4e92\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u4e86\u7cfb\u7edf\u7684\u591a\u8bed\u8a00\u6a21\u578b\u4eff\u771f\uff0c\u8bc4\u4f30\u673a\u5668\u4eba\u534f\u4f5c\u3001\u8fd0\u52a8\u89c4\u5212\u548c\u64cd\u4f5c\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLLMs\u80fd\u591f\u53ef\u9760\u5730\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u7684\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u4e14\u7ecf\u8fc7\u63d0\u793a\u5de5\u7a0b\u6a21\u677f\u540e\uff0c\u6307\u4ee4\u89e3\u6790\u7684\u51c6\u786e\u6027\u663e\u8457\u63d0\u9ad8\u3002\u5728\u4efb\u52a1\u590d\u6742\u6027\u589e\u52a0\u7684\u60c5\u51b5\u4e0b\uff0c\u6700\u9ad8\u590d\u6742\u5ea6\u6d4b\u8bd5\u7684\u6574\u4f53\u51c6\u786e\u7387\u8d85\u8fc788.9%\u3002", "conclusion": "EduSim-LLM\u6709\u6548\u5730\u89e3\u51b3\u4e86\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4e0e\u673a\u5668\u4eba\u63a7\u5236\u4e4b\u95f4\u7684\u969c\u788d\uff0c\u63d0\u5347\u4e86\u4eba\u673a\u4ea4\u4e92\u7684\u76f4\u89c2\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2601.01772", "categories": ["cs.HC", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.01772", "abs": "https://arxiv.org/abs/2601.01772", "authors": ["Manh-Dat Nguyen", "Thomas Do", "Nguyen Thanh Trung Le", "Xuan-The Tran", "Fred Chang", "Chin-Teng Lin"], "title": "EdgeSSVEP: A Fully Embedded SSVEP BCI Platform for Low-Power Real-Time Applications", "comment": null, "summary": "Brain-Computer Interfaces (BCIs) enable users to interact with machines directly via neural activity, yet their real-world deployment is often hindered by bulky and powerhungry hardware. We present EdgeSSVEP, a fully embedded microcontroller-based Steady-State Visually Evoked Potential (SSVEP) BCI platform that performs real-time EEG acquisition, zero-phase filtering, and on-device classification within a lowpower 240 MHz MCU operating at only 222 mW. The system incorporates an 8-channel EEG front end, supports 5-second stimulus durations, and executes the entire SSVEP decoding pipeline locally, eliminating dependence on PC-based processing. EdgeSSVEP was evaluated using six stimulus frequencies (7, 8, 9, 11, 7.5, and 8.5 Hz) with 10 participants. The device achieved 99.17% classification accuracy and 27.33 bits/min Information Transfer Rate (ITR), while consuming substantially less power than conventional desktop-based systems. The system integrates motion sensing to support artifact detection and improve robustness and signal stability in practical environments. For development and debugging, the system also provides optional TCP data streaming to external clients. Overall, EdgeSSVEP offers a scalable, energy-efficient, and secure embedded BCI platform suitable for assistive communication and neurofeedback applications, with potential extensions to accelerometer-based artifact mitigation and broader real-world deployments.", "AI": {"tldr": "EdgeSSVEP \u662f\u4e00\u4e2a\u4f4e\u529f\u8017\u7684\u5d4c\u5165\u5f0f SSVE\u8111\u673a\u63a5\u53e3\u7cfb\u7edf\uff0c\u5177\u5907\u5b9e\u65f6 EEG \u5904\u7406\u529f\u80fd\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u9ad8\u8fbe 99.17%\uff0c\u9002\u7528\u4e8e\u8f85\u52a9\u901a\u4fe1\u548c\u795e\u7ecf\u53cd\u9988\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u4f20\u7edf\u8111\u673a\u63a5\u53e3\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4f53\u79ef\u548c\u80fd\u6e90\u6d88\u8017\u95ee\u9898\uff0c\u5f00\u53d1\u51fa\u4e00\u4e2a\u7d27\u51d1\u4e14\u9ad8\u6548\u7684 BCI \u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5fae\u63a7\u5236\u5668\u5b9e\u73b0\u5b9e\u65f6 EEG \u83b7\u53d6\u3001\u96f6\u76f8\u4f4d\u6ee4\u6ce2\u548c\u5206\u7c7b\uff0c\u6574\u5408\u8fd0\u52a8\u4f20\u611f\u4ee5\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "EdgeSSVEP \u662f\u4e00\u4e2a\u57fa\u4e8e\u5fae\u63a7\u5236\u5668\u7684\u5d4c\u5165\u5f0f\u7a33\u6001\u89c6\u89c9\u8bf1\u53d1\u7535\u4f4d (SSVEP) \u8111\u673a\u63a5\u53e3 (BCI) \u5e73\u53f0\uff0c\u5177\u5907\u5b9e\u65f6 EEG \u83b7\u53d6\u3001\u96f6\u76f8\u4f4d\u6ee4\u6ce2\u548c\u5728\u8bbe\u5907\u4e0a\u5206\u7c7b\u7684\u529f\u80fd\u3002\u5b83\u5728\u4ec5\u8017\u7535 222 \u6beb\u74e6\u7684 240 MHz MCU \u4e0a\u8fd0\u884c\uff0c\u96c6\u6210\u4e86 8 \u901a\u9053 EEG \u524d\u7aef\uff0c\u652f\u6301 5 \u79d2\u7684\u523a\u6fc0\u6301\u7eed\u65f6\u95f4\uff0c\u5b8c\u5168\u672c\u5730\u6267\u884c SSVEP \u89e3\u7801\u6d41\u7a0b\uff0c\u6d88\u9664\u4e86\u5bf9 PC \u5904\u7406\u7684\u4f9d\u8d56\u3002\u8be5\u5e73\u53f0\u5728 10 \u540d\u53c2\u4e0e\u8005\u7684\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86 99.17% \u7684\u5206\u7c7b\u51c6\u786e\u7387\u548c 27.33 \u4f4d/\u5206\u949f\u7684\u4fe1\u606f\u4f20\u8f93\u7387 (ITR)\uff0c\u4e14\u529f\u8017\u8fdc\u4f4e\u4e8e\u4f20\u7edf\u684c\u9762\u7cfb\u7edf\u3002EdgeSSVEP \u8fd8\u96c6\u6210\u4e86\u8fd0\u52a8\u4f20\u611f\u5668\u4ee5\u652f\u6301\u4f2a\u5f71\u68c0\u6d4b\uff0c\u589e\u5f3a\u4e86\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u4fe1\u53f7\u7a33\u5b9a\u6027\u3002\u6b64\u5916\uff0c\u8be5\u7cfb\u7edf\u4e3a\u5f00\u53d1\u548c\u8c03\u8bd5\u63d0\u4f9b\u4e86\u53ef\u9009\u7684 TCP \u6570\u636e\u6d41\u529f\u80fd\uff0c\u4ee5\u4fbf\u5411\u5916\u90e8\u5ba2\u6237\u53d1\u9001\u6570\u636e\u3002\u6574\u4f53\u800c\u8a00\uff0cEdgeSSVEP \u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u8282\u80fd\u548c\u5b89\u5168\u7684\u5d4c\u5165\u5f0f BCI \u5e73\u53f0\uff0c\u9002\u7528\u4e8e\u8f85\u52a9\u901a\u4fe1\u548c\u795e\u7ecf\u53cd\u9988\u5e94\u7528\uff0c\u5e76\u5177\u6709\u9488\u5bf9\u52a0\u901f\u5ea6\u8ba1\u7684\u4f2a\u5f71\u7f13\u89e3\u548c\u66f4\u5e7f\u6cdb\u7684\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u7684\u6f5c\u529b\u3002", "conclusion": "EdgeSSVEP \u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u548c\u9ad8\u6548\u7684 BCI \u5e73\u53f0\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.01282", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01282", "abs": "https://arxiv.org/abs/2601.01282", "authors": ["Fang Nan", "Meher Malladi", "Qingqing Li", "Fan Yang", "Joonas Juola", "Tiziano Guadagnino", "Jens Behley", "Cesar Cadena", "Cyrill Stachniss", "Marco Hutter"], "title": "SAHA: Supervised Autonomous HArvester for selective forest thinning", "comment": null, "summary": "Forestry plays a vital role in our society, creating significant ecological, economic, and recreational value. Efficient forest management involves labor-intensive and complex operations. One essential task for maintaining forest health and productivity is selective thinning, which requires skilled operators to remove specific trees to create optimal growing conditions for the remaining ones. In this work, we present a solution based on a small-scale robotic harvester (SAHA) designed for executing this task with supervised autonomy. We build on a 4.5-ton harvester platform and implement key hardware modifications for perception and automatic control. We implement learning- and model-based approaches for precise control of hydraulic actuators, accurate navigation through cluttered environments, robust state estimation, and reliable semantic estimation of terrain traversability. Integrating state-of-the-art techniques in perception, planning, and control, our robotic harvester can autonomously navigate forest environments and reach targeted trees for selective thinning. We present experimental results from extensive field trials over kilometer-long autonomous missions in northern European forests, demonstrating the harvester's ability to operate in real forests. We analyze the performance and provide the lessons learned for advancing robotic forest management.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u6b3e\u5c0f\u578b\u673a\u5668\u4eba\u6536\u5272\u673a\uff08SAHA\uff09\uff0c\u65e8\u5728\u5b9e\u73b0\u9009\u62e9\u6027\u758f\u4f10\u7684\u81ea\u52a8\u5316\uff0c\u7ecf\u8fc7\u4e00\u7cfb\u5217\u786c\u4ef6\u6539\u9020\u53ca\u5148\u8fdb\u6280\u672f\u96c6\u6210\uff0c\u5df2\u5728\u5317\u6b27\u68ee\u6797\u5b8c\u6210\u4e86\u81ea\u4e3b\u8bd5\u9a8c\u3002", "motivation": "\u6797\u4e1a\u5728\u793e\u4f1a\u4e2d\u53d1\u6325\u7740\u91cd\u8981\u4f5c\u7528\uff0c\u521b\u9020\u4e86\u663e\u8457\u7684\u751f\u6001\u3001\u7ecf\u6d4e\u548c\u5a31\u4e50\u4ef7\u503c\uff0c\u7136\u800c\u9ad8\u6548\u7684\u68ee\u6797\u7ba1\u7406\u5374\u9700\u8981\u4eba\u5de5\u5bc6\u96c6\u4e14\u590d\u6742\u7684\u64cd\u4f5c\u3002", "method": "\u57fa\u4e8e4.5\u5428\u7684\u6536\u5272\u673a\u5e73\u53f0\uff0c\u8fdb\u884c\u4e86\u5173\u952e\u786c\u4ef6\u7684\u6539\u9020\u53ca\u5b66\u4e60\u4e0e\u6a21\u578b\u57fa\u7840\u7684\u63a7\u5236\u65b9\u6cd5\uff0c\u786e\u4fdd\u6db2\u538b\u6267\u884c\u5668\u7684\u7cbe\u51c6\u63a7\u5236\u548c\u590d\u6742\u73af\u5883\u4e0b\u7684\u5bfc\u822a\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u578b\u673a\u5668\u4eba\u6536\u5272\u673a\u7684\u89e3\u51b3\u65b9\u6848\uff08SAHA\uff09\uff0c\u80fd\u591f\u5728\u76d1\u7763\u81ea\u4e3b\u7684\u60c5\u51b5\u4e0b\u6267\u884c\u9009\u62e9\u6027\u758f\u4f10\u4efb\u52a1\u3002", "conclusion": "\u8be5\u673a\u5668\u4eba\u6536\u5272\u673a\u80fd\u591f\u5728\u771f\u5b9e\u68ee\u6797\u4e2d\u64cd\u4f5c\uff0c\u5c55\u73b0\u4e86\u5728\u673a\u5668\u4eba\u68ee\u6797\u7ba1\u7406\u9886\u57df\u7684\u8fdb\u6b65\u53ca\u8868\u73b0\u5206\u6790\u3002"}}
{"id": "2601.02044", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.02044", "abs": "https://arxiv.org/abs/2601.02044", "authors": ["Daniel Hienert", "Heiko Schmidt", "Thomas Kr\u00e4mer", "Dagmar Kern"], "title": "EyeLiveMetrics: Real-time Analysis of Online Reading with Eye Tracking", "comment": null, "summary": "Existing eye tracking software have certain limitations, especially with respect to monitoring reading online: (1) Most eye tracking software record eye tracking data as raw coordinates and stimuli as screen images/videos, but without inherent links between both. Analysts must draw areas of interest (AOIs) on webpage text for more fine-grained reading analysis. (2) The computation and analysis of fixation and reading metrics are done after the experiment and thus cannot be used for live applications. We present EyeLiveMetrics, a browser plugin that automatically maps raw gaze coordinates to text in real time. The plugin instantly calculates, stores, and provides fixation, saccade, and reading measures on words and paragraphs so that gaze behavior can be analyzed immediately. We also discuss the results of a comparative evaluation. EyeLiveMetrics offers a flexible way to measure reading on the web - for research experiments and live applications.", "AI": {"tldr": "EyeLiveMetrics\u63d2\u4ef6\u89e3\u51b3\u4e86\u73b0\u6709\u773c\u52a8\u8ffd\u8e2a\u8f6f\u4ef6\u5728\u5728\u7ebf\u9605\u8bfb\u5206\u6790\u4e2d\u7684\u5c40\u9650\uff0c\u63d0\u4f9b\u5b9e\u65f6\u7684\u773c\u52a8\u6570\u636e\u4e0e\u6587\u672c\u7684\u5173\u8054\uff0c\u652f\u6301\u7814\u7a76\u548c\u5b9e\u65f6\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u7684\u773c\u52a8\u8ffd\u8e2a\u8f6f\u4ef6\u5728\u5728\u7ebf\u9605\u8bfb\u76d1\u6d4b\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u65e0\u6cd5\u5b9e\u65f6\u5206\u6790\u9605\u8bfb\u884c\u4e3a\uff0c\u4e14\u65e0\u6cd5\u5c06\u6ce8\u89c6\u6570\u636e\u4e0e\u7f51\u9875\u5185\u5bb9\u5efa\u7acb\u5185\u5728\u8054\u7cfb\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u6b3e\u6d4f\u89c8\u5668\u63d2\u4ef6EyeLiveMetrics\uff0c\u81ea\u52a8\u5c06\u539f\u59cb\u6ce8\u89c6\u5750\u6807\u5b9e\u65f6\u6620\u5c04\u5230\u7f51\u9875\u6587\u672c\uff0c\u5e76\u5373\u65f6\u8ba1\u7b97\u548c\u5b58\u50a8\u6ce8\u89c6\u3001\u626b\u89c6\u53ca\u9605\u8bfb\u6307\u6807\u3002", "result": "EyeLiveMetrics\u5b9e\u65f6\u8ba1\u7b97\u548c\u5b58\u50a8\u9605\u8bfb\u76f8\u5173\u6307\u6807\uff0c\u7ecf\u8fc7\u6bd4\u8f83\u8bc4\u4f30\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "conclusion": "EyeLiveMetrics\u662f\u4e00\u6b3e\u7075\u6d3b\u7684\u6d4f\u89c8\u5668\u63d2\u4ef6\uff0c\u80fd\u591f\u5b9e\u65f6\u8ffd\u8e2a\u548c\u5206\u6790\u5728\u7ebf\u9605\u8bfb\u884c\u4e3a\uff0c\u4e3a\u7814\u7a76\u5b9e\u9a8c\u548c\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u6d4b\u91cf\u65b9\u5f0f\u3002"}}
{"id": "2601.01438", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01438", "abs": "https://arxiv.org/abs/2601.01438", "authors": ["Russell Buchanan", "Adrian R\u00f6fer", "Jo\u00e3o Moura", "Abhinav Valada", "Sethu Vijayakumar"], "title": "Online Estimation and Manipulation of Articulated Objects", "comment": "This preprint has not undergone peer review or any post-submission improvements or corrections. The Version of Record of this article is published in Autonomous Robots, and is available online at [Link will be updated when available]", "summary": "From refrigerators to kitchen drawers, humans interact with articulated objects effortlessly every day while completing household chores. For automating these tasks, service robots must be capable of manipulating arbitrary articulated objects. Recent deep learning methods have been shown to predict valuable priors on the affordance of articulated objects from vision. In contrast, many other works estimate object articulations by observing the articulation motion, but this requires the robot to already be capable of manipulating the object. In this article, we propose a novel approach combining these methods by using a factor graph for online estimation of articulation which fuses learned visual priors and proprioceptive sensing during interaction into an analytical model of articulation based on Screw Theory. With our method, a robotic system makes an initial prediction of articulation from vision before touching the object, and then quickly updates the estimate from kinematic and force sensing during manipulation. We evaluate our method extensively in both simulations and real-world robotic manipulation experiments. We demonstrate several closed-loop estimation and manipulation experiments in which the robot was capable of opening previously unseen drawers. In real hardware experiments, the robot achieved a 75% success rate for autonomous opening of unknown articulated objects.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u5148\u9a8c\u548c\u81ea\u6211\u611f\u77e5\uff0c\u5e2e\u52a9\u673a\u5668\u4eba\u5728\u64cd\u4f5c\u672a\u77e5\u7269\u4f53\u65f6\u5728\u7ebf\u4f30\u8ba1\u5173\u8282\u52a8\u4f5c\u3002", "motivation": "\u4e3a\u4e86\u8ba9\u670d\u52a1\u673a\u5668\u4eba\u9ad8\u6548\u5b8c\u6210\u5bb6\u52a1\u4efb\u52a1\uff0c\u5fc5\u987b\u4f7f\u5176\u80fd\u591f\u64cd\u4f5c\u4efb\u610f\u5173\u8282\u7269\u4f53\u3002", "method": "\u4f7f\u7528\u56e0\u5b50\u56fe\u7ed3\u5408\u5b66\u4e60\u7684\u89c6\u89c9\u5148\u9a8c\u548c\u81ea\u6211\u611f\u77e5\u8fdb\u884c\u5728\u7ebf\u5173\u8282\u4f30\u8ba1\uff0c\u57fa\u4e8e\u87ba\u65cb\u7406\u8bba\u6784\u5efa\u5206\u6790\u6a21\u578b\u3002", "result": "\u5728\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e8675%\u7684\u672a\u77e5\u5173\u8282\u7269\u4f53\u81ea\u4e3b\u6253\u5f00\u6210\u529f\u7387\u3002", "conclusion": "\u673a\u5668\u4eba\u7684\u521d\u6b65\u5173\u8282\u9884\u6d4b\u548c\u540e\u7eed\u7684\u5feb\u901f\u66f4\u65b0\u6709\u6548\u63d0\u9ad8\u4e86\u5728\u73b0\u5b9e\u4e2d\u64cd\u63a7\u672a\u77e5\u7269\u4f53\u7684\u6210\u529f\u7387\u3002"}}
{"id": "2601.02047", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.02047", "abs": "https://arxiv.org/abs/2601.02047", "authors": ["Thomas Kr\u00e4mer", "Daniel Hienert", "Francesco Chiossi", "Thomas Kosch", "Dagmar Kern"], "title": "Escaping the Filter Bubble: Evaluating Electroencephalographic Theta Band Synchronization as Indicator for Selective Exposure in Online News Reading", "comment": null, "summary": "Selective exposure to online news occurs when users favor information that confirms their beliefs, creating filter bubbles and limiting diverse perspectives. Interactive systems can counter this by recommending different perspectives, but to achieve this, they need a real-time metric for selective exposure. We present an experiment where we evaluate Electroencephalography (EEG) and eye tracking as indicators for selective exposure by using eye tracking to recognize which textual parts participants read and using EEG to quantify the magnitude of selective exposure. Participants read online news while we collected EEG and eye movements with their agreement towards the news. We show that the agreement with news correlates positively with the theta band power in the parietal area. Our results indicate that future interactive systems can sense selective exposure using EEG and eye tracking to propose a more balanced information diet. This work presents an integrated experimental setup that identifies selective exposure using gaze and EEG-based metrics.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u901a\u8fc7EEG\u548c\u773c\u52a8\u8ffd\u8e2a\u8bc6\u522b\u5728\u7ebf\u65b0\u95fb\u9009\u62e9\u6027\u66b4\u9732\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u80fd\u591f\u5e2e\u52a9\u672a\u6765\u7cfb\u7edf\u63a8\u8350\u591a\u5143\u4fe1\u606f\u3002", "motivation": "\u7528\u6237\u503e\u5411\u4e8e\u9009\u62e9\u7b26\u5408\u81ea\u5df1\u4fe1\u5ff5\u7684\u4fe1\u606f\uff0c\u4ece\u800c\u4ea7\u751f\u8fc7\u6ee4\u6ce1\u6cab\u5e76\u9650\u5236\u591a\u5143\u89c6\u89d2\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u6536\u96c6\u53c2\u4e0e\u8005\u7684EEG\u548c\u773c\u52a8\u6570\u636e\uff0c\u5206\u6790\u4ed6\u4eec\u5bf9\u5728\u7ebf\u65b0\u95fb\u7684\u8ba4\u540c\u611f\u53ca\u5176\u4e0e\u9009\u62e9\u6027\u66b4\u9732\u7684\u5173\u7cfb\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8111\u7535\u56fe\uff08EEG\uff09\u548c\u773c\u52a8\u8ffd\u8e2a\u4f5c\u4e3a\u9009\u62e9\u6027\u66b4\u9732\u6307\u6807\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u53c2\u4e0e\u8005\u4e0e\u65b0\u95fb\u7684\u8ba4\u540c\u611f\u4e0e\u8111\u7535\u6ce2\u7684theta\u6ce2\u529f\u7387\u6b63\u76f8\u5173\u3002", "conclusion": "\u672a\u6765\u7684\u4e92\u52a8\u7cfb\u7edf\u53ef\u4ee5\u901a\u8fc7EEG\u548c\u773c\u52a8\u8ffd\u8e2a\u611f\u77e5\u9009\u62e9\u6027\u66b4\u9732\uff0c\u63d0\u8bae\u66f4\u5747\u8861\u7684\u4fe1\u606f\u996e\u98df\u3002"}}
{"id": "2601.01561", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01561", "abs": "https://arxiv.org/abs/2601.01561", "authors": ["Yujian Qiu", "Yuqiu Mu", "Wen Yang", "Hao Zhu"], "title": "AIMS: An Adaptive Integration of Multi-Sensor Measurements for Quadrupedal Robot Localization", "comment": null, "summary": "This paper addresses the problem of accurate localization for quadrupedal robots operating in narrow tunnel-like environments. Due to the long and homogeneous characteristics of such scenarios, LiDAR measurements often provide weak geometric constraints, making traditional sensor fusion methods susceptible to accumulated motion estimation errors. To address these challenges, we propose AIMS, an adaptive LiDAR-IMU-leg odometry fusion method for robust quadrupedal robot localization in degenerate environments. The proposed method is formulated within an error-state Kalman filtering framework, where LiDAR and leg odometry measurements are integrated with IMU-based state prediction, and measurement noise covariance matrices are adaptively adjusted based on online degeneracy-aware reliability assessment. Experimental results obtained in narrow corridor environments demonstrate that the proposed method improves localization accuracy and robustness compared with state-of-the-art approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u9002\u5e94\u6027\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u5347\u56db\u8db3\u673a\u5668\u4eba\u5728\u72ed\u7a84\u73af\u5883\u4e2d\u7684\u5b9a\u4f4daccuracy\u548crobustness\u3002", "motivation": "\u5e94\u5bf9\u7a84\u96a7\u9053\u73af\u5883\u4e2d\u56db\u8db3\u673a\u5668\u4eba\u5b9a\u4f4d\u7cbe\u5ea6\u4f4e\u7684\u95ee\u9898\u3002", "method": "AIMS\uff1a\u81ea\u9002\u5e94LiDAR-IMU-\u817f\u90e8\u91cc\u7a0b\u8ba1\u878d\u5408\u65b9\u6cd5", "result": "\u5728\u72ed\u7a84\u8d70\u5eca\u73af\u5883\u4e2d\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u7a33\u5065\u6027\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728\u72ed\u7a84\u73af\u5883\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u63d0\u5347\u4e86\u5b9a\u4f4d\u51c6\u786e\u6027\u3002"}}
{"id": "2601.02082", "categories": ["cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.02082", "abs": "https://arxiv.org/abs/2601.02082", "authors": ["Yueyang Wang", "Mehmet Dogar", "Gustav Markkula"], "title": "Realistic adversarial scenario generation via human-like pedestrian model for autonomous vehicle control parameter optimisation", "comment": null, "summary": "Autonomous vehicles (AVs) are rapidly advancing and are expected to play a central role in future mobility. Ensuring their safe deployment requires reliable interaction with other road users, not least pedestrians. Direct testing on public roads is costly and unsafe for rare but critical interactions, making simulation a practical alternative. Within simulation-based testing, adversarial scenarios are widely used to probe safety limits, but many prioritise difficulty over realism, producing exaggerated behaviours which may result in AV controllers that are overly conservative. We propose an alternative method, instead using a cognitively inspired pedestrian model featuring both inter-individual and intra-individual variability to generate behaviourally plausible adversarial scenarios. We provide a proof of concept demonstration of this method's potential for AV control optimisation, in closed-loop testing and tuning of an AV controller. Our results show that replacing the rule-based CARLA pedestrian with the human-like model yields more realistic gap acceptance patterns and smoother vehicle decelerations. Unsafe interactions occur only for certain pedestrian individuals and conditions, underscoring the importance of human variability in AV testing. Adversarial scenarios generated by this model can be used to optimise AV control towards safer and more efficient behaviour. Overall, this work illustrates how incorporating human-like road user models into simulation-based adversarial testing can enhance the credibility of AV evaluation and provide a practical basis to behaviourally informed controller optimisation.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u884c\u4eba\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210\u66f4\u5177\u73b0\u5b9e\u611f\u7684\u5bf9\u6297\u573a\u666f\uff0c\u4ece\u800c\u4f18\u5316\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u63a7\u5236\u3002", "motivation": "\u5b89\u5168\u5730\u90e8\u7f72\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u9700\u8981\u4e0e\u5176\u4ed6\u9053\u8def\u7528\u6237\uff08\u7279\u522b\u662f\u884c\u4eba\uff09\u53ef\u9760\u4e92\u52a8\uff0c\u4f46\u76f4\u63a5\u5728\u516c\u5171\u9053\u8def\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u65e2\u6602\u8d35\u53c8\u4e0d\u5b89\u5168\uff0c\u56e0\u6b64\u63a2\u7d22\u4eff\u771f\u65b9\u6cd5\u663e\u5f97\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u91c7\u7528\u8ba4\u77e5\u542f\u53d1\u7684\u884c\u4eba\u6a21\u578b\uff0c\u901a\u8fc7\u6a21\u62df\u751f\u6210\u884c\u4e3a\u5408\u7406\u7684\u5bf9\u6297\u60c5\u5883\uff0c\u4ee5\u4fbf\u5728\u95ed\u73af\u6d4b\u8bd5\u4e2d\u4f18\u5316\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u63a7\u5236\u5668\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba4\u77e5\u542f\u53d1\u7684\u884c\u4eba\u6a21\u578b\uff0c\u5728\u4eff\u771f\u6d4b\u8bd5\u4e2d\u751f\u6210\u884c\u4e3a\u4e0a\u5408\u7406\u7684\u5bf9\u6297\u60c5\u5883\uff0c\u4fc3\u8fdb\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08AVs\uff09\u63a7\u5236\u7684\u4f18\u5316\uff0c\u5e76\u5f3a\u8c03\u4e86\u4eba\u7c7b\u53d8\u5f02\u6027\u5728AV\u6d4b\u8bd5\u4e2d\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u901a\u8fc7\u5c06\u4eba\u7c7b\u6a21\u578b\u7eb3\u5165\u4eff\u771f\u6d4b\u8bd5\uff0c\u80fd\u591f\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u8bc4\u4f30\u7684\u53ef\u4fe1\u5ea6\uff0c\u5e76\u4e3a\u57fa\u4e8e\u884c\u4e3a\u7684\u63a7\u5236\u5668\u4f18\u5316\u63d0\u4f9b\u5b9e\u7528\u57fa\u7840\u3002"}}
{"id": "2601.01577", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01577", "abs": "https://arxiv.org/abs/2601.01577", "authors": ["Tran Tien Dat", "Nguyen Hai An", "Nguyen Khanh Viet Dung", "Nguyen Duy Duc"], "title": "HanoiWorld : A Joint Embedding Predictive Architecture BasedWorld Model for Autonomous Vehicle Controller", "comment": null, "summary": "Current attempts of Reinforcement Learning for Autonomous Controller are data-demanding while the results are under-performed, unstable, and unable to grasp and anchor on the concept of safety, and over-concentrating on noise features due to the nature of pixel reconstruction. While current Self-Supervised Learningapproachs that learning on high-dimensional representations by leveraging the JointEmbedding Predictive Architecture (JEPA) are interesting and an effective alternative, as the idea mimics the natural ability of the human brain in acquiring new skill usingimagination and minimal samples of observations. This study introduces Hanoi-World, a JEPA-based world model that using recurrent neural network (RNN) formaking longterm horizontal planning with effective inference time. Experimentsconducted on the Highway-Env package with difference enviroment showcase the effective capability of making a driving plan while safety-awareness, with considerablecollision rate in comparison with SOTA baselines", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8054\u5408\u5d4c\u5165\u9884\u6d4b\u67b6\u6784\uff08JEPA\uff09\u7684Hanoi-World\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09\u8fdb\u884c\u957f\u671f\u89c4\u5212\uff0c\u5c55\u793a\u4e86\u5728\u5b89\u5168\u610f\u8bc6\u4e0b\u9ad8\u6548\u7684\u9a7e\u9a76\u89c4\u5212\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u5728\u81ea\u4e3b\u63a7\u5236\u65b9\u9762\u6570\u636e\u9700\u6c42\u9ad8\u4e14\u7ed3\u679c\u4e0d\u7a33\u5b9a\uff0c\u4e14\u5728\u5b89\u5168\u6027\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u56e0\u6b64\u8feb\u5207\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u57fa\u4e8eJEPA\u7684\u4e16\u754c\u6a21\u578b\u548c\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09\u8fdb\u884c\u957f\u671f\u89c4\u5212\u548c\u6709\u6548\u63a8\u7406\u3002", "result": "\u5728Highway-Env\u5305\u4e2d\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHanoi-World\u80fd\u591f\u5728\u5b89\u5168\u610f\u8bc6\u4e0b\u6709\u6548\u5236\u5b9a\u9a7e\u9a76\u8ba1\u5212\uff0c\u5176\u78b0\u649e\u7387\u663e\u8457\u4f4e\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u57fa\u51c6\u3002", "conclusion": "Hanoi-World\u5c55\u793a\u4e86\u5728\u590d\u6742\u73af\u5883\u4e2d\u8fdb\u884c\u5b89\u5168\u9a7e\u9a76\u89c4\u5212\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e0e\u73b0\u6709\u57fa\u51c6\u76f8\u6bd4\u8868\u73b0\u51fa\u8f83\u4f4e\u7684\u78b0\u649e\u7387\u3002"}}
{"id": "2601.02167", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.02167", "abs": "https://arxiv.org/abs/2601.02167", "authors": ["Wei He", "Xiang Li", "Per Ola Kristensson", "Ge Lin Kan"], "title": "LocoScooter: Designing a Stationary Scooter-Based Locomotion System for Navigation in Virtual Reality", "comment": "11 pages, 10 figures, conditionally accpeted by IEEE Transactions on Visualization and Computer Graphics (IEEE VR 2026)", "summary": "Virtual locomotion remains a challenge in VR, especially in space-limited environments where room-scale walking is impractical. We present LocoScooter, a low-cost, deployable locomotion interface combining foot-sliding on a compact treadmill with handlebar steering inspired by scooter riding. Built from commodity hardware, it supports embodied navigation through familiar, physically engaging movement. In a within-subject study (N = 14), LocoScooter significantly improved immersion, enjoyment, and bodily involvement over joystick navigation, while maintaining comparable efficiency and usability. Despite higher physical demand, users did not report increased fatigue, suggesting familiar movements can enrich VR navigation.", "AI": {"tldr": "LocoScooter\u662f\u4e00\u4e2a\u4f4e\u6210\u672c\u7684\u865a\u62df\u79fb\u52a8\u63a5\u53e3\uff0c\u7ed3\u5408\u4e86\u811a\u6ed1\u548c\u628a\u624b\u8f6c\u5411\uff0c\u63d0\u5347\u4e86\u865a\u62df\u73b0\u5b9e\u4e2d\u7684\u6c89\u6d78\u4f53\u9a8c\u3002", "motivation": "\u5728\u865a\u62df\u73b0\u5b9e\u4e2d\uff0c\u7279\u522b\u662f\u5728\u7a7a\u95f4\u6709\u9650\u7684\u73af\u5883\u4e2d\uff0c\u4f20\u7edf\u7684\u884c\u8d70\u65b9\u5f0f\u5b58\u5728\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5728\u4e00\u4e2a\u5305\u542b14\u540d\u53c2\u4e0e\u8005\u7684\u5b9e\u9a8c\u4e2d\uff0c\u6bd4\u8f83\u4e86LocoScooter\u4e0e\u4f20\u7edf\u64cd\u7eb5\u6746\u5bfc\u822a\u7684\u6548\u679c\u3002", "result": "LocoScooter\u663e\u8457\u63d0\u9ad8\u4e86\u6c89\u6d78\u611f\u3001\u4e50\u8da3\u548c\u8eab\u4f53\u53c2\u4e0e\u611f\uff0c\u5e76\u4e0e\u64cd\u7eb5\u6746\u5bfc\u822a\u76f8\u6bd4\uff0c\u6548\u7387\u548c\u53ef\u7528\u6027\u76f8\u5f53\u3002", "conclusion": "\u5c3d\u7ba1\u7269\u7406\u9700\u6c42\u8f83\u9ad8\uff0c\u7528\u6237\u6ca1\u6709\u62a5\u544a\u75b2\u52b3\u611f\uff0c\u8868\u660e\u719f\u6089\u7684\u52a8\u4f5c\u53ef\u4ee5\u4e30\u5bcc\u865a\u62df\u73b0\u5b9e\u5bfc\u822a\u4f53\u9a8c\u3002"}}
{"id": "2601.01618", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01618", "abs": "https://arxiv.org/abs/2601.01618", "authors": ["Huajie Tan", "Peterson Co", "Yijie Xu", "Shanyu Rong", "Yuheng Ji", "Cheng Chi", "Xiansheng Chen", "Qiongyu Zhang", "Zhongxia Zhao", "Pengwei Wang", "Zhongyuan Wang", "Shanghang Zhang"], "title": "Action-Sketcher: From Reasoning to Action via Visual Sketches for Long-Horizon Robotic Manipulation", "comment": "26 pages, 14 figures", "summary": "Long-horizon robotic manipulation is increasingly important for real-world deployment, requiring spatial disambiguation in complex layouts and temporal resilience under dynamic interaction. However, existing end-to-end and hierarchical Vision-Language-Action (VLA) policies often rely on text-only cues while keeping plan intent latent, which undermines referential grounding in cluttered or underspecified scenes, impedes effective task decomposition of long-horizon goals with close-loop interaction, and limits causal explanation by obscuring the rationale behind action choices. To address these issues, we first introduce Visual Sketch, an implausible visual intermediate that renders points, boxes, arrows, and typed relations in the robot's current views to externalize spatial intent, connect language to scene geometry. Building on Visual Sketch, we present Action-Sketcher, a VLA framework that operates in a cyclic See-Think-Sketch-Act workflow coordinated by adaptive token-gated strategy for reasoning triggers, sketch revision, and action issuance, thereby supporting reactive corrections and human interaction while preserving real-time action prediction. To enable scalable training and evaluation, we curate diverse corpus with interleaved images, text, Visual Sketch supervision, and action sequences, and train Action-Sketcher with a multi-stage curriculum recipe that combines interleaved sequence alignment for modality unification, language-to-sketch consistency for precise linguistic grounding, and imitation learning augmented with sketch-to-action reinforcement for robustness. Extensive experiments on cluttered scenes and multi-object tasks, in simulation and on real-world tasks, show improved long-horizon success, stronger robustness to dynamic scene changes, and enhanced interpretability via editable sketches and step-wise plans. Project website: https://action-sketcher.github.io", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u6846\u67b6Action-Sketcher\uff0c\u5229\u7528\u89c6\u89c9\u8349\u56fe\u6539\u5584\u4efb\u52a1\u7406\u89e3\u548c\u6267\u884c\uff0c\u589e\u5f3a\u52a8\u6001\u4ea4\u4e92\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u4e0e\u5b9e\u9645\u73af\u5883\u7684\u9002\u5e94\u6027\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u64cd\u4f5c\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5e94\u7528\u9700\u6c42\u589e\u52a0\uff0c\u9700\u8981\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u7a7a\u95f4\u660e\u786e\u6027\u548c\u5728\u52a8\u6001\u4e92\u52a8\u4e0b\u7684\u65f6\u95f4\u97e7\u6027\uff0c\u4f46\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u53c2\u8003\u57fa\u7840\u548c\u4efb\u52a1\u5206\u89e3\u7b49\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u8be5\u6846\u67b6\u91c7\u7528\u5faa\u73af\u7684See-Think-Sketch-Act\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5e76\u7ed3\u5408\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u7684token-gated\u7b56\u7565\uff0c\u4ee5\u589e\u5f3a\u63a8\u7406\u3001\u4fee\u6b63\u8349\u56fe\u548c\u6267\u884c\u884c\u52a8\u7684\u80fd\u529b\u3002\u901a\u8fc7\u8de8\u6a21\u6001\u7684\u591a\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\uff0c\u63d0\u5347\u4e86\u6846\u67b6\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u80fd\u529b\u3002", "result": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u89c9\u8bed\u8a00\u884c\u52a8\u6846\u67b6Action-Sketcher\uff0c\u65e8\u5728\u6539\u5584\u957f\u671f\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6548\u679c\uff0c\u8fd9\u4e00\u8fc7\u7a0b\u4f9d\u8d56\u4e8e\u53ef\u89c6\u5316\u8349\u56fe\u6765\u660e\u786e\u7a7a\u95f4\u610f\u56fe\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u4ea4\u4e92\u5b9e\u73b0\u4efb\u52a1\u5206\u89e3\u548c\u56e0\u679c\u89e3\u91ca\u3002", "conclusion": "Action-Sketcher\u901a\u8fc7\u5229\u7528\u89c6\u89c9\u8349\u56fe\u548c\u591a\u9636\u6bb5\u5b66\u4e60\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u957f\u671f\u64cd\u4f5c\u6210\u529f\u7387\u3001\u5e94\u5bf9\u52a8\u6001\u53d8\u5316\u7684\u9c81\u68d2\u6027\uff0c\u4ee5\u53ca\u901a\u8fc7\u53ef\u7f16\u8f91\u8349\u56fe\u548c\u9010\u6b65\u8ba1\u5212\u63d0\u5347\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2601.02214", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.02214", "abs": "https://arxiv.org/abs/2601.02214", "authors": ["Manuela Chessa", "Michela Chessa", "Lorenzo Gerini", "Matteo Martini", "Kaloyana Naneva", "Fabio Solari"], "title": "Cooperation in Virtual Reality: Exploring Environmental Decision-Making through a Real-Effort Threshold Public Goods Game", "comment": null, "summary": "Digital platforms increasingly support collective action initiatives, yet coordinating geographically dispersed users through digital interfaces remains challenging, particularly in threshold settings where success requires critical mass participation. This study investigates how avatar-based social representation in Virtual Reality (VR) influences coordination in threshold collective action problems. Through a randomized controlled experiment with 188 participants organized in 94 pairs, we examine whether brief avatar exposure affects perceived co-presence and coordination outcomes in a two-player threshold public goods game implemented as a real-effort recycling task. We manipulate a single design feature: participants either briefly interact through avatars before the main task (Pre-Task Avatar treatment) or complete an equivalent activity individually without peer visibility (No Pre-Task Avatar treatment). Our findings reveal that minimal avatar exposure significantly increases perceived co-presence and improves strategic coordination, though not through increased contribution quantity. Participants exposed to peer avatars achieve higher social welfare by coordinating to avoid wasteful over-contribution beyond the threshold. Additionally, we identify VR presence-the sense of 'being there' in the virtual environment-as a stronger predictor of task performance than co-presence itself. This research contributes to Information Systems theory by establishing causal pathways from specific design features to presence to coordination outcomes, demonstrates VR as a rigorous experimental methodology for IS research, and provides actionable insights for designing collaborative platforms supporting sustainability initiatives and threshold collective action problems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u865a\u62df\u73b0\u5b9e\u4e2d\u7684\u5934\u50cf\u793e\u4ea4\u8868\u73b0\u5982\u4f55\u5f71\u54cd\u9608\u503c\u96c6\u4f53\u884c\u52a8\u95ee\u9898\u4e2d\u7684\u534f\u8c03\uff0c\u53d1\u73b0\u7b80\u77ed\u7684\u5934\u50cf\u66b4\u9732\u80fd\u663e\u8457\u63d0\u9ad8\u53c2\u4e0e\u8005\u7684\u5171\u540c\u5728\u573a\u611f\u548c\u534f\u8c03\u6548\u679c\uff0c\u8fdb\u800c\u589e\u52a0\u793e\u4f1a\u798f\u5229\u3002", "motivation": "\u968f\u7740\u6570\u5b57\u5e73\u53f0\u7684\u666e\u53ca\uff0c\u5982\u4f55\u534f\u8c03\u5730\u7406\u5206\u6563\u7528\u6237\u7684\u96c6\u4f53\u884c\u52a8\u65e5\u76ca\u590d\u6742\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u5173\u952e\u53c2\u4e0e\u4eba\u6570\u7684\u9608\u503c\u60c5\u5883\u4e2d\u3002", "method": "\u901a\u8fc7188\u540d\u53c2\u4e0e\u8005\u7684\u968f\u673a\u5bf9\u7167\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e86\u4e24\u79cd\u4ea4\u4e92\u8bbe\u7f6e\u4e0b\u7684\u76f8\u5e94\u7ed3\u679c\uff1a\u5728\u4efb\u52a1\u524d\u4e0e\u5934\u50cf\u4ea4\u4e92\u548c\u5728\u65e0\u5934\u50cf\u4ea4\u4e92\u7684\u60c5\u51b5\u4e0b\u5355\u72ec\u5b8c\u6210\u4efb\u52a1\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u6700\u5c0f\u7684\u5934\u50cf\u66b4\u9732\u663e\u8457\u63d0\u9ad8\u4e86\u611f\u89c9\u4e0a\u7684\u5171\u540c\u5728\u573a\u611f\uff0c\u5e76\u6539\u5584\u4e86\u53c2\u4e0e\u8005\u7684\u534f\u8c03\u7ed3\u679c\uff0c\u53c2\u4e0e\u8005\u5728\u6b64\u6761\u4ef6\u4e0b\u6210\u529f\u4f18\u5316\u4e86\u793e\u4f1a\u798f\u5229\u3002", "conclusion": "\u7b80\u77ed\u7684\u5934\u50cf\u66b4\u9732\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u7528\u6237\u7684\u5171\u540c\u5728\u573a\u611f\u548c\u6218\u7565\u534f\u8c03\u80fd\u529b\uff0c\u589e\u5f3a\u96c6\u4f53\u884c\u52a8\u7684\u6548\u76ca\uff0c\u5c24\u5176\u5728\u53ef\u6301\u7eed\u6027\u5021\u8bae\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.01651", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01651", "abs": "https://arxiv.org/abs/2601.01651", "authors": ["Yucheng Xu", "Xiaofeng Mao", "Elle Miller", "Xinyu Yi", "Yang Li", "Zhibin Li", "Robert B. Fisher"], "title": "DemoBot: Efficient Learning of Bimanual Manipulation with Dexterous Hands From Third-Person Human Videos", "comment": null, "summary": "This work presents DemoBot, a learning framework that enables a dual-arm, multi-finger robotic system to acquire complex manipulation skills from a single unannotated RGB-D video demonstration. The method extracts structured motion trajectories of both hands and objects from raw video data. These trajectories serve as motion priors for a novel reinforcement learning (RL) pipeline that learns to refine them through contact-rich interactions, thereby eliminating the need to learn from scratch. To address the challenge of learning long-horizon manipulation skills, we introduce: (1) Temporal-segment based RL to enforce temporal alignment of the current state with demonstrations; (2) Success-Gated Reset strategy to balance the refinement of readily acquired skills and the exploration of subsequent task stages; and (3) Event-Driven Reward curriculum with adaptive thresholding to guide the RL learning of high-precision manipulation. The novel video processing and RL framework successfully achieved long-horizon synchronous and asynchronous bimanual assembly tasks, offering a scalable approach for direct skill acquisition from human videos.", "AI": {"tldr": "DemoBot\u662f\u4e00\u4e2a\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u4e2a\u672a\u6807\u6ce8RGB-D\u89c6\u9891\u6f14\u793a\u5b9e\u73b0\u590d\u6742\u64cd\u4f5c\u6280\u80fd\u7684\u83b7\u53d6\uff0c\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6539\u8fdb\u8fd0\u52a8\u8f68\u8ff9\u3002", "motivation": "\u89e3\u51b3\u5b66\u4e60\u590d\u6742\u64cd\u4f5c\u6280\u80fd\u7684\u6311\u6218\uff0c\u65e0\u9700\u4ece\u96f6\u5f00\u59cb\u5b66\u4e60\uff0c\u5229\u7528\u89c6\u9891\u6570\u636e\u548c\u5f3a\u5316\u5b66\u4e60\u6765\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u3002", "method": "\u901a\u8fc7\u63d0\u53d6\u89c6\u9891\u4e2d\u7684\u52a8\u4f5c\u8f68\u8ff9\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u6280\u80fd\u63d0\u5347\u548c\u7cbe\u7ec6\u64cd\u4f5c\u3002", "result": "DemoBot\u6210\u529f\u5e94\u7528\u4e8e\u957f\u65f6\u95f4\u540c\u6b65\u548c\u5f02\u6b65\u7684\u53cc\u624b\u64cd\u4f5c\u4efb\u52a1\u3002", "conclusion": "DemoBot\u63d0\u4f9b\u4e86\u4e00\u79cd\u4ece\u4eba\u7c7b\u89c6\u9891\u4e2d\u76f4\u63a5\u83b7\u53d6\u6280\u80fd\u7684\u53ef\u6269\u5c55\u65b9\u6cd5\uff0c\u6210\u529f\u5b8c\u6210\u4e86\u957f\u65f6\u95f4\u7684\u53cc\u624b\u7ec4\u88c5\u4efb\u52a1\u3002"}}
{"id": "2601.01946", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.01946", "abs": "https://arxiv.org/abs/2601.01946", "authors": ["Sichao Song", "Yuki Okafuji", "Takuya Iwamoto", "Jun Baba", "Hiroshi Ishiguro"], "title": "From Metrics to Meaning: Insights from a Mixed-Methods Field Experiment on Retail Robot Deployment", "comment": null, "summary": "We report a mixed-methods field experiment of a conversational service robot deployed under everyday staffing discretion in a live bedding store. Over 12 days we alternated three conditions--Baseline (no robot), Robot-only, and Robot+Fixture--and video-annotated the service funnel from passersby to purchase. An explanatory sequential design then used six post-experiment staff interviews to interpret the quantitative patterns.\n  Quantitatively, the robot increased stopping per passerby (highest with the fixture), yet clerk-led downstream steps per stopper--clerk approach, store entry, assisted experience, and purchase--decreased. Interviews explained this divergence: clerks avoided interrupting ongoing robot-customer talk, struggled with ambiguous timing amid conversational latency, and noted child-centered attraction that often satisfied curiosity at the doorway. The fixture amplified visibility but also anchored encounters at the threshold, creating a well-defined micro-space where needs could ``close'' without moving inside.\n  We synthesize these strands into an integrative account from the initial show of interest on the part of a customer to their entering the store and derive actionable guidance. The results advance the understanding of interactions between customers, staff members, and the robot and offer practical recommendations for deploying service robots in high-touch retail.", "AI": {"tldr": "\u670d\u52a1\u673a\u5668\u4eba\u63d0\u9ad8\u4e86\u987e\u5ba2\u505c\u7559\u7387\uff0c\u4f46\u51cf\u5c11\u4e86\u5458\u5de5\u53c2\u4e0e\u7684\u9500\u552e\u6b65\u9aa4\uff0c\u9700\u66f4\u597d\u5730\u534f\u8c03\u4e24\u8005\u4e92\u52a8\u3002", "motivation": "\u63a2\u8ba8\u670d\u52a1\u673a\u5668\u4eba\u5728\u96f6\u552e\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4ee5\u53ca\u5176\u4e0e\u987e\u5ba2\u548c\u5458\u5de5\u7684\u4e92\u52a8\u65b9\u5f0f\u3002", "method": "\u901a\u8fc7\u5b9e\u9645\u90e8\u7f72\u7684\u73b0\u573a\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e86\u65e0\u673a\u5668\u4eba\u3001\u4ec5\u673a\u5668\u4eba\u53ca\u673a\u5668\u4eba+\u5c55\u793a\u67b6\u4e09\u79cd\u6761\u4ef6\u4e0b\u7684\u987e\u5ba2\u884c\u4e3a\uff0c\u5e76\u8fdb\u884c\u4e86\u5458\u5de5\u8bbf\u8c08\u4ee5\u6df1\u5165\u7406\u89e3\u5b9a\u91cf\u7ed3\u679c\u3002", "result": "\u670d\u52a1\u673a\u5668\u4eba\u5728\u5438\u5f15\u987e\u5ba2\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5bf9\u5458\u5de5\u540e\u7eed\u670d\u52a1\u6b65\u9aa4\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002", "conclusion": "\u670d\u52a1\u673a\u5668\u4eba\u80fd\u5438\u5f15\u987e\u5ba2\u8fdb\u5165\u5e97\u5185\uff0c\u4f46\u53ef\u80fd\u4f7f\u5458\u5de5\u670d\u52a1\u53d8\u5f97\u4e0d\u591f\u4e3b\u52a8\uff0c\u9700\u4f18\u5316\u4eba\u673a\u534f\u4f5c\u3002"}}
{"id": "2601.01675", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01675", "abs": "https://arxiv.org/abs/2601.01675", "authors": ["Snehal s. Dikhale", "Karankumar Patel", "Daksh Dhingra", "Itoshi Naramura", "Akinobu Hayashi", "Soshi Iba", "Nawid Jamali"], "title": "VisuoTactile 6D Pose Estimation of an In-Hand Object using Vision and Tactile Sensor Data", "comment": "Accepted for publication in IEEE Robotics and Automation Letters (RA-L), January 2022. Presented at ICRA 2022. This is the author's version of the manuscript", "summary": "Knowledge of the 6D pose of an object can benefit in-hand object manipulation. In-hand 6D object pose estimation is challenging because of heavy occlusion produced by the robot's grippers, which can have an adverse effect on methods that rely on vision data only. Many robots are equipped with tactile sensors at their fingertips that could be used to complement vision data. In this paper, we present a method that uses both tactile and vision data to estimate the pose of an object grasped in a robot's hand. To address challenges like lack of standard representation for tactile data and sensor fusion, we propose the use of point clouds to represent object surfaces in contact with the tactile sensor and present a network architecture based on pixel-wise dense fusion. We also extend NVIDIA's Deep Learning Dataset Synthesizer to produce synthetic photo-realistic vision data and corresponding tactile point clouds. Results suggest that using tactile data in addition to vision data improves the 6D pose estimate, and our network generalizes successfully from synthetic training to real physical robots.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u4e0e\u89e6\u89c9\u6570\u636e\u76846D\u7269\u4f53\u4f4d\u59ff\u4f30\u8ba1\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u63d0\u9ad8\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u5728\u7269\u4f53\u6293\u53d6\u8fc7\u7a0b\u4e2d\uff0c\u673a\u5668\u4eba\u5939\u722a\u7684\u906e\u6321\u4e25\u91cd\u5f71\u54cd\u4ec5\u4f9d\u8d56\u89c6\u89c9\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u56e0\u6b64\u9700\u8981\u7ed3\u5408\u89e6\u89c9\u6570\u636e\u6765\u6539\u8fdb\u7269\u4f53\u4f4d\u59ff\u4f30\u8ba1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u70b9\u4e91\u7684\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u50cf\u7d20\u7ea7\u5bc6\u96c6\u878d\u5408\u5c06\u89e6\u89c9\u6570\u636e\u4e0e\u89c6\u89c9\u6570\u636e\u7ed3\u5408\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u89e6\u89c9\u6570\u636e\u53ef\u4ee5\u63d0\u9ad86D\u4f4d\u59ff\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u5e76\u4e14\u7f51\u7edc\u80fd\u591f\u6709\u6548\u5730\u5728\u5408\u6210\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5de5\u4f5c\u3002", "conclusion": "\u7ed3\u5408\u89e6\u89c9\u6570\u636e\u4e0e\u89c6\u89c9\u6570\u636e\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u7269\u4f53\u76846D\u4f4d\u59ff\u4f30\u8ba1\u7cbe\u786e\u5ea6\uff0c\u5e76\u4e14\u7f51\u7edc\u53ef\u4ee5\u6210\u529f\u5730\u4ece\u5408\u6210\u8bad\u7ec3\u8fc1\u79fb\u5230\u771f\u5b9e\u673a\u5668\u4eba\u3002"}}
{"id": "2601.01705", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01705", "abs": "https://arxiv.org/abs/2601.01705", "authors": ["Kenneth Kwok", "Basura Fernando", "Qianli Xu", "Vigneshwaran Subbaraju", "Dongkyu Choi", "Boon Kiat Quek"], "title": "Explicit World Models for Reliable Human-Robot Collaboration", "comment": "Accepted to AAAI-26 Bridge Program B10: Making Embodied AI Reliable with Testing and Formal Verification", "summary": "This paper addresses the topic of robustness under sensing noise, ambiguous instructions, and human-robot interaction. We take a radically different tack to the issue of reliable embodied AI: instead of focusing on formal verification methods aimed at achieving model predictability and robustness, we emphasise the dynamic, ambiguous and subjective nature of human-robot interactions that requires embodied AI systems to perceive, interpret, and respond to human intentions in a manner that is consistent, comprehensible and aligned with human expectations. We argue that when embodied agents operate in human environments that are inherently social, multimodal, and fluid, reliability is contextually determined and only has meaning in relation to the goals and expectations of humans involved in the interaction. This calls for a fundamentally different approach to achieving reliable embodied AI that is centred on building and updating an accessible \"explicit world model\" representing the common ground between human and AI, that is used to align robot behaviours with human expectations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u4f20\u611f\u5668\u566a\u58f0\u3001\u6a21\u7cca\u6307\u4ee4\u548c\u4eba\u673a\u4ea4\u4e92\u4e0b\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u4e3b\u5f20\u4e00\u79cd\u4ee5\u4eba\u7c7b\u671f\u671b\u4e3a\u4e2d\u5fc3\u7684\u4eba\u5de5\u667a\u80fd\u53ef\u9760\u6027\u65b9\u6cd5\u3002", "motivation": "\u5728\u793e\u4ea4\u3001\u591a\u6a21\u6001\u548c\u6d41\u52a8\u7684\u4eba\u7c7b\u73af\u5883\u4e2d\uff0c\u4f20\u7edf\u6a21\u578b\u96be\u4ee5\u8fbe\u5230\u9c81\u68d2\u6027\uff0c\u56e0\u6b64\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\u4eba\u673a\u4ea4\u4e92\u7684\u52a8\u6001\u548c\u4e3b\u89c2\u7279\u6027\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u81f4\u529b\u4e8e\u5efa\u7acb\u548c\u66f4\u65b0\u4e00\u4e2a\u53ef\u63a5\u8fd1\u7684\"\u663e\u6027\u4e16\u754c\u6a21\u578b\"\uff0c\u7528\u4e8e\u8868\u793a\u4eba\u7c7b\u4e0eAI\u4e4b\u95f4\u7684\u5171\u540c\u7406\u89e3\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u66f4\u597d\u5730\u4f7f\u673a\u5668\u4eba\u5728\u590d\u6742\u4eba\u9645\u4ea4\u4e92\u4e2d\u7406\u89e3\u548c\u54cd\u5e94\u4eba\u7c7b\u610f\u56fe\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u9c81\u68d2\u6027\u5e94\u5728\u5177\u4f53\u60c5\u5883\u4e2d\u8003\u8651\uff0c\u4ee5\u786e\u4fdd\u673a\u5668\u4eba\u884c\u4e3a\u4e0e\u4eba\u7c7b\u671f\u671b\u7684\u4e00\u81f4\u6027\u3002"}}
{"id": "2601.01726", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.01726", "abs": "https://arxiv.org/abs/2601.01726", "authors": ["Wenhui Chu", "Aobo Jin", "Hardik A. Gohel"], "title": "Simulations and Advancements in MRI-Guided Power-Driven Ferric Tools for Wireless Therapeutic Interventions", "comment": "10 pages, 7 figures", "summary": "Designing a robotic system that functions effectively within the specific environment of a Magnetic Resonance Imaging (MRI) scanner requires solving numerous technical issues, such as maintaining the robot's precision and stability under strong magnetic fields. This research focuses on enhancing MRI's role in medical imaging, especially in its application to guide intravascular interventions using robot-assisted devices. A newly developed computational system is introduced, designed for seamless integration with the MRI scanner, including a computational unit and user interface. This system processes MR images to delineate the vascular network, establishing virtual paths and boundaries within vessels to prevent procedural damage. Key findings reveal the system's capability to create tailored magnetic field gradient patterns for device control, considering the vessel's geometry and safety norms, and adapting to different blood flow characteristics for finer navigation. Additionally, the system's modeling aspect assesses the safety and feasibility of navigating pre-set vascular paths. Conclusively, this system, based on the Qt framework and C/C++, with specialized software modules, represents a major step forward in merging imaging technology with robotic aid, significantly enhancing precision and safety in intravascular procedures.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e0eMRI\u626b\u63cf\u4eea\u96c6\u6210\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u65e8\u5728\u63d0\u5347\u8840\u7ba1\u5185\u4ecb\u5165\u7684\u7cbe\u786e\u6027\u548c\u5b89\u5168\u6027\uff0c\u663e\u8457\u63a8\u52a8\u533b\u5b66\u6210\u50cf\u4e0e\u673a\u5668\u4eba\u6280\u672f\u7684\u7ed3\u5408\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63d0\u5347MRI\u5728\u533b\u5b66\u6210\u50cf\u4e2d\u7684\u5e94\u7528\uff0c\u5c24\u5176\u662f\u5728\u6307\u5bfc\u8840\u7ba1\u5185\u4ecb\u5165\u4e2d\u7684\u673a\u5668\u4eba\u8f85\u52a9\u8bbe\u5907\u3002", "method": "\u8be5\u7cfb\u7edf\u57fa\u4e8eQt\u6846\u67b6\u548cC/C++\uff0c\u7ed3\u5408\u4e13\u7528\u8f6f\u4ef6\u6a21\u5757\uff0c\u7528\u4e8e\u4e0eMRI\u626b\u63cf\u4eea\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u521b\u5efa\u9488\u5bf9\u4e0d\u540c\u8840\u6d41\u7279\u5f81\u7684\u5b9a\u5236\u78c1\u573a\u68af\u5ea6\u6a21\u5f0f\uff0c\u8bc4\u4f30\u5bfc\u822a\u9884\u8bbe\u8840\u7ba1\u8def\u5f84\u7684\u5b89\u5168\u6027\u548c\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5bf9\u63d0\u9ad8\u8840\u7ba1\u5185\u4ecb\u5165\u7a0b\u5e8f\u7684\u7cbe\u786e\u6027\u548c\u5b89\u5168\u6027\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2601.01762", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01762", "abs": "https://arxiv.org/abs/2601.01762", "authors": ["Yanhao Wu", "Haoyang Zhang", "Fei He", "Rui Wu", "Congpei Qiu", "Liang Gao", "Wei Ke", "Tong Zhang"], "title": "AlignDrive: Aligned Lateral-Longitudinal Planning for End-to-End Autonomous Driving", "comment": "underreview", "summary": "End-to-end autonomous driving has rapidly progressed, enabling joint perception and planning in complex environments. In the planning stage, state-of-the-art (SOTA) end-to-end autonomous driving models decouple planning into parallel lateral and longitudinal predictions. While effective, this parallel design can lead to i) coordination failures between the planned path and speed, and ii) underutilization of the drive path as a prior for longitudinal planning, thus redundantly encoding static information. To address this, we propose a novel cascaded framework that explicitly conditions longitudinal planning on the drive path, enabling coordinated and collision-aware lateral and longitudinal planning. Specifically, we introduce a path-conditioned formulation that explicitly incorporates the drive path into longitudinal planning. Building on this, the model predicts longitudinal displacements along the drive path rather than full 2D trajectory waypoints. This design simplifies longitudinal reasoning and more tightly couples it with lateral planning. Additionally, we introduce a planning-oriented data augmentation strategy that simulates rare safety-critical events, such as vehicle cut-ins, by adding agents and relabeling longitudinal targets to avoid collision. Evaluated on the challenging Bench2Drive benchmark, our method sets a new SOTA, achieving a driving score of 89.07 and a success rate of 73.18%, demonstrating significantly improved coordination and safety", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ea7\u8054\u6846\u67b6\uff0c\u5c06\u7eb5\u5411\u89c4\u5212\u660e\u786e\u5730\u4e0e\u884c\u9a76\u8def\u5f84\u6761\u4ef6\u7ed3\u5408\uff0c\u4ece\u800c\u5b9e\u73b0\u534f\u8c03\u548c\u4ee5\u78b0\u649e\u4e3a\u610f\u8bc6\u7684\u7eb5\u5411\u548c\u6a2a\u5411\u89c4\u5212\u3002", "motivation": "\u89e3\u51b3\u4e86\u5f53\u524d\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u5728\u89c4\u5212\u9636\u6bb5\u7684\u534f\u8c03\u5931\u6548\u548c\u4fe1\u606f\u5197\u4f59\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8def\u5f84\u6761\u4ef6\u7684\u5f62\u5f0f\uff0c\u7ed3\u5408\u884c\u9a76\u8def\u5f84\u8fdb\u884c\u7eb5\u5411\u89c4\u5212\uff0c\u5e76\u901a\u8fc7\u4e00\u79cd\u89c4\u5212\u5bfc\u5411\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\u6a21\u62df\u7f55\u89c1\u7684\u5b89\u5168\u5173\u952e\u4e8b\u4ef6\u3002", "result": "\u5728Bench2Drive\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u83b7\u5f9789.07\u7684\u9a7e\u9a76\u8bc4\u5206\u548c73.18%\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728Bench2Drive\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bbe\u5b9a\u4e86\u65b0\u7684\u6280\u672f\u6700\u524d\u6cbf\uff0c\u663e\u793a\u51fa\u663e\u8457\u6539\u5584\u7684\u534f\u8c03\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2601.01822", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01822", "abs": "https://arxiv.org/abs/2601.01822", "authors": ["Shiyong Meng", "Tao Zou", "Bolei Chen", "Chaoxu Mu", "Jianxin Wang"], "title": "DisCo-FLoc: Using Dual-Level Visual-Geometric Contrasts to Disambiguate Depth-Aware Visual Floorplan Localization", "comment": "7 pages, 4 figures", "summary": "Since floorplan data is readily available, long-term persistent, and robust to changes in visual appearance, visual Floorplan Localization (FLoc) has garnered significant attention. Existing methods either ingeniously match geometric priors or utilize sparse semantics to reduce FLoc uncertainty. However, they still suffer from ambiguous FLoc caused by repetitive structures within minimalist floorplans. Moreover, expensive but limited semantic annotations restrict their applicability. To address these issues, we propose DisCo-FLoc, which utilizes dual-level visual-geometric Contrasts to Disambiguate depth-aware visual Floc, without requiring additional semantic labels. Our solution begins with a ray regression predictor tailored for ray-casting-based FLoc, predicting a series of FLoc candidates using depth estimation expertise. In addition, a novel contrastive learning method with position-level and orientation-level constraints is proposed to strictly match depth-aware visual features with the corresponding geometric structures in the floorplan. Such matches can effectively eliminate FLoc ambiguity and select the optimal imaging pose from FLoc candidates. Exhaustive comparative studies on two standard visual Floc benchmarks demonstrate that our method outperforms the state-of-the-art semantic-based method, achieving significant improvements in both robustness and accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDisCo-FLoc\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u89c6\u89c9\u5e73\u9762\u5b9a\u4f4d\u4e2d\u7684\u6a21\u7cca\u95ee\u9898\uff0c\u5229\u7528\u53cc\u5c42\u89c6\u89c9-\u51e0\u4f55\u5bf9\u6bd4\u6765\u6d88\u9664\u6df1\u5ea6\u610f\u8bc6\u89c6\u89c9\u5b9a\u4f4d\u7684\u6b67\u4e49\uff0c\u65e0\u9700\u989d\u5916\u7684\u8bed\u4e49\u6807\u7b7e\u3002", "motivation": "\u76ee\u524d\u7684\u89c6\u89c9\u5e73\u9762\u5b9a\u4f4d\u65b9\u6cd5\u5728\u5904\u7406\u5177\u6709\u91cd\u590d\u7ed3\u6784\u7684\u6781\u7b80\u8bbe\u8ba1\u5e73\u9762\u56fe\u65f6\u9762\u4e34\u6a21\u7cca\u6027\u548c\u8bed\u4e49\u6807\u6ce8\u9650\u5236\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u589e\u5f3a\u5176\u9002\u7528\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u9996\u5148\u4f7f\u7528\u6df1\u5ea6\u4f30\u8ba1\u6280\u672f\u7684\u5c04\u7ebf\u56de\u5f52\u9884\u6d4b\u5668\u6765\u9884\u6d4b\u4e00\u7cfb\u5217FLoc\u5019\u9009\u70b9\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f4d\u7f6e\u7ea7\u548c\u65b9\u5411\u7ea7\u7ea6\u675f\u4e25\u683c\u5339\u914d\u6df1\u5ea6\u611f\u77e5\u89c6\u89c9\u7279\u5f81\u4e0e\u5bf9\u5e94\u7684\u51e0\u4f55\u7ed3\u6784\u3002", "result": "\u901a\u8fc7\u5728\u4e24\u4e2a\u6807\u51c6\u89c6\u89c9\u5e73\u9762\u5b9a\u4f4d\u57fa\u51c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u7684\u6bd4\u8f83\u7814\u7a76\uff0c\u8bc1\u660e\u4e86DisCo-FLoc\u65b9\u6cd5\u5728\u589e\u5f3a\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u65b9\u9762\u7684\u5353\u8d8a\u6027\u80fd\u3002", "conclusion": "DisCo-FLoc\u65b9\u6cd5\u5728\u591a\u9879\u6807\u51c6\u89c6\u89c9\u5e73\u9762\u5b9a\u4f4d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u7684\u8bed\u4e49\u57fa\u7840\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2601.01872", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01872", "abs": "https://arxiv.org/abs/2601.01872", "authors": ["Hongbo Duan", "Shangyi Luo", "Zhiyuan Deng", "Yanbo Chen", "Yuanhao Chiang", "Yi Liu", "Fangming Liu", "Xueqian Wang"], "title": "CausalNav: A Long-term Embodied Navigation System for Autonomous Mobile Robots in Dynamic Outdoor Scenarios", "comment": "Accepted by IEEE Robotics and Automation Letters (RA-L)", "summary": "Autonomous language-guided navigation in large-scale outdoor environments remains a key challenge in mobile robotics, due to difficulties in semantic reasoning, dynamic conditions, and long-term stability. We propose CausalNav, the first scene graph-based semantic navigation framework tailored for dynamic outdoor environments. We construct a multi-level semantic scene graph using LLMs, referred to as the Embodied Graph, that hierarchically integrates coarse-grained map data with fine-grained object entities. The constructed graph serves as a retrievable knowledge base for Retrieval-Augmented Generation (RAG), enabling semantic navigation and long-range planning under open-vocabulary queries. By fusing real-time perception with offline map data, the Embodied Graph supports robust navigation across varying spatial granularities in dynamic outdoor environments. Dynamic objects are explicitly handled in both the scene graph construction and hierarchical planning modules. The Embodied Graph is continuously updated within a temporal window to reflect environmental changes and support real-time semantic navigation. Extensive experiments in both simulation and real-world settings demonstrate superior robustness and efficiency.", "AI": {"tldr": "CausalNav\u662f\u9996\u4e2a\u9488\u5bf9\u52a8\u6001\u6237\u5916\u73af\u5883\u7684\u57fa\u4e8e\u573a\u666f\u56fe\u7684\u8bed\u4e49\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c42\u6b21\u8bed\u4e49\u573a\u666f\u56fe\u5b9e\u73b0\u9ad8\u6548\u5bfc\u822a\u3002", "motivation": "\u79fb\u52a8\u673a\u5668\u4eba\u5728\u5927\u89c4\u6a21\u6237\u5916\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u8bed\u8a00\u5f15\u5bfc\u5bfc\u822a\u9762\u4e34\u8bb8\u591a\u6311\u6218\uff0c\u5982\u8bed\u4e49\u63a8\u7406\u3001\u52a8\u6001\u6761\u4ef6\u548c\u957f\u671f\u7a33\u5b9a\u6027\u3002", "method": "\u4f7f\u7528\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u591a\u5c42\u6b21\u8bed\u4e49\u573a\u666f\u56fe\uff0c\u7ed3\u5408\u5b9e\u65f6\u611f\u77e5\u4e0e\u79bb\u7ebf\u5730\u56fe\u6570\u636e\u8fdb\u884c\u5bfc\u822a\u89c4\u5212\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6CausalNav\uff0c\u4f7f\u7528\u573a\u666f\u56fe\u8fdb\u884c\u8bed\u4e49\u5bfc\u822a\u3002", "conclusion": "\u6211\u4eec\u7684\u6846\u67b6\u5728\u4eff\u771f\u53ca\u73b0\u5b9e\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u7a33\u5065\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2601.01948", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01948", "abs": "https://arxiv.org/abs/2601.01948", "authors": ["Zhihao Gu", "Ming Yang", "Difan Zou", "Dong Xu"], "title": "Learning Diffusion Policy from Primitive Skills for Robot Manipulation", "comment": "Accepted to AAAI2026", "summary": "Diffusion policies (DP) have recently shown great promise for generating actions in robotic manipulation. However, existing approaches often rely on global instructions to produce short-term control signals, which can result in misalignment in action generation. We conjecture that the primitive skills, referred to as fine-grained, short-horizon manipulations, such as ``move up'' and ``open the gripper'', provide a more intuitive and effective interface for robot learning. To bridge this gap, we propose SDP, a skill-conditioned DP that integrates interpretable skill learning with conditional action planning. SDP abstracts eight reusable primitive skills across tasks and employs a vision-language model to extract discrete representations from visual observations and language instructions. Based on them, a lightweight router network is designed to assign a desired primitive skill for each state, which helps construct a single-skill policy to generate skill-aligned actions. By decomposing complex tasks into a sequence of primitive skills and selecting a single-skill policy, SDP ensures skill-consistent behavior across diverse tasks. Extensive experiments on two challenging simulation benchmarks and real-world robot deployments demonstrate that SDP consistently outperforms SOTA methods, providing a new paradigm for skill-based robot learning with diffusion policies.", "AI": {"tldr": "SDP\u662f\u4e00\u79cd\u6280\u80fd\u6761\u4ef6\u7684\u6269\u6563\u653f\u7b56\uff0c\u901a\u8fc7\u4f7f\u7528\u53ef\u89e3\u91ca\u7684\u6280\u80fd\u5b66\u4e60\u548c\u6761\u4ef6\u52a8\u4f5c\u89c4\u5212\uff0c\u63d0\u5347\u673a\u5668\u4eba\u5728\u64cd\u63a7\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u6269\u6563\u653f\u7b56\u4f9d\u8d56\u4e8e\u5168\u5c40\u6307\u4ee4\uff0c\u5bb9\u6613\u5bfc\u81f4\u52a8\u4f5c\u751f\u6210\u4e0d\u5339\u914d\uff0c\u7814\u7a76\u4eba\u5458\u5e0c\u671b\u901a\u8fc7\u4f7f\u7528\u7ec6\u7c92\u5ea6\u7684\u77ed\u65f6\u95f4\u64cd\u63a7\u6280\u80fd\u63d0\u4f9b\u66f4\u76f4\u89c2\u6709\u6548\u7684\u673a\u5668\u4eba\u5b66\u4e60\u63a5\u53e3\u3002", "method": "\u63d0\u51faSDP\uff0c\u901a\u8fc7\u63d0\u53d6\u89c6\u89c9\u89c2\u5bdf\u548c\u8bed\u8a00\u6307\u4ee4\u7684\u79bb\u6563\u8868\u793a\uff0c\u8bbe\u8ba1\u8f7b\u91cf\u8def\u7531\u7f51\u7edc\uff0c\u5c06\u590d\u6742\u4efb\u52a1\u5206\u89e3\u4e3a\u4e00\u7cfb\u5217\u539f\u59cb\u6280\u80fd\uff0c\u751f\u6210\u5bf9\u9f50\u6280\u80fd\u7684\u52a8\u4f5c\u3002", "result": "\u5728\u4e24\u4e2a\u6311\u6218\u6027\u7684\u4eff\u771f\u57fa\u51c6\u548c\u771f\u5b9e\u673a\u5668\u4eba\u90e8\u7f72\u4e2d\uff0cSDP\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "SDP\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u786e\u4fdd\u4e86\u4e00\u81f4\u7684\u6280\u80fd\u884c\u4e3a\uff0c\u5e76\u5728\u590d\u6742\u4efb\u52a1\u5206\u89e3\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u662f\u6280\u80fd\u57fa\u7840\u7684\u673a\u5668\u4eba\u5b66\u4e60\u65b0\u8303\u5f0f\u3002"}}
{"id": "2601.01969", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01969", "abs": "https://arxiv.org/abs/2601.01969", "authors": ["Sichao Song", "Yuki Okafuji", "Kaito Ariu", "Amy Koike"], "title": "What you reward is what you learn: Comparing rewards for online speech policy optimization in public HRI", "comment": null, "summary": "Designing policies that are both efficient and acceptable for conversational service robots in open and diverse environments is non-trivial. Unlike fixed, hand-tuned parameters, online learning can adapt to non-stationary conditions. In this paper, we study how to adapt a social robot's speech policy in the wild. During a 12-day in-situ deployment with over 1,400 public encounters, we cast online policy optimization as a multi-armed bandit problem and use Thompson sampling to select among six actions defined by speech rate (slow/normal/fast) and verbosity (concise/detailed). We compare three complementary binary rewards--Ru (user rating), Rc (conversation closure), and Rt (>=2 turns)--and show that each induces distinct arm distributions and interaction behaviors. We complement the online results with offline evaluations that analyze contextual factors (e.g., crowd level, group size) using video-annotated data. Taken together, we distill ready-to-use design lessons for deploying online optimization of speech policies in real public HRI settings.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u4f18\u5316\u793e\u4ea4\u673a\u5668\u4eba\u7684\u8bed\u8a00\u7b56\u7565\uff0c\u91c7\u7528\u5728\u7ebf\u5b66\u4e60\u65b9\u6cd5\u548c\u591a\u81c2\u8001\u864e\u673a\u7406\u8bba\uff0c\u63d0\u4f9b\u4e86\u5b9e\u65bd\u5728\u7ebf\u4f18\u5316\u7684\u8bbe\u8ba1\u7ecf\u9a8c\u3002", "motivation": "\u4e3a\u5728\u5f00\u653e\u548c\u591a\u6837\u5316\u73af\u5883\u4e2d\u8bbe\u8ba1\u9ad8\u6548\u4e14\u53ef\u63a5\u53d7\u7684\u4f1a\u8bdd\u670d\u52a1\u673a\u5668\u4eba\u653f\u7b56\u63d0\u51fa\u6311\u6218\u3002", "method": "\u5728\u7ebf\u5b66\u4e60\u4e0e\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\u7ed3\u5408\uff0c\u91c7\u7528\u6c64\u666e\u68ee\u91c7\u6837\u4f18\u5316\u793e\u4ea4\u673a\u5668\u4eba\u8bed\u8a00\u7b56\u7565\u3002", "result": "\u901a\u8fc7\u5bf91400\u591a\u6b21\u516c\u5171\u4ea4\u4e92\u7684\u73b0\u573a\u90e8\u7f72\uff0c\u6bd4\u8f83\u4e86\u4e09\u79cd\u8865\u5145\u7684\u4e8c\u5143\u5956\u52b1\u5e76\u5206\u6790\u4e86\u673a\u5668\u4eba\u4e0e\u7528\u6237\u7684\u4ea4\u4e92\u884c\u4e3a\u3002", "conclusion": "\u7814\u7a76\u603b\u7ed3\u4e86\u5728\u5b9e\u9645\u516c\u5171\u4eba\u673a\u4ea4\u4e92\u4e2d\u5b9e\u65bd\u8bed\u8a00\u7b56\u7565\u5728\u7ebf\u4f18\u5316\u7684\u53ef\u7528\u8bbe\u8ba1\u7ecf\u9a8c\u3002"}}
{"id": "2601.01971", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01971", "abs": "https://arxiv.org/abs/2601.01971", "authors": ["Aditya Singh", "Rajpal Singh", "Jishnu Keshavan"], "title": "Deep Robust Koopman Learning from Noisy Data", "comment": null, "summary": "Koopman operator theory has emerged as a leading data-driven approach that relies on a judicious choice of observable functions to realize global linear representations of nonlinear systems in the lifted observable space. However, real-world data is often noisy, making it difficult to obtain an accurate and unbiased approximation of the Koopman operator. The Koopman operator generated from noisy datasets is typically corrupted by noise-induced bias that severely degrades prediction and downstream tracking performance. In order to address this drawback, this paper proposes a novel autoencoder-based neural architecture to jointly learn the appropriate lifting functions and the reduced-bias Koopman operator from noisy data. The architecture initially learns the Koopman basis functions that are consistent for both the forward and backward temporal dynamics of the system. Subsequently, by utilizing the learned forward and backward temporal dynamics, the Koopman operator is synthesized with a reduced bias making the method more robust to noise compared to existing techniques. Theoretical analysis is used to demonstrate significant bias reduction in the presence of training noise. Dynamics prediction and tracking control simulations are conducted for multiple serial manipulator arms, including performance comparisons with leading alternative designs, to demonstrate its robustness under various noise levels. Experimental studies with the Franka FR3 7-DoF manipulator arm are further used to demonstrate the effectiveness of the proposed approach in a practical setting.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u63d0\u5347\u51fd\u6570\u548c\u51cf\u5c0f\u504f\u5dee\u7684Koopman\u7b97\u5b50\uff0c\u514b\u670d\u4e86\u4f7f\u7528\u566a\u58f0\u6570\u636e\u65f6\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u901a\u5e38\u5305\u542b\u566a\u58f0\uff0c\u5bfc\u81f4\u96be\u4ee5\u83b7\u5f97\u51c6\u786e\u7684Koopman\u7b97\u5b50\u8fd1\u4f3c\uff1b\u566a\u58f0\u5f15\u5165\u7684\u504f\u5dee\u4e25\u91cd\u5f71\u54cd\u9884\u6d4b\u548c\u8ddf\u8e2a\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u81ea\u7f16\u7801\u5668\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u8054\u5408\u5b66\u4e60\u9002\u5f53\u7684\u63d0\u5347\u51fd\u6570\u548c\u51cf\u5c11\u504f\u5dee\u7684Koopman\u7b97\u5b50\u3002", "result": "\u901a\u8fc7\u5b66\u4e60\u4e00\u81f4\u7684Koopman\u57fa\u51fd\u6570\u548c\u5408\u6210\u51cf\u5c11\u504f\u5dee\u7684Koopman\u7b97\u5b50\uff0c\u65b9\u6cd5\u5728\u591a\u79cd\u566a\u58f0\u6c34\u5e73\u4e0b\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u4eff\u771f\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5728\u8bad\u7ec3\u566a\u58f0\u5f71\u54cd\u4e0b\u7684\u663e\u8457\u504f\u5dee\u51cf\u5c11\uff0c\u4e14\u5728\u5b9e\u9645\u60c5\u51b5\u4e0b\u5c55\u73b0\u51fa\u826f\u597d\u6548\u679c\u3002"}}
{"id": "2601.02078", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.02078", "abs": "https://arxiv.org/abs/2601.02078", "authors": ["Chenghao Yin", "Da Huang", "Di Yang", "Jichao Wang", "Nanshu Zhao", "Chen Xu", "Wenjun Sun", "Linjie Hou", "Zhijun Li", "Junhui Wu", "Zhaobo Liu", "Zhen Xiao", "Sheng Zhang", "Lei Bao", "Rui Feng", "Zhenquan Pang", "Jiayu Li", "Qian Wang", "Maoqing Yao"], "title": "Genie Sim 3.0 : A High-Fidelity Comprehensive Simulation Platform for Humanoid Robot", "comment": null, "summary": "The development of robust and generalizable robot learning models is critically contingent upon the availability of large-scale, diverse training data and reliable evaluation benchmarks. Collecting data in the physical world poses prohibitive costs and scalability challenges, and prevailing simulation benchmarks frequently suffer from fragmentation, narrow scope, or insufficient fidelity to enable effective sim-to-real transfer. To address these challenges, we introduce Genie Sim 3.0, a unified simulation platform for robotic manipulation. We present Genie Sim Generator, a large language model (LLM)-powered tool that constructs high-fidelity scenes from natural language instructions. Its principal strength resides in rapid and multi-dimensional generalization, facilitating the synthesis of diverse environments to support scalable data collection and robust policy evaluation. We introduce the first benchmark that pioneers the application of LLM for automated evaluation. It leverages LLM to mass-generate evaluation scenarios and employs Vision-Language Model (VLM) to establish an automated assessment pipeline. We also release an open-source dataset comprising more than 10,000 hours of synthetic data across over 200 tasks. Through systematic experimentation, we validate the robust zero-shot sim-to-real transfer capability of our open-source dataset, demonstrating that synthetic data can server as an effective substitute for real-world data under controlled conditions for scalable policy training. For code and dataset details, please refer to: https://github.com/AgibotTech/genie_sim.", "AI": {"tldr": "Genie Sim 3.0\u662f\u4e00\u4e2a\u65b0\u7684\u673a\u5668\u4eba\u6a21\u62df\u5e73\u53f0\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u652f\u6301\u9ad8\u6548\u6570\u636e\u6536\u96c6\u548c\u8bc4\u4f30\uff0c\u5e76\u63d0\u4f9b\u5f00\u6e90\u6570\u636e\u96c6\u3002", "motivation": "\u5f00\u53d1\u5f3a\u5927\u4e14\u5177\u6709\u666e\u9002\u6027\u7684\u673a\u5668\u4eba\u5b66\u4e60\u6a21\u578b\u9700\u8981\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u7684\u8bad\u7ec3\u6570\u636e\u548c\u53ef\u9760\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u800c\u5728\u7269\u7406\u4e16\u754c\u4e2d\u6536\u96c6\u6570\u636e\u9762\u4e34\u9ad8\u6210\u672c\u548c\u53ef\u6269\u5c55\u6027\u6311\u6218\u3002", "method": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u9ad8\u4fdd\u771f\u573a\u666f\u7684\u5de5\u5177\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5efa\u7acb\u81ea\u52a8\u8bc4\u4f30\u7ba1\u9053\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u6570\u636e\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u63a8\u51faGenie Sim 3.0\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u6a21\u62df\u5e73\u53f0\uff0c\u53ca\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u4ece\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u6784\u5efa\u9ad8\u4fdd\u771f\u573a\u666f\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\uff0c\u672c\u7814\u7a76\u9a8c\u8bc1\u4e86\u5f00\u653e\u6e90\u4ee3\u7801\u6570\u636e\u96c6\u7684\u5f3a\u5927\u96f6-shot sim-to-real\u8fc1\u79fb\u80fd\u529b\uff0c\u8868\u660e\u5728\u63a7\u5236\u6761\u4ef6\u4e0b\uff0c\u5408\u6210\u6570\u636e\u53ef\u6709\u6548\u66ff\u4ee3\u771f\u5b9e\u6570\u636e\u3002"}}
{"id": "2601.02085", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02085", "abs": "https://arxiv.org/abs/2601.02085", "authors": ["Meili Sun", "Chunjiang Zhao", "Lichao Yang", "Hao Liu", "Shimin Hu", "Ya Xiong"], "title": "Vision-Based Early Fault Diagnosis and Self-Recovery for Strawberry Harvesting Robots", "comment": null, "summary": "Strawberry harvesting robots faced persistent challenges such as low integration of visual perception, fruit-gripper misalignment, empty grasping, and strawberry slippage from the gripper due to insufficient gripping force, all of which compromised harvesting stability and efficiency in orchard environments. To overcome these issues, this paper proposed a visual fault diagnosis and self-recovery framework that integrated multi-task perception with corrective control strategies. At the core of this framework was SRR-Net, an end-to-end multi-task perception model that simultaneously performed strawberry detection, segmentation, and ripeness estimation, thereby unifying visual perception with fault diagnosis. Based on this integrated perception, a relative error compensation method based on the simultaneous target-gripper detection was designed to address positional misalignment, correcting deviations when error exceeded the tolerance threshold. To mitigate empty grasping and fruit-slippage faults, an early abort strategy was implemented. A micro-optical camera embedded in the end-effector provided real-time visual feedback, enabling grasp detection during the deflating stage and strawberry slip prediction during snap-off through MobileNet V3-Small classifier and a time-series LSTM classifier. Experiments demonstrated that SRR-Net maintained high perception accuracy. For detection, it achieved a precision of 0.895 and recall of 0.813 on strawberries, and 0.972/0.958 on hands. In segmentation, it yielded a precision of 0.887 and recall of 0.747 for strawberries, and 0.974/0.947 for hands. For ripeness estimation, SRR-Net attained a mean absolute error of 0.035, while simultaneously supporting multi-task perception and sustaining a competitive inference speed of 163.35 FPS.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u591a\u4efb\u52a1\u611f\u77e5\u4e0e\u81ea\u6211\u4fee\u590d\u7684\u89c6\u89c9\u6545\u969c\u8bca\u65ad\u6846\u67b6SRR-Net\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8349\u8393\u6536\u83b7\u673a\u5668\u4eba\u4e2d\u7684\u591a\u4e2a\u5173\u952e\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u611f\u77e5\u7cbe\u5ea6\u4e0e\u6536\u83b7\u6548\u7387\u3002", "motivation": "\u9488\u5bf9\u73b0\u6709\u8349\u8393\u6536\u83b7\u673a\u5668\u4eba\u9762\u4e34\u7684\u89c6\u89c9\u611f\u77e5\u4f4e\u96c6\u6210\u5ea6\u3001\u679c\u5b9e\u6293\u53d6\u5bf9\u9f50\u95ee\u9898\u548c\u6293\u53d6\u4e0d\u8db3\u7b49\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u4ee5\u63d0\u5347\u6536\u83b7\u673a\u5668\u4eba\u6027\u80fd\u3002", "method": "\u4f7f\u7528SRR-Net\u4f5c\u4e3a\u6838\u5fc3\u6a21\u578b\u8fdb\u884c\u591a\u4efb\u52a1\u611f\u77e5\uff0c\u5305\u62ec\u8349\u8393\u68c0\u6d4b\u3001\u5206\u5272\u548c\u6210\u719f\u5ea6\u4f30\u8ba1\uff0c\u540c\u65f6\u96c6\u6210\u4e86\u6545\u969c\u8bca\u65ad\u548c\u81ea\u6211\u4fee\u590d\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cSRR-Net\u5728\u89c6\u89c9\u611f\u77e5\u65b9\u9762\u5177\u6709\u9ad8\u7cbe\u5ea6\uff0c\u8349\u8393\u68c0\u6d4b\u7cbe\u5ea6\u4e3a0.895\uff0c\u53ec\u56de\u7387\u4e3a0.813\uff0c\u540c\u65f6\u652f\u6301\u591a\u4e2a\u4efb\u52a1\u5e76\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "SRR-Net\u5728\u8349\u8393\u6536\u83b7\u673a\u5668\u4eba\u4e2d\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u611f\u77e5\u548c\u6545\u969c\u8bca\u65ad\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6536\u83b7\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2601.02125", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02125", "abs": "https://arxiv.org/abs/2601.02125", "authors": ["Zhuoxiong Xu", "Xuanchen Li", "Yuhao Cheng", "Fei Xu", "Yichao Yan", "Xiaokang Yang"], "title": "SingingBot: An Avatar-Driven System for Robotic Face Singing Performance", "comment": null, "summary": "Equipping robotic faces with singing capabilities is crucial for empathetic Human-Robot Interaction. However, existing robotic face driving research primarily focuses on conversations or mimicking static expressions, struggling to meet the high demands for continuous emotional expression and coherence in singing. To address this, we propose a novel avatar-driven framework for appealing robotic singing. We first leverage portrait video generation models embedded with extensive human priors to synthesize vivid singing avatars, providing reliable expression and emotion guidance. Subsequently, these facial features are transferred to the robot via semantic-oriented mapping functions that span a wide expression space. Furthermore, to quantitatively evaluate the emotional richness of robotic singing, we propose the Emotion Dynamic Range metric to measure the emotional breadth within the Valence-Arousal space, revealing that a broad emotional spectrum is crucial for appealing performances. Comprehensive experiments prove that our method achieves rich emotional expressions while maintaining lip-audio synchronization, significantly outperforming existing approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5934\u50cf\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u4ee5\u63d0\u5347\u673a\u5668\u4eba\u7684\u5531\u6b4c\u8868\u73b0\uff0c\u901a\u8fc7\u89c6\u9891\u751f\u6210\u6a21\u578b\u5408\u6210\u751f\u52a8\u7684\u5531\u6b4c\u5934\u50cf\uff0c\u663e\u8457\u63d0\u9ad8\u60c5\u611f\u8868\u8fbe\u548c\u53e3\u578b\u4e0e\u97f3\u9891\u7684\u540c\u6b65\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u4eba\u9762\u90e8\u9a71\u52a8\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5bf9\u8bdd\u6216\u6a21\u4eff\u9759\u6001\u8868\u60c5\uff0c\u65e0\u6cd5\u6ee1\u8db3\u8fde\u7eed\u60c5\u611f\u8868\u8fbe\u548c\u5531\u6b4c\u7684\u4e00\u81f4\u6027\u7684\u9ad8\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u5934\u50cf\u9a71\u52a8\u6846\u67b6\uff0c\u751f\u6210\u5f15\u4eba\u5165\u80dc\u7684\u673a\u5668\u4eba\u5531\u6b4c\u8868\u6f14\u3002", "result": "\u901a\u8fc7\u60c5\u611f\u52a8\u6001\u8303\u56f4\u6307\u6807\u7684\u91cf\u5316\u8bc4\u4f30\uff0c\u8868\u660e\u5e7f\u6cdb\u7684\u60c5\u611f\u8c31\u5bf9\u5438\u5f15\u4eba\u7684\u8868\u6f14\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u7ecf\u8fc7\u5168\u9762\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u60c5\u611f\u8868\u8fbe\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5531\u6b4c\u7684\u60c5\u611f\u8868\u73b0\u529b\u3002"}}
{"id": "2601.02184", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.02184", "abs": "https://arxiv.org/abs/2601.02184", "authors": ["Yuhang Zhang", "S\u00f6ren Schwertfeger"], "title": "Differential Barometric Altimetry for Submeter Vertical Localization and Floor Recognition Indoors", "comment": null, "summary": "Accurate altitude estimation and reliable floor recognition are critical for mobile robot localization and navigation within complex multi-storey environments. In this paper, we present a robust, low-cost vertical estimation framework leveraging differential barometric sensing integrated within a fully ROS-compliant software package. Our system simultaneously publishes real-time altitude data from both a stationary base station and a mobile sensor, enabling precise and drift-free vertical localization. Empirical evaluations conducted in challenging scenarios -- such as fully enclosed stairwells and elevators, demonstrate that our proposed barometric pipeline achieves sub-meter vertical accuracy (RMSE: 0.29 m) and perfect (100%) floor-level identification. In contrast, our results confirm that standalone height estimates, obtained solely from visual- or LiDAR-based SLAM odometry, are insufficient for reliable vertical localization. The proposed ROS-compatible barometric module thus provides a practical and cost-effective solution for robust vertical awareness in real-world robotic deployments. The implementation of our method is released as open source at https://github.com/witsir/differential-barometric.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5dee\u5206\u6c14\u538b\u4f20\u611f\u7684\u4f4e\u6210\u672c\u6846\u67b6\uff0c\u5b9e\u73b0\u79fb\u52a8\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u51c6\u786e\u5782\u76f4\u5b9a\u4f4d\u548c\u697c\u5c42\u8bc6\u522b\u3002", "motivation": "\u79fb\u52a8\u673a\u5668\u4eba\u5728\u590d\u6742\u591a\u5c42\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u548c\u5bfc\u822a\u9700\u8981\u51c6\u786e\u7684\u9ad8\u5ea6\u4f30\u8ba1\u548c\u53ef\u9760\u7684\u697c\u5c42\u8bc6\u522b\u3002", "method": "\u5229\u7528\u5dee\u5206\u6c14\u538b\u4f20\u611f\u5668\u6280\u672f\uff0c\u96c6\u6210\u5728\u5168ROS\u517c\u5bb9\u7684\u8f6f\u4ef6\u5305\u4e2d\uff0c\u5b9e\u65f6\u53d1\u5e03\u9ad8\u5ea6\u6570\u636e\u3002", "result": "\u63d0\u51fa\u7684\u6c14\u538b\u7ba1\u9053\u5728\u8270\u96be\u73af\u5883\u4e2d\u6d4b\u8bd5\u8868\u660e\uff0c\u5b9e\u73b0\u4e86\u4e9a\u7c73\u7ea7\u5782\u76f4\u7cbe\u5ea6\uff08RMSE\uff1a0.29 m\uff09\u548c100%\u7684\u697c\u5c42\u8bc6\u522b\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u548c\u9ad8\u6548\u7684\u5782\u76f4\u4f30\u8ba1\u6846\u67b6\uff0c\u786e\u4fdd\u4e86\u79fb\u52a8\u673a\u5668\u4eba\u5728\u591a\u5c42\u73af\u5883\u4e2d\u7684\u51c6\u786e\u5b9a\u4f4d\u548c\u697c\u5c42\u8bc6\u522b\u3002"}}
{"id": "2601.02295", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.02295", "abs": "https://arxiv.org/abs/2601.02295", "authors": ["Chenyang Ma", "Guangyu Yang", "Kai Lu", "Shitong Xu", "Bill Byrne", "Niki Trigoni", "Andrew Markham"], "title": "CycleVLA: Proactive Self-Correcting Vision-Language-Action Models via Subtask Backtracking and Minimum Bayes Risk Decoding", "comment": "Project Page: https://dannymcy.github.io/cyclevla/", "summary": "Current work on robot failure detection and correction typically operate in a post hoc manner, analyzing errors and applying corrections only after failures occur. This work introduces CycleVLA, a system that equips Vision-Language-Action models (VLAs) with proactive self-correction, the capability to anticipate incipient failures and recover before they fully manifest during execution. CycleVLA achieves this by integrating a progress-aware VLA that flags critical subtask transition points where failures most frequently occur, a VLM-based failure predictor and planner that triggers subtask backtracking upon predicted failure, and a test-time scaling strategy based on Minimum Bayes Risk (MBR) decoding to improve retry success after backtracking. Extensive experiments show that CycleVLA improves performance for both well-trained and under-trained VLAs, and that MBR serves as an effective zero-shot test-time scaling strategy for VLAs. Project Page: https://dannymcy.github.io/cyclevla/", "AI": {"tldr": "CycleVLA\u662f\u4e00\u4e2a\u5177\u5907\u4e3b\u52a8\u81ea\u6211\u7ea0\u6b63\u80fd\u529b\u7684\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u6545\u969c\u53d1\u751f\u524d\u9884\u6d4b\u5e76\u6062\u590d\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u6545\u969c\u68c0\u6d4b\u4e0e\u7ea0\u6b63\u5de5\u4f5c\u901a\u5e38\u662f\u5728\u6545\u969c\u53d1\u751f\u540e\u8fdb\u884c\u7684\uff0c\u7f3a\u4e4f\u4e3b\u52a8\u9884\u9632\u80fd\u529b\u3002", "method": "CycleVLA\u6574\u5408\u4e86\u4e00\u4e2a\u8fdb\u5ea6\u611f\u77e5VLA\u3001\u57fa\u4e8eVLM\u7684\u6545\u969c\u9884\u6d4b\u4e0e\u89c4\u5212\uff0c\u4ee5\u53ca\u57fa\u4e8e\u6700\u5c0f\u8d1d\u53f6\u65af\u98ce\u9669\u7684\u6d4b\u8bd5\u65f6\u7f29\u653e\u7b56\u7565\u3002", "result": "\u5faa\u73afVLA\u7cfb\u7edf\u80fd\u591f\u4e3b\u52a8\u8bc6\u522b\u5373\u5c06\u53d1\u751f\u7684\u6545\u969c\u5e76\u5728\u6267\u884c\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u81ea\u6211\u7ea0\u6b63\u3002", "conclusion": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCycleVLA\u63d0\u5347\u4e86VLA\u7684\u6267\u884c\u6027\u80fd\uff0c\u4e14MBR\u89e3\u7801\u7b56\u7565\u5728\u56de\u9000\u540e\u63d0\u5347\u4e86\u91cd\u8bd5\u6210\u529f\u7387\u3002"}}
