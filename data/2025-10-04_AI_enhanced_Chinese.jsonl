{"id": "2510.01187", "categories": ["cs.HC", "cs.GR", "D.1.7"], "pdf": "https://arxiv.org/pdf/2510.01187", "abs": "https://arxiv.org/abs/2510.01187", "authors": ["Christina Zhang"], "title": "Manim for STEM Education: Visualizing Complex Problems Through Animation", "comment": null, "summary": "Many STEM concepts pose significant learning challenges to students due to\ntheir inherent complexity and abstract nature. Visualizing complex problems\nthrough animations can significantly enhance learning outcomes. However, the\ncreation of animations can be time-consuming and inconvenient. Hence, many\neducators illustrate complex concepts by hand on a board or a digital device.\nAlthough static graphics are helpful for understanding, they are less effective\nthan animations. The free and open-source Python package Manim enables\neducators to create visually compelling animations easily. Python's\nstraightforward syntax, combined with Manim's comprehensive set of built-in\nclasses and methods, greatly simplifies implementation. This article presents a\nseries of examples that demonstrate how Manim can be used to create animated\nvideo lessons for a variety of topics in computer science and mathematics. In\naddition, it analyzes viewer feedback collected across multiple social media\nplatforms to evaluate the effectiveness and accessibility of these\nvisualizations. The article further explores broader potentials of the Manim\nPython library by showcasing demonstrations that extend its applications to\nsubject areas beyond computer science and mathematics.", "AI": {"tldr": "\u4f7f\u7528Manim\u5e93\u521b\u9020\u52a8\u753b\u8bfe\u7a0b\u53ef\u4ee5\u6781\u5927\u63d0\u9ad8STEM\u9886\u57df\u7684\u5b66\u4e60\u6548\u679c\uff0c\u5c24\u5176\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u548c\u6570\u5b66\u4e2d\uff0c\u540c\u65f6\u8be5\u5e93\u4e5f\u6709\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u8bb8\u591aSTEM\u6982\u5ff5\u672c\u8eab\u5177\u6709\u590d\u6742\u6027\u548c\u62bd\u8c61\u6027\uff0c\u52a8\u753b\u80fd\u591f\u6709\u6548\u63d0\u5347\u5b66\u4e60\u6548\u679c\uff0c\u4f46\u5236\u4f5c\u52a8\u753b\u901a\u5e38\u8d39\u65f6\u8d39\u529b\u3002", "method": "\u4f7f\u7528Manim\u5e93\u521b\u5efa\u52a8\u753b\u89c6\u9891\u8bfe\u7a0b\uff0c\u5e76\u901a\u8fc7\u793e\u4ea4\u5a92\u4f53\u5206\u6790\u89c2\u4f17\u53cd\u9988\u6765\u8bc4\u4f30\u5176\u6709\u6548\u6027\u548c\u53ef\u53ca\u6027\u3002", "result": "\u5c55\u793a\u4e86Manim\u5982\u4f55\u7528\u4e8e\u8ba1\u7b97\u673a\u79d1\u5b66\u548c\u6570\u5b66\u7684\u52a8\u753b\u89c6\u9891\u6559\u5b66\uff0c\u540c\u65f6\u63a2\u7d22\u4e86\u5176\u5728\u5176\u4ed6\u5b66\u79d1\u7684\u6f5c\u5728\u5e94\u7528\u3002", "conclusion": "Manim\u5e93\u4e0d\u4ec5\u80fd\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u548c\u6570\u5b66\u9886\u57df\u521b\u9020\u6709\u6548\u7684\u52a8\u753b\u89c6\u9891\u8bfe\u7a0b\uff0c\u8fd8\u80fd\u6269\u5c55\u5e94\u7528\u5230\u5176\u4ed6\u5b66\u79d1\uff0c\u4ece\u800c\u63d0\u5347\u5b66\u4e60\u6548\u679c\u3002"}}
{"id": "2510.01188", "categories": ["cs.HC", "H.5.3; K.4.3"], "pdf": "https://arxiv.org/pdf/2510.01188", "abs": "https://arxiv.org/abs/2510.01188", "authors": ["Xinhui Ye", "Joep Frens", "Jun Hu"], "title": "Beyond Divergence: Characterizing Co-exploration Patterns in Collaborative Design Processes", "comment": "accepted by She Ji: The Journal of Design, Economics, and Innovation.\n  will be published in the September Issue. 29 pages, 13 figures, 1 table in\n  the Appendix", "summary": "Exploration is crucial in the design process and is known for its essential\nrole in fostering creativity and enhancing design outcomes. Within design\nteams, exploration evolves into co-exploration, a collaborative and dynamic\npractice that this study aims to unpack. To investigate this experience, we\nconducted a longitudinal observational study with 61 students across 16 design\nteams. Over five months of weekly diary-interviews, we uncovered the intricate\ndynamics of co-exploration. Our main contribution is a four-dimensional\nframework that identifies five distinct patterns of co-exploration activities.\nOur findings reveal how co-exploration emerges across various activities\nthroughout the design process, demonstrating its role in different team\ninteractions. It fosters a sense of togetherness, keeping design teams\nopen-minded and engaged. This engagement cultivates collective intelligence,\nenabling teams to actively share knowledge, build upon each other's ideas, and\nachieve outcomes beyond individual contributions. Our study underscores the\nvalue of co-exploration, suggesting that it reflects the trajectory of design\nsuccess and warrants further research. We also provide actionable insights,\nequipping future practitioners with strategies to enhance co-exploration in\ndesign collaborations.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5bf9\u8bbe\u8ba1\u56e2\u961f\u8fdb\u884c\u7684\u957f\u671f\u89c2\u5bdf\uff0c\u63a2\u8ba8\u4e86\u534f\u540c\u63a2\u7d22\u7684\u91cd\u8981\u6027\u53ca\u5176\u5bf9\u8bbe\u8ba1\u6210\u529f\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e94\u79cd\u534f\u540c\u63a2\u7d22\u6d3b\u52a8\u7684\u6a21\u5f0f\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u7b56\u7565\u3002", "motivation": "\u63a2\u8ba8\u534f\u4f5c\u4e0e\u52a8\u6001\u5b9e\u8df5\u5728\u8bbe\u8ba1\u56e2\u961f\u4e2d\u7684\u4f5c\u7528\uff0c\u4ee5\u589e\u8fdb\u521b\u9020\u529b\u548c\u8bbe\u8ba1\u6210\u679c\u3002", "method": "\u8fdb\u884c\u4e86\u4e3a\u671f\u4e94\u4e2a\u6708\u7684\u7eb5\u5411\u89c2\u5bdf\u7814\u7a76\uff0c\u6db5\u76d661\u540d\u5b66\u751f\u768416\u4e2a\u8bbe\u8ba1\u56e2\u961f\uff0c\u91c7\u7528\u6bcf\u5468\u7684\u65e5\u8bb0\u8bbf\u8c08\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u56db\u7ef4\u6846\u67b6\uff0c\u8bc6\u522b\u5230\u4e94\u79cd\u4e0d\u540c\u7684\u534f\u540c\u63a2\u7d22\u6d3b\u52a8\u6a21\u5f0f\uff0c\u63ed\u793a\u4e86\u534f\u540c\u63a2\u7d22\u5982\u4f55\u5728\u8bbe\u8ba1\u8fc7\u7a0b\u4e2d\u7684\u5404\u79cd\u6d3b\u52a8\u4e2d\u51fa\u73b0\u3002", "conclusion": "\u534f\u540c\u63a2\u7d22\u4fc3\u8fdb\u4e86\u8bbe\u8ba1\u56e2\u961f\u4e4b\u95f4\u7684\u4e92\u52a8\u548c\u96c6\u4f53\u667a\u80fd\uff0c\u53cd\u6620\u4e86\u8bbe\u8ba1\u6210\u529f\u7684\u8f68\u8ff9\u3002"}}
{"id": "2510.01189", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01189", "abs": "https://arxiv.org/abs/2510.01189", "authors": ["Gianluca De Ninno", "Paola Inverardi", "Francesca Belotti"], "title": "An Anthropologist LLM to Elicit Users' Moral Preferences through Role-Play", "comment": null, "summary": "This study investigates a novel approach to eliciting users' moral\ndecision-making by combining immersive roleplaying games with LLM analysis\ncapabilities. Building on the distinction introduced by Floridi between hard\nethics inspiring and shaping laws-and soft ethics-moral preferences guiding\nindividual behavior within the free space of decisions compliant to laws-we\nfocus on capturing the latter through contextrich, narrative-driven\ninteractions. Grounded in anthropological methods, the role-playing game\nexposes participants to ethically charged scenarios in the domain of digital\nprivacy. Data collected during the sessions were interpreted by a customized\nLLM (\"GPT Anthropologist\"). Evaluation through a cross-validation process shows\nthat both the richness of the data and the interpretive framing significantly\nenhance the model's ability to predict user behavior. Results show that LLMs\ncan be effectively employed to automate and enhance the understanding of user\nmoral preferences and decision-making process in the early stages of software\ndevelopment.", "AI": {"tldr": "\u672c\u7814\u7a76\u7ed3\u5408\u89d2\u8272\u626e\u6f14\u6e38\u620f\u548cLLM\u5206\u6790\uff0c\u63a2\u7d22\u7528\u6237\u9053\u5fb7\u51b3\u7b56\uff0c\u901a\u8fc7\u6570\u636e\u5206\u6790\u63d0\u9ad8\u5bf9\u7528\u6237\u884c\u4e3a\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u5bcc\u6709\u80cc\u666f\u548c\u53d9\u4e8b\u9a71\u52a8\u7684\u4e92\u52a8\u6355\u6349\u7528\u6237\u7684\u9053\u5fb7\u51b3\u7b56\uff0c\u7279\u522b\u662f\u8f6f\u4f26\u7406\u5728\u6cd5\u5f8b\u5408\u89c4\u51b3\u7b56\u4e2d\u7684\u5f71\u54cd\u3002", "method": "\u7ed3\u5408\u6c89\u6d78\u5f0f\u89d2\u8272\u626e\u6f14\u6e38\u620f\u548c\u5b9a\u5236\u7684LLM\u5206\u6790\u80fd\u529b\uff0c\u6536\u96c6\u53c2\u4e0e\u8005\u5728\u6570\u5b57\u9690\u79c1\u9886\u57df\u7684\u4f26\u7406\u60c5\u5883\u4e2d\u7684\u6570\u636e\u3002", "result": "\u901a\u8fc7\u4ea4\u53c9\u9a8c\u8bc1\u8fc7\u7a0b\u7684\u8bc4\u4f30\uff0c\u6570\u636e\u7684\u4e30\u5bcc\u6027\u548c\u89e3\u91ca\u6846\u67b6\u663e\u8457\u589e\u5f3a\u4e86\u6a21\u578b\u9884\u6d4b\u7528\u6237\u884c\u4e3a\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0cLLM\u53ef\u4ee5\u6709\u6548\u5730\u81ea\u52a8\u5316\u5e76\u589e\u5f3a\u7528\u6237\u9053\u5fb7\u504f\u597d\u548c\u51b3\u7b56\u8fc7\u7a0b\u7684\u7406\u89e3\uff0c\u5c24\u5176\u5728\u8f6f\u4ef6\u5f00\u53d1\u7684\u65e9\u671f\u9636\u6bb5\u3002"}}
{"id": "2510.01191", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.01191", "abs": "https://arxiv.org/abs/2510.01191", "authors": ["Paul-Otto M\u00fcller", "Sven Suppelt", "Mario Kupnik", "Oskar von Stryk"], "title": "An Optical Measurement System for Open-Source Tracking of Jaw Motions", "comment": "4 pages, 3 figures, accepted at 2025 IEEE Sensors", "summary": "Precise tracking of the jaw kinematics is crucial for diagnosing various\nmusculoskeletal and neuromuscular diseases affecting the masticatory system and\nfor advancing rehabilitative devices such as jaw exoskeletons, a hardly\nexplored research field, to treat these disorders. We introduce an open-source,\nlow-cost, precise, non-invasive, and biocompatible jaw tracking system based on\noptical motion capture technology to address the need for accessible and\nadaptable research tools. The system encompasses a complete pipeline from data\nacquisition, processing, and kinematic analysis to filtering, visualization,\nand data storage. We evaluated its performance and feasibility in experiments\nwith four participants executing various jaw movements. The system demonstrated\nreliable kinematic tracking with an estimated precision of $(182 \\pm 47)\n{\\mu}m$ and $(0.126 \\pm 0.034) {\\deg}$. Therefore, the open-source nature of\nthe system and its utility comparable to commercial systems make it suitable\nfor many research and development contexts, especially for applications such as\nthe integration and design of jaw exoskeletons and customized diagnostic\nprotocols. The complete system is available at GitHub with the aim of promoting\ninnovation in temporomandibular disorders research and jaw assistive\ntechnology.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u5f00\u6e90\u7684\u4f4e\u6210\u672c\u975e\u4fb5\u5165\u6027\u988c\u90e8\u8ddf\u8e2a\u7cfb\u7edf\uff0c\u9002\u7528\u4e8e\u5480\u56bc\u76f8\u5173\u75be\u75c5\u7814\u7a76\u548c\u5916\u9aa8\u9abc\u8bbe\u8ba1\uff0c\u5df2\u5728GitHub\u53d1\u5e03\u3002", "motivation": "\u7cbe\u786e\u8ddf\u8e2a\u988c\u90e8\u8fd0\u52a8\u5b66\u5bf9\u4e8e\u8bca\u65ad\u5f71\u54cd\u5480\u56bc\u7cfb\u7edf\u7684\u5404\u79cd\u808c\u8089\u9aa8\u9abc\u548c\u795e\u7ecf\u808c\u8089\u75be\u75c5\u81f3\u5173\u91cd\u8981\uff0c\u540c\u65f6\u63a8\u52a8\u76f8\u5173\u5eb7\u590d\u8bbe\u5907\uff08\u5982\u988c\u90e8\u5916\u9aa8\u9abc\uff09\u7684\u7814\u7a76\u3002", "method": "\u57fa\u4e8e\u5149\u5b66\u8fd0\u52a8\u6355\u6349\u6280\u672f\uff0c\u5efa\u7acb\u4e86\u4e00\u4e2a\u4f4e\u6210\u672c\u3001\u7cbe\u786e\u3001\u975e\u4fb5\u5165\u6027\u548c\u751f\u7269\u76f8\u5bb9\u7684\u988c\u90e8\u8ddf\u8e2a\u7cfb\u7edf\uff0c\u6db5\u76d6\u4e86\u6570\u636e\u91c7\u96c6\u3001\u5904\u7406\u3001\u8fd0\u52a8\u5b66\u5206\u6790\u3001\u8fc7\u6ee4\u3001\u53ef\u89c6\u5316\u548c\u6570\u636e\u5b58\u50a8\u7684\u5b8c\u6574\u6d41\u7a0b\u3002", "result": "\u7cfb\u7edf\u5728\u56db\u540d\u53c2\u4e0e\u8005\u6267\u884c\u4e0d\u540c\u988c\u90e8\u8fd0\u52a8\u7684\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4e86\u53ef\u9760\u7684\u8fd0\u52a8\u5b66\u8ddf\u8e2a\uff0c\u4f30\u8ba1\u7cbe\u5ea6\u4e3a$(182 \times  47) {\textmu}m$\u548c$(0.126 \times 0.034) {\textdeg}$\u3002", "conclusion": "\u8be5\u5f00\u6e90\u988c\u90e8\u8ddf\u8e2a\u7cfb\u7edf\u9002\u7528\u4e8e\u591a\u79cd\u7814\u7a76\u4e0e\u5f00\u53d1\u80cc\u666f\uff0c\u5c24\u5176\u662f\u5728\u988c\u9aa8\u5916\u9aa8\u9abc\u7684\u96c6\u6210\u4e0e\u8bbe\u8ba1\u4ee5\u53ca\u5b9a\u5236\u8bca\u65ad\u534f\u8bae\u5e94\u7528\u4e2d\u3002"}}
{"id": "2510.01348", "categories": ["cs.RO", "x", "I.2.9"], "pdf": "https://arxiv.org/pdf/2510.01348", "abs": "https://arxiv.org/abs/2510.01348", "authors": ["Michal Werner", "David \u010capek", "Tom\u00e1\u0161 Musil", "Ond\u0159ej Fran\u011bk", "Tom\u00e1\u0161 B\u00e1\u010da", "Martin Saska"], "title": "Kilometer-Scale GNSS-Denied UAV Navigation via Heightmap Gradients: A Winning System from the SPRIN-D Challenge", "comment": "8 pages", "summary": "Reliable long-range flight of unmanned aerial vehicles (UAVs) in GNSS-denied\nenvironments is challenging: integrating odometry leads to drift, loop closures\nare unavailable in previously unseen areas and embedded platforms provide\nlimited computational power. We present a fully onboard UAV system developed\nfor the SPRIN-D Funke Fully Autonomous Flight Challenge, which required 9 km\nlong-range waypoint navigation below 25 m AGL (Above Ground Level) without GNSS\nor prior dense mapping. The system integrates perception, mapping, planning,\nand control with a lightweight drift-correction method that matches\nLiDAR-derived local heightmaps to a prior geo-data heightmap via\ngradient-template matching and fuses the evidence with odometry in a clustered\nparticle filter. Deployed during the competition, the system executed\nkilometer-scale flights across urban, forest, and open-field terrain and\nreduced drift substantially relative to raw odometry, while running in real\ntime on CPU-only hardware. We describe the system architecture, the\nlocalization pipeline, and the competition evaluation, and we report practical\ninsights from field deployment that inform the design of GNSS-denied UAV\nautonomy.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u65b0\u7684ONBOARD\u7cfb\u7edf\uff0c\u6210\u529f\u51cf\u5c11\u4e86GNSS\u53d7\u9650\u73af\u5883\u4e0b\u65e0\u4eba\u673a\u957f\u9014\u98de\u884c\u7684\u6f02\u79fb\uff0c\u63d0\u4f9b\u4e86\u5bf9\u65e0\u4eba\u673a\u81ea\u4e3b\u6027\u7684\u5b9e\u7528\u89c1\u89e3\u3002", "motivation": "\u5728GNSS\u53d7\u9650\u73af\u5883\u4e2d\uff0c\u65e0\u4eba\u673a\u7684\u957f\u9014\u98de\u884c\u5177\u6709\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u6f02\u79fb\u95ee\u9898\u548c\u7f3a\u4e4f\u56de\u73af\u95ed\u5408\u3002", "method": "\u6574\u5408\u611f\u77e5\u3001\u5730\u56fe\u6784\u5efa\u3001\u89c4\u5212\u548c\u63a7\u5236\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u6f02\u79fb\u6821\u6b63\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u6a21\u677f\u5339\u914d\u5c06LiDAR\u751f\u6210\u7684\u5c40\u90e8\u9ad8\u5ea6\u56fe\u4e0e\u4e4b\u524d\u7684\u5730\u7406\u6570\u636e\u9ad8\u5ea6\u56fe\u8fdb\u884c\u5339\u914d\uff0c\u5e76\u5728\u805a\u7c7b\u7c92\u5b50\u6ee4\u6ce2\u5668\u4e2d\u878d\u5408\u8bc1\u636e\u4e0e\u91cc\u7a0b\u8ba1\u6570\u636e\u3002", "result": "\u7cfb\u7edf\u5728\u7ade\u4e89\u4e2d\u6210\u529f\u6267\u884c\u4e86\u516c\u91cc\u7ea7\u7684\u98de\u884c\u4efb\u52a1\uff0c\u6db5\u76d6\u57ce\u5e02\u3001\u68ee\u6797\u548c\u5f00\u9614\u5730\u5e26\uff0c\u76f8\u8f83\u4e8e\u539f\u59cb\u91cc\u7a0b\u8ba1\u6570\u636e\u663e\u8457\u51cf\u5c11\u6f02\u79fb\uff0c\u540c\u65f6\u5728\u4ec5\u4f7f\u7528CPU\u7684\u786c\u4ef6\u4e0a\u5b9e\u65f6\u8fd0\u884c\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5728GNSS\u53d7\u9650\u73af\u5883\u4e2d\u6210\u529f\u6267\u884c\u4e86\u957f\u671f\u65e0\u4eba\u673a\u81ea\u4e3b\u98de\u884c\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6f02\u79fb\u3002"}}
{"id": "2510.01192", "categories": ["cs.HC", "cs.CY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01192", "abs": "https://arxiv.org/abs/2510.01192", "authors": ["Isabel Pedersen", "Andrea Slane"], "title": "Better Than \"Better Than Nothing\": Design Strategies for Enculturated Empathetic AI Robot Companions for Older Adults", "comment": "26 pages, 6 figures, version submitted to journal", "summary": "The paper asserts that emulating empathy in human-robot interaction is a key\ncomponent to achieve satisfying social, trustworthy, and ethical robot\ninteraction with older people. Following comments from older adult study\nparticipants, the paper identifies a gap. Despite the acceptance of robot care\nscenarios, participants expressed the poor quality of the social aspect.\nCurrent human-robot designs, to a certain extent, neglect to include empathy as\na theorized design pathway. Using rhetorical theory, this paper defines the\nsocio-cultural expectations for convincing empathetic relationships. It\nanalyzes and then summarizes how society understands, values, and negotiates\nempathic interaction between human companions in discursive exchanges, wherein\nempathy acts as a societal value system. Using two public research collections\non robots, with one geared specifically to gerontechnology for older people, it\nsubstantiates the lack of attention to empathy in public materials produced by\nrobot companies. This paper contends that using an empathetic care vocabulary\nas a design pathway is a productive underlying foundation for designing\nhumanoid social robots that aim to support older people's goals of\naging-in-place. It argues that the integration of affective AI into the\nsociotechnical assemblages of human-socially assistive robot interaction ought\nto be scrutinized to ensure it is based on genuine cultural values involving\nempathetic qualities.", "AI": {"tldr": "\u672c\u6587\u5f3a\u8c03\u5728\u8001\u5e74\u4eba\u673a\u5668\u4eba\u4e92\u52a8\u4e2d\u5f15\u5165\u540c\u7406\u5fc3\u8bbe\u8ba1\u8def\u5f84\u7684\u91cd\u8981\u6027\uff0c\u5efa\u8bae\u5e94\u91cd\u89c6\u5176\u6587\u5316\u4ef7\u503c\u4e0e\u793e\u4f1a\u4e92\u52a8\u8d28\u91cf\u7684\u63d0\u5347\u3002", "motivation": "\u63a2\u7d22\u673a\u5668\u4eba\u5728\u8001\u5e74\u62a4\u7406\u573a\u666f\u4e2d\u7684\u63a5\u53d7\u5ea6\u4e0e\u793e\u4f1a\u4e92\u52a8\u8d28\u91cf\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5f3a\u8c03\u540c\u7406\u5fc3\u5728\u8bbe\u8ba1\u4e2d\u7684\u91cd\u8981\u6027\u3002", "method": "\u901a\u8fc7\u4fee\u8f9e\u7406\u8bba\u5206\u6790\u4eba\u7c7b\u4f34\u4fa3\u4e4b\u95f4\u540c\u7406\u5fc3\u4e92\u52a8\u7684\u793e\u4f1a\u6587\u5316\u671f\u671b\uff0c\u5e76\u7814\u7a76\u76f8\u5173\u516c\u5171\u7814\u7a76\u8d44\u6599\u3002", "result": "\u53d1\u73b0\u73b0\u6709\u673a\u5668\u4eba\u8bbe\u8ba1\u5ffd\u89c6\u4e86\u540c\u7406\u5fc3\uff0c\u63d0\u51fa\u4ee5\u540c\u7406\u5fc3\u5173\u6000\u8bcd\u6c47\u4e3a\u8bbe\u8ba1\u57fa\u7840\uff0c\u4ee5\u66f4\u597d\u652f\u6301\u8001\u5e74\u4eba\u7684\u72ec\u7acb\u751f\u6d3b\u76ee\u6807\u3002", "conclusion": "\u5728\u8001\u5e74\u4eba\u673a\u5668\u4eba\u4e92\u52a8\u4e2d\uff0c\u878d\u5165\u540c\u7406\u5fc3\u7684\u8bbe\u8ba1\u8def\u5f84\u662f\u63d0\u5347\u4eba\u673a\u4e92\u52a8\u8d28\u91cf\u7684\u6709\u6548\u7b56\u7565\uff0c\u9700\u91cd\u89c6\u6587\u5316\u4ef7\u503c\u89c2\u4e0e\u540c\u7406\u5fc3\u7279\u5f81\u7684\u7ed3\u5408\u3002"}}
{"id": "2510.01357", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01357", "abs": "https://arxiv.org/abs/2510.01357", "authors": ["Alejandro Gonzalez-Garcia", "Wei Xiao", "Wei Wang", "Alejandro Astudillo", "Wilm Decr\u00e9", "Jan Swevers", "Carlo Ratti", "Daniela Rus"], "title": "Safe Motion Planning and Control Using Predictive and Adaptive Barrier Methods for Autonomous Surface Vessels", "comment": "IROS 2025", "summary": "Safe motion planning is essential for autonomous vessel operations,\nespecially in challenging spaces such as narrow inland waterways. However,\nconventional motion planning approaches are often computationally intensive or\noverly conservative. This paper proposes a safe motion planning strategy\ncombining Model Predictive Control (MPC) and Control Barrier Functions (CBFs).\nWe introduce a time-varying inflated ellipse obstacle representation, where the\ninflation radius is adjusted depending on the relative position and attitude\nbetween the vessel and the obstacle. The proposed adaptive inflation reduces\nthe conservativeness of the controller compared to traditional fixed-ellipsoid\nobstacle formulations. The MPC solution provides an approximate motion plan,\nand high-order CBFs ensure the vessel's safety using the varying inflation\nradius. Simulation and real-world experiments demonstrate that the proposed\nstrategy enables the fully-actuated autonomous robot vessel to navigate through\nnarrow spaces in real time and resolve potential deadlocks, all while ensuring\nsafety.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408MPC\u548cCBFs\u7684\u5b89\u5168\u8fd0\u52a8\u89c4\u5212\u7b56\u7565\uff0c\u901a\u8fc7\u8c03\u6574\u969c\u788d\u7269\u81a8\u80c0\u534a\u5f84\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u63a7\u5236\u5668\u7684\u4fdd\u5b88\u6027\uff0c\u589e\u5f3a\u4e86\u81ea\u4e3b\u8239\u8236\u5728\u72ed\u7a84\u6c34\u57df\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\u3002", "motivation": "\u81ea\u4e3b\u8239\u8236\u5728\u72ed\u7a84\u5185\u9646\u6c34\u9053\u7b49\u6311\u6218\u6027\u7a7a\u95f4\u4e2d\u7684\u5b89\u5168\u8fd0\u52a8\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u8ba1\u7b97\u590d\u6742\u6216\u8fc7\u4e8e\u4fdd\u5b88\u3002", "method": "\u7ed3\u5408\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u548c\u63a7\u5236\u969c\u788d\u51fd\u6570\uff08CBFs\uff09\uff0c\u4f7f\u7528\u65f6\u53d8\u81a8\u80c0\u692d\u5706\u969c\u788d\u7269\u8868\u793a\uff0c\u5e76\u6839\u636e\u8239\u8236\u4e0e\u969c\u788d\u7269\u7684\u76f8\u5bf9\u4f4d\u7f6e\u548c\u59ff\u6001\u8c03\u6574\u81a8\u80c0\u534a\u5f84\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9645\u6d4b\u8bd5\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7b56\u7565\u4f7f\u5168\u9a71\u52a8\u81ea\u4e3b\u673a\u5668\u4eba\u8239\u8236\u80fd\u591f\u5728\u786e\u4fdd\u5b89\u5168\u7684\u540c\u65f6\uff0c\u5b9e\u65f6\u901a\u8fc7\u72ed\u7a84\u7a7a\u95f4\u5e76\u89e3\u51b3\u6f5c\u5728\u6b7b\u9501\u95ee\u9898\u3002", "conclusion": "\u63d0\u51fa\u7684\u5b89\u5168\u8fd0\u52a8\u89c4\u5212\u7b56\u7565\u6709\u6548\u964d\u4f4e\u4e86\u63a7\u5236\u5668\u7684\u4fdd\u5b88\u6027\uff0c\u4f7f\u81ea\u4e3b\u8239\u8236\u80fd\u591f\u5728\u72ed\u7a84\u6c34\u57df\u4e2d\u5b9e\u65f6\u5b89\u5168\u5bfc\u822a\u3002"}}
{"id": "2510.01193", "categories": ["cs.HC", "cs.CY", "68T50", "I.2; I.7; H.4; K.4; J.4"], "pdf": "https://arxiv.org/pdf/2510.01193", "abs": "https://arxiv.org/abs/2510.01193", "authors": ["Vasileios Maltezos", "Roman Kyrychenko", "Aleksi Knuutila"], "title": "How can AI agents support journalists' work? An experiment with designing an LLM-driven intelligent reporting system", "comment": "8 pages, 1 table", "summary": "The integration of artificial intelligence into journalistic practices\nrepresents a transformative shift in how news is gathered, analyzed, and\ndisseminated. Large language models (LLMs), particularly those with agentic\ncapabilities, offer unprecedented opportunities for enhancing journalistic\nworkflows while simultaneously presenting complex challenges for newsroom\nintegration. This research explores how agentic LLMs can support journalists'\nworkflows, based on insights from journalist interviews and from the\ndevelopment of an LLM-based automation tool performing information filtering,\nsummarization, and reporting. The paper details automated aggregation and\nsummarization systems for journalists, presents a technical overview and\nevaluation of a user-centric LLM-driven reporting system (TeleFlash), and\ndiscusses both addressed and unmet journalist needs, with an outlook on future\ndirections for AI-driven tools in journalism.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u63a2\u8ba8\u4e86\u4ee3\u7406\u578b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u63d0\u5347\u65b0\u95fb\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u4eba\u5de5\u667a\u80fd\u5de5\u5177\u5728\u65b0\u95fb\u884c\u4e1a\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u7406\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63d0\u5347\u65b0\u95fb\u91c7\u96c6\u3001\u5206\u6790\u548c\u4f20\u64ad\u4e2d\u7684\u4f5c\u7528\uff0c\u540c\u65f6\u8bc6\u522b\u5b83\u4eec\u5728\u65b0\u95fb\u7f16\u8f91\u5ba4\u96c6\u6210\u4e2d\u9762\u4e34\u7684\u590d\u6742\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5bf9\u8bb0\u8005\u7684\u8bbf\u8c08\u548c\u5f00\u53d1\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316\u5de5\u5177\uff0c\u6df1\u5165\u63a2\u8ba8\u4e86\u4ee3\u7406\u578b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u652f\u6301\u65b0\u95fb\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "\u6587\u7ae0\u63d0\u4f9b\u4e86\u8bb0\u8005\u4e13\u7528\u7684\u81ea\u52a8\u805a\u5408\u548c\u603b\u7ed3\u7cfb\u7edf\u7684\u6280\u672f\u6982\u8ff0\u548c\u8bc4\u4f30\uff0c\u5e76\u8ba8\u8bba\u4e86\u6ee1\u8db3\u548c\u672a\u6ee1\u8db3\u7684\u8bb0\u8005\u9700\u6c42\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u5c55\u793a\u4e86\u4ee3\u7406\u80fd\u529b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u652f\u6301\u65b0\u95fb\u5de5\u4f5c\u6d41\u7a0b\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u4eba\u5de5\u667a\u80fd\u5de5\u5177\u5728\u65b0\u95fb\u62a5\u9053\u4e2d\u7684\u5e94\u7528\u65b9\u5411\u3002"}}
{"id": "2510.01381", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01381", "abs": "https://arxiv.org/abs/2510.01381", "authors": ["Spencer Teetaert", "Sven Lilge", "Jessica Burgner-Kahrs", "Timothy D. Barfoot"], "title": "A Stochastic Framework for Continuous-Time State Estimation of Continuum Robots", "comment": "17 pages, 11 figures. Submitted to IEEE Transactions on Robotics", "summary": "State estimation techniques for continuum robots (CRs) typically involve\nusing computationally complex dynamic models, simplistic shape approximations,\nor are limited to quasi-static methods. These limitations can be sensitive to\nunmodelled disturbances acting on the robot. Inspired by a factor-graph\noptimization paradigm, this work introduces a continuous-time stochastic state\nestimation framework for continuum robots. We introduce factors based on\ncontinuous-time kinematics that are corrupted by a white noise Gaussian process\n(GP). By using a simple robot model paired with high-rate sensing, we show\nadaptability to unmodelled external forces and data dropout. The result\ncontains an estimate of the mean and covariance for the robot's pose, velocity,\nand strain, each of which can be interpolated continuously in time or space.\nThis same interpolation scheme can be used during estimation, allowing for\ninclusion of measurements on states that are not explicitly estimated. Our\nmethod's inherent sparsity leads to a linear solve complexity with respect to\ntime and interpolation queries in constant time. We demonstrate our method on a\nCR with gyroscope and pose sensors, highlighting its versatility in real-world\nsystems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8fde\u7eed\u65f6\u95f4\u968f\u673a\u72b6\u6001\u4f30\u8ba1\u6846\u67b6\uff0c\u80fd\u591f\u5b9e\u65f6\u9002\u5e94\u672a\u5efa\u6a21\u6270\u52a8\uff0c\u5e76\u5728\u771f\u5b9e\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u7684\u8fde\u7eed\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\u6280\u672f\u9762\u4e34\u590d\u6742\u7684\u52a8\u6001\u6a21\u578b\u548c\u7b80\u5316\u5f62\u72b6\u903c\u8fd1\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u5bf9\u672a\u5efa\u6a21\u6270\u52a8\u7684\u9002\u5e94\u6027\u3002", "method": "\u57fa\u4e8e\u8fde\u7eed\u65f6\u95f4\u52a8\u529b\u5b66\u548c\u9ad8\u9891\u4f20\u611f\u5668\uff0c\u7ed3\u5408\u6f5c\u5728\u7684\u9ad8\u65af\u566a\u58f0\u6a21\u578b\u8fdb\u884c\u72b6\u6001\u4f30\u8ba1\u3002", "result": "\u901a\u8fc7\u7b80\u5355\u7684\u673a\u5668\u4eba\u6a21\u578b\u548c\u9ad8\u9891\u7387\u4f20\u611f\u5668\uff0c\u5b9e\u73b0\u4e86\u5bf9\u673a\u5668\u4eba\u59ff\u6001\u3001\u901f\u5ea6\u548c\u5e94\u53d8\u7684\u5747\u503c\u548c\u534f\u65b9\u5dee\u4f30\u8ba1\uff0c\u5e76\u4e14\u652f\u6301\u65f6\u7a7a\u7684\u8fde\u7eed\u63d2\u503c\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u8fde\u7eed\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\u7684\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u672a\u5efa\u6a21\u7684\u5916\u90e8\u6270\u52a8\u548c\u6570\u636e\u4e22\u5931\u3002"}}
{"id": "2510.01194", "categories": ["cs.HC", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.01194", "abs": "https://arxiv.org/abs/2510.01194", "authors": ["Juan Barrientos", "Michaelle P\u00e9rez", "Douglas Gonz\u00e1lez", "Favio Reyna", "Julio Fajardo", "Andrea Lara"], "title": "Development and Evaluation of an AI-Driven Telemedicine System for Prenatal Healthcare", "comment": "Accepted at MICCAI 2025 MIRASOL Workshop, 10 pages, 5 figures", "summary": "Access to obstetric ultrasound is often limited in low-resource settings,\nparticularly in rural areas of low- and middle-income countries. This work\nproposes a human-in-the-loop artificial intelligence (AI) system designed to\nassist midwives in acquiring diagnostically relevant fetal images using blind\nsweep protocols. The system incorporates a classification model along with a\nweb-based platform for asynchronous specialist reviews. By identifying key\nframes in blind sweep studies, the AI system allows specialists to concentrate\non interpretation rather than having to review entire videos. To evaluate its\nperformance, blind sweep videos captured by a small group of soft-trained\nmidwives using a low-cost Point-of-Care Ultrasound (POCUS) device were\nanalyzed. The system demonstrated promising results in identifying standard\nfetal planes from sweeps made by non-experts. A field evaluation indicated good\nusability and a low cognitive workload, suggesting that it has the potential to\nexpand access to prenatal imaging in underserved regions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cdAI\u7cfb\u7edf\uff0c\u901a\u8fc7\u76f2\u626b\u534f\u8bae\u5e2e\u52a9\u52a9\u4ea7\u58eb\u83b7\u53d6\u80ce\u513f\u56fe\u50cf\uff0c\u53ef\u63d0\u5347\u4f4e\u8d44\u6e90\u5730\u533a\u7684\u4ea7\u524d\u5f71\u50cf\u83b7\u53d6\u80fd\u529b\u3002", "motivation": "\u5728\u4f4e\u6536\u5165\u56fd\u5bb6\u548c\u519c\u6751\u5730\u533a\uff0c\u83b7\u5f97\u4ea7\u79d1\u8d85\u58f0\u7684\u673a\u4f1a\u6709\u9650\uff0c\u6025\u9700\u4e00\u79cd\u6709\u6548\u7684\u5de5\u5177\u5e2e\u52a9\u533b\u7597\u5de5\u4f5c\u8005\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u4eba\u673a\u7ed3\u5408\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\uff0c\u8f85\u52a9\u624b\u8bbf\u8005\u4f7f\u7528\u76f2\u626b\u534f\u8bae\u83b7\u53d6\u80ce\u513f\u5f71\u50cf\uff0c\u5e76\u7ed3\u5408\u5206\u7c7b\u6a21\u578b\u548c\u7f51\u7edc\u5e73\u53f0\u8fdb\u884c\u4e13\u5bb6\u7684\u5f02\u6b65\u5ba1\u6838\u3002", "result": "\u7cfb\u7edf\u5728\u5206\u6790\u7531\u7ecf\u8fc7\u7b80\u5355\u57f9\u8bad\u7684\u52a9\u4ea7\u58eb\u4f7f\u7528\u4f4e\u6210\u672cPOCUS\u8bbe\u5907\u62cd\u6444\u7684\u76f2\u626b\u89c6\u9891\u65f6\uff0c\u8868\u73b0\u51fa\u826f\u597d\uff0c\u80fd\u591f\u6709\u6548\u8bc6\u522b\u6807\u51c6\u80ce\u513f\u5e73\u9762\u3002", "conclusion": "\u8be5AI\u7cfb\u7edf\u5728\u8bc6\u522b\u975e\u4e13\u5bb6\u7684\u80ce\u513f\u5e73\u9762\u65b9\u9762\u8868\u73b0\u51fa\u826f\u597d\u7684\u6548\u679c\uff0c\u80fd\u591f\u6269\u5927\u7f3a\u4e4f\u8d44\u6e90\u5730\u533a\u7684\u4ea7\u524d\u5f71\u50cf\u83b7\u53d6\u3002"}}
{"id": "2510.01388", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01388", "abs": "https://arxiv.org/abs/2510.01388", "authors": ["Arthur Zhang", "Xiangyun Meng", "Luca Calliari", "Dong-Ki Kim", "Shayegan Omidshafiei", "Joydeep Biswas", "Ali Agha", "Amirreza Shaban"], "title": "VENTURA: Adapting Image Diffusion Models for Unified Task Conditioned Navigation", "comment": "9 pages, 6 figures, 3 tables", "summary": "Robots must adapt to diverse human instructions and operate safely in\nunstructured, open-world environments. Recent Vision-Language models (VLMs)\noffer strong priors for grounding language and perception, but remain difficult\nto steer for navigation due to differences in action spaces and pretraining\nobjectives that hamper transferability to robotics tasks. Towards addressing\nthis, we introduce VENTURA, a vision-language navigation system that finetunes\ninternet-pretrained image diffusion models for path planning. Instead of\ndirectly predicting low-level actions, VENTURA generates a path mask (i.e. a\nvisual plan) in image space that captures fine-grained, context-aware\nnavigation behaviors. A lightweight behavior-cloning policy grounds these\nvisual plans into executable trajectories, yielding an interface that follows\nnatural language instructions to generate diverse robot behaviors. To scale\ntraining, we supervise on path masks derived from self-supervised tracking\nmodels paired with VLM-augmented captions, avoiding manual pixel-level\nannotation or highly engineered data collection setups. In extensive real-world\nevaluations, VENTURA outperforms state-of-the-art foundation model baselines on\nobject reaching, obstacle avoidance, and terrain preference tasks, improving\nsuccess rates by 33% and reducing collisions by 54% across both seen and unseen\nscenarios. Notably, we find that VENTURA generalizes to unseen combinations of\ndistinct tasks, revealing emergent compositional capabilities. Videos, code,\nand additional materials: https://venturapath.github.io", "AI": {"tldr": "VENTURA\u662f\u4e00\u79cd\u89c6\u89c9-\u8bed\u8a00\u5bfc\u822a\u7cfb\u7edf\uff0c\u901a\u8fc7\u5fae\u8c03\u56fe\u50cf\u6269\u6563\u6a21\u578b\u751f\u6210\u8def\u5f84\u63a9\u7801\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u673a\u5668\u4eba\u9700\u9002\u5e94\u591a\u6837\u7684\u4eba\u7c7b\u6307\u4ee4\u5e76\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5b89\u5168\u64cd\u4f5c\uff0c\u800c\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u884c\u52a8\u7a7a\u95f4\u53ca\u9884\u8bad\u7ec3\u76ee\u6807\u5dee\u5f02\u9650\u5236\u4e86\u5176\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u53ef\u8f6c\u79fb\u6027\u3002", "method": "\u901a\u8fc7\u5fae\u8c03\u4e92\u8054\u7f51\u9884\u8bad\u7ec3\u7684\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff0c\u4f7f\u7528\u8def\u5f84\u63a9\u7801\u751f\u6210\u89c6\u89c9\u8ba1\u5212\uff0c\u5e76\u7ed3\u5408\u8f7b\u91cf\u7ea7\u884c\u4e3a\u514b\u9686\u7b56\u7565\u5b9e\u73b0\u53ef\u6267\u884c\u8f68\u8ff9\u3002", "result": "VENTURA\u751f\u6210\u7684\u89c6\u89c9\u8ba1\u5212\u6709\u6548\u589e\u5f3a\u673a\u5668\u4eba\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u7684\u6267\u884c\u8868\u73b0\uff0c\u5e76\u5728\u9762\u4e34\u4e0d\u540c\u4efb\u52a1\u7ec4\u5408\u65f6\u5c55\u73b0\u51fa\u7d27\u51d1\u7684\u7ec4\u5408\u80fd\u529b\u3002", "conclusion": "VENTURA\u5728\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u4e2d\u8d85\u8fc7\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u57fa\u7840\u6a21\u578b\uff0c\u5728\u76ee\u6807\u5230\u8fbe\u3001\u907f\u969c\u548c\u5730\u5f62\u504f\u597d\u4efb\u52a1\u4e0a\u6210\u529f\u7387\u63d0\u9ad8\u4e8633%\uff0c\u78b0\u649e\u51cf\u5c11\u4e8654%\u3002"}}
{"id": "2510.01195", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.01195", "abs": "https://arxiv.org/abs/2510.01195", "authors": ["Aadarsh Rajiv", "Klaus Mueller"], "title": "LegiScout: A Visual Tool for Understanding Complex Legislation", "comment": null, "summary": "Modern legislative frameworks, such as the Affordable Care Act (ACA), often\ninvolve complex webs of agencies, mandates, and interdependencies. Government\nissued charts attempt to depict these structures but are typically static,\ndense, and difficult to interpret - even for experts. We introduce LegiScout,\nan interactive visualization system that transforms static policy diagrams into\ndynamic, force-directed graphs, enhancing comprehension while preserving\nessential relationships. By integrating data extraction, natural language\nprocessing, and computer vision techniques, LegiScout supports deeper\nexploration of not only the ACA but also a wide range of legislative and\nregulatory frameworks. Our approach enables stakeholders - policymakers,\nanalysts, and the public - to navigate and understand the complexity inherent\nin modern law.", "AI": {"tldr": "LegiScout \u662f\u4e00\u79cd\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u901a\u8fc7\u52a8\u6001\u56fe\u5f62\u5e2e\u52a9\u7528\u6237\u7406\u89e3\u590d\u6742\u7684\u7acb\u6cd5\u6846\u67b6\uff0c\u5982 ACA.", "motivation": "\u6539\u5584\u5bf9\u590d\u6742\u7acb\u6cd5\u6846\u67b6\uff08\u5982 ACA\uff09\u7684\u7406\u89e3\uff0c\u89e3\u51b3\u73b0\u6709\u9759\u6001\u56fe\u8868\u96be\u4ee5\u89e3\u6790\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6570\u636e\u63d0\u53d6\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\uff0c\u5c06\u9759\u6001\u653f\u7b56\u56fe\u8f6c\u5316\u4e3a\u52a8\u6001\u7684\u529b\u5bfc\u5411\u56fe\u3002", "result": "LegiScout \u4f7f\u653f\u7b56\u5236\u5b9a\u8005\u3001\u5206\u6790\u5e08\u548c\u516c\u4f17\u80fd\u591f\u66f4\u6df1\u5165\u5730\u63a2\u7d22\u6cd5\u89c4\u548c\u653f\u7b56\u7684\u590d\u6742\u6027\u3002", "conclusion": "LegiScout \u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u7cfb\u7edf\uff0c\u80fd\u6709\u6548\u63d0\u5347\u5bf9\u590d\u6742\u7acb\u6cd5\u6846\u67b6\u7684\u7406\u89e3\u3002"}}
{"id": "2510.01389", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01389", "abs": "https://arxiv.org/abs/2510.01389", "authors": ["Ulas Berk Karli", "Ziyao Shangguan", "Tesca FItzgerald"], "title": "INSIGHT: INference-time Sequence Introspection for Generating Help Triggers in Vision-Language-Action Models", "comment": null, "summary": "Recent Vision-Language-Action (VLA) models show strong generalization\ncapabilities, yet they lack introspective mechanisms for anticipating failures\nand requesting help from a human supervisor. We present \\textbf{INSIGHT}, a\nlearning framework for leveraging token-level uncertainty signals to predict\nwhen a VLA should request help. Using $\\pi_0$-FAST as the underlying model, we\nextract per-token \\emph{entropy}, \\emph{log-probability}, and Dirichlet-based\nestimates of \\emph{aleatoric and epistemic uncertainty}, and train compact\ntransformer classifiers to map these sequences to help triggers. We explore\nsupervision regimes for strong or weak supervision, and extensively compare\nthem across in-distribution and out-of-distribution tasks. Our results show a\ntrade-off: strong labels enable models to capture fine-grained uncertainty\ndynamics for reliable help detection, while weak labels, though noisier, still\nsupport competitive introspection when training and evaluation are aligned,\noffering a scalable path when dense annotation is impractical. Crucially, we\nfind that modeling the temporal evolution of token-level uncertainty signals\nwith transformers provides far greater predictive power than static\nsequence-level scores. This study provides the first systematic evaluation of\nuncertainty-based introspection in VLAs, opening future avenues for active\nlearning and for real-time error mitigation through selective human\nintervention.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa INSIGHT \u6846\u67b6\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6539\u8fdb VLA \u6a21\u578b\uff0c\u5b9e\u73b0\u6709\u6548\u7684\u81ea\u6211\u76d1\u63a7\u548c\u9519\u8bef\u7ea0\u6b63\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u6709 VLA \u6a21\u578b\u5177\u6709\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u9884\u5224\u5931\u8d25\u548c\u8bf7\u6c42\u4eba\u7c7b\u76d1\u7763\u7684\u5185\u7701\u673a\u5236\u3002", "method": "\u91c7\u7528 $\text{pi}_0$-FAST \u6a21\u578b\uff0c\u63d0\u53d6\u6bcf\u4e2a token \u7684\u71b5\u3001\u5bf9\u6570\u6982\u7387\u548c Dirichlet \u57fa\u4e8e\u7684 aleatoric \u53ca epistemic \u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u5e76\u8bad\u7ec3\u7d27\u51d1\u7684 transformer \u5206\u7c7b\u5668\u8fdb\u884c\u5e2e\u52a9\u89e6\u53d1\u6620\u5c04\u3002", "result": "\u5f3a\u6807\u7b7e\u8f85\u52a9\u6a21\u578b\u6355\u6349\u7ec6\u7c92\u5ea6\u7684\u4e0d\u786e\u5b9a\u6027\u52a8\u6001\u4ee5\u5b9e\u73b0\u53ef\u9760\u7684\u5e2e\u52a9\u68c0\u6d4b\uff0c\u800c\u5f31\u6807\u7b7e\u867d\u7136\u5b58\u5728\u566a\u58f0\u4f46\u5728\u8bad\u7ec3\u548c\u8bc4\u4f30\u4e00\u81f4\u65f6\u4ecd\u80fd\u652f\u6301\u7ade\u4e89\u6027\u7684\u5185\u7701\u3002", "conclusion": "\u901a\u8fc7\u5bf9 VLA \u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\uff0c\u672c\u6587\u5f00\u542f\u4e86\u4e3b\u52a8\u5b66\u4e60\u548c\u5b9e\u65f6\u9519\u8bef\u7f13\u89e3\u7684\u672a\u6765\u65b9\u5411\u3002"}}
{"id": "2510.01382", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.01382", "abs": "https://arxiv.org/abs/2510.01382", "authors": ["Matthew Varona", "Maryam Hedayati", "Matthew Kay", "Carolina Nobre"], "title": "Theory is Shapes", "comment": "Accepted at alt.vis 2025 workshop. 7 pages, 9 figures", "summary": "\"Theory figures\" are a staple of theoretical visualization research. Common\nshapes such as Cartesian planes and flowcharts can be used not only to explain\nconceptual contributions, but to think through and refine the contribution\nitself. Yet, theory figures tend to be limited to a set of standard shapes,\nlimiting the creative and expressive potential of visualization theory. In this\nwork, we explore how the shapes used in theory figures afford different\nunderstandings and explanations of their underlying phenomena. We speculate on\nthe value of visualizing theories using more expressive configurations, such as\nicebergs, horseshoes, M\\\"obius strips, and BLT sandwiches. By reflecting on\nfigure-making's generative role in the practice of theorizing, we conclude that\ntheory is, in fact, shapes.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u66f4\u5177\u8868\u73b0\u529b\u7684\u5f62\u72b6\u6765\u53ef\u89c6\u5316\u7406\u8bba\u7684\u91cd\u8981\u6027\uff0c\u8ba4\u4e3a\u7406\u8bba\u672c\u8d28\u4e0a\u4e0e\u5f62\u72b6\u606f\u606f\u76f8\u5173\u3002", "motivation": "\u63a2\u8ba8\u73b0\u6709\u7684\u65b9\u6cd5\u5982\u4f55\u9650\u5236\u4e86\u7406\u8bba\u53ef\u89c6\u5316\u7684\u521b\u9020\u6027\u548c\u8868\u73b0\u529b\u3002", "method": "\u63a2\u7d22\u4f7f\u7528\u66f4\u5177\u8868\u73b0\u529b\u7684\u5f62\u72b6\u6765\u53ef\u89c6\u5316\u7406\u8bba\u3002", "result": "\u901a\u8fc7\u53cd\u601d\u56fe\u5f62\u5236\u4f5c\u5728\u7406\u8bba\u5b9e\u8df5\u4e2d\u7684\u751f\u6210\u4f5c\u7528\uff0c\u63d0\u51fa\u66f4\u591a\u6837\u5316\u7684\u56fe\u5f62\u53ef\u4ee5\u5e26\u6765\u4e0d\u540c\u7684\u7406\u89e3\u548c\u89e3\u91ca\u3002", "conclusion": "\u7406\u8bba\u5b9e\u9645\u4e0a\u5c31\u662f\u5f62\u72b6\u3002"}}
{"id": "2510.01402", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.01402", "abs": "https://arxiv.org/abs/2510.01402", "authors": ["Hun Kuk Park", "Taekyung Kim", "Dimitra Panagou"], "title": "Beyond Collision Cones: Dynamic Obstacle Avoidance for Nonholonomic Robots via Dynamic Parabolic Control Barrier Functions", "comment": "The first two authors contributed equally to this work. Project page:\n  https://www.taekyung.me/dpcbf", "summary": "Control Barrier Functions (CBFs) are a powerful tool for ensuring the safety\nof autonomous systems, yet applying them to nonholonomic robots in cluttered,\ndynamic environments remains an open challenge. State-of-the-art methods often\nrely on collision-cone or velocity-obstacle constraints which, by only\nconsidering the angle of the relative velocity, are inherently conservative and\ncan render the CBF-based quadratic program infeasible, particularly in dense\nscenarios. To address this issue, we propose a Dynamic Parabolic Control\nBarrier Function (DPCBF) that defines the safe set using a parabolic boundary.\nThe parabola's vertex and curvature dynamically adapt based on both the\ndistance to an obstacle and the magnitude of the relative velocity, creating a\nless restrictive safety constraint. We prove that the proposed DPCBF is valid\nfor a kinematic bicycle model subject to input constraints. Extensive\ncomparative simulations demonstrate that our DPCBF-based controller\nsignificantly enhances navigation success rates and QP feasibility compared to\nbaseline methods. Our approach successfully navigates through dense\nenvironments with up to 100 dynamic obstacles, scenarios where collision\ncone-based methods fail due to infeasibility.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u52a8\u6001\u629b\u7269\u7ebf\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08DPCBF\uff09\uff0c\u6709\u6548\u5e94\u5bf9\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6311\u6218\uff0c\u63d0\u5347\u5b89\u5168\u6027\u4e0e\u53ef\u884c\u6027\u3002", "motivation": "\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u5e94\u7528\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08CBFs\uff09\u786e\u4fdd\u81ea\u4e3b\u7cfb\u7edf\u7684\u5b89\u5168\u6027\uff0c\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u4fdd\u5b88\u6027\u3002", "method": "\u52a8\u6001\u629b\u7269\u7ebf\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08DPCBF\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u629b\u7269\u7ebf\u7684\u9876\u70b9\u548c\u66f2\u7387\u6765\u5b9a\u4e49\u5b89\u5168\u96c6\u3002", "result": "DPCBF\u5728\u542b\u6709\u591a\u8fbe100\u4e2a\u52a8\u6001\u969c\u788d\u7269\u7684\u573a\u666f\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bfc\u822a\u6210\u529f\u7387\u548cQP\u53ef\u884c\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u52a8\u6001\u629b\u7269\u7ebf\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u663e\u8457\u63d0\u9ad8\u4e86\u81ea\u4e3b\u7cfb\u7edf\u5728\u62e5\u6324\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6210\u529f\u7387\u548c\u53ef\u884c\u6027\u3002"}}
{"id": "2510.01453", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01453", "abs": "https://arxiv.org/abs/2510.01453", "authors": ["Saketh Ram Kasibatla", "Kiran Medleri Hiremath", "Raven Rothkopf", "Sorin Lerner", "Haijun Xia", "Brian Hempel"], "title": "The Command Line GUIde: Graphical Interfaces from Man Pages via AI", "comment": "5 pages, 4 figures, In Proceedings of the IEEE Symposium on Visual\n  Languages and Human Centric Computing (VL/HCC), October 2025", "summary": "Although birthed in the era of teletypes, the command line shell survived the\ngraphical interface revolution of the 1980's and lives on in modern desktop\noperating systems. The command line provides access to powerful functionality\nnot otherwise exposed on the computer, but requires users to recall textual\nsyntax and carefully scour documentation. In contrast, graphical interfaces let\nusers organically discover and invoke possible actions through widgets and\nmenus. To better expose the power of the command line, we demonstrate a\nmechanism for automatically creating graphical interfaces for command line\ntools by translating their documentation (in the form of man pages) into\ninterface specifications via AI. Using these specifications, our user-facing\nsystem, called GUIde, presents the command options to the user graphically. We\nevaluate the generated interfaces on a corpus of commands to show to what\ndegree GUIde offers thorough graphical interfaces for users' real-world command\nline tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7AI\u5c06\u547d\u4ee4\u884c\u5de5\u5177\u7684\u6587\u6863\u8f6c\u5316\u4e3a\u56fe\u5f62\u754c\u9762\u89c4\u8303\uff0c\u63a8\u51faGUIde\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u547d\u4ee4\u884c\u7684\u53ef\u7528\u6027\u548c\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u547d\u4ee4\u884c\u867d\u7136\u529f\u80fd\u5f3a\u5927\uff0c\u4f46\u5bf9\u7528\u6237\u800c\u8a00\u96be\u4ee5\u8bb0\u5fc6\u548c\u4f7f\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4f18\u5316\u5176\u53ef\u7528\u6027\uff0c\u4ee5\u9002\u5e94\u73b0\u4ee3\u7528\u6237\u7684\u9700\u6c42\u3002", "method": "\u5229\u7528AI\u5c06\u547d\u4ee4\u884c\u5de5\u5177\u7684\u6587\u6863\uff08\u5982\u624b\u518c\u9875\uff09\u81ea\u52a8\u8f6c\u6362\u4e3a\u56fe\u5f62\u754c\u9762\u89c4\u8303\u3002", "result": "GUIde\u80fd\u591f\u4e3a\u7528\u6237\u7684\u771f\u5b9e\u4e16\u754c\u547d\u4ee4\u884c\u4efb\u52a1\u63d0\u4f9b\u5168\u9762\u7684\u56fe\u5f62\u754c\u9762\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7528\u6237\u4f53\u9a8c\u3002", "conclusion": "GUIde\u901a\u8fc7\u5c06\u547d\u4ee4\u884c\u5de5\u5177\u7684\u6587\u6863\u8f6c\u6362\u4e3a\u754c\u9762\u89c4\u8303\uff0c\u6709\u6548\u4e3a\u7528\u6237\u63d0\u4f9b\u4e86\u56fe\u5f62\u5316\u7684\u547d\u4ee4\u9009\u9879\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u547d\u4ee4\u884c\u7684\u53ef\u7528\u6027\u3002"}}
{"id": "2510.01404", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01404", "abs": "https://arxiv.org/abs/2510.01404", "authors": ["Lexi Foland", "Thomas Cohn", "Adam Wei", "Nicholas Pfaff", "Boyuan Chen", "Russ Tedrake"], "title": "How Well do Diffusion Policies Learn Kinematic Constraint Manifolds?", "comment": "Under review. 8 pages, 3 figures, 3 tables. Additional results\n  available at https://diffusion-learns-kinematic.github.io", "summary": "Diffusion policies have shown impressive results in robot imitation learning,\neven for tasks that require satisfaction of kinematic equality constraints.\nHowever, task performance alone is not a reliable indicator of the policy's\nability to precisely learn constraints in the training data. To investigate, we\nanalyze how well diffusion policies discover these manifolds with a case study\non a bimanual pick-and-place task that encourages fulfillment of a kinematic\nconstraint for success. We study how three factors affect trained policies:\ndataset size, dataset quality, and manifold curvature. Our experiments show\ndiffusion policies learn a coarse approximation of the constraint manifold with\nlearning affected negatively by decreases in both dataset size and quality. On\nthe other hand, the curvature of the constraint manifold showed inconclusive\ncorrelations with both constraint satisfaction and task success. A hardware\nevaluation verifies the applicability of our results in the real world. Project\nwebsite with additional results and visuals:\nhttps://diffusion-learns-kinematic.github.io", "AI": {"tldr": "\u7814\u7a76\u6269\u6563\u7b56\u7565\u5728\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5bf9\u7ea6\u675f\u6d41\u5f62\u7684\u5b66\u4e60\u53d7\u5230\u6570\u636e\u96c6\u5927\u5c0f\u548c\u8d28\u91cf\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u800c\u6d41\u5f62\u66f2\u7387\u7684\u5f71\u54cd\u5219\u4e0d\u660e\u786e\u3002", "motivation": "\u63a2\u7a76\u6269\u6563\u7b56\u7565\u5982\u4f55\u7cbe\u786e\u5730\u5b66\u4e60\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u7ea6\u675f\uff0c\u5e76\u8bc4\u4f30\u5176\u5728\u6ee1\u8db3\u8fd0\u52a8\u5b66\u7ea6\u675f\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5bf9\u53cc\u624b\u6293\u53d6\u4efb\u52a1\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u5206\u6790\u6269\u6563\u7b56\u7565\u5728\u5b66\u4e60\u7ea6\u675f\u6d41\u5f62\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6269\u6563\u7b56\u7565\u5b66\u4e60\u5230\u4e86\u7ea6\u675f\u6d41\u5f62\u7684\u7c97\u7565\u8fd1\u4f3c\uff0c\u6570\u636e\u96c6\u7684\u5927\u5c0f\u548c\u8d28\u91cf\u4e0b\u964d\u4f1a\u5f71\u54cd\u5b66\u4e60\u6548\u679c\uff0c\u6d41\u5f62\u66f2\u7387\u4e0e\u7ea6\u675f\u6ee1\u8db3\u5ea6\u548c\u4efb\u52a1\u6210\u529f\u4e4b\u95f4\u7684\u5173\u7cfb\u4e0d\u660e\u786e\u3002", "conclusion": "\u6269\u6563\u7b56\u7565\u5728\u903c\u8fd1\u7ea6\u675f\u6d41\u5f62\u65b9\u9762\u8868\u73b0\u8f83\u4e3a\u7c97\u7565\uff0c\u4e14\u6570\u636e\u96c6\u7684\u5927\u5c0f\u548c\u8d28\u91cf\u5bf9\u5b66\u4e60\u6548\u679c\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\uff0c\u800c\u6d41\u5f62\u7684\u66f2\u7387\u4e0e\u7ea6\u675f\u6ee1\u8db3\u5ea6\u548c\u4efb\u52a1\u6210\u529f\u4e4b\u95f4\u7684\u5173\u8054\u5c1a\u4e0d\u660e\u786e\u3002"}}
{"id": "2510.01473", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01473", "abs": "https://arxiv.org/abs/2510.01473", "authors": ["Maura E Halstead", "Mark A. Green", "Caroline Jay", "Richard Kingston", "David Topping", "Alexander Singleton"], "title": "From keywords to semantics: Perceptions of large language models in data discovery", "comment": null, "summary": "Current approaches to data discovery match keywords between metadata and\nqueries. This matching requires researchers to know the exact wording that\nother researchers previously used, creating a challenging process that could\nlead to missing relevant data. Large Language Models (LLMs) could enhance data\ndiscovery by removing this requirement and allowing researchers to ask\nquestions with natural language. However, we do not currently know if\nresearchers would accept LLMs for data discovery. Using a human-centered\nartificial intelligence (HCAI) focus, we ran focus groups (N = 27) to\nunderstand researchers' perspectives towards LLMs for data discovery. Our\nconceptual model shows that the potential benefits are not enough for\nresearchers to use LLMs instead of current technology. Barriers prevent\nresearchers from fully accepting LLMs, but features around transparency could\novercome them. Using our model will allow developers to incorporate features\nthat result in an increased acceptance of LLMs for data discovery.", "AI": {"tldr": "\u867d\u7136LLMs\u5728\u6570\u636e\u53d1\u73b0\u4e2d\u6f5c\u5728\u6709\u76ca\uff0c\u4f46\u7814\u7a76\u4eba\u5458\u5bf9\u5176\u63a5\u53d7\u5ea6\u53d7\u963b\uff0c\u900f\u660e\u5ea6\u7279\u5f81\u53ef\u80fd\u6709\u52a9\u4e8e\u63d0\u5347\u63a5\u53d7\u5ea6\u3002", "motivation": "\u5f53\u524d\u7684\u6570\u636e\u53d1\u73b0\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5173\u952e\u8bcd\u5339\u914d\uff0c\u8fd9\u8981\u6c42\u7814\u7a76\u4eba\u5458\u4e86\u89e3\u5148\u524d\u4f7f\u7528\u7684\u786e\u5207\u63aa\u8f9e\uff0c\u56e0\u6b64\u5f88\u96be\u627e\u5230\u76f8\u5173\u6570\u636e\u3002", "method": "\u901a\u8fc7\u5bf927\u540d\u7814\u7a76\u4eba\u5458\u7684\u7126\u70b9\u5c0f\u7ec4\u8fdb\u884c\u7814\u7a76\uff0c\u63a2\u8ba8\u5176\u5bf9LLMs\u5728\u6570\u636e\u53d1\u73b0\u4e2d\u7684\u770b\u6cd5\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u6f5c\u5728\u7684\u597d\u5904\u4e0d\u8db3\u4ee5\u4fc3\u4f7f\u7814\u7a76\u4eba\u5458\u653e\u5f03\u73b0\u6709\u6280\u672f\uff0c\u5f00\u53d1\u8005\u53ef\u4ee5\u5229\u7528\u672c\u6a21\u578b\u8bbe\u8ba1\u51fa\u80fd\u591f\u63d0\u9ad8\u7814\u7a76\u4eba\u5458\u5bf9LLMs\u63a5\u53d7\u5ea6\u7684\u7279\u5f81\u3002", "conclusion": "\u7814\u7a76\u4eba\u5458\u5bf9LLMs\u5728\u6570\u636e\u53d1\u73b0\u4e2d\u7684\u63a5\u53d7\u5ea6\u53d7\u5230\u969c\u788d\u5236\u7ea6\uff0c\u4f46\u900f\u660e\u5ea6\u7279\u5f81\u53ef\u80fd\u6709\u52a9\u4e8e\u514b\u670d\u8fd9\u4e9b\u969c\u788d\u3002"}}
{"id": "2510.01433", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01433", "abs": "https://arxiv.org/abs/2510.01433", "authors": ["Anukriti Singh", "Kasra Torshizi", "Khuzema Habib", "Kelin Yu", "Ruohan Gao", "Pratap Tokekar"], "title": "AFFORD2ACT: Affordance-Guided Automatic Keypoint Selection for Generalizable and Lightweight Robotic Manipulation", "comment": null, "summary": "Vision-based robot learning often relies on dense image or point-cloud\ninputs, which are computationally heavy and entangle irrelevant background\nfeatures. Existing keypoint-based approaches can focus on manipulation-centric\nfeatures and be lightweight, but either depend on manual heuristics or\ntask-coupled selection, limiting scalability and semantic understanding. To\naddress this, we propose AFFORD2ACT, an affordance-guided framework that\ndistills a minimal set of semantic 2D keypoints from a text prompt and a single\nimage. AFFORD2ACT follows a three-stage pipeline: affordance filtering,\ncategory-level keypoint construction, and transformer-based policy learning\nwith embedded gating to reason about the most relevant keypoints, yielding a\ncompact 38-dimensional state policy that can be trained in 15 minutes, which\nperforms well in real-time without proprioception or dense representations.\nAcross diverse real-world manipulation tasks, AFFORD2ACT consistently improves\ndata efficiency, achieving an 82% success rate on unseen objects, novel\ncategories, backgrounds, and distractors.", "AI": {"tldr": "AFFORD2ACT\u662f\u4e00\u4e2a\u6709\u6548\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u6587\u672c\u63d0\u793a\u548c\u5355\u4e00\u56fe\u50cf\u4e2d\u63d0\u53d6\u5173\u952e\u70b9\uff0c\u63d0\u5347\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u64cd\u4f5c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5173\u952e\u70b9\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u624b\u52a8\u542f\u53d1\u5f0f\u6216\u4efb\u52a1\u8026\u5408\u9009\u62e9\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u8bed\u4e49\u7406\u89e3\uff0c\u56e0\u6b64\u63d0\u51faAFFORD2ACT\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "AFFORD2ACT\u901a\u8fc7\u4e09\u4e2a\u9636\u6bb5\u7684\u7ba1\u9053\u5b9e\u73b0\uff1a\u80fd\u529b\u8fc7\u6ee4\u3001\u7c7b\u522b\u7ea7\u5173\u952e\u70b9\u6784\u5efa\u548c\u57fa\u4e8e\u53d8\u6362\u5668\u7684\u7b56\u7565\u5b66\u4e60\uff0c\u5d4c\u5165\u95e8\u63a7\u4ee5\u63a8\u7406\u6700\u76f8\u5173\u7684\u5173\u952e\u70b9\u3002", "result": "AFFORD2ACT\u751f\u6210\u4e00\u7ec4\u6700\u5c0f\u7684\u8bed\u4e492D\u5173\u952e\u70b9\uff0c\u53ef\u4ee5\u572815\u5206\u949f\u5185\u8bad\u7ec3\u51fa\u4e00\u4e2a\u7d27\u51d1\u768438\u7ef4\u72b6\u6001\u7b56\u7565\uff0c\u5e76\u5728\u4e0d\u4f9d\u8d56\u672c\u4f53\u611f\u77e5\u6216\u5bc6\u96c6\u8868\u793a\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5feb\u901f\u5b9e\u65f6\u64cd\u4f5c\u3002", "conclusion": "AFFORD2ACT\u80fd\u5728\u591a\u79cd\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u63d0\u9ad8\u6570\u636e\u6548\u7387\uff0c\u5e76\u5728\u672a\u89c1\u7269\u4f53\u3001\u964c\u751f\u7c7b\u522b\u3001\u80cc\u666f\u548c\u5e72\u6270\u7269\u4e0a\u53d6\u5f97\u826f\u597d\u7684\u8868\u73b0\u3002"}}
{"id": "2510.01537", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.01537", "abs": "https://arxiv.org/abs/2510.01537", "authors": ["Anku Rani", "Valdemar Danry", "Paul Pu Liang", "Andrew B. Lippman", "Pattie Maes"], "title": "Dialogues with AI Reduce Beliefs in Misinformation but Build No Lasting Discernment Skills", "comment": null, "summary": "Given the growing prevalence of fake information, including increasingly\nrealistic AI-generated news, there is an urgent need to train people to better\nevaluate and detect misinformation. While interactions with AI have been shown\nto durably reduce people's beliefs in false information, it is unclear whether\nthese interactions also teach people the skills to discern false information\nthemselves. We conducted a month-long study where 67 participants classified\nnews headline-image pairs as real or fake, discussed their assessments with an\nAI system, followed by an unassisted evaluation of unseen news items to measure\naccuracy before, during, and after AI assistance. While AI assistance produced\nimmediate improvements during AI-assisted sessions (+21\\% average),\nparticipants' unassisted performance on new items declined significantly by\nweek 4 (-15.3\\%). These results indicate that while AI may help immediately, it\nultimately degrades long-term misinformation detection abilities.", "AI": {"tldr": "\u672c\u7814\u7a76\u8868\u660e\uff0cAI\u53ef\u80fd\u5728\u77ed\u671f\u5185\u63d0\u9ad8\u5047\u4fe1\u606f\u8bc6\u522b\u80fd\u529b\uff0c\u4f46\u957f\u65f6\u95f4\u4f7f\u7528\u540e\u4f1a\u524a\u5f31\u8fd9\u4e00\u80fd\u529b\u3002", "motivation": "\u9274\u4e8e\u5047\u4fe1\u606f\uff08\u5305\u62ec\u65e5\u76ca\u903c\u771f\u7684AI\u751f\u6210\u65b0\u95fb\uff09\u7684\u65e5\u76ca\u666e\u904d\u6027\uff0c\u8feb\u5207\u9700\u8981\u8bad\u7ec3\u4eba\u4eec\u66f4\u597d\u5730\u8bc4\u4f30\u548c\u68c0\u6d4b\u8fd9\u4e9b\u4fe1\u606f\u3002", "method": "\u8fdb\u884c\u4e3a\u671f\u4e00\u4e2a\u6708\u7684\u7814\u7a76\uff0c67\u540d\u53c2\u4e0e\u8005\u5bf9\u65b0\u95fb\u6807\u9898-\u56fe\u7247\u7ec4\u5408\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u4e0eAI\u7cfb\u7edf\u8ba8\u8bba\u4ed6\u4eec\u7684\u8bc4\u4f30\uff0c\u968f\u540e\u8fdb\u884c\u4e0d\u53d7\u5e2e\u52a9\u7684\u8bc4\u4f30\u4ee5\u6d4b\u91cf\u51c6\u786e\u6027\u3002", "result": "AI\u7684\u5e2e\u52a9\u5728\u7acb\u5373\u7684\u4f1a\u8bdd\u4e2d\u63d0\u9ad8\u4e86\u53c2\u4e0e\u8005\u7684\u8868\u73b0\uff0c\u4f46\u5728\u7b2c4\u5468\u65f6\uff0c\u4ed6\u4eec\u5bf9\u65b0\u9879\u76ee\u7684\u72ec\u7acb\u8868\u73b0\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u5c3d\u7ba1AI\u53ef\u4ee5\u5728\u77ed\u671f\u5185\u5e2e\u52a9\u63d0\u9ad8\u5bf9\u5047\u4fe1\u606f\u7684\u8bc6\u522b\uff0c\u4f46\u4ece\u957f\u8fdc\u6765\u770b\uff0c\u5b83\u4f1a\u524a\u5f31\u4eba\u4eec\u7684\u5224\u65ad\u80fd\u529b\u3002"}}
{"id": "2510.01438", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01438", "abs": "https://arxiv.org/abs/2510.01438", "authors": ["Minglun Wei", "Xintong Yang", "Yu-Kun Lai", "Ze Ji"], "title": "Differentiable Skill Optimisation for Powder Manipulation in Laboratory Automation", "comment": "4 pages", "summary": "Robotic automation is accelerating scientific discovery by reducing manual\neffort in laboratory workflows. However, precise manipulation of powders\nremains challenging, particularly in tasks such as transport that demand\naccuracy and stability. We propose a trajectory optimisation framework for\npowder transport in laboratory settings, which integrates differentiable\nphysics simulation for accurate modelling of granular dynamics, low-dimensional\nskill-space parameterisation to reduce optimisation complexity, and a\ncurriculum-based strategy that progressively refines task competence over long\nhorizons. This formulation enables end-to-end optimisation of contact-rich\nrobot trajectories while maintaining stability and convergence efficiency.\nExperimental results demonstrate that the proposed method achieves superior\ntask success rates and stability compared to the reinforcement learning\nbaseline.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5fae\u7269\u7406\u6a21\u62df\u548c\u57fa\u4e8e\u8bfe\u7a0b\u7684\u7b56\u7565\uff0c\u63d0\u9ad8\u7c89\u672b\u8fd0\u8f93\u7684\u7cbe\u786e\u64cd\u63a7\uff0c\u4ece\u800c\u52a0\u901f\u79d1\u5b66\u7814\u7a76\u3002", "motivation": "\u89e3\u51b3\u5b9e\u9a8c\u5ba4\u73af\u5883\u4e2d\u7c89\u672b\u8fd0\u8f93\u4efb\u52a1\u7684\u7cbe\u786e\u64cd\u63a7\u6311\u6218\uff0c\u63d0\u5347\u79d1\u5b66\u53d1\u73b0\u7684\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u53ef\u5fae\u7269\u7406\u6a21\u62df\u3001\u4f4e\u7ef4\u6280\u80fd\u7a7a\u95f4\u53c2\u6570\u5316\u548c\u57fa\u4e8e\u8bfe\u7a0b\u7684\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4efb\u52a1\u6210\u529f\u7387\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4efb\u52a1\u6210\u529f\u7387\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u4f18\u4e8e\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2510.01561", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.01561", "abs": "https://arxiv.org/abs/2510.01561", "authors": ["Yaozheng Xia", "Zaiping Zhu", "Bo Pang", "Shaorong Wang", "Sheng Li"], "title": "TimeGazer: Temporal Modeling of Predictive Gaze Stabilization for AR Interaction", "comment": null, "summary": "Gaze stabilization is critical for enabling fluid, accurate, and efficient\ninteraction in immersive augmented reality (AR) environments, particularly\nduring task-oriented visual behaviors. However, fixation sequences captured in\nactive gaze tasks often exhibit irregular dispersion and systematic deviations\nfrom target locations, a variability primarily caused by the combined effects\nof human oculomotor physiology, insufficient AR headset tracking and\ncalibration accuracy, and environmental disturbances, undermining interaction\nperformance and visual engagement. To address this issue, we propose TimeGazer,\nwhich reformulates gaze stabilization as a sequence-to-sequence temporal\nregression problem, predicting idealized fixation trajectories for the\ntarget-fixation phase from historical gaze dynamics in the search phase. We\npresent a synthetic data generation and blending strategy that produces\nspatially concentrated, target-centered fixation references aligned with task\nobjectives, substantially enriching the training space and enhancing model\ngeneralization. We train and evaluate TimeGazer on a hybrid dataset of real and\naugmented gaze sequences collected via Microsoft HoloLens 2 from 54\nparticipants across multiple prediction horizons. Through the user study,\nstatistical results demonstrate that TimeGazer significantly improves\ninteraction accuracy and reduces completion time, confirming that temporal\nmodeling of predictive gaze stabilization can strengthen attentional\nconsistency and responsiveness in task-driven AR interaction. These findings\nhighlight the broader potential of TimeGazer for advancing adaptive gaze-based\ninterfaces and temporal modeling research in immersive systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TimeGazer\uff0c\u4e00\u79cd\u901a\u8fc7\u65f6\u95f4\u5efa\u6a21\u7684\u9884\u6d4b\u51dd\u89c6\u7a33\u5b9a\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u589e\u5f3a\u73b0\u5b9e\u4e2d\u7684\u4ea4\u4e92\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5728\u6c89\u6d78\u5f0f\u589e\u5f3a\u73b0\u5b9e\u73af\u5883\u4e2d\uff0c\u5c24\u5176\u662f\u5728\u4efb\u52a1\u5bfc\u5411\u7684\u89c6\u89c9\u884c\u4e3a\u4e2d\uff0c\u51dd\u89c6\u987a\u5e8f\u8868\u73b0\u51fa\u4e0d\u89c4\u5219\u5206\u6563\u548c\u7cfb\u7edf\u6027\u504f\u5dee\u7684\u95ee\u9898\u3002", "method": "\u5c06\u51dd\u89c6\u7a33\u5b9a\u6027\u91cd\u65b0\u6784\u5efa\u4e3a\u5e8f\u5217\u5230\u5e8f\u5217\u7684\u65f6\u95f4\u56de\u5f52\u95ee\u9898\uff0c\u901a\u8fc7\u5386\u53f2\u51dd\u89c6\u52a8\u6001\u9884\u6d4b\u76ee\u6807\u51dd\u89c6\u9636\u6bb5\u7684\u7406\u60f3\u5316\u51dd\u89c6\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cTimeGazer\u5728\u63d0\u9ad8\u4ea4\u4e92\u51c6\u786e\u6027\u548c\u964d\u4f4e\u5b8c\u6210\u65f6\u95f4\u65b9\u9762\u5177\u6709\u663e\u8457\u6548\u679c\u3002", "conclusion": "TimeGazer\u663e\u8457\u63d0\u9ad8\u4e86\u4ea4\u4e92\u51c6\u786e\u6027\u5e76\u51cf\u5c11\u4e86\u5b8c\u6210\u65f6\u95f4\uff0c\u5c55\u793a\u4e86\u5176\u5728\u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u4ea4\u4e92\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2510.01452", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.01452", "abs": "https://arxiv.org/abs/2510.01452", "authors": ["Laura Connolly", "Tamas Ungi", "Adnan Munawar", "Anton Deguet", "Chris Yeung", "Russell H. Taylor", "Parvin Mousavi", "Gabor Fichtinger Keyvan Hashtrudi-Zaad"], "title": "Touching the tumor boundary: A pilot study on ultrasound based virtual fixtures for breast-conserving surgery", "comment": null, "summary": "Purpose: Delineating tumor boundaries during breast-conserving surgery is\nchallenging as tumors are often highly mobile, non-palpable, and have\nirregularly shaped borders. To address these challenges, we introduce a\ncooperative robotic guidance system that applies haptic feedback for tumor\nlocalization. In this pilot study, we aim to assess if and how this system can\nbe successfully integrated into breast cancer care.\n  Methods: A small haptic robot is retrofitted with an electrocautery blade to\noperate as a cooperatively controlled surgical tool. Ultrasound and\nelectromagnetic navigation are used to identify the tumor boundaries and\nposition. A forbidden region virtual fixture is imposed when the surgical tool\ncollides with the tumor boundary. We conducted a study where users were asked\nto resect tumors from breast simulants both with and without the haptic\nguidance. We then assess the results of these simulated resections both\nqualitatively and quantitatively.\n  Results: Virtual fixture guidance is shown to improve resection margins. On\naverage, users find the task to be less mentally demanding, frustrating, and\neffort intensive when haptic feedback is available. We also discovered some\nunanticipated impacts on surgical workflow that will guide design adjustments\nand training protocol moving forward.\n  Conclusion: Our results suggest that virtual fixtures can help localize tumor\nboundaries in simulated breast-conserving surgery. Future work will include an\nextensive user study to further validate these results and fine-tune our\nguidance system.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u534f\u4f5c\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7528\u4e8e\u6539\u5584\u4e73\u623f\u4fdd\u5b88\u624b\u672f\u4e2d\u80bf\u7624\u8fb9\u754c\u7684\u5b9a\u4f4d\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u7cfb\u7edf\u6709\u6548\u63d0\u5347\u4e86\u5207\u9664\u6548\u679c\u3002", "motivation": "\u5728\u4e73\u623f\u4fdd\u5b88\u624b\u672f\u4e2d\uff0c\u80bf\u7624\u8fb9\u754c\u7684\u5212\u5b9a\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u6b64\u63d0\u51fa\u4e00\u79cd\u534f\u4f5c\u673a\u5668\u4eba\u5f15\u5bfc\u7cfb\u7edf\uff0c\u5e94\u7528\u89e6\u89c9\u53cd\u9988\u6765\u8fdb\u884c\u80bf\u7624\u5b9a\u4f4d\u3002", "method": "\u4f7f\u7528\u5e26\u6709\u7535\u5207\u5272\u5200\u7684\u5c0f\u578b\u89e6\u89c9\u673a\u5668\u4eba\uff0c\u5e76\u7ed3\u5408\u8d85\u58f0\u6ce2\u548c\u7535\u78c1\u5bfc\u822a\u8bc6\u522b\u80bf\u7624\u8fb9\u754c\u3002", "result": "\u865a\u62df\u754c\u9762\u5f15\u5bfc\u88ab\u8bc1\u660e\u80fd\u6539\u5584\u5207\u9664\u8fb9\u7f18\uff0c\u7528\u6237\u5728\u6709\u89e6\u89c9\u53cd\u9988\u65f6\u4efb\u52a1\u7684\u5fc3\u7406\u8d1f\u62c5\u3001\u632b\u8d25\u611f\u548c\u52aa\u529b\u7a0b\u5ea6\u5747\u8f83\u4f4e\u3002", "conclusion": "\u865a\u62df\u754c\u9762\u53ef\u4ee5\u5e2e\u52a9\u5728\u6a21\u62df\u7684\u4e73\u623f\u4fdd\u5b88\u624b\u672f\u4e2d\u5b9a\u4f4d\u80bf\u7624\u8fb9\u754c\u3002"}}
{"id": "2510.01638", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01638", "abs": "https://arxiv.org/abs/2510.01638", "authors": ["Siying Hu", "Yaxing Yao", "Zhicong Lu"], "title": "Towards Human-Centered RegTech: Unpacking Professionals' Strategies and Needs for Using LLMs Safely", "comment": "Accepted to the 4th HCI+NLP@EMNLP 2025 Workshop. (Non-archival)", "summary": "Large Language Models are profoundly changing work patterns in high-risk\nprofessional domains, yet their application also introduces severe and\nunderexplored compliance risks. To investigate this issue, we conducted\nsemi-structured interviews with 24 highly-skilled knowledge workers from\nindustries such as law, healthcare, and finance. The study found that these\nexperts are commonly concerned about sensitive information leakage,\nintellectual property infringement, and uncertainty regarding the quality of\nmodel outputs. In response, they spontaneously adopt various mitigation\nstrategies, such as actively distorting input data and limiting the details in\ntheir prompts. However, the effectiveness of these spontaneous efforts is\nlimited due to a lack of specific compliance guidance and training for Large\nLanguage Models. Our research reveals a significant gap between current NLP\ntools and the actual compliance needs of experts. This paper positions these\nvaluable empirical findings as foundational work for building the next\ngeneration of Human-Centered, Compliance-Driven Natural Language Processing for\nRegulatory Technology (RegTech), providing a critical human-centered\nperspective and design requirements for engineering NLP systems that can\nproactively support expert compliance workflows.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u884c\u4e1a\u4e2d\u7684\u5408\u89c4\u98ce\u9669\uff0c\u53d1\u73b0\u4e13\u5bb6\u4eec\u62c5\u5fe7\u654f\u611f\u4fe1\u606f\u548c\u6a21\u578b\u8f93\u51fa\u8d28\u91cf\uff0c\u5e76\u63d0\u51fa\u5e94\u5bf9\u7b56\u7565\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u6307\u5bfc\uff0c\u5f3a\u8c03\u4e86\u5efa\u8bbe\u5408\u89c4\u9a71\u52a8\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7cfb\u7edf\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4e86\u89e3\u5176\u5bf9\u4e13\u4e1a\u9886\u57df\u7684\u5408\u89c4\u98ce\u9669\u5f71\u54cd\u662f\u8feb\u5207\u9700\u8981\u7684\u3002", "method": "\u7814\u7a76\u91c7\u7528\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u6cd5\uff0c\u5bf924\u540d\u6765\u81ea\u6cd5\u5f8b\u3001\u533b\u7597\u548c\u91d1\u878d\u7b49\u884c\u4e1a\u7684\u9ad8\u6280\u80fd\u77e5\u8bc6\u5de5\u4f5c\u8005\u8fdb\u884c\u4e86\u8c03\u67e5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u53d7\u8bbf\u4e13\u5bb6\u666e\u904d\u62c5\u5fe7\u654f\u611f\u4fe1\u606f\u6cc4\u9732\u3001\u77e5\u8bc6\u4ea7\u6743\u4fb5\u6743\u4ee5\u53ca\u5bf9\u6a21\u578b\u8f93\u51fa\u8d28\u91cf\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u91c7\u53d6\u4e86\u4e0d\u540c\u7684\u5e94\u5bf9\u7b56\u7565\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u5177\u4f53\u7684\u5408\u89c4\u6307\u5bfc\u548c\u57f9\u8bad\uff0c\u8fd9\u4e9b\u52aa\u529b\u7684\u6709\u6548\u6027\u6709\u9650\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u4e13\u4e1a\u9886\u57df\u5e94\u7528\u4e2d\u5b58\u5728\u7684\u5408\u89c4\u98ce\u9669\uff0c\u5e76\u5f3a\u8c03\u4e86\u5efa\u7acb\u4ee5\u4eba\u4e3a\u672c\u3001\u5408\u89c4\u9a71\u52a8\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7cfb\u7edf\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2510.01483", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01483", "abs": "https://arxiv.org/abs/2510.01483", "authors": ["Mohamad Al Mdfaa", "Svetlana Lukina", "Timur Akhtyamov", "Arthur Nigmatzyanov", "Dmitrii Nalberskii", "Sergey Zagoruyko", "Gonzalo Ferrer"], "title": "VL-KnG: Visual Scene Understanding for Navigation Goal Identification using Spatiotemporal Knowledge Graphs", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Vision-language models (VLMs) have shown potential for robot navigation but\nencounter fundamental limitations: they lack persistent scene memory, offer\nlimited spatial reasoning, and do not scale effectively with video duration for\nreal-time application. We present VL-KnG, a Visual Scene Understanding system\nthat tackles these challenges using spatiotemporal knowledge graph construction\nand computationally efficient query processing for navigation goal\nidentification. Our approach processes video sequences in chunks utilizing\nmodern VLMs, creates persistent knowledge graphs that maintain object identity\nover time, and enables explainable spatial reasoning through queryable graph\nstructures. We also introduce WalkieKnowledge, a new benchmark with about 200\nmanually annotated questions across 8 diverse trajectories spanning\napproximately 100 minutes of video data, enabling fair comparison between\nstructured approaches and general-purpose VLMs. Real-world deployment on a\ndifferential drive robot demonstrates practical applicability, with our method\nachieving 77.27% success rate and 76.92% answer accuracy, matching Gemini 2.5\nPro performance while providing explainable reasoning supported by the\nknowledge graph, computational efficiency for real-time deployment across\ndifferent tasks, such as localization, navigation and planning. Code and\ndataset will be released after acceptance.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86VL-KnG\uff0c\u4e00\u4e2a\u89c6\u89c9\u573a\u666f\u7406\u89e3\u7cfb\u7edf\uff0c\u901a\u8fc7\u65f6\u7a7a\u77e5\u8bc6\u56fe\u548c\u9ad8\u6548\u67e5\u8be2\u5904\u7406\u89e3\u51b3\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u9ad8\u6210\u529f\u7387\u548c\u53ef\u89e3\u91ca\u63a8\u7406\u3002", "motivation": "\u9488\u5bf9\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u7f3a\u4e4f\u6301\u4e45\u573a\u666f\u8bb0\u5fc6\u3001\u6709\u9650\u7684\u7a7a\u95f4\u63a8\u7406\u548c\u5b9e\u65f6\u5e94\u7528\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u8fdb\u884c\u6539\u8fdb\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u65f6\u7a7a\u77e5\u8bc6\u56fe\u548c\u9ad8\u6548\u7684\u67e5\u8be2\u5904\u7406\u6765\u89e3\u51b3\u5bfc\u822a\u76ee\u6807\u8bc6\u522b\u95ee\u9898\u3002", "result": "\u5728\u5b9e\u9645\u6848\u4f8b\u4e2d\uff0cVL-KnG\u5728\u5dee\u5206\u9a71\u52a8\u673a\u5668\u4eba\u4e0a\u5c55\u793a\u4e86\u826f\u597d\u7684\u6548\u679c\uff0c\u4e0eGemini 2.5 Pro\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "VL-KnG\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8fbe\u523077.27%\u7684\u6210\u529f\u7387\u548c76.92%\u7684\u56de\u7b54\u51c6\u786e\u7387\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u63a8\u7406\uff0c\u9002\u5408\u5b9e\u65f6\u90e8\u7f72\u3002"}}
{"id": "2510.01862", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.01862", "abs": "https://arxiv.org/abs/2510.01862", "authors": ["Samantha Stedtler", "Marianna Leventi"], "title": "Who is responsible? Social Identity, Robot Errors and Blame Attribution", "comment": "In proceedings of Robophilosophy 2024. Accepted for publication in:\n  Frontiers in Artificial Intelligence and Applications (FAIA), Volume 397:\n  Social Robots with AI: Prospects, Risks, and Responsible Methods, IOS Press", "summary": "This paper argues that conventional blame practices fall short of capturing\nthe complexity of moral experiences, neglecting power dynamics and\ndiscriminatory social practices. It is evident that robots, embodying roles\nlinked to specific social groups, pose a risk of reinforcing stereotypes of how\nthese groups behave or should behave, so they set a normative and descriptive\nstandard. In addition, we argue that faulty robots might create expectations of\nwho is supposed to compensate and repair after their errors, where social\ngroups that are already disadvantaged might be blamed disproportionately if\nthey do not act according to their ascribed roles. This theoretical and\nempirical gap becomes even more urgent to address as there have been\nindications of potential carryover effects from Human-Robot Interactions (HRI)\nto Human-Human Interactions (HHI). We therefore urge roboticists and designers\nto stay in an ongoing conversation about how social traits are conceptualised\nand implemented in this technology. We also argue that one solution could be to\n'embrace the glitch' and to focus on constructively disrupting practices\ninstead of prioritizing efficiency and smoothness of interaction above\neverything else. Apart from considering ethical aspects in the design phase of\nsocial robots, we see our analysis as a call for more research on the\nconsequences of robot stereotyping and blame attribution.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u673a\u5668\u4eba\u5982\u4f55\u5728\u793e\u4f1a\u4e92\u52a8\u4e2d\u5f15\u53d1\u523b\u677f\u5370\u8c61\u548c\u8d23\u4efb\u5f52\u5c5e\u95ee\u9898\uff0c\u5e76\u547c\u5401\u5bf9\u6b64\u8fdb\u884c\u6df1\u5165\u7814\u7a76\u3002", "motivation": "\u4f20\u7edf\u8d23\u5907\u5b9e\u8df5\u65e0\u6cd5\u6355\u6349\u9053\u5fb7\u4f53\u9a8c\u7684\u590d\u6742\u6027\uff0c\u5ffd\u89c6\u4e86\u6743\u529b\u52a8\u6001\u548c\u6b67\u89c6\u6027\u793e\u4f1a\u5b9e\u8df5\u3002", "method": "\u7406\u8bba\u5206\u6790\u4e0e\u5b9e\u8bc1\u7814\u7a76\u7ed3\u5408\uff0c\u63a2\u8ba8\u4eba\u673a\u4ea4\u4e92\u5bf9\u4eba\u9645\u4ea4\u4e92\u7684\u5f71\u54cd\u3002", "result": "\u673a\u5668\u4eba\u53ef\u80fd\u52a0\u5267\u5bf9\u7279\u5b9a\u793e\u4f1a\u7fa4\u4f53\u7684\u523b\u677f\u5370\u8c61\uff0c\u5bfc\u81f4\u4e0d\u6210\u6bd4\u4f8b\u7684\u8d23\u4efb\u5f52\u5c5e\uff0c\u4f7f\u5f31\u52bf\u7fa4\u4f53\u53d7\u5230\u66f4\u591a\u6307\u8d23\u3002", "conclusion": "\u9700\u8981\u66f4\u591a\u7814\u7a76\u6765\u4e86\u89e3\u673a\u5668\u4eba\u523b\u677f\u5370\u8c61\u548c\u8d23\u4efb\u5f52\u5c5e\u7684\u540e\u679c\uff0c\u5e76\u5728\u793e\u4ea4\u673a\u5668\u4eba\u8bbe\u8ba1\u9636\u6bb5\u8003\u8651\u4f26\u7406\u65b9\u9762\u3002"}}
{"id": "2510.01485", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.01485", "abs": "https://arxiv.org/abs/2510.01485", "authors": ["Nicholas B. Andrews", "Yanhao Yang", "Sofya Akhetova", "Kristi A. Morgansen", "Ross L. Hatton"], "title": "Pose Estimation of a Thruster-Driven Bioinspired Multi-Link Robot", "comment": "8 pages, 8 figures, 3 tables", "summary": "This work demonstrates pose (position and shape) estimation for a\nfree-floating, bioinspired multi-link robot with unactuated joints,\nlink-mounted thrusters for control, and a single gyroscope per link, resulting\nin an underactuated, minimally sensed platform. Through a proof-of-concept\nhardware experiment and offline Kalman filter analysis, we show that the\nrobot's pose can be reliably estimated. State estimation is performed using an\nunscented Kalman filter augmented with Gaussian process residual learning to\ncompensate for non-zero-mean, non-Gaussian noise. We further show that a filter\ntrained on a multi-gait dataset (forward, backward, left, right, and turning)\nperforms comparably to one trained on a larger forward-gait-only dataset when\nboth are evaluated on the same forward-gait test trajectory. These results\nreveal overlap in the gait input space, which can be exploited to reduce\ntraining data requirements while enhancing the filter's generalizability across\nmultiple gaits.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u6709\u6548\u7684\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u591a\u6837\u672c\u6b65\u6001\u8bad\u7ec3\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u7684\u901a\u7528\u6027\uff0c\u51cf\u5c11\u4e86\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u3002", "motivation": "\u7814\u7a76\u76ee\u7684\u5728\u4e8e\u5b9e\u73b0\u5bf9\u4e00\u79cd\u81ea\u7531\u6f02\u6d6e\u3001\u4eff\u751f\u7684\u591a\u8fde\u6746\u673a\u5668\u4eba\u7684\u59ff\u6001\u4f30\u8ba1\uff0c\u5c24\u5176\u662f\u5728\u5176\u5173\u8282\u672a\u88ab\u52a8\u4f5c\u63a7\u5236\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u91c7\u7528\u65e0\u5473\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u5e76\u7ed3\u5408\u9ad8\u65af\u8fc7\u7a0b\u6b8b\u5dee\u5b66\u4e60\u6765\u5b9e\u73b0\u72b6\u6001\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u673a\u5668\u4eba\u59ff\u6001\u53ef\u6210\u529f\u4e14\u53ef\u9760\u5730\u4f30\u7b97\uff0c\u5e76\u4e14\u57fa\u4e8e\u591a\u6b65\u6001\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6ee4\u6ce2\u5668\u5728\u540c\u4e00\u524d\u8fdb\u6b65\u6001\u6d4b\u8bd5\u4e0a\u8868\u73b0\u4e0e\u5355\u6b65\u6001\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6ee4\u6ce2\u5668\u76f8\u4eff\u3002", "conclusion": "\u6240\u63d0\u8bae\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u4f30\u7b97\u591a\u8fde\u6746\u673a\u5668\u4eba\u5728\u81ea\u7531\u6f02\u6d6e\u6761\u4ef6\u4e0b\u7684\u59ff\u6001\uff0c\u4e14\u5728\u591a\u79cd\u6b65\u6001\u95f4\u5177\u6709\u8f83\u597d\u7684\u901a\u7528\u80fd\u529b\u3002"}}
{"id": "2510.02040", "categories": ["cs.HC", "cs.CY", "91B14, 91B12, 91B32", "H.5.3; J.4; K.4.2"], "pdf": "https://arxiv.org/pdf/2510.02040", "abs": "https://arxiv.org/abs/2510.02040", "authors": ["Joshua C. Yang", "Noemi Scheurer"], "title": "Komitee Equal Shares: Choosing Together as Voters and as Groups with a Co-designed Virtual Budget Algorithm", "comment": null, "summary": "Public funding processes demand fairness, learning, and outcomes that\nparticipants can understand. We introduce Komitee Equal Shares, a priceable\nvirtual-budget allocation framework that integrates two signals: in voter mode,\nparticipants cast point votes; in evaluator mode, small groups assess proposals\nagainst collectively defined impact fields. The framework extends the Method of\nEqual Shares by translating both signals into virtual spending power and\nproducing voting receipts. We deployed the framework in the 2025 Kultur Komitee\nin Winterthur, Switzerland. Our contributions are: (1) a clear separation of\ndecision modes, addressing a gap in social choice that typically treats\nparticipatory budgeting as preference aggregation while citizens also see\nthemselves as evaluators; and (2) the design of voting receipts that\noperationalise priceability into participant-facing explanations, making\nproportional allocations legible and traceable. The framework generalises to\nparticipatory grant-making and budgeting, offering a model where citizens act\nas voters and evaluators within one proportional, explainable allocation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u865a\u62df\u9884\u7b97\u5206\u914d\u6846\u67b6\uff0c\u589e\u5f3a\u4e86\u516c\u6c11\u5728\u53c2\u4e0e\u5f0f\u9884\u7b97\u4e2d\u7684\u89d2\u8272\uff0c\u63d0\u4f9b\u4e86\u516c\u5e73\u548c\u53ef\u8ffd\u6eaf\u7684\u8d44\u91d1\u5206\u914d\u65b9\u5f0f\u3002", "motivation": "\u516c\u5171\u8d44\u91d1\u8fc7\u7a0b\u9700\u8981\u516c\u5e73\u3001\u516c\u6b63\uff0c\u4e14\u53c2\u4e0e\u8005\u80fd\u591f\u7406\u89e3\u7684\u7ed3\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKomitee Equal Shares\u7684\u865a\u62df\u9884\u7b97\u5206\u914d\u6846\u67b6\uff0c\u901a\u8fc7\u70b9\u6570\u6295\u7968\u548c\u5c0f\u7ec4\u8bc4\u4f30\u63d0\u6848\uff0c\u7ed3\u5408\u4e24\u4e2a\u4fe1\u53f7\u8fdb\u884c\u9884\u7b97\u5206\u914d\u3002", "result": "\u5728\u745e\u58eb\u6e29\u7279\u56fe\u5c14\u76842025\u5e74Kultur Komitee\u4e2d\u90e8\u7f72\u4e86\u8be5\u6846\u67b6\uff0c\u9a8c\u8bc1\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u548c\u53ef\u64cd\u4f5c\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6a21\u578b\uff0c\u4f7f\u516c\u6c11\u80fd\u591f\u5728\u53c2\u4e0e\u5f0f\u9884\u7b97\u548c\u62e8\u6b3e\u4e2d\u540c\u65f6\u626e\u6f14\u6295\u7968\u8005\u548c\u8bc4\u4f30\u8005\uff0c\u4fdd\u969c\u4e86\u516c\u5e73\u548c\u900f\u660e\u6027\u3002"}}
{"id": "2510.01519", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01519", "abs": "https://arxiv.org/abs/2510.01519", "authors": ["Wei Han Chen", "Yuchen Liu", "Alexiy Buynitsky", "Ahmed H. Qureshi"], "title": "Online Hierarchical Policy Learning using Physics Priors for Robot Navigation in Unknown Environments", "comment": null, "summary": "Robot navigation in large, complex, and unknown indoor environments is a\nchallenging problem. The existing approaches, such as traditional\nsampling-based methods, struggle with resolution control and scalability, while\nimitation learning-based methods require a large amount of demonstration data.\nActive Neural Time Fields (ANTFields) have recently emerged as a promising\nsolution by using local observations to learn cost-to-go functions without\nrelying on demonstrations. Despite their potential, these methods are hampered\nby challenges such as spectral bias and catastrophic forgetting, which diminish\ntheir effectiveness in complex scenarios. To address these issues, our approach\ndecomposes the planning problem into a hierarchical structure. At the high\nlevel, a sparse graph captures the environment's global connectivity, while at\nthe low level, a planner based on neural fields navigates local obstacles by\nsolving the Eikonal PDE. This physics-informed strategy overcomes common\npitfalls like spectral bias and neural field fitting difficulties, resulting in\na smooth and precise representation of the cost landscape. We validate our\nframework in large-scale environments, demonstrating its enhanced adaptability\nand precision compared to previous methods, and highlighting its potential for\nonline exploration, mapping, and real-world navigation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u7684\u673a\u5668\u4eba\u5bfc\u822a\u65b9\u6cd5\uff0c\u5229\u7528\u7a00\u758f\u56fe\u548c\u795e\u7ecf\u573a\u8ba1\u5212\uff0c\u89e3\u51b3\u4e86\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u95ee\u9898\uff0c\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u9002\u5e94\u6027\u548c\u7cbe\u786e\u5ea6\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u590d\u6742\u5ba4\u5185\u73af\u5883\u5bfc\u822a\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u4f20\u7edf\u65b9\u6cd5\u7684\u5206\u8fa8\u7387\u63a7\u5236\u548c\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5bf9\u6f14\u793a\u6570\u636e\u7684\u4f9d\u8d56\u3002", "method": "\u91c7\u7528\u5206\u5c42\u7ed3\u6784\u5bf9\u89c4\u5212\u95ee\u9898\u8fdb\u884c\u89e3\u6784\uff0c\u9ad8\u5c42\u4f7f\u7528\u7a00\u758f\u56fe\u6355\u6349\u5168\u7403\u8fde\u901a\u6027\uff0c\u4f4e\u5c42\u91c7\u7528\u57fa\u4e8e\u795e\u7ecf\u573a\u7684\u89c4\u5212\u65b9\u6cd5\u89e3\u51b3\u5c40\u90e8\u969c\u788d\u3002", "result": "\u5728\u5927\u578b\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u6846\u67b6\uff0c\u76f8\u8f83\u4e8e\u73b0\u6709\u65b9\u6cd5\u663e\u793a\u51fa\u66f4\u5f3a\u7684\u9002\u5e94\u6027\u548c\u7cbe\u786e\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9002\u5e94\u6027\u548c\u7cbe\u786e\u5ea6\uff0c\u5177\u6709\u5728\u7ebf\u63a2\u7d22\u3001\u6620\u5c04\u548c\u73b0\u5b9e\u4e16\u754c\u5bfc\u822a\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.02153", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02153", "abs": "https://arxiv.org/abs/2510.02153", "authors": ["Hasan Mahmud", "Najmul Islam", "Satish Krishnan"], "title": "Human-Robo-advisor collaboration in decision-making: Evidence from a multiphase mixed methods experimental study", "comment": null, "summary": "Robo-advisors (RAs) are cost-effective, bias-resistant alternatives to human\nfinancial advisors, yet adoption remains limited. While prior research has\nexamined user interactions with RAs, less is known about how individuals\ninterpret RA roles and integrate their advice into decision-making. To address\nthis gap, this study employs a multiphase mixed methods design integrating a\nbehavioral experiment (N = 334), thematic analysis, and follow-up quantitative\ntesting. Findings suggest that people tend to rely on RAs, with reliance shaped\nby information about RA performance and the framing of advice as gains or\nlosses. Thematic analysis reveals three RA roles in decision-making and four\nuser types, each reflecting distinct patterns of advice integration. In\naddition, a 2 x 2 typology categorizes antecedents of acceptance into enablers\nand inhibitors at both the individual and algorithmic levels. By combining\nbehavioral, interpretive, and confirmatory evidence, this study advances\nunderstanding of human-RA collaboration and provides actionable insights for\ndesigning more trustworthy and adaptive RA systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u7528\u6237\u5bf9\u673a\u5668\u4eba\u987e\u95ee\u7684\u7406\u89e3\u53ca\u5176\u5efa\u8bae\u6574\u5408\uff0c\u53d1\u73b0\u4e0d\u540c\u7528\u6237\u7c7b\u578b\u548cRA\u89d2\u8272\u5f71\u54cd\u4f9d\u8d56\u6027\uff0c\u5e76\u63d0\u4f9b\u8bbe\u8ba1\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u5c3d\u7ba1\u673a\u5668\u4eba\u987e\u95ee\u4f5c\u4e3a\u4eba\u7c7b\u8d22\u52a1\u987e\u95ee\u7684\u6210\u672c\u6548\u76ca\u548c\u6297\u504f\u89c1\u7684\u66ff\u4ee3\u9009\u9879\uff0c\u4f46\u5176\u91c7\u7528\u7387\u4ecd\u7136\u6709\u9650\uff0c\u56e0\u6b64\u7814\u7a76\u4e2a\u4f53\u5bf9RA\u89d2\u8272\u7684\u7406\u89e3\u548c\u5efa\u8bae\u6574\u5408\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u6df7\u5408\u65b9\u6cd5\u8bbe\u8ba1\uff0c\u7ed3\u5408\u884c\u4e3a\u5b9e\u9a8c\u3001\u4e3b\u9898\u5206\u6790\u548c\u540e\u7eed\u7684\u5b9a\u91cf\u6d4b\u8bd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4eba\u4eec\u503e\u5411\u4e8e\u4f9d\u8d56RA\uff0c\u800c\u8fd9\u79cd\u4f9d\u8d56\u53d7\u5230RA\u7ee9\u6548\u4fe1\u606f\u548c\u5efa\u8bae\u6846\u67b6\uff08\u6536\u76ca\u6216\u635f\u5931\uff09\u7684\u5f71\u54cd\u3002\u540c\u65f6\uff0c\u63ed\u793a\u4e86RA\u5728\u51b3\u7b56\u4e2d\u7684\u4e09\u79cd\u89d2\u8272\u548c\u56db\u79cd\u7528\u6237\u7c7b\u578b\uff0c\u4ee5\u53ca\u63a5\u53d7\u7684\u4fc3\u8fdb\u56e0\u7d20\u548c\u6291\u5236\u56e0\u7d20\u3002", "conclusion": "\u672c\u7814\u7a76\u6df1\u5165\u63a2\u8ba8\u4e86\u7528\u6237\u5982\u4f55\u7406\u89e3\u548c\u6574\u5408\u673a\u5668\u4eba\u987e\u95ee\uff08RA\uff09\u7684\u5efa\u8bae\uff0c\u4e3a\u8bbe\u8ba1\u66f4\u53ef\u4fe1\u548c\u9002\u5e94\u6027\u5f3a\u7684RA\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89c1\u89e3\u3002"}}
{"id": "2510.01592", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01592", "abs": "https://arxiv.org/abs/2510.01592", "authors": ["Shun Niijima", "Ryoichi Tsuzaki", "Noriaki Takasugi", "Masaya Kinoshita"], "title": "Real-time Multi-Plane Segmentation Based on GPU Accelerated High-Resolution 3D Voxel Mapping for Legged Robot Locomotion", "comment": "8 pages, 12 figures, This work has been submitted to the IEEE for\n  possible publication. Copyright may be transfered without notice, after which\n  this version may no longer be accessible", "summary": "This paper proposes a real-time multi-plane segmentation method based on\nGPU-accelerated high-resolution 3D voxel mapping for legged robot locomotion.\nExisting online planar mapping approaches struggle to balance accuracy and\ncomputational efficiency: direct depth image segmentation from specific sensors\nsuffers from poor temporal integration, height map-based methods cannot\nrepresent complex 3D structures like overhangs, and voxel-based plane\nsegmentation remains unexplored for real-time applications. To address these\nlimitations, we develop a novel framework that integrates vertex-based\nconnected component labeling with random sample consensus based plane detection\nand convex hull, leveraging GPU parallel computing to rapidly extract planar\nregions from point clouds accumulated in high-resolution 3D voxel maps.\nExperimental results demonstrate that the proposed method achieves fast and\naccurate 3D multi-plane segmentation at over 30 Hz update rate even at a\nresolution of 0.01 m, enabling the detected planes to be utilized in real time\nfor locomotion tasks. Furthermore, we validate the effectiveness of our\napproach through experiments in both simulated environments and physical legged\nrobot platforms, confirming robust locomotion performance when considering 3D\nplanar structures.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGPU\u52a0\u901f\u7684\u5b9e\u65f6\u591a\u5e73\u9762\u5206\u5272\u65b9\u6cd5\uff0c\u6210\u529f\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u590d\u67423D\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u5728\u7ebf\u5e73\u9762\u6620\u5c04\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u4e0e\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u4e14\u4f53\u7d20\u5e73\u9762\u5206\u5272\u5c1a\u672a\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u5f97\u5230\u63a2\u7d22\u3002", "method": "\u91c7\u7528GPU\u52a0\u901f\u7684\u9ad8\u5206\u8fa8\u73873D\u4f53\u7d20\u6620\u5c04\uff0c\u7ed3\u5408\u57fa\u4e8e\u968f\u673a\u91c7\u6837\u4e00\u81f4\u6027\u7684\u5e73\u9762\u68c0\u6d4b\uff0c\u5b9e\u73b0\u5feb\u901f\u63d0\u53d6\u5e73\u9762\u533a\u57df\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u57280.01m\u5206\u8fa8\u7387\u4e0b\u8fbe\u5230\u8d85\u8fc730Hz\u7684\u5b9e\u65f6\u66f4\u65b0\u901f\u7387\uff0c\u51c6\u786e\u5b9e\u73b03D\u591a\u5e73\u9762\u5206\u5272\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u590d\u67423D\u5e73\u9762\u7ed3\u6784\u4e2d\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u8fd0\u52a8\u8868\u73b0\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u4efb\u52a1\u3002"}}
{"id": "2510.02157", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.02157", "abs": "https://arxiv.org/abs/2510.02157", "authors": ["Xuxin Tang", "Rehema Abulikemu", "Eric Krokos", "Kirsten Whitley", "Xuan Wang", "Chris North"], "title": "Agentic Reasoning and Refinement through Semantic Interaction", "comment": null, "summary": "Sensemaking report writing often requires multiple refinements in the\niterative process. While Large Language Models (LLMs) have shown promise in\ngenerating initial reports based on human visual workspace representations,\nthey struggle to precisely incorporate sequential semantic interactions during\nthe refinement process. We introduce VIS-ReAct, a framework that reasons about\nnewly-added semantic interactions in visual workspaces to steer the LLM for\nreport refinement.\n  VIS-ReAct is a two-agent framework: a primary LLM analysis agent interprets\nnew semantic interactions to infer user intentions and generate refinement\nplanning, followed by an LLM refinement agent that updates reports accordingly.\nThrough case study, VIS-ReAct outperforms baseline and VIS-ReAct (without LLM\nanalysis) on targeted refinement, semantic fidelity, and transparent inference.\nResults demonstrate that VIS-ReAct better handles various interaction types and\ngranularities while enhancing the transparency of human-LLM collaboration.", "AI": {"tldr": "VIS-ReAct \u662f\u4e00\u79cd\u6539\u8fdb\u62a5\u544a\u7f16\u5199\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u4e2a\u4ee3\u7406\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u62a5\u544a\u7cbe\u7ec6\u5316\u8fc7\u7a0b\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7cbe\u7ec6\u5316\u8fc7\u7a0b\u4e2d\u96be\u4ee5\u51c6\u786e\u5904\u7406\u987a\u5e8f\u8bed\u4e49\u4e92\u52a8\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa VIS-ReAct \u6846\u67b6\uff0c\u91c7\u7528\u4e24\u4e2a\u4ee3\u7406\uff1a\u5206\u6790\u4ee3\u7406\u7406\u89e3\u65b0\u7684\u8bed\u4e49\u4e92\u52a8\u5e76\u89c4\u5212\u7cbe\u7ec6\u5316\uff0c\u7cbe\u7ec6\u5316\u4ee3\u7406\u4f9d\u636e\u8ba1\u5212\u66f4\u65b0\u62a5\u544a\u3002", "result": "VIS-ReAct \u663e\u8457\u63d0\u9ad8\u4e86\u76ee\u6807\u7cbe\u7ec6\u5316\u3001\u8bed\u4e49\u51c6\u786e\u6027\u548c\u63a8\u7406\u900f\u660e\u5ea6\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u548c\u7c92\u5ea6\u7684\u4e92\u52a8\u3002", "conclusion": "VIS-ReAct \u5728\u9488\u5bf9\u62a5\u544a\u7684\u7cbe\u7ec6\u5316\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u57fa\u51c6\u65b9\u6cd5\uff0c\u5e76\u63d0\u5347\u4e86\u4eba\u7c7b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u900f\u660e\u534f\u4f5c\u80fd\u529b\u3002"}}
{"id": "2510.01603", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01603", "abs": "https://arxiv.org/abs/2510.01603", "authors": ["Sharfin Islam", "Zewen Chen", "Zhanpeng He", "Swapneel Bhatt", "Andres Permuy", "Brock Taylor", "James Vickery", "Pedro Piacenza", "Cheng Zhang", "Matei Ciocarlie"], "title": "MiniBEE: A New Form Factor for Compact Bimanual Dexterity", "comment": null, "summary": "Bimanual robot manipulators can achieve impressive dexterity, but typically\nrely on two full six- or seven- degree-of-freedom arms so that paired grippers\ncan coordinate effectively. This traditional framework increases system\ncomplexity while only exploiting a fraction of the overall workspace for\ndexterous interaction. We introduce the MiniBEE (Miniature Bimanual\nEnd-effector), a compact system in which two reduced-mobility arms (3+ DOF\neach) are coupled into a kinematic chain that preserves full relative\npositioning between grippers. To guide our design, we formulate a kinematic\ndexterity metric that enlarges the dexterous workspace while keeping the\nmechanism lightweight and wearable. The resulting system supports two\ncomplementary modes: (i) wearable kinesthetic data collection with self-tracked\ngripper poses, and (ii) deployment on a standard robot arm, extending dexterity\nacross its entire workspace. We present kinematic analysis and design\noptimization methods for maximizing dexterous range, and demonstrate an\nend-to-end pipeline in which wearable demonstrations train imitation learning\npolicies that perform robust, real-world bimanual manipulation.", "AI": {"tldr": "MiniBEE\u662f\u4e00\u79cd\u7d27\u51d1\u7684\u53cc\u624b\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u6709\u9650\u81ea\u7531\u5ea6\u7684\u81c2\u5e76\u4f18\u5316\u5176\u8fd0\u52a8\u5b66\uff0c\u63d0\u9ad8\u4e86\u53cc\u624b\u534f\u4f5c\u7684\u7075\u6d3b\u6027\u4e0e\u5e94\u7528\u8303\u56f4\u3002", "motivation": "\u4f20\u7edf\u7684\u53cc\u81c2\u673a\u5668\u4eba\u624b\u64cd\u7eb5\u5668\u4f9d\u8d56\u5168\u81ea\u7531\u5ea6\u7684\u81c2\uff0c\u589e\u52a0\u4e86\u7cfb\u7edf\u590d\u6742\u6027\u4e14\u672a\u5145\u5206\u5229\u7528\u5de5\u4f5c\u7a7a\u95f4\u3002", "method": "\u901a\u8fc7\u8fd0\u52a8\u5b66\u5206\u6790\u548c\u8bbe\u8ba1\u4f18\u5316\u65b9\u6cd5\uff0c\u5236\u5b9a\u8fd0\u52a8\u7075\u6d3b\u6027\u6307\u6807\u4ee5\u6269\u5927\u7075\u6d3b\u5de5\u4f5c\u7a7a\u95f4\uff0c\u5e76\u5b9e\u73b0\u53ef\u7a7f\u6234\u6f14\u793a\u8bad\u7ec3\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u3002", "result": "MiniBEE\u7cfb\u7edf\u901a\u8fc7\u5c06\u4e24\u81c2\u8026\u5408\u8fdb\u8fd0\u52a8\u94fe\uff0c\u652f\u6301\u53ef\u7a7f\u6234\u6570\u636e\u91c7\u96c6\u548c\u6807\u51c6\u673a\u5668\u4eba\u624b\u81c2\u4e0a\u7684\u7075\u6d3b\u64cd\u4f5c\uff0c\u5c55\u73b0\u4e86\u4f18\u5316\u7684\u8fd0\u52a8\u5b66\u5206\u6790\u3002", "conclusion": "MiniBEE\u7cfb\u7edf\u901a\u8fc7\u4f18\u5316\u8fd0\u52a8\u5b66\u7075\u6d3b\u6027\uff0c\u5728\u51cf\u5c11\u7cfb\u7edf\u590d\u6742\u6027\u7684\u540c\u65f6\u589e\u5f3a\u4e86\u53cc\u624b\u534f\u4f5c\u7684\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u53ef\u7a7f\u6234\u6570\u636e\u6536\u96c6\u53ca\u6807\u51c6\u673a\u5668\u4eba\u624b\u81c2\u7684\u5ef6\u5c55\u3002"}}
{"id": "2510.02181", "categories": ["cs.HC", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.02181", "abs": "https://arxiv.org/abs/2510.02181", "authors": ["Liang-Yuan Wu", "Dhruv Jain"], "title": "EvolveCaptions: Empowering DHH Users Through Real-Time Collaborative Captioning", "comment": null, "summary": "Automatic Speech Recognition (ASR) systems often fail to accurately\ntranscribe speech from Deaf and Hard of Hearing (DHH) individuals, especially\nduring real-time conversations. Existing personalization approaches typically\nrequire extensive pre-recorded data and place the burden of adaptation on the\nDHH speaker. We present EvolveCaptions, a real-time, collaborative ASR\nadaptation system that supports in-situ personalization with minimal effort.\nHearing participants correct ASR errors during live conversations. Based on\nthese corrections, the system generates short, phonetically targeted prompts\nfor the DHH speaker to record, which are then used to fine-tune the ASR model.\nIn a study with 12 DHH and six hearing participants, EvolveCaptions reduced\nWord Error Rate (WER) across all DHH users within one hour of use, using only\nfive minutes of recording time on average. Participants described the system as\nintuitive, low-effort, and well-integrated into communication. These findings\ndemonstrate the promise of collaborative, real-time ASR adaptation for more\nequitable communication.", "AI": {"tldr": "EvolveCaptions\u662f\u4e00\u4e2a\u5b9e\u65f6\u534f\u4f5c\u7684ASR\u81ea\u9002\u5e94\u7cfb\u7edf\uff0c\u901a\u8fc7\u542c\u529b\u53c2\u4e0e\u8005\u7684\u5b9e\u65f6\u7ea0\u6b63\u548c\u77ed\u65f6\u95f4\u5f55\u97f3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u804b\u4eba\u548c\u542c\u969c\u4eba\u58eb\u7684\u8bed\u97f3\u8bc6\u522b\u51c6\u786e\u6027\u3002", "motivation": "\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u5728\u8f6c\u5f55\u804b\u4eba\u548c\u542c\u969c\u4eba\u58eb\u7684\u8bed\u8a00\u65f6\u5e38\u5e38\u4e0d\u51c6\u786e\uff0c\u73b0\u6709\u4e2a\u6027\u5316\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u9884\u5f55\u6570\u636e\uff0c\u5bf9\u7528\u6237\u9002\u5e94\u6027\u8981\u6c42\u9ad8\u3002", "method": "\u8ba9\u542c\u529b\u7528\u6237\u5728\u5b9e\u65f6\u5bf9\u8bdd\u4e2d\u7ea0\u6b63ASR\u9519\u8bef\uff0c\u57fa\u4e8e\u8fd9\u4e9b\u7ea0\u6b63\u751f\u6210\u77ed\u6682\u7684\uff0c\u8bed\u97f3\u4e0a\u5177\u6709\u9488\u5bf9\u6027\u7684\u63d0\u793a\u4f9bDHH\u7528\u6237\u5f55\u97f3\uff0c\u4ece\u800c\u5fae\u8c03ASR\u6a21\u578b\u3002", "result": "\u5728\u4e00\u9879\u5305\u542b12\u540dDHH\u7528\u6237\u548c6\u540d\u542c\u529b\u7528\u6237\u7684\u7814\u7a76\u4e2d\uff0cEvolveCaptions\u5728\u4f7f\u7528\u4ec5\u4e94\u5206\u949f\u5f55\u97f3\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u4e00\u5c0f\u65f6\u5185\u964d\u4f4e\u4e86\u6240\u6709DHH\u7528\u6237\u7684\u5355\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\u3002", "conclusion": "EvolveCaptions\u5c55\u793a\u4e86\u5b9e\u65f6\u534f\u4f5c\u7684ASR\u81ea\u9002\u5e94\u7cfb\u7edf\u5728\u4fc3\u8fdb\u66f4\u5e73\u7b49\u6c9f\u901a\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.01607", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01607", "abs": "https://arxiv.org/abs/2510.01607", "authors": ["Qiyuan Zeng", "Chengmeng Li", "Jude St. John", "Zhongyi Zhou", "Junjie Wen", "Guorui Feng", "Yichen Zhu", "Yi Xu"], "title": "ActiveUMI: Robotic Manipulation with Active Perception from Robot-Free Human Demonstrations", "comment": "technique report. The website is available at\n  https://activeumi.github.io", "summary": "We present ActiveUMI, a framework for a data collection system that transfers\nin-the-wild human demonstrations to robots capable of complex bimanual\nmanipulation. ActiveUMI couples a portable VR teleoperation kit with sensorized\ncontrollers that mirror the robot's end-effectors, bridging human-robot\nkinematics via precise pose alignment. To ensure mobility and data quality, we\nintroduce several key techniques, including immersive 3D model rendering, a\nself-contained wearable computer, and efficient calibration methods.\nActiveUMI's defining feature is its capture of active, egocentric perception.\nBy recording an operator's deliberate head movements via a head-mounted\ndisplay, our system learns the crucial link between visual attention and\nmanipulation. We evaluate ActiveUMI on six challenging bimanual tasks. Policies\ntrained exclusively on ActiveUMI data achieve an average success rate of 70\\%\non in-distribution tasks and demonstrate strong generalization, retaining a\n56\\% success rate when tested on novel objects and in new environments. Our\nresults demonstrate that portable data collection systems, when coupled with\nlearned active perception, provide an effective and scalable pathway toward\ncreating generalizable and highly capable real-world robot policies.", "AI": {"tldr": "ActiveUMI\u662f\u4e00\u4e2a\u6570\u636e\u6536\u96c6\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bb0\u5f55\u64cd\u4f5c\u8005\u7684\u5934\u90e8\u8fd0\u52a8\u6765\u5b9e\u73b0\u4eba\u673a\u534f\u4f5c\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u6267\u884c\u590d\u6742\u7684\u53cc\u624b\u64cd\u4f5c\uff0c\u8868\u73b0\u51fa\u826f\u597d\u7684\u6210\u529f\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u529b\u5728\u4e8e\u6784\u5efa\u4e00\u4e2a\u80fd\u591f\u8ba9\u673a\u5668\u4eba\u6267\u884c\u590d\u6742\u53cc\u624b\u64cd\u4f5c\u7684\u4eba\u7c7b\u793a\u8303\u6570\u636e\u6536\u96c6\u7cfb\u7edf\u3002", "method": "ActiveUMI\u7ed3\u5408\u4e86\u4fbf\u643a\u5f0f\u865a\u62df\u73b0\u5b9e\u9065\u64cd\u4f5c\u8bbe\u5907\u4e0e\u4f20\u611f\u5668\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u7cbe\u786e\u7684\u4f4d\u59ff\u5bf9\u9f50\u5b9e\u73b0\u4eba\u673a\u8fd0\u52a8\u5b66\u7684\u6865\u63a5\u3002", "result": "\u5728\u516d\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u53cc\u624b\u4efb\u52a1\u4e0a\uff0cActiveUMI\u6536\u96c6\u7684\u6570\u636e\u8bad\u7ec3\u7684\u7b56\u7565\u5728\u539f\u5206\u5e03\u4efb\u52a1\u4e0a\u7684\u5e73\u5747\u6210\u529f\u7387\u4e3a70%\uff0c\u5e76\u5728\u65b0\u7269\u4f53\u548c\u65b0\u73af\u5883\u4e0a\u7684\u6d4b\u8bd5\u4e2d\u4fdd\u630156%\u7684\u6210\u529f\u7387\u3002", "conclusion": "ActiveUMI\u4e3a\u6784\u5efa\u901a\u7528\u548c\u9ad8\u6548\u7684\u673a\u5668\u4eba\u7b56\u7565\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u6570\u636e\u6536\u96c6\u65b9\u6cd5\u3002"}}
{"id": "2510.01986", "categories": ["cs.RO", "cs.HC", "cs.SY", "eess.SY", "math.OC", "q-bio.NC", "I.6"], "pdf": "https://arxiv.org/pdf/2510.01986", "abs": "https://arxiv.org/abs/2510.01986", "authors": ["Varun Kotian", "Vishrut Jain", "Andrea Michelle Rios Lazcano", "Daan Marinus Pool", "Riender Happee", "Barys Shyrokau"], "title": "Reducing Discomfort in Driving Simulators: Motion Cueing for Motion Sickness Mitigation", "comment": null, "summary": "Driving simulators are increasingly used in research and development.\nHowever, simulators often cause motion sickness due to downscaled motion and\nunscaled veridical visuals. In this paper, a motion cueing algorithm is\nproposed that reduces motion sickness as predicted by the subjective vertical\nconflict (SVC) model using model predictive control (MPC). Both sensory\nconflict and specific force errors are penalised in the cost function, allowing\nthe algorithm to jointly optimise fidelity and comfort.\n  Human-in-the-loop experiments were conducted to compare four simulator motion\nsettings: two variations of our MPC-based algorithm, one focused on pure\nspecific force tracking and the second compromising specific force tracking and\nmotion sickness minimisation, as well as reference adaptive washout and no\nmotion cases. The experiments were performed on a hexapod driving simulator\nwith participants exposed to passive driving.\n  Experimental motion sickness results closely matched the sickness model\npredictions. As predicted by the model, the no motion condition yielded the\nlowest sickness levels. However, it was rated lowest in terms of fidelity. The\ncompromise solution reduced sickness by over 50% (average MISC level 3 to 1.5)\ncompared to adaptive washout and the algorithm focusing on specific force\ntracking, without any significant reduction in fidelity rating.\n  The proposed approach for developing MCA that takes into account both the\nsimulator dynamics and time evolution of motion sickness offers a significant\nadvancement in achieving an optimal control of motion sickness and specific\nforce recreation in driving simulators, supporting broader simulator use.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u8fd0\u52a8\u63d0\u793a\u7b97\u6cd5\uff0c\u80fd\u591f\u5728\u51cf\u5c11\u9a7e\u9a76\u6a21\u62df\u5668\u6655\u52a8\u75c5\u7684\u540c\u65f6\u63d0\u9ad8\u4eff\u771f\u903c\u771f\u5ea6\uff0c\u501f\u52a9\u4eba\u673a\u5b9e\u9a8c\u9a8c\u8bc1\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u9a7e\u9a76\u6a21\u62df\u5668\u5728\u7814\u7a76\u548c\u5f00\u53d1\u4e2d\u88ab\u8d8a\u6765\u8d8a\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u7531\u4e8e\u8fd0\u52a8\u7f29\u653e\u548c\u89c6\u89c9\u4e0d\u5bf9\u79f0\uff0c\u5e38\u5bfc\u81f4\u7528\u6237\u6655\u52a8\u75c5\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u7684\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8fd0\u52a8\u63d0\u793a\u7b97\u6cd5\uff0c\u65e8\u5728\u51cf\u5c11\u7531\u4e3b\u89c2\u5782\u76f4\u51b2\u7a81\u6a21\u578b\u9884\u6d4b\u7684\u6655\u52a8\u75c5\u3002\u901a\u8fc7\u4eba\u673a\u53c2\u4e0e\u5b9e\u9a8c\u6bd4\u8f83\u4e0d\u540c\u7684\u6a21\u62df\u5668\u8fd0\u52a8\u8bbe\u7f6e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86\u6655\u52a8\u75c5\uff0c\u7279\u522b\u662f\u5728\u59a5\u534f\u65b9\u6848\u4e2d\uff0c\u6655\u52a8\u75c5\u6c34\u5e73\u964d\u4f4e\u8d85\u8fc750%\u3002\u65e0\u8fd0\u52a8\u72b6\u6001\u4e0b\u7684\u6655\u52a8\u75c5\u6c34\u5e73\u6700\u4f4e\uff0c\u4f46\u4eff\u771f\u903c\u771f\u5ea6\u8bc4\u4ef7\u6700\u4f4e\u3002\u59a5\u534f\u65b9\u6848\u5728\u4e0d\u663e\u8457\u964d\u4f4e\u903c\u771f\u5ea6\u7684\u540c\u65f6\uff0c\u4e5f\u5927\u5e45\u964d\u4f4e\u4e86\u6655\u52a8\u75c5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8fd0\u52a8\u63d0\u793a\u7b97\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u6a21\u62df\u5668\u4e2d\u7684\u6655\u52a8\u75c5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u4eff\u771f\u903c\u771f\u5ea6\uff0c\u652f\u6301\u66f4\u5e7f\u6cdb\u7684\u6a21\u62df\u5668\u5e94\u7528\u3002"}}
{"id": "2510.01642", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01642", "abs": "https://arxiv.org/abs/2510.01642", "authors": ["Zijun Lin", "Jiafei Duan", "Haoquan Fang", "Dieter Fox", "Ranjay Krishna", "Cheston Tan", "Bihan Wen"], "title": "FailSafe: Reasoning and Recovery from Failures in Vision-Language-Action Models", "comment": "Project Page: https://jimntu.github.io/FailSafe/", "summary": "Recent advances in robotic manipulation have integrated low-level robotic\ncontrol into Vision-Language Models (VLMs), extending them into\nVision-Language-Action (VLA) models. Although state-of-the-art VLAs achieve\nstrong performance in downstream robotic applications, supported by large-scale\ncrowd-sourced robot training data, they still inevitably encounter failures\nduring execution. Enabling robots to reason about and recover from\nunpredictable and abrupt failures remains a critical challenge. Existing\nrobotic manipulation datasets, collected in either simulation or the real\nworld, primarily provide only ground-truth trajectories, leaving robots unable\nto recover once failures occur. Moreover, the few datasets that address failure\ndetection typically offer only textual explanations, which are difficult to\nutilize directly in VLA models. To address this gap, we introduce FailSafe, a\nnovel failure generation and recovery system that automatically produces\ndiverse failure cases paired with executable recovery actions. FailSafe can be\nseamlessly applied to any manipulation task in any simulator, enabling scalable\ncreation of failure-action data. To demonstrate its effectiveness, we fine-tune\nLLaVa-OneVision-7B (LLaVa-OV-7B) to build FailSafe-VLM. Experimental results\nshow that FailSafe-VLM successfully helps robotic arm detect and recover from\npotential failures, improving the performance of three state-of-the-art VLA\nmodels pi0-FAST, OpenVLA, OpenVLA-OFT) by up to 22.6% on average across several\ntasks in Maniskill. Furthermore, FailSafe-VLM could generalize across different\nspatial configurations, camera viewpoints, and robotic embodiments. We plan to\nrelease the FailSafe code to the community.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FailSafe\u7cfb\u7edf\uff0c\u4ee5\u89e3\u51b3\u673a\u5668\u4eba\u5728\u6267\u884c\u4efb\u52a1\u8fc7\u7a0b\u4e2d\u9047\u5230\u7684\u5931\u8d25\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u5931\u8d25\u6848\u4f8b\u53ca\u6062\u590d\u884c\u4e3a\u6765\u63d0\u5347VLA\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u673a\u5668\u4eba\u5728\u6267\u884c\u4efb\u52a1\u65f6\u4f1a\u9762\u4e34\u4e0d\u53ef\u9884\u77e5\u7684\u5931\u8d25\uff0c\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u76f8\u5173\u5931\u8d25\u68c0\u6d4b\u4e0e\u6062\u590d\u652f\u6301\uff0c\u5236\u7ea6\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u53ef\u9760\u6027\u3002", "method": "\u5f15\u5165FailSafe\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u5931\u8d25\u6848\u4f8b\u53ca\u5176\u53ef\u6267\u884c\u6062\u590d\u52a8\u4f5c\uff0c\u8fdb\u800c\u4f18\u5316VLA\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "FailSafe-VLM\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\uff0c\u5e73\u5747\u63d0\u5347\u4e86\u4e09\u79cd\u6700\u5148\u8fdb\u7684VLA\u6a21\u578b\uff08pi0-FAST\u3001OpenVLA\u3001OpenVLA-OFT\uff09\u6027\u80fd\u8fbe22.6%\u3002", "conclusion": "FailSafe\u7cfb\u7edf\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5728\u591a\u4efb\u52a1\u4e2d\u7684\u5931\u8d25\u68c0\u6d4b\u4e0e\u6062\u590d\u80fd\u529b\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u914d\u7f6e\u548c\u673a\u5668\u4eba\u5f62\u6001\u4e0b\u5177\u6709\u826f\u597d\u7684\u8fc1\u79fb\u6027\u3002"}}
{"id": "2510.01648", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01648", "abs": "https://arxiv.org/abs/2510.01648", "authors": ["Seungwon Choi", "Donggyu Park", "Seo-Yeon Hwang", "Tae-Wan Kim"], "title": "Statistical Uncertainty Learning for Robust Visual-Inertial State Estimation", "comment": null, "summary": "A fundamental challenge in robust visual-inertial odometry (VIO) is to\ndynamically assess the reliability of sensor measurements. This assessment is\ncrucial for properly weighting the contribution of each measurement to the\nstate estimate. Conventional methods often simplify this by assuming a static,\nuniform uncertainty for all measurements. This heuristic, however, may be\nlimited in its ability to capture the dynamic error characteristics inherent in\nreal-world data. To improve this limitation, we present a statistical framework\nthat learns measurement reliability assessment online, directly from sensor\ndata and optimization results. Our approach leverages multi-view geometric\nconsistency as a form of self-supervision. This enables the system to infer\nlandmark uncertainty and adaptively weight visual measurements during\noptimization. We evaluated our method on the public EuRoC dataset,\ndemonstrating improvements in tracking accuracy with average reductions of\napproximately 24\\% in translation error and 42\\% in rotation error compared to\nbaseline methods with fixed uncertainty parameters. The resulting framework\noperates in real time while showing enhanced accuracy and robustness. To\nfacilitate reproducibility and encourage further research, the source code will\nbe made publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u5728\u7ebf\u5b66\u4e60\u4f20\u611f\u5668\u6d4b\u91cf\u53ef\u9760\u6027\u7684\u7edf\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u6211\u76d1\u7763\u6539\u5584\u89c6\u89c9\u60ef\u6027\u6d4b\u8ddd\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5f3a\u8c03\u52a8\u6001\u8bc4\u4f30\u4f20\u611f\u5668\u6d4b\u91cf\u7684\u53ef\u9760\u6027\u4ee5\u66f4\u597d\u5730\u6743\u8861\u6bcf\u4e2a\u6d4b\u91cf\u5bf9\u72b6\u6001\u4f30\u8ba1\u7684\u8d21\u732e\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u65f6\u6570\u636e\u4e2d\u6355\u6349\u52a8\u6001\u8bef\u5dee\u7279\u6027\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5728\u7ebf\u5b66\u4e60\u4f20\u611f\u5668\u6d4b\u91cf\u53ef\u9760\u6027\u7684\u7edf\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u89c6\u56fe\u51e0\u4f55\u4e00\u81f4\u6027\u4f5c\u4e3a\u81ea\u6211\u76d1\u7763\u7684\u5f62\u5f0f\uff0c\u6765\u63a8\u65ad\u5730\u6807\u4e0d\u786e\u5b9a\u6027\u5e76\u81ea\u9002\u5e94\u52a0\u6743\u89c6\u89c9\u6d4b\u91cf\u3002", "result": "\u5728EuRoC\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5e73\u79fb\u8bef\u5dee\u4e0a\u5e73\u5747\u964d\u4f4e\u4e86\u7ea624%\uff0c\u65cb\u8f6c\u8bef\u5dee\u964d\u4f4e\u4e86\u7ea642%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5b9e\u65f6\u64cd\u4f5c\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u5728EuRoC\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u8d85\u8d8a\u4e86\u57fa\u51c6\u65b9\u6cd5\u3002"}}
{"id": "2510.01661", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01661", "abs": "https://arxiv.org/abs/2510.01661", "authors": ["Yifei Simon Shao", "Yuchen Zheng", "Sunan Sun", "Pratik Chaudhari", "Vijay Kumar", "Nadia Figueroa"], "title": "Symskill: Symbol and Skill Co-Invention for Data-Efficient and Real-Time Long-Horizon Manipulation", "comment": "CoRL 2025 Learning Effective Abstractions for Planning (LEAP)\n  Workshop Best Paper Award (https://sites.google.com/view/symskill)", "summary": "Multi-step manipulation in dynamic environments remains challenging. Two\nmajor families of methods fail in distinct ways: (i) imitation learning (IL) is\nreactive but lacks compositional generalization, as monolithic policies do not\ndecide which skill to reuse when scenes change; (ii) classical task-and-motion\nplanning (TAMP) offers compositionality but has prohibitive planning latency,\npreventing real-time failure recovery. We introduce SymSkill, a unified\nlearning framework that combines the benefits of IL and TAMP, allowing\ncompositional generalization and failure recovery in real-time. Offline,\nSymSkill jointly learns predicates, operators, and skills directly from\nunlabeled and unsegmented demonstrations. At execution time, upon specifying a\nconjunction of one or more learned predicates, SymSkill uses a symbolic planner\nto compose and reorder learned skills to achieve the symbolic goals, while\nperforming recovery at both the motion and symbolic levels in real time.\nCoupled with a compliant controller, SymSkill enables safe and uninterrupted\nexecution under human and environmental disturbances. In RoboCasa simulation,\nSymSkill can execute 12 single-step tasks with 85% success rate. Without\nadditional data, it composes these skills into multi-step plans requiring up to\n6 skill recompositions, recovering robustly from execution failures. On a real\nFranka robot, we demonstrate SymSkill, learning from 5 minutes of unsegmented\nand unlabeled play data, is capable of performing multiple tasks simply by goal\nspecifications. The source code and additional analysis can be found on\nhttps://sites.google.com/view/symskill.", "AI": {"tldr": "SymSkill\u662f\u4e00\u4e2a\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\u548c\u4efb\u52a1\u8fd0\u52a8\u89c4\u5212\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u80fd\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u7684\u591a\u6b65\u9aa4\u64cd\u4f5c\u548c\u6062\u590d\uff0c\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u591a\u6b65\u9aa4\u64cd\u4f5c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5e94\u5bf9\u8fd9\u4e00\u95ee\u9898\u65f6\u5404\u6709\u5c40\u9650\uff0c\u8feb\u5207\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u7684\u5b66\u4e60\u6846\u67b6\u3002", "method": "SymSkill\u901a\u8fc7\u4ece\u672a\u6807\u8bb0\u548c\u672a\u5206\u6bb5\u7684\u793a\u8303\u4e2d\u8054\u5408\u5b66\u4e60\u8c13\u8bcd\u3001\u64cd\u4f5c\u8005\u548c\u6280\u80fd\uff0c\u4f7f\u7528\u7b26\u53f7\u89c4\u5212\u5668\u5728\u6267\u884c\u65f6\u7ec4\u5408\u548c\u91cd\u65b0\u6392\u5e8f\u6280\u80fd\u3002", "result": "\u5728RoboCasa\u4eff\u771f\u4e2d\uff0cSymSkill\u4ee585%\u7684\u6210\u529f\u7387\u5b8c\u621012\u4e2a\u5355\u6b65\u4efb\u52a1\uff0c\u5e76\u80fd\u5c06\u6280\u80fd\u7ec4\u5408\u4e3a\u6700\u591a\u97006\u6b21\u91cd\u7ec4\u7684\u591a\u6b65\u9aa4\u8ba1\u5212\u3002\u5728\u771f\u5b9eFranka\u673a\u5668\u4eba\u4e0a\uff0cSymSkill\u80fd\u591f\u4ece5\u5206\u949f\u7684\u65e0\u6807\u7b7e\u6570\u636e\u4e2d\u5b66\u4e60\u5e76\u6839\u636e\u76ee\u6807\u89c4\u8303\u6267\u884c\u591a\u4e2a\u4efb\u52a1\u3002", "conclusion": "SymSkill\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u5728\u7ebf\u7ec4\u5408\u548c\u6062\u590d\u591a\u6b65\u9aa4\u64cd\u4f5c\uff0c\u4f7f\u5f97\u673a\u5668\u4eba\u80fd\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u9ad8\u6548\u6267\u884c\u4efb\u52a1\u3002"}}
{"id": "2510.01675", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.01675", "abs": "https://arxiv.org/abs/2510.01675", "authors": ["Jaewoo Lee", "Dongjae Lee", "Jinwoo Lee", "Hyungyu Lee", "Yeonjoon Kim", "H. Jin Kim"], "title": "Geometric Backstepping Control of Omnidirectional Tiltrotors Incorporating Servo-Rotor Dynamics for Robustness against Sudden Disturbances", "comment": null, "summary": "This work presents a geometric backstepping controller for a variable-tilt\nomnidirectional multirotor that explicitly accounts for both servo and rotor\ndynamics. Considering actuator dynamics is essential for more effective and\nreliable operation, particularly during aggressive flight maneuvers or recovery\nfrom sudden disturbances. While prior studies have investigated actuator-aware\ncontrol for conventional and fixed-tilt multirotors, these approaches rely on\nlinear relationships between actuator input and wrench, which cannot capture\nthe nonlinearities induced by variable tilt angles. In this work, we exploit\nthe cascade structure between the rigid-body dynamics of the multirotor and its\nnonlinear actuator dynamics to design the proposed backstepping controller and\nestablish exponential stability of the overall system. Furthermore, we reveal\nparametric uncertainty in the actuator model through experiments, and we\ndemonstrate that the proposed controller remains robust against such\nuncertainty. The controller was compared against a baseline that does not\naccount for actuator dynamics across three experimental scenarios: fast\ntranslational tracking, rapid rotational tracking, and recovery from sudden\ndisturbance. The proposed method consistently achieved better tracking\nperformance, and notably, while the baseline diverged and crashed during the\nfastest translational trajectory tracking and the recovery experiment, the\nproposed controller maintained stability and successfully completed the tasks,\nthereby demonstrating its effectiveness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u53cd\u63a8\u63a7\u5236\u5668\uff0c\u8003\u8651\u4f3a\u670d\u548c\u65cb\u7ffc\u52a8\u6001\uff0c\u63d0\u9ad8\u4e86\u53ef\u53d8\u503e\u659c\u5168\u5411\u591a\u65cb\u7ffc\u7684\u64cd\u4f5c\u7a33\u5b9a\u6027\u548c\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u8003\u8651\u6267\u884c\u5668\u52a8\u6001\u4ee5\u63d0\u9ad8\u7075\u6d3b\u98de\u884c\u64cd\u63a7\u7684\u6709\u6548\u6027\u548c\u53ef\u9760\u6027\uff0c\u5c24\u5176\u662f\u5728\u6fc0\u70c8\u98de\u884c\u8fd0\u52a8\u6216\u5e94\u5bf9\u7a81\u53d1\u5e72\u6270\u65f6\u3002", "method": "\u91c7\u7528\u51e0\u4f55\u53cd\u63a8\u63a7\u5236\u5668\u8bbe\u8ba1\uff0c\u5e76\u8003\u8651\u4e86\u4f3a\u670d\u548c\u65cb\u7ffc\u52a8\u6001\uff0c\u4ee5\u5b9e\u73b0\u5168\u7cfb\u7edf\u7684\u6307\u6570\u7a33\u5b9a\u6027\u3002", "result": "\u6240\u63d0\u63a7\u5236\u5668\u5728\u5feb\u901f\u5e73\u79fb\u8ddf\u8e2a\u3001\u5feb\u901f\u65cb\u8f6c\u8ddf\u8e2a\u548c\u5e94\u5bf9\u7a81\u53d1\u5e72\u6270\u7684\u5b9e\u9a8c\u4e2d\u5747\u8d85\u8fc7\u4e86\u4e0d\u8003\u8651\u6267\u884c\u5668\u52a8\u6001\u7684\u57fa\u7ebf\u63a7\u5236\u5668\uff0c\u5e76\u786e\u4fdd\u4e86\u4efb\u52a1\u7684\u6210\u529f\u5b8c\u6210\u3002", "conclusion": "\u6240\u63d0\u63a7\u5236\u5668\u5728\u5404\u79cd\u5b9e\u9a8c\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u8ddf\u8e2a\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u6311\u6218\u6027\u4efb\u52a1\u4e2d\u4fdd\u6301\u7a33\u5b9a\u3002"}}
{"id": "2510.01708", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01708", "abs": "https://arxiv.org/abs/2510.01708", "authors": ["Zixing Lei", "Zibo Zhou", "Sheng Yin", "Yueru Chen", "Qingyao Xu", "Weixin Li", "Yunhong Wang", "Bowei Tang", "Wei Jing", "Siheng Chen"], "title": "PolySim: Bridging the Sim-to-Real Gap for Humanoid Control via Multi-Simulator Dynamics Randomization", "comment": "8 pages, 5 figures", "summary": "Humanoid whole-body control (WBC) policies trained in simulation often suffer\nfrom the sim-to-real gap, which fundamentally arises from simulator inductive\nbias, the inherent assumptions and limitations of any single simulator. These\nbiases lead to nontrivial discrepancies both across simulators and between\nsimulation and the real world. To mitigate the effect of simulator inductive\nbias, the key idea is to train policies jointly across multiple simulators,\nencouraging the learned controller to capture dynamics that generalize beyond\nany single simulator's assumptions. We thus introduce PolySim, a WBC training\nplatform that integrates multiple heterogeneous simulators. PolySim can launch\nparallel environments from different engines simultaneously within a single\ntraining run, thereby realizing dynamics-level domain randomization.\nTheoretically, we show that PolySim yields a tighter upper bound on simulator\ninductive bias than single-simulator training. In experiments, PolySim\nsubstantially reduces motion-tracking error in sim-to-sim evaluations; for\nexample, on MuJoCo, it improves execution success by 52.8 over an IsaacSim\nbaseline. PolySim further enables zero-shot deployment on a real Unitree G1\nwithout additional fine-tuning, showing effective transfer from simulation to\nthe real world. We will release the PolySim code upon acceptance of this work.", "AI": {"tldr": "PolySim\u901a\u8fc7\u8de8\u591a\u4e2a\u6a21\u62df\u5668\u8054\u5408\u8bad\u7ec3WBC\u653f\u7b56\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4ece\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u6548\u679c\uff0c\u51cf\u5c11\u4e86\u8c03\u52a8\u8bef\u5dee\u5e76\u6539\u5584\u4e86\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u5728\u4eff\u771f\u4e2d\u8bad\u7ec3\u7684\u4eba\u5f62\u5168\u8eab\u63a7\u5236\uff08WBC\uff09\u7b56\u7565\u6240\u9762\u4e34\u7684\u4eff\u771f\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5c24\u5176\u662f\u6a21\u62df\u5668\u7684\u5f52\u7eb3\u504f\u5dee\u5e26\u6765\u7684\u95ee\u9898\u3002", "method": "\u4ecb\u7ecdPolySim\uff0c\u4e00\u4e2a\u96c6\u6210\u591a\u4e2a\u5f02\u8d28\u6a21\u62df\u5668\u7684WBC\u8bad\u7ec3\u5e73\u53f0\uff0c\u901a\u8fc7\u540c\u65f6\u542f\u52a8\u591a\u4e2a\u4e0d\u540c\u5f15\u64ce\u7684\u5e76\u884c\u73af\u5883\u5b9e\u73b0\u52a8\u6001\u7ea7\u57df\u968f\u673a\u5316\u3002", "result": "PolySim\u5728MuJoCo\u4e0a\u63d0\u5347\u4e8652.8%\u7684\u6267\u884c\u6210\u529f\u7387\uff0c\u5e76\u5927\u5e45\u51cf\u5c11\u4e86\u8fd0\u52a8\u8ddf\u8e2a\u8bef\u5dee\uff0c\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u8fc1\u79fb\u3002", "conclusion": "PolySim\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u62df\u5230\u6a21\u62df\u7684\u8fd0\u52a8\u8ddf\u8e2a\u8bef\u5dee\uff0c\u5e76\u4e14\u80fd\u591f\u5728\u6ca1\u6709\u989d\u5916\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4ece\u6a21\u62df\u5230\u73b0\u5b9e\u4e16\u754c\u7684\u6709\u6548\u8fc1\u79fb\u3002"}}
{"id": "2510.01711", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01711", "abs": "https://arxiv.org/abs/2510.01711", "authors": ["Taeyoung Kim", "Jimin Lee", "Myungkyu Koo", "Dongyoung Kim", "Kyungmin Lee", "Changyeon Kim", "Younggyo Seo", "Jinwoo Shin"], "title": "Contrastive Representation Regularization for Vision-Language-Action Models", "comment": "20 pages, 12 figures", "summary": "Vision-Language-Action (VLA) models have shown its capabilities in robot\nmanipulation by leveraging rich representations from pre-trained\nVision-Language Models (VLMs). However, their representations arguably remain\nsuboptimal, lacking sensitivity to robotic signals such as control actions and\nproprioceptive states. To address the issue, we introduce Robot State-aware\nContrastive Loss (RS-CL), a simple and effective representation regularization\nfor VLA models, designed to bridge the gap between VLM representations and\nrobotic signals. In particular, RS-CL aligns the representations more closely\nwith the robot's proprioceptive states, by using relative distances between the\nstates as soft supervision. Complementing the original action prediction\nobjective, RS-CL effectively enhances control-relevant representation learning,\nwhile being lightweight and fully compatible with standard VLA training\npipeline. Our empirical results demonstrate that RS-CL substantially improves\nthe manipulation performance of state-of-the-art VLA models; it pushes the\nprior art from 30.8% to 41.5% on pick-and-place tasks in RoboCasa-Kitchen,\nthrough more accurate positioning during grasping and placing, and boosts\nsuccess rates from 45.0% to 58.3% on challenging real-robot manipulation tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRS-CL\uff0c\u901a\u8fc7\u589e\u5f3aVLA\u6a21\u578b\u5bf9\u673a\u5668\u4eba\u72b6\u6001\u7684\u611f\u77e5\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u5bf9\u63a7\u5236\u4fe1\u53f7\u548c\u81ea\u6211\u611f\u77e5\u72b6\u6001\u654f\u611f\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684Robot State-aware Contrastive Loss (RS-CL)\uff0c\u7528\u4e8e\u589e\u5f3aVLA\u6a21\u578b\u5bf9\u673a\u5668\u4eba\u72b6\u6001\u7684\u611f\u77e5\uff0c\u7ed3\u5408\u8f6f\u76d1\u7763\u65b9\u6cd5\u6765\u4f18\u5316\u8868\u793a\u5b66\u4e60\u3002", "result": "RS-CL\u5728RoboCasa-Kitchen\u7684\u6293\u53d6\u548c\u653e\u7f6e\u4efb\u52a1\u4e0a\u5c06\u8868\u73b0\u4ece30.8%\u63d0\u5347\u81f341.5%\uff0c\u5e76\u5728\u5b9e\u9645\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u6210\u529f\u7387\u4ece45.0%\u63d0\u5347\u81f358.3%\u3002", "conclusion": "RS-CL\u663e\u8457\u63d0\u9ad8\u4e86\u5148\u8fdbVLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u6027\u80fd\u4e0a\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728RoboCasa-Kitchen\u4e2d\u7684\u6293\u53d6\u548c\u653e\u7f6e\u4efb\u52a1\u3002"}}
{"id": "2510.01761", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.01761", "abs": "https://arxiv.org/abs/2510.01761", "authors": ["Wendu Zhang", "Heng Wang", "Shuangyi Wang", "Yuanrui Huang"], "title": "Dual-Mode Magnetic Continuum Robot for Targeted Drug Delivery", "comment": "7 pages, 3 figures, under review of ICRA 2026", "summary": "Magnetic continuum robots (MCRs) enable minimally invasive navigation through\ntortuous anatomical channels, yet axially magnetized designs have largely been\nlimited to bending-only motion. To expand deformation capabilities, this paper\npresents a simple assembly that embeds permanent magnets radially within the\ncatheter wall, allowing a single externally steered permanent magnet to\nindependently induce either bending or torsion. A physics-based formulation\ntogether with finite-element analysis establishes the actuation principles, and\nbenchtop experiments validate decoupled mode control under practical fields.\nBuilding on this, a dual-layer blockage mechanism consisting of outer grooves\nand inner plates leverages torsional shear to achieve on-demand drug release.\nFinally, an in-phantom intervention experiment demonstrates end-to-end\noperation: lumen following by bending for target approach, followed by\ntwist-activated release at the site. The resulting compact, cable-free platform\ncombines versatile deformation with precise payload delivery, indicating strong\npotential for next-generation, site-specific therapies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u78c1\u6027\u8fde\u7eed\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u5d4c\u5165\u5f0f\u78c1\u94c1\u5b9e\u73b0\u4e86\u66f4\u590d\u6742\u7684\u8fd0\u52a8\u63a7\u5236\uff0c\u5e76\u6210\u529f\u6f14\u793a\u4e86\u836f\u7269\u91ca\u653e\u529f\u80fd\uff0c\u5177\u6709\u826f\u597d\u7684\u5e94\u7528\u524d\u666f\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u73b0\u6709\u7684\u8f74\u5411\u78c1\u5316\u8bbe\u8ba1\u5728\u8fd0\u52a8\u80fd\u529b\u4e0a\u7684\u9650\u5236\uff0c\u65e8\u5728\u6269\u5c55\u78c1\u6027\u8fde\u7eed\u673a\u5668\u4eba\u7684\u53d8\u5f62\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5728\u5bfc\u7ba1\u58c1\u5185\u5f84\u5411\u5d4c\u5165\u6c38\u4e45\u78c1\u94c1\uff0c\u5229\u7528\u5355\u4e2a\u5916\u90e8\u5f15\u5bfc\u7684\u6c38\u4e45\u78c1\u94c1\u72ec\u7acb\u8bf1\u5bfc\u5f2f\u66f2\u6216\u626d\u8f6c\uff0c\u4ee5\u5b9e\u73b0\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u89e3\u8026\u6a21\u5f0f\u63a7\u5236\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u5c55\u793a\u4e86\u8be5\u7cfb\u7edf\u7684\u6574\u4f53\u64cd\u4f5c\u80fd\u529b\uff0c\u4ece\u800c\u5b9e\u73b0\u9776\u5411\u4ecb\u5165\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u78c1\u6027\u8fde\u7eed\u673a\u5668\u4eba\u7cfb\u7edf\u5177\u6709\u4f18\u8d8a\u7684\u53d8\u5f62\u80fd\u529b\u548c\u7cbe\u786e\u7684\u836f\u7269\u4f20\u9012\u529f\u80fd\uff0c\u663e\u793a\u51fa\u5728\u7279\u5b9a\u6cbb\u7597\u4e2d\u5f3a\u5927\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.01770", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01770", "abs": "https://arxiv.org/abs/2510.01770", "authors": ["Christopher Leet", "Aidan Sciortino", "Sven Koenig"], "title": "An Anytime, Scalable and Complete Algorithm for Embedding a Manufacturing Procedure in a Smart Factory", "comment": null, "summary": "Modern automated factories increasingly run manufacturing procedures using a\nmatrix of programmable machines, such as 3D printers, interconnected by a\nprogrammable transport system, such as a fleet of tabletop robots. To embed a\nmanufacturing procedure into a smart factory, an operator must: (a) assign each\nof its processes to a machine and (b) specify how agents should transport parts\nbetween machines. The problem of embedding a manufacturing process into a smart\nfactory is termed the Smart Factory Embedding (SFE) problem. State-of-the-art\nSFE solvers can only scale to factories containing a couple dozen machines.\nModern smart factories, however, may contain hundreds of machines. We fill this\nhole by introducing the first highly scalable solution to the SFE, TS-ACES, the\nTraffic System based Anytime Cyclic Embedding Solver. We show that TS-ACES is\ncomplete and can scale to SFE instances based on real industrial scenarios with\nmore than a hundred machines.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848TS-ACES\uff0c\u6709\u6548\u89e3\u51b3\u667a\u80fd\u5de5\u5382\u4e2d\u5d4c\u5165\u95ee\u9898\uff0c\u652f\u6301\u8d85\u8fc7\u4e00\u767e\u53f0\u673a\u5668\u7684\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u4ee3\u667a\u80fd\u5de5\u5382\u9700\u8981\u5904\u7406\u5927\u91cf\u673a\u5668\u548c\u590d\u6742\u8fd0\u8f93\u7cfb\u7edf\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u65e0\u6cd5\u6ee1\u8db3\u89c4\u6a21\u8981\u6c42\u3002", "method": "\u63d0\u51faTS-ACES\u65b9\u6cd5\uff0c\u4f5c\u4e3a\u4e00\u79cd\u57fa\u4e8e\u4ea4\u901a\u7cfb\u7edf\u7684\u968f\u65f6\u5faa\u73af\u5d4c\u5165\u6c42\u89e3\u5668\uff0c\u89e3\u51b3\u667a\u80fd\u5de5\u5382\u5d4c\u5165\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eTS-ACES\u80fd\u591f\u5b8c\u6210\u667a\u80fd\u5de5\u5382\u5d4c\u5165\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "TS-ACES\u662f\u4e00\u79cd\u5728\u5b9e\u9645\u5de5\u4e1a\u573a\u666f\u4e2d\u80fd\u591f\u6269\u5c55\u5230\u8d85\u8fc7\u4e00\u767e\u53f0\u673a\u5668\u7684\u667a\u80fd\u5de5\u5382\u5d4c\u5165\u95ee\u9898\u7684\u9996\u4e2a\u9ad8\u53ef\u6269\u5c55\u6027\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.01795", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01795", "abs": "https://arxiv.org/abs/2510.01795", "authors": ["Haibo Hu", "Lianming Huang", "Xinyu Wang", "Yufei Cui", "Nan Guan", "Chun Jason Xue"], "title": "Nav-EE: Navigation-Guided Early Exiting for Efficient Vision-Language Models in Autonomous Driving", "comment": null, "summary": "Vision-Language Models (VLMs) are increasingly applied in autonomous driving\nfor unified perception and reasoning, but high inference latency hinders\nreal-time deployment. Early-exit reduces latency by terminating inference at\nintermediate layers, yet its task-dependent nature limits generalization across\ndiverse scenarios. We observe that this limitation aligns with autonomous\ndriving: navigation systems can anticipate upcoming contexts (e.g.,\nintersections, traffic lights), indicating which tasks will be required. We\npropose Nav-EE, a navigation-guided early-exit framework that precomputes\ntask-specific exit layers offline and dynamically applies them online based on\nnavigation priors. Experiments on CODA, Waymo, and BOSCH show that Nav-EE\nachieves accuracy comparable to full inference while reducing latency by up to\n63.9%. Real-vehicle integration with Autoware Universe further demonstrates\nreduced inference latency (600ms to 300ms), supporting faster decision-making\nin complex scenarios. These results suggest that coupling navigation foresight\nwith early-exit offers a viable path toward efficient deployment of large\nmodels in autonomous systems. Code and data are available at our anonymous\nrepository: https://anonymous.4open.science/r/Nav-EE-BBC4", "AI": {"tldr": "Nav-EE\u6846\u67b6\u901a\u8fc7\u5bfc\u822a\u9884\u89c1\u63d0\u9ad8\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u63a8\u7406\u6548\u7387\uff0c\u663e\u8457\u51cf\u5c11\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u4e2d\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b(VLMs)\u7684\u5e94\u7528\u65e5\u76ca\u589e\u957f\uff0c\u9762\u4e34\u7684\u9ad8\u63a8\u7406\u5ef6\u8fdf\u95ee\u9898\u9650\u5236\u4e86\u5b9e\u65f6\u90e8\u7f72\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5bfc\u822a\u5f15\u5bfc\u7684\u65e9\u671f\u9000\u51fa\u6846\u67b6Nav-EE\uff0c\u8be5\u6846\u67b6\u79bb\u7ebf\u9884\u8ba1\u7b97\u7279\u5b9a\u4efb\u52a1\u7684\u9000\u51fa\u5c42\uff0c\u5e76\u6839\u636e\u5bfc\u822a\u5148\u9a8c\u5728\u7ebf\u52a8\u6001\u5e94\u7528\u3002", "result": "Nav-EE\u5728CODA\u3001Waymo\u548cBOSCH\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5176\u51c6\u786e\u6027\u4e0e\u5b8c\u6574\u63a8\u7406\u76f8\u5f53\uff0c\u540c\u65f6\u5c06\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u4e86\u591a\u8fbe63.9%\u3002\u5728Autoware Universe\u4e0a\u7684\u5b9e\u8f66\u96c6\u6210\u8fdb\u4e00\u6b65\u51cf\u5c11\u63a8\u7406\u5ef6\u8fdf\uff08\u4ece600\u6beb\u79d2\u964d\u81f3300\u6beb\u79d2\uff09\uff0c\u652f\u6301\u590d\u6742\u573a\u666f\u4e2d\u7684\u5feb\u901f\u51b3\u7b56\u3002", "conclusion": "\u5c06\u5bfc\u822a\u9884\u89c1\u4e0e\u65e9\u671f\u9000\u51fa\u76f8\u7ed3\u5408\uff0c\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u4e2d\u5927\u578b\u6a21\u578b\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u884c\u7684\u8def\u5f84\u3002"}}
{"id": "2510.01830", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01830", "abs": "https://arxiv.org/abs/2510.01830", "authors": ["Hongze Wang", "Boyang Sun", "Jiaxu Xing", "Fan Yang", "Marco Hutter", "Dhruv Shah", "Davide Scaramuzza", "Marc Pollefeys"], "title": "What Matters in RL-Based Methods for Object-Goal Navigation? An Empirical Study and A Unified Framework", "comment": null, "summary": "Object-Goal Navigation (ObjectNav) is a critical component toward deploying\nmobile robots in everyday, uncontrolled environments such as homes, schools,\nand workplaces. In this context, a robot must locate target objects in\npreviously unseen environments using only its onboard perception. Success\nrequires the integration of semantic understanding, spatial reasoning, and\nlong-horizon planning, which is a combination that remains extremely\nchallenging. While reinforcement learning (RL) has become the dominant\nparadigm, progress has spanned a wide range of design choices, yet the field\nstill lacks a unifying analysis to determine which components truly drive\nperformance. In this work, we conduct a large-scale empirical study of modular\nRL-based ObjectNav systems, decomposing them into three key components:\nperception, policy, and test-time enhancement. Through extensive controlled\nexperiments, we isolate the contribution of each and uncover clear trends:\nperception quality and test-time strategies are decisive drivers of\nperformance, whereas policy improvements with current methods yield only\nmarginal gains. Building on these insights, we propose practical design\nguidelines and demonstrate an enhanced modular system that surpasses\nState-of-the-Art (SotA) methods by 6.6% on SPL and by a 2.7% success rate. We\nalso introduce a human baseline under identical conditions, where experts\nachieve an average 98% success, underscoring the gap between RL agents and\nhuman-level navigation. Our study not only sets the SotA performance but also\nprovides principled guidance for future ObjectNav development and evaluation.", "AI": {"tldr": "\u672c\u7814\u7a76\u6df1\u5165\u5206\u6790\u4e86\u6a21\u5757\u5316RL\u5728\u76ee\u6807\u5bfc\u822a\u4e2d\u7684\u8868\u73b0\uff0c\u8bc6\u522b\u51fa\u611f\u77e5\u8d28\u91cf\u548c\u6d4b\u8bd5\u7b56\u7565\u4e3a\u4e3b\u8981\u6027\u80fd\u9a71\u52a8\u56e0\u7d20\uff0c\u63d0\u51fa\u4e86\u8bbe\u8ba1\u6307\u5bfc\u5e76\u5c55\u793a\u4e86\u65b0\u7684\u7cfb\u7edf\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5728\u590d\u6742\u548c\u4e0d\u53d7\u63a7\u7684\u73af\u5883\u4e2d\u90e8\u7f72\u79fb\u52a8\u673a\u5668\u4eba\uff0c\u6210\u529f\u9700\u8981\u96c6\u6210\u8bed\u4e49\u7406\u89e3\u3001\u7a7a\u95f4\u63a8\u7406\u548c\u957f\u671f\u89c4\u5212\uff0c\u800c\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5c1a\u672a\u6e05\u6670\u8bc1\u660e\u54ea\u4e9b\u7ec4\u4ef6\u771f\u6b63\u63a8\u52a8\u6027\u80fd\u3002", "method": "\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u89e3\u4e86ObjectNav\u7cfb\u7edf\u7684\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u611f\u77e5\u3001\u7b56\u7565\u548c\u6d4b\u8bd5\u589e\u5f3a\uff0c\u91c7\u7528\u4e86\u5e7f\u6cdb\u7684\u63a7\u5236\u5b9e\u9a8c\u6765\u9694\u79bb\u6bcf\u4e2a\u7ec4\u4ef6\u7684\u8d21\u732e\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7684\u6a21\u5757\u5316\u7cfb\u7edf\uff0c\u5728SPL\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\uff08SotA\uff09\u65b9\u6cd56.6%\u548c2.7%\u7684\u6210\u529f\u7387\uff0c\u800c\u4e13\u5bb6\u5728\u76f8\u540c\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e8698%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u7a81\u663e\u4e86RL\u4ee3\u7406\u4e0e\u4eba\u7c7b\u6c34\u5e73\u5bfc\u822a\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u901a\u8fc7\u5bf9\u6a21\u5757\u5316\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u57fa\u4e8e\u76ee\u6807\u5bfc\u822a\u7cfb\u7edf\u7684\u6df1\u5165\u5206\u6790\uff0c\u63d0\u9ad8\u4e86\u6027\u80fd\u5e76\u4e3a\u672a\u6765\u7684ObjectNav\u53d1\u5c55\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2510.01843", "categories": ["cs.RO", "I.2.9; I.2.8; G.1.6"], "pdf": "https://arxiv.org/pdf/2510.01843", "abs": "https://arxiv.org/abs/2510.01843", "authors": ["Wanyue Li", "Ji Ma", "Minghao Lu", "Peng Lu"], "title": "Like Playing a Video Game: Spatial-Temporal Optimization of Foot Trajectories for Controlled Football Kicking in Bipedal Robots", "comment": "8 pages, 8 figures, conference paper", "summary": "Humanoid robot soccer presents several challenges, particularly in\nmaintaining system stability during aggressive kicking motions while achieving\nprecise ball trajectory control. Current solutions, whether traditional\nposition-based control methods or reinforcement learning (RL) approaches,\nexhibit significant limitations. Model predictive control (MPC) is a prevalent\napproach for ordinary quadruped and biped robots. While MPC has demonstrated\nadvantages in legged robots, existing studies often oversimplify the leg swing\nprogress, relying merely on simple trajectory interpolation methods. This\nseverely constrains the foot's environmental interaction capability, hindering\ntasks such as ball kicking. This study innovatively adapts the spatial-temporal\ntrajectory planning method, which has been successful in drone applications, to\nbipedal robotic systems. The proposed approach autonomously generates foot\ntrajectories that satisfy constraints on target kicking position, velocity, and\nacceleration while simultaneously optimizing swing phase duration. Experimental\nresults demonstrate that the optimized trajectories closely mimic human kicking\nbehavior, featuring a backswing motion. Simulation and hardware experiments\nconfirm the algorithm's efficiency, with trajectory planning times under 1 ms,\nand its reliability, achieving nearly 100 % task completion accuracy when the\nsoccer goal is within the range of -90{\\deg} to 90{\\deg}.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5e94\u7528\u7a7a\u95f4-\u65f6\u95f4\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\u4e8e\u53cc\u8db3\u673a\u5668\u4eba\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8e22\u7403\u8f68\u8ff9\u751f\u6210\uff0c\u4f18\u5316\u4e86\u7a33\u5b9a\u6027\u548c\u7cbe\u786e\u5ea6\uff0c\u4e14\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u63a5\u8fd1\u5b8c\u7f8e\u7684\u4efb\u52a1\u5b8c\u6210\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u4f20\u7edf\u63a7\u5236\u65b9\u6cd5\u548c\u5f3a\u5316\u5b66\u4e60\u5728\u89e3\u51b3\u53cc\u8db3\u673a\u5668\u4eba\u5728\u8db3\u7403\u8fd0\u52a8\u4e2d\u9762\u4e34\u7684\u7a33\u5b9a\u6027\u548c\u7cbe\u786e\u5ea6\u95ee\u9898\u4e0a\u5b58\u5728\u91cd\u5927\u5c40\u9650\u6027\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u4e00\u79cd\u66f4\u6709\u6548\u7684\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u7a7a\u95f4-\u65f6\u95f4\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\uff0c\u751f\u6210\u7b26\u5408\u76ee\u6807\u5c04\u95e8\u4f4d\u7f6e\u3001\u901f\u5ea6\u548c\u52a0\u901f\u5ea6\u7ea6\u675f\u7684\u811a\u90e8\u8fd0\u52a8\u8f68\u8ff9\uff0c\u540c\u65f6\u4f18\u5316\u6446\u52a8\u9636\u6bb5\u7684\u6301\u7eed\u65f6\u95f4\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u4f18\u5316\u7684\u8fd0\u52a8\u8f68\u8ff9\u4e0e\u4eba\u7c7b\u8e22\u7403\u884c\u4e3a\u76f8\u8fd1\uff0c\u4e14\u5728-90\u5ea6\u523090\u5ea6\u7684\u5c04\u95e8\u8303\u56f4\u5185\uff0c\u7b97\u6cd5\u8868\u73b0\u51fa\u9ad8\u5ea6\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u521b\u65b0\u6027\u5730\u5e94\u7528\u7a7a\u95f4-\u65f6\u95f4\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\u4e8e\u53cc\u8db3\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u4f18\u5316\u7684\u811a\u90e8\u8fd0\u52a8\u8f68\u8ff9\uff0c\u80fd\u591f\u9ad8\u6548\u3001\u7cbe\u51c6\u5730\u5b8c\u6210\u5c04\u95e8\u4efb\u52a1\uff0c\u4e14\u8f68\u8ff9\u89c4\u5212\u65f6\u95f4\u5c0f\u4e8e1\u6beb\u79d2\uff0c\u4efb\u52a1\u5b8c\u6210\u7387\u63a5\u8fd1100%\u3002"}}
{"id": "2510.01848", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01848", "abs": "https://arxiv.org/abs/2510.01848", "authors": ["Diram Tabaa", "Gianni Di Caro"], "title": "GreenhouseSplat: A Dataset of Photorealistic Greenhouse Simulations for Mobile Robotics", "comment": null, "summary": "Simulating greenhouse environments is critical for developing and evaluating\nrobotic systems for agriculture, yet existing approaches rely on simplistic or\nsynthetic assets that limit simulation-to-real transfer. Recent advances in\nradiance field methods, such as Gaussian splatting, enable photorealistic\nreconstruction but have so far been restricted to individual plants or\ncontrolled laboratory conditions. In this work, we introduce GreenhouseSplat, a\nframework and dataset for generating photorealistic greenhouse assets directly\nfrom inexpensive RGB images. The resulting assets are integrated into a\nROS-based simulation with support for camera and LiDAR rendering, enabling\ntasks such as localization with fiducial markers. We provide a dataset of 82\ncucumber plants across multiple row configurations and demonstrate its utility\nfor robotics evaluation. GreenhouseSplat represents the first step toward\ngreenhouse-scale radiance-field simulation and offers a foundation for future\nresearch in agricultural robotics.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a8\u51fa\u4e86GreenhouseSplat\u6846\u67b6\uff0c\u5229\u7528RGB\u56fe\u50cf\u751f\u6210\u771f\u5b9e\u611f\u6e29\u5ba4\u8d44\u4ea7\uff0c\u63a8\u52a8\u519c\u4e1a\u673a\u5668\u4eba\u9886\u57df\u7684\u53d1\u5c55\u3002", "motivation": "\u6a21\u62df\u6e29\u5ba4\u73af\u5883\u5bf9\u519c\u4e1a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5f00\u53d1\u548c\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7684\u7b80\u5316\u8d44\u4ea7\u9650\u5236\u4e86\u4eff\u771f\u4e0e\u73b0\u5b9e\u7684\u8f6c\u79fb\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86GreenhouseSplat\u6846\u67b6\uff0c\u4f7f\u7528\u4f4e\u6210\u672cRGB\u56fe\u50cf\u751f\u6210\u771f\u5b9e\u611f\u6e29\u5ba4\u8d44\u4ea7\uff0c\u5e76\u4e0eROS\u4eff\u771f\u96c6\u6210\uff0c\u652f\u6301\u76f8\u673a\u548cLiDAR\u6e32\u67d3\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b82\u4e2a\u9ec4\u74dc\u690d\u7269\u7684\u6570\u636e\u96c6\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u673a\u5668\u4eba\u8bc4\u4f30\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "GreenhouseSplat\u4e3a\u6e29\u5ba4\u89c4\u6a21\u7684\u8f90\u5c04\u573a\u4eff\u771f\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63d0\u5347\u4e86\u519c\u4e1a\u673a\u5668\u4eba\u7814\u7a76\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2510.01869", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.01869", "abs": "https://arxiv.org/abs/2510.01869", "authors": ["Alessandro Nazzari", "Roberto Rubinacci", "Marco Lovera"], "title": "TACOS: Task Agnostic COordinator of a multi-drone System", "comment": "6 pages, 6 figures, accepted as poster at 2025 IEEE International\n  Symposium on Multi-Robot & Multi-Agent Systems", "summary": "When a single pilot is responsible for managing a multi-drone system, the\ntask demands varying levels of autonomy, from direct control of individual\nUAVs, to group-level coordination, to fully autonomous swarm behaviors for\naccomplishing high-level tasks. Enabling such flexible interaction requires a\nframework that supports multiple modes of shared autonomy. As language models\ncontinue to improve in reasoning and planning, they provide a natural\nfoundation for such systems, reducing pilot workload by enabling high-level\ntask delegation through intuitive, language-based interfaces. In this paper we\npresent TACOS (Task-Agnostic COordinator of a multi-drone System), a unified\nframework that enables high-level natural language control of multi-UAV systems\nthrough Large Language Models (LLMs). TACOS integrates three key capabilities\ninto a single architecture: a one-to-many natural language interface for\nintuitive user interaction, an intelligent coordinator for translating user\nintent into structured task plans, and an autonomous agent that executes plans\ninteracting with the real-world. TACOS allows a LLM to interact with a library\nof executable APIs, bridging semantic reasoning with real-time multi-robot\ncoordination. We demonstrate the system in real-world multi-drone system and\nconduct an ablation study to assess the contribution of each module.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTACOS\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u7684\u9ad8\u5c42\u6b21\u81ea\u7136\u8bed\u8a00\u63a7\u5236\uff0c\u901a\u8fc7\u96c6\u6210\u591a\u79cd\u5173\u952e\u80fd\u529b\uff0c\u51cf\u8f7b\u4e86\u98de\u884c\u5458\u5de5\u4f5c\u8d1f\u62c5\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u7cfb\u7edf\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u5355\u4e00\u98de\u884c\u5458\u7ba1\u7406\u591a\u65e0\u4eba\u673a\u7684\u4efb\u52a1\u8981\u6c42\u66f4\u9ad8\u7684\u81ea\u4e3b\u6027\u548c\u7075\u6d3b\u7684\u4ea4\u4e92\u6a21\u5f0f\u3002", "method": "\u63d0\u51fa\u4e86TACOS\u6846\u67b6\uff0c\u5b9e\u73b0\u9ad8\u5c42\u6b21\u81ea\u7136\u8bed\u8a00\u63a7\u5236\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\uff0c\u5e76\u96c6\u6210\u81ea\u7136\u8bed\u8a00\u63a5\u53e3\u3001\u667a\u80fd\u534f\u8c03\u5668\u548c\u81ea\u4e3b\u4ee3\u7406\u3002", "result": "\u5728\u771f\u5b9e\u7684\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u4e2d\u6f14\u793a\u4e86TACOS\uff0c\u8fdb\u884c\u4e86\u5404\u6a21\u5757\u8d21\u732e\u7684\u6d88\u878d\u7814\u7a76\u3002", "conclusion": "TACOS\u6846\u67b6\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63a5\u53e3\u548c\u667a\u80fd\u534f\u8c03\u529f\u80fd\u6709\u6548\u63d0\u5347\u4e86\u5355\u4e00\u98de\u884c\u5458\u5bf9\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u7684\u63a7\u5236\u80fd\u529b\uff0c\u652f\u6301\u9ad8\u5c42\u6b21\u7684\u4efb\u52a1\u5206\u914d\u4e0e\u6267\u884c\u3002"}}
{"id": "2510.01984", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.01984", "abs": "https://arxiv.org/abs/2510.01984", "authors": ["Yue Wang"], "title": "SPARC: Spine with Prismatic and Revolute Compliance for Quadruped Robot", "comment": null, "summary": "We present SPARC, a compact, open-source 3-DoF sagittal-plane spine module\nthat combines revolute (pitch) and prismatic (axial) motion with programmable\ntask-space impedance for quadruped robots. The system integrates three\ntorque-controlled actuators, a custom 1 kHz control board, and a protected\npower unit in a 1.26 kg package, enabling closed-loop stiffness and damping\nshaping along x, z, and theta. We develop an RNEA-based computed-acceleration\ncontroller with smooth Stribeck friction compensation to render spring-damper\nbehavior without explicit inertia shaping. Bench experiments validate the\napproach. Quasi-static push-pull tests show linear force-displacement\ncharacteristics with commanded horizontal stiffness spanning 300-700 N/m and <=\n1.5% relative error (R^2 >= 0.992, narrow 95% CIs). Dynamic\ndisplace-and-release trials confirm mass-spring-damper responses over multiple\ndamping settings, with small, interpretable phase deviations due to\nconfiguration-dependent inertia and low-speed friction effects. A task-space PD\ncontroller produces roughly linear stiffness but with greater variability and\ncoupling sensitivity. SPARC provides a portable platform for systematic studies\nof spine compliance in legged locomotion and will be released with complete\nhardware and firmware resources.", "AI": {"tldr": "SPARC\u662f\u4e00\u4e2a\u7d27\u51d1\u7684\u810a\u67f1\u6a21\u5757\uff0c\u5177\u5907\u4f18\u79c0\u7684\u53ef\u7f16\u7a0b\u963b\u6297\u63a7\u5236\uff0c\u5df2\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6027\u80fd\uff0c\u9002\u5408\u56db\u8db3\u673a\u5668\u4eba\u4f7f\u7528\u3002", "motivation": "\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u7d27\u51d1\u7684\u3001\u5f00\u653e\u6e90\u7801\u7684\u810a\u67f1\u6a21\u5757\uff0c\u4ee5\u4fbf\u5728\u56db\u8db3\u673a\u5668\u4eba\u4e2d\u5b9e\u73b0\u6709\u6548\u7684\u8fd0\u52a8\u63a7\u5236\u548c\u9002\u5e94\u6027\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eRNEA\u7684\u8ba1\u7b97\u52a0\u901f\u5ea6\u63a7\u5236\u5668\u5e76\u5b9e\u73b0\u5e73\u6ed1\u7684Stribeck\u6469\u64e6\u8865\u507f\uff0c\u9a8c\u8bc1\u4e86\u963b\u5c3c\u548c\u521a\u5ea6\u8c03\u8282\u7684\u6548\u679c\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1SPARC\u7684\u7ebf\u6027\u521a\u5ea6\u548c\u8d28\u91cf-\u5f39\u7c27-\u963b\u5c3c\u5668\u54cd\u5e94\uff0c\u6027\u80fd\u6307\u6807\u826f\u597d\uff0c\u80fd\u591f\u652f\u6301\u5404\u79cd\u5b9e\u9a8c\u7814\u7a76\u3002", "conclusion": "SPARC\u7cfb\u7edf\u4e3a\u56db\u8db3\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u7f16\u7a0b\u7684\u4efb\u52a1\u7a7a\u95f4\u963b\u6297\u5e73\u53f0\uff0c\u80fd\u591f\u6709\u6548\u7814\u7a76\u810a\u67f1\u987a\u5e94\u6027\uff0c\u5e76\u5177\u5907\u826f\u597d\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u3002"}}
{"id": "2510.02080", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02080", "abs": "https://arxiv.org/abs/2510.02080", "authors": ["Lingxiang Hu", "Naima Ait Oufroukh", "Fabien Bonardi", "Raymond Ghandour"], "title": "EC3R-SLAM: Efficient and Consistent Monocular Dense SLAM with Feed-Forward 3D Reconstruction", "comment": null, "summary": "The application of monocular dense Simultaneous Localization and Mapping\n(SLAM) is often hindered by high latency, large GPU memory consumption, and\nreliance on camera calibration. To relax this constraint, we propose EC3R-SLAM,\na novel calibration-free monocular dense SLAM framework that jointly achieves\nhigh localization and mapping accuracy, low latency, and low GPU memory\nconsumption. This enables the framework to achieve efficiency through the\ncoupling of a tracking module, which maintains a sparse map of feature points,\nand a mapping module based on a feed-forward 3D reconstruction model that\nsimultaneously estimates camera intrinsics. In addition, both local and global\nloop closures are incorporated to ensure mid-term and long-term data\nassociation, enforcing multi-view consistency and thereby enhancing the overall\naccuracy and robustness of the system. Experiments across multiple benchmarks\nshow that EC3R-SLAM achieves competitive performance compared to\nstate-of-the-art methods, while being faster and more memory-efficient.\nMoreover, it runs effectively even on resource-constrained platforms such as\nlaptops and Jetson Orin NX, highlighting its potential for real-world robotics\napplications.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faEC3R-SLAM\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u5355\u76ee\u5bc6\u96c6SLAM\u6846\u67b6\uff0c\u65e0\u9700\u76f8\u673a\u6807\u5b9a\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u5b9a\u4f4d\u4e0e\u6620\u5c04\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u5e73\u53f0\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u76ee\u5bc6\u96c6SLAM\u53d7\u9650\u4e8e\u9ad8\u5ef6\u8fdf\u3001\u5927GPU\u5185\u5b58\u5360\u7528\u548c\u76f8\u673a\u6807\u5b9a\u7684\u4f9d\u8d56\uff0c\u8feb\u5207\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u7a00\u758f\u7279\u5f81\u70b9\u7684\u8ddf\u8e2a\u6a21\u5757\u548c\u524d\u99883D\u91cd\u5efa\u6a21\u578b\u7684\u6620\u5c04\u6a21\u5757\uff0cEC3R-SLAM\u5b9e\u73b0\u4e86\u9ad8\u5b9a\u4f4d\u548c\u6620\u5c04\u7cbe\u5ea6\uff0c\u540c\u65f6\u4f30\u8ba1\u76f8\u673a\u5185\u53c2\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cEC3R-SLAM\u5728\u63d0\u4f9b\u7ade\u4e89\u6027\u80fd\u7684\u540c\u65f6\uff0c\u6bd4\u6700\u65b0\u65b9\u6cd5\u66f4\u5feb\u3001\u66f4\u8282\u7701\u5185\u5b58\u3002", "conclusion": "EC3R-SLAM\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u6548\u7387\u9ad8\u3001\u4f4e\u5ef6\u8fdf\u548c\u4f4eGPU\u5185\u5b58\u6d88\u8017\u7684\u7279\u70b9\uff0c\u975e\u5e38\u9002\u5408\u5b9e\u9645\u673a\u5668\u4eba\u5e94\u7528\u3002"}}
{"id": "2510.02104", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02104", "abs": "https://arxiv.org/abs/2510.02104", "authors": ["Yunhan Lin", "Wenqi Wu", "Zhijie Zhang", "Huasong Min"], "title": "LangGrasp: Leveraging Fine-Tuned LLMs for Language Interactive Robot Grasping with Ambiguous Instructions", "comment": "8 pages, 6 figures", "summary": "The existing language-driven grasping methods struggle to fully handle\nambiguous instructions containing implicit intents. To tackle this challenge,\nwe propose LangGrasp, a novel language-interactive robotic grasping framework.\nThe framework integrates fine-tuned large language models (LLMs) to leverage\ntheir robust commonsense understanding and environmental perception\ncapabilities, thereby deducing implicit intents from linguistic instructions\nand clarifying task requirements along with target manipulation objects.\nFurthermore, our designed point cloud localization module, guided by 2D part\nsegmentation, enables partial point cloud localization in scenes, thereby\nextending grasping operations from coarse-grained object-level to fine-grained\npart-level manipulation. Experimental results show that the LangGrasp framework\naccurately resolves implicit intents in ambiguous instructions, identifying\ncritical operations and target information that are unstated yet essential for\ntask completion. Additionally, it dynamically selects optimal grasping poses by\nintegrating environmental information. This enables high-precision grasping\nfrom object-level to part-level manipulation, significantly enhancing the\nadaptability and task execution efficiency of robots in unstructured\nenvironments. More information and code are available here:\nhttps://github.com/wu467/LangGrasp.", "AI": {"tldr": "LangGrasp\u662f\u4e00\u4e2a\u65b0\u7684\u8bed\u8a00\u4e92\u52a8\u673a\u5668\u4eba\u6293\u53d6\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u70b9\u4e91\u5b9a\u4f4d\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u5904\u7406\u6a21\u7cca\u6307\u4ee4\u65f6\u7684\u6293\u53d6\u7cbe\u5ea6\u4e0e\u4efb\u52a1\u6267\u884c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u8bed\u8a00\u9a71\u52a8\u6293\u53d6\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u542b\u6709\u9690\u6027\u610f\u56fe\u7684\u6a21\u7cca\u6307\u4ee4\uff0c\u4e9f\u9700\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u63d0\u9ad8\u673a\u5668\u4eba\u6293\u53d6\u80fd\u529b\u3002", "method": "\u8be5\u6846\u67b6\u6574\u5408\u4e86\u5fae\u8c03\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u70b9\u4e91\u5b9a\u4f4d\u6a21\u5757\uff0c\u901a\u8fc72D\u90e8\u4ef6\u5206\u5272\u5b9e\u73b0\u573a\u666f\u4e2d\u7684\u5c40\u90e8\u70b9\u4e91\u5b9a\u4f4d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLangGrasp\u80fd\u591f\u51c6\u786e\u8bc6\u522b\u6a21\u7cca\u6307\u4ee4\u4e2d\u7684\u9690\u6027\u610f\u56fe\uff0c\u5e76\u52a8\u6001\u9009\u62e9\u6700\u4f18\u6293\u53d6\u59ff\u6001\u3002", "conclusion": "LangGrasp\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u542b\u6709\u9690\u6027\u610f\u56fe\u7684\u6a21\u7cca\u6307\u4ee4\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u6293\u53d6\u7cbe\u5ea6\u548c\u4efb\u52a1\u6267\u884c\u6548\u7387\u3002"}}
{"id": "2510.02129", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02129", "abs": "https://arxiv.org/abs/2510.02129", "authors": ["Philip Reichenberg", "Tim Laue"], "title": "Stand Up, NAO! Increasing the Reliability of Stand-Up Motions Through Error Compensation in Position Control", "comment": null, "summary": "Stand-up motions are an indispensable part of humanoid robot soccer. A robot\nincapable of standing up by itself is removed from the game for some time. In\nthis paper, we present our stand-up motions for the NAO robot. Our approach\ndates back to 2019 and has been evaluated and slightly expanded over the past\nsix years. We claim that the main reason for failed stand-up attempts are large\nerrors in the executed joint positions. By addressing such problems by either\nexecuting special motions to free up stuck limbs such as the arms, or by\ncompensating large errors with other joints, we significantly increased the\noverall success rate of our stand-up routine. The motions presented in this\npaper are also used by several other teams in the Standard Platform League,\nwhich thereby achieve similar success rates, as shown in an analysis of videos\nfrom multiple tournaments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7684NAO\u673a\u5668\u4eba\u7ad9\u7acb\u52a8\u4f5c\u901a\u8fc7\u89e3\u51b3\u5173\u8282\u4f4d\u7f6e\u8bef\u5dee\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7ad9\u7acb\u6210\u529f\u7387\uff0c\u5e76\u88ab\u591a\u4e2a\u56e2\u961f\u91c7\u7528\u3002", "motivation": "\u786e\u4fdd\u673a\u5668\u4eba\u80fd\u591f\u72ec\u7acb\u7ad9\u7acb\uff0c\u4ee5\u63d0\u9ad8\u5728\u8db3\u7403\u6bd4\u8d5b\u4e2d\u7684\u8868\u73b0\uff0c\u907f\u514d\u56e0\u65e0\u6cd5\u7ad9\u7acb\u800c\u88ab\u6682\u505c\u3002", "method": "\u9488\u5bf9\u673a\u5668\u4eba\u5173\u8282\u4f4d\u7f6e\u7684\u504f\u5dee\u95ee\u9898\uff0c\u91c7\u7528\u7279\u6b8a\u52a8\u4f5c\u91ca\u653e\u5361\u4f4f\u7684\u80a2\u4f53\uff0c\u5e76\u7528\u5176\u4ed6\u5173\u8282\u8fdb\u884c\u8865\u507f\u3002", "result": "\u901a\u8fc7\u4f18\u5316\u7ad9\u7acb\u52a8\u4f5c\uff0c\u6210\u529f\u7387\u663e\u8457\u63d0\u5347\uff0c\u591a\u4e2a\u56e2\u961f\u5728\u6bd4\u8d5b\u4e2d\u4e5f\u8fbe\u5230\u4e86\u7c7b\u4f3c\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u6211\u4eec\u7684\u7ad9\u7acb\u52a8\u4f5c\u663e\u8457\u63d0\u9ad8\u4e86NAO\u673a\u5668\u4eba\u5728\u6bd4\u8d5b\u4e2d\u7684\u6210\u529f\u7387\uff0c\u5e76\u5f97\u5230\u591a\u4e2a\u56e2\u961f\u7684\u5e94\u7528\u548c\u9a8c\u8bc1\u3002"}}
{"id": "2510.02164", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02164", "abs": "https://arxiv.org/abs/2510.02164", "authors": ["Nathaniel Hanson", "Austin Allison", "Charles DiMarzio", "Ta\u015fk\u0131n Pad\u0131r", "Kristen L. Dorsey"], "title": "SCANS: A Soft Gripper with Curvature and Spectroscopy Sensors for In-Hand Material Differentiation", "comment": "Accepted to IEEE Robotics & Automation Letters Special Issue on\n  Interdisciplinarity and Widening Horizons in Soft Robotics", "summary": "We introduce the soft curvature and spectroscopy (SCANS) system: a versatile,\nelectronics-free, fluidically actuated soft manipulator capable of assessing\nthe spectral properties of objects either in hand or through pre-touch caging.\nThis platform offers a wider spectral sensing capability than previous soft\nrobotic counterparts. We perform a material analysis to explore optimal soft\nsubstrates for spectral sensing, and evaluate both pre-touch and in-hand\nperformance. Experiments demonstrate explainable, statistical separation across\ndiverse object classes and sizes (metal, wood, plastic, organic, paper, foam),\nwith large spectral angle differences between items. Through linear\ndiscriminant analysis, we show that sensitivity in the near-infrared\nwavelengths is critical to distinguishing visually similar objects. These\ncapabilities advance the potential of optics as a multi-functional sensory\nmodality for soft robots. The complete parts list, assembly guidelines, and\nprocessing code for the SCANS gripper are accessible at:\nhttps://parses-lab.github.io/scans/.", "AI": {"tldr": "SCANS\u7cfb\u7edf\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u8f6f\u64cd\u63a7\u5668\uff0c\u80fd\u591f\u5728\u65e0\u7535\u5b50\u9a71\u52a8\u7684\u60c5\u51b5\u4e0b\uff0c\u4e3a\u8f6f\u673a\u5668\u4eba\u63d0\u4f9b\u5148\u8fdb\u7684\u5149\u8c31\u611f\u77e5\u80fd\u529b\uff0c\u5e2e\u52a9\u533a\u5206\u591a\u79cd\u6750\u6599\u3002", "motivation": "\u73b0\u6709\u8f6f\u673a\u5668\u4eba\u5728\u5149\u8c31\u611f\u77e5\u65b9\u9762\u7684\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u63a2\u7d22\u66f4\u5e7f\u6cdb\u7684\u611f\u77e5\u80fd\u529b\u4ee5\u9002\u5e94\u591a\u6837\u5316\u7684\u6750\u6599\u5206\u7c7b\u3002", "method": "\u8f6f\u64cd\u63a7\u5668\u901a\u8fc7\u6d41\u4f53\u9a71\u52a8\u5b9e\u73b0\uff0c\u65e0\u9700\u7535\u5b50\u90e8\u4ef6\uff0c\u91c7\u7528\u7ebf\u6027\u5224\u522b\u5206\u6790\u8bc4\u4f30\u7269\u4f53\u7684\u5149\u8c31\u7279\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793aSCANS\u7cfb\u7edf\u80fd\u591f\u5bf9\u91d1\u5c5e\u3001\u6728\u6750\u3001\u5851\u6599\u3001\u6709\u673a\u7269\u3001\u7eb8\u5f20\u548c\u6ce1\u6cab\u7b49\u4e0d\u540c\u7269\u4f53\u8fdb\u884c\u6709\u6548\u7684\u5149\u8c31\u5206\u6790\uff0c\u5c24\u5176\u5728\u8fd1\u7ea2\u5916\u6ce2\u957f\u4e0b\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u654f\u611f\u6027\u3002", "conclusion": "SCANS\u7cfb\u7edf\u63d0\u9ad8\u4e86\u8f6f\u673a\u5668\u4eba\u5728\u5149\u5b66\u4f20\u611f\u65b9\u9762\u7684\u591a\u529f\u80fd\u6027\uff0c\u80fd\u591f\u6709\u6548\u533a\u5206\u4e0d\u540c\u6750\u6599\u7684\u7269\u54c1\u3002"}}
{"id": "2510.02167", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.02167", "abs": "https://arxiv.org/abs/2510.02167", "authors": ["Sara Strakosova", "Petr Novak", "Petr Kadera"], "title": "Product Digital Twin Supporting End-of-life Phase of Electric Vehicle Batteries Utilizing Product-Process-Resource Asset Network", "comment": "This work has been submitted to the IEEE for possible publication. 6\n  pages, 4 figures", "summary": "In the context of the circular economy, products in their end-of-life phase\nshould be either remanufactured or recycled. Both of these processes are\ncrucial for sustainability and environmental conservation. However,\nmanufacturers often do not support these processes enough by not sharing\nrelevant data. This paper proposes use of a digital twin technology, which is\ncapable to help optimizing the disassembly processes to reduce ecological\nimpact and enhance sustainability. The proposed approach is demonstrated\nthrough a disassembly use-case of the product digital twin of an electric\nvehicle battery. By utilizing product digital twins, challenges associated with\nthe disassembly of electric vehicle batteries can be solved flexibly and\nefficiently for various battery types. As a backbone for the product digital\ntwin representation, the paper uses the paradigm of product-process-resource\nasset networks (PAN). Such networks enable to model relevant relationships\nacross products, production resources, manufacturing processes, and specific\nproduction operations that have to be done in the manufacturing phase of a\nproduct. This paper introduces a Bi-Flow Product-Process-Resource Asset Network\n(Bi-PAN) representation, which extends the PAN paradigm to cover not only the\nmanufacturing, but also the remanufacturing/recycling phase.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5229\u7528\u6570\u5b57\u53cc\u80de\u80ce\u6280\u672f\u548c\u53cc\u6d41\u4ea7\u54c1-\u8fc7\u7a0b-\u8d44\u6e90\u8d44\u4ea7\u7f51\u7edc\uff08Bi-PAN\uff09\u6a21\u578b\u4f18\u5316\u7535\u52a8\u8f66\u7535\u6c60\u7684\u62c6\u89e3\u8fc7\u7a0b\uff0c\u4ee5\u4fc3\u8fdb\u5faa\u73af\u7ecf\u6d4e\u4e2d\u7684\u518d\u5236\u9020\u548c\u56de\u6536\u3002", "motivation": "\u5728\u5faa\u73af\u7ecf\u6d4e\u80cc\u666f\u4e0b\uff0c\u4ea7\u54c1\u751f\u547d\u5468\u671f\u7ed3\u675f\u65f6\u5e94\u5173\u6ce8\u518d\u5236\u9020\u6216\u56de\u6536\uff0c\u4f46\u751f\u4ea7\u5546\u5bf9\u4e8e\u76f8\u5173\u6570\u636e\u7684\u5206\u4eab\u4e0d\u8db3\uff0c\u5f71\u54cd\u4e86\u8fd9\u4e9b\u8fc7\u7a0b\u7684\u5b9e\u65bd\u3002", "method": "\u901a\u8fc7\u4ea7\u54c1\u6570\u5b57\u53cc\u80de\u80ce\u6280\u672f\uff0c\u7ed3\u5408\u53cc\u6d41\u4ea7\u54c1-\u8fc7\u7a0b-\u8d44\u6e90\u8d44\u4ea7\u7f51\u7edc\uff08Bi-PAN\uff09\u6a21\u578b\uff0c\u5bf9\u7535\u52a8\u8f66\u7535\u6c60\u5b9e\u65bd\u62c6\u89e3\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u901a\u8fc7\u5e94\u7528\u4ea7\u54c1\u6570\u5b57\u53cc\u80de\u80ce\uff0c\u80fd\u591f\u7075\u6d3b\u4e14\u9ad8\u6548\u5730\u89e3\u51b3\u7535\u52a8\u8f66\u7535\u6c60\u7684\u62c6\u89e3\u6311\u6218\uff0c\u5e76\u652f\u6301\u66f4\u53ef\u6301\u7eed\u7684\u73af\u5883\u7ba1\u7406\u3002", "conclusion": "\u63d0\u51fa\u7684\u53cc\u6d41\u4ea7\u54c1-\u8fc7\u7a0b-\u8d44\u6e90\u8d44\u4ea7\u7f51\u7edc\uff08Bi-PAN\uff09\u6a21\u578b\u53ef\u4ee5\u4f18\u5316\u7535\u52a8\u8f66\u7535\u6c60\u7684\u62c6\u89e3\u8fc7\u7a0b\uff0c\u4ece\u800c\u652f\u6301\u5faa\u73af\u7ecf\u6d4e\u76f8\u5173\u7684\u518d\u5236\u9020\u548c\u56de\u6536\u8fc7\u7a0b\u3002"}}
{"id": "2510.02178", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02178", "abs": "https://arxiv.org/abs/2510.02178", "authors": ["Jialin Gao", "Donghao Zhou", "Mingjian Liang", "Lihao Liu", "Chi-Wing Fu", "Xiaowei Hu", "Pheng-Ann Heng"], "title": "DisCo-Layout: Disentangling and Coordinating Semantic and Physical Refinement in a Multi-Agent Framework for 3D Indoor Layout Synthesis", "comment": null, "summary": "3D indoor layout synthesis is crucial for creating virtual environments.\nTraditional methods struggle with generalization due to fixed datasets. While\nrecent LLM and VLM-based approaches offer improved semantic richness, they\noften lack robust and flexible refinement, resulting in suboptimal layouts. We\ndevelop DisCo-Layout, a novel framework that disentangles and coordinates\nphysical and semantic refinement. For independent refinement, our Semantic\nRefinement Tool (SRT) corrects abstract object relationships, while the\nPhysical Refinement Tool (PRT) resolves concrete spatial issues via a\ngrid-matching algorithm. For collaborative refinement, a multi-agent framework\nintelligently orchestrates these tools, featuring a planner for placement\nrules, a designer for initial layouts, and an evaluator for assessment.\nExperiments demonstrate DisCo-Layout's state-of-the-art performance, generating\nrealistic, coherent, and generalizable 3D indoor layouts. Our code will be\npublicly available.", "AI": {"tldr": "DisCo-Layout\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u548c\u7269\u7406\u4f18\u5316\u5de5\u5177\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u76843D\u5ba4\u5185\u5e03\u5c40\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u6570\u636e\u96c6\u56fa\u5b9a\u4e0b\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u4ee5\u53caLLM\u548cVLM\u65b9\u6cd5\u5728\u5e03\u5c40\u4f18\u5316\u4e2d\u7684\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u8bed\u4e49\u548c\u7269\u7406\u4e24\u4e2a\u72ec\u7acb\u7684\u5de5\u5177\uff08SRT\u548cPRT\uff09\uff0c\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u6846\u67b6\u8fdb\u884c\u534f\u540c\u4f18\u5316\u3002", "result": "DisCo-Layout\u80fd\u591f\u6709\u6548\u4fee\u6b63\u62bd\u8c61\u5bf9\u8c61\u5173\u7cfb\u548c\u5177\u4f53\u7a7a\u95f4\u95ee\u9898\uff0c\u8868\u73b0\u51fa\u524d\u6cbf\u7684\u6027\u80fd\u3002", "conclusion": "DisCo-Layout\u5c55\u793a\u4e86\u5728\u751f\u6210\u771f\u5b9e\u3001\u8fde\u8d2f\u548c\u53ef\u6cdb\u5316\u76843D\u5ba4\u5185\u5e03\u5c40\u65b9\u9762\u7684\u5353\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.02248", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02248", "abs": "https://arxiv.org/abs/2510.02248", "authors": ["Yan Miao", "Ege Yuceel", "Georgios Fainekos", "Bardh Hoxha", "Hideki Okamoto", "Sayan Mitra"], "title": "Performance-Guided Refinement for Visual Aerial Navigation using Editable Gaussian Splatting in FalconGym 2.0", "comment": null, "summary": "Visual policy design is crucial for aerial navigation. However,\nstate-of-the-art visual policies often overfit to a single track and their\nperformance degrades when track geometry changes. We develop FalconGym 2.0, a\nphotorealistic simulation framework built on Gaussian Splatting (GSplat) with\nan Edit API that programmatically generates diverse static and dynamic tracks\nin milliseconds. Leveraging FalconGym 2.0's editability, we propose a\nPerformance-Guided Refinement (PGR) algorithm, which concentrates visual\npolicy's training on challenging tracks while iteratively improving its\nperformance. Across two case studies (fixed-wing UAVs and quadrotors) with\ndistinct dynamics and environments, we show that a single visual policy trained\nwith PGR in FalconGym 2.0 outperforms state-of-the-art baselines in\ngeneralization and robustness: it generalizes to three unseen tracks with 100%\nsuccess without per-track retraining and maintains higher success rates under\ngate-pose perturbations. Finally, we demonstrate that the visual policy trained\nwith PGR in FalconGym 2.0 can be zero-shot sim-to-real transferred to a\nquadrotor hardware, achieving a 98.6% success rate (69 / 70 gates) over 30\ntrials spanning two three-gate tracks and a moving-gate track.", "AI": {"tldr": "\u5f00\u53d1\u4e86FalconGym 2.0\u4eff\u771f\u6846\u67b6\u5e76\u63d0\u51faPGR\u7b97\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u89c6\u89c9\u7b56\u7565\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u89c6\u89c9\u7b56\u7565\u5bf9\u5355\u4e00\u8f68\u9053\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u4ee5\u53ca\u5728\u8f68\u9053\u51e0\u4f55\u53d8\u5316\u65f6\u6027\u80fd\u4e0b\u964d\u7684\u56f0\u5883\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u9ad8\u65af\u70b9\u4e91\uff08GSplat\uff09\u7684\u4eff\u771f\u6846\u67b6FalconGym 2.0\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6027\u80fd\u6307\u5bfc\u7684\u6539\u8fdb\u7b97\u6cd5\uff08PGR\uff09\uff0c\u4e13\u6ce8\u4e8e\u9ad8\u6311\u6218\u6027\u8f68\u9053\u8fdb\u884c\u89c6\u89c9\u7b56\u7565\u8bad\u7ec3\u3002", "result": "\u901a\u8fc7\u56fa\u5b9a\u7ffc\u65e0\u4eba\u673a\u548c\u56db\u65cb\u7ffc\u6848\u4f8b\u7814\u7a76\uff0cPGR\u8bad\u7ec3\u7684\u5355\u4e00\u89c6\u89c9\u7b56\u7565\u5728\u4e09\u4e2a\u672a\u89c1\u8f68\u9053\u4e0a\u5b9e\u73b0100%\u6210\u529f\u7387\uff0c\u5e76\u5728\u52a8\u6001\u6761\u4ef6\u4e0b\u663e\u793a\u51fa\u66f4\u9ad8\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u4f7f\u7528PGR\u8bad\u7ec3\u7684\u89c6\u89c9\u7b56\u7565\u5728FalconGym 2.0\u4e2d\u5b9e\u73b0\u4e86\u51fa\u8272\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u80fd\u6210\u529f\u8fc1\u79fb\u5230\u5b9e\u9645\u786c\u4ef6\u3002"}}
{"id": "2510.02252", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02252", "abs": "https://arxiv.org/abs/2510.02252", "authors": ["Joao Pedro Araujo", "Yanjie Ze", "Pei Xu", "Jiajun Wu", "C. Karen Liu"], "title": "Retargeting Matters: General Motion Retargeting for Humanoid Motion Tracking", "comment": null, "summary": "Humanoid motion tracking policies are central to building teleoperation\npipelines and hierarchical controllers, yet they face a fundamental challenge:\nthe embodiment gap between humans and humanoid robots. Current approaches\naddress this gap by retargeting human motion data to humanoid embodiments and\nthen training reinforcement learning (RL) policies to imitate these reference\ntrajectories. However, artifacts introduced during retargeting, such as foot\nsliding, self-penetration, and physically infeasible motion are often left in\nthe reference trajectories for the RL policy to correct. While prior work has\ndemonstrated motion tracking abilities, they often require extensive reward\nengineering and domain randomization to succeed. In this paper, we\nsystematically evaluate how retargeting quality affects policy performance when\nexcessive reward tuning is suppressed. To address issues that we identify with\nexisting retargeting methods, we propose a new retargeting method, General\nMotion Retargeting (GMR). We evaluate GMR alongside two open-source\nretargeters, PHC and ProtoMotions, as well as with a high-quality closed-source\ndataset from Unitree. Using BeyondMimic for policy training, we isolate\nretargeting effects without reward tuning. Our experiments on a diverse subset\nof the LAFAN1 dataset reveal that while most motions can be tracked, artifacts\nin retargeted data significantly reduce policy robustness, particularly for\ndynamic or long sequences. GMR consistently outperforms existing open-source\nmethods in both tracking performance and faithfulness to the source motion,\nachieving perceptual fidelity and policy success rates close to the\nclosed-source baseline. Website:\nhttps://jaraujo98.github.io/retargeting_matters. Code:\nhttps://github.com/YanjieZe/GMR.", "AI": {"tldr": "\u672c\u8bba\u6587\u7814\u7a76\u4eba\u7c7b\u52a8\u4f5c\u91cd\u5b9a\u5411\u5bf9\u7c7b\u4eba\u673a\u5668\u4eba\u5f3a\u5316\u5b66\u4e60\u653f\u7b56\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u7684\u4e00\u79cd\u65b0\u65b9\u6cd5GMR\u5728\u8ddf\u8e2a\u6027\u80fd\u548c\u5fe0\u5b9e\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u5176\u4ed6\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u4eba\u7c7b\u52a8\u4f5c\u6570\u636e\u91cd\u5b9a\u5411\u5230\u7c7b\u4eba\u673a\u5668\u4eba\u65f6\u5b58\u5728\u7684\u8eab\u4f53\u4f53\u73b0\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u91cd\u5b9a\u5411\u8fc7\u7a0b\u4e2d\u5f15\u5165\u7684\u4f2a\u5f71\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u63d0\u51fa\u65b0\u65b9\u6cd5General Motion Retargeting (GMR)\u6765\u6539\u5584\u8fd0\u52a8\u91cd\u5b9a\u5411\u8d28\u91cf\uff0c\u5e76\u4f7f\u7528BeyondMimic\u8fdb\u884c\u653f\u7b56\u8bad\u7ec3\uff0c\u4ece\u800c\u9694\u79bb\u91cd\u5b9a\u5411\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5927\u591a\u6570\u52a8\u4f5c\u53ef\u4ee5\u88ab\u8ddf\u8e2a\uff0c\u4f46\u91cd\u5b9a\u5411\u6570\u636e\u4e2d\u7684\u4f2a\u5f71\u4f1a\u663e\u8457\u964d\u4f4e\u7b56\u7565\u7684\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u5bf9\u4e8e\u52a8\u6001\u6216\u957f\u5e8f\u5217\u3002", "conclusion": "\u63d0\u51fa\u7684GMR\u65b9\u6cd5\u5728\u8ddf\u8e2a\u6027\u80fd\u548c\u5fe0\u5b9e\u5ea6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u5f00\u6e90\u65b9\u6cd5\uff0c\u8fbe\u5230\u4e86\u4e0e\u95ed\u6e90\u57fa\u51c6\u76f8\u8fd1\u7684\u611f\u77e5\u4fdd\u771f\u5ea6\u548c\u7b56\u7565\u6210\u529f\u7387\u3002"}}
{"id": "2510.02268", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02268", "abs": "https://arxiv.org/abs/2510.02268", "authors": ["Tianchong Jiang", "Jingtian Ji", "Xiangshan Tan", "Jiading Fang", "Anand Bhattad", "Vitor Guizilini", "Matthew R. Walter"], "title": "Do You Know Where Your Camera Is? View-Invariant Policy Learning with Camera Conditioning", "comment": "Code and project materials are available at\n  ripl.github.io/know_your_camera", "summary": "We study view-invariant imitation learning by explicitly conditioning\npolicies on camera extrinsics. Using Plucker embeddings of per-pixel rays, we\nshow that conditioning on extrinsics significantly improves generalization\nacross viewpoints for standard behavior cloning policies, including ACT,\nDiffusion Policy, and SmolVLA. To evaluate policy robustness under realistic\nviewpoint shifts, we introduce six manipulation tasks in RoboSuite and\nManiSkill that pair \"fixed\" and \"randomized\" scene variants, decoupling\nbackground cues from camera pose. Our analysis reveals that policies without\nextrinsics often infer camera pose using visual cues from static backgrounds in\nfixed scenes; this shortcut collapses when workspace geometry or camera\nplacement shifts. Conditioning on extrinsics restores performance and yields\nrobust RGB-only control without depth. We release the tasks, demonstrations,\nand code at https://ripl.github.io/know_your_camera/ .", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u6761\u4ef6\u5316\u76f8\u673a\u5916\u90e8\u53c2\u6570\u63d0\u9ad8\u89c6\u56fe\u4e0d\u53d8\u7684\u6a21\u4eff\u5b66\u4e60\u80fd\u529b\uff0c\u5bf9\u73b0\u6709\u884c\u4e3a\u514b\u9686\u653f\u7b56\u7684\u6cdb\u5316\u8fdb\u884c\u4e86\u5b9e\u9a8c\u8bc1\u660e\uff0c\u63d0\u4f9b\u4e86\u65b0\u4efb\u52a1\u548c\u4ee3\u7801\u7528\u4e8e\u793e\u533a\u4f7f\u7528\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u653f\u7b56\u5728\u4e0d\u540c\u89c6\u89d2\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u771f\u5b9e\u89c6\u89d2\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u4f7f\u7528Plucker\u5d4c\u5165\u5bf9\u6bcf\u4e2a\u50cf\u7d20\u7684\u5149\u7ebf\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u5728\u6807\u51c6\u884c\u4e3a\u514b\u9686\u653f\u7b56\u4e2d\uff0c\u5305\u62ecACT\u3001Diffusion Policy\u548cSmolVLA\uff0c\u6761\u4ef6\u5316\u5916\u90e8\u53c2\u6570\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\uff0c\u6211\u4eec\u53d1\u73b0\u6ca1\u6709\u5916\u90e8\u53c2\u6570\u7684\u653f\u7b56\u4f9d\u8d56\u9759\u6001\u80cc\u666f\u7684\u89c6\u89c9\u7ebf\u7d22\uff0c\u5bfc\u81f4\u5728\u5de5\u4f5c\u533a\u51e0\u4f55\u5f62\u72b6\u6216\u76f8\u673a\u4f4d\u7f6e\u53d8\u5316\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u800c\u6761\u4ef6\u5316\u5916\u90e8\u53c2\u6570\u53ef\u4ee5\u6062\u590d\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u5730\u5bf9\u653f\u7b56\u8fdb\u884c\u76f8\u673a\u5916\u90e8\u6761\u4ef6\u5316\uff0c\u589e\u5f3a\u4e86\u89c6\u56fe\u4e0d\u53d8\u6a21\u4eff\u5b66\u4e60\u7684\u6548\u679c\u3002"}}
{"id": "2510.02298", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.02298", "abs": "https://arxiv.org/abs/2510.02298", "authors": ["Wenye Yu", "Jun Lv", "Zixi Ying", "Yang Jin", "Chuan Wen", "Cewu Lu"], "title": "ARMADA: Autonomous Online Failure Detection and Human Shared Control Empower Scalable Real-world Deployment and Adaptation", "comment": null, "summary": "Imitation learning has shown promise in learning from large-scale real-world\ndatasets. However, pretrained policies usually perform poorly without\nsufficient in-domain data. Besides, human-collected demonstrations entail\nsubstantial labour and tend to encompass mixed-quality data and redundant\ninformation. As a workaround, human-in-the-loop systems gather domain-specific\ndata for policy post-training, and exploit closed-loop policy feedback to offer\ninformative guidance, but usually require full-time human surveillance during\npolicy rollout. In this work, we devise ARMADA, a multi-robot deployment and\nadaptation system with human-in-the-loop shared control, featuring an\nautonomous online failure detection method named FLOAT. Thanks to FLOAT, ARMADA\nenables paralleled policy rollout and requests human intervention only when\nnecessary, significantly reducing reliance on human supervision. Hence, ARMADA\nenables efficient acquisition of in-domain data, and leads to more scalable\ndeployment and faster adaptation to new scenarios. We evaluate the performance\nof ARMADA on four real-world tasks. FLOAT achieves nearly 95% accuracy on\naverage, surpassing prior state-of-the-art failure detection approaches by over\n20%. Besides, ARMADA manifests more than 4$\\times$ increase in success rate and\ngreater than 2$\\times$ reduction in human intervention rate over multiple\nrounds of policy rollout and post-training, compared to previous\nhuman-in-the-loop learning methods.", "AI": {"tldr": "ARMADA\u7cfb\u7edf\u901a\u8fc7\u5728\u7ebf\u6545\u969c\u68c0\u6d4bFLOAT\uff0c\u4f18\u5316\u4e86\u591a\u673a\u5668\u4eba\u90e8\u7f72\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6210\u529f\u7387\u5e76\u964d\u4f4e\u4e86\u5bf9\u4eba\u7c7b\u76d1\u63a7\u7684\u9700\u6c42\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u7f3a\u4e4f\u8db3\u591f\u9886\u57df\u6570\u636e\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u4eba\u7c7b\u6536\u96c6\u7684\u793a\u8303\u6570\u636e\u901a\u5e38\u8d28\u91cf\u53c2\u5dee\u4e0d\u9f50\uff0c\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aARMADA\u7684\u591a\u673a\u5668\u4eba\u90e8\u7f72\u548c\u9002\u5e94\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86\u4eba\u673a\u534f\u4f5c\u5171\u4eab\u63a7\u5236\u548c\u540d\u4e3aFLOAT\u7684\u5728\u7ebf\u6545\u969c\u68c0\u6d4b\u65b9\u6cd5\u3002", "result": "ARMADA\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u5353\u8d8a\uff0cFLOAT\u7684\u6545\u969c\u68c0\u6d4b\u51c6\u786e\u7387\u63a5\u8fd195%\uff0c\u653f\u7b56\u6210\u529f\u7387\u63d0\u9ad8\u4e864\u500d\uff0c\u4eba\u7c7b\u5e72\u9884\u7387\u51cf\u5c11\u4e862\u500d\u3002", "conclusion": "ARMADA\u7cfb\u7edf\u663e\u8457\u63d0\u9ad8\u4e86\u653f\u7b56\u6210\u529f\u7387\uff0c\u5e76\u51cf\u5c11\u4e86\u5bf9\u4eba\u7c7b\u5e72\u9884\u7684\u4f9d\u8d56\uff0c\u5c55\u793a\u4e86\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
