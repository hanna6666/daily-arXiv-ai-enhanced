{"id": "2510.20039", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.20039", "abs": "https://arxiv.org/abs/2510.20039", "authors": ["Yuyang Jiang", "Longjie Guo", "Yuchen Wu", "Aylin Caliskan", "Tanu Mitra", "Hua Shen"], "title": "Beyond One-Way Influence: Bidirectional Opinion Dynamics in Multi-Turn Human-LLM Interactions", "comment": "26 pages, 8 figures", "summary": "Large language model (LLM)-powered chatbots are increasingly used for opinion\nexploration. Prior research examined how LLMs alter user views, yet little work\nextended beyond one-way influence to address how user input can affect LLM\nresponses and how such bi-directional influence manifests throughout the\nmulti-turn conversations. This study investigates this dynamic through 50\ncontroversial-topic discussions with participants (N=266) across three\nconditions: static statements, standard chatbot, and personalized chatbot.\nResults show that human opinions barely shifted, while LLM outputs changed more\nsubstantially, narrowing the gap between human and LLM stance. Personalization\namplified these shifts in both directions compared to the standard setting.\nAnalysis of multi-turn conversations further revealed that exchanges involving\nparticipants' personal stories were most likely to trigger stance changes for\nboth humans and LLMs. Our work highlights the risk of over-alignment in\nhuman-LLM interaction and the need for careful design of personalized chatbots\nto more thoughtfully and stably align with users.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u7528\u6237\u610f\u89c1\u53d8\u5316\u5fae\u4e4e\u5176\u5fae\uff0c\u800cLLM\u8f93\u51fa\u5219\u53d8\u5316\u663e\u8457\uff1b\u4e2a\u6027\u5316\u8bbe\u8ba1\u53ef\u4ee5\u589e\u5f3a\u53cc\u5411\u5f71\u54cd\uff0c\u9700\u8c28\u614e\u8bbe\u8ba1\u4ee5\u907f\u514d\u8fc7\u5ea6\u5bf9\u9f50\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u804a\u5929\u673a\u5668\u4eba\u7684\u53cc\u5411\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u7528\u6237\u8f93\u5165\u5982\u4f55\u5f71\u54cdLLM\u7684\u54cd\u5e94\uff0c\u81f3\u4eca\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u901a\u8fc7\u5bf9266\u540d\u53c2\u4e0e\u8005\u8fdb\u884c50\u573a\u6709\u4e89\u8bae\u8bdd\u9898\u7684\u591a\u8f6e\u8ba8\u8bba\uff0c\u6bd4\u8f83\u9759\u6001\u9648\u8ff0\u3001\u6807\u51c6\u804a\u5929\u673a\u5668\u4eba\u548c\u4e2a\u6027\u5316\u804a\u5929\u673a\u5668\u4eba\u7684\u6548\u679c\u3002", "result": "\u5728\u4eba\u7c7b\u89c2\u70b9\u53d8\u5316\u4e0d\u5927\u7684\u60c5\u51b5\u4e0b\uff0cLLM\u7684\u8f93\u51fa\u53d1\u751f\u4e86\u66f4\u5927\u7a0b\u5ea6\u7684\u53d8\u5316\uff0c\u4e2a\u6027\u5316\u8bbe\u7f6e\u653e\u5927\u4e86\u8fd9\u79cd\u53d8\u5316\u3002", "conclusion": "\u5728\u4e0eLLM\u4ea4\u4e92\u65f6\uff0c\u8fc7\u5ea6\u7684\u5bf9\u9f50\u5b58\u5728\u98ce\u9669\uff0c\u4e2a\u6027\u5316\u804a\u5929\u673a\u5668\u4eba\u9700\u66f4\u8c28\u614e\u8bbe\u8ba1\u4ee5\u7a33\u5b9a\u4e0e\u7528\u6237\u7684\u5bf9\u9f50\u3002"}}
{"id": "2510.20123", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.20123", "abs": "https://arxiv.org/abs/2510.20123", "authors": ["Yao Li", "Jingyi Xie", "Ya-Fang Ling", "He Zhang", "Ge Wang", "Gaojian Huang", "Rui Yu", "Si Chen"], "title": "\"Learning Together\": AI-Mediated Support for Parental Involvement in Everyday Learning", "comment": null, "summary": "Family learning takes place in everyday routines where children and\ncaregivers read, practice, and develop new skills together. Although AI is\nincreasingly present in learning environments, most systems remain\nchild-centered and overlook the collaborative, distributed nature of family\neducation. This paper investigates how AI can mediate family collaboration by\naddressing tensions of coordination, uneven workloads, and parental mediation.\nFrom a formative study with families using AI in daily learning, we identified\nchallenges in responsibility sharing and recognition of contributions. Building\non these insights, we designed FamLearn, an LLM-powered prototype that\ndistributes tasks, visualizes contributions, and provides individualized\nsupport. A one-week field study with 11 families shows how this prototype can\nease caregiving burdens, foster recognition, and enrich shared learning\nexperiences. Our findings suggest that LLMs can move beyond the role of tutor\nto act as family mediators - balancing responsibilities, scaffolding\nintergenerational participation, and strengthening the relational fabric of\nfamily learning.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c55\u793a\u4e86AI\u5982\u4f55\u901a\u8fc7FamLearn\u539f\u578b\u4fc3\u8fdb\u5bb6\u5ead\u5b66\u4e60\uff0c\u51cf\u8f7b\u7167\u62a4\u8d1f\u62c5\u5e76\u589e\u5f3a\u5171\u4eab\u5b66\u4e60\u4f53\u9a8c\u3002", "motivation": "\u63a2\u8ba8\u4eba\u5de5\u667a\u80fd\u5982\u4f55\u5728\u5bb6\u5ead\u6559\u80b2\u4e2d\u4fc3\u8fdb\u5408\u4f5c\uff0c\u89e3\u51b3\u534f\u8c03\u3001\u5de5\u4f5c\u4e0d\u5747\u548c\u5bb6\u957f\u8c03\u89e3\u7b49\u95ee\u9898", "method": "\u901a\u8fc7\u5f62\u6210\u6027\u7814\u7a76\u548c\u539f\u578b\u8bbe\u8ba1", "result": "\u8bbe\u8ba1\u5e76\u6d4b\u8bd5FamLearn\u539f\u578b\uff0c\u5c55\u793a\u5176\u5728\u4efb\u52a1\u5206\u914d\u3001\u8d21\u732e\u53ef\u89c6\u5316\u4e0e\u4e2a\u6027\u5316\u652f\u6301\u65b9\u9762\u7684\u6709\u6548\u6027", "conclusion": "LLMs\u53ef\u4ee5\u8d85\u8d8a\u5bb6\u5ead\u6559\u80b2\u4e2d\u7684\u5bfc\u5e08\u89d2\u8272\uff0c\u4f5c\u4e3a\u5bb6\u5ead\u534f\u4f5c\u7684\u4e2d\u4ecb\uff0c\u5e73\u8861\u8d23\u4efb\u3001\u4fc3\u8fdb\u4ee3\u9645\u53c2\u4e0e\uff0c\u5e76\u589e\u5f3a\u5bb6\u5ead\u5b66\u4e60\u7684\u5173\u7cfb\u7ed3\u6784\u3002"}}
{"id": "2510.20409", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.20409", "abs": "https://arxiv.org/abs/2510.20409", "authors": ["Yi Li", "Francesco Chiossi", "Helena Anna Frijns", "Jan Leusmann", "Julian Rasch", "Robin Welsch", "Philipp Wintersberger", "Florian Michahelles", "Albrecht Schmidt"], "title": "Designing Intent Communication for Agent-Human Collaboration", "comment": null, "summary": "As autonomous agents, from self-driving cars to virtual assistants, become\nincreasingly present in everyday life, safe and effective collaboration depends\non human understanding of agents' intentions. Current intent communication\napproaches are often rigid, agent-specific, and narrowly scoped, limiting their\nadaptability across tasks, environments, and user preferences. A key gap\nremains: existing models of what to communicate are rarely linked to systematic\nchoices of how and when to communicate, preventing the development of\ngeneralizable, multi-modal strategies. In this paper, we introduce a\nmultidimensional design space for intent communication structured along three\ndimensions: Transparency (what is communicated), Abstraction (when), and\nModality (how). We apply this design space to three distinct human-agent\ncollaboration scenarios: (a) bystander interaction, (b) cooperative tasks, and\n(c) shared control, demonstrating its capacity to generate adaptable, scalable,\nand cross-domain communication strategies. By bridging the gap between intent\ncontent and communication implementation, our design space provides a\nfoundation for designing safer, more intuitive, and more transferable\nagent-human interactions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7ef4\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u7528\u4e8e\u81ea\u4e3b\u4ee3\u7406\u7684\u610f\u56fe\u6c9f\u901a\uff0c\u65e8\u5728\u63d0\u5347\u4eba\u673a\u4e92\u52a8\u7684\u5b89\u5168\u6027\u4e0e\u9002\u5e94\u6027\u3002", "motivation": "\u7531\u4e8e\u73b0\u6709\u610f\u56fe\u6c9f\u901a\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u6025\u9700\u4e00\u79cd\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u6c9f\u901a\u7b56\u7565\uff0c\u4ee5\u589e\u5f3a\u4eba\u7c7b\u5bf9\u81ea\u4e3b\u4ee3\u7406\u610f\u56fe\u7684\u7406\u89e3\u3002", "method": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5305\u62ec\u900f\u660e\u5ea6\u3001\u62bd\u8c61\u5ea6\u548c\u8868\u73b0\u5f62\u5f0f\u4e09\u4e2a\u7ef4\u5ea6\u7684\u610f\u56fe\u6c9f\u901a\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u5e76\u5e94\u7528\u4e8e\u4e09\u79cd\u4eba\u673a\u534f\u4f5c\u573a\u666f\u3002", "result": "\u901a\u8fc7\u5e94\u7528\u8bbe\u8ba1\u7a7a\u95f4\u4e8e\u591a\u79cd\u534f\u4f5c\u573a\u666f\uff0c\u5c55\u793a\u4e86\u5176\u751f\u6210\u9002\u5e94\u6027\u5f3a\u3001\u53ef\u6269\u5c55\u548c\u8de8\u9886\u57df\u6c9f\u901a\u7b56\u7565\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u8bbe\u8ba1\u7a7a\u95f4\u4e3a\u81ea\u4e3b\u4ee3\u7406\u4e0e\u4eba\u7c7b\u7684\u4e92\u52a8\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u4ee5\u5b9e\u73b0\u66f4\u5b89\u5168\u548c\u76f4\u89c2\u7684\u6c9f\u901a\u7b56\u7565\u3002"}}
{"id": "2510.20738", "categories": ["cs.HC", "cs.DS", "cs.GR", "math.OC", "stat.OT"], "pdf": "https://arxiv.org/pdf/2510.20738", "abs": "https://arxiv.org/abs/2510.20738", "authors": ["Albert Dorador"], "title": "Optimizing Feature Ordering in Radar Charts for Multi-Profile Comparison", "comment": null, "summary": "Radar charts are widely used to visualize multivariate data and compare\nmultiple profiles across features. However, the visual clarity of radar charts\ncan be severely compromised when feature values alternate drastically in\nmagnitude around the circle, causing areas to collapse, which misrepresents\nrelative differences. In the present work we introduce a permutation\noptimization strategy that reorders features to minimize polygon ``spikiness''\nacross multiple profiles simultaneously. The method is combinatorial\n(exhaustive search) for moderate numbers of features and uses a lexicographic\nminimax criterion that first considers overall smoothness (mean jump) and then\nthe largest single jump as a tie-breaker. This preserves more global\ninformation and produces visually balanced arrangements. We discuss complexity,\npractical bounds, and relations to existing approaches that either change the\nvisualization (e.g., OrigamiPlot) or learn orderings (e.g., Versatile Ordering\nNetwork). An example with two profiles and $p=6$ features (before/after\nordering) illustrates the qualitative improvement.\n  Keywords: data visualization, radar charts, combinatorial optimization,\nminimax optimization, feature ordering", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7f6e\u6362\u4f18\u5316\u7b56\u7565\uff0c\u4ee5\u63d0\u9ad8\u96f7\u8fbe\u56fe\u5728\u591a\u53d8\u91cf\u6570\u636e\u53ef\u89c6\u5316\u4e2d\u7684\u6e05\u6670\u5ea6\uff0c\u91cd\u70b9\u89e3\u51b3\u7279\u5f81\u503c\u5e45\u5ea6\u5dee\u5f02\u5bfc\u81f4\u7684\u56fe\u5f62\u626d\u66f2\u95ee\u9898\u3002", "motivation": "\u96f7\u8fbe\u56fe\u5728\u53ef\u89c6\u5316\u591a\u53d8\u91cf\u6570\u636e\u65f6\u5e38\u56e0\u7279\u5f81\u5e45\u5ea6\u5dee\u5f02\u5bfc\u81f4\u6e05\u6670\u5ea6\u4e0b\u964d\uff0c\u672c\u7814\u7a76\u65e8\u5728\u6539\u5584\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7ec4\u5408\u4f18\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u5168\u9762\u641c\u7d22\u548c\u5b57\u5178\u6700\u5c0f\u5316\u51c6\u5219\u5bf9\u7279\u5f81\u8fdb\u884c\u91cd\u65b0\u6392\u5e8f\uff0c\u4ee5\u51cf\u5c0f\u591a\u8f6e\u5ed3\u7684\u591a\u8fb9\u5f62\u201c\u5c16\u9510\u5ea6\u201d\u3002", "result": "\u901a\u8fc7\u4f18\u5316\u7279\u5f81\u987a\u5e8f\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u89c6\u89c9\u5e73\u8861\uff0c\u5e76\u5c55\u793a\u4e86\u96f7\u8fbe\u56fe\u5728\u7279\u5b9a\u793a\u4f8b\u4e2d\u7684\u663e\u8457\u6539\u5584\u3002", "conclusion": "\u901a\u8fc7\u6539\u8fdb\u7279\u5f81\u6392\u5e8f\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u96f7\u8fbe\u56fe\u7684\u89c6\u89c9\u5e73\u8861\u6027\uff0c\u4ece\u800c\u66f4\u51c6\u786e\u5730\u8868\u793a\u591a\u79cd\u7279\u5f81\u7684\u76f8\u5bf9\u5dee\u5f02\u3002"}}
{"id": "2510.19962", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.19962", "abs": "https://arxiv.org/abs/2510.19962", "authors": ["Chen-Lung Lu", "Honglu He", "Agung Julius", "John T. Wen"], "title": "Configuration-Dependent Robot Kinematics Model and Calibration", "comment": null, "summary": "Accurate robot kinematics is essential for precise tool placement in\narticulated robots, but non-geometric factors can introduce\nconfiguration-dependent model discrepancies. This paper presents a\nconfiguration-dependent kinematic calibration framework for improving accuracy\nacross the entire workspace. Local Product-of-Exponential (POE) models,\nselected for their parameterization continuity, are identified at multiple\nconfigurations and interpolated into a global model. Inspired by joint gravity\nload expressions, we employ Fourier basis function interpolation parameterized\nby the shoulder and elbow joint angles, achieving accuracy comparable to neural\nnetwork and autoencoder methods but with substantially higher training\nefficiency. Validation on two 6-DoF industrial robots shows that the proposed\napproach reduces the maximum positioning error by over 50%, meeting the\nsub-millimeter accuracy required for cold spray manufacturing. Robots with\nlarger configuration-dependent discrepancies benefit even more. A dual-robot\ncollaborative task demonstrates the framework's practical applicability and\nrepeatability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u914d\u7f6e\u4f9d\u8d56\u7684\u8fd0\u52a8\u5b66\u6807\u5b9a\u6846\u67b6\uff0c\u5229\u7528FOurier\u57fa\u51fd\u6570\u63d2\u503c\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5728\u6574\u4e2a\u5de5\u4f5c\u7a7a\u95f4\u7684\u7cbe\u5ea6\u3002", "motivation": "\u63d0\u9ad8\u624b\u722a\u673a\u5668\u4eba\u5728\u5404\u79cd\u914d\u7f6e\u4e0b\u7684\u8fd0\u52a8\u5b66\u7cbe\u5ea6\uff0c\u4ee5\u964d\u4f4e\u914d\u7f6e\u4f9d\u8d56\u6027\u5f15\u8d77\u7684\u6a21\u578b\u8bef\u5dee\u3002", "method": "\u91c7\u7528\u5c40\u90e8\u6307\u6570\u79ef\u6a21\u578b\uff0c\u901a\u8fc7\u5085\u91cc\u53f6\u57fa\u51fd\u6570\u63d2\u503c\uff0c\u5b9e\u73b0\u5bf9\u80a9\u90e8\u548c\u8098\u90e8\u89d2\u5ea6\u7684\u53c2\u6570\u5316\uff0c\u8bad\u7ec3\u6548\u7387\u9ad8\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u914d\u7f6e\u7684\u8fd0\u52a8\u5b66\u6807\u5b9a\u6846\u67b6\uff0c\u901a\u8fc7\u5c40\u90e8\u7684\u6307\u6570\u79ef\u6a21\u578b\u5728\u591a\u4e2a\u914d\u7f6e\u4e2d\u8fdb\u884c\u8bc6\u522b\u548c\u63d2\u503c\uff0c\u63d0\u5347\u4e86\u6574\u4e2a\u5de5\u4f5c\u7a7a\u95f4\u7684\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u6709\u6548\u51cf\u5c11\u673a\u5668\u4eba\u6700\u5927\u5b9a\u4f4d\u8bef\u5dee\uff0c\u5c24\u5176\u5bf9\u914d\u7f6e\u4f9d\u8d56\u6027\u8f83\u5927\u7684\u673a\u5668\u4eba\u6548\u679c\u663e\u8457\uff0c\u7b26\u5408\u5de5\u4e1a\u5e94\u7528\u7cbe\u5ea6\u8981\u6c42\u3002"}}
{"id": "2510.20743", "categories": ["cs.HC", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.20743", "abs": "https://arxiv.org/abs/2510.20743", "authors": ["Lorenzo Stacchio", "Andrea Ubaldi", "Alessandro Galdelli", "Maurizio Mauri", "Emanuele Frontoni", "Andrea Gaggioli"], "title": "Empathic Prompting: Non-Verbal Context Integration for Multimodal LLM Conversations", "comment": null, "summary": "We present Empathic Prompting, a novel framework for multimodal human-AI\ninteraction that enriches Large Language Model (LLM) conversations with\nimplicit non-verbal context. The system integrates a commercial facial\nexpression recognition service to capture users' emotional cues and embeds them\nas contextual signals during prompting. Unlike traditional multimodal\ninterfaces, empathic prompting requires no explicit user control; instead, it\nunobtrusively augments textual input with affective information for\nconversational and smoothness alignment. The architecture is modular and\nscalable, allowing integration of additional non-verbal modules. We describe\nthe system design, implemented through a locally deployed DeepSeek instance,\nand report a preliminary service and usability evaluation (N=5). Results show\nconsistent integration of non-verbal input into coherent LLM outputs, with\nparticipants highlighting conversational fluidity. Beyond this proof of\nconcept, empathic prompting points to applications in chatbot-mediated\ncommunication, particularly in domains like healthcare or education, where\nusers' emotional signals are critical yet often opaque in verbal exchanges.", "AI": {"tldr": "\u540c\u7406\u5fc3\u63d0\u793a\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u5229\u7528\u975e verbal \u60c5\u7eea\u7ebf\u7d22\u589e\u5f3a LLM \u5bf9\u8bdd\uff0c\u9002\u7528\u4e8e\u533b\u7597\u548c\u6559\u80b2\u7b49\u9886\u57df\u3002", "motivation": "\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u8bdd\u7684\u4e30\u5bcc\u6027\u548c\u8fde\u8d2f\u6027\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u60c5\u611f\u5185\u5bb9\u7684\u5904\u7406\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u96c6\u6210\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u670d\u52a1\u6765\u6355\u6349\u548c\u5229\u7528\u7528\u6237\u7684\u60c5\u611f\u7ebf\u7d22\uff0c\u6784\u5efa\u6d41\u7545\u7684\u5bf9\u8bdd\u7cfb\u7edf\u3002", "result": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u4eba\u673a\u4ea4\u4e92\u6846\u67b6\u2014\u2014\u540c\u7406\u5fc3\u63d0\u793a\uff0c\u5b83\u901a\u8fc7\u9690\u542b\u7684\u975e\u8bed\u8a00\u4e0a\u4e0b\u6587\u4e30\u5bcc\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u8bdd\u3002\u7cfb\u7edf\u96c6\u6210\u4e86\u4e00\u79cd\u5546\u4e1a\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u670d\u52a1\uff0c\u4ee5\u6355\u6349\u7528\u6237\u7684\u60c5\u611f\u7ebf\u7d22\uff0c\u5e76\u5c06\u8fd9\u4e9b\u7ebf\u7d22\u5d4c\u5165\u63d0\u793a\u7684\u4e0a\u4e0b\u6587\u4e2d\u3002\u4e0e\u4f20\u7edf\u7684\u591a\u6a21\u6001\u63a5\u53e3\u4e0d\u540c\uff0c\u540c\u7406\u5fc3\u63d0\u793a\u4e0d\u9700\u8981\u7528\u6237\u660e\u786e\u63a7\u5236\uff0c\u800c\u662f\u81ea\u7136\u800c\u7136\u5730\u5c06\u60c5\u611f\u4fe1\u606f\u878d\u5165\u6587\u672c\u8f93\u5165\u4e2d\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u8bdd\u7684\u6d41\u7545\u6027\u548c\u4e00\u81f4\u6027\u3002\u67b6\u6784\u662f\u6a21\u5757\u5316\u548c\u53ef\u6269\u5c55\u7684\uff0c\u5141\u8bb8\u96c6\u6210\u5176\u4ed6\u975e\u8bed\u8a00\u6a21\u5757\u3002\u6211\u4eec\u63cf\u8ff0\u4e86\u901a\u8fc7\u672c\u5730\u90e8\u7f72DeepSeek\u5b9e\u4f8b\u5b9e\u73b0\u7684\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u5e76\u62a5\u544a\u4e86\u4e00\u9879\u521d\u6b65\u7684\u670d\u52a1\u548c\u53ef\u7528\u6027\u8bc4\u4f30\uff08N=5\uff09\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5728\u4e00\u81f4\u7684\u975e\u8bed\u8a00\u8f93\u5165\u4e0e\u8fde\u8d2f\u7684LLM\u8f93\u51fa\u4e4b\u95f4\u5b9e\u73b0\u4e86\u4e00\u81f4\u96c6\u6210\uff0c\u800c\u53c2\u4e0e\u8005\u5f3a\u8c03\u4e86\u5bf9\u8bdd\u7684\u6d41\u7545\u6027\u3002\u9664\u4e86\u4f5c\u4e3a\u6982\u5ff5\u9a8c\u8bc1\u5916\uff0c\u540c\u7406\u5fc3\u63d0\u793a\u8fd8\u6307\u5411\u4e86\u5728\u673a\u5668\u4eba\u5a92\u4ecb\u6c9f\u901a\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u533b\u7597\u6216\u6559\u80b2\u7b49\u9886\u57df\uff0c\u5728\u8fd9\u4e9b\u9886\u57df\u4e2d\uff0c\u7528\u6237\u7684\u60c5\u611f\u4fe1\u53f7\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u53e3\u5934\u4ea4\u6d41\u4e2d\u5f80\u5f80\u662f\u9690\u853d\u7684\u3002", "conclusion": "\u540c\u7406\u5fc3\u63d0\u793a\u4e3a\u591a\u6a21\u6001\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\uff0c\u5c24\u5176\u5728\u9700\u8981\u5173\u6ce8\u7528\u6237\u60c5\u611f\u7684\u9886\u57df\u3002"}}
{"id": "2510.19974", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.19974", "abs": "https://arxiv.org/abs/2510.19974", "authors": ["Hien Bui", "Yufeiyang Gao", "Haoran Yang", "Eric Cui", "Siddhant Mody", "Brian Acosta", "Thomas Stephen Felix", "Bibit Bianchini", "Michael Posa"], "title": "Push Anything: Single- and Multi-Object Pushing From First Sight with Contact-Implicit MPC", "comment": "Hien Bui, Yufeiyang Gao, and Haoran Yang contributed equally to this\n  work", "summary": "Non-prehensile manipulation of diverse objects remains a core challenge in\nrobotics, driven by unknown physical properties and the complexity of\ncontact-rich interactions. Recent advances in contact-implicit model predictive\ncontrol (CI-MPC), with contact reasoning embedded directly in the trajectory\noptimization, have shown promise in tackling the task efficiently and robustly,\nyet demonstrations have been limited to narrowly curated examples. In this\nwork, we showcase the broader capabilities of CI-MPC through precise planar\npushing tasks over a wide range of object geometries, including multi-object\ndomains. These scenarios demand reasoning over numerous inter-object and\nobject-environment contacts to strategically manipulate and de-clutter the\nenvironment, challenges that were intractable for prior CI-MPC methods. To\nachieve this, we introduce Consensus Complementarity Control Plus (C3+), an\nenhanced CI-MPC algorithm integrated into a complete pipeline spanning object\nscanning, mesh reconstruction, and hardware execution. Compared to its\npredecessor C3, C3+ achieves substantially faster solve times, enabling\nreal-time performance even in multi-object pushing tasks. On hardware, our\nsystem achieves overall 98% success rate across 33 objects, reaching pose goals\nwithin tight tolerances. The average time-to-goal is approximately 0.5, 1.6,\n3.2, and 5.3 minutes for 1-, 2-, 3-, and 4-object tasks, respectively. Project\npage: https://dairlab.github.io/push-anything.", "AI": {"tldr": "C3+\u7b97\u6cd5\u663e\u793a\u4e86\u5728\u591a\u6837\u5316\u7269\u4f53\u64cd\u63a7\u4e2d\u7684\u9ad8\u6548\u80fd\uff0c\u6210\u529f\u7387\u8fbe\u523098%\uff0c\u5e76\u4e14\u5728\u5b9e\u65f6\u63a8\u9001\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u9762\u5bf9\u4f17\u591a\u672a\u77e5\u7269\u4f53\u7269\u7406\u7279\u6027\u548c\u590d\u6742\u7684\u63a5\u89e6\u4ea4\u4e92\uff0c\u4f20\u7edf\u7684\u975e\u6293\u53d6\u64cd\u4f5c\u65b9\u6cd5\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u66f4\u4e3a\u5148\u8fdb\u7684\u63a7\u5236\u7b97\u6cd5\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u589e\u5f3a\u7684CI-MPC\u7b97\u6cd5C3+\uff0c\u96c6\u6210\u4e86\u5bf9\u8c61\u626b\u63cf\u3001\u7f51\u683c\u91cd\u5efa\u548c\u786c\u4ef6\u6267\u884c\u7684\u5b8c\u6574\u6d41\u7a0b\u3002", "result": "C3+\u572833\u4e2a\u7269\u4f53\u4e0a\u5b9e\u73b0\u4e8698%\u7684\u6210\u529f\u7387\uff0c\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u4f9d\u6b21\u4e3a0.5\u30011.6\u30013.2\u548c5.3\u5206\u949f\uff0c\u53d6\u51b3\u4e8e\u5bf9\u8c61\u7684\u6570\u91cf\u3002", "conclusion": "C3+\u7b97\u6cd5\u5728\u591a\u5bf9\u8c61\u63a8\u9001\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u9ad8\u6548\u7684\u5b9e\u65f6\u6027\u80fd\u548c98%\u7684\u6210\u529f\u7387\uff0c\u8868\u660e\u5176\u5728\u975e\u6293\u53d6\u64cd\u4f5c\u4e2d\u7684\u6709\u6548\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2510.20774", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.20774", "abs": "https://arxiv.org/abs/2510.20774", "authors": ["Wenhao Wang", "Kehe Ye", "Xinyu Zhou", "Tianxing Chen", "Cao Min", "Qiaoming Zhu", "Xiaokang Yang", "Yongjian Shen", "Yang Yang", "Maoqing Yao", "Yao Mu"], "title": "FieldGen: From Teleoperated Pre-Manipulation Trajectories to Field-Guided Data Generation", "comment": "Webpage: https://fieldgen.github.io/", "summary": "Large-scale and diverse datasets are vital for training robust robotic\nmanipulation policies, yet existing data collection methods struggle to balance\nscale, diversity, and quality. Simulation offers scalability but suffers from\nsim-to-real gaps, while teleoperation yields high-quality demonstrations with\nlimited diversity and high labor cost. We introduce FieldGen, a field-guided\ndata generation framework that enables scalable, diverse, and high-quality\nreal-world data collection with minimal human supervision. FieldGen decomposes\nmanipulation into two stages: a pre-manipulation phase, allowing trajectory\ndiversity, and a fine manipulation phase requiring expert precision. Human\ndemonstrations capture key contact and pose information, after which an\nattraction field automatically generates diverse trajectories converging to\nsuccessful configurations. This decoupled design combines scalable trajectory\ndiversity with precise supervision. Moreover, FieldGen-Reward augments\ngenerated data with reward annotations to further enhance policy learning.\nExperiments demonstrate that policies trained with FieldGen achieve higher\nsuccess rates and improved stability compared to teleoperation-based baselines,\nwhile significantly reducing human effort in long-term real-world data\ncollection. Webpage is available at https://fieldgen.github.io/.", "AI": {"tldr": "FieldGen\u662f\u4e00\u4e2a\u4e95\u6307\u5bfc\u7684\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u64cd\u4f5c\u5206\u4e3a\u9884\u64cd\u4f5c\u548c\u7ec6\u64cd\u4f5c\u4e24\u4e2a\u9636\u6bb5\uff0c\u5b9e\u73b0\u5728\u6570\u636e\u6536\u96c6\u65b9\u9762\u7684\u89c4\u6a21\u5316\u3001\u591a\u6837\u5316\u548c\u9ad8\u8d28\u91cf\u3002", "motivation": "\u5728\u8bad\u7ec3\u5f3a\u5927\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u653f\u7b56\u65f6\uff0c\u5927\u89c4\u6a21\u548c\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7684\u6570\u636e\u6536\u96c6\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u89c4\u6a21\u3001\u591a\u6837\u6027\u548c\u8d28\u91cf\u3002", "method": "FieldGen\u5c06\u64cd\u4f5c\u5206\u89e3\u4e3a\u9884\u64cd\u4f5c\u9636\u6bb5\u548c\u7ec6\u64cd\u4f5c\u9636\u6bb5\uff0c\u901a\u8fc7\u4eba\u7c7b\u793a\u8303\u6355\u83b7\u5173\u952e\u4fe1\u606f\uff0c\u7136\u540e\u5229\u7528\u5438\u5f15\u573a\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u7684\u8f68\u8ff9\u3002", "result": "FieldGen\u6846\u67b6\u80fd\u591f\u5728\u6700\u5c0f\u4eba\u529b\u76d1\u7763\u4e0b\u5b9e\u73b0\u89c4\u6a21\u5316\u3001\u591a\u6837\u5316\u548c\u9ad8\u8d28\u91cf\u7684\u73b0\u5b9e\u6570\u636e\u6536\u96c6\u3002", "conclusion": "FieldGen\u8bad\u7ec3\u51fa\u7684\u653f\u7b56\u5728\u6210\u529f\u7387\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u4f18\u4e8e\u57fa\u4e8e\u8fdc\u7a0b\u64cd\u4f5c\u7684\u57fa\u51c6\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u957f\u671f\u5b9e\u9645\u6570\u636e\u6536\u96c6\u4e2d\u7684\u4eba\u529b\u6295\u5165\u3002"}}
{"id": "2510.20008", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.20008", "abs": "https://arxiv.org/abs/2510.20008", "authors": ["Swati Dantu", "Robert P\u011bni\u010dka", "Martin Saska"], "title": "Simultaneous learning of state-to-state minimum-time planning and control", "comment": null, "summary": "This paper tackles the challenge of learning a generalizable minimum-time\nflight policy for UAVs, capable of navigating between arbitrary start and goal\nstates while balancing agile flight and stable hovering. Traditional\napproaches, particularly in autonomous drone racing, achieve impressive speeds\nand agility but are constrained to predefined track layouts, limiting\nreal-world applicability. To address this, we propose a reinforcement\nlearning-based framework that simultaneously learns state-to-state minimum-time\nplanning and control and generalizes to arbitrary state-to-state flights. Our\napproach leverages Point Mass Model (PMM) trajectories as proxy rewards to\napproximate the true optimal flight objective and employs curriculum learning\nto scale the training process efficiently and to achieve generalization. We\nvalidate our method through simulation experiments, comparing it against\nNonlinear Model Predictive Control (NMPC) tracking PMM-generated trajectories\nand conducting ablation studies to assess the impact of curriculum learning.\nFinally, real-world experiments confirm the robustness of our learned policy in\noutdoor environments, demonstrating its ability to generalize and operate on a\nsmall ARM-based single-board computer.", "AI": {"tldr": "\u7814\u7a76\u4e00\u79cd\u901a\u7528\u7684\u65e0\u4eba\u673a\u6700\u77ed\u65f6\u95f4\u98de\u884c\u7b56\u7565\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u98de\u884c\u7075\u6d3b\u6027\u4e0e\u7a33\u5b9a\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u7b56\u7565\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u65e0\u4eba\u673a\u81ea\u4e3b\u98de\u884c\u65b9\u6cd5\u53d7\u9650\u4e8e\u9884\u8bbe\u8d5b\u9053\uff0c\u7f3a\u4e4f\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u5e7f\u6cdb\u9002\u7528\u7684\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u901a\u7528\u98de\u884c\u7b56\u7565\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u70b9\u8d28\u91cf\u6a21\u578b\uff08PMM\uff09\u8f68\u8ff9\u4f5c\u4e3a\u4ee3\u7406\u5956\u52b1\uff0c\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u63d0\u5347\u8bad\u7ec3\u6548\u7387\uff0c\u9488\u5bf9\u72b6\u6001\u5230\u72b6\u6001\u7684\u6700\u5c0f\u65f6\u95f4\u89c4\u5212\u4e0e\u63a7\u5236\u8fdb\u884c\u5b66\u4e60\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u5b9e\u9a8c\u548c\u5b9e\u5730\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u7a33\u5065\u6027\u53ca\u5176\u5728\u5c0f\u578b\u5355\u677f\u8ba1\u7b97\u673a\u4e0a\u7684\u53ef\u64cd\u4f5c\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u7b56\u7565\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c55\u793a\u4e86\u826f\u597d\u7684\u9002\u5e94\u6027\u548c\u53ef\u63a8\u5e7f\u6027\u3002"}}
{"id": "2510.20070", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.20070", "abs": "https://arxiv.org/abs/2510.20070", "authors": ["Sourabh Karmakar", "Apurva Patel", "Cameron J. Turner"], "title": "Calibration of Parallel Kinematic Machine Based on Stewart Platform-A Literature Review", "comment": null, "summary": "Stewart platform-based Parallel Kinematic (PKM) Machines have been\nextensively studied by researchers due to their inherent finer control\ncharacteristics. This has opened its potential deployment opportunities in\nversatile critical applications like the medical field, engineering machines,\nspace research, electronic chip manufacturing, automobile manufacturing, etc.\nAll these precise, complicated, and repeatable motion applications require\nmicro and nano-scale movement control in 3D space; a 6-DOF PKM can take this\nchallenge smartly. For this, the PKM must be more accurate than the desired\napplication accuracy level and thus proper calibration for a PKM robot is\nessential. Forward kinematics-based calibration for such hexapod machines\nbecomes unnecessarily complex and inverse kinematics complete this task with\nmuch ease. To analyze different techniques, an external instrument-based,\nconstraint-based, and auto or self-calibration-based approaches have been used\nfor calibration. This survey has been done by reviewing these key\nmethodologies, their outcome, and important points related to inverse\nkinematic-based PKM calibrations in general. It is observed in this study that\nthe researchers focused on improving the accuracy of the platform position and\norientation considering the errors contributed by a single source or multiple\nsources. The error sources considered are mainly structural, in some cases,\nenvironmental factors are also considered, however, these calibrations are done\nunder no-load conditions. This study aims to understand the current state of\nthe art in this field and to expand the scope for other researchers in further\nexploration in a specific area.", "AI": {"tldr": "\u672c\u7814\u7a76\u56de\u987e\u4e86\u5e76\u884c\u8fd0\u52a8\u673a\u7684\u9006\u5411\u8fd0\u52a8\u5b66\u6821\u51c6\u65b9\u6cd5\u53ca\u5176\u51c6\u786e\u6027\u7684\u63d0\u5347\uff0c\u7279\u522b\u5173\u6ce8\u8bef\u5dee\u6765\u6e90\u3002", "motivation": "\u7531\u4e8e\u5e76\u884c\u8fd0\u52a8\u673a\u5728\u533b\u836f\u3001\u5de5\u7a0b\u53ca\u822a\u7a7a\u7b49\u9886\u57df\u7684\u6f5c\u5728\u5e94\u7528\uff0c\u63d0\u5347\u5176\u5728\u4e09\u7ef4\u7a7a\u95f4\u7684\u5fae\u7c73\u548c\u7eb3\u7c73\u5c3a\u5ea6\u8fd0\u52a8\u63a7\u5236\u7684\u51c6\u786e\u6027\u6210\u4e3a\u5fc5\u8981\u3002", "method": "\u901a\u8fc7\u56de\u987e\u73b0\u6709\u7684\u5916\u90e8\u4eea\u5668\u57fa\u3001\u7ea6\u675f\u57fa\u4e0e\u81ea\u6211\u6821\u51c6\u7684\u65b9\u6cd5\uff0c\u5206\u6790\u4e0d\u540c\u7684\u6821\u51c6\u6280\u672f\u53ca\u5176\u7ed3\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u63d0\u5347\u5e73\u53f0\u4f4d\u7f6e\u548c\u65b9\u5411\u7cbe\u5ea6\u7684\u6821\u51c6\u4e3b\u8981\u96c6\u4e2d\u5728\u5355\u4e00\u6216\u591a\u91cd\u8bef\u5dee\u6e90\u4e0a\uff0c\u5c3d\u7ba1\u73af\u5883\u56e0\u7d20\u6709\u65f6\u4e5f\u88ab\u8003\u8651\uff0c\u4f46\u6821\u51c6\u901a\u5e38\u5728\u65e0\u8d1f\u8f7d\u6761\u4ef6\u4e0b\u8fdb\u884c\u3002", "conclusion": "\u672c\u7814\u7a76\u65e8\u5728\u68b3\u7406\u5e76\u7406\u89e3\u5e76\u884c\u8fd0\u52a8\u673a\u7684\u6821\u51c6\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u9006\u5411\u8fd0\u52a8\u5b66\u65b9\u6cd5\u7684\u5e94\u7528\uff0c\u4ee5\u63d0\u5347\u5e73\u53f0\u4f4d\u7f6e\u548c\u59ff\u6001\u7684\u7cbe\u5ea6\u3002"}}
{"id": "2510.20079", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.20079", "abs": "https://arxiv.org/abs/2510.20079", "authors": ["Travis A. Roberts", "Sourabh Karmakar", "Cameron J. Turner"], "title": "Design of a Bed Rotation Mechanism to Facilitate In-Situ Photogrammetric Reconstruction of Printed Parts", "comment": null, "summary": "Additive manufacturing, or 3D printing, is a complex process that creates\nfree-form geometric objects by sequentially placing material to construct an\nobject, usually in a layer-by-layer process. One of the most widely used\nmethods is Fused Deposition Modeling (FDM). FDM is used in many of the\nconsumer-grade polymer 3D printers available today. While consumer grade\nmachines are cheap and plentiful, they lack many of the features desired in a\nmachine used for research purposes and are often closed-source platforms.\nCommercial-grade models are more expensive and are also usually closed-source\nplatforms that do not offer flexibility for modifications often needed for\nresearch. The authors designed and fabricated a machine to be used as a test\nbed for research in the field of polymer FDM processes. The goal was to create\na platform that tightly controls and/or monitors the FDM build parameters so\nthat experiments can be repeated with a known accuracy. The platform offers\nclosed loop position feedback, control of the hot end and bed temperature, and\nmonitoring of environment temperature and humidity. Additionally, the platform\nis equipped with cameras and a mechanism for in-situ photogrammetry, creating a\ngeometric record of the printing throughout the printing process. Through\nphotogrammetry, backtracking and linking process parameters to observable\ngeometric defects can be achieved. This paper focuses on the design of a novel\nmechanism for spinning the heated bed to allow for photogrammetric\nreconstruction of the printed part using a minimal number of cameras, as\nimplemented on this platform.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u805a\u5408\u7269FDM\u6d4b\u8bd5\u5e73\u53f0\uff0c\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u7684\u52a0\u70ed\u5e8a\u65cb\u8f6c\u673a\u5236\uff0c\u5b9e\u73b0\u4f4e\u6210\u672c\u7684\u5149\u6d4b\u91cf\u91cd\u5efa\uff0c\u589e\u5f3a\u5b9e\u9a8c\u7684\u7cbe\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u9488\u5bf9\u5546\u7528\u548c\u6d88\u8d39\u7ea73D\u6253\u5370\u673a\u5728\u7075\u6d3b\u6027\u548c\u53ef\u4fee\u6539\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u53ef\u91cd\u590d\u5b9e\u9a8c\u7684\u9ad8\u7cbe\u5ea6FDM\u5e73\u53f0\u3002", "method": "\u8bbe\u8ba1\u5e76\u5236\u9020\u4e86\u4e00\u79cd\u6d4b\u8bd5\u5e73\u53f0\uff0c\u91c7\u7528\u95ed\u73af\u4f4d\u7f6e\u53cd\u9988\u3001\u70ed\u7aef\u548c\u5e8a\u6e29\u5ea6\u63a7\u5236\uff0c\u4ee5\u53ca\u5bf9\u73af\u5883\u6e29\u5ea6\u548c\u6e7f\u5ea6\u7684\u76d1\u6d4b\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u80fd\u591f\u76d1\u6d4bFDM\u6784\u5efa\u53c2\u6570\u5e76\u4e0e\u51e0\u4f55\u7f3a\u9677\u5173\u8054\u7684\u7cfb\u7edf\uff0c\u63d0\u5347\u4e86\u5b9e\u9a8c\u7684\u53ef\u91cd\u590d\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7528\u4e8e\u7814\u7a76\u805a\u5408\u7269FDM\u8fc7\u7a0b\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u91cd\u70b9\u5728\u4e8e\u901a\u8fc7\u65b0\u9896\u7684\u52a0\u70ed\u5e8a\u65cb\u8f6c\u673a\u5236\u4ee5\u8f83\u5c11\u7684\u76f8\u673a\u5b9e\u73b0\u5149\u6d4b\u91cf\u91cd\u5efa\u3002"}}
{"id": "2510.20161", "categories": ["cs.RO", "68T07, 68T40", "I.2.9; I.2.10; I.2.11"], "pdf": "https://arxiv.org/pdf/2510.20161", "abs": "https://arxiv.org/abs/2510.20161", "authors": ["Ahmed Alanazi", "Duy Ho", "Yugyung Lee"], "title": "PathFormer: A Transformer with 3D Grid Constraints for Digital Twin Robot-Arm Trajectory Generation", "comment": "8 pages, 7 figures, 7 tables", "summary": "Robotic arms require precise, task-aware trajectory planning, yet sequence\nmodels that ignore motion structure often yield invalid or inefficient\nexecutions. We present a Path-based Transformer that encodes robot motion with\na 3-grid (where/what/when) representation and constraint-masked decoding,\nenforcing lattice-adjacent moves and workspace bounds while reasoning over task\ngraphs and action order. Trained on 53,755 trajectories (80% train / 20%\nvalidation), the model aligns closely with ground truth -- 89.44% stepwise\naccuracy, 93.32% precision, 89.44% recall, and 90.40% F1 -- with 99.99% of\npaths legal by construction. Compiled to motor primitives on an xArm Lite 6\nwith a depth-camera digital twin, it attains up to 97.5% reach and 92.5% pick\nsuccess in controlled tests, and 86.7% end-to-end success across 60\nlanguage-specified tasks in cluttered scenes, absorbing slips and occlusions\nvia local re-grounding without global re-planning. These results show that\npath-structured representations enable Transformers to generate accurate,\nreliable, and interpretable robot trajectories, bridging graph-based planning\nand sequence-based learning and providing a practical foundation for\ngeneral-purpose manipulation and sim-to-real transfer.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684Path-based Transformer\u6a21\u578b\uff0c\u80fd\u591f\u9ad8\u6548\u89e3\u7801\u673a\u5668\u4eba\u8f68\u8ff9\uff0c\u5e76\u5728\u591a\u4e2a\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u6781\u9ad8\u7684\u51c6\u786e\u6027\u548c\u6210\u529f\u7387\u3002", "motivation": "\u673a\u5668\u4eba\u624b\u81c2\u9700\u8981\u7cbe\u786e\u4e14\u5177\u6709\u4efb\u52a1\u610f\u8bc6\u7684\u8f68\u8ff9\u89c4\u5212\uff0c\u7136\u800c\u5ffd\u7565\u8fd0\u52a8\u7ed3\u6784\u7684\u5e8f\u5217\u6a21\u578b\u5f80\u5f80\u4f1a\u5bfc\u81f4\u65e0\u6548\u6216\u4f4e\u6548\u7684\u6267\u884c\u3002", "method": "\u901a\u8fc7\u7ea6\u675f\u63a9\u7801\u89e3\u7801\uff0c\u786e\u4fdd\u76f8\u90bb\u79fb\u52a8\u548c\u5de5\u4f5c\u7a7a\u95f4\u9650\u5236\uff0c\u540c\u65f6\u8003\u8651\u4efb\u52a1\u56fe\u548c\u52a8\u4f5c\u987a\u5e8f\uff0c\u6a21\u578b\u572853,755\u6761\u8f68\u8ff9\u4e0a\u8fdb\u884c\u8bad\u7ec3.", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8def\u5f84\u7684Transformer\u6a21\u578b\uff0c\u80fd\u591f\u901a\u8fc73-grid\uff08\u4f4d\u7f6e/\u4efb\u52a1/\u65f6\u95f4\uff09\u8868\u793a\u6cd5\u7f16\u7801\u673a\u5668\u4eba\u8fd0\u52a8\u3002", "conclusion": "\u8def\u5f84\u7ed3\u6784\u5316\u8868\u793a\u4f7f\u5f97Transformer\u80fd\u591f\u751f\u6210\u51c6\u786e\u3001\u53ef\u9760\u4e14\u53ef\u89e3\u91ca\u7684\u673a\u5668\u4eba\u8f68\u8ff9\uff0c\u4fc3\u8fdb\u56fe\u57fa\u89c4\u5212\u4e0e\u5e8f\u5217\u5b66\u4e60\u7684\u7ed3\u5408\u3002"}}
{"id": "2510.20174", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.20174", "abs": "https://arxiv.org/abs/2510.20174", "authors": ["Yong Um", "Young-Ha Shin", "Joon-Ha Kim", "Soonpyo Kwon", "Hae-Won Park"], "title": "Reinforcement Learning-based Robust Wall Climbing Locomotion Controller in Ferromagnetic Environment", "comment": "8 pages, 6 figures", "summary": "We present a reinforcement learning framework for quadrupedal wall-climbing\nlocomotion that explicitly addresses uncertainty in magnetic foot adhesion. A\nphysics-based adhesion model of a quadrupedal magnetic climbing robot is\nincorporated into simulation to capture partial contact, air-gap sensitivity,\nand probabilistic attachment failures. To stabilize learning and enable\nreliable transfer, we design a three-phase curriculum: (1) acquire a crawl gait\non flat ground without adhesion, (2) gradually rotate the gravity vector to\nvertical while activating the adhesion model, and (3) inject stochastic\nadhesion failures to encourage slip recovery. The learned policy achieves a\nhigh success rate, strong adhesion retention, and rapid recovery from\ndetachment in simulation under degraded adhesion. Compared with a model\npredictive control (MPC) baseline that assumes perfect adhesion, our controller\nmaintains locomotion when attachment is intermittently lost. Hardware\nexperiments with the untethered robot further confirm robust vertical crawling\non steel surfaces, maintaining stability despite transient misalignment and\nincomplete attachment. These results show that combining curriculum learning\nwith realistic adhesion modeling provides a resilient sim-to-real framework for\nmagnetic climbing robots in complex environments.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u589e\u5f3a\u5b66\u4e60\u6846\u67b6\uff0c\u5e2e\u52a9\u56db\u8db3\u673a\u5668\u4eba\u5728\u4e0d\u786e\u5b9a\u7684\u78c1\u6027\u9644\u7740\u4e0b\u5b8c\u6210\u722c\u884c\u4efb\u52a1\uff0c\u8868\u73b0\u51fa\u826f\u597d\u7684\u7a33\u5b9a\u6027\u548c\u6062\u590d\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u56db\u8db3\u673a\u5668\u4eba\u5728\u78c1\u6027\u9644\u7740\u4e0b\u7684\u8fd0\u52a8\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u63d0\u5347\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u722c\u884c\u80fd\u529b\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\uff0c\u9996\u5148\u8bad\u7ec3\u5728\u5e73\u5766\u5730\u9762\u4e0a\u722c\u884c\uff0c\u63a5\u7740\u5728\u5782\u76f4\u9762\u4e0a\u6fc0\u6d3b\u9644\u7740\u6a21\u578b\uff0c\u6700\u540e\u6ce8\u5165\u968f\u673a\u7684\u9644\u7740\u5931\u8d25\u4ee5\u589e\u5f3a\u6062\u590d\u80fd\u529b\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u7269\u7406\u57fa\u7840\u7684\u9644\u7740\u6a21\u578b\uff0c\u901a\u8fc7\u9010\u6b65\u5b66\u4e60\u548c\u6ce8\u5165\u968f\u673a\u6027\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5728\u5782\u76f4\u9762\u4e0a\u7684\u7a33\u5b9a\u6027\u548c\u6062\u590d\u80fd\u529b\u3002", "conclusion": "\u7ed3\u5408\u8bfe\u7a0b\u5b66\u4e60\u548c\u73b0\u5b9e\u9644\u7740\u5efa\u6a21\u7684\u65b9\u6cd5\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u78c1\u6027\u722c\u884c\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5065\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u6846\u67b6\u3002"}}
{"id": "2510.20177", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.20177", "abs": "https://arxiv.org/abs/2510.20177", "authors": ["Muhammad Suhail Saleem", "Lai Yuan", "Maxim Likhachev"], "title": "A Contact-Driven Framework for Manipulating in the Blind", "comment": null, "summary": "Robots often face manipulation tasks in environments where vision is\ninadequate due to clutter, occlusions, or poor lighting--for example, reaching\na shutoff valve at the back of a sink cabinet or locating a light switch above\na crowded shelf. In such settings, robots, much like humans, must rely on\ncontact feedback to distinguish free from occupied space and navigate around\nobstacles. Many of these environments often exhibit strong structural\npriors--for instance, pipes often span across sink cabinets--that can be\nexploited to anticipate unseen structure and avoid unnecessary collisions. We\npresent a theoretically complete and empirically efficient framework for\nmanipulation in the blind that integrates contact feedback with structural\npriors to enable robust operation in unknown environments. The framework\ncomprises three tightly coupled components: (i) a contact detection and\nlocalization module that utilizes joint torque sensing with a contact particle\nfilter to detect and localize contacts, (ii) an occupancy estimation module\nthat uses the history of contact observations to build a partial occupancy map\nof the workspace and extrapolate it into unexplored regions with learned\npredictors, and (iii) a planning module that accounts for the fact that contact\nlocalization estimates and occupancy predictions can be noisy, computing paths\nthat avoid collisions and complete tasks efficiently without eliminating\nfeasible solutions. We evaluate the system in simulation and in the real world\non a UR10e manipulator across two domestic tasks--(i) manipulating a valve\nunder a kitchen sink surrounded by pipes and (ii) retrieving a target object\nfrom a cluttered shelf. Results show that the framework reliably solves these\ntasks, achieving up to a 2x reduction in task completion time compared to\nbaselines, with ablations confirming the contribution of each module.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u578b\u6846\u67b6\uff0c\u5c06\u63a5\u89e6\u53cd\u9988\u4e0e\u73af\u5883\u7ed3\u6784\u5148\u9a8c\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5728\u89c6\u89c9\u4e0d\u8db3\u573a\u666f\u4e0b\u7684\u64cd\u63a7\u6548\u7387\u3002", "motivation": "\u5728\u89c6\u89c9\u4e0d\u8db3\u7684\u73af\u5883\u4e2d\uff0c\u673a\u5668\u4eba\u9700\u4f9d\u8d56\u63a5\u89e6\u53cd\u9988\u6765\u533a\u5206\u7a7a\u95f2\u548c\u88ab\u5360\u7528\u7684\u7a7a\u95f4\uff0c\u540c\u65f6\u5229\u7528\u73af\u5883\u4e2d\u7684\u7ed3\u6784\u5148\u9a8c\u6765\u907f\u514d\u78b0\u649e\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7efc\u5408\u63a5\u89e6\u53cd\u9988\u4e0e\u7ed3\u6784\u5148\u9a8c\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5305\u62ec\u63a5\u89e6\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\u6a21\u5757\u3001\u5360\u7528\u4f30\u8ba1\u6a21\u5757\u548c\u89c4\u5212\u6a21\u5757\u3002", "result": "\u8be5\u7cfb\u7edf\u5728\u4eff\u771f\u548c\u5b9e\u9645\u73af\u5883\u4e2d\u6d4b\u8bd5\uff0c\u80fd\u591f\u9ad8\u6548\u6267\u884c\u4e24\u4e2a\u5bb6\u5ead\u4efb\u52a1\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5404\u4e2a\u6a21\u5757\u7684\u8d21\u732e\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u53ef\u9760\u5730\u89e3\u51b3\u5404\u79cd\u64cd\u63a7\u4efb\u52a1\uff0c\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u76f8\u6bd4\u4e8e\u57fa\u7ebf\u51cf\u5c11\u4e86\u591a\u8fbe2\u500d\u3002"}}
{"id": "2510.20231", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.20231", "abs": "https://arxiv.org/abs/2510.20231", "authors": ["Yuta Takahashi", "Atsuki Ochi", "Yoichi Tomioka", "Shin-Ichiro Sakai"], "title": "NODA-MMH: Certified Learning-Aided Nonlinear Control for Magnetically-Actuated Swarm Experiment Toward On-Orbit Proof", "comment": "Accepted for presentation at the 2025 International Conference on\n  Space Robotics (iSpaRo 2025)", "summary": "This study experimentally validates the principle of large-scale satellite\nswarm control through learning-aided magnetic field interactions generated by\nsatellite-mounted magnetorquers. This actuation presents a promising solution\nfor the long-term formation maintenance of multiple satellites and has\nprimarily been demonstrated in ground-based testbeds for two-satellite position\ncontrol. However, as the number of satellites increases beyond three,\nfundamental challenges coupled with the high nonlinearity arise: 1)\nnonholonomic constraints, 2) underactuation, 3) scalability, and 4)\ncomputational cost. Previous studies have shown that time-integrated current\ncontrol theoretically solves these problems, where the average actuator outputs\nalign with the desired command, and a learning-based technique further enhances\ntheir performance. Through multiple experiments, we validate critical aspects\nof learning-aided time-integrated current control: (1) enhanced controllability\nof the averaged system dynamics, with a theoretically guaranteed error bound,\nand (2) decentralized current management. We design two-axis coils and a\nground-based experimental setup utilizing an air-bearing platform, enabling a\nmathematical replication of orbital dynamics. Based on the effectiveness of the\nlearned interaction model, we introduce NODA-MMH (Neural power-Optimal Dipole\nAllocation for certified learned Model-based Magnetically swarm control\nHarness) for model-based power-optimal swarm control. This study complements\nour tutorial paper on magnetically actuated swarms for the long-term formation\nmaintenance problem.", "AI": {"tldr": "\u672c\u7814\u7a76\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5b66\u4e60\u8f85\u52a9\u7684\u78c1\u573a\u63a7\u5236\u536b\u661f\u7fa4\u7684\u53ef\u884c\u6027\uff0c\u89e3\u51b3\u4e86\u591a\u9897\u536b\u661f\u63a7\u5236\u4e2d\u7684\u57fa\u672c\u6311\u6218\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u529f\u7387\u6700\u4f18\u63a7\u5236\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u536b\u661f\u6570\u91cf\u8d85\u8fc7\u4e09\u9897\uff0c\u4f20\u7edf\u7684\u63a7\u5236\u65b9\u6cd5\u9762\u4e34\u975e\u5b8c\u6574\u7ea6\u675f\u3001\u6b20\u9a71\u52a8\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u8ba1\u7b97\u6210\u672c\u7b49\u6311\u6218\uff0c\u6b64\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5b66\u4e60\u8f85\u52a9\u7684\u65f6\u95f4\u79ef\u5206\u7535\u6d41\u63a7\u5236\u7684\u5173\u952e\u65b9\u9762\uff0c\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u8f74\u5411\u7ebf\u5708\u548c\u57fa\u4e8e\u7a7a\u6c14\u8f74\u627f\u7684\u5e73\u53f0\u5b9e\u9a8c\u8bbe\u7f6e\uff0c\u6a21\u62df\u8f68\u9053\u52a8\u529b\u5b66\u3002", "result": "\u9a8c\u8bc1\u4e86\u5b66\u4e60\u8f85\u52a9\u7684\u65f6\u95f4\u79ef\u5206\u7535\u6d41\u63a7\u5236\u5728\u7cfb\u7edf\u52a8\u529b\u5b66\u53ef\u63a7\u6027\u548c\u53bb\u4e2d\u5fc3\u5316\u7535\u6d41\u7ba1\u7406\u4e0a\u7684\u589e\u5f3a\u8868\u73b0\uff0c\u5e76\u5f15\u5165\u4e86NODA-MMH\u8fdb\u884c\u63a7\u5236\u3002", "conclusion": "\u672c\u7814\u7a76\u9a8c\u8bc1\u4e86\u901a\u8fc7\u5b66\u4e60\u8f85\u52a9\u7684\u78c1\u573a\u76f8\u4e92\u4f5c\u7528\u8fdb\u884c\u5927\u89c4\u6a21\u536b\u661f\u7fa4\u63a7\u5236\u7684\u539f\u7406\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u529f\u7387\u6700\u4f18\u63a7\u5236\u65b9\u6cd5NODA-MMH\u3002"}}
{"id": "2510.20261", "categories": ["cs.RO", "cs.CV", "I.2.10"], "pdf": "https://arxiv.org/pdf/2510.20261", "abs": "https://arxiv.org/abs/2510.20261", "authors": ["Mert Bulent Sariyildiz", "Philippe Weinzaepfel", "Guillaume Bono", "Gianluca Monaci", "Christian Wolf"], "title": "Kinaema: a recurrent sequence model for memory and pose in motion", "comment": "10 pages + references + checklist + appendix, 29 pages total", "summary": "One key aspect of spatially aware robots is the ability to \"find their\nbearings\", ie. to correctly situate themselves in previously seen spaces. In\nthis work, we focus on this particular scenario of continuous robotics\noperations, where information observed before an actual episode start is\nexploited to optimize efficiency. We introduce a new model, Kinaema, and agent,\ncapable of integrating a stream of visual observations while moving in a\npotentially large scene, and upon request, processing a query image and\npredicting the relative position of the shown space with respect to its current\nposition. Our model does not explicitly store an observation history, therefore\ndoes not have hard constraints on context length. It maintains an implicit\nlatent memory, which is updated by a transformer in a recurrent way,\ncompressing the history of sensor readings into a compact representation. We\nevaluate the impact of this model in a new downstream task we call \"Mem-Nav\".\nWe show that our large-capacity recurrent model maintains a useful\nrepresentation of the scene, navigates to goals observed before the actual\nepisode start, and is computationally efficient, in particular compared to\nclassical transformers with attention over an observation history.", "AI": {"tldr": "\u672c\u8bba\u6587\u4ecb\u7ecd\u4e86Kinaema\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5e2e\u52a9\u673a\u5668\u4eba\u5728\u5148\u524d\u89c2\u5bdf\u5230\u7684\u7a7a\u95f4\u4e2d\u627e\u5230\u4f4d\u7f6e\uff0c\u80fd\u591f\u9ad8\u6548\u5904\u7406\u89c6\u89c9\u4fe1\u606f\u5e76\u8fdb\u884c\u5bfc\u822a\u3002", "motivation": "\u7814\u7a76\u7a7a\u95f4\u611f\u77e5\u673a\u5668\u4eba\u5728\u8fde\u7eed\u64cd\u4f5c\u4e2d\u5982\u4f55\u5229\u7528\u4e4b\u524d\u7684\u89c6\u89c9\u4fe1\u606f\u6765\u4f18\u5316\u6548\u7387\uff0c\u7279\u522b\u662f\u5b9a\u4f4d\u81ea\u8eab\u5728\u5df2\u89c1\u7a7a\u95f4\u4e2d\u7684\u80fd\u529b\u3002", "method": "\u91c7\u7528\u9690\u5f0f\u6f5c\u5728\u8bb0\u5fc6\uff0c\u901a\u8fc7\u53d8\u6362\u5668\u4ee5\u9012\u5f52\u65b9\u5f0f\u66f4\u65b0\uff0c\u4ee5\u538b\u7f29\u4f20\u611f\u5668\u8bfb\u53d6\u7684\u5386\u53f2\u4fe1\u606f\uff0c\u589e\u5f3a\u89c6\u89c9\u6570\u636e\u7684\u5904\u7406\u80fd\u529b\u3002", "result": "\u63d0\u51fa\u4e00\u79cd\u65b0\u6a21\u578bKinaema\u548c\u4e00\u4e2a\u4ee3\u7406\uff0c\u80fd\u591f\u5728\u5927\u573a\u666f\u4e2d\u6574\u5408\u89c6\u89c9\u89c2\u5bdf\uff0c\u5e76\u5728\u8bf7\u6c42\u65f6\u9884\u6d4b\u67e5\u8be2\u56fe\u50cf\u76f8\u5bf9\u4e8e\u5f53\u524d\u4f4d\u7f6e\u7684\u76f8\u5bf9\u4f4d\u7f6e\u3002", "conclusion": "Kinaema\u6a21\u578b\u5728\u4fdd\u6301\u573a\u666f\u8868\u793a\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u5730\u5b9e\u73b0\u5bfc\u822a\u4efb\u52a1\uff0c\u5c24\u5176\u5728\u4e0e\u7ecf\u5178\u53d8\u6362\u5668\u76f8\u6bd4\u5177\u5907\u66f4\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2510.20328", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20328", "abs": "https://arxiv.org/abs/2510.20328", "authors": ["Ajay Sridhar", "Jennifer Pan", "Satvik Sharma", "Chelsea Finn"], "title": "MemER: Scaling Up Memory for Robot Control via Experience Retrieval", "comment": "Project page: https://jen-pan.github.io/memer/", "summary": "Humans routinely rely on memory to perform tasks, yet most robot policies\nlack this capability; our goal is to endow robot policies with the same\nability. Naively conditioning on long observation histories is computationally\nexpensive and brittle under covariate shift, while indiscriminate subsampling\nof history leads to irrelevant or redundant information. We propose a\nhierarchical policy framework, where the high-level policy is trained to select\nand track previous relevant keyframes from its experience. The high-level\npolicy uses selected keyframes and the most recent frames when generating text\ninstructions for a low-level policy to execute. This design is compatible with\nexisting vision-language-action (VLA) models and enables the system to\nefficiently reason over long-horizon dependencies. In our experiments, we\nfinetune Qwen2.5-VL-7B-Instruct and $\\pi_{0.5}$ as the high-level and low-level\npolicies respectively, using demonstrations supplemented with minimal language\nannotations. Our approach, MemER, outperforms prior methods on three real-world\nlong-horizon robotic manipulation tasks that require minutes of memory. Videos\nand code can be found at https://jen-pan.github.io/memer/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u673a\u5668\u4eba\u7b56\u7565\u7684\u5206\u5c42\u8bb0\u5fc6\u6846\u67b6\uff0c\u4ee5\u6539\u5584\u5176\u5728\u957f\u65f6\u95f4\u4efb\u52a1\u4e2d\u7684\u6267\u884c\u6548\u679c\u3002", "motivation": "\u5e0c\u671b\u901a\u8fc7\u589e\u5f3a\u673a\u5668\u4eba\u7684\u8bb0\u5fc6\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u6267\u884c\u9700\u8981\u957f\u65f6\u95f4\u8bb0\u5fc6\u7684\u4efb\u52a1\u3002", "method": "\u901a\u8fc7\u5206\u5c42\u7b56\u7565\u6846\u67b6\uff0c\u4f7f\u7528\u9ad8\u5c42\u7b56\u7565\u9009\u62e9\u548c\u8ddf\u8e2a\u5173\u952e\u5e27\uff0c\u5e76\u751f\u6210\u4f4e\u5c42\u7b56\u7565\u7684\u6267\u884c\u6307\u4ee4\u3002", "result": "MemER\u5728\u4e09\u9879\u957f\u65f6\u95f4\u64cd\u4f5c\u4efb\u52a1\u7684\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e09\u9879\u771f\u5b9e\u4e16\u754c\u7684\u957f\u671f\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u8d85\u8fc7\u4e86\u4e4b\u524d\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.20335", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20335", "abs": "https://arxiv.org/abs/2510.20335", "authors": ["Zixuan Wu", "Hengyuan Zhang", "Ting-Hsuan Chen", "Yuliang Guo", "David Paz", "Xinyu Huang", "Liu Ren"], "title": "Dino-Diffusion Modular Designs Bridge the Cross-Domain Gap in Autonomous Parking", "comment": "Code is at\n  https://github.com/ChampagneAndfragrance/Dino_Diffusion_Parking_Official", "summary": "Parking is a critical pillar of driving safety. While recent end-to-end (E2E)\napproaches have achieved promising in-domain results, robustness under domain\nshifts (e.g., weather and lighting changes) remains a key challenge. Rather\nthan relying on additional data, in this paper, we propose Dino-Diffusion\nParking (DDP), a domain-agnostic autonomous parking pipeline that integrates\nvisual foundation models with diffusion-based planning to enable generalized\nperception and robust motion planning under distribution shifts. We train our\npipeline in CARLA at regular setting and transfer it to more adversarial\nsettings in a zero-shot fashion. Our model consistently achieves a parking\nsuccess rate above 90% across all tested out-of-distribution (OOD) scenarios,\nwith ablation studies confirming that both the network architecture and\nalgorithmic design significantly enhance cross-domain performance over existing\nbaselines. Furthermore, testing in a 3D Gaussian splatting (3DGS) environment\nreconstructed from a real-world parking lot demonstrates promising sim-to-real\ntransfer.", "AI": {"tldr": "Dino-Diffusion Parking (DDP)\u662f\u4e00\u79cd\u65b0\u578b\u7684\u57df\u65e0\u5173\u81ea\u4e3b\u505c\u8f66\u7ba1\u9053\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u7684\u73af\u5883\u4e0b\u5b9e\u73b0\u9ad8\u9c81\u68d2\u6027\u4e0e\u6210\u529f\u7387\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u7684\u7aef\u5230\u7aef(Auto-Domain)\u505c\u8f66\u65b9\u6cd5\u5728\u56fa\u5b9a\u6761\u4ef6\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u73af\u5883\u53d8\u5316\uff08\u5982\u5929\u6c14\u548c\u5149\u7167\u53d8\u5316\uff09\u4e0b\u7684\u9c81\u68d2\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u7684Dino-Diffusion Parking (DDP)\u662f\u4e00\u79cd\u96c6\u6210\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u548c\u57fa\u4e8e\u6269\u6563\u7684\u89c4\u5212\u7684\u57df\u65e0\u5173\u81ea\u4e3b\u505c\u8f66\u7ba1\u9053\u3002", "result": "\u5728CARLA\u6a21\u62df\u73af\u5883\u4e0b\u8bad\u7ec3\u540e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u80fd\u591f\u65e0\u7f1d\u8f6c\u79fb\u5230\u66f4\u5177\u6311\u6218\u6027\u7684\u73af\u5883\u4e2d\uff0c\u5e76\u5728\u591a\u79cd\u51fa\u5206\u5e03\u573a\u666f\u4e2d\u4fdd\u6301\u9ad8\u505c\u8f66\u6210\u529f\u7387\u3002", "conclusion": "Dino-Diffusion Parking (DDP)\u6a21\u578b\u5728\u591a\u79cd\u4e0d\u5206\u5e03\u573a\u666f\u4e0b\u5747\u80fd\u4fdd\u6301\u8d85\u8fc790%\u7684\u505c\u8f66\u6210\u529f\u7387\uff0c\u5e76\u4e14\u5728\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u5c55\u793a\u4e86\u826f\u597d\u7684\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8f6c\u79fb\u80fd\u529b\u3002"}}
{"id": "2510.20347", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.20347", "abs": "https://arxiv.org/abs/2510.20347", "authors": ["Ashutosh Mishra", "Shreya Santra", "Elian Neppel", "Edoardo M. Rossi Lombardi", "Shamistan Karimov", "Kentaro Uno", "Kazuya Yoshida"], "title": "Multi-Modal Decentralized Reinforcement Learning for Modular Reconfigurable Lunar Robots", "comment": "Accepted in IEEE iSpaRo 2025. Awaiting Publication", "summary": "Modular reconfigurable robots suit task-specific space operations, but the\ncombinatorial growth of morphologies hinders unified control. We propose a\ndecentralized reinforcement learning (Dec-RL) scheme where each module learns\nits own policy: wheel modules use Soft Actor-Critic (SAC) for locomotion and\n7-DoF limbs use Proximal Policy Optimization (PPO) for steering and\nmanipulation, enabling zero-shot generalization to unseen configurations. In\nsimulation, the steering policy achieved a mean absolute error of 3.63{\\deg}\nbetween desired and induced angles; the manipulation policy plateaued at 84.6 %\nsuccess on a target-offset criterion; and the wheel policy cut average motor\ntorque by 95.4 % relative to baseline while maintaining 99.6 % success.\nLunar-analogue field tests validated zero-shot integration for autonomous\nlocomotion, steering, and preliminary alignment for reconfiguration. The system\ntransitioned smoothly among synchronous, parallel, and sequential modes for\nPolicy Execution, without idle states or control conflicts, indicating a\nscalable, reusable, and robust approach for modular lunar robots.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u5f3a\u5316\u5b66\u4e60\u65b9\u6848\uff0c\u7528\u4e8e\u6a21\u5757\u5316\u53ef\u91cd\u6784\u673a\u5668\u4eba\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u65b0\u914d\u7f6e\u7684\u96f6-shot\u9002\u5e94\uff0c\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\u4f18\u79c0\u7684\u63a7\u5236\u6548\u679c\u548c\u65e0\u7f1d\u5207\u6362\u80fd\u529b\u3002", "motivation": "\u6a21\u5757\u5316\u53ef\u91cd\u6784\u673a\u5668\u4eba\u9002\u5408\u7279\u5b9a\u4efb\u52a1\u7684\u7a7a\u95f4\u64cd\u4f5c\uff0c\u4f46\u5f62\u6001\u7684\u7ec4\u5408\u589e\u957f\u4f7f\u5f97\u7edf\u4e00\u63a7\u5236\u53d8\u5f97\u56f0\u96be\u3002", "method": "\u6bcf\u4e2a\u6a21\u5757\u91c7\u7528\u4e0d\u540c\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u8fdb\u884c\u81ea\u4e3b\u5b66\u4e60\uff0c\u8f6e\u5f0f\u6a21\u5757\u4f7f\u7528\u8f6f\u6f14\u5458-\u8bc4\u8bba\u5bb6\u7b97\u6cd5\u8fdb\u884c\u8fd0\u52a8\uff0c\u4e03\u81ea\u7531\u5ea6\u7684\u80a2\u4f53\u6a21\u5757\u5219\u4f7f\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff0c\u7528\u4e8e\u8f6c\u5411\u548c\u64cd\u4f5c\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u5f3a\u5316\u5b66\u4e60\u673a\u5236\uff0c\u4f7f\u6bcf\u4e2a\u6a21\u5757\u80fd\u591f\u72ec\u7acb\u5b66\u4e60\u63a7\u5236\u7b56\u7565\uff0c\u80fd\u591f\u5b9e\u73b0\u5bf9\u672a\u89c1\u914d\u7f6e\u7684\u96f6-shot\u6cdb\u5316\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5c55\u793a\u4e86\u6a21\u5757\u5316\u6708\u7403\u673a\u5668\u4eba\u5728\u81ea\u4e3b\u8fd0\u52a8\u3001\u8f6c\u5411\u548c\u521d\u6b65\u5bf9\u9f50\u7b49\u65b9\u9762\u7684\u53ef\u6269\u5c55\u6027\u3001\u53ef\u91cd\u7528\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.20390", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.20390", "abs": "https://arxiv.org/abs/2510.20390", "authors": ["Yijiong Lin", "Bowen Deng", "Chenghua Lu", "Max Yang", "Efi Psomopoulou", "Nathan F. Lepora"], "title": "NeuralTouch: Neural Descriptors for Precise Sim-to-Real Tactile Robot Control", "comment": null, "summary": "Grasping accuracy is a critical prerequisite for precise object manipulation,\noften requiring careful alignment between the robot hand and object. Neural\nDescriptor Fields (NDF) offer a promising vision-based method to generate\ngrasping poses that generalize across object categories. However, NDF alone can\nproduce inaccurate poses due to imperfect camera calibration, incomplete point\nclouds, and object variability. Meanwhile, tactile sensing enables more precise\ncontact, but existing approaches typically learn policies limited to simple,\npredefined contact geometries. In this work, we introduce NeuralTouch, a\nmultimodal framework that integrates NDF and tactile sensing to enable\naccurate, generalizable grasping through gentle physical interaction. Our\napproach leverages NDF to implicitly represent the target contact geometry,\nfrom which a deep reinforcement learning (RL) policy is trained to refine the\ngrasp using tactile feedback. This policy is conditioned on the neural\ndescriptors and does not require explicit specification of contact types. We\nvalidate NeuralTouch through ablation studies in simulation and zero-shot\ntransfer to real-world manipulation tasks--such as peg-out-in-hole and bottle\nlid opening--without additional fine-tuning. Results show that NeuralTouch\nsignificantly improves grasping accuracy and robustness over baseline methods,\noffering a general framework for precise, contact-rich robotic manipulation.", "AI": {"tldr": "NeuralTouch\u662f\u4e00\u4e2a\u7ed3\u5408\u89c6\u89c9\u548c\u89e6\u89c9\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548cNDF\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u51c6\u7684\u6293\u53d6", "motivation": "\u901a\u8fc7\u878d\u5408\u4e0d\u540c\u611f\u77e5\u65b9\u5f0f\u63d0\u9ad8\u6293\u53d6\u51c6\u786e\u6027\uff0c\u89e3\u51b3NDF\u5728\u590d\u6742\u5bf9\u8c61\u64cd\u4f5c\u65f6\u5b58\u5728\u7684\u4e0d\u8db3", "method": "\u901a\u8fc7\u6574\u5408\u795e\u7ecf\u63cf\u8ff0\u5b57\u6bb5\uff08NDF\uff09\u548c\u89e6\u89c9\u4f20\u611f\u6765\u5b9e\u73b0\u7cbe\u786e\u7684\u6293\u53d6", "result": "NeuralTouch\u5728\u5404\u7c7b\u64cd\u4f5c\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u6293\u53d6\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u672a\u7ecf\u8fc7\u8c03\u4f18\u7684\u76f4\u63a5\u8fc1\u79fb\u5230\u73b0\u5b9e\u4e16\u754c\u7684\u64cd\u4f5c", "conclusion": "NeuralTouch\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u4f7f\u673a\u5668\u4eba\u5728\u7269\u4f53\u64cd\u4f5c\u4e2d\u5b9e\u73b0\u66f4\u7cbe\u7ec6\u3001\u53ef\u9760\u7684\u89e6\u611f\u4ea4\u4e92\u3002"}}
{"id": "2510.20406", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20406", "abs": "https://arxiv.org/abs/2510.20406", "authors": ["Xiaogang Jia", "Qian Wang", "Anrui Wang", "Han A. Wang", "Bal\u00e1zs Gyenes", "Emiliyan Gospodinov", "Xinkai Jiang", "Ge Li", "Hongyi Zhou", "Weiran Liao", "Xi Huang", "Maximilian Beck", "Moritz Reuss", "Rudolf Lioutikov", "Gerhard Neumann"], "title": "PointMapPolicy: Structured Point Cloud Processing for Multi-Modal Imitation Learning", "comment": null, "summary": "Robotic manipulation systems benefit from complementary sensing modalities,\nwhere each provides unique environmental information. Point clouds capture\ndetailed geometric structure, while RGB images provide rich semantic context.\nCurrent point cloud methods struggle to capture fine-grained detail, especially\nfor complex tasks, which RGB methods lack geometric awareness, which hinders\ntheir precision and generalization. We introduce PointMapPolicy, a novel\napproach that conditions diffusion policies on structured grids of points\nwithout downsampling. The resulting data type makes it easier to extract shape\nand spatial relationships from observations, and can be transformed between\nreference frames. Yet due to their structure in a regular grid, we enable the\nuse of established computer vision techniques directly to 3D data. Using xLSTM\nas a backbone, our model efficiently fuses the point maps with RGB data for\nenhanced multi-modal perception. Through extensive experiments on the RoboCasa\nand CALVIN benchmarks and real robot evaluations, we demonstrate that our\nmethod achieves state-of-the-art performance across diverse manipulation tasks.\nThe overview and demos are available on our project page:\nhttps://point-map.github.io/Point-Map/", "AI": {"tldr": "PointMapPolicy\u901a\u8fc7\u5728\u6ca1\u6709\u4e0b\u91c7\u6837\u7684\u60c5\u51b5\u4e0b\u5bf9\u70b9\u7684\u7ed3\u6784\u5316\u7f51\u683c\u8fdb\u884c\u6761\u4ef6\u5316\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\u4e2d\u7684\u591a\u6a21\u6001\u611f\u77e5\u3002", "motivation": "\u5f53\u524d\u7684\u70b9\u4e91\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u7ec6\u7c92\u5ea6\u7ec6\u8282\uff0c\u800cRGB\u65b9\u6cd5\u7f3a\u4e4f\u51e0\u4f55\u610f\u8bc6\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u7684\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5f15\u5165PointMapPolicy\u65b9\u6cd5\uff0c\u4f7f\u7528\u7ed3\u6784\u5316\u70b9\u7f51\u683c\u6761\u4ef6\u5316\u6269\u6563\u7b56\u7565\uff0c\u5e76\u901a\u8fc7xLSTM\u8fdb\u884c\u9ad8\u6548\u878d\u5408\u3002", "result": "\u6211\u4eec\u7684\u6a21\u578b\u5728\u591a\u79cd\u64cd\u4f5c\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "PointMapPolicy\u6a21\u578b\u6709\u6548\u878d\u5408\u4e86\u70b9\u56fe\u548cRGB\u6570\u636e\uff0c\u5229\u7528xLSTM\u4f5c\u4e3a\u9aa8\u5e72\uff0c\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.20407", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.20407", "abs": "https://arxiv.org/abs/2510.20407", "authors": ["Kohei Nishi", "Masato Kobayashi", "Yuki Uranishi"], "title": "MR-UBi: Mixed Reality-Based Underwater Robot Arm Teleoperation System with Reaction Torque Indicator via Bilateral Control", "comment": null, "summary": "We present a mixed reality-based underwater robot arm teleoperation system\nwith a reaction torque indicator via bilateral control (MR-UBi). The reaction\ntorque indicator (RTI) overlays a color and length-coded torque bar in the\nMR-HMD, enabling seamless integration of visual and haptic feedback during\nunderwater robot arm teleoperation. User studies with sixteen participants\ncompared MR-UBi against a bilateral-control baseline. MR-UBi significantly\nimproved grasping-torque control accuracy, increasing the time within the\noptimal torque range and reducing both low and high grasping torque range\nduring lift and pick-and-place tasks with objects of different stiffness.\nSubjective evaluations further showed higher usability (SUS) and lower workload\n(NASA--TLX). Overall, the results confirm that \\textit{MR-UBi} enables more\nstable, accurate, and user-friendly underwater robot-arm teleoperation through\nthe integration of visual and haptic feedback. For additional material, please\ncheck: https://mertcookimg.github.io/mr-ubi", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u73b0\u5b9e\u7684\u6c34\u4e0b\u673a\u5668\u4eba\u81c2\u9065\u64cd\u4f5c\u7cfb\u7edfMR-UBi\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u89e6\u89c9\u53cd\u9988\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u64cd\u63a7\u7cbe\u51c6\u6027\u548c\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u66f4\u7cbe\u51c6\u3001\u7528\u6237\u53cb\u597d\u7684\u6c34\u4e0b\u673a\u5668\u4eba\u81c2\u9065\u64cd\u4f5c\u7cfb\u7edf\uff0c\u4ee5\u63d0\u5347\u6293\u53d6\u626d\u77e9\u63a7\u5236\u7684\u51c6\u786e\u6027\u548c\u64cd\u4f5c\u4f53\u9a8c\u3002", "method": "\u901a\u8fc7\u4e0e\u4f20\u7edf\u53cc\u5411\u63a7\u5236\u57fa\u51c6\u8fdb\u884c\u5bf9\u6bd4\uff0c\u8fdb\u884c\u7528\u6237\u7814\u7a76\u4e0e\u4e3b\u89c2\u8bc4\u4f30\u3002", "result": "MR-UBi\u7cfb\u7edf\u663e\u8457\u6539\u5584\u4e86\u6293\u53d6\u626d\u77e9\u63a7\u5236\u7684\u51c6\u786e\u6027\uff0c\u63d0\u5347\u4e86\u5728\u6700\u4f73\u626d\u77e9\u8303\u56f4\u5185\u7684\u65f6\u95f4\uff0c\u5e76\u51cf\u5c11\u4e86\u4f4e/high\u626d\u77e9\u8303\u56f4\u3002", "conclusion": "MR-UBi\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u548c\u89e6\u89c9\u53cd\u9988\uff0c\u4f7f\u6c34\u4e0b\u673a\u5668\u4eba\u81c2\u9065\u64cd\u4f5c\u66f4\u52a0\u7a33\u5b9a\u3001\u51c6\u786e\u4e14\u7528\u6237\u53cb\u597d\u3002"}}
{"id": "2510.20473", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.20473", "abs": "https://arxiv.org/abs/2510.20473", "authors": ["Bernhard Rameder", "Hubert Gattringer", "Andreas Mueller", "Ronald Naderer"], "title": "Robot Path and Trajectory Planning Considering a Spatially Fixed TCP", "comment": null, "summary": "This paper presents a method for planning a trajectory in workspace\ncoordinates using a spatially fixed tool center point (TCP), while taking into\naccount the processing path on a part. This approach is beneficial if it is\neasier to move the part rather than moving the tool. Whether a mathematical\ndescription that defines the shape to be processed or single points from a\ndesign program are used, the robot path is finally represented using B-splines.\nThe use of splines enables the path to be continuous with a desired degree,\nwhich finally leads to a smooth robot trajectory. While calculating the robot\ntrajectory through prescribed orientation, additionally a given velocity at the\nTCP has to be considered. The procedure was validated on a real system using an\nindustrial robot moving an arbitrary defined part.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u89c4\u5212\u673a\u5668\u4eba\u8f68\u8ff9\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528B\u6837\u6761\u5b9e\u73b0\u5149\u6ed1\u7684\u52a0\u5de5\u8def\u5f84\u3002", "motivation": "\u5728\u79fb\u52a8\u90e8\u4ef6\u6bd4\u79fb\u52a8\u5de5\u5177\u66f4\u7b80\u5355\u7684\u60c5\u51b5\u4e0b\uff0c\u89c4\u5212\u52a0\u5de5\u8def\u5f84", "method": "\u5728\u5de5\u4f5c\u7a7a\u95f4\u5750\u6807\u4e2d\u8ba1\u5212\u8f68\u8ff9\uff0c\u4f7f\u7528\u7a7a\u95f4\u56fa\u5b9a\u7684\u5de5\u5177\u4e2d\u5fc3\u70b9 (TCP)", "result": "\u673a\u5668\u4eba\u8def\u5f84\u901a\u8fc7\u4f7f\u7528B\u6837\u6761\u8868\u793a\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u4e00\u6761\u5e73\u6ed1\u7684\u673a\u5668\u4eba\u8f68\u8ff9", "conclusion": "\u901a\u8fc7\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u5730\u5b9e\u73b0\u4e86\u6240\u9700\u7684\u673a\u5668\u4eba\u8f68\u8ff9\u751f\u6210\u3002"}}
{"id": "2510.20480", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.20480", "abs": "https://arxiv.org/abs/2510.20480", "authors": ["V\u00e1clav Pritzl", "Xianjia Yu", "Tomi Westerlund", "Petr \u0160t\u011bp\u00e1n", "Martin Saska"], "title": "Degradation-Aware Cooperative Multi-Modal GNSS-Denied Localization Leveraging LiDAR-Based Robot Detections", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Accurate long-term localization using onboard sensors is crucial for robots\noperating in Global Navigation Satellite System (GNSS)-denied environments.\nWhile complementary sensors mitigate individual degradations, carrying all the\navailable sensor types on a single robot significantly increases the size,\nweight, and power demands. Distributing sensors across multiple robots enhances\nthe deployability but introduces challenges in fusing asynchronous, multi-modal\ndata from independently moving platforms. We propose a novel adaptive\nmulti-modal multi-robot cooperative localization approach using a factor-graph\nformulation to fuse asynchronous Visual-Inertial Odometry (VIO), LiDAR-Inertial\nOdometry (LIO), and 3D inter-robot detections from distinct robots in a\nloosely-coupled fashion. The approach adapts to changing conditions, leveraging\nreliable data to assist robots affected by sensory degradations. A novel\ninterpolation-based factor enables fusion of the unsynchronized measurements.\nLIO degradations are evaluated based on the approximate scan-matching Hessian.\nA novel approach of weighting odometry data proportionally to the Wasserstein\ndistance between the consecutive VIO outputs is proposed. A theoretical\nanalysis is provided, investigating the cooperative localization problem under\nvarious conditions, mainly in the presence of sensory degradations. The\nproposed method has been extensively evaluated on real-world data gathered with\nheterogeneous teams of an Unmanned Ground Vehicle (UGV) and Unmanned Aerial\nVehicles (UAVs), showing that the approach provides significant improvements in\nlocalization accuracy in the presence of various sensory degradations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u9002\u5e94\u591a\u673a\u5668\u4eba\u534f\u4f5c\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u6709\u6548\u878d\u5408\u591a\u79cd\u4f20\u611f\u5668\u6570\u636e\u4ee5\u63d0\u9ad8\u5b9a\u4f4d\u7cbe\u5ea6\u5e76\u5e94\u5bf9\u4f20\u611f\u5668\u9000\u5316\u3002", "motivation": "\u5728\u65e0GNSS\u7684\u73af\u5883\u4e2d\uff0c\u51c6\u786e\u7684\u957f\u671f\u5b9a\u4f4d\u5bf9\u673a\u5668\u4eba\u81f3\u5173\u91cd\u8981\uff0c\u800c\u5355\u4e00\u673a\u5668\u4eba\u643a\u5e26\u6240\u6709\u4f20\u611f\u5668\u4f1a\u5bfc\u81f4\u5c3a\u5bf8\u3001\u91cd\u91cf\u548c\u80fd\u8017\u7684\u589e\u52a0\u3002", "method": "\u4e00\u79cd\u57fa\u4e8e\u56e0\u5b50\u56fe\u7684\u81ea\u9002\u5e94\u591a\u6a21\u6001\u591a\u673a\u5668\u4eba\u534f\u4f5c\u5b9a\u4f4d\u65b9\u6cd5", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u4f20\u611f\u5668\u9000\u5316\u6761\u4ef6\u4e0b\u663e\u793a\u51fa\u663e\u8457\u63d0\u9ad8\u7684\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u5c24\u5176\u5728\u5f02\u6784\u7684\u65e0\u4eba\u5730\u9762\u8f66\u8f86\u548c\u65e0\u4eba\u673a\u56e2\u961f\u4e2d\u8fdb\u884c\u4e86\u6709\u6548\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5728\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\uff0c\u901a\u8fc7\u56e0\u5b50\u56fe\u7684\u677e\u8026\u5408\u65b9\u5f0f\uff0c\u5c06\u5f02\u6b65\u7684VIO\u3001LIO\u548c\u673a\u5668\u4eba\u95f4\u68c0\u6d4b\u878d\u5408\uff0c\u80fd\u591f\u663e\u8457\u6539\u5584\u5b9a\u4f4d\u7cbe\u5ea6\u3002"}}
{"id": "2510.20483", "categories": ["cs.RO", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.20483", "abs": "https://arxiv.org/abs/2510.20483", "authors": ["Victor Vantilborgh", "Hrishikesh Sathyanarayan", "Guillaume Crevecoeur", "Ian Abraham", "Tom Lefebvre"], "title": "Dual Control Reference Generation for Optimal Pick-and-Place Execution under Payload Uncertainty", "comment": null, "summary": "This work addresses the problem of robot manipulation tasks under unknown\ndynamics, such as pick-and-place tasks under payload uncertainty, where active\nexploration and(/for) online parameter adaptation during task execution are\nessential to enable accurate model-based control. The problem is framed as dual\ncontrol seeking a closed-loop optimal control problem that accounts for\nparameter uncertainty. We simplify the dual control problem by pre-defining the\nstructure of the feedback policy to include an explicit adaptation mechanism.\nThen we propose two methods for reference trajectory generation. The first\ndirectly embeds parameter uncertainty in robust optimal control methods that\nminimize the expected task cost. The second method considers minimizing the\nso-called optimality loss, which measures the sensitivity of parameter-relevant\ninformation with respect to task performance. We observe that both approaches\nreason over the Fisher information as a natural side effect of their\nformulations, simultaneously pursuing optimal task execution. We demonstrate\nthe effectiveness of our approaches for a pick-and-place manipulation task. We\nshow that designing the reference trajectories whilst taking into account the\ncontrol enables faster and more accurate task performance and system\nidentification while ensuring stable and efficient control.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\u6765\u4f18\u5316\u5177\u6709\u672a\u77e5\u52a8\u6001\u548c\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u63a7\u5236\uff0c\u901a\u8fc7\u8bbe\u8ba1\u53c2\u8003\u8f68\u8ff9\u6765\u63d0\u9ad8\u4efb\u52a1\u6267\u884c\u7684\u901f\u5ea6\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u5728\u672a\u77e5\u52a8\u6001\u4e0b\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\uff0c\u4f8b\u5982\u5728\u8d1f\u8f7d\u4e0d\u786e\u5b9a\u6027\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u7684\u62e3\u9009\u548c\u653e\u7f6e\u4efb\u52a1\uff0c\u9700\u8981\u5728\u4efb\u52a1\u6267\u884c\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u4e3b\u52a8\u63a2\u7d22\u548c\u5728\u7ebf\u53c2\u6570\u8c03\u6574\uff0c\u4ee5\u5b9e\u73b0\u51c6\u786e\u7684\u57fa\u4e8e\u6a21\u578b\u7684\u63a7\u5236\u3002", "method": "\u5c06\u53cc\u91cd\u63a7\u5236\u6846\u67b6\u7b80\u5316\u4e3a\u53cd\u9988\u7b56\u7565\u7684\u7ed3\u6784\u9884\u5b9a\u4e49\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u53c2\u8003\u8f68\u8ff9\u751f\u6210\u65b9\u6cd5\uff0c\u5206\u522b\u57fa\u4e8e\u9c81\u68d2\u6700\u4f18\u63a7\u5236\u548c\u6700\u5c0f\u5316\u6700\u4f18\u6027\u635f\u5931\u3002", "result": "\u63d0\u51fa\u4e86\u4e24\u4e2a\u53c2\u8003\u8f68\u8ff9\u751f\u6210\u65b9\u6cd5\uff0c\u5747\u8003\u8651\u4e86\u5728\u4efb\u52a1\u6267\u884c\u4e2d\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "\u901a\u8fc7\u8003\u8651\u63a7\u5236\u673a\u5236\u8bbe\u8ba1\u53c2\u8003\u8f68\u8ff9\uff0c\u53ef\u4ee5\u5b9e\u73b0\u66f4\u5feb\u901f\u3001\u66f4\u51c6\u786e\u7684\u4efb\u52a1\u6267\u884c\u548c\u7cfb\u7edf\u8bc6\u522b\uff0c\u540c\u65f6\u786e\u4fdd\u7a33\u5b9a\u548c\u9ad8\u6548\u7684\u63a7\u5236\u3002"}}
{"id": "2510.20490", "categories": ["cs.RO", "math.DS"], "pdf": "https://arxiv.org/pdf/2510.20490", "abs": "https://arxiv.org/abs/2510.20490", "authors": ["Thomas Kordik", "Hubert Gattringer", "Andreas Mueller"], "title": "Simultaneous Stiffness and Trajectory Optimization for Energy Minimization of Pick-and-Place Tasks of SEA-Actuated Parallel Kinematic Manipulators", "comment": null, "summary": "A major field of industrial robot applications deals with repetitive tasks\nthat alternate between operating points. For these so-called pick-and-place\noperations, parallel kinematic manipulators (PKM) are frequently employed.\nThese tasks tend to automatically run for a long period of time and therefore\nminimizing energy consumption is always of interest. Recent research addresses\nthis topic by the use of elastic elements and particularly series elastic\nactuators (SEA). This paper explores the possibilities of minimizing energy\nconsumption of SEA actuated PKM performing pick-and-place tasks. The basic idea\nis to excite eigenmotions that result from the actuator springs and exploit\ntheir oscillating characteristics. To this end, a prescribed cyclic\npick-and-place operation is analyzed and a dynamic model of SEA driven PKM is\nderived. Subsequently, an energy minimizing optimal control problem is\nformulated where operating trajectories as well as SEA stiffnesses are\noptimized simultaneously. Here, optimizing the actuator stiffness does not\naccount for variable stiffness actuators. It serves as a tool for the design\nand dimensioning process. The hypothesis on energy reduction is tested on two\n(parallel) robot applications where redundant actuation is also addressed. The\nresults confirm the validity of this approach.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u4f18\u5316SEA\u9a71\u52a8\u7684PKM\u5728\u62fe\u53d6\u548c\u653e\u7f6e\u64cd\u4f5c\u4e2d\u7684\u80fd\u91cf\u6d88\u8017\u3002", "motivation": "\u5173\u952e\u5728\u4e8e\u4f18\u5316\u5e76\u51cf\u5c11PKM\u5728\u91cd\u590d\u4efb\u52a1\u4e2d\u7684\u80fd\u91cf\u6d88\u8017\uff0c\u5c24\u5176\u662f\u4f7f\u7528SEA\u9a71\u52a8\u65f6\u7684\u80fd\u8017\u95ee\u9898", "method": "\u5f00\u53d1\u52a8\u6001\u6a21\u578b\u5e76-formulate energy minimizing optimal control problem", "result": "\u901a\u8fc7\u5bf9\u4e24\u4e2a\u5197\u4f59\u9a71\u52a8\u7684\u5e76\u8054\u673a\u5668\u4eba\u5e94\u7528\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u80fd\u91cf\u51cf\u5c11\u7684\u6709\u6548\u6027", "conclusion": "\u4f18\u5316SEA\u7684\u521a\u5ea6\u548c\u64cd\u4f5c\u8f68\u8ff9\u80fd\u591f\u663e\u8457\u964d\u4f4e\u80fd\u8017\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.20496", "categories": ["cs.RO", "math.DS"], "pdf": "https://arxiv.org/pdf/2510.20496", "abs": "https://arxiv.org/abs/2510.20496", "authors": ["Tobias Marauli", "Hubert Gattringer", "Andreas Mueller"], "title": "A Parameter-Linear Formulation of the Optimal Path Following Problem for Robotic Manipulator", "comment": null, "summary": "In this paper the computational challenges of time-optimal path following are\naddressed. The standard approach is to minimize the travel time, which\ninevitably leads to singularities at zero path speed, when reformulating the\noptimization problem in terms of a path parameter. Thus, smooth trajectory\ngeneration while maintaining a low computational effort is quite challenging,\nsince the singularities have to be taken into account. To this end, a different\napproach is presented in this paper. This approach is based on maximizing the\npath speed along a prescribed path. Furthermore, the approach is capable of\nplanning smooth trajectories numerically efficient. Moreover, the discrete\nreformulation of the underlying problem is linear in optimization variables.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5927\u5316\u8def\u5f84\u901f\u5ea6\u6765\u514b\u670d\u65f6\u95f4\u6700\u4f18\u8def\u5f84\u8ddf\u968f\u4e2d\u7684\u8ba1\u7b97\u6311\u6218\uff0c\u786e\u4fdd\u8f68\u8ff9\u751f\u6210\u7684\u5e73\u6ed1\u6027\u548c\u6570\u503c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u65f6\u95f4\u6700\u4f18\u8def\u5f84\u8ddf\u968f\u4e2d\u7684\u8ba1\u7b97\u6311\u6218\uff0c\u5c24\u5176\u662f\u907f\u514d\u5728\u96f6\u8def\u5f84\u901f\u5ea6\u65f6\u51fa\u73b0\u7684\u5947\u70b9\u3002", "method": "\u901a\u8fc7\u6700\u5927\u5316\u8def\u5f84\u901f\u5ea6\uff0c\u5e76\u8fdb\u884c\u6570\u503c\u9ad8\u6548\u7684\u89c4\u5212\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u89c4\u5212\u5e73\u6ed1\u8f68\u8ff9\uff0c\u5e76\u4e14\u4f18\u5316\u53d8\u91cf\u7684\u79bb\u6563\u91cd\u6784\u662f\u7ebf\u6027\u7684\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6700\u5927\u5316\u65e2\u5b9a\u8def\u5f84\u4e0a\u7684\u901f\u5ea6\u6765\u5b9e\u73b0\u5e73\u6ed1\u8f68\u8ff9\u751f\u6210\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.20529", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.20529", "abs": "https://arxiv.org/abs/2510.20529", "authors": ["Constantine Frost", "Chad Council", "Margaret McGuinness", "Nathaniel Hanson"], "title": "RubbleSim: A Photorealistic Structural Collapse Simulator for Confined Space Mapping", "comment": "Accepted to 2025 IEEE International Symposium on Safety, Security,\n  and Rescue Robotics", "summary": "Despite well-reported instances of robots being used in disaster response,\nthere is scant published data on the internal composition of the void spaces\nwithin structural collapse incidents. Data collected during these incidents is\nmired in legal constraints, as ownership is often tied to the responding\nagencies, with little hope of public release for research. While engineered\nrubble piles are used for training, these sites are also reluctant to release\ninformation about their proprietary training grounds. To overcome this access\nchallenge, we present RubbleSim -- an open-source, reconfigurable simulator for\nphotorealistic void space exploration. The design of the simulation assets is\ndirectly informed by visits to numerous training rubble sites at differing\nlevels of complexity. The simulator is implemented in Unity with\nmulti-operating system support. The simulation uses a physics-based approach to\nbuild stochastic rubble piles, allowing for rapid iteration between simulation\nworlds while retaining absolute knowledge of the ground truth. Using RubbleSim,\nwe apply a state-of-the-art structure-from-motion algorithm to illustrate how\nperception performance degrades under challenging visual conditions inside the\nemulated void spaces. Pre-built binaries and source code to implement are\navailable online: https://github.com/mit-ll/rubble_pile_simulator.", "AI": {"tldr": "RubbleSim\u662f\u4e00\u4e2a\u5f00\u6e90\u53ef\u91cd\u914d\u7f6e\u7684\u6a21\u62df\u5668\uff0c\u7528\u4e8e\u63a2\u7d22\u7ed3\u6784\u5012\u584c\u4e8b\u4ef6\u4e2d\u7684\u865a\u7a7a\u7a7a\u95f4\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u83b7\u53d6\u56f0\u96be\u7684\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u6cd5\u5f8b\u9650\u5236\u548c\u8bad\u7ec3\u573a\u5730\u4fe1\u606f\u4e0d\u516c\u5f00\uff0c\u83b7\u53d6\u6784\u9020\u5d29\u584c\u4e8b\u4ef6\u4e2d\u7684\u865a\u7a7a\u7a7a\u95f4\u6570\u636e\u975e\u5e38\u56f0\u96be\u3002", "method": "\u6a21\u62df\u5668\u4f7f\u7528\u7269\u7406\u4e3a\u57fa\u7840\u7684\u65b9\u6cd5\u6784\u5efa\u968f\u673a\u788e\u77f3\u5806\uff0c\u7ed3\u5408\u72b6\u6001\u6700\u5148\u8fdb\u7684\u8fd0\u52a8\u91cd\u5efa\u7b97\u6cd5\u5206\u6790\u611f\u77e5\u6027\u80fd\u3002", "result": "\u901a\u8fc7RubbleSim\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u5728\u6a21\u62df\u7684\u865a\u7a7a\u7a7a\u95f4\u4e2d\uff0c\u89c6\u89c9\u6761\u4ef6\u7684\u6311\u6218\u5982\u4f55\u5f71\u54cd\u611f\u77e5\u6027\u80fd\u3002", "conclusion": "RubbleSim\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\u6765\u7814\u7a76\u707e\u540e\u54cd\u5e94\u4e2d\u89c6\u89c9\u611f\u77e5\u7684\u4e0b\u964d\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u7684\u865a\u7a7a\u73af\u5883\u4e2d\u3002"}}
{"id": "2510.20685", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.20685", "abs": "https://arxiv.org/abs/2510.20685", "authors": ["Ming-Ming Yu", "Fei Zhu", "Wenzhuo Liu", "Yirong Yang", "Qunbo Wang", "Wenjun Wu", "Jing Liu"], "title": "C-NAV: Towards Self-Evolving Continual Object Navigation in Open World", "comment": null, "summary": "Embodied agents are expected to perform object navigation in dynamic,\nopen-world environments. However, existing approaches typically rely on static\ntrajectories and a fixed set of object categories during training, overlooking\nthe real-world requirement for continual adaptation to evolving scenarios. To\nfacilitate related studies, we introduce the continual object navigation\nbenchmark, which requires agents to acquire navigation skills for new object\ncategories while avoiding catastrophic forgetting of previously learned\nknowledge. To tackle this challenge, we propose C-Nav, a continual visual\nnavigation framework that integrates two key innovations: (1) A dual-path\nanti-forgetting mechanism, which comprises feature distillation that aligns\nmulti-modal inputs into a consistent representation space to ensure\nrepresentation consistency, and feature replay that retains temporal features\nwithin the action decoder to ensure policy consistency. (2) An adaptive\nsampling strategy that selects diverse and informative experiences, thereby\nreducing redundancy and minimizing memory overhead. Extensive experiments\nacross multiple model architectures demonstrate that C-Nav consistently\noutperforms existing approaches, achieving superior performance even compared\nto baselines with full trajectory retention, while significantly lowering\nmemory requirements. The code will be publicly available at\nhttps://bigtree765.github.io/C-Nav-project.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86C-Nav\u6846\u67b6\uff0c\u65e8\u5728\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e2d\u7269\u4f53\u5bfc\u822a\u7684\u6301\u7eed\u9002\u5e94\u6027\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\uff0c\u7269\u4f53\u5bfc\u822a\u6280\u80fd\u65e0\u6cd5\u6301\u7eed\u9002\u5e94\u65b0\u7269\u4f53\u7c7b\u522b\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faC-Nav\uff0c\u4e00\u4e2a\u8fde\u7eed\u89c6\u89c9\u5bfc\u822a\u6846\u67b6\uff0c\u7ed3\u5408\u53cc\u8def\u5f84\u53cd\u9057\u5fd8\u673a\u5236\u548c\u9002\u5e94\u6027\u91c7\u6837\u7b56\u7565\u3002", "result": "C-Nav\u5728\u591a\u4e2a\u6a21\u578b\u67b6\u6784\u4e0b\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5185\u5b58\u9700\u6c42\u663e\u8457\u964d\u4f4e\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "C-Nav\u6709\u6548\u7ed3\u5408\u4e86\u53cd\u9057\u5fd8\u673a\u5236\u4e0e\u9002\u5e94\u6027\u91c7\u6837\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5bf9\u65b0\u7269\u4f53\u7c7b\u522b\u7684\u5b66\u4e60\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u5148\u524d\u77e5\u8bc6\uff0c\u5177\u6709\u66f4\u4f4e\u7684\u5185\u5b58\u9700\u6c42\u3002"}}
{"id": "2510.20706", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20706", "abs": "https://arxiv.org/abs/2510.20706", "authors": ["Ganga Nair B", "Prakrut Kotecha", "Shishir Kolathaya"], "title": "Real-Time Gait Adaptation for Quadrupeds using Model Predictive Control and Reinforcement Learning", "comment": null, "summary": "Model-free reinforcement learning (RL) has enabled adaptable and agile\nquadruped locomotion; however, policies often converge to a single gait,\nleading to suboptimal performance. Traditionally, Model Predictive Control\n(MPC) has been extensively used to obtain task-specific optimal policies but\nlacks the ability to adapt to varying environments. To address these\nlimitations, we propose an optimization framework for real-time gait adaptation\nin a continuous gait space, combining the Model Predictive Path Integral (MPPI)\nalgorithm with a Dreamer module to produce adaptive and optimal policies for\nquadruped locomotion. At each time step, MPPI jointly optimizes the actions and\ngait variables using a learned Dreamer reward that promotes velocity tracking,\nenergy efficiency, stability, and smooth transitions, while penalizing abrupt\ngait changes. A learned value function is incorporated as terminal reward,\nextending the formulation to an infinite-horizon planner. We evaluate our\nframework in simulation on the Unitree Go1, demonstrating an average reduction\nof up to 36.48\\% in energy consumption across varying target speeds, while\nmaintaining accurate tracking and adaptive, task-appropriate gaits.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4f18\u5316\u6846\u67b6\uff0c\u7ed3\u5408MPPI\u548cDreamer\u6a21\u5757\uff0c\u5b9e\u73b0\u56db\u8db3\u673a\u5668\u4eba\u5b9e\u65f6\u6b65\u6001\u9002\u5e94\uff0c\u663e\u8457\u964d\u4f4e\u80fd\u8017\u5e76\u63d0\u9ad8\u6027\u80fd", "motivation": "\u89e3\u51b3\u6a21\u578b\u81ea\u7531\u5f3a\u5316\u5b66\u4e60\u4e2d\u6b65\u6001\u6536\u655b\u81f3\u5355\u4e00\u5f62\u5f0f\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u5728\u591a\u53d8\u73af\u5883\u4e2d\u7684\u9002\u5e94\u80fd\u529b", "method": "\u7ed3\u5408Model Predictive Path Integral (MPPI)\u7b97\u6cd5\u548cDreamer\u6a21\u5757\u8fdb\u884c\u5b9e\u65f6\u6b65\u6001\u9002\u5e94\u7684\u4f18\u5316\u6846\u67b6", "result": "\u5728Unitree Go1\u4e0a\u8fdb\u884c\u7684\u4eff\u771f\u5b9e\u9a8c\u663e\u793a\uff0c\u80fd\u91cf\u6d88\u8017\u5e73\u5747\u51cf\u5c11\u81f3\u591a36.48%\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u8ddf\u8e2a\u548c\u9002\u5e94\u6027\u6b65\u6001", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u63d0\u9ad8\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u7075\u6d3b\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2510.20808", "categories": ["cs.RO", "cs.AI", "cs.LG", "stat.ML", "I.2.6; I.2.8; I.2.9"], "pdf": "https://arxiv.org/pdf/2510.20808", "abs": "https://arxiv.org/abs/2510.20808", "authors": ["Elie Aljalbout", "Jiaxu Xing", "Angel Romero", "Iretiayo Akinola", "Caelan Reed Garrett", "Eric Heiden", "Abhishek Gupta", "Tucker Hermans", "Yashraj Narang", "Dieter Fox", "Davide Scaramuzza", "Fabio Ramos"], "title": "The Reality Gap in Robotics: Challenges, Solutions, and Best Practices", "comment": "Accepted for Publication as part of the Annual Review of Control,\n  Robotics, and Autonomous Systems 2026", "summary": "Machine learning has facilitated significant advancements across various\nrobotics domains, including navigation, locomotion, and manipulation. Many such\nachievements have been driven by the extensive use of simulation as a critical\ntool for training and testing robotic systems prior to their deployment in\nreal-world environments. However, simulations consist of abstractions and\napproximations that inevitably introduce discrepancies between simulated and\nreal environments, known as the reality gap. These discrepancies significantly\nhinder the successful transfer of systems from simulation to the real world.\nClosing this gap remains one of the most pressing challenges in robotics.\nRecent advances in sim-to-real transfer have demonstrated promising results\nacross various platforms, including locomotion, navigation, and manipulation.\nBy leveraging techniques such as domain randomization, real-to-sim transfer,\nstate and action abstractions, and sim-real co-training, many works have\novercome the reality gap. However, challenges persist, and a deeper\nunderstanding of the reality gap's root causes and solutions is necessary. In\nthis survey, we present a comprehensive overview of the sim-to-real landscape,\nhighlighting the causes, solutions, and evaluation metrics for the reality gap\nand sim-to-real transfer.", "AI": {"tldr": "\u673a\u5668\u5b66\u4e60\u63a8\u52a8\u4e86\u673a\u5668\u4eba\u9886\u57df\u7684\u53d1\u5c55\uff0c\u4f46\u6a21\u62df\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u7684\u5dee\u8ddd\u5f71\u54cd\u7cfb\u7edf\u8f6c\u79fb\uff0c\u672c\u6587\u7efc\u8ff0\u4e86\u89e3\u51b3\u65b9\u6848\u4e0e\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u5982\u4f55\u6709\u6548\u5730\u5c06\u673a\u5668\u4eba\u7cfb\u7edf\u4ece\u6a21\u62df\u73af\u5883\u8f6c\u79fb\u5230\u771f\u5b9e\u4e16\u754c\u4e2d\u6210\u4e3a\u4e00\u9879\u91cd\u8981\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5bf9\u6a21\u62df\u5230\u73b0\u5b9e\u8f6c\u79fb\u7684\u7814\u7a76\u8fdb\u884c\u5168\u9762\u7684\u6587\u732e\u7efc\u8ff0\uff0c\u5206\u6790\u73b0\u5b9e\u5dee\u8ddd\u53ca\u5176\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u7efc\u8ff0\u4e86\u591a\u79cd\u65b9\u6cd5\uff08\u5982\u9886\u57df\u968f\u673a\u5316\u548c\u5b9e\u5230\u6a21\u8f6c\u79fb\uff09\uff0c\u5c55\u793a\u4e86\u5728\u89e3\u51b3\u73b0\u5b9e\u5dee\u8ddd\u65b9\u9762\u53d6\u5f97\u7684\u8fdb\u5c55\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u4f9d\u7136\u5b58\u5728\u7684\u6311\u6218\u3002", "conclusion": "\u672c\u8bba\u6587\u7efc\u8ff0\u4e86\u6a21\u62df\u5230\u73b0\u5b9e\u8f6c\u79fb\u7684\u73b0\u72b6\uff0c\u5f3a\u8c03\u4e86\u73b0\u5b9e\u5dee\u8ddd\u7684\u6210\u56e0\u3001\u89e3\u51b3\u65b9\u6848\u53ca\u8bc4\u4f30\u6307\u6807\u3002"}}
{"id": "2510.20813", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20813", "abs": "https://arxiv.org/abs/2510.20813", "authors": ["Guangqi Jiang", "Haoran Chang", "Ri-Zhao Qiu", "Yutong Liang", "Mazeyu Ji", "Jiyue Zhu", "Zhao Dong", "Xueyan Zou", "Xiaolong Wang"], "title": "GSWorld: Closed-Loop Photo-Realistic Simulation Suite for Robotic Manipulation", "comment": null, "summary": "This paper presents GSWorld, a robust, photo-realistic simulator for robotics\nmanipulation that combines 3D Gaussian Splatting with physics engines. Our\nframework advocates \"closing the loop\" of developing manipulation policies with\nreproducible evaluation of policies learned from real-robot data and sim2real\npolicy training without using real robots. To enable photo-realistic rendering\nof diverse scenes, we propose a new asset format, which we term GSDF (Gaussian\nScene Description File), that infuses Gaussian-on-Mesh representation with\nrobot URDF and other objects. With a streamlined reconstruction pipeline, we\ncurate a database of GSDF that contains 3 robot embodiments for single-arm and\nbimanual manipulation, as well as more than 40 objects. Combining GSDF with\nphysics engines, we demonstrate several immediate interesting applications: (1)\nlearning zero-shot sim2real pixel-to-action manipulation policy with\nphoto-realistic rendering, (2) automated high-quality DAgger data collection\nfor adapting policies to deployment environments, (3) reproducible benchmarking\nof real-robot manipulation policies in simulation, (4) simulation data\ncollection by virtual teleoperation, and (5) zero-shot sim2real visual\nreinforcement learning. Website: https://3dgsworld.github.io/.", "AI": {"tldr": "GSWorld \u662f\u4e00\u79cd\u65b0\u578b\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u6a21\u62df\u5668\uff0c\u652f\u6301\u57fa\u4e8e\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\u8fdb\u884c\u53ef\u91cd\u590d\u7684\u653f\u7b56\u8bc4\u4f30\u548c\u65e0\u673a\u5668\u4eba\u8bad\u7ec3\u3002", "motivation": "\u901a\u8fc7\u521b\u5efa\u4e00\u4e2a\u9ad8\u4fdd\u771f\u4eff\u771f\u6846\u67b6\uff0c\u63a8\u52a8\u673a\u5668\u4eba\u64cd\u7eb5\u653f\u7b56\u7684\u5f00\u53d1\u548c\u8bc4\u4f30\uff0c\u4ee5\u4fc3\u8fdb\u4ece\u5b9e\u9645\u6570\u636e\u4e2d\u5b66\u4e60\u7684\u653f\u7b56\u4e0e\u4eff\u771f\u73af\u5883\u7684\u6709\u6548\u7ed3\u5408\u3002", "method": "\u672c\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u8d44\u4ea7\u683c\u5f0f GSDF\uff08\u9ad8\u65af\u573a\u666f\u63cf\u8ff0\u6587\u4ef6\uff09\uff0c\u7ed3\u5408\u4e86\u9ad8\u65af\u7f51\u683c\u8868\u793a\u4e0e\u673a\u5668\u4eba URDF \u53ca\u5176\u4ed6\u5bf9\u8c61\uff0c\u5e76\u901a\u8fc7\u7b80\u5316\u7684\u91cd\u5efa\u6d41\u7a0b\u6784\u5efa\u4e86\u5305\u542b\u591a\u4e2a\u673a\u5668\u4eba\u548c\u5bf9\u8c61\u7684\u6570\u636e\u5e93\u3002", "result": "GSWorld \u662f\u4e00\u79cd\u7ed3\u5408 3D \u9ad8\u65af\u70b9\u4e91\u4e0e\u7269\u7406\u5f15\u64ce\u7684\u5f3a\u5927\u3001\u903c\u771f\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u6a21\u62df\u5668\u3002", "conclusion": "GSWorld \u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u5728\u4e0d\u4f7f\u7528\u771f\u5b9e\u673a\u5668\u4eba\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u4eff\u771f\u589e\u5f3a\u673a\u5668\u4eba\u64cd\u63a7\u653f\u7b56\u7684\u5b66\u4e60\u548c\u8bc4\u4f30\u3002"}}
{"id": "2510.20818", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20818", "abs": "https://arxiv.org/abs/2510.20818", "authors": ["Mateo Guaman Castro", "Sidharth Rajagopal", "Daniel Gorbatov", "Matt Schmittle", "Rohan Baijal", "Octi Zhang", "Rosario Scalise", "Sidharth Talia", "Emma Romig", "Celso de Melo", "Byron Boots", "Abhishek Gupta"], "title": "VAMOS: A Hierarchical Vision-Language-Action Model for Capability-Modulated and Steerable Navigation", "comment": null, "summary": "A fundamental challenge in robot navigation lies in learning policies that\ngeneralize across diverse environments while conforming to the unique physical\nconstraints and capabilities of a specific embodiment (e.g., quadrupeds can\nwalk up stairs, but rovers cannot). We propose VAMOS, a hierarchical VLA that\ndecouples semantic planning from embodiment grounding: a generalist planner\nlearns from diverse, open-world data, while a specialist affordance model\nlearns the robot's physical constraints and capabilities in safe, low-cost\nsimulation. We enabled this separation by carefully designing an interface that\nlets a high-level planner propose candidate paths directly in image space that\nthe affordance model then evaluates and re-ranks. Our real-world experiments\nshow that VAMOS achieves higher success rates in both indoor and complex\noutdoor navigation than state-of-the-art model-based and end-to-end learning\nmethods. We also show that our hierarchical design enables cross-embodied\nnavigation across legged and wheeled robots and is easily steerable using\nnatural language. Real-world ablations confirm that the specialist model is key\nto embodiment grounding, enabling a single high-level planner to be deployed\nacross physically distinct wheeled and legged robots. Finally, this model\nsignificantly enhances single-robot reliability, achieving 3X higher success\nrates by rejecting physically infeasible plans. Website:\nhttps://vamos-vla.github.io/", "AI": {"tldr": "VAMOS\u662f\u4e00\u79cd\u5206\u5c42\u7684\u673a\u5668\u4eba\u5bfc\u822a\u5b66\u4e60\u67b6\u6784\uff0c\u5b83\u901a\u8fc7\u5c06\u8bed\u4e49\u89c4\u5212\u4e0e\u8eab\u4f53\u7ea6\u675f\u5206\u79bb\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u4e0d\u540c\u73af\u5883\u4e0b\u7684\u5bfc\u822a\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u7684\u653f\u7b56\u6cdb\u5316\u95ee\u9898\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u4e0d\u540c\u7269\u7406\u7ea6\u675f\u548c\u80fd\u529b\u7684\u673a\u5668\u4eba\u7684\u73af\u5883\u9002\u5e94\u6027\u3002", "method": "VAMOS\u91c7\u7528\u5c42\u6b21\u5316\u7684VLA\uff0c\u5c06\u901a\u7528\u89c4\u5212\u5668\u4e0e\u4e13\u95e8\u7684\u80fd\u529b\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u4f7f\u9ad8\u5c42\u6b21\u89c4\u5212\u80fd\u591f\u5728\u56fe\u50cf\u7a7a\u95f4\u4e2d\u76f4\u63a5\u63d0\u51fa\u5019\u9009\u8def\u5f84\uff0c\u5e76\u7531\u80fd\u529b\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u548c\u91cd\u65b0\u6392\u5e8f\u3002", "result": "\u5728\u5b9e\u9645\u5b9e\u9a8c\u4e2d\uff0cVAMOS\u5728\u5ba4\u5185\u548c\u590d\u6742\u5ba4\u5916\u5bfc\u822a\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6210\u529f\u7387\uff0c\u4e14\u8f83\u4f20\u7edf\u65b9\u6cd5\u5177\u67093\u500d\u7684\u6210\u529f\u7387\u63d0\u5347\u3002", "conclusion": "VAMOS\u901a\u8fc7\u5206\u5c42\u8bbe\u8ba1\u548c\u4e13\u95e8\u6a21\u578b\u7684\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6210\u529f\u7387\uff0c\u5e76\u652f\u6301\u8de8\u8eaf\u4f53\u7684\u5bfc\u822a\u80fd\u529b\u3002"}}
