{"id": "2511.03727", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03727", "abs": "https://arxiv.org/abs/2511.03727", "authors": ["Chenyu Hou", "Hua Yu", "Gaoxia Zhu", "John Derek Anas", "Jiao Liu", "Yew Soon Ong"], "title": "MazeMate: An LLM-Powered Chatbot to Support Computational Thinking in Gamified Programming Learning", "comment": null, "summary": "Computational Thinking (CT) is a foundational problem-solving skill, and\ngamified programming environments are a widely adopted approach to cultivating\nit. While large language models (LLMs) provide on-demand programming support,\ncurrent applications rarely foster CT development. We present MazeMate, an\nLLM-powered chatbot embedded in a 3D Maze programming game, designed to deliver\nadaptive, context-sensitive scaffolds aligned with CT processes in maze solving\nand maze design. We report on the first classroom implementation with 247\nundergraduates. Students rated MazeMate as moderately helpful, with higher\nperceived usefulness for maze solving than for maze design. Thematic analysis\nconfirmed support for CT processes such as decomposition, abstraction, and\nalgorithmic thinking, while also revealing limitations in supporting maze\ndesign, including mismatched suggestions and fabricated algorithmic solutions.\nThese findings demonstrate the potential of LLM-based scaffolding to support CT\nand underscore directions for design refinement to enhance MazeMate usability\nin authentic classrooms.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86MazeMate\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u804a\u5929\u673a\u5668\u4eba\uff0c\u65e8\u5728\u63d0\u5347\u8ba1\u7b97\u601d\u7ef4\uff0c\u5728\u8ff7\u5bab\u6c42\u89e3\u548c\u8bbe\u8ba1\u4e2d\u63d0\u4f9b\u652f\u6301\uff0c\u521d\u6b65\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u6709\u52a9\u4e8e\u8ba1\u7b97\u601d\u7ef4\u7684\u67d0\u4e9b\u8fc7\u7a0b\uff0c\u4f46\u5728\u8bbe\u8ba1\u6307\u5bfc\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\u3002", "motivation": "\u968f\u7740\u7f16\u7a0b\u73af\u5883\u7684\u6e38\u620f\u5316\uff0c\u5f53\u524d\u5e94\u7528\u5c1a\u672a\u5145\u5206\u4fc3\u8fdb\u8ba1\u7b97\u601d\u7ef4\u7684\u53d1\u5c55\uff0c\u4e9f\u9700\u4f18\u5316\u3002", "method": "\u901a\u8fc7\u5728247\u540d\u672c\u79d1\u751f\u4e2d\u5b9e\u65bdMazeMate\u8fdb\u884c\u8bfe\u5802\u5b9e\u9a8c\uff0c\u6536\u96c6\u5b66\u751f\u5bf9\u5176\u5e2e\u52a9\u7a0b\u5ea6\u7684\u53cd\u9988\u3002", "result": "\u5b66\u751f\u5bf9MazeMate\u7684\u8bc4\u4ef7\u4e2d\u7b49\uff0c\u8ff7\u5bab\u6c42\u89e3\u7684\u611f\u77e5\u6548\u7528\u9ad8\u4e8e\u8ff7\u5bab\u8bbe\u8ba1\uff0c\u5e76\u5728\u4e3b\u9898\u5206\u6790\u4e2d\u53d1\u73b0\u652f\u6491\u8ba1\u7b97\u601d\u7ef4\u7684\u8fc7\u7a0b\uff0c\u5b58\u5728\u5efa\u8bae\u4e0d\u5339\u914d\u548c\u865a\u6784\u7b97\u6cd5\u89e3\u51b3\u65b9\u6848\u7684\u5c40\u9650\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u652f\u67b6\u5728\u652f\u6301\u8ba1\u7b97\u601d\u7ef4\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u8ff7\u5bab\u8bbe\u8ba1\u65b9\u9762\u5b58\u5728\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2511.03728", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03728", "abs": "https://arxiv.org/abs/2511.03728", "authors": ["Sanidhya Vijayvargiya", "Rahul Lokesh"], "title": "Efficient On-Device Agents via Adaptive Context Management", "comment": "27 pages, 5 figures", "summary": "On-device AI agents offer the potential for personalized, low-latency\nassistance, but their deployment is fundamentally constrained by limited memory\ncapacity, which restricts usable context. This reduced practical context window\ncreates a trade-off between supporting rich, stateful interactions with complex\ntool capabilities and maintaining on-device feasibility. We break this\ntrade-off with a framework for context-efficient on-device agents, driven by\nthree synergistic optimizations (1) a dynamic memory system using specialized\nLoRA adapters to distill conversational history into a compressed, and\nstructured Context State Object; (2) a minimalist serialization format for tool\nschemas to minimize token overhead per tool; and (3) a just-in-time\nschema-passing mechanism that loads full tool definitions only upon tool\nselection. We instantiate this framework by adapting a 3B parameter SLM to\ncontext-efficient trajectories and rigorously evaluate it against a\nconventional baseline on complex user tasks. Our agent matches, or exceeds, the\nperformance of a conventional baseline while dramatically compressing context,\nachieving more than a 6-fold reduction in initial system prompt context and a\n10- to 25-fold reduction in context growth rate based on the interaction\nverbosity, demonstrating that strategic context management is key to unlocking\ncapable and persistent on-device AI.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u5185\u5b58\u548c\u4e0a\u4e0b\u6587\u7ba1\u7406\uff0c\u63d0\u9ad8\u4e86\u8bbe\u5907\u7aefAI\u4ee3\u7406\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u8bbe\u5907\u7aefAI\u4ee3\u7406\u6f5c\u5728\u7684\u4e2a\u6027\u5316\u548c\u4f4e\u5ef6\u8fdf\u8f85\u52a9\u529f\u80fd\u53d7\u5230\u5185\u5b58\u5bb9\u91cf\u9650\u5236\u7684\u5236\u7ea6\uff0c\u5bfc\u81f4\u53ef\u7528\u4e0a\u4e0b\u6587\u53d7\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u5185\u5b58\u7cfb\u7edf\u3001\u6700\u7b80\u5316\u5e8f\u5217\u5316\u683c\u5f0f\u548c\u5373\u65f6\u6a21\u5f0f\u4f20\u9012\u673a\u5236\u7684\u6846\u67b6\uff0c\u4ee5\u4f18\u5316\u8bbe\u5907\u7aef\u4ee3\u7406\u7684\u4e0a\u4e0b\u6587\u6548\u7387\u3002", "result": "\u8be5\u4ee3\u7406\u7684\u8868\u73b0\u4e0e\u5e38\u89c4\u57fa\u51c6\u76f8\u5339\u914d\u6216\u8d85\u8d8a\uff0c\u540c\u65f6\u663e\u8457\u538b\u7f29\u4e0a\u4e0b\u6587\uff0c\u5b9e\u73b0\u4e86\u8d85\u8fc76\u500d\u7684\u521d\u59cb\u7cfb\u7edf\u63d0\u793a\u4e0a\u4e0b\u6587\u538b\u7f29\u548c10\u523025\u500d\u7684\u4e0a\u4e0b\u6587\u589e\u957f\u7387\u51cf\u5c11\u3002", "conclusion": "\u901a\u8fc7\u6218\u7565\u6027\u4e0a\u4e0b\u6587\u7ba1\u7406\uff0c\u53ef\u4ee5\u91ca\u653e\u5177\u5907\u80fd\u529b\u548c\u6301\u4e45\u6027\u7684\u8bbe\u5907\u7aefAI\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.03729", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03729", "abs": "https://arxiv.org/abs/2511.03729", "authors": ["Zhiyin Zhou"], "title": "Beyond Chat: a Framework for LLMs as Human-Centered Support Systems", "comment": null, "summary": "Large language models are moving beyond transactional question answering to\nact as companions, coaches, mediators, and curators that scaffold human growth,\ndecision-making, and well-being. This paper proposes a role-based framework for\nhuman-centered LLM support systems, compares real deployments across domains,\nand identifies cross-cutting design principles: transparency, personalization,\nguardrails, memory with privacy, and a balance of empathy and reliability. It\noutlines evaluation metrics that extend beyond accuracy to trust, engagement,\nand longitudinal outcomes. It also analyzes risks including over-reliance,\nhallucination, bias, privacy exposure, and unequal access, and proposes future\ndirections spanning unified evaluation, hybrid human-AI models, memory\narchitectures, cross-domain benchmarking, and governance. The goal is to\nsupport responsible integration of LLMs in sensitive settings where people need\naccompaniment and guidance, not only answers.", "AI": {"tldr": "\u672c\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u4eba\u7c7b\u4e2d\u5fc3\u7684\u652f\u6301\u7cfb\u7edf\uff0c\u63d0\u51fa\u4e86\u8bbe\u8ba1\u539f\u5219\u53ca\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u6f5c\u5728\u98ce\u9669\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u4e3a\u4e86\u652f\u6301\u4eba\u7c7b\u7684\u6210\u957f\u3001\u51b3\u7b56\u548c\u798f\u7949\uff0c\u63a8\u52a8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8d85\u8d8a\u4f20\u7edf\u7684\u95ee\u7b54\u89d2\u8272\u3002", "method": "\u901a\u8fc7\u63d0\u51fa\u57fa\u4e8e\u89d2\u8272\u7684\u6846\u67b6\u548c\u8bc4\u4f30\u8de8\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u5206\u6790\u8bbe\u8ba1\u539f\u5219\u548c\u8bc4\u4ef7\u6307\u6807\u3002", "result": "\u63d0\u51fa\u4e86\u8de8\u9886\u57df\u7684\u8bbe\u8ba1\u539f\u5219\u3001\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u5206\u6790\u4e86\u6f5c\u5728\u98ce\u9669\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "conclusion": "\u672c\u8bba\u6587\u5f3a\u8c03\u4e86\u5728\u654f\u611f\u73af\u5883\u4e2d\u5f15\u5165\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\uff0c\u9700\u8981\u8d1f\u8d23\u4efb\u5730\u6574\u5408\u4ee5\u786e\u4fdd\u9002\u5f53\u7684\u966a\u4f34\u548c\u6307\u5bfc\u3002"}}
{"id": "2511.03730", "categories": ["cs.HC", "cs.AI", "I.2.0"], "pdf": "https://arxiv.org/pdf/2511.03730", "abs": "https://arxiv.org/abs/2511.03730", "authors": ["Joe Shymanski", "Jacob Brue", "Sandip Sen"], "title": "Not All Explanations are Created Equal: Investigating the Pitfalls of Current XAI Evaluation", "comment": "The authors' accepted manuscript of Chapter 9 in Bi-directionality in\n  Human-AI Collaborative Systems (Springer, 2025). The final published version\n  is available at https://doi.org/10.1016/B978-0-44-340553-2.00015-0. 27 pages,\n  12 figures, 3 tables", "summary": "Explainable Artificial Intelligence (XAI) aims to create transparency in\nmodern AI models by offering explanations of the models to human users. There\nare many ways in which researchers have attempted to evaluate the quality of\nthese XAI models, such as user studies or proposed objective metrics like\n\"fidelity\". However, these current XAI evaluation techniques are ad hoc at best\nand not generalizable. Thus, most studies done within this field conduct simple\nuser surveys to analyze the difference between no explanations and those\ngenerated by their proposed solution. We do not find this to provide adequate\nevidence that the explanations generated are of good quality since we believe\nany kind of explanation will be \"better\" in most metrics when compared to none\nat all. Thus, our study looks to highlight this pitfall: most explanations,\nregardless of quality or correctness, will increase user satisfaction. We also\npropose that emphasis should be placed on actionable explanations. We\ndemonstrate the validity of both of our claims using an agent assistant to\nteach chess concepts to users. The results of this chapter will act as a call\nto action in the field of XAI for more comprehensive evaluation techniques for\nfuture research in order to prove explanation quality beyond user satisfaction.\nAdditionally, we present an analysis of the scenarios in which placebic or\nactionable explanations would be most useful.", "AI": {"tldr": "\u672c\u7814\u7a76\u6307\u51fa\u5f53\u524dXAI\u6a21\u578b\u8bc4\u4f30\u7684\u4e0d\u8db3\u4e4b\u5904\uff0c\u5f3a\u8c03\u5fc5\u987b\u5f00\u53d1\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6280\u672f\uff0c\u4ee5\u9a8c\u8bc1\u89e3\u91ca\u8d28\u91cf\uff0c\u800c\u4e0d\u4ec5\u4ec5\u4f9d\u8d56\u4e8e\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "motivation": "\u5f53\u524dXAI\u8bc4\u4f30\u6280\u672f\u5b58\u5728\u4e34\u65f6\u6027\u548c\u5c40\u9650\u6027\uff0c\u5927\u591a\u6570\u7814\u7a76\u4f9d\u8d56\u7b80\u5355\u7684\u7528\u6237\u8c03\u67e5\u6765\u6bd4\u8f83\u6709\u65e0\u89e3\u91ca\u7684\u6548\u679c\uff0c\u8fd9\u6837\u7684\u8bc4\u4f30\u4e0d\u8db3\u4ee5\u8bc1\u660e\u89e3\u91ca\u7684\u8d28\u91cf\u3002", "method": "\u4f7f\u7528\u4ee3\u7406\u52a9\u624b\u6559\u6388\u7528\u6237\u56fd\u9645\u8c61\u68cb\u6982\u5ff5\u7684\u5b9e\u9a8c\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u7814\u7a76\uff0c\u53d1\u73b0\u5927\u591a\u6570\u89e3\u91ca\u65e0\u8bba\u8d28\u91cf\u5982\u4f55\uff0c\u90fd\u80fd\u63d0\u5347\u7528\u6237\u6ee1\u610f\u5ea6\uff0c\u5e76\u5f3a\u8c03\u91c7\u53d6\u6709\u884c\u52a8\u6027\u7684\u89e3\u91ca\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u672a\u6765\u7684XAI\u7814\u7a76\u9700\u8981\u5efa\u7acb\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6280\u672f\uff0c\u4ee5\u4fbf\u80fd\u591f\u8bc4\u5224\u89e3\u91ca\u8d28\u91cf\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u7528\u6237\u6ee1\u610f\u5ea6\u3002"}}
{"id": "2511.03931", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.03931", "abs": "https://arxiv.org/abs/2511.03931", "authors": ["Iman Adibnazari", "Harsh Sharma", "Myungsun Park", "Jacobo Cervera-Torralba", "Boris Kramer", "Michael T. Tolley"], "title": "Dynamic Shape Control of Soft Robots Enabled by Data-Driven Model Reduction", "comment": "20 Pages, 8 Figures", "summary": "Soft robots have shown immense promise in settings where they can leverage\ndynamic control of their entire bodies. However, effective dynamic shape\ncontrol requires a controller that accounts for the robot's high-dimensional\ndynamics--a challenge exacerbated by a lack of general-purpose tools for\nmodeling soft robots amenably for control. In this work, we conduct a\ncomparative study of data-driven model reduction techniques for generating\nlinear models amendable to dynamic shape control. We focus on three\nmethods--the eigensystem realization algorithm, dynamic mode decomposition with\ncontrol, and the Lagrangian operator inference (LOpInf) method. Using each\nclass of model, we explored their efficacy in model predictive control policies\nfor the dynamic shape control of a simulated eel-inspired soft robot in three\nexperiments: 1) tracking simulated reference trajectories guaranteed to be\nfeasible, 2) tracking reference trajectories generated from a biological model\nof eel kinematics, and 3) tracking reference trajectories generated by a\nreduced-scale physical analog. In all experiments, the LOpInf-based policies\ngenerated lower tracking errors than policies based on other models.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5bf9\u6bd4\u4e09\u79cd\u6a21\u578b\u964d\u7ef4\u6280\u672f\uff0c\u663e\u793aLOpInf\u5728\u8f6f\u673a\u5668\u4eba\u52a8\u6001\u5f62\u72b6\u63a7\u5236\u4e2d\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u89e3\u51b3\u8f6f\u673a\u5668\u4eba\u52a8\u6001\u63a7\u5236\u4e2d\u7684\u9ad8\u7ef4\u52a8\u6001\u5efa\u6a21\u96be\u9898\uff0c\u63a2\u7d22\u6709\u6548\u7684\u63a7\u5236\u5de5\u5177\u3002", "method": "\u6bd4\u8f83\u4e09\u79cd\u6570\u636e\u9a71\u52a8\u6a21\u578b\u964d\u7ef4\u6280\u672f\uff1a\u7279\u5f81\u7cfb\u7edf\u5b9e\u73b0\u7b97\u6cd5\u3001\u5e26\u63a7\u5236\u7684\u52a8\u6001\u6a21\u6001\u5206\u89e3\u548cLagrangian\u7b97\u5b50\u63a8\u65ad\u6cd5\u3002", "result": "\u5728\u6a21\u62df\u7684\u8f6f\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\uff0cLOpInf\u65b9\u6cd5\u5728\u8ddf\u8e2a\u8bef\u5dee\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u4e24\u79cd\u6a21\u578b\u3002", "conclusion": "LOpInf\u65b9\u6cd5\u5728\u52a8\u6001\u5f62\u72b6\u63a7\u5236\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u80fd\u591f\u6709\u6548\u964d\u4f4e\u8ddf\u8e2a\u8bef\u5dee\u3002"}}
{"id": "2511.03731", "categories": ["cs.HC", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.03731", "abs": "https://arxiv.org/abs/2511.03731", "authors": ["Fengming Liu", "Shubin Yu"], "title": "MimiTalk: Revolutionizing Qualitative Research with Dual-Agent AI", "comment": "30 pages", "summary": "We present MimiTalk, a dual-agent constitutional AI framework designed for\nscalable and ethical conversational data collection in social science research.\nThe framework integrates a supervisor model for strategic oversight and a\nconversational model for question generation. We conducted three studies: Study\n1 evaluated usability with 20 participants; Study 2 compared 121 AI interviews\nto 1,271 human interviews from the MediaSum dataset using NLP metrics and\npropensity score matching; Study 3 involved 10 interdisciplinary researchers\nconducting both human and AI interviews, followed by blind thematic analysis.\nResults across studies indicate that MimiTalk reduces interview anxiety,\nmaintains conversational coherence, and outperforms human interviews in\ninformation richness, coherence, and stability. AI interviews elicit technical\ninsights and candid views on sensitive topics, while human interviews better\ncapture cultural and emotional nuances. These findings suggest that dual-agent\nconstitutional AI supports effective human-AI collaboration, enabling\nreplicable, scalable and quality-controlled qualitative research.", "AI": {"tldr": "MimiTalk\u662f\u4e00\u4e2a\u53cc\u4ee3\u7406\u5baa\u6cd5AI\u6846\u67b6\uff0c\u4f18\u5316\u4e86\u5b9a\u6027\u7814\u7a76\u4e2d\u7684\u4eba\u673a\u4e92\u52a8\uff0c\u63d0\u9ad8\u4e86\u8bbf\u8c08\u7684\u8d28\u91cf\u548c\u6709\u6548\u6027\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u53ef\u6269\u5c55\u548c\u4f26\u7406\u7684\u5bf9\u8bdd\u6570\u636e\u6536\u96c6\u6846\u67b6\uff0c\u4ee5\u652f\u6301\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u4e09\u9879\u7814\u7a76\u8bc4\u4f30MimiTalk\u6846\u67b6\uff1a1) \u8bc4\u4f3020\u540d\u53c2\u4e0e\u8005\u7684\u53ef\u7528\u6027\uff1b2) \u5bf9\u6bd4121\u4e2aAI\u8bbf\u8c08\u548c1271\u4e2a\u4eba\u7c7b\u8bbf\u8c08\uff1b3) 10\u540d\u8de8\u5b66\u79d1\u7814\u7a76\u8005\u8fdb\u884c\u4eba\u673a\u8bbf\u8c08\u540e\u8fdb\u884c\u76f2\u76ee\u4e3b\u9898\u5206\u6790\u3002", "result": "MimiTalk\u5728\u51cf\u8f7b\u8bbf\u8c08\u7126\u8651\u3001\u4fdd\u6301\u5bf9\u8bdd\u8fde\u8d2f\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4e14\u5728\u4fe1\u606f\u4e30\u5bcc\u6027\u3001\u8fde\u8d2f\u6027\u548c\u7a33\u5b9a\u6027\u4e0a\u8d85\u8fc7\u4eba\u7c7b\u8bbf\u8c08\u3002AI\u8bbf\u8c08\u80fd\u5f15\u51fa\u6280\u672f\u89c1\u89e3\u4e0e\u654f\u611f\u8bdd\u9898\u7684\u5766\u8bda\u770b\u6cd5\uff0c\u800c\u4eba\u7c7b\u8bbf\u8c08\u5219\u66f4\u80fd\u6355\u6349\u6587\u5316\u548c\u60c5\u611f\u7684\u7ec6\u5fae\u5dee\u522b\u3002", "conclusion": "\u53cc\u4ee3\u7406\u5baa\u6cd5AI\u652f\u6301\u6709\u6548\u7684\u4eba\u673a\u5408\u4f5c\uff0c\u80fd\u591f\u5b9e\u73b0\u53ef\u590d\u5236\u3001\u53ef\u6269\u5c55\u548c\u8d28\u91cf\u63a7\u5236\u7684\u5b9a\u6027\u7814\u7a76\u3002"}}
{"id": "2511.03996", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.03996", "abs": "https://arxiv.org/abs/2511.03996", "authors": ["Yushi Wang", "Changsheng Luo", "Penghui Chen", "Jianran Liu", "Weijian Sun", "Tong Guo", "Kechang Yang", "Biao Hu", "Yangang Zhang", "Mingguo Zhao"], "title": "Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots", "comment": "Project page: https://humanoid-kick.github.io", "summary": "Humanoid soccer poses a representative challenge for embodied intelligence,\nrequiring robots to operate within a tightly coupled perception-action loop.\nHowever, existing systems typically rely on decoupled modules, resulting in\ndelayed responses and incoherent behaviors in dynamic environments, while\nreal-world perceptual limitations further exacerbate these issues. In this\nwork, we present a unified reinforcement learning-based controller that enables\nhumanoid robots to acquire reactive soccer skills through the direct\nintegration of visual perception and motion control. Our approach extends\nAdversarial Motion Priors to perceptual settings in real-world dynamic\nenvironments, bridging motion imitation and visually grounded dynamic control.\nWe introduce an encoder-decoder architecture combined with a virtual perception\nsystem that models real-world visual characteristics, allowing the policy to\nrecover privileged states from imperfect observations and establish active\ncoordination between perception and action. The resulting controller\ndemonstrates strong reactivity, consistently executing coherent and robust\nsoccer behaviors across various scenarios, including real RoboCup matches.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u89c6\u89c9\u611f\u77e5\u4e0e\u8fd0\u52a8\u63a7\u5236\u7684\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5668\uff0c\u6210\u529f\u5e94\u5bf9\u4eba\u5f62\u8db3\u7403\u6311\u6218\u3002", "motivation": "\u4eba\u5f62\u8db3\u7403\u5bf9\u5177\u8eab\u667a\u80fd\u6784\u6210\u4e86\u6311\u6218\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u7cfb\u7edf\u6a21\u5757\u8026\u5408\u4e0d\u826f\u3001\u53cd\u5e94\u8fdf\u7f13\u548c\u884c\u4e3a\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u7edf\u4e00\u7684\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5668\uff0c\u76f4\u63a5\u6574\u5408\u89c6\u89c9\u611f\u77e5\u548c\u8fd0\u52a8\u63a7\u5236\uff0c\u5e76\u6269\u5c55\u5bf9\u6297\u8fd0\u52a8\u5148\u9a8c\u81f3\u73b0\u5b9e\u4e16\u754c\u52a8\u6001\u73af\u5883\u4e2d\u7684\u611f\u77e5\u8bbe\u7f6e\u3002", "result": "\u8be5\u63a7\u5236\u5668\u5728\u591a\u79cd\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u53cd\u5e94\u6027\uff0c\u80fd\u591f\u9ad8\u6548\u6267\u884c\u673a\u5668\u4eba\u8db3\u7403\u8fd0\u52a8\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u63a7\u5236\u5668\u5728\u5404\u79cd\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u53cd\u5e94\u80fd\u529b\uff0c\u80fd\u591f\u4e00\u81f4\u5730\u6267\u884c\u8fde\u8d2f\u4e14\u7a33\u5065\u7684\u8db3\u7403\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5728\u771f\u5b9e\u7684RoboCup\u6bd4\u8d5b\u4e2d\u3002"}}
{"id": "2511.03732", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03732", "abs": "https://arxiv.org/abs/2511.03732", "authors": ["Hans Schumann", "Louis Rosenberg", "Ganesh Mani", "Gregg Willcox"], "title": "Conversational Collective Intelligence (CCI) using Hyperchat AI in an Authentic Forecasting Task", "comment": null, "summary": "Hyperchat AI is a novel agentic technology that enables thoughtful\nconversations among networked human groups of potentially unlimited size. It\nallows large teams to discuss complex issues, brainstorm ideas, surface risks,\nassess alternatives and efficiently converge on optimized solutions that\namplify the group's Collective Intelligence (CI). A formal study was conducted\nto quantify the forecasting accuracy of human groups using Hyperchat AI to\nconversationally predict the outcome of Major League Baseball (MLB) games.\nDuring an 8-week period, networked groups of approximately 24 sports fans were\ntasked with collaboratively forecasting the winners of 59 baseball games\nthrough real-time conversation facilitated by AI agents. The results showed\nthat when debating the games using Hyperchat AI technology, the groups\nconverged on High Confidence predictions that significantly outperformed Vegas\nbetting markets. Specifically, groups were 78% accurate in their High\nConfidence picks, a statistically strong result vs the Vegas odds of 57%\n(p=0.020). Had the groups bet against the spread (ATS) on these games, they\nwould have achieved a 46% ROI against Vegas betting markets. In addition, High\nConfidence forecasts that were generated through above-average conversation\nrates were 88% accurate, suggesting that real-time interactive deliberation is\ncentral to amplified accuracy.", "AI": {"tldr": "Hyperchat AI\u901a\u8fc7\u4fc3\u8fdb\u5b9e\u65f6\u5bf9\u8bdd\uff0c\u5e2e\u52a9\u4eba\u7fa4\u66f4\u51c6\u786e\u5730\u9884\u6d4bMLB\u6bd4\u8d5b\u7ed3\u679c\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u5728\u5927\u578b\u4eba\u7fa4\u4e2d\u4fc3\u8fdb\u6df1\u5165\u8ba8\u8bba\u3001\u534f\u4f5c\u548c\u4f18\u5316\u51b3\u7b56\u7684\u667a\u80fd\u5de5\u5177\u3002", "method": "\u8fdb\u884c\u4e86\u4e3a\u671f8\u5468\u7684\u7814\u7a76\uff0c28\u540d\u4f53\u80b2\u8ff7\u901a\u8fc7Hyperchat AI\u8fdb\u884c59\u573aMLB\u6bd4\u8d5b\u7684\u9884\u6d4b\u3002", "result": "\u4f7f\u7528Hyperchat AI\u7684\u7ec4\u5728\u9884\u6d4b\u51c6\u786e\u7387\u4e0a\u8fbe\u523078%\uff0c\u663e\u8457\u8d85\u8fc7\u4e86\u535a\u5f69\u5e02\u573a\u768457%\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u65f6\u5bf9\u8bdd\uff0cHyperchat AI\u663e\u8457\u63d0\u9ad8\u4e86\u4eba\u7fa4\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002"}}
{"id": "2511.04009", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04009", "abs": "https://arxiv.org/abs/2511.04009", "authors": ["Chenzui Li", "Yiming Chen", "Xi Wu", "Giacinto Barresi", "Fei Chen"], "title": "Integrating Ergonomics and Manipulability for Upper Limb Postural Optimization in Bimanual Human-Robot Collaboration", "comment": "7 pages, 7 figures, IROS 2025 accepted", "summary": "This paper introduces an upper limb postural optimization method for\nenhancing physical ergonomics and force manipulability during bimanual\nhuman-robot co-carrying tasks. Existing research typically emphasizes human\nsafety or manipulative efficiency, whereas our proposed method uniquely\nintegrates both aspects to strengthen collaboration across diverse conditions\n(e.g., different grasping postures of humans, and different shapes of objects).\nSpecifically, the joint angles of a simplified human skeleton model are\noptimized by minimizing the cost function to prioritize safety and manipulative\ncapability. To guide humans towards the optimized posture, the reference\nend-effector poses of the robot are generated through a transformation module.\nA bimanual model predictive impedance controller (MPIC) is proposed for our\nhuman-like robot, CURI, to recalibrate the end effector poses through planned\ntrajectories. The proposed method has been validated through various subjects\nand objects during human-human collaboration (HHC) and human-robot\ncollaboration (HRC). The experimental results demonstrate significant\nimprovement in muscle conditions by comparing the activation of target muscles\nbefore and after optimization.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0a\u80a2\u59ff\u52bf\u4f18\u5316\u65b9\u6cd5\uff0c\u4ee5\u589e\u5f3a\u4eba\u673a\u534f\u4f5c\u8fc7\u7a0b\u4e2d\u7684\u8eab\u4f53\u5de5\u6548\u5b66\u548c\u64cd\u63a7\u80fd\u529b\uff0c\u7ecf\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\u4e86\u826f\u597d\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4eba\u7c7b\u5b89\u5168\u6216\u64cd\u4f5c\u6548\u7387\uff0c\u800c\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u540c\u65f6\u6574\u5408\u4e86\u8fd9\u4e24\u4e2a\u65b9\u9762\uff0c\u4ee5\u4fc3\u8fdb\u4eba\u673a\u534f\u4f5c\u3002", "method": "\u5bf9\u7b80\u5316\u4eba\u7c7b\u9aa8\u9abc\u6a21\u578b\u7684\u5173\u8282\u89d2\u5ea6\u8fdb\u884c\u4f18\u5316\uff0c\u4f7f\u7528\u9884\u6d4b\u963b\u6297\u63a7\u5236\u5668\u8c03\u8282\u673a\u5668\u4eba\u7684\u672b\u7aef\u6267\u884c\u5668\u59ff\u6001\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u4f18\u5316\u540e\u76ee\u6807\u808c\u8089\u7684\u6fc0\u6d3b\u60c5\u51b5\u663e\u8457\u6539\u5584\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u9ad8\u4e86\u808c\u8089\u72b6\u6001\uff0c\u5e76\u5728\u4e0d\u540c\u5408\u4f5c\u60c5\u5883\u4e0b\u8868\u73b0\u51fa\u826f\u597d\u7684\u4f18\u5316\u6548\u679c\u3002"}}
{"id": "2511.03733", "categories": ["cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.03733", "abs": "https://arxiv.org/abs/2511.03733", "authors": ["Pratham Gandhi"], "title": "HACI: A Haptic-Audio Code Interface to Improve Educational Outcomes for Visually Impaired Introductory Programming Students", "comment": null, "summary": "This thesis introduces the Haptic-Audio Code Interface (HACI), an educational\ntool designed to enhance programming education for visually impaired (VI)\nstudents by integrating haptic and audio feedback to compensate for the absence\nof visual cues. HACI consists of a non-resource-intensive web application\nsupporting JavaScript program development, execution, and debugging, connected\nvia a cable to an Arduino-powered glove with six integrated haptic motors to\nprovide physical feedback to VI programmers. Motivated by the need to provide\nequitable educational opportunities in computer science, HACI aims to improve\nnon-visual code navigation, comprehension, summarizing, editing, and debugging\nfor students with visual impairments while minimizing cognitive load. This work\ndetails HACI's design principles, technical implementation, and a preliminary\nevaluation through a pilot study conducted with undergraduate Computer Science\nstudents. Findings indicate that HACI aids in the non-visual navigation and\nunderstanding of programming constructs, although challenges remain in refining\nfeedback mechanisms to ensure consistency and reliability, as well as\nsupplementing the current functionality with a more feature-reach and\ncustomizable accessible learning experience which will allow visually impaired\nstudents to fully utilize interleaved haptic and audio feedback. The study\nunderscores the transformative potential of haptic and audio feedback in\neducational practices for the visually impaired, setting a foundation for\nfuture research and development in accessible programming education. This\nthesis contributes to the field of accessible technology by demonstrating how\ntactile and auditory feedback can be effectively integrated into educational\ntools, thereby broadening accessibility in STEM education.", "AI": {"tldr": "\u672c\u8bba\u6587\u4ecb\u7ecd\u7684HACI\u662f\u4e00\u79cd\u901a\u8fc7\u89e6\u89c9\u548c\u97f3\u9891\u53cd\u9988\u5e2e\u52a9\u89c6\u89c9\u969c\u788d\u5b66\u751f\u5b66\u4e60\u7f16\u7a0b\u7684\u6559\u80b2\u5de5\u5177\uff0c\u521d\u6b65\u8bc4\u4f30\u663e\u793a\u5176\u6709\u52a9\u4e8e\u975e\u89c6\u89c9\u7684\u7f16\u7a0b\u4f53\u9a8c\uff0c\u4f46\u4ecd\u9700\u6539\u8fdb\u53cd\u9988\u673a\u5236\u3002", "motivation": "\u4e3a\u4e86\u7ed9\u89c6\u89c9\u969c\u788d\u5b66\u751f\u63d0\u4f9b\u516c\u5e73\u7684\u8ba1\u7b97\u673a\u79d1\u5b66\u6559\u80b2\u673a\u4f1a\uff0c\u964d\u4f4e\u8ba4\u77e5\u8d1f\u62c5\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e00\u4e2a\u96c6\u6210\u89e6\u89c9\u548c\u97f3\u9891\u53cd\u9988\u7684web\u5e94\u7528\u7a0b\u5e8f\uff0c\u4ee5\u53ca\u4e00\u4e2a\u914d\u6709\u516d\u4e2a\u89e6\u89c9\u7535\u673a\u7684Arduino\u624b\u5957\uff0c\u8fdb\u884c\u521d\u6b65\u8bc4\u4f30\u3002", "result": "HACI\u4fc3\u8fdb\u4e86\u7f16\u7a0b\u7ed3\u6784\u7684\u975e\u89c6\u89c9\u5bfc\u822a\u548c\u7406\u89e3\uff0c\u4f46\u5728\u53cd\u9988\u673a\u5236\u548c\u529f\u80fd\u6269\u5c55\u4e0a\u4ecd\u9762\u4e34\u6311\u6218\u3002", "conclusion": "HACI\u5728\u7f16\u7a0b\u6559\u80b2\u4e2d\u5bf9\u89c6\u89c9\u969c\u788d\u5b66\u751f\u5177\u6709\u53d8\u9769\u6f5c\u529b\uff0c\u901a\u8fc7\u89e6\u89c9\u4e0e\u97f3\u9891\u53cd\u9988\u7684\u6574\u5408\uff0c\u63d0\u5347\u4e86\u53ef\u8bbf\u95ee\u6027\u3002"}}
{"id": "2511.04042", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04042", "abs": "https://arxiv.org/abs/2511.04042", "authors": ["Kailun Ji", "Xiaoyu Hu", "Xinyu Zhang", "Jun Chen"], "title": "An LLM-based Framework for Human-Swarm Teaming Cognition in Disaster Search and Rescue", "comment": null, "summary": "Large-scale disaster Search And Rescue (SAR) operations are persistently\nchallenged by complex terrain and disrupted communications. While Unmanned\nAerial Vehicle (UAV) swarms offer a promising solution for tasks like wide-area\nsearch and supply delivery, yet their effective coordination places a\nsignificant cognitive burden on human operators. The core human-machine\ncollaboration bottleneck lies in the ``intention-to-action gap'', which is an\nerror-prone process of translating a high-level rescue objective into a\nlow-level swarm command under high intensity and pressure. To bridge this gap,\nthis study proposes a novel LLM-CRF system that leverages Large Language Models\n(LLMs) to model and augment human-swarm teaming cognition. The proposed\nframework initially captures the operator's intention through natural and\nmulti-modal interactions with the device via voice or graphical annotations. It\nthen employs the LLM as a cognitive engine to perform intention comprehension,\nhierarchical task decomposition, and mission planning for the UAV swarm. This\nclosed-loop framework enables the swarm to act as a proactive partner,\nproviding active feedback in real-time while reducing the need for manual\nmonitoring and control, which considerably advances the efficacy of the SAR\ntask. We evaluate the proposed framework in a simulated SAR scenario.\nExperimental results demonstrate that, compared to traditional order and\ncommand-based interfaces, the proposed LLM-driven approach reduced task\ncompletion time by approximately $64.2\\%$ and improved task success rate by\n$7\\%$. It also leads to a considerable reduction in subjective cognitive\nworkload, with NASA-TLX scores dropping by $42.9\\%$. This work establishes the\npotential of LLMs to create more intuitive and effective human-swarm\ncollaborations in high-stakes scenarios.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7cfb\u7edf\uff0c\u65e8\u5728\u6539\u5584\u590d\u6742\u5730\u5f62\u4e0b\u7684\u65e0\u4eba\u673a\u7fa4\u4f53\u5728\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u6548\u7387\u548c\u6548\u679c\uff0c\u901a\u8fc7\u81ea\u7136\u4ea4\u4e92\u6355\u6349\u64cd\u4f5c\u8005\u610f\u56fe\uff0c\u5b9e\u73b0\u66f4\u5feb\u901f\u7684\u4efb\u52a1\u5b8c\u6210\u548c\u8f83\u4f4e\u7684\u8ba4\u77e5\u8d1f\u62c5\u3002", "motivation": "\u5e0c\u671b\u89e3\u51b3\u5728\u590d\u6742\u5730\u5f62\u548c\u901a\u4fe1\u963b\u65ad\u4e0b\uff0c\u5927\u89c4\u6a21\u6551\u63f4\u64cd\u4f5c\u4e2d\u4eba\u673a\u5408\u4f5c\u5b58\u5728\u7684\u610f\u56fe\u4e0e\u884c\u52a8\u4e4b\u95f4\u7684\u6c9f\u901a\u74f6\u9888\u3002", "method": "\u63d0\u51faLLM-CRF\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u7136\u591a\u6a21\u5f0f\u4ea4\u4e92\u6355\u6349\u64cd\u4f5c\u8005\u610f\u56fe\uff0c\u5e76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4efb\u52a1\u5206\u89e3\u548c\u89c4\u5212\u3002", "result": "\u5728\u6a21\u62dfSAR\u573a\u666f\u4e2d\uff0cLLM\u9a71\u52a8\u7684\u65b9\u6cd5\u6bd4\u4f20\u7edf\u6307\u4ee4\u754c\u9762\u51cf\u5c11\u4e86\u7ea664.2%\u7684\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\uff0c\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad8\u4e867%\uff0c\u4e3b\u89c2\u8ba4\u77e5\u8d1f\u62c5\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5927\u89c4\u6a21\u6551\u63f4\u64cd\u4f5c\u4e2d\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u53ef\u663e\u8457\u6539\u5584\u4eba\u673a\u534f\u4f5c\uff0c\u63d0\u9ad8\u6548\u7387\uff0c\u5e76\u964d\u4f4e\u64cd\u4f5c\u8d1f\u62c5\u3002"}}
{"id": "2511.03907", "categories": ["cs.HC", "cs.AI", "I.2.1; J.3"], "pdf": "https://arxiv.org/pdf/2511.03907", "abs": "https://arxiv.org/abs/2511.03907", "authors": ["Liam Bakar", "Zachary Englhardt", "Vidya Srinivas", "Girish Narayanswamy", "Dilini Nissanka", "Shwetak Patel", "Vikram Iyer"], "title": "SnappyMeal: Design and Longitudinal Evaluation of a Multimodal AI Food Logging Application", "comment": "24 pages, 15 figures", "summary": "Food logging, both self-directed and prescribed, plays a critical role in\nuncovering correlations between diet, medical, fitness, and health outcomes.\nThrough conversations with nutritional experts and individuals who practice\ndietary tracking, we find current logging methods, such as handwritten and\napp-based journaling, are inflexible and result in low adherence and\npotentially inaccurate nutritional summaries. These findings, corroborated by\nprior literature, emphasize the urgent need for improved food logging methods.\nIn response, we propose SnappyMeal, an AI-powered dietary tracking system that\nleverages multimodal inputs to enable users to more flexibly log their food\nintake. SnappyMeal introduces goal-dependent follow-up questions to\nintelligently seek missing context from the user and information retrieval from\nuser grocery receipts and nutritional databases to improve accuracy. We\nevaluate SnappyMeal through publicly available nutrition benchmarks and a\nmulti-user, 3-week, in-the-wild deployment capturing over 500 logged food\ninstances. Users strongly praised the multiple available input methods and\nreported a strong perceived accuracy. These insights suggest that multimodal AI\nsystems can be leveraged to significantly improve dietary tracking flexibility\nand context-awareness, laying the groundwork for a new class of intelligent\nself-tracking applications.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51faSnappyMeal\uff0c\u4e00\u4e2a\u5229\u7528AI\u548c\u591a\u6a21\u6001\u8f93\u5165\u7684\u996e\u98df\u8ffd\u8e2a\u7cfb\u7edf\uff0c\u65e8\u5728\u6539\u5584\u996e\u98df\u8bb0\u5f55\u7684\u7075\u6d3b\u6027\u4e0e\u51c6\u786e\u6027\uff0c\u5e76\u83b7\u5f97\u7528\u6237\u79ef\u6781\u53cd\u9988\u3002", "motivation": "\u5f53\u524d\u996e\u98df\u8bb0\u5f55\u65b9\u6cd5\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u5bfc\u81f4\u4f4e\u4f9d\u4ece\u6027\u548c\u6f5c\u5728\u7684\u4e0d\u51c6\u786e\u6027\uff0c\u8feb\u5207\u9700\u8981\u6539\u5584\u996e\u98df\u8bb0\u5f55\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5bf9\u8425\u517b\u4e13\u5bb6\u548c\u7528\u6237\u7684\u8bbf\u8c08\uff0c\u7ed3\u5408AI\u6280\u672f\u548c\u591a\u6a21\u6001\u8f93\u5165\uff0cSnappyMeal\u7cfb\u7edf\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "SnappyMeal\u7cfb\u7edf\u901a\u8fc7\u667a\u80fd\u5316\u7684\u8ddf\u8fdb\u95ee\u9898\u548c\u4fe1\u606f\u68c0\u7d22\uff0c\u63d0\u9ad8\u4e86\u996e\u98df\u8bb0\u5f55\u7684\u51c6\u786e\u6027\uff0c\u7528\u6237\u5bf9\u5176\u591a\u4e2a\u8f93\u5165\u65b9\u5f0f\u8868\u793a\u9ad8\u5ea6\u8ba4\u53ef\u3002", "conclusion": "\u591a\u6a21\u6001AI\u7cfb\u7edf\u80fd\u663e\u8457\u63d0\u9ad8\u996e\u98df\u8bb0\u5f55\u7684\u7075\u6d3b\u6027\u548c\u4e0a\u4e0b\u6587\u610f\u8bc6\uff0c\u5960\u5b9a\u4e86\u667a\u80fd\u81ea\u6211\u8ffd\u8e2a\u5e94\u7528\u7684\u65b0\u57fa\u7840\u3002"}}
{"id": "2511.04052", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04052", "abs": "https://arxiv.org/abs/2511.04052", "authors": ["Kyongsik Yun", "David Bayard", "Gerik Kubiak", "Austin Owens", "Andrew Johnson", "Ryan Johnson", "Dan Scharf", "Thomas Lu"], "title": "Enhancing Fault-Tolerant Space Computing: Guidance Navigation and Control (GNC) and Landing Vision System (LVS) Implementations on Next-Gen Multi-Core Processors", "comment": null, "summary": "Future planetary exploration missions demand high-performance, fault-tolerant\ncomputing to enable autonomous Guidance, Navigation, and Control (GNC) and\nLander Vision System (LVS) operations during Entry, Descent, and Landing (EDL).\nThis paper evaluates the deployment of GNC and LVS algorithms on\nnext-generation multi-core processors--HPSC, Snapdragon VOXL2, and AMD Xilinx\nVersal--demonstrating up to 15x speedup for LVS image processing and over 250x\nspeedup for Guidance for Fuel-Optimal Large Divert (GFOLD) trajectory\noptimization compared to legacy spaceflight hardware. To ensure computational\nreliability, we present ARBITER (Asynchronous Redundant Behavior Inspection for\nTrusted Execution and Recovery), a Multi-Core Voting (MV) mechanism that\nperforms real-time fault detection and correction across redundant cores.\nARBITER is validated in both static optimization tasks (GFOLD) and dynamic\nclosed-loop control (Attitude Control System). A fault injection study further\nidentifies the gradient computation stage in GFOLD as the most sensitive to\nbit-level errors, motivating selective protection strategies and vector-based\noutput arbitration. This work establishes a scalable and energy-efficient\narchitecture for future missions, including Mars Sample Return, Enceladus\nOrbilander, and Ceres Sample Return, where onboard autonomy, low latency, and\nfault resilience are critical.", "AI": {"tldr": "\u672c\u8bba\u6587\u7814\u7a76\u591a\u6838\u5904\u7406\u5668\u5728\u672a\u6765\u884c\u661f\u63a2\u7d22\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51faARBITER\u673a\u5236\u4ee5\u589e\u5f3a\u8ba1\u7b97\u7684\u5bb9\u9519\u80fd\u529b\uff0c\u5e76\u53d6\u5f97\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u672a\u6765\u7684\u884c\u661f\u63a2\u7d22\u4efb\u52a1\u8feb\u5207\u9700\u8981\u9ad8\u6027\u80fd\u548c\u5bb9\u9519\u8ba1\u7b97\uff0c\u4ee5\u652f\u6301\u81ea\u4e3b\u5bfc\u822a\u4e0e\u63a7\u5236\u3002", "method": "\u8bc4\u4f30\u4e86\u591a\u6838\u5904\u7406\u5668\u4e0aGNC\u548cLVS\u7b97\u6cd5\u7684\u90e8\u7f72\uff0c\u5e76\u63d0\u51faARBITER\u673a\u5236\u5b9e\u73b0\u5b9e\u65f6\u6545\u969c\u68c0\u6d4b\u4e0e\u6062\u590d\u3002", "result": "\u5728\u4e0e\u4f20\u7edf\u822a\u5929\u786c\u4ef6\u76f8\u6bd4\uff0cLVS\u56fe\u50cf\u5904\u7406\u901f\u5ea6\u63d0\u5347\u8fbe15\u500d\uff0cLFOLD\u8f68\u8ff9\u4f18\u5316\u7684\u901f\u5ea6\u63d0\u5347\u8d85\u8fc7250\u500d\uff0cARBITER\u673a\u5236\u6709\u6548\u63d0\u9ad8\u4e86\u8ba1\u7b97\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u672a\u6765\u884c\u661f\u63a2\u7d22\u4efb\u52a1\u5efa\u7acb\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u80fd\u7684\u8ba1\u7b97\u67b6\u6784\uff0c\u7279\u522b\u662f\u5728\u81ea\u4e3b\u6027\u548c\u6545\u969c\u6062\u590d\u65b9\u9762\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2511.03916", "categories": ["cs.HC", "cs.ET"], "pdf": "https://arxiv.org/pdf/2511.03916", "abs": "https://arxiv.org/abs/2511.03916", "authors": ["Ellen Simpson", "Ryan Ermovick", "Mona Sloane"], "title": "Human Resource Management and AI: A Contextual Transparency Database", "comment": null, "summary": "AI tools are proliferating in human resources management (HRM) and\nrecruiting, helping to mediate access to the labor market. As these systems\nspread, profession-specific transparency needs emerging from black-boxed\nsystems in HRM move into focus. Prior work often frames transparency\ntechnically or abstractly, but we contend AI transparency is a social project\nshaped by materials, meanings, and competencies of practice. This paper\nintroduces the Talent Acquisition and Recruiting AI (TARAI) Index, situating AI\nsystems within the social practice of recruiting by examining product\nfunctionality, claims, assumptions, and AI clarity. Built through an iterative,\nmixed-methods process, the database demonstrates how transparency emerges: not\nas a fixed property, but as a dynamic outcome shaped by professional practices,\ninteractions, and competencies. By centering social practice, our work offers a\ngrounded, actionable approach to understanding and articulating AI transparency\nin HR and provides a blueprint for participatory database design for contextual\ntransparency in professional practice.", "AI": {"tldr": "\u968f\u7740AI\u5de5\u5177\u5728\u62db\u8058\u4e2d\u7684\u666e\u53ca\uff0c\u8bba\u6587\u63d0\u51fa\u4e86TARAI\u6307\u6570\uff0c\u5f3a\u8c03AI\u900f\u660e\u5ea6\u9700\u5728\u5b9e\u8df5\u4e2d\u7406\u89e3\uff0c\u5e76\u4e0d\u662f\u56fa\u5b9a\u5c5e\u6027\uff0c\u800c\u662f\u53d7\u4e13\u4e1a\u5b9e\u8df5\u548c\u4e92\u52a8\u5f71\u54cd\u7684\u52a8\u6001\u7ed3\u679c\u3002", "motivation": "\u968f\u7740AI\u5de5\u5177\u5728\u62db\u8058\u548c\u4eba\u529b\u8d44\u6e90\u7ba1\u7406\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9488\u5bf9\u8fd9\u4e9b\u9ed1\u7bb1\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u9700\u6c42\u65e5\u76ca\u7a81\u51fa\uff0c\u7279\u522b\u662f\u5728\u5404\u4e2a\u804c\u4e1a\u7279\u5b9a\u9886\u57df\u3002", "method": "\u8fd0\u7528\u8fed\u4ee3\u7684\u6df7\u5408\u65b9\u6cd5\u8fc7\u7a0b\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u6570\u636e\u5e93\uff0c\u8be5\u6570\u636e\u5e93\u8bc4\u4f30\u4e86AI\u7cfb\u7edf\u5728\u62db\u8058\u9886\u57df\u7684\u529f\u80fd\u6027\u3001\u58f0\u660e\u3001\u5047\u8bbe\u53ca\u5176\u6e05\u6670\u5ea6\u3002", "result": "\u8be5\u8bba\u6587\u5c55\u793a\u4e86\u900f\u660e\u5ea6\u7684\u52a8\u6001\u751f\u6210\u8fc7\u7a0b\uff0c\u5f3a\u8c03\u900f\u660e\u5ea6\u662f\u4e00\u4e2a\u793e\u4f1a\u6027\u9879\u76ee\uff0c\u800c\u975e\u5355\u7eaf\u7684\u6280\u672f\u6216\u62bd\u8c61\u6982\u5ff5\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5efa\u7acb\u4eba\u624d\u83b7\u53d6\u4e0e\u62db\u8058AI\u6307\u6570\uff08TARAI\uff09\uff0c\u5f3a\u8c03AI\u900f\u660e\u5ea6\u4e0d\u4ec5\u662f\u4e00\u4e2a\u56fa\u5b9a\u5c5e\u6027\uff0c\u800c\u662f\u4e00\u4e2a\u7531\u4e13\u4e1a\u5b9e\u8df5\u3001\u4e92\u52a8\u548c\u80fd\u529b\u52a8\u6001\u5f62\u6210\u7684\u7ed3\u679c\uff0c\u4e3a\u4eba\u529b\u8d44\u6e90\u7ba1\u7406\u4e2d\u7684AI\u900f\u660e\u5ea6\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6027\u548c\u53ef\u64cd\u4f5c\u6027\u7684\u65b9\u6cd5\u3002"}}
{"id": "2511.04109", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04109", "abs": "https://arxiv.org/abs/2511.04109", "authors": ["Yanbo Pang", "Qingkai Li", "Mingguo Zhao"], "title": "CBMC-V3: A CNS-inspired Control Framework Towards Manipulation Agility with SNN", "comment": null, "summary": "As robotic arm applications extend beyond industrial settings into\nhealthcare, service, and daily life, existing control algorithms struggle to\nachieve the agile manipulation required for complex environments with dynamic\ntrajectories, unpredictable interactions, and diverse objects. This paper\npresents a biomimetic control framework based on Spiking Neural Networks (SNN),\ninspired by the human Central Nervous System (CNS), to achieve agile control in\nsuch environments. The proposed framework features five control modules\n(cerebral cortex, cerebellum, thalamus, brainstem, spinal cord), three\nhierarchical control levels (first-order, second-order, third-order), and two\ninformation pathways (ascending, descending). Each module is fully implemented\nusing SNN. The spinal cord module uses spike encoding and Leaky\nIntegrate-and-Fire (LIF) neurons for feedback control. The brainstem module\nemploys a network of LIF and non-spiking LIF neurons to dynamically adjust\nspinal cord parameters via reinforcement learning. The thalamus module\nsimilarly adjusts the cerebellum's torque outputs. The cerebellum module uses a\nrecurrent SNN to learn the robotic arm's dynamics through regression, providing\nfeedforward gravity compensation torques. The framework is validated both in\nsimulation and on real-world robotic arm platform under various loads and\ntrajectories. Results demonstrate that our method outperforms the\nindustrial-grade position control in manipulation agility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5c16\u5cf0\u795e\u7ecf\u7f51\u7edc\u7684\u751f\u7269\u4eff\u751f\u63a7\u5236\u6846\u67b6\uff0c\u89e3\u51b3\u73b0\u6709\u63a7\u5236\u7b97\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9650\u5236\uff0c\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u65b9\u6cd5\u5728\u67d4\u6027\u64cd\u63a7\u4e0a\u4f18\u4e8e\u5de5\u4e1a\u6807\u51c6\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u81c2\u5e94\u7528\u4e0d\u65ad\u6269\u5c55\u81f3\u533b\u7597\u3001\u670d\u52a1\u548c\u65e5\u5e38\u751f\u6d3b\uff0c\u73b0\u6709\u63a7\u5236\u7b97\u6cd5\u5728\u590d\u6742\u548c\u52a8\u6001\u73af\u5883\u4e2d\u96be\u4ee5\u5b9e\u73b0\u7075\u6d3b\u64cd\u4f5c\u3002", "method": "\u5229\u7528\u57fa\u4e8e\u5c16\u5cf0\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u7684\u63a7\u5236\u6a21\u5757\uff0c\u8be5\u6846\u67b6\u5206\u4e3a\u4e94\u4e2a\u6a21\u5757\u548c\u4e09\u4e2a\u5c42\u6b21\uff0c\u901a\u8fc7\u53cd\u9988\u63a7\u5236\u548c\u5f3a\u5316\u5b66\u4e60\u52a8\u6001\u8c03\u6574\u53c2\u6570\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9645\u673a\u5668\u4eba\u81c2\u5e73\u53f0\u7684\u9a8c\u8bc1\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u64cd\u4f5c\u7075\u6d3b\u6027\u4e0a\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u5de5\u4e1a\u7ea7\u63a7\u5236\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u751f\u7269\u4eff\u751f\u63a7\u5236\u6846\u67b6\u5728\u7075\u6d3b\u63a7\u5236\u65b9\u9762\u4f18\u4e8e\u5de5\u4e1a\u6807\u51c6\u7684\u4f4d\u7f6e\u63a7\u5236\uff0c\u589e\u5f3a\u4e86\u673a\u5668\u4eba\u81c2\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u80fd\u529b\u3002"}}
{"id": "2511.04050", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.04050", "abs": "https://arxiv.org/abs/2511.04050", "authors": ["Zenan Chen", "Ruijiang Gao", "Yingzhi Liang"], "title": "Revealing AI Reasoning Increases Trust but Crowds Out Unique Human Knowledge", "comment": "38 pages", "summary": "Effective human-AI collaboration requires humans to accurately gauge AI\ncapabilities and calibrate their trust accordingly. Humans often have\ncontext-dependent private information, referred to as Unique Human Knowledge\n(UHK), that is crucial for deciding whether to accept or override AI's\nrecommendations. We examine how displaying AI reasoning affects trust and UHK\nutilization through a pre-registered, incentive-compatible experiment (N =\n752). We find that revealing AI reasoning, whether brief or extensive, acts as\na powerful persuasive heuristic that significantly increases trust and\nagreement with AI recommendations. Rather than helping participants\nappropriately calibrate their trust, this transparency induces over-trust that\ncrowds out UHK utilization. Our results highlight the need for careful\nconsideration when revealing AI reasoning and call for better information\ndesign in human-AI collaboration systems.", "AI": {"tldr": "\u663e\u793aAI\u63a8\u7406\u53ef\u4ee5\u589e\u8fdb\u4fe1\u4efb\uff0c\u4f46\u53ef\u80fd\u5bfc\u81f4\u8fc7\u5ea6\u4fe1\u4efb\uff0c\u635f\u5bb3\u4eba\u7c7b\u5728\u51b3\u7b56\u4e2d\u7684\u72ec\u7279\u77e5\u8bc6\u5229\u7528\u3002", "motivation": "\u63a2\u8ba8\u4eba\u7c7b\u5982\u4f55\u8bc4\u4f30AI\u80fd\u529b\u5e76\u6839\u636e\u6b64\u8c03\u6574\u4fe1\u4efb\uff0c\u7279\u522b\u5173\u6ce8\u72ec\u7279\u4eba\u7c7b\u77e5\u8bc6\u5bf9\u51b3\u7b56\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u4e00\u6b21\u9884\u6ce8\u518c\u7684\u6fc0\u52b1\u517c\u5bb9\u5b9e\u9a8c\uff08N=752\uff09\u8fdb\u884c\u7814\u7a76\u3002", "result": "\u663e\u793aAI\u63a8\u7406\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9AI\u5efa\u8bae\u7684\u4fe1\u4efb\u548c\u4e00\u81f4\u6027\uff0c\u4f46\u5bfc\u81f4\u4e86\u8fc7\u5ea6\u4fe1\u4efb\uff0c\u6291\u5236\u4e86UHK\u7684\u4f7f\u7528\u3002", "conclusion": "\u5728\u663e\u793aAI\u63a8\u7406\u7684\u60c5\u51b5\u4e0b\uff0c\u8fc7\u5ea6\u4fe1\u4efbAI\u4f1a\u6291\u5236\u72ec\u7279\u4eba\u7c7b\u77e5\u8bc6\uff08UHK\uff09\u7684\u5229\u7528\u3002"}}
{"id": "2511.04131", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04131", "abs": "https://arxiv.org/abs/2511.04131", "authors": ["Yitang Li", "Zhengyi Luo", "Tonghe Zhang", "Cunxi Dai", "Anssi Kanervisto", "Andrea Tirinzoni", "Haoyang Weng", "Kris Kitani", "Mateusz Guzek", "Ahmed Touati", "Alessandro Lazaric", "Matteo Pirotta", "Guanya Shi"], "title": "BFM-Zero: A Promptable Behavioral Foundation Model for Humanoid Control Using Unsupervised Reinforcement Learning", "comment": null, "summary": "Building Behavioral Foundation Models (BFMs) for humanoid robots has the\npotential to unify diverse control tasks under a single, promptable generalist\npolicy. However, existing approaches are either exclusively deployed on\nsimulated humanoid characters, or specialized to specific tasks such as\ntracking. We propose BFM-Zero, a framework that learns an effective shared\nlatent representation that embeds motions, goals, and rewards into a common\nspace, enabling a single policy to be prompted for multiple downstream tasks\nwithout retraining. This well-structured latent space in BFM-Zero enables\nversatile and robust whole-body skills on a Unitree G1 humanoid in the real\nworld, via diverse inference methods, including zero-shot motion tracking, goal\nreaching, and reward optimization, and few-shot optimization-based adaptation.\nUnlike prior on-policy reinforcement learning (RL) frameworks, BFM-Zero builds\nupon recent advancements in unsupervised RL and Forward-Backward (FB) models,\nwhich offer an objective-centric, explainable, and smooth latent representation\nof whole-body motions. We further extend BFM-Zero with critical reward shaping,\ndomain randomization, and history-dependent asymmetric learning to bridge the\nsim-to-real gap. Those key design choices are quantitatively ablated in\nsimulation. A first-of-its-kind model, BFM-Zero establishes a step toward\nscalable, promptable behavioral foundation models for whole-body humanoid\ncontrol.", "AI": {"tldr": "BFM-Zero\u662f\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u6f5c\u5728\u8868\u793a\u5b9e\u73b0\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u5bf9\u591a\u79cd\u4eba\u5f62\u673a\u5668\u4eba\u4efb\u52a1\u7684\u6709\u6548\u63a7\u5236\uff0c\u63a8\u52a8\u4e86\u53ef\u6269\u5c55\u884c\u4e3a\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u3002", "motivation": "\u6784\u5efa\u4eba\u5f62\u673a\u5668\u4eba\u884c\u4e3a\u57fa\u7840\u6a21\u578b\u53ef\u4ee5\u7edf\u4e00\u591a\u6837\u7684\u63a7\u5236\u4efb\u52a1\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u6a21\u62df\u73af\u5883\u6216\u7279\u5b9a\u4efb\u52a1\u3002", "method": "BFM-Zero\u6846\u67b6\u901a\u8fc7\u5171\u4eab\u6f5c\u5728\u8868\u793a\uff0c\u5c06\u52a8\u4f5c\u3001\u76ee\u6807\u548c\u5956\u52b1\u5d4c\u5165\u5171\u540c\u7a7a\u95f4\uff0c\u652f\u6301\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u7684\u63d0\u793a\uff0c\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "BFM-Zero\u5728Unitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u7684\u9c81\u68d2\u5168\u8eab\u6280\u80fd\uff0c\u5c55\u793a\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "BFM-Zero\u5c55\u793a\u4e86\u53ef\u6269\u5c55\u7684\u901a\u7528\u884c\u4e3a\u57fa\u7840\u6a21\u578b\u5728\u5b9e\u9645\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2511.04081", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.04081", "abs": "https://arxiv.org/abs/2511.04081", "authors": ["Kyrie Zhixuan Zhou", "Justin Eric Chen", "Xiang Zheng", "Yaoyao Qian", "Yunpeng Xiao", "Kai Shu"], "title": "\"Everyone Else Does It\": The Rise of Preprinting Culture in Computing Disciplines", "comment": null, "summary": "Preprinting has become a norm in fast-paced computing fields such as\nartificial intelligence (AI) and human-computer interaction (HCI). In this\npaper, we conducted semistructured interviews with 15 academics in these fields\nto reveal their motivations and perceptions of preprinting. The results found a\nclose relationship between preprinting and characteristics of the fields,\nincluding the huge number of papers, competitiveness in career advancement,\nprevalence of scooping, and imperfect peer review system - preprinting comes to\nthe rescue in one way or another for the participants. Based on the results, we\nreflect on the role of preprinting in subverting the traditional publication\nmode and outline possibilities of a better publication ecosystem. Our study\ncontributes by inspecting the community aspects of preprinting practices\nthrough talking to academics.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u8bbf\u8c0815\u4f4d\u5b66\u8005\uff0c\u63a2\u8ba8\u4e86\u9884\u5370\u672c\u5728AI\u548cHCI\u9886\u57df\u7684\u52a8\u673a\u4e0e\u5f71\u54cd\uff0c\u53d1\u73b0\u5176\u5728\u4f20\u7edf\u51fa\u7248\u6a21\u5f0f\u4e2d\u626e\u6f14\u4e86\u98a0\u8986\u89d2\u8272\u3002", "motivation": "\u968f\u7740\u8ba1\u7b97\u9886\u57df\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u9884\u5370\u672c\u5df2\u6210\u4e3a\u4e00\u79cd\u5e38\u6001\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63ed\u793a\u5b66\u8005\u4eec\u5bf9\u6b64\u7684\u52a8\u673a\u53ca\u770b\u6cd5\u3002", "method": "\u901a\u8fc7\u5bf915\u4f4d\u76f8\u5173\u9886\u57df\u5b66\u8005\u8fdb\u884c\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\uff0c\u8c03\u67e5\u4ed6\u4eec\u5bf9\u9884\u5370\u672c\u7684\u52a8\u673a\u548c\u770b\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u9884\u5370\u672c\u4e0e\u9886\u57df\u7279\u5f81\u5bc6\u5207\u76f8\u5173\uff0c\u5305\u62ec\u8bba\u6587\u6570\u91cf\u5e9e\u5927\u3001\u804c\u4e1a\u7ade\u4e89\u6fc0\u70c8\u3001\u62a2\u5148\u53d1\u8868\u73b0\u8c61\u666e\u904d\uff0c\u4ee5\u53ca\u4e0d\u5b8c\u5584\u7684\u540c\u884c\u8bc4\u5ba1\u7cfb\u7edf\uff0c\u9884\u5370\u672c\u5728\u8fd9\u4e9b\u65b9\u9762\u4e3a\u53c2\u4e0e\u8005\u63d0\u4f9b\u4e86\u5e2e\u52a9\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u63a2\u8ba8\u4e86\u9884\u5370\u672c\u5728\u4eba\u5de5\u667a\u80fd\u548c\u4eba\u673a\u4ea4\u4e92\u9886\u57df\u4e2d\u7684\u610f\u4e49\uff0c\u5e76\u53cd\u601d\u4e86\u5b83\u5bf9\u4f20\u7edf\u51fa\u7248\u6a21\u5f0f\u7684\u98a0\u8986\u4f5c\u7528\u3002"}}
{"id": "2511.04180", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04180", "abs": "https://arxiv.org/abs/2511.04180", "authors": ["Yizhen Yin", "Dapeng Feng", "Hongbo Chen", "Yuhua Qi"], "title": "PUL-SLAM: Path-Uncertainty Co-Optimization with Lightweight Stagnation Detection for Efficient Robotic Exploration", "comment": null, "summary": "Existing Active SLAM methodologies face issues such as slow exploration speed\nand suboptimal paths. To address these limitations, we propose a hybrid\nframework combining a Path-Uncertainty Co-Optimization Deep Reinforcement\nLearning framework and a Lightweight Stagnation Detection mechanism. The\nPath-Uncertainty Co-Optimization framework jointly optimizes travel distance\nand map uncertainty through a dual-objective reward function, balancing\nexploration and exploitation. The Lightweight Stagnation Detection reduces\nredundant exploration through Lidar Static Anomaly Detection and Map Update\nStagnation Detection, terminating episodes on low expansion rates. Experimental\nresults show that compared with the frontier-based method and RRT method, our\napproach shortens exploration time by up to 65% and reduces path distance by up\nto 42%, significantly improving exploration efficiency in complex environments\nwhile maintaining reliable map completeness. Ablation studies confirm that the\ncollaborative mechanism accelerates training convergence. Empirical validation\non a physical robotic platform demonstrates the algorithm's practical\napplicability and its successful transferability from simulation to real-world\nenvironments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e0e\u8f7b\u91cf\u7ea7\u68c0\u6d4b\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4e3b\u52a8SLAM\u7684\u63a2\u7d22\u6548\u7387\u3002", "motivation": "\u9488\u5bf9\u73b0\u6709\u4e3b\u52a8SLAM\u65b9\u6cd5\u5728\u63a2\u7d22\u901f\u5ea6\u6162\u548c\u8def\u5f84\u4f18\u5316\u4e0d\u8db3\u7b49\u95ee\u9898\u8fdb\u884c\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u8def\u5f84\u4e0d\u786e\u5b9a\u6027\u534f\u540c\u4f18\u5316\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u548c\u8f7b\u91cf\u7ea7\u505c\u6ede\u68c0\u6d4b\u673a\u5236\u7684\u6df7\u5408\u65b9\u6cd5\u3002", "result": "\u63a2\u7d22\u65f6\u95f4\u7f29\u77ed\u6700\u591a\u8fbe65%\uff0c\u8def\u5f84\u8ddd\u79bb\u51cf\u5c11\u6700\u591a\u8fbe42%\u3002", "conclusion": "\u8be5\u6df7\u5408\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u590d\u6742\u73af\u5883\u4e2d\u7684\u63a2\u7d22\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u9760\u7684\u5730\u56fe\u5b8c\u6574\u6027\u3002"}}
{"id": "2511.04144", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04144", "abs": "https://arxiv.org/abs/2511.04144", "authors": ["Boxuan Ma", "Huiyong Li", "Gen Li", "Li Chen", "Cheng Tang", "Yinjie Xie", "Chenghao Gu", "Atsushi Shimada", "Shin'ichi Konomi"], "title": "Scaffolding Metacognition in Programming Education: Understanding Student-AI Interactions and Design Implications", "comment": null, "summary": "Generative AI tools such as ChatGPT now provide novice programmers with\nunprecedented access to instant, personalized support. While this holds clear\npromise, their influence on students' metacognitive processes remains\nunderexplored. Existing work has largely focused on correctness and usability,\nwith limited attention to whether and how students' use of AI assistants\nsupports or bypasses key metacognitive processes. This study addresses that gap\nby analyzing student-AI interactions through a metacognitive lens in\nuniversity-level programming courses. We examined more than 10,000 dialogue\nlogs collected over three years, complemented by surveys of students and\neducators. Our analysis focused on how prompts and responses aligned with\nmetacognitive phases and strategies. Synthesizing these findings across data\nsources, we distill design considerations for AI-powered coding assistants that\naim to support rather than supplant metacognitive engagement. Our findings\nprovide guidance for developing educational AI tools that strengthen students'\nlearning processes in programming education.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u751f\u6210\u5f0fAI\u5de5\u5177\u5bf9\u5927\u5b66\u7f16\u7a0b\u8bfe\u7a0b\u4e2d\u5b66\u751f\u5143\u8ba4\u77e5\u8fc7\u7a0b\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u4f9b\u4e86\u589e\u5f3a\u6559\u80b2AI\u5de5\u5177\u7684\u8bbe\u8ba1\u5efa\u8bae\u3002", "motivation": "\u867d\u7136\u751f\u6210\u5f0fAI\u5de5\u5177\u5982ChatGPT\u4e3a\u521d\u5b66\u7a0b\u5e8f\u5458\u63d0\u4f9b\u4e86\u524d\u6240\u672a\u6709\u7684\u5373\u65f6\u4e2a\u6027\u5316\u652f\u6301\uff0c\u4f46\u5176\u5bf9\u5b66\u751f\u5143\u8ba4\u77e5\u8fc7\u7a0b\u7684\u5f71\u54cd\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5206\u6790\u5927\u5b66\u7f16\u7a0b\u8bfe\u7a0b\u4e2d\u8d85\u8fc710,000\u4e2a\u5bf9\u8bdd\u65e5\u5fd7\uff0c\u5e76\u7ed3\u5408\u5b66\u751f\u548c\u6559\u5e08\u7684\u8c03\u67e5\u6570\u636e\uff0c\u5173\u6ce8\u63d0\u793a\u548c\u56de\u5e94\u4e0e\u5143\u8ba4\u77e5\u9636\u6bb5\u548c\u7b56\u7565\u7684\u5bf9\u9f50\u60c5\u51b5\u3002", "result": "\u901a\u8fc7\u591a\u79cd\u6570\u636e\u6e90\u7efc\u5408\u5206\u6790\uff0c\u63d0\u70bc\u51fa\u65e8\u5728\u652f\u6301\u800c\u975e\u53d6\u4ee3\u5143\u8ba4\u77e5\u53c2\u4e0e\u7684AI\u7f16\u7a0b\u52a9\u624b\u7684\u8bbe\u8ba1\u8003\u8651\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u6559\u80b2AI\u5de5\u5177\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u4ee5\u589e\u5f3a\u5b66\u751f\u5728\u7f16\u7a0b\u6559\u80b2\u4e2d\u7684\u5b66\u4e60\u8fc7\u7a0b\u3002"}}
{"id": "2511.04199", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04199", "abs": "https://arxiv.org/abs/2511.04199", "authors": ["Shenglin Wang", "Mingtong Dai", "Jingxuan Su", "Lingbo Liu", "Chunjie Chen", "Xinyu Wu", "Liang Lin"], "title": "GraspView: Active Perception Scoring and Best-View Optimization for Robotic Grasping in Cluttered Environments", "comment": null, "summary": "Robotic grasping is a fundamental capability for autonomous manipulation, yet\nremains highly challenging in cluttered environments where occlusion, poor\nperception quality, and inconsistent 3D reconstructions often lead to unstable\nor failed grasps. Conventional pipelines have widely relied on RGB-D cameras to\nprovide geometric information, which fail on transparent or glossy objects and\ndegrade at close range. We present GraspView, an RGB-only robotic grasping\npipeline that achieves accurate manipulation in cluttered environments without\ndepth sensors. Our framework integrates three key components: (i) global\nperception scene reconstruction, which provides locally consistent, up-to-scale\ngeometry from a single RGB view and fuses multi-view projections into a\ncoherent global 3D scene; (ii) a render-and-score active perception strategy,\nwhich dynamically selects next-best-views to reveal occluded regions; and (iii)\nan online metric alignment module that calibrates VGGT predictions against\nrobot kinematics to ensure physical scale consistency. Building on these\ntailor-designed modules, GraspView performs best-view global grasping, fusing\nmulti-view reconstructions and leveraging GraspNet for robust execution.\nExperiments on diverse tabletop objects demonstrate that GraspView\nsignificantly outperforms both RGB-D and single-view RGB baselines, especially\nunder heavy occlusion, near-field sensing, and with transparent objects. These\nresults highlight GraspView as a practical and versatile alternative to RGB-D\npipelines, enabling reliable grasping in unstructured real-world environments.", "AI": {"tldr": "GraspView\u662f\u4e00\u4e2a\u65e0\u6df1\u5ea6\u4f20\u611f\u5668\u7684RGB\u5355\u4e00\u6293\u53d6\u4f53\u7cfb\uff0c\u7ed3\u5408\u591a\u9879\u6280\u672f\uff0c\u663e\u793a\u51fa\u5728\u590d\u6742\u73af\u5883\u4e2d\u76f8\u6bd4\u4f20\u7edfRGB-D\u7cfb\u7edf\u5177\u6709\u66f4\u9ad8\u7684\u6293\u53d6\u53ef\u9760\u6027\u548c\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u5728\u6742\u4e71\u73af\u5883\u4e2d\uff0c\u4f20\u7edf\u7684RGB-D\u6444\u50cf\u5934\u5728\u5904\u7406\u900f\u660e\u6216\u5149\u6ed1\u7269\u4f53\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u65e0\u6cd5\u63d0\u4f9b\u7a33\u5b9a\u7684\u6293\u53d6\uff1b\u56e0\u6b64\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u5b9e\u73b0\u7cbe\u51c6\u7684\u64cd\u4f5c\u3002", "method": "GraspView\u96c6\u6210\u4e86\u4e09\u5927\u5173\u952e\u7ec4\u4ef6\uff1a\u5168\u5c40\u611f\u77e5\u573a\u666f\u91cd\u5efa\u3001\u6e32\u67d3\u4e0e\u8bc4\u5206\u7684\u4e3b\u52a8\u611f\u77e5\u7b56\u7565\u3001\u5728\u7ebf\u5ea6\u91cf\u5bf9\u9f50\u6a21\u5757\u3002", "result": "GraspView\u5728\u5404\u79cd\u684c\u9762\u7269\u4f53\u5b9e\u9a8c\u4e2d\uff0c\u8868\u73b0\u663e\u8457\u4f18\u4e8eRGB-D\u548c\u5355\u89c6\u56feRGB\u57fa\u7ebf\uff0c\u5c24\u5176\u5728\u91cd\u5ea6\u906e\u6321\u3001\u8fd1\u573a\u4f20\u611f\u548c\u900f\u660e\u7269\u4f53\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "GraspView\u662f\u4e00\u4e2a\u6709\u6548\u4e14\u591a\u529f\u80fd\u7684RGB-only\u6293\u53d6\u7ba1\u9053\uff0c\u80fd\u591f\u5728\u65e0\u7ed3\u6784\u7684\u771f\u5b9e\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u9760\u7684\u6293\u53d6\u3002"}}
{"id": "2511.04166", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.04166", "abs": "https://arxiv.org/abs/2511.04166", "authors": ["Rui Liu", "Runsheng Zhang", "Shixiao Wang"], "title": "Graph Neural Networks for User Satisfaction Classification in Human-Computer Interaction", "comment": null, "summary": "This study focuses on the problem of user satisfaction classification and\nproposes a framework based on graph neural networks to address the limitations\nof traditional methods in handling complex interaction relationships and\nmultidimensional features. User behaviors, interface elements, and their\npotential connections are abstracted into a graph structure, and joint modeling\nof nodes and edges is used to capture semantics and dependencies in the\ninteraction process. Graph convolution and attention mechanisms are introduced\nto fuse local features and global context, and global pooling with a\nclassification layer is applied to achieve automated satisfaction\nclassification. The method extracts deep patterns from structured data and\nimproves adaptability and robustness in multi-source heterogeneous and dynamic\nenvironments. To verify effectiveness, a public user satisfaction survey\ndataset from Kaggle is used, and results are compared with multiple baseline\nmodels across several performance metrics. Experiments show that the method\noutperforms existing approaches in accuracy, F1-Score, AUC, and Precision,\ndemonstrating the advantage of graph-based modeling in satisfaction prediction\ntasks. The study not only enriches the theoretical framework of user modeling\nbut also highlights its practical value in optimizing human-computer\ninteraction experience.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u7528\u6237\u6ee1\u610f\u5ea6\u5206\u7c7b\u6846\u67b6\uff0c\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u591a\u9879\u6027\u80fd\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u63d0\u5347\u4eba\u673a\u4ea4\u4e92\u4f53\u9a8c\u3002", "motivation": "\u9488\u5bf9\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u4ea4\u4e92\u5173\u7cfb\u548c\u591a\u7ef4\u7279\u5f81\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u6846\u67b6\u4ee5\u63d0\u9ad8\u7528\u6237\u6ee1\u610f\u5ea6\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u4f7f\u7528\u56fe\u5377\u79ef\u548c\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u5c40\u90e8\u7279\u5f81\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\uff0c\u901a\u8fc7\u5168\u5c40\u6c60\u5316\u548c\u5206\u7c7b\u5c42\u5b9e\u73b0\u81ea\u52a8\u5316\u6ee1\u610f\u5ea6\u5206\u7c7b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728Kaggle\u7684\u516c\u5171\u7528\u6237\u6ee1\u610f\u5ea6\u8c03\u67e5\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u4e0e\u591a\u4e2a\u57fa\u51c6\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5404\u9879\u6027\u80fd\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u7528\u6237\u6ee1\u610f\u5ea6\u5206\u7c7b\u6846\u67b6\u5728\u51c6\u786e\u6027\u3001F1\u5206\u6570\u3001AUC\u548c\u7cbe\u786e\u5ea6\u7b49\u591a\u4e2a\u6027\u80fd\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u56fe\u6a21\u578b\u5728\u6ee1\u610f\u5ea6\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\u3002"}}
{"id": "2511.04249", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04249", "abs": "https://arxiv.org/abs/2511.04249", "authors": ["Marco Iannotta", "Yuxuan Yang", "Johannes A. Stork", "Erik Schaffernicht", "Todor Stoyanov"], "title": "Can Context Bridge the Reality Gap? Sim-to-Real Transfer of Context-Aware Policies", "comment": null, "summary": "Sim-to-real transfer remains a major challenge in reinforcement learning (RL)\nfor robotics, as policies trained in simulation often fail to generalize to the\nreal world due to discrepancies in environment dynamics. Domain Randomization\n(DR) mitigates this issue by exposing the policy to a wide range of randomized\ndynamics during training, yet leading to a reduction in performance. While\nstandard approaches typically train policies agnostic to these variations, we\ninvestigate whether sim-to-real transfer can be improved by conditioning the\npolicy on an estimate of the dynamics parameters -- referred to as context. To\nthis end, we integrate a context estimation module into a DR-based RL framework\nand systematically compare SOTA supervision strategies. We evaluate the\nresulting context-aware policies in both a canonical control benchmark and a\nreal-world pushing task using a Franka Emika Panda robot. Results show that\ncontext-aware policies outperform the context-agnostic baseline across all\nsettings, although the best supervision strategy depends on the task.", "AI": {"tldr": "\u901a\u8fc7\u4e0a\u4e0b\u6587\u4f30\u8ba1\u6a21\u5757\u548c\u9886\u57df\u968f\u673a\u5316\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u80fd\u5728\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8f6c\u79fb\u4e2d\u63d0\u9ad8\u653f\u7b56\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u8fdb\u884c\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u8f6c\u79fb\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u8bad\u7ec3\u5728\u4eff\u771f\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u6548\u679c\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "method": "\u5c06\u4e0a\u4e0b\u6587\u4f30\u8ba1\u6a21\u5757\u96c6\u6210\u5230\u57fa\u4e8e\u9886\u57df\u968f\u673a\u5316\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u5e76\u7cfb\u7edf\u6027\u6bd4\u8f83\u6700\u5148\u8fdb\u7684\u76d1\u7763\u7b56\u7565\u3002", "result": "\u4e0a\u4e0b\u6587\u611f\u77e5\u7b56\u7565\u5728\u5178\u578b\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u4e16\u754c\u63a8\u6324\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8a\u4e86\u65e0\u4e0a\u4e0b\u6587\u7b56\u7565\u7684\u57fa\u7ebf\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u611f\u77e5\u7b56\u7565\u5728\u6240\u6709\u8bbe\u7f6e\u4e2d\u5747\u4f18\u4e8e\u4e0a\u4e0b\u6587\u65e0\u5173\u7684\u57fa\u7ebf\uff0c\u5c3d\u7ba1\u6700\u4f73\u76d1\u7763\u7b56\u7565\u56e0\u4efb\u52a1\u800c\u5f02\u3002"}}
{"id": "2511.04219", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.04219", "abs": "https://arxiv.org/abs/2511.04219", "authors": ["Mingzhi Lin", "Teng Huang", "Han Ding", "Cui Zhao", "Fei Wang", "Ge Wang", "Wei Xi"], "title": "Active Domain Adaptation for mmWave-based HAR via Renyi Entropy-based Uncertainty Estimation", "comment": null, "summary": "Human Activity Recognition (HAR) using mmWave radar provides a non-invasive\nalternative to traditional sensor-based methods but suffers from domain shift,\nwhere model performance declines in new users, positions, or environments. To\naddress this, we propose mmADA, an Active Domain Adaptation (ADA) framework\nthat efficiently adapts mmWave-based HAR models with minimal labeled data.\nmmADA enhances adaptation by introducing Renyi Entropy-based uncertainty\nestimation to identify and label the most informative target samples.\nAdditionally, it leverages contrastive learning and pseudo-labeling to refine\nfeature alignment using unlabeled data. Evaluations with a TI IWR1443BOOST\nradar across multiple users, positions, and environments show that mmADA\nachieves over 90% accuracy in various cross-domain settings. Comparisons with\nfive baselines confirm its superior adaptation performance, while further tests\non unseen users, environments, and two additional open-source datasets validate\nits robustness and generalization.", "AI": {"tldr": "mmADA\u662f\u4e00\u4e2a\u6709\u6548\u7684\u4e3b\u52a8\u9886\u57df\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5c0f\u6807\u8bb0\u6570\u636e\u9002\u5e94mmWave\u96f7\u8fbe\u7684\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u5728\u65b0\u7528\u6237\u548c\u73af\u5883\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3mmWave\u96f7\u8fbe\u5728\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\u4e2d\u7684\u9886\u57df\u79fb\u52a8\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u65b0\u7528\u6237\u3001\u4f4d\u7f6e\u548c\u73af\u5883\u4e0b\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5f15\u5165Renyi\u71b5\u7684\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u4f30\u8ba1\u3001\u5bf9\u6bd4\u5b66\u4e60\u548c\u4f2a\u6807\u8bb0\u7b49\u6280\u672f\uff0cmmADA\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4e3b\u52a8\u9886\u57df\u9002\u5e94\u3002", "result": "mmADA\u5728\u591a\u4e2a\u7528\u6237\u3001\u4f4d\u7f6e\u548c\u73af\u5883\u4e0b\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5404\u79cd\u9886\u57df\u95f4\u8bbe\u7f6e\u4e0b\u8d85\u8fc790%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5728\u4e0e\u4e94\u4e2a\u57fa\u7ebf\u7684\u6bd4\u8f83\u4e2d\u5c55\u73b0\u4e86\u4f18\u8d8a\u7684\u9002\u5e94\u6027\u80fd\u3002", "conclusion": "mmADA\u6846\u67b6\u5728\u4e0d\u540c\u7528\u6237\u548c\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u8d85\u8fc790%\u7684\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u5c55\u793a\u4e86\u5176\u5f3a\u5927\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.04251", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04251", "abs": "https://arxiv.org/abs/2511.04251", "authors": ["Jinfeng Liang", "Haocheng Guo", "Ximin Lyu"], "title": "Design and Control of a Coaxial Dual-rotor Reconfigurable Tailsitter UAV Based on Swashplateless Mechanism", "comment": "8 pages 12 figures", "summary": "The tailsitter vertical takeoff and landing (VTOL) UAV is widely used due to\nits lower dead weight, which eliminates the actuators and mechanisms for\ntilting. However, the tailsitter UAV is susceptible to wind disturbances in\nmulti-rotor mode, as it exposes a large frontal fuselage area. To address this\nissue, our tailsitter UAV features a reconfigurable wing design, allowing wings\nto retract in multi-rotor mode and extend in fixed- wing mode. Considering\npower efficiency, we design a coaxial heterogeneous dual-rotor configuration,\nwhich significantly re- duces the total power consumption. To reduce structural\nweight and simplify structural complexity, we employ a swashplateless mechanism\nwith an improved design to control pitch and roll in multi-rotor mode. We\noptimize the structure of the swashplateless mechanism by adding flapping\nhinges, which reduces vibration during cyclic acceleration and deceleration.\nFinally, we perform comprehensive transition flight tests to validate stable\nflight performance across the entire flight envelope of the tailsitter UAV.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u6539\u8fdb\u7684\u5c3e\u6446\u673a UAV \u8bbe\u8ba1\uff0c\u91c7\u7528\u53ef\u91cd\u914d\u7f6e\u673a\u7ffc\u548c\u540c\u8f74\u53cc\u65cb\u7ffc\u914d\u7f6e\uff0c\u4ee5\u5e94\u5bf9\u98ce\u5e72\u6270\u5e76\u63d0\u9ad8\u80fd\u6548\uff0c\u6700\u7ec8\u901a\u8fc7\u98de\u884c\u6d4b\u8bd5\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "motivation": "\u9488\u5bf9\u5c3e\u6446\u673a UAV \u5728\u591a\u65cb\u7ffc\u6a21\u5f0f\u4e0b\u6613\u53d7\u98ce\u5e72\u6270\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u8bbe\u8ba1\u4ee5\u63d0\u9ad8\u5176\u52a8\u529b\u6548\u7387\u548c\u7ed3\u6784\u7b80\u5316\u3002", "method": "\u91c7\u7528\u53ef\u91cd\u914d\u7f6e\u7684\u673a\u7ffc\u8bbe\u8ba1\u548c\u540c\u8f74\u5f02\u6784\u53cc\u65cb\u7ffc\u914d\u7f6e\uff0c\u4f18\u5316\u4e86\u65e0\u6447\u6746\u673a\u6784\uff0c\u5e76\u901a\u8fc7\u589e\u52a0\u62cd\u6253\u94f0\u94fe\u6765\u51cf\u5c11\u632f\u52a8\u3002", "result": "\u901a\u8fc7\u65b0\u8bbe\u8ba1\u7684\u8235\u673a\u673a\u5236\u548c\u51cf\u5c11\u7684\u603b\u4f53\u529f\u8017\uff0c\u5b9e\u73b0\u4e86\u5c3e\u6446\u673a\u5728\u56fa\u5b9a\u7ffc\u548c\u591a\u65cb\u7ffc\u6a21\u5f0f\u4e0b\u7684\u6709\u6548\u8f6c\u6362\u548c\u7a33\u5b9a\u98de\u884c\u3002", "conclusion": "\u5bf9\u5c3e\u6446\u673a UAV \u7684\u98de\u884c\u6027\u80fd\u8fdb\u884c\u4e86\u5168\u9762\u7684\u8fc7\u6e21\u98de\u884c\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u6574\u4e2a\u98de\u884c\u52a8\u4f5c\u8303\u56f4\u5185\u7684\u7a33\u5b9a\u98de\u884c\u6027\u80fd\u3002"}}
{"id": "2511.04262", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.04262", "abs": "https://arxiv.org/abs/2511.04262", "authors": ["Eric M\u00f6rth", "Morgan L. Turner", "Cydney Nielsen", "Xianhao Carton Liu", "Mark Keller", "Lisa Choy", "John Conroy", "Tabassum Kakar", "Clarence Yapp", "Alex Wong", "Peter Sorger", "Liam McLaughlin", "Sanjay Jain", "Johanna Beyer", "Hanspeter Pfister", "Chen Zhu-Tian", "Nils Gehlenborg"], "title": "Vitessce Link: A Mixed Reality and 2D Display Hybrid Approach for Visual Analysis of 3D Tissue Maps", "comment": null, "summary": "Advances in spatial omics and high-resolution imaging enable the creation of\nthree-dimensional (3D) tissue maps that capture cellular organization and\ninteractions in situ. While these data provide critical insights into tissue\nfunction and disease, their exploration is often constrained by tools limited\nto 2D displays or stereoscopic rendering without analytical integration. We\npresent Vitessce Link, a web-based hybrid framework that unites a 3D\nstereoscopic view in mixed reality with a synchronized 2D display environment.\nUsers can navigate volumetric data with intuitive hand gestures while\ncontrolling channels, filters, and derived data views through the Vitessce\nplatform. Built on open standards and running entirely in the browser, Vitessce\nLink minimizes friction, supports integration with computational notebooks, and\nsynchronizes interactions across devices via a lightweight WebSocket\narchitecture. Case studies in nephrology and oncology demonstrate how the\nhybrid approach enhances segmentation evaluation, distance measurement, and\ninterpretation of spatial relationships. Vitessce Link establishes a paradigm\nfor integrative, web-native analysis of 3D tissue maps.", "AI": {"tldr": "Vitessce Link\u662f\u4e00\u4e2a\u7f51\u7edc\u539f\u751f\u7684\u6df7\u5408\u6846\u67b6\uff0c\u901a\u8fc73D\u7acb\u4f53\u89c6\u56fe\u548c\u540c\u6b65\u76842D\u5c55\u793a\uff0c\u4fc3\u8fdb\u4e86\u5bf93D\u7ec4\u7ec7\u56fe\u8c31\u7684\u5206\u6790\u3002", "motivation": "\u968f\u7740\u7a7a\u95f4\u7ec4\u5b66\u548c\u9ad8\u5206\u8fa8\u7387\u6210\u50cf\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u9700\u8981\u65b0\u7684\u5de5\u5177\u6765\u5168\u9762\u63a2\u7d223D\u7ec4\u7ec7\u56fe\u8c31\u6570\u636e\uff0c\u800c\u73b0\u6709\u5de5\u5177\u901a\u5e38\u5c40\u9650\u4e8e2D\u663e\u793a\u3002", "method": "\u901a\u8fc7WebSocket\u67b6\u6784\u5b9e\u73b02D\u548c3D\u6df7\u5408\u73b0\u5b9e\u7684\u540c\u6b65\u663e\u793a\u4e0e\u4ea4\u4e92\uff0c\u652f\u6301\u7528\u6237\u901a\u8fc7\u624b\u52bf\u64cd\u4f5c\u5bfc\u822a\u4f53\u79ef\u6570\u636e\u3002", "result": "\u5728\u80be\u810f\u75c5\u5b66\u548c\u80bf\u7624\u5b66\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u5c55\u793a\u4e86\u8be5\u6df7\u5408\u65b9\u6cd5\u5982\u4f55\u63d0\u5347\u5206\u5272\u8bc4\u4f30\u3001\u8ddd\u79bb\u6d4b\u91cf\u548c\u7a7a\u95f4\u5173\u7cfb\u7684\u89e3\u6790\u3002", "conclusion": "Vitessce Link\u5efa\u7acb\u4e86\u4e00\u4e2a\u7528\u4e8e3D\u7ec4\u7ec7\u56fe\u8c31\u96c6\u6210\u5206\u6790\u7684\u7f51\u7edc\u539f\u751f\u8303\u5f0f\u3002"}}
{"id": "2511.04320", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04320", "abs": "https://arxiv.org/abs/2511.04320", "authors": ["Kuankuan Sima", "Longbin Tang", "Haozhe Ma", "Lin Zhao"], "title": "MacroNav: Multi-Task Context Representation Learning Enables Efficient Navigation in Unknown Environments", "comment": null, "summary": "Autonomous navigation in unknown environments requires compact yet expressive\nspatial understanding under partial observability to support high-level\ndecision making. Existing approaches struggle to balance rich contextual\nrepresentation with navigation efficiency. We present MacroNav, a\nlearning-based navigation framework featuring two key components: (1) a\nlightweight context encoder trained via multi-task self-supervised learning to\ncapture multi-scale, navigation-centric spatial representations; and (2) a\nreinforcement learning policy that seamlessly integrates these representations\nwith graph-based reasoning for efficient action selection. Extensive\nexperiments demonstrate the context encoder's efficient and robust\nenvironmental understanding. Real-world deployments further validate MacroNav's\neffectiveness, yielding significant gains over state-of-the-art navigation\nmethods in both Success Rate (SR) and Success weighted by Path Length (SPL),\nwhile maintaining low computational cost. Code will be released upon\nacceptance.", "AI": {"tldr": "MacroNav\u662f\u4e00\u4e2a\u5b66\u4e60\u9a71\u52a8\u7684\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u7f16\u7801\u5668\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u672a\u77e5\u73af\u5883\u4e2d\u5bfc\u822a\u6548\u7387\u548c\u7a7a\u95f4\u7406\u89e3\u7684\u6311\u6218\uff0c\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "motivation": "\u5728\u672a\u77e5\u73af\u5883\u4e2d\u81ea\u4e3b\u5bfc\u822a\u9700\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u72b6\u6001\u4e0b\uff0c\u5b9e\u73b0\u7d27\u51d1\u800c\u5bcc\u6709\u8868\u73b0\u529b\u7684\u7a7a\u95f4\u7406\u89e3\uff0c\u4ee5\u652f\u6301\u9ad8\u5c42\u51b3\u7b56\u3002", "method": "\u91c7\u7528\u5b66\u4e60-based\u65b9\u6cd5\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u7684\u4e0a\u4e0b\u6587\u7f16\u7801\u5668\u548c\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u56fe\u63a8\u7406\u5b9e\u73b0\u9ad8\u6548\u7684\u52a8\u4f5c\u9009\u62e9\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u548c\u5b9e\u9645\u90e8\u7f72\uff0cMacroNav\u663e\u793a\u51fa\u4e86\u5176\u73af\u5883\u7406\u89e3\u7684\u9ad8\u6548\u6027\u4e0e\u7a33\u5065\u6027\uff0c\u6210\u529f\u7387\u548c\u8def\u5f84\u957f\u5ea6\u52a0\u6743\u6210\u529f\u7387\u5747\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "MacroNav\u5728\u5bfc\u822a\u6548\u7387\u548c\u73af\u5883\u7406\u89e3\u65b9\u9762\u5927\u5927\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u8def\u5f84\u957f\u5ea6\u52a0\u6743\u6210\u529f\u7387\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u4f4e\u3002"}}
{"id": "2511.04366", "categories": ["cs.HC", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.04366", "abs": "https://arxiv.org/abs/2511.04366", "authors": ["Weiyan Shi", "Kenny Tsu Wei Choo"], "title": "Towards Aligning Multimodal LLMs with Human Experts: A Focus on Parent-Child Interaction", "comment": "work in progress", "summary": "While multimodal large language models (MLLMs) are increasingly applied in\nhuman-centred AI systems, their ability to understand complex social\ninteractions remains uncertain. We present an exploratory study on aligning\nMLLMs with speech-language pathologists (SLPs) in analysing joint attention in\nparent-child interactions, a key construct in early social-communicative\ndevelopment. Drawing on interviews and video annotations with three SLPs, we\ncharacterise how observational cues of gaze, action, and vocalisation inform\ntheir reasoning processes. We then test whether an MLLM can approximate this\nworkflow through a two-stage prompting, separating observation from judgment.\nOur findings reveal that alignment is more robust at the observation layer,\nwhere experts share common descriptors, than at the judgement layer, where\ninterpretive criteria diverge. We position this work as a case-based probe into\nexpert-AI alignment in complex social behaviour, highlighting both the\nfeasibility and the challenges of applying MLLMs to socially situated\ninteraction analysis.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b(MMLMs)\u5728\u5206\u6790\u7236\u6bcd\u4e0e\u513f\u7ae5\u4e92\u52a8\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u5176\u5728\u89c2\u5bdf\u5c42\u9762\u7684\u5bf9\u9f50\u66f4\u4e3a\u6709\u6548\uff0c\u4f46\u5728\u5224\u65ad\u5c42\u9762\u5b58\u5728\u4e13\u5bb6\u89e3\u8bfb\u6807\u51c6\u7684\u5dee\u5f02\u3002", "motivation": "\u63a2\u8ba8\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u590d\u6742\u793e\u4ea4\u4e92\u52a8\u4e2d\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u65e9\u671f\u793e\u4ea4\u4f20\u64ad\u53d1\u5c55\u4e2d\u7684\u89d2\u8272\u3002", "method": "\u901a\u8fc7\u4e0e\u4e09\u4f4d\u8a00\u8bed\u8bed\u8a00 \u043f\u0430\u0442\u043e\u043b\u043e\u0433\u043e\u0432 (SLPs) \u7684\u8bbf\u8c08\u548c\u89c6\u9891\u6807\u6ce8\uff0c\u5206\u6790\u4e86\u513f\u7ae5\u4e0e\u7236\u6bcd\u4e92\u52a8\u4e2d\u7684\u5171\u540c\u6ce8\u610f\u529b\u3002", "result": "\u53d1\u73b0MLLM\u5728\u89c2\u5bdf\u5c42\u9762\u80fd\u66f4\u597d\u5730\u6a21\u62df\u4e13\u5bb6\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u800c\u5728\u5224\u65ad\u5c42\u9762\u5b58\u5728\u8f83\u5927\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u663e\u793a\uff0c\u4e13\u5bb6\u4e0eAI\u5728\u89c2\u5bdf\u9636\u6bb5\u7684\u5bf9\u9f50\u66f4\u4e3a\u7a33\u5065\uff0c\u800c\u5728\u5224\u65ad\u5c42\u9762\u5219\u5b58\u5728\u89e3\u8bfb\u6807\u51c6\u7684\u5dee\u5f02\uff0c\u5f3a\u8c03\u4e86\u5728\u590d\u6742\u793e\u4ea4\u884c\u4e3a\u5206\u6790\u4e2d\u5e94\u7528MLLMs\u7684\u53ef\u884c\u6027\u4e0e\u6311\u6218\u3002"}}
{"id": "2511.04357", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04357", "abs": "https://arxiv.org/abs/2511.04357", "authors": ["Ma\u00eblic Neau", "Zoe Falomir", "Paulo E. Santos", "Anne-Gwenn Bosser", "C\u00e9dric Buche"], "title": "GraSP-VLA: Graph-based Symbolic Action Representation for Long-Horizon Planning with VLA Policies", "comment": null, "summary": "Deploying autonomous robots that can learn new skills from demonstrations is\nan important challenge of modern robotics. Existing solutions often apply\nend-to-end imitation learning with Vision-Language Action (VLA) models or\nsymbolic approaches with Action Model Learning (AML). On the one hand, current\nVLA models are limited by the lack of high-level symbolic planning, which\nhinders their abilities in long-horizon tasks. On the other hand, symbolic\napproaches in AML lack generalization and scalability perspectives. In this\npaper we present a new neuro-symbolic approach, GraSP-VLA, a framework that\nuses a Continuous Scene Graph representation to generate a symbolic\nrepresentation of human demonstrations. This representation is used to generate\nnew planning domains during inference and serves as an orchestrator for\nlow-level VLA policies, scaling up the number of actions that can be reproduced\nin a row. Our results show that GraSP-VLA is effective for modeling symbolic\nrepresentations on the task of automatic planning domain generation from\nobservations. In addition, results on real-world experiments show the potential\nof our Continuous Scene Graph representation to orchestrate low-level VLA\npolicies in long-horizon tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faGraSP-VLA\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u8fdb\u884c\u8ba1\u5212\u57df\u751f\u6210\uff0c\u5e76\u5728\u957f\u65f6\u95f4\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u826f\u597d\u7684\u534f\u8c03\u80fd\u529b\u3002", "motivation": "\u73b0\u4ee3\u673a\u5668\u4eba\u9762\u4e34\u4ece\u6f14\u793a\u4e2d\u5b66\u4e60\u65b0\u6280\u80fd\u7684\u6311\u6218\uff0c\u73b0\u6709\u7684\u89e3\u51b3\u65b9\u6848\u5728\u9ad8\u6c34\u5e73\u7b26\u53f7\u89c4\u5212\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5GraSP-VLA\uff0c\u901a\u8fc7\u8fde\u7eed\u573a\u666f\u56fe\u8868\u793a\u751f\u6210\u7b26\u53f7\u8868\u793a\uff0c\u5e76\u7528\u4e8e\u63a8\u7406\u671f\u95f4\u7684\u65b0\u89c4\u5212\u9886\u57df\u751f\u6210\u3002", "result": "GraSP-VLA\u5728\u81ea\u52a8\u89c4\u5212\u9886\u57df\u751f\u6210\u53ca\u5728\u957f\u65f6\u95f4\u4efb\u52a1\u4e2d\u534f\u8c03\u4f4e\u7ea7VLA\u7b56\u7565\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002", "conclusion": "GraSP-VLA\u5728\u81ea\u52a8\u89c4\u5212\u9886\u57df\u751f\u6210\u65b9\u9762\u8868\u73b0\u6709\u6548\uff0c\u5e76\u663e\u793a\u51fa\u5728\u957f\u65f6\u95f4\u4efb\u52a1\u4e2d\u534f\u8c03\u4f4e\u7ea7VLA\u7b56\u7565\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.04383", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.04383", "abs": "https://arxiv.org/abs/2511.04383", "authors": ["Yingping Yang", "Guangtao You", "Jiayi Chen", "Jiazhou Chen"], "title": "HPC-Vis: A Visual Analytic System for Interactive Exploration of Historical Painter Cohorts", "comment": null, "summary": "More than ten thousand Chinese historical painters are recorded in the\nliterature; their cohort analysis has always been a key area of research on\nChinese painting history for both professional historians and amateur\nenthusiasts. However, these painters have very diverse artistic styles and an\nextremely complex network of inheritance relationships (e.g., master-apprentice\nor style imitation relationships); traditional cohort analysis methods not only\nheavily rely on field experience, but also cost a lot of time and effort with\nnumerous but scattered historical documents. In this paper, we propose HPC-Vis,\na visual analytical system for interactive exploration of historical painter\ncohorts. Firstly, a three-stage reconstruction algorithm for inheritance\nrelationships of painters is proposed, which automatically converts the complex\nrelationship graph of historical painters into a forest structure that contains\nmultiple trees with clear inheriting chains, and we visually encoded this\nforest as a mountain map to intuitively show potential cohorts of historical\npainters. Secondly, a unified artistic style label system with three levels\n(i.e., subjects, techniques, and emotions) is established by using large\nlanguage models, and it is further visually encoded as a new foldable nested\ndoughnut chart. Finally, a visually guided human-computer collaborative\ninteractive exploration mechanism is constructed, in which a painter cohort\nrecommendation model is designed by integrating style, identity, time, space,\nand relationships. Two case studies and a user study demonstrate the advantage\nof HPC-Vis on assisting historians in discovering, defining, and validating\ncohorts of historical painters.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHPC-Vis\uff0c\u4e00\u4e2a\u7528\u4e8e\u5386\u53f2\u753b\u5bb6\u7fa4\u4f53\u5206\u6790\u7684\u53ef\u89c6\u5316\u7cfb\u7edf\uff0c\u5229\u7528\u591a\u79cd\u6280\u672f\u91cd\u6784\u590d\u6742\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u7814\u7a76\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u5386\u53f2\u753b\u5bb6\u590d\u6742\u5173\u7cfb\u548c\u591a\u6837\u827a\u672f\u98ce\u683c\u65f6\u6548\u7387\u4f4e\u3001\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86HPC-Vis\u7cfb\u7edf\uff0c\u5305\u62ec\u4e09\u9636\u6bb5\u91cd\u6784\u7b97\u6cd5\u3001\u7edf\u4e00\u7684\u827a\u672f\u98ce\u683c\u6807\u7b7e\u7cfb\u7edf\u548c\u4eba\u673a\u534f\u4f5c\u4ea4\u4e92\u63a2\u7d22\u673a\u5236\u3002", "result": "\u6210\u529f\u91cd\u6784\u5386\u53f2\u753b\u5bb6\u4e4b\u95f4\u7684\u7ee7\u627f\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86HPC-Vis\u5728\u753b\u5bb6\u7fa4\u4f53\u5206\u6790\u4e2d\u7684\u4f18\u52bf\u3002", "conclusion": "HPC-Vis\u4e3a\u5386\u53f2\u753b\u5bb6\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53ef\u89c6\u5316\u4ea4\u4e92\u63a2\u7d22\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7814\u7a76\u8005\u53d1\u73b0\u548c\u9a8c\u8bc1\u5386\u53f2\u753b\u5bb6\u7fa4\u4f53\u7684\u80fd\u529b\u3002"}}
{"id": "2511.04375", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04375", "abs": "https://arxiv.org/abs/2511.04375", "authors": ["Anna M\u00e9sz\u00e1ros", "Javier Alonso-Mora", "Jens Kober"], "title": "Studying the Effect of Explicit Interaction Representations on Learning Scene-level Distributions of Human Trajectories", "comment": null, "summary": "Effectively capturing the joint distribution of all agents in a scene is\nrelevant for predicting the true evolution of the scene and in turn providing\nmore accurate information to the decision processes of autonomous vehicles.\nWhile new models have been developed for this purpose in recent years, it\nremains unclear how to best represent the joint distributions particularly from\nthe perspective of the interactions between agents. Thus far there is no clear\nconsensus on how best to represent interactions between agents; whether they\nshould be learned implicitly from data by neural networks, or explicitly\nmodeled using the spatial and temporal relations that are more grounded in\nhuman decision-making. This paper aims to study various means of describing\ninteractions within the same network structure and their effect on the final\nlearned joint distributions. Our findings show that more often than not, simply\nallowing a network to establish interactive connections between agents based on\ndata has a detrimental effect on performance. Instead, having well defined\ninteractions (such as which agent of an agent pair passes first at an\nintersection) can often bring about a clear boost in performance.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4ee3\u7406\u95f4\u4ea4\u4e92\u7684\u8868\u793a\u5bf9\u8054\u5408\u5206\u5e03\u5b66\u4e60\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u660e\u786e\u5b9a\u4e49\u7684\u4ea4\u4e92\u5173\u7cfb\u80fd\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u800c\u4f9d\u8d56\u6570\u636e\u5b66\u4e60\u7684\u4ea4\u4e92\u5219\u5e38\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u5c3d\u7ba1\u8fd1\u5e74\u6765\u4e3a\u6b64\u76ee\u7684\u5f00\u53d1\u4e86\u65b0\u7684\u6a21\u578b\uff0c\u4f46\u5982\u4f55\u6700\u4f73\u5730\u8868\u793a\u8054\u5408\u5206\u5e03\uff0c\u5c24\u5176\u662f\u4ece\u4ee3\u7406\u95f4\u7684\u4ea4\u4e92\u89d2\u5ea6\uff0c\u4ecd\u4e0d\u6e05\u695a\u3002", "method": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u540c\u4e00\u7f51\u7edc\u7ed3\u6784\u4e2d\u63cf\u8ff0\u4ee3\u7406\u4e4b\u95f4\u4ea4\u4e92\u7684\u591a\u79cd\u65b9\u5f0f\u53ca\u5176\u5bf9\u6700\u7ec8\u5b66\u4e60\u7684\u8054\u5408\u5206\u5e03\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u660e\u786e\u5b9a\u4e49\u7684\u4ea4\u4e92\uff08\u4f8b\u5982\u4ea4\u53c9\u53e3\u4ee3\u7406\u5bf9\u4e2d\u54ea\u4e2a\u4ee3\u7406\u5148\u901a\u8fc7\uff09\u901a\u5e38\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u800c\u8ba9\u7f51\u7edc\u6839\u636e\u6570\u636e\u5efa\u7acb\u4ea4\u4e92\u8fde\u63a5\u5219\u4f1a\u5bf9\u6027\u80fd\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002", "conclusion": "\u5728\u573a\u666f\u4e2d\u6709\u6548\u6355\u83b7\u6240\u6709\u4ee3\u7406\u7684\u8054\u5408\u5206\u5e03\u5bf9\u4e8e\u9884\u6d4b\u573a\u666f\u7684\u771f\u5b9e\u6f14\u53d8\u4ee5\u53ca\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u4fe1\u606f\u7ed9\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u8fc7\u7a0b\u81f3\u5173\u91cd\u8981\u3002\u901a\u8fc7\u660e\u786e\u5b9a\u4e49\u4e92\u52a8\u5173\u7cfb\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u800c\u4ec5\u4f9d\u8d56\u7f51\u7edc\u57fa\u4e8e\u6570\u636e\u5efa\u7acb\u4e92\u52a8\u8fde\u63a5\u901a\u5e38\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002"}}
{"id": "2511.04478", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04478", "abs": "https://arxiv.org/abs/2511.04478", "authors": ["Hyo Jin Do", "Zahra Ashktorab", "Jasmina Gajcin", "Erik Miehling", "Mart\u00edn Santill\u00e1n Cooper", "Qian Pan", "Elizabeth M. Daly", "Werner Geyer"], "title": "Generate, Evaluate, Iterate: Synthetic Data for Human-in-the-Loop Refinement of LLM Judges", "comment": "29 pages, 4 figures", "summary": "The LLM-as-a-judge paradigm enables flexible, user-defined evaluation, but\nits effectiveness is often limited by the scarcity of diverse, representative\ndata for refining criteria. We present a tool that integrates synthetic data\ngeneration into the LLM-as-a-judge workflow, empowering users to create\ntailored and challenging test cases with configurable domains, personas,\nlengths, and desired outcomes, including borderline cases. The tool also\nsupports AI-assisted inline editing of existing test cases. To enhance\ntransparency and interpretability, it reveals the prompts and explanations\nbehind each generation. In a user study (N=24), 83% of participants preferred\nthe tool over manually creating or selecting test cases, as it allowed them to\nrapidly generate diverse synthetic data without additional workload. The\ngenerated synthetic data proved as effective as hand-crafted data for both\nrefining evaluation criteria and aligning with human preferences. These\nfindings highlight synthetic data as a promising alternative, particularly in\ncontexts where efficiency and scalability are critical.", "AI": {"tldr": "\u5f15\u5165\u5408\u6210\u6570\u636e\u751f\u6210\u5de5\u5177\u4ee5\u589e\u5f3aLLM-as-a-judge\u7684\u7075\u6d3b\u6027\uff0c\u7528\u6237\u53ef\u5feb\u901f\u521b\u5efa\u591a\u6837\u5316\u7684\u6d4b\u8bd5\u7528\u4f8b\uff0c\u7ed3\u679c\u663e\u793a\u5408\u6210\u6570\u636e\u5728\u8bc4\u4f30\u4e2d\u540c\u6837\u6709\u6548\u3002", "motivation": "\u89e3\u51b3LLM-as-a-judge\u8303\u5f0f\u4e2d\u6570\u636e\u7a00\u7f3a\u548c\u8bc4\u4f30\u6807\u51c6\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u5408\u6210\u6570\u636e\u751f\u6210\u5de5\u5177\u5230LLM-as-a-judge\u5de5\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u652f\u6301\u7528\u6237\u521b\u5efa\u53ef\u5b9a\u5236\u7684\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5e76\u8fdb\u884cAI\u8f85\u52a9\u7f16\u8f91\u3002", "result": "\u7528\u6237\u7814\u7a76\u663e\u793a83%\u7684\u53c2\u4e0e\u8005\u504f\u597d\u4f7f\u7528\u5de5\u5177\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u4e14\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u4e0e\u624b\u5de5\u5236\u4f5c\u7684\u6570\u636e\u5728\u6548\u679c\u4e0a\u76f8\u5f53\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u4f5c\u4e3a\u8bc4\u4f30\u6807\u51c6\u7cbe\u70bc\u548c\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u7279\u522b\u5728\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u81f3\u5173\u91cd\u8981\u7684\u73af\u5883\u4e2d\u3002"}}
{"id": "2511.04381", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04381", "abs": "https://arxiv.org/abs/2511.04381", "authors": ["Dexin wang", "Faliang Chang", "Chunsheng Liu"], "title": "ForeRobo: Unlocking Infinite Simulation Data for 3D Goal-driven Robotic Manipulation", "comment": null, "summary": "Efficiently leveraging simulation to acquire advanced manipulation skills is\nboth challenging and highly significant. We introduce \\textit{ForeRobo}, a\ngenerative robotic agent that utilizes generative simulations to autonomously\nacquire manipulation skills driven by envisioned goal states. Instead of\ndirectly learning low-level policies, we advocate integrating generative\nparadigms with classical control. Our approach equips a robotic agent with a\nself-guided \\textit{propose-generate-learn-actuate} cycle. The agent first\nproposes the skills to be acquired and constructs the corresponding simulation\nenvironments; it then configures objects into appropriate arrangements to\ngenerate skill-consistent goal states (\\textit{ForeGen}). Subsequently, the\nvirtually infinite data produced by ForeGen are used to train the proposed\nstate generation model (\\textit{ForeFormer}), which establishes point-wise\ncorrespondences by predicting the 3D goal position of every point in the\ncurrent state, based on the scene state and task instructions. Finally,\nclassical control algorithms are employed to drive the robot in real-world\nenvironments to execute actions based on the envisioned goal states. Compared\nwith end-to-end policy learning methods, ForeFormer offers superior\ninterpretability and execution efficiency. We train and benchmark ForeFormer\nacross a variety of rigid-body and articulated-object manipulation tasks, and\nobserve an average improvement of 56.32\\% over the state-of-the-art state\ngeneration models, demonstrating strong generality across different\nmanipulation patterns. Moreover, in real-world evaluations involving more than\n20 robotic tasks, ForeRobo achieves zero-shot sim-to-real transfer and exhibits\nremarkable generalization capabilities, attaining an average success rate of\n79.28\\%.", "AI": {"tldr": "ForeRobo\u901a\u8fc7\u751f\u6210\u4eff\u771f\u7ed3\u5408\u7ecf\u5178\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u64cd\u4f5c\u6280\u80fd\u5b66\u4e60\uff0c\u5c55\u793a\u51fa\u5728\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u7684\u5f3a\u5927\u63a8\u5e7f\u80fd\u529b\u548c\u6210\u529f\u7387\u3002", "motivation": "\u9ad8\u6548\u5229\u7528\u4eff\u771f\u6280\u672f\u83b7\u53d6\u9ad8\u7ea7\u64cd\u4f5c\u6280\u80fd\u662f\u4e00\u9879\u91cd\u8981\u4e14\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002", "method": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u751f\u6210\u578b\u673a\u5668\u4eba\u4ee3\u7406ForeRobo\uff0c\u7ed3\u5408\u751f\u6210\u4eff\u771f\u4e0e\u7ecf\u5178\u63a7\u5236\uff0c\u901a\u8fc7\u81ea\u6211\u6307\u5bfc\u7684\textit{propose-generate-learn-actuate}\u5faa\u73af\u83b7\u53d6\u64cd\u4f5c\u6280\u80fd\u3002", "result": "ForeRobo\u5728\u5404\u79cd\u64cd\u4f5c\u4efb\u52a1\u4e2d\u76f8\u8f83\u4e8e\u6700\u5148\u8fdb\u7684\u72b6\u6001\u751f\u6210\u6a21\u578b\u5e73\u5747\u63d0\u9ad8\u4e8656.32%\u3002", "conclusion": "ForeRobo\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u63a8\u5e7f\u80fd\u529b\u548c\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u6210\u529f\u8868\u73b0\uff0c\u8fbe\u5230\u4e8679.28%\u7684\u5e73\u5747\u6210\u529f\u7387\u3002"}}
{"id": "2511.04487", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.04487", "abs": "https://arxiv.org/abs/2511.04487", "authors": ["Jaime Banks"], "title": "Perceptions of AI Bad Behavior: Variations on Discordant Non-Performance", "comment": null, "summary": "Popular discourses are thick with narratives of generative AI's problematic\nfunctions and outcomes, yet there is little understanding of how non-experts\nconsider AI activities to constitute bad behavior. This study starts to bridge\nthat gap through inductive analysis of interviews with non-experts (N = 28)\nfocusing on large-language models in general and their bad behavior,\nspecifically. Results suggest bad behaviors are not especially salient when\npeople discuss AI generally but the notion of AI behaving badly is easily\nengaged when prompted, and bad behavior becomes even more salient when\nevaluating specific AI behaviors. Types of observed behaviors considered bad\nmostly align with their inspiring moral foundations; across all observed\nbehaviors, some variations on non-performance and social discordance were\npresent. By scaffolding findings at the intersections of moral foundations\ntheory, construal level theory, and moral dyadism, a tentative framework for\nconsidering AI bad behavior is proposed.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u975e\u4e13\u5bb6\u5bf9AI\u4e0d\u5f53\u884c\u4e3a\u7684\u770b\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u521d\u6b65\u6846\u67b6\uff0c\u5f3a\u8c03\u9053\u5fb7\u57fa\u7840\u3001\u63a8\u7406\u6c34\u5e73\u548c\u9053\u5fb7\u4e8c\u5143\u8bba\u5bf9AI\u884c\u4e3a\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u586b\u8865\u5bf9\u975e\u4e13\u5bb6\u5982\u4f55\u770b\u5f85AI\u4e0d\u5f53\u884c\u4e3a\u7406\u89e3\u7684\u7a7a\u767d\uff0c\u63a2\u7d22\u4eba\u4eec\u5982\u4f55\u8ba8\u8bba\u548c\u8bc4\u4f30AI\u7684\u4e0d\u5f53\u884c\u4e3a\u3002", "method": "\u901a\u8fc7\u5bf928\u540d\u975e\u4e13\u5bb6\u7684\u8bbf\u8c08\u8fdb\u884c\u5f52\u7eb3\u5206\u6790\uff0c\u805a\u7126\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ca\u5176\u4e0d\u5f53\u884c\u4e3a\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u8ba8\u8bbaAI\u65f6\uff0c\u4e0d\u5f53\u884c\u4e3a\u5e76\u4e0d\u7279\u522b\u663e\u8457\uff0c\u4f46\u5728\u7279\u5b9a\u60c5\u51b5\u4e0b\u5f88\u5bb9\u6613\u88ab\u63d0\u53ca\uff0c\u5e76\u4e14\u5728\u8bc4\u4f30\u5177\u4f53\u884c\u4e3a\u65f6\u53d8\u5f97\u66f4\u4e3a\u663e\u8457\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8003\u8651AI\u4e0d\u5f53\u884c\u4e3a\u7684\u521d\u6b65\u6846\u67b6\uff0c\u5f3a\u8c03\u9053\u5fb7\u57fa\u7840\u7406\u8bba\u3001\u63a8\u7406\u6c34\u5e73\u7406\u8bba\u548c\u9053\u5fb7\u4e8c\u5143\u8bba\u7684\u4ea4\u6c47\u70b9\u3002"}}
{"id": "2511.04421", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04421", "abs": "https://arxiv.org/abs/2511.04421", "authors": ["Yueyang Weng", "Xiaopeng Zhang", "Yongjin Mu", "Yingcong Zhu", "Yanjie Li", "Qi Liu"], "title": "Temporal Action Selection for Action Chunking", "comment": null, "summary": "Action chunking is a widely adopted approach in Learning from Demonstration\n(LfD). By modeling multi-step action chunks rather than single-step actions,\naction chunking significantly enhances modeling capabilities for human expert\npolicies. However, the reduced decision frequency restricts the utilization of\nrecent observations, degrading reactivity - particularly evident in the\ninadequate adaptation to sensor noise and dynamic environmental changes.\nExisting efforts to address this issue have primarily resorted to trading off\nreactivity against decision consistency, without achieving both. To address\nthis limitation, we propose a novel algorithm, Temporal Action Selector (TAS),\nwhich caches predicted action chunks from multiple timesteps and dynamically\nselects the optimal action through a lightweight selector network. TAS achieves\nbalanced optimization across three critical dimensions: reactivity, decision\nconsistency, and motion coherence. Experiments across multiple tasks with\ndiverse base policies show that TAS significantly improves success rates -\nyielding an absolute gain of up to 73.3%. Furthermore, integrating TAS as a\nbase policy with residual reinforcement learning (RL) substantially enhances\ntraining efficiency and elevates the performance plateau. Experiments in both\nsimulation and physical robots confirm the method's efficacy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7b97\u6cd5TAS\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u4f5c\u5206\u5757\u5728\u53cd\u5e94\u6027\u4e0e\u51b3\u7b56\u4e00\u81f4\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6210\u529f\u7387\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u52a8\u4f5c\u5206\u5757\u65b9\u6cd5\u4e2d\u53cd\u5e94\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u4f20\u611f\u5668\u566a\u58f0\u548c\u52a8\u6001\u73af\u5883\u53d8\u5316\u4e0b\u7684\u9002\u5e94\u6027\u5dee\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7b97\u6cd5TAS\uff0c\u901a\u8fc7\u7f13\u5b58\u6765\u81ea\u591a\u4e2a\u65f6\u95f4\u6b65\u7684\u9884\u6d4b\u52a8\u4f5c\u5757\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u5316\u7684\u9009\u62e9\u7f51\u7edc\u52a8\u6001\u9009\u62e9\u6700\u4f73\u52a8\u4f5c\u3002", "result": "\u5728\u591a\u9879\u4efb\u52a1\u7684\u5b9e\u9a8c\u4e2d\uff0cTAS\u7684\u6210\u529f\u7387\u63d0\u5347\u4e86\u9ad8\u8fbe73.3%\u3002\u5728\u96c6\u6210TAS\u548c\u6b8b\u5dee\u5f3a\u5316\u5b66\u4e60\u540e\uff0c\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u5e73\u53f0\u90fd\u6709\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "TAS\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u529f\u7387\uff0c\u5e76\u5728\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u4e0a\u5e26\u6765\u4e86\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2511.04614", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.04614", "abs": "https://arxiv.org/abs/2511.04614", "authors": ["Seok-Hyun Ga", "Chun-Yen Chang", "Sonya Martin"], "title": "Students' Acceptance of Arduino Technology Integration in Student-Led Science Inquiry: Insights from the Technology Acceptance Model", "comment": "13 pages, 3 figures, 2 tables", "summary": "This study examines high school students' acceptance of Arduino technology in\na student-led, inquiry-based science class, using the extended Technology\nAcceptance Model (TAM2) as a guiding framework. Through qualitative analysis of\ninterviews and classroom observations, we explored how students perceived\nArduino's usefulness and ease of use. Going beyond traditional quantitative TAM\nstudies, this qualitative TAM research provides a nuanced, in-depth\nunderstanding of the contextual factors shaping technology acceptance. Key\nfindings reveal that acceptance was driven not only by instrumental factors\nlike job relevance and output quality but also by the unique sociocultural\ncontext of the Korean education system, where technology use was perceived as\nvaluable for university admissions (subjective norm and image). Critically,\nunlike earlier research that emphasized programming challenges, participants in\nthis study found Arduino accessible and intuitive, thanks to integrated visual\nblock-coding tools. These findings highlight the importance of both\ntechnological design and pedagogical support in shaping students' experiences.\nImplications for science curriculum design, teacher preparation, and equitable\ntechnology integration in secondary education are discussed.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u9ad8\u4e2d\u751f\u5728\u79d1\u5b66\u8bfe\u5802\u4e0a\u5bf9Arduino\u6280\u672f\u7684\u63a5\u53d7\u5ea6\uff0c\u53d1\u73b0\u793e\u4f1a\u6587\u5316\u56e0\u7d20\u548c\u6280\u672f\u8bbe\u8ba1\u5bf9\u63a5\u53d7\u5ea6\u6709\u91cd\u8981\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u4e86\u89e3\u9ad8\u4e2d\u5b66\u751f\u5728\u57fa\u4e8e\u8be2\u95ee\u7684\u79d1\u5b66\u8bfe\u5802\u4e0a\u5bf9Arduino\u6280\u672f\u7684\u63a5\u53d7\u5ea6\u3002", "method": "\u901a\u8fc7\u5b9a\u6027\u5206\u6790\u8bbf\u8c08\u548c\u8bfe\u5802\u89c2\u5bdf\uff0c\u63a2\u8ba8\u5b66\u751f\u5bf9Arduino\u7684\u611f\u77e5\u3002", "result": "\u5173\u952e\u53d1\u73b0\u8868\u660e\uff0c\u63a5\u53d7\u5ea6\u4e0d\u4ec5\u53d7\u5230\u5de5\u4f5c\u76f8\u5173\u6027\u548c\u8f93\u51fa\u8d28\u91cf\u7b49\u5de5\u5177\u6027\u56e0\u7d20\u7684\u9a71\u52a8\uff0c\u4e5f\u53d7\u5230\u97e9\u56fd\u6559\u80b2\u7cfb\u7edf\u72ec\u7279\u7684\u793e\u4f1a\u6587\u5316\u80cc\u666f\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u6280\u672f\u8bbe\u8ba1\u548c\u6559\u5b66\u652f\u6301\u5bf9\u5b66\u751f\u4f53\u9a8c\u5177\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u5fc5\u987b\u8003\u8651\u793e\u4f1a\u6587\u5316\u56e0\u7d20\u3002"}}
{"id": "2511.04555", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04555", "abs": "https://arxiv.org/abs/2511.04555", "authors": ["Tao Lin", "Yilei Zhong", "Yuxin Du", "Jingjing Zhang", "Jiting Liu", "Yinxinyu Chen", "Encheng Gu", "Ziyan Liu", "Hongyi Cai", "Yanwen Zou", "Lixing Zou", "Zhaoye Zhou", "Gen Li", "Bo Zhao"], "title": "Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment", "comment": "Github: https://github.com/MINT-SJTU/Evo-1", "summary": "Vision-Language-Action (VLA) models have emerged as a powerful framework that\nunifies perception, language, and control, enabling robots to perform diverse\ntasks through multimodal understanding. However, current VLA models typically\ncontain massive parameters and rely heavily on large-scale robot data\npretraining, leading to high computational costs during training, as well as\nlimited deployability for real-time inference. Moreover, most training\nparadigms often degrade the perceptual representations of the vision-language\nbackbone, resulting in overfitting and poor generalization to downstream tasks.\nIn this work, we present Evo-1, a lightweight VLA model that reduces\ncomputation and improves deployment efficiency, while maintaining strong\nperformance without pretraining on robot data. Evo-1 builds on a native\nmultimodal Vision-Language model (VLM), incorporating a novel cross-modulated\ndiffusion transformer along with an optimized integration module, together\nforming an effective architecture. We further introduce a two-stage training\nparadigm that progressively aligns action with perception, preserving the\nrepresentations of the VLM. Notably, with only 0.77 billion parameters, Evo-1\nachieves state-of-the-art results on the Meta-World and RoboTwin suite,\nsurpassing the previous best models by 12.4% and 6.9%, respectively, and also\nattains a competitive result of 94.8% on LIBERO. In real-world evaluations,\nEvo-1 attains a 78% success rate with high inference frequency and low memory\noverhead, outperforming all baseline methods. We release code, data, and model\nweights to facilitate future research on lightweight and efficient VLA models.", "AI": {"tldr": "Evo-1\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u8f7b\u91cf\u5316VLA\u6a21\u578b\uff0c\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u63d0\u5347\u4e86\u90e8\u7f72\u6548\u7387\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u65e0\u9700\u4f9d\u8d56\u673a\u5668\u4eba\u6570\u636e\u9884\u8bad\u7ec3\u3002", "motivation": "\u5f53\u524d\u7684VLA\u6a21\u578b\u53c2\u6570\u5e9e\u5927\uff0c\u5bf9\u5927\u89c4\u6a21\u673a\u5668\u4eba\u6570\u636e\u9884\u8bad\u7ec3\u4f9d\u8d56\u5f3a\uff0c\u5bfc\u81f4\u8bad\u7ec3\u65f6\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u5b9e\u65f6\u63a8\u7406\u65f6\u53ef\u90e8\u7f72\u6027\u5dee\uff0c\u540c\u65f6\u4e5f\u964d\u4f4e\u4e86\u611f\u77e5\u8868\u793a\u7684\u8d28\u91cf\uff0c\u51fa\u73b0\u8fc7\u62df\u5408\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "method": "Evo-1\u91c7\u7528\u4e86\u4ea4\u53c9\u8c03\u5236\u6269\u6563\u53d8\u6362\u5668\u548c\u4f18\u5316\u7684\u96c6\u6210\u6a21\u5757\uff0c\u5e76\u5f15\u5165\u4e86\u5206\u9636\u6bb5\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u4f7f\u52a8\u4f5c\u4e0e\u611f\u77e5\u9010\u6b65\u5bf9\u9f50\uff0c\u4fdd\u6301\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u8868\u73b0\u3002", "result": "Evo-1\u4ec5\u4f7f\u75280.77\u4ebf\u53c2\u6570\uff0c\u5728Meta-World\u548cRoboTwin\u6d4b\u8bd5\u96c6\u4e2d\u5206\u522b\u8d85\u8d8a\u4e86\u4e4b\u524d\u6700\u4f73\u6a21\u578b12.4%\u548c6.9%\uff0c\u5728LIBERO\u4e0a\u7684\u6210\u7ee9\u4e5f\u8fbe\u5230\u4e8694.8%\u3002\u5728\u5b9e\u9645\u8bc4\u4f30\u4e2d\uff0cEvo-1\u7684\u6210\u529f\u7387\u8fbe\u523078%\uff0c\u63a8\u7406\u9891\u7387\u9ad8\u4e14\u5185\u5b58\u5f00\u9500\u5c0f\uff0c\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Evo-1\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\uff0c\u5177\u6709\u8f83\u4f4e\u7684\u8ba1\u7b97\u9700\u6c42\u548c\u9ad8\u6548\u7684\u90e8\u7f72\u80fd\u529b\uff0c\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u673a\u5668\u4eba\u6570\u636e\u9884\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u4fdd\u6301\u5f3a\u5927\u7684\u6027\u80fd\u3002"}}
{"id": "2511.04679", "categories": ["cs.RO", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.04679", "abs": "https://arxiv.org/abs/2511.04679", "authors": ["Qingzhou Lu", "Yao Feng", "Baiyu Shi", "Michael Piseno", "Zhenan Bao", "C. Karen Liu"], "title": "GentleHumanoid: Learning Upper-body Compliance for Contact-rich Human and Object Interaction", "comment": "Home page: https://gentle-humanoid.axell.top", "summary": "Humanoid robots are expected to operate in human-centered environments where\nsafe and natural physical interaction is essential. However, most recent\nreinforcement learning (RL) policies emphasize rigid tracking and suppress\nexternal forces. Existing impedance-augmented approaches are typically\nrestricted to base or end-effector control and focus on resisting extreme\nforces rather than enabling compliance. We introduce GentleHumanoid, a\nframework that integrates impedance control into a whole-body motion tracking\npolicy to achieve upper-body compliance. At its core is a unified spring-based\nformulation that models both resistive contacts (restoring forces when pressing\nagainst surfaces) and guiding contacts (pushes or pulls sampled from human\nmotion data). This formulation ensures kinematically consistent forces across\nthe shoulder, elbow, and wrist, while exposing the policy to diverse\ninteraction scenarios. Safety is further supported through task-adjustable\nforce thresholds. We evaluate our approach in both simulation and on the\nUnitree G1 humanoid across tasks requiring different levels of compliance,\nincluding gentle hugging, sit-to-stand assistance, and safe object\nmanipulation. Compared to baselines, our policy consistently reduces peak\ncontact forces while maintaining task success, resulting in smoother and more\nnatural interactions. These results highlight a step toward humanoid robots\nthat can safely and effectively collaborate with humans and handle objects in\nreal-world environments.", "AI": {"tldr": "GentleHumanoid\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u963b\u6297\u63a7\u5236\u4e0e\u8fd0\u52a8\u8ffd\u8e2a\uff0c\u5b9e\u73b0\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u4e0e\u4eba\u7c7b\u4e92\u52a8\u65f6\u7684\u987a\u5e94\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u9488\u5bf9\u76ee\u524d\u5f3a\u5316\u5b66\u4e60\u653f\u7b56\u5728\u4e0e\u5916\u90e8\u529b\u91cf\u4e92\u52a8\u65f6\u8fc7\u4e8e\u521a\u6027\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u6765\u5b9e\u73b0\u66f4\u81ea\u7136\u548c\u5b89\u5168\u7684\u4e92\u52a8\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u57fa\u4e8e\u5f39\u7c27\u7684\u63a7\u5236\u673a\u5236\uff0c\u96c6\u6210\u963b\u6297\u63a7\u5236\u4e0e\u5168\u8eab\u8fd0\u52a8\u8ddf\u8e2a\u7b56\u7565\uff0c\u7740\u91cd\u4e8e\u4e0a\u534a\u8eab\u7684\u987a\u5e94\u6027\u3002", "result": "GentleHumanoid\u5728\u6a21\u62df\u548cUnitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u5728\u6e29\u548c\u62e5\u62b1\u3001\u8d77\u5750\u8f85\u52a9\u548c\u5b89\u5168\u7269\u4f53\u64cd\u4f5c\u7b49\u4efb\u52a1\u4e2d\uff0c\u5cf0\u503c\u63a5\u89e6\u529b\u663e\u8457\u964d\u4f4e\uff0c\u540c\u65f6\u4efb\u52a1\u6210\u529f\u7387\u7ef4\u6301\u5728\u9ad8\u6c34\u5e73\u3002", "conclusion": "\u8be5\u7814\u7a76\u7684\u7ed3\u679c\u8868\u660e\uff0cGentleHumanoid\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u63d0\u9ad8\u4eba\u5f62\u673a\u5668\u4eba\u5728\u4e0e\u4eba\u7c7b\u4e92\u52a8\u65f6\u7684\u5b89\u5168\u6027\u548c\u81ea\u7136\u6027\uff0c\u4ece\u800c\u63a8\u52a8\u4eba\u5f62\u673a\u5668\u4eba\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2511.04664", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04664", "abs": "https://arxiv.org/abs/2511.04664", "authors": ["Phat Nguyen", "Erfan Aasi", "Shiva Sreeram", "Guy Rosman", "Andrew Silva", "Sertac Karaman", "Daniela Rus"], "title": "SAFe-Copilot: Unified Shared Autonomy Framework", "comment": null, "summary": "Autonomous driving systems remain brittle in rare, ambiguous, and\nout-of-distribution scenarios, where human driver succeed through contextual\nreasoning. Shared autonomy has emerged as a promising approach to mitigate such\nfailures by incorporating human input when autonomy is uncertain. However, most\nexisting methods restrict arbitration to low-level trajectories, which\nrepresent only geometric paths and therefore fail to preserve the underlying\ndriving intent. We propose a unified shared autonomy framework that integrates\nhuman input and autonomous planners at a higher level of abstraction. Our\nmethod leverages Vision Language Models (VLMs) to infer driver intent from\nmulti-modal cues -- such as driver actions and environmental context -- and to\nsynthesize coherent strategies that mediate between human and autonomous\ncontrol. We first study the framework in a mock-human setting, where it\nachieves perfect recall alongside high accuracy and precision. A human-subject\nsurvey further shows strong alignment, with participants agreeing with\narbitration outcomes in 92% of cases. Finally, evaluation on the Bench2Drive\nbenchmark demonstrates a substantial reduction in collision rate and\nimprovement in overall performance compared to pure autonomy. Arbitration at\nthe level of semantic, language-based representations emerges as a design\nprinciple for shared autonomy, enabling systems to exercise common-sense\nreasoning and maintain continuity with human intent.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u7ea7\u522b\u7684\u5171\u4eab\u81ea\u4e3b\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u65ad\u9a7e\u9a76\u610f\u56fe\uff0c\u663e\u8457\u63d0\u9ad8\u81ea\u4e3b\u9a7e\u9a76\u7cfb\u7edf\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u9488\u5bf9\u73b0\u6709\u65b9\u6cd5\u53ea\u9650\u4e8e\u4f4e\u5c42\u6b21\u8f68\u8ff9\uff0c\u65e0\u6cd5\u6355\u6349\u9a7e\u9a76\u610f\u56fe\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u73b0\u6709\u5171\u4eab\u81ea\u4e3b\u65b9\u6cd5\u5728\u590d\u6742\u4ee5\u53ca\u4e0d\u786e\u5b9a\u60c5\u51b5\u4e0b\u7684\u5c40\u9650\u6027\u3002", "method": "\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4ece\u591a\u6a21\u6001\u7ebf\u7d22\u4e2d\u63a8\u65ad\u9a7e\u9a76\u610f\u56fe\uff0c\u5e76\u7efc\u5408\u4eba\u7c7b\u548c\u81ea\u4e3b\u63a7\u5236\u7684\u7b56\u7565\u3002", "result": "\u5728Mock-Human\u8bbe\u7f6e\u4e0b\u53d6\u5f97\u5b8c\u7f8e\u7684\u56de\u5fc6\u7387\u548c\u9ad8\u51c6\u786e\u7387\uff1b\u572897%\u7684\u60c5\u51b5\u4e0b\uff0c\u53c2\u4e0e\u8005\u4e0e\u88c1\u51b3\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5e76\u5728Bench2Drive\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u964d\u4f4e\u7684\u78b0\u649e\u7387\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u5171\u4eab\u81ea\u4e3b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u8bed\u4e49\u548c\u8bed\u8a00\u57fa\u5143\u7684\u8868\u793a\u5b9e\u73b0\u4eba\u7c7b\u8f93\u5165\u548c\u81ea\u4e3b\u89c4\u5212\u7684\u6574\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002"}}
{"id": "2511.04665", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04665", "abs": "https://arxiv.org/abs/2511.04665", "authors": ["Kaifeng Zhang", "Shuo Sha", "Hanxiao Jiang", "Matthew Loper", "Hyunjong Song", "Guangyan Cai", "Zhuo Xu", "Xiaochen Hu", "Changxi Zheng", "Yunzhu Li"], "title": "Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation of Soft-Body Interactions", "comment": "Website: https://real2sim-eval.github.io/", "summary": "Robotic manipulation policies are advancing rapidly, but their direct\nevaluation in the real world remains costly, time-consuming, and difficult to\nreproduce, particularly for tasks involving deformable objects. Simulation\nprovides a scalable and systematic alternative, yet existing simulators often\nfail to capture the coupled visual and physical complexity of soft-body\ninteractions. We present a real-to-sim policy evaluation framework that\nconstructs soft-body digital twins from real-world videos and renders robots,\nobjects, and environments with photorealistic fidelity using 3D Gaussian\nSplatting. We validate our approach on representative deformable manipulation\ntasks, including plush toy packing, rope routing, and T-block pushing,\ndemonstrating that simulated rollouts correlate strongly with real-world\nexecution performance and reveal key behavioral patterns of learned policies.\nOur results suggest that combining physics-informed reconstruction with\nhigh-quality rendering enables reproducible, scalable, and accurate evaluation\nof robotic manipulation policies. Website: https://real2sim-eval.github.io/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u73b0\u5b9e\u89c6\u9891\u751f\u6210\u8f6f\u4f53\u6570\u5b57\u53cc\u80de\u80ce\u7684\u6846\u67b6\uff0c\u80fd\u591f\u4ee5\u9ad8\u4fdd\u771f\u5ea6\u8bc4\u4f30\u673a\u5668\u4eba\u64cd\u63a7\u7b56\u7565\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u53d8\u5f62\u64cd\u63a7\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u673a\u5668\u4eba\u64cd\u63a7\u7b56\u7565\u7684\u5feb\u901f\u53d1\u5c55\u4f7f\u5f97\u76f4\u63a5\u5728\u73b0\u5b9e\u4e2d\u8bc4\u4f30\u53d8\u5f97\u6210\u672c\u9ad8\u6602\u4e14\u96be\u4ee5\u91cd\u73b0\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u53d8\u5f62\u7269\u4f53\u7684\u4efb\u52a1\u4e2d\u3002", "method": "\u6784\u5efa\u4ece\u73b0\u5b9e\u4e16\u754c\u89c6\u9891\u751f\u6210\u7684\u8f6f\u4f53\u6570\u5b57\u53cc\u80de\u80ce\uff0c\u5e76\u4f7f\u75283D\u9ad8\u65af\u70b9\u4e91\u6e32\u67d3\u51fa\u5177\u6709\u7167\u7247\u771f\u5b9e\u611f\u7684\u673a\u5668\u4eba\u3001\u7269\u4f53\u548c\u73af\u5883\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u4e0e\u73b0\u5b9e\u6267\u884c\u6027\u80fd\u5f3a\u76f8\u5173\uff0c\u5e76\u63ed\u793a\u4e86\u5b66\u4e60\u7b56\u7565\u7684\u5173\u952e\u884c\u4e3a\u6a21\u5f0f\u3002", "conclusion": "\u901a\u8fc7\u5c06\u7269\u7406\u4fe1\u606f\u91cd\u5efa\u4e0e\u9ad8\u8d28\u91cf\u6e32\u67d3\u76f8\u7ed3\u5408\uff0c\u80fd\u591f\u5b9e\u73b0\u673a\u5668\u4eba\u64cd\u63a7\u7b56\u7565\u7684\u53ef\u91cd\u590d\u3001\u53ef\u6269\u5c55\u548c\u51c6\u786e\u7684\u8bc4\u4f30\u3002"}}
{"id": "2511.04671", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04671", "abs": "https://arxiv.org/abs/2511.04671", "authors": ["Maximus A. Pace", "Prithwish Dan", "Chuanruo Ning", "Atiksh Bhardwaj", "Audrey Du", "Edward W. Duan", "Wei-Chiu Ma", "Kushal Kedia"], "title": "X-Diffusion: Training Diffusion Policies on Cross-Embodiment Human Demonstrations", "comment": null, "summary": "Human videos can be recorded quickly and at scale, making them an appealing\nsource of training data for robot learning. However, humans and robots differ\nfundamentally in embodiment, resulting in mismatched action execution. Direct\nkinematic retargeting of human hand motion can therefore produce actions that\nare physically infeasible for robots. Despite these low-level differences,\nhuman demonstrations provide valuable motion cues about how to manipulate and\ninteract with objects. Our key idea is to exploit the forward diffusion\nprocess: as noise is added to actions, low-level execution differences fade\nwhile high-level task guidance is preserved. We present X-Diffusion, a\nprincipled framework for training diffusion policies that maximally leverages\nhuman data without learning dynamically infeasible motions. X-Diffusion first\ntrains a classifier to predict whether a noisy action is executed by a human or\nrobot. Then, a human action is incorporated into policy training only after\nadding sufficient noise such that the classifier cannot discern its embodiment.\nActions consistent with robot execution supervise fine-grained denoising at low\nnoise levels, while mismatched human actions provide only coarse guidance at\nhigher noise levels. Our experiments show that naive co-training under\nexecution mismatches degrades policy performance, while X-Diffusion\nconsistently improves it. Across five manipulation tasks, X-Diffusion achieves\na 16% higher average success rate than the best baseline. The project website\nis available at https://portal-cornell.github.io/X-Diffusion/.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u7684X-Diffusion\u6846\u67b6\u6709\u6548\u5229\u7528\u4eba\u7c7b\u89c6\u9891\u6570\u636e\uff0c\u901a\u8fc7\u6269\u6563\u8fc7\u7a0b\u5904\u7406\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u64cd\u4f5c\u6210\u529f\u7387\u7684\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5229\u7528\u4eba\u7c7b\u5f55\u50cf\u5feb\u901f\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u540c\u65f6\u514b\u670d\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u52a8\u4f5c\u6267\u884c\u5dee\u5f02\uff0c\u4ee5\u63d0\u9ad8\u673a\u5668\u4eba\u5b66\u4e60\u7684\u6548\u679c\u3002", "method": "\u4f7f\u7528\u6269\u6563\u8fc7\u7a0b\u4f5c\u4e3a\u6846\u67b6\uff0c\u901a\u8fc7\u52a0\u566a\u58f0\u5904\u7406\u6765\u4ece\u4eba\u7c7b\u6f14\u793a\u4e2d\u62bd\u53d6\u53ef\u884c\u7684\u673a\u5668\u4eba\u52a8\u4f5c\u3002", "result": "X-Diffusion\u5728\u4e94\u4e2a\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6bd4\u6700\u4f73\u57fa\u7ebf\u9ad8\u51fa16%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u5c55\u793a\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "conclusion": "X-Diffusion\u5728\u6267\u884c\u673a\u5668\u4eba\u7684\u64cd\u4f5c\u65f6\u6709\u6548\u5229\u7528\u4e86\u4eba\u7c7b\u7684\u89c6\u9891\u6570\u636e\uff0c\u514b\u670d\u4e86\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u6267\u884c\u5dee\u5f02\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u529f\u7387\u63d0\u5347\u3002"}}
