{"id": "2512.14952", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.14952", "abs": "https://arxiv.org/abs/2512.14952", "authors": ["Iddo Yehoshua Wald", "Amber Maimon", "Shiyao Zhang", "Dennis K\u00fcster", "Robert Porzel", "Tanja Schultz", "Rainer Malaka"], "title": "Breathe with Me: Synchronizing Biosignals for User Embodiment in Robots", "comment": "Accepted to appear in the ACM/IEEE International Conference on Human-Robot Interaction (HRI '26), Edinburgh, United Kingdom. Iddo Yehoshua Wald and Amber Maimon contributed equally", "summary": "Embodiment of users within robotic systems has been explored in human-robot interaction, most often in telepresence and teleoperation. In these applications, synchronized visuomotor feedback can evoke a sense of body ownership and agency, contributing to the experience of embodiment. We extend this work by employing embreathment, the representation of the user's own breath in real time, as a means for enhancing user embodiment experience in robots. In a within-subjects experiment, participants controlled a robotic arm, while its movements were either synchronized or non-synchronized with their own breath. Synchrony was shown to significantly increase body ownership, and was preferred by most participants. We propose the representation of physiological signals as a novel interoceptive pathway for human-robot interaction, and discuss implications for telepresence, prosthetics, collaboration with robots, and shared autonomy.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u901a\u8fc7\u5b9e\u65f6\u547c\u5438\u53cd\u9988\u589e\u5f3a\u7528\u6237\u5728\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u8eab\u4f53\u8ba4\u540c\u611f\uff0c\u53d1\u73b0\u4e0e\u81ea\u8eab\u547c\u5438\u540c\u6b65\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u663e\u8457\u589e\u52a0\u4e86\u8eab\u4f53\u6240\u6709\u6743\u611f\u3002", "motivation": "\u8fd1\u5e74\u6765\u5173\u4e8e\u7528\u6237\u5728\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u4f53\u73b0\u611f\u7814\u7a76\u9010\u6e10\u589e\u591a\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u5982\u4f55\u901a\u8fc7\u547c\u5438\u53cd\u9988\u6765\u589e\u5f3a\u8fd9\u4e00\u4f53\u9a8c\u3002", "method": "\u8fdb\u884c\u4e86\u4e00\u9879\u88ab\u8bd5\u95f4\u5b9e\u9a8c\uff0c\u53c2\u4e0e\u8005\u5728\u63a7\u5236\u673a\u5668\u4eba\u624b\u81c2\u65f6\uff0c\u624b\u81c2\u7684\u8fd0\u52a8\u8981\u4e48\u4e0e\u4ed6\u4eec\u7684\u547c\u5438\u540c\u6b65\uff0c\u8981\u4e48\u4e0d\u540c\u6b65\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0c\u547c\u5438\u540c\u6b65\u663e\u8457\u589e\u52a0\u4e86\u53c2\u4e0e\u8005\u7684\u8eab\u4f53\u6240\u6709\u6743\u611f\uff0c\u5e76\u4e14\u5927\u591a\u6570\u53c2\u4e0e\u8005\u66f4\u559c\u6b22\u540c\u6b65\u7684\u4f53\u9a8c\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u751f\u7406\u4fe1\u53f7\u7684\u8868\u73b0\u4f5c\u4e3a\u4eba\u673a\u4ea4\u4e92\u7684\u65b0\u5185\u611f\u77e5\u9014\u5f84\uff0c\u63a2\u8ba8\u4e86\u5176\u5728\u8fdc\u7a0b\u5b58\u5728\u3001\u5047\u80a2\u3001\u4e0e\u673a\u5668\u4eba\u534f\u4f5c\u53ca\u5171\u4eab\u81ea\u4e3b\u6027\u65b9\u9762\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2512.15020", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.15020", "abs": "https://arxiv.org/abs/2512.15020", "authors": ["Wenlong Xia", "Jinhao Zhang", "Ce Zhang", "Yaojia Wang", "Youmin Gong", "Jie Mei"], "title": "ISS Policy : Scalable Diffusion Policy with Implicit Scene Supervision", "comment": null, "summary": "Vision-based imitation learning has enabled impressive robotic manipulation skills, but its reliance on object appearance while ignoring the underlying 3D scene structure leads to low training efficiency and poor generalization. To address these challenges, we introduce \\emph{Implicit Scene Supervision (ISS) Policy}, a 3D visuomotor DiT-based diffusion policy that predicts sequences of continuous actions from point cloud observations. We extend DiT with a novel implicit scene supervision module that encourages the model to produce outputs consistent with the scene's geometric evolution, thereby improving the performance and robustness of the policy. Notably, ISS Policy achieves state-of-the-art performance on both single-arm manipulation tasks (MetaWorld) and dexterous hand manipulation (Adroit). In real-world experiments, it also demonstrates strong generalization and robustness. Additional ablation studies show that our method scales effectively with both data and parameters. Code and videos will be released.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u9690\u5f0f\u573a\u666f\u76d1\u7763\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u57fa\u4e8e\u89c6\u89c9\u7684\u6a21\u4eff\u5b66\u4e60\u7684\u8bad\u7ec3\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u89c6\u89c9\u7684\u6a21\u4eff\u5b66\u4e60\u4e2d\u5bf9\u7269\u4f53\u5916\u89c2\u7684\u4f9d\u8d56\uff0c\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5f15\u5165\u9690\u5f0f\u573a\u666f\u76d1\u7763\uff08ISS\uff09\u7b56\u7565\uff0c\u57fa\u4e8e3D\u89c6\u89c9\u8fd0\u52a8\u7684DiT\u6269\u5c55\uff0c\u5229\u7528\u70b9\u4e91\u89c2\u5bdf\u9884\u6d4b\u8fde\u7eed\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "\u5728\u5355\u81c2\u64cd\u63a7\u4efb\u52a1\uff08MetaWorld\uff09\u548c\u7075\u5de7\u624b\u64cd\u63a7\uff08Adroit\uff09\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u5c55\u73b0\u5f3a\u5927\u7684\u6cdb\u5316\u4e0e\u9c81\u68d2\u6027\u3002", "conclusion": "\u65b9\u6cd5\u5728\u6570\u636e\u548c\u53c2\u6570\u4e0a\u5177\u5907\u826f\u597d\u7684\u6269\u5c55\u6027\uff0c\u4ee3\u7801\u548c\u89c6\u9891\u5c06\u4f1a\u53d1\u5e03\u3002"}}
{"id": "2512.15047", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.15047", "abs": "https://arxiv.org/abs/2512.15047", "authors": ["Yunheng Wang", "Yixiao Feng", "Yuetong Fang", "Shuning Zhang", "Tan Jing", "Jian Li", "Xiangrui Jiang", "Renjing Xu"], "title": "HERO: Hierarchical Traversable 3D Scene Graphs for Embodied Navigation Among Movable Obstacles", "comment": null, "summary": "3D Scene Graphs (3DSGs) constitute a powerful representation of the physical world, distinguished by their abilities to explicitly model the complex spatial, semantic, and functional relationships between entities, rendering a foundational understanding that enables agents to interact intelligently with their environment and execute versatile behaviors. Embodied navigation, as a crucial component of such capabilities, leverages the compact and expressive nature of 3DSGs to enable long-horizon reasoning and planning in complex, large-scale environments. However, prior works rely on a static-world assumption, defining traversable space solely based on static spatial layouts and thereby treating interactable obstacles as non-traversable. This fundamental limitation severely undermines their effectiveness in real-world scenarios, leading to limited reachability, low efficiency, and inferior extensibility. To address these issues, we propose HERO, a novel framework for constructing Hierarchical Traversable 3DSGs, that redefines traversability by modeling operable obstacles as pathways, capturing their physical interactivity, functional semantics, and the scene's relational hierarchy. The results show that, relative to its baseline, HERO reduces PL by 35.1% in partially obstructed environments and increases SR by 79.4% in fully obstructed ones, demonstrating substantially higher efficiency and reachability.", "AI": {"tldr": "HERO\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5c42\u53ef\u884c3D\u573a\u666f\u56fe\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u53ef\u64cd\u4f5c\u969c\u788d\u5efa\u6a21\u4e3a\u8def\u5f84\u6765\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5bfc\u822a\u6548\u7387\u548c\u53ef\u8fbe\u6027\u3002", "motivation": "\u4f20\u7edf\u76843D\u573a\u666f\u56fe\u65b9\u6cd5\u57fa\u4e8e\u9759\u6001\u4e16\u754c\u5047\u8bbe\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u4ea4\u4e92\u969c\u788d\uff0c\u9650\u5236\u4e86\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\u3002", "method": "\u63d0\u51faHERO\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u5c42\u6b21\u5316\u7684\u53ef\u884c3D\u573a\u666f\u56fe\uff0c\u5c06\u53ef\u64cd\u4f5c\u969c\u788d\u89c6\u4e3a\u53ef\u901a\u884c\u7684\u8def\u5f84\uff0c\u91cd\u65b0\u5b9a\u4e49\u53ef\u901a\u884c\u6027\u3002", "result": "HERO\u5728\u90e8\u5206\u969c\u788d\u73af\u5883\u4e2d\u51cf\u5c11\u4e8635.1%\u7684PL\uff0c\u5728\u5b8c\u5168\u969c\u788d\u73af\u5883\u4e2d\u63d0\u9ad8\u4e8679.4%\u7684SR\uff0c\u8868\u660e\u5176\u5728\u6548\u7387\u548c\u53ef\u8fbe\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "HERO\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u5f0f\u6765\u7406\u89e3\u548c\u5b9e\u73b0\u667a\u80fd\u5bfc\u822a\uff0c\u9002\u5e94\u590d\u6742\u548c\u52a8\u6001\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u9700\u6c42\u3002"}}
{"id": "2512.15080", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.15080", "abs": "https://arxiv.org/abs/2512.15080", "authors": ["Gaurav Bansal"], "title": "NAP3D: NeRF Assisted 3D-3D Pose Alignment for Autonomous Vehicles", "comment": "10 pages, 5 figures, 2 tables", "summary": "Accurate localization is essential for autonomous vehicles, yet sensor noise and drift over time can lead to significant pose estimation errors, particularly in long-horizon environments. A common strategy for correcting accumulated error is visual loop closure in SLAM, which adjusts the pose graph when the agent revisits previously mapped locations. These techniques typically rely on identifying visual mappings between the current view and previously observed scenes and often require fusing data from multiple sensors.\n  In contrast, this work introduces NeRF-Assisted 3D-3D Pose Alignment (NAP3D), a complementary approach that leverages 3D-3D correspondences between the agent's current depth image and a pre-trained Neural Radiance Field (NeRF). By directly aligning 3D points from the observed scene with synthesized points from the NeRF, NAP3D refines the estimated pose even from novel viewpoints, without relying on revisiting previously observed locations.\n  This robust 3D-3D formulation provides advantages over conventional 2D-3D localization methods while remaining comparable in accuracy and applicability. Experiments demonstrate that NAP3D achieves camera pose correction within 5 cm on a custom dataset, robustly outperforming a 2D-3D Perspective-N-Point baseline. On TUM RGB-D, NAP3D consistently improves 3D alignment RMSE by approximately 6 cm compared to this baseline given varying noise, despite PnP achieving lower raw rotation and translation parameter error in some regimes, highlighting NAP3D's improved geometric consistency in 3D space. By providing a lightweight, dataset-agnostic tool, NAP3D complements existing SLAM and localization pipelines when traditional loop closure is unavailable.", "AI": {"tldr": "NAP3D \u662f\u4e00\u79cd\u65b0\u7684 3D-3D \u4f4d\u59ff\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5229\u7528\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u6765\u4f18\u5316\u76f8\u673a\u4f4d\u59ff\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u4e14\u4e0d\u4f9d\u8d56\u4e8e\u56de\u8bbf\u5df2\u89c2\u5bdf\u5730\u70b9\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u7684\u9700\u6c42\u589e\u52a0\uff0c\u51c6\u786e\u5b9a\u4f4d\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\uff0c\u800c\u73b0\u6709\u7684\u5b9a\u4f4d\u65b9\u6cd5\u5728\u9762\u5bf9\u4f20\u611f\u5668\u566a\u58f0\u548c\u957f\u65f6\u95f4\u6f02\u79fb\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u5f15\u5165 NeRF-Assisted 3D-3D Pose Alignment (NAP3D)\uff0c\u901a\u8fc7\u76f4\u63a5\u5bf9\u9f50\u89c2\u5bdf\u573a\u666f\u7684 3D \u70b9\u4e0e NeRF \u5408\u6210\u7684\u70b9\uff0c\u6765\u4f18\u5316\u76f8\u673a\u4f4d\u59ff\u3002", "result": "\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684 3D-3D \u4f4d\u59ff\u5bf9\u9f50\u65b9\u6cd5 NAP3D\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528\u6df1\u5ea6\u56fe\u4e0e\u9884\u8bad\u7ec3\u7684\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u4e4b\u95f4\u7684 3D-3D \u5bf9\u5e94\u5173\u7cfb\u6765\u4f18\u5316\u76f8\u673a\u4f4d\u59ff\u3002", "conclusion": "NAP3D \u5728\u5404\u79cd\u566a\u58f0\u6761\u4ef6\u4e0b\u63d0\u5347\u4e86 3D \u5bf9\u9f50\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u73b0\u6709SLAM\u548c\u5b9a\u4f4d\u7ba1\u9053\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u8865\u5145\u5de5\u5177\u3002"}}
{"id": "2512.14965", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.14965", "abs": "https://arxiv.org/abs/2512.14965", "authors": ["Renkai Ma", "Dominique Geissler", "Stefan Feuerriegel", "Tobias Lauinger", "Damon McCoy", "Pamela Wisniewski"], "title": "Analyzing Social Media Claims regarding Youth Online Safety Features to Identify Problem Areas and Communication Gaps", "comment": "Accepted to ACM CSCW 2026. 31 pages", "summary": "Social media platforms have faced increasing scrutiny over whether and how they protect youth online. While online risks to children have been well-documented by prior research, how social media platforms communicate about these risks and their efforts to improve youth safety have not been holistically examined. To fill this gap, we analyzed N=352 press releases and safety-related blogs published between 2019 and 2024 by four platforms popular among youth: YouTube, TikTok, Meta (Facebook and Instagram), and Snapchat. Leveraging both inductive and deductive qualitative approaches, we developed a comprehensive framework of seven problem areas where risks arise, and a taxonomy of safety features that social media platforms claim address these risks. Our analysis revealed uneven emphasis across problem areas, with most communications focused on Content Exposure and Interpersonal Communication, whereas less emphasis was placed on Content Creation, Data Access, and Platform Access. Additionally, we identified three problematic communication practices related to their described safety features, including discrepancies between feature implementation and availability, unclear or inconsistent explanations of safety feature operation, and a lack of evidence regarding the effectiveness of safety features in mitigating risks once implemented. Based on these findings, we discuss the communication gaps between risks and the described safety features, as well as the tensions in achieving transparency in platform communication. Our analysis of platform communication informs guidelines for responsibly communicating about youth safety features.", "AI": {"tldr": "\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u5728\u9752\u5c11\u5e74\u5728\u7ebf\u5b89\u5168\u65b9\u9762\u9762\u4e34\u5ba1\u67e5\uff0c\u5c3d\u7ba1\u5df2\u6709\u7814\u7a76\u63ed\u793a\u4e86\u5bf9\u513f\u7ae5\u7684\u5728\u7ebf\u98ce\u9669\uff0c\u4f46\u5e73\u53f0\u5982\u4f55\u6c9f\u901a\u8fd9\u4e9b\u98ce\u9669\u53ca\u5176\u5b89\u5168\u63aa\u65bd\u5c1a\u672a\u88ab\u5168\u9762\u5ba1\u89c6\u3002\u901a\u8fc7\u5206\u6790\u56db\u4e2a\u5e73\u53f0\u7684352\u7bc7\u65b0\u95fb\u7a3f\u548c\u5b89\u5168\u76f8\u5173\u535a\u5ba2\uff0c\u8bc6\u522b\u51fa\u4e03\u4e2a\u98ce\u9669\u9886\u57df\u548c\u5b89\u5168\u7279\u5f81\uff0c\u540c\u65f6\u53d1\u73b0\u5e73\u53f0\u5728\u6c9f\u901a\u4e2d\u5b58\u5728\u4e0d\u5e73\u8861\u548c\u95ee\u9898\u6027\u505a\u6cd5\u3002", "motivation": "\u7814\u7a76\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u5728\u63d0\u9ad8\u9752\u5c11\u5e74\u5b89\u5168\u65b9\u9762\u7684\u6c9f\u901a\u65b9\u5f0f\u548c\u6548\u679c\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u5206\u67902019\u81f32024\u5e74\u95f4\uff0c\u5728YouTube\u3001TikTok\u3001Meta\u548cSnapchat\u7b49\u56db\u4e2a\u5e73\u53f0\u53d1\u5e03\u7684352\u7bc7\u65b0\u95fb\u7a3f\u548c\u5b89\u5168\u76f8\u5173\u535a\u5ba2\uff0c\u91c7\u7528\u5f52\u7eb3\u548c\u6f14\u7ece\u7684\u5b9a\u6027\u5206\u6790\u65b9\u6cd5\u3002", "result": "\u8bc6\u522b\u51fa\u4e03\u4e2a\u98ce\u9669\u9886\u57df\u4ee5\u53ca\u5e73\u53f0\u58f0\u79f0\u7684\u5b89\u5168\u7279\u5f81\uff0c\u5728\u6c9f\u901a\u4e0a\u53d1\u73b0\u5bf9\u5185\u5bb9\u66dd\u5149\u548c\u4eba\u9645\u4ea4\u6d41\u7684\u4fa7\u91cd\uff0c\u800c\u5bf9\u5185\u5bb9\u521b\u4f5c\u3001\u6570\u636e\u8bbf\u95ee\u548c\u5e73\u53f0\u8bbf\u95ee\u7684\u5173\u6ce8\u8f83\u5c11\uff1b\u540c\u65f6\u63ed\u793a\u4e09\u79cd\u6c9f\u901a\u95ee\u9898\uff0c\u5305\u62ec\u5b9e\u65bd\u4e0e\u53ef\u7528\u6027\u4e4b\u95f4\u7684\u5dee\u5f02\u3001\u5b89\u5168\u7279\u5f81\u64cd\u4f5c\u8bf4\u660e\u4e0d\u6e05\u6670\u53ca\u7f3a\u4e4f\u6709\u6548\u6027\u7684\u8bc1\u636e\u3002", "conclusion": "\u6b64\u5206\u6790\u63d0\u4f9b\u4e86\u5e73\u53f0\u6c9f\u901a\u4e2d\u7684\u98ce\u9669\u4e0e\u5b89\u5168\u7279\u5f81\u4e4b\u95f4\u7684\u6c9f\u901a\u5dee\u8ddd\u548c\u900f\u660e\u5ea6\u95ee\u9898\u7684\u89c1\u89e3\uff0c\u4e3a\u8d1f\u8d23\u4efb\u5730\u6c9f\u901a\u9752\u5c11\u5e74\u5b89\u5168\u7279\u5f81\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2512.15111", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.15111", "abs": "https://arxiv.org/abs/2512.15111", "authors": ["Dongmyeong Lee", "Jesse Quattrociocchi", "Christian Ellis", "Rwik Rana", "Amanda Adkins", "Adam Uccello", "Garrett Warnell", "Joydeep Biswas"], "title": "BEV-Patch-PF: Particle Filtering with BEV-Aerial Feature Matching for Off-Road Geo-Localization", "comment": null, "summary": "We propose BEV-Patch-PF, a GPS-free sequential geo-localization system that integrates a particle filter with learned bird's-eye-view (BEV) and aerial feature maps. From onboard RGB and depth images, we construct a BEV feature map. For each 3-DoF particle pose hypothesis, we crop the corresponding patch from an aerial feature map computed from a local aerial image queried around the approximate location. BEV-Patch-PF computes a per-particle log-likelihood by matching the BEV feature to the aerial patch feature. On two real-world off-road datasets, our method achieves 7.5x lower absolute trajectory error (ATE) on seen routes and 7.0x lower ATE on unseen routes than a retrieval-based baseline, while maintaining accuracy under dense canopy and shadow. The system runs in real time at 10 Hz on an NVIDIA Tesla T4, enabling practical robot deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0GPS\u7684\u987a\u5e8f\u5730\u7406\u5b9a\u4f4d\u7cfb\u7edfBEV-Patch-PF\uff0c\u7ed3\u5408\u4e86\u7c92\u5b50\u6ee4\u6ce2\u5668\u4e0e\u5b66\u4e60\u7684\u9e1f\u77b0\u89c6\u56fe\u548c\u822a\u7a7a\u7279\u5f81\u56fe\uff0c\u6027\u80fd\u663e\u8457\u63d0\u9ad8\u3002", "motivation": "\u5728\u65e0GPS\u73af\u5883\u4e0b\u7684\u7cbe\u786e\u5730\u7406\u5b9a\u4f4d\u9700\u6c42\uff0c\u7279\u522b\u662f\u5bf9\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u652f\u6301\u3002", "method": "\u901a\u8fc7\u57fa\u4e8eRGB\u548c\u6df1\u5ea6\u56fe\u50cf\u6784\u5efa\u9e1f\u77b0\u7279\u5f81\u56fe\uff0c\u7ed3\u5408\u7c92\u5b50\u6ee4\u6ce2\u7b97\u6cd5\uff0c\u66f4\u597d\u5730\u5339\u914d\u7279\u5f81\u5e76\u8ba1\u7b97\u6bcf\u4e2a\u7c92\u5b50\u7684\u5bf9\u6570\u4f3c\u7136\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u8d8a\u91ce\u6570\u636e\u96c6\u4e0a\uff0c\u65b9\u6cd5\u5728\u5df2\u89c1\u548c\u672a\u89c1\u8def\u5f84\u4e0a\u5206\u522b\u5b9e\u73b07.5\u500d\u548c7.0\u500d\u8f83\u4f4e\u7684\u7edd\u5bf9\u8f68\u8ff9\u8bef\u5dee\uff0c\u540c\u65f6\u5728\u5bc6\u96c6\u6811\u51a0\u548c\u9634\u5f71\u4e0b\u4fdd\u6301\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5728NVIDIA Tesla T4\u4e0a\u4ee510 Hz\u7684\u5b9e\u65f6\u901f\u5ea6\u8fd0\u884c\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u673a\u5668\u4eba\u90e8\u7f72\u3002"}}
{"id": "2512.14977", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.14977", "abs": "https://arxiv.org/abs/2512.14977", "authors": ["Stuart Winby", "Wei Xu"], "title": "Human-Centered AI Maturity Model (HCAI-MM): An Organizational Design Perspective", "comment": null, "summary": "Human-centered artificial intelligence (HCAI) is an approach to AI design, development, and deployment that prioritizes human needs, values, and experiences, ensuring that technology enhances human capabilities, well-being, and workforce empowerment. While HCAI has gained prominence in academic discourse and organizational practice, its implementation remains constrained by the absence of methodological guidance and structured frameworks. In particular, HCAI and organizational design practices are often treated separately, despite their interdependence in shaping effective socio-technical systems. This chapter addresses this gap by introducing the Human-Centered AI Maturity Model (HCAI-MM), a structured framework that enables organizations to evaluate, monitor, and advance their capacity to design and implement HCAI solutions. The model specifies stages of maturity, metrics, tools, governance mechanisms, and best practices, supported by case studies, while also incorporating an organizational design methodology that operationalizes maturity progression. Encompassing dimensions such as human-AI collaboration, explainability, fairness, and user experience, the HCAI-MM provides a roadmap for organizations to move from novice to advanced levels of maturity, aligning AI technologies with human values and organizational design principles.", "AI": {"tldr": "\u4eba\u672c\u4eba\u5de5\u667a\u80fd\uff08HCAI\uff09\u4f18\u5148\u8003\u8651\u4eba\u7c7b\u9700\u6c42\u548c\u4ef7\u503c\uff0c\u4f46\u7f3a\u4e4f\u65b9\u6cd5\u8bba\u6307\u5bfc\u3002\u672c\u7ae0\u8282\u4ecb\u7ecd\u4e86\u4eba\u672c\u4eba\u5de5\u667a\u80fd\u6210\u719f\u5ea6\u6a21\u578b\uff08HCAI-MM\uff09\uff0c\u4e3a\u7ec4\u7ec7\u8bbe\u8ba1\u548c\u5b9e\u65bdHCAI\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u6846\u67b6\u3002", "motivation": "\u63a8\u52a8\u4eba\u672cAI\u7684\u8bbe\u8ba1\u548c\u5b9e\u65bd\uff0c\u5f3a\u8c03\u4eba\u7c7b\u9700\u6c42\u4e0e\u4ef7\u503c\u7684\u91cd\u8981\u6027\uff0c\u540c\u65f6\u586b\u8865HCAI\u4e0e\u7ec4\u7ec7\u8bbe\u8ba1\u4e4b\u95f4\u7684\u7f3a\u53e3\u3002", "method": "\u901a\u8fc7\u5efa\u7acb\u4eba\u672c\u4eba\u5de5\u667a\u80fd\u6210\u719f\u5ea6\u6a21\u578b\uff0c\u7ed3\u6784\u5316\u5730\u8bc4\u4f30\u7ec4\u7ec7\u7684\u4eba\u672cAI\u80fd\u529b\uff0c\u5e76\u63d0\u4f9b\u76f8\u5173\u7684\u9636\u6bb5\u3001\u6307\u6807\u3001\u5de5\u5177\u53ca\u6cbb\u7406\u673a\u5236\u3002", "result": "\u63d0\u51fa\u4e86\u4eba\u672c\u4eba\u5de5\u667a\u80fd\u6210\u719f\u5ea6\u6a21\u578b (HCAI-MM)\uff0c\u8be5\u6a21\u578b\u4e3a\u7ec4\u7ec7\u8bc4\u4f30\u548c\u4fc3\u8fdb\u8bbe\u8ba1\u548c\u5b9e\u65bdHCAI\u89e3\u51b3\u65b9\u6848\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u6846\u67b6\u3002", "conclusion": "HCAI-MM\u4f5c\u4e3a\u4e00\u4e2a\u652f\u6301\u7ec4\u7ec7\u4eba\u672cAI\u5b9e\u65bd\u7684\u5de5\u5177\uff0c\u5e2e\u52a9\u7ec4\u7ec7\u5236\u5b9a\u6700\u4f73\u5b9e\u8df5\u5e76\u63d0\u5347\u5176\u5728\u4eba\u672cAI\u65b9\u9762\u7684\u6210\u719f\u5ea6\uff0c\u4ee5\u66f4\u597d\u5730\u6ee1\u8db3\u4eba\u7c7b\u7684\u9700\u8981\u548c\u4ef7\u503c\u3002"}}
{"id": "2512.15195", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.15195", "abs": "https://arxiv.org/abs/2512.15195", "authors": ["J\u00f6rg Gamerdinger", "Sven Teufel", "Stephan Amann", "Lukas Marc Listl", "Oliver Bringmann"], "title": "EPSM: A Novel Metric to Evaluate the Safety of Environmental Perception in Autonomous Driving", "comment": "Submitted at IEEE IV 2026", "summary": "Extensive evaluation of perception systems is crucial for ensuring the safety of intelligent vehicles in complex driving scenarios. Conventional performance metrics such as precision, recall and the F1-score assess the overall detection accuracy, but they do not consider the safety-relevant aspects of perception. Consequently, perception systems that achieve high scores in these metrics may still cause misdetections that could lead to severe accidents. Therefore, it is important to evaluate not only the overall performance of perception systems, but also their safety. We therefore introduce a novel safety metric for jointly evaluating the most critical perception tasks, object and lane detection. Our proposed framework integrates a new, lightweight object safety metric that quantifies the potential risk associated with object detection errors, as well as an lane safety metric including the interdependence between both tasks that can occur in safety evaluation. The resulting combined safety score provides a unified, interpretable measure of perception safety performance. Using the DeepAccident dataset, we demonstrate that our approach identifies safety critical perception errors that conventional performance metrics fail to capture. Our findings emphasize the importance of safety-centric evaluation methods for perception systems in autonomous driving.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b89\u5168\u6307\u6807\u6846\u67b6\uff0c\u4ee5\u96c6\u6210\u548c\u8bc4\u4f30\u667a\u80fd\u8f66\u8f86\u611f\u77e5\u7cfb\u7edf\u7684\u5b89\u5168\u6027\uff0c\u5f3a\u8c03\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u786e\u4fdd\u667a\u80fd\u8f66\u8f86\u5728\u590d\u6742\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u5b89\u5168\u6027\uff0c\u8bc4\u4f30\u611f\u77e5\u7cfb\u7edf\u7684\u91cd\u8981\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u5b89\u5168\u6307\u6807\uff0c\u8054\u5408\u8bc4\u4f30\u5173\u952e\u7684\u611f\u77e5\u4efb\u52a1\u2014\u2014\u7269\u4f53\u548c\u8f66\u9053\u68c0\u6d4b\uff0c\u5e76\u96c6\u6210\u8f7b\u91cf\u7ea7\u7269\u4f53\u5b89\u5168\u6307\u6807\u548c\u8f66\u9053\u5b89\u5168\u6307\u6807\u3002", "result": "\u901a\u8fc7DeepAccident\u6570\u636e\u96c6\uff0c\u5c55\u793a\u8be5\u65b9\u6cd5\u80fd\u591f\u8bc6\u522b\u4f20\u7edf\u6027\u80fd\u6307\u6807\u65e0\u6cd5\u6355\u6349\u7684\u5b89\u5168\u5173\u952e\u611f\u77e5\u9519\u8bef\u3002", "conclusion": "\u5f3a\u8c03\u4ee5\u5b89\u5168\u4e3a\u4e2d\u5fc3\u7684\u8bc4\u4f30\u65b9\u6cd5\u5728\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2512.15117", "categories": ["cs.HC", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.15117", "abs": "https://arxiv.org/abs/2512.15117", "authors": ["Pilyoung Kim", "Yun Xie", "Sujin Yang"], "title": "I am here for you\": How relational conversational AI appeals to adolescents, especially those who are socially and emotionally vulnerable", "comment": null, "summary": "General-purpose conversational AI chatbots and AI companions increasingly provide young adolescents with emotionally supportive conversations, raising questions about how conversational style shapes anthropomorphism and emotional reliance. In a preregistered online experiment with 284 adolescent-parent dyads, youth aged 11-15 and their parents read two matched transcripts in which a chatbot responded to an everyday social problem using either a relational style (first-person, affiliative, commitment language) or a transparent style (explicit nonhumanness, informational tone). Adolescents more often preferred the relational than the transparent style, whereas parents were more likely to prefer transparent style than adolescents. Adolescents rated the relational chatbot as more human-like, likable, trustworthy and emotionally close, while perceiving both styles as similarly helpful. Adolescents who preferred relational style had lower family and peer relationship quality and higher stress and anxiety than those preferring transparent style or both chatbots. These findings identify conversational style as a key design lever for youth AI safety, showing that relational framing heightens anthropomorphism, trust and emotional closeness and can be especially appealing to socially and emotionally vulnerable adolescents, who may be at increased risk for emotional reliance on conversational AI.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5bf9\u8bdd\u98ce\u683c\u5bf9\u9752\u5c11\u5e74\u7279\u5b9a\u60c5\u611f\u652f\u6301\u7684\u5f71\u54cd\uff0c\u7ed3\u679c\u663e\u793a\u5173\u7cfb\u98ce\u683c\u66f4\u53d7\u9752\u5c11\u5e74\u9752\u7750\uff0c\u4e14\u63d0\u5347\u4e86\u5bf9\u804a\u5929\u673a\u5668\u4eba\u7684\u4eba\u6027\u5316\u8ba4\u77e5\u4e0e\u4fe1\u4efb\u3002", "motivation": "\u63a2\u8ba8\u5bf9\u8bdd\u98ce\u683c\u5982\u4f55\u5f71\u54cd\u9752\u5c11\u5e74\u5bf9\u804a\u5929\u673a\u5668\u4eba\u7684\u4eba\u6027\u5316\u548c\u60c5\u611f\u4f9d\u8d56\uff0c\u4ee5\u6539\u5584AI\u5bf9\u9752\u5c11\u5e74\u7684\u60c5\u611f\u652f\u6301\u3002", "method": "\u901a\u8fc7\u5728\u7ebf\u5b9e\u9a8c\uff0c\u5bf9284\u5bf9\u9752\u5c11\u5e74\u548c\u7236\u6bcd\u8fdb\u884c\u7814\u7a76\uff0c\u6bd4\u8f83\u4e86\u5173\u7cfb\u98ce\u683c\u4e0e\u900f\u660e\u98ce\u683c\u7684\u804a\u5929\u8bb0\u5f55\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4f7f\u7528\u5173\u7cfb\u98ce\u683c\u7684\u804a\u5929\u673a\u5668\u4eba\u66f4\u80fd\u5438\u5f15\u9752\u5c11\u5e74\uff0c\u589e\u5f3a\u4eba\u6027\u5316\u3001\u4fe1\u4efb\u611f\u548c\u60c5\u611f\u4eb2\u5bc6\u611f\u3002", "conclusion": "\u5173\u7cfb\u98ce\u683c\u7684\u5bf9\u8bdd\u8bbe\u8ba1\u80fd\u591f\u63d0\u9ad8\u9752\u5c11\u5e74\u5bf9\u804a\u5929\u673a\u5668\u4eba\u7684\u4fe1\u4efb\u548c\u4f9d\u8d56\u6027\uff0c\u8fd9\u5bf9\u60c5\u611f\u8106\u5f31\u7684\u9752\u5c11\u5e74\u6709\u7279\u6b8a\u5f71\u54cd\u3002"}}
{"id": "2512.15215", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.15215", "abs": "https://arxiv.org/abs/2512.15215", "authors": ["Erik Brorsson", "Kristian Ceder", "Ze Zhang", "Sabino Francesco Roselli", "Endre Er\u0151s", "Martin Dahl", "Beatrice Alenljung", "Jessica Lindblom", "Thanh Bui", "Emmanuel Dean", "Lennart Svensson", "Kristofer Bengtsson", "Per-Lage G\u00f6tvall", "Knut \u00c5kesson"], "title": "Infrastructure-based Autonomous Mobile Robots for Internal Logistics -- Challenges and Future Perspectives", "comment": null, "summary": "The adoption of Autonomous Mobile Robots (AMRs) for internal logistics is accelerating, with most solutions emphasizing decentralized, onboard intelligence. While AMRs in indoor environments like factories can be supported by infrastructure, involving external sensors and computational resources, such systems remain underexplored in the literature. This paper presents a comprehensive overview of infrastructure-based AMR systems, outlining key opportunities and challenges. To support this, we introduce a reference architecture combining infrastructure-based sensing, on-premise cloud computing, and onboard autonomy. Based on the architecture, we review core technologies for localization, perception, and planning. We demonstrate the approach in a real-world deployment in a heavy-vehicle manufacturing environment and summarize findings from a user experience (UX) evaluation. Our aim is to provide a holistic foundation for future development of scalable, robust, and human-compatible AMR systems in complex industrial environments.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u7840\u8bbe\u65bd\u9a71\u52a8\u7684\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u63a2\u8ba8\u6280\u672f\u53ca\u7528\u6237\u4f53\u9a8c\uff0c\u5e76\u63d0\u51fa\u67b6\u6784\u4ee5\u63a8\u52a8\u672a\u6765\u53d1\u5c55", "motivation": "\u7814\u7a76AMR\u7cfb\u7edf\u5728\u590d\u6742\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u5e94\u7528\uff0c\u5f3a\u8c03\u57fa\u7840\u8bbe\u65bd\u652f\u6301\u7684\u91cd\u8981\u6027", "method": "\u63d0\u4f9b\u4e00\u4e2a\u57fa\u7840\u67b6\u6784\u7ed3\u5408\u57fa\u7840\u8bbe\u65bd\u4f20\u611f\u3001\u73b0\u573a\u4e91\u8ba1\u7b97\u548c\u8f66\u8f7d\u81ea\u4e3b\u6027", "result": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u57fa\u7840\u8bbe\u65bd\u7684AMR\u7cfb\u7edf\u7684\u5168\u9762\u6982\u8ff0\uff0c\u603b\u7ed3\u4e86\u5173\u952e\u6280\u672f\u53ca\u7528\u6237\u4f53\u9a8c\u8bc4\u4f30", "conclusion": "\u4e3a\u672a\u6765\u5927\u578b\u3001\u7a33\u5065\u4e14\u4eba\u6027\u5316\u7684AMR\u7cfb\u7edf\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840"}}
{"id": "2512.15220", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.15220", "abs": "https://arxiv.org/abs/2512.15220", "authors": ["Vahid Pooryousef", "Tim Dwyer", "Richard Bassed", "Maxime Cordeil", "Lonni Besan\u00e7on"], "title": "Lessons Learnt from Expert-Centred Studies Exploring Opportunities and Challenges for Immersive Forensic Investigation", "comment": null, "summary": "Research studies involving human participants present challenges, including strict ethical considerations, participant recruitment, costs, and many human factors. While human-computer interaction researchers are familiar with these challenges and current solutions, expert-centred studies can be even more challenging in ways that researchers may not anticipate. This issue is particularly important as research grants are increasingly based on practical and real-world problems, which necessitate close collaboration with experts. In this paper, we reflect on and discuss the challenges, solutions, and specific requirements that arose during our expert-centred studies conducted over three years of a PhD study exploring immersive forensic investigation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u8fdb\u884c\u4e13\u5bb6\u4e2d\u5fc3\u7814\u7a76\u65f6\u9047\u5230\u7684\u591a\u79cd\u6311\u6218\u548c\u89e3\u51b3\u65b9\u6848\uff0c\u5f3a\u8c03\u4e86\u4e0e\u4e13\u5bb6\u7d27\u5bc6\u5408\u4f5c\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u968f\u7740\u7814\u7a76\u7ecf\u8d39\u65e5\u76ca\u57fa\u4e8e\u5b9e\u8df5\u6027\u548c\u73b0\u5b9e\u95ee\u9898\uff0c\u7814\u7a76\u8005\u9700\u8981\u6df1\u5165\u4e86\u89e3\u4e0e\u4e13\u5bb6\u5408\u4f5c\u7684\u96be\u70b9\u548c\u5e94\u5bf9\u7b56\u7565\u3002", "method": "\u53cd\u601d\u5e76\u8ba8\u8bba\u4e13\u5bb6\u4e2d\u5fc3\u7814\u7a76\u4e2d\u7684\u6311\u6218\u548c\u89e3\u51b3\u65b9\u6848", "result": "\u8bc6\u522b\u5e76\u5e94\u5bf9\u5728\u8fdb\u884c\u6709\u5173\u6c89\u6d78\u5f0f\u6cd5\u533b\u8c03\u67e5\u7684\u4e13\u5bb6\u4e2d\u5fc3\u7814\u7a76\u65f6\u6240\u9047\u5230\u7684\u6311\u6218\u548c\u5177\u4f53\u8981\u6c42", "conclusion": "\u4e13\u5bb6\u4e2d\u5fc3\u7814\u7a76\u7684\u590d\u6742\u6027\u8981\u6c42\u7814\u7a76\u8005\u5177\u5907\u7075\u6d3b\u5e94\u5bf9\u7b56\u7565\uff0c\u4ee5\u786e\u4fdd\u7814\u7a76\u7684\u6210\u529f\u5b9e\u65bd\u3002"}}
{"id": "2512.15258", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15258", "abs": "https://arxiv.org/abs/2512.15258", "authors": ["Yuze Wu", "Mo Zhu", "Xingxing Li", "Yuheng Du", "Yuxin Fan", "Wenjun Li", "Xin Zhou", "Fei Gao"], "title": "VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments", "comment": null, "summary": "This paper proposes VLA-AN, an efficient and onboard Vision-Language-Action (VLA) framework dedicated to autonomous drone navigation in complex environments. VLA-AN addresses four major limitations of existing large aerial navigation models: the data domain gap, insufficient temporal navigation with reasoning, safety issues with generative action policies, and onboard deployment constraints. First, we construct a high-fidelity dataset utilizing 3D Gaussian Splatting (3D-GS) to effectively bridge the domain gap. Second, we introduce a progressive three-stage training framework that sequentially reinforces scene comprehension, core flight skills, and complex navigation capabilities. Third, we design a lightweight, real-time action module coupled with geometric safety correction. This module ensures fast, collision-free, and stable command generation, mitigating the safety risks inherent in stochastic generative policies. Finally, through deep optimization of the onboard deployment pipeline, VLA-AN achieves a robust real-time 8.3x improvement in inference throughput on resource-constrained UAVs. Extensive experiments demonstrate that VLA-AN significantly improves spatial grounding, scene reasoning, and long-horizon navigation, achieving a maximum single-task success rate of 98.1%, and providing an efficient, practical solution for realizing full-chain closed-loop autonomy in lightweight aerial robots.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u7684VLA-AN\u6846\u67b6\u6709\u6548\u63d0\u5347\u65e0\u4eba\u673a\u5bfc\u822a\u80fd\u529b\uff0c\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u7684\u4e3b\u8981\u9650\u5236\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u6210\u529f\u7387\u548c\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u5927\u89c4\u6a21\u65e0\u4eba\u673a\u5bfc\u822a\u6a21\u578b\u7684\u9650\u5236", "method": "\u6784\u5efa\u9ad8\u4fdd\u771f\u6570\u636e\u96c6\uff0c\u5e76\u5f15\u5165\u6e10\u8fdb\u5f0f\u4e09\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\u548c\u8f7b\u91cf\u7ea7\u5b9e\u65f6\u52a8\u4f5c\u6a21\u5757", "result": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u65e0\u4eba\u673a\u4e0a\u5b9e\u73b0\u4e86\u5b9e\u65f6\u63a8\u7406\u541e\u5410\u91cf\u76848.3\u500d\u63d0\u5347\uff0c\u5355\u4efb\u52a1\u6210\u529f\u7387\u8fbe\u523098.1%", "conclusion": "VLA-AN\u663e\u8457\u6539\u5584\u7a7a\u95f4\u5b9a\u4f4d\u3001\u573a\u666f\u63a8\u7406\u4e0e\u957f\u8ddd\u79bb\u5bfc\u822a\uff0c\u63d0\u4f9b\u8f7b\u91cf\u7ea7\u65e0\u4eba\u673a\u5168\u94fe\u95ed\u73af\u81ea\u4e3b\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.15263", "categories": ["cs.HC", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.15263", "abs": "https://arxiv.org/abs/2512.15263", "authors": ["Ashirbad Samantaray", "Taranjit Kaur", "Sapna S Mishra", "Kritika Lohia", "Chayan Majumder", "Sheffali Gulati", "Tapan Kumar Gandhi"], "title": "Development of Immersive Virtual and Augmented Reality-Based Joint Attention Training Platform for Children with Autism", "comment": null, "summary": "Joint Attention (JA), a crucial social skill for developing shared focus, is often impaired in children with Autism Spectrum Disorder (ASD), affecting social communication and highlighting the need for early intervention. Addressing gaps in prior research, such as limited use of immersive technology and reliance on distracting peripherals, we developed a novel JA training platform using Augmented Reality (AR) and Virtual Reality (VR) devices. The platform integrates eye gaze-based interactions to ensure participants undivided attention. To validate the platform, we conducted experiments on ASD (N=19) and Neurotypical (NT) (N=13) participants under a trained pediatric neurologist's supervision. For quantitative analysis, we employed key measures such as the number of correct responses, the duration of establishing eye contact (s), and the duration of registering a response (s), along with correlations to CARS scores and age. Results from AR-based experiments showed NT participants registered responses significantly faster (<0.00001) than ASD participants. A correlation (Spearman coefficient=0.57, p=0.03) was found between ASD participants response time and CARS scores. A similar trend was observed in VR-based experiments. When comparing response accuracy in ASD participants across platforms, AR yielded a higher correctness rate (92.30%) than VR (69.49%), indicating AR's greater effectiveness. These findings suggest that immersive technology can aid JA training in ASD. Future studies should explore long-term benefits and real-world applicability.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408AR\u548cVR\u7684\u5171\u540c\u6ce8\u610f\u529b\u57f9\u8bad\u5e73\u53f0\uff0c\u7ed3\u679c\u8868\u660eAR\u5728\u8bad\u7ec3\u81ea\u95ed\u75c7\u513f\u7ae5\u7684\u5171\u540c\u6ce8\u610f\u529b\u65b9\u9762\u5177\u6709\u66f4\u9ad8\u7684\u6548\u679c\uff0c\u5f3a\u8c03\u4e86\u6c89\u6d78\u5f0f\u6280\u672f\u5728\u5e72\u9884\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u81ea\u95ed\u75c7\u8c31\u7cfb\u969c\u788d\u513f\u7ae5\u5728\u5171\u540c\u6ce8\u610f\u529b\u65b9\u9762\u5b58\u5728\u7f3a\u9677\uff0c\u5f71\u54cd\u793e\u4ea4\u6c9f\u901a\uff0c\u5f3a\u8c03\u4e86\u65e9\u671f\u5e72\u9884\u7684\u5fc5\u8981\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u4f7f\u7528\u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u548c\u865a\u62df\u73b0\u5b9e\uff08VR\uff09\u8bbe\u5907\u7684\u65b0\u7684\u5171\u540c\u6ce8\u610f\u529b\u57f9\u8bad\u5e73\u53f0\uff0c\u96c6\u6210\u4e86\u57fa\u4e8e\u773c\u52a8\u7684\u4e92\u52a8\u4ee5\u786e\u4fdd\u53c2\u4e0e\u8005\u7684\u4e13\u6ce8\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u8be5\u5e73\u53f0\uff0c\u5728\u4e34\u5e8a\u795e\u7ecf\u5b66\u5bb6\u76d1\u7763\u4e0b\u5bf9\u81ea\u95ed\u75c7\uff08N=19\uff09\u548c\u795e\u7ecf\u5178\u578b\uff08N=13\uff09\u53c2\u4e0e\u8005\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "AR\u5b9e\u9a8c\u4e2d\uff0c\u795e\u7ecf\u5178\u578b\u53c2\u4e0e\u8005\u7684\u53cd\u5e94\u901f\u5ea6\u663e\u8457\u5feb\u4e8e\u81ea\u95ed\u75c7\u53c2\u4e0e\u8005\uff08p<0.00001\uff09\u3002\u540c\u65f6\u53d1\u73b0\u81ea\u95ed\u75c7\u53c2\u4e0e\u8005\u7684\u53cd\u5e94\u65f6\u95f4\u4e0eCARS\u8bc4\u5206\u4e4b\u95f4\u5b58\u5728\u76f8\u5173\u6027\uff08Spearman\u7cfb\u6570=0.57, p=0.03\uff09\u3002\u5728\u5bf9\u6bd4\u81ea\u95ed\u75c7\u53c2\u4e0e\u8005\u5728\u4e0d\u540c\u5e73\u53f0\u4e0a\u7684\u53cd\u5e94\u51c6\u786e\u5ea6\u65f6\uff0cAR\u7684\u6b63\u786e\u7387\uff0892.30%\uff09\u9ad8\u4e8eVR\uff0869.49%\uff09\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u6c89\u6d78\u5f0f\u6280\u672f\u53ef\u4ee5\u6709\u6548\u5e2e\u52a9\u81ea\u95ed\u75c7\u7684\u5171\u540c\u6ce8\u610f\u529b\u8bad\u7ec3\uff0c\u672a\u6765\u7684\u7814\u7a76\u5e94\u63a2\u8ba8\u5176\u957f\u671f\u6548\u679c\u548c\u73b0\u5b9e\u5e94\u7528\u3002"}}
{"id": "2512.15282", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.15282", "abs": "https://arxiv.org/abs/2512.15282", "authors": ["Martijn IJtsma", "Salvatore Hargis"], "title": "A Network-Based Framework for Modeling and Analyzing Human-Robot Coordination Strategies", "comment": "Under review at IEEE Transactions on Human-Machine Systems. 12 pages, 5 figures", "summary": "Studies of human-robot interaction in dynamic and unstructured environments show that as more advanced robotic capabilities are deployed, the need for cooperative competencies to support collaboration with human problem-holders increases. Designing human-robot systems to meet these demands requires an explicit understanding of the work functions and constraints that shape the feasibility of alternative joint work strategies. Yet existing human-robot interaction frameworks either emphasize computational support for real-time execution or rely on static representations for design, offering limited support for reasoning about coordination dynamics during early-stage conceptual design. To address this gap, this article presents a novel computational framework for analyzing joint work strategies in human-robot systems by integrating techniques from functional modeling with graph-theoretic representations. The framework characterizes collective work in terms of the relationships among system functions and the physical and informational structure of the work environment, while explicitly capturing how coordination demands evolve over time. Its use during conceptual design is demonstrated through a case study in disaster robotics, which shows how the framework can be used to support early trade-space exploration of human-robot coordination strategies and to identify cooperative competencies that support flexible management of coordination overhead. These results show how the framework makes coordination demands and their temporal evolution explicit, supporting design-time reasoning about cooperative competency requirements and work demands prior to implementation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u6846\u67b6\u5206\u6790\u4eba\u673a\u7cfb\u7edf\u4e2d\u7684\u8054\u5408\u5de5\u4f5c\u7b56\u7565\uff0c\u5f3a\u8c03\u534f\u8c03\u9700\u6c42\u7684\u6f14\u53d8\uff0c\u652f\u6301\u8bbe\u8ba1\u9636\u6bb5\u7684\u5408\u4f5c\u80fd\u529b\u8981\u6c42\u63a8\u7406\u3002", "motivation": "\u968f\u7740\u66f4\u5148\u8fdb\u7684\u673a\u5668\u4eba\u80fd\u529b\u7684\u90e8\u7f72\uff0c\u4eba\u673a\u5408\u4f5c\u80fd\u529b\u7684\u9700\u6c42\u589e\u52a0\uff0c\u56e0\u6b64\u9700\u8981\u8bbe\u8ba1\u80fd\u591f\u652f\u6301\u8fd9\u4e9b\u9700\u6c42\u7684\u4eba\u673a\u7cfb\u7edf\uff0c\u5e76\u660e\u786e\u7406\u89e3\u5de5\u4f5c\u529f\u80fd\u53ca\u5176\u7ea6\u675f\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u96c6\u6210\u529f\u80fd\u5efa\u6a21\u4e0e\u56fe\u8bba\u8868\u793a\uff0c\u5206\u6790\u4eba\u673a\u7cfb\u7edf\u4e2d\u7684\u8054\u5408\u5de5\u4f5c\u7b56\u7565\u3002", "result": "\u8be5\u6846\u67b6\u901a\u8fc7\u7cfb\u7edf\u529f\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\u4ee5\u53ca\u5de5\u4f5c\u73af\u5883\u7684\u7269\u7406\u548c\u4fe1\u606f\u7ed3\u6784\u6765\u8868\u5f81\u96c6\u4f53\u5de5\u4f5c\uff0c\u5e76\u660e\u786e\u6355\u6349\u534f\u8c03\u9700\u6c42\u5982\u4f55\u968f\u65f6\u95f4\u6f14\u53d8\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u707e\u96be\u673a\u5668\u4eba\u9886\u57df\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u80fd\u591f\u652f\u6301\u65e9\u671f\u7684\u4eba\u673a\u534f\u8c03\u7b56\u7565\u7684\u63a2\u7d22\uff0c\u5e76\u8bc6\u522b\u652f\u6301\u7075\u6d3b\u7ba1\u7406\u534f\u8c03\u5f00\u9500\u7684\u5408\u4f5c\u80fd\u529b\u3002"}}
{"id": "2512.15325", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15325", "abs": "https://arxiv.org/abs/2512.15325", "authors": ["Agnieszka Bienkowska", "Jacek Malecki", "Alexander Mathiesen-Ohman", "Katarzyna Tworek"], "title": "Managing Ambiguity: A Proof of Concept of Human-AI Symbiotic Sense-making based on Quantum-Inspired Cognitive Mechanism of Rogue Variable Detection", "comment": "19 pages, 6 figures", "summary": "Organizations increasingly operate in environments characterized by volatility, uncertainty, complexity, and ambiguity (VUCA), where early indicators of change often emerge as weak, fragmented signals. Although artificial intelligence (AI) is widely used to support managerial decision-making, most AI-based systems remain optimized for prediction and resolution, leading to premature interpretive closure under conditions of high ambiguity. This creates a gap in management science regarding how human-AI systems can responsibly manage ambiguity before it crystallizes into error or crisis. This study addresses this gap by presenting a proof of concept (PoC) of the LAIZA human-AI augmented symbiotic intelligence system and its patented process: Systems and Methods for Quantum-Inspired Rogue Variable Modeling (QRVM), Human-in-the-Loop Decoherence, and Collective Cognitive Inference. The mechanism operationalizes ambiguity as a non-collapsed cognitive state, detects persistent interpretive breakdowns (rogue variables), and activates structured human-in-the-loop clarification when autonomous inference becomes unreliable. Empirically, the article draws on a three-month case study conducted in 2025 within the AI development, involving prolonged ambiguity surrounding employee intentions and intellectual property boundaries. The findings show that preserving interpretive plurality enabled early scenario-based preparation, including proactive patent protection, allowing decisive and disruption-free action once ambiguity collapsed. The study contributes to management theory by reframing ambiguity as a first-class construct and demonstrates the practical value of human-AI symbiosis for organizational resilience in VUCA environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728VUCA\u73af\u5883\u4e2d\u5982\u4f55\u901a\u8fc7LAIZA\u4eba\u673a\u5171\u751f\u7cfb\u7edf\u7ba1\u7406\u6a21\u7cca\u6027\uff0c\u4ee5\u589e\u5f3a\u7ec4\u7ec7\u97e7\u6027\u3002", "motivation": "\u5728VUCA\u73af\u5883\u4e2d\uff0c\u7ec4\u7ec7\u9700\u8981\u6709\u6548\u7ba1\u7406\u6a21\u7cca\u6027\u4ee5\u907f\u514d\u51b3\u7b56\u9519\u8bef\uff0c\u800c\u73b0\u6709\u7684AI\u7cfb\u7edf\u5728\u9ad8\u5ea6\u6a21\u7cca\u7684\u6761\u4ef6\u4e0b\u5bb9\u6613\u5bfc\u81f4\u89e3\u91ca\u63d0\u524d\u95ed\u5408\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86LAIZA\u4eba\u673a\u589e\u5f3a\u5171\u751f\u667a\u80fd\u7cfb\u7edf\u53ca\u5176\u4e13\u5229\u6d41\u7a0b\uff0c\u901a\u8fc7\u5c06\u6a21\u7cca\u6027\u89c6\u4e3a\u975e\u5d29\u584c\u7684\u8ba4\u77e5\u72b6\u6001\uff0c\u53ca\u65f6\u68c0\u6d4b\u5230\u89e3\u91ca\u6027\u7834\u88c2\u5e76\u8fdb\u884c\u4eba\u7c7b\u4ecb\u5165\u6f84\u6e05\u3002", "result": "\u901a\u8fc7\u4e09\u4e2a\u6708\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u53d1\u73b0\u4fdd\u6301\u89e3\u91ca\u7684\u591a\u6837\u6027\u6709\u52a9\u4e8e\u63d0\u524d\u51c6\u5907\u60c5\u666f\uff0c\u5305\u62ec\u4e3b\u52a8\u8fdb\u884c\u4e13\u5229\u4fdd\u62a4\uff0c\u4ece\u800c\u5728\u6a21\u7cca\u6027\u5d29\u6e83\u540e\u80fd\u591f\u679c\u65ad\u884c\u52a8\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u63d0\u51faLAIZA\u4eba\u673a\u589e\u5f3a\u5171\u751f\u667a\u80fd\u7cfb\u7edf\u7684\u6982\u5ff5\u8bc1\u660e\uff0c\u5f3a\u8c03\u5c06\u6a21\u7cca\u6027\u4f5c\u4e3a\u91cd\u8981\u6784\u5efa\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u4eba\u673a\u5171\u751f\u5728VUCA\u73af\u5883\u4e2d\u63d0\u9ad8\u7ec4\u7ec7\u97e7\u6027\u7684\u5b9e\u9645\u4ef7\u503c\u3002"}}
{"id": "2512.15309", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.15309", "abs": "https://arxiv.org/abs/2512.15309", "authors": ["Kai Zhang", "Shoubin Chen", "Dong Li", "Baiyang Zhang", "Tao Huang", "Zehao Wu", "Jiasheng Chen", "Bo Zhang"], "title": "GuangMing-Explorer: A Four-Legged Robot Platform for Autonomous Exploration in General Environments", "comment": "6 pages, published in ICUS2025", "summary": "Autonomous exploration is a fundamental capability that tightly integrates perception, planning, control, and motion execution. It plays a critical role in a wide range of applications, including indoor target search, mapping of extreme environments, resource exploration, etc. Despite significant progress in individual components, a holistic and practical description of a completely autonomous exploration system, encompassing both hardware and software, remains scarce. In this paper, we present GuangMing-Explorer, a fully integrated autonomous exploration platform designed for robust operation across diverse environments. We provide a comprehensive overview of the system architecture, including hardware design, software stack, algorithm deployment, and experimental configuration. Extensive real-world experiments demonstrate the platform's effectiveness and efficiency in executing autonomous exploration tasks, highlighting its potential for practical deployment in complex and unstructured environments.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86GuangMing-Explorer\uff0c\u4e00\u4e2a\u5168\u9762\u81ea\u4e3b\u63a2\u7d22\u5e73\u53f0\uff0c\u5c55\u793a\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u76ee\u524d\u5173\u4e8e\u5b8c\u5168\u81ea\u52a8\u5316\u63a2\u6d4b\u7cfb\u7edf\u7684\u6574\u4f53\u548c\u5b9e\u7528\u63cf\u8ff0\u8f83\u5c11\uff0c\u800c\u81ea\u4e3b\u63a2\u7d22\u662f\u96c6\u611f\u77e5\u3001\u89c4\u5212\u3001\u63a7\u5236\u548c\u8fd0\u52a8\u6267\u884c\u4e8e\u4e00\u4f53\u7684\u57fa\u672c\u80fd\u529b\uff0c\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u63d0\u51fa\u4e86GuangMing-Explorer\uff0c\u4e00\u4e2a\u5168\u9762\u96c6\u6210\u7684\u81ea\u4e3b\u63a2\u7d22\u5e73\u53f0\uff0c\u6db5\u76d6\u786c\u4ef6\u8bbe\u8ba1\u3001\u8f6f\u4ef6\u5806\u6808\u3001\u7b97\u6cd5\u90e8\u7f72\u548c\u5b9e\u9a8c\u914d\u7f6e\u7684\u7cfb\u7edf\u67b6\u6784\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9645\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86\u8be5\u5e73\u53f0\u5728\u6267\u884c\u81ea\u4e3b\u63a2\u7d22\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u548c\u6548\u7387\uff0c\u8bc1\u660e\u5176\u5728\u590d\u6742\u548c\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u3002", "conclusion": "GuangMing-Explorer\u5e73\u53f0\u5c55\u793a\u4e86\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u5f3a\u5927\u7684\u64cd\u4f5c\u80fd\u529b\uff0c\u5e76\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2512.15343", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.15343", "abs": "https://arxiv.org/abs/2512.15343", "authors": ["Efe Bozkir", "Enkelejda Kasneci"], "title": "Exploring User Acceptance and Concerns toward LLM-powered Conversational Agents in Immersive Extended Reality", "comment": null, "summary": "The rapid development of generative artificial intelligence (AI) and large language models (LLMs), and the availability of services that make them accessible, have led the general public to begin incorporating them into everyday life. The extended reality (XR) community has also sought to integrate LLMs, particularly in the form of conversational agents, to enhance user experience and task efficiency. When interacting with such conversational agents, users may easily disclose sensitive information due to the naturalistic flow of the conversations, and combining such conversational data with fine-grained sensor data may lead to novel privacy issues. To address these issues, a user-centric understanding of technology acceptance and concerns is essential. Therefore, to this end, we conducted a large-scale crowdsourcing study with 1036 participants, examining user decision-making processes regarding LLM-powered conversational agents in XR, across factors of XR setting type, speech interaction type, and data processing location. We found that while users generally accept these technologies, they express concerns related to security, privacy, social implications, and trust. Our results suggest that familiarity plays a crucial role, as daily generative AI use is associated with greater acceptance. In contrast, previous ownership of XR devices is linked to less acceptance, possibly due to existing familiarity with the settings. We also found that men report higher acceptance with fewer concerns than women. Regarding data type sensitivity, location data elicited the most significant concern, while body temperature and virtual object states were considered least sensitive. Overall, our study highlights the importance of practitioners effectively communicating their measures to users, who may remain distrustful. We conclude with implications and recommendations for LLM-powered XR.", "AI": {"tldr": "\u968f\u7740\u751f\u6210\u6027\u4eba\u5de5\u667a\u80fd\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u7528\u6237\u5728\u6269\u5c55\u73b0\u5b9e\u4e2d\u63a5\u53d7\u8fd9\u4e9b\u6280\u672f\uff0c\u4f46\u5bf9\u5b89\u5168\u3001\u9690\u79c1\u7b49\u65b9\u9762\u5b58\u5728\u62c5\u5fe7\uff0c\u5c24\u5176\u5bf9\u4e8e\u6570\u636e\u654f\u611f\u6027\u6709\u660e\u663e\u5dee\u5f02\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4e86\u89e3\u7528\u6237\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6269\u5c55\u73b0\u5b9e\u4e2d\u5e94\u7528\u7684\u63a5\u53d7\u5ea6\u548c\u62c5\u5fe7\uff0c\u63a2\u8ba8\u7528\u6237\u51b3\u7b56\u8fc7\u7a0b\u5728\u6280\u672f\u4f7f\u7528\u4e2d\u7684\u5f71\u54cd\u3002", "method": "\u6211\u4eec\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u7684\u4f17\u5305\u7814\u7a76\uff0c\u6d89\u53ca1036\u540d\u53c2\u4e0e\u8005\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u4e0d\u540c\u6269\u5c55\u73b0\u5b9e\u73af\u5883\u3001\u8bed\u97f3\u4ea4\u4e92\u7c7b\u578b\u53ca\u6570\u636e\u5904\u7406\u5730\u70b9\u5bf9\u7528\u6237\u51b3\u7b56\u7684\u5f71\u54cd\u3002", "result": "\u8be5\u7814\u7a76\u5c55\u793a\u4e861036\u540d\u53c2\u4e0e\u8005\u5728\u6269\u5c55\u73b0\u5b9e\u73af\u5883\u4e2d\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u5bf9\u8bdd\u4ee3\u7406\u7684\u63a5\u53d7\u5ea6\u53ca\u5176\u62c5\u5fe7\u3002\u6574\u4f53\u4e0a\uff0c\u7528\u6237\u63a5\u53d7\u6b64\u6280\u672f\uff0c\u4f46\u5728\u5b89\u5168\u6027\u3001\u9690\u79c1\u3001\u793e\u4f1a\u5f71\u54cd\u548c\u4fe1\u4efb\u65b9\u9762\u8868\u73b0\u51fa\u987e\u8651\u3002", "conclusion": "\u4e3a\u4e86\u4fc3\u8fdb\u7528\u6237\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u6269\u5c55\u73b0\u5b9e\u6280\u672f\u7684\u63a5\u53d7\uff0c\u76f8\u5173\u4ece\u4e1a\u8005\u9700\u8981\u6709\u6548\u5730\u4e0e\u7528\u6237\u6c9f\u901a\u6280\u672f\u4f7f\u7528\u63aa\u65bd\uff0c\u4ee5\u51cf\u5c11\u7528\u6237\u7684\u4e0d\u4fe1\u4efb\u611f\u3002"}}
{"id": "2512.15379", "categories": ["cs.RO", "cs.CR", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.15379", "abs": "https://arxiv.org/abs/2512.15379", "authors": ["Michael Amir", "Manon Flageat", "Amanda Prorok"], "title": "Remotely Detectable Robot Policy Watermarking", "comment": null, "summary": "The success of machine learning for real-world robotic systems has created a new form of intellectual property: the trained policy. This raises a critical need for novel methods that verify ownership and detect unauthorized, possibly unsafe misuse. While watermarking is established in other domains, physical policies present a unique challenge: remote detection. Existing methods assume access to the robot's internal state, but auditors are often limited to external observations (e.g., video footage). This ``Physical Observation Gap'' means the watermark must be detected from signals that are noisy, asynchronous, and filtered by unknown system dynamics. We formalize this challenge using the concept of a \\textit{glimpse sequence}, and introduce Colored Noise Coherency (CoNoCo), the first watermarking strategy designed for remote detection. CoNoCo embeds a spectral signal into the robot's motions by leveraging the policy's inherent stochasticity. To show it does not degrade performance, we prove CoNoCo preserves the marginal action distribution. Our experiments demonstrate strong, robust detection across various remote modalities, including motion capture and side-way/top-down video footage, in both simulated and real-world robot experiments. This work provides a necessary step toward protecting intellectual property in robotics, offering the first method for validating the provenance of physical policies non-invasively, using purely remote observations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aColored Noise Coherency (CoNoCo) \u7684\u6c34\u5370\u7b56\u7565\uff0c\u65e8\u5728\u901a\u8fc7\u8fdc\u7a0b\u68c0\u6d4b\u65b9\u6cd5\u4fdd\u62a4\u673a\u5668\u4eba\u5de5\u667a\u80fd\u7684\u77e5\u8bc6\u4ea7\u6743\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u5728\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u6210\u529f\u50ac\u751f\u4e86\u65b0\u7684\u77e5\u8bc6\u4ea7\u6743\u5f62\u5f0f\uff1a\u8bad\u7ec3\u7b56\u7565\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u9a8c\u8bc1\u6240\u6709\u6743\u5e76\u68c0\u6d4b\u672a\u7ecf\u6388\u6743\u7684\u4f7f\u7528\u3002", "method": "\u4f7f\u7528\u4e00\u79cd\u79f0\u4e3aColored Noise Coherency (CoNoCo) \u7684\u6c34\u5370\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u901a\u8fc7\u5d4c\u5165\u5149\u8c31\u4fe1\u53f7\u5230\u673a\u5668\u4eba\u7684\u52a8\u4f5c\u4e2d\uff0c\u5229\u7528\u7b56\u7565\u7684\u56fa\u6709\u968f\u673a\u6027\u6765\u5b9e\u73b0\u8fdc\u7a0b\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCoNoCo\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u7684\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\uff0c\u80fd\u591f\u5728\u5404\u79cd\u8fdc\u7a0b\u68c0\u6d4b\u65b9\u5f0f\u4e0b\uff08\u5305\u62ec\u52a8\u4f5c\u6355\u6349\u548c\u89c6\u9891\u76d1\u63a7\uff09\u5b9e\u73b0\u5f3a\u5065\u7684\u6c34\u5370\u68c0\u6d4b\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5CoNoCo\uff0c\u7528\u4e8e\u901a\u8fc7\u8fdc\u7a0b\u89c2\u5bdf\u9a8c\u8bc1\u673a\u5668\u5b66\u4e60\u8bad\u7ec3\u7b56\u7565\u7684\u6240\u6709\u6743\uff0c\u8fd9\u5728\u673a\u5668\u4eba\u9886\u57df\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2512.15491", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.15491", "abs": "https://arxiv.org/abs/2512.15491", "authors": ["Omar Namnakani", "Yasmeen Abdrabou", "Jonathan Grizou", "Mohamed Khamis"], "title": "GazeBlend: Exploring Paired Gaze-Based Input Techniques for Navigation and Selection Tasks on Mobile Devices", "comment": "26 pages, 10 figures", "summary": "The potential of gaze for hands-free mobile interaction is increasingly evident. While each gaze input technique presents distinct advantages and limitations, a combination can amplify strengths and mitigate challenges. We report on the results of a user study (N=24), in which we compared the usability and performance of pairing three popular gaze input techniques: Dwell Time, Pursuits, and Gaze Gestures, for navigation and selection tasks while sitting and walking. Results show that pairing gestures for navigation with either Dwell time or Pursuits for selection improves task completion time and rate compared to using either individually. We discuss the implications of pairing gaze input techniques, such as how Pursuits may negatively impact other techniques, likely due to the visual clutter it adds, how integrating gestures for navigation reduces the chances of unintentional selections, and the impact of motor activity on performance. Our findings provide insights for effective gaze-enabled interfaces.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4e09\u79cd\u6ce8\u89c6\u8f93\u5165\u6280\u672f\u7684\u7ed3\u5408\u5728\u79fb\u52a8\u4ea4\u4e92\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u8fd9\u79cd\u7ed3\u5408\u53ef\u4ee5\u63d0\u9ad8\u4efb\u52a1\u5b8c\u6210\u6548\u7387\u3002", "motivation": "\u7814\u7a76\u8868\u660e\uff0c\u6ce8\u89c6\u5728\u65e0\u624b\u79fb\u52a8\u4ea4\u4e92\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u800c\u7ec4\u5408\u4e0d\u540c\u7684\u6ce8\u89c6\u8f93\u5165\u6280\u672f\u53ef\u4ee5\u653e\u5927\u4f18\u52bf\u5e76\u51cf\u8f7b\u6311\u6218\u3002", "method": "\u901a\u8fc7\u7528\u6237\u7814\u7a76\uff0c\u5bf9\u6bd4\u4e86\u4e09\u79cd\u6ce8\u89c6\u8f93\u5165\u6280\u672f\uff08Dwell Time\u3001Pursuits\u548cGaze Gestures\uff09\u7684\u53ef\u7528\u6027\u548c\u8868\u73b0\uff0c\u6d89\u53ca\u5750\u7740\u548c\u8d70\u52a8\u4e24\u79cd\u60c5\u51b5\u4e0b\u7684\u5bfc\u822a\u548c\u9009\u62e9\u4efb\u52a1\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5c06\u5bfc\u822a\u624b\u52bf\u4e0eDwell Time\u6216Pursuits\u7ed3\u5408\u4f7f\u7528\u80fd\u63d0\u9ad8\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u548c\u6210\u529f\u7387\uff0c\u4e0e\u5355\u72ec\u4f7f\u7528\u76f8\u6bd4\u4f18\u52bf\u660e\u663e\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u5173\u4e8e\u6709\u6548\u6ce8\u89c6\u9a71\u52a8\u754c\u9762\u7684\u89c1\u89e3\uff0c\u63a2\u8ba8\u4e86\u4e0d\u540c\u8f93\u5165\u6280\u672f\u4e4b\u95f4\u7684\u76f8\u4e92\u5f71\u54cd\u53ca\u5176\u5bf9\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\u3002"}}
{"id": "2512.15411", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.15411", "abs": "https://arxiv.org/abs/2512.15411", "authors": ["Zhenhan Yin", "Xuanhan Wang", "Jiahao Jiang", "Kaiyuan Deng", "Pengqi Chen", "Shuangle Li", "Chong Liu", "Xing Xu", "ingkuan Song", "Lianli Gao", "Heng Tao Shen"], "title": "MiVLA: Towards Generalizable Vision-Language-Action Model with Human-Robot Mutual Imitation Pre-training", "comment": null, "summary": "While leveraging abundant human videos and simulated robot data poses a scalable solution to the scarcity of real-world robot data, the generalization capability of existing vision-language-action models (VLAs) remains limited by mismatches in camera views, visual appearance, and embodiment morphologies. To overcome this limitation, we propose MiVLA, a generalizable VLA empowered by human-robot mutual imitation pre-training, which leverages inherent behavioral similarity between human hands and robotic arms to build a foundation of strong behavioral priors for both human actions and robotic control. Specifically, our method utilizes kinematic rules with left/right hand coordinate systems for bidirectional alignment between human and robot action spaces. Given human or simulated robot demonstrations, MiVLA is trained to forecast behavior trajectories for one embodiment, and imitate behaviors for another one unseen in the demonstration. Based on this mutual imitation, it integrates the behavioral fidelity of real-world human data with the manipulative diversity of simulated robot data into a unified model, thereby enhancing the generalization capability for downstream tasks. Extensive experiments conducted on both simulation and real-world platforms with three robots (ARX, PiPer and LocoMan), demonstrate that MiVLA achieves strong improved generalization capability, outperforming state-of-the-art VLAs (e.g., $\\boldsymbol\u03c0_{0}$, $\\boldsymbol\u03c0_{0.5}$ and H-RDT) by 25% in simulation, and 14% in real-world robot control tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMiVLA\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u4eba\u673a\u4e92\u6a21\u4eff\u7684\u9884\u8bad\u7ec3\uff0c\u63d0\u5347\u4e86\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u514b\u670d\u4e86\u89c6\u89d2\u3001\u89c6\u89c9\u5916\u89c2\u548c\u5f62\u6001\u5339\u914d\u7684\u9650\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8\u6a21\u578b\u5728\u6cdb\u5316\u80fd\u529b\u4e0a\u53d7\u5230\u76f8\u673a\u89c6\u89d2\u3001\u89c6\u89c9\u5916\u89c2\u548c\u673a\u5668\u4eba\u5f62\u6001\u4e0d\u5339\u914d\u7684\u9650\u5236\uff0c\u56e0\u6b64\u9700\u8981\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u9ad8\u5176\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51faMiVLA\u6a21\u578b\uff0c\u5229\u7528\u4eba\u624b\u4e0e\u673a\u5668\u4eba\u624b\u81c2\u95f4\u7684\u884c\u4e3a\u76f8\u4f3c\u6027\u8fdb\u884c\u4e92\u6a21\u4eff\u9884\u8bad\u7ec3\uff0c\u91c7\u7528\u8fd0\u52a8\u5b66\u89c4\u5219\u5b9e\u73b0\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u52a8\u4f5c\u7a7a\u95f4\u7684\u53cc\u5411\u5bf9\u9f50\uff0c\u5e76\u57fa\u4e8e\u4eba\u7c7b\u6216\u6a21\u62df\u673a\u5668\u4eba\u6f14\u793a\u8bad\u7ec3\u884c\u4e3a\u8f68\u8ff9\u9884\u6d4b\u548c\u6a21\u4eff\u3002", "result": "\u5728\u4eff\u771f\u5e73\u53f0\u4e0a\uff0cMiVLA\u5728\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u4e2d\u63d0\u5347\u4e8625%\u7684\u8868\u73b0\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u63d0\u9ad8\u4e8614%\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u6700\u5148\u8fdb\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8\u6a21\u578b\u3002", "conclusion": "MiVLA\u6a21\u578b\u6709\u6548\u6574\u5408\u4e86\u771f\u5b9e\u4e16\u754c\u4eba\u6570\u636e\u7684\u884c\u4e3a\u4fdd\u771f\u5ea6\u4e0e\u6a21\u62df\u673a\u5668\u4eba\u6570\u636e\u7684\u64cd\u4f5c\u591a\u6837\u6027\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2512.15514", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.15514", "abs": "https://arxiv.org/abs/2512.15514", "authors": ["Lu Ying", "Junxiu Tang", "Tingying He", "Jean-Daniel Fekete"], "title": "A Constructive Scientific Methodology to Improve Climate Figures from IPCC", "comment": null, "summary": "We propose a methodology to improve figures from the Intergovernmental Panel on Climate Change (IPCC), ensuring that all modifications remain scientifically rigorous. IPCC figures are notoriously difficult to understand, and although designers have proposed alternatives, these lack formal IPCC validation and can be dismissed by skeptics. To address this gap, our approach starts from official IPCC figures. We gather their associated learning objectives and devise tests to score a pool of figure readers to assess how well they learn the objectives.We define improvement as higher scores obtained by a comparable reader pool after viewing a revised figure, where all modifications undergo review to ensure scientific validity. This assessment gives freedom to designers, who can deviate from the original design while making sure the objectives are still met and improved. We demonstrate the methodology through a case study and describe unexpected challenges encountered during the process.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u6c14\u5019\u53d8\u5316\u9886\u57dfIPCC\u56fe\u8868\u7684\u65b9\u6cd5\uff0c\u4ee5\u786e\u4fdd\u6240\u6709\u4fee\u6539\u4ecd\u7136\u79d1\u5b66\u4e25\u8c28\uff0c\u5e76\u901a\u8fc7\u5b66\u4e60\u76ee\u6807\u548c\u9605\u8bfb\u8005\u8bc4\u5206\u8bc4\u4f30\u5176\u6548\u679c\u3002", "motivation": "IPCC\u56fe\u8868\u96be\u4ee5\u7406\u89e3\uff0c\u73b0\u6709\u8bbe\u8ba1\u7f3a\u4e4f\u79d1\u5b66\u9a8c\u8bc1\uff0c\u5bb9\u6613\u88ab\u6000\u7591\u3002", "method": "\u4ece\u5b98\u65b9IPCC\u56fe\u8868\u5f00\u59cb\uff0c\u6536\u96c6\u5b66\u4e60\u76ee\u6807\uff0c\u5236\u5b9a\u6d4b\u8bd5\u4ee5\u8bc4\u4f30\u9605\u8bfb\u8005\u7684\u5b66\u4e60\u6548\u679c\uff0c\u5e76\u901a\u8fc7\u8bc4\u5206\u6bd4\u8f83\u539f\u59cb\u548c\u6539\u8fdb\u540e\u56fe\u8868\u7684\u6548\u679c\u3002", "result": "\u6539\u8fdb\u540e\u56fe\u8868\u7684\u9605\u8bfb\u8005\u5f97\u5206\u663e\u8457\u63d0\u9ad8\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u8bbe\u8ba1\u81ea\u7531\u5ea6\u3002", "conclusion": "\u672c\u6587\u7684\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u56fe\u8868\u7684\u53ef\u7406\u89e3\u6027\uff0c\u8fd8\u4e3a\u8bbe\u8ba1\u8005\u63d0\u4f9b\u4e86\u5728\u4fdd\u6301\u79d1\u5b66\u4e25\u8c28\u6027\u7684\u57fa\u7840\u4e0a\u8fdb\u884c\u521b\u9020\u6027\u4fee\u6539\u7684\u81ea\u7531\u3002"}}
{"id": "2512.15448", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.15448", "abs": "https://arxiv.org/abs/2512.15448", "authors": ["Sinan Emre", "Victor Barasuol", "Matteo Villa", "Claudio Semini"], "title": "Load-Based Variable Transmission Mechanism for Robotic Applications", "comment": "22nd International Conference on Advanced Robotics (ICAR 2025)", "summary": "This paper presents a Load-Based Variable Transmission (LBVT) mechanism designed to enhance robotic actuation by dynamically adjusting the transmission ratio in response to external torque demands. Unlike existing variable transmission systems that require additional actuators for active control, the proposed LBVT mechanism leverages a pre-tensioned spring and a four-bar linkage to passively modify the transmission ratio, thereby reducing the complexity of robot joint actuation systems. The effectiveness of the LBVT mechanism is evaluated through simulation-based analyses. The results confirm that the system achieves up to a 40 percent increase in transmission ratio upon reaching a predefined torque threshold, effectively amplifying joint torque when required without additional actuation. Furthermore, the simulations demonstrate a torque amplification effect triggered when the applied force exceeds 18 N, highlighting the system ability to autonomously respond to varying load conditions. This research contributes to the development of lightweight, efficient, and adaptive transmission systems for robotic applications, particularly in legged robots where dynamic torque adaptation is critical.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1f\u8f7d\u7684\u53ef\u53d8\u4f20\u52a8\u673a\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u4f20\u52a8\u6bd4\u6765\u589e\u5f3a\u673a\u5668\u4eba\u9a71\u52a8\uff0c\u80fd\u5728\u4e0d\u589e\u52a0\u6267\u884c\u5668\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u653e\u5927\u5173\u8282\u626d\u77e9\u3002", "motivation": "\u65e8\u5728\u51cf\u5c11\u673a\u5668\u4eba\u5173\u8282\u9a71\u52a8\u7cfb\u7edf\u7684\u590d\u6742\u6027\uff0c\u63d0\u9ad8\u5176\u9002\u5e94\u6027\u548c\u6548\u7387\u3002", "method": "\u901a\u8fc7\u9884\u5f20\u7d27\u5f39\u7c27\u548c\u56db\u6746\u8054\u52a8\u673a\u6784\u88ab\u52a8\u5730\u8c03\u8282\u4f20\u52a8\u6bd4\uff0c\u800c\u4e0d\u662f\u9700\u8981\u989d\u5916\u7684\u6267\u884c\u5668\u8fdb\u884c\u4e3b\u52a8\u63a7\u5236\u3002", "result": "\u5728\u9884\u8bbe\u7684\u626d\u77e9\u9608\u503c\u4e0b\uff0c\u4f20\u52a8\u6bd4\u63d0\u9ad8\u4e8640%\uff0c\u5e76\u4e14\u5728\u65bd\u52a0\u529b\u8d85\u8fc718N\u65f6\u89e6\u53d1\u4e86\u626d\u77e9\u653e\u5927\u6548\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8f7b\u91cf\u3001\u9ad8\u6548\u3001\u81ea\u9002\u5e94\u7684\u673a\u5668\u4eba\u4f20\u52a8\u7cfb\u7edf\u7684\u53d1\u5c55\u505a\u51fa\u4e86\u8d21\u732e\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u626d\u77e9\u9002\u5e94\u6027\u81f3\u5173\u91cd\u8981\u7684\u56db\u8db3\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u3002"}}
{"id": "2512.15557", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.15557", "abs": "https://arxiv.org/abs/2512.15557", "authors": ["Evgenii Kruzhkov", "Raphael Memmesheimer", "Sven Behnke"], "title": "OMCL: Open-vocabulary Monte Carlo Localization", "comment": "Accepted to IEEE RA-L", "summary": "Robust robot localization is an important prerequisite for navigation planning. If the environment map was created from different sensors, robot measurements must be robustly associated with map features. In this work, we extend Monte Carlo Localization using vision-language features. These open-vocabulary features enable to robustly compute the likelihood of visual observations, given a camera pose and a 3D map created from posed RGB-D images or aligned point clouds. The abstract vision-language features enable to associate observations and map elements from different modalities. Global localization can be initialized by natural language descriptions of the objects present in the vicinity of locations. We evaluate our approach using Matterport3D and Replica for indoor scenes and demonstrate generalization on SemanticKITTI for outdoor scenes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u673a\u5668\u4eba\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00\u7279\u5f81\u7ed3\u5408\u591a\u79cd\u4f20\u611f\u5668\u6570\u636e\uff0c\u589e\u5f3a\u4e86\u4e0d\u540c\u73af\u5883\u4e0b\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u5728\u4e0d\u540c\u4f20\u611f\u5668\u521b\u5efa\u7684\u73af\u5883\u5730\u56fe\u4e2d\uff0c\u673a\u5668\u4eba\u6d4b\u91cf\u9700\u8981\u7a33\u5065\u5730\u4e0e\u5730\u56fe\u7279\u5f81\u5173\u8054\uff0c\u4ee5\u5b9e\u73b0\u6709\u6548\u7684\u5bfc\u822a\u89c4\u5212\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u8499\u7279\u5361\u7f57\u5b9a\u4f4d\uff0c\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u7279\u5f81\u6765\u8ba1\u7b97\u89c6\u89c9\u89c2\u6d4b\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u5229\u7528\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u8fdb\u884c\u5168\u5c40\u5b9a\u4f4d\u521d\u59cb\u5316\u3002", "result": "\u6269\u5c55\u4e86\u8499\u7279\u5361\u7f57\u5b9a\u4f4d\uff0c\u901a\u8fc7\u5f15\u5165\u89c6\u89c9-\u8bed\u8a00\u7279\u5f81\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5728\u5bfc\u822a\u8ba1\u5212\u4e2d\u7684\u5b9a\u4f4d\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5ba4\u5185\u573a\u666f\uff08Matterport3D\u548cReplica\uff09\u548c\u5ba4\u5916\u573a\u666f\uff08SemanticKITTI\uff09\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u663e\u793a\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2512.15597", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.15597", "abs": "https://arxiv.org/abs/2512.15597", "authors": ["Giacomo Picardi", "Saverio Iacoponi", "Matias Carandell", "Jorge Aguirregomezcorta", "Mrudul Chellapurath", "Joaquin del Rio", "Marcello Calisti", "Iacopo Aguzzi"], "title": "An Open Toolkit for Underwater Field Robotics", "comment": "10 pages, 8 figures", "summary": "Underwater robotics is becoming increasingly important for marine science, environmental monitoring, and subsea industrial operations, yet the development of underwater manipulation and actuation systems remains restricted by high costs, proprietary designs, and limited access to modular, research-oriented hardware. While open-source initiatives have democratized vehicle construction and control software, a substantial gap persists for joint-actuated systems-particularly those requiring waterproof, feedback-enabled actuation suitable for manipulators, grippers, and bioinspired devices. As a result, many research groups face lengthy development cycles, limited reproducibility, and difficulty transitioning laboratory prototypes to field-ready platforms.\n  To address this gap, we introduce an open, cost-effective hardware and software toolkit for underwater manipulation research. The toolkit includes a depth-rated Underwater Robotic Joint (URJ) with early leakage detection, compact control and power management electronics, and a ROS2-based software stack for sensing and multi-mode actuation. All CAD models, fabrication files, PCB sources, firmware, and ROS2 packages are openly released, enabling local manufacturing, modification, and community-driven improvement.\n  The toolkit has undergone extensive laboratory testing and multiple field deployments, demonstrating reliable operation up to 40 m depth across diverse applications, including a 3-DoF underwater manipulator, a tendon-driven soft gripper, and an underactuated sediment sampler. These results validate the robustness, versatility, and reusability of the toolkit for real marine environments.\n  By providing a fully open, field-tested platform, this work aims to lower the barrier to entry for underwater manipulation research, improve reproducibility, and accelerate innovation in underwater field robotics.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5f00\u653e\u7684\u6c34\u4e0b\u64cd\u4f5c\u7814\u7a76\u5de5\u5177\u5305\uff0c\u65e8\u5728\u51cf\u5c11\u5f00\u53d1\u6210\u672c\uff0c\u5e76\u4fc3\u8fdb\u884c\u4e1a\u521b\u65b0\uff0c\u7ecf\u8fc7\u5e7f\u6cdb\u6d4b\u8bd5\u4ee5\u9a8c\u8bc1\u5176\u53ef\u9760\u6027\u548c\u591a\u7528\u9014\u6027\u3002", "motivation": "\u968f\u7740\u6c34\u4e0b\u673a\u5668\u4eba\u5728\u6d77\u6d0b\u79d1\u5b66\u3001\u73af\u5883\u76d1\u6d4b\u548c\u6d77\u5e95\u5de5\u4e1a\u4f5c\u4e1a\u4e2d\u7684\u91cd\u8981\u6027\u65e5\u76ca\u589e\u52a0\uff0c\u73b0\u6709\u7684\u6c34\u4e0b\u64cd\u4f5c\u548c\u9a71\u52a8\u7cfb\u7edf\u7684\u53d1\u5c55\u53d7\u9ad8\u6210\u672c\u3001\u4e13\u6709\u8bbe\u8ba1\u53ca\u6a21\u5757\u5316\u7814\u7a76\u786c\u4ef6\u7684\u6709\u9650\u83b7\u53d6\u5f71\u54cd\u3002", "method": "\u5f00\u53d1\u4e00\u4e2a\u5305\u62ec\u6c34\u4e0b\u673a\u5668\u4eba\u5173\u8282\uff08URJ\uff09\u3001\u63a7\u5236\u548c\u7535\u6e90\u7ba1\u7406\u7535\u5b50\u5143\u4ef6\u53ca\u57fa\u4e8eROS2\u7684\u8f6f\u4ef6\u6808\u5728\u5185\u7684\u5de5\u5177\u5305\uff0c\u5e76\u516c\u5f00\u6240\u6709\u8bbe\u8ba1\u6587\u4ef6\uff0c\u5b9e\u73b0\u672c\u5730\u5236\u9020\u548c\u793e\u533a\u9a71\u52a8\u7684\u6539\u8fdb\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f00\u653e\u3001\u7ecf\u6d4e\u5b9e\u60e0\u7684\u6c34\u4e0b\u64cd\u4f5c\u7814\u7a76\u5de5\u5177\u5305\uff0c\u5305\u62ec\u6df1\u5ea6\u8bc4\u4f30\u7684\u6c34\u4e0b\u673a\u5668\u4eba\u5173\u8282\uff08URJ\uff09\uff0c\u6709\u65e9\u671f\u6cc4\u6f0f\u68c0\u6d4b\u529f\u80fd\uff0c\u4ee5\u53ca\u57fa\u4e8eROS2\u7684\u8f6f\u4ef6\u6808\uff0c\u6210\u529f\u7ecf\u8fc7\u591a\u9879\u5b9e\u9a8c\u548c\u73b0\u573a\u6d4b\u8bd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6c34\u4e0b\u5f00\u53d1\u5e94\u7528\u3002", "conclusion": "\u8fd9\u4e2a\u5de5\u4f5c\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u5b8c\u5168\u5f00\u653e\u548c\u7ecf\u8fc7\u73b0\u573a\u6d4b\u8bd5\u7684\u5e73\u53f0\uff0c\u65e8\u5728\u964d\u4f4e\u6c34\u4e0b\u64cd\u4f5c\u7814\u7a76\u7684\u8fdb\u5165\u95e8\u69db\uff0c\u63d0\u9ad8\u53ef\u91cd\u590d\u6027\uff0c\u52a0\u901f\u6c34\u4e0b\u673a\u5668\u4eba\u9886\u57df\u7684\u521b\u65b0\u3002"}}
{"id": "2512.15692", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.15692", "abs": "https://arxiv.org/abs/2512.15692", "authors": ["Jonas Pai", "Liam Achenbach", "Victoriano Montesinos", "Benedek Forrai", "Oier Mees", "Elvis Nava"], "title": "mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs", "comment": null, "summary": "Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data. As a result, despite improved semantic generalization, the policy must implicitly infer complex physical dynamics and temporal dependencies solely from robot trajectories. This reliance creates an unsustainable data burden, necessitating continuous, large-scale expert data collection to compensate for the lack of innate physical understanding. We contend that while vision-language pretraining effectively captures semantic priors, it remains blind to physical causality. A more effective paradigm leverages video to jointly capture semantics and visual dynamics during pretraining, thereby isolating the remaining task of low-level control. To this end, we introduce \\model, a novel Video-Action Model (VAM) that pairs a pretrained Internet-scale video model with a flow matching-based action decoder conditioned on its latent representations. The decoder serves as an Inverse Dynamics Model (IDM), generating low-level robot actions from the latent representation of video-space action plans. Our extensive evaluation shows that our approach achieves state-of-the-art performance on simulated and real-world robotic manipulation tasks, improving sample efficiency by 10x and convergence speed by 2x compared to traditional VLA architectures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u65b0\u7684\u89c6\u9891-\u52a8\u4f5c\u6a21\u578b\uff08VAM\uff09\uff0c\u901a\u8fc7\u89c6\u9891\u9884\u8bad\u7ec3\u6539\u5584\u673a\u5668\u4eba\u64cd\u4f5c\u6027\u80fd\uff0c\u6837\u672c\u6548\u7387\u63d0\u534710\u500d\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff08VLA\uff09\u4f9d\u8d56\u4e8e\u4e0e\u7269\u7406\u52a8\u6001\u548c\u65f6\u95f4\u4f9d\u8d56\u6027\u65e0\u5173\u7684\u9759\u6001\u7f51\u9875\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5bfc\u81f4\u673a\u5668\u4eba\u5728\u6267\u884c\u64cd\u4f5c\u65f6\u9700\u63a8\u65ad\u590d\u6742\u7684\u7269\u7406\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u89c6\u9891-\u52a8\u4f5c\u6a21\u578b\uff08VAM\uff09\uff0c\u7ed3\u5408\u4e86\u9884\u8bad\u7ec3\u7684\u5927\u89c4\u6a21\u89c6\u9891\u6a21\u578b\u548c\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u52a8\u4f5c\u89e3\u7801\u5668\uff0c\u5229\u7528\u6f5c\u5728\u8868\u793a\u751f\u6210\u4f4e\u7ea7\u673a\u5668\u4eba\u52a8\u4f5c\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u6837\u672c\u6548\u7387\u4e0a\u63d0\u5347\u4e8610\u500d\uff0c\u6536\u655b\u901f\u5ea6\u63d0\u5347\u4e862\u500d\uff0c\u8fbe\u5230\u4e86\u6700\u65b0\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u89c6\u9891\u6a21\u578b\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u6355\u6349\u8bed\u4e49\u548c\u89c6\u89c9\u52a8\u6001\uff0c\u76f8\u6bd4\u4f20\u7edf\u7684VLA\u67b6\u6784\u663e\u8457\u6539\u5584\u4e86\u673a\u5668\u4eba\u7684\u64cd\u4f5c\u80fd\u529b\u3002"}}
