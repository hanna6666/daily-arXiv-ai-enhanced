{"id": "2601.20968", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.20968", "abs": "https://arxiv.org/abs/2601.20968", "authors": ["Yulie Arad", "Stav Ashur", "Nancy M. Amato"], "title": "Quick Heuristic Validation of Edges in Dynamic Roadmap Graphs", "comment": null, "summary": "In this paper we tackle the problem of adjusting roadmap graphs for robot motion planning to non-static environments. We introduce the \"Red-Green-Gray\" paradigm, a modification of the SPITE method, capable of classifying the validity status of nodes and edges using cheap heuristic checks, allowing fast semi-lazy roadmap updates. Given a roadmap, we use simple computational geometry methods to approximate the swept volumes of robots and perform lazy collision checks, and label a subset of the edges as invalid (red), valid (green), or unknown (gray). We present preliminary experimental results comparing our method to the well-established technique of Leven and Hutchinson, and showing increased accuracy as well as the ability to correctly label edges as invalid while maintaining comparable update runtimes.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8c03\u6574\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u7684\u8def\u7ebf\u56fe\uff0c\u901a\u8fc7\u7ea2-\u7eff-\u7070\u7684\u5206\u7c7b\u65b9\u6cd5\u63d0\u5347\u6709\u6548\u6027\u6807\u8bb0\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8c03\u6574\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u7684\u8def\u7ebf\u56fe\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\"\u7ea2-\u7eff-\u7070\"\u8303\u5f0f\uff0c\u901a\u8fc7\u4f4e\u6210\u672c\u7684\u542f\u53d1\u5f0f\u68c0\u67e5\u6765\u5206\u7c7b\u8282\u70b9\u548c\u8fb9\u7684\u6709\u6548\u6027\u72b6\u6001\uff0c\u8fdb\u884c\u5feb\u901f\u7684\u534a\u60f0\u6027\u8def\u7ebf\u56fe\u66f4\u65b0\u3002", "result": "\u4e0eLeven\u548cHutchinson\u7684\u6280\u672f\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u540c\u65f6\u5728\u66f4\u65b0\u8fd0\u884c\u65f6\u95f4\u4e0a\u4fdd\u6301\u4e86\u53ef\u6bd4\u8f83\u7684\u6c34\u5e73\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4fdd\u969c\u76f8\u4f3c\u66f4\u65b0\u8fd0\u884c\u65f6\u95f4\u7684\u540c\u65f6\uff0c\u51c6\u786e\u6027\u663e\u8457\u63d0\u5347\uff0c\u80fd\u591f\u6b63\u786e\u5c06\u8fb9\u6807\u8bb0\u4e3a\u65e0\u6548\u3002"}}
{"id": "2601.21011", "categories": ["cs.RO", "cs.MA", "cs.OS", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.21011", "abs": "https://arxiv.org/abs/2601.21011", "authors": ["Anshul Ranjan", "Anoosh Damodar", "Neha Chougule", "Dhruva S Nayak", "Anantharaman P. N", "Shylaja S S"], "title": "Meta-ROS: A Next-Generation Middleware Architecture for Adaptive and Scalable Robotic Systems", "comment": "Checkout the Python Library - https://pypi.org/project/metaros/ To be Submitted in ACM Transactions on Autonomous and Adaptive Systems (TAAS) Journal", "summary": "The field of robotics faces significant challenges related to the complexity and interoperability of existing middleware frameworks, like ROS2, which can be difficult for new developers to adopt. To address these issues, we propose Meta-ROS, a novel middleware solution designed to streamline robotics development by simplifying integration, enhancing performance, and ensuring cross-platform compatibility. Meta-ROS leverages modern communication protocols, such as Zenoh and ZeroMQ, to enable efficient and low-latency communication across diverse hardware platforms, while also supporting various data types like audio, images, and video. We evaluated Meta-ROS's performance through comprehensive testing, comparing it with existing middleware frameworks like ROS1 and ROS2. The results demonstrated that Meta-ROS outperforms ROS2, achieving up to 30% higher throughput, significantly reducing message latency, and optimizing resource usage. Additionally, its robust hardware support and developer-centric design facilitate seamless integration and ease of use, positioning Meta-ROS as an ideal solution for modern, real-time robotics AI applications.", "AI": {"tldr": "Meta-ROS\u662f\u4e00\u79cd\u65b0\u4e2d\u95f4\u4ef6\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7b80\u5316\u96c6\u6210\u548c\u63d0\u5347\u6027\u80fd\uff0c\u89e3\u51b3\u4e86ROS2\u7684\u590d\u6742\u6027\u548c\u4e92\u64cd\u4f5c\u6027\u95ee\u9898\uff0c\u5e76\u5728\u591a\u9879\u6027\u80fd\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8eROS2\u3002", "motivation": "\u5e94\u5bf9\u73b0\u6709\u4e2d\u95f4\u4ef6\u6846\u67b6\u5982ROS2\u5728\u590d\u6742\u6027\u548c\u4e92\u64cd\u4f5c\u6027\u4e0a\u5e26\u6765\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u4e3a\u4e86\u5e2e\u52a9\u65b0\u5f00\u53d1\u8005\u66f4\u5bb9\u6613\u4e0a\u624b\u3002", "method": "\u901a\u8fc7\u4e0e\u73b0\u6709\u4e2d\u95f4\u4ef6\u6846\u67b6\u5982ROS1\u548cROS2\u7684\u6bd4\u8f83\u6d4b\u8bd5\uff0c\u8bc4\u4f30Meta-ROS\u7684\u6027\u80fd\u3002", "result": "Meta-ROS\u8fbe\u5230\u4e86\u6700\u9ad830%\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6d88\u606f\u5ef6\u8fdf\uff0c\u5e76\u4f18\u5316\u4e86\u8d44\u6e90\u4f7f\u7528\u3002", "conclusion": "Meta-ROS\u5728\u5904\u7406\u6027\u80fd\u3001\u5ef6\u8fdf\u548c\u8d44\u6e90\u4f7f\u7528\u4e0a\u663e\u8457\u4f18\u4e8eROS2\uff0c\u662f\u73b0\u4ee3\u5b9e\u65f6\u673a\u5668\u4ebaAI\u5e94\u7528\u7684\u7406\u60f3\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.21027", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21027", "abs": "https://arxiv.org/abs/2601.21027", "authors": ["Youngim Nam", "Jungbin Kim", "Kyungtae Kang", "Cheolhyeon Kwon"], "title": "Track-centric Iterative Learning for Global Trajectory Optimization in Autonomous Racing", "comment": null, "summary": "This paper presents a global trajectory optimization framework for minimizing lap time in autonomous racing under uncertain vehicle dynamics. Optimizing the trajectory over the full racing horizon is computationally expensive, and tracking such a trajectory in the real world hardly assures global optimality due to uncertain dynamics. Yet, existing work mostly focuses on dynamics learning at the tracking level, without updating the trajectory itself to account for the learned dynamics. To address these challenges, we propose a track-centric approach that directly learns and optimizes the full-horizon trajectory. We first represent trajectories through a track-agnostic parametric space in light of the wavelet transform. This space is then efficiently explored using Bayesian optimization, where the lap time of each candidate is evaluated by running simulations with the learned dynamics. This optimization is embedded in an iterative learning framework, where the optimized trajectory is deployed to collect real-world data for updating the dynamics, progressively refining the trajectory over the iterations. The effectiveness of the proposed framework is validated through simulations and real-world experiments, demonstrating lap time improvement of up to 20.7% over a nominal baseline and consistently outperforming state-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u7403\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u65b9\u6cd5\u7ed3\u5408\u5b9e\u65f6\u6570\u636e\u66f4\u65b0\uff0c\u5b9e\u73b0\u4e86\u81ea\u4e3b\u8d5b\u8f66\u5728\u4e0d\u786e\u5b9a\u52a8\u6001\u4e0b\u7684\u8f68\u8ff9\u4f18\u5316\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5708\u901f\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u8ddf\u8e2a\u6c34\u5e73\u7684\u52a8\u6001\u5b66\u4e60\uff0c\u800c\u672a\u80fd\u66f4\u65b0\u8f68\u8ff9\u4ee5\u8003\u8651\u6240\u5b66\u4e60\u7684\u52a8\u6001\u3002\u56e0\u6b64\uff0c\u8feb\u5207\u9700\u8981\u4e00\u4e2a\u5168\u5c40\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ddf\u8e2a\u6c34\u5e73\u7684\u5168\u666f\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\uff0c\u4f7f\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\u63a2\u7d22\u53c2\u6570\u5316\u7a7a\u95f4\uff0c\u7ed3\u5408\u5b9e\u65f6\u6570\u636e\u66f4\u65b0\u52a8\u6001\uff0c\u4ee5\u5b9e\u73b0\u8f68\u8ff9\u7684\u9010\u6b65\u4f18\u5316\u3002", "result": "\u7ecf\u8fc7\u8fed\u4ee3\u5b66\u4e60\uff0c\u4f18\u5316\u540e\u7684\u8f68\u8ff9\u5728\u4eff\u771f\u548c\u5b9e\u9a13\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe20.7%\u7684\u65f6\u95f4\u51cf\u5c11\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u591a\u4e2a\u8fed\u4ee3\u4e2d\u4f18\u5316\u8f68\u8ff9\uff0c\u5e76\u5728\u6a21\u62df\u548c\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u53d6\u5f97\u4e86\u6700\u9ad820.7%\u7684\u65f6\u95f4\u6539\u5584\u3002"}}
{"id": "2601.21063", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21063", "abs": "https://arxiv.org/abs/2601.21063", "authors": ["Pierre-Yves Lajoie", "Karthik Soma", "Haechan Mark Bong", "Alice Lemieux-Bourque", "Rongge Zhang", "Vivek Shankar Varadharajan", "Giovanni Beltrame"], "title": "Multi-Robot Decentralized Collaborative SLAM in Planetary Analogue Environments: Dataset, Challenges, and Lessons Learned", "comment": null, "summary": "Decentralized collaborative simultaneous localization and mapping (C-SLAM) is essential to enable multirobot missions in unknown environments without relying on preexisting localization and communication infrastructure. This technology is anticipated to play a key role in the exploration of the Moon, Mars, and other planets. In this article, we share insights and lessons learned from C-SLAM experiments involving three robots operating on a Mars analogue terrain and communicating over an ad hoc network. We examine the impact of limited and intermittent communication on C-SLAM performance, as well as the unique localization challenges posed by planetary-like environments. Additionally, we introduce a novel dataset collected during our experiments, which includes real-time peer-to-peer inter-robot throughput and latency measurements. This dataset aims to support future research on communication-constrained, decentralized multirobot operations.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u591a\u673a\u5668\u4eba\u5728\u53bb\u4e2d\u5fc3\u5316\u534f\u4f5cSLAM\u4e2d\u9762\u4e34\u7684\u901a\u4fe1\u9650\u5236\u548c\u5b9a\u4f4d\u6311\u6218\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u76f8\u5173\u9886\u57df\u7684\u7814\u7a76\u3002", "motivation": "\u5728\u672a\u77e5\u73af\u5883\u4e2d\u5b9e\u73b0\u591a\u673a\u5668\u4eba\u4efb\u52a1\u7684\u53bb\u4e2d\u5fc3\u5316\u534f\u4f5cSLAM\u662f\u63a2\u7d22\u6708\u7403\u3001\u706b\u661f\u548c\u5176\u4ed6\u884c\u661f\u7684\u91cd\u8981\u6280\u672f\u3002", "method": "\u901a\u8fc7\u5bf9\u4e09\u53f0\u673a\u5668\u4eba\u5728\u706b\u661f\u7c7b\u5730\u5f62\u4e0a\u7684\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u53bb\u4e2d\u5fc3\u5316C-SLAM\u5728\u6709\u9650\u548c\u95f4\u6b47\u6027\u901a\u4fe1\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6709\u9650\u7684\u901a\u4fe1\u6761\u4ef6\u5bf9C-SLAM\u7684\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5e76\u4e14\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e86\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u5b9e\u65f6\u901a\u4fe1\u541e\u5410\u91cf\u548c\u5ef6\u8fdf\u6d4b\u91cf\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u5173\u591a\u673a\u5668\u4eba\u5728\u7c7b\u4f3c\u884c\u661f\u73af\u5883\u4e2d\u8fdb\u884c\u53bb\u4e2d\u5fc3\u5316\u534f\u4f5c\u540c\u65f6\u5b9a\u4f4d\u4e0e\u5730\u56fe\u6784\u5efa\uff08C-SLAM\uff09\u7684\u5b9e\u7528\u89c1\u89e3\uff0c\u5f3a\u8c03\u4e86\u901a\u4fe1\u9650\u5236\u5bf9C-SLAM\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u4f9b\u4e86\u521b\u65b0\u7684\u6570\u636e\u96c6\u4ee5\u652f\u6301\u672a\u6765\u7684\u7814\u7a76\u3002"}}
{"id": "2601.21001", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.21001", "abs": "https://arxiv.org/abs/2601.21001", "authors": ["Ron Fulbright"], "title": "Designing the Interactive Memory Archive (IMA): A Socio-Technical Framework for AI-Mediated Reminiscence and Cultural Memory Preservation", "comment": "10 pages, 2 figures, 45 references cited", "summary": "This paper introduces the Interactive Memory Archive (IMA), a conceptual framework for AI-mediated reminiscence designed to support cognitive en-gagement among older adults experiencing memory loss. IMA integrates multimodal sensing, natural language conversational scaffolding, and cloud-based archiving within the familiar form of a large format historical picture book. The model theorizes reminiscence as a guided, context-aware interaction eliciting autobiographical memories and preserving them as cul-tural artifacts. The paper positions IMA as a theoretical contribution, articu-lates testable propositions, and outlines a research agenda for future empiri-cal, technical, and ethical inquiry.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a\u4e92\u52a8\u8bb0\u5fc6\u6863\u6848\uff08IMA\uff09\u7684\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7AI\u652f\u6301\u8001\u5e74\u4eba\u7684\u56de\u5fc6\u4e92\u52a8\uff0c\u5e2e\u52a9\u4ed6\u4eec\u5e94\u5bf9\u8bb0\u5fc6\u4e27\u5931\u3002", "motivation": "\u968f\u7740\u4eba\u53e3\u8001\u9f84\u5316\uff0c\u8d8a\u6765\u8d8a\u591a\u7684\u8001\u5e74\u4eba\u9762\u4e34\u8bb0\u5fc6\u9000\u5316\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u5de5\u5177\u548c\u65b9\u6cd5\u6765\u4fc3\u8fdb\u4ed6\u4eec\u7684\u8ba4\u77e5\u53c2\u4e0e\u3002", "method": "IMA\u6846\u67b6\u7ed3\u5408\u591a\u6a21\u6001\u4f20\u611f\u3001\u81ea\u7136\u8bed\u8a00\u5bf9\u8bdd\u652f\u6491\u548c\u57fa\u4e8e\u4e91\u7684\u5b58\u6863\uff0c\u5448\u73b0\u4e3a\u4e00\u79cd\u5927\u578b\u5386\u53f2\u56fe\u518c\u7684\u5f62\u5f0f\u3002", "result": "\u8bba\u6587\u5b9a\u4f4dIMA\u4e3a\u7406\u8bba\u8d21\u732e\uff0c\u63d0\u51fa\u53ef\u6d4b\u8bd5\u7684\u5047\u8bbe\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u5b9e\u8bc1\u3001\u6280\u672f\u548c\u4f26\u7406\u63a2\u7d22\u5236\u5b9a\u4e86\u7814\u7a76\u8bae\u7a0b\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u4e92\u52a8\u8bb0\u5fc6\u6863\u6848\uff08IMA\uff09\u4e3a\u8f85\u52a9\u8001\u5e74\u4eba\u8bb0\u5fc6\u673a\u5236\u63d0\u4f9b\u4e86\u4e00\u4e2a\u521b\u65b0\u7684\u6846\u67b6\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2601.21129", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21129", "abs": "https://arxiv.org/abs/2601.21129", "authors": ["Guangping Liu", "Tipu Sultan", "Vittorio Di Giorgio", "Nick Hawkins", "Flavio Esposito", "Madi Babaiasl"], "title": "WheelArm-Sim: A Manipulation and Navigation Combined Multimodal Synthetic Data Generation Simulator for Unified Control in Assistive Robotics", "comment": "Accepted to IEEE International Symposium on Medical Robotics (ISMR) 2026", "summary": "Wheelchairs and robotic arms enhance independent living by assisting individuals with upper-body and mobility limitations in their activities of daily living (ADLs). Although recent advancements in assistive robotics have focused on Wheelchair-Mounted Robotic Arms (WMRAs) and wheelchairs separately, integrated and unified control of the combination using machine learning models remains largely underexplored. To fill this gap, we introduce the concept of WheelArm, an integrated cyber-physical system (CPS) that combines wheelchair and robotic arm controls. Data collection is the first step toward developing WheelArm models. In this paper, we present WheelArm-Sim, a simulation framework developed in Isaac Sim for synthetic data collection. We evaluate its capability by collecting a manipulation and navigation combined multimodal dataset, comprising 13 tasks, 232 trajectories, and 67,783 samples. To demonstrate the potential of the WheelArm dataset, we implement a baseline model for action prediction in the mustard-picking task. The results illustrate that data collected from WheelArm-Sim is feasible for a data-driven machine learning model for integrated control.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86WheelArm\uff0c\u4e00\u4e2a\u96c6\u6210\u4e86\u8f6e\u6905\u548c\u673a\u68b0\u81c2\u63a7\u5236\u7684\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\uff0c\u5f00\u53d1\u4e86WheelArm-Sim\u4eff\u771f\u6846\u67b6\u4ee5\u6536\u96c6\u591a\u6a21\u6001\u6570\u636e\uff0c\u7ed3\u679c\u8868\u660e\u5176\u5bf9\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u96c6\u6210\u63a7\u5236\u662f\u53ef\u884c\u7684\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u8f6e\u6905\u548c\u673a\u68b0\u81c2\u7684\u96c6\u6210\u548c\u7edf\u4e00\u63a7\u5236\uff0c\u586b\u8865\u5f53\u524d\u7814\u7a76\u4e2d\u7684\u7a7a\u767d\u3002", "method": "\u4f7f\u7528Isaac Sim\u5f00\u53d1\u4e86WheelArm-Sim\u4eff\u771f\u6846\u67b6\uff0c\u4ee5\u8fdb\u884c\u5408\u6210\u6570\u636e\u6536\u96c6\u3002", "result": "\u6536\u96c6\u4e86\u4e00\u4e2a\u5305\u542b13\u4e2a\u4efb\u52a1\u3001232\u6761\u8f68\u8ff9\u548c67,783\u4e2a\u6837\u672c\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5e76\u5728\u82a5\u83dc\u91c7\u6458\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u57fa\u51c6\u6a21\u578b\u7684\u52a8\u4f5c\u9884\u6d4b\u3002", "conclusion": "WheelArm-Sim\u6240\u6536\u96c6\u7684\u6570\u636e\u4e3a\u57fa\u4e8e\u6570\u636e\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u96c6\u6210\u63a7\u5236\u63d0\u4f9b\u4e86\u53ef\u884c\u6027\u3002"}}
{"id": "2601.21043", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21043", "abs": "https://arxiv.org/abs/2601.21043", "authors": ["Micha\u0142 Patryk Miazga", "Hannah Bussmann", "Antti Oulasvirta", "Patrick Ebel"], "title": "Log2Motion: Biomechanical Motion Synthesis from Touch Logs", "comment": null, "summary": "Touch data from mobile devices are collected at scale but reveal little about the interactions that produce them. While biomechanical simulations can illuminate motor control processes, they have not yet been developed for touch interactions. To close this gap, we propose a novel computational problem: synthesizing plausible motion directly from logs. Our key insight is a reinforcement learning-driven musculoskeletal forward simulation that generates biomechanically plausible motion sequences consistent with events recorded in touch logs. We achieve this by integrating a software emulator into a physics simulator, allowing biomechanical models to manipulate real applications in real-time. Log2Motion produces rich syntheses of user movements from touch logs, including estimates of motion, speed, accuracy, and effort. We assess the plausibility of generated movements by comparing against human data from a motion capture study and prior findings, and demonstrate Log2Motion in a large-scale dataset. Biomechanical motion synthesis provides a new way to understand log data, illuminating the ergonomics and motor control underlying touch interactions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7Log2Motion\u4ece\u89e6\u6478\u65e5\u5fd7\u5408\u6210\u751f\u7269\u529b\u5b66\u8fd0\u52a8\uff0c\u4e3a\u7406\u89e3\u89e6\u6478\u4ea4\u4e92\u63d0\u4f9b\u65b0\u89c1\u89e3\u3002", "motivation": "\u5c3d\u7ba1\u5927\u89c4\u6a21\u6536\u96c6\u4e86\u79fb\u52a8\u8bbe\u5907\u7684\u89e6\u6478\u6570\u636e\uff0c\u4f46\u5bf9\u5176\u4ea7\u751f\u7684\u4ea4\u4e92\u5374\u77e5\u4e4b\u751a\u5c11\uff0c\u9700\u901a\u8fc7\u65b0\u7684\u8ba1\u7b97\u95ee\u9898\u6765\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u808c\u8089\u9aa8\u9abc\u524d\u5411\u4eff\u771f\uff0c\u7ed3\u5408\u8f6f\u4ef6\u4eff\u771f\u5668\u4e0e\u7269\u7406\u4eff\u771f\u5668\uff0c\u751f\u6210\u7b26\u5408\u89e6\u6478\u65e5\u5fd7\u4e8b\u4ef6\u7684\u751f\u7269\u529b\u5b66\u8fd0\u52a8\u5e8f\u5217\u3002", "result": "\u5f00\u53d1\u4e86Log2Motion\u7cfb\u7edf\uff0c\u80fd\u591f\u4ece\u89e6\u6478\u65e5\u5fd7\u4e2d\u5408\u6210\u7528\u6237\u8fd0\u52a8\u7684\u4e30\u5bcc\u5408\u6210\u6570\u636e\uff0c\u5305\u62ec\u8fd0\u52a8\u3001\u901f\u5ea6\u3001\u51c6\u786e\u6027\u548c\u52aa\u529b\u7b49\u4f30\u8ba1\u503c\u3002", "conclusion": "Log2Motion\u4e3a\u7406\u89e3\u65e5\u5fd7\u6570\u636e\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u63ed\u793a\u4e86\u89e6\u6478\u4ea4\u4e92\u80cc\u540e\u7684\u751f\u7269\u529b\u5b66\u548c\u8fd0\u52a8\u63a7\u5236\u3002"}}
{"id": "2601.21173", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.21173", "abs": "https://arxiv.org/abs/2601.21173", "authors": ["Zeyi Liu", "Shuang Liu", "Jihai Min", "Zhaoheng Zhang", "Jun Cen", "Pengyu Han", "Songqiao Hu", "Zihan Meng", "Xiao He", "Donghua Zhou"], "title": "InspecSafe-V1: A Multimodal Benchmark for Safety Assessment in Industrial Inspection Scenarios", "comment": "15 pages, 7 figures", "summary": "With the rapid development of industrial intelligence and unmanned inspection, reliable perception and safety assessment for AI systems in complex and dynamic industrial sites has become a key bottleneck for deploying predictive maintenance and autonomous inspection. Most public datasets remain limited by simulated data sources, single-modality sensing, or the absence of fine-grained object-level annotations, which prevents robust scene understanding and multimodal safety reasoning for industrial foundation models. To address these limitations, InspecSafe-V1 is released as the first multimodal benchmark dataset for industrial inspection safety assessment that is collected from routine operations of real inspection robots in real-world environments. InspecSafe-V1 covers five representative industrial scenarios, including tunnels, power facilities, sintering equipment, oil and gas petrochemical plants, and coal conveyor trestles. The dataset is constructed from 41 wheeled and rail-mounted inspection robots operating at 2,239 valid inspection sites, yielding 5,013 inspection instances. For each instance, pixel-level segmentation annotations are provided for key objects in visible-spectrum images. In addition, a semantic scene description and a corresponding safety level label are provided according to practical inspection tasks. Seven synchronized sensing modalities are further included, including infrared video, audio, depth point clouds, radar point clouds, gas measurements, temperature, and humidity, to support multimodal anomaly recognition, cross-modal fusion, and comprehensive safety assessment in industrial environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u65b0\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6 InspecSafe-V1\uff0c\u65e8\u5728\u6539\u5584 AI \u7cfb\u7edf\u5728\u590d\u6742\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u53ef\u9760\u611f\u77e5\u548c\u5b89\u5168\u8bc4\u4f30\uff0c\u6db5\u76d6\u591a\u4e2a\u5de5\u4e1a\u573a\u666f\uff0c\u5e76\u63d0\u4f9b\u591a\u79cd\u4f20\u611f\u5668\u6570\u636e\u3002", "motivation": "\u968f\u7740\u5de5\u4e1a\u667a\u80fd\u548c\u65e0\u4eba\u68c0\u9a8c\u7684\u8fc5\u901f\u53d1\u5c55\uff0c\u53ef\u9760\u7684\u611f\u77e5\u4e0e\u5b89\u5168\u8bc4\u4f30\u6210\u4e3a\u90e8\u7f72\u9884\u6d4b\u6027\u7ef4\u62a4\u548c\u81ea\u4e3b\u68c0\u67e5\u7684\u5173\u952e\u74f6\u9888\uff0c\u800c\u73b0\u6709\u6570\u636e\u96c6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u8fd9\u4fc3\u4f7f\u7814\u7a76\u8005\u5f00\u53d1 InspecSafe-V1\u3002", "method": "InspecSafe-V1 \u6570\u636e\u96c6\u6536\u96c6\u81ea\u771f\u5b9e\u5de5\u4e1a\u73af\u5883\u4e2d\uff0c\u878d\u5408\u591a\u79cd\u4f20\u611f\u5668\u6570\u636e\uff0c\u5305\u62ec\u7ea2\u5916\u89c6\u9891\u3001\u97f3\u9891\u3001\u6df1\u5ea6\u70b9\u4e91\u7b49\uff0c\u63d0\u4f9b\u50cf\u7d20\u7ea7\u5206\u5272\u6807\u6ce8\u548c\u8bed\u4e49\u573a\u666f\u63cf\u8ff0\u3002", "result": "\u6570\u636e\u96c6\u6db5\u76d6\u4e94\u79cd\u5178\u578b\u5de5\u4e1a\u573a\u666f\uff0c\u5305\u542b\u6765\u81ea 2,239 \u4e2a\u6709\u6548\u68c0\u67e5\u7ad9\u70b9\u7684 5,013 \u6b21\u68c0\u67e5\u5b9e\u4f8b\uff0c\u652f\u6301\u4e86\u591a\u6a21\u6001\u5f02\u5e38\u8bc6\u522b\u548c\u5168\u9762\u5b89\u5168\u8bc4\u4f30\u7684\u7814\u7a76\u3002", "conclusion": "InspecSafe-V1 \u6570\u636e\u96c6\u4e3a\u5de5\u4e1a inspection \u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u4e86\u591a\u6a21\u6001\u57fa\u51c6\uff0c\u4fc3\u8fdb\u4e86\u590d\u6742\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u53ef\u9760\u611f\u77e5\u548c\u5b89\u5168\u8bc4\u4f30\uff0c\u652f\u6301\u4e86\u5148\u8fdb\u7684\u5f02\u5e38\u8bc6\u522b\u548c\u5b89\u5168\u8bc4\u4f30\u7814\u7a76\u3002"}}
{"id": "2601.21045", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.21045", "abs": "https://arxiv.org/abs/2601.21045", "authors": ["Kamrul Hasan", "Oleg V. Komogortsev"], "title": "Eye Feel You: A DenseNet-driven User State Prediction Approach", "comment": "10 pages, 1 figure, 4 tables", "summary": "Subjective self-reports, collected with eye-tracking data, reveal perceived states like fatigue, effort, and task difficulty. However, these reports are costly to collect and challenging to interpret consistently in longitudinal studies. In this work, we focus on determining whether objective gaze dynamics can reliably predict subjective reports across repeated recording rounds in the eye-tracking dataset. We formulate subjective-report prediction as a supervised regression problem and propose a DenseNet-based deep learning regressor that learns predictive representations from gaze velocity signals. We conduct two complementary experiments to clarify our aims. First, the cross-round generalization experiment tests whether models trained on earlier rounds transfer to later rounds, evaluating the models' ability to capture longitudinal changes. Second, cross-subject generalization tests models' robustness by predicting subjective outcomes for new individuals. These experiments aim to reduce reliance on hand-crafted feature designs and clarify which states of subjective experience systematically appear in oculomotor behavior over time.", "AI": {"tldr": "\u5229\u7528DenseNet\u6a21\u578b\u901a\u8fc7\u76ee\u5149\u52a8\u529b\u5b66\u6570\u636e\u51c6\u786e\u9884\u6d4b\u4e3b\u89c2\u72b6\u6001\uff0c\u51cf\u5c11\u5bf9\u81ea\u6211\u62a5\u544a\u7684\u4f9d\u8d56\u3002", "motivation": "\u73b0\u6709\u7684\u4e3b\u89c2\u81ea\u6211\u62a5\u544a\u6536\u96c6\u6210\u672c\u9ad8\u4e14\u89e3\u91ca\u96be\u5ea6\u5927\uff0c\u6025\u9700\u4e00\u79cd\u5ba2\u89c2\u7684\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u5c06\u4e3b\u89c2\u62a5\u544a\u9884\u6d4b\u95ee\u9898\u89c6\u4e3a\u76d1\u7763\u56de\u5f52\u95ee\u9898\uff0c\u63d0\u51faDenseNet\u6df1\u5ea6\u5b66\u4e60\u56de\u5f52\u5668\uff0c\u5e76\u8fdb\u884c\u8de8\u8f6e\u6b21\u548c\u8de8\u4e2a\u4f53\u7684\u6cdb\u5316\u5b9e\u9a8c\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDenseNet\u7684\u6df1\u5ea6\u5b66\u4e60\u56de\u5f52\u5668\uff0c\u901a\u8fc7\u76ee\u5149\u52a8\u529b\u5b66\u6570\u636e\u9884\u6d4b\u4e3b\u89c2\u81ea\u6211\u62a5\u544a\u7684\u611f\u77e5\u72b6\u6001\uff0c\u5982\u75b2\u52b3\u3001\u52aa\u529b\u548c\u4efb\u52a1\u96be\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u4e24\u9879\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u65f6\u95f4\u6bb5\u53ca\u4e0d\u540c\u4e2a\u4f53\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8868\u660e\u76ee\u5149\u52a8\u6001\u80fd\u591f\u53ef\u9760\u5730\u9884\u6d4b\u4e3b\u89c2\u611f\u53d7\u3002"}}
{"id": "2601.21188", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21188", "abs": "https://arxiv.org/abs/2601.21188", "authors": ["Hao Cheng", "Feitian Zhang"], "title": "Disturbance-Aware Flight Control of Robotic Gliding Blimp via Moving Mass Actuation", "comment": null, "summary": "Robotic blimps, as lighter-than-air (LTA) aerial systems, offer long endurance and inherently safe operation but remain highly susceptible to wind disturbances. Building on recent advances in moving mass actuation, this paper addresses the lack of disturbance-aware control frameworks for LTA platforms by explicitly modeling and compensating for wind-induced effects. A moving horizon estimator (MHE) infers real-time wind perturbations and provides these estimates to a model predictive controller (MPC), enabling robust trajectory and heading regulation under varying wind conditions. The proposed approach leverages a two-degree-of-freedom (2-DoF) moving-mass mechanism to generate both inertial and aerodynamic moments for attitude and heading control, thereby enhancing flight stability in disturbance-prone environments. Extensive flight experiments under headwind and crosswind conditions show that the integrated MHE-MPC framework significantly outperforms baseline PID control, demonstrating its effectiveness for disturbance-aware LTA flight.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u79fb\u52a8\u89c6\u91ce\u4f30\u8ba1\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u63a7\u5236\u6846\u67b6\uff0c\u4ee5\u589e\u5f3a\u673a\u5668\u4eba\u98de\u8247\u5728\u98ce\u5e72\u6270\u4e0b\u7684\u98de\u884c\u7a33\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u81f4\u529b\u4e8e\u89e3\u51b3\u73b0\u6709\u8f7b\u4e8e air (LTA) \u5e73\u53f0\u7f3a\u4e4f\u5bf9\u98ce\u6270\u52a8\u7684\u63a7\u5236\u6846\u67b6\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u79fb\u52a8\u89c6\u91ce\u4f30\u8ba1\u5668\uff08MHE\uff09\u63a8\u65ad\u5b9e\u65f6\u98ce\u6270\u52a8\uff0c\u5e76\u5c06\u8fd9\u4e9b\u4f30\u8ba1\u503c\u63d0\u4f9b\u7ed9\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\uff08MPC\uff09\u3002", "result": "\u901a\u8fc7\u63d0\u51fa\u7684MHE-MPC\u6846\u67b6\uff0c\u5728\u4e0d\u540c\u98ce\u51b5\u4e0b\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u8f68\u8ff9\u548c\u822a\u5411\u8c03\u8282\u3002", "conclusion": "\u91c7\u75282\u81ea\u7531\u5ea6\u79fb\u52a8\u8d28\u5fc3\u673a\u5236\u751f\u6210\u60ef\u6027\u548c\u6c14\u52a8\u529b\u77e9\uff0c\u63d0\u9ad8\u4e86\u5728\u5e72\u6270\u73af\u5883\u4e2d\u7684\u98de\u884c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2601.21057", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.21057", "abs": "https://arxiv.org/abs/2601.21057", "authors": ["Kamrul Hasan", "Oleg V. Komogortsev"], "title": "Privatization of Synthetic Gaze: Attenuating State Signatures in Diffusion-Generated Eye Movements", "comment": "9 pages, 4 figures", "summary": "The recent success of deep learning (DL) has enabled the generation of high-quality synthetic gaze data. However, such data also raises privacy concerns because gaze sequences can encode subjects' internal states, like fatigue, emotional load, or stress. Ideally, synthetic gaze should preserve the signal quality of real recordings and remove or attenuate state-related, privacy-sensitive attributes. Many recent DL-based generative models focus on replicating real gaze trajectories and do not explicitly consider subjective reports or the privatization of internal states. However, in this work, we consider a recent diffusion-based gaze synthesis approach and examine correlations between synthetic gaze features and subjective reports (e.g., fatigue and related self-reported states). Our result shows that these correlations are trivial, which suggests the generative approach suppresses state-related features. Moreover, synthetic gaze preserves necessary signal characteristics similar to those of real data, which supports its use for privacy-preserving gaze-based applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4e00\u79cd\u6269\u6563\u5f0f\u773c\u52a8\u5408\u6210\u65b9\u6cd5\uff0c\u53d1\u73b0\u5176\u751f\u6210\u7684\u773c\u52a8\u6570\u636e\u4e0e\u4e3b\u89c2\u72b6\u6001\u7684\u76f8\u5173\u6027\u5fae\u5f31\uff0c\u8868\u660e\u6709\u6548\u6291\u5236\u4e86\u4e0e\u72b6\u6001\u76f8\u5173\u7684\u79c1\u5bc6\u5c5e\u6027\uff0c\u5e76\u4fdd\u7559\u4e86\u5fc5\u8981\u7684\u4fe1\u53f7\u7279\u6027\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u7684\u6210\u529f\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u773c\u52a8\u6570\u636e\u6210\u4e3a\u53ef\u80fd\uff0c\u4f46\u8fd9\u4e9b\u6570\u636e\u5f15\u53d1\u4e86\u9690\u79c1\u95ee\u9898\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u5982\u4f55\u5728\u4ea7\u751f\u773c\u52a8\u6570\u636e\u65f6\u4fdd\u7559\u4fe1\u53f7\u8d28\u91cf\u540c\u65f6\u53bb\u9664\u72b6\u6001\u76f8\u5173\u7684\u654f\u611f\u5c5e\u6027\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u4e86\u57fa\u4e8e\u6269\u6563\u7684\u773c\u52a8\u5408\u6210\u65b9\u6cd5\uff0c\u5206\u6790\u5408\u6210\u773c\u52a8\u7279\u5f81\u4e0e\u4e3b\u89c2\u62a5\u544a\uff08\u5982\u75b2\u52b3\uff09\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002", "result": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u6240\u91c7\u7528\u7684\u6269\u6563\u5f0f\u773c\u52a8\u5408\u6210\u65b9\u6cd5\u4f7f\u5f97\u5408\u6210\u7684\u773c\u52a8\u6570\u636e\u4e0e\u4e3b\u89c2\u62a5\u544a\uff08\u5982\u75b2\u52b3\uff09\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u5fae\u4e0d\u8db3\u9053\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u6709\u6548\u6291\u5236\u4e86\u4e0e\u72b6\u6001\u76f8\u5173\u7684\u7279\u5f81\u3002\u540c\u65f6\uff0c\u5408\u6210\u773c\u52a8\u6570\u636e\u4fdd\u7559\u4e86\u4e0e\u771f\u5b9e\u6570\u636e\u76f8\u4f3c\u7684\u5fc5\u8981\u4fe1\u53f7\u7279\u6027\uff0c\u652f\u6301\u5176\u5728\u9690\u79c1\u4fdd\u62a4\u7684\u773c\u52a8\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u6269\u6563\u5f0f\u773c\u52a8\u5408\u6210\u6709\u6548\u6291\u5236\u4e86\u4e2a\u4f53\u72b6\u6001\u76f8\u5173\u7684\u7279\u5f81\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u4e0e\u771f\u5b9e\u6570\u636e\u76f8\u4f3c\u7684\u4fe1\u53f7\u7279\u6027\uff0c\u9002\u5408\u7528\u4e8e\u9690\u79c1\u4fdd\u62a4\u7684\u5e94\u7528\u3002"}}
{"id": "2601.21251", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21251", "abs": "https://arxiv.org/abs/2601.21251", "authors": ["Ce Hao", "Xuanran Zhai", "Yaohua Liu", "Harold Soh"], "title": "Abstracting Robot Manipulation Skills via Mixture-of-Experts Diffusion Policies", "comment": null, "summary": "Diffusion-based policies have recently shown strong results in robot manipulation, but their extension to multi-task scenarios is hindered by the high cost of scaling model size and demonstrations. We introduce Skill Mixture-of-Experts Policy (SMP), a diffusion-based mixture-of-experts policy that learns a compact orthogonal skill basis and uses sticky routing to compose actions from a small, task-relevant subset of experts at each step. A variational training objective supports this design, and adaptive expert activation at inference yields fast sampling without oversized backbones. We validate SMP in simulation and on a real dual-arm platform with multi-task learning and transfer learning tasks, where SMP achieves higher success rates and markedly lower inference cost than large diffusion baselines. These results indicate a practical path toward scalable, transferable multi-task manipulation: learn reusable skills once, activate only what is needed, and adapt quickly when tasks change.", "AI": {"tldr": "SMP\u662f\u4e00\u79cd\u65b0\u578b\u6269\u6563\u57fa\u6df7\u5408\u4e13\u5bb6\u7b56\u7565\uff0c\u901a\u8fc7\u5b66\u4e60\u7d27\u51d1\u7684\u6280\u80fd\u57fa\u7840\u548c\u5728\u63a8\u7406\u65f6\u7075\u6d3b\u6fc0\u6d3b\u4e13\u5bb6\uff0c\u5b9e\u73b0\u4e86\u591a\u4efb\u52a1 Robot \u64cd\u4f5c\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u6269\u6563\u57fa\u653f\u7b56\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u591a\u4efb\u52a1\u573a\u666f\u4e2d\u7684\u6269\u5c55\u53d7\u9650\u4e8e\u6a21\u578b\u89c4\u6a21\u548c\u6f14\u793a\u7684\u9ad8\u6210\u672c\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u6df7\u5408\u4e13\u5bb6\u7b56\u7565\uff08SMP\uff09\uff0c\u5b66\u4e60\u538b\u7f29\u7684\u6b63\u4ea4\u6280\u80fd\u57fa\u7840\uff0c\u5e76\u5728\u6bcf\u4e2a\u6b65\u9aa4\u4e2d\u4f7f\u7528\u7c98\u6027\u8def\u7531\u4ece\u4e00\u4e2a\u5c0f\u7684\u3001\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u4e13\u5bb6\u5b50\u96c6\u4e2d\u7ec4\u5408\u52a8\u4f5c\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u53cc\u81c2\u5e73\u53f0\u4e0a\u9a8c\u8bc1SMP\uff0c\u663e\u793a\u5176\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u8fc1\u79fb\u5b66\u4e60\u4efb\u52a1\u4e2d\u6bd4\u5927\u578b\u6269\u6563\u57fa\u7ebf\u6709\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u660e\u663e\u8f83\u4f4e\u7684\u63a8\u7406\u6210\u672c\u3002", "conclusion": "SMP\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u8fc1\u79fb\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u663e\u8457\u8f83\u4f4e\u7684\u63a8\u7406\u6210\u672c\uff0c\u663e\u793a\u4e86\u53ef\u6269\u5c55\u548c\u53ef\u8f6c\u79fb\u7684\u591a\u4efb\u52a1\u64cd\u4f5c\u7684\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2601.21141", "categories": ["cs.HC", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.21141", "abs": "https://arxiv.org/abs/2601.21141", "authors": ["Po-Hsun Chen", "Ivan C. H. Liu"], "title": "Optimization and Mobile Deployment for Anthropocene Neural Style Transfer", "comment": "7 pages, 11 figures, submitted to SIGGRAPH 2026", "summary": "This paper presents AnthropoCam, a mobile-based neural style transfer (NST) system optimized for the visual synthesis of Anthropocene environments. Unlike conventional artistic NST, which prioritizes painterly abstraction, stylizing human-altered landscapes demands a careful balance between amplifying material textures and preserving semantic legibility. Industrial infrastructures, waste accumulations, and modified ecosystems contain dense, repetitive patterns that are visually expressive yet highly susceptible to semantic erosion under aggressive style transfer.\n  To address this challenge, we systematically investigate the impact of NST parameter configurations on the visual translation of Anthropocene textures, including feature layer selection, style and content loss weighting, training stability, and output resolution. Through controlled experiments, we identify an optimal parameter manifold that maximizes stylistic expression while preventing semantic erasure. Our results demonstrate that appropriate combinations of convolutional depth, loss ratios, and resolution scaling enable the faithful transformation of anthropogenic material properties into a coherent visual language.\n  Building on these findings, we implement a low-latency, feed-forward NST pipeline deployed on mobile devices. The system integrates a React Native frontend with a Flask-based GPU backend, achieving high-resolution inference within 3-5 seconds on general mobile hardware. This enables real-time, in-situ visual intervention at the site of image capture, supporting participatory engagement with Anthropocene landscapes.\n  By coupling domain-specific NST optimization with mobile deployment, AnthropoCam reframes neural style transfer as a practical and expressive tool for real-time environmental visualization in the Anthropocene.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86AnthropoCam\uff0c\u4e00\u4e2a\u4e13\u4e3a\u4eba\u7c7b\u4e16\u73af\u5883\u7684\u89c6\u89c9\u5408\u6210\u4f18\u5316\u7684\u79fb\u52a8\u795e\u7ecf\u98ce\u683c\u8fc1\u79fb\u7cfb\u7edf\uff0c\u63a2\u8ba8\u4e86\u5176\u53c2\u6570\u914d\u7f6e\u5bf9\u89c6\u89c9\u8f6c\u6362\u7684\u5f71\u54cd\uff0c\u5e76\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u7684\u5b9e\u65f6\u5904\u7406\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u4eba\u7c7b\u4e16\u666f\u89c2\u7684\u590d\u6742\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u5177\u4f53\u9886\u57df\u4f18\u5316\u7684\u795e\u7ecf\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\uff0c\u4ee5\u4fbf\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u8fdb\u884c\u5b9e\u65f6\u7684\u89c6\u89c9\u5e72\u9884\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5730\u7814\u7a76NST\u53c2\u6570\u914d\u7f6e\uff08\u5982\u7279\u5f81\u5c42\u9009\u62e9\u3001\u635f\u5931\u6743\u91cd\u3001\u8bad\u7ec3\u7a33\u5b9a\u6027\u7b49\uff09\uff0c\u5f00\u5c55\u4e86\u63a7\u5236\u5b9e\u9a8c\uff0c\u8bc6\u522b\u4e86\u6700\u5927\u5316\u98ce\u683c\u8868\u8fbe\u7684\u6700\u4f73\u53c2\u6570\u7ec4\u5408\uff0c\u5e76\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u7684\u524d\u9988\u7ba1\u9053\u3002", "result": "\u901a\u8fc7\u5408\u9002\u7684\u5377\u79ef\u6df1\u5ea6\u3001\u635f\u5931\u6bd4\u4f8b\u548c\u5206\u8fa8\u7387\u7f29\u653e\u7684\u7ec4\u5408\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u4eba\u7c7b\u5bf9\u6750\u6599\u7279\u6027\u7684\u4fdd\u771f\u8f6c\u6362\uff0c\u5e76\u5728\u4e00\u822c\u79fb\u52a8\u786c\u4ef6\u4e0a\u5b9e\u73b0\u4e86\u6bcf3-5\u79d2\u7684\u9ad8\u5206\u8fa8\u7387\u63a8\u65ad\u3002", "conclusion": "AnthropoCam\u662f\u4e00\u4e2a\u6709\u6548\u7684\u79fb\u52a8\u795e\u7ecf\u98ce\u683c\u8fc1\u79fb\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u771f\u5b9e\u65f6\u95f4\u5185\u5b9e\u73b0\u5bf9\u4eba\u7c7b\u6539\u9020\u73af\u5883\u7684\u89c6\u89c9\u5408\u6210\uff0c\u4fc3\u8fdb\u5bf9\u4eba\u7c7b\u4e16\u666f\u89c2\u7684\u53c2\u4e0e\u6027\u4e92\u52a8\u3002"}}
{"id": "2601.21297", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.21297", "abs": "https://arxiv.org/abs/2601.21297", "authors": ["Byeongjun Kim", "H. Jin Kim"], "title": "Deep QP Safety Filter: Model-free Learning for Reachability-based Safety Filter", "comment": "Accepted at L4DC 2026", "summary": "We introduce Deep QP Safety Filter, a fully data-driven safety layer for black-box dynamical systems. Our method learns a Quadratic-Program (QP) safety filter without model knowledge by combining Hamilton-Jacobi (HJ) reachability with model-free learning. We construct contraction-based losses for both the safety value and its derivatives, and train two neural networks accordingly. In the exact setting, the learned critic converges to the viscosity solution (and its derivative), even for non-smooth values. Across diverse dynamical systems -- even including a hybrid system -- and multiple RL tasks, Deep QP Safety Filter substantially reduces pre-convergence failures while accelerating learning toward higher returns than strong baselines, offering a principled and practical route to safe, model-free control.", "AI": {"tldr": "\u63d0\u51faDeep QP\u5b89\u5168\u8fc7\u6ee4\u5668\uff0c\u901a\u8fc7\u65e0\u6a21\u578b\u5b66\u4e60\u548cH-J\u53ef\u8fbe\u6027\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u9ed1\u76d2\u52a8\u6001\u7cfb\u7edf\u7684\u5b89\u5168\u63a7\u5236\uff0c\u51cf\u5c11\u5931\u8d25\u5e76\u52a0\u5feb\u5b66\u4e60\u3002", "motivation": "\u5728\u65e0\u6a21\u578b\u52a8\u6001\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u5b89\u5168\u63a7\u5236\uff0c\u51cf\u5c11\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u98ce\u9669\u548c\u5931\u8d25\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u54c8\u5bc6\u987f-\u96c5\u53ef\u6bd4(HJ)\u53ef\u8fbe\u6027\u4e0e\u65e0\u6a21\u578b\u5b66\u4e60\uff0c\u5b66\u4e60\u4e8c\u6b21\u89c4\u5212(QP)\u5b89\u5168\u8fc7\u6ee4\u5668\uff0c\u6784\u5efa\u57fa\u4e8e\u6536\u7f29\u7684\u635f\u5931\u51fd\u6570\uff0c\u5206\u522b\u8bad\u7ec3\u4e24\u4e2a\u795e\u7ecf\u7f51\u7edc\u3002", "result": "\u5728\u5404\u79cd\u52a8\u6001\u7cfb\u7edf\uff08\u5305\u62ec\u6df7\u5408\u7cfb\u7edf\uff09\u548c\u591a\u4e2a\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0cDeep QP\u5b89\u5168\u8fc7\u6ee4\u5668\u663e\u8457\u51cf\u5c11\u4e86\u9884\u6536\u655b\u5931\u8d25\uff0c\u540c\u65f6\u52a0\u5feb\u4e86\u5b66\u4e60\u8fc7\u7a0b\uff0c\u8fbe\u5230\u4e86\u6bd4\u8f83\u5f3a\u57fa\u7ebf\u66f4\u9ad8\u7684\u6536\u76ca\u3002", "conclusion": "Deep QP\u5b89\u5168\u8fc7\u6ee4\u5668\u4e3a\u9ed1\u76d2\u52a8\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u5b9e\u7528\u7684\u5b89\u5168\u63a7\u5236\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u5b89\u5168\u548c\u9ad8\u6548\u7684\u65e0\u6a21\u578b\u63a7\u5236\u3002"}}
{"id": "2601.21264", "categories": ["cs.HC", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.21264", "abs": "https://arxiv.org/abs/2601.21264", "authors": ["Yoonsang Kim", "Swapnil Dey", "Arie Kaufman"], "title": "Evaluating Spatialized Auditory Cues for Rapid Attention Capture in XR", "comment": "8 pages, 4 figures. This is the author's version of the article that will appear at the IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (IEEE VRW) 2026", "summary": "In time-critical eXtended reality (XR) scenarios where users must rapidly reorient their attention to hazards, alerts, or instructions while engaged in a primary task, spatial audio can provide an immediate directional cue without occupying visual bandwidth. However, such scenarios can afford only a brief auditory exposure, requiring users to interpret sound direction quickly and without extended listening or head-driven refinement. This paper reports a controlled exploratory study of rapid spatial-audio localization in XR. Using HRTF-rendered broadband stimuli presented from a semi-dense set of directions around the listener, we quantify how accurately users can infer coarse direction from brief audio alone. We further examine the effects of short-term visuo-auditory feedback training as a lightweight calibration mechanism. Our findings show that brief spatial cues can convey coarse directional information, and that even short calibration can improve users' perception of aural signals. While these results highlight the potential of spatial audio for rapid attention guidance, they also show that auditory cues alone may not provide sufficient precision for complex or high-stakes tasks, and that spatial audio may be most effective when complemented by other sensory modalities or visual cues, without relying on head-driven refinement. We leverage this study on spatial audio as a preliminary investigation into a first-stage attention-guidance channel for wearable XR (e.g., VR head-mounted displays and AR smart glasses), and provide design insights on stimulus selection and calibration for time-critical use.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u65f6\u95f4\u7d27\u8feb\u7684XR\u573a\u666f\u4e2d\uff0c\u77ed\u65f6\u95f4\u7684\u7a7a\u95f4\u97f3\u9891\u5b9a\u4f4d\u5bf9\u7528\u6237\u7684\u5f15\u5bfc\u6548\u679c\uff0c\u5e76\u63d0\u51fa\u4e86\u8bbe\u8ba1\u5efa\u8bae\u3002", "motivation": "\u5728\u65f6\u95f4\u7d27\u8feb\u7684XR\u573a\u666f\u4e2d\uff0c\u7528\u6237\u9700\u8981\u8fc5\u901f\u8f6c\u79fb\u6ce8\u610f\u529b\uff0c\u7a7a\u95f4\u97f3\u9891\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u6709\u6548\u7684\u63d0\u793a\u65b9\u5f0f\u3002", "method": "\u901a\u8fc7HRTF\u6e32\u67d3\u7684\u5bbd\u5e26\u523a\u6fc0\uff0c\u4ece\u591a\u4e2a\u65b9\u5411\u5411\u542c\u4f17\u5448\u73b0\uff0c\u91cf\u5316\u7528\u6237\u5bf9\u77ed\u6682\u97f3\u9891\u7684\u65b9\u5411\u611f\u77e5\u80fd\u529b\uff0c\u5e76\u6d4b\u8bd5\u77ed\u671f\u89c6\u542c\u53cd\u9988\u8bad\u7ec3\u7684\u6548\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u7b80\u77ed\u7684\u7a7a\u95f4\u97f3\u9891\u7ebf\u7d22\u53ef\u4ee5\u4f20\u8fbe\u7c97\u7565\u7684\u65b9\u5411\u4fe1\u606f\uff0c\u77ed\u671f\u6821\u51c6\u53ef\u4ee5\u63d0\u9ad8\u7528\u6237\u5bf9\u58f0\u97f3\u4fe1\u53f7\u7684\u611f\u77e5\u3002", "conclusion": "\u7a7a\u95f4\u97f3\u9891\u53ef\u4ee5\u5feb\u901f\u5f15\u5bfc\u6ce8\u610f\u529b\uff0c\u4f46\u4ec5\u4f9d\u8d56\u542c\u89c9\u4fe1\u53f7\u7684\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u9700\u7ed3\u5408\u5176\u4ed6\u611f\u5b98\u4fe1\u606f\u3002"}}
{"id": "2601.21346", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21346", "abs": "https://arxiv.org/abs/2601.21346", "authors": ["Wei Zuo", "Chengyang Li", "Yikun Wang", "Bingyang Cheng", "Zeyi Ren", "Shuai Wang", "Derrick Wing Kwan Ng", "Yik-Chung Wu"], "title": "HPTune: Hierarchical Proactive Tuning for Collision-Free Model Predictive Control", "comment": "Accepted by IEEE ICASSP 2026", "summary": "Parameter tuning is a powerful approach to enhance adaptability in model predictive control (MPC) motion planners. However, existing methods typically operate in a myopic fashion that only evaluates executed actions, leading to inefficient parameter updates due to the sparsity of failure events (e.g., obstacle nearness or collision). To cope with this issue, we propose to extend evaluation from executed to non-executed actions, yielding a hierarchical proactive tuning (HPTune) framework that combines both a fast-level tuning and a slow-level tuning. The fast one adopts risk indicators of predictive closing speed and predictive proximity distance, and the slow one leverages an extended evaluation loss for closed-loop backpropagation. Additionally, we integrate HPTune with the Doppler LiDAR that provides obstacle velocities apart from position-only measurements for enhanced motion predictions, thus facilitating the implementation of HPTune. Extensive experiments on high-fidelity simulator demonstrate that HPTune achieves efficient MPC tuning and outperforms various baseline schemes in complex environments. It is found that HPTune enables situation-tailored motion planning by formulating a safe, agile collision avoidance strategy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684HPTune\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u5c55\u8bc4\u4f30\u672a\u6267\u884c\u7684\u52a8\u4f5c\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u4e2d\u7684\u53c2\u6570\u8c03\u4f18\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u53c2\u6570\u8c03\u4f18\u65b9\u6cd5\u5728\u8fd0\u52a8\u89c4\u5212\u4e2d\u5f80\u5f80\u53ea\u8bc4\u4f30\u6267\u884c\u8fc7\u7684\u52a8\u4f5c\uff0c\u5bfc\u81f4\u56e0\u5931\u8d25\u4e8b\u4ef6\u7a00\u5c11\u800c\u4ea7\u751f\u7684\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u7684\u4e3b\u52a8\u8c03\u4f18\u6846\u67b6\uff08HPTune\uff09\uff0c\u7ed3\u5408\u5feb\u901f\u548c\u6162\u901f\u8c03\u4f18\uff0c\u5176\u4e2d\u5feb\u901f\u8c03\u4f18\u91c7\u7528\u98ce\u9669\u6307\u6807\uff0c\u6162\u901f\u8c03\u4f18\u5229\u7528\u95ed\u73af\u53cd\u5411\u4f20\u64ad\u7684\u6269\u5c55\u8bc4\u4f30\u635f\u5931\u3002", "result": "HPTune\u5728\u9ad8\u4fdd\u771f\u6a21\u62df\u5668\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u6709\u6548\u5730\u5b9e\u73b0\u4e86MPC\u8c03\u4f18\uff0c\u5e76\u8d85\u8d8a\u4e86\u5404\u79cd\u57fa\u51c6\u65b9\u6848\u3002", "conclusion": "HPTune\u80fd\u591f\u5b9e\u73b0\u9762\u5411\u60c5\u5883\u7684\u8fd0\u52a8\u89c4\u5212\uff0c\u4e3a\u5b89\u5168\u3001\u7075\u6d3b\u7684\u907f\u969c\u7b56\u7565\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2601.21271", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.21271", "abs": "https://arxiv.org/abs/2601.21271", "authors": ["Tram Thi Minh Tran", "Soojeong Yoo", "Oliver Weidlich", "Yidan Cao", "Xinyan Yu", "Xin Cheng", "Yin Ye", "Natalia Gulbransen-Diaz", "Callum Parker"], "title": "Envisioning Audio Augmented Reality in Everyday Life", "comment": null, "summary": "While visual augmentation dominates the augmented reality landscape, devices like Meta Ray-Ban audio smart glasses signal growing industry movement toward audio augmented reality (AAR). Hearing is a primary channel for sensing context, anticipating change, and navigating social space, yet AAR's everyday potential remains underexplored. We address this gap through a collaborative autoethnography (N=5, authoring) and an online survey (N=74). We identify ten roles for AAR, grouped into three categories: task- and utility-oriented, emotional and social, and perceptual collaborator. These roles are further layered with a rhythmic and embodied collaborator framing, mapping them onto micro-, meso-, and macro-rhythms of everyday life. Our analysis surfaces nuanced tensions, such as blocking distractions without erasing social presence, highlighting the need for context-aware design. This paper contributes a foundational and forward-looking framework for AAR in everyday life, providing design groundwork for systems attuned to daily routines, sensory engagement, and social expectations.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u97f3\u9891\u589e\u5f3a\u73b0\u5b9e(AAR)\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u7684\u6f5c\u529b\uff0c\u8bc6\u522b\u4e86\u5341\u79cd\u89d2\u8272\uff0c\u5e76\u5f3a\u8c03\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9\u589e\u5f3a\u6280\u672f\u5df2\u4e3b\u5bfc\u589e\u5f3a\u73b0\u5b9e\u9886\u57df\uff0c\u4f46\u97f3\u9891\u589e\u5f3a\u73b0\u5b9e(AAR)\u7684\u6f5c\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u534f\u4f5c\u81ea\u6211\u6c11\u65cf\u5fd7(N=5, \u4f5c\u8005)\u548c\u5728\u7ebf\u8c03\u67e5(N=74)\u7684\u65b9\u6cd5\u8fdb\u884c\u7814\u7a76\u3002", "result": "\u8bc6\u522b\u4e86AAR\u7684\u5341\u79cd\u89d2\u8272\uff0c\u8fd9\u4e9b\u89d2\u8272\u5206\u4e3a\u4efb\u52a1\u548c\u6548\u7528\u5bfc\u5411\u3001\u60c5\u611f\u4e0e\u793e\u4f1a\u3001\u611f\u77e5\u534f\u4f5c\u8005\u4e09\u7c7b\uff0c\u5e76\u6620\u5c04\u5230\u65e5\u5e38\u751f\u6d3b\u7684\u5fae\u3001\u4e2d\u3001\u5b8f\u8282\u5f8b\u3002", "conclusion": "\u672c\u6587\u4e3a\u65e5\u5e38\u751f\u6d3b\u4e2d\u7684\u97f3\u9891\u589e\u5f3a\u73b0\u5b9e(AAR)\u63d0\u4f9b\u4e86\u57fa\u7840\u6027\u548c\u524d\u77bb\u6027\u7684\u6846\u67b6\uff0c\u5e76\u4e3a\u9002\u5e94\u65e5\u5e38\u751f\u6d3b\u3001\u611f\u5b98\u53c2\u4e0e\u548c\u793e\u4f1a\u671f\u671b\u7684\u7cfb\u7edf\u8bbe\u8ba1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.21363", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21363", "abs": "https://arxiv.org/abs/2601.21363", "authors": ["Weidong Huang", "Zhehan Li", "Hangxin Liu", "Biao Hou", "Yao Su", "Jingwen Zhang"], "title": "Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control", "comment": "ICLR 2026", "summary": "Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u7ed3\u5408SAC\u7684\u9ad8\u6548\u9884\u8bad\u7ec3\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u65b0\u73af\u5883\u4e2d\u7684\u9002\u5e94\u80fd\u529b\uff0c\u964d\u4f4e\u4e86\u63a2\u7d22\u98ce\u9669\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6574\u4f53\u6548\u7387\u3002", "motivation": "\u5728\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4e2d\uff0c\u4f20\u7edf\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u65b0\u7684\u73af\u5883\u4e2d\u9002\u5e94\u65f6\u6837\u672c\u6548\u7387\u4f4e\uff0c\u540c\u65f6\u5e0c\u671b\u63d0\u9ad8\u8fd9\u79cd\u9002\u5e94\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u504f\u5dee\u7684\u8f6f\u6f14\u5458-\u6279\u8bc4\u7b97\u6cd5\uff08SAC\uff09\u8fdb\u884c\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u8fdb\u884c\u9002\u5e94\u5fae\u8c03\u3002", "result": "\u901a\u8fc7\u91c7\u7528\u5927\u6279\u91cf\u66f4\u65b0\u548c\u9ad8\u6570\u636e\u66f4\u65b0\u6bd4\uff08UTD\uff09\uff0c\u6211\u4eec\u6210\u529f\u5b9e\u73b0\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u7b56\u7565\u7684\u96f6-shot \u90e8\u7f72\uff0c\u5e76\u5728\u65b0\u73af\u5883\u4e2d\u6709\u6548\u5fae\u8c03\u3002", "conclusion": "\u7406\u8bba\u4e0e\u5b9e\u8bc1\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\u589e\u5f3a\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u9002\u5e94\u65b0\u73af\u5883\u7684\u80fd\u529b\uff0c\u540c\u65f6\u8fd8\u4f18\u5316\u4e86\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u548c\u9ad8\u6548\u5fae\u8c03\u8fc7\u7a0b\u7684\u7ed3\u5408\u3002"}}
{"id": "2601.21460", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.21460", "abs": "https://arxiv.org/abs/2601.21460", "authors": ["Suifang Zhou", "Qi Gong", "Ximing Shen", "RAY LC"], "title": "Tell Me What I Missed: Tell Me What I Missed: Interacting with GPT during Recalling of One-Time Witnessed Events", "comment": "19 pages, 9 figures, CHI 2026", "summary": "LLM-assisted technologies are increasingly used to support cognitive processing and information interpretation, yet their role in aiding memory recall, and how people choose to engage with them, remains underexplored. We studied participants who watched a short robbery video (approximating a one-time eyewitness scenario) and composed recall statements using either a default GPT or a guided GPT prompted with a standardized eyewitness protocol. Results show that, in the default condition, participants who believed they had a clearer understanding of the event were more likely to trust GPT's output, whereas in the guided condition, participants showed stronger alignment between subjective clarity and actual recall. Additionally, participants evaluated the legitimacy of the individuals in the incident differently across conditions. Interaction analysis further revealed that default-GPT users spontaneously developed diverse strategies, including building on existing recollections, requesting potentially missing details, and treating GPT as a recall coach. This work shows how GPT-user interplay can subconsciously shape beliefs and perceptions of remembered events.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u4f7f\u7528GPT\u8f85\u52a9\u7684\u6280\u672f\u53ef\u4ee5\u5f71\u54cd\u8bb0\u5fc6\u56de\u5fc6\u7684\u4fe1\u4efb\u5ea6\u548c\u7b56\u7565\u9009\u62e9\u3002", "motivation": "\u63a2\u8ba8LLM\u8f85\u52a9\u6280\u672f\u5728\u8bb0\u5fc6\u56de\u5fc6\u4e2d\u7684\u4f5c\u7528\u53ca\u5176\u4f7f\u7528\u8005\u4e92\u52a8\u65b9\u5f0f\u3002", "method": "\u53c2\u4e0e\u8005\u89c2\u770b\u77ed\u89c6\u9891\u5e76\u4f7f\u7528\u9ed8\u8ba4GPT\u6216\u5f15\u5bfcGPT\u6784\u5efa\u56de\u5fc6\u9648\u8ff0\u3002", "result": "\u53d1\u73b0\u9ed8\u8ba4\u6761\u4ef6\u4e0b\uff0c\u53c2\u4e0e\u8005\u5bf9\u4e8b\u4ef6\u7406\u89e3\u7684\u4e3b\u89c2\u4fe1\u5fc3\u5f71\u54cd\u4ed6\u4eec\u5bf9GPT\u8f93\u51fa\u7684\u4fe1\u4efb\uff1b\u800c\u5728\u5f15\u5bfc\u6761\u4ef6\u4e0b\uff0c\u4e3b\u89c2\u6e05\u6670\u5ea6\u4e0e\u5b9e\u9645\u56de\u5fc6\u4e4b\u95f4\u7684\u5bf9\u9f50\u66f4\u5f3a\uff0c\u540c\u65f6\u4e0d\u540c\u6761\u4ef6\u4e0b\u53c2\u4e0e\u8005\u5bf9\u4e8b\u4ef6\u7684\u8bc4\u4ef7\u4e5f\u6709\u6240\u4e0d\u540c\u3002", "conclusion": "GPT\u4e0e\u7528\u6237\u7684\u4e92\u52a8\u53ef\u80fd\u4f1a\u6f5c\u5728\u5730\u5f71\u54cd\u4ed6\u4eec\u5bf9\u8bb0\u5fc6\u4e8b\u4ef6\u7684\u4fe1\u5ff5\u548c\u611f\u77e5\u3002"}}
{"id": "2601.21394", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21394", "abs": "https://arxiv.org/abs/2601.21394", "authors": ["Leonidas Askianakis", "Aleksandr Artemov"], "title": "Towards Space-Based Environmentally-Adaptive Grasping", "comment": null, "summary": "Robotic manipulation in unstructured environments requires reliable execution under diverse conditions, yet many state-of-the-art systems still struggle with high-dimensional action spaces, sparse rewards, and slow generalization beyond carefully curated training scenarios. We study these limitations through the example of grasping in space environments. We learn control policies directly in a learned latent manifold that fuses (grammarizes) multiple modalities into a structured representation for policy decision-making. Building on GPU-accelerated physics simulation, we instantiate a set of single-shot manipulation tasks and achieve over 95% task success with Soft Actor-Critic (SAC)-based reinforcement learning in less than 1M environment steps, under continuously varying grasping conditions from step 1. This empirically shows faster convergence than representative state-of-the-art visual baselines under the same open-loop single-shot conditions. Our analysis indicates that explicitly reasoning in latent space yields more sample-efficient learning and improved robustness to novel object and gripper geometries, environmental clutter, and sensor configurations compared to standard baselines. We identify remaining limitations and outline directions toward fully adaptive and generalizable grasping in the extreme conditions of space.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5728\u6f5c\u5728\u6d41\u5f62\u4e2d\u5b66\u4e60\u6293\u53d6\u63a7\u5236\u7b56\u7565\uff0c\u514b\u670d\u4e86\u9ad8\u7ef4\u52a8\u4f5c\u7a7a\u95f4\u548c\u7a00\u758f\u5956\u52b1\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u5728\u7a7a\u95f4\u73af\u5883\u4e2d\u9ad8\u6548\u7684\u5355\u6b21\u6293\u53d6\u3002", "motivation": "\u9488\u5bf9\u4f20\u7edf\u673a\u5668\u4eba\u64cd\u4f5c\u5728\u9ad8\u7ef4\u52a8\u4f5c\u7a7a\u95f4\u3001\u7a00\u758f\u5956\u52b1\u548c\u672a\u7ecf\u8fc7\u7cbe\u5fc3\u8bbe\u7f6e\u7684\u8bad\u7ec3\u60c5\u666f\u4e0b\u7f3a\u4e4f\u53ef\u9760\u6267\u884c\u7684\u95ee\u9898\u8fdb\u884c\u7814\u7a76\u3002", "method": "\u5728\u5b66\u4e60\u7684\u6f5c\u5728\u6d41\u5f62\u4e2d\u76f4\u63a5\u5b66\u4e60\u63a7\u5236\u7b56\u7565\uff0c\u4e0e\u57fa\u4e8eSAC\u7684\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u8fdb\u884c\u5355\u6b21\u6293\u53d6\u4efb\u52a1\u7684\u6a21\u62df\uff0c\u53d6\u5f97\u8d85\u8fc795%\u7684\u4efb\u52a1\u6210\u529f\u7387\u3002", "result": "\u5728\u4e0d\u65ad\u53d8\u5316\u7684\u6293\u53d6\u6761\u4ef6\u4e0b\uff0c\u901a\u8fc7GPU\u52a0\u901f\u7684\u7269\u7406\u4eff\u771f\u548c\u5355\u6b21\u64cd\u4f5c\u4efb\u52a1\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u8f83\u5feb\u7684\u6536\u655b\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u89c6\u89c9\u57fa\u7ebf\u3002", "conclusion": "\u660e\u786e\u63a8\u7406\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u53ef\u4ee5\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u6837\u672c\u5b66\u4e60\uff0c\u5e76\u63d0\u9ad8\u5bf9\u65b0\u7269\u4f53\u548c\u5939\u5177\u51e0\u4f55\u5f62\u72b6\u3001\u73af\u5883\u6df7\u4e71\u4ee5\u53ca\u4f20\u611f\u5668\u914d\u7f6e\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2601.21490", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.21490", "abs": "https://arxiv.org/abs/2601.21490", "authors": ["Fabian Albers", "Sebastian Strau\u00df", "Nikol Rummel", "Nils K\u00f6bis"], "title": "Are they just delegating? Cross-Sample Predictions on University Students' & Teachers' Use of AI", "comment": "27 pages, 5 figures", "summary": "Mutual trust between teachers and students is a prerequisite for effective teaching, learning, and assessment in higher education. Accurate predictions about the other group's use of generative artificial intelligence (AI) are fundamental for such trust. However, the disruptive rise of AI has transformed academic work practices, raising important questions about how teachers and students use these tools and how well they can estimate each other's usage. While the frequency of use is well studied, little is known about how AI is used, and comparisons with similar practices are rare. This study surveyed German university teachers (N = 113) and students (N = 123) on the frequency of AI use and the degree of delegation across six identical academic tasks. Participants also provided incentivized cross-sample predictions of the other group's AI use to assess the accuracy of their predictions. We find that students reported higher use of AI and greater delegation than teachers. Both groups significantly overestimated the other group's use, with teachers predicting very frequent use and high delegation by students, and students assuming teachers use AI similarly to themselves. These findings reveal a perception gap between teachers' and students' expectations and actual AI use. Such gaps may hinder trust and effective collaboration, underscoring the need for open dialogue about AI practices in academia and for policies that support the equitable and transparent integration of AI tools in higher education.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u6559\u5e08\u548c\u5b66\u751f\u5728\u4f7f\u7528\u751f\u6210\u6027\u4eba\u5de5\u667a\u80fd\u65b9\u9762\u5b58\u5728\u8ba4\u77e5\u5dee\u8ddd\uff0c\u5b66\u751f\u62a5\u544a\u7684\u4f7f\u7528\u9891\u7387\u548c\u59d4\u6258\u7a0b\u5ea6\u9ad8\u4e8e\u6559\u5e08\uff0c\u53cc\u65b9\u8fc7\u9ad8\u4f30\u8ba1\u4e86\u5bf9\u65b9\u7684AI\u4f7f\u7528\u60c5\u51b5\u3002", "motivation": "\u5728\u9ad8\u7b49\u6559\u80b2\u4e2d\uff0c\u6559\u5e08\u4e0e\u5b66\u751f\u4e4b\u95f4\u7684\u76f8\u4e92\u4fe1\u4efb\u662f\u6709\u6548\u6559\u5b66\u3001\u5b66\u4e60\u548c\u8bc4\u4f30\u7684\u5148\u51b3\u6761\u4ef6\u3002", "method": "\u672c\u7814\u7a76\u5bf9113\u540d\u5fb7\u56fd\u5927\u5b66\u6559\u5e08\u548c123\u540d\u5b66\u751f\u8fdb\u884c\u4e86\u8c03\u67e5\uff0c\u6db5\u76d6\u516d\u4e2a\u76f8\u540c\u7684\u5b66\u672f\u4efb\u52a1\uff0c\u8003\u5bdfAI\u4f7f\u7528\u7684\u9891\u7387\u53ca\u59d4\u6258\u7a0b\u5ea6\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5b66\u751f\u7684AI\u4f7f\u7528\u62a5\u544a\u548c\u59d4\u6258\u7a0b\u5ea6\u9ad8\u4e8e\u6559\u5e08\uff0c\u53cc\u65b9\u5747\u663e\u8457\u9ad8\u4f30\u4e86\u5bf9\u65b9\u7684AI\u4f7f\u7528\u60c5\u51b5\u3002", "conclusion": "\u6559\u5e08\u548c\u5b66\u751f\u4e4b\u95f4\u5bf9\u4eba\u5de5\u667a\u80fd(AI)\u4f7f\u7528\u7684\u8ba4\u77e5\u5dee\u8ddd\u53ef\u80fd\u4f1a\u963b\u788d\u4fe1\u4efb\u548c\u6709\u6548\u5408\u4f5c\uff0c\u56e0\u6b64\u9700\u8981\u5728\u5b66\u672f\u754c\u8fdb\u884c\u5f00\u653e\u7684\u5bf9\u8bdd\uff0c\u5e76\u5236\u5b9a\u652f\u6301\u516c\u5e73\u900f\u660e\u6574\u5408AI\u5de5\u5177\u7684\u653f\u7b56\u3002"}}
{"id": "2601.21409", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21409", "abs": "https://arxiv.org/abs/2601.21409", "authors": ["Weitao An", "Qi Liu", "Chenghao Xu", "Jiayi Chai", "Xu Yang", "Kun Wei", "Cheng Deng"], "title": "DSCD-Nav: Dual-Stance Cooperative Debate for Object Navigation", "comment": null, "summary": "Adaptive navigation in unfamiliar indoor environments is crucial for household service robots. Despite advances in zero-shot perception and reasoning from vision-language models, existing navigation systems still rely on single-pass scoring at the decision layer, leading to overconfident long-horizon errors and redundant exploration. To tackle these problems, we propose Dual-Stance Cooperative Debate Navigation (DSCD-Nav), a decision mechanism that replaces one-shot scoring with stance-based cross-checking and evidence-aware arbitration to improve action reliability under partial observability. Specifically, given the same observation and candidate action set, we explicitly construct two stances by conditioning the evaluation on diverse and complementary objectives: a Task-Scene Understanding (TSU) stance that prioritizes goal progress from scene-layout cues, and a Safety-Information Balancing (SIB) stance that emphasizes risk and information value. The stances conduct a cooperative debate and make policy by cross-checking their top candidates with cue-grounded arguments. Then, a Navigation Consensus Arbitration (NCA) agent is employed to consolidate both sides' reasons and evidence, optionally triggering lightweight micro-probing to verify uncertain choices, preserving NCA's primary intent while disambiguating. Experiments on HM3Dv1, HM3Dv2, and MP3D demonstrate consistent improvements in success and path efficiency while reducing exploration redundancy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDSCD-Nav\uff0c\u901a\u8fc7\u53cc\u7acb\u573a\u5408\u4f5c\u8fa9\u8bba\u673a\u5236\u6539\u5584\u5bb6\u5ead\u670d\u52a1\u673a\u5668\u4eba\u5728\u672a\u77e5\u5ba4\u5185\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u5bfc\u822a\u7cfb\u7edf\u5728\u51b3\u7b56\u5c42\u9762\u4f9d\u8d56\u5355\u6b21\u8bc4\u5206\uff0c\u5bfc\u81f4\u8fc7\u4e8e\u81ea\u4fe1\u7684\u957f\u65f6\u95f4\u9519\u8bef\u548c\u5197\u4f59\u7684\u63a2\u7d22\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u673a\u5236\u6765\u6539\u5584\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u7acb\u573a\u5408\u4f5c\u8fa9\u8bba\u5bfc\u822a\u673a\u5236 (DSCD-Nav)\uff0c\u53d6\u4ee3\u4e86\u4e00\u822c\u7684\u5355\u6b21\u8bc4\u5206\u65b9\u6cd5\uff0c\u4ee5\u7acb\u573a\u4e3a\u57fa\u7840\u7684\u4ea4\u53c9\u68c0\u67e5\u548c\u57fa\u4e8e\u8bc1\u636e\u7684\u4ef2\u88c1\uff0c\u63d0\u9ad8\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u60c5\u51b5\u4e0b\u7684\u52a8\u4f5c\u53ef\u9760\u6027\u3002", "result": "\u5728HM3Dv1\u3001HM3Dv2\u548cMP3D\u4e0a\u7684\u5b9e\u9a8c\u5c55\u793a\u4e86\u6210\u529f\u7387\u548c\u8def\u5f84\u6548\u7387\u7684\u4e00\u81f4\u6027\u6539\u5584\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u63a2\u7d22\u7684\u5197\u4f59\u3002", "conclusion": "\u5728\u4e0d\u719f\u6089\u7684\u5ba4\u5185\u73af\u5883\u4e2d\u5e94\u7528\u7684 Adaptive navigation \u6280\u672f\u786e\u5b9e\u6709\u6548\u63d0\u5347\u4e86\u5bb6\u5ead\u670d\u52a1\u673a\u5668\u4eba\u5728\u51b3\u7b56\u4e0e\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u80fd\u529b\u3002"}}
{"id": "2601.21492", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.21492", "abs": "https://arxiv.org/abs/2601.21492", "authors": ["Thomas Herrmann"], "title": "Organizational Practices and Socio-Technical Design of Human-Centered AI", "comment": "29 pages, 3 figures, 1 table, Published in: Wei Xu, (Ed.) 2026, Handbook of Human-Centered Artificial Intelligence, pp 1-45, Springe Nature Singapore", "summary": "This contribution explores how the integration of Artificial Intelligence (AI) into organizational practices can be effectively framed through a socio-technical perspective to comply with the requirements of Human-centered AI (HCAI). Instead of viewing AI merely as a technical tool, the analysis emphasizes the importance of embedding AI into communication, collaboration, and decision-making processes within organizations from a human-centered perspective. Ten case-based patterns illustrate how AI support of predictive maintenance can be organized to address quality assurance and continuous improvement and to provide different types of sup-port for HCAI. The analysis shows that AI adoption often requires and enables new forms of organizational learning, where specialists jointly interpret AI output, adapt workflows, and refine rules for system improve-ment. Different dimensions and levels of socio-technical integration of AI are considered to reflect the effort and benefits of keeping the organization in the loop.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u793e\u4f1a\u6280\u672f\u89c6\u89d2\u6709\u6548\u6574\u5408AI\uff0c\u4ee5\u652f\u6301\u4eba\u4e2d\u5fc3\u7684\u4eba\u5de5\u667a\u80fd\uff0c\u5f3a\u8c03\u7ec4\u7ec7\u5b66\u4e60\u4e0eAI\u8f93\u51fa\u7684\u5171\u540c\u89e3\u91ca\u548c\u5de5\u4f5c\u6d41\u7a0b\u7684\u9002\u5e94\u6027\u3002", "motivation": "\u63a2\u8ba8\u5982\u4f55\u4ece\u4eba\u672c\u89c6\u89d2\u6709\u6548\u6574\u5408AI\u4ee5\u6ee1\u8db3HCAI\u7684\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u6848\u4f8b\u5206\u6790\u63a2\u8ba8AI\u5728\u7ec4\u7ec7\u5b9e\u8df5\u4e2d\u7684\u793e\u4f1a\u6280\u672f\u6574\u5408\u3002", "result": "\u5c55\u793a\u4e86AI\u652f\u6301\u9884\u6d4b\u7ef4\u62a4\u7684\u5341\u79cd\u6848\u4f8b\u6a21\u5f0f\uff0c\u5f3a\u8c03\u4e86\u5176\u5728\u8d28\u91cf\u4fdd\u969c\u4e0e\u6301\u7eed\u6539\u8fdb\u4e2d\u7684\u7ec4\u7ec7\u4f5c\u7528\u3002", "conclusion": "AI\u7684\u91c7\u7528\u5728\u7ec4\u7ec7\u4e2d\u5e38\u5e38\u9700\u8981\u5e76\u4fc3\u8fdb\u65b0\u5f62\u5f0f\u7684\u7ec4\u7ec7\u5b66\u4e60\uff0c\u4ee5\u652f\u6301\u4eba\u4e2d\u5fc3\u7684\u4eba\u5de5\u667a\u80fd(HCAI)\u3002"}}
{"id": "2601.21413", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21413", "abs": "https://arxiv.org/abs/2601.21413", "authors": ["Andreas Mueller"], "title": "Singularity-Free Lie Group Integration and Geometrically Consistent Evaluation of Multibody System Models Described in Terms of Standard Absolute Coordinates", "comment": "10 pages", "summary": "A classical approach to the multibody systems (MBS) modeling is to use absolute coordinates, i.e., a set of (possibly redundant) coordinates that describe the absolute position and orientation of the individual bodies with respect to an inertial frame (IFR). A well-known problem for the time integration of the equations of motion (EOM) is the lack of a singularity-free parameterization of spatial motions, which is usually tackled by using unit quaternions. Lie group integration methods were proposed as an alternative approach to the singularity-free time integration. At the same time, Lie group formulations of EOM naturally respect the geometry of spatial motions during integration. Lie group integration methods, operating directly on the configuration space Lie group, are incompatible with standard formulations of the EOM, and cannot be implemented in existing MBS simulation codes without a major restructuring. The contribution of this paper is twofold: (1) A framework for interfacing Lie group integrators to standard EOM formulations is presented. It allows describing MBS in terms of various absolute coordinates and at the same using Lie group integration schemes. (2) A method for consistently incorporating the geometry of rigid body motions into the evaluation of EOM in absolute coordinates integrated with standard vector space integration schemes. The direct product group and the semidirect product group SO(3)xR3 and the semidirect product group SE(3) are used for representing rigid body motions. The key element is the local-global transitions (LGT) transition map, which facilitates the update of (global) absolute coordinates in terms of the (local) coordinates on the Lie group. This LGT map is specific to the absolute coordinates, the local coordinates on the Lie group, and the Lie group used to represent rigid body configurations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u4f53\u7cfb\u7edf\u5efa\u6a21\u6846\u67b6\uff0c\u7ed3\u5408\u4e86Lie\u7fa4\u79ef\u5206\u548c\u7edd\u5bf9\u5750\u6807\uff0c\u89e3\u51b3\u4e86\u65f6\u95f4\u79ef\u5206\u4e2d\u7684\u5947\u5f02\u6027\u95ee\u9898\uff0c\u5e76\u7ef4\u62a4\u4e86\u8fd0\u52a8\u7684\u51e0\u4f55\u7279\u6027\u3002", "motivation": "\u9488\u5bf9\u591a\u4f53\u7cfb\u7edf\u5efa\u6a21\u8fc7\u7a0b\u4e2d\u7684\u5947\u5f02\u6027\u95ee\u9898\uff0c\u5c1d\u8bd5\u4f7f\u7528Lie\u7fa4\u79ef\u5206\u65b9\u6cd5\u907f\u514d\u5e38\u89c4\u65f6\u95f4\u79ef\u5206\u4e2d\u7684\u5947\u5f02\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u7a7a\u95f4\u8fd0\u52a8\u7684\u51e0\u4f55\u7279\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u5c06Lie\u7fa4\u79ef\u5206\u5668\u4e0e\u6807\u51c6EOM\u516c\u5f0f\u63a5\u53e3\u76f8\u8fde\uff0c\u5e76\u5f15\u5165\u5c40\u90e8-\u5168\u7403\u8f6c\u79fb\uff08LGT\uff09\u6620\u5c04\uff0c\u4ee5\u4fbf\u5728\u7edd\u5bf9\u5750\u6807\u4e0eLie\u7fa4\u5c40\u90e8\u5750\u6807\u4e4b\u95f4\u8fdb\u884c\u6709\u6548\u66f4\u65b0\u3002", "result": "\u6210\u529f\u5efa\u7acb\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u7edd\u5bf9\u5750\u6807\u548cLie\u7fa4\u79ef\u5206\uff0c\u4f7f\u5f97\u591a\u4f53\u7cfb\u7edf\u53ef\u4ee5\u5728\u6807\u51c6\u8ba1\u7b97\u4ee3\u7801\u4e2d\u5b9e\u73b0\uff0c\u65e0\u9700\u91cd\u5927\u91cd\u6784\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u5c06Lie\u7fa4\u79ef\u5206\u5668\u4e0e\u6807\u51c6EOM\u516c\u5f0f\u8fde\u63a5\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u51e0\u4f55\u6027\u8d28\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u591a\u4f53\u7cfb\u7edf\u7684\u5efa\u6a21\u4e0e\u6a21\u62df\u3002"}}
{"id": "2601.21518", "categories": ["cs.HC", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.21518", "abs": "https://arxiv.org/abs/2601.21518", "authors": ["Xinyi Zhang", "Mamtaj Akter", "Heajun An", "Minqian Liu", "Qi Zhang", "Lifu Huang", "Jin-Hee Cho", "Pamela J. Wisniewski", "Sang Won Lee"], "title": "From Vulnerable to Resilient: Examining Parent and Teen Perceptions on How to Respond to Unwanted Cybergrooming Advances", "comment": null, "summary": "Cybergrooming is a form of online abuse that threatens teens' mental health and physical safety. Yet, most prior work has focused on detecting perpetrators' behaviors, leaving a limited understanding of how teens might respond to such unwanted advances. To address this gap, we conducted an online survey with 74 participants -- 51 parents and 23 teens -- who responded to simulated cybergrooming scenarios in two ways: responses that they think would make teens more vulnerable or resilient to unwanted sexual advances. Through a mixed-methods analysis, we identified four types of vulnerable responses (encouraging escalation, accepting an advance, displaying vulnerability, and negating risk concern) and four types of protective strategies (setting boundaries, directly declining, signaling risk awareness, and leveraging avoidance techniques). As the cybergrooming risk escalated, both vulnerable responses and protective strategies showed a corresponding progression. This study contributes a teen-centered understanding of cybergrooming, a labeled dataset, and a stage-based taxonomy of perceived protective strategies, while offering implications for educational programs and sociotechnical interventions.", "AI": {"tldr": "\u672c\u7814\u7a76\u5173\u6ce8\u9752\u5c11\u5e74\u5728\u7f51\u7edc grooming \u60c5\u5883\u4e0b\u7684\u53cd\u5e94\uff0c\u8bc6\u522b\u4e86\u8106\u5f31\u53cd\u5e94\u53ca\u4fdd\u62a4\u7b56\u7565\uff0c\u5e76\u63d0\u51fa\u76f8\u5e94\u7684\u6559\u80b2\u548c\u5e72\u9884\u5efa\u8bae\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u4ee5\u5f80\u7814\u7a76\u4e2d\u5bf9\u9752\u5c11\u5e74\u5728\u9762\u5bf9 cybergrooming \u65f6\u53cd\u5e94\u7684\u7406\u89e3\u7a7a\u767d\uff0c\u4ee5\u5e2e\u52a9\u5b9e\u65bd\u6709\u6548\u7684\u6559\u80b2\u4e0e\u5e72\u9884\u3002", "method": "\u901a\u8fc7\u5bf974\u540d\u53c2\u4e0e\u8005\u7684\u5728\u7ebf\u8c03\u67e5\uff0c\u6a21\u62df cybergrooming \u60c5\u666f\uff0c\u5e76\u5206\u6790\u4ed6\u4eec\u7684\u56de\u5e94\uff0c\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u7684\u5206\u6790\u65b9\u5f0f\u3002", "result": "\u8be5\u7814\u7a76\u901a\u8fc7\u8c03\u67e5\u53d1\u73b0\u9752\u5c11\u5e74\u5bf9\u7f51\u7edc grooming \u7684\u53cd\u5e94\uff0c\u8bc6\u522b\u51fa\u8106\u5f31\u53cd\u5e94\u4e0e\u4fdd\u62a4\u7b56\u7565\u7684\u4e0d\u540c\u7c7b\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u9752\u5c11\u5e74\u5728\u9762\u5bf9\u7f51\u7edc grooming \u65f6\u7684\u5e94\u5bf9\u673a\u5236\uff0c\u53ca\u6559\u80b2\u548c\u5e72\u9884\u7684\u5efa\u8bae\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9752\u5c11\u5e74\u5982\u4f55\u5e94\u5bf9\u7f51\u7edc grooming \u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u4e3a\u96c6\u4e2d\u89c6\u89d2\u7684\u7406\u89e3\uff0c\u5e76\u5f62\u6210\u4e86\u6807\u7b7e\u6570\u636e\u96c6\u4e0e\u5206\u9636\u6bb5\u4fdd\u62a4\u7b56\u7565\u5206\u7c7b\uff0c\u4ee5\u4fc3\u8fdb\u6559\u80b2\u9879\u76ee\u548c\u793e\u4f1a\u6280\u672f\u5e72\u9884\u3002"}}
{"id": "2601.21416", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21416", "abs": "https://arxiv.org/abs/2601.21416", "authors": ["Alexandre Chapin", "Bruno Machado", "Emmanuel Dellandr\u00e9a", "Liming Chen"], "title": "Spotlighting Task-Relevant Features: Object-Centric Representations for Better Generalization in Robotic Manipulation", "comment": null, "summary": "The generalization capabilities of robotic manipulation policies are heavily influenced by the choice of visual representations. Existing approaches typically rely on representations extracted from pre-trained encoders, using two dominant types of features: global features, which summarize an entire image via a single pooled vector, and dense features, which preserve a patch-wise embedding from the final encoder layer. While widely used, both feature types mix task-relevant and irrelevant information, leading to poor generalization under distribution shifts, such as changes in lighting, textures, or the presence of distractors. In this work, we explore an intermediate structured alternative: Slot-Based Object-Centric Representations (SBOCR), which group dense features into a finite set of object-like entities. This representation permits to naturally reduce the noise provided to the robotic manipulation policy while keeping enough information to efficiently perform the task. We benchmark a range of global and dense representations against intermediate slot-based representations, across a suite of simulated and real-world manipulation tasks ranging from simple to complex. We evaluate their generalization under diverse visual conditions, including changes in lighting, texture, and the presence of distractors. Our findings reveal that SBOCR-based policies outperform dense and global representation-based policies in generalization settings, even without task-specific pretraining. These insights suggest that SBOCR is a promising direction for designing visual systems that generalize effectively in dynamic, real-world robotic environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e2d\u95f4\u63d2\u69fd\u57fa\u7279\u5f81\u8868\u793a(SBOCR)\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\uff0c\u53d1\u73b0\u5176\u80fd\u6709\u6548\u589e\u5f3a\u6a21\u578b\u5728\u591a\u53d8\u89c6\u89c9\u6761\u4ef6\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7684\u7279\u5f81\u7c7b\u578b\u5c06\u4e0e\u4efb\u52a1\u65e0\u5173\u7684\u4fe1\u606f\u6df7\u5408\uff0c\u5bfc\u81f4\u5728\u5206\u5e03\u53d8\u5316\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u56e0\u6b64\u4e9f\u9700\u63a2\u7d22\u4e2d\u95f4\u7ed3\u6784\u7684\u7279\u5f81\u8868\u793a\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u4e0d\u540c\u7684\u5168\u5c40\u548c\u7a20\u5bc6\u7279\u5f81\u8868\u793a\uff0c\u8bc4\u4f30\u4e2d\u95f4\u63d2\u69fd\u57fa\u8868\u793a\u7684\u6709\u6548\u6027\uff0c\u6d89\u53ca\u7b80\u5355\u5230\u590d\u6742\u7684\u6a21\u62df\u548c\u73b0\u5b9e\u64cd\u4f5c\u4efb\u52a1\u3002", "result": "SBOCR\u7684\u7b56\u7565\u5728\u591a\u79cd\u89c6\u89c9\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5373\u4f7f\u6ca1\u6709\u4efb\u52a1\u7279\u5b9a\u7684\u9884\u8bad\u7ec3\u3002", "conclusion": "SBOCR\u57fa\u4e8e\u7684\u7b56\u7565\u5728\u4e0d\u540c\u89c6\u89c9\u6761\u4ef6\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u4f18\u4e8e\u57fa\u4e8e\u7a20\u5bc6\u548c\u5168\u5c40\u7279\u5f81\u7684\u7b56\u7565\uff0c\u8868\u660eSBOCR\u662f\u4e00\u79cd\u6709\u6548\u7684\u89c6\u89c9\u7cfb\u7edf\u8bbe\u8ba1\u65b9\u5411\u3002"}}
{"id": "2601.21650", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21650", "abs": "https://arxiv.org/abs/2601.21650", "authors": ["Alexander Erlei", "Federico Cau", "Radoslav Georgiev", "Sagar Kumar", "Kilian Bizer", "Ujwal Gadiraju"], "title": "When Life Gives You AI, Will You Turn It Into A Market for Lemons? Understanding How Information Asymmetries About AI System Capabilities Affect Market Outcomes and Adoption", "comment": null, "summary": "AI consumer markets are characterized by severe buyer-supplier market asymmetries. Complex AI systems can appear highly accurate while making costly errors or embedding hidden defects. While there have been regulatory efforts surrounding different forms of disclosure, large information gaps remain. This paper provides the first experimental evidence on the important role of information asymmetries and disclosure designs in shaping user adoption of AI systems. We systematically vary the density of low-quality AI systems and the depth of disclosure requirements in a simulated AI product market to gauge how people react to the risk of accidentally relying on a low-quality AI system. Then, we compare participants' choices to a rational Bayesian model, analyzing the degree to which partial information disclosure can improve AI adoption. Our results underscore the deleterious effects of information asymmetries on AI adoption, but also highlight the potential of partial disclosure designs to improve the overall efficiency of human decision-making.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u4fe1\u606f\u4e0d\u5bf9\u79f0\u53ca\u4fe1\u606f\u62ab\u9732\u8bbe\u8ba1\u5bf9AI\u7cfb\u7edf\u91c7\u7528\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u90e8\u5206\u4fe1\u606f\u62ab\u9732\u6709\u52a9\u4e8e\u6539\u5584\u4eba\u7c7b\u51b3\u7b56\u6548\u7387\u3002", "motivation": "AI\u6d88\u8d39\u5e02\u573a\u7684\u4e70\u5356\u53cc\u65b9\u5b58\u5728\u663e\u8457\u7684\u4fe1\u606f\u4e0d\u5bf9\u79f0\uff0c\u5c3d\u7ba1\u5df2\u6709\u76d1\u7ba1\u63aa\u65bd\uff0c\u4f46\u4fe1\u606f\u7f3a\u53e3\u4f9d\u7136\u5b58\u5728\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u53d8\u5316\u4f4e\u8d28\u91cfAI\u7cfb\u7edf\u7684\u5bc6\u5ea6\u548c\u4fe1\u606f\u62ab\u9732\u6df1\u5ea6\uff0c\u5728\u6a21\u62dfAI\u4ea7\u54c1\u5e02\u573a\u4e2d\u8fdb\u884c\u5b9e\u9a8c\uff0c\u89c2\u5bdf\u53c2\u4e0e\u8005\u5bf9\u4f4e\u8d28\u91cfAI\u7cfb\u7edf\u7684\u53cd\u5e94\u3002", "result": "\u53c2\u4e0e\u8005\u7684\u9009\u62e9\u4e0e\u7406\u6027\u8d1d\u53f6\u65af\u6a21\u578b\u7684\u6bd4\u8f83\u663e\u793a\uff0c\u90e8\u5206\u4fe1\u606f\u62ab\u9732\u53ef\u4ee5\u63d0\u9ad8AI\u7cfb\u7edf\u7684\u91c7\u7528\u7387\u3002", "conclusion": "\u4fe1\u606f\u4e0d\u5bf9\u79f0\u5bf9AI\u7684\u91c7\u7528\u6709\u8d1f\u9762\u5f71\u54cd\uff0c\u4f46\u7f3a\u4e4f\u900f\u660e\u5ea6\u7684\u4fe1\u606f\u62ab\u9732\u8bbe\u8ba1\u53ef\u4ee5\u6539\u5584\u4eba\u7c7b\u51b3\u7b56\u6548\u7387\u3002"}}
{"id": "2601.21449", "categories": ["cs.RO", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.21449", "abs": "https://arxiv.org/abs/2601.21449", "authors": ["Zeyu He", "Yuchang Zhang", "Yuanzhen Zhou", "Miao Tao", "Hengjie Li", "Yang Tian", "Jia Zeng", "Tai Wang", "Wenzhe Cai", "Yilun Chen", "Ning Gao", "Jiangmiao Pang"], "title": "Nimbus: A Unified Embodied Synthetic Data Generation Framework", "comment": null, "summary": "Scaling data volume and diversity is critical for generalizing embodied intelligence. While synthetic data generation offers a scalable alternative to expensive physical data acquisition, existing pipelines remain fragmented and task-specific. This isolation leads to significant engineering inefficiency and system instability, failing to support the sustained, high-throughput data generation required for foundation model training. To address these challenges, we present Nimbus, a unified synthetic data generation framework designed to integrate heterogeneous navigation and manipulation pipelines. Nimbus introduces a modular four-layer architecture featuring a decoupled execution model that separates trajectory planning, rendering, and storage into asynchronous stages. By implementing dynamic pipeline scheduling, global load balancing, distributed fault tolerance, and backend-specific rendering optimizations, the system maximizes resource utilization across CPU, GPU, and I/O resources. Our evaluation demonstrates that Nimbus achieves a 2-3X improvement in end-to-end throughput compared to unoptimized baselines and ensuring robust, long-term operation in large-scale distributed environments. This framework serves as the production backbone for the InternData suite, enabling seamless cross-domain data synthesis.", "AI": {"tldr": "Nimbus\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u7ec4\u5316\u8bbe\u8ba1\u548c\u4f18\u5316\uff0c\u63d0\u9ad8\u4e86\u6570\u636e\u751f\u6210\u6548\u7387\uff0c\u652f\u6301\u5927\u89c4\u6a21\u6570\u636e\u5408\u6210\u3002", "motivation": "\u968f\u7740\u6570\u636e\u91cf\u548c\u591a\u6837\u6027\u7684\u589e\u52a0\uff0c\u5408\u6210\u6570\u636e\u751f\u6210\u6210\u4e3a\u514b\u670d\u6602\u8d35\u7269\u7406\u6570\u636e\u83b7\u53d6\u7684\u5173\u952e\uff0c\u4f46\u73b0\u6709\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u6d41\u7a0b\u8fc7\u4e8e\u5206\u6563\u548c\u7279\u5b9a\u4efb\u52a1\uff0c\u5bfc\u81f4\u5de5\u7a0b\u6548\u7387\u4f4e\u4e0b\u3002", "method": "Nimbus\u91c7\u7528\u4e86\u7edf\u4e00\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u91c7\u7528\u6a21\u7ec4\u5316\u56db\u5c42\u67b6\u6784\uff0c\u5206\u79bb\u4e86\u8f68\u8ff9\u89c4\u5212\u3001\u6e32\u67d3\u548c\u5b58\u50a8\u7b49\u6d41\u7a0b\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u52a8\u6001\u7ba1\u9053\u8c03\u5ea6\u548c\u5168\u7403\u8d1f\u8f7d\u5e73\u8861\u7b49\u4f18\u5316\u3002", "result": "Nimbus\u76f8\u8f83\u4e8e\u672a\u4f18\u5316\u7684\u57fa\u7ebf\u5b9e\u73b0\u4e862-3\u500d\u7684\u7aef\u5230\u7aef\u541e\u5410\u91cf\u63d0\u5347\uff0c\u540c\u65f6\u5728\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u4fdd\u6301\u4e86\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "Nimbus\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u751f\u6210\u6548\u7387\uff0c\u5e76\u4e14\u5728\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u8868\u73b0\u7a33\u5065\uff0c\u4e3a\u8de8\u9886\u57df\u6570\u636e\u5408\u6210\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2601.21791", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.21791", "abs": "https://arxiv.org/abs/2601.21791", "authors": ["Valerie Tan", "Luisa Jost", "Jens Gerken", "Max Pascher"], "title": "Preliminary Results of a Scoping Review on Assistive Technologies for Adults with ADHD", "comment": "9 pages, 1 fvigure", "summary": "Attention Deficit Hyperactivity Disorder (ADHD), characterized by inattention, hyperactivity, and impulsivity, is prevalent in the adult population. Long perceived and treated as a childhood condition, ADHD and its characteristics nonetheless impact a significant portion of adults today. In contrast to children with ADHD, adults with ADHD face unique challenges in the workplace and in higher education. In this work-in-progress paper, we present a scoping review as a foundation to understand and explore existing technology-based approaches to support adults with ADHD. In total, our search returned 3,538 papers upon which we selected, based on PRISMA-ScR, a total of 46 papers for in-depth analysis. Our initial findings highlight that most papers take on a therapeutic or intervention perspective instead of a more positive support perspective. Our analysis also found a tremendous increase in recent papers on the topic, which highlights that more and more researchers are becoming aware of the need to address ADHD with adults. For the future, we aim to further analyze the corpus and identify research gaps and potentials for further development of ADHD assistive technologies.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u63ed\u793a\u6210\u4ebaADHD\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u6307\u51fa\u76ee\u524d\u5927\u591a\u6570\u5b66\u672f\u5de5\u4f5c\u96c6\u4e2d\u4e8e\u6cbb\u7597\u800c\u975e\u652f\u6301\uff0c\u672a\u6765\u9700\u5f3a\u8c03\u6280\u672f\u6027\u89e3\u51b3\u65b9\u6848\u7684\u53d1\u5c55\u3002", "motivation": "\u6210\u4ebaADHD\u867d\u7136\u957f\u4e45\u4ee5\u6765\u88ab\u89c6\u4e3a\u513f\u7ae5\u75be\u75c5\uff0c\u4f46\u5728\u804c\u573a\u548c\u9ad8\u7b49\u6559\u80b2\u4e2d\u5bf9\u6210\u5e74\u4eba\u4ea7\u751f\u4e86\u72ec\u7279\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u9488\u5bf9\u6210\u4eba\u7684\u652f\u6301\u6280\u672f\u3002", "method": "\u901a\u8fc7PRISMA-ScR\u65b9\u6cd5\u8fdb\u884c\u6587\u732e\u68c0\u7d22\u548c\u7b5b\u9009\uff0c\u6700\u7ec8\u5206\u6790\u4e8646\u7bc7\u76f8\u5173\u8bba\u6587\u3002", "result": "\u521d\u6b65\u53d1\u73b0\u8868\u660e\uff0c\u6709\u5173\u6210\u4ebaADHD\u7684\u7814\u7a76\u6b63\u5728\u589e\u52a0\uff0c\u4f46\u4ecd\u9700\u66f4\u591a\u5173\u6ce8\u79ef\u6781\u652f\u6301\u7684\u6280\u672f\u6027\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u672c\u7814\u7a76\u53d1\u73b0\uff0c\u9488\u5bf9\u6210\u4ebaADHD\u7684\u6280\u672f\u652f\u6301\u7814\u7a76\u5728\u6570\u91cf\u4e0a\u663e\u8457\u589e\u52a0\uff0c\u4f46\u5927\u591a\u6570\u6587\u732e\u4fa7\u91cd\u4e8e\u6cbb\u7597\u548c\u5e72\u9884\uff0c\u800c\u975e\u79ef\u6781\u652f\u6301\u7684\u65b9\u6cd5\u3002"}}
{"id": "2601.21454", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.21454", "abs": "https://arxiv.org/abs/2601.21454", "authors": ["Shanliang Yao", "Zhuoxiao Li", "Runwei Guan", "Kebin Cao", "Meng Xia", "Fuping Hu", "Sen Xu", "Yong Yue", "Xiaohui Zhu", "Weiping Ding", "Ryan Wen Liu"], "title": "4D-CAAL: 4D Radar-Camera Calibration and Auto-Labeling for Autonomous Driving", "comment": null, "summary": "4D radar has emerged as a critical sensor for autonomous driving, primarily due to its enhanced capabilities in elevation measurement and higher resolution compared to traditional 3D radar. Effective integration of 4D radar with cameras requires accurate extrinsic calibration, and the development of radar-based perception algorithms demands large-scale annotated datasets. However, existing calibration methods often employ separate targets optimized for either visual or radar modalities, complicating correspondence establishment. Furthermore, manually labeling sparse radar data is labor-intensive and unreliable. To address these challenges, we propose 4D-CAAL, a unified framework for 4D radar-camera calibration and auto-labeling. Our approach introduces a novel dual-purpose calibration target design, integrating a checkerboard pattern on the front surface for camera detection and a corner reflector at the center of the back surface for radar detection. We develop a robust correspondence matching algorithm that aligns the checkerboard center with the strongest radar reflection point, enabling accurate extrinsic calibration. Subsequently, we present an auto-labeling pipeline that leverages the calibrated sensor relationship to transfer annotations from camera-based segmentations to radar point clouds through geometric projection and multi-feature optimization. Extensive experiments demonstrate that our method achieves high calibration accuracy while significantly reducing manual annotation effort, thereby accelerating the development of robust multi-modal perception systems for autonomous driving.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa4D-CAAL\u6846\u67b6\uff0c\u89e3\u51b3\u96f7\u8fbe\u4e0e\u6444\u50cf\u5934\u7684\u6807\u5b9a\u4e0e\u81ea\u52a8\u6807\u6ce8\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u6807\u5b9a\u7cbe\u5ea6\uff0c\u964d\u4f4e\u4eba\u5de5\u6807\u6ce8\u5de5\u4f5c\u91cf\uff0c\u63a8\u52a8\u81ea\u4e3b\u9a7e\u9a76\u6280\u672f\u7684\u53d1\u5c55\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6807\u5b9a\u65b9\u6cd5\u4e2d\u5149\u5b66\u548c\u96f7\u8fbe\u6a21\u6001\u4e4b\u95f4\u7684\u76ee\u6807\u5206\u79bb\uff0c\u7b80\u5316\u5bf9\u5e94\u5173\u7cfb\u7684\u5efa\u7acb\uff0c\u540c\u65f6\u964d\u4f4e\u7a00\u758f\u96f7\u8fbe\u6570\u636e\u624b\u52a8\u6807\u6ce8\u7684\u52b3\u52a8\u5f3a\u5ea6\u548c\u4e0d\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u76844D\u96f7\u8fbe\u6444\u50cf\u5934\u6807\u5b9a\u548c\u81ea\u52a8\u6807\u6ce8\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u91cd\u7528\u9014\u6807\u5b9a\u76ee\u6807\u8bbe\u8ba1\uff0c\u5f00\u53d1\u4e86\u7a33\u5065\u7684\u5bf9\u5e94\u5339\u914d\u7b97\u6cd5\u548c\u81ea\u52a8\u6807\u6ce8 pipeline\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728\u6807\u5b9a\u7cbe\u5ea6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u51cf\u5c11\u4e86\u4eba\u5de5\u6ce8\u91ca\u7684\u9700\u6c42\uff0c\u4e3a\u591a\u6a21\u6001\u611f\u77e5\u7cfb\u7edf\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u52a0\u901f\u3002", "conclusion": "4D-CAAL\u6846\u67b6\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u76844D\u96f7\u8fbe\u4e0e\u6444\u50cf\u5934\u6807\u5b9a\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u5de5\u6807\u6ce8\u7684\u5de5\u4f5c\u91cf\uff0c\u4fc3\u8fdb\u4e86\u81ea\u4e3b\u9a7e\u9a76\u591a\u6a21\u6001\u611f\u77e5\u7cfb\u7edf\u7684\u5f00\u53d1\u3002"}}
{"id": "2601.21920", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.21920", "abs": "https://arxiv.org/abs/2601.21920", "authors": ["Upol Ehsan", "Samir Passi", "Koustuv Saha", "Todd McNutt", "Mark O. Riedl", "Sara Alcorn"], "title": "From Future of Work to Future of Workers: Addressing Asymptomatic AI Harms for Dignified Human-AI Interaction", "comment": null, "summary": "In the future of work discourse, AI is touted as the ultimate productivity amplifier. Yet, beneath the efficiency gains lie subtle erosions of human expertise and agency. This paper shifts focus from the future of work to the future of workers by navigating the AI-as-Amplifier Paradox: AI's dual role as enhancer and eroder, simultaneously strengthening performance while eroding underlying expertise. We present a year-long study on the longitudinal use of AI in a high-stakes workplace among cancer specialists. Initial operational gains hid ``intuition rust'': the gradual dulling of expert judgment. These asymptomatic effects evolved into chronic harms, such as skill atrophy and identity commoditization. Building on these findings, we offer a framework for dignified Human-AI interaction co-constructed with professional knowledge workers facing AI-induced skill erosion without traditional labor protections. The framework operationalizes sociotechnical immunity through dual-purpose mechanisms that serve institutional quality goals while building worker power to detect, contain, and recover from skill erosion, and preserve human identity. Evaluated across healthcare and software engineering, our work takes a foundational step toward dignified human-AI interaction futures by balancing productivity with the preservation of human expertise.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8AI\u5728\u5de5\u4f5c\u573a\u6240\u7684\u53cc\u91cd\u5f71\u54cd\uff0c\u63d0\u51fa\u6846\u67b6\u4ee5\u4fc3\u8fdb\u5c0a\u4e25\u7684\u4eba\u673a\u4e92\u52a8\uff0c\u65e8\u5728\u5e73\u8861\u751f\u4ea7\u529b\u4e0e\u4e13\u4e1a\u77e5\u8bc6\u7684\u4fdd\u5b58\u3002", "motivation": "\u63a2\u8ba8AI\u7684\u53cc\u91cd\u89d2\u8272\uff1a\u589e\u5f3a\u548c\u4fb5\u8680\uff0c\u540c\u65f6\u63d0\u5347\u8868\u73b0\u5374\u524a\u5f31\u57fa\u672c\u4e13\u4e1a\u77e5\u8bc6\u3002", "method": "\u8fdb\u884c\u4e86\u4e00\u5e74\u7684\u7814\u7a76\uff0c\u89c2\u5bdf\u764c\u75c7\u4e13\u5bb6\u5728\u9ad8\u98ce\u9669\u5de5\u4f5c\u73af\u5883\u4e2dAI\u7684\u957f\u671f\u4f7f\u7528\u3002", "result": "\u53d1\u73b0\u521d\u671f\u7684\u64cd\u4f5c\u589e\u76ca\u63a9\u76d6\u4e86\u4e13\u5bb6\u5224\u65ad\u7684\u9010\u6e10\u51cf\u5f31\uff0c\u957f\u671f\u540e\u679c\u8868\u73b0\u4e3a\u6280\u80fd\u840e\u7f29\u548c\u8eab\u4efd\u5546\u54c1\u5316\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u4fc3\u8fdb\u5c0a\u4e25\u7684\u4eba\u673a\u4e92\u52a8\uff0c\u5e73\u8861\u751f\u4ea7\u529b\u4e0e\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u7684\u4fdd\u5b58\u3002"}}
{"id": "2601.21474", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21474", "abs": "https://arxiv.org/abs/2601.21474", "authors": ["Xingyu Zhang", "Chaofan Zhang", "Boyue Zhang", "Zhinan Peng", "Shaowei Cui", "Shuo Wang"], "title": "DexTac: Learning Contact-aware Visuotactile Policies via Hand-by-hand Teaching", "comment": null, "summary": "For contact-intensive tasks, the ability to generate policies that produce comprehensive tactile-aware motions is essential. However, existing data collection and skill learning systems for dexterous manipulation often suffer from low-dimensional tactile information. To address this limitation, we propose DexTac, a visuo-tactile manipulation learning framework based on kinesthetic teaching. DexTac captures multi-dimensional tactile data-including contact force distributions and spatial contact regions-directly from human demonstrations. By integrating these rich tactile modalities into a policy network, the resulting contact-aware agent enables a dexterous hand to autonomously select and maintain optimal contact regions during complex interactions. We evaluate our framework on a challenging unimanual injection task. Experimental results demonstrate that DexTac achieves a 91.67% success rate. Notably, in high-precision scenarios involving small-scale syringes, our approach outperforms force-only baselines by 31.67%. These results underscore that learning multi-dimensional tactile priors from human demonstrations is critical for achieving robust, human-like dexterous manipulation in contact-rich environments.", "AI": {"tldr": "DexTac\u662f\u4e00\u4e2a\u57fa\u4e8e\u52a8\u89c9\u6559\u5b66\u7684\u89c6\u89c9-\u89e6\u89c9\u64cd\u63a7\u5b66\u4e60\u6846\u67b6\uff0c\u63d0\u5347\u4e86\u89e6\u89c9\u4fe1\u606f\u7684\u6355\u83b7\u53ca\u5728\u590d\u6742\u4e92\u52a8\u4e2d\u7684\u81ea\u6211\u4f18\u5316\u63a5\u89e6\u533a\u57df\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u7075\u5de7\u64cd\u4f5c\u6570\u636e\u6536\u96c6\u548c\u6280\u80fd\u5b66\u4e60\u7cfb\u7edf\u5b58\u5728\u89e6\u89c9\u4fe1\u606f\u7ef4\u5ea6\u4f4e\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u6709\u6548\u7684\u89e6\u6478\u611f\u77e5\u8fd0\u52a8\u751f\u6210\u3002", "method": "DexTac\u6846\u67b6\u901a\u8fc7\u52a8\u89c9\u6559\u5b66\u76f4\u63a5\u4ece\u4eba\u7c7b\u6f14\u793a\u6355\u83b7\u591a\u7ef4\u89e6\u89c9\u6570\u636e\uff0c\u5e76\u5c06\u8fd9\u4e9b\u6570\u636e\u6574\u5408\u8fdb\u7b56\u7565\u7f51\u7edc\u4e2d\u3002", "result": "DexTac\u5728\u5355\u624b\u6ce8\u5c04\u4efb\u52a1\u4e2d\u7684\u6210\u529f\u7387\u8fbe\u523091.67%\uff0c\u4e14\u5728\u9ad8\u7cbe\u5ea6\u5c0f\u89c4\u6a21\u6ce8\u5c04\u5668\u573a\u666f\u4e0b\uff0c\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u529b\u7684\u4fe1\u606f\u65b9\u684831.67%\u3002", "conclusion": "\u4ece\u4eba\u7c7b\u6f14\u793a\u4e2d\u5b66\u4e60\u591a\u7ef4\u89e6\u89c9\u4f18\u5148\u7ea7\u5bf9\u4e8e\u5728\u63a5\u89e6\u4e30\u5bcc\u73af\u5883\u4e2d\u5b9e\u73b0\u7a33\u5065\u4eba\u7c7b\u822c\u7075\u5de7\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2601.21965", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.21965", "abs": "https://arxiv.org/abs/2601.21965", "authors": ["Deeksha M. Shama", "Dimitra Emmanouilidou", "Ivan J. Tashev"], "title": "Cognitive Load Estimation Using Brain Foundation Models and Interpretability for BCIs", "comment": null, "summary": "Accurately monitoring cognitive load in real time is critical for Brain-Computer Interfaces (BCIs) that adapt to user engagement and support personalized learning. Electroencephalography (EEG) offers a non-invasive, cost-effective modality for capturing neural activity, though traditional methods often struggle with cross-subject variability and task-specific preprocessing. We propose leveraging Brain Foundation Models (BFMs), large pre-trained neural networks, to extract generalizable EEG features for cognitive load estimation. We adapt BFMs for long-term EEG monitoring and show that fine-tuning a small subset of layers yields improved accuracy over the state-of-the-art. Despite their scale, BFMs allow for real-time inference with a longer context window. To address often-overlooked interpretability challenges, we apply Partition SHAP (SHapley Additive exPlanations) to quantify feature importance. Our findings reveal consistent emphasis on prefrontal regions linked to cognitive control, while longitudinal trends suggest learning progression. These results position BFMs as efficient and interpretable tools for continuous cognitive load monitoring in real-world BCIs.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u5c06\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\uff08BFMs\uff09\u5e94\u7528\u4e8e\u8111\u7535\u56fe\uff08EEG\uff09\u6570\u636e\uff0c\u4ee5\u63d0\u9ad8\u8ba4\u77e5\u8d1f\u8377\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u51c6\u786e\u76d1\u6d4b\u8ba4\u77e5\u8d1f\u8377\u5bf9\u9002\u5e94\u7528\u6237\u53c2\u4e0e\u5ea6\u7684\u8111\u673a\u63a5\u53e3\uff08BCIs\uff09\u548c\u4e2a\u6027\u5316\u5b66\u4e60\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5229\u7528\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7684\u8111\u6a21\u578b\uff08BFMs\uff09\uff0c\u5bf9EEG\u6570\u636e\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u5bf9\u5c11\u91cf\u5c42\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u63d0\u5347\u8ba4\u77e5\u8d1f\u8377\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "result": "BFMs\u5728\u5b9e\u65f6\u63a8\u7406\u4e2d\u8868\u73b0\u51fa\u957f\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u5e76\u63ed\u793a\u4e86\u4e0e\u8ba4\u77e5\u63a7\u5236\u76f8\u5173\u7684\u524d\u989d\u53f6\u533a\u57df\u7684\u91cd\u8981\u6027\u53ca\u5b66\u4e60\u8fdb\u5c55\u7684\u957f\u671f\u8d8b\u52bf\u3002", "conclusion": "BFMs\u4e3a\u5b9e\u65f6\u8ba4\u77e5\u8d1f\u8377\u76d1\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u548c\u53ef\u89e3\u91ca\u7684\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u4e2d\u3002"}}
{"id": "2601.21504", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21504", "abs": "https://arxiv.org/abs/2601.21504", "authors": ["Anna Rothenh\u00e4usler", "Markus Mazzola", "Andreas Look", "Raghu Rajan", "Joschka B\u00f6decker"], "title": "Don't double it: Efficient Agent Prediction in Occlusions", "comment": null, "summary": "Occluded traffic agents pose a significant challenge for autonomous vehicles, as hidden pedestrians or vehicles can appear unexpectedly, yet this problem remains understudied. Existing learning-based methods, while capable of inferring the presence of hidden agents, often produce redundant occupancy predictions where a single agent is identified multiple times. This issue complicates downstream planning and increases computational load. To address this, we introduce MatchInformer, a novel transformer-based approach that builds on the state-of-the-art SceneInformer architecture. Our method improves upon prior work by integrating Hungarian Matching, a state-of-the-art object matching algorithm from object detection, into the training process to enforce a one-to-one correspondence between predictions and ground truth, thereby reducing redundancy. We further refine trajectory forecasts by decoupling an agent's heading from its motion, a strategy that improves the accuracy and interpretability of predicted paths. To better handle class imbalances, we propose using the Matthews Correlation Coefficient (MCC) to evaluate occupancy predictions. By considering all entries in the confusion matrix, MCC provides a robust measure even in sparse or imbalanced scenarios. Experiments on the Waymo Open Motion Dataset demonstrate that our approach improves reasoning about occluded regions and produces more accurate trajectory forecasts than prior methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7684MatchInformer\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u5308\u7259\u5229\u5339\u914d\u548c\u89e3\u8026\u4ee3\u7406\u671d\u5411\u4e0e\u8fd0\u52a8\uff0c\u80fd\u6539\u5584\u88ab\u906e\u6321\u4ea4\u901a\u53c2\u4e0e\u8005\u7684\u8bc6\u522b\u548c\u8f68\u8ff9\u9884\u6d4b\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u8bc6\u522b\u88ab\u906e\u6321\u7684\u884c\u4eba\u6216\u8f66\u8f86\u65f6\u6240\u9762\u4e34\u7684\u6311\u6218\uff0c\u51cf\u5c11\u5197\u4f59\u5360\u7528\u9884\u6d4b\u4ee5\u964d\u4f4e\u8ba1\u7b97\u8d1f\u62c5\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u53d8\u538b\u5668\u7684MatchInformer\u65b9\u6cd5\uff0c\u7ed3\u5408\u5308\u7259\u5229\u5339\u914d\u7b97\u6cd5\u8fdb\u884c\u8bad\u7ec3\uff0c\u51cf\u5c11\u5197\u4f59\uff0c\u5e76\u5c06\u4ee3\u7406\u7684\u671d\u5411\u4e0e\u8fd0\u52a8\u89e3\u8026\u4ee5\u6539\u8fdb\u8f68\u8ff9\u9884\u6d4b\u3002", "result": "\u5728Waymo\u5f00\u653e\u8fd0\u52a8\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMatchInformer\u5728\u63a8\u7406\u88ab\u906e\u6321\u533a\u57df\u548c\u8f68\u8ff9\u9884\u6d4b\u7684\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MatchInformer\u65b9\u6cd5\u901a\u8fc7\u6574\u5408\u5308\u7259\u5229\u5339\u914d\uff0c\u5e76\u63d0\u9ad8\u8f68\u8ff9\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u5904\u7406\u88ab\u906e\u6321\u4ea4\u901a\u53c2\u4e0e\u8005\u65f6\u7684\u6311\u6218\u3002"}}
{"id": "2601.21977", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.21977", "abs": "https://arxiv.org/abs/2601.21977", "authors": ["Javier Argota S\u00e1nchez-Vaquerizo", "Luis Borunda Monsivais"], "title": "From Particles to Agents: Hallucination as a Metric for Cognitive Friction in Spatial Simulation", "comment": "Paper selected for the workshop Human Cognition, AI, and the Future of HCI: Navigating the Disruptive and Wild Landscape of Large Language Models and Agentic AI as part of the Human-Computer Interaction (HCI) conference of the Alpine region (AlpCHI 2026) hosted at the Congressi Stefano Franscini, March 1st to March 5th, 2026 on Monte Verit\u00e0 in Ascona, Switzerland", "summary": "Traditional architectural simulations (e.g. Computational Fluid Dynamics, evacuation, structural analysis) model elements as deterministic physics-based \"particles\" rather than cognitive \"agents\". To bridge this, we introduce \\textbf{Agentic Environmental Simulations}, where Large Multimodal generative models actively predict the next state of spatial environments based on semantic expectation. Drawing on examples from accessibility-oriented AR pipelines and multimodal digital twins, we propose a shift from chronological time-steps to Episodic Spatial Reasoning, where simulations advance through meaningful, surprisal-triggered events. Within this framework we posit AI hallucinations as diagnostic tools. By formalizing the \\textbf{Cognitive Friction} ($C_f$) it is possible to reveal \"Phantom Affordances\", i.e. semiotic ambiguities in built space. Finally, we challenge current HCI paradigms by treating environments as dynamic cognitive partners and propose a human-centered framework of cognitive orchestration for designing AI-driven simulations that preserve autonomy, affective clarity, and cognitive integrity.", "AI": {"tldr": "\u63d0\u51faAgentic Environmental Simulations\uff0c\u7ed3\u5408\u5927\u578b\u591a\u6a21\u6001\u751f\u6210\u6a21\u578b\u4e0e\u60c5\u666f\u7a7a\u95f4\u63a8\u7406\uff0c\u6784\u5efa\u4ee5\u4eba\u7c7b\u4e3a\u4e2d\u5fc3\u7684AI\u9a71\u52a8\u4eff\u771f\u6846\u67b6\uff0c\u4ee5\u589e\u52a0\u7a7a\u95f4\u73af\u5883\u7684\u4e92\u52a8\u6027\u548c\u8ba4\u77e5\u6027\u3002", "motivation": "\u65e8\u5728\u5c06\u4f20\u7edf\u7684\u7269\u7406\u57fa\u7840\u4eff\u771f\u8f6c\u53d8\u4e3a\u4ee5\u4ee3\u7406\u4e3a\u57fa\u7840\u7684\u6a21\u62df\uff0c\u63d0\u5347\u7a7a\u95f4\u73af\u5883\u7684\u4e92\u52a8\u6027\u4e0e\u8ba4\u77e5\u6027\u3002", "method": "\u901a\u8fc7\u5f15\u5165Agentic Environmental Simulations\uff0c\u5229\u7528\u5927\u578b\u591a\u6a21\u6001\u751f\u6210\u6a21\u578b\u8fdb\u884c\u7a7a\u95f4\u73af\u5883\u7684\u4e0b\u4e00\u72b6\u6001\u9884\u6d4b\uff0c\u91c7\u7528\u4e8b\u4ef6\u9a71\u52a8\u7684\u60c5\u666f\u7a7a\u95f4\u63a8\u7406\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u5f62\u5f0f\u5316\u8ba4\u77e5\u6469\u64e6(C_f)\u4ee5\u63ed\u793a\u5efa\u7b51\u7a7a\u95f4\u4e2d\u7684\u201c\u5e7b\u5f71\u53ef\u4f9b\u6027\u201d\uff0c\u5e76\u63a2\u7d22\u5c06\u73af\u5883\u89c6\u4e3a\u52a8\u6001\u8ba4\u77e5\u4f19\u4f34\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u4eba\u7c7b\u4e3a\u4e2d\u5fc3\u7684\u8ba4\u77e5\u7f16\u6392\u6846\u67b6\uff0c\u7528\u4e8e\u8bbe\u8ba1\u4fdd\u7559\u81ea\u4e3b\u6027\u3001\u60c5\u611f\u6e05\u6670\u6027\u548c\u8ba4\u77e5\u5b8c\u6574\u6027\u7684AI\u9a71\u52a8\u4eff\u771f\u3002"}}
{"id": "2601.21506", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.21506", "abs": "https://arxiv.org/abs/2601.21506", "authors": ["Joonhee Lee", "Hyunseung Shin", "Jeonggil Ko"], "title": "IROS: A Dual-Process Architecture for Real-Time VLM-Based Indoor Navigation", "comment": null, "summary": "Indoor mobile robot navigation requires fast responsiveness and robust semantic understanding, yet existing methods struggle to provide both. Classical geometric approaches such as SLAM offer reliable localization but depend on detailed maps and cannot interpret human-targeted cues (e.g., signs, room numbers) essential for indoor reasoning. Vision-Language-Action (VLA) models introduce semantic grounding but remain strictly reactive, basing decisions only on visible frames and failing to anticipate unseen intersections or reason about distant textual cues. Vision-Language Models (VLMs) provide richer contextual inference but suffer from high computational latency, making them unsuitable for real-time operation on embedded platforms. In this work, we present IROS, a real-time navigation framework that combines VLM-level contextual reasoning with the efficiency of lightweight perceptual modules on low-cost, on-device hardware. Inspired by Dual Process Theory, IROS separates fast reflexive decisions (System One) from slow deliberative reasoning (System Two), invoking the VLM only when necessary. Furthermore, by augmenting compact VLMs with spatial and textual cues, IROS delivers robust, human-like navigation with minimal latency. Across five real-world buildings, IROS improves decision accuracy and reduces latency by 66% compared to continuous VLM-based navigation.", "AI": {"tldr": "IROS\u662f\u4e00\u4e2a\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u8f7b\u91cf\u611f\u77e5\u6a21\u5757\u7684\u5b9e\u65f6\u5ba4\u5185\u5bfc\u822a\u6846\u67b6\uff0c\u80fd\u591f\u9ad8\u6548\u5904\u7406\u590d\u6742\u7684\u8bed\u4e49\u7406\u89e3\u4e0e\u51b3\u7b56\u3002", "motivation": "\u73b0\u6709\u5ba4\u5185\u79fb\u52a8\u673a\u5668\u4eba\u5bfc\u822a\u65b9\u6cd5\u5728\u53cd\u5e94\u901f\u5ea6\u548c\u8bed\u4e49\u7406\u89e3\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u6765\u5e73\u8861\u8fd9\u4e24\u8005\u3002", "method": "IROS\u6846\u67b6\u57fa\u4e8e\u53cc\u8fc7\u7a0b\u7406\u8bba\uff0c\u5c06\u5feb\u901f\u53cd\u5e94\u51b3\u7b56\u4e0e\u6162\u901f\u6df1\u601d\u719f\u8651\u63a8\u7406\u5206\u5f00\uff0c\u4ec5\u5728\u5fc5\u8981\u65f6\u8c03\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u540c\u65f6\u589e\u5f3a\u7d27\u51d1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u7a7a\u95f4\u53ca\u6587\u672c\u7ebf\u7d22\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u7684\u5bfc\u822a\u3002", "result": "IROS\u6846\u67b6\u7ed3\u5408\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\u4e0e\u8f7b\u91cf\u611f\u77e5\u6a21\u5757\u7684\u6548\u7387\uff0c\u65e8\u5728\u5728\u4f4e\u6210\u672c\u786c\u4ef6\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u5ba4\u5185\u5bfc\u822a\u3002", "conclusion": "IROS\u5728\u4e94\u4e2a\u771f\u5b9e\u5efa\u7b51\u4e2d\u76f8\u6bd4\u4e8e\u57fa\u4e8e\u6301\u7eed\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5bfc\u822a\u63d0\u9ad8\u4e86\u51b3\u7b56\u51c6\u786e\u6027\uff0c\u5e76\u5c06\u5ef6\u8fdf\u51cf\u5c11\u4e8666%\u3002"}}
{"id": "2601.22013", "categories": ["cs.HC", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.22013", "abs": "https://arxiv.org/abs/2601.22013", "authors": ["Catherine Yeh", "Anh Truong", "Mira Dontcheva", "Bryan Wang"], "title": "Vidmento: Creating Video Stories Through Context-Aware Expansion With Generative Video", "comment": "25 pages, 18 figures", "summary": "Video storytelling is often constrained by available material, limiting creative expression and leaving undesired narrative gaps. Generative video offers a new way to address these limitations by augmenting captured media with tailored visuals. To explore this potential, we interviewed eight video creators to identify opportunities and challenges in integrating generative video into their workflows. Building on these insights and established filmmaking principles, we developed Vidmento, a tool for authoring hybrid video stories that combine captured and generated media through context-aware expansion. Vidmento surfaces opportunities for story development, generates clips that blend stylistically and narratively with surrounding media, and provides controls for refinement. In a study with 12 creators, Vidmento supported narrative development and exploration by systematically expanding initial materials with generative media, enabling expressive video storytelling aligned with creative intent. We highlight how creators bridge story gaps with generative content and where they find this blending capability most valuable.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u751f\u6210\u89c6\u9891\u5982\u4f55\u589e\u5f3a\u89c6\u9891\u6545\u4e8b\u8bb2\u8ff0\uff0c\u4ecb\u7ecd\u4e86Vidmento\u5de5\u5177\u7684\u5f00\u53d1\u53ca\u5176\u5bf9\u521b\u4f5c\u8005\u7684\u652f\u6301\u3002", "motivation": "\u89e3\u51b3\u89c6\u9891\u53d9\u4e8b\u4e2d\u7531\u6750\u6599\u9650\u5236\u5bfc\u81f4\u7684\u521b\u9020\u6027\u8868\u8fbe\u4e0d\u8db3\u548c\u53d9\u4e8b\u7a7a\u767d\u7684\u95ee\u9898\u3002", "method": "\u91c7\u8bbf\u89c6\u9891\u521b\u4f5c\u8005\uff0c\u8bc6\u522b\u751f\u6210\u89c6\u9891\u6574\u5408\u7684\u673a\u4f1a\u4e0e\u6311\u6218\uff0c\u5f00\u53d1Vidmento\u5de5\u5177", "result": "Vidmento\u652f\u6301\u901a\u8fc7\u751f\u6210\u5a92\u4f53\u7cfb\u7edf\u6027\u5730\u6269\u5c55\u521d\u59cb\u6750\u6599\uff0c\u4fc3\u8fdb\u53d9\u4e8b\u53d1\u5c55\u548c\u63a2\u7d22\uff0c\u589e\u5f3a\u89c6\u9891\u53d9\u4e8b\u7684\u8868\u73b0\u529b\u3002", "conclusion": "\u521b\u9020\u8005\u5229\u7528\u751f\u6210\u5185\u5bb9\u5f25\u8865\u6545\u4e8b\u6f0f\u6d1e\uff0c\u5e76\u53d1\u73b0\u8fd9\u4e00\u878d\u5408\u80fd\u529b\u5728\u7279\u5b9a\u60c5\u51b5\u4e0b\u6700\u5177\u4ef7\u503c\u3002"}}
{"id": "2601.21548", "categories": ["cs.RO", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2601.21548", "abs": "https://arxiv.org/abs/2601.21548", "authors": ["Irene Ambrosini", "Ingo Blakowski", "Dmitrii Zendrikov", "Cristiano Capone", "Luna Gava", "Giacomo Indiveri", "Chiara De Luca", "Chiara Bartolozzi"], "title": "Training slow silicon neurons to control extremely fast robots with spiking reinforcement learning", "comment": null, "summary": "Air hockey demands split-second decisions at high puck velocities, a challenge we address with a compact network of spiking neurons running on a mixed-signal analog/digital neuromorphic processor. By co-designing hardware and learning algorithms, we train the system to achieve successful puck interactions through reinforcement learning in a remarkably small number of trials. The network leverages fixed random connectivity to capture the task's temporal structure and adopts a local e-prop learning rule in the readout layer to exploit event-driven activity for fast and efficient learning. The result is real-time learning with a setup comprising a computer and the neuromorphic chip in-the-loop, enabling practical training of spiking neural networks for robotic autonomous systems. This work bridges neuroscience-inspired hardware with real-world robotic control, showing that brain-inspired approaches can tackle fast-paced interaction tasks while supporting always-on learning in intelligent machines.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u7d27\u51d1\u7684\u8109\u51b2\u795e\u7ecf\u5143\u7f51\u7edc\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5728\u6781\u5c11\u7684\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u7a7a\u6c14\u66f2\u68cd\u7403\u63a7\u5236\uff0c\u5c55\u793a\u4e86\u795e\u7ecf\u503e\u5411\u786c\u4ef6\u5728\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u7a7a\u6c14\u66f2\u68cd\u7403\u9700\u8981\u5728\u9ad8\u901f\u5ea6\u4e0b\u8fdb\u884c\u5feb\u901f\u51b3\u7b56\uff0c\u8fd9\u5bf9\u4f20\u7edf\u7cfb\u7edf\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u56e0\u6b64\u5f00\u53d1\u4e00\u4e2a\u9ad8\u6548\u7684\u5b66\u4e60\u7cfb\u7edf\u5bf9\u4e8e\u673a\u5668\u4eba\u81ea\u4e3b\u63a7\u5236\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5728\u6df7\u5408\u4fe1\u53f7\u6a21\u62df/\u6570\u5b57\u795e\u7ecf\u5f62\u6001\u5904\u7406\u5668\u4e0a\u8fd0\u884c\u7684\u8109\u51b2\u795e\u7ecf\u5143\u7f51\u7edc\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u4e8b\u4ef6\u9a71\u52a8\u6d3b\u52a8\u6765\u8fdb\u884c\u5feb\u901f\u6709\u6548\u7684\u5b66\u4e60\u3002", "result": "\u7cfb\u7edf\u5728\u771f\u5b9e\u65f6\u95f4\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u901a\u8fc7\u8ba1\u7b97\u673a\u4e0e\u795e\u7ecf\u5f62\u6001\u82af\u7247\u7684\u8054\u5408\uff0c\u5b9e\u73b0\u4e86\u5bf9\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u6709\u6548\u8bad\u7ec3\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u786c\u4ef6\u4e0e\u73b0\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u63a7\u5236\u7684\u7ed3\u5408\uff0c\u8bc1\u660e\u4e86\u57fa\u4e8e\u5927\u8111\u7684\u89e3\u51b3\u65b9\u6848\u80fd\u591f\u5e94\u5bf9\u5feb\u901f\u4e92\u52a8\u4efb\u52a1\uff0c\u5e76\u652f\u6301\u667a\u80fd\u673a\u5668\u7684\u6301\u7eed\u5b66\u4e60\u3002"}}
{"id": "2601.22081", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.22081", "abs": "https://arxiv.org/abs/2601.22081", "authors": ["Yichun Zhao", "Miguel A. Nacenta", "Mahadeo A. Sukhai", "Sowmya Somanath"], "title": "Accessibility-Driven Information Transformations in Mixed-Visual Ability Work Teams", "comment": "To appear in ACM CHI 2026. DOI: https://doi.org/10.1145/3772318.3790872", "summary": "Blind and low-vision (BLV) employees in mixed-visual ability teams often encounter information (e.g., PDFs, diagrams) in inaccessible formats. To enable teamwork, teams must transform these representations by modifying or re-creating them into accessible forms. However, these transformations are frequently overlooked, lack infrastructural support, and cause additional labour. To design systems that move beyond one-off accommodations to effective mixed-ability collaboration, we need a deeper understanding of the representations, their transformations and how they occur. We conducted a week-long diary study with follow-up interviews with 23 BLV and sighted professionals from five legal, non-profit, and consulting teams, documenting 36 transformation cases. Our analysis characterizes how teams perform representational transformations for accessibility: how they are triggered proactively or reactively, how they simplify or enhance, and four common patterns in which workers coordinate with each other to address representational incompatibility. Our findings uncover opportunities for designing systems that can better support mixed-visual ability work.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u76f2\u4eba\u4e0e\u4f4e\u89c6\u529b\u5458\u5de5\u5728\u6df7\u5408\u89c6\u89c9\u80fd\u529b\u56e2\u961f\u4e2d\u7684\u4fe1\u606f\u53ef\u8bbf\u95ee\u6027\u95ee\u9898\uff0c\u5e76\u5206\u6790\u4e86\u4e0e\u4e4b\u76f8\u5173\u7684\u8f6c\u5316\u8fc7\u7a0b\u548c\u534f\u8c03\u6a21\u5f0f\u3002", "motivation": "\u76f2\u4eba\u4e0e\u4f4e\u89c6\u529b\u5458\u5de5\u5728\u6df7\u5408\u89c6\u89c9\u80fd\u529b\u56e2\u961f\u4e2d\u7ecf\u5e38\u9047\u5230\u4fe1\u606f\uff08\u4f8b\u5982 PDF \u548c\u56fe\u8868\uff09\u65e0\u6cd5\u8bbf\u95ee\u7684\u95ee\u9898\u3002", "method": "\u8fdb\u884c\u4e86\u4e3a\u671f\u4e00\u5468\u7684\u65e5\u8bb0\u7814\u7a76\uff0c\u5e76\u5bf9\u6765\u81ea\u4e94\u4e2a\u6cd5\u5f8b\u3001\u975e\u8425\u5229\u548c\u54a8\u8be2\u56e2\u961f\u768423\u540d\u76f2\u4eba\u4e0e\u4f4e\u89c6\u529b\u4ee5\u53ca\u89c6\u529b\u6b63\u5e38\u7684\u4e13\u4e1a\u4eba\u5458\u8fdb\u884c\u4e86\u8ddf\u8fdb\u8bbf\u8c08\uff0c\u8bb0\u5f55\u4e8636\u4e2a\u8f6c\u5316\u6848\u4f8b\u3002", "result": "\u6211\u4eec\u7684\u5206\u6790\u63cf\u7ed8\u4e86\u56e2\u961f\u5982\u4f55\u8fdb\u884c\u53ef\u8bbf\u95ee\u6027\u7684\u8868\u5f81\u8f6c\u5316\uff0c\u5305\u62ec\u8f6c\u5316\u7684\u89e6\u53d1\u65b9\u5f0f\uff08\u4e3b\u52a8\u6216\u88ab\u52a8\uff09\u3001\u590d\u6742\u6027\u6216\u589e\u5f3a\u6027\uff0c\u4ee5\u53ca\u5458\u5de5\u5728\u89e3\u51b3\u8868\u5f81\u4e0d\u517c\u5bb9\u65f6\u7684\u56db\u79cd\u5e38\u89c1\u534f\u8c03\u6a21\u5f0f\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u66f4\u597d\u5730\u652f\u6301\u6df7\u5408\u89c6\u89c9\u80fd\u529b\u5de5\u4f5c\u7684\u7cfb\u7edf\u8bbe\u8ba1\u673a\u4f1a\u3002"}}
{"id": "2601.21602", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21602", "abs": "https://arxiv.org/abs/2601.21602", "authors": ["Jianli Sun", "Bin Tian", "Qiyao Zhang", "Chengxiang Li", "Zihan Song", "Zhiyong Cui", "Yisheng Lv", "Yonglin Tian"], "title": "AIR-VLA: Vision-Language-Action Systems for Aerial Manipulation", "comment": null, "summary": "While Vision-Language-Action (VLA) models have achieved remarkable success in ground-based embodied intelligence, their application to Aerial Manipulation Systems (AMS) remains a largely unexplored frontier. The inherent characteristics of AMS, including floating-base dynamics, strong coupling between the UAV and the manipulator, and the multi-step, long-horizon nature of operational tasks, pose severe challenges to existing VLA paradigms designed for static or 2D mobile bases. To bridge this gap, we propose AIR-VLA, the first VLA benchmark specifically tailored for aerial manipulation. We construct a physics-based simulation environment and release a high-quality multimodal dataset comprising 3000 manually teleoperated demonstrations, covering base manipulation, object & spatial understanding, semantic reasoning, and long-horizon planning. Leveraging this platform, we systematically evaluate mainstream VLA models and state-of-the-art VLM models. Our experiments not only validate the feasibility of transferring VLA paradigms to aerial systems but also, through multi-dimensional metrics tailored to aerial tasks, reveal the capabilities and boundaries of current models regarding UAV mobility, manipulator control, and high-level planning. AIR-VLA establishes a standardized testbed and data foundation for future research in general-purpose aerial robotics. The resource of AIR-VLA will be available at https://anonymous.4open.science/r/AIR-VLA-dataset-B5CC/.", "AI": {"tldr": "\u63d0\u51fa\u4e86AIR-VLA\u57fa\u51c6\uff0c\u9488\u5bf9\u822a\u7a7a\u64cd\u63a7\u7cfb\u7edf\uff0c\u9a8c\u8bc1\u4e86\u73b0\u6709VLA\u6a21\u578b\u5728\u6b64\u9886\u57df\u7684\u53ef\u884c\u6027\u548c\u5c40\u9650\u6027\u3002", "motivation": "\u63a2\u7d22Vision-Language-Action\u6a21\u578b\u5728\u822a\u7a7a\u64cd\u63a7\u7cfb\u7edf\u4e2d\u7684\u9002\u7528\u6027\uff0c\u586b\u8865\u5f53\u524d\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u5efa\u7acb\u4e00\u4e2a\u7269\u7406\u57fa\u7840\u7684\u6a21\u62df\u73af\u5883\u5e76\u53d1\u5e03\u5305\u542b3000\u4e2a\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u6570\u636e\u96c6\u7684AIR-VLA\u57fa\u51c6\uff0c\u4e13\u95e8\u7528\u4e8e\u822a\u7a7a\u64cd\u63a7\u3002", "result": "\u6210\u529f\u9a8c\u8bc1\u4e86\u5c06VLA\u8303\u5f0f\u8f6c\u79fb\u5230\u822a\u7a7a\u7cfb\u7edf\u7684\u53ef\u884c\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728UAV\u673a\u52a8\u6027\u3001\u64cd\u7eb5\u5668\u63a7\u5236\u548c\u9ad8\u5c42\u6b21\u89c4\u5212\u65b9\u9762\u7684\u80fd\u529b\u548c\u5c40\u9650\u6027\u3002", "conclusion": "AIR-VLA\u4e3a\u672a\u6765\u822a\u5929\u673a\u5668\u4eba\u7814\u7a76\u5efa\u7acb\u4e86\u6807\u51c6\u5316\u7684\u6d4b\u8bd5\u5e73\u53f0\u548c\u6570\u636e\u57fa\u7840\uff0c\u4fc3\u8fdb\u4e86\u822a\u7a7a\u64cd\u63a7\u7cfb\u7edf\u7684\u7814\u7a76\u3002"}}
{"id": "2601.22082", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.22082", "abs": "https://arxiv.org/abs/2601.22082", "authors": ["Yi Fei Cheng", "Jarod Bloch", "Alexander Wang", "Andrea Bianchi", "Anusha Withana", "Anhong Guo", "Laurie M. Heller", "David Lindlbauer"], "title": "Auditorily Embodied Conversational Agents: Effects of Spatialization and Situated Audio Cues on Presence and Social Perception", "comment": null, "summary": "Embodiment can enhance conversational agents, such as increasing their perceived presence. This is typically achieved through visual representations of a virtual body; however, visual modalities are not always available, such as when users interact with agents using headphones or display-less glasses. In this work, we explore auditory embodiment. By introducing auditory cues of bodily presence - through spatially localized voice and situated Foley audio from environmental interactions - we investigate how audio alone can convey embodiment and influence perceptions of a conversational agent. We conducted a 2 (spatialization: monaural vs. spatialized) x 2 (Foley: none vs. Foley) within-subjects study, where participants (n=24) engaged in conversations with agents. Our results show that spatialization and Foley increase co-presence, but reduce users' perceptions of the agent's attention and other social attributes.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165\u7a7a\u95f4\u5316\u58f0\u97f3\u548cFoley\u97f3\u6548\uff0c\u542c\u89c9\u5316\u8eab\u53ef\u4ee5\u63d0\u5347\u4f1a\u8bdd\u4ee3\u7406\u7684\u5b58\u5728\u611f\uff0c\u4f46\u53ef\u80fd\u4f1a\u524a\u5f31\u7528\u6237\u5bf9\u4ee3\u7406\u7684\u793e\u4ea4\u5c5e\u6027\u5370\u8c61\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u5bfb\u627e\u5728\u7f3a\u4e4f\u89c6\u89c9\u5448\u73b0\u7684\u60c5\u51b5\u4e0b\u589e\u5f3a\u4f1a\u8bdd\u4ee3\u7406\u611f\u89c9\u5b58\u5728\u611f\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u4f7f\u7528\u8033\u673a\u6216\u65e0\u663e\u793a\u5668\u773c\u955c\u65f6\u3002", "method": "\u8fdb\u884c\u4e86\u4e00\u4e2a2x2\u7684\u5728\u4f53\u5185\u7814\u7a76\uff0c\u6bd4\u8f83\u4e86\u7a7a\u95f4\u5316\u58f0\u97f3\u4e0e\u666e\u901a\u58f0\u97f3\uff0c\u4ee5\u53ca\u6709\u65e0Foley\u97f3\u6548\u5bf9\u4f1a\u8bdd\u4ee3\u7406\u611f\u77e5\u7684\u5f71\u54cd\uff0c\u53c2\u4e0e\u8005\u5b9e\u9645\u8fdb\u884c\u5bf9\u8bdd\u3002", "result": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u542c\u89c9\u5316\u8eab\u5982\u4f55\u589e\u5f3a\u4f1a\u8bdd\u4ee3\u7406\u7684\u5b58\u5728\u611f\uff0c\u7279\u522b\u662f\u5728\u7528\u6237\u4f7f\u7528\u8033\u673a\u6216\u65e0\u663e\u793a\u5668\u773c\u955c\u65f6\u3002\u5b9e\u9a8c\u53d1\u73b0\uff0c\u7a7a\u95f4\u5316\u7684\u58f0\u97f3\u548c\u73af\u5883\u4ea4\u4e92\u7684Foley\u97f3\u6548\u589e\u5f3a\u4e86\u5171\u5b58\u611f\uff0c\u4f46\u964d\u4f4e\u4e86\u7528\u6237\u5bf9\u4ee3\u7406\u5173\u6ce8\u548c\u5176\u4ed6\u793e\u4f1a\u5c5e\u6027\u7684\u8ba4\u77e5\u3002", "conclusion": "\u542c\u89c9\u5316\u8eab\u901a\u8fc7\u7a7a\u95f4\u5316\u58f0\u548cFoley\u97f3\u6548\u53ef\u4ee5\u589e\u5f3a\u7528\u6237\u7684\u5171\u5b58\u611f\uff0c\u5c3d\u7ba1\u8fd9\u53ef\u80fd\u4f1a\u5f71\u54cd\u4ed6\u4eec\u5bf9\u4ee3\u7406\u793e\u4ea4\u5c5e\u6027\u7684\u611f\u77e5\u3002"}}
{"id": "2601.21667", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.21667", "abs": "https://arxiv.org/abs/2601.21667", "authors": ["Hao Ju", "Shaofei Huang", "Hongyu Li", "Zihan Ding", "Si Liu", "Meng Wang", "Zhedong Zheng"], "title": "From Instruction to Event: Sound-Triggered Mobile Manipulation", "comment": null, "summary": "Current mobile manipulation research predominantly follows an instruction-driven paradigm, where agents rely on predefined textual commands to execute tasks. However, this setting confines agents to a passive role, limiting their autonomy and ability to react to dynamic environmental events. To address these limitations, we introduce sound-triggered mobile manipulation, where agents must actively perceive and interact with sound-emitting objects without explicit action instructions. To support these tasks, we develop Habitat-Echo, a data platform that integrates acoustic rendering with physical interaction. We further propose a baseline comprising a high-level task planner and low-level policy models to complete these tasks. Extensive experiments show that the proposed baseline empowers agents to actively detect and respond to auditory events, eliminating the need for case-by-case instructions. Notably, in the challenging dual-source scenario, the agent successfully isolates the primary source from overlapping acoustic interference to execute the first interaction, and subsequently proceeds to manipulate the secondary object, verifying the robustness of the baseline.", "AI": {"tldr": "\u5f15\u5165\u58f0\u89e6\u53d1\u7684\u79fb\u52a8\u64cd\u63a7\u6982\u5ff5\uff0c\u6539\u5584\u4f20\u7edf\u6307\u4ee4\u9a71\u52a8\u7684\u673a\u5668\u4eba\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u79fb\u52a8\u64cd\u63a7\u7814\u7a76\u53d7\u9650\u4e8e\u6307\u4ee4\u9a71\u52a8\u8303\u5f0f\uff0c\u9650\u5236\u4e86\u4ee3\u7406\u7684\u81ea\u4e3b\u6027\u548c\u5bf9\u52a8\u6001\u4e8b\u4ef6\u7684\u53cd\u5e94\u80fd\u529b\u3002", "method": "\u5f00\u53d1Habitat-Echo\u6570\u636e\u5e73\u53f0\uff0c\u96c6\u6210\u58f0\u5b66\u6e32\u67d3\u4e0e\u7269\u7406\u4ea4\u4e92\uff0c\u63d0\u51fa\u9ad8\u5c42\u4efb\u52a1\u89c4\u5212\u4e0e\u4f4e\u5c42\u7b56\u7565\u6a21\u578b\u7684\u57fa\u7ebf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u57fa\u7ebf\u4f7f\u4ee3\u7406\u80fd\u591f\u4e3b\u52a8\u63a2\u6d4b\u548c\u54cd\u5e94\u542c\u89c9\u4e8b\u4ef6\uff0c\u5728\u52a8\u6001\u53cc\u6e90\u573a\u666f\u4e2d\u6210\u529f\u9694\u79bb\u4e3b\u8981\u58f0\u6e90\u5e76\u6267\u884c\u4ea4\u4e92\u3002", "conclusion": "\u58f0\u89e6\u53d1\u79fb\u52a8\u64cd\u63a7\u9a8c\u8bc1\u4e86\u57fa\u7ebf\u7684\u9c81\u68d2\u6027\uff0c\u51cf\u8f7b\u4e86\u5bf9\u9010\u4e2a\u6307\u4ee4\u7684\u4f9d\u8d56\u3002"}}
{"id": "2601.21712", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21712", "abs": "https://arxiv.org/abs/2601.21712", "authors": ["Xuanran Zhai", "Binkai Ou", "Yemin Wang", "Hui Yi Leong", "Qiaojun Yu", "Ce Hao", "Yaohua Liu"], "title": "CoFreeVLA: Collision-Free Dual-Arm Manipulation via Vision-Language-Action Model and Risk Estimation", "comment": null, "summary": "Vision Language Action (VLA) models enable instruction following manipulation, yet dualarm deployment remains unsafe due to under modeled selfcollisions between arms and grasped objects. We introduce CoFreeVLA, which augments an endtoend VLA with a short horizon selfcollision risk estimator that predicts collision likelihood from proprioception, visual embeddings, and planned actions. The estimator gates risky commands, recovers to safe states via risk-guided adjustments, and shapes policy refinement for safer rollouts. It is pre-trained with model-based collision labels and posttrained on real robot rollouts for calibration. On five bimanual tasks with the PiPER robot arm, CoFreeVLA reduces selfcollisions and improves success rates versus RDT and APEX.", "AI": {"tldr": "CoFreeVLA\u901a\u8fc7\u81ea\u6211\u78b0\u649e\u98ce\u9669\u4f30\u8ba1\u63d0\u5347\u4e86\u53cc\u81c2\u64cd\u4f5c\u7684\u5b89\u5168\u6027\u548c\u6210\u529f\u7387\u3002", "motivation": "\u53cc\u81c2\u64cd\u63a7\u5728\u9075\u5faa\u6307\u4ee4\u6267\u884c\u65f6\u9762\u4e34\u81ea\u6211\u78b0\u649e\u7684\u95ee\u9898\uff0c\u73b0\u6709\u6a21\u578b\u672a\u80fd\u5145\u5206\u8003\u8651\u8fd9\u4e00\u70b9\u3002", "method": "\u901a\u8fc7\u589e\u6dfb\u77ed\u671f\u81ea\u6211\u78b0\u649e\u98ce\u9669\u4f30\u8ba1\u5668\uff0c\u5229\u7528\u672c\u4f53\u611f\u77e5\u3001\u89c6\u89c9\u5d4c\u5165\u548c\u89c4\u5212\u52a8\u4f5c\u9884\u6d4b\u78b0\u649e\u53ef\u80fd\u6027\uff0c\u5e76\u5728\u4e94\u4e2a\u53cc\u81c2\u4efb\u52a1\u4e2d\u5e94\u7528\u4e8ePiPER\u673a\u5668\u4eba\u624b\u81c2\u3002", "result": "CoFreeVLA\u51cf\u5c11\u4e86\u81ea\u6211\u78b0\u649e\uff0c\u6539\u5584\u4e86\u5728\u4e94\u4e2a\u53cc\u81c2\u4efb\u52a1\u4e2d\u7684\u6210\u529f\u7387\u3002", "conclusion": "CoFreeVLA\u5728\u51cf\u5c11\u81ea\u6211\u78b0\u649e\u5e76\u63d0\u9ad8\u6210\u529f\u7387\u65b9\u9762\u4f18\u4e8eRDT\u548cAPEX\u3002"}}
{"id": "2601.21713", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21713", "abs": "https://arxiv.org/abs/2601.21713", "authors": ["Donatien Delehelle", "Fei Chen", "Darwin Caldwell"], "title": "Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations", "comment": "6 pages, 4 figures,", "summary": "Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u589e\u5f3a\u5b66\u4e60\u65b9\u6cd5\u7528\u4e8e\u5e03\u6599\u64cd\u63a7\uff0c\u4f18\u5316\u8bbe\u8ba1\u9009\u62e9\u4f7f\u6a21\u578b\u66f4\u5c0f\u3001\u8bad\u7ec3\u65f6\u95f4\u66f4\u77ed\uff0c\u540c\u65f6\u5728SoftGym\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5e03\u6599\u64cd\u63a7\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u4f46\u7531\u4e8e\u72b6\u6001\u7a7a\u95f4\u7ef4\u5ea6\u9ad8\u3001\u52a8\u6001\u590d\u6742\u53ca\u81ea\u906e\u6321\u95ee\u9898\uff0c\u673a\u5668\u4eba\u64cd\u63a7\u9762\u4e34\u6311\u6218\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u7684\u589e\u5f3a\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8fdb\u884c\u6709\u6548\u8bbe\u8ba1\uff0c\u51cf\u5c11\u6a21\u578b\u89c4\u6a21\u548c\u8bad\u7ec3\u65f6\u95f4\uff0c\u540c\u65f6\u786e\u4fdd\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u6709\u6548\u8fc1\u79fb\u3002", "result": "\u672c\u6587\u63a2\u8ba8\u4e86\u5e03\u6599\u64cd\u63a7\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u589e\u5f3a\u5b66\u4e60(RL)\u65b9\u6cd5\u6765\u89e3\u51b3\u6b64\u95ee\u9898\u3002\u901a\u8fc7\u4f18\u5316\u6a21\u578b\u8bbe\u8ba1\u548c\u9009\u62e9\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6a21\u578b\u7684\u89c4\u6a21\u548c\u8bad\u7ec3\u65f6\u95f4\uff0c\u540c\u65f6\u5728SoftGym\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u9ad8\u6548\u7684\u8bbe\u8ba1\u9009\u62e9\uff0c\u4f5c\u8005\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6a21\u62df\u5b66\u4e60\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u64cd\u4f5c\u6027\u80fd\uff0c\u5e76\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u826f\u597d\u7684\u8fc1\u79fb\u6548\u679c\uff0c\u8868\u660e\u4e86\u7483\u6a21\u62df\u5b66\u4e60\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.21772", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21772", "abs": "https://arxiv.org/abs/2601.21772", "authors": ["Carmen D. R. Pita-Romero", "Pedro Arias-Perez", "Miguel Fernandez-Cortizas", "Rafael Perez-Segui", "Pascual Campoy"], "title": "Flocking behavior for dynamic and complex swarm structures", "comment": null, "summary": "Maintaining the formation of complex structures with multiple UAVs and achieving complex trajectories remains a major challenge. This work presents an algorithm for implementing the flocking behavior of UAVs based on the concept of Virtual Centroid to easily develop a structure for the flock. The approach builds on the classical virtual-based behavior, providing a theoretical framework for incorporating enhancements to dynamically control both the number of agents and the formation of the structure. Simulation tests and real-world experiments were conducted, demonstrating its simplicity even with complex formations and complex trajectories.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u865a\u62df\u8d28\u5fc3\u7684\u65e0\u4eba\u673a\u7f16\u961f\u7b97\u6cd5\uff0c\u80fd\u6709\u6548\u7ef4\u62a4\u590d\u6742\u7ed3\u6784\u548c\u8f68\u8ff9\uff0c\u7ecf\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u7b80\u5355\u6709\u6548\u3002", "motivation": "\u7ef4\u62a4\u591a\u65e0\u4eba\u673a\u7684\u590d\u6742\u7ed3\u6784\u5e76\u5b9e\u73b0\u590d\u6742\u8f68\u8ff9\u7684\u63a7\u5236\u662f\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8be5\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u865a\u62df\u8d28\u5fc3\u7684\u7b97\u6cd5\uff0c\u7ed3\u5408\u7ecf\u5178\u7684\u865a\u62df\u884c\u4e3a\uff0c\u5bf9\u65e0\u4eba\u673a\u7f16\u961f\u7684\u6570\u91cf\u548c\u5f62\u5f0f\u8fdb\u884c\u52a8\u6001\u63a7\u5236\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u6d4b\u8bd5\u548c\u5b9e\u9645\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u7b97\u6cd5\u5728\u5904\u7406\u590d\u6742\u961f\u5f62\u548c\u8f68\u8ff9\u65f6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728\u5b9e\u73b0\u590d\u6742\u961f\u5f62\u548c\u8f68\u8ff9\u63a7\u5236\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u7b80\u4fbf\u6027\u3002"}}
{"id": "2601.21829", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21829", "abs": "https://arxiv.org/abs/2601.21829", "authors": ["Bsher Karbouj", "Baha Eddin Gaaloul", "Jorg Kruger"], "title": "GAZELOAD A Multimodal Eye-Tracking Dataset for Mental Workload in Industrial Human-Robot Collaboration", "comment": null, "summary": "This article describes GAZELOAD, a multimodal dataset for mental workload estimation in industrial human-robot collaboration. The data were collected in a laboratory assembly testbed where 26 participants interacted with two collaborative robots (UR5 and Franka Emika Panda) while wearing Meta ARIA smart glasses. The dataset time-synchronizes eye-tracking signals (pupil diameter, fixations, saccades, eye gaze, gaze transition entropy, fixation dispersion index) with environmental real-time and continuous measurements (illuminance) and task and robot context (bench, task block, induced faults), under controlled manipulations of task difficulty and ambient conditions. For each participant and workload-graded task block, we provide CSV files with ocular metrics aggregated into 250 ms windows, environmental logs, and self-reported mental workload ratings on a 1-10 Likert scale, organized in participant-specific folders alongside documentation. These data can be used to develop and benchmark algorithms for mental workload estimation, feature extraction, and temporal modeling in realistic industrial HRC scenarios, and to investigate the influence of environmental factors such as lighting on eye-based workload markers.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86 GAZELOAD \u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5fc3\u7406\u8d1f\u8377\u4f30\u8ba1\uff0c\u5305\u542b\u65f6\u95f4\u540c\u6b65\u7684\u773c\u52a8\u4e0e\u73af\u5883\u6570\u636e\uff0c\u9002\u7528\u4e8e\u5de5\u4e1a\u4eba\u673a\u534f\u4f5c\u573a\u666f\u3002", "motivation": "\u4e3a\u4e86\u91cf\u5316\u5de5\u4e1a\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u5fc3\u7406\u8d1f\u8377\uff0c\u5e76\u7814\u7a76\u73af\u5883\u56e0\u7d20\u5bf9\u773c\u52a8\u6807\u8bb0\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5728\u5b9e\u9a8c\u5ba4\u88c5\u914d\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u6536\u96c6\u53c2\u4e0e\u8005\u4e0e\u534f\u4f5c\u673a\u5668\u4eba\u4e92\u52a8\u65f6\u7684\u773c\u52a8\u8ffd\u8e2a\u6570\u636e\u4e0e\u73af\u5883\u5b9e\u65f6\u6d4b\u91cf\uff0c\u65f6\u95f4\u540c\u6b65\u8fdb\u884c\u3002", "result": "\u751f\u6210\u7684 GALELOAD \u6570\u636e\u96c6\u5305\u542b\u773c\u52a8\u6307\u6807\u3001\u73af\u5883\u8bb0\u5f55\u548c\u81ea\u6211\u62a5\u544a\u7684\u5fc3\u7406\u8d1f\u8377\u8bc4\u5206\uff0c\u53ef\u7528\u4e8e\u7b97\u6cd5\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "GAZELOAD \u6570\u636e\u96c6\u4e3a\u5de5\u4e1a\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u5fc3\u7406\u8d1f\u8377\u4f30\u8ba1\u63d0\u4f9b\u4e86\u591a\u6a21\u6001\u6570\u636e\uff0c\u9002\u7528\u4e8e\u7b97\u6cd5\u5f00\u53d1\u4e0e\u73af\u5883\u56e0\u7d20\u7814\u7a76\u3002"}}
{"id": "2601.21876", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21876", "abs": "https://arxiv.org/abs/2601.21876", "authors": ["He Li", "Zhaowei Chen", "Rui Gao", "Guoliang Li", "Qi Hao", "Shuai Wang", "Chengzhong Xu"], "title": "LLM-Driven Scenario-Aware Planning for Autonomous Driving", "comment": null, "summary": "Hybrid planner switching framework (HPSF) for autonomous driving needs to reconcile high-speed driving efficiency with safe maneuvering in dense traffic. Existing HPSF methods often fail to make reliable mode transitions or sustain efficient driving in congested environments, owing to heuristic scene recognition and low-frequency control updates. To address the limitation, this paper proposes LAP, a large language model (LLM) driven, adaptive planning method, which switches between high-speed driving in low-complexity scenes and precise driving in high-complexity scenes, enabling high qualities of trajectory generation through confined gaps. This is achieved by leveraging LLM for scene understanding and integrating its inference into the joint optimization of mode configuration and motion planning. The joint optimization is solved using tree-search model predictive control and alternating minimization. We implement LAP by Python in Robot Operating System (ROS). High-fidelity simulation results show that the proposed LAP outperforms other benchmarks in terms of both driving time and success rate.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u89c4\u5212\u65b9\u6cd5\uff08LAP\uff09\uff0c\u80fd\u5728\u4e0d\u540c\u590d\u6742\u5ea6\u573a\u666f\u4e2d\u9ad8\u6548\u5207\u6362\u9a7e\u9a76\u6a21\u5f0f\uff0c\u4ece\u800c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7684\u6548\u7387\u4e0e\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6df7\u5408\u89c4\u5212\u5668\u5207\u6362\u6846\u67b6\u5728\u5bc6\u96c6\u4ea4\u901a\u4e2d\u96be\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u9a7e\u9a76\u4e0e\u5b89\u5168\u7684\u673a\u52a8\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u901a\u8fc7\u6811\u641c\u7d22\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u548c\u4ea4\u66ff\u6700\u5c0f\u5316\u89e3\u51b3\u8054\u5408\u4f18\u5316\u95ee\u9898\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u573a\u666f\u7406\u89e3\u548c\u6a21\u5f0f\u914d\u7f6e\u3001\u8fd0\u52a8\u89c4\u5212\u7684\u8054\u5408\u4f18\u5316\u3002", "result": "\u9ad8\u4fdd\u771f\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0cLAP\u5728\u884c\u9a76\u65f6\u95f4\u548c\u6210\u529f\u7387\u4e0a\u90fd\u8d85\u8fc7\u4e86\u5176\u4ed6\u57fa\u51c6\u3002", "conclusion": "LAP\u5728\u884c\u9a76\u65f6\u95f4\u548c\u6210\u529f\u7387\u4e0a\u4f18\u4e8e\u5176\u4ed6\u57fa\u51c6\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.21884", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21884", "abs": "https://arxiv.org/abs/2601.21884", "authors": ["Pratik Ingle", "J\u00f8rn Lambertsen", "Kasper St\u00f8y", "Andres Faina"], "title": "Multi-Modular MANTA-RAY: A Modular Soft Surface Platform for Distributed Multi-Object Manipulation", "comment": "8 pages", "summary": "Manipulation surfaces control objects by actively deforming their shape rather than directly grasping them. While dense actuator arrays can generate complex deformations, they also introduce high degrees of freedom (DOF), increasing system complexity and limiting scalability. The MANTA-RAY (Manipulation with Adaptive Non-rigid Textile Actuation with Reduced Actuation densitY) platform addresses these challenges by leveraging a soft, fabric-based surface with reduced actuator density to manipulate fragile and heterogeneous objects. Previous studies focused on single-module implementations supported by four actuators, whereas the feasibility and benefits of a scalable, multi-module configuration remain unexplored. In this work, we present a distributed, modular, and scalable variant of the MANTA-RAY platform that maintains manipulation performance with a reduced actuator density. The proposed multi-module MANTA-RAY platform and control strategy employs object passing between modules and a geometric transformation driven PID controller that directly maps tilt-angle control outputs to actuator commands, eliminating the need for extensive data-driven or black-box training. We evaluate system performance in simulation across surface configurations of varying modules (3x3 and 4x4) and validate its feasibility through experiments on a physical 2x2 hardware prototype. The system successfully manipulates objects with diverse geometries, masses, and textures including fragile items such as eggs and apples as well as enabling parallel manipulation. The results demonstrate that the multi-module MANTA-RAY improves scalability and enables coordinated manipulation of multiple objects across larger areas, highlighting its potential for practical, real-world applications.", "AI": {"tldr": "MANTA-RAY\u5e73\u53f0\u901a\u8fc7\u5206\u5e03\u5f0f\u6a21\u5757\u964d\u4f4e\u81f4\u52a8\u5668\u5bc6\u5ea6\uff0c\u63d0\u9ad8\u53ef\u6269\u5c55\u6027\uff0c\u5b9e\u73b0\u6709\u6548\u64cd\u63a7\u591a\u79cd\u5bf9\u8c61\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u867d\u7136\u5bc6\u96c6\u7684\u81f4\u52a8\u5668\u9635\u5217\u53ef\u4ee5\u4ea7\u751f\u590d\u6742\u7684\u53d8\u5f62\uff0c\u4f46\u5b83\u4eec\u4e5f\u5e26\u6765\u4e86\u9ad8\u81ea\u7531\u5ea6\uff0c\u589e\u52a0\u4e86\u7cfb\u7edf\u590d\u6742\u6027\u5e76\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002", "method": "\u901a\u8fc7\u5206\u5e03\u5f0f\u3001\u6a21\u5757\u5316\u548c\u53ef\u6269\u5c55\u7684MANTA-RAY\u5e73\u53f0\uff0c\u4f7f\u7528\u7269\u4f53\u5728\u6a21\u5757\u4e4b\u95f4\u4f20\u9012\uff0c\u5e76\u7ed3\u5408\u51e0\u4f55\u53d8\u6362\u9a71\u52a8\u7684PID\u63a7\u5236\u5668\uff0c\u76f4\u63a5\u5c06\u503e\u659c\u89d2\u5ea6\u63a7\u5236\u8f93\u51fa\u6620\u5c04\u5230\u81f4\u52a8\u5668\u6307\u4ee4\u3002", "result": "\u8be5\u7cfb\u7edf\u6210\u529f\u64cd\u63a7\u4e0d\u540c\u51e0\u4f55\u5f62\u72b6\u3001\u8d28\u91cf\u548c\u7eb9\u7406\u7684\u7269\u4f53\uff0c\u5305\u62ec\u8106\u5f31\u7269\u54c1\uff08\u5982\u9e21\u86cb\u548c\u82f9\u679c\uff09\uff0c\u540c\u65f6\u5b9e\u73b0\u5e76\u884c\u64cd\u4f5c\u3002", "conclusion": "\u591a\u6a21\u5757MANTA-RAY\u5e73\u53f0\u63d0\u9ad8\u4e86\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u80fd\u591f\u5728\u66f4\u5e7f\u6cdb\u7684\u533a\u57df\u5185\u534f\u8c03\u64cd\u4f5c\u591a\u4e2a\u7269\u4f53\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.21926", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21926", "abs": "https://arxiv.org/abs/2601.21926", "authors": ["Jinhao Zhang", "Wenlong Xia", "Yaojia Wang", "Zhexuan Zhou", "Huizhe Li", "Yichen Lai", "Haoming Song", "Youmin Gong", "Jie Me"], "title": "Information Filtering via Variational Regularization for Robot Manipulation", "comment": null, "summary": "Diffusion-based visuomotor policies built on 3D visual representations have achieved strong performance in learning complex robotic skills. However, most existing methods employ an oversized denoising decoder. While increasing model capacity can improve denoising, empirical evidence suggests that it also introduces redundancy and noise in intermediate feature blocks. Crucially, we find that randomly masking backbone features at inference time (without changing training) can improve performance, confirming the presence of task-irrelevant noise in intermediate features. To this end, we propose Variational Regularization (VR), a lightweight module that imposes a timestep-conditioned Gaussian over backbone features and applies a KL-divergence regularizer, forming an adaptive information bottleneck. Extensive experiments on three simulation benchmarks (RoboTwin2.0, Adroit, and MetaWorld) show that, compared to the baseline DP3, our approach improves the success rate by 6.1% on RoboTwin2.0 and by 4.1% on Adroit and MetaWorld, achieving new state-of-the-art results. Real-world experiments further demonstrate that our method performs well in practical deployments. Code will released.", "AI": {"tldr": "\u91c7\u7528\u53d8\u5206\u6b63\u5219\u5316(VR)\u6a21\u5757\u6539\u5584\u4e86\u53bb\u566a\u6548\u679c\uff0c\u5728\u591a\u4e2a\u4eff\u771f\u57fa\u51c6\u4e0a\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6210\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u8fc7\u5927\u7684\u53bb\u566a\u89e3\u7801\u5668\uff0c\u867d\u7136\u589e\u52a0\u6a21\u578b\u5bb9\u91cf\u53ef\u4ee5\u6539\u5584\u53bb\u566a\uff0c\u4f46\u4e5f\u4f1a\u5f15\u5165\u5197\u4f59\u548c\u566a\u58f0\u3002\u6211\u4eec\u53d1\u73b0\u968f\u673a\u63a9\u7801\u5728\u63a8\u65ad\u65f6\u53ef\u4ee5\u6539\u5584\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u53d8\u5206\u6b63\u5219\u5316(VR)\uff0c\u8fd9\u662f\u4e00\u4e2a\u8f7b\u91cf\u6a21\u5757\uff0c\u65bd\u52a0\u65f6\u95f4\u6b65\u6761\u4ef6\u7684\u9ad8\u65af\u5206\u5e03\u4e8e\u4e3b\u5e72\u7279\u5f81\uff0c\u5e76\u5e94\u7528KL\u6563\u5ea6\u6b63\u5219\u5316\uff0c\u5f62\u6210\u81ea\u9002\u5e94\u4fe1\u606f\u74f6\u9888\u3002", "result": "\u5728\u4e09\u4e2a\u4eff\u771f\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u57fa\u7ebfDP3\uff0c\u5e76\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6848\u5728RoboTwin2.0\u4e0a\u6539\u5584\u4e866.1%\u7684\u6210\u529f\u7387\uff0c\u5728Adroit\u548cMetaWorld\u4e0a\u6539\u5584\u4e864.1%\uff0c\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7ed3\u679c\u3002"}}
{"id": "2601.21971", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21971", "abs": "https://arxiv.org/abs/2601.21971", "authors": ["Lorenzo Mazza", "Ariel Rodriguez", "Rayan Younis", "Martin Lelis", "Ortrun Hellig", "Chenpan Li", "Sebastian Bodenstedt", "Martin Wagner", "Stefanie Speidel"], "title": "MoE-ACT: Improving Surgical Imitation Learning Policies through Supervised Mixture-of-Experts", "comment": null, "summary": "Imitation learning has achieved remarkable success in robotic manipulation, yet its application to surgical robotics remains challenging due to data scarcity, constrained workspaces, and the need for an exceptional level of safety and predictability. We present a supervised Mixture-of-Experts (MoE) architecture designed for phase-structured surgical manipulation tasks, which can be added on top of any autonomous policy. Unlike prior surgical robot learning approaches that rely on multi-camera setups or thousands of demonstrations, we show that a lightweight action decoder policy like Action Chunking Transformer (ACT) can learn complex, long-horizon manipulation from less than 150 demonstrations using solely stereo endoscopic images, when equipped with our architecture. We evaluate our approach on the collaborative surgical task of bowel grasping and retraction, where a robot assistant interprets visual cues from a human surgeon, executes targeted grasping on deformable tissue, and performs sustained retraction. We benchmark our method against state-of-the-art Vision-Language-Action (VLA) models and the standard ACT baseline. Our results show that generalist VLAs fail to acquire the task entirely, even under standard in-distribution conditions. Furthermore, while standard ACT achieves moderate success in-distribution, adopting a supervised MoE architecture significantly boosts its performance, yielding higher success rates in-distribution and demonstrating superior robustness in out-of-distribution scenarios, including novel grasp locations, reduced illumination, and partial occlusions. Notably, it generalizes to unseen testing viewpoints and also transfers zero-shot to ex vivo porcine tissue without additional training, offering a promising pathway toward in vivo deployment. To support this, we present qualitative preliminary results of policy roll-outs during in vivo porcine surgery.", "AI": {"tldr": "\u63d0\u51fa\u76d1\u7763\u7684MoE\u67b6\u6784\uff0c\u80fd\u4f7f\u8f7b\u91cf\u7ea7\u884c\u52a8\u7f16\u7801\u5668\u5728\u5916\u79d1\u64cd\u4f5c\u4efb\u52a1\u4e2d\u4ece\u5c11\u91cf\u6f14\u793a\u4e2d\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u9ad8\u6210\u529f\u7387\u5e76\u5177\u5907\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u4eff\u6548\u5b66\u4e60\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u9886\u57df\u53d6\u5f97\u6210\u529f\uff0c\u4f46\u5728\u5916\u79d1\u673a\u5668\u4eba\u5e94\u7528\u4e0a\u7531\u4e8e\u6570\u636e\u532e\u4e4f\u548c\u5b89\u5168\u6027\u8981\u6c42\u7b49\u6311\u6218\uff0c\u9700\u63a2\u7d22\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u76d1\u7763\u7684\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\uff08MoE\uff09\u7528\u4e8e\u76f8\u4f4d\u7ed3\u6784\u7684\u5916\u79d1\u64cd\u4f5c\u4efb\u52a1\uff0c\u8be5\u67b6\u6784\u53ef\u5728\u4efb\u4f55\u81ea\u4e3b\u7b56\u7565\u4e4b\u4e0a\u8fdb\u884c\u6dfb\u52a0\u3002", "result": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7684\u52a8\u4f5c\u7f16\u7801\u5668\u7b56\u7565\uff08\u5982\u52a8\u4f5c\u5206\u5757\u53d8\u6362\u5668ACT\uff09\uff0c\u53ef\u4ee5\u4ece\u5c11\u4e8e150\u4e2a\u6f14\u793a\u4e2d\u4ec5\u4f9d\u9760\u7acb\u4f53\u5185\u7aa5\u955c\u56fe\u50cf\u5b66\u4e60\u590d\u6742\u7684\u957f\u65f6\u95f4\u64cd\u4f5c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5916\u79d1\u52a9\u624b\u673a\u5668\u4eba\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u6267\u884c\u590d\u6742\u7684\u5916\u79d1\u4efb\u52a1\uff0c\u5e76\u6210\u529f\u8f6c\u79fb\u5230\u672a\u89c1\u6d4b\u8bd5\u89c6\u89d2\u548c\u65e0\u989d\u5916\u8bad\u7ec3\u7684\u732a\u4f53\u7ec4\u7ec7\u4e0a\u3002"}}
{"id": "2601.21976", "categories": ["cs.RO", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2601.21976", "abs": "https://arxiv.org/abs/2601.21976", "authors": ["Alex S. Miller", "Leo McElroy", "Jeffrey H. Lang"], "title": "Macro-Scale Electrostatic Origami Motor", "comment": null, "summary": "Foldable robots have been an active area of robotics research due to their high volume-to-mass ratio, easy packability, and shape adaptability. For locomotion, previously developed foldable robots have either embedded linear actuators in, or attached non-folding rotary motors to, their structure. Further, those actuators directly embedded in the structure of the folding medium all contributed to linear or folding motion, not to continuous rotary motion. On the macro-scale there has not yet been a folding continuous rotary actuator. This paper details the development and testing of the first macro-scale origami rotary motor that can be folded flat, and then unfurled to operate. Using corona discharge for torque production, the prototype motor achieved an expansion ratio of 2.5:1, reached a top speed of 1440 rpm when driven at -29 kV, and exhibited a maximum output torque over 0.15 mN m with an active component torque density of 0.04 Nm/kg.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u9996\u4e2a\u5b8f\u89c2\u6298\u53e0\u8fde\u7eed\u65cb\u8f6c\u7535\u673a\uff0c\u80fd\u591f\u6298\u53e0\u5e76\u5c55\u5f00\u64cd\u4f5c\uff0c\u4f7f\u7528\u7535\u6655\u653e\u7535\u4ea7\u751f\u626d\u77e9\uff0c\u5c55\u73b0\u51fa\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u6298\u53e0\u673a\u5668\u4eba\u5728\u4f53\u79ef\u4e0e\u8d28\u91cf\u6bd4\u3001\u4fbf\u643a\u6027\u548c\u5f62\u72b6\u9002\u5e94\u6027\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u7136\u800c\u76ee\u524d\u7f3a\u4e4f\u5b8f\u89c2\u6298\u53e0\u8fde\u7eed\u65cb\u8f6c\u9a71\u52a8\u5668\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u5b8f\u89c2\u89c4\u6a21\u7684\u6298\u53e0\u5f0f\u65cb\u8f6c\u7535\u673a\uff0c\u91c7\u7528\u7535\u6655\u653e\u7535\u4f5c\u4e3a\u626d\u77e9\u6e90\uff0c\u80fd\u591f\u6298\u53e0\u5e73\u653e\u5e76\u5c55\u5f00\u8fd0\u884c\u3002", "result": "\u539f\u578b\u7535\u673a\u8fbe\u52302.5:1\u7684\u6269\u5c55\u6bd4\uff0c\u5728-29 kV\u9a71\u52a8\u4e0b\u8fbe\u52301440 rpm\u7684\u6700\u9ad8\u8f6c\u901f\uff0c\u5e76\u5c55\u73b0\u8d850.15 mN m\u7684\u6700\u5927\u8f93\u51fa\u626d\u77e9\u548c0.04 Nm/kg\u7684\u626d\u77e9\u5bc6\u5ea6\u3002", "conclusion": "\u672c\u7814\u7a76\u7684\u6298\u53e0\u65cb\u8f6c\u7535\u673a\u5c55\u793a\u4e86\u5728\u6298\u53e0\u673a\u5668\u4eba\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\uff0c\u5177\u6709\u5b9e\u9645\u64cd\u4f5c\u7684\u53ef\u884c\u6027\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2601.22018", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.22018", "abs": "https://arxiv.org/abs/2601.22018", "authors": ["Jinhao Zhang", "Zhexuan Zhou", "Huizhe Li", "Yichen Lai", "Wenlong Xia", "Haoming Song", "Youmin Gong", "Jie Me"], "title": "PocketDP3: Efficient Pocket-Scale 3D Visuomotor Policy", "comment": null, "summary": "Recently, 3D vision-based diffusion policies have shown strong capability in learning complex robotic manipulation skills. However, a common architectural mismatch exists in these models: a tiny yet efficient point-cloud encoder is often paired with a massive decoder. Given a compact scene representation, we argue that this may lead to substantial parameter waste in the decoder. Motivated by this observation, we propose PocketDP3, a pocket-scale 3D diffusion policy that replaces the heavy conditional U-Net decoder used in prior methods with a lightweight Diffusion Mixer (DiM) built on MLP-Mixer blocks. This architecture enables efficient fusion across temporal and channel dimensions, significantly reducing model size. Notably, without any additional consistency distillation techniques, our method supports two-step inference without sacrificing performance, improving practicality for real-time deployment. Across three simulation benchmarks--RoboTwin2.0, Adroit, and MetaWorld--PocketDP3 achieves state-of-the-art performance with fewer than 1% of the parameters of prior methods, while also accelerating inference. Real-world experiments further demonstrate the practicality and transferability of our method in real-world settings. Code will be released.", "AI": {"tldr": "PocketDP3\u662f\u4e00\u79cd\u65b0\u7684\u8f7b\u91cf\u7ea73D\u6269\u6563\u7b56\u7565\uff0c\u901a\u8fc7\u91c7\u7528Diffusion Mixer\u5927\u5e45\u51cf\u5c11\u53c2\u6570\uff0c\u5e76\u5728\u591a\u4e2a\u4eff\u771f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u9886\u5148\u6027\u80fd\u3002", "motivation": "\u89c2\u5bdf\u5230\u73b0\u67093D\u89c6\u89c9\u6269\u6563\u7b56\u7565\u4e2d\u7684\u89e3\u7801\u5668\u53c2\u6570\u6d6a\u8d39\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u7d27\u51d1\u573a\u666f\u8868\u793a\u7684\u65b0\u67b6\u6784\u3002", "method": "\u63d0\u51fa\u4e86PocketDP3\uff0c\u4e00\u79cd\u57fa\u4e8eMLP-Mixer\u5757\u7684\u8f7b\u91cf\u7ea7Diffusion Mixer\uff08DiM\uff09\uff0c\u53d6\u4ee3\u4e86\u4e4b\u524d\u65b9\u6cd5\u4e2d\u91cd\u578b\u6761\u4ef6U-Net\u89e3\u7801\u5668\uff0c\u5b9e\u73b0\u4e86\u53c2\u6570\u9ad8\u6548\u878d\u5408\u3002", "result": "PocketDP3\u5728RoboTwin2.0\u3001Adroit\u548cMetaWorld\u7b49\u4e09\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u53cc\u6b65\u63a8\u7406\u4e14\u4e0d\u635f\u5931\u6027\u80fd\u3002", "conclusion": "PocketDP3\u5728\u4e09\u4e2a\u4eff\u771f\u57fa\u51c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u53c2\u6570\u91cf\u4e0d\u8db31%\uff0c\u540c\u65f6\u52a0\u5feb\u4e86\u63a8\u7406\u901f\u5ea6\u3002"}}
{"id": "2601.22074", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.22074", "abs": "https://arxiv.org/abs/2601.22074", "authors": ["Kevin Zakka", "Qiayuan Liao", "Brent Yi", "Louis Le Lay", "Koushil Sreenath", "Pieter Abbeel"], "title": "mjlab: A Lightweight Framework for GPU-Accelerated Robot Learning", "comment": "Code is available at https://github.com/mujocolab/mjlab", "summary": "We present mjlab, a lightweight, open-source framework for robot learning that combines GPU-accelerated simulation with composable environments and minimal setup friction. mjlab adopts the manager-based API introduced by Isaac Lab, where users compose modular building blocks for observations, rewards, and events, and pairs it with MuJoCo Warp for GPU-accelerated physics. The result is a framework installable with a single command, requiring minimal dependencies, and providing direct access to native MuJoCo data structures. mjlab ships with reference implementations of velocity tracking, motion imitation, and manipulation tasks.", "AI": {"tldr": "mjlab\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u6613\u4e8e\u5b89\u88c5\u4e14\u5177\u5907GPU\u52a0\u901f\u7684\u5f00\u6e90\u673a\u5668\u4eba\u5b66\u4e60\u6846\u67b6\uff0c\u63d0\u4f9b\u7ec4\u5408\u5316\u73af\u5883\u548c\u591a\u79cd\u53c2\u8003\u5b9e\u73b0\u3002", "motivation": "\u65e8\u5728\u964d\u4f4e\u673a\u5668\u4eba\u5b66\u4e60\u6846\u67b6\u7684\u5b89\u88c5\u548c\u4f7f\u7528\u590d\u6742\u6027\uff0c\u63d0\u4f9b\u5feb\u901f\u8bbf\u95eeMuJoCo\u6570\u636e\u7ed3\u6784\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u7ba1\u7406\u5668\u7684API\uff0c\u7ed3\u5408MuJoCo Warp\u8fdb\u884c\u7269\u7406\u52a0\u901f\uff0c\u7528\u6237\u53ef\u4ee5\u8f7b\u677e\u7ec4\u5408\u89c2\u5bdf\u3001\u5956\u52b1\u548c\u4e8b\u4ef6\u7684\u6a21\u5757\u5316\u6784\u5efa\u5757\u3002", "result": "mjlab\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u5355\u4e00\u547d\u4ee4\u5b89\u88c5\uff0c\u5e76\u9644\u5e26\u4e86\u591a\u79cd\u53c2\u8003\u5b9e\u73b0\uff0c\u5305\u62ec\u901f\u5ea6\u8ddf\u8e2a\u3001\u52a8\u4f5c\u6a21\u4eff\u548c\u64cd\u4f5c\u4efb\u52a1\u3002", "conclusion": "mjlab\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u5f00\u6e90\u7684\u673a\u5668\u4eba\u5b66\u4e60\u6846\u67b6\uff0c\u5177\u5907GPU\u52a0\u901f\u7684\u6a21\u62df\u548c\u6a21\u5757\u5316\u73af\u5883\uff0c\u6781\u5927\u5730\u7b80\u5316\u4e86\u8bbe\u7f6e\u8fc7\u7a0b\u3002"}}
{"id": "2601.22090", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.22090", "abs": "https://arxiv.org/abs/2601.22090", "authors": ["Runsheng Wang", "Katelyn Lee", "Xinyue Zhu", "Lauren Winterbottom", "Dawn M. Nilsen", "Joel Stein", "Matei Ciocarlie"], "title": "ReactEMG Stroke: Healthy-to-Stroke Few-shot Adaptation for sEMG-Based Intent Detection", "comment": null, "summary": "Surface electromyography (sEMG) is a promising control signal for assist-as-needed hand rehabilitation after stroke, but detecting intent from paretic muscles often requires lengthy, subject-specific calibration and remains brittle to variability. We propose a healthy-to-stroke adaptation pipeline that initializes an intent detector from a model pretrained on large-scale able-bodied sEMG, then fine-tunes it for each stroke participant using only a small amount of subject-specific data. Using a newly collected dataset from three individuals with chronic stroke, we compare adaptation strategies (head-only tuning, parameter-efficient LoRA adapters, and full end-to-end fine-tuning) and evaluate on held-out test sets that include realistic distribution shifts such as within-session drift, posture changes, and armband repositioning. Across conditions, healthy-pretrained adaptation consistently improves stroke intent detection relative to both zero-shot transfer and stroke-only training under the same data budget; the best adaptation methods improve average transition accuracy from 0.42 to 0.61 and raw accuracy from 0.69 to 0.78. These results suggest that transferring a reusable healthy-domain EMG representation can reduce calibration burden while improving robustness for real-time post-stroke intent detection.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5065\u5eb7\u5230\u4e2d\u98ce\u7684\u9002\u5e94\u7ba1\u9053\uff0c\u901a\u8fc7\u5065\u5eb7\u4eba\u7fa4\u7684\u5927\u89c4\u6a21sEMG\u6a21\u578b\u521d\u59cb\u5316\u610f\u56fe\u68c0\u6d4b\u5668\uff0c\u5e76\u4ec5\u4f7f\u7528\u5c11\u91cf\u4e2a\u4f53\u6570\u636e\u8fdb\u884c\u5fae\u8c03\uff0c\u4ece\u800c\u63d0\u9ad8\u4e2d\u98ce\u540e\u5eb7\u590d\u7684\u610f\u56fe\u68c0\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u7814\u7a76\u5e0c\u671b\u901a\u8fc7\u5229\u7528\u5065\u5eb7\u4eba\u7fa4\u7684sEMG\u6a21\u578b\u6765\u63d0\u9ad8\u4e2d\u98ce\u540e\u5eb7\u590d\u4e2d\u5bf9\u4e8e\u610f\u56fe\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u51cf\u5c11\u4f20\u7edf\u65b9\u6cd5\u4e2d\u6240\u9700\u7684\u4e2a\u4f53\u6821\u51c6\u65f6\u95f4\u3002", "method": "\u901a\u8fc7\u5c06\u5927\u89c4\u6a21\u7684\u5065\u5eb7\u4eba\u7fa4sEMG\u9884\u8bad\u7ec3\u6a21\u578b\u521d\u59cb\u5316\uff0c\u5e76\u4f7f\u7528\u5c11\u91cf\u7684\u4e2d\u98ce\u53c2\u4e0e\u8005\u7684\u7279\u5b9a\u6570\u636e\u8fdb\u884c\u5fae\u8c03\u3002\u6bd4\u8f83\u4e86\u4e0d\u540c\u7684\u9002\u5e94\u7b56\u7565\uff0c\u5305\u62ec\u5934\u90e8\u8c03\u4f18\u3001\u53c2\u6570\u9ad8\u6548LoRA\u9002\u914d\u5668\u4ee5\u53ca\u5168\u7aef\u5230\u7aef\u7684\u5fae\u8c03\u3002", "result": "\u6700\u597d\u7684\u9002\u5e94\u65b9\u6cd5\u4f7f\u5e73\u5747\u8f6c\u6362\u51c6\u786e\u7387\u4ece0.42\u63d0\u9ad8\u81f30.61\uff0c\u539f\u59cb\u51c6\u786e\u7387\u4ece0.69\u63d0\u9ad8\u81f30.78\uff0c\u5c55\u793a\u4e86\u5065\u5eb7\u9886\u57dfEMG\u8868\u5f81\u5728\u4e2d\u98ce\u610f\u56fe\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5065\u5eb7\u9884\u8bad\u7ec3\u7684\u9002\u5e94\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u4e2d\u98ce\u610f\u56fe\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u964d\u4f4e\u4e86\u6821\u51c6\u8d1f\u62c5\uff0c\u5e76\u589e\u5f3a\u4e86\u5b9e\u65f6\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2601.22153", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.22153", "abs": "https://arxiv.org/abs/2601.22153", "authors": ["Haozhe Xie", "Beichen Wen", "Jiarui Zheng", "Zhaoxi Chen", "Fangzhou Hong", "Haiwen Diao", "Ziwei Liu"], "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation", "comment": "Project Page: https://www.infinitescript.com/project/dynamic-vla/ GitHub: https://github.com/hzxie/DynamicVLA", "summary": "Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.", "AI": {"tldr": "\u52a8\u6001\u64cd\u4f5c\u4ecd\u7136\u662fVLA\u6a21\u578b\u7684\u6311\u6218\uff0cDynamicVLA\u901a\u8fc7\u65b0\u6846\u67b6\u548cDOM\u57fa\u51c6\u63d0\u5347\u4e86\u52a8\u6001\u7269\u4f53\u64cd\u4f5c\u7684\u901f\u5ea6\u4e0e\u7cbe\u51c6\u5ea6\u3002", "motivation": "\u5c3d\u7ba1VLA\u6a21\u578b\u5728\u9759\u6001\u64cd\u4f5c\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u52a8\u6001\u573a\u666f\u4e2d\u5374\u9762\u4e34\u5feb\u901f\u611f\u77e5\u3001\u65f6\u95f4\u9884\u5224\u548c\u6301\u7eed\u63a7\u5236\u7684\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\u6765\u5e94\u5bf9\u52a8\u6001\u7269\u4f53\u64cd\u4f5c\u3002", "method": "\u8be5\u6846\u67b6\u91c7\u7528\u4e86\u4e00\u79cd\u7d27\u51d1\u76840.4B VLA\uff0c\u5229\u7528\u5377\u79ef\u89c6\u89c9\u7f16\u7801\u5668\u8fdb\u884c\u7a7a\u95f4\u9ad8\u6548\u3001\u7ed3\u6784\u4e0a\u771f\u5b9e\u7684\u7f16\u7801\uff0c\u652f\u6301\u5feb\u901f\u7684\u591a\u6a21\u6001\u63a8\u7406\uff0c\u540c\u65f6\u5b9e\u73b0\u91cd\u53e0\u63a8\u7406\u548c\u6267\u884c\uff0c\u51cf\u5c11\u5ef6\u8fdf\uff0c\u53ca\u65f6\u9002\u5e94\u7269\u4f53\u8fd0\u52a8\uff0c\u4ee5\u53ca\u901a\u8fc7\u65f6\u95f4\u5bf9\u9f50\u7684\u884c\u52a8\u6267\u884c\u5f25\u5408\u611f\u77e5\u4e0e\u6267\u884c\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "result": "\u901a\u8fc7\u5efa\u7acb\u52a8\u6001\u7269\u4f53\u64cd\u4f5c\uff08DOM\uff09\u57fa\u51c6\uff0c\u6536\u96c6\u4e86200K\u4e2a\u5408\u6210\u7684\u52a8\u6001\u64cd\u4f5c\u573a\u666f\u548c\u5bf9\u8c61\uff0c\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u8bc4\u4f30\uff0c\u8868\u660eDynamicVLA\u5728\u591a\u4e2a\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "DynamicVLA\u7cfb\u7edf\u5728\u52a8\u6001\u7269\u4f53\u64cd\u4f5c\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u7684\u53cd\u5e94\u901f\u5ea6\u3001\u611f\u77e5\u548c\u6cdb\u5316\u80fd\u529b\u7684\u63d0\u5347\uff0c\u6210\u4e3a\u52a8\u6001\u7269\u4f53\u64cd\u4f5c\u7684\u7edf\u4e00\u6846\u67b6\u3002"}}
