<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 11]
- [cs.HC](#cs.HC) [Total: 15]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Gaussian Variational Inference with Non-Gaussian Factors for State Estimation: A UWB Localization Case Study](https://arxiv.org/abs/2512.19855)
*Andrew Stirling,Mykola Lukashchuk,Dmitry Bagaev,Wouter Kouw,James R. Forbes*

Main category: cs.RO

TL;DR: 本研究扩展了ESGVI算法，使其能够在矩阵李群上进行状态估计，并引入因子处理重尾和偏斜噪声分布，验证了算法在UWB定位中的有效性。


<details>
  <summary>Details</summary>
Motivation: 提高状态估计的精度和一致性，特别是在存在非视距和多径效应的超宽带定位场景中。

Method: 将ESGVI算法推广到矩阵李群，添加因子处理重尾和偏斜噪声，保留了算法的稀疏性和无导数特性。

Result: 扩展的ESGVI算法在UWB定位实验中验证了其改进的准确性和可比的一致性。

Conclusion: 所提出的方法在具有丰富NLOS测量的UWB实验中表现出更高的准确性，同时保持了ESGVI的稀疏和无导数结构。

Abstract: This letter extends the exactly sparse Gaussian variational inference (ESGVI) algorithm for state estimation in two complementary directions. First, ESGVI is generalized to operate on matrix Lie groups, enabling the estimation of states with orientation components while respecting the underlying group structure. Second, factors are introduced to accommodate heavy-tailed and skewed noise distributions, as commonly encountered in ultra-wideband (UWB) localization due to non-line-of-sight (NLOS) and multipath effects. Both extensions are shown to integrate naturally within the ESGVI framework while preserving its sparse and derivative-free structure. The proposed approach is validated in a UWB localization experiment with NLOS-rich measurements, demonstrating improved accuracy and comparable consistency. Finally, a Python implementation within a factor-graph-based estimation framework is made open-source (https://github.com/decargroup/gvi_ws) to support broader research use.

</details>


### [2] [A Time-efficient Prioritised Scheduling Algorithm to Optimise Initial Flock Formation of Drones](https://arxiv.org/abs/2512.19914)
*Sujan Warnakulasooriya,Andreas Willig,Xiaobing Wu*

Main category: cs.RO

TL;DR: 本文提出了一种时间高效的优先级调度算法，通过为每个无人机分配优先级来改善无人机编队的初始形成过程，从而生成无碰撞的轨迹。


<details>
  <summary>Details</summary>
Motivation: 随着无人机应用的扩展，编队能力增强但在初始形成过程中面临挑战，现有算法在效率和可扩展性方面存在不足。

Method: 通过为每个无人机根据潜在碰撞数量和达到目标位置的可能性分配优先级，并计算适当的延迟以确保无碰撞路径。

Result: 仿真结果表明，提出的算法在性能和计算效率上均优于现有的基于耦合度的优先级规划方法。

Conclusion: 提出的算法在生成无碰撞轨迹方面表现优于现有方法，能够处理多达5000架无人机的编队。

Abstract: Drone applications continue to expand across various domains, with flocking offering enhanced cooperative capabilities but introducing significant challenges during initial formation. Existing flocking algorithms often struggle with efficiency and scalability, particularly when potential collisions force drones into suboptimal trajectories. This paper presents a time-efficient prioritised scheduling algorithm that improves the initial formation process of drone flocks. The method assigns each drone a priority based on its number of potential collisions and its likelihood of reaching its target position without permanently obstructing other drones. Using this hierarchy, each drone computes an appropriate delay to ensure a collision-free path. Simulation results show that the proposed algorithm successfully generates collision-free trajectories for flocks of up to 5000 drones and outperforms the coupling-degree-based heuristic prioritised planning method (CDH-PP) in both performance and computational efficiency.

</details>


### [3] [Bring My Cup! Personalizing Vision-Language-Action Models with Visual Attentive Prompting](https://arxiv.org/abs/2512.20014)
*Sangoh Lee,Sangwoo Mo,Wook-Shin Han*

Main category: cs.RO

TL;DR: 本文提出了一种名为视觉关注提示（VAP）的新方法，旨在帮助机器人准确识别和操作用户特定的个人物品，超越了传统的视觉-语言-行动（VLA）模型的局限。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型在处理个性化指令时存在困难，特别是当需要识别与训练集视觉相似的特定对象时，因此需要一种新方法来应对用户特定物品的操作。

Method: VAP作为一种感知适配器，通过使用参考图像作为非参数视觉记忆，利用开放词汇检测和基于嵌入的匹配来定位个人物体，并通过高亮显示目标物体和重写指令来注入视觉提示。

Result: 通过构建两个人工模拟基准和一个真实世界的桌面基准，VAP在多个机器人和任务中的个性化操作表现出色，成功率和正确物体操作均优于其他基准。

Conclusion: 实验结果表明，VAP方法在特定物品操作任务中优于传统策略和基础学习模型，从而有效提升了机器人的个性化物品操作能力。

Abstract: While Vision-Language-Action (VLA) models generalize well to generic instructions, they struggle with personalized commands such as "bring my cup", where the robot must act on one specific instance among visually similar objects. We study this setting of manipulating personal objects, in which a VLA must identify and control a user-specific object unseen during training using only a few reference images. To address this challenge, we propose Visual Attentive Prompting (VAP), a simple-yet-effective training-free perceptual adapter that equips frozen VLAs with top-down selective attention. VAP treats the reference images as a non-parametric visual memory, grounds the personal object in the scene through open-vocabulary detection and embedding-based matching, and then injects this grounding as a visual prompt by highlighting the object and rewriting the instruction. We construct two simulation benchmarks, Personalized-SIMPLER and Personalized-VLABench, and a real-world tabletop benchmark to evaluate personalized manipulation across multiple robots and tasks. Experiments show that VAP consistently outperforms generic policies and token-learning baselines in both success rate and correct-object manipulation, helping to bridge the gap between semantic understanding and instance-level control.

</details>


### [4] [LoLA: Long Horizon Latent Action Learning for General Robot Manipulation](https://arxiv.org/abs/2512.20166)
*Xiaofan Wang,Xingyu Gao,Jianlong Fu,Zuolei Li,Dean Fortier,Galen Mullins,Andrey Kolobov,Baining Guo*

Main category: cs.RO

TL;DR: 提出了一种新的框架LoLA，通过长期的多视角观察和机器人本体感知，提升机器人在长时间范围内的操作能力。


<details>
  <summary>Details</summary>
Motivation: 提高机器人在长时间范围的操作任务中的表现，解决现有VLA模型对历史信息的忽视。

Method: 构建了一个使用Vision-Language模型编码上下文特征的框架，并引入State-Aware Latent Re-representation模块，将视觉和语言指令转换为可执行的机器人动作。

Result: LoLA框架能有效地处理长时间范围的语言指导机器人操作任务，整合了多视角观察和机器人本体感知，以生成连贯的操作序列。

Conclusion: LoLA在长时间范围的操作任务中显著超越了现有的最先进方法，证明其有效性。

Abstract: The capability of performing long-horizon, language-guided robotic manipulation tasks critically relies on leveraging historical information and generating coherent action sequences. However, such capabilities are often overlooked by existing Vision-Language-Action (VLA) models. To solve this challenge, we propose LoLA (Long Horizon Latent Action Learning), a framework designed for robot manipulation that integrates long-term multi-view observations and robot proprioception to enable multi-step reasoning and action generation. We first employ Vision-Language Models to encode rich contextual features from historical sequences and multi-view observations. We further introduces a key module, State-Aware Latent Re-representation, which transforms visual inputs and language commands into actionable robot motion space. Unlike existing VLA approaches that merely concatenate robot proprioception (e.g., joint angles) with VL embeddings, this module leverages such robot states to explicitly ground VL representations in physical scale through a learnable "embodiment-anchored" latent space. We trained LoLA on diverse robotic pre-training datasets and conducted extensive evaluations on simulation benchmarks (SIMPLER and LIBERO), as well as two real-world tasks on Franka and Bi-Manual Aloha robots. Results show that LoLA significantly outperforms prior state-of-the-art methods (e.g., pi0), particularly in long-horizon manipulation tasks.

</details>


### [5] [Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation](https://arxiv.org/abs/2512.20188)
*Teqiang Zou,Hongliang Zeng,Yuxuan Nong,Yifan Li,Kehui Liu,Haotian Yang,Xinyang Ling,Xin Li,Lianyang Ma*

Main category: cs.RO

TL;DR: DuoCore-FS是一个新的异步框架，通过设立快慢通道来提高机器人操控性能，显著提升任务成功率和反应速度。


<details>
  <summary>Details</summary>
Motivation: 解决目前视觉-语言-行动系统在执行过程中的同步限制，提高机器人操作的实时性和控制稳定性。

Method: DuoCore-FS采用快慢通道设计，利用潜在表示缓冲区和全身动作标记器来连接VLM和动作专家，确保高频动作生成与语义推理的协同。

Result: 提出了一种异步的Fast-Slow VLA框架（DuoCore-FS），实现高效的动作生成和丰富的推理。

Conclusion: DuoCore-FS能有效实现机器人动作的快速生成与语言推理，同时保持统一的政策学习，展示了比以往模型更优的实际应用效果。

Abstract: Most Vision-Language-Action (VLA) systems integrate a Vision-Language Model (VLM) for semantic reasoning with an action expert generating continuous action signals, yet both typically run at a single unified frequency. As a result, policy performance is constrained by the low inference speed of large VLMs. This mandatory synchronous execution severely limits control stability and real-time performance in whole-body robotic manipulation, which involves more joints, larger motion spaces, and dynamically changing views. We introduce a truly asynchronous Fast-Slow VLA framework (DuoCore-FS), organizing the system into a fast pathway for high-frequency action generation and a slow pathway for rich VLM reasoning. The system is characterized by two key features. First, a latent representation buffer bridges the slow and fast systems. It stores instruction semantics and action-reasoning representation aligned with the scene-instruction context, providing high-level guidance to the fast pathway. Second, a whole-body action tokenizer provides a compact, unified representation of whole-body actions. Importantly, the VLM and action expert are still jointly trained end-to-end, preserving unified policy learning while enabling asynchronous execution. DuoCore-FS supports a 3B-parameter VLM while achieving 30 Hz whole-body action-chunk generation, approximately three times as fast as prior VLA models with comparable model sizes. Real-world whole-body manipulation experiments demonstrate improved task success rates and significantly enhanced responsiveness compared to synchronous Fast-Slow VLA baselines. The implementation of DuoCore-FS, including training, inference, and deployment, is provided to commercial users by Astribot as part of the Astribot robotic platform.

</details>


### [6] [UrbanV2X: A Multisensory Vehicle-Infrastructure Dataset for Cooperative Navigation in Urban Areas](https://arxiv.org/abs/2512.20224)
*Qijun Qin,Ziqi Zhang,Yihan Zhong,Feng Huang,Xikun Liu,Runzhi Hu,Hang Chen,Wei Hu,Dongzhe Su,Jun Zhang,Hoi-Fung Ng,Weisong Wen*

Main category: cs.RO

TL;DR: UrbanV2X数据集通过车辆与基础设施的传感器信息共享，支持在复杂城市环境中实现自主驾驶研究。


<details>
  <summary>Details</summary>
Motivation: 推动全面自主驾驶与复杂城市环境中的车辆-基础设施合作导航研究

Method: 收集并同步多传感器数据

Result: 提供了一个全面的多传感器数据集UrbanV2X，涵盖车辆与路边基础设施的数据，并公开可用

Conclusion: UrbanV2X数据集为智能出行应用研究提供了宝贵的基础，并鼓励在密集城市环境中开展进一步的探索。

Abstract: Due to the limitations of a single autonomous vehicle, Cellular Vehicle-to-Everything (C-V2X) technology opens a new window for achieving fully autonomous driving through sensor information sharing. However, real-world datasets supporting vehicle-infrastructure cooperative navigation in complex urban environments remain rare. To address this gap, we present UrbanV2X, a comprehensive multisensory dataset collected from vehicles and roadside infrastructure in the Hong Kong C-V2X testbed, designed to support research on smart mobility applications in dense urban areas. Our onboard platform provides synchronized data from multiple industrial cameras, LiDARs, 4D radar, ultra-wideband (UWB), IMU, and high-precision GNSS-RTK/INS navigation systems. Meanwhile, our roadside infrastructure provides LiDAR, GNSS, and UWB measurements. The entire vehicle-infrastructure platform is synchronized using the Precision Time Protocol (PTP), with sensor calibration data provided. We also benchmark various navigation algorithms to evaluate the collected cooperative data. The dataset is publicly available at https://polyu-taslab.github.io/UrbanV2X/.

</details>


### [7] [KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System](https://arxiv.org/abs/2512.20299)
*Zhongyu Xia,Wenhao Chen,Yongtao Wang,Ming-Hsuan Yang*

Main category: cs.RO

TL;DR: KnowVal是一个先进的自动驾驶系统，通过开放世界感知和知识检索的协同集成，实现视觉语言推理。


<details>
  <summary>Details</summary>
Motivation: 现有的方法过于依赖数据驱动学习，难以捕捉决策的复杂逻辑。

Method: 构建全面的驾驶知识图，编码交通法规、驾驶原则和伦理规范，并开发基于LLM的检索机制和人类偏好数据集。

Result: 实验结果显示，KnowVal在nuScenes上实现了最低的碰撞率，并在Bench2Drive上取得了最先进的结果。

Conclusion: KnowVal显著提高了规划性能，并在保持与现有架构兼容的同时，达成了最低的碰撞率和最先进的Bench2Drive结果。

Abstract: Visual-language reasoning, driving knowledge, and value alignment are essential for advanced autonomous driving systems. However, existing approaches largely rely on data-driven learning, making it difficult to capture the complex logic underlying decision-making through imitation or limited reinforcement rewards. To address this, we propose KnowVal, a new autonomous driving system that enables visual-language reasoning through the synergistic integration of open-world perception and knowledge retrieval. Specifically, we construct a comprehensive driving knowledge graph that encodes traffic laws, defensive driving principles, and ethical norms, complemented by an efficient LLM-based retrieval mechanism tailored for driving scenarios. Furthermore, we develop a human-preference dataset and train a Value Model to guide interpretable, value-aligned trajectory assessment. Experimental results show that our method substantially improves planning performance while remaining compatible with existing architectures. Notably, KnowVal achieves the lowest collision rate on nuScenes and state-of-the-art results on Bench2Drive.

</details>


### [8] [Pneumatic bladder links with wide range of motion joints for articulated inflatable robots](https://arxiv.org/abs/2512.20322)
*Katsu Uchiyama,Ryuma Niiyama*

Main category: cs.RO

TL;DR: 研究了可充气机器人的多种应用，提出了一种新型的关节结构，表现出良好的承载能力和灵活性。


<details>
  <summary>Details</summary>
Motivation: 探索可充气机器人在不同应用中的潜力，并改善其运动能力和承载能力。

Method: 开发了一种由多个气动泡囊链接组成的可充气机器人，使用滚动接触关节（Hillberry joints）作为连接，提升了灵活性和活动范围。

Result: 通过该机制实现了3自由度手臂移动500克负载，并用2自由度和1自由度手臂提升了3.4公斤和5公斤的负载，同时验证了腿部运动的有效性。

Conclusion: 该研究提出的充气机器人及其新颖的关节设计展现了广泛的应用潜力，能够有效承载不同重量的负载并实现多自由度的运动。

Abstract: Exploration of various applications is the frontier of research on inflatable robots. We proposed an articulated robots consisting of multiple pneumatic bladder links connected by rolling contact joints called Hillberry joints. The bladder link is made of a double-layered structure of tarpaulin sheet and polyurethane sheet, which is both airtight and flexible in shape. The integration of the Hilberry joint into an inflatable robot is also a new approach. The rolling contact joint allows wide range of motion of $\pm 150 ^{\circ}$, the largest among the conventional inflatable joints. Using the proposed mechanism for inflatable robots, we demonstrated moving a 500 g payload with a 3-DoF arm and lifting 3.4 kg and 5 kg payloads with 2-DoF and 1-DoF arms, respectively. We also experimented with a single 3-DoF inflatable leg attached to a dolly to show that the proposed structure worked for legged locomotion.

</details>


### [9] [FAR-AVIO: Fast and Robust Schur-Complement Based Acoustic-Visual-Inertial Fusion Odometry with Sensor Calibration](https://arxiv.org/abs/2512.20355)
*Hao Wei,Peiji Wang,Qianhao Wang,Tong Qin,Fei Gao,Yulin Si*

Main category: cs.RO

TL;DR: 提出了一种名为FAR-AVIO的声学-视觉-惯性测程框架，针对水下机器人进行优化，克服传统方法中的计算负担。


<details>
  <summary>Details</summary>
Motivation: 水下环境严峻挑战视觉-惯性测程系统的性能，现有的声学-视觉-惯性融合方法虽然准确，但计算开销极大，迫切需要优化以适应低资源平台。

Method: 基于Schur互补的扩展卡尔曼滤波器，对姿态与地标状态进行联合优化，同时采用自适应加权调整和可靠性评估模块，以提供在线健康监测与校准。

Result: FAR-AVIO在数值模拟和实际水下实验中表现优异，展现了高定位精度和计算效率，证明了其在现实应用中的有效性。

Conclusion: FAR-AVIO在水下SLAM定位精度和计算效率上优于现有方法，适用于低功耗嵌入式平台，推动了水下机器人技术的发展。

Abstract: Underwater environments impose severe challenges to visual-inertial odometry systems, as strong light attenuation, marine snow and turbidity, together with weakly exciting motions, degrade inertial observability and cause frequent tracking failures over long-term operation. While tightly coupled acoustic-visual-inertial fusion, typically implemented through an acoustic Doppler Velocity Log (DVL) integrated with visual-inertial measurements, can provide accurate state estimation, the associated graph-based optimization is often computationally prohibitive for real-time deployment on resource-constrained platforms. Here we present FAR-AVIO, a Schur-Complement based, tightly coupled acoustic-visual-inertial odometry framework tailored for underwater robots. FAR-AVIO embeds a Schur complement formulation into an Extended Kalman Filter(EKF), enabling joint pose-landmark optimization for accuracy while maintaining constant-time updates by efficiently marginalizing landmark states. On top of this backbone, we introduce Adaptive Weight Adjustment and Reliability Evaluation(AWARE), an online sensor health module that continuously assesses the reliability of visual, inertial and DVL measurements and adaptively regulates their sigma weights, and we develop an efficient online calibration scheme that jointly estimates DVL-IMU extrinsics, without dedicated calibration manoeuvres. Numerical simulations and real-world underwater experiments consistently show that FAR-AVIO outperforms state-of-the-art underwater SLAM baselines in both localization accuracy and computational efficiency, enabling robust operation on low-power embedded platforms. Our implementation has been released as open source software at https://far-vido.gitbook.io/far-vido-docs.

</details>


### [10] [Drift-Corrected Monocular VIO and Perception-Aware Planning for Autonomous Drone Racing](https://arxiv.org/abs/2512.20475)
*Maulana Bisyir Azhari,Donghun Han,Je In You,Sungjun Park,David Hyunchul Shim*

Main category: cs.RO

TL;DR: 本论文展示了一种能够在传感器限制下进行高效无人机比赛的系统，通过融合视觉和定位数据来修正漂移，并实现了多个比赛类别的优异成绩。


<details>
  <summary>Details</summary>
Motivation: 解决无人机在快速飞行和激烈机动过程中因传感器限制导致的漂移问题，确保高性能和准确性。

Method: 采用卡尔曼滤波器将视觉惯性测量输出与YOLO检测得到的全局位置融合，使用感知意识规划器生成功能明确的轨迹。

Result: 本论文介绍了在阿布扎比无人驾驶赛车联盟（A2RL）与无人机冠军联赛（DCL）比赛中，使用单一相机和低质量惯性测量单元进行高速度自动驾驶无人机比赛的系统。该系统成功地通过将视觉惯性测量输出与基于YOLO的网关检测器的全局定位测量结合，并利用卡尔曼滤波器进行漂移修正。

Conclusion: 我们的系统在时间范围内表现出色，通过视觉和全局位置的融合，实现了高效的无人机自主飞行，且在多个比赛中获得奖项。

Abstract: The Abu Dhabi Autonomous Racing League(A2RL) x Drone Champions League competition(DCL) requires teams to perform high-speed autonomous drone racing using only a single camera and a low-quality inertial measurement unit -- a minimal sensor set that mirrors expert human drone racing pilots. This sensor limitation makes the system susceptible to drift from Visual-Inertial Odometry (VIO), particularly during long and fast flights with aggressive maneuvers. This paper presents the system developed for the championship, which achieved a competitive performance. Our approach corrected VIO drift by fusing its output with global position measurements derived from a YOLO-based gate detector using a Kalman filter. A perception-aware planner generated trajectories that balance speed with the need to keep gates visible for the perception system. The system demonstrated high performance, securing podium finishes across multiple categories: third place in the AI Grand Challenge with top speed of 43.2 km/h, second place in the AI Drag Race with over 59 km/h, and second place in the AI Multi-Drone Race. We detail the complete architecture and present a performance analysis based on experimental data from the competition, contributing our insights on building a successful system for monocular vision-based autonomous drone flight.

</details>


### [11] [LightTact: A Visual-Tactile Fingertip Sensor for Deformation-Independent Contact Sensing](https://arxiv.org/abs/2512.20591)
*Changyi Lin,Boda Huo,Mingyang Yu,Emily Ruppel,Bingqing Chen,Jonathan Francis,Ding Zhao*

Main category: cs.RO

TL;DR: LightTact是一种新型的视觉触觉传感器，可在无变形的情况下直接可视化接触，适用于轻触交互。


<details>
  <summary>Details</summary>
Motivation: 现有触觉传感器主要依赖变形来推断接触，难以感知轻接触交互，因此开发了一种新的传感器来解决这一问题。

Method: LightTact采用环境屏蔽的光学配置，抑制非接触区域的光照，只传输真实接触产生的散射光。

Result: 提出了一种名为LightTact的视觉触觉传感器，能够在无宏观表面变形的情况下检测接触。

Conclusion: LightTact凭借其光学原理，能够有效地检测轻微接触，并在多种应用中表现出良好的性能。

Abstract: Contact often occurs without macroscopic surface deformation, such as during interaction with liquids, semi-liquids, or ultra-soft materials. Most existing tactile sensors rely on deformation to infer contact, making such light-contact interactions difficult to perceive robustly. To address this, we present LightTact, a visual-tactile fingertip sensor that makes contact directly visible via a deformation-independent, optics-based principle. LightTact uses an ambient-blocking optical configuration that suppresses both external light and internal illumination at non-contact regions, while transmitting only the diffuse light generated at true contacts. As a result, LightTact produces high-contrast raw images in which non-contact pixels remain near-black (mean gray value < 3) and contact pixels preserve the natural appearance of the contacting surface. Built on this, LightTact achieves accurate pixel-level contact segmentation that is robust to material properties, contact force, surface appearance, and environmental lighting. We further integrate LightTact on a robotic arm and demonstrate manipulation behaviors driven by extremely light contact, including water spreading, facial-cream dipping, and thin-film interaction. Finally, we show that LightTact's spatially aligned visual-tactile images can be directly interpreted by existing vision-language models, enabling resistor value reasoning for robotic sorting.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [12] [Bidirectional human-AI collaboration in brain tumour assessments improves both expert human and AI agent performance](https://arxiv.org/abs/2512.19707)
*James K Ruffle,Samia Mohinta,Guilherme Pombo,Asthik Biswas,Alan Campbell,Indran Davagnanam,David Doig,Ahmed Hamman,Harpreet Hyare,Farrah Jabeen,Emma Lim,Dermot Mallon,Stephanie Owen,Sophie Wilkinson,Sebastian Brandner,Parashkev Nachev*

Main category: cs.HC

TL;DR: 本研究显示人类与AI的合作可在医学影像中显著提升脑肿瘤患者的诊断准确性与元认知能力。


<details>
  <summary>Details</summary>
Motivation: 研究人类与AI的合作如何提升医疗诊断的准确性，以探索AI在医疗中更有效的应用方式。

Method: 通过对比分析人类放射科医生与AI代理在脑肿瘤患者影像诊断中的表现，评估两者之间的相互支持效果。

Result: 本研究探讨了人类与人工智能（AI）的合作在医学影像中对脑肿瘤患者的影响，发现无论是人类专家还是AI代理，在相互支持下，双方的准确性和元认知能力都有所提升。研究表明，通过人类的支持，AI呈现出最高的效益。

Conclusion: 人工智能在医疗领域的最大价值可能在于增强人类智能而非取代它。

Abstract: The benefits of artificial intelligence (AI) human partnerships-evaluating how AI agents enhance expert human performance-are increasingly studied. Though rarely evaluated in healthcare, an inverse approach is possible: AI benefiting from the support of an expert human agent. Here, we investigate both human-AI clinical partnership paradigms in the magnetic resonance imaging-guided characterisation of patients with brain tumours. We reveal that human-AI partnerships improve accuracy and metacognitive ability not only for radiologists supported by AI, but also for AI agents supported by radiologists. Moreover, the greatest patient benefit was evident with an AI agent supported by a human one. Synergistic improvements in agent accuracy, metacognitive performance, and inter-rater agreement suggest that AI can create more capable, confident, and consistent clinical agents, whether human or model-based. Our work suggests that the maximal value of AI in healthcare could emerge not from replacing human intelligence, but from AI agents that routinely leverage and amplify it.

</details>


### [13] [Predicting Student Actions in a Procedural Training Environment](https://arxiv.org/abs/2512.19810)
*Diego Riofrío-Luzcando,Jaime Ramírez,Marta Berrocal-Lobo*

Main category: cs.HC

TL;DR: 该论文提出了一种基于历史日志的集体学生模型，用于预测学生行为并改进智能辅导系统的反馈。


<details>
  <summary>Details</summary>
Motivation: 数据挖掘在用户表现预测方面的潜力尚未充分利用于学生行为预测，尤其在程序培训环境中。

Method: 通过将学生日志分组为簇，并基于簇内事件序列创建扩展自动机，构建学生行为预测模型。

Result: 本文提出了一种集体学生模型，通过分析学生的历史日志来构建，以预测学生在程序培训环境中的行为。

Conclusion: 验证结果显示该模型能够提供合理的预测，并提升针对不同学生类型的辅导反馈效果。

Abstract: Data mining is known to have a potential for predicting user performance. However, there are few studies that explore its potential for predicting student behavior in a procedural training environment. This paper presents a collective student model, which is built from past student logs. These logs are firstly grouped into clusters. Then an extended automaton is created for each cluster based on the sequences of events found in the cluster logs. The main objective of this model is to predict the actions of new students for improving the tutoring feedback provided by an intelligent tutoring system. The proposed model has been validated using student logs collected in a 3D virtual laboratory for teaching biotechnology. As a result of this validation, we concluded that the model can provide reasonably good predictions and can support tutoring feedback that is better adapted to each student type.

</details>


### [14] [How Tech Workers Contend with Hazards of Humanlikeness in Generative AI](https://arxiv.org/abs/2512.19832)
*Mark Díaz,Renee Shelby,Eric Corbett,Andrew Smart*

Main category: cs.HC

TL;DR: 生成式AI的类人特征虽然推动其快速采用，但也引发了对潜在风险的担忧。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI具有人类特征所带来的机遇和风险

Method: 通过对30位专业人士进行焦点小组访谈

Result: 揭示出对人类特征生成式AI的知识环境不稳定，专业人士的不同观点揭示出多种潜在风险

Conclusion: 为了有效降低风险，专业人士需要更全面的支持，包括更清晰的人类特征概念。

Abstract: Generative AI's humanlike qualities are driving its rapid adoption in professional domains. However, this anthropomorphic appeal raises concerns from HCI and responsible AI scholars about potential hazards and harms, such as overtrust in system outputs. To investigate how technology workers navigate these humanlike qualities and anticipate emergent harms, we conducted focus groups with 30 professionals across six job functions (ML engineering, product policy, UX research and design, product management, technology writing, and communications). Our findings reveal an unsettled knowledge environment surrounding humanlike generative AI, where workers' varying perspectives illuminate a range of potential risks for individuals, knowledge work fields, and society. We argue that workers require comprehensive support, including clearer conceptions of ``humanlikeness'' to effectively mitigate these risks. To aid in mitigation strategies, we provide a conceptual map articulating the identified hazards and their connection to conflated notions of ``humanlikeness.''

</details>


### [15] [Visualizing a Collective Student Model for Procedural Training Environments](https://arxiv.org/abs/2512.19885)
*Diego Riofrío-Luzcando,Jaime RamÍrez,Cristian Moral,Angélica de Antonio,Marta Berrocal-Lobo*

Main category: cs.HC

TL;DR: 本研究设计了一个面向2D/3D虚拟培训环境的学生模型可视化工具，旨在提高教师的教学效率和学生的学习表现。


<details>
  <summary>Details</summary>
Motivation: 在2D/3D虚拟环境中，学生互动复杂，需要有效的可视化来帮助分析行为和改进教学策略。

Method: 设计师生模型的可视化以及开发相应的网页工具

Result: 开发的可视化工具可以帮助教师改善教学，增强智能辅导系统的辅导策略，并且通过实验验证了其在3D虚拟实验室中的有效性。

Conclusion: 该工具有效提升了教师的教学方法，并通过实验证明能改善学生在虚拟环境下的表现。

Abstract: Visualization plays a relevant role for discovering patterns in big sets of data. In fact, the most common way to help a human with a pattern interpretation is through a graphic. In 2D/3D virtual environments for procedural training the student interaction is more varied and complex than in traditional e-learning environments. Therefore, the visualization and interpretation of students' behaviors becomes a challenge. This motivated us to design the visualization of a collective student model built from student logs taken from 2D/3D virtual environments for procedural training. This paper presents the design decisions that enable a suitable visualization of this model to instructors as well as a web tool that implements this visualization and is intended: to help instructors to improve their own teaching; and to enhance the tutoring strategy of an Intelligent Tutoring System. Then, this paper illustrates, with three detailed examples, how this tool can be used to those educational purposes. Next, the paper presents an experiment for validating the utility of the tool. In this experiment we show how the tool can help to modify the tutoring strategy of a 3D virtual laboratory. In this way, it is shown that the proposed visualization of the model can serve to improve the performance of students in 2D/3D virtual environments for procedural training.

</details>


### [16] [Free-Will vs Free-Wheel: Understanding Community Accessibility Requirements of Wheelchair Users through Interviews, Participatory Action, and Modeling](https://arxiv.org/abs/2512.19898)
*Hanna Noyce,Emily Olejniczak,Vaskar Raychoudhury,Roger O. Smith,Md Osman Gani*

Main category: cs.HC

TL;DR: 本研究提出并验证了用户-轮椅-环境三维无障碍模型，强调应改善轮椅使用者的社区参与。


<details>
  <summary>Details</summary>
Motivation: 研究旨在识别轮椅用户面临的无障碍挑战，进而开发无障碍模型，以改善残疾人士的社区参与。

Method: 本研究通过深入的定性分析，包括半结构化访谈和参与式行动研究，验证了三维无障碍模型。

Result: 研究验证了多个关于轮椅用户社区访问的假设，并识别了对更无障碍路径规划工具和资源的需求。

Conclusion: 本研究加强了用户-轮椅-环境三维无障碍模型，并强调了对轮椅使用者社区通行的深入理解。

Abstract: Community participation is an important aspect of an individuals physical and mental well-being. This participation is often limited for persons with disabilities, especially those with ambulatory impairments due to the inability to optimally navigate the community. Accessibility is a multi-faceted problem and varies from person to person. Moreover, it depends on various personal and environmental factors. Despite significant research conducted to understand challenges faced by wheelchair users, developing an accessibility model for wheelchair users by identifying various characteristic features has not been thoroughly studied. In this research, we propose a three-dimensional model of accessibility and validate it through in-depth qualitative analysis involving semi-structured interviews and participatory action research. The outcomes of our studies validated many of our hypotheses about community access for wheelchair users and identified a need for more accessible path planning tools and resources. Overall, this research strengthened our three-dimensional User-Wheelchair-Environment model of accessibility.

</details>


### [17] [Detecting cyberbullying in Spanish texts through deep learning techniques](https://arxiv.org/abs/2512.19899)
*Paúl Cumba-Armijos,Diego Riofrío-Luzcando,Verónica Rodríguez-Arboleda,Joe Carrión-Jumbo*

Main category: cs.HC

TL;DR: 本研究通过社交网络收集西班牙语欺凌表达，并通过深度学习训练模型，以自动检测网络欺凌事件。


<details>
  <summary>Details</summary>
Motivation: 希望自动检测可能对社会弱势群体产生负面影响的事件。

Method: 利用深度学习技术，通过卷积神经网络训练模型。

Result: 建立了一个可以识别侮辱、种族主义、恐同攻击等西班牙语网络欺凌表达的模型。

Conclusion: 研究成功创建了一种预测模型，可以识别西班牙语中的网络欺凌表达。

Abstract: Recent recollected data suggests that it is possible to automatically detect events that may negatively affect the most vulnerable parts of our society, by using any communication technology like social networks or messaging applications. This research consolidates and prepares a corpus with Spanish bullying expressions taken from Twitter in order to use them as an input to train a convolutional neuronal network through deep learning techniques. As a result of this training, a predictive model was created, which can identify Spanish cyberbullying expressions such as insults, racism, homophobic attacks, and so on.

</details>


### [18] [Developers' Experience with Generative AI -- First Insights from an Empirical Mixed-Methods Field Study](https://arxiv.org/abs/2512.19926)
*Charlotte Brandebusemeyer,Tobias Schimmer,Bert Arnrich*

Main category: cs.HC

TL;DR: 本研究采用多模态方法，关注开发者与生成性AI的互动体验。结果表明，适度使用AI助手可提高效率和降低工作负载，但过度使用则适得其反。


<details>
  <summary>Details</summary>
Motivation: 探索程序员与生成性人工智能（GenAI）助手的互动体验，以优化这种关系，使其更符合开发者的需求和实际工作环境。

Method: 通过在公司环境下设计受控与非受控的混合方法研究，收集行为数据与主观体验数据，以比较不同交互类型对效率、准确性和工作负载感知的影响。

Result: 研究显示，适度使用GitHub Copilot的代码建议或聊天提示可以提高工作效率，并降低感知工作负载，而过度或同时使用这两种方法则会减弱这些好处。

Conclusion: 在日常工作任务中，开发者与生成性AI的互动增加了认知负荷和生产力，建议采用类似的研究设计以全面评价GenAI工具的使用体验。

Abstract: With the rise of AI-powered coding assistants, firms and programmers are exploring how to optimize their interaction with them. Research has so far mainly focused on evaluating output quality and productivity gains, leaving aside the developers' experience during the interaction. In this study, we take a multimodal, developer-centered approach to gain insights into how professional developers experience the interaction with Generative AI (GenAI) in their natural work environment in a firm. The aim of this paper is (1) to demonstrate a feasible mixed-method study design with controlled and uncontrolled study periods within a firm setting, (2) to give first insights from complementary behavioral and subjective experience data on developers' interaction with GitHub Copilot and (3) to compare the impact of interaction types (no Copilot use, in-code suggestions, chat prompts or both in-code suggestions and chat prompts) on efficiency, accuracy and perceived workload whilst working on different task categories. Results of the controlled sessions in this study indicate that moderate use of either in-code suggestions or chat prompts improves efficiency (task duration) and reduces perceived workload compared to not using Copilot, while excessive or combined use lessens these benefits. Accuracy (task completion) profits from chat interaction. In general, subjective perception of workload aligns with objective behavioral data in this study. During the uncontrolled period of the study, both higher cognitive load and productivity were perceived when interacting with AI during everyday working tasks. This study motivates the use of comparable study designs, in e.g. workshop or hackathon settings, to evaluate GenAI tools holistically and realistically with a focus on the developers' experience.

</details>


### [19] [Stories That Teach: Eastern Wisdom for Human-AI Creative Partnerships](https://arxiv.org/abs/2512.19999)
*Kexin Nie,Xin Tang,Mengyao Guo,Ze Gao*

Main category: cs.HC

TL;DR: 通过“缺口与填充”方法，本研讨会探讨了人机合作在HCI视觉叙事教育中的应用，结合理论与实践，帮助参与者保持创造性，同时利用AI增强叙事能力。


<details>
  <summary>Details</summary>
Motivation: 回应中国HCI社区对文化知情和教育方法的需求，以促进创造性教育中的AI整合。

Method: 采用三阶段方法：建立叙事基础、识别缺口、AI增强实践，结合理论和实践。

Result: 本次研讨会探讨了人机合作在HCI视觉叙事教育中的创新方法，采用了我们建立的“缺口与填充”方法。结合东亚美学哲学，如中国的负空间传统和日本的“间”（ma）概念，我们展示了如何在利用AI辅助的同时保持学生的创造性发挥。研讨会分为三个阶段：建立以人为主的叙事基础、识别战略性缺口，以及进行AI增强的合作。通过理论与实践相结合，参与者能够创建引人入胜的HCI视觉叙事，展示有效的人机合作。参与者通过连续艺术技巧、故事板练习及引导式AI整合，学习如何在保留叙事连贯性和创造性视野的同时，传达复杂的交互概念、可访问性解决方案和用户体验流程。

Conclusion: 本次研讨会为中国HCI社区提供了切合文化背景和教育需求的人机合作AI整合方法。

Abstract: This workshop explores innovative human-AI collaboration methodologies in HCI visual storytelling education through our established "gap-and-fill" approach. Drawing on Eastern aesthetic philosophies of intentional emptiness, including Chinese negative-space traditions, Japanese "ma" concepts, and contemporary design minimalism, we demonstrate how educators can teach students to maintain creative agency while strategically leveraging AI assistance. During this workshop, participants will experience a structured three-phase methodology: creating a human-led narrative foundation, identifying strategic gaps, and collaborating on AI enhancements. The workshop combines theoretical foundations with intensive hands-on practice, enabling participants to create compelling HCI visual narratives that demonstrate effective human-AI partnership. Through sequential art techniques, storyboarding exercises, and guided AI integration, attendees learn to communicate complex interactive concepts, accessibility solutions, and user experience flows while preserving narrative coherence and creative vision. Building on our successful workshops at ACM C&C 2025, this session specifically addresses the needs of the Chinese HCI community for culturally informed and pedagogically sound approaches to AI integration in creative education.

</details>


### [20] [/UnmuteAll: Modeling Verbal Communication Patterns of Collaborative Contexts in MOBA Games](https://arxiv.org/abs/2512.20116)
*Yongchan Son,Jahun Jang,Been An,Jimoon Kang,Eunji Park*

Main category: cs.HC

TL;DR: 本研究提出了一种自动评估电子竞技团队语音交流模式的框架，发现有效沟通的参与和频率随团队情况而异。


<details>
  <summary>Details</summary>
Motivation: 探讨在电子竞技团队中的语音基础沟通模式，以填补相关研究的空白

Method: 构建网络评估语音交流模式

Result: 发现电子竞技选手在合作情况下表现出更广泛和更平衡的参与，以及随着时间的推移发言次数的增加，决策时的发言增长最为显著，同时团队间存在受专业培训影响的差异

Conclusion: 本研究提供了一种可推广的工具，用于分析团队沟通的有效性。

Abstract: Team communication plays a vital role in supporting collaboration in multiplayer online games. Therefore, numerous studies were conducted to examine communication patterns in esports teams. While non-verbal communication has been extensively investigated, research on assessing voice-based verbal communication patterns remains relatively understudied. In this study, we propose a framework that automatically assesses verbal communication patterns by constructing networks with utterances transcribed from voice recordings. Through a data collection study, we obtained 84 game sessions from five League of Legends teams and subsequently investigated how verbal communication patterns varied across different conditions. As a result, we revealed that esports players exhibited broader and more balanced participation in collaborative situations, increased utterances over time with the largest rise in decision making, and team-level differences that were contingent on effective professional training. Building upon these findings, this study provides a generalizable tool for analyzing effective team communication.

</details>


### [21] [Dreamcrafter: Immersive Editing of 3D Radiance Fields Through Flexible, Generative Inputs and Outputs](https://arxiv.org/abs/2512.20129)
*Cyrus Vachha,Yixiao Kang,Zach Dive,Ashwat Chidambaram,Anik Gupta,Eunice Jun,Bjoern Hartmann*

Main category: cs.HC

TL;DR: 本论文提出了一种名为Dreamcrafter的VR基础3D场景编辑系统，整合了生成性AI与实时3D Radiance Field编辑，旨在降低3D场景创作的门槛。


<details>
  <summary>Details</summary>
Motivation: 旨在整合沉浸式3D内容直接操作与AI技术的优势，降低创建3D场景的障碍。

Method: 设计了一个模块化架构，集成生成性AI算法，同时结合自然语言和直接操作的多种控制方式。

Result: 实证研究发现，生成性AI接口在场景编辑和世界构建方面增强了创造性。

Conclusion: Dreamcrafter通过结合生成性AI与用户控制选项，提升了3D场景编辑中的创造性与交互性。

Abstract: Authoring 3D scenes is a central task for spatial computing applications. Competing visions for lowering existing barriers are (1) focus on immersive, direct manipulation of 3D content or (2) leverage AI techniques that capture real scenes (3D Radiance Fields such as, NeRFs, 3D Gaussian Splatting) and modify them at a higher level of abstraction, at the cost of high latency. We unify the complementary strengths of these approaches and investigate how to integrate generative AI advances into real-time, immersive 3D Radiance Field editing. We introduce Dreamcrafter, a VR-based 3D scene editing system that: (1) provides a modular architecture to integrate generative AI algorithms; (2) combines different levels of control for creating objects, including natural language and direct manipulation; and (3) introduces proxy representations that support interaction during high-latency operations. We contribute empirical findings on control preferences and discuss how generative AI interfaces beyond text input enhance creativity in scene editing and world building.

</details>


### [22] [RESPOND: Risk-Enhanced Structured Pattern for LLM-driven Online Node-level Decision-making](https://arxiv.org/abs/2512.20179)
*Dan Chen,Heye Huang,Tiantian Chen,Zheng Li,Yongji Li,Yuhui Xu,Sikai Chen*

Main category: cs.HC

TL;DR: 提出RESPOND，一个基于结构化决策的驾驶代理，改进了场景检索与反思效率，展示了在真实驾驶场景中的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决基于LLM的驾驶代理在场景检索和反思效率低下的问题。

Method: 采用5x3矩阵来表示场景，并结合混合规则与LLM决策管道，具有两级记忆机制。

Result: RESPOND框架在高速公路环境中优于现有的LLM和强化学习驾驶代理，且碰撞次数显著减少。

Conclusion: RESPOND在自主驾驶、个性化驾驶辅助及主动危险缓解方面展现了潜力，能有效减少驾驶中的风险。

Abstract: Current LLM-based driving agents that rely on unstructured plain-text memory suffer from low-precision scene retrieval and inefficient reflection. To address this limitation, we present RESPOND, a structured decision-making framework for LLM-driven agents grounded in explicit risk patterns. RESPOND represents each ego-centric scene using a unified 5 by 3 matrix that encodes spatial topology and road constraints, enabling consistent and reliable retrieval of spatial risk configurations. Based on this representation, a hybrid rule and LLM decision pipeline is developed with a two-tier memory mechanism. In high-risk contexts, exact pattern matching enables rapid and safe reuse of verified actions, while in low-risk contexts, sub-pattern matching supports personalized driving style adaptation. In addition, a pattern-aware reflection mechanism abstracts tactical corrections from crash and near-miss frames to update structured memory, achieving one-crash-to-generalize learning. Extensive experiments demonstrate the effectiveness of RESPOND. In highway-env, RESPOND outperforms state-of-the-art LLM-based and reinforcement learning based driving agents while producing substantially fewer collisions. With step-wise human feedback, the agent acquires a Sporty driving style within approximately 20 decision steps through sub-pattern abstraction. For real-world validation, RESPOND is evaluated on 53 high-risk cut-in scenarios extracted from the HighD dataset. For each event, intervention is applied immediately before the cut-in and RESPOND re-decides the driving action. Compared to recorded human behavior, RESPOND reduces subsequent risk in 84.9 percent of scenarios, demonstrating its practical feasibility under real-world driving conditions. These results highlight RESPONDs potential for autonomous driving, personalized driving assistance, and proactive hazard mitigation.

</details>


### [23] [Competing or Collaborating? The Role of Hackathon Formats in Shaping Team Dynamics and Project Choices](https://arxiv.org/abs/2512.20181)
*Sadia Nasrin Tisha,Md Nazmus Sakib,Sanorita Dey*

Main category: cs.HC

TL;DR: 本研究分析了性别特定和常规黑客马拉松的格式差异，发现前者强调支持性学习环境，而后者则更具竞争性，建议结合两者优点设计更平衡的混合型黑客马拉松。


<details>
  <summary>Details</summary>
Motivation: 研究黑客马拉松的设计如何影响学生的学习体验和参与度，探索不同格式的结构差异。

Method: 采用混合方法分析两种不同的黑客马拉松格式（性别特定和常规）的团队动态、项目主题、角色分配和环境设置。

Result: 发现性别特定黑客马拉松（GS）促进了协作和支持的氛围，而常规黑客马拉松（RI）更倾向于竞争驱动，有助于设计更具吸引力和公平性的黑客马拉松体验。

Conclusion: 提出了一种结合两种黑客马拉松格式优点的混合模型，以实现竞争与包容之间的平衡，从而改善参与体验。

Abstract: Hackathons have emerged as dynamic platforms for fostering innovation, collaboration, and skill development in the technology sector. Structural differences across hackathon formats raise important questions about how event design can shape student learning experiences and engagement. This study examines two distinct hackathon formats: a gender-specific hackathon (GS) and a regular institutional hackathon (RI). Using a mixed-methods approach, we analyze variations in team dynamics, project themes, role assignments, and environmental settings. Our findings indicate that GS hackathon foster a collaborative and supportive atmosphere, emphasizing personal growth and community learning, with projects often centered on health and well-being. In contrast, RI hackathon tend to promote a competitive, outcome-driven environment, with projects frequently addressing entertainment and environmental sustainability. Based on these insights, we propose a hybrid hackathon model that combines the strengths of both formats to balance competition with inclusivity. This work contributes to the design of more engaging, equitable, and pedagogically effective hackathon experiences.

</details>


### [24] [The Effect of Empathic Expression Levels in Virtual Human Interaction: A Controlled Experiment](https://arxiv.org/abs/2512.20221)
*Sung Park,Daeho Yoon,Jungmin Lee*

Main category: cs.HC

TL;DR: 本研究探讨了虚拟人互动中不同同理表达水平对用户体验的影响，结果表明视频基础的同理表达更能引发情感同理心。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能系统在日常生活中的应用日益广泛，交互代理表现出同理心的能力对于有效的人机交互变得至关重要，特别是在情感敏感的背景中。

Method: 通过在咨询风格交互环境中进行的实验（70名参与者），比较三种虚拟人条件的用户体验

Result: 视频基础的同理表达条件在情感同理心方面显著高于中立基线，而在对话基础的条件下边际高。

Conclusion: 虚拟人中的同理表达应视为一个渐进的设计变量，视觉线索在塑造情感用户体验方面起着决定性作用。

Abstract: As artificial intelligence (AI) systems become increasingly embedded in everyday life, the ability of interactive agents to express empathy has become critical for effective human-AI interaction, particularly in emotionally sensitive contexts. Rather than treating empathy as a binary capability, this study examines how different levels of empathic expression in virtual human interaction influence user experience. We conducted a between-subject experiment (n = 70) in a counseling-style interaction context, comparing three virtual human conditions: a neutral dialogue-based agent, a dialogue-based empathic agent, and a video-based empathic agent that incorporates users' facial cues. Participants engaged in a 15-minute interaction and subsequently evaluated their experience using subjective measures of empathy and interaction quality. Results from analysis of variance (ANOVA) revealed significant differences across conditions in affective empathy, perceived naturalness of facial movement, and appropriateness of facial expression. The video-based empathic expression condition elicited significantly higher affective empathy than the neutral baseline (p < .001) and marginally higher levels than the dialogue-based condition (p < .10). In contrast, cognitive empathy did not differ significantly across conditions. These findings indicate that empathic expression in virtual humans should be conceptualized as a graded design variable, rather than a binary capability, with visually grounded cues playing a decisive role in shaping affective user experience.

</details>


### [25] [Structured Visualization Design Knowledge for Grounding Generative Reasoning and Situated Feedback](https://arxiv.org/abs/2512.20306)
*Péter Ferenc Gyarmati,Dominik Moritz,Torsten Möller,Laura Koesten*

Main category: cs.HC

TL;DR: 该研究提出了一种目录方案，利用自然语言和语义元数据结构化可视化设计知识，以提供灵活的设计指导，促进生成系统的推理并验证专家知识。


<details>
  <summary>Details</summary>
Motivation: 解决符号系统与生成模型之间的张力，通过提供灵活的设计指导来提升自动化可视化设计的有效性。

Method: 通过将可视化设计知识结构化为具有语义类型元数据的自然语言指南，进行专家研究以观察实践者如何适应不同情境。

Result: 我们的方案展示了744条指南，并揭示了不同来源之间的冲突建议以及领域间的可转移原则。

Conclusion: 我们提出的目录方案能够捕捉设计知识，使得专家可以创造灵活且可查询的自然语言指南。

Abstract: Automated visualization design navigates a tension between symbolic systems and generative models. Constraint solvers enforce structural and perceptual validity, but the rules they require are difficult to author and too rigid to capture situated design knowledge. Large language models require no formal rules and can reason about contextual nuance, but they prioritize popular conventions over empirically grounded best practices. We address this tension by proposing a cataloging scheme that structures visualization design knowledge as natural-language guidelines with semantically typed metadata. This allows experts to author knowledge that machines can query. An expert study ($N=18$) indicates that practitioners routinely adapt heuristics to situational factors such as audience and communicative intent. To capture this reasoning, guideline sections specify not only advice but also the contexts where it applies, exceptions that invalidate it, and the sources from which it derives. We demonstrate the scheme's expressiveness by cataloging 744 guidelines drawn from cognitive science, accessibility standards, data journalism, and research on rhetorical aspects of visual communication. We embed guideline sections in a vector space, opening the knowledge itself to structural analysis. This reveals conflicting advice across sources and transferable principles between domains. Rather than replacing constraint-based tools, our scheme provides what they lack: situated guidance that generative systems can retrieve to ground their reasoning, users can verify against cited sources, and experts can author as knowledge evolves.

</details>


### [26] [A human-centered approach to reframing job satisfaction in the BIM-enabled construction industry](https://arxiv.org/abs/2512.20584)
*Sharareh Mirzaei,Stephanie Bunt,Susan M Bogus*

Main category: cs.HC

TL;DR: 本研究探讨了建筑信息模型(BIM)在建筑行业中对工作满意度的影响，揭示了BIM使用与工作满意度之间的复杂关系。


<details>
  <summary>Details</summary>
Motivation: 随着建筑行业的数字化转型，确保新技术提升人类体验而非阻碍变得至关重要，BIM的影响特定于工作满意度亟待研究。

Method: 采用Hackman和Oldham的工作特征模型，创建调查问卷并用偏最小二乘结构方程模型分析调查结果。

Result: 研究发现，BIM的使用并不必然增加整体工作满意度，仅有部分BIM相关维度对工作满意度产生正面影响。

Conclusion: 在AEC行业，持续的工作满意度更依赖于人性化因素，如合作和在数字工作流中的有意义的参与，而非技术自主性。

Abstract: As the construction industry undergoes rapid digital transformation, ensuring that new technologies enhance rather than hinder human experience has become essential. The inclusion of Building Information Modeling (BIM) plays a central role in this shift, yet its influence on job satisfaction remains underexplored. In response, this study developed a human-centered measurement model for evaluating job satisfaction in BIM work environments by adapting Hackman and Oldham's Job Characteristics Model for the architecture, engineering, and construction (AEC) industry to create a survey that captured industry perspectives on BIM use and job satisfaction. The model uses Partial Least Squares Structural Equation Modeling to analyze the survey results and identify what dimensions of BIM-related work affect job satisfaction. While it was hypothesized that BIM use increases job satisfaction, the results show that only some dimensions of BIM use positively impact BIM job satisfaction; the use of BIM does not guarantee an increase in overall job satisfaction. Additionally, more frequent BIM use was not associated with higher satisfaction levels. These findings suggest that in the AEC industry, sustainable job satisfaction depends less on technological autonomy and more on human-centric factors, particularly collaboration and meaningful engagement within digital workflows.

</details>
