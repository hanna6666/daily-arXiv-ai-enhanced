{"id": "2601.00087", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00087", "abs": "https://arxiv.org/abs/2601.00087", "authors": ["Zhaoan Wang", "Junchao Li", "Mahdi Mohammad", "Shaoping Xiao"], "title": "Reinforcement learning with timed constraints for robotics motion planning", "comment": null, "summary": "Robotic systems operating in dynamic and uncertain environments increasingly require planners that satisfy complex task sequences while adhering to strict temporal constraints. Metric Interval Temporal Logic (MITL) offers a formal and expressive framework for specifying such time-bounded requirements; however, integrating MITL with reinforcement learning (RL) remains challenging due to stochastic dynamics and partial observability. This paper presents a unified automata-based RL framework for synthesizing policies in both Markov Decision Processes (MDPs) and Partially Observable Markov Decision Processes (POMDPs) under MITL specifications. MITL formulas are translated into Timed Limit-Deterministic Generalized Büchi Automata (Timed-LDGBA) and synchronized with the underlying decision process to construct product timed models suitable for Q-learning. A simple yet expressive reward structure enforces temporal correctness while allowing additional performance objectives. The approach is validated in three simulation studies: a $5 \\times 5$ grid-world formulated as an MDP, a $10 \\times 10$ grid-world formulated as a POMDP, and an office-like service-robot scenario. Results demonstrate that the proposed framework consistently learns policies that satisfy strict time-bounded requirements under stochastic transitions, scales to larger state spaces, and remains effective in partially observable environments, highlighting its potential for reliable robotic planning in time-critical and uncertain settings."}
{"id": "2601.00126", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00126", "abs": "https://arxiv.org/abs/2601.00126", "authors": ["Utkarsh A Mishra", "David He", "Yongxin Chen", "Danfei Xu"], "title": "Compositional Diffusion with Guided search for Long-Horizon Planning", "comment": "38 pages, 18 figures", "summary": "Generative models have emerged as powerful tools for planning, with compositional approaches offering particular promise for modeling long-horizon task distributions by composing together local, modular generative models. This compositional paradigm spans diverse domains, from multi-step manipulation planning to panoramic image synthesis to long video generation. However, compositional generative models face a critical challenge: when local distributions are multimodal, existing composition methods average incompatible modes, producing plans that are neither locally feasible nor globally coherent. We propose Compositional Diffusion with Guided Search (CDGS), which addresses this \\emph{mode averaging} problem by embedding search directly within the diffusion denoising process. Our method explores diverse combinations of local modes through population-based sampling, prunes infeasible candidates using likelihood-based filtering, and enforces global consistency through iterative resampling between overlapping segments. CDGS matches oracle performance on seven robot manipulation tasks, outperforming baselines that lack compositionality or require long-horizon training data. The approach generalizes across domains, enabling coherent text-guided panoramic images and long videos through effective local-to-global message passing. More details: https://cdgsearch.github.io/"}
{"id": "2601.00163", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00163", "abs": "https://arxiv.org/abs/2601.00163", "authors": ["Junfeng Chen", "Yuxiao Zhu", "Xintong Zhang", "Bing Luo", "Meng Guo"], "title": "SLEI3D: Simultaneous Exploration and Inspection via Heterogeneous Fleets under Limited Communication", "comment": null, "summary": "Robotic fleets such as unmanned aerial and ground vehicles have been widely used for routine inspections of static environments, where the areas of interest are known and planned in advance. However, in many applications, such areas of interest are unknown and should be identified online during exploration. Thus, this paper considers the problem of simultaneous exploration, inspection of unknown environments and then real-time communication to a mobile ground control station to report the findings. The heterogeneous robots are equipped with different sensors, e.g., long-range lidars for fast exploration and close-range cameras for detailed inspection. Furthermore, global communication is often unavailable in such environments, where the robots can only communicate with each other via ad-hoc wireless networks when they are in close proximity and free of obstruction. This work proposes a novel planning and coordination framework (SLEI3D) that integrates the online strategies for collaborative 3D exploration, adaptive inspection and timely communication (via the intermit-tent or proactive protocols). To account for uncertainties w.r.t. the number and location of features, a multi-layer and multi-rate planning mechanism is developed for inter-and-intra robot subgroups, to actively meet and coordinate their local plans. The proposed framework is validated extensively via high-fidelity simulations of numerous large-scale missions with up to 48 robots and 384 thousand cubic meters. Hardware experiments of 7 robots are also conducted. Project website is available at https://junfengchen-robotics.github.io/SLEI3D/."}
{"id": "2601.00238", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00238", "abs": "https://arxiv.org/abs/2601.00238", "authors": ["Julia Di", "Kenneth A. W. Hoffmann", "Tony G. Chen", "Tian-Ao Ren", "Mark R. Cutkosky"], "title": "SLAP: Slapband-based Autonomous Perching Drone with Failure Recovery for Vertical Tree Trunks", "comment": "Paper accepted to IEEE Aerospace Conference 2026. This is a pre-print", "summary": "Perching allows unmanned aerial vehicles (UAVs) to reduce energy consumption, remain anchored for surface sampling operations, or stably survey their surroundings. Previous efforts for perching on vertical surfaces have predominantly focused on lightweight mechanical design solutions with relatively scant system-level integration. Furthermore, perching strategies for vertical surfaces commonly require high-speed, aggressive landing operations that are dangerous for a surveyor drone with sensitive electronics onboard. This work presents the preliminary investigation of a perching approach suitable for larger drones that both gently perches on vertical tree trunks and reacts and recovers from perch failures. The system in this work, called SLAP, consists of vision-based perch site detector, an IMU (inertial-measurement-unit)-based perch failure detector, an attitude controller for soft perching, an optical close-range detection system, and a fast active elastic gripper with microspines made from commercially-available slapbands. We validated this approach on a modified 1.2 kg commercial quadrotor with component and system analysis. Initial human-in-the-loop autonomous indoor flight experiments achieved a 75% perch success rate on a real oak tree segment across 20 flights, and 100% perch failure recovery across 2 flights with induced failures."}
{"id": "2601.00001", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.00001", "abs": "https://arxiv.org/abs/2601.00001", "authors": ["Kai Liu", "Michelle L. Aebersold", "Mark Lindquist", "Haoting Gao"], "title": "Augmented Reality Indoor Wayfinding in Hospital Environments An Empirical Study on Navigation Efficiency, User Experience, and Cognitive Load", "comment": null, "summary": "Hospitals are among the most cognitively demanding indoor environments, especially for patients and visitors unfamiliar with their layout. This study investigates the effectiveness of an augmented reality (AR)-based handheld navigation system compared to traditional paper maps in a large hospital setting. Through a mixed-methods experiment with 32 participants, we measured navigation performance, cognitive workload (NASA-TLX), situational anxiety (STAI-State), spatial behavior, and user satisfaction. Results show that AR users completed navigation tasks significantly faster, made fewer errors, and reported lower anxiety and workload. However, paper map users demonstrated stronger spatial memory in sketch-based recall tasks, highlighting a trade-off between real-time efficiency and long-term spatial learning. We discuss implications for inclusive AR design, spatial cognition, and healthcare accessibility, offering actionable design strategies for adaptive indoor navigation tools."}
{"id": "2601.00271", "categories": ["cs.RO", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.00271", "abs": "https://arxiv.org/abs/2601.00271", "authors": ["Yuya Nagai", "Hiromitsu Nakamura", "Narito Shinmachi", "Yuta Higashizono", "Satoshi Ono"], "title": "Vehicle Painting Robot Path Planning Using Hierarchical Optimization", "comment": null, "summary": "In vehicle production factories, the vehicle painting process employs multiple robotic arms to simultaneously apply paint to car bodies advancing along a conveyor line. Designing paint paths for these robotic arms, which involves assigning car body areas to arms and determining paint sequences for each arm, remains a time-consuming manual task for engineers, indicating the demand for automation and design time reduction. The unique constraints of the painting process hinder the direct application of conventional robotic path planning techniques, such as those used in welding. Therefore, this paper formulates the design of paint paths as a hierarchical optimization problem, where the upper-layer subproblem resembles a vehicle routing problem (VRP), and the lower-layer subproblem involves detailed path planning. This approach allows the use of different optimization algorithms at each layer, and permits flexible handling of constraints specific to the vehicle painting process through the design of variable representation, constraints, repair operators, and an initialization process at the upper and lower layers. Experiments with three commercially available vehicle models demonstrated that the proposed method can automatically design paths that satisfy all constraints for vehicle painting with quality comparable to those created manually by engineers."}
{"id": "2601.00326", "categories": ["cs.HC", "cs.MM", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.00326", "abs": "https://arxiv.org/abs/2601.00326", "authors": ["Torin Hopkins", "Shih-Yu Ma", "Suibi Che-Chuan Weng", "Ming-Yuan Pai", "Ellen Yi-Luen Do", "Luca Turchet"], "title": "MR-DAW: Towards Collaborative Digital Audio Workstations in Mixed Reality", "comment": null, "summary": "Digital Audio Workstations (DAWs) are central to modern music production but often encumber the musician's workflow, tethering them to a desk and hindering natural interaction with their instrument. Furthermore, effective remote collaboration remains a significant challenge, with existing solutions hampered by network latency and asynchronous file sharing. This paper investigates the potential of Mixed Reality (MR) to overcome these barriers, creating an intuitive environment for real-time, remote musical collaboration. We employ qualitative and speculative design techniques to better understand: 1) how players currently use DAWs, and 2) to imagine a speculative future of collaborative MR-DAWs. To facilitate this discussion, we developed and evaluated the usability of a design probe, MR-DAW. An MR system enabling multiple, geographically dispersed users to control a single, shared DAW instance while moving freely in their local spaces. Our networked system enables each remote musician to use a physical foot pedal for collaborative looping, merging a familiar, hands-free interaction with a shared virtual session. Based on interviews and system evaluations with 20 musicians, we analyze current practices, report on the user experience with our MR system, and speculate on the future of musical collaboration in MR. Our results highlight the affordances of MR for unencumbered musical interaction and provide a speculative outlook on the future of remote collaborative DAWs in the Musical Metaverse."}
{"id": "2601.00275", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00275", "abs": "https://arxiv.org/abs/2601.00275", "authors": ["Dusan Nemec", "Gal Versano", "Itai Savin", "Vojtech Simak", "Juraj Kekelak", "Itzik Klein"], "title": "Pure Inertial Navigation in Challenging Environments with Wheeled and Chassis Mounted Inertial Sensors", "comment": null, "summary": "Autonomous vehicles and wheeled robots are widely used in many applications in both indoor and outdoor settings. In practical situations with limited GNSS signals or degraded lighting conditions, the navigation solution may rely only on inertial sensors and as result drift in time due to errors in the inertial measurement. In this work, we propose WiCHINS, a wheeled and chassis inertial navigation system by combining wheel-mounted-inertial sensors with a chassis-mounted inertial sensor for accurate pure inertial navigation. To that end, we derive a three-stage framework, each with a dedicated extended Kalman filter. This framework utilizes the benefits of each location (wheel/body) during the estimation process. To evaluate our proposed approach, we employed a dataset with five inertial measurement units with a total recording time of 228.6 minutes. We compare our approach with four other inertial baselines and demonstrate an average position error of 11.4m, which is $2.4\\%$ of the average traveled distance, using two wheels and one body inertial measurement units. As a consequence, our proposed method enables robust navigation in challenging environments and helps bridge the pure-inertial performance gap."}
{"id": "2601.00333", "categories": ["cs.HC", "cs.MM", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.00333", "abs": "https://arxiv.org/abs/2601.00333", "authors": ["Suibi Che-Chuan Weng", "Torin Hopkins", "Shih-Yu Ma", "Amy Banic", "Ellen Yi-Luen Do"], "title": "Effects of Limited Field of View on Musical Collaboration Experience with Avatars in Extended Reality", "comment": null, "summary": "During musical collaboration, visual cues are essential for communication between musicians. Extended Reality (XR) applications, often used with head-mounted displays like Augmented Reality (AR) glasses, can limit the field of view (FOV) of players. We conducted a study to investigate the effects of limited FOV on co-presence, gesture recognition, overall enjoyment, and reaction time.\n  Initially, we observed experienced musicians collaborating informally with and without visual occlusion, noting that collaboration suffered with limited FOV. We then conducted a within-subjects study with 19 participants, comparing an unrestricted FOV holographic setup called HoloJam to Nreal AR glasses with a 52$^{\\circ}$ limited FOV. In the AR setup, we tested two conditions: standard AR with a 52$^{\\circ}$ FOV and a modified AR notification system called Mini Musicians.\n  Results showed that HoloJam provided higher co-presence, quicker gesture recognition, and greater enjoyment. The Mini Musicians application reduced reaction time and maintained enjoyment compared to the standard AR setup. We conclude that limited FOV impacts musical collaboration, but notifications can improve reaction time and should be considered in future XR music collaborations."}
{"id": "2601.00305", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00305", "abs": "https://arxiv.org/abs/2601.00305", "authors": ["Prashant Kumar", "Yukiyasu Domae", "Weiwei Wan", "Kensuke Harada"], "title": "Replaceable Bit-based Gripper for Picking Cluttered Food Items", "comment": null, "summary": "The food packaging industry goes through changes in food items and their weights quite rapidly. These items range from easy-to-pick, single-piece food items to flexible, long and cluttered ones. We propose a replaceable bit-based gripper system to tackle the challenge of weight-based handling of cluttered food items. The gripper features specialized food attachments(bits) that enhance its grasping capabilities, and a belt replacement system allows switching between different food items during packaging operations. It offers a wide range of control options, enabling it to grasp and drop specific weights of granular, cluttered, and entangled foods. We specifically designed bits for two flexible food items that differ in shape: ikura(salmon roe) and spaghetti. They represent the challenging categories of sticky, granular food and long, sticky, cluttered food, respectively. The gripper successfully picked up both spaghetti and ikura and demonstrated weight-specific dropping of these items with an accuracy over 80% and 95% respectively. The gripper system also exhibited quick switching between different bits, leading to the handling of a large range of food items."}
{"id": "2601.00382", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.00382", "abs": "https://arxiv.org/abs/2601.00382", "authors": ["Nelly Elsayed"], "title": "Unseen Risks of Clinical Speech-to-Text Systems: Transparency, Privacy, and Reliability Challenges in AI-Driven Documentation", "comment": null, "summary": "AI-driven speech-to-text (STT) documentation systems are increasingly adopted in clinical settings to reduce documentation burden and improve workflow efficiency. However, their rapid deployment has outpaced understanding of the associated socio-technical risks, including transparency, reliability, patient autonomy, workflow alignment, and organizational governance. A clearer analysis of these risks is needed to support safe and equitable integration into healthcare practice. This study synthesizes interdisciplinary evidence from technical performance research, regulatory and ethical standards, clinical workflow analyses, and organizational policy guidance. The synthesis was used to develop a multi-layered socio-technical conceptual framework for evaluating and governing STT systems. Findings show that STT systems operate within tightly coupled socio-technical environments in which model performance, clinician oversight, patient rights, workflow design, and institutional governance are interdependent. The study offers a structured socio-technical governance framework and an implementation roadmap that outlines readiness assessment, vendor evaluation, pilot deployment, clinician training, ongoing monitoring, and iterative improvement. The framework emphasizes safeguards that protect patient autonomy, documentation integrity, and institutional trust while enabling the efficient and beneficial use of STT technologies. This work provides actionable guidance for healthcare organizations seeking to adopt STT systems responsibly and equitably."}
{"id": "2601.00465", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00465", "abs": "https://arxiv.org/abs/2601.00465", "authors": ["Dennis Christmann", "Juan F. Gutierrez", "Sthiti Padhi", "Patrick Plörer", "Aditya Takur", "Simona Silvestri", "Andres Gomez"], "title": "Space Debris Removal using Nano-Satellites controlled by Low-Power Autonomous Agents", "comment": "This is an open-access, author-archived version of a manuscript published in European Conference on Multi-Agent Systems 2024", "summary": "Space debris is an ever-increasing problem in space travel. There are already many old, no longer functional spacecraft and debris orbiting the earth, which endanger both the safe operation of satellites and space travel. Small nano-satellite swarms can address this problem by autonomously de-orbiting debris safely into the Earth's atmosphere. This work builds on the recent advances of autonomous agents deployed in resource-constrained platforms and shows a first simplified approach how such intelligent and autonomous nano-satellite swarms can be realized. We implement our autonomous agent software on wireless microcontrollers and perform experiments on a specialized test-bed to show the feasibility and overall energy efficiency of our approach."}
{"id": "2601.00570", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.00570", "abs": "https://arxiv.org/abs/2601.00570", "authors": ["Ananya Bhattacharjee", "Jina Suh", "Mohit Chandra", "Javier Hernandez"], "title": "User Perceptions of an LLM-Based Chatbot for Cognitive Reappraisal of Stress: Feasibility Study", "comment": null, "summary": "Cognitive reappraisal is a well-studied emotion regulation strategy that helps individuals reinterpret stressful situations to reduce their impact. Many digital mental health tools struggle to support this process because rigid scripts fail to accommodate how users naturally describe stressors. This study examined the feasibility of an LLM-based single-session intervention (SSI) for workplace stress reappraisal. We assessed short-term changes in stress-related outcomes and examined design tensions during use. We conducted a feasibility study with 100 employees at a large technology company who completed a structured cognitive reappraisal session delivered by a GPT-4o-based chatbot. Pre-post measures included perceived stress intensity, stress mindset, perceived demand, and perceived resources. These outcomes were analyzed using paired Wilcoxon signed-rank tests with correction for multiple comparisons. We also examined sentiment and stress trajectories across conversation quartiles using two RoBERTa-based classifiers and an LLM-based stress rater. Open-ended responses were analyzed using thematic analysis. Results showed significant reductions in perceived stress intensity and significant improvements in stress mindset. Changes in perceived resources and perceived demand trended in expected directions but were not statistically significant. Automated analyses indicated consistent declines in negative sentiment and stress over the course of the interaction. Qualitative findings suggested that participants valued the structured prompts for organizing thoughts, gaining perspective, and feeling acknowledged. Participants also reported tensions around scriptedness, preferred interaction length, and reactions to AI-driven empathy. These findings highlight both the promise and the design constraints of integrating LLMs into DMH interventions for workplace settings."}
{"id": "2601.00545", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00545", "abs": "https://arxiv.org/abs/2601.00545", "authors": ["Varun Agrawal", "Frank Dellaert"], "title": "Variable Elimination in Hybrid Factor Graphs for Discrete-Continuous Inference & Estimation", "comment": null, "summary": "Many hybrid problems in robotics involve both continuous and discrete components, and modeling them together for estimation tasks has been a long standing and difficult problem. Hybrid Factor Graphs give us a mathematical framework to model these types of problems, however existing approaches for solving them are based on approximations. In this work, we propose an efficient Hybrid Factor Graph framework alongwith a variable elimination algorithm to produce a hybrid Bayes network, which can then be used for exact Maximum A Posteriori estimation and marginalization over both sets of variables. Our approach first develops a novel hybrid Gaussian factor which can connect to both discrete and continuous variables, and a hybrid conditional which can represent multiple continuous hypotheses conditioned on the discrete variables. Using these representations, we derive the process of hybrid variable elimination under the Conditional Linear Gaussian scheme, giving us exact posteriors as hybrid Bayes network. To bound the number of discrete hypotheses, we use a tree-structured representation of the factors coupled with a simple pruning and probabilistic assignment scheme, which allows for tractable inference. We demonstrate the applicability of our framework on a SLAM dataset with ambiguous measurements, where discrete choices for the most likely measurement have to be made. Our demonstrated results showcase the accuracy, generality, and simplicity of our hybrid factor graph framework."}
{"id": "2601.00579", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.00579", "abs": "https://arxiv.org/abs/2601.00579", "authors": ["Obada Kraishan"], "title": "The AI Invisibility Effect: Understanding Human-AI Interaction When Users Don't Recognize Artificial Intelligence", "comment": "18 pages, 4 figures, 8 tables. Data and code available upon request", "summary": "The fast integration of artificial intelligence into mobile applications has completely changed the digital landscape; however, the impact of this change on user perception of AI features remains poorly understood. This large-scale analysis examined 1,484,633 mobile application reviews across 422 applications (200 AI-featuring, 222 control) from iOS App Store and Google Play Store. By employing sentiment classification, topic modeling, and concern-benefit categorization, we identified a major disconnect: only 11.9% of reviews mentioned AI, even though 47.4% of applications featured AI capabilities. AI-featuring applications received significantly lower ratings than traditional applications (d = 0.40); however, hierarchical regression revealed a hidden pattern - the negative relationship reversed after controlling for AI mentions and review characteristics (b = 0.405, p < .001). Privacy dominated user concerns (34.8% of concern-expressing reviews), while efficiency represented the primary benefit (42.3%). Effects varied greatly by category, from positive for Assistant applications (d = 0.55) to negative for Entertainment (d = -0.23). These findings suggest that AI features often operate below user awareness thresholds, and it is the explicit recognition of AI, rather than its mere presence, that drives negative evaluations. This challenges basic assumptions about technology acceptance in AI systems."}
{"id": "2601.00555", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00555", "abs": "https://arxiv.org/abs/2601.00555", "authors": ["Abu Hanif Muhammad Syarubany", "Farhan Zaki Rahmani", "Trio Widianto"], "title": "LLM-Based Agentic Exploration for Robot Navigation & Manipulation with Skill Orchestration", "comment": null, "summary": "This paper presents an end-to-end LLM-based agentic exploration system for an indoor shopping task, evaluated in both Gazebo simulation and a corresponding real-world corridor layout. The robot incrementally builds a lightweight semantic map by detecting signboards at junctions and storing direction-to-POI relations together with estimated junction poses, while AprilTags provide repeatable anchors for approach and alignment. Given a natural-language shopping request, an LLM produces a constrained discrete action at each junction (direction and whether to enter a store), and a ROS finite-state main controller executes the decision by gating modular motion primitives, including local-costmap-based obstacle avoidance, AprilTag approaching, store entry, and grasping. Qualitative results show that the integrated stack can perform end-to-end task execution from user instruction to multi-store navigation and object retrieval, while remaining modular and debuggable through its text-based map and logged decision history."}
{"id": "2601.00592", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.00592", "abs": "https://arxiv.org/abs/2601.00592", "authors": ["Sanjida Islam Era", "Ishika Tarin Ime", "A. B. M. Alim Al Islam"], "title": "Evaluating Web Accessibility and Usability in Bangladesh: A Comparative Analysis of Government and Non-Government Websites", "comment": null, "summary": "Ensuring digital accessibility is essential for inclusive access to online services. However, many government and non-government websites that provide critical services - such as education, healthcare, and public administration - continue to exhibit significant accessibility and usability barriers. This study evaluates the accessibility of Bangladeshi government and non-government websites under WCAG~2.2 by combining automated accessibility assessments with user-reported feedback. A total of 212 websites were analyzed using multiple automated tools, complemented by a survey of 103 users to capture real-world usability, accessibility, and security experiences. The results reveal substantial disparities between government and non-government websites, highlighting persistent issues related to navigation complexity, interaction cost, visual readability, accessibility feature adoption, and authentication mechanisms. While non-government websites generally demonstrate better usability and functional performance, accessibility support remains inconsistent across both categories. The findings underscore the need for regular accessibility audits, user-centered design practices, and policy-driven interventions to improve digital inclusivity and ensure equitable access to online services for diverse user populations."}
{"id": "2601.00580", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.00580", "abs": "https://arxiv.org/abs/2601.00580", "authors": ["Kanghoon Lee", "Hyeonjun Kim", "Jiachen Li", "Jinkyoo Park"], "title": "Priority-Aware Multi-Robot Coverage Path Planning", "comment": "IEEE Robotics and Automation Letters, 8 pages, 10 figures", "summary": "Multi-robot systems are widely used for coverage tasks that require efficient coordination across large environments. In Multi-Robot Coverage Path Planning (MCPP), the objective is typically to minimize the makespan by generating non-overlapping paths for full-area coverage. However, most existing methods assume uniform importance across regions, limiting their effectiveness in scenarios where some zones require faster attention. We introduce the Priority-Aware MCPP (PA-MCPP) problem, where a subset of the environment is designated as prioritized zones with associated weights. The goal is to minimize, in lexicographic order, the total priority-weighted latency of zone coverage and the overall makespan. To address this, we propose a scalable two-phase framework combining (1) greedy zone assignment with local search, spanning-tree-based path planning, and (2) Steiner-tree-guided residual coverage. Experiments across diverse scenarios demonstrate that our method significantly reduces priority-weighted latency compared to standard MCPP baselines, while maintaining competitive makespan. Sensitivity analyses further show that the method scales well with the number of robots and that zone coverage behavior can be effectively controlled by adjusting priority weights."}
{"id": "2601.00670", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.00670", "abs": "https://arxiv.org/abs/2601.00670", "authors": ["Argha Kamal Samanta", "Deepak Mewada", "Monalisa Sarma", "Debasis Samanta"], "title": "Wave2Word: A Multimodal Transformer Framework for Joint EEG-Text Alignment and Multi-Task Representation Learning in Neurocritical Care", "comment": null, "summary": "Continuous electroencephalography (EEG) is routinely used in neurocritical care to monitor seizures and other harmful brain activity, including rhythmic and periodic patterns that are clinically significant. Although deep learning methods have achieved high accuracy in seizure detection, most existing approaches remain seizure-centric, rely on discrete-label supervision, and are primarily evaluated using accuracy-based metrics. A central limitation of current EEG modeling practice is the weak correspondence between learned representations and how EEG findings are interpreted and summarized in clinical workflows. Harmful EEG activity exhibits overlapping patterns, graded expert agreement, and temporal persistence, which are not well captured by classification objectives alone. This work proposes a multimodal EEG representation learning framework that integrates signal-domain modeling with structured clinical language supervision. First, raw EEG is transformed into a longitudinal bipolar montage and time-frequency representations. Second, dual transformer-based encoders model complementary temporal and frequency-centric dependencies and are fused using an adaptive gating mechanism. Third, EEG embeddings are aligned with structured expert consensus descriptions through a contrastive objective. Finally, an EEG-conditioned text reconstruction loss is introduced as a representation-level constraint alongside standard classification loss. Experimental evaluation using a controlled train-validation-test split achieves a six-class test accuracy of 0.9797. Ablation analyses show that removing contrastive alignment reduces cross-modal retrieval performance from Recall@10 of 0.3390 to 0.0045, despite minimal change in classification accuracy. These findings demonstrate that discriminative accuracy does not reliably reflect representation quality for clinically meaningful EEG modeling."}
{"id": "2601.00609", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.00609", "abs": "https://arxiv.org/abs/2601.00609", "authors": ["Mehdi Heydari Shahna", "Pauli Mustalahti", "Jouni Mattila"], "title": "NMPC-Augmented Visual Navigation and Safe Learning Control for Large-Scale Mobile Robots", "comment": null, "summary": "A large-scale mobile robot (LSMR) is a high-order multibody system that often operates on loose, unconsolidated terrain, which reduces traction. This paper presents a comprehensive navigation and control framework for an LSMR that ensures stability and safety-defined performance, delivering robust operation on slip-prone terrain by jointly leveraging high-performance techniques. The proposed architecture comprises four main modules: (1) a visual pose-estimation module that fuses onboard sensors and stereo cameras to provide an accurate, low-latency robot pose, (2) a high-level nonlinear model predictive control that updates the wheel motion commands to correct robot drift from the robot reference pose on slip-prone terrain, (3) a low-level deep neural network control policy that approximates the complex behavior of the wheel-driven actuation mechanism in LSMRs, augmented with robust adaptive control to handle out-of-distribution disturbances, ensuring that the wheels accurately track the updated commands issued by high-level control module, and (4) a logarithmic safety module to monitor the entire robot stack and guarantees safe operation. The proposed low-level control framework guarantees uniform exponential stability of the actuation subsystem, while the safety module ensures the whole system-level safety during operation. Comparative experiments on a 6,000 kg LSMR actuated by two complex electro-hydrostatic drives, while synchronizing modules operating at different frequencies."}
{"id": "2601.00765", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.00765", "abs": "https://arxiv.org/abs/2601.00765", "authors": ["Joslyn Orgill", "Andra Rice", "Max Fowler", "Seth Poulsen"], "title": "The Effect of Transparency on Students' Perceptions of AI Graders", "comment": null, "summary": "The development of effective autograders is key for scaling assessment and feedback. While NLP based autograding systems for open-ended response questions have been found to be beneficial for providing immediate feedback, autograders are not always liked, understood, or trusted by students. Our research tested the effect of transparency on students' attitudes towards autograders. Transparent autograders increased students' perceptions of autograder accuracy and willingness to discuss autograders in survey comments, but did not improve other related attitudes -- such as willingness to be graded by them on a test -- relative to the control without transparency. However, this lack of impact may be due to higher measured student trust towards autograders in this study than in prior work in the field. We briefly discuss possible reasons for this trend."}
{"id": "2601.00610", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00610", "abs": "https://arxiv.org/abs/2601.00610", "authors": ["Mehdi Heydari Shahna", "Pauli Mustalahti", "Jouni Mattila"], "title": "Vision-based Goal-Reaching Control for Mobile Robots Using a Hierarchical Learning Framework", "comment": null, "summary": "Reinforcement learning (RL) is effective in many robotic applications, but it requires extensive exploration of the state-action space, during which behaviors can be unsafe. This significantly limits its applicability to large robots with complex actuators operating on unstable terrain. Hence, to design a safe goal-reaching control framework for large-scale robots, this paper decomposes the whole system into a set of tightly coupled functional modules. 1) A real-time visual pose estimation approach is employed to provide accurate robot states to 2) an RL motion planner for goal-reaching tasks that explicitly respects robot specifications. The RL module generates real-time smooth motion commands for the actuator system, independent of its underlying dynamic complexity. 3) In the actuation mechanism, a supervised deep learning model is trained to capture the complex dynamics of the robot and provide this model to 4) a model-based robust adaptive controller that guarantees the wheels track the RL motion commands even on slip-prone terrain. 5) Finally, to reduce human intervention, a mathematical safety supervisor monitors the robot, stops it on unsafe faults, and autonomously guides it back to a safe inspection area. The proposed framework guarantees uniform exponential stability of the actuation system and safety of the whole operation. Experiments on a 6,000 kg robot in different scenarios confirm the effectiveness of the proposed framework."}
{"id": "2601.00754", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.00754", "abs": "https://arxiv.org/abs/2601.00754", "authors": ["Maria Teresa Parreira", "Isabel Neto", "Filipa Rocha", "Wendy Ju"], "title": "Calling for Backup: How Children Navigate Successive Robot Communication Failures", "comment": null, "summary": "How do children respond to repeated robot errors? While prior research has examined adult reactions to successive robot errors, children's responses remain largely unexplored. In this study, we explore children's reactions to robot social errors and performance errors. For the latter, this study reproduces the successive robot failure paradigm of Liu et al. with child participants (N=59, ages 8-10) to examine how young users respond to repeated robot conversational errors. Participants interacted with a robot that failed to understand their prompts three times in succession, with their behavioral responses video-recorded and analyzed. We found both similarities and differences compared to adult responses from the original study. Like adults, children adjusted their prompts, modified their verbal tone, and exhibited increasingly emotional non-verbal responses throughout successive errors. However, children demonstrated more disengagement behaviors, including temporarily ignoring the robot or actively seeking an adult. Errors did not affect participants' perception of the robot, suggesting more flexible conversational expectations in children. These findings inform the design of more effective and developmentally appropriate human-robot interaction systems for young users."}
{"id": "2601.00614", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.00614", "abs": "https://arxiv.org/abs/2601.00614", "authors": ["Mogens Plessen"], "title": "From 2D to 3D terrain-following area coverage path planning", "comment": "6 pages, 10 figures, 1 table", "summary": "An algorithm for 3D terrain-following area coverage path planning is presented. Multiple adjacent paths are generated that are (i) locally apart from each other by a distance equal to the working width of a machinery, while (ii) simultaneously floating at a projection distance equal to a specific working height above the terrain. The complexities of the algorithm in comparison to its 2D equivalent are highlighted. These include uniformly spaced elevation data generation using an Inverse Distance Weighting-approach and a local search. Area coverage path planning results for real-world 3D data within an agricultural context are presented to validate the algorithm."}
{"id": "2601.00675", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00675", "abs": "https://arxiv.org/abs/2601.00675", "authors": ["Tony Lee", "Andrew Wagenmaker", "Karl Pertsch", "Percy Liang", "Sergey Levine", "Chelsea Finn"], "title": "RoboReward: General-Purpose Vision-Language Reward Models for Robotics", "comment": null, "summary": "A well-designed reward is critical for effective reinforcement learning-based policy improvement. In real-world robotic domains, obtaining such rewards typically requires either labor-intensive human labeling or brittle, handcrafted objectives. Vision-language models (VLMs) have shown promise as automatic reward models, yet their effectiveness on real robot tasks is poorly understood. In this work, we aim to close this gap by introducing (1) \\textbf{RoboReward}, a robotics reward dataset and benchmark built on large-scale real-robot corpora from Open X-Embodiment (OXE) and RoboArena, and (2) vision-language reward models trained on this dataset (RoboReward 4B/8B). Because OXE is success-heavy and lacks failure examples, we propose a \\emph{negative examples data augmentation} pipeline that generates calibrated \\emph{negatives} and \\emph{near-misses} via counterfactual relabeling of successful episodes and temporal clipping to create partial-progress outcomes from the same videos. Using this framework, we produce an extensive training and evaluation dataset that spans diverse tasks and embodiments and enables systematic evaluation of whether state-of-the-art VLMs can reliably provide rewards for robotics. Our evaluation of leading open-weight and proprietary VLMs reveals that no model excels across all tasks, underscoring substantial room for improvement. We then train general-purpose 4B- and 8B-parameter models that outperform much larger VLMs in assigning rewards for short-horizon robotic tasks. Finally, we deploy the 8B-parameter reward VLM in real-robot reinforcement learning and find that it improves policy learning over Gemini Robotics-ER 1.5, a frontier physical reasoning VLM trained on robotics data, by a large margin, while substantially narrowing the gap to RL training with human-provided rewards."}
{"id": "2601.00702", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00702", "abs": "https://arxiv.org/abs/2601.00702", "authors": ["Samuel Cerezo", "Javier Civera"], "title": "DefVINS: Visual-Inertial Odometry for Deformable Scenes", "comment": "4 figures, 3 tables. Submitted to RA-L", "summary": "Deformable scenes violate the rigidity assumptions underpinning classical visual-inertial odometry (VIO), often leading to over-fitting to local non-rigid motion or severe drift when deformation dominates visual parallax. We introduce DefVINS, a visual-inertial odometry framework that explicitly separates a rigid, IMU-anchored state from a non--rigid warp represented by an embedded deformation graph. The system is initialized using a standard VIO procedure that fixes gravity, velocity, and IMU biases, after which non-rigid degrees of freedom are activated progressively as the estimation becomes well conditioned. An observability analysis is included to characterize how inertial measurements constrain the rigid motion and render otherwise unobservable modes identifiable in the presence of deformation. This analysis motivates the use of IMU anchoring and informs a conditioning-based activation strategy that prevents ill-posed updates under poor excitation. Ablation studies demonstrate the benefits of combining inertial constraints with observability-aware deformation activation, resulting in improved robustness under non-rigid environments."}
{"id": "2601.00754", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.00754", "abs": "https://arxiv.org/abs/2601.00754", "authors": ["Maria Teresa Parreira", "Isabel Neto", "Filipa Rocha", "Wendy Ju"], "title": "Calling for Backup: How Children Navigate Successive Robot Communication Failures", "comment": null, "summary": "How do children respond to repeated robot errors? While prior research has examined adult reactions to successive robot errors, children's responses remain largely unexplored. In this study, we explore children's reactions to robot social errors and performance errors. For the latter, this study reproduces the successive robot failure paradigm of Liu et al. with child participants (N=59, ages 8-10) to examine how young users respond to repeated robot conversational errors. Participants interacted with a robot that failed to understand their prompts three times in succession, with their behavioral responses video-recorded and analyzed. We found both similarities and differences compared to adult responses from the original study. Like adults, children adjusted their prompts, modified their verbal tone, and exhibited increasingly emotional non-verbal responses throughout successive errors. However, children demonstrated more disengagement behaviors, including temporarily ignoring the robot or actively seeking an adult. Errors did not affect participants' perception of the robot, suggesting more flexible conversational expectations in children. These findings inform the design of more effective and developmentally appropriate human-robot interaction systems for young users."}
