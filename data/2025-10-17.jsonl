{"id": "2510.13810", "categories": ["cs.HC", "cs.CY", "cs.RO", "I.2.9; J.5; K.4.2"], "pdf": "https://arxiv.org/pdf/2510.13810", "abs": "https://arxiv.org/abs/2510.13810", "authors": ["Minja Axelsson", "Lea Luka Sikau"], "title": "Choreographing Trash Cans: On Speculative Futures of Weak Robots in Public Spaces", "comment": "Accepted at the British Computer Society's Special Interest Group in\n  Human Computer Interaction Conference (BCS HCI 2025), Futures track. 5 pages,\n  no figures", "summary": "Delivering groceries or cleaning airports, mobile robots exist in public\nspaces. While these examples showcase robots that execute tasks, this paper\nexplores mobile robots that encourage posthuman collaboration rather than\nmanaging environments independently. With feigned fragility, cuteness and\nincomplete functionalities, the so-called \"weak robots\" invite passersby to\nengage not only on a utilitarian level, but also through imaginative and\nemotional responses. After examining the workings of \"weak robots\" by queering\nnotions of function and ability, we introduce two speculative design fiction\nvignettes that describe choreographies of such robots in future urban spaces --\none exploring a utopian weak robot and the other a dystopian weak robot. We\nintroduce these speculations in order to discuss how different values may drive\ndesign decisions, and how such decisions may shape and drive different\nsocio-technical futures in which robots and humans share public spaces that\nincentivise collaboration."}
{"id": "2510.13811", "categories": ["cs.HC", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13811", "abs": "https://arxiv.org/abs/2510.13811", "authors": ["Jessica Witte", "Edmund Lee", "Lisa Brausem", "Verity Shillabeer", "Chiara Bonacchi"], "title": "Generative AI in Heritage Practice: Improving the Accessibility of Heritage Guidance", "comment": "21 pages", "summary": "This paper discusses the potential for integrating Generative Artificial\nIntelligence (GenAI) into professional heritage practice with the aim of\nenhancing the accessibility of public-facing guidance documents. We developed\nHAZEL, a GenAI chatbot fine-tuned to assist with revising written guidance\nrelating to heritage conservation and interpretation. Using quantitative\nassessments, we compare HAZEL's performance to that of ChatGPT (GPT-4) in a\nseries of tasks related to the guidance writing process. The results of this\ncomparison indicate a slightly better performance of HAZEL over ChatGPT,\nsuggesting that the GenAI chatbot is more effective once the underlying large\nlanguage model (LLM) has been fine-tuned. However, we also note significant\nlimitations, particularly in areas requiring cultural sensitivity and more\nadvanced technical expertise. These findings suggest that, while GenAI cannot\nreplace human heritage professionals in technical authoring tasks, its\npotential to automate and expedite certain aspects of guidance writing could\noffer valuable benefits to heritage organisations, especially in\nresource-constrained contexts."}
{"id": "2510.13812", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.13812", "abs": "https://arxiv.org/abs/2510.13812", "authors": ["Bridget Dwyer", "Matthew Flathers", "Akane Sano", "Allison Dempsey", "Andrea Cipriani", "Asim H. Gazi", "Carla Gorban", "Carolyn I. Rodriguez", "Charles Stromeyer IV", "Darlene King", "Eden Rozenblit", "Gillian Strudwick", "Jake Linardon", "Jiaee Cheong", "Joseph Firth", "Julian Herpertz", "Julian Schwarz", "Margaret Emerson", "Martin P. Paulus", "Michelle Patriquin", "Yining Hua", "Soumya Choudhary", "Steven Siddals", "Laura Ospina Pinillos", "Jason Bantjes", "Steven Scheuller", "Xuhai Xu", "Ken Duckworth", "Daniel H. Gillison", "Michael Wood", "John Torous"], "title": "MindBenchAI: An Actionable Platform to Evaluate the Profile and Performance of Large Language Models in a Mental Healthcare Context", "comment": null, "summary": "Individuals are increasingly utilizing large language model (LLM)based tools\nfor mental health guidance and crisis support in place of human experts. While\nAI technology has great potential to improve health outcomes, insufficient\nempirical evidence exists to suggest that AI technology can be deployed as a\nclinical replacement; thus, there is an urgent need to assess and regulate such\ntools. Regulatory efforts have been made and multiple evaluation frameworks\nhave been proposed, however,field-wide assessment metrics have yet to be\nformally integrated. In this paper, we introduce a comprehensive online\nplatform that aggregates evaluation approaches and serves as a dynamic online\nresource to simplify LLM and LLM-based tool assessment: MindBenchAI. At its\ncore, MindBenchAI is designed to provide easily accessible/interpretable\ninformation for diverse stakeholders (patients, clinicians, developers,\nregulators, etc.). To create MindBenchAI, we built off our work developing\nMINDapps.org to support informed decision-making around smartphone app use for\nmental health, and expanded the technical MINDapps.org framework to encompass\nnovel large language model (LLM) functionalities through benchmarking\napproaches. The MindBenchAI platform is designed as a partnership with the\nNational Alliance on Mental Illness (NAMI) to provide assessment tools that\nsystematically evaluate LLMs and LLM-based tools with objective and transparent\ncriteria from a healthcare standpoint, assessing both profile (i.e. technical\nfeatures, privacy protections, and conversational style) and performance\ncharacteristics (i.e. clinical reasoning skills)."}
{"id": "2510.13813", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.13813", "abs": "https://arxiv.org/abs/2510.13813", "authors": ["Sunny Choi"], "title": "Puzzlegram: a Serious Game Designed for the Elderly in Group Settings", "comment": "7 pages, 3 figures", "summary": "An original serious game prototype named 'Puzzlegram' is created for the\nelderly demographic in group settings as the target players. Puzzlegram is\nprecisely designed to accentuate memory, auditory interaction as well as haptic\nresponse to visual signals with the use of music. Music is introduced as a key\ncomponent for establishing the game design that provides a source of meaningful\ncontextualization (familiar music from the past) for setting the game\nmechanics, which facilitated the construction of the serious game design\nprocess. The discussion topics raised include the need to design serious games\nfor fostering meaningful interactions, as well as developing a thorough\nframework for constructing purposeful design for serious games. A potential\nintegral of artificial intelligence to Puzzlegram may involve assigning a novel\ndimension to its existing problem solving task by adapting to varying states of\ncognitive function for monitoring purposes based on an individual's interaction\nwith the game."}
{"id": "2510.14000", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14000", "abs": "https://arxiv.org/abs/2510.14000", "authors": ["Mingyang Jiang", "Yueyuan Li", "Jiaru Zhang", "Songan Zhang", "Ming Yang"], "title": "A Diffusion-Refined Planner with Reinforcement Learning Priors for Confined-Space Parking", "comment": null, "summary": "The growing demand for parking has increased the need for automated parking\nplanning methods that can operate reliably in confined spaces. In restricted\nand complex environments, high-precision maneuvers are required to achieve a\nhigh success rate in planning, yet existing approaches often rely on explicit\naction modeling, which faces challenges when accurately modeling the optimal\naction distribution. In this paper, we propose DRIP, a diffusion-refined\nplanner anchored in reinforcement learning (RL) prior action distribution, in\nwhich an RL-pretrained policy provides prior action distributions to regularize\nthe diffusion training process. During the inference phase the denoising\nprocess refines these coarse priors into more precise action distributions. By\nsteering the denoising trajectory through the reinforcement learning prior\ndistribution during training, the diffusion model inherits a well-informed\ninitialization, resulting in more accurate action modeling, a higher planning\nsuccess rate, and reduced inference steps. We evaluate our approach across\nparking scenarios with varying degrees of spatial constraints. Experimental\nresults demonstrate that our method significantly improves planning performance\nin confined-space parking environments while maintaining strong generalization\nin common scenarios."}
{"id": "2510.13814", "categories": ["cs.HC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13814", "abs": "https://arxiv.org/abs/2510.13814", "authors": ["Roussel Rahman", "Aashwin Ananda Mishra", "Wan-Lin Hu"], "title": "Reversing the Lens: Using Explainable AI to Understand Human Expertise", "comment": null, "summary": "Both humans and machine learning models learn from experience, particularly\nin safety- and reliability-critical domains. While psychology seeks to\nunderstand human cognition, the field of Explainable AI (XAI) develops methods\nto interpret machine learning models. This study bridges these domains by\napplying computational tools from XAI to analyze human learning. We modeled\nhuman behavior during a complex real-world task -- tuning a particle\naccelerator -- by constructing graphs of operator subtasks. Applying techniques\nsuch as community detection and hierarchical clustering to archival operator\ndata, we reveal how operators decompose the problem into simpler components and\nhow these problem-solving structures evolve with expertise. Our findings\nilluminate how humans develop efficient strategies in the absence of globally\noptimal solutions, and demonstrate the utility of XAI-based methods for\nquantitatively studying human cognition."}
{"id": "2510.14018", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14018", "abs": "https://arxiv.org/abs/2510.14018", "authors": ["Adam Morris", "Timothy Pelham", "Edmund R. Hunt"], "title": "Spatially Intelligent Patrol Routes for Concealed Emitter Localization by Robot Swarms", "comment": null, "summary": "This paper introduces a method for designing spatially intelligent robot\nswarm behaviors to localize concealed radio emitters. We use differential\nevolution to generate geometric patrol routes that localize unknown signals\nindependently of emitter parameters, a key challenge in electromagnetic\nsurveillance. Patrol shape and antenna type are shown to influence information\ngain, which in turn determines the effective triangulation coverage. We\nsimulate a four-robot swarm across eight configurations, assigning\npre-generated patrol routes based on a specified patrol shape and sensing\ncapability (antenna type: omnidirectional or directional). An emitter is placed\nwithin the map for each trial, with randomized position, transmission power and\nfrequency. Results show that omnidirectional localization success rates are\ndriven primarily by source location rather than signal properties, with\nfailures occurring most often when sources are placed in peripheral areas of\nthe map. Directional antennas are able to overcome this limitation due to their\nhigher gain and directivity, with an average detection success rate of 98.75%\ncompared to 80.25% for omnidirectional. Average localization errors range from\n1.01-1.30 m for directional sensing and 1.67-1.90 m for omnidirectional\nsensing; while directional sensing also benefits from shorter patrol edges.\nThese results demonstrate that a swarm's ability to predict electromagnetic\nphenomena is directly dependent on its physical interaction with the\nenvironment. Consequently, spatial intelligence, realized here through\noptimized patrol routes and antenna selection, is a critical design\nconsideration for effective robotic surveillance."}
{"id": "2510.14141", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.14141", "abs": "https://arxiv.org/abs/2510.14141", "authors": ["Teale W. Masrani", "Geoffrey Messier", "Amy Voida", "Gina Dimitropoulos", "Helen Ai He"], "title": "Understanding Data Usage when Making High-Stakes Frontline Decisions in Homelessness Services", "comment": null, "summary": "Frontline staff of emergency shelters face challenges such as vicarious\ntrauma, compassion fatigue, and burnout. The technology they use is often not\ndesigned for their unique needs, and can feel burdensome on top of their\nalready cognitively and emotionally taxing work. While existing literature\nfocuses on data-driven technologies that automate or streamline frontline\ndecision-making about vulnerable individuals, we discuss scenarios in which\nstaff may resist such automation. We then suggest how data-driven technologies\ncan better align with their human-centred decision-making processes. This paper\npresents findings from a qualitative fieldwork study conducted from 2022 to\n2024 at a large emergency shelter in Canada. The goal of this fieldwork was to\nco-design, develop, and deploy an interactive data-navigation interface that\nsupports frontline staff when making collaborative, high-stakes decisions about\nindividuals experiencing homelessness. By reflecting on this fieldwork, we\ncontribute insight into the role that administrative shelter data play during\ndecision-making, and unpack staff members' apparent reluctance to outsource\ndecisions about vulnerable individuals to data systems. Our findings suggest a\ndata-outsourcing continuum, which we discuss in terms of how designers may\ncreate technologies to support compassionate, data-driven decision-making in\nnonprofit domains."}
{"id": "2510.14063", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14063", "abs": "https://arxiv.org/abs/2510.14063", "authors": ["Nan Li", "Jiming Ren", "Haris Miller", "Samuel Coogan", "Karen M. Feigh", "Ye Zhao"], "title": "Adaptive Obstacle-Aware Task Assignment and Planning for Heterogeneous Robot Teaming", "comment": "16 pages, 11 figures, 4 tables", "summary": "Multi-Agent Task Assignment and Planning (MATP) has attracted growing\nattention but remains challenging in terms of scalability, spatial reasoning,\nand adaptability in obstacle-rich environments. To address these challenges, we\npropose OATH: Adaptive Obstacle-Aware Task Assignment and Planning for\nHeterogeneous Robot Teaming, which advances MATP by introducing a novel\nobstacle-aware strategy for task assignment. First, we develop an adaptive\nHalton sequence map, the first known application of Halton sampling with\nobstacle-aware adaptation in MATP, which adjusts sampling density based on\nobstacle distribution. Second, we propose a cluster-auction-selection framework\nthat integrates obstacle-aware clustering with weighted auctions and\nintra-cluster task selection. These mechanisms jointly enable effective\ncoordination among heterogeneous robots while maintaining scalability and\nnear-optimal allocation performance. In addition, our framework leverages an\nLLM to interpret human instructions and directly guide the planner in real\ntime. We validate OATH in NVIDIA Isaac Sim, showing substantial improvements in\ntask assignment quality, scalability, adaptability to dynamic changes, and\noverall execution performance compared to state-of-the-art MATP baselines. A\nproject website is available at https://llm-oath.github.io/."}
{"id": "2510.14247", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.14247", "abs": "https://arxiv.org/abs/2510.14247", "authors": ["Kentaro Takahira", "Yuki Ueno"], "title": "VisAider: AI-Assisted Context-Aware Visualization Support for Data Presentations", "comment": null, "summary": "Effective real-time data presentation is essential in small-group interactive\ncontexts, where discussions evolve dynamically and presenters must adapt\nvisualizations to shifting audience interests. However, most existing\ninteractive visualization systems rely on fixed mappings between user actions\nand visualization commands, limiting their ability to support richer operations\nsuch as changing visualization types, adjusting data transformations, or\nincorporating additional datasets on the fly during live presentations. This\nwork-in-progress paper presents VisAider, an AI-assisted interactive data\npresentation prototype that continuously analyzes the live presentation\ncontext, including the available dataset, active visualization, ongoing\nconversation, and audience profile, to generate ranked suggestions for relevant\nvisualization aids. Grounded in a formative study with experienced data\nanalysts, we identified key challenges in adapting visual content in real time\nand distilled design considerations to guide system development. A prototype\nimplementation demonstrates the feasibility of this approach in simulated\nscenarios, and preliminary testing highlights challenges in inferring\nappropriate data transformations, resolving ambiguous visualization tasks, and\nachieving low-latency responsiveness. Ongoing work focuses on addressing these\nlimitations, integrating the system into presentation environments, and\npreparing a summative user study to evaluate usability and communicative\nimpact."}
{"id": "2510.14065", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14065", "abs": "https://arxiv.org/abs/2510.14065", "authors": ["Gaoyuan Liu", "Joris de Winter", "Yuri Durodie", "Denis Steckelmacher", "Ann Nowe", "Bram Vanderborght"], "title": "Optimistic Reinforcement Learning-Based Skill Insertions for Task and Motion Planning", "comment": null, "summary": "Task and motion planning (TAMP) for robotics manipulation necessitates\nlong-horizon reasoning involving versatile actions and skills. While\ndeterministic actions can be crafted by sampling or optimizing with certain\nconstraints, planning actions with uncertainty, i.e., probabilistic actions,\nremains a challenge for TAMP. On the contrary, Reinforcement Learning (RL)\nexcels in acquiring versatile, yet short-horizon, manipulation skills that are\nrobust with uncertainties. In this letter, we design a method that integrates\nRL skills into TAMP pipelines. Besides the policy, a RL skill is defined with\ndata-driven logical components that enable the skill to be deployed by symbolic\nplanning. A plan refinement sub-routine is designed to further tackle the\ninevitable effect uncertainties. In the experiments, we compare our method with\nbaseline hierarchical planning from both TAMP and RL fields and illustrate the\nstrength of the method. The results show that by embedding RL skills, we extend\nthe capability of TAMP to domains with probabilistic skills, and improve the\nplanning efficiency compared to the previous methods."}
{"id": "2510.14267", "categories": ["cs.HC", "H.5.2; K.4.2; H.1.2"], "pdf": "https://arxiv.org/pdf/2510.14267", "abs": "https://arxiv.org/abs/2510.14267", "authors": ["Ricardo Gonzalez", "Fannie Liu", "Blair MacIntyre", "David Saffo"], "title": "TapNav: Adaptive Spatiotactile Screen Readers for Tactually Guided Touchscreen Interactions for Blind and Low Vision People", "comment": "29 pages, 6 figure, 5 tables, Preprint of manuscript accepted to\n  IMWUT. To appear on December Issue 2025", "summary": "Screen readers are audio-based software that Blind and Low Vision (BLV)\npeople use to interact with computing devices, such as tablets and smartphones.\nAlthough this technology has significantly improved the accessibility of\ntouchscreen devices, the sequential nature of audio limits the bandwidth of\ninformation users can receive and process. We introduce TapNav, an adaptive\nspatiotactile screen reader prototype developed to interact with touchscreen\ninterfaces spatially. TapNav's screen reader provides adaptive auditory\nfeedback that, in combination with a tactile overlay, conveys spatial\ninformation and location of interface elements on-screen. We evaluated TapNav\nwith 12 BLV users who interacted with TapNav to explore a data visualization\nand interact with a bank transactions application. Our qualitative findings\nshow that touch points and spatially constrained navigation helped users\nanticipate outcomes for faster exploration, and offload cognitive load to\ntouch. We provide design guidelines for creating tactile overlays for adaptive\nspatiotactile screen readers and discuss their generalizability beyond our\nexploratory data analysis and everyday application navigation scenarios."}
{"id": "2510.14072", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14072", "abs": "https://arxiv.org/abs/2510.14072", "authors": ["Hemjyoti Das", "Christian Ott"], "title": "Partial Feedback Linearization Control of a Cable-Suspended Multirotor Platform for Stabilization of an Attached Load", "comment": "Accepted for IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025", "summary": "In this work, we present a novel control approach based on partial feedback\nlinearization (PFL) for the stabilization of a suspended aerial platform with\nan attached load. Such systems are envisioned for various applications in\nconstruction sites involving cranes, such as the holding and transportation of\nheavy objects. Our proposed control approach considers the underactuation of\nthe whole system while utilizing its coupled dynamics for stabilization. We\ndemonstrate using numerical stability analysis that these coupled terms are\ncrucial for the stabilization of the complete system. We also carried out\nrobustness analysis of the proposed approach in the presence of external wind\ndisturbances, sensor noise, and uncertainties in system dynamics. As our\nenvisioned target application involves cranes in outdoor construction sites,\nour control approaches rely on only onboard sensors, thus making it suitable\nfor such applications. We carried out extensive simulation studies and\nexperimental tests to validate our proposed control approach."}
{"id": "2510.14277", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.14277", "abs": "https://arxiv.org/abs/2510.14277", "authors": ["Yichen Yu", "Yifan Jiang", "Mandy Lui", "Qiao Jin"], "title": "GenLARP: Enabling Immersive Live Action Role-Play through LLM-Generated Worlds and Characters", "comment": null, "summary": "We introduce GenLARP, a virtual reality (VR) system that transforms\npersonalized stories into immersive live action role-playing (LARP)\nexperiences. GenLARP enables users to act as both creators and players,\nallowing them to design characters based on their descriptions and live in the\nstory world. Generative AI and agents powered by Large Language Models (LLMs)\nenrich these experiences."}
{"id": "2510.14117", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14117", "abs": "https://arxiv.org/abs/2510.14117", "authors": ["Zhiyuan Wu", "Yijiong Lin", "Yongqiang Zhao", "Xuyang Zhang", "Zhuo Chen", "Nathan Lepora", "Shan Luo"], "title": "ViTacGen: Robotic Pushing with Vision-to-Touch Generation", "comment": null, "summary": "Robotic pushing is a fundamental manipulation task that requires tactile\nfeedback to capture subtle contact forces and dynamics between the end-effector\nand the object. However, real tactile sensors often face hardware limitations\nsuch as high costs and fragility, and deployment challenges involving\ncalibration and variations between different sensors, while vision-only\npolicies struggle with satisfactory performance. Inspired by humans' ability to\ninfer tactile states from vision, we propose ViTacGen, a novel robot\nmanipulation framework designed for visual robotic pushing with vision-to-touch\ngeneration in reinforcement learning to eliminate the reliance on\nhigh-resolution real tactile sensors, enabling effective zero-shot deployment\non visual-only robotic systems. Specifically, ViTacGen consists of an\nencoder-decoder vision-to-touch generation network that generates contact depth\nimages, a standardized tactile representation, directly from visual image\nsequence, followed by a reinforcement learning policy that fuses visual-tactile\ndata with contrastive learning based on visual and generated tactile\nobservations. We validate the effectiveness of our approach in both simulation\nand real world experiments, demonstrating its superior performance and\nachieving a success rate of up to 86\\%."}
{"id": "2510.14308", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.14308", "abs": "https://arxiv.org/abs/2510.14308", "authors": ["Yimeng Liu", "Misha Sra", "Jeevana Priya Inala", "Chenglong Wang"], "title": "ReUseIt: Synthesizing Reusable AI Agent Workflows for Web Automation", "comment": null, "summary": "AI-powered web agents have the potential to automate repetitive tasks, such\nas form filling, information retrieval, and scheduling, but they struggle to\nreliably execute these tasks without human intervention, requiring users to\nprovide detailed guidance during every run. We address this limitation by\nautomatically synthesizing reusable workflows from an agent's successful and\nfailed attempts. These workflows incorporate execution guards that help agents\ndetect and fix errors while keeping users informed of progress and issues. Our\napproach enables agents to successfully complete repetitive tasks of the same\ntype with minimal intervention, increasing the success rates from 24.2% to\n70.1% across fifteen tasks. To evaluate this approach, we invited nine users\nand found that our agent helped them complete web tasks with a higher success\nrate and less guidance compared to two baseline methods, as well as allowed\nusers to easily monitor agent behavior and understand failures."}
{"id": "2510.14234", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.14234", "abs": "https://arxiv.org/abs/2510.14234", "authors": ["Ning Han", "Gu Gong", "Bin Zhang", "Yuexuan Xu", "Bohan Yang", "Yunhui Liu", "David Navarro-Alarcon"], "title": "Prescribed Performance Control of Deformable Object Manipulation in Spatial Latent Space", "comment": null, "summary": "Manipulating three-dimensional (3D) deformable objects presents significant\nchallenges for robotic systems due to their infinite-dimensional state space\nand complex deformable dynamics. This paper proposes a novel model-free\napproach for shape control with constraints imposed on key points. Unlike\nexisting methods that rely on feature dimensionality reduction, the proposed\ncontroller leverages the coordinates of key points as the feature vector, which\nare extracted from the deformable object's point cloud using deep learning\nmethods. This approach not only reduces the dimensionality of the feature space\nbut also retains the spatial information of the object. By extracting key\npoints, the manipulation of deformable objects is simplified into a visual\nservoing problem, where the shape dynamics are described using a deformation\nJacobian matrix. To enhance control accuracy, a prescribed performance control\nmethod is developed by integrating barrier Lyapunov functions (BLF) to enforce\nconstraints on the key points. The stability of the closed-loop system is\nrigorously analyzed and verified using the Lyapunov method. Experimental\nresults further demonstrate the effectiveness and robustness of the proposed\nmethod."}
{"id": "2510.14513", "categories": ["cs.HC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14513", "abs": "https://arxiv.org/abs/2510.14513", "authors": ["Juheon Choi", "Juyoung Lee", "Jian Kim", "Chanyoung Kim", "Taewon Min", "W. Bradley Knox", "Min Kyung Lee", "Kimin Lee"], "title": "State Your Intention to Steer Your Attention: An AI Assistant for Intentional Digital Living", "comment": null, "summary": "When working on digital devices, people often face distractions that can lead\nto a decline in productivity and efficiency, as well as negative psychological\nand emotional impacts. To address this challenge, we introduce a novel\nArtificial Intelligence (AI) assistant that elicits a user's intention,\nassesses whether ongoing activities are in line with that intention, and\nprovides gentle nudges when deviations occur. The system leverages a large\nlanguage model to analyze screenshots, application titles, and URLs, issuing\nnotifications when behavior diverges from the stated goal. Its detection\naccuracy is refined through initial clarification dialogues and continuous user\nfeedback. In a three-week, within-subjects field deployment with 22\nparticipants, we compared our assistant to both a rule-based intent reminder\nsystem and a passive baseline that only logged activity. Results indicate that\nour AI assistant effectively supports users in maintaining focus and aligning\ntheir digital behavior with their intentions. Our source code is publicly\navailable at this url https://intentassistant.github.io"}
{"id": "2510.14293", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14293", "abs": "https://arxiv.org/abs/2510.14293", "authors": ["Yushi Du", "Yixuan Li", "Baoxiong Jia", "Yutang Lin", "Pei Zhou", "Wei Liang", "Yanchao Yang", "Siyuan Huang"], "title": "Learning Human-Humanoid Coordination for Collaborative Object Carrying", "comment": null, "summary": "Human-humanoid collaboration shows significant promise for applications in\nhealthcare, domestic assistance, and manufacturing. While compliant robot-human\ncollaboration has been extensively developed for robotic arms, enabling\ncompliant human-humanoid collaboration remains largely unexplored due to\nhumanoids' complex whole-body dynamics. In this paper, we propose a\nproprioception-only reinforcement learning approach, COLA, that combines leader\nand follower behaviors within a single policy. The model is trained in a\nclosed-loop environment with dynamic object interactions to predict object\nmotion patterns and human intentions implicitly, enabling compliant\ncollaboration to maintain load balance through coordinated trajectory planning.\nWe evaluate our approach through comprehensive simulator and real-world\nexperiments on collaborative carrying tasks, demonstrating the effectiveness,\ngeneralization, and robustness of our model across various terrains and\nobjects. Simulation experiments demonstrate that our model reduces human effort\nby 24.7%. compared to baseline approaches while maintaining object stability.\nReal-world experiments validate robust collaborative carrying across different\nobject types (boxes, desks, stretchers, etc.) and movement patterns\n(straight-line, turning, slope climbing). Human user studies with 23\nparticipants confirm an average improvement of 27.4% compared to baseline\nmodels. Our method enables compliant human-humanoid collaborative carrying\nwithout requiring external sensors or complex interaction models, offering a\npractical solution for real-world deployment."}
{"id": "2510.14591", "categories": ["cs.HC", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14591", "abs": "https://arxiv.org/abs/2510.14591", "authors": ["Michelle S. Lam", "Omar Shaikh", "Hallie Xu", "Alice Guo", "Diyi Yang", "Jeffrey Heer", "James A. Landay", "Michael S. Bernstein"], "title": "Just-In-Time Objectives: A General Approach for Specialized AI Interactions", "comment": null, "summary": "Large language models promise a broad set of functions, but when not given a\nspecific objective, they default to milquetoast results such as drafting emails\nlittered with cliches. We demonstrate that inferring the user's in-the-moment\nobjective, then rapidly optimizing for that singular objective, enables LLMs to\nproduce tools, interfaces, and responses that are more responsive and desired.\nWe contribute an architecture for automatically inducing just-in-time\nobjectives by passively observing user behavior, then steering downstream AI\nsystems through generation and evaluation against this objective. Inducing\njust-in-time objectives (e.g., \"Clarify the abstract's research contribution\")\nenables automatic generation of tools, e.g., those that critique a draft based\non relevant HCI methodologies, anticipate related researchers' reactions, or\nsurface ambiguous terminology. In a series of experiments (N=14, N=205) on\nparticipants' own tasks, JIT objectives enable LLM outputs that achieve 66-86%\nwin rates over typical LLMs, and in-person use sessions (N=17) confirm that JIT\nobjectives produce specialized tools unique to each participant."}
{"id": "2510.14300", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14300", "abs": "https://arxiv.org/abs/2510.14300", "authors": ["Weijie Shen", "Yitian Liu", "Yuhao Wu", "Zhixuan Liang", "Sijia Gu", "Dehui Wang", "Tian Nian", "Lei Xu", "Yusen Qin", "Jiangmiao Pang", "Xinping Guan", "Xiaokang Yang", "Yao Mu"], "title": "Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning", "comment": null, "summary": "Vision-Language-Action (VLA) models are experiencing rapid development and\ndemonstrating promising capabilities in robotic manipulation tasks. However,\nscaling up VLA models presents several critical challenges: (1) Training new\nVLA models from scratch demands substantial computational resources and\nextensive datasets. Given the current scarcity of robot data, it becomes\nparticularly valuable to fully leverage well-pretrained VLA model weights\nduring the scaling process. (2) Real-time control requires carefully balancing\nmodel capacity with computational efficiency. To address these challenges, We\npropose AdaMoE, a Mixture-of-Experts (MoE) architecture that inherits\npretrained weights from dense VLA models, and scales up the action expert by\nsubstituting the feedforward layers into sparsely activated MoE layers. AdaMoE\nemploys a decoupling technique that decouples expert selection from expert\nweighting through an independent scale adapter working alongside the\ntraditional router. This enables experts to be selected based on task relevance\nwhile contributing with independently controlled weights, allowing\ncollaborative expert utilization rather than winner-takes-all dynamics. Our\napproach demonstrates that expertise need not monopolize. Instead, through\ncollaborative expert utilization, we can achieve superior performance while\nmaintaining computational efficiency. AdaMoE consistently outperforms the\nbaseline model across key benchmarks, delivering performance gains of 1.8% on\nLIBERO and 9.3% on RoboTwin. Most importantly, a substantial 21.5% improvement\nin real-world experiments validates its practical effectiveness for robotic\nmanipulation tasks."}
{"id": "2510.14598", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.14598", "abs": "https://arxiv.org/abs/2510.14598", "authors": ["Francesco Vona", "Giulia Valcamonica", "Franca Garzotto"], "title": "Two Explorative Studies on Tangible Augmented Reality for Neurodevelopmental Disorders", "comment": "16 pages", "summary": "Tangible Augmented Reality (TAR) is an interaction paradigm that integrates\nphysical and digital worlds to create immersive, interactive experiences. This\npaper explores two TAR applications, Holomarket and Along the Oceanic Flow\n(ATOF), and presents insights from two exploratory studies evaluating their\nusability and likeability among individuals with neurodevelopmental disorders\n(NDD). Holomarket is designed to simulate a supermarket shopping experience,\nhelping users develop essential life skills such as item selection, basic\narithmetic, and money handling. Participants interacted with augmented food\nitems and a smart cash register, navigating a virtual supermarket environment.\nWhile participants enjoyed the realistic setting and tangible interactions,\nsome usability challenges, such as difficulty manipulating virtual objects and\ndiscomfort with prolonged headset use, were noted. ATOF transforms the user\nenvironment into an oceanic world, where participants use a dolphin-shaped\nsmart object to complete tasks like collecting items and solving puzzles. This\napplication aims to improve motor coordination and cognitive skills.\nParticipants appreciated the immersive experience, the customizable tasks, and\nthe tangible dolphin interface. However, some faced difficulties interacting\nwith specific virtual elements. Overall, both applications demonstrated\npotential as therapeutic tools for NDD, offering engaging and immersive\nexperiences. Despite some usability challenges and hardware limitations, the\npositive feedback suggests that TAR could play a crucial role in future\ntherapeutic interventions. Further research is needed to refine these\napplications and enhance user interaction and comfort."}
{"id": "2510.14338", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14338", "abs": "https://arxiv.org/abs/2510.14338", "authors": ["Yuanhong Zeng", "Anushri Dixit"], "title": "Risk-Aware Reinforcement Learning with Bandit-Based Adaptation for Quadrupedal Locomotion", "comment": null, "summary": "In this work, we study risk-aware reinforcement learning for quadrupedal\nlocomotion. Our approach trains a family of risk-conditioned policies using a\nConditional Value-at-Risk (CVaR) constrained policy optimization technique that\nprovides improved stability and sample efficiency. At deployment, we adaptively\nselect the best performing policy from the family of policies using a\nmulti-armed bandit framework that uses only observed episodic returns, without\nany privileged environment information, and adapts to unknown conditions on the\nfly. Hence, we train quadrupedal locomotion policies at various levels of\nrobustness using CVaR and adaptively select the desired level of robustness\nonline to ensure performance in unknown environments. We evaluate our method in\nsimulation across eight unseen settings (by changing dynamics, contacts,\nsensing noise, and terrain) and on a Unitree Go2 robot in previously unseen\nterrains. Our risk-aware policy attains nearly twice the mean and tail\nperformance in unseen environments compared to other baselines and our\nbandit-based adaptation selects the best-performing risk-aware policy in\nunknown terrain within two minutes of operation."}
{"id": "2510.14603", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.14603", "abs": "https://arxiv.org/abs/2510.14603", "authors": ["Francesco Vona", "Michael Stern", "Navid Ashrafi", "Julia Schorlemmer", "Jessica Stemann", "Jan-Niklas Voigt-Antons"], "title": "Sales Skills Training in Virtual Reality: An evaluation utilizing CAVE and Virtual Avatars", "comment": null, "summary": "This study investigates the potential of virtual reality (VR) for enhancing\nsales skills training using a Cave Automatic Virtual Environment (CAVE). VR\ntechnology enables users to practice interpersonal and negotiation skills in\ncontrolled, immersive environments that mimic real-world scenarios. In this\nstudy, participants engaged in sales simulations set in a virtual dealership,\ninteracting with avatars in different work settings and with various\ncommunication styles. The research employed a within-subjects experimental\ndesign involving 20 university students. Each participant experienced four\ndistinct sales scenarios randomized for environmental and customer conditions.\nTraining effectiveness was assessed using validated metrics alongside custom\nexperience questions. Findings revealed consistent user experience and presence\nacross all scenarios, with no significant differences detected based on\ncommunication styles or environmental conditions. The study highlights the\nadvantages of semi-immersive VR systems for collaborative learning, peer\nfeedback, and realistic training environments. However, further research is\nrecommended to refine VR designs, improve engagement, and maximize skills\ntransfer to real-world applications."}
{"id": "2510.14357", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14357", "abs": "https://arxiv.org/abs/2510.14357", "authors": ["Xiaobei Zhao", "Xingqi Lyu", "Xiang Li"], "title": "SUM-AgriVLN: Spatial Understanding Memory for Agricultural Vision-and-Language Navigation", "comment": null, "summary": "Agricultural robots are emerging as powerful assistants across a wide range\nof agricultural tasks, nevertheless, still heavily rely on manual operation or\nfixed rail systems for movement. The AgriVLN method and the A2A benchmark\npioneeringly extend Vision-and-Language Navigation (VLN) to the agricultural\ndomain, enabling robots to navigate to the target positions following the\nnatural language instructions. In practical agricultural scenarios, navigation\ninstructions often repeatedly occur, yet AgriVLN treat each instruction as an\nindependent episode, overlooking the potential of past experiences to provide\nspatial context for subsequent ones. To bridge this gap, we propose the method\nof Spatial Understanding Memory for Agricultural Vision-and-Language Navigation\n(SUM-AgriVLN), in which the SUM module employs spatial understanding and save\nspatial memory through 3D reconstruction and representation. When evaluated on\nthe A2A benchmark, our SUM-AgriVLN effectively improves Success Rate from 0.47\nto 0.54 with slight sacrifice on Navigation Error from 2.91m to 2.93m,\ndemonstrating the state-of-the-art performance in the agricultural domain.\nCode: https://github.com/AlexTraveling/SUM-AgriVLN."}
{"id": "2510.14607", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.14607", "abs": "https://arxiv.org/abs/2510.14607", "authors": ["Francesco Vona", "Evelyn Romanjuk", "Sina Hinzmann", "Julia Schorlemmer", "Navid Ashrafi", "Jan-Niklas Voigt-Antons"], "title": "Exploring the Effects of Different Asymmetric Game Designs on User Experience in Collaborative Virtual Reality", "comment": null, "summary": "The risk of isolation in virtual reality (VR) stems from the immersive nature\nof the technology. VR can transport users to entirely virtual environments,\noften disconnecting them from the physical world and real-life interactions.\nAsymmetric multiplayer options have been explored to address this issue and\nencourage social interaction by requiring players to communicate and\ncollaborate to achieve common objectives. Nevertheless, research on\nimplementing these designs and their effects is limited, mainly due to the\nnovelty of multiplayer VR gaming. This article investigates how different game\ndesign approaches affect the player experience during an asymmetric multiplayer\nVR game. Four versions of a VR experience were created and tested in a study\ninvolving 74 participants. Each version differs in terms of the sharing of\nvirtual environments (shared vs separated) and the players' dependency on the\nexperience (mutual vs unidirectional). The results showed that variations in\ngame design influenced aspects of the player experience, such as system\nusability, pragmatic UX quality, immersion control, and intrinsic motivation.\nNotably, the player roles and the co-presence in the virtual environment did\nnot simultaneously impact these aspects, suggesting that the degree to which\nplayers depend on each other changes the player experience."}
{"id": "2510.14414", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.14414", "abs": "https://arxiv.org/abs/2510.14414", "authors": ["Baris Baysal", "Omid Arfaie", "Ramazan Unal"], "title": "RoboANKLE: Design, Development, and Functional Evaluation of a Robotic Ankle with a Motorized Compliant Unit", "comment": null, "summary": "This study presents a powered transtibial prosthesis with complete push-off\nassistance, RoboANKLE. The design aims to fulfill specific requirements, such\nas a sufficient range of motion (RoM) while providing the necessary torque for\nachieving natural ankle motion in daily activities. Addressing the challenges\nfaced in designing active transtibial prostheses, such as maintaining energetic\nautonomy and minimizing weight, is vital for the study. With this aim, we try\nto imitate the human ankle by providing extensive push-off assistance to\nachieve a natural-like torque profile. Thus, Energy Store and Extended Release\nmechanism (ESER) is employed with a novel Extra Energy Storage (EES) mechanism.\nKinematic and kinetic analyses are carried out to determine the design\nparameters and assess the design performance. Subsequently, a Computer-Aided\nDesign (CAD) model is built and used in comprehensive dynamic and structural\nanalyses. These analyses are used for the design performance evaluation and\ndetermine the forces and torques applied to the prosthesis, which aids in\noptimizing the design for minimal weight via structural analysis and topology\noptimization. The design of the prototype is then finalized and manufactured\nfor experimental evaluation to validate the design and functionality. The\nprototype is realized with a mass of 1.92 kg and dimensions of 261x107x420 mm.\nThe Functional evaluations of the RoboANKLE revealed that it is capable of\nachieving the natural maximum dorsi-flexion angle with 95% accuracy. Also,\nThanks to the implemented mechanisms, the results show that RoboANKLE can\ngenerate 57% higher than the required torque for natural walking. The result of\nthe power generation capacity of the RoboANKLE is 10% more than the natural\npower during the gait cycle."}
{"id": "2510.14611", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14611", "abs": "https://arxiv.org/abs/2510.14611", "authors": ["Markus Klar", "Sebastian Stein", "Fraser Paterson", "John H. Williamson", "Roderick Murray-Smith"], "title": "An Active Inference Model of Mouse Point-and-Click Behaviour", "comment": "12 pages + Appendix; Accepted to 6th International Workshop on Active\n  Inference (IWAI 2025)", "summary": "We explore the use of Active Inference (AIF) as a computational user model\nfor spatial pointing, a key problem in Human-Computer Interaction (HCI). We\npresent an AIF agent with continuous state, action, and observation spaces,\nperforming one-dimensional mouse pointing and clicking. We use a simple\nunderlying dynamic system to model the mouse cursor dynamics with realistic\nperceptual delay. In contrast to previous optimal feedback control-based\nmodels, the agent's actions are selected by minimizing Expected Free Energy,\nsolely based on preference distributions over percepts, such as observing\nclicking a button correctly. Our results show that the agent creates plausible\npointing movements and clicks when the cursor is over the target, with similar\nend-point variance to human users. In contrast to other models of pointing, we\nincorporate fully probabilistic, predictive delay compensation into the agent.\nThe agent shows distinct behaviour for differing target difficulties without\nthe need to retune system parameters, as done in other approaches. We discuss\nthe simulation results and emphasize the challenges in identifying the correct\nconfiguration of an AIF agent interacting with continuous systems."}
{"id": "2510.14454", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14454", "abs": "https://arxiv.org/abs/2510.14454", "authors": ["Tao Huang", "Huayi Wang", "Junli Ren", "Kangning Yin", "Zirui Wang", "Xiao Chen", "Feiyu Jia", "Wentao Zhang", "Junfeng Long", "Jingbo Wang", "Jiangmiao Pang"], "title": "Towards Adaptable Humanoid Control via Adaptive Motion Tracking", "comment": "9 pages", "summary": "Humanoid robots are envisioned to adapt demonstrated motions to diverse\nreal-world conditions while accurately preserving motion patterns. Existing\nmotion prior approaches enable well adaptability with a few motions but often\nsacrifice imitation accuracy, whereas motion-tracking methods achieve accurate\nimitation yet require many training motions and a test-time target motion to\nadapt. To combine their strengths, we introduce AdaMimic, a novel motion\ntracking algorithm that enables adaptable humanoid control from a single\nreference motion. To reduce data dependence while ensuring adaptability, our\nmethod first creates an augmented dataset by sparsifying the single reference\nmotion into keyframes and applying light editing with minimal physical\nassumptions. A policy is then initialized by tracking these sparse keyframes to\ngenerate dense intermediate motions, and adapters are subsequently trained to\nadjust tracking speed and refine low-level actions based on the adjustment,\nenabling flexible time warping that further improves imitation accuracy and\nadaptability. We validate these significant improvements in our approach in\nboth simulation and the real-world Unitree G1 humanoid robot in multiple tasks\nacross a wide range of adaptation conditions. Videos and code are available at\nhttps://taohuang13.github.io/adamimic.github.io/."}
{"id": "2510.14691", "categories": ["cs.HC", "cs.MM", "cs.SD", "H.5.5; H.5.2; J.5"], "pdf": "https://arxiv.org/pdf/2510.14691", "abs": "https://arxiv.org/abs/2510.14691", "authors": ["Caio Nunes", "Bosco Borges", "Georgia Cruz", "Ticianne Darin"], "title": "If You Hold Me Without Hurting Me: Pathways to Designing Game Audio for Healthy Escapism and Player Well-being", "comment": "5 pages. Presented and discussed at the CHI PLAY 2025 Workshop\n  Exploring Future Directions for Healthy Escapism and Self-Regulation in\n  Games, Pittsburgh, USA, October 13, 2025", "summary": "Escapism in games can support recovery or lead to harmful avoidance.\nSelf-regulation, understood as combining autonomy with positive outcomes, is\nkey to this distinction. We argue that audio, often overlooked, plays a central\nrole in regulation. It can modulate arousal, mark transitions, and provide\nclosure, yet its contribution to well-being remains underexplored. This paper\nidentifies methodological and accessibility gaps that limit recognition of\naudio's potential and outlines ways to address them. We aim to encourage\nresearchers and developers to integrate audio more deliberately into the design\nand study of healthier escapist play."}
{"id": "2510.14467", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14467", "abs": "https://arxiv.org/abs/2510.14467", "authors": ["Shang-Fu Chen", "Co Yong", "Shao-Hua Sun"], "title": "Restoring Noisy Demonstration for Imitation Learning With Diffusion Models", "comment": "Published in IEEE Transactions on Neural Networks and Learning\n  Systems (TNNLS)", "summary": "Imitation learning (IL) aims to learn a policy from expert demonstrations and\nhas been applied to various applications. By learning from the expert policy,\nIL methods do not require environmental interactions or reward signals.\nHowever, most existing imitation learning algorithms assume perfect expert\ndemonstrations, but expert demonstrations often contain imperfections caused by\nerrors from human experts or sensor/control system inaccuracies. To address the\nabove problems, this work proposes a filter-and-restore framework to best\nleverage expert demonstrations with inherent noise. Our proposed method first\nfilters clean samples from the demonstrations and then learns conditional\ndiffusion models to recover the noisy ones. We evaluate our proposed framework\nand existing methods in various domains, including robot arm manipulation,\ndexterous manipulation, and locomotion. The experiment results show that our\nproposed framework consistently outperforms existing methods across all the\ntasks. Ablation studies further validate the effectiveness of each component\nand demonstrate the framework's robustness to different noise types and levels.\nThese results confirm the practical applicability of our framework to noisy\noffline demonstration data."}
{"id": "2510.14911", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.14911", "abs": "https://arxiv.org/abs/2510.14911", "authors": ["Paul D. S. Fink", "Justin R. Brown", "Rachel Coombs", "Emily A. Hamby", "Kyle J. James", "Aisha Harris", "Jacob Bond", "Morgan E. Andrulis", "Nicholas A. Giudice"], "title": "Dude, Where's My (Autonomous) Car? Defining an Accessible Description Logic for Blind and Low Vision Travelers Using Autonomous Vehicles", "comment": "This is the version of the article accepted to Universal Access in\n  the Information Society", "summary": "Purpose: Autonomous vehicles (AVs) are becoming a promising transportation\nsolution for blind and low-vision (BLV) travelers, offering the potential for\ngreater independent mobility. This paper explores the information needs of BLV\nusers across multiple steps of the transportation journey, including finding\nand navigating to, entering, and exiting vehicles independently.\n  Methods: A survey with 202 BLV respondents and interviews with 12 BLV\nindividuals revealed the perspectives of BLV end-users and informed the\nsequencing of natural language information required for successful travel.\nWhereas the survey identified key information needs across the three trip\nsegments, the interviews helped prioritize how that information should be\npresented in a sequence of accessible descriptions to travelers.\n  Results: Taken together, the survey and interviews reveal that BLV users\nprioritize knowing the vehicle's make and model and how to find the correct\nvehicle during the navigation phase. They also emphasize the importance of\nconfirmations about the vehicle's destination and onboard safety features upon\nentering the vehicle. While exiting, BLV users value information about hazards\nand obstacles, as well as knowing which side of the vehicle to exit.\nFurthermore, results highlight that BLV travelers desire using their own\nsmartphone devices when receiving information from AVs and prefer audio-based\ninteraction.\n  Conclusion: The findings from this research contribute a structured framework\nfor delivering trip-related information to BLV users, useful for designers\nincorporating natural language descriptions tailored to each travel segment.\nThis work offers important contributions for sequencing transportation-related\ndescriptions throughout the AV journey, ultimately enhancing the mobility and\nindependence of BLV individuals."}
{"id": "2510.14511", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.14511", "abs": "https://arxiv.org/abs/2510.14511", "authors": ["Mingtian Du", "Suhas Raghavendra Kulkarni", "Simone Kager", "Domenico Campolo"], "title": "Stability Criteria and Motor Performance in Delayed Haptic Dyadic Interactions Mediated by Robots", "comment": null, "summary": "This paper establishes analytical stability criteria for robot-mediated\nhuman-human (dyadic) interaction systems, focusing on haptic communication\nunder network-induced time delays. Through frequency-domain analysis supported\nby numerical simulations, we identify both delay-independent and\ndelay-dependent stability criteria. The delay-independent criterion guarantees\nstability irrespective of the delay, whereas the delay-dependent criterion is\ncharacterised by a maximum tolerable delay before instability occurs. The\ncriteria demonstrate dependence on controller and robot dynamic parameters,\nwhere increasing stiffness reduces the maximum tolerable delay in a non-linear\nmanner, thereby heightening system vulnerability. The proposed criteria can be\ngeneralised to a wide range of robot-mediated interactions and serve as design\nguidelines for stable remote dyadic systems. Experiments with robots performing\nhuman-like movements further illustrate the correlation between stability and\nmotor performance. The findings of this paper suggest the prerequisites for\neffective delay-compensation strategies."}
{"id": "2510.14914", "categories": ["cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14914", "abs": "https://arxiv.org/abs/2510.14914", "authors": ["Ruhan Yang", "Ellen Yi-Luen Do"], "title": "Design of Paper Robot Building Kits", "comment": null, "summary": "Building robots is an engaging activity that provides opportunities for\nhands-on learning. However, traditional robot-building kits are usually costly\nwith limited functionality due to material and technology constraints. To\nimprove the accessibility and flexibility of such kits, we take paper as the\nbuilding material and extensively explore the versatility of paper-based\ninteractions. Based on an analysis of current robot-building kits and\npaper-based interaction research, we propose a design space for devising paper\nrobots. We also analyzed our building kit designs using this design space,\nwhere these kits demonstrate the potential of paper as a cost-effective\nmaterial for robot building. As a starting point, our design space and building\nkit examples provide a guideline that inspires and informs future research and\ndevelopment of novel paper robot-building kits."}
{"id": "2510.14546", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14546", "abs": "https://arxiv.org/abs/2510.14546", "authors": ["Matti Pekkanen", "Francesco Verdoja", "Ville Kyrki"], "title": "QuASH: Using Natural-Language Heuristics to Query Visual-Language Robotic Maps", "comment": "Submitted to ICRA 2026", "summary": "Embeddings from Visual-Language Models are increasingly utilized to represent\nsemantics in robotic maps, offering an open-vocabulary scene understanding that\nsurpasses traditional, limited labels. Embeddings enable on-demand querying by\ncomparing embedded user text prompts to map embeddings via a similarity metric.\nThe key challenge in performing the task indicated in a query is that the robot\nmust determine the parts of the environment relevant to the query.\n  This paper proposes a solution to this challenge. We leverage\nnatural-language synonyms and antonyms associated with the query within the\nembedding space, applying heuristics to estimate the language space relevant to\nthe query, and use that to train a classifier to partition the environment into\nmatches and non-matches. We evaluate our method through extensive experiments,\nquerying both maps and standard image benchmarks. The results demonstrate\nincreased queryability of maps and images. Our querying technique is agnostic\nto the representation and encoder used, and requires limited training."}
{"id": "2510.14584", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14584", "abs": "https://arxiv.org/abs/2510.14584", "authors": ["Benno Wingender", "Nils Dengler", "Rohit Menon", "Sicong Pan", "Maren Bennewitz"], "title": "A Generalized Placeability Metric for Model-Free Unified Pick-and-Place Reasoning", "comment": null, "summary": "To reliably pick and place unknown objects under real-world sensing noise\nremains a challenging task, as existing methods rely on strong object priors\n(e.g., CAD models), or planar-support assumptions, limiting generalization and\nunified reasoning between grasping and placing. In this work, we introduce a\ngeneralized placeability metric that evaluates placement poses directly from\nnoisy point clouds, without any shape priors. The metric jointly scores\nstability, graspability, and clearance. From raw geometry, we extract the\nsupport surfaces of the object to generate diverse candidates for\nmulti-orientation placement and sample contacts that satisfy collision and\nstability constraints. By conditioning grasp scores on each candidate\nplacement, our proposed method enables model-free unified pick-and-place\nreasoning and selects grasp-place pairs that lead to stable, collision-free\nplacements. On unseen real objects and non-planar object supports, our metric\ndelivers CAD-comparable accuracy in predicting stability loss and generally\nproduces more physically plausible placements than learning-based predictors."}
{"id": "2510.14612", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14612", "abs": "https://arxiv.org/abs/2510.14612", "authors": ["Gabriel Fischer Abati", "João Carlos Virgolino Soares", "Giulio Turrisi", "Victor Barasuol", "Claudio Semini"], "title": "Proprioceptive Image: An Image Representation of Proprioceptive Data from Quadruped Robots for Contact Estimation Learning", "comment": null, "summary": "This paper presents a novel approach for representing proprioceptive\ntime-series data from quadruped robots as structured two-dimensional images,\nenabling the use of convolutional neural networks for learning\nlocomotion-related tasks. The proposed method encodes temporal dynamics from\nmultiple proprioceptive signals, such as joint positions, IMU readings, and\nfoot velocities, while preserving the robot's morphological structure in the\nspatial arrangement of the image. This transformation captures inter-signal\ncorrelations and gait-dependent patterns, providing a richer feature space than\ndirect time-series processing. We apply this concept in the problem of contact\nestimation, a key capability for stable and adaptive locomotion on diverse\nterrains. Experimental evaluations on both real-world datasets and simulated\nenvironments show that our image-based representation consistently enhances\nprediction accuracy and generalization over conventional sequence-based models,\nunderscoring the potential of cross-modal encoding strategies for robotic state\nlearning. Our method achieves superior performance on the contact dataset,\nimproving contact state accuracy from 87.7% to 94.5% over the recently proposed\nMI-HGNN method, using a 15 times shorter window size."}
{"id": "2510.14615", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14615", "abs": "https://arxiv.org/abs/2510.14615", "authors": ["Edward Sandra", "Lander Vanroye", "Dries Dirckx", "Ruben Cartuyvels", "Jan Swevers", "Wilm Decré"], "title": "Accelerated Multi-Modal Motion Planning Using Context-Conditioned Diffusion Models", "comment": "This paper has been submitted and has not yet been peer reviewed or\n  accepted for publication", "summary": "Classical methods in robot motion planning, such as sampling-based and\noptimization-based methods, often struggle with scalability towards\nhigher-dimensional state spaces and complex environments. Diffusion models,\nknown for their capability to learn complex, high-dimensional and multi-modal\ndata distributions, provide a promising alternative when applied to motion\nplanning problems and have already shown interesting results. However, most of\nthe current approaches train their model for a single environment, limiting\ntheir generalization to environments not seen during training. The techniques\nthat do train a model for multiple environments rely on a specific camera to\nprovide the model with the necessary environmental information and therefore\nalways require that sensor. To effectively adapt to diverse scenarios without\nthe need for retraining, this research proposes Context-Aware Motion Planning\nDiffusion (CAMPD). CAMPD leverages a classifier-free denoising probabilistic\ndiffusion model, conditioned on sensor-agnostic contextual information. An\nattention mechanism, integrated in the well-known U-Net architecture,\nconditions the model on an arbitrary number of contextual parameters. CAMPD is\nevaluated on a 7-DoF robot manipulator and benchmarked against state-of-the-art\napproaches on real-world tasks, showing its ability to generalize to unseen\nenvironments and generate high-quality, multi-modal trajectories, at a fraction\nof the time required by existing methods."}
{"id": "2510.14627", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14627", "abs": "https://arxiv.org/abs/2510.14627", "authors": ["Yao Zhong", "Hanzhi Chen", "Simon Schaefer", "Anran Zhang", "Stefan Leutenegger"], "title": "GOPLA: Generalizable Object Placement Learning via Synthetic Augmentation of Human Arrangement", "comment": null, "summary": "Robots are expected to serve as intelligent assistants, helping humans with\neveryday household organization. A central challenge in this setting is the\ntask of object placement, which requires reasoning about both semantic\npreferences (e.g., common-sense object relations) and geometric feasibility\n(e.g., collision avoidance). We present GOPLA, a hierarchical framework that\nlearns generalizable object placement from augmented human demonstrations. A\nmulti-modal large language model translates human instructions and visual\ninputs into structured plans that specify pairwise object relationships. These\nplans are then converted into 3D affordance maps with geometric common sense by\na spatial mapper, while a diffusion-based planner generates placement poses\nguided by test-time costs, considering multi-plan distributions and collision\navoidance. To overcome data scarcity, we introduce a scalable pipeline that\nexpands human placement demonstrations into diverse synthetic training data.\nExtensive experiments show that our approach improves placement success rates\nby 30.04 percentage points over the runner-up, evaluated on positioning\naccuracy and physical plausibility, demonstrating strong generalization across\na wide range of real-world robotic placement scenarios."}
{"id": "2510.14643", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14643", "abs": "https://arxiv.org/abs/2510.14643", "authors": ["Lara Brudermüller", "Brandon Hung", "Xinghao Zhu", "Jiuguang Wang", "Nick Hawes", "Preston Culbertson", "Simon Le Cleac'h"], "title": "Generative Models From and For Sampling-Based MPC: A Bootstrapped Approach For Adaptive Contact-Rich Manipulation", "comment": "9 pages, 5 figures", "summary": "We present a generative predictive control (GPC) framework that amortizes\nsampling-based Model Predictive Control (SPC) by bootstrapping it with\nconditional flow-matching models trained on SPC control sequences collected in\nsimulation. Unlike prior work relying on iterative refinement or gradient-based\nsolvers, we show that meaningful proposal distributions can be learned directly\nfrom noisy SPC data, enabling more efficient and informed sampling during\nonline planning. We further demonstrate, for the first time, the application of\nthis approach to real-world contact-rich loco-manipulation with a quadruped\nrobot. Extensive experiments in simulation and on hardware show that our method\nimproves sample efficiency, reduces planning horizon requirements, and\ngeneralizes robustly across task variations."}
{"id": "2510.14647", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14647", "abs": "https://arxiv.org/abs/2510.14647", "authors": ["Jialei Huang", "Yang Ye", "Yuanqing Gong", "Xuezhou Zhu", "Yang Gao", "Kaifeng Zhang"], "title": "Spatially anchored Tactile Awareness for Robust Dexterous Manipulation", "comment": "8 pages", "summary": "Dexterous manipulation requires precise geometric reasoning, yet existing\nvisuo-tactile learning methods struggle with sub-millimeter precision tasks\nthat are routine for traditional model-based approaches. We identify a key\nlimitation: while tactile sensors provide rich contact information, current\nlearning frameworks fail to effectively leverage both the perceptual richness\nof tactile signals and their spatial relationship with hand kinematics. We\nbelieve an ideal tactile representation should explicitly ground contact\nmeasurements in a stable reference frame while preserving detailed sensory\ninformation, enabling policies to not only detect contact occurrence but also\nprecisely infer object geometry in the hand's coordinate system. We introduce\nSaTA (Spatially-anchored Tactile Awareness for dexterous manipulation), an\nend-to-end policy framework that explicitly anchors tactile features to the\nhand's kinematic frame through forward kinematics, enabling accurate geometric\nreasoning without requiring object models or explicit pose estimation. Our key\ninsight is that spatially grounded tactile representations allow policies to\nnot only detect contact occurrence but also precisely infer object geometry in\nthe hand's coordinate system. We validate SaTA on challenging dexterous\nmanipulation tasks, including bimanual USB-C mating in free space, a task\ndemanding sub-millimeter alignment precision, as well as light bulb\ninstallation requiring precise thread engagement and rotational control, and\ncard sliding that demands delicate force modulation and angular precision.\nThese tasks represent significant challenges for learning-based methods due to\ntheir stringent precision requirements. Across multiple benchmarks, SaTA\nsignificantly outperforms strong visuo-tactile baselines, improving success\nrates by up to 30 percentage while reducing task completion times by 27\npercentage."}
{"id": "2510.14677", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.14677", "abs": "https://arxiv.org/abs/2510.14677", "authors": ["Steffen Hagedorn", "Luka Donkov", "Aron Distelzweig", "Alexandru P. Condurache"], "title": "When Planners Meet Reality: How Learned, Reactive Traffic Agents Shift nuPlan Benchmarks", "comment": null, "summary": "Planner evaluation in closed-loop simulation often uses rule-based traffic\nagents, whose simplistic and passive behavior can hide planner deficiencies and\nbias rankings. Widely used IDM agents simply follow a lead vehicle and cannot\nreact to vehicles in adjacent lanes, hindering tests of complex interaction\ncapabilities. We address this issue by integrating the state-of-the-art learned\ntraffic agent model SMART into nuPlan. Thus, we are the first to evaluate\nplanners under more realistic conditions and quantify how conclusions shift\nwhen narrowing the sim-to-real gap. Our analysis covers 14 recent planners and\nestablished baselines and shows that IDM-based simulation overestimates\nplanning performance: nearly all scores deteriorate. In contrast, many planners\ninteract better than previously assumed and even improve in multi-lane,\ninteraction-heavy scenarios like lane changes or turns. Methods trained in\nclosed-loop demonstrate the best and most stable driving performance. However,\nwhen reaching their limits in augmented edge-case scenarios, all learned\nplanners degrade abruptly, whereas rule-based planners maintain reasonable\nbasic behavior. Based on our results, we suggest SMART-reactive simulation as a\nnew standard closed-loop benchmark in nuPlan and release the SMART agents as a\ndrop-in alternative to IDM at https://github.com/shgd95/InteractiveClosedLoop."}
{"id": "2510.14768", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14768", "abs": "https://arxiv.org/abs/2510.14768", "authors": ["Fan Yang", "Zixuan Huang", "Abhinav Kumar", "Sergio Aguilera Marinovic", "Soshi Iba", "Rana Soltani Zarrin", "Dmitry Berenson"], "title": "Leveraging Neural Descriptor Fields for Learning Contact-Aware Dynamic Recovery", "comment": null, "summary": "Real-world dexterous manipulation often encounters unexpected errors and\ndisturbances, which can lead to catastrophic failures, such as dropping the\nmanipulated object. To address this challenge, we focus on the problem of\ncatching a falling object while it remains within grasping range and,\nimportantly, resetting the system to a configuration favorable for resuming the\nprimary manipulation task. We propose Contact-Aware Dynamic Recovery (CADRE), a\nreinforcement learning framework that incorporates a Neural Descriptor Field\n(NDF)-inspired module to extract implicit contact features. Compared to methods\nthat rely solely on object pose or point cloud input, NDFs can directly reason\nabout finger-object correspondence and adapt to different object geometries.\nOur experiments show that incorporating contact features improves training\nefficiency, enhances convergence performance for RL training, and ultimately\nleads to more successful recoveries. Additionally, we demonstrate that CADRE\ncan generalize zero-shot to unseen objects with different geometries."}
{"id": "2510.14771", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14771", "abs": "https://arxiv.org/abs/2510.14771", "authors": ["Xu Chi", "Chao Zhang", "Yang Su", "Lingfeng Dou", "Fujia Yang", "Jiakuo Zhao", "Haoyu Zhou", "Xiaoyou Jia", "Yong Zhou", "Shan An"], "title": "Open TeleDex: A Hardware-Agnostic Teleoperation System for Imitation Learning based Dexterous Manipulation", "comment": "17 pages", "summary": "Accurate and high-fidelity demonstration data acquisition is a critical\nbottleneck for deploying robot Imitation Learning (IL) systems, particularly\nwhen dealing with heterogeneous robotic platforms. Existing teleoperation\nsystems often fail to guarantee high-precision data collection across diverse\ntypes of teleoperation devices. To address this, we developed Open TeleDex, a\nunified teleoperation framework engineered for demonstration data collection.\nOpen TeleDex specifically tackles the TripleAny challenge, seamlessly\nsupporting any robotic arm, any dexterous hand, and any external input device.\nFurthermore, we propose a novel hand pose retargeting algorithm that\nsignificantly boosts the interoperability of Open TeleDex, enabling robust and\naccurate compatibility with an even wider spectrum of heterogeneous master and\nslave equipment. Open TeleDex establishes a foundational, high-quality, and\npublicly available platform for accelerating both academic research and\nindustry development in complex robotic manipulation and IL."}
{"id": "2510.14783", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14783", "abs": "https://arxiv.org/abs/2510.14783", "authors": ["Aderik Verraest", "Stavrow Bahnam", "Robin Ferede", "Guido de Croon", "Christophe De Wagter"], "title": "SkyDreamer: Interpretable End-to-End Vision-Based Drone Racing with Model-Based Reinforcement Learning", "comment": null, "summary": "Autonomous drone racing (ADR) systems have recently achieved champion-level\nperformance, yet remain highly specific to drone racing. While end-to-end\nvision-based methods promise broader applicability, no system to date\nsimultaneously achieves full sim-to-real transfer, onboard execution, and\nchampion-level performance. In this work, we present SkyDreamer, to the best of\nour knowledge, the first end-to-end vision-based ADR policy that maps directly\nfrom pixel-level representations to motor commands. SkyDreamer builds on\ninformed Dreamer, a model-based reinforcement learning approach where the world\nmodel decodes to privileged information only available during training. By\nextending this concept to end-to-end vision-based ADR, the world model\neffectively functions as an implicit state and parameter estimator, greatly\nimproving interpretability. SkyDreamer runs fully onboard without external aid,\nresolves visual ambiguities by tracking progress using the state decoded from\nthe world model's hidden state, and requires no extrinsic camera calibration,\nenabling rapid deployment across different drones without retraining.\nReal-world experiments show that SkyDreamer achieves robust, high-speed flight,\nexecuting tight maneuvers such as an inverted loop, a split-S and a ladder,\nreaching speeds of up to 21 m/s and accelerations of up to 6 g. It further\ndemonstrates a non-trivial visual sim-to-real transfer by operating on\npoor-quality segmentation masks, and exhibits robustness to battery depletion\nby accurately estimating the maximum attainable motor RPM and adjusting its\nflight path in real-time. These results highlight SkyDreamer's adaptability to\nimportant aspects of the reality gap, bringing robustness while still achieving\nextremely high-speed, agile flight."}
{"id": "2510.14827", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14827", "abs": "https://arxiv.org/abs/2510.14827", "authors": ["Yufei Zhu", "Shih-Min Yang", "Andrey Rudenko", "Tomasz P. Kucner", "Achim J. Lilienthal", "Martin Magnusson"], "title": "Neural Implicit Flow Fields for Spatio-Temporal Motion Mapping", "comment": null, "summary": "Safe and efficient robot operation in complex human environments can benefit\nfrom good models of site-specific motion patterns. Maps of Dynamics (MoDs)\nprovide such models by encoding statistical motion patterns in a map, but\nexisting representations use discrete spatial sampling and typically require\ncostly offline construction. We propose a continuous spatio-temporal MoD\nrepresentation based on implicit neural functions that directly map coordinates\nto the parameters of a Semi-Wrapped Gaussian Mixture Model. This removes the\nneed for discretization and imputation for unevenly sampled regions, enabling\nsmooth generalization across both space and time. Evaluated on a large public\ndataset with long-term real-world people tracking data, our method achieves\nbetter accuracy of motion representation and smoother velocity distributions in\nsparse regions while still being computationally efficient, compared to\navailable baselines. The proposed approach demonstrates a powerful and\nefficient way of modeling complex human motion patterns."}
{"id": "2510.14830", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14830", "abs": "https://arxiv.org/abs/2510.14830", "authors": ["Kun Lei", "Huanyu Li", "Dongjie Yu", "Zhenyu Wei", "Lingxiao Guo", "Zhennan Jiang", "Ziyu Wang", "Shiyu Liang", "Huazhe Xu"], "title": "RL-100: Performant Robotic Manipulation with Real-World Reinforcement Learning", "comment": "https://lei-kun.github.io/RL-100/", "summary": "Real-world robotic manipulation in homes and factories demands reliability,\nefficiency, and robustness that approach or surpass skilled human operators. We\npresent RL-100, a real-world reinforcement learning training framework built on\ndiffusion visuomotor policies trained bu supervised learning. RL-100 introduces\na three-stage pipeline. First, imitation learning leverages human priors.\nSecond, iterative offline reinforcement learning uses an Offline Policy\nEvaluation procedure, abbreviated OPE, to gate PPO-style updates that are\napplied in the denoising process for conservative and reliable improvement.\nThird, online reinforcement learning eliminates residual failure modes. An\nadditional lightweight consistency distillation head compresses the multi-step\nsampling process in diffusion into a single-step policy, enabling\nhigh-frequency control with an order-of-magnitude reduction in latency while\npreserving task performance. The framework is task-, embodiment-, and\nrepresentation-agnostic and supports both 3D point clouds and 2D RGB inputs, a\nvariety of robot platforms, and both single-step and action-chunk policies. We\nevaluate RL-100 on seven real-robot tasks spanning dynamic rigid-body control,\nsuch as Push-T and Agile Bowling, fluids and granular pouring, deformable cloth\nfolding, precise dexterous unscrewing, and multi-stage orange juicing. RL-100\nattains 100\\% success across evaluated trials for a total of 900 out of 900\nepisodes, including up to 250 out of 250 consecutive trials on one task. The\nmethod achieves near-human teleoperation or better time efficiency and\ndemonstrates multi-hour robustness with uninterrupted operation lasting up to\ntwo hours."}
{"id": "2510.14849", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.14849", "abs": "https://arxiv.org/abs/2510.14849", "authors": ["Marcello Sorge", "Nicola Cigarini", "Riccardo Lorigiola", "Giulia Michieletto", "Andrea Masiero", "Angelo Cenedese", "Alberto Guarnieri"], "title": "Multi Agent Switching Mode Controller for Sound Source localization", "comment": null, "summary": "Source seeking is an important topic in robotic research, especially\nconsidering sound-based sensors since they allow the agents to locate a target\neven in critical conditions where it is not possible to establish a direct line\nof sight. In this work, we design a multi- agent switching mode control\nstrategy for acoustic-based target localization. Two scenarios are considered:\nsingle source localization, in which the agents are driven maintaining a rigid\nformation towards the target, and multi-source scenario, in which each agent\nsearches for the targets independently from the others."}
{"id": "2510.14851", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.14851", "abs": "https://arxiv.org/abs/2510.14851", "authors": ["Jakob Bichler", "Andreu Matoses Gimenez", "Javier Alonso-Mora"], "title": "SADCHER: Scheduling using Attention-based Dynamic Coalitions of Heterogeneous Robots in Real-Time", "comment": "7 pages, 5 figures. 2025 IEEE Int. Symposium on Multi-Robot and\n  Multi-Agent Systems (MRS 2025). Website and Code:\n  https://autonomousrobots.nl/paper_websites/sadcher_MRTA/", "summary": "We present Sadcher, a real-time task assignment framework for heterogeneous\nmulti-robot teams that incorporates dynamic coalition formation and task\nprecedence constraints. Sadcher is trained through Imitation Learning and\ncombines graph attention and transformers to predict assignment rewards between\nrobots and tasks. Based on the predicted rewards, a relaxed bipartite matching\nstep generates high-quality schedules with feasibility guarantees. We\nexplicitly model robot and task positions, task durations, and robots'\nremaining processing times, enabling advanced temporal and spatial reasoning\nand generalization to environments with different spatiotemporal distributions\ncompared to training. Trained on optimally solved small-scale instances, our\nmethod can scale to larger task sets and team sizes. Sadcher outperforms other\nlearning-based and heuristic baselines on randomized, unseen problems for small\nand medium-sized teams with computation times suitable for real-time operation.\nWe also explore sampling-based variants and evaluate scalability across robot\nand task counts. In addition, we release our dataset of 250,000 optimal\nschedules: https://autonomousrobots.nl/paper_websites/sadcher_MRTA/"}
{"id": "2510.14893", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14893", "abs": "https://arxiv.org/abs/2510.14893", "authors": ["Helene J. Levy", "Brett T. Lopez"], "title": "STITCHER: Constrained Trajectory Planning in Known Environments with Real-Time Motion Primitive Search", "comment": null, "summary": "Autonomous high-speed navigation through large, complex environments requires\nreal-time generation of agile trajectories that are dynamically feasible,\ncollision-free, and satisfy state or actuator constraints. Modern trajectory\nplanning techniques primarily use numerical optimization, as they enable the\nsystematic computation of high-quality, expressive trajectories that satisfy\nvarious constraints. However, stringent requirements on computation time and\nthe risk of numerical instability can limit the use of optimization-based\nplanners in safety-critical scenarios. This work presents an optimization-free\nplanning framework called STITCHER that stitches short trajectory segments\ntogether with graph search to compute long-range, expressive, and near-optimal\ntrajectories in real-time. STITCHER outperforms modern optimization-based\nplanners through our innovative planning architecture and several algorithmic\ndevelopments that make real-time planning possible. Extensive simulation\ntesting is performed to analyze the algorithmic components that make up\nSTITCHER, along with a thorough comparison with two state-of-the-art\noptimization planners. Simulation tests show that safe trajectories can be\ncreated within a few milliseconds for paths that span the entirety of two 50 m\nx 50 m environments. Hardware tests with a custom quadrotor verify that\nSTITCHER can produce trackable paths in real-time while respecting nonconvex\nconstraints, such as limits on tilt angle and motor forces, which are otherwise\nhard to include in optimization-based planners."}
{"id": "2510.14902", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14902", "abs": "https://arxiv.org/abs/2510.14902", "authors": ["Han Zhao", "Jiaxuan Zhang", "Wenxuan Song", "Pengxiang Ding", "Donglin Wang"], "title": "VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation", "comment": null, "summary": "Current vision-language-action (VLA) models, pre-trained on large-scale\nrobotic data, exhibit strong multi-task capabilities and generalize well to\nvariations in visual and language instructions for manipulation. However, their\nsuccess rate drops significantly when faced with object concepts outside the\ntraining data, such as unseen object descriptions and textures in the dataset.\nTo address this, we propose a novel agentic framework, VLA^2, which leverages\nOpenVLA as the execution backbone and effectively leverages external modules\nsuch as web retrieval and object detection to provide visual and textual\nknowledge about target objects to the VLA. This approach mitigates\ngeneralization failure when handling out-of-distribution objects. Based on the\nLIBERO simulation environment, we introduced novel objects and object\ndescriptions to construct a new evaluation benchmark with three difficulty\nlevels to test the effectiveness of our method. Our framework successfully\noutperformed the current state-of-the-art models on our designed hard-level\ngeneralization benchmark. Compared to the standalone OpenVLA baseline, VLA^2\nachieves a 44.2% improvement in the success rate in the hard-level benchmark\nand an average improvement of 20.2% in all customized environments without any\nperformance degradation on in-domain tasks. Project website:\nhttps://vla-2.github.io."}
{"id": "2510.14930", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14930", "abs": "https://arxiv.org/abs/2510.14930", "authors": ["Binghao Huang", "Jie Xu", "Iretiayo Akinola", "Wei Yang", "Balakumar Sundaralingam", "Rowland O'Flaherty", "Dieter Fox", "Xiaolong Wang", "Arsalan Mousavian", "Yu-Wei Chao", "Yunzhu Li"], "title": "VT-Refine: Learning Bimanual Assembly with Visuo-Tactile Feedback via Simulation Fine-Tunin", "comment": "Accepted by 9th Conference on Robot Learning (CoRL 2025); Website:\n  https://binghao-huang.github.io/vt_refine/", "summary": "Humans excel at bimanual assembly tasks by adapting to rich tactile feedback\n-- a capability that remains difficult to replicate in robots through\nbehavioral cloning alone, due to the suboptimality and limited diversity of\nhuman demonstrations. In this work, we present VT-Refine, a visuo-tactile\npolicy learning framework that combines real-world demonstrations,\nhigh-fidelity tactile simulation, and reinforcement learning to tackle precise,\ncontact-rich bimanual assembly. We begin by training a diffusion policy on a\nsmall set of demonstrations using synchronized visual and tactile inputs. This\npolicy is then transferred to a simulated digital twin equipped with simulated\ntactile sensors and further refined via large-scale reinforcement learning to\nenhance robustness and generalization. To enable accurate sim-to-real transfer,\nwe leverage high-resolution piezoresistive tactile sensors that provide normal\nforce signals and can be realistically modeled in parallel using\nGPU-accelerated simulation. Experimental results show that VT-Refine improves\nassembly performance in both simulation and the real world by increasing data\ndiversity and enabling more effective policy fine-tuning. Our project page is\navailable at https://binghao-huang.github.io/vt_refine/."}
{"id": "2510.14947", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.14947", "abs": "https://arxiv.org/abs/2510.14947", "authors": ["Blake Werner", "Lizhi Yang", "Aaron D. Ames"], "title": "Architecture Is All You Need: Diversity-Enabled Sweet Spots for Robust Humanoid Locomotion", "comment": "8 pages", "summary": "Robust humanoid locomotion in unstructured environments requires\narchitectures that balance fast low-level stabilization with slower perceptual\ndecision-making. We show that a simple layered control architecture (LCA), a\nproprioceptive stabilizer running at high rate, coupled with a compact low-rate\nperceptual policy, enables substantially more robust performance than\nmonolithic end-to-end designs, even when using minimal perception encoders.\nThrough a two-stage training curriculum (blind stabilizer pretraining followed\nby perceptual fine-tuning), we demonstrate that layered policies consistently\noutperform one-stage alternatives in both simulation and hardware. On a Unitree\nG1 humanoid, our approach succeeds across stair and ledge tasks where one-stage\nperceptual policies fail. These results highlight that architectural separation\nof timescales, rather than network scale or complexity, is the key enabler for\nrobust perception-conditioned locomotion."}
{"id": "2510.14952", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14952", "abs": "https://arxiv.org/abs/2510.14952", "authors": ["Zhe Li", "Cheng Chi", "Yangyang Wei", "Boan Zhu", "Yibo Peng", "Tao Huang", "Pengwei Wang", "Zhongyuan Wang", "Shanghang Zhang", "Chang Xu"], "title": "From Language to Locomotion: Retargeting-free Humanoid Control via Motion Latent Guidance", "comment": null, "summary": "Natural language offers a natural interface for humanoid robots, but existing\nlanguage-guided humanoid locomotion pipelines remain cumbersome and unreliable.\nThey typically decode human motion, retarget it to robot morphology, and then\ntrack it with a physics-based controller. However, this multi-stage process is\nprone to cumulative errors, introduces high latency, and yields weak coupling\nbetween semantics and control. These limitations call for a more direct pathway\nfrom language to action, one that eliminates fragile intermediate stages.\nTherefore, we present RoboGhost, a retargeting-free framework that directly\nconditions humanoid policies on language-grounded motion latents. By bypassing\nexplicit motion decoding and retargeting, RoboGhost enables a diffusion-based\npolicy to denoise executable actions directly from noise, preserving semantic\nintent and supporting fast, reactive control. A hybrid causal\ntransformer-diffusion motion generator further ensures long-horizon consistency\nwhile maintaining stability and diversity, yielding rich latent representations\nfor precise humanoid behavior. Extensive experiments demonstrate that RoboGhost\nsubstantially reduces deployment latency, improves success rates and tracking\naccuracy, and produces smooth, semantically aligned locomotion on real\nhumanoids. Beyond text, the framework naturally extends to other modalities\nsuch as images, audio, and music, providing a general foundation for\nvision-language-action humanoid systems."}
{"id": "2510.14959", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.14959", "abs": "https://arxiv.org/abs/2510.14959", "authors": ["Lizhi Yang", "Blake Werner", "Massimiliano de Sa Aaron D. Ames"], "title": "CBF-RL: Safety Filtering Reinforcement Learning in Training with Control Barrier Functions", "comment": "8 pages", "summary": "Reinforcement learning (RL), while powerful and expressive, can often\nprioritize performance at the expense of safety. Yet safety violations can lead\nto catastrophic outcomes in real-world deployments. Control Barrier Functions\n(CBFs) offer a principled method to enforce dynamic safety -- traditionally\ndeployed \\emph{online} via safety filters. While the result is safe behavior,\nthe fact that the RL policy does not have knowledge of the CBF can lead to\nconservative behaviors. This paper proposes CBF-RL, a framework for generating\nsafe behaviors with RL by enforcing CBFs \\emph{in training}. CBF-RL has two key\nattributes: (1) minimally modifying a nominal RL policy to encode safety\nconstraints via a CBF term, (2) and safety filtering of the policy rollouts in\ntraining. Theoretically, we prove that continuous-time safety filters can be\ndeployed via closed-form expressions on discrete-time roll-outs. Practically,\nwe demonstrate that CBF-RL internalizes the safety constraints in the learned\npolicy -- both enforcing safer actions and biasing towards safer rewards --\nenabling safe deployment without the need for an online safety filter. We\nvalidate our framework through ablation studies on navigation tasks and on the\nUnitree G1 humanoid robot, where CBF-RL enables safer exploration, faster\nconvergence, and robust performance under uncertainty, enabling the humanoid\nrobot to avoid obstacles and climb stairs safely in real-world settings without\na runtime safety filter."}
{"id": "2510.14968", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.14968", "abs": "https://arxiv.org/abs/2510.14968", "authors": ["Mingxuan Yan", "Yuping Wang", "Zechun Liu", "Jiachen Li"], "title": "RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in Long-Horizon Tasks", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025); Project Website: rdd-neurips.github.io", "summary": "To tackle long-horizon tasks, recent hierarchical vision-language-action\n(VLAs) frameworks employ vision-language model (VLM)-based planners to\ndecompose complex manipulation tasks into simpler sub-tasks that low-level\nvisuomotor policies can easily handle. Typically, the VLM planner is finetuned\nto learn to decompose a target task. This finetuning requires target task\ndemonstrations segmented into sub-tasks by either human annotation or heuristic\nrules. However, the heuristic subtasks can deviate significantly from the\ntraining data of the visuomotor policy, which degrades task performance. To\naddress these issues, we propose a Retrieval-based Demonstration Decomposer\n(RDD) that automatically decomposes demonstrations into sub-tasks by aligning\nthe visual features of the decomposed sub-task intervals with those from the\ntraining data of the low-level visuomotor policies. Our method outperforms the\nstate-of-the-art sub-task decomposer on both simulation and real-world tasks,\ndemonstrating robustness across diverse settings. Code and more results are\navailable at rdd-neurips.github.io."}
{"id": "2510.13810", "categories": ["cs.HC", "cs.CY", "cs.RO", "I.2.9; J.5; K.4.2"], "pdf": "https://arxiv.org/pdf/2510.13810", "abs": "https://arxiv.org/abs/2510.13810", "authors": ["Minja Axelsson", "Lea Luka Sikau"], "title": "Choreographing Trash Cans: On Speculative Futures of Weak Robots in Public Spaces", "comment": "Accepted at the British Computer Society's Special Interest Group in\n  Human Computer Interaction Conference (BCS HCI 2025), Futures track. 5 pages,\n  no figures", "summary": "Delivering groceries or cleaning airports, mobile robots exist in public\nspaces. While these examples showcase robots that execute tasks, this paper\nexplores mobile robots that encourage posthuman collaboration rather than\nmanaging environments independently. With feigned fragility, cuteness and\nincomplete functionalities, the so-called \"weak robots\" invite passersby to\nengage not only on a utilitarian level, but also through imaginative and\nemotional responses. After examining the workings of \"weak robots\" by queering\nnotions of function and ability, we introduce two speculative design fiction\nvignettes that describe choreographies of such robots in future urban spaces --\none exploring a utopian weak robot and the other a dystopian weak robot. We\nintroduce these speculations in order to discuss how different values may drive\ndesign decisions, and how such decisions may shape and drive different\nsocio-technical futures in which robots and humans share public spaces that\nincentivise collaboration."}
{"id": "2510.14914", "categories": ["cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14914", "abs": "https://arxiv.org/abs/2510.14914", "authors": ["Ruhan Yang", "Ellen Yi-Luen Do"], "title": "Design of Paper Robot Building Kits", "comment": null, "summary": "Building robots is an engaging activity that provides opportunities for\nhands-on learning. However, traditional robot-building kits are usually costly\nwith limited functionality due to material and technology constraints. To\nimprove the accessibility and flexibility of such kits, we take paper as the\nbuilding material and extensively explore the versatility of paper-based\ninteractions. Based on an analysis of current robot-building kits and\npaper-based interaction research, we propose a design space for devising paper\nrobots. We also analyzed our building kit designs using this design space,\nwhere these kits demonstrate the potential of paper as a cost-effective\nmaterial for robot building. As a starting point, our design space and building\nkit examples provide a guideline that inspires and informs future research and\ndevelopment of novel paper robot-building kits."}
