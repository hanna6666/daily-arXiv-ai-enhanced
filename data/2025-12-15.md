<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 20]
- [cs.HC](#cs.HC) [Total: 13]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [WholeBodyVLA: Towards Unified Latent VLA for Whole-Body Loco-Manipulation Control](https://arxiv.org/abs/2512.11047)
*Haoran Jiang,Jin Chen,Qingwen Bu,Li Chen,Modi Shi,Yanjie Zhang,Delong Li,Chuanzhe Suo,Chuang Wang,Zhihui Peng,Hongyang Li*

Main category: cs.RO

TL;DR: 本文提出WholeBodyVLA框架，通过低成本视频学习运动-操作能力，并优化运动指令执行，显著提升类人机器人在大空间中的运动-操作性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在运动-操作能力上有所不足，限制了机器人在大空间执行复杂任务的能力，因此需要新的方法来获取运动-操作知识和增强动作执行的精确性。

Method: 提出了一个统一的潜在学习框架，结合视觉-语言-动作系统和专为核心运动设计的RL策略，提升了类人机器人的运动-操作能力。

Result: 本文提出了WholeBodyVLA框架，通过视觉、语言和动作系统从低成本的无动作自我中心视频中学习丰富的运动-操作知识，进而实现更加准确和稳定的核心运动操控。

Conclusion: WholeBodyVLA成功地实现了大空间的类人机器人运动-操作任务，相比于传统基线性能提升了21.3%。

Abstract: Humanoid robots require precise locomotion and dexterous manipulation to perform challenging loco-manipulation tasks. Yet existing approaches, modular or end-to-end, are deficient in manipulation-aware locomotion. This confines the robot to a limited workspace, preventing it from performing large-space loco-manipulation. We attribute this to: (1) the challenge of acquiring loco-manipulation knowledge due to the scarcity of humanoid teleoperation data, and (2) the difficulty of faithfully and reliably executing locomotion commands, stemming from the limited precision and stability of existing RL controllers. To acquire richer loco-manipulation knowledge, we propose a unified latent learning framework that enables Vision-Language-Action (VLA) system to learn from low-cost action-free egocentric videos. Moreover, an efficient human data collection pipeline is devised to augment the dataset and scale the benefits. To more precisely execute the desired locomotion commands, we present a loco-manipulation-oriented (LMO) RL policy specifically tailored for accurate and stable core loco-manipulation movements, such as advancing, turning, and squatting. Building on these components, we introduce WholeBodyVLA, a unified framework for humanoid loco-manipulation. To the best of our knowledge, WholeBodyVLA is one of its kind enabling large-space humanoid loco-manipulation. It is verified via comprehensive experiments on the AgiBot X2 humanoid, outperforming prior baseline by 21.3%. It also demonstrates strong generalization and high extensibility across a broad range of tasks.

</details>


### [2] [Taxonomy and Modular Tool System for Versatile and Effective Non-Prehensile Manipulations](https://arxiv.org/abs/2512.11080)
*Cedric-Pascal Sommer,Robert J. Wood,Justin Werfel*

Main category: cs.RO

TL;DR: 本文探讨了非抓握动作的机器人末端执行器的关键属性，提出了一种模块化工具系统，以增强标准两指夹持器在各种操作中的功能性和有效性。


<details>
  <summary>Details</summary>
Motivation: 为了提高机器人在非抓握动作（如按压、摩擦和刮擦）中的效率，提出了一种新的分类方法，系统地理解这些操作的有效性。

Method: 通过建立一个基于非驱动末端执行器关键属性的分类法，提出了一种模块化工具系统，以支持标准两指夹持器执行多种操作。

Result: 展示了该工具系统在航空航天和家居场景中应用的有效性，能够处理多种非抓握和抓握操作。

Conclusion: 该研究为增强机器人在各种操作中的能力提供了基础，同时强调了对不同工具特性的理解的重要性。

Abstract: General-purpose robotic end-effectors of limited complexity, like the parallel-jaw gripper, are appealing for their balance of simplicity and effectiveness in a wide range of manipulation tasks. However, while many such manipulators offer versatility in grasp-like interactions, they are not optimized for non-prehensile actions like pressing, rubbing, or scraping -- manipulations needed for many common tasks. To perform such tasks, humans use a range of different body parts or tools with different rigidity, friction, etc., according to the properties most effective for a given task. Here, we discuss a taxonomy for the key properties of a non-actuated end-effector, laying the groundwork for a systematic understanding of the affordances of non-prehensile manipulators. We then present a modular tool system, based on the taxonomy, that can be used by a standard two-fingered gripper to extend its versatility and effectiveness in performing such actions. We demonstrate the application of the tool system in aerospace and household scenarios that require a range of non-prehensile and prehensile manipulations.

</details>


### [3] [Design and Experimental Validation of Closed-Form CBF-Based Safe Control for Stewart Platform Under Multiple Constraints](https://arxiv.org/abs/2512.11125)
*Benedictus C. G. Cinun,Tua A. Tamba,Immanuel R. Santjoko,Xiaofeng Wang,Michael A. Gunarso,Bin Hu*

Main category: cs.RO

TL;DR: 本研究提出了一种新颖的闭合形式控制障碍函数框架，用于高效实时地保证Stewart机器人平台的安全性，显著减少了计算需求。


<details>
  <summary>Details</summary>
Motivation: 需要一种高效且实时的方法来保证Stewart机器人平台的安全性，避免在每个控制步骤都求解二次规划问题。

Method: 提出了一种闭合形式的控制障碍函数框架，通过显式的控制律同时处理多个位置和速度约束。

Result: 控制器在定制的Stewart平台原型上经过仿真和硬件实验验证，展示了安全性能，计算时间减少了超过一个数量级。

Conclusion: 该方法提供了一种可靠且计算轻量级的实时安全控制框架，效果与基于QP的方案相当。

Abstract: This letter presents a closed-form solution of Control Barrier Function (CBF) framework for enforcing safety constraints on a Stewart robotic platform. The proposed method simultaneously handles multiple position and velocity constraints through an explicit closed-form control law, eliminating the need to solve a Quadratic Program (QP) at every control step and enabling efficient real-time implementation. This letter derives necessary and sufficient conditions under which the closed-form expression remains non-singular, thereby ensuring well-posedness of the CBF solution to multi-constraint problem. The controller is validated in both simulation and hardware experiments on a custom-built Stewart platform prototype, demonstrating safetyguaranteed performance that is comparable to the QP-based formulation, while reducing computation time by more than an order of magnitude. The results confirm that the proposed approach provides a reliable and computationally lightweight framework for real-time safe control of parallel robotic systems. The experimental videos are available on the project website. (https://nail-uh.github.io/StewartPlatformSafeControl.github.io/)

</details>


### [4] [Learning Category-level Last-meter Navigation from RGB Demonstrations of a Single-instance](https://arxiv.org/abs/2512.11173)
*Tzu-Hsien Lee,Fidan Mahmudova,Karthik Desingh*

Main category: cs.RO

TL;DR: 提出了一种对象中心的模仿学习框架，成功实现四足移动操控器的最后一米导航，确保在多种挑战性环境中对未见目标的定位。


<details>
  <summary>Details</summary>
Motivation: RGB导航系统无法满足精确定位的需求，这使得操作策略难以在训练演示的分布内运行，导致执行失败。

Method: 使用基于RGB的观察与目标图像及文本提示相结合的方法，配合语言驱动的分割模块和空间评分矩阵解码器，实现精确的对象定位和相对姿态推理。

Result: 实验结果显示，使用新方法在边缘对齐和对象对齐的成功率分别达到73.47%和96.94%。

Conclusion: 该研究提出了一种基于对象的模仿学习框架，能够在不依赖深度、LiDAR或地图先验的情况下，实现移动操作机器人的精确定位。

Abstract: Achieving precise positioning of the mobile manipulator's base is essential for successful manipulation actions that follow. Most of the RGB-based navigation systems only guarantee coarse, meter-level accuracy, making them less suitable for the precise positioning phase of mobile manipulation. This gap prevents manipulation policies from operating within the distribution of their training demonstrations, resulting in frequent execution failures. We address this gap by introducing an object-centric imitation learning framework for last-meter navigation, enabling a quadruped mobile manipulator robot to achieve manipulation-ready positioning using only RGB observations from its onboard cameras. Our method conditions the navigation policy on three inputs: goal images, multi-view RGB observations from the onboard cameras, and a text prompt specifying the target object. A language-driven segmentation module and a spatial score-matrix decoder then supply explicit object grounding and relative pose reasoning. Using real-world data from a single object instance within a category, the system generalizes to unseen object instances across diverse environments with challenging lighting and background conditions. To comprehensively evaluate this, we introduce two metrics: an edge-alignment metric, which uses ground truth orientation, and an object-alignment metric, which evaluates how well the robot visually faces the target. Under these metrics, our policy achieves 73.47% success in edge-alignment and 96.94% success in object-alignment when positioning relative to unseen target objects. These results show that precise last-meter navigation can be achieved at a category-level without depth, LiDAR, or map priors, enabling a scalable pathway toward unified mobile manipulation. Project page: https://rpm-lab-umn.github.io/category-level-last-meter-nav/

</details>


### [5] [Seeing to Act, Prompting to Specify: A Bayesian Factorization of Vision Language Action Policy](https://arxiv.org/abs/2512.11218)
*Kechun Xu,Zhenjie Zhu,Anzhe Chen,Shuqi Zhao,Qing Huang,Yifei Yang,Haojian Lu,Rong Xiong,Masayoshi Tomizuka,Yue Wang*

Main category: cs.RO

TL;DR: BayesVLA通过贝叶斯分解解决VLA模型在微调中遭遇的视觉语言模型遗忘问题，提高了泛化能力与指令跟随能力。


<details>
  <summary>Details</summary>
Motivation: 研究视觉-语言-行动(VLA)模型在分布外泛化时面临的灾难性遗忘问题，特别是在微调过程中视觉语言模型(VLM)的遗忘。

Method: 提出BayesVLA，一个贝叶斯分解方法，将策略分解为视觉-行动先验和语言条件似然，旨在增强模型的泛化能力和指令跟随能力，并结合预接触和后接触阶段以更好地利用预训练基础模型。

Result: 通过信息论分析证实了BayesVLA在缓解快捷学习方面的有效性，并在针对未见指令、物体和环境的广泛实验中表现出比现有方法更优越的泛化能力。

Conclusion: BayesVLA能够有效解决VLA模型在微调过程中的灾难性遗忘问题，从而提升模型的准确性与泛化能力。

Abstract: The pursuit of out-of-distribution generalization in Vision-Language-Action (VLA) models is often hindered by catastrophic forgetting of the Vision-Language Model (VLM) backbone during fine-tuning. While co-training with external reasoning data helps, it requires experienced tuning and data-related overhead. Beyond such external dependencies, we identify an intrinsic cause within VLA datasets: modality imbalance, where language diversity is much lower than visual and action diversity. This imbalance biases the model toward visual shortcuts and language forgetting. To address this, we introduce BayesVLA, a Bayesian factorization that decomposes the policy into a visual-action prior, supporting seeing-to-act, and a language-conditioned likelihood, enabling prompt-to-specify. This inherently preserves generalization and promotes instruction following. We further incorporate pre- and post-contact phases to better leverage pre-trained foundation models. Information-theoretic analysis formally validates our effectiveness in mitigating shortcut learning. Extensive experiments show superior generalization to unseen instructions, objects, and environments compared to existing methods. Project page is available at: https://xukechun.github.io/papers/BayesVLA.

</details>


### [6] [Elevation Aware 2D/3D Co-simulation Framework for Large-scale Traffic Flow and High-fidelity Vehicle Dynamics](https://arxiv.org/abs/2512.11249)
*Chandra Raskoti,Weizi Li*

Main category: cs.RO

TL;DR: 本文提出了一种自动化的、考虑地形的共同仿真框架，能够生成平滑的高程轮廓，并有效支持城市复杂地形的模拟。


<details>
  <summary>Details</summary>
Motivation: 现有工具在捕捉现实世界高程方面的不足限制了其在复杂城市地形中的应用。

Method: 通过将SUMO与CARLA集成，利用OpenStreetMap路网和USGS高程数据，构建物理一致的3D环境。

Result: 在多个旧金山地区的演示中，展示了该框架的可扩展性和再现陡峭、非规则地形的能力。

Conclusion: 该框架为高保真自动驾驶车辆在具有高度变化的城市环境中的测试提供了实用基础。

Abstract: Reliable testing of autonomous driving systems requires simulation environments that combine large-scale traffic modeling with realistic 3D perception and terrain. Existing tools rarely capture real-world elevation, limiting their usefulness in cities with complex topography. This paper presents an automated, elevation-aware co-simulation framework that integrates SUMO with CARLA using a pipeline that fuses OpenStreetMap road networks and USGS elevation data into physically consistent 3D environments. The system generates smooth elevation profiles, validates geometric accuracy, and enables synchronized 2D-3D simulation across platforms. Demonstrations on multiple regions of San Francisco show the framework's scalability and ability to reproduce steep and irregular terrain. The result is a practical foundation for high-fidelity autonomous vehicle testing in realistic, elevation-rich urban settings.

</details>


### [7] [Optimal Control and Structurally-Informed Gradient Optimization of a Custom 4-DOF Rigid-Body Manipulator](https://arxiv.org/abs/2512.11250)
*Brock Marcinczyk,Logan E. Beaver*

Main category: cs.RO

TL;DR: 开发了一个结合降阶PMP控制器和梯度下降的框架，以高效处理4-DOF刚体操纵器的控制问题。


<details>
  <summary>Details</summary>
Motivation: 构建一个控制中心的框架以提高4自由度刚体 manipulator 的性能。

Method: 结合降阶 Pontryagin 最大原则(PMP)控制器和物理信息的梯度下降阶段。

Result: 提供相应的运动学轨迹和动态一致的时间范围，并生成闭合形式的逆动力学输入。

Conclusion: 该框架保持严格的控制理论结构，同时以计算高效的方式嵌入了操作器的物理约束和加载行为。

Abstract: This work develops a control-centric framework for a custom 4-DOF rigid-body manipulator by coupling a reduced-order Pontryagin's Maximum Principle (PMP) controller with a physics-informed Gradient Descent stage. The reduced PMP model provides a closed-form optimal control law for the joint accelerations, while the Gradient Descent module determines the corresponding time horizons by minimizing a cost functional built directly from the full Rigid-Body Dynamics. Structural-mechanics reaction analysis is used only to initialize feasible joint velocities-most critically the azimuthal component-ensuring that the optimizer begins in a physically admissible region. The resulting kinematic trajectories and dynamically consistent time horizons are then supplied to the symbolic Euler-Lagrange model to yield closed-form inverse-dynamics inputs. This pipeline preserves a strict control-theoretic structure while embedding the physical constraints and loading behavior of the manipulator in a computationally efficient way.

</details>


### [8] [Towards Logic-Aware Manipulation: A Knowledge Primitive for VLM-Based Assistants in Smart Manufacturing](https://arxiv.org/abs/2512.11275)
*Suchang Chen,Daqiang Guo*

Main category: cs.RO

TL;DR: 本文提出了一种对象中心的操作逻辑框架，用于提升视觉-语言模型在制造中的应用，重点帮助机器人完成接触丰富的操作。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型在机器人操作中侧重于广泛的语义泛化，但通常忽略了制造过程中执行关键的参数。

Method: 我们提出了一个包含八个字段的操作逻辑架构τ，并在3D打印机线圈移除任务中实例化该框架，结合知识库进行分析和评估规划质量。

Result: 通过适应于现有规划基准的计划质量指标分析了τ条件下的视觉-语言模型规划，并展示了该架构如何支持数据增强和逻辑感知的提示机制。

Conclusion: 该方法为智能制造中的助手系统提供了一个基础，能够在训练和测试阶段提升模型的效果和可靠性。

Abstract: Existing pipelines for vision-language models (VLMs) in robotic manipulation prioritize broad semantic generalization from images and language, but typically omit execution-critical parameters required for contact-rich actions in manufacturing cells. We formalize an object-centric manipulation-logic schema, serialized as an eight-field tuple τ, which exposes object, interface, trajectory, tolerance, and force/impedance information as a first-class knowledge signal between human operators, VLM-based assistants, and robot controllers. We instantiate τ and a small knowledge base (KB) on a 3D-printer spool-removal task in a collaborative cell, and analyze τ-conditioned VLM planning using plan-quality metrics adapted from recent VLM/LLM planning benchmarks, while demonstrating how the same schema supports taxonomy-tagged data augmentation at training time and logic-aware retrieval-augmented prompting at test time as a building block for assistant systems in smart manufacturing enterprises.

</details>


### [9] [Incremental Validation of Automated Driving Functions using Generic Volumes in Micro- Operational Design Domains](https://arxiv.org/abs/2512.11351)
*Steffen Schäfer,Martin Cichon*

Main category: cs.RO

TL;DR: 本文提出了一种结构化方法，通过细分操作设计域（ODDs）为微操作设计域（mODDs），生成测试案例以确保自动驾驶系统的感知性能在各种实际情况下的有效性。


<details>
  <summary>Details</summary>
Motivation: 针对高级自动驾驶系统在现实条件下的验证需求，本文旨在解决从理论分类到实际测试案例转化过程中的无序性和不完整性问题。

Method: 本文提出了一种通过将ODDs细分为可管理部分（micro-ODDs）并用抽象对象表示法生成测试案例的方法，示范了这一方法在一维横向引导操作中的应用。

Result: 实验结果表明，边缘案例在障碍物检测中的系统探索和基于车辆行为观察的感知质量评估，支持为安全论证开发标准化框架的必要性。

Conclusion: 该研究提出的结构化方法为自动驾驶系统的测试提供了一种有效的框架，能够系统地探索边缘案例并评估感知质量，为自动驾驶功能的验证和授权奠定基础。

Abstract: The validation of highly automated, perception-based driving systems must ensure that they function correctly under the full range of real-world conditions. Scenario-based testing is a prominent approach to addressing this challenge, as it involves the systematic simulation of objects and environments. Operational Design Domains (ODDs) are usually described using a taxonomy of qualitative designations for individual objects. However, the process of transitioning from taxonomy to concrete test cases remains unstructured, and completeness is theoretical. This paper introduces a structured method of subdividing the ODD into manageable sections, termed micro-ODDs (mODDs), and deriving test cases with abstract object representations. This concept is demonstrated using a one-dimensional, laterally guided manoeuvre involving a shunting locomotive within a constrained ODD. In this example, mODDs are defined and refined into narrow taxonomies that enable test case generation. Obstacles are represented as generic cubes of varying sizes, providing a simplified yet robust means of evaluating perception performance. A series of tests were conducted in a closed-loop, co-simulated virtual environment featuring photorealistic rendering and simulated LiDAR, GNSS and camera sensors. The results demonstrate how edge cases in obstacle detection can be systematically explored and how perception quality can be evaluated based on observed vehicle behaviour, using crash versus safe stop as the outcome metrics. These findings support the development of a standardised framework for safety argumentation and offer a practical step towards the validation and authorisation of automated driving functions.

</details>


### [10] [An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges](https://arxiv.org/abs/2512.11362)
*Chao Xu,Suyu Zhang,Yang Liu,Baigui Sun,Weihong Chen,Bo Xu,Qi Liu,Juncheng Wang,Shujun Wang,Shan Luo,Jan Peters,Athanasios V. Vasilakos,Stefanos Zafeiriou,Jiankang Deng*

Main category: cs.RO

TL;DR: 本调查研究提供了视觉语言行动（VLA）模型的系统指南，分析了该领域的模块、历史里程碑和核心挑战，并为新手和经验丰富的研究者提供基础和战略路线图。


<details>
  <summary>Details</summary>
Motivation: VLA模型正在推动机器人领域的变革，使机器能够理解指令并与物理世界互动。

Method: 调查提出了VLA模型的发展路径，分为基本模块、历史里程碑及主要挑战，重点分析五个关键挑战：表示、执行、泛化、安全性及数据集与评估。

Result: 详细 breakdown 了 VLA 领域面临的五大挑战，并回顾现有方法，指出未来的研究机会。

Conclusion: 本研究旨在加速学习和激发新思路，为相关领域的新手和研究者提供资源。

Abstract: Vision-Language-Action (VLA) models are driving a revolution in robotics, enabling machines to understand instructions and interact with the physical world. This field is exploding with new models and datasets, making it both exciting and challenging to keep pace with. This survey offers a clear and structured guide to the VLA landscape. We design it to follow the natural learning path of a researcher: we start with the basic Modules of any VLA model, trace the history through key Milestones, and then dive deep into the core Challenges that define recent research frontier. Our main contribution is a detailed breakdown of the five biggest challenges in: (1) Representation, (2) Execution, (3) Generalization, (4) Safety, and (5) Dataset and Evaluation. This structure mirrors the developmental roadmap of a generalist agent: establishing the fundamental perception-action loop, scaling capabilities across diverse embodiments and environments, and finally ensuring trustworthy deployment-all supported by the essential data infrastructure. For each of them, we review existing approaches and highlight future opportunities. We position this paper as both a foundational guide for newcomers and a strategic roadmap for experienced researchers, with the dual aim of accelerating learning and inspiring new ideas in embodied intelligence. A live version of this survey, with continuous updates, is maintained on our \href{https://suyuz1.github.io/Survery/}{project page}.

</details>


### [11] [The Influence of Human-like Appearance on Expected Robot Explanations](https://arxiv.org/abs/2512.11746)
*Hana Kopecka,Jose Such*

Main category: cs.RO

TL;DR: 本研究探讨了机器人的人类外观如何影响用户期待的解释，发现外观越人性化，用户期待的解释越人性化。


<details>
  <summary>Details</summary>
Motivation: 探讨机器人外观对用户心理模型和人机交互的影响，特别是其对用户期待解释的影响。

Method: 设计了一项包含视觉刺激的被试间研究，测试三种不同人类外观的家庭服务机器人，受访者被要求描述他们期待的机器人行为解释。

Result: 结果显示，大多数解释在人性化方面存在显著性，且人性化解释与人类外观之间存在正相关关系，同时也观察到非人性化解释和机器人描述的趋势。

Conclusion: 人类对机器人的期望解释受到该机器人外观的显著影响，尤其是在人类外观与人性化解释之间存在正相关关系。

Abstract: A robot's appearance is a known factor influencing user's mental model and human-robot interaction, that has not been studied in the context of its influence in expected robot explanations. In this study, we investigate whether and to what extent the human-like appearance of robots elicits anthropomorphism, which is conceptualised as an attribution of mental capacities, and how the level of anthropomorphism is revealed in explanations that people expect to receive. We designed a between-subject study comprising conditions with visual stimuli of three domestic service robots with varying human-like appearance, and we prompted respondents to provide explanations they would expect to receive from the robot for the same robot actions. We found that most explanations were anthropomorphic across all conditions. However, there is a positive correlation between the anthropomorphic explanations and human-like appearance. We also report on more nuanced trends observed in non-anthropomorphic explanations and trends in robot descriptions.

</details>


### [12] [CarlaNCAP: A Framework for Quantifying the Safety of Vulnerable Road Users in Infrastructure-Assisted Collective Perception Using EuroNCAP Scenarios](https://arxiv.org/abs/2512.11551)
*Jörg Gamerdinger,Sven Teufel,Simon Roller,Oliver Bringmann*

Main category: cs.RO

TL;DR: 本研究提出了一种评估基础设施辅助集体感知技术对于提高脆弱道路用户安全性的框架，并通过一项包含11k帧的CarlaNCAP数据集展示了其在事故避免率上的显著优势。


<details>
  <summary>Details</summary>
Motivation: 随着路面用户的增加，交通事故风险显著上升，特别是脆弱道路用户在城市环境中面临更高风险，因此需要有效的技术解决方案来提高他们的安全性。

Method: 提出评估基础设施辅助集体感知技术的框架，并使用包含安全关键欧洲新车评估计划（EuroNCAP）场景的CarlaNCAP数据集进行深入模拟研究。

Result: 通过模拟研究，发现基础设施辅助的集体感知技术能够在安全关键场景中显著降低事故发生率，事故避免率可达到100%，而只有配备33%传感器的车辆则为33%。

Conclusion: 基础设施辅助的集体感知显著提升了脆弱道路用户的安全性，并提供了强有力的数据支持以促使决策者采用该技术。

Abstract: The growing number of road users has significantly increased the risk of accidents in recent years. Vulnerable Road Users (VRUs) are particularly at risk, especially in urban environments where they are often occluded by parked vehicles or buildings. Autonomous Driving (AD) and Collective Perception (CP) are promising solutions to mitigate these risks. In particular, infrastructure-assisted CP, where sensor units are mounted on infrastructure elements such as traffic lights or lamp posts, can help overcome perceptual limitations by providing enhanced points of view, which significantly reduces occlusions. To encourage decision makers to adopt this technology, comprehensive studies and datasets demonstrating safety improvements for VRUs are essential. In this paper, we propose a framework for evaluating the safety improvement by infrastructure-based CP specifically targeted at VRUs including a dataset with safety-critical EuroNCAP scenarios (CarlaNCAP) with 11k frames. Using this dataset, we conduct an in-depth simulation study and demonstrate that infrastructure-assisted CP can significantly reduce accident rates in safety-critical scenarios, achieving up to 100% accident avoidance compared to a vehicle equipped with sensors with only 33%. Code is available at https://github.com/ekut-es/carla_ncap

</details>


### [13] [Cross-Entropy Optimization of Physically Grounded Task and Motion Plans](https://arxiv.org/abs/2512.11571)
*Andreu Matoses Gimenez,Nils Wilde,Chris Pek,Javier Alonso-Mora*

Main category: cs.RO

TL;DR: 本文提出一种结合高层次策略与低层次运动控制的算法，利用GPU加速的物理仿真，优化机器人在复杂环境中执行任务的能力。


<details>
  <summary>Details</summary>
Motivation: 传统的TAMP算法在简化和抽象问题时，往往忽略了动力学和复杂接触，使得计划在现实系统中实现时可能失败。因此，探索更高效的方法，提高机器人的操作可靠性。

Method: 利用GPU并行物理仿真器计算既包含高层次动作也包含低层次运动控制器的计划实现，考虑动态和环境接触。

Result: 通过交叉熵优化，采样控制器参数以获得低成本解决方案，机器人可以直接执行计算出的计划。

Conclusion: 该方法使机器人能够更可靠地执行对象操作任务，利用环境的几何特性来移动物体。

Abstract: Autonomously performing tasks often requires robots to plan high-level discrete actions and continuous low-level motions to realize them. Previous TAMP algorithms have focused mainly on computational performance, completeness, or optimality by making the problem tractable through simplifications and abstractions. However, this comes at the cost of the resulting plans potentially failing to account for the dynamics or complex contacts necessary to reliably perform the task when object manipulation is required. Additionally, approaches that ignore effects of the low-level controllers may not obtain optimal or feasible plan realizations for the real system. We investigate the use of a GPU-parallelized physics simulator to compute realizations of plans with motion controllers, explicitly accounting for dynamics, and considering contacts with the environment. Using cross-entropy optimization, we sample the parameters of the controllers, or actions, to obtain low-cost solutions. Since our approach uses the same controllers as the real system, the robot can directly execute the computed plans. We demonstrate our approach for a set of tasks where the robot is able to exploit the environment's geometry to move an object. Website and code: https://andreumatoses.github.io/research/parallel-realization

</details>


### [14] [UniBYD: A Unified Framework for Learning Robotic Manipulation Across Embodiments Beyond Imitation of Human Demonstrations](https://arxiv.org/abs/2512.11609)
*Tingyu Yuan,Biaoliang Guan,Wen Ye,Ziyan Tian,Yi Yang,Weijie Zhou,Yan Huang,Peng Wang,Chaoyang Zhao,Jinqiao Wang*

Main category: cs.RO

TL;DR: 本文提出了UniBYD框架，通过动态强化学习算法结合统一形态表示，显著提高了机器人操控的成功率，超越了单纯模仿人类操作。


<details>
  <summary>Details</summary>
Motivation: 由于机器人与人类手之间的体现差距，学习人类示范的能力受到限制，现有研究普遍仅限于模仿人类操控，因此需要新的方法以提高任务表现。

Method: 设计了一个包含统一形态表示的动态PPO算法，结合混合Markov基础的阴影引擎，使得强化学习能在不同机器人手形态间有效探索操控策略。

Result: 提出了一种统一的框架UniBYD，通过动态强化学习算法探索与机器人物理特性相适应的操控策略，实现了对多种机器人手形态的统一建模。

Conclusion: UniBYD通过引入统一形态表示和动态PPO算法，使机器人能够更好地探索适应其形态的操控策略，从而提高了操控任务的成功率。

Abstract: In embodied intelligence, the embodiment gap between robotic and human hands brings significant challenges for learning from human demonstrations. Although some studies have attempted to bridge this gap using reinforcement learning, they remain confined to merely reproducing human manipulation, resulting in limited task performance. In this paper, we propose UniBYD, a unified framework that uses a dynamic reinforcement learning algorithm to discover manipulation policies aligned with the robot's physical characteristics. To enable consistent modeling across diverse robotic hand morphologies, UniBYD incorporates a unified morphological representation (UMR). Building on UMR, we design a dynamic PPO with an annealed reward schedule, enabling reinforcement learning to transition from imitation of human demonstrations to explore policies adapted to diverse robotic morphologies better, thereby going beyond mere imitation of human hands. To address the frequent failures of learning human priors in the early training stage, we design a hybrid Markov-based shadow engine that enables reinforcement learning to imitate human manipulations in a fine-grained manner. To evaluate UniBYD comprehensively, we propose UniManip, the first benchmark encompassing robotic manipulation tasks spanning multiple hand morphologies. Experiments demonstrate a 67.90% improvement in success rate over the current state-of-the-art. Upon acceptance of the paper, we will release our code and benchmark at https://github.com/zhanheng-creator/UniBYD.

</details>


### [15] [Architecting Large Action Models for Human-in-the-Loop Intelligent Robots](https://arxiv.org/abs/2512.11620)
*Kanisorn Sangchai,Methasit Boonpun,Withawin Kraipetchara,Paulo Garcia*

Main category: cs.RO

TL;DR: 该研究展示了一种通过组合现成的基础模型构建高效的大型动作模型的方法，旨在提升智能机器人在感知、推理和行动中的能力，同时增强控制性、可解释性和可验证性。


<details>
  <summary>Details</summary>
Motivation: 实现自主操作和与其他智能体（人类或人工智能）交互的智能机器人需要整合环境感知、推理和行动，这是目前技术面临的挑战。

Method: 通过结合现成的基础模型和符号包装器，结合对输出的验证，构建可靠的大型动作模型，无需庞大的端到端训练。

Result: 实验表明，通过生成规划域定义语言（PDDL）代码驱动行动执行，可以有效减轻动作幻觉问题。

Conclusion: 本研究结果为机器人大型动作模型的设计和开发提供了支持，并指出确保安全性所需解决的持续挑战。

Abstract: The realization of intelligent robots, operating autonomously and interacting with other intelligent agents, human or artificial, requires the integration of environment perception, reasoning, and action. Classic Artificial Intelligence techniques for this purpose, focusing on symbolic approaches, have long-ago hit the scalability wall on compute and memory costs. Advances in Large Language Models in the past decade (neural approaches) have resulted in unprecedented displays of capability, at the cost of control, explainability, and interpretability. Large Action Models aim at extending Large Language Models to encompass the full perception, reasoning, and action cycle; however, they typically require substantially more comprehensive training and suffer from the same deficiencies in reliability. Here, we show it is possible to build competent Large Action Models by composing off-the-shelf foundation models, and that their control, interpretability, and explainability can be effected by incorporating symbolic wrappers and associated verification on their outputs, achieving verifiable neuro-symbolic solutions for intelligent robots. Our experiments on a multi-modal robot demonstrate that Large Action Model intelligence does not require massive end-to-end training, but can be achieved by integrating efficient perception models with a logic-driven core. We find that driving action execution through the generation of Planning Domain Definition Language (PDDL) code enables a human-in-the-loop verification stage that effectively mitigates action hallucinations. These results can support practitioners in the design and development of robotic Large Action Models across novel industries, and shed light on the ongoing challenges that must be addressed to ensure safety in the field.

</details>


### [16] [Bench-Push: Benchmarking Pushing-based Navigation and Manipulation Tasks for Mobile Robots](https://arxiv.org/abs/2512.11736)
*Ninghan Zhong,Steven Caro,Megnath Ramesh,Rishi Bhatnagar,Avraiem Iskandar,Stephen L. Smith*

Main category: cs.RO

TL;DR: 提出了Bench-Push，一个用于评估基于推的移动机器人任务的统一基准，包含模拟环境和新评估指标。


<details>
  <summary>Details</summary>
Motivation: 随着移动机器人在拥挤环境中的应用增多，推动式交互的必要性日益显著，而当前评估方法无法满足横向比较需求。

Method: 提出一个名为Bench-Push的统一基准，针对基于推的移动机器人导航和操作任务进行评估。

Result: Bench-Push包括多个组件，提供了多种模拟环境和新评估指标。

Conclusion: Bench-Push为推动基于推的机器人研究和比较提供了一个标准化的框架，并促进了研究的可重复性。

Abstract: Mobile robots are increasingly deployed in cluttered environments with movable objects, posing challenges for traditional methods that prohibit interaction. In such settings, the mobile robot must go beyond traditional obstacle avoidance, leveraging pushing or nudging strategies to accomplish its goals. While research in pushing-based robotics is growing, evaluations rely on ad hoc setups, limiting reproducibility and cross-comparison. To address this, we present Bench-Push, the first unified benchmark for pushing-based mobile robot navigation and manipulation tasks. Bench-Push includes multiple components: 1) a comprehensive range of simulated environments that capture the fundamental challenges in pushing-based tasks, including navigating a maze with movable obstacles, autonomous ship navigation in ice-covered waters, box delivery, and area clearing, each with varying levels of complexity; 2) novel evaluation metrics to capture efficiency, interaction effort, and partial task completion; and 3) demonstrations using Bench-Push to evaluate example implementations of established baselines across environments. Bench-Push is open-sourced as a Python library with a modular design. The code, documentation, and trained models can be found at https://github.com/IvanIZ/BenchNPIN.

</details>


### [17] [BLURR: A Boosted Low-Resource Inference for Vision-Language-Action Models](https://arxiv.org/abs/2512.11769)
*Xiaoyu Ma,Zhengqing Yuan,Zheyuan Zhang,Kaiwen Shi,Lichao Sun,Yanfang Ye*

Main category: cs.RO

TL;DR: BLURR 是一种轻量级的推理封装，能有效加速现有的视觉-语言-动作（VLA）控制器，在保持任务成功率的同时显著降低计算负荷和延迟。


<details>
  <summary>Details</summary>
Motivation: VLA模型在零-shot操作中表现出色，但其推理过程通常过于复杂，不适合响应快速的网页演示或在普通GPU上进行高频率的机器人控制。

Method: BLURR通过结合指令前缀键值缓存、混合精度执行和单步展开调度来实现控制加速，无需重新训练或更改模型检查点。

Result: 在基于SimplerEnv的评估中，BLURR在有效FLOPs和实际延迟方面显著降低，同时保持与原始控制器相当的任务成功率。

Conclusion: BLURR 提供了一种在紧张计算预算下部署现代 VLA 策略的实用方法，用户可以通过交互式网页演示实时切换控制器和推理选项。

Abstract: Vision-language-action (VLA) models enable impressive zero shot manipulation, but their inference stacks are often too heavy for responsive web demos or high frequency robot control on commodity GPUs. We present BLURR, a lightweight inference wrapper that can be plugged into existing VLA controllers without retraining or changing model checkpoints. Instantiated on the pi-zero VLA controller, BLURR keeps the original observation interfaces and accelerates control by combining an instruction prefix key value cache, mixed precision execution, and a single step rollout schedule that reduces per step computation. In our SimplerEnv based evaluation, BLURR maintains task success rates comparable to the original controller while significantly lowering effective FLOPs and wall clock latency. We also build an interactive web demo that allows users to switch between controllers and toggle inference options in real time while watching manipulation episodes. This highlights BLURR as a practical approach for deploying modern VLA policies under tight compute budgets.

</details>


### [18] [ProbeMDE: Uncertainty-Guided Active Proprioception for Monocular Depth Estimation in Surgical Robotics](https://arxiv.org/abs/2512.11773)
*Britton Jordan,Jordan Thompson,Jesse F. d'Almeida,Hao Li,Nithesh Kumar,Susheela Sharma Stern,Ipek Oguz,Robert J. Webster,Daniel Brown,Alan Kuntz,James Ferguson*

Main category: cs.RO

TL;DR: 本文提出ProbeMDE，通过结合RGB图像和稀疏深度测量来提高复杂环境下的单目深度估计准确性，验证结果表明性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中，例如外科场景，单目深度估计的预测不确定性和不准确性成为了一个重要问题。

Method: 提出ProbeMDE，一种成本意识的主动感知框架，结合RGB图像和稀疏的自我感知测量，利用多个深度估计模型进行深度图预测。

Result: 在模拟和物理实验中进行验证，结果显示该方法在标准深度估计指标上超越了基线方法，准确性更高，所需自我感知测量的数量较少。

Conclusion: ProbeMDE有效提升了复杂环境下的单目深度估计性能，通过结合 RGB 图像和稀疏的深度测量，降低了不确定性和提高了准确性。

Abstract: Monocular depth estimation (MDE) provides a useful tool for robotic perception, but its predictions are often uncertain and inaccurate in challenging environments such as surgical scenes where textureless surfaces, specular reflections, and occlusions are common. To address this, we propose ProbeMDE, a cost-aware active sensing framework that combines RGB images with sparse proprioceptive measurements for MDE. Our approach utilizes an ensemble of MDE models to predict dense depth maps conditioned on both RGB images and on a sparse set of known depth measurements obtained via proprioception, where the robot has touched the environment in a known configuration. We quantify predictive uncertainty via the ensemble's variance and measure the gradient of the uncertainty with respect to candidate measurement locations. To prevent mode collapse while selecting maximally informative locations to propriocept (touch), we leverage Stein Variational Gradient Descent (SVGD) over this gradient map. We validate our method in both simulated and physical experiments on central airway obstruction surgical phantoms. Our results demonstrate that our approach outperforms baseline methods across standard depth estimation metrics, achieving higher accuracy while minimizing the number of required proprioceptive measurements.

</details>


### [19] [Agile Flight Emerges from Multi-Agent Competitive Racing](https://arxiv.org/abs/2512.11781)
*Vineet Pasumarti,Lorenzo Bianchi,Antonio Loquercio*

Main category: cs.RO

TL;DR: 通过多智能体竞争和稀疏奖励，训练出先进的低级控制能力，实现了更好的模拟与现实转移。


<details>
  <summary>Details</summary>
Motivation: 随着环境复杂性的增加，研究如何利用稀疏奖励和多智能体竞争提高智能体在真实世界中的表现。

Method: 通过多智能体竞争和稀疏高层目标（赢得比赛）进行训练，采用强化学习

Result: 在模拟和现实世界中，证明了此方法优于独立训练的常见范式，特别是在复杂环境中表现更佳

Conclusion: 多智能体竞争产生的策略在现实世界中的转移性和通用性优于单智能体的进展奖励训练

Abstract: Through multi-agent competition and the sparse high-level objective of winning a race, we find that both agile flight (e.g., high-speed motion pushing the platform to its physical limits) and strategy (e.g., overtaking or blocking) emerge from agents trained with reinforcement learning. We provide evidence in both simulation and the real world that this approach outperforms the common paradigm of training agents in isolation with rewards that prescribe behavior, e.g., progress on the raceline, in particular when the complexity of the environment increases, e.g., in the presence of obstacles. Moreover, we find that multi-agent competition yields policies that transfer more reliably to the real world than policies trained with a single-agent progress-based reward, despite the two methods using the same simulation environment, randomization strategy, and hardware. In addition to improved sim-to-real transfer, the multi-agent policies also exhibit some degree of generalization to opponents unseen at training time. Overall, our work, following in the tradition of multi-agent competitive game-play in digital domains, shows that sparse task-level rewards are sufficient for training agents capable of advanced low-level control in the physical world.
  Code: https://github.com/Jirl-upenn/AgileFlight_MultiAgent

</details>


### [20] [AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis](https://arxiv.org/abs/2512.11797)
*Junjie Ye,Rong Xue,Basile Van Hoorick,Pavel Tokmakov,Muhammad Zubair Irshad,Yue Wang,Vitor Guizilini*

Main category: cs.RO

TL;DR: AnchorDream是一种基于运动的生成模型，通过将机器人运动渲染与视频扩散模型相结合，解决了现有方法在模仿学习中的局限性，生成多样化、高质量的机器人数据。


<details>
  <summary>Details</summary>
Motivation: 大规模多样化的机器人演示数据的获取是模仿学习的主要瓶颈，而现有的生成模型在行为生成和一致性上存在缺陷。

Method: AnchorDream利用预训练的视频扩散模型，根据机器人运动渲染条件化扩散过程，以防止幻觉并生成一致的对象和环境。

Result: 在模拟器基准测试中，相对增益达到36.4%，在现实世界研究中性能几乎翻倍，显示出生成数据对后续策略学习的一致改进。

Conclusion: 将生成的世界模型与机器人运动结合起来，为扩大模仿学习提供了实践路径。

Abstract: The collection of large-scale and diverse robot demonstrations remains a major bottleneck for imitation learning, as real-world data acquisition is costly and simulators offer limited diversity and fidelity with pronounced sim-to-real gaps. While generative models present an attractive solution, existing methods often alter only visual appearances without creating new behaviors, or suffer from embodiment inconsistencies that yield implausible motions. To address these limitations, we introduce AnchorDream, an embodiment-aware world model that repurposes pretrained video diffusion models for robot data synthesis. AnchorDream conditions the diffusion process on robot motion renderings, anchoring the embodiment to prevent hallucination while synthesizing objects and environments consistent with the robot's kinematics. Starting from only a handful of human teleoperation demonstrations, our method scales them into large, diverse, high-quality datasets without requiring explicit environment modeling. Experiments show that the generated data leads to consistent improvements in downstream policy learning, with relative gains of 36.4% in simulator benchmarks and nearly double performance in real-world studies. These results suggest that grounding generative world models in robot motion provides a practical path toward scaling imitation learning.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [21] [Measuring skill-based uplift from AI in a real biological laboratory](https://arxiv.org/abs/2512.10960)
*Ethan Obie Romero-Severson,Tara Harvey,Nick Generous,Phillip M. Mach*

Main category: cs.HC

TL;DR: 本研究评估了AI推理模型对无实验室经验参与者实验技能提升的影响。结果显示AI的介入显著提高了参与者的技能水平，同时为未来关于AI与生物安全关系的研究提供了数据支持。


<details>
  <summary>Details</summary>
Motivation: 研究AI系统在真实场景中的应用，以预测其潜在风险和益处，尤其是在生物应用领域。

Method: 通过对使用AI推理模型的参与者进行实验测量，并与仅能访问互联网的对照组进行比较，评估技能提升的影响。

Result: 参与者在转化大肠杆菌、诱导报告肽表达及通过质谱确认表达方面的成功率被量化记录。

Conclusion: AI系统的使用对参与者的实验技能有显著提升，实验设计和执行经验也为未来研究提供了重要教训。

Abstract: Understanding how AI systems are used by people in real situations that mirror aspects of both legitimate and illegitimate use is key to predicting the risks and benefits of AI systems. This is especially true in biological applications, where skill rather than knowledge is often the primary barrier for an untrained person. The challenge is that these studies are difficult to execute well and can take months to plan and run.
  Here we report the results of a pilot study that attempted to empirically measure the magnitude of \emph{skills-based uplift} caused by access to an AI reasoning model, compared with a control group that had only internet access. Participants -- drawn from a diverse pool of Los Alamos National Laboratory employees with no prior wet-lab experience -- were asked to transform \ecoli{} with a provided expression construct, induce expression of a reporter peptide, and have expression confirmed by mass spectrometry.
  We recorded quantitative outcomes (e.g., successful completion of experimental segments) and qualitative observations about how participants interacted with the AI system, the internet, laboratory equipment, and one another. We present the results of the study and lessons learned in designing and executing this type of study, and we discuss these results in the context of future studies of the evolving relationship between AI and global biosecurity.

</details>


### [22] [AI as Cognitive Amplifier: Rethinking Human Judgment in the Age of Generative AI](https://arxiv.org/abs/2512.10961)
*Tao An*

Main category: cs.HC

TL;DR: 这篇论文提出了AI作为人类能力的认知放大器，而非替代品的观点，并探讨了用户专业知识如何影响AI工具的输出质量。


<details>
  <summary>Details</summary>
Motivation: 观察到AI工具的使用效果因用户而异，旨在提供一个基于实践的视角。

Method: 基于人机交互、认知增强理论和教育技术的研究，以及在企业培训中的实地观察，分析专家与新手之间的性能差异。

Result: 提出AI参与的三层模型，强调用户的领域知识和反思能力对于AI输出质量的重要性。

Conclusion: 强调在工作发展和AI系统设计中的重要性，倡导加强领域专业知识和评估判断的综合方法。

Abstract: Through extensive experience training professionals and individual users in AI tool adoption since the GPT-3 era, I have observed a consistent pattern: the same AI tool produces dramatically different results depending on who uses it. While some frame AI as a replacement for human intelligence, and others warn of cognitive decline, this position paper argues for a third perspective grounded in practical observation: AI as a cognitive amplifier that magnifies existing human capabilities rather than substituting for them. Drawing on research in human-computer interaction, cognitive augmentation theory, and educational technology, alongside field observations from corporate training across writing, software development, and data analysis domains, I present a framework positioning AI tools as intelligence amplification systems where output quality depends fundamentally on user expertise and judgment. Through analysis of empirical studies on expert-novice differences and systematic observations from professional training contexts, I demonstrate that domain knowledge, quality judgment, and iterative refinement capabilities create substantial performance gaps between users. I propose a three-level model of AI engagement -- from passive acceptance through iterative collaboration to cognitive direction -- and argue that the transition between levels requires not technical training but development of domain expertise and metacognitive skills. This position has critical implications for workforce development and AI system design. Rather than focusing solely on AI literacy or technical prompt engineering, I advocate for integrated approaches that strengthen domain expertise, evaluative judgment, and reflective practice.

</details>


### [23] [Immutable Explainability: Towards Verifiable and Auditable Affective AI](https://arxiv.org/abs/2512.11065)
*Marcelo Fransoy,Alejandro Hossian,Hernán Merlino*

Main category: cs.HC

TL;DR: 本研究介绍了一种不可篡改可解释性的新架构，通过模糊逻辑推理引擎和区块链技术，解决了情感AI系统的黑箱问题和审计日志不可信的问题。


<details>
  <summary>Details</summary>
Motivation: 情感人工智能的广泛应用面临决策过程不透明和审计日志不可靠的挑战，因此需要一种新的架构来提高其透明度和信任度。

Method: 提出了结合模糊逻辑的可解释推理引擎与区块链技术的不可篡改可解释性架构，并在西班牙MEACorpus 2023上验证了该方法的有效性。

Result: 实验结果表明，所提出的模糊融合方法在性能上优于基线方法，并为情感AI系统提供了可靠的解释和审计记录。

Conclusion: 本研究提出的不可篡改可解释性架构有效解决了情感人工智能在透明性和审计日志可靠性方面的主要问题，能够为未来的情感AI系统提供可信赖的解释和用户数据控制。

Abstract: Affective artificial intelligence has made substantial advances in recent years; yet two critical issues persist, particularly in sensitive applications. First, these systems frequently operate as 'black boxes', leaving their decision-making processes opaque. Second, audit logs often lack reliability, as the entity operating the system may alter them. In this work, we introduce the concept of Immutable Explainability, an architecture designed to address both challenges simultaneously. Our approach combines an interpretable inference engine - implemented through fuzzy logic to produce a transparent trace of each decision - with a cryptographic anchoring mechanism that records this trace on a blockchain, ensuring that it is tamper-evident and independently verifiable. To validate the approach, we implemented a heuristic pipeline integrating lexical and prosodic analysis within an explicit Mamdani-type multimodal fusion engine. Each inference generates an auditable record that is subsequently anchored on a public blockchain (Sepolia Testnet). We evaluated the system using the Spanish MEACorpus 2023, employing both the original corpus transcriptions and those generated by Whisper. The results show that our fuzzy-fusion approach outperforms baseline methods (linear and unimodal fusion). Beyond these quantitative outcomes, our primary objective is to establish a foundation for affective AI systems that offer transparent explanations, trustworthy audit trails, and greater user control over personal data.

</details>


### [24] [Your plan may succeed, but what about failure? Investigating how people use ChatGPT for long-term life task planning](https://arxiv.org/abs/2512.11096)
*Ben Wang,Jiqun Liu*

Main category: cs.HC

TL;DR: 本研究探索了人们如何使用ChatGPT进行长期规划，发现它能帮助构建目标和生成想法，但输出往往缺乏个性化和真实感，因此用户需主动调整结果。


<details>
  <summary>Details</summary>
Motivation: 虽然长期生活任务规划复杂且不确定，但关于AI如何支持这一过程的研究仍然较少。

Method: 通过对14名参与者进行访谈研究，分析其使用ChatGPT进行长期规划的过程和反馈。

Result: ChatGPT作为反思伙伴在结构化目标和保持动力方面表现出色，但输出通常过于通用，缺乏个性化和适应性，参与者需要对结果进行验证和调整。

Conclusion: 研究表明AI在长期生活任务规划中提供支持，但存在个性化与适应性不足的问题。

Abstract: Long-term life task planning is inherently complex and uncertain, yet little is known about how emerging AI systems support this process. This study investigates how people use ChatGPT for such planning tasks, focusing on user practices, uncertainties, and perceptions of AI assistance. We conducted an interview study with 14 participants who engaged in long-term planning activities using ChatGPT, combining analysis of their prompts and interview responses. The task topics across diverse domains, including personal well-being, event planning, and professional learning, along with prompts to initiate, refine, and contextualize plans. ChatGPT helped structure complex goals into manageable steps, generate ideas, and sustain motivation, serving as a reflective partner. Yet its outputs were often generic or idealized, lacking personalization, contextual realism, and adaptability, requiring users to actively adapt and verify results. Participants expressed a need for AI systems that provide adaptive and trustworthy guidance while acknowledging uncertainty and potential failure in long-term planning. Our findings show how AI supports long-term life task planning under evolving uncertainty and highlight design implications for systems that are adaptive, uncertainty-aware, and capable of supporting long-term planning as an evolving human-AI collaboration.

</details>


### [25] [Supporting Medicinal Chemists in Iterative Hypothesis Generation for Drug Target Identification](https://arxiv.org/abs/2512.11105)
*Youngseung Jeon,Christopher Hwang,Ziwen Li,Taylor Le Lievre,Jesus J. Campagna,Cohn Whitaker,Varghese John,Eunice Jun,Xiang Anthony Chen*

Main category: cs.HC

TL;DR: HAPPIER是一个AI工具，通过多标准支持和迭代思维过程，帮助药物化学家生成高质量假设。


<details>
  <summary>Details</summary>
Motivation: 药物发现过程低效，需要更好的工具来支持目标蛋白质识别和假设生成。

Method: 开发AI工具HAPPIER，支持多标准目标识别和假设生成

Result: HAPPIER工具通过集成的图形组件支持药物化学家高效探索和验证蛋白质，提升假设生成效率，增加高信心假设的数量，支持迭代思维过程。

Conclusion: HAPPIER工具能够增强药物发现过程中的假设生成，并提升研究人员对假设结果的信心。

Abstract: While drug discovery is vital for human health, the process remains inefficient. Medicinal chemists must navigate a vast protein space to identify target proteins that meet three criteria: physical and functional interactions, therapeutic impact, and docking potential. Prior approaches have provided fragmented support for each criterion, limiting the generation of promising hypotheses for wet-lab experiments. We present HAPPIER, an AI-powered tool that supports hypothesis generation with integrated multi-criteria support for target identification. HAPPIER enables medicinal chemists to 1) efficiently explore and verify proteins in a single integrated graph component showing multi-criteria satisfaction and 2) validate AI suggestions with domain knowledge. These capabilities facilitate iterative cycles of divergent and convergent thinking, essential for hypothesis generation. We evaluated HAPPIER with ten medicinal chemists, finding that it increased the number of high-confidence hypotheses and support for the iterative cycle, and further demonstrated the relationship between engaging in such cycles and confidence in outputs.

</details>


### [26] [Breast-Rehab: A Postoperative Breast Cancer Rehabilitation Training Assessment System Based on Human Action Recognition](https://arxiv.org/abs/2512.11245)
*Zikang Chen,Tan Xie,Qinchuan Wang,Heming Zheng,Xudong Lu*

Main category: cs.HC

TL;DR: 开发了一种低成本的移动健康系统Breast-Rehab，以改善乳腺癌患者的上肢康复监测，结合创新的算法和用户友好的平台。


<details>
  <summary>Details</summary>
Motivation: 旨在解决乳腺癌幸存者术后上肢功能障碍及家庭康复训练依从性低的问题，同时克服常用VR系统的硬件负担。

Method: 开发了一个名为Breast-Rehab的低成本移动健康（mHealth）系统，结合人类动作识别算法和检索增强生成（RAG）框架。

Result: Breast-Rehab通过融合视觉和3D骨骼数据，能够准确分割在非控制环境中录制的康复运动视频，并生成临床相关的评估报告。初步临床研究验证了该系统的可行性和用户接受度。

Conclusion: Breast-Rehab为乳腺癌幸存者提供了一种AI驱动的、在家康复监测的完整验证流程，有效提高了患者的锻炼频率。

Abstract: Postoperative upper limb dysfunction is prevalent among breast cancer survivors, yet their adherence to at-home rehabilitation exercises is low amidst limited nursing resources. The hardware overhead of commonly adopted VR-based mHealth solutions further hinders their widespread clinical application. Therefore, we developed Breast-Rehab, a novel, low-cost mHealth system to provide patients with out-of-hospital upper limb rehabilitation management. Breast-Rehab integrates a bespoke human action recognition algorithm with a retrieval-augmented generation (RAG) framework. By fusing visual and 3D skeletal data, our model accurately segments exercise videos recorded in uncontrolled home environments, outperforming standard models. These segmented clips, combined with a domain-specific knowledge base, guide a multi-modal large language model to generate clinically relevant assessment reports. This approach significantly reduces computational overhead and mitigates model hallucinations. We implemented the system as a WeChat Mini Program and a nurse-facing dashboard. A preliminary clinical study validated the system's feasibility and user acceptance, with patients achieving an average exercise frequency of 0.59 sessions/day over a two-week period. This work thus presents a complete, validated pipeline for AI-driven, at-home rehabilitation monitoring.

</details>


### [27] [Words to Describe What I'm Feeling: Exploring the Potential of AI Agents for High Subjectivity Decisions in Advance Care Planning](https://arxiv.org/abs/2512.11276)
*Kellie Yu Hui Sim,Pin Sym Foong,Chenyu Zhao,Melanie Yi Ning Quek,Swarangi Subodh Mehta,Kenny Tsu Wei Choo*

Main category: cs.HC

TL;DR: 随着人们年龄增长和照护网络缩小，可靠的提前护理计划支持需求增加。我们开发了一个体验原型，研究参与者在ACP决策中使用代理的策略和需求，并提出设计建议以平衡风险和收益。


<details>
  <summary>Details</summary>
Motivation: 随着人口老龄化和照护者网络的缩小，对于可靠的提前护理计划（ACP）支持的需求显著增加。

Method: 我们构建了一个名为acpagent的体验原型，并邀请15名参与者在4个研讨会上训练代理，以便在ACP决策中作为个人代理。

Result: 我们分析了参与者的应对策略和功能需求，并将结果映射到代理自主性与人类控制的轴线上。以上研究结果显示，AI在ACP中可能发挥新的作用。

Conclusion: 我们提出了关于AI在提前护理计划（ACP）中扮演新角色的可能性，认为代理可以成为个人的倡导者，并随着时间的推移建立互通性。

Abstract: Serious illness can deprive patients of the capacity to speak for themselves. As populations age and caregiver networks shrink, the need for reliable support in Advance Care Planning (ACP) grows. To probe this fraught design space of using proxy agents for high-risk, high-subjectivity decisions, we built an experience prototype (\acpagent{}) and asked 15 participants in 4 workshops to train it to be their personal proxy in ACP decisions. We analysed their coping strategies and feature requests and mapped the results onto axes of agent autonomy and human control. Our findings argue for a potential new role of AI in ACP where agents act as personal advocates for individuals, building mutual intelligibility over time. We conclude with design recommendations to balance the risks and benefits of such an agent.

</details>


### [28] [AI Autonomy or Human Dependency? Defining the Boundary in Responsible AI with the $α$-Coefficient](https://arxiv.org/abs/2512.11295)
*Nattaya Mairittha,Gabriel Phorncharoenmusikul,Sorawit Worapradidth*

Main category: cs.HC

TL;DR: 现有AI系统依赖人类劳动力，提出AFHE框架以确保AI具备最低功能独立性，并通过算法验证其独立性，重塑人类角色。


<details>
  <summary>Details</summary>
Motivation: 当代AI系统结构性依赖人类劳动，导致伦理和经济的双重危机，需要重新设计AI以实现真正的独立性。

Method: 提出了AI自律系数（alpha）来量化AI功能独立性，并引入AFHE部署算法以确保系统在上线前达到设定标准。

Result: 本研究提出了一种新的AI设计标准，旨在解决当前AI系统中普遍存在的对人工依赖的结构性问题。

Conclusion: 通过引入AFHE框架和AI自律系数，推动AI系统朝向更高的独立性与透明性发展，从而减少对人类劳动的误用。

Abstract: The integrity of contemporary AI systems is undermined by a critical design flaw: the misappropriation of Human-in-the-Loop (HITL) models to mask systems that are fundamentally reliant on human labor. We term this structural reliance Human-Instead-of-AI (HISOAI). HISOAI systems represent an ethical failure and an unsustainable economic dependency, where human workers function as hidden operational fallbacks rather than strategic collaborators. To rectify this, we propose the AI-First, Human-Empowered (AFHE) paradigm. AFHE mandates a technological design where the AI component must achieve a minimum, quantifiable level of functional independence prior to deployment. This standard is formalized through the AI Autonomy Coefficient (alpha), a metric that determines the proportion of tasks that the AI successfully processes without mandatory human substitution. We introduce the AFHE Deployment Algorithm, an algorithmic gate that requires the system to meet a specified alpha threshold across both offline and shadow testing. By enforcing this structural separation, the AFHE framework redefines the human's role to focus exclusively on high-value tasks, including ethical oversight, boundary pushing, and strategic model tuning, thereby ensuring true system transparency and operational independence. This work advocates for a critical shift toward metric-driven, structurally sound AI architecture, moving the industry beyond deceptive human dependency toward verifiable autonomy.

</details>


### [29] [Mirror Skin: In Situ Visualization of Robot Touch Intent on Robotic Skin](https://arxiv.org/abs/2512.11472)
*David Wagmann,Matti Krüger,Chao Wang,Jürgen Steimle*

Main category: cs.HC

TL;DR: 本研究提出了一种名为Mirror Skin的高分辨率视觉反馈系统，以有效传达机器人触觉意图，增强人机交互的安全性和可预测性。


<details>
  <summary>Details</summary>
Motivation: 机器人触觉意图的有效传达是促进安全和可预测的人机交互的关键因素。

Method: 通过与虚拟现实专家进行结构化设计探索，迭代完善Mirror Skin的六个关键维度，之后进行了一项受控用户研究。

Result: 研究表明，Mirror Skin显著提高了对触摸意图的解释准确性，并减少了反应时间。

Conclusion: 视觉反馈在机器人皮肤上的潜力显示出可有效沟通人机触摸交互。

Abstract: Effective communication of robotic touch intent is a key factor in promoting safe and predictable physical human-robot interaction (pHRI). While intent communication has been widely studied, existing approaches lack the spatial specificity and semantic depth necessary to convey robot touch actions. We present Mirror Skin, a cephalopod-inspired concept that utilizes high-resolution, mirror-like visual feedback on robotic skin. By mapping in-situ visual representations of a human's body parts onto the corresponding robot's touch region, Mirror Skin communicates who shall initiate touch, where it will occur, and when it is imminent. To inform the design of Mirror Skin, we conducted a structured design exploration with experts in virtual reality (VR), iteratively refining six key dimensions. A subsequent controlled user study demonstrated that Mirror Skin significantly enhances accuracy and reduces response times for interpreting touch intent. These findings highlight the potential of visual feedback on robotic skin to communicate human-robot touch interactions.

</details>


### [30] [Say it or AI it: Evaluating Hands-Free Text Correction in Virtual Reality](https://arxiv.org/abs/2512.11564)
*Ziming Li,Joffrey Guilmet,Suzanne Sorli,Hai-Ning Liang,Diego Monteiro*

Main category: cs.HC

TL;DR: 在虚拟现实中，无控制器的文本纠正采用AI方法表现优于语音输入。


<details>
  <summary>Details</summary>
Motivation: 探讨在虚拟现实中无控制器的文本纠正技术，以提高用户的可用性和效率。

Method: 评估基于AI的文本纠正方法与语音输入的比较。

Result: 发现AI文本纠正的可用性优于语音输入。

Conclusion: 在虚拟现实中，AI文本纠正方法在无控制器的情况下提升了用户体验。

Abstract: Text entry in Virtual Reality (VR) is challenging, even when accounting for the use of controllers. Prior work has tackled this challenge head-on, improving the efficiency of input methods. These techniques have the advantage of allowing for relatively straightforward text correction. However, text correction without the use of controllers is a topic that has not received the same amount of attention, even though it can be desirable in several scenarios, and can even be the source of frustration. Large language models have been adopted and evaluated as a corrective methodology, given their high power for predictions. Nevertheless, their predictions are not always correct, which can lead to lower usability. In this paper, we investigate whether, for text correction in VR that is hands-free, the use of AI could surpass in terms of usability and efficiency. We observed better usability for AI text correction when compared to voice input.

</details>


### [31] [From Verification Burden to Trusted Collaboration: Design Goals for LLM-Assisted Literature Reviews](https://arxiv.org/abs/2512.11661)
*Brenda Nogueira,Werner Geyer,Andrew Anderson,Toby Jia-Jun Li,Dongwhi Kim,Nuno Moniz,Nitesh V. Chawla*

Main category: cs.HC

TL;DR: 本研究探讨了大型语言模型在文献综述中的应用，识别出信任、验证和工具依赖的痛点，并提出了一个框架以应对这些挑战，促进研究者与AI之间的协作。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在学术写作中应用广泛，但在文献综述中的具体实施、局限性和设计挑战尚未得到充分研究。

Method: 进行了一项用户研究，收集各学科研究者的反馈，以理解他们在使用LLMs进行相关工作调查时的实践、优势和痛点。

Result: 本文通过对多个学科研究者的用户研究，探讨了他们在文献综述过程中使用大型语言模型（LLMs）的实际做法、好处和痛点，识别出存在的三个主要问题并提出解决方案。

Conclusion: 提出了六个设计目标和一个高层框架，以通过改进相关论文可视化、每一步的验证和人类反馈的整合提高LLMs在学术写作中的实用性。

Abstract: Large Language Models (LLMs) are increasingly embedded in academic writing practices. Although numerous studies have explored how researchers employ these tools for scientific writing, their concrete implementation, limitations, and design challenges within the literature review process remain underexplored. In this paper, we report a user study with researchers across multiple disciplines to characterize current practices, benefits, and \textit{pain points} in using LLMs to investigate related work. We identified three recurring gaps: (i) lack of trust in outputs, (ii) persistent verification burden, and (iii) requiring multiple tools. This motivates our proposal of six design goals and a high-level framework that operationalizes them through improved related papers visualization, verification at every step, and human-feedback alignment with generation-guided explanations. Overall, by grounding our work in the practical, day-to-day needs of researchers, we designed a framework that addresses these limitations and models real-world LLM-assisted writing, advancing trust through verifiable actions and fostering practical collaboration between researchers and AI systems.

</details>


### [32] [Natural Language Interaction for Editing Visual Knowledge Graphs](https://arxiv.org/abs/2512.11674)
*Reza Shahriari,Eric D. Ragan,Jaime Ruiz*

Main category: cs.HC

TL;DR: 本研究探讨通过自然语言输入编辑知识图谱的有效性，发现其显著优于传统图形界面交互。


<details>
  <summary>Details</summary>
Motivation: 传统的图形用户界面在处理大量图形编辑时显得繁琐，用户需要逐个选择和更新项，影响效率，因此探讨自然语言输入的潜力。

Method: 进行用户研究，对比图形用户界面（GUI）编辑和两种自然语言编辑方法

Result: 自然语言方法的效果明显优于传统的GUI交互

Conclusion: 自然语言输入作为编辑网络图的替代方法，相比于传统界面编辑，能显著提升用户效率。

Abstract: Knowledge graphs are often visualized using node-link diagrams that reveal relationships and structure. In many applications using graphs, it is desirable to allow users to edit graphs to ensure data accuracy or provides updates. Commonly in graph visualization, users can interact directly with the visual elements by clicking and typing updates to specific items through traditional interaction methods in the graphical user interface. However, it can become tedious to make many updates due to the need to individually select and change numerous items in a graph. Our research investigates natural language input as an alternative method for editing network graphs. We present a user study comparing GUI graph editing with two natural language alternatives to contribute novel empirical data of the trade-offs of the different interaction methods. The findings show natural language methods to be significantly more effective than traditional GUI interaction.

</details>


### [33] [From Signal to Turn: Interactional Friction in Modular Speech-to-Speech Pipelines](https://arxiv.org/abs/2512.11724)
*Titaya Mairittha,Tanakon Sawanglok,Panuwit Raden,Jirapast Buntub,Thanapat Warunee,Napat Asawachaisuvikrom,Thanaphum Saiwongin*

Main category: cs.HC

TL;DR: 本论文探讨了语音AI系统交互中的摩擦点，认为这些问题源于模块化设计的结构性后果，呼吁优化系统连接以提升对话流畅性。


<details>
  <summary>Details</summary>
Motivation: 尽管语音AI系统的生成能力显著提升，但交互过程中常常出现对话中断现象，影响用户体验。

Method: 通过对一个代表性生产系统的分析，识别出导致对话中断的三种模式，并进行系统级别的探讨。

Result: 本研究分析了模块化语音系统中的交互摩擦，发现其导致的三种对话中断模式：时间错位、表达平坦化和修复僵化。

Conclusion: 构建自然的语音AI是一种基础设施设计挑战，需要从优化孤立组件转向精心编排它们之间的连接。

Abstract: While voice-based AI systems have achieved remarkable generative capabilities, their interactions often feel conversationally broken. This paper examines the interactional friction that emerges in modular Speech-to-Speech Retrieval-Augmented Generation (S2S-RAG) pipelines. By analyzing a representative production system, we move beyond simple latency metrics to identify three recurring patterns of conversational breakdown: (1) Temporal Misalignment, where system delays violate user expectations of conversational rhythm; (2) Expressive Flattening, where the loss of paralinguistic cues leads to literal, inappropriate responses; and (3) Repair Rigidity, where architectural gating prevents users from correcting errors in real-time. Through system-level analysis, we demonstrate that these friction points should not be understood as defects or failures, but as structural consequences of a modular design that prioritizes control over fluidity. We conclude that building natural spoken AI is an infrastructure design challenge, requiring a shift from optimizing isolated components to carefully choreographing the seams between them.

</details>
