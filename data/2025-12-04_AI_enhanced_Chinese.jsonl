{"id": "2512.03166", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03166", "abs": "https://arxiv.org/abs/2512.03166", "authors": ["Aya Taourirte", "Md Sohag Mia"], "title": "Multi-Agent Reinforcement Learning and Real-Time Decision-Making in Robotic Soccer for Virtual Environments", "comment": null, "summary": "The deployment of multi-agent systems in dynamic, adversarial environments like robotic soccer necessitates real-time decision-making, sophisticated cooperation, and scalable algorithms to avoid the curse of dimensionality. While Reinforcement Learning (RL) offers a promising framework, existing methods often struggle with the multi-granularity of tasks (long-term strategy vs. instant actions) and the complexity of large-scale agent interactions. This paper presents a unified Multi-Agent Reinforcement Learning (MARL) framework that addresses these challenges. First, we establish a baseline using Proximal Policy Optimization (PPO) within a client-server architecture for real-time action scheduling, with PPO demonstrating superior performance (4.32 avg. goals, 82.9% ball control). Second, we introduce a Hierarchical RL (HRL) structure based on the options framework to decompose the problem into a high-level trajectory planning layer (modeled as a Semi-Markov Decision Process) and a low-level action execution layer, improving global strategy (avg. goals increased to 5.26). Finally, to ensure scalability, we integrate mean-field theory into the HRL framework, simplifying many-agent interactions into a single agent vs. the population average. Our mean-field actor-critic method achieves a significant performance boost (5.93 avg. goals, 89.1% ball control, 92.3% passing accuracy) and enhanced training stability. Extensive simulations of 4v4 matches in the Webots environment validate our approach, demonstrating its potential for robust, scalable, and cooperative behavior in complex multi-agent domains.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u5206\u5c42\u7ed3\u6784\u548c\u5e73\u5747\u573a\u7406\u8bba\uff0c\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u5728\u5bf9\u6297\u73af\u5883\u4e2d\u7684\u51b3\u7b56\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u6709\u6548\u7684\u534f\u4f5c\u4e0e\u63a7\u5236\u80fd\u529b\u3002", "motivation": "\u5728\u52a8\u6001\u5bf9\u6297\u73af\u5883\u4e2d\uff0c\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u9762\u4e34\u4efb\u52a1\u591a\u7c92\u5ea6\u548c\u5927\u89c4\u6a21\u667a\u80fd\u4f53\u4ea4\u4e92\u7684\u6311\u6218\uff0c\u4e9f\u9700\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u5efa\u7acb\u57fa\u7ebf\uff0c\u7ed3\u5408\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\uff08HRL\uff09\u548c\u5e73\u5747\u573a\u7406\u8bba\u8fdb\u884c\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u51b3\u7b56\u5236\u5b9a\u3002", "result": "\u901a\u8fc7\u5f15\u5165\u5206\u5c42\u7ed3\u6784\u548c\u5e73\u5747\u573a\u7406\u8bba\uff0c\u6700\u7ec8\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5305\u62ec\u5e73\u5747\u76ee\u6807\u5f97\u5206\u8fbe\u52305.93\uff0c\u7403\u63a7\u738789.1%\uff0c\u4f20\u7403\u51c6\u786e\u738792.3%\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u7edf\u4e00\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5728\u590d\u6742\u7684\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u4e86\u51fa\u8272\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u65f6\u95f4\u8c03\u5ea6\u548c\u7b56\u7565\u89c4\u5212\u65b9\u9762\u663e\u8457\u63d0\u5347\u4e86\u76ee\u6807\u5f97\u5206\u548c\u63a7\u5236\u80fd\u529b\u3002"}}
{"id": "2512.03194", "categories": ["cs.RO", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.03194", "abs": "https://arxiv.org/abs/2512.03194", "authors": ["Johannes Gaber", "Meshal Alharbi", "Daniele Gammelli", "Gioele Zardini"], "title": "GRAND: Guidance, Rebalancing, and Assignment for Networked Dispatch in Multi-Agent Path Finding", "comment": null, "summary": "Large robot fleets are now common in warehouses and other logistics settings, where small control gains translate into large operational impacts. In this article, we address task scheduling for lifelong Multi-Agent Pickup-and-Delivery (MAPD) and propose a hybrid method that couples learning-based global guidance with lightweight optimization. A graph neural network policy trained via reinforcement learning outputs a desired distribution of free agents over an aggregated warehouse graph. This signal is converted into region-to-region rebalancing through a minimum-cost flow, and finalized by small, local assignment problems, preserving accuracy while keeping per-step latency within a 1 s compute budget. On congested warehouse benchmarks from the League of Robot Runners (LRR) with up to 500 agents, our approach improves throughput by up to 10% over the 2024 winning scheduler while maintaining real-time execution. The results indicate that coupling graph-structured learned guidance with tractable solvers reduces congestion and yields a practical, scalable blueprint for high-throughput scheduling in large fleets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u4f18\u5316\u7b97\u6cd5\u7684\u6df7\u5408\u4efb\u52a1\u8c03\u5ea6\u65b9\u6cd5\uff0c\u80fd\u5728\u5927\u578b\u673a\u5668\u4eba\u961f\u4f0d\u4e2d\u63d0\u5347\u8c03\u5ea6\u6548\u7387\u3002", "motivation": "\u5728\u4ed3\u5e93\u548c\u7269\u6d41\u73af\u5883\u4e2d\uff0c\u5927\u89c4\u6a21\u673a\u5668\u4eba\u961f\u4f0d\u7684\u4f7f\u7528\u65e5\u76ca\u666e\u904d\uff0c\u5c0f\u7684\u63a7\u5236\u589e\u76ca\u4f1a\u5bf9\u64cd\u4f5c\u4ea7\u751f\u5927\u7684\u5f71\u54cd\uff0c\u56e0\u6b64\u63d0\u9ad8\u591a\u667a\u80fd\u4f53\u4efb\u52a1\u8c03\u5ea6\u6548\u7387\u662f\u975e\u5e38\u91cd\u8981\u7684\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u751f\u6210\u671f\u671b\u7684\u673a\u5668\u4eba\u5206\u5e03\u4fe1\u53f7\uff0c\u5e76\u901a\u8fc7\u6700\u5c0f\u6210\u672c\u6d41\u8fdb\u884c\u533a\u57df\u95f4\u7684\u518d\u5e73\u8861\uff0c\u6700\u540e\u901a\u8fc7\u5c0f\u89c4\u6a21\u7684\u672c\u5730\u5206\u914d\u95ee\u9898\u5b8c\u6210\u8c03\u5ea6\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u5b66\u4e60\u57fa\u7840\u7684\u5168\u5c40\u6307\u5bfc\u548c\u8f7b\u91cf\u4f18\u5316\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8c03\u5ea6\u6548\u7387\u3002", "conclusion": "\u5c06\u56fe\u7ed3\u6784\u7684\u5b66\u4e60\u6307\u5bfc\u4e0e\u53ef\u5904\u7406\u7684\u6c42\u89e3\u5668\u7ed3\u5408\uff0c\u80fd\u591f\u6709\u6548\u964d\u4f4e\u62e5\u5835\uff0c\u63d0\u4f9b\u9ad8\u541e\u5410\u91cf\u7684\u5927\u89c4\u6a21\u8c03\u5ea6\u84dd\u56fe\u3002"}}
{"id": "2512.03256", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03256", "abs": "https://arxiv.org/abs/2512.03256", "authors": ["Albert H. Li", "Ivan Dario Jimenez Rodriguez", "Joel W. Burdick", "Yisong Yue", "Aaron D. Ames"], "title": "KALIKO: Kalman-Implicit Koopman Operator Learning For Prediction of Nonlinear Dynamical Systems", "comment": null, "summary": "Long-horizon dynamical prediction is fundamental in robotics and control, underpinning canonical methods like model predictive control. Yet, many systems and disturbance phenomena are difficult to model due to effects like nonlinearity, chaos, and high-dimensionality. Koopman theory addresses this by modeling the linear evolution of embeddings of the state under an infinite-dimensional linear operator that can be approximated with a suitable finite basis of embedding functions, effectively trading model nonlinearity for representational complexity. However, explicitly computing a good choice of basis is nontrivial, and poor choices may cause inaccurate forecasts or overfitting. To address this, we present Kalman-Implicit Koopman Operator (KALIKO) Learning, a method that leverages the Kalman filter to implicitly learn embeddings corresponding to latent dynamics without requiring an explicit encoder. KALIKO produces interpretable representations consistent with both theory and prior works, yielding high-quality reconstructions and inducing a globally linear latent dynamics. Evaluated on wave data generated by a high-dimensional PDE, KALIKO surpasses several baselines in open-loop prediction and in a demanding closed-loop simulated control task: stabilizing an underactuated manipulator's payload by predicting and compensating for strong wave disturbances.", "AI": {"tldr": "KALIKO\u5b66\u4e60\u5229\u7528\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u9690\u5f0f\u5b66\u4e60\u6f5c\u5728\u52a8\u529b\u5b66\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u7cfb\u7edf\u9884\u6d4b\uff0c\u8d85\u8d8a\u4e86\u591a\u4e2a\u57fa\u51c6\u3002", "motivation": "\u9488\u5bf9\u590d\u6742\u7cfb\u7edf\u96be\u4ee5\u5efa\u6a21\u7684\u95ee\u9898\uff0c\u5982\u975e\u7ebf\u6027\u3001\u6df7\u6c8c\u548c\u9ad8\u7ef4\u73b0\u8c61\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "Kalman-Implicit Koopman Operator (KALIKO) Learning", "result": "KALIKO\u65b9\u6cd5\u5229\u7528\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u9690\u5f0f\u5b66\u4e60\u5bf9\u5e94\u4e8e\u6f5c\u5728\u52a8\u529b\u5b66\u7684\u5d4c\u5165\uff0c\u80fd\u591f\u4ea7\u751f\u9ad8\u8d28\u91cf\u7684\u91cd\u6784\u548c\u5168\u7403\u7ebf\u6027\u7684\u6f5c\u5728\u52a8\u529b\u5b66\u3002", "conclusion": "KALIKO\u5728\u4e0d\u8981\u6c42\u663e\u5f0f\u7f16\u7801\u5668\u7684\u60c5\u51b5\u4e0b\uff0c\u6210\u529f\u7a33\u5b9a\u4e86\u5728\u5f3a\u6ce2\u52a8\u4f5c\u7528\u4e0b\u7684\u6b20\u9a71\u52a8\u64cd\u7eb5\u5668\u7684\u8d1f\u8f7d\u3002"}}
{"id": "2512.03347", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03347", "abs": "https://arxiv.org/abs/2512.03347", "authors": ["William van den Bogert", "Gregory Linkowski", "Nima Fazeli"], "title": "GOMP: Grasped Object Manifold Projection for Multimodal Imitation Learning of Manipulation", "comment": "8 pages, 8 figures, 2 tables", "summary": "Imitation Learning (IL) holds great potential for learning repetitive manipulation tasks, such as those in industrial assembly. However, its effectiveness is often limited by insufficient trajectory precision due to compounding errors. In this paper, we introduce Grasped Object Manifold Projection (GOMP), an interactive method that mitigates these errors by constraining a non-rigidly grasped object to a lower-dimensional manifold. GOMP assumes a precise task in which a manipulator holds an object that may shift within the grasp in an observable manner and must be mated with a grounded part. Crucially, all GOMP enhancements are learned from the same expert dataset used to train the base IL policy, and are adjusted with an n-arm bandit-based interactive component. We propose a theoretical basis for GOMP's improvement upon the well-known compounding error bound in IL literature. We demonstrate the framework on four precise assembly tasks using tactile feedback, and note that the approach remains modality-agnostic. Data and videos are available at williamvdb.github.io/GOMPsite.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faGOMP\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u975e\u521a\u6027\u6293\u53d6\u5bf9\u8c61\u9650\u5236\u5230\u8f83\u4f4e\u7ef4\u6d41\u5f62\uff0c\u6709\u6548\u51cf\u5c11\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u9519\u8bef\u53e0\u52a0\u95ee\u9898\uff0c\u63d0\u9ad8\u5de5\u4e1a\u7ec4\u88c5\u4efb\u52a1\u4e2d\u7684\u7cbe\u786e\u6027\u3002", "motivation": "\u7814\u7a76\u9a71\u52a8\u6e90\u4e8e\u6a21\u4eff\u5b66\u4e60\u5728\u91cd\u590d\u64cd\u7eb5\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u4f46\u9762\u4e34\u7684\u6311\u6218\u662f\u7531\u4e8e\u9519\u8bef\u53e0\u52a0\u5bfc\u81f4\u7684\u8f68\u8ff9\u7cbe\u5ea6\u4e0d\u8db3\u3002", "method": "GOMP\u7ed3\u5408\u4e86\u57fa\u4e8en\u81c2\u8001\u864e\u673a\u7684\u4ea4\u4e92\u7ec4\u4ef6\uff0c\u901a\u8fc7\u9650\u5236\u975e\u521a\u6027\u6293\u53d6\u5bf9\u8c61\u5230\u8f83\u4f4e\u7ef4\u6d41\u5f62\u6765\u6539\u8fdb\u6a21\u4eff\u5b66\u4e60\u3002", "result": "\u5728\u56db\u4e2a\u7cbe\u786e\u7ec4\u88c5\u4efb\u52a1\u4e2d\u8fd0\u7528GOMP\uff0c\u5c55\u793a\u4e86\u8be5\u6846\u67b6\u5728\u5229\u7528\u89e6\u89c9\u53cd\u9988\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5e76\u4fdd\u6301\u4e86\u6a21\u6001\u65e0\u5173\u6027\u3002", "conclusion": "GOMP\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u4eff\u5b66\u4e60\u5728\u5de5\u4e1a\u7ec4\u88c5\u4efb\u52a1\u4e2d\u7684\u7cbe\u786e\u6027\uff0c\u51cf\u5c11\u4e86\u7531\u4e8e\u9519\u8bef\u53e0\u52a0\u9020\u6210\u7684\u8f68\u8ff9\u4e0d\u51c6\u786e\u6027\u3002"}}
{"id": "2512.03186", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.03186", "abs": "https://arxiv.org/abs/2512.03186", "authors": ["Colin Barry", "Edward Jay Wang"], "title": "Smartphone Vibrometric Force Estimation for Grip Related Strength Measurements", "comment": null, "summary": "Hand grip strength is a widely used clinical biomarker linked to mobility, frailty, surgical outcomes, and overall health. This work explores a novel, phone only approach for estimating grip related force using a smartphone's built in vibration motor and inertial measurement unit. When the phone vibrates, applied finger force modulates the amplitude of high frequency accelerometer and gyroscope signals through Vibrometric Force Estimation. We profiled a Google Pixel 4 using synchronized IMU data and ground truth force measurements across varied force trajectories, then trained ridge regression models for both absolute and relative force prediction. In 15 fold hold one out validation, absolute force estimation achieved a mean absolute error of 1.88 lbs, while relative force estimation achieved a mean error of 10.1%. Although the method captures pinch type force rather than standardized full hand HGS, the results demonstrate the feasibility of smartphone based strength assessment using only on device sensors. This approach may enable large scale, low burden functional health measurements once profiling is completed for major smartphone models.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u667a\u80fd\u624b\u673a\u5185\u7f6e\u4f20\u611f\u5668\u4f30\u7b97\u63e1\u529b\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u793a\u4e86\u667a\u80fd\u624b\u673a\u5728\u529f\u80fd\u5065\u5eb7\u8bc4\u4f30\u4e2d\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u63e1\u529b\u662f\u4e0e\u79fb\u52a8\u6027\u3001\u865a\u5f31\u3001\u624b\u672f\u7ed3\u679c\u53ca\u6574\u4f53\u5065\u5eb7\u76f8\u5173\u7684\u91cd\u8981\u4e34\u5e8a\u751f\u7269\u6807\u5fd7\u7269\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e00\u79cd\u4fbf\u6377\u7684\u624b\u673a\u4f30\u7b97\u63e1\u529b\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u667a\u80fd\u624b\u673a\u7684\u632f\u52a8\u7535\u673a\u548c\u60ef\u6027\u6d4b\u91cf\u5355\u5143\uff0c\u901a\u8fc7\u632f\u52a8\u6765\u8c03\u8282\u9ad8\u9891\u52a0\u901f\u5ea6\u8ba1\u548c\u9640\u87ba\u4eea\u4fe1\u53f7\u7684\u5e45\u5ea6\uff0c\u5b9e\u73b0\u63e1\u529b\u4f30\u7b97\u3002", "result": "\u572815\u6298\u4ea4\u53c9\u9a8c\u8bc1\u4e2d\uff0c\u7edd\u5bf9\u52b2\u529b\u91cf\u6d4b\u91cf\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a1.88\u78c5\uff0c\u76f8\u5bf9\u52b2\u529b\u91cf\u6d4b\u91cf\u7684\u5e73\u5747\u8bef\u5dee\u4e3a10.1%\u3002", "conclusion": "\u867d\u7136\u6240\u6d4b\u91cf\u7684\u662f\u634f\u63e1\u529b\uff0c\u800c\u975e\u6807\u51c6\u7684\u5168\u638c\u63e1\u529b\uff0c\u4f46\u6b64\u65b9\u6cd5\u5c55\u793a\u4e86\u4f7f\u7528\u667a\u80fd\u624b\u673a\u8fdb\u884c\u529b\u91cf\u8bc4\u4f30\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.03397", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03397", "abs": "https://arxiv.org/abs/2512.03397", "authors": ["Seungwon Choi", "Dong-Gyu Park", "Seo-Yeon Hwang", "Tae-Wan Kim"], "title": "Surfel-LIO: Fast LiDAR-Inertial Odometry with Pre-computed Surfels and Hierarchical Z-order Voxel Hashing", "comment": null, "summary": "LiDAR-inertial odometry (LIO) is an active research area, as it enables accurate real-time state estimation in GPS-denied environments. Recent advances in map data structures and spatial indexing have significantly improved the efficiency of LIO systems. Nevertheless, we observe that two aspects may still leave room for improvement: (1) nearest neighbor search often requires examining multiple spatial units to gather sufficient points for plane fitting, and (2) plane parameters are typically recomputed at every iteration despite unchanged map geometry. Motivated by these observations, we propose Surfel-LIO, which employs a hierarchical voxel structure (hVox) with pre-computed surfel representation. This design enables O(1) correspondence retrieval without runtime neighbor enumeration or plane fitting, combined with Z-order curve encoding for cache-friendly spatial indexing. Experimental results on the M3DGR dataset demonstrate that our method achieves significantly faster processing speed compared to recent state-of-the-art methods while maintaining comparable state estimation accuracy. Our implementation is publicly available at https://github.com/93won/lidar_inertial_odometry.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684LiDAR-\u60ef\u6027\u6d4b\u7a0b\uff08LIO\uff09\u65b9\u6cd5Surfel-LIO\uff0c\u901a\u8fc7\u91c7\u7528\u5c42\u6b21\u4f53\u7d20\u7ed3\u6784\u548c\u9884\u8ba1\u7b97\u7684surfels\u8868\u793a\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u901f\u5ea6\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u6539\u5584LIO\u7cfb\u7edf\u5728\u90bb\u8fd1\u641c\u7d22\u548c\u5e73\u9762\u53c2\u6570\u91cd\u7b97\u65b9\u9762\u7684\u6548\u7387\u95ee\u9898\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u7684Surfel-LIO\u5229\u7528\u5c42\u6b21\u4f53\u7d20\u7ed3\u6784\uff08hVox\uff09\u548cZ-order\u66f2\u7ebf\u7f16\u7801\u5b9e\u73b0O(1)\u7684\u5bf9\u5e94\u83b7\u53d6\uff0c\u907f\u514d\u4e86\u8fd0\u884c\u65f6\u7684\u90bb\u8fd1\u679a\u4e3e\u548c\u5e73\u9762\u62df\u5408\u3002", "result": "\u5728M3DGR\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u7684\u5904\u7406\u901f\u5ea6\u663e\u8457\u5feb\u4e8e\u6700\u8fd1\u7684\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u7ef4\u6301\u4e86\u76f8\u5f53\u7684\u72b6\u6001\u4f30\u8ba1\u7cbe\u5ea6\u3002", "conclusion": "Surfel-LIO\u65b9\u6cd5\u5728\u4fdd\u6301\u7cbe\u786e\u7684\u72b6\u6001\u4f30\u8ba1\u7684\u540c\u65f6\uff0c\u663e\u8457\u52a0\u5feb\u4e86\u5904\u7406\u901f\u5ea6\uff0c\u5c55\u793a\u4e86\u5728GPS\u7f3a\u5931\u73af\u5883\u4e0bLIO\u7cfb\u7edf\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2512.03289", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.03289", "abs": "https://arxiv.org/abs/2512.03289", "authors": ["Aaron C Elkins", "Sanchit Singh", "Adrian Kieback", "Sawyer Blankenship", "Uyiosa Philip Amadasun", "Aman Chadha"], "title": "DAWZY: A New Addition to AI powered \"Human in the Loop\" Music Co-creation", "comment": null, "summary": "Digital Audio Workstations (DAWs) offer fine control, but mapping high-level intent (e.g., \"warm the vocals\") to low-level edits breaks creative flow. Existing artificial intelligence (AI) music generators are typically one-shot, limiting opportunities for iterative development and human contribution. We present DAWZY, an open-source assistant that turns natural-language (text/voice/hum) requests into reversible actions in REAPER. DAWZY keeps the DAW as the creative hub with a minimal GUI and voice-first interface. DAWZY uses LLM-based code generation as a novel way to significantly reduce the time users spend familiarizing themselves with large interfaces, replacing hundreds of buttons and drop-downs with a chat box. DAWZY also uses three Model Context Protocol tools for live state queries, parameter adjustment, and AI beat generation. It maintains grounding by refreshing state before mutation and ensures safety and reversibility with atomic scripts and undo. In evaluations, DAWZY performed reliably on common production tasks and was rated positively by users across Usability, Control, Learning, Collaboration, and Enjoyment.", "AI": {"tldr": "DAWZY\u662f\u4e00\u4e2a\u5f00\u6e90\u52a9\u624b\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5c06\u8bf7\u6c42\u8f6c\u5316\u4e3aREAPER\u4e2d\u7684\u53ef\u9006\u52a8\u4f5c\uff0c\u63d0\u5347\u4e86DAW\u7684\u521b\u4f5c\u4f53\u9a8c\u3002", "motivation": "\u89e3\u51b3\u9ad8\u5c42\u610f\u56fe\u4e0e\u4f4e\u5c42\u7f16\u8f91\u4e4b\u95f4\u7684\u6620\u5c04\u96be\u9898\uff0c\u51cf\u5c11\u7528\u6237\u5bf9\u590d\u6742\u754c\u9762\u7684\u719f\u6089\u65f6\u95f4\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7801\u751f\u6210\u548c\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\u5de5\u5177\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u8bf7\u6c42\u7684\u53ef\u9006\u52a8\u4f5c\u3002", "result": "DAWZY\u5728\u5e38\u89c1\u751f\u4ea7\u4efb\u52a1\u4e2d\u8868\u73b0\u53ef\u9760\uff0c\u7528\u6237\u5728\u53ef\u7528\u6027\u3001\u63a7\u5236\u6027\u3001\u5b66\u4e60\u6027\u3001\u534f\u4f5c\u6027\u548c\u4e50\u8da3\u4e0a\u7ed9\u4e88\u79ef\u6781\u8bc4\u4ef7\u3002", "conclusion": "DAWZY\u6709\u6548\u63d0\u5347\u4e86\u6570\u5b57\u97f3\u9891\u5de5\u4f5c\u7ad9 (DAW) \u7684\u521b\u4f5c\u6548\u7387\uff0c\u7528\u6237\u8bc4\u4ef7\u79ef\u6781\u3002"}}
{"id": "2512.03422", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03422", "abs": "https://arxiv.org/abs/2512.03422", "authors": ["Tianchen Deng", "Yue Pan", "Shenghai Yuan", "Dong Li", "Chen Wang", "Mingrui Li", "Long Chen", "Lihua Xie", "Danwei Wang", "Jingchuan Wang", "Javier Civera", "Hesheng Wang", "Weidong Chen"], "title": "What Is The Best 3D Scene Representation for Robotics? From Geometric to Foundation Models", "comment": null, "summary": "In this paper, we provide a comprehensive overview of existing scene representation methods for robotics, covering traditional representations such as point clouds, voxels, signed distance functions (SDF), and scene graphs, as well as more recent neural representations like Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and the emerging Foundation Models. While current SLAM and localization systems predominantly rely on sparse representations like point clouds and voxels, dense scene representations are expected to play a critical role in downstream tasks such as navigation and obstacle avoidance. Moreover, neural representations such as NeRF, 3DGS, and foundation models are well-suited for integrating high-level semantic features and language-based priors, enabling more comprehensive 3D scene understanding and embodied intelligence. In this paper, we categorized the core modules of robotics into five parts (Perception, Mapping, Localization, Navigation, Manipulation). We start by presenting the standard formulation of different scene representation methods and comparing the advantages and disadvantages of scene representation across different modules. This survey is centered around the question: What is the best 3D scene representation for robotics? We then discuss the future development trends of 3D scene representations, with a particular focus on how the 3D Foundation Model could replace current methods as the unified solution for future robotic applications. The remaining challenges in fully realizing this model are also explored. We aim to offer a valuable resource for both newcomers and experienced researchers to explore the future of 3D scene representations and their application in robotics. We have published an open-source project on GitHub and will continue to add new works and technologies to this project.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u673a\u5668\u4eba\u9886\u57df\u4e2d\u7684\u573a\u666f\u8868\u793a\u65b9\u6cd5\uff0c\u5305\u62ec\u4f20\u7edf\u7684\u70b9\u4e91\u3001\u4f53\u7d20\u53ca\u65b0\u5174\u7684\u795e\u7ecf\u8868\u793a\u3002", "motivation": "\u63a2\u8ba8\u54ea\u79cd3D\u573a\u666f\u8868\u793a\u65b9\u6cd5\u6700\u9002\u5408\u673a\u5668\u4eba\uff0c\u65e8\u5728\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u8d44\u6e90\u3002", "method": "\u6bd4\u8f83\u4e0d\u540c\u573a\u666f\u8868\u793a\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u5c06\u673a\u5668\u4eba\u6838\u5fc3\u6a21\u5757\u5206\u4e3a\u611f\u77e5\u3001\u6620\u5c04\u3001\u5b9a\u4f4d\u3001\u5bfc\u822a\u548c\u64cd\u63a7\u4e94\u4e2a\u90e8\u5206\u3002", "result": "\u5bf9\u73b0\u6709\u573a\u666f\u8868\u793a\u65b9\u6cd5\u8fdb\u884c\u5168\u9762\u6bd4\u8f83\uff0c\u5e76\u8ba8\u8bba\u672a\u6765\u7684\u53d1\u5c55\u8d8b\u52bf\u3002", "conclusion": "\u672a\u6765\u76843D\u57fa\u7840\u6a21\u578b\u6709\u6f5c\u529b\u6210\u4e3a\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u4ecd\u9762\u4e34\u4e00\u4e9b\u6311\u6218\u3002"}}
{"id": "2512.03398", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.03398", "abs": "https://arxiv.org/abs/2512.03398", "authors": ["Quan Zhou", "Cameron Cassidy", "Alyson Yin", "Stacy Branham"], "title": "Teacher, But Also Student: Challenges and Tech Needs of Adult Braille Learners with Sight", "comment": null, "summary": "Braille literacy is critical for blind individuals' independence and quality of life, yet literacy rates continue to decline. Though braille instructors in integrated K-12 classrooms play a central role in literacy development in blind youth, prior research on braille learning almost exclusively focuses on blind adolescent students. As a result, we still know little about how sighted adult teachers learn braille. To address this, we interviewed 14 educators, including 13 certificated Teachers of Students with Visual Impairments (TVIs) and 1 paraeducator, who learned braille as adults. We found that they: (1) lack consistent braille exposure to reinforce knowledge and skill; (2) have limited time to practice due to myriad responsibilities of adulthood; and thus, (3) seek learning tools that are engaging and efficient. Our research draws attention to the needs of a group of braille learners who have been overlooked and identifies new design opportunities to facilitate braille literacy.", "AI": {"tldr": "\u672c\u7814\u7a76\u5173\u6ce8\u6210\u4eba\u89c6\u89c9\u969c\u788d\u6559\u5e08\u5b66\u4e60\u76f2\u6587\u7684\u6311\u6218\uff0c\u901a\u8fc7\u8bbf\u8c08\u63ed\u793a\u4e86\u4ed6\u4eec\u9762\u4e34\u7684\u7f3a\u4e4f\u63a5\u89e6\u548c\u7ec3\u4e60\u65f6\u95f4\u7684\u95ee\u9898\uff0c\u5e76\u6307\u51fa\u8bbe\u8ba1\u6709\u6548\u5b66\u4e60\u5de5\u5177\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u5f3a\u8c03\u76f2\u6587\u8bfb\u5199\u80fd\u529b\u5bf9\u76f2\u4eba\u72ec\u7acb\u548c\u751f\u6d3b\u8d28\u91cf\u7684\u91cd\u8981\u6027\uff0c\u5e76\u6307\u51fa\u770b\u4f3c\u88ab\u5ffd\u89c6\u7684\u6210\u4eba\u76f2\u6587\u5b66\u4e60\u8005\u7684\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u5bf914\u540d\u63a5\u53d7\u8fc7\u76f2\u6587\u57f9\u8bad\u7684\u6559\u80b2\u5de5\u4f5c\u8005\u8fdb\u884c\u8bbf\u8c08\uff0c\u63a2\u8ba8\u4ed6\u4eec\u7684\u5b66\u4e60\u7ecf\u5386\u548c\u6311\u6218\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u8fd9\u4e9b\u6559\u5e08\u5728\u5b66\u4e60\u76f2\u6587\u65f6\uff0c\u7f3a\u4e4f\u4e00\u81f4\u7684\u76f2\u6587\u63a5\u89e6\u3001\u6709\u9650\u7684\u7ec3\u4e60\u65f6\u95f4\uff0c\u5e76\u5e0c\u671b\u627e\u5230\u6709\u8da3\u4e14\u9ad8\u6548\u7684\u5b66\u4e60\u5de5\u5177\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u6210\u4eba\u89c6\u89c9\u969c\u788d\u6559\u5e08\u5b66\u4e60\u76f2\u6587\u7684\u6311\u6218\uff0c\u5e76\u6307\u51fa\u4e86\u8bbe\u8ba1\u6709\u6548\u5b66\u4e60\u5de5\u5177\u7684\u673a\u4f1a\u3002"}}
{"id": "2512.03429", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03429", "abs": "https://arxiv.org/abs/2512.03429", "authors": ["Raul Steinmetz", "Fabio Demo Rosa", "Victor Augusto Kich", "Jair Augusto Bottega", "Ricardo Bedin Grando", "Daniel Fernando Tello Gamarra"], "title": "World Models for Autonomous Navigation of Terrestrial Robots from LIDAR Observations", "comment": "Accepted for publication in the Journal of Intelligent and Fuzzy Systems", "summary": "Autonomous navigation of terrestrial robots using Reinforcement Learning (RL) from LIDAR observations remains challenging due to the high dimensionality of sensor data and the sample inefficiency of model-free approaches. Conventional policy networks struggle to process full-resolution LIDAR inputs, forcing prior works to rely on simplified observations that reduce spatial awareness and navigation robustness. This paper presents a novel model-based RL framework built on top of the DreamerV3 algorithm, integrating a Multi-Layer Perceptron Variational Autoencoder (MLP-VAE) within a world model to encode high-dimensional LIDAR readings into compact latent representations. These latent features, combined with a learned dynamics predictor, enable efficient imagination-based policy optimization. Experiments on simulated TurtleBot3 navigation tasks demonstrate that the proposed architecture achieves faster convergence and higher success rate compared to model-free baselines such as SAC, DDPG, and TD3. It is worth emphasizing that the DreamerV3-based agent attains a 100% success rate across all evaluated environments when using the full dataset of the Turtlebot3 LIDAR (360 readings), while model-free methods plateaued below 85%. These findings demonstrate that integrating predictive world models with learned latent representations enables more efficient and robust navigation from high-dimensional sensory data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8eDreamerV3\u7684\u6a21\u578b\u9a71\u52a8\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u5904\u7406\u9ad8\u7ef4\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\uff0c\u5b9e\u73b0\u9ad8\u6210\u529f\u7387\u7684\u81ea\u4e3b\u5bfc\u822a\u3002", "motivation": "\u5e94\u5bf9\u9ad8\u7ef4\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\u7684\u5904\u7406\u548c\u6a21\u578b\u65e0\u5173\u65b9\u6cd5\u7684\u6837\u672c\u6548\u7387\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u9c81\u68d2\u7684\u81ea\u4e3b\u5bfc\u822a\u80fd\u529b", "method": "\u57fa\u4e8eDreamerV3\u7b97\u6cd5\u7684\u6a21\u578b\u9a71\u52a8\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u5c42\u611f\u77e5\u5668\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08MLP-VAE\uff09\u548c\u5b66\u4e60\u7684\u52a8\u6001\u9884\u6d4b\u5668", "result": "\u5728\u6a21\u62dfTurtleBot3\u5bfc\u822a\u4efb\u52a1\u4e2d\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u6bd4\u6a21\u578b\u65e0\u5173\u57fa\u7ebf\uff08\u5982SAC\uff0cDDPG\u548cTD3\uff09\u6536\u655b\u66f4\u5feb\uff0c\u6210\u529f\u7387\u66f4\u9ad8\uff0c\u800cDreamerV3\u4ee3\u7406\u5728360\u4e2a\u6fc0\u5149\u96f7\u8fbe\u8bfb\u53d6\u6570\u636e\u4e0a\u53d6\u5f97100%\u7684\u6210\u529f\u7387", "conclusion": "\u5c06\u9884\u6d4b\u4e16\u754c\u6a21\u578b\u4e0e\u5b66\u4e60\u7684\u6f5c\u5728\u8868\u793a\u7ed3\u5408\u80fd\u591f\u6709\u6548\u63d0\u5347\u6765\u81ea\u9ad8\u7ef4\u4f20\u611f\u5668\u6570\u636e\u7684\u5bfc\u822a\u6027\u80fd\uff0c\u786e\u4fdd\u4e86\u5728\u5404\u4e2a\u8bc4\u4f30\u73af\u5883\u4e2d\u7684\u9ad8\u6210\u529f\u7387\u3002"}}
{"id": "2512.03406", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.03406", "abs": "https://arxiv.org/abs/2512.03406", "authors": ["Junsang Park", "Sarah Brown", "Sharon Lynn Chu"], "title": "Why Some Seek AI, Others Seek Therapists: Mental Health in the Age of Generative AI", "comment": null, "summary": "As generative artificial intelligence (GAI) enters the mental health landscape, questions arise about how individuals weigh AI tools against human therapists. Drawing on the Health Belief Model (HBM), this study examined belief-based predictors of intention to use GAI and therapists across two populations: a university sample (N = 1,155) and a nationally representative adult sample (N = 651). Using repeated-measures ANOVA and LASSO regression, we found that therapists were consistently valued for emotional, relational, and personalization benefits, while GAI was favored for accessibility and affordability. Yet structural advantages alone did not predict adoption; emotional benefit and personalization emerged as decisive factors. Adoption patterns diverged across groups: students treated GAI as a complement, whereas national adults approached it as a substitute. Concerns about privacy and reliability constrained GAI use in both groups. These findings extend HBM to multi-modality contexts and highlight design implications for trustworthy, emotionally resonant digital mental health tools.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u4e2d\u4e2a\u4f53\u5bf9GAI\u548c\u6cbb\u7597\u5e08\u7684\u4f7f\u7528\u610f\u56fe\uff0c\u53d1\u73b0\u60c5\u611f\u5229\u76ca\u548c\u4e2a\u6027\u5316\u662f\u4f7f\u7528GAI\u7684\u5173\u952e\u56e0\u7d20\uff0c\u91c7\u7528\u6a21\u5f0f\u56e0\u7fa4\u4f53\u800c\u5f02\u3002", "motivation": "\u63a2\u8ba8\u5728\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u4e2d\uff0c\u4e2a\u4f53\u5982\u4f55\u5e73\u8861\u4f7f\u7528\u751f\u6210\u6027\u4eba\u5de5\u667a\u80fd\u5de5\u5177\u4e0e\u4eba\u7c7b\u6cbb\u7597\u5e08\u3002", "method": "\u901a\u8fc7\u5bf9\u5927\u5b66\u6837\u672c(N=1,155)\u548c\u5168\u56fd\u4ee3\u8868\u6027\u6210\u4eba\u6837\u672c(N=651)\u7684\u7814\u7a76\uff0c\u91c7\u7528\u91cd\u590d\u6d4b\u91cfANOVA\u548cLASSO\u56de\u5f52\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u6cbb\u7597\u5e08\u5728\u60c5\u611f\u3001\u5173\u7cfb\u548c\u4e2a\u6027\u5316\u5229\u76ca\u65b9\u9762\u53d7\u5230\u91cd\u89c6\uff0c\u800cGAI\u5728\u53ef\u83b7\u53d6\u6027\u548c\u7ecf\u6d4e\u6027\u4e0a\u66f4\u5177\u4f18\u52bf\u3002", "conclusion": "\u7814\u7a76\u6269\u5c55\u4e86\u5065\u5eb7\u4fe1\u5ff5\u6a21\u578b(HBM)\u5728\u591a\u6a21\u6001\u80cc\u666f\u4e0b\u7684\u5e94\u7528\uff0c\u5f3a\u8c03\u4e86\u8bbe\u8ba1\u53ef\u4fe1\u3001\u60c5\u611f\u5171\u9e23\u7684\u6570\u5b57\u5fc3\u7406\u5065\u5eb7\u5de5\u5177\u7684\u610f\u4e49\u3002"}}
{"id": "2512.03444", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.03444", "abs": "https://arxiv.org/abs/2512.03444", "authors": ["Davood Soleymanzadeh", "Xiao Liang", "Minghui Zheng"], "title": "PerFACT: Motion Policy with LLM-Powered Dataset Synthesis and Fusion Action-Chunking Transformers", "comment": null, "summary": "Deep learning methods have significantly enhanced motion planning for robotic manipulators by leveraging prior experiences within planning datasets. However, state-of-the-art neural motion planners are primarily trained on small datasets collected in manually generated workspaces, limiting their generalizability to out-of-distribution scenarios. Additionally, these planners often rely on monolithic network architectures that struggle to encode critical planning information. To address these challenges, we introduce Motion Policy with Dataset Synthesis powered by large language models (LLMs) and Fusion Action-Chunking Transformers (PerFACT), which incorporates two key components. Firstly, a novel LLM-powered workspace generation method, MotionGeneralizer, enables large-scale planning data collection by producing a diverse set of semantically feasible workspaces. Secondly, we introduce Fusion Motion Policy Networks (MpiNetsFusion), a generalist neural motion planner that uses a fusion action-chunking transformer to better encode planning signals and attend to multiple feature modalities. Leveraging MotionGeneralizer, we collect 3.5M trajectories to train and evaluate MpiNetsFusion against state-of-the-art planners, which shows that the proposed MpiNetsFusion can plan several times faster on the evaluated tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u591a\u6837\u5de5\u4f5c\u7a7a\u95f4\uff0c\u5e76\u5f15\u5165\u878d\u5408\u52a8\u4f5c\u5757\u53d8\u6362\u5668\u63d0\u5347\u89c4\u5212\u901f\u5ea6\u548c\u7cbe\u5ea6\uff0c\u5b9e\u9a8c\u8868\u660e\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u89c4\u5212\u901f\u5ea6\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u63d0\u5347\u673a\u5668\u4eba\u624b\u81c2\u7684\u8fd0\u52a8\u89c4\u5212\u6548\u7387\uff0c\u89e3\u51b3\u73b0\u6709\u795e\u7ecf\u8fd0\u52a8\u89c4\u5212\u5668\u5728\u5c0f\u6570\u636e\u96c6\u548c\u5355\u4e00\u7f51\u7edc\u67b6\u6784\u4e0b\u7684\u5c40\u9650\u6027\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(MotionGeneralizer)\u8fdb\u884c\u5de5\u4f5c\u7a7a\u95f4\u751f\u6210\uff0c\u5e76\u901a\u8fc7Fusion Motion Policy Networks (MpiNetsFusion)\u63d0\u5347\u89c4\u5212\u4fe1\u53f7\u7f16\u7801\u548c\u591a\u7279\u5f81\u6a21\u6001\u7684\u5173\u6ce8\u3002", "result": "\u63d0\u51fa\u4e86Motion Policy\u4e0e\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548cFusion Action-Chunking Transformers\uff08PerFACT\uff09\u9a71\u52a8\u7684\u6570\u636e\u96c6\u751f\u6210\u65b9\u6cd5\uff0c\u4ece\u800c\u589e\u5f3a\u8fd0\u52a8\u89c4\u5212\u80fd\u529b\u3002", "conclusion": "MpiNetsFusion \u6218\u80dc\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u89c4\u5212\u5668\uff0c\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u66f4\u5feb\u7684\u89c4\u5212\u901f\u5ea6\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2512.03485", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.03485", "abs": "https://arxiv.org/abs/2512.03485", "authors": ["Rui Sheng", "Zelin Zang", "Jiachen Wang", "Yan Luo", "Zixin Chen", "Yan Zhou", "Shaolun Ruan", "Huamin Qu"], "title": "CellScout: Visual Analytics for Mining Biomarkers in Cell State Discovery", "comment": null, "summary": "Cell state discovery is crucial for understanding biological systems and enhancing medical outcomes. A key aspect of this process is identifying distinct biomarkers that define specific cell states. However, difficulties arise from the co-discovery process of cell states and biomarkers: biologists often use dimensionality reduction to visualize cells in a two- dimensional space. Then they usually interpret visually clustered cells as distinct states, from which they seek to identify unique biomarkers. However, this assumption is often invalid due to internal inconsistencies in a cluster, making the process trial-and-error and highly uncertain. Therefore, biologists urgently need effective tools to help uncover the hidden association relationships between different cell populations and their potential biomarkers. To address this problem, we first designed a machine-learning algorithm based on the Mixture-of-Experts (MoE) technique to identify meaningful associations between cell populations and biomarkers. We further developed a visual analytics system, CellScout, in collaboration with biologists, to help them explore and refine these association relationships to advance cell state discovery. We validated our system through expert interviews, from which we further selected a representative case to demonstrate its effectiveness in discovering new cell states.", "AI": {"tldr": "\u672c\u8bba\u6587\u8bbe\u8ba1\u4e86\u4e00\u79cd\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u548c\u4e00\u4e2a\u89c6\u89c9\u5206\u6790\u7cfb\u7edf\u4ee5\u5e2e\u52a9\u751f\u7269\u5b66\u5bb6\u6709\u6548\u53d1\u73b0\u7ec6\u80de\u72b6\u6001\u53ca\u5176\u751f\u7269\u6807\u5fd7\u7269\u3002", "motivation": "\u7ec6\u80de\u72b6\u6001\u7684\u53d1\u73b0\u5bf9\u7406\u89e3\u751f\u7269\u7cfb\u7edf\u548c\u6539\u5584\u533b\u7597\u7ed3\u679c\u81f3\u5173\u91cd\u8981\uff0c\u800c\u8bc6\u522b\u7279\u5b9a\u7ec6\u80de\u72b6\u6001\u7684\u72ec\u7279\u751f\u7269\u6807\u5fd7\u7269\u662f\u8fd9\u4e00\u8fc7\u7a0b\u7684\u5173\u952e\u3002", "method": "\u9996\u5148\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8eMixture-of-Experts\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u7136\u540e\u5f00\u53d1\u4e86\u540d\u4e3aCellScout\u7684\u89c6\u89c9\u5206\u6790\u7cfb\u7edf\u3002", "result": "\u901a\u8fc7\u8bbe\u8ba1\u57fa\u4e8eMixture-of-Experts\u6280\u672f\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u548c\u5f00\u53d1\u540d\u4e3aCellScout\u7684\u89c6\u89c9\u5206\u6790\u7cfb\u7edf\uff0c\u6211\u4eec\u6210\u529f\u5730\u63ed\u793a\u4e86\u7ec6\u80de\u7fa4\u4f53\u548c\u751f\u7269\u6807\u5fd7\u7269\u4e4b\u95f4\u7684\u5173\u8054\u5173\u7cfb\uff0c\u5e76\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cCellScout\u7cfb\u7edf\u5728\u53d1\u73b0\u65b0\u7684\u7ec6\u80de\u72b6\u6001\u65b9\u9762\u6709\u6548\uff0c\u5e76\u80fd\u5e2e\u52a9\u751f\u7269\u5b66\u5bb6\u63a2\u7d22\u548c\u7ec6\u5316\u7ec6\u80de\u7fa4\u4f53\u4e0e\u751f\u7269\u6807\u5fd7\u7269\u4e4b\u95f4\u7684\u5173\u8054\u5173\u7cfb\u3002"}}
{"id": "2512.03522", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03522", "abs": "https://arxiv.org/abs/2512.03522", "authors": ["Gihyeon Lee", "Jungwoo Lee", "Juwon Kim", "Young-Sik Shin", "Younggun Cho"], "title": "MSG-Loc: Multi-Label Likelihood-based Semantic Graph Matching for Object-Level Global Localization", "comment": "Accepted in IEEE Robotics and Automation Letters (2025)", "summary": "Robots are often required to localize in environments with unknown object classes and semantic ambiguity. However, when performing global localization using semantic objects, high semantic ambiguity intensifies object misclassification and increases the likelihood of incorrect associations, which in turn can cause significant errors in the estimated pose. Thus, in this letter, we propose a multi-label likelihood-based semantic graph matching framework for object-level global localization. The key idea is to exploit multi-label graph representations, rather than single-label alternatives, to capture and leverage the inherent semantic context of object observations. Based on these representations, our approach enhances semantic correspondence across graphs by combining the likelihood of each node with the maximum likelihood of its neighbors via context-aware likelihood propagation. For rigorous validation, data association and pose estimation performance are evaluated under both closed-set and open-set detection configurations. In addition, we demonstrate the scalability of our approach to large-vocabulary object categories in both real-world indoor scenes and synthetic environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6807\u7b7e\u4f3c\u7136\u8bed\u4e49\u56fe\u5339\u914d\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u9ad8\u8bed\u4e49\u6b67\u4e49\u73af\u5883\u4e2d\u7684\u5168\u5c40\u5b9a\u4f4d\u6027\u80fd\u3002", "motivation": "\u5728\u9ad8\u8bed\u4e49\u6b67\u4e49\u7684\u73af\u5883\u4e2d\uff0c\u7269\u4f53\u7684\u8bef\u5206\u7c7b\u548c\u4e0d\u6b63\u786e\u5173\u8054\u53ef\u80fd\u5bfc\u81f4\u673a\u5668\u4eba\u5b9a\u4f4d\u9519\u8bef\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u6709\u6548\u7684\u5168\u5c40\u5b9a\u4f4d\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6807\u7b7e\u4f3c\u7136\u7684\u8bed\u4e49\u56fe\u5339\u914d\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u8282\u70b9\u7684\u4f3c\u7136\u6027\u4e0e\u90bb\u5c45\u8282\u70b9\u7684\u6700\u5927\u4f3c\u7136\u6027\u7ed3\u5408\u8fdb\u884c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4f3c\u7136\u4f20\u64ad\u3002", "result": "\u5229\u7528\u95ed\u96c6\u548c\u5f00\u96c6\u68c0\u6d4b\u914d\u7f6e\u5bf9\u6570\u636e\u5173\u8054\u548c\u4f4d\u59ff\u4f30\u8ba1\u6027\u80fd\u8fdb\u884c\u8bc4\u4f30\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5927\u578b\u8bcd\u6c47\u7269\u4f53\u5206\u7c7b\u4e2d\u5177\u6709\u826f\u597d\u7684\u6269\u5c55\u6027\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u591a\u6807\u7b7e\u56fe\u5339\u914d\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u672a\u77e5\u7c7b\u7269\u4f53\u73af\u5883\u4e2d\u7684\u5168\u5c40\u5b9a\u4f4d\u7cbe\u5ea6\u3002"}}
{"id": "2512.03495", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.03495", "abs": "https://arxiv.org/abs/2512.03495", "authors": ["Rui Sheng", "Yifang Wang", "Xingbo Wang", "Shun Dai", "Qingyu Guo", "Tai-Quan Peng", "Huamin Qu", "Dongyu Liu"], "title": "EMINDS: Understanding User Behavior Progression for Mental Health Exploration on Social Media", "comment": null, "summary": "Mental health is an urgent societal issue, and social scientists are increasingly turning to online mental health communities (OMHCs) to analyze user behavior data for early intervention. However, existing sequence mining techniques fall short of the urgent need to explore the behavior progression of different groups (e.g., recovery or deterioration groups) and track the potential long-term impact of behaviors on mental health status. To address this issue, we introduce EMINDS, a visual analytics system built on a novel automatic mining pipeline that extracts distinct behavior stages and assesses the potential impact of frequent stage patterns on mental health status over time. The system includes a set of interactive visualizations that summarize the meaning of each behavior stage and the evolution of different stage patterns. We feature a pattern-centric Sankey diagram to reveal contextual information about the impact of stage patterns on mental health, helping experts understand the specific changes in sequences before and after a stage pattern. We evaluated the effectiveness and usability of EMINDS through two case studies and expert interviews, which examined the potential stage patterns impacting long-term mental health by analyzing user behaviors on Reddit.", "AI": {"tldr": "\u672c\u8bba\u6587\u4ecb\u7ecd\u4e86EMINDS\uff0c\u4e00\u4e2a\u7528\u4e8e\u5206\u6790\u5728\u7ebf\u5fc3\u7406\u5065\u5eb7\u793e\u533a\u7528\u6237\u884c\u4e3a\u7684\u53ef\u89c6\u5316\u5206\u6790\u7cfb\u7edf\uff0c\u65e8\u5728\u8bc6\u522b\u4e0d\u540c\u884c\u4e3a\u9636\u6bb5\u53ca\u5176\u5bf9\u5fc3\u7406\u5065\u5eb7\u7684\u5f71\u54cd\u3002", "motivation": "\u9488\u5bf9\u5fc3\u7406\u5065\u5eb7\u65e5\u76ca\u4e25\u91cd\u7684\u793e\u4f1a\u95ee\u9898\uff0c\u7814\u7a76\u4eba\u5458\u5e0c\u671b\u901a\u8fc7\u5728\u7ebf\u5fc3\u7406\u5065\u5eb7\u793e\u533a\u7684\u6570\u636e\u5206\u6790\u5b9e\u73b0\u65e9\u671f\u5e72\u9884\u3002", "method": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u7684\u81ea\u52a8\u6316\u6398\u6d41\u7a0b\uff0c\u5229\u7528\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u5c55\u793a\u548cSankey\u56fe\u8868\u6765\u5206\u6790\u7528\u6237\u884c\u4e3a\u53d8\u5316\u548c\u9636\u6bb5\u6a21\u5f0f\u7684\u957f\u671f\u5f71\u54cd\u3002", "result": "\u63d0\u51fa\u4e86EMINDS\uff0c\u4e00\u4e2a\u65b0\u7684\u53ef\u89c6\u5206\u6790\u7cfb\u7edf\uff0c\u80fd\u591f\u63d0\u53d6\u4e0d\u540c\u884c\u4e3a\u9636\u6bb5\u5e76\u8bc4\u4f30\u884c\u4e3a\u6a21\u5f0f\u5bf9\u5fc3\u7406\u5065\u5eb7\u7684\u6f5c\u5728\u957f\u671f\u5f71\u54cd\u3002", "conclusion": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u548c\u4e13\u5bb6\u8bbf\u8c08\u8bc4\u4f30\u4e86EMINDS\u7684\u6709\u6548\u6027\u548c\u53ef\u7528\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4e86\u89e3\u7528\u6237\u884c\u4e3a\u5bf9\u957f\u671f\u5fc3\u7406\u5065\u5eb7\u5f71\u54cd\u65b9\u9762\u7684\u5e94\u7528\u3002"}}
{"id": "2512.03538", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03538", "abs": "https://arxiv.org/abs/2512.03538", "authors": ["Yuhang Huang", "Shilong Zou", "Jiazhao Zhang", "Xinwang Liu", "Ruizhen Hu", "Kai Xu"], "title": "AdaPower: Specializing World Foundation Models for Predictive Manipulation", "comment": null, "summary": "World Foundation Models (WFMs) offer remarkable visual dynamics simulation capabilities, yet their application to precise robotic control remains limited by the gap between generative realism and control-oriented precision. While existing approaches use WFMs as synthetic data generators, they suffer from high computational costs and underutilization of pre-trained VLA policies. We introduce \\textbf{AdaPower} (\\textbf{Ada}pt and Em\\textbf{power}), a lightweight adaptation framework that transforms general-purpose WFMs into specialist world models through two novel components: Temporal-Spatial Test-Time Training (TS-TTT) for inference-time adaptation and Memory Persistence (MP) for long-horizon consistency. Integrated within a Model Predictive Control framework, our adapted world model empowers pre-trained VLAs, achieving over 41\\% improvement in task success rates on LIBERO benchmarks without policy retraining, while preserving computational efficiency and generalist capabilities.", "AI": {"tldr": "AdaPower\u6846\u67b6\u901a\u8fc7\u4e24\u4e2a\u65b0\u9896\u7684\u7ec4\u4ef6\u6539\u5584WFM\u5728\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u5e94\u7528\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u7684\u4efb\u52a1\u6210\u529f\u7387\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3WFMs\u5728\u7cbe\u51c6\u673a\u5668\u4eba\u63a7\u5236\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u65e8\u5728\u63d0\u5347\u63a7\u5236\u7cbe\u5ea6\u548c\u5229\u7528\u7387\u3002", "method": "\u91c7\u7528\u4e86\u65f6\u95f4-\u7a7a\u95f4\u6d4b\u8bd5\u65f6\u8bad\u7ec3\uff08TS-TTT\uff09\u548c\u8bb0\u5fc6\u6301\u7eed\u6027\uff08MP\uff09\u4e24\u4e2a\u65b0\u673a\u5236\uff0c\u96c6\u6210\u4e8e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\u4e2d\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAdaPower\u7684\u8f7b\u91cf\u7ea7\u9002\u5e94\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u901a\u7528\u7684\u4e16\u754c\u57fa\u7840\u6a21\u578b\uff08WFM\uff09\u8f6c\u53d8\u4e3a\u4e13\u95e8\u7684\u4e16\u754c\u6a21\u578b\uff0c\u6539\u5584\u4e86\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u751f\u6210\u73b0\u5b9e\u548c\u63a7\u5236\u7cbe\u5ea6\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "conclusion": "AdaPower\u4f7f\u9884\u8bad\u7ec3\u7684VLAs\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad8\u8d85\u8fc741%\uff0c\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7b56\u7565\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u4e0e\u901a\u7528\u6027\u3002"}}
{"id": "2512.03519", "categories": ["cs.HC", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.03519", "abs": "https://arxiv.org/abs/2512.03519", "authors": ["Ben Larwood", "Oliver J. Sutton", "Callum Cockburn"], "title": "Left shifting analysis of Human-Autonomous Team interactions to analyse risks of autonomy in high-stakes AI systems", "comment": "Published in: IfSE Annual Systems Engineering Conference Proceedings 2025", "summary": "Developing high-stakes autonomous systems that include Artificial Intelligence (AI) components is complex; the consequences of errors can be catastrophic, yet it is challenging to plan for all operational cases. In stressful scenarios for the human operator, such as short decision-making timescales, the risk of failures is exacerbated. A lack of understanding of AI failure modes obstructs this and so blocks the robust implementation of applications of AI in smart systems. This prevents early risk identification, leading to increased time, risk and cost of projects.\n  A key tenet of Systems Engineering and acquisition engineering is centred around a \"left-shift\" in test and evaluation activities to earlier in the system lifecycle, to allow for \"accelerated delivery of [systems] that work\". We argue it is therefore essential that this shift includes the analysis of AI failure cases as part of the design stages of the system life cycle. Our proposed framework enables the early characterisation of risks emerging from human-autonomy teaming (HAT) in operational contexts. The cornerstone of this is a new analysis of AI failure modes, built on the seminal modelling of human-autonomy teams laid out by LaMonica et al., 2022. Using the analysis of the interactions between human and autonomous systems and exploring the failure modes within each aspect, our approach provides a way to systematically identify human-AI interactions risks across the operational domain of the system of interest. The understanding of the emergent behaviour enables increased robustness of the system, for which the analysis should be undertaken over the whole scope of its operational design domain. This approach is illustrated through an example use case for an AI assistant supporting a Command & Control (C2) System.", "AI": {"tldr": "\u4e3a\u4e86\u52a0\u5f3aAI\u5728\u9ad8\u98ce\u9669\u81ea\u4e3b\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8bbe\u8ba1\u9636\u6bb5\u53ca\u65e9\u5206\u6790AI\u5931\u8d25\u6a21\u5f0f\u548c\u76f8\u5173\u98ce\u9669\u3002", "motivation": "\u5f00\u53d1\u9ad8\u98ce\u9669\u81ea\u4e3b\u7cfb\u7edf\u7684\u590d\u6742\u6027\u4e0e\u4eba\u4e3a\u64cd\u4f5c\u5458\u5728\u7d27\u5f20\u51b3\u7b56\u65f6\u7684\u98ce\u9669\u76f8\u5173\uff0c\u7f3a\u4e4f\u5bf9AI\u5931\u8d25\u6a21\u5f0f\u7684\u7406\u89e3\u59a8\u788d\u4e86AI\u5728\u667a\u80fd\u7cfb\u7edf\u4e2d\u7684\u5f3a\u5065\u5b9e\u65bd\u3002", "method": "\u57fa\u4e8eLaMonica et al. (2022)\u7684\u4eba\u673a\u81ea\u4e3b\u56e2\u961f\u5efa\u6a21\uff0c\u7cfb\u7edf\u6027\u5730\u5206\u6790\u4eba\u7c7b\u4e0e\u81ea\u4e3b\u7cfb\u7edf\u4e4b\u95f4\u7684\u4ea4\u4e92\uff0c\u8bc6\u522b\u6240\u6709\u64cd\u4f5c\u57df\u5185\u7684\u98ce\u9669\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5728\u7cfb\u7edf\u751f\u547d\u5468\u671f\u7684\u8bbe\u8ba1\u9636\u6bb5\u65e9\u671f\u8bc6\u522b\u4eba\u673a\u534f\u4f5c\u4e2d\u7684AI\u5931\u8d25\u6a21\u5f0f\u53ca\u76f8\u5173\u98ce\u9669\u3002", "conclusion": "\u901a\u8fc7\u7406\u89e3\u65b0\u5174\u884c\u4e3a\uff0c\u53ef\u4ee5\u63d0\u9ad8\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\uff0c\u786e\u4fdd\u5728\u6574\u4e2a\u64cd\u4f5c\u8bbe\u8ba1\u9886\u57df\u8fdb\u884c\u5168\u9762\u7684\u5206\u6790\u3002"}}
{"id": "2512.03548", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03548", "abs": "https://arxiv.org/abs/2512.03548", "authors": ["Zexin Lin", "Yebin Zhong", "Hanwen Wan", "Jiu Cheng", "Zhenglong Sun", "Xiaoqiang Ji"], "title": "A Learning-based Control Methodology for Transitioning VTOL UAVs", "comment": null, "summary": "Transition control poses a critical challenge in Vertical Take-Off and Landing Unmanned Aerial Vehicle (VTOL UAV) development due to the tilting rotor mechanism, which shifts the center of gravity and thrust direction during transitions. Current control methods' decoupled control of altitude and position leads to significant vibration, and limits interaction consideration and adaptability. In this study, we propose a novel coupled transition control methodology based on reinforcement learning (RL) driven controller. Besides, contrasting to the conventional phase-transition approach, the ST3M method demonstrates a new perspective by treating cruise mode as a special case of hover. We validate the feasibility of applying our method in simulation and real-world environments, demonstrating efficient controller development and migration while accurately controlling UAV position and attitude, exhibiting outstanding trajectory tracking and reduced vibrations during the transition process.", "AI": {"tldr": "\u4e3aVTOL\u65e0\u4eba\u673a\u8fc7\u6e21\u63a7\u5236\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8026\u5408\u65b9\u6cd5\uff0c\u6709\u6548\u51cf\u5c11\u632f\u52a8\u5e76\u63d0\u5347\u8f68\u8ff9\u8ddf\u8e2a\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3VTOL\u65e0\u4eba\u673a\u5728\u8fc7\u6e21\u63a7\u5236\u4e2d\u7684\u632f\u52a8\u95ee\u9898\u53ca\u9650\u4e8e\u73b0\u6709\u63a7\u5236\u65b9\u6cd5\u7684\u9002\u5e94\u6027\u4e0d\u8db3\u3002", "method": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8026\u5408\u8fc7\u6e21\u63a7\u5236\u65b9\u6cd5\uff0c\u5e94\u7528\u4e8eVTOL\u65e0\u4eba\u673a\u7684\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u3002", "result": "\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u63a7\u5236\u5668\u5f00\u53d1\u548c\u8fc1\u79fb\uff0c\u51c6\u786e\u63a7\u5236\u65e0\u4eba\u673a\u7684\u4f4d\u7f6e\u548c\u59ff\u6001\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8026\u5408\u8fc7\u6e21\u63a7\u5236\u65b9\u6cd5\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u8fc7\u6e21\u8fc7\u7a0b\u4e2d\u7684\u632f\u52a8\u5e76\u63d0\u5347\u4e86\u8f68\u8ff9\u8ddf\u8e2a\u80fd\u529b\u3002"}}
{"id": "2512.03568", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.03568", "abs": "https://arxiv.org/abs/2512.03568", "authors": ["Ruican Zhong", "David W. McDonald", "Gary Hsieh"], "title": "Synthetic Cognitive Walkthrough: Aligning Large Language Model Performance with Human Cognitive Walkthrough", "comment": null, "summary": "Conducting usability testing like cognitive walkthrough (CW) can be costly. Recent developments in large language models (LLMs), with visual reasoning and UI navigation capabilities, present opportunities to automate CW. We explored whether LLMs (GPT-4 and Gemini-2.5-pro) can simulate human behavior in CW by comparing their walkthroughs with human participants. While LLMs could navigate interfaces and provide reasonable rationales, their behavior differed from humans. LLM-prompted CW achieved higher task completion rates than humans and followed more optimal navigation paths, while identifying fewer potential failure points. However, follow-up studies demonstrated that with additional prompting, LLMs can predict human-identified failure points, aligning their performance with human participants. Our work highlights that while LLMs may not replicate human behaviors exactly, they can be leveraged for scaling usability walkthroughs and providing UI insights, offering a valuable complement to traditional usability testing.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u53ef\u7528\u6027\u8d70\u67e5\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u4efb\u52a1\u5b8c\u6210\u7387\u548c\u5bfc\u822a\u8def\u5f84\u4e0a\u4f18\u4e8e\u4eba\u7c7b\uff0c\u4f46\u5728\u6f5c\u5728\u5931\u8d25\u70b9\u8bc6\u522b\u65b9\u9762\u8868\u73b0\u6b20\u4f73\uff0c\u7136\u800c\u7ecf\u8fc7\u63d0\u793a\u540e\u53ef\u6539\u5584\u8868\u73b0\u3002", "motivation": "\u5bfb\u627e\u964d\u4f4e\u53ef\u7528\u6027\u6d4b\u8bd5\u6210\u672c\u7684\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5229\u7528LLMs\u7684\u89c6\u89c9\u63a8\u7406\u548c\u7528\u6237\u754c\u9762\u5bfc\u822a\u80fd\u529b\u3002", "method": "\u6bd4\u8f83GPT-4\u548cGemini-2.5-pro\u7b49LLMs\u4e0e\u4eba\u7c7b\u53c2\u4e0e\u8005\u5728\u8ba4\u77e5\u8d70\u67e5\uff08CW\uff09\u4e2d\u7684\u8868\u73b0\u3002", "result": "LLMs\u5728\u63a5\u53e3\u5bfc\u822a\u548c\u4efb\u52a1\u5b8c\u6210\u7387\u4e0a\u4f18\u4e8e\u4eba\u7c7b\uff0c\u4f46\u5728\u8bc6\u522b\u6f5c\u5728\u5931\u8d25\u70b9\u65f6\u8868\u73b0\u4e0d\u5982\u4eba\u7c7b\uff0c\u7ecf\u8fc7\u989d\u5916\u63d0\u793a\u540e\uff0cLLMs\u80fd\u591f\u9884\u6d4b\u4eba\u7c7b\u8bc6\u522b\u7684\u5931\u8d25\u70b9\u3002", "conclusion": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0d\u80fd\u5b8c\u5168\u590d\u5236\u4eba\u7c7b\u7684\u884c\u4e3a\uff0c\u4f46\u5b83\u4eec\u53ef\u4ee5\u5728\u53ef\u7528\u6027\u6d4b\u8bd5\u4e2d\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u8865\u5145\uff0c\u5c24\u5176\u662f\u5728\u81ea\u52a8\u5316\u548c\u89c4\u6a21\u5316\u53ef\u7528\u6027\u8d70\u67e5\u65b9\u9762\u3002"}}
{"id": "2512.03556", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03556", "abs": "https://arxiv.org/abs/2512.03556", "authors": ["Yinzhou Tang", "Yu Shang", "Yinuo Chen", "Bingwen Wei", "Xin Zhang", "Shu'ang Yu", "Liangzhi Shi", "Chao Yu", "Chen Gao", "Wei Wu", "Yong Li"], "title": "RoboScape-R: Unified Reward-Observation World Models for Generalizable Robotics Training via RL", "comment": null, "summary": "Achieving generalizable embodied policies remains a key challenge. Traditional policy learning paradigms, including both Imitation Learning (IL) and Reinforcement Learning (RL), struggle to cultivate generalizability across diverse scenarios. While IL policies often overfit to specific expert trajectories, RL suffers from the inherent lack of a unified and general reward signal necessary for effective multi-scene generalization. We posit that the world model is uniquely capable of serving as a universal environment proxy to address this limitation. However, current world models primarily focus on their ability to predict observations and still rely on task-specific, handcrafted reward functions, thereby failing to provide a truly general training environment. Toward this problem, we propose RoboScape-R, a framework leveraging the world model to serve as a versatile, general-purpose proxy for the embodied environment within the RL paradigm. We introduce a novel world model-based general reward mechanism that generates ''endogenous'' rewards derived from the model's intrinsic understanding of real-world state transition dynamics. Extensive experiments demonstrate that RoboScape-R effectively addresses the limitations of traditional RL methods by providing an efficient and general training environment that substantially enhances the generalization capability of embodied policies. Our approach offers critical insights into utilizing the world model as an online training strategy and achieves an average 37.5% performance improvement over baselines under out-of-domain scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRoboScape-R\u6846\u67b6\uff0c\u5229\u7528\u4e16\u754c\u6a21\u578b\u89e3\u51b3\u4f20\u7edfRL\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u589e\u5f3a\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u6a21\u4eff\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u7b56\u7565\u7684\u901a\u7528\u6027\uff0c\u5c24\u5176\u5728\u591a\u573a\u666f\u4e2d\u7f3a\u4e4f\u7edf\u4e00\u7684\u5956\u52b1\u4fe1\u53f7\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aRoboScape-R\u7684\u6846\u67b6\uff0c\u5229\u7528\u4e16\u754c\u6a21\u578b\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u901a\u7528\u4ee3\u7406\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u4e16\u754c\u6a21\u578b\u7684\u5956\u52b1\u673a\u5236\u3002", "result": "RoboScape-R\u901a\u8fc7\u751f\u6210\u5185\u751f\u5956\u52b1\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5d4c\u5165\u5f0f\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u57df\u5916\u573a\u666f\u4e0b\u5e73\u5747\u6027\u80fd\u63d0\u9ad8\u4e8637.5%\u3002", "conclusion": "RoboScape-R\u6709\u6548\u5229\u7528\u4e16\u754c\u6a21\u578b\u4f5c\u4e3a\u5728\u7ebf\u8bad\u7ec3\u7b56\u7565\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u901a\u7528\u7684\u8bad\u7ec3\u73af\u5883\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5d4c\u5165\u5f0f\u653f\u7b56\u7684\u8868\u73b0\u3002"}}
{"id": "2512.03636", "categories": ["cs.HC", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.03636", "abs": "https://arxiv.org/abs/2512.03636", "authors": ["\u013dubo\u0161 Hl\u00e1dek", "Bernhard U. Seeber"], "title": "Head, posture, and full-body gestures in interactive communication", "comment": "7 figures, 10 tables, 30 pages", "summary": "When face-to-face communication becomes effortful due to background noise or interfering talkers, the role of visual cues becomes increasingly important for communication success. While previous research has selectively examined head or hand movements, here we explore movements of the whole body in acoustically adverse conditions. We hypothesized that increasing background noise in conversations would lead to increased gesture frequency in hand, head, trunk, and leg movements typical of conversation. Increased use of hand movements should support the speaker's role, while increased head and trunk movements may help the listener. We conducted a free dyadic conversation experiment with normal-hearing participants (n=8) in a virtual acoustic environment. Conversational movements were described with a newly developed labeling system for typical conversational actions, and the frequency of individual types was analyzed. In addition, we analyzed gesture quality by assessing hand-speech synchrony, with the hypothesis that higher levels of background noise would lead to a loss of synchrony according to an interactive coupling model. Higher noise levels led to increased hand-gesture complexity during speaking and listening, more pronounced up-down head movements, and contrary to expectations, head movements during listening generally decreased relative to speaking. Synchrony and peak velocity were unaffected by noise, while gesture quality scaled only modestly. The results support previous findings regarding gesturing frequency, but we found only limited evidence for changes in speech-gesture synchrony. This work reveals communication patterns of the whole body and illustrates multimodal adaptation to communication demands.", "AI": {"tldr": "\u5728\u566a\u58f0\u6761\u4ef6\u4e0b\u8fdb\u884c\u7684\u5bf9\u8bdd\u5b9e\u9a8c\u8868\u660e\uff0c\u80cc\u666f\u566a\u58f0\u589e\u52a0\u4f1a\u5bfc\u81f4\u624b\u52bf\u9891\u7387\u548c\u590d\u6742\u6027\u4e0a\u5347\uff0c\u4f46\u624b\u4e0e\u8bed\u97f3\u7684\u540c\u6b65\u6027\u53d8\u5316\u6709\u9650\u3002", "motivation": "\u9762\u5bf9\u80cc\u666f\u566a\u97f3\u6216\u5e72\u6270\u8005\u8bf4\u8bdd\u5bfc\u81f4\u9762\u5bf9\u9762\u6c9f\u901a\u56f0\u96be\u65f6\uff0c\u89c6\u89c9\u7ebf\u7d22\u5728\u6c9f\u901a\u4e2d\u7684\u91cd\u8981\u6027\u589e\u52a0\u3002", "method": "\u8fdb\u884c\u81ea\u7531\u7684\u53cc\u4eba\u5bf9\u8bdd\u5b9e\u9a8c\uff0c\u901a\u8fc7\u65b0\u5f00\u53d1\u7684\u6807\u8bb0\u7cfb\u7edf\u63cf\u8ff0\u4f1a\u8bdd\u52a8\u4f5c\u3002", "result": "\u5728\u66f4\u9ad8\u7684\u566a\u58f0\u6c34\u5e73\u4e0b\uff0c\u624b\u52bf\u590d\u6742\u6027\u589e\u52a0\uff0c\u4ece\u800c\u5f71\u54cd\u4e0e\u8bf4\u8bdd\u548c\u542c\u529b\u76f8\u5173\u7684\u52a8\u4f5c\uff0c\u4f46\u5934\u90e8\u52a8\u4f5c\u76f8\u5bf9\u8bf4\u8bdd\u6709\u6240\u51cf\u5c11\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u652f\u6301\u4e86\u80a2\u4f53\u52a8\u4f5c\u5728\u6c9f\u901a\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u8868\u660e\u5728\u9762\u5bf9\u6311\u6218\u7684\u6c9f\u901a\u73af\u5883\u4e2d\uff0c\u53c2\u4e0e\u8005\u4f1a\u901a\u8fc7\u5404\u79cd\u65b9\u5f0f\u9002\u5e94\u4ea4\u8c08\u9700\u6c42\u3002"}}
{"id": "2512.03630", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03630", "abs": "https://arxiv.org/abs/2512.03630", "authors": ["Shifa Sulaiman", "Amarnath H", "Simon Bogh", "Naresh Marturi"], "title": "Multimodal Control of Manipulators: Coupling Kinematics and Vision for Self-Driving Laboratory Operations", "comment": null, "summary": "Motion planning schemes are used for planning motions of a manipulator from an initial pose to a final pose during a task execution. A motion planning scheme generally comprises of a trajectory planning method and an inverse kinematic solver to determine trajectories and joints solutions respectively. In this paper, 3 motion planning schemes developed based on Jacobian methods are implemented to traverse a redundant manipulator with a coupled finger gripper through given trajectories. RRT* algorithm is used for planning trajectories and screw theory based forward kinematic equations are solved for determining joint solutions of the manipulator and gripper. Inverse solutions are computed separately using 3 Jacobian based methods such as Jacobian Transpose (JT), Pseudo Inverse (PI), and Damped Least Square (DLS) methods. Space Jacobian and manipulability measurements of the manipulator and gripper are obtained using screw theory formulations. Smoothness and RMSE error of generated trajectories and velocity continuity, acceleration profile, jerk, and snap values of joint motions are analysed for determining an efficient motion planning method for a given task. Advantages and disadvantages of the proposed motion planning schemes mentioned above are analysed using simulation studies to determine a suitable inverse solution technique for the tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e09\u79cd\u57fa\u4e8e\u96c5\u53ef\u6bd4\u65b9\u6cd5\u7684\u8fd0\u52a8\u89c4\u5212\u65b9\u6848\uff0c\u7528\u4e8e\u63a7\u5236\u5197\u4f59\u673a\u68b0\u624b\u548c\u5939\u722a\u6cbf\u9884\u5b9a\u8f68\u8ff9\u8fd0\u52a8\uff0c\u5e76\u6bd4\u8f83\u4e0d\u540c\u96c5\u53ef\u6bd4\u9006\u89e3\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u5197\u4f59\u673a\u68b0\u624b\u4e0e\u5939\u722a\u7684\u9ad8\u6548\u3001\u5e73\u6ed1\u7684\u8fd0\u52a8\u89c4\u5212\uff0c\u6bd4\u8f83\u4e0d\u540c\u7684\u9006\u89e3\u65b9\u6cd5\u4ee5\u627e\u5230\u6700\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5b9e\u65bd\u57fa\u4e8e\u96c5\u53ef\u6bd4\u6cd5\u7684\u4e09\u79cd\u8fd0\u52a8\u89c4\u5212\u65b9\u6848\uff0c\u4f7f\u7528RRT*\u7b97\u6cd5\u8fdb\u884c\u8f68\u8ff9\u89c4\u5212\uff0c\u5e76\u72ec\u7acb\u8ba1\u7b97\u96c5\u53ef\u6bd4\u9006\u89e3\u3002", "result": "\u751f\u6210\u7684\u8f68\u8ff9\u7684\u5e73\u6ed1\u5ea6\u548c\u5747\u65b9\u6839\u8bef\u5dee\uff08RMSE\uff09\u8fdb\u884c\u4e86\u5206\u6790\uff0c\u540c\u65f6\u8bc4\u4f30\u4e86\u5173\u8282\u8fd0\u52a8\u7684\u901f\u5ea6\u8fde\u7eed\u6027\u3001\u52a0\u901f\u5ea6\u3001\u98a4\u632f\u548c\u77ac\u6001\u503c\u3002", "conclusion": "\u901a\u8fc7\u4eff\u771f\u7814\u7a76\u5206\u6790\u4e86\u4e09\u79cd\u96c5\u53ef\u6bd4\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u786e\u5b9a\u9002\u5408\u7279\u5b9a\u4efb\u52a1\u7684\u9006\u89e3\u6280\u672f\u3002"}}
{"id": "2512.03784", "categories": ["cs.HC", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2512.03784", "abs": "https://arxiv.org/abs/2512.03784", "authors": ["Guisong Liu", "Jiansong Zhang", "Yinpei Luo", "Guoliang Wei", "Shuqing Sun", "Shiyang Deng", "Pengfei Wei", "Nanxi Chen"], "title": "Sleep Modulation: The Challenge of Transitioning from Open Loop to Closed Loop", "comment": null, "summary": "Sleep disorders have emerged as a critical global health issue, highlighting the urgent need for effective and widely accessible intervention technologies. Non-invasive brain stimulation has garnered attention as it enables direct or indirect modulation of neural activity, thereby promoting sleep enhancement in a safe and unobtrusive manner. This class of approaches is collectively referred to as sleep modulation. To date, the majority of sleep modulation research relies on open-loop paradigms with empirically determined parameters, while achieving individual adaptation and modulation accuracy remains a distant objective. The paradigm-specific constraints inherent to open-loop designs represent a major obstacle to clinical translation and large-scale deployment in home environments. In this paper, we delineate fundamental paradigms of sleep modulation, critically examine the intrinsic limitations of open-loop approaches, and formally conceptualize sleep closed-loop modulation. We further provide a comprehensive synthesis of prior studies involving five commonly employed modulation techniques, evaluating their potential integration within a closed-loop framework. Finally, we identify three primary challenges in constructing an effective sleep closed-loop modulation system: sensor solution selection, monitoring model design, and modulation strategy design, while also proposing potential solutions. Collectively, this work aims to advance the paradigm shift of sleep modulation from open-loop toward closed-loop systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u7761\u7720\u8c03\u8282\u4ece\u5f00\u653e\u5f0f\u5411\u95ed\u73af\u7cfb\u7edf\u8f6c\u53d8\u7684\u5fc5\u8981\u6027\uff0c\u5ba1\u67e5\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6784\u5efa\u6709\u6548\u95ed\u73af\u7cfb\u7edf\u7684\u6311\u6218\u548c\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u7761\u7720\u969c\u788d\u5df2\u6210\u4e3a\u5168\u7403\u5065\u5eb7\u95ee\u9898\uff0c\u4e9f\u9700\u6709\u6548\u548c\u5e7f\u6cdb\u53ef\u53ca\u7684\u5e72\u9884\u6280\u672f\u3002", "method": "\u901a\u8fc7\u56de\u987e\u4e94\u79cd\u5e38\u7528\u8c03\u8282\u6280\u672f\uff0c\u8bc4\u4f30\u5176\u5728\u95ed\u73af\u6846\u67b6\u4e2d\u7684\u6f5c\u5728\u6574\u5408\uff0c\u5e76\u8bc6\u522b\u5728\u6784\u5efa\u6709\u6548\u95ed\u73af\u8c03\u8282\u7cfb\u7edf\u65f6\u7684\u4e3b\u8981\u6311\u6218\u3002", "result": "\u63d0\u51fa\u4e86\u7761\u7720\u95ed\u73af\u8c03\u8282\u7684\u6982\u5ff5\uff0c\u5e76\u8bc6\u522b\u4e86\u6784\u5efa\u6709\u6548\u7cfb\u7edf\u7684\u4e3b\u8981\u6311\u6218\u3002", "conclusion": "\u672c\u5de5\u4f5c\u65e8\u5728\u63a8\u52a8\u7761\u7720\u8c03\u8282\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u4ece\u5f00\u653e\u5f0f\u7cfb\u7edf\u53d1\u5c55\u5230\u95ed\u73af\u7cfb\u7edf\u3002"}}
{"id": "2512.03639", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03639", "abs": "https://arxiv.org/abs/2512.03639", "authors": ["Kilian Schweppe", "Anne-Kathrin Schmuck"], "title": "Context-Triggered Contingency Games for Strategic Multi-Agent Interaction", "comment": null, "summary": "We address the challenge of reliable and efficient interaction in autonomous multi-agent systems, where agents must balance long-term strategic objectives with short-term dynamic adaptation. We propose context-triggered contingency games, a novel integration of strategic games derived from temporal logic specifications with dynamic contingency games solved in real time. Our two-layered architecture leverages strategy templates to guarantee satisfaction of high-level objectives, while a new factor-graph-based solver enables scalable, real-time model predictive control of dynamic interactions. The resulting framework ensures both safety and progress in uncertain, interactive environments. We validate our approach through simulations and hardware experiments in autonomous driving and robotic navigation, demonstrating efficient, reliable, and adaptive multi-agent interaction.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7ed3\u5408\u65f6\u5e8f\u903b\u8f91\u548c\u52a8\u6001\u5e94\u6025\u6e38\u620f\u7684\u53cc\u5c42\u67b6\u6784\uff0c\u65e8\u5728\u63d0\u9ad8\u81ea\u4e3b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u4ea4\u4e92\u6548\u7387\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u89e3\u51b3\u81ea\u4e3b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u957f\u671f\u6218\u7565\u76ee\u6807\u4e0e\u77ed\u671f\u52a8\u6001\u9002\u5e94\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u9ad8\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u4ea4\u4e92\u6548\u7387\u548c\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u56e0\u5b50\u56fe\u7684\u6c42\u89e3\u5668\uff0c\u7ed3\u5408\u6218\u7565\u6a21\u677f\uff0c\u8fdb\u884c\u52a8\u6001\u4ea4\u4e92\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u57fa\u4e8e\u65f6\u5e8f\u903b\u8f91\u7684\u6218\u7565\u6e38\u620f\u4e0e\u5b9e\u65f6\u52a8\u6001\u5e94\u6025\u6e38\u620f\uff0c\u65e8\u5728\u63d0\u9ad8\u81ea\u4e3b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u4ea4\u4e92\u7684\u53ef\u9760\u6027\u4e0e\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u81ea\u52a8\u9a7e\u9a76\u548c\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u53d6\u5f97\u826f\u597d\u6548\u679c\u3002", "conclusion": "\u6846\u67b6\u5728\u52a8\u6001\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u4fdd\u8bc1\u4e86\u5b89\u5168\u6027\u548c\u8fdb\u5c55\uff0c\u901a\u8fc7\u6a21\u62df\u548c\u786c\u4ef6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2512.03878", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.03878", "abs": "https://arxiv.org/abs/2512.03878", "authors": ["Zhiyin Zhou"], "title": "Adhera: A Human-Centered Health Informatics Solution for Reducing Informal Caregiver Burden through Improved Medication Adherence", "comment": null, "summary": "The growing global population of older adults, combined with ongoing healthcare workforce shortages, has increased reliance on informal caregivers, including family members and friends who provide unpaid support to individuals with chronic illnesses. Among their daily responsibilities, medication management remains one of the most demanding and error-prone tasks. Non-adherence to prescribed regimens not only undermines patient outcomes but also intensifies caregiver stress, anxiety, and fatigue. Although digital health technologies have proliferated to address adherence, most solutions focus exclusively on patients and neglect the informational and emotional needs of caregivers. This paper introduces Adhera, a caregiver-inclusive health informatics system designed to support medication adherence while reducing caregiver burden. Using a mixed-methods research design that included fifteen semi-structured caregiver interviews, sixty-five survey responses, and five pharmacist consultations, this study identified three primary challenges: caregiver stress related to uncertainty about medication intake, fragmented communication with healthcare professionals, and distrust in existing digital tools. Informed by the CeHRes Roadmap 2.0 and the Triple Bottom Line by Design and Culture (TBLD+C) framework, as well as recent co-design studies involving caregivers, Adhera integrates a sensor-equipped smart pill organizer with a mobile companion application that records intake events, sends real-time reminders, and provides caregivers with synchronized adherence data. Preliminary evaluation suggests that Adhera enhances visibility, improves caregiver confidence, and streamlines medication routines. This study contributes to the field of health informatics by demonstrating how human-centered design and collaborative frameworks can align technical innovation with empathy-driven care.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aAdhera\u7684\u5065\u5eb7\u4fe1\u606f\u7cfb\u7edf\uff0c\u65e8\u5728\u652f\u6301\u836f\u7269\u4f9d\u4ece\u6027\u540c\u65f6\u51cf\u5c11\u7167\u987e\u8005\u8d1f\u62c5\u3002", "motivation": "\u5e94\u5bf9\u8001\u5e74\u4eba\u5bf9\u836f\u7269\u7ba1\u7406\u7684\u9700\u6c42\uff0c\u51cf\u8f7b\u975e\u6b63\u5f0f\u7167\u987e\u8005\u7684\u538b\u529b", "method": "\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\u8bbe\u8ba1\uff0c\u5305\u62ec\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u3001\u8c03\u67e5\u95ee\u5377\u548c\u836f\u5242\u5e08\u54a8\u8be2", "result": "Adhera\u7cfb\u7edf\u901a\u8fc7\u4f20\u611f\u5668\u836f\u4e38\u7ec4\u7ec7\u5668\u4e0e\u79fb\u52a8\u5e94\u7528\u7a0b\u5e8f\u63d0\u9ad8\u4e86\u53ef\u89c6\u6027\u5e76\u6539\u5584\u4e86\u7167\u987e\u8005\u7684\u4fe1\u5fc3", "conclusion": "\u901a\u8fc7\u4eba\u672c\u8bbe\u8ba1\u548c\u534f\u4f5c\u6846\u67b6\uff0cAdhera\u5c55\u793a\u4e86\u6280\u672f\u521b\u65b0\u4e0e\u4ee5\u540c\u60c5\u4e3a\u9a71\u52a8\u7684\u62a4\u7406\u76f8\u7ed3\u5408\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2512.03684", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03684", "abs": "https://arxiv.org/abs/2512.03684", "authors": ["Shahid Ansari", "Mahendra Kumar Gohil", "Yusuke Maeda", "Bishakh Bhattacharya"], "title": "A Novel Approach to Tomato Harvesting Using a Hybrid Gripper with Semantic Segmentation and Keypoint Detection", "comment": null, "summary": "This paper presents an autonomous tomato-harvesting system built around a hybrid robotic gripper that combines six soft auxetic fingers with a rigid exoskeleton and a latex basket to achieve gentle, cage-like grasping. The gripper is driven by a servo-actuated Scotch--yoke mechanism, and includes separator leaves that form a conical frustum for fruit isolation, with an integrated micro-servo cutter for pedicel cutting. For perception, an RGB--D camera and a Detectron2-based pipeline perform semantic segmentation of ripe/unripe tomatoes and keypoint localization of the pedicel and fruit center under occlusion and variable illumination. An analytical model derived using the principle of virtual work relates servo torque to grasp force, enabling design-level reasoning about actuation requirements. During execution, closed-loop grasp-force regulation is achieved using a proportional--integral--derivative controller with feedback from force-sensitive resistors mounted on selected fingers to prevent slip and bruising. Motion execution is supported by Particle Swarm Optimization (PSO)--based trajectory planning for a 5-DOF manipulator. Experiments demonstrate complete picking cycles (approach, separation, cutting, grasping, transport, release) with an average cycle time of 24.34~s and an overall success rate of approximately 80\\%, while maintaining low grasp forces (0.20--0.50~N). These results validate the proposed hybrid gripper and integrated vision--control pipeline for reliable harvesting in cluttered environments.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u578b\u673a\u5668\u4eba\u6293\u624b\u7684\u81ea\u4e3b\u756a\u8304\u6536\u5272\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86\u8f6f\u6027\u6307\u5934\u548c\u521a\u6027\u5916\u9aa8\u9abc\uff0c\u914d\u5907\u89c6\u89c9\u63a7\u5236\u7ba1\u9053\uff0c\u5b9e\u73b0\u53ef\u9760\u7684\u6536\u5272\u3002", "motivation": "\u53d1\u5c55\u4e00\u79cd\u81ea\u4e3b\u7684\u756a\u8304\u6536\u5272\u7cfb\u7edf\uff0c\u4ee5\u63d0\u9ad8\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u6536\u5272\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u516d\u4e2a\u8f6f\u6027\u52a9\u62c9\u624b\u6307\u3001\u4f3a\u670d\u9a71\u52a8\u7684\u673a\u5236\u3001RGB-D\u6444\u50cf\u673a\u8fdb\u884c\u611f\u77e5\uff0c\u4ee5\u53ca\u57fa\u4e8e\u53cd\u9988\u7684\u95ed\u73af\u6293\u63e1\u529b\u8c03\u8282\u7b49\u6280\u672f\u3002", "result": "\u7cfb\u7edf\u5b9e\u73b0\u4e86\u5b8c\u6574\u7684\u91c7\u6458\u5468\u671f\uff0c\u5e73\u5747\u5468\u671f\u65f6\u95f4\u4e3a24.34\u79d2\uff0c\u6210\u529f\u7387\u7ea6\u4e3a80%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6df7\u5408\u6293\u624b\u548c\u96c6\u6210\u89c6\u89c9\u63a7\u5236\u7ba1\u9053\u9a8c\u8bc1\u4e86\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u5177\u6709\u6f5c\u5728\u7684\u519c\u4e1a\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2512.03945", "categories": ["cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03945", "abs": "https://arxiv.org/abs/2512.03945", "authors": ["Michael Schiffmann", "Sabina Jeschke", "Anja Richert"], "title": "Classification of User Satisfaction in HRI with Social Signals in the Wild", "comment": "15 pages, 3 figures. This paper has been accepted for publication at ICSR+AI 2025", "summary": "Socially interactive agents (SIAs) are being used in various scenarios and are nearing productive deployment. Evaluating user satisfaction with SIAs' performance is a key factor in designing the interaction between the user and SIA. Currently, subjective user satisfaction is primarily assessed manually through questionnaires or indirectly via system metrics. This study examines the automatic classification of user satisfaction through analysis of social signals, aiming to enhance both manual and autonomous evaluation methods for SIAs. During a field trial at the Deutsches Museum Bonn, a Furhat Robotics head was employed as a service and information hub, collecting an \"in-the-wild\" dataset. This dataset comprises 46 single-user interactions, including questionnaire responses and video data. Our method focuses on automatically classifying user satisfaction based on time series classification. We use time series of social signal metrics derived from the body pose, time series of facial expressions, and physical distance. This study compares three feature engineering approaches on different machine learning models. The results confirm the method's effectiveness in reliably identifying interactions with low user satisfaction without the need for manually annotated datasets. This approach offers significant potential for enhancing SIA performance and user experience through automated feedback mechanisms.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u901a\u8fc7\u5206\u6790\u793e\u4ea4\u4fe1\u53f7\u81ea\u52a8\u5206\u7c7b\u7528\u6237\u6ee1\u610f\u5ea6\uff0c\u4ee5\u63d0\u5347\u793e\u4f1a\u4e92\u52a8\u4ee3\u7406\uff08SIA\uff09\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u8bc4\u4f30\u7528\u6237\u5bf9SIA\u8868\u73b0\u7684\u6ee1\u610f\u5ea6\u662f\u8bbe\u8ba1\u7528\u6237\u4e0eSIA\u4e92\u52a8\u7684\u5173\u952e\u56e0\u7d20\u3002", "method": "\u91c7\u7528\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u65b9\u6cd5\uff0c\u5206\u6790\u6765\u81ea\u8eab\u4f53\u59ff\u52bf\u3001\u9762\u90e8\u8868\u60c5\u4e0e\u7269\u7406\u8ddd\u79bb\u7684\u793e\u4ea4\u4fe1\u53f7\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u624b\u52a8\u6807\u6ce8\u6570\u636e\u96c6\u5373\u80fd\u53ef\u9760\u8bc6\u522b\u4f4e\u6ee1\u610f\u5ea6\u4e92\u52a8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u8bc6\u522b\u4f4e\u7528\u6237\u6ee1\u610f\u5ea6\u7684\u4e92\u52a8\uff0c\u5177\u5907\u63d0\u5347SIA\u6027\u80fd\u548c\u7528\u6237\u4f53\u9a8c\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.03707", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03707", "abs": "https://arxiv.org/abs/2512.03707", "authors": ["Sundas Rafat Mulkana", "Ronyu Yu", "Tanaya Guha", "Emma Li"], "title": "ContactRL: Safe Reinforcement Learning based Motion Planning for Contact based Human Robot Collaboration", "comment": "8 pages, 7 figures", "summary": "In collaborative human-robot tasks, safety requires not only avoiding collisions but also ensuring safe, intentional physical contact. We present ContactRL, a reinforcement learning (RL) based framework that directly incorporates contact safety into the reward function through force feedback. This enables a robot to learn adaptive motion profiles that minimize human-robot contact forces while maintaining task efficiency. In simulation, ContactRL achieves a low safety violation rate of 0.2\\% with a high task success rate of 87.7\\%, outperforming state-of-the-art constrained RL baselines. In order to guarantee deployment safety, we augment the learned policy with a kinetic energy based Control Barrier Function (eCBF) shield. Real-world experiments on an UR3e robotic platform performing small object handovers from a human hand across 360 trials confirm safe contact, with measured normal forces consistently below 10N. These results demonstrate that ContactRL enables safe and efficient physical collaboration, thereby advancing the deployment of collaborative robots in contact-rich tasks.", "AI": {"tldr": "ContactRL\u6846\u67b6\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u4e86\u5b89\u5168\u7684\u7269\u7406\u63a5\u89e6\u548c\u9ad8\u6548\u7684\u4eba\u673a\u534f\u4f5c\uff0c\u5e76\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u4e2d\u53d6\u5f97\u51fa\u8272\u7684\u6210\u679c\u3002", "motivation": "\u8be5\u7814\u7a76\u7684\u52a8\u673a\u662f\u786e\u4fdd\u4eba\u673a\u534f\u4f5c\u4e2d\u4e0d\u4ec5\u907f\u514d\u78b0\u649e\uff0c\u8fd8\u8981\u786e\u4fdd\u5b89\u5168\u7684\u3001\u6545\u610f\u7684\u7269\u7406\u63a5\u89e6\u3002", "method": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0cContactRL\u5c06\u63a5\u89e6\u5b89\u5168\u76f4\u63a5\u6574\u5408\u5230\u5956\u52b1\u51fd\u6570\u4e2d\uff0c\u5e76\u901a\u8fc7\u52a8\u529b\u5b66\u80fd\u91cf\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u589e\u5f3a\u7b56\u7565\u7684\u5b89\u5168\u6027\u3002", "result": "ContactRL\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u65e8\u5728\u786e\u4fdd\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u5b89\u5168\u63a5\u89e6\u3002\u901a\u8fc7\u5f15\u5165\u63a5\u89e6\u5b89\u5168\u7684\u5956\u52b1\u51fd\u6570\uff0c\u732b\u8f66\u53ef\u4ee5\u9002\u5e94\u6027\u5730\u964d\u4f4e\u63a5\u89e6\u529b\uff0c\u4ece\u800c\u4fdd\u6301\u4efb\u52a1\u6548\u7387\u3002\u540c\u65f6\uff0c\u8be5\u6846\u67b6\u8fd8\u7ed3\u5408\u4e86\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u6765\u4fdd\u969c\u90e8\u7f72\u5b89\u5168\uff0c\u5e76\u5728\u5b9e\u9645\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6548\u80fd\u3002", "conclusion": "ContactRL\u5c55\u793a\u4e86\u5728\u63a5\u89e6\u4e30\u5bcc\u7684\u4efb\u52a1\u4e2d\uff0c\u534f\u4f5c\u673a\u5668\u4eba\u53ef\u4ee5\u5b89\u5168\u4e14\u9ad8\u6548\u5730\u6267\u884c\u4efb\u52a1\uff0c\u63a8\u52a8\u4e86\u4eba\u673a\u534f\u4f5c\u673a\u5668\u4eba\u7684\u5e94\u7528\u3002"}}
{"id": "2512.03988", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.03988", "abs": "https://arxiv.org/abs/2512.03988", "authors": ["Jathushan Kaetheeswaran", "Boyi Ma", "Ali Abedi", "Milad Lankarany", "Shehroz Khan"], "title": "HEART-Watch: A multimodal physiological dataset from a Google Pixel Watch across different physical states", "comment": null, "summary": "Consumer-grade smartwatches offer a new personalized health monitoring option for general consumers globally as cardiovascular diseases continue to prevail as the leading cause of global mortality. The development and validation of reliable cardiovascular monitoring algorithms for these consumer-grade devices requires realistic biosignal data from diverse sets of participants. However, the availability of public consumer-grade smartwatch datasets with synchronized cardiovascular biosignals is limited, and existing datasets do not offer rich demographic diversity in their participant cohorts, leading to potentially biased algorithm development. This paper presents HEART-Watch, a multimodal physiological dataset collected from temporally synchronized wrist-worn Google Pixel Watch 2 electrocardiogram (ECG), photoplethysmography, and accelerometer signals from a diverse cohort of 40 healthy adults across three physical states - sitting, standing and walking with reference chest ECG. Intermittent upper arm blood pressure measurements and concurrent biosignals were collected as an additional biomarker for future research. The motivation, methodology, and initial analyses of results are presented. HEART-Watch is intended to support the development and benchmarking of robust algorithms for cardiovascular analyses on consumer-grade smartwatches across diverse populations.", "AI": {"tldr": "\u672c\u8bba\u6587\u4ecb\u7ecd\u4e86HEART-Watch\u6570\u636e\u96c6\uff0c\u65e8\u5728\u63d0\u9ad8\u667a\u80fd\u624b\u8868\u5fc3\u8840\u7ba1\u76d1\u6d4b\u7b97\u6cd5\u7684\u53ef\u9760\u6027\u548c\u9002\u7528\u6027\uff0c\u57fa\u4e8e\u591a\u6837\u5316\u7684\u53c2\u4e0e\u8005\u6570\u636e\u3002", "motivation": "\u793e\u4f1a\u4e0a\u5fc3\u8840\u7ba1\u75be\u75c5\u662f\u4e3b\u8981\u7684\u6b7b\u4ea1\u539f\u56e0\uff0c\u800c\u6d88\u8d39\u7ea7\u667a\u80fd\u624b\u8868\u4e3a\u4e2a\u4eba\u5065\u5eb7\u76d1\u6d4b\u63d0\u4f9b\u4e86\u65b0\u9009\u62e9\uff0c\u4f46\u7f3a\u4e4f\u591a\u6837\u5316\u7684\u751f\u7269\u4fe1\u53f7\u6570\u636e", "method": "\u901a\u8fc7\u6536\u96c6\u6765\u81ea40\u540d\u5065\u5eb7\u6210\u5e74\u4eba\u7684\u591a\u6a21\u6001\u751f\u7406\u4fe1\u53f7\uff0c\u5305\u62ecECG\u3001\u5149\u7535\u5bb9\u79ef\u8109\u640f\u6ce2\u548c\u52a0\u901f\u5ea6\u4fe1\u53f7\uff0c\u5e76\u8fdb\u884c\u4e86\u591a\u79cd\u8eab\u4f53\u72b6\u6001\u4e0b\u7684\u76d1\u6d4b\u3002", "result": "\u63d0\u51fa\u4e86HEART-Watch\uff0c\u4e00\u4e2a\u5305\u542b\u591a\u6a21\u6001\u751f\u7406\u6570\u636e\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5fc3\u8840\u7ba1\u5206\u6790\u7b97\u6cd5\u7684\u5f00\u53d1\u4e0e\u57fa\u51c6\u6d4b\u8bd5", "conclusion": "HEART-Watch\u6570\u636e\u96c6\u5c06\u4e3a\u5fc3\u8840\u7ba1\u5206\u6790\u7b97\u6cd5\u7684\u5f00\u53d1\u63d0\u4f9b\u652f\u6301\uff0c\u65e8\u5728\u51cf\u5c11\u7b97\u6cd5\u5f00\u53d1\u4e2d\u7684\u504f\u5dee"}}
{"id": "2512.03729", "categories": ["cs.RO", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.03729", "abs": "https://arxiv.org/abs/2512.03729", "authors": ["Samantha Chapin", "Kenneth Stewart", "Roxana Leontie", "Carl Glen Henshaw"], "title": "Autonomous Planning In-space Assembly Reinforcement-learning free-flYer (APIARY) International Space Station Astrobee Testing", "comment": "iSpaRo 2025, Best Paper Award in Orbital Robotics", "summary": "The US Naval Research Laboratory's (NRL's) Autonomous Planning In-space Assembly Reinforcement-learning free-flYer (APIARY) experiment pioneers the use of reinforcement learning (RL) for control of free-flying robots in the zero-gravity (zero-G) environment of space. On Tuesday, May 27th 2025 the APIARY team conducted the first ever, to our knowledge, RL control of a free-flyer in space using the NASA Astrobee robot on-board the International Space Station (ISS). A robust 6-degrees of freedom (DOF) control policy was trained using an actor-critic Proximal Policy Optimization (PPO) network within the NVIDIA Isaac Lab simulation environment, randomizing over goal poses and mass distributions to enhance robustness. This paper details the simulation testing, ground testing, and flight validation of this experiment. This on-orbit demonstration validates the transformative potential of RL for improving robotic autonomy, enabling rapid development and deployment (in minutes to hours) of tailored behaviors for space exploration, logistics, and real-time mission needs.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u5c55\u793a\u4e86\u5728\u56fd\u9645\u7a7a\u95f4\u7ad9\u4e0a\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u81ea\u7531\u98de\u884c\u673a\u5668\u4eba\u7684\u9996\u6b21\u5c1d\u8bd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u592a\u7a7a\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u4e86\u5728\u592a\u7a7a\u63a2\u7d22\u3001\u7269\u6d41\u548c\u5b9e\u65f6\u4efb\u52a1\u9700\u6c42\u4e2d\u63d0\u9ad8\u673a\u5668\u4eba\u7684\u81ea\u4e3b\u6027\uff0c\u5feb\u901f\u5f00\u53d1\u548c\u90e8\u7f72\u5b9a\u5236\u5316\u884c\u4e3a\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u63a7\u5236\u5728\u96f6\u91cd\u529b\u73af\u5883\u4e2d\u7684\u81ea\u7531\u98de\u884c\u673a\u5668\u4eba\uff0c\u91c7\u7528Proximal Policy Optimization (PPO)\u7f51\u7edc\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002", "result": "\u5728\u56fd\u9645\u7a7a\u95f4\u7ad9\u4e0a\u6210\u529f\u5b9e\u73b0\u4e86\u57fa\u4e8eRL\u7684\u9996\u4e2a\u81ea\u7531\u98de\u884c\u673a\u5668\u4eba\u63a7\u5236\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86RL\u5728\u63d0\u9ad8\u673a\u5668\u4eba\u81ea\u4e3b\u6027\u65b9\u9762\u7684\u53d8\u9769\u6f5c\u529b\u3002", "conclusion": "\u8be5\u5b9e\u9a8c\u8868\u660e\uff0c\u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u663e\u8457\u63d0\u5347\u592a\u7a7a\u673a\u5668\u4eba\u81ea\u4e3b\u6027\uff0c\u652f\u6491\u672a\u6765\u590d\u6742\u4efb\u52a1\u7684\u6267\u884c\u3002"}}
{"id": "2512.03991", "categories": ["cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03991", "abs": "https://arxiv.org/abs/2512.03991", "authors": ["Michael Schiffmann", "Felix Struth", "Sabina Jeschke", "Anja Richert"], "title": "When to Say \"Hi\" - Learn to Open a Conversation with an in-the-wild Dataset", "comment": "6 pages, 3 figures, 5 tables. This paper has been accepted for publication at IEEE ROMAN 2025", "summary": "The social capabilities of socially interactive agents (SIA) are a key to successful and smooth interactions between the user and the SIA. A successful start of the interaction is one of the essential factors for satisfying SIA interactions. For a service and information task in which the SIA helps with information, e.g. about the location, it is an important skill to master the opening of the conversation and to recognize which interlocutor opens the conversation and when. We are therefore investigating the extent to which the opening of the conversation can be trained using the user's body language as an input for machine learning to ensure smooth conversation starts for the interaction. In this paper we propose the Interaction Initiation System (IIS) which we developed, trained and validated using an in-the-wild data set. In a field test at the Deutsches Museum Bonn, a Furhat robot from Furhat Robotics was used as a service and information point. Over the period of use we collected the data of \\textit{N} = 201 single user interactions for the training of the algorithms. We can show that the IIS, achieves a performance that allows the conclusion that this system is able to determine the greeting period and the opener of the interaction.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u793e\u4ea4\u4e92\u52a8\u4ee3\u7406\uff08SIA\uff09\u5982\u4f55\u901a\u8fc7\u7528\u6237\u7684\u80a2\u4f53\u8bed\u8a00\u8bad\u7ec3\u4ea4\u4e92\u5f00\u542f\uff0c\u4ee5\u786e\u4fdd\u987a\u5229\u7684\u5bf9\u8bdd\u5f00\u59cb\uff0c\u63d0\u51fa\u4e86\u4ea4\u4e92\u542f\u52a8\u7cfb\u7edf\uff08IIS\uff09\uff0c\u901a\u8fc7\u5b9e\u5730\u6d4b\u8bd5\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u793e\u4ea4\u4e92\u52a8\u4ee3\u7406\u7684\u793e\u4ea4\u80fd\u529b\u662f\u7528\u6237\u4e0e\u5176\u987a\u7545\u4e92\u52a8\u7684\u5173\u952e\uff0c\u800c\u6210\u529f\u7684\u5bf9\u8bdd\u5f00\u542f\u662f\u63d0\u9ad8\u7528\u6237\u6ee1\u610f\u5ea6\u7684\u91cd\u8981\u56e0\u7d20\u3002", "method": "\u4f7f\u7528\u7528\u6237\u7684\u80a2\u4f53\u8bed\u8a00\u4f5c\u4e3a\u8f93\u5165\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u8bad\u7ec3\u4ea4\u4e92\u542f\u52a8\u7cfb\u7edf\uff0c\u5e76\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u8fdb\u884c\u6570\u636e\u6536\u96c6\u548c\u6a21\u578b\u9a8c\u8bc1\u3002", "result": "IIS\u7cfb\u7edf\u5728\u5b9e\u5730\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u80fd\u591f\u51c6\u786e\u5224\u65ad\u95ee\u5019\u65f6\u673a\u548c\u5bf9\u8bdd\u5f00\u542f\u8005\uff0c\u4ece\u800c\u63d0\u5347\u4ea4\u4e92\u4f53\u9a8c\u3002", "conclusion": "\u4ea4\u4e92\u542f\u52a8\u7cfb\u7edf\uff08IIS\uff09\u80fd\u591f\u6709\u6548\u8bc6\u522b\u5bf9\u8bdd\u7684\u5f00\u542f\u548c\u95ee\u5019\u65f6\u673a\uff0c\u63d0\u5347SIA\u4e0e\u7528\u6237\u7684\u4ea4\u4e92\u8d28\u91cf\u3002"}}
{"id": "2512.03736", "categories": ["cs.RO", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.03736", "abs": "https://arxiv.org/abs/2512.03736", "authors": ["Kenneth Stewart", "Samantha Chapin", "Roxana Leontie", "Carl Glen Henshaw"], "title": "Crossing the Sim2Real Gap Between Simulation and Ground Testing to Space Deployment of Autonomous Free-flyer Control", "comment": "published at iSpaRo 2025", "summary": "Reinforcement learning (RL) offers transformative potential for robotic control in space. We present the first on-orbit demonstration of RL-based autonomous control of a free-flying robot, the NASA Astrobee, aboard the International Space Station (ISS). Using NVIDIA's Omniverse physics simulator and curriculum learning, we trained a deep neural network to replace Astrobee's standard attitude and translation control, enabling it to navigate in microgravity. Our results validate a novel training pipeline that bridges the simulation-to-reality (Sim2Real) gap, utilizing a GPU-accelerated, scientific-grade simulation environment for efficient Monte Carlo RL training. This successful deployment demonstrates the feasibility of training RL policies terrestrially and transferring them to space-based applications. This paves the way for future work in In-Space Servicing, Assembly, and Manufacturing (ISAM), enabling rapid on-orbit adaptation to dynamic mission requirements.", "AI": {"tldr": "\u672c\u6587\u5728\u56fd\u9645\u7a7a\u95f4\u7ad9\u9996\u6b21\u5c55\u793a\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u673a\u5668\u4eba\u81ea\u4e3b\u63a7\u5236\uff0c\u8868\u660e\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u7a7a\u95f4\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u548c\u53ef\u884c\u6027\u3002", "motivation": "\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u5728\u5fae\u91cd\u529b\u73af\u5883\u4e2d\u63d0\u9ad8\u673a\u5668\u4eba\u7684\u81ea\u4e3b\u5bfc\u822a\u80fd\u529b\u3002", "method": "\u5c06\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u7528\u4e8e\u81ea\u4e3b\u63a7\u5236\u81ea\u7531\u98de\u884c\u673a\u5668\u4eba Astrobee\uff0c\u4ee5\u66ff\u4ee3\u6807\u51c6\u59ff\u6001\u548c\u4f4d\u79fb\u63a7\u5236\u3002", "result": "\u9996\u6b21\u5728\u56fd\u9645\u7a7a\u95f4\u7ad9\u4e0a\u6210\u529f\u6f14\u793a\u57fa\u4e8eRL\u7684Astrobee\u81ea\u7531\u98de\u884c\u63a7\u5236\uff0c\u9a8c\u8bc1\u4e86\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u7f29\u5c0f\u4e86\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u3002", "conclusion": "\u6210\u529f\u7684\u5b9e\u9a8c\u4e3a\u672a\u6765\u7684\u5728\u8f68\u670d\u52a1\u3001\u7ec4\u88c5\u548c\u5236\u9020\u5de5\u4f5c\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u8fc5\u901f\u9002\u5e94\u52a8\u6001\u4efb\u52a1\u9700\u6c42\u3002"}}
{"id": "2512.04030", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.04030", "abs": "https://arxiv.org/abs/2512.04030", "authors": ["Patricia Marcella Evite"], "title": "Affordances of Digital and Blockchain-based Community Currencies: The Case of Sarafu Network in Kenya", "comment": "20 pages, 6 figures. Accepted for publication at International Journal of Community Currency Research", "summary": "Community currencies (CCs) have been adopting innovative systems to overcome implementational hurdles from issuing paper currencies. Using a qualitative approach, this paper examined this digital transition of Sarafu Network in Kenya and its predecessor CCs as a case study. From the original vouchers launched in 2010, the foundation Grassroots Economics introduced a digital interface in 2016 that operates on a feature phone, and then integrated blockchain technology starting in 2018, undergoing several migrations before becoming settling on its current iteration called Community Asset Vouchers on the Celo blockchain since 2023. Using affordances from human-computer interaction, the research shows that digitalization and blockchain improved the facilitation of economic activities of the local communities, both their typical market transactions as well as traditional reciprocal labor exchanges, by offering more functionalities compared to the analog version of Sarafu. The unique contributions of blockchain include enabling automation of holding tax calculations and linking the vouchers to the mainstream monetary system via stablecoins facilitated by a series of smart contracts also known as the liquidity pool. The study also finds that there is an inherent trade-off between blockchain benefits and user interface complexity. Hence, balancing innovation and community needs remains a challenge.", "AI": {"tldr": "\u672c\u8bba\u6587\u7814\u7a76\u4e86\u80af\u5c3c\u4e9aSarafu\u7f51\u7edc\u7684\u6570\u5b57\u5316\u8f6c\u578b\uff0c\u63ed\u793a\u533a\u5757\u94fe\u6280\u672f\u5728\u4fc3\u8fdb\u793e\u533a\u7ecf\u6d4e\u6d3b\u52a8\u65b9\u9762\u7684\u4f18\u52bf\u4e0e\u6311\u6218\u3002", "motivation": "\u63a2\u8ba8\u793e\u533a\u8d27\u5e01\u5728\u5b9e\u65bd\u7eb8\u5e01\u65f6\u6240\u9762\u4e34\u7684\u6311\u6218\uff0c\u4ee5\u53ca\u6570\u5b57\u5316\u8f6c\u578b\u7684\u5fc5\u8981\u6027\u548c\u521b\u65b0\u6027\u3002", "method": "\u91c7\u7528\u5b9a\u6027\u7814\u7a76\u65b9\u6cd5\uff0c\u5206\u6790Sarafu\u7f51\u7edc\u53ca\u5176\u524d\u8eab\u7684\u6848\u4f8b\u3002", "result": "\u7814\u7a76\u663e\u793a\uff0c\u6570\u5b57\u5316\u548c\u533a\u5757\u94fe\u6280\u672f\u6539\u5584\u4e86\u5f53\u5730\u793e\u533a\u7684\u7ecf\u6d4e\u6d3b\u52a8 facilitation\u3002", "conclusion": "\u5e73\u8861\u533a\u5757\u94fe\u7684\u521b\u65b0\u4f18\u52bf\u4e0e\u7528\u6237\u754c\u9762\u7684\u590d\u6742\u6027\u662f\u793e\u533a\u8d27\u5e01\u6570\u5b57\u5316\u7684\u4e3b\u8981\u6311\u6218\u3002"}}
{"id": "2512.03743", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03743", "abs": "https://arxiv.org/abs/2512.03743", "authors": ["Kehlani Fay", "Darin Anthony Djapri", "Anya Zorin", "James Clinton", "Ali El Lahib", "Hao Su", "Michael T. Tolley", "Sha Yi", "Xiaolong Wang"], "title": "Cross-embodied Co-design for Dexterous Hands", "comment": null, "summary": "Dexterous manipulation is limited by both control and design, without consensus as to what makes manipulators best for performing dexterous tasks. This raises a fundamental challenge: how should we design and control robot manipulators that are optimized for dexterity? We present a co-design framework that learns task-specific hand morphology and complementary dexterous control policies. The framework supports 1) an expansive morphology search space including joint, finger, and palm generation, 2) scalable evaluation across the wide design space via morphology-conditioned cross-embodied control, and 3) real-world fabrication with accessible components. We evaluate the approach across multiple dexterous tasks, including in-hand rotation with simulation and real deployment. Our framework enables an end-to-end pipeline that can design, train, fabricate, and deploy a new robotic hand in under 24 hours. The full framework will be open-sourced and available on our website.", "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u4e2a\u5171\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u80fd\u591f\u5feb\u901f\u8bbe\u8ba1\u3001\u8bad\u7ec3\u548c\u5236\u9020\u65b0\u7684\u673a\u5668\u4eba\u624b\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u7075\u5de7\u4efb\u52a1\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u8bbe\u8ba1\u548c\u63a7\u5236\u6700\u4f18\u7684\u673a\u5668\u4eba\u64cd\u7eb5\u5668\u4ee5\u5b9e\u73b0\u7075\u5de7\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5171\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u5b66\u4e60\u4efb\u52a1\u7279\u5b9a\u7684\u624b\u90e8\u5f62\u6001\u548c\u4e92\u8865\u7684\u7075\u5de7\u63a7\u5236\u7b56\u7565\u3002", "result": "\u8be5\u6846\u67b6\u652f\u6301\u6269\u5c55\u7684\u5f62\u6001\u641c\u7d22\u7a7a\u95f4\uff0c\u80fd\u591f\u8fdb\u884c\u8de8\u8eab\u4f53\u63a7\u5236\u7684\u53ef\u6269\u5c55\u8bc4\u4f30\uff0c\u5e76\u5b9e\u73b0\u73b0\u5b9e\u4e16\u754c\u7684\u5236\u9020\uff0c\u4e14\u5728\u591a\u79cd\u7075\u5de7\u4efb\u52a1\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u572824\u5c0f\u65f6\u5185\u5b8c\u6210\u673a\u5668\u4eba\u624b\u7684\u8bbe\u8ba1\u3001\u8bad\u7ec3\u3001\u5236\u9020\u548c\u90e8\u7f72\uff0c\u5e76\u5c06\u5728\u7f51\u7ad9\u4e0a\u5f00\u6e90\u3002"}}
{"id": "2512.03756", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03756", "abs": "https://arxiv.org/abs/2512.03756", "authors": ["Marlon Steiner", "Royden Wagner", "\u00d6mer Sahin Tas", "Christoph Stiller"], "title": "Prediction-Driven Motion Planning: Route Integration Strategies in Attention-Based Prediction Models", "comment": "In Proceedings of the IEEE International Conference on Intelligent Transportation Systems (ITSC), Gold Coast, AUSTRALIA, 18-21 November 2025", "summary": "Combining motion prediction and motion planning offers a promising framework for enhancing interactions between automated vehicles and other traffic participants. However, this introduces challenges in conditioning predictions on navigation goals and ensuring stable, kinematically feasible trajectories. Addressing the former challenge, this paper investigates the extension of attention-based motion prediction models with navigation information. By integrating the ego vehicle's intended route and goal pose into the model architecture, we bridge the gap between multi-agent motion prediction and goal-based motion planning. We propose and evaluate several architectural navigation integration strategies to our model on the nuPlan dataset. Our results demonstrate the potential of prediction-driven motion planning, highlighting how navigation information can enhance both prediction and planning tasks. Our implementation is at: https://github.com/KIT-MRT/future-motion.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u8fd0\u52a8\u9884\u6d4b\u4e0e\u8fd0\u52a8\u89c4\u5212\u7ed3\u5408\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4f20\u5165\u5bfc\u822a\u4fe1\u606f\u6765\u6539\u5584\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u4e0e\u4ea4\u901a\u53c2\u4e0e\u8005\u4e4b\u95f4\u7684\u4ea4\u4e92\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u8fd0\u52a8\u9884\u6d4b\u5728\u5bfc\u822a\u76ee\u6807\u6761\u4ef6\u4e0b\u7684\u7a33\u5b9a\u6027\u548c\u8fd0\u52a8\u89c4\u5212\u7684\u53ef\u884c\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u9884\u6d4b\u4e0e\u76ee\u6807\u5bfc\u5411\u8fd0\u52a8\u89c4\u5212\u7684\u6865\u63a5\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u6269\u5c55\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u8fd0\u52a8\u9884\u6d4b\u6a21\u578b\uff0c\u5c06\u8f66\u8f86\u7684\u671f\u671b\u8def\u7ebf\u548c\u76ee\u6807\u4f4d\u7f6e\u7eb3\u5165\u6a21\u578b\u67b6\u6784\u4e2d\uff0c\u5e76\u5728nuPlan\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e0d\u540c\u7684\u96c6\u6210\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u9884\u6d4b\u9a71\u52a8\u7684\u8fd0\u52a8\u89c4\u5212\u5177\u6709\u826f\u597d\u7684\u6027\u80fd\uff0c\u5bfc\u822a\u4fe1\u606f\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u548c\u89c4\u5212\u4efb\u52a1\u7684\u6548\u679c\u3002", "conclusion": "\u6574\u5408\u5bfc\u822a\u4fe1\u606f\u80fd\u591f\u6709\u6548\u63d0\u5347\u8fd0\u52a8\u9884\u6d4b\u548c\u89c4\u5212\u7684\u6027\u80fd\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u63d0\u4f9b\u65b0\u7684\u89c6\u89d2\u548c\u65b9\u6cd5\u3002"}}
{"id": "2512.03772", "categories": ["cs.RO", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.03772", "abs": "https://arxiv.org/abs/2512.03772", "authors": ["Gabriele Fadini", "Deepak Ingole", "Tong Duy Son", "Alisa Rupenyan"], "title": "Bayesian Optimization for Automatic Tuning of Torque-Level Nonlinear Model Predictive Control", "comment": "6 pages, 7 figures, 3 tables", "summary": "This paper presents an auto-tuning framework for torque-based Nonlinear Model Predictive Control (nMPC), where the MPC serves as a real-time controller for optimal joint torque commands. The MPC parameters, including cost function weights and low-level controller gains, are optimized using high-dimensional Bayesian Optimization (BO) techniques, specifically Sparse Axis-Aligned Subspace (SAASBO) with a digital twin (DT) to achieve precise end-effector trajectory real-time tracking on an UR10e robot arm. The simulation model allows efficient exploration of the high-dimensional parameter space, and it ensures safe transfer to hardware. Our simulation results demonstrate significant improvements in tracking performance (+41.9%) and reduction in solve times (-2.5%) compared to manually-tuned parameters. Moreover, experimental validation on the real robot follows the trend (with a +25.8% improvement), emphasizing the importance of digital twin-enabled automated parameter optimization for robotic operations.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u8c03\u4f18\u6846\u67b6\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u4f18\u5316\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u8f68\u8ff9\u8ddf\u8e2a\u6539\u8fdb\u3002", "motivation": "\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u6267\u884c\u4efb\u52a1\u65f6\u7684\u8f68\u8ff9\u8ddf\u8e2a\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u4f7f\u7528\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08nMPC\uff09\u65f6\u3002", "method": "\u4f7f\u7528\u9ad8\u7ef4\u8d1d\u53f6\u65af\u4f18\u5316\u6280\u672f\u4f18\u5316\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\u53c2\u6570\uff0c\u4ee5\u5b9e\u73b0UR10e\u673a\u5668\u4eba\u624b\u81c2\u7684\u6700\u4f73\u5173\u8282\u626d\u77e9\u6307\u4ee4\u3002", "result": "\u901a\u8fc7\u4f7f\u7528\u6570\u5b57\u5b6a\u751f\u6280\u672f\u4f18\u5316\u7684\u53c2\u6570\u76f8\u6bd4\u624b\u52a8\u8c03\u4f18\uff0c\u5b9e\u73b0\u4e8641.9%\u7684\u8ddf\u8e2a\u6027\u80fd\u63d0\u5347\u548c2.5%\u7684\u6c42\u89e3\u65f6\u95f4\u7f29\u77ed\uff1b\u5b9e\u9645\u5b9e\u9a8c\u4e5f\u663e\u793a\u51fa25.8%\u7684\u6027\u80fd\u6539\u5584\u3002", "conclusion": "\u6570\u5b57\u5b6a\u751f\u6280\u672f\u652f\u6301\u7684\u81ea\u52a8\u5316\u53c2\u6570\u4f18\u5316\u662f\u63d0\u9ad8\u673a\u5668\u4eba\u64cd\u4f5c\u6027\u80fd\u7684\u91cd\u8981\u56e0\u7d20\u3002"}}
{"id": "2512.03774", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03774", "abs": "https://arxiv.org/abs/2512.03774", "authors": ["Johannes Fischer", "Marlon Steiner", "\u00d6mer Sahin Tas", "Christoph Stiller"], "title": "Safety Reinforced Model Predictive Control (SRMPC): Improving MPC with Reinforcement Learning for Motion Planning in Autonomous Driving", "comment": null, "summary": "Model predictive control (MPC) is widely used for motion planning, particularly in autonomous driving. Real-time capability of the planner requires utilizing convex approximation of optimal control problems (OCPs) for the planner. However, such approximations confine the solution to a subspace, which might not contain the global optimum. To address this, we propose using safe reinforcement learning (SRL) to obtain a new and safe reference trajectory within MPC. By employing a learning-based approach, the MPC can explore solutions beyond the close neighborhood of the previous one, potentially finding global optima. We incorporate constrained reinforcement learning (CRL) to ensure safety in automated driving, using a handcrafted energy function-based safety index as the constraint objective to model safe and unsafe regions. Our approach utilizes a state-dependent Lagrangian multiplier, learned concurrently with the safe policy, to solve the CRL problem. Through experimentation in a highway scenario, we demonstrate the superiority of our approach over both MPC and SRL in terms of safety and performance measures.", "AI": {"tldr": "\u7ed3\u5408\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u4e0e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\u5b89\u5168\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\u5e94\u7528\u53d7\u9650\u4e8e\u51f8\u8fd1\u4f3c\uff0c\u65e0\u6cd5\u5bfb\u627e\u5168\u5c40\u6700\u4f18\u89e3\u3002", "method": "\u91c7\u7528\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\uff08SRL\uff09\u548c\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\uff08CRL\uff09\uff0c\u5f15\u5165\u624b\u5de5\u8bbe\u8ba1\u7684\u80fd\u91cf\u51fd\u6570\u4f5c\u4e3a\u5b89\u5168\u7ea6\u675f\uff0c\u4ee5\u89e3\u51b3CRL\u95ee\u9898\u3002", "result": "\u63d0\u51fa\u7ed3\u5408\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\uff08SRL\uff09\u4e0eMPC\uff0c\u5229\u7528\u65b0\u7684\u5b89\u5168\u53c2\u8003\u8f68\u8ff9\u6539\u5584\u8fd0\u52a8\u89c4\u5212\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u548c\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u7684MPC\u548cSRL\u3002"}}
{"id": "2512.03795", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03795", "abs": "https://arxiv.org/abs/2512.03795", "authors": ["Jia Hu", "Zhexi Lian", "Xuerun Yan", "Ruiang Bi", "Dou Shen", "Yu Ruan", "Haoran Wang"], "title": "MPCFormer: A physics-informed data-driven approach for explainable socially-aware autonomous driving", "comment": "17 pages, 18 figures", "summary": "Autonomous Driving (AD) vehicles still struggle to exhibit human-like behavior in highly dynamic and interactive traffic scenarios. The key challenge lies in AD's limited ability to interact with surrounding vehicles, largely due to a lack of understanding the underlying mechanisms of social interaction. To address this issue, we introduce MPCFormer, an explainable socially-aware autonomous driving approach with physics-informed and data-driven coupled social interaction dynamics. In this model, the dynamics are formulated into a discrete space-state representation, which embeds physics priors to enhance modeling explainability. The dynamics coefficients are learned from naturalistic driving data via a Transformer-based encoder-decoder architecture. To the best of our knowledge, MPCFormer is the first approach to explicitly model the dynamics of multi-vehicle social interactions. The learned social interaction dynamics enable the planner to generate manifold, human-like behaviors when interacting with surrounding traffic. By leveraging the MPC framework, the approach mitigates the potential safety risks typically associated with purely learning-based methods. Open-looped evaluation on NGSIM dataset demonstrates that MPCFormer achieves superior social interaction awareness, yielding the lowest trajectory prediction errors compared with other state-of-the-art approach. The prediction achieves an ADE as low as 0.86 m over a long prediction horizon of 5 seconds. Close-looped experiments in highly intense interaction scenarios, where consecutive lane changes are required to exit an off-ramp, further validate the effectiveness of MPCFormer. Results show that MPCFormer achieves the highest planning success rate of 94.67%, improves driving efficiency by 15.75%, and reduces the collision rate from 21.25% to 0.5%, outperforming a frontier Reinforcement Learning (RL) based planner.", "AI": {"tldr": "MPCFormer\u662f\u4e00\u4e2a\u57fa\u4e8e\u7269\u7406\u4e0e\u6570\u636e\u7684\u793e\u4f1a\u4ea4\u4e92\u52a8\u6001\u5efa\u6a21\u7684\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u66f4\u7b26\u5408\u4eba\u7c7b\u884c\u4e3a\u7684\u4ea4\u4e92\uff0c\u5e76\u5728\u591a\u9879\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u52a8\u6001\u4ea4\u901a\u573a\u666f\u4e2d\u7684\u4eba\u7c7b\u884c\u4e3a\u8868\u73b0\uff0c\u5c24\u5176\u662f\u5728\u4e0e\u5468\u56f4\u8f66\u8f86\u7684\u4ea4\u4e92\u65b9\u9762\u3002", "method": "MPCFormer\uff0c\u4e00\u79cd\u5177\u6709\u7269\u7406\u77e5\u8bc6\u548c\u6570\u636e\u9a71\u52a8\u7684\u793e\u4f1a\u4ea4\u4e92\u52a8\u6001\u5efa\u6a21\u7684\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\u3002", "result": "MPCFormer\u5728NGSIM\u6570\u636e\u96c6\u4e0a\u7684\u5f00\u73af\u8bc4\u4f30\u663e\u793a\uff0c\u8f68\u8ff9\u9884\u6d4b\u8bef\u5dee\u6700\u4f4e\uff0cADE\u4e3a0.86\u7c73\u3002\u95ed\u73af\u5b9e\u9a8c\u4e2d\uff0c\u89c4\u5212\u6210\u529f\u7387\u4e3a94.67%\uff0c\u9a7e\u9a76\u6548\u7387\u63d0\u9ad815.75%\uff0c\u78b0\u649e\u7387\u4ece21.25%\u964d\u81f30.5%\u3002", "conclusion": "MPCFormer\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u591a\u8f66\u8f86\u793e\u4f1a\u4ea4\u4e92\u7684\u52a8\u6001\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u590d\u6742\u4ea4\u901a\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u964d\u4f4e\u4e86\u5b89\u5168\u98ce\u9669\u3002"}}
{"id": "2512.03828", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03828", "abs": "https://arxiv.org/abs/2512.03828", "authors": ["Dominykas Strazdas", "Magnus Jung", "Jan Marquenie", "Ingo Siegert", "Ayoub Al-Hamadi"], "title": "IM HERE: Interaction Model for Human Effort Based Robot Engagement", "comment": "8 pages, 5 figures", "summary": "The effectiveness of human-robot interaction often hinges on the ability to cultivate engagement - a dynamic process of cognitive involvement that supports meaningful exchanges. Many existing definitions and models of engagement are either too vague or lack the ability to generalize across different contexts. We introduce IM HERE, a novel framework that models engagement effectively in human-human, human-robot, and robot-robot interactions. By employing an effort-based description of bilateral relationships between entities, we provide an accurate breakdown of relationship patterns, simplifying them to focus placement and four key states. This framework captures mutual relationships, group behaviors, and actions conforming to social norms, translating them into specific directives for autonomous systems. By integrating both subjective perceptions and objective states, the model precisely identifies and describes miscommunication. The primary objective of this paper is to automate the analysis, modeling, and description of social behavior, and to determine how autonomous systems can behave in accordance with social norms for full social integration while simultaneously pursuing their own social goals.", "AI": {"tldr": "IM HERE\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5efa\u6a21\u4eba\u7c7b\u548c\u673a\u5668\u4eba\u4e92\u52a8\u4e2d\u7684\u53c2\u4e0e\u5ea6\uff0c\u5f3a\u8c03\u53cc\u8fb9\u5173\u7cfb\u548c\u793e\u4f1a\u884c\u4e3a\u3002", "motivation": "\u76ee\u524d\u5173\u4e8e\u53c2\u4e0e\u5ea6\u7684\u5b9a\u4e49\u548c\u6a21\u578b\u5b58\u5728\u6a21\u7cca\u6027\uff0c\u65e0\u6cd5\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5404\u79cd\u573a\u666f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u6709\u6548\u7684\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898.", "method": "\u91c7\u7528\u57fa\u4e8e\u52aa\u529b\u7684\u63cf\u8ff0\uff0c\u5206\u6790\u4e0d\u540c\u5b9e\u4f53\u95f4\u7684\u53cc\u8fb9\u5173\u7cfb\uff0c\u91cd\u70b9\u5173\u6ce8\u53c2\u4e0e\u5ea6\u548c\u56db\u4e2a\u5173\u952e\u72b6\u6001\u3002", "result": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86IM HERE\u6846\u67b6\uff0c\u65e8\u5728\u6709\u6548\u5efa\u6a21\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u95f4\u7684\u4e92\u52a8\u4e2d\u7684\u53c2\u4e0e\u5ea6\u3002", "conclusion": "IM HERE\u6846\u67b6\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9075\u5faa\u793e\u4f1a\u89c4\u8303\u7684\u660e\u786e\u6307\u4ee4\uff0c\u6709\u52a9\u4e8e\u5b83\u4eec\u5b9e\u73b0\u793e\u4f1a\u6574\u5408\u5e76\u8ffd\u6c42\u81ea\u8eab\u76ee\u6807\u3002"}}
{"id": "2512.03874", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03874", "abs": "https://arxiv.org/abs/2512.03874", "authors": ["Lei Zhang", "Diwen Zheng", "Kaixin Bai", "Zhenshan Bing", "Zoltan-Csaba Marton", "Zhaopeng Chen", "Alois Christian Knoll", "Jianwei Zhang"], "title": "OmniDexVLG: Learning Dexterous Grasp Generation from Vision Language Model-Guided Grasp Semantics, Taxonomy and Functional Affordance", "comment": "Project Website: https://sites.google.com/view/omnidexvlg, 16 pages", "summary": "Dexterous grasp generation aims to produce grasp poses that align with task requirements and human interpretable grasp semantics. However, achieving semantically controllable dexterous grasp synthesis remains highly challenging due to the lack of unified modeling of multiple semantic dimensions, including grasp taxonomy, contact semantics, and functional affordance. To address these limitations, we present OmniDexVLG, a multimodal, semantics aware grasp generation framework capable of producing structurally diverse and semantically coherent dexterous grasps under joint language and visual guidance. Our approach begins with OmniDexDataGen, a semantic rich dexterous grasp dataset generation pipeline that integrates grasp taxonomy guided configuration sampling, functional affordance contact point sampling, taxonomy aware differential force closure grasp sampling, and physics based optimization and validation, enabling systematic coverage of diverse grasp types. We further introduce OmniDexReasoner, a multimodal grasp type semantic reasoning module that leverages multi agent collaboration, retrieval augmented generation, and chain of thought reasoning to infer grasp related semantics and generate high quality annotations that align language instructions with task specific grasp intent. Building upon these components, we develop a unified Vision Language Grasping generation model that explicitly incorporates grasp taxonomy, contact structure, and functional affordance semantics, enabling fine grained control over grasp synthesis from natural language instructions. Extensive experiments in simulation and real world object grasping and ablation studies demonstrate that our method substantially outperforms state of the art approaches in terms of grasp diversity, contact semantic diversity, functional affordance diversity, and semantic consistency.", "AI": {"tldr": "OmniDexVLG\u662f\u4e00\u4e2a\u65b0\u578b\u7684\u8bed\u4e49\u611f\u77e5\u6293\u53d6\u751f\u6210\u6846\u67b6\uff0c\u80fd\u591f\u6839\u636e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u751f\u6210\u591a\u6837\u5316\u4e14\u4e00\u81f4\u7684\u7075\u5de7\u6293\u53d6\u3002", "motivation": "\u5b9e\u73b0\u8bed\u4e49\u53ef\u63a7\u7684\u7075\u5de7\u6293\u53d6\u5408\u6210\u7531\u4e8e\u7f3a\u4e4f\u591a\u8bed\u4e49\u7ef4\u5ea6\u7684\u7edf\u4e00\u5efa\u6a21\u800c\u9762\u4e34\u6311\u6218\uff0c\u5982\u6293\u53d6\u5206\u7c7b\u3001\u63a5\u89e6\u8bed\u4e49\u548c\u529f\u80fd\u8d4b\u6743\u3002", "method": "\u8be5\u6846\u67b6\u901a\u8fc7OmniDexDataGen\u751f\u6210\u8bed\u4e49\u4e30\u5bcc\u7684\u6293\u53d6\u6570\u636e\u96c6\uff0c\u5e76\u5f15\u5165OmniDexReasoner\u8fdb\u884c\u591a\u6a21\u6001\u8bed\u4e49\u63a8\u7406\uff0c\u6700\u7ec8\u6784\u5efa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u89c6\u89c9\u8bed\u8a00\u6293\u53d6\u751f\u6210\u6a21\u578b\u3002", "result": "\u63d0\u4ea4\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u3001\u8bed\u4e49\u611f\u77e5\u7684\u6293\u53d6\u751f\u6210\u6846\u67b6OmniDexVLG\uff0c\u80fd\u591f\u5728\u8bed\u8a00\u548c\u89c6\u89c9\u6307\u5bfc\u4e0b\u751f\u6210\u7ed3\u6784\u591a\u6837\u4e14\u8bed\u4e49\u4e00\u81f4\u7684\u7075\u5de7\u6293\u53d6\u3002", "conclusion": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6293\u53d6\u591a\u6837\u6027\u3001\u63a5\u89e6\u8bed\u4e49\u591a\u6837\u6027\u3001\u529f\u80fd\u8d4b\u6743\u591a\u6837\u6027\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2512.03886", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.03886", "abs": "https://arxiv.org/abs/2512.03886", "authors": ["Brais Fontan-Costas", "M. Diaz-Cacho", "Ruben Fernandez-Boullon", "Manuel Alonso-Carracedo", "Javier Perez-Robles"], "title": "A Modular Architecture Design for Autonomous Driving Racing in Controlled Environments", "comment": null, "summary": "This paper presents an Autonomous System (AS) architecture for vehicles in a closed circuit. The AS performs precision tasks including computer vision for environment perception, positioning and mapping for accurate localization, path planning for optimal trajectory generation, and control for precise vehicle actuation. Each subsystem operates independently while connecting data through a cohesive pipeline architecture. The system implements a modular design that combines state-of-the-art technologies for real-time autonomous navigation in controlled environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5c01\u95ed\u7535\u8def\u8f66\u8f86\u7684\u81ea\u4e3b\u7cfb\u7edf\u67b6\u6784\uff0c\u96c6\u6210\u591a\u79cd\u6280\u672f\u5b9e\u73b0\u7cbe\u786e\u5bfc\u822a\u548c\u63a7\u5236\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63d0\u5347\u8f66\u8f86\u5728\u5c01\u95ed\u7535\u8def\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u80fd\u529b\uff0c\u4ee5\u5b9e\u73b0\u7cbe\u786e\u4efb\u52a1\u6267\u884c\u3002", "method": "\u91c7\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u5b9a\u4f4d\u548c\u5730\u56fe\u7ed8\u5236\u3001\u8def\u5f84\u89c4\u5212\u4ee5\u53ca\u63a7\u5236\u7b49\u5b50\u7cfb\u7edf\u8fdb\u884c\u64cd\u4f5c\u3002", "result": "\u901a\u8fc7\u5148\u8fdb\u7684\u6280\u672f\u7ec4\u5408\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u5728\u53d7\u63a7\u73af\u5883\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u81ea\u4e3b\u5bfc\u822a\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u7684\u81ea\u4e3b\u7cfb\u7edf\u67b6\u6784\uff0c\u80fd\u591f\u5b9e\u73b0\u7cbe\u786e\u7684\u8f66\u8f86\u63a7\u5236\u548c\u5b9e\u65f6\u5bfc\u822a\u3002"}}
{"id": "2512.03891", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03891", "abs": "https://arxiv.org/abs/2512.03891", "authors": ["Ying-Kuan Tsai", "Yi-Ping Chen", "Vispi Karkaria", "Wei Chen"], "title": "Digital Twin-based Control Co-Design of Full Vehicle Active Suspensions via Deep Reinforcement Learning", "comment": "28 pages, 17 figures", "summary": "Active suspension systems are critical for enhancing vehicle comfort, safety, and stability, yet their performance is often limited by fixed hardware designs and control strategies that cannot adapt to uncertain and dynamic operating conditions. Recent advances in digital twins (DTs) and deep reinforcement learning (DRL) offer new opportunities for real-time, data-driven optimization across a vehicle's lifecycle. However, integrating these technologies into a unified framework remains an open challenge. This work presents a DT-based control co-design (CCD) framework for full-vehicle active suspensions using multi-generation design concepts. By integrating automatic differentiation into DRL, we jointly optimize physical suspension components and control policies under varying driver behaviors and environmental uncertainties. DRL also addresses the challenge of partial observability, where only limited states can be sensed and fed back to the controller, by learning optimal control actions directly from available sensor information. The framework incorporates model updating with quantile learning to capture data uncertainty, enabling real-time decision-making and adaptive learning from digital-physical interactions. The approach demonstrates personalized optimization of suspension systems under two distinct driving settings (mild and aggressive). Results show that the optimized systems achieve smoother trajectories and reduce control efforts by approximately 43% and 52% for mild and aggressive, respectively, while maintaining ride comfort and stability. Contributions include: developing a DT-enabled CCD framework integrating DRL and uncertainty-aware model updating for full-vehicle active suspensions, introducing a multi-generation design strategy for self-improving systems, and demonstrating personalized optimization of active suspension systems for distinct driver types.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6570\u5b57\u53cc\u80de\u80ce\u7684\u63a7\u5236\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u4f18\u5316\u5168\u8f66\u4e3b\u52a8\u60ac\u6302\u7cfb\u7edf\u3002", "motivation": "\u63d0\u5347\u8f66\u8f86\u8212\u9002\u6027\u3001\u5b89\u5168\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u540c\u65f6\u514b\u670d\u56fa\u5b9a\u786c\u4ef6\u8bbe\u8ba1\u548c\u63a7\u5236\u7b56\u7565\u7684\u9650\u5236\u3002", "method": "DT-based control co-design (CCD) framework for active suspensions", "result": "\u4f18\u5316\u7684\u60ac\u6302\u7cfb\u7edf\u5728\u4e24\u79cd\u9a7e\u9a76\u6761\u4ef6\u4e0b\uff08\u6e29\u548c\u548c\u6fc0\u8fdb\uff09\u5b9e\u73b0\u4e86\u66f4\u5e73\u7a33\u7684\u884c\u9a76\u8f68\u8ff9\uff0c\u5e76\u5c06\u63a7\u5236\u52aa\u529b\u5206\u522b\u51cf\u5c11\u7ea643%\u548c52%\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5168\u8f66\u4e3b\u52a8\u60ac\u6302\u7cfb\u7edf\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86DRL\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6a21\u578b\u66f4\u65b0\uff0c\u6709\u52a9\u4e8e\u5728\u53d8\u5316\u7684\u9a7e\u9a76\u884c\u4e3a\u548c\u73af\u5883\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e2a\u6027\u5316\u4f18\u5316\u3002"}}
{"id": "2512.03911", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.03911", "abs": "https://arxiv.org/abs/2512.03911", "authors": ["Kenneth Stewart", "Roxana Leontie", "Samantha Chapin", "Joe Hays", "Sumit Bam Shrestha", "Carl Glen Henshaw"], "title": "Autonomous Reinforcement Learning Robot Control with Intel's Loihi 2 Neuromorphic Hardware", "comment": "Submitted for review at NICE 2026 (Neuro-Inspired Computational Elements) conference", "summary": "We present an end-to-end pipeline for deploying reinforcement learning (RL) trained Artificial Neural Networks (ANNs) on neuromorphic hardware by converting them into spiking Sigma-Delta Neural Networks (SDNNs). We demonstrate that an ANN policy trained entirely in simulation can be transformed into an SDNN compatible with Intel's Loihi 2 architecture, enabling low-latency and energy-efficient inference. As a test case, we use an RL policy for controlling the Astrobee free-flying robot, similar to a previously hardware in space-validated controller. The policy, trained with Rectified Linear Units (ReLUs), is converted to an SDNN and deployed on Intel's Loihi 2, then evaluated in NVIDIA's Omniverse Isaac Lab simulation environment for closed-loop control of Astrobee's motion. We compare execution performance between GPU and Loihi 2. The results highlight the feasibility of using neuromorphic platforms for robotic control and establish a pathway toward energy-efficient, real-time neuromorphic computation in future space and terrestrial robotics applications.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5c06\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684ANN\u8f6c\u5316\u4e3a\u9002\u7528\u4e8e\u795e\u7ecf\u5f62\u6001\u786c\u4ef6SDNN\u7684\u7aef\u5230\u7aef\u6d41\u7a0b\uff0c\u5c55\u793a\u5728Astrobee\u673a\u5668\u4eba\u4e0a\u7684\u5e94\u7528\uff0c\u63d0\u5347\u4e86\u80fd\u6548\u548c\u5b9e\u65f6\u6027\u3002", "motivation": "\u63a2\u8ba8\u5982\u4f55\u5c06\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u6709\u6548\u5730\u90e8\u7f72\u5230\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u4e0a\uff0c\u4ee5\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u80fd\u6548\u7684\u63a8\u7406\u64cd\u4f5c\u3002", "method": "\u901a\u8fc7\u5c06\u5728\u6a21\u62df\u4e2d\u8bad\u7ec3\u7684ANN\u653f\u7b56\u8f6c\u5316\u4e3a\u8109\u51b2Sigma-Delta\u795e\u7ecf\u7f51\u7edc\uff0c\u90e8\u7f72\u4e8eIntel Loihi 2\u4e0a\uff0c\u5e76\u5728NVIDIA\u7684Omniverse Isaac Lab\u73af\u5883\u4e2d\u8fdb\u884c\u95ed\u73af\u63a7\u5236\u8bc4\u4f30\u3002", "result": "\u6210\u529f\u5c06\u8bad\u7ec3\u7684ANN\u653f\u7b56\u8f6c\u5316\u4e3a\u517c\u5bb9Intel Loihi 2\u67b6\u6784\u7684\u8109\u51b2Sigma-Delta\u795e\u7ecf\u7f51\u7edc\uff0c\u5e76\u5728\u6a21\u62df\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6267\u884c\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u5229\u7528\u795e\u7ecf\u5f62\u6001\u5e73\u53f0\u8fdb\u884c\u673a\u5668\u4eba\u63a7\u5236\u662f\u53ef\u884c\u7684\uff0c\u4e3a\u672a\u6765\u7684\u7a7a\u95f4\u4e0e\u5730\u9762\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u80fd\u6548\u9ad8\u3001\u5b9e\u65f6\u7684\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u8def\u5f84\u3002"}}
{"id": "2512.03913", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03913", "abs": "https://arxiv.org/abs/2512.03913", "authors": ["Jeongeun Park", "Jihwan Yoon", "Byungwoo Jeon", "Juhan Park", "Jinwoo Shin", "Namhoon Cho", "Kyungjae Lee", "Sangdoo Yun", "Sungjoon Choi"], "title": "Hierarchical Vision Language Action Model Using Success and Failure Demonstrations", "comment": "https://vine-vla.github.io/", "summary": "Prior Vision-Language-Action (VLA) models are typically trained on teleoperated successful demonstrations, while discarding numerous failed attempts that occur naturally during data collection. However, these failures encode where and how policies can be fragile, information that can be exploited to improve robustness. We address this problem by leveraging mixed-quality datasets to learn failure-aware reasoning at planning time. We introduce VINE, a hierarchical vision-language-action model that separates high-level reasoning (System 2) from low-level control (System 1) under a hierarchical reinforcement learning formalism, making failures usable as a structured learning signal rather than noisy supervision. System 2 performs feasibility-guided tree search over a 2D scene-graph abstraction: it proposes subgoal transitions, predicts success probabilities from both successes and failures, and prunes brittle branches before execution, effectively casting plan evaluation as feasibility scoring. The selected subgoal sequence is then passed to System 1, which executes low-level actions without modifying the agent's core skills. Trained entirely from offline teleoperation data, VINE integrates negative experience directly into the decision loop. Across challenging manipulation tasks, this approach consistently improves success rates and robustness, demonstrating that failure data is an essential resource for converting the broad competence of VLAs into robust execution.", "AI": {"tldr": "VINE\u662f\u4e00\u4e2a\u5c42\u6b21\u5316\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u5229\u7528\u5931\u8d25\u6570\u636e\u63d0\u5347\u89c4\u5212\u548c\u6267\u884c\u8fc7\u7a0b\u7684\u9c81\u68d2\u6027\u3002\u5728\u5c42\u6b21\u5316\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e0b\uff0c\u5b83\u5c06\u9ad8\u5c42\u63a8\u7406\u4e0e\u4f4e\u5c42\u63a7\u5236\u5206\u79bb\uff0c\u901a\u8fc7\u5bf9\u6570\u636e\u96c6\u4e2d\u7684\u6210\u529f\u4e0e\u5931\u8d25\u8fdb\u884c\u5145\u5206\u5229\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5229\u7528\u5931\u8d25\u793a\u4f8b\u4e2d\u7684\u4fe1\u606f\u6765\u6539\u5584\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u8106\u5f31\u6027\uff0c\u4ece\u800c\u63d0\u5347\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002", "method": "\u8be5\u6a21\u578b\u5728\u5c42\u6b21\u5316\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e0b\u8fd0\u4f5c\uff0c\u9ad8\u5c42\u8fdb\u884c\u53ef\u884c\u6027\u5f15\u5bfc\u7684\u6811\u641c\u7d22\uff0c\u7ed3\u5408\u6210\u529f\u4e0e\u5931\u8d25\u7684\u6982\u7387\u9884\u6d4b\uff0c\u4f4e\u5c42\u5219\u6267\u884c\u5177\u4f53\u52a8\u4f5c\u3002", "result": "\u5728\u590d\u6742\u7684\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u901a\u8fc7\u5f15\u5165\u5931\u8d25\u6570\u636e\uff0cVINE\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u6709\u6548\u5229\u7528\u5931\u8d25\u6570\u636e\uff0cVINE\u5c55\u793a\u4e86\u5728\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u4e2d\u63d0\u5347\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\u7684\u6f5c\u529b\uff0c\u8bc1\u660e\u4e86\u5931\u8d25\u4fe1\u606f\u7684\u4ef7\u503c\u3002"}}
{"id": "2512.03936", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03936", "abs": "https://arxiv.org/abs/2512.03936", "authors": ["Aron Distelzweig", "Yiwei Wang", "Faris Janjo\u0161", "Marcel Hallgarten", "Mihai Dobre", "Alexander Langmann", "Joschka Boedecker", "Johannes Betz"], "title": "Driving is a Game: Combining Planning and Prediction with Bayesian Iterative Best Response", "comment": null, "summary": "Autonomous driving planning systems perform nearly perfectly in routine scenarios using lightweight, rule-based methods but still struggle in dense urban traffic, where lane changes and merges require anticipating and influencing other agents. Modern motion predictors offer highly accurate forecasts, yet their integration into planning is mostly rudimental: discarding unsafe plans. Similarly, end-to-end models offer a one-way integration that avoids the challenges of joint prediction and planning modeling under uncertainty. In contrast, game-theoretic formulations offer a principled alternative but have seen limited adoption in autonomous driving. We present Bayesian Iterative Best Response (BIBeR), a framework that unifies motion prediction and game-theoretic planning into a single interaction-aware process. BIBeR is the first to integrate a state-of-the-art predictor into an Iterative Best Response (IBR) loop, repeatedly refining the strategies of the ego vehicle and surrounding agents. This repeated best-response process approximates a Nash equilibrium, enabling bidirectional adaptation where the ego both reacts to and shapes the behavior of others. In addition, our proposed Bayesian confidence estimation quantifies prediction reliability and modulates update strength, more conservative under low confidence and more decisive under high confidence. BIBeR is compatible with modern predictors and planners, combining the transparency of structured planning with the flexibility of learned models. Experiments show that BIBeR achieves an 11% improvement over state-of-the-art planners on highly interactive interPlan lane-change scenarios, while also outperforming existing approaches on standard nuPlan benchmarks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u4e3b\u9a7e\u9a76\u89c4\u5212\u6846\u67b6BIBeR\uff0c\u7ed3\u5408\u4e86\u8fd0\u52a8\u9884\u6d4b\u548c\u535a\u5f08\u7406\u8bba\u89c4\u5212\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u590d\u6742\u57ce\u5e02\u4ea4\u901a\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u81ea\u4e3b\u9a7e\u9a76\u7cfb\u7edf\u5728\u5bc6\u96c6\u57ce\u5e02\u4ea4\u901a\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u53d8\u5316\u8f66\u9053\u548c\u5408\u5e76\u8fc7\u7a0b\u4e2d\uff0c\u9700\u8981\u66f4\u597d\u5730\u9884\u6d4b\u548c\u5f71\u54cd\u5176\u4ed6\u4ea4\u901a\u53c2\u4e0e\u8005\u3002", "method": "BIBeR\u901a\u8fc7\u5c06\u5148\u8fdb\u7684\u8fd0\u52a8\u9884\u6d4b\u5668\u6574\u5408\u8fdb\u4e00\u4e2a\u8fed\u4ee3\u6700\u4f73\u54cd\u5e94\u5faa\u73af\u4e2d\uff0c\u53cd\u590d\u4f18\u5316\u81ea\u6211\u8f66\u8f86\u53ca\u5468\u56f4\u4ee3\u7406\u7684\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBIBeR\u5728\u9ad8\u5ea6\u4e92\u52a8\u7684\u8f66\u9053\u53d8\u6362\u573a\u666f\u4e0a\u6bd4\u6700\u5148\u8fdb\u7684\u89c4\u5212\u8005\u63d0\u9ad8\u4e8611%\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728nuPlan\u57fa\u51c6\u4e0a\u4e5f\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "BIBeR\u6846\u67b6\u5728\u9ad8\u5ea6\u4e92\u52a8\u7684\u4ea4\u901a\u573a\u666f\u4e2d\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u63d0\u5347\u4e86\u81ea\u4e3b\u9a7e\u9a76\u7cfb\u7edf\u7684\u51b3\u7b56\u80fd\u529b\u3002"}}
{"id": "2512.03958", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.03958", "abs": "https://arxiv.org/abs/2512.03958", "authors": ["Xiaobei Zhao", "Xingqi Lyu", "Xiang Li"], "title": "MDE-AgriVLN: Agricultural Vision-and-Language Navigation with Monocular Depth Estimation", "comment": null, "summary": "Agricultural robots are serving as powerful assistants across a wide range of agricultural tasks, nevertheless, still heavily relying on manual operations or railway systems for movement. The AgriVLN method and the A2A benchmark pioneeringly extend Vision-and-Language Navigation (VLN) to the agricultural domain, enabling a robot to navigate to a target position following a natural language instruction. Unlike human binocular vision, most agricultural robots are only given a single camera for monocular vision, which results in limited spatial perception. To bridge this gap, we present the method of Agricultural Vision-and-Language Navigation with Monocular Depth Estimation (MDE-AgriVLN), in which we propose the MDE module generating depth features from RGB images, to assist the decision-maker on reasoning. When evaluated on the A2A benchmark, our MDE-AgriVLN method successfully increases Success Rate from 0.23 to 0.32 and decreases Navigation Error from 4.43m to 4.08m, demonstrating the state-of-the-art performance in the agricultural VLN domain. Code: https://github.com/AlexTraveling/MDE-AgriVLN.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86MDE-AgriVLN\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u589e\u5f3a\u519c\u4e1a\u673a\u5668\u4eba\u5728\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4e0b\u7684\u5bfc\u822a\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u519c\u4e1a\u673a\u5668\u4eba\u5728\u5404\u79cd\u519c\u4e1a\u4efb\u52a1\u4e2d\u53d1\u6325\u7740\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u4ecd\u7136\u4f9d\u8d56\u4eba\u5de5\u64cd\u4f5c\u6216\u94c1\u8def\u7cfb\u7edf\u8fdb\u884c\u79fb\u52a8\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u519c\u4e1a\u89c6\u89c9\u4e0e\u8bed\u8a00\u5bfc\u822a\u65b9\u6cd5\uff0c\u7ed3\u5408\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u6a21\u5757\uff0c\u4eceRGB\u56fe\u50cf\u751f\u6210\u6df1\u5ea6\u7279\u5f81\u3002", "result": "MDE-AgriVLN\u65b9\u6cd5\u5728A2A\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6210\u529f\u5c06\u6210\u529f\u7387\u4ece0.23\u63d0\u9ad8\u52300.32\uff0c\u5bfc\u822a\u8bef\u5dee\u4ece4.43\u7c73\u964d\u4f4e\u52304.08\u7c73\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u519c\u4e1a\u89c6\u89c9-\u8bed\u8a00\u5bfc\u822a\u9886\u57df\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u63a8\u52a8\u4e86\u519c\u4e1a\u673a\u5668\u4eba\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.03995", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.03995", "abs": "https://arxiv.org/abs/2512.03995", "authors": ["Levi Burner", "Guido de Croon", "Yiannis Aloimonos"], "title": "Artificial Microsaccade Compensation: Stable Vision for an Ornithopter", "comment": "29 pages, 5 figures, 2 tables, under review", "summary": "Animals with foveated vision, including humans, experience microsaccades, small, rapid eye movements that they are not aware of. Inspired by this phenomenon, we develop a method for \"Artificial Microsaccade Compensation\". It can stabilize video captured by a tailless ornithopter that has resisted attempts to use camera-based sensing because it shakes at 12-20 Hz. Our approach minimizes changes in image intensity by optimizing over 3D rotation represented in SO(3). This results in a stabilized video, computed in real time, suitable for human viewing, and free from distortion. When adapted to hold a fixed viewing orientation, up to occasional saccades, it can dramatically reduce inter-frame motion while also benefiting from an efficient recursive update. When compared to Adobe Premier Pro's warp stabilizer, which is widely regarded as the best commercial video stabilization software available, our method achieves higher quality results while also running in real time.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u7a33\u5b9a\u6280\u672f\uff0c\u80fd\u6709\u6548\u8865\u507f\u9ad8\u901f\u9707\u52a8\u5bfc\u81f4\u7684\u6296\u52a8\uff0c\u5b9e\u65f6\u5904\u7406\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u8f6f\u4ef6\u3002", "motivation": "\u53d7\u52a8\u7269\u5fae\u98a4\u52a8\u73b0\u8c61\u542f\u53d1\uff0c\u6211\u4eec\u5e0c\u671b\u6539\u5584\u89c6\u9891\u7a33\u5b9a\u6027\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u901f\u9707\u52a8\u73af\u5883\u4e0b\u3002", "method": "\u901a\u8fc7\u4f18\u5316SO(3)\u8868\u793a\u76843D\u65cb\u8f6c\uff0c\u6700\u5c0f\u5316\u56fe\u50cf\u5f3a\u5ea6\u53d8\u5316\uff0c\u5b9e\u73b0\u5b9e\u65f6\u7a33\u5b9a\u89c6\u9891\u3002", "result": "\u53d1\u5c55\u4e86\u4e00\u79cd\u65b0\u7684\u201c\u4eba\u5de5\u5fae\u98a4\u52a8\u8865\u507f\u201d\u65b9\u6cd5\uff0c\u80fd\u5b9e\u73b0\u5b9e\u65f6\u89c6\u9891\u7a33\u5b9a\u5904\u7406\uff0c\u6548\u679c\u8d85\u8d8a\u5f53\u524d\u6700\u4f73\u5546\u4e1a\u8f6f\u4ef6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5b9a\u5411\u7a33\u5b9a\u89c6\u9891\uff0c\u540c\u65f6\u80fd\u591f\u5904\u7406\u5076\u5c14\u7684\u5feb\u901f\u8fd0\u52a8\uff0c\u663e\u8457\u964d\u4f4e\u5e27\u95f4\u8fd0\u52a8\uff0c\u6548\u679c\u7406\u60f3\u3002"}}
