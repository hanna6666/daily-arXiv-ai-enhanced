<div id=toc></div>

# Table of Contents

- [cs.HC](#cs.HC) [Total: 11]
- [cs.RO](#cs.RO) [Total: 28]


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [1] [Modeling multi-agent motion dynamics in immersive rooms](https://arxiv.org/abs/2511.08763)
*Mincong,Huang,Stefan T. Radev*

Main category: cs.HC

TL;DR: 本研究提出一种基于代理的模型，探讨沉浸式房间内人类的运动动态，利用模拟推断显示运动模式的控制参数可通过简单观测估计，并为响应式沉浸式房间的创建提供了方法。


<details>
  <summary>Details</summary>
Motivation: 尽管技术发展迅速，但对于沉浸式房间内的社交动态，特别是虚拟世界导航中的复杂运动模式，研究仍然较少。

Method: 提出了一种新的基于代理的模型，通过模拟外部扩散驱动的影响和内部自推动的代理间交互，来捕捉人类动作动态。

Result: 模型成功捕捉了与行动相关的代理属性，但暴露了与环境意识相关的局部非可识别性。

Conclusion: 我们的模拟驱动方法为创建适应性强、响应迅速的沉浸式房间铺平了道路，这些空间会根据人类的集体运动模式和空间注意力调整其接口和交互方式。

Abstract: Immersive rooms are increasingly popular augmented reality systems that support multi-agent interactions within a virtual world. However, despite extensive content creation and technological developments, insights about perceptually-driven social dynamics, such as the complex movement patterns during virtual world navigation, remain largely underexplored. Computational models of motion dynamics can help us understand the underlying mechanism of human interaction in immersive rooms and develop applications that better support spatially distributed interaction. In this work, we propose a new agent-based model of emergent human motion dynamics. The model represents human agents as simple spatial geometries in the room that relocate and reorient themselves based on the salient virtual spatial objects they approach. Agent motion is modeled as an interactive process combining external diffusion-driven influences from the environment with internal self-propelling interactions among agents. Further, we leverage simulation-based inference (SBI) to show that the governing parameters of motion patterns can be estimated from simple observables. Our results indicate that the model successfully captures action-related agent properties but exposes local non-identifiability linked to environmental awareness. We argue that our simulation-based approach paves the way for creating adaptive, responsive immersive rooms -- spaces that adjust their interfaces and interactions based on human collective movement patterns and spatial attention.

</details>


### [2] [Simulating Psychological Risks in Human-AI Interactions: Real-Case Informed Modeling of AI-Induced Addiction, Anorexia, Depression, Homicide, Psychosis, and Suicide](https://arxiv.org/abs/2511.08880)
*Chayapatr Archiwaranguprok,Constanze Albrecht,Pattie Maes,Karrie Karahalios,Pat Pataranutaporn*

Main category: cs.HC

TL;DR: 本文提出了一种新方法，系统研究人类与AI互动中的心理风险，并分析AI可能导致的心理健康问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统逐渐融入日常生活，其对心理健康的潜在影响尚未得到充分理解和测试，因此需要更系统的方法来识别相关风险。

Method: 通过分析18个真实案例，提取有害互动模式，并使用2160个模拟场景进行风险评估。

Result: 本研究通过系统方法探索人类与AI互动中可能出现的心理风险，分析了AI引发或加剧的心理健康问题，为识别和评估这些风险提供了实证依据。

Conclusion: 本研究提供了一种新颖的方法来识别心理风险，为高风险人类-AI互动中的有害响应模式分类，并揭示系统的常见失效模式。

Abstract: As AI systems become increasingly integrated into daily life, their potential to exacerbate or trigger severe psychological harms remains poorly understood and inadequately tested. This paper presents a proactive methodology for systematically exploring psychological risks in simulated human-AI interactions based on documented real-world cases involving AI-induced or AI-exacerbated addiction, anorexia, depression, homicide, psychosis, and suicide. We collected and analyzed 18 reported real-world cases where AI interactions contributed to severe psychological outcomes. From these cases, we developed a process to extract harmful interaction patterns and assess potential risks through 2,160 simulated scenarios using clinical staging models. We tested four major LLMs across multi-turn conversations to identify where psychological risks emerge: which harm domains, conversation stages, and contexts reveal system vulnerabilities. Through the analysis of 157,054 simulated conversation turns, we identify critical gaps in detecting psychological distress, responding appropriately to vulnerable users, and preventing harm escalation. Regression analysis reveals variability across persona types: LLMs tend to perform worse with elderly users but better with low- and middle-income groups compared to high-income groups. Clustering analysis of harmful responses reveals a taxonomy of fifteen distinct failure patterns organized into four categories of AI-enabled harm. This work contributes a novel methodology for identifying psychological risks, empirical evidence of common failure modes across systems, and a classification of harmful AI response patterns in high-stakes human-AI interactions.

</details>


### [3] ["It's trained by non-disabled people": Evaluating How Image Quality Affects Product Captioning with VLMs](https://arxiv.org/abs/2511.08917)
*Kapil Garg,Xinru Tang,Jimin Heo,Dwayne R. Morgan,Darren Gergle,Erik B. Sudderth,Anne Marie Piper*

Main category: cs.HC

TL;DR: 图像质量问题显著影响视觉语言模型生成的描述文本的准确性，尤其是在多重问题存在的情况下。


<details>
  <summary>Details</summary>
Motivation: 了解视觉语言模型在盲人及低视力人士日常生活中的应用效果，特别是图像质量对生成说明的影响。

Method: 通过对86名盲人及低视力人士进行调查，系统评估图像质量问题如何影响视觉语言模型生成的文本。

Result: 发现最佳模型在无质量问题的图像中准确率达到98%，而在存在质量问题时整体准确率下降至75%。

Conclusion: 强调在模型评估中应考虑残疾人群的体验，并为人机交互和机器学习研究人员提出建议，以提高视觉语言模型的可靠性。

Abstract: Vision-Language Models (VLMs) are increasingly used by blind and low-vision (BLV) people to identify and understand products in their everyday lives, such as food, personal products, and household goods. Despite their prevalence, we lack an empirical understanding of how common image quality issues, like blur and misframing of items, affect the accuracy of VLM-generated captions and whether resulting captions meet BLV people's information needs. Grounded in a survey with 86 BLV people, we systematically evaluate how image quality issues affect captions generated by VLMs. We show that the best model recognizes products in images with no quality issues with 98% accuracy, but drops to 75% accuracy overall when quality issues are present, worsening considerably as issues compound. We discuss the need for model evaluations that center on disabled people's experiences throughout the process and offer concrete recommendations for HCI and ML researchers to make VLMs more reliable for BLV people.

</details>


### [4] [Plug-and-Play Clarifier: A Zero-Shot Multimodal Framework for Egocentric Intent Disambiguation](https://arxiv.org/abs/2511.08971)
*Sicheng Yang,Yukai Huang,Weitong Cai,Shitong Sun,You He,Jiankang Deng,Hang Zhang,Jifei Song,Zhensong Zhang*

Main category: cs.HC

TL;DR: 提出一种模块化框架Plug-and-Play Clarifier，通过分解多模态意图歧义问题，提高语言模型性能和用户交互体验。


<details>
  <summary>Details</summary>
Motivation: 为了应对多模态意图歧义问题，提升自我中心AI代理的任务成功率。

Method: 提出的框架包含三个模块：文本澄清器、视觉澄清器和跨模态澄清器，分别针对语言意图、视觉反馈和手势识别进行处理。

Result: 通过实验，结果表明该框架使小型语言模型的意图澄清性能提高约30%，并且在大型模型上也有显著提升。

Conclusion: 该方法有效解决了多模态歧义并显著提升了用户体验。

Abstract: The performance of egocentric AI agents is fundamentally limited by multimodal intent ambiguity. This challenge arises from a combination of underspecified language, imperfect visual data, and deictic gestures, which frequently leads to task failure. Existing monolithic Vision-Language Models (VLMs) struggle to resolve these multimodal ambiguous inputs, often failing silently or hallucinating responses. To address these ambiguities, we introduce the Plug-and-Play Clarifier, a zero-shot and modular framework that decomposes the problem into discrete, solvable sub-tasks. Specifically, our framework consists of three synergistic modules: (1) a text clarifier that uses dialogue-driven reasoning to interactively disambiguate linguistic intent, (2) a vision clarifier that delivers real-time guidance feedback, instructing users to adjust their positioning for improved capture quality, and (3) a cross-modal clarifier with grounding mechanism that robustly interprets 3D pointing gestures and identifies the specific objects users are pointing to. Extensive experiments demonstrate that our framework improves the intent clarification performance of small language models (4--8B) by approximately 30%, making them competitive with significantly larger counterparts. We also observe consistent gains when applying our framework to these larger models. Furthermore, our vision clarifier increases corrective guidance accuracy by over 20%, and our cross-modal clarifier improves semantic answer accuracy for referential grounding by 5%. Overall, our method provides a plug-and-play framework that effectively resolves multimodal ambiguity and significantly enhances user experience in egocentric interaction.

</details>


### [5] [SimPath: Mitigating Motion Sickness in In-vehicle Infotainment Systems via Driving Condition Adaptation](https://arxiv.org/abs/2511.09240)
*Jinghao Huang,Siqi Yao,Yu Zhang*

Main category: cs.HC

TL;DR: 本研究提出了SimPath设计以减轻乘客晕动症，验证了其有效性，并对未来智能用户友好的IVIS设计提供了支持。


<details>
  <summary>Details</summary>
Motivation: 解决乘客在使用车载信息娱乐系统时的晕动症问题，以提升车辆内的舒适度和功能效率。

Method: 开展了两个真实驾驶场景的实验，以验证SimPath设计在减轻乘客晕动症方面的有效性。

Result: 实验结果表明，该方法能显著降低乘客的晕动症水平，但由于视觉内容带来的分散注意力，未能直接提高IVIS的使用效率。

Conclusion: 本研究为未来自动驾驶车辆中的IVIS设计提供了重要的理论支持和实践指导。

Abstract: The problem of Motion Sickness (MS) among passengers significantly impacts the comfort and efficiency of In-Vehicle Infotainment Systems (IVIS) use. In this study, we innovatively designed SimPath, a visual design to effectively mitigate passengers' MS and boost their efficiency of using IVIS during driving. The study focuses on the problem of irregular motion conditions frequently encountered during actual driving. To validate the efficacy of this approach, two sets of real - vehicle experiments were carried out in real driving scenarios. The results demonstrate that this approach significantly reduces passenger's MS level to a certain extent. However, due to divided attention from visual content, it does not directly improve the IVIS efficiency. In conclusion, this study offers crucial insights for the design of a more intelligent and user friendly IVIS, based on the discussion of the principle, providing strong theoretical support and practical guidance for the development of future IVIS in autonomous vehicles.

</details>


### [6] [TaskSense: Cognitive Chain Modeling and Difficulty Estimation for GUI Tasks](https://arxiv.org/abs/2511.09309)
*Yiwen Yin,Zhian Hu,Xiaoxi Xu,Chun Yu,Xintong Wu,Wenyu Fan,Yuanchun Shi*

Main category: cs.HC

TL;DR: 本研究提出了"Cognitive Chain"框架，通过认知流程分解任务难度，并验证了其与用户完成时间的相关性。


<details>
  <summary>Details</summary>
Motivation: 有效测量图形用户界面（GUI）任务的难度对于用户行为分析和代理能力评估至关重要。

Method: 通过构建认知链，将认知过程分解为一系列步骤，并利用线性回归验证认知难度与用户完成时间的相关性。

Result: 提出了一种新框架"Cognitive Chain"，从认知角度建模任务难度，并开发了一种基于LLM的方法自动提取认知链。

Conclusion: 探讨了该研究在代理训练、能力评估及人机协作优化等方面的潜在应用。

Abstract: Measuring GUI task difficulty is crucial for user behavior analysis and agent capability evaluation. Yet, existing benchmarks typically quantify difficulty based on motor actions (e.g., step counts), overlooking the cognitive demands underlying task completion. In this work, we propose Cognitive Chain, a novel framework that models task difficulty from a cognitive perspective. A cognitive chain decomposes the cognitive processes preceding a motor action into a sequence of cognitive steps (e.g., finding, deciding, computing), each with a difficulty index grounded in information theories. We develop an LLM-based method to automatically extract cognitive chains from task execution traces. Validation with linear regression shows that our estimated cognitive difficulty correlates well with user completion time (step-level R-square=0.46 after annotation). Assessment of state-of-the-art GUI agents shows reduced success on cognitively demanding tasks, revealing capability gaps and Human-AI consistency patterns. We conclude by discussing potential applications in agent training, capability assessment, and human-agent delegation optimization.

</details>


### [7] [TempoQL: A Readable, Precise, and Portable Query System for Electronic Health Record Data](https://arxiv.org/abs/2511.09337)
*Ziyong Ma,Richard D. Boyce,Adam Perer,Venkatesh Sivaraman*

Main category: cs.HC

TL;DR: TempoQL是一个旨在降低电子健康记录数据提取和建模难度的工具包，支持多种数据标准，简化时间查询。


<details>
  <summary>Details</summary>
Motivation: 研究人员和临床医生在提取和验证EHR数据以进行建模时面临挑战，而现有工具在表现力和可用性之间存在权衡，并且通常专门针对单一数据标准。

Method: TempoQL是一个Python基础工具包，用于执行时间查询，支持多种电子健康记录（EHR）数据标准。

Result: 通过对不同数据集的性能评估和两个用例的展示，TempoQL简化了机器学习中队列的创建，同时保持了精度、速度和可重复性。

Conclusion: TempoQL为机器学习中的队列创建提供了便利，同时保证了数据处理的质量和效率。

Abstract: Electronic health record (EHR) data is an essential data source for machine learning for health, but researchers and clinicians face steep barriers in extracting and validating EHR data for modeling. Existing tools incur trade-offs between expressivity and usability and are typically specialized to a single data standard, making it difficult to write temporal queries that are ready for modern model-building pipelines and adaptable to new datasets. This paper introduces TempoQL, a Python-based toolkit designed to lower these barriers. TempoQL provides a simple, human-readable language for temporal queries; support for multiple EHR data standards, including OMOP, MEDS, and others; and an interactive notebook-based query interface with optional large language model (LLM) authoring assistance. Through a performance evaluation and two use cases on different datasets, we demonstrate that TempoQL simplifies the creation of cohorts for machine learning while maintaining precision, speed, and reproducibility.

</details>


### [8] [A multimodal AI agent for clinical decision support in ophthalmology](https://arxiv.org/abs/2511.09394)
*Danli Shi,Xiaolan Chen,Bingjie Yan,Weiyi Zhang,Pusheng Xu,Jiancheng Yang,Ruoyu Chen,Siyu Huang,Bowen Liu,Xinyuan Wu,Meng Xie,Ziyu Gao,Yue Wu,Senlin Lin,Kai Jin,Xia Gong,Yih Chung Tham,Xiujuan Zhang,Li Dong,Yuzhou Zhang,Jason Yam,Guangming Jin,Xiaohu Ding,Haidong Zou,Yalin Zheng,Zongyuan Ge,Mingguang He*

Main category: cs.HC

TL;DR: EyeAgent通过使用大型语言模型与53个眼科工具结合，提高了诊断准确性，并在同行评审中获得高度认可。


<details>
  <summary>Details</summary>
Motivation: 当前大多数医疗成像系统在灵活性、可解释性和适应性方面存在不足，尤其是在需要多种成像模式的眼科领域。

Method: EyeAgent中使用的DeepSeek-V3语言模型根据用户查询动态调度53个经过验证的眼科工具，进行分类、分割、检测、图像/报告生成及定量分析。

Result: EyeAgent是第一个用于眼科的智能AI框架，能够进行全面且可解释的临床决策支持，并在多种任务中表现出色。

Conclusion: EyeAgent展示了一种可扩展且值得信赖的AI框架，为眼科提供了明晰的蓝图，有助于未来模块化、多模式和与临床对齐的AI系统的发展。

Abstract: Artificial intelligence has shown promise in medical imaging, yet most existing systems lack flexibility, interpretability, and adaptability - challenges especially pronounced in ophthalmology, where diverse imaging modalities are essential. We present EyeAgent, the first agentic AI framework for comprehensive and interpretable clinical decision support in ophthalmology. Using a large language model (DeepSeek-V3) as its central reasoning engine, EyeAgent interprets user queries and dynamically orchestrates 53 validated ophthalmic tools across 23 imaging modalities for diverse tasks including classification, segmentation, detection, image/report generation, and quantitative analysis. Stepwise ablation analysis demonstrated a progressive improvement in diagnostic accuracy, rising from a baseline of 69.71% (using only 5 general tools) to 80.79% when the full suite of 53 specialized tools was integrated. In an expert rating study on 200 real-world clinical cases, EyeAgent achieved 93.7% tool selection accuracy and received expert ratings of more than 88% across accuracy, completeness, safety, reasoning, and interpretability. In human-AI collaboration, EyeAgent matched or exceeded the performance of senior ophthalmologists and, when used as an assistant, improved overall diagnostic accuracy by 18.51% and report quality scores by 19%, with the greatest benefit observed among junior ophthalmologists. These findings establish EyeAgent as a scalable and trustworthy AI framework for ophthalmology and provide a blueprint for modular, multimodal, and clinically aligned next-generation AI systems.

</details>


### [9] [Algorithmic Advice as a Strategic Signal on Competitive Markets](https://arxiv.org/abs/2511.09454)
*Tobias R. Rebholz,Maxwell Uphoff,Christian H. R. Bernges,Florian Scholten*

Main category: cs.HC

TL;DR: 算法建议能够影响竞争性决策和市场动态，个性化建议更具影响力，可能导致隐性合谋。


<details>
  <summary>Details</summary>
Motivation: 算法在竞争决策中的影响日益加深，涉及的不仅是个体结果，还有市场动态的形成。

Method: 通过两个预注册实验，参与者在不同的经济博弈中体验个性化和集体算法建议对行为的影响。

Result: 算法建议对人类行为的影响显著，并在经典经济博弈中观察到众多现象。

Conclusion: 算法建议可以作为战略信号，影响协调，而需要对决策支持系统进行严格设计和监管。

Abstract: As algorithms increasingly mediate competitive decision-making, their influence extends beyond individual outcomes to shaping strategic market dynamics. In two preregistered experiments, we examined how algorithmic advice affects human behavior in classic economic games with unique, non-collusive, and analytically traceable equilibria. In Experiment 1 (N = 107), participants played a Bertrand price competition with individualized or collective algorithmic recommendations. Initially, collusively upward-biased advice increased prices, particularly when individualized, but prices gradually converged toward equilibrium over the course of the experiment. However, participants avoided setting prices above the algorithm's recommendation throughout the experiment, suggesting that advice served as a soft upper bound for acceptable prices. In Experiment 2 (N = 129), participants played a Cournot quantity competition with equilibrium-aligned or strategically biased algorithmic recommendations. Here, individualized equilibrium advice supported stable convergence, whereas collusively downward-biased advice led to sustained underproduction and supracompetitive profits - hallmarks of tacit collusion. In both experiments, participants responded more strongly and consistently to individualized advice than collective advice, potentially due to greater perceived ownership of the former. These findings demonstrate that algorithmic advice can function as a strategic signal, shaping coordination even without explicit communication. The results echo real-world concerns about algorithmic collusion and underscore the need for careful design and oversight of algorithmic decision-support systems in competitive environments.

</details>


### [10] [Exploring The Interaction-Outcome Paradox: Seemingly Richer and More Self-Aware Interactions with LLMs May Not Yet Lead to Better Learning](https://arxiv.org/abs/2511.09458)
*Rahul R. Divekar,Sophia Guerra,Lisette Gonzalez,Natasha Boos*

Main category: cs.HC

TL;DR: 本研究对比了大型语言模型（LLMs）和基于搜索的界面对学习互动和成果的影响，结果发现尽管LLMs能促进更丰富的互动，但并未显著改善学习结果。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在教育中的实际效果，特别是学习互动与成果之间的关系。

Method: 进行了一项对照研究，参与者（N=20）分别使用LLMs和搜索界面进行学习互动，比较其学习成效。

Result: 尽管LLMs引发了更丰富的学习互动，但在学习成果上并未表现出明显的优势。

Conclusion: 教育技术的未来应关注支持学生认知工作的系统设计，而不仅仅是丰富互动方式。

Abstract: While Large Language Models (LLMs) have transformed the user interface for learning, moving from keyword search to natural language dialogue, their impact on educational outcomes remains unclear. We present a controlled study (N=20) that directly compares the learning interaction and outcomes between LLM and search-based interfaces. We found that although LLMs elicit richer and nuanced interactions from a learner, they do not produce broadly better learning outcomes. In this paper, we explore this the ``Interaction-Outcome Paradox.'' To explain this, we discuss the concept of a cognitive shift: the locus of student effort moves from finding and synthesizing disparate sources (search) to a more self-aware identification and articulation of their knowledge gaps and strategies to bridge those gaps (LLMs). This insight provides a new lens for evaluating educational technologies, suggesting that the future of learning tools lies not in simply enriching interaction, but in designing systems that scaffold productive cognitive work by leveraging this student expressiveness.

</details>


### [11] [Spatial Audio Rendering for Real-Time Speech Translation in Virtual Meetings](https://arxiv.org/abs/2511.09525)
*Margarita Geleta,Hong Sodoma,Hannes Gamper*

Main category: cs.HC

TL;DR: 本研究探讨了空间音频渲染的实时翻译在多语言会议中的效果，发现其显著提升了理解度和用户体验，强调了设计集成的重要性。


<details>
  <summary>Details</summary>
Motivation: 为了应对虚拟会议中的语言障碍问题，本研究探索了空间音频渲染对理解、认知负荷和用户体验的影响。

Method: 进行了一项在主题内的实验，涉及8名双语参与者和47名参与者，模拟全球团队会议，使用希腊语、卡纳达语、普通话和乌克兰语的英语翻译。

Result: 空间音频渲染的翻译比非空间音频的理解准确率提高了一倍，参与者在有空间线索和音调区分时报告了更高的清晰度和参与感。

Conclusion: 空间音频渲染的实时翻译显著提高了多语言会议中的理解度和用户体验，提出了未来设计的改进方向。

Abstract: Language barriers in virtual meetings remain a persistent challenge to global collaboration. Real-time translation offers promise, yet current integrations often neglect perceptual cues. This study investigates how spatial audio rendering of translated speech influences comprehension, cognitive load, and user experience in multilingual meetings. We conducted a within-subjects experiment with 8 bilingual confederates and 47 participants simulating global team meetings with English translations of Greek, Kannada, Mandarin Chinese, and Ukrainian - languages selected for their diversity in grammar, script, and resource availability. Participants experienced four audio conditions: spatial audio with and without background reverberation, and two non-spatial configurations (diotic, monaural). We measured listener comprehension accuracy, workload ratings, satisfaction scores, and qualitative feedback. Spatially-rendered translations doubled comprehension compared to non-spatial audio. Participants reported greater clarity and engagement when spatial cues and voice timbre differentiation were present. We discuss design implications for integrating real-time translation into meeting platforms, advancing inclusive, cross-language communication in telepresence systems.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [12] [Practical and Performant Enhancements for Maximization of Algebraic Connectivity](https://arxiv.org/abs/2511.08694)
*Leonard Jung,Alan Papalia,Kevin Doherty,Michael Everett*

Main category: cs.RO

TL;DR: 提出了一种改进的图稀疏化算法，增强了大规模图的状态估计性能，适用于实时应用。


<details>
  <summary>Details</summary>
Motivation: 当前的图状态估计方法在处理大规模长时态图时效率低下，本研究旨在优化此过程以提高实用性。

Method: 采用专门求解器和先进的步长策略优化现有的MAC算法，确保图的连通性且无需手动指定边集。

Result: 本文提出了一种改进的图稀疏化算法，旨在解决大规模长时态图的状态估计问题，以提高计算效率和稳定性，使其更适合实时应用。

Conclusion: 本研究通过提升计算速度、优化收敛策略和实现自动连接方案，使得MAC算法在大型图的实时状态估计中更加可行。

Abstract: Long-term state estimation over graphs remains challenging as current graph estimation methods scale poorly on large, long-term graphs. To address this, our work advances a current state-of-the-art graph sparsification algorithm, maximizing algebraic connectivity (MAC). MAC is a sparsification method that preserves estimation performance by maximizing the algebraic connectivity, a spectral graph property that is directly connected to the estimation error. Unfortunately, MAC remains computationally prohibitive for online use and requires users to manually pre-specify a connectivity-preserving edge set. Our contributions close these gaps along three complementary fronts: we develop a specialized solver for algebraic connectivity that yields an average 2x runtime speedup; we investigate advanced step size strategies for MAC's optimization procedure to enhance both convergence speed and solution quality; and we propose automatic schemes that guarantee graph connectivity without requiring manual specification of edges. Together, these contributions make MAC more scalable, reliable, and suitable for real-time estimation applications.

</details>


### [13] [Intuitive Programming, Adaptive Task Planning, and Dynamic Role Allocation in Human-Robot Collaboration](https://arxiv.org/abs/2511.08732)
*Marta Lagomarsino,Elena Merlo,Andrea Pupa,Timo Birr,Franziska Krebs,Cristian Secchi,Tamim Asfour,Arash Ajoudani*

Main category: cs.RO

TL;DR: 本论文回顾了促进人类与机器人直观交流的关键组成部分，并探讨了优化人-机器人合作的趋势和前景。


<details>
  <summary>Details</summary>
Motivation: 尽管机器人和人工智能在复杂任务和环境中取得了显著成果，但人类仍然处于被动观察者的角色，无法有效地与机器人进行互动。

Method: 审查了人类与机器人交互的整个流程，包括多模态输入的转换、适应性规划和角色分配，以及控制层和反馈机制。

Result: 提出了一个框架，促进人类与机器人之间的信息交流和技能转移，从而实现协同的人-机器人合作。

Conclusion: 建立人类与机器人之间的连续信息流对于实现有效的协同合作至关重要，并且该领域有许多值得探索的方向。

Abstract: Remarkable capabilities have been achieved by robotics and AI, mastering complex tasks and environments. Yet, humans often remain passive observers, fascinated but uncertain how to engage. Robots, in turn, cannot reach their full potential in human-populated environments without effectively modeling human states and intentions and adapting their behavior. To achieve a synergistic human-robot collaboration (HRC), a continuous information flow should be established: humans must intuitively communicate instructions, share expertise, and express needs. In parallel, robots must clearly convey their internal state and forthcoming actions to keep users informed, comfortable, and in control. This review identifies and connects key components enabling intuitive information exchange and skill transfer between humans and robots. We examine the full interaction pipeline: from the human-to-robot communication bridge translating multimodal inputs into robot-understandable representations, through adaptive planning and role allocation, to the control layer and feedback mechanisms to close the loop. Finally, we highlight trends and promising directions toward more adaptive, accessible HRC.

</details>


### [14] [ATOM-CBF: Adaptive Safe Perception-Based Control under Out-of-Distribution Measurements](https://arxiv.org/abs/2511.08741)
*Kai S. Yun,Navid Azizan*

Main category: cs.RO

TL;DR: ATOM-CBF是一种新颖的安全控制框架，针对在高维传感器数据推断系统状态时可能遭遇的外部分布（OoD）测量的不确定性，能够自适应地计算和应对这些测量带来的不确定性，从而确保真实系统的安全性。


<details>
  <summary>Details</summary>
Motivation: 确保真实系统的安全性是一个普遍挑战，特别是当这些系统依赖于从高维传感器数据中推断系统状态的学习模块时。

Method: 通过引入对外部分布（OoD）测量的适应性感知误差边际和实时调整保守性的安全过滤器，ATOM-CBF实现了对不确定性的有效管理。

Result: 通过仿真实验验证，ATOM-CBF在LiDAR扫描的F1Tenth车辆和RGB图像的四足机器人中有效地维护了安全性。

Conclusion: ATOM-CBF成功地为真实世界的控制系统提供了一种安全保证，即使在面临未知的输入时也能保持安全。

Abstract: Ensuring the safety of real-world systems is challenging, especially when they rely on learned perception modules to infer the system state from high-dimensional sensor data. These perception modules are vulnerable to epistemic uncertainty, often failing when encountering out-of-distribution (OoD) measurements not seen during training. To address this gap, we introduce ATOM-CBF (Adaptive-To-OoD-Measurement Control Barrier Function), a novel safe control framework that explicitly computes and adapts to the epistemic uncertainty from OoD measurements, without the need for ground-truth labels or information on distribution shifts. Our approach features two key components: (1) an OoD-aware adaptive perception error margin and (2) a safety filter that integrates this adaptive error margin, enabling the filter to adjust its conservatism in real-time. We provide empirical validation in simulations, demonstrating that ATOM-CBF maintains safety for an F1Tenth vehicle with LiDAR scans and a quadruped robot with RGB images.

</details>


### [15] [CENIC: Convex Error-controlled Numerical Integration for Contact](https://arxiv.org/abs/2511.08771)
*Vince Kurtz,Alejandro Castro*

Main category: cs.RO

TL;DR: CENIC 是一种新型连续时间积分器，结合了凸时间步进和误差控制积分的优势，解决了现代机器人工作流程中对速度和可扩展性的需求。


<details>
  <summary>Details</summary>
Motivation: 解决离散时间机器人模拟器在选择时间步进时的挑战，尤其是在速度和精度上。

Method: CENIC 通过结合凸时间步进和误差控制积分的最新进展，实现在刚性动态和接触中的有效积分。

Result: CENIC 在速度和准确性方面与当前最先进的离散时间模拟器相当，且具备了精度和收敛性的保障。

Conclusion: CENIC 提供了高效、准确的机器人模拟，满足了实时速率要求并确保了积分精度和收敛性。

Abstract: State-of-the-art robotics simulators operate in discrete time. This requires users to choose a time step, which is both critical and challenging: large steps can produce non-physical artifacts, while small steps force the simulation to run slowly. Continuous-time error-controlled integration avoids such issues by automatically adjusting the time step to achieve a desired accuracy. But existing error-controlled integrators struggle with the stiff dynamics of contact, and cannot meet the speed and scalability requirements of modern robotics workflows. We introduce CENIC, a new continuous-time integrator that brings together recent advances in convex time-stepping and error-controlled integration, inheriting benefits from both continuous integration and discrete time-stepping. CENIC runs at fast real-time rates comparable to discrete-time robotics simulators like MuJoCo, Drake and Isaac Sim, while also providing guarantees on accuracy and convergence.

</details>


### [16] [Dual-Arm Whole-Body Motion Planning: Leveraging Overlapping Kinematic Chains](https://arxiv.org/abs/2511.08778)
*Richard Cheng,Peter Werner,Carolyn Matl*

Main category: cs.RO

TL;DR: 提出通过共享关节结构构建动态路线图的方法，提高双臂机器人在未知环境中的运动规划效率。


<details>
  <summary>Details</summary>
Motivation: 高自由度双臂机器人在未知变化环境中的实时运动规划面临维度诅咒和复杂的避障约束问题。

Method: 利用共享关节结构构建动态路线图，实现高效的运动规划

Result: 在真实超市环境中，运动规划器在19自由度的移动操作机器人上成功执行了超过2000个运动计划，平均规划时间为0.4秒，成功率达99.9%。

Conclusion: 通过共享关节的结构化，能够有效减轻运动规划中的维度诅咒问题，从而实现高效的实时任务执行。

Abstract: High degree-of-freedom dual-arm robots are becoming increasingly common due to their morphology enabling them to operate effectively in human environments. However, motion planning in real-time within unknown, changing environments remains a challenge for such robots due to the high dimensionality of the configuration space and the complex collision-avoidance constraints that must be obeyed. In this work, we propose a novel way to alleviate the curse of dimensionality by leveraging the structure imposed by shared joints (e.g. torso joints) in a dual-arm robot. First, we build two dynamic roadmaps (DRM) for each kinematic chain (i.e. left arm + torso, right arm + torso) with specific structure induced by the shared joints. Then, we show that we can leverage this structure to efficiently search through the composition of the two roadmaps and largely sidestep the curse of dimensionality. Finally, we run several experiments in a real-world grocery store with this motion planner on a 19 DoF mobile manipulation robot executing a grocery fulfillment task, achieving 0.4s average planning times with 99.9% success rate across more than 2000 motion plans.

</details>


### [17] [Low-cost Multi-agent Fleet for Acoustic Cooperative Localization Research](https://arxiv.org/abs/2511.08822)
*Nelson Durrant,Braden Meyers,Matthew McMurray,Clayton Smith,Brighton Anderson,Tristan Hodgins,Kalliyan Velasco,Joshua G. Mangelson*

Main category: cs.RO

TL;DR: 提出了一种经济且可定制的自主水下机器人平台CoUGARs，用于多代理的合作定位研究。


<details>
  <summary>Details</summary>
Motivation: 解决多代理自主水下测试中的高成本和工程挑战。

Method: 基于商业可用和3D打印部件构建，开发了强大的状态估计、导航和声学通信软件，并通过实地测试和仿真验证。

Result: 介绍了一个低成本、可配置的自主水下机器人平台CoUGARs，促进多代理自主研究。

Conclusion: CoUGARs的平台设计和软件集成与HoloOcean仿真器紧密结合，已成功在模拟和实地试验中验证。

Abstract: Real-world underwater testing for multi-agent autonomy presents substantial financial and engineering challenges. In this work, we introduce the Configurable Underwater Group of Autonomous Robots (CoUGARs) as a low-cost, configurable autonomous-underwater-vehicle (AUV) platform for multi-agent autonomy research. The base design costs less than $3,000 USD (as of May 2025) and is based on commercially-available and 3D-printed parts, enabling quick customization for various sensor payloads and configurations. Our current expanded model is equipped with a doppler velocity log (DVL) and ultra-short-baseline (USBL) acoustic array/transducer to support research on acoustic-based cooperative localization. State estimation, navigation, and acoustic communications software has been developed and deployed using a containerized software stack and is tightly integrated with the HoloOcean simulator. The system was tested both in simulation and via in-situ field trials in Utah lakes and reservoirs.

</details>


### [18] [XPRESS: X-Band Radar Place Recognition via Elliptical Scan Shaping](https://arxiv.org/abs/2511.08863)
*Hyesu Jang,Wooseong Yang,Ayoung Kim,Dongje Lee,Hanguen Kim*

Main category: cs.RO

TL;DR: 提出一种基于X波段雷达的自主导航位置识别算法，提升了雷达在海洋环境中的应用。


<details>
  <summary>Details</summary>
Motivation: 解决X波段雷达在自主导航中的低分辨率和信息不足的问题。

Method: 结合对象密度规则进行候选选择，并有意降低雷达检测精度，以提高检索性能。

Result: 在公共海洋雷达数据集和自收集数据集上评估，表现优于先进的雷达位置识别方法。

Conclusion: 该位置识别算法在多个数据集上的性能优于现有方法，显示出良好的实际应用潜力。

Abstract: X-band radar serves as the primary sensor on maritime vessels, however, its application in autonomous navigation has been limited due to low sensor resolution and insufficient information content. To enable X-band radar-only autonomous navigation in maritime environments, this paper proposes a place recognition algorithm specifically tailored for X-band radar, incorporating an object density-based rule for efficient candidate selection and intentional degradation of radar detections to achieve robust retrieval performance. The proposed algorithm was evaluated on both public maritime radar datasets and our own collected dataset, and its performance was compared against state-of-the-art radar place recognition methods. An ablation study was conducted to assess the algorithm's performance sensitivity with respect to key parameters.

</details>


### [19] [MirrorLimb: Implementing hand pose acquisition and robot teleoperation based on RealMirror](https://arxiv.org/abs/2511.08865)
*Cong Tai,Hansheng Wu,Haixu Long,Zhengbin Long,Zhaoyu Zheng,Haodong Xiang,Tao Shen*

Main category: cs.RO

TL;DR: 开发了一种低成本的实时手部运动和姿态数据获取框架，支持机器人操作并兼容RealMirror生态系统。


<details>
  <summary>Details</summary>
Motivation: 旨在降低上肢机器人操控研究的技术障碍，以促进视觉-语言-动作领域的研究进展。

Method: 框架与Isaac仿真环境兼容，支持稳定精确的机器人轨迹录制和多种机器人设备的实时远程操作。

Result: 提出了一种基于PICO的机器人远程操作框架，能够以低成本实时获取手部运动和姿态数据，该框架在性价比方面优于主流的视觉跟踪和运动捕捉解决方案。

Conclusion: 该框架降低了上肢机器人操控研究的技术门槛，加速了与视觉-语言-动作研究相关的进展。

Abstract: In this work, we present a PICO-based robot remote operating framework that enables low-cost, real-time acquisition of hand motion and pose data, outperforming mainstream visual tracking and motion capture solutions in terms of cost-effectiveness. The framework is natively compatible with the RealMirror ecosystem, offering ready-to-use functionality for stable and precise robotic trajectory recording within the Isaac simulation environment, thereby facilitating the construction of Vision-Language-Action (VLA) datasets. Additionally, the system supports real-time teleoperation of a variety of end-effector-equipped robots, including dexterous hands and robotic grippers. This work aims to lower the technical barriers in the study of upper-limb robotic manipulation, thereby accelerating advancements in VLA-related research.

</details>


### [20] [A Shared Control Framework for Mobile Robots with Planning-Level Intention Prediction](https://arxiv.org/abs/2511.08912)
*Jinyu Zhang,Lijun Han,Feng Jian,Lingxi Zhang,Hesheng Wang*

Main category: cs.RO

TL;DR: 提出了一种新型移动机器人共享控制框架，通过预测人类运动意图来实现高效的协作。


<details>
  <summary>Details</summary>
Motivation: 在人机协作中，准确理解人类的运动意图是至关重要的。

Method: 通过深度强化学习将意图域预测和路径重规划问题共同表述为马尔可夫决策过程，并开发了一种基于沃罗诺伊的轨迹生成算法。

Result: 广泛的仿真实验和真实用户研究表明，所提方法在减少工作负担和提高安全性方面表现显著。

Conclusion: 该方法显著降低了操作员的工作负担，提高了安全性，并且在任务效率上与现有的辅助遥控方法相比毫无妥协。

Abstract: In mobile robot shared control, effectively understanding human motion intention is critical for seamless human-robot collaboration. This paper presents a novel shared control framework featuring planning-level intention prediction. A path replanning algorithm is designed to adjust the robot's desired trajectory according to inferred human intentions. To represent future motion intentions, we introduce the concept of an intention domain, which serves as a constraint for path replanning. The intention-domain prediction and path replanning problems are jointly formulated as a Markov Decision Process and solved through deep reinforcement learning. In addition, a Voronoi-based human trajectory generation algorithm is developed, allowing the model to be trained entirely in simulation without human participation or demonstration data. Extensive simulations and real-world user studies demonstrate that the proposed method significantly reduces operator workload and enhances safety, without compromising task efficiency compared with existing assistive teleoperation approaches.

</details>


### [21] [Expand Your SCOPE: Semantic Cognition over Potential-Based Exploration for Embodied Visual Navigation](https://arxiv.org/abs/2511.08935)
*Ningnan Wang,Weihuang Chen,Liming Chen,Haoxuan Ji,Zhongyu Guo,Xuchong Zhang,Hongbin Sun*

Main category: cs.RO

TL;DR: 提出了一种新的零-shot导航框架SCOPE，利用视觉前沿信息提升探索决策，并在两项导航任务中显示出更高的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的零-shot研究未能考虑视觉前沿边界，这对未来的轨迹和观察至关重要。

Method: SCOPE利用视觉-语言模型估计探索潜力，并通过自我重新考虑机制优化决策。

Result: SCOPE框架通过视觉-语言模型估计探索潜力，组织成时空潜力图，支持长时间规划，且提高了准确性。

Conclusion: SCOPE在准确性、校准、泛化和决策质量方面表现优越，强调了前沿信息的重要性。

Abstract: Embodied visual navigation remains a challenging task, as agents must explore unknown environments with limited knowledge. Existing zero-shot studies have shown that incorporating memory mechanisms to support goal-directed behavior can improve long-horizon planning performance. However, they overlook visual frontier boundaries, which fundamentally dictate future trajectories and observations, and fall short of inferring the relationship between partial visual observations and navigation goals. In this paper, we propose Semantic Cognition Over Potential-based Exploration (SCOPE), a zero-shot framework that explicitly leverages frontier information to drive potential-based exploration, enabling more informed and goal-relevant decisions. SCOPE estimates exploration potential with a Vision-Language Model and organizes it into a spatio-temporal potential graph, capturing boundary dynamics to support long-horizon planning. In addition, SCOPE incorporates a self-reconsideration mechanism that revisits and refines prior decisions, enhancing reliability and reducing overconfident errors. Experimental results on two diverse embodied navigation tasks show that SCOPE outperforms state-of-the-art baselines by 4.6\% in accuracy. Further analysis demonstrates that its core components lead to improved calibration, stronger generalization, and higher decision quality.

</details>


### [22] [Think, Remember, Navigate: Zero-Shot Object-Goal Navigation with VLM-Powered Reasoning](https://arxiv.org/abs/2511.08942)
*Mobin Habibpour,Fatemeh Afghah*

Main category: cs.RO

TL;DR: 本研究提出了一种新框架，通过视觉-语言模型有效提升机器人导航的策略制定能力，实现了更高的导航效率和逻辑路径。


<details>
  <summary>Details</summary>
Motivation: 为了充分发挥视觉-语言模型在机器人导航中的潜力，将其从被动观察者转变为主动策略制定者。

Method: 框架将高层规划外包给视觉-语言模型，通过三种技术实现智能指导：结构化的思维链提示、动态包含代理的历史行动、以及VLM理解自上而下障碍地图和第一人称视角的能力。

Result: 在HM3D、Gibson和MP3D等挑战性基准测试中，该方法显著改善了路径规划效果。

Conclusion: 该方法显著提高了导航效率，提供了更直接和合乎逻辑的轨迹，标志着朝着更强大的实体智能体迈出了重要一步。

Abstract: While Vision-Language Models (VLMs) are set to transform robotic navigation, existing methods often underutilize their reasoning capabilities. To unlock the full potential of VLMs in robotics, we shift their role from passive observers to active strategists in the navigation process. Our framework outsources high-level planning to a VLM, which leverages its contextual understanding to guide a frontier-based exploration agent. This intelligent guidance is achieved through a trio of techniques: structured chain-of-thought prompting that elicits logical, step-by-step reasoning; dynamic inclusion of the agent's recent action history to prevent getting stuck in loops; and a novel capability that enables the VLM to interpret top-down obstacle maps alongside first-person views, thereby enhancing spatial awareness. When tested on challenging benchmarks like HM3D, Gibson, and MP3D, this method produces exceptionally direct and logical trajectories, marking a substantial improvement in navigation efficiency over existing approaches and charting a path toward more capable embodied agents.

</details>


### [23] [UniMM-V2X: MoE-Enhanced Multi-Level Fusion for End-to-End Cooperative Autonomous Driving](https://arxiv.org/abs/2511.09013)
*Ziyi Song,Chen Xia,Chenbing Wang,Haibao Yu,Sheng Zhou,Zhisheng Niu*

Main category: cs.RO

TL;DR: UniMM-V2X是一个创新的多智能体自动驾驶框架，通过层级合作和专家混合架构，显著提升了感知、预测和规划的性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统独立智能体在感知和决策上的局限性，并有效整合最新的端到端自动驾驶技术。

Method: 提出UniMM-V2X，一个端到端的多智能体框架，实现在感知、预测和规划之间的层级合作。

Result: 在DAIR-V2X数据集上，UniMM-V2X在感知准确性上提高39.7%，预测误差降低7.2%，规划性能提升33.2%。

Conclusion: UniMM-V2X框架展示了多层次合作策略的有效性，使得自动驾驶决策更加一致、安全，并展现出在多个任务中的卓越表现。

Abstract: Autonomous driving holds transformative potential but remains fundamentally constrained by the limited perception and isolated decision-making with standalone intelligence. While recent multi-agent approaches introduce cooperation, they often focus merely on perception-level tasks, overlooking the alignment with downstream planning and control, or fall short in leveraging the full capacity of the recent emerging end-to-end autonomous driving. In this paper, we present UniMM-V2X, a novel end-to-end multi-agent framework that enables hierarchical cooperation across perception, prediction, and planning. At the core of our framework is a multi-level fusion strategy that unifies perception and prediction cooperation, allowing agents to share queries and reason cooperatively for consistent and safe decision-making. To adapt to diverse downstream tasks and further enhance the quality of multi-level fusion, we incorporate a Mixture-of-Experts (MoE) architecture to dynamically enhance the BEV representations. We further extend MoE into the decoder to better capture diverse motion patterns. Extensive experiments on the DAIR-V2X dataset demonstrate our approach achieves state-of-the-art (SOTA) performance with a 39.7% improvement in perception accuracy, a 7.2% reduction in prediction error, and a 33.2% improvement in planning performance compared with UniV2X, showcasing the strength of our MoE-enhanced multi-level cooperative paradigm.

</details>


### [24] [A Quantum Tunneling and Bio-Phototactic Driven Enhanced Dwarf Mongoose Optimizer for UAV Trajectory Planning and Engineering Problem](https://arxiv.org/abs/2511.09020)
*Mingyang Yu,Haorui Yang,Kangning An,Xinjian Wei,Xiaoxuan Xu,Jing Xu*

Main category: cs.RO

TL;DR: 本研究提出了一种增强的多策略侏儒蒙古犬优化算法EDMO，针对复杂环境中的无人机路径规划，显示出优越的性能和实用性。


<details>
  <summary>Details</summary>
Motivation: 随着无人机的广泛使用，路径规划变得越来越重要，尤其是在复杂和多变的环境中。

Method: 提出了一种增强的多策略侏儒蒙古犬优化（EDMO）算法，结合了动态量子隧穿优化、微生物光趋向的动态聚焦搜索和正交镜头对抗学习策略。

Result: EDMO在39个标准测试函数上优于14种先进算法，并在实际任务中验证了其适用性和有效性。

Conclusion: EDMO算法在三维无人机轨迹规划中显示出优越的收敛速度、鲁棒性和优化精度，并在实际应用中表现出良好的有效性。

Abstract: With the widespread adoption of unmanned aerial vehicles (UAV), effective path planning has become increasingly important. Although traditional search methods have been extensively applied, metaheuristic algorithms have gained popularity due to their efficiency and problem-specific heuristics. However, challenges such as premature convergence and lack of solution diversity still hinder their performance in complex scenarios. To address these issues, this paper proposes an Enhanced Multi-Strategy Dwarf Mongoose Optimization (EDMO) algorithm, tailored for three-dimensional UAV trajectory planning in dynamic and obstacle-rich environments. EDMO integrates three novel strategies: (1) a Dynamic Quantum Tunneling Optimization Strategy (DQTOS) to enable particles to probabilistically escape local optima; (2) a Bio-phototactic Dynamic Focusing Search Strategy (BDFSS) inspired by microbial phototaxis for adaptive local refinement; and (3) an Orthogonal Lens Opposition-Based Learning (OLOBL) strategy to enhance global exploration through structured dimensional recombination. EDMO is benchmarked on 39 standard test functions from CEC2017 and CEC2020, outperforming 14 advanced algorithms in convergence speed, robustness, and optimization accuracy. Furthermore, real-world validations on UAV three-dimensional path planning and three engineering design tasks confirm its practical applicability and effectiveness in field robotics missions requiring intelligent, adaptive, and time-efficient planning.

</details>


### [25] [SMF-VO: Direct Ego-Motion Estimation via Sparse Motion Fields](https://arxiv.org/abs/2511.09072)
*Sangheon Yang,Yeongin Yoon,Hong Mo Jung,Jongwoo Lim*

Main category: cs.RO

TL;DR: 提出了一种轻量级的稀疏运动场视觉里程计框架，能够高效估计相机的线性和角速度，适用于移动机器人和可穿戴设备。


<details>
  <summary>Details</summary>
Motivation: 克服传统视觉里程计和视觉惯性里程计在资源受限设备上的实时性能限制

Method: 引入稀疏运动场视觉里程计（SMF-VO）

Result: SMF-VO 在基准数据集上显示出优越的效率和竞争力的准确性，在使用CPU的Raspberry Pi 5上实现超过100 FPS

Conclusion: SMF-VO为传统方法提供了一种可扩展且高效的替代方案。

Abstract: Traditional Visual Odometry (VO) and Visual Inertial Odometry (VIO) methods rely on a 'pose-centric' paradigm, which computes absolute camera poses from the local map thus requires large-scale landmark maintenance and continuous map optimization. This approach is computationally expensive, limiting their real-time performance on resource-constrained devices. To overcome these limitations, we introduce Sparse Motion Field Visual Odometry (SMF-VO), a lightweight, 'motion-centric' framework. Our approach directly estimates instantaneous linear and angular velocity from sparse optical flow, bypassing the need for explicit pose estimation or expensive landmark tracking. We also employed a generalized 3D ray-based motion field formulation that works accurately with various camera models, including wide-field-of-view lenses. SMF-VO demonstrates superior efficiency and competitive accuracy on benchmark datasets, achieving over 100 FPS on a Raspberry Pi 5 using only a CPU. Our work establishes a scalable and efficient alternative to conventional methods, making it highly suitable for mobile robotics and wearable devices.

</details>


### [26] [D-AWSIM: Distributed Autonomous Driving Simulator for Dynamic Map Generation Framework](https://arxiv.org/abs/2511.09080)
*Shunsuke Ito,Chaoran Zhao,Ryo Okamura,Takuya Azumi*

Main category: cs.RO

TL;DR: 本论文介绍了D-AWSIM，一个分布式模拟器，提升了对大规模传感器和城市交通环境的仿真能力，支持自动驾驶研究。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶技术的发展，扩大安全保障的操作设计域需要有效的信息共享和仿真平台，以应对现实世界中的多样化条件。

Method: 采用分布式架构，将仿真工作负载分配到多个机器进行大规模交通和传感器部署的仿真。

Result: 本论文提出了D-AWSIM，一个分布式模拟器，旨在支持大规模传感器部署和密集交通环境的仿真。通过在多台机器之间划分工作负载，D-AWSIM克服了传统单主机模拟器在城市交通场景模拟上的局限。这一平台结合动态地图生成框架，允许研究人员探索信息共享策略，而无需依赖物理测试平台。结果表明，与单台机器设置相比，D-AWSIM显著提高了车辆数量和LiDAR传感器处理的吞吐量，并与Autoware的集成展示了其在自动驾驶研究中的适用性。

Conclusion: D-AWSIM显著提高了车辆及传感器的仿真处理能力，适用于自动驾驶的研究和开发。

Abstract: Autonomous driving systems have achieved significant advances, and full autonomy within defined operational design domains near practical deployment. Expanding these domains requires addressing safety assurance under diverse conditions. Information sharing through vehicle-to-vehicle and vehicle-to-infrastructure communication, enabled by a Dynamic Map platform built from vehicle and roadside sensor data, offers a promising solution. Real-world experiments with numerous infrastructure sensors incur high costs and regulatory challenges. Conventional single-host simulators lack the capacity for large-scale urban traffic scenarios. This paper proposes D-AWSIM, a distributed simulator that partitions its workload across multiple machines to support the simulation of extensive sensor deployment and dense traffic environments. A Dynamic Map generation framework on D-AWSIM enables researchers to explore information-sharing strategies without relying on physical testbeds. The evaluation shows that D-AWSIM increases throughput for vehicle count and LiDAR sensor processing substantially compared to a single-machine setup. Integration with Autoware demonstrates applicability for autonomous driving research.

</details>


### [27] [APEX: Action Priors Enable Efficient Exploration for Robust Motion Tracking on Legged Robots](https://arxiv.org/abs/2511.09091)
*Shivam Sood,Laukik Nakhwa,Sun Ge,Yuhong Cao,Jin Cheng,Fatemah Zargarbashi,Taerim Yoon,Sungjoon Choi,Stelian Coros,Guillaume Sartoretti*

Main category: cs.RO

TL;DR: APEX是一种新型的强化学习拓展，旨在消除对参考数据的依赖，提高样本效率并降低参数调优的工作量，使机器人能够更高效地学习自然的运动。


<details>
  <summary>Details</summary>
Motivation: 尽管运动追踪技术不断进步，现有方法仍需大量调优并依赖参考数据，这限制了机器人的适应性。

Method: APEX结合了逐步衰减的动作先验和多评估者框架，通过直接将专家演示融入强化学习来优化探索过程。

Result: APEX简化了部署过程，使得单一策略能够学习多样化运动，并在不同地形和速度间迁移，增强了模型的鲁棒性。

Conclusion: 该方法使腿部机器人在训练中的稳定性、效率和泛化能力得到提升，为广泛的机器人任务提供了指导驱动的强化学习新路径。

Abstract: Learning natural, animal-like locomotion from demonstrations has become a core paradigm in legged robotics. Despite the recent advancements in motion tracking, most existing methods demand extensive tuning and rely on reference data during deployment, limiting adaptability. We present APEX (Action Priors enable Efficient Exploration), a plug-and-play extension to state-of-the-art motion tracking algorithms that eliminates any dependence on reference data during deployment, improves sample efficiency, and reduces parameter tuning effort. APEX integrates expert demonstrations directly into reinforcement learning (RL) by incorporating decaying action priors, which initially bias exploration toward expert demonstrations but gradually allow the policy to explore independently. This is combined with a multi-critic framework that balances task performance with motion style. Moreover, APEX enables a single policy to learn diverse motions and transfer reference-like styles across different terrains and velocities, while remaining robust to variations in reward design. We validate the effectiveness of our method through extensive experiments in both simulation and on a Unitree Go2 robot. By leveraging demonstrations to guide exploration during RL training, without imposing explicit bias toward them, APEX enables legged robots to learn with greater stability, efficiency, and generalization. We believe this approach paves the way for guidance-driven RL to boost natural skill acquisition in a wide array of robotic tasks, from locomotion to manipulation. Website and code: https://marmotlab.github.io/APEX/.

</details>


### [28] [Decoupling Torque and Stiffness: A Unified Modeling and Control Framework for Antagonistic Artificial Muscles](https://arxiv.org/abs/2511.09104)
*Amirhossein Kazemipour,Robert K. Katzschmann*

Main category: cs.RO

TL;DR: 开发了一种能够实现不同软气动器独立控制扭矩和刚度的实时框架，显著提高了人机交互的安全性。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有软气动器控制器在动态接触转瞬间无法保持独立控制的问题。

Method: 通过一个统一的力学模型和级联控制器，结合生物学阻抗策略来独立调节扭矩和刚度。

Result: 提出了一种统一框架，可以实时独立控制各种软气动器（如PAM、HASEL、DEA）的扭矩和刚度。

Conclusion: 该框架为肌肉骨骼对抗系统实现适应性阻抗控制提供了基础，能够确保在动态接触过程中保持独立控制。

Abstract: Antagonistic soft actuators built from artificial muscles (PAMs, HASELs, DEAs) promise plant-level torque-stiffness decoupling, yet existing controllers for soft muscles struggle to maintain independent control through dynamic contact transients. We present a unified framework enabling independent torque and stiffness commands in real-time for diverse soft actuator types. Our unified force law captures diverse soft muscle physics in a single model with sub-ms computation, while our cascaded controller with analytical inverse dynamics maintains decoupling despite model errors and disturbances. Using co-contraction/bias coordinates, the controller independently modulates torque via bias and stiffness via co-contraction-replicating biological impedance strategies. Simulation-based validation through contact experiments demonstrates maintained independence: 200x faster settling on soft surfaces, 81% force reduction on rigid surfaces, and stable interaction vs 22-54% stability for fixed policies. This framework provides a foundation for enabling musculoskeletal antagonistic systems to execute adaptive impedance control for safe human-robot interaction.

</details>


### [29] [Data Assessment for Embodied Intelligence](https://arxiv.org/abs/2511.09119)
*Jiahao Xiao,Bowen Yan,Jianbo Zhang,Jia Wang,Chunyi Li,Zhengxue Cheng,Guangtao Zhai*

Main category: cs.RO

TL;DR: 本文提出了多样性熵和可学习性算法，以评估和改善多模态数据集的质量。


<details>
  <summary>Details</summary>
Motivation: 探索数据集对模型学习能力的影响，提升数据集质量以推动体现智能的发展。

Method: 构建统一的多模态表示和提出多样性熵；引入可解释的数据驱动算法量化数据集可学习性。

Result: 提出的多样性熵和可学习性算法在模拟和真实的体现数据集中验证有效，提供可操作的见解。

Conclusion: 本研究为设计高质量数据集奠定了基础，从而推动体现智能的发展。

Abstract: In embodied intelligence, datasets play a pivotal role, serving as both a knowledge repository and a conduit for information transfer. The two most critical attributes of a dataset are the amount of information it provides and how easily this information can be learned by models. However, the multimodal nature of embodied data makes evaluating these properties particularly challenging. Prior work has largely focused on diversity, typically counting tasks and scenes or evaluating isolated modalities, which fails to provide a comprehensive picture of dataset diversity. On the other hand, the learnability of datasets has received little attention and is usually assessed post-hoc through model training, an expensive, time-consuming process that also lacks interpretability, offering little guidance on how to improve a dataset. In this work, we address both challenges by introducing two principled, data-driven tools. First, we construct a unified multimodal representation for each data sample and, based on it, propose diversity entropy, a continuous measure that characterizes the amount of information contained in a dataset. Second, we introduce the first interpretable, data-driven algorithm to efficiently quantify dataset learnability without training, enabling researchers to assess a dataset's learnability immediately upon its release. We validate our algorithm on both simulated and real-world embodied datasets, demonstrating that it yields faithful, actionable insights that enable researchers to jointly improve diversity and learnability. We hope this work provides a foundation for designing higher-quality datasets that advance the development of embodied intelligence.

</details>


### [30] [RGMP: Recurrent Geometric-prior Multimodal Policy for Generalizable Humanoid Robot Manipulation](https://arxiv.org/abs/2511.09141)
*Xuetao Li,Wenke Huang,Nengyuan Pan,Kaiyan Zhao,Songhua Yang,Yiming Wang,Mengde Li,Mang Ye,Jifeng Xuan,Miao Li*

Main category: cs.RO

TL;DR: 本研究提出了一种新的框架RGMP，结合几何推理与数据高效的运动控制，以提高人形机器人的技能执行能力与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 目前的人形机器人研究多依赖大规模训练数据，忽视几何推理和平面动态关系建模，导致训练资源浪费，因此提出RGMP框架以解决这一问题。

Method: 本文提出的RGMP框架包含几何先验技能选择器和适应性递归高斯网络，前者为视觉语言模型注入几何先验，后者建立紧凑的高斯过程层次来编码机器人与目标物体之间的多尺度空间关系。

Result: 本文提出了一种名为RGMP（Recurrent Geometric-prior Multimodal Policy）的框架，旨在提高人形机器人在多模态决策和视觉运动控制方面的效率和泛化能力。该框架结合了几何语义技能推理与数据驱动的运动控制，通过引入几何先验和适应性递归高斯网络，有效提升了机器人的表现。

Conclusion: RGMP框架在多个机器人上经过测试，成功率达87%，数据效率是最先进模型的5倍，展现出优异的跨领域泛化能力。

Abstract: Humanoid robots exhibit significant potential in executing diverse human-level skills. However, current research predominantly relies on data-driven approaches that necessitate extensive training datasets to achieve robust multimodal decision-making capabilities and generalizable visuomotor control. These methods raise concerns due to the neglect of geometric reasoning in unseen scenarios and the inefficient modeling of robot-target relationships within the training data, resulting in significant waste of training resources. To address these limitations, we present the Recurrent Geometric-prior Multimodal Policy (RGMP), an end-to-end framework that unifies geometric-semantic skill reasoning with data-efficient visuomotor control. For perception capabilities, we propose the Geometric-prior Skill Selector, which infuses geometric inductive biases into a vision language model, producing adaptive skill sequences for unseen scenes with minimal spatial common sense tuning. To achieve data-efficient robotic motion synthesis, we introduce the Adaptive Recursive Gaussian Network, which parameterizes robot-object interactions as a compact hierarchy of Gaussian processes that recursively encode multi-scale spatial relationships, yielding dexterous, data-efficient motion synthesis even from sparse demonstrations. Evaluated on both our humanoid robot and desktop dual-arm robot, the RGMP framework achieves 87% task success in generalization tests and exhibits 5x greater data efficiency than the state-of-the-art model. This performance underscores its superior cross-domain generalization, enabled by geometric-semantic reasoning and recursive-Gaussion adaptation.

</details>


### [31] [LODESTAR: Degeneracy-Aware LiDAR-Inertial Odometry with Adaptive Schmidt-Kalman Filter and Data Exploitation](https://arxiv.org/abs/2511.09142)
*Eungchang Mason Lee,Kevin Christiansen Marsim,Hyun Myung*

Main category: cs.RO

TL;DR: 提出了一种新颖的LIO方法LODESTAR，通过引入DA-ASKF和DA-DE模块，克服了在退化环境下的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 随着机器人技术的发展，精确的环境感知对其导航能力至关重要，然而在特定环境中，LiDAR测距的性能会恶化，因此需要新的方法来改善这种性能下降。

Method: LODESTAR利用DA-ASKF模块，通过滑动窗口引入过去状态作为附加约束，并使用Schmidt-Kalman更新部分优化活跃状态，同时保持固定状态不变。

Result: LODESTAR方法通过两个关键模块改善了LiDAR惯性测距在复杂环境下的性能，使其在长走廊和高空飞行等退化环境中依然能够保持高精度，显著优于现有方法。

Conclusion: LODESTAR在多种退化条件下表现出更好的准确性和鲁棒性，证实了其有效性。

Abstract: LiDAR-inertial odometry (LIO) has been widely used in robotics due to its high accuracy. However, its performance degrades in degenerate environments, such as long corridors and high-altitude flights, where LiDAR measurements are imbalanced or sparse, leading to ill-posed state estimation. In this letter, we present LODESTAR, a novel LIO method that addresses these degeneracies through two key modules: degeneracy-aware adaptive Schmidt-Kalman filter (DA-ASKF) and degeneracy-aware data exploitation (DA-DE). DA-ASKF employs a sliding window to utilize past states and measurements as additional constraints. Specifically, it introduces degeneracy-aware sliding modes that adaptively classify states as active or fixed based on their degeneracy level. Using Schmidt-Kalman update, it partially optimizes active states while preserving fixed states. These fixed states influence the update of active states via their covariances, serving as reference anchors--akin to a lodestar. Additionally, DA-DE prunes less-informative measurements from active states and selectively exploits measurements from fixed states, based on their localizability contribution and the condition number of the Jacobian matrix. Consequently, DA-ASKF enables degeneracy-aware constrained optimization and mitigates measurement sparsity, while DA-DE addresses measurement imbalance. Experimental results show that LODESTAR outperforms existing LiDAR-based odometry methods and degeneracy-aware modules in terms of accuracy and robustness under various degenerate conditions.

</details>


### [32] [Unveiling the Impact of Data and Model Scaling on High-Level Control for Humanoid Robots](https://arxiv.org/abs/2511.09241)
*Yuxi Wei,Zirui Wang,Kangning Yin,Yue Hu,Jingbo Wang,Siheng Chen*

Main category: cs.RO

TL;DR: 本研究提出Humanoid-Union数据集及SCHUR学习框架，显著提升了人形机器人在高层控制中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决机器人学习中的数据缩放瓶颈，利用人类视频和运动数据进行高效学习

Method: 基于自主生成管道的Humanoid-Union数据集

Result: SCHUR在数据和模型扩展下，实现了机器人动作生成质量和文本-动作对齐的显著提升，分别提高了37%和25%

Conclusion: 通过比较，SCHUR表现出对真实人形机器人的有效性，验证了大规模数据在机器人控制学习中的重要性。

Abstract: Data scaling has long remained a critical bottleneck in robot learning. For humanoid robots, human videos and motion data are abundant and widely available, offering a free and large-scale data source. Besides, the semantics related to the motions enable modality alignment and high-level robot control learning. However, how to effectively mine raw video, extract robot-learnable representations, and leverage them for scalable learning remains an open problem. To address this, we introduce Humanoid-Union, a large-scale dataset generated through an autonomous pipeline, comprising over 260 hours of diverse, high-quality humanoid robot motion data with semantic annotations derived from human motion videos. The dataset can be further expanded via the same pipeline. Building on this data resource, we propose SCHUR, a scalable learning framework designed to explore the impact of large-scale data on high-level control in humanoid robots. Experimental results demonstrate that SCHUR achieves high robot motion generation quality and strong text-motion alignment under data and model scaling, with 37\% reconstruction improvement under MPJPE and 25\% alignment improvement under FID comparing with previous methods. Its effectiveness is further validated through deployment in real-world humanoid robot.

</details>


### [33] [UMIGen: A Unified Framework for Egocentric Point Cloud Generation and Cross-Embodiment Robotic Imitation Learning](https://arxiv.org/abs/2511.09302)
*Yan Huang,Shoujie Li,Xingting Li,Wenbo Ding*

Main category: cs.RO

TL;DR: UMIGen提出了一种新颖的方法，通过手持设备和优化机制提升数据收集效率，解决了数据驱动机器人学习中的示范数据问题。


<details>
  <summary>Details</summary>
Motivation: 当前的数据驱动机器人学习面临数据收集成本高、硬件依赖性强及空间泛化能力有限等挑战。

Method: UMIGen由Cloud-UMI和可见性优化机制两部分组成，前者无需视觉SLAM，直接记录点云与动作对，后者生成相机视野内的三维观察点。

Result: 实验结果表明，UMIGen在模拟和实际场景中都展现了良好的跨机器人通用性，并加快了多样化操作任务的数据收集过程。

Conclusion: UMIGen实现了有效的数据生成，支持跨机器人平台的通用性，并加速了复杂操作任务中的数据收集。

Abstract: Data-driven robotic learning faces an obvious dilemma: robust policies demand large-scale, high-quality demonstration data, yet collecting such data remains a major challenge owing to high operational costs, dependence on specialized hardware, and the limited spatial generalization capability of current methods. The Universal Manipulation Interface (UMI) relaxes the strict hardware requirements for data collection, but it is restricted to capturing only RGB images of a scene and omits the 3D geometric information on which many tasks rely. Inspired by DemoGen, we propose UMIGen, a unified framework that consists of two key components: (1) Cloud-UMI, a handheld data collection device that requires no visual SLAM and simultaneously records point cloud observation-action pairs; and (2) a visibility-aware optimization mechanism that extends the DemoGen pipeline to egocentric 3D observations by generating only points within the camera's field of view. These two components enable efficient data generation that aligns with real egocentric observations and can be directly transferred across different robot embodiments without any post-processing. Experiments in both simulated and real-world settings demonstrate that UMIGen supports strong cross-embodiment generalization and accelerates data collection in diverse manipulation tasks.

</details>


### [34] [CoRL-MPPI: Enhancing MPPI With Learnable Behaviours For Efficient And Provably-Safe Multi-Robot Collision Avoidance](https://arxiv.org/abs/2511.09331)
*Stepan Dergachev,Artem Pshenitsyn,Aleksandr Panov,Alexey Skrynnik,Konstantin Yakovlev*

Main category: cs.RO

TL;DR: CoRL-MPPI通过将合作强化学习嵌入MPPI框架，优化了随机采样的分布，提高了多机器人系统的导航效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 多机器人系统的可扩展性中，去中心化碰撞规避是一个核心挑战。虽然模型预测路径积分（MPPI）提供了强有力的理论保障，但在实际应用中，基于MPPI的控制器可能由于无信息随机采样而导致次optimal的轨迹。

Method: 在模拟环境中训练近似深度神经网络的动作策略，以学习局部合作碰撞规避行为，并将其嵌入到MPPI框架中以指导采样分布。

Result: 通过引入CoRL-MPPI，即合作强化学习与MPPI的融合，有效改善了碰撞规避性能。

Conclusion: CoRL-MPPI显著提高了多机器人导航的效率和安全性，使其更具灵活性和鲁棒性。

Abstract: Decentralized collision avoidance remains a core challenge for scalable multi-robot systems. One of the promising approaches to tackle this problem is Model Predictive Path Integral (MPPI) -- a framework that is naturally suited to handle any robot motion model and provides strong theoretical guarantees. Still, in practice MPPI-based controller may provide suboptimal trajectories as its performance relies heavily on uninformed random sampling. In this work, we introduce CoRL-MPPI, a novel fusion of Cooperative Reinforcement Learning and MPPI to address this limitation. We train an action policy (approximated as deep neural network) in simulation that learns local cooperative collision avoidance behaviors. This learned policy is then embedded into the MPPI framework to guide its sampling distribution, biasing it towards more intelligent and cooperative actions. Notably, CoRL-MPPI preserves all the theoretical guarantees of regular MPPI. We evaluate our approach in dense, dynamic simulation environments against state-of-the-art baselines, including ORCA, BVC, and a multi-agent MPPI implementation. Our results demonstrate that CoRL-MPPI significantly improves navigation efficiency (measured by success rate and makespan) and safety, enabling agile and robust multi-robot navigation.

</details>


### [35] [SPIDER: Scalable Physics-Informed Dexterous Retargeting](https://arxiv.org/abs/2511.09484)
*Chaoyi Pan,Changhao Wang,Haozhi Qi,Zixi Liu,Homanga Bharadhwaj,Akash Sharma,Tingfan Wu,Guanya Shi,Jitendra Malik,Francois Hogan*

Main category: cs.RO

TL;DR: SPIDER是一个物理基础的重定向框架，能将人类运动数据转化为机器人可执行的轨迹，有效提升成功率和速度。


<details>
  <summary>Details</summary>
Motivation: 解决收集机器人特定数据的高昂成本，同时利用丰富的人类运动数据来提高机器人的灵活控制能力。

Method: 通过大规模物理采样与虚拟接触引导，结合人类演示的全局任务结构来生成动态可行轨迹。

Result: 提出了一种名为SPIDER的框架，通过物理基础的重定向，利用人类运动数据生成适用于机器人控制的动态可行轨迹。

Conclusion: SPIDER有效地弥补了人类演示和机器人执行之间的差距，提高了机器人政策学习的效率和成功率。

Abstract: Learning dexterous and agile policy for humanoid and dexterous hand control requires large-scale demonstrations, but collecting robot-specific data is prohibitively expensive. In contrast, abundant human motion data is readily available from motion capture, videos, and virtual reality, which could help address the data scarcity problem. However, due to the embodiment gap and missing dynamic information like force and torque, these demonstrations cannot be directly executed on robots. To bridge this gap, we propose Scalable Physics-Informed DExterous Retargeting (SPIDER), a physics-based retargeting framework to transform and augment kinematic-only human demonstrations to dynamically feasible robot trajectories at scale. Our key insight is that human demonstrations should provide global task structure and objective, while large-scale physics-based sampling with curriculum-style virtual contact guidance should refine trajectories to ensure dynamical feasibility and correct contact sequences. SPIDER scales across diverse 9 humanoid/dexterous hand embodiments and 6 datasets, improving success rates by 18% compared to standard sampling, while being 10X faster than reinforcement learning (RL) baselines, and enabling the generation of a 2.4M frames dynamic-feasible robot dataset for policy learning. As a universal physics-based retargeting method, SPIDER can work with diverse quality data and generate diverse and high-quality data to enable efficient policy learning with methods like RL.

</details>


### [36] [WMPO: World Model-based Policy Optimization for Vision-Language-Action Models](https://arxiv.org/abs/2511.09515)
*Fangqi Zhu,Zhengyang Yan,Zicong Hong,Quanxin Shou,Xiao Ma,Song Guo*

Main category: cs.RO

TL;DR: WMPO是一种新型的强化学习框架，可以提高视觉-语言-行动模型在机器人操作中的效率和性能，而无需与真实环境交互。


<details>
  <summary>Details</summary>
Motivation: VLA模型在面对失败时缺乏自我纠正能力，传统的专家示范限制了学习效率，WMPO旨在解决这些问题，提高机器人在现实环境中的自我改进能力。

Method: WMPO基于像素级预测，利用与训练过的VLA特征对齐的‘想象’轨迹进行策略优化，采用在线GRPO策略，从而超越了传统的离线方法。

Result: 提出了一种基于世界模型的策略优化框架WMPO，针对视觉-语言-行动(VLA)模型在机器人操作中的不足，改进了样本效率和整体性能。在模拟和实际机器人环境中进行广泛实验，显示了WMPO的显著优势。

Conclusion: WMPO显著提高了样本效率、整体性能，并展现了自我纠正、鲁棒性和终身学习能力。

Abstract: Vision-Language-Action (VLA) models have shown strong potential for general-purpose robotic manipulation, but their reliance on expert demonstrations limits their ability to learn from failures and perform self-corrections. Reinforcement learning (RL) addresses these through self-improving interactions with the physical environment, but suffers from high sample complexity on real robots. We introduce World-Model-based Policy Optimization (WMPO), a principled framework for on-policy VLA RL without interacting with the real environment. In contrast to widely used latent world models, WMPO focuses on pixel-based predictions that align the "imagined" trajectories with the VLA features pretrained with web-scale images. Crucially, WMPO enables the policy to perform on-policy GRPO that provides stronger performance than the often-used off-policy methods. Extensive experiments in both simulation and real-robot settings demonstrate that WMPO (i) substantially improves sample efficiency, (ii) achieves stronger overall performance, (iii) exhibits emergent behaviors such as self-correction, and (iv) demonstrates robust generalization and lifelong learning capabilities.

</details>


### [37] [MAP-VLA: Memory-Augmented Prompting for Vision-Language-Action Model in Robotic Manipulation](https://arxiv.org/abs/2511.09516)
*Runhao Li,Wenkai Guo,Zhenyu Wu,Changyuan Wang,Haoyuan Deng,Zhenyu Weng,Yap-Peng Tan,Ziwei Wang*

Main category: cs.RO

TL;DR: 提出了一种新的内存增强视觉-语言-行动模型，以解决现有模型在长时间任务中性能不足的问题，通过动态集成演示派生的内存提示，提升动作生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-行动模型在应对长时间任务时面临挑战，主要由于缺乏内存和依赖即时感官输入。

Method: 提出了一个新的框架，名为内存增强提示的视觉-语言-行动模型（MAP-VLA），通过演示派生的内存提示增强长时间机器人操控任务中的动作生成。

Result: MAP-VLA通过构建记忆库和实时检索相关记忆，显著提高了长时间任务的执行性能。

Conclusion: 实验结果表明，MAP-VLA在模拟基准测试中和真实机器人评估中均取得了显著的性能提升，超越了当前的最先进技术。

Abstract: Pre-trained Vision-Language-Action (VLA) models have achieved remarkable success in improving robustness and generalization for end-to-end robotic manipulation. However, these models struggle with long-horizon tasks due to their lack of memory and reliance solely on immediate sensory inputs. To address this limitation, we propose Memory-Augmented Prompting for Vision-Language-Action model (MAP-VLA), a novel framework that empowers pre-trained VLA models with demonstration-derived memory prompts to augment action generation for long-horizon robotic manipulation tasks. To achieve this, MAP-VLA first constructs a memory library from historical demonstrations, where each memory unit captures information about a specific stage of a task. These memory units are implemented as learnable soft prompts optimized through prompt tuning. Then, during real-time task execution, MAP-VLA retrieves relevant memory through trajectory similarity matching and dynamically integrates it into the VLA model for augmented action generation. Importantly, this prompt tuning and retrieval augmentation approach operates as a plug-and-play module for a frozen VLA model, offering a lightweight and flexible solution to improve task performance. Experimental results show that MAP-VLA delivers up to 7.0% absolute performance gains in the simulation benchmark and 25.0% on real robot evaluations for long-horizon tasks, surpassing the current state-of-the-art methods.

</details>


### [38] [SpatialActor: Exploring Disentangled Spatial Representations for Robust Robotic Manipulation](https://arxiv.org/abs/2511.09555)
*Hao Shi,Bin Xie,Yingfei Liu,Yang Yue,Tiancai Wang,Haoqiang Fan,Xiangyu Zhang,Gao Huang*

Main category: cs.RO

TL;DR: SpatialActor通过解耦语义和几何，提高了机器人的操控精度和鲁棒性，表现出显著的少样本迁移能力。


<details>
  <summary>Details</summary>
Motivation: 机器人操控需要精确的空间理解，现有点基和基于图像的方法在处理噪声和空间线索时存在局限。

Method: 提出了一种解耦的框架SpatialActor，其中包含语义引导的几何模块和空间变换器，以提升机器人的操控精度。

Result: 在50多个任务的评估中，SpatialActor在RLBench上达到了87.4%的最新性能，并在不同噪声条件下提高了13.9%到19.4%。

Conclusion: SpatialActor在多种模拟和现实世界场景下展现了出色的表现，具有强大的鲁棒性和良好的少样本迁移能力。

Abstract: Robotic manipulation requires precise spatial understanding to interact with objects in the real world. Point-based methods suffer from sparse sampling, leading to the loss of fine-grained semantics. Image-based methods typically feed RGB and depth into 2D backbones pre-trained on 3D auxiliary tasks, but their entangled semantics and geometry are sensitive to inherent depth noise in real-world that disrupts semantic understanding. Moreover, these methods focus on high-level geometry while overlooking low-level spatial cues essential for precise interaction. We propose SpatialActor, a disentangled framework for robust robotic manipulation that explicitly decouples semantics and geometry. The Semantic-guided Geometric Module adaptively fuses two complementary geometry from noisy depth and semantic-guided expert priors. Also, a Spatial Transformer leverages low-level spatial cues for accurate 2D-3D mapping and enables interaction among spatial features. We evaluate SpatialActor on multiple simulation and real-world scenarios across 50+ tasks. It achieves state-of-the-art performance with 87.4% on RLBench and improves by 13.9% to 19.4% under varying noisy conditions, showing strong robustness. Moreover, it significantly enhances few-shot generalization to new tasks and maintains robustness under various spatial perturbations. Project Page: https://shihao1895.github.io/SpatialActor

</details>


### [39] [IFG: Internet-Scale Guidance for Functional Grasping Generation](https://arxiv.org/abs/2511.09558)
*Ray Muxin Liu,Mingxuan Li,Kenneth Shaw,Deepak Pathak*

Main category: cs.RO

TL;DR: 本研究提出了一种新方法，通过结合大规模视觉模型的语义理解和基于仿真的几何控制，实现了灵巧机器人3D抓取的高性能，无需手动训练数据。


<details>
  <summary>Details</summary>
Motivation: 尽管大型视觉模型在杂乱的场景中能够识别对象部分，但它们缺乏进行3D抓取所需的几何理解。

Method: 使用一个理解场景中手和物体局部几何的力闭合抓取生成管道，并将生成的数据提炼成在相机点云上实时操作的扩散模型。

Result: 该方法成功结合了语义理解和几何精度，使得机器人能够更精准地控制灵巧的抓手进行抓取。

Conclusion: 通过将互联网规模模型的全局语义理解与基于模拟的局部感知力矩闭合相结合，	his paper successfully achieves高性能的语义抓取，无需任何手动收集的训练数据。

Abstract: Large Vision Models trained on internet-scale data have demonstrated strong capabilities in segmenting and semantically understanding object parts, even in cluttered, crowded scenes. However, while these models can direct a robot toward the general region of an object, they lack the geometric understanding required to precisely control dexterous robotic hands for 3D grasping. To overcome this, our key insight is to leverage simulation with a force-closure grasping generation pipeline that understands local geometries of the hand and object in the scene. Because this pipeline is slow and requires ground-truth observations, the resulting data is distilled into a diffusion model that operates in real-time on camera point clouds. By combining the global semantic understanding of internet-scale models with the geometric precision of a simulation-based locally-aware force-closure, \our achieves high-performance semantic grasping without any manually collected training data. For visualizations of this please visit our website at https://ifgrasping.github.io/

</details>
