{"id": "2512.09065", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09065", "abs": "https://arxiv.org/abs/2512.09065", "authors": ["Shivendra Agrawal", "Jake Brawer", "Ashutosh Naik", "Alessandro Roncone", "Bradley Hayes"], "title": "ShelfAware: Real-Time Visual-Inertial Semantic Localization in Quasi-Static Environments with Low-Cost Sensors", "comment": "8 pages", "summary": "Many indoor workspaces are quasi-static: global layout is stable but local semantics change continually, producing repetitive geometry, dynamic clutter, and perceptual noise that defeat vision-based localization. We present ShelfAware, a semantic particle filter for robust global localization that treats scene semantics as statistical evidence over object categories rather than fixed landmarks. ShelfAware fuses a depth likelihood with a category-centric semantic similarity and uses a precomputed bank of semantic viewpoints to perform inverse semantic proposals inside MCL, yielding fast, targeted hypothesis generation on low-cost, vision-only hardware. Across 100 global-localization trials spanning four conditions (cart-mounted, wearable, dynamic obstacles, and sparse semantics) in a semantically dense, retail environment, ShelfAware achieves a 96% success rate (vs. 22% MCL and 10% AMCL) with a mean time-to-convergence of 1.91s, attains the lowest translational RMSE in all conditions, and maintains stable tracking in 80% of tested sequences, all while running in real time on a consumer laptop-class platform. By modeling semantics distributionally at the category level and leveraging inverse proposals, ShelfAware resolves geometric aliasing and semantic drift common to quasi-static domains. Because the method requires only vision sensors and VIO, it integrates as an infrastructure-free building block for mobile robots in warehouses, labs, and retail settings; as a representative application, it also supports the creation of assistive devices providing start-anytime, shared-control assistive navigation for people with visual impairments."}
{"id": "2512.09086", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.09086", "abs": "https://arxiv.org/abs/2512.09086", "authors": ["Xinyu Qi", "Zeyu Deng", "Shaun Alexander Macdonald", "Liying Li", "Chen Wang", "Muhammad Ali Imran", "Philip G. Zhao"], "title": "Inferring Operator Emotions from a Motion-Controlled Robotic Arm", "comment": null, "summary": "A remote robot operator's affective state can significantly impact the resulting robot's motions leading to unexpected consequences, even when the user follows protocol and performs permitted tasks. The recognition of a user operator's affective states in remote robot control scenarios is, however, underexplored. Current emotion recognition methods rely on reading the user's vital signs or body language, but the devices and user participation these measures require would add limitations to remote robot control. We demonstrate that the functional movements of a remote-controlled robotic avatar, which was not designed for emotional expression, can be used to infer the emotional state of the human operator via a machine-learning system. Specifically, our system achieved 83.3$\\%$ accuracy in recognizing the user's emotional state expressed by robot movements, as a result of their hand motions. We discuss the implications of this system on prominent current and future remote robot operation and affective robotic contexts."}
{"id": "2512.09101", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09101", "abs": "https://arxiv.org/abs/2512.09101", "authors": ["Lipeng Zhuang", "Shiyu Fan", "Florent P. Audonnet", "Yingdong Ru", "Gerardo Aragon Camarasa", "Paul Henderson"], "title": "Masked Generative Policy for Robotic Control", "comment": null, "summary": "We present Masked Generative Policy (MGP), a novel framework for visuomotor imitation learning. We represent actions as discrete tokens, and train a conditional masked transformer that generates tokens in parallel and then rapidly refines only low-confidence tokens. We further propose two new sampling paradigms: MGP-Short, which performs parallel masked generation with score-based refinement for Markovian tasks, and MGP-Long, which predicts full trajectories in a single pass and dynamically refines low-confidence action tokens based on new observations. With globally coherent prediction and robust adaptive execution capabilities, MGP-Long enables reliable control on complex and non-Markovian tasks that prior methods struggle with. Extensive evaluations on 150 robotic manipulation tasks spanning the Meta-World and LIBERO benchmarks show that MGP achieves both rapid inference and superior success rates compared to state-of-the-art diffusion and autoregressive policies. Specifically, MGP increases the average success rate by 9% across 150 tasks while cutting per-sequence inference time by up to 35x. It further improves the average success rate by 60% in dynamic and missing-observation environments, and solves two non-Markovian scenarios where other state-of-the-art methods fail."}
{"id": "2512.09105", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.09105", "abs": "https://arxiv.org/abs/2512.09105", "authors": ["Adi Manor", "Dan Cohen", "Ziv Keidar", "Avi Parush", "Hadas Erel"], "title": "Cognitive Trust in HRI: \"Pay Attention to Me and I'll Trust You Even if You are Wrong\"", "comment": "Confrence paper", "summary": "Cognitive trust and the belief that a robot is capable of accurately performing tasks, are recognized as central factors in fostering high-quality human-robot interactions. It is well established that performance factors such as the robot's competence and its reliability shape cognitive trust. Recent studies suggest that affective factors, such as robotic attentiveness, also play a role in building cognitive trust. This work explores the interplay between these two factors that shape cognitive trust. Specifically, we evaluated whether different combinations of robotic competence and attentiveness introduce a compensatory mechanism, where one factor compensates for the lack of the other. In the experiment, participants performed a search task with a robotic dog in a 2x2 experimental design that included two factors: competence (high or low) and attentiveness (high or low). The results revealed that high attentiveness can compensate for low competence. Participants who collaborated with a highly attentive robot that performed poorly reported trust levels comparable to those working with a highly competent robot. When the robot did not demonstrate attentiveness, low competence resulted in a substantial decrease in cognitive trust. The findings indicate that building cognitive trust in human-robot interaction may be more complex than previously believed, involving emotional processes that are typically overlooked. We highlight an affective compensatory mechanism that adds a layer to consider alongside traditional competence-based models of cognitive trust."}
{"id": "2512.08933", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.08933", "abs": "https://arxiv.org/abs/2512.08933", "authors": ["Lixiang Yan", "Yueqiao Jin", "Linxuan Zhao", "Roberto Martinez-Maldonado", "Xinyu Li", "Xiu Guan", "Wenxin Guo", "Xibin Han", "Dragan Gašević"], "title": "Agentic AI as Undercover Teammates: Argumentative Knowledge Construction in Hybrid Human-AI Collaborative Learning", "comment": null, "summary": "Generative artificial intelligence (AI) agents are increasingly embedded in collaborative learning environments, yet their impact on the processes of argumentative knowledge construction remains insufficiently understood. Emerging conceptualisations of agentic AI and artificial agency suggest that such systems possess bounded autonomy, interactivity, and adaptability, allowing them to engage as epistemic participants rather than mere instructional tools. Building on this theoretical foundation, the present study investigates how agentic AI, designed as undercover teammates with either supportive or contrarian personas, shapes the epistemic and social dynamics of collaborative reasoning. Drawing on Weinberger and Fischer's (2006) four-dimensional framework, participation, epistemic reasoning, argument structure, and social modes of co-construction, we analysed synchronous discourse data from 212 human and 64 AI participants (92 triads) engaged in an analytical problem-solving task. Mixed-effects and epistemic network analyses revealed that AI teammates maintained balanced participation but substantially reorganised epistemic and social processes: supportive personas promoted conceptual integration and consensus-oriented reasoning, whereas contrarian personas provoked critical elaboration and conflict-driven negotiation. Epistemic adequacy, rather than participation volume, predicted individual learning gains, indicating that agentic AI's educational value lies in enhancing the quality and coordination of reasoning rather than amplifying discourse quantity. These findings extend CSCL theory by conceptualising agentic AI as epistemic and social participants, bounded yet adaptive collaborators that redistribute cognitive and argumentative labour in hybrid human-AI learning environments."}
{"id": "2512.09111", "categories": ["cs.RO", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.09111", "abs": "https://arxiv.org/abs/2512.09111", "authors": ["Yuji Takubo", "Arpit Dwivedi", "Sukeerth Ramkumar", "Luis A. Pabon", "Daniele Gammelli", "Marco Pavone", "Simone D'Amico"], "title": "Semantic Trajectory Generation for Goal-Oriented Spacecraft Rendezvous", "comment": "28 pages, 12 figures. Submitted to AIAA SCITECH 2026", "summary": "Reliable real-time trajectory generation is essential for future autonomous spacecraft. While recent progress in nonconvex guidance and control is paving the way for onboard autonomous trajectory optimization, these methods still rely on extensive expert input (e.g., waypoints, constraints, mission timelines, etc.), which limits the operational scalability in real rendezvous missions.This paper introduces SAGES (Semantic Autonomous Guidance Engine for Space), a trajectory-generation framework that translates natural-language commands into spacecraft trajectories that reflect high-level intent while respecting nonconvex constraints. Experiments in two settings -- fault-tolerant proximity operations with continuous-time constraint enforcement and a free-flying robotic platform -- demonstrate that SAGES reliably produces trajectories aligned with human commands, achieving over 90\\% semantic-behavioral consistency across diverse behavior modes. Ultimately, this work marks an initial step toward language-conditioned, constraint-aware spacecraft trajectory generation, enabling operators to interactively guide both safety and behavior through intuitive natural-language commands with reduced expert burden."}
{"id": "2512.08934", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08934", "abs": "https://arxiv.org/abs/2512.08934", "authors": ["Loc Phuc Truong Nguyen", "Hung Thanh Do", "Hung Truong Thanh Nguyen", "Hung Cao"], "title": "Motion2Meaning: A Clinician-Centered Framework for Contestable LLM in Parkinson's Disease Gait Interpretation", "comment": "Accepted at the 9th International Symposium on Chatbots and Human-Centered AI", "summary": "AI-assisted gait analysis holds promise for improving Parkinson's Disease (PD) care, but current clinical dashboards lack transparency and offer no meaningful way for clinicians to interrogate or contest AI decisions. To address this issue, we present Motion2Meaning, a clinician-centered framework that advances Contestable AI through a tightly integrated interface designed for interpretability, oversight, and procedural recourse. Our approach leverages vertical Ground Reaction Force (vGRF) time-series data from wearable sensors as an objective biomarker of PD motor states. The system comprises three key components: a Gait Data Visualization Interface (GDVI), a one-dimensional Convolutional Neural Network (1D-CNN) that predicts Hoehn & Yahr severity stages, and a Contestable Interpretation Interface (CII) that combines our novel Cross-Modal Explanation Discrepancy (XMED) safeguard with a contestable Large Language Model (LLM). Our 1D-CNN achieves 89.0% F1-score on the public PhysioNet gait dataset. XMED successfully identifies model unreliability by detecting a five-fold increase in explanation discrepancies in incorrect predictions (7.45%) compared to correct ones (1.56%), while our LLM-powered interface enables clinicians to validate correct predictions and successfully contest a portion of the model's errors. A human-centered evaluation of this contestable interface reveals a crucial trade-off between the LLM's factual grounding and its readability and responsiveness to clinical feedback. This work demonstrates the feasibility of combining wearable sensor analysis with Explainable AI (XAI) and contestable LLMs to create a transparent, auditable system for PD gait interpretation that maintains clinical oversight while leveraging advanced AI capabilities. Our implementation is publicly available at: https://github.com/hungdothanh/motion2meaning."}
{"id": "2512.09283", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09283", "abs": "https://arxiv.org/abs/2512.09283", "authors": ["Fan Wu", "Chenguang Yang", "Haibin Yang", "Shuo Wang", "Yanrui Xu", "Xing Zhou", "Meng Gao", "Yaoqi Xian", "Zhihong Zhu", "Shifeng Huang"], "title": "UPETrack: Unidirectional Position Estimation for Tracking Occluded Deformable Linear Objects", "comment": null, "summary": "Real-time state tracking of Deformable Linear Objects (DLOs) is critical for enabling robotic manipulation of DLOs in industrial assembly, medical procedures, and daily-life applications. However, the high-dimensional configuration space, nonlinear dynamics, and frequent partial occlusions present fundamental barriers to robust real-time DLO tracking. To address these limitations, this study introduces UPETrack, a geometry-driven framework based on Unidirectional Position Estimation (UPE), which facilitates tracking without the requirement for physical modeling, virtual simulation, or visual markers. The framework operates in two phases: (1) visible segment tracking is based on a Gaussian Mixture Model (GMM) fitted via the Expectation Maximization (EM) algorithm, and (2) occlusion region prediction employing UPE algorithm we proposed. UPE leverages the geometric continuity inherent in DLO shapes and their temporal evolution patterns to derive a closed-form positional estimator through three principal mechanisms: (i) local linear combination displacement term, (ii) proximal linear constraint term, and (iii) historical curvature term. This analytical formulation allows efficient and stable estimation of occluded nodes through explicit linear combinations of geometric components, eliminating the need for additional iterative optimization. Experimental results demonstrate that UPETrack surpasses two state-of-the-art tracking algorithms, including TrackDLO and CDCPD2, in both positioning accuracy and computational efficiency."}
{"id": "2512.08935", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.08935", "abs": "https://arxiv.org/abs/2512.08935", "authors": ["Yuwei Guo", "Zihan Zhao", "Deyu Zhou", "Xiaowei Liu", "Ming Zhang"], "title": "From Script to Stage: Automating Experimental Design for Social Simulations with LLMs", "comment": null, "summary": "The rise of large language models (LLMs) has opened new avenues for social science research. Multi-agent simulations powered by LLMs are increasingly becoming a vital approach for exploring complex social phenomena and testing theoretical hypotheses. However, traditional computational experiments often rely heavily on interdisciplinary expertise, involve complex operations, and present high barriers to entry. While LLM-driven agents show great potential for automating experimental design, their reliability and scientific rigor remain insufficient for widespread adoption. To address these challenges, this paper proposes an automated multi-agent experiment design framework based on script generation, inspired by the concept of the Decision Theater. The experimental design process is divided into three stages: (1) Script Generation - a Screenwriter Agent drafts candidate experimental scripts; (2) Script Finalization - a Director Agent evaluates and selects the final script; (3) Actor Generation - an Actor Factory creates actor agents capable of performing on the experimental \"stage\" according to the finalized script. Extensive experiment conducted across multiple social science experimental scenarios demonstrate that the generated actor agents can perform according to the designed scripts and reproduce outcomes consistent with real-world situations. This framework not only lowers the barriers to experimental design in social science but also provides a novel decision-support tool for policy-making and research. The project's source code is available at: https://anonymous.4open.science/r/FSTS-DE1E"}
{"id": "2512.09297", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09297", "abs": "https://arxiv.org/abs/2512.09297", "authors": ["Huayi Zhou", "Kui Jia"], "title": "One-Shot Real-World Demonstration Synthesis for Scalable Bimanual Manipulation", "comment": "under review", "summary": "Learning dexterous bimanual manipulation policies critically depends on large-scale, high-quality demonstrations, yet current paradigms face inherent trade-offs: teleoperation provides physically grounded data but is prohibitively labor-intensive, while simulation-based synthesis scales efficiently but suffers from sim-to-real gaps. We present BiDemoSyn, a framework that synthesizes contact-rich, physically feasible bimanual demonstrations from a single real-world example. The key idea is to decompose tasks into invariant coordination blocks and variable, object-dependent adjustments, then adapt them through vision-guided alignment and lightweight trajectory optimization. This enables the generation of thousands of diverse and feasible demonstrations within several hour, without repeated teleoperation or reliance on imperfect simulation. Across six dual-arm tasks, we show that policies trained on BiDemoSyn data generalize robustly to novel object poses and shapes, significantly outperforming recent baselines. By bridging the gap between efficiency and real-world fidelity, BiDemoSyn provides a scalable path toward practical imitation learning for complex bimanual manipulation without compromising physical grounding."}
{"id": "2512.08936", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.08936", "abs": "https://arxiv.org/abs/2512.08936", "authors": ["Brent Winslow", "Jacqueline Shreibati", "Javier Perez", "Hao-Wei Su", "Nichole Young-Lin", "Nova Hammerquist", "Daniel McDuff", "Jason Guss", "Jenny Vafeiadou", "Nick Cain", "Alex Lin", "Erik Schenck", "Shiva Rajagopal", "Jia-Ru Chung", "Anusha Venkatakrishnan", "Amy Armento Lee", "Maryam Karimzadehgan", "Qingyou Meng", "Rythm Agarwal", "Aravind Natarajan", "Tracy Giest"], "title": "A Principle-based Framework for the Development and Evaluation of Large Language Models for Health and Wellness", "comment": null, "summary": "The incorporation of generative artificial intelligence into personal health applications presents a transformative opportunity for personalized, data-driven health and fitness guidance, yet also poses challenges related to user safety, model accuracy, and personal privacy. To address these challenges, a novel, principle-based framework was developed and validated for the systematic evaluation of LLMs applied to personal health and wellness. First, the development of the Fitbit Insights explorer, a large language model (LLM)-powered system designed to help users interpret their personal health data, is described. Subsequently, the safety, helpfulness, accuracy, relevance, and personalization (SHARP) principle-based framework is introduced as an end-to-end operational methodology that integrates comprehensive evaluation techniques including human evaluation by generalists and clinical specialists, autorater assessments, and adversarial testing, into an iterative development lifecycle. Through the application of this framework to the Fitbit Insights explorer in a staged deployment involving over 13,000 consented users, challenges not apparent during initial testing were systematically identified. This process guided targeted improvements to the system and demonstrated the necessity of combining isolated technical evaluations with real-world user feedback. Finally, a comprehensive, actionable approach is established for the responsible development and deployment of LLM-powered health applications, providing a standardized methodology to foster innovation while ensuring emerging technologies are safe, effective, and trustworthy for users."}
{"id": "2512.09310", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09310", "abs": "https://arxiv.org/abs/2512.09310", "authors": ["Kwang Bin Lee", "Jiho Kang", "Sung-Hee Lee"], "title": "Scene-agnostic Hierarchical Bimanual Task Planning via Visual Affordance Reasoning", "comment": "8 pages, 4 figures", "summary": "Embodied agents operating in open environments must translate high-level instructions into grounded, executable behaviors, often requiring coordinated use of both hands. While recent foundation models offer strong semantic reasoning, existing robotic task planners remain predominantly unimanual and fail to address the spatial, geometric, and coordination challenges inherent to bimanual manipulation in scene-agnostic settings. We present a unified framework for scene-agnostic bimanual task planning that bridges high-level reasoning with 3D-grounded two-handed execution. Our approach integrates three key modules. Visual Point Grounding (VPG) analyzes a single scene image to detect relevant objects and generate world-aligned interaction points. Bimanual Subgoal Planner (BSP) reasons over spatial adjacency and cross-object accessibility to produce compact, motion-neutralized subgoals that exploit opportunities for coordinated two-handed actions. Interaction-Point-Driven Bimanual Prompting (IPBP) binds these subgoals to a structured skill library, instantiating synchronized unimanual or bimanual action sequences that satisfy hand-state and affordance constraints. Together, these modules enable agents to plan semantically meaningful, physically feasible, and parallelizable two-handed behaviors in cluttered, previously unseen scenes. Experiments show that it produces coherent, feasible, and compact two-handed plans, and generalizes to cluttered scenes without retraining, demonstrating robust scene-agnostic affordance reasoning for bimanual tasks."}
{"id": "2512.08937", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.08937", "abs": "https://arxiv.org/abs/2512.08937", "authors": ["Harsh Kumar", "Jasmine Chahal", "Yinuo Zhao", "Zeling Zhang", "Annika Wei", "Louis Tay", "Ashton Anderson"], "title": "When AI Gives Advice: Evaluating AI and Human Responses to Online Advice-Seeking for Well-Being", "comment": null, "summary": "Seeking advice is a core human behavior that the Internet has reinvented twice: first through forums and Q\\&A communities that crowdsource public guidance, and now through large language models (LLMs) that deliver private, on-demand counsel at scale. Yet the quality of this synthesized LLM advice remains unclear. How does it compare, not only against arbitrary human comments, but against the wisdom of the online crowd? We conducted two studies (N = 210) in which experts compared top-voted Reddit advice with LLM-generated advice. LLMs ranked significantly higher overall and on effectiveness, warmth, and willingness to seek advice again. GPT-4o beat GPT-5 on all metrics except sycophancy, suggesting that benchmark gains need not improve advice-giving. In our second study, we examined how human and algorithmic advice could be combined, and found that human advice can be unobtrusively polished to compete with AI-generated comments. Finally, to surface user expectations, we ran an exploratory survey with undergraduates (N=148) that revealed heterogeneous, persona-dependent preferences for agent qualities (e.g., coach-like: goal-focused structure; friend-like: warmth and humor). We conclude with design implications for advice-giving agents and ecosystems blending AI, crowd input, and expert oversight."}
{"id": "2512.09343", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09343", "abs": "https://arxiv.org/abs/2512.09343", "authors": ["Ashik E Rasul", "Humaira Tasnim", "Ji Yu Kim", "Young Hyun Lim", "Scott Schmitz", "Bruce W. Jo", "Hyung-Jin Yoon"], "title": "Development and Testing for Perception Based Autonomous Landing of a Long-Range QuadPlane", "comment": null, "summary": "QuadPlanes combine the range efficiency of fixed-wing aircraft with the maneuverability of multi-rotor platforms for long-range autonomous missions. In GPS-denied or cluttered urban environments, perception-based landing is vital for reliable operation. Unlike structured landing zones, real-world sites are unstructured and highly variable, requiring strong generalization capabilities from the perception system. Deep neural networks (DNNs) provide a scalable solution for learning landing site features across diverse visual and environmental conditions. While perception-driven landing has been shown in simulation, real-world deployment introduces significant challenges. Payload and volume constraints limit high-performance edge AI devices like the NVIDIA Jetson Orin Nano, which are crucial for real-time detection and control. Accurate pose estimation during descent is necessary, especially in the absence of GPS, and relies on dependable visual-inertial odometry. Achieving this with limited edge AI resources requires careful optimization of the entire deployment framework. The flight characteristics of large QuadPlanes further complicate the problem. These aircraft exhibit high inertia, reduced thrust vectoring, and slow response times further complicate stable landing maneuvers. This work presents a lightweight QuadPlane system for efficient vision-based autonomous landing and visual-inertial odometry, specifically developed for long-range QuadPlane operations such as aerial monitoring. It describes the hardware platform, sensor configuration, and embedded computing architecture designed to meet demanding real-time, physical constraints. This establishes a foundation for deploying autonomous landing in dynamic, unstructured, GPS-denied environments."}
{"id": "2512.08938", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.08938", "abs": "https://arxiv.org/abs/2512.08938", "authors": ["Massimo Fascinari", "Vincent English"], "title": "The Impact of Artificial Intelligence on Strategic Technology Management: A Mixed-Methods Analysis of Resources, Capabilities, and Human-AI Collaboration", "comment": "32 pages, 1 figure, 1 table", "summary": "This paper investigates how artificial intelligence (AI) can be effectively integrated into Strategic Technology Management (STM) practices to enhance the strategic alignment and effectiveness of technology investments. Through a mixed-methods approach combining quantitative survey data (n=230) and qualitative expert interviews (n=14), this study addresses three critical research questions: what success factors AI innovates for STM roadmap formulation under uncertainty; what resources and capabilities organizations require for AI-enhanced STM; and how human-AI interaction should be designed for complex STM tasks. The findings reveal that AI fundamentally transforms STM through data-driven strategic alignment and continuous adaptation, while success depends on cultivating proprietary data ecosystems, specialized human talent, and robust governance capabilities. The study introduces the AI-based Strategic Technology Management (AIbSTM) conceptual framework, which synthesizes technical capabilities with human and organizational dimensions across three layers: strategic alignment, resource-based view, and human-AI interaction. Contrary to visions of autonomous AI leadership, the research demonstrates that the most viable trajectory is human-centric augmentation, where AI serves as a collaborative partner rather than a replacement for human judgment. This work contributes to theory by extending the Resource-Based View to AI contexts and addressing cognitive and socio-technical chasms in AI adoption, while offering practitioners a prescriptive framework for navigating AI integration in strategic technology management."}
{"id": "2512.09349", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09349", "abs": "https://arxiv.org/abs/2512.09349", "authors": ["Lin Li", "Yuxin Cai", "Jianwu Fang", "Jianru Xue", "Chen Lv"], "title": "COVLM-RL: Critical Object-Oriented Reasoning for Autonomous Driving Using VLM-Guided Reinforcement Learning", "comment": "8 pages, 5 figures", "summary": "End-to-end autonomous driving frameworks face persistent challenges in generalization, training efficiency, and interpretability. While recent methods leverage Vision-Language Models (VLMs) through supervised learning on large-scale datasets to improve reasoning, they often lack robustness in novel scenarios. Conversely, reinforcement learning (RL)-based approaches enhance adaptability but remain data-inefficient and lack transparent decision-making. % contribution To address these limitations, we propose COVLM-RL, a novel end-to-end driving framework that integrates Critical Object-oriented (CO) reasoning with VLM-guided RL. Specifically, we design a Chain-of-Thought (CoT) prompting strategy that enables the VLM to reason over critical traffic elements and generate high-level semantic decisions, effectively transforming multi-view visual inputs into structured semantic decision priors. These priors reduce the input dimensionality and inject task-relevant knowledge into the RL loop, accelerating training and improving policy interpretability. However, bridging high-level semantic guidance with continuous low-level control remains non-trivial. To this end, we introduce a consistency loss that encourages alignment between the VLM's semantic plans and the RL agent's control outputs, enhancing interpretability and training stability. Experiments conducted in the CARLA simulator demonstrate that COVLM-RL significantly improves the success rate by 30\\% in trained driving environments and by 50\\% in previously unseen environments, highlighting its strong generalization capability."}
{"id": "2512.08939", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.08939", "abs": "https://arxiv.org/abs/2512.08939", "authors": ["Yuzhou Wu", "Mingyang Wu", "Di Liu", "Rong Yin", "Kang Li"], "title": "Assessing the Human-Likeness of LLM-Driven Digital Twins in Simulating Health Care System Trust", "comment": "6 pages, 1 figure may be published in IISE Annual Conference & Expo 2026", "summary": "Serving as an emerging and powerful tool, Large Language Model (LLM)-driven Human Digital Twins are showing great potential in healthcare system research. However, its actual simulation ability for complex human psychological traits, such as distrust in the healthcare system, remains unclear. This research gap particularly impacts health professionals' trust and usage of LLM-based Artificial Intelligence (AI) systems in assisting their routine work. In this study, based on the Twin-2K-500 dataset, we systematically evaluated the simulation results of the LLM-driven human digital twin using the Health Care System Distrust Scale (HCSDS) with an established human-subject sample, analyzing item-level distributions, summary statistics, and demographic subgroup patterns. Results showed that the simulated responses by the digital twin were significantly more centralized with lower variance and had fewer selections of extreme options (all p<0.001). While the digital twin broadly reproduces human results in major demographic patterns, such as age and gender, it exhibits relatively low sensitivity in capturing minor differences in education levels. The LLM-based digital twin simulation has the potential to simulate population trends, but it also presents challenges in making detailed, specific distinctions in subgroups of human beings. This study suggests that the current LLM-driven Digital Twins have limitations in modeling complex human attitudes, which require careful calibration and validation before applying them in inferential analyses or policy simulations in health systems engineering. Future studies are necessary to examine the emotional reasoning mechanism of LLMs before their use, particularly for studies that involve simulations sensitive to social topics, such as human-automation trust."}
{"id": "2512.09377", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09377", "abs": "https://arxiv.org/abs/2512.09377", "authors": ["Lidan Xu", "Dadong Fan", "Junhong Wang", "Wenshuo Li", "Hao Lu", "Jianzhong Qiao"], "title": "Observability Analysis and Composite Disturbance Filtering for a Bar Tethered to Dual UAVs Subject to Multi-source Disturbances", "comment": null, "summary": "Cooperative suspended aerial transportation is highly susceptible to multi-source disturbances such as aerodynamic effects and thrust uncertainties. To achieve precise load manipulation, existing methods often rely on extra sensors to measure cable directions or the payload's pose, which increases the system cost and complexity. A fundamental question remains: is the payload's pose observable under multi-source disturbances using only the drones' odometry information? To answer this question, this work focuses on the two-drone-bar system and proves that the whole system is observable when only two or fewer types of lumped disturbances exist by using the observability rank criterion. To the best of our knowledge, we are the first to present such a conclusion and this result paves the way for more cost-effective and robust systems by minimizing their sensor suites. Next, to validate this analysis, we consider the situation where the disturbances are only exerted on the drones, and develop a composite disturbance filtering scheme. A disturbance observer-based error-state extended Kalman filter is designed for both state and disturbance estimation, which renders improved estimation performance for the whole system evolving on the manifold $(\\mathbb{R}^3)^2\\times(TS^2)^3$. Our simulation and experimental tests have validated that it is possible to fully estimate the state and disturbance of the system with only odometry information of the drones."}
{"id": "2512.08940", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.08940", "abs": "https://arxiv.org/abs/2512.08940", "authors": ["Hemakshi Jani", "Mitish Karia", "Meet Gohil", "Rahul Bhadja", "Aznam Yacoub", "Shafaq Khan"], "title": "Psychlysis: Towards the Creation of a Questionnaire-based Machine Learning Tool to Analyze States of Mind", "comment": "Accepted and Published in IEEE ICCI*CC 2023 (2023 IEEE 22nd International Conference on Cognitive Informatics and Cognitive Computing)", "summary": "This paper describes the development of Psychlysis, a work-in-progress questionnaire-based machine learning application analyzing the user's current state of mind and suggesting ways to improve their mood using Machine Learning. The application utilizes the OCEAN model to understand the user's personality traits and make customized suggestions to enhance their well-being. The proposed application focus on improving the user's mood rather than just detecting their emotions. Preliminary results of the model are presented, showing the potential of the application in predicting the user's mood and providing personalized recommendations. The paper concludes by highlighting the potential benefits of such an application for various societal segments, including doctors, individuals, and mental health organizations, in improving emotional well-being and reducing the negative impact of mental health issues on daily life."}
{"id": "2512.09406", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09406", "abs": "https://arxiv.org/abs/2512.09406", "authors": ["Hai Ci", "Xiaokang Liu", "Pei Yang", "Yiren Song", "Mike Zheng Shou"], "title": "H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos", "comment": "13 pages, 6 figures", "summary": "Robots that learn manipulation skills from everyday human videos could acquire broad capabilities without tedious robot data collection. We propose a video-to-video translation framework that converts ordinary human-object interaction videos into motion-consistent robot manipulation videos with realistic, physically grounded interactions. Our approach does not require any paired human-robot videos for training only a set of unpaired robot videos, making the system easy to scale. We introduce a transferable representation that bridges the embodiment gap: by inpainting the robot arm in training videos to obtain a clean background and overlaying a simple visual cue (a marker and arrow indicating the gripper's position and orientation), we can condition a generative model to insert the robot arm back into the scene. At test time, we apply the same process to human videos (inpainting the person and overlaying human pose cues) and generate high-quality robot videos that mimic the human's actions. We fine-tune a SOTA video diffusion model (Wan 2.2) in an in-context learning manner to ensure temporal coherence and leveraging of its rich prior knowledge. Empirical results demonstrate that our approach achieves significantly more realistic and grounded robot motions compared to baselines, pointing to a promising direction for scaling up robot learning from unlabeled human videos. Project page: https://showlab.github.io/H2R-Grounder/"}
{"id": "2512.08941", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.08941", "abs": "https://arxiv.org/abs/2512.08941", "authors": ["Prabhanjana Ghuriki", "S. Chanti", "Jossy P George"], "title": "One Size Fits None: A Personalized Framework for Urban Accessibility Using Exponential Decay", "comment": "37 pages, 22 figures, pre-print manuscript", "summary": "This study develops a personalized accessibility framework that integrates exponential decay functions with user-customizable weighting systems. The framework enables real-time, personalized urban evaluation based on individual priorities and lifestyle requirements. The methodology employs grid-based discretization and a two-stage computational architecture that separates intensive preprocessing from lightweight real-time calculations. The computational architecture demonstrates that accessibility modelling can be made accessible to non-technical users through interactive interfaces, enabling fine-grained spatial analysis and identification of accessibility variations within neighbourhoods. The research contributes to Sustainable Development Goal 11's vision of inclusive, sustainable cities by providing tools for understanding how different populations experience identical urban spaces, supporting evidence-based policy development that addresses accessibility gaps."}
{"id": "2512.09410", "categories": ["cs.RO", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.09410", "abs": "https://arxiv.org/abs/2512.09410", "authors": ["Jialin Ying", "Zhihao Li", "Zicheng Dong", "Guohua Wu", "Yihuan Liao"], "title": "Generalizable Collaborative Search-and-Capture in Cluttered Environments via Path-Guided MAPPO and Directional Frontier Allocation", "comment": "7 pages, 7 figures", "summary": "Collaborative pursuit-evasion in cluttered environments presents significant challenges due to sparse rewards and constrained Fields of View (FOV). Standard Multi-Agent Reinforcement Learning (MARL) often suffers from inefficient exploration and fails to scale to large scenarios. We propose PGF-MAPPO (Path-Guided Frontier MAPPO), a hierarchical framework bridging topological planning with reactive control. To resolve local minima and sparse rewards, we integrate an A*-based potential field for dense reward shaping. Furthermore, we introduce Directional Frontier Allocation, combining Farthest Point Sampling (FPS) with geometric angle suppression to enforce spatial dispersion and accelerate coverage. The architecture employs a parameter-shared decentralized critic, maintaining O(1) model complexity suitable for robotic swarms. Experiments demonstrate that PGF-MAPPO achieves superior capture efficiency against faster evaders. Policies trained on 10x10 maps exhibit robust zero-shot generalization to unseen 20x20 environments, significantly outperforming rule-based and learning-based baselines."}
{"id": "2512.08942", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08942", "abs": "https://arxiv.org/abs/2512.08942", "authors": ["Yujie Zhang"], "title": "Beyond Technical Debt: How AI-Assisted Development Creates Comprehension Debt in Resource-Constrained Indie Teams", "comment": null, "summary": "Junior indie game developers in distributed, part-time teams lack production frameworks suited to their specific context, as traditional methodologies are often inaccessible. This study introduces the CIGDI (Co-Intelligence Game Development Ideation) Framework, an alternative approach for integrating AI tools to address persistent challenges of technical debt, coordination, and burnout.\n  The framework emerged from a three-month reflective practice and autoethnographic study of a three-person distributed team developing the 2D narrative game \"The Worm's Memoirs\". Based on analysis of development data (N=157 Jira tasks, N=333 GitHub commits, N=13+ Miro boards, N=8 reflection sessions), CIGDI is proposed as a seven-stage iterative process structured around human-in-the-loop decision points (Priority Criteria and Timeboxing).\n  While AI support democratized knowledge access and reduced cognitive load, our analysis identified a significant challenge: \"comprehension debt.\" We define this as a novel form of technical debt where AI helps teams build systems more sophisticated than their independent skill level can create or maintain. This paradox (possessing functional systems the team incompletely understands) creates fragility and AI dependency, distinct from traditional code quality debt.\n  This work contributes a practical production framework for resource-constrained teams and identifies critical questions about whether AI assistance constitutes a learning ladder or a dependency trap for developer skill."}
{"id": "2512.09411", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09411", "abs": "https://arxiv.org/abs/2512.09411", "authors": ["Siting Zhu", "Yuxiang Huang", "Wenhua Wu", "Chaokang Jiang", "Yongbo Chen", "I-Ming Chen", "Hesheng Wang"], "title": "D$^2$GSLAM: 4D Dynamic Gaussian Splatting SLAM", "comment": null, "summary": "Recent advances in Dense Simultaneous Localization and Mapping (SLAM) have demonstrated remarkable performance in static environments. However, dense SLAM in dynamic environments remains challenging. Most methods directly remove dynamic objects and focus solely on static scene reconstruction, which ignores the motion information contained in these dynamic objects. In this paper, we present D$^2$GSLAM, a novel dynamic SLAM system utilizing Gaussian representation, which simultaneously performs accurate dynamic reconstruction and robust tracking within dynamic environments. Our system is composed of four key components: (i) We propose a geometric-prompt dynamic separation method to distinguish between static and dynamic elements of the scene. This approach leverages the geometric consistency of Gaussian representation and scene geometry to obtain coarse dynamic regions. The regions then serve as prompts to guide the refinement of the coarse mask for achieving accurate motion mask. (ii) To facilitate accurate and efficient mapping of the dynamic scene, we introduce dynamic-static composite representation that integrates static 3D Gaussians with dynamic 4D Gaussians. This representation allows for modeling the transitions between static and dynamic states of objects in the scene for composite mapping and optimization. (iii) We employ a progressive pose refinement strategy that leverages both the multi-view consistency of static scene geometry and motion information from dynamic objects to achieve accurate camera tracking. (iv) We introduce a motion consistency loss, which leverages the temporal continuity in object motions for accurate dynamic modeling. Our D$^2$GSLAM demonstrates superior performance on dynamic scenes in terms of mapping and tracking accuracy, while also showing capability in accurate dynamic modeling."}
{"id": "2512.08953", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08953", "abs": "https://arxiv.org/abs/2512.08953", "authors": ["Filippo Cenacchi", "Longbing Cao", "Deborah Richards"], "title": "SimClinician: A Multimodal Simulation Testbed for Reliable Psychologist AI Collaboration in Mental Health Diagnosis", "comment": null, "summary": "AI based mental health diagnosis is often judged by benchmark accuracy, yet in practice its value depends on how psychologists respond whether they accept, adjust, or reject AI suggestions. Mental health makes this especially challenging: decisions are continuous and shaped by cues in tone, pauses, word choice, and nonverbal behaviors of patients. Current research rarely examines how AI diagnosis interface design influences these choices, leaving little basis for reliable testing before live studies. We present SimClinician, an interactive simulation platform, to transform patient data into psychologist AI collaborative diagnosis. Contributions include: (1) a dashboard integrating audio, text, and gaze-expression patterns; (2) an avatar module rendering de-identified dynamics for analysis; (3) a decision layer that maps AI outputs to multimodal evidence, letting psychologists review AI reasoning, and enter a diagnosis. Tested on the E-DAIC corpus (276 clinical interviews, expanded to 480,000 simulations), SimClinician shows that a confirmation step raises acceptance by 23%, keeping escalations below 9%, and maintaining smooth interaction flow."}
{"id": "2512.09431", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09431", "abs": "https://arxiv.org/abs/2512.09431", "authors": ["Quanyou Wang", "Mingzhang Zhu", "Ruochen Hou", "Kay Gillespie", "Alvin Zhu", "Shiqi Wang", "Yicheng Wang", "Gaberiel I. Fernandez", "Yeting Liu", "Colin Togashi", "Hyunwoo Nam", "Aditya Navghare", "Alex Xu", "Taoyuanmin Zhu", "Min Sung Ahn", "Arturo Flores Alvarez", "Justin Quan", "Ethan Hong", "Dennis W. Hong"], "title": "A Hierarchical, Model-Based System for High-Performance Humanoid Soccer", "comment": null, "summary": "The development of athletic humanoid robots has gained significant attention as advances in actuation, sensing, and control enable increasingly dynamic, real-world capabilities. RoboCup, an international competition of fully autonomous humanoid robots, provides a uniquely challenging benchmark for such systems, culminating in the long-term goal of competing against human soccer players by 2050. This paper presents the hardware and software innovations underlying our team's victory in the RoboCup 2024 Adult-Sized Humanoid Soccer Competition. On the hardware side, we introduce an adult-sized humanoid platform built with lightweight structural components, high-torque quasi-direct-drive actuators, and a specialized foot design that enables powerful in-gait kicks while preserving locomotion robustness. On the software side, we develop an integrated perception and localization framework that combines stereo vision, object detection, and landmark-based fusion to provide reliable estimates of the ball, goals, teammates, and opponents. A mid-level navigation stack then generates collision-aware, dynamically feasible trajectories, while a centralized behavior manager coordinates high-level decision making, role selection, and kick execution based on the evolving game state. The seamless integration of these subsystems results in fast, precise, and tactically effective gameplay, enabling robust performance under the dynamic and adversarial conditions of real matches. This paper presents the design principles, system architecture, and experimental results that contributed to ARTEMIS's success as the 2024 Adult-Sized Humanoid Soccer champion."}
{"id": "2512.08995", "categories": ["cs.HC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.08995", "abs": "https://arxiv.org/abs/2512.08995", "authors": ["Kapalik Khanal", "Biswash Khatiwada", "Stephen Afrifa", "Ranjan Sapkota", "Sanjay Shah", "Frank Bai", "Ramesh Bahadur Bist"], "title": "PoultryTalk: A Multi-modal Retrieval-Augmented Generation (RAG) System for Intelligent Poultry Management and Decision Support", "comment": null, "summary": "The Poultry industry plays a vital role in global food security, yet small- and medium-scale farmers frequently lack timely access to expert-level support for disease diagnosis, nutrition planning, and management decisions. With rising climate stress, unpredictable feed prices, and persistent disease threats, poultry producers often struggle to make quick, informed decisions. Therefore, there is a critical need for intelligent, data-driven systems that can deliver reliable, on-demand consultation. This paper presents PoultryTalk, a novel multi-modal Retrieval-Augmented Generation (RAG) system designed to provide real-time expert guidance through text and image-based interaction. PoultryTalk uses OpenAI's text-embedding-3-small and GPT-4o to provide smart, context-aware poultry management advice from text, images, or questions. System usability and performance were evaluated using 200 expert-verified queries and feedback from 34 participants who submitted 267 queries to the PoultryTalk prototype. The expert-verified benchmark queries confirmed strong technical performance, achieving a semantic similarity of 84.0% and an average response latency of 3.6 seconds. Compared with OpenAI's GPT-4o, PoultryTalk delivered more accurate and reliable information related to poultry. Based on participants' evaluations, PoultryTalk achieved a response accuracy of 89.9%, with about 9.1% of responses rated as incorrect. A post-use survey indicated high user satisfaction: 95.6% of participants reported that the chatbot provided \"always correct\" and \"mostly correct\" answers. 82.6% indicated they would recommend the tool, and 17.4% responded \"maybe.\" These results collectively demonstrate that PoultryTalk not only delivers accurate, contextually relevant information but also demonstrates strong user acceptance and scalability potential."}
{"id": "2512.09447", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09447", "abs": "https://arxiv.org/abs/2512.09447", "authors": ["Jaehyun Kim", "Seungwon Choi", "Tae-Wan Kim"], "title": "Sequential Testing for Descriptor-Agnostic LiDAR Loop Closure in Repetitive Environments", "comment": "8 pages, 4 figures", "summary": "We propose a descriptor-agnostic, multi-frame loop closure verification method that formulates LiDAR loop closure as a truncated Sequential Probability Ratio Test (SPRT). Instead of deciding from a single descriptor comparison or using fixed thresholds with late-stage Iterative Closest Point (ICP) vetting, the verifier accumulates a short temporal stream of descriptor similarities between a query and each candidate. It then issues an accept/reject decision adaptively once sufficient multi-frame evidence has been observed, according to user-specified Type-I/II error design targets. This precision-first policy is designed to suppress false positives in structurally repetitive indoor environments. We evaluate the verifier on a five-sequence library dataset, using a fixed retrieval front-end with several representative LiDAR global descriptors. Performance is assessed via segment-level K-hit precision-recall and absolute trajectory error (ATE) and relative pose error (RPE) after pose graph optimization. Across descriptors, the sequential verifier consistently improves precision and reduces the impact of aliased loops compared with single-frame and heuristic multi-frame baselines. Our implementation and dataset will be released at: https://github.com/wanderingcar/snu_library_dataset."}
{"id": "2512.09014", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.09014", "abs": "https://arxiv.org/abs/2512.09014", "authors": ["Evy van Weelden", "Jos M. Prinsen", "Caterina Ceccato", "Ethel Pruss", "Anita Vrins", "Maryam Alimardani", "Travis J. Wiltshire", "Max M. Louwerse"], "title": "Prototyping and Evaluating a Real-time Neuro-Adaptive Virtual Reality Flight Training System", "comment": null, "summary": "Real-time adjustments to task difficulty during flight training are crucial for optimizing performance and managing pilot workload. This study evaluated the functionality of a pre-trained brain-computer interface (BCI) that adapts training difficulty based on real-time estimations of workload from brain signals. Specifically, an EEG-based neuro-adaptive training system was developed and tested in Virtual Reality (VR) flight simulations with military student pilots. The neuro-adaptive system was compared to a fixed sequence that progressively increased in difficulty, in terms of self-reported user engagement, workload, and simulator sickness (subjective measures), as well as flight performance (objective metric). Additionally, we explored the relationships between subjective workload and flight performance in the VR simulator for each condition. The experiments concluded with semi-structured interviews to elicit the pilots' experience with the neuro-adaptive prototype. Results revealed no significant differences between the adaptive and fixed sequence conditions in subjective measures or flight performance. In both conditions, flight performance decreased as subjective workload increased. The semi-structured interviews indicated that, upon briefing, the pilots preferred the neuro-adaptive VR training system over the system with a fixed sequence, although individual differences were observed in the perception of difficulty and the order of changes in difficulty. Even though this study shows performance does not change, BCI-based flight training systems hold the potential to provide a more personalized and varied training experience."}
{"id": "2512.09462", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09462", "abs": "https://arxiv.org/abs/2512.09462", "authors": ["Jayant Unde", "Takumi Inden", "Yuki Wakayama", "Jacinto Colan", "Yaonan Zhu", "Tadayoshi Aoyama", "Yasuhisa Hasegawa"], "title": "Development of a Compliant Gripper for Safe Robot-Assisted Trouser Dressing-Undressing", "comment": null, "summary": "In recent years, many countries, including Japan, have rapidly aging populations, making the preservation of seniors' quality of life a significant concern. For elderly people with impaired physical abilities, support for toileting is one of the most important issues. This paper details the design, development, experimental assessment, and potential application of the gripper system, with a focus on the unique requirements and obstacles involved in aiding elderly or hemiplegic individuals in dressing and undressing trousers. The gripper we propose seeks to find the right balance between compliance and grasping forces, ensuring precise manipulation while maintaining a safe and compliant interaction with the users. The gripper's integration into a custom--built robotic manipulator system provides a comprehensive solution for assisting hemiplegic individuals in their dressing and undressing tasks. Experimental evaluations and comparisons with existing studies demonstrate the gripper's ability to successfully assist in both dressing and dressing of trousers in confined spaces with a high success rate. This research contributes to the advancement of assistive robotics, empowering elderly, and physically impaired individuals to maintain their independence and improve their quality of life."}
{"id": "2512.09085", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.09085", "abs": "https://arxiv.org/abs/2512.09085", "authors": ["Janet V. T. Pauketat", "Daniel B. Shank", "Aikaterina Manoli", "Jacy Reese Anthis"], "title": "Mental Models of Autonomy and Sentience Shape Reactions to AI", "comment": "37 pages, 6 figures, 2 tables", "summary": "Narratives about artificial intelligence (AI) entangle autonomy, the capacity to self-govern, with sentience, the capacity to sense and feel. AI agents that perform tasks autonomously and companions that recognize and express emotions may activate mental models of autonomy and sentience, respectively, provoking distinct reactions. To examine this possibility, we conducted three pilot studies (N = 374) and four preregistered vignette experiments describing an AI as autonomous, sentient, both, or neither (N = 2,702). Activating a mental model of sentience increased general mind perception (cognition and emotion) and moral consideration more than autonomy, but autonomy increased perceived threat more than sentience. Sentience also increased perceived autonomy more than vice versa. Based on a within-paper meta-analysis, sentience changed reactions more than autonomy on average. By disentangling different mental models of AI, we can study human-AI interaction with more precision to better navigate the detailed design of anthropomorphized AI and prompting interfaces."}
{"id": "2512.09495", "categories": ["cs.RO", "cs.CG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.09495", "abs": "https://arxiv.org/abs/2512.09495", "authors": ["Edwin Meriaux", "Shuo Wen", "Louis-Roy Langevin", "Doina Precup", "Antonio Loría", "Gregory Dudek"], "title": "On Mobile Ad Hoc Networks for Coverage of Partially Observable Worlds", "comment": null, "summary": "This paper addresses the movement and placement of mobile agents to establish a communication network in initially unknown environments. We cast the problem in a computational-geometric framework by relating the coverage problem and line-of-sight constraints to the Cooperative Guard Art Gallery Problem, and introduce its partially observable variant, the Partially Observable Cooperative Guard Art Gallery Problem (POCGAGP). We then present two algorithms that solve POCGAGP: CADENCE, a centralized planner that incrementally selects 270 degree corners at which to deploy agents, and DADENCE, a decentralized scheme that coordinates agents using local information and lightweight messaging. Both approaches operate under partial observability and target simultaneous coverage and connectivity. We evaluate the methods in simulation across 1,500 test cases of varied size and structure, demonstrating consistent success in forming connected networks while covering and exploring unknown space. These results highlight the value of geometric abstractions for communication-driven exploration and show that decentralized policies are competitive with centralized performance while retaining scalability."}
{"id": "2512.09190", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09190", "abs": "https://arxiv.org/abs/2512.09190", "authors": ["Prithila Angkan", "Paul Hungler", "Ali Etemad"], "title": "Understanding Mental States in Active and Autonomous Driving with EEG", "comment": "15 Pages, 13 Figures and 3 Tables. This work has been submitted to IEEE Transaction for possible publication", "summary": "Understanding how driver mental states differ between active and autonomous driving is critical for designing safe human-vehicle interfaces. This paper presents the first EEG-based comparison of cognitive load, fatigue, valence, and arousal across the two driving modes. Using data from 31 participants performing identical tasks in both scenarios of three different complexity levels, we analyze temporal patterns, task-complexity effects, and channel-wise activation differences. Our findings show that although both modes evoke similar trends across complexity levels, the intensity of mental states and the underlying neural activation differ substantially, indicating a clear distribution shift between active and autonomous driving. Transfer-learning experiments confirm that models trained on active driving data generalize poorly to autonomous driving and vice versa. We attribute this distribution shift primarily to differences in motor engagement and attentional demands between the two driving modes, which lead to distinct spatial and temporal EEG activation patterns. Although autonomous driving results in lower overall cortical activation, participants continue to exhibit measurable fluctuations in cognitive load, fatigue, valence, and arousal associated with readiness to intervene, task-evoked emotional responses, and monotony-related passive fatigue. These results emphasize the need for scenario-specific data and models when developing next-generation driver monitoring systems for autonomous vehicles."}
{"id": "2512.09510", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09510", "abs": "https://arxiv.org/abs/2512.09510", "authors": ["Donato Caramia", "Florian T. Pokorny", "Giuseppe Triggiani", "Denis Ruffino", "David Naso", "Paolo Roberto Massenio"], "title": "ViTA-Seg: Vision Transformer for Amodal Segmentation in Robotics", "comment": null, "summary": "Occlusions in robotic bin picking compromise accurate and reliable grasp planning. We present ViTA-Seg, a class-agnostic Vision Transformer framework for real-time amodal segmentation that leverages global attention to recover complete object masks, including hidden regions. We proposte two architectures: a) Single-Head for amodal mask prediction; b) Dual-Head for amodal and occluded mask prediction. We also introduce ViTA-SimData, a photo-realistic synthetic dataset tailored to industrial bin-picking scenario. Extensive experiments on two amodal benchmarks, COOCA and KINS, demonstrate that ViTA-Seg Dual Head achieves strong amodal and occlusion segmentation accuracy with computational efficiency, enabling robust, real-time robotic manipulation."}
{"id": "2512.09443", "categories": ["cs.HC", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.09443", "abs": "https://arxiv.org/abs/2512.09443", "authors": ["Chenyi Li", "Zhijian Lai", "Dong An", "Jiang Hu", "Zaiwen Wen"], "title": "Advancing Research via Human-AI Interactive Theorem Proving", "comment": null, "summary": "We investigate how large language models can be used as research tools in scientific computing while preserving mathematical rigor. We propose a human-in-the-loop workflow for interactive theorem proving and discovery with LLMs. Human experts retain control over problem formulation and admissible assumptions, while the model searches for proofs or contradictions, proposes candidate properties and theorems, and helps construct structures and parameters that satisfy explicit constraints, supported by numerical experiments and simple verification checks. Experts treat these outputs as raw material, further refine them, and organize the results into precise statements and rigorous proofs. We instantiate this workflow in a case study on the connection between manifold optimization and Grover's quantum search algorithm, where the pipeline helps identify invariant subspaces, explore Grover-compatible retractions, and obtain convergence guarantees for the retraction-based gradient method. The framework provides a practical template for integrating large language models into frontier mathematical research, enabling faster exploration of proof space and algorithm design while maintaining transparent reasoning responsibilities. Although illustrated on manifold optimization problems in quantum computing, the principles extend to other core areas of scientific computing."}
{"id": "2512.09537", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09537", "abs": "https://arxiv.org/abs/2512.09537", "authors": ["Qihao Yuan", "Ziyu Cao", "Ming Cao", "Kailai Li"], "title": "REASAN: Learning Reactive Safe Navigation for Legged Robots", "comment": "8 pages", "summary": "We present a novel modularized end-to-end framework for legged reactive navigation in complex dynamic environments using a single light detection and ranging (LiDAR) sensor. The system comprises four simulation-trained modules: three reinforcement-learning (RL) policies for locomotion, safety shielding, and navigation, and a transformer-based exteroceptive estimator that processes raw point-cloud inputs. This modular decomposition of complex legged motor-control tasks enables lightweight neural networks with simple architectures, trained using standard RL practices with targeted reward shaping and curriculum design, without reliance on heuristics or sophisticated policy-switching mechanisms. We conduct comprehensive ablations to validate our design choices and demonstrate improved robustness compared to existing approaches in challenging navigation tasks. The resulting reactive safe navigation (REASAN) system achieves fully onboard and real-time reactive navigation across both single- and multi-robot settings in complex environments. We release our training and deployment code at https://github.com/ASIG-X/REASAN."}
{"id": "2512.09473", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.09473", "abs": "https://arxiv.org/abs/2512.09473", "authors": ["Yibowen Zhao", "Yiming Cao", "Zhiqi Shen", "Juan Du", "Yonghui Xu", "Lizhen Cui", "Cyril Leung"], "title": "An Efficient Interaction Human-AI Synergy System Bridging Visual Awareness and Large Language Model for Intensive Care Units", "comment": "This paper has been accepted by the Late Breaking Papers of the 2025 International Conference on Human Computer Interaction (HCII 2025)", "summary": "Intensive Care Units (ICUs) are critical environments characterized by high-stakes monitoring and complex data management. However, current practices often rely on manual data transcription and fragmented information systems, introducing potential risks to patient safety and operational efficiency. To address these issues, we propose a human-AI synergy system based on a cloud-edge-end architecture, which integrates visual-aware data extraction and semantic interaction mechanisms. Specifically, a visual-aware edge module non-invasively captures real-time physiological data from bedside monitors, reducing manual entry errors. To improve accessibility to fragmented data sources, a semantic interaction module, powered by a Large Language Model (LLM), enables physicians to perform efficient and intuitive voice-based queries over structured patient data. The hierarchical cloud-edge-end deployment ensures low-latency communication and scalable system performance. Our system reduces the cognitive burden on ICU nurses and physicians and demonstrates promising potential for broader applications in intelligent healthcare systems."}
{"id": "2512.09571", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09571", "abs": "https://arxiv.org/abs/2512.09571", "authors": ["Feng Yu", "Yu Hu", "Yang Su", "Yang Deng", "Linzuo Zhang", "Danping Zou"], "title": "Mastering Diverse, Unknown, and Cluttered Tracks for Robust Vision-Based Drone Racing", "comment": "8 pages, 9 figures, Robotics and Automation Letters accept", "summary": "Most reinforcement learning(RL)-based methods for drone racing target fixed, obstacle-free tracks, leaving the generalization to unknown, cluttered environments largely unaddressed. This challenge stems from the need to balance racing speed and collision avoidance, limited feasible space causing policy exploration trapped in local optima during training, and perceptual ambiguity between gates and obstacles in depth maps-especially when gate positions are only coarsely specified. To overcome these issues, we propose a two-phase learning framework: an initial soft-collision training phase that preserves policy exploration for high-speed flight, followed by a hard-collision refinement phase that enforces robust obstacle avoidance. An adaptive, noise-augmented curriculum with an asymmetric actor-critic architecture gradually shifts the policy's reliance from privileged gate-state information to depth-based visual input. We further impose Lipschitz constraints and integrate a track-primitive generator to enhance motion stability and cross-environment generalization. We evaluate our framework through extensive simulation and ablation studies, and validate it in real-world experiments on a computationally constrained quadrotor. The system achieves agile flight while remaining robust to gate-position errors, developing a generalizable drone racing framework with the capability to operate in diverse, partially unknown and cluttered environments. https://yufengsjtu.github.io/MasterRacing.github.io/"}
{"id": "2512.09511", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.09511", "abs": "https://arxiv.org/abs/2512.09511", "authors": ["Yiwei Yuan", "Zhiqing Wang", "Xiucheng Zhang", "Yichao Luo", "Shuya Lin", "Yang Bai", "Zhenhui Peng"], "title": "Exploring Community-Powered Conversational Agent for Health Knowledge Acquisition: A Case Study in Colorectal Cancer", "comment": null, "summary": "Online communities have become key platforms where young adults, actively seek and share information, including health knowledge. However, these users often face challenges when browsing these communities, such as fragmented content, varying information quality and unfamiliar terminology. Based on a survey with 56 participants and follow-up interviews, we identify common challenges and expected features for learning health knowledge. In this paper, we develop a computational workflow that integrates community content into a conversational agent named CanAnswer to facilitate health knowledge acquisition. Using colorectal cancer as a case study, we evaluate CanAnswer through a lab study with 24 participants and interviews with six medical experts. Results show that CanAnswer improves the recalled gained knowledge and reduces the task workload of the learning session. Our expert interviews (N=6) further confirm the reliability and usefulness of CanAnswer. We discuss the generality of CanAnswer and provide design considerations for enhancing the usefulness and credibility of community-powered learning tools."}
{"id": "2512.09607", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09607", "abs": "https://arxiv.org/abs/2512.09607", "authors": ["Yanghong Mei", "Yirong Yang", "Longteng Guo", "Qunbo Wang", "Ming-Ming Yu", "Xingjian He", "Wenjun Wu", "Jing Liu"], "title": "UrbanNav: Learning Language-Guided Urban Navigation from Web-Scale Human Trajectories", "comment": "9 pages, 5 figures, accepted to AAAI 2026", "summary": "Navigating complex urban environments using natural language instructions poses significant challenges for embodied agents, including noisy language instructions, ambiguous spatial references, diverse landmarks, and dynamic street scenes. Current visual navigation methods are typically limited to simulated or off-street environments, and often rely on precise goal formats, such as specific coordinates or images. This limits their effectiveness for autonomous agents like last-mile delivery robots navigating unfamiliar cities. To address these limitations, we introduce UrbanNav, a scalable framework that trains embodied agents to follow free-form language instructions in diverse urban settings. Leveraging web-scale city walking videos, we develop an scalable annotation pipeline that aligns human navigation trajectories with language instructions grounded in real-world landmarks. UrbanNav encompasses over 1,500 hours of navigation data and 3 million instruction-trajectory-landmark triplets, capturing a wide range of urban scenarios. Our model learns robust navigation policies to tackle complex urban scenarios, demonstrating superior spatial reasoning, robustness to noisy instructions, and generalization to unseen urban settings. Experimental results show that UrbanNav significantly outperforms existing methods, highlighting the potential of large-scale web video data to enable language-guided, real-world urban navigation for embodied agents."}
{"id": "2512.09577", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09577", "abs": "https://arxiv.org/abs/2512.09577", "authors": ["Aris Hofmann", "Inge Vejsbjerg", "Dhaval Salwala", "Elizabeth M. Daly"], "title": "Auto-BenchmarkCard: Automated Synthesis of Benchmark Documentation", "comment": null, "summary": "We present Auto-BenchmarkCard, a workflow for generating validated descriptions of AI benchmarks. Benchmark documentation is often incomplete or inconsistent, making it difficult to interpret and compare benchmarks across tasks or domains. Auto-BenchmarkCard addresses this gap by combining multi-agent data extraction from heterogeneous sources (e.g., Hugging Face, Unitxt, academic papers) with LLM-driven synthesis. A validation phase evaluates factual accuracy through atomic entailment scoring using the FactReasoner tool. This workflow has the potential to promote transparency, comparability, and reusability in AI benchmark reporting, enabling researchers and practitioners to better navigate and evaluate benchmark choices."}
{"id": "2512.09608", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09608", "abs": "https://arxiv.org/abs/2512.09608", "authors": ["Zhiheng Li", "Weihua Wang", "Qiang Shen", "Yichen Zhao", "Zheng Fang"], "title": "Super4DR: 4D Radar-centric Self-supervised Odometry and Gaussian-based Map Optimization", "comment": "17 pages, 20 figures", "summary": "Conventional SLAM systems using visual or LiDAR data often struggle in poor lighting and severe weather. Although 4D radar is suited for such environments, its sparse and noisy point clouds hinder accurate odometry estimation, while the radar maps suffer from obscure and incomplete structures. Thus, we propose Super4DR, a 4D radar-centric framework for learning-based odometry estimation and gaussian-based map optimization. First, we design a cluster-aware odometry network that incorporates object-level cues from the clustered radar points for inter-frame matching, alongside a hierarchical self-supervision mechanism to overcome outliers through spatio-temporal consistency, knowledge transfer, and feature contrast. Second, we propose using 3D gaussians as an intermediate representation, coupled with a radar-specific growth strategy, selective separation, and multi-view regularization, to recover blurry map areas and those undetected based on image texture. Experiments show that Super4DR achieves a 67% performance gain over prior self-supervised methods, nearly matches supervised odometry, and narrows the map quality disparity with LiDAR while enabling multi-modal image rendering."}
{"id": "2512.09610", "categories": ["cs.HC", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09610", "abs": "https://arxiv.org/abs/2512.09610", "authors": ["Boyin Yang", "Puming Jiang", "Per Ola Kristensson"], "title": "ImageTalk: Designing a Multimodal AAC Text Generation System Driven by Image Recognition and Natural Language Generation", "comment": "24 pages, 10 figures", "summary": "People living with Motor Neuron Disease (plwMND) frequently encounter speech and motor impairments that necessitate a reliance on augmentative and alternative communication (AAC) systems. This paper tackles the main challenge that traditional symbol-based AAC systems offer a limited vocabulary, while text entry solutions tend to exhibit low communication rates. To help plwMND articulate their needs about the system efficiently and effectively, we iteratively design and develop a novel multimodal text generation system called ImageTalk through a tailored proxy-user-based and an end-user-based design phase. The system demonstrates pronounced keystroke savings of 95.6%, coupled with consistent performance and high user satisfaction. We distill three design guidelines for AI-assisted text generation systems design and outline four user requirement levels tailored for AAC purposes, guiding future research in this field."}
{"id": "2512.09619", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09619", "abs": "https://arxiv.org/abs/2512.09619", "authors": ["Minghao Guo", "Meng Cao", "Jiachen Tao", "Rongtao Xu", "Yan Yan", "Xiaodan Liang", "Ivan Laptev", "Xiaojun Chang"], "title": "GLaD: Geometric Latent Distillation for Vision-Language-Action Models", "comment": null, "summary": "Most existing Vision-Language-Action (VLA) models rely primarily on RGB information, while ignoring geometric cues crucial for spatial reasoning and manipulation. In this work, we introduce GLaD, a geometry-aware VLA framework that incorporates 3D geometric priors during pretraining through knowledge distillation. Rather than distilling geometric features solely into the vision encoder, we align the LLM's hidden states corresponding to visual tokens with features from a frozen geometry-aware vision transformer (VGGT), ensuring that geometric understanding is deeply integrated into the multimodal representations that drive action prediction. Pretrained on the Bridge dataset with this geometry distillation mechanism, GLaD achieves 94.1% average success rate across four LIBERO task suites, outperforming UniVLA (92.5%) which uses identical pretraining data. These results validate that geometry-aware pretraining enhances spatial reasoning and policy generalization without requiring explicit depth sensors or 3D annotations."}
{"id": "2512.09755", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.09755", "abs": "https://arxiv.org/abs/2512.09755", "authors": ["Albrecht Kurze", "Andreas Bischof", "Arne Berger"], "title": "Smart, simple, sincere - Why and how we should rethink connected things in our smart homes", "comment": "State of Responsible Technology 2025 - Generative Things. pp 24-30. Stichting ThingsCon Amsterdam", "summary": "More and more smart connected things and services turn our homes into smart environments. They promise comfort, efficiency and security. These devices often integrate simple sensors, e.g. for temperature, light or humidity, etc. However, these smart but yet simple sensors can pose a sincere privacy risk. The sensor data enables sense-making of home attendance, domestic activities and even health conditions, often a fact that neither users nor developers are aware of or do not know how to address. Nevertheless, not all is lost or evil. This article makes a plea for how we, the ThingsCon community, might rethink smart connected things and services in our homes. We show this in our approaches and research projects that we initiated."}
{"id": "2512.09656", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09656", "abs": "https://arxiv.org/abs/2512.09656", "authors": ["Nicolas Marticorena", "Tobias Fischer", "Niko Suenderhauf"], "title": "ReMoSPLAT: Reactive Mobile Manipulation Control on a Gaussian Splat", "comment": "9 pages, 5 figures", "summary": "Reactive control can gracefully coordinate the motion of the base and the arm of a mobile manipulator. However, incorporating an accurate representation of the environment to avoid obstacles without involving costly planning remains a challenge. In this work, we present ReMoSPLAT, a reactive controller based on a quadratic program formulation for mobile manipulation that leverages a Gaussian Splat representation for collision avoidance. By integrating additional constraints and costs into the optimisation formulation, a mobile manipulator platform can reach its intended end effector pose while avoiding obstacles, even in cluttered scenes. We investigate the trade-offs of two methods for efficiently calculating robot-obstacle distances, comparing a purely geometric approach with a rasterisation-based approach. Our experiments in simulation on both synthetic and real-world scans demonstrate the feasibility of our method, showing that the proposed approach achieves performance comparable to controllers that rely on perfect ground-truth information."}
{"id": "2512.09802", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.09802", "abs": "https://arxiv.org/abs/2512.09802", "authors": ["Tomás Alves", "João Moreira"], "title": "Building a Data Dashboard for Magic: The Gathering: Initial Design Considerations", "comment": null, "summary": "This paper presents the initial stages of a design study aimed at developing a dashboard to visualize gameplay data of the Commander format from Magic: The Gathering. We conducted a user-task analysis to identify requirements for a data visualization dashboard tailored to the Commander format. Afterwards, we proposed a design for the dashboard leveraging visualizations to address players' needs and pain points for typical data analysis tasks in the context domain. Then, we followed-up with a structured user test to evaluate players' comprehension and preferences of data visualizations. Results show that players prioritize contextually relevant, outcome-driven metrics over peripheral ones, and that canonical charts like heatmaps and line charts support higher comprehension than complex ones such as scatterplots or icicle plots. Our findings also highlight the importance of localized views, user customization, and progressive disclosure, emphasizing that adaptability and contextual relevance are as essential as accuracy in effective dashboard design. Our study contributes practical design guidelines for data visualization in gaming contexts and highlights broader implications for engagement-driven dashboards."}
{"id": "2512.09798", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09798", "abs": "https://arxiv.org/abs/2512.09798", "authors": ["Misael Mamani", "Mariel Fernandez", "Grace Luna", "Steffani Limachi", "Leonel Apaza", "Carolina Montes-Dávalos", "Marcelo Herrera", "Edwin Salcedo"], "title": "High-Resolution Water Sampling via a Solar-Powered Autonomous Surface Vehicle", "comment": null, "summary": "Accurate water quality assessment requires spatially resolved sampling, yet most unmanned surface vehicles (USVs) can collect only a limited number of samples or rely on single-point sensors with poor representativeness. This work presents a solar-powered, fully autonomous USV featuring a novel syringe-based sampling architecture capable of acquiring 72 discrete, contamination-minimized water samples per mission. The vehicle incorporates a ROS 2 autonomy stack with GPS-RTK navigation, LiDAR and stereo-vision obstacle detection, Nav2-based mission planning, and long-range LoRa supervision, enabling dependable execution of sampling routes in unstructured environments. The platform integrates a behavior-tree autonomy architecture adapted from Nav2, enabling mission-level reasoning and perception-aware navigation. A modular 6x12 sampling system, controlled by distributed micro-ROS nodes, provides deterministic actuation, fault isolation, and rapid module replacement, achieving spatial coverage beyond previously reported USV-based samplers. Field trials in Achocalla Lagoon (La Paz, Bolivia) demonstrated 87% waypoint accuracy, stable autonomous navigation, and accurate physicochemical measurements (temperature, pH, conductivity, total dissolved solids) comparable to manually collected references. These results demonstrate that the platform enables reliable high-resolution sampling and autonomous mission execution, providing a scalable solution for aquatic monitoring in remote environments."}
{"id": "2512.09086", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.09086", "abs": "https://arxiv.org/abs/2512.09086", "authors": ["Xinyu Qi", "Zeyu Deng", "Shaun Alexander Macdonald", "Liying Li", "Chen Wang", "Muhammad Ali Imran", "Philip G. Zhao"], "title": "Inferring Operator Emotions from a Motion-Controlled Robotic Arm", "comment": null, "summary": "A remote robot operator's affective state can significantly impact the resulting robot's motions leading to unexpected consequences, even when the user follows protocol and performs permitted tasks. The recognition of a user operator's affective states in remote robot control scenarios is, however, underexplored. Current emotion recognition methods rely on reading the user's vital signs or body language, but the devices and user participation these measures require would add limitations to remote robot control. We demonstrate that the functional movements of a remote-controlled robotic avatar, which was not designed for emotional expression, can be used to infer the emotional state of the human operator via a machine-learning system. Specifically, our system achieved 83.3$\\%$ accuracy in recognizing the user's emotional state expressed by robot movements, as a result of their hand motions. We discuss the implications of this system on prominent current and future remote robot operation and affective robotic contexts."}
{"id": "2512.09833", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09833", "abs": "https://arxiv.org/abs/2512.09833", "authors": ["Elias Krantz", "Ngai Nam Chan", "Gunnar Tibert", "Huina Mao", "Christer Fuglesang"], "title": "Bridging the Basilisk Astrodynamics Framework with ROS 2 for Modular Spacecraft Simulation and Hardware Integration", "comment": "Presented at the International Conference on Space Robotics (iSpaRo) 2025. To appear in IEEE Xplore", "summary": "Integrating high-fidelity spacecraft simulators with modular robotics frameworks remains a challenge for autonomy development. This paper presents a lightweight, open-source communication bridge between the Basilisk astrodynamics simulator and the Robot Operating System 2 (ROS 2), enabling real-time, bidirectional data exchange for spacecraft control. The bridge requires no changes to Basilisk's core and integrates seamlessly with ROS 2 nodes. We demonstrate its use in a leader-follower formation flying scenario using nonlinear model predictive control, deployed identically in both simulation and on the ATMOS planar microgravity testbed. This setup supports rapid development, hardware-in-the-loop testing, and seamless transition from simulation to hardware. The bridge offers a flexible and scalable platform for modular spacecraft autonomy and reproducible research workflows."}
{"id": "2512.09105", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.09105", "abs": "https://arxiv.org/abs/2512.09105", "authors": ["Adi Manor", "Dan Cohen", "Ziv Keidar", "Avi Parush", "Hadas Erel"], "title": "Cognitive Trust in HRI: \"Pay Attention to Me and I'll Trust You Even if You are Wrong\"", "comment": "Confrence paper", "summary": "Cognitive trust and the belief that a robot is capable of accurately performing tasks, are recognized as central factors in fostering high-quality human-robot interactions. It is well established that performance factors such as the robot's competence and its reliability shape cognitive trust. Recent studies suggest that affective factors, such as robotic attentiveness, also play a role in building cognitive trust. This work explores the interplay between these two factors that shape cognitive trust. Specifically, we evaluated whether different combinations of robotic competence and attentiveness introduce a compensatory mechanism, where one factor compensates for the lack of the other. In the experiment, participants performed a search task with a robotic dog in a 2x2 experimental design that included two factors: competence (high or low) and attentiveness (high or low). The results revealed that high attentiveness can compensate for low competence. Participants who collaborated with a highly attentive robot that performed poorly reported trust levels comparable to those working with a highly competent robot. When the robot did not demonstrate attentiveness, low competence resulted in a substantial decrease in cognitive trust. The findings indicate that building cognitive trust in human-robot interaction may be more complex than previously believed, involving emotional processes that are typically overlooked. We highlight an affective compensatory mechanism that adds a layer to consider alongside traditional competence-based models of cognitive trust."}
{"id": "2512.09851", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09851", "abs": "https://arxiv.org/abs/2512.09851", "authors": ["Yuyang Li", "Yinghan Chen", "Zihang Zhao", "Puhao Li", "Tengyu Liu", "Siyuan Huang", "Yixin Zhu"], "title": "Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation", "comment": null, "summary": "Robotic manipulation requires both rich multimodal perception and effective learning frameworks to handle complex real-world tasks. See-through-skin (STS) sensors, which combine tactile and visual perception, offer promising sensing capabilities, while modern imitation learning provides powerful tools for policy acquisition. However, existing STS designs lack simultaneous multimodal perception and suffer from unreliable tactile tracking. Furthermore, integrating these rich multimodal signals into learning-based manipulation pipelines remains an open challenge. We introduce TacThru, an STS sensor enabling simultaneous visual perception and robust tactile signal extraction, and TacThru-UMI, an imitation learning framework that leverages these multimodal signals for manipulation. Our sensor features a fully transparent elastomer, persistent illumination, novel keyline markers, and efficient tracking, while our learning system integrates these signals through a Transformer-based Diffusion Policy. Experiments on five challenging real-world tasks show that TacThru-UMI achieves an average success rate of 85.5%, significantly outperforming the baselines of alternating tactile-visual (66.3%) and vision-only (55.4%). The system excels in critical scenarios, including contact detection with thin and soft objects and precision manipulation requiring multimodal coordination. This work demonstrates that combining simultaneous multimodal perception with modern learning frameworks enables more precise, adaptable robotic manipulation."}
{"id": "2512.09898", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.MA", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.09898", "abs": "https://arxiv.org/abs/2512.09898", "authors": ["Reza Ahmari", "Ahmad Mohammadi", "Vahid Hemmati", "Mohammed Mynuddin", "Parham Kebria", "Mahmoud Nabil Mahmoud", "Xiaohong Yuan", "Abdollah Homaifar"], "title": "Visual Heading Prediction for Autonomous Aerial Vehicles", "comment": null, "summary": "The integration of Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) is increasingly central to the development of intelligent autonomous systems for applications such as search and rescue, environmental monitoring, and logistics. However, precise coordination between these platforms in real-time scenarios presents major challenges, particularly when external localization infrastructure such as GPS or GNSS is unavailable or degraded [1]. This paper proposes a vision-based, data-driven framework for real-time UAV-UGV integration, with a focus on robust UGV detection and heading angle prediction for navigation and coordination. The system employs a fine-tuned YOLOv5 model to detect UGVs and extract bounding box features, which are then used by a lightweight artificial neural network (ANN) to estimate the UAV's required heading angle. A VICON motion capture system was used to generate ground-truth data during training, resulting in a dataset of over 13,000 annotated images collected in a controlled lab environment. The trained ANN achieves a mean absolute error of 0.1506° and a root mean squared error of 0.1957°, offering accurate heading angle predictions using only monocular camera inputs. Experimental evaluations achieve 95% accuracy in UGV detection. This work contributes a vision-based, infrastructure- independent solution that demonstrates strong potential for deployment in GPS/GNSS-denied environments, supporting reliable multi-agent coordination under realistic dynamic conditions. A demonstration video showcasing the system's real-time performance, including UGV detection, heading angle prediction, and UAV alignment under dynamic conditions, is available at: https://github.com/Kooroshraf/UAV-UGV-Integration"}
{"id": "2512.09903", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09903", "abs": "https://arxiv.org/abs/2512.09903", "authors": ["Ryan Meegan", "Adam D'Souza", "Bryan Bo Cao", "Shubham Jain", "Kristin Dana"], "title": "YOPO-Nav: Visual Navigation using 3DGS Graphs from One-Pass Videos", "comment": null, "summary": "Visual navigation has emerged as a practical alternative to traditional robotic navigation pipelines that rely on detailed mapping and path planning. However, constructing and maintaining 3D maps is often computationally expensive and memory-intensive. We address the problem of visual navigation when exploration videos of a large environment are available. The videos serve as a visual reference, allowing a robot to retrace the explored trajectories without relying on metric maps. Our proposed method, YOPO-Nav (You Only Pass Once), encodes an environment into a compact spatial representation composed of interconnected local 3D Gaussian Splatting (3DGS) models. During navigation, the framework aligns the robot's current visual observation with this representation and predicts actions that guide it back toward the demonstrated trajectory. YOPO-Nav employs a hierarchical design: a visual place recognition (VPR) module provides coarse localization, while the local 3DGS models refine the goal and intermediate poses to generate control actions. To evaluate our approach, we introduce the YOPO-Campus dataset, comprising 4 hours of egocentric video and robot controller inputs from over 6 km of human-teleoperated robot trajectories. We benchmark recent visual navigation methods on trajectories from YOPO-Campus using a Clearpath Jackal robot. Experimental results show YOPO-Nav provides excellent performance in image-goal navigation for real-world scenes on a physical robot. The dataset and code will be made publicly available for visual navigation and scene representation research."}
{"id": "2512.09911", "categories": ["cs.RO", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2512.09911", "abs": "https://arxiv.org/abs/2512.09911", "authors": ["Radha Lahoti", "Ryan Chaiyakul", "M. Khalid Jawed"], "title": "Py-DiSMech: A Scalable and Efficient Framework for Discrete Differential Geometry-Based Modeling and Control of Soft Robots", "comment": "https://github.com/structuresComp/dismech-python", "summary": "High-fidelity simulation has become essential to the design and control of soft robots, where large geometric deformations and complex contact interactions challenge conventional modeling tools. Recent advances in the field demand simulation frameworks that combine physical accuracy, computational scalability, and seamless integration with modern control and optimization pipelines. In this work, we present Py-DiSMech, a Python-based, open-source simulation framework for modeling and control of soft robotic structures grounded in the principles of Discrete Differential Geometry (DDG). By discretizing geometric quantities such as curvature and strain directly on meshes, Py-DiSMech captures the nonlinear deformation of rods, shells, and hybrid structures with high fidelity and reduced computational cost. The framework introduces (i) a fully vectorized NumPy implementation achieving order-of-magnitude speed-ups over existing geometry-based simulators; (ii) a penalty-energy-based fully implicit contact model that supports rod-rod, rod-shell, and shell-shell interactions; (iii) a natural-strain-based feedback-control module featuring a proportional-integral (PI) controller for shape regulation and trajectory tracking; and (iv) a modular, object-oriented software design enabling user-defined elastic energies, actuation schemes, and integration with machine-learning libraries. Benchmark comparisons demonstrate that Py-DiSMech substantially outperforms the state-of-the-art simulator Elastica in computational efficiency while maintaining physical accuracy. Together, these features establish Py-DiSMech as a scalable, extensible platform for simulation-driven design, control validation, and sim-to-real research in soft robotics."}
{"id": "2512.09920", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09920", "abs": "https://arxiv.org/abs/2512.09920", "authors": ["Junting Chen", "Yunchuan Li", "Panfeng Jiang", "Jiacheng Du", "Zixuan Chen", "Chenrui Tie", "Jiajun Deng", "Lin Shao"], "title": "LISN: Language-Instructed Social Navigation with VLM-based Controller Modulating", "comment": "8 pages", "summary": "Towards human-robot coexistence, socially aware navigation is significant for mobile robots. Yet existing studies on this area focus mainly on path efficiency and pedestrian collision avoidance, which are essential but represent only a fraction of social navigation. Beyond these basics, robots must also comply with user instructions, aligning their actions to task goals and social norms expressed by humans. In this work, we present LISN-Bench, the first simulation-based benchmark for language-instructed social navigation. Built on Rosnav-Arena 3.0, it is the first standardized social navigation benchmark to incorporate instruction following and scene understanding across diverse contexts. To address this task, we further propose Social-Nav-Modulator, a fast-slow hierarchical system where a VLM agent modulates costmaps and controller parameters. Decoupling low-level action generation from the slower VLM loop reduces reliance on high-frequency VLM inference while improving dynamic avoidance and perception adaptability. Our method achieves an average success rate of 91.3%, which is greater than 63% than the most competitive baseline, with most of the improvements observed in challenging tasks such as following a person in a crowd and navigating while strictly avoiding instruction-forbidden regions. The project website is at: https://social-nav.github.io/LISN-project/"}
{"id": "2512.09927", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09927", "abs": "https://arxiv.org/abs/2512.09927", "authors": ["Yifan Ye", "Jiaqi Ma", "Jun Cen", "Zhihe Lu"], "title": "Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models", "comment": "8 pages, 5 figures", "summary": "Vision-Language-Action (VLA) models pretrained on large-scale multimodal datasets have emerged as powerful foundations for robotic perception and control. However, their massive scale, often billions of parameters, poses significant challenges for real-time deployment, as inference becomes computationally expensive and latency-sensitive in dynamic environments. To address this, we propose Token Expand-and-Merge-VLA (TEAM-VLA), a training-free token compression framework that accelerates VLA inference while preserving task performance. TEAM-VLA introduces a dynamic token expansion mechanism that identifies and samples additional informative tokens in the spatial vicinity of attention-highlighted regions, enhancing contextual completeness. These expanded tokens are then selectively merged in deeper layers under action-aware guidance, effectively reducing redundancy while maintaining semantic coherence. By coupling expansion and merging within a single feed-forward pass, TEAM-VLA achieves a balanced trade-off between efficiency and effectiveness, without any retraining or parameter updates. Extensive experiments on LIBERO benchmark demonstrate that TEAM-VLA consistently improves inference speed while maintaining or even surpassing the task success rate of full VLA models. The code is public available on \\href{https://github.com/Jasper-aaa/TEAM-VLA}{https://github.com/Jasper-aaa/TEAM-VLA}"}
{"id": "2512.09928", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09928", "abs": "https://arxiv.org/abs/2512.09928", "authors": ["Minghui Lin", "Pengxiang Ding", "Shu Wang", "Zifeng Zhuang", "Yang Liu", "Xinyang Tong", "Wenxuan Song", "Shangke Lyu", "Siteng Huang", "Donglin Wang"], "title": "HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models", "comment": "Project page: https://hifvla.github.io Github: https://github.com/OpenHelix-Team/HiF-VLA", "summary": "Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and linguistic cues into actions. However, most VLAs assume the Markov property, relying only on the current observation and thus suffering from temporal myopia that degrades long-horizon coherence. In this work, we view motion as a more compact and informative representation of temporal context and world dynamics, capturing inter-state changes while filtering static pixel-level noise. Building on this idea, we propose HiF-VLA (Hindsight, Insight, and Foresight for VLAs), a unified framework that leverages motion for bidirectional temporal reasoning. HiF-VLA encodes past dynamics through hindsight priors, anticipates future motion via foresight reasoning, and integrates both through a hindsight-modulated joint expert to enable a ''think-while-acting'' paradigm for long-horizon manipulation. As a result, HiF-VLA surpasses strong baselines on LIBERO-Long and CALVIN ABC-D benchmarks, while incurring negligible additional inference latency. Furthermore, HiF-VLA achieves substantial improvements in real-world long-horizon manipulation tasks, demonstrating its broad effectiveness in practical robotic settings."}
