{"id": "2602.15060", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15060", "abs": "https://arxiv.org/abs/2602.15060", "authors": ["Tengjie Zhu", "Guanyu Cai", "Yang Zhaohui", "Guanzhu Ren", "Haohui Xie", "ZiRui Wang", "Junsong Wu", "Jingbo Wang", "Xiaokang Yang", "Yao Mu", "Yichao Yan", "Yichao Yan"], "title": "CLOT: Closed-Loop Global Motion Tracking for Whole-Body Humanoid Teleoperation", "comment": null, "summary": "Long-horizon whole-body humanoid teleoperation remains challenging due to accumulated global pose drift, particularly on full-sized humanoids. Although recent learning-based tracking methods enable agile and coordinated motions, they typically operate in the robot's local frame and neglect global pose feedback, leading to drift and instability during extended execution. In this work, we present CLOT, a real-time whole-body humanoid teleoperation system that achieves closed-loop global motion tracking via high-frequency localization feedback. CLOT synchronizes operator and robot poses in a closed loop, enabling drift-free human-to-humanoid mimicry over long timehorizons. However, directly imposing global tracking rewards in reinforcement learning, often results in aggressive and brittle corrections. To address this, we propose a data-driven randomization strategy that decouples observation trajectories from reward evaluation, enabling smooth and stable global corrections. We further regularize the policy with an adversarial motion prior to suppress unnatural behaviors. To support CLOT, we collect 20 hours of carefully curated human motion data for training the humanoid teleoperation policy. We design a transformer-based policy and train it for over 1300 GPU hours. The policy is deployed on a full-sized humanoid with 31 DoF (excluding hands). Both simulation and real-world experiments verify high-dynamic motion, high-precision tracking, and strong robustness in sim-to-real humanoid teleoperation. Motion data, demos and code can be found in our website.", "AI": {"tldr": "CLOT\u662f\u4e00\u4e2a\u5b9e\u65f6\u7c7b\u4eba\u673a\u5668\u4eba\u8fdc\u7a0b\u64cd\u4f5c\u7cfb\u7edf\uff0c\u901a\u8fc7\u95ed\u73af\u5168\u7403\u8fd0\u52a8\u8ddf\u8e2a\u548c\u6570\u636e\u9a71\u52a8\u968f\u673a\u5316\u7b56\u7565\uff0c\u5b9e\u73b0\u9ad8\u52a8\u6001\u3001\u65e0\u6f02\u79fb\u7684\u64cd\u4f5c\uff0c\u9a8c\u8bc1\u4e86\u5728\u957f\u671f\u6267\u884c\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u957f\u671f\u7684\u5168\u8eab\u7c7b\u4eba\u673a\u5668\u4eba\u8fdc\u7a0b\u64cd\u4f5c\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5168\u5c3a\u5bf8\u7c7b\u4eba\u673a\u5668\u4eba\u4e2d\uff0c\u7531\u4e8e\u5168\u7403\u59ff\u6001\u6f02\u79fb\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faCLOT\uff0c\u4e00\u4e2a\u5b9e\u65f6\u5168\u8eab\u7c7b\u4eba\u673a\u5668\u4eba\u8fdc\u7a0b\u64cd\u4f5c\u7cfb\u7edf\uff0c\u901a\u8fc7\u9ad8\u9891\u5b9a\u4f4d\u53cd\u9988\u5b9e\u73b0\u95ed\u73af\u7684\u5168\u7403\u8fd0\u52a8\u8ddf\u8e2a\u3002\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u968f\u673a\u5316\u7b56\u7565\u548c\u5bf9\u6297\u8fd0\u52a8\u5148\u9a8c\u6765\u5e73\u6ed1\u548c\u7a33\u5b9a\u5730\u8fdb\u884c\u5168\u7403\u7ea0\u6b63\u3002", "result": "CLOT\u80fd\u591f\u5728\u957f\u65f6\u95f4\u5185\u5b9e\u73b0\u65e0\u6f02\u79fb\u7684\u4eba\u4e0e\u7c7b\u4eba\u673a\u5668\u4eba\u6a21\u4eff\uff0c\u4e14\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u9ad8\u52a8\u6001\u8fd0\u52a8\u3001\u9ad8\u7cbe\u5ea6\u8ddf\u8e2a\u548c\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "CLOT\u4e3a\u7c7b\u4eba\u673a\u5668\u4eba\u8fdc\u7a0b\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u5c11\u4e86\u64cd\u4f5c\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\u548c\u6f02\u79fb\u95ee\u9898\u3002"}}
{"id": "2602.15061", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15061", "abs": "https://arxiv.org/abs/2602.15061", "authors": ["Zihan Zhang", "Haohui Que", "Junhan Chang", "Xin Zhang", "Hao Wei", "Tong Zhu"], "title": "Safe-SDL:Establishing Safety Boundaries and Control Mechanisms for AI-Driven Self-Driving Laboratories", "comment": null, "summary": "The emergence of Self-Driving Laboratories (SDLs) transforms scientific discovery methodology by integrating AI with robotic automation to create closed-loop experimental systems capable of autonomous hypothesis generation, experimentation, and analysis. While promising to compress research timelines from years to weeks, their deployment introduces unprecedented safety challenges differing from traditional laboratories or purely digital AI. This paper presents Safe-SDL, a comprehensive framework for establishing robust safety boundaries and control mechanisms in AI-driven autonomous laboratories. We identify and analyze the critical ``Syntax-to-Safety Gap'' -- the disconnect between AI-generated syntactically correct commands and their physical safety implications -- as the central challenge in SDL deployment. Our framework addresses this gap through three synergistic components: (1) formally defined Operational Design Domains (ODDs) that constrain system behavior within mathematically verified boundaries, (2) Control Barrier Functions (CBFs) that provide real-time safety guarantees through continuous state-space monitoring, and (3) a novel Transactional Safety Protocol (CRUTD) that ensures atomic consistency between digital planning and physical execution. We ground our theoretical contributions through analysis of existing implementations including UniLabOS and the Osprey architecture, demonstrating how these systems instantiate key safety principles. Evaluation against the LabSafety Bench reveals that current foundation models exhibit significant safety failures, demonstrating that architectural safety mechanisms are essential rather than optional. Our framework provides both theoretical foundations and practical implementation guidance for safe deployment of autonomous scientific systems, establishing the groundwork for responsible acceleration of AI-driven discovery.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86Safe-SDL\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5efa\u7acb\u5b89\u5168\u8fb9\u754c\u548c\u63a7\u5236\u673a\u5236\u6765\u89e3\u51b3\u81ea\u9a71\u52a8\u5b9e\u9a8c\u5ba4\u7684\u5b89\u5168\u6311\u6218\uff0c\u91cd\u70b9\u8ba8\u8bba\u4e86AI\u751f\u6210\u547d\u4ee4\u4e0e\u5176\u7269\u7406\u5b89\u5168\u5f71\u54cd\u4e4b\u95f4\u7684\u8131\u8282\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u79cd\u534f\u540c\u7ec4\u4ef6\u4ee5\u786e\u4fdd\u5b89\u5168\u6027\u3002", "motivation": "\u81ea\u9a71\u52a8\u5b9e\u9a8c\u5ba4\u7684\u51fa\u73b0\u901a\u8fc7\u5c06AI\u4e0e\u673a\u5668\u4eba\u81ea\u52a8\u5316\u6574\u5408\uff0c\u53d8\u9769\u4e86\u79d1\u5b66\u53d1\u73b0\u7684\u65b9\u6cd5\u8bba\uff0c\u4f46\u4e5f\u5f15\u5165\u4e86\u524d\u6240\u672a\u6709\u7684\u5b89\u5168\u6311\u6218\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7efc\u5408\u6846\u67b6Safe-SDL\uff0c\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a\u6b63\u5f0f\u5b9a\u4e49\u7684\u64cd\u4f5c\u8bbe\u8ba1\u57df(Odds)\u3001\u63a7\u5236\u969c\u788d\u51fd\u6570(CBFs)\u548c\u65b0\u9896\u7684\u4e8b\u52a1\u5b89\u5168\u534f\u8bae(CRUTD)\uff0c\u4ee5\u786e\u4fddAI\u5b9e\u9a8c\u5ba4\u7684\u5b89\u5168\u3002", "result": "Evaluation against LabSafety Bench reveals significant safety failures in current foundation models, indicating\u5efa\u7b51\u5b89\u5168\u673a\u5236\u7684\u91cd\u8981\u6027\u3002", "conclusion": "Safe-SDL\u4e3aAI\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u5b9e\u9a8c\u5ba4\u63d0\u4f9b\u4e86\u5b89\u5168\u90e8\u7f72\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u6307\u5bfc\uff0c\u5960\u5b9a\u4e86\u8d1f\u8d23\u4efb\u52a0\u901fAI\u9a71\u52a8\u53d1\u73b0\u7684\u57fa\u7840\u3002"}}
{"id": "2602.15063", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.15063", "abs": "https://arxiv.org/abs/2602.15063", "authors": ["Yufeng Wang", "Yuan Xu", "Anastasia Nikolova", "Yuxuan Wang", "Jianyu Wang", "Chongyang Wang", "Xin Tong"], "title": "How Do We Research Human-Robot Interaction in the Age of Large Language Models? A Systematic Review", "comment": null, "summary": "Advances in large language models (LLMs) are profoundly reshaping the field of human-robot interaction (HRI). While prior work has highlighted the technical potential of LLMs, few studies have systematically examined their human-centered impact (e.g., human-oriented understanding, user modeling, and levels of autonomy), making it difficult to consolidate emerging challenges in LLM-driven HRI systems. Therefore, we conducted a systematic literature search following the PRISMA guideline, identifying 86 articles that met our inclusion criteria. Our findings reveal that: (1) LLMs are transforming the fundamentals of HRI by reshaping how robots sense context, generate socially grounded interactions, and maintain continuous alignment with human needs in embodied settings; and (2) current research is largely exploratory, with different studies focusing on different facets of LLM-driven HRI, resulting in wide-ranging choices of experimental setups, study methods, and evaluation metrics. Finally, we identify key design considerations and challenges, offering a coherent overview and guidelines for future research at the intersection of LLMs and HRI.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6027\u7814\u7a76\u4e86\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5bf9\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u4ea4\u4e92\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5176\u5728\u57fa\u672c\u673a\u5236\u4e0a\u4ea7\u751f\u4e86\u53d8\u9769\uff0c\u5e76\u6307\u51fa\u5f53\u524d\u7814\u7a76\u7684\u591a\u6837\u6027\u548c\u672a\u6765\u7814\u7a76\u7684\u6307\u5bfc\u65b9\u9488\u3002", "motivation": "\u63a2\u8ba8\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5728\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u4ea4\u4e92\u4e2d\u7684\u5f71\u54cd\uff0c\u5c3d\u7ba1\u5df2\u6709\u7814\u7a76\u5f3a\u8c03\u5176\u6280\u672f\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8003\u5bdf\u5176\u4eba\u672c\u5f71\u54cd\u7684\u7814\u7a76\u3002", "method": "\u6839\u636ePRISMA\u6307\u5357\u8fdb\u884c\u7cfb\u7edf\u6587\u732e\u68c0\u7d22\uff0c\u7b5b\u9009\u51fa86\u7bc7\u7b26\u5408\u7eb3\u5165\u6807\u51c6\u7684\u6587\u7ae0\u3002", "result": "\u53d1\u73b0\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u6b63\u5728\u8f6c\u53d8\u673a\u5668\u4eba\u611f\u77e5\u4e0a\u4e0b\u6587\u3001\u751f\u6210\u793e\u4ea4\u4e92\u52a8\u548c\u4e0e\u4eba\u7c7b\u9700\u6c42\u6301\u7eed\u5bf9\u9f50\u7684\u57fa\u672c\u673a\u5236\uff1b\u5f53\u524d\u7684\u7814\u7a76\u5927\u591a\u4e3a\u63a2\u7d22\u6027\uff0c\u96c6\u4e2d\u5728\u4e0d\u540c\u65b9\u9762\uff0c\u5bfc\u81f4\u5b9e\u9a8c\u8bbe\u7f6e\u3001\u7814\u7a76\u65b9\u6cd5\u548c\u8bc4\u4f30\u6307\u6807\u7684\u591a\u6837\u6027\u3002", "conclusion": "\u8bc6\u522b\u51fa\u5173\u952e\u8bbe\u8ba1\u8003\u8651\u56e0\u7d20\u548c\u6311\u6218\uff0c\u4e3a\u672a\u6765\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u673a\u4ea4\u4e92\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u6982\u8ff0\u548c\u6307\u5bfc\u65b9\u9488\u3002"}}
{"id": "2602.15092", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15092", "abs": "https://arxiv.org/abs/2602.15092", "authors": ["Xuanyun Qiu", "Dorian Verdel", "Hector Cervantes-Culebro", "Alexis Devillard", "Etienne Burdet"], "title": "Augmenting Human Balance with Generic Supernumerary Robotic Limbs", "comment": null, "summary": "Supernumerary robotic limbs (SLs) have the potential to transform a wide range of human activities, yet their usability remains limited by key technical challenges, particularly in ensuring safety and achieving versatile control. Here, we address the critical problem of maintaining balance in the human-SLs system, a prerequisite for safe and comfortable augmentation tasks. Unlike previous approaches that developed SLs specifically for stability support, we propose a general framework for preserving balance with SLs designed for generic use. Our hierarchical three-layer architecture consists of: (i) a prediction layer that estimates human trunk and center of mass (CoM) dynamics, (ii) a planning layer that generates optimal CoM trajectories to counteract trunk movements and computes the corresponding SL control inputs, and (iii) a control layer that executes these inputs on the SL hardware. We evaluated the framework with ten participants performing forward and lateral bending tasks. The results show a clear reduction in stance instability, demonstrating the framework's effectiveness in enhancing balance. This work paves the path towards safe and versatile human-SLs interactions. [This paper has been submitted for publication to IEEE.]", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u67b6\u6784\u89e3\u51b3\u8d85\u6570\u673a\u5668\u4eba\u80a2\u4f53\u7684\u5e73\u8861\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u660e\u6548\u679c\u663e\u8457\u3002", "motivation": "\u8d85\u6570\u673a\u5668\u4eba\u80a2\u4f53\u5728\u8f6c\u53d8\u4eba\u7c7b\u6d3b\u52a8\u65b9\u9762\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5f53\u524d\u7684\u6280\u672f\u6311\u6218\u9650\u5236\u4e86\u5176\u53ef\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5c42\u6b21\u5316\u7684\u4e09\u5c42\u67b6\u6784\uff0c\u5305\u542b\u9884\u6d4b\u5c42\u3001\u89c4\u5212\u5c42\u548c\u63a7\u5236\u5c42\uff0c\u89e3\u51b3SLs\u4e0e\u4eba\u7c7b\u7cfb\u7edf\u4e2d\u4fdd\u6301\u5e73\u8861\u7684\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u5bf9\u5341\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u524d\u5411\u548c\u4fa7\u5411\u5f2f\u66f2\u4efb\u52a1\u7684\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u6846\u67b6\u663e\u8457\u964d\u4f4e\u4e86\u59ff\u6001\u4e0d\u7a33\u5b9a\u6027\uff0c\u63d0\u9ad8\u4e86\u5e73\u8861\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u6846\u67b6\u4e3a\u5b89\u5168\u548c\u591a\u529f\u80fd\u7684\u4eba\u7c7b-\u8d85\u6570\u673a\u5668\u4eba\u80a2\u4f53\u4ea4\u4e92\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.15219", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.15219", "abs": "https://arxiv.org/abs/2602.15219", "authors": ["Wooyoung Jung"], "title": "Multi-Agent Home Energy Management Assistant", "comment": "27 pages, 9 figures", "summary": "The growing complexity in home energy management demands advanced systems that guide occupants toward informed energy decisions. Large language model (LLM)-integrated home energy management systems (HEMS) have shown promise, but prior studies relied on prompt engineering or pre-built platforms with limited customization of agent behavior, or assessed performance through single-turn or -task evaluations. This study introduces a multi-agent home energy management assistant (HEMA), built on LangChain and LangGraph, designed to adaptively and intelligently handle real-world use cases of HEMS with full system customization capability. It carefully classifies user queries via a self-consistency classifier, requests three specialized agents (Analysis, Knowledge, and Control) to prepare accurate, adaptive responses using purpose-built analysis and control tools and retrieval augmented generation under the reasoning and acting mechanism. HEMA was rigorously assessed using two different experimental analyses via an LLM-as-user approach: (1) analytical and informative capabilities using combinatorial test cases of various personas and differing scenarios against three alternative system configurations relying on vanilla LLM and (2) control capabilities using various control scenarios. Out of 295 test cases, HEMA acquired a 91.9% goal achievement rate, successfully fulfilling user requests while providing high levels of factual accuracy, action correctness, interaction quality, and system efficiency, especially when compared to alternative system configurations. Collectively, this study contributes to the advancement of the human-centered design of LLM-integrated HEMS by demonstrating the feasibility and value of agentic architectures, and by clarifying the architectural requirements and evaluation criteria necessary to support adaptive, sustained human-artificial intelligence collaboration in HEMS.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u667a\u80fd\u4f53\u5bb6\u5ead\u80fd\u6e90\u7ba1\u7406\u52a9\u624b\uff08HEMA\uff09\uff0c\u53ef\u667a\u80fd\u9002\u5e94\u5b9e\u9645\u7528\u4f8b\uff0c\u901a\u8fc7\u81ea\u6211\u4e00\u81f4\u6027\u5206\u7c7b\u5668\u5206\u7c7b\u7528\u6237\u67e5\u8be2\u5e76\u8bf7\u6c42\u4e13\u95e8\u7684\u4ee3\u7406\u54cd\u5e94\uff0c\u5c55\u73b0\u51fa91.9%\u7684\u76ee\u6807\u8fbe\u6210\u7387\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u5bb6\u5ead\u80fd\u6e90\u7ba1\u7406\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u5bb6\u5c45\u80fd\u6e90\u7ba1\u7406\u7684\u590d\u6742\u6027\u4e0d\u65ad\u589e\u52a0\uff0c\u4e9f\u9700\u5148\u8fdb\u7cfb\u7edf\u5f15\u5bfc\u7528\u6237\u505a\u51fa\u660e\u667a\u7684\u80fd\u6e90\u51b3\u7b56\uff0c\u800c\u73b0\u6709\u6a21\u578b\u5728\u5b9a\u5236\u5316\u548c\u6027\u80fd\u8bc4\u4f30\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u672c\u7814\u7a76\u57fa\u4e8eLangChain\u548cLangGraph\u6784\u5efaHEMA\uff0c\u91c7\u7528\u81ea\u6211\u4e00\u81f4\u6027\u5206\u7c7b\u5668\u8bc6\u522b\u7528\u6237\u67e5\u8be2\uff0c\u5229\u7528\u4e09\u4e2a\u4e13\u95e8\u7684\u4ee3\u7406\uff08\u5206\u6790\u3001\u77e5\u8bc6\u3001\u63a7\u5236\uff09\u751f\u6210\u7cbe\u51c6\u7684\u54cd\u5e94\u3002", "result": "\u7ecf\u8fc7295\u4e2a\u6d4b\u8bd5\u6848\u4f8b\u7684\u4e25\u683c\u8bc4\u4f30\uff0cHEMA\u5728\u7528\u6237\u8bf7\u6c42\u7684\u6ee1\u8db3\u7387\u4e3a91.9%\uff0c\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u884c\u52a8\u6b63\u786e\u6027\u3001\u4ea4\u4e92\u8d28\u91cf\u548c\u7cfb\u7edf\u6548\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u662f\u76f8\u8f83\u4e8e\u5176\u4ed6\u7cfb\u7edf\u914d\u7f6e\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u67b6\u6784\u7684\u5bb6\u5ead\u80fd\u6e90\u7ba1\u7406\u52a9\u624b\uff08HEMA\uff09\u7684\u53ef\u884c\u6027\u4e0e\u4ef7\u503c\uff0c\u5bf9\u4eba\u672c\u8bbe\u8ba1\u7684LLM\u96c6\u6210\u5bb6\u5ead\u80fd\u6e90\u7ba1\u7406\u7cfb\u7edf\uff08HEMS\uff09\u8fdb\u884c\u4e86\u603b\u7ed3\uff0c\u660e\u786e\u4e86\u652f\u6301\u4eba\u673a\u534f\u4f5c\u7684\u67b6\u6784\u9700\u6c42\u548c\u8bc4\u4f30\u6807\u51c6\u3002"}}
{"id": "2602.15162", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15162", "abs": "https://arxiv.org/abs/2602.15162", "authors": ["Fernando Ca\u00f1adas-Ar\u00e1nega", "Francisco J. Ma\u00f1as-\u00c1lvarez", "Jos\u00e9 L- Guzm\u00e1n", "Jos\u00e9 C. Moreno", "Jos\u00e9 L. Blanco-Claraco"], "title": "A ROS2 Benchmarking Framework for Hierarchical Control Strategies in Mobile Robots for Mediterranean Greenhouses", "comment": "53 pages", "summary": "Mobile robots operating in agroindustrial environments, such as Mediterranean greenhouses, are subject to challenging conditions, including uneven terrain, variable friction, payload changes, and terrain slopes, all of which significantly affect control performance and stability. Despite the increasing adoption of robotic platforms in agriculture, the lack of standardized, reproducible benchmarks impedes fair comparisons and systematic evaluations of control strategies under realistic operating conditions. This paper presents a comprehensive benchmarking framework for evaluating mobile robot controllers in greenhouse environments. The proposed framework integrates an accurate three dimensional model of the environment, a physics based simulator, and a hierarchical control architecture comprising low, mid, and high level control layers. Three benchmark categories are defined to enable modular assessment, ranging from actuator level control to full autonomous navigation. Additionally, three disturbance scenarios payload variation, terrain type, and slope are explicitly modeled to replicate real world agricultural conditions. To ensure objective and reproducible evaluation, standardized performance metrics are introduced, including the Squared Absolute Error (SAE), the Squared Control Input (SCI), and composite performance indices. Statistical analysis based on repeated trials is employed to mitigate the influence of sensor noise and environmental variability. The framework is further enhanced by a plugin based architecture that facilitates seamless integration of user defined controllers and planners. The proposed benchmark provides a robust and extensible tool for the quantitative comparison of classical, predictive, and planning based control strategies in realistic conditions, bridging the gap between simulation based analysis and real world agroindustrial applications.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u519c\u4e1a\u5de5\u4e1a\u73af\u5883\u4e2d\u79fb\u52a8\u673a\u5668\u4eba\u63a7\u5236\u7b56\u7565\u8bc4\u4f30\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u6307\u6807\u4e0e\u6270\u52a8\u573a\u666f\uff0c\u4fbf\u4e8e\u516c\u5e73\u6bd4\u8f83\u4e0e\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "motivation": "\u63d0\u9ad8\u79fb\u52a8\u673a\u5668\u4eba\u5728\u519c\u4e1a\u5de5\u4e1a\u73af\u5883\u4e2d\uff08\u5982\u5730\u4e2d\u6d77\u6e29\u5ba4\uff09\u7684\u63a7\u5236\u6027\u80fd\u548c\u7a33\u5b9a\u6027\uff0c\u901a\u8fc7\u5236\u5b9a\u6807\u51c6\u5316\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u4fbf\u4e8e\u63a7\u5236\u7b56\u7565\u7684\u6bd4\u8f83\u4e0e\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7efc\u5408\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u7ed3\u5408\u4e09\u7ef4\u73af\u5883\u6a21\u578b\u3001\u7269\u7406\u4eff\u771f\u5668\u548c\u5c42\u6b21\u63a7\u5236\u67b6\u6784\uff0c\u6db5\u76d6\u4f4e\u3001\u4e2d\u3001\u9ad8\u7ea7\u63a7\u5236\u5c42\uff0c\u5e76\u5b9a\u4e49\u4e09\u7c7b\u57fa\u51c6\uff0c\u6a21\u62df\u6270\u52a8\u573a\u666f\u4ee5\u590d\u5236\u771f\u5b9e\u519c\u4e1a\u6761\u4ef6\u3002", "result": "\u5efa\u7acb\u4e86\u7edf\u4e00\u7684\u6027\u80fd\u8bc4\u4f30\u6307\u6807\u5982\u79f0\u65b9\u5dee\u7edd\u5bf9\u8bef\u5dee(SAE)\u3001\u65b9\u5dee\u63a7\u5236\u8f93\u5165(SCI)\u548c\u7efc\u5408\u6027\u80fd\u6307\u6570\uff0c\u589e\u5f3a\u4e86\u901a\u8fc7\u63d2\u4ef6\u67b6\u6784\u6574\u5408\u7528\u6237\u5b9a\u4e49\u7684\u63a7\u5236\u5668\u548c\u89c4\u5212\u8005\u7684\u80fd\u529b\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u4e14\u53ef\u6269\u5c55\u7684\u5de5\u5177\uff0c\u4ee5\u4fbf\u5728\u771f\u5b9e\u6761\u4ef6\u4e0b\u5b9a\u91cf\u6bd4\u8f83\u7ecf\u5178\u3001\u9884\u6d4b\u548c\u57fa\u4e8e\u89c4\u5212\u7684\u63a7\u5236\u7b56\u7565\uff0c\u4e3a\u4eff\u771f\u5206\u6790\u4e0e\u73b0\u5b9e\u519c\u4e1a\u5e94\u7528\u4e4b\u95f4\u67b6\u8d77\u6865\u6881\u3002"}}
{"id": "2602.15237", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.15237", "abs": "https://arxiv.org/abs/2602.15237", "authors": ["Rodrigo Gutierrez Maquilon", "Marita Hueber", "Georg Regal", "Manfred Tscheligi"], "title": "Ground-Truth Depth in Vision Language Models: Spatial Context Understanding in Conversational AI for XR-Robotic Support in Emergency First Response", "comment": null, "summary": "Large language models (LLMs) are increasingly used in emergency first response (EFR) applications to support situational awareness (SA) and decision-making, yet most operate on text or 2D imagery and offer little support for core EFR SA competencies like spatial reasoning. We address this gap by evaluating a prototype that fuses robot-mounted depth sensing and YOLO detection with a vision language model (VLM) capable of verbalizing metrically-grounded distances of detected objects (e.g., the chair is 3.02 meters away). In a mixed-reality toxic-smoke scenario, participants estimated distances to a victim and an exit window under three conditions: video-only, depth-agnostic VLM, and depth-augmented VLM. Depth-augmentation improved objective accuracy and stability, e.g., the victim and window distance estimation error dropped, while raising situational awareness without increasing workload. Conversely, depth- agnostic assistance increased workload and slightly worsened accuracy. We contribute to human SA augmentation by demonstrating that metrically grounded, object-centric verbal information supports spatial reasoning in EFR and improves decision-relevant judgments under time pressure.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u901a\u8fc7\u6df1\u5ea6\u589e\u5f3a\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6539\u5584\u5e94\u6025\u54cd\u5e94\u7684\u60c5\u5883\u610f\u8bc6\u548c\u51b3\u7b56\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u6df1\u5ea6\u611f\u77e5\u548c\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u5408\u6765\u63d0\u9ad8\u7d27\u6025\u60c5\u51b5\u4e2d\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "method": "\u8bc4\u4f30\u4e00\u4e2a\u878d\u5408\u4e86\u673a\u5668\u4eba\u6df1\u5ea6\u4f20\u611f\u5668\u548cYOLO\u68c0\u6d4b\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b( VLM)\u7684\u539f\u578b\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u8868\u8ff0\u88ab\u68c0\u6d4b\u7269\u4f53\u7684\u5ea6\u91cf\u8ddd\u79bb\u3002", "result": "\u5728\u6df7\u5408\u73b0\u5b9e\u7684\u6709\u6bd2\u70df\u96fe\u573a\u666f\u4e2d\uff0c\u53c2\u4e0e\u8005\u5728\u4e09\u79cd\u6761\u4ef6\u4e0b\uff08\u89c6\u9891\u4ec5\uff0c\u6df1\u5ea6\u65e0\u5173 VLM\uff0c\u548c\u6df1\u5ea6\u589e\u5f3a VLM\uff09\u8bc4\u4f30\u53d7\u5bb3\u8005\u548c\u51fa\u53e3\u7a97\u6237\u7684\u8ddd\u79bb\u3002", "conclusion": "\u6df1\u5ea6\u589e\u5f3a\u63d0\u5347\u4e86\u76ee\u6807\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u63d0\u9ad8\u4e86\u60c5\u5883\u610f\u8bc6\u800c\u672a\u589e\u52a0\u5de5\u4f5c\u8d1f\u62c5\uff0c\u53cd\u4e4b\uff0c\u6df1\u5ea6\u65e0\u5173\u7684\u8f85\u52a9\u5219\u589e\u52a0\u4e86\u5de5\u4f5c\u8d1f\u62c5\u5e76\u7565\u5fae\u964d\u4f4e\u4e86\u51c6\u786e\u6027\u3002"}}
{"id": "2602.15201", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15201", "abs": "https://arxiv.org/abs/2602.15201", "authors": ["Ren\u00e9 Zurbr\u00fcgg", "Andrei Cramariuc", "Marco Hutter"], "title": "DexEvolve: Evolutionary Optimization for Robust and Diverse Dexterous Grasp Synthesis", "comment": null, "summary": "Dexterous grasping is fundamental to robotics, yet data-driven grasp prediction heavily relies on large, diverse datasets that are costly to generate and typically limited to a narrow set of gripper morphologies. Analytical grasp synthesis can be used to scale data collection, but necessary simplifying assumptions often yield physically infeasible grasps that need to be filtered in high-fidelity simulators, significantly reducing the total number of grasps and their diversity.\n  We propose a scalable generate-and-refine pipeline for synthesizing large-scale, diverse, and physically feasible grasps. Instead of using high-fidelity simulators solely for verification and filtering, we leverage them as an optimization stage that continuously improves grasp quality without discarding precomputed candidates. More specifically, we initialize an evolutionary search with a seed set of analytically generated, potentially suboptimal grasps. We then refine these proposals directly in a high-fidelity simulator (Isaac Sim) using an asynchronous, gradient-free evolutionary algorithm, improving stability while maintaining diversity. In addition, this refinement stage can be guided toward human preferences and/or domain-specific quality metrics without requiring a differentiable objective. We further distill the refined grasp distribution into a diffusion model for robust real-world deployment, and highlight the role of diversity for both effective training and during deployment. Experiments on a newly introduced Handles dataset and a DexGraspNet subset demonstrate that our approach achieves over 120 distinct stable grasps per object (a 1.7-6x improvement over unrefined analytical methods) while outperforming diffusion-based alternatives by 46-60\\% in unique grasp coverage.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u6293\u53d6\u751f\u6210\u548c\u4f18\u5316\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u591a\u6837\u5316\u548c\u7269\u7406\u53ef\u884c\u7684\u6293\u53d6\u65b9\u6848\uff0c\u5927\u5e45\u63d0\u9ad8\u4e86\u6293\u53d6\u7a33\u5b9a\u6027\u548c\u8986\u76d6\u5ea6\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u6570\u636e\u9a71\u52a8\u7684\u6293\u53d6\u9884\u6d4b\u5728\u751f\u6210\u548c\u4f7f\u7528\u591a\u6837\u5316\u6293\u53d6\u6570\u636e\u96c6\u65b9\u9762\u6240\u9762\u4e34\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u9650\u5236\u4e8e\u72ed\u7a84\u7684\u6293\u53d6\u5668\u5f62\u6001\u548c\u751f\u6210\u6602\u8d35\u7684\u6570\u636e\u96c6\u65f6\u7684\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u751f\u6210-\u7cbe\u70bc\u7ba1\u9053\uff0c\u901a\u8fc7\u5728\u9ad8\u4fdd\u771f\u6a21\u62df\u5668\u4e2d\u8fdb\u884c\u8fdb\u5316\u641c\u7d22\u548c\u4f18\u5316\uff0c\u6301\u7eed\u63d0\u9ad8\u6293\u53d6\u8d28\u91cf\u800c\u4e0d\u4e22\u5f03\u4e4b\u524d\u751f\u6210\u7684\u5019\u9009\u6293\u53d6\u3002", "result": "\u5728Handles\u6570\u636e\u96c6\u548cDexGraspNet\u5b50\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u6bcf\u4e2a\u7269\u4f53\u80fd\u591f\u5b9e\u73b0\u8d85\u8fc7120\u79cd\u4e0d\u540c\u7684\u7a33\u5b9a\u6293\u53d6\uff0c\u6bd4\u672a\u4f18\u5316\u7684\u5206\u6790\u65b9\u6cd5\u63d0\u9ad8\u4e861.7-6\u500d\uff0c\u5e76\u5728\u72ec\u7279\u6293\u53d6\u8986\u76d6\u7387\u4e0a\u8d85\u8fc7\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd546-60%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4fc3\u8fdb\u4e86\u9ad8\u6548\u6293\u53d6\u751f\u6210\uff0c\u5f3a\u8c03\u4e86\u591a\u6837\u6027\u5728\u8bad\u7ec3\u548c\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u9ad8\u4fdd\u771f\u6a21\u62df\u5668\u5728\u4f18\u5316\u6293\u53d6\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.15245", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15245", "abs": "https://arxiv.org/abs/2602.15245", "authors": ["Ankit Bhattarai", "Hannah Selder", "Florian Fischer", "Arthur Fleig", "Per Ola Kristensson"], "title": "MyoInteract: A Framework for Fast Prototyping of Biomechanical HCI Tasks using Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL)-based biomechanical simulations have the potential to revolutionise HCI research and interaction design, but currently lack usability and interpretability. Using the Human Action Cycle as a design lens, we identify key limitations of biomechanical RL frameworks and develop MyoInteract, a novel framework for fast prototyping of biomechanical HCI tasks. MyoInteract allows designers to setup tasks, user models, and training parameters from an easy-to-use GUI within minutes. It trains and evaluates muscle-actuated simulated users within minutes, reducing training times by up to 98%. A workshop study with 12 interaction designers revealed that MyoInteract allowed novices in biomechanical RL to successfully setup, train, and assess goal-directed user movements within a single session. By transforming biomechanical RL from a days-long expert task into an accessible hour-long workflow, this work significantly lowers barriers to entry and accelerates iteration cycles in HCI biomechanics research.", "AI": {"tldr": "MyoInteract\u662f\u4e00\u4e2a\u65b0\u7684\u751f\u7269\u673a\u68b0HCI\u4efb\u52a1\u5feb\u901f\u539f\u578b\u6846\u67b6\uff0c\u80fd\u5728\u51e0\u5206\u949f\u5185\u8bbe\u5b9a\u4efb\u52a1\u548c\u7528\u6237\u6a21\u578b\uff0c\u5e76\u5c06\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1198%\u3002", "motivation": "\u751f\u7269\u673a\u68b0RL\u6a21\u62df\u5728HCI\u7814\u7a76\u548c\u4ea4\u4e92\u8bbe\u8ba1\u4e2d\u5177\u6709\u9769\u547d\u6027\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u53ef\u7528\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u4eba\u7c7b\u884c\u52a8\u5468\u671f\u4f5c\u4e3a\u8bbe\u8ba1\u89c6\u89d2\uff0c\u5f00\u53d1\u4e86MyoInteract\u6846\u67b6\uff0c\u63d0\u4f9b\u6613\u7528\u7684\u56fe\u5f62\u7528\u6237\u754c\u9762\uff0c\u5feb\u901f\u8bbe\u5b9a\u4efb\u52a1\u548c\u8bad\u7ec3\u53c2\u6570\u3002", "result": "\u7ecf\u8fc7\u4e0e12\u540d\u4ea4\u4e92\u8bbe\u8ba1\u5e08\u7684\u5de5\u4f5c\u574a\u7814\u7a76\uff0cMyoInteract\u4f7f\u751f\u7269\u673a\u68b0RL\u7684\u521d\u5b66\u8005\u80fd\u591f\u5728\u4e00\u6b21\u4f1a\u8bae\u4e2d\u6210\u529f\u8bbe\u5b9a\u3001\u8bad\u7ec3\u548c\u8bc4\u4f30\u7528\u6237\u7684\u76ee\u6807\u5bfc\u5411\u8fd0\u52a8\u3002", "conclusion": "MyoInteract\u6846\u67b6\u663e\u8457\u964d\u4f4e\u4e86\u751f\u7269\u673a\u68b0RL\u7684\u4f7f\u7528\u95e8\u69db\uff0c\u52a0\u5feb\u4e86HCI\u751f\u7269\u529b\u5b66\u7814\u7a76\u4e2d\u7684\u8fed\u4ee3\u5468\u671f\u3002"}}
{"id": "2602.15258", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15258", "abs": "https://arxiv.org/abs/2602.15258", "authors": ["Sebastian Donnelly", "Ruth Anderson", "George Economides", "James Broughton", "Peter Ball", "Alexander Rast", "Andrew Bradley"], "title": "SEG-JPEG: Simple Visual Semantic Communications for Remote Operation of Automated Vehicles over Unreliable Wireless Networks", "comment": null, "summary": "Remote Operation is touted as being key to the rapid deployment of automated vehicles. Streaming imagery to control connected vehicles remotely currently requires a reliable, high throughput network connection, which can be limited in real-world remote operation deployments relying on public network infrastructure. This paper investigates how the application of computer vision assisted semantic communication can be used to circumvent data loss and corruption associated with traditional image compression techniques. By encoding the segmentations of detected road users into colour coded highlights within low resolution greyscale imagery, the required data rate can be reduced by 50 \\% compared with conventional techniques, while maintaining visual clarity. This enables a median glass-to-glass latency of below 200ms even when the network data rate is below 500kbit/s, while clearly outlining salient road users to enhance situational awareness of the remote operator. The approach is demonstrated in an area of variable 4G mobile connectivity using an automated last-mile delivery vehicle. With this technique, the results indicate that large-scale deployment of remotely operated automated vehicles could be possible even on the often constrained public 4G/5G mobile network, providing the potential to expedite the nationwide roll-out of automated vehicles.", "AI": {"tldr": "\u672c\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u8ba1\u7b97\u673a\u89c6\u89c9\u8f85\u52a9\u7684\u8bed\u4e49\u901a\u4fe1\u6765\u51cf\u5c11\u9065\u63a7\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u56fe\u50cf\u4f20\u8f93\u7684\u6570\u636e\u9700\u6c42\uff0c\u6210\u529f\u5728\u4f4e\u6570\u636e\u901f\u7387\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7684\u9065\u63a7\u64cd\u4f5c\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u786e\u4fdd\u53ef\u9760\u7684\u8fdc\u7a0b\u64cd\u4f5c\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u5c24\u4e3a\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u516c\u5171\u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u901a\u8fc7\u5c06\u68c0\u6d4b\u5230\u7684\u9053\u8def\u7528\u6237\u7684\u5206\u5272\u7f16\u7801\u6210\u4f4e\u5206\u8fa8\u7387\u7070\u5ea6\u56fe\u50cf\u4e2d\u7684\u5f69\u8272\u9ad8\u4eae\uff0c\u964d\u4f4e\u4e86\u6570\u636e\u4f20\u8f93\u7387\uff0c\u4f7f\u56fe\u50cf\u4f20\u8f93\u7684\u5ef6\u8fdf\u5927\u5e45\u964d\u4f4e\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u89c6\u89c9\u6e05\u6670\u5ea6\u3002", "result": "\u901a\u8fc7\u5e94\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u8f85\u52a9\u7684\u8bed\u4e49\u901a\u4fe1\u6280\u672f\uff0c\u80fd\u591f\u6709\u6548\u51cf\u5c11\u4f20\u7edf\u56fe\u50cf\u538b\u7f29\u6280\u672f\u6240\u5bfc\u81f4\u7684\u6570\u636e\u635f\u5931\u548c\u6570\u636e\u635f\u574f\u95ee\u9898\uff0c\u4ece\u800c\u4f7f\u9065\u63a7\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u8fdc\u7a0b\u64cd\u4f5c\u6210\u4e3a\u53ef\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u91c7\u7528\u8be5\u6280\u672f\u7684\u8fdc\u7a0b\u64cd\u4f5c\u81ea\u52a8\u8f66\u8f86\u7684\u5e7f\u6cdb\u90e8\u7f72\u5728\u5f53\u524d\u516c\u51714G/5G\u79fb\u52a8\u7f51\u7edc\u4e0a\u662f\u53ef\u884c\u7684\uff0c\u8fd9\u53ef\u80fd\u52a0\u901f\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u5168\u56fd\u8303\u56f4\u5185\u7684\u63a8\u5e7f\u3002"}}
{"id": "2602.15265", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.15265", "abs": "https://arxiv.org/abs/2602.15265", "authors": ["Aleksey Komissarov"], "title": "From Diagnosis to Inoculation: Building Cognitive Resistance to AI Disempowerment", "comment": "11 pages, 1 table. Perspective / Position Paper", "summary": "Recent empirical research by Sharma et al. (2026) demonstrated that AI assistant interactions carry meaningful potential for situational human disempowerment, including reality distortion, value judgment distortion, and action distortion. While this work provides a critical diagnosis of the problem, concrete pedagogical interventions remain underexplored. I present an AI literacy framework built around eight cross-cutting Learning Outcomes (LOs), developed independently through teaching practice and subsequently found to align with Sharma et al.'s disempowerment taxonomy. I report a case study from a publicly available online course, where a co-teaching methodology--with AI serving as an active voice co-instructor--was used to deliver this framework. Drawing on inoculation theory (McGuire, 1961)--a well-established persuasion research framework recently applied to misinformation prebunking by the Cambridge school (van der Linden, 2022; Roozenbeek & van der Linden, 2019)--I argue that AI literacy cannot be acquired through declarative knowledge alone, but requires guided exposure to AI failure modes, including the sycophantic validation and authority projection patterns identified by Sharma et al. This application of inoculation theory to AI-specific distortion is, to my knowledge, novel. I discuss the convergence between the pedagogically-derived framework and Sharma et al.'s empirically-derived taxonomy, and argue that this convergence--two independent approaches arriving at similar problem descriptions--strengthens the case for both the diagnosis and the proposed educational response.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cdAI\u7d20\u517b\u6846\u67b6\uff0c\u5f3a\u8c03\u901a\u8fc7\u5f15\u5bfc\u5b66\u4e60\u8005\u63a5\u89e6AI\u5931\u6548\u6a21\u5f0f\u6765\u5e94\u5bf9AI\u4ea4\u4e92\u5e26\u6765\u7684\u6f5c\u5728\u5931\u80fd\u95ee\u9898\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u673a\u662f\u9488\u5bf9Sharma\u7b49\u4eba\u63d0\u51fa\u7684AI\u52a9\u624b\u4e92\u52a8\u53ef\u80fd\u5bfc\u81f4\u7684\u60c5\u5883\u6027\u4eba\u7c7b\u5931\u80fd\u95ee\u9898\uff0c\u5bfb\u6c42\u6709\u6548\u7684\u6559\u80b2\u5e72\u9884\u63aa\u65bd\u4ee5\u63d0\u5347\u4eba\u4eec\u7684AI\u7d20\u517b\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u4e0eAI\u5171\u540c\u6388\u8bfe\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86AI\u7d20\u517b\u6846\u67b6\u7684\u5b9e\u65bd\uff0c\u7ed3\u5408\u514d\u75ab\u7406\u8bba\u7684\u5e94\u7528\uff0c\u4fc3\u8fdb\u5b66\u4e60\u8005\u5bf9AI\u5931\u6548\u6a21\u5f0f\u7684\u7406\u89e3\u3002", "result": "\u8be5\u6846\u67b6\u4e0eSharma\u7b49\u4eba\u7684\u5931\u80fd\u5206\u7c7b\u4f53\u7cfb\u5b58\u5728\u5171\u9e23\uff0c\u8868\u660e\u4e24\u8005\u5728\u95ee\u9898\u63cf\u8ff0\u4e0a\u7684\u4e00\u81f4\u6027\uff0c\u589e\u5f3a\u4e86\u5bf9\u95ee\u9898\u8bca\u65ad\u548c\u6559\u80b2\u54cd\u5e94\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5e94\u7528\u514d\u75ab\u7406\u8bba\uff0c\u63d0\u51fa\u4e86\u4e00\u79cdAI\u7d20\u517b\u6846\u67b6\uff0c\u5f3a\u8c03\u4e86\u5728\u6559\u5b66\u8fc7\u7a0b\u4e2d\u5bf9AI\u5931\u6548\u6a21\u5f0f\u7684\u5f15\u5bfc\u6027\u66dd\u5149\u662f\u5fc5\u8981\u7684\uff0c\u4ee5\u6709\u6548\u5e94\u5bf9\u4eba\u7c7b\u5728\u4e0eAI\u4e92\u52a8\u4e2d\u53ef\u80fd\u906d\u9047\u7684\u5931\u80fd\u73b0\u8c61\u3002\u6b64\u5916\uff0c\u6846\u67b6\u4e0eSharma\u7b49\u4eba\u7684\u5206\u7c7b\u4f53\u7cfb\u4e4b\u95f4\u7684\u5171\u9e23\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u5bf9\u95ee\u9898\u8bca\u65ad\u548c\u6559\u80b2\u56de\u5e94\u7684\u652f\u6301\u3002"}}
{"id": "2602.15309", "categories": ["cs.RO", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2602.15309", "abs": "https://arxiv.org/abs/2602.15309", "authors": ["Mostafa A. Atalla", "Anand S. Sekar", "Remi van Starkenburg", "David J. Jager", "Aim\u00e9e Sakes", "Micha\u00ebl Wiertlewski", "Paul Breedveld"], "title": "OSCAR: An Ovipositor-Inspired Self-Propelling Capsule Robot for Colonoscopy", "comment": null, "summary": "Self-propelling robotic capsules eliminate shaft looping of conventional colonoscopy, reducing patient discomfort. However, reliably moving within the slippery, viscoelastic environment of the colon remains a significant challenge. We present OSCAR, an ovipositor-inspired self-propelling capsule robot that translates the transport strategy of parasitic wasps into a propulsion mechanism for colonoscopy. OSCAR mechanically encodes the ovipositor-inspired motion pattern through a spring-loaded cam system that drives twelve circumferential sliders in a coordinated, phase-shifted sequence. By tuning the motion profile to maximize the retract phase relative to the advance phase, the capsule creates a controlled friction anisotropy at the interface that generates net forward thrust. We developed an analytical model incorporating a Kelvin-Voigt formulation to capture the viscoelastic stick--slip interactions between the sliders and the tissue, linking the asymmetry between advance and retract phase durations to mean thrust, and slider-reversal synchronization to thrust stability. Comprehensive force characterization experiments in ex-vivo porcine colon revealed a mean steady-state traction force of 0.85 N, closely matching the model. Furthermore, experiments confirmed that thrust generation is speed-independent and scales linearly with the phase asymmetry, in agreement with theoretical predictions, underscoring the capsule's predictable performance and scalability. In locomotion validation experiments, OSCAR demonstrated robust performance, achieving an average speed of 3.08 mm/s, a velocity sufficient to match the cecal intubation times of conventional colonoscopy. By coupling phase-encoded friction anisotropy with a predictive model, OSCAR delivers controllable thrust generation at low normal loads, enabling safer and more robust self-propelling locomotion for robotic capsule colonoscopy.", "AI": {"tldr": "OSCAR\u662f\u4e00\u79cd\u6a21\u4eff\u5bc4\u751f\u9ec4\u8702\u4ea7\u5375\u5668\u7684\u81ea\u63a8\u8fdb\u80f6\u56ca\u673a\u5668\u4eba\uff0c\u80fd\u591f\u5728\u7ed3\u80a0\u4e2d\u7a33\u5b9a\u8fd0\u52a8\uff0c\u751f\u6210\u53ef\u63a7\u63a8\u529b\uff0c\u4ece\u800c\u63d0\u9ad8\u7ed3\u80a0\u955c\u68c0\u67e5\u7684\u5b89\u5168\u6027\u548c\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u7ed3\u80a0\u955c\u68c0\u67e5\u4e2d\u7684\u8f74\u5411\u73af\u8def\u4f1a\u589e\u52a0\u60a3\u8005\u7684\u4e0d\u9002\u611f\uff0c\u56e0\u6b64\u9700\u8981\u907f\u514d\u8fd9\u79cd\u60c5\u51b5\u3002", "method": "OSCAR\u901a\u8fc7\u5f39\u7c27\u52a0\u8f7d\u51f8\u8f6e\u7cfb\u7edf\u5b9e\u73b0\u4e86\u6a21\u4eff\u5bc4\u751f\u9ec4\u8702\u4ea7\u5375\u5668\u7684\u8fd0\u52a8\u6a21\u5f0f\uff0c\u8be5\u7cfb\u7edf\u9a71\u52a8\u5341\u4e8c\u4e2a\u5706\u5468\u6ed1\u5757\u4ee5\u534f\u8c03\u7684\u3001\u76f8\u4f4d\u504f\u79fb\u7684\u987a\u5e8f\u8fd0\u52a8\u3002", "result": "\u5728\u4f53\u5916\u732a\u7ed3\u80a0\u7684\u7efc\u5408\u529b\u7279\u6027\u5b9e\u9a8c\u4e2d\uff0c\u5e73\u5747\u7a33\u6001\u7275\u5f15\u529b\u4e3a0.85 N\uff0c\u7b26\u5408\u6a21\u578b\u9884\u6d4b\uff0c\u540c\u65f6\u9a8c\u8bc1\u4e86\u63a8\u529b\u751f\u6210\u7684\u901f\u5ea6\u65e0\u5173\u6027\uff0c\u5e76\u4e0e\u76f8\u4f4d\u4e0d\u5bf9\u79f0\u6027\u7ebf\u6027\u76f8\u5173\uff0c\u5f3a\u8c03\u4e86\u80f6\u56ca\u7684\u53ef\u9884\u6d4b\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u901a\u8fc7\u5c06\u76f8\u4f4d\u7f16\u7801\u7684\u6469\u64e6\u5404\u5411\u5f02\u6027\u4e0e\u9884\u6d4b\u6a21\u578b\u76f8\u7ed3\u5408\uff0cOSCAR\u5b9e\u73b0\u4e86\u5728\u4f4e\u6cd5\u5411\u8f7d\u8377\u4e0b\u53ef\u63a7\u7684\u63a8\u529b\u751f\u6210\uff0c\u4ece\u800c\u4e3a\u673a\u5668\u4eba\u80f6\u56ca\u7ed3\u80a0\u955c\u68c0\u67e5\u63d0\u4f9b\u4e86\u66f4\u5b89\u5168\u548c\u66f4\u7a33\u5065\u7684\u81ea\u63a8\u8fdb\u8fd0\u52a8\u3002"}}
{"id": "2602.15280", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.15280", "abs": "https://arxiv.org/abs/2602.15280", "authors": ["Samuel Reinders", "Munazza Zaib", "Matthew Butler", "Bongshin Lee", "Ingrid Zukerman", "Lizhen Qu", "Kim Marriott"], "title": "Supporting Multimodal Data Interaction on Refreshable Tactile Displays: An Architecture to Combine Touch and Conversational AI", "comment": "Paper to be presented at IEEE PacificVis 2026 (VisNotes)", "summary": "Combining conversational AI with refreshable tactile displays (RTDs) offers significant potential for creating accessible data visualization for people who are blind or have low vision (BLV). To support researchers and developers building accessible data visualizations with RTDs, we present a multimodal data interaction architecture along with an open-source reference implementation. Our system is the first to combine touch input with a conversational agent on an RTD, enabling deictic queries that fuse touch context with spoken language, such as \"what is the trend between these points?\" The architecture addresses key technical challenges, including touch sensing on RTDs, visual-to-tactile encoding, integrating touch context with conversational AI, and synchronizing multimodal output. Our contributions are twofold: (1) a technical architecture integrating RTD hardware, external touch sensing, and conversational AI to enable multimodal data interaction; and (2) an open-source reference implementation demonstrating its feasibility. This work provides a technical foundation to support future research in multimodal accessible data visualization.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u591a\u6a21\u6001\u6570\u636e\u4ea4\u4e92\u67b6\u6784\uff0c\u7ed3\u5408\u5bf9\u8bdd\u5f0f\u4eba\u5de5\u667a\u80fd\u4e0e\u53ef\u5237\u65b0\u89e6\u89c9\u663e\u793a\u5668\uff0c\u4ee5\u63d0\u5347\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u4eba\u7fa4\u7684\u6570\u636e\u53ef\u89c6\u5316\u4f53\u9a8c\uff0c\u5e76\u7ed9\u51fa\u4e86\u4e00\u79cd\u5f00\u6e90\u5b9e\u73b0\u3002", "motivation": "\u7ed3\u5408\u5bf9\u8bdd\u5f0f\u4eba\u5de5\u667a\u80fd\u4e0e\u53ef\u5237\u65b0\u89e6\u89c9\u663e\u793a\u5668\uff08RTDs\uff09\uff0c\u4e3a\u76f2\u4eba\u6216\u4f4e\u89c6\u529b\u4eba\u7fa4\u521b\u9020\u53ef\u8bbf\u95ee\u7684\u6570\u636e\u53ef\u89c6\u5316\u63d0\u4f9b\u4e86\u91cd\u8981\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6570\u636e\u4ea4\u4e92\u67b6\u6784\uff0c\u4ee5\u53ca\u4e00\u4e2a\u5f00\u6e90\u7684\u53c2\u8003\u5b9e\u73b0\u3002", "result": "\u6211\u4eec\u7684\u7cfb\u7edf\u662f\u9996\u4e2a\u5c06\u89e6\u6478\u8f93\u5165\u4e0e\u5bf9\u8bdd\u4ee3\u7406\u7ed3\u5408\u5728RTD\u4e0a\u7684\u5b9e\u73b0\uff0c\u80fd\u591f\u652f\u6301\u4f9d\u636e\u89e6\u6478\u4e0a\u4e0b\u6587\u4e0e\u53e3\u8bed\u8fdb\u884c\u7684\u6307\u793a\u67e5\u8be2\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u672a\u6765\u5728\u591a\u6a21\u6001\u7684\u53ef\u8bbf\u95ee\u6570\u636e\u53ef\u89c6\u5316\u9886\u57df\u63d0\u4f9b\u4e86\u6280\u672f\u57fa\u7840\u3002"}}
{"id": "2602.15351", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15351", "abs": "https://arxiv.org/abs/2602.15351", "authors": ["Kei Takahashi", "Hikaru Sasaki", "Takamitsu Matsubara"], "title": "Feasibility-aware Imitation Learning from Observation with Multimodal Feedback", "comment": null, "summary": "Imitation learning frameworks that learn robot control policies from demonstrators' motions via hand-mounted demonstration interfaces have attracted increasing attention. However, due to differences in physical characteristics between demonstrators and robots, this approach faces two limitations: i) the demonstration data do not include robot actions, and ii) the demonstrated motions may be infeasible for robots. These limitations make policy learning difficult. To address them, we propose Feasibility-Aware Behavior Cloning from Observation (FABCO). FABCO integrates behavior cloning from observation, which complements robot actions using robot dynamics models, with feasibility estimation. In feasibility estimation, the demonstrated motions are evaluated using a robot-dynamics model, learned from the robot's execution data, to assess reproducibility under the robot's dynamics. The estimated feasibility is used for multimodal feedback and feasibility-aware policy learning to improve the demonstrator's motions and learn robust policies. Multimodal feedback provides feasibility through the demonstrator's visual and haptic senses to promote feasible demonstrated motions. Feasibility-aware policy learning reduces the influence of demonstrated motions that are infeasible for robots, enabling the learning of policies that robots can execute stably. We conducted experiments with 15 participants on two tasks and confirmed that FABCO improves imitation learning performance by more than 3.2 times compared to the case without feasibility feedback.", "AI": {"tldr": "FABCO\u662f\u4e00\u79cd\u8003\u8651\u53ef\u884c\u6027\u7684\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u673a\u5668\u4eba\u52a8\u529b\u5b66\u6a21\u578b\u548c\u53ef\u884c\u6027\u4f30\u8ba1\uff0c\u6539\u5584\u4e86\u673a\u5668\u4eba\u63a7\u5236\u7b56\u7565\u7684\u5b66\u4e60\u3002", "motivation": "\u4f20\u7edf\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u56e0\u6f14\u793a\u8005\u548c\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u7269\u7406\u5dee\u5f02\u800c\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u65b9\u9762\u3002", "method": "FABCO\u901a\u8fc7\u7ed3\u5408\u89c2\u5bdf\u7684\u884c\u4e3a\u514b\u9686\u4e0e\u53ef\u884c\u6027\u4f30\u8ba1\uff0c\u5229\u7528\u673a\u5668\u4eba\u52a8\u529b\u5b66\u6a21\u578b\u8bc4\u4f30\u6f14\u793a\u52a8\u4f5c\u7684\u53ef\u884c\u6027\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u4eff\u5b66\u4e60\u7684\u6548\u679c\u548c\u7a33\u5b9a\u6027\u3002", "result": "FABCO\u901a\u8fc7\u7ed3\u5408\u89c2\u5bdf\u884c\u4e3a\u514b\u9686\u548c\u53ef\u884c\u6027\u4f30\u8ba1\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u5b66\u4e60\u7a33\u5065\u7684\u63a7\u5236\u7b56\u7565\uff0c\u572815\u540d\u53c2\u4e0e\u8005\u7684\u5b9e\u9a8c\u4e2d\uff0cFABCO\u5728\u6a21\u4eff\u5b66\u4e60\u6027\u80fd\u4e0a\u63d0\u9ad8\u4e863.2\u500d\u4ee5\u4e0a\u3002", "conclusion": "FABCO\u6709\u6548\u89e3\u51b3\u4e86\u6f14\u793a\u6570\u636e\u4e0e\u673a\u5668\u4eba\u6267\u884c\u80fd\u529b\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u63d0\u5347\u4e86\u6a21\u4eff\u5b66\u4e60\u7684\u53ef\u884c\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2602.15413", "categories": ["cs.HC", "cs.DL"], "pdf": "https://arxiv.org/pdf/2602.15413", "abs": "https://arxiv.org/abs/2602.15413", "authors": ["Jonas Oppenlaender"], "title": "StatCounter: A Longitudinal Study of a Portable Scholarly Metric Display", "comment": "Published in the proceedings of 10th ACM International Symposium on Pervasive Displays (PerDis '26)", "summary": "This study explores a handheld, battery-operated e-ink device displaying Google Scholar citation statistics. The StatCounter places academic metrics into the flow of daily life rather than a desktop context. The work draws on a first-person, longitudinal auto-ethnographic inquiry examining how constant access to scholarly metrics influences motivation, attention, reflection, and emotional responses across work and non-work settings. The ambient proximity and pervasive availability of scholarly metrics invites frequent micro-checks, short reflective pauses, but also introduces moments of second-guessing when numbers drop or stagnate. Carrying the device prompts new narratives about academic identity, including a sense of companionship during travel and periods away from the office. Over time, the presence of the device turns metrics from an occasional reference into an ambient background of scholarly life. The study contributes insight into how situated, embodied access to academic metrics reshapes their meaning, and frames opportunities for designing tools that engage with scholarly evaluation in reflective ways.", "AI": {"tldr": "\u672c\u7814\u7a76\u8003\u5bdf\u4e86\u4e00\u79cd\u624b\u6301\u7535\u5b50\u8bbe\u5907\u5982\u4f55\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u5c55\u793a\u5b66\u672f\u6307\u6807\uff0c\u5e76\u5206\u6790\u5176\u5bf9\u5b66\u672f\u52a8\u673a\u3001\u6ce8\u610f\u529b\u548c\u53cd\u601d\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u4e86\u89e3\u5982\u4f55\u5728\u975e\u4f20\u7edf\u73af\u5883\u4e2d\u901a\u8fc7\u6280\u672f\u624b\u6bb5\u8d4b\u4e88\u5b66\u672f\u6307\u6807\u65b0\u7684\u610f\u4e49\uff0c\u4fc3\u4f7f\u4e2a\u4f53\u5bf9\u5b66\u672f\u8868\u73b0\u7684\u6301\u7eed\u5173\u6ce8\u4e0e\u53cd\u601d\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u7b2c\u4e00\u4eba\u79f0\u7684\u7eb5\u5411\u81ea\u6211\u6c11\u65cf\u5fd7\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u643a\u5e26\u8bbe\u5907\u8fc7\u7a0b\u4e2d\u5bf9\u5b66\u672f\u6307\u6807\u7684\u611f\u77e5\u548c\u53cd\u5e94\u3002", "result": "\u53d1\u73b0\u8bbe\u5907\u7684\u5b58\u5728\u4f7f\u5b66\u672f\u6307\u6807\u4ece\u5076\u5c14\u53c2\u8003\u8f6c\u53d8\u4e3a\u5b66\u672f\u751f\u6d3b\u7684\u80cc\u666f\uff0c\u5f71\u54cd\u4e86\u5b66\u672f\u8eab\u4efd\u7684\u53d9\u4e8b\u548c\u4e2a\u4f53\u7684\u60c5\u611f\u4f53\u9a8c\u3002", "conclusion": "\u672c\u7814\u7a76\u6df1\u5165\u63a2\u8ba8\u4e86\u968f\u65f6\u968f\u5730\u8bbf\u95ee\u5b66\u672f\u6307\u6807\u5982\u4f55\u6539\u53d8\u5b66\u672f\u8eab\u4efd\u548c\u8bc4\u4ef7\u7684\u610f\u4e49\uff0c\u63d0\u51fa\u4e86\u8bbe\u8ba1\u5177\u6709\u53cd\u601d\u610f\u4e49\u7684\u5b66\u672f\u8bc4\u4ef7\u5de5\u5177\u7684\u673a\u4f1a\u548c\u6846\u67b6\u3002"}}
{"id": "2602.15354", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.15354", "abs": "https://arxiv.org/abs/2602.15354", "authors": ["Jose Luis Peralta-Cabezas", "Miguel Torres-Torriti", "Marcelo Guarini-Hermann"], "title": "A Comparison of Bayesian Prediction Techniques for Mobile Robot Trajectory Tracking", "comment": "Accepted in Robotica (Dec. 2007), vol. 26, n. 5, pp. 571-585 (c) 2008 Cambridge University Press. https://doi.org/10.1017/S0263574708004153", "summary": "This paper presents a performance comparison of different estimation and prediction techniques applied to the problem of tracking multiple robots. The main performance criteria are the magnitude of the estimation or prediction error, the computational effort and the robustness of each method to non-Gaussian noise. Among the different techniques compared are the well known Kalman filters and their different variants (e.g. extended and unscented), and the more recent techniques relying on Sequential Monte Carlo Sampling methods, such as particle filters and Gaussian Mixture Sigma Point Particle Filter.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u591a\u79cd\u673a\u5668\u4eba\u8ddf\u8e2a\u7684\u4f30\u8ba1\u548c\u9884\u6d4b\u6280\u672f\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u5728\u4f30\u8ba1\u8bef\u5dee\u3001\u8ba1\u7b97\u6548\u7387\u53ca\u5bf9\u566a\u58f0\u9c81\u68d2\u6027\u65b9\u9762\u7684\u8868\u73b0\u3002", "motivation": "\u968f\u7740\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5e94\u7528\u9700\u6c42\u589e\u52a0\uff0c\u5bfb\u627e\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u8ddf\u8e2a\u6280\u672f\u6210\u4e3a\u5fc5\u8981\u3002", "method": "\u5206\u6790\u6bd4\u8f83\u4e86\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u53ca\u5176\u53d8\u79cd\u548c\u5e8f\u5217\u8499\u7279\u5361\u7f57\u91c7\u6837\u65b9\u6cd5\uff0c\u5305\u62ec\u7c92\u5b50\u6ee4\u6ce2\u5668\u548c\u9ad8\u65af\u6df7\u5408 sigma \u70b9\u7c92\u5b50\u6ee4\u6ce2\u5668\u3002", "result": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u591a\u79cd\u673a\u5668\u4eba\u8ddf\u8e2a\u7684\u4f30\u8ba1\u548c\u9884\u6d4b\u6280\u672f\u7684\u6027\u80fd\uff0c\u5305\u62ec\u4f20\u7edf\u7684\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u53ca\u5176\u53d8\u79cd\u3001\u57fa\u4e8e\u5e8f\u5217\u8499\u7279\u5361\u7f57\u91c7\u6837\u65b9\u6cd5\u7684\u6280\u672f\u7b49\u3002", "conclusion": "\u7814\u7a76\u6307\u51fa\u4e0d\u540c\u65b9\u6cd5\u5728\u4f30\u8ba1\u8bef\u5dee\u3001\u8ba1\u7b97\u52aa\u529b\u548c\u5bf9\u975e\u9ad8\u65af\u566a\u58f0\u7684\u9c81\u68d2\u6027\u65b9\u9762\u6709\u663e\u8457\u5dee\u5f02\u3002"}}
{"id": "2602.15489", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.15489", "abs": "https://arxiv.org/abs/2602.15489", "authors": ["Artur Solomonik", "Nicolas Ruiz", "Hendrik Heuer"], "title": "Reflecting on 1,000 Social Media Journeys: Generational Patterns in Platform Transition", "comment": null, "summary": "Social media has billions of users, but we still do not fully understand why users prefer one platform over another. Establishing new platforms among already popular competitors is difficult. Prior research has richly documented people's experiences within individual platforms, yet situating those experiences within the entirety of a user's social media experience remains challenging. What platforms have people used, and why have they transitioned between them? We collected data from a quota-based sample of 1,000 U.S. participants. We introduce the concept of \\emph{Social Media Journeys} to study the entirety of their social media experiences systematically. We identify push and pull factors across the social media landscape. We also show how different generations adopted social media platforms based on personal needs. With this work, we advance HCI by moving towards holistic perspectives when discussing social media technology, offering new insights for platform design, governance, and regulation.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u793e\u4ea4\u5a92\u4f53\u65c5\u7a0b\u7684\u6982\u5ff5\uff0c\u5206\u6790\u4e86\u7528\u6237\u793e\u4ea4\u5a92\u4f53\u7ecf\u5386\u7684\u63a8\u52a8\u4e0e\u62c9\u52a8\u56e0\u7d20\uff0c\u4e3a\u5e73\u53f0\u8bbe\u8ba1\u548c\u6cbb\u7406\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002", "motivation": "\u5c3d\u7ba1\u793e\u4ea4\u5a92\u4f53\u7528\u6237\u4f17\u591a\uff0c\u4f46\u6211\u4eec\u4ecd\u672a\u5b8c\u5168\u7406\u89e3\u7528\u6237\u4e3a\u4f55\u504f\u7231\u67d0\u4e00\u5e73\u53f0\u800c\u975e\u53e6\u4e00\u5e73\u53f0\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u77e5\u8bc6\u7a7a\u767d\uff0c\u5e76\u8003\u5bdf\u7528\u6237\u5728\u4e0d\u540c\u5e73\u53f0\u4e4b\u95f4\u7684\u8f6c\u79fb\u539f\u56e0\u3002", "method": "\u5bf9\u6765\u81ea1000\u540d\u7f8e\u56fd\u53c2\u4e0e\u8005\u7684\u914d\u989d\u6837\u672c\u6570\u636e\u8fdb\u884c\u6536\u96c6\u4e0e\u5206\u6790\uff0c\u4ee5\u7cfb\u7edf\u5730\u7814\u7a76\u7528\u6237\u7684\u793e\u4ea4\u5a92\u4f53\u7ecf\u5386\uff0c\u8bc6\u522b\u63a8\u52a8\u548c\u62c9\u52a8\u7684\u56e0\u7d20\u3002", "result": "\u672c\u7814\u7a76\u8bc6\u522b\u4e86\u793e\u4ea4\u5a92\u4f53\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u63a8\u52a8\u548c\u62c9\u52a8\u56e0\u7d20\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u4ee3\u9645\u6839\u636e\u4e2a\u4eba\u9700\u6c42\u91c7\u7528\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u7684\u65b9\u5f0f\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5168\u9762\u5206\u6790\u7528\u6237\u5728\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u7ecf\u5386\uff0c\u63d0\u51fa\u4e86\u793e\u4ea4\u5a92\u4f53\u65c5\u7a0b\u7684\u6982\u5ff5\uff0c\u4e3a\u5e73\u53f0\u8bbe\u8ba1\u3001\u6cbb\u7406\u548c\u76d1\u7ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2602.15357", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15357", "abs": "https://arxiv.org/abs/2602.15357", "authors": ["Xinhao Chen", "Hongkun Yao", "Anuruddha Bhattacharjee", "Suraj Raval", "Lamar O. Mair", "Yancy Diaz-Mercado", "Axel Krieger"], "title": "Fluoroscopy-Constrained Magnetic Robot Control via Zernike-Based Field Modeling and Nonlinear MPC", "comment": null, "summary": "Magnetic actuation enables surgical robots to navigate complex anatomical pathways while reducing tissue trauma and improving surgical precision. However, clinical deployment is limited by the challenges of controlling such systems under fluoroscopic imaging, which provides low frame rate and noisy pose feedback. This paper presents a control framework that remains accurate and stable under such conditions by combining a nonlinear model predictive control (NMPC) framework that directly outputs coil currents, an analytically differentiable magnetic field model based on Zernike polynomials, and a Kalman filter to estimate the robot state. Experimental validation is conducted with two magnetic robots in a 3D-printed fluid workspace and a spine phantom replicating drug delivery in the epidural space. Results show the proposed control method remains highly accurate when feedback is downsampled to 3 Hz with added Gaussian noise (sigma = 2 mm), mimicking clinical fluoroscopy. In the spine phantom experiments, the proposed method successfully executed a drug delivery trajectory with a root mean square (RMS) position error of 1.18 mm while maintaining safe clearance from critical anatomical boundaries.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u63a7\u5236\u6846\u67b6\uff0c\u80fd\u591f\u63d0\u9ad8\u78c1\u9a71\u52a8\u624b\u672f\u673a\u5668\u4eba\u5728\u4f4e\u5e27\u7387\u548c\u566a\u58f0\u6761\u4ef6\u4e0b\u7684\u63a7\u5236\u7cbe\u5ea6\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u5728\u836f\u7269\u9012\u9001\u4e2d\u7684\u6709\u6548\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u89e3\u51b3\u5728\u8367\u5149\u6210\u50cf\u6761\u4ef6\u4e0b\uff0c\u63a7\u5236\u78c1\u9a71\u52a8\u624b\u672f\u673a\u5668\u4eba\u65f6\u9762\u4e34\u7684\u4f4e\u5e27\u7387\u548c\u566a\u58f0\u53cd\u9988\u7684\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u624b\u672f\u7cbe\u5ea6\u5e76\u51cf\u5c11\u7ec4\u7ec7\u521b\u4f24\u3002", "method": "\u7ed3\u5408\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236(NMPC)\u3001\u57fa\u4e8eZernike\u591a\u9879\u5f0f\u7684\u53ef\u5fae\u5206\u78c1\u573a\u6a21\u578b\u548c\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u7684\u63a7\u5236\u6846\u67b6\u3002", "result": "\u5728\u53cd\u9988\u9891\u7387\u964d\u81f33Hz\u5e76\u6dfb\u52a0\u9ad8\u65af\u566a\u58f0\u7684\u60c5\u51b5\u4e0b\uff0c\u8be5\u63a7\u5236\u65b9\u6cd5\u4ecd\u4fdd\u6301\u9ad8\u7cbe\u5ea6\uff1b\u5728\u810a\u67f1\u6a21\u578b\u5b9e\u9a8c\u4e2d\uff0c\u6210\u529f\u6267\u884c\u836f\u7269\u9012\u9001\u8f68\u8ff9\uff0c\u5747\u65b9\u6839\u4f4d\u7f6e\u8bef\u5dee\u4e3a1.18mm\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u63a7\u5236\u65b9\u6cd5\u5728\u6a21\u62df\u4e34\u5e8a\u73af\u5883\u4e0b\u5177\u6709\u9ad8\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\uff0c\u80fd\u591f\u6709\u6548\u6267\u884c\u836f\u7269\u9012\u9001\u8f68\u8ff9\uff0c\u5e76\u786e\u4fdd\u4e0e\u5173\u952e\u89e3\u5256\u8fb9\u754c\u7684\u5b89\u5168\u95f4\u9694\u3002"}}
{"id": "2602.15569", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.15569", "abs": "https://arxiv.org/abs/2602.15569", "authors": ["Johannes Kirmayr", "Raphael Wennmacher", "Khanh Huynh", "Lukas Stappen", "Elisabeth Andr\u00e9", "Florian Alt"], "title": "\"What Are You Doing?\": Effects of Intermediate Feedback from Agentic LLM In-Car Assistants During Multi-Step Processing", "comment": "Accepted (conditionally) at CHI 2026", "summary": "Agentic AI assistants that autonomously perform multi-step tasks raise open questions for user experience: how should such systems communicate progress and reasoning during extended operations, especially in attention-critical contexts such as driving? We investigate feedback timing and verbosity from agentic LLM-based in-car assistants through a controlled, mixed-methods study (N=45) comparing planned steps and intermediate results feedback against silent operation with final-only response. Using a dual-task paradigm with an in-car voice assistant, we found that intermediate feedback significantly improved perceived speed, trust, and user experience while reducing task load - effects that held across varying task complexities and interaction contexts. Interviews further revealed user preferences for an adaptive approach: high initial transparency to establish trust, followed by progressively reducing verbosity as systems prove reliable, with adjustments based on task stakes and situational context. We translate our empirical findings into design implications for feedback timing and verbosity in agentic assistants, balancing transparency and efficiency.", "AI": {"tldr": "\u63a2\u7d22\u81ea\u4e3b\u667a\u80fd\u52a9\u624b\u5982\u4f55\u5728\u9a7e\u9a76\u7b49\u6ce8\u610f\u529b\u5173\u952e\u573a\u666f\u4e2d\u4f20\u8fbe\u53cd\u9988\uff0c\u53d1\u73b0\u4e2d\u95f4\u53cd\u9988\u63d0\u9ad8\u7528\u6237\u4f53\u9a8c\u53ca\u4fe1\u4efb\uff0c\u5e76\u63d0\u51fa\u8bbe\u8ba1\u5efa\u8bae\u3002", "motivation": "\u63a2\u8ba8\u81ea\u4e3b\u6267\u884c\u591a\u6b65\u4efb\u52a1\u7684\u667a\u80fd\u52a9\u624b\u5982\u4f55\u5728\u6ce8\u610f\u529b\u5173\u952e\u7684\u573a\u666f\u4e2d\u6709\u6548\u5730\u4e0e\u7528\u6237\u6c9f\u901a\u8fdb\u5ea6\u548c\u63a8\u7406\u3002", "method": "\u901a\u8fc7\u5bf945\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u53d7\u63a7\u7684\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\uff0c\u6bd4\u8f83\u8ba1\u5212\u6b65\u9aa4\u548c\u4e2d\u95f4\u7ed3\u679c\u53cd\u9988\u4e0e\u4ec5\u63d0\u4f9b\u6700\u7ec8\u54cd\u5e94\u7684\u65e0\u53cd\u9988\u64cd\u4f5c\u3002", "result": "\u4e2d\u95f4\u53cd\u9988\u663e\u8457\u63d0\u9ad8\u4e86\u7528\u6237\u7684\u611f\u77e5\u901f\u5ea6\u3001\u4fe1\u4efb\u548c\u7528\u6237\u4f53\u9a8c\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u4efb\u52a1\u8d1f\u62c5\uff0c\u5e76\u5728\u4e0d\u540c\u7684\u4efb\u52a1\u590d\u6742\u6027\u548c\u4e92\u52a8\u73af\u5883\u4e2d\u4fdd\u6301\u4e86\u8fd9\u4e9b\u6548\u679c\u3002", "conclusion": "\u7528\u6237\u503e\u5411\u4e8e\u4e00\u79cd\u81ea\u9002\u5e94\u7684\u53cd\u9988\u65b9\u5f0f\uff0c\u521d\u671f\u9ad8\u900f\u660e\u5ea6\u4ee5\u5efa\u7acb\u4fe1\u4efb\uff0c\u968f\u540e\u9010\u6b65\u51cf\u5c11\u8be6\u7ec6\u4fe1\u606f\uff0c\u6839\u636e\u4efb\u52a1\u98ce\u9669\u548c\u60c5\u5883\u80cc\u666f\u8c03\u6574\u3002"}}
{"id": "2602.15397", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15397", "abs": "https://arxiv.org/abs/2602.15397", "authors": ["Zibin Dong", "Yicheng Liu", "Shiduo Zhang", "Baijun Ye", "Yifu Yuan", "Fei Ni", "Jingjing Gong", "Xipeng Qiu", "Hang Zhao", "Yinchuan Li", "Jianye Hao"], "title": "ActionCodec: What Makes for Good Action Tokenizers", "comment": null, "summary": "Vision-Language-Action (VLA) models leveraging the native autoregressive paradigm of Vision-Language Models (VLMs) have demonstrated superior instruction-following and training efficiency. Central to this paradigm is action tokenization, yet its design has primarily focused on reconstruction fidelity, failing to address its direct impact on VLA optimization. Consequently, the fundamental question of \\textit{what makes for good action tokenizers} remains unanswered. In this paper, we bridge this gap by establishing design principles specifically from the perspective of VLA optimization. We identify a set of best practices based on information-theoretic insights, including maximized temporal token overlap, minimized vocabulary redundancy, enhanced multimodal mutual information, and token independence. Guided by these principles, we introduce \\textbf{ActionCodec}, a high-performance action tokenizer that significantly enhances both training efficiency and VLA performance across diverse simulation and real-world benchmarks. Notably, on LIBERO, a SmolVLM2-2.2B fine-tuned with ActionCodec achieves a 95.5\\% success rate without any robotics pre-training. With advanced architectural enhancements, this reaches 97.4\\%, representing a new SOTA for VLA models without robotics pre-training. We believe our established design principles, alongside the released model, will provide a clear roadmap for the community to develop more effective action tokenizers.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u4eceVLA\u4f18\u5316\u7684\u89d2\u5ea6\u5efa\u7acb\u8bbe\u8ba1\u539f\u5219\uff0c\u63d0\u51fa\u9ad8\u6548\u7684\u52a8\u4f5c\u6807\u8bb0\u5668ActionCodec\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u52a8\u4f5c\u6807\u8bb0\u8bbe\u8ba1\u66f4\u591a\u5173\u6ce8\u4e8e\u91cd\u6784\u7cbe\u5ea6\uff0c\u800c\u5ffd\u89c6\u4e86\u5176\u5bf9Vision-Language-Action (VLA)\u6a21\u578b\u4f18\u5316\u7684\u76f4\u63a5\u5f71\u54cd\u3002", "method": "\u5efa\u7acb\u4eceVLA\u4f18\u5316\u7684\u89d2\u5ea6\u51fa\u53d1\u7684\u8bbe\u8ba1\u539f\u5219\uff0c\u5e76\u6839\u636e\u4fe1\u606f\u8bba\u89c1\u89e3\uff0c\u8bc6\u522b\u4e00\u7ec4\u6700\u4f73\u5b9e\u8df5\uff0c\u63a8\u51fa\u9ad8\u6027\u80fd\u7684\u52a8\u4f5c\u6807\u8bb0\u5668ActionCodec\u3002", "result": "ActionCodec\u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u548cVLA\u6027\u80fd\uff0c\u5728LIBERO\u57fa\u51c6\u4e0a\uff0c\u7ecf\u8fc7ActionCodec\u5fae\u8c03\u7684SmolVLM2-2.2B\u8fbe\u5230\u4e8695.5%\u7684\u6210\u529f\u7387\uff0c\u6700\u65b0\u67b6\u6784\u7684\u6027\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u81f397.4%\u3002", "conclusion": "\u6211\u4eec\u7684\u8bbe\u8ba1\u539f\u5219\u548c\u53d1\u5e03\u7684\u6a21\u578b\u4e3a\u793e\u533a\u5f00\u53d1\u66f4\u6709\u6548\u7684\u52a8\u4f5c\u6807\u8bb0\u5668\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u8def\u7ebf\u56fe\u3002"}}
{"id": "2602.15631", "categories": ["cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.15631", "abs": "https://arxiv.org/abs/2602.15631", "authors": ["Lan Luo", "Dongyijie Primo Pan", "Junhua Zhu", "Muzhi Zhou", "Pan Hui"], "title": "Meflex: A Multi-agent Scaffolding System for Entrepreneurial Ideation Iteration via Nonlinear Business Plan Writing", "comment": null, "summary": "Business plan (BP) writing plays a key role in entrepreneurship education by helping learners construct, evaluate, and iteratively refine their ideas. However, conventional BP writing remains a rigid, linear process that often fails to reflect the dynamic and recursive nature of entrepreneurial ideation. This mismatch is particularly challenging for novice entrepreneurial students, who struggle with the substantial cognitive demands of developing and refining ideas. While reflection and meta-reflection are critical strategies for fostering divergent and convergent thinking, existing writing tools rarely scaffold these higher-order processes. To address this gap, we present the Meflex System, a large language model (LLM)-based writing tool that integrates BP writing scaffolding with a nonlinear idea canvas to support iterative ideation through reflection and meta-reflection. We report findings from an exploratory user study with 30 participants that examined the system's usability and cognitive impact. Results show that Meflex effectively scaffolds BP writing, promotes divergent thinking through LLM-supported reflection, and enhances meta-reflective awareness while reducing cognitive load during complex idea development. These findings highlight the potential of non-linear LLM-based writing tools to foster deeper and coherent entrepreneurial thinking.", "AI": {"tldr": "Meflex\u7cfb\u7edf\u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5199\u4f5c\u5de5\u5177\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u521b\u610f\u753b\u5e03\u548c\u53cd\u601d\u652f\u6301\u5e2e\u52a9\u521b\u4e1a\u6559\u80b2\uff0c\u51cf\u8f7b\u8ba4\u77e5\u8d1f\u62c5\u5e76\u4fc3\u8fdb\u6df1\u5c42\u6b21\u7684\u521b\u4e1a\u601d\u7ef4\u3002", "motivation": "\u4f20\u7edf\u5546\u4e1a\u8ba1\u5212\u5199\u4f5c\u65b9\u6cd5\u8fc7\u4e8e\u50f5\u5316\uff0c\u65e0\u6cd5\u53cd\u6620\u521b\u4e1a\u601d\u7ef4\u7684\u52a8\u6001\u7279\u6027\uff0c\u8fd9\u5bf9\u521d\u5b66\u8005\u9020\u6210\u4e86\u8ba4\u77e5\u8d1f\u62c5\u3002", "method": "\u901a\u8fc7\u4e0e30\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u63a2\u7d22\u6027\u7528\u6237\u7814\u7a76\uff0c\u8bc4\u4f30Meflex\u7cfb\u7edf\u7684\u53ef\u7528\u6027\u548c\u8ba4\u77e5\u5f71\u54cd\u3002", "result": "Meflex\u7cfb\u7edf\u6709\u6548\u7ed3\u5408\u5546\u4e1a\u8ba1\u5212\u5199\u4f5c\u7684\u652f\u6491\u548c\u975e\u7ebf\u6027\u521b\u610f\u753b\u5e03\uff0c\u652f\u6301\u901a\u8fc7\u53cd\u601d\u548c\u5143\u53cd\u601d\u63a8\u52a8\u8fed\u4ee3\u521b\u610f\u3002", "conclusion": "Meflex\u7cfb\u7edf\u663e\u8457\u4fc3\u8fdb\u4e86\u5546\u4e1a\u8ba1\u5212\u5199\u4f5c\uff0c\u63d0\u5347\u4e86\u53d1\u6563\u601d\u7ef4\u548c\u5143\u53cd\u601d\u610f\u8bc6\uff0c\u540c\u65f6\u51cf\u8f7b\u4e86\u5728\u590d\u6742\u521b\u610f\u53d1\u5c55\u8fc7\u7a0b\u4e2d\u7684\u8ba4\u77e5\u8d1f\u62c5\u3002"}}
{"id": "2602.15398", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15398", "abs": "https://arxiv.org/abs/2602.15398", "authors": ["Abdelrahman Metwally", "Monijesu James", "Aleksey Fedoseev", "Miguel Altamirano Cabrera", "Dzmitry Tsetserukou", "Andrey Somov"], "title": "Hybrid F' and ROS2 Architecture for Vision-Based Autonomous Flight: Design and Experimental Validation", "comment": "Paper accepted to ICIT 2026", "summary": "Autonomous aerospace systems require architectures that balance deterministic real-time control with advanced perception capabilities. This paper presents an integrated system combining NASA's F' flight software framework with ROS2 middleware via Protocol Buffers bridging. We evaluate the architecture through a 32.25-minute indoor quadrotor flight test using vision-based navigation. The vision system achieved 87.19 Hz position estimation with 99.90\\% data continuity and 11.47 ms mean latency, validating real-time performance requirements. All 15 ground commands executed successfully with 100 % success rate, demonstrating robust F'--PX4 integration. System resource utilization remained low (15.19 % CPU, 1,244 MB RAM) with zero stale telemetry messages, confirming efficient operation on embedded platforms. Results validate the feasibility of hybrid flight-software architectures combining certification-grade determinism with flexible autonomy for autonomous aerial vehicles.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408NASA\u7684F'\u98de\u884c\u8f6f\u4ef6\u6846\u67b6\u4e0eROS2\u4e2d\u95f4\u4ef6\u7684\u81ea\u4e3b\u822a\u7a7a\u7cfb\u7edf\u67b6\u6784\uff0c\u7ecf\u8fc732.25\u5206\u949f\u7684\u5ba4\u5185\u98de\u884c\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u5b9e\u65f6\u6027\u80fd\u548c\u9ad8\u6548\u6027\u3002", "motivation": "\u968f\u7740\u81ea\u4e3b\u822a\u7a7a\u7cfb\u7edf\u7684\u9700\u6c42\u589e\u52a0\uff0c\u4e9f\u9700\u5e73\u8861\u786e\u5b9a\u6027\u5b9e\u65f6\u63a7\u5236\u4e0e\u5148\u8fdb\u611f\u77e5\u80fd\u529b\u7684\u67b6\u6784\u3002", "method": "\u5c06NASA\u7684F'\u98de\u884c\u8f6f\u4ef6\u6846\u67b6\u4e0eROS2\u4e2d\u95f4\u4ef6\u901a\u8fc7Protocol Buffers\u8fde\u63a5\uff0c\u8fdb\u884c\u5ba4\u5185\u56db\u65cb\u7ffc\u98de\u884c\u6d4b\u8bd5\u3002", "result": "\u98de\u884c\u6d4b\u8bd5\u4e2d\uff0c\u89c6\u89c9\u7cfb\u7edf\u8fbe\u523087.19 Hz\u7684\u4f4d\u7f6e\u4f30\u8ba1\uff0c\u6570\u636e\u8fde\u7eed\u6027\u4e3a99.90\u001a\uff0c\u5e73\u5747\u5ef6\u8fdf\u4e3a11.47\u6beb\u79d2\uff0c\u6240\u670915\u4e2a\u5730\u9762\u6307\u4ee4100%\u6210\u529f\u6267\u884c\uff0cCPU\u548cRAM\u4f7f\u7528\u7387\u4fdd\u6301\u4f4e\u6c34\u5e73\u3002", "conclusion": "\u672c\u7814\u7a76\u7ed3\u679c\u9a8c\u8bc1\u4e86\u7ed3\u5408\u8ba4\u8bc1\u7b49\u7ea7\u786e\u5b9a\u6027\u4e0e\u7075\u6d3b\u81ea\u4e3b\u6027\u7684\u6df7\u5408\u98de\u884c\u8f6f\u4ef6\u67b6\u6784\u5728\u81ea\u4e3b\u822a\u7a7a\u5668\u4e2d\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2602.15698", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15698", "abs": "https://arxiv.org/abs/2602.15698", "authors": ["Ning Wang", "Chen Liang"], "title": "How to Disclose? Strategic AI Disclosure in Crowdfunding", "comment": null, "summary": "As artificial intelligence (AI) increasingly integrates into crowdfunding practices, strategic disclosure of AI involvement has become critical. Yet, empirical insights into how different disclosure strategies influence investor decisions remain limited. Drawing on signaling theory and Aristotle's rhetorical framework, we examine how mandatory AI disclosure affects crowdfunding performance and how substantive signals (degree of AI involvement) and rhetorical signals (logos/explicitness, ethos/authenticity, pathos/emotional tone) moderate these effects. Leveraging Kickstarter's mandatory AI disclosure policy as a natural experiment and four supplementary online experiments, we find that mandatory AI disclosure significantly reduces crowdfunding performance: funds raised decline by 39.8% and backer counts by 23.9% for AI-involved projects. However, this adverse effect is systematically moderated by disclosure strategy. Greater AI involvement amplifies the negative effects of AI disclosure, while high authenticity and high explicitness mitigate them. Interestingly, excessive positive emotional tone (a strategy creators might intuitively adopt to counteract AI skepticism) backfires and exacerbates negative outcomes. Supplementary randomized experiments identify two underlying mechanisms: perceived creator competence and AI washing concerns. Substantive signals primarily affect competence judgments, whereas rhetorical signals operate through varied pathways: either mediator alone or both in sequence. These findings provide theoretical and practical insights for entrepreneurs, platforms, and policymakers strategically managing AI transparency in high-stakes investment contexts.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5f3a\u5236\u6027AI\u62ab\u9732\u5bf9\u4f17\u7b79\u8868\u73b0\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u62ab\u9732\u7b56\u7565\u663e\u8457\u8c03\u8282\u8fd9\u4e00\u5f71\u54cd\uff0c\u5f3a\u8c03\u4e86\u900f\u660e\u5ea6\u7ba1\u7406\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u9010\u6e10\u878d\u5165\u4f17\u7b79\u5b9e\u8df5\uff0c\u63a2\u7d22\u4e0d\u540c\u62ab\u9732\u7b56\u7565\u5982\u4f55\u5f71\u54cd\u6295\u8d44\u8005\u51b3\u7b56\u663e\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5229\u7528Kickstarter\u7684\u5f3a\u5236\u6027AI\u62ab\u9732\u653f\u7b56\u4f5c\u4e3a\u81ea\u7136\u5b9e\u9a8c\uff0c\u540c\u65f6\u7ed3\u5408\u56db\u4e2a\u5728\u7ebf\u5b9e\u9a8c\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5f3a\u5236\u6027AI\u62ab\u9732\u4f7f\u5f97\u4f17\u7b79\u8d44\u91d1\u51cf\u5c1139.8%\uff0c\u652f\u6301\u8005\u6570\u91cf\u51cf\u5c1123.9%\u3002\u62ab\u9732\u7b56\u7565\u7684\u9009\u62e9\u5bf9\u8fd9\u4e9b\u8d1f\u9762\u6548\u679c\u5177\u6709\u7cfb\u7edf\u6027\u7684\u8c03\u8282\u4f5c\u7528\u3002", "conclusion": "\u5f3a\u5236\u6027AI\u62ab\u9732\u663e\u8457\u964d\u4f4e\u4e86\u4f17\u7b79\u8868\u73b0\uff0c\u4f46\u4e0d\u540c\u7684\u62ab\u9732\u7b56\u7565\u5bf9\u8be5\u6548\u679c\u5177\u6709\u8c03\u8282\u4f5c\u7528\u3002"}}
{"id": "2602.15400", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15400", "abs": "https://arxiv.org/abs/2602.15400", "authors": ["Zerui Li", "Hongpei Zheng", "Fangguo Zhao", "Aidan Chan", "Jian Zhou", "Sihao Lin", "Shijie Li", "Qi Wu"], "title": "One Agent to Guide Them All: Empowering MLLMs for Vision-and-Language Navigation via Explicit World Representation", "comment": null, "summary": "A navigable agent needs to understand both high-level semantic instructions and precise spatial perceptions. Building navigation agents centered on Multimodal Large Language Models (MLLMs) demonstrates a promising solution due to their powerful generalization ability. However, the current tightly coupled design dramatically limits system performance. In this work, we propose a decoupled design that separates low-level spatial state estimation from high-level semantic planning. Unlike previous methods that rely on predefined, oversimplified textual maps, we introduce an interactive metric world representation that maintains rich and consistent information, allowing MLLMs to interact with and reason on it for decision-making. Furthermore, counterfactual reasoning is introduced to further elicit MLLMs' capacity, while the metric world representation ensures the physical validity of the produced actions. We conduct comprehensive experiments in both simulated and real-world environments. Our method establishes a new zero-shot state-of-the-art, achieving 48.8\\% Success Rate (SR) in R2R-CE and 42.2\\% in RxR-CE benchmarks. Furthermore, to validate the versatility of our metric representation, we demonstrate zero-shot sim-to-real transfer across diverse embodiments, including a wheeled TurtleBot 4 and a custom-built aerial drone. These real-world deployments verify that our decoupled framework serves as a robust, domain-invariant interface for embodied Vision-and-Language navigation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u8bbe\u8ba1\u7684\u5bfc\u822a\u6846\u67b6\uff0c\u5229\u7528\u4ea4\u4e92\u5f0f\u5ea6\u91cf\u4e16\u754c\u8868\u793a\u548c\u53cd\u4e8b\u5b9e\u63a8\u7406\uff0c\u5728\u591a\u79cd\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u8fbe\u5230\u4e86\u65b0\u7684\u96f6-shot\u6700\u4f18\u8868\u73b0\u3002", "motivation": "\u6784\u5efa\u4e00\u4e2a\u6709\u6548\u7684\u5bfc\u822a\u667a\u80fd\u4f53\uff0c\u9700\u8981\u540c\u65f6\u7406\u89e3\u9ad8\u5c42\u6b21\u7684\u8bed\u4e49\u6307\u4ee4\u548c\u7cbe\u786e\u7684\u7a7a\u95f4\u611f\u77e5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u8bbe\u8ba1\uff0c\u5c06\u4f4e\u5c42\u6b21\u7684\u7a7a\u95f4\u72b6\u6001\u4f30\u8ba1\u4e0e\u9ad8\u5c42\u6b21\u7684\u8bed\u4e49\u89c4\u5212\u5206\u79bb\uff0c\u4f7f\u7528\u4ea4\u4e92\u5f0f\u5ea6\u91cf\u4e16\u754c\u8868\u793a\uff0c\u7ed3\u5408\u53cd\u4e8b\u5b9e\u63a8\u7406\uff0c\u589e\u5f3a\u4e86\u667a\u80fd\u4f53\u7684\u51b3\u7b56\u80fd\u529b\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u8fdb\u884c\u5168\u9762\u5b9e\u9a8c\uff0c\u65b9\u6cd5\u5728R2R-CE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e8648.8%\u7684\u6210\u529f\u7387\uff0cRxR-CE\u4e2d\u4e3a42.2%\u3002", "conclusion": "\u8be5\u89e3\u8026\u6846\u67b6\u8bc1\u660e\u4e86\u4f5c\u4e3a\u4e00\u79cd\u5f3a\u5927\u7684\u3001\u9886\u57df\u4e0d\u53d8\u7684\u63a5\u53e3\uff0c\u652f\u6301\u66f4\u597d\u7684Vision-and-Language\u5bfc\u822a\uff0c\u5b9e\u9645\u5e94\u7528\u4e2d\u5c55\u793a\u4e86\u96f6-shot\u4ece\u6a21\u62df\u5230\u771f\u5b9e\u7684\u8fc1\u79fb\u80fd\u529b\u3002"}}
{"id": "2602.15738", "categories": ["cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15738", "abs": "https://arxiv.org/abs/2602.15738", "authors": ["Bel\u00e9n Mart\u00edn-Urcelay", "Yoonsang Lee", "Matthieu R. Bloch", "Christopher J. Rozell"], "title": "Beyond Labels: Information-Efficient Human-in-the-Loop Learning using Ranking and Selection Queries", "comment": null, "summary": "Integrating human expertise into machine learning systems often reduces the role of experts to labeling oracles, a paradigm that limits the amount of information exchanged and fails to capture the nuances of human judgment. We address this challenge by developing a human-in-the-loop framework to learn binary classifiers with rich query types, consisting of item ranking and exemplar selection. We first introduce probabilistic human response models for these rich queries motivated by the relationship experimentally observed between the perceived implicit score of an item and its distance to the unknown classifier. Using these models, we then design active learning algorithms that leverage the rich queries to increase the information gained per interaction. We provide theoretical bounds on sample complexity and develop a tractable and computationally efficient variational approximation. Through experiments with simulated annotators derived from crowdsourced word-sentiment and image-aesthetic datasets, we demonstrate significant reductions on sample complexity. We further extend active learning strategies to select queries that maximize information rate, explicitly balancing informational value against annotation cost. This algorithm in the word sentiment classification task reduces learning time by more than 57\\% compared to traditional label-only active learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4eba\u673a\u534f\u540c\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e30\u5bcc\u7684\u67e5\u8be2\u7c7b\u578b\u548c\u4e3b\u52a8\u5b66\u4e60\u7b97\u6cd5\uff0c\u63d0\u9ad8\u4e86\u4fe1\u606f\u83b7\u53d6\u6548\u7387\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u4e86\u6837\u672c\u590d\u6742\u5ea6\u548c\u5b66\u4e60\u65f6\u95f4\u3002", "motivation": "\u4f20\u7edf\u7684\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u5c06\u4e13\u5bb6\u7684\u89d2\u8272\u9650\u5236\u5728\u6807\u6ce8\u795e oracle\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u4eba\u7c7b\u5224\u65ad\u7684\u7ec6\u5fae\u5dee\u522b\uff0c\u5bfc\u81f4\u4fe1\u606f\u4ea4\u6d41\u7684\u4e0d\u8db3\u3002", "method": "\u5229\u7528\u5e26\u6709\u4eba\u7c7b\u53cd\u9988\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u4e30\u5bcc\u7684\u67e5\u8be2\u7c7b\u578b\uff08\u5982\u9879\u76ee\u6392\u540d\u548c\u793a\u4f8b\u9009\u62e9\uff09\uff0c\u8bbe\u8ba1\u4e3b\u52a8\u5b66\u4e60\u7b97\u6cd5\uff0c\u5e76\u91c7\u7528\u6982\u7387\u4eba\u7c7b\u54cd\u5e94\u6a21\u578b\u548c\u53d8\u5206\u8fd1\u4f3c\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86\u4f7f\u7528\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6837\u672c\u590d\u6742\u5ea6\u4e0a\u663e\u8457\u51cf\u5c11\uff0c\u7279\u522b\u662f\u5728\u8bcd\u6c47\u60c5\u611f\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u5b66\u4e60\u65f6\u95f4\u6bd4\u4f20\u7edf\u7684\u4ec5\u6807\u7b7e\u4e3b\u52a8\u5b66\u4e60\u51cf\u5c11\u4e8657%\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u7684\u6846\u67b6\u663e\u8457\u964d\u4f4e\u4e86\u6837\u672c\u590d\u6742\u5ea6\uff0c\u5e76\u63d0\u9ad8\u4e86\u4fe1\u606f\u83b7\u53d6\u7684\u6548\u7387\uff0c\u80fd\u591f\u6709\u6548\u5730\u5728\u6807\u7b7e\u4ec5\u7684\u4e3b\u52a8\u5b66\u4e60\u4e2d\u51cf\u5c11\u5b66\u4e60\u65f6\u95f4\u3002"}}
{"id": "2602.15424", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15424", "abs": "https://arxiv.org/abs/2602.15424", "authors": ["Branimir \u0106aran", "Vladimir Mili\u0107", "Bojan Jerbi\u0107"], "title": "Lyapunov-Based $\\mathcal{L}_2$-Stable PI-Like Control of a Four-Wheel Independently Driven and Steered Robot", "comment": "SUBMITTED FOR POTENTIAL PUBLICATION IN IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION", "summary": "In this letter, Lyapunov-based synthesis of a PI-like controller is proposed for $\\mathcal{L}_2$-stable motion control of an independently driven and steered four-wheel mobile robot. An explicit, structurally verified model is used to enable systematic controller design with stability and performance guarantees suitable for real-time operation. A Lyapunov function is constructed to yield explicit bounds and $\\mathcal{L}_2$ stability results, supporting feedback synthesis that reduces configuration dependent effects. The resulting control law maintains a PI-like form suitable for standard embedded implementation while preserving rigorous stability properties. Effectiveness and robustness are demonstrated experimentally on a real four-wheel mobile robot platform.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLyapunov\u7684PI\u7c7b\u63a7\u5236\u5668\u7528\u4e8e\u56db\u8f6e\u79fb\u52a8\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u63a7\u5236\uff0c\u786e\u4fdd\u4e86\u5b9e\u65f6\u64cd\u4f5c\u7684\u7a33\u5b9a\u6027\u4e0e\u6027\u80fd\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u5176\u6709\u6548\u6027\u548c\u575a\u56fa\u6027\u3002", "motivation": "\u7814\u7a76\u76ee\u6807\u662f\u5b9e\u73b0\u56db\u8f6e\u79fb\u52a8\u673a\u5668\u4eba\u7684$\textbf{L}_2$\u7a33\u5b9a\u8fd0\u52a8\u63a7\u5236\uff0c\u63d0\u9ad8\u5b9e\u65f6\u64cd\u4f5c\u7684\u5b89\u5168\u6027\u548c\u6709\u6548\u6027\u3002", "method": "\u57fa\u4e8eLyapunov\u7684\u65b9\u6cd5\u5408\u6210PI\u7c7b\u63a7\u5236\u5668\uff0c\u5229\u7528\u663e\u6027\u6a21\u578b\u8fdb\u884c\u7cfb\u7edf\u5316\u8bbe\u8ba1\uff0c\u786e\u4fdd\u7a33\u5b9a\u6027\u4e0e\u6027\u80fd\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u63a7\u5236\u6cd5\u5f8b\u4fdd\u6301PI\u7c7b\u5f62\u5f0f\uff0c\u5177\u5907\u663e\u8457\u7684\u7a33\u5b9a\u6027\u5c5e\u6027\uff0c\u5e76\u5728\u5b9e\u9645\u56db\u8f6e\u79fb\u52a8\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6548\u679c\u4e0e\u5f3a\u5065\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684PI\u7c7b\u63a7\u5236\u5668\u5728\u56db\u8f6e\u79fb\u52a8\u673a\u5668\u4eba\u4e0a\u6709\u6548\u4e14\u7a33\u5065\u5730\u8fd0\u4f5c\uff0c\u8bc1\u660e\u4e86\u5b83\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u548c\u7a33\u5b9a\u6027\u7279\u6027\u3002"}}
{"id": "2602.15745", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.15745", "abs": "https://arxiv.org/abs/2602.15745", "authors": ["Ashlee Milton", "Dan Runningen", "Loren Terveen", "Harmanpreet Kaur", "Stevie Chancellor"], "title": "Unraveling Entangled Feeds: Rethinking Social Media Design to Enhance User Well-being", "comment": "Conditionally accepted to the 2026 CHI Conference on Human Factors in Computing Systems", "summary": "Social media platforms have rapidly adopted algorithmic curation with little consideration for the potential harm to users' mental well-being. We present findings from design workshops with 21 participants diagnosed with mental illness about their interactions with social media platforms. We find that users develop cause-and-effect explanations, or folk theories, to understand their experiences with algorithmic curation. These folk theories highlight a breakdown in algorithmic design that we explain using the framework of entanglement, a phenomenon where there is a disconnect between users' actions and platform outcomes on an emotional level. Participants' designs to address entanglement and mitigate harms centered on contextualizing their engagement and restoring explicit user control on social media. The conceptualization of entanglement and the resulting design recommendations have implications for social computing and recommender systems research, particularly in evaluating and designing social media platforms that support users' mental well-being.", "AI": {"tldr": "\u672c\u7814\u7a76\u53d1\u73b0\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u7684\u7b97\u6cd5\u8bbe\u8ba1\u5f71\u54cd\u7528\u6237\u5fc3\u7406\u5065\u5eb7\uff0c\u7528\u6237\u901a\u8fc7\u5de5\u4f5c\u574a\u63d0\u51fa\u4e86\u6539\u5584\u4e92\u52a8\u7684\u5efa\u8bae\uff0c\u5f3a\u8c03\u4e86\u7528\u6237\u63a7\u5236\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u63a2\u8ba8\u793e\u4ea4\u5a92\u4f53\u7b97\u6cd5\u5bf9\u7528\u6237\u5fc3\u7406\u5065\u5eb7\u7684\u6f5c\u5728\u5f71\u54cd\uff0c\u5e76\u7406\u89e3\u7528\u6237\u5982\u4f55\u89e3\u91ca\u4ed6\u4eec\u7684\u4f53\u9a8c\u3002", "method": "\u901a\u8fc7\u4e0e21\u540d\u5fc3\u7406\u75be\u75c5\u60a3\u8005\u7684\u8bbe\u8ba1\u5de5\u4f5c\u574a\uff0c\u63a2\u7d22\u4ed6\u4eec\u4e0e\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u7684\u4e92\u52a8\u4e0e\u7406\u89e3\u3002", "result": "\u7528\u6237\u53d1\u5c55\u4e86\u56e0\u679c\u89e3\u91ca\uff0c\u63ed\u793a\u4e86\u7b97\u6cd5\u8bbe\u8ba1\u4e2d\u7684\u65ad\u88c2\u73b0\u8c61\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u5584\u7528\u6237\u4f53\u9a8c\u7684\u8bbe\u8ba1\u5efa\u8bae\u3002", "conclusion": "\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u7684\u7b97\u6cd5\u8bbe\u8ba1\u4e2d\uff0c\u7528\u6237\u5fc3\u7406\u5065\u5eb7\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u56de\u5f52\u7528\u6237\u63a7\u5236\u6743\u7684\u8bbe\u8ba1\u65b9\u6848\u3002"}}
{"id": "2602.15513", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15513", "abs": "https://arxiv.org/abs/2602.15513", "authors": ["Ji Li", "Jing Xia", "Mingyi Li", "Shiyan Hu"], "title": "Improving MLLMs in Embodied Exploration and Question Answering with Human-Inspired Memory Modeling", "comment": null, "summary": "Deploying Multimodal Large Language Models as the brain of embodied agents remains challenging, particularly under long-horizon observations and limited context budgets. Existing memory assisted methods often rely on textual summaries, which discard rich visual and spatial details and remain brittle in non-stationary environments. In this work, we propose a non-parametric memory framework that explicitly disentangles episodic and semantic memory for embodied exploration and question answering. Our retrieval-first, reasoning-assisted paradigm recalls episodic experiences via semantic similarity and verifies them through visual reasoning, enabling robust reuse of past observations without rigid geometric alignment. In parallel, we introduce a program-style rule extraction mechanism that converts experiences into structured, reusable semantic memory, facilitating cross-environment generalization. Extensive experiments demonstrate state-of-the-art performance on embodied question answering and exploration benchmarks, yielding a 7.3% gain in LLM-Match and an 11.4% gain in LLM MatchXSPL on A-EQA, as well as +7.7% success rate and +6.8% SPL on GOAT-Bench. Analyses reveal that our episodic memory primarily improves exploration efficiency, while semantic memory strengthens complex reasoning of embodied agents.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u53c2\u6570\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u6027\u533a\u5206\u60c5\u8282\u8bb0\u5fc6\u548c\u8bed\u4e49\u8bb0\u5fc6\uff0c\u6539\u5584\u4e86\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5728\u957f\u671f\u89c2\u5bdf\u4e0b\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u63a8\u7406\u548c\u63a2\u7d22\u4efb\u52a1\u4e2d\u3002", "motivation": "\u5728\u957f\u671f\u89c2\u5bdf\u548c\u6709\u9650\u4e0a\u4e0b\u6587\u9884\u7b97\u7684\u60c5\u51b5\u4e0b\uff0c\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5177\u8eab\u4ee3\u7406\u7684\u5e94\u7528\u4ecd\u7136\u5b58\u5728\u6311\u6218\uff1b\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u53c2\u6570\u8bb0\u5fc6\u6846\u67b6\uff0c\u5229\u7528\u8bed\u4e49\u76f8\u4f3c\u6027\u53ec\u56de\u60c5\u8282\u7ecf\u9a8c\uff0c\u5e76\u901a\u8fc7\u89c6\u89c9\u63a8\u7406\u8fdb\u884c\u9a8c\u8bc1\uff1b\u540c\u65f6\u5f15\u5165\u4e86\u4e00\u79cd\u7a0b\u5e8f\u98ce\u683c\u7684\u89c4\u5219\u63d0\u53d6\u673a\u5236\uff0c\u5c06\u7ecf\u9a8c\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u7684\u53ef\u91cd\u7528\u8bed\u4e49\u8bb0\u5fc6\u3002", "result": "\u5728\u591a\u4e2a\u5177\u8eab\u95ee\u7b54\u548c\u63a2\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8868\u73b0\u51fa\u6700\u4f73\u6027\u80fd\uff0cLLM-Match\u63d0\u9ad8\u4e867.3%\uff0cLLM MatchXSPL\u63d0\u9ad8\u4e8611.4%\uff0cGOAT-Bench\u7684\u6210\u529f\u7387\u63d0\u9ad8\u4e867.7%\u3002", "conclusion": "\u6211\u4eec\u7684\u975e\u53c2\u6570\u8bb0\u5fc6\u6846\u67b6\u5728\u63a2\u7d22\u548c\u95ee\u7b54\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\uff0c\u6539\u8fdb\u4e86\u4ee3\u7406\u7684\u63a2\u7d22\u6548\u7387\u548c\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2602.15533", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15533", "abs": "https://arxiv.org/abs/2602.15533", "authors": ["Welf Rehberg", "Mihir Kulkarni", "Philipp Weiss", "Kostas Alexis"], "title": "Efficient Knowledge Transfer for Jump-Starting Control Policy Learning of Multirotors through Physics-Aware Neural Architectures", "comment": "8 pages. Accepted to IEEE Robotics and Automation Letters", "summary": "Efficiently training control policies for robots is a major challenge that can greatly benefit from utilizing knowledge gained from training similar systems through cross-embodiment knowledge transfer. In this work, we focus on accelerating policy training using a library-based initialization scheme that enables effective knowledge transfer across multirotor configurations. By leveraging a physics-aware neural control architecture that combines a reinforcement learning-based controller and a supervised control allocation network, we enable the reuse of previously trained policies. To this end, we utilize a policy evaluation-based similarity measure that identifies suitable policies for initialization from a library. We demonstrate that this measure correlates with the reduction in environment interactions needed to reach target performance and is therefore suited for initialization. Extensive simulation and real-world experiments confirm that our control architecture achieves state-of-the-art control performance, and that our initialization scheme saves on average up to $73.5\\%$ of environment interactions (compared to training a policy from scratch) across diverse quadrotor and hexarotor designs, paving the way for efficient cross-embodiment transfer in reinforcement learning.", "AI": {"tldr": "\u901a\u8fc7\u57fa\u4e8e\u5e93\u7684\u521d\u59cb\u5316\u65b9\u6848\u548c\u76f8\u4f3c\u6027\u5ea6\u91cf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u65cb\u7ffc\u653f\u7b56\u8bad\u7ec3\uff0c\u8282\u7701\u4e86\u5927\u91cf\u73af\u5883\u4ea4\u4e92\u3002", "motivation": "\u5229\u7528\u4ece\u8bad\u7ec3\u7c7b\u4f3c\u7cfb\u7edf\u4e2d\u83b7\u5f97\u7684\u77e5\u8bc6\uff0c\u901a\u8fc7\u8de8\u5f62\u6001\u77e5\u8bc6\u8f6c\u79fb\u52a0\u901f\u673a\u5668\u4eba\u63a7\u5236\u653f\u7b56\u7684\u8bad\u7ec3\u3002", "method": "\u4e00\u79cd\u57fa\u4e8e\u5e93\u7684\u521d\u59cb\u5316\u65b9\u6848\uff0c\u7ed3\u5408\u7269\u7406\u611f\u77e5\u795e\u7ecf\u63a7\u5236\u67b6\u6784\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5668\u548c\u76d1\u7763\u63a7\u5236\u5206\u914d\u7f51\u7edc\uff0c\u52a0\u901f\u591a\u65cb\u7ffc\u914d\u7f6e\u7684\u653f\u7b56\u8bad\u7ec3\u3002", "result": "\u901a\u8fc7\u4f7f\u7528\u57fa\u4e8e\u653f\u7b56\u8bc4\u4ef7\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf\uff0c\u8bc6\u522b\u9002\u5408\u521d\u59cb\u5316\u7684\u653f\u7b56\uff0c\u4ece\u800c\u91cd\u7528\u4ee5\u524d\u8bad\u7ec3\u7684\u653f\u7b56\u3002\u4eff\u771f\u5b9e\u9a8c\u548c\u73b0\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u63a7\u5236\u67b6\u6784\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u63a7\u5236\u6027\u80fd\uff0c\u5e73\u5747\u8282\u7701\u4e8673.5%\u7684\u73af\u5883\u4ea4\u4e92\u3002", "conclusion": "\u8be5\u521d\u59cb\u5316\u65b9\u6848\u6709\u6548\u51cf\u5c11\u4e86\u8bad\u7ec3\u6240\u9700\u7684\u73af\u5883\u4ea4\u4e92\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8de8\u5f62\u6001\u8fc1\u79fb\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.15684", "categories": ["cs.RO", "cs.AI", "cs.HC", "eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.15684", "abs": "https://arxiv.org/abs/2602.15684", "authors": ["Feras Kiki", "Pouya P. Niaz", "Alireza Madani", "Cagatay Basdogan"], "title": "Estimating Human Muscular Fatigue in Dynamic Collaborative Robotic Tasks with Learning-Based Models", "comment": "ICRA 2026 Original Contribution, Vienne, Austria", "summary": "Assessing human muscle fatigue is critical for optimizing performance and safety in physical human-robot interaction(pHRI). This work presents a data-driven framework to estimate fatigue in dynamic, cyclic pHRI using arm-mounted surface electromyography(sEMG). Subject-specific machine-learning regression models(Random Forest, XGBoost, and Linear Regression predict the fraction of cycles to fatigue(FCF) from three frequency-domain and one time-domain EMG features, and are benchmarked against a convolutional neural network(CNN) that ingests spectrograms of filtered EMG. Framing fatigue estimation as regression (rather than classification) captures continuous progression toward fatigue, supporting earlier detection, timely intervention, and adaptive robot control. In experiments with ten participants, a collaborative robot under admittance control guided repetitive lateral (left-right) end-effector motions until muscular fatigue. Average FCF RMSE across participants was 20.8+/-4.3% for the CNN, 23.3+/-3.8% for Random Forest, 24.8+/-4.5% for XGBoost, and 26.9+/-6.1% for Linear Regression. To probe cross-task generalization, one participant additionally performed unseen vertical (up-down) and circular repetitions; models trained only on lateral data were tested directly and largely retained accuracy, indicating robustness to changes in movement direction, arm kinematics, and muscle recruitment, while Linear Regression deteriorated. Overall, the study shows that both feature-based ML and spectrogram-based DL can estimate remaining work capacity during repetitive pHRI, with the CNN delivering the lowest error and the tree-based models close behind. The reported transfer to new motion patterns suggests potential for practical fatigue monitoring without retraining for every task, improving operator protection and enabling fatigue-aware shared autonomy, for safer fatigue-adaptive pHRI control.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8868\u9762\u808c\u7535\u56fe(sEMG)\u8bc4\u4f30\u4eba\u7c7b\u808c\u8089\u75b2\u52b3\uff0c\u65e8\u5728\u4f18\u5316\u8eab\u4f53\u4eba\u673a\u4ea4\u4e92(pHRI)\u4e2d\u7684\u6027\u80fd\u4e0e\u5b89\u5168\u6027\u3002", "motivation": "\u8bc4\u4f30\u808c\u8089\u75b2\u52b3\u5bf9\u4e8e\u4fdd\u969c\u4eba\u673a\u4ea4\u4e92\uff08pHRI\uff09\u7684\u5b89\u5168\u548c\u63d0\u5347\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e86\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u56de\u5f52\u6a21\u578b\uff08\u5982\u968f\u673a\u68ee\u6797\u3001XGBoost\u548c\u7ebf\u6027\u56de\u5f52\uff09\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\uff0c\u5206\u6790\u808c\u7535\u56fe\u7279\u5f81\u4ee5\u9884\u6d4b\u75b2\u52b3\u8fdb\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cCNN\u5b9e\u73b0\u4e86\u5e73\u574720.8%\u7684\u75b2\u52b3\u9884\u6d4b\u8bef\u5dee\uff0c\u4f18\u4e8e\u5176\u4ed6\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002\u5bf9\u4e8e\u672a\u89c1\u7684\u4efb\u52a1\uff0c\u6a21\u578b\u5728\u51c6\u786e\u6027\u4e0a\u4e5f\u663e\u793a\u51fa\u4e00\u5b9a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u4e0d\u540c\u7684\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u91cd\u590d\u6027\u4eba\u673a\u4ea4\u4e92\u8fc7\u7a0b\u4e2d\u7684\u5269\u4f59\u5de5\u4f5c\u80fd\u529b\uff0cCNN\u63d0\u4f9b\u4e86\u6700\u4f73\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5177\u6709\u5728\u4e0d\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u9002\u5e94\u65b0\u52a8\u4f5c\u6a21\u5f0f\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.15543", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15543", "abs": "https://arxiv.org/abs/2602.15543", "authors": ["Young-Chae Son", "Jung-Woo Lee", "Yoon-Ji Choi", "Dae-Kwan Ko", "Soo-Chul Lim"], "title": "Selective Perception for Robot: Task-Aware Attention in Multimodal VLA", "comment": null, "summary": "In robotics, Vision-Language-Action (VLA) models that integrate diverse multimodal signals from multi-view inputs have emerged as an effective approach. However, most prior work adopts static fusion that processes all visual inputs uniformly, which incurs unnecessary computational overhead and allows task-irrelevant background information to act as noise. Inspired by the principles of human active perception, we propose a dynamic information fusion framework designed to maximize the efficiency and robustness of VLA models. Our approach introduces a lightweight adaptive routing architecture that analyzes the current text prompt and observations from a wrist-mounted camera in real-time to predict the task-relevance of multiple camera views. By conditionally attenuating computations for views with low informational utility and selectively providing only essential visual features to the policy network, Our framework achieves computation efficiency proportional to task relevance. Furthermore, to efficiently secure large-scale annotation data for router training, we established an automated labeling pipeline utilizing Vision-Language Models (VLMs) to minimize data collection and annotation costs. Experimental results in real-world robotic manipulation scenarios demonstrate that the proposed approach achieves significant improvements in both inference efficiency and control performance compared to existing VLA models, validating the effectiveness and practicality of dynamic information fusion in resource-constrained, real-time robot control environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u52a8\u6001\u4fe1\u606f\u878d\u5408\u6846\u67b6\uff0c\u63d0\u5347VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u6548\u7387\u548c\u63a7\u5236\u6027\u80fd\u3002", "motivation": "\u65e8\u5728\u63d0\u9ad8\u73b0\u6709\u9759\u6001\u878d\u5408\u65b9\u6cd5\u5728\u591a\u89c6\u89d2\u8f93\u5165\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u51cf\u5c11\u4e0e\u4efb\u52a1\u65e0\u5173\u7684\u80cc\u666f\u4fe1\u606f\u566a\u58f0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u4fe1\u606f\u878d\u5408\u6846\u67b6\uff0c\u5229\u7528\u8f7b\u91cf\u81ea\u9002\u5e94\u8def\u7531\u67b6\u6784\u5b9e\u65f6\u5206\u6790\u6587\u672c\u63d0\u793a\u548c\u6765\u81ea\u624b\u8155\u5b89\u88c5\u6444\u50cf\u5934\u7684\u89c2\u5bdf\uff0c\u9884\u6d4b\u591a\u6444\u50cf\u673a\u89c6\u56fe\u7684\u4efb\u52a1\u76f8\u5173\u6027\u3002", "result": "\u5728\u771f\u5b9e\u673a\u5668\u4eba\u64cd\u4f5c\u573a\u666f\u7684\u5b9e\u9a8c\u7ed3\u679c\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u63a8\u7406\u6548\u7387\u548c\u63a7\u5236\u6027\u80fd\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684VLA\u6a21\u578b\u3002", "conclusion": "\u52a8\u6001\u4fe1\u606f\u878d\u5408\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5b9e\u65f6\u673a\u5668\u4eba\u63a7\u5236\u73af\u5883\u4e2d\u5177\u6709\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2602.15767", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.15767", "abs": "https://arxiv.org/abs/2602.15767", "authors": ["Atharva S Kashyap", "Ugne Aleksandra Morkute", "Patricia Alves-Oliveira"], "title": "Robot-Assisted Social Dining as a White Glove Service", "comment": "20 pages, 9 figures. Proceedings of the 2026 CHI Conference on Human Factors in Computing Systems (CHI '26)", "summary": "Robot-assisted feeding enables people with disabilities who require assistance eating to enjoy a meal independently and with dignity. However, existing systems have only been tested in-lab or in-home, leaving in-the-wild social dining contexts (e.g., restaurants) largely unexplored. Designing a robot for such contexts presents unique challenges, such as dynamic and unsupervised dining environments that a robot needs to account for and respond to. Through speculative participatory design with people with disabilities, supported by semi-structured interviews and a custom AI-based visual storyboarding tool, we uncovered ideal scenarios for in-the-wild social dining. Our key insight suggests that such systems should: embody the principles of a white glove service where the robot (1) supports multimodal inputs and unobtrusive outputs; (2) has contextually sensitive social behavior and prioritizes the user; (3) has expanded roles beyond feeding; (4) adapts to other relationships at the dining table. Our work has implications for in-the-wild and group contexts of robot-assisted feeding.", "AI": {"tldr": "\u7814\u7a76\u4e86\u673a\u5668\u4eba\u8f85\u52a9\u5582\u517b\u5728\u73b0\u5b9e\u793e\u4ea4\u996e\u98df\u573a\u666f\u4e2d\u7684\u8bbe\u8ba1\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u673a\u5668\u4eba\u5e94\u5177\u5907\u7684\u5173\u952e\u7279\u5f81\uff0c\u4ee5\u63d0\u5347\u7528\u6237\u7684\u72ec\u7acb\u6027\u548c\u5c31\u9910\u4f53\u9a8c\u3002", "motivation": "\u65e8\u5728\u5e2e\u52a9\u9700\u8981\u5582\u517b\u8f85\u52a9\u7684\u6b8b\u75be\u4eba\u58eb\u5728\u793e\u4ea4\u996e\u98df\u73af\u5883\u4e2d\u72ec\u7acb\u5c31\u9910\uff0c\u63d0\u5347\u4ed6\u4eec\u7684\u5c31\u9910\u4f53\u9a8c\u548c\u5c0a\u4e25\u3002", "method": "\u901a\u8fc7\u4e0e\u6b8b\u75be\u4eba\u58eb\u7684\u6295\u673a\u6027\u53c2\u4e0e\u8bbe\u8ba1\uff0c\u7ed3\u5408\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u548c\u81ea\u5b9a\u4e49\u57fa\u4e8eAI\u7684\u89c6\u89c9\u6545\u4e8b\u677f\u5de5\u5177\uff0c\u63a2\u7d22\u9002\u7528\u4e8e\u73b0\u5b9e\u793e\u4ea4\u996e\u98df\u573a\u666f\u7684\u8bbe\u8ba1\u7406\u5ff5\u3002", "result": "\u63ed\u793a\u4e86\u7406\u60f3\u7684\u5916\u90e8\u793e\u4ea4\u5c31\u9910\u573a\u666f\uff0c\u5e76\u63d0\u51fa\u673a\u5668\u4eba\u5e94\u5177\u5907\u7684\u7279\u5f81\uff0c\u5305\u62ec\u591a\u6a21\u6001\u8f93\u5165\u548c\u4f4e\u8c03\u8f93\u51fa\u3001\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u793e\u4ea4\u884c\u4e3a\u3001\u8d85\u8d8a\u5582\u98df\u7684\u89d2\u8272\u3001\u9002\u5e94\u9910\u684c\u4e0a\u7684\u4e0d\u540c\u5173\u7cfb\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u5728\u73b0\u5b9e\u4e16\u754c\u548c\u56e2\u4f53\u73af\u5883\u4e2d\u8fdb\u884c\u673a\u5668\u4eba\u8f85\u52a9\u5582\u517b\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u4e0e\u8bbe\u8ba1\u539f\u5219\u3002"}}
{"id": "2602.15549", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15549", "abs": "https://arxiv.org/abs/2602.15549", "authors": ["Guoqin Tang", "Qingxuan Jia", "Gang Chen", "Tong Li", "Zeyuan Huang", "Zihang Lv", "Ning Ji"], "title": "VLM-DEWM: Dynamic External World Model for Verifiable and Resilient Vision-Language Planning in Manufacturing", "comment": null, "summary": "Vision-language model (VLM) shows promise for high-level planning in smart manufacturing, yet their deployment in dynamic workcells faces two critical challenges: (1) stateless operation, they cannot persistently track out-of-view states, causing world-state drift; and (2) opaque reasoning, failures are difficult to diagnose, leading to costly blind retries. This paper presents VLM-DEWM, a cognitive architecture that decouples VLM reasoning from world-state management through a persistent, queryable Dynamic External World Model (DEWM). Each VLM decision is structured into an Externalizable Reasoning Trace (ERT), comprising action proposal, world belief, and causal assumption, which is validated against DEWM before execution. When failures occur, discrepancy analysis between predicted and observed states enables targeted recovery instead of global replanning. We evaluate VLM-DEWM on multi-station assembly, large-scale facility exploration, and real-robot recovery under induced failures. Compared to baseline memory-augmented VLM systems, VLM DEWM improves state-tracking accuracy from 56% to 93%, increases recovery success rate from below 5% to 95%, and significantly reduces computational overhead through structured memory. These results establish VLM-DEWM as a verifiable and resilient solution for long-horizon robotic operations in dynamic manufacturing environments.", "AI": {"tldr": "VLM-DEWM\u662f\u4e00\u79cd\u878d\u5408\u52a8\u6001\u5916\u90e8\u4e16\u754c\u6a21\u578b\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u67b6\u6784\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5728\u52a8\u6001\u5236\u9020\u73af\u5883\u4e2d\u7684\u51b3\u7b56\u80fd\u529b\u548c\u6545\u969c\u6062\u590d\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001\u5de5\u4f5c\u5355\u5143\u4e2d\u9762\u4e34\u65e0\u6cd5\u6301\u4e45\u8ddf\u8e2a\u72b6\u6001\u548c\u63a8\u7406\u4e0d\u900f\u660e\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u52a8\u6001\u5916\u90e8\u4e16\u754c\u6a21\u578b\uff08DEWM\uff09\u89e3\u8026VLM\u63a8\u7406\u4e0e\u4e16\u754c\u72b6\u6001\u7ba1\u7406\u7684\u8ba4\u77e5\u67b6\u6784\uff0c\u6bcf\u4e2a\u51b3\u7b56\u7684\u5916\u90e8\u5316\u63a8\u7406\u8ffd\u8e2a\uff08ERT\uff09\u7ed3\u6784\u5316\u4e3a\u884c\u52a8\u63d0\u6848\u3001\u4e16\u754c\u4fe1\u5ff5\u548c\u56e0\u679c\u5047\u8bbe\u3002", "result": "\u4e0e\u57fa\u7ebf\u5185\u5b58\u589e\u5f3aVLM\u7cfb\u7edf\u76f8\u6bd4\uff0cVLM-DEWM\u5728\u72b6\u6001\u8ffd\u8e2a\u51c6\u786e\u6027\u4e0a\u63d0\u9ad8\u81f393%\uff0c\u6545\u969c\u6062\u590d\u6210\u529f\u7387\u63d0\u9ad8\u81f395%\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "VLM-DEWM\u88ab\u786e\u7acb\u4e3a\u52a8\u6001\u5236\u9020\u73af\u5883\u4e2d\u957f\u65f6\u95f4\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u53ef\u9a8c\u8bc1\u548c\u97e7\u6027\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.15567", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15567", "abs": "https://arxiv.org/abs/2602.15567", "authors": ["Jieting Long", "Dechuan Liu", "Weidong Cai", "Ian Manchester", "Weiming Zhi"], "title": "Constraining Streaming Flow Models for Adapting Learned Robot Trajectory Distributions", "comment": "8 pages, 8 figure", "summary": "Robot motion distributions often exhibit multi-modality and require flexible generative models for accurate representation. Streaming Flow Policies (SFPs) have recently emerged as a powerful paradigm for generating robot trajectories by integrating learned velocity fields directly in action space, enabling smooth and reactive control. However, existing formulations lack mechanisms for adapting trajectories post-training to enforce safety and task-specific constraints. We propose Constraint-Aware Streaming Flow (CASF), a framework that augments streaming flow policies with constraint-dependent metrics that reshape the learned velocity field during execution. CASF models each constraint, defined in either the robot's workspace or configuration space, as a differentiable distance function that is converted into a local metric and pulled back into the robot's control space. Far from restricted regions, the resulting metric reduces to the identity; near constraint boundaries, it smoothly attenuates or redirects motion, effectively deforming the underlying flow to maintain safety. This allows trajectories to be adapted in real time, ensuring that robot actions respect joint limits, avoid collisions, and remain within feasible workspaces, while preserving the multi-modal and reactive properties of streaming flow policies. We demonstrate CASF in simulated and real-world manipulation tasks, showing that it produces constraint-satisfying trajectories that remain smooth, feasible, and dynamically consistent, outperforming standard post-hoc projection baselines.", "AI": {"tldr": "CASF\u6846\u67b6\u7ed3\u5408\u7ea6\u675f\u76f8\u5173\u5ea6\u91cf\uff0c\u589e\u5f3a\u6d41\u52a8\u7b56\u7565\u7684\u9002\u5e94\u80fd\u529b\uff0c\u786e\u4fdd\u673a\u5668\u4eba\u5728\u6267\u884c\u8fc7\u7a0b\u4e2d\u9075\u5b88\u5b89\u5168\u4e0e\u4efb\u52a1\u7279\u5b9a\u7ea6\u675f\u3002", "motivation": "\u673a\u5668\u4eba\u8fd0\u52a8\u5206\u5e03\u901a\u5e38\u8868\u73b0\u51fa\u591a\u6a21\u6001\u7279\u6027\uff0c\u9700\u8981\u7075\u6d3b\u7684\u751f\u6210\u6a21\u578b\u4ee5\u8fdb\u884c\u51c6\u786e\u7684\u8868\u793a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u6d41\u52a8\u7b56\u7565\u7684\u6846\u67b6\uff08CASF\uff09\uff0c\u5229\u7528\u7ea6\u675f\u76f8\u5173\u7684\u5ea6\u91cf\u5728\u6267\u884c\u8fc7\u7a0b\u4e2d\u91cd\u5851\u5b66\u4e60\u5230\u7684\u901f\u5ea6\u573a\u3002", "result": "CASF\u5141\u8bb8\u5b9e\u65f6\u65f6\u95f4\u9002\u5e94\u8f68\u8ff9\uff0c\u786e\u4fdd\u673a\u5668\u4eba\u52a8\u4f5c\u9075\u5b88\u5173\u8282\u9650\u5236\u3001\u907f\u514d\u78b0\u649e\uff0c\u5e76\u4fdd\u6301\u5728\u53ef\u884c\u7684\u5de5\u4f5c\u7a7a\u95f4\u5185\uff0c\u540c\u65f6\u4fdd\u6301\u6d41\u52a8\u7b56\u7565\u7684\u591a\u6a21\u6001\u6027\u548c\u53cd\u5e94\u6027\u3002", "conclusion": "CASF\u80fd\u591f\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u7684\u64cd\u63a7\u4efb\u52a1\u4e2d\u4ea7\u751f\u6ee1\u8db3\u7ea6\u675f\u7684\u8f68\u8ff9\uff0c\u4e14\u8fd9\u4e9b\u8f68\u8ff9\u5e73\u6ed1\u3001\u53ef\u884c\u4e14\u52a8\u6001\u4e00\u81f4\uff0c\u4f18\u4e8e\u6807\u51c6\u7684\u540e\u5904\u7406\u6295\u5f71\u57fa\u7ebf\u3002"}}
{"id": "2602.15608", "categories": ["cs.RO", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2602.15608", "abs": "https://arxiv.org/abs/2602.15608", "authors": ["Mostafa A. Atalla", "Daan van Bemmel", "Jack Cummings", "Paul Breedveld", "Micha\u00ebl Wiertlewski", "Aim\u00e9e Sakes"], "title": "Grip as Needed, Glide on Demand: Ultrasonic Lubrication for Robotic Locomotion", "comment": "Accepted for publication in the 2026 IEEE International Conference on Robotics and Automation (ICRA) in Vienna", "summary": "Friction is the essential mediator of terrestrial locomotion, yet in robotic systems it is almost always treated as a passive property fixed by surface materials and conditions. Here, we introduce ultrasonic lubrication as a method to actively control friction in robotic locomotion. By exciting resonant structures at ultrasonic frequencies, contact interfaces can dynamically switch between \"grip\" and \"slip\" states, enabling locomotion. We developed two friction control modules, a cylindrical design for lumen-like environments and a flat-plate design for external surfaces, and integrated them into bio-inspired systems modeled after inchworm and wasp ovipositor locomotion. Both systems achieved bidirectional locomotion with nearly perfect locomotion efficiencies that exceeded 90%. Friction characterization experiments further demonstrated substantial friction reduction across various surfaces, including rigid, soft, granular, and biological tissue interfaces, under dry and wet conditions, and on surfaces with different levels of roughness, confirming the broad applicability of ultrasonic lubrication to locomotion tasks. These findings establish ultrasonic lubrication as a viable active friction control mechanism for robotic locomotion, with the potential to reduce design complexity and improve efficiency of robotic locomotion systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u578b\u8d85\u58f0\u6ce2\u6da6\u6ed1\u65b9\u6cd5\uff0c\u53ef\u4e3b\u52a8\u63a7\u5236\u673a\u5668\u4eba\u8fd0\u52a8\u4e2d\u7684\u6469\u64e6\u529b\uff0c\u5c55\u793a\u4e86\u5728\u591a\u79cd\u8868\u9762\u6761\u4ef6\u4e0b\u4f18\u79c0\u7684\u8fd0\u52a8\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u4e0a\uff0c\u673a\u5668\u4eba\u7684\u6469\u64e6\u529b\u88ab\u89c6\u4e3a\u56fa\u5b9a\u7684\uff0c\u88ab\u8868\u9762\u6750\u6599\u548c\u73af\u5883\u6761\u4ef6\u51b3\u5b9a\u3002\u672c\u7814\u7a76\u65e8\u5728\u53d1\u5c55\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u4e3b\u52a8\u8c03\u63a7\u6469\u64e6\uff0c\u4ee5\u63d0\u9ad8\u673a\u68b0\u7cfb\u7edf\u7684\u8fd0\u52a8\u80fd\u529b\u548c\u6548\u7387\u3002", "method": "\u901a\u8fc7\u6fc0\u52b1\u8d85\u58f0\u6ce2\u9891\u7387\u7684\u5171\u632f\u7ed3\u6784\uff0c\u5b9e\u73b0\u5728\u4e0d\u540c\u8868\u9762\u6761\u4ef6\u4e0b\u7684\u6469\u64e6\u72b6\u6001\u5207\u6362\uff0c\u8bbe\u8ba1\u5e76\u6d4b\u8bd5\u4e86\u4e24\u79cd\u6469\u64e6\u63a7\u5236\u6a21\u5757\uff0c\u5e76\u6574\u5408\u5230\u4eff\u751f\u7cfb\u7edf\u4e2d\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8d85\u58f0\u6ce2\u6da6\u6ed1\u6280\u672f\uff0c\u80fd\u5b9e\u65f6\u63a7\u5236\u673a\u5668\u4eba\u8fd0\u52a8\u4e2d\u7684\u6469\u64e6\u529b\uff0c\u4ece\u800c\u63d0\u9ad8\u8fd0\u52a8\u6548\u7387\u548c\u8bbe\u8ba1\u7075\u6d3b\u6027\u3002", "conclusion": "\u8d85\u58f0\u6ce2\u6da6\u6ed1\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u6709\u6548\u7684\u4e3b\u52a8\u6469\u64e6\u63a7\u5236\u673a\u5236\uff0c\u5e7f\u6cdb\u9002\u7528\u4e8e\u4e0d\u540c\u8868\u9762\uff0c\u663e\u8457\u964d\u4f4e\u6469\u64e6\u529b\u5e76\u5b9e\u73b0\u9ad8\u6548\u7684\u53cc\u5411\u8fd0\u52a8\u3002"}}
{"id": "2602.15633", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15633", "abs": "https://arxiv.org/abs/2602.15633", "authors": ["Haichao Liu", "Yufeng Hu", "Shuang Wang", "Kangjun Guo", "Jun Ma", "Jinni Zhou"], "title": "SpecFuse: A Spectral-Temporal Fusion Predictive Control Framework for UAV Landing on Oscillating Marine Platforms", "comment": "8 pages, 5 figures, 4 tables", "summary": "Autonomous landing of Uncrewed Aerial Vehicles (UAVs) on oscillating marine platforms is severely constrained by wave-induced multi-frequency oscillations, wind disturbances, and prediction phase lags in motion prediction. Existing methods either treat platform motion as a general random process or lack explicit modeling of wave spectral characteristics, leading to suboptimal performance under dynamic sea conditions. To address these limitations, we propose SpecFuse: a novel spectral-temporal fusion predictive control framework that integrates frequency-domain wave decomposition with time-domain recursive state estimation for high-precision 6-DoF motion forecasting of Uncrewed Surface Vehicles (USVs). The framework explicitly models dominant wave harmonics to mitigate phase lags, refining predictions in real time via IMU data without relying on complex calibration. Additionally, we design a hierarchical control architecture featuring a sampling-based HPO-RRT* algorithm for dynamic trajectory planning under non-convex constraints and a learning-augmented predictive controller that fuses data-driven disturbance compensation with optimization-based execution. Extensive validations (2,000 simulations + 8 lake experiments) show our approach achieves a 3.2 cm prediction error, 4.46 cm landing deviation, 98.7% / 87.5% success rates (simulation / real-world), and 82 ms latency on embedded hardware, outperforming state-of-the-art methods by 44%-48% in accuracy. Its robustness to wave-wind coupling disturbances supports critical maritime missions such as search and rescue and environmental monitoring. All code, experimental configurations, and datasets will be released as open-source to facilitate reproducibility.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u63a7\u5236\u6846\u67b6SpecFuse\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u5728\u6ce2\u52a8\u6d77\u6d0b\u5e73\u53f0\u4e0a\u7684\u81ea\u4e3b\u7740\u9646\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8fd0\u52a8\u9884\u6d4b\u7cbe\u5ea6\u548c\u6210\u529f\u7387\u3002", "motivation": "\u65e0\u4eba\u673a\u5728\u6ce2\u52a8\u7684\u6d77\u6d0b\u5e73\u53f0\u4e0a\u81ea\u4e3b\u7740\u9646\u9762\u4e34\u591a\u9891\u632f\u8361\u3001\u98ce\u5e72\u6270\u548c\u8fd0\u52a8\u9884\u6d4b\u4e2d\u7684\u9884\u6d4b\u76f8\u4f4d\u5ef6\u8fdf\u7b49\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSpecFuse\u7684\u5149\u8c31-\u65f6\u95f4\u878d\u5408\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u9891\u57df\u6ce2\u5206\u89e3\u548c\u65f6\u57df\u9012\u5f52\u72b6\u6001\u4f30\u8ba1\uff0c\u4ee5\u5b9e\u73b0\u65e0\u8f7d\u4eba\u6c34\u9762\u8f66\u8f86\u7684\u9ad8\u7cbe\u5ea66\u81ea\u7531\u5ea6\u8fd0\u52a8\u9884\u6d4b\u3002", "result": "\u901a\u8fc72,000\u6b21\u6a21\u62df\u548c8\u6b21\u6e56\u6cca\u5b9e\u9a8c\uff0c\u65b9\u6cd5\u5b9e\u73b0\u4e863.2\u5398\u7c73\u7684\u9884\u6d4b\u8bef\u5dee\uff0c4.46\u5398\u7c73\u7684\u7740\u9646\u504f\u5dee\uff0c\u4ee5\u53ca\u5728\u6a21\u62df/\u73b0\u5b9e\u4e16\u754c\u4e2d98.7%/87.5%\u7684\u6210\u529f\u7387\uff0c\u5d4c\u5165\u5f0f\u786c\u4ef6\u5ef6\u8fdf\u4e3a82\u6beb\u79d2\uff0c\u51c6\u786e\u6027\u6bd4\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e8644%-48%\u3002", "conclusion": "\u8be5\u6846\u67b6\u7684\u9c81\u68d2\u6027\u652f\u6301\u5173\u952e\u6d77\u4e0a\u4efb\u52a1\uff0c\u5982\u641c\u6551\u548c\u73af\u5883\u76d1\u6d4b\uff0c\u6240\u6709\u4ee3\u7801\u3001\u5b9e\u9a8c\u914d\u7f6e\u548c\u6570\u636e\u96c6\u5c06\u4f5c\u4e3a\u5f00\u6e90\u53d1\u5e03\uff0c\u4ee5\u4fc3\u8fdb\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2602.15642", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15642", "abs": "https://arxiv.org/abs/2602.15642", "authors": ["Alexander Wachter", "Alexander Willert", "Marc-Philip Ecker", "Christian Hartl-Nesic"], "title": "Spatially-Aware Adaptive Trajectory Optimization with Controller-Guided Feedback for Autonomous Racing", "comment": "Accepted at ICRA 2026", "summary": "We present a closed-loop framework for autonomous raceline optimization that combines NURBS-based trajectory representation, CMA-ES global trajectory optimization, and controller-guided spatial feedback. Instead of treating tracking errors as transient disturbances, our method exploits them as informative signals of local track characteristics via a Kalman-inspired spatial update. This enables the construction of an adaptive, acceleration-based constraint map that iteratively refines trajectories toward near-optimal performance under spatially varying track and vehicle behavior. In simulation, our approach achieves a 17.38% lap time reduction compared to a controller parametrized with maximum static acceleration. On real hardware, tested with different tire compounds ranging from high to low friction, we obtain a 7.60% lap time improvement without explicitly parametrizing friction. This demonstrates robustness to changing grip conditions in real-world scenarios.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u4e2a\u4f18\u5316\u6280\u672f\u7684\u95ed\u73af\u6846\u67b6\uff0c\u6210\u529f\u5730\u4f18\u5316\u4e86\u5728\u4e0d\u540c\u6293\u5730\u529b\u6761\u4ef6\u4e0b\u7684\u81ea\u52a8\u8d5b\u8f66\u8f68\u8ff9\u3002", "motivation": "\u901a\u8fc7\u5229\u7528\u8ddf\u8e2a\u8bef\u5dee\u4f5c\u4e3a\u8f68\u9053\u7279\u5f81\u7684\u4fe1\u53f7\uff0c\u6784\u5efa\u81ea\u9002\u5e94\u7684\u52a0\u901f\u5ea6\u7ea6\u675f\u56fe\uff0c\u4ee5\u9010\u6b65\u4f18\u5316\u8f68\u8ff9\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u95ed\u73af\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u57fa\u4e8eNURBS\u7684\u8f68\u8ff9\u8868\u793a\u3001CMA-ES\u5168\u5c40\u8f68\u8ff9\u4f18\u5316\u548c\u57fa\u4e8e\u63a7\u5236\u5668\u7684\u7a7a\u95f4\u53cd\u9988\u3002", "result": "\u5728\u6a21\u62df\u4e2d\uff0c\u5b9e\u73b0\u4e86\u4e0e\u6700\u5927\u9759\u6001\u52a0\u901f\u5ea6\u53c2\u6570\u5316\u63a7\u5236\u5668\u76f8\u6bd4\uff0c17.38%\u7684\u5708\u901f\u51cf\u5c11\uff0c\u771f\u5b9e\u786c\u4ef6\u6d4b\u8bd5\u4e2d\u5728\u5404\u79cd\u8f6e\u80ce\u590d\u5408\u6750\u6599\u4e0b\u83b7\u5f97\u4e867.60%\u7684\u5708\u901f\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u7684\u6293\u5730\u529b\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u4e86\u7a33\u5065\u6027\uff0c\u5e76\u5728\u771f\u5b9e\u786c\u4ef6\u6d4b\u8bd5\u4e2d\u63d0\u9ad8\u4e867.60%\u7684\u5708\u901f\u3002"}}
{"id": "2602.15721", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15721", "abs": "https://arxiv.org/abs/2602.15721", "authors": ["Jingtian Yan", "Yulun Zhang", "Zhenting Liu", "Han Zhang", "He Jiang", "Jingkai Chen", "Stephen F. Smith", "Jiaoyang Li"], "title": "Lifelong Scalable Multi-Agent Realistic Testbed and A Comprehensive Study on Design Choices in Lifelong AGV Fleet Management Systems", "comment": null, "summary": "We present Lifelong Scalable Multi-Agent Realistic Testbed (LSMART), an open-source simulator to evaluate any Multi-Agent Path Finding (MAPF) algorithm in a Fleet Management System (FMS) with Automated Guided Vehicles (AGVs). MAPF aims to move a group of agents from their corresponding starting locations to their goals. Lifelong MAPF (LMAPF) is a variant of MAPF that continuously assigns new goals for agents to reach. LMAPF applications, such as autonomous warehouses, often require a centralized, lifelong system to coordinate the movement of a fleet of robots, typically AGVs. However, existing works on MAPF and LMAPF often assume simplified kinodynamic models, such as pebble motion, as well as perfect execution and communication for AGVs. Prior work has presented SMART, a software capable of evaluating any MAPF algorithms while considering agent kinodynamics, communication delays, and execution uncertainties. However, SMART is designed for MAPF, not LMAPF. Generalizing SMART to an FMS requires many more design choices. First, an FMS parallelizes planning and execution, raising the question of when to plan. Second, given planners with varying optimality and differing agent-model assumptions, one must decide how to plan. Third, when the planner fails to return valid solutions, the system must determine how to recover. In this paper, we first present LSMART, an open-source simulator that incorporates all these considerations to evaluate any MAPF algorithms in an FMS. We then provide experiment results based on state-of-the-art methods for each design choice, offering guidance on how to effectively design centralized lifelong AGV Fleet Management Systems. LSMART is available at https://smart-mapf.github.io/lifelong-smart.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86 LSMART\uff0c\u4e00\u79cd\u5f00\u653e\u6e90\u4ee3\u7801\u6a21\u62df\u5668\uff0c\u7528\u4e8e\u5728\u8230\u961f\u7ba1\u7406\u7cfb\u7edf\u4e2d\u8bc4\u4f30\u591a\u667a\u80fd\u4f53\u8def\u5f84\u5bfb\u627e\u7b97\u6cd5\uff0c\u8003\u8651\u4e86\u52a8\u6001\u89c4\u5212\u3001\u89c4\u5212\u65f6\u673a\u548c\u6062\u590d\u7b56\u7565\u7b49\u590d\u6742\u56e0\u7d20\u3002", "motivation": "\u53d7\u76ca\u4e8e\u81ea\u52a8\u5316\u4ed3\u5e93\u7b49\u5e94\u7528\u7684\u9700\u6c42\uff0c\u672c\u6587\u65e8\u5728\u6784\u5efa\u4e00\u4e2a\u80fd\u591f\u652f\u6301\u7075\u6d3b\u76ee\u6807\u5206\u914d\u548c\u6301\u7eed\u7ba1\u7406\u7684\u591a\u667a\u80fd\u4f53\u8def\u5f84\u5bfb\u627e\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa LSMART \u6a21\u62df\u5668\uff0c\u901a\u8fc7\u8003\u8651\u591a\u4e2a\u8bbe\u8ba1\u9009\u62e9\uff0c\u5305\u62ec\u89c4\u5212\u548c\u6267\u884c\u7684\u5e76\u884c\u5316\u3001\u89c4\u5212\u65b9\u6cd5\u7684\u9009\u62e9\u4ee5\u53ca\u5728\u5931\u8d25\u65f6\u7684\u6062\u590d\u7b56\u7565\uff0c\u6765\u8bc4\u4f30\u591a\u667a\u80fd\u4f53\u8def\u5f84\u5bfb\u627e\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u57fa\u4e8e\u6700\u65b0\u7684\u65b9\u6cd5\uff0c\u4e3a\u8bbe\u8ba1\u4e2d\u5fc3\u5316\u7684\u7ec8\u8eab AGV \u8230\u961f\u7ba1\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6307\u5bfc\u3002", "conclusion": "LSMART \u63d0\u4f9b\u4e86\u4e00\u4e2a\u5305\u542b\u590d\u6742\u8bbe\u8ba1\u9009\u62e9\u7684\u5f00\u653e\u6e90\u4ee3\u7801\u6a21\u62df\u5668\uff0c\u4ee5\u6709\u6548\u8bc4\u4f30\u591a\u667a\u80fd\u4f53\u8def\u5f84\u5bfb\u627e\u7b97\u6cd5\u5728\u81ea\u52a8\u5316\u5f15\u5bfc\u8f66\u8f86\u8230\u961f\u7ba1\u7406\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2602.15733", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15733", "abs": "https://arxiv.org/abs/2602.15733", "authors": ["Qiang Zhang", "Jiahao Ma", "Peiran Liu", "Shuai Shi", "Zeran Su", "Zifan Wang", "Jingkai Sun", "Wei Cui", "Jialin Yu", "Gang Han", "Wen Zhao", "Pihai Sun", "Kangning Yin", "Jiaxu Wang", "Jiahang Cao", "Lingfeng Zhang", "Hao Cheng", "Xiaoshuai Hao", "Yiding Ji", "Junwei Liang", "Jian Tang", "Renjing Xu", "Yijie Guo"], "title": "MeshMimic: Geometry-Aware Humanoid Motion Learning through 3D Scene Reconstruction", "comment": "17 pages, 6 figures", "summary": "Humanoid motion control has witnessed significant breakthroughs in recent years, with deep reinforcement learning (RL) emerging as a primary catalyst for achieving complex, human-like behaviors. However, the high dimensionality and intricate dynamics of humanoid robots make manual motion design impractical, leading to a heavy reliance on expensive motion capture (MoCap) data. These datasets are not only costly to acquire but also frequently lack the necessary geometric context of the surrounding physical environment. Consequently, existing motion synthesis frameworks often suffer from a decoupling of motion and scene, resulting in physical inconsistencies such as contact slippage or mesh penetration during terrain-aware tasks. In this work, we present MeshMimic, an innovative framework that bridges 3D scene reconstruction and embodied intelligence to enable humanoid robots to learn coupled \"motion-terrain\" interactions directly from video. By leveraging state-of-the-art 3D vision models, our framework precisely segments and reconstructs both human trajectories and the underlying 3D geometry of terrains and objects. We introduce an optimization algorithm based on kinematic consistency to extract high-quality motion data from noisy visual reconstructions, alongside a contact-invariant retargeting method that transfers human-environment interaction features to the humanoid agent. Experimental results demonstrate that MeshMimic achieves robust, highly dynamic performance across diverse and challenging terrains. Our approach proves that a low-cost pipeline utilizing only consumer-grade monocular sensors can facilitate the training of complex physical interactions, offering a scalable path toward the autonomous evolution of humanoid robots in unstructured environments.", "AI": {"tldr": "MeshMimic\u662f\u4e00\u4e2a\u521b\u65b0\u6846\u67b6\uff0c\u5229\u7528\u89c6\u9891\u5b66\u4e60\u4eba\u5f62\u673a\u5668\u4eba\u4e0e\u5730\u5f62\u7684\u8026\u5408\u4ea4\u4e92\uff0c\u53d6\u5f97\u4e86\u5728\u591a\u6837\u590d\u6742\u5730\u5f62\u4e0a\u53ef\u9760\u7684\u8868\u73b0\uff0c\u4e14\u53ef\u901a\u8fc7\u4f4e\u6210\u672c\u7ba1\u9053\u5b9e\u73b0\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u8bbe\u8ba1\u7684\u5c40\u9650\u6027\uff0c\u514b\u670d\u5bf9\u6602\u8d35\u8fd0\u52a8\u6355\u6349\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u4ee5\u53ca\u8fd0\u52a8\u4e0e\u573a\u666f\u4e4b\u95f4\u7684\u7269\u7406\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7ed3\u54083D\u573a\u666f\u91cd\u5efa\u548c\u4f53\u73b0\u667a\u80fd\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u6700\u5148\u8fdb\u76843D\u89c6\u89c9\u6a21\u578b\u6765\u5206\u6790\u4eba\u7c7b\u8f68\u8ff9\u548c\u5730\u5f62\u51e0\u4f55\u7ed3\u6784\uff0c\u5e76\u5e94\u7528\u57fa\u4e8e\u8fd0\u52a8\u5b66\u4e00\u81f4\u6027\u7684\u4f18\u5316\u7b97\u6cd5\u548c\u63a5\u89e6\u4e0d\u53d8\u7684\u91cd\u5b9a\u5411\u65b9\u6cd5\u63d0\u53d6\u9ad8\u8d28\u91cf\u8fd0\u52a8\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eMeshMimic\u5728\u591a\u79cd\u590d\u6742\u5730\u5f62\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u52a8\u6001\u6027\u80fd\u3002", "conclusion": "MeshMimic\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u7684\u8bad\u7ec3\u65b9\u6848\uff0c\u53ef\u4ee5\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u4fc3\u8fdb\u4eba\u5f62\u673a\u5668\u4eba\u590d\u6742\u7269\u7406\u4ea4\u4e92\u7684\u81ea\u4e3b\u6f14\u5316\u3002"}}
{"id": "2602.15813", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15813", "abs": "https://arxiv.org/abs/2602.15813", "authors": ["Haochen Zhang", "Nirav Savaliya", "Faizan Siddiqui", "Enna Sachdeva"], "title": "FAST-EQA: Efficient Embodied Question Answering with Global and Local Region Relevancy", "comment": "WACV 2026", "summary": "Embodied Question Answering (EQA) combines visual scene understanding, goal-directed exploration, spatial and temporal reasoning under partial observability. A central challenge is to confine physical search to question-relevant subspaces while maintaining a compact, actionable memory of observations. Furthermore, for real-world deployment, fast inference time during exploration is crucial. We introduce FAST-EQA, a question-conditioned framework that (i) identifies likely visual targets, (ii) scores global regions of interest to guide navigation, and (iii) employs Chain-of-Thought (CoT) reasoning over visual memory to answer confidently. FAST-EQA maintains a bounded scene memory that stores a fixed-capacity set of region-target hypotheses and updates them online, enabling robust handling of both single and multi-target questions without unbounded growth. To expand coverage efficiently, a global exploration policy treats narrow openings and doors as high-value frontiers, complementing local target seeking with minimal computation. Together, these components focus the agent's attention, improve scene coverage, and improve answer reliability while running substantially faster than prior approaches. On HMEQA and EXPRESS-Bench, FAST-EQA achieves state-of-the-art performance, while performing competitively on OpenEQA and MT-HM3D.", "AI": {"tldr": "FAST-EQA\u662f\u4e00\u79cd\u4f18\u5316\u7684Embodied Question Answering\u6846\u67b6\uff0c\u901a\u8fc7\u95ee\u9898\u5f15\u5bfc\u7684\u89c6\u89c9\u76ee\u6807\u8bc6\u522b\u548c\u573a\u666f\u8bb0\u5fc6\u7ba1\u7406\uff0c\u63d0\u5347\u63a2\u7d22\u6548\u7387\u548c\u56de\u7b54\u53ef\u9760\u6027\u3002", "motivation": "\u5728\u771f\u5b9e\u573a\u666f\u4e2d\uff0c\u5feb\u901f\u63a8\u7406\u548c\u6709\u6548\u7684\u641c\u7d22\u7b56\u7565\u662fEmbodied Question Answering\u7684\u6838\u5fc3\u6311\u6218\u3002", "method": "FAST-EQA\u6846\u67b6\u5177\u5907\u76ee\u6807\u8bc6\u522b\u3001\u5174\u8da3\u533a\u57df\u8bc4\u5206\u4e0eChain-of-Thought\u63a8\u7406\uff0c\u7ef4\u62a4\u6709\u9650\u7684\u573a\u666f\u8bb0\u5fc6\u4ee5\u5e94\u5bf9\u591a\u76ee\u6807\u95ee\u9898\u3002", "result": "\u5728HMEQA\u548cEXPRESS-Bench\u4e0a\uff0cFAST-EQA\u8868\u73b0\u4f18\u8d8a\uff0c\u4e14\u5728OpenEQA\u548cMT-HM3D\u4e0a\u4e5f\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u6ce8\u610f\u529b\u5f15\u5bfc\u548c\u63a2\u7d22\u7b56\u7565\uff0cFAST-EQA\u5728\u901f\u5ea6\u548c\u56de\u7b54\u8d28\u91cf\u4e0a\u5747\u8d85\u8d8a\u4e86\u4ee5\u5f80\u65b9\u6cd5\u3002"}}
{"id": "2602.15827", "categories": ["cs.RO", "cs.AI", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.15827", "abs": "https://arxiv.org/abs/2602.15827", "authors": ["Zhen Wu", "Xiaoyu Huang", "Lujie Yang", "Yuanhang Zhang", "Koushil Sreenath", "Xi Chen", "Pieter Abbeel", "Rocky Duan", "Angjoo Kanazawa", "Carmelo Sferrazza", "Guanya Shi", "C. Karen Liu"], "title": "Perceptive Humanoid Parkour: Chaining Dynamic Human Skills via Motion Matching", "comment": null, "summary": "While recent advances in humanoid locomotion have achieved stable walking on varied terrains, capturing the agility and adaptivity of highly dynamic human motions remains an open challenge. In particular, agile parkour in complex environments demands not only low-level robustness, but also human-like motion expressiveness, long-horizon skill composition, and perception-driven decision-making. In this paper, we present Perceptive Humanoid Parkour (PHP), a modular framework that enables humanoid robots to autonomously perform long-horizon, vision-based parkour across challenging obstacle courses. Our approach first leverages motion matching, formulated as nearest-neighbor search in a feature space, to compose retargeted atomic human skills into long-horizon kinematic trajectories. This framework enables the flexible composition and smooth transition of complex skill chains while preserving the elegance and fluidity of dynamic human motions. Next, we train motion-tracking reinforcement learning (RL) expert policies for these composed motions, and distill them into a single depth-based, multi-skill student policy, using a combination of DAgger and RL. Crucially, the combination of perception and skill composition enables autonomous, context-aware decision-making: using only onboard depth sensing and a discrete 2D velocity command, the robot selects and executes whether to step over, climb onto, vault or roll off obstacles of varying geometries and heights. We validate our framework with extensive real-world experiments on a Unitree G1 humanoid robot, demonstrating highly dynamic parkour skills such as climbing tall obstacles up to 1.25m (96% robot height), as well as long-horizon multi-obstacle traversal with closed-loop adaptation to real-time obstacle perturbations.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u611f\u77e5\u4eba\u5f62\u8dd1\u9177\uff08PHP\uff09\u6846\u67b6\uff0c\u4f7f\u4eba\u5f62\u673a\u5668\u4eba\u80fd\u591f\u81ea\u4e3b\u6267\u884c\u57fa\u4e8e\u89c6\u89c9\u7684\u957f\u8ddd\u79bb\u8dd1\u9177\uff0c\u5e76\u5728\u590d\u6742\u969c\u788d\u7269\u8bfe\u7a0b\u4e2d\u5c55\u73b0\u4eba\u7c7b\u822c\u7684\u52a8\u6001\u52a8\u4f5c\u548c\u51b3\u7b56\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u8fd1\u5e74\u6765\u5728\u7c7b\u4eba locomotion \u9886\u57df\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u6a21\u4eff\u4eba\u7c7b\u9ad8\u5ea6\u52a8\u6001\u7684\u8fd0\u52a8\u8868\u73b0\u4ecd\u7136\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u7075\u6d3b\u8dd1\u9177\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528\u8fd0\u52a8\u5339\u914d\u6280\u672f\uff0c\u901a\u8fc7\u6700\u8fd1\u90bb\u641c\u7d22\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u7ec4\u5408\u91cd\u65b0\u5b9a\u4f4d\u7684\u4eba\u7c7b\u6280\u80fd\u6210\u957f\u671f\u8fd0\u52a8\u8f68\u8ff9\uff0c\u5e76\u4f7f\u7528\u8fd0\u52a8\u8ddf\u8e2a\u5f3a\u5316\u5b66\u4e60\u4e13\u5bb6\u653f\u7b56\u8fdb\u884c\u7b56\u7565\u63d0\u70bc\uff0c\u6700\u7ec8\u5f62\u6210\u4e00\u4e2a\u5355\u4e00\u7684\u3001\u591a\u6280\u80fd\u7684\u5b66\u751f\u7b56\u7565\u3002", "result": "\u6211\u4eec\u7684\u6846\u67b6\u901a\u8fc7\u5bf9Unitree G1\u7c7b\u4eba\u673a\u5668\u4eba\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u5730\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u9ad8\u5ea6\u52a8\u6001\u7684\u8dd1\u9177\u6280\u80fd\uff0c\u80fd\u591f\u6500\u722c\u9ad8\u8fbe1.25\u7c73\u7684\u969c\u788d\uff0c\u5e76\u9002\u5e94\u5b9e\u65f6\u969c\u788d\u53d8\u5316\u3002", "conclusion": "\u672c\u8bba\u6587\u63d0\u51fa\u7684\u611f\u77e5\u4eba\u5f62\u8dd1\u9177\u6846\u67b6\uff08PHP\uff09\u4f7f\u4eba\u5f62\u673a\u5668\u4eba\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e2d\u81ea\u4e3b\u7ba1\u7406\u957f\u65f6\u95f4\u7684\u8dd1\u9177\u884c\u4e3a\uff0c\u5c55\u793a\u51fa\u9ad8\u6c34\u5e73\u7684\u52a8\u6001\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2602.15828", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15828", "abs": "https://arxiv.org/abs/2602.15828", "authors": ["Yuxuan Kuang", "Sungjae Park", "Katerina Fragkiadaki", "Shubham Tulsiani"], "title": "Dex4D: Task-Agnostic Point Track Policy for Sim-to-Real Dexterous Manipulation", "comment": "Project page: https://dex4d.github.io/", "summary": "Learning generalist policies capable of accomplishing a plethora of everyday tasks remains an open challenge in dexterous manipulation. In particular, collecting large-scale manipulation data via real-world teleoperation is expensive and difficult to scale. While learning in simulation provides a feasible alternative, designing multiple task-specific environments and rewards for training is similarly challenging. We propose Dex4D, a framework that instead leverages simulation for learning task-agnostic dexterous skills that can be flexibly recomposed to perform diverse real-world manipulation tasks. Specifically, Dex4D learns a domain-agnostic 3D point track conditioned policy capable of manipulating any object to any desired pose. We train this 'Anypose-to-Anypose' policy in simulation across thousands of objects with diverse pose configurations, covering a broad space of robot-object interactions that can be composed at test time. At deployment, this policy can be zero-shot transferred to real-world tasks without finetuning, simply by prompting it with desired object-centric point tracks extracted from generated videos. During execution, Dex4D uses online point tracking for closed-loop perception and control. Extensive experiments in simulation and on real robots show that our method enables zero-shot deployment for diverse dexterous manipulation tasks and yields consistent improvements over prior baselines. Furthermore, we demonstrate strong generalization to novel objects, scene layouts, backgrounds, and trajectories, highlighting the robustness and scalability of the proposed framework.", "AI": {"tldr": "Dex4D\u6846\u67b6\u901a\u8fc7\u5728\u4eff\u771f\u4e2d\u5b66\u4e60\u4efb\u52a1\u65e0\u5173\u7684\u7075\u5de7\u6280\u80fd\uff0c\u80fd\u591f\u96f6\u6b21\u8f6c\u79fb\u5230\u73b0\u5b9e\u4e16\u754c\u4e2d\u6267\u884c\u591a\u6837\u7684\u7075\u5de7\u64cd\u4f5c\uff0c\u4e0e\u4ee5\u5f80\u57fa\u7ebf\u76f8\u6bd4\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u6539\u8fdb\u3002", "motivation": "\u5f53\u524d\u5728\u7075\u5de7\u64cd\u4f5c\u4e2d\uff0c\u5b9e\u73b0\u80fd\u591f\u5b8c\u6210\u5404\u79cd\u65e5\u5e38\u4efb\u52a1\u7684\u901a\u7528\u7b56\u7565\u4ecd\u7136\u662f\u4e00\u9879\u5f00\u653e\u7684\u6311\u6218\uff0c\u56e0\u5927\u89c4\u6a21\u6536\u96c6\u771f\u5b9e\u4e16\u754c\u7684\u64cd\u4f5c\u6570\u636e\u65e2\u6602\u8d35\u53c8\u96be\u4ee5\u6269\u5c55\u3002", "method": "Dex4D\u5229\u7528\u4eff\u771f\u5b66\u4e60\u4e00\u4e2a\u6761\u4ef6\u5316\u76843D\u70b9\u8f68\u8ff9\u65e0\u5173\u7b56\u7565\uff0c\u80fd\u591f\u5c06\u4efb\u610f\u7269\u4f53\u64cd\u7eb5\u5230\u4efb\u4f55\u671f\u671b\u59ff\u6001\uff0c\u5e76\u5728\u4eff\u771f\u4e2d\u5bf9\u6570\u5343\u4e2a\u7269\u4f53\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u652f\u6301\u591a\u6837\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u7684\u96f6\u6b21\u90e8\u7f72\uff0c\u5e76\u5728\u65b0\u7684\u7269\u4f53\u3001\u573a\u666f\u5e03\u5c40\u548c\u80cc\u666f\u4e0a\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Dex4D\u5c55\u793a\u4e86\u5176\u6846\u67b6\u7684\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u591a\u6837\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
