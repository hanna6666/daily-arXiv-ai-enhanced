<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 29]
- [cs.HC](#cs.HC) [Total: 12]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [VER: Vision Expert Transformer for Robot Learning via Foundation Distillation and Dynamic Routing](https://arxiv.org/abs/2510.05213)
*Yixiao Wang,Mingxiao Huo,Zhixuan Liang,Yushi Du,Lingfeng Sun,Haotian Lin,Jinghuan Shang,Chensheng Peng,Mohit Bansal,Mingyu Ding,Masayoshi Tomizuka*

Main category: cs.RO

TL;DR: VER通过提炼多个视觉模型，优化机器人学习过程，支持灵活、高效的任务适应。


<details>
  <summary>Details</summary>
Motivation: 解决个体VFMs在特定领域内的局限性，旨在提高多任务的泛化能力。

Method: 通过预训练将多个视觉基础模型(VFM)提炼为视觉专家库，然后微调一个轻量级路由网络以动态选择任务相关专家。

Result: VER改进了动态专家选择的灵活性和精确性，支持高效的参数微调，促进专家的可扩展利用和自适应机器人领域知识整合。

Conclusion: VER在17项多样化机器人任务中实现了最先进的性能，并减小了任务无关区域的大范数离群值。

Abstract: Pretrained vision foundation models (VFMs) advance robotic learning via rich
visual representations, yet individual VFMs typically excel only in specific
domains, limiting generality across tasks. Distilling multiple VFMs into a
unified representation for policy can mitigate this limitation but often yields
inflexible task-specific feature selection and requires costly full re-training
to incorporate robot-domain knowledge. We propose VER, a Vision Expert
transformer for Robot learning. During pretraining, VER distills multiple VFMs
into a vision expert library. It then fine-tunes only a lightweight routing
network (fewer than 0.4% of parameters) to dynamically select task-relevant
experts from the pretrained library for downstream robot tasks. We further
introduce Patchwise Expert Routing with Curriculum Top-K Annealing to improve
both flexibility and precision of dynamic expert selection. Moreover, VER
supports parameter-efficient finetuning for scalable expert utilization and
adaptive robot-domain knowledge integration. Across 17 diverse robotic tasks
and multiple policy heads, VER achieves state-of-the-art performance. We find
that VER reduces large-norm outliers in task-irrelevant regions (e.g.,
background) and concentrates on task-critical regions. Visualizations and codes
can be found in https://yixiaowang7.github.io/ver_page/.

</details>


### [2] [Adaptive Dynamics Planning for Robot Navigation](https://arxiv.org/abs/2510.05330)
*Lu Yuanjie,Mao Mingyang,Xu Tong,Wang Linji,Lin Xiaomin,Xiao Xuesu*

Main category: cs.RO

TL;DR: 自适应动态规划（ADP）通过增强学习调整动态属性，实现更高效和安全的自主导航。


<details>
  <summary>Details</summary>
Motivation: 当前自主机器人导航系统在动态规划中存在不连续性，导致在高度约束环境下的轨迹跟踪失败。

Method: 将ADP集成到三种不同的规划器中，并设计了独立的ADP导航系统进行基准测试。

Result: 提出了自适应动态规划（ADP），通过增强学习动态调整机器人动态属性，提高了导航成功率、安全性和效率。

Conclusion: 实验结果表明，相较于其他基准，ADP在导航成功率和安全性方面均有显著提升。

Abstract: Autonomous robot navigation systems often rely on hierarchical planning,
where global planners compute collision-free paths without considering
dynamics, and local planners enforce dynamics constraints to produce executable
commands. This discontinuity in dynamics often leads to trajectory tracking
failure in highly constrained environments. Recent approaches integrate
dynamics within the entire planning process by gradually decreasing its
fidelity, e.g., increasing integration steps and reducing collision checking
resolution, for real-time planning efficiency. However, they assume that the
fidelity of the dynamics should decrease according to a manually designed
scheme. Such static settings fail to adapt to environmental complexity
variations, resulting in computational overhead in simple environments or
insufficient dynamics consideration in obstacle-rich scenarios. To overcome
this limitation, we propose Adaptive Dynamics Planning (ADP), a
learning-augmented paradigm that uses reinforcement learning to dynamically
adjust robot dynamics properties, enabling planners to adapt across diverse
environments. We integrate ADP into three different planners and further design
a standalone ADP-based navigation system, benchmarking them against other
baselines. Experiments in both simulation and real-world tests show that ADP
consistently improves navigation success, safety, and efficiency.

</details>


### [3] [A multi-modal tactile fingertip design for robotic hands to enhance dexterous manipulation](https://arxiv.org/abs/2510.05382)
*Zhuowei Xu,Zilin Si,Kevin Zhang,Oliver Kroemer,Zeynep Temel*

Main category: cs.RO

TL;DR: 本研究提出了一种低成本、易于制作的适应性机器人手指尖设计，集成了多模态触觉传感器，显著提高了操作的精确度和多样性。


<details>
  <summary>Details</summary>
Motivation: 触觉传感技术在提升机器人操作精度和灵活性方面具有巨大潜力，但受限于高传感器成本、制造和集成挑战以及信号提取的困难。

Method: 本研究使用应变计传感器和接触麦克风传感器集成到紧凑的机器人手指尖设计中，以捕捉静态力和高频振动。

Result: 应变计传感器在0-5 N范围内提供可重复的2D平面力测量，接触麦克风传感器能够区分接触材料特性。通过三个灵巧操作任务验证我们设计的有效性。

Conclusion: 我们的设计能够在不同操作阶段灵活使用多种触觉传感技术，显著提高任务表现。

Abstract: Tactile sensing holds great promise for enhancing manipulation precision and
versatility, but its adoption in robotic hands remains limited due to high
sensor costs, manufacturing and integration challenges, and difficulties in
extracting expressive and reliable information from signals. In this work, we
present a low-cost, easy-to-make, adaptable, and compact fingertip design for
robotic hands that integrates multi-modal tactile sensors. We use strain gauge
sensors to capture static forces and a contact microphone sensor to measure
high-frequency vibrations during contact. These tactile sensors are integrated
into a compact design with a minimal sensor footprint, and all sensors are
internal to the fingertip and therefore not susceptible to direct wear and tear
from interactions. From sensor characterization, we show that strain gauge
sensors provide repeatable 2D planar force measurements in the 0-5 N range and
the contact microphone sensor has the capability to distinguish contact
material properties. We apply our design to three dexterous manipulation tasks
that range from zero to full visual occlusion. Given the expressiveness and
reliability of tactile sensor readings, we show that different tactile sensing
modalities can be used flexibly in different stages of manipulation, solely or
together with visual observations to achieve improved task performance. For
instance, we can precisely count and unstack a desired number of paper cups
from a stack with 100\% success rate which is hard to achieve with vision only.

</details>


### [4] [Towards Online Robot Interaction Adaptation to Human Upper-limb Mobility Impairments in Return-to-Work Scenarios](https://arxiv.org/abs/2510.05425)
*Marta Lagomarsino,Francesco Tassi*

Main category: cs.RO

TL;DR: 本文提出了一种在线框架，通过适应用户的臂部残疾，改进人机交互，以促进工作参与。


<details>
  <summary>Details</summary>
Motivation: 现有工作环境未能充分包容上下肢残疾人士，因此需要开发新的方法来促进他们的工作参与。

Method: 提出了一种新的在线适应性人机交互框架，结合了特定关节限制的运动模型与分层最优控制器。

Result: 通过手传任务测试，框架能够个性化互动，鼓励用户根据其功能限制程度使用关节，并与现有的人机交互方法进行定量和定性比较。

Conclusion: 该框架能根据用户的残疾适应性调整，使互动更加个性化和有效。

Abstract: Work environments are often inadequate and lack inclusivity for individuals
with upper-body disabilities. This paper presents a novel online framework for
adaptive human-robot interaction (HRI) that accommodates users' arm mobility
impairments, ultimately aiming to promote active work participation. Unlike
traditional human-robot collaboration approaches that assume able-bodied users,
our method integrates a mobility model for specific joint limitations into a
hierarchical optimal controller. This allows the robot to generate reactive,
mobility-aware behaviour online and guides the user's impaired limb to exploit
residual functional mobility. The framework was tested in handover tasks
involving different upper-limb mobility impairments (i.e., emulated elbow and
shoulder arthritis, and wrist blockage), under both standing and seated
configurations with task constraints using a mobile manipulator, and
complemented by quantitative and qualitative comparisons with state-of-the-art
ergonomic HRI approaches. Preliminary results indicated that the framework can
personalise the interaction to fit within the user's impaired range of motion
and encourage joint usage based on the severity of their functional
limitations.

</details>


### [5] [Active Semantic Perception](https://arxiv.org/abs/2510.05430)
*Huayi Tang,Pratik Chaudhari*

Main category: cs.RO

TL;DR: 提出了一种基于语义的主动感知方法，通过分层场景图和大型语言模型来增强室内环境探索的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 开发一种主动式语义感知的方法，利用场景的语义进行探索等任务。

Method: 利用大型语言模型（LLMs）来采样与部分观测一致的未观察区域的场景图，并计算潜在路径点的信息增益。

Result: 我们的实验表明，此方法在复杂的3D室内环境中相较于基线方法具有更高的效率和准确性。

Conclusion: 我们的方法在复杂的3D室内环境中能够比基线方法更快更准确地确定环境的语义。

Abstract: We develop an approach for active semantic perception which refers to using
the semantics of the scene for tasks such as exploration. We build a compact,
hierarchical multi-layer scene graph that can represent large, complex indoor
environments at various levels of abstraction, e.g., nodes corresponding to
rooms, objects, walls, windows etc. as well as fine-grained details of their
geometry. We develop a procedure based on large language models (LLMs) to
sample plausible scene graphs of unobserved regions that are consistent with
partial observations of the scene. These samples are used to compute an
information gain of a potential waypoint for sophisticated spatial reasoning,
e.g., the two doors in the living room can lead to either a kitchen or a
bedroom. We evaluate this approach in complex, realistic 3D indoor environments
in simulation. We show using qualitative and quantitative experiments that our
approach can pin down the semantics of the environment quicker and more
accurately than baseline approaches.

</details>


### [6] [AD-NODE: Adaptive Dynamics Learning with Neural ODEs for Mobile Robots Control](https://arxiv.org/abs/2510.05443)
*Shao-Yi Yu,Jen-Wei Wang,Maya Horii,Vikas Garg,Tarek Zohdi*

Main category: cs.RO

TL;DR: 本文提出了一种自适应动力学模型，使移动机器人在不确定环境中能有效工作，无需直接了解环境，基于神经常微分方程，并通过两阶段训练学习环境表示，适用于不同复杂度的机器人平台。


<details>
  <summary>Details</summary>
Motivation: 移动机器人在各种领域的重要性日益增加，在不确定环境中需要有效的动力学模型，尤其是在缺乏直接环境信息时。

Method: 提出了一种基于神经常微分方程的自适应动力学模型，通过两阶段训练学习潜在环境表示。

Result: 通过在三种复杂性递增的机器人平台上进行的目标到达和路径跟踪任务，验证了方法的有效性。

Conclusion: 实证结果表明，该方法能够处理时空变化的环境变化，适用于模拟和现实系统。

Abstract: Mobile robots, such as ground vehicles and quadrotors, are becoming
increasingly important in various fields, from logistics to agriculture, where
they automate processes in environments that are difficult to access for
humans. However, to perform effectively in uncertain environments using
model-based controllers, these systems require dynamics models capable of
responding to environmental variations, especially when direct access to
environmental information is limited. To enable such adaptivity and facilitate
integration with model predictive control, we propose an adaptive dynamics
model which bypasses the need for direct environmental knowledge by inferring
operational environments from state-action history. The dynamics model is based
on neural ordinary equations, and a two-phase training procedure is used to
learn latent environment representations. We demonstrate the effectiveness of
our approach through goal-reaching and path-tracking tasks on three robotic
platforms of increasing complexity: a 2D differential wheeled robot with
changing wheel contact conditions, a 3D quadrotor in variational wind fields,
and the Sphero BOLT robot under two contact conditions for real-world
deployment. Empirical results corroborate that our method can handle temporally
and spatially varying environmental changes in both simulation and real-world
systems.

</details>


### [7] [Correlation-Aware Dual-View Pose and Velocity Estimation for Dynamic Robotic Manipulation](https://arxiv.org/abs/2510.05536)
*Mahboubeh Zarei,Robin Chhabra,Farrokh Janabi-Sharifi*

Main category: cs.RO

TL;DR: 提出了一种新型的去中心化融合方法，通过双视角测量实现对机器人操作手的姿态和速度的高效估计。


<details>
  <summary>Details</summary>
Motivation: 机器人操作中，准确的姿态和速度估计是有效空间任务规划的必要条件。

Method: 使用双视角测量，在每个操作手上运行两个独立的自适应扩展卡尔曼滤波器，并在流形上更新状态。

Result: 在UFactory xArm 850上进行实验，所提出方法对目标跟踪展示了显著的效果和稳定性。

Conclusion: 实验结果验证了所提出的去中心化双视角估计框架的有效性和鲁棒性，且相较于现有最先进的方法显示出一致的改进。

Abstract: Accurate pose and velocity estimation is essential for effective spatial task
planning in robotic manipulators. While centralized sensor fusion has
traditionally been used to improve pose estimation accuracy, this paper
presents a novel decentralized fusion approach to estimate both pose and
velocity. We use dual-view measurements from an eye-in-hand and an eye-to-hand
vision sensor configuration mounted on a manipulator to track a target object
whose motion is modeled as random walk (stochastic acceleration model). The
robot runs two independent adaptive extended Kalman filters formulated on a
matrix Lie group, developed as part of this work. These filters predict poses
and velocities on the manifold $\mathbb{SE}(3) \times \mathbb{R}^3 \times
\mathbb{R}^3$ and update the state on the manifold $\mathbb{SE}(3)$. The final
fused state comprising the fused pose and velocities of the target is obtained
using a correlation-aware fusion rule on Lie groups. The proposed method is
evaluated on a UFactory xArm 850 equipped with Intel RealSense cameras,
tracking a moving target. Experimental results validate the effectiveness and
robustness of the proposed decentralized dual-view estimation framework,
showing consistent improvements over state-of-the-art methods.

</details>


### [8] [ARRC: Advanced Reasoning Robot Control - Knowledge-Driven Autonomous Manipulation Using Retrieval-Augmented Generation](https://arxiv.org/abs/2510.05547)
*Eugene Vorobiov,Ammar Jaleel Mahmood,Salim Rezvani,Robin Chhabra*

Main category: cs.RO

TL;DR: 本研究介绍了ARRC系统，该系统通过结合RAG、RGB-D感知和安全执行机制，有效地将自然语言指令转化为安全的机器人控制，显著提升了计划的有效性与适应性。


<details>
  <summary>Details</summary>
Motivation: 开发一个实用的系统，将自然语言指令与本地机器人的安全控制连接起来。

Method: 结合了检索增强生成（RAG）、RGB-D感知和受保护的执行机制的机器人控制系统。

Result: 实验证明了所提方法的有效性，展现了RAG基础计划在提高计划有效性和适应性方面的潜力，同时保持了对机器人的低级控制和感知的本地化。

Conclusion: 本研究提出的ARRC系统能够有效地将自然语言指令转化为安全的机器人控制，显著提高了计划的有效性和适应性。

Abstract: We present ARRC (Advanced Reasoning Robot Control), a practical system that
connects natural-language instructions to safe local robotic control by
combining Retrieval-Augmented Generation (RAG) with RGB-D perception and
guarded execution on an affordable robot arm. The system indexes curated robot
knowledge (movement patterns, task templates, and safety heuristics) in a
vector database, retrieves task-relevant context for each instruction, and
conditions a large language model (LLM) to produce JSON-structured action
plans. Plans are executed on a UFactory xArm 850 fitted with a Dynamixel-driven
parallel gripper and an Intel RealSense D435 camera. Perception uses AprilTag
detections fused with depth to produce object-centric metric poses. Execution
is enforced via software safety gates: workspace bounds, speed and force caps,
timeouts, and bounded retries. We describe the architecture, knowledge design,
integration choices, and a reproducible evaluation protocol for tabletop scan,
approach, and pick-place tasks. Experimental results demonstrate the efficacy
of the proposed approach. Our design shows that RAG-based planning can
substantially improve plan validity and adaptability while keeping perception
and low-level control local to the robot.

</details>


### [9] [GO-Flock: Goal-Oriented Flocking in 3D Unknown Environments with Depth Maps](https://arxiv.org/abs/2510.05553)
*Yan Rui Tan,Wenqi Liu,Wai Lun Leong,John Guan Zhong Tan,Wayne Wen Huei Yong,Fan Shi,Rodney Swee Huat Teo*

Main category: cs.RO

TL;DR: GO-Flock框架结合了规划与APF控制，改善了在障碍物环境中群体活动的表现。


<details>
  <summary>Details</summary>
Motivation: APF方法在群体控制中广泛使用，但在障碍物存在时常面临死锁和局部最优等挑战，现有解决方案往往消极，导致导航效率低下。

Method: GO-Flock包含感知模块用于提取路径点和虚拟代理以避免障碍，以及集体导航模块采用新型APF策略实现有效的群体行为。

Result: 提出了GO-Flock框架，有效融合规划与反应式APF控制，提升了在复杂环境中的群体导航能力。

Conclusion: GO-Flock展示了在障碍环境中更高效的群体行为，并通过实际实验验证了其有效性。

Abstract: Artificial Potential Field (APF) methods are widely used for reactive
flocking control, but they often suffer from challenges such as deadlocks and
local minima, especially in the presence of obstacles. Existing solutions to
address these issues are typically passive, leading to slow and inefficient
collective navigation. As a result, many APF approaches have only been
validated in obstacle-free environments or simplified, pseudo 3D simulations.
This paper presents GO-Flock, a hybrid flocking framework that integrates
planning with reactive APF-based control. GO-Flock consists of an upstream
Perception Module, which processes depth maps to extract waypoints and virtual
agents for obstacle avoidance, and a downstream Collective Navigation Module,
which applies a novel APF strategy to achieve effective flocking behavior in
cluttered environments. We evaluate GO-Flock against passive APF-based
approaches to demonstrate their respective merits, such as their flocking
behavior and the ability to overcome local minima. Finally, we validate
GO-Flock through obstacle-filled environment and also hardware-in-the-loop
experiments where we successfully flocked a team of nine drones, six physical
and three virtual, in a forest environment.

</details>


### [10] [DeLTa: Demonstration and Language-Guided Novel Transparent Object Manipulation](https://arxiv.org/abs/2510.05662)
*Taeyeop Lee,Gyuree Kang,Bowen Wen,Youngho Kim,Seunghyeok Back,In So Kweon,David Hyunchul Shim,Kuk-Jin Yoon*

Main category: cs.RO

TL;DR: 我们提出了一种新的DeLTa框架，通过结合深度估计和视觉语言规划，提升透明物体操作的精度和通用性。


<details>
  <summary>Details</summary>
Motivation: 应对现有技术在处理新对象和长时域操控方面的局限性。

Method: 结合深度估计、6D姿态估计和视觉语言规划，进行精确的透明物体操控。

Result: 全面评估表明，DeLTa在长时域需要精确操控的场景中优于其他当前方法。

Conclusion: 我们的DeLTa方法在解决透明物体长时域操控能力方面比现有方法表现显著更佳。

Abstract: Despite the prevalence of transparent object interactions in human everyday
life, transparent robotic manipulation research remains limited to
short-horizon tasks and basic grasping capabilities.Although some methods have
partially addressed these issues, most of them have limitations in
generalizability to novel objects and are insufficient for precise long-horizon
robot manipulation. To address this limitation, we propose DeLTa (Demonstration
and Language-Guided Novel Transparent Object Manipulation), a novel framework
that integrates depth estimation, 6D pose estimation, and vision-language
planning for precise long-horizon manipulation of transparent objects guided by
natural task instructions. A key advantage of our method is its
single-demonstration approach, which generalizes 6D trajectories to novel
transparent objects without requiring category-level priors or additional
training. Additionally, we present a task planner that refines the
VLM-generated plan to account for the constraints of a single-arm, eye-in-hand
robot for long-horizon object manipulation tasks. Through comprehensive
evaluation, we demonstrate that our method significantly outperforms existing
transparent object manipulation approaches, particularly in long-horizon
scenarios requiring precise manipulation capabilities. Project page:
https://sites.google.com/view/DeLTa25/

</details>


### [11] [Verifier-free Test-Time Sampling for Vision Language Action Models](https://arxiv.org/abs/2510.05681)
*Suhyeok Jang,Dongyoung Kim,Changyeon Kim,Youngsuk Kim,Jinwoo Shin*

Main category: cs.RO

TL;DR: 提出了一种新颖的测试时间扩展框架MG-Select，利用模型内部属性来提升视觉-语言-动作模型的选择能力，而不需要额外训练。


<details>
  <summary>Details</summary>
Motivation: 解决当前视觉-语言-动作模型在精密任务上的性能局限性，特别是在没有额外训练或外部模块的情况下提高模型的选择能力。

Method: Masking Distribution Guided Selection (MG-Select)

Result: MG-Select在真实世界的任务中实现了28%/35%的性能提升，以及在RoboCasa挑选和放置任务中相较于30次演示的训练取得了168%的相对增益。

Conclusion: MG-Select有效提升了视觉-语言-动作模型在多种任务中的性能，尤其是在精度要求高的场景中，展现了强大的适应性和鲁棒性。

Abstract: Vision-Language-Action models (VLAs) have demonstrated remarkable performance
in robot control. However, they remain fundamentally limited in tasks that
require high precision due to their single-inference paradigm. While test-time
scaling approaches using external verifiers have shown promise, they require
additional training and fail to generalize to unseen conditions. We propose
Masking Distribution Guided Selection (MG-Select), a novel test-time scaling
framework for VLAs that leverages the model's internal properties without
requiring additional training or external modules. Our approach utilizes KL
divergence from a reference action token distribution as a confidence metric
for selecting the optimal action from multiple candidates. We introduce a
reference distribution generated by the same VLA but with randomly masked
states and language conditions as inputs, ensuring maximum uncertainty while
remaining aligned with the target task distribution. Additionally, we propose a
joint training strategy that enables the model to learn both conditional and
unconditional distributions by applying dropout to state and language
conditions, thereby further improving the quality of the reference
distribution. Our experiments demonstrate that MG-Select achieves significant
performance improvements, including a 28%/35% improvement in real-world
in-distribution/out-of-distribution tasks, along with a 168% relative gain on
RoboCasa pick-and-place tasks trained with 30 demonstrations.

</details>


### [12] [Oracle-Guided Masked Contrastive Reinforcement Learning for Visuomotor Policies](https://arxiv.org/abs/2510.05692)
*Yuhang Zhang,Jiaping Xiao,Chao Yan,Mir Feroskhan*

Main category: cs.RO

TL;DR: 本研究提出了OMC-RL框架，通过分阶段学习来提升样本效率与政策性能，克服高维视觉输入与动作输出结合的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决高维视觉输入与敏捷动作输出结合带来的样本效率低和sim-to-real差距大的问题。

Method: Oracle-Guided Masked Contrastive Reinforcement Learning (OMC-RL)

Result: OMC-RL在模拟和真实环境中实现了优越的样本效率和政策性能，同时改善了在多样和感知复杂场景中的泛化能力。

Conclusion: OMC-RL有效提升了学习的样本效率和性能，并在复杂场景中表现出良好的泛化能力。

Abstract: A prevailing approach for learning visuomotor policies is to employ
reinforcement learning to map high-dimensional visual observations directly to
action commands. However, the combination of high-dimensional visual inputs and
agile maneuver outputs leads to long-standing challenges, including low sample
efficiency and significant sim-to-real gaps. To address these issues, we
propose Oracle-Guided Masked Contrastive Reinforcement Learning (OMC-RL), a
novel framework designed to improve the sample efficiency and asymptotic
performance of visuomotor policy learning. OMC-RL explicitly decouples the
learning process into two stages: an upstream representation learning stage and
a downstream policy learning stage. In the upstream stage, a masked Transformer
module is trained with temporal modeling and contrastive learning to extract
temporally-aware and task-relevant representations from sequential visual
inputs. After training, the learned encoder is frozen and used to extract
visual representations from consecutive frames, while the Transformer module is
discarded. In the downstream stage, an oracle teacher policy with privileged
access to global state information supervises the agent during early training
to provide informative guidance and accelerate early policy learning. This
guidance is gradually reduced to allow independent exploration as training
progresses. Extensive experiments in simulated and real-world environments
demonstrate that OMC-RL achieves superior sample efficiency and asymptotic
policy performance, while also improving generalization across diverse and
perceptually complex scenarios.

</details>


### [13] [Stable Robot Motions on Manifolds: Learning Lyapunov-Constrained Neural Manifold ODEs](https://arxiv.org/abs/2510.05707)
*David Boetius,Abdelrahman Abdelnaby,Ashok Kumar,Stefan Leue,Abdalla Swikir,Fares J. Abu-Dakka*

Main category: cs.RO

TL;DR: 本论文提出了一种框架，利用神经普通微分方程在黎曼流形上学习稳定动态系统，并通过实验验证探索其在机器人运动中的应用。


<details>
  <summary>Details</summary>
Motivation: 从数据中学习稳定的动态系统对于安全可靠的机器人运动规划和控制至关重要，但在黎曼流形定义的轨迹上拓展稳定性保证面临重大挑战。

Method: 使用神经普通微分方程，确保在流形上通过投影神经矢量场来满足Lyapunov稳定性标准，以实现在每个系统状态下的稳定性。

Result: 通过在单位四元数（S^3）和对称正定矩阵流形等黎曼LASA数据集上的应用，以及在	extbf{R}^3 	imes S^3上演化的机器人运动，我们证明了该方法的性能、可扩展性和实用性。

Conclusion: 我们提出了一种新的框架，通过神经普通微分方程在黎曼流形上学习稳定的动态系统，并在实际实验中展示了其出色的性能和适用性。

Abstract: Learning stable dynamical systems from data is crucial for safe and reliable
robot motion planning and control. However, extending stability guarantees to
trajectories defined on Riemannian manifolds poses significant challenges due
to the manifold's geometric constraints. To address this, we propose a general
framework for learning stable dynamical systems on Riemannian manifolds using
neural ordinary differential equations. Our method guarantees stability by
projecting the neural vector field evolving on the manifold so that it strictly
satisfies the Lyapunov stability criterion, ensuring stability at every system
state. By leveraging a flexible neural parameterisation for both the base
vector field and the Lyapunov function, our framework can accurately represent
complex trajectories while respecting manifold constraints by evolving
solutions directly on the manifold. We provide an efficient training strategy
for applying our framework and demonstrate its utility by solving Riemannian
LASA datasets on the unit quaternion (S^3) and symmetric positive-definite
matrix manifolds, as well as robotic motions evolving on \mathbb{R}^3 \times
S^3. We demonstrate the performance, scalability, and practical applicability
of our approach through extensive simulations and by learning robot motions in
a real-world experiment.

</details>


### [14] [Federated Split Learning for Resource-Constrained Robots in Industrial IoT: Framework Comparison, Optimization Strategies, and Future Directions](https://arxiv.org/abs/2510.05713)
*Wanli Ni,Hui Tian,Shuai Wang,Chengyang Li,Lei Sun,Zhaohui Yang*

Main category: cs.RO

TL;DR: 本文研究了资源受限机器人在工业场景中应用的联邦分割学习（FedSL）框架，比较了不同框架的性能，提出了优化技术，并总结了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 在智能工厂的工业物联网系统中，数据隐私、通信效率和设备异构性是关键问题，因此需要探索适合资源受限机器人应用的FedSL框架。

Method: 对同步、异步、层次和异构FedSL框架进行比较，探讨其在动态工业条件下的工作流程、可扩展性、适应性和局限性。

Result: 模拟结果验证了这些框架在工业检测场景下的性能。本文还系统分类了三种标记融合策略，并提出了增强FedSL实施效率的自适应优化技术。

Conclusion: 本文总结了FedSL在工业场景中的实施，并指出了未来研究方向和开放问题。

Abstract: Federated split learning (FedSL) has emerged as a promising paradigm for
enabling collaborative intelligence in industrial Internet of Things (IoT)
systems, particularly in smart factories where data privacy, communication
efficiency, and device heterogeneity are critical concerns. In this article, we
present a comprehensive study of FedSL frameworks tailored for
resource-constrained robots in industrial scenarios. We compare synchronous,
asynchronous, hierarchical, and heterogeneous FedSL frameworks in terms of
workflow, scalability, adaptability, and limitations under dynamic industrial
conditions. Furthermore, we systematically categorize token fusion strategies
into three paradigms: input-level (pre-fusion), intermediate-level
(intra-fusion), and output-level (post-fusion), and summarize their respective
strengths in industrial applications. We also provide adaptive optimization
techniques to enhance the efficiency and feasibility of FedSL implementation,
including model compression, split layer selection, computing frequency
allocation, and wireless resource management. Simulation results validate the
performance of these frameworks under industrial detection scenarios. Finally,
we outline open issues and research directions of FedSL in future smart
manufacturing systems.

</details>


### [15] [Precise and Efficient Collision Prediction under Uncertainty in Autonomous Driving](https://arxiv.org/abs/2510.05729)
*Marc Kaufeld,Johannes Betz*

Main category: cs.RO

TL;DR: 本研究提出了两种半解析方法，以计算自主车辆在不确定条件下与周围障碍物相碰撞的概率，同时兼顾实时计算之间的平衡。


<details>
  <summary>Details</summary>
Motivation: 由于噪声感知、定位误差和其他交通参与者的预测不确定性，现有的确定性碰撞检查常常不够准确或过于保守，以至于需要更好的风险评估方法。

Method: 两种半解析方法计算与任意凸障碍物的碰撞概率，分别通过空间重叠的概率和随机边界穿越的概率进行评估，并完整考虑了位置、方向和速度等状态的不确定性。

Result: 研究介绍了两种高效的方法来估计自主驾驶中在不确定驾驶条件下计划轨迹的碰撞风险。

Conclusion: 提出的方法能够以高精度计算碰撞概率，并大幅度降低计算时间，适用于风险感知轨迹规划。

Abstract: This research introduces two efficient methods to estimate the collision risk
of planned trajectories in autonomous driving under uncertain driving
conditions. Deterministic collision checks of planned trajectories are often
inaccurate or overly conservative, as noisy perception, localization errors,
and uncertain predictions of other traffic participants introduce significant
uncertainty into the planning process. This paper presents two semi-analytic
methods to compute the collision probability of planned trajectories with
arbitrary convex obstacles. The first approach evaluates the probability of
spatial overlap between an autonomous vehicle and surrounding obstacles, while
the second estimates the collision probability based on stochastic boundary
crossings. Both formulations incorporate full state uncertainties, including
position, orientation, and velocity, and achieve high accuracy at computational
costs suitable for real-time planning. Simulation studies verify that the
proposed methods closely match Monte Carlo results while providing significant
runtime advantages, enabling their use in risk-aware trajectory planning. The
collision estimation methods are available as open-source software:
https://github.com/TUM-AVS/Collision-Probability-Estimation

</details>


### [16] [Human-in-the-loop Optimisation in Robot-assisted Gait Training](https://arxiv.org/abs/2510.05780)
*Andreas Christou,Andreas Sochopoulos,Elliot Lister,Sethu Vijayakumar*

Main category: cs.RO

TL;DR: 可穿戴机器人可以量化监测步态并提供适应性辅助，但由于步态的个体差异，设计能适应个体特征的控制器至关重要。本研究使用人机协同优化，一定程度上提升个性化辅助，但没有观察到对参与者表现的显著影响，这突显了人机共适应的重要性以及当前个性化方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 可穿戴机器人用于步态监测和独立性提高，但步态模式的显著个体差异要求设计能够适应每个个体特征的机器人控制器。

Method: 本研究采用了协方差矩阵适应进化策略（CMA-ES）来针对下肢外骨骼的助力控制器进行持续的优化，并在一个为期两天的实验中对六名健康个体进行了测试。

Result: 这篇论文探讨了可穿戴机器人在步态训练中的应用，尤其是如何通过人机协同优化提供个性化的帮助。研究表明，尽管CMA-ES算法能够为每个参与者优化出独特的刚度设置，但在验证试验中并未对参与者的表现产生明显影响。这表明人机协同适应和人类行为的变异性对结果的影响可能超过了个性化助力控制器的潜在好处，因此，对于现有个性化方法的局限性提供了见解，并确定了在人机协同优化实施中的关键挑战。

Conclusion: 人机协同适应与人类行为变异性可能对步态训练的效果影响显著，超出个性化助力控制器的潜在好处。

Abstract: Wearable robots offer a promising solution for quantitatively monitoring gait
and providing systematic, adaptive assistance to promote patient independence
and improve gait. However, due to significant interpersonal and intrapersonal
variability in walking patterns, it is important to design robot controllers
that can adapt to the unique characteristics of each individual. This paper
investigates the potential of human-in-the-loop optimisation (HILO) to deliver
personalised assistance in gait training. The Covariance Matrix Adaptation
Evolution Strategy (CMA-ES) was employed to continuously optimise an
assist-as-needed controller of a lower-limb exoskeleton. Six healthy
individuals participated over a two-day experiment. Our results suggest that
while the CMA-ES appears to converge to a unique set of stiffnesses for each
individual, no measurable impact on the subjects' performance was observed
during the validation trials. These findings highlight the impact of
human-robot co-adaptation and human behaviour variability, whose effect may be
greater than potential benefits of personalising rule-based assistive
controllers. Our work contributes to understanding the limitations of current
personalisation approaches in exoskeleton-assisted gait rehabilitation and
identifies key challenges for effective implementation of human-in-the-loop
optimisation in this domain.

</details>


### [17] [VCoT-Grasp: Grasp Foundation Models with Visual Chain-of-Thought Reasoning for Language-driven Grasp Generation](https://arxiv.org/abs/2510.05827)
*Haoran Zhang,Shuanghao Bai,Wanqi Zhou,Yuedi Zhang,Qi Zhang,Pengxiang Ding,Cheng Chi,Donglin Wang,Badong Chen*

Main category: cs.RO

TL;DR: VCoT-Grasp是一种结合视觉推理的端到端抓取模型，能在复杂环境中有效提升抓取成功率与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前的抓取基础模型在对话和物体语义上过分强调，导致单一物体抓取的性能不佳，并且缺乏良好的推理和泛化能力。

Method: 提出了一种端到端的抓取基础模型VCoT-Grasp，结合了视觉链思维推理以增强抓取生成的视觉理解。

Result: VCoT-Grasp在VCoT-GraspSet和真实机器人上的大量实验表明，该方法显著提高了抓取成功率。

Conclusion: VCoT-Grasp模型显著提高了抓取成功率，并能有效地适应未见过的对象、背景和干扰物。

Abstract: Robotic grasping is one of the most fundamental tasks in robotic
manipulation, and grasp detection/generation has long been the subject of
extensive research. Recently, language-driven grasp generation has emerged as a
promising direction due to its practical interaction capabilities. However,
most existing approaches either lack sufficient reasoning and generalization
capabilities or depend on complex modular pipelines. Moreover, current grasp
foundation models tend to overemphasize dialog and object semantics, resulting
in inferior performance and restriction to single-object grasping. To maintain
strong reasoning ability and generalization in cluttered environments, we
propose VCoT-Grasp, an end-to-end grasp foundation model that incorporates
visual chain-of-thought reasoning to enhance visual understanding for grasp
generation. VCoT-Grasp adopts a multi-turn processing paradigm that dynamically
focuses on visual inputs while providing interpretable reasoning traces. For
training, we refine and introduce a large-scale dataset, VCoT-GraspSet,
comprising 167K synthetic images with over 1.36M grasps, as well as 400+
real-world images with more than 1.2K grasps, annotated with intermediate
bounding boxes. Extensive experiments on both VCoT-GraspSet and real robot
demonstrate that our method significantly improves grasp success rates and
generalizes effectively to unseen objects, backgrounds, and distractors. More
details can be found at https://zhanghr2001.github.io/VCoT-Grasp.github.io.

</details>


### [18] [A Co-Design Framework for Energy-Aware Monoped Jumping with Detailed Actuator Modeling](https://arxiv.org/abs/2510.05923)
*Aman Singh,Aastha Mishra,Deepak Kapa,Suryank Joshi,Shishir Kolathaya*

Main category: cs.RO

TL;DR: 提出一种新三阶段共设计优化框架，最大化单脚跳跃高度并降低能耗，减少设计迭代。


<details>
  <summary>Details</summary>
Motivation: 现有的共设计框架多关注于最大化高度或最小化能量，而忽略了两者之间的权衡，导致设计难以复现。

Method: 该框架结合了机械设计、控制参数优化并引入了现实的驱动器质量模型，形成了一种新的优化流程。

Result: 本文提出了一种新型的三阶段共设计优化框架，旨在同时最大化单脚跳跃高度并最小化机械能耗。通过引入现实的驱动器质量模型，该方法在一个统一的框架内优化机械设计（包括减速箱）和控制参数。结果设计输出可以自动生成适合直接制造的参数化CAD模型，显著减少手动设计迭代。实验评估表明，与基线设计相比，机械能耗减少了50%，同时达到0.8米的跳跃高度。

Conclusion: 该方法显著提高了跳跃性能并减少了能耗，为单脚设计提供了更高效的优化路径。

Abstract: A monoped's jump height and energy consumption depend on both, its mechanical
design and control strategy. Existing co-design frameworks typically optimize
for either maximum height or minimum energy, neglecting their trade-off. They
also often omit gearbox parameter optimization and use oversimplified actuator
mass models, producing designs difficult to replicate in practice. In this
work, we introduce a novel three-stage co-design optimization framework that
jointly maximizes jump height while minimizing mechanical energy consumption of
a monoped. The proposed method explicitly incorporates realistic actuator mass
models and optimizes mechanical design (including gearbox) and control
parameters within a unified framework. The resulting design outputs are then
used to automatically generate a parameterized CAD model suitable for direct
fabrication, significantly reducing manual design iterations. Our experimental
evaluations demonstrate a 50 percent reduction in mechanical energy consumption
compared to the baseline design, while achieving a jump height of 0.8m. Video
presentation is available at http://y2u.be/XW8IFRCcPgM

</details>


### [19] [Learning to Crawl: Latent Model-Based Reinforcement Learning for Soft Robotic Adaptive Locomotion](https://arxiv.org/abs/2510.05957)
*Vaughn Gzenda,Robin Chhabra*

Main category: cs.RO

TL;DR: 本文提出了一种基于模型的强化学习方法，以潜在动态优化软体机器人的运动策略，且在模拟中表现良好。


<details>
  <summary>Details</summary>
Motivation: 由于模型不准确、传感器噪声和运动模式的发现，控制软体机器人爬行器的策略设计具有挑战性，因此需要一种新的方法来改进这一过程。

Method: 采用基于模型的强化学习框架，通过传感器获取的隐含动态来预测运动，并通过动作-评论者算法优化运动策略。

Result: 本研究提出了一种基于模型的强化学习框架，通过从传感器获取的潜在动态信息来指导动作-评论者算法优化软体爬行机器人的运动策略。

Conclusion: 潜在动态的模型基础强化学习方法能够实现在噪声传感器反馈下的自适应运动，显示了软体机器人在复杂环境中运动的潜力。

Abstract: Soft robotic crawlers are mobile robots that utilize soft body deformability
and compliance to achieve locomotion through surface contact. Designing control
strategies for such systems is challenging due to model inaccuracies, sensor
noise, and the need to discover locomotor gaits. In this work, we present a
model-based reinforcement learning (MB-RL) framework in which latent dynamics
inferred from onboard sensors serve as a predictive model that guides an
actor-critic algorithm to optimize locomotor policies. We evaluate the
framework on a minimal crawler model in simulation using inertial measurement
units and time-of-flight sensors as observations. The learned latent dynamics
enable short-horizon motion prediction while the actor-critic discovers
effective locomotor policies. This approach highlights the potential of
latent-dynamics MB-RL for enabling embodied soft robotic adaptive locomotion
based solely on noisy sensor feedback.

</details>


### [20] [The DISTANT Design for Remote Transmission and Steering Systems for Planetary Robotics](https://arxiv.org/abs/2510.05981)
*Cristina Luna,Alba Guerra,Almudena Moreno,Manuel Esquer,Willy Roa,Mateusz Krawczak,Robert Popela,Piotr Osica,Davide Nicolis*

Main category: cs.RO

TL;DR: 本论文提出了一种名为DISTANT的新型探测车驱动与转向系统设计，旨在提升探测车在极端环境中的行驶能力。


<details>
  <summary>Details</summary>
Motivation: 在极端环境和长距离探测任务中，保护敏感组件免受热循环、尘埃污染和机械磨损的影响。

Method: 采用双叉臂悬挂配置，结合万向节与卷扬驱动转向系统，进行全面的权衡分析以确定最佳架构。

Result: 设计能独立控制车轮牵引、转向和悬挂，确保所有电机化组件位于受保护的环境内，并计划在2026年Q1进行1:3比例的面包板制造测试与验证。

Conclusion: DISTANT设计具备在50公里的远程穿越任务中维持性能的能力，并具备防尘和热管理方案，适合长期探测任务。

Abstract: Planetary exploration missions require robust locomotion systems capable of
operating in extreme environments over extended periods. This paper presents
the DISTANT (Distant Transmission and Steering Systems) design, a novel
approach for relocating rover traction and steering actuators from
wheel-mounted positions to a thermally protected warm box within the rover
body. The design addresses critical challenges in long-distance traversal
missions by protecting sensitive components from thermal cycling, dust
contamination, and mechanical wear. A double wishbone suspension configuration
with cardan joints and capstan drive steering has been selected as the optimal
architecture following comprehensive trade-off analysis. The system enables
independent wheel traction, steering control, and suspension management whilst
maintaining all motorisation within the protected environment. The design meets
a 50 km traverse requirement without performance degradation, with integrated
dust protection mechanisms and thermal management solutions. Testing and
validation activities are planned for Q1 2026 following breadboard
manufacturing at 1:3 scale.

</details>


### [21] [AI-Enabled Capabilities to Facilitate Next-Generation Rover Surface Operations](https://arxiv.org/abs/2510.05985)
*Cristina Luna,Robert Field,Steven Kay*

Main category: cs.RO

TL;DR: 本论文介绍了一种结合多种AI技术的系统，提升行星探测器的探索效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 当前行星探测器的低行驶速度限制了探索效率，因此提出改进方案以提高其自主导航和操作能力。

Method: 集成三种组件，包括远程障碍物检测、多人机器人协作框架以及深度学习地形分类研究。

Result: 本研究提出的集成AI系统可以显著提高行星探测器的自主性和效率，突破当前约10 cm/s的速度限制。

Conclusion: 经过在火星类环境中的现场验证，该系统展示了可行性，并在行进速度、分类精度和操作安全性方面有显著提升。

Abstract: Current planetary rovers operate at traverse speeds of approximately 10 cm/s,
fundamentally limiting exploration efficiency. This work presents integrated AI
systems which significantly improve autonomy through three components: (i) the
FASTNAV Far Obstacle Detector (FOD), capable of facilitating sustained 1.0 m/s
speeds via computer vision-based obstacle detection; (ii) CISRU, a multi-robot
coordination framework enabling human-robot collaboration for in-situ resource
utilisation; and (iii) the ViBEKO and AIAXR deep learning-based terrain
classification studies. Field validation in Mars analogue environments
demonstrated these systems at Technology Readiness Level 4, providing
measurable improvements in traverse speed, classification accuracy, and
operational safety for next-generation planetary missions.

</details>


### [22] [Coordinate-Consistent Localization via Continuous-Time Calibration and Fusion of UWB and SLAM Observations](https://arxiv.org/abs/2510.05992)
*Tien-Dat Nguyen,Thien-Minh Nguyen,Vinh-Hao Nguyen*

Main category: cs.RO

TL;DR: 本文提出了一种新的两阶段方法，通过融合UWB和SLAM数据，实现一致和准确的自主机器人定位。


<details>
  <summary>Details</summary>
Motivation: 解决SLAM每次运行坐标原点重置的问题，同时结合UWB提供的固定锚点以确保跨会话的一致坐标参考。

Method: 提出一种分为两个阶段的方法，校准并融合UWB数据和SLAM数据，以实现一致且准确的定位。

Result: 通过实验验证，在NTU VIRAL数据集的六种无人机飞行场景中，仅使用一次运行的数据进行校准，在后续运行中实现了准确的定位。

Conclusion: 校准一次运行数据足以支持后续运行中的精确定位，且源代码已公开，以促进社区发展。

Abstract: Onboard simultaneous localization and mapping (SLAM) methods are commonly
used to provide accurate localization information for autonomous robots.
However, the coordinate origin of SLAM estimate often resets for each run. On
the other hand, UWB-based localization with fixed anchors can ensure a
consistent coordinate reference across sessions; however, it requires an
accurate assignment of the anchor nodes' coordinates. To this end, we propose a
two-stage approach that calibrates and fuses UWB data and SLAM data to achieve
coordinate-wise consistent and accurate localization in the same environment.
In the first stage, we solve a continuous-time batch optimization problem by
using the range and odometry data from one full run, incorporating height
priors and anchor-to-anchor distance factors to recover the anchors' 3D
positions. For the subsequent runs in the second stage, a sliding-window
optimization scheme fuses the UWB and SLAM data, which facilitates accurate
localization in the same coordinate system. Experiments are carried out on the
NTU VIRAL dataset with six scenarios of UAV flight, and we show that
calibration using data in one run is sufficient to enable accurate localization
in the remaining runs. We release our source code to benefit the community at
https://github.com/ntdathp/slam-uwb-calibration.

</details>


### [23] [Cross-Embodiment Dexterous Hand Articulation Generation via Morphology-Aware Learning](https://arxiv.org/abs/2510.06068)
*Heng Zhang,Kevin Yuchen Ma,Mike Zheng Shou,Weisi Lin,Yan Wu*

Main category: cs.RO

TL;DR: 研究了一种新的方法，通过本征抓握生成适用于不同手型的抓取策略，解决了以往方法中对大型数据集的依赖和特定手型的限制。


<details>
  <summary>Details</summary>
Motivation: 解决多指抓握中的高维关节优化问题，引入了一种不依赖于特定手型且能够在不同手型间泛化的抓取生成方法。

Method: 构建了一个考虑手的形态描述的端到端框架，通过本征抓握集与物体点云和手腕姿态结合，回归低维空间的关节系数，最终解码为全关节的运动。

Result: 提出了一种基于本征抓握的端到端框架，用于跨身体现的抓取生成，显著提高了多指手的抓取成功率。

Conclusion: 新方法在三种灵巧手的仿真测试中，抓取成功率达到91.9%。在对未见手的少样本适应中，成功率为85.6%。

Abstract: Dexterous grasping with multi-fingered hands remains challenging due to
high-dimensional articulations and the cost of optimization-based pipelines.
Existing end-to-end methods require training on large-scale datasets for
specific hands, limiting their ability to generalize across different
embodiments. We propose an eigengrasp-based, end-to-end framework for
cross-embodiment grasp generation. From a hand's morphology description, we
derive a morphology embedding and an eigengrasp set. Conditioned on these,
together with the object point cloud and wrist pose, an amplitude predictor
regresses articulation coefficients in a low-dimensional space, which are
decoded into full joint articulations. Articulation learning is supervised with
a Kinematic-Aware Articulation Loss (KAL) that emphasizes fingertip-relevant
motions and injects morphology-specific structure. In simulation on unseen
objects across three dexterous hands, our model attains a 91.9% average grasp
success rate with less than 0.4 seconds inference per grasp. With few-shot
adaptation to an unseen hand, it achieves 85.6% success on unseen objects in
simulation, and real-world experiments on this few-shot generalized hand
achieve an 87% success rate. The code and additional materials will be made
available upon publication on our project website
https://connor-zh.github.io/cross_embodiment_dexterous_grasping.

</details>


### [24] [Multi-Robot Distributed Optimization for Exploration and Mapping of Unknown Environments using Bioinspired Tactile-Sensor](https://arxiv.org/abs/2510.06085)
*Roman Ibrahimov,Jannik Matthias Heinen*

Main category: cs.RO

TL;DR: 提出了一种基于生物启发的多机器人系统，通过分布式优化实现未知环境的有效探索与映射，结果显示其在多个方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 实现未知环境的有效探索与映射，提升任务分配效率。

Method: 采用生物启发的多机器人系统，通过分布式优化进行环境探索和映射。

Result: 实验表明，该系统在覆盖、碰撞率和地图准确性方面具有良好表现。

Conclusion: 该系统在有效覆盖、最小化碰撞和构建准确的2D地图方面表现出色。

Abstract: This project proposes a bioinspired multi-robot system using Distributed
Optimization for efficient exploration and mapping of unknown environments.
Each robot explores its environment and creates a map, which is afterwards put
together to form a global 2D map of the environment. Inspired by wall-following
behaviors, each robot autonomously explores its neighborhood based on a tactile
sensor, similar to the antenna of a cockroach, mounted on the surface of the
robot. Instead of avoiding obstacles, robots log collision points when they
touch obstacles. This decentralized control strategy ensures effective task
allocation and efficient exploration of unknown terrains, with applications in
search and rescue, industrial inspection, and environmental monitoring. The
approach was validated through experiments using e-puck robots in a simulated
1.5 x 1.5 m environment with three obstacles. The results demonstrated the
system's effectiveness in achieving high coverage, minimizing collisions, and
constructing accurate 2D maps.

</details>


### [25] [Towards Autonomous Tape Handling for Robotic Wound Redressing](https://arxiv.org/abs/2510.06127)
*Xiao Liang,Lu Shen,Peihan Zhang,Soofiyan Atar,Florian Richter,Michael Yip*

Main category: cs.RO

TL;DR: 这项研究提出了一个自主框架，用于改善慢性伤口护理中的胶带操作，强调了胶带初始脱离和安全放置的能力，展示了该方法在自动化伤口护理中的潜力。


<details>
  <summary>Details</summary>
Motivation: 慢性伤口护理对患者造成重大影响，现有护理过程繁琐且需专业人员，亟需自动化技术的介入以降低成本和改善治疗效果。

Method: 采用人类遥操作示范的力反馈模仿学习方法来处理胶带脱离，并使用数值轨迹优化方法确保胶带的光滑粘附和无皱应用。

Result: 通过广泛实验验证，所提方法在定量评估和综合伤口处理流水线中均表现出可靠的性能。

Conclusion: 胶带操作在伤口护理自动化中是一个关键步骤，研究表明所提方法在实验中表现可靠。

Abstract: Chronic wounds, such as diabetic, pressure, and venous ulcers, affect over
6.5 million patients in the United States alone and generate an annual cost
exceeding \$25 billion. Despite this burden, chronic wound care remains a
routine yet manual process performed exclusively by trained clinicians due to
its critical safety demands. We envision a future in which robotics and
automation support wound care to lower costs and enhance patient outcomes. This
paper introduces an autonomous framework for one of the most fundamental yet
challenging subtasks in wound redressing: adhesive tape manipulation.
Specifically, we address two critical capabilities: tape initial detachment
(TID) and secure tape placement. To handle the complex adhesive dynamics of
detachment, we propose a force-feedback imitation learning approach trained
from human teleoperation demonstrations. For tape placement, we develop a
numerical trajectory optimization method based to ensure smooth adhesion and
wrinkle-free application across diverse anatomical surfaces. We validate these
methods through extensive experiments, demonstrating reliable performance in
both quantitative evaluations and integrated wound redressing pipelines. Our
results establish tape manipulation as an essential step toward practical
robotic wound care automation.

</details>


### [26] [Vision-Guided Targeted Grasping and Vibration for Robotic Pollination in Controlled Environments](https://arxiv.org/abs/2510.06146)
*Jaehwan Jeong,Tuan-Anh Vu,Radha Lahoti,Jiawen Wang,Vivek Alumootil,Sangpil Kim,M. Khalid Jawed*

Main category: cs.RO

TL;DR: 该论文提出了一种基于视觉的机器人授粉框架，能在受限环境下实现高效、精准的授粉操作。


<details>
  <summary>Details</summary>
Motivation: 寻求在缺乏风力授粉和商业授粉者被限制的受控农业中，开发高效的授粉替代方案。

Method: 使用RGB-D传感器进行3D植物重构，针对性抓取规划，以及基于物理的振动建模来实现授粉。

Result: 实验表明，主要干的抓取成功率为92.5%，验证了该方法的安全性与有效性。

Conclusion: 本研究展示的机器人系统有效集成了视觉抓取和振动建模，为自动化精准授粉提供了新方案。

Abstract: Robotic pollination offers a promising alternative to manual labor and
bumblebee-assisted methods in controlled agriculture, where wind-driven
pollination is absent and regulatory restrictions limit the use of commercial
pollinators. In this work, we present and validate a vision-guided robotic
framework that uses data from an end-effector mounted RGB-D sensor and combines
3D plant reconstruction, targeted grasp planning, and physics-based vibration
modeling to enable precise pollination. First, the plant is reconstructed in 3D
and registered to the robot coordinate frame to identify obstacle-free grasp
poses along the main stem. Second, a discrete elastic rod model predicts the
relationship between actuation parameters and flower dynamics, guiding the
selection of optimal pollination strategies. Finally, a manipulator with soft
grippers grasps the stem and applies controlled vibrations to induce pollen
release. End-to-end experiments demonstrate a 92.5\% main-stem grasping success
rate, and simulation-guided optimization of vibration parameters further
validates the feasibility of our approach, ensuring that the robot can safely
and effectively perform pollination without damaging the flower. To our
knowledge, this is the first robotic system to jointly integrate vision-based
grasping and vibration modeling for automated precision pollination.

</details>


### [27] [A Preview of HoloOcean 2.0](https://arxiv.org/abs/2510.06160)
*Blake Romrell,Abigail Austin,Braden Meyers,Ryan Anderson,Carter Noh,Joshua G. Mangelson*

Main category: cs.RO

TL;DR: HoloOcean 2.0是一个先进的海洋模拟器，支持多种任务，并引入了多个新特性，包括迁移到Unreal Engine 5.3及ROS2支持。


<details>
  <summary>Details</summary>
Motivation: 随着海洋机器人领域的关注增加，开发更高保真度的海洋仿真系统对于自主海洋机器人开发和验证变得至关重要。

Method: 通过采用Unreal Engine 5.3和Fossen的车辆动力学模型，增强了海洋传感器、物理和视觉渲染的仿真能力。

Result: HoloOcean 2.0推出了一系列新特性，显著提高了海洋模拟的功能和效率，包括更高效的声呐实现和环境生成工具。

Conclusion: HoloOcean 2.0为海洋机器人系统的发展提供了更高的仿真精度和多样性，有助于推动自主海洋机器人的进步。

Abstract: Marine robotics simulators play a fundamental role in the development of
marine robotic systems. With increased focus on the marine robotics field in
recent years, there has been significant interest in developing higher
fidelitysimulation of marine sensors, physics, and visual rendering
capabilities to support autonomous marine robot development and validation.
HoloOcean 2.0, the next major release of HoloOcean, brings state-of-the-art
features under a general marine simulator capable of supporting a variety of
tasks. New features in HoloOcean 2.0 include migration to Unreal Engine (UE)
5.3, advanced vehicle dynamics using models from Fossen, and support for ROS2
using a custom bridge. Additional features are currently in development,
including significantly more efficient ray tracing-based sidescan,
forward-looking, and bathymetric sonar implementations; semantic sensors;
environment generation tools; volumetric environmental effects; and realistic
waves.

</details>


### [28] [DYMO-Hair: Generalizable Volumetric Dynamics Modeling for Robot Hair Manipulation](https://arxiv.org/abs/2510.06199)
*Chengyang Zhao,Uksang Yoo,Arkadeep Narayan Chaudhury,Giljoo Nam,Jonathan Francis,Jeffrey Ichnowski,Jean Oh*

Main category: cs.RO

TL;DR: DYMO-Hair是一种基于模型的机器人发型护理系统，采用新颖的动态学习方法，通过预训练的3D潜在空间实现高效的发型调整，在未见过的发型任务中表现出色，具备实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 美发是日常生活的重要活动，但对行动能力有限的人士来说无法获得，同时对自主机器人系统来说也存在挑战。

Method: 该系统基于一种新颖的动态学习范式，结合动作条件的潜在状态编辑机制及紧凑的三维潜在空间，以提高对发型的泛化能力。

Result: DYMO-Hair的动态模型在模拟中表现优于多个基线，并且在未见过的发型样本上执行封闭环发型设计任务时，最终几何误差平均降低22%，成功率提高42%。

Conclusion: DYMO-Hair为基于模型的机器人护理奠定了基础，致力于推进更具通用性、灵活性和可及性的机器人发型设计系统。

Abstract: Hair care is an essential daily activity, yet it remains inaccessible to
individuals with limited mobility and challenging for autonomous robot systems
due to the fine-grained physical structure and complex dynamics of hair. In
this work, we present DYMO-Hair, a model-based robot hair care system. We
introduce a novel dynamics learning paradigm that is suited for volumetric
quantities such as hair, relying on an action-conditioned latent state editing
mechanism, coupled with a compact 3D latent space of diverse hairstyles to
improve generalizability. This latent space is pre-trained at scale using a
novel hair physics simulator, enabling generalization across previously unseen
hairstyles. Using the dynamics model with a Model Predictive Path Integral
(MPPI) planner, DYMO-Hair is able to perform visual goal-conditioned hair
styling. Experiments in simulation demonstrate that DYMO-Hair's dynamics model
outperforms baselines on capturing local deformation for diverse, unseen
hairstyles. DYMO-Hair further outperforms baselines in closed-loop hair styling
tasks on unseen hairstyles, with an average of 22% lower final geometric error
and 42% higher success rate than the state-of-the-art system. Real-world
experiments exhibit zero-shot transferability of our system to wigs, achieving
consistent success on challenging unseen hairstyles where the state-of-the-art
system fails. Together, these results introduce a foundation for model-based
robot hair care, advancing toward more generalizable, flexible, and accessible
robot hair styling in unconstrained physical environments. More details are
available on our project page: https://chengyzhao.github.io/DYMOHair-web/.

</details>


### [29] [EmbodiedCoder: Parameterized Embodied Mobile Manipulation via Modern Coding Model](https://arxiv.org/abs/2510.06207)
*Zefu Lin,Rongxu Cui,Chen Hanning,Xiangyu Wang,Junjia Xu,Xiaojuan Jin,Chen Wenbo,Hui Zhou,Lue Fan,Wenling Li,Zhaoxiang Zhang*

Main category: cs.RO

TL;DR: EmbodiedCoder是一个无训练要求的机器人操作框架，利用编码模型生成执行轨迹，增强可解释性和通用性。


<details>
  <summary>Details</summary>
Motivation: 当前的机器人控制方法在处理自然语言指令时面临挑战，特别是在多样化环境中表现不佳，主要原因是依赖大型标注数据集和缺乏可解释性。

Method: 通过在代码中基础高层指令，EmbodiedCoder实现了灵活的对象处理和动作合成，而无需额外的数据收集或调整。

Result: 提出了EmbodiedCoder，一个无须训练的框架，能够直接生成可执行的机器人轨迹，实现灵活的对象几何参数化和轨迹合成。

Conclusion: EmbodiedCoder在多样化长期任务中展现出强大的性能，可以有效地推广到新物体和环境，并且提供了解决高层推理与低层控制之间联系的可解释方法。

Abstract: Recent advances in control robot methods, from end-to-end
vision-language-action frameworks to modular systems with predefined
primitives, have advanced robots' ability to follow natural language
instructions. Nonetheless, many approaches still struggle to scale to diverse
environments, as they often rely on large annotated datasets and offer limited
interpretability.In this work, we introduce EmbodiedCoder, a training-free
framework for open-world mobile robot manipulation that leverages coding models
to directly generate executable robot trajectories. By grounding high-level
instructions in code, EmbodiedCoder enables flexible object geometry
parameterization and manipulation trajectory synthesis without additional data
collection or fine-tuning.This coding-based paradigm provides a transparent and
generalizable way to connect perception with manipulation. Experiments on real
mobile robots show that EmbodiedCoder achieves robust performance across
diverse long-term tasks and generalizes effectively to novel objects and
environments.Our results demonstrate an interpretable approach for bridging
high-level reasoning and low-level control, moving beyond fixed primitives
toward versatile robot intelligence. See the project page at:
https://anonymous.4open.science/w/Embodied-Coder/

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [30] [CLAd-VR: Cognitive Load-based Adaptive Training for Machining Tasks in Virtual Reality](https://arxiv.org/abs/2510.05249)
*Bhavya Matam,Adamay Mann,Kachina Studer,Christian Gabbianelli,Sonia Castelo,John Liu,Claudio Silva,Dishita Turakhia*

Main category: cs.HC

TL;DR: 本文介绍了一种名为CLAd-VR的自适应虚拟现实培训系统，旨在针对制造行业的员工技能提升，利用实时EEG感知来调整训练内容，增加学习效果和长期记忆。


<details>
  <summary>Details</summary>
Motivation: 随着制造业对员工技能提升的需求增加，现有的静态培训系统无法满足个性化学习需求，因此需要一种基于学习者认知负荷的自适应培训解决方案。

Method: 系统使用可穿戴EEG设备实时捕捉学员的神经活动，通过LSTM模型对认知负荷进行分类，并根据分类结果调整培训内容。

Result: 系统成功地实现了对学员认知负荷的实时分类，并能够动态调整教学难度和提供适当指导，提升了培训的适应性和效果。

Conclusion: CLAd-VR系统通过实时监测学员的认知负荷，实现了动态调整任务难度和个性化指导，提高了培训的有效性。

Abstract: With the growing need to effectively support workforce upskilling in the
manufacturing sector, virtual reality is gaining popularity as a scalable
training solution. However, most current systems are designed as static,
step-by-step tutorials and do not adapt to a learner's needs or cognitive load,
which is a critical factor in learning and longterm retention. We address this
limitation with CLAd-VR, an adaptive VR training system that integrates
realtime EEG-based sensing to measure the learner's cognitive load and adapt
instruction accordingly, specifically for domain-specific tasks in
manufacturing. The system features a VR training module for a precision
drilling task, designed with multimodal instructional elements including
animations, text, and video. Our cognitive load sensing pipeline uses a
wearable EEG device to capture the trainee's neural activity, which is
processed through an LSTM model to classify their cognitive load as low,
optimal, or high in real time. Based on these classifications, the system
dynamically adjusts task difficulty and delivers adaptive guidance using voice
guidance, visual cues, or ghost hand animations. This paper introduces CLAd-VR
system's architecture, including the EEG sensing hardware, real-time inference
model, and adaptive VR interface.

</details>


### [31] [Chrysalis: A Unified System for Comparing Active Teaching and Passive Learning with AI Agents in Education](https://arxiv.org/abs/2510.05271)
*Prashanth Arun,Vinita Vader,Erya Xu,Brent McCready-Branch,Sarah Seabrook,Kyle Scholz,Ana Crisan,Igor Grossmann,Pascal Poupart*

Main category: cs.HC

TL;DR: 本文比较了使用LLM作为导师与可教代理的学生体验，发现两者在互动模式与学习效果上存在差异，并强调了未来研究的重要性。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型的普及，AI辅助学习的需求显著增加，旨在探讨LLM在教育中的不同应用方式。

Method: 通过在Chrysalis系统上进行的36名参与者的探究性研究，探讨了学生在辅导学习和教学学习这两种模式下的体验和偏好。

Result: 研究揭示了学生在与LLM互动时的偏好，并分析了在这两种互动模式下智力谦逊等构念的差异。

Conclusion: 这项研究首次直接比较了将大型语言模型（LLM）作为导师与作为可教代理之间的效果，并为未来的研究打开了新方向。

Abstract: AI-assisted learning has seen a remarkable uptick over the last few years,
mainly due to the rise in popularity of Large Language Models (LLMs). Their
ability to hold long-form, natural language interactions with users makes them
excellent resources for exploring school- and university-level topics in a
dynamic, active manner. We compare students' experiences when interacting with
an LLM companion in two capacities: tutored learning and learning-by-teaching.
We do this using Chrysalis, an LLM-based system that we have designed to
support both AI tutors and AI teachable agents for any topic. Through a
within-subject exploratory study with 36 participants, we present insights into
student preferences between the two strategies and how constructs such as
intellectual humility vary between these two interaction modes. To our
knowledge, we are the first to conduct a direct comparison study on the effects
of using an LLM as a tutor versus as a teachable agent on multiple topics. We
hope that our work opens up new avenues for future research in this area.

</details>


### [32] [When Should Users Check? A Decision-Theoretic Model of Confirmation Frequency in Multi-Step AI Agent Tasks](https://arxiv.org/abs/2510.05307)
*Jieyu Zhou,Aryan Roy,Sneh Gupta,Daniel Weitekamp,Christopher J. MacLellan*

Main category: cs.HC

TL;DR: 该研究提出了一种新的确认方法，改善了AI代理的用户交互体验，减少了任务完成时间。


<details>
  <summary>Details</summary>
Motivation: 现有的AI代理执行多步骤任务时，用户在执行过程中控制有限，导致确认方法脆弱且易出错。

Method: 通过对48名参与者进行的在被试研究，评估了基于CDCR模式的决策理论模型。

Result: 81%的参与者更喜欢中间确认的方法，任务完成时间减少了13.54%。

Conclusion: 参与者更喜欢中间确认的方法，任务完成时间减少了13.54%。

Abstract: Existing AI agents typically execute multi-step tasks autonomously and only
allow user confirmation at the end. During execution, users have little
control, making the confirm-at-end approach brittle: a single error can cascade
and force a complete restart. Confirming every step avoids such failures, but
imposes tedious overhead. Balancing excessive interruptions against costly
rollbacks remains an open challenge. We address this problem by modeling
confirmation as a minimum time scheduling problem. We conducted a formative
study with eight participants, which revealed a recurring
Confirmation-Diagnosis-Correction-Redo (CDCR) pattern in how users monitor
errors. Based on this pattern, we developed a decision-theoretic model to
determine time-efficient confirmation point placement. We then evaluated our
approach using a within-subjects study where 48 participants monitored AI
agents and repaired their mistakes while executing tasks. Results show that 81
percent of participants preferred our intermediate confirmation approach over
the confirm-at-end approach used by existing systems, and task completion time
was reduced by 13.54 percent.

</details>


### [33] [Exploring Student Choice and the Use of Multimodal Generative AI in Programming Learning](https://arxiv.org/abs/2510.05417)
*Xinying Hou,Ruiwei Xiao,Runlong Ye,Michael Liut,John Stamper*

Main category: cs.HC

TL;DR: 本研究分析了本科生如何选择和使用多模态生成式人工智能工具，目的是理解其在编程学习中的应用和选择标准。


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究探讨了生成式人工智能在编程学习中的好处与潜在问题，但大多数集中于文本到文本的交互，缺乏对多模态工具的研究。

Method: 通过16个自言自语的会话，结合参与者观察和后续半结构化访谈，研究了学生在完成编程问题时对多模态生成式人工智能工具的选择及其依据。

Result: 本研究探讨了本科生在使用多模态生成式人工智能工具时的选择和工作方式，以及他们的选择标准。

Conclusion: 随着多模态通信成为教育领域的趋势，本研究旨在激发对学生如何与多模态生成式人工智能互动的进一步研究。

Abstract: The broad adoption of Generative AI (GenAI) is impacting Computer Science
education, and recent studies found its benefits and potential concerns when
students use it for programming learning. However, most existing explorations
focus on GenAI tools that primarily support text-to-text interaction. With
recent developments, GenAI applications have begun supporting multiple modes of
communication, known as multimodality. In this work, we explored how
undergraduate programming novices choose and work with multimodal GenAI tools,
and their criteria for choices. We selected a commercially available multimodal
GenAI platform for interaction, as it supports multiple input and output
modalities, including text, audio, image upload, and real-time screen-sharing.
Through 16 think-aloud sessions that combined participant observation with
follow-up semi-structured interviews, we investigated student modality choices
for GenAI tools when completing programming problems and the underlying
criteria for modality selections. With multimodal communication emerging as the
future of AI in education, this work aims to spark continued exploration on
understanding student interaction with multimodal GenAI in the context of CS
education.

</details>


### [34] [Bloom: Designing for LLM-Augmented Behavior Change Interactions](https://arxiv.org/abs/2510.05449)
*Matthew Jörke,Defne Genç,Valentin Teutschbein,Shardul Sapkota,Sarah Chung,Paul Schmiedmayer,Maria Ines Campero,Abby C. King,Emma Brunskill,James A. Landay*

Main category: cs.HC

TL;DR: Bloom应用结合LLM和用户界面提升了健康行为转变的心态，促进了身体活动，但行为改变效果在LLM与对照组之间无显著差异。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在促进身体活动方面的潜力，尤其是通过引入更加多样化的交互方式，而不仅限于文本交互。

Method: 开发了一个名为Bloom的应用程序，通过整合基于LLM的健康辅导聊天机器人与成熟的用户界面交互，进行了为期四周的随机对照实验。

Result: LLM条件下的参与者在心理结果上显示出信念增强、享受度提高和自我同情感增加，尽管两个条件都提高了身体活动水平，但在行为改变上未见显著差异。

Conclusion: 大型语言模型（LLMs）在促进健康行为改变方面可能更加有效地影响心态，而非立竿见影地改变行为。

Abstract: Large language models (LLMs) offer novel opportunities to support health
behavior change, yet existing work has narrowly focused on text-only
interactions. Building on decades of HCI research demonstrating the
effectiveness of UI-based interactions, we present Bloom, an application for
physical activity promotion that integrates an LLM-based health coaching
chatbot with established UI-based interactions. As part of Bloom's development,
we conducted a redteaming evaluation and contribute a safety benchmark dataset.
In a four-week randomized field study (N=54) comparing Bloom to a non-LLM
control, we observed important shifts in psychological outcomes: participants
in the LLM condition reported stronger beliefs that activity was beneficial,
greater enjoyment, and more self-compassion. Both conditions significantly
increased physical activity levels, doubling the proportion of participants
meeting recommended weekly guidelines, though we observed no significant
differences between conditions. Instead, our findings suggest that LLMs may be
more effective at shifting mindsets that precede longer-term behavior change.

</details>


### [35] [Two Modes of Reflection: How Temporal, Spatial, and Social Distances Affect Reflective Writing in Family Caregiving](https://arxiv.org/abs/2510.05510)
*Shunpei Norihama,Yuka Iwane,Jo Takezawa,Simo Hosio,Mari Hirano,Naomi Yamashita,Koji Yatani*

Main category: cs.HC

TL;DR: 家庭照顾者的反思写作受心理距离影响，近距促进情感表达，远距有助于客观分析，设计应考虑这些因素。


<details>
  <summary>Details</summary>
Motivation: 探讨个人经历写作对家庭照顾者心理健康的影响，尤其是如何在适当时机进行反思写作。

Method: 通过为期三周的实地研究，使用聊天机器人收集并分析47位照顾者的反思写作，获取958个写作条目和5412个编码片段。

Result: 发现两种反思模式：近距条件下的细节丰富和情感释放，远距条件下的平静客观和认知重评。参与者面临近距离和远距离带来的权衡。

Conclusion: 心理距离影响反思写作的方式，设计应考虑这种影响以支持家庭照顾者的心理健康。

Abstract: Writing about personal experiences can improve well-being, but for family
caregivers, fixed or user-initiated schedules often miss the right moments.
Drawing on Construal Level Theory, we conducted a three-week field study with
47 caregivers using a chatbot that delivered daily reflective writing prompts
and captured temporal, spatial, and social contexts. We collected 958 writing
entries, resulting in 5,412 coded segments. Our Analysis revealed two
reflective modes. Under proximal conditions, participants produced detailed,
emotion-rich, and care recipient-focused narratives that supported emotional
release. Under distal conditions, they generated calmer, self-focused, and
analytic accounts that enabled objective reflection and cognitive reappraisal.
Participants described trade-offs: proximity preserved vivid detail but limited
objectivity, while distance enabled analysis but risked memory loss. This work
contributes empirical evidence of how psychological distances shape reflective
writing and proposes design implications for distance-aware Just-in-Time
Adaptive Interventions for family caregivers' mental health support.

</details>


### [36] [Locability: An Ability-Based Ranking Model for Virtual Reality Locomotion Techniques](https://arxiv.org/abs/2510.05679)
*Rachel L. Franz,Jacob O. Wobbrock*

Main category: cs.HC

TL;DR: 本研究开发了一种机器学习模型，能够根据用户的交互数据和问卷信息高效预测适合用户的虚拟现实移动技术，特别是针对上半身运动障碍的用户，促进虚拟现实系统的可及性。


<details>
  <summary>Details</summary>
Motivation: 随着虚拟现实技术的发展，存在许多移动技术，然而用户在选择最适合自己的技术时面临困难，尤其是存在不同的身体能力时。

Method: 使用机器学习技术，并基于20名有无上半身运动障碍的用户的数据，进行建模以预测用户最快的移动技术排名。

Result: 以92%的准确率预测用户最快的移动技术，预测的移动时间与实际时间相差不超过12%；能够以61%的准确率对六种移动技术进行速度排名，且预测与观察时间相差不超过8%。

Conclusion: 本研究通过机器学习技术预测用户最适合的虚拟现实移动技术，提高了虚拟现实的可及性，特别是针对上半身运动障碍用户。

Abstract: There are over a hundred virtual reality (VR) locomotion techniques that
exist today, with new ones being designed as VR technology evolves. The
different ways of controlling locomotion techniques (e.g., gestures, button
inputs, body movements), along with the diversity of upper-body motor
impairments, can make it difficult for a user to know which locomotion
technique is best suited to their particular abilities. Moreover,
trial-and-error can be difficult, time-consuming, and costly. Using machine
learning techniques and data from 20 people with and without upper-body motor
impairments, we developed a modeling approach to predict a ranked list of a
user's fastest techniques based on questionnaire and interaction data. We found
that a user's fastest technique could be predicted based on interaction data
with 92% accuracy and that predicted locomotion times were within 12% of
observed times. The model we trained could also rank six locomotion techniques
based on speed with 61% accuracy and that predictions were within 8% of
observed times. Our findings contribute to growing research in VR accessibility
by taking an ability-based design approach to adapt systems to users'
abilities.

</details>


### [37] [Vipera: Blending Visual and LLM-Driven Guidance for Systematic Auditing of Text-to-Image Generative AI](https://arxiv.org/abs/2510.05742)
*Yanwei Huang,Wesley Hanwen Deng,Sijia Xiao,Motahhare Eslami,Jason I. Hong,Arpit Narechania,Adam Perer*

Main category: cs.HC

TL;DR: 为了改善对生成AI的审计方法，研究团队开发了Vipera，一个交互式审计界面，帮助审计员更有效地导航和分析AI生成的输出。


<details>
  <summary>Details</summary>
Motivation: 现有的AI审计方法在结构化探索AI生成输出的广泛空间方面面临挑战。

Method: 进行了一项控制实验，涉及24名具有AI审计经验的参与者，以验证Vipera的有效性。

Result: Vipera是一个互动审计界面，利用多个视觉线索（包括场景图）来增强图像理解，并激励审计员组织和探索审计标准。

Conclusion: Vipera有效帮助审计员在庞大的AI输出空间中导航，并在与多种标准互动时组织分析。

Abstract: Despite their increasing capabilities, text-to-image generative AI systems
are known to produce biased, offensive, and otherwise problematic outputs.
While recent advancements have supported testing and auditing of generative AI,
existing auditing methods still face challenges in supporting effectively
explore the vast space of AI-generated outputs in a structured way. To address
this gap, we conducted formative studies with five AI auditors and synthesized
five design goals for supporting systematic AI audits. Based on these insights,
we developed Vipera, an interactive auditing interface that employs multiple
visual cues including a scene graph to facilitate image sensemaking and inspire
auditors to explore and hierarchically organize the auditing criteria.
Additionally, Vipera leverages LLM-powered suggestions to facilitate
exploration of unexplored auditing directions. Through a controlled experiment
with 24 participants experienced in AI auditing, we demonstrate Vipera's
effectiveness in helping auditors navigate large AI output spaces and organize
their analyses while engaging with diverse criteria.

</details>


### [38] [The Interplay of Attention and Memory in Visual Enumeration](https://arxiv.org/abs/2510.05833)
*B. Sankar,Devottama Sen,Dibakar Sen*

Main category: cs.HC

TL;DR: 使用沉浸式虚拟现实系统研究视觉枚举过程中的注意力与记忆之间的关系，发现任务意图是影响表现的主要因素，复杂性增加显著降低准确性和记忆召回能力。


<details>
  <summary>Details</summary>
Motivation: 传统平面屏幕研究无法捕捉人类在大视觉场景中执行视觉枚举的认知动态。

Method: 开发沉浸式虚拟现实系统，结合眼动追踪，进行两阶段实验，参与者进行简单和复杂物体的枚举，根据任务意图和物品空间布局进行系统变化。

Result: 参与者在执行选择性计数时面临显著的认知成本，复杂刺激显著降低准确性和记忆召回，空间布局的影响在高认知任务下次要且统计上不显著。

Conclusion: 真实世界的视觉枚举受到语义处理的认知负担限制，而不仅仅是视觉搜索的机械过程。

Abstract: Humans navigate and understand complex visual environments by subconsciously
quantifying what they see, a process known as visual enumeration. However,
traditional studies using flat screens fail to capture the cognitive dynamics
of this process over the large visual fields of real-world scenes. To address
this gap, we developed an immersive virtual reality system with integrated
eye-tracking to investigate the interplay between attention and memory during
complex enumeration. We conducted a two-phase experiment where participants
enumerated scenes of either simple abstract shapes or complex real-world
objects, systematically varying the task intent (e.g., selective vs. exhaustive
counting) and the spatial layout of items. Our results reveal that task intent
is the dominant factor driving performance, with selective counting imposing a
significant cognitive cost that was dramatically amplified by stimulus
complexity. The semantic processing required for real-world objects reduced
accuracy and suppressed memory recall, while the influence of spatial layout
was secondary and statistically non-significant when a higher-order cognitive
task intent was driving the human behaviour. We conclude that real-world
enumeration is fundamentally constrained by the cognitive load of semantic
processing, not just the mechanics of visual search. Our findings demonstrate
that under high cognitive demand, the effort to understand what we are seeing
directly limits our capacity to remember it.

</details>


### [39] [From "Arbitrary Timberland" To "Skyline Charts": Is Visualization At Risk From The Pollution of Scientific Literature?](https://arxiv.org/abs/2510.05844)
*Lonni Besançon*

Main category: cs.HC

TL;DR: 可视化研究面临造假问题，研究者需共同参与维护研究质量。


<details>
  <summary>Details</summary>
Motivation: 警惕可视化领域内的造假现象及其对学术社区的影响。

Method: 分析可视化研究中存在的欺诈性做法，并提供预防措施。

Result: 识别几种值得关注的可疑做法，并呼吁学者积极参与改进。

Conclusion: 研究者应积极参与修正科学记录，以维护可视化研究的质量。

Abstract: In this essay, I argue that, while visualization research does not seem to be
directly at risk of being corrupted by the current massive wave of polluted
research, certain visualization concepts are being used in fraudulent fashions
and fields close to ours are being targeted. Worse, the society publishing our
work is overwhelmed by thousands of questionable papers that are being,
unfortunately, published. As a community, and if we want our research to remain
as good as it currently is, I argue that we should all get involved with our
variety of skills to help identify and correct the current scientific record. I
thus aim to present a few questionable practices that are worth knowing about
when reviewing for fields using visualization research, and hopefully will
never be useful when reviewing for our main venues. I also argue that our skill
set could become particularly relevant in the future and invite scholars of the
fields to try to get involved.

</details>


### [40] [Taxonomy of User Needs and Actions](https://arxiv.org/abs/2510.06124)
*Renee Shelby,Fernando Diaz,Vinodkumar Prabhakaran*

Main category: cs.HC

TL;DR: 本文介绍了TUNA框架，旨在捕捉用户在对话式AI中的需求和行为，促进更安全和更负责任的对话系统设计。


<details>
  <summary>Details</summary>
Motivation: 随着对话式人工智能的普及，需要一种框架来捕捉用户的适应性和社会实践，以超越现有的对话行为分类。

Method: 通过对1193次人机对话的定性分析，以及理论回顾和不同领域的验证，发展出TUNA框架。

Result: TUNA框架将用户行为组织为一个三层次的层级，涵盖信息获取、合成、程序指导、内容创作、社交互动和元对话等方面。

Conclusion: 提出了TUNA框架，为理解和设计更安全、响应性更强且更具问责性的对话系统提供了系统性的词汇和分类工具。

Abstract: The growing ubiquity of conversational AI highlights the need for frameworks
that capture not only users' instrumental goals but also the situated,
adaptive, and social practices through which they achieve them. Existing
taxonomies of conversational behavior either overgeneralize, remain
domain-specific, or reduce interactions to narrow dialogue functions. To
address this gap, we introduce the Taxonomy of User Needs and Actions (TUNA),
an empirically grounded framework developed through iterative qualitative
analysis of 1193 human-AI conversations, supplemented by theoretical review and
validation across diverse contexts. TUNA organizes user actions into a
three-level hierarchy encompassing behaviors associated with information
seeking, synthesis, procedural guidance, content creation, social interaction,
and meta-conversation. By centering user agency and appropriation practices,
TUNA enables multi-scale evaluation, supports policy harmonization across
products, and provides a backbone for layering domain-specific taxonomies. This
work contributes a systematic vocabulary for describing AI use, advancing both
scholarly understanding and practical design of safer, more responsive, and
more accountable conversational systems.

</details>


### [41] [Observing Interaction Rather Than Interfaces](https://arxiv.org/abs/2510.06156)
*Guillaume Rivière*

Main category: cs.HC

TL;DR: 本论文提出了一种新的实验方法，旨在观察和研究人机交互，而非仅限于界面，推动HCI科学的发展并优化原型设计。


<details>
  <summary>Details</summary>
Motivation: 当前HCI领域存在孤立的经验发现，主要集中于特定的技术和任务，因而需要一种新的方法更好地研究人机交互。

Method: 提出了一种实验方法论，通过应用案例的治疗进行观察，以产生和复制覆盖各种实验条件的结果。

Result: 通过在开发应用原型时进行观察，优化这些原型以更好地完成最终用户任务，最终目标是描述人机交互的物理特性。

Conclusion: 本论文展示了一种新的实验方法论，旨在通过观察人机交互而非仅仅观察界面，来推动HCI科学的发展。

Abstract: The science of Human-Computer Interaction (HCI) is populated by isolated
empirical findings, often tied to specific technologies, designs, and tasks.
This situation probably lies in observing the wrong object of study, that is to
say, observing interfaces rather than interaction. This paper proposes an
experimental methodology, powered by a research methodology, that enables
tackling the ambition of observing interaction (rather than interfaces). These
observations are done during the treatment of applicative cases, allowing to
generate and replicate results covering various experimental conditions,
expressed from the need of end users and the evolution of technologies.
Performing these observations when developing applicative prototypes
illustrating novel technologies' utility allows, in the same time, to benefit
from an optimization of these prototypes to better accomplish end users tasks.
This paper depicts a long term research direction, from generating the initial
observations of interaction properties and their replication, to their
integration, that would then lead to exploring the possible relations existing
between those properties, to end toward the description of human-computer
interaction's physics.

</details>
