{"id": "2511.10826", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.10826", "abs": "https://arxiv.org/abs/2511.10826", "authors": ["Monika Blue Kwapisz", "Yoav Ackerman", "Jennifer Nguyen", "Prashanth Rajivan"], "title": "Surveillance and Disability in Online Proctored Exams: Student Perspectives and Design Implications", "comment": null, "summary": "Online proctoring systems (OPS) are technologies and services that are used to monitor students during an online exam to deter cheating. However, OPS often violates student privacy by implementing overly intrusive surveillance to which students cannot consent meaningfully. The technologies used in OPS have been shown to unfairly flag students with disabilities. Our reflexive thematic analysis of interviews with students who have first-hand experience with online invigilated exams and who have disability accommodations points to their anxiety about the interaction between surveillance and their disabilities, leading to fears about misrepresentation and increased cognitive load on the exam. Students describe the compromises they need to make with their privacy and accommodations to take remote tests and share their privacy values. We present the implications for the design of OPS to mitigate the issues faced by disabled students.", "AI": {"tldr": "\u5728\u7ebf\u76d1\u8003\u7cfb\u7edf\u5bf9\u9690\u79c1\u7684\u8fc7\u5ea6\u76d1\u63a7\u5bf9\u6b8b\u75be\u5b66\u751f\u9020\u6210\u4e86\u4e0d\u516c\u5e73\u5f71\u54cd\uff0c\u589e\u52a0\u4e86\u4ed6\u4eec\u7684\u7126\u8651\u548c\u8ba4\u77e5\u8d1f\u62c5\u3002", "motivation": "\u7814\u7a76\u5728\u7ebf\u76d1\u8003\u7cfb\u7edf\u5bf9\u5b66\u751f\u9690\u79c1\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u7cfb\u7edf\u5982\u4f55\u5bf9\u6b8b\u75be\u5b66\u751f\u9020\u6210\u4e0d\u516c\u5e73\u7684\u8d1f\u62c5\u3002", "method": "\u5bf9\u6709\u5728\u7ebf\u76d1\u8003\u7ecf\u5386\u7684\u6b8b\u75be\u5b66\u751f\u8fdb\u884c\u53cd\u601d\u6027\u4e3b\u9898\u5206\u6790\u7684\u8bbf\u8c08\u3002", "result": "\u901a\u8fc7\u5bf9\u7ecf\u5386\u8fc7\u5728\u7ebf\u76d1\u8003\u7684\u6b8b\u75be\u5b66\u751f\u8fdb\u884c\u8bbf\u8c08\uff0c\u63ed\u793a\u4e86\u76d1\u63a7\u4e0e\u6b8b\u75be\u4e4b\u95f4\u7684\u7d27\u5f20\u5173\u7cfb\u53ca\u5176\u5bf9\u5b66\u751f\u7684\u5fc3\u7406\u8d1f\u62c5\u3002", "conclusion": "\u5e94\u91cd\u65b0\u8bbe\u8ba1\u5728\u7ebf\u76d1\u8003\u7cfb\u7edf\uff0c\u4ee5\u4fdd\u62a4\u6b8b\u75be\u5b66\u751f\u7684\u9690\u79c1\u548c\u516c\u5e73\u8003\u8bd5\u3002"}}
{"id": "2511.11112", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.11112", "abs": "https://arxiv.org/abs/2511.11112", "authors": ["Yihan Hou", "Yilin Ye", "Liangwei Wang", "Huamin Qu", "Wei Zeng"], "title": "C2Views: Knowledge-based Colormap Design for Multiple-View Consistency", "comment": "15 pages, 10 figures. Accepted to Proceedings of the Pacific Conference on Computer Graphics and Applications, 2025", "summary": "Multiple-view (MV) visualization provides a comprehensive and integrated perspective on complex data, establishing itself as an effective method for visual communication and exploratory data analysis. While existing studies have predominantly focused on designing explicit visual linkages and coordinated interactions to facilitate the exploration of MV visualizations, these approaches often demand extra graphical and interactive effort, overlooking the potential of color as an effective channel for encoding data and relationships. Addressing this oversight, we introduce C2Views, a new framework for colormap design that implicitly shows the relation across views. We begin by structuring the components and their relationships within MVs into a knowledge-based graph specification, wherein colormaps, data, and views are denoted as entities, and the interactions among them are illustrated as relations. Building on this representation, we formulate the design criteria as an optimization problem and employ a genetic algorithm enhanced by Pareto optimality, generating colormaps that balance single-view effectiveness and multiple-view consistency. Our approach is further complemented with an interactive interface for user-intended refinement. We demonstrate the feasibility of C2Views through various colormap design examples for MVs, underscoring its adaptability to diverse data relationships and view layouts. Comparative user studies indicate that our method outperforms the existing approach in facilitating color distinction and enhancing multiple-view consistency, thereby simplifying data exploration processes.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51faC2Views\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u8272\u56fe\u8bbe\u8ba1\uff0c\u589e\u5f3a\u591a\u89c6\u56fe\u4e00\u81f4\u6027\uff0c\u63d0\u5347\u6570\u636e\u63a2\u7d22\u4f53\u9a8c\u3002", "motivation": "\u8fc7\u5f80\u7814\u7a76\u8fc7\u4e8e\u6ce8\u91cd\u663e\u5f0f\u8fde\u63a5\u4e0e\u4ea4\u4e92\uff0c\u5ffd\u7565\u4e86\u989c\u8272\u5728\u6570\u636e\u7f16\u7801\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51faC2Views\u6846\u67b6\uff0c\u5229\u7528\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u8272\u56fe\u8bbe\u8ba1\uff0c\u589e\u5f3a\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u3002", "result": "C2Views\u751f\u6210\u7684\u8272\u56fe\u5728\u7b80\u5316\u6570\u636e\u63a2\u7d22\u8fc7\u7a0b\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "C2Views\u6709\u6548\u6574\u5408\u4e86\u989c\u8272\u7f16\u7801\u4e0e\u591a\u89c6\u56fe\u5173\u7cfb\uff0c\u63d0\u5347\u4e86\u89c6\u89c9\u4f20\u8fbe\u6548\u679c\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u7684\u6570\u636e\u5173\u7cfb\u3002"}}
{"id": "2511.11187", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.11187", "abs": "https://arxiv.org/abs/2511.11187", "authors": ["Ludwig Felder", "Jacob Miller", "Markus Wallinger", "Stephen Kobourov", "Chunyang Chen"], "title": "ReTrace: Interactive Visualizations for Reasoning Traces of Large Reasoning Models", "comment": null, "summary": "Recent advances in Large Language Models have led to Large Reasoning Models, which produce step-by-step reasoning traces. These traces offer insight into how models think and their goals, improving explainability and helping users follow the logic, learn the process, and even debug errors. These traces, however, are often verbose and complex, making them cognitively demanding to comprehend. We address this challenge with ReTrace, an interactive system that structures and visualizes textual reasoning traces to support understanding. We use a validated reasoning taxonomy to produce structured reasoning data and investigate two types of interactive visualizations thereof. In a controlled user study, both visualizations enabled users to comprehend the model's reasoning more accurately and with less perceived effort than a raw text baseline. The results of this study could have design implications for making long and complex machine-generated reasoning processes more usable and transparent, an important step in AI explainability.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86ReTrace\uff0c\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u548c\u53ef\u89c6\u5316\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u63d0\u5347\u7528\u6237\u5bf9\u6a21\u578b\u601d\u7ef4\u65b9\u5f0f\u7684\u7406\u89e3\uff0c\u51cf\u5c11\u8ba4\u77e5\u8d1f\u62c5\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u6b65\uff0c\u7528\u6237\u9700\u7406\u89e3\u590d\u6742\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u63d0\u5347\u8fd9\u4e9b\u63a8\u7406\u7684\u53ef\u7406\u89e3\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u4f7f\u7528\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u63a8\u7406\u5206\u7c7b\u6cd5\u751f\u6210\u7ed3\u6784\u5316\u63a8\u7406\u6570\u636e\uff0c\u5e76\u8c03\u67e5\u4e24\u79cd\u7c7b\u578b\u7684\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u3002\u5728\u53d7\u63a7\u7528\u6237\u7814\u7a76\u4e2d\uff0c\u8bc4\u4f30\u4e86\u8fd9\u4e24\u79cd\u53ef\u89c6\u5316\u7684\u6709\u6548\u6027\u3002", "result": "\u901a\u8fc7\u7528\u6237\u7814\u7a76\uff0c\u53d1\u73b0\u4f7f\u7528\u53ef\u89c6\u5316\u7684\u7528\u6237\u5728\u7406\u89e3\u6a21\u578b\u63a8\u7406\u65f6\u66f4\u51c6\u786e\u4e14\u611f\u77e5\u52aa\u529b\u66f4\u5c11\uff0c\u76f8\u6bd4\u4e8e\u539f\u59cb\u6587\u672c\u57fa\u7ebf\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cReTrace\u7cfb\u7edf\u7684\u53ef\u89c6\u5316\u4f7f\u7528\u6237\u80fd\u591f\u66f4\u51c6\u786e\u3001\u8f7b\u677e\u5730\u7406\u89e3\u6a21\u578b\u7684\u63a8\u7406\uff0c\u5177\u6709\u91cd\u8981\u7684AI\u53ef\u89e3\u91ca\u6027\u8bbe\u8ba1\u610f\u4e49\u3002"}}
{"id": "2511.11209", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.11209", "abs": "https://arxiv.org/abs/2511.11209", "authors": ["Piero Romare", "Farzaneh Karegar", "Simone Fischer-H\u00fcbner"], "title": "Towards Usable Privacy Management for IoT TAPs: Deriving Privacy Clusters and Preference Profiles", "comment": null, "summary": "IoT Trigger-Action Platforms (TAPs) typically offer coarse-grained permission controls. Even when fine-grained controls are available, users are likely overwhelmed by the complexity of setting privacy preferences. This paper contributes to usable privacy management for TAPs by deriving privacy clusters and profiles for different types of users that can be semi-automatically assigned or suggested to them. We developed and validated a questionnaire, based on users' privacy concerns regarding confidentiality and control and their requirements towards transparency in TAPs. In an online study (N=301), where participants were informed about potential privacy risks, we clustered users by their privacy concerns and requirements into Basic, Medium and High Privacy clusters. These clusters were then characterized by the users' data sharing preferences, based on a factorial vignette approach, considering the data categories, the data recipient types, and the purpose of data sharing. Our findings show three distinct privacy profiles, providing a foundation for more usable privacy controls in TAPs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u7528\u7684\u9690\u79c1\u7ba1\u7406\u65b9\u6cd5\uff0c\u9488\u5bf9\u4e0d\u540c\u7528\u6237\u7fa4\u4f53\u7684\u9690\u79c1\u5173\u5207\u4e0e\u9700\u6c42\u8fdb\u884c\u805a\u7c7b\uff0c\u5f62\u6210\u4e09\u79cd\u9690\u79c1\u6863\u6848\uff0c\u4e3aTAP\u63d0\u4f9b\u66f4\u53cb\u597d\u7684\u9690\u79c1\u63a7\u5236\u3002", "motivation": "\u6539\u5584\u7269\u8054\u7f51\u89e6\u53d1-\u52a8\u4f5c\u5e73\u53f0\u7684\u9690\u79c1\u7ba1\u7406\uff0c\u7279\u522b\u662f\u5728\u7528\u6237\u9700\u6c42\u4e0e\u9690\u79c1\u504f\u597d\u914d\u7f6e\u65f6\uff0c\u964d\u4f4e\u590d\u6742\u6027\u3002", "method": "\u5f00\u53d1\u548c\u9a8c\u8bc1\u4e86\u4e00\u4efd\u95ee\u5377\uff0c\u4ee5\u7528\u6237\u5bf9\u9690\u79c1\u7684\u5173\u6ce8\u4e3a\u57fa\u7840\uff0c\u5bf9\u7528\u6237\u8fdb\u884c\u805a\u7c7b\u5206\u6790\u3002", "result": "\u751f\u6210\u4e86\u4e09\u4e2a\u4e0d\u540c\u7684\u9690\u79c1\u6863\u6848\uff08\u57fa\u7840\u3001\u4e2d\u7b49\u548c\u9ad8\u9690\u79c1\uff09\uff0c\u4e3aTAPs\u7684\u53ef\u7528\u9690\u79c1\u63a7\u5236\u63d0\u4f9b\u57fa\u7840\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u7528\u6237\u9690\u79c1\u504f\u597d\u7684\u591a\u6837\u6027\uff0c\u4e3aTAP\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u53c2\u8003\uff0c\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u9690\u79c1\u4fdd\u62a4\u3002"}}
{"id": "2511.10762", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.10762", "abs": "https://arxiv.org/abs/2511.10762", "authors": ["Nikolaos Tsagkas", "Andreas Sochopoulos", "Duolikun Danier", "Sethu Vijayakumar", "Alexandros Kouris", "Oisin Mac Aodha", "Chris Xiaoxuan Lu"], "title": "Attentive Feature Aggregation or: How Policies Learn to Stop Worrying about Robustness and Attend to Task-Relevant Visual Cues", "comment": "This paper stems from a split of our earlier work \"When Pre-trained Visual Representations Fall Short: Limitations in Visuo-Motor Robot Learning.\" While \"The Temporal Trap\" replaces the original and focuses on temporal entanglement, this companion study examines policy robustness and task-relevant visual cue selection", "summary": "The adoption of pre-trained visual representations (PVRs), leveraging features from large-scale vision models, has become a popular paradigm for training visuomotor policies. However, these powerful representations can encode a broad range of task-irrelevant scene information, making the resulting trained policies vulnerable to out-of-domain visual changes and distractors. In this work we address visuomotor policy feature pooling as a solution to the observed lack of robustness in perturbed scenes. We achieve this via Attentive Feature Aggregation (AFA), a lightweight, trainable pooling mechanism that learns to naturally attend to task-relevant visual cues, ignoring even semantically rich scene distractors. Through extensive experiments in both simulation and the real world, we demonstrate that policies trained with AFA significantly outperform standard pooling approaches in the presence of visual perturbations, without requiring expensive dataset augmentation or fine-tuning of the PVR. Our findings show that ignoring extraneous visual information is a crucial step towards deploying robust and generalisable visuomotor policies. Project Page: tsagkas.github.io/afa", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7279\u5f81\u6c60\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u63d0\u9ad8\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5728\u5e72\u6270\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u8bc1\u660e\u4e86\u7cbe\u7b80\u89c6\u89c9\u4fe1\u606f\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u89c6\u89c9\u8868\u793a\u867d\u7136\u6709\u6548\uff0c\u4f46\u5728\u590d\u6742\u573a\u666f\u4e2d\u6613\u53d7\u5230\u4e0d\u76f8\u5173\u4fe1\u606f\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u8bad\u7ec3\u7b56\u7565\u7684\u8106\u5f31\u6027\uff0c\u9700\u5bfb\u627e\u89e3\u51b3\u65b9\u6848\u4ee5\u589e\u5f3a\u7b56\u7565\u7684\u9c81\u68d2\u6027\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u6ce8\u610f\u7279\u5f81\u805a\u5408\uff08AFA\uff09\u673a\u5236\uff0c\u81ea\u52a8\u805a\u7126\u4e8e\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u89c6\u89c9\u7ebf\u7d22\uff0c\u5ffd\u7565\u573a\u666f\u4e2d\u7684\u5e72\u6270\u4fe1\u606f\u3002", "result": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u53ef\u8bad\u7ec3\u7279\u5f81\u6c60\u5316\u673a\u5236\uff08AFA\uff09\uff0c\u7528\u4e8e\u63d0\u5347\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u7684\u9c81\u68d2\u6027\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u9884\u8bad\u7ec3\u89c6\u89c9\u8868\u793a\u5728\u9762\u5bf9\u6270\u52a8\u573a\u666f\u65f6\u7684\u8106\u5f31\u6027\u95ee\u9898\u3002", "conclusion": "\u6211\u4eec\u53d1\u73b0\uff0c\u5ffd\u89c6\u65e0\u5173\u7684\u89c6\u89c9\u4fe1\u606f\u662f\u90e8\u7f72\u9c81\u68d2\u4e14\u53ef\u6cdb\u5316\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u7684\u91cd\u8981\u6b65\u9aa4\u3002"}}
{"id": "2511.11229", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.11229", "abs": "https://arxiv.org/abs/2511.11229", "authors": ["Pavlos Panagiotidis", "Jocelyn Spence", "Nils Jager"], "title": "Devising Experiments with Interactive Environments", "comment": "Presented at Performing Space 2025 (Organised by the Performing Space Association & University of the Peloponnese), Nafplio, Greece, 4-7 July 2025", "summary": "This paper reports a practice-based investigation into authoring responsive light and sound in immersive performance without writing code. A modular system couples live gesture, position, and speech inputs to scenographic outputs through a visual logic layer that performers can operate in rehearsal. Across six workshops with eight professional performance-makers, we staged a progression from parallel ensemble and technical training to integrated dramaturgy, culminating in a single-spectator scratch immersive performance with interactive elements. This paper details the system's building blocks and the workshop arc. A reflexive reading of workshop video logs, post-workshop focus groups, and facilitator notes surfaced three ensemble-level strategies that made the technology workable in a hybrid devising/design practice: rotating roles between operator, performer, and mediator; embracing controlled imperfection as a creative resource; and using technology-describing metaphors to support creative practice.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u65e0\u7f16\u7801\u7684\u54cd\u5e94\u5f0f\u5149\u58f0\u521b\u4f5c\uff0c\u5f00\u53d1\u4e86\u6a21\u5757\u5316\u7cfb\u7edf\uff0c\u7ecf\u8fc7\u5de5\u4f5c\u574a\u5b9e\u8df5\uff0c\u603b\u7ed3\u51fa\u4e09\u79cd\u96c6\u4f53\u7b56\u7565\u3002", "motivation": "\u8fd9\u4e2a\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u65e0\u7f16\u7801\u7684\u521b\u4f5c\u65b9\u5f0f\uff0c\u63d0\u9ad8\u8868\u6f14\u8005\u5728\u6c89\u6d78\u5f0f\u8868\u6f14\u4e2d\u5bf9\u6280\u672f\u7684\u64cd\u4f5c\u80fd\u529b\uff0c\u4fc3\u8fdb\u5176\u521b\u4f5c\u81ea\u7531\u3002", "method": "\u901a\u8fc7\u516d\u4e2a\u5de5\u4f5c\u574a\u7684\u5b9e\u8df5\uff0c\u4ee5\u53ca\u5206\u6790\u5f55\u50cf\u3001\u7126\u70b9\u5c0f\u7ec4\u8ba8\u8bba\u548c\u6307\u5bfc\u8005\u7b14\u8bb0\uff0c\u5f62\u6210\u4e86\u4e00\u5957\u6709\u6548\u7684\u5de5\u4f5c\u65b9\u6cd5\u3002", "result": "\u8be5\u8bba\u6587\u62a5\u544a\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5b9e\u8df5\u7684\u7814\u7a76\uff0c\u63a2\u8ba8\u4e86\u5982\u4f55\u5728\u6c89\u6d78\u5f0f\u8868\u6f14\u4e2d\u65e0\u7f16\u7801\u5730\u521b\u4f5c\u54cd\u5e94\u5f0f\u5149\u548c\u58f0\u97f3\u3002\u4f5c\u8005\u5f00\u53d1\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u7cfb\u7edf\uff0c\u5c06\u5b9e\u65f6\u624b\u52bf\u3001\u4f4d\u7f6e\u548c\u8bed\u97f3\u8f93\u5165\u4e0e\u573a\u666f\u8f93\u51fa\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u4e00\u4e2a\u53ef\u89c6\u5316\u903b\u8f91\u5c42\u4f7f\u8868\u6f14\u8005\u80fd\u591f\u5728\u6392\u7ec3\u4e2d\u64cd\u4f5c\u3002\u7ecf\u8fc7\u516d\u4e2a\u5de5\u4f5c\u574a\u7684\u5b9e\u8df5\uff0c\u6700\u7ec8\u5448\u73b0\u51fa\u4e00\u4e2a\u5305\u542b\u4e92\u52a8\u5143\u7d20\u7684\u5355\u89c2\u4f17\u6c89\u6d78\u5f0f\u8868\u6f14\u3002\u8bba\u6587\u8be6\u7ec6\u53d9\u8ff0\u4e86\u7cfb\u7edf\u7684\u6784\u5efa\u6a21\u5757\u548c\u5de5\u4f5c\u574a\u7684\u6f14\u53d8\u8fc7\u7a0b\uff0c\u4ee5\u53ca\u901a\u8fc7\u5206\u6790\u5de5\u4f5c\u574a\u5f55\u50cf\u3001\u540e\u671f\u7126\u70b9\u5c0f\u7ec4\u8ba8\u8bba\u548c\u6307\u5bfc\u8005\u7b14\u8bb0\uff0c\u63d0\u51fa\u7684\u4e09\u79cd\u4f7f\u6280\u672f\u5728\u6df7\u5408\u521b\u4f5c/\u8bbe\u8ba1\u5b9e\u8df5\u4e2d\u53ef\u884c\u7684\u96c6\u4f53\u7b56\u7565\uff1a\u89d2\u8272\u8f6e\u6362\u3001\u63a5\u7eb3\u53d7\u63a7\u7684\u4e0d\u5b8c\u7f8e\u4f5c\u4e3a\u521b\u9020\u6027\u8d44\u6e90\uff0c\u4ee5\u53ca\u4f7f\u7528\u6280\u672f\u63cf\u8ff0\u9690\u55bb\u6765\u652f\u6301\u521b\u4f5c\u5b9e\u8df5\u3002", "conclusion": "\u7eb8\u4e0a\u603b\u7ed3\u7684\u4e09\u79cd\u7b56\u7565\u4e3a\u6280\u672f\u4e0e\u521b\u4f5c\u5b9e\u8df5\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86\u6c89\u6d78\u5f0f\u8868\u6f14\u7684\u521b\u65b0\u3002"}}
{"id": "2511.10770", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.10770", "abs": "https://arxiv.org/abs/2511.10770", "authors": ["Samson Oruma", "Ricardo Colomo-Palacios", "Vasileios Gkioulos"], "title": "From Framework to Reliable Practice: End-User Perspectives on Social Robots in Public Spaces", "comment": "26 pages, 3 figures", "summary": "As social robots increasingly enter public environments, their acceptance depends not only on technical reliability but also on ethical integrity, accessibility, and user trust. This paper reports on a pilot deployment of an ARI social robot functioning as a university receptionist, designed in alignment with the SecuRoPS framework for secure and ethical social robot deployment. Thirty-five students and staff interacted with the robot and provided structured feedback on safety, privacy, usability, accessibility, and transparency. The results show generally positive perceptions of physical safety, data protection, and ethical behavior, while also highlighting challenges related to accessibility, inclusiveness, and dynamic interaction. Beyond the empirical findings, the study demonstrates how theoretical frameworks for ethical and secure design can be implemented in real-world contexts through end-user evaluation. It also provides a public GitHub repository containing reusable templates for ARI robot applications to support reproducibility and lower the entry barrier for new researchers. By combining user perspectives with practical technical resources, this work contributes to ongoing discussions in AI and society and supports the development of trustworthy, inclusive, and ethically responsible social robots for public spaces.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u793e\u4ea4\u673a\u5668\u4eba\u7684\u4f26\u7406\u53ca\u7528\u6237\u4fe1\u4efb\uff0c\u5c55\u793a\u4e86\u5728\u5927\u5b66\u73af\u5883\u4e2d\u4f7f\u7528\u793e\u4ea4\u673a\u5668\u4eba\u5e76\u6536\u96c6\u53cd\u9988\u7684\u7ecf\u9a8c\uff0c\u65e8\u5728\u63a8\u52a8\u66f4\u4e3a\u53ef\u4fe1\u548c\u5305\u5bb9\u7684\u793e\u4ea4\u673a\u5668\u4eba\u5f00\u53d1\u3002", "motivation": "\u968f\u7740\u793e\u4ea4\u673a\u5668\u4eba\u5728\u516c\u5171\u73af\u5883\u4e2d\u7684\u5e94\u7528\u589e\u52a0\uff0c\u63a2\u8ba8\u5176\u5728\u6280\u672f\u53ef\u9760\u6027\u4e4b\u5916\u7684\u4f26\u7406\u5b8c\u6574\u6027\u3001\u53ef\u63a5\u53d7\u6027\u548c\u7528\u6237\u4fe1\u4efb\u7684\u5fc5\u8981\u6027\u3002", "method": "\u5b9e\u65bd\u4e86\u9488\u5bf9\u793e\u4ea4\u673a\u5668\u4eba\u5728\u5927\u5b66\u73af\u5883\u4e2d\u7684\u4f7f\u7528\u7684\u8bd5\u70b9\u90e8\u7f72\uff0c\u6536\u96c6\u4e8635\u540d\u5e08\u751f\u7684\u53cd\u9988\uff0c\u8bc4\u4f30\u673a\u5668\u4eba\u7684\u5b89\u5168\u6027\u3001\u9690\u79c1\u3001\u53ef\u7528\u6027\u3001\u53ef\u53ca\u6027\u548c\u900f\u660e\u5ea6\u3002", "result": "\u8c03\u67e5\u7ed3\u679c\u8868\u660e\uff0c\u7528\u6237\u5bf9\u673a\u5668\u4eba\u7684\u7269\u7406\u5b89\u5168\u3001\u6570\u636e\u4fdd\u62a4\u548c\u4f26\u7406\u884c\u4e3a\u6301\u79ef\u6781\u770b\u6cd5\uff0c\u4f46\u5728\u53ef\u53ca\u6027\u3001\u5305\u5bb9\u6027\u548c\u52a8\u6001\u4ea4\u4e92\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u7528\u6237\u8bc4\u4f30\u5c55\u793a\u4e86\u7406\u8bba\u6846\u67b6\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u5b9e\u65bd\uff0c\u4e3a\u793e\u4f1a\u673a\u5668\u4eba\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4f26\u7406\u4e0e\u5b89\u5168\u6027\u7684\u53c2\u8003\uff0c\u540c\u65f6\u652f\u6301\u53ef\u91cd\u590d\u6027\u548c\u65b0\u7814\u7a76\u8005\u7684\u8fdb\u5165\u3002"}}
{"id": "2511.11287", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.11287", "abs": "https://arxiv.org/abs/2511.11287", "authors": ["Sven Schultze", "Meike Verena Kietzmann", "Nils-Lucas Sch\u00f6nfeld", "Ruth Stock-Homburg"], "title": "Building the Web for Agents: A Declarative Framework for Agent-Web Interaction", "comment": "for associated documentation, see https://svenschultze.github.io/VOIX/", "summary": "The increasing deployment of autonomous AI agents on the web is hampered by a fundamental misalignment: agents must infer affordances from human-oriented user interfaces, leading to brittle, inefficient, and insecure interactions. To address this, we introduce VOIX, a web-native framework that enables websites to expose reliable, auditable, and privacy-preserving capabilities for AI agents through simple, declarative HTML elements. VOIX introduces <tool> and <context> tags, allowing developers to explicitly define available actions and relevant state, thereby creating a clear, machine-readable contract for agent behavior. This approach shifts control to the website developer while preserving user privacy by disconnecting the conversational interactions from the website. We evaluated the framework's practicality, learnability, and expressiveness in a three-day hackathon study with 16 developers. The results demonstrate that participants, regardless of prior experience, were able to rapidly build diverse and functional agent-enabled web applications. Ultimately, this work provides a foundational mechanism for realizing the Agentic Web, enabling a future of seamless and secure human-AI collaboration on the web.", "AI": {"tldr": "VOIX\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u8fc7\u7b80\u6d01\u7684HTML\u6807\u7b7e\u6765\u5b9a\u4e49AI\u4ee3\u7406\u53ef\u7528\u529f\u80fd\u7684\u65b0\u6846\u67b6\uff0c\u4ece\u800c\u4fc3\u8fdb\u4eba\u673a\u534f\u4f5c\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u81ea\u4e3bAI\u4ee3\u7406\u5728\u7f51\u7edc\u4e0a\u7684\u90e8\u7f72\u53d7\u5230\u4eba\u673a\u4ea4\u4e92\u754c\u9762\u4e0d\u5339\u914d\u7684\u5236\u7ea6\uff0c\u5bfc\u81f4\u4ea4\u4e92\u8106\u5f31\u3001\u6548\u7387\u4f4e\u4e0b\u548c\u4e0d\u5b89\u5168\u3002", "method": "VOIX\u5f15\u5165\u4e86<tool>\u548c<context>\u6807\u7b7e\uff0c\u5141\u8bb8\u5f00\u53d1\u8005\u660e\u786e\u5730\u5b9a\u4e49\u53ef\u7528\u52a8\u4f5c\u548c\u76f8\u5173\u72b6\u6001\uff0c\u5728\u5f00\u53d1\u8005\u4e0e\u7528\u6237\u4e4b\u95f4\u5efa\u7acb\u6e05\u6670\u7684\u673a\u5668\u53ef\u8bfb\u5408\u540c\u3002\u901a\u8fc7\u4e09\u5929\u7684\u9ed1\u5ba2\u9a6c\u62c9\u677e\u7814\u7a76\uff0c\u8bc4\u4f30\u4e86\u6846\u67b6\u7684\u5b9e\u7528\u6027\u3001\u5b66\u4e60\u80fd\u529b\u548c\u8868\u73b0\u529b\uff0c\u7ed3\u679c\u663e\u793a\u53c2\u4e0e\u8005\u80fd\u591f\u5feb\u901f\u6784\u5efa\u591a\u6837\u5316\u7684\u529f\u80fd\u6027\u4ee3\u7406\u652f\u6301\u7684Web\u5e94\u7528\u3002", "result": "\u6211\u4eec\u63d0\u51fa\u4e86VOIX\uff0c\u4e00\u4e2a\u7f51\u7edc\u539f\u751f\u6846\u67b6\uff0c\u4f7f\u7f51\u7ad9\u80fd\u591f\u901a\u8fc7\u7b80\u5355\u7684HTML\u5143\u7d20\u5411AI\u4ee3\u7406\u66b4\u9732\u53ef\u9760\u3001\u53ef\u5ba1\u8ba1\u548c\u4fdd\u62a4\u9690\u79c1\u7684\u529f\u80fd\u3002", "conclusion": "\u8fd9\u4e00\u5de5\u4f5c\u63d0\u4f9b\u4e86\u5b9e\u73b0\u4ee3\u7406\u7f51\u7edc\u7684\u57fa\u7840\u673a\u5236\uff0c\u4f7f\u672a\u6765\u4eba\u7c7b\u4e0eAI\u5728\u7f51\u7edc\u4e0a\u7684\u534f\u4f5c\u66f4\u52a0\u65e0\u7f1d\u548c\u5b89\u5168\u3002"}}
{"id": "2511.10792", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.10792", "abs": "https://arxiv.org/abs/2511.10792", "authors": ["Eric Mugford", "Jonathan D. Gammell"], "title": "$\\rm{A}^{\\rm{SAR}}$: $\\varepsilon$-Optimal Graph Search for Minimum Expected-Detection-Time Paths with Path Budget Constraints for Search and Rescue", "comment": "Submitted to IEEE International Conference on Robotics and Automation (ICRA) 2026, 8 pages, 4 figures, 2 tables. The corresponding video can be found at https://www.youtube.com/watch?v=R73-YKWY78M", "summary": "Searches are conducted to find missing persons and/or objects given uncertain information, imperfect observers and large search areas in Search and Rescue (SAR). In many scenarios, such as Maritime SAR, expected survival times are short and optimal search could increase the likelihood of success. This optimization problem is complex for nontrivial problems given its probabilistic nature.\n  Stochastic optimization methods search large problems by nondeterministically sampling the space to reduce the effective size of the problem. This has been used in SAR planning to search otherwise intractably large problems but the stochastic nature provides no formal guarantees on the quality of solutions found in finite time.\n  This paper instead presents $\\rm{A}^{\\rm{SAR}}$, an $\\varepsilon$-optimal search algorithm for SAR planning. It calculates a heuristic to bound the search space and uses graph-search methods to find solutions that are formally guaranteed to be within a user-specified factor, $\\varepsilon$, of the optimal solution. It finds better solutions faster than existing optimization approaches in operational simulations. It is also demonstrated with a real-world field trial on Lake Ontario, Canada, where it was used to locate a drifting manikin in only 150s.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684SAR\u89c4\u5212\u7b97\u6cd5A^SAR\uff0c\u80fd\u591f\u5728\u4fdd\u8bc1\u89e3\u7684\u8d28\u91cf\u4fdd\u8bc1\u7684\u540c\u65f6\uff0c\u52a0\u901f\u641c\u7d22\u8fc7\u7a0b\u3002", "motivation": "\u5728\u641c\u6551(SAR)\u4e2d\uff0c\u9762\u5bf9\u4e0d\u786e\u5b9a\u7684\u4fe1\u606f\u3001\u89c2\u5bdf\u8005\u7684\u4e0d\u5b8c\u7f8e\u4ee5\u53ca\u5e9e\u5927\u7684\u641c\u7d22\u533a\u57df\uff0c\u4f18\u5316\u641c\u7d22\u4ee5\u63d0\u5347\u6210\u529f\u6982\u7387\u7684\u91cd\u8981\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aA^SAR\u7684\u03b5-\u6700\u4f18\u641c\u7d22\u7b97\u6cd5\uff0c\u5229\u7528\u542f\u53d1\u5f0f\u65b9\u6cd5\u6765\u9650\u5236\u641c\u7d22\u7a7a\u95f4\uff0c\u5e76\u5e94\u7528\u56fe\u641c\u7d22\u65b9\u6cd5\u627e\u5230\u89e3\u3002", "result": "A^SAR\u7b97\u6cd5\u80fd\u5feb\u901f\u627e\u5230\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e14\u5728\u73b0\u5b9e\u573a\u666f\u8bd5\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u5728150\u79d2\u5185\u5b9a\u4f4d\u6f02\u6d6e\u4eba\u5076\u3002", "conclusion": "A^SAR\u7b97\u6cd5\u5728\u4f18\u5316SAR\u89c4\u5212\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u53ef\u7528\u4e8e\u66f4\u6709\u6548\u7684\u641c\u6551\u884c\u52a8\u3002"}}
{"id": "2511.11476", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11476", "abs": "https://arxiv.org/abs/2511.11476", "authors": ["Angela Lopez-Cardona", "Mireia Masias Bruns", "Nuwan T. Attygalle", "Sebastian Idesis", "Matteo Salvatori", "Konstantinos Raftopoulos", "Konstantinos Oikonomou", "Saravanakumar Duraisamy", "Parvin Emami", "Nacera Latreche", "Alaa Eddine Anis Sahraoui", "Michalis Vakallelis", "Jean Vanderdonckt", "Ioannis Arapakis", "Luis A. Leiva"], "title": "Context-aware Adaptive Visualizations for Critical Decision Making", "comment": null, "summary": "Effective decision-making often relies on timely insights from complex visual data. While Information Visualization (InfoVis) dashboards can support this process, they rarely adapt to users' cognitive state, and less so in real time. We present Symbiotik, an intelligent, context-aware adaptive visualization system that leverages neurophysiological signals to estimate mental workload (MWL) and dynamically adapt visual dashboards using reinforcement learning (RL). Through a user study with 120 participants and three visualization types, we demonstrate that our approach improves task performance and engagement. Symbiotik offers a scalable, real-time adaptation architecture, and a validated methodology for neuroadaptive user interfaces.", "AI": {"tldr": "Symbiotik\u662f\u4e00\u79cd\u667a\u80fd\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u9002\u5e94\u53ef\u89c6\u5316\u7cfb\u7edf\uff0c\u80fd\u591f\u6839\u636e\u5fc3\u7406\u8d1f\u8377\u5b9e\u65f6\u4f18\u5316\u53ef\u89c6\u5316\u4eea\u8868\u677f\uff0c\u63d0\u9ad8\u7528\u6237 performance\u548c engagement\u3002", "motivation": "\u73b0\u6709\u7684\u4fe1\u606f\u53ef\u89c6\u5316\u4eea\u8868\u677f\u7f3a\u4e4f\u5b9e\u65f6\u9002\u5e94\u80fd\u529b\uff0c\u4e0d\u80fd\u6839\u636e\u7528\u6237\u7684\u8ba4\u77e5\u72b6\u6001\u8fdb\u884c\u8c03\u6574\u3002", "method": "\u4f7f\u7528\u589e\u5f3a\u5b66\u4e60\u52a8\u6001\u9002\u5e94\u53ef\u89c6\u5316\u4eea\u8868\u677f\uff0c\u57fa\u4e8e\u795e\u7ecf\u751f\u7406\u4fe1\u53f7\u4f30\u8ba1\u5fc3\u7406\u8d1f\u8377\u3002", "result": "\u901a\u8fc7\u5bf9120\u540d\u53c2\u4e0e\u8005\u548c\u4e09\u79cd\u53ef\u89c6\u5316\u7c7b\u578b\u7684\u7528\u6237\u7814\u7a76\uff0c\u8bc1\u660e\u4e86Symbiotik\u65b9\u6848\u7684\u6709\u6548\u6027\u3002", "conclusion": "Symbiotik\u7cfb\u7edf\u80fd\u63d0\u9ad8\u7528\u6237\u4efb\u52a1\u8868\u73b0\u4e0e\u53c2\u4e0e\u5ea6\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u5b9e\u65f6\u9002\u5e94\u67b6\u6784\u53ca\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u65b9\u6cd5\u8bba\u3002"}}
{"id": "2511.10816", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.10816", "abs": "https://arxiv.org/abs/2511.10816", "authors": ["William Harris", "Lucas Yager", "Syler Sylvester", "Elizabeth Peiros", "Micheal C. Yip"], "title": "An Investigation into Dynamically Extensible and Retractable Robotic Leg Linkages for Multi-task Execution in Search and Rescue Scenarios", "comment": null, "summary": "Search and rescue (SAR) robots are required to quickly traverse terrain and perform high-force rescue tasks, necessitating both terrain adaptability and controlled high-force output. Few platforms exist today for SAR, and fewer still have the ability to cover both tasks of terrain adaptability and high-force output when performing extraction. While legged robots offer significant ability to traverse uneven terrain, they typically are unable to incorporate mechanisms that provide variable high-force outputs, unlike traditional wheel-based drive trains. This work introduces a novel concept for a dynamically extensible and retractable robot leg. Leveraging a dynamically extensible and retractable five-bar linkage design, it allows for mechanically switching between height-advantaged and force-advantaged configurations via a geometric transformation. A testbed evaluated leg performance across linkage geometries and operating modes, with empirical and analytical analyses conducted on stride length, force output, and stability. The results demonstrate that the morphing leg offers a promising path toward SAR robots that can both navigate terrain quickly and perform rescue tasks effectively.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u53ef\u53d8\u5f62\u817f\u90e8\u8bbe\u8ba1\uff0c\u65e8\u5728\u63d0\u5347\u641c\u7d22\u4e0e\u6551\u63f4\u673a\u5668\u4eba\u7684\u5730\u5f62\u9002\u5e94\u80fd\u529b\u548c\u9ad8\u529b\u8f93\u51fa\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u641c\u7d22\u548c\u6551\u63f4\u5e73\u53f0\u5728\u5730\u5f62\u9002\u5e94\u6027\u548c\u9ad8\u529b\u8f93\u51fa\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u8feb\u5207\u9700\u8981\u4e00\u79cd\u65b0\u578b\u8bbe\u8ba1\u6765\u6ee1\u8db3\u8fd9\u4e9b\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u52a8\u6001\u53ef\u6269\u5c55\u548c\u53ef\u6536\u7f29\u7684\u4e94\u6746\u94fe\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u5728\u9ad8\u5ea6\u4f18\u52bf\u548c\u529b\u4f18\u52bf\u914d\u7f6e\u4e4b\u95f4\u7684\u673a\u68b0\u5207\u6362\u3002\u5bf9\u817f\u90e8\u6027\u80fd\u8fdb\u884c\u4e86\u5b9e\u8bc1\u548c\u5206\u6790\u8bc4\u4f30\uff0c\u6d4b\u8bd5\u4e86\u4e0d\u540c\u94fe\u51e0\u4f55\u548c\u64cd\u4f5c\u6a21\u5f0f\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u53d8\u5f62\u817f\u90e8\u8bbe\u8ba1\u5728\u884c\u7a0b\u957f\u5ea6\u3001\u529b\u8f93\u51fa\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u5f62\u53d8\u817f\u90e8\u8bbe\u8ba1\u4e3a\u641c\u7d22\u4e0e\u6551\u63f4\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u5e0c\u671b\u7684\u65b9\u6848\uff0c\u4f7f\u5176\u80fd\u591f\u5feb\u901f\u7a7f\u8d8a\u5730\u5f62\u5e76\u6709\u6548\u6267\u884c\u6551\u63f4\u4efb\u52a1\u3002"}}
{"id": "2511.10822", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.10822", "abs": "https://arxiv.org/abs/2511.10822", "authors": ["Kota Kondo", "Yuwei Wu", "Vijay Kumar", "Jonathan P. How"], "title": "MIGHTY: Hermite Spline-based Efficient Trajectory Planning", "comment": "9 pages, 10 figures", "summary": "Hard-constraint trajectory planners often rely on commercial solvers and demand substantial computational resources. Existing soft-constraint methods achieve faster computation, but either (1) decouple spatial and temporal optimization or (2) restrict the search space. To overcome these limitations, we introduce MIGHTY, a Hermite spline-based planner that performs spatiotemporal optimization while fully leveraging the continuous search space of a spline. In simulation, MIGHTY achieves a 9.3% reduction in computation time and a 13.1% reduction in travel time over state-of-the-art baselines, with a 100% success rate. In hardware, MIGHTY completes multiple high-speed flights up to 6.7 m/s in a cluttered static environment and long-duration flights with dynamically added obstacles.", "AI": {"tldr": "MIGHTY\u91c7\u7528Hermite\u6837\u6761\u8fdb\u884c\u65f6\u7a7a\u4f18\u5316\uff0c\u663e\u8457\u52a0\u5feb\u4e86\u8ba1\u7b97\u901f\u5ea6\u548c\u51cf\u5c11\u4e86\u65c5\u884c\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5728\u7a7a\u95f4\u4e0e\u65f6\u95f4\u7684\u4f18\u5316\u4e0a\u76f8\u4e92 decouple\uff0c\u8981\u4e48\u9650\u5236\u641c\u7d22\u7a7a\u95f4\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u5b9e\u73b0\u7efc\u5408\u4f18\u5316\u3002", "method": "MIGHTY\u4f7f\u7528\u57fa\u4e8eHermite\u6837\u6761\u7684\u89c4\u5212\u65b9\u6cd5\uff0c\u8fdb\u884c\u65f6\u7a7a\u5168\u5c40\u4f18\u5316\uff0c\u5b8c\u5168\u5229\u7528\u6837\u6761\u7684\u8fde\u7eed\u641c\u7d22\u7a7a\u95f4\u3002", "result": "MIGHTY\u662f\u4e00\u79cd\u57fa\u4e8eHermite\u6837\u6761\u7684\u8f68\u8ff9\u89c4\u5212\u5668\uff0c\u80fd\u591f\u5728\u4e0d\u964d\u4f4e\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u65f6\u7a7a\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8f6f\u7ea6\u675f\u65b9\u6cd5\u7684\u4e0d\u8db3\u4e4b\u5904\u3002", "conclusion": "MIGHTY\u5728\u6a21\u62df\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u51cf\u5c11\u4e86\u8ba1\u7b97\u65f6\u95f4\u548c\u65c5\u884c\u65f6\u95f4\uff0c\u5e76\u5728\u5b9e\u9645\u786c\u4ef6\u4e0a\u6709\u6548\u6267\u884c\u9ad8\u901f\u5ea6\u98de\u884c\u3002"}}
{"id": "2511.10858", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.10858", "abs": "https://arxiv.org/abs/2511.10858", "authors": ["Dimitria Silveria", "Kleber Cabral", "Peter Jardine", "Sidney Givigi"], "title": "Decentralized Swarm Control via SO(3) Embeddings for 3D Trajectories", "comment": null, "summary": "This paper presents a novel decentralized approach for achieving emergent behavior in multi-agent systems with minimal information sharing. Based on prior work in simple orbits, our method produces a broad class of stable, periodic trajectories by stabilizing the system around a Lie group-based geometric embedding. Employing the Lie group SO(3), we generate a wider range of periodic curves than existing quaternion-based methods. Furthermore, we exploit SO(3) properties to eliminate the need for velocity inputs, allowing agents to receive only position inputs. We also propose a novel phase controller that ensures uniform agent separation, along with a formal stability proof. Validation through simulations and experiments showcases the method's adaptability to complex low-level dynamics and disturbances.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53bb\u4e2d\u5fc3\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c11\u7684\u4fe1\u606f\u5171\u4eab\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u6d8c\u73b0\u884c\u4e3a\u3002", "motivation": "\u901a\u8fc7\u51cf\u5c11\u4fe1\u606f\u5171\u4eab\uff0c\u63a2\u7d22\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u6d8c\u73b0\u884c\u4e3a\u7684\u5b9e\u73b0\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eLie\u7fa4\u7684\u51e0\u4f55\u5d4c\u5165\uff0c\u7279\u522b\u662fLie\u7fa4SO(3)\uff0c\u4ea7\u751f\u7a33\u6001\u5468\u671f\u8f68\u8ff9\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u76f8\u4f4d\u63a7\u5236\u5668\u6765\u786e\u4fdd\u667a\u80fd\u4f53\u95f4\u5747\u5300\u5206\u79bb\u3002", "result": "\u6a21\u62df\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u6b64\u65b9\u6cd5\u5728\u590d\u6742\u4f4e\u7ea7\u52a8\u6001\u548c\u5e72\u6270\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2511.10864", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.10864", "abs": "https://arxiv.org/abs/2511.10864", "authors": ["Jose Vasquez", "Xuping Zhang"], "title": "WetExplorer: Automating Wetland Greenhouse-Gas Surveys with an Autonomous Mobile Robot", "comment": "To be published in 2025 IEEE International Conference on Robotics and Biomimetics", "summary": "Quantifying greenhouse-gases (GHG) in wetlands is critical for climate modeling and restoration assessment, yet manual sampling is labor-intensive, and time demanding. We present WetExplorer, an autonomous tracked robot that automates the full GHG-sampling workflow. The robot system integrates low-ground-pressure locomotion, centimeter-accurate lift placement, dual-RTK sensor fusion, obstacle avoidance planning, and deep-learning perception in a containerized ROS2 stack. Outdoor trials verified that the sensor-fusion stack maintains a mean localization error of 1.71 cm, the vision module estimates object pose with 7 mm translational and 3\u00b0 rotational accuracy, while indoor trials demonstrated that the full motion-planning pipeline positions the sampling chamber within a global tolerance of 70 mm while avoiding obstacles, all without human intervention. By eliminating the manual bottleneck, WetExplorer enables high-frequency, multi-site GHG measurements and opens the door for dense, long-duration datasets in saturated wetland terrain.", "AI": {"tldr": "WetExplorer\u662f\u4e00\u79cd\u81ea\u4e3b\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u9ad8\u7cbe\u5ea6\u4f20\u611f\u5668\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u81ea\u52a8\u5316\u6e29\u5ba4\u6c14\u4f53\u91c7\u6837\uff0c\u89e3\u51b3\u4e86\u624b\u52a8\u91c7\u6837\u7684\u65f6\u95f4\u548c\u52b3\u52a8\u95ee\u9898\u3002", "motivation": "\u6e7f\u5730\u4e2d\u7684\u6e29\u5ba4\u6c14\u4f53\u5b9a\u91cf\u5bf9\u4e8e\u6c14\u5019\u5efa\u6a21\u548c\u6062\u590d\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u624b\u52a8\u91c7\u6837\u52b3\u52a8\u5f3a\u5ea6\u5927\u4e14\u8017\u65f6\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3aWetExplorer\u7684\u81ea\u4e3b\u673a\u5668\u4eba\uff0c\u96c6\u6210\u4e86\u591a\u79cd\u4f20\u611f\u5668\u548c\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6e29\u5ba4\u6c14\u4f53(GHG)\u7684\u91c7\u6837\u5de5\u4f5c\u6d41\u7a0b\u81ea\u52a8\u5316\u3002", "result": "WetExplorer\u901a\u8fc7\u96c6\u6210\u4f4e\u5730\u538b\u79fb\u52a8\u3001\u5398\u7c73\u7ea7\u63d0\u5347\u5b9a\u4f4d\u3001\u53ccRTK\u4f20\u611f\u5668\u878d\u5408\u3001\u969c\u788d\u7269\u89c4\u907f\u8ba1\u5212\u548c\u6df1\u5ea6\u5b66\u4e60\u611f\u77e5\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u9891\u3001\u591a\u5730\u70b9\u7684\u6e29\u5ba4\u6c14\u4f53\u6d4b\u91cf\u3002\u6237\u5916\u8bd5\u9a8c\u8868\u660e\uff0c\u4f20\u611f\u5668\u878d\u5408\u7cfb\u7edf\u7ef4\u6301\u4e86\u5e73\u5747\u5b9a\u4f4d\u8bef\u5dee\u4e3a1.71\u5398\u7c73\uff0c\u89c6\u89c9\u6a21\u5757\u5728\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u4e2d\u5b9e\u73b0\u4e867\u6beb\u7c73\u7684\u5e73\u79fb\u7cbe\u5ea6\u548c3\u00b0\u7684\u65cb\u8f6c\u7cbe\u5ea6\u3002", "conclusion": "WetExplorer\u7684\u5f00\u53d1\u663e\u8457\u63d0\u9ad8\u4e86\u6e7f\u5730\u6e29\u5ba4\u6c14\u4f53\u6d4b\u91cf\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u957f\u65f6\u95f4\u3001\u5927\u89c4\u6a21\u7684\u6570\u636e\u91c7\u96c6\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2511.10874", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.10874", "abs": "https://arxiv.org/abs/2511.10874", "authors": ["Yorai Shaoul", "Zhe Chen", "Mohamed Naveed Gul Mohamed", "Federico Pecora", "Maxim Likhachev", "Jiaoyang Li"], "title": "Collaborative Multi-Robot Non-Prehensile Manipulation via Flow-Matching Co-Generation", "comment": null, "summary": "Coordinating a team of robots to reposition multiple objects in cluttered environments requires reasoning jointly about where robots should establish contact, how to manipulate objects once contact is made, and how to navigate safely and efficiently at scale. Prior approaches typically fall into two extremes -- either learning the entire task or relying on privileged information and hand-designed planners -- both of which struggle to handle diverse objects in long-horizon tasks. To address these challenges, we present a unified framework for collaborative multi-robot, multi-object non-prehensile manipulation that integrates flow-matching co-generation with anonymous multi-robot motion planning. Within this framework, a generative model co-generates contact formations and manipulation trajectories from visual observations, while a novel motion planner conveys robots at scale. Crucially, the same planner also supports coordination at the object level, assigning manipulated objects to larger target structures and thereby unifying robot- and object-level reasoning within a single algorithmic framework. Experiments in challenging simulated environments demonstrate that our approach outperforms baselines in both motion planning and manipulation tasks, highlighting the benefits of generative co-design and integrated planning for scaling collaborative manipulation to complex multi-agent, multi-object settings. Visit gco-paper.github.io for code and demonstrations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6d41\u5339\u914d\u5171\u540c\u751f\u6210\u548c\u8fd0\u52a8\u89c4\u5212\uff0c\u63d0\u5347\u591a\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u64cd\u63a7\u591a\u4e2a\u7269\u4f53\u7684\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u65b9\u6cd5\u5728\u5904\u7406\u591a\u6837\u5316\u7269\u4f53\u548c\u957f\u65f6\u95f4\u4efb\u52a1\u65f6\u7684\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5b66\u4e60\u4efb\u52a1\u548c\u4f9d\u8d56\u7279\u6743\u4fe1\u606f\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u7ed3\u5408\u6d41\u5339\u914d\u5171\u540c\u751f\u6210\u548c\u533f\u540d\u591a\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\uff0c\u4ee5\u5b9e\u73b0\u534f\u540c\u591a\u673a\u5668\u4eba\u3001\u591a\u7269\u4f53\u7684\u975e\u6293\u53d6\u64cd\u63a7\u3002", "result": "\u5728\u590d\u6742\u7684\u6a21\u62df\u73af\u5883\u4e2d\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8fd0\u52a8\u89c4\u5212\u548c\u64cd\u63a7\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u57fa\u7ebf\uff0c\u663e\u793a\u51fa\u751f\u6210\u5171\u540c\u8bbe\u8ba1\u548c\u7efc\u5408\u89c4\u5212\u7684\u4f18\u52bf\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u6846\u67b6\u7edf\u4e00\u4e86\u673a\u5668\u4eba\u548c\u7269\u4f53\u5c42\u9762\u7684\u63a8\u7406\uff0c\u4e3a\u5728\u590d\u6742\u591a\u4ee3\u7406\u3001\u591a\u7269\u4f53\u8bbe\u7f6e\u4e2d\u6269\u5c55\u534f\u540c\u64cd\u63a7\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.10901", "categories": ["cs.RO", "cond-mat.soft"], "pdf": "https://arxiv.org/pdf/2511.10901", "abs": "https://arxiv.org/abs/2511.10901", "authors": ["Deniz Kerimoglu", "Nicholas D. Naclerio", "Sean Chu", "Andrew Krohn", "Vineet Kupunaram", "Alexander Schepelmann", "Daniel I. Goldman", "Elliot W. Hawkes"], "title": "Terradynamics and design of tip-extending robotic anchors", "comment": null, "summary": "Most engineered pilings require substantially more force to be driven into the ground than they can resist during extraction. This requires relatively heavy equipment for insertion, which is problematic for anchoring in hard-to-access sites, including in extraterrestrial locations. In contrast, for tree roots, the external reaction force required to extract is much greater than required to insert--little more than the weight of the seed initiates insertion. This is partly due to the mechanism by which roots insert into the ground: tip extension. Proof-of-concept robotic prototypes have shown the benefits of using this mechanism, but a rigorous understanding of the underlying granular mechanics and how they inform the design of a robotic anchor is lacking. Here, we study the terradynamics of tip-extending anchors compared to traditional piling-like intruders, develop a set of design insights, and apply these to create a deployable robotic anchor. Specifically, we identify that to increase an anchor's ratio of extraction force to insertion force, it should: (i) extend beyond a critical depth; (ii) include hair-like protrusions; (iii) extend near-vertically, and (iv) incorporate multiple smaller anchors rather than a single large anchor. Synthesizing these insights, we developed a lightweight, soft robotic, root-inspired anchoring device that inserts into the ground with a reaction force less than its weight. We demonstrate that the 300 g device can deploy a series of temperature sensors 45 cm deep into loose Martian regolith simulant while anchoring with an average of 120 N, resulting in an anchoring-to-weight ratio of 40:1.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8c03\u67e5\u4e86\u7c7b\u6839\u673a\u68b0\u624b\u7684\u63d2\u5165\u4e0e\u63d0\u53d6\u884c\u4e3a\uff0c\u63d0\u51fa\u4e86\u8bbe\u8ba1\u89c1\u89e3\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u53ef\u90e8\u7f72\u7684\u8f7b\u91cf\u7ea7\u8f6f\u4f53\u673a\u5668\u4eba\u951a\u5b9a\u88c5\u7f6e\u3002", "motivation": "\u4f20\u7edf\u951a\u56fa\u6869\u5728\u5916\u661f\u7b49\u96be\u4ee5\u63a5\u8fd1\u7684\u5730\u65b9\u9700\u8981\u66f4\u5927\u7684\u9a71\u52a8\u529b\uff0c\u800c\u690d\u7269\u6839\u7cfb\u7684\u63d2\u5165\u673a\u5236\u76f8\u6bd4\u4e4b\u4e0b\u8981\u6c42\u7684\u5916\u529b\u8f83\u5c0f\uff0c\u63d0\u4f9b\u4e86\u65b0\u7684\u8bbe\u8ba1\u601d\u8def\u3002", "method": "\u7814\u7a76\u6839\u90e8\u6269\u5c55\u951a\u4e0e\u4f20\u7edf\u6869\u5f0f\u5165\u4fb5\u8005\u7684\u571f\u4f53\u52a8\u529b\u5b66\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u8bbe\u8ba1\u89c1\u89e3\u3002", "result": "\u65b0\u5f00\u53d1\u7684\u673a\u5668\u4eba\u88c5\u7f6e\u5728\u91cd300\u514b\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u4ee5120 N\u7684\u529b\u951a\u5b9a\u5e76\u5411\u4e0b\u63d2\u516545\u5398\u7c73\uff0c\u8868\u73b0\u51fa40:1\u7684\u951a\u5b9a\u4e0e\u91cd\u529b\u6bd4\u3002", "conclusion": "\u91c7\u7528\u6839\u90e8\u673a\u5236\u8bbe\u8ba1\u7684\u673a\u5668\u4eba\u951a\u5b9a\u88c5\u7f6e\u5728\u91cd\u91cf\u8f83\u8f7b\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u951a\u5b9a\u80fd\u529b\uff0c\u9002\u5408\u5728\u5916\u661f\u73af\u5883\u4e2d\u4f7f\u7528\u3002"}}
{"id": "2511.10987", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.10987", "abs": "https://arxiv.org/abs/2511.10987", "authors": ["Wenbin Bai", "Qiyu Chen", "Xiangbo Lin", "Jianwen Li", "Quancheng Li", "Hejiang Pan", "Yi Sun"], "title": "Dexterous Manipulation Transfer via Progressive Kinematic-Dynamic Alignment", "comment": "13 pages, 15 figures. Accepted by AAAI 2026", "summary": "The inherent difficulty and limited scalability of collecting manipulation data using multi-fingered robot hand hardware platforms have resulted in severe data scarcity, impeding research on data-driven dexterous manipulation policy learning. To address this challenge, we present a hand-agnostic manipulation transfer system. It efficiently converts human hand manipulation sequences from demonstration videos into high-quality dexterous manipulation trajectories without requirements of massive training data. To tackle the multi-dimensional disparities between human hands and dexterous hands, as well as the challenges posed by high-degree-of-freedom coordinated control of dexterous hands, we design a progressive transfer framework: first, we establish primary control signals for dexterous hands based on kinematic matching; subsequently, we train residual policies with action space rescaling and thumb-guided initialization to dynamically optimize contact interactions under unified rewards; finally, we compute wrist control trajectories with the objective of preserving operational semantics. Using only human hand manipulation videos, our system automatically configures system parameters for different tasks, balancing kinematic matching and dynamic optimization across dexterous hands, object categories, and tasks. Extensive experimental results demonstrate that our framework can automatically generate smooth and semantically correct dexterous hand manipulation that faithfully reproduces human intentions, achieving high efficiency and strong generalizability with an average transfer success rate of 73%, providing an easily implementable and scalable method for collecting robot dexterous manipulation data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u624b\u65e0\u5173\u7684\u64cd\u63a7\u8f6c\u79fb\u7cfb\u7edf\uff0c\u80fd\u591f\u4ece\u4eba\u624b\u64cd\u63a7\u89c6\u9891\u4e2d\u81ea\u52a8\u751f\u6210\u7075\u5de7\u64cd\u63a7\u8f68\u8ff9\uff0c\u514b\u670d\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u591a\u6307\u673a\u5668\u4eba\u624b\u786c\u4ef6\u5e73\u53f0\u6536\u96c6\u64cd\u63a7\u6570\u636e\u7684\u56f0\u96be\u4e0e\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4ee5\u4fc3\u8fdb\u6570\u636e\u9a71\u52a8\u7684\u7075\u5de7\u64cd\u63a7\u7b56\u7565\u5b66\u4e60\u7814\u7a76\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6e10\u8fdb\u8f6c\u79fb\u6846\u67b6\uff0c\u5305\u62ec\u57fa\u4e8e\u8fd0\u52a8\u5b66\u5339\u914d\u7684\u4e3b\u8981\u63a7\u5236\u4fe1\u53f7\u5efa\u7acb\u3001\u5e26\u6709\u52a8\u4f5c\u7a7a\u95f4\u91cd\u65b0\u7f29\u653e\u7684\u6b8b\u5dee\u7b56\u7565\u8bad\u7ec3\uff0c\u4ee5\u53ca\u4fdd\u7559\u64cd\u4f5c\u8bed\u4e49\u7684\u8155\u90e8\u63a7\u5236\u8f68\u8ff9\u8ba1\u7b97\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u79cd\u624b\u65e0\u5173\u7684\u64cd\u63a7\u8f6c\u79fb\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u7075\u5de7\u64cd\u63a7\u8f68\u8ff9\u751f\u6210\uff0c\u4e14\u65e0\u9700\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u6846\u67b6\u80fd\u9ad8\u6548\u751f\u6210\u5e73\u6ed1\u4e14\u8bed\u4e49\u6b63\u786e\u7684\u7075\u5de7\u624b\u64cd\u63a7\uff0c\u5177\u6709\u5f3a\u5927\u7684\u901a\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2511.10989", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.10989", "abs": "https://arxiv.org/abs/2511.10989", "authors": ["Prab Prasertying", "Paulo Garcia", "Warisa Sritriratanarak"], "title": "Dynamic Reconfiguration of Robotic Swarms: Coordination and Control for Precise Shape Formation", "comment": "accepted at the 9th International Conference on Algorithms, Computing and Systems (ICACS 2025)", "summary": "Coordination of movement and configuration in robotic swarms is a challenging endeavor. Deciding when and where each individual robot must move is a computationally complex problem. The challenge is further exacerbated by difficulties inherent to physical systems, such as measurement error and control dynamics. Thus, how to best determine the optimal path for each robot, when moving from one configuration to another, and how to best perform such determination and effect corresponding motion remains an open problem. In this paper, we show an algorithm for such coordination of robotic swarms. Our methods allow seamless transition from one configuration to another, leveraging geometric formulations that are mapped to the physical domain through appropriate control, localization, and mapping techniques. This paves the way for novel applications of robotic swarms by enabling more sophisticated distributed behaviors.", "AI": {"tldr": "\u672c\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7b97\u6cd5\u4ee5\u63d0\u9ad8\u673a\u5668\u4eba\u7fa4\u4f53\u7684\u8fd0\u52a8\u534f\u8c03\u6027\u548c\u914d\u7f6e\u8f6c\u6362\u80fd\u529b\uff0c\u5229\u7528\u51e0\u4f55\u6784\u9020\u548c\u7269\u7406\u63a7\u5236\u6280\u672f\u5b9e\u73b0\u66f4\u590d\u6742\u7684\u5206\u5e03\u5f0f\u884c\u4e3a\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u7fa4\u4f53\u7684\u8fd0\u52a8\u534f\u8c03\u4e2d\uff0c\u5982\u4f55\u9ad8\u6548\u786e\u5b9a\u6bcf\u4e2a\u673a\u5668\u4eba\u5728\u79fb\u52a8\u5230\u65b0\u914d\u7f6e\u65f6\u7684\u6700\u4f18\u8def\u5f84\u662f\u4e00\u4e2a\u590d\u6742\u7684\u8ba1\u7b97\u95ee\u9898\uff0c\u7269\u7406\u7cfb\u7edf\u7684\u5185\u5728\u56f0\u96be\u8fdb\u4e00\u6b65\u589e\u52a0\u4e86\u6311\u6218\u3002", "method": "\u4f7f\u7528\u51e0\u4f55\u6784\u9020\u65b9\u6cd5\uff0c\u7ed3\u5408\u9002\u5f53\u7684\u63a7\u5236\u3001\u5b9a\u4f4d\u548c\u6620\u5c04\u6280\u672f\uff0c\u786e\u4fdd\u673a\u5668\u4eba\u4ece\u4e00\u79cd\u914d\u7f6e\u65e0\u7f1d\u8fc7\u6e21\u5230\u53e6\u4e00\u79cd\u914d\u7f6e\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b97\u6cd5\uff0c\u4ee5\u534f\u8c03\u673a\u5668\u4eba\u7fa4\u4f53\u7684\u8fd0\u52a8\u548c\u914d\u7f6e\uff0c\u89e3\u51b3\u4e86\u4ece\u4e00\u4e2a\u914d\u7f6e\u5e73\u6ed1\u8fc7\u6e21\u5230\u53e6\u4e00\u4e2a\u914d\u7f6e\u7684\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u534f\u8c03\u673a\u5668\u4eba\u7fa4\u4f53\u7684\u8fd0\u52a8\uff0c\u63d0\u9ad8\u5176\u5728\u7269\u7406\u73af\u5883\u4e2d\u7684\u5e94\u7528\u6f5c\u80fd\u3002"}}
{"id": "2511.11011", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11011", "abs": "https://arxiv.org/abs/2511.11011", "authors": ["Zhiwei Zhang", "Hui Zhang", "Xieyuanli Chen", "Kaihong Huang", "Chenghao Shi", "Huimin Lu"], "title": "Latent-Space Autoregressive World Model for Efficient and Robust Image-Goal Navigation", "comment": null, "summary": "Traditional navigation methods rely heavily on accurate localization and mapping. In contrast, world models that capture environmental dynamics in latent space have opened up new perspectives for navigation tasks, enabling systems to move beyond traditional multi-module pipelines. However, world model often suffers from high computational costs in both training and inference. To address this, we propose LS-NWM - a lightweight latent space navigation world model that is trained and operates entirely in latent space, compared to the state-of-the-art baseline, our method reduces training time by approximately 3.2x and planning time by about 447x,while further improving navigation performance with a 35% higher SR and an 11% higher SPL. The key idea is that accurate pixel-wise environmental prediction is unnecessary for navigation. Instead, the model predicts future latent states based on current observational features and action inputs, then performs path planning and decision-making within this compact representation, significantly improving computational efficiency. By incorporating an autoregressive multi-frame prediction strategy during training, the model effectively captures long-term spatiotemporal dependencies, thereby enhancing navigation performance in complex scenarios. Experimental results demonstrate that our method achieves state-of-the-art navigation performance while maintaining a substantial efficiency advantage over existing approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6f5c\u5728\u7a7a\u95f4\u5bfc\u822a\u4e16\u754c\u6a21\u578bLS-NWM\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5bfc\u822a\u6027\u80fd\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u5bfc\u822a\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u51c6\u786e\u7684\u5b9a\u4f4d\u548c\u6620\u5c04\uff0c\u800c\u4e16\u754c\u6a21\u578b\u901a\u8fc7\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u6355\u6349\u73af\u5883\u52a8\u6001\uff0c\u4e3a\u5bfc\u822a\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "method": "\u5f00\u53d1\u4e86LS-NWM\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u6f5c\u5728\u7a7a\u95f4\u5bfc\u822a\u4e16\u754c\u6a21\u578b\uff0c\u5b8c\u5168\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u8bad\u7ec3\u548c\u64cd\u4f5c\u3002", "result": "\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u8bad\u7ec3\u65f6\u95f4\u4e0a\u51cf\u5c11\u4e86\u5927\u7ea63.2\u500d\uff0c\u5728\u89c4\u5212\u65f6\u95f4\u4e0a\u51cf\u5c11\u4e86\u7ea6447\u500d\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u5bfc\u822a\u6027\u80fd\uff0c\u6210\u529f\u5b9e\u73b0\u4e8635%\u66f4\u9ad8\u7684SR\u548c11%\u66f4\u9ad8\u7684SPL\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4fdd\u6301\u6548\u7387\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u5bfc\u822a\u6027\u80fd\u3002"}}
{"id": "2511.11022", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11022", "abs": "https://arxiv.org/abs/2511.11022", "authors": ["Hyunchul Bae", "Eunjae Lee", "Jehyeop Han", "Minhee Kang", "Jaehyeon Kim", "Junggeun Seo", "Minkyun Noh", "Heejin Ahn"], "title": "Miniature Testbed for Validating Multi-Agent Cooperative Autonomous Driving", "comment": "8 pages", "summary": "Cooperative autonomous driving, which extends vehicle autonomy by enabling real-time collaboration between vehicles and smart roadside infrastructure, remains a challenging yet essential problem. However, none of the existing testbeds employ smart infrastructure equipped with sensing, edge computing, and communication capabilities. To address this gap, we design and implement a 1:15-scale miniature testbed, CIVAT, for validating cooperative autonomous driving, consisting of a scaled urban map, autonomous vehicles with onboard sensors, and smart infrastructure. The proposed testbed integrates V2V and V2I communication with the publish-subscribe pattern through a shared Wi-Fi and ROS2 framework, enabling information exchange between vehicles and infrastructure to realize cooperative driving functionality. As a case study, we validate the system through infrastructure-based perception and intersection management experiments.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540d\u4e3aCIVAT\u76841:15\u6bd4\u4f8b\u5fae\u578b\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7528\u4e8e\u9a8c\u8bc1\u5408\u4f5c\u81ea\u52a8\u9a7e\u9a76\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6d4b\u8bd5\u5e73\u53f0\u7f3a\u4e4f\u667a\u80fd\u57fa\u7840\u8bbe\u65bd\u7684\u7f3a\u9677\u3002", "motivation": "\u73b0\u6709\u7684\u6d4b\u8bd5\u5e73\u53f0\u672a\u91c7\u7528\u5177\u5907\u611f\u77e5\u3001\u8fb9\u7f18\u8ba1\u7b97\u548c\u901a\u4fe1\u80fd\u529b\u7684\u667a\u80fd\u57fa\u7840\u8bbe\u65bd\uff0c\u9650\u5236\u4e86\u5408\u4f5c\u81ea\u52a8\u9a7e\u9a76\u7684\u7814\u7a76\u548c\u53d1\u5c55\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u65bd\u4e86\u4e00\u4e2a\u5305\u542b\u7f29\u653e\u57ce\u5e02\u5730\u56fe\u3001\u81ea\u4e3b\u8f66\u8f86\u548c\u667a\u80fd\u57fa\u7840\u8bbe\u65bd\u7684\u5fae\u578b\u6d4b\u8bd5\u5e73\u53f0\uff0c\u96c6\u6210\u4e86\u8f66\u8f86\u4e0e\u8f66\u8f86(V2V)\u53ca\u8f66\u8f86\u4e0e\u57fa\u7840\u8bbe\u65bd(V2I)\u901a\u4fe1\u3002", "result": "\u901a\u8fc7\u57fa\u7840\u8bbe\u65bd\u611f\u77e5\u548c\u4ea4\u53c9\u53e3\u7ba1\u7406\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86CIVAT\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "conclusion": "CIVAT\u5e73\u53f0\u6709\u6548\u5b9e\u73b0\u4e86\u8f66\u8f86\u4e0e\u57fa\u7840\u8bbe\u65bd\u4e4b\u95f4\u7684\u4fe1\u606f\u4ea4\u6362\uff0c\u4e3a\u5408\u4f5c\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u9a8c\u8bc1\u5de5\u5177\u3002"}}
{"id": "2511.11052", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11052", "abs": "https://arxiv.org/abs/2511.11052", "authors": ["Jinxuan Zhu", "Chenrui Tie", "Xinyi Cao", "Yuran Wang", "Jingxiang Guo", "Zixuan Chen", "Haonan Chen", "Junting Chen", "Yangyu Xiao", "Ruihai Wu", "Lin Shao"], "title": "AdaptPNP: Integrating Prehensile and Non-Prehensile Skills for Adaptive Robotic Manipulation", "comment": null, "summary": "Non-prehensile (NP) manipulation, in which robots alter object states without forming stable grasps (for example, pushing, poking, or sliding), significantly broadens robotic manipulation capabilities when grasping is infeasible or insufficient. However, enabling a unified framework that generalizes across different tasks, objects, and environments while seamlessly integrating non-prehensile and prehensile (P) actions remains challenging: robots must determine when to invoke NP skills, select the appropriate primitive for each context, and compose P and NP strategies into robust, multi-step plans. We introduce ApaptPNP, a vision-language model (VLM)-empowered task and motion planning framework that systematically selects and combines P and NP skills to accomplish diverse manipulation objectives. Our approach leverages a VLM to interpret visual scene observations and textual task descriptions, generating a high-level plan skeleton that prescribes the sequence and coordination of P and NP actions. A digital-twin based object-centric intermediate layer predicts desired object poses, enabling proactive mental rehearsal of manipulation sequences. Finally, a control module synthesizes low-level robot commands, with continuous execution feedback enabling online task plan refinement and adaptive replanning through the VLM. We evaluate ApaptPNP across representative P&NP hybrid manipulation tasks in both simulation and real-world environments. These results underscore the potential of hybrid P&NP manipulation as a crucial step toward general-purpose, human-level robotic manipulation capabilities. Project Website: https://sites.google.com/view/adaptpnp/home", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u975e\u6293\u63e1\u548c\u6293\u63e1\u52a8\u4f5c\u7684\u4efb\u52a1\u548c\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u5347\u673a\u5668\u4eba\u5728\u591a\u6837\u5316\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u7075\u6d3b\u6027\u548c\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22\u5728\u6293\u63e1\u4e0d\u53ef\u884c\u7684\u60c5\u51b5\u4e0b\uff0c\u673a\u5668\u4eba\u5982\u4f55\u6709\u6548\u8fdb\u884c\u975e\u6293\u63e1\u64cd\u4f5c\uff0c\u4ee5\u63d0\u5347\u673a\u5668\u4eba\u7684\u64cd\u4f5c\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u9ad8\u5c42\u6b21\u89c4\u5212\uff0c\u4ee5\u89e3\u91ca\u89c6\u89c9\u573a\u666f\u548c\u6587\u672c\u4efb\u52a1\u63cf\u8ff0\uff0c\u7ed3\u5408\u52a8\u6001\u7684\u5bf9\u8c61\u4e2d\u5fc3\u4e2d\u95f4\u5c42\u548c\u63a7\u5236\u6a21\u5757\u4f18\u5316\u6267\u884c\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u8bc4\u4f30ApaptPNP\u5728\u4ee3\u8868\u6027\u6df7\u5408\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u8868\u660e\u5176\u5728\u591a\u4efb\u52a1\u5904\u7406\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "ApaptPNP\u5c55\u793a\u4e86\u6df7\u5408\u975e\u6293\u63e1\u4e0e\u6293\u63e1\u64cd\u4f5c\u5728\u5b9e\u73b0\u666e\u9002\u6027\u4eba\u7c7b\u6c34\u5e73\u64cd\u63a7\u80fd\u529b\u65b9\u9762\u7684\u91cd\u8981\u6f5c\u529b\u3002"}}
{"id": "2511.11218", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11218", "abs": "https://arxiv.org/abs/2511.11218", "authors": ["Chenhao Liu", "Leyun Jiang", "Yibo Wang", "Kairan Yao", "Jinchen Fu", "Xiaoyu Ren"], "title": "Humanoid Whole-Body Badminton via Multi-Stage Reinforcement Learning", "comment": null, "summary": "Humanoid robots have demonstrated strong capability for interacting with deterministic scenes across locomotion, manipulation, and more challenging loco-manipulation tasks. Yet the real world is dynamic, quasi-static interactions are insufficient to cope with the various environmental conditions. As a step toward more dynamic interaction scenario, we present a reinforcement-learning-based training pipeline that produces a unified whole-body controller for humanoid badminton, enabling coordinated lower-body footwork and upper-body striking without any motion priors or expert demonstrations. Training follows a three-stage curriculum: first footwork acquisition, then precision-guided racket swing generation, and finally task-focused refinement, yielding motions in which both legs and arms serve the hitting objective. For deployment, we incorporate an Extended Kalman Filter (EKF) to estimate and predict shuttlecock trajectories for target striking. We also introduce a prediction-free variant that dispenses with EKF and explicit trajectory prediction. To validate the framework, we conduct five sets of experiment in both simulation and the real world. In simulation, two robots sustain a rally of 21 consecutive hits. Moreover, the prediction-free variant achieves successful hits with comparable performance relative to the target-known policy. In real-world tests, both the prediction and controller module exhibit high accuracy, and on-court hitting achieves an outgoing shuttle speed up to 10 m/s with a mean return landing distance of 3.5 m. These experiment results show that our humanoid robot can deliver highly dynamic while precise goal striking in badminton, and can be adapted to more dynamism critical domains.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4f7f\u4eba\u5f62\u673a\u5668\u4eba\u80fd\u591f\u5728\u52a8\u6001\u7fbd\u6bdb\u7403\u6bd4\u8d5b\u4e2d\u5b9e\u73b0\u7cbe\u786e\u51fb\u7403\uff0c\u5e76\u8868\u73b0\u51fa\u9ad8\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u4eba\u5f62\u673a\u5668\u4eba\u5728\u786e\u5b9a\u6027\u573a\u666f\u4e2d\u7684\u4ea4\u4e92\u80fd\u529b\u5f3a\uff0c\u4f46\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5374\u9762\u4e34\u6311\u6218\uff0c\u56e0\u6b64\u672c\u7814\u7a76\u65e8\u5728\u63d0\u9ad8\u4eba\u5f62\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u4ea4\u4e92\u80fd\u529b\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u7ba1\u9053\uff0c\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a\u9996\u5148\u83b7\u53d6\u811a\u6b65\u79fb\u52a8\uff0c\u7136\u540e\u751f\u6210\u7cbe\u51c6\u7684\u6325\u62cd\u52a8\u4f5c\uff0c\u6700\u540e\u8fdb\u884c\u4efb\u52a1\u5bfc\u5411\u7684\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4eff\u771f\u4e2d\u4e24\u4e2a\u673a\u5668\u4eba\u80fd\u591f\u6301\u7eed\u8fdb\u884c21\u6b21\u51fb\u7403\uff0c\u800c\u5728\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u4e2d\uff0c\u51fb\u7403\u901f\u5ea6\u8fbe\u523010\u7c73/\u79d2\uff0c\u8fd4\u56de\u843d\u70b9\u5e73\u5747\u8ddd\u79bb\u4e3a3.5\u7c73\uff0c\u8868\u660e\u673a\u5668\u4eba\u7684\u51fb\u7403\u7cbe\u786e\u5ea6\u9ad8\u3002", "conclusion": "\u8be5\u8bba\u6587\u5c55\u793a\u4e86\u4e00\u4e2a\u8bad\u7ec3\u7ba1\u9053\uff0c\u4f7f\u4eba\u5f62\u673a\u5668\u4eba\u80fd\u591f\u5728\u7fbd\u6bdb\u7403\u8fd0\u52a8\u4e2d\u5b9e\u73b0\u52a8\u6001\u4e14\u7cbe\u786e\u7684\u51fb\u7403\u76ee\u6807\uff0c\u5e76\u53ef\u9002\u7528\u4e8e\u66f4\u52a8\u6001\u7684\u9886\u57df\u3002"}}
{"id": "2511.11223", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11223", "abs": "https://arxiv.org/abs/2511.11223", "authors": ["Sverre Herland", "Amit Parag", "Elling Ruud \u00d8ye", "Fangyi Zhang", "Fouad Makiyeh", "Aleksander Lillienskiold", "Abhaya Pal Singh", "Edward H. Adelson", "Francois Chaumette", "Alexandre Krupa", "Peter Corke", "Ekrem Misimi"], "title": "Sashimi-Bot: Autonomous Tri-manual Advanced Manipulation and Cutting of Deformable Objects", "comment": null, "summary": "Advanced robotic manipulation of deformable, volumetric objects remains one of the greatest challenges due to their pliancy, frailness, variability, and uncertainties during interaction. Motivated by these challenges, this article introduces Sashimi-Bot, an autonomous multi-robotic system for advanced manipulation and cutting, specifically the preparation of sashimi. The objects that we manipulate, salmon loins, are natural in origin and vary in size and shape, they are limp and deformable with poorly characterized elastoplastic parameters, while also being slippery and hard to hold. The three robots straighten the loin; grasp and hold the knife; cut with the knife in a slicing motion while cooperatively stabilizing the loin during cutting; and pick up the thin slices from the cutting board or knife blade. Our system combines deep reinforcement learning with in-hand tool shape manipulation, in-hand tool cutting, and feedback of visual and tactile information to achieve robustness to the variabilities inherent in this task. This work represents a milestone in robotic manipulation of deformable, volumetric objects that may inspire and enable a wide range of other real-world applications.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Sashimi-Bot\uff0c\u4e00\u4e2a\u81ea\u4e3b\u591a\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u80fd\u591f\u9ad8\u6548\u64cd\u63a7\u548c\u5207\u5272\u53d8\u5f62\u7684\u4e09\u6587\u9c7c\uff0c\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u548c\u591a\u79cd\u53cd\u9988\u673a\u5236\uff0c\u4ee3\u8868\u4e86\u8be5\u9886\u57df\u7684\u91cd\u5927\u8fdb\u5c55\u3002", "motivation": "\u9762\u5bf9\u53d8\u5f62\u4f53\u79ef\u7269\u4f53\u64cd\u63a7\u7684\u6311\u6218\uff0c\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u9ad8\u6548\u51c6\u786e\u5904\u7406\u5982\u4e09\u6587\u9c7c loin \u7b49\u81ea\u7136\u7269\u4f53\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u3002", "method": "\u8be5\u7cfb\u7edf\u7ed3\u5408\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u3001\u5de5\u5177\u5f62\u72b6\u64cd\u63a7\u3001\u5200\u5177\u5207\u5272\u548c\u89c6\u89c9\u4e0e\u89e6\u89c9\u4fe1\u606f\u53cd\u9988\uff0c\u4ee5\u5e94\u5bf9\u4efb\u52a1\u4e2d\u7684\u5404\u79cd\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u4e09\u53f0\u673a\u5668\u4eba\u80fd\u591f\u534f\u4f5c\u5b8c\u6210\u4e09\u6587\u9c7c\u7684\u6574\u7406\u3001\u5207\u5272\u548c\u53d6\u8d70\u8584\u7247\uff0c\u8868\u73b0\u51fa\u5728\u5904\u7406\u4e0d\u89c4\u5219\u548c\u6ed1\u817b\u7269\u4f53\u4e0a\u7684\u9ad8\u5ea6\u9c81\u68d2\u6027\u3002", "conclusion": "Sashimi-Bot\u662f\u4e00\u79cd\u81ea\u4e3b\u591a\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u80fd\u591f\u9ad8\u6548\u5730\u64cd\u63a7\u548c\u5207\u5272\u53d8\u5f62\u7684\u4f53\u79ef\u7269\u4f53\uff0c\u4ee3\u8868\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u4e00\u4e2a\u91cd\u8981\u91cc\u7a0b\u7891\u3002"}}
{"id": "2511.11298", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11298", "abs": "https://arxiv.org/abs/2511.11298", "authors": ["Yihao Zhang", "Yuankai Qi", "Xi Zheng"], "title": "Experiences from Benchmarking Vision-Language-Action Models for Robotic Manipulation", "comment": null, "summary": "Foundation models applied in robotics, particularly \\textbf{Vision--Language--Action (VLA)} models, hold great promise for achieving general-purpose manipulation. Yet, systematic real-world evaluations and cross-model comparisons remain scarce. This paper reports our \\textbf{empirical experiences} from benchmarking four representative VLAs -- \\textbf{ACT}, \\textbf{OpenVLA--OFT}, \\textbf{RDT-1B}, and \\boldmath{$\u03c0_0$} -- across four manipulation tasks conducted in both simulation and on the \\textbf{ALOHA Mobile} platform. We establish a \\textbf{standardized evaluation framework} that measures performance along three key dimensions: (1) \\textit{accuracy and efficiency} (success rate and time-to-success), (2) \\textit{adaptability} across in-distribution, spatial out-of-distribution, and instance-plus-spatial out-of-distribution settings, and (3) \\textit{language instruction-following accuracy}. Through this process, we observe that \\boldmath{$\u03c0_0$} demonstrates superior adaptability in out-of-distribution scenarios, while \\textbf{ACT} provides the highest stability in-distribution. Further analysis highlights differences in computational demands, data-scaling behavior, and recurring failure modes such as near-miss grasps, premature releases, and long-horizon state drift. These findings reveal practical trade-offs among VLA model architectures in balancing precision, generalization, and deployment cost, offering actionable insights for selecting and deploying VLAs in real-world robotic manipulation tasks.", "AI": {"tldr": "\u672c\u8bba\u6587\u8bc4\u4f30\u4e86\u591a\u79cdVLA\u6a21\u578b\u5728\u73b0\u5b9e\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u4f9b\u4e86\u4e00\u5957\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u63ed\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u5728\u9002\u5e94\u6027\u3001\u7a33\u5b9a\u6027\u548c\u8ba1\u7b97\u9700\u6c42\u65b9\u9762\u7684\u5dee\u5f02\uff0c\u5f3a\u8c03\u4e86\u5728\u9009\u62e9\u548c\u90e8\u7f72VLA\u65f6\u7684\u5b9e\u7528\u6743\u8861\u3002", "motivation": "\u63a8\u52a8VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u8bc4\u4f30\u4e0d\u540c\u6a21\u578b\u7684\u8868\u73b0\u53ca\u5176\u5728\u5b9e\u9645\u64cd\u4f5c\u4e2d\u7684\u9002\u7528\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u5efa\u7acb\u4e86\u4e00\u5957\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4ece\u51c6\u786e\u6027\u548c\u6548\u7387\u3001\u9002\u5e94\u6027\u4ee5\u53ca\u8bed\u8a00\u6307\u4ee4\u8ddf\u968f\u51c6\u786e\u6027\u4e09\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u8bc4\u6d4b\u3002", "result": "\u672c\u7814\u7a76\u5bf9\u56db\u79cd\u4ee3\u8868\u6027\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u62ecACT\u3001OpenVLA-OFT\u3001RDT-1B\u548c\u03c00\uff0c\u9488\u5bf9\u56db\u4e2a\u64cd\u4f5c\u4efb\u52a1\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u4f30\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u6a21\u578b\u03c00\u5728\u4e0d\u5728\u5206\u5e03\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u9002\u5e94\u6027\uff0c\u800cACT\u5728\u5206\u5e03\u573a\u666f\u4e2d\u63d0\u4f9b\u6700\u9ad8\u7684\u7a33\u5b9a\u6027\uff0c\u5f3a\u8c03\u4e86\u5728\u9009\u62e9\u548c\u90e8\u7f72VLA\u6a21\u578b\u65f6\u7684\u6743\u8861\u4e0e\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2511.11310", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.11310", "abs": "https://arxiv.org/abs/2511.11310", "authors": ["Joseph Abdo", "Aditya Shibu", "Moaiz Saeed", "Abdul Maajid Aga", "Apsara Sivaprazad", "Mohamed Al-Musleh"], "title": "Simulating an Autonomous System in CARLA using ROS 2", "comment": null, "summary": "Autonomous racing offers a rigorous setting to stress test perception, planning, and control under high speed and uncertainty. This paper proposes an approach to design and evaluate a software stack for an autonomous race car in CARLA: Car Learning to Act simulator, targeting competitive driving performance in the Formula Student UK Driverless (FS-AI) 2025 competition. By utilizing a 360\u00b0 light detection and ranging (LiDAR), stereo camera, global navigation satellite system (GNSS), and inertial measurement unit (IMU) sensor via ROS 2 (Robot Operating System), the system reliably detects the cones marking the track boundaries at distances of up to 35 m. Optimized trajectories are computed considering vehicle dynamics and simulated environmental factors such as visibility and lighting to navigate the track efficiently. The complete autonomous stack is implemented in ROS 2 and validated extensively in CARLA on a dedicated vehicle (ADS-DV) before being ported to the actual hardware, which includes the Jetson AGX Orin 64GB, ZED2i Stereo Camera, Robosense Helios 16P LiDAR, and CHCNAV Inertial Navigation System (INS).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u4e3b\u8d5b\u8f66\u7684\u8f6f\u4ef6\u5806\u6808\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u91c7\u96c6\u548c\u5904\u7406\u591a\u79cd\u4f20\u611f\u5668\u6570\u636e\uff0c\u6210\u529f\u9a8c\u8bc1\u4e86\u7ade\u4e89\u9a7e\u9a76\u6027\u80fd\uff0c\u5e76\u79fb\u690d\u81f3\u771f\u5b9e\u786c\u4ef6\u3002", "motivation": "\u81ea\u4e3b\u8d5b\u8f66\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6d4b\u8bd5\u611f\u77e5\u3001\u89c4\u5212\u548c\u63a7\u5236\u7684\u4e25\u683c\u73af\u5883\uff0c\u5177\u6709\u9ad8\u901f\u5ea6\u548c\u4e0d\u786e\u5b9a\u6027\u7684\u7279\u5f81\uff0c\u4e3a\u63d0\u5347\u7ade\u4e89\u9a7e\u9a76\u6027\u80fd\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002", "method": "\u5229\u7528ROS 2\u5b9e\u73b0\u4e00\u4e2a\u5b8c\u6574\u7684\u81ea\u4e3b\u7cfb\u7edf\u5806\u6808\uff0c\u4f7f\u7528LiDAR\u3001\u7acb\u4f53\u76f8\u673a\u3001GNSS\u548cIMU\u4f20\u611f\u5668\u8fdb\u884c\u6570\u636e\u91c7\u96c6\u548c\u5904\u7406\u3002", "result": "\u8be5\u7cfb\u7edf\u80fd\u591f\u572835\u7c73\u7684\u8ddd\u79bb\u5185\u53ef\u9760\u68c0\u6d4b\u8d5b\u9053\u8fb9\u754c\u7684\u5706\u9525\uff0c\u5e76\u4f18\u5316\u8f66\u8f86\u8f68\u8ff9\u4ee5\u9ad8\u6548\u7a7f\u8d8a\u8d5b\u9053\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u8bbe\u8ba1\u5e76\u8bc4\u4f30\u4e86\u4e00\u4e2a\u9002\u7528\u4e8e\u81ea\u4e3b\u8d5b\u8f66\u7684\u8f6f\u4ef6\u5806\u6808\uff0c\u7ecf\u8fc7\u5927\u91cf\u9a8c\u8bc1\u540e\u5c06\u5176\u79fb\u690d\u5230\u5b9e\u9645\u786c\u4ef6\u4e0a\u3002"}}
{"id": "2511.11456", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11456", "abs": "https://arxiv.org/abs/2511.11456", "authors": ["Xuyang Zhang", "Jiaqi Jiang", "Zhuo Chen", "Yongqiang Zhao", "Tianqi Yang", "Daniel Fernandes Gomes", "Jianan Wang", "Shan Luo"], "title": "SimTac: A Physics-Based Simulator for Vision-Based Tactile Sensing with Biomorphic Structures", "comment": null, "summary": "Tactile sensing in biological organisms is deeply intertwined with morphological form, such as human fingers, cat paws, and elephant trunks, which enables rich and adaptive interactions through a variety of geometrically complex structures. In contrast, vision-based tactile sensors in robotics have been limited to simple planar geometries, with biomorphic designs remaining underexplored. To address this gap, we present SimTac, a physics-based simulation framework for the design and validation of biomorphic tactile sensors. SimTac consists of particle-based deformation modeling, light-field rendering for photorealistic tactile image generation, and a neural network for predicting mechanical responses, enabling accurate and efficient simulation across a wide range of geometries and materials. We demonstrate the versatility of SimTac by designing and validating physical sensor prototypes inspired by biological tactile structures and further demonstrate its effectiveness across multiple Sim2Real tactile tasks, including object classification, slip detection, and contact safety assessment. Our framework bridges the gap between bio-inspired design and practical realisation, expanding the design space of tactile sensors and paving the way for tactile sensing systems that integrate morphology and sensing to enable robust interaction in unstructured environments.", "AI": {"tldr": "SimTac\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u7269\u7406\u6a21\u62df\u6846\u67b6\uff0c\u7528\u4e8e\u8bbe\u8ba1\u548c\u9a8c\u8bc1\u751f\u7269\u5f62\u6001\u89e6\u89c9\u4f20\u611f\u5668\uff0c\u5177\u6709\u591a\u6837\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u751f\u7269\u4f53\u4e2d\u7684\u89e6\u89c9\u611f\u77e5\u4e0e\u5f62\u6001\u5f62\u5f0f\u5bc6\u5207\u76f8\u5173\uff0c\u800c\u73b0\u6709\u7684\u673a\u5668\u4eba\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u5668\u4ec5\u9650\u4e8e\u7b80\u5355\u7684\u5e73\u9762\u51e0\u4f55\u5f62\u72b6\uff0c\u7f3a\u4e4f\u751f\u7269\u5f62\u6001\u8bbe\u8ba1\u7684\u63a2\u7d22\u3002", "method": "SimTac\u5305\u542b\u7c92\u5b50\u57fa\u7684\u5f62\u53d8\u5efa\u6a21\u3001\u5149\u573a\u6e32\u67d3\u7528\u4e8e\u751f\u6210\u903c\u771f\u7684\u89e6\u89c9\u56fe\u50cf\uff0c\u4ee5\u53ca\u9884\u6d4b\u673a\u68b0\u54cd\u5e94\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u652f\u6301\u5bf9\u591a\u79cd\u51e0\u4f55\u548c\u6750\u6599\u7684\u51c6\u786e\u9ad8\u6548\u6a21\u62df\u3002", "result": "\u63d0\u51fa\u4e86SimTac\uff0c\u4e00\u4e2a\u57fa\u4e8e\u7269\u7406\u7684\u6a21\u62df\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u7269\u5f62\u6001\u89e6\u89c9\u4f20\u611f\u5668\u7684\u8bbe\u8ba1\u548c\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u5176\u5728\u591a\u79cd\u51e0\u4f55\u5f62\u72b6\u548c\u6750\u6599\u4e0b\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u8bbe\u8ba1\u51fa\u7075\u611f\u6765\u81ea\u751f\u7269\u89e6\u89c9\u7ed3\u6784\u7684\u4f20\u611f\u5668\u539f\u578b\uff0c\u5e76\u5728\u591a\u4e2aSim2Real\u4efb\u52a1\u4e2d\u6709\u6548\u5e94\u7528\u3002", "conclusion": "SimTac\u6865\u63a5\u4e86\u4eff\u751f\u8bbe\u8ba1\u4e0e\u5b9e\u9645\u5b9e\u73b0\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u62d3\u5c55\u4e86\u89e6\u89c9\u4f20\u611f\u5668\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u4e3a\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u575a\u56fa\u7684\u4ea4\u4e92\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.11478", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11478", "abs": "https://arxiv.org/abs/2511.11478", "authors": ["Nhat Chung", "Taisei Hanyu", "Toan Nguyen", "Huy Le", "Frederick Bumgarner", "Duy Minh Ho Nguyen", "Khoa Vo", "Kashu Yamazaki", "Chase Rainwater", "Tung Kieu", "Anh Nguyen", "Ngan Le"], "title": "Rethinking Progression of Memory State in Robotic Manipulation: An Object-Centric Perspective", "comment": "Accepted at AAAI 2026", "summary": "As embodied agents operate in increasingly complex environments, the ability to perceive, track, and reason about individual object instances over time becomes essential, especially in tasks requiring sequenced interactions with visually similar objects. In these non-Markovian settings, key decision cues are often hidden in object-specific histories rather than the current scene. Without persistent memory of prior interactions (what has been interacted with, where it has been, or how it has changed) visuomotor policies may fail, repeat past actions, or overlook completed ones. To surface this challenge, we introduce LIBERO-Mem, a non-Markovian task suite for stress-testing robotic manipulation under object-level partial observability. It combines short- and long-horizon object tracking with temporally sequenced subgoals, requiring reasoning beyond the current frame. However, vision-language-action (VLA) models often struggle in such settings, with token scaling quickly becoming intractable even for tasks spanning just a few hundred frames. We propose Embodied-SlotSSM, a slot-centric VLA framework built for temporal scalability. It maintains spatio-temporally consistent slot identities and leverages them through two mechanisms: (1) slot-state-space modeling for reconstructing short-term history, and (2) a relational encoder to align the input tokens with action decoding. Together, these components enable temporally grounded, context-aware action prediction. Experiments show Embodied-SlotSSM's baseline performance on LIBERO-Mem and general tasks, offering a scalable solution for non-Markovian reasoning in object-centric robotic policies.", "AI": {"tldr": "LIBERO-Mem\u662f\u4e00\u4e2a\u975e\u9a6c\u5c14\u53ef\u592b\u4efb\u52a1\u5957\u4ef6\uff0c\u6311\u6218\u673a\u5668\u4eba\u5728\u5bf9\u8c61\u7ea7\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u4e0b\u7684\u64cd\u4f5c\uff0cEmbodied-SlotSSM\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5728\u590d\u6742\u73af\u5883\u4e2d\uff0c\u673a\u5668\u4eba\u9700\u8981\u8bc6\u522b\u3001\u8ddf\u8e2a\u5e76\u63a8\u7406\u5404\u4e2a\u5bf9\u8c61\u5b9e\u4f8b\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u987a\u5e8f\u4ea4\u4e92\u7684\u4efb\u52a1\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86\u4ee5\u63d2\u69fd\u4e3a\u4e2d\u5fc3\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6846\u67b6\uff08Embodied-SlotSSM\uff09\uff0c\u7528\u4e8e\u65f6\u95f4\u53ef\u6269\u5c55\u6027\uff0c\u7ed3\u5408\u4e86\u63d2\u69fd\u72b6\u6001\u7a7a\u95f4\u5efa\u6a21\u548c\u5173\u7cfb\u7f16\u7801\u5668\u3002", "result": "Embodied-SlotSSM\u80fd\u591f\u5b9e\u73b0\u65f6\u57df\u57fa\u7840\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u52a8\u4f5c\u9884\u6d4b\uff0c\u5e76\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u975e\u9a6c\u5c14\u53ef\u592b\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u826f\u597d\u7684\u57fa\u51c6\u8868\u73b0\u3002", "conclusion": "Embodied-SlotSSM\u5728LIBERO-Mem\u548c\u5176\u4ed6\u4e00\u822c\u4efb\u52a1\u4e0a\u7684\u57fa\u51c6\u8868\u73b0\u663e\u793a\u51fa\u5176\u5728\u5bf9\u8c61\u4e2d\u5fc3\u673a\u5668\u4eba\u7b56\u7565\u4e2d\u975e\u9a6c\u5c14\u53ef\u592b\u63a8\u7406\u7684\u53ef\u6269\u5c55\u6027\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.11484", "categories": ["cs.RO", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.11484", "abs": "https://arxiv.org/abs/2511.11484", "authors": ["Mustafa Erdem K\u0131rm\u0131z\u0131g\u00fcl", "Hasan Feyzi Do\u011fruyol", "Haluk Bayram"], "title": "A Comparative Evaluation of Prominent Methods in Autonomous Vehicle Certification", "comment": null, "summary": "The \"Vision Zero\" policy, introduced by the Swedish Parliament in 1997, aims to eliminate fatalities and serious injuries resulting from traffic accidents. To achieve this goal, the use of self-driving vehicles in traffic is envisioned and a roadmap for the certification of self-driving vehicles is aimed to be determined. However, it is still unclear how the basic safety requirements that autonomous vehicles must meet will be verified and certified, and which methods will be used. This paper focuses on the comparative evaluation of the prominent methods planned to be used in the certification process of autonomous vehicles. It examines the prominent methods used in the certification process, develops a pipeline for the certification process of autonomous vehicles, and determines the stages, actors, and areas where the addressed methods can be applied.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u745e\u5178\u5b9e\u65bd\u7684\u201c\u96f6\u6b7b\u4ea1\u201d\u653f\u7b56\u4e0b\uff0c\u81ea\u4e3b\u9a7e\u9a76\u6c7d\u8f66\u7684\u8ba4\u8bc1\u8fc7\u7a0b\u53ca\u5176\u4e3b\u8981\u65b9\u6cd5\u7684\u6bd4\u8f83\u8bc4\u4f30\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u586b\u8865\u81ea\u4e3b\u9a7e\u9a76\u6c7d\u8f66\u5b89\u5168\u6027\u9a8c\u8bc1\u548c\u8ba4\u8bc1\u65b9\u6cd5\u4e0a\u7684\u7a7a\u767d\uff0c\u4ee5\u652f\u6301\u201c\u96f6\u6b7b\u4ea1\u201d\u653f\u7b56\u7684\u76ee\u6807\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u8bc4\u4f30\u8ba1\u5212\u7528\u4e8e\u81ea\u4e3b\u9a7e\u9a76\u6c7d\u8f66\u8ba4\u8bc1\u8fc7\u7a0b\u7684\u4e3b\u8981\u65b9\u6cd5\uff0c\u6784\u5efa\u4e86\u8ba4\u8bc1\u6d41\u7a0b\u7684\u7ba1\u9053\u3002", "result": "\u4e3a\u81ea\u4e3b\u9a7e\u9a76\u6c7d\u8f66\u7684\u8ba4\u8bc1\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u6d41\u7a0b\u548c\u65b9\u6cd5\u5e94\u7528\u573a\u666f\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u5957\u81ea\u4e3b\u9a7e\u9a76\u6c7d\u8f66\u7684\u8ba4\u8bc1\u6d41\u7a0b\uff0c\u5305\u62ec\u5404\u4e2a\u9636\u6bb5\u548c\u76f8\u5173\u65b9\u6cd5\u7684\u5e94\u7528\u3002"}}
{"id": "2511.11512", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11512", "abs": "https://arxiv.org/abs/2511.11512", "authors": ["Yiyun Zhou", "Mingjing Xu", "Jingwei Shi", "Quanjiang Li", "Jingyuan Chen"], "title": "Collaborative Representation Learning for Alignment of Tactile, Language, and Vision Modalities", "comment": null, "summary": "Tactile sensing offers rich and complementary information to vision and language, enabling robots to perceive fine-grained object properties. However, existing tactile sensors lack standardization, leading to redundant features that hinder cross-sensor generalization. Moreover, existing methods fail to fully integrate the intermediate communication among tactile, language, and vision modalities. To address this, we propose TLV-CoRe, a CLIP-based Tactile-Language-Vision Collaborative Representation learning method. TLV-CoRe introduces a Sensor-Aware Modulator to unify tactile features across different sensors and employs tactile-irrelevant decoupled learning to disentangle irrelevant tactile features. Additionally, a Unified Bridging Adapter is introduced to enhance tri-modal interaction within the shared representation space. To fairly evaluate the effectiveness of tactile models, we further propose the RSS evaluation framework, focusing on Robustness, Synergy, and Stability across different methods. Experimental results demonstrate that TLV-CoRe significantly improves sensor-agnostic representation learning and cross-modal alignment, offering a new direction for multimodal tactile representation.", "AI": {"tldr": "TLV-CoRe\u662f\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u89e6\u89c9\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u7edf\u4e00\u89e6\u89c9\u7279\u5f81\u548c\u589e\u5f3a\u6a21\u6001\u95f4\u4e92\u52a8\uff0c\u63d0\u5347\u4e86\u8de8\u6a21\u6001\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u89e6\u89c9\u4f20\u611f\u5668\u7f3a\u4e4f\u6807\u51c6\u5316\uff0c\u5bfc\u81f4\u5197\u4f59\u7279\u5f81\u963b\u788d\u8de8\u4f20\u611f\u5668\u6cdb\u5316\uff0c\u540c\u65f6\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u6574\u5408\u89e6\u89c9\u3001\u8bed\u8a00\u548c\u89c6\u89c9\u6a21\u6001\u4e4b\u95f4\u7684\u4e2d\u95f4\u4ea4\u6d41\u3002", "method": "TLV-CoRe\u662f\u4e00\u79cd\u57fa\u4e8eCLIP\u7684\u89e6\u89c9-\u8bed\u8a00-\u89c6\u89c9\u534f\u540c\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u5f15\u5165\u4f20\u611f\u5668\u611f\u77e5\u8c03\u5236\u5668\u548c\u7edf\u4e00\u6865\u63a5\u9002\u914d\u5668\u6765\u589e\u5f3a\u4e09\u6a21\u6001\u4ea4\u4e92\u3002", "result": "\u901a\u8fc7\u5f15\u5165\u4f20\u611f\u5668\u611f\u77e5\u8c03\u5236\u5668\u548c\u5206\u79bb\u65e0\u5173\u89e6\u89c9\u7279\u5f81\u7684\u5b66\u4e60\uff0cTLV-CoRe\u6709\u6548\u5730\u7edf\u4e00\u4e86\u4e0d\u540c\u4f20\u611f\u5668\u7684\u89e6\u89c9\u7279\u5f81\uff0c\u5e76\u901a\u8fc7RSS\u8bc4\u4f30\u6846\u67b6\u8bc4\u4f30\u6a21\u578b\u6548\u679c\u3002", "conclusion": "TLV-CoRe\u663e\u8457\u63d0\u9ad8\u4e86\u4f20\u611f\u5668\u65e0\u5173\u7684\u8868\u793a\u5b66\u4e60\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u5f00\u542f\u4e86\u591a\u6a21\u6001\u89e6\u89c9\u8868\u793a\u7684\u65b0\u65b9\u5411\u3002"}}
{"id": "2511.11514", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11514", "abs": "https://arxiv.org/abs/2511.11514", "authors": ["Max M. Sun", "Jueun Kwon", "Todd Murphey"], "title": "Scalable Coverage Trajectory Synthesis on GPUs as Statistical Inference", "comment": "Presented at the \"Workshop on Fast Motion Planning and Control in the Era of Parallelism\" at Robotics: Science and Systems 2025. Workshop website: https://sites.google.com/rice.edu/parallelized-planning-control/", "summary": "Coverage motion planning is essential to a wide range of robotic tasks. Unlike conventional motion planning problems, which reason over temporal sequences of states, coverage motion planning requires reasoning over the spatial distribution of entire trajectories, making standard motion planning methods limited in computational efficiency and less amenable to modern parallelization frameworks. In this work, we formulate the coverage motion planning problem as a statistical inference problem from the perspective of flow matching, a generative modeling technique that has gained significant attention in recent years. The proposed formulation unifies commonly used statistical discrepancy measures, such as Kullback-Leibler divergence and Sinkhorn divergence, with a standard linear quadratic regulator problem. More importantly, it decouples the generation of trajectory gradients for coverage from the synthesis of control under nonlinear system dynamics, enabling significant acceleration through parallelization on modern computational architectures, particularly Graphics Processing Units (GPUs). This paper focuses on the advantages of this formulation in terms of scalability through parallelization, highlighting its computational benefits compared to conventional methods based on waypoint tracking.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u8986\u76d6\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5e94\u5bf9\u4f20\u7edf\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u5728\u5904\u7406\u8986\u76d6\u8fd0\u52a8\u89c4\u5212\u65f6\u7684\u8ba1\u7b97\u6548\u7387\u4f4e\u548c\u4e0d\u6613\u5e76\u884c\u5316\u7684\u95ee\u9898\u3002", "method": "\u5c06\u8986\u76d6\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\u6784\u5efa\u4e3a\u7edf\u8ba1\u63a8\u65ad\u95ee\u9898\uff0c\u8fd0\u7528\u6d41\u5339\u914d\u6d41\u884c\u7684\u751f\u6210\u5efa\u6a21\u6280\u672f\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u89e3\u8026\u8f68\u8ff9\u68af\u5ea6\u751f\u6210\u548c\u63a7\u5236\u5408\u6210\uff0c\u5b9e\u73b0\u663e\u8457\u52a0\u901f\uff0c\u7279\u522b\u9002\u7528\u4e8eGPU\u67b6\u6784\u3002", "conclusion": "\u8fd9\u79cd\u57fa\u4e8e\u7edf\u8ba1\u63a8\u65ad\u7684\u8986\u76d6\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u663e\u793a\u51fa\u5728\u53ef\u6269\u5c55\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u7684\u660e\u663e\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u4f7f\u7528\u73b0\u4ee3\u8ba1\u7b97\u67b6\u6784\u65f6\u3002"}}
{"id": "2511.11520", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11520", "abs": "https://arxiv.org/abs/2511.11520", "authors": ["Wei-Cheng Tseng", "Jinwei Gu", "Qinsheng Zhang", "Hanzi Mao", "Ming-Yu Liu", "Florian Shkurti", "Lin Yen-Chen"], "title": "Scalable Policy Evaluation with Video World Models", "comment": null, "summary": "Training generalist policies for robotic manipulation has shown great promise, as they enable language-conditioned, multi-task behaviors across diverse scenarios. However, evaluating these policies remains difficult because real-world testing is expensive, time-consuming, and labor-intensive. It also requires frequent environment resets and carries safety risks when deploying unproven policies on physical robots. Manually creating and populating simulation environments with assets for robotic manipulation has not addressed these issues, primarily due to the significant engineering effort required and the often substantial sim-to-real gap, both in terms of physics and rendering. In this paper, we explore the use of action-conditional video generation models as a scalable way to learn world models for policy evaluation. We demonstrate how to incorporate action conditioning into existing pre-trained video generation models. This allows leveraging internet-scale in-the-wild online videos during the pre-training stage, and alleviates the need for a large dataset of paired video-action data, which is expensive to collect for robotic manipulation. Our paper examines the effect of dataset diversity, pre-trained weight and common failure cases for the proposed evaluation pipeline.Our experiments demonstrate that, across various metrics, including policy ranking and the correlation between actual policy values and predicted policy values, these models offer a promising approach for evaluating policies without requiring real-world interactions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u52a8\u4f5c\u6761\u4ef6\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u8bc4\u4f30\u5b66\u4e60\u4e16\u754c\u6a21\u578b\uff0c\u4ece\u800c\u907f\u514d\u73b0\u5b9e\u4e16\u754c\u6d4b\u8bd5\u7684\u9ad8\u6210\u672c\u548c\u98ce\u9669\u3002", "motivation": "\u8bad\u7ec3\u901a\u7528\u7b56\u7565\u4ee5\u8fdb\u884c\u673a\u5668\u4eba\u64cd\u4f5c\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5176\u8bc4\u4f30\u56f0\u96be\uff0c\u539f\u56e0\u5728\u4e8e\u73b0\u5b9e\u4e16\u754c\u6d4b\u8bd5\u6210\u672c\u9ad8\u3001\u8017\u65f6\u4e14\u4eba\u5de5\u5bc6\u96c6\uff0c\u540c\u65f6\u8fd8\u5b58\u5728\u5b89\u5168\u98ce\u9669\u3002", "method": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5c06\u52a8\u4f5c\u6761\u4ef6\u96c6\u6210\u5230\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e2d\uff0c\u5229\u7528\u4e92\u8054\u7f51\u89c4\u6a21\u7684\u5728\u7ebf\u89c6\u9891\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u4ee5\u964d\u4f4e\u6536\u96c6\u6570\u636e\u7684\u6210\u672c\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5229\u7528\u57fa\u4e8e\u52a8\u4f5c\u6761\u4ef6\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u5b66\u4e60\u7528\u4e8e\u7b56\u7565\u8bc4\u4f30\u7684\u4e16\u754c\u6a21\u578b\uff0c\u964d\u4f4e\u5bf9\u73b0\u5b9e\u4e16\u754c\u4ea4\u4e92\u7684\u9700\u6c42\u3002", "conclusion": "\u901a\u8fc7\u5404\u79cd\u6307\u6807\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u7b56\u7565\u6392\u540d\u53ca\u5b9e\u9645\u653f\u7b56\u503c\u4e0e\u9884\u6d4b\u653f\u7b56\u503c\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u4e0a\uff0c\u6709\u52a9\u4e8e\u8bc4\u4f30\u7b56\u7565\uff0c\u800c\u65e0\u9700\u73b0\u5b9e\u4e16\u754c\u7684\u4e92\u52a8\u3002"}}
{"id": "2511.11529", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11529", "abs": "https://arxiv.org/abs/2511.11529", "authors": ["Luisa Mao", "Garret Warnell", "Peter Stone", "Joydeep Biswas"], "title": "Terrain Costmap Generation via Scaled Preference Conditioning", "comment": null, "summary": "Successful autonomous robot navigation in off-road domains requires the ability to generate high-quality terrain costmaps that are able to both generalize well over a wide variety of terrains and rapidly adapt relative costs at test time to meet mission-specific needs. Existing approaches for costmap generation allow for either rapid test-time adaptation of relative costs (e.g., semantic segmentation methods) or generalization to new terrain types (e.g., representation learning methods), but not both. In this work, we present scaled preference conditioned all-terrain costmap generation (SPACER), a novel approach for generating terrain costmaps that leverages synthetic data during training in order to generalize well to new terrains, and allows for rapid test-time adaptation of relative costs by conditioning on a user-specified scaled preference context. Using large-scale aerial maps, we provide empirical evidence that SPACER outperforms other approaches at generating costmaps for terrain navigation, with the lowest measured regret across varied preferences in five of seven environments for global path planning.", "AI": {"tldr": "SPACER\u662f\u4e00\u79cd\u65b0\u7684\u5730\u5f62\u6210\u672c\u56fe\u751f\u6210\u65b9\u6cd5\uff0c\u80fd\u5728\u65b0\u5730\u5f62\u4e2d\u826f\u597d\u6cdb\u5316\uff0c\u5141\u8bb8\u5feb\u901f\u9002\u5e94\u76f8\u5bf9\u6210\u672c\uff0c\u6d4b\u8bd5\u8868\u660e\u5176\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u8d8a\u91ce\u9886\u57df\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\uff0c\u9700\u8981\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5730\u5f62\u6210\u672c\u56fe\uff0c\u8fd9\u4e9b\u6210\u672c\u56fe\u4e0d\u4ec5\u8981\u80fd\u5e7f\u6cdb\u5730\u9002\u5e94\u591a\u79cd\u5730\u5f62\uff0c\u8fd8\u8981\u80fd\u5728\u6d4b\u8bd5\u65f6\u6839\u636e\u7279\u5b9a\u4efb\u52a1\u9700\u6c42\u8fc5\u901f\u8c03\u6574\u76f8\u5bf9\u6210\u672c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSPACER\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u5408\u6210\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u66f4\u597d\u5730\u751f\u6210\u9002\u5e94\u65b0\u5730\u5f62\u7684\u6210\u672c\u56fe\uff0c\u540c\u65f6\u5141\u8bb8\u5728\u6d4b\u8bd5\u65f6\u5feb\u901f\u9002\u5e94\u76f8\u5bf9\u6210\u672c\u3002", "result": "SPACER\u65b9\u6cd5\u5728\u751f\u6210\u5730\u5f62\u5bfc\u822a\u6210\u672c\u56fe\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5728\u4e03\u4e2a\u73af\u5883\u4e2d\u4e94\u4e2a\u73af\u5883\u4e2d\u5bf9\u4e8e\u5168\u5c40\u8def\u5f84\u89c4\u5212\u6d4b\u5f97\u7684\u9057\u61be\u503c\u6700\u4f4e\u3002", "conclusion": "SPACER\u5229\u7528\u5408\u6210\u6570\u636e\u548c\u7528\u6237\u6307\u5b9a\u7684\u504f\u597d\u4e0a\u4e0b\u6587\uff0c\u6210\u529f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6210\u672c\u56fe\uff0c\u8bc1\u660e\u4e86\u5728\u590d\u6742\u5730\u5f62\u5bfc\u822a\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.11533", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11533", "abs": "https://arxiv.org/abs/2511.11533", "authors": ["Jueun Kwon", "Max M. Sun", "Todd Murphey"], "title": "Volumetric Ergodic Control", "comment": "8 pages, 8 figures", "summary": "Ergodic control synthesizes optimal coverage behaviors over spatial distributions for nonlinear systems. However, existing formulations model the robot as a non-volumetric point, but in practice a robot interacts with the environment through its body and sensors with physical volume. In this work, we introduce a new ergodic control formulation that optimizes spatial coverage using a volumetric state representation. Our method preserves the asymptotic coverage guarantees of ergodic control, adds minimal computational overhead for real-time control, and supports arbitrary sample-based volumetric models. We evaluate our method across search and manipulation tasks -- with multiple robot dynamics and end-effector geometries or sensor models -- and show that it improves coverage efficiency by more than a factor of two while maintaining a 100% task completion rate across all experiments, outperforming the standard ergodic control method. Finally, we demonstrate the effectiveness of our method on a robot arm performing mechanical erasing tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6709\u5229\u4e8e\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u4f53\u79ef\u72b6\u6001\u8868\u793a\u7684\u904d\u5386\u63a7\u5236\u65b9\u6cd5\uff0c\u4f18\u5316\u7a7a\u95f4\u8986\u76d6\uff0c\u663e\u8457\u63d0\u9ad8\u8986\u76d6\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u904d\u5386\u63a7\u5236\u65b9\u6cd5\u4e2d\uff0c\u673a\u5668\u4eba\u88ab\u7b80\u5316\u4e3a\u65e0\u4f53\u79ef\u70b9\u800c\u65e0\u6cd5\u53cd\u6620\u5b9e\u9645\u7269\u7406\u4ea4\u4e92\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u4f53\u79ef\u72b6\u6001\u8868\u793a\u7684\u904d\u5386\u63a7\u5236\uff0c\u5176\u8ba1\u7b97\u5f00\u9500\u6700\u5c0f\uff0c\u5e76\u652f\u6301\u4efb\u610f\u6837\u672c\u57fa\u7840\u7684\u4f53\u79ef\u6a21\u578b\u3002", "result": "\u5728\u641c\u7d22\u548c\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u8986\u76d6\u6548\u7387\uff0c\u5e76\u5728\u673a\u68b0\u64e6\u9664\u4efb\u52a1\u4e2d\u6210\u529f\u5e94\u7528\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u5728\u591a\u4efb\u52a1\u5b9e\u9a8c\u4e2d\u5c55\u73b0\u51fa\u8d85\u8fc7\u4e24\u500d\u7684\u8986\u76d6\u6548\u7387\u63d0\u5347\uff0c\u5e76\u4fdd\u6301100%\u7684\u4efb\u52a1\u5b8c\u6210\u7387\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u904d\u5386\u63a7\u5236\u65b9\u6cd5\u3002"}}
