{"id": "2511.05642", "categories": ["cs.RO", "cs.AR", "cs.CV", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.05642", "abs": "https://arxiv.org/abs/2511.05642", "authors": ["Justin Williams", "Kishor Datta Gupta", "Roy George", "Mrinmoy Sarkar"], "title": "Lite VLA: Efficient Vision-Language-Action Control on CPU-Bound Edge Robots", "comment": null, "summary": "The deployment of artificial intelligence models at the edge is increasingly critical for autonomous robots operating in GPS-denied environments where local, resource-efficient reasoning is essential. This work demonstrates the feasibility of deploying small Vision-Language Models (VLMs) on mobile robots to achieve real-time scene understanding and reasoning under strict computational constraints. Unlike prior approaches that separate perception from mobility, the proposed framework enables simultaneous movement and reasoning in dynamic environments using only on-board hardware. The system integrates a compact VLM with multimodal perception to perform contextual interpretation directly on embedded hardware, eliminating reliance on cloud connectivity. Experimental validation highlights the balance between computational efficiency, task accuracy, and system responsiveness. Implementation on a mobile robot confirms one of the first successful deployments of small VLMs for concurrent reasoning and mobility at the edge. This work establishes a foundation for scalable, assured autonomy in applications such as service robotics, disaster response, and defense operations.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728GPS\u53d7\u9650\u73af\u5883\u4e0b\u7684\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u65f6\u573a\u666f\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff0c\u6d88\u9664\u4e86\u5bf9\u4e91\u8fde\u63a5\u7684\u4f9d\u8d56\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u81ea\u4e3b\u51b3\u7b56\u3002", "motivation": "\u5728GPS\u53d7\u9650\u73af\u5883\u4e0b\uff0c\u79fb\u52a8\u673a\u5668\u4eba\u9700\u8981\u8fdb\u884c\u672c\u5730\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u63a8\u7406\uff0c\u4ee5\u5b9e\u73b0\u81ea\u4e3b\u8fd0\u884c\u3002", "method": "\u901a\u8fc7\u96c6\u6210\u7d27\u51d1\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u6001\u611f\u77e5\uff0c\u5b9e\u65bd\u4e86\u4e00\u79cd\u5141\u8bb8\u79fb\u52a8\u548c\u63a8\u7406\u540c\u65f6\u8fdb\u884c\u7684\u65b0\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5728\u8ba1\u7b97\u6548\u7387\u3001\u4efb\u52a1\u51c6\u786e\u6027\u548c\u54cd\u5e94\u6027\u4e4b\u95f4\u7684\u5e73\u8861\uff0c\u5e76\u5b9e\u73b0\u4e86\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8fb9\u7f18\u7684\u6210\u529f\u90e8\u7f72\u3002", "conclusion": "\u8be5\u7814\u7a76\u5b9e\u73b0\u4e86\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u79fb\u52a8\u673a\u5668\u4eba\u4e0a\u7684\u6210\u529f\u5e94\u7528\uff0c\u4e3a\u8fb9\u7f18\u8ba1\u7b97\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u81ea\u4e3b\u51b3\u7b56\u57fa\u7840\u3002"}}
{"id": "2511.05680", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05680", "abs": "https://arxiv.org/abs/2511.05680", "authors": ["Jeong-Jung Kim", "Doo-Yeol Koh", "Chang-Hyun Kim"], "title": "VLM-driven Skill Selection for Robotic Assembly Tasks", "comment": null, "summary": "This paper presents a robotic assembly framework that combines Vision-Language Models (VLMs) with imitation learning for assembly manipulation tasks. Our system employs a gripper-equipped robot that moves in 3D space to perform assembly operations. The framework integrates visual perception, natural language understanding, and learned primitive skills to enable flexible and adaptive robotic manipulation. Experimental results demonstrate the effectiveness of our approach in assembly scenarios, achieving high success rates while maintaining interpretability through the structured primitive skill decomposition.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u548c\u6a21\u4eff\u5b66\u4e60\u7684\u673a\u5668\u4eba\u7ec4\u88c5\u6846\u67b6\uff0c\u7528\u4e8e\u66f4\u7075\u6d3b\u7684\u7ec4\u88c5\u64cd\u4f5c\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u96c6\u6210\u5148\u8fdb\u7684\u6280\u672f\u63d0\u5347\u673a\u5668\u4eba\u5728\u7ec4\u88c5\u4efb\u52a1\u4e2d\u7684\u7075\u6d3b\u6027\u548c\u9002\u5e94\u80fd\u529b\u3002", "method": "\u5c06\u89c6\u89c9\u611f\u77e5\u3001\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u548c\u5b66\u4e60\u7684\u57fa\u672c\u6280\u80fd\u76f8\u7ed3\u5408\uff0c\u4ee5\u5b9e\u73b0\u7075\u6d3b\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7ec4\u88c5\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u9ad8\u6210\u529f\u7387\u3002", "conclusion": "\u6240\u63d0\u6846\u67b6\u5728\u7ec4\u88c5\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u9ad8\u6210\u529f\u7387\uff0c\u5e76\u4fdd\u6301\u4e86\u6280\u80fd\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2511.05785", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05785", "abs": "https://arxiv.org/abs/2511.05785", "authors": ["Lianhao Yin", "Haiping Yu", "Pascal Spino", "Daniela Rus"], "title": "A Unified Stochastic Mechanism Underlying Collective Behavior in Ants, Physical Systems, and Robotic Swarms", "comment": null, "summary": "Biological swarms, such as ant colonies, achieve collective goals through decentralized and stochastic individual behaviors. Similarly, physical systems composed of gases, liquids, and solids exhibit random particle motion governed by entropy maximization, yet do not achieve collective objectives. Despite this analogy, no unified framework exists to explain the stochastic behavior in both biological and physical systems. Here, we present empirical evidence from \\textit{Formica polyctena} ants that reveals a shared statistical mechanism underlying both systems: maximization under different energy function constraints. We further demonstrate that robotic swarms governed by this principle can exhibit scalable, decentralized cooperation, mimicking physical phase-like behaviors with minimal individual computation. These findings established a unified stochastic model linking biological, physical, and robotic swarms, offering a scalable principle for designing robust and intelligent swarm robotics.", "AI": {"tldr": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u751f\u7269\u548c\u7269\u7406\u7cfb\u7edf\u4e2d\u968f\u673a\u884c\u4e3a\u7684\u5171\u4eab\u7edf\u8ba1\u673a\u5236\uff0c\u5e76\u5efa\u7acb\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u968f\u673a\u6a21\u578b\u4ee5\u8bbe\u8ba1\u66f4\u667a\u80fd\u7684\u7fa4\u4f53\u673a\u5668\u4eba\u3002", "motivation": "\u751f\u7269\u7fa4\u4f53\u5982\u8681\u7fa4\u901a\u8fc7\u5206\u6563\u548c\u968f\u673a\u7684\u4e2a\u4f53\u884c\u4e3a\u5b9e\u73b0\u96c6\u4f53\u76ee\u6807\uff0c\u800c\u7269\u7406\u7cfb\u7edf\u867d\u5177\u968f\u673a\u7c92\u5b50\u8fd0\u52a8\u5374\u672a\u80fd\u5b9e\u73b0\u96c6\u4f53\u76ee\u6807\u3002\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u6765\u89e3\u91ca\u751f\u7269\u548c\u7269\u7406\u7cfb\u7edf\u4e2d\u7684\u968f\u673a\u884c\u4e3a\u3002", "method": "\u57fa\u4e8e\u5bf9\u7ea2\u706b\u8681\uff08Formica polyctena\uff09\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u8bc1\u660e\u4e86\u5728\u4e0d\u540c\u80fd\u91cf\u51fd\u6570\u7ea6\u675f\u4e0b\u7684\u6700\u5927\u5316\u5171\u4eab\u7edf\u8ba1\u673a\u5236\u3002", "result": "\u5c55\u793a\u4e86\u57fa\u4e8e\u8be5\u539f\u5219\u7684\u673a\u5668\u4eba\u7fa4\u4f53\u80fd\u591f\u8868\u73b0\u51fa\u53ef\u6269\u5c55\u7684\u3001\u5206\u6563\u7684\u5408\u4f5c\uff0c\u6a21\u62df\u5177\u6709\u6700\u5c0f\u4e2a\u4f53\u8ba1\u7b97\u7684\u7269\u7406\u76f8\u4f3c\u884c\u4e3a\u3002", "conclusion": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u968f\u673a\u6a21\u578b\uff0c\u5c06\u751f\u7269\u3001\u7269\u7406\u548c\u673a\u5668\u4eba\u7fa4\u4f53\u8054\u7cfb\u8d77\u6765\uff0c\u4e3a\u8bbe\u8ba1\u5f3a\u5927\u800c\u667a\u80fd\u7684\u7fa4\u4f53\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u539f\u5219\u3002"}}
{"id": "2511.05791", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05791", "abs": "https://arxiv.org/abs/2511.05791", "authors": ["Manav Kulshrestha", "S. Talha Bukhari", "Damon Conover", "Aniket Bera"], "title": "VLAD-Grasp: Zero-shot Grasp Detection via Vision-Language Models", "comment": "8 pages, 4 figures, under review", "summary": "Robotic grasping is a fundamental capability for autonomous manipulation; however, most existing methods rely on large-scale expert annotations and necessitate retraining to handle new objects. We present VLAD-Grasp, a Vision-Language model Assisted zero-shot approach for Detecting grasps. From a single RGB-D image, our method (1) prompts a large vision-language model to generate a goal image where a straight rod \"impales\" the object, representing an antipodal grasp, (2) predicts depth and segmentation to lift this generated image into 3D, and (3) aligns generated and observed object point clouds via principal component analysis and correspondence-free optimization to recover an executable grasp pose. Unlike prior work, our approach is training-free and does not rely on curated grasp datasets. Despite this, VLAD-Grasp achieves performance that is competitive with or superior to that of state-of-the-art supervised models on the Cornell and Jacquard datasets. We further demonstrate zero-shot generalization to novel real-world objects on a Franka Research 3 robot, highlighting vision-language foundation models as powerful priors for robotic manipulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u673a\u5668\u4eba\u6293\u53d6\u65b9\u6cd5VLAD-Grasp\uff0c\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u751f\u6210\u76ee\u6807\u56fe\u50cf\u5e76\u5b9e\u73b0\u96f6\u6837\u672c\u6293\u53d6\uff0c\u6027\u80fd\u4f18\u4e8e\u8bb8\u591a\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u3002", "motivation": "\u63d0\u5347\u673a\u5668\u4eba\u6293\u53d6\u80fd\u529b\uff0c\u51cf\u5c11\u5bf9\u5927\u91cf\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u89e3\u51b3\u5904\u7406\u65b0\u7269\u4f53\u65f6\u9700\u8981\u91cd\u8bad\u7ec3\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528RGB-D\u56fe\u50cf\uff0c\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5bf9\u5e94\u76ee\u6807\u56fe\u50cf\uff0c\u7ed3\u5408\u6df1\u5ea6\u548c\u5206\u5272\u4fe1\u606f\uff0c\u5c06\u56fe\u50cf\u5347\u7ef4\u81f33D\uff0c\u7136\u540e\u4f7f\u7528\u4e3b\u6210\u5206\u5206\u6790\u548c\u65e0\u5bf9\u5e94\u4f18\u5316\u5bf9\u70b9\u4e91\u8fdb\u884c\u5bf9\u9f50\uff0c\u6062\u590d\u53ef\u6267\u884c\u7684\u6293\u53d6\u59ff\u52bf\u3002", "result": "VLAD-Grasp\u5728\u65e0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u5176\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u7684\u76d1\u7763\u6a21\u578b\u76f8\u5f53\u6216\u4f18\u4e8e\u4e4b\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u63a8\u65ad\u3002", "conclusion": "VLAD-Grasp\u5c55\u793a\u4e86\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u63a7\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u5b9e\u73b0\u5bf9\u65b0\u7269\u4f53\u7684\u96f6\u6837\u672c\u6cdb\u5316\u3002"}}
{"id": "2511.05706", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05706", "abs": "https://arxiv.org/abs/2511.05706", "authors": ["Wendan Jiang", "Shiyuan Wang", "Hiba Eltigani", "Rukhshan Haroon", "Abdullah Bin Faisal", "Fahad Dogar"], "title": "AdvisingWise: Supporting Academic Advising in Higher Educations Through a Human-in-the-Loop Multi-Agent Framework", "comment": "18 pages, 6 figures", "summary": "Academic advising is critical to student success in higher education, yet high student-to-advisor ratios limit advisors' capacity to provide timely support, particularly during peak periods. Recent advances in Large Language Models (LLMs) present opportunities to enhance the advising process. We present AdvisingWise, a multi-agent system that automates time-consuming tasks, such as information retrieval and response drafting, while preserving human oversight. AdvisingWise leverages authoritative institutional resources and adaptively prompts students about their academic backgrounds to generate reliable, personalized responses. All system responses undergo human advisor validation before delivery to students. We evaluate AdvisingWise through a mixed-methods approach: (1) expert evaluation on responses of 20 sample queries, (2) LLM-as-a-judge evaluation of the information retrieval strategy, and (3) a user study with 8 academic advisors to assess the system's practical utility. Our evaluation shows that AdvisingWise produces accurate, personalized responses. Advisors reported increasingly positive perceptions after using AdvisingWise, as their initial concerns about reliability and personalization diminished. We conclude by discussing the implications of human-AI synergy on the practice of academic advising.", "AI": {"tldr": "AdvisingWise \u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u5b66\u672f\u6307\u5bfc\u7cfb\u7edf\uff0c\u5229\u7528 LLM \u63d0\u9ad8\u54a8\u8be2\u6548\u7387\uff0c\u7ecf\u8fc7\u8bc4\u4f30\u663e\u793a\u80fd\u591f\u4ea7\u751f\u53ef\u9760\u7684\u4e2a\u6027\u5316\u56de\u590d\uff0c\u589e\u5f3a\u4e86\u987e\u95ee\u7684\u79ef\u6781\u6027\u3002", "motivation": "\u9ad8\u5b66\u751f\u4e0e\u987e\u95ee\u6bd4\u7387\u9650\u5236\u4e86\u987e\u95ee\u63d0\u4f9b\u53ca\u65f6\u652f\u6301\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u9ad8\u5cf0\u671f\u3002", "method": "\u901a\u8fc7\u6df7\u5408\u65b9\u6cd5\u8bc4\u4f30 AdvisingWise\uff0c\u5305\u62ec\u4e13\u5bb6\u8bc4\u4f30\u3001\u4fe1\u606f\u68c0\u7d22\u7b56\u7565\u7684 LLM \u8bc4\u5224\u548c\u7528\u6237\u7814\u7a76\u3002", "result": "AdvisingWise \u6210\u529f\u5730\u81ea\u52a8\u5316\u4e86\u4fe1\u606f\u68c0\u7d22\u548c\u56de\u590d\u8d77\u8349\uff0c\u4ea7\u751f\u51c6\u786e\u7684\u4e2a\u6027\u5316\u56de\u590d\uff0c\u4e14\u7ecf\u8fc7\u4eba\u7c7b\u987e\u95ee\u9a8c\u8bc1\u3002", "conclusion": "\u4eba\u673a\u534f\u540c\u5728\u5b66\u672f\u6307\u5bfc\u5b9e\u8df5\u4e2d\u7684\u5f71\u54cd\u503c\u5f97\u6df1\u5165\u63a2\u8ba8\uff0cAdvisingWise \u80fd\u591f\u4ea7\u751f\u51c6\u786e\u3001\u4e2a\u6027\u5316\u7684\u56de\u590d\uff0c\u4e14\u63d0\u9ad8\u4e86\u987e\u95ee\u7684\u79ef\u6781\u6027\u3002"}}
{"id": "2511.05816", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05816", "abs": "https://arxiv.org/abs/2511.05816", "authors": ["Taku Okawara", "Ryo Nishibe", "Mao Kasano", "Kentaro Uno", "Kazuya Yoshida"], "title": "3D Mapping Using a Lightweight and Low-Power Monocular Camera Embedded inside a Gripper of Limbed Climbing Robots", "comment": "International Conference on Space Robotics (iSpaRo)", "summary": "Limbed climbing robots are designed to explore challenging vertical walls, such as the skylights of the Moon and Mars. In such robots, the primary role of a hand-eye camera is to accurately estimate 3D positions of graspable points (i.e., convex terrain surfaces) thanks to its close-up views. While conventional climbing robots often employ RGB-D cameras as hand-eye cameras to facilitate straightforward 3D terrain mapping and graspable point detection, RGB-D cameras are large and consume considerable power.\n  This work presents a 3D terrain mapping system designed for space exploration using limbed climbing robots equipped with a monocular hand-eye camera. Compared to RGB-D cameras, monocular cameras are more lightweight, compact structures, and have lower power consumption. Although monocular SLAM can be used to construct 3D maps, it suffers from scale ambiguity. To address this limitation, we propose a SLAM method that fuses monocular visual constraints with limb forward kinematics. The proposed method jointly estimates time-series gripper poses and the global metric scale of the 3D map based on factor graph optimization.\n  We validate the proposed framework through both physics-based simulations and real-world experiments. The results demonstrate that our framework constructs a metrically scaled 3D terrain map in real-time and enables autonomous grasping of convex terrain surfaces using a monocular hand-eye camera, without relying on RGB-D cameras. Our method contributes to scalable and energy-efficient perception for future space missions involving limbed climbing robots. See the video summary here: https://youtu.be/fMBrrVNKJfc", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5355\u76ee\u624b\u773c\u76f8\u673a\u76843D\u5730\u5f62\u6620\u5c04\u7cfb\u7edf\uff0c\u9002\u7528\u4e8e\u63a2\u7d22\u6708\u7403\u548c\u706b\u661f\u7684\u6500\u722c\u673a\u5668\u4eba\u3002", "motivation": "\u4f20\u7edf\u7684RGB-D\u76f8\u673a\u867d\u7136\u65b9\u4fbf3D\u5730\u5f62\u6620\u5c04\uff0c\u4f46\u4f53\u79ef\u5927\u3001\u8017\u7535\u591a\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b87\u5b99\u63a2\u6d4b\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5355\u76eeSLAM\u4e0e\u80a2\u4f53\u524d\u5411\u8fd0\u52a8\u5b66\u7ed3\u5408\u7684SLAM\u65b9\u6cd5\uff0c\u901a\u8fc7\u56e0\u5b50\u56fe\u4f18\u5316\u5171\u540c\u4f30\u8ba1\u65f6\u5e8f\u5939\u722a\u59ff\u6001\u548c3D\u5730\u56fe\u7684\u5168\u7403\u5ea6\u91cf\u5c3a\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u5b9e\u65f6\u6784\u5efa\u5e26\u6709\u5ea6\u91cf\u7f29\u653e\u76843D\u5730\u5f62\u5730\u56fe\uff0c\u5e76\u901a\u8fc7\u5355\u76ee\u76f8\u673a\u5b9e\u73b0\u81ea\u4e3b\u6293\u53d6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u878d\u5408\u5355\u76ee\u89c6\u89c9\u7ea6\u675f\u548c\u80a2\u4f53\u524d\u5411\u8fd0\u52a8\u5b66\uff0c\u5b9e\u73b0\u4e86\u5728\u5b9e\u65f6\u6784\u5efa3D\u5730\u5f62\u5730\u56fe\u7684\u540c\u65f6\u8fdb\u884c\u81ea\u4e3b\u6293\u53d6\u3002"}}
{"id": "2511.05737", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.05737", "abs": "https://arxiv.org/abs/2511.05737", "authors": ["George X. Wang", "Yuyang Shen"], "title": "Home Environment and Student Creative Thinking: An Educational Data Science Analysis of PISA 2022", "comment": "Accepted for presentation at the 2026 AERA Annual Meeting (SIG: Technology, Instruction, Cognition & Learning)", "summary": "This study investigates how student exposure to resources in their home environments relates to creative thinking performance, using data from the PISA 2022 Creative Thinking assessment. It focuses on two primary questions: (1) How strongly is exposure to cultural, educational, and digital resources associated with creativity? (2) Do students perform better on divergent thinking tasks when physically engaged or digitally stimulated? Drawing on a sample of 15,425 students from 60 countries, the study applies high-dimensional regression and factor analysis to identify patterns across a wide range of exposure variables. To model the latent structure of home environment variables, we conducted a Confirmatory Factor Analysis. The analysis specified two latent factors: Physical Exposure and Digital Exposure. The model demonstrated excellent fit, with a Comparative Fit Index (CFI) of 0.971 and a Root Mean Square Error of Approximation (RMSEA) of 0.038. When both factors were entered together in the regression, physical and digital exposures each contributed unique explanatory power. There is no indication that one simply proxies the other; rather, they appear to be complementary dimensions of a creative home environment. This study offers compelling international evidence that both physical and digital resources in the home environment play significant, independent, and complementary roles in shaping adolescent creative thinking abilities. These findings have direct implications for efforts to promote creativity and equity in education.", "AI": {"tldr": "\u5bb6\u5ead\u73af\u5883\u4e2d\u7684\u6587\u5316\u3001\u6559\u80b2\u4e0e\u6570\u5b57\u8d44\u6e90\u4e0e\u5b66\u751f\u521b\u9020\u529b\u8868\u73b0\u5bc6\u5207\u76f8\u5173\uff0c\u7269\u7406\u548c\u6570\u5b57\u523a\u6fc0\u5bf9\u521b\u9020\u6027\u601d\u7ef4\u5747\u6709\u72ec\u7acb\u8d21\u732e\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u5bb6\u5ead\u73af\u5883\u5bf9\u5b66\u751f\u521b\u9020\u6027\u601d\u7ef4\u8868\u73b0\u7684\u5f71\u54cd\uff0c\u7279\u522b\u5173\u6ce8\u6587\u5316\u3001\u6559\u80b2\u548c\u6570\u5b57\u8d44\u6e90\u7684\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u5bf9\u6765\u81ea60\u4e2a\u56fd\u5bb6\u768415425\u540d\u5b66\u751f\u7684\u6570\u636e\u5206\u6790\uff0c\u5e94\u7528\u9ad8\u7ef4\u56de\u5f52\u548c\u9a8c\u8bc1\u6027\u56e0\u5b50\u5206\u6790\uff0c\u8bc6\u522b\u4e86\u5bb6\u5ead\u73af\u5883\u8d44\u6e90\u7684\u6f5c\u5728\u7ed3\u6784\u3002", "result": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u5bb6\u5ead\u73af\u5883\u4e2d\u7684\u7269\u7406\u548c\u6570\u5b57\u8d44\u6e90\u5bf9\u9752\u5c11\u5e74\u521b\u9020\u529b\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4e8c\u8005\u5728\u4fc3\u8fdb\u521b\u9020\u6027\u601d\u7ef4\u7684\u80fd\u529b\u65b9\u9762\u5404\u81ea\u72ec\u7acb\u4e14\u4e92\u8865\u3002", "conclusion": "\u672c\u7814\u7a76\u5f3a\u8c03\uff0c\u5728\u5bb6\u5ead\u73af\u5883\u4e2d\u4fc3\u8fdb\u521b\u9020\u6027\u601d\u7ef4\u9700\u8981\u540c\u65f6\u91cd\u89c6\u7269\u7406\u548c\u6570\u5b57\u8d44\u6e90\u7684\u63d0\u4f9b\uff0c\u4e8c\u8005\u662f\u4e92\u4e3a\u8865\u5145\u7684\u56e0\u7d20\u3002"}}
{"id": "2511.05855", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05855", "abs": "https://arxiv.org/abs/2511.05855", "authors": ["Jiayu Zhou", "Qiwei Wu", "Jian Li", "Zhe Chen", "Xiaogang Xiong", "Renjing Xu"], "title": "Gentle Manipulation Policy Learning via Demonstrations from VLM Planned Atomic Skills", "comment": "Accepted for the 40th Annual AAAI Conference on Artificial Intelligence (2026)", "summary": "Autonomous execution of long-horizon, contact-rich manipulation tasks traditionally requires extensive real-world data and expert engineering, posing significant cost and scalability challenges. This paper proposes a novel framework integrating hierarchical semantic decomposition, reinforcement learning (RL), visual language models (VLMs), and knowledge distillation to overcome these limitations. Complex tasks are decomposed into atomic skills, with RL-trained policies for each primitive exclusively in simulation. Crucially, our RL formulation incorporates explicit force constraints to prevent object damage during delicate interactions. VLMs perform high-level task decomposition and skill planning, generating diverse expert demonstrations. These are distilled into a unified policy via Visual-Tactile Diffusion Policy for end-to-end execution. We conduct comprehensive ablation studies exploring different VLM-based task planners to identify optimal demonstration generation pipelines, and systematically compare imitation learning algorithms for skill distillation. Extensive simulation experiments and physical deployment validate that our approach achieves policy learning for long-horizon manipulation without costly human demonstrations, while the VLM-guided atomic skill framework enables scalable generalization to diverse tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u6280\u672f\u89e3\u51b3\u4e86\u81ea\u4e3b\u64cd\u4f5c\u957f\u671f\u4efb\u52a1\u4e2d\u7684\u6210\u672c\u548c\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u65e0\u6602\u8d35\u793a\u8303\u7684\u7b56\u7565\u5b66\u4e60\u3002", "motivation": "\u4f20\u7edf\u7684\u81ea\u4e3b\u6267\u884c\u957f\u671f\u3001\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\u9700\u8981\u5927\u91cf\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u548c\u4e13\u5bb6\u5de5\u7a0b\uff0c\u5b58\u5728\u6210\u672c\u548c\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0c\u56e0\u6b64\u63d0\u51fa\u6b64\u6846\u67b6\u4ee5\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u5206\u5c42\u8bed\u4e49\u5206\u89e3\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u77e5\u8bc6\u84b8\u998f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u590d\u6742\u4efb\u52a1\u5206\u89e3\u4e3a\u539f\u5b50\u6280\u80fd\u7684\u6846\u67b6\uff0c\u5e76\u5728\u4eff\u771f\u4e2d\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u7684\u4eff\u771f\u5b9e\u9a8c\u548c\u5b9e\u9645\u90e8\u7f72\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u901a\u8fc7VLM\u5f15\u5bfc\u7684\u4efb\u52a1\u89c4\u5212\u548c\u6280\u80fd\u84b8\u998f\u7684\u4f18\u52bf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5728\u6ca1\u6709\u6602\u8d35\u4eba\u7c7b\u793a\u8303\u7684\u60c5\u51b5\u4e0b\u5bf9\u957f\u671f\u64cd\u4f5c\u7b56\u7565\u7684\u5b66\u4e60\uff0c\u5e76\u4e14VLM\u5f15\u5bfc\u7684\u539f\u5b50\u6280\u80fd\u6846\u67b6\u5b9e\u73b0\u4e86\u5bf9\u591a\u79cd\u4efb\u52a1\u7684\u53ef\u6269\u5c55\u6027\u63a8\u5e7f\u3002"}}
{"id": "2511.05744", "categories": ["cs.HC", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.05744", "abs": "https://arxiv.org/abs/2511.05744", "authors": ["Kexin Liang", "Simeon C. Calvert", "J. W. C. van Lint"], "title": "Adaptive Time Budgets for Safe and Comfortable Vehicle Control Transition in Conditionally Automated Driving", "comment": "10 pages, 6 figures", "summary": "Conditionally automated driving requires drivers to resume vehicle control promptly when automation reaches its operational limits. Ensuring smooth vehicle control transitions is critical for the safety and efficiency of mixed-traffic transportation systems, where complex interactions and variable traffic behaviors pose additional challenges. This study addresses this challenge by introducing an adaptive time budget framework that provides drivers with sufficient time to complete takeovers both safely and comfortably across diverse scenarios. We focus in particular on the takeover buffer, that is, the extra time available after drivers consciously resume control to complete evasive maneuvers. A driving simulator experiment is conducted to evaluate the influence of different takeover buffer lengths on safety-related indicators (minimum time-to-collision, maximum deceleration, and steering wheel angle) and subjective assessments (perceived time sufficiency, perceived risk, and performance satisfaction). Results show that (i) takeover buffers of about 5-6 seconds consistently lead to optimal safety and comfort; and (ii) drivers prefer relatively stable takeover buffers across varying traffic densities and n-back tasks. This study introduces an adaptive time budget framework that dynamically allocates transition time by incorporating a predicted takeover time and a preferred takeover buffer (piecewise function). This can serve as an important first step toward providing drivers with sufficient time to resume vehicle control across diverse scenarios, which needs to be validated in more diverse and real-world driving contexts. By aligning the provided time budget with driver needs under specific circumstances, the adaptive framework can improve reliability of control transitions, facilitate human-centered automated driving, reduce crash risk, and maintain overall traffic efficiency.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u9a7e\u9a76\u6a21\u62df\u5b9e\u9a8c\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u5e94\u6027\u65f6\u95f4\u9884\u7b97\u6846\u67b6\uff0c\u53d1\u73b05-6\u79d2\u7684\u63a5\u7ba1\u7f13\u51b2\u65f6\u95f4\u6709\u5229\u4e8e\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u548c\u8212\u9002\u6027\u3002", "motivation": "\u786e\u4fdd\u9a7e\u9a76\u5458\u5728\u6761\u4ef6\u81ea\u52a8\u9a7e\u9a76\u4e2d\u80fd\u591f\u53ca\u65f6\u6062\u590d\u63a7\u5236\uff0c\u4ee5\u5e94\u5bf9\u590d\u6742\u7684\u4ea4\u901a\u884c\u4e3a\u5e76\u63d0\u9ad8\u6df7\u5408\u4ea4\u901a\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u4e0e\u6548\u7387\u3002", "method": "\u901a\u8fc7\u9a7e\u9a76\u6a21\u62df\u5668\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u4e0d\u540c\u63a5\u7ba1\u7f13\u51b2\u65f6\u95f4\u5bf9\u5b89\u5168\u6307\u6807\uff08\u5982\u6700\u5c0f\u78b0\u649e\u65f6\u95f4\u3001\u6700\u5927\u51cf\u901f\u5ea6\u3001\u65b9\u5411\u76d8\u89d2\u5ea6\uff09\u53ca\u4e3b\u89c2\u8bc4\u4f30\uff08\u5982\u65f6\u95f4\u5145\u8db3\u611f\u3001\u98ce\u9669\u611f\u77e5\u548c\u8868\u73b0\u6ee1\u610f\u5ea6\uff09\u7684\u5f71\u54cd\u3002", "result": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u9002\u5e94\u6027\u65f6\u95f4\u9884\u7b97\u6846\u67b6\uff0c\u4ee5\u786e\u4fdd\u5728\u6761\u4ef6\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u9a7e\u9a76\u5458\u80fd\u591f\u5b89\u5168\u8212\u9002\u5730\u6062\u590d\u5bf9\u8f66\u8f86\u7684\u63a7\u5236\u3002\u901a\u8fc7\u9a7e\u9a76\u6a21\u62df\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u4e86\u4e0d\u540c\u63a5\u7ba1\u7f13\u51b2\u65f6\u95f4\u5bf9\u5b89\u5168\u6307\u6807\u548c\u4e3b\u89c2\u8bc4\u4f30\u7684\u5f71\u54cd\u3002\u7ed3\u679c\u663e\u793a\uff0c5-6\u79d2\u7684\u63a5\u7ba1\u7f13\u51b2\u65f6\u95f4\u6700\u4f18\uff0c\u5e76\u4e14\u9a7e\u9a76\u5458\u503e\u5411\u4e8e\u4fdd\u6301\u76f8\u5bf9\u7a33\u5b9a\u7684\u63a5\u7ba1\u7f13\u51b2\u65f6\u95f4\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u9884\u6d4b\u7684\u63a5\u7ba1\u65f6\u95f4\u548c\u9996\u9009\u7684\u63a5\u7ba1\u7f13\u51b2\uff0c\u4ee5\u52a8\u6001\u5206\u914d\u8fc7\u6e21\u65f6\u95f4\uff0c\u8fd9\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002", "conclusion": "\u9002\u5e94\u6027\u65f6\u95f4\u9884\u7b97\u6846\u67b6\u53ef\u4ee5\u6839\u636e\u7279\u5b9a\u60c5\u51b5\u4e0b\u9a7e\u9a76\u5458\u7684\u9700\u6c42\u52a8\u6001\u8c03\u6574\u63a5\u7ba1\u65f6\u95f4\uff0c\u4ece\u800c\u63d0\u9ad8\u63a7\u5236\u8fc7\u6e21\u7684\u53ef\u9760\u6027\u5e76\u51cf\u5c11\u78b0\u649e\u98ce\u9669\u3002"}}
{"id": "2511.05858", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05858", "abs": "https://arxiv.org/abs/2511.05858", "authors": ["Chuanyu Li", "Chaoyi Liu", "Daotan Wang", "Shuyu Zhang", "Lusong Li", "Zecui Zeng", "Fangchen Liu", "Jing Xu", "Rui Chen"], "title": "ViTaMIn-B: A Reliable and Efficient Visuo-Tactile Bimanual Manipulation Interface", "comment": null, "summary": "Handheld devices have opened up unprecedented opportunities to collect large-scale, high-quality demonstrations efficiently. However, existing systems often lack robust tactile sensing or reliable pose tracking to handle complex interaction scenarios, especially for bimanual and contact-rich tasks. In this work, we propose ViTaMIn-B, a more capable and efficient handheld data collection system for such tasks. We first design DuoTact, a novel compliant visuo-tactile sensor built with a flexible frame to withstand large contact forces during manipulation while capturing high-resolution contact geometry. To enhance the cross-sensor generalizability, we propose reconstructing the sensor's global deformation as a 3D point cloud and using it as the policy input. We further develop a robust, unified 6-DoF bimanual pose acquisition process using Meta Quest controllers, which eliminates the trajectory drift issue in common SLAM-based methods. Comprehensive user studies confirm the efficiency and high usability of ViTaMIn-B among novice and expert operators. Furthermore, experiments on four bimanual manipulation tasks demonstrate its superior task performance relative to existing systems.", "AI": {"tldr": "ViTaMIn-B\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u624b\u6301\u6570\u636e\u6536\u96c6\u7cfb\u7edf\uff0c\u5177\u6709\u6539\u8fdb\u7684\u89e6\u89c9\u4f20\u611f\u5668\u548c\u589e\u5f3a\u7684\u59ff\u6001\u83b7\u53d6\u8fc7\u7a0b\uff0c\u9002\u7528\u4e8e\u53cc\u624b\u590d\u6742\u64cd\u4f5c\u3002", "motivation": "\u76ee\u524d\u7684\u7cfb\u7edf\u5728\u590d\u6742\u4ea4\u4e92\u573a\u666f\u4e0b\u7f3a\u4e4f\u53ef\u9760\u7684\u89e6\u89c9\u611f\u77e5\u548c\u59ff\u6001\u8ddf\u8e2a\u80fd\u529b\uff0cViTaMIn-B\u65e8\u5728\u586b\u8865\u8fd9\u4e9b\u7a7a\u767d\uff0c\u4f7f\u5f97\u9ad8\u8d28\u91cf\u6f14\u793a\u7684\u6570\u636e\u6536\u96c6\u66f4\u52a0\u9ad8\u6548\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u540d\u4e3aDuoTact\u7684\u65b0\u578b\u5408\u89c4\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u5668\uff0c\u4ee5\u53ca\u57fa\u4e8eMeta Quest\u63a7\u5236\u5668\u7684\u7edf\u4e006\u81ea\u7531\u5ea6\u53cc\u624b\u59ff\u6001\u83b7\u53d6\u8fc7\u7a0b\u3002", "result": "ViTaMIn-B\u662f\u4e00\u79cd\u65b0\u7684\u624b\u6301\u6570\u636e\u6536\u96c6\u7cfb\u7edf\uff0c\u65e8\u5728\u6539\u5584\u590d\u6742\u4ea4\u4e92\u573a\u666f\u4e0b\u7684\u89e6\u89c9\u611f\u77e5\u548c\u59ff\u6001\u8ddf\u8e2a\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u53cc\u624b\u914d\u5408\u548c\u63a5\u89e6\u9891\u7e41\u7684\u4efb\u52a1\u4e2d\u3002", "conclusion": "\u7528\u6237\u7814\u7a76\u548c\u5b9e\u9a8c\u8868\u660e\uff0cViTaMIn-B\u5728\u65b0\u624b\u548c\u4e13\u5bb6\u7684\u64cd\u4f5c\u4e2d\u5747\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u826f\u597d\u7684\u53ef\u7528\u6027\uff0c\u4e14\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u7cfb\u7edf\u3002"}}
{"id": "2511.05769", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05769", "abs": "https://arxiv.org/abs/2511.05769", "authors": ["Kathleen W. Guan", "Sarthak Giri", "Mohammed Amara", "Bernard J. Jansen", "Enrico Liscio", "Milena Esherick", "Mohammed Al Owayyed", "Ausrine Ratkute", "Gayane Sedrakyan", "Mark de Reuver", "Joao Fernando Ferreira Goncalves", "Caroline A. Figueroa"], "title": "Lived Experience in Dialogue: Co-designing Personalization in Large Language Models to Support Youth Mental Well-being", "comment": null, "summary": "Youth increasingly turn to large language models (LLMs) for mental well-being support, yet current personalization in LLMs can overlook the heterogeneous lived experiences shaping their needs. We conducted a participatory study with youth, parents, and youth care workers (N=38), using co-created youth personas as scaffolds, to elicit community perspectives on how LLMs can facilitate more meaningful personalization to support youth mental well-being. Analysis identified three themes: person-centered contextualization responsive to momentary needs, explicit boundaries around scope and offline referral, and dialogic scaffolding for reflection and autonomy. We mapped these themes to persuasive design features for task suggestions, social facilitation, and system trustworthiness, and created corresponding dialogue extracts to guide LLM fine-tuning. Our findings demonstrate how lived experience can be operationalized to inform design features in LLMs, which can enhance the alignment of LLM-based interventions with the realities of youth and their communities, contributing to more effectively personalized digital well-being tools.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u793e\u533a\u89c6\u89d2\u4fc3\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e2a\u6027\u5316\u8bbe\u8ba1\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u652f\u6301\u9752\u5c11\u5e74\u7684\u5fc3\u7406\u5065\u5eb7\u3002", "motivation": "\u7531\u4e8e\u9752\u5c11\u5e74\u8d8a\u6765\u8d8a\u4f9d\u8d56\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u652f\u6301\u5fc3\u7406\u5065\u5eb7\uff0c\u5f53\u524d\u7684\u4e2a\u6027\u5316\u53ef\u80fd\u5ffd\u89c6\u4e86\u5f71\u54cd\u5176\u9700\u6c42\u7684\u591a\u6837\u5316\u751f\u6d3b\u7ecf\u9a8c\u3002", "method": "\u8fdb\u884c\u4e86\u4e00\u9879\u53c2\u4e0e\u6027\u7814\u7a76\uff0c\u53c2\u4e0e\u8005\u5305\u62ec\u9752\u5c11\u5e74\u3001\u5bb6\u957f\u548c\u9752\u5c11\u5e74\u62a4\u7406\u5de5\u4f5c\u8005\uff0c\u517138\u4eba\u3002", "result": "\u5206\u6790\u8bc6\u522b\u51fa\u4e09\u5927\u4e3b\u9898\uff0c\u5305\u62ec\u4ee5\u4eba\u4e3a\u672c\u7684\u60c5\u5883\u5316\u3001\u660e\u786e\u7684\u8303\u56f4\u548c\u79bb\u7ebf\u8f6c\u8bca\u754c\u9650\uff0c\u4ee5\u53ca\u4fc3\u8fdb\u53cd\u601d\u548c\u81ea\u4e3b\u6743\u7684\u5bf9\u8bdd\u6027\u652f\u67b6\uff0c\u8fdb\u800c\u4e0e\u529d\u670d\u8bbe\u8ba1\u7279\u5f81\u76f8\u7ed3\u5408\u3002", "conclusion": "\u901a\u8fc7\u8003\u8651\u9752\u5c11\u5e74\u7684\u751f\u6d3b\u7ecf\u9a8c\uff0c\u53ef\u4ee5\u6539\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e2a\u6027\u5316\u8bbe\u8ba1\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u652f\u6301\u5176\u5fc3\u7406\u5065\u5eb7\u3002"}}
{"id": "2511.05886", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05886", "abs": "https://arxiv.org/abs/2511.05886", "authors": ["Lei Shi", "Yongju Kim", "Xinzhi Zhong", "Wissam Kontar", "Qichao Liu", "Soyoung Ahn"], "title": "Fair and Safe: A Real-Time Hierarchical Control Framework for Intersections", "comment": null, "summary": "Ensuring fairness in the coordination of connected and automated vehicles at intersections is essential for equitable access, social acceptance, and long-term system efficiency, yet it remains underexplored in safety-critical, real-time traffic control. This paper proposes a fairness-aware hierarchical control framework that explicitly integrates inequity aversion into intersection management. At the top layer, a centralized allocation module assigns control authority (i.e., selects a single vehicle to execute its trajectory) by maximizing a utility that accounts for waiting time, urgency, control history, and velocity deviation. At the bottom layer, the authorized vehicle executes a precomputed trajectory using a Linear Quadratic Regulator (LQR) and applies a high-order Control Barrier Function (HOCBF)-based safety filter for real-time collision avoidance. Simulation results across varying traffic demands and demand distributions demonstrate that the proposed framework achieves near-perfect fairness, eliminates collisions, reduces average delay, and maintains real-time feasibility. These results highlight that fairness can be systematically incorporated without sacrificing safety or performance, enabling scalable and equitable coordination for future autonomous traffic systems.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u96c6\u6210\u4e0d\u5e73\u7b49\u538c\u6076\u7684\u516c\u5e73\u611f\u77e5\u5206\u5c42\u63a7\u5236\u6846\u67b6\uff0c\u4ee5\u7ba1\u7406\u4ea4\u53c9\u53e3\u7684\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u6846\u67b6\u5728\u5b9e\u73b0\u5b89\u5168\u7684\u540c\u65f6\u4e5f\u80fd\u8fbe\u5230\u51e0\u4e4e\u5b8c\u7f8e\u7684\u516c\u5e73\u6027\u3002", "motivation": "\u4e3a\u786e\u4fdd\u8fde\u63a5\u548c\u81ea\u52a8\u5316\u8f66\u8f86\u5728\u4ea4\u53c9\u53e3\u7684\u534f\u8c03\u516c\u5e73\u6027\uff0c\u63d0\u9ad8\u793e\u4f1a\u63a5\u53d7\u5ea6\u548c\u7cfb\u7edf\u957f\u671f\u6548\u7387\uff0c\u5c24\u5176\u662f\u5728\u5b89\u5168\u5173\u952e\u7684\u5b9e\u65f6\u4ea4\u901a\u63a7\u5236\u4e2d\uff0c\u516c\u5e73\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u672a\u88ab\u5145\u5206\u63a2\u8ba8\u7684\u9886\u57df\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u5c42\u63a7\u5236\u6846\u67b6\uff0c\u9876\u5c42\u4e3a\u96c6\u4e2d\u5206\u914d\u6a21\u5757\u8d1f\u8d23\u63a7\u5236\u6743\u7684\u5206\u914d\uff0c\u5e95\u5c42\u4f7f\u7528\u7ebf\u6027\u4e8c\u6b21\u8c03\u8282\u5668\u548c\u57fa\u4e8e\u9ad8\u9636\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u7684\u5b89\u5168\u8fc7\u6ee4\u5668\u6267\u884c\u8f68\u8ff9\u3002", "result": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u516c\u5e73\u611f\u77e5\u7684\u5206\u5c42\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u7ba1\u7406\u4ea4\u53c9\u53e3\u7684\u8fde\u63a5\u548c\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff0c\u65e8\u5728\u786e\u4fdd\u516c\u5e73\u6027\u5e76\u63d0\u9ad8\u7cfb\u7edf\u6548\u7387\u3002", "conclusion": "\u901a\u8fc7\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6846\u67b6\u53ef\u4ee5\u5728\u4e0d\u540c\u4ea4\u901a\u9700\u6c42\u4e0b\u5b9e\u73b0\u51e0\u4e4e\u5b8c\u7f8e\u7684\u516c\u5e73\u6027\uff0c\u6d88\u9664\u78b0\u649e\uff0c\u964d\u4f4e\u5e73\u5747\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u53ef\u884c\u6027\uff0c\u8bc1\u660e\u4e86\u516c\u5e73\u6027\u53ef\u4ee5\u5728\u4e0d\u727a\u7272\u5b89\u5168\u6027\u6216\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u7cfb\u7edf\u5316\u5730\u7eb3\u5165\u672a\u6765\u7684\u81ea\u52a8\u9a7e\u9a76\u4ea4\u901a\u7cfb\u7edf\u3002"}}
{"id": "2511.05817", "categories": ["cs.HC", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.05817", "abs": "https://arxiv.org/abs/2511.05817", "authors": ["Weiyan Shi", "Sunaya Upadhyay", "Geraldine Quek", "Kenny Tsu Wei Choo"], "title": "TalkSketch: Multimodal Generative AI for Real-time Sketch Ideation with Speech", "comment": "Accepted at AAAI 2026 Workshop on Creative AI for Live Interactive Performances (CLIP). To be published in Springer CCIS series", "summary": "Sketching is a widely used medium for generating and exploring early-stage design concepts. While generative AI (GenAI) chatbots are increasingly used for idea generation, designers often struggle to craft effective prompts and find it difficult to express evolving visual concepts through text alone. In the formative study (N=6), we examined how designers use GenAI during ideation, revealing that text-based prompting disrupts creative flow. To address these issues, we developed TalkSketch, an embedded multimodal AI sketching system that integrates freehand drawing with real-time speech input. TalkSketch aims to support a more fluid ideation process through capturing verbal descriptions during sketching and generating context-aware AI responses. Our work highlights the potential of GenAI tools to engage the design process itself rather than focusing on output.", "AI": {"tldr": "\u63d0\u51fa\u4e86TalkSketch\uff0c\u4e00\u4e2a\u7ed3\u5408\u624b\u7ed8\u4e0e\u8bed\u97f3\u8f93\u5165\u7684\u591a\u6a21\u6001AI\u8349\u56fe\u7cfb\u7edf\uff0c\u4ee5\u6539\u5584\u8bbe\u8ba1\u5e08\u5728\u521b\u610f\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u6d41\u7545\u6027\u548c\u8868\u8fbe\u80fd\u529b\u3002", "motivation": "\u8bbe\u8ba1\u5e08\u5728\u4f7f\u7528\u751f\u6210\u5f0fAI\u8fdb\u884c\u521b\u610f\u751f\u6210\u65f6\u5e38\u9762\u4e34\u6587\u672c\u63d0\u793a\u4e0d\u591f\u6709\u6548\u548c\u96be\u4ee5\u8868\u8fbe\u89c6\u89c9\u6982\u5ff5\u7684\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u7684\u5de5\u5177\u6765\u652f\u6301\u8bbe\u8ba1\u8fc7\u7a0b\u3002", "method": "\u901a\u8fc7\u5bf96\u540d\u8bbe\u8ba1\u5e08\u7684\u5f62\u6210\u6027\u7814\u7a76\uff0c\u63a2\u8ba8\u4ed6\u4eec\u5728\u6784\u601d\u65f6\u5982\u4f55\u4f7f\u7528\u751f\u6210\u5f0fAI\uff0c\u5e76\u5206\u6790\u4e86\u6587\u672c\u63d0\u793a\u5bf9\u521b\u9020\u6027\u6d41\u7a0b\u7684\u5f71\u54cd\u3002", "result": "\u672c\u7814\u7a76\u63d0\u51fa\u7684TalkSketch\u7cfb\u7edf\u901a\u8fc7\u5c06\u81ea\u7531\u624b\u7ed8\u4e0e\u5b9e\u65f6\u8bed\u97f3\u8f93\u5165\u76f8\u7ed3\u5408\uff0c\u6539\u5584\u4e86\u8bbe\u8ba1\u5e08\u5728\u6784\u601d\u9636\u6bb5\u4e0e\u751f\u6210\u5f0fAI\u804a\u5929\u673a\u5668\u4eba\u4ea4\u4e92\u65f6\u7684\u4f53\u9a8c\u3002", "conclusion": "TalkSketch\u7cfb\u7edf\u5c55\u793a\u4e86\u751f\u6210\u5f0fAI\u5de5\u5177\u5728\u8bbe\u8ba1\u8fc7\u7a0b\u4e2d\u53c2\u4e0e\u521b\u9020\u6027\u601d\u7ef4\u7684\u6f5c\u529b\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5173\u6ce8\u6700\u7ec8\u4ea7\u54c1\u3002"}}
{"id": "2511.05889", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05889", "abs": "https://arxiv.org/abs/2511.05889", "authors": ["Zeyuan Feng", "Haimingyue Zhang", "Somil Bansal"], "title": "From Words to Safety: Language-Conditioned Safety Filtering for Robot Navigation", "comment": null, "summary": "As robots become increasingly integrated into open-world, human-centered environments, their ability to interpret natural language instructions and adhere to safety constraints is critical for effective and trustworthy interaction. Existing approaches often focus on mapping language to reward functions instead of safety specifications or address only narrow constraint classes (e.g., obstacle avoidance), limiting their robustness and applicability. We propose a modular framework for language-conditioned safety in robot navigation. Our framework is composed of three core components: (1) a large language model (LLM)-based module that translates free-form instructions into structured safety specifications, (2) a perception module that grounds these specifications by maintaining object-level 3D representations of the environment, and (3) a model predictive control (MPC)-based safety filter that enforces both semantic and geometric constraints in real time. We evaluate the effectiveness of the proposed framework through both simulation studies and hardware experiments, demonstrating that it robustly interprets and enforces diverse language-specified constraints across a wide range of environments and scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u7684\u8bed\u8a00\u6761\u4ef6\u5b89\u5168\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u5b89\u5168\u89c4\u8303\u89e3\u91ca\u548c\u9002\u5e94\u6027\u65b9\u9762\u7684\u5c40\u9650\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u5728\u5f00\u653e\u4e16\u754c\u548c\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u73af\u5883\u4e2d\u65e5\u76ca\u96c6\u6210\uff0c\u89e3\u8bfb\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u548c\u9075\u5faa\u5b89\u5168\u7ea6\u675f\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6846\u67b6\u7531\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\u7ec4\u6210\uff1a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u89c4\u8303\u8f6c\u6362\u6a21\u5757\u3001\u73af\u5883\u5bf9\u8c61\u76843D\u8868\u793a\u611f\u77e5\u6a21\u5757\u4ee5\u53ca\u57fa\u4e8e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u5b89\u5168\u8fc7\u6ee4\u5668\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u7814\u7a76\u548c\u786c\u4ef6\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u5728\u4e0d\u540c\u73af\u5883\u548c\u573a\u666f\u4e0b\u80fd\u591f\u7a33\u5065\u5730\u89e3\u8bfb\u548c\u5b9e\u65bd\u591a\u6837\u7684\u8bed\u8a00\u89c4\u5b9a\u7ea6\u675f\u3002", "conclusion": "\u63d0\u51fa\u7684\u6a21\u5757\u5316\u6846\u67b6\u6709\u6548\u5730\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4e0e\u5b89\u5168\u7ea6\u675f\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u5728\u591a\u79cd\u73af\u5883\u4e0b\u7684\u5b89\u5168\u5bfc\u822a\u3002"}}
{"id": "2511.05875", "categories": ["cs.HC", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05875", "abs": "https://arxiv.org/abs/2511.05875", "authors": ["Mohd Ruhul Ameen", "Akif Islam"], "title": "Towards a Humanized Social-Media Ecosystem: AI-Augmented HCI Design Patterns for Safety, Agency & Well-Being", "comment": "6 pages, 5 tables, 7 figures, and 2 algorithm tables. Accepted at International Conference on Signal Processing, Information, Communication and Systems (SPICSCON 2025)", "summary": "Social platforms connect billions of people, yet their engagement-first algorithms often work on users rather than with them, amplifying stress, misinformation, and a loss of control. We propose Human-Layer AI (HL-AI)--user-owned, explainable intermediaries that sit in the browser between platform logic and the interface. HL-AI gives people practical, moment-to-moment control without requiring platform cooperation. We contribute a working Chrome/Edge prototype implementing five representative pattern frameworks--Context-Aware Post Rewriter, Post Integrity Meter, Granular Feed Curator, Micro-Withdrawal Agent, and Recovery Mode--alongside a unifying mathematical formulation balancing user utility, autonomy costs, and risk thresholds. Evaluation spans technical accuracy, usability, and behavioral outcomes. The result is a suite of humane controls that help users rewrite before harm, read with integrity cues, tune feeds with intention, pause compulsive loops, and seek shelter during harassment, all while preserving agency through explanations and override options. This prototype offers a practical path to retrofit today's feeds with safety, agency, and well-being, inviting rigorous cross-cultural user evaluation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u4eba\u7c7b\u5c42AI\uff08HL-AI\uff09\u7684\u6280\u672f\uff0c\u65e8\u5728\u589e\u5f3a\u7528\u6237\u5728\u793e\u4ea4\u5e73\u53f0\u4e2d\u7684\u63a7\u5236\u529b\uff0c\u901a\u8fc7\u4e94\u79cd\u4ee3\u8868\u6027\u6846\u67b6\u548c\u6570\u5b66\u516c\u5f0f\u5b9e\u73b0\u5b89\u5168\u4e0e agency\u3002", "motivation": "\u793e\u4ea4\u5e73\u53f0\u7684\u7b97\u6cd5\u5f80\u5f80\u53ea\u5173\u6ce8\u7528\u6237\u7684\u53c2\u4e0e\u5ea6\uff0c\u5e76\u5728\u67d0\u79cd\u7a0b\u5ea6\u4e0a\u64cd\u63a7\u7528\u6237\uff0c\u5bfc\u81f4\u7528\u6237\u9762\u4e34\u538b\u529b\u3001\u9519\u8bef\u4fe1\u606f\u548c\u5931\u53bb\u63a7\u5236\u611f\u3002", "method": "\u901a\u8fc7\u5f00\u53d1\u4e00\u4e2a\u539f\u578b\u5e76\u5b9e\u73b0\u4e94\u4e2a\u4e3b\u8981\u529f\u80fd\u6846\u67b6\uff0c\u7ed3\u5408\u7528\u6237\u6548\u7528\u3001\u81ea\u6cbb\u6210\u672c\u548c\u98ce\u9669\u9608\u503c\u7684\u6570\u5b66\u5e73\u8861\uff0c\u8bc4\u4f30\u5176\u6280\u672f\u51c6\u786e\u6027\u3001\u53ef\u7528\u6027\u548c\u884c\u4e3a\u7ed3\u679c\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u6237\u81ea\u6709\u7684\u3001\u53ef\u89e3\u91ca\u7684\u4e2d\u4ecb\u6280\u672f\uff0c\u65e8\u5728\u4e3a\u7528\u6237\u63d0\u4f9b\u5b9e\u65f6\u63a7\u5236\u3002\u8fd9\u4e00\u539f\u578b\u5728Chrome\u548cEdge\u6d4f\u89c8\u5668\u4e2d\u5b9e\u73b0\uff0c\u5e2e\u52a9\u7528\u6237\u5728\u9762\u5bf9\u6f5c\u5728\u5371\u5bb3\u65f6\u91c7\u53d6\u9884\u9632\u63aa\u65bd\u3002", "conclusion": "\u6240\u63d0\u4f9b\u7684\u539f\u578b\u4e3a\u4eca\u65e5\u7684\u793e\u4ea4\u5e73\u53f0\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u5b89\u5168\u548c\u63a7\u5236\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u6587\u5316\u591a\u6837\u6027\u4e2d\u8fdb\u884c\u7528\u6237\u8bc4\u4f30\u3002"}}
{"id": "2511.05936", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05936", "abs": "https://arxiv.org/abs/2511.05936", "authors": ["Soujanya Poria", "Navonil Majumder", "Chia-Yu Hung", "Amir Ali Bagherzadeh", "Chuan Li", "Kenneth Kwok", "Ziwei Wang", "Cheston Tan", "Jiajun Wu", "David Hsu"], "title": "10 Open Challenges Steering the Future of Vision-Language-Action Models", "comment": "AAAI 2026 (Senior Track)", "summary": "Due to their ability of follow natural language instructions, vision-language-action (VLA) models are increasingly prevalent in the embodied AI arena, following the widespread success of their precursors -- LLMs and VLMs. In this paper, we discuss 10 principal milestones in the ongoing development of VLA models -- multimodality, reasoning, data, evaluation, cross-robot action generalization, efficiency, whole-body coordination, safety, agents, and coordination with humans. Furthermore, we discuss the emerging trends of using spatial understanding, modeling world dynamics, post training, and data synthesis -- all aiming to reach these milestones. Through these discussions, we hope to bring attention to the research avenues that may accelerate the development of VLA models into wider acceptability.", "AI": {"tldr": "\u672c\u6587\u56f4\u7ed5VLA\u6a21\u578b\u7684\u53d1\u5c55\u8ba8\u8bba\u4e8610\u4e2a\u91cc\u7a0b\u7891\u548c\u65b0\u5174\u8d8b\u52bf\uff0c\u65e8\u5728\u63a8\u52a8\u5176\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u3002", "motivation": "\u4e3a\u4e86\u63d0\u5347VLA\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8ddf\u968f\u65b9\u9762\u7684\u80fd\u529b\uff0c\u63a8\u52a8\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5e7f\u6cdb\u63a5\u53d7\u3002", "method": "\u901a\u8fc7\u56de\u987e\u5f53\u524d\u53d1\u5c55\u4e2d\u768410\u4e2a\u4e3b\u8981\u91cc\u7a0b\u7891\uff0c\u5e76\u63a2\u8ba8\u65b0\u5174\u8d8b\u52bf\u3002", "result": "\u63d0\u51fa\u4e86\u56f4\u7ed5\u591a\u6a21\u6001\u6027\u3001\u63a8\u7406\u7b49\u5341\u4e2a\u91cc\u7a0b\u7891\u7684\u8ba8\u8bba\uff0c\u5e76\u6307\u51fa\u4e86\u7a7a\u95f4\u7406\u89e3\u7b49\u65b0\u5174\u8d8b\u52bf\u3002", "conclusion": "\u672c\u6587\u8ba8\u8bba\u4e86\u591a\u9879\u91cd\u8981\u91cc\u7a0b\u7891\u548c\u65b0\u5174\u8d8b\u52bf\uff0c\u4e3aVLA\u6a21\u578b\u7684\u7814\u53d1\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2511.05952", "categories": ["cs.HC", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.05952", "abs": "https://arxiv.org/abs/2511.05952", "authors": ["Takekazu Kitagishi", "Chun-Wei Ooi", "Yuichi Hiroi", "Jun Rekimoto"], "title": "Pinching Visuo-haptic Display: Investigating Cross-Modal Effects of Visual Textures on Electrostatic Cloth Tactile Sensations", "comment": "10 pages, 8 figures, 3 tables. Presented at ACM International Conference on Multimodal Interaction (ICMI) 2025", "summary": "This paper investigates how visual texture presentation influences tactile perception when interacting with electrostatic cloth displays. We propose a visuo-haptic system that allows users to pinch and rub virtual fabrics while feeling realistic frictional sensations modulated by electrostatic actuation. Through a user study, we examined the cross-modal effects between visual roughness and perceived tactile friction. The results demonstrate that visually rough textures amplify the perceived frictional force, even under identical electrostatic stimuli. These findings contribute to the understanding of multimodal texture perception and provide design insights for haptic feedback in virtual material interfaces.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u89c6\u89c9\u7eb9\u7406\u5bf9\u89e6\u89c9\u611f\u77e5\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u89c6\u89c9\u7c97\u7cd9\u5ea6\u589e\u5f3a\u4e86\u611f\u77e5\u5230\u7684\u6469\u64e6\u529b\u3002", "motivation": "\u7814\u7a76\u89c6\u89c9\u7eb9\u7406\u5982\u4f55\u5f71\u54cd\u7528\u7535\u9759\u6001\u5e03\u6599\u663e\u793a\u5668\u8fdb\u884c\u7684\u89e6\u89c9\u611f\u77e5\u3002", "method": "\u901a\u8fc7\u7528\u6237\u7814\u7a76\u8003\u5bdf\u89c6\u89c9\u7c97\u7cd9\u5ea6\u4e0e\u89e6\u89c9\u6469\u64e6\u4e4b\u95f4\u7684\u8de8\u6a21\u6001\u6548\u5e94\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89c6-\u89e6\u89c9\u7cfb\u7edf\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u634f\u548c\u64e6\u62ed\u865a\u62df\u7ec7\u7269\uff0c\u540c\u65f6\u611f\u53d7\u5230\u7531\u7535\u9759\u6001\u6fc0\u6d3b\u8c03\u5236\u7684\u771f\u5b9e\u6469\u64e6\u611f\u3002", "conclusion": "\u89c6\u89c9\u7c97\u7cd9\u7eb9\u7406\u589e\u5f3a\u4e86\u6469\u64e6\u611f\uff0c\u5373\u4f7f\u7535\u9759\u6001\u523a\u6fc0\u76f8\u540c\uff0c\u8fd9\u4e00\u53d1\u73b0\u4e3a\u591a\u6a21\u6001\u7eb9\u7406\u611f\u77e5\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u89e3\uff0c\u5e76\u4e3a\u865a\u62df\u6750\u6599\u754c\u9762\u7684\u89e6\u89c9\u53cd\u9988\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2511.05995", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05995", "abs": "https://arxiv.org/abs/2511.05995", "authors": ["Jianbo Yuan", "Jing Dai", "Yerui Fan", "Yaxiong Wu", "Yunpeng Liang", "Weixin Yan"], "title": "Robustness study of the bio-inspired musculoskeletal arm robot based on the data-driven iterative learning algorithm", "comment": "20 pages, 13 figures", "summary": "The human arm exhibits remarkable capabilities, including both explosive power and precision, which demonstrate dexterity, compliance, and robustness in unstructured environments. Developing robotic systems that emulate human-like operational characteristics through musculoskeletal structures has long been a research focus. In this study, we designed a novel lightweight tendon-driven musculoskeletal arm (LTDM-Arm), featuring a seven degree-of-freedom (DOF) skeletal joint system and a modularized artificial muscular system (MAMS) with 15 actuators. Additionally, we employed a Hilly-type muscle model and data-driven iterative learning control (DDILC) to learn and refine activation signals for repetitive tasks within a finite time frame. We validated the anti-interference capabilities of the musculoskeletal system through both simulations and experiments. The results show that the LTDM-Arm system can effectively achieve desired trajectory tracking tasks, even under load disturbances of 20 % in simulation and 15 % in experiments. This research lays the foundation for developing advanced robotic systems with human-like operational performance.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u578b\u8f7b\u91cf\u5316\u808c\u8171\u9a71\u52a8\u7684\u808c\u8089\u9aa8\u9abc\u81c2\uff08LTDM-Arm\uff09\uff0c\u80fd\u591f\u5728\u8d1f\u8f7d\u5e72\u6270\u4e0b\u6709\u6548\u5b8c\u6210\u9884\u5b9a\u8f68\u8ff9\u8ddf\u8e2a\u4efb\u52a1\u3002", "motivation": "\u6a21\u4eff\u4eba\u7c7b\u81c2\u7684\u7075\u6d3b\u6027\u3001\u9002\u5e94\u6027\u548c\u7a33\u5065\u6027\uff0c\u4ee5\u63a8\u52a8\u673a\u5668\u4eba\u6280\u672f\u7684\u8fdb\u6b65\u3002", "method": "\u91c7\u7528\u4e03\u81ea\u7531\u5ea6\u5173\u8282\u7cfb\u7edf\u548c\u6a21\u5757\u5316\u4eba\u5de5\u808c\u8089\u7cfb\u7edf\uff08MAMS\uff09\uff0c\u7ed3\u5408Hilly\u578b\u808c\u8089\u6a21\u578b\u548c\u6570\u636e\u9a71\u52a8\u7684\u8fed\u4ee3\u5b66\u4e60\u63a7\u5236\uff08DDILC\uff09\u4f18\u5316\u6fc0\u6d3b\u4fe1\u53f7\u3002", "result": "LTDM-Arm\u5728\u4eff\u771f\u4e2d\u627f\u53d720%\u8d1f\u8f7d\u5e72\u6270\uff0c\u5b9e\u9a8c\u4e2d\u627f\u53d715%\u8d1f\u8f7d\u5e72\u6270\uff0c\u4ecd\u80fd\u6709\u6548\u8fdb\u884c\u8f68\u8ff9\u8ddf\u8e2a\u3002", "conclusion": "LTDM-Arm\u7cfb\u7edf\u4e3a\u7c7b\u4eba\u64cd\u4f5c\u6027\u80fd\u7684\u9ad8\u7ea7\u673a\u5668\u4eba\u7cfb\u7edf\u5f00\u53d1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.06147", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.06147", "abs": "https://arxiv.org/abs/2511.06147", "authors": ["Muhammad Abdullah Sohail", "Amna Hassan", "Shaheer Hammad", "Salaar Masood", "Suleman Shahid"], "title": "Towards Misinformation Resilience in Pakistan: A Participatory Study with Low-Socioeconomic Status Adults", "comment": "1o pages", "summary": "Digital misinformation disproportionately affects low-socioeconomic status (SES) populations. While interventions for the Global South exist, they often report limited success, particularly among marginalized communities. Through a three-phase participatory study with 41 low-SES Pakistani adults, we conducted formative interviews to understand their information practices, followed by co-design sessions that translated these user-identified needs into concrete design requirements. Our findings reveal a sophisticated moral economy of sharing and a layered ecology of trust that prioritizes communal welfare. These insights inform the Scaffolded Support Model, a user-derived framework integrating on-demand assistance with gradual, inoculation-based skill acquisition. We instantiated this model in our prototype, \"Pehchaan,\" and conducted usability testing (N=15), which confirmed its strong acceptance and cultural resonance, validating our culturally grounded approach. Our work contributes a foundational empirical account of non-Western misinformation practices, a replicable participatory methodology for inclusive design, and actionable principles for building information resilience in resource-constrained contexts.", "AI": {"tldr": "\u6570\u5b57\u9519\u8bef\u4fe1\u606f\u5bf9\u4f4e\u793e\u4f1a\u7ecf\u6d4e\u5730\u4f4d\uff08SES\uff09\u4eba\u7fa4\u5f71\u54cd\u663e\u8457\uff0c\u7814\u7a76\u57fa\u4e8e\u5df4\u57fa\u65af\u5766\u4f4eSES\u6210\u5e74\u4eba\uff0c\u5f00\u53d1\u51fa\u652f\u6301\u6a21\u578b\u4ee5\u589e\u5f3a\u4fe1\u606f\u5f39\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u4e86\u89e3\u4f4eSES\u4eba\u7fa4\u7684\u4fe1\u606f\u4e60\u60ef\uff0c\u4ee5\u5e94\u5bf9\u6570\u5b57\u9519\u8bef\u4fe1\u606f\u5bf9\u5176\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u4e09\u9636\u6bb5\u7684\u53c2\u4e0e\u5f0f\u7814\u7a76\uff0c\u5305\u62ec\u5f62\u6210\u6027\u8bbf\u8c08\u548c\u5171\u540c\u8bbe\u8ba1\u4f1a\u8bae\uff0c\u660e\u786e\u7528\u6237\u9700\u6c42\u5e76\u8f6c\u5316\u4e3a\u8bbe\u8ba1\u8981\u6c42\u3002", "result": "\u5f00\u53d1\u4e86Pehchaan\u539f\u578b\uff0c\u5e76\u8fdb\u884c\u53ef\u7528\u6027\u6d4b\u8bd5\uff0c\u7ed3\u679c\u663e\u793a\u5176\u6587\u5316\u9002\u5e94\u6027\u5f3a\uff0c\u7528\u6237\u63a5\u53d7\u5ea6\u9ad8\uff0c\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u6587\u5316\u7684\u8bbe\u8ba1\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\u5728\u4f4eSES\u80cc\u666f\u4e0b\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u51fa\u53ef\u91cd\u590d\u7684\u8bbe\u8ba1\u65b9\u6cd5\u548c\u4fe1\u606f\u97e7\u6027\u539f\u5219\u3002"}}
{"id": "2511.06102", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06102", "abs": "https://arxiv.org/abs/2511.06102", "authors": ["Mohammed Abboodi"], "title": "Development and testing of novel soft sleeve actuators", "comment": "PhD thesis", "summary": "Aging populations and the rising prevalence of neurological and musculoskeletal disorders increase the demand for wearable mobility assistive devices that are effective, comfortable, and anatomically compatible. Many existing systems use rigid mechanisms and bulky interfaces that impede force transmission and reduce wearability. This study introduces a soft sleeve actuation architecture that conforms to the limb while transmitting forces and moments efficiently. We develop three soft sleeve actuators that produce linear, bending, and twisting motion, and an omnidirectional design that combines these motions in one device. Actuators are fabricated from thermoplastic elastomers using a customized fused filament fabrication process that produces airtight and compliant structures and resolves leakage observed with conventional methods. A dedicated experimental platform quantifies kinematic outputs such as displacement, angle, and twist, and kinetic outputs such as force and torque under low pneumatic pressures. A parametric study varies geometric features and material properties to determine their influence on performance. Results show reproducible multi axis motion with improved transfer of force to the limb and reduced need for complex attachment hardware. The work establishes a unified and manufacturable framework for soft sleeve actuation that enables compact and user centered assistive technologies with enhanced kinematic and kinetic performance.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8f6f\u8896\u9a71\u52a8\u67b6\u6784\uff0c\u901a\u8fc7\u4e09\u4e2a\u4e0d\u540c\u7684\u8f6f\u8896\u9a71\u52a8\u5668\u5b9e\u73b0\u591a\u8f74\u8fd0\u52a8\uff0c\u63d0\u5347\u4e86\u529b\u91cf\u4f20\u9012\uff0c\u5e76\u964d\u4f4e\u4e86\u590d\u6742\u9644\u4ef6\u7684\u9700\u6c42\u3002", "motivation": "\u5e94\u5bf9\u8001\u9f84\u5316\u4eba\u53e3\u548c\u795e\u7ecf\u53ca\u808c\u8089\u9aa8\u9abc\u75be\u75c5\u65e5\u76ca\u589e\u52a0\u7684\u9700\u6c42\uff0c\u5f00\u53d1\u6709\u6548\u3001\u8212\u9002\u4e14\u7b26\u5408\u89e3\u5256\u5b66\u7684\u53ef\u7a7f\u6234\u79fb\u52a8\u8f85\u52a9\u8bbe\u5907\u3002", "method": "\u5f00\u53d1\u4e09\u79cd\u8f6f\u8896\u9a71\u52a8\u5668\uff0c\u91c7\u7528\u70ed\u5851\u6027\u5f39\u6027\u4f53\u4f7f\u7528\u5b9a\u5236\u7684\u7194\u878d\u7ea4\u7ef4\u5236\u9020\u5de5\u827a\uff0c\u5b9e\u9a8c\u5e73\u53f0\u91cf\u5316\u8fd0\u52a8\u5b66\u548c\u52a8\u529b\u5b66\u8f93\u51fa\uff0c\u5e76\u8fdb\u884c\u53c2\u6570\u7814\u7a76\u3002", "result": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u8f6f\u8896\u9a71\u52a8\u67b6\u6784\uff0c\u80fd\u591f\u6709\u6548\u4f20\u9012\u529b\u91cf\u548c\u529b\u77e9\uff0c\u4e14\u9002\u5e94\u80a2\u4f53\u5f62\u72b6\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8f6f\u8896\u9a71\u52a8\u6280\u672f\u63d0\u4f9b\u4e86\u7edf\u4e00\u548c\u53ef\u5236\u9020\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5b9e\u73b0\u7d27\u51d1\u548c\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u8f85\u52a9\u6280\u672f\uff0c\u5177\u6709\u589e\u5f3a\u7684\u8fd0\u52a8\u5b66\u548c\u52a8\u529b\u5b66\u6027\u80fd\u3002"}}
{"id": "2511.06195", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06195", "abs": "https://arxiv.org/abs/2511.06195", "authors": ["Mira Winick", "Naisha Agarwal", "Chiheb Boussema", "Ingrid Lee", "Camilo Vargas", "Jeff Burke"], "title": "AI as intermediary in modern-day ritual: An immersive, interactive production of the roller disco musical Xanadu at UCLA", "comment": null, "summary": "Interfaces for contemporary large language, generative media, and perception AI models are often engineered for single user interaction. We investigate ritual as a design scaffold for developing collaborative, multi-user human-AI engagement. We consider the specific case of an immersive staging of the musical Xanadu performed at UCLA in Spring 2025. During a two-week run, over five hundred audience members contributed sketches and jazzercise moves that vision language models translated to virtual scenery elements and from choreographic prompts. This paper discusses four facets of interaction-as-ritual within the show: audience input as offerings that AI transforms into components of the ritual; performers as ritual guides, demonstrating how to interact with technology and sorting audience members into cohorts; AI systems as instruments \"played\" by the humans, in which sensing, generative components, and stagecraft create systems that can be mastered over time; and reciprocity of interaction, in which the show's AI machinery guides human behavior as well as being guided by humans, completing a human-AI feedback loop that visibly reshapes the virtual world. Ritual served as a frame for integrating linear narrative, character identity, music and interaction. The production explored how AI systems can support group creativity and play, addressing a critical gap in prevailing single user AI design paradigms.", "AI": {"tldr": "\u672c\u8bba\u6587\u63a2\u8ba8\u4e86\u4ee5\u4eea\u5f0f\u4e3a\u6846\u67b6\u7684\u591a\u7528\u6237\u4eba\u673a\u4e92\u52a8\uff0c\u5c55\u793a\u4e86AI\u5728\u652f\u6301\u7fa4\u4f53\u521b\u9020\u529b\u548c\u6e38\u620f\u4e2d\u7684\u4f5c\u7528\uff0c\u586b\u8865\u4e86\u5355\u7528\u6237AI\u8bbe\u8ba1\u7684\u7a7a\u767d\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570AI\u63a5\u53e3\u8bbe\u8ba1\u4ec5\u9762\u5411\u5355\u7528\u6237\uff0c\u7f3a\u4e4f\u5bf9\u591a\u7528\u6237\u534f\u4f5c\u548c\u521b\u9020\u529b\u7684\u8003\u8651\u3002", "method": "\u901a\u8fc7\u5c06\u4eea\u5f0f\u4f5c\u4e3a\u8bbe\u8ba1\u6846\u67b6\uff0c\u5206\u6790\u4e86\u57282025\u5e74UCLA\u4e3e\u529e\u7684\u97f3\u4e50\u5267\u300aXanadu\u300b\u4e2d\uff0c\u89c2\u4f17\u4e92\u52a8\u5982\u4f55\u5f71\u54cdAI\u751f\u6210\u7684\u865a\u62df\u573a\u666f\u548c\u821e\u8e48\u5143\u7d20\u3002", "result": "\u7814\u7a76\u5c55\u793a\u4e86\u89c2\u4f17\u7684\u8f93\u5165\u53ef\u4ee5\u4f5c\u4e3aAI\u8f6c\u5316\u6210\u4eea\u5f0f\u5143\u7d20\uff0c\u4f53\u73b0\u4e86\u4eba\u673a\u4e4b\u95f4\u7684\u4e92\u60e0\u4e92\u52a8\u53ca\u5176\u5982\u4f55\u91cd\u5851\u865a\u62df\u7a7a\u95f4\u3002", "conclusion": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u5927\u578b\u8bed\u8a00\u548c\u751f\u6210\u5a92\u4f53\u6a21\u578b\u4e2d\uff0c\u901a\u8fc7\u4eea\u5f0f\u6846\u67b6\u4fc3\u8fdb\u4eba\u673a\u591a\u7528\u6237\u4e92\u52a8\u7684\u53ef\u80fd\u6027\uff0c\u5f3a\u8c03\u4e86AI\u5982\u4f55\u5728\u7ec4\u5408\u6027\u521b\u4f5c\u4e2d\u53d1\u6325\u4f5c\u7528\u3002"}}
{"id": "2511.06141", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06141", "abs": "https://arxiv.org/abs/2511.06141", "authors": ["Marc Duclusaud", "Gr\u00e9goire Passault", "Vincent Padois", "Olivier Ly"], "title": "PlaCo: a QP-based robot planning and control framework", "comment": null, "summary": "This article introduces PlaCo, a software framework designed to simplify the formulation and solution of Quadratic Programming (QP)-based planning and control problems for robotic systems. PlaCo provides a high-level interface that abstracts away the low-level mathematical formulation of QP problems, allowing users to specify tasks and constraints in a modular and intuitive manner. The framework supports both Python bindings for rapid prototyping and a C++ implementation for real-time performance.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86PlaCo\uff0c\u4e00\u4e2a\u65e8\u5728\u7b80\u5316\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u4e8c\u6b21\u89c4\u5212(QP)\u95ee\u9898\u7684\u89c4\u5212\u548c\u63a7\u5236\u7684\u8f6f\u4ef6\u6846\u67b6\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u89c4\u5212\u548c\u63a7\u5236\u95ee\u9898\u4e0a\u7684\u5904\u7406\u6548\u7387\uff0c\u5bf9QP\u95ee\u9898\u7684\u89e3\u51b3\u8fdb\u884c\u4e86\u7b80\u5316\u548c\u62bd\u8c61\u3002", "method": "\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u9ad8\u5c42\u6b21\u7684\u63a5\u53e3\uff0cPlaCo\u5c4f\u853d\u4e86\u4e8c\u6b21\u89c4\u5212\u95ee\u9898\u7684\u4f4e\u5c42\u6570\u5b66\u8868\u8fbe\uff0c\u7528\u6237\u53ef\u4ee5\u66f4\u5bb9\u6613\u5730\u8fdb\u884c\u4efb\u52a1\u548c\u7ea6\u675f\u7684\u5b9a\u4e49\u3002", "result": "PlaCo\u7684\u8bbe\u8ba1\u4f7f\u5f97\u7528\u6237\u53ef\u4ee5\u5feb\u901f\u539f\u578b\u5f00\u53d1\uff0c\u5e76\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u5b9e\u73b0\u51fa\u8272\u7684\u6027\u80fd\u3002", "conclusion": "PlaCo\u4e3a\u7528\u6237\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u5c42\u63a5\u53e3\uff0c\u4f7f\u5f97\u4efb\u52a1\u548c\u7ea6\u675f\u7684\u6307\u5b9a\u66f4\u52a0\u6a21\u5757\u5316\u548c\u76f4\u89c2\uff0c\u540c\u65f6\u652f\u6301Python\u548cC++\u5b9e\u73b0\u4ee5\u6ee1\u8db3\u4e0d\u540c\u9700\u6c42\u3002"}}
{"id": "2511.06297", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06297", "abs": "https://arxiv.org/abs/2511.06297", "authors": ["Jihyeon Park", "Jiyoon Myung", "Seone Shin", "Jungki Son", "Joohyung Han"], "title": "Decomate: Leveraging Generative Models for Co-Creative SVG Animation", "comment": "Accepted at the 1st Workshop on Generative and Protective AI for Content Creation (NeurIPS 2025)", "summary": "Designers often encounter friction when animating static SVG graphics, especially when the visual structure does not match the desired level of motion detail. Existing tools typically depend on predefined groupings or require technical expertise, which limits designers' ability to experiment and iterate independently. We present Decomate, a system that enables intuitive SVG animation through natural language. Decomate leverages a multimodal large language model to restructure raw SVGs into semantically meaningful, animation-ready components. Designers can then specify motions for each component via text prompts, after which the system generates corresponding HTML/CSS/JS animations. By supporting iterative refinement through natural language interaction, Decomate integrates generative AI into creative workflows, allowing animation outcomes to be directly shaped by user intent.", "AI": {"tldr": "Decomate \u662f\u4e00\u6b3e\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684 SVG \u52a8\u753b\u5de5\u5177\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u91cd\u7ec4 SVG \u7ed3\u6784\uff0c\u7b80\u5316\u52a8\u753b\u521b\u5efa\u8fc7\u7a0b\uff0c\u652f\u6301\u7528\u6237\u8fdb\u884c\u610f\u56fe\u9a71\u52a8\u7684\u52a8\u753b\u8bbe\u8ba1\u3002", "motivation": "\u8bbe\u8ba1\u5e08\u5728\u5bf9\u9759\u6001 SVG \u56fe\u5f62\u8fdb\u884c\u52a8\u753b\u5904\u7406\u65f6\u5e38\u5e38\u9047\u5230\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u89c6\u89c9\u7ed3\u6784\u4e0e\u6240\u9700\u8fd0\u52a8\u7ec6\u8282\u4e0d\u5339\u914d\u65f6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u5de5\u5177\u6765\u63d0\u5347\u5176\u72ec\u7acb\u5b9e\u9a8c\u548c\u8fed\u4ee3\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u539f\u59cb SVG \u7ed3\u6784\u91cd\u7ec4\u4e3a\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u4e14\u9002\u5408\u52a8\u753b\u7684\u7ec4\u4ef6\uff0c\u5e76\u5141\u8bb8\u8bbe\u8ba1\u5e08\u901a\u8fc7\u6587\u672c\u63d0\u793a\u6307\u5b9a\u5404\u4e2a\u7ec4\u4ef6\u7684\u8fd0\u52a8\u3002", "result": "\u8bbe\u8ba1\u5e08\u80fd\u591f\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\uff0c\u4f7f\u52a8\u753b\u7ed3\u679c\u80fd\u76f4\u63a5\u53cd\u6620\u7528\u6237\u7684\u610f\u56fe\uff0c\u4fc3\u8fdb\u521b\u610f\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u751f\u6210\u6027 AI \u7684\u96c6\u6210\u3002", "conclusion": "Decomate \u4e3a\u8bbe\u8ba1\u5e08\u63d0\u4f9b\u4e86\u4e00\u79cd\u76f4\u89c2\u3001\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684 SVG \u52a8\u753b\u521b\u5efa\u65b9\u5f0f\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u91cd\u6784 SVG \u56fe\u5f62\uff0c\u4f7f\u5176\u80fd\u591f\u751f\u6210\u52a8\u753b\u3002"}}
{"id": "2511.06182", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06182", "abs": "https://arxiv.org/abs/2511.06182", "authors": ["Peican Lin", "Gan Sun", "Chenxi Liu", "Fazeng Li", "Weihong Ren", "Yang Cong"], "title": "OpenVLN: Open-world aerial Vision-Language Navigation", "comment": "Content: 8 pages 4 figures, conference under review", "summary": "Vision-language models (VLMs) have been widely-applied in ground-based vision-language navigation (VLN). However, the vast complexity of outdoor aerial environments compounds data acquisition challenges and imposes long-horizon trajectory planning requirements on Unmanned Aerial Vehicles (UAVs), introducing novel complexities for aerial VLN. To address these challenges, we propose a data-efficient Open-world aerial Vision-Language Navigation (i.e., OpenVLN) framework, which could execute language-guided flight with limited data constraints and enhance long-horizon trajectory planning capabilities in complex aerial environments. Specifically, we reconfigure a reinforcement learning framework to optimize the VLM for UAV navigation tasks, which can efficiently fine-tune VLM by using rule-based policies under limited training data. Concurrently, we introduce a long-horizon planner for trajectory synthesis that dynamically generates precise UAV actions via value-based rewards. To the end, we conduct sufficient navigation experiments on the TravelUAV benchmark with dataset scaling across diverse reward settings. Our method demonstrates consistent performance gains of up to 4.34% in Success Rate, 6.19% in Oracle Success Rate, and 4.07% in Success weighted by Path Length over baseline methods, validating its deployment efficacy for long-horizon UAV navigation in complex aerial environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u6846\u67b6OpenVLN\uff0c\u63d0\u9ad8\u65e0\u4eba\u673a\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8bed\u8a00\u5bfc\u822a\u6548\u7387\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u65e0\u4eba\u673a\u5728\u590d\u6742\u6237\u5916\u73af\u5883\u4e2d\u8fdb\u884c\u8bed\u8a00\u9a71\u52a8\u5bfc\u822a\u65f6\u6570\u636e\u83b7\u53d6\u548c\u957f\u8fdc\u8f68\u8ff9\u89c4\u5212\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u7684\u5f00\u653e\u4e16\u754c\u7a7a\u4e2d\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u6846\u67b6\uff08OpenVLN\uff09\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316VLM\u5e76\u5f15\u5165\u957f\u8fdc\u89c4\u5212\u5668\u3002", "result": "\u5728TravelUAV\u57fa\u51c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u6210\u529f\u7387\u3001oracle\u6210\u529f\u7387\u548c\u8def\u5f84\u957f\u5ea6\u52a0\u6743\u6210\u529f\u7387\u4e0a\u76f8\u8f83\u57fa\u7ebf\u65b9\u6cd5\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9a8c\u8bc1\u4e86\u5728\u590d\u6742\u7a7a\u4e2d\u73af\u5883\u4e2d\u8fdb\u884c\u957f\u8fdc\u65e0\u4eba\u673a\u5bfc\u822a\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.06447", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06447", "abs": "https://arxiv.org/abs/2511.06447", "authors": ["Mert Yazan", "Frederik Bungaran Ishak Situmeang", "Suzan Verberne"], "title": "Personality over Precision: Exploring the Influence of Human-Likeness on ChatGPT Use for Search", "comment": "Accepted at NIP-IR@SIGIR'25", "summary": "Conversational search interfaces, like ChatGPT, offer an interactive, personalized, and engaging user experience compared to traditional search. On the downside, they are prone to cause overtrust issues where users rely on their responses even when they are incorrect. What aspects of the conversational interaction paradigm drive people to adopt it, and how it creates personalized experiences that lead to overtrust, is not clear. To understand the factors influencing the adoption of conversational interfaces, we conducted a survey with 173 participants. We examined user perceptions regarding trust, human-likeness (anthropomorphism), and design preferences between ChatGPT and Google. To better understand the overtrust phenomenon, we asked users about their willingness to trade off factuality for constructs like ease of use or human-likeness. Our analysis identified two distinct user groups: those who use both ChatGPT and Google daily (DUB), and those who primarily rely on Google (DUG). The DUB group exhibited higher trust in ChatGPT, perceiving it as more human-like, and expressed greater willingness to trade factual accuracy for enhanced personalization and conversational flow. Conversely, the DUG group showed lower trust toward ChatGPT but still appreciated aspects like ad-free experiences and responsive interactions. Demographic analysis further revealed nuanced patterns, with middle-aged adults using ChatGPT less frequently yet trusting it more, suggesting potential vulnerability to misinformation. Our findings contribute to understanding user segmentation, emphasizing the critical roles of personalization and human-likeness in conversational IR systems, and reveal important implications regarding users' willingness to compromise factual accuracy for more engaging interactions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u7528\u6237\u5728\u5bf9\u8bdd\u641c\u7d22\u7cfb\u7edf\u4e2d\u503e\u5411\u4e8e\u76f8\u4fe1\u548c\u4f7f\u7528ChatGPT\u7684\u539f\u56e0\uff0c\u7a81\u51fa\u4e2a\u6027\u5316\u548c\u4eba\u6027\u5316\u5728\u7528\u6237\u4fe1\u4efb\u4e0e\u4f7f\u7528\u51b3\u7b56\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e5f\u53cd\u6620\u4e86\u7528\u6237\u5728\u4f7f\u7528\u4e2d\u53ef\u80fd\u5b58\u5728\u7684\u8fc7\u5ea6\u4fe1\u4efb\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u7406\u89e3\u5bf9\u8bdd\u754c\u9762\u7684\u91c7\u7528\u56e0\u7d20\uff0c\u4ee5\u53ca\u5b83\u5982\u4f55\u521b\u9020\u4e2a\u6027\u5316\u4f53\u9a8c\u5e76\u5bfc\u81f4\u8fc7\u5ea6\u4fe1\u4efb\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u5c55\u5f00\u4e86\u6b64\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u5bf9173\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u8c03\u67e5\uff0c\u5206\u6790\u7528\u6237\u5bf9\u4fe1\u4efb\u3001\u4eba\u6027\u5316\u548c\u8bbe\u8ba1\u504f\u597d\u7684\u770b\u6cd5\uff0c\u5e76\u63a2\u8ba8\u7528\u6237\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u4e0e\u6613\u7528\u6027\u3001\u4eba\u6027\u5316\u4e4b\u95f4\u7684\u53d6\u820d\u610f\u613f\u3002", "result": "DUB\u7ec4\u7528\u6237\u66f4\u4fe1\u4efbChatGPT\uff0c\u503e\u5411\u4e8e\u4ee5\u4e2a\u6027\u5316\u548c\u5bf9\u8bdd\u6d41\u7545\u6027\u4e3a\u91cd\uff0c\u800cDUG\u7ec4\u5219\u5bf9ChatGPT\u7684\u4fe1\u4efb\u5ea6\u8f83\u4f4e\uff0c\u4f46\u4ecd\u9752\u7750\u65e0\u5e7f\u544a\u548c\u54cd\u5e94\u4e92\u52a8\u3002", "conclusion": "\u7528\u6237\u5728\u4f7f\u7528\u5bf9\u8bdd\u641c\u7d22\u754c\u9762\u65f6\u66f4\u503e\u5411\u4e8e\u727a\u7272\u4e8b\u5b9e\u51c6\u786e\u6027\u4ee5\u83b7\u5f97\u66f4\u5177\u4e2a\u6027\u5316\u548c\u4e92\u52a8\u6027\u7684\u4f53\u9a8c\uff0c\u5c24\u5176\u662f\u5bf9ChatGPT\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u4fe1\u4efb\u548c\u4eba\u6027\u5316\u611f\u77e5\u3002\u8fd9\u4e00\u7814\u7a76\u5f3a\u8c03\u4e86\u4e2a\u6027\u5316\u548c\u4eba\u6027\u5316\u5728\u5bf9\u8bdd\u641c\u7d22\u7cfb\u7edf\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.06202", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06202", "abs": "https://arxiv.org/abs/2511.06202", "authors": ["Shahram Najam Syed", "Yatharth Ahuja", "Arthur Jakobsson", "Jeff Ichnowski"], "title": "ExpReS-VLA: Specializing Vision-Language-Action Models Through Experience Replay and Retrieval", "comment": "10 pages, 5 figures, submitted to ICRA 2026. Equal contribution by first two authors", "summary": "Vision-Language-Action models such as OpenVLA show impressive zero-shot generalization across robotic manipulation tasks but often fail to adapt efficiently to new deployment environments. In many real-world applications, consistent high performance on a limited set of tasks is more important than broad generalization. We propose ExpReS-VLA, a method for specializing pre-trained VLA models through experience replay and retrieval while preventing catastrophic forgetting. ExpReS-VLA stores compact feature representations from the frozen vision backbone instead of raw image-action pairs, reducing memory usage by approximately 97 percent. During deployment, relevant past experiences are retrieved using cosine similarity and used to guide adaptation, while prioritized experience replay emphasizes successful trajectories. We also introduce Thresholded Hybrid Contrastive Loss, which enables learning from both successful and failed attempts. On the LIBERO simulation benchmark, ExpReS-VLA improves success rates from 82.6 to 93.1 percent on spatial reasoning tasks and from 61 to 72.3 percent on long-horizon tasks. On physical robot experiments with five manipulation tasks, it reaches 98 percent success on both seen and unseen settings, compared to 84.7 and 32 percent for naive fine-tuning. Adaptation takes 31 seconds using 12 demonstrations on a single RTX 5090 GPU, making the approach practical for real robot deployment.", "AI": {"tldr": "ExpReS-VLA\u662f\u4e00\u79cd\u80fd\u6709\u6548\u9002\u5e94\u65b0\u73af\u5883\u7684Vision-Language-Action\u6a21\u578b\uff0c\u901a\u8fc7\u7ecf\u9a8c\u91cd\u653e\u4e0e\u63d0\u53d6\uff0c\u6210\u529f\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u8868\u73b0\u3002", "motivation": "\u9488\u5bf9\u73b0\u6709VLA\u6a21\u578b\u5728\u65b0\u73af\u5883\u9002\u5e94\u6027\u5dee\u7684\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u65b0\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u4e00\u81f4\u9ad8\u6027\u80fd\u3002", "method": "\u91c7\u7528\u7ecf\u9a8c\u91cd\u653e\u548c\u68c0\u7d22\u65b9\u6cd5\uff0c\u7ed3\u5408\u9608\u503c\u6df7\u5408\u5bf9\u6bd4\u635f\u5931\uff0c\u6709\u6548\u6307\u5bfc\u6a21\u578b\u7684\u9002\u5e94\u8fc7\u7a0b\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u9057\u5fd8\u73b0\u8c61\u3002", "result": "ExpReS-VLA\u5c55\u793a\u4e86\u5728\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u663e\u8457\u6210\u529f\u7387\u63d0\u5347\uff0c\u5c24\u5176\u662f\u5728\u7279\u5b9a\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u66f4\u5f3a\u3002", "conclusion": "ExpReS-VLA\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u663e\u8457\u7684\u6210\u529f\u7387\u63d0\u9ad8\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u673a\u5668\u4eba\u9002\u5e94\u6027\u548c\u8bb0\u5fc6\u6548\u7387\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.06468", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.06468", "abs": "https://arxiv.org/abs/2511.06468", "authors": ["Dan Zhang"], "title": "Towards Attention-Aware Large Language Models: Integrating Real-Time Eye-Tracking and EEG for Adaptive AI Responses", "comment": "9 pages", "summary": "This project proposes an attention-aware LLM that integrates EEG and eye tracking to monitor and measure user attention dynamically. To realize this, the project will integrate real-time EEG and eye-tracking data into an LLM-based interactive system and classify the user's attention state on the fly. The system can identify five attention states: High Attention, Stable Attention, Dropping Attention, Cognitive Overload, and Distraction. It responds accordingly to each state, with a particular focus on adapting to decreased attention, distraction, and cognitive overload to improve user engagement and reduce cognitive load.", "AI": {"tldr": "\u8be5\u9879\u76ee\u5f00\u53d1\u4e86\u4e00\u79cd\u5173\u6ce8\u5ea6\u611f\u77e5\u7684LLM\uff0c\u901a\u8fc7\u6574\u5408EEG\u548c\u773c\u52a8\u8ffd\u8e2a\uff0c\u5b9e\u73b0\u52a8\u6001\u76d1\u6d4b\u7528\u6237\u6ce8\u610f\u529b\u3002", "motivation": "\u8be5\u9879\u76ee\u7684\u52a8\u673a\u662f\u6539\u5584\u7528\u6237\u53c2\u4e0e\u5ea6\uff0c\u540c\u65f6\u51cf\u8f7b\u8ba4\u77e5\u8d1f\u62c5\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u54cd\u5e94\u7528\u6237\u7684\u4e0d\u540c\u6ce8\u610f\u529b\u72b6\u6001\u3002", "method": "\u9879\u76ee\u65b9\u6cd5\u662f\u5b9e\u65f6\u6574\u5408EEG\u548c\u773c\u52a8\u8ffd\u8e2a\u6570\u636e\uff0c\u5e76\u5728\u57fa\u4e8eLLM\u7684\u4e92\u52a8\u7cfb\u7edf\u4e2d\u5373\u65f6\u5206\u7c7b\u7528\u6237\u7684\u6ce8\u610f\u529b\u72b6\u6001\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u8bc6\u522b\u4e94\u79cd\u6ce8\u610f\u529b\u72b6\u6001\uff0c\u5e76\u9488\u5bf9\u6bcf\u79cd\u72b6\u6001\u505a\u51fa\u76f8\u5e94\u53cd\u5e94\uff0c\u5c24\u5176\u5173\u6ce8\u5982\u4f55\u5904\u7406\u6ce8\u610f\u529b\u4e0b\u964d\u3001\u5e72\u6270\u548c\u8ba4\u77e5\u8d85\u8f7d\u7684\u60c5\u51b5\u3002", "conclusion": "\u8be5\u9879\u76ee\u63d0\u51fa\u4e86\u4e00\u79cd\u5173\u6ce8\u5ea6\u611f\u77e5\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u901a\u8fc7\u6574\u5408\u8111\u7535\u56fe\uff08EEG\uff09\u548c\u773c\u52a8\u8ffd\u8e2a\u6280\u672f\u52a8\u6001\u76d1\u6d4b\u548c\u6d4b\u91cf\u7528\u6237\u6ce8\u610f\u529b\u3002"}}
{"id": "2511.06240", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06240", "abs": "https://arxiv.org/abs/2511.06240", "authors": ["Tzu-Jung Lin", "Jia-Fong Yeh", "Hung-Ting Su", "Chung-Yi Lin", "Yi-Ting Chen", "Winston H. Hsu"], "title": "Affordance-Guided Coarse-to-Fine Exploration for Base Placement in Open-Vocabulary Mobile Manipulation", "comment": "Accepted to AAAI 2026", "summary": "In open-vocabulary mobile manipulation (OVMM), task success often hinges on the selection of an appropriate base placement for the robot. Existing approaches typically navigate to proximity-based regions without considering affordances, resulting in frequent manipulation failures. We propose Affordance-Guided Coarse-to-Fine Exploration, a zero-shot framework for base placement that integrates semantic understanding from vision-language models (VLMs) with geometric feasibility through an iterative optimization process. Our method constructs cross-modal representations, namely Affordance RGB and Obstacle Map+, to align semantics with spatial context. This enables reasoning that extends beyond the egocentric limitations of RGB perception. To ensure interaction is guided by task-relevant affordances, we leverage coarse semantic priors from VLMs to guide the search toward task-relevant regions and refine placements with geometric constraints, thereby reducing the risk of convergence to local optima. Evaluated on five diverse open-vocabulary mobile manipulation tasks, our system achieves an 85% success rate, significantly outperforming classical geometric planners and VLM-based methods. This demonstrates the promise of affordance-aware and multimodal reasoning for generalizable, instruction-conditioned planning in OVMM.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u5f00\u653e\u8bcd\u6c47\u79fb\u52a8\u64cd\u63a7\u4efb\u52a1\u4e2d\u673a\u5668\u4eba\u7684\u57fa\u5ea7\u653e\u7f6e\u6548\u80fd\uff0c\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u7406\u89e3\u548c\u51e0\u4f55\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u8fc7\u7a0b\u5b9e\u73b0\u66f4\u4f18\u5e03\u5c40\u3002", "motivation": "\u5728\u5f00\u653e\u8bcd\u6c47\u79fb\u52a8\u64cd\u63a7\uff08OVMM\uff09\u4e2d\uff0c\u4efb\u52a1\u6210\u529f\u901a\u5e38\u4f9d\u8d56\u4e8e\u673a\u5668\u4eba\u57fa\u5ea7\u653e\u7f6e\u7684\u9009\u62e9\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u4e86\u6709\u6548\u6027\uff0c\u5bfc\u81f4\u9891\u7e41\u7684\u64cd\u4f5c\u5931\u8d25\u3002", "method": "\u6211\u4eec\u7684\u529e\u6cd5\u901a\u8fc7\u6784\u5efa\u8de8\u6a21\u6001\u8868\u793a\uff08\u6709\u6548\u6027RGB\u548c\u969c\u788d\u7269\u5730\u56fe+\uff09\uff0c\u5229\u7528VLM\u7684\u7c97\u8bed\u4e49\u5148\u9a8c\u6765\u5f15\u5bfc\u641c\u7d22\uff0c\u5e76\u5728\u51e0\u4f55\u7ea6\u675f\u4e0b\u7ec6\u5316\u5e03\u5c40\uff0c\u4ece\u800c\u51cf\u5c11\u6536\u655b\u5230\u5c40\u90e8\u6700\u4f18\u7684\u98ce\u9669\u3002", "result": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u2018\u6709\u6548\u6027\u5f15\u5bfc\u7684\u7c97\u5230\u7ec6\u63a2\u7d22\u2019\u7684\u96f6\u6837\u672c\u6846\u67b6\uff0c\u5728\u4e94\u4e2a\u591a\u6837\u7684\u5f00\u653e\u8bcd\u6c47\u79fb\u52a8\u64cd\u63a7\u4efb\u52a1\u4e2d\uff0c\u53d6\u5f9785%\u7684\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u7ecf\u5178\u7684\u51e0\u4f55\u89c4\u5212\u548c\u57fa\u4e8eVLM\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u8868\u660e\uff0c\u8003\u8651\u6709\u6548\u6027\u548c\u591a\u6a21\u6001\u63a8\u7406\u5728\u5f00\u653e\u8bcd\u6c47\u79fb\u52a8\u64cd\u63a7\u4e2d\u7684\u89c4\u5212\u5177\u6709\u6f5c\u5728\u7684\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2511.06532", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.06532", "abs": "https://arxiv.org/abs/2511.06532", "authors": ["Klaus Stephan", "Maximilian Eibl", "Albrecht Kurze"], "title": "HugSense: Exploring the Sensing Capabilities of Inflatables", "comment": "Mensch und Computer 2025 - Demo Session, September 1st 2025, Chemnitz, Germany", "summary": "What information can we get using inflatables as sensors? While using inflatables as actuators for various interactions has been widely adopted in the HCI community, using the sensing capabilities of inflatables is much less common. Almost all inflatable setups include air pressure sensors as part of the automation when pressurizing or deflating, but the full potential of those sensors is rarely explored. This paper shows how to turn a complete pillow into a force sensor using an inflatable and a simple pneumatics setup including an air pressure sensor. We will show that this setup yields accurate and interesting data that warrants further exploration and elaborate on the potential for practical applications.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u5145\u6c14\u7269\u4f53\u7684\u4f20\u611f\u80fd\u529b\uff0c\u63d0\u51fa\u5c06\u5145\u6c14\u6795\u5934\u8f6c\u53d8\u4e3a\u529b\u4f20\u611f\u5668\u7684\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u5176\u5e94\u7528\u7684\u6f5c\u529b\u3002", "motivation": "HCI\u9886\u57df\u5df2\u7ecf\u5e7f\u6cdb\u4f7f\u7528\u5145\u6c14\u7269\u4f53\u4f5c\u4e3a\u6267\u884c\u5668\uff0c\u4f46\u5176\u4f20\u611f\u80fd\u529b\u5374\u672a\u5f97\u5230\u5145\u5206\u5229\u7528\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u5145\u6c14\u88c5\u7f6e\u548c\u7a7a\u6c14\u538b\u529b\u4f20\u611f\u5668\uff0c\u6784\u5efa\u4e00\u4e2a\u5145\u6c14\u6795\u5934\u7684\u529b\u4f20\u611f\u5668\u3002", "result": "\u8be5\u5145\u6c14\u88c5\u7f6e\u80fd\u591f\u63d0\u4f9b\u51c6\u786e\u4e14\u6709\u8da3\u7684\u6570\u636e\uff0c\u663e\u793a\u51fa\u5176\u4f5c\u4e3a\u4f20\u611f\u5668\u7684\u6709\u6548\u6027\u548c\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u5145\u6c14\u7269\u4f53\u4f5c\u4e3a\u4f20\u611f\u5668\u7684\u5e94\u7528\u6f5c\u529b\u503c\u5f97\u6df1\u5165\u7814\u7a76\uff0c\u5e76\u53ef\u80fd\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53d1\u6325\u91cd\u8981\u4f5c\u7528\u3002"}}
{"id": "2511.06267", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06267", "abs": "https://arxiv.org/abs/2511.06267", "authors": ["Jiayi Chen", "Wei Zhao", "Liangwang Ruan", "Baoquan Chen", "He Wang"], "title": "Robust Differentiable Collision Detection for General Objects", "comment": null, "summary": "Collision detection is a core component of robotics applications such as simulation, control, and planning. Traditional algorithms like GJK+EPA compute witness points (i.e., the closest or deepest-penetration pairs between two objects) but are inherently non-differentiable, preventing gradient flow and limiting gradient-based optimization in contact-rich tasks such as grasping and manipulation. Recent work introduced efficient first-order randomized smoothing to make witness points differentiable; however, their direction-based formulation is restricted to convex objects and lacks robustness for complex geometries. In this work, we propose a robust and efficient differentiable collision detection framework that supports both convex and concave objects across diverse scales and configurations. Our method introduces distance-based first-order randomized smoothing, adaptive sampling, and equivalent gradient transport for robust and informative gradient computation. Experiments on complex meshes from DexGraspNet and Objaverse show significant improvements over existing baselines. Finally, we demonstrate a direct application of our method for dexterous grasp synthesis to refine the grasp quality. The code is available at https://github.com/JYChen18/DiffCollision.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u53ef\u5fae\u78b0\u649e\u68c0\u6d4b\u6846\u67b6\uff0c\u652f\u6301\u590d\u6742\u5f62\u72b6\u7269\u4f53\uff0c\u5b9e\u73b0\u4e86\u5728\u6293\u53d6\u548c\u64cd\u4f5c\u4e2d\u7684\u4f18\u5316\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u78b0\u649e\u68c0\u6d4b\u7b97\u6cd5\u5728\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u4e2d\u7684\u68af\u5ea6\u6d41\u52a8\u9650\u5236\uff0c\u7279\u522b\u662f\u5728\u6293\u53d6\u548c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u4f18\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u652f\u6301\u51f8\u5f62\u548c\u51f9\u5f62\u7269\u4f53\u7684\u53ef\u5fae\u78b0\u649e\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u8ddd\u79bb\u57fa\u7840\u7684\u4e00\u9636\u968f\u673a\u5e73\u6ed1\u3001\u9002\u5e94\u6027\u91c7\u6837\u548c\u7b49\u6548\u68af\u5ea6\u4f20\u8f93\u5b9e\u73b0\u9c81\u68d2\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u68af\u5ea6\u8ba1\u7b97\u3002", "result": "\u5728DexGraspNet\u548cObjaverse\u590d\u6742\u7f51\u683c\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u65b9\u6cd5\u5728\u590d\u6742\u5e94\u7528\u5982\u7075\u5de7\u6293\u53d6\u5408\u6210\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6548\u679c\uff0c\u4ee3\u7801\u53ef\u4f9b\u4f7f\u7528\u3002"}}
{"id": "2511.06688", "categories": ["cs.HC", "cs.CY", "cs.DL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.06688", "abs": "https://arxiv.org/abs/2511.06688", "authors": ["Chadani Acharya"], "title": "Accessibility Gaps in U.S. Government Dashboards for Blind and Low-Vision Residents", "comment": "Preprint. Accessibility audit of six U.S. public dashboard ecosystems; 1 figure, 2 tables", "summary": "Public dashboards are now a common way for US government agencies to share high stakes information with residents. We audited six live systems at federal, state, and city levels: CDC respiratory illness, HUD homelessness PIT and HIC, California HCD Annual Progress Report, New York City Mayor's Management Report, Houston Permitting, and Chicago public health and budget dashboards. Using a rubric based on screen reader needs and WCAG, we checked five items: (1) discoverability of key metrics by assistive tech, (2) keyboard access without mouse hover, (3) clear semantic labels for axes, series, and categories, (4) short plain language status and trend notes, and (5) machine readable tables or CSVs that mirror what sighted users see. Findings are mixed. Many charts fail basic discoverability or depend on hover, which blocks keyboard and screen reader use. Plain language summaries are common in CDC and Chicago, but rare in HUD and Houston. Machine readable data is strong for NYC, California, and HUD; it is weaker or unclear for Houston. Several sites promise service for the public or for customers yet do not name accessibility in their descriptions. Across systems we also observe urgency inversion: faster, operational dashboards tend to provide fewer accessible affordances than slower accountability dashboards. These patterns matter for equal participation and for ADA Title II compliance that references WCAG 2.1 AA. We propose three steps for any public dashboard: add a brief status and trend text at the same update cadence, publish a matching table or CSV of the visual metrics, and state an explicit accessibility commitment.", "AI": {"tldr": "\u5bf9\u7f8e\u56fd\u653f\u5e9c\u516c\u5171\u4eea\u8868\u76d8\u7684\u5ba1\u8ba1\u663e\u793a\u5176\u5728\u53ef\u8bbf\u95ee\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5bf9\u8f85\u52a9\u6280\u672f\u7684\u652f\u6301\u548c\u6570\u636e\u7684\u53ef\u8bfb\u6027\u3002", "motivation": "\u901a\u8fc7\u63d0\u9ad8\u516c\u5171\u4eea\u8868\u76d8\u7684\u53ef\u8bbf\u95ee\u6027\uff0c\u786e\u4fdd\u6240\u6709\u5c45\u6c11\uff0c\u7279\u522b\u662f\u6b8b\u969c\u4eba\u58eb\u80fd\u591f\u5e73\u7b49\u53c2\u4e0e\u5e76\u6ee1\u8db3ADA Title II\u5bf9\u53ef\u8bbf\u95ee\u6027\u7684\u8981\u6c42\u3002", "method": "\u57fa\u4e8e\u5c4f\u5e55\u9605\u8bfb\u5668\u9700\u6c42\u548cWCAG\u6307\u5357\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u5ba1\u6838\u4e86\u516d\u4e2a\u653f\u5e9c\u516c\u5171\u4eea\u8868\u76d8\uff0c\u68c0\u67e5\u4e3b\u8981\u6307\u6807\u7684\u53d1\u73b0\u6027\u3001\u952e\u76d8\u65e0\u9f20\u6807\u8bbf\u95ee\u3001\u6e05\u6670\u7684\u8bed\u4e49\u6807\u7b7e\u3001\u7b80\u6d01\u7684\u8bed\u8a00\u72b6\u6001\u548c\u8d8b\u52bf\u8bf4\u660e\uff0c\u4ee5\u53ca\u6570\u636e\u7684\u673a\u5668\u53ef\u8bfb\u6027\u3002", "result": "\u5ba1\u8ba1\u53d1\u73b0\u591a\u4e2a\u4eea\u8868\u76d8\u5728\u53ef\u53d1\u73b0\u6027\u548c\u53ef\u8bfb\u6027\u65b9\u9762\u5b58\u5728\u95ee\u9898\uff0c\u5c24\u5176\u662f\u4e0e\u8f85\u52a9\u6280\u672f\u7684\u517c\u5bb9\u6027\uff0c\u6b64\u5916\uff0c\u52a0\u901f\u8fd0\u8425\u4eea\u8868\u76d8\u7684\u53ef\u8bbf\u95ee\u6027\u901a\u5e38\u4f4e\u4e8e\u6162\u901f\u8d23\u4efb\u4eea\u8868\u76d8\u3002", "conclusion": "\u4e3a\u4e86\u589e\u5f3a\u516c\u4f17\u4eea\u8868\u76d8\u7684\u53ef\u8bbf\u95ee\u6027\uff0c\u5efa\u8bae\u589e\u52a0\u72b6\u6001\u548c\u8d8b\u52bf\u8bf4\u660e\u3001\u53d1\u5e03\u5339\u914d\u7684\u6570\u636e\u8868\u6216CSV\u3001\u4ee5\u53ca\u660e\u786e\u7684\u53ef\u8bbf\u95ee\u6027\u627f\u8bfa\u3002"}}
{"id": "2511.06311", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06311", "abs": "https://arxiv.org/abs/2511.06311", "authors": ["Seiichi Yamamoto", "Hiroki Ishizuka", "Takumi Kawasetsu", "Koh Hosoda", "Takayuki Kameoka", "Kango Yanagida", "Takato Horii", "Sei Ikeda", "Osamu Oshiro"], "title": "External Photoreflective Tactile Sensing Based on Surface Deformation Measurement", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "We present a tactile sensing method enabled by the mechanical compliance of soft robots; an externally attachable photoreflective module reads surface deformation of silicone skin to estimate contact force without embedding tactile transducers. Locating the sensor off the contact interface reduces damage risk, preserves softness, and simplifies fabrication and maintenance. We first characterize the optical sensing element and the compliant skin, thendetermine the design of a prototype tactile sensor. Compression experiments validate the approach, exhibiting a monotonic force output relationship consistent with theory, low hysteresis, high repeatability over repeated cycles, and small response indentation speeds. We further demonstrate integration on a soft robotic gripper, where the module reliably detects grasp events. Compared with liquid filled or wireembedded tactile skins, the proposed modular add on architecture enhances durability, reduces wiring complexity, and supports straightforward deployment across diverse robot geometries. Because the sensing principle reads skin strain patterns, it also suggests extensions to other somatosensory cues such as joint angle or actuator state estimation from surface deformation. Overall, leveraging surface compliance with an external optical module provides a practical and robust route to equip soft robots with force perception while preserving structural flexibility and manufacturability, paving the way for robotic applications and safe human robot collaboration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5916\u90e8\u5149\u6a21\u5757\u611f\u77e5\u8f6f\u4f53\u673a\u5668\u4eba\u63a5\u89e6\u529b\u7684\u65b0\u65b9\u6cd5\uff0c\u5177\u6709\u9ad8\u8010\u4e45\u6027\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u67d4\u6027\u4e14\u7b80\u5316\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u89e6\u89c9\u611f\u5e94\u65b9\u6cd5\uff0c\u907f\u514d\u4f20\u611f\u5668\u635f\u574f\u4e0e\u590d\u6742\u7684\u5236\u9020\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528\u53ef\u5916\u90e8\u9644\u52a0\u7684\u5149\u53cd\u5c04\u6a21\u5757\uff0c\u6d4b\u91cf\u7845\u80f6\u8868\u9762\u7684\u53d8\u5f62\u6765\u4f30\u8ba1\u63a5\u89e6\u529b\uff0c\u7701\u53bb\u5d4c\u5165\u5f0f\u89e6\u89c9\u4f20\u611f\u5668\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u529b\u8f93\u51fa\u5173\u7cfb\uff0c\u4f4e\u8fdf\u6ede\u6027\uff0c\u9ad8\u91cd\u590d\u6027\uff0c\u80fd\u591f\u53ef\u9760\u5730\u5728\u8f6f\u4f53\u6293\u53d6\u5668\u4e0a\u68c0\u6d4b\u6293\u53d6\u4e8b\u4ef6\u3002", "conclusion": "\u901a\u8fc7\u5916\u90e8\u5149\u5b66\u6a21\u5757\u8bfb\u53d6\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u8868\u9762\u53d8\u5f62\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u63a5\u89e6\u529b\u611f\u77e5\u65b9\u6cd5\uff0c\u517c\u987e\u4e86\u67d4\u6027\u548c\u53ef\u5236\u9020\u6027\u3002"}}
{"id": "2511.06782", "categories": ["cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.06782", "abs": "https://arxiv.org/abs/2511.06782", "authors": ["Qiang Wang", "Liying Yang"], "title": "HEDN: A Hard-Easy Dual Network with Task Difficulty Assessment for EEG Emotion Recognition", "comment": null, "summary": "Multi-source domain adaptation represents an effective approach to addressing individual differences in cross-subject EEG emotion recognition. However, existing methods treat all source domains equally, neglecting the varying transfer difficulties between different source domains and the target domain. This oversight can lead to suboptimal adaptation. To address this challenge, we propose a novel Hard-Easy Dual Network (HEDN), which dynamically identifies \"Hard Source\" and \"Easy Source\" through a Task Difficulty Assessment (TDA) mechanism and establishes two specialized knowledge adaptation branches. Specifically, the Hard Network is dedicated to handling \"Hard Source\" with higher transfer difficulty by aligning marginal distribution differences between source and target domains. Conversely, the Easy Network focuses on \"Easy Source\" with low transfer difficulty, utilizing a prototype classifier to model intra-class clustering structures while generating reliable pseudo-labels for the target domain through a prototype-guided label propagation algorithm. Extensive experiments on two benchmark datasets, SEED and SEED-IV, demonstrate that HEDN achieves state-of-the-art performance in cross-subject EEG emotion recognition, with average accuracies of 93.58\\% on SEED and 79.82\\% on SEED-IV, respectively. These results confirm the effectiveness and generalizability of HEDN in cross-subject EEG emotion recognition.", "AI": {"tldr": "\u901a\u8fc7HEDN\u7f51\u7edc\u8bc6\u522b\u6e90\u9886\u57df\u7684\u8f6c\u79fb\u96be\u5ea6\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684EEG\u60c5\u7eea\u8bc6\u522b\uff0c\u8fbe\u621093.58%\u548c79.82%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u5e94\u5bf9\u8de8\u5b66\u79d1EEG\u60c5\u7eea\u8bc6\u522b\u4e2d\u7684\u4e2a\u4f53\u5dee\u5f02\uff0c\u63d0\u5347\u6e90\u9886\u57df\u5230\u76ee\u6807\u9886\u57df\u7684\u9002\u5e94\u6027\u3002", "method": "\u5229\u7528\u4efb\u52a1\u96be\u5ea6\u8bc4\u4f30\u673a\u5236(TDA)\u52a8\u6001\u8bc6\u522b\u6e90\u9886\u57df\u7684\u96be\u5ea6\uff0c\u5e76\u5efa\u7acb\u4e13\u95e8\u7684\u77e5\u8bc6\u9002\u5e94\u5206\u652f\u3002", "result": "\u63d0\u51fa\u4e86HEDN\uff0c\u5206\u522b\u5bf9\u56f0\u96be\u6e90\u548c\u7b80\u5355\u6e90\u8fdb\u884c\u77e5\u8bc6\u8fc1\u79fb\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u60c5\u7eea\u8bc6\u522b\u7684\u51c6\u786e\u7387\u3002", "conclusion": "HEDN\u5728\u8de8\u5b66\u79d1EEG\u60c5\u7eea\u8bc6\u522b\u4e0a\u5c55\u73b0\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2511.06371", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06371", "abs": "https://arxiv.org/abs/2511.06371", "authors": ["Yingnan Zhao", "Xinmiao Wang", "Dewei Wang", "Xinzhe Liu", "Dan Lu", "Qilong Han", "Peng Liu", "Chenjia Bai"], "title": "Towards Adaptive Humanoid Control via Multi-Behavior Distillation and Reinforced Fine-Tuning", "comment": null, "summary": "Humanoid robots are promising to learn a diverse set of human-like locomotion behaviors, including standing up, walking, running, and jumping. However, existing methods predominantly require training independent policies for each skill, yielding behavior-specific controllers that exhibit limited generalization and brittle performance when deployed on irregular terrains and in diverse situations. To address this challenge, we propose Adaptive Humanoid Control (AHC) that adopts a two-stage framework to learn an adaptive humanoid locomotion controller across different skills and terrains. Specifically, we first train several primary locomotion policies and perform a multi-behavior distillation process to obtain a basic multi-behavior controller, facilitating adaptive behavior switching based on the environment. Then, we perform reinforced fine-tuning by collecting online feedback in performing adaptive behaviors on more diverse terrains, enhancing terrain adaptability for the controller. We conduct experiments in both simulation and real-world experiments in Unitree G1 robots. The results show that our method exhibits strong adaptability across various situations and terrains. Project website: https://ahc-humanoid.github.io.", "AI": {"tldr": "\u81ea\u9002\u5e94\u4eba\u5f62\u63a7\u5236\uff08AHC\uff09\u65b9\u6cd5\u901a\u8fc7\u591a\u884c\u4e3a\u84b8\u998f\u548c\u5728\u7ebf\u53cd\u9988\u5f3a\u5316\u8c03\u4f18\uff0c\u5b9e\u73b0\u4e86\u5728\u591a\u79cd\u60c5\u51b5\u548c\u5730\u5f62\u4e2d\u81ea\u9002\u5e94\u7684\u4eba\u5f62\u673a\u5668\u4eba locomotion \u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u4e3a\u6bcf\u4e2a\u6280\u80fd\u8bad\u7ec3\u72ec\u7acb\u653f\u7b56\uff0c\u5bfc\u81f4\u63a7\u5236\u5668\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u5bf9\u4e8e\u4e0d\u89c4\u5219\u5730\u5f62\u548c\u4e0d\u540c\u60c5\u51b5\u4e0b\u7684\u8868\u73b0\u8106\u5f31\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u9996\u5148\u8bad\u7ec3\u4e3b\u8981\u6b65\u6001\u653f\u7b56\u5e76\u8fdb\u884c\u591a\u884c\u4e3a\u84b8\u998f\uff0c\u7136\u540e\u901a\u8fc7\u5728\u7ebf\u53cd\u9988\u8fdb\u884c\u5f3a\u5316\u5fae\u8c03\uff0c\u63d0\u5347\u63a7\u5236\u5668\u5728\u590d\u6742\u5730\u5f62\u4e0a\u7684\u9002\u5e94\u6027\u3002", "result": "\u6211\u4eec\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u4eba\u5f62\u63a7\u5236\uff08AHC\uff09\u65b9\u6cd5\u5728\u4e0d\u540c\u6280\u80fd\u548c\u5730\u5f62\u4e2d\u5b66\u4e60\u81ea\u9002\u5e94\u7684\u6b65\u6001\u63a7\u5236\u5668\uff0c\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u9002\u5e94\u6027\u3002", "conclusion": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u5404\u79cd\u60c5\u51b5\u4e0b\u548c\u5730\u5f62\u4e2d\u90fd\u5177\u6709\u5f3a\u5927\u7684\u9002\u5e94\u80fd\u529b\u3002"}}
{"id": "2511.06804", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.06804", "abs": "https://arxiv.org/abs/2511.06804", "authors": ["Minwoo Jeong", "Jeeyun Chang", "Yoonjin Yoon"], "title": "AgentSUMO: An Agentic Framework for Interactive Simulation Scenario Generation in SUMO via Large Language Models", "comment": "Submitted to Transportation Research Part C (under review)", "summary": "The growing complexity of urban mobility systems has made traffic simulation indispensable for evidence-based transportation planning and policy evaluation. However, despite the analytical capabilities of platforms such as the Simulation of Urban MObility (SUMO), their application remains largely confined to domain experts. Developing realistic simulation scenarios requires expertise in network construction, origin-destination modeling, and parameter configuration for policy experimentation, creating substantial barriers for non-expert users such as policymakers, urban planners, and city officials. Moreover, the requests expressed by these users are often incomplete and abstract-typically articulated as high-level objectives, which are not well aligned with the imperative, sequential workflows employed in existing language-model-based simulation frameworks. To address these challenges, this study proposes AgentSUMO, an agentic framework for interactive simulation scenario generation via large language models. AgentSUMO departs from imperative, command-driven execution by introducing an adaptive reasoning layer that interprets user intents, assesses task complexity, infers missing parameters, and formulates executable simulation plans. The framework is structured around two complementary components, the Interactive Planning Protocol, which governs reasoning and user interaction, and the Model Context Protocol, which manages standardized communication and orchestration among simulation tools. Through this design, AgentSUMO converts abstract policy objectives into executable simulation scenarios. Experiments on urban networks in Seoul and Manhattan demonstrate that the agentic workflow achieves substantial improvements in traffic flow metrics while maintaining accessibility for non-expert users, successfully bridging the gap between policy goals and executable simulation workflows.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86AgentSUMO\u6846\u67b6\uff0c\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e3a\u975e\u4e13\u4e1a\u7528\u6237\u751f\u6210\u4ea4\u4e92\u5f0f\u4ea4\u901a\u4eff\u771f\u573a\u666f\uff0c\u4ee5\u89e3\u51b3\u57ce\u5e02\u4ea4\u901a\u7cfb\u7edf\u65e5\u76ca\u590d\u6742\u7684\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u57ce\u5e02\u4ea4\u901a\u7cfb\u7edf\u53d8\u5f97\u8d8a\u6765\u8d8a\u590d\u6742\uff0c\u4f20\u7edf\u7684\u4eff\u771f\u5de5\u5177\u5bf9\u975e\u4e13\u4e1a\u7528\u6237\u9020\u6210\u4e86\u5f88\u9ad8\u7684\u4f7f\u7528\u95e8\u69db\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u6765\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u81ea\u9002\u5e94\u63a8\u7406\u5c42\uff0cAgentSUMO\u80fd\u591f\u89e3\u6790\u7528\u6237\u610f\u56fe\u3001\u8bc4\u4f30\u4efb\u52a1\u590d\u6742\u6027\u3001\u63a8\u65ad\u7f3a\u5931\u53c2\u6570\u5e76\u5236\u5b9a\u53ef\u6267\u884c\u7684\u4eff\u771f\u8ba1\u5212\u3002", "result": "\u5728\u9996\u5c14\u548c\u66fc\u54c8\u987f\u7684\u57ce\u5e02\u7f51\u7edc\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAgentSUMO\u6846\u67b6\u663e\u8457\u6539\u5584\u4e86\u4ea4\u901a\u6d41\u91cf\u6307\u6807\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5bf9\u975e\u4e13\u4e1a\u7528\u6237\u7684\u53ef\u8bbf\u95ee\u6027\u3002", "conclusion": "AgentSUMO\u6210\u529f\u5c06\u62bd\u8c61\u7684\u653f\u7b56\u76ee\u6807\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684\u4eff\u771f\u573a\u666f\uff0c\u63d0\u5347\u4e86\u4ea4\u901a\u6d41\u91cf\u6307\u6807\uff0c\u5e76\u4e14\u5bf9\u975e\u4e13\u4e1a\u7528\u6237\u4fdd\u6301\u4e86\u53ef\u64cd\u4f5c\u6027\u3002"}}
{"id": "2511.06378", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.06378", "abs": "https://arxiv.org/abs/2511.06378", "authors": ["Prajval Kumar Murali", "Mohsen Kaboli"], "title": "ArtReg: Visuo-Tactile based Pose Tracking and Manipulation of Unseen Articulated Objects", "comment": "Under review", "summary": "Robots operating in real-world environments frequently encounter unknown objects with complex structures and articulated components, such as doors, drawers, cabinets, and tools. The ability to perceive, track, and manipulate these objects without prior knowledge of their geometry or kinematic properties remains a fundamental challenge in robotics. In this work, we present a novel method for visuo-tactile-based tracking of unseen objects (single, multiple, or articulated) during robotic interaction without assuming any prior knowledge regarding object shape or dynamics. Our novel pose tracking approach termed ArtReg (stands for Articulated Registration) integrates visuo-tactile point clouds in an unscented Kalman Filter formulation in the SE(3) Lie Group for point cloud registration. ArtReg is used to detect possible articulated joints in objects using purposeful manipulation maneuvers such as pushing or hold-pulling with a two-robot team. Furthermore, we leverage ArtReg to develop a closed-loop controller for goal-driven manipulation of articulated objects to move the object into the desired pose configuration. We have extensively evaluated our approach on various types of unknown objects through real robot experiments. We also demonstrate the robustness of our method by evaluating objects with varying center of mass, low-light conditions, and with challenging visual backgrounds. Furthermore, we benchmarked our approach on a standard dataset of articulated objects and demonstrated improved performance in terms of pose accuracy compared to state-of-the-art methods. Our experiments indicate that robust and accurate pose tracking leveraging visuo-tactile information enables robots to perceive and interact with unseen complex articulated objects (with revolute or prismatic joints).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u89c6\u89c9\u548c\u89e6\u89c9\u7684\u8ddf\u8e2a\u65b9\u6cd5ArtReg\uff0c\u80fd\u5728\u4e0d\u77e5\u9053\u7269\u4f53\u51e0\u4f55\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u5bf9\u590d\u6742\u5173\u8282\u7269\u4f53\u7684\u611f\u77e5\u548c\u64cd\u63a7\u3002", "motivation": "\u9762\u5bf9\u672a\u77e5\u590d\u6742\u7ed3\u6784\u7684\u7269\u4f53\uff0c\u673a\u5668\u4eba\u9700\u8981\u5728\u6ca1\u6709\u5148\u524d\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u611f\u77e5\u3001\u8ddf\u8e2a\u548c\u64cd\u63a7\uff0c\u8fd9\u662f\u673a\u5668\u4eba\u6280\u672f\u7684\u4e00\u4e2a\u57fa\u672c\u6311\u6218\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u89c6\u89c9-\u89e6\u89c9\u7684\u8ddf\u8e2a\u65b9\u6cd5ArtReg\uff0c\u901a\u8fc7\u65e0\u524d\u77e5\u8bc6\u7684\u65b9\u5f0f\u6574\u5408\u70b9\u4e91\u6570\u636e\uff0c\u5b9e\u73b0\u7269\u4f53\u7684\u52a8\u6001\u8ddf\u8e2a\u548c\u64cd\u63a7\u3002", "result": "\u901a\u8fc7\u5b9e\u9645\u673a\u5668\u4eba\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u7269\u4f53\u548c\u5177\u6709\u6311\u6218\u6027\u73af\u5883\u4e0b\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5b9e\u73b0\u4e86\u76ee\u6807\u9a71\u52a8\u7684\u64cd\u63a7\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5bf9\u672a\u77e5\u590d\u6742\u5173\u8282\u7269\u4f53\u7684\u8bc6\u522b\u548c\u64cd\u63a7\u80fd\u529b\uff0c\u5c55\u73b0\u4e86\u5728\u5404\u79cd\u6311\u6218\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2511.06914", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.06914", "abs": "https://arxiv.org/abs/2511.06914", "authors": ["Kawshik Kumar Paul", "Mahdi Hasnat Siyam", "Khandokar Md. Rahat Hossain"], "title": "A Low-Cost Embedded System for Automated Patient Queue and Health Data Management in Private Medical Chambers", "comment": null, "summary": "This paper presents the design and implementation of a low-cost microcontroller-based system for managing patient queues and preliminary health data collection in private medical chambers. Patient registration, queue management, and the collection of fundamental health metrics such as heart rate and body temperature are automated by the system. The proposed setup integrates an ATmega32 microcontroller, an LM35 temperature sensor, an XD-58C pulse sensor, 4x4 matrix keypads, and 16x2 LCD displays. The system separates patient-side input from doctor-side control, allowing doctors to call patients sequentially with a single button. Experimental evaluation conducted under limited hardware conditions demonstrates that the system reduces manual labor and contact-based data collection, making it feasible for small private practices in developing regions.", "AI": {"tldr": "\u672c\u8bba\u6587\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5fae\u63a7\u5236\u5668\u7684\u4f4e\u6210\u672c\u7cfb\u7edf\uff0c\u7528\u4e8e\u7ba1\u7406\u60a3\u8005\u6392\u961f\u548c\u6536\u96c6\u521d\u6b65\u5065\u5eb7\u6570\u636e\u3002", "motivation": "\u4e3a\u79c1\u7acb\u533b\u7597\u673a\u6784\u63d0\u4f9b\u4fbf\u6377\u7684\u60a3\u8005\u767b\u8bb0\u548c\u5065\u5eb7\u6570\u636e\u6536\u96c6\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u96c6\u6210ATmega32\u5fae\u63a7\u5236\u5668\u3001LM35\u6e29\u5ea6\u4f20\u611f\u5668\u3001XD-58C\u8109\u640f\u4f20\u611f\u5668\u30014x4\u77e9\u9635\u952e\u76d8\u548c16x2 LCD\u663e\u793a\u5668\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u6709\u9650\u786c\u4ef6\u6761\u4ef6\u4e0b\u6709\u6548\u8fd0\u884c\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u6709\u6548\u51cf\u5c11\u4e86\u624b\u52a8\u52b3\u52a8\u548c\u57fa\u4e8e\u63a5\u89e6\u7684\u6570\u636e\u6536\u96c6\uff0c\u9002\u5408\u53d1\u5c55\u5730\u533a\u7684\u5c0f\u578b\u79c1\u4eba\u8bca\u6240\u3002"}}
{"id": "2511.06385", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.06385", "abs": "https://arxiv.org/abs/2511.06385", "authors": ["Ralf R\u00f6mer", "Julian Balletshofer", "Jakob Thumm", "Marco Pavone", "Angela P. Schoellig", "Matthias Althoff"], "title": "From Demonstrations to Safe Deployment: Path-Consistent Safety Filtering for Diffusion Policies", "comment": "Project page: https://tum-lsy.github.io/pacs/. 8 pages, 4 figures", "summary": "Diffusion policies (DPs) achieve state-of-the-art performance on complex manipulation tasks by learning from large-scale demonstration datasets, often spanning multiple embodiments and environments. However, they cannot guarantee safe behavior, so external safety mechanisms are needed. These, however, alter actions in ways unseen during training, causing unpredictable behavior and performance degradation. To address these problems, we propose path-consistent safety filtering (PACS) for DPs. Our approach performs path-consistent braking on a trajectory computed from the sequence of generated actions. In this way, we keep execution consistent with the policy's training distribution, maintaining the learned, task-completing behavior. To enable a real-time deployment and handle uncertainties, we verify safety using set-based reachability analysis. Our experimental evaluation in simulation and on three challenging real-world human-robot interaction tasks shows that PACS (a) provides formal safety guarantees in dynamic environments, (b) preserves task success rates, and (c) outperforms reactive safety approaches, such as control barrier functions, by up to 68% in terms of task success. Videos are available at our project website: https://tum-lsy.github.io/pacs/.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u8def\u5f84\u4e00\u81f4\u6027\u5b89\u5168\u8fc7\u6ee4\uff08PACS\uff09\u65b9\u6cd5\uff0c\u65e8\u5728\u4e3a\u6269\u6563\u653f\u7b56\u63d0\u4f9b\u5b89\u5168\u4fdd\u969c\uff0c\u4fdd\u6301\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u53cd\u5e94\u5b89\u5168\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u6269\u6563\u653f\u7b56(DPs)\u5728\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u5b89\u5168\u6027\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u5916\u90e8\u5b89\u5168\u673a\u5236\u5e26\u6765\u7684\u4e0d\u53ef\u9884\u6d4b\u884c\u4e3a\u548c\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u8def\u5f84\u4e00\u81f4\u6027\u5b89\u5168\u8fc7\u6ee4\uff08PACS\uff09\u65b9\u6cd5", "result": "PACS\u63d0\u4f9b\u4e86\u52a8\u6001\u73af\u5883\u4e2d\u7684\u6b63\u5f0f\u5b89\u5168\u4fdd\u969c\uff0c\u4fdd\u6301\u4e86\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5e76\u5728\u4efb\u52a1\u6210\u529f\u7387\u4e0a\u6bd4\u53cd\u5e94\u5b89\u5168\u65b9\u6cd5\uff08\u5982\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff09\u63d0\u9ad8\u4e8668%\u3002", "conclusion": "PACS\u6709\u6548\u5730\u89e3\u51b3\u4e86\u6269\u6563\u653f\u7b56\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u5b89\u5168\u6027\u548c\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u90e8\u7f72\u3002"}}
{"id": "2511.06954", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.06954", "abs": "https://arxiv.org/abs/2511.06954", "authors": ["Yuchong Zhang", "Yong Ma", "Di Fu", "Stephanie Zubicueta Portales", "Morten Fjeld", "Danica Kragic"], "title": "Personalizing Emotion-aware Conversational Agents? Exploring User Traits-driven Conversational Strategies for Enhanced Interaction", "comment": null, "summary": "Conversational agents (CAs) are increasingly embedded in daily life, yet their ability to navigate user emotions efficiently is still evolving. This study investigates how users with varying traits -- gender, personality, and cultural background -- adapt their interaction strategies with emotion-aware CAs in specific emotional scenarios. Using an emotion-aware CA prototype expressing five distinct emotions (neutral, happy, sad, angry, and fear) through male and female voices, we examine how interaction dynamics shift across different voices and emotional contexts through empirical studies. Our findings reveal distinct variations in user engagement and conversational strategies based on individual traits, emphasizing the value of personalized, emotion-sensitive interactions. By analyzing both qualitative and quantitative data, we demonstrate that tailoring CAs to user characteristics can enhance user satisfaction and interaction quality. This work underscores the critical need for ongoing research to design CAs that not only recognize but also adaptively respond to emotional needs, ultimately supporting a diverse user groups more effectively.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u7528\u6237\u7684\u4e2a\u6027\u7279\u5f81\u5982\u4f55\u5f71\u54cd\u5176\u4e0e\u60c5\u611f\u611f\u77e5\u5bf9\u8bdd\u4ee3\u7406\u7684\u4e92\u52a8\uff0c\u7ed3\u679c\u663e\u793a\u4e2a\u6027\u5316\u8bbe\u8ba1\u80fd\u63d0\u5347\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "motivation": "\u63a2\u8ba8\u7528\u6237\u5982\u4f55\u4ee5\u4e0d\u540c\u7684\u7279\u5f81\uff08\u5982\u6027\u522b\u3001\u4e2a\u6027\u548c\u6587\u5316\u80cc\u666f\uff09\u8c03\u6574\u4e0e\u60c5\u611f\u611f\u77e5\u7684\u5bf9\u8bdd\u4ee3\u7406\uff08CA\uff09\u4e4b\u95f4\u7684\u4e92\u52a8\u7b56\u7565\uff0c\u4ee5\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\uff0c\u4f7f\u7528\u8868\u8fbe\u4e94\u79cd\u4e0d\u540c\u60c5\u611f\u7684\u60c5\u611f\u611f\u77e5\u5bf9\u8bdd\u4ee3\u7406\u539f\u578b\uff0c\u5206\u6790\u7528\u6237\u5728\u4e0d\u540c\u8bed\u97f3\u548c\u60c5\u611f\u4e0a\u4e0b\u6587\u4e2d\u7684\u4e92\u52a8\u52a8\u6001\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u7528\u6237\u53c2\u4e0e\u5ea6\u548c\u5bf9\u8bdd\u7b56\u7565\u56e0\u4e2a\u4f53\u7279\u5f81\u800c\u5f02\uff0c\u5f3a\u8c03\u4e2a\u6027\u5316\u3001\u60c5\u611f\u654f\u611f\u7684\u4e92\u52a8\u4ef7\u503c\u3002", "conclusion": "\u6301\u7eed\u7684\u7814\u7a76\u662f\u5fc5\u8981\u7684\uff0c\u4ee5\u8bbe\u8ba1\u80fd\u591f\u8bc6\u522b\u5e76\u9002\u5e94\u7528\u6237\u60c5\u611f\u9700\u6c42\u7684\u5bf9\u8bdd\u4ee3\u7406\uff0c\u4ece\u800c\u66f4\u6709\u6548\u652f\u6301\u591a\u6837\u5316\u7684\u7528\u6237\u7fa4\u4f53\u3002"}}
{"id": "2511.06397", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06397", "abs": "https://arxiv.org/abs/2511.06397", "authors": ["Cong Wen", "Yunfei Li", "Kexin Liu", "Yixin Qiu", "Xuanhong Liao", "Tianyu Wang", "Dingchuan Liu", "Tao Zhang", "Ximin Lyu"], "title": "Whole-Body Control With Terrain Estimation of A 6-DoF Wheeled Bipedal Robot", "comment": "8 pages, 8 figures", "summary": "Wheeled bipedal robots have garnered increasing attention in exploration and inspection. However, most research simplifies calculations by ignoring leg dynamics, thereby restricting the robot's full motion potential. Additionally, robots face challenges when traversing uneven terrain. To address the aforementioned issue, we develop a complete dynamics model and design a whole-body control framework with terrain estimation for a novel 6 degrees of freedom wheeled bipedal robot. This model incorporates the closed-loop dynamics of the robot and a ground contact model based on the estimated ground normal vector. We use a LiDAR inertial odometry framework and improved Principal Component Analysis for terrain estimation. Task controllers, including PD control law and LQR, are employed for pose control and centroidal dynamics-based balance control, respectively. Furthermore, a hierarchical optimization approach is used to solve the whole-body control problem. We validate the performance of the terrain estimation algorithm and demonstrate the algorithm's robustness and ability to traverse uneven terrain through both simulation and real-world experiments.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u516d\u81ea\u7531\u5ea6\u8f6e\u5f0f\u53cc\u8db3\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u5b8c\u6574\u7684\u52a8\u6001\u6a21\u578b\u548c\u5168\u8eab\u63a7\u5236\u6846\u67b6\u7ed3\u5408\u5730\u5f62\u4f30\u8ba1\uff0c\u5b9e\u73b0\u5bf9\u4e0d\u5e73\u5766\u5730\u5f62\u7684\u6709\u6548\u884c\u9a76\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7b80\u5316\u8ba1\u7b97\uff0c\u5ffd\u7565\u817f\u90e8\u52a8\u6001\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u6f5c\u529b\uff0c\u540c\u65f6\u5728\u4e0d\u5e73\u5766\u5730\u5f62\u4e0a\u884c\u9a76\u9762\u4e34\u6311\u6218\u3002", "method": "\u5f00\u53d1\u5b8c\u6574\u52a8\u6001\u6a21\u578b\u548c\u5168\u8eab\u63a7\u5236\u6846\u67b6\u7ed3\u5408\u5730\u5f62\u4f30\u8ba1", "result": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u7684\u516d\u81ea\u7531\u5ea6\u8f6e\u5f0f\u53cc\u8db3\u673a\u5668\u4eba\uff0c\u9a8c\u8bc1\u4e86\u5730\u5f62\u4f30\u8ba1\u7b97\u6cd5\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u6210\u529f\u5728\u4e0d\u5e73\u5766\u5730\u5f62\u4e0a\u884c\u9a76\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u63a7\u5236\u6846\u67b6\u80fd\u591f\u63d0\u9ad8\u8f6e\u5f0f\u53cc\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e0a\u7684\u8fd0\u52a8\u80fd\u529b\uff0c\u5177\u6709\u826f\u597d\u7684\u5b9e\u9645\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2511.07085", "categories": ["cs.HC", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.07085", "abs": "https://arxiv.org/abs/2511.07085", "authors": ["Xijie Zhang", "Fengliang He", "Hong-Ning Dai"], "title": "Achieving Effective Virtual Reality Interactions via Acoustic Gesture Recognition based on Large Language Models", "comment": "5 pages, 4 figures, 1 table, under review at ICASSP 2026", "summary": "Natural and efficient interaction remains a critical challenge for virtual reality and augmented reality (VR/AR) systems. Vision-based gesture recognition suffers from high computational cost, sensitivity to lighting conditions, and privacy leakage concerns. Acoustic sensing provides an attractive alternative: by emitting inaudible high-frequency signals and capturing their reflections, channel impulse response (CIR) encodes how gestures perturb the acoustic field in a low-cost and user-transparent manner. However, existing CIR-based gesture recognition methods often rely on extensive training of models on large labeled datasets, making them unsuitable for few-shot VR scenarios. In this work, we propose the first framework that leverages large language models (LLMs) for CIR-based gesture recognition in VR/AR systems. Despite LLMs' strengths, it is non-trivial to achieve few-shot and zero-shot learning of CIR gestures due to their inconspicuous features. To tackle this challenge, we collect differential CIR rather than original CIR data. Moreover, we construct a real-world dataset collected from 10 participants performing 15 gestures across three categories (digits, letters, and shapes), with 10 repetitions each. We then conduct extensive experiments on this dataset using an LLM-adopted classifier. Results show that our LLM-based framework achieves accuracy comparable to classical machine learning baselines, while requiring no domain-specific retraining.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684LLM\u6846\u67b6\u7528\u4e8eCIR\u624b\u52bf\u8bc6\u522b\uff0c\u4ee5\u89e3\u51b3VR/AR\u4e2d\u5c0f\u6837\u672c\u5b66\u4e60\u7684\u95ee\u9898\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709CIR\u624b\u52bf\u8bc6\u522b\u65b9\u6cd5\u5728\u5c0f\u6837\u672c\u60c5\u51b5\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u6ee1\u8db3VR\u573a\u666f\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684CIR\u624b\u52bf\u8bc6\u522b\u6846\u67b6\uff0c\u9002\u7528\u4e8eVR/AR\u7cfb\u7edf\u3002", "result": "\u8be5\u6846\u67b6\u5728\u65b0\u6784\u5efa\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528LLM\u5206\u7c7b\u5668\u8fbe\u5230\u4e86\u4e0e\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u57fa\u7ebf\u76f8\u5f53\u7684\u51c6\u786e\u6027\uff0c\u4e14\u65e0\u9700\u9886\u57df\u7279\u5b9a\u7684\u518d\u8bad\u7ec3\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u624b\u52bf\u8bc6\u522b\u6846\u67b6\u5728\u51c6\u786e\u6027\u548c\u7528\u6237\u900f\u660e\u6027\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u73b0\u4ee3VR/AR\u7cfb\u7edf\u3002"}}
{"id": "2511.06434", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06434", "abs": "https://arxiv.org/abs/2511.06434", "authors": ["Wenkang Hu", "Xincheng Tang", "Yanzhi E", "Yitong Li", "Zhengjie Shu", "Wei Li", "Huamin Wang", "Ruigang Yang"], "title": "Real Garment Benchmark (RGBench): A Comprehensive Benchmark for Robotic Garment Manipulation featuring a High-Fidelity Scalable Simulator", "comment": "2026 AAAI Accept", "summary": "While there has been significant progress to use simulated data to learn robotic manipulation of rigid objects, applying its success to deformable objects has been hindered by the lack of both deformable object models and realistic non-rigid body simulators. In this paper, we present Real Garment Benchmark (RGBench), a comprehensive benchmark for robotic manipulation of garments. It features a diverse set of over 6000 garment mesh models, a new high-performance simulator, and a comprehensive protocol to evaluate garment simulation quality with carefully measured real garment dynamics. Our experiments demonstrate that our simulator outperforms currently available cloth simulators by a large margin, reducing simulation error by 20% while maintaining a speed of 3 times faster. We will publicly release RGBench to accelerate future research in robotic garment manipulation. Website: https://rgbench.github.io/", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86Real Garment Benchmark\uff08RGBench\uff09\uff0c\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u53ef\u53d8\u5f62\u7269\u4f53\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u65b0\u7684\u9ad8\u6548\u6a21\u62df\u5de5\u5177\u548c\u4e30\u5bcc\u7684\u670d\u88c5\u6a21\u578b\u3002", "motivation": "\u867d\u7136\u5728\u521a\u6027\u7269\u4f53\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4e0a\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5bf9\u53ef\u53d8\u5f62\u7269\u4f53\u7684\u5e94\u7528\u53d7\u9650\u4e8e\u7f3a\u4e4f\u76f8\u5e94\u7684\u6a21\u578b\u548c\u6a21\u62df\u5668\u3002\u56e0\u6b64\uff0c\u672c\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5229\u7528\u9ad8\u6027\u80fd\u6a21\u62df\u5668\u548c\u5927\u91cf\u5b9e\u9645\u670d\u88c5\u52a8\u6001\u6570\u636e\u8fdb\u884c\u5b9e\u9a8c\u548c\u8bc4\u4f30\u3002", "result": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Real Garment Benchmark\uff08RGBench\uff09\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u670d\u88c5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u3002\u8be5\u57fa\u51c6\u5305\u62ec6000\u591a\u4e2a\u670d\u88c5\u7f51\u683c\u6a21\u578b\uff0c\u65b0\u7684\u9ad8\u6027\u80fd\u6a21\u62df\u5668\uff0c\u4ee5\u53ca\u8bc4\u4f30\u670d\u88c5\u6a21\u62df\u8d28\u91cf\u7684\u7efc\u5408\u534f\u8bae\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u6a21\u62df\u5668\u5728\u6a21\u62df\u7cbe\u5ea6\u548c\u901f\u5ea6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6a21\u62df\u5668\u3002", "conclusion": "RGBench\u5c06\u4fc3\u8fdb\u673a\u5668\u4eba\u670d\u88c5\u64cd\u4f5c\u7684\u672a\u6765\u7814\u7a76\uff0c\u5e76\u63d0\u9ad8\u6a21\u62df\u7684\u771f\u5b9e\u611f\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2511.07223", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07223", "abs": "https://arxiv.org/abs/2511.07223", "authors": ["Mohammad Hasan Payandeh", "Lin-Ping Yuan", "Jian Zhao"], "title": "NoteEx: Interactive Visual Context Manipulation for LLM-Assisted Exploratory Data Analysis in Computational Notebooks", "comment": null, "summary": "Computational notebooks have become popular for Exploratory Data Analysis (EDA), augmented by LLM-based code generation and result interpretation. Effective LLM assistance hinges on selecting informative context -- the minimal set of cells whose code, data, or outputs suffice to answer a prompt. As notebooks grow long and messy, users can lose track of the mental model of their analysis. They thus fail to curate appropriate contexts for LLM tasks, causing frustration and tedious prompt engineering. We conducted a formative study (n=6) that surfaced challenges in LLM context selection and mental model maintenance. Therefore, we introduce NoteEx, a JupyterLab extension that provides a semantic visualization of the EDA workflow, allowing analysts to externalize their mental model, specify analysis dependencies, and enable interactive selection of task-relevant contexts for LLMs. A user study (n=12) against a baseline shows that NoteEx improved mental model retention and context selection, leading to more accurate and relevant LLM responses.", "AI": {"tldr": "NoteEx\u662f\u4e00\u79cdJupyterLab\u6269\u5c55\uff0c\u901a\u8fc7\u8bed\u4e49\u53ef\u89c6\u5316\u5e2e\u52a9\u7528\u6237\u66f4\u6709\u6548\u5730\u9009\u62e9\u4e0a\u4e0b\u6587\uff0c\u6539\u5584\u6570\u636e\u5206\u6790\u4e2d\u7684LLM\u5e94\u7528\u3002", "motivation": "\u89e3\u51b3\u7528\u6237\u5728\u4f7f\u7528LLM\u8f85\u52a9\u8fdb\u884c\u6570\u636e\u5206\u6790\u65f6\uff0c\u56e0\u4e0a\u4e0b\u6587\u9009\u62e9\u548c\u5fc3\u7406\u6a21\u578b\u7ef4\u62a4\u4e0d\u5f53\u5f15\u53d1\u7684\u632b\u8d25\u611f\u3002", "method": "\u901a\u8fc7\u5f62\u6210\u6027\u7814\u7a76\u548c\u7528\u6237\u7814\u7a76\u8bc4\u4f30NoteEx\u7684\u6548\u679c\u3002", "result": "NoteEx\u663e\u8457\u6539\u5584\u4e86\u7528\u6237\u7684\u5fc3\u7406\u6a21\u578b\u4fdd\u6301\u548c\u4e0a\u4e0b\u6587\u9009\u62e9\uff0c\u63d0\u5347\u4e86LLM\u56de\u590d\u7684\u51c6\u786e\u6027\u548c\u76f8\u5173\u6027\u3002", "conclusion": "NoteEx\u80fd\u591f\u5e2e\u52a9\u7528\u6237\u66f4\u597d\u5730\u7ef4\u62a4\u5fc3\u7406\u6a21\u578b\u53ca\u4e0a\u4e0b\u6587\u9009\u62e9\uff0c\u4fc3\u8fdb\u66f4\u6709\u6548\u7684\u6570\u636e\u5206\u6790\u548cLLM\u4ea4\u4e92\u3002"}}
{"id": "2511.06465", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06465", "abs": "https://arxiv.org/abs/2511.06465", "authors": ["Lingfan Bao", "Tianhu Peng", "Chengxu Zhou"], "title": "Sim-to-Real Transfer in Deep Reinforcement Learning for Bipedal Locomotion", "comment": "Sim-to-real for bipedal locomotion chapter", "summary": "This chapter addresses the critical challenge of simulation-to-reality (sim-to-real) transfer for deep reinforcement learning (DRL) in bipedal locomotion. After contextualizing the problem within various control architectures, we dissect the ``curse of simulation'' by analyzing the primary sources of sim-to-real gap: robot dynamics, contact modeling, state estimation, and numerical solvers. Building on this diagnosis, we structure the solutions around two complementary philosophies. The first is to shrink the gap through model-centric strategies that systematically improve the simulator's physical fidelity. The second is to harden the policy, a complementary approach that uses in-simulation robustness training and post-deployment adaptation to make the policy inherently resilient to model inaccuracies. The chapter concludes by synthesizing these philosophies into a strategic framework, providing a clear roadmap for developing and evaluating robust sim-to-real solutions.", "AI": {"tldr": "\u672c\u7ae0\u63a2\u8ba8\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u53cc\u8db3\u884c\u8d70\u4e2d\u7684\u6a21\u62df\u5230\u73b0\u5b9e\u8f6c\u79fb\u6311\u6218\uff0c\u5206\u6790\u4e86\u4e3b\u8981\u5dee\u8ddd\u6765\u6e90\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u7269\u7406\u903c\u771f\u5ea6\u548c\u589e\u5f3a\u7b56\u7565\u9c81\u68d2\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u53cc\u8db3\u884c\u8d70\u4e2d\u6a21\u62df\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u7684\u8f6c\u79fb\u95ee\u9898\u7684\u6311\u6218\u3002", "method": "\u5206\u6790\u53cc\u8db3\u884c\u8d70\u4e2d\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6a21\u62df\u5230\u73b0\u5b9e\u8f6c\u79fb\u95ee\u9898\u53ca\u5176\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u63d0\u51fa\u4e86\u4e24\u79cd\u4e92\u8865\u7684\u89e3\u51b3\u7b56\u7565\uff1a\u4e00\u79cd\u662f\u901a\u8fc7\u63d0\u9ad8\u6a21\u62df\u5668\u7684\u7269\u7406\u903c\u771f\u5ea6\u6765\u7f29\u5c0f\u5dee\u8ddd\uff0c\u53e6\u4e00\u79cd\u662f\u901a\u8fc7\u5728\u6a21\u62df\u4e2d\u8fdb\u884c\u9c81\u68d2\u6027\u8bad\u7ec3\u6765\u589e\u5f3a\u7b56\u7565\u7684\u6297\u5e72\u6270\u6027\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u8fd9\u4e24\u79cd\u7b56\u7565\uff0c\u5efa\u7acb\u4e86\u4e00\u5957\u6218\u7565\u6846\u67b6\uff0c\u4e3a\u5f00\u53d1\u548c\u8bc4\u4f30\u5f3a\u5065\u7684\u6a21\u62df\u5230\u73b0\u5b9e\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u8def\u7ebf\u56fe\u3002"}}
{"id": "2511.07277", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.07277", "abs": "https://arxiv.org/abs/2511.07277", "authors": ["Michelle Huang", "Violeta J. Rodriguez", "Koustuv Saha", "Tal August"], "title": "Designing Beyond Language: Sociotechnical Barriers in AI Health Technologies for Limited English Proficiency", "comment": null, "summary": "Limited English proficiency (LEP) patients in the U.S. face systemic barriers to healthcare beyond language and interpreter access, encompassing procedural and institutional constraints. AI advances may support communication and care through on-demand translation and visit preparation, but also risk exacerbating existing inequalities. We conducted storyboard-driven interviews with 14 patient navigators to explore how AI could shape care experiences for Spanish-speaking LEP individuals. We identified tensions around linguistic and cultural misunderstandings, privacy concerns, and opportunities and risks for AI to augment care workflows. Participants highlighted structural factors that can undermine trust in AI systems, including sensitive information disclosure, unstable technology access, and low digital literacy. While AI tools can potentially alleviate social barriers and institutional constraints, there are risks of misinformation and uprooting human camaraderie. Our findings contribute design considerations for AI that support LEP patients and care teams via rapport-building, education, and language support, and minimizing disruptions to existing practices.", "AI": {"tldr": "\u6b64\u7814\u7a76\u63a2\u8ba8\u4e86\u4eba\u5de5\u667a\u80fd\u5982\u4f55\u5728\u533b\u7597\u4e2d\u5e2e\u52a9\u6709\u9650\u82f1\u8bed\u80fd\u529b\u60a3\u8005\uff0c\u4f46\u540c\u65f6\u4e5f\u5b58\u5728\u6f5c\u5728\u7684\u98ce\u9669\u548c\u969c\u788d\u3002", "motivation": "\u63a2\u8ba8\u5982\u4f55\u501f\u52a9\u4eba\u5de5\u667a\u80fd\u6280\u672f\u6539\u5584\u6709\u9650\u82f1\u8bed\u80fd\u529b\uff08LEP\uff09\u60a3\u8005\u7684\u533b\u7597\u4f53\u9a8c\uff0c\u5c24\u5176\u662f\u897f\u73ed\u7259\u8bed\u60a3\u8005\u9762\u4e34\u7684\u8bed\u8a00\u4e0e\u6587\u5316\u969c\u788d\u3002", "method": "\u901a\u8fc7\u5bf914\u540d\u60a3\u8005\u5bfc\u822a\u5458\u8fdb\u884c\u6545\u4e8b\u677f\u9a71\u52a8\u8bbf\u8c08\uff0c\u4ee5\u6df1\u5165\u4e86\u89e3\u4eba\u5de5\u667a\u80fd\u5bf9\u897f\u73ed\u7259\u8bedLEP\u60a3\u8005\u7684\u6f5c\u5728\u5f71\u54cd\u3002", "result": "\u8bc6\u522b\u4e86\u4eba\u5de5\u667a\u80fd\u5728\u533b\u7597\u62a4\u7406\u4e2d\u53ef\u80fd\u5e26\u6765\u7684\u673a\u9047\u4e0e\u98ce\u9669\uff0c\u5305\u62ec\u8bed\u8a00\u548c\u6587\u5316\u8bef\u89e3\u3001\u9690\u79c1\u95ee\u9898\uff0c\u4ee5\u53ca\u5bf9\u60a3\u8005\u4fe1\u4efb\u7684\u5f71\u54cd\u3002", "conclusion": "\u5c3d\u7ba1\u4eba\u5de5\u667a\u80fd\u5de5\u5177\u53ef\u80fd\u5e2e\u52a9\u51cf\u8f7b\u793e\u4f1a\u969c\u788d\u548c\u5236\u5ea6\u9650\u5236\uff0c\u4f46\u4ecd\u9700\u8b66\u60d5\u865a\u5047\u4fe1\u606f\u7684\u4f20\u64ad\u548c\u4eba\u9645\u5173\u7cfb\u7684\u7834\u574f\u3002"}}
{"id": "2511.06496", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.06496", "abs": "https://arxiv.org/abs/2511.06496", "authors": ["Keke Long", "Jiacheng Guo", "Tianyun Zhang", "Hongkai Yu", "Xiaopeng Li"], "title": "A Low-Rank Method for Vision Language Model Hallucination Mitigation in Autonomous Driving", "comment": null, "summary": "Vision Language Models (VLMs) are increasingly used in autonomous driving to help understand traffic scenes, but they sometimes produce hallucinations, which are false details not grounded in the visual input. Detecting and mitigating hallucinations is challenging when ground-truth references are unavailable and model internals are inaccessible. This paper proposes a novel self-contained low-rank approach to automatically rank multiple candidate captions generated by multiple VLMs based on their hallucination levels, using only the captions themselves without requiring external references or model access. By constructing a sentence-embedding matrix and decomposing it into a low-rank consensus component and a sparse residual, we use the residual magnitude to rank captions: selecting the one with the smallest residual as the most hallucination-free. Experiments on the NuScenes dataset demonstrate that our approach achieves 87% selection accuracy in identifying hallucination-free captions, representing a 19% improvement over the unfiltered baseline and a 6-10% improvement over multi-agent debate method. The sorting produced by sparse error magnitudes shows strong correlation with human judgments of hallucinations, validating our scoring mechanism. Additionally, our method, which can be easily parallelized, reduces inference time by 51-67% compared to debate approaches, making it practical for real-time autonomous driving applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f4e\u79e9\u65b9\u6cd5\u6765\u81ea\u52a8\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6807\u9898\u7684\u5e7b\u89c9\u6c34\u5e73\uff0c\u5229\u7528\u6b8b\u5dee\u5927\u5c0f\u5bf9\u5176\u8fdb\u884c\u6392\u5e8f\uff0c\u5b9e\u9a8c\u8868\u660e\u65b9\u6cd5\u6709\u6548\u4e14\u6548\u7387\u9ad8\u3002", "motivation": "\u5728\u81ea\u4e3b\u9a7e\u9a76\u4e2d\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u4f1a\u751f\u6210\u57fa\u4e8e\u8f93\u5165\u7684\u865a\u5047\u7ec6\u8282\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u68c0\u6d4b\u548c\u51cf\u8f7b\u8fd9\u79cd\u5e7b\u89c9\u73b0\u8c61\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u53e5\u5b50\u5d4c\u5165\u77e9\u9635\uff0c\u5c06\u5176\u5206\u89e3\u4e3a\u4f4e\u79e9\u5171\u8bc6\u6210\u5206\u548c\u7a00\u758f\u6b8b\u5dee\uff0c\u4f7f\u7528\u7a00\u758f\u6b8b\u5dee\u7684\u5927\u5c0f\u6765\u5bf9\u5019\u9009\u8bf4\u660e\u8fdb\u884c\u6392\u5e8f\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f4e\u79e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u6392\u540d\u4e0d\u540c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u5019\u9009\u8bf4\u660e\uff0c\u6765\u68c0\u6d4b\u548c\u51cf\u8f7b\u5e7b\u89c9\u73b0\u8c61\u3002\u8be5\u65b9\u6cd5\u53ea\u4f7f\u7528\u8bf4\u660e\u6587\u672c\uff0c\u4e0d\u9700\u8981\u5916\u90e8\u53c2\u8003\u6216\u6a21\u578b\u5185\u90e8\u8bbf\u95ee\uff0c\u6d89\u53ca\u901a\u8fc7\u6784\u5efa\u53e5\u5b50\u5d4c\u5165\u77e9\u9635\u5e76\u5c06\u5176\u5206\u89e3\u4e3a\u4f4e\u79e9\u5171\u8bc6\u6210\u5206\u548c\u7a00\u758f\u6b8b\u5dee\u6765\u5b9e\u73b0\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8bc6\u522b\u5e7b\u89c9-free\u6807\u9898\u7684\u51c6\u786e\u7387\u4e0a\u8d85\u8fc7\u5176\u4ed6\u65b9\u6cd5\uff0c\u4e14\u80fd\u663e\u8457\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\uff0c\u975e\u5e38\u9002\u5408\u5b9e\u65f6\u81ea\u4e3b\u9a7e\u9a76\u5e94\u7528\u3002"}}
{"id": "2511.07401", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.07401", "abs": "https://arxiv.org/abs/2511.07401", "authors": ["Benjamin Lebrun", "Christoph Bartneck", "David Kaber", "Andrew Vonasch"], "title": "People Perceive More Phantom Costs From Autonomous Agents When They Make Unreasonably Generous Offers", "comment": "20 pages, 5 figures, 4 tables", "summary": "People often reject offers that are too generous due to the perception of hidden drawbacks referred to as \"phantom costs.\" We hypothesized that this perception and the decision-making vary based on the type of agent making the offer (human vs. robot) and the degree to which the agent is perceived to be autonomous or have the capacity for self-interest. To test this conjecture, participants (N = 855) engaged in a car-buying simulation where a human or robot sales agent, described as either autonomous or not, offered either a small (5%) or large (85%) discount. Results revealed that the robot was perceived as less self-interested than the human, which reduced the perception of phantom costs. While larger discounts increased phantom costs, they also increased purchase intentions, suggesting that perceived benefits can outweigh phantom costs. Importantly, phantom costs were not only attributed to the agent participants interacted with, but also to the product and the agent's manager, highlighting at least three sources of suspicion. These findings deepen our understanding of to whom people assign responsibility and how perceptions shape both human-human and human-robot interactions, with implications for ethical AI design and marketing strategies.", "AI": {"tldr": "\u4eba\u4eec\u56e0\u770b\u4f3c\u9690\u79d8\u7684\u7f3a\u9677\u800c\u62d2\u7edd\u8fc7\u4e8e\u6177\u6168\u7684\u62a5\u4ef7\u3002\u7814\u7a76\u63a2\u8ba8\u4e86\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u9500\u552e\u4ee3\u7406\u7684\u4e0d\u540c\u5f71\u54cd\uff0c\u53d1\u73b0\u673a\u5668\u4eba\u56e0\u88ab\u89c6\u4e3a\u8f83\u5c11\u81ea\u5229\u800c\u964d\u4f4e\u4e86\u9690\u6027\u6210\u672c\u611f\u77e5\u3002", "motivation": "\u7814\u7a76\u4eba\u4eec\u5bf9\u8fc7\u4e8e\u6177\u6168\u62a5\u4ef7\u7684\u62d2\u7edd\u53ca\u5176\u80cc\u540e\u7684\u5fc3\u7406\u673a\u5236\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u540c\u4ee3\u7406\u7c7b\u578b\u7684\u5f71\u54cd\u4e0b\u3002", "method": "\u901a\u8fc7\u4e00\u9879\u6c7d\u8f66\u8d2d\u4e70\u6a21\u62df\u5b9e\u9a8c\uff0c\u53c2\u4e0e\u8005\u5728\u4e0d\u540c\u4ea4\u6613\u6761\u4ef6\u4e0b\u4e0e\u4eba\u7c7b\u6216\u673a\u5668\u4eba\u9500\u552e\u4ee3\u7406\u4e92\u52a8\u3002", "result": "\u673a\u5668\u4eba\u6bd4\u4eba\u7c7b\u88ab\u8ba4\u4e3a\u66f4\u5c11\u81ea\u5229\uff0c\u51cf\u5c11\u4e86\u9690\u6027\u6210\u672c\u611f\u77e5\uff1b\u5927\u6298\u6263\u867d\u7136\u589e\u52a0\u9690\u6027\u6210\u672c\uff0c\u4f46\u4e5f\u63d0\u9ad8\u4e86\u8d2d\u4e70\u610f\u56fe\uff0c\u663e\u793a\u611f\u77e5\u5229\u76ca\u53ef\u4ee5\u8d85\u8fc7\u9690\u6027\u6210\u672c\u3002", "conclusion": "\u4eba\u4eec\u5bf9\u8d23\u4efb\u7684\u5f52\u5c5e\u53ca\u5176\u611f\u77e5\u5f71\u54cd\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u4e92\u52a8\uff0c\u8fd9\u5bf9\u4eba\u5de5\u667a\u80fd\u7684\u4f26\u7406\u8bbe\u8ba1\u548c\u8425\u9500\u7b56\u7565\u6709\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2511.06500", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06500", "abs": "https://arxiv.org/abs/2511.06500", "authors": ["JiaHao Wu", "ShengWen Yu"], "title": "Adaptive PID Control for Robotic Systems via Hierarchical Meta-Learning and Reinforcement Learning with Physics-Based Data Augmentation", "comment": "21 pages,12 tables, 6 figures", "summary": "Proportional-Integral-Derivative (PID) controllers remain the predominant choice in industrial robotics due to their simplicity and reliability. However, manual tuning of PID parameters for diverse robotic platforms is time-consuming and requires extensive domain expertise. This paper presents a novel hierarchical control framework that combines meta-learning for PID initialization and reinforcement learning (RL) for online adaptation. To address the sample efficiency challenge, a \\textit{physics-based data augmentation} strategy is introduced that generates virtual robot configurations by systematically perturbing physical parameters, enabling effective meta-learning with limited real robot data. The proposed approach is evaluated on two heterogeneous platforms: a 9-DOF Franka Panda manipulator and a 12-DOF Laikago quadruped robot. Experimental results demonstrate that the proposed method achieves 16.6\\% average improvement on Franka Panda (6.26\u00b0 MAE), with exceptional gains in high-load joints (J2: 80.4\\% improvement from 12.36\u00b0 to 2.42\u00b0). Critically, this work discovers the \\textit{optimization ceiling effect}: RL achieves dramatic improvements when meta-learning exhibits localized high-error joints, but provides no benefit (0.0\\%) when baseline performance is uniformly strong, as observed in Laikago. The method demonstrates robust performance under disturbances (parameter uncertainty: +19.2\\%, no disturbance: +16.6\\%, average: +10.0\\%) with only 10 minutes of training time. Multi-seed analysis across 100 random initializations confirms stable performance (4.81+/-1.64\\% average). These results establish that RL effectiveness is highly dependent on meta-learning baseline quality and error distribution, providing important design guidance for hierarchical control systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5143\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u5206\u5c42\u63a7\u5236\u6846\u67b6\uff0c\u89e3\u51b3\u4e86PID\u63a7\u5236\u5668\u53c2\u6570\u8c03\u8282\u7684\u6311\u6218\uff0c\u901a\u8fc7\u7269\u7406\u6570\u636e\u589e\u5f3a\u7b56\u7565\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u5e73\u53f0\u4e0a\u5747\u6709\u663e\u8457\u6539\u8fdb\uff0c\u4e14\u5f3a\u5316\u5b66\u4e60\u7684\u6548\u679c\u4f9d\u8d56\u4e8e\u5143\u5b66\u4e60\u7684\u8d28\u91cf\u548c\u8bef\u5dee\u5206\u5e03\u3002", "motivation": "\u5728\u5de5\u4e1a\u673a\u5668\u4eba\u4e2d\uff0cPID\u63a7\u5236\u5668\u56e0\u5176\u7b80\u5355\u6027\u548c\u53ef\u9760\u6027\u4ecd\u7136\u662f\u4e3b\u8981\u9009\u62e9\uff0c\u4f46\u624b\u52a8\u8c03\u8282PID\u53c2\u6570\u8017\u65f6\u4e14\u9700\u8981\u5e7f\u6cdb\u7684\u9886\u57df\u4e13\u957f\u3002", "method": "\u5efa\u7acb\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5c42\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u5143\u5b66\u4e60\u8fdb\u884cPID\u521d\u59cb\u5316\u548c\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u5728\u7ebf\u9002\u5e94\uff0c\u91c7\u7528\u57fa\u4e8e\u7269\u7406\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\u6765\u63d0\u9ad8\u6837\u672c\u6548\u7387\u3002", "result": "\u5728Franka Panda\uff08\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a6.26\u00b0\uff09\u4e0a\u83b7\u5f97\u4e8616.6%\u7684\u5e73\u5747\u6539\u5584\uff0c\u5728Laikago\u5e73\u53f0\u4e0a\u53d1\u73b0\u4f18\u5316\u5929\u82b1\u677f\u6548\u5e94\uff0c\u4e14\u8be5\u65b9\u6cd5\u5728\u5e72\u6270\u4e0b\u8868\u73b0\u7a33\u5065\uff0c\u8bad\u7ec3\u65f6\u95f4\u4ec5\u970010\u5206\u949f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u5143\u5b66\u4e60\u57fa\u51c6\u8d28\u91cf\u548c\u8bef\u5dee\u5206\u5e03\uff0c\u4e3a\u5206\u5c42\u63a7\u5236\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002"}}
{"id": "2511.07275", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.07275", "abs": "https://arxiv.org/abs/2511.07275", "authors": ["David Black", "Septimiu Salcudean"], "title": "Robotic versus Human Teleoperation for Remote Ultrasound", "comment": "Under review at IEEE TMRB. Extended version of a paper presented at the Hamlyn Symposium for Medical Robotics, 2025", "summary": "Diagnostic medical ultrasound is widely used, safe, and relatively low cost but requires a high degree of expertise to acquire and interpret the images. Personnel with this expertise are often not available outside of larger cities, leading to difficult, costly travel and long wait times for rural populations. To address this issue, tele-ultrasound techniques are being developed, including robotic teleoperation and recently human teleoperation, in which a novice user is remotely guided in a hand-over-hand manner through mixed reality to perform an ultrasound exam. These methods have not been compared, and their relative strengths are unknown. Human teleoperation may be more practical than robotics for small communities due to its lower cost and complexity, but this is only relevant if the performance is comparable. This paper therefore evaluates the differences between human and robotic teleoperation, examining practical aspects such as setup time and flexibility and experimentally comparing performance metrics such as completion time, position tracking, and force consistency. It is found that human teleoperation does not lead to statistically significant differences in completion time or position accuracy, with mean differences of 1.8% and 0.5%, respectively, and provides more consistent force application despite being substantially more practical and accessible.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u4eba\u7c7b\u548c\u673a\u5668\u4eba\u8fdc\u7a0b\u64cd\u4f5c\u5728\u8d85\u58f0\u68c0\u67e5\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4eba\u7c7b\u8fdc\u7a0b\u64cd\u4f5c\u5728\u591a\u65b9\u9762\u66f4\u5177\u4f18\u52bf\uff0c\u4e14\u6027\u80fd\u76f8\u5f53", "motivation": "\u89e3\u51b3\u504f\u8fdc\u5730\u533a\u7f3a\u4e4f\u4e13\u4e1a\u8d85\u58f0\u68c0\u67e5\u4eba\u5458\u7684\u95ee\u9898\uff0c\u4fc3\u8fdb\u8d85\u58f0\u6280\u672f\u7684\u53ef\u53ca\u6027", "method": "\u6bd4\u8f83\u4eba\u7c7b\u8fdc\u7a0b\u64cd\u4f5c\u4e0e\u673a\u5668\u4eba\u8fdc\u7a0b\u64cd\u4f5c\u7684\u5dee\u5f02", "result": "\u4eba\u7c7b\u8fdc\u7a0b\u64cd\u4f5c\u5728\u5b8c\u6210\u65f6\u95f4\u3001\u4f4d\u7f6e\u51c6\u786e\u5ea6\u4e0a\u4e0e\u673a\u5668\u4eba\u8fdc\u7a0b\u64cd\u4f5c\u65e0\u663e\u8457\u5dee\u5f02\uff0c\u4f46\u63d0\u4f9b\u4e86\u66f4\u4e00\u81f4\u7684\u529b\u5e94\u7528", "conclusion": "\u4eba\u7c7b\u8fdc\u7a0b\u64cd\u4f5c\u4ee5\u5176\u66f4\u9ad8\u7684\u5b9e\u7528\u6027\u548c\u4fbf\u6377\u6027\uff0c\u6210\u4e3a\u504f\u8fdc\u5730\u533a\u8d85\u58f0\u68c0\u67e5\u7684\u6709\u6548\u9009\u62e9\u3002"}}
{"id": "2511.06515", "categories": ["cs.RO", "math.DS"], "pdf": "https://arxiv.org/pdf/2511.06515", "abs": "https://arxiv.org/abs/2511.06515", "authors": ["Cormac O'Neill", "Jasmine Terrones", "H. Harry Asada"], "title": "Koopman global linearization of contact dynamics for robot locomotion and manipulation enables elaborate control", "comment": null, "summary": "Controlling robots that dynamically engage in contact with their environment is a pressing challenge. Whether a legged robot making-and-breaking contact with a floor, or a manipulator grasping objects, contact is everywhere. Unfortunately, the switching of dynamics at contact boundaries makes control difficult. Predictive controllers face non-convex optimization problems when contact is involved. Here, we overcome this difficulty by applying Koopman operators to subsume the segmented dynamics due to contact changes into a unified, globally-linear model in an embedding space. We show that viscoelastic contact at robot-environment interactions underpins the use of Koopman operators without approximation to control inputs. This methodology enables the convex Model Predictive Control of a legged robot, and the real-time control of a manipulator engaged in dynamic pushing. In this work, we show that our method allows robots to discover elaborate control strategies in real-time over time horizons with multiple contact changes, and the method is applicable to broad fields beyond robotics.", "AI": {"tldr": "\u901a\u8fc7\u5e94\u7528Koopman\u7b97\u5b50\uff0c\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u514b\u670d\u4e86\u63a5\u89e6\u52a8\u6001\u5207\u6362\u5e26\u6765\u7684\u63a7\u5236\u56f0\u96be\uff0c\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u5728\u4e0e\u73af\u5883\u4e92\u52a8\u65f6\u7684\u5b9e\u65f6\u63a7\u5236\u3002", "motivation": "\u52a8\u6001\u63a5\u89e6\u63a7\u5236\u662f\u673a\u5668\u4eba\u7684\u4e00\u5927\u6311\u6218\uff0c\u63a5\u89e6\u8fb9\u754c\u7684\u52a8\u529b\u5b66\u5207\u6362\u4f7f\u5f97\u63a7\u5236\u53d8\u5f97\u590d\u6742\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5b9e\u73b0\u63a7\u5236\u3002", "method": "\u4f7f\u7528Koopman\u7b97\u5b50\u5c06\u63a5\u89e6\u53d8\u5316\u5f15\u8d77\u7684\u5206\u6bb5\u52a8\u529b\u5b66\u6574\u5408\u4e3a\u7edf\u4e00\u7684\u5168\u5c40\u7ebf\u6027\u6a21\u578b\uff0c\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u63a7\u5236\u3002", "result": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5229\u7528Koopman\u7b97\u5b50\u5c06\u7531\u4e8e\u63a5\u89e6\u53d8\u5316\u800c\u5206\u6bb5\u7684\u52a8\u529b\u5b66\u5408\u5e76\u4e3a\u7edf\u4e00\u7684\u5168\u5c40\u7ebf\u6027\u6a21\u578b\uff0c\u4ece\u800c\u63a7\u5236\u4e0e\u73af\u5883\u52a8\u6001\u63a5\u89e6\u7684\u673a\u5668\u4eba\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u91c7\u7528Koopman\u7b97\u5b50\u7684\u65b9\u6cd5\u80fd\u591f\u4f7f\u673a\u5668\u4eba\u5728\u591a\u63a5\u89e6\u53d8\u5316\u7684\u65f6\u95f4\u8303\u56f4\u5185\u5b9e\u65f6\u53d1\u73b0\u590d\u6742\u7684\u63a7\u5236\u7b56\u7565\uff0c\u8be5\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u9886\u57df\u4e4b\u5916\u4e5f\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2511.06575", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.06575", "abs": "https://arxiv.org/abs/2511.06575", "authors": ["Jun Wang", "Yevgeniy Vorobeychik", "Yiannis Kantaros"], "title": "CoFineLLM: Conformal Finetuning of LLMs for Language-Instructed Robot Planning", "comment": null, "summary": "Large Language Models (LLMs) have recently emerged as planners for language-instructed agents, generating sequences of actions to accomplish natural language tasks. However, their reliability remains a challenge, especially in long-horizon tasks, since they often produce overconfident yet wrong outputs. Conformal Prediction (CP) has been leveraged to address this issue by wrapping LLM outputs into prediction sets that contain the correct action with a user-defined confidence. When the prediction set is a singleton, the planner executes that action; otherwise, it requests help from a user. This has led to LLM-based planners that can ensure plan correctness with a user-defined probability. However, as LLMs are trained in an uncertainty-agnostic manner, without awareness of prediction sets, they tend to produce unnecessarily large sets, particularly at higher confidence levels, resulting in frequent human interventions limiting autonomous deployment. To address this, we introduce CoFineLLM (Conformal Finetuning for LLMs), the first CP-aware finetuning framework for LLM-based planners that explicitly reduces prediction-set size and, in turn, the need for user interventions. We evaluate our approach on multiple language-instructed robot planning problems and show consistent improvements over uncertainty-aware and uncertainty-agnostic finetuning baselines in terms of prediction-set size, and help rates. Finally, we demonstrate robustness of our method to out-of-distribution scenarios in hardware experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5CoFineLLM\uff0c\u901a\u8fc7\u5fae\u8c03LLM\u4ee5\u51cf\u5c11\u7528\u6237\u5e72\u9884\uff0c\u63d0\u9ad8\u89c4\u5212\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u65f6\u95f4\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u6027\u8f83\u4f4e\uff0c\u5bb9\u6613\u4ea7\u751f\u8fc7\u5ea6\u81ea\u4fe1\u7684\u9519\u8bef\u8f93\u51fa\uff0c\u56e0\u6b64\u9700\u8981\u63d0\u9ad8\u5176\u8f93\u51fa\u7684\u51c6\u786e\u6027\u548c\u51cf\u5c11\u7528\u6237\u7684\u5e72\u9884\u3002", "method": "\u91c7\u7528\u5171\u5f62\u9884\u6d4b\u6846\u67b6\uff0c\u5bf9LLM\u8fdb\u884c\u5fae\u8c03\uff0c\u4ece\u800c\u4f7f\u6a21\u578b\u80fd\u591f\u751f\u6210\u66f4\u5c0f\u7684\u9884\u6d4b\u96c6\u5408\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoFineLLM\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u5f62\u9884\u6d4b\uff08CP\uff09\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u51cf\u5c11\u9884\u6d4b\u96c6\u5408\u7684\u5927\u5c0f\uff0c\u63d0\u9ad8\u81ea\u52a8\u5316\u80fd\u529b\uff0c\u964d\u4f4e\u7528\u6237\u5e72\u9884\u3002", "conclusion": "CoFineLLM\u663e\u8457\u51cf\u5c0f\u4e86LLM\u7684\u9884\u6d4b\u96c6\u5408\u5927\u5c0f\uff0c\u964d\u4f4e\u4e86\u5bf9\u7528\u6237\u5e2e\u52a9\u7684\u9700\u6c42\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u89c4\u5212\u4efb\u52a1\u7684\u6548\u679c\u3002"}}
{"id": "2511.06578", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.06578", "abs": "https://arxiv.org/abs/2511.06578", "authors": ["Kaustubh Singh", "Shivam Kumar", "Shashikant Pawar", "Sandeep Manjanna"], "title": "Underactuated Biomimetic Autonomous Underwater Vehicle for Ecosystem Monitoring", "comment": "ICRA RUNE 2024 Workshop Paper", "summary": "In this paper, we present an underactuated biomimetic underwater robot that is suitable for ecosystem monitoring in both marine and freshwater environments. We present an updated mechanical design for a fish-like robot and propose minimal actuation behaviors learned using reinforcement learning techniques. We present our preliminary mechanical design of the tail oscillation mechanism and illustrate the swimming behaviors on FishGym simulator, where the reinforcement learning techniques will be tested on", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u6d77\u6d0b\u548c\u6de1\u6c34\u751f\u6001\u76d1\u6d4b\u7684\u6b20\u9a71\u52a8\u4eff\u751f\u6c34\u4e0b\u673a\u5668\u4eba\uff0c\u7ed3\u5408\u4e86\u673a\u5236\u8bbe\u8ba1\u548c\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u3002", "motivation": "\u5f00\u53d1\u9002\u5408\u751f\u6001\u76d1\u6d4b\u7684\u6c34\u4e0b\u673a\u5668\u4eba\uff0c\u63d0\u9ad8\u6c34\u4e0b\u63a2\u6d4b\u80fd\u529b\u548c\u6548\u7387\u3002", "method": "\u91c7\u7528\u66f4\u65b0\u7684\u673a\u68b0\u8bbe\u8ba1\uff0c\u5e76\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u5b66\u4e60\u6700\u5c0f\u9a71\u52a8\u884c\u4e3a\u3002", "result": "\u521d\u6b65\u673a\u68b0\u8bbe\u8ba1\u548c\u6e38\u6cf3\u884c\u4e3a\u5728FishGym\u6a21\u62df\u5668\u4e0a\u6d4b\u8bd5\uff0c\u5c55\u793a\u51fa\u673a\u5668\u4eba\u5728\u63a7\u5236\u548c\u6e38\u52a8\u65b9\u9762\u7684\u826f\u597d\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u663e\u793a\u4e86\u65b0\u578b\u9c7c\u7c7b\u673a\u5668\u4eba\u5728\u751f\u6001\u76d1\u6d4b\u4e2d\u7684\u6f5c\u529b\uff0c\u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u6709\u6548\u63d0\u5347\u5176\u6e38\u6cf3\u884c\u4e3a\u3002"}}
{"id": "2511.06619", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06619", "abs": "https://arxiv.org/abs/2511.06619", "authors": ["Chuheng Zhang", "Rushuai Yang", "Xiaoyu Chen", "Kaixin Wang", "Li Zhao", "Yi Chen", "Jiang Bian"], "title": "How Do VLAs Effectively Inherit from VLMs?", "comment": null, "summary": "Vision-language-action (VLA) models hold the promise to attain generalizable embodied control. To achieve this, a pervasive paradigm is to leverage the rich vision-semantic priors of large vision-language models (VLMs). However, the fundamental question persists: How do VLAs effectively inherit the prior knowledge from VLMs? To address this critical question, we introduce a diagnostic benchmark, GrinningFace, an emoji tabletop manipulation task where the robot arm is asked to place objects onto printed emojis corresponding to language instructions. This task design is particularly revealing -- knowledge associated with emojis is ubiquitous in Internet-scale datasets used for VLM pre-training, yet emojis themselves are largely absent from standard robotics datasets. Consequently, they provide a clean proxy: successful task completion indicates effective transfer of VLM priors to embodied control. We implement this diagnostic task in both simulated environment and a real robot, and compare various promising techniques for knowledge transfer. Specifically, we investigate the effects of parameter-efficient fine-tuning, VLM freezing, co-training, predicting discretized actions, and predicting latent actions. Through systematic evaluation, our work not only demonstrates the critical importance of preserving VLM priors for the generalization of VLA but also establishes guidelines for future research in developing truly generalizable embodied AI systems.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86GrinningFace\u57fa\u51c6\u4efb\u52a1\uff0c\u8bc4\u4f30VLA\u6a21\u578b\u5982\u4f55\u5229\u7528VLM\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u901a\u8fc7\u4efb\u52a1\u7684\u6210\u529f\u4e0e\u5426\u6765\u89c2\u5bdf\u77e5\u8bc6\u8f6c\u79fb\u7684\u6548\u679c\uff0c\u5e76\u6bd4\u8f83\u591a\u79cd\u77e5\u8bc6\u8f6c\u79fb\u6280\u672f\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u89e3\u51b3VLAs\u5982\u4f55\u6709\u6548\u7ee7\u627fVLM\u7684\u5148\u9a8c\u77e5\u8bc6\u8fd9\u4e00\u5173\u952e\u95ee\u9898\uff0c\u4ee5\u5b9e\u73b0\u66f4\u52a0\u53ef\u63a8\u5e7f\u7684\u5177\u8eab\u63a7\u5236\u80fd\u529b\u3002", "method": "\u5728\u6a21\u62df\u73af\u5883\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5b9e\u65bdGrinningFace\u4efb\u52a1\uff0c\u6bd4\u8f83\u4e86\u591a\u79cd\u77e5\u8bc6\u8f6c\u79fb\u6280\u672f\uff0c\u5305\u62ec\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u3001VLM\u51bb\u7ed3\u3001\u534f\u540c\u8bad\u7ec3\u548c\u52a8\u4f5c\u9884\u6d4b\u7b49\u3002", "result": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8(VLA)\u6a21\u578b\u5982\u4f55\u6709\u6548\u7ee7\u627f\u6765\u81ea\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b(VLM)\u7684\u77e5\u8bc6\u3002\u901a\u8fc7\u5f15\u5165\u4e00\u9879\u540d\u4e3aGrinningFace\u7684\u8bca\u65ad\u57fa\u51c6\uff0c\u8bc4\u4ef7\u673a\u5668\u4eba\u5728\u6267\u884cemoji\u8868\u60c5\u56fe\u6807\u7684\u64cd\u4f5c\u4efb\u52a1\u65f6\u7684\u8868\u73b0\uff0c\u65e8\u5728\u901a\u8fc7\u8be5\u4efb\u52a1\u6765\u8bc4\u4f30VLA\u6a21\u578b\u7684\u77e5\u8bc6\u8f6c\u79fb\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u4fdd\u6301VLM\u5148\u9a8c\u77e5\u8bc6\u5bf9\u4e8eVLA\u7684\u6cdb\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u672a\u6765\u5f00\u53d1\u53ef\u6cdb\u5316\u7684\u5177\u8eabAI\u7cfb\u7edf\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2511.06667", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06667", "abs": "https://arxiv.org/abs/2511.06667", "authors": ["Andrew Choi", "Dezhong Tong"], "title": "Rapidly Learning Soft Robot Control via Implicit Time-Stepping", "comment": "Code: https://github.com/QuantuMope/dismech-rl", "summary": "With the explosive growth of rigid-body simulators, policy learning in simulation has become the de facto standard for most rigid morphologies. In contrast, soft robotic simulation frameworks remain scarce and are seldom adopted by the soft robotics community. This gap stems partly from the lack of easy-to-use, general-purpose frameworks and partly from the high computational cost of accurately simulating continuum mechanics, which often renders policy learning infeasible. In this work, we demonstrate that rapid soft robot policy learning is indeed achievable via implicit time-stepping. Our simulator of choice, DisMech, is a general-purpose, fully implicit soft-body simulator capable of handling both soft dynamics and frictional contact. We further introduce delta natural curvature control, a method analogous to delta joint position control in rigid manipulators, providing an intuitive and effective means of enacting control for soft robot learning. To highlight the benefits of implicit time-stepping and delta curvature control, we conduct extensive comparisons across four diverse soft manipulator tasks against one of the most widely used soft-body frameworks, Elastica. With implicit time-stepping, parallel stepping of 500 environments achieves up to 6x faster speeds for non-contact cases and up to 40x faster for contact-rich scenarios. Finally, a comprehensive sim-to-sim gap evaluation--training policies in one simulator and evaluating them in another--demonstrates that implicit time-stepping provides a rare free lunch: dramatic speedups achieved without sacrificing accuracy.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u901a\u8fc7\u9690\u5f0f\u65f6\u95f4\u6b65\u8fdb\u5b9e\u73b0\u5feb\u901f\u8f6f\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\uff0c\u4ee5\u53ca\u4ecb\u7ecd\u4e86 delta \u81ea\u7136\u66f2\u7387\u63a7\u5236\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u586b\u8865\u8f6f\u673a\u5668\u4eba\u6a21\u62df\u6846\u67b6\u7684\u7f3a\u53e3\uff0c\u63d0\u9ad8\u8f6f\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u7684\u6548\u7387\u548c\u53ef\u64cd\u4f5c\u6027\u3002", "method": "\u4f7f\u7528\u901a\u7528\u7684\u9690\u5f0f\u8f6f\u4f53\u6a21\u62df\u5668DisMech\u548cdelta\u81ea\u7136\u66f2\u7387\u63a7\u5236\u65b9\u6cd5\u8fdb\u884c\u8f6f\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u3002", "result": "\u4f7f\u7528\u9690\u5f0f\u65f6\u95f4\u6b65\u8fdb\u5b9e\u73b0\u6700\u592740\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u62df\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u9690\u5f0f\u65f6\u95f4\u6b65\u8fdb\u80fd\u591f\u5728\u4e0d\u727a\u7272\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u5b9e\u73b0\u663e\u8457\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u5f62\u5f0f\u4e0a\u4e3a\u8f6f\u673a\u5668\u4eba\u653f\u7b56\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2511.06673", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06673", "abs": "https://arxiv.org/abs/2511.06673", "authors": ["Joel Kemp", "Andre Farinha", "David Howard", "Krishna Manaswi Digumarti", "Josh Pinskier"], "title": "Programmable Telescopic Soft Pneumatic Actuators for Deployable and Shape Morphing Soft Robots", "comment": "8 pages, 10 figures, Submitted to Robosoft 2026", "summary": "Soft Robotics presents a rich canvas for free-form and continuum devices capable of exerting forces in any direction and transforming between arbitrary configurations. However, there is no current way to tractably and directly exploit the design freedom due to the curse of dimensionality. Parameterisable sets of designs offer a pathway towards tractable, modular soft robotics that appropriately harness the behavioural freeform of soft structures to create rich embodied behaviours. In this work, we present a parametrised class of soft actuators, Programmable Telescopic Soft Pneumatic Actuators (PTSPAs). PTSPAs expand axially on inflation for deployable structures and manipulation in challenging confined spaces. We introduce a parametric geometry generator to customise actuator models from high-level inputs, and explore the new design space through semi-automated experimentation and systematic exploration of key parameters. Using it we characterise the actuators' extension/bending, expansion, and stiffness and reveal clear relationships between key design parameters and performance. Finally we demonstrate the application of the actuators in a deployable soft quadruped whose legs deploy to walk, enabling automatic adaptation to confined spaces. PTSPAs present new design paradigm for deployable and shape morphing structures and wherever large length changes are required.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53c2\u6570\u5316\u8f6f\u6c14\u52a8\u9a71\u52a8\u5668\uff08PTSPAs\uff09\uff0c\u901a\u8fc7\u53c2\u6570\u5316\u51e0\u4f55\u751f\u6210\u5668\u5b9a\u5236\u9a71\u52a8\u5668\uff0c\u7814\u7a76\u4e86\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u5e76\u5728\u53ef\u90e8\u7f72\u7684\u8f6f\u56db\u8db3\u52a8\u7269\u4e2d\u5c55\u793a\u4e86\u5176\u5e94\u7528\u3002", "motivation": "\u7531\u4e8e\u7ef4\u5ea6\u8bc5\u5492\uff0c\u76ee\u524d\u6ca1\u6709\u6709\u6548\u7684\u65b9\u5f0f\u6765\u76f4\u63a5\u5229\u7528\u8f6f\u673a\u5668\u4eba\u8bbe\u8ba1\u7684\u81ea\u7531\u5ea6\uff0c\u53c2\u6570\u5316\u7684\u8bbe\u8ba1\u96c6\u4e3a\u8fd9\u79cd\u8f6f\u673a\u5668\u4eba\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u884c\u7684\u8def\u5f84\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u5316\u7684\u51e0\u4f55\u751f\u6210\u5668\uff0c\u7528\u4e8e\u4ece\u9ad8\u5c42\u8f93\u5165\u5b9a\u5236\u8f6f\u9a71\u52a8\u5668\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u534a\u81ea\u52a8\u5316\u5b9e\u9a8c\u548c\u5bf9\u5173\u952e\u53c2\u6570\u7684\u7cfb\u7edf\u63a2\u7d22\u6765\u7814\u7a76\u65b0\u7684\u8bbe\u8ba1\u7a7a\u95f4\u3002", "result": "\u4f7f\u7528PTSPAs\uff0c\u6211\u4eec\u8868\u5f81\u4e86\u9a71\u52a8\u5668\u7684\u4f38\u957f/\u5f2f\u66f2\u3001\u6269\u5c55\u548c\u521a\u5ea6\uff0c\u5e76\u63ed\u793a\u4e86\u5173\u952e\u8bbe\u8ba1\u53c2\u6570\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u660e\u786e\u5173\u7cfb\u3002\u8fd8\u5c55\u793a\u4e86\u8fd9\u4e9b\u9a71\u52a8\u5668\u5728\u53ef\u90e8\u7f72\u7684\u8f6f\u56db\u8db3\u52a8\u7269\u4e2d\u7684\u5e94\u7528\uff0c\u80fd\u591f\u5728\u72ed\u5c0f\u7a7a\u95f4\u5185\u81ea\u52a8\u9002\u5e94\u884c\u8d70\u3002", "conclusion": "PTSPAs\u4e3a\u53ef\u90e8\u7f72\u548c\u5f62\u72b6\u53d8\u6362\u7ed3\u6784\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u8bbe\u8ba1\u8303\u5f0f\uff0c\u9002\u7528\u4e8e\u9700\u8981\u5927\u5e45\u5ea6\u957f\u5ea6\u53d8\u5316\u7684\u5e94\u7528\u3002"}}
{"id": "2511.06745", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06745", "abs": "https://arxiv.org/abs/2511.06745", "authors": ["Lan Thi Ha Nguyen", "Kien Ton Manh", "Anh Do Duc", "Nam Pham Hai"], "title": "Physically-Grounded Goal Imagination: Physics-Informed Variational Autoencoder for Self-Supervised Reinforcement Learning", "comment": null, "summary": "Self-supervised goal-conditioned reinforcement learning enables robots to autonomously acquire diverse skills without human supervision. However, a central challenge is the goal setting problem: robots must propose feasible and diverse goals that are achievable in their current environment. Existing methods like RIG (Visual Reinforcement Learning with Imagined Goals) use variational autoencoder (VAE) to generate goals in a learned latent space but have the limitation of producing physically implausible goals that hinder learning efficiency. We propose Physics-Informed RIG (PI-RIG), which integrates physical constraints directly into the VAE training process through a novel Enhanced Physics-Informed Variational Autoencoder (Enhanced p3-VAE), enabling the generation of physically consistent and achievable goals. Our key innovation is the explicit separation of the latent space into physics variables governing object dynamics and environmental factors capturing visual appearance, while enforcing physical consistency through differential equation constraints and conservation laws. This enables the generation of physically consistent and achievable goals that respect fundamental physical principles such as object permanence, collision constraints, and dynamic feasibility. Through extensive experiments, we demonstrate that this physics-informed goal generation significantly improves the quality of proposed goals, leading to more effective exploration and better skill acquisition in visual robotic manipulation tasks including reaching, pushing, and pick-and-place scenarios.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7269\u7406\u4fe1\u606f\u5f15\u5bfc\u7684\u53d8\u4f53\uff08PI-RIG\uff09\uff0c\u901a\u8fc7\u589e\u5f3a\u7684VAE\u751f\u6210\u7269\u7406\u4e00\u81f4\u7684\u76ee\u6807\uff0c\u4ece\u800c\u6539\u5584\u673a\u5668\u4eba\u5728\u89c6\u89c9\u64cd\u63a7\u4efb\u52a1\u4e2d\u7684\u6280\u80fd\u83b7\u53d6\u6548\u679c\u3002", "motivation": "\u81ea\u76d1\u7763\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\u4f7f\u5f97\u673a\u5668\u4eba\u80fd\u591f\u5728\u6ca1\u6709\u4eba\u7c7b\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u81ea\u4e3b\u83b7\u53d6\u591a\u6837\u5316\u7684\u6280\u80fd\uff0c\u4f46\u5b58\u5728\u76ee\u6807\u8bbe\u5b9a\u7684\u95ee\u9898\uff0c\u5373\u673a\u5668\u4eba\u9700\u8981\u63d0\u51fa\u5728\u5f53\u524d\u73af\u5883\u4e2d\u53ef\u5b9e\u73b0\u4e14\u591a\u6837\u7684\u76ee\u6807\u3002", "method": "\u7ed3\u5408\u7269\u7406\u7ea6\u675f\u8fdb\u884cVAE\u8bad\u7ec3\uff0c\u660e\u786e\u5206\u79bb\u6f5c\u5728\u7a7a\u95f4\u5e76\u5f3a\u5236\u6267\u884c\u7269\u7406\u4e00\u81f4\u6027\uff0c\u4ee5\u751f\u6210\u53ef\u5b9e\u73b0\u7684\u76ee\u6807\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7269\u7406\u4fe1\u606f\u5f15\u5bfc\u7684\u53d8\u4f53\uff0c\u8be5\u53d8\u4f53\u901a\u8fc7\u589e\u5f3a\u7684\u7269\u7406\u4fe1\u606f\u53d8\u5206\u81ea\u7f16\u7801\u5668\u751f\u6210\u7269\u7406\u4e00\u81f4\u4e14\u53ef\u5b9e\u73b0\u7684\u76ee\u6807\uff0c\u5927\u5927\u63d0\u9ad8\u4e86\u76ee\u6807\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "\u7ecf\u8fc7\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7269\u7406\u4fe1\u606f\u5f15\u5bfc\u7684\u76ee\u6807\u751f\u6210\u663e\u8457\u63d0\u9ad8\u4e86\u63d0\u51fa\u76ee\u6807\u7684\u8d28\u91cf\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u5728\u5404\u79cd\u89c6\u89c9\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u63a2\u7d22\u6548\u679c\u548c\u6280\u80fd\u83b7\u53d6\u6548\u7387\u3002"}}
{"id": "2511.06749", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.06749", "abs": "https://arxiv.org/abs/2511.06749", "authors": ["Weining Lu", "Deer Bin", "Lian Ma", "Ming Ma", "Zhihao Ma", "Xiangyang Chen", "Longfei Wang", "Yixiao Feng", "Zhouxian Jiang", "Yongliang Shi", "Bin Liang"], "title": "Semi-distributed Cross-modal Air-Ground Relative Localization", "comment": "7 pages, 3 figures. Accepted by IROS 2025", "summary": "Efficient, accurate, and flexible relative localization is crucial in air-ground collaborative tasks. However, current approaches for robot relative localization are primarily realized in the form of distributed multi-robot SLAM systems with the same sensor configuration, which are tightly coupled with the state estimation of all robots, limiting both flexibility and accuracy. To this end, we fully leverage the high capacity of Unmanned Ground Vehicle (UGV) to integrate multiple sensors, enabling a semi-distributed cross-modal air-ground relative localization framework. In this work, both the UGV and the Unmanned Aerial Vehicle (UAV) independently perform SLAM while extracting deep learning-based keypoints and global descriptors, which decouples the relative localization from the state estimation of all agents. The UGV employs a local Bundle Adjustment (BA) with LiDAR, camera, and an IMU to rapidly obtain accurate relative pose estimates. The BA process adopts sparse keypoint optimization and is divided into two stages: First, optimizing camera poses interpolated from LiDAR-Inertial Odometry (LIO), followed by estimating the relative camera poses between the UGV and UAV. Additionally, we implement an incremental loop closure detection algorithm using deep learning-based descriptors to maintain and retrieve keyframes efficiently. Experimental results demonstrate that our method achieves outstanding performance in both accuracy and efficiency. Unlike traditional multi-robot SLAM approaches that transmit images or point clouds, our method only transmits keypoint pixels and their descriptors, effectively constraining the communication bandwidth under 0.3 Mbps. Codes and data will be publicly available on https://github.com/Ascbpiac/cross-model-relative-localization.git.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6709\u6548\u7684\u534a\u5206\u5e03\u5f0f\u7a7a\u5730\u76f8\u5bf9\u5b9a\u4f4d\u6846\u67b6\uff0c\u5229\u7528UGV\u4e0eUAV\u540c\u65f6\u72ec\u7acb\u6267\u884cSLAM\uff0c\u80fd\u63d0\u9ad8\u7075\u6d3b\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u591a\u673a\u5668\u4ebaSLAM\u7cfb\u7edf\u53d7\u9650\u4e8e\u6240\u6709\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\u7684\u8026\u5408\uff0c\u5f71\u54cd\u7075\u6d3b\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "UGV\u548cUAV\u72ec\u7acb\u6267\u884cSLAM\uff0cUGV\u5229\u7528LiDAR\u3001\u76f8\u673a\u548cIMU\u8fdb\u884c\u5c40\u90e8\u675f\u8c03\u6574\uff0c\u4f18\u5316\u76f8\u673a\u59ff\u6001\u5e76\u4f30\u8ba1UGV\u4e0eUAV\u95f4\u7684\u76f8\u5bf9\u59ff\u6001\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u76f8\u5bf9\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u4ec5\u4f20\u8f93\u5173\u952e\u70b9\u50cf\u7d20\u53ca\u5176\u63cf\u8ff0\u7b26\uff0c\u6709\u6548\u63a7\u5236\u901a\u4fe1\u5e26\u5bbd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u7ea6\u675f\u901a\u4fe1\u5e26\u5bbd\uff0c\u9002\u7528\u4e8e\u7a7a\u5730\u534f\u4f5c\u4efb\u52a1\u3002"}}
{"id": "2511.06754", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.06754", "abs": "https://arxiv.org/abs/2511.06754", "authors": ["Taisei Hanyu", "Nhat Chung", "Huy Le", "Toan Nguyen", "Yuki Ikebe", "Anthony Gunderman", "Duy Nguyen Ho Minh", "Khoa Vo", "Tung Kieu", "Kashu Yamazaki", "Chase Rainwater", "Anh Nguyen", "Ngan Le"], "title": "SlotVLA: Towards Modeling of Object-Relation Representations in Robotic Manipulation", "comment": "under review", "summary": "Inspired by how humans reason over discrete objects and their relationships, we explore whether compact object-centric and object-relation representations can form a foundation for multitask robotic manipulation. Most existing robotic multitask models rely on dense embeddings that entangle both object and background cues, raising concerns about both efficiency and interpretability. In contrast, we study object-relation-centric representations as a pathway to more structured, efficient, and explainable visuomotor control. Our contributions are two-fold. First, we introduce LIBERO+, a fine-grained benchmark dataset designed to enable and evaluate object-relation reasoning in robotic manipulation. Unlike prior datasets, LIBERO+ provides object-centric annotations that enrich demonstrations with box- and mask-level labels as well as instance-level temporal tracking, supporting compact and interpretable visuomotor representations. Second, we propose SlotVLA, a slot-attention-based framework that captures both objects and their relations for action decoding. It uses a slot-based visual tokenizer to maintain consistent temporal object representations, a relation-centric decoder to produce task-relevant embeddings, and an LLM-driven module that translates these embeddings into executable actions. Experiments on LIBERO+ demonstrate that object-centric slot and object-relation slot representations drastically reduce the number of required visual tokens, while providing competitive generalization. Together, LIBERO+ and SlotVLA provide a compact, interpretable, and effective foundation for advancing object-relation-centric robotic manipulation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faLIBERO+\u6570\u636e\u96c6\u548cSlotVLA\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u8c61-\u5173\u7cfb\u4e2d\u5fc3\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u63a2\u8ba8\u7d27\u51d1\u3001\u5bf9\u8c61\u4e2d\u5fc3\u548c\u5bf9\u8c61-\u5173\u7cfb\u8868\u793a\u5728\u591a\u4efb\u52a1\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u6f5c\u529b\uff0c\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u7684\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\u3002", "method": "\u7814\u7a76\u5bf9\u8c61-\u5173\u7cfb\u4e2d\u5fc3\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86LIBERO+\u57fa\u51c6\u6570\u636e\u96c6\u4e0eSlotVLA\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u8c61\u4e2d\u5fc3\u548c\u5bf9\u8c61-\u5173\u7cfb\u4ee3\u8868\u663e\u8457\u51cf\u5c11\u4e86\u6240\u9700\u7684\u89c6\u89c9\u6807\u8bb0\u6570\u91cf\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u7ade\u4e89\u6027\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "LIBERO+\u548cSlotVLA\u4e3a\u57fa\u4e8e\u5bf9\u8c61\u5173\u7cfb\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u7d27\u51d1\u3001\u53ef\u89e3\u91ca\u548c\u6709\u6548\u7684\u57fa\u7840\uff0c\u80fd\u591f\u63d0\u5347\u673a\u5668\u4eba\u7684\u64cd\u63a7\u80fd\u529b\u3002"}}
{"id": "2511.06796", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06796", "abs": "https://arxiv.org/abs/2511.06796", "authors": ["MD-Nazmus Sunbeam"], "title": "Human-Level Actuation for Humanoids", "comment": "61 pages, 8 figures, 7 tables, and 12 numbered equations", "summary": "Claims that humanoid robots achieve ``human-level'' actuation are common but rarely quantified. Peak torque or speed specifications tell us little about whether a joint can deliver the right combination of torque, power, and endurance at task-relevant postures and rates. We introduce a comprehensive framework that makes ``human-level'' measurable and comparable across systems. Our approach has three components. First, a kinematic \\emph{DoF atlas} standardizes joint coordinate systems and ranges of motion using ISB-based conventions, ensuring that human and robot joints are compared in the same reference frames. Second, \\emph{Human-Equivalence Envelopes (HEE)} define per-joint requirements by measuring whether a robot meets human torque \\emph{and} power simultaneously at the same joint angle and rate $(q,\u03c9)$, weighted by positive mechanical work in task-specific bands (walking, stairs, lifting, reaching, and hand actions). Third, the \\emph{Human-Level Actuation Score (HLAS)} aggregates six physically grounded factors: workspace coverage (ROM and DoF), HEE coverage, torque-mode bandwidth, efficiency, and thermal sustainability. We provide detailed measurement protocols using dynamometry, electrical power monitoring, and thermal testing that yield every HLAS input from reproducible experiments. A worked example demonstrates HLAS computation for a multi-joint humanoid, showing how the score exposes actuator trade-offs (gearing ratio versus bandwidth and efficiency) that peak-torque specifications obscure. The framework serves as both a design specification for humanoid development and a benchmarking standard for comparing actuation systems, with all components grounded in published human biomechanics data.", "AI": {"tldr": "\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u4f7f\u5f97\u7c7b\u4eba\u673a\u5668\u4eba\u5728\u6fc0\u6d3b\u80fd\u529b\u4e0a\u8fbe\u6210\u201c\u4eba\u7c7b\u6c34\u5e73\u201d\u7684\u91cf\u5316\u548c\u53ef\u6bd4\u5206\u6790\u3002", "motivation": "\u5f53\u524d\u5173\u4e8e\u7c7b\u4eba\u673a\u5668\u4eba\u7684\u201c\u4eba\u7c7b\u6c34\u5e73\u201d\u6fc0\u6d3b\u80fd\u529b\u7684\u58f0\u660e\u591a\u4e3a\u5b9a\u6027\uff0c\u7f3a\u4e4f\u5b9e\u9645\u7684\u5b9a\u91cf\u5206\u6790\u548c\u6bd4\u8f83\u3002\u8fd9\u4e00\u7814\u7a76\u5e0c\u671b\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u8fd0\u52a8\u5b66\u81ea\u7531\u5ea6\u56fe\u8c31\u3001\u5b9a\u4e49\u4eba\u7c7b\u7b49\u6548\u5305\u7edc\u548c\u8ba1\u7b97\u6fc0\u6d3b\u8bc4\u5206\uff0c\u7ed3\u5408\u52a8\u529b\u5b66\u3001\u529f\u7387\u76d1\u6d4b\u548c\u70ed\u6d4b\u8bd5\u7b49\u65b9\u6cd5\u8fdb\u884c\u8be6\u7ec6\u6d4b\u91cf\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7efc\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u548c\u6bd4\u8f83\u7c7b\u4eba\u673a\u5668\u4eba\u5728\u201c\u4eba\u7c7b\u6c34\u5e73\u201d\u6fc0\u6d3b\u65b9\u9762\u7684\u80fd\u529b\u3002\u901a\u8fc7\u5efa\u7acb\u8fd0\u52a8\u5b66\u81ea\u7531\u5ea6\uff08DoF\uff09\u56fe\u8c31\u3001\u5b9a\u4e49\u4eba\u7c7b\u7b49\u6548\u5305\u7edc\uff08HEE\uff09\u548c\u8ba1\u7b97\u4eba\u7c7b\u6c34\u5e73\u6fc0\u6d3b\u8bc4\u5206\uff08HLAS\uff09\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u660e\u786e\u673a\u5668\u4eba\u5728\u6267\u884c\u7279\u5b9a\u52a8\u4f5c\u65f6\u7684\u626d\u77e9\u3001\u529f\u7387\u548c\u8010\u529b\u8981\u6c42\uff0c\u5e76\u901a\u8fc7\u4e00\u7cfb\u5217\u5b9e\u9a8c\u63d0\u4f9b\u53ef\u91cd\u590d\u7684\u6d4b\u91cf\u534f\u8bae\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7c7b\u4eba\u673a\u5668\u4eba\u7684\u8bbe\u8ba1\u89c4\u8303\u548c\u6fc0\u6d3b\u7cfb\u7edf\u7684\u6bd4\u8f83\u6807\u51c6\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u8fdb\u4e00\u6b65\u7684\u53d1\u5c55\u548c\u4f18\u5316\u3002"}}
{"id": "2511.06801", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06801", "abs": "https://arxiv.org/abs/2511.06801", "authors": ["Praveen Kumar", "Tushar Sandhan"], "title": "Vision-Aided Online A* Path Planning for Efficient and Safe Navigation of Service Robots", "comment": "10 pages", "summary": "The deployment of autonomous service robots in human-centric environments is hindered by a critical gap in perception and planning. Traditional navigation systems rely on expensive LiDARs that, while geometrically precise, are seman- tically unaware, they cannot distinguish a important document on an office floor from a harmless piece of litter, treating both as physically traversable. While advanced semantic segmentation exists, no prior work has successfully integrated this visual intelligence into a real-time path planner that is efficient enough for low-cost, embedded hardware. This paper presents a frame- work to bridge this gap, delivering context-aware navigation on an affordable robotic platform. Our approach centers on a novel, tight integration of a lightweight perception module with an online A* planner. The perception system employs a semantic segmentation model to identify user-defined visual constraints, enabling the robot to navigate based on contextual importance rather than physical size alone. This adaptability allows an operator to define what is critical for a given task, be it sensitive papers in an office or safety lines in a factory, thus resolving the ambiguity of what to avoid. This semantic perception is seamlessly fused with geometric data. The identified visual constraints are projected as non-geometric obstacles onto a global map that is continuously updated from sensor data, enabling robust navigation through both partially known and unknown environments. We validate our framework through extensive experiments in high-fidelity simulations and on a real-world robotic platform. The results demonstrate robust, real-time performance, proving that a cost- effective robot can safely navigate complex environments while respecting critical visual cues invisible to traditional planners.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u81ea\u4e3b\u670d\u52a1\u673a\u5668\u4eba\u5728\u590d\u6742\u4eba\u7c7b\u73af\u5883\u4e2d\u5bfc\u822a\u7684\u611f\u77e5\u4e0e\u89c4\u5212\u96be\u9898.", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5bfc\u822a\u7cfb\u7edf\u7f3a\u4e4f\u8bed\u4e49\u611f\u77e5\u95ee\u9898\uff0c\u63d0\u5347\u81ea\u4e3b\u670d\u52a1\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u4e0e\u5bfc\u822a\u80fd\u529b.", "method": "\u901a\u8fc7\u5c06\u8f7b\u91cf\u7ea7\u611f\u77e5\u6a21\u5757\u4e0e\u5728\u7ebfA*\u89c4\u5212\u5668\u7d27\u5bc6\u96c6\u6210\uff0c\u4f7f\u7528\u8bed\u4e49\u5206\u5272\u6a21\u578b\u8bc6\u522b\u7528\u6237\u5b9a\u4e49\u7684\u89c6\u89c9\u7ea6\u675f\uff0c\u8fdb\u800c\u6307\u5bfc\u673a\u5668\u4eba\u5bfc\u822a.", "result": "\u9a8c\u8bc1\u8868\u660e\u8be5\u6846\u67b6\u5728\u9ad8\u4fdd\u771f\u4eff\u771f\u548c\u5b9e\u9645\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u7684\u5b9e\u65f6\u8868\u73b0\u5f3a\u5927\uff0c\u80fd\u6709\u6548\u5e94\u5bf9\u590d\u6742\u73af\u5883\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u8be5\u6846\u67b6\u7684\u673a\u5668\u4eba\u80fd\u591f\u5b89\u5168\u9ad8\u6548\u5730\u5e94\u5bf9\u590d\u6742\u73af\u5883\u4e2d\u7684\u89c6\u89c9\u7ebf\u7d22\uff0c\u800c\u4e0d\u4f9d\u8d56\u4f20\u7edf\u89c4\u5212\u65b9\u6cd5."}}
{"id": "2511.06839", "categories": ["cs.RO", "cs.CV", "eess.SY", "math.DS"], "pdf": "https://arxiv.org/pdf/2511.06839", "abs": "https://arxiv.org/abs/2511.06839", "authors": ["Selim Ahmet Iz", "Mustafa Unel"], "title": "Vision-Based System Identification of a Quadrotor", "comment": null, "summary": "This paper explores the application of vision-based system identification techniques in quadrotor modeling and control. Through experiments and analysis, we address the complexities and limitations of quadrotor modeling, particularly in relation to thrust and drag coefficients. Grey-box modeling is employed to mitigate uncertainties, and the effectiveness of an onboard vision system is evaluated. An LQR controller is designed based on a system identification model using data from the onboard vision system. The results demonstrate consistent performance between the models, validating the efficacy of vision based system identification. This study highlights the potential of vision-based techniques in enhancing quadrotor modeling and control, contributing to improved performance and operational capabilities. Our findings provide insights into the usability and consistency of these techniques, paving the way for future research in quadrotor performance enhancement, fault detection, and decision-making processes.", "AI": {"tldr": "\u672c\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u89c6\u89c9\u7684\u7cfb\u7edf\u8bc6\u522b\u6280\u672f\u5728\u56db\u65cb\u7ffc\u6a21\u578b\u548c\u63a7\u5236\u4e2d\u7684\u5e94\u7528\uff0c\u7ed3\u679c\u663e\u793a\u8fd9\u4e9b\u6280\u672f\u80fd\u591f\u63d0\u5347\u56db\u65cb\u7ffc\u7684\u5efa\u6a21\u7cbe\u5ea6\u548c\u63a7\u5236\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u672a\u6765\u7814\u7a76\u7684\u6f5c\u529b\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u56db\u65cb\u7ffc\u5efa\u6a21\u4e2d\u590d\u6742\u6027\u548c\u9650\u5236\uff0c\u5c24\u5176\u662f\u63a8\u529b\u548c\u963b\u529b\u7cfb\u6570\u65b9\u9762\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7070\u7bb1\u5efa\u6a21\u6280\u672f\uff0c\u7ed3\u5408\u4ece\u673a\u8f7d\u89c6\u89c9\u7cfb\u7edf\u83b7\u53d6\u7684\u6570\u636e\uff0c\u8bbe\u8ba1\u4e86LQR\u63a7\u5236\u5668\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u548c\u5206\u6790\uff0c\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u89c6\u89c9\u7684\u7cfb\u7edf\u8bc6\u522b\u7684\u6709\u6548\u6027\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u6a21\u578b\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u663e\u793a\u4e86\u57fa\u4e8e\u89c6\u89c9\u7684\u7cfb\u7edf\u8bc6\u522b\u6280\u672f\u5728\u56db\u65cb\u7ffc\u5efa\u6a21\u548c\u63a7\u5236\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7a81\u663e\u4e86\u5176\u5728\u63d0\u5347\u56db\u65cb\u7ffc\u6027\u80fd\u548c\u64cd\u4f5c\u80fd\u529b\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.06892", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06892", "abs": "https://arxiv.org/abs/2511.06892", "authors": ["Kailin Tong", "Selim Solmaz", "Kenan Mujkic", "Gottfried Allmer", "Bo Leng"], "title": "Multi-Agent AI Framework for Road Situation Detection and C-ITS Message Generation", "comment": "submitted to TRA 2026", "summary": "Conventional road-situation detection methods achieve strong performance in predefined scenarios but fail in unseen cases and lack semantic interpretation, which is crucial for reliable traffic recommendations. This work introduces a multi-agent AI framework that combines multimodal large language models (MLLMs) with vision-based perception for road-situation monitoring. The framework processes camera feeds and coordinates dedicated agents for situation detection, distance estimation, decision-making, and Cooperative Intelligent Transport System (C-ITS) message generation. Evaluation is conducted on a custom dataset of 103 images extracted from 20 videos of the TAD dataset. Both Gemini-2.0-Flash and Gemini-2.5-Flash were evaluated. The results show 100\\% recall in situation detection and perfect message schema correctness; however, both models suffer from false-positive detections and have reduced performance in terms of number of lanes, driving lane status and cause code. Surprisingly, Gemini-2.5-Flash, though more capable in general tasks, underperforms Gemini-2.0-Flash in detection accuracy and semantic understanding and incurs higher latency (Table II). These findings motivate further work on fine-tuning specialized LLMs or MLLMs tailored for intelligent transportation applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u89c6\u89c9\u611f\u77e5\u7684\u591a\u667a\u80fd\u4f53AI\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u9053\u8def\u60c5\u51b5\u76d1\u6d4b\uff0c\u5c3d\u7ba1\u5728\u68c0\u6d4b\u53ec\u56de\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4ecd\u5b58\u5728\u8bef\u62a5\u548c\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u6307\u51fa\u4e86\u5bf9\u4e13\u95e8\u9886\u57df\u6a21\u578b\u5fae\u8c03\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u9053\u8def\u60c5\u51b5\u68c0\u6d4b\u65b9\u6cd5\u5728\u9884\u5b9a\u4e49\u573a\u666f\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u672a\u89c1\u8fc7\u7684\u6848\u4f8b\u4e2d\u5bb9\u6613\u5931\u8d25\uff0c\u4e14\u7f3a\u4e4f\u8bed\u4e49\u89e3\u91ca\uff0c\u8fd9\u5bf9\u53ef\u9760\u7684\u4ea4\u901a\u63a8\u8350\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53AI\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u57fa\u4e8e\u89c6\u89c9\u7684\u611f\u77e5\uff0c\u7528\u4e8e\u9053\u8def\u60c5\u51b5\u76d1\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u60c5\u51b5\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u4e86100%\u7684\u53ec\u56de\u7387\uff0c\u6d88\u606f\u683c\u5f0f\u6b63\u786e\u6027\u5b8c\u7f8e\uff0c\u4f46\u6a21\u578b\u5b58\u5728\u8bef\u62a5\u4e14\u5728\u8f66\u9053\u6570\u91cf\u3001\u884c\u9a76\u8f66\u9053\u72b6\u6001\u548c\u539f\u56e0\u4ee3\u7801\u65b9\u9762\u7684\u8868\u73b0\u4e0b\u964d\u3002Gemini-2.5-Flash\u5c3d\u7ba1\u5728\u4e00\u822c\u4efb\u52a1\u4e0a\u66f4\u5f3a\uff0c\u4f46\u5728\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u4e14\u5ef6\u8fdf\u66f4\u9ad8\u3002", "conclusion": "\u8be5\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u9700\u8981\u5bf9\u7279\u5b9a\u9886\u57df\u7684LLM\u6216MLLM\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u63d0\u5347\u667a\u80fd\u4ea4\u901a\u5e94\u7528\u7684\u8868\u73b0\u3002"}}
{"id": "2511.06919", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06919", "abs": "https://arxiv.org/abs/2511.06919", "authors": ["Luis Diener", "Jens Kalkkuhl", "Markus Enzweiler"], "title": "Integration of Visual SLAM into Consumer-Grade Automotive Localization", "comment": "This manuscript has been submitted to the IEEE for possible publication", "summary": "Accurate ego-motion estimation in consumer-grade vehicles currently relies on proprioceptive sensors, i.e. wheel odometry and IMUs, whose performance is limited by systematic errors and calibration. While visual-inertial SLAM has become a standard in robotics, its integration into automotive ego-motion estimation remains largely unexplored. This paper investigates how visual SLAM can be integrated into consumer-grade vehicle localization systems to improve performance. We propose a framework that fuses visual SLAM with a lateral vehicle dynamics model to achieve online gyroscope calibration under realistic driving conditions. Experimental results demonstrate that vision-based integration significantly improves gyroscope calibration accuracy and thus enhances overall localization performance, highlighting a promising path toward higher automotive localization accuracy. We provide results on both proprietary and public datasets, showing improved performance and superior localization accuracy on a public benchmark compared to state-of-the-art methods.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u5c06\u89c6\u89c9SLAM\u5e94\u7528\u4e8e\u6d88\u8d39\u7ea7\u8f66\u8f86\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u4ee5\u6539\u5584\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u9640\u87ba\u4eea\u6821\u51c6\u548c\u6574\u4f53\u5b9a\u4f4d\u6027\u80fd\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u63d0\u5347\u6d88\u8d39\u7ea7\u8f66\u8f86\u7684\u81ea\u6211\u5b9a\u4f4d\u51c6\u786e\u6027\uff0c\u514b\u670d\u73b0\u6709\u4f20\u611f\u5668\u7684\u7cfb\u7edf\u8bef\u5dee\u548c\u6821\u51c6\u95ee\u9898\u3002", "method": "\u5c06\u89c6\u89c9SLAM\u4e0e\u6a2a\u5411\u8f66\u8f86\u52a8\u529b\u5b66\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u8fdb\u884c\u5728\u7ebf\u9640\u87ba\u4eea\u6821\u51c6\u3002", "result": "\u89c6\u89c9SLAM\u7684\u96c6\u6210\u663e\u8457\u63d0\u9ad8\u4e86\u9640\u87ba\u4eea\u6821\u51c6\u7684\u51c6\u786e\u6027\uff0c\u4ece\u800c\u589e\u5f3a\u6574\u4f53\u5b9a\u4f4d\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u878d\u5408\u89c6\u89c9SLAM\u4e0e\u8f66\u8f86\u52a8\u529b\u5b66\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u4e3a\u63d0\u9ad8\u6c7d\u8f66\u5b9a\u4f4d\u51c6\u786e\u6027\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.06998", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.06998", "abs": "https://arxiv.org/abs/2511.06998", "authors": ["Jin Huang", "Yingqiang Wang", "Ying Chen"], "title": "Raspi$^2$USBL: An open-source Raspberry Pi-Based Passive Inverted Ultra-Short Baseline Positioning System for Underwater Robotics", "comment": null, "summary": "Precise underwater positioning remains a fundamental challenge for underwater robotics since global navigation satellite system (GNSS) signals cannot penetrate the sea surface. This paper presents Raspi$^2$USBL, an open-source, Raspberry Pi-based passive inverted ultra-short baseline (piUSBL) positioning system designed to provide a low-cost and accessible solution for underwater robotic research. The system comprises a passive acoustic receiver and an active beacon. The receiver adopts a modular hardware architecture that integrates a hydrophone array, a multichannel preamplifier, an oven-controlled crystal oscillator (OCXO), a Raspberry Pi 5, and an MCC-series data acquisition (DAQ) board. Apart from the Pi 5, OCXO, and MCC board, the beacon comprises an impedance-matching network, a power amplifier, and a transmitting transducer. An open-source C++ software framework provides high-precision clock synchronization and triggering for one-way travel-time (OWTT) messaging, while performing real-time signal processing, including matched filtering, array beamforming, and adaptive gain control, to estimate the time of flight (TOF) and direction of arrival (DOA) of received signals. The Raspi$^2$USBL system was experimentally validated in an anechoic tank, freshwater lake, and open-sea trials. Results demonstrate a slant-range accuracy better than 0.1%, a bearing accuracy within 0.1$^\\circ$, and stable performance over operational distances up to 1.3 km. These findings confirm that low-cost, reproducible hardware can deliver research-grade underwater positioning accuracy. By releasing both the hardware and software as open-source, Raspi$^2$USBL provides a unified reference platform that lowers the entry barrier for underwater robotics laboratories, fosters reproducibility, and promotes collaborative innovation in underwater acoustic navigation and swarm robotics.", "AI": {"tldr": "Raspi$^2$USBL\u662f\u4e00\u4e2a\u5f00\u6e90\u7684Raspberry Pi\u57fa\u7840\u7684\u6c34\u4e0b\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u5b9e\u60e0\u4e14\u9ad8\u7cbe\u5ea6\u7684\u6c34\u4e0b\u5b9a\u4f4d\u65b9\u6848\u3002", "motivation": "\u7cbe\u786e\u7684\u6c34\u4e0b\u5b9a\u4f4d\u662f\u6c34\u4e0b\u673a\u5668\u4eba\u9762\u4e34\u7684\u57fa\u672c\u6311\u6218\uff0c\u56e0\u4e3a\u5168\u7403\u5bfc\u822a\u536b\u661f\u7cfb\u7edf\uff08GNSS\uff09\u4fe1\u53f7\u65e0\u6cd5\u7a7f\u900f\u6d77\u9762\u3002", "method": "\u7cfb\u7edf\u5305\u62ec\u4e00\u4e2a\u88ab\u52a8\u58f0\u7eb3\u63a5\u6536\u5668\u548c\u4e00\u4e2a\u4e3b\u52a8\u4fe1\u6807\uff0c\u7ed3\u5408\u6a21\u5757\u5316\u786c\u4ef6\u67b6\u6784\u548c\u5f00\u6e90\u8f6f\u4ef6\u6846\u67b6\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u65f6\u95f4\u540c\u6b65\u548c\u4fe1\u53f7\u5904\u7406\u3002", "result": "\u8be5\u7cfb\u7edf\u5728\u6c34\u4e0b\u5b9a\u4f4d\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u4f9b\u4e86\u4f4e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "Raspi$^2$USBL\u901a\u8fc7\u5f00\u6e90\u786c\u4ef6\u548c\u8f6f\u4ef6\uff0c\u964d\u4f4e\u4e86\u6c34\u4e0b\u673a\u5668\u4eba\u5b9e\u9a8c\u5ba4\u7684\u5165\u95e8\u969c\u788d\uff0c\u4fc3\u8fdb\u4e86\u53ef\u91cd\u590d\u6027\u548c\u534f\u4f5c\u521b\u65b0\u3002"}}
{"id": "2511.07081", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.07081", "abs": "https://arxiv.org/abs/2511.07081", "authors": ["Guanghu Xie", "Mingxu Li", "Songwei Wu", "Yang Liu", "Zongwu Xie", "Baoshi Cao", "Hong Liu"], "title": "HDCNet: A Hybrid Depth Completion Network for Grasping Transparent and Reflective Objects", "comment": null, "summary": "Depth perception of transparent and reflective objects has long been a critical challenge in robotic manipulation.Conventional depth sensors often fail to provide reliable measurements on such surfaces, limiting the performance of robots in perception and grasping tasks. To address this issue, we propose a novel depth completion network,HDCNet,which integrates the complementary strengths of Transformer,CNN and Mamba architectures.Specifically,the encoder is designed as a dual-branch Transformer-CNN framework to extract modality-specific features. At the shallow layers of the encoder, we introduce a lightweight multimodal fusion module to effectively integrate low-level features. At the network bottleneck,a Transformer-Mamba hybrid fusion module is developed to achieve deep integration of high-level semantic and global contextual information, significantly enhancing depth completion accuracy and robustness. Extensive evaluations on multiple public datasets demonstrate that HDCNet achieves state-of-the-art(SOTA) performance in depth completion tasks.Furthermore,robotic grasping experiments show that HDCNet substantially improves grasp success rates for transparent and reflective objects,achieving up to a 60% increase.", "AI": {"tldr": "\u63d0\u51faHDCNet\uff0c\u89e3\u51b3\u900f\u660e\u548c\u53cd\u5c04\u7269\u4f53\u7684\u6df1\u5ea6\u611f\u77e5\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u6df1\u5ea6\u5b8c\u6210\u548c\u6293\u53d6\u6210\u529f\u7387\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u4f20\u611f\u5668\u5728\u900f\u660e\u548c\u53cd\u5c04\u8868\u9762\u4e0a\u7684\u53ef\u9760\u6027\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u5728\u611f\u77e5\u548c\u6293\u53d6\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u6df1\u5ea6\u5b8c\u6210\u7f51\u7edcHDCNet\uff0c\u878d\u5408\u4e86Transformer\u3001CNN\u548cMamba\u67b6\u6784\uff0c\u91c7\u7528\u53cc\u5206\u652fTransformer-CNN\u6846\u67b6\u548c\u6df7\u5408\u878d\u5408\u6a21\u5757\u3002", "result": "HDCNet\u5728\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51faSOTA\u7684\u6df1\u5ea6\u5b8c\u6210\u6027\u80fd\uff0c\u5e76\u5728\u6293\u53d6\u5b9e\u9a8c\u4e2d\u63d0\u9ad8\u4e86\u6210\u529f\u7387\uff0c\u6700\u591a\u589e\u52a060%\u3002", "conclusion": "HDCNet\u663e\u8457\u63d0\u9ad8\u4e86\u900f\u660e\u548c\u53cd\u5c04\u7269\u4f53\u7684\u6df1\u5ea6\u5b8c\u6210\u7cbe\u5ea6\u548c\u673a\u5668\u4eba\u6293\u53d6\u6210\u529f\u7387\u3002"}}
{"id": "2511.07155", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.07155", "abs": "https://arxiv.org/abs/2511.07155", "authors": ["Thomas Steinecker", "Alexander Bienemann", "Denis Trescher", "Thorsten Luettel", "Mirko Maehlisch"], "title": "Dynamics-Decoupled Trajectory Alignment for Sim-to-Real Transfer in Reinforcement Learning for Autonomous Driving", "comment": null, "summary": "Reinforcement learning (RL) has shown promise in robotics, but deploying RL on real vehicles remains challenging due to the complexity of vehicle dynamics and the mismatch between simulation and reality. Factors such as tire characteristics, road surface conditions, aerodynamic disturbances, and vehicle load make it infeasible to model real-world dynamics accurately, which hinders direct transfer of RL agents trained in simulation. In this paper, we present a framework that decouples motion planning from vehicle control through a spatial and temporal alignment strategy between a virtual vehicle and the real system. An RL agent is first trained in simulation using a kinematic bicycle model to output continuous control actions. Its behavior is then distilled into a trajectory-predicting agent that generates finite-horizon ego-vehicle trajectories, enabling synchronization between virtual and real vehicles. At deployment, a Stanley controller governs lateral dynamics, while longitudinal alignment is maintained through adaptive update mechanisms that compensate for deviations between virtual and real trajectories. We validate our approach on a real vehicle and demonstrate that the proposed alignment strategy enables robust zero-shot transfer of RL-based motion planning from simulation to reality, successfully decoupling high-level trajectory generation from low-level vehicle control.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u7a7a\u95f4\u548c\u65f6\u95f4\u5bf9\u9f50\u7b56\u7565\u7684\u6846\u67b6\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u771f\u5b9e\u8f66\u8f86\u4e0a\u7684\u5e94\u7528\uff0c\u89e3\u51b3\u4e86\u6a21\u62df\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u7684\u8f6c\u79fb\u6311\u6218\u3002", "motivation": "\u867d\u7136\u5f3a\u5f3a\u5316\u5b66\u4e60\u5728\u673a\u5668\u4eba\u9886\u57df\u6709\u6f5c\u529b\uff0c\u4f46\u7531\u4e8e\u8f66\u8f86\u52a8\u529b\u5b66\u590d\u6742\u6027\u548c\u6a21\u62df\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\uff0c\u5b9e\u9645\u5e94\u7528\u4ecd\u7136\u9762\u4e34\u6311\u6218\u3002", "method": "\u901a\u8fc7\u91c7\u7528\u8fd0\u52a8\u89c4\u5212\u4e0e\u63a7\u5236\u89e3\u8026\u7684\u6846\u67b6\uff0c\u8bad\u7ec3RL\u4ee3\u7406\u751f\u6210\u63a7\u5236\u52a8\u4f5c\uff0c\u5e76\u5728\u771f\u5b9e\u8f66\u8f86\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u7a7a\u95f4\u548c\u65f6\u95f4\u5bf9\u9f50\u7b56\u7565\u5c06\u8fd0\u52a8\u89c4\u5212\u4e0e\u8f66\u8f86\u63a7\u5236\u89e3\u8026\u7684\u6846\u67b6\uff0c\u8fdb\u800c\u5b9e\u73b0\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u771f\u5b9e\u8f66\u8f86\u4e0a\u7684\u5e94\u7528\u3002\u901a\u8fc7\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8bad\u7ec3RL\u4ee3\u7406\u5e76\u63d0\u53d6\u5176\u884c\u4e3a\uff0c\u53ef\u4ee5\u5b9e\u73b0\u865a\u62df\u8f66\u8f86\u4e0e\u771f\u5b9e\u7cfb\u7edf\u7684\u540c\u6b65\uff0c\u5e76\u6700\u7ec8\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u8f66\u8f86\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u5bf9\u9f50\u7b56\u7565\u4fc3\u6210\u4e86\u57fa\u4e8eRL\u7684\u8fd0\u52a8\u89c4\u5212\u4ece\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u65e0\u7f1d\u8f6c\u79fb\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u5c42\u8f68\u8ff9\u751f\u6210\u4e0e\u4f4e\u5c42\u8f66\u8f86\u63a7\u5236\u7684\u89e3\u8026\u3002"}}
{"id": "2511.07175", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.07175", "abs": "https://arxiv.org/abs/2511.07175", "authors": ["Marvin R\u00fcdt", "Constantin Enke", "Kai Furmans"], "title": "Automated Generation of Continuous-Space Roadmaps for Routing Mobile Robot Fleets", "comment": "submitted to the IEEE for possible publication; 8 pages, 6 figures, 2 tables", "summary": "Efficient routing of mobile robot fleets is crucial in intralogistics, where delays and deadlocks can substantially reduce system throughput. Roadmap design, specifying feasible transport routes, directly affects fleet coordination and computational performance. Existing approaches are either grid-based, compromising geometric precision, or continuous-space approaches that disregard practical constraints. This paper presents an automated roadmap generation approach that bridges this gap by operating in continuous-space, integrating station-to-station transport demand and enforcing minimum distance constraints for nodes and edges. By combining free space discretization, transport demand-driven $K$-shortest-path optimization, and path smoothing, the approach produces roadmaps tailored to intralogistics applications. Evaluation across multiple intralogistics use cases demonstrates that the proposed approach consistently outperforms established baselines (4-connected grid, 8-connected grid, and random sampling), achieving lower structural complexity, higher redundancy, and near-optimal path lengths, enabling efficient and robust routing of mobile robot fleets.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u8def\u7ebf\u56fe\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u79fb\u52a8\u673a\u5668\u4eba\u8f66\u961f\u5728\u5185\u90e8\u7269\u6d41\u4e2d\u7684\u6548\u7387\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u79fb\u52a8\u673a\u5668\u4eba\u8f66\u961f\u7684\u9ad8\u6548\u8def\u7531\u5bf9\u5185\u90e8\u7269\u6d41\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u51e0\u4f55\u7cbe\u5ea6\u548c\u5b9e\u9645\u7ea6\u675f\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u81ea\u7531\u7a7a\u95f4\u79bb\u6563\u5316\u3001\u9700\u6c42\u9a71\u52a8\u7684K\u6700\u77ed\u8def\u4f18\u5316\u548c\u8def\u5f84\u5e73\u6ed1\u6280\u672f\uff0c\u751f\u6210\u9002\u5408\u5185\u90e8\u7269\u6d41\u7684\u8def\u7ebf\u56fe\u3002", "result": "\u5728\u591a\u4e2a\u6848\u4f8b\u4e2d\uff0c\u6240\u63d0\u65b9\u6cd5\u76f8\u8f83\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u66f4\u4f4e\u7684\u7ed3\u6784\u590d\u6742\u6027\u3001\u66f4\u9ad8\u7684\u5197\u4f59\u6027\u548c\u63a5\u8fd1\u6700\u4f18\u7684\u8def\u5f84\u957f\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u5185\u90e8\u7269\u6d41\u7528\u4f8b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u4e14\u7a33\u5065\u7684\u79fb\u52a8\u673a\u5668\u4eba\u8def\u7531\u3002"}}
{"id": "2511.07292", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.07292", "abs": "https://arxiv.org/abs/2511.07292", "authors": ["Simon Gerstenecker", "Andreas Geiger", "Katrin Renz"], "title": "PlanT 2.0: Exposing Biases and Structural Flaws in Closed-Loop Driving", "comment": null, "summary": "Most recent work in autonomous driving has prioritized benchmark performance and methodological innovation over in-depth analysis of model failures, biases, and shortcut learning. This has led to incremental improvements without a deep understanding of the current failures. While it is straightforward to look at situations where the model fails, it is hard to understand the underlying reason. This motivates us to conduct a systematic study, where inputs to the model are perturbed and the predictions observed. We introduce PlanT 2.0, a lightweight, object-centric planning transformer designed for autonomous driving research in CARLA. The object-level representation enables controlled analysis, as the input can be easily perturbed (e.g., by changing the location or adding or removing certain objects), in contrast to sensor-based models. To tackle the scenarios newly introduced by the challenging CARLA Leaderboard 2.0, we introduce multiple upgrades to PlanT, achieving state-of-the-art performance on Longest6 v2, Bench2Drive, and the CARLA validation routes. Our analysis exposes insightful failures, such as a lack of scene understanding caused by low obstacle diversity, rigid expert behaviors leading to exploitable shortcuts, and overfitting to a fixed set of expert trajectories. Based on these findings, we argue for a shift toward data-centric development, with a focus on richer, more robust, and less biased datasets. We open-source our code and model at https://github.com/autonomousvision/plant2.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PlanT 2.0\uff0c\u7528\u4e8e\u6df1\u5165\u5206\u6790\u81ea\u4e3b\u9a7e\u9a76\u6a21\u578b\u7684\u5931\u6548\u548c\u504f\u89c1\uff0c\u5f3a\u8c03\u6570\u636e\u9a71\u52a8\u5f00\u53d1\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u4e4b\u524d\u7684\u5de5\u4f5c\u7f3a\u4e4f\u5bf9\u6a21\u578b\u5931\u6548\u3001\u504f\u89c1\u548c\u5feb\u6377\u5b66\u4e60\u7684\u6df1\u523b\u7406\u89e3\uff0c\u5bfc\u81f4\u53ea\u5173\u6ce8\u57fa\u51c6\u6027\u80fd\u548c\u65b9\u6cd5\u521b\u65b0\u3002", "method": "\u901a\u8fc7\u5bf9\u6a21\u578b\u8f93\u5165\u8fdb\u884c\u6270\u52a8\u5e76\u89c2\u5bdf\u9884\u6d4b\uff0c\u8fdb\u884c\u7cfb\u7edf\u6027\u7814\u7a76\u548c\u5206\u6790\u3002", "result": "\u5f15\u5165\u4e86PlanT 2.0\uff0c\u5e76\u5728CARLA Leaderboard 2.0\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u4e00\u4e9b\u91cd\u8981\u7684\u5931\u6548\uff0c\u4f8b\u5982\u5bf9\u4f4e\u969c\u788d\u7269\u591a\u6837\u6027\u7684\u573a\u666f\u7406\u89e3\u4e0d\u8db3\u548c\u5bf9\u56fa\u5b9a\u4e13\u5bb6\u8f68\u8ff9\u7684\u8fc7\u62df\u5408\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5bfb\u627e\u4e30\u5bcc\u3001\u5f3a\u5927\u4e14\u66f4\u5c11\u504f\u89c1\u7684\u6570\u636e\u96c6\u5bf9\u81ea\u4e3b\u9a7e\u9a76\u7684\u53d1\u5c55\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2511.07375", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.07375", "abs": "https://arxiv.org/abs/2511.07375", "authors": ["Shaohang Han", "Joris Verhagen", "Jana Tumova"], "title": "Exact Smooth Reformulations for Trajectory Optimization Under Signal Temporal Logic Specifications", "comment": null, "summary": "We study motion planning under Signal Temporal Logic (STL), a useful formalism for specifying spatial-temporal requirements. We pose STL synthesis as a trajectory optimization problem leveraging the STL robustness semantics. To obtain a differentiable problem without approximation error, we introduce an exact reformulation of the max and min operators. The resulting method is exact, smooth, and sound. We validate it in numerical simulations, demonstrating its practical performance.", "AI": {"tldr": "\u7814\u7a76\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91\u4e0b\u7684\u8fd0\u52a8\u89c4\u5212\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7cbe\u786e\u7684\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\u3002", "motivation": "\u5229\u7528STL\u7684\u9c81\u68d2\u6027\u8bed\u4e49\u4ee5\u6ee1\u8db3\u7a7a\u95f4-\u65f6\u95f4\u8981\u6c42\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u5bf9\u6700\u5927\u548c\u6700\u5c0f\u8fd0\u7b97\u7b26\u7684\u7cbe\u786e\u91cd\u6784\uff0c\u5c06STL\u5408\u6210\u89c6\u4e3a\u8f68\u8ff9\u4f18\u5316\u95ee\u9898\u3002", "result": "\u8be5\u65b9\u6cd5\u662f\u7cbe\u786e\u7684\u3001\u5e73\u6ed1\u7684\uff0c\u5177\u6709\u53ef\u9760\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6570\u503c\u6a21\u62df\u4e2d\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\u3002"}}
{"id": "2511.07381", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.07381", "abs": "https://arxiv.org/abs/2511.07381", "authors": ["Yizhe Zhu", "Zhang Ye", "Boce Hu", "Haibo Zhao", "Yu Qi", "Dian Wang", "Robert Platt"], "title": "Residual Rotation Correction using Tactile Equivariance", "comment": "8 pages", "summary": "Visuotactile policy learning augments vision-only policies with tactile input, facilitating contact-rich manipulation. However, the high cost of tactile data collection makes sample efficiency the key requirement for developing visuotactile policies. We present EquiTac, a framework that exploits the inherent SO(2) symmetry of in-hand object rotation to improve sample efficiency and generalization for visuotactile policy learning. EquiTac first reconstructs surface normals from raw RGB inputs of vision-based tactile sensors, so rotations of the normal vector field correspond to in-hand object rotations. An SO(2)-equivariant network then predicts a residual rotation action that augments a base visuomotor policy at test time, enabling real-time rotation correction without additional reorientation demonstrations. On a real robot, EquiTac accurately achieves robust zero-shot generalization to unseen in-hand orientations with very few training samples, where baselines fail even with more training data. To our knowledge, this is the first tactile learning method to explicitly encode tactile equivariance for policy learning, yielding a lightweight, symmetry-aware module that improves reliability in contact-rich tasks.", "AI": {"tldr": "EquiTac\u6846\u67b6\u63d0\u9ad8\u4e86\u89c6\u89c9\u89e6\u89c9\u7b56\u7565\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\u3002", "motivation": "\u53d7\u9650\u4e8e\u89e6\u89c9\u6570\u636e\u6536\u96c6\u7684\u9ad8\u6210\u672c\uff0c\u63d0\u5347\u6837\u672c\u6548\u7387\u6210\u4e3a\u89c6\u89c9\u89e6\u89c9\u7b56\u7565\u53d1\u5c55\u7684\u5173\u952e\u8981\u6c42\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u91cd\u5efa\u8868\u9762\u6cd5\u7ebf\u5e76\u5229\u7528SO(2)\u5bf9\u79f0\u6027\u8fdb\u884c\u7b56\u7565\u5b66\u4e60\uff0c\u4ee5\u63d0\u5347\u6837\u672c\u6548\u7387\u548c\u5f3a\u5316\u5b66\u4e60\u8868\u73b0\u3002", "result": "EquiTac\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u5bf9\u672a\u89c1\u624b\u4e2d\u65b9\u5411\u7684\u96f6-shot\u6cdb\u5316\uff0c\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "EquiTac\u5728\u5904\u7406\u672a\u77e5\u7684\u624b\u4e2d\u5bf9\u8c61\u65b9\u5411\u65f6\u8868\u73b0\u51fa\u8272\uff0c\u4e14\u4ec5\u9700\u5c11\u91cf\u8bad\u7ec3\u6837\u672c\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u7ebf\u3002"}}
{"id": "2511.07407", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.07407", "abs": "https://arxiv.org/abs/2511.07407", "authors": ["Zhengjie Xu", "Ye Li", "Kwan-yee Lin", "Stella X. Yu"], "title": "Unified Humanoid Fall-Safety Policy from a Few Demonstrations", "comment": null, "summary": "Falling is an inherent risk of humanoid mobility. Maintaining stability is thus a primary safety focus in robot control and learning, yet no existing approach fully averts loss of balance. When instability does occur, prior work addresses only isolated aspects of falling: avoiding falls, choreographing a controlled descent, or standing up afterward. Consequently, humanoid robots lack integrated strategies for impact mitigation and prompt recovery when real falls defy these scripts. We aim to go beyond keeping balance to make the entire fall-and-recovery process safe and autonomous: prevent falls when possible, reduce impact when unavoidable, and stand up when fallen. By fusing sparse human demonstrations with reinforcement learning and an adaptive diffusion-based memory of safe reactions, we learn adaptive whole-body behaviors that unify fall prevention, impact mitigation, and rapid recovery in one policy. Experiments in simulation and on a Unitree G1 demonstrate robust sim-to-real transfer, lower impact forces, and consistently fast recovery across diverse disturbances, pointing towards safer, more resilient humanoids in real environments. Videos are available at https://firm2025.github.io/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u65b9\u6cd5\uff0c\u4f7f\u4eba\u5f62\u673a\u5668\u4eba\u5728\u9762\u4e34\u6454\u5012\u65f6\u80fd\u591f\u5b89\u5168\u3001\u81ea\u4e3b\u5730\u9884\u9632\u6454\u5012\u3001\u51cf\u8f7b\u51b2\u51fb\u5e76\u8fc5\u901f\u6062\u590d\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u6548\u679c\u663e\u8457\u3002", "motivation": "\u786e\u4fdd\u4eba\u5f62\u673a\u5668\u4eba\u5728\u8fd0\u52a8\u8fc7\u7a0b\u4e2d\u7684\u5b89\u5168\uff0c\u5c24\u5176\u662f\u5728\u9762\u4e34\u5931\u53bb\u5e73\u8861\u548c\u6454\u5012\u7684\u98ce\u9669\u65f6\uff0c\u63d0\u51fa\u4e00\u79cd\u7efc\u5408\u7684\u7b56\u7565\u6765\u5e94\u5bf9\u6454\u5012\u548c\u6062\u590d\u7684\u6574\u4e2a\u8fc7\u7a0b\u3002", "method": "\u878d\u5408\u4eba\u7c7b\u793a\u8303\u4e0e\u5f3a\u5316\u5b66\u4e60\uff0c\u5229\u7528\u9002\u5e94\u6027\u6269\u6563\u8bb0\u5fc6\u5b66\u4e60\u5168\u8eab\u884c\u4e3a\uff0c\u5f62\u6210\u4e00\u79cd\u7efc\u5408\u7684\u5e94\u5bf9\u6454\u5012\u7684\u7b56\u7565\u3002", "result": "\u901a\u8fc7\u7ed3\u5408\u7a00\u758f\u7684\u4eba\u7c7b\u793a\u8303\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u9002\u5e94\u6027\u7684\u57fa\u4e8e\u6269\u6563\u7684\u5b89\u5168\u53cd\u5e94\u8bb0\u5fc6\uff0c\u5b66\u4e60\u5230\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u5168\u8eab\u884c\u4e3a\uff0c\u5b9e\u73b0\u4e86\u6454\u5012\u9884\u9632\u3001\u51b2\u51fb\u7f13\u89e3\u548c\u5feb\u901f\u6062\u590d\u3002", "conclusion": "\u901a\u8fc7\u6a21\u62df\u548cUnitree G1\u673a\u5668\u4eba\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7ed3\u679c\u663e\u793a\u6d89\u53ca\u6454\u5012\u7684\u98ce\u9669\u660e\u663e\u964d\u4f4e\uff0c\u6062\u590d\u901f\u5ea6\u8f83\u5feb\uff0c\u672a\u6765\u6709\u52a9\u4e8e\u63d0\u5347\u4eba\u5f62\u673a\u5668\u4eba\u7684\u5b89\u5168\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2511.07410", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07410", "abs": "https://arxiv.org/abs/2511.07410", "authors": ["Hao Wang", "Sathwik Karnik", "Bea Lim", "Somil Bansal"], "title": "Using Vision Language Models as Closed-Loop Symbolic Planners for Robotic Applications: A Control-Theoretic Perspective", "comment": null, "summary": "Large Language Models (LLMs) and Vision Language Models (VLMs) have been widely used for embodied symbolic planning. Yet, how to effectively use these models for closed-loop symbolic planning remains largely unexplored. Because they operate as black boxes, LLMs and VLMs can produce unpredictable or costly errors, making their use in high-level robotic planning especially challenging. In this work, we investigate how to use VLMs as closed-loop symbolic planners for robotic applications from a control-theoretic perspective. Concretely, we study how the control horizon and warm-starting impact the performance of VLM symbolic planners. We design and conduct controlled experiments to gain insights that are broadly applicable to utilizing VLMs as closed-loop symbolic planners, and we discuss recommendations that can help improve the performance of VLM symbolic planners.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u4ece\u63a7\u5236\u7406\u8bba\u7684\u89d2\u5ea6\u6709\u6548\u5730\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5e94\u7528\u4e8e\u95ed\u73af\u7b26\u53f7\u89c4\u5212\uff0c\u7279\u522b\u662f\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u53ca\u5176\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u7b26\u53f7\u89c4\u5212\u4e2d\u7684\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u95ed\u73af\u7b26\u53f7\u89c4\u5212\u7684\u6709\u6548\u4f7f\u7528\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u8bbe\u8ba1\u5e76\u8fdb\u884c\u53d7\u63a7\u5b9e\u9a8c\uff0c\u4ee5\u63a2\u8ba8VLM\u5728\u95ed\u73af\u7b26\u53f7\u89c4\u5212\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u5173\u6ce8\u63a7\u5236\u89c6\u91ce\u548c\u70ed\u542f\u52a8\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u63a7\u5236\u89c6\u91ce\u548c\u70ed\u542f\u52a8\u5bf9VLM\u7b26\u53f7\u89c4\u5212\u5e08\u7684\u6027\u80fd\u5177\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e9b\u63d0\u5347\u5176\u6027\u80fd\u7684\u5efa\u8bae\u3002", "conclusion": "\u901a\u8fc7\u5bf9\u63a7\u5236\u89c6\u91ce\u548c\u70ed\u542f\u52a8\u5bf9VLM\u7b26\u53f7\u89c4\u5212\u5e08\u6027\u80fd\u5f71\u54cd\u7684\u7814\u7a76\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e9b\u6539\u8fdb\u5efa\u8bae\uff0c\u4ee5\u589e\u5f3aVLM\u5728\u9ad8\u5c42\u673a\u5668\u4eba\u89c4\u5212\u4e2d\u7684\u5e94\u7528\u6548\u679c\u3002"}}
{"id": "2511.07416", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.07416", "abs": "https://arxiv.org/abs/2511.07416", "authors": ["Jiageng Mao", "Sicheng He", "Hao-Ning Wu", "Yang You", "Shuyang Sun", "Zhicheng Wang", "Yanan Bao", "Huizhong Chen", "Leonidas Guibas", "Vitor Guizilini", "Howard Zhou", "Yue Wang"], "title": "Robot Learning from a Physical World Model", "comment": "Project page: https://pointscoder.github.io/PhysWorld_Web/", "summary": "We introduce PhysWorld, a framework that enables robot learning from video generation through physical world modeling. Recent video generation models can synthesize photorealistic visual demonstrations from language commands and images, offering a powerful yet underexplored source of training signals for robotics. However, directly retargeting pixel motions from generated videos to robots neglects physics, often resulting in inaccurate manipulations. PhysWorld addresses this limitation by coupling video generation with physical world reconstruction. Given a single image and a task command, our method generates task-conditioned videos and reconstructs the underlying physical world from the videos, and the generated video motions are grounded into physically accurate actions through object-centric residual reinforcement learning with the physical world model. This synergy transforms implicit visual guidance into physically executable robotic trajectories, eliminating the need for real robot data collection and enabling zero-shot generalizable robotic manipulation. Experiments on diverse real-world tasks demonstrate that PhysWorld substantially improves manipulation accuracy compared to previous approaches. Visit \\href{https://pointscoder.github.io/PhysWorld_Web/}{the project webpage} for details.", "AI": {"tldr": "PhysWorld\u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u7269\u7406\u4e16\u754c\u5efa\u6a21\u5b9e\u73b0\u673a\u5668\u4eba\u4ece\u89c6\u9891\u751f\u6210\u4e2d\u5b66\u4e60\uff0c\u7ed3\u5408\u89c6\u9891\u751f\u6210\u4e0e\u7269\u7406\u4e16\u754c\u91cd\u5efa\uff0c\u751f\u6210\u4efb\u52a1\u5bfc\u5411\u89c6\u9891\u5e76\u53ef\u4ee5\u5c06\u89c6\u9891\u4e2d\u7684\u52a8\u4f5c\u8f6c\u5316\u4e3a\u7269\u7406\u53ef\u6267\u884c\u7684\u673a\u5668\u4eba\u8f68\u8ff9\u3002", "motivation": "\u6700\u8fd1\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u80fd\u591f\u6839\u636e\u8bed\u8a00\u6307\u4ee4\u548c\u56fe\u50cf\u5408\u6210\u903c\u771f\u7684\u89c6\u89c9\u6f14\u793a\uff0c\u8fd9\u662f\u673a\u5668\u4eba\u8bad\u7ec3\u7684\u5f3a\u5927\u4f46\u5c1a\u672a\u5145\u5206\u5f00\u53d1\u7684\u4fe1\u53f7\u6765\u6e90\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u672a\u80fd\u8003\u8651\u7269\u7406\u89c4\u5f8b\u3002", "method": "\u901a\u8fc7\u5355\u5f20\u56fe\u50cf\u548c\u4efb\u52a1\u6307\u4ee4\u751f\u6210\u4efb\u52a1\u5bfc\u5411\u7684\u89c6\u9891\uff0c\u540c\u65f6\u91cd\u5efa\u89c6\u9891\u4e2d\u7684\u7269\u7406\u4e16\u754c\uff0c\u5e76\u901a\u8fc7\u7269\u5bf9\u8c61\u4e2d\u5fc3\u7684\u6b8b\u5dee\u5f3a\u5316\u5b66\u4e60\u5c06\u89c6\u9891\u52a8\u4f5c\u8f6c\u5316\u4e3a\u7269\u7406\u53ef\u6267\u884c\u7684\u64cd\u4f5c\u3002", "result": "\u5728\u591a\u79cd\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPhysWorld\u76f8\u6bd4\u4e4b\u524d\u7684\u65b9\u6cd5\u5728\u64cd\u63a7\u51c6\u786e\u6027\u4e0a\u6709\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "PhysWorld\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u64cd\u63a7\u7684\u51c6\u786e\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u96f6\u6837\u672c\u6cdb\u5316\u7684\u673a\u5668\u4eba\u64cd\u63a7\u3002"}}
{"id": "2511.07418", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.DC", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.07418", "abs": "https://arxiv.org/abs/2511.07418", "authors": ["Zhao-Heng Yin", "Pieter Abbeel"], "title": "Lightning Grasp: High Performance Procedural Grasp Synthesis with Contact Fields", "comment": "Code: https://github.com/zhaohengyin/lightning-grasp", "summary": "Despite years of research, real-time diverse grasp synthesis for dexterous hands remains an unsolved core challenge in robotics and computer graphics. We present Lightning Grasp, a novel high-performance procedural grasp synthesis algorithm that achieves orders-of-magnitude speedups over state-of-the-art approaches, while enabling unsupervised grasp generation for irregular, tool-like objects. The method avoids many limitations of prior approaches, such as the need for carefully tuned energy functions and sensitive initialization. This breakthrough is driven by a key insight: decoupling complex geometric computation from the search process via a simple, efficient data structure - the Contact Field. This abstraction collapses the problem complexity, enabling a procedural search at unprecedented speeds. We open-source our system to propel further innovation in robotic manipulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLightning Grasp\u7684\u65b0\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5bf9\u4e0d\u89c4\u5219\u5de5\u5177\u72b6\u7269\u4f53\u7684\u5b9e\u65f6\u591a\u6837\u5316\u6293\u53d6\u5408\u6210\uff0c\u901f\u5ea6\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb\u51e0\u4e2a\u6570\u91cf\u7ea7\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u673a\u5668\u4eba\u548c\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u4e2d\u7075\u5de7\u624b\u7684\u5b9e\u65f6\u591a\u6837\u5316\u6293\u53d6\u5408\u6210\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7a0b\u5e8f\u5316\u6293\u53d6\u5408\u6210\u7b97\u6cd5\uff0c\u5229\u7528Contact Field\u6570\u636e\u7ed3\u6784\u89e3\u8026\u51e0\u4f55\u8ba1\u7b97\u548c\u641c\u7d22\u8fc7\u7a0b\u3002", "result": "\u8be5\u65b9\u6cd5\u76f8\u6bd4\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5177\u6709\u663e\u8457\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u5e76\u4e14\u80fd\u591f\u5728\u65e0\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u5bf9\u4e0d\u89c4\u5219\u7269\u4f53\u751f\u6210\u6293\u53d6\u3002", "conclusion": "Lightning Grasp\u901a\u8fc7\u5f15\u5165Contact Field\u6570\u636e\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b9e\u65f6\u6293\u53d6\u5408\u6210\u7684\u6548\u7387\uff0c\u5e76\u6d88\u9664\u4e86\u5bf9\u590d\u6742\u80fd\u91cf\u51fd\u6570\u548c\u521d\u59cb\u5316\u7684\u4f9d\u8d56\u3002"}}
{"id": "2511.05683", "categories": ["cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05683", "abs": "https://arxiv.org/abs/2511.05683", "authors": ["Eric Godden", "Jacquie Groenewegen", "Michael Wheeler", "Matthew K. X. J. Pan"], "title": "Social-Physical Interactions with Virtual Characters: Evaluating the Impact of Physicality through Encountered-Type Haptics", "comment": "9 pages", "summary": "This work investigates how robot-mediated physicality influences the perception of social-physical interactions with virtual characters. ETHOS (Encountered-Type Haptics for On-demand Social interaction) is an encountered-type haptic display that integrates a torque-controlled manipulator and interchangeable props with a VR headset to enable three gestures: object handovers, fist bumps, and high fives. We conducted a user study to examine how ETHOS adds physicality to virtual character interactions and how this affects presence, realism, enjoyment, and connection metrics. Each participant experienced one interaction under three conditions: no physicality (NP), static physicality (SP), and dynamic physicality (DP). SP extended the purely virtual baseline (NP) by introducing tangible props for direct contact, while DP further incorporated motion and impact forces to emulate natural touch. Results show presence increased stepwise from NP to SP to DP. Realism, enjoyment, and connection also improved with added physicality, though differences between SP and DP were not significant. Comfort remained consistent across conditions, indicating no added psychological friction. These findings demonstrate the experiential value of ETHOS and motivate the integration of encountered-type haptics into socially meaningful VR experiences.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0cETHOS\u589e\u52a0\u7269\u7406\u63a5\u89e6\u80fd\u63d0\u5347\u865a\u62df\u89d2\u8272\u4e92\u52a8\u7684\u6c89\u6d78\u611f\u548c enjoyment\u3002", "motivation": "\u63a2\u8ba8\u673a\u5668\u4eba\u4ecb\u5bfc\u7684\u7269\u7406\u6027\u5982\u4f55\u5f71\u54cd\u4e0e\u865a\u62df\u89d2\u8272\u7684\u793e\u4ea4\u7269\u7406\u4e92\u52a8\u611f\u77e5\u3002", "method": "\u901a\u8fc7\u7528\u6237\u7814\u7a76\uff0c\u6bd4\u8f83\u4e86\u65e0\u7269\u7406\u6027\u3001\u9759\u6001\u7269\u7406\u6027\u548c\u52a8\u6001\u7269\u7406\u6027\u4e09\u79cd\u6761\u4ef6\u4e0b\u7684\u4e92\u52a8\u4f53\u9a8c\u3002", "result": "ETHOS\uff08\u6309\u9700\u793e\u4ea4\u4e92\u52a8\u7684\u9047\u5230\u578b\u89e6\u89c9\u6280\u672f\uff09\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u589e\u52a0\u7269\u7406\u6027\u53ef\u4ee5\u6539\u5584\u7528\u6237\u7684\u6c89\u6d78\u611f\u3001\u771f\u5b9e\u611f\u3001\u4eab\u53d7\u5ea6\u548c\u8fde\u63a5\u611f\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u5c55\u793a\u4e86ETHOS\u7684\u4f53\u9a8c\u4ef7\u503c\uff0c\u5e76\u6fc0\u52b1\u5c06\u9047\u5230\u578b\u89e6\u89c9\u6280\u672f\u6574\u5408\u5230\u793e\u4f1a\u4e92\u52a8\u7684\u865a\u62df\u73b0\u5b9e\u4f53\u9a8c\u4e2d\u3002"}}
