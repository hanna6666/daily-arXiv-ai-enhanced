{"id": "2510.18877", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18877", "abs": "https://arxiv.org/abs/2510.18877", "authors": ["Zhen Wu", "Jiaxin Shi", "R. Charles Murray", "Carolyn Ros\u00e9", "Micah San Andres"], "title": "LLM Bazaar: A Service Design for Supporting Collaborative Learning with an LLM-Powered Multi-Party Collaboration Infrastructure", "comment": "https://repository.isls.org//handle/1/11832", "summary": "For nearly two decades, conversational agents have played a critical role in\nstructuring interactions in collaborative learning, shaping group dynamics, and\nsupporting student engagement. The recent integration of large language models\n(LLMs) into these agents offers new possibilities for fostering critical\nthinking and collaborative problem solving. In this work, we begin with an open\nsource collaboration support architecture called Bazaar and integrate an\nLLM-agent shell that enables introduction of LLM-empowered, real time, context\nsensitive collaborative support for group learning. This design and\ninfrastructure paves the way for exploring how tailored LLM-empowered\nenvironments can reshape collaborative learning outcomes and interaction\npatterns.", "AI": {"tldr": "\u672c\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u534f\u4f5c\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u7684LLM\u4ee3\u7406\u67b6\u6784\u4ee5\u652f\u6301\u5b9e\u65f6\u5b66\u4e60\uff0c\u5e76\u7814\u7a76\u5176\u5bf9\u5b66\u4e60\u6548\u679c\u548c\u4e92\u52a8\u7684\u5f71\u54cd\u3002", "motivation": "\u4e3a\u4e86\u6539\u5584\u534f\u4f5c\u5b66\u4e60\u4e2d\u7684\u4e92\u52a8\u7ed3\u6784\u3001\u7ec4\u52a8\u529b\u548c\u5b66\u751f\u53c2\u4e0e\u5ea6\uff0c\u5229\u7528\u6700\u65b0\u7684\u5927\u8bed\u8a00\u6a21\u578b\u6280\u672f\u3002", "method": "\u5728\u73b0\u6709\u7684\u5f00\u6e90\u534f\u4f5c\u652f\u6301\u67b6\u6784Bazaar\u7684\u57fa\u7840\u4e0a\uff0c\u96c6\u6210\u4e86\u4e00\u4e2aLLM\u4ee3\u7406\u63a5\u53e3\uff0c\u63d0\u4f9b\u5b9e\u65f6\u3001\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u534f\u4f5c\u652f\u6301\u3002", "result": "\u8bba\u6587\u7ed3\u679c\u8868\u660e\uff0c\u96c6\u6210LLM\u7684\u4ee3\u7406\u63a5\u53e3\u80fd\u4e3a\u5c0f\u7ec4\u5b66\u4e60\u63d0\u4f9b\u6709\u9488\u5bf9\u6027\u7684\u652f\u6301\uff0c\u4ece\u800c\u53ef\u80fd\u63d0\u5347\u5b66\u4e60\u6548\u679c\u548c\u4e92\u52a8\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u589e\u5f3a\u534f\u4f5c\u5b66\u4e60\u7684\u652f\u6301\u73af\u5883\uff0c\u5e76\u6307\u51fa\u8fd9\u4e00\u8fdb\u5c55\u53ef\u80fd\u6539\u53d8\u5b66\u4e60\u7ed3\u679c\u548c\u4e92\u52a8\u6a21\u5f0f\u3002"}}
{"id": "2510.18878", "categories": ["cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18878", "abs": "https://arxiv.org/abs/2510.18878", "authors": ["Brij Bridhin Desai", "Yukta Arvind", "Aswathi Mundayatt", "Jaya Sreevalsan-Nair"], "title": "CityAQVis: Integrated ML-Visualization Sandbox Tool for Pollutant Estimation in Urban Regions Using Multi-Source Data (Software Article)", "comment": "19 pages, 10 figures, 2 tables", "summary": "Urban air pollution poses significant risks to public health, environmental\nsustainability, and policy planning. Effective air quality management requires\npredictive tools that can integrate diverse datasets and communicate complex\nspatial and temporal pollution patterns. There is a gap in interactive tools\nwith seamless integration of forecasting and visualization of spatial\ndistributions of air pollutant concentrations. We present CityAQVis, an\ninteractive machine learning ML sandbox tool designed to predict and visualize\npollutant concentrations at the ground level using multi-source data, which\nincludes satellite observations, meteorological parameters, population density,\nelevation, and nighttime lights. While traditional air quality visualization\ntools often lack forecasting capabilities, CityAQVis enables users to build and\ncompare predictive models, visualizing the model outputs and offering insights\ninto pollution dynamics at the ground level. The pilot implementation of the\ntool is tested through case studies predicting nitrogen dioxide (NO2)\nconcentrations in metropolitan regions, highlighting its adaptability to\nvarious pollutants. Through an intuitive graphical user interface (GUI), the\nuser can perform comparative visualizations of the spatial distribution of\nsurface-level pollutant concentration in two different urban scenarios. Our\nresults highlight the potential of ML-driven visual analytics to improve\nsituational awareness and support data-driven decision-making in air quality\nmanagement.", "AI": {"tldr": "CityAQVis\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u673a\u5668\u5b66\u4e60\u5de5\u5177\uff0c\u65e8\u5728\u5229\u7528\u591a\u6e90\u6570\u636e\u9884\u6d4b\u548c\u53ef\u89c6\u5316\u57ce\u5e02\u7a7a\u6c14\u6c61\u67d3\u7269\u6d53\u5ea6\uff0c\u6539\u5584\u7a7a\u6c14\u8d28\u91cf\u7ba1\u7406\u3002", "motivation": "\u5e94\u5bf9\u57ce\u5e02\u7a7a\u6c14\u6c61\u67d3\u5bf9\u516c\u5171\u5065\u5eb7\u548c\u73af\u5883\u53ef\u6301\u7eed\u6027\u7684\u91cd\u5927\u98ce\u9669\uff0c\u586b\u8865\u7f3a\u4e4f\u6709\u6548\u96c6\u6210\u9884\u6d4b\u4e0e\u53ef\u89c6\u5316\u5de5\u5177\u7684\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u591a\u6e90\u6570\u636e\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f0f\u673a\u5668\u5b66\u4e60\u5de5\u5177CityAQVis\u9884\u6d4b\u548c\u53ef\u89c6\u5316\u5730\u9762\u6c61\u67d3\u7269\u6d53\u5ea6\u3002", "result": "\u5f00\u53d1\u7684CityAQVis\u5de5\u5177\u5728\u6848\u4f8b\u7814\u7a76\u4e2d\u6210\u529f\u9884\u6d4b\u4e86\u57ce\u5e02\u5730\u533a\u7684\u4e8c\u6c27\u5316\u6c2e\u6d53\u5ea6\uff0c\u5e76\u5c55\u793a\u4e86\u5bf9\u4e0d\u540c\u6c61\u67d3\u7269\u7684\u9002\u5e94\u6027\u3002", "conclusion": "CityAQVis\u80fd\u591f\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u63a8\u52a8\u53ef\u89c6\u5316\u5206\u6790\uff0c\u63d0\u5347\u7a7a\u6c14\u8d28\u91cf\u7ba1\u7406\u4e2d\u7684\u60c5\u5883\u610f\u8bc6\u4e0e\u6570\u636e\u9a71\u52a8\u51b3\u7b56\u80fd\u529b\u3002"}}
{"id": "2510.18879", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.18879", "abs": "https://arxiv.org/abs/2510.18879", "authors": ["Mayamin Hamid Raha", "Ali Reza Tavakkoli", "Chris Webb", "Mobin Habibpour", "Janice Coen", "Eric Rowell", "Fatemeh Afghah"], "title": "FIRETWIN: Digital Twin Advancing Multi-Modal Sensing, Interactive Analytics for Wildfire Response", "comment": "8 pages, 6 figures, accepted in IEEE International Workshop on\n  Computer-Aided Modeling and Design of Communication Links and Networks\n  (CAMAD)", "summary": "Current wildfire management systems lack integrated virtual environments that\ncombine historical data with immersive digital representations, hindering deep\nanalysis and effective decision making. This paper introduces FIRETWIN, a\ncyber-physical Digital Twin (DT) designed to bridge complex ecological data and\noperationally relevant, high-fidelity visualizations for actionable incident\nresponse. FIRETWIN generates a dynamic 3D virtual globe that visualizes\nevolving fire behavior in real time, driven by output from physics-based fire\nmodels. The system supports multimodal perspectives, including satellite and\ndrone viewpoints comparable to NOAA GOES-18 imagery - enabling comprehensive\nscenario analysis. Users interact with the environment to assess current fire\nconditions, anticipate progression, and evaluate available resources.\nLeveraging Google Maps, Unreal Engine, and pre-generated outputs from the CAWFE\ncoupled weather-wildland fire model, we reconstruct the spread of the 2014 King\nFire in California Eldorado National Forest. Procedural forest generation and\nparticle-level fire control enable a level of realism and interactivity not\npossible in field training.", "AI": {"tldr": "FIRETWIN\u662f\u4e00\u4e2a\u7ed3\u5408\u5386\u53f2\u6570\u636e\u548c\u5b9e\u65f6\u6570\u5b57\u53ef\u89c6\u5316\u7684\u6570\u5b57\u53cc\u80de\u80ce\uff0c\u53ef\u7528\u4e8e\u6539\u5584\u91ce\u706b\u7ba1\u7406\u4e0e\u54cd\u5e94\u3002", "motivation": "\u76ee\u524d\u7684\u91ce\u706b\u7ba1\u7406\u7cfb\u7edf\u7f3a\u4e4f\u5c06\u5386\u53f2\u6570\u636e\u4e0e\u6c89\u6d78\u5f0f\u6570\u5b57\u8868\u8fbe\u76f8\u7ed3\u5408\u7684\u96c6\u6210\u865a\u62df\u73af\u5883\uff0c\u8fd9\u5f71\u54cd\u4e86\u6df1\u5165\u5206\u6790\u548c\u6709\u6548\u51b3\u7b56\u7684\u80fd\u529b\u3002", "method": "FIRETWIN\u5229\u7528\u8c37\u6b4c\u5730\u56fe\u3001\u865a\u5e7b\u5f15\u64ce\u53caCAWFE\u6a21\u578b\u751f\u6210\u7684\u8f93\u51fa\uff0c\u91cd\u5efa2014\u5e74\u52a0\u5229\u798f\u5c3c\u4e9a\u5dde\u91d1\u706b\u7684\u4f20\u64ad\uff0c\u63d0\u4f9b\u9ad8\u5ea6\u771f\u5b9e\u548c\u4e92\u52a8\u7684\u73af\u5883\u3002", "result": "\u672c\u6587\u4ecb\u7ecd\u4e86FIRETWIN\uff0c\u8fd9\u662f\u4e00\u4e2a\u65e8\u5728\u6865\u63a5\u590d\u6742\u751f\u6001\u6570\u636e\u548c\u9ad8\u4fdd\u771f\u53ef\u89c6\u5316\u7684\u7f51\u7edc\u7269\u7406\u6570\u5b57\u53cc\u80de\u80ce\uff0c\u4e3a\u53ef\u64cd\u4f5c\u7684\u4e8b\u4ef6\u54cd\u5e94\u63d0\u4f9b\u652f\u6301\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u65f6\u6a21\u62df\u548c\u4ea4\u4e92\uff0cFIRETWIN\u4e3a\u7528\u6237\u63d0\u4f9b\u4e86\u5bf9\u706b\u707e\u884c\u4e3a\u7684\u6df1\u5165\u7406\u89e3\u548c\u54cd\u5e94\u80fd\u529b\uff0c\u6781\u5927\u5730\u63d0\u5347\u4e86\u91ce\u706b\u7ba1\u7406\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.18880", "categories": ["cs.HC", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.18880", "abs": "https://arxiv.org/abs/2510.18880", "authors": ["Rory Sayres", "Yuexing Hao", "Abbi Ward", "Amy Wang", "Beverly Freeman", "Serena Zhan", "Diego Ardila", "Jimmy Li", "I-Ching Lee", "Anna Iurchenko", "Siyi Kou", "Kartikeya Badola", "Jimmy Hu", "Bhawesh Kumar", "Keith Johnson", "Supriya Vijay", "Justin Krogue", "Avinatan Hassidim", "Yossi Matias", "Dale R. Webster", "Sunny Virmani", "Yun Liu", "Quang Duong", "Mike Schaekermann"], "title": "Towards Better Health Conversations: The Benefits of Context-seeking", "comment": null, "summary": "Navigating health questions can be daunting in the modern information\nlandscape. Large language models (LLMs) may provide tailored, accessible\ninformation, but also risk being inaccurate, biased or misleading. We present\ninsights from 4 mixed-methods studies (total N=163), examining how people\ninteract with LLMs for their own health questions. Qualitative studies revealed\nthe importance of context-seeking in conversational AIs to elicit specific\ndetails a person may not volunteer or know to share. Context-seeking by LLMs\nwas valued by participants, even if it meant deferring an answer for several\nturns. Incorporating these insights, we developed a \"Wayfinding AI\" to\nproactively solicit context. In a randomized, blinded study, participants rated\nthe Wayfinding AI as more helpful, relevant, and tailored to their concerns\ncompared to a baseline AI. These results demonstrate the strong impact of\nproactive context-seeking on conversational dynamics, and suggest design\npatterns for conversational AI to help navigate health topics.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u5982\u4f55\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63d0\u9ad8\u5065\u5eb7\u4fe1\u606f\u83b7\u53d6\u7684\u4f53\u9a8c\uff0c\u5f3a\u8c03\u4e3b\u52a8\u5bfb\u6c42\u80cc\u666f\u4fe1\u606f\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u76f8\u5e94\u7684\u5bf9\u8bddAI\u8bbe\u8ba1\u5efa\u8bae\u3002", "motivation": "\u73b0\u4ee3\u4fe1\u606f\u73af\u5883\u4e2d\uff0c\u83b7\u53d6\u5065\u5eb7\u95ee\u9898\u7684\u4fe1\u606f\u53ef\u80fd\u5f88\u56f0\u96be\uff0cLLMs\u53ef\u4ee5\u63d0\u4f9b\u5b9a\u5236\u7684\u3001\u4fbf\u6377\u7684\u4fe1\u606f\uff0c\u4f46\u4e5f\u5b58\u5728\u4e0d\u51c6\u786e\u548c\u504f\u89c1\u7684\u98ce\u9669\u3002", "method": "\u901a\u8fc74\u9879\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\uff0c\u5bf9\u603b\u5171163\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u5206\u6790\uff0c\u8003\u5bdf\u4eba\u4eec\u5982\u4f55\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e92\u52a8\u4ee5\u89e3\u7b54\u5065\u5eb7\u95ee\u9898\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u201c\u5bfc\u822aAI\u201d\uff0c\u5728\u968f\u673a\u53cc\u76f2\u7814\u7a76\u4e2d\uff0c\u53c2\u4e0e\u8005\u5bf9\u5176\u597d\u8bc4\uff0c\u8ba4\u4e3a\u5176\u6bd4\u57fa\u7840AI\u66f4\u6709\u5e2e\u52a9\u3001\u76f8\u5173\u548c\u9488\u5bf9\u6027\u5f3a\u3002", "conclusion": "\u4e3b\u52a8\u5bfb\u6c42\u80cc\u666f\u4fe1\u606f\u5bf9\u5bf9\u8bdd\u52a8\u6001\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5efa\u8bae\u4e86\u7528\u4e8e\u5065\u5eb7\u4e3b\u9898\u7684\u5bf9\u8bddAI\u8bbe\u8ba1\u6a21\u5f0f\u3002"}}
{"id": "2510.18986", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.18986", "abs": "https://arxiv.org/abs/2510.18986", "authors": ["Alberto Sanchez-Delgado", "Jo\u00e3o Carlos Virgolino Soares", "Victor Barasuol", "Claudio Semini"], "title": "Towards Proprioceptive Terrain Mapping with Quadruped Robots for Exploration in Planetary Permanently Shadowed Regions", "comment": "Published in the Proceedings of the International Conference on Space\n  Robotics (iSpaRo 2025)", "summary": "Permanently Shadowed Regions (PSRs) near the lunar poles are of interest for\nfuture exploration due to their potential to contain water ice and preserve\ngeological records. Their complex, uneven terrain favors the use of legged\nrobots, which can traverse challenging surfaces while collecting in-situ data,\nand have proven effective in Earth analogs, including dark caves, when equipped\nwith onboard lighting. While exteroceptive sensors like cameras and lidars can\ncapture terrain geometry and even semantic information, they cannot quantify\nits physical interaction with the robot, a capability provided by\nproprioceptive sensing. We propose a terrain mapping framework for quadruped\nrobots, which estimates elevation, foot slippage, energy cost, and stability\nmargins from internal sensing during locomotion. These metrics are\nincrementally integrated into a multi-layer 2.5D gridmap that reflects terrain\ninteraction from the robot's perspective. The system is evaluated in a\nsimulator that mimics a lunar environment, using the 21 kg quadruped robot\nAliengo, showing consistent mapping performance under lunar gravity and terrain\nconditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u56db\u8db3\u673a\u5668\u4eba\u5728\u6708\u7403\u6781\u5730\u6c38\u4e45\u9634\u5f71\u533a\u8fdb\u884c\u5730\u5f62\u6620\u5c04\u7684\u65b0\u6846\u67b6\uff0c\u80fd\u591f\u901a\u8fc7\u5185\u90e8\u4f20\u611f\u5668\u8bc4\u4f30\u5730\u5f62\u53ca\u5176\u4e0e\u673a\u5668\u4eba\u7684\u4ea4\u4e92\uff0c\u5b9e\u73b0\u5728\u6a21\u62df\u7684\u6708\u7403\u73af\u5883\u4e2d\u7684\u6709\u6548\u5e94\u7528\u3002", "motivation": "\u9274\u4e8e\u6c38\u8fdc\u9634\u5f71\u533a\u53ef\u80fd\u542b\u6709\u6c34\u51b0\u548c\u5730\u8d28\u8bb0\u5f55\uff0c\u9488\u5bf9\u8fd9\u4e9b\u590d\u6742\u5730\u5f62\uff0c\u56db\u8db3\u673a\u5668\u4eba\u80fd\u591f\u6709\u6548\u83b7\u53d6\u539f\u4f4d\u6570\u636e\uff0c\u662f\u672a\u6765\u63a2\u7d22\u7684\u7406\u60f3\u9009\u62e9\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5185\u90e8\u4f20\u611f\u5668\u7684\u5730\u5f62\u6620\u5c04\u6846\u67b6\uff0c\u4f30\u7b97\u6d77\u62d4\u3001\u8db3\u90e8\u6ed1\u52a8\u3001\u80fd\u91cf\u6210\u672c\u548c\u7a33\u5b9a\u6027\u8fb9\u9645\uff0c\u5e76\u96c6\u6210\u5230\u4e00\u4e2a\u591a\u5c422.5D\u7f51\u683c\u56fe\u4e2d\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u56db\u8db3\u673a\u5668\u4eba\u5728\u6708\u7403\u6781\u5730\u6c38\u4e45\u9634\u5f71\u533a\u8fdb\u884c\u5730\u5f62\u6620\u5c04\u7684\u6846\u67b6\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5728\u6a21\u62df\u7684\u6708\u7403\u73af\u5883\u4e2d\u8fdb\u884c\u8bc4\u4f30\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u6708\u7403\u91cd\u529b\u548c\u5730\u5f62\u6761\u4ef6\u4e0b\u7684\u4e00\u81f4\u6620\u5c04\u6027\u80fd\u3002"}}
{"id": "2510.18881", "categories": ["cs.HC", "cs.CY", "K.3.1; K.3.2; I.2.6; H.2.8"], "pdf": "https://arxiv.org/pdf/2510.18881", "abs": "https://arxiv.org/abs/2510.18881", "authors": ["G\u00f6khan Ak\u00e7ap\u0131nar"], "title": "Detecting AI-Assisted Cheating in Online Exams through Behavior Analytics", "comment": "Accepted in the Proceedings of the IADIS International Conference on\n  Cognition and Exploratory Learning in the Digital Age (CELDA), 2025", "summary": "AI-assisted cheating has emerged as a significant threat in the context of\nonline exams. Advanced browser extensions now enable large language models\n(LLMs) to answer questions presented in online exams within seconds, thereby\ncompromising the security of these assessments. In this study, the behaviors of\nstudents (N = 52) on an online exam platform during a proctored, face-to-face\nexam were analyzed using clustering methods, with the aim of identifying groups\nof students exhibiting suspicious behavior potentially associated with\ncheating. Additionally, students in different clusters were compared in terms\nof their exam scores. Suspicious exam behaviors in this study were defined as\nselecting text within the question area, right-clicking, and losing focus on\nthe exam page. The total frequency of these behaviors performed by each student\nduring the exam was extracted, and k-Means clustering was employed for the\nanalysis. The findings revealed that students were classified into six clusters\nbased on their suspicious behaviors. It was found that students in four of the\nsix clusters, representing approximately 33% of the total sample, exhibited\nsuspicious behaviors at varying levels. When the exam scores of these students\nwere compared, it was observed that those who engaged in suspicious behaviors\nscored, on average, 30-40 points higher than those who did not. Although\nfurther research is necessary to validate these findings, this preliminary\nstudy provides significant insights into the detection of AI-assisted cheating\nin online exams using behavior analytics.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5b66\u751f\u5728\u5728\u7ebf\u8003\u8bd5\u4e2d\u53ef\u80fd\u51fa\u73b0\u7684AI\u8f85\u52a9\u4f5c\u5f0a\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u805a\u7c7b\u65b9\u6cd5\u5206\u6790\u4e86\u53ef\u7591\u884c\u4e3a\u4e0e\u8003\u8bd5\u6210\u7ee9\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "motivation": "\u9274\u4e8eAI\u8f85\u52a9\u4f5c\u5f0a\u5bf9\u5728\u7ebf\u8003\u8bd5\u7684\u5b89\u5168\u6027\u9020\u6210\u5a01\u80c1\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u884c\u4e3a\u5206\u6790\u53d1\u73b0\u53ef\u7591\u7684\u5b66\u751f\u7fa4\u4f53\u3002", "method": "\u91c7\u7528k-Means\u805a\u7c7b\u5206\u6790\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u5b66\u751f\u5728\u76d1\u8003\u4e0b\u5728\u7ebf\u8003\u8bd5\u4e2d\u7684\u53ef\u7591\u884c\u4e3a\uff0c\u5b9a\u4e49\u4e86\u9009\u4e2d\u6587\u672c\u3001\u53f3\u952e\u70b9\u51fb\u548c\u5931\u53bb\u9875\u9762\u5173\u6ce8\u5ea6\u7b49\u884c\u4e3a\u3002", "result": "\u672c\u7814\u7a76\u901a\u8fc7\u805a\u7c7b\u5206\u6790\uff0c\u8bc6\u522b\u4e86\u5728\u7ebf\u8003\u8bd5\u4e2d\u8868\u73b0\u51fa\u53ef\u7591\u884c\u4e3a\u7684\u5b66\u751f\u7fa4\u4f53\uff0c\u5e76\u6bd4\u8f83\u4e86\u4ed6\u4eec\u7684\u8003\u8bd5\u6210\u7ee9\u3002\u5927\u7ea633%\u7684\u5b66\u751f\u8868\u73b0\u51fa\u4e0d\u540c\u7a0b\u5ea6\u7684\u53ef\u7591\u884c\u4e3a\uff0c\u800c\u8fd9\u4e9b\u5b66\u751f\u7684\u8003\u8bd5\u6210\u7ee9\u5e73\u5747\u9ad8\u51fa30-40\u5206\u3002", "conclusion": "\u5c3d\u7ba1\u9700\u8981\u8fdb\u4e00\u6b65\u9a8c\u8bc1\uff0c\u8fd9\u9879\u521d\u6b65\u7814\u7a76\u4e3a\u8bc6\u522b\u5728\u7ebf\u8003\u8bd5\u4e2d\u7684AI\u8f85\u52a9\u4f5c\u5f0a\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u89c1\u89e3\u3002"}}
{"id": "2510.18991", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.18991", "abs": "https://arxiv.org/abs/2510.18991", "authors": ["Chinmay Burgul", "Yewei Huang", "Michalis Chatzispyrou", "Ioannis Rekleitis", "Alberto Quattrini Li", "Marios Xanthidis"], "title": "Underwater Dense Mapping with the First Compact 3D Sonar", "comment": "8 pages, 12 figures", "summary": "In the past decade, the adoption of compact 3D range sensors, such as LiDARs,\nhas driven the developments of robust state-estimation pipelines, making them a\nstandard sensor for aerial, ground, and space autonomy. Unfortunately, poor\npropagation of electromagnetic waves underwater, has limited the\nvisibility-independent sensing options of underwater state-estimation to\nacoustic range sensors, which provide 2D information including, at-best,\nspatially ambiguous information. This paper, to the best of our knowledge, is\nthe first study examining the performance, capacity, and opportunities arising\nfrom the recent introduction of the first compact 3D sonar. Towards that\npurpose, we introduce calibration procedures for extracting the extrinsics\nbetween the 3D sonar and a camera and we provide a study on acoustic response\nin different surfaces and materials. Moreover, we provide novel mapping and\nSLAM pipelines tested in deployments in underwater cave systems and other\ngeometrically and acoustically challenging underwater environments. Our\nassessment showcases the unique capacity of 3D sonars to capture consistent\nspatial information allowing for detailed reconstructions and localization in\ndatasets expanding to hundreds of meters. At the same time it highlights\nremaining challenges related to acoustic propagation, as found also in other\nacoustic sensors. Datasets collected for our evaluations would be released and\nshared with the community to enable further research advancements.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ecb\u7ecd\u7d27\u51d1\u578b3D\u58f0\u7eb3\u5728\u6c34\u4e0b\u72b6\u6001\u4f30\u8ba1\u4e2d\u7684\u5e94\u7528\u53ca\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u65b0\u578b\u7684\u6620\u5c04\u548cSLAM\u7ba1\u9053\u3002", "motivation": "\u63a2\u8ba8\u5728\u6c34\u4e0b\u72b6\u6001\u4f30\u8ba1\u4e2d\u91c7\u7528\u7d27\u51d1\u578b3D\u58f0\u7eb3\u7684\u6f5c\u529b\u548c\u673a\u4f1a\uff0c\u56e0\u4e3a\u73b0\u6709\u7684\u58f0\u5b66\u4f20\u611f\u5668\u5728\u4fe1\u606f\u83b7\u53d6\u4e0a\u5b58\u5728\u9650\u5236\u3002", "method": "\u672c\u6587\u63d0\u51fa\u58f0\u7eb3\u4e0e\u6444\u50cf\u5934\u95f4\u7684\u6807\u5b9a\u6d41\u7a0b\uff0c\u5e76\u7814\u7a76\u4e0d\u540c\u8868\u9762\u548c\u6750\u6599\u7684\u58f0\u5b66\u54cd\u5e94\uff0c\u540c\u65f6\u6d4b\u8bd5\u5728\u590d\u6742\u6c34\u4e0b\u73af\u5883\u4e2d\u7684\u6620\u5c04\u548cSLAM\u7ba1\u9053\u3002", "result": "\u9996\u6b21\u8bc4\u4f30\u7d27\u51d1\u578b3D\u58f0\u7eb3\u7684\u6027\u80fd\u548c\u5bb9\u91cf\uff0c\u5c55\u793a\u5176\u5728\u6c34\u4e0b\u73af\u5883\u4e2d\u7684\u72ec\u7279\u80fd\u529b\u3002", "conclusion": "3D\u58f0\u7eb3\u80fd\u591f\u6355\u83b7\u4e00\u81f4\u7684\u7a7a\u95f4\u4fe1\u606f\uff0c\u4fbf\u4e8e\u8be6\u7ec6\u91cd\u5efa\u548c\u5b9a\u4f4d\uff0c\u4f46\u5728\u58f0\u5b66\u4f20\u64ad\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002"}}
{"id": "2510.19008", "categories": ["cs.HC", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.19008", "abs": "https://arxiv.org/abs/2510.19008", "authors": ["Joydeep Chandra", "Satyam Kumar Navneet"], "title": "Plural Voices, Single Agent: Towards Inclusive AI in Multi-User Domestic Spaces", "comment": null, "summary": "Domestic AI agents faces ethical, autonomy, and inclusion challenges,\nparticularly for overlooked groups like children, elderly, and Neurodivergent\nusers. We present the Plural Voices Model (PVM), a novel single-agent framework\nthat dynamically negotiates multi-user needs through real-time value alignment,\nleveraging diverse public datasets on mental health, eldercare, education, and\nmoral reasoning. Using human+synthetic curriculum design with fairness-aware\nscenarios and ethical enhancements, PVM identifies core values, conflicts, and\naccessibility requirements to inform inclusive principles. Our privacy-focused\nprototype features adaptive safety scaffolds, tailored interactions (e.g.,\nstep-by-step guidance for Neurodivergent users, simple wording for children),\nand equitable conflict resolution. In preliminary evaluations, PVM outperforms\nmulti-agent baselines in compliance (76% vs. 70%), fairness (90% vs. 85%),\nsafety-violation rate (0% vs. 7%), and latency. Design innovations, including\nvideo guidance, autonomy sliders, family hubs, and adaptive safety dashboards,\ndemonstrate new directions for ethical and inclusive domestic AI, for building\nuser-centered agentic systems in plural domestic contexts. Our Codes and Model\nare been open sourced, available for reproduction:\nhttps://github.com/zade90/Agora", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u5bb6\u7528AI\u6846\u67b6PVM\uff0c\u65e8\u5728\u89e3\u51b3\u4f26\u7406\u548c\u5305\u5bb9\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u534f\u5546\u6ee1\u8db3\u591a\u7528\u6237\u9700\u6c42\uff0c\u521d\u6b65\u8bc4\u4ef7\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u4e3a\u5e94\u5bf9\u5bb6\u7528AI\u5728\u4f26\u7406\u3001\u81ea\u6cbb\u548c\u5305\u5bb9\u6027\u65b9\u9762\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5bf9\u513f\u7ae5\u3001\u8001\u5e74\u4eba\u548c\u795e\u7ecf\u591a\u6837\u6027\u7528\u6237\u7684\u5173\u6ce8\u3002", "method": "\u901a\u8fc7\u91c7\u7528\u516c\u5171\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u4eba\u7c7b + \u5408\u6210\u8bfe\u7a0b\u8bbe\u8ba1\u548c\u516c\u5e73\u610f\u8bc6\u573a\u666f\uff0c\u5f00\u53d1\u51fa\u4e00\u4e2a\u52a8\u6001\u534f\u5546\u7528\u6237\u9700\u6c42\u7684\u5355\u4ee3\u7406\u6846\u67b6\u3002", "result": "PVM\u5728\u5404\u9879\u6027\u80fd\u6307\u6807\u4e0a\u8d85\u8d8a\u4e86\u591a\u4ee3\u7406\u57fa\u7ebf\uff0c\u5e76\u901a\u8fc7\u8bbe\u8ba1\u521b\u65b0\u5c55\u793a\u4e86\u7528\u6237\u4e2d\u5fc3\u7cfb\u7edf\u7684\u6f5c\u529b\u3002", "conclusion": "PVM\u5728\u5408\u89c4\u6027\u3001\u516c\u5e73\u6027\u3001\u5b89\u5168\u6027\u548c\u5ef6\u8fdf\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u591a\u4ee3\u7406\u57fa\u7ebf\uff0c\u4e3a\u4f26\u7406\u548c\u5305\u5bb9\u6027\u5bb6\u7528AI\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.18996", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.18996", "abs": "https://arxiv.org/abs/2510.18996", "authors": ["Susheel Vadakkekuruppath", "Herman B. Amundsen", "Jason M. O'Kane", "Marios Xanthidis"], "title": "SHRUMS: Sensor Hallucination for Real-time Underwater Motion Planning with a Compact 3D Sonar", "comment": "8 pages, 5 figures", "summary": "Autonomous navigation in 3D is a fundamental problem for autonomy. Despite\nmajor advancements in terrestrial and aerial settings due to improved range\nsensors including LiDAR, compact sensors with similar capabilities for\nunderwater robots have only recently become available, in the form of 3D\nsonars. This paper introduces a novel underwater 3D navigation pipeline, called\nSHRUMS (Sensor Hallucination for Robust Underwater Motion planning with 3D\nSonar). To the best of the authors' knowledge, SHRUMS is the first underwater\nautonomous navigation stack to integrate a 3D sonar. The proposed pipeline\nexhibits strong robustness while operating in complex 3D environments in spite\nof extremely poor visibility conditions. To accommodate the intricacies of the\nnovel sensor data stream while achieving real-time locally optimal performance,\nSHRUMS introduces the concept of hallucinating sensor measurements from\nnon-existent sensors with convenient arbitrary parameters, tailored to\napplication specific requirements. The proposed concepts are validated with\nreal 3D sonar sensor data, utilizing real inputs in challenging settings and\nlocal maps constructed in real-time. Field deployments validating the proposed\napproach in full are planned in the very near future.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86SHRUMS\uff0c\u4e00\u79cd\u65b0\u578b\u6c34\u4e0b3D\u5bfc\u822a\u65b9\u6848\uff0c\u96c6\u6210\u4e863D\u58f0\u7eb3\uff0c\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u9c81\u68d2\u5bfc\u822a\u3002", "motivation": "\u89e3\u51b3\u6c34\u4e0b\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u7684\u4f20\u611f\u5668\u7f3a\u4e4f\u548c\u80fd\u89c1\u5ea6\u6781\u5dee\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u865a\u6784\u4f20\u611f\u5668\u6d4b\u91cf\uff0c\u5e76\u5b9e\u65f6\u4f18\u5316\uff0c\u540c\u65f6\u5904\u7406\u590d\u67423D\u73af\u5883\u4e2d\u7684\u4f20\u611f\u5668\u6570\u636e\u6d41\u3002", "result": "SHRUMS\u5728\u5b9e\u9645\u76843D\u58f0\u7eb3\u6570\u636e\u4e0a\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u5e76\u5c06\u5728\u672a\u6765\u8fdb\u884c\u5b9e\u9645\u573a\u5730\u7684\u90e8\u7f72\u3002", "conclusion": "SHRUMS\u662f\u9996\u4e2a\u96c6\u62103D\u58f0\u7eb3\u7684\u6c34\u4e0b\u81ea\u4e3b\u5bfc\u822a\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u590d\u6742\u7684\u6c34\u4e0b\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.19017", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.19017", "abs": "https://arxiv.org/abs/2510.19017", "authors": ["Wei Xiang", "Yunkai Xu", "Yuyang Fang", "Zhuyu Teng", "Zhaoqu Jiang", "Beijia Hu", "Jinguo Yang"], "title": "SocializeChat: A GPT-Based AAC Tool Grounded in Personal Memories to Support Social Communication", "comment": "Accepted to the IEEE International Conference on Systems, Man, and\n  Cybernetics 2025 (IEEE SMC 2025). Personal use permitted. For other uses,\n  permission must be obtained from IEEE", "summary": "Elderly people with speech impairments often face challenges in engaging in\nmeaningful social communication, particularly when using Augmentative and\nAlternative Communication (AAC) tools that primarily address basic needs.\nMoreover, effective chats often rely on personal memories, which is hard to\nextract and reuse. We introduce SocializeChat, an AAC tool that generates\nsentence suggestions by drawing on users' personal memory records. By\nincorporating topic preference and interpersonal closeness, the system reuses\npast experience and tailors suggestions to different social contexts and\nconversation partners. SocializeChat not only leverages past experiences to\nsupport interaction, but also treats conversations as opportunities to create\nnew memories, fostering a dynamic cycle between memory and communication. A\nuser study shows its potential to enhance the inclusivity and relevance of\nAAC-supported social interaction.", "AI": {"tldr": "SocializeChat\u5229\u7528\u7528\u6237\u7684\u4e2a\u4eba\u8bb0\u5fc6\u751f\u6210\u793e\u4ea4\u804a\u5929\u5efa\u8bae\uff0c\u589e\u5f3aAAC\u5de5\u5177\u7684\u4e92\u52a8\u6027\u548c\u76f8\u5173\u6027\u3002", "motivation": "\u8001\u5e74\u4eba\u9762\u4e34\u4e0e\u4ed6\u4eba\u8fdb\u884c\u6709\u6548\u793e\u4ea4\u6c9f\u901a\u7684\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u4f7f\u7528\u4e3b\u8981\u89e3\u51b3\u57fa\u672c\u9700\u6c42\u7684\u8f85\u52a9\u4e0e\u66ff\u4ee3\u6c9f\u901a\u5de5\u5177\uff08AAC\uff09\u65f6\u3002", "method": "\u8be5\u7cfb\u7edf\u7ed3\u5408\u4e86\u7528\u6237\u7684\u4e3b\u9898\u504f\u597d\u548c\u4eba\u9645\u5173\u7cfb\u7684\u4eb2\u5bc6\u7a0b\u5ea6\uff0c\u4ece\u800c\u5b9a\u5236\u4e0d\u540c\u793e\u4ea4\u573a\u666f\u548c\u5bf9\u8bdd\u4f19\u4f34\u7684\u5efa\u8bae\u3002", "result": "SocializeChat \u662f\u4e00\u79cdAAC\u5de5\u5177\uff0c\u901a\u8fc7\u7528\u6237\u7684\u4e2a\u4eba\u8bb0\u5fc6\u8bb0\u5f55\u751f\u6210\u53e5\u5b50\u5efa\u8bae\uff0c\u4ece\u800c\u6539\u5584\u793e\u4ea4\u6c9f\u901a\u3002", "conclusion": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cSocializeChat\u5728\u63d0\u5347AAC\u652f\u6301\u7684\u793e\u4ea4\u4e92\u52a8\u7684\u5305\u5bb9\u6027\u548c\u76f8\u5173\u6027\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2510.18999", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.18999", "abs": "https://arxiv.org/abs/2510.18999", "authors": ["Zhirui Dai", "Qihao Qian", "Tianxing Fan", "Nikolay Atanasov"], "title": "$\\nabla$-SDF: Learning Euclidean Signed Distance Functions Online with Gradient-Augmented Octree Interpolation and Neural Residual", "comment": null, "summary": "Estimation of signed distance functions (SDFs) from point cloud data has been\nshown to benefit many robot autonomy capabilities, including localization,\nmapping, motion planning, and control. Methods that support online and\nlarge-scale SDF reconstruction tend to rely on discrete volumetric data\nstructures, which affect the continuity and differentiability of the SDF\nestimates. Recently, using implicit features, neural network methods have\ndemonstrated high-fidelity and differentiable SDF reconstruction but they tend\nto be less efficient, can experience catastrophic forgetting and memory\nlimitations in large environments, and are often restricted to truncated SDFs.\nThis work proposes $\\nabla$-SDF, a hybrid method that combines an explicit\nprior obtained from gradient-augmented octree interpolation with an implicit\nneural residual. Our method achieves non-truncated (Euclidean) SDF\nreconstruction with computational and memory efficiency comparable to\nvolumetric methods and differentiability and accuracy comparable to neural\nnetwork methods. Extensive experiments demonstrate that \\methodname{}\noutperforms the state of the art in terms of accuracy and efficiency, providing\na scalable solution for downstream tasks in robotics and computer vision.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u65b9\u6cd5 $\nabla$-SDF\uff0c\u65e8\u5728\u63d0\u9ad8\u7b26\u53f7\u8ddd\u79bb\u51fd\u6570\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u4ee5\u4fbf\u5728\u673a\u5668\u4eba\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u5b9e\u73b0\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u4ece\u70b9\u4e91\u6570\u636e\u4e2d\u4f30\u8ba1\u7b26\u53f7\u8ddd\u79bb\u51fd\u6570\uff08SDF\uff09\u5bf9\u673a\u5668\u4eba\u81ea\u4e3b\u80fd\u529b\u7684\u63d0\u5347\u5341\u5206\u5fc5\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u6548\u7387\u3001\u6301\u7eed\u6027\u548c\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u5b58\u5728\u5c40\u9650\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u663e\u5f0f\u5148\u9a8c\u548c\u9690\u5f0f\u795e\u7ecf\u6b8b\u5dee\uff0c\u901a\u8fc7\u68af\u5ea6\u589e\u5f3a\u7684\u516b\u53c9\u6811\u63d2\u503c\u5b9e\u73b0SDF\u91cd\u5efa\u3002", "result": "\u63d0\u51fa\u4e86 $\nabla$-SDF \u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u57fa\u4e8e\u68af\u5ea6\u589e\u5f3a\u7684\u516b\u53c9\u6811\u63d2\u503c\u7684\u663e\u5f0f\u5148\u9a8c\u548c\u9690\u5f0f\u795e\u7ecf\u6b8b\u5dee\uff0c\u5b9e\u73b0\u4e86\u975e\u622a\u65ad\u7684SDF\u91cd\u5efa\uff0c\u540c\u65f6\u5728\u8ba1\u7b97\u548c\u5185\u5b58\u6548\u7387\u4e0a\u4e0e\u4f53\u7d20\u65b9\u6cd5\u76f8\u5f53\uff0c\u5728\u53ef\u5fae\u6027\u548c\u51c6\u786e\u6027\u4e0a\u4e0e\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "$\nabla$-SDF \u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u4e0b\u6e38\u4efb\u52a1\u3002"}}
{"id": "2510.19024", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.19024", "abs": "https://arxiv.org/abs/2510.19024", "authors": ["Jingruo Chen", "TungYen Wang", "Marie Williams", "Natalia Jordan", "Mingyi Shao", "Linda Zhang", "Susan R. Fussell"], "title": "Examining the Impact of Label Detail and Content Stakes on User Perceptions of AI-Generated Images on Social Media", "comment": "In Companion Publication of the 2025 Conference on Computer-Supported\n  Cooperative Work and Social Computing (CSCW Companion '25)", "summary": "AI-generated images are increasingly prevalent on social media, raising\nconcerns about trust and authenticity. This study investigates how different\nlevels of label detail (basic, moderate, maximum) and content stakes (high vs.\nlow) influence user engagement with and perceptions of AI-generated images\nthrough a within-subjects experimental study with 105 participants. Our\nfindings reveal that increasing label detail enhances user perceptions of label\ntransparency but does not affect user engagement. However, content stakes\nsignificantly impact user engagement and perceptions, with users demonstrating\nhigher engagement and trust in low-stakes images. These results suggest that\nsocial media platforms can adopt detailed labels to improve transparency\nwithout compromising user engagement, offering insights for effective labeling\nstrategies for AI-generated content.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u6807\u7b7e\u8be6\u7ec6\u7a0b\u5ea6\u548c\u5185\u5bb9\u98ce\u9669\u5bf9\u7528\u6237\u53c2\u4e0e\u5ea6\u53ca\u611f\u77e5\u7684\u5f71\u54cd\uff0c\u7ed3\u679c\u663e\u793a\u66f4\u8be6\u7ec6\u7684\u6807\u7b7e\u63d0\u5347\u4e86\u900f\u660e\u5ea6\uff0c\u4f46\u5185\u5bb9\u98ce\u9669\u5bf9\u7528\u6237\u53c2\u4e0e\u5ea6\u5f71\u54cd\u663e\u8457\u3002", "motivation": "\u968f\u7740AI\u751f\u6210\u56fe\u50cf\u5728\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u65e5\u76ca\u666e\u53ca\uff0c\u4eba\u4eec\u5bf9\u4fe1\u4efb\u548c\u771f\u5b9e\u6027\u7684\u62c5\u5fe7\u65e5\u76ca\u589e\u5f3a\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u6807\u7b7e\u7684\u8be6\u7ec6\u7a0b\u5ea6\u548c\u5185\u5bb9\u98ce\u9669\u5bf9\u7528\u6237\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5bf9105\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u7684\u7ec4\u5185\u5b9e\u9a8c\u7814\u7a76\uff0c\u8003\u5bdf\u4e0d\u540c\u6807\u7b7e\u8be6\u7ec6\u7a0b\u5ea6\u548c\u5185\u5bb9\u98ce\u9669\u5bf9\u7528\u6237\u53c2\u4e0e\u5ea6\u4ee5\u53ca\u5bf9AI\u751f\u6210\u56fe\u50cf\u611f\u77e5\u7684\u5f71\u54cd\u3002", "result": "\u8be5\u7814\u7a76\u53d1\u73b0\uff0c\u63d0\u5347\u6807\u7b7e\u7684\u8be6\u7ec6\u7a0b\u5ea6\u53ef\u4ee5\u589e\u5f3a\u7528\u6237\u5bf9\u6807\u7b7e\u900f\u660e\u5ea6\u7684\u611f\u77e5\uff0c\u4f46\u672a\u5bf9\u7528\u6237\u53c2\u4e0e\u5ea6\u4ea7\u751f\u5f71\u54cd\u3002\u5185\u5bb9\u7684\u98ce\u9669\u7b49\u7ea7\u5bf9\u7528\u6237\u53c2\u4e0e\u5ea6\u548c\u4fe1\u4efb\u611f\u6709\u663e\u8457\u5f71\u54cd\uff0c\u7528\u6237\u5728\u4f4e\u98ce\u9669\u56fe\u50cf\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u53c2\u4e0e\u5ea6\u548c\u4fe1\u4efb\u611f\u3002", "conclusion": "\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u53ef\u4ee5\u91c7\u7528\u8be6\u7ec6\u6807\u7b7e\u4ee5\u63d0\u9ad8\u900f\u660e\u5ea6\uff0c\u800c\u4e0d\u5f71\u54cd\u7528\u6237\u53c2\u4e0e\u5ea6\uff0c\u5efa\u8baeAI\u751f\u6210\u5185\u5bb9\u7684\u6709\u6548\u6807\u7b7e\u7b56\u7565\u3002"}}
{"id": "2510.19054", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.19054", "abs": "https://arxiv.org/abs/2510.19054", "authors": ["Shiyu Liu", "Ilija Hadzic", "Akshay Gupta", "Aliasghar Arab"], "title": "Motion Planning and Control of an Overactuated 4-Wheel Drive with Constrained Independent Steering", "comment": "7 pages, 5 figures, 3 tables, video available at\n  https://youtu.be/8l9s2Wb_vec, To appear at IEEE 2025 International Conference\n  on Advanced Robotics", "summary": "This paper addresses motion planning and con- trol of an overactuated 4-wheel\ndrive train with independent steering (4WIS) where mechanical constraints\nprevent the wheels from executing full 360-degree rotations (swerve). The\nconfiguration space of such a robot is constrained and contains discontinuities\nthat affect the smoothness of the robot motion. We introduce a mathematical\nformulation of the steering constraints and derive discontinuity planes that\npartition the velocity space into regions of smooth and efficient motion. We\nfurther design the motion planner for path tracking and ob- stacle avoidance\nthat explicitly accounts for swerve constraints and the velocity transition\nsmoothness. The motion controller uses local feedback to generate actuation\nfrom the desired velocity, while properly handling the discontinuity crossing\nby temporarily stopping the motion and repositioning the wheels. We implement\nthe proposed motion planner as an extension to ROS Navigation package and\nevaluate the system in simulation and on a physical robot.", "AI": {"tldr": "\u672c\u8bba\u6587\u9488\u5bf9\u5177\u6709\u8f6c\u5411\u7ea6\u675f\u7684\u56db\u8f6e\u9a71\u52a8\u8f66\u8f86\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u8fd0\u52a8\u89c4\u5212\u548c\u63a7\u5236\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5e73\u6ed1\u7684\u8def\u5f84\u8ddf\u8e2a\u548c\u969c\u788d\u7269\u907f\u8ba9\u3002", "motivation": "\u89e3\u51b3\u7531\u4e8e\u673a\u68b0\u7ea6\u675f\u5bfc\u81f4\u7684\u8fd0\u52a8\u4e0d\u5e73\u6ed1\u548c\u63a7\u5236\u95ee\u9898\uff0c\u63d0\u9ad8\u56db\u8f6e\u9a71\u52a8\u72ec\u7acb\u8f6c\u5411\u7cfb\u7edf\u7684\u8fd0\u52a8\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8fd0\u52a8\u89c4\u5212\u548c\u63a7\u5236\u65b9\u6cd5\uff0c\u9488\u5bf9\u5177\u72ec\u7acb\u8f6c\u5411\u7684\u56db\u8f6e\u9a71\u52a8\uff084WIS\uff09\u8f66\u8f86\u3002\u5728\u8003\u8651\u673a\u68b0\u7ea6\u675f\u4e0b\uff0c\u8be5\u8f66\u8f86\u7684\u8f6e\u5b50\u65e0\u6cd5\u6267\u884c\u5b8c\u6574\u7684360\u5ea6\u65cb\u8f6c\u3002", "result": "\u63d0\u51fa\u7684\u8fd0\u52a8\u89c4\u5212\u5668\u80fd\u591f\u5b9e\u73b0\u7cbe\u786e\u7684\u8def\u5f84\u8ddf\u8e2a\u548c\u969c\u788d\u7269\u907f\u8ba9\uff0c\u540c\u65f6\u8003\u8651\u4e86\u8f6c\u5411\u7ea6\u675f\u548c\u901f\u5ea6\u8f6c\u6362\u7684\u5e73\u6ed1\u6027\uff0c\u5e76\u5728\u4eff\u771f\u548c\u5b9e\u9645\u673a\u5668\u4eba\u4e0a\u5f97\u5230\u9a8c\u8bc1\u3002", "conclusion": "\u5728ROS\u5bfc\u822a\u5305\u7684\u57fa\u7840\u4e0a\u5b9e\u73b0\u4e86\u8be5\u8fd0\u52a8\u89c4\u5212\u5668\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u8fd0\u52a8\u7684\u5e73\u6ed1\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2510.19031", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.19031", "abs": "https://arxiv.org/abs/2510.19031", "authors": ["Akilan Amithasagaran", "Sagnik Dakshit", "Bhavani Suryadevara", "Lindsey Stockton"], "title": "CLiVR: Conversational Learning System in Virtual Reality with AI-Powered Patients", "comment": null, "summary": "Simulations constitute a fundamental component of medical and nursing\neducation and traditionally employ standardized patients (SP) and high-fidelity\nmanikins to develop clinical reasoning and communication skills. However, these\nmethods require substantial resources, limiting accessibility and scalability.\nIn this study, we introduce CLiVR, a Conversational Learning system in Virtual\nReality that integrates large language models (LLMs), speech processing, and 3D\navatars to simulate realistic doctor-patient interactions. Developed in Unity\nand deployed on the Meta Quest 3 platform, CLiVR enables trainees to engage in\nnatural dialogue with virtual patients. Each simulation is dynamically\ngenerated from a syndrome-symptom database and enhanced with sentiment analysis\nto provide feedback on communication tone. Through an expert user study\ninvolving medical school faculty (n=13), we assessed usability, realism, and\nperceived educational impact. Results demonstrated strong user acceptance, high\nconfidence in educational potential, and valuable feedback for improvement.\nCLiVR offers a scalable, immersive supplement to SP-based training.", "AI": {"tldr": "CLiVR\u662f\u4e00\u4e2a\u865a\u62df\u73b0\u5b9e\u7cfb\u7edf\uff0c\u901a\u8fc7\u5bf9\u8bdd\u5b66\u4e60\u6280\u672f\u548c\u60c5\u611f\u5206\u6790\uff0c\u6539\u8fdb\u4f20\u7edf\u533b\u5b66\u6559\u80b2\u4e2d\u7684\u6a21\u62df\u8bad\u7ec3\u3002", "motivation": "\u4f20\u7edf\u7684\u6a21\u62df\u60a3\u8005\u57f9\u8bad\u65b9\u6cd5\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u96be\u4ee5\u63a8\u5e7f\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u53ef\u62d3\u5c55\u7684\u8bad\u7ec3\u65b9\u6848\u3002", "method": "\u4f7f\u7528Unity\u5f00\u53d1\uff0c\u57fa\u4e8e\u7efc\u5408\u75c7-\u75c7\u72b6\u6570\u636e\u5e93\u52a8\u6001\u751f\u6210\u6a21\u62df\u573a\u666f\uff0c\u5e76\u8fdb\u884c\u60c5\u611f\u5206\u6790\u4ee5\u63d0\u4f9b\u4ea4\u6d41\u53cd\u9988\u3002", "result": "\u672c\u7814\u7a76\u5f00\u53d1\u7684CLiVR\u7cfb\u7edf\uff0c\u5728\u865a\u62df\u73b0\u5b9e\u4e2d\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c3D\u5316\u8eab\uff0c\u65e8\u5728\u63d0\u4f9b\u66f4\u5177\u53ef\u6269\u5c55\u6027\u548c\u6c89\u6d78\u611f\u7684\u533b\u5b66\u6559\u80b2\u8bad\u7ec3\u3002", "conclusion": "CLiVR\u7cfb\u7edf\u5728\u533b\u7597\u6559\u80b2\u4e2d\u663e\u793a\u51fa\u826f\u597d\u7684\u7528\u6237\u63a5\u53d7\u5ea6\u548c\u826f\u597d\u7684\u6559\u80b2\u6f5c\u529b\uff0c\u8868\u660e\u5176\u4f5c\u4e3a\u6807\u51c6\u5316\u60a3\u8005\u8bad\u7ec3\u7684\u53ef\u6269\u5c55\u8865\u5145\u3002"}}
{"id": "2510.19058", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.19058", "abs": "https://arxiv.org/abs/2510.19058", "authors": ["Fausto Vega", "Jon Arrizabalaga", "Ryan Watson", "Zachary Manchester"], "title": "Convex Maneuver Planning for Spacecraft Collision Avoidance", "comment": "8 pages, 6 figures, Accepted to International Space Robotics\n  Conference", "summary": "Conjunction analysis and maneuver planning for spacecraft collision avoidance\nremains a manual and time-consuming process, typically involving repeated\nforward simulations of hand-designed maneuvers. With the growing density of\nsatellites in low-Earth orbit (LEO), autonomy is becoming essential for\nefficiently evaluating and mitigating collisions. In this work, we present an\nalgorithm to design low-thrust collision-avoidance maneuvers for short-term\nconjunction events. We first formulate the problem as a nonconvex\nquadratically-constrained quadratic program (QCQP), which we then relax into a\nconvex semidefinite program (SDP) using Shor's relaxation. We demonstrate\nempirically that the relaxation is tight, which enables the recovery of\nglobally optimal solutions to the original nonconvex problem. Our formulation\nproduces a minimum-energy solution while ensuring a desired probability of\ncollision at the time of closest approach. Finally, if the desired probability\nof collision cannot be satisfied, we relax this constraint into a penalty,\nyielding a minimum-risk solution. We validate our algorithm with a\nhigh-fidelity simulation of a satellite conjunction in low-Earth orbit with a\nsimulated conjunction data message (CDM), demonstrating its effectiveness in\nreducing collision risk.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u81ea\u52a8\u5316\u7b97\u6cd5\uff0c\u7528\u4e8e\u8bbe\u8ba1\u4f4e\u63a8\u529b\u7684\u822a\u5929\u5668\u78b0\u649e\u907f\u514d\u673a\u52a8\uff0c\u9002\u5e94\u65e5\u76ca\u589e\u957f\u7684\u4f4e\u5730\u7403\u8f68\u9053\u536b\u661f\u5bc6\u5ea6\u3002", "motivation": "\u968f\u7740\u4f4e\u5730\u7403\u8f68\u9053\u536b\u661f\u6570\u91cf\u589e\u52a0\uff0c\u4f20\u7edf\u7684\u822a\u5929\u5668\u78b0\u649e\u907f\u514d\u65b9\u6cd5\u8017\u65f6\u4e14\u4f9d\u8d56\u4eba\u5de5\uff0c\u8feb\u5207\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u975e\u51f9\u4e8c\u6b21\u7ea6\u675f\u4e8c\u6b21\u89c4\u5212\uff08QCQP\uff09\uff0c\u5e76\u901a\u8fc7Shor\u677e\u5f1b\u8f6c\u5316\u4e3a\u51f8\u534a\u6b63\u5b9a\u89c4\u5212\uff08SDP\uff09\uff0c\u4ece\u800c\u83b7\u5f97\u539f\u59cb\u95ee\u9898\u7684\u5168\u5c40\u6700\u4f18\u89e3\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f4e\u63a8\u529b\u7684\u77ed\u671f\u78b0\u649e\u907f\u514d\u673a\u52a8\u8bbe\u8ba1\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u9ad8\u4fdd\u771f\u6a21\u62df\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u7b97\u6cd5\u80fd\u591f\u5728\u786e\u4fdd\u78b0\u649e\u6982\u7387\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u6700\u4f4e\u80fd\u91cf\u548c\u6700\u4f4e\u98ce\u9669\u7684\u78b0\u649e\u907f\u514d\u7b56\u7565\uff0c\u4e14\u6a21\u62df\u7ed3\u679c\u8868\u660e\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.19033", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19033", "abs": "https://arxiv.org/abs/2510.19033", "authors": ["Andrew Anderson", "Fatima A. Moussaoui", "Jimena Noa Guevara", "Md Montaser Hamid", "Margaret Burnett"], "title": "\"Over-the-Hood\" AI Inclusivity Bugs and How 3 AI Product Teams Found and Fixed Them", "comment": null, "summary": "While much research has shown the presence of AI's \"under-the-hood\" biases\n(e.g., algorithmic, training data, etc.), what about \"over-the-hood\"\ninclusivity biases: barriers in user-facing AI products that disproportionately\nexclude users with certain problem-solving approaches? Recent research has\nbegun to report the existence of such biases -- but what do they look like, how\nprevalent are they, and how can developers find and fix them? To find out, we\nconducted a field study with 3 AI product teams, to investigate what kinds of\nAI inclusivity bugs exist uniquely in user-facing AI products, and whether/how\nAI product teams might harness an existing (non-AI-oriented) inclusive design\nmethod to find and fix them. The teams' work resulted in identifying 6 types of\nAI inclusivity bugs arising 83 times, fixes covering 47 of these bug instances,\nand a new variation of the GenderMag inclusive design method, GenderMag-for-AI,\nthat is especially effective at detecting certain kinds of AI inclusivity bugs.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7528\u6237\u9762\u5bf9\u7684AI\u4ea7\u54c1\u4e2d\u7684\u5305\u5bb9\u6027\u504f\u89c1\uff0c\u53d1\u73b0\u5e76\u4fee\u590d\u4e86\u591a\u79cdAI\u5305\u5bb9\u6027\u9519\u8bef\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u8bbe\u8ba1\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76AI\u4ea7\u54c1\u4e2d\u7684\u7528\u6237\u9762\u5bf9\u7684\u5305\u5bb9\u6027\u504f\u89c1\uff0c\u7279\u522b\u662f\u5982\u4f55\u6392\u9664\u67d0\u4e9b\u7528\u6237\u7684\u89e3\u51b3\u95ee\u9898\u65b9\u6cd5\u7684\u969c\u788d\u3002", "method": "\u901a\u8fc7\u4e0e3\u4e2aAI\u4ea7\u54c1\u56e2\u961f\u8fdb\u884c\u73b0\u573a\u7814\u7a76\uff0c\u63a2\u7d22\u7528\u6237\u9762\u5bf9\u7684AI\u4ea7\u54c1\u4e2d\u7684\u5305\u5bb9\u6027\u9519\u8bef\u53ca\u5176\u4fee\u590d\u65b9\u6cd5\u3002", "result": "\u8bc6\u522b\u51fa6\u79cdAI\u5305\u5bb9\u6027\u9519\u8bef\uff0c\u5171\u51fa\u73b083\u6b21\uff0c\u5e76\u4fee\u590d\u4e8647\u4e2a\u5b9e\u4f8b\uff0c\u540c\u65f6\u5f00\u53d1\u4e86\u65b0\u7684GenderMag-for-AI\u65b9\u6cd5\u3002", "conclusion": "AI\u4ea7\u54c1\u56e2\u961f\u53ef\u4ee5\u5229\u7528GenderMag-for-AI\u65b9\u6cd5\u6709\u6548\u5730\u8bc6\u522b\u548c\u4fee\u590d\u5305\u5bb9\u6027\u9519\u8bef\u3002"}}
{"id": "2510.19068", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.19068", "abs": "https://arxiv.org/abs/2510.19068", "authors": ["Shifa Sulaiman", "Mohammad Gohari", "Francesco Schetter", "Fanny Ficuciello"], "title": "A Learning-based Model Reference Adaptive Controller Implemented on a Prosthetic Hand Wrist", "comment": "International Conference on Social Robotics + AI", "summary": "The functionality and natural motion of prosthetic hands remain limited by\nthe challenges in controlling compliant wrist mechanisms. Current control\nstrategies often lack adaptability and incur high computational costs, which\nimpedes real-time deployment in assistive robotics. To address this gap, this\nstudy presents a computationally efficient Neural Network (NN)-based Model\nReference Adaptive Controller (MRAC) for a tendon-driven soft continuum wrist\nintegrated with a prosthetic hand. The dynamic modeling of the wrist is\nformulated using Timoshenko beam theory, capturing both shear and bending\ndeformations. The proposed NN-MRAC estimates the required tendon forces from\ndeflection errors and minimizes deviation from a reference model through online\nadaptation. Simulation results demonstrate improved precision with a root mean\nsquare error (RMSE) of $6.14 \\times 10^{-4}$ m and a settling time of $3.2$s.\nExperimental validations confirm real-time applicability, with an average RMSE\nof $5.66 \\times 10^{-3}$ m, steady-state error of $8.05 \\times 10^{-3}$ m, and\nsettling time of $1.58$ s. These results highlight the potential of the\ncontroller to enhance motion accuracy and responsiveness in soft prosthetic\nsystems, thereby advancing the integration of adaptive intelligent control in\nwearable assistive devices.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u5668\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u5047\u80a2\u624b\u8155\u7684\u63a7\u5236\u7cbe\u5ea6\u548c\u5b9e\u65f6\u6027\u80fd\uff0c\u4e3a\u53ef\u7a7f\u6234\u52a9\u529b\u8bbe\u5907\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u73b0\u6709\u63a7\u5236\u7b56\u7565\u5728\u5b9e\u65f6\u90e8\u7f72\u4e2d\u7f3a\u4e4f\u9002\u5e94\u6027\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u7684\u95ee\u9898\uff0c\u63d0\u5347\u5047\u80a2\u624b\u7684\u529f\u80fd\u6027\u548c\u81ea\u7136\u8fd0\u52a8\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u6a21\u578b\u53c2\u8003\u81ea\u9002\u5e94\u63a7\u5236\u5668\uff0c\u7ed3\u5408\u4e86\u987a\u5e94\u6027\u8155\u673a\u5236\u548c\u5047\u80a2\u624b\uff0c\u5229\u7528Timoshenko\u6881\u7406\u8bba\u8fdb\u884c\u52a8\u6001\u5efa\u6a21\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793aRMSE\u4e3a$6.14 \times 10^{-4}$ m\uff0c\u7a33\u5b9a\u65f6\u95f4\u4e3a$3.2$s\uff1b\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\u5e73\u5747RMSE\u4e3a$5.66 \times 10^{-3}$ m\uff0c\u7a33\u6001\u8bef\u5dee\u4e3a$8.05 \times 10^{-3}$ m\uff0c\u7a33\u5b9a\u65f6\u95f4\u4e3a$1.58$s\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u6a21\u578b\u53c2\u8003\u81ea\u9002\u5e94\u63a7\u5236\u5668\uff0c\u80fd\u591f\u63d0\u9ad8\u8f6f\u673a\u5668\u4eba\u624b\u90e8\u7684\u8fd0\u52a8\u7cbe\u5ea6\u548c\u54cd\u5e94\u80fd\u529b\uff0c\u63a8\u52a8\u53ef\u7a7f\u6234\u8f85\u52a9\u8bbe\u5907\u7684\u667a\u80fd\u63a7\u5236\u6574\u5408\u3002"}}
{"id": "2510.19086", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.19086", "abs": "https://arxiv.org/abs/2510.19086", "authors": ["Joshua Nijiati Alimujiang"], "title": "When Strings Tug at Algorithm: Human-AI Sovereignty and Entanglement in Nomadic Improvisational Music Performance as a Decolonial Exploration", "comment": "14 pages, 5 figures", "summary": "As emergent artificial intelligence technologies increasingly assert roles as\nassistants within intangible cultural heritage contexts, researchers and\nartists observe existing questions on the theme of agency negotiation, cultural\nresistance, and technical critique. This research interrogates power dynamics\nin human-AI sovereignty and entanglement for nomadic improvisational Dutar\nperformance, a living cultural heritage through a long-necked lute from the\nCentral Asia region. To investigate tensions between human agency and\ncomputational hegemony, the researcher and artists examined and iterated a\nfeedback workflow that captures live performance data, processes digital\ntransformations, and creates a real-time interactive art experience via\nimmersive environments. Empirical data from artists and audience reveal\nmodulations where musicians selectively embrace or reject algorithmic\nsuggestions to preserve creative identity. The author concludes that decolonial\npotential requires redesigning tools or systems for cultural survivance, where\ntechnology becomes not merely a feedback environment but a site for decolonial\npraxis, challenging computational hegemony in digital ecosystems.", "AI": {"tldr": "\u7814\u7a76\u4eba\u7c7b\u4e0eAI\u7684\u6743\u529b\u52a8\u6001\uff0c\u7279\u522b\u662f\u5728\u6587\u5316\u9057\u4ea7\u80cc\u666f\u4e0b\uff0c\u5f3a\u8c03\u521b\u4f5c\u8005\u7684\u81ea\u4e3b\u6027\u548c\u53bb\u6b96\u6c11\u5316\u5b9e\u8df5\u3002", "motivation": "\u63a2\u8ba8\u4eba\u7c7b\u4e0e\u4eba\u5de5\u667a\u80fd\u4e4b\u95f4\u7684\u6743\u529b\u52a8\u6001\uff0c\u7279\u522b\u662f\u5728\u6e38\u7267\u5373\u5174\u8868\u6f14\u7684\u80cc\u666f\u4e0b", "method": "\u5b9e\u8bc1\u7814\u7a76\u4e0e\u827a\u672f\u521b\u4f5c", "result": "\u53d1\u73b0\u97f3\u4e50\u5bb6\u5728\u521b\u4f5c\u8fc7\u7a0b\u4e2d\u9009\u62e9\u6027\u5730\u63a5\u53d7\u6216\u62d2\u7edd\u7b97\u6cd5\u5efa\u8bae\uff0c\u4ee5\u7ef4\u62a4\u4ed6\u4eec\u7684\u521b\u4f5c\u8eab\u4efd", "conclusion": "\u53bb\u6b96\u6c11\u5316\u7684\u6f5c\u529b\u8981\u6c42\u91cd\u65b0\u8bbe\u8ba1\u6587\u5316\u751f\u5b58\u5de5\u5177\uff0c\u4f7f\u6280\u672f\u6210\u4e3a\u53bb\u6b96\u6c11\u5b9e\u8df5\u7684\u573a\u6240\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u53cd\u9988\u73af\u5883\u3002"}}
{"id": "2510.19074", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.19074", "abs": "https://arxiv.org/abs/2510.19074", "authors": ["Yilang Liu", "Haoxiang You", "Ian Abraham"], "title": "Sample-Based Hybrid Mode Control: Asymptotically Optimal Switching of Algorithmic and Non-Differentiable Control Modes", "comment": null, "summary": "This paper investigates a sample-based solution to the hybrid mode control\nproblem across non-differentiable and algorithmic hybrid modes. Our approach\nreasons about a set of hybrid control modes as an integer-based optimization\nproblem where we select what mode to apply, when to switch to another mode, and\nthe duration for which we are in a given control mode. A sample-based variation\nis derived to efficiently search the integer domain for optimal solutions. We\nfind our formulation yields strong performance guarantees that can be applied\nto a number of robotics-related tasks. In addition, our approach is able to\nsynthesize complex algorithms and policies to compound behaviors and achieve\nchallenging tasks. Last, we demonstrate the effectiveness of our approach in\nreal-world robotic examples that require reactive switching between long-term\nplanning and high-frequency control.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u6837\u672c\u57fa\u7840\u7684\u6df7\u5408\u63a7\u5236\u6a21\u5f0f\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u4efb\u52a1\uff0c\u5177\u6709\u6548\u80fd\u4fdd\u8bc1\u3002", "motivation": "\u89e3\u51b3\u975e\u53ef\u5fae\u548c\u7b97\u6cd5\u6df7\u5408\u6a21\u5f0f\u4e0b\u7684\u63a7\u5236\u95ee\u9898", "method": "\u57fa\u4e8e\u6837\u672c\u7684\u6df7\u5408\u6a21\u5f0f\u63a7\u5236\u65b9\u6cd5", "result": "\u901a\u8fc7\u6574\u6570\u4f18\u5316\u9009\u62e9\u63a7\u5236\u6a21\u5f0f\u3001\u5207\u6362\u65f6\u673a\u4e0e\u6301\u7eed\u65f6\u957f\uff0c\u5f97\u5230\u5f3a\u6709\u6548\u7684\u6027\u80fd\u4fdd\u8bc1\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u673a\u5668\u4eba\u4efb\u52a1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u9700\u8981\u53cd\u5e94\u5f0f\u5207\u6362\u7684\u957f\u65f6\u95f4\u89c4\u5212\u548c\u9ad8\u9891\u63a7\u5236\u7684\u5b9e\u9645\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u8bc1\u660e\u4e86\u6709\u6548\u6027\u3002"}}
{"id": "2510.19252", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.19252", "abs": "https://arxiv.org/abs/2510.19252", "authors": ["Yingtian Shi", "Jinda Yang", "Yuhan Wang", "Yiwen Yin", "Haoyu Li", "Kunyu Gao", "Chun Yu"], "title": "LLMartini: Seamless and Interactive Leveraging of Multiple LLMs through Comparison and Composition", "comment": null, "summary": "The growing diversity of large language models (LLMs) means users often need\nto compare and combine outputs from different models to obtain higher-quality\nor more comprehensive responses. However, switching between separate interfaces\nand manually integrating outputs is inherently inefficient, leading to a high\ncognitive burden and fragmented workflows. To address this, we present\nLLMartini, a novel interactive system that supports seamless comparison,\nselection, and intuitive cross-model composition tools. The system decomposes\nresponses into semantically aligned segments based on task-specific criteria,\nautomatically merges consensus content, and highlights model differences\nthrough color coding while preserving unique contributions. In a user study\n(N=18), LLMartini significantly outperformed conventional manual methods across\nall measured metrics, including task completion time, cognitive load, and user\nsatisfaction. Our work highlights the importance of human-centered design in\nenhancing the efficiency and creativity of multi-LLM interactions and offers\npractical implications for leveraging the complementary strengths of various\nlanguage models.", "AI": {"tldr": "LLMartini\u662f\u4e00\u4e2a\u4e92\u52a8\u7cfb\u7edf\uff0c\u65e8\u5728\u7b80\u5316\u591a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u95f4\u7684\u6bd4\u8f83\u548c\u7ec4\u5408\uff0c\u901a\u8fc7\u81ea\u52a8\u5408\u5e76\u5171\u8bc6\u5185\u5bb9\u548c\u9ad8\u4eae\u6a21\u578b\u5dee\u5f02\u6765\u63d0\u9ad8\u6548\u7387\uff0c\u7528\u6237\u7814\u7a76\u663e\u793a\u5176\u6548\u679c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u7528\u6237\u9700\u8981\u5728\u4e0d\u540c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e4b\u95f4\u6bd4\u8f83\u548c\u6574\u5408\u8f93\u51fa\uff0c\u4ee5\u83b7\u5f97\u66f4\u9ad8\u8d28\u91cf\u7684\u54cd\u5e94\uff0c\u4f46\u624b\u52a8\u6574\u5408\u8f93\u51fa\u6548\u7387\u4f4e\u4e0b\uff0c\u5bfc\u81f4\u8ba4\u77e5\u8d1f\u62c5\u589e\u52a0\u3002", "method": "LLMartini\u7cfb\u7edf\u7684\u5f00\u53d1\u4e0e\u7528\u6237\u7814\u7a76", "result": "LLMartini\u5728\u7528\u6237\u7814\u7a76\u4e2d\u663e\u8457\u8d85\u8fc7\u4f20\u7edf\u624b\u52a8\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u3001\u51cf\u5c11\u4e86\u8ba4\u77e5\u8d1f\u62c5\uff0c\u589e\u52a0\u4e86\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u4ee5\u4eba\u4e3a\u672c\u7684\u8bbe\u8ba1\u5728\u63d0\u9ad8\u591a\u8bed\u8a00\u6a21\u578b\u4ea4\u4e92\u6548\u7387\u548c\u521b\u9020\u529b\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5bf9\u6709\u6548\u5229\u7528\u4e0d\u540c\u8bed\u8a00\u6a21\u578b\u7684\u4e92\u8865\u4f18\u52bf\u63d0\u51fa\u4e86\u5b9e\u9645\u5efa\u8bae\u3002"}}
{"id": "2510.19081", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.19081", "abs": "https://arxiv.org/abs/2510.19081", "authors": ["Shifa Sulaiman", "Tobias Busk Jensen", "Stefan Hein Bengtson", "Simon B\u00f8gh"], "title": "Kinematic Analysis and Integration of Vision Algorithms for a Mobile Manipulator Employed Inside a Self-Driving Laboratory", "comment": "International Journal of Intelligent Robotics and Applications 2025", "summary": "Recent advances in robotics and autonomous systems have broadened the use of\nrobots in laboratory settings, including automated synthesis, scalable reaction\nworkflows, and collaborative tasks in self-driving laboratories (SDLs). This\npaper presents a comprehensive development of a mobile manipulator designed to\nassist human operators in such autonomous lab environments. Kinematic modeling\nof the manipulator is carried out based on the Denavit Hartenberg (DH)\nconvention and inverse kinematics solution is determined to enable precise and\nadaptive manipulation capabilities. A key focus of this research is enhancing\nthe manipulator ability to reliably grasp textured objects as a critical\ncomponent of autonomous handling tasks. Advanced vision-based algorithms are\nimplemented to perform real-time object detection and pose estimation, guiding\nthe manipulator in dynamic grasping and following tasks. In this work, we\nintegrate a vision method that combines feature-based detection with\nhomography-driven pose estimation, leveraging depth information to represent an\nobject pose as a $2$D planar projection within $3$D space. This adaptive\ncapability enables the system to accommodate variations in object orientation\nand supports robust autonomous manipulation across diverse environments. By\nenabling autonomous experimentation and human-robot collaboration, this work\ncontributes to the scalability and reproducibility of next-generation chemical\nlaboratories", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u79fb\u52a8\u64cd\u63a7\u5668\uff0c\u80fd\u591f\u901a\u8fc7\u5148\u8fdb\u7684\u89c6\u89c9\u7b97\u6cd5\u548c\u8fd0\u52a8\u5b66\u5efa\u6a21\uff0c\u5728\u52a8\u6001\u6293\u53d6\u548c\u8ddf\u968f\u4efb\u52a1\u4e2d\u63d0\u9ad8\u673a\u5668\u4eba\u5bf9\u7eb9\u7406\u7269\u4f53\u7684\u6293\u53d6\u80fd\u529b\uff0c\u4fc3\u8fdb\u5b9e\u9a8c\u5ba4\u7684\u81ea\u4e3b\u5b9e\u9a8c\u548c\u4eba\u673a\u534f\u4f5c\u3002", "motivation": "\u63d0\u9ad8\u5b9e\u9a8c\u5ba4\u4e2d\u673a\u5668\u4eba\u5728\u81ea\u52a8\u5316\u5408\u6210\u3001\u53ef\u6269\u5c55\u53cd\u5e94\u5de5\u4f5c\u6d41\u7a0b\u548c\u534f\u4f5c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u80fd\u529b\u3002", "method": "\u57fa\u4e8eDenavit Hartenberg\uff08DH\uff09\u89c4\u5219\u8fdb\u884c\u8fd0\u52a8\u5b66\u5efa\u6a21\uff0c\u5e94\u7528\u9006\u8fd0\u52a8\u5b66\u89e3\u51b3\u65b9\u6848\u4ee5\u53ca\u89c6\u89c9\u65b9\u6cd5\u7ed3\u5408\u7279\u5f81\u68c0\u6d4b\u4e0e\u5355\u5e94\u6027\u9a71\u52a8\u7684\u59ff\u6001\u4f30\u8ba1\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u79cd\u79fb\u52a8\u64cd\u63a7\u5668\uff0c\u80fd\u591f\u5728\u81ea\u4e3b\u5b9e\u9a8c\u5ba4\u73af\u5883\u4e2d\u534f\u52a9\u4eba\u7c7b\u64cd\u4f5c\u5458\u3002", "conclusion": "\u8be5\u7814\u7a76\u4f7f\u5f97\u673a\u5668\u4eba\u80fd\u591f\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u8fdb\u884c\u6709\u6548\u7684\u81ea\u4e3b\u64cd\u4f5c\uff0c\u589e\u5f3a\u4e86\u4e0b\u4e00\u4ee3\u5316\u5b66\u5b9e\u9a8c\u5ba4\u7684\u53ef\u6269\u5c55\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2510.19351", "categories": ["cs.HC", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.19351", "abs": "https://arxiv.org/abs/2510.19351", "authors": ["Nilesh Ramgolam", "Gustavo Carneiro", "Hsiang-Ting", "Chen"], "title": "Learning To Defer To A Population With Limited Demonstrations", "comment": "Accepted to IEEE DICTA 2025 (poster). 7 pages, 2 figures", "summary": "This paper addresses the critical data scarcity that hinders the practical\ndeployment of learning to defer (L2D) systems to the population. We introduce a\ncontext-aware, semi-supervised framework that uses meta-learning to generate\nexpert-specific embeddings from only a few demonstrations. We demonstrate the\nefficacy of a dual-purpose mechanism, where these embeddings are used first to\ngenerate a large corpus of pseudo-labels for training, and subsequently to\nenable on-the-fly adaptation to new experts at test-time. The experiment\nresults on three different datasets confirm that a model trained on these\nsynthetic labels rapidly approaches oracle-level performance, validating the\ndata efficiency of our approach. By resolving a key training bottleneck, this\nwork makes adaptive L2D systems more practical and scalable, paving the way for\nhuman-AI collaboration in real-world environments. To facilitate\nreproducibility and address implementation details not covered in the main\ntext, we provide our source code and training configurations at\nhttps://github.com/nil123532/learning-to-defer-to-a-population-with-limited-demonstrations.", "AI": {"tldr": "\u8fd9\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u6807\u7b7e\u6765\u63d0\u5347L2D\u7cfb\u7edf\u7684\u9002\u5e94\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5b66\u4e60\u5ef6\u8fdf(L2D)\u7cfb\u7edf\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u534a\u76d1\u7763\u6846\u67b6\uff0c\u5229\u7528\u5143\u5b66\u4e60\u751f\u6210\u4e13\u5bb6\u7279\u5b9a\u7684\u5d4c\u5165", "result": "\u5728\u4e09\u79cd\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u5408\u6210\u6807\u7b7e\u8bad\u7ec3\u7684\u6a21\u578b\u8fc5\u901f\u63a5\u8fd1oracle\u6c34\u5e73\u7684\u6027\u80fd", "conclusion": "\u901a\u8fc7\u89e3\u51b3\u5173\u952e\u7684\u8bad\u7ec3\u74f6\u9888\uff0c\u6b64\u7814\u7a76\u63d0\u9ad8\u4e86\u81ea\u9002\u5e94L2D\u7cfb\u7edf\u7684\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4fc3\u8fdb\u4e86\u4eba\u7c7b\u4e0eAI\u7684\u534f\u4f5c\u3002"}}
{"id": "2510.19101", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.19101", "abs": "https://arxiv.org/abs/2510.19101", "authors": ["Matthew Jiang", "Shipeng Liu", "Feifei Qian"], "title": "Safe Active Navigation and Exploration for Planetary Environments Using Proprioceptive Measurements", "comment": null, "summary": "Legged robots can sense terrain through force interactions during locomotion,\noffering more reliable traversability estimates than remote sensing and serving\nas scouts for guiding wheeled rovers in challenging environments. However, even\nlegged scouts face challenges when traversing highly deformable or unstable\nterrain. We present Safe Active Exploration for Granular Terrain (SAEGT), a\nnavigation framework that enables legged robots to safely explore unknown\ngranular environments using proprioceptive sensing, particularly where visual\ninput fails to capture terrain deformability. SAEGT estimates the safe region\nand frontier region online from leg-terrain interactions using Gaussian Process\nregression for traversability assessment, with a reactive controller for\nreal-time safe exploration and navigation. SAEGT demonstrated its ability to\nsafely explore and navigate toward a specified goal using only proprioceptively\nestimated traversability in simulation.", "AI": {"tldr": "SAEGT\u662f\u4e00\u4e2a\u56db\u8db3\u673a\u5668\u4eba\u5bfc\u822a\u6846\u67b6\uff0c\u80fd\u591f\u5728\u672a\u77e5\u9897\u7c92\u73af\u5883\u4e2d\u5b89\u5168\u63a2\u7d22\uff0c\u89e3\u51b3\u89c6\u89c9\u4f20\u611f\u5668\u65e0\u6cd5\u6355\u6349\u5730\u5f62\u53d8\u5f62\u7684\u95ee\u9898\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u672c\u4f53\u611f\u77e5\u4f20\u611f\u5668\u63d0\u4f9b\u6bd4\u9065\u611f\u66f4\u53ef\u9760\u7684\u53ef\u901a\u884c\u6027\u4f30\u8ba1\uff0c\u4ee5\u4fbf\u5728\u6311\u6218\u6027\u73af\u5883\u4e2d\u5f15\u5bfc\u8f6e\u5f0f\u6f2b\u6e38\u8005\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u6765\u8bc4\u4f30\u901a\u8fc7\u817f\u90e8\u4e0e\u5730\u9762\u76f8\u4e92\u4f5c\u7528\u5f97\u5230\u7684\u5b89\u5168\u533a\u57df\u548c\u524d\u6cbf\u533a\u57df\uff0c\u5e76\u914d\u5408\u4e00\u79cd\u5b9e\u65f6\u7684\u53cd\u5e94\u5f0f\u63a7\u5236\u5668\u8fdb\u884c\u5b89\u5168\u63a2\u7d22\u548c\u5bfc\u822a\u3002", "result": "SAEGT\u5728\u4eff\u771f\u4e2d\u5c55\u793a\u4e86\u80fd\u591f\u5b89\u5168\u63a2\u7d22\u548c\u5bfc\u822a\uff0c\u4e14\u53ea\u4f9d\u9760\u672c\u4f53\u611f\u77e5\u4f30\u8ba1\u7684\u53ef\u901a\u884c\u6027\u6765\u5b9e\u73b0\u76ee\u6807\u7684\u80fd\u529b\u3002", "conclusion": "SAEGT\u6846\u67b6\u80fd\u591f\u5e2e\u52a9\u56db\u8db3\u673a\u5668\u4eba\u5b89\u5168\u5730\u63a2\u7d22\u672a\u77e5\u7684\u9897\u7c92\u73af\u5883\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u65e0\u6cd5\u6355\u6349\u5730\u5f62\u53d8\u5f62\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2510.19512", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.19512", "abs": "https://arxiv.org/abs/2510.19512", "authors": ["Cedric Faas", "Sophie Kerstan", "Richard Uth", "Markus Langer", "Anna Maria Feit"], "title": "Design Considerations for Human Oversight of AI: Insights from Co-Design Workshops and Work Design Theory", "comment": null, "summary": "As AI systems become increasingly capable and autonomous, domain experts'\nroles are shifting from performing tasks themselves to overseeing AI-generated\noutputs. Such oversight is critical, as undetected errors can have serious\nconsequences or undermine the benefits of AI. Effective oversight, however,\ndepends not only on detecting and correcting AI errors but also on the\nmotivation and engagement of the oversight personnel and the meaningfulness\nthey see in their work. Yet little is known about how domain experts approach\nand experience the oversight task and what should be considered to design\neffective and motivational interfaces that support human oversight. To address\nthese questions, we conducted four co-design workshops with domain experts from\npsychology and computer science. We asked them to first oversee an AI-based\ngrading system, and then discuss their experiences and needs during oversight.\nFinally, they collaboratively prototyped interfaces that could support them in\ntheir oversight task. Our thematic analysis revealed four key user\nrequirements: understanding tasks and responsibilities, gaining insight into\nthe AI's decision-making, contributing meaningfully to the process, and\ncollaborating with peers and the AI. We integrated these empirical insights\nwith the SMART model of work design to develop a generalizable framework of\ntwelve design considerations. Our framework links interface characteristics and\nuser requirements to the psychological processes underlying effective and\nsatisfying work. Being grounded in work design theory, we expect these\nconsiderations to be applicable across domains and discuss how they extend\nexisting guidelines for human-AI interaction and theoretical frameworks for\neffective human oversight by providing concrete guidance on the design of\nengaging and meaningful interfaces that support human oversight of AI systems.", "AI": {"tldr": "AI\u7cfb\u7edf\u7684\u53d1\u5c55\u4f7f\u5f97\u9886\u57df\u4e13\u5bb6\u7684\u89d2\u8272\u8f6c\u53d8\u4e3a\u76d1\u7763AI\u8f93\u51fa\u3002\u672c\u6587\u901a\u8fc7\u4e0e\u9886\u57df\u4e13\u5bb6\u7684\u5408\u4f5c\uff0c\u63d0\u51fa\u4e86\u7528\u6237\u5728\u76d1\u7763\u4e2d\u7684\u5173\u952e\u9700\u6c42\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u4ee5\u6307\u5bfc\u4eba\u673a\u4ea4\u4e92\u754c\u9762\u7684\u8bbe\u8ba1\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u8d8a\u6765\u8d8a\u81ea\u4e3b\uff0c\u9886\u57df\u4e13\u5bb6\u7684\u76d1\u7763\u89d2\u8272\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u800c\u76ee\u524d\u5bf9\u5982\u4f55\u6709\u6548\u8bbe\u8ba1\u652f\u6301\u76d1\u7763\u7684\u63a5\u53e3\u4e86\u89e3\u751a\u5c11\u3002", "method": "\u901a\u8fc7\u4e0e\u5fc3\u7406\u5b66\u548c\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u4e13\u5bb6\u8fdb\u884c\u7684\u56db\u4e2a\u5171\u540c\u8bbe\u8ba1\u5de5\u4f5c\u574a\uff0c\u8fdb\u884c\u7ecf\u9a8c\u4ea4\u6d41\u548c\u754c\u9762\u539f\u578b\u8bbe\u8ba1\u3002", "result": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728AI\u7cfb\u7edf\u65e5\u76ca\u81ea\u4e3b\u7684\u80cc\u666f\u4e0b\uff0c\u9886\u57df\u4e13\u5bb6\u5728\u76d1\u7763AI\u8f93\u51fa\u65b9\u9762\u7684\u4f5c\u7528\uff0c\u4ee5\u53ca\u5982\u4f55\u8bbe\u8ba1\u6709\u6548\u7684\u754c\u9762\u6765\u652f\u6301\u4eba\u7c7b\u76d1\u7763\u5de5\u4f5c\u3002\u901a\u8fc7\u4e0e\u5fc3\u7406\u5b66\u548c\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u4e13\u5bb6\u8fdb\u884c\u7684\u5171\u540c\u8bbe\u8ba1\u5de5\u4f5c\u574a\uff0c\u63d0\u51fa\u4e86\u7528\u6237\u5bf9\u76d1\u63a7\u4efb\u52a1\u7684\u56db\u4e2a\u5173\u952e\u9700\u6c42\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u9700\u6c42\u548cSMART\u5de5\u4f5c\u8bbe\u8ba1\u6a21\u578b\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u63a8\u5e7f\u7684\u8bbe\u8ba1\u6846\u67b6\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7528\u6237\u9700\u6c42\u548c\u5de5\u4f5c\u8bbe\u8ba1\u7406\u8bba\u7684\u6846\u67b6\uff0c\u671f\u671b\u80fd\u4e3a\u4eba\u7c7b\u76d1\u7763AI\u7cfb\u7edf\u7684\u754c\u9762\u8bbe\u8ba1\u63d0\u4f9b\u5b9e\u8d28\u6027\u5efa\u8bae\uff0c\u589e\u5f3a\u5de5\u4f5c\u53c2\u4e0e\u611f\u4e0e\u610f\u4e49\u3002"}}
{"id": "2510.19128", "categories": ["cs.RO", "cs.AI", "68T40 (Primary), 70Q05 (Secondary)"], "pdf": "https://arxiv.org/pdf/2510.19128", "abs": "https://arxiv.org/abs/2510.19128", "authors": ["Mehran Ghafarian Tamizi", "Homayoun Honari", "Amir Mehdi Soufi Enayati", "Aleksey Nozdryn-Plotnicki", "Homayoun Najjaran"], "title": "A Cross-Environment and Cross-Embodiment Path Planning Framework via a Conditional Diffusion Model", "comment": "20 pages, 9 figures", "summary": "Path planning for a robotic system in high-dimensional cluttered environments\nneeds to be efficient, safe, and adaptable for different environments and\nhardware. Conventional methods face high computation time and require extensive\nparameter tuning, while prior learning-based methods still fail to generalize\neffectively. The primary goal of this research is to develop a path planning\nframework capable of generalizing to unseen environments and new robotic\nmanipulators without the need for retraining. We present GADGET (Generalizable\nand Adaptive Diffusion-Guided Environment-aware Trajectory generation), a\ndiffusion-based planning model that generates joint-space trajectories\nconditioned on voxelized scene representations as well as start and goal\nconfigurations. A key innovation is GADGET's hybrid dual-conditioning mechanism\nthat combines classifier-free guidance via learned scene encoding with\nclassifier-guided Control Barrier Function (CBF) safety shaping, integrating\nenvironment awareness with real-time collision avoidance directly in the\ndenoising process. This design supports zero-shot transfer to new environments\nand robotic embodiments without retraining. Experimental results show that\nGADGET achieves high success rates with low collision intensity in\nspherical-obstacle, bin-picking, and shelf environments, with CBF guidance\nfurther improving safety. Moreover, comparative evaluations indicate strong\nperformance relative to both sampling-based and learning-based baselines.\nFurthermore, GADGET provides transferability across Franka Panda, Kinova Gen3\n(6/7-DoF), and UR5 robots, and physical execution on a Kinova Gen3 demonstrates\nits ability to generate safe, collision-free trajectories in real-world\nsettings.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u8def\u5f84\u89c4\u5212\u6846\u67b6GADGET\uff0c\u80fd\u591f\u6709\u6548\u9002\u5e94\u672a\u77e5\u73af\u5883\uff0c\u4e0d\u9700\u91cd\u8bad\u7ec3\uff0c\u5b9e\u73b0\u5b89\u5168\u7684\u78b0\u649e\u907f\u514d\uff0c\u4e14\u5728\u591a\u79cd\u673a\u5668\u4eba\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u8fc1\u79fb\u80fd\u529b\u4e0e\u6027\u80fd\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u8ba1\u7b97\u65f6\u95f4\u957f\u3001\u53c2\u6570\u8c03\u8282\u56f0\u96be\uff0c\u4ee5\u53ca\u73b0\u6709\u5b66\u4e60\u57fa\u7840\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u6cdb\u5316\u7684\u95ee\u9898\u3002", "method": "\u4ecb\u7ecd\u4e86GADGET\uff0c\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u91c7\u7528\u6df7\u5408\u53cc\u6761\u4ef6\u673a\u5236\uff0c\u5c06\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u4e0e\u5206\u7c7b\u5668\u5f15\u5bfc\u63a7\u5236\u969c\u788d\u51fd\u6570\uff08CBF\uff09\u5b89\u5168\u5851\u5f62\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u73af\u5883\u611f\u77e5\u4e0e\u5b9e\u65f6\u78b0\u649e\u907f\u514d\u3002", "result": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8def\u5f84\u89c4\u5212\u6846\u67b6GADGET\uff0c\u53ef\u4ee5\u6709\u6548\u5904\u7406\u9ad8\u7ef4\u6742\u4e71\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u4e0d\u540c\u7684\u73af\u5883\u548c\u786c\u4ef6\u4e0a\u8fdb\u884c\u65e0\u987b\u91cd\u8bad\u7ec3\u7684\u6709\u6548\u8fc1\u79fb\uff0c\u4fdd\u6301\u9ad8\u6210\u529f\u7387\u548c\u4f4e\u78b0\u649e\u5f3a\u5ea6\uff0c\u4e14\u5728\u5404\u79cd\u5b9e\u9a8c\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "GADGET\u5728\u65b0\u73af\u5883\u548c\u591a\u79cd\u673a\u5668\u4eba\u7cfb\u7edf\u95f4\u5177\u6709\u826f\u597d\u7684\u8fc1\u79fb\u80fd\u529b\uff0c\u4e14\u5728\u5b9e\u9645\u7269\u7406\u6267\u884c\u4e2d\u80fd\u751f\u6210\u5b89\u5168\u3001\u65e0\u78b0\u649e\u7684\u8f68\u8ff9\uff0c\u663e\u793a\u51fa\u5176\u9ad8\u6548\u6027\u4e0e\u9002\u5e94\u6027\u3002"}}
{"id": "2510.19532", "categories": ["cs.HC", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.19532", "abs": "https://arxiv.org/abs/2510.19532", "authors": ["Selena Luo", "Mark S. Keller", "Tabassum Kakar", "Lisa Choy", "Nils Gehlenborg"], "title": "EasyVitessce: auto-magically adding interactivity to Scverse single-cell and spatial biology plots", "comment": null, "summary": "EasyVitessce is a Python package that turns existing static Scanpy and\nSpatialData plots into interactive visualizations by virtue of adding a single\nline of Python code. The package uses Vitessce internally to render interactive\nplots, and abstracts away technical details involved with configuration of\nVitessce. The resulting interactive plots can be viewed in computational\nnotebook environments or their configurations can be exported for usage in\nother contexts such as web applications, enhancing the utility of popular\nScverse Python plotting APIs. EasyVitessce is released under the MIT License\nand available on the Python Package Index (PyPI). The source code is publicly\navailable on GitHub.", "AI": {"tldr": "EasyVitessce\u662f\u4e00\u4e2aPython\u5305\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u4e00\u884c\u4ee3\u7801\u5c06\u9759\u6001\u56fe\u8f6c\u5316\u4e3a\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\uff0c\u4fbf\u4e8e\u5728\u8ba1\u7b97\u7b14\u8bb0\u672c\u548c\u7f51\u9875\u5e94\u7528\u4e2d\u4f7f\u7528\u3002", "motivation": "\u63d0\u9ad8Scverse Python\u7ed8\u56feAPI\u7684\u5b9e\u7528\u6027\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u66f4\u8f7b\u677e\u5730\u521b\u5efa\u4ea4\u4e92\u5f0f\u56fe\u5f62", "method": "\u901a\u8fc7\u589e\u52a0\u4e00\u884cPython\u4ee3\u7801\u5c06\u9759\u6001Scanpy\u548cSpatialData\u56fe\u8f6c\u6362\u4e3a\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u9759\u6001\u56fe\u5f62\u7684\u4e92\u52a8\u6027\uff0c\u7528\u6237\u53ea\u9700\u7b80\u5355\u914d\u7f6e\u5373\u53ef\u4f7f\u7528Vitessce\u8fdb\u884c\u53ef\u89c6\u5316", "conclusion": "EasyVitessce\u7b80\u5316\u4e86\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u7684\u521b\u5efa\u8fc7\u7a0b\uff0c\u63d0\u5347\u4e86\u6570\u636e\u53ef\u89c6\u5316\u7684\u4fbf\u6377\u6027\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2510.19200", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.19200", "abs": "https://arxiv.org/abs/2510.19200", "authors": ["Matteo Bortolon", "Nuno Ferreira Duarte", "Plinio Moreno", "Fabio Poiesi", "Jos\u00e9 Santos-Victor", "Alessio Del Bue"], "title": "GRASPLAT: Enabling dexterous grasping through novel view synthesis", "comment": "Accepted IROS 2025", "summary": "Achieving dexterous robotic grasping with multi-fingered hands remains a\nsignificant challenge. While existing methods rely on complete 3D scans to\npredict grasp poses, these approaches face limitations due to the difficulty of\nacquiring high-quality 3D data in real-world scenarios. In this paper, we\nintroduce GRASPLAT, a novel grasping framework that leverages consistent 3D\ninformation while being trained solely on RGB images. Our key insight is that\nby synthesizing physically plausible images of a hand grasping an object, we\ncan regress the corresponding hand joints for a successful grasp. To achieve\nthis, we utilize 3D Gaussian Splatting to generate high-fidelity novel views of\nreal hand-object interactions, enabling end-to-end training with RGB data.\nUnlike prior methods, our approach incorporates a photometric loss that refines\ngrasp predictions by minimizing discrepancies between rendered and real images.\nWe conduct extensive experiments on both synthetic and real-world grasping\ndatasets, demonstrating that GRASPLAT improves grasp success rates up to 36.9%\nover existing image-based methods. Project page:\nhttps://mbortolon97.github.io/grasplat/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7684\u65b0\u6846\u67b6 GRASPLAT \u5229\u7528 RGB \u56fe\u50cf\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u673a\u5668\u4eba\u6293\u53d6\uff0c\u514b\u670d\u4e86 3D \u6570\u636e\u83b7\u53d6\u7684\u56f0\u96be\u3002", "motivation": "\u89e3\u51b3\u591a\u6307\u673a\u5668\u4eba\u624b\u722a\u7684\u7075\u6d3b\u6293\u53d6\u95ee\u9898\uff0c\u514b\u670d\u4f9d\u8d56\u5b8c\u6574 3D \u626b\u63cf\u9884\u6d4b\u6293\u53d6\u59ff\u52bf\u7684\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u5408\u6210\u771f\u5b9e\u7269\u4f53\u4e0e\u624b\u90e8\u4ea4\u4e92\u7684\u9ad8\u4fdd\u771f\u56fe\u50cf\uff0c\u5229\u7528 RGB \u56fe\u50cf\u8bad\u7ec3\u6a21\u578b\uff0c\u7ed3\u5408 3D \u9ad8\u65af\u70b9\u4e91\u6280\u672f\u751f\u6210\u65b0\u7684\u89c6\u56fe\uff0c\u5e76\u4f7f\u7528\u5149\u5ea6\u635f\u5931\u6765\u4f18\u5316\u6293\u53d6\u9884\u6d4b\u3002", "result": "GRASPLAT \u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6293\u53d6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u663e\u8457\u6539\u5584\u4e86\u6293\u53d6\u6210\u529f\u7387\u3002", "conclusion": "GRASPLAT \u63d0\u9ad8\u4e86\u57fa\u4e8e\u56fe\u50cf\u7684\u65b9\u6cd5\u7684\u6293\u53d6\u6210\u529f\u7387\uff0c\u6700\u5927\u63d0\u9ad8\u4e86 36.9%\u3002"}}
{"id": "2510.19604", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.19604", "abs": "https://arxiv.org/abs/2510.19604", "authors": ["Francesco Vona", "Mohamed Amer", "Omar Abdellatif", "Michelle Celina Hallmann", "Maximilian Warsinke", "Adriana-Simona Mihaita", "Jan-Niklas Voigt-Antons"], "title": "Unmanned Aerial Vehicles Control in a Digital Twin: Exploring the Effect of Different Points of View on User Experience in Virtual Reality", "comment": null, "summary": "Controlling Unmanned Aerial Vehicles (UAVs) is a cognitively demanding task,\nwith accidents often arising from insufficient situational awareness,\ninadequate training, and poor user experiences. Providing more intuitive and\nimmersive visual feedback, particularly through Digital Twin technologies,\noffers new opportunities to enhance pilot awareness and overall experience\nquality. In this study, we investigate how different virtual points of view\n(POVs) influence user experience and performance during UAV piloting in Virtual\nReality (VR), utilizing a digital twin that faithfully replicates the\nreal-world flight environment. We developed a VR application that enables\nparticipants to control a physical DJI Mini 4 Pro drone while immersed in a\ndigital twin with four distinct camera perspectives: Baseline View (static\nexternal), First-Person View, Chase View, and Third-Person View. Nineteen\nparticipants completed a series of ring-based obstacle courses from each\nperspective. In addition to objective flight data, we collected standardized\nsubjective assessments of user experience, presence, workload, cybersickness,\nand situational awareness. Quantitative analyses revealed that the First-Person\nView was associated with significantly higher mental demand and effort, greater\ntrajectory deviation, but smoother control inputs compared to the Third-Person\nand Chase perspectives. Complementing these findings, preference data indicated\nthat the Third-Person View was most consistently favored, whereas the\nFirst-Person View elicited polarized reactions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e0d\u540c\u865a\u62df\u89c6\u89d2\u5bf9\u65e0\u4eba\u673a\u64cd\u63a7\u4f53\u9a8c\u7684\u5f71\u54cd\uff0c\u7ed3\u679c\u663e\u793a\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u8981\u6c42\u66f4\u9ad8\u7684\u6ce8\u610f\u529b\u548c\u52aa\u529b\uff0c\u800c\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\u66f4\u53d7\u7528\u6237\u9752\u7750\u3002", "motivation": "\u63d0\u9ad8\u98de\u884c\u5458\u7684\u60c5\u5883\u610f\u8bc6\u548c\u6574\u4f53\u7528\u6237\u4f53\u9a8c\uff0c\u51cf\u5c11\u4eba\u4e3a\u9519\u8bef\u3002", "method": "\u5728\u865a\u62df\u73b0\u5b9e\u4e2d\u4f7f\u7528\u6570\u5b57\u53cc\u80de\u80ce\u6280\u672f\u7814\u7a76\u4e0d\u540c\u89c6\u89d2\u5bf9\u65e0\u4eba\u673a\u98de\u884c\u63a7\u5236\u7684\u5f71\u54cd\u3002", "result": "\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u5e26\u6765\u4e86\u66f4\u9ad8\u7684\u5fc3\u7406\u9700\u6c42\u548c\u5de5\u4f5c\u91cf\uff0c\u4f46\u63a7\u5236\u66f4\u6d41\u7545\uff1b\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\u6700\u53d7\u6b22\u8fce\u3002", "conclusion": "\u9009\u62e9\u5408\u9002\u7684\u89c6\u89d2\u5bf9\u4e8e\u63d0\u5347UAV\u64cd\u4f5c\u4f53\u9a8c\u81f3\u5173\u91cd\u8981\uff0c\u7b2c\u4e00\u4eba\u79f0\u548c\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\u5404\u6709\u4f18\u7f3a\u70b9\u3002"}}
{"id": "2510.19268", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19268", "abs": "https://arxiv.org/abs/2510.19268", "authors": ["Mingen Li", "Houjian Yu", "Yixuan Huang", "Youngjin Hong", "Changhyun Choi"], "title": "Hierarchical DLO Routing with Reinforcement Learning and In-Context Vision-language Models", "comment": "8 pages, 6 figures, 3 tables", "summary": "Long-horizon routing tasks of deformable linear objects (DLOs), such as\ncables and ropes, are common in industrial assembly lines and everyday life.\nThese tasks are particularly challenging because they require robots to\nmanipulate DLO with long-horizon planning and reliable skill execution.\nSuccessfully completing such tasks demands adapting to their nonlinear\ndynamics, decomposing abstract routing goals, and generating multi-step plans\ncomposed of multiple skills, all of which require accurate high-level reasoning\nduring execution. In this paper, we propose a fully autonomous hierarchical\nframework for solving challenging DLO routing tasks. Given an implicit or\nexplicit routing goal expressed in language, our framework leverages\nvision-language models~(VLMs) for in-context high-level reasoning to synthesize\nfeasible plans, which are then executed by low-level skills trained via\nreinforcement learning. To improve robustness in long horizons, we further\nintroduce a failure recovery mechanism that reorients the DLO into\ninsertion-feasible states. Our approach generalizes to diverse scenes involving\nobject attributes, spatial descriptions, as well as implicit language commands.\nIt outperforms the next best baseline method by nearly 50% and achieves an\noverall success rate of 92.5% across long-horizon routing scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u81ea\u4e3b\u7684\u5206\u5c42\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u53ef\u53d8\u5f62\u7ebf\u6027\u7269\u4f53\u7684\u957f\u65f6\u95f4\u8def\u7531\u4efb\u52a1\uff0c\u6210\u529f\u7387\u4e3a92.5%\u3002", "motivation": "\u89e3\u51b3\u53ef\u53d8\u5f62\u7ebf\u6027\u7269\u4f53(DLO)\u7684\u957f\u65f6\u95f4\u8def\u7531\u4efb\u52a1\u6240\u9762\u4e34\u7684\u975e\u7ebf\u6027\u52a8\u6001\u548c\u6280\u80fd\u6267\u884c\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5b8c\u5168\u81ea\u4e3b\u7684\u5206\u5c42\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u4f4e\u7ea7\u6280\u80fd\u6267\u884c\u3002", "result": "\u4e0e\u6b21\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u7684\u8868\u73b0\u63d0\u9ad8\u4e86\u8fd150%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u957f\u65f6\u95f4\u8def\u7531\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u603b\u6210\u529f\u7387\u8fbe\u523092.5%\u3002"}}
{"id": "2510.19656", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.19656", "abs": "https://arxiv.org/abs/2510.19656", "authors": ["S M Rakib Ul Karim", "Rownak Ara Rasul", "Tunazzina Sultana"], "title": "Sentiment Analysis of Social Media Data for Predicting Consumer Behavior Trends Using Machine Learning", "comment": "21 pages, 10 figures, conference", "summary": "In the era of rapid technological advancement, social media platforms such as\nTwitter (X) have emerged as indispensable tools for gathering consumer\ninsights, capturing diverse opinions, and understanding public attitudes. This\nresearch applies advanced machine learning methods for sentiment analysis on\nTwitter data, with a focus on predicting consumer trends. Using the\nSentiment140 dataset, the study detects evolving patterns in consumer\npreferences with \"car\" as an example. A structured workflow was used to clean\nand prepare data for analysis. Machine learning models, including Support\nVector Machines (SVM), Naive Bayes, Long Short-Term Memory (LSTM) networks, and\nBidirectional Encoder Representations from Transformers (BERT), were employed\nto classify sentiments and predict trends. Model performance was measured using\naccuracy, precision, recall, and F1 score, with BERT achieving the highest\nresults (Accuracy: 83.48%, Precision: 79.37%, Recall: 90.60%, F1: 84.61).\nResults show that LSTM and BERT effectively capture linguistic and contextual\npatterns, improving prediction accuracy and providing insights into consumer\nbehavior. Temporal analysis revealed sentiment shifts across time, while Named\nEntity Recognition (NER) identified related terms and themes. This research\naddresses challenges like sarcasm detection and multilingual data processing,\noffering a scalable framework for generating actionable consumer insights.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u673a\u5668\u5b66\u4e60\u8fdb\u884cTwitter\u60c5\u611f\u5206\u6790\uff0c\u63ed\u793a\u4e86\u6d88\u8d39\u8005\u8d8b\u52bf\uff0c\u5e76\u5f00\u53d1\u51fa\u53ef\u6269\u5c55\u7684\u6846\u67b6\u4ee5\u5e94\u5bf9\u591a\u8bed\u8a00\u6570\u636e\u548c\u8bbd\u523a\u68c0\u6d4b\u7b49\u6311\u6218\u3002", "motivation": "\u5728\u5feb\u901f\u6280\u672f\u8fdb\u6b65\u7684\u65f6\u4ee3\uff0c\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u5982Twitter\u6210\u4e3a\u83b7\u53d6\u6d88\u8d39\u8005\u6d1e\u5bdf\u548c\u7406\u89e3\u516c\u4f17\u6001\u5ea6\u7684\u91cd\u8981\u5de5\u5177\u3002", "method": "\u4f7f\u7528\u5148\u8fdb\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884cTwitter\u6570\u636e\u7684\u60c5\u611f\u5206\u6790\uff0c\u7279\u522b\u5173\u6ce8\u6d88\u8d39\u8d8b\u52bf\u7684\u9884\u6d4b\u3002", "result": "\u901a\u8fc7\u4f7f\u7528Sentiment140\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u4e86\u4ee5'car'\u4e3a\u4f8b\u7684\u6d88\u8d39\u8005\u504f\u597d\u53d8\u5316\u6a21\u5f0f\uff0cBERT\u6a21\u578b\u5728\u60c5\u611f\u5206\u7c7b\u548c\u8d8b\u52bf\u9884\u6d4b\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "LSTM\u548cBERT\u6a21\u578b\u6709\u6548\u6355\u83b7\u8bed\u8a00\u548c\u4e0a\u4e0b\u6587\u6a21\u5f0f\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u6d88\u8d39\u8005\u884c\u4e3a\u7684\u6d1e\u5bdf\u3002"}}
{"id": "2510.19289", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.19289", "abs": "https://arxiv.org/abs/2510.19289", "authors": ["Kefeng Huang", "Jonathon Pipe", "Alice E. Martin", "Tianyuan Wang", "Barnabas A. Franklin", "Andy M. Tyrrell", "Ian J. S. Fairlamb", "Jihong Zhu"], "title": "TARMAC: A Taxonomy for Robot Manipulation in Chemistry", "comment": null, "summary": "Chemistry laboratory automation aims to increase throughput, reproducibility,\nand safety, yet many existing systems still depend on frequent human\nintervention. Advances in robotics have reduced this dependency, but without a\nstructured representation of the required skills, autonomy remains limited to\nbespoke, task-specific solutions with little capacity to transfer beyond their\ninitial design. Current experiment abstractions typically describe\nprotocol-level steps without specifying the robotic actions needed to execute\nthem. This highlights the lack of a systematic account of the manipulation\nskills required for robots in chemistry laboratories. To address this gap, we\nintroduce TARMAC - a Taxonomy for Robot Manipulation in Chemistry - a\ndomain-specific framework that defines and organizes the core manipulations\nneeded in laboratory practice. Based on annotated teaching-lab demonstrations\nand supported by experimental validation, TARMAC categorizes actions according\nto their functional role and physical execution requirements. Beyond serving as\na descriptive vocabulary, TARMAC can be instantiated as robot-executable\nprimitives and composed into higher-level macros, enabling skill reuse and\nsupporting scalable integration into long-horizon workflows. These\ncontributions provide a structured foundation for more flexible and autonomous\nlaboratory automation. More information is available at\nhttps://tarmac-paper.github.io/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTARMAC\u7684\u5206\u7c7b\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u5316\u5b66\u5b9e\u9a8c\u5ba4\u7684\u81ea\u52a8\u5316\u6c34\u5e73\uff0c\u51cf\u5c11\u4eba\u7c7b\u5e72\u9884\uff0c\u5e76\u63d0\u4f9b\u7075\u6d3b\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u6280\u80fd\u3002", "motivation": "\u65e8\u5728\u51cf\u5c11\u5316\u5b66\u5b9e\u9a8c\u5ba4\u81ea\u52a8\u5316\u4e2d\u5bf9\u4eba\u7c7b\u5e72\u9884\u7684\u4f9d\u8d56\uff0c\u5e76\u63d0\u5347\u73b0\u6709\u673a\u5668\u4eba\u7684\u72ec\u7acb\u6027\u4e0e\u9002\u5e94\u6027\u3002", "method": "\u901a\u8fc7\u6ce8\u91ca\u7684\u6559\u5b66\u5b9e\u9a8c\u5ba4\u6f14\u793a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u4ecb\u7ecd\u4e86\u4e00\u79cd\u9488\u5bf9\u5316\u5b66\u64cd\u63a7\u7684\u5206\u7c7b\u6cd5\u3002", "result": "TARMAC\u6846\u67b6\u6210\u529f\u5206\u7c7b\u5e76\u7ec4\u7ec7\u4e86\u5b9e\u9a8c\u5ba4\u6240\u9700\u7684\u6838\u5fc3\u64cd\u63a7\u6280\u80fd\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u6267\u884c\u66f4\u590d\u6742\u7684\u4efb\u52a1\u3002", "conclusion": "\u63d0\u51fa\u7684TARMAC\u6846\u67b6\u4e3a\u5316\u5b66\u5b9e\u9a8c\u5ba4\u7684\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u548c\u53ef\u6269\u5c55\u7684\u57fa\u7840\uff0c\u4fc3\u8fdb\u4e86\u673a\u5668\u4eba\u6280\u80fd\u7684\u7075\u6d3b\u4f7f\u7528\u548c\u957f\u671f\u5de5\u4f5c\u6d41\u7a0b\u7684\u6574\u5408\u3002"}}
{"id": "2510.19685", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19685", "abs": "https://arxiv.org/abs/2510.19685", "authors": ["Omar Alsaiari", "Nilufar Baghaei", "Jason M. Lodge", "Omid Noroozi", "Dragan Ga\u0161evi\u0107", "Marie Boden", "Hassan Khosravi"], "title": "Directive, Metacognitive or a Blend of Both? A Comparison of AI-Generated Feedback Types on Student Engagement, Confidence, and Outcomes", "comment": null, "summary": "Feedback is one of the most powerful influences on student learning, with\nextensive research examining how best to implement it in educational settings.\nIncreasingly, feedback is being generated by artificial intelligence (AI),\noffering scalable and adaptive responses. Two widely studied approaches are\ndirective feedback, which gives explicit explanations and reduces cognitive\nload to speed up learning, and metacognitive feedback which prompts learners to\nreflect, track their progress, and develop self-regulated learning (SRL)\nskills. While both approaches have clear theoretical advantages, their\ncomparative effects on engagement, confidence, and quality of work remain\nunderexplored. This study presents a semester-long randomised controlled trial\nwith 329 students in an introductory design and programming course using an\nadaptive educational platform. Participants were assigned to receive directive,\nmetacognitive, or hybrid AI-generated feedback that blended elements of both\ndirective and metacognitive feedback. Results showed that revision behaviour\ndiffered across feedback conditions, with Hybrid prompting the most revisions\ncompared to Directive and Metacognitive. Confidence ratings were uniformly\nhigh, and resource quality outcomes were comparable across conditions. These\nfindings highlight the promise of AI in delivering feedback that balances\nclarity with reflection. Hybrid approaches, in particular, show potential to\ncombine actionable guidance for immediate improvement with opportunities for\nself-reflection and metacognitive growth.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u6307\u4ee4\u6027\u548c\u5143\u8ba4\u77e5\u53cd\u9988\u5bf9\u5b66\u751f\u5b66\u4e60\u7684\u5f71\u54cd\uff0c\u7ed3\u679c\u663e\u793a\u6df7\u5408\u53cd\u9988\u80fd\u6709\u6548\u4fc3\u8fdb\u4fee\u8ba2\u884c\u4e3a\uff0c\u7ed3\u5408\u5373\u65f6\u6539\u8fdb\u6307\u5bfc\u4e0e\u81ea\u6211\u53cd\u601d\u673a\u4f1a\u3002", "motivation": "\u63a2\u7d22\u4e0d\u540c\u7c7b\u578b\u53cd\u9988\u5bf9\u5b66\u751f\u53c2\u4e0e\u5ea6\u3001\u81ea\u4fe1\u5fc3\u548c\u5de5\u4f5c\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728AI\u751f\u6210\u53cd\u9988\u7684\u80cc\u666f\u4e0b\u3002", "method": "\u8fdb\u884c\u4e86\u4e00\u5b66\u671f\u7684\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\uff0c\u6d89\u53ca329\u540d\u5b66\u751f\uff0c\u6bd4\u8f83\u4e86\u6307\u4ee4\u6027\u53cd\u9988\u3001\u5143\u8ba4\u77e5\u53cd\u9988\u53ca\u5176\u6df7\u5408\u53cd\u9988\u7684\u6548\u679c\u3002", "result": "\u6df7\u5408\u53cd\u9988\u7ec4\u7684\u4fee\u8ba2\u884c\u4e3a\u6700\u4e3a\u6d3b\u8dc3\uff0c\u81ea\u4fe1\u5fc3\u8bc4\u5206\u666e\u904d\u8f83\u9ad8\uff0c\u800c\u5404\u7ec4\u7684\u8d44\u6e90\u8d28\u91cf\u7ed3\u679c\u76f8\u5f53\u3002", "conclusion": "\u6df7\u5408\u53cd\u9988\u5728\u4fc3\u8fdb\u5b66\u751f\u4fee\u8ba2\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u80fd\u7ed3\u5408\u660e\u786e\u6307\u5bfc\u548c\u81ea\u6211\u53cd\u601d\u7684\u673a\u4f1a\u3002"}}
{"id": "2510.19356", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.19356", "abs": "https://arxiv.org/abs/2510.19356", "authors": ["Yu Fang", "Xinyu Wang", "Xuehe Zhang", "Wanli Xue", "Mingwei Zhang", "Shengyong Chen", "Jie Zhao"], "title": "Imitation Learning Policy based on Multi-Step Consistent Integration Shortcut Model", "comment": null, "summary": "The wide application of flow-matching methods has greatly promoted the\ndevelopment of robot imitation learning. However, these methods all face the\nproblem of high inference time. To address this issue, researchers have\nproposed distillation methods and consistency methods, but the performance of\nthese methods still struggles to compete with that of the original diffusion\nmodels and flow-matching models. In this article, we propose a one-step\nshortcut method with multi-step integration for robot imitation learning. To\nbalance the inference speed and performance, we extend the multi-step\nconsistency loss on the basis of the shortcut model, split the one-step loss\ninto multi-step losses, and improve the performance of one-step inference.\nSecondly, to solve the problem of unstable optimization of the multi-step loss\nand the original flow-matching loss, we propose an adaptive gradient allocation\nmethod to enhance the stability of the learning process. Finally, we evaluate\nthe proposed method in two simulation benchmarks and five real-world\nenvironment tasks. The experimental results verify the effectiveness of the\nproposed algorithm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdb\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u7b97\u6cd5\u6765\u63d0\u9ad8\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u7684\u63a8\u65ad\u901f\u5ea6\u548c\u6027\u80fd", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6d41\u5339\u914d\u65b9\u6cd5\u5728\u63a8\u65ad\u65f6\u95f4\u4e0a\u9ad8\u7684\u6548\u7387\u95ee\u9898", "method": "\u63d0\u51fa\u4e00\u79cd\u5177\u6709\u591a\u6b65\u96c6\u6210\u7684\u5355\u6b65\u5feb\u6377\u65b9\u6cd5\u7528\u4e8e\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60", "result": "\u901a\u8fc7\u6269\u5c55\u591a\u6b65\u4e00\u81f4\u6027\u635f\u5931\uff0c\u62c6\u5206\u5355\u6b65\u635f\u5931\u5e76\u63d0\u9ad8\u5355\u6b65\u63a8\u65ad\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u51fa\u81ea\u9002\u5e94\u68af\u5ea6\u5206\u914d\u65b9\u6cd5\u589e\u5f3a\u5b66\u4e60\u8fc7\u7a0b\u7684\u7a33\u5b9a\u6027\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u7b97\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u79cd\u6a21\u62df\u57fa\u51c6\u548c\u73b0\u5b9e\u73af\u5883\u4efb\u52a1\u4e2d\u6709\u6548\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2510.19691", "categories": ["cs.HC", "cs.CY", "cs.MM", "cs.SE", "H.5.2; H.1.2; K.8.0"], "pdf": "https://arxiv.org/pdf/2510.19691", "abs": "https://arxiv.org/abs/2510.19691", "authors": ["R. Gonz\u00e1lez-Ib\u00e1\u00f1ez", "J. Mac\u00edas-C\u00e1ceres", "M. Villalta-Paucar"], "title": "LifeSync-Games: Toward a Video Game Paradigm for Promoting Responsible Gaming and Human Development", "comment": "8 pages, 4 figures, 1 table, 66 references", "summary": "Technological advancements have made video games a central part of the\ndigital lives of nearly 3 billion people worldwide. Although games can address\nvarious social, physical, and psychological needs, their potential to support\nhuman development and well-being remains underutilized. Research highlights\nboth negative effects, such as addiction and isolation, and positive outcomes\nlike cognitive improvements and problem-solving skills. However, public\ndiscourse and regulation often focus more on risks than benefits. To address\nthis imbalance, we present LifeSync-Games, a framework leveraging simplified\ndigital twins to connect virtual gameplay with real-life activities. This\nreciprocal relationship aims to enhance the developmental value of gaming by\npromoting self-regulation and fostering growth across physical, mental, and\nsocial domains. We present the framework's theoretical foundations,\ntechnological components, design guidelines, and evaluation approaches.\nAdditionally, we present early applications in both new and bestselling games\nto demonstrate its versatility and practical relevance.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u6e38\u620f\u5728\u4fc3\u8fdb\u4eba\u7c7b\u53d1\u5c55\u548c\u798f\u7949\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u63d0\u51fa\u4e86LifeSync-Games\u6846\u67b6\uff0c\u4ee5\u6539\u8fdb\u6e38\u620f\u7684\u793e\u4f1a\u3001\u5fc3\u7406\u548c\u8eab\u4f53\u5f71\u54cd\u3002", "motivation": "\u867d\u7136\u6e38\u620f\u53ef\u4ee5\u6ee1\u8db3\u591a\u79cd\u793e\u4f1a\u548c\u5fc3\u7406\u9700\u6c42\uff0c\u4f46\u5176\u4fc3\u8fdb\u4eba\u7c7b\u53d1\u5c55\u7684\u6f5c\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u5229\u7528\uff0c\u5c24\u5176\u662f\u5728\u516c\u4f17\u8ba8\u8bba\u548c\u76d1\u7ba1\u4e2d\u66f4\u591a\u5173\u6ce8\u98ce\u9669\u800c\u975e\u76ca\u5904\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5229\u7528\u6570\u5b57\u53cc\u80de\u80ce\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u7406\u8bba\u57fa\u7840\u3001\u6280\u672f\u7ec4\u4ef6\u3001\u8bbe\u8ba1\u6307\u5357\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86LifeSync-Games\u6846\u67b6\uff0c\u4ee5\u5229\u7528\u6570\u5b57\u53cc\u80de\u80ce\u8fde\u63a5\u865a\u62df\u6e38\u620f\u4e0e\u73b0\u5b9e\u751f\u6d3b\u6d3b\u52a8\uff0c\u65e8\u5728\u589e\u5f3a\u6e38\u620f\u7684\u5f00\u53d1\u4ef7\u503c\uff0c\u5e76\u4fc3\u8fdb\u8eab\u5fc3\u548c\u793e\u4f1a\u9886\u57df\u7684\u6210\u957f\u3002", "conclusion": "\u901a\u8fc7\u5c06\u865a\u62df\u6e38\u620f\u4e0e\u771f\u5b9e\u6d3b\u52a8\u76f8\u7ed3\u5408\uff0cLifeSync-Games\u6846\u67b6\u80fd\u591f\u66f4\u597d\u5730\u4fc3\u8fdb\u81ea\u6211\u8c03\u8282\u548c\u5168\u9762\u53d1\u5c55\u3002"}}
{"id": "2510.19364", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.19364", "abs": "https://arxiv.org/abs/2510.19364", "authors": ["Golnaz Raja", "Ruslan Agishev", "Milo\u0161 Pr\u00e1gr", "Joni Pajarinen", "Karel Zimmermann", "Arun Kumar Singh", "Reza Ghabcheloo"], "title": "ProTerrain: Probabilistic Physics-Informed Rough Terrain World Modeling", "comment": "This paper is submitted to IEEE International Conference on Robotics\n  and Automation (ICRA) 2026", "summary": "Uncertainty-aware robot motion prediction is crucial for downstream\ntraversability estimation and safe autonomous navigation in unstructured,\noff-road environments, where terrain is heterogeneous and perceptual\nuncertainty is high. Most existing methods assume deterministic or spatially\nindependent terrain uncertainties, ignoring the inherent local correlations of\n3D spatial data and often producing unreliable predictions. In this work, we\nintroduce an efficient probabilistic framework that explicitly models spatially\ncorrelated aleatoric uncertainty over terrain parameters as a probabilistic\nworld model and propagates this uncertainty through a differentiable physics\nengine for probabilistic trajectory forecasting. By leveraging structured\nconvolutional operators, our approach provides high-resolution multivariate\npredictions at manageable computational cost. Experimental evaluation on a\npublicly available dataset shows significantly improved uncertainty estimation\nand trajectory prediction accuracy over aleatoric uncertainty estimation\nbaselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6982\u7387\u6846\u67b6\uff0c\u6539\u8fdb\u4e86\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u8f68\u8ff9\u9884\u6d4b\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7684\u975e\u7ed3\u6784\u5316\u73af\u5883\u3002", "motivation": "\u9488\u5bf9\u73b0\u6709\u65b9\u6cd5\u5ffd\u75653D\u7a7a\u95f4\u6570\u636e\u5c40\u90e8\u76f8\u5173\u6027\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u65b0\u7684\u65b9\u6cd5\u89e3\u51b3\u4e0d\u786e\u5b9a\u6027\u8f83\u9ad8\u7684\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u9884\u6d4b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7ed3\u6784\u5377\u79ef\u8fd0\u7b97\u7b26\uff0c\u7ed3\u5408\u53ef\u5fae\u5206\u7269\u7406\u5f15\u64ce\uff0c\u5bf9\u5730\u5f62\u53c2\u6570\u7684\u7a7a\u95f4\u76f8\u5173\u4e0d\u786e\u5b9a\u6027\u8fdb\u884c\u5efa\u6a21\u548c\u4f20\u64ad\uff0c\u8fdb\u884c\u8f68\u8ff9\u9884\u6d4b\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6982\u7387\u6846\u67b6\uff0c\u7528\u4e8e\u660e\u786e\u5efa\u6a21\u5730\u5f62\u53c2\u6570\u7684\u7a7a\u95f4\u76f8\u5173\u7684\u771f\u5b9e\u6027\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u53ef\u5fae\u5206\u7269\u7406\u5f15\u64ce\u4f20\u64ad\u8fd9\u79cd\u4e0d\u786e\u5b9a\u6027\u4ee5\u8fdb\u884c\u8f68\u8ff9\u9884\u6d4b\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u4fdd\u8bc1\u8ba1\u7b97\u6210\u672c\u53ef\u63a7\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u4f9b\u9ad8\u5206\u8fa8\u7387\u7684\u591a\u53d8\u91cf\u9884\u6d4b\uff0c\u5e76\u5728\u5b9e\u9a8c\u8bc1\u660e\u4e2d\u663e\u793a\u51fa\u663e\u8457\u6539\u5584\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u8f68\u8ff9\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u9ad8\u4e0d\u786e\u5b9a\u6027\u73af\u5883\u4e2d\u673a\u5668\u4eba\u8fd0\u52a8\u9884\u6d4b\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7684\u53ef\u9760\u6027\u548c\u7cbe\u5ea6\u3002"}}
{"id": "2510.19373", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19373", "abs": "https://arxiv.org/abs/2510.19373", "authors": ["Basavasagar Patil", "Sydney Belt", "Jayjun Lee", "Nima Fazeli", "Bernadette Bucher"], "title": "Using Temperature Sampling to Effectively Train Robot Learning Policies on Imbalanced Datasets", "comment": null, "summary": "Increasingly large datasets of robot actions and sensory observations are\nbeing collected to train ever-larger neural networks. These datasets are\ncollected based on tasks and while these tasks may be distinct in their\ndescriptions, many involve very similar physical action sequences (e.g., 'pick\nup an apple' versus 'pick up an orange'). As a result, many datasets of robotic\ntasks are substantially imbalanced in terms of the physical robotic actions\nthey represent. In this work, we propose a simple sampling strategy for policy\ntraining that mitigates this imbalance. Our method requires only a few lines of\ncode to integrate into existing codebases and improves generalization. We\nevaluate our method in both pre-training small models and fine-tuning large\nfoundational models. Our results show substantial improvements on low-resource\ntasks compared to prior state-of-the-art methods, without degrading performance\non high-resource tasks. This enables more effective use of model capacity for\nmulti-task policies. We also further validate our approach in a real-world\nsetup on a Franka Panda robot arm across a diverse set of tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u91c7\u6837\u7b56\u7565\u6765\u89e3\u51b3\u673a\u5668\u4eba\u4efb\u52a1\u6570\u636e\u96c6\u4e2d\u7684\u64cd\u4f5c\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u7ecf\u8fc7\u9a8c\u8bc1\uff0c\u8be5\u7b56\u7565\u5728\u4f4e\u8d44\u6e90\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d44\u6e90\u4efb\u52a1\u7684\u8868\u73b0\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u4f20\u611f\u5668\u89c2\u5bdf\u6570\u636e\u96c6\u7684\u589e\u5927\uff0c\u73b0\u6709\u7684\u6570\u636e\u96c6\u5728\u7269\u7406\u64cd\u4f5c\u5e8f\u5217\u4e0a\u5b58\u5728\u4e0d\u5e73\u8861\u6027\uff0c\u8fd9\u53ef\u80fd\u5f71\u54cd\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u679c\u3002", "method": "\u901a\u8fc7\u51e0\u884c\u4ee3\u7801\u96c6\u6210\u5230\u73b0\u6709\u4ee3\u7801\u57fa\u7840\u4e2d\uff0c\u5b9e\u65bd\u7b80\u5355\u7684\u91c7\u6837\u7b56\u7565\uff0c\u4ee5\u6539\u5584\u6218\u7565\u8bad\u7ec3\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u91c7\u6837\u7b56\u7565\uff0c\u4ee5\u7f13\u89e3\u6570\u636e\u96c6\u4e2d\u7269\u7406\u64cd\u4f5c\u7684\u4e0d\u5e73\u8861\u6027\uff0c\u589e\u5f3a\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u53ef\u6709\u6548\u63d0\u5347\u591a\u4efb\u52a1\u6a21\u578b\u7684\u80fd\u529b\uff0c\u5e76\u5728\u771f\u5b9e\u73af\u5883\u4e0b\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.19415", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.19415", "abs": "https://arxiv.org/abs/2510.19415", "authors": ["Abdelrahman Sayed Sayed"], "title": "Risk Assessment of an Autonomous Underwater Snake Robot in Confined Operations", "comment": "9 pages, 6 figures, Accepted for publication in OCEANS 2023 -\n  Limerick", "summary": "The growing interest in ocean discovery imposes a need for inspection and\nintervention in confined and demanding environments. Eely's slender shape, in\naddition to its ability to change its body configurations, makes articulated\nunderwater robots an adequate option for such environments. However, operation\nof Eely in such environments imposes demanding requirements on the system, as\nit must deal with uncertain and unstructured environments, extreme\nenvironmental conditions, and reduced navigational capabilities. This paper\nproposes a Bayesian approach to assess the risks of losing Eely during two\nmission scenarios. The goal of this work is to improve Eely's performance and\nthe likelihood of mission success. Sensitivity analysis results are presented\nin order to demonstrate the causes having the highest impact on losing Eely.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u8d1d\u53f6\u65af\u65b9\u6cd5\u6765\u8bc4\u4f30Eely\u5728\u6d77\u6d0b\u63a2\u7d22\u4e2d\u7684\u5931\u843d\u98ce\u9669\uff0c\u5e76\u901a\u8fc7\u654f\u611f\u6027\u5206\u6790\u8bc6\u522b\u5f71\u54cd\u5931\u843d\u7684\u4e3b\u8981\u56e0\u7d20\u3002", "motivation": "\u968f\u7740\u5bf9\u6d77\u6d0b\u63a2\u7d22\u7684\u5174\u8da3\u65e5\u76ca\u589e\u957f\uff0c\u5bf9\u53d7\u9650\u548c\u82db\u523b\u73af\u5883\u4e2d\u7684\u68c0\u67e5\u548c\u5e72\u9884\u9700\u6c42\u4e5f\u968f\u4e4b\u589e\u52a0\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u65b9\u6cd5\u5bf9Eely\u5728\u590d\u6742\u4efb\u52a1\u573a\u666f\u4e2d\u7684\u98ce\u9669\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u8fdb\u884c\u654f\u611f\u6027\u5206\u6790\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8d1d\u53f6\u65af\u65b9\u6cd5\u6765\u8bc4\u4f30\u5728\u4e24\u79cd\u4efb\u52a1\u573a\u666f\u4e2d\u5931\u53bbEely\u7684\u98ce\u9669\uff0c\u65e8\u5728\u63d0\u9ad8Eely\u7684\u6027\u80fd\u548c\u4efb\u52a1\u6210\u529f\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "\u654f\u611f\u6027\u5206\u6790\u7ed3\u679c\u8868\u660e\u5f71\u54cd\u5931\u53bbEely\u7684\u56e0\u7d20\uff0c\u53ef\u4ee5\u5e2e\u52a9\u63d0\u9ad8\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u80fd\u529b\u3002"}}
{"id": "2510.19430", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.19430", "abs": "https://arxiv.org/abs/2510.19430", "authors": ["GigaBrain Team", "Angen Ye", "Boyuan Wang", "Chaojun Ni", "Guan Huang", "Guosheng Zhao", "Haoyun Li", "Jie Li", "Jiagang Zhu", "Lv Feng", "Peng Li", "Qiuping Deng", "Runqi Ouyang", "Wenkang Qin", "Xinze Chen", "Xiaofeng Wang", "Yang Wang", "Yifan Li", "Yilong Li", "Yiran Ding", "Yuan Xu", "Yun Ye", "Yukun Zhou", "Zhehao Dong", "Zhenan Wang", "Zhichao Liu", "Zheng Zhu"], "title": "GigaBrain-0: A World Model-Powered Vision-Language-Action Model", "comment": "https://gigabrain0.github.io/", "summary": "Training Vision-Language-Action (VLA) models for generalist robots typically\nrequires large-scale real-world robot data, which is expensive and\ntime-consuming to collect. The inefficiency of physical data collection\nseverely limits the scalability, and generalization capacity of current VLA\nsystems. To address this challenge, we introduce GigaBrain-0, a novel VLA\nfoundation model empowered by world model-generated data (e.g., video\ngeneration, real2real transfer, human transfer, view transfer, sim2real\ntransfer data). By leveraging world models to generate diverse data at scale,\nGigaBrain-0 significantly reduces reliance on real robot data while improving\ncross-task generalization. Our approach further improves policy robustness\nthrough RGBD input modeling and embodied Chain-of-Thought (CoT) supervision,\nenabling the model to reason about spatial geometry, object states, and\nlong-horizon dependencies during task execution. This leads to substantial\ngains in real-world performance on dexterous, long-horizon, and mobile\nmanipulation tasks. Extensive experiments demonstrate that GigaBrain-0 achieves\nsuperior generalization across variations in appearances (e.g., textures,\ncolors), object placements, and camera viewpoints. Additionally, we present\nGigaBrain-0-Small, an optimized lightweight variant designed to run efficiently\non devices such as the NVIDIA Jetson AGX Orin.", "AI": {"tldr": "GigaBrain-0\u662f\u4e00\u4e2a\u65b0\u578b\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c(VLA)\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u751f\u6210\u4e16\u754c\u6a21\u578b\u6570\u636e\u964d\u4f4e\u5bf9\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u63d0\u5347\u8de8\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u4e0e\u653f\u7b56\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u5f53\u524dVLA\u7cfb\u7edf\u56e0\u9700\u6536\u96c6\u5927\u89c4\u6a21\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\u800c\u5bfc\u81f4\u7684\u4f4e\u6548\u7387\u548c\u6269\u5c55\u6027\u9650\u5236\u3002", "method": "\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u7684\u4e16\u754c\u6a21\u578b\u6570\u636e\u548c\u5b9e\u65bdRGBD\u8f93\u5165\u5efa\u6a21\uff0c\u4ee5\u53ca\u4f7f\u7528\u5177\u8eab\u7684\u601d\u7ef4\u94fe\u76d1\u7763\uff0c\u63d0\u5347\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "GigaBrain-0\u5728\u7075\u6d3b\u3001\u957f\u671f\u53ca\u79fb\u52a8\u64cd\u7eb5\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u5b9e\u5730\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u4e14\u5f00\u53d1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684GigaBrain-0-Small\u7248\u672c\uff0c\u9002\u7528\u4e8e\u9ad8\u6548\u8fd0\u884c\u4e8e\u7279\u5b9a\u8bbe\u5907\u3002", "conclusion": "GigaBrain-0\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u5b9e\u5730\u8868\u73b0\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u4e0d\u540c\u5916\u89c2\u548c\u73af\u5883\u6761\u4ef6\u4e0b\u7684\u4f18\u8d8a\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.19495", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19495", "abs": "https://arxiv.org/abs/2510.19495", "authors": ["Kevin Huang", "Rosario Scalise", "Cleah Winston", "Ayush Agrawal", "Yunchu Zhang", "Rohan Baijal", "Markus Grotz", "Byron Boots", "Benjamin Burchfiel", "Hongkai Dai", "Masha Itkina", "Paarth Shah", "Abhishek Gupta"], "title": "Using Non-Expert Data to Robustify Imitation Learning via Offline Reinforcement Learning", "comment": null, "summary": "Imitation learning has proven effective for training robots to perform\ncomplex tasks from expert human demonstrations. However, it remains limited by\nits reliance on high-quality, task-specific data, restricting adaptability to\nthe diverse range of real-world object configurations and scenarios. In\ncontrast, non-expert data -- such as play data, suboptimal demonstrations,\npartial task completions, or rollouts from suboptimal policies -- can offer\nbroader coverage and lower collection costs. However, conventional imitation\nlearning approaches fail to utilize this data effectively. To address these\nchallenges, we posit that with right design decisions, offline reinforcement\nlearning can be used as a tool to harness non-expert data to enhance the\nperformance of imitation learning policies. We show that while standard offline\nRL approaches can be ineffective at actually leveraging non-expert data under\nthe sparse data coverage settings typically encountered in the real world,\nsimple algorithmic modifications can allow for the utilization of this data,\nwithout significant additional assumptions. Our approach shows that broadening\nthe support of the policy distribution can allow imitation algorithms augmented\nby offline RL to solve tasks robustly, showing considerably enhanced recovery\nand generalization behavior. In manipulation tasks, these innovations\nsignificantly increase the range of initial conditions where learned policies\nare successful when non-expert data is incorporated. Moreover, we show that\nthese methods are able to leverage all collected data, including partial or\nsuboptimal demonstrations, to bolster task-directed policy performance. This\nunderscores the importance of algorithmic techniques for using non-expert data\nfor robust policy learning in robotics.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u975e\u4e13\u5bb6\u6570\u636e\u63d0\u5347\u6a21\u4eff\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u4f20\u7edf\u6a21\u4eff\u5b66\u4e60\u5bf9\u9ad8\u8d28\u91cf\u4e13\u5bb6\u6570\u636e\u7684\u4f9d\u8d56\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u8fc7\u4e8e\u4f9d\u8d56\u9ad8\u8d28\u91cf\u4e13\u5bb6\u6570\u636e\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u9002\u5e94\u6027\uff0c\u800c\u975e\u4e13\u5bb6\u6570\u636e\u7684\u6709\u6548\u5229\u7528\u53ef\u4ee5\u964d\u4f4e\u6570\u636e\u6536\u96c6\u6210\u672c\uff0c\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u7b80\u5355\u7684\u7b97\u6cd5\u4fee\u6539\uff0c\u4f7f\u5f97\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u5728\u7a00\u758f\u6570\u636e\u8986\u76d6\u73af\u5883\u4e2d\u5145\u5206\u5229\u7528\u975e\u4e13\u5bb6\u6570\u636e\u3002", "result": "\u901a\u8fc7\u5f15\u5165\u975e\u4e13\u5bb6\u6570\u636e\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5728\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u5b66\u4e60\u6548\u679c\uff0c\u6269\u5927\u4e86\u6210\u529f\u7684\u521d\u59cb\u6761\u4ef6\u8303\u56f4\uff0c\u63d0\u5347\u4e86\u4efb\u52a1\u5bfc\u5411\u7684\u7b56\u7565\u6027\u80fd\u3002", "conclusion": "\u91c7\u7528\u5408\u9002\u7684\u7b97\u6cd5\u8bbe\u8ba1\uff0c\u80fd\u591f\u6709\u6548\u5229\u7528\u975e\u4e13\u5bb6\u6570\u636e\uff0c\u589e\u5f3a\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u7b97\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.19541", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.19541", "abs": "https://arxiv.org/abs/2510.19541", "authors": ["Francesco Schetter", "Shifa Sulaiman", "Shoby George", "Paolino De Risi", "Fanny Ficuciello"], "title": "Optimizing Prosthetic Wrist Movement: A Model Predictive Control Approach", "comment": "International Conference on Social Robotics + AI 2025", "summary": "The integration of advanced control strategies into prosthetic hands is\nessential to improve their adaptability and performance. In this study, we\npresent an implementation of a Model Predictive Control (MPC) strategy to\nregulate the motions of a soft continuum wrist section attached to a\ntendon-driven prosthetic hand with less computational effort. MPC plays a\ncrucial role in enhancing the functionality and responsiveness of prosthetic\nhands. By leveraging predictive modeling, this approach enables precise\nmovement adjustments while accounting for dynamic user interactions. This\nadvanced control strategy allows for the anticipation of future movements and\nadjustments based on the current state of the prosthetic device and the\nintentions of the user. Kinematic and dynamic modelings are performed using\nEuler-Bernoulli beam and Lagrange methods respectively. Through simulation and\nexperimental validations, we demonstrate the effectiveness of MPC in optimizing\nwrist articulation and user control. Our findings suggest that this technique\nsignificantly improves the prosthetic hand dexterity, making movements more\nnatural and intuitive. This research contributes to the field of robotics and\nbiomedical engineering by offering a promising direction for intelligent\nprosthetic systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u5b9e\u65bd\u4e86\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7b56\u7565\uff0c\u63d0\u9ad8\u4e86\u5047\u80a2\u624b\u7684\u7075\u6d3b\u6027\u548c\u76f4\u89c2\u6027\uff0c\u4f18\u5316\u4e86\u52a8\u6001\u7528\u6237\u4ea4\u4e92\u7684\u8fd0\u52a8\u8c03\u6574\u3002", "motivation": "\u63d0\u9ad8\u5047\u80a2\u624b\u7684\u9002\u5e94\u6027\u548c\u6027\u80fd\u3002", "method": "\u91c7\u7528\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7b56\u7565", "result": "\u901a\u8fc7\u6a21\u62df\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0cMPC\u663e\u8457\u4f18\u5316\u4e86\u624b\u8155\u5173\u8282\u8fd0\u52a8\u548c\u7528\u6237\u63a7\u5236\u3002", "conclusion": "\u8be5\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u5047\u80a2\u624b\u7684\u7075\u5de7\u5ea6\uff0c\u4e3a\u667a\u80fd\u5047\u80a2\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002"}}
{"id": "2510.19655", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.19655", "abs": "https://arxiv.org/abs/2510.19655", "authors": ["Hongyu Ding", "Ziming Xu", "Yudong Fang", "You Wu", "Zixuan Chen", "Jieqi Shi", "Jing Huo", "Yifan Zhang", "Yang Gao"], "title": "LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language Navigation in Continuous Environments", "comment": null, "summary": "Zero-shot Vision-and-Language Navigation in Continuous Environments (VLN-CE)\nrequires an agent to navigate unseen environments based on natural language\ninstructions without any prior training. Current methods face a critical\ntrade-off: either rely on environment-specific waypoint predictors that limit\nscene generalization, or underutilize the reasoning capabilities of large\nmodels during navigation. We introduce LaViRA, a simple yet effective zero-shot\nframework that addresses this dilemma by decomposing action into a\ncoarse-to-fine hierarchy: Language Action for high-level planning, Vision\nAction for perceptual grounding, and Robot Action for robust navigation. This\nmodular decomposition allows us to leverage the distinct strengths of different\nscales of Multimodal Large Language Models (MLLMs) at each stage, creating a\nsystem that is powerful in its reasoning, grounding and practical control.\nLaViRA significantly outperforms existing state-of-the-art methods on the\nVLN-CE benchmark, demonstrating superior generalization capabilities in unseen\nenvironments, while maintaining transparency and efficiency for real-world\ndeployment.", "AI": {"tldr": "LaViRA\u662f\u4e00\u4e2a\u6709\u6548\u7684\u96f6-shot\u89c6\u89c9-\u8bed\u8a00\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u52a8\u4f5c\u5c42\u7ea7\u6765\u89e3\u51b3\u73af\u5883\u6cdb\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5728VLN-CE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u65b9\u6cd5\u5728\u573a\u666f\u6cdb\u5316\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u540c\u65f6\u5229\u7528\u5927\u578b\u6a21\u578b\u63a8\u7406\u80fd\u529b\u4e0e\u4e0d\u4f9d\u8d56\u4e8e\u73af\u5883\u7279\u5b9a\u7684\u8def\u5f84\u9884\u6d4b\u65b9\u9762\u7684\u6743\u8861\u3002", "method": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u5c06\u52a8\u4f5c\u5206\u89e3\u4e3a\u7c97\u5230\u7ec6\u7684\u5c42\u7ea7\u7ed3\u6784\u7684\u96f6-shot\u6846\u67b6\uff0c\u5305\u542b\u8bed\u8a00\u52a8\u4f5c\u3001\u89c6\u89c9\u52a8\u4f5c\u548c\u673a\u5668\u4eba\u52a8\u4f5c\u3002", "result": "LaViRA\u5728\u672a\u89c1\u73af\u5883\u4e2d\u5c55\u793a\u4e86\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5229\u7528\u4e0d\u540c\u89c4\u6a21\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u70b9\u6765\u589e\u5f3a\u63a8\u7406\u3001\u5b9a\u4f4d\u548c\u63a7\u5236\u7684\u80fd\u529b\u3002", "conclusion": "LaViRA\u5728VLN-CE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u65b0\u65b9\u6cd5\uff0c\u5177\u5907\u5728\u672a\u89c1\u73af\u5883\u4e2d\u51fa\u8272\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u900f\u660e\u6027\u548c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2510.19663", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.19663", "abs": "https://arxiv.org/abs/2510.19663", "authors": ["Vojt\u011bch Vrba", "Viktor Walter", "Petr \u0160t\u011bp\u00e1n", "Martin Saska"], "title": "Fast Marker Detection for UV-Based Visual Relative Localisation in Agile UAV Swarms", "comment": null, "summary": "A novel approach for the fast onboard detection of isolated markers for\nvisual relative localisation of multiple teammates in agile UAV swarms is\nintroduced in this paper. As the detection forms a key component of real-time\nlocalisation systems, a three-fold innovation is presented, consisting of an\noptimised procedure for CPUs, a GPU shader program, and a functionally\nequivalent FPGA streaming architecture. For the proposed CPU and GPU solutions,\nthe mean processing time per pixel of input camera frames was accelerated by\ntwo to three orders of magnitude compared to the state of the art. For the\nlocalisation task, the proposed FPGA architecture offered the most significant\noverall acceleration by minimising the total delay from camera exposure to\ndetection results. Additionally, the proposed solutions were evaluated on\nvarious 32-bit and 64-bit embedded platforms to demonstrate their efficiency,\nas well as their feasibility for applications using low-end UAVs and MAVs.\nThus, it has become a crucial enabling technology for agile UAV swarming.", "AI": {"tldr": "\u672c\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7528\u4e8e\u5feb\u901f\u68c0\u6d4b\u5b64\u7acb\u6807\u8bb0\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u7075\u6d3b\u7684\u65e0\u4eba\u673a\u7fa4\u4f53\uff0c\u4ee5\u63d0\u5347\u5b9e\u65f6\u5b9a\u4f4d\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u7075\u6d3b\u65e0\u4eba\u673a\u7fa4\u4f53\u6280\u672f\u7684\u53d1\u5c55\uff0c\u5b9e\u65f6\u5b9a\u4f4d\u7cfb\u7edf\u4e2d\u5bf9\u5b64\u7acb\u6807\u8bb0\u7684\u5feb\u901f\u68c0\u6d4b\u53d8\u5f97\u6108\u52a0\u91cd\u8981\uff0c\u4e9f\u9700\u63d0\u5347\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "method": "\u8bba\u6587\u4e2d\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u91cd\u521b\u65b0\uff0c\u5305\u62ec\u9488\u5bf9CPU\u7684\u4f18\u5316\u6d41\u7a0b\u3001GPU\u7740\u8272\u5668\u7a0b\u5e8f\u548c\u529f\u80fd\u76f8\u540c\u7684FPGA\u6d41\u67b6\u6784\uff0c\u663e\u8457\u52a0\u5feb\u4e86\u8f93\u5165\u76f8\u673a\u5e27\u7684\u5904\u7406\u901f\u5ea6\u3002", "result": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u5feb\u901f\u5728\u673a\u8f7d\u7cfb\u7edf\u4e2d\u68c0\u6d4b\u5b64\u7acb\u6807\u8bb0\uff0c\u4ece\u800c\u8fdb\u884c\u591a\u961f\u53cb\u5728\u7075\u6d3b\u65e0\u4eba\u673a\uff08UAV\uff09\u7fa4\u4f53\u4e2d\u7684\u89c6\u89c9\u76f8\u5bf9\u5b9a\u4f4d\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u89e3\u51b3\u65b9\u6848\u5728\u591a\u79cd\u5d4c\u5165\u5f0f\u5e73\u53f0\u4e0a\u7ecf\u8fc7\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u4f4e\u7aef\u65e0\u4eba\u673a\u548c\u5fae\u578b\u65e0\u4eba\u673a\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u548c\u53ef\u884c\u6027\uff0c\u6210\u4e3a\u7075\u6d3b\u65e0\u4eba\u673a\u7fa4\u4f53\u6280\u672f\u7684\u91cd\u8981\u652f\u6491\u3002"}}
{"id": "2510.19752", "categories": ["cs.RO", "cs.AI", "68T40", "I.2.9; I.2.8"], "pdf": "https://arxiv.org/pdf/2510.19752", "abs": "https://arxiv.org/abs/2510.19752", "authors": ["Ameesh Shah", "William Chen", "Adwait Godbole", "Federico Mora", "Sanjit A. Seshia", "Sergey Levine"], "title": "Learning Affordances at Inference-Time for Vision-Language-Action Models", "comment": "7 pages and appendix", "summary": "Solving complex real-world control tasks often takes multiple tries: if we\nfail at first, we reflect on what went wrong, and change our strategy\naccordingly to avoid making the same mistake. In robotics,\nVision-Language-Action models (VLAs) offer a promising path towards solving\ncomplex control tasks, but lack the ability to contextually and dynamically\nreadjust behavior when they fail to accomplish a task. In this work, we\nintroduce Learning from Inference-Time Execution (LITEN), which connects a VLA\nlow-level policy to a high-level VLM that conditions on past experiences by\nincluding them in-context, allowing it to learn the affordances and\ncapabilities of the low-level VLA. Our approach iterates between a reasoning\nphase that generates and executes plans for the low-level VLA, and an\nassessment phase that reflects on the resulting execution and draws useful\nconclusions to be included in future reasoning contexts. Unlike similar\napproaches to self-refinement in non-robotics domains, LITEN must reflect on\nunstructured real-world robot trajectories (e.g., raw videos), which requires\nstructured guiderails during assessment. Our experimental results demonstrate\nLITEN is able to effectively learn from past experience to generate plans that\nuse high-affordance instructions to accomplish long-horizon tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLITEN\uff0c\u4e00\u79cd\u7ed3\u5408\u4f4e\u5c42VLA\u548c\u9ad8\u5c42VLM\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u5931\u8d25\u540e\u901a\u8fc7\u53cd\u601d\u8c03\u6574\u884c\u4e3a\uff0c\u751f\u6210\u66f4\u6709\u6548\u7684\u4efb\u52a1\u6267\u884c\u8ba1\u5212\u3002", "motivation": "\u5f53\u524dVLA\u6a21\u578b\u5728\u5931\u8d25\u540e\u7f3a\u4e4f\u52a8\u6001\u8c03\u6574\u884c\u4e3a\u7684\u80fd\u529b\uff0c\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u590d\u6742\u63a7\u5236\u4efb\u52a1", "method": "\u5f15\u5165\u5b66\u4e60\u63a8\u7406\u65f6\u6267\u884c\uff08LITEN\uff09\uff0c\u5c06VLA\u4f4e\u5c42\u7b56\u7565\u4e0e\u9ad8\u5c42VLM\u8fde\u63a5", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aLITEN\u80fd\u591f\u6709\u6548\u5b66\u4e60\u8fc7\u53bb\u7ecf\u9a8c\uff0c\u751f\u6210\u4f7f\u7528\u9ad8\u6548\u6307\u4ee4\u7684\u8ba1\u5212\u6765\u5b8c\u6210\u957f\u671f\u4efb\u52a1", "conclusion": "LITEN\u901a\u8fc7\u53cd\u601d\u6267\u884c\u7ed3\u679c\uff0c\u80fd\u591f\u5728\u590d\u6742\u7684\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u6709\u6548\u5b66\u4e60\u548c\u8c03\u6574\u7b56\u7565\u3002"}}
{"id": "2510.19766", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.19766", "abs": "https://arxiv.org/abs/2510.19766", "authors": ["Hongyu Ding", "Xinyue Liang", "Yudong Fang", "You Wu", "Jieqi Shi", "Jing Huo", "Wenbin Li", "Jing Wu", "Yu-Kun Lai", "Yang Gao"], "title": "SEA: Semantic Map Prediction for Active Exploration of Uncertain Areas", "comment": null, "summary": "In this paper, we propose SEA, a novel approach for active robot exploration\nthrough semantic map prediction and a reinforcement learning-based hierarchical\nexploration policy. Unlike existing learning-based methods that rely on\none-step waypoint prediction, our approach enhances the agent's long-term\nenvironmental understanding to facilitate more efficient exploration. We\npropose an iterative prediction-exploration framework that explicitly predicts\nthe missing areas of the map based on current observations. The difference\nbetween the actual accumulated map and the predicted global map is then used to\nguide exploration. Additionally, we design a novel reward mechanism that\nleverages reinforcement learning to update the long-term exploration\nstrategies, enabling us to construct an accurate semantic map within limited\nsteps. Experimental results demonstrate that our method significantly\noutperforms state-of-the-art exploration strategies, achieving superior\ncoverage ares of the global map within the same time constraints.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u8bed\u4e49\u5730\u56fe\u9884\u6d4b\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u5206\u5c42\u63a2\u7d22\u7b56\u7565\u7684\u673a\u5668\u4eba\u63a2\u7d22\u65b0\u65b9\u6cd5\uff0c\u8d85\u8d8a\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u501f\u52a9\u5bf9\u73af\u5883\u7684\u957f\u671f\u7406\u89e3\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u7684\u63a2\u7d22\u6548\u7387\uff0c\u586b\u8865\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u5728\u5355\u6b65\u8def\u5f84\u9884\u6d4b\u4e2d\u7684\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u8fed\u4ee3\u9884\u6d4b-\u63a2\u7d22\u6846\u67b6\uff0c\u57fa\u4e8e\u5f53\u524d\u89c2\u6d4b\u663e\u5f0f\u9884\u6d4b\u5730\u56fe\u7f3a\u5931\u533a\u57df\uff0c\u5e76\u4f7f\u7528\u5b9e\u9645\u7d2f\u79ef\u5730\u56fe\u4e0e\u9884\u6d4b\u5730\u56fe\u7684\u5dee\u5f02\u6307\u5bfc\u63a2\u7d22\uff0c\u540c\u65f6\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u66f4\u65b0\u957f\u671f\u63a2\u7d22\u7b56\u7565\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e3b\u52a8\u673a\u5668\u4eba\u63a2\u7d22\u65b9\u6cd5SEA\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u8bed\u4e49\u5730\u56fe\u9884\u6d4b\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5206\u5c42\u63a2\u7d22\u7b56\u7565\u6765\u5b9e\u73b0\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u76f8\u540c\u65f6\u95f4\u9650\u5236\u4e0b\u663e\u8457\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u6700\u5148\u8fdb\u63a2\u7d22\u7b56\u7565\u3002"}}
