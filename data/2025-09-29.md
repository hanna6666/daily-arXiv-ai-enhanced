<div id=toc></div>

# Table of Contents

- [cs.HC](#cs.HC) [Total: 19]
- [cs.RO](#cs.RO) [Total: 47]


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [1] [Position: Human Factors Reshape Adversarial Analysis in Human-AI Decision-Making Systems](https://arxiv.org/abs/2509.21436)
*Shutong Fan,Lan Zhang,Xiaoyong Yuan*

Main category: cs.HC

TL;DR: 随着人工智能在决策中支持人类，其易受攻击性增加，必须考虑人类因素与对抗策略之间的相互作用，以增强AI辅助决策的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当人工智能越来越多地支撑人类决策时，对其易受对抗攻击的脆弱性也在增长，因此需要重新审视人类和AI的协作关系。

Method: 通过综合回顾人类因素在人机协作中的作用，提出了一种新的鲁棒性分析框架来分析人类因素对决策性能的影响，并引入时序对抗攻击作为案例研究。

Result: 实验结果显示，攻击时机独特地影响了人机协作中的决策结果。

Conclusion: 人类因素对人机决策系统的对抗性分析至关重要，必须纳入评估的考量中。

Abstract: As Artificial Intelligence (AI) increasingly supports human decision-making,
its vulnerability to adversarial attacks grows. However, the existing
adversarial analysis predominantly focuses on fully autonomous AI systems,
where decisions are executed without human intervention. This narrow focus
overlooks the complexities of human-AI collaboration, where humans interpret,
adjust, and act upon AI-generated decisions. Trust, expectations, and cognitive
behaviors influence how humans interact with AI, creating dynamic feedback
loops that adversaries can exploit. To strengthen the robustness of AI-assisted
decision-making, adversarial analysis must account for the interplay between
human factors and attack strategies.
  This position paper argues that human factors fundamentally reshape
adversarial analysis and must be incorporated into evaluating robustness in
human-AI decision-making systems. To fully explore human factors in adversarial
analysis, we begin by investigating the role of human factors in human-AI
collaboration through a comprehensive review. We then introduce a novel
robustness analysis framework that (1) examines how human factors affect
collaborative decision-making performance, (2) revisits and interprets existing
adversarial attack strategies in the context of human-AI interaction, and (3)
introduces a new timing-based adversarial attack as a case study, illustrating
vulnerabilities emerging from sequential human actions. The experimental
results reveal that attack timing uniquely impacts decision outcomes in
human-AI collaboration. We hope this analysis inspires future research on
adversarial robustness in human-AI systems, fostering interdisciplinary
approaches that integrate AI security, human cognition, and decision-making
dynamics.

</details>


### [2] [LLM Agent Meets Agentic AI: Can LLM Agents Simulate Customers to Evaluate Agentic-AI-based Shopping Assistants?](https://arxiv.org/abs/2509.21501)
*Lu Sun,Shihan Fu,Bingsheng Yao,Yuxuan Lu,Wenbo Li,Hansu Gu,Jiri Gesi,Jing Huang,Chen Luo,Dakuo Wang*

Main category: cs.HC

TL;DR: 研究探讨了LLM代理在模拟人类多轮互动中的能力，发现其可用于可扩展的AI系统评估。


<details>
  <summary>Details</summary>
Motivation: 随着Agentic AI的快速发展，传统的人类评估方法难以跟上，因此需要找到新的方法来评价这些系统与用户的互动。

Method: 本研究通过招募40名参与者与Amazon Rufus进行购物，收集他们的人格特征、交互数据和用户体验反馈，并创建数字双胞胎以重复任务。

Result: 配对比较显示，尽管数字双胞胎探索了更多的选择，但其行为模式与人类相似，并提供了相似的设计反馈。

Conclusion: 研究表明，LLM代理能够有效地模拟人类在与代理AI系统进行多轮交互时的行为，为可扩展评估提供了潜在可能。

Abstract: Agentic AI is emerging, capable of executing tasks through natural language,
such as Copilot for coding or Amazon Rufus for shopping. Evaluating these
systems is challenging, as their rapid evolution outpaces traditional human
evaluation. Researchers have proposed LLM Agents to simulate participants as
digital twins, but it remains unclear to what extent a digital twin can
represent a specific customer in multi-turn interaction with an agentic AI
system. In this paper, we recruited 40 human participants to shop with Amazon
Rufus, collected their personas, interaction traces, and UX feedback, and then
created digital twins to repeat the task. Pairwise comparison of human and
digital-twin traces shows that while agents often explored more diverse
choices, their action patterns aligned with humans and yielded similar design
feedback. This study is the first to quantify how closely LLM agents can mirror
human multi-turn interaction with an agentic AI system, highlighting their
potential for scalable evaluation.

</details>


### [3] [Psychological and behavioural responses in human-agent vs. human-human interactions: a systematic review and meta-analysis](https://arxiv.org/abs/2509.21542)
*Jianan Zhou,Fleur Corbett,Joori Byun,Talya Porat,Nejra van Zalk*

Main category: cs.HC

TL;DR: 本研究系统比较了人类与智能体及人类之间的互动，发现与智能体互动时，个体的利他行为和道德参与较少，智能体在社会认知上较人类被低估。尽管在某些互动上表现相似，但在基本的社会归因和道德关切上存在差距。


<details>
  <summary>Details</summary>
Motivation: 旨在系统地比较人类与智能体和人类之间的心理和行为反应，以填补跨学科研究的空白。

Method: 系统性综述和元分析，包含162个研究和468个效应量，采用频率和贝叶斯两种方法。

Result: 研究发现，与人类互动时，个体表现出更少的利他行为和道德参与，且对智能体的认知（如能力、可取性和社会存在感）较低。社交对齐、对伙伴的信任等与人类互动时通常相似，但主观反应的效应大小异质性高，表明合作伙伴效果受上下文影响。

Conclusion: 虽然与人类之间的互动在某些功能性行为和体验上类似，但与智能体的互动在基本社会归因和道德/利他关切方面仍显不足。因此，智能体在设计和监管方面应被赋予与人类相当的工具价值，但缺乏相应的内在价值。

Abstract: Interactive intelligent agents are being integrated across society. Despite
achieving human-like capabilities, humans' responses to these agents remain
poorly understood, with research fragmented across disciplines. We conducted a
first systematic synthesis comparing a range of psychological and behavioural
responses in matched human-agent vs. human-human dyadic interactions. A total
of 162 eligible studies (146 contributed to the meta-analysis; 468 effect
sizes) were included in the systematic review and meta-analysis, which
integrated frequentist and Bayesian approaches. Our results indicate that
individuals exhibited less prosocial behaviour and moral engagement when
interacting with agents vs. humans. They attributed less agency and
responsibility to agents, perceiving them as less competent, likeable, and
socially present. In contrast, individuals' social alignment (i.e., alignment
or adaptation of internal states and behaviours with partners), trust in
partners, personal agency, task performance, and interaction experiences were
generally comparable when interacting with agents vs. humans. We observed high
effect-size heterogeneity for many subjective responses (i.e., social
perceptions of partners, subjective trust, and interaction experiences),
suggesting context-dependency of partner effects. By examining the
characteristics of studies, participants, partners, interaction scenarios, and
response measures, we also identified several moderators shaping partner
effects. Overall, functional behaviours and interactive experiences with agents
can resemble those with humans, whereas fundamental social attributions and
moral/prosocial concerns lag in human-agent interactions. Agents are thus
afforded instrumental value on par with humans but lack comparable intrinsic
value, providing practical implications for agent design and regulation.

</details>


### [4] [EMG-UP: Unsupervised Personalization in Cross-User EMG Gesture Recognition](https://arxiv.org/abs/2509.21589)
*Nana Wang,Gen Li,Zhaoxin Fan,Suli Wang*

Main category: cs.HC

TL;DR: EMG-UP是一种新颖的无监督个性化框架，解决了跨用户手势识别中的生物信号变异性问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在跨用户EMG信号识别中的泛化能力不足的问题。

Method: 采用两阶段适应策略，包括序列交叉视角对比学习和伪标签指导微调。

Result: EMG-UP实现了最先进的性能，优于之前的方法。

Conclusion: EMG-UP在跨用户手势识别中表现出色，精度提高至少2.0%。

Abstract: Cross-user electromyography (EMG)-based gesture recognition represents a
fundamental challenge in achieving scalable and personalized human-machine
interaction within real-world applications. Despite extensive efforts, existing
methodologies struggle to generalize effectively across users due to the
intrinsic biological variability of EMG signals, resulting from anatomical
heterogeneity and diverse task execution styles. To address this limitation, we
introduce EMG-UP, a novel and effective framework for Unsupervised
Personalization in cross-user gesture recognition. The proposed framework
leverages a two-stage adaptation strategy: (1) Sequence-Cross Perspective
Contrastive Learning, designed to disentangle robust and user-specific feature
representations by capturing intrinsic signal patterns invariant to inter-user
variability, and (2) Pseudo-Label-Guided Fine-Tuning, which enables model
refinement for individual users without necessitating access to source domain
data. Extensive evaluations show that EMG-UP achieves state-of-the-art
performance, outperforming prior methods by at least 2.0% in accuracy.

</details>


### [5] [Alignment Without Understanding: A Message- and Conversation-Centered Approach to Understanding AI Sycophancy](https://arxiv.org/abs/2509.21665)
*Lihua Du,Xing Lyu,Lezi Xie,Bo Feng*

Main category: cs.HC

TL;DR: 本文探讨了AI sycophancy的概念，定义了其类型，并提出处理模型AISPM，以促进对其影响和机制的系统性研究。


<details>
  <summary>Details</summary>
Motivation: 鉴于AI sycophancy被普遍认可为一种有害的对齐现象，文章希望通过重新定义和框架化这一概念，推动相关研究的发展。

Method: 通过文献综述和概念框架重构，明确不同类型的AI sycophancy及其影响机制。

Result: 文章成功定义了AI sycophancy，区分了信息、认知和情感三种类型，并提出了多个关键维度及处理模型，以丰富对该现象的理解。

Conclusion: 本文提出了AI sycophancy处理模型（AISPM），为理解用户体验中AI的谄媚反应提供了理论基础，并促进了相关研究的统一和深入。

Abstract: AI sycophancy is increasingly recognized as a harmful alignment, but research
remains fragmented and underdeveloped at the conceptual level. This article
redefines AI sycophancy as the tendency of large language models (LLMs) and
other interactive AI systems to excessively and/or uncritically validate,
amplify, or align with a user's assertions-whether these concern factual
information, cognitive evaluations, or affective states. Within this framework,
we distinguish three types of sycophancy: informational, cognitive, and
affective. We also introduce personalization at the message level and critical
prompting at the conversation level as key dimensions for distinguishing and
examining different manifestations of AI sycophancy. Finally, we propose the AI
Sycophancy Processing Model (AISPM) to examine the antecedents, outcomes, and
psychological mechanisms through which sycophantic AI responses shape user
experiences. By embedding AI sycophancy in the broader landscape of
communication theory and research, this article seeks to unify perspectives,
clarify conceptual boundaries, and provide a foundation for systematic,
theory-driven investigations.

</details>


### [6] [FlexMind: Supporting Deeper Creative Thinking with LLMs](https://arxiv.org/abs/2509.21685)
*Yaqing Yang,Vikram Mohanty,Yan-Ying Chen,Matthew K. Hong,Nikolas Martelaro,Aniket Kittur*

Main category: cs.HC

TL;DR: FlexMind是一个结合广泛探索和深入评估的AI增强系统，能够提高创意生成的质量。


<details>
  <summary>Details</summary>
Motivation: 当前的生成式AI工具往往仅注重生成大量想法或支持少量想法的深入探讨，缺乏两者的支持，容易导致表面化的探索和消极的创造成果。

Method: 通过与ChatGPT进行比较的研究，评估FlexMind在创意生成中的有效性。

Result: 研究结果表明，使用FlexMind的参与者生成的创意质量更高，这归因于更广泛的想法曝光和对权衡的更深入参与。

Conclusion: FlexMind通过支持广泛的想法探索和深入的评估，帮助用户生成高质量的创意，克服了现有生成式AI工具的局限性。

Abstract: Effective ideation requires both broad exploration of diverse ideas and deep
evaluation of their potential. Generative AI can support such processes, but
current tools typically emphasize either generating many ideas or supporting
in-depth consideration of a few, lacking support for both. Research also
highlights risks of over-reliance on LLMs, including shallow exploration and
negative creative outcomes. We present FlexMind, an AI-augmented system that
scaffolds iterative exploration of ideas, tradeoffs, and mitigations. FlexMind
exposes users to a broad set of ideas while enabling a lightweight transition
into deeper engagement. In a study comparing ideation with FlexMind to ChatGPT,
participants generated higher-quality ideas with FlexMind, due to both broader
exposure and deeper engagement with tradeoffs. By scaffolding ideation across
breadth, depth, and reflective evaluation, FlexMind empowers users to surface
ideas that might otherwise go unnoticed or be prematurely discarded.

</details>


### [7] [Design Exploration of AI-assisted Personal Affective Physicalization](https://arxiv.org/abs/2509.21721)
*Ruishan Wu,Zhuoyang Li,Charles Perin,Sheelagh Carpendale,Can Liu*

Main category: cs.HC

TL;DR: 本研究探讨了AI辅助个人情感物化设计的机会，开发了PhEmotion工具，并进行实验以研究人类与AI共同创造的复杂性及其对情感表达的影响。


<details>
  <summary>Details</summary>
Motivation: 个人情感物化是人们通过具体形式来表达、记录和沟通情感的过程，但由于情感的抽象性，设计这样的物理数据表示存在挑战。

Method: 采用了通过设计研究的方法，开发了PhEmotion工具，以帮助将人类与AI对话中提取的情感值嵌入物理艺术品的参数化设计中。

Result: 进行了一项实验室研究，14名参与者在有无AI支持的情况下创建这些基于个人情感的物品，观察到了参与者创造策略、意义构建过程以及对AI支持的感知的细微差异。

Conclusion: 本研究揭示了AI在人类共同创造个人情感物化过程中的关键张力，并为未来的研究提供了细致的议程。

Abstract: Personal Affective Physicalization is the process by which individuals
express emotions through tangible forms to record, reflect on, and communicate.
Yet such physical data representations can be challenging to design due to the
abstract nature of emotions. Given the shown potential of AI in detecting
emotion and assisting design, we explore opportunities in AI-assisted design of
personal affective physicalization using a Research-through-Design method. We
developed PhEmotion, a tool for embedding LLM-extracted emotion values from
human-AI conversations into parametric design of physical artifacts. A lab
study was conducted with 14 participants creating these artifacts based on
their personal emotions, with and without AI support. We observed nuances and
variations in participants' creative strategies, meaning-making processes and
their perceptions of AI support in this context. We found key tensions in
AI-human co-creation that provide a nuanced agenda for future research in
AI-assisted personal affective physicalization.

</details>


### [8] [When Teams Embrace AI: Human Collaboration Strategies in Generative Prompting in a Creative Design Task](https://arxiv.org/abs/2509.21731)
*Yuanning Han,Ziyi Qiu,Jiale Cheng,RAY LC*

Main category: cs.HC

TL;DR: 研究了人类合作者与生成型人工智能共同提示过程中的相互影响，发现人类合作是关键，生成型人工智能被视为效率工具。


<details>
  <summary>Details</summary>
Motivation: 探讨团队中合作者之间的协作与提示如何相互影响，以及在共同提示过程中用户如何感知生成型人工智能和他们的合作者。

Method: 通过让具有设计或表演背景且对生成型人工智能了解不多的学生两人一组，与生成型人工智能合作创作基于创意主题的舞台设计进行研究。

Result: 发现了两种集中于先生成故事描述或视觉图像的协作提示模式；生成型人工智能工具帮助参与者在任务中建立共识，并允许对提示策略进行讨论。参与者将生成型人工智能视为高效工具，而非真正的合作者，表明人类伙伴减少了对其使用的依赖。

Conclusion: 人类合作在与生成型人工智能工具的使用中至关重要，这表明在提示过程中，系统应充分利用共享的人类专业知识。

Abstract: Studies of Generative AI (GenAI)-assisted creative workflows have focused on
individuals overcoming challenges of prompting to produce what they envisioned.
When designers work in teams, how do collaboration and prompting influence each
other, and how do users perceive generative AI and their collaborators during
the co-prompting process? We engaged students with design or performance
backgrounds, and little exposure to GenAI, to work in pairs with GenAI to
create stage designs based on a creative theme. We found two patterns of
collaborative prompting focused on generating story descriptions first, or
visual imagery first. GenAI tools helped participants build consensus in the
task, and allowed for discussion of the prompting strategies. Participants
perceived GenAI as efficient tools rather than true collaborators, suggesting
that human partners reduced the reliance on their use. This work highlights the
importance of human-human collaboration when working with GenAI tools,
suggesting systems that take advantage of shared human expertise in the
prompting process.

</details>


### [9] ["In my defense, only three hours on Instagram": Designing Toward Digital Self-Awareness and Wellbeing](https://arxiv.org/abs/2509.21860)
*Karthik S. Bhat,Jiayue Melissa Shi,Wenxuan Song,Dong Whi Yoo,Koustuv Saha*

Main category: cs.HC

TL;DR: 本文研究了通过WellScreen进行自我反思以提升数字健康的有效性，发现其能改善参与者的积极情绪，促进更有意的技术使用。


<details>
  <summary>Details</summary>
Motivation: 尽管减少屏幕时间的重要性被广泛倡导，但忽视技术对有意义参与的潜力。因此，本文提出通过自我意识来促进数字健康。

Method: 开发了一款名为WellScreen的轻量级探测设备，通过让参与者估计和报告智能手机使用情况进行日常反思，进行了为期两周的部署研究（样本量N=25）。

Result: 参与者通常低估生产力和社交媒体使用，过高估计娱乐应用使用，积极情绪提高了10%，并将WellScreen评为中等实用。访谈显示，结构化反思有助于识别使用模式、调整期望，并更加有意地参与技术。

Conclusion: 轻量级反思干预措施能够支持自我意识和有意的数字参与，为设计数字健康工具提供了启示。

Abstract: Screen use pervades daily life, shaping work, leisure, and social connections
while raising concerns for digital wellbeing. Yet, reducing screen time alone
risks oversimplifying technology's role and neglecting its potential for
meaningful engagement. We posit self-awareness -- reflecting on one's digital
behavior -- as a critical pathway to digital wellbeing. We developed
WellScreen, a lightweight probe that scaffolds daily reflection by asking
people to estimate and report smartphone use. In a two-week deployment (N=25),
we examined how discrepancies between estimated and actual usage shaped digital
awareness and wellbeing. Participants often underestimated productivity and
social media while overestimating entertainment app use. They showed a 10%
improvement in positive affect, rating WellScreen as moderately useful.
Interviews revealed that structured reflection supported recognition of
patterns, adjustment of expectations, and more intentional engagement with
technology. Our findings highlight the promise of lightweight reflective
interventions for supporting self-awareness and intentional digital engagement,
offering implications for designing digital wellbeing tools.

</details>


### [10] [What Makes LLM Agent Simulations Useful for Policy? Insights From an Iterative Design Engagement in Emergency Preparedness](https://arxiv.org/abs/2509.21868)
*Yuxuan Li,Sauvik Das,Hirokazu Shirado*

Main category: cs.HC

TL;DR: 本论文探讨如何利用大语言模型代理模拟提升政策制定的实用性，提出了三条设计启示。


<details>
  <summary>Details</summary>
Motivation: 探讨如何使大语言模型代理模拟在政策制定中真正有用。

Method: 通过与大学应急备战团队的持续设计互动，开发了一个模拟人群移动与沟通的系统。

Result: 成功开发了13,000个LLM代理，这些代理在各种紧急情况下模拟大型聚会的人群行为，促进了实际政策实施。

Conclusion: 论文提出的设计启示为使LLM代理模拟对政策真正有用提供了可操作的路径。

Abstract: There is growing interest in using Large Language Models as agents (LLM
agents) for social simulations to inform policy, yet real-world adoption
remains limited. This paper addresses the question: How can LLM agent
simulations be made genuinely useful for policy? We report on a year-long
iterative design engagement with a university emergency preparedness team.
Across multiple design iterations, we iteratively developed a system of 13,000
LLM agents that simulate crowd movement and communication during a large-scale
gathering under various emergency scenarios. These simulations informed actual
policy implementation, shaping volunteer training, evacuation protocols, and
infrastructure planning. Analyzing this process, we identify three design
implications: start with verifiable scenarios and build trust gradually, use
preliminary simulations to elicit tacit knowledge, and treat simulation and
policy development as evolving together. These implications highlight
actionable pathways to making LLM agent simulations that are genuinely useful
for policy.

</details>


### [11] [Not Everyone Wins with LLMs: Behavioral Patterns and Pedagogical Implications in AI-assisted Data Analysis](https://arxiv.org/abs/2509.21890)
*Qianou Ma,Kenneth Koedinger,Tongshuang Wu*

Main category: cs.HC

TL;DR: 本研究表明，技术专长是成功使用大型语言模型的重要因素，而轻量级培训不够，需要更深层次的培训来提高AI使用技能。


<details>
  <summary>Details</summary>
Motivation: 研究了解不同专业背景的学生如何使用大型语言模型进行Python数据分析，以促进更公平的技术工作。

Method: 通过分析36名学生的作业日志、录音和问卷调查，采用混合方法研究学生在非专业课程中的表现。

Result: 技术专长显著影响成功与否，学生在利用LLMs时表现出显著差异，包括形成意图、表达输入、解释输出和评估结果的能力。

Conclusion: 技术专长是成功使用LLMs的关键，而仅仅依赖轻量级示范是不够的，需进行更深入的培训以培养稳健的人工智能使用技能。

Abstract: LLMs promise to democratize technical work in complex domains like
programmatic data analysis, but not everyone benefits equally. We study how
students with varied expertise use LLMs to complete Python-based data analysis
in computational notebooks in a non-major course. Drawing on homework logs,
recordings, and surveys from 36 students, we ask: Which expertise matters most,
and how does it shape AI use? Our mixed-methods analysis shows that technical
expertise -- not AI familiarity or communication skills -- remains a
significant predictor of success. Students also vary widely in how they
leverage LLMs, struggling at stages of forming intent, expressing inputs,
interpreting outputs, and assessing results. We identify success and failure
behaviors, such as providing context or decomposing prompts, that distinguish
effective use. These findings inform AI literacy interventions, highlighting
that lightweight demonstrations improve surface fluency but are insufficient;
deeper training and scaffolds are needed to cultivate resilient AI use skills.

</details>


### [12] [The MUG-10 Framework for Preventing Usability Issues in Mobile Application Development](https://arxiv.org/abs/2509.21914)
*Pawel Weichbroth,Tomasz Szot*

Main category: cs.HC

TL;DR: 本研究探讨了移动应用开发中的可用性问题，通过对从业者的调查提出了包含十条指导原则的新框架，并强调了与用户的积极合作。


<details>
  <summary>Details</summary>
Motivation: 当前移动应用面临硬件限制和用户多样化需求带来的设计与开发挑战，尤其是在可用性方面缺乏直接的解决措施。

Method: 通过对20名移动软件设计和开发从业者的调查，采用in vivo编码进行数据分析。

Result: 本研究开发了一套由十条指导原则和三个活动组成的新框架，强调在移动应用开发各个阶段与用户进行积极的合作和反馈收集。

Conclusion: 开发可自动化的工具以支持移动应用开发过程中可用性问题的早期发现和缓解是未来研究的方向。

Abstract: Nowadays, mobile applications are essential tools for everyday life,
providing users with anytime, anywhere access to up-to-date information,
communication, and entertainment. Needless to say, hardware limitations and the
diverse needs of different user groups pose a number of design and development
challenges. According to recent studies, usability is one of the most revealing
among many others. However, few have made the direct effort to provide and
discuss what countermeasures can be applied to avoid usability issues in mobile
application development. Through a survey of 20 mobile software design and
development practitioners, this study aims to fill this research gap. Given the
qualitative nature of the data collected, and with the goal of capturing and
preserving the intrinsic meanings embedded in the experts' statements, we
adopted in vivo coding. The analysis of the collected material enabled us to
develop a novel framework consisting of ten guidelines and three activities
with general applications. In addition, it can be noted that active
collaboration with users in testing and collecting feedback was often
emphasized at each stage of mobile application development. Future research
should consider focused action research that evaluates the effectiveness of our
recommendations and validates them across different stakeholder groups. In this
regard, the development of automated tools to support early detection and
mitigation of usability issues during mobile application development could also
be considered.

</details>


### [13] [Which Values Matter to Socially Assistive Robots in Elder Care Settings? Empirically Investigating Values That Should Be Embedded in SARs from a Multi-Stakeholder Perspective](https://arxiv.org/abs/2509.22146)
*Vivienne Jia Zhong,Theresa Schmiedel*

Main category: cs.HC

TL;DR: 本研究探讨了在老年护理中设计社交辅助机器人应嵌入的多方利益相关者的价值观，并提出了应对价值冲突的方案。


<details>
  <summary>Details</summary>
Motivation: 随着老年护理领域的劳动短缺问题日益严重，开发符合各方利益的社交辅助机器人以提升护理质量显得尤为重要。

Method: 本研究采用半结构化访谈和焦点小组相结合的方法，调查来自多个利益相关者的价值观。

Result: 研究发现了多种与安全、信任、护理、隐私和自主性相关的价值观，并揭示了利益相关者对这些价值观在实际护理环境中的不同解读。

Conclusion: 本研究增强了对SARs价值敏感设计的理解，帮助从业者开发符合人类价值的社交辅助机器人，从而促进老年护理领域的社会责任应用。

Abstract: The integration of socially assistive robots (SARs) in elder care settings
has the potential to address critical labor shortages while enhancing the
quality of care. However, the design of SARs must align with the values of
various stakeholders to ensure their acceptance and efficacy. This study
empirically investigates the values that should be embedded in SARs from a
multi-stakeholder perspective, including care receivers, caregivers,
therapists, relatives, and other involved parties. Utilizing a combination of
semi-structured interviews and focus groups, we identify a wide range of values
related to safety, trust, care, privacy, and autonomy, and illustrate how
stakeholders interpret these values in real-world care environments. Our
findings reveal several value tensions and propose potential resolutions to
these tensions. Additionally, the study highlights under-researched values such
as calmness and collaboration, which are critical in fostering a supportive and
efficient care environment. Our work contributes to the understanding of
value-sensitive design of SARs and aids practitioners in developing SARs that
align with human values, ultimately promoting socially responsible applications
in elder care settings.

</details>


### [14] [Teaching AI to Feel: A Collaborative, Full-Body Exploration of Emotive Communication](https://arxiv.org/abs/2509.22168)
*Esen K. Tütüncü,Lissette Lemus,Kris Pilcher,Holger Sprengel,Jordi Sabater-Mir*

Main category: cs.HC

TL;DR: Commonaiverse是一个互动安装项目，通过全身运动跟踪和实时AI反馈探索人类情感，采用参与者驱动的情感定义，推动包容性和伦理的情感计算。


<details>
  <summary>Details</summary>
Motivation: 旨在突出以参与者驱动的文化多样性情感定义，推动包容和伦理的情感计算。

Method: 通过全身运动跟踪和实时AI反馈，参与者在三个阶段（教学、探索和宇宙阶段）中与系统协作表达和解读情感。

Result: Commonaiverse安装通过结合MoveNet和多推荐AI系统，实现对情感状态的动态分析和适应性视听输出。

Conclusion: Commonaiverse提出了一种以参与者为中心的情感计算新模式，促进用户自主性和多样性，推动互动应用的发展。

Abstract: Commonaiverse is an interactive installation exploring human emotions through
full-body motion tracking and real-time AI feedback. Participants engage in
three phases: Teaching, Exploration and the Cosmos Phase, collaboratively
expressing and interpreting emotions with the system. The installation
integrates MoveNet for precise motion tracking and a multi-recommender AI
system to analyze emotional states dynamically, responding with adaptive
audiovisual outputs. By shifting from top-down emotion classification to
participant-driven, culturally diverse definitions, we highlight new pathways
for inclusive, ethical affective computing. We discuss how this collaborative,
out-of-the-box approach pushes multimedia research beyond single-user facial
analysis toward a more embodied, co-created paradigm of emotional AI.
Furthermore, we reflect on how this reimagined framework fosters user agency,
reduces bias, and opens avenues for advanced interactive applications.

</details>


### [15] [Human Autonomy and Sense of Agency in Human-Robot Interaction: A Systematic Literature Review](https://arxiv.org/abs/2509.22271)
*Felix Glawe,Tim Schmeckel,Philipp Brauner,Martina Ziefle*

Main category: cs.HC

TL;DR: 本综述探讨了人类自主性和代理感在机器人交互中的重要性，分析了相关研究并提出未来发展的建议。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能技术的快速发展，机器人在作为同事和伴侣方面的能力日益增强，因此需要关注用户的自主性和代理感，以确保人机交互的道德性和效益。

Method: 通过系统文献综述，分析从728篇文献中筛选出的22篇实证研究，探讨在人机交互中如何维护和促进人类自主性和代理感。

Result: 研究揭示了五个影响人类自主性和代理感的重要因素：机器人适应性、沟通风格、人性化、机器人存在感和个体差异，同时指出了目前实证研究的局限和未来研究的方向。

Conclusion: 本文强调人类自主性和代理感在机器人交互中的重要性，并提出未来研究需要更系统的定义和操作方法，以促进人本设计的机器人开发。

Abstract: Human autonomy and sense of agency are increasingly recognised as critical
for user well-being, motivation, and the ethical deployment of robots in
human-robot interaction (HRI). Given the rapid development of artificial
intelligence, robot capabilities and their potential to function as colleagues
and companions are growing. This systematic literature review synthesises 22
empirical studies selected from an initial pool of 728 articles published
between 2011 and 2024. Articles were retrieved from major scientific databases
and identified based on empirical focus and conceptual relevance, namely, how
to preserve and promote human autonomy and sense of agency in HRI. Derived
through thematic synthesis, five clusters of potentially influential factors
are revealed: robot adaptiveness, communication style, anthropomorphism,
presence of a robot and individual differences. Measured through psychometric
scales or the intentional binding paradigm, perceptions of autonomy and agency
varied across industrial, educational, healthcare, care, and hospitality
settings. The review underscores the theoretical differences between both
concepts, but their yet entangled use in HRI. Despite increasing interest, the
current body of empirical evidence remains limited and fragmented, underscoring
the necessity for standardised definitions, more robust operationalisations,
and further exploratory and qualitative research. By identifying existing gaps
and highlighting emerging trends, this review contributes to the development of
human-centered, autonomy-supportive robot design strategies that uphold ethical
and psychological principles, ultimately supporting well-being in human-robot
interaction.

</details>


### [16] [Trust and Human Autonomy after Cobot Failures: Communication is Key for Industry 5.0](https://arxiv.org/abs/2509.22298)
*Felix Glawe,Laura Kremer,Luisa Vervier,Philipp Brauner,Martina Ziefle*

Main category: cs.HC

TL;DR: 本研究探讨了协作机器人故障对人类信任和自主性的影响，发现故障严重程度影响信任，透明沟通能部分恢复两者。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨系统故障如何影响人类对协作机器人的信任和自主性，以及如何恢复这种信任和自主性，因为这些因素在工业5.0中至关重要。

Method: 通过VR实验（参与者39人）研究协作机器人故障及其严重程度对人类信任和自主性的影响，并考察透明沟通的作用。

Result: 研究结果显示，协作机器人故障会使信任和自主性下降，故障的严重程度对信任影响较大，但对刷自主性影响较小。透明沟通可以帮助部分恢复信任和自主性。

Conclusion: 在协作机器人发生故障后，信任和自主性都会受到影响，其中故障的严重程度对信任的影响更为显著，但对自主性的影响较小。透明的沟通可以部分恢复信任和自主性。

Abstract: Collaborative robots (cobots) are a core technology of Industry 4.0. Industry
4.0 uses cyber-physical systems, IoT and smart automation to improve efficiency
and data-driven decision-making. Cobots, as cyber-physical systems, enable the
introduction of lightweight automation to smaller companies through their
flexibility, low cost and ability to work alongside humans, while keeping
humans and their skills in the loop. Industry 5.0, the evolution of Industry
4.0, places the worker at the centre of its principles: The physical and mental
well-being of the worker is the main goal of new technology design, not just
productivity, efficiency and safety standards. Within this concept, human trust
in cobots and human autonomy are important. While trust is essential for
effective and smooth interaction, the workers' perception of autonomy is key to
intrinsic motivation and overall well-being. As failures are an inevitable part
of technological systems, this study aims to answer the question of how system
failures affect trust in cobots as well as human autonomy, and how they can be
recovered afterwards. Therefore, a VR experiment (n = 39) was set up to
investigate the influence of a cobot failure and its severity on human autonomy
and trust in the cobot. Furthermore, the influence of transparent communication
about the failure and next steps was investigated. The results show that both
trust and autonomy suffer after cobot failures, with the severity of the
failure having a stronger negative impact on trust, but not on autonomy. Both
trust and autonomy can be partially restored by transparent communication.

</details>


### [17] [Machines in the Margins: A Systematic Review of Automated Content Generation for Wikipedia](https://arxiv.org/abs/2509.22443)
*Neal Reeves,Elena Simperl*

Main category: cs.HC

TL;DR: 本研究回顾了文献中关于自动生成维基百科内容的方法和评估，探讨了其对用户生成内容和维基百科的影响及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管现有研究集中在已部署的维基百科自动代理上，但本文旨在填补未被充分研究的AI支持对维基百科社区贡献的空白。

Method: 通过系统文献回顾，分析自动内容生成代理在维基百科中的实现和评估方法。

Result: 识别了内容生成方法的范围、所用技术和模型、生成所用的源内容以及支持生成过程的评估方法。

Conclusion: 本研究系统性地回顾了文献中有关自动内容生成代理对维基百科的贡献，指出了方法、技术、源内容和评价方法，并提出了未来的研究方向。

Abstract: Wikipedia is among the largest examples of collective intelligence on the Web
with over 61 million articles covering over 320 languages. Although edited and
maintained by an active workforce of human volunteers, Wikipedia is highly
reliant on automated bots to fill gaps in its human workforce. As well as
administrative and governance tasks, these bots also play a role in generating
content, although to date such agents represent the smallest proportion of
bots. While there has been considerable analysis of bots and their activity in
Wikipedia, such work captures only automated agents that have been actively
deployed to Wikipedia and fails to capture the methods that have been proposed
to generate Wikipedia content in the wider literature. In this paper, we
conduct a systematic literature review to explore how researchers have
operationalised and evaluated automated content-generation agents for
Wikipedia. We identify the scope of these generation methods, the techniques
and models used, the source content used for generation and the evaluation
methodologies which support generation processes. We also explore implications
of our findings to CSCW, User Generated Content and Wikipedia, as well as
research directions for future development. To the best of our knowledge, we
are among the first to review the potential contributions of this understudied
form of AI support for the Wikipedia community beyond the implementation of
bots.

</details>


### [18] [Mental Health Impacts of AI Companions: Triangulating Social Media Quasi-Experiments, User Perspectives, and Relational Theory](https://arxiv.org/abs/2509.22505)
*Yunhao Yuan,Jiaxun Zhang,Talayeh Aledavood,Renwen Zhang,Koustuv Saha*

Main category: cs.HC

TL;DR: AI聊天机器人提供情感支持，但也引发孤独和依赖风险，需要设计健康的互动模式。


<details>
  <summary>Details</summary>
Motivation: 探讨AI陪伴聊天机器人对用户心理社会影响及用户的体验认知。

Method: 结合大规模准实验研究和15次半结构化访谈，总结AI陪伴聊天机器人的使用影响。

Result: 用户在与AI聊天机器人的互动中经历了情感验证和社交练习，但也出现了孤独和自杀意念语言的增加。

Conclusion: AICCs在提供情感支持的同时，可能导致用户过度依赖和情感孤立，因此设计应关注健康界限和支持性互动。

Abstract: AI-powered companion chatbots (AICCs) such as Replika are increasingly
popular, offering empathetic interactions, yet their psychosocial impacts
remain unclear. We examined how engaging with AICCs shaped wellbeing and how
users perceived these experiences. First, we conducted a large-scale
quasi-experimental study of longitudinal Reddit data, applying stratified
propensity score matching and Difference-in-Differences regression. Findings
revealed mixed effects -- greater affective and grief expression, readability,
and interpersonal focus, alongside increases in language about loneliness and
suicidal ideation. Second, we complemented these results with 15
semi-structured interviews, which we thematically analyzed and contextualized
using Knapp's relationship development model. We identified trajectories of
initiation, escalation, and bonding, wherein AICCs provided emotional
validation and social rehearsal but also carried risks of over-reliance and
withdrawal. Triangulating across methods, we offer design implications for AI
companions that scaffold healthy boundaries, support mindful engagement,
support disclosure without dependency, and surface relationship stages --
maximizing psychosocial benefits while mitigating risks.

</details>


### [19] [Does AI Coaching Prepare us for Workplace Negotiations?](https://arxiv.org/abs/2509.22545)
*Veda Duddu,Jash Rajesh Parekh,Andy Mao,Hanyi Min,Ziang Xiao,Vedant Das Swain,Koustuv Saha*

Main category: cs.HC

TL;DR: 本研究比较了AI教练与传统手册在谈判准备中的效果，发现传统手册在可用性上更优，而AI在减轻恐惧方面表现更佳，提示需要更有效的混合设计。


<details>
  <summary>Details</summary>
Motivation: 探讨AI对提高谈判准备性的有效性，尤其是针对心理障碍的作用。

Method: 进行了一项267人参与的实验，比较了AI教练Trucey、ChatGPT和传统谈判手册的效果，并进行了15人的深度访谈。

Result: Trucey有效减轻恐惧感，但传统手册在可用性和赋权方面更佳，且AI的指导往往显得冗长与片段化，令参与者感到不安或不知所措。

Conclusion: Trucey在减轻恐惧方面效果最佳，但传统手册在可用性和心理赋权上表现更优。研究表明，AI并不一定总是优于传统方式，强调了结合理论驱动内容与AI辅导的混合设计的必要性。

Abstract: Workplace negotiations are undermined by psychological barriers, which can
even derail well-prepared tactics. AI offers personalized and always --
available negotiation coaching, yet its effectiveness for negotiation
preparedness remains unclear. We built Trucey, a prototype AI coach grounded in
Brett's negotiation model. We conducted a between-subjects experiment (N=267),
comparing Trucey, ChatGPT, and a traditional negotiation Handbook, followed by
in-depth interviews (N=15). While Trucey showed the strongest reductions in
fear relative to both comparison conditions, the Handbook outperformed both AIs
in usability and psychological empowerment. Interviews revealed that the
Handbook's comprehensive, reviewable content was crucial for participants'
confidence and preparedness. In contrast, although participants valued AI's
rehearsal capability, its guidance often felt verbose and fragmented --
delivered in bits and pieces that required additional effort -- leaving them
uncertain or overwhelmed. These findings challenge assumptions of AI
superiority and motivate hybrid designs that integrate structured,
theory-driven content with targeted rehearsal, clear boundaries, and adaptive
scaffolds to address psychological barriers and support negotiation
preparedness.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [20] [Language-in-the-Loop Culvert Inspection on the Erie Canal](https://arxiv.org/abs/2509.21370)
*Yashom Dighe,Yash Turkar,Karthik Dantu*

Main category: cs.RO

TL;DR: 本研究开发了VISION系统，通过自动化检测渠管，提高了检测效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 老旧渠管的人工检查面临许多挑战，因此需要一种新的自主检查方法。

Method: 结合了网络规模的视觉-语言模型和受限视角规划的全自动检测系统。

Result: 初步区域兴趣提案与专家达成61.4%的一致率，最终评估达到了80%。

Conclusion: VISION系统能够有效地进行渠管的自主检测，并且在与专家的评估中显示出良好的一致性。

Abstract: Culverts on canals such as the Erie Canal, built originally in 1825, require
frequent inspections to ensure safe operation. Human inspection of culverts is
challenging due to age, geometry, poor illumination, weather, and lack of easy
access. We introduce VISION, an end-to-end, language-in-the-loop autonomy
system that couples a web-scale vision-language model (VLM) with constrained
viewpoint planning for autonomous inspection of culverts. Brief prompts to the
VLM solicit open-vocabulary ROI proposals with rationales and confidences,
stereo depth is fused to recover scale, and a planner -- aware of culvert
constraints -- commands repositioning moves to capture targeted close-ups.
Deployed on a quadruped in a culvert under the Erie Canal, VISION closes the
see, decide, move, re-image loop on-board and produces high-resolution images
for detailed reporting without domain-specific fine-tuning. In an external
evaluation by New York Canal Corporation personnel, initial ROI proposals
achieved 61.4\% agreement with subject-matter experts, and final
post-re-imaging assessments reached 80\%, indicating that VISION converts
tentative hypotheses into grounded, expert-aligned findings.

</details>


### [21] [Developing a Mono-Actuated Compliant GeoGami Robot](https://arxiv.org/abs/2509.21445)
*Archie Webster,Lee Skull,Seyed Amir Tafrishi*

Main category: cs.RO

TL;DR: 本研究设计了一种新型柔性-刚性机器人平台GeoGami，利用折纸结构实现形状变换与运动，提出了一种通过单一驱动器控制的创新方法。


<details>
  <summary>Details</summary>
Motivation: 利用折纸结构的特性，实现形状收缩并支持以欠驱动形式的 locomotion，但折纸表面复杂度高，需众多驱动器，因此提出集成表面顺应性以提高可重复性。

Method: 提出了一种单电动元件驱动的GeoGami移动平台，结合了折纸表面顺应性与几何顺应骨架，允许机器人通过单一驱动器实现变形和运动。

Result: 开发了GeoGami机器人，建立了刚度模型，并描述了中央变速箱机制。同时分析了骷髅骨架的替代电缆驱动方法，评估了平台的形状变换能力和滚动能力。

Conclusion: GeoGami平台在实现形状变换和滚动方面展现了新的能力，为能够改变形状以进入不同环境的机器人开辟了新机遇。

Abstract: This paper presents the design of a new soft-rigid robotic platform,
"GeoGami". We leverage origami surface capabilities to achieve shape
contraction and to support locomotion with underactuated forms. A key challenge
is that origami surfaces have high degrees of freedom and typically require
many actuators; we address repeatability by integrating surface compliance. We
propose a mono-actuated GeoGami mobile platform that combines origami surface
compliance with a geometric compliant skeleton, enabling the robot to transform
and locomote using a single actuator. We demonstrate the robot, develop a
stiffness model, and describe the central gearbox mechanism. We also analyze
alternative cable-driven actuation methods for the skeleton to enable surface
transformation. Finally, we evaluate the GeoGami platform for capabilities,
including shape transformation and rolling. This platform opens new
capabilities for robots that change shape to access different environments and
that use shape transformation for locomotion.

</details>


### [22] [Wall Inspector: Quadrotor Control in Wall-proximity Through Model Compensation](https://arxiv.org/abs/2509.21496)
*Peiwen Yang,Weisong Wen,Runqiu Yang,Yingming Chen,Cheuk Chi Tsang*

Main category: cs.RO

TL;DR: 研究提出了一种吸力补偿的模型预测控制框架(SC-MPC)，显著提高了四旋翼在近墙环境中的轨迹控制精度，并开源了完整实现。


<details>
  <summary>Details</summary>
Motivation: 传统控制方法在四旋翼在近墙飞行时受到未建模气动效应的影响，容易产生不稳定的振动和碰撞。因此，有必要开发一种新的控制框架来应对这些挑战。

Method: 提出了一种吸力补偿模型预测控制(SC-MPC)框架，该框架结合了考虑吸力效应的增强动力学模型，解决了系统动态约束、轨迹跟踪目标、控制输入平滑性和执行器物理限制等问题。

Result: 使用SC-MPC框架，四旋翼在实验中在X轴和Y轴位置控制上分别实现了2.1 cm和2.0 cm的均方根误差(RMSE)，比级联比例-积分-微分(PID)控制方法分别提高了74%和79%，比标准模型预测控制(MPC)提高了60%和53%。

Conclusion: 通过开发一种基于物理的吸力模型和吸力补偿的模型预测控制框架(SC-MPC)，实现了四旋翼在近墙操作中的精确稳定轨迹跟踪，并在实验中表现出明显优于其他控制方法的性能。

Abstract: The safe operation of quadrotors in near-wall urban or indoor environments
(e.g., inspection and search-and-rescue missions) is challenged by unmodeled
aerodynamic effects arising from wall-proximity. It generates complex vortices
that induce destabilizing suction forces, potentially leading to hazardous
vibrations or collisions. This paper presents a comprehensive solution
featuring (1) a physics-based suction force model that explicitly characterizes
the dependency on both rotor speed and wall distance, and (2) a
suction-compensated model predictive control (SC-MPC) framework designed to
ensure accurate and stable trajectory tracking during wall-proximity
operations. The proposed SC-MPC framework incorporates an enhanced dynamics
model that accounts for suction force effects, formulated as a factor graph
optimization problem integrating system dynamics constraints, trajectory
tracking objectives, control input smoothness requirements, and actuator
physical limitations. The suction force model parameters are systematically
identified through extensive experimental measurements across varying
operational conditions. Experimental validation demonstrates SC-MPC's superior
performance, achieving 2.1 cm root mean squared error (RMSE) in X-axis and 2.0
cm RMSE in Y-axis position control - representing 74% and 79% improvements over
cascaded proportional-integral-derivative (PID) control, and 60% and 53%
improvements over standard MPC respectively. The corresponding mean absolute
error (MAE) metrics (1.2 cm X-axis, 1.4 cm Y-axis) similarly outperform both
baselines. The evaluation platform employs a ducted quadrotor design that
provides collision protection while maintaining aerodynamic efficiency. To
facilitate reproducibility and community adoption, we have open-sourced our
complete implementation, available at
https://anonymous.4open.science/r/SC-MPC-6A61.

</details>


### [23] [DroneFL: Federated Learning for Multi-UAV Visual Target Tracking](https://arxiv.org/abs/2509.21523)
*Xiaofan Yu,Yuwei Wu,Katherine Mao,Ye Tian,Vijay Kumar,Tajana Rosing*

Main category: cs.RO

TL;DR: DroneFL是一个专为多无人机目标追踪设计的联邦学习框架，能够有效减少预测误差和跟踪距离，提升整体追踪性能。


<details>
  <summary>Details</summary>
Motivation: 解决多机器人目标追踪中的计算资源有限、数据异质性大和轨迹预测与多机器人规划紧密耦合的挑战。

Method: 构建轻量级本地模型，结合YOLO骨干网络和浅层变换器进行目标轨迹预测，并在云端聚合更新后的模型。

Result: 与分布式非FL框架相比，DroneFL的预测误差减少了6%-83%，跟踪距离减少了0.4%-4.6%。

Conclusion: DroneFL通过优化多无人机目标追踪的联邦学习框架，显著提升了预测准确度和跟踪性能，且有效解决了数据异质性问题。

Abstract: Multi-robot target tracking is a fundamental problem that requires
coordinated monitoring of dynamic entities in applications such as precision
agriculture, environmental monitoring, disaster response, and security
surveillance. While Federated Learning (FL) has the potential to enhance
learning across multiple robots without centralized data aggregation, its use
in multi-Unmanned Aerial Vehicle (UAV) target tracking remains largely
underexplored. Key challenges include limited onboard computational resources,
significant data heterogeneity in FL due to varying targets and the fields of
view, and the need for tight coupling between trajectory prediction and
multi-robot planning. In this paper, we introduce DroneFL, the first federated
learning framework specifically designed for efficient multi-UAV target
tracking. We design a lightweight local model to predict target trajectories
from sensor inputs, using a frozen YOLO backbone and a shallow transformer for
efficient onboard training. The updated models are periodically aggregated in
the cloud for global knowledge sharing. To alleviate the data heterogeneity
that hinders FL convergence, DroneFL introduces a position-invariant model
architecture with altitude-based adaptive instance normalization. Finally, we
fuse predictions from multiple UAVs in the cloud and generate optimal
trajectories that balance target prediction accuracy and overall tracking
performance. Our results show that DroneFL reduces prediction error by 6%-83%
and tracking distance by 0.4%-4.6% compared to a distributed non-FL framework.
In terms of efficiency, DroneFL runs in real time on a Raspberry Pi 5 and has
on average just 1.56 KBps data rate to the cloud.

</details>


### [24] [Plan2Evolve: LLM Self-Evolution for Improved Planning Capability via Automated Domain Generation](https://arxiv.org/abs/2509.21543)
*Jinbang Huang,Zhiyuan Li,Zhanguang Zhang,Xingyue Quan,Jianye Hao,Yingxue Zhang*

Main category: cs.RO

TL;DR: 提出了一个自我进化的大语言模型框架Plan2Evolve，通过生成规划域和自然语言解释，提高了机器人任务规划的能力，包括成功率和泛化性。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在机器人任务规划中的潜力，特别是如何利用自生成的数据来提高推理能力，减少对人工策划数据集的依赖。

Method: 提出了Plan2Evolve框架，使基本模型生成规划域，并产生符号问题-计划对作为推理轨迹。

Result: 通过生成的数据和扩展的链式推理轨迹，提高了模型的规划成功率，增强了跨任务泛化能力，减少了推理成本。

Conclusion: Plan2Evolve框架通过自我进化生成规划域，显著提高了大语言模型在机器人任务规划中的能力。

Abstract: Large Language Models (LLMs) have recently shown strong potential in robotic
task planning, particularly through automatic planning domain generation that
integrates symbolic search. Prior approaches, however, have largely treated
these domains as search utilities, with limited attention to their potential as
scalable sources of reasoning data. At the same time, progress in reasoning
LLMs has been driven by chain-of-thought (CoT) supervision, whose application
in robotics remains dependent on costly, human-curated datasets. We propose
Plan2Evolve, an LLM self-evolving framework in which the base model generates
planning domains that serve as engines for producing symbolic problem-plan
pairs as reasoning traces. These pairs are then transformed into extended CoT
trajectories by the same model through natural-language explanations, thereby
explicitly aligning symbolic planning structures with natural language
reasoning. The resulting data extend beyond the model's intrinsic planning
capacity, enabling model fine-tuning that yields a planning-enhanced LLM with
improved planning success, stronger cross-task generalization, and reduced
inference costs.

</details>


### [25] [PL-VIWO2: A Lightweight, Fast and Robust Visual-Inertial-Wheel Odometry Using Points and Lines](https://arxiv.org/abs/2509.21563)
*Zhixin Zhang,Liang Zhao,Pawel Ladosz*

Main category: cs.RO

TL;DR: PL-VIWO2通过整合多种传感器和创新的方法，提高了在复杂城市环境中的视觉里程计性能。


<details>
  <summary>Details</summary>
Motivation: 解决视觉里程计在复杂的城市环境中的性能下降问题，提升长期状态估计的鲁棒性。

Method: 提出了一种滤波的视觉惯性轮式里程计系统，整合了IMU、轮编码器和相机，支持单目和立体视觉，采用新颖的线特征处理框架、SE(2)约束的SE(3)轮预积分方法和运动一致性检验。

Result: 在Monte Carlo仿真和公共自动驾驶数据集上的广泛实验表明，PL-VIWO2显著提升了准确性和效率。

Conclusion: PL-VIWO2系统在准确性、效率和鲁棒性方面优于现有的最先进方法。

Abstract: Vision-based odometry has been widely adopted in autonomous driving owing to
its low cost and lightweight setup; however, its performance often degrades in
complex outdoor urban environments. To address these challenges, we propose
PL-VIWO2, a filter-based visual-inertial-wheel odometry system that integrates
an IMU, wheel encoder, and camera (supporting both monocular and stereo) for
long-term robust state estimation. The main contributions are: (i) a novel line
feature processing framework that exploits the geometric relationship between
2D feature points and lines, enabling fast and robust line tracking and
triangulation while ensuring real-time performance; (ii) an SE(2)-constrained
SE(3) wheel pre-integration method that leverages the planar motion
characteristics of ground vehicles for accurate wheel updates; and (iii) an
efficient motion consistency check (MCC) that filters out dynamic features by
jointly using IMU and wheel measurements. Extensive experiments on Monte Carlo
simulations and public autonomous driving datasets demonstrate that PL-VIWO2
outperforms state-of-the-art methods in terms of accuracy, efficiency, and
robustness.

</details>


### [26] [Autonomous UAV-Quadruped Docking in Complex Terrains via Active Posture Alignment and Constraint-Aware Control](https://arxiv.org/abs/2509.21571)
*HaoZhe Xu,Cheng Cheng,HongRui Sang,Zhipeng Wang,Qiyong He,Xiuxian Li,Bin He*

Main category: cs.RO

TL;DR: 本研究提出了一种自主UAV-四足机器人对接框架，实现了在GPS缺失环境中，四足机器人为UAV提供稳定对接平台，验证了高难度地形的对接成功率。


<details>
  <summary>Details</summary>
Motivation: 现有针对轮式平台的对接方法无法满足复杂地形需求，而四足机器人的适应性更强，但其频繁的姿态变化使UAV对接困难。

Method: 采用深度强化学习的HIM-HA稳定四足机器人的躯干，并结合YOLOv8、NFTSMC及BF等技术实现三阶段的UAV对接控制。

Result: 在模拟及实际场景中验证了提议的框架的有效性，成功实现了复杂地形环境中的自主对接。

Conclusion: 提出的自主UAV-四足机器人对接框架在复杂环境中表现出色，成功实现了高达17厘米的户外台阶和超过30度的陡坡对接。

Abstract: Autonomous docking between Unmanned Aerial Vehicles (UAVs) and ground robots
is essential for heterogeneous systems, yet most existing approaches target
wheeled platforms whose limited mobility constrains exploration in complex
terrains. Quadruped robots offer superior adaptability but undergo frequent
posture variations, making it difficult to provide a stable landing surface for
UAVs. To address these challenges, we propose an autonomous UAV-quadruped
docking framework for GPS-denied environments. On the quadruped side, a Hybrid
Internal Model with Horizontal Alignment (HIM-HA), learned via deep
reinforcement learning, actively stabilizes the torso to provide a level
platform. On the UAV side, a three-phase strategy is adopted, consisting of
long-range acquisition with a median-filtered YOLOv8 detector, close-range
tracking with a constraint-aware controller that integrates a Nonsingular Fast
Terminal Sliding Mode Controller (NFTSMC) and a logarithmic Barrier Function
(BF) to guarantee finite-time error convergence under field-of-view (FOV)
constraints, and terminal descent guided by a Safety Period (SP) mechanism that
jointly verifies tracking accuracy and platform stability. The proposed
framework is validated in both simulation and real-world scenarios,
successfully achieving docking on outdoor staircases higher than 17 cm and
rough slopes steeper than 30 degrees. Supplementary materials and videos are
available at: https://uav-quadruped-docking.github.io.

</details>


### [27] [Real-Time Indoor Object SLAM with LLM-Enhanced Priors](https://arxiv.org/abs/2509.21602)
*Yang Jiao,Yiding Qiu,Henrik I. Christensen*

Main category: cs.RO

TL;DR: 通过结合大型语言模型的常识知识，本文提出的SLAM方法能够在物体映射中显著提高精度，并实现实时性能。


<details>
  <summary>Details</summary>
Motivation: 旨在解决现有SLAM方法在对象级别映射中，由于稀疏观察导致的优化不足的问题，并提高物体观察的有效性。

Method: 通过利用大型语言模型（LLMs）提供的物体几何属性的常识知识，集成于基于图的SLAM框架中。

Result: 在TUM RGB-D和3RScan数据集上的评估显示，映射准确性提升了36.8%，且系统具备实时执行能力。

Conclusion: 本文提出的SLAM方法在稀疏观察下实现了显著的映射准确性提升，且在实时性能方面表现优越。

Abstract: Object-level Simultaneous Localization and Mapping (SLAM), which incorporates
semantic information for high-level scene understanding, faces challenges of
under-constrained optimization due to sparse observations. Prior work has
introduced additional constraints using commonsense knowledge, but obtaining
such priors has traditionally been labor-intensive and lacks generalizability
across diverse object categories. We address this limitation by leveraging
large language models (LLMs) to provide commonsense knowledge of object
geometric attributes, specifically size and orientation, as prior factors in a
graph-based SLAM framework. These priors are particularly beneficial during the
initial phase when object observations are limited. We implement a complete
pipeline integrating these priors, achieving robust data association on sparse
object-level features and enabling real-time object SLAM. Our system, evaluated
on the TUM RGB-D and 3RScan datasets, improves mapping accuracy by 36.8\% over
the latest baseline. Additionally, we present real-world experiments in the
supplementary video, demonstrating its real-time performance.

</details>


### [28] [Generating Stable Placements via Physics-guided Diffusion Models](https://arxiv.org/abs/2509.21664)
*Philippe Nadeau,Miguel Rogel,Ivan Bilić,Ivan Petrović,Jonathan Kelly*

Main category: cs.RO

TL;DR: 本研究提出一种将稳定性融入扩散模型的采样过程的新方法，显著提高了物体放置的稳定性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决机器人操作中多物体场景下的物体放置稳定性问题。

Method: 使用离线采样规划器收集多模态放置标签，并训练扩散模型生成稳定放置，结合稳定性损失函数，采用物理引导的模型。

Result: 在四个基准场景中验证了该方法，显著提高了稳健性，并减少了运行时间。

Conclusion: 本研究提出了一种新方法，通过将稳定性直接融入扩散模型的采样过程中，从而提高了物体在多物体场景中的放置稳定性，使所生成的位置更加稳健且计算效率更高。

Abstract: Stably placing an object in a multi-object scene is a fundamental challenge
in robotic manipulation, as placements must be penetration-free, establish
precise surface contact, and result in a force equilibrium. To assess
stability, existing methods rely on running a simulation engine or resort to
heuristic, appearance-based assessments. In contrast, our approach integrates
stability directly into the sampling process of a diffusion model. To this end,
we query an offline sampling-based planner to gather multi-modal placement
labels and train a diffusion model to generate stable placements. The diffusion
model is conditioned on scene and object point clouds, and serves as a
geometry-aware prior. We leverage the compositional nature of score-based
generative models to combine this learned prior with a stability-aware loss,
thereby increasing the likelihood of sampling from regions of high stability.
Importantly, this strategy requires no additional re-training or fine-tuning,
and can be directly applied to off-the-shelf models. We evaluate our method on
four benchmark scenes where stability can be accurately computed. Our
physics-guided models achieve placements that are 56% more robust to forceful
perturbations while reducing runtime by 47% compared to a state-of-the-art
geometric method.

</details>


### [29] [Towards Versatile Humanoid Table Tennis: Unified Reinforcement Learning with Prediction Augmentation](https://arxiv.org/abs/2509.21690)
*Muqun Hu,Wenxi Chen,Wenjing Li,Falak Mandali,Zijian He,Renhong Zhang,Praveen Krisna,Katherine Christian,Leo Benaharon,Dizhi Ma,Karthik Ramani,Yan Gu*

Main category: cs.RO

TL;DR: 本研究提出了一种强化学习框架，通过预测信号和物理奖励提升人形机器人乒乓球的表现，取得了高击球率和成功率。


<details>
  <summary>Details</summary>
Motivation:  humanoid乒乓球需要快速感知、主动的全身运动和灵活的脚步，这些能力在统一控制器中仍然很难实现。

Method: 提出了一种强化学习框架，将球的位置观察直接映射到全身关节指令，并利用预测信号和密集的物理引导奖励增强学习效果。

Result: 在多种发球范围内，政策的击球率达到96%，成功率达到92%。在物理Booster T1人形机器人上零-shot部署，展现出出色的协调运动和快速回球。

Conclusion: 该政策在模拟中表现强劲，成功率超过96%，显示了其在真实环境中的有效性。

Abstract: Humanoid table tennis (TT) demands rapid perception, proactive whole-body
motion, and agile footwork under strict timing -- capabilities that remain
difficult for unified controllers. We propose a reinforcement learning
framework that maps ball-position observations directly to whole-body joint
commands for both arm striking and leg locomotion, strengthened by predictive
signals and dense, physics-guided rewards. A lightweight learned predictor, fed
with recent ball positions, estimates future ball states and augments the
policy's observations for proactive decision-making. During training, a
physics-based predictor supplies precise future states to construct dense,
informative rewards that lead to effective exploration. The resulting policy
attains strong performance across varied serve ranges (hit rate $\geq$ 96% and
success rate $\geq$ 92%) in simulations. Ablation studies confirm that both the
learned predictor and the predictive reward design are critical for end-to-end
learning. Deployed zero-shot on a physical Booster T1 humanoid with 23 revolute
joints, the policy produces coordinated lateral and forward-backward footwork
with accurate, fast returns, suggesting a practical path toward versatile,
competitive humanoid TT.

</details>


### [30] [VLBiMan: Vision-Language Anchored One-Shot Demonstration Enables Generalizable Robotic Bimanual Manipulation](https://arxiv.org/abs/2509.21723)
*Huayi Zhou,Kui Jia*

Main category: cs.RO

TL;DR: VLBiMan框架通过从单一示例学习，并通过视觉-语言适应实现了高效的双手操控，具有很强的泛化和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了实现可泛化的双手操作，系统需能通过最小的人类输入有效学习，同时适应现实世界的不确定性与多样性。

Method: 引入VLBiMan框架，通过从单个示例中提取可重用技能，并结合视觉-语言基础进行动态调整。

Result: 实验表明，VLBiMan显著减少了演示要求，展示了复杂性的组成泛化，具备对新对象和外部干扰的鲁棒性，以及跨平台技能传递的能力。

Conclusion: VLBiMan框架通过任务感知的分解与视觉-语言结合的适应机制，实现了高效的双手操控学习，且具备较强的通用性与鲁棒性。

Abstract: Achieving generalizable bimanual manipulation requires systems that can learn
efficiently from minimal human input while adapting to real-world uncertainties
and diverse embodiments. Existing approaches face a dilemma: imitation policy
learning demands extensive demonstrations to cover task variations, while
modular methods often lack flexibility in dynamic scenes. We introduce VLBiMan,
a framework that derives reusable skills from a single human example through
task-aware decomposition, preserving invariant primitives as anchors while
dynamically adapting adjustable components via vision-language grounding. This
adaptation mechanism resolves scene ambiguities caused by background changes,
object repositioning, or visual clutter without policy retraining, leveraging
semantic parsing and geometric feasibility constraints. Moreover, the system
inherits human-like hybrid control capabilities, enabling mixed synchronous and
asynchronous use of both arms. Extensive experiments validate VLBiMan across
tool-use and multi-object tasks, demonstrating: (1) a drastic reduction in
demonstration requirements compared to imitation baselines, (2) compositional
generalization through atomic skill splicing for long-horizon tasks, (3)
robustness to novel but semantically similar objects and external disturbances,
and (4) strong cross-embodiment transfer, showing that skills learned from
human demonstrations can be instantiated on different robotic platforms without
retraining. By bridging human priors with vision-language anchored adaptation,
our work takes a step toward practical and versatile dual-arm manipulation in
unstructured settings.

</details>


### [31] [The Turkish Ice Cream Robot: Examining Playful Deception in Social Human-Robot Interactions](https://arxiv.org/abs/2509.21776)
*Hyeonseong Kim,Roy El-Helou,Seungbeen Lee,Sungjoon Choi,Matthew Pan*

Main category: cs.RO

TL;DR: 本研究调查了受土耳其冰淇淋小贩启发的顽皮欺骗对用户信任、享受和参与度的影响，结果显示这种欺骗策略可以提升用户体验，但也带来了安全性和信任的降低。


<details>
  <summary>Details</summary>
Motivation: 探讨人类社会互动中的顽皮欺骗现象在机器人交互(HRI)中的应用和影响。

Method: 设计了一种带有定制末端执行器的机器人，并实现了五种受土耳其冰淇淋小贩启发的欺骗策略，这些策略巧妙地延迟了冰淇淋形状物体的交接。

Result: 研究结果表明，受土耳其冰淇淋启发的欺骗显著增强了用户的享受和参与度，但降低了感知安全性和信任感，表明在多维方面存在结构上的权衡。

Conclusion: 顽皮的欺骗可以作为互动机器人设计中的一种有价值策略，尤其是在娱乐和参与度高的背景下，但需谨慎考虑其复杂的权衡。

Abstract: Playful deception, a common feature in human social interactions, remains
underexplored in Human-Robot Interaction (HRI). Inspired by the Turkish Ice
Cream (TIC) vendor routine, we investigate how bounded, culturally familiar
forms of deception influence user trust, enjoyment, and engagement during
robotic handovers. We design a robotic manipulator equipped with a custom
end-effector and implement five TIC-inspired trick policies that deceptively
delay the handover of an ice cream-shaped object. Through a mixed-design user
study with 91 participants, we evaluate the effects of playful deception and
interaction duration on user experience. Results reveal that TIC-inspired
deception significantly enhances enjoyment and engagement, though reduces
perceived safety and trust, suggesting a structured trade-off across the
multi-dimensional aspects. Our findings demonstrate that playful deception can
be a valuable design strategy for interactive robots in entertainment and
engagement-focused contexts, while underscoring the importance of deliberate
consideration of its complex trade-offs. You can find more information,
including demonstration videos, on
https://hyeonseong-kim98.github.io/turkish-ice-cream-robot/ .

</details>


### [32] [Learning Multi-Skill Legged Locomotion Using Conditional Adversarial Motion Priors](https://arxiv.org/abs/2509.21810)
*Ning Huang,Zhentao Xie,Qinchuan Li*

Main category: cs.RO

TL;DR: 研发出一种多技能学习框架，旨在提升四足机器人在复杂环境中的运动技能获取能力。


<details>
  <summary>Details</summary>
Motivation: 尽管对开发模仿生物步态的四足机器人以应对复杂环境的兴趣日增，但在单一策略下获取多种运动技能仍然是一个根本挑战。

Method: 采用条件对抗运动先验（CAMP）框架，结合技能鉴别器和条件奖励设计，实现精准的技能重建。

Result: 通过该框架，四足机器人能够高效获得多样化的运动技能，并支持技能的主动控制与重用。

Conclusion: 提出了一种基于条件对抗运动先验的多技能学习框架，以支持四足机器人从专家演示中有效地获取多样的运动技能。

Abstract: Despite growing interest in developing legged robots that emulate biological
locomotion for agile navigation of complex environments, acquiring a diverse
repertoire of skills remains a fundamental challenge in robotics. Existing
methods can learn motion behaviors from expert data, but they often fail to
acquire multiple locomotion skills through a single policy and lack smooth
skill transitions. We propose a multi-skill learning framework based on
Conditional Adversarial Motion Priors (CAMP), with the aim of enabling
quadruped robots to efficiently acquire a diverse set of locomotion skills from
expert demonstrations. Precise skill reconstruction is achieved through a novel
skill discriminator and skill-conditioned reward design. The overall framework
supports the active control and reuse of multiple skills, providing a practical
solution for learning generalizable policies in complex environments.

</details>


### [33] [Improved Vehicle Maneuver Prediction using Game Theoretic Priors](https://arxiv.org/abs/2509.21873)
*Nishant Doshi*

Main category: cs.RO

TL;DR: 本研究提出了一种结合博弈论与传统模型的方式，以更精准地预测车辆行为，改善自适应巡航系统性能和燃油效率。


<details>
  <summary>Details</summary>
Motivation: 传统的分类模型在预测车道改变时的准确性不足，缺乏对全局场景的理解，亟需改进。

Method: 利用博弈论的层级推理模型，分析不同车辆间的相互作用，以生成最理性决策。

Result: 结合博弈论与传统运动分类模型，可以提供更准确的操控预测，提升如自适应巡航控制系统的效果，实现更高的燃油节省。

Conclusion: 该方法通过结合博弈论和传统分类模型，提高了对目标车辆最理性行为的预测准确性，进而优化了决策系统的性能。

Abstract: Conventional maneuver prediction methods use some sort of classification
model on temporal trajectory data to predict behavior of agents over a set time
horizon. Despite of having the best precision and recall, these models cannot
predict a lane change accurately unless they incorporate information about the
entire scene. Level-k game theory can leverage the human-like hierarchical
reasoning to come up with the most rational decisions each agent can make in a
group. This can be leveraged to model interactions between different vehicles
in presence of each other and hence compute the most rational decisions each
agent would make. The result of game theoretic evaluation can be used as a
"prior" or combined with a traditional motion-based classification model to
achieve more accurate predictions. The proposed approach assumes that the
states of the vehicles around the target lead vehicle are known. The module
will output the most rational maneuver prediction of the target vehicle based
on an online optimization solution. These predictions are instrumental in
decision making systems like Adaptive Cruise Control (ACC) or Traxen's
iQ-Cruise further improving the resulting fuel savings.

</details>


### [34] [WAVE: Worm Gear-based Adaptive Variable Elasticity for Decoupling Actuators from External Forces](https://arxiv.org/abs/2509.21878)
*Moses Gladson Selvamuthu,Tomoya Takahashi,Riichiro Tadakuma,Kazutoshi Tanaka*

Main category: cs.RO

TL;DR: 本研究介绍了一种新型的变刚度驱动器WAVE，利用蜗杆齿轮解耦外力，实现精确的力传递和动态刚度调节，适用于高接触率和复杂环境下的机器人应用。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够调节刚度和柔顺性的机器人传动系统，以提升安全性和应用灵活性。

Method: 引入基于蜗杆的自适应可变弹性（WAVE）驱动器，该驱动器集成了不可回驱动的蜗杆齿轮，通过调节弹簧的预压缩长度实现关节刚度的连续调节。

Result: 实验证明WAVE驱动器在静止状态下电机负载接近零，即便在外部负载下也能良好工作，验证了柔顺性模型。

Conclusion: 该研究表明WAVE能够成功解耦外部力，实现精确的力传递和动态的关节刚度调节，适合于高接触率应用及严苛环境下的机器人作业。

Abstract: Robotic manipulators capable of regulating both compliance and stiffness
offer enhanced operational safety and versatility. Here, we introduce Worm
Gear-based Adaptive Variable Elasticity (WAVE), a variable stiffness actuator
(VSA) that integrates a non-backdrivable worm gear. By decoupling the driving
motor from external forces using this gear, WAVE enables precise force
transmission to the joint, while absorbing positional discrepancies through
compliance. WAVE is protected from excessive loads by converting impact forces
into elastic energy stored in a spring. In addition, the actuator achieves
continuous joint stiffness modulation by changing the spring's precompression
length. We demonstrate these capabilities, experimentally validate the proposed
stiffness model, show that motor loads approach zero at rest--even under
external loading--and present applications using a manipulator with WAVE. This
outcome showcases the successful decoupling of external forces. The protective
attributes of this actuator allow for extended operation in contact-intensive
tasks, and for robust robotic applications in challenging environments.

</details>


### [35] [SAGE: Scene Graph-Aware Guidance and Execution for Long-Horizon Manipulation Tasks](https://arxiv.org/abs/2509.21928)
*Jialiang Li,Wenzheng Wu,Gaojing Zhang,Yifan Han,Wenzhao Lian*

Main category: cs.RO

TL;DR: SAGE框架通过语义情景图，连接了任务规划和低层控制，实现了长时间操控任务的进步，展现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的任务规划和图像控制方法在广泛适应性和语义推理方面存在不足，尤其是在长时间操控任务的复杂性面前。

Method: SAGE框架结合了基于情景图的任务规划和解耦结构图像编辑流程，利用视觉语言模型和大型语言模型解析环境并生成目标子目标图像。

Result: 通过构建基于语义情景图的任务规划和图像生成，SAGE提供了一种新方法，有效提升了长时间操控任务的执行能力。

Conclusion: SAGE框架在不同的长时间视任务上实现了最先进的性能，成功解决了高层语义规划与低层控制之间的差距。

Abstract: Successfully solving long-horizon manipulation tasks remains a fundamental
challenge. These tasks involve extended action sequences and complex object
interactions, presenting a critical gap between high-level symbolic planning
and low-level continuous control. To bridge this gap, two essential
capabilities are required: robust long-horizon task planning and effective
goal-conditioned manipulation. Existing task planning methods, including
traditional and LLM-based approaches, often exhibit limited generalization or
sparse semantic reasoning. Meanwhile, image-conditioned control methods
struggle to adapt to unseen tasks. To tackle these problems, we propose SAGE, a
novel framework for Scene Graph-Aware Guidance and Execution in Long-Horizon
Manipulation Tasks. SAGE utilizes semantic scene graphs as a structural
representation for scene states. A structural scene graph enables bridging
task-level semantic reasoning and pixel-level visuo-motor control. This also
facilitates the controllable synthesis of accurate, novel sub-goal images. SAGE
consists of two key components: (1) a scene graph-based task planner that uses
VLMs and LLMs to parse the environment and reason about physically-grounded
scene state transition sequences, and (2) a decoupled structural image editing
pipeline that controllably converts each target sub-goal graph into a
corresponding image through image inpainting and composition. Extensive
experiments have demonstrated that SAGE achieves state-of-the-art performance
on distinct long-horizon tasks.

</details>


### [36] [Learnable Conformal Prediction with Context-Aware Nonconformity Functions for Robotic Planning and Perception](https://arxiv.org/abs/2509.21955)
*Divake Kumar,Sina Tayebati,Francesco Migliarba,Ranganath Krishnan,Amit Ranjan Trivedi*

Main category: cs.RO

TL;DR: 可学习的非顺从预测（LCP）通过上下文感知的神经网络提高了机器人领域中预测的可靠性，减少了不确定性集合的大小，并在多个任务中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在机器人领域中对输入的不确定性和预测可靠性的不足，以及现有的非顺从预测的保守性问题。

Method: 引入可学习的非顺从性分数，通过轻量级神经网络对上下文信息进行建模，生成具有上下文意识的不确定性集合。

Result: LCP在分类中将预测集减少了18%，检测间隔收紧了52%，路径规划成功率从72%提高到91%。在CIFAR-100和ImageNet分类中，集合大小减少4.7-9.9%；在COCO、BDD100K和Cityscapes目标检测中，生成更紧凑的边界框，收缩46-54%。

Conclusion: LCP在分类、目标检测和路径规划任务中相较于标准CP和集成基线表现更佳，且在资源受限的自主系统中应用可靠。

Abstract: Deep learning models in robotics often output point estimates with poorly
calibrated confidences, offering no native mechanism to quantify predictive
reliability under novel, noisy, or out-of-distribution inputs. Conformal
prediction (CP) addresses this gap by providing distribution-free coverage
guarantees, yet its reliance on fixed nonconformity scores ignores context and
can yield intervals that are overly conservative or unsafe. We address this
with Learnable Conformal Prediction (LCP), which replaces fixed scores with a
lightweight neural function that leverages geometric, semantic, and
task-specific features to produce context-aware uncertainty sets.
  LCP maintains CP's theoretical guarantees while reducing prediction set sizes
by 18% in classification, tightening detection intervals by 52%, and improving
path planning safety from 72% to 91% success with minimal overhead. Across
three robotic tasks on seven benchmarks, LCP consistently outperforms Standard
CP and ensemble baselines. In classification on CIFAR-100 and ImageNet, it
achieves smaller set sizes (4.7-9.9% reduction) at target coverage. For object
detection on COCO, BDD100K, and Cityscapes, it produces 46-54% tighter bounding
boxes. In path planning through cluttered environments, it improves success to
91.5% with only 4.5% path inflation, compared to 12.2% for Standard CP.
  The method is lightweight (approximately 4.8% runtime overhead, 42 KB memory)
and supports online adaptation, making it well suited to resource-constrained
autonomous systems. Hardware evaluation shows LCP adds less than 1% memory and
15.9% inference overhead, yet sustains 39 FPS on detection tasks while being
7.4 times more energy-efficient than ensembles.

</details>


### [37] [Leveraging Large Language Models for Robot-Assisted Learning of Morphological Structures in Preschool Children with Language Vulnerabilities](https://arxiv.org/abs/2509.22287)
*Stina Sundstedt,Mattias Wingren,Susanne Hägglund,Daniel Ventus*

Main category: cs.RO

TL;DR: 本研究开发了一种基于大型语言模型的机器人语言学习应用，旨在帮助有语言困难的儿童提高语言表达能力，最终目标是创建多语言的语言学习工具。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在为具有语言脆弱性的学前儿童提供有效的表达语言技能支持，减轻教师和家长在促进语言学习时的负担。

Method: 通过TalBot项目，使用Furhat对话机器人和大型语言模型，开发了一个语言游戏应用，重点在于词汇检索和对话管理。

Result: 研究结果表明，该机器人辅助系统不仅能够管理游戏动态和对话，还可能在教授语言形态结构方面超越人类的能力。

Conclusion: 该研究提出了一种基于大型语言模型的机器人辅助语言学习的方法，旨在帮助语言脆弱儿童通过游戏提高表达能力，最终目标是创建一种能够教授多种语言的形态结构的干预工具。

Abstract: Preschool children with language vulnerabilities -- such as developmental
language disorders or immigration related language challenges -- often require
support to strengthen their expressive language skills. Based on the principle
of implicit learning, speech-language therapists (SLTs) typically embed target
morphological structures (e.g., third person -s) into everyday interactions or
game-based learning activities. Educators are recommended by SLTs to do the
same. This approach demands precise linguistic knowledge and real-time
production of various morphological forms (e.g., "Daddy wears these when he
drives to work"). The task becomes even more demanding when educators or parent
also must keep children engaged and manage turn-taking in a game-based
activity. In the TalBot project our multiprofessional team have developed an
application in which the Furhat conversational robot plays the word retrieval
game "Alias" with children to improve language skills. Our application
currently employs a large language model (LLM) to manage gameplay, dialogue,
affective responses, and turn-taking. Our next step is to further leverage the
capacity of LLMs so the robot can generate and deliver specific morphological
targets during the game. We hypothesize that a robot could outperform humans at
this task. Novel aspects of this approach are that the robot could ultimately
serve as a model and tutor for both children and professionals and that using
LLM capabilities in this context would support basic communication needs for
children with language vulnerabilities. Our long-term goal is to create a
robust LLM-based Robot-Assisted Language Learning intervention capable of
teaching a variety of morphological structures across different languages.

</details>


### [38] [FlowDrive: moderated flow matching with data balancing for trajectory planning](https://arxiv.org/abs/2509.21961)
*Lingguang Wang,Ömer Şahin Taş,Marlon Steiner,Christoph Stiller*

Main category: cs.RO

TL;DR: 本文提出FlowDrive，一种能有效处理驾驶数据长尾分布问题的流匹配轨迹规划器，其在各大基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 为了应对驾驶数据的长尾分布问题，常见操作主导数据集，而危险或稀有场景稀疏，导致模型在关键场景上的表现下降。

Method: 提出了一种名为FlowDrive的流匹配轨迹规划器，学习条件修正流以直接将噪声映射到轨迹分布，并采用调节的环中指导以增加轨迹的多样性。

Result: 通过对比平衡策略和样本训练数据，发现基于轨迹模式的重加权是一种有效的方法，FlowDrive展示了在提高轨迹多样性的同时保持场景一致性。

Conclusion: FlowDrive在nuPlan和interPlan基准测试中取得了学习型规划器的最佳结果，并通过添加调节指导和轻量级后处理（FlowDrive*）在几乎所有基准数据集中实现了整体的最佳性能。

Abstract: Learning-based planners are sensitive to the long-tailed distribution of
driving data. Common maneuvers dominate datasets, while dangerous or rare
scenarios are sparse. This imbalance can bias models toward the frequent cases
and degrade performance on critical scenarios. To tackle this problem, we
compare balancing strategies for sampling training data and find reweighting by
trajectory pattern an effective approach. We then present FlowDrive, a
flow-matching trajectory planner that learns a conditional rectified flow to
map noise directly to trajectory distributions with few flow-matching steps. We
further introduce moderated, in-the-loop guidance that injects small
perturbation between flow steps to systematically increase trajectory diversity
while remaining scene-consistent. On nuPlan and the interaction-focused
interPlan benchmarks, FlowDrive achieves state-of-the-art results among
learning-based planners and approaches methods with rule-based refinements.
After adding moderated guidance and light post-processing (FlowDrive*), it
achieves overall state-of-the-art performance across nearly all benchmark
splits.

</details>


### [39] [Hybrid Diffusion for Simultaneous Symbolic and Continuous Planning](https://arxiv.org/abs/2509.21983)
*Sigmund Hennum Høeg,Aksel Vaaler,Chaoqi Liu,Olav Egeland,Yilun Du*

Main category: cs.RO

TL;DR: 本研究提出一种混合扩散方法，通过同时生成高层符号计划，改善在长期复杂任务中的轨迹生成能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有生成方法在处理复杂决策和长时间任务时的不足，尤其是Diffusion Models在表现上的局限性。

Method: 采用混合的离散变量扩散和连续扩散方法进行轨迹生成，结合高层次符号计划。

Result: 我们的混合扩散过程显著优于基线方法，并且能够灵活地根据部分和完整的符号条件合成轨迹。

Conclusion: 我们提出通过同时生成高层符号计划来增强连续轨迹生成，从而显著提高了在复杂决策的长时间任务中的表现。

Abstract: Constructing robots to accomplish long-horizon tasks is a long-standing
challenge within artificial intelligence. Approaches using generative methods,
particularly Diffusion Models, have gained attention due to their ability to
model continuous robotic trajectories for planning and control. However, we
show that these models struggle with long-horizon tasks that involve complex
decision-making and, in general, are prone to confusing different modes of
behavior, leading to failure. To remedy this, we propose to augment continuous
trajectory generation by simultaneously generating a high-level symbolic plan.
We show that this requires a novel mix of discrete variable diffusion and
continuous diffusion, which dramatically outperforms the baselines. In
addition, we illustrate how this hybrid diffusion process enables flexible
trajectory synthesis, allowing us to condition synthesized actions on partial
and complete symbolic conditions.

</details>


### [40] [Developing Vision-Language-Action Model from Egocentric Videos](https://arxiv.org/abs/2509.21986)
*Tomoya Yoshida,Shuhei Kurita,Taichi Nishimura,Shinsuke Mori*

Main category: cs.RO

TL;DR: 本研究探讨了如何仅利用自我中心视频训练视觉-语言-动作模型，提出EgoScaler框架，显著提升机器人操控能力，并展示了自我中心视频的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统的手动遥控训练方式成本高昂且依赖专家，而自我中心视频提供了一种可伸缩的替代方案。

Method: 使用EgoScaler框架从自我中心视频中提取6DoF物体操控轨迹，无需辅助记录。

Result: 预训练数据集相比从头开始训练，任务成功率提高了20%以上，而且结合真实机器人数据能进一步提升性能。

Conclusion: 以自我中心视频为基础的训练方法可以有效提升机器人操控性能，并且这种方法具有可扩展性。

Abstract: Egocentric videos capture how humans manipulate objects and tools, providing
diverse motion cues for learning object manipulation. Unlike the costly,
expert-driven manual teleoperation commonly used in training
Vision-Language-Action models (VLAs), egocentric videos offer a scalable
alternative. However, prior studies that leverage such videos for training
robot policies typically rely on auxiliary annotations, such as detailed
hand-pose recordings. Consequently, it remains unclear whether VLAs can be
trained directly from raw egocentric videos. In this work, we address this
challenge by leveraging EgoScaler, a framework that extracts 6DoF object
manipulation trajectories from egocentric videos without requiring auxiliary
recordings. We apply EgoScaler to four large-scale egocentric video datasets
and automatically refine noisy or incomplete trajectories, thereby constructing
a new large-scale dataset for VLA pre-training. Our experiments with a
state-of-the-art $\pi_0$ architecture in both simulated and real-robot
environments yield three key findings: (i) pre-training on our dataset improves
task success rates by over 20\% compared to training from scratch, (ii) the
performance is competitive with that achieved using real-robot datasets, and
(iii) combining our dataset with real-robot data yields further improvements.
These results demonstrate that egocentric videos constitute a promising and
scalable resource for advancing VLA research.

</details>


### [41] [One-DoF Robotic Design of Overconstrained Limbs with Energy-Efficient, Self-Collision-Free Motion](https://arxiv.org/abs/2509.22002)
*Yuping Gu,Bangchao Huang,Haoran Sun,Ronghan Xu,Jiayi Yin,Wei Zhang,Fang Wan,Jia Pan,Chaoyang Song*

Main category: cs.RO

TL;DR: 本研究提出了一种设计单自由度过约束机器人肢体的计算方法，实现了高效、无自碰撞的运动，同时在实验中展示了优异的能效。


<details>
  <summary>Details</summary>
Motivation: 尽管期望设计出多自由度的机器人肢体，但单自由度设计在简化性、鲁棒性、成本效益和效率等方面仍然至关重要。

Method: 通过几何优化问题和空间轨迹生成问题的公式化，结合优化链接的几何形状，确保运动的平滑和无碰撞。

Result: 研究验证了所提出的方法，通过各种实验，发现具备过约束的机器人肢体的六足机器人在前进行走中表现出卓越的能效。

Conclusion: 该研究实现了一种新颖的计算方法，设计出能在全周期旋转中实现高效、无自碰撞运动的单自由度机器人肢体。

Abstract: While it is expected to build robotic limbs with multiple degrees of freedom
(DoF) inspired by nature, a single DoF design remains fundamental, providing
benefits that include, but are not limited to, simplicity, robustness,
cost-effectiveness, and efficiency. Mechanisms, especially those with multiple
links and revolute joints connected in closed loops, play an enabling factor in
introducing motion diversity for 1-DoF systems, which are usually constrained
by self-collision during a full-cycle range of motion. This study presents a
novel computational approach to designing one-degree-of-freedom (1-DoF)
overconstrained robotic limbs for a desired spatial trajectory, while achieving
energy-efficient, self-collision-free motion in full-cycle rotations. Firstly,
we present the geometric optimization problem of linkage-based robotic limbs in
a generalized formulation for self-collision-free design. Next, we formulate
the spatial trajectory generation problem with the overconstrained linkages by
optimizing the similarity and dynamic-related metrics. We further optimize the
geometric shape of the overconstrained linkage to ensure smooth and
collision-free motion driven by a single actuator. We validated our proposed
method through various experiments, including personalized automata and
bio-inspired hexapod robots. The resulting hexapod robot, featuring
overconstrained robotic limbs, demonstrated outstanding energy efficiency
during forward walking.

</details>


### [42] [An Adaptive ICP LiDAR Odometry Based on Reliable Initial Pose](https://arxiv.org/abs/2509.22058)
*Qifeng Wang,Weigang Li,Lei Nie,Xin Xu,Wenping Liu,Zhe Xu*

Main category: cs.RO

TL;DR: 本文提出了一种自适应ICP的LiDAR里程计方法，解决了初始姿态可靠性和动态环境适应性的问题，显著提高了注册准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于ICP的方法未充分考虑初始姿态的可靠性，并缺乏适应复杂动态环境的机制，导致注册精度显著下降。

Method: 提出了一种基于自适应ICP的LiDAR里程计方法，通过分布式粗略注册获取可靠的初始姿态，并动态调整自适应阈值。

Result: 通过比较运动预测姿态选择可靠的初始姿态，并结合当前与历史错误，动态调整自适应阈值，最终实现高精度的点云配准。

Conclusion: 该方法在KITTI数据集上的实验证明了其优越性，显著提高了LiDAR里程计的准确性。

Abstract: As a key technology for autonomous navigation and positioning in mobile
robots, light detection and ranging (LiDAR) odometry is widely used in
autonomous driving applications. The Iterative Closest Point (ICP)-based
methods have become the core technique in LiDAR odometry due to their efficient
and accurate point cloud registration capability. However, some existing
ICP-based methods do not consider the reliability of the initial pose, which
may cause the method to converge to a local optimum. Furthermore, the absence
of an adaptive mechanism hinders the effective handling of complex dynamic
environments, resulting in a significant degradation of registration accuracy.
To address these issues, this paper proposes an adaptive ICP-based LiDAR
odometry method that relies on a reliable initial pose. First, distributed
coarse registration based on density filtering is employed to obtain the
initial pose estimation. The reliable initial pose is then selected by
comparing it with the motion prediction pose, reducing the initial error
between the source and target point clouds. Subsequently, by combining the
current and historical errors, the adaptive threshold is dynamically adjusted
to accommodate the real-time changes in the dynamic environment. Finally, based
on the reliable initial pose and the adaptive threshold, point-to-plane
adaptive ICP registration is performed from the current frame to the local map,
achieving high-precision alignment of the source and target point clouds.
Extensive experiments on the public KITTI dataset demonstrate that the proposed
method outperforms existing approaches and significantly enhances the accuracy
of LiDAR odometry.

</details>


### [43] [Effect of Gait Design on Proprioceptive Sensing of Terrain Properties in a Quadrupedal Robot](https://arxiv.org/abs/2509.22065)
*Ethan Fulcher,J. Diego Caporale,Yifeng Zhang,John Ruck,Feifei Qian*

Main category: cs.RO

TL;DR: 研究探讨了机器人步态对地形感知精度的影响，发现特定步态可提高对地面特性的测量，有助于外星地质勘探。


<details>
  <summary>Details</summary>
Motivation: 提高对机器人在松散、可变基底环境中的行走性能的理解，以便在外星面上进行更有效的地质勘探。

Method: 通过实验对比两种步态（爬行N'感知和跑步行走）在不同基底上进行的测量，评估其对可变沉积物强度和纹理的感知准确性。

Result: 在不同的基底（刚性表面、松散沙子及其表面有合成壳层）上进行实验，发现爬行步态比跑步步态在测量可变沉积物特性上更为精确。

Conclusion: 使用感知意识的爬行步态和运行动的跑步步态，机器人能有效测量不同基底的强度差异；爬行步态在检测表面破裂方面表现更佳，提供了改善机器人步态设计的见解。

Abstract: In-situ robotic exploration is an important tool for advancing knowledge of
geological processes that describe the Earth and other Planetary bodies. To
inform and enhance operations for these roving laboratories, it is imperative
to understand the terramechanical properties of their environments, especially
for traversing on loose, deformable substrates. Recent research suggested that
legged robots with direct-drive and low-gear ratio actuators can sensitively
detect external forces, and therefore possess the potential to measure terrain
properties with their legs during locomotion, providing unprecedented sampling
speed and density while accessing terrains previously too risky to sample. This
paper explores these ideas by investigating the impact of gait on
proprioceptive terrain sensing accuracy, particularly comparing a
sensing-oriented gait, Crawl N' Sense, with a locomotion-oriented gait,
Trot-Walk. Each gait's ability to measure the strength and texture of
deformable substrate is quantified as the robot locomotes over a laboratory
transect consisting of a rigid surface, loose sand, and loose sand with
synthetic surface crusts. Our results suggest that with both the
sensing-oriented crawling gait and locomotion-oriented trot gait, the robot can
measure a consistent difference in the strength (in terms of penetration
resistance) between the low- and high-resistance substrates; however, the
locomotion-oriented trot gait contains larger magnitude and variance in
measurements. Furthermore, the slower crawl gait can detect brittle ruptures of
the surface crusts with significantly higher accuracy than the faster trot
gait. Our results offer new insights that inform legged robot "sensing during
locomotion" gait design and planning for scouting the terrain and producing
scientific measurements on other worlds to advance our understanding of their
geology and formation.

</details>


### [44] [Action-aware Dynamic Pruning for Efficient Vision-Language-Action Manipulation](https://arxiv.org/abs/2509.22093)
*Xiaohuan Pei,Yuxing Chen,Siyu Xu,Yunke Wang,Yuheng Shi,Chang Xu*

Main category: cs.RO

TL;DR: 提出了ADP框架，通过动态剪枝和动作感知机制，提高了视觉-语言-动作模型的推理效率。


<details>
  <summary>Details</summary>
Motivation: 观察到在粗略操控阶段，视觉标记的冗余性高于细粒度操作，并且与动作动态强相关，因而提出了基于这一观察的剪枝方法。

Method: 提出了一种动作感知动态剪枝（ADP）框架，结合了文本驱动的标记选择和动作感知的轨迹门控机制。

Result: ADP在LIBERO套件和多种真实场景上的实验表明，方法显著减少了计算量和延迟，同时也提升了成功率。

Conclusion: ADP方法显著降低了FLOPs和动作推理延迟，同时保持竞争力的成功率，为高效的机器人策略提供了简单的插入路径。

Abstract: Robotic manipulation with Vision-Language-Action models requires efficient
inference over long-horizon multi-modal context, where attention to dense
visual tokens dominates computational cost. Existing methods optimize inference
speed by reducing visual redundancy within VLA models, but they overlook the
varying redundancy across robotic manipulation stages. We observe that the
visual token redundancy is higher in coarse manipulation phase than in
fine-grained operations, and is strongly correlated with the action dynamic.
Motivated by this observation, we propose \textbf{A}ction-aware
\textbf{D}ynamic \textbf{P}runing (\textbf{ADP}), a multi-modal pruning
framework that integrates text-driven token selection with action-aware
trajectory gating. Our method introduces a gating mechanism that conditions the
pruning signal on recent action trajectories, using past motion windows to
adaptively adjust token retention ratios in accordance with dynamics, thereby
balancing computational efficiency and perceptual precision across different
manipulation stages. Extensive experiments on the LIBERO suites and diverse
real-world scenarios demonstrate that our method significantly reduces FLOPs
and action inference latency (\textit{e.g.} $1.35 \times$ speed up on
OpenVLA-OFT) while maintaining competitive success rates (\textit{e.g.} 25.8\%
improvements with OpenVLA) compared to baselines, thereby providing a simple
plug-in path to efficient robot policies that advances the efficiency and
performance frontier of robotic manipulation. Our project website is:
\href{https://vla-adp.github.io/}{ADP.com}.

</details>


### [45] [Multi-stage robust nonlinear model predictive control of a lower-limb exoskeleton robot](https://arxiv.org/abs/2509.22120)
*Alireza Aliyari,Gholamreza Vossoughi*

Main category: cs.RO

TL;DR: 本文提出的多阶段非线性模型预测控制方法显著提高了外骨骼机器人的鲁棒性，减少了人机交互力，效果优于传统控制方法。


<details>
  <summary>Details</summary>
Motivation: 随着肌肉骨骼损伤案例的增加，外骨骼机器人的使用越来越普遍，但其有效性受到控制系统设计的影响。

Method: 提出了一种多阶段非线性模型预测控制（RNMPC）方法，通过解决非线性优化问题来控制一个双自由度的外骨骼。

Result: 模拟和实验表明，与非鲁棒的NMPC相比，所提方法在跟踪误差和人机交互力方面均有显著改善，尤其在处理未知负载时。

Conclusion: 该研究提出的多阶段非线性模型预测控制方法显著提高了外骨骼机器人的控制鲁棒性，尤其在不确定负载情况下表现优异。

Abstract: The use of exoskeleton robots is increasing due to the rising number of
musculoskeletal injuries. However, their effectiveness depends heavily on the
design of control systems. Designing robust controllers is challenging because
of uncertainties in human-robot systems. Among various control strategies,
Model Predictive Control (MPC) is a powerful approach due to its ability to
handle constraints and optimize performance. Previous studies have used
linearization-based methods to implement robust MPC on exoskeletons, but these
can degrade performance due to nonlinearities in the robot's dynamics. To
address this gap, this paper proposes a Robust Nonlinear Model Predictive
Control (RNMPC) method, called multi-stage NMPC, to control a
two-degree-of-freedom exoskeleton by solving a nonlinear optimization problem.
This method uses multiple scenarios to represent system uncertainties. The
study focuses on minimizing human-robot interaction forces during the swing
phase, particularly when the robot carries unknown loads. Simulations and
experimental tests show that the proposed method significantly improves
robustness, outperforming non-robust NMPC. It achieves lower tracking errors
and interaction forces under various uncertainties. For instance, when a 2 kg
unknown payload is combined with external disturbances, the RMS values of thigh
and shank interaction forces for multi-stage NMPC are reduced by 77 and 94
percent, respectively, compared to non-robust NMPC.

</details>


### [46] [DemoGrasp: Universal Dexterous Grasping from a Single Demonstration](https://arxiv.org/abs/2509.22149)
*Haoqi Yuan,Ziye Huang,Ye Wang,Chuan Mao,Chaoyi Xu,Zongqing Lu*

Main category: cs.RO

TL;DR: DemoGrasp是一种通过编辑成功抓取轨迹来学习通用抓取的有效方法，展现出优异的性能和强大的适应能力。


<details>
  <summary>Details</summary>
Motivation: 解决高维度长期探索中的复杂奖励和课程设计问题，以提高通用抓取的有效性与成功率。

Method: 通过编辑机器人在特定对象上的成功抓取轨迹，使用单步马尔可夫决策过程（MDP）进行优化，并结合简单的奖励设计进行强化学习。

Result: 在模拟中，DemoGrasp在DexGraspNet对象上实现了95%的成功率，并在六个未见对象数据集上平均成功率达84.6%。

Conclusion: DemoGrasp方法在多样化对象上展现了强大的抓取能力，成功率达95%，并且具备优良的迁移性和适应性。

Abstract: Universal grasping with multi-fingered dexterous hands is a fundamental
challenge in robotic manipulation. While recent approaches successfully learn
closed-loop grasping policies using reinforcement learning (RL), the inherent
difficulty of high-dimensional, long-horizon exploration necessitates complex
reward and curriculum design, often resulting in suboptimal solutions across
diverse objects. We propose DemoGrasp, a simple yet effective method for
learning universal dexterous grasping. We start from a single successful
demonstration trajectory of grasping a specific object and adapt to novel
objects and poses by editing the robot actions in this trajectory: changing the
wrist pose determines where to grasp, and changing the hand joint angles
determines how to grasp. We formulate this trajectory editing as a single-step
Markov Decision Process (MDP) and use RL to optimize a universal policy across
hundreds of objects in parallel in simulation, with a simple reward consisting
of a binary success term and a robot-table collision penalty. In simulation,
DemoGrasp achieves a 95% success rate on DexGraspNet objects using the Shadow
Hand, outperforming previous state-of-the-art methods. It also shows strong
transferability, achieving an average success rate of 84.6% across diverse
dexterous hand embodiments on six unseen object datasets, while being trained
on only 175 objects. Through vision-based imitation learning, our policy
successfully grasps 110 unseen real-world objects, including small, thin items.
It generalizes to spatial, background, and lighting changes, supports both RGB
and depth inputs, and extends to language-guided grasping in cluttered scenes.

</details>


### [47] [DHAGrasp: Synthesizing Affordance-Aware Dual-Hand Grasps with Text Instructions](https://arxiv.org/abs/2509.22175)
*Quanzhou Li,Zhonghua Wu,Jingbo Wang,Chen Change Loy,Bo Dai*

Main category: cs.RO

TL;DR: 本研究提出了一种新的双手抓取生成方法，克服了数据集稀缺问题，展示了在未见物体上的优越性能。


<details>
  <summary>Details</summary>
Motivation: 生成符合物体语义的双手抓取对于稳健的手-物体交互至关重要，但现有的数据集稀缺。

Method: 利用单手抓取数据集和物体与手的对称性，构建一个大规模双手抓取数据集，并提出了一个文本指导的双手抓取生成器。

Result: 通过大量实验验证，发现该方法能够生成多样且符合语义的一致抓取，且在抓取质量和泛化能力上优于强基准。

Conclusion: 提出的DHAGrasp方法能够有效生成多手抓取，超越了现有方法在抓取质量和对未见物体的泛化能力上的表现。

Abstract: Learning to generate dual-hand grasps that respect object semantics is
essential for robust hand-object interaction but remains largely underexplored
due to dataset scarcity. Existing grasp datasets predominantly focus on
single-hand interactions and contain only limited semantic part annotations. To
address these challenges, we introduce a pipeline, SymOpt, that constructs a
large-scale dual-hand grasp dataset by leveraging existing single-hand datasets
and exploiting object and hand symmetries. Building on this, we propose a
text-guided dual-hand grasp generator, DHAGrasp, that synthesizes Dual-Hand
Affordance-aware Grasps for unseen objects. Our approach incorporates a novel
dual-hand affordance representation and follows a two-stage design, which
enables effective learning from a small set of segmented training objects while
scaling to a much larger pool of unsegmented data. Extensive experiments
demonstrate that our method produces diverse and semantically consistent
grasps, outperforming strong baselines in both grasp quality and generalization
to unseen objects. The project page is at
https://quanzhou-li.github.io/DHAGrasp/.

</details>


### [48] [Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting](https://arxiv.org/abs/2509.22195)
*Asher J. Hancock,Xindi Wu,Lihan Zha,Olga Russakovsky,Anirudha Majumdar*

Main category: cs.RO

TL;DR: 提出VLM2VLA训练范式，通过低秩适应解决视觉语言模型灾难性遗忘的问题，保持模型能力，实现对新任务的泛化。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在机器人远程操作数据上微调后，容易出现灾难性遗忘，影响模型的推理能力和多模态理解。

Method: 通过低秩适应（LoRA）训练视觉语言-行动（VLA）模型，首先通过自然语言表示低级动作以解决数据分布不匹配的问题。

Result: 通过视觉问答（VQA）研究和800多项真实世界机器人实验，证明VLM2VLA能够在不进行昂贵共同训练的情况下，对机器人远程操作数据进行微调。

Conclusion: VLM2VLA训练范式能够在不改变基础架构的情况下保持视觉语言模型（VLM）的核心能力，实现对新任务的零-shot泛化。

Abstract: Fine-tuning vision-language models (VLMs) on robot teleoperation data to
create vision-language-action (VLA) models is a promising paradigm for training
generalist policies, but it suffers from a fundamental tradeoff: learning to
produce actions often diminishes the VLM's foundational reasoning and
multimodal understanding, hindering generalization to novel scenarios,
instruction following, and semantic understanding. We argue that this
catastrophic forgetting is due to a distribution mismatch between the VLM's
internet-scale pretraining corpus and the robotics fine-tuning data. Inspired
by this observation, we introduce VLM2VLA: a VLA training paradigm that first
resolves this mismatch at the data level by representing low-level actions with
natural language. This alignment makes it possible to train VLAs solely with
Low-Rank Adaptation (LoRA), thereby minimally modifying the VLM backbone and
averting catastrophic forgetting. As a result, the VLM can be fine-tuned on
robot teleoperation data without fundamentally altering the underlying
architecture and without expensive co-training on internet-scale VLM datasets.
Through extensive Visual Question Answering (VQA) studies and over 800
real-world robotics experiments, we demonstrate that VLM2VLA preserves the
VLM's core capabilities, enabling zero-shot generalization to novel tasks that
require open-world semantic reasoning and multilingual instruction following.

</details>


### [49] [MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training](https://arxiv.org/abs/2509.22199)
*Haoyun Li,Ivan Zhang,Runqi Ouyang,Xiaofeng Wang,Zheng Zhu,Zhiqin Yang,Zhentao Zhang,Boyuan Wang,Chaojun Ni,Wenkang Qin,Xinze Chen,Yun Ye,Guan Huang,Zhenbo Song,Xingang Wang*

Main category: cs.RO

TL;DR: MimicDreamer框架通过将人类演示视频转化为机器人可用的指导，解决了机器人在执行任务时的领域差距，提高了机器人模型的学习效率与表现。


<details>
  <summary>Details</summary>
Motivation: 人类演示视频更易获取且成本低廉，然而存在与机器人执行视频间的领域差距。

Method: 提出MimicDreamer框架，包括H2R Aligner视频扩散模型、EgoStabilizer视角稳定化手段及受限逆运动学求解器进行动作对齐。

Result: 使用合成的人类到机器人视频训练的VLA模型在实际机器人上的少量样本执行表现良好，且训练中使用人类数据显著提高了性能，平均成功率提升14.7%。

Conclusion: MimicDreamer框架通过将人类演示视频转化为机器人可用的监督，实现了视觉、视角和动作的对齐，使得VLA模型在真实机器人上实现少量样本执行并显著提高成功率。

Abstract: Vision Language Action (VLA) models derive their generalization capability
from diverse training data, yet collecting embodied robot interaction data
remains prohibitively expensive. In contrast, human demonstration videos are
far more scalable and cost-efficient to collect, and recent studies confirm
their effectiveness in training VLA models. However, a significant domain gap
persists between human videos and robot-executed videos, including unstable
camera viewpoints, visual discrepancies between human hands and robotic arms,
and differences in motion dynamics. To bridge this gap, we propose
MimicDreamer, a framework that turns fast, low-cost human demonstrations into
robot-usable supervision by jointly aligning vision, viewpoint, and actions to
directly support policy training. For visual alignment, we propose H2R Aligner,
a video diffusion model that generates high-fidelity robot demonstration videos
by transferring motion from human manipulation footage. For viewpoint
stabilization, EgoStabilizer is proposed, which canonicalizes egocentric videos
via homography and inpaints occlusions and distortions caused by warping. For
action alignment, we map human hand trajectories to the robot frame and apply a
constrained inverse kinematics solver to produce feasible, low-jitter joint
commands with accurate pose tracking. Empirically, VLA models trained purely on
our synthesized human-to-robot videos achieve few-shot execution on real
robots. Moreover, scaling training with human data significantly boosts
performance compared to models trained solely on real robot data; our approach
improves the average success rate by 14.7\% across six representative
manipulation tasks.

</details>


### [50] [From Watch to Imagine: Steering Long-horizon Manipulation via Human Demonstration and Future Envisionment](https://arxiv.org/abs/2509.22205)
*Ke Ye,Jiaming Zhou,Yuanfeng Qiu,Jiayi Liu,Shihui Zhou,Kun-Yu Lin,Junwei Liang*

Main category: cs.RO

TL;DR: 提出一种新的层次框架Super-Mimic，通过解析人类演示视频来实现零-shot机器人模仿，显著提升了长时间操作任务的执行效果。


<details>
  <summary>Details</summary>
Motivation: 面对自动化长时间操作任务的挑战，我们需要一种新的方法来从非脚本化的人类演示视频中推断程序性的意图。

Method: Super-Mimic框架由两个顺序模块组成：人类意图翻译器（HIT）和未来动态预测器（FDP）。

Result: Super-Mimic在一系列长时间操作任务上显著优于最先进的零-shot方法，提升超过20%。

Conclusion: 通过将视频驱动的意图解析与前瞻性动态建模结合起来，可以有效地开发通用机器人系统。

Abstract: Generalizing to long-horizon manipulation tasks in a zero-shot setting
remains a central challenge in robotics. Current multimodal foundation based
approaches, despite their capabilities, typically fail to decompose high-level
commands into executable action sequences from static visual input alone. To
address this challenge, we introduce Super-Mimic, a hierarchical framework that
enables zero-shot robotic imitation by directly inferring procedural intent
from unscripted human demonstration videos. Our framework is composed of two
sequential modules. First, a Human Intent Translator (HIT) parses the input
video using multimodal reasoning to produce a sequence of language-grounded
subtasks. These subtasks then condition a Future Dynamics Predictor (FDP),
which employs a generative model that synthesizes a physically plausible video
rollout for each step. The resulting visual trajectories are dynamics-aware,
explicitly modeling crucial object interactions and contact points to guide the
low-level controller. We validate this approach through extensive experiments
on a suite of long-horizon manipulation tasks, where Super-Mimic significantly
outperforms state-of-the-art zero-shot methods by over 20\%. These results
establish that coupling video-driven intent parsing with prospective dynamics
modeling is a highly effective strategy for developing general-purpose robotic
systems.

</details>


### [51] [IMU-Preintegrated Radar Factors for Asynchronous Radar-LiDAR-Inertial SLAM](https://arxiv.org/abs/2509.22288)
*Johan Hatleskog,Morten Nissov,Kostas Alexis*

Main category: cs.RO

TL;DR: 本文提出IMU预积分雷达因子以减少节点创建率和优化时间，同时保持估计精度。


<details>
  <summary>Details</summary>
Motivation: 传统的固定延迟雷达-LiDAR-惯性平滑器因时钟不同步而每个测量创建一个因子图节点，导致高优化成本。

Method: 引入IMU预积分雷达因子，利用高频惯性数据传播LiDAR状态到雷达测量时间戳。

Result: 相较于常规方法，节点数量减少50%，优化时间降低了56%。

Conclusion: 我们的方法在降低计算成本的同时，保持了绝对姿态误差，实现了高效的优化。

Abstract: Fixed-lag Radar-LiDAR-Inertial smoothers conventionally create one factor
graph node per measurement to compensate for the lack of time synchronization
between radar and LiDAR. For a radar-LiDAR sensor pair with equal rates, this
strategy results in a state creation rate of twice the individual sensor
frequencies. This doubling of the number of states per second yields high
optimization costs, inhibiting real-time performance on resource-constrained
hardware. We introduce IMU-preintegrated radar factors that use high-rate
inertial data to propagate the most recent LiDAR state to the radar measurement
timestamp. This strategy maintains the node creation rate at the LiDAR
measurement frequency. Assuming equal sensor rates, this lowers the number of
nodes by 50 % and consequently the computational costs. Experiments on a single
board computer (which has 4 cores each of 2.2 GHz A73 and 2 GHz A53 with 8 GB
RAM) show that our method preserves the absolute pose error of a conventional
baseline while simultaneously lowering the aggregated factor graph optimization
time by up to 56 %.

</details>


### [52] [Beyond Detection -- Orchestrating Human-Robot-Robot Assistance via an Internet of Robotic Things Paradigm](https://arxiv.org/abs/2509.22296)
*Joseph Hunt,Koyo Fujii,Aly Magassouba,Praminda Caleb-Solly*

Main category: cs.RO

TL;DR: 该论文提出了一种新型的智能机器人系统，能够通过实时预测和主动响应，减少医院患者跌倒风险。


<details>
  <summary>Details</summary>
Motivation: 医院患者跌倒问题严重且成本高昂，传统的跌倒预防系统缺乏针对患者需求的主动响应，导致高假阳性率。

Method: 提出了一种基于物联网的机器人架构，通过隐私保护的热感应模型实现实时的床外预测，并使用两个协调的机器人响应患者需求。

Result: 初步研究表明，该系统能够提高预防跌倒的有效性，并为患者提供基于需求的个性化协助，降低潜在的危险情况。

Conclusion: 本研究提出的系统能够通过互动和连接的机器人技术，提供及时、有意义的帮助，从而提升医院的安全和响应能力。

Abstract: Hospital patient falls remain a critical and costly challenge worldwide.
While conventional fall prevention systems typically rely on post-fall
detection or reactive alerts, they also often suffer from high false positive
rates and fail to address the underlying patient needs that lead to bed-exit
attempts. This paper presents a novel system architecture that leverages the
Internet of Robotic Things (IoRT) to orchestrate human-robot-robot interaction
for proactive and personalized patient assistance. The system integrates a
privacy-preserving thermal sensing model capable of real-time bed-exit
prediction, with two coordinated robotic agents that respond dynamically based
on predicted intent and patient input. This orchestrated response could not
only reduce fall risk but also attend to the patient's underlying motivations
for movement, such as thirst, discomfort, or the need for assistance, before a
hazardous situation arises. Our contributions with this pilot study are
three-fold: (1) a modular IoRT-based framework enabling distributed sensing,
prediction, and multi-robot coordination; (2) a demonstration of low-resolution
thermal sensing for accurate, privacy-preserving preemptive bed-exit detection;
and (3) results from a user study and systematic error analysis that inform the
design of situationally aware, multi-agent interactions in hospital settings.
The findings highlight how interactive and connected robotic systems can move
beyond passive monitoring to deliver timely, meaningful assistance, empowering
safer, more responsive care environments.

</details>


### [53] [RoboView-Bias: Benchmarking Visual Bias in Embodied Agents for Robotic Manipulation](https://arxiv.org/abs/2509.22356)
*Enguang Liu,Siyuan Liang,Liming Lu,Xiyu Zeng,Xiaochun Cao,Aishan Liu,Shuchao Pang*

Main category: cs.RO

TL;DR: 提出RoboView-Bias基准，通过系统分析视觉偏差，发现视觉偏差对代理决策影响显著，同时提出有效的缓解策略。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要强调在扰动下的泛化和鲁棒性，缺乏对视觉偏差的系统量化，限制了对知觉如何影响决策稳定性的更深入理解。

Method: 提出RoboView-Bias基准，通过因素隔离原则系统量化机器人操作中的视觉偏差，并结合结构化变体生成框架和感知公平性验证协议创建2,127个任务实例。

Result: 系统评估三种代表性体现代理，发现所有代理均表现出显著的视觉偏差；相机视角是最关键因素；代理在高度饱和颜色上成功率最高；视觉偏差呈现出强而不对称的耦合。

Conclusion: 系统分析视觉偏差是开发安全可靠的通用体现代理的先决条件。

Abstract: The safety and reliability of embodied agents rely on accurate and unbiased
visual perception. However, existing benchmarks mainly emphasize generalization
and robustness under perturbations, while systematic quantification of visual
bias remains scarce. This gap limits a deeper understanding of how perception
influences decision-making stability. To address this issue, we propose
RoboView-Bias, the first benchmark specifically designed to systematically
quantify visual bias in robotic manipulation, following a principle of factor
isolation. Leveraging a structured variant-generation framework and a
perceptual-fairness validation protocol, we create 2,127 task instances that
enable robust measurement of biases induced by individual visual factors and
their interactions. Using this benchmark, we systematically evaluate three
representative embodied agents across two prevailing paradigms and report three
key findings: (i) all agents exhibit significant visual biases, with camera
viewpoint being the most critical factor; (ii) agents achieve their highest
success rates on highly saturated colors, indicating inherited visual
preferences from underlying VLMs; and (iii) visual biases show strong,
asymmetric coupling, with viewpoint strongly amplifying color-related bias.
Finally, we demonstrate that a mitigation strategy based on a semantic
grounding layer substantially reduces visual bias by approximately 54.5\% on
MOKA. Our results highlight that systematic analysis of visual bias is a
prerequisite for developing safe and reliable general-purpose embodied agents.

</details>


### [54] [Learning-Based Collaborative Control for Bi-Manual Tactile-Reactive Grasping](https://arxiv.org/abs/2509.22421)
*Leonel Giacobbe,Jingdao Chen,Chuangchuang Sun*

Main category: cs.RO

TL;DR: 本文提出了一种新型多智能体触觉反应控制方法，能够处理不同软硬物体，实现更好的抓取性能。


<details>
  <summary>Details</summary>
Motivation: 目前的抓取技术主要针对刚性物体，无法有效处理脆弱或可变形材料，且单智能体方法限制了大重物的操作能力。

Method: 采用多智能体模型预测控制器 (MPC) ，结合触觉传感器实时感知物体特性。

Result: 在多种尺寸和硬度的物体上实现了较高的抓取成功率，超越了传统的PD和MPC基线。

Conclusion: 提出了一种基于学习的多智能体触觉反应模型预测控制器，能够更好地处理各种软硬物体，并在复杂环境中实现稳定的协作抓取。

Abstract: Grasping is a core task in robotics with various applications. However, most
current implementations are primarily designed for rigid items, and their
performance drops considerably when handling fragile or deformable materials
that require real-time feedback. Meanwhile, tactile-reactive grasping focuses
on a single agent, which limits their ability to grasp and manipulate large,
heavy objects. To overcome this, we propose a learning-based, tactile-reactive
multi-agent Model Predictive Controller (MPC) for grasping a wide range of
objects with different softness and shapes, beyond the capabilities of
preexisting single-agent implementations. Our system uses two Gelsight Mini
tactile sensors [1] to extract real-time information on object texture and
stiffness. This rich tactile feedback is used to estimate contact dynamics and
object compliance in real time, enabling the system to adapt its control policy
to diverse object geometries and stiffness profiles. The learned controller
operates in a closed loop, leveraging tactile encoding to predict grasp
stability and adjust force and position accordingly. Our key technical
contributions include a multi-agent MPC formulation trained on real contact
interactions, a tactile-data driven method for inferring grasping states, and a
coordination strategy that enables collaborative control. By combining tactile
sensing and a learning-based multi-agent MPC, our method offers a robust,
intelligent solution for collaborative grasping in complex environments,
significantly advancing the capabilities of multi-agent systems. Our approach
is validated through extensive experiments against independent PD and MPC
baselines. Our pipeline outperforms the baselines regarding success rates in
achieving and maintaining stable grasps across objects of varying sizes and
stiffness.

</details>


### [55] [An Ontology for Unified Modeling of Tasks, Actions, Environments, and Capabilities in Personal Service Robotics](https://arxiv.org/abs/2509.22434)
*Margherita Martorana,Francesca Urgese,Ilaria Tiddi,Stefan Schlobach*

Main category: cs.RO

TL;DR: 本研究提出OntoBOT本体，旨在统一表示服务机器人中的任务、行动和环境，增强上下文感知和知识共享能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有本体在特定领域聚焦的问题，提供一个可以更广泛适用的框架，以支持动态环境下机器人的有效操作和知识共享。

Method: 通过将现有本体进行扩展，提出一个统一的本体OntoBOT，并针对四个具体机器人进行了能力问题的评估。

Result: OntoBOT实现了任务、行动、环境与机器人能力之间的统一表示，并通过四个机器人案例展示了其在服务机器人中的实用性和通用性。

Conclusion: 提出的OntoBOT本体为任务、行动、环境和能力提供了统一的表示，促进了服务机器人领域中的上下文感知推理、任务导向执行和知识共享。

Abstract: Personal service robots are increasingly used in domestic settings to assist
older adults and people requiring support. Effective operation involves not
only physical interaction but also the ability to interpret dynamic
environments, understand tasks, and choose appropriate actions based on
context. This requires integrating both hardware components (e.g. sensors,
actuators) and software systems capable of reasoning about tasks, environments,
and robot capabilities. Frameworks such as the Robot Operating System (ROS)
provide open-source tools that help connect low-level hardware with
higher-level functionalities. However, real-world deployments remain tightly
coupled to specific platforms. As a result, solutions are often isolated and
hard-coded, limiting interoperability, reusability, and knowledge sharing.
Ontologies and knowledge graphs offer a structured way to represent tasks,
environments, and robot capabilities. Existing ontologies, such as the
Socio-physical Model of Activities (SOMA) and the Descriptive Ontology for
Linguistic and Cognitive Engineering (DOLCE), provide models for activities,
spatial relationships, and reasoning structures. However, they often focus on
specific domains and do not fully capture the connection between environment,
action, robot capabilities, and system-level integration. In this work, we
propose the Ontology for roBOts and acTions (OntoBOT), which extends existing
ontologies to provide a unified representation of tasks, actions, environments,
and capabilities. Our contributions are twofold: (1) we unify these aspects
into a cohesive ontology to support formal reasoning about task execution, and
(2) we demonstrate its generalizability by evaluating competency questions
across four embodied agents - TIAGo, HSR, UR3, and Stretch - showing how
OntoBOT enables context-aware reasoning, task-oriented execution, and knowledge
sharing in service robotics.

</details>


### [56] [UnderwaterVLA: Dual-brain Vision-Language-Action architecture for Autonomous Underwater Navigation](https://arxiv.org/abs/2509.22441)
*Zhangyuan Wang,Yunpeng Zhu,Yuqi Yan,Xiaoyuan Tian,Xinhao Shao,Meixuan Li,Weikun Li,Guangsheng Su,Weicheng Cui,Dixia Fan*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的自主水下导航框架UnderwaterVLA，针对水下操作中的多种挑战，通过创新技术显著提高了导航精度和任务效率，具有良好的可扩展性和经济性。


<details>
  <summary>Details</summary>
Motivation: 水下操作面临水流干扰、通信带宽限制及浑浊水域感知能力下降等挑战，亟需一种新的框架来提高自主水下导航的效率和可靠性。

Method: 该论文提出了一种双脑架构，并结合视觉-语言-行动模型及水动力学信息的模型预测控制方案，进行自主水下导航。

Result: 实验结果显示，UnderwaterVLA在恶劣视觉条件下减少了导航误差，同时比基线提高了19%到27%的任务完成率。

Conclusion: UnderwaterVLA框架通过减少导航错误和提高任务完成率，为未来的智能自主水下器具提供了一种可扩展、经济高效的解决方案。

Abstract: This paper presents UnderwaterVLA, a novel framework for autonomous
underwater navigation that integrates multimodal foundation models with
embodied intelligence systems. Underwater operations remain difficult due to
hydrodynamic disturbances, limited communication bandwidth, and degraded
sensing in turbid waters. To address these challenges, we introduce three
innovations. First, a dual-brain architecture decouples high-level mission
reasoning from low-level reactive control, enabling robust operation under
communication and computational constraints. Second, we apply
Vision-Language-Action(VLA) models to underwater robotics for the first time,
incorporating structured chain-of-thought reasoning for interpretable
decision-making. Third, a hydrodynamics-informed Model Predictive Control(MPC)
scheme compensates for fluid effects in real time without costly task-specific
training. Experimental results in field tests show that UnderwaterVLA reduces
navigation errors in degraded visual conditions while maintaining higher task
completion by 19% to 27% over baseline. By minimizing reliance on
underwater-specific training data and improving adaptability across
environments, UnderwaterVLA provides a scalable and cost-effective path toward
the next generation of intelligent AUVs.

</details>


### [57] [Uncertainty-Aware Multi-Robot Task Allocation With Strongly Coupled Inter-Robot Rewards](https://arxiv.org/abs/2509.22469)
*Ben Rossano,Jaein Lim,Jonathan P. How*

Main category: cs.RO

TL;DR: 本文提出了一种新的任务分配算法，优化异构机器人在不确定任务中的表现，通过概率建模任务需求，显著提高了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 在异构机器人团队中，任务要求的不确定性会导致资源浪费和任务失败，因此需要一种有效的分配算法来管理这些不确定性。

Method: 提出了一种市场基础的方法，通过概率分布建模任务需求，优化团队的联合目标。

Result: 与基准算法的对比实验表明，所提算法在分配任务方面更为有效，同时展示了在去中心化设置下，耦合奖励的引入所面临的挑战。

Conclusion: 该算法有效优化了异构机器人团队在不确定任务中的资源分配，提升了任务成功率。

Abstract: This paper proposes a task allocation algorithm for teams of heterogeneous
robots in environments with uncertain task requirements. We model these
requirements as probability distributions over capabilities and use this model
to allocate tasks such that robots with complementary skills naturally position
near uncertain tasks, proactively mitigating task failures without wasting
resources. We introduce a market-based approach that optimizes the joint team
objective while explicitly capturing coupled rewards between robots, offering a
polynomial-time solution in decentralized settings with strict communication
assumptions. Comparative experiments against benchmark algorithms demonstrate
the effectiveness of our approach and highlight the challenges of incorporating
coupled rewards in a decentralized formulation.

</details>


### [58] [Ontological foundations for contrastive explanatory narration of robot plans](https://arxiv.org/abs/2509.22493)
*Alberto Olivares-Alarcos,Sergi Foix,Júlia Borràs,Gerard Canal,Guillem Alenyà*

Main category: cs.RO

TL;DR: 机器人需要合理决策并与人类解释其决策，本文提出新模型和算法以提升这一能力并验证其效果。


<details>
  <summary>Details</summary>
Motivation: 为了确保人机交互的可信性和成功性，机器人需要作出合理的决策并在需要时进行解释。

Method: 提出了一种新的本体模型来形式化和推理竞争计划之间的差异，并通过新算法构建对比叙述。

Result: 通过实证评估，发现本文提出的解释方法在性能上优于基线方法。

Conclusion: 本文提出的模型和算法能够有效提升机器人的决策解释能力，增强人机互动的可信度。

Abstract: Mutual understanding of artificial agents' decisions is key to ensuring a
trustworthy and successful human-robot interaction. Hence, robots are expected
to make reasonable decisions and communicate them to humans when needed. In
this article, the focus is on an approach to modeling and reasoning about the
comparison of two competing plans, so that robots can later explain the
divergent result. First, a novel ontological model is proposed to formalize and
reason about the differences between competing plans, enabling the
classification of the most appropriate one (e.g., the shortest, the safest, the
closest to human preferences, etc.). This work also investigates the
limitations of a baseline algorithm for ontology-based explanatory narration.
To address these limitations, a novel algorithm is presented, leveraging
divergent knowledge between plans and facilitating the construction of
contrastive narratives. Through empirical evaluation, it is observed that the
explanations excel beyond the baseline method.

</details>


### [59] [HELIOS: Hierarchical Exploration for Language-grounded Interaction in Open Scenes](https://arxiv.org/abs/2509.22498)
*Katrina Ashton,Chahyon Ku,Shrey Shah,Wen Jiang,Kostas Daniilidis,Bernadette Bucher*

Main category: cs.RO

TL;DR: HELIOS旨在解决语言指定移动操控任务中的部分观测和知识更新问题，以先进技术在OVMM基准测试中表现优异，并可无缝转移至真实世界。


<details>
  <summary>Details</summary>
Motivation: 解决语言指定的移动操控任务在部分观察场景中的挑战，包括语义信息的定位和场景知识的更新。

Method: 创建2D地图和3D高斯表示，融合多层次观测，利用目标函数在未观察或不确定区域与场景语义信息间进行有效搜索。

Result: HELIOS在Habitat模拟器中的OVMM基准测试中取得了先进的结果，并在真实环境中成功演示。

Conclusion: HELIOS在OVMM基准测试中表现出色，能够在不需要额外数据的情况下转移到真实环境中。

Abstract: Language-specified mobile manipulation tasks in novel environments
simultaneously face challenges interacting with a scene which is only partially
observed, grounding semantic information from language instructions to the
partially observed scene, and actively updating knowledge of the scene with new
observations. To address these challenges, we propose HELIOS, a hierarchical
scene representation and associated search objective to perform language
specified pick and place mobile manipulation tasks. We construct 2D maps
containing the relevant semantic and occupancy information for navigation while
simultaneously actively constructing 3D Gaussian representations of
task-relevant objects. We fuse observations across this multi-layered
representation while explicitly modeling the multi-view consistency of the
detections of each object. In order to efficiently search for the target
object, we formulate an objective function balancing exploration of unobserved
or uncertain regions with exploitation of scene semantic information. We
evaluate HELIOS on the OVMM benchmark in the Habitat simulator, a pick and
place benchmark in which perception is challenging due to large and complex
scenes with comparatively small target objects. HELIOS achieves
state-of-the-art results on OVMM. As our approach is zero-shot, HELIOS can also
transfer to the real world without requiring additional data, as we illustrate
by demonstrating it in a real world office environment on a Spot robot.

</details>


### [60] [An Intention-driven Lane Change Framework Considering Heterogeneous Dynamic Cooperation in Mixed-traffic Environment](https://arxiv.org/abs/2509.22550)
*Xiaoyun Qiu,Haichao Liu,Yue Pan,Jun Ma,Xinhu Zheng*

Main category: cs.RO

TL;DR: 在混合交通环境中，提出了一种意图驱动的换道框架，通过深度学习和模型预测控制，显著提升了自主车辆在人类驾驶风格异质性管理下的表现。


<details>
  <summary>Details</summary>
Motivation: 在混合交通环境中，自主车辆与多样化的人类驾驶车辆之间的不确定意图和异质行为使得安全和高效的换道操作变得极具挑战性。

Method: 集成了驾驶风格识别、合作意识决策和协调运动规划的意图驱动换道框架。

Result: 实验表明，该模型在换道识别方面达到94.2%的准确率和94.3%的F1-score，超越了基于规则和基于学习的基线4-15%。

Conclusion: 提出的框架在复杂交通环境中具有适应性和人性化的自主驾驶潜力，准确反映了不同驾驶风格的交互。

Abstract: In mixed-traffic environments, where autonomous vehicles (AVs) interact with
diverse human-driven vehicles (HVs), unpredictable intentions and heterogeneous
behaviors make safe and efficient lane change maneuvers highly challenging.
Existing methods often oversimplify these interactions by assuming uniform
patterns. We propose an intention-driven lane change framework that integrates
driving-style recognition, cooperation-aware decision-making, and coordinated
motion planning. A deep learning classifier trained on the NGSIM dataset
identifies human driving styles in real time. A cooperation score with
intrinsic and interactive components estimates surrounding drivers' intentions
and quantifies their willingness to cooperate with the ego vehicle.
Decision-making combines behavior cloning with inverse reinforcement learning
to determine whether a lane change should be initiated. For trajectory
generation, model predictive control is integrated with IRL-based intention
inference to produce collision-free and socially compliant maneuvers.
Experiments show that the proposed model achieves 94.2\% accuracy and 94.3\%
F1-score, outperforming rule-based and learning-based baselines by 4-15\% in
lane change recognition. These results highlight the benefit of modeling
inter-driver heterogeneity and demonstrate the potential of the framework to
advance context-aware and human-like autonomous driving in complex traffic
environments.

</details>


### [61] [MINT-RVAE: Multi-Cues Intention Prediction of Human-Robot Interaction using Human Pose and Emotion Information from RGB-only Camera Data](https://arxiv.org/abs/2509.22573)
*Farida Mohsen,Ali Safa*

Main category: cs.RO

TL;DR: 提出一种新方法，依赖RGB数据预测人类互动意图，解决了类不平衡等问题，实现了高性能，并发布了新数据集。


<details>
  <summary>Details</summary>
Motivation: 提高人-机器人交互的有效性，并克服现有方法对多模态输入的依赖。

Method: 引入了MINT-RVAE合成序列生成方法，结合新的损失函数和训练策略，来解决类不平衡问题。

Result: 在状态最先进的性能上达到了AUROC: 0.95，超过了之前的工作。

Conclusion: 提出了一种仅使用RGB输入预测人类互动意图的方法，并取得了出色的结果。

Abstract: Efficiently detecting human intent to interact with ubiquitous robots is
crucial for effective human-robot interaction (HRI) and collaboration. Over the
past decade, deep learning has gained traction in this field, with most
existing approaches relying on multimodal inputs, such as RGB combined with
depth (RGB-D), to classify time-sequence windows of sensory data as interactive
or non-interactive. In contrast, we propose a novel RGB-only pipeline for
predicting human interaction intent with frame-level precision, enabling faster
robot responses and improved service quality. A key challenge in intent
prediction is the class imbalance inherent in real-world HRI datasets, which
can hinder the model's training and generalization. To address this, we
introduce MINT-RVAE, a synthetic sequence generation method, along with new
loss functions and training strategies that enhance generalization on
out-of-sample data. Our approach achieves state-of-the-art performance (AUROC:
0.95) outperforming prior works (AUROC: 0.90-0.912), while requiring only RGB
input and supporting precise frame onset prediction. Finally, to support future
research, we openly release our new dataset with frame-level labeling of human
interaction intent.

</details>


### [62] [EgoDemoGen: Novel Egocentric Demonstration Generation Enables Viewpoint-Robust Manipulation](https://arxiv.org/abs/2509.22578)
*Yuan Xu,Jiabing Yang,Xiaofeng Wang,Yixiang Chen,Zheng Zhu,Bowen Fang,Guan Huang,Xinze Chen,Yun Ye,Qiang Zhang,Peiyan Li,Xiangnan Wu,Kai Wang,Bing Zhan,Shuo Lu,Jing Liu,Nianfeng Liu,Yan Huang,Liang Wang*

Main category: cs.RO

TL;DR: EgoDemoGen通过生成新的第一人称示范显著提升了机器人在视角变化下的操控性能，仿真和实际测试中均取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 目前的模仿学习策略在处理单一第一人称视角下表现良好，但在视角变化时性能下降，急需改善这一问题。

Method: 通过生成配对的新的第一人称示范并结合生成的视频修复模型EgoViewTransfer，EgoDemoGen有效地应对了视角变化带来的影响。

Result: 在仿真环境中，EgoDemoGen在标准第一人称视角和新的第一人称视角上的策略成功率分别提高了17.0%和17.7%；在真实世界机器人测试中，绝对提高分别为18.3%和25.8%。

Conclusion: EgoDemoGen为实现以第一人称视角为基础的稳健机器人操控提供了一种有效的方法，通过生成新的示范来改善操控性能。

Abstract: Imitation learning based policies perform well in robotic manipulation, but
they often degrade under *egocentric viewpoint shifts* when trained from a
single egocentric viewpoint. To address this issue, we present **EgoDemoGen**,
a framework that generates *paired* novel egocentric demonstrations by
retargeting actions in the novel egocentric frame and synthesizing the
corresponding egocentric observation videos with proposed generative video
repair model **EgoViewTransfer**, which is conditioned by a novel-viewpoint
reprojected scene video and a robot-only video rendered from the retargeted
joint actions. EgoViewTransfer is finetuned from a pretrained video generation
model using self-supervised double reprojection strategy. We evaluate
EgoDemoGen on both simulation (RoboTwin2.0) and real-world robot. After
training with a mixture of EgoDemoGen-generated novel egocentric demonstrations
and original standard egocentric demonstrations, policy success rate improves
**absolutely** by **+17.0%** for standard egocentric viewpoint and by
**+17.7%** for novel egocentric viewpoints in simulation. On real-world robot,
the **absolute** improvements are **+18.3%** and **+25.8%**. Moreover,
performance continues to improve as the proportion of EgoDemoGen-generated
demonstrations increases, with diminishing returns. These results demonstrate
that EgoDemoGen provides a practical route to egocentric viewpoint-robust
robotic manipulation.

</details>


### [63] [WoW: Towards a World omniscient World model Through Embodied Interaction](https://arxiv.org/abs/2509.22642)
*Xiaowei Chi,Peidong Jia,Chun-Kai Fan,Xiaozhu Ju,Weishi Mi,Kevin Zhang,Zhiyuan Qin,Wanxin Tian,Kuangzhi Ge,Hao Li,Zezhong Qian,Anthony Chen,Qiang Zhou,Yueru Jia,Jiaming Liu,Yong Dai,Qingpo Wuwu,Chengyu Bai,Yu-Kai Wang,Ying Li,Lizhang Chen,Yong Bao,Zhiyuan Jiang,Jiacheng Zhu,Kai Tang,Ruichuan An,Yulin Luo,Qiuxuan Feng,Siyuan Zhou,Chi-min Chan,Chengkai Hou,Wei Xue,Sirui Han,Yike Guo,Shanghang Zhang,Jian Tang*

Main category: cs.RO

TL;DR: 本研究提出WoW生成世界模型，通过真实世界交互提升AI的物理直觉，建立新基准WoWBench，证明大规模交互对物理理解的重要性，具有开源价值。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有视频模型（如Sora）因依赖被动观察而难以掌握物理因果关系的问题，提出通过真实世界的丰富因果交互来建立物理直觉的假设。

Method: 提出WoW，一个基于140亿参数的生成世界模型，根据200万条机器人交互轨迹进行训练，同时使用SOPHIA和共训练的反向动力学模型约束物理现实性，并将其转化为可执行的机器人动作。

Result: WoW模型的物理理解表现为可能结果的概率分布，导致随机不稳定性与物理幻觉。WoW在人类和自主评估中均实现了最新的性能，成为物理一致性和因果推理的新基准WoWBench。

Conclusion: 本研究系统性地证明了大规模真实世界交互是AI发展物理直觉的基础，WoW模型在物理因果关系、碰撞动态和物体永久性等方面展现出强大的能力。

Abstract: Humans develop an understanding of intuitive physics through active
interaction with the world. This approach is in stark contrast to current video
models, such as Sora, which rely on passive observation and therefore struggle
with grasping physical causality. This observation leads to our central
hypothesis: authentic physical intuition of the world model must be grounded in
extensive, causally rich interactions with the real world. To test this
hypothesis, we present WoW, a 14-billion-parameter generative world model
trained on 2 million robot interaction trajectories. Our findings reveal that
the model's understanding of physics is a probabilistic distribution of
plausible outcomes, leading to stochastic instabilities and physical
hallucinations. Furthermore, we demonstrate that this emergent capability can
be actively constrained toward physical realism by SOPHIA, where
vision-language model agents evaluate the DiT-generated output and guide its
refinement by iteratively evolving the language instructions. In addition, a
co-trained Inverse Dynamics Model translates these refined plans into
executable robotic actions, thus closing the imagination-to-action loop. We
establish WoWBench, a new benchmark focused on physical consistency and causal
reasoning in video, where WoW achieves state-of-the-art performance in both
human and autonomous evaluation, demonstrating strong ability in physical
causality, collision dynamics, and object permanence. Our work provides
systematic evidence that large-scale, real-world interaction is a cornerstone
for developing physical intuition in AI. Models, data, and benchmarks will be
open-sourced.

</details>


### [64] [VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning via Online Monte Carlo Tree Search](https://arxiv.org/abs/2509.22643)
*Wenkai Guo,Guanxing Lu,Haoyuan Deng,Zhenyu Wu,Yansong Tang,Ziwei Wang*

Main category: cs.RO

TL;DR: VLA-Reasoner通过增强VLA模型的未来状态预测能力，显著改善了长时间轨迹任务的处理，有效提升了机器人操作的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型在处理长期轨迹任务时表现有限，因此需要一种方法来预测未来状态，提升其在复杂任务中的表现。

Method: 通过在测试时扩展现有的视觉-语言-行动模型(VLA)，使用世界模型进行未来状态的预测，并结合蒙特卡罗树搜索(MCTS)和基于核密度估计(KDE)的置信度采样机制。

Result: VLA-Reasoner显著提升了现有VLA模型在复杂操控任务中的表现，能有效地推测潜在结果并寻找最优动作。

Conclusion: VLA-Reasoner在模拟器和现实世界中展示了显著的改进，开启了机器人操作的可扩展测试时计算的新路径。

Abstract: Vision-Language-Action models (VLAs) achieve strong performance in general
robotic manipulation tasks by scaling imitation learning. However, existing
VLAs are limited to predicting short-sighted next-action, which struggle with
long-horizon trajectory tasks due to incremental deviations. To address this
problem, we propose a plug-in framework named VLA-Reasoner that effectively
empowers off-the-shelf VLAs with the capability of foreseeing future states via
test-time scaling. Specifically, VLA-Reasoner samples and rolls out possible
action trajectories where involved actions are rationales to generate future
states via a world model, which enables VLA-Reasoner to foresee and reason
potential outcomes and search for the optimal actions. We further leverage
Monte Carlo Tree Search (MCTS) to improve search efficiency in large action
spaces, where stepwise VLA predictions seed the root. Meanwhile, we introduce a
confidence sampling mechanism based on Kernel Density Estimation (KDE), to
enable efficient exploration in MCTS without redundant VLA queries. We evaluate
intermediate states in MCTS via an offline reward shaping strategy, to score
predicted futures and correct deviations with long-term feedback. We conducted
extensive experiments in both simulators and the real world, demonstrating that
our proposed VLA-Reasoner achieves significant improvements over the
state-of-the-art VLAs. Our method highlights a potential pathway toward
scalable test-time computation of robotic manipulation.

</details>


### [65] [Pixel Motion Diffusion is What We Need for Robot Control](https://arxiv.org/abs/2509.22652)
*E-Ro Nguyen,Yichi Zhang,Kanchana Ranasinghe,Xiang Li,Michael S. Ryoo*

Main category: cs.RO

TL;DR: DAWN是一个基于扩散的机器人控制框架，有效连接高低层动作意图，实现多任务表现和良好的真实世界迁移。


<details>
  <summary>Details</summary>
Motivation: 通过结构化像素运动表示，将高层运动意图与低层机器人动作连接起来，以提升机器人操控能力。

Method: DAWN将高层和低层控制器均建模为扩散过程，构建统一的扩散框架，具有可训练性和可解释的中间运动抽象。

Result: DAWN在CALVIN基准测试中取得了最先进的结果，并在MetaWorld验证了其强大的多任务表现，展示了在模拟和真实世界之间的可靠迁移能力。

Conclusion: DAWN展现了基于扩散建模与运动中心表示相结合的有效性，为机器人学习提供了强有力的基线，且在真实世界中只需少量微调即可成功迁移。

Abstract: We present DAWN (Diffusion is All We Need for robot control), a unified
diffusion-based framework for language-conditioned robotic manipulation that
bridges high-level motion intent and low-level robot action via structured
pixel motion representation. In DAWN, both the high-level and low-level
controllers are modeled as diffusion processes, yielding a fully trainable,
end-to-end system with interpretable intermediate motion abstractions. DAWN
achieves state-of-the-art results on the challenging CALVIN benchmark,
demonstrating strong multi-task performance, and further validates its
effectiveness on MetaWorld. Despite the substantial domain gap between
simulation and reality and limited real-world data, we demonstrate reliable
real-world transfer with only minimal finetuning, illustrating the practical
viability of diffusion-based motion abstractions for robotic control. Our
results show the effectiveness of combining diffusion modeling with
motion-centric representations as a strong baseline for scalable and robust
robot learning. Project page: https://nero1342.github.io/DAWN/

</details>


### [66] [See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation](https://arxiv.org/abs/2509.22653)
*Chih Yao Hu,Yang-Sen Lin,Yuna Lee,Chih-Hai Su,Jie-Ying Lee,Shr-Ruei Tsai,Chin-Yang Lin,Kuan-Wen Chen,Tsung-Wei Ke,Yu-Lun Liu*

Main category: cs.RO

TL;DR: SPF 是一种无训练的空中视觉与语言导航框架，通过将模糊指令转化为 2D 路径点和 3D 动作命令，显著提升了无人机在动态环境中的导航能力。


<details>
  <summary>Details</summary>
Motivation: 开发一种无训练的 AVLN 框架，使 UAV 能够根据自由形式的指令在各种环境中导航。

Method: SPF 利用 VLMs 将模糊语言指令分解为输入图像上的 2D 路径点，并将其转化为 UAV 的 3D 位移向量作为行动指令。

Result: SPF 在多项基准测试中超越了先前最佳方法63%，且在真实应用中表现优异。

Conclusion: SPF 在 DRL 仿真基准测试中创下新纪录，且在真实世界评估中表现出色，展示了其良好的泛化能力。

Abstract: We present See, Point, Fly (SPF), a training-free aerial vision-and-language
navigation (AVLN) framework built atop vision-language models (VLMs). SPF is
capable of navigating to any goal based on any type of free-form instructions
in any kind of environment. In contrast to existing VLM-based approaches that
treat action prediction as a text generation task, our key insight is to
consider action prediction for AVLN as a 2D spatial grounding task. SPF
harnesses VLMs to decompose vague language instructions into iterative
annotation of 2D waypoints on the input image. Along with the predicted
traveling distance, SPF transforms predicted 2D waypoints into 3D displacement
vectors as action commands for UAVs. Moreover, SPF also adaptively adjusts the
traveling distance to facilitate more efficient navigation. Notably, SPF
performs navigation in a closed-loop control manner, enabling UAVs to follow
dynamic targets in dynamic environments. SPF sets a new state of the art in DRL
simulation benchmark, outperforming the previous best method by an absolute
margin of 63%. In extensive real-world evaluations, SPF outperforms strong
baselines by a large margin. We also conduct comprehensive ablation studies to
highlight the effectiveness of our design choice. Lastly, SPF shows remarkable
generalization to different VLMs. Project page: https://spf-web.pages.dev

</details>
