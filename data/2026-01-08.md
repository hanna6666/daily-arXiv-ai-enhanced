<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 20]
- [cs.HC](#cs.HC) [Total: 4]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Revisiting Continuous-Time Trajectory Estimation via Gaussian Processes and the Magnus Expansion](https://arxiv.org/abs/2601.03360)
*Timothy Barfoot,Cedric Le Gentil,Sven Lilge*

Main category: cs.RO

TL;DR: 本研究通过Magnus扩展提供了一种全局高斯过程先验用于李群的连续时间轨迹估计，克服了以往方法的局限性，并进行了相关性能比较。


<details>
  <summary>Details</summary>
Motivation: 针对以往方法在李群上的局部高斯过程的不够优雅的问题，寻求一个更通用的解决方案。

Method: 利用Magnus扩展的方法推导李群的全局高斯过程先验，并进行数值比较。

Result: 通过数值比较，可以评估全局高斯过程与局部高斯过程在连续时间轨迹估计中的相对优缺点。

Conclusion: 通过Magnus扩展导出在李群上的全局高斯过程先验提供了一种更优雅和通用的解决方案。

Abstract: Continuous-time state estimation has been shown to be an effective means of (i) handling asynchronous and high-rate measurements, (ii) introducing smoothness to the estimate, (iii) post hoc querying the estimate at times other than those of the measurements, and (iv) addressing certain observability issues related to scanning-while-moving sensors. A popular means of representing the trajectory in continuous time is via a Gaussian process (GP) prior, with the prior's mean and covariance functions generated by a linear time-varying (LTV) stochastic differential equation (SDE) driven by white noise. When the state comprises elements of Lie groups, previous works have resorted to a patchwork of local GPs each with a linear time-invariant SDE kernel, which while effective in practice, lacks theoretical elegance. Here we revisit the full LTV GP approach to continuous-time trajectory estimation, deriving a global GP prior on Lie groups via the Magnus expansion, which offers a more elegant and general solution. We provide a numerical comparison between the two approaches and discuss their relative merits.

</details>


### [2] [Lunar Rover Cargo Transport: Mission Concept and Field Test](https://arxiv.org/abs/2601.03371)
*Alexander Krawciw,Nicolas Olmedo,Faizan Rehmatullah,Maxime Desjardins-Goulet,Pascal Toupin,Timothy D. Barfoot*

Main category: cs.RO

TL;DR: 本研究展示了Lidar Teach and Repeat在未来月球地表操作中，实现自动运输货物的有效性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 为了实现未来月球表面的自动化运输，开发一种可以安全导航并可靠搬运货物的解决方案。

Method: 使用Lidar Teach and Repeat方法进行半自动化驾驶，建立安全路径网络并进行全自动搬运 cargos。

Result: 在加拿大航天局的模拟月球地形进行为期两周的现场测试，成功完成了货物的收集和交付。

Conclusion: 系统在恶劣环境下表现出色，能够精确导航并安全运输货物。

Abstract: In future operations on the lunar surface, automated vehicles will be required to transport cargo between known locations. Such vehicles must be able to navigate precisely in safe regions to avoid natural hazards, human-constructed infrastructure, and dangerous dark shadows. Rovers must be able to park their cargo autonomously within a small tolerance to achieve a successful pickup and delivery. In this field test, Lidar Teach and Repeat provides an ideal autonomy solution for transporting cargo in this way. A one-tonne path-to-flight rover was driven in a semi-autonomous remote-control mode to create a network of safe paths. Once the route was taught, the rover immediately repeated the entire network of paths autonomously while carrying cargo. The closed-loop performance is accurate enough to align the vehicle to the cargo and pick it up. This field report describes a two-week deployment at the Canadian Space Agency's Analogue Terrain, culminating in a simulated lunar operation to evaluate the system's capabilities. Successful cargo collection and delivery were demonstrated in harsh environmental conditions.

</details>


### [3] [Towards Zero-Knowledge Task Planning via a Language-based Approach](https://arxiv.org/abs/2601.03398)
*Liam Merz Hoffmeister,Brian Scassellati,Daniel Rakita*

Main category: cs.RO

TL;DR: 本研究提出零知识任务规划问题，利用大型语言模型生成和调整行为树，提高了任务执行的性能，展示了无任务特定知识情况下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在没有任务特定知识的情况下，寻找有效的任务规划方法。

Method: 提出了利用大型语言模型将自然语言指令分解为子任务并生成行为树的初步方法。

Result: 经过实验验证，本方法在AI2-THOR模拟器中相比于依赖任务特定知识的其他方法，显著提升了整体任务性能。

Conclusion: 本研究展示了大型语言模型在无任务特定知识情况下有效解决零知识任务规划问题的潜力，并提供了一个稳健的自动行为生成框架。

Abstract: In this work, we introduce and formalize the Zero-Knowledge Task Planning (ZKTP) problem, i.e., formulating a sequence of actions to achieve some goal without task-specific knowledge. Additionally, we present a first investigation and approach for ZKTP that leverages a large language model (LLM) to decompose natural language instructions into subtasks and generate behavior trees (BTs) for execution. If errors arise during task execution, the approach also uses an LLM to adjust the BTs on-the-fly in a refinement loop. Experimental validation in the AI2-THOR simulator demonstrate our approach's effectiveness in improving overall task performance compared to alternative approaches that leverage task-specific knowledge. Our work demonstrates the potential of LLMs to effectively address several aspects of the ZKTP problem, providing a robust framework for automated behavior generation with no task-specific setup.

</details>


### [4] [Cost-Effective Radar Sensors for Field-Based Water Level Monitoring with Sub-Centimeter Accuracy](https://arxiv.org/abs/2601.03447)
*Anna Zavei-Boroda,J. Toby Minear,Kyle Harlow,Dusty Woods,Christoffer Heckman*

Main category: cs.RO

TL;DR: 本研究展示了一种基于雷达的低成本水位监测方法，能在各种环境条件下实现高精度的水位估计。


<details>
  <summary>Details</summary>
Motivation: 传统水位监测方法成本高且覆盖范围有限，因此需要寻找低成本的替代方案。

Method: 评估商业雷达传感器在实际场地测试中的表现，并应用统计滤波技术来提高精度。

Result: 单个雷达传感器在实践中能够实现厘米级精度，仅需最小的校准。

Conclusion: 使用商业雷达传感器可以实现厘米级精度的水位监测，适合无人机和机器人平台的自主监测。

Abstract: Water level monitoring is critical for flood management, water resource allocation, and ecological assessment, yet traditional methods remain costly and limited in coverage. This work explores radar-based sensing as a low-cost alternative for water level estimation, leveraging its non-contact nature and robustness to environmental conditions. Commercial radar sensors are evaluated in real-world field tests, applying statistical filtering techniques to improve accuracy. Results show that a single radar sensor can achieve centimeter-scale precision with minimal calibration, making it a practical solution for autonomous water monitoring using drones and robotic platforms.

</details>


### [5] [FIRE-VLM: A Vision-Language-Driven Reinforcement Learning Framework for UAV Wildfire Tracking in a Physics-Grounded Fire Digital Twin](https://arxiv.org/abs/2601.03449)
*Chris Webb,Mobin Habibpour,Mayamin Hamid Raha,Ali Reza Tavakkoli,Janice Coen,Fatemeh Afghah*

Main category: cs.RO

TL;DR: 本文介绍了一种新的火灾监测框架FIRE-VLM，结合了高保真的物理模拟和视觉-语言模型，显著提高了无人机在火灾检测中的效率和精确性。


<details>
  <summary>Details</summary>
Motivation: 现有的无人机导航方法依赖于简化的模拟器和监督感知流程，缺乏与物理真实火灾环境交互的体现代理，对于火灾监控的实时性和准确性需求迫切。

Method: 通过在高保真、基于物理的火灾数字双胞胎环境中利用VLM引导的强化学习框架进行训练，使用PPO代理和双视角无人机传感器。

Result: 在五个数字双胞胎评估任务中，VLM引导的策略将检测时间缩短最多6倍，增加视场内停留时间，证明了系统在监测效果上的显著提升。

Conclusion: FIRE-VLM显著提高了无人机火灾监测的效率和准确性，是迄今为止首次在物理真实的数字双胞胎火灾环境中展示的基于强化学习的无人机火灾监测系统。

Abstract: Wildfire monitoring demands autonomous systems capable of reasoning under extreme visual degradation, rapidly evolving physical dynamics, and scarce real-world training data. Existing UAV navigation approaches rely on simplified simulators and supervised perception pipelines, and lack embodied agents interacting with physically realistic fire environments. We introduce FIRE-VLM, the first end-to-end vision-language model (VLM) guided reinforcement learning (RL) framework trained entirely within a high-fidelity, physics-grounded wildfire digital twin. Built from USGS Digital Elevation Model (DEM) terrain, LANDFIRE fuel inventories, and semi-physical fire-spread solvers, this twin captures terrain-induced runs, wind-driven acceleration, smoke plume occlusion, and dynamic fuel consumption. Within this environment, a PPO agent with dual-view UAV sensing is guided by a CLIP-style VLM. Wildfire-specific semantic alignment scores, derived from a single prompt describing active fire and smoke plumes, are integrated as potential-based reward shaping signals. Our contributions are: (1) a GIS-to-simulation pipeline for constructing wildfire digital twins; (2) a VLM-guided RL agent for UAV firefront tracking; and (3) a wildfire-aware reward design that combines physical terms with VLM semantics. Across five digital-twin evaluation tasks, our VLM-guided policy reduces time-to-detection by up to 6 times, increases time-in-FOV, and is, to our knowledge, the first RL-based UAV wildfire monitoring system demonstrated in kilometer-scale, physics-grounded digital-twin fires.

</details>


### [6] [A Vision-Language-Action Model with Visual Prompt for OFF-Road Autonomous Driving](https://arxiv.org/abs/2601.03519)
*Liangdong Zhang,Yiming Nie,Haoyang Li,Fanjie Kong,Baobao Zhang,Shunxin Huang,Kai Fu,Chen Min,Liang Xiao*

Main category: cs.RO

TL;DR: 提出了一种新框架OFF-EMMA来提高越野自动驾驶的轨迹规划效率和准确性，通过视觉提示块和自一致性推理策略解决了传统方法在动态环境中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统的越野轨迹规划方法在动态环境中适应性差，因此需要一种新方式来提高效率和准确性。

Method: 采用OFF-EMMA框架，结合视觉提示块和链式自一致性推理策略，使用语义分割掩码增强空间理解，改进轨迹规划。

Result: 在RELLIS-3D数据集上，OFF-EMMA较现有方法显著提升性能，平均L2误差减少了13.3%，失败率从16.52%降低到6.56%。

Conclusion: OFF-EMMA通过引入新框架和策略，有效提升了越野环境中的自动驾驶轨迹规划性能。

Abstract: Efficient trajectory planning in off-road terrains presents a formidable challenge for autonomous vehicles, often necessitating complex multi-step pipelines. However, traditional approaches exhibit limited adaptability in dynamic environments. To address these limitations, this paper proposes OFF-EMMA, a novel end-to-end multimodal framework designed to overcome the deficiencies of insufficient spatial perception and unstable reasoning in visual-language-action (VLA) models for off-road autonomous driving scenarios. The framework explicitly annotates input images through the design of a visual prompt block and introduces a chain-of-thought with self-consistency (COT-SC) reasoning strategy to enhance the accuracy and robustness of trajectory planning. The visual prompt block utilizes semantic segmentation masks as visual prompts, enhancing the spatial understanding ability of pre-trained visual-language models for complex terrains. The COT- SC strategy effectively mitigates the error impact of outliers on planning performance through a multi-path reasoning mechanism. Experimental results on the RELLIS-3D off-road dataset demonstrate that OFF-EMMA significantly outperforms existing methods, reducing the average L2 error of the Qwen backbone model by 13.3% and decreasing the failure rate from 16.52% to 6.56%.

</details>


### [7] [From Score to Sound: An End-to-End MIDI-to-Motion Pipeline for Robotic Cello Performance](https://arxiv.org/abs/2601.03562)
*Samantha Sudhoff,Pranesh Velmurugan,Jiashu Liu,Vincent Zhao,Yung-Hsiang Lu,Kristen Yeon-Ji Yun*

Main category: cs.RO

TL;DR: 提出了一种新型的MIDI乐谱转机器人运动的管道，使UR5e机器人大提琴手能够以人类般的方式演奏，而无需使用运动捕捉技术。


<details>
  <summary>Details</summary>
Motivation: 为了使机器人音乐家在演奏弦乐器时能够实现高精度的音符准确性和音乐表现力。

Method: 通过端到端的MIDI乐谱转机器运动管道，将音乐输入直接转换为与碰撞-aware的拉弓动作，为UR5e机器人提供演奏能力。

Result: 与132名人类参与者的Musical Turing Test比较，我们的机器人演奏被认为能够达到人类水平，并且发布了对比人的参考录音作为未来研究的基准。

Conclusion: 我们提出的残差强化学习方法有助于改善机器人控制，开辟了提高弦乐交叉效率和音质的未来机会。

Abstract: Robot musicians require precise control to obtain proper note accuracy, sound quality, and musical expression. Performance of string instruments, such as violin and cello, presents a significant challenge due to the precise control required over bow angle and pressure to produce the desired sound. While prior robotic cellists focus on accurate bowing trajectories, these works often rely on expensive motion capture techniques, and fail to sightread music in a human-like way.
  We propose a novel end-to-end MIDI score to robotic motion pipeline which converts musical input directly into collision-aware bowing motions for a UR5e robot cellist. Through use of Universal Robot Freedrive feature, our robotic musician can achieve human-like sound without the need for motion capture. Additionally, this work records live joint data via Real-Time Data Exchange (RTDE) as the robot plays, providing labeled robotic playing data from a collection of five standard pieces to the research community. To demonstrate the effectiveness of our method in comparison to human performers, we introduce the Musical Turing Test, in which a collection of 132 human participants evaluate our robot's performance against a human baseline. Human reference recordings are also released, enabling direct comparison for future studies. This evaluation technique establishes the first benchmark for robotic cello performance. Finally, we outline a residual reinforcement learning methodology to improve upon baseline robotic controls, highlighting future opportunities for improved string-crossing efficiency and sound quality.

</details>


### [8] [Locomotion Beyond Feet](https://arxiv.org/abs/2601.03607)
*Tae Hoon Yang,Haochen Shi,Jiacheng Hu,Zhicong Zhang,Daniel Jiang,Weizhuo Wang,Yao He,Zhen Wu,Yuming Chen,Yifan Hou,Monroe Kennedy,Shuran Song,C. Karen Liu*

Main category: cs.RO

TL;DR: 本文提出了一种新的人形机器人运动系统，能够在复杂地形中通过全身运动实现稳定和支持，结合了关键帧动画与强化学习，效果显著。


<details>
  <summary>Details</summary>
Motivation: 大多数人形机器人运动方法专注于腿部的步态，但自然人类经常依赖手、膝盖和肘部在复杂环境中建立额外的接触点以增强稳定性和支持。

Method: 结合物理基础的关键帧动画与强化学习，使用分层框架中的特定地形运动跟踪策略和基于视觉的技能规划器。

Result: 该系统在极具挑战性的地形上成功执行全身运动，能够应对低净高空间、膝高墙、平台和陡坡等复杂环境。

Conclusion: Locomotion Beyond Feet系统能够实现稳健的全身运动，并且在多种障碍物和地形序列中具备良好的泛化能力。

Abstract: Most locomotion methods for humanoid robots focus on leg-based gaits, yet natural bipeds frequently rely on hands, knees, and elbows to establish additional contacts for stability and support in complex environments. This paper introduces Locomotion Beyond Feet, a comprehensive system for whole-body humanoid locomotion across extremely challenging terrains, including low-clearance spaces under chairs, knee-high walls, knee-high platforms, and steep ascending and descending stairs. Our approach addresses two key challenges: contact-rich motion planning and generalization across diverse terrains. To this end, we combine physics-grounded keyframe animation with reinforcement learning. Keyframes encode human knowledge of motor skills, are embodiment-specific, and can be readily validated in simulation or on hardware, while reinforcement learning transforms these references into robust, physically accurate motions. We further employ a hierarchical framework consisting of terrain-specific motion-tracking policies, failure recovery mechanisms, and a vision-based skill planner. Real-world experiments demonstrate that Locomotion Beyond Feet achieves robust whole-body locomotion and generalizes across obstacle sizes, obstacle instances, and terrain sequences.

</details>


### [9] [Dual-Attention Heterogeneous GNN for Multi-robot Collaborative Area Search via Deep Reinforcement Learning](https://arxiv.org/abs/2601.03686)
*Lina Zhu,Jiyu Cheng,Yuehu Liu,Wei Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种双重注意力异构图神经网络（DA-HGNN），通过深度强化学习训练，以解决多机器人协作领域搜索中的探索与覆盖任务平衡问题。


<details>
  <summary>Details</summary>
Motivation: 多机器人协作搜索中，探索未知区域和覆盖特定目标之间的平衡是一大挑战，现有方法受限于同质图表示，无法有效建模和解决这个问题。

Method: 提出一种双重注意力异构图神经网络（DA-HGNN），利用深度强化学习训练，构建了一个包含机器人节点、边界节点和兴趣节点的异构图，并引入双重注意力机制。

Result: 通过在iGibson模拟器中进行的大量实验，验证了所提方法的优越可扩展性和泛化能力。

Conclusion: 本研究提出的DA-HGNN能够有效平衡探索未知领域和覆盖特定目标，提高多机器人协作搜索的效率。

Abstract: In multi-robot collaborative area search, a key challenge is to dynamically balance the two objectives of exploring unknown areas and covering specific targets to be rescued. Existing methods are often constrained by homogeneous graph representations, thus failing to model and balance these distinct tasks. To address this problem, we propose a Dual-Attention Heterogeneous Graph Neural Network (DA-HGNN) trained using deep reinforcement learning. Our method constructs a heterogeneous graph that incorporates three entity types: robot nodes, frontier nodes, and interesting nodes, as well as their historical states. The dual-attention mechanism comprises the relational-aware attention and type-aware attention operations. The relational-aware attention captures the complex spatio-temporal relationships among robots and candidate goals. Building on this relational-aware heterogeneous graph, the type-aware attention separately computes the relevance between robots and each goal type (frontiers vs. points of interest), thereby decoupling the exploration and coverage from the unified tasks. Extensive experiments conducted in interactive 3D scenarios within the iGibson simulator, leveraging the Gibson and MatterPort3D datasets, validate the superior scalability and generalization capability of the proposed approach.

</details>


### [10] [PointWorld: Scaling 3D World Models for In-The-Wild Robotic Manipulation](https://arxiv.org/abs/2601.03782)
*Wenlong Huang,Yu-Wei Chao,Arsalan Mousavian,Ming-Yu Liu,Dieter Fox,Kaichun Mo,Li Fei-Fei*

Main category: cs.RO

TL;DR: PointWorld是一种预训练的3D世界模型，通过3D点流预测机器人动作对环境的反应，具有快速推断能力，能在实际中实现复杂操作。


<details>
  <summary>Details</summary>
Motivation: 人类能够预测身体行动的环境反应，这种能力对于机器人操作同样重要。

Method: 提出了PointWorld，一个大型预训练的3D世界模型，通过3D点流统一状态和动作，并预测3D中每个像素的位移。

Result: PointWorld经过严格的大规模实证研究，优化了3D世界建模的设计原则，实现在0.1秒内的实时推断速度。

Conclusion: 利用单个预训练的检查点，Franka机器人能够高效地进行物体操作，无需演示或后训练，仅通过捕获单张自然图像。

Abstract: Humans anticipate, from a glance and a contemplated action of their bodies, how the 3D world will respond, a capability that is equally vital for robotic manipulation. We introduce PointWorld, a large pre-trained 3D world model that unifies state and action in a shared 3D space as 3D point flows: given one or few RGB-D images and a sequence of low-level robot action commands, PointWorld forecasts per-pixel displacements in 3D that respond to the given actions. By representing actions as 3D point flows instead of embodiment-specific action spaces (e.g., joint positions), this formulation directly conditions on physical geometries of robots while seamlessly integrating learning across embodiments. To train our 3D world model, we curate a large-scale dataset spanning real and simulated robotic manipulation in open-world environments, enabled by recent advances in 3D vision and simulated environments, totaling about 2M trajectories and 500 hours across a single-arm Franka and a bimanual humanoid. Through rigorous, large-scale empirical studies of backbones, action representations, learning objectives, partial observability, data mixtures, domain transfers, and scaling, we distill design principles for large-scale 3D world modeling. With a real-time (0.1s) inference speed, PointWorld can be efficiently integrated in the model-predictive control (MPC) framework for manipulation. We demonstrate that a single pre-trained checkpoint enables a real-world Franka robot to perform rigid-body pushing, deformable and articulated object manipulation, and tool use, without requiring any demonstrations or post-training and all from a single image captured in-the-wild. Project website at https://point-world.github.io/.

</details>


### [11] [Generational Replacement and Learning for High-Performing and Diverse Populations in Evolvable Robots](https://arxiv.org/abs/2601.03807)
*K. Ege de Bruin,Kyrre Glette,Kai Olav Ellefsen*

Main category: cs.RO

TL;DR: 通过结合世代替换和内部学习，可以在提高进化机器人多样性的同时保持其性能，并强调评估指标的重要性。


<details>
  <summary>Details</summary>
Motivation: 为了克服控制器适应演化形态所需时间的问题，以及新设计难以进入演化种群，增加多样性以防止快速收敛于少数设计。

Method: 将世代替换与个体内部学习相结合，以提高演化过程中机器人的多样性和性能。

Result: 结合世代替换和个体学习后，发现能够在保持性能的前提下显著提高种群的多样性，并且不同的性能评估方法可能得出不同的结论。

Conclusion: 通过结合世代替换和个体学习，能够在保持机器人性能的同时提高种群多样性，并指出评估性能时所用的指标对研究学习的重要性。

Abstract: Evolutionary Robotics offers the possibility to design robots to solve a specific task automatically by optimizing their morphology and control together. However, this co-optimization of body and control is challenging, because controllers need some time to adapt to the evolving morphology - which may make it difficult for new and promising designs to enter the evolving population. A solution to this is to add intra-life learning, defined as an additional controller optimization loop, to each individual in the evolving population. A related problem is the lack of diversity often seen in evolving populations as evolution narrows the search down to a few promising designs too quickly. This problem can be mitigated by implementing full generational replacement, where offspring robots replace the whole population. This solution for increasing diversity usually comes at the cost of lower performance compared to using elitism. In this work, we show that combining such generational replacement with intra-life learning can increase diversity while retaining performance. We also highlight the importance of performance metrics when studying learning in morphologically evolving robots, showing that evaluating according to function evaluations versus according to generations of evolution can give different conclusions.

</details>


### [12] [Integrating Sample Inheritance into Bayesian Optimization for Evolutionary Robotics](https://arxiv.org/abs/2601.03813)
*K. Ege de Bruin,Kyrre Glette,Kai Olav Ellefsen*

Main category: cs.RO

TL;DR: 本研究探讨了进化机器人中使用贝叶斯优化进行控制器优化，利用样本继承机制提升学习效率，即使在较低的学习预算下也能优化机器人形态和控制。


<details>
  <summary>Details</summary>
Motivation: 在进化机器人中，机器人形态和控制需要同步优化，传统方法在面对新形态时往往需要重头开始，消耗大量学习资源。

Method: 采用贝叶斯优化进行控制器优化，并探讨了两种样本继承方式：一种是直接将父体样本转移给子体，另一种是对父体最佳样本在子体上重新评估，比较其效果。

Result: 重新评估的方法表现最佳，基于先前样本的继承也优于无继承的情况。虽然单个形态的学习预算不足，但通过代际继承，可以在世代间积累学习适应。

Conclusion: 继承机制可以在不需要大规模学习预算的情况下，提升进化机器人的性能，为机器人设计提供高效路径。

Abstract: In evolutionary robotics, robot morphologies are designed automatically using evolutionary algorithms. This creates a body-brain optimization problem, where both morphology and control must be optimized together. A common approach is to include controller optimization for each morphology, but starting from scratch for every new body may require a high controller learning budget. We address this by using Bayesian optimization for controller optimization, exploiting its sample efficiency and strong exploration capabilities, and using sample inheritance as a form of Lamarckian inheritance. Under a deliberately low controller learning budget for each morphology, we investigate two types of sample inheritance: (1) transferring all the parent's samples to the offspring to be used as prior without evaluating them, and (2) reevaluating the parent's best samples on the offspring. Both are compared to a baseline without inheritance. Our results show that reevaluation performs best, with prior-based inheritance also outperforming no inheritance. Analysis reveals that while the learning budget is too low for a single morphology, generational inheritance compensates for this by accumulating learned adaptations across generations. Furthermore, inheritance mainly benefits offspring morphologies that are similar to their parents. Finally, we demonstrate the critical role of the environment, with more challenging environments resulting in more stable walking gaits. Our findings highlight that inheritance mechanisms can boost performance in evolutionary robotics without needing large learning budgets, offering an efficient path toward more capable robot design.

</details>


### [13] [Towards Safe Autonomous Driving: A Real-Time Motion Planning Algorithm on Embedded Hardware](https://arxiv.org/abs/2601.03904)
*Korbinian Moller,Glenn Johannes Tungka,Lucas Jürgens,Johannes Betz*

Main category: cs.RO

TL;DR: 本论文致力于提高自动驾驶汽车的安全性，提出了一种轻量级轨迹规划方法，在保证实时性的同时，确保在主规划器失效的情况下仍能安全运行。


<details>
  <summary>Details</summary>
Motivation: 确保自动驾驶汽车的功能安全需要能够在系统故障时保持可控性的运动规划模块。

Method: 在支持实时操作系统的汽车级嵌入式平台上部署了一种轻量级基于采样的轨迹规划器。

Result: 实验结果表明，轨迹规划在具备安全认证硬件上是可行的，表现出确定性的时间行为和有限的延迟。

Conclusion: 本研究突出了将主动后备机制集成到下一代安全框架中的潜力和挑战。

Abstract: Ensuring the functional safety of Autonomous Vehicles (AVs) requires motion planning modules that not only operate within strict real-time constraints but also maintain controllability in case of system faults. Existing safeguarding concepts, such as Online Verification (OV), provide safety layers that detect infeasible planning outputs. However, they lack an active mechanism to ensure safe operation in the event that the main planner fails. This paper presents a first step toward an active safety extension for fail-operational Autonomous Driving (AD). We deploy a lightweight sampling-based trajectory planner on an automotive-grade, embedded platform running a Real-Time Operating System (RTOS). The planner continuously computes trajectories under constrained computational resources, forming the foundation for future emergency planning architectures. Experimental results demonstrate deterministic timing behavior with bounded latency and minimal jitter, validating the feasibility of trajectory planning on safety-certifiable hardware. The study highlights both the potential and the remaining challenges of integrating active fallback mechanisms as an integral part of next-generation safeguarding frameworks. The code is available at: https://github.com/TUM-AVS/real-time-motion-planning

</details>


### [14] [An Event-Based Opto-Tactile Skin](https://arxiv.org/abs/2601.03907)
*Mohammadreza Koolani,Simeon Bamford,Petr Trunin,Simon F. Müller-Cleve,Matteo Lo Preti,Fulvio Mastrogiovanni,Lucia Beccai,Chiara Bartolozzi*

Main category: cs.RO

TL;DR: 本文提出了一种基于动态视觉传感器和柔性光波导皮肤的触觉传感系统，表现出高效的按压定位能力，并在极度减少数据情况下依然有效。


<details>
  <summary>Details</summary>
Motivation: 开发一种新型神经形态、事件驱动的触觉传感系统，增强柔性大面积皮肤的感知能力。

Method: 使用两台动态视觉传感器（DVS）相机，通过三角测量法和噪声应用的基于密度的空间聚类（DBSCAN）来估算按压力在2D皮肤表面的位置信息。

Result: 在4620 mm²的探测区域中，95%的压点的定位均方根误差（RMSE）为4.66 mm，且即使在极度减少事件数据下，平均定位误差仅增加至9.33 mm，仍能有效定位85%的按压试验。

Conclusion: 该触觉传感系统能够在减少事件数据的情况下保持功能，为未来实现低功耗和减轻计算负担的应用提供了可能性。

Abstract: This paper presents a neuromorphic, event-driven tactile sensing system for soft, large-area skin, based on the Dynamic Vision Sensors (DVS) integrated with a flexible silicone optical waveguide skin. Instead of repetitively scanning embedded photoreceivers, this design uses a stereo vision setup comprising two DVS cameras looking sideways through the skin. Such a design produces events as changes in brightness are detected, and estimates press positions on the 2D skin surface through triangulation, utilizing Density-Based Spatial Clustering of Applications with Noise (DBSCAN) to find the center of mass of contact events resulting from pressing actions. The system is evaluated over a 4620 mm2 probed area of the skin using a meander raster scan. Across 95 % of the presses visible to both cameras, the press localization achieved a Root-Mean-Squared Error (RMSE) of 4.66 mm. The results highlight the potential of this approach for wide-area flexible and responsive tactile sensors in soft robotics and interactive environments. Moreover, we examined how the system performs when the amount of event data is strongly reduced. Using stochastic down-sampling, the event stream was reduced to 1/1024 of its original size. Under this extreme reduction, the average localization error increased only slightly (from 4.66 mm to 9.33 mm), and the system still produced valid press localizations for 85 % of the trials. This reduction in pass rate is expected, as some presses no longer produce enough events to form a reliable cluster for triangulation. These results show that the sensing approach remains functional even with very sparse event data, which is promising for reducing power consumption and computational load in future implementations. The system exhibits a detection latency distribution with a characteristic width of 31 ms.

</details>


### [15] [CoINS: Counterfactual Interactive Navigation via Skill-Aware VLM](https://arxiv.org/abs/2601.03956)
*Kangjie Zhou,Zhejia Wen,Zhiyong Zhuo,Zike Yan,Pengying Wu,Ieng Hou U,Shuaiyang Li,Han Gao,Kang Ding,Wenhan Cao,Wei Pan,Chang Liu*

Main category: cs.RO

TL;DR: 本文提出CoINS框架，通过整合技能感知与反事实推理，显著提升机器人在复杂环境中的互动导航能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在机器人导航中缺乏对具体物理能力的理解，无法处理需要主动交互的复杂环境。

Method: 提出一种名为CoINS的框架，通过技能感知的推理与低级执行的结合来实现反事实互动导航。

Result: 该框架通过微调一个名为InterNav-VLM的视觉-语言模型，提高机器人在复杂环境中的互动导航能力。

Conclusion: CoINS显著优于现有基线，特别是在复杂长时间场景中成功率提高17%，表现出超过80%的改进。

Abstract: Recent Vision-Language Models (VLMs) have demonstrated significant potential in robotic planning. However, they typically function as semantic reasoners, lacking an intrinsic understanding of the specific robot's physical capabilities. This limitation is particularly critical in interactive navigation, where robots must actively modify cluttered environments to create traversable paths. Existing VLM-based navigators are predominantly confined to passive obstacle avoidance, failing to reason about when and how to interact with objects to clear blocked paths. To bridge this gap, we propose Counterfactual Interactive Navigation via Skill-aware VLM (CoINS), a hierarchical framework that integrates skill-aware reasoning and robust low-level execution. Specifically, we fine-tune a VLM, named InterNav-VLM, which incorporates skill affordance and concrete constraint parameters into the input context and grounds them into a metric-scale environmental representation. By internalizing the logic of counterfactual reasoning through fine-tuning on the proposed InterNav dataset, the model learns to implicitly evaluate the causal effects of object removal on navigation connectivity, thereby determining interaction necessity and target selection. To execute the generated high-level plans, we develop a comprehensive skill library through reinforcement learning, specifically introducing traversability-oriented strategies to manipulate diverse objects for path clearance. A systematic benchmark in Isaac Sim is proposed to evaluate both the reasoning and execution aspects of interactive navigation. Extensive simulations and real-world experiments demonstrate that CoINS significantly outperforms representative baselines, achieving a 17\% higher overall success rate and over 80\% improvement in complex long-horizon scenarios compared to the best-performing baseline

</details>


### [16] [Stable Language Guidance for Vision-Language-Action Models](https://arxiv.org/abs/2601.04052)
*Zhihao Zhan,Yuhao Chen,Jiaying Zhou,Qinhan Lv,Hao Liu,Keze Wang,Liang Lin,Guangrun Wang*

Main category: cs.RO

TL;DR: 残余语义引导（RSS）是一种通过解耦物理可供性和语义执行来增强视觉-语言-行动模型鲁棒性的概率框架。


<details>
  <summary>Details</summary>
Motivation: 我们识别出"模态崩溃"现象，其中强视觉先验覆盖了稀疏的语言信号，导致代理过拟合于特定指令表述。

Method: RSS采用了两种理论创新：1）蒙特卡罗句法整合，通过密集的LLM驱动的分布扩展来近似真实的语义后验；2）残余可供性引导，一种双流解码机制，通过减去视觉可供性先验来明确孤立语言的因果影响。

Result: 在多样化的操作基准测试中，RSS展示了卓越的鲁棒性，甚至在对抗性语言扰动下仍能保持性能。

Conclusion: 残余语义引导（RSS）有效提高了视觉-语言-行动模型在面对语言扰动时的鲁棒性。

Abstract: Vision-Language-Action (VLA) models have demonstrated impressive capabilities in generalized robotic control; however, they remain notoriously brittle to linguistic perturbations. We identify a critical ``modality collapse'' phenomenon where strong visual priors overwhelm sparse linguistic signals, causing agents to overfit to specific instruction phrasings while ignoring the underlying semantic intent. To address this, we propose \textbf{Residual Semantic Steering (RSS)}, a probabilistic framework that disentangles physical affordance from semantic execution. RSS introduces two theoretical innovations: (1) \textbf{Monte Carlo Syntactic Integration}, which approximates the true semantic posterior via dense, LLM-driven distributional expansion, and (2) \textbf{Residual Affordance Steering}, a dual-stream decoding mechanism that explicitly isolates the causal influence of language by subtracting the visual affordance prior. Theoretical analysis suggests that RSS effectively maximizes the mutual information between action and intent while suppressing visual distractors. Empirical results across diverse manipulation benchmarks demonstrate that RSS achieves state-of-the-art robustness, maintaining performance even under adversarial linguistic perturbations.

</details>


### [17] [CLAP: Contrastive Latent Action Pretraining for Learning Vision-Language-Action Models from Human Videos](https://arxiv.org/abs/2601.04061)
*Chubin Zhang,Jianan Wang,Zifeng Gao,Yue Su,Tianru Dai,Cai Zhou,Jiwen Lu,Yansong Tang*

Main category: cs.RO

TL;DR: 提出了一种对比潜在动作预训练框架，解决了视觉干扰问题，通过将视频数据与机器人轨迹对齐，提高了机器人操作技能。


<details>
  <summary>Details</summary>
Motivation: 当前，通用的视觉-语言-动作模型受到机器人数据稀缺的限制，因此探索如何有效利用视频数据至关重要。

Method: 通过对比学习，将视频的视觉潜在空间与机器人的本体潜在空间对齐，构建了一个双重模型CLAP-NTP和CLAP-RF。

Result: CLAP（对比潜在动作预训练）显著提高了从人类视频转移技能到机器人执行的能力。

Conclusion: CLAP通过对比学习有效地解决了机器人数据稀缺问题，并在多项实验中超越了强基线。

Abstract: Generalist Vision-Language-Action models are currently hindered by the scarcity of robotic data compared to the abundance of human video demonstrations. Existing Latent Action Models attempt to leverage video data but often suffer from visual entanglement, capturing noise rather than manipulation skills. To address this, we propose Contrastive Latent Action Pretraining (CLAP), a framework that aligns the visual latent space from videos with a proprioceptive latent space from robot trajectories. By employing contrastive learning, CLAP maps video transitions onto a quantized, physically executable codebook. Building on this representation, we introduce a dual-formulation VLA framework offering both CLAP-NTP, an autoregressive model excelling at instruction following and object generalization, and CLAP-RF, a Rectified Flow-based policy designed for high-frequency, precise manipulation. Furthermore, we propose a Knowledge Matching (KM) regularization strategy to mitigate catastrophic forgetting during fine-tuning. Extensive experiments demonstrate that CLAP significantly outperforms strong baselines, enabling the effective transfer of skills from human videos to robotic execution. Project page: https://lin-shan.com/CLAP/.

</details>


### [18] [Wow, wo, val! A Comprehensive Embodied World Model Evaluation Turing Test](https://arxiv.org/abs/2601.04137)
*Chun-Kai Fan,Xiaowei Chi,Xiaozhu Ju,Hao Li,Yong Bao,Yu-Kai Wang,Lizhang Chen,Zhiyuan Jiang,Kuangzhi Ge,Ying Li,Weishi Mi,Qingpo Wuwu,Peidong Jia,Yulin Luo,Kevin Zhang,Zhiyuan Qin,Yong Dai,Sirui Han,Yike Guo,Shanghang Zhang,Jian Tang*

Main category: cs.RO

TL;DR: 本文介绍了WoW-World-Eval基准，评估视频基础模型在体现AI中的有效性，并发现目前模型在长时间规划和物理一致性上的表现不足，强调了进一步研究的必要性。


<details>
  <summary>Details</summary>
Motivation: 随着体现在AI应用的增长，传统视频基础模型的有效性和可靠性并未得到充分验证，需要建立标准化框架以评估其在实际应用中的表现。

Method: 提出了包含22个指标的综合评估协议，并通过分析现有模型在长时间规划和物理一致性上的表现，揭示了视频生成与现实世界之间的差距。

Result: 本文提出了WoW-World-Eval基准，目的是评估视频基础模型在体现AI中的表现。通过对609个机器人操作数据的分析，发现当前模型在长时间规划和物理一致性上存在显著差距，展示了生成视频与现实世界之间的明显差距。

Conclusion: 尽管采用视频基础模型为体现在AI的任务提供了潜力，但在生成一致性和现实世界应用方面仍有待提升，迫切需要建立基准以解决这些问题。

Abstract: As world models gain momentum in Embodied AI, an increasing number of works explore using video foundation models as predictive world models for downstream embodied tasks like 3D prediction or interactive generation. However, before exploring these downstream tasks, video foundation models still have two critical questions unanswered: (1) whether their generative generalization is sufficient to maintain perceptual fidelity in the eyes of human observers, and (2) whether they are robust enough to serve as a universal prior for real-world embodied agents. To provide a standardized framework for answering these questions, we introduce the Embodied Turing Test benchmark: WoW-World-Eval (Wow,wo,val). Building upon 609 robot manipulation data, Wow-wo-val examines five core abilities, including perception, planning, prediction, generalization, and execution. We propose a comprehensive evaluation protocol with 22 metrics to assess the models' generation ability, which achieves a high Pearson Correlation between the overall score and human preference (>0.93) and establishes a reliable foundation for the Human Turing Test. On Wow-wo-val, models achieve only 17.27 on long-horizon planning and at best 68.02 on physical consistency, indicating limited spatiotemporal consistency and physical reasoning. For the Inverse Dynamic Model Turing Test, we first use an IDM to evaluate the video foundation models' execution accuracy in the real world. However, most models collapse to $\approx$ 0% success, while WoW maintains a 40.74% success rate. These findings point to a noticeable gap between the generated videos and the real world, highlighting the urgency and necessity of benchmarking World Model in Embodied AI.

</details>


### [19] [Hierarchical GNN-Based Multi-Agent Learning for Dynamic Queue-Jump Lane and Emergency Vehicle Corridor Formation](https://arxiv.org/abs/2601.04177)
*Haoran Su*

Main category: cs.RO

TL;DR: 提出了一种基于图神经网络的多智能体强化学习框架，以协调联网车辆形成紧急通道，显著提高紧急车辆通行效率。


<details>
  <summary>Details</summary>
Motivation: 现有策略无法适应动态条件，急需提高紧急车辆在拥堵交通中通过的效率。

Method: 采用分层图神经网络（GNN）和多智能体近端策略优化（MAPPO），通过高层规划和低层控制实现全球策略和轨迹执行的协调。

Result: 在仿真中，紧急车辆的通行时间减少了28.3%，与无协调交通相比减少了44.6%；碰撞率接近零（0.3%），同时保持81%的背景交通效率。

Conclusion: 将GNN与分层学习相结合有效提升了智能交通系统的性能，框架在多种场景下具有良好的鲁棒性。

Abstract: Emergency vehicles require rapid passage through congested traffic, yet existing strategies fail to adapt to dynamic conditions. We propose a novel hierarchical graph neural network (GNN)-based multi-agent reinforcement learning framework to coordinate connected vehicles for emergency corridor formation. Our approach uses a high-level planner for global strategy and low-level controllers for trajectory execution, utilizing graph attention networks to scale with variable agent counts. Trained via Multi-Agent Proximal Policy Optimization (MAPPO), the system reduces emergency vehicle travel time by 28.3% compared to baselines and 44.6% compared to uncoordinated traffic in simulations. The design achieves near-zero collision rates (0.3%) while maintaining 81% of background traffic efficiency. Ablation and generalization studies confirm the framework's robustness across diverse scenarios. These results demonstrate the effectiveness of combining GNNs with hierarchical learning for intelligent transportation systems.

</details>


### [20] [Embedding Autonomous Agents in Resource-Constrained Robotic Platforms](https://arxiv.org/abs/2601.04191)
*Negar Halakou,Juan F. Gutierrez,Ye Sun,Han Jiang,Xueming Wu,Yilun Song,Andres Gomez*

Main category: cs.RO

TL;DR: 本研究结合了自主代理与双轮机器人，实现了在动态环境中的自主决策，表现出在实时执行中的高效性。


<details>
  <summary>Details</summary>
Motivation: 嵌入式设备通常受到资源限制且处于动态环境中，因此需要具备独立决策的能力，以提升系统响应性和降低对外部控制的依赖。

Method: 将程序化的自主代理与双轮机器人集成，使其能够基于自身的决策和传感器数据探测迷宫。

Result: 通过集成使用AgentSpeak编程的自主代理与小型双轮机器人，在资源受限的嵌入式设备上实现自主决策，从而提高系统的响应能力并减少对外部控制的依赖。

Conclusion: 实验结果表明，该系统能够在资源受限的硬件上实时高效地执行自主决策，通过287个推理循环在59秒内成功解决了迷宫。

Abstract: Many embedded devices operate under resource constraints and in dynamic environments, requiring local decision-making capabilities. Enabling devices to make independent decisions in such environments can improve the responsiveness of the system and reduce the dependence on constant external control. In this work, we integrate an autonomous agent, programmed using AgentSpeak, with a small two-wheeled robot that explores a maze using its own decision-making and sensor data. Experimental results show that the agent successfully solved the maze in 59 seconds using 287 reasoning cycles, with decision phases taking less than one millisecond. These results indicate that the reasoning process is efficient enough for real-time execution on resource-constrained hardware. This integration demonstrates how high-level agent-based control can be applied to resource-constrained embedded systems for autonomous operation.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [21] [A Tool for Estimating Success Rates of Raycasting-Based Object Selection in Virtual Reality](https://arxiv.org/abs/2601.03522)
*Tatsuya Okuno,Haruto Shimizu,Nobuhito Kasahara,Taiyu Honma,Shota Yamanaka,Homei Miyashita*

Main category: cs.HC

TL;DR: 本论文提出了一种在Unity开发工具中估算对象选择成功率的系统，以帮助改进XR设备的用户界面设计。


<details>
  <summary>Details</summary>
Motivation: 随着XR设备的普及，3D交互日益普遍，UI开发者需要重视可用性，以提升用户体验。

Method: 提出了一种系统，能够在Unity开发工具中估算对象选择的成功率，并验证其理论基础，描述工具功能，收集VR开发者的反馈。

Result: 对VR开发者进行的反馈表明该工具在评估对象选择成功率时具有实用性。

Conclusion: 该工具为VR系统研发与研究之间架起了桥梁，促进了UI改善的实践应用。

Abstract: As XR devices become widespread, 3D interaction has become commonplace, and UI developers are increasingly required to consider usability to deliver better user experiences. The HCI community has long studied target-pointing performance, and research on 3D environments has progressed substantially. However, for practitioners to directly leverage research findings in UI improvements, practical tools are needed. To bridge this gap between research and development in VR systems, we propose a system that estimates object selection success rates within a development tool (Unity). In this paper, we validate the underlying theory, describe the tool's functions, and report feedback from VR developers who tried the tool to assess its usefulness.

</details>


### [22] [Dissolving a Digital Relationship: A Critical Examination of Digital Severance Behaviours in Close Relationships](https://arxiv.org/abs/2601.03551)
*Michael Yin,Angela Chiang,Robert Xiao*

Main category: cs.HC

TL;DR: 本研究探讨数字分离行为如何影响人际关系，尤其是在社交媒体环境下的权力与控制问题。


<details>
  <summary>Details</summary>
Motivation: 随着互动日益转向在线，理解数字关系中分离行为的广大影响势在必行。

Method: 通过对30名参与者的访谈，分析他们在数字分离中的体验与理解。

Result: 揭示数字分离的主题，包括个人愿望之间的不平衡，导致双方的无能感和模糊的失落感。

Conclusion: 研究强调数字分离与权力和控制的关系，并探讨了其对数字关系的实质影响。

Abstract: Fulfilling social connections are crucial for human well-being and belonging, but not all relationships last forever. As interactions increasingly move online, the act of digitally severing a relationship - e.g. through blocking or unfriending - has become progressively more common as well. This study considers actions of "digital severance" through interviews with 30 participants with experience as the initiator and/or recipient of such situations. Through a critical interpretative lens, we explore how people perceive and interpret their severance experience and how the online setting of social media shapes these dynamics. We develop themes that position digital severance as being intertwined with power and control, and we highlight (im)balances between an individual's desires that can lead to feelings of disempowerment and ambiguous loss for both parties. We discuss the implications of our research, outlining three key tensions and four open questions regarding digital relationships, meaning-making, and design outcomes for future exploration.

</details>


### [23] [AR Object Layout Method Using Miniature Room Generated from Depth Data](https://arxiv.org/abs/2601.03588)
*Keiichi Ihara,Ikkaku Kawaguchi*

Main category: cs.HC

TL;DR: 本研究探讨了在增强现实中使用世界缩小技术支持AR布局，通过实验验证了其有效性，显著降低了用户的工作负担，但总操作时间无显著差异。


<details>
  <summary>Details</summary>
Motivation: 由于在增强现实布局中用户难以自由改变虚拟对象的位置和大小，现有的技术难以满足需求，因此本研究探讨了WIM技术在AR中的应用。

Method: 本研究构建了一个系统，利用AR设备的深度传感器实时获取房间网格，创建和更新房间的微型模型，用户可以利用微型物体移动和缩放虚拟对象。通过实验评估用户在设计AR布局时的工作负担。

Result: 本研究证明了在增强现实中应用世界缩小（WIM）技术可以有效支持AR布局，显著降低用户在物理和时间方面的工作负担。

Conclusion: 虽然总操作时间没有显著差异，但使用WIM技术确实减轻了用户在操作虚拟对象时的负担，提升了用户体验。

Abstract: In augmented reality (AR), users can place virtual objects anywhere in a real-world room, called AR layout. Although several object manipulation techniques have been proposed in AR, it is difficult to use them for AR layout owing to the difficulty in freely changing the position and size of virtual objects. In this study, we make the World-in-Miniature (WIM) technique available in AR to support AR layout. The WIM technique is a manipulation technique that uses miniatures, which has been proposed as a manipulation technique for virtual reality (VR). Our system uses the AR device's depth sensors to acquire a mesh of the room in real-time to create and update a miniature of a room in real-time. In our system, users can use miniature objects to move virtual objects to arbitrary positions and scale them to arbitrary sizes. In addition, because the miniature object can be manipulated instead of the real-scale object, we assumed that our system will shorten the placement time and reduce the workload of the user. In our previous study, we created a prototype and investigated the properties of manipulating miniature objects in AR. In this study, we conducted an experiment to evaluate how our system can support AR layout. To conduct a task close to the actual use, we used various objects and made the participants design an AR layout of their own will. The results showed that our system significantly reduced workload in physical and temporal demand. Although, there was no significant difference in the total manipulation time.

</details>


### [24] [Beyond Physical Labels: Redefining Domains for Robust WiFi-based Gesture Recognition](https://arxiv.org/abs/2601.03825)
*Xiang Zhang,Huan Yan,Jinyang Huang,Bin Liu,Yuanhao Feng,Jianchun Liu,Meng Li,Fusang Zhang,Zhi Liu*

Main category: cs.HC

TL;DR: 本文提出了一种新的WiFi手势识别系统GesFi，通过WiFi潜在域挖掘自我重新定义领域，实现稳健的跨域泛化。


<details>
  <summary>Details</summary>
Motivation: 为了应对频繁变化的环境和分布的变化，改进手势识别系统的鲁棒性。

Method: GesFi系统处理WiFi接收器收集的原始传感数据，使用CSI-ratio降噪、短时快速傅里叶变换和可视化技术生成标准化输入表示，并通过类的对抗学习来抑制手势语义，同时利用无监督聚类自动发现负责分布变化的潜在域因素，这些潜在域通过对抗学习进行对齐以支持稳健的跨域泛化。

Result: GesFi在多个公共数据集和真实世界环境中的表现优于现有对抗方法，性能提升高达78%和50%。

Conclusion: GesFi系统在多对和单对设置中表现优越，能够实现稳健的手势推断，并在大多数跨域任务中超越了之前的泛化方法。

Abstract: In this paper, we propose GesFi, a novel WiFi-based gesture recognition system that introduces WiFi latent domain mining to redefine domains directly from the data itself. GesFi first processes raw sensing data collected from WiFi receivers using CSI-ratio denoising, Short-Time Fast Fourier Transform, and visualization techniques to generate standardized input representations. It then employs class-wise adversarial learning to suppress gesture semantic and leverages unsupervised clustering to automatically uncover latent domain factors responsible for distributional shifts. These latent domains are then aligned through adversarial learning to support robust cross-domain generalization. Finally, the system is applied to the target environment for robust gesture inference. We deployed GesFi under both single-pair and multi-pair settings using commodity WiFi transceivers, and evaluated it across multiple public datasets and real-world environments. Compared to state-of-the-art baselines, GesFi achieves up to 78% and 50% performance improvements over existing adversarial methods, and consistently outperforms prior generalization approaches across most cross-domain tasks.

</details>
