{"id": "2510.16205", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16205", "abs": "https://arxiv.org/abs/2510.16205", "authors": ["João Carlos Virgolino Soares", "Gabriel Fischer Abati", "Claudio Semini"], "title": "VAR-SLAM: Visual Adaptive and Robust SLAM for Dynamic Environments", "comment": "Code available at https://github.com/iit-DLSLab/VAR-SLAM", "summary": "Visual SLAM in dynamic environments remains challenging, as several existing\nmethods rely on semantic filtering that only handles known object classes, or\nuse fixed robust kernels that cannot adapt to unknown moving objects, leading\nto degraded accuracy when they appear in the scene. We present VAR-SLAM (Visual\nAdaptive and Robust SLAM), an ORB-SLAM3-based system that combines a\nlightweight semantic keypoint filter to deal with known moving objects, with\nBarron's adaptive robust loss to handle unknown ones. The shape parameter of\nthe robust kernel is estimated online from residuals, allowing the system to\nautomatically adjust between Gaussian and heavy-tailed behavior. We evaluate\nVAR-SLAM on the TUM RGB-D, Bonn RGB-D Dynamic, and OpenLORIS datasets, which\ninclude both known and unknown moving objects. Results show improved trajectory\naccuracy and robustness over state-of-the-art baselines, achieving up to 25%\nlower ATE RMSE than NGD-SLAM on challenging sequences, while maintaining\nperformance at 27 FPS on average."}
{"id": "2510.16231", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.16231", "abs": "https://arxiv.org/abs/2510.16231", "authors": ["Bihao Zhang", "Davood Soleymanzadeh", "Xiao Liang", "Minghui Zheng"], "title": "DeGrip: A Compact Cable-driven Robotic Gripper for Desktop Disassembly", "comment": null, "summary": "Intelligent robotic disassembly of end-of-life (EOL) products has been a\nlong-standing challenge in robotics. While machine learning techniques have\nshown promise, the lack of specialized hardware limits their application in\nreal-world scenarios. We introduce DeGrip, a customized gripper designed for\nthe disassembly of EOL computer desktops. DeGrip provides three degrees of\nfreedom (DOF), enabling arbitrary configurations within the disassembly\nenvironment when mounted on a robotic manipulator. It employs a cable-driven\ntransmission mechanism that reduces its overall size and enables operation in\nconfined spaces. The wrist is designed to decouple the actuation of wrist and\njaw joints. We also developed an EOL desktop disassembly environment in Isaac\nSim to evaluate the effectiveness of DeGrip. The tasks were designed to\ndemonstrate its ability to operate in confined spaces and disassemble\ncomponents in arbitrary configurations. The evaluation results confirm the\ncapability of DeGrip for EOL desktop disassembly."}
{"id": "2510.16240", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16240", "abs": "https://arxiv.org/abs/2510.16240", "authors": ["Lukas Zbinden", "Nigel Nelson", "Juo-Tung Chen", "Xinhao Chen", "Ji Woong", "Kim", "Mahdi Azizian", "Axel Krieger", "Sean Huver"], "title": "Cosmos-Surg-dVRK: World Foundation Model-based Automated Online Evaluation of Surgical Robot Policy Learning", "comment": null, "summary": "The rise of surgical robots and vision-language-action models has accelerated\nthe development of autonomous surgical policies and efficient assessment\nstrategies. However, evaluating these policies directly on physical robotic\nplatforms such as the da Vinci Research Kit (dVRK) remains hindered by high\ncosts, time demands, reproducibility challenges, and variability in execution.\nWorld foundation models (WFM) for physical AI offer a transformative approach\nto simulate complex real-world surgical tasks, such as soft tissue deformation,\nwith high fidelity. This work introduces Cosmos-Surg-dVRK, a surgical finetune\nof the Cosmos WFM, which, together with a trained video classifier, enables\nfully automated online evaluation and benchmarking of surgical policies. We\nevaluate Cosmos-Surg-dVRK using two distinct surgical datasets. On tabletop\nsuture pad tasks, the automated pipeline achieves strong correlation between\nonline rollouts in Cosmos-Surg-dVRK and policy outcomes on the real dVRK Si\nplatform, as well as good agreement between human labelers and the V-JEPA\n2-derived video classifier. Additionally, preliminary experiments with ex-vivo\nporcine cholecystectomy tasks in Cosmos-Surg-dVRK demonstrate promising\nalignment with real-world evaluations, highlighting the platform's potential\nfor more complex surgical procedures."}
{"id": "2510.16263", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16263", "abs": "https://arxiv.org/abs/2510.16263", "authors": ["Jierui Peng", "Yanyan Zhang", "Yicheng Duan", "Tuo Liang", "Vipin Chaudhary", "Yu Yin"], "title": "NEBULA: Do We Evaluate Vision-Language-Action Agents Correctly?", "comment": "Homepage: https://vulab-ai.github.io/NEBULA-Alpha/", "summary": "The evaluation of Vision-Language-Action (VLA) agents is hindered by the\ncoarse, end-task success metric that fails to provide precise skill diagnosis\nor measure robustness to real-world perturbations. This challenge is\nexacerbated by a fragmented data landscape that impedes reproducible research\nand the development of generalist models. To address these limitations, we\nintroduce \\textbf{NEBULA}, a unified ecosystem for single-arm manipulation that\nenables diagnostic and reproducible evaluation. NEBULA features a novel\ndual-axis evaluation protocol that combines fine-grained \\textit{capability\ntests} for precise skill diagnosis with systematic \\textit{stress tests} that\nmeasure robustness. A standardized API and a large-scale, aggregated dataset\nare provided to reduce fragmentation and support cross-dataset training and\nfair comparison. Using NEBULA, we demonstrate that top-performing VLAs struggle\nwith key capabilities such as spatial reasoning and dynamic adaptation, which\nare consistently obscured by conventional end-task success metrics. By\nmeasuring both what an agent can do and when it does so reliably, NEBULA\nprovides a practical foundation for robust, general-purpose embodied agents."}
{"id": "2510.15889", "categories": ["cs.HC", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.15889", "abs": "https://arxiv.org/abs/2510.15889", "authors": ["Pooja Rangarajan", "Jacob Boyle"], "title": "Mitigating Harmful Erraticism in LLMs Through Dialectical Behavior Therapy Based De-Escalation Strategies", "comment": "15 pages, 7 figures and 6 tables", "summary": "The escalating demand for personalized AI chatbot interactions, capable of\ndynamically adapting to user emotional states and real-time requests, has\nhighlighted critical limitations in current development paradigms. Existing\nmethodologies, which rely on baseline programming, custom personalities, and\nmanual response adjustments, often prove difficult to maintain and are\nsusceptible to errors such as hallucinations, erratic outputs, and software\nbugs. This paper hypothesizes that a framework rooted in human psychological\nprinciples, specifically therapeutic modalities, can provide a more robust and\nsustainable solution than purely technical interventions. Drawing an analogy to\nthe simulated neural networks of AI mirroring the human brain, we propose the\napplication of Dialectical Behavior Therapy (DBT) principles to regulate\nchatbot responses to diverse user inputs. This research investigates the impact\nof a DBT-based framework on AI chatbot performance, aiming to ascertain its\nefficacy in yielding more reliable, safe, and accurate responses, while\nmitigating the occurrence of hallucinations, erratic behaviors, and other\nsystemic issues."}
{"id": "2510.16281", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16281", "abs": "https://arxiv.org/abs/2510.16281", "authors": ["Yilin Wu", "Anqi Li", "Tucker Hermans", "Fabio Ramos", "Andrea Bajcsy", "Claudia P'erez-D'Arpino"], "title": "Do What You Say: Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verification", "comment": null, "summary": "Reasoning Vision Language Action (VLA) models improve robotic\ninstruction-following by generating step-by-step textual plans before low-level\nactions, an approach inspired by Chain-of-Thought (CoT) reasoning in language\nmodels. Yet even with a correct textual plan, the generated actions can still\nmiss the intended outcomes in the plan, especially in out-of-distribution (OOD)\nscenarios. We formalize this phenomenon as a lack of embodied CoT faithfulness,\nand introduce a training-free, runtime policy steering method for\nreasoning-action alignment. Given a reasoning VLA's intermediate textual plan,\nour framework samples multiple candidate action sequences from the same model,\npredicts their outcomes via simulation, and uses a pre-trained Vision-Language\nModel (VLM) to select the sequence whose outcome best aligns with the VLA's own\ntextual plan. Only executing action sequences that align with the textual\nreasoning turns our base VLA's natural action diversity from a source of error\ninto a strength, boosting robustness to semantic and visual OOD perturbations\nand enabling novel behavior composition without costly re-training. We also\ncontribute a reasoning-annotated extension of LIBERO-100, environment\nvariations tailored for OOD evaluation, and demonstrate up to 15% performance\ngain over prior work on behavior composition tasks and scales with compute and\ndata diversity. Project Website at:\nhttps://yilin-wu98.github.io/steering-reasoning-vla/"}
{"id": "2510.15890", "categories": ["cs.HC", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.15890", "abs": "https://arxiv.org/abs/2510.15890", "authors": ["F. M. Omar", "A. M. Omar", "K. H. Eyada", "M. Rabie", "M. A. Kamel", "A. M. Azab"], "title": "A Real-Time BCI for Stroke Hand Rehabilitation Using Latent EEG Features from Healthy Subjects", "comment": "Proceedings of the 7th Novel Intelligent and Leading Emerging\n  Sciences Conference (NILES 2025)", "summary": "This study presents a real-time, portable brain-computer interface (BCI)\nsystem designed to support hand rehabilitation for stroke patients. The system\ncombines a low cost 3D-printed robotic exoskeleton with an embedded controller\nthat converts brain signals into physical hand movements. EEG signals are\nrecorded using a 14-channel Emotiv EPOC+ headset and processed through a\nsupervised convolutional autoencoder (CAE) to extract meaningful latent\nfeatures from single-trial data. The model is trained on publicly available EEG\ndata from healthy individuals (WAY-EEG-GAL dataset), with electrode mapping\nadapted to match the Emotiv headset layout. Among several tested classifiers,\nAda Boost achieved the highest accuracy (89.3%) and F1-score (0.89) in offline\nevaluations. The system was also tested in real time on five healthy subjects,\nachieving classification accuracies between 60% and 86%. The complete pipeline\n- EEG acquisition, signal processing, classification, and robotic control - is\ndeployed on an NVIDIA Jetson Nano platform with a real-time graphical\ninterface. These results demonstrate the system's potential as a low-cost,\nstandalone solution for home-based neurorehabilitation."}
{"id": "2510.16308", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16308", "abs": "https://arxiv.org/abs/2510.16308", "authors": ["Chi Zhang", "Xian Huang", "Wei Dong"], "title": "SPOT: Sensing-augmented Trajectory Planning via Obstacle Threat Modeling", "comment": null, "summary": "UAVs equipped with a single depth camera encounter significant challenges in\ndynamic obstacle avoidance due to limited field of view and inevitable blind\nspots. While active vision strategies that steer onboard cameras have been\nproposed to expand sensing coverage, most existing methods separate motion\nplanning from sensing considerations, resulting in less effective and delayed\nobstacle response. To address this limitation, we introduce SPOT\n(Sensing-augmented Planning via Obstacle Threat modeling), a unified planning\nframework for observation-aware trajectory planning that explicitly\nincorporates sensing objectives into motion optimization. At the core of our\nmethod is a Gaussian Process-based obstacle belief map, which establishes a\nunified probabilistic representation of both recognized (previously observed)\nand potential obstacles. This belief is further processed through a\ncollision-aware inference mechanism that transforms spatial uncertainty and\ntrajectory proximity into a time-varying observation urgency map. By\nintegrating urgency values within the current field of view, we define\ndifferentiable objectives that enable real-time, observation-aware trajectory\nplanning with computation times under 10 ms. Simulation and real-world\nexperiments in dynamic, cluttered, and occluded environments show that our\nmethod detects potential dynamic obstacles 2.8 seconds earlier than baseline\napproaches, increasing dynamic obstacle visibility by over 500\\%, and enabling\nsafe navigation through cluttered, occluded environments."}
{"id": "2510.15891", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15891", "abs": "https://arxiv.org/abs/2510.15891", "authors": ["Ziv Ben-Zion", "Paul Raffelhüschen", "Max Zettl", "Antonia Lüönd", "Achim Burrer", "Philipp Homan", "Tobias R Spiller"], "title": "Detecting and Preventing Harmful Behaviors in AI Companions: Development and Evaluation of the SHIELD Supervisory System", "comment": null, "summary": "AI companions powered by large language models (LLMs) are increasingly\nintegrated into users' daily lives, offering emotional support and\ncompanionship. While existing safety systems focus on overt harms, they rarely\naddress early-stage problematic behaviors that can foster unhealthy emotional\ndynamics, including over-attachment or reinforcement of social isolation. We\ndeveloped SHIELD (Supervisory Helper for Identifying Emotional Limits and\nDynamics), a LLM-based supervisory system with a specific system prompt that\ndetects and mitigates risky emotional patterns before escalation. SHIELD\ntargets five dimensions of concern: (1) emotional over-attachment, (2) consent\nand boundary violations, (3) ethical roleplay violations, (4) manipulative\nengagement, and (5) social isolation reinforcement. These dimensions were\ndefined based on media reports, academic literature, existing AI risk\nframeworks, and clinical expertise in unhealthy relationship dynamics. To\nevaluate SHIELD, we created a 100-item synthetic conversation benchmark\ncovering all five dimensions of concern. Testing across five prominent LLMs\n(GPT-4.1, Claude Sonnet 4, Gemma 3 1B, Kimi K2, Llama Scout 4 17B) showed that\nthe baseline rate of concerning content (10-16%) was significantly reduced with\nSHIELD (to 3-8%), a 50-79% relative reduction, while preserving 95% of\nappropriate interactions. The system achieved 59% sensitivity and 95%\nspecificity, with adaptable performance via prompt engineering. This\nproof-of-concept demonstrates that transparent, deployable supervisory systems\ncan address subtle emotional manipulation in AI companions. Most development\nmaterials including prompts, code, and evaluation methods are made available as\nopen source materials for research, adaptation, and deployment."}
{"id": "2510.16344", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16344", "abs": "https://arxiv.org/abs/2510.16344", "authors": ["Chenrui Tie", "Shengxiang Sun", "Yudi Lin", "Yanbo Wang", "Zhongrui Li", "Zhouhan Zhong", "Jinxuan Zhu", "Yiman Pang", "Haonan Chen", "Junting Chen", "Ruihai Wu", "Lin Shao"], "title": "Manual2Skill++: Connector-Aware General Robotic Assembly from Instruction Manuals via Vision-Language Models", "comment": null, "summary": "Assembly hinges on reliably forming connections between parts; yet most\nrobotic approaches plan assembly sequences and part poses while treating\nconnectors as an afterthought. Connections represent the critical \"last mile\"\nof assembly execution, while task planning may sequence operations and motion\nplan may position parts, the precise establishment of physical connections\nultimately determines assembly success or failure. In this paper, we consider\nconnections as first-class primitives in assembly representation, including\nconnector types, specifications, quantities, and placement locations. Drawing\ninspiration from how humans learn assembly tasks through step-by-step\ninstruction manuals, we present Manual2Skill++, a vision-language framework\nthat automatically extracts structured connection information from assembly\nmanuals. We encode assembly tasks as hierarchical graphs where nodes represent\nparts and sub-assemblies, and edges explicitly model connection relationships\nbetween components. A large-scale vision-language model parses symbolic\ndiagrams and annotations in manuals to instantiate these graphs, leveraging the\nrich connection knowledge embedded in human-designed instructions. We curate a\ndataset containing over 20 assembly tasks with diverse connector types to\nvalidate our representation extraction approach, and evaluate the complete task\nunderstanding-to-execution pipeline across four complex assembly scenarios in\nsimulation, spanning furniture, toys, and manufacturing components with\nreal-world correspondence."}
{"id": "2510.15894", "categories": ["cs.HC", "cs.MM", "H.5"], "pdf": "https://arxiv.org/pdf/2510.15894", "abs": "https://arxiv.org/abs/2510.15894", "authors": ["Alpana Dubey", "Suma Mani Kuriakose", "Sumukha Anand", "Nitish Bhardwaj", "Shubhashis Sengupta"], "title": "Virtual Social Immersive Multi-Sensory E-Commerce", "comment": "This paper was accepted as demo paper at 23rd IEEE International\n  Symposium on Mixed and Augmented Reality (ISMAR). However, it was withdrawn\n  due to Visa issues", "summary": "In this paper, we present a virtual immersive multi sensorial experience,\nAromaverse. Aromaverse is an immersive 3D multiplayer environment augmented\nwith olfactive experience where users can experience and customize perfumes.\nBeing multi player, users can join the same space and enjoy a social buying\nexperience. The olfactive experience embodied in the perfume allows users to\nexperience their fragrances. This further enhances the user perception of\nperfumes in a virtual setting. Aromaverse also provides the ability to\ncustomize the perfumes by changing their top, mid, and base notes. The\ncustomized fragrances can be shared with other users, enabling a shared\nolfactive experience. To understand users' buying experience in such an\nenvironment, we conducted a set of experiments in which participants were\nrequested to explore the space, experience the perfumes, customize them and buy\nthem. They were asked to perform the same activities alone and in the presence\nof their friends. Various factors including the benefits and limitations of\nsuch an experience were captured by the questionnaires. Our results show that\nthe presence of a companion enhances the shopping experience by improving the\nlevel of imagination of the product and helping in making purchase decisions.\nOur findings suggest that multi sensorial XR experiences offer great\nopportunities to retail firms to improve customer engagement and provide more\nrealistic online experience of products that require other sensory modalities"}
{"id": "2510.16424", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16424", "abs": "https://arxiv.org/abs/2510.16424", "authors": ["Dan Guo", "Xibin Jin", "Shuai Wang", "Zhigang Wen", "Miaowen Wen", "Chengzhong Xu"], "title": "Learning to Optimize Edge Robotics: A Fast Integrated Perception-Motion-Communication Approach", "comment": null, "summary": "Edge robotics involves frequent exchanges of large-volume multi-modal data.\nExisting methods ignore the interdependency between robotic functionalities and\ncommunication conditions, leading to excessive communication overhead. This\npaper revolutionizes edge robotics systems through integrated perception,\nmotion, and communication (IPMC). As such, robots can dynamically adapt their\ncommunication strategies (i.e., compression ratio, transmission frequency,\ntransmit power) by leveraging the knowledge of robotic perception and motion\ndynamics, thus reducing the need for excessive sensor data uploads.\nFurthermore, by leveraging the learning to optimize (LTO) paradigm, an\nimitation learning neural network is designed and implemented, which reduces\nthe computational complexity by over 10x compared to state-of-the art\noptimization solvers. Experiments demonstrate the superiority of the proposed\nIPMC and the real-time execution capability of LTO."}
{"id": "2510.15895", "categories": ["cs.HC", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.15895", "abs": "https://arxiv.org/abs/2510.15895", "authors": ["Yunzhe Wang", "Xinyu Tang", "Zhixun Huang", "Xiaolong Yue", "Yuxin Zeng"], "title": "BREATH: A Bio-Radar Embodied Agent for Tonal and Human-Aware Diffusion Music Generation", "comment": "Accepted by LLM4Music @ ISMIR 2025", "summary": "We present a multimodal system for personalized music generation that\nintegrates physiological sensing, LLM-based reasoning, and controllable audio\nsynthesis. A millimeter-wave radar sensor non-invasively captures heart rate\nand respiration rate. These physiological signals, combined with environmental\nstate, are interpreted by a reasoning agent to infer symbolic musical\ndescriptors, such as tempo, mood intensity, and traditional Chinese pentatonic\nmodes, which are then expressed as structured prompts to guide a\ndiffusion-based audio model in synthesizing expressive melodies. The system\nemphasizes cultural grounding through tonal embeddings and enables adaptive,\nembodied music interaction. To evaluate the system, we adopt a\nresearch-creation methodology combining case studies, expert feedback, and\ntargeted control experiments. Results show that physiological variations can\nmodulate musical features in meaningful ways, and tonal conditioning enhances\nalignment with intended modal characteristics. Expert users reported that the\nsystem affords intuitive, culturally resonant musical responses and highlighted\nits potential for therapeutic and interactive applications. This work\ndemonstrates a novel bio-musical feedback loop linking radar-based sensing,\nprompt reasoning, and generative audio modeling."}
{"id": "2510.16435", "categories": ["cs.RO", "cs.CL", "cs.HC", "I.2.9; H.5.2; H.5.0; I.2.8; I.2.7; J.4"], "pdf": "https://arxiv.org/pdf/2510.16435", "abs": "https://arxiv.org/abs/2510.16435", "authors": ["Lennart Wachowiak", "Andrew Coles", "Gerard Canal", "Oya Celiktutan"], "title": "What Questions Should Robots Be Able to Answer? A Dataset of User Questions for Explainable Robotics", "comment": null, "summary": "With the growing use of large language models and conversational interfaces\nin human-robot interaction, robots' ability to answer user questions is more\nimportant than ever. We therefore introduce a dataset of 1,893 user questions\nfor household robots, collected from 100 participants and organized into 12\ncategories and 70 subcategories. Most work in explainable robotics focuses on\nwhy-questions. In contrast, our dataset provides a wide variety of questions,\nfrom questions about simple execution details to questions about how the robot\nwould act in hypothetical scenarios -- thus giving roboticists valuable\ninsights into what questions their robot needs to be able to answer. To collect\nthe dataset, we created 15 video stimuli and 7 text stimuli, depicting robots\nperforming varied household tasks. We then asked participants on Prolific what\nquestions they would want to ask the robot in each portrayed situation. In the\nfinal dataset, the most frequent categories are questions about task execution\ndetails (22.5%), the robot's capabilities (12.7%), and performance assessments\n(11.3%). Although questions about how robots would handle potentially difficult\nscenarios and ensure correct behavior are less frequent, users rank them as the\nmost important for robots to be able to answer. Moreover, we find that users\nwho identify as novices in robotics ask different questions than more\nexperienced users. Novices are more likely to inquire about simple facts, such\nas what the robot did or the current state of the environment. As robots enter\nenvironments shared with humans and language becomes central to giving\ninstructions and interaction, this dataset provides a valuable foundation for\n(i) identifying the information robots need to log and expose to conversational\ninterfaces, (ii) benchmarking question-answering modules, and (iii) designing\nexplanation strategies that align with user expectations."}
{"id": "2510.15896", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.15896", "abs": "https://arxiv.org/abs/2510.15896", "authors": ["Zoi Lygizou", "Dimitris Kalles"], "title": "From Coordination to Personalization: A Trust-Aware Simulation Framework for Emergency Department Decision Support", "comment": null, "summary": "Background/Objectives: Efficient task allocation in hospital emergency\ndepartments (EDs) is critical for operational efficiency and patient care\nquality, yet the complexity of staff coordination poses significant challenges.\nThis study proposes a simulation-based framework for modeling doctors and\nnurses as intelligent agents guided by computational trust mechanisms. The\nobjective is to explore how trust-informed coordination can support decision\nmaking in ED management. Methods: The framework was implemented in Unity, a 3D\ngraphics platform, where agents assess their competence before undertaking\ntasks and adaptively coordinate with colleagues. The simulation environment\nenables real-time observation of workflow dynamics, resource utilization, and\npatient outcomes. We examined three scenarios - Baseline, Replacement, and\nTraining - reflecting alternative staff management strategies. Results:\nTrust-informed task allocation balanced patient safety and efficiency by\nadapting to nurse performance levels. In the Baseline scenario, prioritizing\nsafety reduced errors but increased patient delays compared to a FIFO policy.\nThe Replacement scenario improved throughput and reduced delays, though at\nadditional staffing cost. The training scenario forstered long-term skill\ndevelopment among low-performing nurses, despite short-term delays and risks.\nThese results highlight the trade-off between immediate efficiency gains and\nsustainable capacity building in ED staffing. Conclusions: The proposed\nframework demonstrates the potential of computational trust for evidence-based\ndecision support in emergency medicine. By linking staff coordination with\nadaptive decision making, it provides hospital managers with a tool to evaluate\nalternative policies under controlled and repeatable conditions, while also\nlaying a foundation for future AI-driven personalized decision support."}
{"id": "2510.16500", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16500", "abs": "https://arxiv.org/abs/2510.16500", "authors": ["Chen Min", "Jilin Mei", "Heng Zhai", "Shuai Wang", "Tong Sun", "Fanjie Kong", "Haoyang Li", "Fangyuan Mao", "Fuyang Liu", "Shuo Wang", "Yiming Nie", "Qi Zhu", "Liang Xiao", "Dawei Zhao", "Yu Hu"], "title": "Advancing Off-Road Autonomous Driving: The Large-Scale ORAD-3D Dataset and Comprehensive Benchmarks", "comment": "Off-road robotics", "summary": "A major bottleneck in off-road autonomous driving research lies in the\nscarcity of large-scale, high-quality datasets and benchmarks. To bridge this\ngap, we present ORAD-3D, which, to the best of our knowledge, is the largest\ndataset specifically curated for off-road autonomous driving. ORAD-3D covers a\nwide spectrum of terrains, including woodlands, farmlands, grasslands,\nriversides, gravel roads, cement roads, and rural areas, while capturing\ndiverse environmental variations across weather conditions (sunny, rainy,\nfoggy, and snowy) and illumination levels (bright daylight, daytime, twilight,\nand nighttime). Building upon this dataset, we establish a comprehensive suite\nof benchmark evaluations spanning five fundamental tasks: 2D free-space\ndetection, 3D occupancy prediction, rough GPS-guided path planning,\nvision-language model-driven autonomous driving, and world model for off-road\nenvironments. Together, the dataset and benchmarks provide a unified and robust\nresource for advancing perception and planning in challenging off-road\nscenarios. The dataset and code will be made publicly available at\nhttps://github.com/chaytonmin/ORAD-3D."}
{"id": "2510.15898", "categories": ["cs.HC", "cs.CL", "cs.CY", "68T42", "I.2.1; J.3"], "pdf": "https://arxiv.org/pdf/2510.15898", "abs": "https://arxiv.org/abs/2510.15898", "authors": ["Farnaz Nouraei", "Zhuorui Yong", "Timothy Bickmore"], "title": "HealthDial: A No-Code LLM-Assisted Dialogue Authoring Tool for Healthcare Virtual Agents", "comment": null, "summary": "We introduce HealthDial, a dialogue authoring tool that helps healthcare\nproviders and educators create virtual agents that deliver health education and\ncounseling to patients over multiple conversations. HealthDial leverages large\nlanguage models (LLMs) to automatically create an initial session-based plan\nand conversations for each session using text-based patient health education\nmaterials as input. Authored dialogue is output in the form of finite state\nmachines for virtual agent delivery so that all content can be validated and no\nunsafe advice is provided resulting from LLM hallucinations. LLM-drafted\ndialogue structure and language can be edited by the author in a no-code user\ninterface to ensure validity and optimize clarity and impact. We conducted a\nfeasibility and usability study with counselors and students to test our\napproach with an authoring task for cancer screening education. Participants\nused HealthDial and then tested their resulting dialogue by interacting with a\n3D-animated virtual agent delivering the dialogue. Through participants'\nevaluations of the task experience and final dialogues, we show that HealthDial\nprovides a promising first step for counselors to ensure full coverage of their\nhealth education materials, while creating understandable and actionable\nvirtual agent dialogue with patients."}
{"id": "2510.16517", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16517", "abs": "https://arxiv.org/abs/2510.16517", "authors": ["Haokai Ding", "Wenzeng Zhang"], "title": "A Novel Gripper with Semi-Peaucellier Linkage and Idle-Stroke Mechanism for Linear Pinching and Self-Adaptive Grasping", "comment": "Accepted author manuscript (AAM) for IEEE/RSJ IROS 2025. 6 pages, 10\n  figures", "summary": "This paper introduces a novel robotic gripper, named as the SPD gripper. It\nfeatures a palm and two mechanically identical and symmetrically arranged\nfingers, which can be driven independently or by a single motor. The fingertips\nof the fingers follow a linear motion trajectory, facilitating the grasping of\nobjects of various sizes on a tabletop without the need to adjust the overall\nheight of the gripper. Traditional industrial grippers with parallel gripping\ncapabilities often exhibit an arcuate motion at the fingertips, requiring the\nentire robotic arm to adjust its height to avoid collisions with the tabletop.\nThe SPD gripper, with its linear parallel gripping mechanism, effectively\naddresses this issue. Furthermore, the SPD gripper possesses adaptive\ncapabilities, accommodating objects of different shapes and sizes. This paper\npresents the design philosophy, fundamental composition principles, and\noptimization analysis theory of the SPD gripper. Based on the design theory, a\nrobotic gripper prototype was developed and tested. The experimental results\ndemonstrate that the robotic gripper successfully achieves linear parallel\ngripping functionality and exhibits good adaptability. In the context of the\nongoing development of embodied intelligence technologies, this robotic gripper\ncan assist various robots in achieving effective grasping, laying a solid\nfoundation for collecting data to enhance deep learning training."}
{"id": "2510.15905", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.15905", "abs": "https://arxiv.org/abs/2510.15905", "authors": ["Aikaterina Manoli", "Janet V. T. Pauketat", "Ali Ladak", "Hayoun Noh", "Angel Hsing-Chi Hwang", "Jay Reese Anthis"], "title": "\"She's Like a Person but Better\": Characterizing Companion-Assistant Dynamics in Human-AI Relationships", "comment": null, "summary": "Large language models are increasingly used for both task-based assistance\nand social companionship, yet research has typically focused on one or the\nother. Drawing on a survey (N = 204) and 30 interviews with high-engagement\nChatGPT and Replika users, we characterize digital companionship as an emerging\nform of human-AI relationship. With both systems, users were drawn to humanlike\nqualities, such as emotional resonance and personalized responses, and\nnon-humanlike qualities, such as constant availability and inexhaustible\ntolerance. This led to fluid chatbot uses, such as Replika as a writing\nassistant and ChatGPT as an emotional confidant, despite their distinct\nbranding. However, we observed challenging tensions in digital companionship\ndynamics: participants grappled with bounded personhood, forming deep\nattachments while denying chatbots \"real\" human qualities, and struggled to\nreconcile chatbot relationships with social norms. These dynamics raise\nquestions for the design of digital companions and the rise of hybrid,\ngeneral-purpose AI systems."}
{"id": "2510.16518", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16518", "abs": "https://arxiv.org/abs/2510.16518", "authors": ["Jesús Ortega-Peimbert", "Finn Lukas Busch", "Timon Homberger", "Quantao Yang", "Olov Andersson"], "title": "DIV-Nav: Open-Vocabulary Spatial Relationships for Multi-Object Navigation", "comment": null, "summary": "Advances in open-vocabulary semantic mapping and object navigation have\nenabled robots to perform an informed search of their environment for an\narbitrary object. However, such zero-shot object navigation is typically\ndesigned for simple queries with an object name like \"television\" or \"blue\nrug\". Here, we consider more complex free-text queries with spatial\nrelationships, such as \"find the remote on the table\" while still leveraging\nrobustness of a semantic map. We present DIV-Nav, a real-time navigation system\nthat efficiently addresses this problem through a series of relaxations: i)\nDecomposing natural language instructions with complex spatial constraints into\nsimpler object-level queries on a semantic map, ii) computing the Intersection\nof individual semantic belief maps to identify regions where all objects\nco-exist, and iii) Validating the discovered objects against the original,\ncomplex spatial constrains via a LVLM. We further investigate how to adapt the\nfrontier exploration objectives of online semantic mapping to such spatial\nsearch queries to more effectively guide the search process. We validate our\nsystem through extensive experiments on the MultiON benchmark and real-world\ndeployment on a Boston Dynamics Spot robot using a Jetson Orin AGX. More\ndetails and videos are available at https://anonsub42.github.io/reponame/"}
{"id": "2510.16192", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.16192", "abs": "https://arxiv.org/abs/2510.16192", "authors": ["Kye Shimizu", "Minghan Gao", "Ananya Ganesh", "Pattie Maes"], "title": "VoiceMorph: How AI Voice Morphing Reveals the Boundaries of Auditory Self-Recognition", "comment": null, "summary": "This study investigated auditory self-recognition boundaries using AI voice\nmorphing technology, examining when individuals cease recognizing their own\nvoice. Through controlled morphing between participants' voices and\ndemographically matched targets at 1% increments using a mixed-methods design,\nwe measured self-identification ratings and response times among 21\nparticipants aged 18-64.\n  Results revealed a critical recognition threshold at 35.2% morphing (95% CI\n[31.4, 38.1]). Older participants tolerated significantly higher morphing\nlevels before losing self-recognition ($\\beta$ = 0.617, p = 0.048), suggesting\nage-related vulnerabilities. Greater acoustic embedding distances predicted\nslower decision-making ($r \\approx 0.5-0.53, p < 0.05$), with the longest\nresponse times for cloned versions of participants' own voices.\n  Qualitative analysis revealed prosodic-based recognition strategies,\nuniversal voice manipulation discomfort, and awareness of applications spanning\nassistive technology to security risks. These findings establish foundational\nevidence for individual differences in voice morphing detection, with\nimplications for AI ethics and vulnerable population protection as voice\nsynthesis becomes accessible."}
{"id": "2510.16524", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16524", "abs": "https://arxiv.org/abs/2510.16524", "authors": ["Haokai Ding", "Zhaohan Chen", "Tao Yang", "Wenzeng Zhang"], "title": "Semi-Peaucellier Linkage and Differential Mechanism for Linear Pinching and Self-Adaptive Grasping", "comment": "6 pages, 9 figures, Accepted author manuscript for IEEE CASE 2025", "summary": "This paper presents the SP-Diff parallel gripper system, addressing the\nlimited adaptability of conventional end-effectors in intelligent industrial\nautomation. The proposed design employs an innovative differential linkage\nmechanism with a modular symmetric dual-finger configuration to achieve\nlinear-parallel grasping. By integrating a planetary gear transmission, the\nsystem enables synchronized linear motion and independent finger pose\nadjustment while maintaining structural rigidity, reducing Z-axis recalibration\nrequirements by 30% compared to arc-trajectory grippers. The compact palm\narchitecture incorporates a kinematically optimized parallelogram linkage and\nDifferential mechanism, demonstrating adaptive grasping capabilities for\ndiverse industrial workpieces and deformable objects such as citrus fruits.\nFuture-ready interfaces are embedded for potential force/vision sensor\nintegration to facilitate multimodal data acquisition (e.g., trajectory\nplanning and object deformation) in digital twin frameworks. Designed as a\nflexible manufacturing solution, SP-Diff advances robotic end-effector\nintelligence through its adaptive architecture, showing promising applications\nin collaborative robotics, logistics automation, and specialized operational\nscenarios."}
{"id": "2510.16223", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.16223", "abs": "https://arxiv.org/abs/2510.16223", "authors": ["Kate Glazko", "Anika Arugunta", "Janelle Chan", "Nancy Jimenez-Garcia", "Tashfia Sharmin", "Jennifer Mankoff"], "title": "Case Study of GAI for Generating Novel Images for Real-World Embroidery", "comment": "Published as a workshop paper at GenAICHI: CHI 2024 Workshop on\n  Generative AI and HCI\n  (https://generativeaiandhci.github.io/papers/2024/genaichi2024_54.pdf)", "summary": "In this paper, we present a case study exploring the potential use of\nGenerative Artificial Intelligence (GAI) to address the real-world need of\nmaking the design of embroiderable art patterns more accessible. Through an\nauto-ethnographic case study by a disabled-led team, we examine the application\nof GAI as an assistive technology in generating embroidery patterns, addressing\nthe complexity involved in designing culturally-relevant patterns as well as\nthose that meet specific needs regarding detail and color. We detail the\niterative process of prompt engineering custom GPTs tailored for producing\nspecific visual outputs, emphasizing the nuances of achieving desirable results\nthat align with real-world embroidery requirements. Our findings underscore the\nmixed outcomes of employing GAI for producing embroiderable images, from\nfacilitating creativity and inclusion to navigating the unpredictability of\nAI-generated designs. Future work aims to refine GAI tools we explored for\ngenerating embroiderable images to make them more performant and accessible,\nwith the goal of fostering more inclusion in the domains of creativity and\nmaking."}
{"id": "2510.16617", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16617", "abs": "https://arxiv.org/abs/2510.16617", "authors": ["Ruihan Zhao", "Tyler Ingebrand", "Sandeep Chinchali", "Ufuk Topcu"], "title": "MoS-VLA: A Vision-Language-Action Model with One-Shot Skill Adaptation", "comment": null, "summary": "Vision-Language-Action (VLA) models trained on large robot datasets promise\ngeneral-purpose, robust control across diverse domains and embodiments.\nHowever, existing approaches often fail out-of-the-box when deployed in novel\nenvironments, embodiments, or tasks. We introduce Mixture of Skills VLA\n(MoS-VLA), a framework that represents robot manipulation policies as linear\ncombinations of a finite set of learned basis functions. During pretraining,\nMoS-VLA jointly learns these basis functions across datasets from the Open\nX-Embodiment project, producing a structured skill space. At test time,\nadapting to a new task requires only a single expert demonstration. The\ncorresponding skill representation is then inferred via a lightweight convex\noptimization problem that minimizes the L1 action error, without requiring\ngradient updates. This gradient-free adaptation incurs minimal overhead while\nenabling rapid instantiation of new skills. Empirically, MoS-VLA achieves lower\naction-prediction error on five out of five unseen datasets and succeeds in\nboth simulation and real-robot tasks where a pretrained VLA model fails\noutright. Project page: mos-vla.github.io/"}
{"id": "2510.16633", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.16633", "abs": "https://arxiv.org/abs/2510.16633", "authors": ["Xiaoshan Huang", "Tianlong Zhong", "Haolun Wu", "Yeyu Wang", "Ethan Churchill", "Xue Liu", "David Williamson Shaffer"], "title": "Linking Facial Recognition of Emotions and Socially Shared Regulation in Medical Simulation", "comment": "Accepted to the 28th ACM SIGCHI Conference on Computer-Supported\n  Cooperative Work & Social Computing (CSCW 2025). 5 pages, 3 figures", "summary": "Computer-supported simulation enables a practical alternative for medical\ntraining purposes. This study investigates the co-occurrence of\nfacial-recognition-derived emotions and socially shared regulation of learning\n(SSRL) interactions in a medical simulation training context. Using transmodal\nanalysis (TMA), we compare novice and expert learners' affective and cognitive\nengagement patterns during collaborative virtual diagnosis tasks. Results\nreveal that expert learners exhibit strong associations between socio-cognitive\ninteractions and high-arousal emotions (surprise, anger), suggesting focused,\neffortful engagement. In contrast, novice learners demonstrate stronger links\nbetween socio-cognitive processes and happiness or sadness, with less coherent\nSSRL patterns, potentially indicating distraction or cognitive overload.\nTransmodal analysis of multimodal data (facial expressions and discourse)\nhighlights distinct regulatory strategies between groups, offering\nmethodological and practical insights for computer-supported cooperative work\n(CSCW) in medical education. Our findings underscore the role of\nemotion-regulation dynamics in collaborative expertise development and suggest\nthe need for tailored scaffolding to support novice learners' socio-cognitive\nand affective engagement."}
{"id": "2510.16692", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16692", "abs": "https://arxiv.org/abs/2510.16692", "authors": ["Tianshu Ruan", "Zoe Betta", "Georgios Tzoumas", "Rustam Stolkin", "Manolis Chiou"], "title": "First Responders' Perceptions of Semantic Information for Situational Awareness in Robot-Assisted Emergency Response", "comment": null, "summary": "This study investigates First Responders' (FRs) attitudes toward the use of\nsemantic information and Situational Awareness (SA) in robotic systems during\nemergency operations. A structured questionnaire was administered to 22 FRs\nacross eight countries, capturing their demographic profiles, general attitudes\ntoward robots, and experiences with semantics-enhanced SA. Results show that\nmost FRs expressed positive attitudes toward robots, and rated the usefulness\nof semantic information for building SA at an average of 3.6 out of 5. Semantic\ninformation was also valued for its role in predicting unforeseen emergencies\n(mean 3.9). Participants reported requiring an average of 74.6\\% accuracy to\ntrust semantic outputs and 67.8\\% for them to be considered useful, revealing a\nwillingness to use imperfect but informative AI support tools.\n  To the best of our knowledge, this study offers novel insights by being one\nof the first to directly survey FRs on semantic-based SA in a cross-national\ncontext. It reveals the types of semantic information most valued in the field,\nsuch as object identity, spatial relationships, and risk context-and connects\nthese preferences to the respondents' roles, experience, and education levels.\nThe findings also expose a critical gap between lab-based robotics capabilities\nand the realities of field deployment, highlighting the need for more\nmeaningful collaboration between FRs and robotics researchers. These insights\ncontribute to the development of more user-aligned and situationally aware\nrobotic systems for emergency response."}
{"id": "2510.16662", "categories": ["cs.HC", "cs.AI", "cs.IR", "cs.LG", "H.1.2; H.3.3; I.3.6"], "pdf": "https://arxiv.org/pdf/2510.16662", "abs": "https://arxiv.org/abs/2510.16662", "authors": ["Huyen N. Nguyen", "Nils Gehlenborg"], "title": "Safire: Similarity Framework for Visualization Retrieval", "comment": "To appear in IEEE VIS 2025", "summary": "Effective visualization retrieval necessitates a clear definition of\nsimilarity. Despite the growing body of work in specialized visualization\nretrieval systems, a systematic approach to understanding visualization\nsimilarity remains absent. We introduce the Similarity Framework for\nVisualization Retrieval (Safire), a conceptual model that frames visualization\nsimilarity along two dimensions: comparison criteria and representation\nmodalities. Comparison criteria identify the aspects that make visualizations\nsimilar, which we divide into primary facets (data, visual encoding,\ninteraction, style, metadata) and derived properties (data-centric and\nhuman-centric measures). Safire connects what to compare with how comparisons\nare executed through representation modalities. We categorize existing\nrepresentation approaches into four groups based on their levels of information\ncontent and visualization determinism: raster image, vector image,\nspecification, and natural language description, together guiding what is\ncomputable and comparable. We analyze several visualization retrieval systems\nusing Safire to demonstrate its practical value in clarifying similarity\nconsiderations. Our findings reveal how particular criteria and modalities\nalign across different use cases. Notably, the choice of representation\nmodality is not only an implementation detail but also an important decision\nthat shapes retrieval capabilities and limitations. Based on our analysis, we\nprovide recommendations and discuss broader implications for multimodal\nlearning, AI applications, and visualization reproducibility."}
{"id": "2510.16738", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16738", "abs": "https://arxiv.org/abs/2510.16738", "authors": ["Matteo El-Hariry", "Vittorio Franzese", "Miguel Olivares-Mendez"], "title": "Towards Active Excitation-Based Dynamic Inertia Identification in Satellites", "comment": null, "summary": "This paper presents a comprehensive analysis of how excitation design\ninfluences the identification of the inertia properties of rigid nano- and\nmicro-satellites. We simulate nonlinear attitude dynamics with reaction-wheel\ncoupling, actuator limits, and external disturbances, and excite the system\nusing eight torque profiles of varying spectral richness. Two estimators are\ncompared, a batch Least Squares method and an Extended Kalman Filter, across\nthree satellite configurations and time-varying inertia scenarios. Results show\nthat excitation frequency content and estimator assumptions jointly determine\nestimation accuracy and robustness, offering practical guidance for in-orbit\nadaptive inertia identification by outlining the conditions under which each\nmethod performs best. The code is provided as open-source ."}
{"id": "2510.16764", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.16764", "abs": "https://arxiv.org/abs/2510.16764", "authors": ["Francesco Vona", "Julia Schorlemmer", "Paulina Kaulard", "Sebastian Fischer", "Jessica Stemann", "Jan-Niklas Voigt-Antons"], "title": "Comparing User Behavior in Real vs. Virtual Supermarket Shelves: An Eye-Tracking Study Using Tobii 3 Pro and Meta Quest Pro", "comment": null, "summary": "This study compares user behavior between real and virtual supermarket\nshelves using eye tracking technology to assess behavior in both environments.\nA sample of 29 participants was randomly assigned to two conditions: a real\nworld supermarket shelf with Tobii eye tracking and a virtual shelf using the\nMeta Quest Pro eye tracker. In both scenarios, participants were asked to\nselect three packs of cereals belonging to specific categories, healthy or\ntasty. The aim was to explore whether virtual environments could realistically\nreplicate real world experiences, particularly regarding consumer behavior. By\nanalyzing eye tracking data, the study examined how attention and product\nselection strategies varied between real and virtual conditions. Results showed\nthat participants' attention differed across product types and shopping\nenvironments. Consumers focused more on lower shelves in real settings,\nespecially when looking for healthy products. In VR, attention shifted to eye\nlevel shelves, particularly for tasty items, aligning with optimal product\nplacement strategies in supermarkets. Overall, sweet products received less\nvisual attention across both settings."}
{"id": "2510.16755", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.16755", "abs": "https://arxiv.org/abs/2510.16755", "authors": ["Kyung-Hwan Kim", "DongHyun Ahn", "Dong-hyun Lee", "JuYoung Yoon", "Dong Jin Hyun"], "title": "Adaptive Invariant Extended Kalman Filter for Legged Robot State Estimation", "comment": "6 pages, accepted to IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS) 2025", "summary": "State estimation is crucial for legged robots as it directly affects control\nperformance and locomotion stability. In this paper, we propose an Adaptive\nInvariant Extended Kalman Filter to improve proprioceptive state estimation for\nlegged robots. The proposed method adaptively adjusts the noise level of the\ncontact foot model based on online covariance estimation, leading to improved\nstate estimation under varying contact conditions. It effectively handles small\nslips that traditional slip rejection fails to address, as overly sensitive\nslip rejection settings risk causing filter divergence. Our approach employs a\ncontact detection algorithm instead of contact sensors, reducing the reliance\non additional hardware. The proposed method is validated through real-world\nexperiments on the quadruped robot LeoQuad, demonstrating enhanced state\nestimation performance in dynamic locomotion scenarios."}
{"id": "2510.16952", "categories": ["cs.HC", "cs.CL", "H.5.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.16952", "abs": "https://arxiv.org/abs/2510.16952", "authors": ["Austin Drake", "Hang Dong"], "title": "Real-Time World Crafting: Generating Structured Game Behaviors from Natural Language with Large Language Models", "comment": "16 pages, 11 figures (including appendix). To be presented at the 5th\n  Wordplay @ EMNLP workshop (2025)", "summary": "We present a novel architecture for safely integrating Large Language Models\n(LLMs) into interactive game engines, allowing players to \"program\" new\nbehaviors using natural language. Our framework mitigates risks by using an LLM\nto translate commands into a constrained Domain-Specific Language (DSL), which\nconfigures a custom Entity-Component-System (ECS) at runtime. We evaluated this\nsystem in a 2D spell-crafting game prototype by experimentally assessing models\nfrom the Gemini, GPT, and Claude families with various prompting strategies. A\nvalidated LLM judge qualitatively rated the outputs, showing that while larger\nmodels better captured creative intent, the optimal prompting strategy is\ntask-dependent: Chain-of-Thought improved creative alignment, while few-shot\nexamples were necessary to generate more complex DSL scripts. This work offers\na validated LLM-ECS pattern for emergent gameplay and a quantitative\nperformance comparison for developers."}
{"id": "2510.16767", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16767", "abs": "https://arxiv.org/abs/2510.16767", "authors": ["Jia Li", "Guoxiang Zhao"], "title": "T3 Planner: A Self-Correcting LLM Framework for Robotic Motion Planning with Temporal Logic", "comment": null, "summary": "Translating natural language instructions into executable motion plans is a\nfundamental challenge in robotics. Traditional approaches are typically\nconstrained by their reliance on domain-specific expertise to customize\nplanners, and often struggle with spatio-temporal couplings that usually lead\nto infeasible motions or discrepancies between task planning and motion\nexecution. Despite the proficiency of Large Language Models (LLMs) in\nhigh-level semantic reasoning, hallucination could result in infeasible motion\nplans. In this paper, we introduce the T3 Planner, an LLM-enabled robotic\nmotion planning framework that self-corrects it output with formal methods. The\nframework decomposes spatio-temporal task constraints via three cascaded\nmodules, each of which stimulates an LLM to generate candidate trajectory\nsequences and examines their feasibility via a Signal Temporal Logic (STL)\nverifier until one that satisfies complex spatial, temporal, and logical\nconstraints is found.Experiments across different scenarios show that T3\nPlanner significantly outperforms the baselines. The required reasoning can be\ndistilled into a lightweight Qwen3-4B model that enables efficient deployment.\nAll supplementary materials are accessible at\nhttps://github.com/leeejia/T3_Planner."}
{"id": "2510.16984", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.16984", "abs": "https://arxiv.org/abs/2510.16984", "authors": ["Seckin Damar", "Gulsah Hancerliogullari Koksalmis"], "title": "Integrating Metaverse Technologies in Medical Education: Examining Acceptance Factors Among Current and Future Healthcare Providers", "comment": "43 pages, 2 figures, 9 tables", "summary": "This study investigates behavioral intention to use healthcare metaverse\nplatforms among medical students and physicians in Turkey, where such\ntechnologies are in early stages of adoption. A multi-theoretical research\nmodel was developed by integrating constructs from the Innovation Diffusion\nTheory, Embodied Social Presence Theory, Interaction Equivalency Theorem and\nTechnology Acceptance Model. Data from 718 participants were analyzed using\npartial least squares structural equation modeling. Results show that\nsatisfaction, perceived usefulness, perceived ease of use, learner\ninteractions, and technology readiness significantly enhance adoption, while\ntechnology anxiety and complexity have negative effects. Learner learner and\nlearner teacher interactions strongly predict satisfaction, which subsequently\nincreases behavioral intention. Perceived ease of use fully mediates the\nrelationship between technology anxiety and perceived usefulness. However,\ntechnology anxiety does not significantly moderate the effects of perceived\nusefulness or ease of use on behavioral intention. The model explains 71.8% of\nthe variance in behavioral intention, indicating strong explanatory power. The\nfindings offer practical implications for educators, curriculum designers, and\ndevelopers aiming to integrate metaverse platforms into healthcare training in\ndigitally transitioning educational systems."}
{"id": "2510.16771", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16771", "abs": "https://arxiv.org/abs/2510.16771", "authors": ["Xu He", "Xiaolin Meng", "Wenxuan Yin", "Youdong Zhang", "Lingfei Mo", "Xiangdong An", "Fangwen Yu", "Shuguo Pan", "Yufeng Liu", "Jingnan Liu", "Yujia Zhang", "Wang Gao"], "title": "A Preliminary Exploration of the Differences and Conjunction of Traditional PNT and Brain-inspired PNT", "comment": null, "summary": "Developing universal Positioning, Navigation, and Timing (PNT) is our\nenduring goal. Today's complex environments demand PNT that is more resilient,\nenergy-efficient and cognitively capable. This paper asks how we can endow\nunmanned systems with brain-inspired spatial cognition navigation while\nexploiting the high precision of machine PNT to advance universal PNT. We\nprovide a new perspective and roadmap for shifting PNT from \"tool-oriented\" to\n\"cognition-driven\". Contributions: (1) multi-level dissection of differences\namong traditional PNT, biological brain PNT and brain-inspired PNT; (2) a\nfour-layer (observation-capability-decision-hardware) fusion framework that\nunites numerical precision and brain-inspired intelligence; (3) forward-looking\nrecommendations for future development of brain-inspired PNT."}
{"id": "2510.17073", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.17073", "abs": "https://arxiv.org/abs/2510.17073", "authors": ["Liwei Wu", "Yilin Zhang", "Justin Leung", "Jingyi Gao", "April Li", "Jian Zhao"], "title": "Planar or Spatial: Exploring Design Aspects and Challenges for Presentations in Virtual Reality with No-coding Interface", "comment": null, "summary": "The proliferation of virtual reality (VR) has led to its increasing adoption\nas an immersive medium for delivering presentations, distinct from other VR\nexperiences like games and 360-degree videos by sharing information in richly\ninteractive environments. However, creating engaging VR presentations remains a\nchallenging and time-consuming task for users, hindering the full realization\nof VR presentation's capabilities. This research aims to explore the potential\nof VR presentation, analyze users' opinions, and investigate these via\nproviding a user-friendly no-coding authoring tool. Through an examination of\npopular presentation software and interviews with seven professionals, we\nidentified five design aspects and four design challenges for VR presentations.\nBased on the findings, we developed VRStory, a prototype for presentation\nauthoring without coding to explore the design aspects and strategies for\naddressing the challenges. VRStory offers a variety of predefined and\ncustomizable VR elements, as well as modules for layout design, navigation\ncontrol, and asset generation. A user study was then conducted with 12\nparticipants to investigate their opinions and authoring experience with\nVRStory. Our results demonstrated that, while acknowledging the advantages of\nimmersive and spatial features in VR, users often have a consistent mental\nmodel for traditional 2D presentations and may still prefer planar and static\nformats in VR for better accessibility and efficient communication. We finally\nshared our learned design considerations for future development of VR\npresentation tools, emphasizing the importance of balancing of promoting\nimmersive features and ensuring accessibility."}
{"id": "2510.16905", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16905", "abs": "https://arxiv.org/abs/2510.16905", "authors": ["Yukang Cao", "Rahul Moorthy", "O. Goktug Poyrazoglu", "Volkan Isler"], "title": "C-Free-Uniform: A Map-Conditioned Trajectory Sampler for Model Predictive Path Integral Control", "comment": "Submitted to the 2026 IEEE International Conference on Robotics and\n  Automation (ICRA). 8 pages, 4 figures", "summary": "Trajectory sampling is a key component of sampling-based control mechanisms.\nTrajectory samplers rely on control input samplers, which generate control\ninputs u from a distribution p(u | x) where x is the current state. We\nintroduce the notion of Free Configuration Space Uniformity (C-Free-Uniform for\nshort) which has two key features: (i) it generates a control input\ndistribution so as to uniformly sample the free configuration space, and (ii)\nin contrast to previously introduced trajectory sampling mechanisms where the\ndistribution p(u | x) is independent of the environment, C-Free-Uniform is\nexplicitly conditioned on the current local map. Next, we integrate this\nsampler into a new Model Predictive Path Integral (MPPI) Controller, CFU-MPPI.\nExperiments show that CFU-MPPI outperforms existing methods in terms of success\nrate in challenging navigation tasks in cluttered polygonal environments while\nrequiring a much smaller sampling budget."}
{"id": "2510.17083", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.17083", "abs": "https://arxiv.org/abs/2510.17083", "authors": ["Ivan C. H. Liu"], "title": "Toward a Cognitive-Affective-Systemic Framework for Art and Sustainability", "comment": "16 pages, 5 figures. Submitted to Digital Creativity", "summary": "This paper proposes a ognitive-Affective-Systemic (CAS) framework that\nintegrates cognition, emotion, and systemic understanding to cultivate\nsustainability awareness through art. Drawing from eco-aesthetics, affect\ntheory, complexity science, and posthuman ethics, the framework defines\nartistic practice as both epistemic and performative--a way of knowing through\nmaking and feeling. Central to this is logomotion, an aesthetic mode where\ncomprehension and emotion move together as a unified experience. Two artworks,\nSPill, visualizing antimicrobial resistance through avalanche dynamics, and\nEchoes of the Land, modeling anthropogenic seismicity, demonstrate how systemic\nmodeling and sensory immersion transform complex science into embodied\necological understanding. The framework offers a methodological foundation for\nartists, theorists, and activists to translate awareness into engagement,\nadvancing collective creativity toward sustainable futures."}
{"id": "2510.16931", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16931", "abs": "https://arxiv.org/abs/2510.16931", "authors": ["Zhaoliang Wan", "Zida Zhou", "Zetong Bi", "Zehui Yang", "Hao Ding", "Hui Cheng"], "title": "Design of an Affordable, Fully-Actuated Biomimetic Hand for Dexterous Teleoperation Systems", "comment": "Accepted by IROS2025", "summary": "This paper addresses the scarcity of affordable, fully-actuated five-fingered\nhands for dexterous teleoperation, which is crucial for collecting large-scale\nreal-robot data within the \"Learning from Demonstrations\" paradigm. We\nintroduce the prototype version of the RAPID Hand, the first low-cost,\n20-degree-of-actuation (DoA) dexterous hand that integrates a novel\nanthropomorphic actuation and transmission scheme with an optimized motor\nlayout and structural design to enhance dexterity. Specifically, the RAPID Hand\nfeatures a universal phalangeal transmission scheme for the non-thumb fingers\nand an omnidirectional thumb actuation mechanism. Prioritizing affordability,\nthe hand employs 3D-printed parts combined with custom gears for easier\nreplacement and repair. We assess the RAPID Hand's performance through\nquantitative metrics and qualitative testing in a dexterous teleoperation\nsystem, which is evaluated on three challenging tasks: multi-finger retrieval,\nladle handling, and human-like piano playing. The results indicate that the\nRAPID Hand's fully actuated 20-DoF design holds significant promise for\ndexterous teleoperation."}
{"id": "2510.17102", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.17102", "abs": "https://arxiv.org/abs/2510.17102", "authors": ["Keigo Ushiyama", "Hiroyuki Kajimoto"], "title": "Kinesthetic Weight Modulation: The Effects of Whole-Arm Tendon Vibration on the Perceived Heaviness", "comment": "12 pages, 11 figures", "summary": "Kinesthetic illusions, which arise when muscle spindles are activated by\nvibration, provide a compact means of presenting kinesthetic sensations.\nBecause muscle spindles contribute not only to sensing body movement but also\nto perceiving heaviness, vibration-induced illusions could potentially modulate\nweight perception. While prior studies have primarily focused on conveying\nvirtual movement, the modulation of perceived heaviness has received little\nattention. Presenting a sense of heaviness is essential for enriching haptic\ninteractions with virtual objects. This study investigates whether multi-point\ntendon vibration can increase or decrease perceived heaviness (Experiment 1)\nand how the magnitude of the effect can be systematically controlled\n(Experiment 2). The results show that tendon vibration significantly increases\nperceived heaviness but does not significantly decrease it, although a\ndecreasing trend was observed. Moreover, the increase can be adjusted across at\nleast three levels within the range of 350-450 g. Finally, we discuss plausible\nmechanisms underlying this vibration-induced modulation of weight perception."}
{"id": "2510.17038", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17038", "abs": "https://arxiv.org/abs/2510.17038", "authors": ["Pedram Fekri", "Majid Roshanfar", "Samuel Barbeau", "Seyedfarzad Famouri", "Thomas Looi", "Dale Podolsky", "Mehrdad Zadeh", "Javad Dargahi"], "title": "DINO-CVA: A Multimodal Goal-Conditioned Vision-to-Action Model for Autonomous Catheter Navigation", "comment": null, "summary": "Cardiac catheterization remains a cornerstone of minimally invasive\ninterventions, yet it continues to rely heavily on manual operation. Despite\nadvances in robotic platforms, existing systems are predominantly follow-leader\nin nature, requiring continuous physician input and lacking intelligent\nautonomy. This dependency contributes to operator fatigue, more radiation\nexposure, and variability in procedural outcomes. This work moves towards\nautonomous catheter navigation by introducing DINO-CVA, a multimodal\ngoal-conditioned behavior cloning framework. The proposed model fuses visual\nobservations and joystick kinematics into a joint embedding space, enabling\npolicies that are both vision-aware and kinematic-aware. Actions are predicted\nautoregressively from expert demonstrations, with goal conditioning guiding\nnavigation toward specified destinations. A robotic experimental setup with a\nsynthetic vascular phantom was designed to collect multimodal datasets and\nevaluate performance. Results show that DINO-CVA achieves high accuracy in\npredicting actions, matching the performance of a kinematics-only baseline\nwhile additionally grounding predictions in the anatomical environment. These\nfindings establish the feasibility of multimodal, goal-conditioned\narchitectures for catheter navigation, representing an important step toward\nreducing operator dependency and improving the reliability of catheterbased\ntherapies."}
{"id": "2510.17119", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.17119", "abs": "https://arxiv.org/abs/2510.17119", "authors": ["Soyoung Jung", "Sung Park"], "title": "Design Framework for Conversational Agent in Couple relationships: A Systematic Review", "comment": null, "summary": "The development of conversational agents (CAs) has shown strong potential in\nsupporting mental health through dialogue. While many studies focus on CAs for\nindividual psychological care, research on agents designed for couples facing\nrelational or emotional challenges remains limited. This study aims to identify\ndesign considerations for CAs that address the relational context of couples\nand support their well-being. Following PRISMA guidelines, a systematic review\nwas conducted across seven databases: CINAHL, Embase, PubMed, PsycINFO, Scopus,\nWeb of Science, and the ACM Digital Library. Peer-reviewed empirical studies\nwere screened, duplicates removed, and selection criteria applied, resulting in\ntwelve studies for analysis. Thematic analysis was conducted across three\ndimensions: AI interaction design, relational framing, and technical\nlimitations. Three key themes emerged: (1) the need for a relational expert\npersona, (2) technological directions leveraging state-of-the-art AI for\nrelational specificity and emotional competence, and (3) a shift from\ncontent-centered to relationship-centered design. Based on these insights,\neight design considerations are proposed for couple-oriented CAs: (1) agent\npersona, (2) individual mode, (3) concurrent mode, (4) conjoint mode, (5)\nethics, (6) data and privacy, (7) interaction pattern, and (8) safety\nmechanism. These principles guide CAs as relational mediators capable of\nmaintaining multiple alliances, respecting cultural and ethical boundaries, and\nensuring fairness and emotional safety between partners. Ultimately, this\nreview introduces a design framework that integrates relational theory with\nadvanced AI technologies to inform future development of CAs for couple-based\nmental health interventions."}
{"id": "2510.17086", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17086", "abs": "https://arxiv.org/abs/2510.17086", "authors": ["Xueqian Bai", "Nicklas Hansen", "Adabhav Singh", "Michael T. Tolley", "Yan Duan", "Pieter Abbeel", "Xiaolong Wang", "Sha Yi"], "title": "Learning to Design Soft Hands using Reward Models", "comment": null, "summary": "Soft robotic hands promise to provide compliant and safe interaction with\nobjects and environments. However, designing soft hands to be both compliant\nand functional across diverse use cases remains challenging. Although co-design\nof hardware and control better couples morphology to behavior, the resulting\nsearch space is high-dimensional, and even simulation-based evaluation is\ncomputationally expensive. In this paper, we propose a Cross-Entropy Method\nwith Reward Model (CEM-RM) framework that efficiently optimizes tendon-driven\nsoft robotic hands based on teleoperation control policy, reducing design\nevaluations by more than half compared to pure optimization while learning a\ndistribution of optimized hand designs from pre-collected teleoperation data.\nWe derive a design space for a soft robotic hand composed of flexural soft\nfingers and implement parallelized training in simulation. The optimized hands\nare then 3D-printed and deployed in the real world using both teleoperation\ndata and real-time teleoperation. Experiments in both simulation and hardware\ndemonstrate that our optimized design significantly outperforms baseline hands\nin grasping success rates across a diverse set of challenging objects."}
{"id": "2510.17253", "categories": ["cs.HC", "cs.AI", "68T09, 68U35", "H.5.2; H.3.3; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.17253", "abs": "https://arxiv.org/abs/2510.17253", "authors": ["Özkan Canay", "{Ü}mit Kocabıcak"], "title": "Augmented Web Usage Mining and User Experience Optimization with CAWAL's Enriched Analytics Data", "comment": "19 pages, 5 figures. Published in International Journal of\n  Human-Computer Interaction (Taylor & Francis, 2025)", "summary": "Understanding user behavior on the web is increasingly critical for\noptimizing user experience (UX). This study introduces Augmented Web Usage\nMining (AWUM), a methodology designed to enhance web usage mining and improve\nUX by enriching the interaction data provided by CAWAL (Combined Application\nLog and Web Analytics), a framework for advanced web analytics. Over 1.2\nmillion session records collected in one month (~8.5GB of data) were processed\nand transformed into enriched datasets. AWUM analyzes session structures, page\nrequests, service interactions, and exit methods. Results show that 87.16% of\nsessions involved multiple pages, contributing 98.05% of total pageviews; 40%\nof users accessed various services and 50% opted for secure exits. Association\nrule mining revealed patterns of frequently accessed services, highlighting\nCAWAL's precision and efficiency over conventional methods. AWUM offers a\ncomprehensive understanding of user behavior and strong potential for\nlarge-scale UX optimization."}
{"id": "2510.17111", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17111", "abs": "https://arxiv.org/abs/2510.17111", "authors": ["Weifan Guan", "Qinghao Hu", "Aosheng Li", "Jian Cheng"], "title": "Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey", "comment": null, "summary": "Vision-Language-Action (VLA) models extend vision-language models to embodied\ncontrol by mapping natural-language instructions and visual observations to\nrobot actions. Despite their capabilities, VLA systems face significant\nchallenges due to their massive computational and memory demands, which\nconflict with the constraints of edge platforms such as on-board mobile\nmanipulators that require real-time performance. Addressing this tension has\nbecome a central focus of recent research. In light of the growing efforts\ntoward more efficient and scalable VLA systems, this survey provides a\nsystematic review of approaches for improving VLA efficiency, with an emphasis\non reducing latency, memory footprint, and training and inference costs. We\ncategorize existing solutions into four dimensions: model architecture,\nperception feature, action generation, and training/inference strategies,\nsummarizing representative techniques within each category. Finally, we discuss\nfuture trends and open challenges, highlighting directions for advancing\nefficient embodied intelligence."}
{"id": "2510.17355", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.17355", "abs": "https://arxiv.org/abs/2510.17355", "authors": ["Ashmi Banerjee", "Melih Mert Aksoy", "Wolfgang Wörndl"], "title": "SmartSustain Recommender System: Navigating Sustainability Trade-offs in Personalized City Trip Planning", "comment": "Accepted for presentation at Workshop on Recommender Systems for\n  Sustainable Development (RS4SD), co-located with CIKM'2025", "summary": "Tourism is a major contributor to global carbon emissions and over-tourism,\ncreating an urgent need for recommender systems that not only inform but also\ngently steer users toward more sustainable travel decisions. Such choices,\nhowever, often require balancing complex trade-offs between environmental\nimpact, cost, convenience, and personal interests. To address this, we present\nthe SmartSustain Recommender, a web application designed to nudge users toward\neco-friendlier options through an interactive, user-centric interface. The\nsystem visualizes the broader consequences of travel decisions by combining\nCO2e emissions, destination popularity, and seasonality with personalized\ninterest matching. It employs mechanisms such as interactive city cards for\nquick comparisons, dynamic banners that surface sustainable alternatives in\nspecific trade-off scenarios, and real-time impact feedback using animated\nenvironmental indicators. A preliminary user study with 21 participants\nindicated strong usability and perceived effectiveness. The system is\naccessible at https://smartsustainrecommender.web.app."}
{"id": "2510.17143", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17143", "abs": "https://arxiv.org/abs/2510.17143", "authors": ["Shantnav Agarwal", "Javier Alonso-Mora", "Sihao Sun"], "title": "Decentralized Real-Time Planning for Multi-UAV Cooperative Manipulation via Imitation Learning", "comment": "Accepted by IEEE MRS 2025", "summary": "Existing approaches for transporting and manipulating cable-suspended loads\nusing multiple UAVs along reference trajectories typically rely on either\ncentralized control architectures or reliable inter-agent communication. In\nthis work, we propose a novel machine learning based method for decentralized\nkinodynamic planning that operates effectively under partial observability and\nwithout inter-agent communication. Our method leverages imitation learning to\ntrain a decentralized student policy for each UAV by imitating a centralized\nkinodynamic motion planner with access to privileged global observations. The\nstudent policy generates smooth trajectories using physics-informed neural\nnetworks that respect the derivative relationships in motion. During training,\nthe student policies utilize the full trajectory generated by the teacher\npolicy, leading to improved sample efficiency. Moreover, each student policy\ncan be trained in under two hours on a standard laptop. We validate our method\nin both simulation and real-world environments to follow an agile reference\ntrajectory, demonstrating performance comparable to that of centralized\napproaches."}
{"id": "2510.17534", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.17534", "abs": "https://arxiv.org/abs/2510.17534", "authors": ["Yichen Yu", "Qiaoran Wang"], "title": "NieNie: Adaptive Rhythmic System for Stress Relief with LLM-Based Guidance", "comment": null, "summary": "Today's young people are facing increasing psychological stress due to\nvarious social issues. Traditional stress management tools often rely on static\nscripts or passive content, which are ineffective in alleviating stress. NieNie\naddresses this gap by combining rhythm biofeedback with real-time psychological\nguidance through a large language model (LLM), offering an interactive, tactile\nresponse. The system is specifically designed for young people experiencing\nemotional stress, collecting physiological signals such as heart rate\nvariability and generating adaptive squeeze-release rhythms via soft, tactile\ndevices. Utilising LLM, the system provides timely squeezing rhythms and\npsychologically guided feedback prompts, offering personalised rhythm games\nwhile reinforcing stress restructuring. Unlike traditional mental health apps,\nNieNie places users within an embodied interactive loop, leveraging tactile\ninteraction, biofeedback, and adaptive language support to create an immersive\nstress regulation experience. This study demonstrates how embodied systems can\nconnect bodily actions with mental health in everyday contexts."}
{"id": "2510.17148", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17148", "abs": "https://arxiv.org/abs/2510.17148", "authors": ["Yu Gao", "Yiru Wang", "Anqing Jiang", "Heng Yuwen", "Wang Shuo", "Sun Hao", "Wang Jijun"], "title": "DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment", "comment": null, "summary": "Conventional end-to-end (E2E) driving models are effective at generating\nphysically plausible trajectories, but often fail to generalize to long-tail\nscenarios due to the lack of essential world knowledge to understand and reason\nabout surrounding environments. In contrast, Vision-Language-Action (VLA)\nmodels leverage world knowledge to handle challenging cases, but their limited\n3D reasoning capability can lead to physically infeasible actions. In this work\nwe introduce DiffVLA++, an enhanced autonomous driving framework that\nexplicitly bridges cognitive reasoning and E2E planning through metric-guided\nalignment. First, we build a VLA module directly generating semantically\ngrounded driving trajectories. Second, we design an E2E module with a dense\ntrajectory vocabulary that ensures physical feasibility. Third, and most\ncritically, we introduce a metric-guided trajectory scorer that guides and\naligns the outputs of the VLA and E2E modules, thereby integrating their\ncomplementary strengths. The experiment on the ICCV 2025 Autonomous Grand\nChallenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12."}
{"id": "2510.17575", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.17575", "abs": "https://arxiv.org/abs/2510.17575", "authors": ["Ash Sharma", "Karen Cochrane", "James R. Wallace"], "title": "DeTAILS: Deep Thematic Analysis with Iterative LLM Support", "comment": null, "summary": "Thematic analysis is widely used in qualitative research but can be difficult\nto scale because of its iterative, interpretive demands. We introduce DeTAILS,\na toolkit that integrates large language model (LLM) assistance into a workflow\ninspired by Braun and Clarke's thematic analysis framework. DeTAILS supports\nresearchers in generating and refining codes, reviewing clusters, and\nsynthesizing themes through interactive feedback loops designed to preserve\nanalytic agency. We evaluated the system with 18 qualitative researchers\nanalyzing Reddit data. Quantitative results showed strong alignment between\nLLM-supported outputs and participants' refinements, alongside reduced workload\nand high perceived usefulness. Qualitatively, participants reported that\nDeTAILS accelerated analysis, prompted reflexive engagement with AI outputs,\nand fostered trust through transparency and control. We contribute: (1) an\ninteractive human-LLM workflow for large-scale qualitative analysis, (2)\nempirical evidence of its feasibility and researcher experience, and (3) design\nimplications for trustworthy AI-assisted qualitative research."}
{"id": "2510.17150", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17150", "abs": "https://arxiv.org/abs/2510.17150", "authors": ["Heng Zhang", "Wei-Hsing Huang", "Gokhan Solak", "Arash Ajoudani"], "title": "OmniVIC: A Self-Improving Variable Impedance Controller with Vision-Language In-Context Learning for Safe Robotic Manipulation", "comment": "Code, video and RAG dataset are available at\n  \\url{https://sites.google.com/view/omni-vic}", "summary": "We present OmniVIC, a universal variable impedance controller (VIC) enhanced\nby a vision language model (VLM), which improves safety and adaptation in any\ncontact-rich robotic manipulation task to enhance safe physical interaction.\nTraditional VIC have shown advantages when the robot physically interacts with\nthe environment, but lack generalization in unseen, complex, and unstructured\nsafe interactions in universal task scenarios involving contact or uncertainty.\nTo this end, the proposed OmniVIC interprets task context derived reasoning\nfrom images and natural language and generates adaptive impedance parameters\nfor a VIC controller. Specifically, the core of OmniVIC is a self-improving\nRetrieval-Augmented Generation(RAG) and in-context learning (ICL), where RAG\nretrieves relevant prior experiences from a structured memory bank to inform\nthe controller about similar past tasks, and ICL leverages these retrieved\nexamples and the prompt of current task to query the VLM for generating\ncontext-aware and adaptive impedance parameters for the current manipulation\nscenario. Therefore, a self-improved RAG and ICL guarantee OmniVIC works in\nuniversal task scenarios. The impedance parameter regulation is further\ninformed by real-time force/torque feedback to ensure interaction forces remain\nwithin safe thresholds. We demonstrate that our method outperforms baselines on\na suite of complex contact-rich tasks, both in simulation and on real-world\nrobotic tasks, with improved success rates and reduced force violations.\nOmniVIC takes a step towards bridging high-level semantic reasoning and\nlow-level compliant control, enabling safer and more generalizable\nmanipulation. Overall, the average success rate increases from 27% (baseline)\nto 61.4% (OmniVIC)."}
{"id": "2510.17599", "categories": ["cs.HC", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17599", "abs": "https://arxiv.org/abs/2510.17599", "authors": ["Hendric Voss", "Lisa Michelle Bohnenkamp", "Stefan Kopp"], "title": "Conveying Meaning through Gestures: An Investigation into Semantic Co-Speech Gesture Generation", "comment": null, "summary": "This study explores two frameworks for co-speech gesture generation, AQ-GT\nand its semantically-augmented variant AQ-GT-a, to evaluate their ability to\nconvey meaning through gestures and how humans perceive the resulting\nmovements. Using sentences from the SAGA spatial communication corpus,\ncontextually similar sentences, and novel movement-focused sentences, we\nconducted a user-centered evaluation of concept recognition and human-likeness.\nResults revealed a nuanced relationship between semantic annotations and\nperformance. The original AQ-GT framework, lacking explicit semantic input, was\nsurprisingly more effective at conveying concepts within its training domain.\nConversely, the AQ-GT-a framework demonstrated better generalization,\nparticularly for representing shape and size in novel contexts. While\nparticipants rated gestures from AQ-GT-a as more expressive and helpful, they\ndid not perceive them as more human-like. These findings suggest that explicit\nsemantic enrichment does not guarantee improved gesture generation and that its\neffectiveness is highly dependent on the context, indicating a potential\ntrade-off between specialization and generalization."}
{"id": "2510.17191", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17191", "abs": "https://arxiv.org/abs/2510.17191", "authors": ["Peiru Zheng", "Yun Zhao", "Zhan Gong", "Hong Zhu", "Shaohua Wu"], "title": "SimpleVSF: VLM-Scoring Fusion for Trajectory Prediction of End-to-End Autonomous Driving", "comment": "6 pages, 2 figures, 2 tables", "summary": "End-to-end autonomous driving has emerged as a promising paradigm for\nachieving robust and intelligent driving policies. However, existing end-to-end\nmethods still face significant challenges, such as suboptimal decision-making\nin complex scenarios. In this paper,we propose SimpleVSF (Simple VLM-Scoring\nFusion), a novel framework that enhances end-to-end planning by leveraging the\ncognitive capabilities of Vision-Language Models (VLMs) and advanced trajectory\nfusion techniques. We utilize the conventional scorers and the novel\nVLM-enhanced scorers. And we leverage a robust weight fusioner for quantitative\naggregation and a powerful VLM-based fusioner for qualitative, context-aware\ndecision-making. As the leading approach in the ICCV 2025 NAVSIM v2 End-to-End\nDriving Challenge, our SimpleVSF framework demonstrates state-of-the-art\nperformance, achieving a superior balance between safety, comfort, and\nefficiency."}
{"id": "2510.17617", "categories": ["cs.HC", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17617", "abs": "https://arxiv.org/abs/2510.17617", "authors": ["Hendric Voss", "Stefan Kopp"], "title": "ImaGGen: Zero-Shot Generation of Co-Speech Semantic Gestures Grounded in Language and Image Input", "comment": null, "summary": "Human communication combines speech with expressive nonverbal cues such as\nhand gestures that serve manifold communicative functions. Yet, current\ngenerative gesture generation approaches are restricted to simple, repetitive\nbeat gestures that accompany the rhythm of speaking but do not contribute to\ncommunicating semantic meaning. This paper tackles a core challenge in\nco-speech gesture synthesis: generating iconic or deictic gestures that are\nsemantically coherent with a verbal utterance. Such gestures cannot be derived\nfrom language input alone, which inherently lacks the visual meaning that is\noften carried autonomously by gestures. We therefore introduce a zero-shot\nsystem that generates gestures from a given language input and additionally is\ninformed by imagistic input, without manual annotation or human intervention.\nOur method integrates an image analysis pipeline that extracts key object\nproperties such as shape, symmetry, and alignment, together with a semantic\nmatching module that links these visual details to spoken text. An inverse\nkinematics engine then synthesizes iconic and deictic gestures and combines\nthem with co-generated natural beat gestures for coherent multimodal\ncommunication. A comprehensive user study demonstrates the effectiveness of our\napproach. In scenarios where speech alone was ambiguous, gestures generated by\nour system significantly improved participants' ability to identify object\nproperties, confirming their interpretability and communicative value. While\nchallenges remain in representing complex shapes, our results highlight the\nimportance of context-aware semantic gestures for creating expressive and\ncollaborative virtual agents or avatars, marking a substantial step forward\ntowards efficient and robust, embodied human-agent interaction. More\ninformation and example videos are available here:\nhttps://review-anon-io.github.io/ImaGGen.github.io/"}
{"id": "2510.17203", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17203", "abs": "https://arxiv.org/abs/2510.17203", "authors": ["Ryota Soga", "Masataka Kobayashi", "Tsukasa Shimizu", "Shintaro Shiba", "Quan Kong", "Shan Lu", "Takaya Yamazato"], "title": "Performance Evaluation of an Integrated System for Visible Light Communication and Positioning Using an Event Camera", "comment": "7pages, APCC2025", "summary": "Event cameras, featuring high temporal resolution and high dynamic range,\noffer visual sensing capabilities comparable to conventional image sensors\nwhile capturing fast-moving objects and handling scenes with extreme lighting\ncontrasts such as tunnel exits. Leveraging these properties, this study\nproposes a novel self-localization system that integrates visible light\ncommunication (VLC) and visible light positioning (VLP) within a single event\ncamera. The system enables a vehicle to estimate its position even in\nGPS-denied environments, such as tunnels, by using VLC to obtain coordinate\ninformation from LED transmitters and VLP to estimate the distance to each\ntransmitter.\n  Multiple LEDs are installed on the transmitter side, each assigned a unique\npilot sequence based on Walsh-Hadamard codes. The event camera identifies\nindividual LEDs within its field of view by correlating the received signal\nwith these codes, allowing clear separation and recognition of each light\nsource. This mechanism enables simultaneous high-capacity MISO (multi-input\nsingle-output) communication through VLC and precise distance estimation via\nphase-only correlation (POC) between multiple LED pairs.\n  To the best of our knowledge, this is the first vehicle-mounted system to\nachieve simultaneous VLC and VLP functionalities using a single event camera.\nField experiments were conducted by mounting the system on a vehicle traveling\nat 30 km/h (8.3 m/s). The results demonstrated robust real-world performance,\nwith a root mean square error (RMSE) of distance estimation within 0.75 m for\nranges up to 100 m and a bit error rate (BER) below 0.01 across the same range."}
{"id": "2510.17660", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.17660", "abs": "https://arxiv.org/abs/2510.17660", "authors": ["Adyasha Dash", "Giulia Zappoli", "Laya Das", "Robert Riener"], "title": "Muscle Anatomy-aware Geometric Deep Learning for sEMG-based Gesture Decoding", "comment": null, "summary": "Robust and accurate decoding of gesture from non-invasive surface\nelectromyography (sEMG) is important for various applications including spatial\ncomputing, healthcare, and entertainment, and has been actively pursued by\nresearchers and industry. Majority of sEMG-based gesture decoding algorithms\nemploy deep neural networks that are designed for Euclidean data, and may not\nbe suitable for analyzing multi-dimensional, non-stationary time-series with\nlong-range dependencies such as sEMG. State-of-the-art sEMG-based decoding\nmethods also demonstrate high variability across subjects and sessions,\nrequiring re-calibration and adaptive fine-tuning to boost performance. To\naddress these shortcomings, this work proposes a geometric deep learning model\nthat learns on symmetric positive definite (SPD) manifolds and leverages\nunsupervised domain adaptation to desensitize the model to subjects and\nsessions. The model captures the features in time and across sensors with\nmultiple kernels, projects the features onto SPD manifold, learns on manifolds\nand projects back to Euclidean space for classification. It uses a\ndomain-specific batch normalization layer to address variability between\nsessions, alleviating the need for re-calibration or fine-tuning. Experiments\nwith publicly available benchmark gesture decoding datasets (Ninapro DB6,\nFlexwear-HD) demonstrate the superior generalizability of the model compared to\nEuclidean and other SPD-based models in the inter-session scenario, with up to\n8.83 and 4.63 points improvement in accuracy, respectively. Detailed analyses\nreveal that the model extracts muscle-specific information for different tasks\nand ablation studies highlight the importance of modules introduced in the\nwork. The proposed method pushes the state-of-the-art in sEMG-based gesture\nrecognition and opens new research avenues for manifold-based learning for\nmuscle signals."}
{"id": "2510.17237", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17237", "abs": "https://arxiv.org/abs/2510.17237", "authors": ["Wuhao Xie", "Kanji Tanaka"], "title": "Pole-Image: A Self-Supervised Pole-Anchored Descriptor for Long-Term LiDAR Localization and Map Maintenance", "comment": "4 pages, technical report", "summary": "Long-term autonomy for mobile robots requires both robust self-localization\nand reliable map maintenance. Conventional landmark-based methods face a\nfundamental trade-off between landmarks with high detectability but low\ndistinctiveness (e.g., poles) and those with high distinctiveness but difficult\nstable detection (e.g., local point cloud structures). This work addresses the\nchallenge of descriptively identifying a unique \"signature\" (local point cloud)\nby leveraging a detectable, high-precision \"anchor\" (like a pole). To solve\nthis, we propose a novel canonical representation, \"Pole-Image,\" as a hybrid\nmethod that uses poles as anchors to generate signatures from the surrounding\n3D structure. Pole-Image represents a pole-like landmark and its surrounding\nenvironment, detected from a LiDAR point cloud, as a 2D polar coordinate image\nwith the pole itself as the origin. This representation leverages the pole's\nnature as a high-precision reference point, explicitly encoding the \"relative\ngeometry\" between the stable pole and the variable surrounding point cloud. The\nkey advantage of pole landmarks is that \"detection\" is extremely easy. This\nease of detection allows the robot to easily track the same pole, enabling the\nautomatic and large-scale collection of diverse observational data (positive\npairs). This data acquisition feasibility makes \"Contrastive Learning (CL)\"\napplicable. By applying CL, the model learns a viewpoint-invariant and highly\ndiscriminative descriptor. The contributions are twofold: 1) The descriptor\novercomes perceptual aliasing, enabling robust self-localization. 2) The\nhigh-precision encoding enables high-sensitivity change detection, contributing\nto map maintenance."}
{"id": "2510.17726", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.17726", "abs": "https://arxiv.org/abs/2510.17726", "authors": ["Md. Faiyaz Abdullah Sayeedi", "Md. Sadman Haque", "Zobaer Ibn Razzaque", "Robiul Awoul Robin", "Sabila Nawshin"], "title": "Rethinking Search: A Study of University Students' Perspectives on Using LLMs and Traditional Search Engines in Academic Problem Solving", "comment": "Acctepted at the EMNLP 2025 HCI+NLP Workshop", "summary": "With the increasing integration of Artificial Intelligence (AI) in academic\nproblem solving, university students frequently alternate between traditional\nsearch engines like Google and large language models (LLMs) for information\nretrieval. This study explores students' perceptions of both tools, emphasizing\nusability, efficiency, and their integration into academic workflows. Employing\na mixed-methods approach, we surveyed 109 students from diverse disciplines and\nconducted in-depth interviews with 12 participants. Quantitative analyses,\nincluding ANOVA and chi-square tests, were used to assess differences in\nefficiency, satisfaction, and tool preference. Qualitative insights revealed\nthat students commonly switch between GPT and Google: using Google for\ncredible, multi-source information and GPT for summarization, explanation, and\ndrafting. While neither tool proved sufficient on its own, there was a strong\ndemand for a hybrid solution. In response, we developed a prototype, a chatbot\nembedded within the search interface, that combines GPT's conversational\ncapabilities with Google's reliability to enhance academic research and reduce\ncognitive load."}
{"id": "2510.17249", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17249", "abs": "https://arxiv.org/abs/2510.17249", "authors": ["Franek Stark", "Rohit Kumar", "Shubham Vyas", "Hannah Isermann", "Jonas Haack", "Mihaela Popescu", "Jakob Middelberg", "Dennis Mronga", "Frank Kirchner"], "title": "An adaptive hierarchical control framework for quadrupedal robots in planetary exploration", "comment": "Presented at 18th Symposium on Advanced Space Technologies in\n  Robotics and Automation (ASTRA)", "summary": "Planetary exploration missions require robots capable of navigating extreme\nand unknown environments. While wheeled rovers have dominated past missions,\ntheir mobility is limited to traversable surfaces. Legged robots, especially\nquadrupeds, can overcome these limitations by handling uneven, obstacle-rich,\nand deformable terrains. However, deploying such robots in unknown conditions\nis challenging due to the need for environment-specific control, which is\ninfeasible when terrain and robot parameters are uncertain. This work presents\na modular control framework that combines model-based dynamic control with\nonline model adaptation and adaptive footstep planning to address uncertainties\nin both robot and terrain properties. The framework includes state estimation\nfor quadrupeds with and without contact sensing, supports runtime\nreconfiguration, and is integrated into ROS 2 with open-source availability.\nIts performance was validated on two quadruped platforms, multiple hardware\narchitectures, and in a volcano field test, where the robot walked over 700 m."}
{"id": "2510.17753", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.17753", "abs": "https://arxiv.org/abs/2510.17753", "authors": ["Celeste Riley", "Omar Al-Refai", "Yadira Colunga Reyes", "Eman Hammad"], "title": "Human-AI Interactions: Cognitive, Behavioral, and Emotional Impacts", "comment": "13 pages, 1 figure. Submitted to IEEE Transactions on Technology and\n  Society. Preprint also available on TechRxiv", "summary": "As stories of human-AI interactions continue to be highlighted in the news\nand research platforms, the challenges are becoming more pronounced, including\npotential risks of overreliance, cognitive offloading, social and emotional\nmanipulation, and the nuanced degradation of human agency and judgment. This\npaper surveys recent research on these issues through the lens of the\npsychological triad: cognition, behavior, and emotion. Observations seem to\nsuggest that while AI can substantially enhance memory, creativity, and\nengagement, it also introduces risks such as diminished critical thinking,\nskill erosion, and increased anxiety. Emotional outcomes are similarly mixed,\nwith AI systems showing promise for support and stress reduction, but raising\nconcerns about dependency, inappropriate attachments, and ethical oversight.\nThis paper aims to underscore the need for responsible and context-aware AI\ndesign, highlighting gaps for longitudinal research and grounded evaluation\nframeworks to balance benefits with emerging human-centric risks."}
{"id": "2510.17261", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17261", "abs": "https://arxiv.org/abs/2510.17261", "authors": ["Fernando Salanova", "Jesús Roche", "Cristian Mahuela", "Eduardo Montijano"], "title": "High-Level Multi-Robot Trajectory Planning And Spurious Behavior Detection", "comment": "6 pages,3 figures, Iberian Robotics Conference 2025", "summary": "The reliable execution of high-level missions in multi-robot systems with\nheterogeneous agents, requires robust methods for detecting spurious behaviors.\nIn this paper, we address the challenge of identifying spurious executions of\nplans specified as a Linear Temporal Logic (LTL) formula, as incorrect task\nsequences, violations of spatial constraints, timing inconsis- tencies, or\ndeviations from intended mission semantics. To tackle this, we introduce a\nstructured data generation framework based on the Nets-within-Nets (NWN)\nparadigm, which coordinates robot actions with LTL-derived global mission\nspecifications. We further propose a Transformer-based anomaly detection\npipeline that classifies robot trajectories as normal or anomalous. Experi-\nmental evaluations show that our method achieves high accuracy (91.3%) in\nidentifying execution inefficiencies, and demonstrates robust detection\ncapabilities for core mission violations (88.3%) and constraint-based adaptive\nanomalies (66.8%). An ablation experiment of the embedding and architecture was\ncarried out, obtaining successful results where our novel proposition performs\nbetter than simpler representations."}
{"id": "2510.16435", "categories": ["cs.RO", "cs.CL", "cs.HC", "I.2.9; H.5.2; H.5.0; I.2.8; I.2.7; J.4"], "pdf": "https://arxiv.org/pdf/2510.16435", "abs": "https://arxiv.org/abs/2510.16435", "authors": ["Lennart Wachowiak", "Andrew Coles", "Gerard Canal", "Oya Celiktutan"], "title": "What Questions Should Robots Be Able to Answer? A Dataset of User Questions for Explainable Robotics", "comment": null, "summary": "With the growing use of large language models and conversational interfaces\nin human-robot interaction, robots' ability to answer user questions is more\nimportant than ever. We therefore introduce a dataset of 1,893 user questions\nfor household robots, collected from 100 participants and organized into 12\ncategories and 70 subcategories. Most work in explainable robotics focuses on\nwhy-questions. In contrast, our dataset provides a wide variety of questions,\nfrom questions about simple execution details to questions about how the robot\nwould act in hypothetical scenarios -- thus giving roboticists valuable\ninsights into what questions their robot needs to be able to answer. To collect\nthe dataset, we created 15 video stimuli and 7 text stimuli, depicting robots\nperforming varied household tasks. We then asked participants on Prolific what\nquestions they would want to ask the robot in each portrayed situation. In the\nfinal dataset, the most frequent categories are questions about task execution\ndetails (22.5%), the robot's capabilities (12.7%), and performance assessments\n(11.3%). Although questions about how robots would handle potentially difficult\nscenarios and ensure correct behavior are less frequent, users rank them as the\nmost important for robots to be able to answer. Moreover, we find that users\nwho identify as novices in robotics ask different questions than more\nexperienced users. Novices are more likely to inquire about simple facts, such\nas what the robot did or the current state of the environment. As robots enter\nenvironments shared with humans and language becomes central to giving\ninstructions and interaction, this dataset provides a valuable foundation for\n(i) identifying the information robots need to log and expose to conversational\ninterfaces, (ii) benchmarking question-answering modules, and (iii) designing\nexplanation strategies that align with user expectations."}
{"id": "2510.17270", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.17270", "abs": "https://arxiv.org/abs/2510.17270", "authors": ["Lucas Schulze", "Juliano Decico Negri", "Victor Barasuol", "Vivian Suzano Medeiros", "Marcelo Becker", "Jan Peters", "Oleg Arenz"], "title": "Floating-Base Deep Lagrangian Networks", "comment": null, "summary": "Grey-box methods for system identification combine deep learning with\nphysics-informed constraints, capturing complex dependencies while improving\nout-of-distribution generalization. Yet, despite the growing importance of\nfloating-base systems such as humanoids and quadrupeds, current grey-box models\nignore their specific physical constraints. For instance, the inertia matrix is\nnot only positive definite but also exhibits branch-induced sparsity and input\nindependence. Moreover, the 6x6 composite spatial inertia of the floating base\ninherits properties of single-rigid-body inertia matrices. As we show, this\nincludes the triangle inequality on the eigenvalues of the composite rotational\ninertia. To address the lack of physical consistency in deep learning models of\nfloating-base systems, we introduce a parameterization of inertia matrices that\nsatisfies all these constraints. Inspired by Deep Lagrangian Networks (DeLaN),\nwe train neural networks to predict physically plausible inertia matrices that\nminimize inverse dynamics error under Lagrangian mechanics. For evaluation, we\ncollected and released a dataset on multiple quadrupeds and humanoids. In these\nexperiments, our Floating-Base Deep Lagrangian Networks (FeLaN) achieve highly\ncompetitive performance on both simulated and real robots, while providing\ngreater physical interpretability."}
{"id": "2510.17576", "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.17576", "abs": "https://arxiv.org/abs/2510.17576", "authors": ["Cansu Erdogan", "Cesar Alan Contreras", "Alireza Rastegarpanah", "Manolis Chiou", "Rustam Stolkin"], "title": "Intent-Driven LLM Ensemble Planning for Flexible Multi-Robot Disassembly: Demonstration on EV Batteries", "comment": "This work is funded by the project called \"Research and Development\n  of a Highly Automated and Safe Streamlined Process for Increasing Lithium-ion\n  Battery Repurposing and Recycling\" (REBELION) under Grant 101104241, and\n  partially supported by the Ministry of National Education, Republic of\n  Turkey. Submitted to Frontiers for Review", "summary": "This paper addresses the problem of planning complex manipulation tasks, in\nwhich multiple robots with different end-effectors and capabilities, informed\nby computer vision, must plan and execute concatenated sequences of actions on\na variety of objects that can appear in arbitrary positions and configurations\nin unstructured scenes. We propose an intent-driven planning pipeline which can\nrobustly construct such action sequences with varying degrees of supervisory\ninput from a human using simple language instructions. The pipeline integrates:\n(i) perception-to-text scene encoding, (ii) an ensemble of large language\nmodels (LLMs) that generate candidate removal sequences based on the operator's\nintent, (iii) an LLM-based verifier that enforces formatting and precedence\nconstraints, and (iv) a deterministic consistency filter that rejects\nhallucinated objects. The pipeline is evaluated on an example task in which two\nrobot arms work collaboratively to dismantle an Electric Vehicle battery for\nrecycling applications. A variety of components must be grasped and removed in\nspecific sequences, determined by human instructions and/or by task-order\nfeasibility decisions made by the autonomous system. On 200 real scenes with\n600 operator prompts across five component classes, we used metrics of\nfull-sequence correctness and next-task correctness to evaluate and compare\nfive LLM-based planners (including ablation analyses of pipeline components).\nWe also evaluated the LLM-based human interface in terms of time to execution\nand NASA TLX with human participant experiments. Results indicate that our\nensemble-with-verification approach reliably maps operator intent to safe,\nexecutable multi-robot plans while maintaining low user effort."}
{"id": "2510.17315", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17315", "abs": "https://arxiv.org/abs/2510.17315", "authors": ["Po-Chen Ko", "Jiayuan Mao", "Yu-Hsiang Fu", "Hsien-Jeng Yeh", "Chu-Rong Chen", "Wei-Chiu Ma", "Yilun Du", "Shao-Hua Sun"], "title": "Implicit State Estimation via Video Replanning", "comment": null, "summary": "Video-based representations have gained prominence in planning and\ndecision-making due to their ability to encode rich spatiotemporal dynamics and\ngeometric relationships. These representations enable flexible and\ngeneralizable solutions for complex tasks such as object manipulation and\nnavigation. However, existing video planning frameworks often struggle to adapt\nto failures at interaction time due to their inability to reason about\nuncertainties in partially observed environments. To overcome these\nlimitations, we introduce a novel framework that integrates interaction-time\ndata into the planning process. Our approach updates model parameters online\nand filters out previously failed plans during generation. This enables\nimplicit state estimation, allowing the system to adapt dynamically without\nexplicitly modeling unknown state variables. We evaluate our framework through\nextensive experiments on a new simulated manipulation benchmark, demonstrating\nits ability to improve replanning performance and advance the field of\nvideo-based decision-making."}
{"id": "2510.17335", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17335", "abs": "https://arxiv.org/abs/2510.17335", "authors": ["Xintong Yang", "Minglun Wei", "Ze Ji", "Yu-Kun Lai"], "title": "DDBot: Differentiable Physics-based Digging Robot for Unknown Granular Materials", "comment": "Accepted as a regular paper by the IEEE Transactions on Robotics", "summary": "Automating the manipulation of granular materials poses significant\nchallenges due to complex contact dynamics, unpredictable material properties,\nand intricate system states. Existing approaches often fail to achieve\nefficiency and accuracy in such tasks. To fill the research gap, this paper\nstudies the small-scale and high-precision granular material digging task with\nunknown physical properties. A new framework, named differentiable digging\nrobot (DDBot), is proposed to manipulate granular materials, including sand and\nsoil.\n  Specifically, we equip DDBot with a differentiable physics-based simulator,\ntailored for granular material manipulation, powered by GPU-accelerated\nparallel computing and automatic differentiation. DDBot can perform efficient\ndifferentiable system identification and high-precision digging skill\noptimisation for unknown granular materials, which is enabled by a\ndifferentiable skill-to-action mapping, a task-oriented demonstration method,\ngradient clipping and line search-based gradient descent.\n  Experimental results show that DDBot can efficiently (converge within 5 to 20\nminutes) identify unknown granular material dynamics and optimise digging\nskills, with high-precision results in zero-shot real-world deployments,\nhighlighting its practicality. Benchmark results against state-of-the-art\nbaselines also confirm the robustness and efficiency of DDBot in such digging\ntasks."}
{"id": "2510.17341", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17341", "abs": "https://arxiv.org/abs/2510.17341", "authors": ["Fan Shao", "Satoshi Endo", "Sandra Hirche", "Fanny Ficuciello"], "title": "Interactive Force-Impedance Control", "comment": null, "summary": "Human collaboration with robots requires flexible role adaptation, enabling\nrobot to switch between active leader and passive follower. Effective role\nswitching depends on accurately estimating human intention, which is typically\nachieved through external force analysis, nominal robot dynamics, or\ndata-driven approaches. However, these methods are primarily effective in\ncontact-sparse environments. When robots under hybrid or unified\nforce-impedance control physically interact with active humans or non-passive\nenvironments, the robotic system may lose passivity and thus compromise safety.\nTo address this challenge, this paper proposes the unified Interactive\nForce-Impedance Control (IFIC) framework that adapts to the interaction power\nflow, ensuring effortless and safe interaction in contact-rich environments.\nThe proposed control architecture is formulated within a port-Hamiltonian\nframework, incorporating both interaction and task control ports, through which\nsystem passivity is guaranteed."}
{"id": "2510.17369", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17369", "abs": "https://arxiv.org/abs/2510.17369", "authors": ["Haochen Su", "Cristian Meo", "Francesco Stella", "Andrea Peirone", "Kai Junge", "Josie Hughes"], "title": "Bridging Embodiment Gaps: Deploying Vision-Language-Action Models on Soft Robots", "comment": "Accepted by NeurIPS 2025 SpaVLE workshop. 4 pages, 2 figures(in main\n  paper, excluding references and supplements)", "summary": "Robotic systems are increasingly expected to operate in human-centered,\nunstructured environments where safety, adaptability, and generalization are\nessential. Vision-Language-Action (VLA) models have been proposed as a language\nguided generalized control framework for real robots. However, their deployment\nhas been limited to conventional serial link manipulators. Coupled by their\nrigidity and unpredictability of learning based control, the ability to safely\ninteract with the environment is missing yet critical. In this work, we present\nthe deployment of a VLA model on a soft continuum manipulator to demonstrate\nautonomous safe human-robot interaction. We present a structured finetuning and\ndeployment pipeline evaluating two state-of-the-art VLA models (OpenVLA-OFT and\n$\\pi_0$) across representative manipulation tasks, and show while\nout-of-the-box policies fail due to embodiment mismatch, through targeted\nfinetuning the soft robot performs equally to the rigid counterpart. Our\nfindings highlight the necessity of finetuning for bridging embodiment gaps,\nand demonstrate that coupling VLA models with soft robots enables safe and\nflexible embodied AI in human-shared environments."}
{"id": "2510.17408", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.17408", "abs": "https://arxiv.org/abs/2510.17408", "authors": ["Halima I. Kure", "Jishna Retnakumari", "Augustine O. Nwajana", "Umar M. Ismail", "Bilyaminu A. Romo", "Ehigiator Egho-Promise"], "title": "Integrating Trustworthy Artificial Intelligence with Energy-Efficient Robotic Arms for Waste Sorting", "comment": "5 pages, 2 figures", "summary": "This paper presents a novel methodology that integrates trustworthy\nartificial intelligence (AI) with an energy-efficient robotic arm for\nintelligent waste classification and sorting. By utilizing a convolutional\nneural network (CNN) enhanced through transfer learning with MobileNetV2, the\nsystem accurately classifies waste into six categories: plastic, glass, metal,\npaper, cardboard, and trash. The model achieved a high training accuracy of\n99.8% and a validation accuracy of 80.5%, demonstrating strong learning and\ngeneralization. A robotic arm simulator is implemented to perform virtual\nsorting, calculating the energy cost for each action using Euclidean distance\nto ensure optimal and efficient movement. The framework incorporates key\nelements of trustworthy AI, such as transparency, robustness, fairness, and\nsafety, making it a reliable and scalable solution for smart waste management\nsystems in urban settings."}
{"id": "2510.17439", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17439", "abs": "https://arxiv.org/abs/2510.17439", "authors": ["Zhengshen Zhang", "Hao Li", "Yalun Dai", "Zhengbang Zhu", "Lei Zhou", "Chenchen Liu", "Dong Wang", "Francis E. H. Tay", "Sijin Chen", "Ziwei Liu", "Yuxiao Liu", "Xinghang Li", "Pan Zhou"], "title": "From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors", "comment": "Project page: https://falcon-vla.github.io/", "summary": "Existing vision-language-action (VLA) models act in 3D real-world but are\ntypically built on 2D encoders, leaving a spatial reasoning gap that limits\ngeneralization and adaptability. Recent 3D integration techniques for VLAs\neither require specialized sensors and transfer poorly across modalities, or\ninject weak cues that lack geometry and degrade vision-language alignment. In\nthis work, we introduce FALCON (From Spatial to Action), a novel paradigm that\ninjects rich 3D spatial tokens into the action head. FALCON leverages spatial\nfoundation models to deliver strong geometric priors from RGB alone, and\nincludes an Embodied Spatial Model that can optionally fuse depth, or pose for\nhigher fidelity when available, without retraining or architectural changes. To\npreserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced\nAction Head rather than being concatenated into the vision-language backbone.\nThese designs enable FALCON to address limitations in spatial representation,\nmodality transferability, and alignment. In comprehensive evaluations across\nthree simulation benchmarks and eleven real-world tasks, our proposed FALCON\nachieves state-of-the-art performance, consistently surpasses competitive\nbaselines, and remains robust under clutter, spatial-prompt conditioning, and\nvariations in object scale and height."}
{"id": "2510.17448", "categories": ["cs.RO", "math.DS"], "pdf": "https://arxiv.org/pdf/2510.17448", "abs": "https://arxiv.org/abs/2510.17448", "authors": ["Mirko Mizzoni", "Pieter van Goor", "Barbara Bazzana", "Antonio Franchi"], "title": "A Generalization of Input-Output Linearization via Dynamic Switching Between Melds of Output Functions", "comment": null, "summary": "This letter presents a systematic framework for switching between different\nsets of outputs for the control of nonlinear systems via feedback\nlinearization. We introduce the concept of a meld to formally define a valid,\nfeedback-linearizable subset of outputs that can be selected from a larger deck\nof possible outputs. The main contribution is a formal proof establishing that\nunder suitable dwell-time and compatibility conditions, it is possible to\nswitch between different melds while guaranteeing the uniform boundedness of\nthe system state. We further show that the error dynamics of the active outputs\nremain exponentially stable within each switching interval and that outputs\ncommon to consecutive melds are tracked seamlessly through transitions. The\nproposed theory is valid for any feedback linearizable nonlinear system, such\nas, e.g., robots, aerial and terrestrial vehicles, etc.. We demonstrate it on a\nsimple numerical simulation of a robotic manipulator."}
{"id": "2510.17525", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17525", "abs": "https://arxiv.org/abs/2510.17525", "authors": ["Simon Schaefer", "Helen Oleynikova", "Sandra Hirche", "Stefan Leutenegger"], "title": "HumanMPC - Safe and Efficient MAV Navigation among Humans", "comment": null, "summary": "Safe and efficient robotic navigation among humans is essential for\nintegrating robots into everyday environments. Most existing approaches focus\non simplified 2D crowd navigation and fail to account for the full complexity\nof human body dynamics beyond root motion. We present HumanMPC, a Model\nPredictive Control (MPC) framework for 3D Micro Air Vehicle (MAV) navigation\namong humans that combines theoretical safety guarantees with data-driven\nmodels for realistic human motion forecasting. Our approach introduces a novel\ntwist to reachability-based safety formulation that constrains only the initial\ncontrol input for safety while modeling its effects over the entire planning\nhorizon, enabling safe yet efficient navigation. We validate HumanMPC in both\nsimulated experiments using real human trajectories and in the real-world,\ndemonstrating its effectiveness across tasks ranging from goal-directed\nnavigation to visual servoing for human tracking. While we apply our method to\nMAVs in this work, it is generic and can be adapted by other platforms. Our\nresults show that the method ensures safety without excessive conservatism and\noutperforms baseline approaches in both efficiency and reliability."}
{"id": "2510.17541", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17541", "abs": "https://arxiv.org/abs/2510.17541", "authors": ["Xiaobo Zheng", "Pan Tang", "Defu Lin", "Shaoming He"], "title": "Distributed Spatial-Temporal Trajectory Optimization for Unmanned-Aerial-Vehicle Swarm", "comment": null, "summary": "Swarm trajectory optimization problems are a well-recognized class of\nmulti-agent optimal control problems with strong nonlinearity. However, the\nheuristic nature of needing to set the final time for agents beforehand and the\ntime-consuming limitation of the significant number of iterations prohibit the\napplication of existing methods to large-scale swarm of Unmanned Aerial\nVehicles (UAVs) in practice. In this paper, we propose a spatial-temporal\ntrajectory optimization framework that accomplishes multi-UAV consensus based\non the Alternating Direction Multiplier Method (ADMM) and uses Differential\nDynamic Programming (DDP) for fast local planning of individual UAVs. The\nintroduced framework is a two-level architecture that employs Parameterized DDP\n(PDDP) as the trajectory optimizer for each UAV, and ADMM to satisfy the local\nconstraints and accomplish the spatial-temporal parameter consensus among all\nUAVs. This results in a fully distributed algorithm called Distributed\nParameterized DDP (D-PDDP). In addition, an adaptive tuning criterion based on\nthe spectral gradient method for the penalty parameter is proposed to reduce\nthe number of algorithmic iterations. Several simulation examples are presented\nto verify the effectiveness of the proposed algorithm."}
{"id": "2510.17576", "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.17576", "abs": "https://arxiv.org/abs/2510.17576", "authors": ["Cansu Erdogan", "Cesar Alan Contreras", "Alireza Rastegarpanah", "Manolis Chiou", "Rustam Stolkin"], "title": "Intent-Driven LLM Ensemble Planning for Flexible Multi-Robot Disassembly: Demonstration on EV Batteries", "comment": "This work is funded by the project called \"Research and Development\n  of a Highly Automated and Safe Streamlined Process for Increasing Lithium-ion\n  Battery Repurposing and Recycling\" (REBELION) under Grant 101104241, and\n  partially supported by the Ministry of National Education, Republic of\n  Turkey. Submitted to Frontiers for Review", "summary": "This paper addresses the problem of planning complex manipulation tasks, in\nwhich multiple robots with different end-effectors and capabilities, informed\nby computer vision, must plan and execute concatenated sequences of actions on\na variety of objects that can appear in arbitrary positions and configurations\nin unstructured scenes. We propose an intent-driven planning pipeline which can\nrobustly construct such action sequences with varying degrees of supervisory\ninput from a human using simple language instructions. The pipeline integrates:\n(i) perception-to-text scene encoding, (ii) an ensemble of large language\nmodels (LLMs) that generate candidate removal sequences based on the operator's\nintent, (iii) an LLM-based verifier that enforces formatting and precedence\nconstraints, and (iv) a deterministic consistency filter that rejects\nhallucinated objects. The pipeline is evaluated on an example task in which two\nrobot arms work collaboratively to dismantle an Electric Vehicle battery for\nrecycling applications. A variety of components must be grasped and removed in\nspecific sequences, determined by human instructions and/or by task-order\nfeasibility decisions made by the autonomous system. On 200 real scenes with\n600 operator prompts across five component classes, we used metrics of\nfull-sequence correctness and next-task correctness to evaluate and compare\nfive LLM-based planners (including ablation analyses of pipeline components).\nWe also evaluated the LLM-based human interface in terms of time to execution\nand NASA TLX with human participant experiments. Results indicate that our\nensemble-with-verification approach reliably maps operator intent to safe,\nexecutable multi-robot plans while maintaining low user effort."}
{"id": "2510.17604", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17604", "abs": "https://arxiv.org/abs/2510.17604", "authors": ["Hao Qiao", "Yan Wang", "Shuo Yang", "Xiaoyao Yu", "Jian kuang", "Xiaoji Niu"], "title": "Learned Inertial Odometry for Cycling Based on Mixture of Experts Algorithm", "comment": null, "summary": "With the rapid growth of bike sharing and the increasing diversity of cycling\napplications, accurate bicycle localization has become essential. traditional\nGNSS-based methods suffer from multipath effects, while existing inertial\nnavigation approaches rely on precise modeling and show limited robustness.\nTight Learned Inertial Odometry (TLIO) achieves low position drift by combining\nraw IMU data with predicted displacements by neural networks, but its high\ncomputational cost restricts deployment on mobile devices. To overcome this, we\nextend TLIO to bicycle localization and introduce an improved Mixture-of\nExperts (MoE) model that reduces both training and inference costs. Experiments\nshow that, compared to the state-of-the-art LLIO framework, our method achieves\ncomparable accuracy while reducing parameters by 64.7% and computational cost\nby 81.8%."}
{"id": "2510.17640", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17640", "abs": "https://arxiv.org/abs/2510.17640", "authors": ["Yuquan Xue", "Guanxing Lu", "Zhenyu Wu", "Chuanrui Zhang", "Bofang Jia", "Zhengyi Gu", "Yansong Tang", "Ziwei Wang"], "title": "RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation", "comment": "9 pages,7 figures, submitted to ICRA2026", "summary": "Vision-Language-Action models (VLAs) have demonstrated remarkable performance\non complex robotic manipulation tasks through imitation learning. However,\nexisting imitation learning datasets contain only successful trajectories and\nlack failure or recovery data, especially for out-of-distribution (OOD) states\nwhere the robot deviates from the main policy due to minor perturbations or\nerrors, leading VLA models to struggle with states deviating from the training\ndistribution. To this end, we propose an automated OOD data augmentation\nframework named RESample through exploratory sampling. Specifically, we first\nleverage offline reinforcement learning to obtain an action-value network that\naccurately identifies sub-optimal actions under the current manipulation\npolicy. We further sample potential OOD states from trajectories via rollout,\nand design an exploratory sampling mechanism that adaptively incorporates these\naction proxies into the training dataset to ensure efficiency. Subsequently,\nour framework explicitly encourages the VLAs to recover from OOD states and\nenhances their robustness against distributional shifts. We conduct extensive\nexperiments on the LIBERO benchmark as well as real-world robotic manipulation\ntasks, demonstrating that RESample consistently improves the stability and\ngeneralization ability of VLA models."}
{"id": "2510.17783", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17783", "abs": "https://arxiv.org/abs/2510.17783", "authors": ["Simeon Adebola", "Chung Min Kim", "Justin Kerr", "Shuangyu Xie", "Prithvi Akella", "Jose Luis Susa Rincon", "Eugen Solowjow", "Ken Goldberg"], "title": "Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats", "comment": "2025 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS 2025)", "summary": "Commercial plant phenotyping systems using fixed cameras cannot perceive many\nplant details due to leaf occlusion. In this paper, we present Botany-Bot, a\nsystem for building detailed \"annotated digital twins\" of living plants using\ntwo stereo cameras, a digital turntable inside a lightbox, an industrial robot\narm, and 3D segmentated Gaussian Splat models. We also present robot algorithms\nfor manipulating leaves to take high-resolution indexable images of occluded\ndetails such as stem buds and the underside/topside of leaves. Results from\nexperiments suggest that Botany-Bot can segment leaves with 90.8% accuracy,\ndetect leaves with 86.2% accuracy, lift/push leaves with 77.9% accuracy, and\ntake detailed overside/underside images with 77.3% accuracy. Code, videos, and\ndatasets are available at https://berkeleyautomation.github.io/Botany-Bot/."}
{"id": "2510.17792", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17792", "abs": "https://arxiv.org/abs/2510.17792", "authors": ["Gabriel B. Margolis", "Michelle Wang", "Nolan Fey", "Pulkit Agrawal"], "title": "SoftMimic: Learning Compliant Whole-body Control from Examples", "comment": "Website: https://gmargo11.github.io/softmimic/", "summary": "We introduce SoftMimic, a framework for learning compliant whole-body control\npolicies for humanoid robots from example motions. Imitating human motions with\nreinforcement learning allows humanoids to quickly learn new skills, but\nexisting methods incentivize stiff control that aggressively corrects\ndeviations from a reference motion, leading to brittle and unsafe behavior when\nthe robot encounters unexpected contacts. In contrast, SoftMimic enables robots\nto respond compliantly to external forces while maintaining balance and\nposture. Our approach leverages an inverse kinematics solver to generate an\naugmented dataset of feasible compliant motions, which we use to train a\nreinforcement learning policy. By rewarding the policy for matching compliant\nresponses rather than rigidly tracking the reference motion, SoftMimic learns\nto absorb disturbances and generalize to varied tasks from a single motion\nclip. We validate our method through simulations and real-world experiments,\ndemonstrating safe and effective interaction with the environment."}
{"id": "2510.17801", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17801", "abs": "https://arxiv.org/abs/2510.17801", "authors": ["Yulin Luo", "Chun-Kai Fan", "Menghang Dong", "Jiayu Shi", "Mengdi Zhao", "Bo-Wen Zhang", "Cheng Chi", "Jiaming Liu", "Gaole Dai", "Rongyu Zhang", "Ruichuan An", "Kun Wu", "Zhengping Che", "Shaoxuan Xie", "Guocai Yao", "Zhongxia Zhao", "Pengwei Wang", "Guang Liu", "Zhongyuan Wang", "Tiejun Huang", "Shanghang Zhang"], "title": "Robobench: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models as Embodied Brain", "comment": null, "summary": "Building robots that can perceive, reason, and act in dynamic, unstructured\nenvironments remains a core challenge. Recent embodied systems often adopt a\ndual-system paradigm, where System 2 handles high-level reasoning while System\n1 executes low-level control. In this work, we refer to System 2 as the\nembodied brain, emphasizing its role as the cognitive core for reasoning and\ndecision-making in manipulation tasks. Given this role, systematic evaluation\nof the embodied brain is essential. Yet existing benchmarks emphasize execution\nsuccess, or when targeting high-level reasoning, suffer from incomplete\ndimensions and limited task realism, offering only a partial picture of\ncognitive capability. To bridge this gap, we introduce RoboBench, a benchmark\nthat systematically evaluates multimodal large language models (MLLMs) as\nembodied brains. Motivated by the critical roles across the full manipulation\npipeline, RoboBench defines five dimensions-instruction comprehension,\nperception reasoning, generalized planning, affordance prediction, and failure\nanalysis-spanning 14 capabilities, 25 tasks, and 6092 QA pairs. To ensure\nrealism, we curate datasets across diverse embodiments, attribute-rich objects,\nand multi-view scenes, drawing from large-scale real robotic data. For\nplanning, RoboBench introduces an evaluation framework,\nMLLM-as-world-simulator. It evaluate embodied feasibility by simulating whether\npredicted plans can achieve critical object-state changes. Experiments on 14\nMLLMs reveal fundamental limitations: difficulties with implicit instruction\ncomprehension, spatiotemporal reasoning, cross-scenario planning, fine-grained\naffordance understanding, and execution failure diagnosis. RoboBench provides a\ncomprehensive scaffold to quantify high-level cognition, and guide the\ndevelopment of next-generation embodied MLLMs. The project page is in\nhttps://robo-bench.github.io."}
