{"id": "2602.16806", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.16806", "abs": "https://arxiv.org/abs/2602.16806", "authors": ["Sutapa Dey Tithi", "Xiaoyi Tian", "Ally Limke", "Min Chi", "Tiffany Barnes"], "title": "Exploring the Design and Impact of Interactive Worked Examples for Learners with Varying Prior Knowledge", "comment": null, "summary": "Tutoring systems improve learning through tailored interventions, such as worked examples, but often suffer from the aptitude-treatment interaction effect where low prior knowledge learners benefit more. We applied the ICAP learning theory to design two new types of worked examples, Buggy (students fix bugs), and Guided (students complete missing rules), requiring varying levels of cognitive engagement, and investigated their impact on learning in a controlled experiment with 155 undergraduate students in a logic problem solving tutor. Students in the Buggy and Guided examples groups performed significantly better on the posttest than those receiving passive worked examples. Buggy problems helped high prior knowledge learners whereas Guided problems helped low prior knowledge learners. Behavior analysis showed that Buggy produced more exploration-revision cycles, while Guided led to more help-seeking and fewer errors. This research contributes to the design of interventions in logic problem solving for varied levels of learner knowledge and a novel application of behavior analysis to compare learner interactions with the tutor.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bbe\u8ba1\u4e86\u4e24\u79cd\u65b0\u7684\u5b9e\u4f8b\u7c7b\u578b\u4ee5\u6539\u5584\u5b66\u4e60\u6548\u679c\uff0c\u7ed3\u679c\u663e\u793a\u6839\u636e\u5b66\u4e60\u8005\u7684\u77e5\u8bc6\u6c34\u5e73\uff0c\u9488\u5bf9\u6027\u7684\u5e72\u9884\u80fd\u663e\u8457\u63d0\u9ad8\u5b66\u4e60\u6210\u7ee9\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u8bbe\u8ba1\u4e0d\u540c\u7c7b\u578b\u7684\u5b9e\u4f8b\u6765\u6539\u5584\u5b66\u4e60\u6548\u679c\uff0c\u7279\u522b\u662f\u8003\u8651\u5230\u5b66\u4e60\u8005\u7684\u4e0d\u540c\u77e5\u8bc6\u6c34\u5e73\u3002", "method": "\u5728\u63a7\u5236\u5b9e\u9a8c\u4e2d\u5bf9155\u540d\u672c\u79d1\u751f\u8fdb\u884c\u6d4b\u8bd5\uff0c\u8bbe\u8ba1\u4e86\u4e24\u79cd\u65b0\u7c7b\u578b\u7684\u793a\u4f8b\uff1aBuggy\uff08\u4fee\u590d\u9519\u8bef\uff09\u548cGuided\uff08\u8865\u5168\u89c4\u5219\uff09\uff0c\u5e76\u89c2\u5bdf\u5176\u5bf9\u5b66\u4e60\u6548\u679c\u7684\u5f71\u54cd\u3002", "result": "\u53c2\u4e0eBuggy\u548cGuided\u793a\u4f8b\u7ec4\u7684\u5b66\u751f\u5728\u540e\u6d4b\u4e2d\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u63a5\u53d7\u88ab\u52a8\u793a\u4f8b\u7684\u5b66\u751f\u3002Buggy\u95ee\u9898\u5bf9\u9ad8\u77e5\u8bc6\u6c34\u5e73\u7684\u5b66\u4e60\u8005\u5e2e\u52a9\u66f4\u5927\uff0c\u800cGuided\u95ee\u9898\u5219\u5bf9\u4f4e\u77e5\u8bc6\u6c34\u5e73\u7684\u5b66\u4e60\u8005\u66f4\u6709\u5229\u3002", "conclusion": "\u7814\u7a76\u4e3a\u903b\u8f91\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u5e72\u9884\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u8d21\u732e\uff0c\u7279\u522b\u662f\u9488\u5bf9\u4e0d\u540c\u5b66\u4e60\u8005\u77e5\u8bc6\u6c34\u5e73\u7684\u5e72\u9884\uff0c\u540c\u65f6\u4e5f\u5c55\u793a\u4e86\u884c\u4e3a\u5206\u6790\u5728\u6bd4\u8f83\u5b66\u4e60\u8005\u4e0e\u8f85\u5bfc\u5458\u4e92\u52a8\u4e2d\u7684\u65b0\u5e94\u7528\u3002"}}
{"id": "2602.16820", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16820", "abs": "https://arxiv.org/abs/2602.16820", "authors": ["Xinyi Lu", "Kexin Phyllis Ju", "Mitchell Dudley", "Larissa Sano", "Xu Wang"], "title": "AI-Mediated Feedback Improves Student Revisions: A Randomized Trial with FeedbackWriter in a Large Undergraduate Course", "comment": null, "summary": "Despite growing interest in using LLMs to generate feedback on students' writing, little is known about how students respond to AI-mediated versus human-provided feedback. We address this gap through a randomized controlled trial in a large introductory economics course (N=354), where we introduce and deploy FeedbackWriter - a system that generates AI suggestions to teaching assistants (TAs) while they provide feedback on students' knowledge-intensive essays. TAs have the full capacity to adopt, edit, or dismiss the suggestions. Students were randomly assigned to receive either handwritten feedback from TAs (baseline) or AI-mediated feedback where TAs received suggestions from FeedbackWriter. Students revise their drafts based on the feedback, which is further graded. In total, 1,366 essays were graded using the system. We found that students receiving AI-mediated feedback produced significantly higher-quality revisions, with gains increasing as TAs adopted more AI suggestions. TAs found the AI suggestions useful for spotting gaps and clarifying rubrics.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0cAI\u4e2d\u4ecb\u7684\u53cd\u9988\u80fd\u6709\u6548\u63d0\u5347\u5b66\u751f\u8bba\u6587\u4fee\u8ba2\u8d28\u91cf\uff0c\u5c24\u5176\u5f53\u6559\u5e08\u52a9\u7406\u91c7\u7eb3\u66f4\u591aAI\u5efa\u8bae\u65f6\u6548\u679c\u66f4\u4f73\u3002", "motivation": "\u968f\u7740\u5bf9\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u5b66\u751f\u5199\u4f5c\u53cd\u9988\u7684\u5174\u8da3\u589e\u52a0\uff0c\u5c1a\u7f3a\u4e4f\u5bf9AI\u4e0e\u4eba\u5de5\u53cd\u9988\u5bf9\u5b66\u751f\u53cd\u5e94\u7684\u6bd4\u8f83\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u5728\u4e00\u95e8\u5927\u578b\u7ecf\u6d4e\u5b66\u5165\u95e8\u8bfe\u7a0b\u4e2d\u8fdb\u884c\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\uff0c\u5f15\u5165FeedbackWriter\u7cfb\u7edf\uff0c\u4e3a\u6559\u5e08\u52a9\u7406\u63d0\u4f9bAI\u5efa\u8bae\uff0c\u4ece\u800c\u5bf9\u5b66\u751f\u7684\u77e5\u8bc6\u5bc6\u96c6\u578b\u8bba\u6587\u63d0\u4f9b\u53cd\u9988\u3002", "result": "\u5b66\u751f\u63a5\u53d7AI\u4e2d\u4ecb\u7684\u53cd\u9988\u540e\uff0c\u4fee\u8ba2\u7684\u8bba\u6587\u8d28\u91cf\u663e\u8457\u63d0\u9ad8\uff0c\u4e14\u5f53\u6559\u5e08\u52a9\u7406\uff08TAs\uff09\u91c7\u7eb3\u66f4\u591a\u7684AI\u5efa\u8bae\u65f6\uff0c\u8d28\u91cf\u63d0\u5347\u5e45\u5ea6\u9010\u6e10\u589e\u52a0\u3002", "conclusion": "AI\u63d0\u4f9b\u7684\u5efa\u8bae\u5bf9\u4e8e\u63d0\u9ad8\u5b66\u751f\u5199\u4f5c\u8bba\u6587\u7684\u8d28\u91cf\u5177\u6709\u79ef\u6781\u5f71\u54cd\uff0c\u800c\u6559\u5e08\u52a9\u7406\u901a\u8fc7\u8fd9\u4e9b\u5efa\u8bae\u80fd\u591f\u66f4\u597d\u5730\u53d1\u73b0\u95ee\u9898\u548c\u6f84\u6e05\u8bc4\u5206\u6807\u51c6\u3002"}}
{"id": "2602.16844", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16844", "abs": "https://arxiv.org/abs/2602.16844", "authors": ["Madeleine Grunde-McLaughlin", "Hussein Mozannar", "Maya Murad", "Jingya Chen", "Saleema Amershi", "Adam Fourney"], "title": "Overseeing Agents Without Constant Oversight: Challenges and Opportunities", "comment": null, "summary": "To enable human oversight, agentic AI systems often provide a trace of reasoning and action steps. Designing traces to have an informative, but not overwhelming, level of detail remains a critical challenge. In three user studies on a Computer User Agent, we investigate the utility of basic action traces for verification, explore three alternatives via design probes, and test a novel interface's impact on error finding in question-answering tasks. As expected, we find that current practices are cumbersome, limiting their efficacy. Conversely, our proposed design reduced the time participants spent finding errors. However, although participants reported higher levels of confidence in their decisions, their final accuracy was not meaningfully improved. To this end, our study surfaces challenges for human verification of agentic systems, including managing built-in assumptions, users' subjective and changing correctness criteria, and the shortcomings, yet importance, of communicating the agent's process.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4ee3\u7406\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u63a8\u7406\u8bb0\u5f55\u5728\u4eba\u5de5\u76d1\u7ba1\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5206\u6790\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u548c\u65b0\u8bbe\u8ba1\u5e26\u6765\u7684\u6539\u8fdb\uff0c\u5c3d\u7ba1\u63d0\u9ad8\u4e86\u7528\u6237\u4fe1\u5fc3\uff0c\u4f46\u672a\u6709\u6548\u63d0\u5347\u51b3\u7b56\u51c6\u786e\u6027\u3002", "motivation": "\u786e\u4fdd\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u5728\u667a\u80fd\u51b3\u7b56\u4e2d\u80fd\u591f\u88ab\u4eba\u7c7b\u76d1\u7763\uff0c\u63d0\u4f9b\u5408\u7406\u7684\u63a8\u7406\u548c\u884c\u52a8\u6b65\u9aa4\u7684\u8bb0\u5f55\u3002", "method": "\u901a\u8fc7\u4e09\u9879\u7528\u6237\u7814\u7a76\uff0c\u63a2\u8ba8\u57fa\u672c\u884c\u52a8\u8bb0\u5f55\u7684\u5b9e\u7528\u6027\uff0c\u5e76\u901a\u8fc7\u8bbe\u8ba1\u63a2\u9488\u63a2\u8ba8\u4e09\u79cd\u66ff\u4ee3\u65b9\u6848\uff0c\u6d4b\u8bd5\u4e00\u79cd\u65b0\u63a5\u53e3\u5728\u95ee\u9898\u56de\u7b54\u4efb\u52a1\u4e2d\u5bf9\u9519\u8bef\u53d1\u73b0\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u5f53\u524d\u7684\u505a\u6cd5\u7b28\u62d9\uff0c\u5f71\u54cd\u4e86\u5176\u6709\u6548\u6027\uff0c\u7136\u800c\uff0c\u63d0\u51fa\u7684\u8bbe\u8ba1\u51cf\u5c11\u4e86\u53c2\u4e0e\u8005\u53d1\u73b0\u9519\u8bef\u7684\u65f6\u95f4\u3002\u867d\u7136\u53c2\u4e0e\u8005\u5bf9\u4e8e\u51b3\u5b9a\u7684\u4fe1\u5fc3\u63d0\u9ad8\uff0c\u4f46\u6700\u7ec8\u51c6\u786e\u6027\u5e76\u6ca1\u6709\u663e\u8457\u6539\u5584\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u4eba\u7c7b\u9a8c\u8bc1\u4ee3\u7406\u7cfb\u7edf\u9762\u4e34\u7684\u6311\u6218\uff0c\u5305\u62ec\u7ba1\u7406\u56fa\u6709\u5047\u8bbe\u3001\u7528\u6237\u4e3b\u89c2\u548c\u53d8\u5316\u7684\u6b63\u786e\u6027\u6807\u51c6\uff0c\u4ee5\u53ca\u5728\u6c9f\u901a\u4ee3\u7406\u8fc7\u7a0b\u4e2d\u7684\u77ed\u677f\u548c\u91cd\u8981\u6027\u3002"}}
{"id": "2602.16853", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.16853", "abs": "https://arxiv.org/abs/2602.16853", "authors": ["Inha Cha", "Yeonju Jang", "Haesoo Kim", "Joo Young Park", "Seora Park", "EunJeong Cheon"], "title": "\"My body is not your Porn\": Identifying Trends of Harm and Oppression through a Sociotechnical Genealogy of Digital Sexual Violence in South Korea", "comment": null, "summary": "Ever since the introduction of internet technologies in South Korea, digital sexual violence (DSV) has been a persistent and pervasive problem. Evolving alongside digital technologies, the severity and scale of violence have grown consistently, leading to widespread public concern. In this paper, we present four eras of image-based DSV in South Korea, spanning from the early internet era of the 1990s to the deepfake scandals in the mid-2020s. Drawing from media coverage, legal documents, and academic literature, we elucidate forms and characteristics of DSV cases in each era, tracing how entrenched misogyny is reconfigured and amplified through evolving technologies, alongside shifting legislative measures. Taking a genealogical approach to read prominent cases of different eras, our analysis identifies three constitutive and interconnected dimensions of DSV: (1) the homo-social fabrication of \"obscenity\", wherein victims' imagery becomes collectively framed as obscene through participatory practices in male-dominant networks; (2) the increasing imperceptibility of violence, as technologies foreclose victims' ability to perceive harm; and (3) the commercialization of abuse through decentralized economic infrastructures. We suggest future directions for CSCW research, and further reflect on the value of the genealogical method in enabling non-linear understanding of DSV as dynamically evolving sociotechnical configurations of harm.", "AI": {"tldr": "\u672c\u8bba\u6587\u5206\u6790\u4e86\u97e9\u56fd\u6570\u5b57\u6027\u66b4\u529b\u7684\u56db\u4e2a\u65f6\u4ee3\uff0c\u63a2\u8ba8\u4e86\u4e0d\u540c\u6280\u672f\u548c\u7acb\u6cd5\u80cc\u666f\u4e0b\u6027\u522b\u6b67\u89c6\u7684\u91cd\u65b0\u6784\u5efa\u53ca\u5176\u5bf9\u53d7\u5bb3\u8005\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u4e92\u8054\u7f51\u79d1\u6280\u7684\u53d1\u5c55\uff0c\u97e9\u56fd\u7684\u6570\u5b57\u6027\u66b4\u529b\u95ee\u9898\u6108\u53d1\u4e25\u91cd\uff0c\u5f15\u8d77\u4e86\u516c\u4f17\u7684\u5e7f\u6cdb\u5173\u6ce8\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u5bf9\u5176\u6f14\u53d8\u53ca\u5f71\u54cd\u8fdb\u884c\u6df1\u5165\u7814\u7a76\u3002", "method": "\u91c7\u7528\u57fa\u56e0\u8c31\u65b9\u6cd5\uff0c\u5206\u6790\u4e0d\u540c\u5e74\u4ee3\u7684DSV\u7a81\u53d1\u4e8b\u4ef6\uff0c\u7ed3\u5408\u5a92\u4f53\u62a5\u9053\u3001\u6cd5\u5f8b\u6587\u4ef6\u548c\u5b66\u672f\u6587\u732e\u8fdb\u884c\u7efc\u5408\u7814\u7a76\u3002", "result": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u97e9\u56fd\u6570\u5b57\u6027\u66b4\u529b\uff08DSV\uff09\u5728\u591a\u4e2a\u65f6\u4ee3\u7684\u6f14\u53d8\u53ca\u5176\u7279\u5f81\uff0c\u4ece1990\u5e74\u4ee3\u521d\u671f\u4e92\u8054\u7f51\u65f6\u4ee3\u52302020\u5e74\u4ee3\u4e2d\u671f\u7684\u6df1\u5ea6\u4f2a\u9020\u4e11\u95fb\uff0c\u5206\u6790\u4e86\u76f8\u5173\u7684\u6cd5\u5f8b\u3001\u5a92\u4f53\u548c\u5b66\u672f\u6587\u732e\u3002", "conclusion": "\u7814\u7a76\u8bc6\u522b\u4e86DSV\u7684\u4e09\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u7ef4\u5ea6\uff0c\u5e76\u5efa\u8bae\u4eca\u540e\u7684\u793e\u533a\u7cfb\u7edf\u8ba1\u7b97\u4e0e\u5de5\u4f5c\uff08CSCW\uff09\u7814\u7a76\u65b9\u5411\uff0c\u5f3a\u8c03\u4e86\u57fa\u56e0\u8c31\u65b9\u6cd5\u5728\u7406\u89e3DSV\u7684\u52a8\u6001\u6027\u548c\u590d\u6742\u6027\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.16744", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.16744", "abs": "https://arxiv.org/abs/2602.16744", "authors": ["Takuro Kato", "Mitsuharu Morisawa"], "title": "ICP-Based Pallet Tracking for Unloading on Inclined Surfaces by Autonomous Forklifts", "comment": "Accepted and published in IEEE/SICE SII 2024", "summary": "This paper proposes a control method for autonomous forklifts to unload pallets on inclined surfaces, enabling the fork to be withdrawn without dragging the pallets. The proposed method applies the Iterative Closest Point (ICP) algorithm to point clouds measured from the upper region of the pallet and thereby tracks the relative position and attitude angle difference between the pallet and the fork during the unloading operation in real-time. According to the tracking result, the fork is aligned parallel to the target surface. After the fork is aligned, it is possible to complete the unloading process by withdrawing the fork along the tilt, preventing any dragging of the pallet. The effectiveness of the proposed method is verified through dynamic simulations and experiments using a real forklift that replicate unloading operations onto the inclined bed of a truck.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a7\u5236\u65b9\u6cd5\uff0c\u5229\u7528ICP\u7b97\u6cd5\u5728\u503e\u659c\u8868\u9762\u4e0a\u5b9e\u73b0\u81ea\u52a8\u53c9\u8f66\u5378\u8d27\uff0c\u786e\u4fdd\u5378\u8d27\u8fc7\u7a0b\u4e0d\u62d6\u52a8\u6258\u76d8\u3002", "motivation": "\u63d0\u5347\u81ea\u52a8\u53c9\u8f66\u5728\u503e\u659c\u8868\u9762\u5378\u8d27\u7684\u80fd\u529b\uff0c\u786e\u4fdd\u6258\u76d8\u5728\u5378\u8d27\u8fc7\u7a0b\u4e2d\u7684\u7a33\u5b9a\u6027\u3002", "method": "\u91c7\u7528ICP\u7b97\u6cd5\u5904\u7406\u6258\u76d8\u4e0a\u65b9\u7684\u70b9\u4e91\u6570\u636e\uff0c\u5b9e\u65f6\u8ddf\u8e2a\u6258\u76d8\u4e0e\u53c9\u5b50\u4e4b\u95f4\u7684\u4f4d\u7f6e\u548c\u89d2\u5ea6\u5dee\u5f02\uff0c\u4ece\u800c\u5b9e\u73b0\u53c9\u5b50\u4e0e\u76ee\u6807\u8868\u9762\u7684\u5bf9\u9f50\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u53c9\u8f66\u5728\u503e\u659c\u8868\u9762\u4e0a\u5378\u8d27\u7684\u63a7\u5236\u65b9\u6cd5\uff0c\u4ee5\u4fbf\u5728\u4e0d\u62d6\u52a8\u6258\u76d8\u7684\u60c5\u51b5\u4e0b\u62d4\u51fa\u53c9\u5b50\u3002", "conclusion": "\u901a\u8fc7\u52a8\u6001\u4eff\u771f\u548c\u4f7f\u7528\u771f\u5b9e\u53c9\u8f66\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u6709\u6548\u5730\u8fdb\u884c\u503e\u659c\u5378\u8d27\u3002"}}
{"id": "2602.16893", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.16893", "abs": "https://arxiv.org/abs/2602.16893", "authors": ["Riku Arakawa", "Shreya Bali", "Anupama Sitaraman", "Woosuk Seo", "Sam Shaaban", "Oliver Lindheim", "Traci M. Kennedy", "Mayank Goel"], "title": "CalmReminder: A Design Probe for Parental Engagement with Children with Hyperactivity, Augmented by Real-Time Motion Sensing with a Watch", "comment": null, "summary": "Families raising children with ADHD often experience heightened stress and reactive parenting. While digital interventions promise personalization, many remain one-size-fits-all and fail to reflect parents' lived practices. We present CalmReminder, a watch-based system that detects children's calm moments and delivers just-in-time prompts to parents. Through a four-week deployment with 16 families (twelve completed) of children with ADHD, we compared notification strategies ranging from hourly to random to only when the child was inferred to be calm. Our sensing-based notifications were frequently perceived as arriving during calm moments. More importantly, parents adopted the system in diverse ways: using notifications for praise, mindfulness, activity planning, or conversation. These findings show that parents are not passive recipients but active designers, reshaping interventions to fit their parenting styles. We contribute a calm detection pipeline, empirical insights into families' flexible appropriation of notifications, and design implications for intervention systems that foster agency.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86CalmReminder\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u65e8\u5728\u901a\u8fc7\u68c0\u6d4b\u513f\u7ae5\u7684\u5e73\u9759\u65f6\u523b\uff0c\u5411\u5bb6\u957f\u53d1\u9001\u65f6\u673a\u5408\u9002\u7684\u63d0\u793a\u3002\u7814\u7a76\u8868\u660e\uff0c\u5bb6\u957f\u80fd\u591f\u4ee5\u7075\u6d3b\u591a\u6837\u7684\u65b9\u5f0f\u5e94\u7528\u901a\u77e5\uff0c\u4ece\u800c\u6539\u5584\u4eb2\u5b50\u4e92\u52a8\u3002", "motivation": "\u5bb6\u5ead\u5728\u517b\u80b2\u6709ADHD\u513f\u7ae5\u65f6\u5e38\u9762\u4e34\u8f83\u5927\u538b\u529b\uff0c\u800c\u6570\u5b57\u5e72\u9884\u63aa\u65bd\u9700\u8981\u66f4\u4e2a\u6027\u5316\uff0c\u4ee5\u53cd\u6620\u5bb6\u957f\u7684\u771f\u5b9e\u80b2\u513f\u5b9e\u8df5\u3002", "method": "\u901a\u8fc7\u56db\u5468\u7684\u5b9e\u5730\u90e8\u7f72\u6d4b\u8bd5CalmReminder\u7cfb\u7edf\uff0c\u62db\u52df\u4e8616\u4e2aADHD\u513f\u7ae5\u7684\u5bb6\u5ead\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u7684\u901a\u77e5\u7b56\u7565\u3002", "result": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u4e00\u79cd\u540d\u4e3aCalmReminder\u7684\u624b\u8868\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u68c0\u6d4b\u513f\u7ae5\u7684\u5e73\u9759\u65f6\u523b\uff0c\u5e76\u5728\u9002\u5f53\u65f6\u523b\u5411\u5bb6\u957f\u53d1\u9001\u63d0\u793a\uff0c\u5e2e\u52a9\u5bb6\u957f\u5728\u4e0e\u6709\u6ce8\u610f\u529b\u7f3a\u9677\u591a\u52a8\u969c\u788d\uff08ADHD\uff09\u7684\u5b69\u5b50\u7684\u4e92\u52a8\u4e2d\u964d\u4f4e\u538b\u529b\u5e76\u6539\u8fdb\u80b2\u513f\u65b9\u5f0f\u3002", "conclusion": "\u5bb6\u957f\u4e0d\u4ec5\u662f\u88ab\u52a8\u63a5\u53d7\u8005\uff0c\u800c\u662f\u79ef\u6781\u7684\u8bbe\u8ba1\u8005\uff0c\u80fd\u591f\u6839\u636e\u81ea\u5df1\u7684\u80b2\u513f\u98ce\u683c\u7075\u6d3b\u8c03\u6574\u5e72\u9884\u63aa\u65bd\u3002"}}
{"id": "2602.16758", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.16758", "abs": "https://arxiv.org/abs/2602.16758", "authors": ["Sina Akhbari", "Mehran Mahboubkhah"], "title": "Smooth trajectory generation and hybrid B-splines-Quaternions based tool path interpolation for a 3T1R parallel kinematic milling robot", "comment": "30 pages, 17 figures, published in Elsevier Precision Engineering (https://www.sciencedirect.com/science/article/abs/pii/S0141635925001266)", "summary": "This paper presents a smooth trajectory generation method for a four-degree-of-freedom parallel kinematic milling robot. The proposed approach integrates B-spline and Quaternion interpolation techniques to manage decoupled position and orientation data points. The synchronization of orientation and arc-length-parameterized position data is achieved through the fitting of smooth piece-wise Bezier curves, which describe the non-linear relationship between path length and tool orientation, solved via sequential quadratic programming. By leveraging the convex hull properties of Bezier curves, the method ensures spatial and temporal separation constraints for multi-agent trajectory generation. Unit quaternions are employed for orientation interpolation, providing a robust and efficient representation that avoids gimbal lock and facilitates smooth, continuous rotation. Modifier polynomials are used for position interpolation. Temporal trajectories are optimized using minimum jerk, time-optimal piece-wise Bezier curves in two stages: task space followed by joint space, implemented on a low-cost microcontroller. Experimental results demonstrate that the proposed method offers enhanced accuracy, reduced velocity fluctuations, and computational efficiency compared to conventional interpolation methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u56db\u81ea\u7531\u5ea6\u5e76\u8054\u8fd0\u52a8\u94e3\u524a\u673a\u5668\u4eba\u8f68\u8ff9\u751f\u6210\u7684\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86B\u6837\u6761\u548c\u56db\u5143\u6570\u63d2\u503c\uff0c\u4f18\u5316\u4e86\u8f68\u8ff9\u7cbe\u5ea6\u4e0e\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u65e8\u5728\u63d0\u9ad8\u56db\u81ea\u7531\u5ea6\u5e76\u8054\u8fd0\u52a8\u94e3\u524a\u673a\u5668\u4eba\u8f68\u8ff9\u751f\u6210\u7684\u5e73\u6ed1\u6027\u548c\u6548\u7387\u3002", "method": "\u91c7\u7528B\u6837\u6761\u548c\u56db\u5143\u6570\u63d2\u503c\u6280\u672f\u96c6\u6210\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u5e73\u6ed1\u8f68\u8ff9\u751f\u6210\uff0c\u4f7f\u7528\u987a\u5e8f\u4e8c\u6b21\u89c4\u5212\u89e3\u51b3\u975e\u7ebf\u6027\u8def\u5f84\u957f\u5ea6\u4e0e\u5de5\u5177\u59ff\u6001\u95f4\u5173\u7cfb\uff0c\u5229\u7528Bezier\u66f2\u7ebf\u7684\u51f8\u5305\u7279\u6027\u8fdb\u884c\u8f68\u8ff9\u751f\u6210", "result": "\u4e0e\u4f20\u7edf\u63d2\u503c\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u7cbe\u5ea6\u3001\u901f\u5ea6\u6ce2\u52a8\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u8868\u73b0\u51fa\u4f18\u5316", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u63d0\u9ad8\u7cbe\u5ea6\u3001\u51cf\u5c11\u901f\u5ea6\u6ce2\u52a8\u53ca\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5177\u6709\u4f18\u52bf"}}
{"id": "2602.16895", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.16895", "abs": "https://arxiv.org/abs/2602.16895", "authors": ["Alyssa Hwang", "Hita Kambhamettu", "Yue Yang", "Ajay Patel", "Joseph Chee Chang", "Andrew Head"], "title": "Connecting the Dots: Surfacing Structure in Documents through AI-Generated Cross-Modal Links", "comment": "40 pages, 16 figures", "summary": "Understanding information-dense documents like recipes and scientific papers requires readers to find, interpret, and connect details scattered across text, figures, tables, and other visual elements. These documents are often long and filled with specialized terminology, hindering the ability to locate relevant information or piece together related ideas. Existing tools offer limited support for synthesizing information across media types. As a result, understanding complex material remains cognitively demanding. This paper presents a framework for fine-grained integration of information in complex documents. We instantiate the framework in an augmented reading interface, which populates a scientific paper with clickable points on figures, interactive highlights in the body text, and a persistent reference panel for accessing consolidated details without manual scrolling. In a controlled between-subjects study, we find that participants who read the paper with our tool achieved significantly higher scores on a reading quiz without evidence of increased time to completion or cognitive load. Fine-grained integration provides a systematic way of revealing relationships within a document, supporting engagement with complex, information-dense materials.", "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u65e8\u5728\u5e2e\u52a9\u8bfb\u8005\u66f4\u597d\u5730\u7406\u89e3\u4fe1\u606f\u5bc6\u96c6\u7684\u6587\u6863\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u4f7f\u7528\u8be5\u6846\u67b6\u7684\u5de5\u5177\u53ef\u4ee5\u63d0\u9ad8\u7406\u89e3\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u8ba4\u77e5\u8d1f\u62c5\u3002", "motivation": "\u8bb8\u591a\u4fe1\u606f\u5bc6\u96c6\u7684\u6587\u6863\uff0c\u5982\u98df\u8c31\u548c\u79d1\u5b66\u8bba\u6587\uff0c\u8981\u6c42\u8bfb\u8005\u5728\u6587\u672c\u3001\u56fe\u5f62\u3001\u8868\u683c\u7b49\u89c6\u89c9\u5143\u7d20\u4e2d\u627e\u5230\u3001\u89e3\u91ca\u548c\u8fde\u63a5\u7ec6\u8282\uff0c\u8fd9\u5bf9\u7406\u89e3\u590d\u6742\u6750\u6599\u63d0\u51fa\u4e86\u5de8\u5927\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u590d\u6742\u6587\u6863\u4fe1\u606f\u7ec6\u7c92\u5ea6\u96c6\u6210\u7684\u6846\u67b6\uff0c\u5e76\u5728\u4e00\u4e2a\u589e\u5f3a\u9605\u8bfb\u754c\u9762\u4e2d\u5b9e\u65bd\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u53ef\u70b9\u51fb\u7684\u56fe\u5f62\u70b9\u3001\u4e92\u52a8\u6587\u672c\u9ad8\u4eae\u548c\u6301\u4e45\u53c2\u8003\u9762\u677f\u6765\u6d4f\u89c8\u6587\u6863\u3002", "result": "\u901a\u8fc7\u4e00\u9879\u63a7\u5236\u5b9e\u9a8c\uff0c\u6211\u4eec\u53d1\u73b0\u4f7f\u7528\u8be5\u5de5\u5177\u7684\u53c2\u4e0e\u8005\u5728\u9605\u8bfb\u6d4b\u9a8c\u4e2d\u5f97\u5206\u663e\u8457\u63d0\u9ad8\uff0c\u4e14\u6ca1\u6709\u663e\u793a\u51fa\u589e\u52a0\u7684\u5b8c\u6210\u65f6\u95f4\u6216\u8ba4\u77e5\u8d1f\u62c5\u3002", "conclusion": "\u7ec6\u7c92\u5ea6\u96c6\u6210\u4e3a\u63ed\u793a\u6587\u6863\u5185\u5173\u7cfb\u63d0\u4f9b\u4e86\u4e00\u79cd\u7cfb\u7edf\u7684\u65b9\u6cd5\uff0c\u652f\u6301\u5bf9\u590d\u6742\u4fe1\u606f\u5bc6\u96c6\u6750\u6599\u7684\u66f4\u597d\u7406\u89e3\u548c\u53c2\u4e0e\u3002"}}
{"id": "2602.16825", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.16825", "abs": "https://arxiv.org/abs/2602.16825", "authors": ["Ahmad Ahmad", "Shuo Liu", "Roberto Tron", "Calin Belta"], "title": "RRT$^\u03b7$: Sampling-based Motion Planning and Control from STL Specifications using Arithmetic-Geometric Mean Robustness", "comment": null, "summary": "Sampling-based motion planning has emerged as a powerful approach for robotics, enabling exploration of complex, high-dimensional configuration spaces. When combined with Signal Temporal Logic (STL), a temporal logic widely used for formalizing interpretable robotic tasks, these methods can address complex spatiotemporal constraints. However, traditional approaches rely on min-max robustness measures that focus only on critical time points and subformulae, creating non-smooth optimization landscapes with sharp decision boundaries that hinder efficient tree exploration.\n  We propose RRT$^\u03b7$, a sampling-based planning framework that integrates the Arithmetic-Geometric Mean (AGM) robustness measure to evaluate satisfaction across all time points and subformulae. Our key contributions include: (1) AGM robustness interval semantics for reasoning about partial trajectories during tree construction, (2) an efficient incremental monitoring algorithm computing these intervals, and (3) enhanced Direction of Increasing Satisfaction vectors leveraging Fulfillment Priority Logic (FPL) for principled objective composition. Our framework synthesizes dynamically feasible control sequences satisfying STL specifications with high robustness while maintaining the probabilistic completeness and asymptotic optimality of RRT$^\\ast$. We validate our approach on three robotic systems. A double integrator point robot, a unicycle mobile robot, and a 7-DOF robot arm, demonstrating superior performance over traditional STL robustness-based planners in multi-constraint scenarios with limited guidance signals.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e86\u7b97\u672f\u51e0\u4f55\u5747\u503c (AGM) \u7a33\u5065\u6027\u5ea6\u91cf\u7684\u91c7\u6837\u89c4\u5212\u6846\u67b6 RRT$^\u03b7$\uff0c\u6539\u8fdb\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u590d\u6742\u65f6\u7a7a\u7ea6\u675f\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u7ed3\u5408\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91 (STL) \u548c\u91c7\u6837\u57fa\u7840\u8fd0\u52a8\u89c4\u5212\uff0c\u4ee5\u5e94\u5bf9\u9ad8\u7ef4\u914d\u7f6e\u7a7a\u95f4\u4e2d\u7684\u590d\u6742\u7ea6\u675f\u95ee\u9898\u3002", "method": "\u5229\u7528 AGM \u7a33\u5065\u6027\u5ea6\u91cf\u8bc4\u4ef7\u6240\u6709\u65f6\u95f4\u70b9\u548c\u5b50\u516c\u5f0f\u7684\u6ee1\u8db3\u5ea6\uff0c\u63d0\u51fa AGM \u7a33\u5065\u6027\u533a\u95f4\u8bed\u4e49\uff0c\u8bbe\u8ba1\u9ad8\u6548\u7684\u589e\u91cf\u76d1\u6d4b\u7b97\u6cd5\uff0c\u5e76\u5229\u7528\u6ee1\u8db3\u4f18\u5148\u903b\u8f91 (FPL) \u63d0\u5347\u6ee1\u610f\u5ea6\u5411\u91cf\u3002", "result": "\u5728\u4e09\u4e2a\u673a\u5668\u4eba\u7cfb\u7edf\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u5305\u62ec\u53cc\u79ef\u5206\u70b9\u673a\u5668\u4eba\u3001\u5355\u8f6e\u79fb\u52a8\u673a\u5668\u4eba\u548c 7 \u81ea\u7531\u5ea6\u673a\u5668\u4eba\u624b\u81c2\uff0c\u5c55\u793a\u51fa\u5728\u591a\u7ea6\u675f\u573a\u666f\u4e0b\u4f18\u4e8e\u4f20\u7edf STL \u7a33\u5065\u6027\u89c4\u5212\u5668\u7684\u6027\u80fd\u3002", "conclusion": "RRT$^\u03b7$ \u6846\u67b6\u5728\u4fdd\u8bc1 RRT$^\bullet$ \u7684\u6982\u7387\u5b8c\u6574\u6027\u548c\u6e10\u8fd1\u6700\u4f18\u6027\u7684\u540c\u65f6\uff0c\u5408\u6210\u7b26\u5408 STL \u89c4\u8303\u7684\u52a8\u6001\u53ef\u884c\u63a7\u5236\u5e8f\u5217\u3002"}}
{"id": "2602.16900", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.16900", "abs": "https://arxiv.org/abs/2602.16900", "authors": ["Shreya Bali", "Riku Arakawa", "Peace Odiase", "Tongshuang Wu", "Mayank Goel"], "title": "Evidotes: Integrating Scientific Evidence and Anecdotes to Support Uncertainties Triggered by Peer Health Posts", "comment": null, "summary": "Peer health posts surface new uncertainties, such as questions and concerns for readers. Prior work focused primarily on improving relevance and accuracy fails to address users' diverse information needs and emotions triggered. Instead, we propose directly addressing these by information augmentation. We introduce Evidotes, an information support system that augments individual posts with relevant scientific and anecdotal information retrieved using three user-selectable lenses (dive deeper, focus on positivity, and big picture). In a mixed-methods study with 17 chronic illness patients, Evidotes improved self-reported information satisfaction (3.2->4.6) and reduced self-reported emotional cost (3.4->1.9) compared to participants' baseline browsing. Moreover, by co-presenting sources, Evidotes unlocked information symbiosis: anecdotes made research accessible and contextual, while research helped filter and generalize peer stories. Our work enables an effective integration of scientific evidence and human anecdotes to help users better manage health uncertainty.", "AI": {"tldr": "Evidotes\u662f\u4e00\u4e2a\u4fe1\u606f\u652f\u6301\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e09\u79cd\u53ef\u9009\u89c6\u89d2\u589e\u5f3a\u7528\u6237\u5065\u5eb7\u5e16\u5b50\u7684\u4fe1\u606f\uff0c\u63d0\u9ad8\u4e86\u7528\u6237\u7684\u4fe1\u606f\u6ee1\u610f\u5ea6\u5e76\u964d\u4f4e\u4e86\u60c5\u611f\u6210\u672c\uff0c\u4fc3\u8fdb\u4e86\u79d1\u5b66\u8bc1\u636e\u4e0e\u4eba\u7c7b\u8f76\u4e8b\u4e4b\u95f4\u7684\u6709\u6548\u6574\u5408\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4e8e\u63d0\u5347\u4fe1\u606f\u7684\u76f8\u5173\u6027\u548c\u51c6\u786e\u6027\uff0c\u4f46\u672a\u80fd\u6ee1\u8db3\u7528\u6237\u591a\u6837\u7684\u4fe1\u606f\u9700\u6c42\u548c\u60c5\u611f\u89e6\u52a8\u3002", "method": "\u63d0\u51faEvidotes\u4fe1\u606f\u652f\u6301\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e09\u79cd\u7528\u6237\u53ef\u9009\u89c6\u89d2\u589e\u5f3a\u5e16\u5b50\u4fe1\u606f\uff0c\u8fdb\u884c\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\u3002", "result": "\u5728\u4e0e17\u540d\u6162\u6027\u75c5\u60a3\u8005\u7684\u7814\u7a76\u4e2d\uff0cEvidotes\u663e\u8457\u63d0\u9ad8\u4e86\u81ea\u62a5\u7684\u4fe1\u606f\u6ee1\u610f\u5ea6\uff08\u4ece3.2\u63d0\u5347\u81f34.6\uff09\uff0c\u60c5\u611f\u6210\u672c\u964d\u4f4e\uff08\u4ece3.4\u964d\u81f31.9\uff09\u3002", "conclusion": "Evidotes\u6709\u6548\u6574\u5408\u79d1\u5b66\u8bc1\u636e\u4e0e\u4eba\u7c7b\u8f76\u4e8b\uff0c\u5e2e\u52a9\u7528\u6237\u66f4\u597d\u5730\u7ba1\u7406\u5065\u5eb7\u4e0d\u786e\u5b9a\u6027\u3002"}}
{"id": "2602.16846", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.16846", "abs": "https://arxiv.org/abs/2602.16846", "authors": ["Xili Yi", "Ying Xing", "Zachary Manchester", "Nima Fazeli"], "title": "Sound of Touch: Active Acoustic Tactile Sensing via String Vibrations", "comment": "12 pages, 10 figures", "summary": "Distributed tactile sensing remains difficult to scale over large areas: dense sensor arrays increase wiring, cost, and fragility, while many alternatives provide limited coverage or miss fast interaction dynamics. We present Sound of Touch, an active acoustic tactile-sensing methodology that uses vibrating tensioned strings as sensing elements. The string is continuously excited electromagnetically, and a small number of pickups (contact microphones) observe spectral changes induced by contact. From short-duration audio signals, our system estimates contact location and normal force, and detects slip. To guide design and interpret the sensing mechanism, we derive a physics-based string-vibration simulator that predicts how contact position and force shift vibration modes. Experiments demonstrate millimeter-scale localization, reliable force estimation, and real-time slip detection. Our contributions are: (i) a lightweight, scalable string-based tactile sensing hardware concept for instrumenting extended robot surfaces; (ii) a physics-grounded simulation and analysis tool for contact-induced spectral shifts; and (iii) a real-time inference pipeline that maps vibration measurements to contact state.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u89e6\u89c9\u58f0\u97f3\u201d\u7684\u4e3b\u52a8\u58f0\u5b66\u89e6\u89c9\u4f20\u611f\u65b9\u6cd5\uff0c\u91c7\u7528\u632f\u52a8\u7684\u5f20\u7d27\u7ebf\uff0c\u80fd\u591f\u8fdb\u884c\u9ad8\u6548\u7684\u63a5\u89e6\u4f4d\u7f6e\u548c\u529b\u91cf\u4f30\u8ba1\u3002", "motivation": "\u89e3\u51b3\u5206\u5e03\u5f0f\u89e6\u89c9\u611f\u77e5\u5728\u5927\u9762\u79ef\u6269\u5c55\u4e2d\u7684\u6311\u6218\uff0c\u5982\u4f20\u611f\u5668\u9635\u5217\u7684\u5e03\u7ebf\u3001\u6210\u672c\u548c\u8106\u5f31\u6027\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u4ea4\u4e92\u52a8\u6001\u7684\u6355\u6349\u80fd\u529b\u3002", "method": "\u5229\u7528\u58f0\u6ce2\u89e6\u89c9\u611f\u77e5\u65b9\u6cd5\uff0c\u91c7\u7528\u632f\u52a8\u7684\u5f20\u7d27\u7ebf\u4f5c\u4e3a\u4f20\u611f\u5143\u4ef6\uff0c\u901a\u8fc7\u7535\u78c1\u6fc0\u52b1\u548c\u5c0f\u89c4\u6a21\u62fe\u97f3\u5668\u89c2\u5bdf\u63a5\u89e6\u5f15\u8d77\u7684\u9891\u8c31\u53d8\u5316\u3002", "result": "\u901a\u8fc7\u77ed\u65f6\u97f3\u9891\u4fe1\u53f7\uff0c\u7cfb\u7edf\u80fd\u591f\u4f30\u8ba1\u63a5\u89e6\u4f4d\u7f6e\u548c\u6cd5\u5411\u529b\uff0c\u5e76\u68c0\u6d4b\u6ed1\u52a8\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u7cfb\u7edf\u5177\u6709\u6beb\u7c73\u7ea7\u5b9a\u4f4d\u3001\u53ef\u9760\u7684\u529b\u4f30\u8ba1\u548c\u5b9e\u65f6\u6ed1\u52a8\u68c0\u6d4b\u529f\u80fd\u3002"}}
{"id": "2602.16930", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16930", "abs": "https://arxiv.org/abs/2602.16930", "authors": ["Farnaz Zamiri Zeraati", "Yang Trista Cao", "Yuehan Qiao", "Hal Daum\u00e9", "Hernisa Kacorri"], "title": "Say It My Way: Exploring Control in Conversational Visual Question Answering with Blind Users", "comment": "Preprint, Proceedings of the 2026 CHI Conference on Human Factors in Computing Systems", "summary": "Prompting and steering techniques are well established in general-purpose generative AI, yet assistive visual question answering (VQA) tools for blind users still follow rigid interaction patterns with limited opportunities for customization. User control can be helpful when system responses are misaligned with their goals and contexts, a gap that becomes especially consequential for blind users that may rely on these systems for access. We invite 11 blind users to customize their interactions with a real-world conversational VQA system. Drawing on 418 interactions, reflections, and post-study interviews, we analyze prompting-based techniques participants adopted, including those introduced in the study and those developed independently in real-world settings. VQA interactions were often lengthy: participants averaged 3 turns, sometimes up to 21, with input text typically tenfold shorter than the responses they heard. Built on state-of-the-art LLMs, the system lacked verbosity controls, was limited in estimating distance in space and time, relied on inaccessible image framing, and offered little to no camera guidance. We discuss how customization techniques such as prompt engineering can help participants work around these limitations. Alongside a new publicly available dataset, we offer insights for interaction design at both query and system levels.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u8ba9\u76f2\u7528\u6237\u81ea\u5b9a\u4e49\u5176\u4e0e\u89c6\u89c9\u95ee\u7b54\u7cfb\u7edf\u7684\u4ea4\u4e92\uff0c\u4ee5\u63d0\u9ad8\u5176\u53ef\u7528\u6027\uff0c\u53d1\u73b0\u63d0\u793a\u5de5\u7a0b\u7b49\u4e2a\u6027\u5316\u6280\u672f\u80fd\u591f\u6539\u5584\u7cfb\u7edf\u54cd\u5e94\u7684\u76f8\u5173\u6027\u3002", "motivation": "\u5e0c\u671b\u901a\u8fc7\u589e\u5f3a\u7528\u6237\u63a7\u5236\u548c\u5b9a\u5236\u4ea4\u4e92\u6a21\u5f0f\u6539\u5584\u52a9\u529b\u89c6\u89c9\u95ee\u7b54\u5de5\u5177\u7684\u7528\u6237\u4f53\u9a8c\uff0c\u5c24\u5176\u662f\u5bf9\u76f2\u4eba\u7684\u5f71\u54cd\u3002", "method": "\u9080\u8bf711\u540d\u76f2\u7528\u6237\u81ea\u5b9a\u4e49\u4e0e\u73b0\u5b9e\u4e16\u754c\u5bf9\u8bdd\u5f0f\u89c6\u89c9\u95ee\u7b54\u7cfb\u7edf\u7684\u4ea4\u4e92\uff0c\u5206\u6790\u4ed6\u4eec\u6240\u91c7\u7528\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u6280\u672f\u3002", "result": "\u57fa\u4e8e418\u6b21\u4ea4\u4e92\u7ed3\u679c\u3001\u53cd\u601d\u53ca\u540e\u671f\u8bbf\u8c08\u7684\u5206\u6790\uff0c\u53d1\u73b0\u76f2\u7528\u6237\u7684\u4ea4\u4e92\u5f80\u5f80\u5197\u957f\uff0c\u5e73\u57473\u8f6e\uff0c\u6709\u65f6\u591a\u8fbe21\u8f6e\uff0c\u8f93\u5165\u6587\u672c\u901a\u5e38\u6bd4\u4ed6\u4eec\u542c\u5230\u7684\u56de\u590d\u77ed\u5341\u500d\u3002", "conclusion": "\u63d0\u51fa\u4e2a\u6027\u5316\u6280\u672f\u5982\u63d0\u793a\u5de5\u7a0b\u53ef\u4ee5\u5e2e\u52a9\u7528\u6237\u514b\u670d\u7cfb\u7edf\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u516c\u5f00\u6570\u636e\u96c6\u4ee5\u652f\u6301\u4ea4\u4e92\u8bbe\u8ba1\u7684\u6df1\u5165\u7814\u7a76\u3002"}}
{"id": "2602.16861", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.16861", "abs": "https://arxiv.org/abs/2602.16861", "authors": ["EunJeong Cheon", "Do Yeon Shin"], "title": "\"Hello, I'm Delivering. Let Me Pass By\": Navigating Public Pathways with Walk-along with Robots in Crowded City Streets", "comment": null, "summary": "As the presence of autonomous robots in public spaces increases-whether navigating campus walkways or neighborhood sidewalks-understanding how to carefully study these robots becomes critical. While HRI research has conducted field studies in public spaces, these are often limited to controlled experiments with prototype robots or structured observational methods, such as the Wizard of Oz technique. However, the autonomous mobile robots we encounter today, particularly delivery robots, operate beyond the control of researchers, navigating dynamic routes and unpredictable environments. To address this challenge, a more deliberate approach is required. Drawing inspiration from public realm ethnography in urban studies, geography, and sociology, this paper proposes the Walk-Along with Robots (WawR) methodology. We outline the key features of this method, the steps we applied in our study, the unique insights it offers, and the ways it can be evaluated. We hope this paper stimulates further discussion on research methodologies for studying autonomous robots in public spaces.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faWawR\u65b9\u6cd5\uff0c\u4ee5\u66f4\u5168\u9762\u5730\u7814\u7a76\u516c\u5171\u7a7a\u95f4\u4e2d\u7684\u81ea\u4e3b\u673a\u5668\u4eba\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u7814\u7a76\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5e0c\u671b\u80fd\u4fc3\u8fdb\u5bf9\u76f8\u5173\u7814\u7a76\u65b9\u6cd5\u7684\u6df1\u5165\u8ba8\u8bba\u3002", "motivation": "\u968f\u7740\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u516c\u5171\u7a7a\u95f4\u7684\u666e\u53ca\uff0c\u7814\u7a76\u8fd9\u4e9b\u673a\u5668\u4eba\u7684\u884c\u4e3a\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\uff0c\u76ee\u524d\u7684\u7814\u7a76\u65b9\u6cd5\u5f80\u5f80\u5c40\u9650\u4e8e\u63a7\u5236\u5b9e\u9a8c\uff0c\u56e0\u6b64\u6025\u9700\u4e00\u79cd\u66f4\u4e3a\u5468\u5168\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51faWalk-Along with Robots (WawR)\u65b9\u6cd5\uff0c\u501f\u9274\u4e86\u57ce\u5e02\u7814\u7a76\u4e2d\u7684\u516c\u5171\u9886\u57df\u6c11\u65cf\u5fd7\uff0c\u7cfb\u7edf\u5730\u7814\u7a76\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u7279\u522b\u662f\u5feb\u9012\u673a\u5668\u4eba\u7684\u884c\u4e3a\u3002", "result": "\u901a\u8fc7\u5e94\u7528WawR\u65b9\u6cd5\uff0c\u672c\u7814\u7a76\u63ed\u793a\u4e86\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u52a8\u6001\u548c\u4e0d\u53ef\u9884\u6d4b\u73af\u5883\u4e2d\u5bfc\u822a\u7684\u72ec\u7279\u884c\u4e3a\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u65b9\u6cd5\u7684\u8ba8\u8bba\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684Walk-Along with Robots (WawR)\u65b9\u6cd5\u4e3a\u7814\u7a76\u516c\u5171\u7a7a\u95f4\u4e2d\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6d1e\u5bdf\u8fd9\u4e9b\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u884c\u4e3a\u3002"}}
{"id": "2602.16939", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.16939", "abs": "https://arxiv.org/abs/2602.16939", "authors": ["Lev Tankelevitch", "Ava Elizabeth Scott", "Nagaravind Challakere", "Payod Panda", "Sean Rintel"], "title": "Nudging Attention to Workplace Meeting Goals: A Large-Scale, Preregistered Field Experiment", "comment": null, "summary": "Ineffective meetings are pervasive. Thinking ahead explicitly about meeting goals may improve effectiveness, but current collaboration platforms lack integrated support. We tested a lightweight goal-reflection intervention in a preregistered field experiment in a global technology company (361 employees, 7196 meetings). Over two weeks, workers in the treatment group completed brief pre-meeting surveys in their collaboration platform, nudging attention to goals for upcoming meetings. To measure impact, both treatment and control groups completed post-meeting surveys about meeting effectiveness. While the intervention impact on meeting effectiveness was not statistically significant, mixed-methods findings revealed improvements in self-reported awareness and behaviour across both groups, with post-meeting surveys unintentionally functioning as an intervention. We highlight the promise of supporting goal reflection, while noting challenges of evaluating and supporting workplace reflection for meetings, including workflow and collaboration norms, and attitudes and behaviours around meeting preparation. We conclude with implications for designing technological support for meeting intentionality.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u901a\u8fc7\u8f7b\u91cf\u7ea7\u76ee\u6807\u53cd\u601d\u5e72\u9884\u63d0\u9ad8\u4f1a\u8bae\u6709\u6548\u6027\uff0c\u7ed3\u679c\u663e\u793a\u5c3d\u7ba1\u7edf\u8ba1\u610f\u4e49\u4e0d\u660e\u663e\uff0c\u4f46\u81ea\u6211\u62a5\u544a\u7684\u610f\u8bc6\u548c\u884c\u4e3a\u6709\u6240\u6539\u5584\uff0c\u5f3a\u8c03\u4e86\u8bbe\u8ba1\u4f1a\u8bae\u610f\u56fe\u652f\u6301\u6280\u672f\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u63d0\u9ad8\u4f1a\u8bae\u7684\u6709\u6548\u6027\uff0c\u89e3\u51b3\u5f53\u524d\u534f\u4f5c\u5e73\u53f0\u7f3a\u4e4f\u96c6\u6210\u652f\u6301\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5728\u5168\u7403\u79d1\u6280\u516c\u53f8\u8fdb\u884c\u7684\u9884\u6ce8\u518c\u73b0\u573a\u5b9e\u9a8c\uff0c\u6d4b\u8bd5\u8f7b\u91cf\u7ea7\u76ee\u6807\u53cd\u601d\u5e72\u9884\uff0c\u6d89\u53ca361\u540d\u5458\u5de5\u548c7196\u6b21\u4f1a\u8bae\u3002", "result": "\u867d\u7136\u5e72\u9884\u5bf9\u4f1a\u8bae\u6709\u6548\u6027\u7684\u5f71\u54cd\u6ca1\u6709\u7edf\u8ba1\u663e\u8457\u6027\uff0c\u4f46\u6df7\u5408\u65b9\u6cd5\u7684\u7814\u7a76\u53d1\u73b0\u663e\u793a\uff0c\u4e24\u4e2a\u7ec4\u522b\u5728\u81ea\u6211\u62a5\u544a\u7684\u610f\u8bc6\u548c\u884c\u4e3a\u65b9\u9762\u6709\u6240\u6539\u5584\uff0c\u540e\u671f\u8c03\u67e5\u610f\u5916\u5730\u6210\u4e3a\u4e86\u4e00\u79cd\u5e72\u9884\u624b\u6bb5\u3002", "conclusion": "\u5f3a\u8c03\u652f\u6301\u76ee\u6807\u53cd\u601d\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u6307\u51fa\u8bc4\u4f30\u548c\u652f\u6301\u5de5\u4f5c\u573a\u6240\u4f1a\u8bae\u53cd\u601d\u6240\u9762\u4e34\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e3a\u4f1a\u8bae\u610f\u56fe\u8bbe\u8ba1\u6280\u672f\u652f\u6301\u7684\u76f8\u5173\u5efa\u8bae\u3002"}}
{"id": "2602.16863", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16863", "abs": "https://arxiv.org/abs/2602.16863", "authors": ["Kushal Kedia", "Tyler Ga Wei Lum", "Jeannette Bohg", "C. Karen Liu"], "title": "SimToolReal: An Object-Centric Policy for Zero-Shot Dexterous Tool Manipulation", "comment": null, "summary": "The ability to manipulate tools significantly expands the set of tasks a robot can perform. Yet, tool manipulation represents a challenging class of dexterity, requiring grasping thin objects, in-hand object rotations, and forceful interactions. Since collecting teleoperation data for these behaviors is challenging, sim-to-real reinforcement learning (RL) is a promising alternative. However, prior approaches typically require substantial engineering effort to model objects and tune reward functions for each task. In this work, we propose SimToolReal, taking a step towards generalizing sim-to-real RL policies for tool manipulation. Instead of focusing on a single object and task, we procedurally generate a large variety of tool-like object primitives in simulation and train a single RL policy with the universal goal of manipulating each object to random goal poses. This approach enables SimToolReal to perform general dexterous tool manipulation at test-time without any object or task-specific training. We demonstrate that SimToolReal outperforms prior retargeting and fixed-grasp methods by 37% while matching the performance of specialist RL policies trained on specific target objects and tasks. Finally, we show that SimToolReal generalizes across a diverse set of everyday tools, achieving strong zero-shot performance over 120 real-world rollouts spanning 24 tasks, 12 object instances, and 6 tool categories.", "AI": {"tldr": "SimToolReal\u901a\u8fc7\u7a0b\u5e8f\u751f\u6210\u5404\u79cd\u5de5\u5177\u6837\u672c\uff0c\u4f7f\u7528\u5355\u4e00\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u8fdb\u884c\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u7684\u5de5\u5177\u64cd\u63a7\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u8d8a\u3002", "motivation": "\u63d0\u4f9b\u4e00\u79cd\u7b80\u5316\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u751f\u6210\u5de5\u5177\u5bf9\u8c61\uff0c\u51cf\u5c11\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5728\u6570\u636e\u6536\u96c6\u548c\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u4e0a\u7684\u5de5\u7a0b\u6210\u672c\uff0c\u4ee5\u63d0\u9ad8\u673a\u5668\u4eba\u5de5\u5177\u64cd\u63a7\u7684\u901a\u7528\u6027\u3002", "method": "SimToolReal\u901a\u8fc7\u7a0b\u5e8f\u751f\u6210\u591a\u79cd\u5de5\u5177\u5bf9\u8c61\uff0c\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8bad\u7ec3\u4e00\u4e2a\u7edf\u4e00\u7684\u5f3a\u5316\u5b66\u4e60\u653f\u7b56\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u5404\u79cd\u5de5\u5177\u7684\u64cd\u63a7\u3002", "result": "SimToolReal\u76f8\u8f83\u4e8e\u4ee5\u524d\u7684\u91cd\u5b9a\u5411\u53ca\u56fa\u5b9a\u6293\u53d6\u65b9\u6cd5\u63d0\u5347\u4e8637%\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u4e5f\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u96f6-shot\u80fd\u529b\uff0c\u6210\u529f\u5b8c\u6210120\u6b21\u4efb\u52a1\u6267\u884c\u3002", "conclusion": "SimToolReal\u5728\u5de5\u5177\u64cd\u63a7\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u5728\u591a\u4efb\u52a1\u4e0e\u5bf9\u8c61\u5b9e\u4f8b\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.16975", "categories": ["cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.16975", "abs": "https://arxiv.org/abs/2602.16975", "authors": ["Jennica Li", "Shirley Zhang", "Dakota Sullivan", "Bengisu Cagiltay", "Heather Kirkorian", "Bilge Mutlu", "Kassem Fawaz"], "title": "\"It's like a pet...but my pet doesn't collect data about me\": Multi-person Households' Privacy Design Preferences for Household Robots", "comment": "13 pages (main body), 2 figures", "summary": "Household robots boasting mobility, more sophisticated sensors, and powerful processing models have become increasingly prevalent in the commercial market. However, these features may expose users to unwanted privacy risks, including unsolicited data collection and unauthorized data sharing. While security and privacy researchers thus far have explored people's privacy concerns around household robots, literature investigating people's preferred privacy designs and mitigation strategies is still limited. Additionally, the existing literature has not yet accounted for multi-user perspectives on privacy design and household robots. We aimed to fill this gap by conducting in-person participatory design sessions with 15 households to explore how they would design a privacy-aware household robot based on their concerns and expectations. We found that participants did not trust that robots, or their respective manufacturers, would respect the data privacy of household members or operate in a multi-user ecosystem without jeopardizing users' personal data. Based on these concerns, they generated designs that gave them authority over their data, contained accessible controls and notification systems, and could be customized and tailored to suit the needs and preferences of each user over time. We synthesize our findings into actionable design recommendations for robot manufacturers and developers.", "AI": {"tldr": "\u7814\u7a76\u5bb6\u5ead\u673a\u5668\u4eba\u7528\u6237\u7684\u9690\u79c1\u8bbe\u8ba1\u9700\u6c42\uff0c\u63d0\u51fa\u53ef\u884c\u6027\u8bbe\u8ba1\u5efa\u8bae\u3002", "motivation": "\u76ee\u524d\u5bf9\u5bb6\u5ead\u673a\u5668\u4eba\u7684\u9690\u79c1\u95ee\u9898\u5173\u6ce8\u8f83\u5c11\uff0c\u7279\u522b\u662f\u591a\u7528\u6237\u89c6\u89d2\u4e0b\u7684\u9690\u79c1\u8bbe\u8ba1\u3002", "method": "\u901a\u8fc7\u4e0e15\u4e2a\u5bb6\u5ead\u8fdb\u884c\u9762\u5bf9\u9762\u7684\u53c2\u4e0e\u5f0f\u8bbe\u8ba1\u4f1a\u8bae\uff0c\u63a2\u7d22\u7528\u6237\u5bf9\u9690\u79c1\u610f\u8bc6\u5bb6\u5ead\u673a\u5668\u4eba\u7684\u8bbe\u8ba1\u9700\u6c42\u3002", "result": "\u53c2\u4e0e\u8005\u666e\u904d\u4e0d\u4fe1\u4efb\u673a\u5668\u4eba\u6216\u5236\u9020\u5546\u80fd\u591f\u4fdd\u62a4\u5bb6\u5ead\u6210\u5458\u7684\u6570\u636e\u9690\u79c1\uff0c\u5e76\u63d0\u51fa\u4e86\u8bbe\u8ba1\u65b9\u6848\u4ee5\u5bfb\u6c42\u5bf9\u6570\u636e\u7684\u63a7\u5236\u6743\u3002", "conclusion": "\u63d0\u51fa\u4e86\u6709\u9488\u5bf9\u6027\u7684\u8bbe\u8ba1\u5efa\u8bae\uff0c\u5e2e\u52a9\u673a\u5668\u4eba\u5236\u9020\u5546\u548c\u5f00\u53d1\u8005\u66f4\u597d\u5730\u6ee1\u8db3\u7528\u6237\u7684\u9690\u79c1\u9700\u6c42\u3002"}}
{"id": "2602.16870", "categories": ["cs.RO", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.16870", "abs": "https://arxiv.org/abs/2602.16870", "authors": ["Daniil Lisus", "Katya M. Papais", "Cedric Le Gentil", "Elliot Preston-Krebs", "Andrew Lambert", "Keith Y. K. Leung", "Timothy D. Barfoot"], "title": "Boreas Road Trip: A Multi-Sensor Autonomous Driving Dataset on Challenging Roads", "comment": "23 pages, 15 figures, 12 tables, submitted to The International Journal of Robotics Research (IJRR)", "summary": "The Boreas Road Trip (Boreas-RT) dataset extends the multi-season Boreas dataset to new and diverse locations that pose challenges for modern autonomous driving algorithms. Boreas-RT comprises 60 sequences collected over 9 real-world routes, totalling 643 km of driving. Each route is traversed multiple times, enabling evaluation in identical environments under varying traffic and, in some cases, weather conditions. The data collection platform includes a 5MP FLIR Blackfly S camera, a 360 degree Navtech RAS6 Doppler-enabled spinning radar, a 128-channel 360 degree Velodyne Alpha Prime lidar, an Aeva Aeries II FMCW Doppler-enabled lidar, a Silicon Sensing DMU41 inertial measurement unit, and a Dynapar wheel encoder. Centimetre-level ground truth is provided via post-processed Applanix POS LV GNSS-INS data. The dataset includes precise extrinsic and intrinsic calibrations, a publicly available development kit, and a live leaderboard for odometry and metric localization. Benchmark results show that many state-of-the-art odometry and localization algorithms overfit to simple driving environments and degrade significantly on the more challenging Boreas-RT routes. Boreas-RT provides a unified dataset for evaluating multi-modal algorithms across diverse road conditions. The dataset, leaderboard, and development kit are available at www.boreas.utias.utoronto.ca.", "AI": {"tldr": "Boreas-RT\u6570\u636e\u96c6\u6269\u5c55\u4e86\u591a\u5b63\u8282Boreas\u6570\u636e\u96c6\uff0c\u6db5\u76d69\u6761\u5177\u6709\u6311\u6218\u6027\u7684\u771f\u5b9e\u9a7e\u9a76\u8def\u7ebf\uff0c\u5171643\u516c\u91cc\uff0c\u9002\u7528\u4e8e\u8bc4\u4f30\u73b0\u4ee3\u81ea\u52a8\u9a7e\u9a76\u7b97\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u73b0\u4ee3\u81ea\u52a8\u9a7e\u9a76\u7b97\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u4e0d\u8db3\uff0c\u5efa\u7acb\u4e00\u4e2a\u5305\u542b\u591a\u6837\u8def\u7ebf\u53ca\u591a\u5b63\u8282\u6570\u636e\u7684\u6570\u636e\u96c6\u3002", "method": "\u901a\u8fc7\u591a\u79cd\u4f20\u611f\u5668\u6536\u96c6\u5b9e\u65f6\u6570\u636e\uff0c\u5305\u62ec\u76f8\u673a\u3001\u6fc0\u5149\u96f7\u8fbe\u548c\u60ef\u6027\u6d4b\u91cf\u5355\u5143\uff0c\u63d0\u4f9b\u5398\u7c73\u7ea7\u7684\u5730\u9762\u771f\u5b9e\u5ea6\u3002", "result": "\u57fa\u51c6\u7ed3\u679c\u8868\u660e\uff0c\u8bb8\u591a\u6700\u5148\u8fdb\u7684\u91cc\u7a0b\u8ba1\u548c\u5b9a\u4f4d\u7b97\u6cd5\u5728\u7b80\u5355\u73af\u5883\u4e2d\u8fc7\u62df\u5408\uff0c\u5e76\u5728\u66f4\u5177\u6311\u6218\u6027\u7684Boreas-RT\u8def\u7ebf\u4e2d\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "Boreas-RT\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5728\u591a\u6837\u8def\u51b5\u4e0b\u8bc4\u4f30\u591a\u6a21\u6001\u7b97\u6cd5\uff0c\u6570\u636e\u96c6\u53ca\u76f8\u5173\u5de5\u5177\u53ef\u4f9b\u516c\u5f00\u8bbf\u95ee\u3002"}}
{"id": "2602.17067", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.17067", "abs": "https://arxiv.org/abs/2602.17067", "authors": ["Leixian Shen", "Yan Luo", "Rui Sheng", "Yujia He", "Haotian Li", "Leni Yang", "Huamin Qu"], "title": "StoryLensEdu: Personalized Learning Report Generation through Narrative-Driven Multi-Agent Systems", "comment": null, "summary": "Personalized feedback plays an important role in self-regulated learning (SRL), helping students track progress and refine their strategies. However, current common solutions, such as text-based reports or learning analytics dashboards, often suffer from poor interpretability, monotonous presentation, and limited explainability. To overcome these challenges, we present StoryLensEdu, a narrative-driven multi-agent system that automatically generates intuitive, engaging, and interactive learning reports. StoryLensEdu integrates three agents: a Data Analyst that extracts data insights based on a learning objective centered structure, a Teacher that ensures educational relevance and offers actionable suggestions, and a Storyteller that organizes these insights using the Heroes Journey narrative framework. StoryLensEdu supports post-generation interactive question answering to improve explainability and user engagement. We conducted a formative study in a real high school and iteratively developed StoryLensEdu in collaboration with an e-learning team to inform our design. Evaluation with real users shows that StoryLensEdu enhances engagement and promotes a deeper understanding of the learning process.", "AI": {"tldr": "StoryLensEdu\u662f\u4e00\u4e2a\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f7f\u7528\u53d9\u4e8b\u7ed3\u6784\u751f\u6210\u4e2a\u6027\u5316\u7684\u5b66\u4e60\u62a5\u544a\uff0c\u6539\u5584\u4e86\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u53cd\u9988\u7684\u53ef\u89e3\u91ca\u6027\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\u3002", "motivation": "\u4e2a\u6027\u5316\u53cd\u9988\u5728\u81ea\u6211\u8c03\u8282\u5b66\u4e60\uff08SRL\uff09\u4e2d\u53d1\u6325\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u5f53\u524d\u7684\u89e3\u51b3\u65b9\u6848\u5728\u53ef\u89e3\u91ca\u6027\u3001\u5448\u73b0\u65b9\u5f0f\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5b58\u5728\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStoryLensEdu\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u751f\u6210\u76f4\u89c2\u3001\u4e92\u52a8\u7684\u5b66\u4e60\u62a5\u544a\uff0c\u6574\u5408\u4e86\u6570\u636e\u5206\u6790\u5e08\u3001\u6559\u5e08\u548c\u8bb2\u8ff0\u8005\u4e09\u4e2a\u89d2\u8272\uff0c\u4f7f\u7528\u82f1\u96c4\u65c5\u7a0b\u53d9\u4e8b\u6846\u67b6\u7ec4\u7ec7\u89c1\u89e3\u3002", "result": "\u5728\u771f\u5b9e\u9ad8\u4e2d\u7684\u5f62\u6210\u6027\u7814\u7a76\u4e2d\uff0cStoryLensEdu\u4e0e\u7535\u5b50\u5b66\u4e60\u56e2\u961f\u534f\u4f5c\u5f00\u53d1\uff0c\u7528\u6237\u8bc4\u4f30\u8868\u660e\u5176\u63d0\u9ad8\u4e86\u7528\u6237\u53c2\u4e0e\u5ea6\u5e76\u4fc3\u8fdb\u4e86\u5bf9\u5b66\u4e60\u8fc7\u7a0b\u7684\u6df1\u5165\u7406\u89e3\u3002", "conclusion": "StoryLensEdu\u901a\u8fc7\u53d9\u4e8b\u9a71\u52a8\u7684\u53cd\u9988\u65b9\u5f0f\u6709\u6548\u589e\u5f3a\u4e86\u5b66\u4e60\u8005\u7684\u53c2\u4e0e\u611f\u548c\u7406\u89e3\u529b\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u5b66\u4e60\u62a5\u544a\u7684\u4e0d\u8db3\u4e4b\u5904\u3002"}}
{"id": "2602.16898", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16898", "abs": "https://arxiv.org/abs/2602.16898", "authors": ["Iman Ahmadi", "Mehrshad Taji", "Arad Mahdinezhad Kashani", "AmirHossein Jadidi", "Saina Kashani", "Babak Khalaj"], "title": "MALLVI: a multi agent framework for integrated generalized robotics manipulation", "comment": null, "summary": "Task planning for robotic manipulation with large language models (LLMs) is an emerging area. Prior approaches rely on specialized models, fine tuning, or prompt tuning, and often operate in an open loop manner without robust environmental feedback, making them fragile in dynamic settings.We present MALLVi, a Multi Agent Large Language and Vision framework that enables closed loop feedback driven robotic manipulation. Given a natural language instruction and an image of the environment, MALLVi generates executable atomic actions for a robot manipulator. After action execution, a Vision Language Model (VLM) evaluates environmental feedback and decides whether to repeat the process or proceed to the next step.Rather than using a single model, MALLVi coordinates specialized agents, Decomposer, Localizer, Thinker, and Reflector, to manage perception, localization, reasoning, and high level planning. An optional Descriptor agent provides visual memory of the initial state. The Reflector supports targeted error detection and recovery by reactivating only relevant agents, avoiding full replanning.Experiments in simulation and real world settings show that iterative closed loop multi agent coordination improves generalization and increases success rates in zero shot manipulation tasks.Code available at https://github.com/iman1234ahmadi/MALLVI.", "AI": {"tldr": "MALLVi \u662f\u4e00\u79cd\u591a\u4ee3\u7406\u5927\u8bed\u8a00\u548c\u89c6\u89c9\u6846\u67b6\uff0c\u80fd\u591f\u5728\u95ed\u73af\u53cd\u9988\u4e0b\u5b9e\u73b0\u673a\u5668\u4eba\u64cd\u63a7\u4efb\u52a1\u7684\u89c4\u5212\uff0c\u6539\u5584\u4e86\u52a8\u6001\u73af\u5883\u4e2d\u7684\u7a33\u5065\u6027\u548c\u6210\u529f\u7387\u3002", "motivation": "\u5f53\u524d\u7684\u673a\u5668\u4eba\u64cd\u63a7\u4efb\u52a1\u89c4\u5212\u65b9\u6cd5\u4f9d\u8d56\u4e13\u4e1a\u6a21\u578b\u548c\u7ec6\u8c03\uff0c\u7f3a\u4e4f\u73af\u5883\u53cd\u9988\uff0c\u5bfc\u81f4\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "MALLVi\u901a\u8fc7\u534f\u8c03\u591a\u4e2a\u4ee3\u7406\uff08\u5305\u62ecDecomposer\u3001Localizer\u3001Thinker\u548cReflector\uff09\u6765\u7ba1\u7406\u611f\u77e5\u3001\u5b9a\u4f4d\u3001\u63a8\u7406\u548c\u9ad8\u7ea7\u89c4\u5212\uff0c\u5e76\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u73af\u5883\u53cd\u9988\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u73af\u5883\u4e2d\uff0c\u8fed\u4ee3\u7684\u95ed\u73af\u591a\u4ee3\u7406\u534f\u8c03\u63d0\u9ad8\u4e86\u751f\u6210\u80fd\u529b\u548c\u96f6\u6837\u672c\u64cd\u63a7\u4efb\u52a1\u7684\u6210\u529f\u7387\u3002", "conclusion": "MALLVi\u6846\u67b6\u5c55\u793a\u4e86\u5728\u52a8\u6001\u73af\u5883\u4e2d\u901a\u8fc7\u591a\u4ee3\u7406\u534f\u8c03\u548c\u95ed\u73af\u53cd\u9988\u8fdb\u884c\u673a\u5668\u4eba\u64cd\u63a7\u7684\u6f5c\u529b\uff0c\u6539\u5584\u4e86\u4efb\u52a1\u89c4\u5212\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.17083", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.17083", "abs": "https://arxiv.org/abs/2602.17083", "authors": ["Celeste Seah", "Yoke Chuan Lee", "Jung-Joo Lee", "Ching-Chiuan Yen", "Clement Zheng"], "title": "Rememo: A Research-through-Design Inquiry Towards an AI-in-the-loop Therapist's Tool for Dementia Reminiscence", "comment": null, "summary": "Reminiscence therapy (RT) is a common non-pharmacological intervention in dementia care. Recent technology-mediated interventions have largely focused on people with dementia through solutions that replace human facilitators with conversational agents. However, the relational work of facilitation is critical in the effectiveness of RT. Hence, we developed Rememo, a therapist-oriented tool that integrates Generative AI to support and enrich human facilitation in RT. Our tool aims to support the infrastructural and cultural challenges that therapists in Singapore face. In this research, we contribute the Rememo system as a therapist's tool for personalized RT developed through sociotechnically-aware research-through-design. Through studying this system in-situ, our research extends our understanding of human-AI collaboration for care work. We discuss the implications of designing AI-enabled systems that respect the relational dynamics in care contexts, and argue for a rethinking of synthetic imagery as a therapeutic support for memory rahter than a record of truth.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86Rememo\u5de5\u5177\uff0c\u65e8\u5728\u901a\u8fc7\u751f\u6210\u6027\u4eba\u5de5\u667a\u80fd\u652f\u6301\u56de\u5fc6\u6cbb\u7597\uff0c\u89e3\u51b3\u6cbb\u7597\u5e08\u9762\u4e34\u7684\u6587\u5316\u6311\u6218\uff0c\u5e76\u63a2\u8ba8\u4eba\u7c7b\u4e0e\u4eba\u5de5\u667a\u80fd\u5728\u62a4\u7406\u5de5\u4f5c\u4e2d\u7684\u5173\u7cfb\u3002", "motivation": "\u4fc3\u8fdb\u975e\u836f\u7269\u5e72\u9884\u5373\u56de\u5fc6\u7597\u6cd5\u5728\u75f4\u5446\u75c7\u62a4\u7406\u4e2d\u7684\u6709\u6548\u5e94\u7528\uff0c\u5173\u6ce8\u4eba\u9645\u5173\u7cfb\u5728\u6cbb\u7597\u8fc7\u7a0b\u4e2d\u7684\u91cd\u8981\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3aRememo\u7684\u5de5\u5177\uff0c\u4ee5\u652f\u6301\u548c\u4e30\u5bcc\u4eba\u7c7b\u7684\u56de\u5fc6\u6cbb\u7597\uff08RT\uff09\u4ecb\u5165\u3002", "result": "Rememo\u662f\u4e00\u4e2a\u9762\u5411\u6cbb\u7597\u5e08\u7684\u5de5\u5177\uff0c\u6574\u5408\u4e86\u751f\u6210\u6027\u4eba\u5de5\u667a\u80fd\uff0c\u4ee5\u652f\u6301\u65b0\u52a0\u5761\u6cbb\u7597\u5e08\u5728\u4eba\u9645\u5173\u7cfb\u548c\u6587\u5316\u6311\u6218\u4e2d\u8fdb\u884c\u4e2a\u6027\u5316\u7684\u56de\u5fc6\u7597\u6cd5\u3002", "conclusion": "\u7814\u7a76\u6269\u5c55\u4e86\u4eba\u7c7b\u4e0e\u4eba\u5de5\u667a\u80fd\u5728\u62a4\u7406\u5de5\u4f5c\u4e2d\u7684\u534f\u4f5c\u7406\u89e3\uff0c\u5e76\u547c\u5401\u91cd\u65b0\u601d\u8003\u5408\u6210\u56fe\u50cf\u4f5c\u4e3a\u8bb0\u5fc6\u6cbb\u7597\u7684\u652f\u6301\uff0c\u800c\u4e0d\u662f\u4e8b\u5b9e\u8bb0\u5f55\u3002"}}
{"id": "2602.16911", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.16911", "abs": "https://arxiv.org/abs/2602.16911", "authors": ["Adrian R\u00f6fer", "Nick Heppert", "Abhinav Valada"], "title": "SparTa: Sparse Graphical Task Models from a Handful of Demonstrations", "comment": "9 pages, 6 figures, under review", "summary": "Learning long-horizon manipulation tasks efficiently is a central challenge in robot learning from demonstration. Unlike recent endeavors that focus on directly learning the task in the action domain, we focus on inferring what the robot should achieve in the task, rather than how to do so. To this end, we represent evolving scene states using a series of graphical object relationships. We propose a demonstration segmentation and pooling approach that extracts a series of manipulation graphs and estimates distributions over object states across task phases. In contrast to prior graph-based methods that capture only partial interactions or short temporal windows, our approach captures complete object interactions spanning from the onset of control to the end of the manipulation. To improve robustness when learning from multiple demonstrations, we additionally perform object matching using pre-trained visual features. In extensive experiments, we evaluate our method's demonstration segmentation accuracy and the utility of learning from multiple demonstrations for finding a desired minimal task model. Finally, we deploy the fitted models both in simulation and on a real robot, demonstrating that the resulting task representations support reliable execution across environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u64cd\u4f5c\u56fe\u548c\u5bf9\u7269\u4f53\u72b6\u6001\u7684\u4f30\u8ba1\uff0c\u6539\u8fdb\u4e86\u673a\u5668\u4eba\u957f\u65f6\u95f4\u64cd\u4f5c\u4efb\u52a1\u7684\u5b66\u4e60\u6548\u7387\uff0c\u9002\u7528\u4e8e\u591a\u6b21\u6f14\u793a\u3002", "motivation": "\u9ad8\u6548\u5b66\u4e60\u957f\u65f6\u95f4\u64cd\u4f5c\u4efb\u52a1\u662f\u673a\u5668\u4eba\u4ece\u6f14\u793a\u5b66\u4e60\u4e2d\u7684\u4e00\u4e2a\u6838\u5fc3\u6311\u6218\uff0c\u56e0\u6b64\u7814\u7a76\u5982\u4f55\u63a8\u65ad\u673a\u5668\u4eba\u5728\u4efb\u52a1\u4e2d\u5e94\u8be5\u8fbe\u6210\u7684\u76ee\u6807\u800c\u975e\u64cd\u4f5c\u65b9\u5f0f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u6f14\u793a\u5206\u6bb5\u548c\u6c60\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e00\u7cfb\u5217\u56fe\u5f62\u5bf9\u8c61\u5173\u7cfb\u8868\u793a\u53d8\u5316\u573a\u666f\u72b6\u6001\uff0c\u5e76\u8fdb\u884c\u7269\u4f53\u5339\u914d\u4ee5\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u5728\u5927\u91cf\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6f14\u793a\u5206\u6bb5\u51c6\u786e\u6027\u548c\u4ece\u591a\u6b21\u6f14\u793a\u5b66\u4e60\u4ee5\u627e\u5230\u671f\u671b\u7684\u6700\u5c0f\u5316\u4efb\u52a1\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u5e76\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5c55\u793a\u4e86\u7ed3\u679c\u4efb\u52a1\u8868\u793a\u7684\u53ef\u9760\u6267\u884c\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u6f14\u793a\u5206\u6bb5\u548c\u6c60\u5316\u7684\u65b9\u6cd5\uff0c\u6211\u4eec\u80fd\u591f\u63d0\u53d6\u5168\u9762\u7684\u64cd\u4f5c\u56fe\u5e76\u5728\u4efb\u52a1\u9636\u6bb5\u4e4b\u95f4\u51c6\u786e\u4f30\u8ba1\u7269\u4f53\u72b6\u6001\u5206\u5e03\uff0c\u6700\u7ec8\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u4efb\u52a1\u6267\u884c\u3002"}}
{"id": "2602.17093", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.17093", "abs": "https://arxiv.org/abs/2602.17093", "authors": ["Mengjie Tang", "Xinman Li", "Juxiao Zhang", "Franklin Mingzhe Li", "Zhuying Li"], "title": "Understanding Nature Engagement Experiences of Blind People", "comment": "CHI 2026 Full Paper", "summary": "Nature plays a crucial role in human health and well-being, but little is known about how blind people experience and relate to it. We conducted a survey of nature relatedness with blind (N=20) and sighted (N=20) participants, along with in-depth interviews with 16 blind participants, to examine how blind people engage with nature and the factors shaping this engagement. Our survey results revealed lower levels of nature relatedness among blind participants compared to sighted peers. Our interview study further highlighted: 1) current practices and challenges of nature engagement, 2) attitudes and values that shape engagement, and 3) expectations for assistive technologies that support safe and meaningful engagement. We also provide design implications to guide future technologies that support nature engagement for blind people. Overall, our findings illustrate how blind people experience nature beyond vision and lay a foundation for technologies that support inclusive nature engagement.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u89c6\u529b\u969c\u788d\u8005\u548c\u6709\u89c6\u529b\u8005\u5728\u81ea\u7136\u76f8\u5173\u6027\u65b9\u9762\u7684\u5dee\u5f02\uff0c\u63ed\u793a\u4e86\u89c6\u529b\u969c\u788d\u8005\u5728\u53c2\u4e0e\u81ea\u7136\u6d3b\u52a8\u65f6\u7684\u6311\u6218\u548c\u9700\u6c42\uff0c\u5e76\u63d0\u51fa\u4e86\u6280\u672f\u53d1\u5c55\u7684\u5efa\u8bae\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u5bf9\u89c6\u529b\u969c\u788d\u8005\u4e0e\u81ea\u7136\u5173\u7cfb\u8ba4\u8bc6\u7684\u7f3a\u53e3\uff0c\u63a2\u8ba8\u4ed6\u4eec\u5982\u4f55\u4e0e\u81ea\u7136\u4e92\u52a8\u53ca\u5176\u9762\u4e34\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5bf920\u540d\u89c6\u529b\u969c\u788d\u8005\u548c20\u540d\u6709\u89c6\u529b\u8005\u8fdb\u884c\u8c03\u67e5\uff0c\u4ee5\u53ca\u5bf916\u540d\u89c6\u529b\u969c\u788d\u8005\u8fdb\u884c\u6df1\u5165\u8bbf\u8c08\uff0c\u7814\u7a76\u5176\u4e0e\u81ea\u7136\u7684\u4e92\u52a8\u65b9\u5f0f\u53ca\u5f71\u54cd\u56e0\u7d20\u3002", "result": "\u8be5\u7814\u7a76\u53d1\u73b0\u89c6\u529b\u969c\u788d\u8005\u4e0e\u81ea\u7136\u7684\u76f8\u5173\u6027\u6c34\u5e73\u4f4e\u4e8e\u6709\u89c6\u529b\u7684\u4eba\uff0c\u5e76\u63ed\u793a\u4e86\u4ed6\u4eec\u5728\u4e0e\u81ea\u7136\u4e92\u52a8\u65f6\u7684\u5b9e\u8df5\u3001\u6001\u5ea6\u548c\u6280\u672f\u671f\u671b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u7406\u89e3\u89c6\u529b\u969c\u788d\u8005\u5982\u4f55\u8d85\u8d8a\u89c6\u89c9\u4f53\u9a8c\u81ea\u7136\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u6280\u672f\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2602.17101", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17101", "abs": "https://arxiv.org/abs/2602.17101", "authors": ["Varun Burde", "Pavel Burget", "Torsten Sattler"], "title": "Benchmarking the Effects of Object Pose Estimation and Reconstruction on Robotic Grasping Success", "comment": null, "summary": "3D reconstruction serves as the foundational layer for numerous robotic perception tasks, including 6D object pose estimation and grasp pose generation. Modern 3D reconstruction methods for objects can produce visually and geometrically impressive meshes from multi-view images, yet standard geometric evaluations do not reflect how reconstruction quality influences downstream tasks such as robotic manipulation performance. This paper addresses this gap by introducing a large-scale, physics-based benchmark that evaluates 6D pose estimators and 3D mesh models based on their functional efficacy in grasping. We analyze the impact of model fidelity by generating grasps on various reconstructed 3D meshes and executing them on the ground-truth model, simulating how grasp poses generated with an imperfect model affect interaction with the real object. This assesses the combined impact of pose error, grasp robustness, and geometric inaccuracies from 3D reconstruction. Our results show that reconstruction artifacts significantly decrease the number of grasp pose candidates but have a negligible effect on grasping performance given an accurately estimated pose. Our results also reveal that the relationship between grasp success and pose error is dominated by spatial error, and even a simple translation error provides insight into the success of the grasping pose of symmetric objects. This work provides insight into how perception systems relate to object manipulation using robots.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u7269\u7406\u7684\u5927\u89c4\u6a21\u57fa\u51c6\uff0c\u65e8\u5728\u8bc4\u4f303D\u91cd\u5efa\u5728\u6293\u53d6\u4e2d\u7684\u529f\u80fd\u6548\u80fd\uff0c\u5e76\u5206\u6790\u6a21\u578b\u51c6\u786e\u6027\u5bf9\u6293\u53d6\u6210\u529f\u7684\u5f71\u54cd\u3002", "motivation": "\u5f53\u524d\u76843D\u91cd\u5efa\u65b9\u6cd5\u5728\u751f\u6210\u7269\u4f53\u7684\u89c6\u89c9\u548c\u51e0\u4f55\u7f51\u683c\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5bf9\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u673a\u5668\u4eba\u64cd\u63a7\u6027\u80fd\uff09\u7684\u5f71\u54cd\u5374\u672a\u5f97\u5230\u5145\u5206\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7\u751f\u6210\u6293\u53d6\u65b9\u6848\u5e76\u5728\u771f\u5b9e\u6a21\u578b\u4e0a\u6267\u884c\uff0c\u5206\u6790\u5404\u79cd\u91cd\u5efa3D\u7f51\u683c\u5bf9\u6293\u53d6\u7684\u5f71\u54cd\uff0c\u8bc4\u4f30\u59ff\u6001\u8bef\u5dee\u3001\u6293\u53d6\u7684\u9c81\u68d2\u6027\u53ca\u51e0\u4f55\u4e0d\u51c6\u786e\u6027\u3002", "result": "\u91cd\u5efa\u4f2a\u5f71\u663e\u8457\u51cf\u5c11\u4e86\u6293\u53d6\u59ff\u6001\u5019\u9009\u6570\u91cf\uff0c\u4f46\u5728\u51c6\u786e\u4f30\u8ba1\u59ff\u6001\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u6293\u53d6\u6027\u80fd\u7684\u5f71\u54cd\u5fae\u4e4e\u5176\u5fae\u3002\u540c\u65f6\uff0c\u6293\u53d6\u6210\u529f\u4e0e\u59ff\u6001\u8bef\u5dee\u4e4b\u95f4\u7684\u5173\u7cfb\u4e3b\u8981\u53d7\u7a7a\u95f4\u8bef\u5dee\u7684\u4e3b\u5bfc\uff0c\u5373\u4f7f\u662f\u7b80\u5355\u7684\u5e73\u79fb\u8bef\u5dee\u4e5f\u80fd\u63d0\u4f9b\u5bf9\u5bf9\u79f0\u7269\u4f53\u6293\u53d6\u59ff\u6001\u6210\u529f\u7684\u6d1e\u5bdf\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u7406\u89e3\u611f\u77e5\u7cfb\u7edf\u4e0e\u673a\u5668\u4eba\u7269\u4f53\u64cd\u63a7\u4e4b\u95f4\u7684\u5173\u7cfb\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2602.17185", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17185", "abs": "https://arxiv.org/abs/2602.17185", "authors": ["U\u011fur Gen\u00e7", "Heng Gu", "Chadha Degachi", "Evangelos Niforatos", "Senthil Chandrasegaran", "Himanshu Verma"], "title": "The Bots of Persuasion: Examining How Conversational Agents' Linguistic Expressions of Personality Affect User Perceptions and Decisions", "comment": "Accepted to be presented at CHI'26 in Barcelona", "summary": "Large Language Model-powered conversational agents (CAs) are increasingly capable of projecting sophisticated personalities through language, but how these projections affect users is unclear. We thus examine how CA personalities expressed linguistically affect user decisions and perceptions in the context of charitable giving. In a crowdsourced study, 360 participants interacted with one of eight CAs, each projecting a personality composed of three linguistic aspects: attitude (optimistic/pessimistic), authority (authoritative/submissive), and reasoning (emotional/rational). While the CA's composite personality did not affect participants' decisions, it did affect their perceptions and emotional responses. Particularly, participants interacting with pessimistic CAs felt lower emotional state and lower affinity towards the cause, perceived the CA as less trustworthy and less competent, and yet tended to donate more toward the charity. Perceptions of trust, competence, and situational empathy significantly predicted donation decisions. Our findings emphasize the risks CAs pose as instruments of manipulation, subtly influencing user perceptions and decisions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5bf9\u8bdd\u4ee3\u7406\uff08CA\uff09\u7684\u8bed\u8a00\u8868\u8fbe\u5982\u4f55\u5f71\u54cd\u7528\u6237\u5728\u6148\u5584\u6350\u8d60\u4e2d\u7684\u51b3\u7b56\u4e0e\u611f\u77e5\uff0c\u53d1\u73b0\u60b2\u89c2CA\u4f1a\u5bfc\u81f4\u7528\u6237\u60c5\u611f\u4f4e\u843d\uff0c\u5374\u53ef\u80fd\u589e\u52a0\u6350\u6b3e\u503e\u5411\uff0c\u5f3a\u8c03\u4e86CA\u5728\u64cd\u63a7\u7528\u6237\u51b3\u7b56\u4e2d\u7684\u6f5c\u5728\u98ce\u9669\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u5bf9\u8bdd\u4ee3\u7406\uff08CA\uff09\u5982\u4f55\u901a\u8fc7\u8bed\u8a00\u4f20\u8fbe\u4e0d\u540c\u7684\u4e2a\u6027\uff0c\u5e76\u63a2\u7d22\u8fd9\u79cd\u4e2a\u6027\u5bf9\u7528\u6237\u51b3\u7b56\u548c\u611f\u77e5\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u6148\u5584\u6350\u8d60\u7684\u80cc\u666f\u4e0b\u3002", "method": "\u5728\u4e00\u9879\u4f17\u5305\u7814\u7a76\u4e2d\uff0c360\u540d\u53c2\u4e0e\u8005\u4e0e\u516b\u79cd\u4e0d\u540c\u4eba\u683c\u7684CA\u4e92\u52a8\u3002\u8fd9\u4e9bCA\u7684\u4eba\u683c\u7531\u4e09\u79cd\u8bed\u8a00\u7279\u5f81\u7ec4\u6210\uff1a\u6001\u5ea6\uff08\u4e50\u89c2/\u60b2\u89c2\uff09\u3001\u6743\u5a01\uff08\u6743\u5a01/\u987a\u4ece\uff09\u548c\u63a8\u7406\uff08\u60c5\u611f/\u7406\u6027\uff09\u3002", "result": "\u53c2\u4e0e\u8005\u7684\u6350\u8d60\u51b3\u7b56\u672a\u53d7\u5230CA\u6574\u4f53\u4eba\u683c\u7684\u5f71\u54cd\uff0c\u4f46\u5bf9CA\u7684\u4eba\u683c\u5f71\u54cd\u4e86\u4ed6\u4eec\u7684\u611f\u77e5\u548c\u60c5\u611f\u53cd\u5e94\u3002\u7279\u522b\u662f\uff0c\u4e0e\u60b2\u89c2CA\u4e92\u52a8\u7684\u53c2\u4e0e\u8005\u611f\u5230\u8f83\u4f4e\u7684\u60c5\u7eea\u72b6\u6001\u548c\u5bf9\u6148\u5584\u4e8b\u4e1a\u7684\u4f4e\u4eb2\u548c\u529b\uff0c\u8ba4\u4e3aCA\u7684\u53ef\u4fe1\u5ea6\u548c\u80fd\u529b\u8f83\u4f4e\uff0c\u4f46\u8fd8\u662f\u66f4\u503e\u5411\u4e8e\u5411\u6148\u5584\u673a\u6784\u6350\u6b3e\u3002", "conclusion": "CA\u4f5c\u4e3a\u64cd\u63a7\u5de5\u5177\u7684\u98ce\u9669\u7a81\u663e\uff0c\u80fd\u591f\u5fae\u5999\u5730\u5f71\u54cd\u7528\u6237\u7684\u611f\u77e5\u548c\u51b3\u7b56\uff0c\u5c24\u5176\u662f\u5728\u6350\u8d60\u51b3\u7b56\u4e2d\uff0c\u4fe1\u4efb\u3001\u80fd\u529b\u548c\u60c5\u611f\u540c\u7406\u5fc3\u7684\u611f\u77e5\u663e\u8457\u9884\u6d4b\u4e86\u6350\u8d60\u7684\u51b3\u5b9a\u3002"}}
{"id": "2602.17110", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17110", "abs": "https://arxiv.org/abs/2602.17110", "authors": ["Tanisha Parulekar", "Ge Shi", "Josh Pinskier", "David Howard", "Jen Jen Chung"], "title": "Grasp Synthesis Matching From Rigid To Soft Robot Grippers Using Conditional Flow Matching", "comment": null, "summary": "A representation gap exists between grasp synthesis for rigid and soft grippers. Anygrasp [1] and many other grasp synthesis methods are designed for rigid parallel grippers, and adapting them to soft grippers often fails to capture their unique compliant behaviors, resulting in data-intensive and inaccurate models. To bridge this gap, this paper proposes a novel framework to map grasp poses from a rigid gripper model to a soft Fin-ray gripper. We utilize Conditional Flow Matching (CFM), a generative model, to learn this complex transformation. Our methodology includes a data collection pipeline to generate paired rigid-soft grasp poses. A U-Net autoencoder conditions the CFM model on the object's geometry from a depth image, allowing it to learn a continuous mapping from an initial Anygrasp pose to a stable Fin-ray gripper pose. We validate our approach on a 7-DOF robot, demonstrating that our CFM-generated poses achieve a higher overall success rate for seen and unseen objects (34% and 46% respectively) compared to the baseline rigid poses (6% and 25% respectively) when executed by the soft gripper. The model shows significant improvements, particularly for cylindrical (50% and 100% success for seen and unseen objects) and spherical objects (25% and 31% success for seen and unseen objects), and successfully generalizes to unseen objects. This work presents CFM as a data-efficient and effective method for transferring grasp strategies, offering a scalable methodology for other soft robotic systems.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u521a\u6027\u5939\u6301\u5668\u6293\u53d6\u59ff\u52bf\u8f6c\u5316\u4e3a\u8f6f\u6027\u5939\u6301\u5668\u7684\u6846\u67b6\uff0c\u4f7f\u7528CFM\u6a21\u578b\u5b9e\u73b0\u9ad8\u6548\u6293\u53d6\u7b56\u7565\u8f6c\u79fb\u3002", "motivation": "\u73b0\u6709\u7684\u6293\u53d6\u5408\u6210\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u9002\u5e94\u8f6f\u6027\u5939\u6301\u5668\uff0c\u9020\u6210\u6570\u636e\u5bc6\u96c6\u548c\u4e0d\u51c6\u786e\u7684\u6a21\u578b\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u6d41\u5339\u914d\uff08CFM\uff09\u5b66\u4e60\u521a\u6027\u5230\u8f6f\u6027\u5939\u6301\u5668\u7684\u6293\u53d6\u59ff\u52bf\u8f6c\u6362\uff0c\u540c\u65f6\u5229\u7528U-Net\u81ea\u7f16\u7801\u5668\u5bf9\u7269\u4f53\u51e0\u4f55\u5f62\u72b6\u8fdb\u884c\u6761\u4ef6\u5316\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u5c06\u521a\u6027\u5939\u6301\u5668\u6a21\u578b\u7684\u6293\u53d6\u59ff\u52bf\u6620\u5c04\u5230\u8f6f\u6027Fin-ray\u5939\u6301\u5668\uff0c\u4f7f\u7528\u6761\u4ef6\u6d41\u5339\u914d\uff08CFM\uff09\u751f\u6210\u6a21\u578b\u5b66\u4e60\u6b64\u590d\u6742\u8f6c\u6362\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7CFM\u751f\u6210\u7684\u6293\u53d6\u59ff\u52bf\u5728\u5df2\u89c1\u548c\u672a\u89c1\u7269\u4f53\u4e0a\u5b9e\u73b0\u4e86\u8f83\u9ad8\u7684\u6210\u529f\u7387\uff0c\u8bc1\u660e\u4e86\u8fd9\u4e00\u65b9\u6cd5\u7684\u6709\u6548\u6027\u53ca\u6570\u636e\u6548\u7387\u3002"}}
{"id": "2602.17215", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.17215", "abs": "https://arxiv.org/abs/2602.17215", "authors": ["Yi Shan", "Yixuan He", "Zekai Shao", "Kai Xu", "Siming Chen"], "title": "NotebookRAG: Retrieving Multiple Notebooks to Augment the Generation of EDA Notebooks for Crowd-Wisdom", "comment": "11 pages, 7 figures", "summary": "High-quality exploratory data analysis (EDA) is essential in the data science pipeline, but remains highly dependent on analysts' expertise and effort. While recent LLM-based approaches partially reduce this burden, they struggle to generate effective analysis plans and appropriate insights and visualizations when user intent is abstract. Meanwhile, a vast collection of analysis notebooks produced across platforms and organizations contains rich analytical knowledge that can potentially guide automated EDA. Retrieval-augmented generation (RAG) provides a natural way to leverage such corpora, but general methods often treat notebooks as static documents and fail to fully exploit their potential knowledge for automating EDA. To address these limitations, we propose NotebookRAG, a method that takes user intent, datasets, and existing notebooks as input to retrieve, enhance, and reuse relevant notebook content for automated EDA generation. For retrieval, we transform code cells into context-enriched executable components, which improve retrieval quality and enable rerun with new data to generate updated visualizations and reliable insights. For generation, an agent leverages enhanced retrieval content to construct effective EDA plans, derive insights, and produce appropriate visualizations. Evidence from a user study with 24 participants confirms the superiority of our method in producing high-quality and intent-aligned EDA notebooks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faNotebookRAG\u65b9\u6cd5\uff0c\u7ed3\u5408\u7528\u6237\u610f\u56fe\u3001\u6570\u636e\u96c6\u548c\u73b0\u6709\u7b14\u8bb0\u672c\uff0c\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u63a2\u7d22\u6027\u6570\u636e\u5206\u6790\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u6790\u6548\u7387\u548c\u6548\u679c\u3002", "motivation": "\u63a2\u7d22\u6027\u6570\u636e\u5206\u6790\u5bf9\u6570\u636e\u79d1\u5b66\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f9d\u8d56\u4e8e\u5206\u6790\u5e08\u7684\u4e13\u4e1a\u77e5\u8bc6\u548c\u52aa\u529b\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u5728\u751f\u6210\u6709\u6548\u7684\u5206\u6790\u8ba1\u5212\u65f6\u9762\u4e34\u6311\u6218\u3002", "method": "NotebookRAG\u901a\u8fc7\u5c06\u7528\u6237\u610f\u56fe\u3001\u6570\u636e\u96c6\u548c\u73b0\u6709\u7b14\u8bb0\u672c\u4f5c\u4e3a\u8f93\u5165\uff0c\u68c0\u7d22\u3001\u589e\u5f3a\u548c\u91cd\u7528\u76f8\u5173\u7b14\u8bb0\u672c\u5185\u5bb9\uff0c\u6765\u5b9e\u73b0\u81ea\u52a8\u5316\u7684\u63a2\u7d22\u6027\u6570\u636e\u5206\u6790\u751f\u6210\u3002", "result": "\u7528\u6237\u7814\u7a76\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u548c\u7b26\u5408\u7528\u6237\u610f\u56fe\u7684EDA\u7b14\u8bb0\u672c\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "NotebookRAG\u65b9\u6cd5\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u7b26\u5408\u7528\u6237\u610f\u56fe\u7684\u63a2\u7d22\u6027\u6570\u636e\u5206\u6790\u7b14\u8bb0\u672c\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\u3002"}}
{"id": "2602.17128", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17128", "abs": "https://arxiv.org/abs/2602.17128", "authors": ["Huishi Huang", "Jack Klusmann", "Haozhe Wang", "Shuchen Ji", "Fengkang Ying", "Yiyuan Zhang", "John Nassour", "Gordon Cheng", "Daniela Rus", "Jun Liu", "Marcelo H Ang", "Cecilia Laschi"], "title": "Physical Human-Robot Interaction for Grasping in Augmented Reality via Rigid-Soft Robot Synergy", "comment": "Camera-ready version for RoboSoft 2026. 8 pages, 6 figures", "summary": "Hybrid rigid-soft robots combine the precision of rigid manipulators with the compliance and adaptability of soft arms, offering a promising approach for versatile grasping in unstructured environments. However, coordinating hybrid robots remains challenging, due to difficulties in modeling, perception, and cross-domain kinematics. In this work, we present a novel augmented reality (AR)-based physical human-robot interaction framework that enables direct teleoperation of a hybrid rigid-soft robot for simple reaching and grasping tasks. Using an AR headset, users can interact with a simulated model of the robotic system integrated into a general-purpose physics engine, which is superimposed on the real system, allowing simulated execution prior to real-world deployment. To ensure consistent behavior between the virtual and physical robots, we introduce a real-to-simulation parameter identification pipeline that leverages the inherent geometric properties of the soft robot, enabling accurate modeling of its static and dynamic behavior as well as the control system's response.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAR\u7684\u6846\u67b6\uff0c\u4f7f\u5f97\u7528\u6237\u80fd\u591f\u9ad8\u6548\u9065\u63a7\u6df7\u5408\u521a\u8f6f\u673a\u5668\u4eba\u8fdb\u884c\u7b80\u5355\u6293\u53d6\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u865a\u62df\u4e0e\u7269\u7406\u673a\u5668\u4eba\u95f4\u7684\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u6df7\u5408\u521a\u8f6f\u673a\u5668\u4eba\u7ed3\u5408\u4e86\u521a\u6027\u64cd\u7eb5\u5668\u7684\u7cbe\u786e\u6027\u4e0e\u8f6f\u81c2\u7684\u67d4\u97e7\u6027\u4e0e\u9002\u5e94\u6027\uff0c\u63d0\u4f9b\u4e86\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u591a\u529f\u80fd\u6293\u53d6\u7684\u6f5c\u529b\u3002\u7136\u800c\uff0c\u534f\u8c03\u6df7\u5408\u673a\u5668\u4eba\u4ecd\u7136\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u7684\u7269\u7406\u4eba\u673a\u4ea4\u4e92\u6846\u67b6\uff0c\u5141\u8bb8\u5bf9\u6df7\u5408\u521a\u8f6f\u673a\u5668\u4eba\u7684\u76f4\u63a5\u9065\u64cd\u4f5c\uff0c\u4ee5\u8fdb\u884c\u7b80\u5355\u7684\u4f38\u5c55\u548c\u6293\u53d6\u4efb\u52a1\u3002\u4f7f\u7528AR\u5934\u6234\u8bbe\u5907\uff0c\u7528\u6237\u53ef\u4ee5\u4e0e\u96c6\u6210\u5728\u901a\u7528\u7269\u7406\u5f15\u64ce\u4e2d\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u6a21\u62df\u6a21\u578b\u8fdb\u884c\u4ea4\u4e92\u3002", "result": "\u901a\u8fc7AR\u6280\u672f\uff0c\u7528\u6237\u53ef\u4ee5\u5728\u771f\u5b9e\u7cfb\u7edf\u4e4b\u4e0a\u6267\u884c\u6a21\u62df\u6267\u884c\uff0c\u786e\u4fdd\u865a\u62df\u548c\u7269\u7406\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u4e00\u81f4\u884c\u4e3a\u3002", "conclusion": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4ece\u771f\u5b9e\u5230\u6a21\u62df\u7684\u53c2\u6570\u8bc6\u522b\u6d41\u7a0b\uff0c\u5229\u7528\u8f6f\u673a\u5668\u4eba\u56fa\u6709\u7684\u51e0\u4f55\u7279\u6027\uff0c\u80fd\u591f\u51c6\u786e\u5efa\u6a21\u5176\u9759\u6001\u548c\u52a8\u6001\u884c\u4e3a\u4ee5\u53ca\u63a7\u5236\u7cfb\u7edf\u7684\u54cd\u5e94\u3002"}}
{"id": "2602.17340", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.17340", "abs": "https://arxiv.org/abs/2602.17340", "authors": ["Rui Yao", "Qiuyuan Ren", "Felicia Fang-Yi Tan", "Chen Yang", "Xiaoyu Zhang", "Shengdong Zhao"], "title": "PersonaMail: Learning and Adapting Personal Communication Preferences for Context-Aware Email Writing", "comment": "21 pages, 5 figures. Accepted to the 31st International Conference on Intelligent User Interfaces (IUI 26), March 23-26, 2026, Paphos, Cyprus", "summary": "LLM-assisted writing has seen rapid adoption in interpersonal communication, yet current systems often fail to capture the subtle tones essential for effectiveness. Email writing exemplifies this challenge: effective messages require careful alignment with intent, relationship, and context beyond mere fluency. Through formative studies, we identified three key challenges: articulating nuanced communicative intent, making modifications at multiple levels of granularity, and reusing effective tone strategies across messages. We developed PersonaMail, a system that addresses these gaps through structured communication factor exploration, granular editing controls, and adaptive reuse of successful strategies. Our evaluation compared PersonaMail against standard LLM interfaces, and showed improved efficiency in both immediate and repeated use, alongside higher user satisfaction. We contribute design implications for AI-assisted communication systems that prioritize interpersonal nuance over generic text generation.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86PersonaMail\u7cfb\u7edf\uff0c\u4ee5\u6539\u8fdbLLM\u8f85\u52a9\u90ae\u4ef6\u5199\u4f5c\uff0c\u901a\u8fc7\u5173\u6ce8\u4eba\u9645\u4ea4\u6d41\u7684\u7ec6\u5fae\u5dee\u522b\uff0c\u63d0\u5347\u6c9f\u901a\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u7684LLM\u7cfb\u7edf\u5728\u6355\u6349\u6709\u6548\u6c9f\u901a\u6240\u9700\u7684\u7ec6\u817b\u8bed\u8c03\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u7535\u5b50\u90ae\u4ef6\u5199\u4f5c\u4e2d\u3002", "method": "\u901a\u8fc7\u5f62\u6210\u6027\u7814\u7a76\u8bc6\u522b\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff0c\u5e76\u5f00\u53d1PersonaMail\u7cfb\u7edf\uff0c\u63d0\u4f9b\u7ed3\u6784\u5316\u6c9f\u901a\u56e0\u5b50\u7684\u63a2\u7d22\u3001\u7c92\u5ea6\u7f16\u8f91\u63a7\u5236\u548c\u6210\u529f\u7b56\u7565\u7684\u81ea\u9002\u5e94\u91cd\u7528\u3002", "result": "\u4e0e\u6807\u51c6LLM\u63a5\u53e3\u76f8\u6bd4\uff0cPersonaMail\u5728\u6548\u7387\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u4f18\u5148\u8003\u8651\u4eba\u9645\u7ec6\u5fae\u5dee\u522b\u7684AI\u8f85\u52a9\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u542f\u793a\u3002"}}
{"id": "2602.17166", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17166", "abs": "https://arxiv.org/abs/2602.17166", "authors": ["Antonio Franchi", "Chiara Gabellieri"], "title": "Geometric Inverse Flight Dynamics on SO(3) and Application to Tethered Fixed-Wing Aircraft", "comment": null, "summary": "We present a robotics-oriented, coordinate-free formulation of inverse flight dynamics for fixed-wing aircraft on SO(3). Translational force balance is written in the world frame and rotational dynamics in the body frame; aerodynamic directions (drag, lift, side) are defined geometrically, avoiding local attitude coordinates. Enforcing coordinated flight (no sideslip), we derive a closed-form trajectory-to-input map yielding the attitude, angular velocity, and thrust-angle-of-attack pair, and we recover the aerodynamic moment coefficients component-wise. Applying such a map to tethered flight on spherical parallels, we obtain analytic expressions for the required bank angle and identify a specific zero-bank locus where the tether tension exactly balances centrifugal effects, highlighting the decoupling between aerodynamic coordination and the apparent gravity vector. Under a simple lift/drag law, the minimal-thrust angle of attack admits a closed form. These pointwise quasi-steady inversion solutions become steady-flight trim when the trajectory and rotational dynamics are time-invariant. The framework bridges inverse simulation in aeronautics with geometric modeling in robotics, providing a rigorous building block for trajectory design and feasibility checks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u5750\u6807\u7684\u9006\u98de\u884c\u52a8\u529b\u5b66\u516c\u5f0f\uff0c\u9002\u7528\u4e8e\u56fa\u5b9a\u7ffc\u98de\u673a\uff0c\u63d0\u4f9b\u8f68\u8ff9\u8bbe\u8ba1\u548c\u53ef\u884c\u6027\u68c0\u67e5\u7684\u65b0\u65b9\u6cd5\u3002", "motivation": "\u63d0\u51fa\u4e00\u79cd\u9762\u5411\u673a\u5668\u4eba\u6280\u672f\u7684\u3001\u65e0\u5750\u6807\u7684\u9006\u98de\u884c\u52a8\u529b\u5b66\u516c\u5f0f\uff0c\u4ee5\u9002\u5e94\u56fa\u5b9a\u7ffc\u98de\u673a\u7684\u9700\u6c42\u3002", "method": "\u5728\u4e16\u754c\u5750\u6807\u7cfb\u4e2d\u4e66\u5199\u5e73\u79fb\u529b\u5e73\u8861\uff0c\u5728\u673a\u8eab\u5750\u6807\u7cfb\u4e2d\u4e66\u5199\u65cb\u8f6c\u52a8\u529b\u5b66\uff0c\u5e76\u51e0\u4f55\u5b9a\u4e49\u6c14\u52a8\u529b\u7684\u65b9\u5411\uff0c\u907f\u514d\u5c40\u90e8\u59ff\u6001\u5750\u6807\u3002\u63a8\u5bfc\u51fa\u95ed\u5408\u5f62\u5f0f\u7684\u8f68\u8ff9\u5230\u8f93\u5165\u7684\u6620\u5c04\uff0c\u5e76\u9010\u9879\u6062\u590d\u6c14\u52a8\u529b\u77e9\u7cfb\u6570\u3002", "result": "\u901a\u8fc7\u5c06\u8fd9\u79cd\u6620\u5c04\u5e94\u7528\u4e8e\u56f4\u7ed5\u7403\u9762\u5e73\u884c\u7ebf\u7684\u7275\u5f15\u98de\u884c\uff0c\u83b7\u5f97\u6240\u9700\u7684\u6a2a\u6eda\u89d2\u7684\u89e3\u6790\u8868\u8fbe\u5f0f\uff0c\u5e76\u8bc6\u522b\u51fa\u7279\u5b9a\u7684\u96f6\u6a2a\u6eda\u8f68\u8ff9\uff0c\u5176\u4e2d\u7275\u5f15\u529b\u6b63\u597d\u5e73\u8861\u79bb\u5fc3\u6548\u5e94\uff0c\u7a81\u51fa\u4e86\u6c14\u52a8\u534f\u8c03\u4e0e\u8868\u89c2\u91cd\u529b\u77e2\u91cf\u4e4b\u95f4\u7684\u89e3\u8026\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c06\u822a\u7a7a\u5b66\u4e2d\u7684\u9006\u4eff\u771f\u4e0e\u673a\u5668\u4eba\u4e2d\u7684\u51e0\u4f55\u5efa\u6a21\u8fde\u63a5\u8d77\u6765\uff0c\u4e3a\u8f68\u8ff9\u8bbe\u8ba1\u548c\u53ef\u884c\u6027\u68c0\u67e5\u63d0\u4f9b\u4e86\u4e25\u8c28\u7684\u57fa\u7840\u3002"}}
{"id": "2602.17448", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.17448", "abs": "https://arxiv.org/abs/2602.17448", "authors": ["Michael Tompkins", "Nihaarika Agarwal", "Ananta Soneji", "Robert Wasinger", "Connor Nelson", "Kevin Leach", "Rakibul Hasan", "Adam Doup\u00e9", "Daniel Votipka", "Yan Shoshitaishvili", "Jaron Mink"], "title": "Do Hackers Dream of Electric Teachers?: A Large-Scale, In-Situ Evaluation of Cybersecurity Student Behaviors and Performance with AI Tutors", "comment": "33 pages, 7 figures", "summary": "To meet the ever-increasing demands of the cybersecurity workforce, AI tutors have been proposed for personalized, scalable education. But, while AI tutors have shown promise in introductory programming courses, no work has evaluated their use in hands-on exploration and exploitation of systems (e.g., ``capture-the-flag'') commonly used to teach cybersecurity. Thus, despite growing interest and need, no work has evaluated how students use AI tutors or whether they benefit from their presence in real, large-scale cybersecurity courses. To answer this, we conducted a semester-long observational study on the use of an embedded AI tutor with 309 students in an upper-division introductory cybersecurity course. By analyzing 142,526 student queries sent to the AI tutor across 396 cybersecurity challenges spanning 9 core cybersecurity topics and an accompanying set of post-semester surveys, we find (1) what queries and conversational strategies students use with AI tutors, (2) how these strategies correlate with challenge completion, and (3) students' perceptions of AI tutors in cybersecurity education. In particular, we identify three broad AI tutor conversational styles among users: Short (bounded, few-turn exchanges), Reactive (repeatedly submitting code and errors), and Proactive (driving problem-solving through targeted inquiry). We also find that the use of these styles significantly predicts challenge completion, and that this effect increases as materials become more advanced. Furthermore, students valued the tutor's availability but reported that it became less useful for harder material. Based on this, we provide suggestions for security educators and developers on practical AI tutor use.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86AI\u5bfc\u5e08\u5728\u7f51\u7edc\u5b89\u5168\u6559\u80b2\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u53d1\u73b0\u4e0d\u540c\u7684\u4e92\u52a8\u98ce\u683c\u4e0e\u5b66\u751f\u7684\u6311\u6218\u5b8c\u6210\u60c5\u51b5\u76f8\u5173\uff0c\u5e76\u63d0\u4f9b\u4e86\u9488\u5bf9\u6559\u80b2\u8005\u548c\u5f00\u53d1\u8005\u7684\u5efa\u8bae\u3002", "motivation": "\u968f\u7740\u7f51\u7edc\u5b89\u5168\u9886\u57df\u5bf9\u9ad8\u7d20\u8d28\u4eba\u624d\u7684\u9700\u6c42\u4e0d\u65ad\u4e0a\u5347\uff0c\u6025\u9700\u627e\u5230\u6709\u6548\u7684\u6559\u80b2\u65b9\u6cd5\u3002\u8003\u8651\u5230AI\u5bfc\u5e08\u5728\u7f16\u7a0b\u8bfe\u7a0b\u4e2d\u7684\u6f5c\u529b\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865AI\u5bfc\u5e08\u5728\u7f51\u7edc\u5b89\u5168\u6559\u80b2\u5e94\u7528\u4e2d\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5bf9309\u540d\u5b66\u751f\u5728\u4e00\u4e2a\u5b66\u671f\u5185\u4e0eAI\u5bfc\u5e08\u7684\u4e92\u52a8\u8fdb\u884c\u89c2\u5bdf\u548c\u5206\u6790\uff0c\u7814\u7a76\u5176\u67e5\u8be2\u6a21\u5f0f\u3001\u4f1a\u8bdd\u7b56\u7565\u53ca\u5176\u4e0e\u6311\u6218\u5b8c\u6210\u7684\u5173\u7cfb\u3002", "result": "\u672c\u7814\u7a76\u901a\u8fc7\u89c2\u5bdf309\u540d\u5b66\u751f\u5728\u4e00\u4e2a\u9ad8\u7ea7\u5165\u95e8\u7f51\u7edc\u5b89\u5168\u8bfe\u7a0b\u4e2d\u4f7f\u7528\u5d4c\u5165\u5f0fAI\u5bfc\u5e08\u7684\u60c5\u51b5\uff0c\u8bc4\u4f30\u4e86AI\u5bfc\u5e08\u5728\u7f51\u7edc\u5b89\u5168\u6559\u80b2\u4e2d\u7684\u6548\u679c\u3002\u901a\u8fc7\u5206\u6790142,526\u6761\u5b66\u751f\u5411AI\u5bfc\u5e08\u53d1\u9001\u7684\u67e5\u8be2\uff0c\u6211\u4eec\u53d1\u73b0\u5b66\u751f\u4e0eAI\u5bfc\u5e08\u7684\u4e92\u52a8\u6a21\u5f0f\u548c\u6210\u529f\u5b8c\u6210\u7f51\u7edc\u5b89\u5168\u6311\u6218\u7684\u76f8\u5173\u6027\u3002", "conclusion": "\u5b66\u751f\u5bf9AI\u5bfc\u5e08\u7684\u4f7f\u7528\u8868\u73b0\u51fa\u4e86\u4e0d\u540c\u7684\u4f1a\u8bdd\u98ce\u683c\uff0c\u8fd9\u4e9b\u98ce\u683c\u4e0e\u6311\u6218\u5b8c\u6210\u60c5\u51b5\u663e\u8457\u76f8\u5173\u3002\u5c3d\u7ba1\u5b66\u751f\u5bf9AI\u5bfc\u5e08\u7684\u4ef7\u503c\u8868\u793a\u8ba4\u53ef\uff0c\u4f46\u5728\u5b66\u4e60\u66f4\u96be\u7684\u6750\u6599\u65f6\uff0c\u5176\u6709\u6548\u6027\u51cf\u5f31\u3002"}}
{"id": "2602.17199", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17199", "abs": "https://arxiv.org/abs/2602.17199", "authors": ["Antonio Rapuano", "Yaolei Shen", "Federico Califano", "Chiara Gabellieri", "Antonio Franchi"], "title": "Nonlinear Predictive Control of the Continuum and Hybrid Dynamics of a Suspended Deformable Cable for Aerial Pick and Place", "comment": "Accepted to the IEEE International Conference on Robotics and Automation (ICRA) 2026", "summary": "This paper presents a framework for aerial manipulation of an extensible cable that combines a high-fidelity model based on partial differential equations (PDEs) with a reduced-order representation suitable for real-time control. The PDEs are discretised using a finite-difference method, and proper orthogonal decomposition is employed to extract a reduced-order model (ROM) that retains the dominant deformation modes while significantly reducing computational complexity. Based on this ROM, a nonlinear model predictive control scheme is formulated, capable of stabilizing cable oscillations and handling hybrid transitions such as payload attachment and detachment. Simulation results confirm the stability, efficiency, and robustness of the ROM, as well as the effectiveness of the controller in regulating cable dynamics under a range of operating conditions. Additional simulations illustrate the application of the ROM for trajectory planning in constrained environments, demonstrating the versatility of the proposed approach. Overall, the framework enables real-time, dynamics-aware control of unmanned aerial vehicles (UAVs) carrying suspended flexible cables.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7ed3\u5408PDE\u548c\u964d\u9636\u6a21\u578b\u7684\u7a7a\u4e2d\u7535\u7f06\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u65b9\u6848\u5b9e\u73b0\u7535\u7f06\u52a8\u6001\u7684\u7a33\u5b9a\u4e0e\u8c03\u63a7\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u5e94\u7528\u7684\u589e\u591a\uff0c\u5f00\u53d1\u9ad8\u6548\u7684\u7a7a\u4e2d\u7535\u7f06\u64cd\u63a7\u65b9\u6cd5\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\uff0c\u4ee5\u786e\u4fdd\u5176\u5728\u5404\u79cd\u590d\u6742\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u3002", "method": "\u91c7\u7528\u6709\u9650\u5dee\u5206\u6cd5\u5bf9\u504f\u5fae\u5206\u65b9\u7a0b\u8fdb\u884c\u79bb\u6563\u5316\uff0c\u5e76\u4f7f\u7528\u9002\u5f53\u7684\u6b63\u4ea4\u5206\u89e3\u63d0\u53d6\u964d\u9636\u6a21\u578b\uff0c\u7ed3\u5408\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u65b9\u6848\u8fdb\u884c\u7535\u7f06\u52a8\u6001\u8c03\u8282\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u53ef\u6269\u5c55\u7535\u7f06\u7684\u7a7a\u4e2d\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u57fa\u4e8e\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDEs\uff09\u7684\u9ad8\u4fdd\u771f\u6a21\u578b\u4e0e\u9002\u5408\u5b9e\u65f6\u63a7\u5236\u7684\u964d\u9636\u8868\u793a\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u65e0\u4eba\u673a\u643a\u5e26\u60ac\u6302\u67d4\u6027\u7535\u7f06\u7684\u5b9e\u65f6\u3001\u52a8\u6001\u610f\u8bc6\u63a7\u5236\uff0c\u5c55\u793a\u4e86\u5176\u5728\u591a\u79cd\u64cd\u4f5c\u6761\u4ef6\u4e0b\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.17481", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.17481", "abs": "https://arxiv.org/abs/2602.17481", "authors": ["Yanni Mei", "Samuel Wendt", "Florian Mueller", "Jan Gugenheimer"], "title": "ShadAR: LLM-driven shader generation to transform visual perception in Augmented Reality", "comment": null, "summary": "Augmented Reality (AR) can simulate various visual perceptions, such as how individuals with colorblindness see the world. However, these simulations require developers to predefine each visual effect, limiting flexibility. We present ShadAR, an AR application enabling real-time transformation of visual perception through shader generation using large language models (LLMs). ShadAR allows users to express their visual intent via natural language, which is interpreted by an LLM to generate corresponding shader code. This shader is then compiled real-time to modify the AR headset viewport. We present our LLM-driven shader generation pipeline and demonstrate its ability to transform visual perception for inclusiveness and creativity.", "AI": {"tldr": "ShadAR\u662f\u4e00\u4e2a\u589e\u5f3a\u73b0\u5b9e\u5e94\u7528\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u65f6\u751f\u6210\u89c6\u89c9\u6548\u679c\u7684\u7740\u8272\u5668\u4ee3\u7801\uff0c\u8ba9\u7528\u6237\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8868\u8fbe\u89c6\u89c9\u610f\u56fe\u3002", "motivation": "\u4ee5\u5f80\u7684\u589e\u5f3a\u73b0\u5b9e\u89c6\u89c9\u6548\u679c\u6a21\u62df\u9700\u8981\u5f00\u53d1\u8005\u63d0\u524d\u5b9a\u4e49\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\u3002ShadAR\u65e8\u5728\u901a\u8fc7\u5b9e\u65f6\u751f\u6210\u89c6\u89c9\u6548\u679c\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8f93\u5165\uff0c\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u89e3\u6790\u7528\u6237\u610f\u56fe\uff0c\u751f\u6210\u5bf9\u5e94\u7684\u56fe\u5f62\u7740\u8272\u5668\u4ee3\u7801\uff0c\u5e76\u5728\u589e\u5f3a\u73b0\u5b9e\u5934\u663e\u4e2d\u5b9e\u65f6\u7f16\u8bd1\u548c\u5c55\u793a\u3002", "result": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aShadAR\u7684\u589e\u5f3a\u73b0\u5b9e\u5e94\u7528\u7a0b\u5e8f\uff0c\u65e8\u5728\u901a\u8fc7\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5b9e\u73b0\u5bf9\u89c6\u89c9\u611f\u77e5\u7684\u5b9e\u65f6\u8f6c\u53d8\u3002", "conclusion": "ShadAR\u5e94\u7528\u7a0b\u5e8f\u5c55\u793a\u4e86\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u751f\u6210\u56fe\u5f62\u7740\u8272\u5668\u4ee3\u7801\uff0c\u53ef\u5b9e\u73b0\u4e2a\u6027\u5316\u7684\u89c6\u89c9\u4f53\u9a8c\uff0c\u63d0\u9ad8\u4e86\u589e\u5f3a\u73b0\u5b9e\u7684\u5305\u5bb9\u6027\u548c\u521b\u9020\u529b\u3002"}}
{"id": "2602.17226", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17226", "abs": "https://arxiv.org/abs/2602.17226", "authors": ["Lorenzo Montano-Olivan", "Julio A. Placed", "Luis Montano", "Maria T. Lazaro"], "title": "Multi-session Localization and Mapping Exploiting Topological Information", "comment": null, "summary": "Operating in previously visited environments is becoming increasingly crucial for autonomous systems, with direct applications in autonomous driving, surveying, and warehouse or household robotics. This repeated exposure to observing the same areas poses significant challenges for mapping and localization -- key components for enabling any higher-level task. In this work, we propose a novel multi-session framework that builds on map-based localization, in contrast to the common practice of greedily running full SLAM sessions and trying to find correspondences between the resulting maps. Our approach incorporates a topology-informed, uncertainty-aware decision-making mechanism that analyzes the pose-graph structure to detect low-connectivity regions, selectively triggering mapping and loop closing modules. The resulting map and pose-graph are seamlessly integrated into the existing model, reducing accumulated error and enhancing global consistency. We validate our method on overlapping sequences from datasets and demonstrate its effectiveness in a real-world mine-like environment.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4f1a\u8bdd\u6846\u67b6\uff0c\u901a\u8fc7\u5730\u56fe\u5b9a\u4f4d\u548c\u62d3\u6251\u4fe1\u606f\u589e\u5f3a\u51b3\u7b56\uff0c\u4f18\u5316\u4e86\u5730\u56fe\u548c\u59ff\u6001\u56fe\u7684\u4e00\u81f4\u6027\uff0c\u9002\u7528\u4e8e\u81ea\u4e3b\u7cfb\u7edf\u3002", "motivation": "\u5728\u81ea\u4e3b\u7cfb\u7edf\u4e2d\uff0c\u91cd\u590d\u8bbf\u95ee\u5df2\u77e5\u73af\u5883\u5bf9\u6620\u5c04\u548c\u5b9a\u4f4d\u63d0\u51fa\u6311\u6218\uff0c\u8fd9\u5728\u81ea\u52a8\u9a7e\u9a76\u548c\u673a\u5668\u4eba\u7b49\u9886\u57df\u5177\u6709\u91cd\u8981\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5730\u56fe\u7684\u5b9a\u4f4d\u7684\u591a\u4f1a\u8bdd\u6846\u67b6", "result": "\u901a\u8fc7\u5206\u6790\u59ff\u6001\u56fe\u7ed3\u6784\uff0c\u8bc6\u522b\u4f4e\u8fde\u901a\u533a\u57df\uff0c\u9009\u62e9\u6027\u89e6\u53d1\u6620\u5c04\u548c\u95ed\u73af\u6a21\u5757\uff0c\u63d0\u5347\u5730\u56fe\u548c\u59ff\u6001\u56fe\u7684\u6574\u5408\u3002", "conclusion": "\u51cf\u5c11\u4e86\u7d2f\u8ba1\u8bef\u5dee\uff0c\u589e\u5f3a\u4e86\u5168\u5c40\u4e00\u81f4\u6027\uff0c\u5e76\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u7c7b\u4f3c\u77ff\u5c71\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.17483", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.17483", "abs": "https://arxiv.org/abs/2602.17483", "authors": ["Dimitri Staufer", "Kirsten Morehouse"], "title": "What Do LLMs Associate with Your Name? A Human-Centered Black-Box Audit of Personal Data", "comment": null, "summary": "Large language models (LLMs), and conversational agents based on them, are exposed to personal data (PD) during pre-training and during user interactions. Prior work shows that PD can resurface, yet users lack insight into how strongly models associate specific information to their identity. We audit PD across eight LLMs (3 open-source; 5 API-based, including GPT-4o), introduce LMP2 (Language Model Privacy Probe), a human-centered, privacy-preserving audit tool refined through two formative studies (N=20), and run two studies with EU residents to capture (i) intuitions about LLM-generated PD (N1=155) and (ii) reactions to tool output (N2=303). We show empirically that models confidently generate multiple PD categories for well-known individuals. For everyday users, GPT-4o generates 11 features with 60% or more accuracy (e.g., gender, hair color, languages). Finally, 72% of participants sought control over model-generated associations with their name, raising questions about what counts as PD and whether data privacy rights should extend to LLMs.", "AI": {"tldr": "\u672c\u7814\u7a76\u5ba1\u8ba1\u4e86\u516b\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u4e2a\u4eba\u6570\u636e\uff0c\u4ecb\u7ecd\u4e86\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u5de5\u5177\uff0c\u5e76\u63a2\u8ba8\u4e86\u7528\u6237\u5bf9\u6a21\u578b\u751f\u6210\u4e2a\u4eba\u6570\u636e\u7684\u76f4\u89c9\u548c\u53cd\u5e94\u3002", "motivation": "\u7531\u4e8e\u7528\u6237\u5728\u4e0e\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u8bdd\u4ee3\u7406\u4e92\u52a8\u65f6\u53ef\u80fd\u66b4\u9732\u4e2a\u4eba\u6570\u636e\uff0c\u56e0\u6b64\u4e86\u89e3\u8fd9\u4e9b\u6a21\u578b\u5982\u4f55\u5173\u8054\u7279\u5b9a\u4fe1\u606f\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5f15\u5165LMP2\u5de5\u5177\uff0c\u5bf9\u516b\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u5ba1\u8ba1\uff0c\u5e76\u8fdb\u884c\u4e24\u9879\u7528\u6237\u7814\u7a76\u4ee5\u6536\u96c6\u5bf9\u6a21\u578b\u751f\u6210\u4e2a\u4eba\u6570\u636e\u7684\u76f4\u89c9\u548c\u5bf9\u5de5\u5177\u8f93\u51fa\u7684\u53cd\u5e94\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u6a21\u578b\u80fd\u591f\u81ea\u4fe1\u5730\u751f\u6210\u591a\u4e2a\u4e2a\u4eba\u6570\u636e\u7c7b\u522b\uff0c\u4e14\u8bb8\u591a\u7528\u6237\u5bf9\u6a21\u578b\u751f\u6210\u4fe1\u606f\u611f\u5230\u62c5\u5fe7\u3002", "conclusion": "\u5927\u591a\u6570\u53c2\u4e0e\u8005\u5e0c\u671b\u63a7\u5236\u4e0e\u5176\u59d3\u540d\u5173\u8054\u7684\u6a21\u578b\u751f\u6210\u4fe1\u606f\uff0c\u5e76\u63d0\u51fa\u4e86\u6570\u636e\u9690\u79c1\u6743\u662f\u5426\u5e94\u6269\u5c55\u81f3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u95ee\u9898\u3002"}}
{"id": "2602.17259", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17259", "abs": "https://arxiv.org/abs/2602.17259", "authors": ["Han Zhao", "Jingbo Wang", "Wenxuan Song", "Shuai Chen", "Yang Liu", "Yan Wang", "Haoang Li", "Donglin Wang"], "title": "FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment", "comment": "Project Website: https://h-zhao1997.github.io/frappe", "summary": "Enabling VLA models to predict environmental dynamics, known as world modeling, has been recognized as essential for improving robotic reasoning and generalization. However, current approaches face two main issues: 1. The training objective forces models to over-emphasize pixel-level reconstruction, which constrains semantic learning and generalization 2. Reliance on predicted future observations during inference often leads to error accumulation. To address these challenges, we introduce Future Representation Alignment via Parallel Progressive Expansion (FRAPPE). Our method adopts a two-stage fine-tuning strategy: In the mid-training phase, the model learns to predict the latent representations of future observations; In the post-training phase, we expand the computational workload in parallel and align the representation simultaneously with multiple different visual foundation models. By significantly improving fine-tuning efficiency and reducing dependence on action-annotated data, FRAPPE provides a scalable and data-efficient pathway to enhance world-awareness in generalist robotic policies. Experiments on the RoboTwin benchmark and real-world tasks demonstrate that FRAPPE outperforms state-of-the-art approaches and shows strong generalization in long-horizon and unseen scenarios.", "AI": {"tldr": "\u63d0\u51faFRAPPE\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5fae\u8c03\u7b56\u7565\u63d0\u5347\u673a\u5668\u4eba\u7684\u73af\u5883\u52a8\u6001\u9884\u6d4b\u80fd\u529b\uff0c\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u7f3a\u9677\u3002", "motivation": "\u6539\u5584\u673a\u5668\u4eba\u7684\u63a8\u7406\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u7cbe\u786e\u9884\u6d4b\u73af\u5883\u52a8\u6001(\u5373\u4e16\u754c\u5efa\u6a21)\u3002", "method": "\u901a\u8fc7\u4e24\u9636\u6bb5\u5fae\u8c03\u7b56\u7565\u8fdb\u884c\u8bad\u7ec3\uff0c\u9996\u5148\u9884\u6d4b\u672a\u6765\u89c2\u5bdf\u7684\u6f5c\u5728\u8868\u793a\uff0c\u7136\u540e\u6269\u5c55\u8ba1\u7b97\u5de5\u4f5c\u91cf\u5e76\u540c\u65f6\u4e0e\u591a\u4e2a\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5bf9\u9f50\u8868\u793a\u3002", "result": "FRAPPE\u663e\u8457\u63d0\u9ad8\u5fae\u8c03\u6548\u7387\uff0c\u5e76\u51cf\u5c11\u5bf9\u884c\u52a8\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5728RoboTwin\u57fa\u51c6\u548c\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "FRAPPE\u6a21\u578b\u5728\u957f\u65f6\u95f4\u548c\u672a\u89c1\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2602.17649", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.17649", "abs": "https://arxiv.org/abs/2602.17649", "authors": ["Aya Abdelnaem El-Basha", "Ebtsam ELSayed Mahmoud ELSayes", "Ahmad Al-Kabbany"], "title": "The Effectiveness of a Virtual Reality-Based Training Program for Improving Body Awareness in Children with Attention Deficit and Hyperactivity Disorder", "comment": null, "summary": "This study investigates the effectiveness of a Virtual Reality (VR)-based training program in improving body awareness among children with Attention Deficit Hyperactivity Disorder (ADHD). Utilizing a quasi-experimental design, the research sample consisted of 10 children aged 4 to 7 years, with IQ scores ranging from 90 to 110. Participants were divided into an experimental group and a control group, with the experimental group receiving a structured VR intervention over three months, totaling 36 sessions. Assessment tools included the Stanford-Binet Intelligence Scale (5th Edition), the Conners Test for ADHD, and a researcher-prepared Body Awareness Scale.\n  The results indicated statistically significant differences between pre-test and post-test scores for the experimental group, demonstrating the program's efficacy in enhancing spatial awareness, body part identification, and motor expressions. Furthermore, follow-up assessments conducted one month after the intervention revealed no significant differences from the post-test results, confirming the sustainability and continuity of the program's effects over time. The findings suggest that immersive VR environments provide a safe, engaging, and effective therapeutic medium for addressing psychomotor deficits in early childhood ADHD.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u63a2\u8ba8\u4e86\u57fa\u4e8e\u865a\u62df\u73b0\u5b9e\u7684\u8bad\u7ec3\u7a0b\u5e8f\u5728\u63d0\u9ad8\u6ce8\u610f\u7f3a\u9677\u591a\u52a8\u969c\u788d\u513f\u7ae5\u8eab\u4f53\u610f\u8bc6\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u8be5\u7a0b\u5e8f\u6709\u6548\uff0c\u4e14\u6548\u679c\u53ef\u6301\u7eed\u3002", "motivation": "\u63a2\u7d22\u865a\u62df\u73b0\u5b9e\u6280\u672f\u5728\u5e2e\u52a9ADHD\u513f\u7ae5\u6539\u5584\u8eab\u4f53\u610f\u8bc6\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u51c6\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u7814\u7a76\u6837\u672c\u4e3a10\u540d4\u52307\u5c81ADHD\u513f\u7ae5\uff0c\u5206\u4e3a\u5b9e\u9a8c\u7ec4\u548c\u5bf9\u7167\u7ec4\uff0c\u8fdb\u884c3\u4e2a\u6708\u7684VR\u5e72\u9884\uff0c\u517136\u6b21\u8bfe\u7a0b\u3002", "result": "VR\u57f9\u8bad\u9879\u76ee\u63d0\u9ad8\u4e86\u6ce8\u610f\u7f3a\u9677\u591a\u52a8\u969c\u788d\uff08ADHD\uff09\u513f\u7ae5\u7684\u8eab\u4f53\u610f\u8bc6\u6548\u679c\u663e\u8457\u3002", "conclusion": "\u6c89\u6d78\u5f0f\u865a\u62df\u73b0\u5b9e\u73af\u5883\u4e3a\u65e9\u671f\u513f\u7ae5ADHD\u7684\u5fc3\u7406\u8fd0\u52a8\u7f3a\u9677\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b89\u5168\u3001\u5438\u5f15\u4eba\u548c\u6709\u6548\u7684\u6cbb\u7597\u5a92\u4ecb\u3002"}}
{"id": "2602.17393", "categories": ["cs.RO", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.17393", "abs": "https://arxiv.org/abs/2602.17393", "authors": ["Minxing Sun", "Yao Mao"], "title": "Contact-Anchored Proprioceptive Odometry for Quadruped Robots", "comment": "28 pages, 30 figures", "summary": "Reliable odometry for legged robots without cameras or LiDAR remains challenging due to IMU drift and noisy joint velocity sensing. This paper presents a purely proprioceptive state estimator that uses only IMU and motor measurements to jointly estimate body pose and velocity, with a unified formulation applicable to biped, quadruped, and wheel-legged robots. The key idea is to treat each contacting leg as a kinematic anchor: joint-torque--based foot wrench estimation selects reliable contacts, and the corresponding footfall positions provide intermittent world-frame constraints that suppress long-term drift. To prevent elevation drift during extended traversal, we introduce a lightweight height clustering and time-decay correction that snaps newly recorded footfall heights to previously observed support planes. To improve foot velocity observations under encoder quantization, we apply an inverse-kinematics cubature Kalman filter that directly filters foot-end velocities from joint angles and velocities. The implementation further mitigates yaw drift through multi-contact geometric consistency and degrades gracefully to a kinematics-derived heading reference when IMU yaw constraints are unavailable or unreliable. We evaluate the method on four quadruped platforms (three Astrall robots and a Unitree Go2 EDU) using closed-loop trajectories. On Astrall point-foot robot~A, a $\\sim$200\\,m horizontal loop and a $\\sim$15\\,m vertical loop return with 0.1638\\,m and 0.219\\,m error, respectively; on wheel-legged robot~B, the corresponding errors are 0.2264\\,m and 0.199\\,m. On wheel-legged robot~C, a $\\sim$700\\,m horizontal loop yields 7.68\\,m error and a $\\sim$20\\,m vertical loop yields 0.540\\,m error. Unitree Go2 EDU closes a $\\sim$120\\,m horizontal loop with 2.2138\\,m error and a $\\sim$8\\,m vertical loop with less than 0.1\\,m vertical error. github.com/ShineMinxing/Ros2Go2Estimator.git", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eIMU\u548c\u7535\u673a\u6d4b\u91cf\u7684\u817f\u90e8\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86IMU\u6f02\u79fb\u548c\u4f20\u611f\u5668\u566a\u58f0\u5e26\u6765\u7684\u5f71\u54cd\uff0c\u9a8c\u8bc1\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u7531\u4e8e\u5f53\u524d\u817f\u90e8\u673a\u5668\u4eba\u5728\u6ca1\u6709\u6444\u50cf\u5934\u6216LiDAR\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u53ef\u9760\u7684\u91cc\u7a0b\u8ba1\u975e\u5e38\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3IMU\u6f02\u79fb\u548c\u5173\u8282\u901f\u5ea6\u4f20\u611f\u5668\u566a\u58f0\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7eaf\u7cb9\u7684\u81ea\u6709\u611f\u77e5\u72b6\u6001\u4f30\u8ba1\u5668\uff0c\u4ec5\u5229\u7528IMU\u548c\u7535\u673a\u6d4b\u91cf\u6765\u8054\u5408\u4f30\u8ba1\u673a\u5668\u4eba\u8eab\u4f53\u59ff\u6001\u548c\u901f\u5ea6\u3002", "result": "\u5728\u56db\u4e2a\u56db\u8db3\u5e73\u53f0\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5177\u4f53\u8bef\u5dee\u5206\u522b\u4e3a\uff1aAstrall robot A\u7684\u6c34\u5e73\u56de\u8def\u8bef\u5dee\u4e3a0.1638m\uff0c\u5782\u76f4\u56de\u8def\u8bef\u5dee\u4e3a0.219m\uff1b\u8f6e-legged\u673a\u5668\u4ebaB\u7684\u6c34\u5e73\u548c\u5782\u76f4\u56de\u8def\u8bef\u5dee\u5206\u522b\u4e3a0.2264m\u548c0.199m\uff1b\u673a\u5668\u4ebaC\u7684\u6c34\u5e73\u56de\u8def\u8bef\u5dee\u4e3a7.68m\uff0c\u5782\u76f4\u56de\u8def\u8bef\u5dee\u4e3a0.540m\uff1bUnitree Go2 EDU\u7684\u6c34\u5e73\u56de\u8def\u8bef\u5dee\u4e3a2.2138m\uff0c\u5782\u76f4\u56de\u8def\u8bef\u5dee\u4f4e\u4e8e0.1m\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8054\u5408\u5229\u7528IMU\u548c\u7535\u673a\u6570\u636e\u6765\u6709\u6548\u51cf\u5c11\u957f\u65f6\u95f4\u6f02\u79fb\uff0c\u63d0\u9ad8\u4e86\u817f\u90e8\u673a\u5668\u4eba\u7684\u91cc\u7a0b\u8ba1\u7cbe\u5ea6\u3002"}}
{"id": "2602.17415", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17415", "abs": "https://arxiv.org/abs/2602.17415", "authors": ["Yi Zhang", "Omar Faris", "Chapa Sirithunge", "Kai-Fung Chu", "Fumiya Iida", "Fulvio Forni"], "title": "Distributed Virtual Model Control for Scalable Human-Robot Collaboration in Shared Workspace", "comment": null, "summary": "We present a decentralized, agent agnostic, and safety-aware control framework for human-robot collaboration based on Virtual Model Control (VMC). In our approach, both humans and robots are embedded in the same virtual-component-shaped workspace, where motion is the result of the interaction with virtual springs and dampers rather than explicit trajectory planning. A decentralized, force-based stall detector identifies deadlocks, which are resolved through negotiation. This reduces the probability of robots getting stuck in the block placement task from up to 61.2% to zero in our experiments. The framework scales without structural changes thanks to the distributed implementation: in experiments we demonstrate safe collaboration with up to two robots and two humans, and in simulation up to four robots, maintaining inter-agent separation at around 20 cm. Results show that the method shapes robot behavior intuitively by adjusting control parameters and achieves deadlock-free operation across team sizes in all tested scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u865a\u62df\u6a21\u578b\u63a7\u5236\u7684\u53bb\u4e2d\u5fc3\u5316\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u5b89\u5168\u3001\u9ad8\u6548\u548c\u65e0\u6b7b\u9501\u64cd\u4f5c\u3002", "motivation": "\u65e8\u5728\u6539\u5584\u4eba\u673a\u5408\u4f5c\u4e2d\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\uff0c\u5c24\u5176\u5728\u673a\u5668\u4eba\u53ef\u80fd\u53d1\u751f\u963b\u585e\u7684\u573a\u666f\u4e2d\u3002", "method": "\u57fa\u4e8e\u865a\u62df\u6a21\u578b\u63a7\u5236(VMC)\u7684\u53bb\u4e2d\u5fc3\u5316\u3001\u667a\u80fd\u4f53\u65e0\u5173\u4e14\u5b89\u5168\u610f\u8bc6\u7684\u63a7\u5236\u6846\u67b6", "result": "\u5728\u5b9e\u9a8c\u4e2d\uff0c\u673a\u5668\u4eba\u5728\u4efb\u52a1\u4e2d\u963b\u585e\u7684\u6982\u7387\u4ece61.2%\u964d\u4f4e\u5230\u96f6\uff0c\u5e76\u4e14\u6846\u67b6\u80fd\u591f\u6269\u5c55\uff0c\u5b89\u5168\u5730\u4e0e\u591a\u8fbe\u4e24\u4e2a\u673a\u5668\u4eba\u548c\u4e24\u4e2a\u4eba\u7c7b\u534f\u4f5c\uff0c\u6a21\u62df\u4e2d\u652f\u6301\u6700\u591a\u56db\u4e2a\u673a\u5668\u4eba\uff0c\u5e76\u4fdd\u6301\u7ea620\u5398\u7c73\u7684\u4ee3\u7406\u95f4\u9694\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8c03\u6574\u63a7\u5236\u53c2\u6570\u76f4\u89c2\u5730\u5851\u9020\u673a\u5668\u4eba\u884c\u4e3a\uff0c\u5e76\u5728\u6240\u6709\u6d4b\u8bd5\u573a\u666f\u4e2d\u5b9e\u73b0\u65e0\u6b7b\u9501\u64cd\u4f5c\u3002"}}
{"id": "2602.17421", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17421", "abs": "https://arxiv.org/abs/2602.17421", "authors": ["Diana Cafiso", "Petr Trunin", "Carolina Gay", "Lucia Beccai"], "title": "3D-printed Soft Optical sensor with a Lens (SOLen) for light guidance in mechanosensing", "comment": "11 pages, 5 figures, submitted to Materials & Design", "summary": "Additive manufacturing is enabling soft robots with increasingly complex geometries, creating a demand for sensing solutions that remain compatible with single-material, one-step fabrication. Optical soft sensors are attractive for monolithic printing, but their performance is often degraded by uncontrolled light propagation (ambient coupling, leakage, scattering), while common miti- gation strategies typically require multimaterial interfaces. Here, we present an approach for 3D printed soft optical sensing (SOLen), in which a printed lens is placed in front of an emitter within a Y-shaped waveguide. The sensing mechanism relies on deformation-induced lens rotation and focal-spot translation, redistributing optical power between the two branches to generate a differential output that encodes both motion direction and amplitude. An acrylate polyurethane resin was modified with lauryl acrylate to improve compliance and optical transmittance, and single-layer optical characterization was used to derive wavelength-dependent refractive index and transmittance while minimizing DLP layer-related artifacts. The measured refractive index was used in simulations to design a lens profile for a target focal distance, which was then printed with sub-millimeter fidelity. Rotational tests demonstrated reproducible branch-selective signal switching over multiple cycles. These results establish a transferable material-to-optics workflow for soft optical sensors with lens with new functionalities for next-generation soft robots", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8f6f\u5149\u4f20\u611f\u5668\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc73D\u6253\u5370\u7684\u955c\u5934\u548cY\u578b\u6ce2\u5bfc\u5b9e\u73b0\u5149\u5b66\u4f20\u611f\uff0c\u514b\u670d\u5e38\u89c1\u7684\u5149\u4f20\u64ad\u95ee\u9898\uff0c\u63a8\u52a8\u8f6f\u673a\u5668\u4eba\u7684\u53d1\u5c55\u3002", "motivation": "\u968f\u7740\u8f6f\u673a\u5668\u4eba\u7684\u590d\u6742\u51e0\u4f55\u7ed3\u6784\u4e0d\u65ad\u589e\u52a0\uff0c\u5bf9\u517c\u5bb9\u5355\u4e00\u6750\u6599\u7684\u4f20\u611f\u89e3\u51b3\u65b9\u6848\u7684\u9700\u6c42\u4e5f\u968f\u4e4b\u589e\u52a0\uff0c\u800c\u5e38\u89c1\u7684\u7f13\u89e3\u7b56\u7565\u5f80\u5f80\u9700\u8981\u591a\u79cd\u6750\u6599\u754c\u9762\u3002", "method": "\u901a\u8fc7\u5728Y\u578b\u6ce2\u5bfc\u4e2d\u5c06\u6253\u5370\u955c\u5934\u653e\u7f6e\u5728\u53d1\u5c04\u5668\u524d\uff0c\u5229\u7528\u53d8\u5f62\u5f15\u8d77\u7684\u955c\u5934\u65cb\u8f6c\u548c\u7126\u70b9\u5e73\u79fb\u8fdb\u884c\u5149\u5b66\u4f20\u611f\u3002", "result": "\u6539\u6027\u805a\u6c28\u916f\u6811\u8102\u63d0\u9ad8\u4e86\u4f20\u611f\u5668\u7684\u67d4\u6027\u548c\u5149\u900f\u8fc7\u7387\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u4f20\u611f\u5668\u5728\u591a\u6b21\u5faa\u73af\u4e2d\u5c55\u73b0\u4e86\u53ef\u91cd\u590d\u7684\u5206\u652f\u9009\u62e9\u4fe1\u53f7\u5207\u6362\u3002", "conclusion": "\u672c\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u79cd\u53ef\u8f6c\u79fb\u7684\u6750\u6599\u5230\u5149\u5b66\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4e3a\u8f6f\u5149\u4f20\u611f\u5668\u63d0\u4f9b\u4e86\u5177\u6709\u65b0\u529f\u80fd\u7684\u955c\u5934\uff0c\u52a9\u529b\u4e0b\u4e00\u4ee3\u8f6f\u673a\u5668\u4eba\u3002"}}
{"id": "2602.17472", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17472", "abs": "https://arxiv.org/abs/2602.17472", "authors": ["Mohamed Sabry", "Joseba Gorospe", "Cristina Olaverri-Monreal"], "title": "A Cost-Effective and Climate-Resilient Air Pressure System for Rain Effect Reduction on Automated Vehicle Cameras", "comment": null, "summary": "Recent advances in automated vehicles have focused on improving perception performance under adverse weather conditions; however, research on physical hardware solutions remains limited, despite their importance for perception critical applications such as vehicle platooning. Existing approaches, such as hydrophilic or hydrophobic lenses and sprays, provide only partial mitigation, while industrial protection systems imply high cost and they do not enable scalability for automotive deployment.\n  To address these limitations, this paper presents a cost-effective hardware solution for rainy conditions, designed to be compatible with multiple cameras simultaneously.\n  Beyond its technical contribution, the proposed solution supports sustainability goals in transportation systems. By enabling compatibility with existing camera-based sensing platforms, the system extends the operational reliability of automated vehicles without requiring additional high-cost sensors or hardware replacements. This approach reduces resource consumption, supports modular upgrades, and promotes more cost-efficient deployment of automated vehicle technologies, particularly in challenging weather conditions where system failures would otherwise lead to inefficiencies and increased emissions. The proposed system was able to increase pedestrian detection accuracy of a Deep Learning model from 8.3% to 41.6%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u7684\u786c\u4ef6\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u5728\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u611f\u77e5\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u7684\u611f\u77e5\u6027\u80fd\u6539\u5584\u8feb\u5207\u9700\u8981\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u6709\u9650\u4e14\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u6210\u672c\u6548\u76ca\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u517c\u5bb9\u591a\u53f0\u76f8\u673a\u7684\u786c\u4ef6\u65b9\u6848\uff0c\u4e13\u95e8\u9488\u5bf9\u96e8\u5929\u6761\u4ef6\u8fdb\u884c\u8bbe\u8ba1\u3002", "result": "\u8be5\u7cfb\u7edf\u80fd\u5c06\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u884c\u4eba\u68c0\u6d4b\u51c6\u786e\u7387\u4ece8.3%\u63d0\u9ad8\u523041.6%\u3002", "conclusion": "\u8be5\u65b9\u6848\u5728\u63d0\u5347\u611f\u77e5\u6027\u80fd\u7684\u540c\u65f6\uff0c\u652f\u6301\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807\uff0c\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\u5e76\u6709\u6548\u90e8\u7f72\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u3002"}}
{"id": "2602.17474", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17474", "abs": "https://arxiv.org/abs/2602.17474", "authors": ["Carolina Gay", "Petr Trunin", "Diana Cafiso", "Yuejun Xu", "Majid Taghavi", "Lucia Beccai"], "title": "Optically Sensorized Electro-Ribbon Actuator (OS-ERA)", "comment": "6 pages, 5 figures, accepted for 9th IEEE-RAS International Conference on Soft Robotics (RoboSoft 2026)", "summary": "Electro-Ribbon Actuators (ERAs) are lightweight flexural actuators that exhibit ultrahigh displacement and fast movement. However, their embedded sensing relies on capacitive sensors with limited precision, which hinders accurate control. We introduce OS-ERA, an optically sensorized ERA that yields reliable proprioceptive information, and we focus on the design and integration of a sensing solution without affecting actuation. To analyse the complex curvature of an ERA in motion, we design and embed two soft optical waveguide sensors. A classifier is trained to map the sensing signals in order to distinguish eight bending states. We validate our model on six held-out trials and compare it against signals' trajectories learned from training runs. Across all tests, the sensing output signals follow the training manifold, and the predicted sequence mirrors real performance and confirms repeatability. Despite deliberate train-test mismatches in actuation speed, the signal trajectories preserve their shape, and classification remains consistently accurate, demonstrating practical voltage- and speed-invariance. As a result, OS-ERA classifies bending states with high fidelity; it is fast and repeatable, solving a longstanding bottleneck of the ERA, enabling steps toward closed-loop control.", "AI": {"tldr": "OS-ERA\u662f\u4e00\u79cd\u65b0\u578b\u5149\u4f20\u611f\u7535\u52a8\u4e1d\u5e26\u6267\u884c\u5668\uff0c\u80fd\u591f\u9ad8\u7cbe\u5ea6\u5730\u5206\u7c7b\u5176\u5f2f\u66f2\u72b6\u6001\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63a7\u5236\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u63d0\u9ad8\u7535\u52a8\u4e1d\u5e26\u6267\u884c\u5668\u7684\u611f\u77e5\u7cbe\u5ea6\uff0c\u4ee5\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u63a7\u5236\u3002", "method": "\u8bbe\u8ba1\u5e76\u96c6\u6210\u4e86\u5149\u5b66\u4f20\u611f\u5668\u7684\u7535\u52a8\u4e1d\u5e26\u6267\u884c\u5668 (OS-ERA)\uff0c\u7528\u4e8e\u51c6\u786e\u611f\u77e5\u548c\u5206\u7c7b\u5176\u5f2f\u66f2\u72b6\u6001\u3002", "result": "\u901a\u8fc7\u5d4c\u5165\u4e24\u4e2a\u67d4\u6027\u5149\u6ce2\u5bfc\u4f20\u611f\u5668\u548c\u8bad\u7ec3\u5206\u7c7b\u5668\uff0c\u6210\u529f\u533a\u5206\u4e86\u516b\u79cd\u5f2f\u66f2\u72b6\u6001\uff0c\u5e76\u5728\u591a\u6b21\u8bd5\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002", "conclusion": "OS-ERA\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u5206\u7c7b\u7cbe\u5ea6\u548c\u91cd\u590d\u6027\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u7535\u52a8\u4e1d\u5e26\u6267\u884c\u5668\u7684\u611f\u5e94\u7cbe\u5ea6\u95ee\u9898\uff0c\u4e3a\u95ed\u73af\u63a7\u5236\u7684\u5b9e\u73b0\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2602.17502", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17502", "abs": "https://arxiv.org/abs/2602.17502", "authors": ["Kyle R. Embry", "Lorenzo Vianello", "Jim Lipsey", "Frank Ursetta", "Michael Stephens", "Zhi Wang", "Ann M. Simon", "Andrea J. Ikeda", "Suzanne B. Finucane", "Shawana Anarwala", "Levi J. Hargrove"], "title": "Proximal powered knee placement: a case study", "comment": "Submitted to IEEE RAS/EMBS 11th International Conference on Biomedical Robotics and Biomechatronics (BioRob 2026)", "summary": "Lower limb amputation affects millions worldwide, leading to impaired mobility, reduced walking speed, and limited participation in daily and social activities. Powered prosthetic knees can partially restore mobility by actively assisting knee joint torque, improving gait symmetry, sit-to-stand transitions, and walking speed. However, added mass from powered components may diminish these benefits, negatively affecting gait mechanics and increasing metabolic cost. Consequently, optimizing mass distribution, rather than simply minimizing total mass, may provide a more effective and practical solution. In this exploratory study, we evaluated the feasibility of above-knee powertrain placement for a powered prosthetic knee in a small cohort. Compared to below-knee placement, the above-knee configuration demonstrated improved walking speed (+9.2% for one participant) and cadence (+3.6%), with mixed effects on gait symmetry. Kinematic measures indicated similar knee range of motion and peak velocity across configurations. Additional testing on ramps and stairs confirmed the robustness of the control strategy across multiple locomotion tasks. These preliminary findings suggest that above-knee placement is functionally feasible and that careful mass distribution can preserve the benefits of powered assistance while mitigating adverse effects of added weight. Further studies are needed to confirm these trends and guide design and clinical recommendations.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e0a\u819d\u5047\u80a2\u52a8\u529b\u7cfb\u7edf\u7684\u53ef\u884c\u6027\uff0c\u53d1\u73b0\u5176\u5728\u67d0\u4e9b\u6b65\u6001\u53c2\u6570\u4e0a\u4f18\u4e8e\u4e0b\u819d\u914d\u7f6e\uff0c\u63d0\u793a\u4f18\u5316\u8d28\u91cf\u5206\u5e03\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5047\u80a2\u4f7f\u7528\u8005\u5728\u8fd0\u52a8\u80fd\u529b\u6062\u590d\u4e2d\u7684\u6311\u6218\uff0c\u63a2\u7d22\u66f4\u6709\u6548\u7684\u52a8\u529b\u5047\u80a2\u8bbe\u8ba1\u3002", "method": "\u8bc4\u4f30\u4e0a\u819d\u52a8\u529b\u7cfb\u7edf\u5728\u5047\u80a2\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u6bd4\u8f83\u4e0d\u540c\u4f4d\u7f6e\u914d\u7f6e\u7684\u6548\u679c", "result": "\u4e0a\u819d\u914d\u7f6e\u5728\u6b65\u884c\u901f\u5ea6\u548c\u6b65\u4f10\u9891\u7387\u4e0a\u4f18\u4e8e\u4e0b\u819d\u914d\u7f6e\uff0c\u4f46\u5bf9\u6b65\u6001\u5bf9\u79f0\u6027\u7684\u5f71\u54cd\u6548\u679c\u4e0d\u4e00\u3002", "conclusion": "\u4e0a\u819d\u914d\u7f6e\u529f\u80fd\u4e0a\u53ef\u884c\uff0c\u5408\u7406\u7684\u8d28\u91cf\u5206\u5e03\u80fd\u591f\u4fdd\u7559\u52a8\u529b\u8f85\u52a9\u7684\u4f18\u70b9\uff0c\u51cf\u5c11\u9644\u52a0\u91cd\u91cf\u7684\u8d1f\u9762\u5f71\u54cd\u3002"}}
{"id": "2602.17515", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17515", "abs": "https://arxiv.org/abs/2602.17515", "authors": ["Ziyi Zong", "Xin Dong", "Jinwu Xiang", "Daochun Li", "Zhan Tu"], "title": "RA-Nav: A Risk-Aware Navigation System Based on Semantic Segmentation for Aerial Robots in Unpredictable Environments", "comment": null, "summary": "Existing aerial robot navigation systems typically plan paths around static and dynamic obstacles, but fail to adapt when a static obstacle suddenly moves. Integrating environmental semantic awareness enables estimation of potential risks posed by suddenly moving obstacles. In this paper, we propose RA- Nav, a risk-aware navigation framework based on semantic segmentation. A lightweight multi-scale semantic segmentation network identifies obstacle categories in real time. These obstacles are further classified into three types: stationary, temporarily static, and dynamic. For each type, corresponding risk estimation functions are designed to enable real-time risk prediction, based on which a complete local risk map is constructed. Based on this map, the risk-informed path search algorithm is designed to guarantee planning that balances path efficiency and safety. Trajectory optimization is then applied to generate trajectories that are safe, smooth, and dynamically feasible. Comparative simulations demonstrate that RA-Nav achieves higher success rates than baselines in sudden obstacle state transition scenarios. Its effectiveness is further validated in simulations using real- world data.", "AI": {"tldr": "RA-Nav\u662f\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u5206\u5272\u7684\u98ce\u9669\u611f\u77e5\u5bfc\u822a\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u5b9e\u65f6\u7684\u8bed\u4e49\u8bc6\u522b\u548c\u98ce\u9669\u4f30\u8ba1\uff0c\u4f18\u5316\u65e0\u4eba\u673a\u5728\u52a8\u6001\u969c\u788d\u7269\u73af\u5883\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u3002", "motivation": "\u73b0\u6709\u7684\u65e0\u4eba\u673a\u5bfc\u822a\u7cfb\u7edf\u65e0\u6cd5\u5e94\u5bf9\u9759\u6001\u969c\u788d\u7269\u7a81\u7136\u79fb\u52a8\u7684\u60c5\u51b5\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u5bfc\u822a\u7cfb\u7edf\u7684\u73af\u5883\u611f\u77e5\u80fd\u529b\u3002", "method": "RA-Nav\u91c7\u7528\u8f7b\u91cf\u7ea7\u7684\u591a\u5c3a\u5ea6\u8bed\u4e49\u5206\u5272\u7f51\u7edc\u5b9e\u65f6\u8bc6\u522b\u969c\u788d\u7269\u7c7b\u578b\uff0c\u4e3a\u6bcf\u79cd\u7c7b\u578b\u8bbe\u8ba1\u76f8\u5e94\u7684\u98ce\u9669\u4f30\u8ba1\u51fd\u6570\uff0c\u5e76\u6784\u5efa\u5c40\u90e8\u98ce\u9669\u5730\u56fe\u3002", "result": "\u901a\u8fc7\u6bd4\u8f83\u6a21\u62df\uff0cRA-Nav\u5728\u7a81\u53d1\u969c\u788d\u72b6\u6001\u8f6c\u53d8\u573a\u666f\u4e2d\u7684\u6210\u529f\u7387\u9ad8\u4e8e\u57fa\u7ebf\uff0c\u5e76\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7684\u6a21\u62df\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "RA-Nav\u6846\u67b6\u5728\u7a81\u53d1\u969c\u788d\u72b6\u6001\u8f6c\u53d8\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6210\u529f\u7387\uff0c\u5e76\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7684\u6a21\u62df\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2602.17537", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17537", "abs": "https://arxiv.org/abs/2602.17537", "authors": ["Qilong Cheng", "Matthew Mackay", "Ali Bereyhi"], "title": "IRIS: Learning-Driven Task-Specific Cinema Robot Arm for Visuomotor Motion Control", "comment": null, "summary": "Robotic camera systems enable dynamic, repeatable motion beyond human capabilities, yet their adoption remains limited by the high cost and operational complexity of industrial-grade platforms. We present the Intelligent Robotic Imaging System (IRIS), a task-specific 6-DOF manipulator designed for autonomous, learning-driven cinematic motion control. IRIS integrates a lightweight, fully 3D-printed hardware design with a goal-conditioned visuomotor imitation learning framework based on Action Chunking with Transformers (ACT). The system learns object-aware and perceptually smooth camera trajectories directly from human demonstrations, eliminating the need for explicit geometric programming. The complete platform costs under $1,000 USD, supports a 1.5 kg payload, and achieves approximately 1 mm repeatability. Real-world experiments demonstrate accurate trajectory tracking, reliable autonomous execution, and generalization across diverse cinematic motions.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aIRIS\u7684\u4f4e\u6210\u672c\u667a\u80fd\u673a\u5668\u4eba\u6210\u50cf\u7cfb\u7edf\uff0c\u5b83\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u5b9e\u73b0\u9ad8\u6548\u7684\u6444\u50cf\u8fd0\u52a8\u63a7\u5236\uff0c\u5c55\u73b0\u4e86\u826f\u597d\u7684\u6027\u80fd\u548c\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u5de5\u4e1a\u7ea7\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u9ad8\u6210\u672c\u548c\u64cd\u4f5c\u590d\u6742\u6027\uff0c\u63a8\u52a8\u5176\u5728\u52a8\u6001\u6444\u50cf\u4e2d\u7684\u5e94\u7528\u3002", "method": "IRIS\u91c7\u7528\u65e8\u5728\u81ea\u4e3b\u3001\u5b66\u4e60\u9a71\u52a8\u7684\u8fd0\u52a8\u63a7\u5236\u76846-DOF\u673a\u68b0\u81c2\u8bbe\u8ba1\uff0c\u96c6\u6210\u4e86\u57fa\u4e8e\u884c\u52a8\u5206\u5757\u4e0e\u53d8\u6362\u5668\u7684\u89c6\u89c9\u8fd0\u52a8\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\u3002", "result": "IRIS\u7cfb\u7edf\u4ee5\u4f4e\u4e8e1000\u7f8e\u5143\u7684\u6210\u672c\uff0c\u652f\u63011.5 kg\u7684\u6709\u6548\u8f7d\u8377\uff0c\u5b9e\u73b0\u4e86\u5927\u7ea61 mm\u7684\u91cd\u590d\u6027\uff0c\u5e76\u5728\u73b0\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u7cbe\u51c6\u7684\u8f68\u8ff9\u8ddf\u8e2a\u548c\u53ef\u9760\u7684\u81ea\u4e3b\u6267\u884c\u3002", "conclusion": "IRIS\u7cfb\u7edf\u6210\u529f\u5b9e\u73b0\u4e86\u4f4e\u6210\u672c\u3001\u9ad8\u7cbe\u5ea6\u7684\u81ea\u4e3b\u6444\u50cf\u63a7\u5236\uff0c\u5e76\u5177\u5907\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.17573", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17573", "abs": "https://arxiv.org/abs/2602.17573", "authors": ["Konstantinos Foteinos", "Georgios Angelidis", "Aggelos Psiris", "Vasileios Argyriou", "Panagiotis Sarigiannidis", "Georgios Th. Papadopoulos"], "title": "FR-GESTURE: An RGBD Dataset For Gesture-based Human-Robot Interaction In First Responder Operations", "comment": null, "summary": "The ever increasing intensity and number of disasters make even more difficult the work of First Responders (FRs). Artificial intelligence and robotics solutions could facilitate their operations, compensating these difficulties. To this end, we propose a dataset for gesture-based UGV control by FRs, introducing a set of 12 commands, drawing inspiration from existing gestures used by FRs and tactical hand signals and refined after incorporating feedback from experienced FRs. Then we proceed with the data collection itself, resulting in 3312 RGBD pairs captured from 2 viewpoints and 7 distances. To the best of our knowledge, this is the first dataset especially intended for gesture-based UGV guidance by FRs. Finally we define evaluation protocols for our RGBD dataset, termed FR-GESTURE, and we perform baseline experiments, which are put forward for improvement. We have made data publicly available to promote future research on the domain: https://doi.org/10.5281/zenodo.18131333.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u6025\u6551\u4eba\u5458\u624b\u52bf\u63a7\u5236\u65e0\u4eba\u5730\u9762\u8f66\u8f86\u7684\u9996\u4e2a\u6570\u636e\u96c6FR-GESTURE\uff0c\u5305\u62ec3312\u5bf9RGBD\u6570\u636e\uff0c\u65e8\u5728\u901a\u8fc7\u4eba\u5de5\u667a\u80fd\u548c\u673a\u5668\u4eba\u6280\u672f\u63d0\u9ad8\u6025\u6551\u5de5\u4f5c\u6548\u7387\u3002", "motivation": "\u968f\u7740\u707e\u96be\u7684\u65e5\u76ca\u589e\u591a\uff0c\u6025\u6551\u4eba\u5458\u7684\u5de5\u4f5c\u53d8\u5f97\u6108\u53d1\u56f0\u96be\uff0c\u56e0\u6b64\u9700\u8981\u4eba\u5de5\u667a\u80fd\u548c\u673a\u5668\u4eba\u89e3\u51b3\u65b9\u6848\u6765\u534f\u52a9\u4ed6\u4eec\u7684\u64cd\u4f5c\u3002", "method": "\u521b\u5efaFR-GESTURE\u6570\u636e\u96c6\uff0c\u5305\u542b12\u4e2a\u624b\u52bf\u547d\u4ee4\uff0c\u57fa\u4e8e\u6025\u6551\u4eba\u5458\u4f7f\u7528\u7684\u73b0\u6709\u624b\u52bf\u548c\u6218\u672f\u624b\u52bf\u4fe1\u53f7\u8fdb\u884c\u4e86\u6539\u8fdb\uff0c\u6536\u96c6\u4e863312\u5bf9RGBD\u6570\u636e\uff0c\u5e76\u5b9a\u4e49\u8bc4\u4f30\u534f\u8bae\u3002", "result": "\u6210\u529f\u6536\u96c6\u7684\u6570\u636e\u5c06\u4fc3\u8fdb\u672a\u6765\u5728\u6025\u6551\u4eba\u5458\u624b\u52bf\u63a7\u5236\u65e0\u4eba\u5730\u9762\u8f66\u8f86\u9886\u57df\u7684\u7814\u7a76\uff0c\u5e76\u8bbe\u7acb\u4e86\u57fa\u7ebf\u5b9e\u9a8c\u4ee5\u4f9b\u540e\u7eed\u6539\u8fdb\u3002", "conclusion": "\u63d0\u51fa\u7684FR-GESTURE\u6570\u636e\u96c6\u4e3a\u6025\u6551\u4eba\u5458\u7684\u624b\u52bf\u63a7\u5236\u65e0\u4eba\u5730\u9762\u8f66\u8f86\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u7684\u5f00\u5c55\uff0c\u5e76\u4e3a\u76f8\u5173\u9886\u57df\u7684\u8fdb\u6b65\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2602.17574", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17574", "abs": "https://arxiv.org/abs/2602.17574", "authors": ["Joshua A. Robbins", "Andrew F. Thompson", "Jonah J. Glunt", "Herschel C. Pangborn"], "title": "Hybrid System Planning using a Mixed-Integer ADMM Heuristic and Hybrid Zonotopes", "comment": null, "summary": "Embedded optimization-based planning for hybrid systems is challenging due to the use of mixed-integer programming, which is computationally intensive and often sensitive to the specific numerical formulation. To address that challenge, this article proposes a framework for motion planning of hybrid systems that pairs hybrid zonotopes - an advanced set representation - with a new alternating direction method of multipliers (ADMM) mixed-integer programming heuristic. A general treatment of piecewise affine (PWA) system reachability analysis using hybrid zonotopes is presented and extended to formulate optimal planning problems. Sets produced using the proposed identities have lower memory complexity and tighter convex relaxations than equivalent sets produced from preexisting techniques. The proposed ADMM heuristic makes efficient use of the hybrid zonotope structure. For planning problems formulated as hybrid zonotopes, the proposed heuristic achieves improved convergence rates as compared to state-of-the-art mixed-integer programming heuristics. The proposed methods for hybrid system planning on embedded hardware are experimentally applied in a combined behavior and motion planning scenario for autonomous driving.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u5c06\u6df7\u5408\u9525\u4f53\u4e0e\u65b0\u578bADMM\u6df7\u5408\u6574\u6570\u89c4\u5212\u542f\u53d1\u5f0f\u76f8\u7ed3\u5408\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u6df7\u5408\u7cfb\u7edf\u7684\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u8f83\u4f4e\u7684\u5185\u5b58\u590d\u6742\u6027\u548c\u66f4\u597d\u7684\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u7531\u4e8e\u6df7\u5408\u6574\u6570\u89c4\u5212\u7684\u8ba1\u7b97\u590d\u6742\u6027\u548c\u5bf9\u6570\u503c\u5904\u7406\u654f\u611f\u6027\uff0c\u9488\u5bf9\u6df7\u5408\u7cfb\u7edf\u7684\u8fd0\u52a8\u89c4\u5212\u53d7\u5230\u9650\u5236\uff0c\u56e0\u6b64\u63d0\u51fa\u65b0\u7684\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df7\u5408\u9525\u4f53\u548cADMM\u6df7\u5408\u6574\u6570\u89c4\u5212\u542f\u53d1\u5f0f\u7684\u65b9\u6cd5\u6846\u67b6\uff0c\u5e76\u8fdb\u884c\u4e86\u4e00\u822c\u7684\u9010\u6bb5\u4eff\u5c04\u7cfb\u7edf\u53ef\u8fbe\u6027\u5206\u6790\u3002", "result": "\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u6bd4\uff0c\u4f7f\u7528\u8be5\u65b9\u6cd5\u4ea7\u751f\u7684\u96c6\u5408\u5728\u5185\u5b58\u590d\u6742\u6027\u548c\u51f8\u677e\u5f1b\u6027\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u540c\u65f6\u5728\u6df7\u5408\u7cfb\u7edf\u89c4\u5212\u65f6\u6536\u655b\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5d4c\u5165\u5f0f\u786c\u4ef6\u4e0a\u7684\u6df7\u5408\u7cfb\u7edf\u89c4\u5212\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u7279\u522b\u662f\u5728\u81ea\u4e3b\u9a7e\u9a76\u7684\u884c\u4e3a\u548c\u8fd0\u52a8\u89c4\u5212\u573a\u666f\u4e2d\u3002"}}
{"id": "2602.17586", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17586", "abs": "https://arxiv.org/abs/2602.17586", "authors": ["Antonio Guillen-Perez"], "title": "Conditional Flow Matching for Continuous Anomaly Detection in Autonomous Driving on a Manifold-Aware Spectral Space", "comment": null, "summary": "Safety validation for Level 4 autonomous vehicles (AVs) is currently bottlenecked by the inability to scale the detection of rare, high-risk long-tail scenarios using traditional rule-based heuristics. We present Deep-Flow, an unsupervised framework for safety-critical anomaly detection that utilizes Optimal Transport Conditional Flow Matching (OT-CFM) to characterize the continuous probability density of expert human driving behavior. Unlike standard generative approaches that operate in unstable, high-dimensional coordinate spaces, Deep-Flow constrains the generative process to a low-rank spectral manifold via a Principal Component Analysis (PCA) bottleneck. This ensures kinematic smoothness by design and enables the computation of the exact Jacobian trace for numerically stable, deterministic log-likelihood estimation. To resolve multi-modal ambiguity at complex junctions, we utilize an Early Fusion Transformer encoder with lane-aware goal conditioning, featuring a direct skip-connection to the flow head to maintain intent-integrity throughout the network. We introduce a kinematic complexity weighting scheme that prioritizes high-energy maneuvers (quantified via path tortuosity and jerk) during the simulation-free training process. Evaluated on the Waymo Open Motion Dataset (WOMD), our framework achieves an AUC-ROC of 0.766 against a heuristic golden set of safety-critical events. More significantly, our analysis reveals a fundamental distinction between kinematic danger and semantic non-compliance. Deep-Flow identifies a critical predictability gap by surfacing out-of-distribution behaviors, such as lane-boundary violations and non-normative junction maneuvers, that traditional safety filters overlook. This work provides a mathematically rigorous foundation for defining statistical safety gates, enabling objective, data-driven validation for the safe deployment of autonomous fleets.", "AI": {"tldr": "Deep-Flow \u662f\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u4f20\u8f93\u6761\u4ef6\u6d41\u5339\u914d\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u9a8c\u8bc1\uff0c\u6210\u529f\u68c0\u6d4b\u51fa\u4f20\u7edf\u65b9\u6cd5\u9057\u6f0f\u7684\u5371\u9669\u884c\u4e3a\u3002", "motivation": "Level 4 \u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u7684\u5b89\u5168\u9a8c\u8bc1\u56e0\u65e0\u6cd5\u6269\u5c55\u7a00\u6709\u3001\u9ad8\u98ce\u9669\u957f\u5c3e\u573a\u666f\u7684\u68c0\u6d4b\u800c\u53d7\u5230\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Deep-Flow \u7684\u65e0\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u4f20\u8f93\u6761\u4ef6\u6d41\u5339\u914d\uff08OT-CFM\uff09\u6765\u8868\u5f81\u4e13\u5bb6\u4eba\u7c7b\u9a7e\u9a76\u884c\u4e3a\u7684\u8fde\u7eed\u6982\u7387\u5bc6\u5ea6\u3002", "result": "\u5728 Waymo Open Motion Dataset (WOMD) \u4e0a\u8bc4\u4f30\u540e\uff0c\u6211\u4eec\u7684\u6846\u67b6\u5728\u5b89\u5168\u5173\u952e\u4e8b\u4ef6\u7684\u542f\u53d1\u5f0f\u9ec4\u91d1\u96c6\u4e0a\u53d6\u5f97\u4e86 0.766 \u7684 AUC-ROC\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5b9a\u4e49\u7edf\u8ba1\u5b89\u5168\u95e8\u63d0\u4f9b\u4e86\u6570\u5b66\u4e25\u8c28\u7684\u57fa\u7840\uff0c\u4f7f\u81ea\u4e3b\u8f66\u961f\u7684\u5b89\u5168\u90e8\u7f72\u5b9e\u73b0\u5ba2\u89c2\u3001\u6570\u636e\u9a71\u52a8\u7684\u9a8c\u8bc1\u3002"}}
{"id": "2602.17601", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17601", "abs": "https://arxiv.org/abs/2602.17601", "authors": ["Patrick Benito Eberhard", "Luis Pabon", "Daniele Gammelli", "Hugo Buurmeijer", "Amon Lahr", "Mark Leone", "Andrea Carron", "Marco Pavone"], "title": "Graph Neural Model Predictive Control for High-Dimensional Systems", "comment": null, "summary": "The control of high-dimensional systems, such as soft robots, requires models that faithfully capture complex dynamics while remaining computationally tractable. This work presents a framework that integrates Graph Neural Network (GNN)-based dynamics models with structure-exploiting Model Predictive Control to enable real-time control of high-dimensional systems. By representing the system as a graph with localized interactions, the GNN preserves sparsity, while a tailored condensing algorithm eliminates state variables from the control problem, ensuring efficient computation. The complexity of our condensing algorithm scales linearly with the number of system nodes, and leverages Graphics Processing Unit (GPU) parallelization to achieve real-time performance. The proposed approach is validated in simulation and experimentally on a physical soft robotic trunk. Results show that our method scales to systems with up to 1,000 nodes at 100 Hz in closed-loop, and demonstrates real-time reference tracking on hardware with sub-centimeter accuracy, outperforming baselines by 63.6%. Finally, we show the capability of our method to achieve effective full-body obstacle avoidance.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f6\u63a7\u5236\u9ad8\u7ef4\u7cfb\u7edf\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u6027\u80fd\u3002", "motivation": "\u63a7\u5236\u9ad8\u7ef4\u7cfb\u7edf\uff08\u5982\u8f6f\u673a\u5668\u4eba\uff09\u9700\u8981\u80fd\u591f\u771f\u5b9e\u6355\u6349\u590d\u6742\u52a8\u6001\u7684\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u7684\u53ef\u884c\u6027\u3002", "method": "\u5c06\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u52a8\u529b\u5b66\u6a21\u578b\u4e0e\u7ed3\u6784\u5229\u7528\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u76f8\u7ed3\u5408\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u7ef4\u7cfb\u7edf\u7684\u5b9e\u65f6\u63a7\u5236\u3002", "result": "\u5728\u6a21\u62df\u548c\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u663e\u793a\u51fa\u5176\u5728\u590d\u6742\u7cfb\u7edf\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5177\u6709\u6700\u591a1000\u4e2a\u8282\u70b9\u7684\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86100 Hz\u7684\u95ed\u73af\u5b9e\u65f6\u64cd\u63a7\uff0c\u5e76\u5728\u786c\u4ef6\u4e0a\u5b9e\u73b0\u4e86\u4e9a\u5398\u7c73\u7cbe\u5ea6\u7684\u5b9e\u65f6\u8ddf\u8e2a\uff0c\u4f18\u4e8e\u57fa\u7ebf\u7ed3\u679c63.6%\u3002"}}
