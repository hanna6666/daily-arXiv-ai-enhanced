{"id": "2511.09602", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.09602", "abs": "https://arxiv.org/abs/2511.09602", "authors": ["Sizhe Wang", "Yifan Yang", "Yongkang Luo", "Daheng Li", "Wei Wei", "Yan Zhang", "Peiying Hu", "Yunjin Fu", "Haonan Duan", "Jia Sun", "Peng Wang"], "title": "ScaleADFG: Affordance-based Dexterous Functional Grasping via Scalable Dataset", "comment": "Accepted by IEEE Robotics and Automation Letters", "summary": "Dexterous functional tool-use grasping is essential for effective robotic manipulation of tools. However, existing approaches face significant challenges in efficiently constructing large-scale datasets and ensuring generalizability to everyday object scales. These issues primarily arise from size mismatches between robotic and human hands, and the diversity in real-world object scales. To address these limitations, we propose the ScaleADFG framework, which consists of a fully automated dataset construction pipeline and a lightweight grasp generation network. Our dataset introduce an affordance-based algorithm to synthesize diverse tool-use grasp configurations without expert demonstrations, allowing flexible object-hand size ratios and enabling large robotic hands (compared to human hands) to grasp everyday objects effectively. Additionally, we leverage pre-trained models to generate extensive 3D assets and facilitate efficient retrieval of object affordances. Our dataset comprising five object categories, each containing over 1,000 unique shapes with 15 scale variations. After filtering, the dataset includes over 60,000 grasps for each 2 dexterous robotic hands. On top of this dataset, we train a lightweight, single-stage grasp generation network with a notably simple loss design, eliminating the need for post-refinement. This demonstrates the critical importance of large-scale datasets and multi-scale object variant for effective training. Extensive experiments in simulation and on real robot confirm that the ScaleADFG framework exhibits strong adaptability to objects of varying scales, enhancing functional grasp stability, diversity, and generalizability. Moreover, our network exhibits effective zero-shot transfer to real-world objects. Project page is available at https://sizhe-wang.github.io/ScaleADFG_webpage", "AI": {"tldr": "\u63d0\u51fa\u65b0\u7684ScaleADFG\u6846\u67b6\uff0c\u5229\u7528\u81ea\u52a8\u6570\u636e\u96c6\u6784\u5efa\u548c\u8f7b\u91cf\u7ea7\u6293\u53d6\u751f\u6210\uff0c\u5b9e\u73b0\u5bf9\u5404\u79cd\u5c3a\u5bf8\u7269\u4f53\u7684\u6709\u6548\u6293\u53d6\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u4e00\u822c\u5316\u5230\u65e5\u5e38\u7269\u4f53\u5c3a\u5bf8\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "ScaleADFG\u6846\u67b6\uff0c\u5305\u62ec\u5168\u81ea\u52a8\u6570\u636e\u96c6\u6784\u5efa\u7ba1\u9053\u548c\u8f7b\u91cf\u7ea7\u6293\u53d6\u751f\u6210\u7f51\u7edc\u3002", "result": "\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u5305\u542b\u4e94\u7c7b\u7269\u4f53\u548c\u8d85\u8fc760,000\u79cd\u6293\u53d6\u65b9\u5f0f\u7684\u6570\u636e\u96c6\uff0c\u63d0\u9ad8\u4e86\u6293\u53d6\u7684\u7a33\u5b9a\u6027\u3001\u591a\u6837\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "ScaleADFG\u6846\u67b6\u5728\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u5f3a\u9002\u5e94\u6027\uff0c\u5e76\u80fd\u6709\u6548\u5b9e\u73b0\u96f6-shot\u8f6c\u79fb\u3002"}}
{"id": "2511.09695", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.09695", "abs": "https://arxiv.org/abs/2511.09695", "authors": ["David Minkwan Kim", "K. M. Brian Lee", "Yong Hyeok Seo", "Nikola Raicevic", "Runfa Blark Li", "Kehan Long", "Chan Seon Yoon", "Dong Min Kang", "Byeong Jo Lim", "Young Pyoung Kim", "Nikolay Atanasov", "Truong Nguyen", "Se Woong Jun", "Young Wook Kim"], "title": "A Shared-Autonomy Construction Robotic System for Overhead Works", "comment": "4pages, 8 figures, ICRA construction workshop", "summary": "We present the ongoing development of a robotic system for overhead work such as ceiling drilling. The hardware platform comprises a mobile base with a two-stage lift, on which a bimanual torso is mounted with a custom-designed drilling end effector and RGB-D cameras. To support teleoperation in dynamic environments with limited visibility, we use Gaussian splatting for online 3D reconstruction and introduce motion parameters to model moving objects. For safe operation around dynamic obstacles, we developed a neural configuration-space barrier approach for planning and control. Initial feasibility studies demonstrate the capability of the hardware in drilling, bolting, and anchoring, and the software in safe teleoperation in a dynamic environment.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7528\u4e8e\u9ad8\u7a7a\u4f5c\u4e1a\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u5177\u5907\u52a8\u6001\u73af\u5883\u4e0b\u7684\u5b89\u5168\u8fdc\u7a0b\u64cd\u4f5c\u80fd\u529b\u3002", "motivation": "\u76ee\u524d\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u7528\u4e8e\u5929\u82b1\u677f\u94bb\u5b54\u7b49\u9ad8\u7a7a\u4f5c\u4e1a\uff0c\u9762\u4e34\u52a8\u6001\u73af\u5883\u548c\u53ef\u89c1\u6027\u6709\u9650\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u9ad8\u65af\u5206\u5e03\u8fdb\u884c\u5728\u7ebf3D\u91cd\u5efa\uff0c\u5e76\u5f15\u5165\u8fd0\u52a8\u53c2\u6570\u6765\u5efa\u6a21\u79fb\u52a8\u7269\u4f53\uff0c\u540c\u65f6\u5f00\u53d1\u4e86\u795e\u7ecf\u914d\u7f6e\u7a7a\u95f4\u969c\u788d\u65b9\u6cd5\u7528\u4e8e\u89c4\u5212\u548c\u63a7\u5236\u3002", "result": "\u786c\u4ef6\u5e73\u53f0\u80fd\u591f\u8fdb\u884c\u94bb\u5b54\u3001\u87ba\u6813\u56fa\u5b9a\u548c\u951a\u56fa\u64cd\u4f5c\uff0c\u8f6f\u4ef6\u5728\u52a8\u6001\u73af\u5883\u4e2d\u652f\u6301\u5b89\u5168\u7684\u8fdc\u7a0b\u64cd\u4f5c\u3002", "conclusion": "\u521d\u6b65\u53ef\u884c\u6027\u7814\u7a76\u8868\u660e\u8be5\u7cfb\u7edf\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b89\u5168\u8fdc\u7a0b\u64cd\u4f5c\u7684\u80fd\u529b\uff0c\u4ee5\u53ca\u786c\u4ef6\u5728\u94bb\u5b54\u3001\u87ba\u6813\u56fa\u5b9a\u548c\u951a\u56fa\u65b9\u9762\u7684\u5e94\u7528\u3002"}}
{"id": "2511.09727", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.09727", "abs": "https://arxiv.org/abs/2511.09727", "authors": ["Stelios Zarifis", "Ioannis Chalkiadakis", "Artemis Chardouveli", "Vasiliki Moutzouri", "Aggelos Sotirchos", "Katerina Papadimitriou", "Panagiotis Filntisis", "Niki Efthymiou", "Petros Maragos", "Katerina Pastra"], "title": "Baby Sophia: A Developmental Approach to Self-Exploration through Self-Touch and Hand Regard", "comment": "5 pages, 3 tables", "summary": "Inspired by infant development, we propose a Reinforcement Learning (RL) framework for autonomous self-exploration in a robotic agent, Baby Sophia, using the BabyBench simulation environment. The agent learns self-touch and hand regard behaviors through intrinsic rewards that mimic an infant's curiosity-driven exploration of its own body. For self-touch, high-dimensional tactile inputs are transformed into compact, meaningful representations, enabling efficient learning. The agent then discovers new tactile contacts through intrinsic rewards and curriculum learning that encourage broad body coverage, balance, and generalization. For hand regard, visual features of the hands, such as skin-color and shape, are learned through motor babbling. Then, intrinsic rewards encourage the agent to perform novel hand motions, and follow its hands with its gaze. A curriculum learning setup from single-hand to dual-hand training allows the agent to reach complex visual-motor coordination. The results of this work demonstrate that purely curiosity-based signals, with no external supervision, can drive coordinated multimodal learning, imitating an infant's progression from random motor babbling to purposeful behaviors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5185\u5728\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u6a21\u62df\u5a74\u513f\u5728\u81ea\u6211\u63a2\u7d22\u4e2d\u7684\u5b66\u4e60\uff0c\u7279\u522b\u662f\u5728\u81ea\u6211\u89e6\u6478\u548c\u624b\u90e8\u5173\u6ce8\u65b9\u9762\u3002", "motivation": "\u7075\u611f\u6765\u6e90\u4e8e\u5a74\u513f\u7684\u53d1\u5c55\u8fc7\u7a0b\uff0c\u65e8\u5728\u8bbe\u8ba1\u51fa\u80fd\u591f\u81ea\u6211\u63a2\u7d22\u548c\u5b66\u4e60\u7684\u673a\u5668\u4eba\u4ee3\u7406\u3002", "method": "\u4f7f\u7528BabyBench\u6a21\u62df\u73af\u5883\uff0c\u901a\u8fc7\u5185\u5728\u5956\u52b1\u548c\u8bfe\u7a0b\u5b66\u4e60\uff0c\u8bad\u7ec3\u673a\u5668\u4eba\u4ee3\u7406\u8fdb\u884c\u81ea\u6211\u89e6\u6478\u548c\u624b\u90e8\u5173\u6ce8\uff0c\u7ed3\u5408\u9ad8\u7ef4\u89e6\u89c9\u8f93\u5165\u548c\u89c6\u89c9\u7279\u5f81\u7684\u5b66\u4e60\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u6a21\u62df\u5a74\u513f\u81ea\u6211\u63a2\u7d22\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u5185\u5728\u5956\u52b1\u5f15\u5bfc\u673a\u5668\u4eba\u4ee3\u7406\u8fdb\u884c\u6709\u6548\u7684\u81ea\u6211\u89e6\u6478\u548c\u624b\u90e8\u5173\u6ce8\u884c\u4e3a\u7684\u5b66\u4e60\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4ec5\u4f9d\u9760\u597d\u5947\u5fc3\u9a71\u52a8\u7684\u5185\u5728\u4fe1\u53f7\u53ef\u4ee5\u6709\u6548\u63a8\u52a8\u673a\u5668\u4eba\u81ea\u4e3b\u7684\u591a\u6a21\u6001\u5b66\u4e60\uff0c\u4eff\u6548\u5a74\u513f\u4ece\u968f\u673a\u8fd0\u52a8\u5230\u6709\u76ee\u7684\u884c\u4e3a\u7684\u8fdb\u5316\u8fc7\u7a0b\u3002"}}
{"id": "2511.09790", "categories": ["cs.RO", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.09790", "abs": "https://arxiv.org/abs/2511.09790", "authors": ["Eshika Pathak", "Ahmed Aboudonia", "Sandeep Banik", "Naira Hovakimyan"], "title": "A Robust Task-Level Control Architecture for Learned Dynamical Systems", "comment": null, "summary": "Dynamical system (DS)-based learning from demonstration (LfD) is a powerful tool for generating motion plans in the operation (`task') space of robotic systems. However, the realization of the generated motion plans is often compromised by a ''task-execution mismatch'', where unmodeled dynamics, persistent disturbances, and system latency cause the robot's actual task-space state to diverge from the desired motion trajectory. We propose a novel task-level robust control architecture, L1-augmented Dynamical Systems (L1-DS), that explicitly handles the task-execution mismatch in tracking a nominal motion plan generated by any DS-based LfD scheme. Our framework augments any DS-based LfD model with a nominal stabilizing controller and an L1 adaptive controller. Furthermore, we introduce a windowed Dynamic Time Warping (DTW)-based target selector, which enables the nominal stabilizing controller to handle temporal misalignment for improved phase-consistent tracking. We demonstrate the efficacy of our architecture on the LASA and IROS handwriting datasets.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u63a7\u5236\u67b6\u6784L1-DS\uff0c\u6709\u6548\u5e94\u5bf9\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u4efb\u52a1\u6267\u884c\u4e0d\u5339\u914d\uff0c\u63d0\u5347\u8fd0\u52a8\u8ba1\u5212\u7684\u5b9e\u73b0\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u8fd0\u52a8\u8ba1\u5212\u5b9e\u65bd\u8fc7\u7a0b\u4e2d\u56e0\u672a\u5efa\u6a21\u52a8\u529b\u5b66\u3001\u6301\u7eed\u6270\u52a8\u548c\u7cfb\u7edf\u5ef6\u8fdf\u5bfc\u81f4\u7684\u4efb\u52a1\u6267\u884c\u4e0d\u5339\u914d\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u540d\u4e49\u7a33\u5b9a\u63a7\u5236\u5668\u548cL1\u81ea\u9002\u5e94\u63a7\u5236\u5668\uff0c\u7ed3\u5408\u52a8\u6001\u65f6\u95f4\u89c4\u6574(DTW)\u7684\u7a97\u53e3\u9009\u62e9\u5668\uff0c\u589e\u5f3a\u4efb\u610f\u57fa\u4e8eDS\u7684LfD\u6a21\u578b\u3002", "result": "\u5728LASA\u548cIROS\u624b\u5199\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u67b6\u6784\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684L1\u589e\u5f3a\u52a8\u529b\u7cfb\u7edf\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u4efb\u52a1\u6267\u884c\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u4f18\u5316\u4e86\u8fd0\u52a8\u8f68\u8ff9\u8ddf\u8e2a\u3002"}}
{"id": "2511.09612", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.09612", "abs": "https://arxiv.org/abs/2511.09612", "authors": ["Joshua Holstein", "Patrick Hemmer", "Gerhard Satzger", "Wei Sun"], "title": "When Thinking Pays Off: Incentive Alignment for Human-AI Collaboration", "comment": null, "summary": "Collaboration with artificial intelligence (AI) has improved human decision-making across various domains by leveraging the complementary capabilities of humans and AI. Yet, humans systematically overrely on AI advice, even when their independent judgment would yield superior outcomes, fundamentally undermining the potential of human-AI complementarity. Building on prior work, we identify prevailing incentive structures in human-AI decision-making as a structural driver of this overreliance. To address this misalignment, we propose an alternative incentive mechanism designed to counteract systemic overreliance. We empirically evaluate this approach through a behavioral experiment with 180 participants, finding that the proposed mechanism significantly reduces overreliance. We also show that while appropriately designed incentives can enhance collaboration and decision quality, poorly designed incentives may distort behavior, introduce unintended consequences, and ultimately degrade performance. These findings underscore the importance of aligning incentives with task context and human-AI complementarities, and suggest that effective collaboration requires a shift toward context-sensitive incentive design.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u4eba\u7c7b\u5bf9AI\u7684\u8fc7\u5ea6\u4f9d\u8d56\u524a\u5f31\u4e86\u4eba\u673a\u4e92\u8865\uff0c\u63d0\u51fa\u65b0\u6fc0\u52b1\u673a\u5236\u53ef\u663e\u8457\u51cf\u5c11\u8fd9\u79cd\u8fc7\u5ea6\u4f9d\u8d56\u3002", "motivation": "\u63a2\u7d22\u4eba\u7c7b\u4e0eAI\u534f\u4f5c\u4e2d\u7684\u8fc7\u5ea6\u4f9d\u8d56\u95ee\u9898\u53ca\u5176\u9a71\u52a8\u56e0\u7d20\uff0c\u5bfb\u6c42\u6539\u5584\u6fc0\u52b1\u673a\u5236\u4ee5\u63d0\u5347\u51b3\u7b56\u8d28\u91cf\u3002", "method": "\u901a\u8fc7180\u540d\u53c2\u4e0e\u8005\u7684\u884c\u4e3a\u5b9e\u9a8c\u8bc4\u4f30\u6fc0\u52b1\u673a\u5236\u7684\u6548\u679c\u3002", "result": "\u672c\u6587\u7814\u7a76\u4e86\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u5728\u5404\u4e2a\u9886\u57df\u5bf9\u4eba\u7c7b\u51b3\u7b56\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4eba\u7c7b\u5bf9AI\u5efa\u8bae\u7684\u8fc7\u5ea6\u4f9d\u8d56\u524a\u5f31\u4e86\u4eba\u7c7b\u4e0eAI\u7684\u4e92\u8865\u6f5c\u529b\u3002\u7814\u7a76\u8bc6\u522b\u4e86\u8fd9\u4e00\u73b0\u8c61\u7684\u7ed3\u6784\u9a71\u52a8\u56e0\u7d20\uff0c\u5373\u4eba\u7c7b\uff0dAI\u51b3\u7b56\u4e2d\u7684\u6fc0\u52b1\u7ed3\u6784\u4e0d\u5339\u914d\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6fc0\u52b1\u673a\u5236\u5e76\u901a\u8fc7180\u540d\u53c2\u4e0e\u8005\u7684\u884c\u4e3a\u5b9e\u9a8c\u8fdb\u884c\u4e86\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u673a\u5236\u663e\u8457\u51cf\u5c11\u4e86\u8fc7\u5ea6\u4f9d\u8d56\u73b0\u8c61\uff0c\u540c\u65f6\u9610\u660e\u4e86\u6fc0\u52b1\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u6709\u6548\u7684\u534f\u4f5c\u9700\u8981\u6839\u636e\u4efb\u52a1\u80cc\u666f\u548c\u4eba\u673a\u4e92\u8865\u6027\u8c03\u6574\u6fc0\u52b1\u8bbe\u8ba1\uff0c\u907f\u514d\u4e0d\u826f\u6fc0\u52b1\u5bfc\u81f4\u7684\u884c\u4e3a\u626d\u66f2\u3002"}}
{"id": "2511.09836", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.09836", "abs": "https://arxiv.org/abs/2511.09836", "authors": ["Kaleb Ben Naveed", "Utkrisht Sahai", "Anouck Girard", "Dimitra Panagou"], "title": "Provably Safe Stein Variational Clarity-Aware Informative Planning", "comment": "Submitted to Learning for Dynamics & Control Conference 2026. Paper Website: (https://usahai18.github.io/stein_clarity/)", "summary": "Autonomous robots are increasingly deployed for information-gathering tasks in environments that vary across space and time. Planning informative and safe trajectories in such settings is challenging because information decays when regions are not revisited. Most existing planners model information as static or uniformly decaying, ignoring environments where the decay rate varies spatially; those that model non-uniform decay often overlook how it evolves along the robot's motion, and almost all treat safety as a soft penalty. In this paper, we address these challenges. We model uncertainty in the environment using clarity, a normalized representation of differential entropy from our earlier work that captures how information improves through new measurements and decays over time when regions are not revisited. Building on this, we present Stein Variational Clarity-Aware Informative Planning, a framework that embeds clarity dynamics within trajectory optimization and enforces safety through a low-level filtering mechanism based on our earlier gatekeeper framework for safety verification. The planner performs Bayesian inference-based learning via Stein variational inference, refining a distribution over informative trajectories while filtering each nominal Stein informative trajectory to ensure safety. Hardware experiments and simulations across environments with varying decay rates and obstacles demonstrate consistent safety and reduced information deficits.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8003\u8651\u4fe1\u606f\u52a8\u6001\u8870\u51cf\u548c\u5b89\u5168\u6027\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5ffd\u7565\u4e86\u4fe1\u606f\u8870\u51cf\u7684\u590d\u6742\u6027\u548c\u5b89\u5168\u6027\u7684\u786c\u7ea6\u675f\uff0c\u4e9f\u9700\u5f00\u53d1\u66f4\u4f73\u7684\u89c4\u5212\u7b56\u7565\u6765\u5e94\u5bf9\u53d8\u52a8\u7684\u73af\u5883\u3002", "method": "\u91c7\u7528\u4e86Stein\u53d8\u5206\u63a8\u65ad\u65b9\u6cd5\uff0c\u7ed3\u5408\u6e05\u6670\u5ea6\u52a8\u6001\uff0c\u5b9e\u73b0\u8f68\u8ff9\u4f18\u5316\uff0c\u540c\u65f6\u901a\u8fc7\u4f4e\u7ea7\u8fc7\u6ee4\u673a\u5236\u589e\u5f3a\u5b89\u5168\u6027\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u9488\u5bf9\u52a8\u6001\u548c\u4e0d\u5747\u5300\u4fe1\u606f\u8870\u51cf\u73af\u5883\u4e2d\u81ea\u4e3b\u673a\u5668\u4eba\u7684\u4fe1\u606f\u6536\u96c6\u4efb\u52a1\u8fdb\u884c\u4f18\u5316\uff0c\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\u548c\u4fe1\u606f\u83b7\u53d6\u6548\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u5b9e\u9a8c\u663e\u793a\u4e86\u5353\u8d8a\u7684\u5b89\u5168\u6027\u548c\u6709\u6548\u7684\u4fe1\u606f\u83b7\u53d6\u80fd\u529b\u3002"}}
{"id": "2511.09658", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.09658", "abs": "https://arxiv.org/abs/2511.09658", "authors": ["Ujjaini Das", "Shreya Kappala", "Meng Chen", "Mina Huh", "Amy Pavel"], "title": "Co-Designing Multimodal Systems for Accessible Remote Dance Instruction", "comment": null, "summary": "Videos make exercise instruction widely available, but they rely on visual demonstrations that blind and low vision (BLV) learners cannot see. While audio descriptions (AD) can make videos accessible, describing movements remains challenging as the AD must convey \\textit{what} to do (mechanics, location, orientation) and \\textit{how} to do it (speed, fluidity, timing). Prior work thus used \\textit{multimodal instruction} to support BLV learners with individual simple movements. However, it is unclear how these approaches scale to dance instruction with unique, complex movements and precise timing constraints. To inform accessible remote dance instruction systems, we conducted three co-design workshops (N=28) with BLV dancers, instructors, and experts in sound, haptics, and AD. Participants designed 8 systems revealing common themes: staged learning to dissect routines, crafting vocabularies for movements, and selectively using modalities (narration for movement structure, sound for expression, and haptics for spatial cues). We conclude with design recommendations to make learning dance accessible.", "AI": {"tldr": "\u672c\u7814\u7a76\u65e8\u5728\u4e3a\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u821e\u8005\u5236\u5b9a\u53ef\u53ca\u7684\u821e\u8e48\u6559\u5b66\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e0e\u53c2\u4e0e\u8005\u5171\u540c\u8bbe\u8ba1\uff0c\u63d0\u51fa\u4e86\u821e\u8e48\u5b66\u4e60\u7684\u8bbe\u8ba1\u5efa\u8bae\u548c\u7cfb\u7edf\u3002", "motivation": "\u65e8\u5728\u4f7f\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u5b66\u4e60\u8005\u80fd\u591f\u66f4\u597d\u5730\u53c2\u4e0e\u821e\u8e48\u5b66\u4e60\uff0c\u89e3\u51b3\u73b0\u6709\u89c6\u9891\u6559\u5b66\u5728\u89c6\u89c9\u4e0a\u7684\u9650\u5236\u3002", "method": "\u901a\u8fc7\u4e0e28\u540dBLV\u821e\u8005\u3001\u6559\u7ec3\u53ca\u58f0\u97f3\u3001\u89e6\u89c9\u548c\u97f3\u9891\u63cf\u8ff0\u4e13\u5bb6\u8fdb\u884c\u4e09\u6b21\u5171\u540c\u8bbe\u8ba1\u7814\u8ba8\u4f1a\uff0c\u83b7\u53d6\u53c2\u4e0e\u8005\u7684\u8bbe\u8ba1\u65b9\u6848\u3002", "result": "\u53c2\u4e0e\u8005\u8bbe\u8ba1\u4e868\u4e2a\u7cfb\u7edf\uff0c\u63ed\u793a\u4e86\u5171\u540c\u4e3b\u9898\uff0c\u5305\u62ec\u5206\u9636\u6bb5\u5b66\u4e60\u3001\u4e3a\u52a8\u4f5c\u521b\u4f5c\u8bcd\u6c47\u3001\u9009\u62e9\u6027\u4f7f\u7528\u4e0d\u540c\u7684\u6559\u5b66\u6a21\u5f0f\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4f7f\u821e\u8e48\u5b66\u4e60\u53ef\u53ca\u7684\u8bbe\u8ba1\u5efa\u8bae\u3002"}}
{"id": "2511.09885", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.09885", "abs": "https://arxiv.org/abs/2511.09885", "authors": ["Shashwat Singh", "Zilin Si", "Zeynep Temel"], "title": "PuffyBot: An Untethered Shape Morphing Robot for Multi-environment Locomotion", "comment": "8 pages, 10 figures, IEEE RoboSoft 2026", "summary": "Amphibians adapt their morphologies and motions to accommodate movement in both terrestrial and aquatic environments. Inspired by these biological features, we present PuffyBot, an untethered shape morphing robot capable of changing its body morphology to navigate multiple environments. Our robot design leverages a scissor-lift mechanism driven by a linear actuator as its primary structure to achieve shape morphing. The transformation enables a volume change from 255.00 cm3 to 423.75 cm3, modulating the buoyant force to counteract a downward force of 3.237 N due to 330 g mass of the robot. A bell-crank linkage is integrated with the scissor-lift mechanism, which adjusts the servo-actuated limbs by 90 degrees, allowing a seamless transition between crawling and swimming modes. The robot is fully waterproof, using thermoplastic polyurethane (TPU) fabric to ensure functionality in aquatic environments. The robot can operate untethered for two hours with an onboard battery of 1000 mA h. Our experimental results demonstrate multi-environment locomotion, including crawling on the land, crawling on the underwater floor, swimming on the water surface, and bimodal buoyancy adjustment to submerge underwater or resurface. These findings show the potential of shape morphing to create versatile and energy efficient robotic platforms suitable for diverse environments.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aPuffyBot\u7684\u53d8\u5f62\u673a\u5668\u4eba\uff0c\u80fd\u591f\u5728\u9646\u5730\u4e0e\u6c34\u4e2d\u81ea\u5982\u79fb\u52a8\uff0c\u5177\u6709\u826f\u597d\u7684\u9002\u5e94\u6027\u548c\u80fd\u6548\u3002", "motivation": "\u53d7\u5230\u4e24\u6816\u52a8\u7269\u5f62\u6001\u4e0e\u52a8\u4f5c\u9002\u5e94\u6027\u7684\u542f\u53d1\uff0c\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5728\u591a\u79cd\u73af\u5883\u4e2d\u7075\u6d3b\u79fb\u52a8\u7684\u673a\u5668\u4eba\u3002", "method": "\u8be5\u8bba\u6587\u91c7\u7528\u526a\u5200\u5347\u964d\u673a\u5236\u548c\u5feb\u901f\u8054\u52a8\u673a\u6784\u8bbe\u8ba1\uff0c\u4ee5\u5b9e\u73b0\u673a\u5668\u4eba\u7684\u5f62\u6001\u53d8\u5316\u548c\u591a\u73af\u5883\u79fb\u52a8\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u673a\u5668\u4eba\u80fd\u5728\u9646\u5730\u4e0a\u722c\u884c\u3001\u6c34\u4e0b\u722c\u884c\u3001\u6c34\u9762\u6e38\u6cf3\uff0c\u5e76\u5177\u5907\u53cc\u6a21\u6d6e\u529b\u8c03\u6574\u7684\u80fd\u529b\u3002", "conclusion": "\u8fd9\u4e9b\u7814\u7a76\u7ed3\u679c\u8868\u660e\u5f62\u6001\u53d8\u5f62\u6280\u672f\u80fd\u591f\u521b\u9020\u51fa\u9002\u5e94\u591a\u79cd\u73af\u5883\u7684\u591a\u529f\u80fd\u4e14\u9ad8\u6548\u80fd\u7684\u673a\u5668\u4eba\u5e73\u53f0\u3002"}}
{"id": "2511.09667", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.09667", "abs": "https://arxiv.org/abs/2511.09667", "authors": ["Zekun Wu", "Mayank Jobanputra", "Vera Demberg", "Jessica Hullman", "Anna Maria Feit"], "title": "How AI Responses Shape User Beliefs: The Effects of Information Detail and Confidence on Belief Strength and Stance", "comment": null, "summary": "The growing use of AI-generated responses in everyday tools raises concern about how subtle features such as supporting detail or tone of confidence may shape people's beliefs. To understand this, we conducted a pre-registered online experiment (N = 304) investigating how the detail and confidence of AI-generated responses influence belief change. We introduce an analysis framework with two targeted measures: belief switch and belief shift. These distinguish between users changing their initial stance after AI input and the extent to which they adjust their conviction toward or away from the AI's stance, thereby quantifying not only categorical changes but also more subtle, continuous adjustments in belief strength that indicate a reinforcement or weakening of existing beliefs. Using this framework, we find that detailed responses with medium confidence are associated with the largest overall belief changes. Highly confident messages tend to elicit belief shifts but induce fewer stance reversals. Our results also show that task type (fact-checking versus opinion evaluation), prior conviction, and perceived stance agreement further modulate the extent and direction of belief change. These findings illustrate how different properties of AI responses interact with user beliefs in subtle but potentially consequential ways and raise practical as well as ethical considerations for the design of LLM-powered systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5728\u7ebf\u5b9e\u9a8c\u63a2\u8ba8AI\u751f\u6210\u56de\u5e94\u7684\u7ec6\u8282\u548c\u4fe1\u5fc3\u5bf9\u4fe1\u5ff5\u53d8\u5316\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4e24\u8005\u7684\u7ec4\u5408\u5bf9\u4fe1\u5ff5\u53d8\u5316\u6709\u663e\u8457\u4f5c\u7528\u3002", "motivation": "\u63a2\u8ba8AI\u751f\u6210\u7684\u56de\u5e94\u5982\u4f55\u901a\u8fc7\u7ec6\u5fae\u7279\u5f81\u5f71\u54cd\u4eba\u4eec\u7684\u4fe1\u5ff5\uff0c\u7279\u522b\u662f\u7ec6\u8282\u548c\u4fe1\u5fc3\u7b49\u7279\u5f81\u3002", "method": "\u8fdb\u884c\u4e86\u4e00\u4e2a\u9884\u6ce8\u518c\u7684\u5728\u7ebf\u5b9e\u9a8c\uff0c\u53c2\u4e0e\u8005\u4e3a304\u4eba\uff0c\u4f7f\u7528\u4e86\u4fe1\u5ff5\u8f6c\u6362\u548c\u4fe1\u5ff5\u504f\u79fb\u4e24\u79cd\u5206\u6790\u6307\u6807\u3002", "result": "\u8be6\u7ec6\u56de\u5e94\u4e0e\u4e2d\u7b49\u4fe1\u5fc3\u5173\u8054\u7684\u4fe1\u5ff5\u53d8\u5316\u6700\u5927\uff0c\u800c\u9ad8\u5ea6\u81ea\u4fe1\u7684\u56de\u5e94\u66f4\u503e\u5411\u4e8e\u5f15\u53d1\u4fe1\u5ff5\u8f6c\u53d8\u4f46\u8f83\u5c11\u5f15\u8d77\u7acb\u573a\u53cd\u8f6c\u3002", "conclusion": "\u4e0d\u540c\u7684AI\u56de\u5e94\u5c5e\u6027\u4ee5\u5fae\u5999\u4f46\u6f5c\u5728\u91cd\u8981\u7684\u65b9\u5f0f\u4e0e\u7528\u6237\u4fe1\u5ff5\u76f8\u4e92\u4f5c\u7528\uff0c\u5728\u8bbe\u8ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u7cfb\u7edf\u65f6\u9700\u8003\u8651\u8fd9\u4e9b\u4f26\u7406\u548c\u5b9e\u8df5\u95ee\u9898\u3002"}}
{"id": "2511.09932", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.09932", "abs": "https://arxiv.org/abs/2511.09932", "authors": ["Hanwen Wang"], "title": "A Study on Enhancing the Generalization Ability of Visuomotor Policies via Data Augmentation", "comment": null, "summary": "The generalization ability of visuomotor policy is crucial, as a good policy should be deployable across diverse scenarios. Some methods can collect large amounts of trajectory augmentation data to train more generalizable imitation learning policies, aimed at handling the random placement of objects on the scene's horizontal plane. However, the data generated by these methods still lack diversity, which limits the generalization ability of the trained policy. To address this, we investigate the performance of policies trained by existing methods across different scene layout factors via automate the data generation for those factors that significantly impact generalization. We have created a more extensively randomized dataset that can be efficiently and automatically generated with only a small amount of human demonstration. The dataset covers five types of manipulators and two types of grippers, incorporating extensive randomization factors such as camera pose, lighting conditions, tabletop texture, and table height across six manipulation tasks. We found that all of these factors influence the generalization ability of the policy. Applying any form of randomization enhances policy generalization, with diverse trajectories particularly effective in bridging visual gap. Notably, we investigated on low-cost manipulator the effect of the scene randomization proposed in this work on enhancing the generalization capability of visuomotor policies for zero-shot sim-to-real transfer.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u4e0d\u540c\u573a\u666f\u5e03\u5c40\u56e0\u7d20\u4e0b\u8bad\u7ec3\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u6570\u636e\u96c6\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u8fd9\u4e00\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u63d0\u5347\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u6211\u4eec\u63a2\u8ba8\u73b0\u6709\u65b9\u6cd5\u5728\u4e0d\u540c\u573a\u666f\u5e03\u5c40\u56e0\u7d20\u4e0b\u7684\u8868\u73b0\uff0c\u610f\u8bc6\u5230\u6570\u636e\u7684\u591a\u6837\u6027\u5bf9\u8bad\u7ec3\u6548\u679c\u7684\u91cd\u8981\u6027\u3002", "method": "\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u8f83\u4e3a\u5168\u9762\u7684\u968f\u673a\u5316\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u591a\u79cd\u64cd\u4f5c\u673a\u5668\u4eba\u548c\u5939\u5177\u7c7b\u578b\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u4ec5\u9700\u5c11\u91cf\u4eba\u7c7b\u793a\u8303\u6765\u63d0\u9ad8\u6570\u636e\u7684\u591a\u6837\u6027\u3002", "result": "\u53d1\u73b0\u573a\u666f\u7684\u968f\u673a\u5316\u56e0\u7d20\u5bf9\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5e94\u7528\u65f6\u53ef\u63d0\u5347\u653f\u7b56\u7684\u9002\u5e94\u6027\uff0c\u5c24\u5176\u662f\u5728\u96f6-shot\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u7684\u573a\u666f\u4e2d\u3002", "conclusion": "\u901a\u8fc7\u5e94\u7528\u4e0d\u540c\u5f62\u5f0f\u7684\u968f\u673a\u5316\uff0c\u5c24\u5176\u662f\u591a\u6837\u5316\u7684\u8f68\u8ff9\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8fd9\u4e3a\u5b9e\u73b0\u96f6-shot\u4ece\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2511.09813", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.09813", "abs": "https://arxiv.org/abs/2511.09813", "authors": ["Gabrielle M Gauthier", "Eesha Ali", "Amna Asim", "Sarah Cornell-Maier", "Lori A. Zoellner"], "title": "I've Seen Enough: Measuring the Toll of Content Moderation on Mental Health", "comment": null, "summary": "Human content moderators (CMs) routinely review distressing digital content at scale. Beyond exposure, the work context (e.g., workload, team structure, and support) may shape mental health outcomes. We examined a cross sectional international CM sample (N = 166) and a U.S. prospective CM sample, including a comparison group of data labelers or tech support workers (N = 45) and gold standard diagnostic interviews. Predictors included workplace factors (e.g., hours per day distressing content, culture), cognitive-affective individual differences, and coping. Across samples, probable diagnoses based on validated clinical cutoffs were elevated (PTSD: 25.9 to 26.3%; depression: 42.1 to 48.5%; somatic symptoms: 68.7 to 89.5%; alcohol misuse: 10.5% to 18.3%). In the U.S. sample, CMs had higher interviewer rated PTSD severity (d = 1.50), likelihood of a current mood disorder (RR = 8.22), and lifetime major depressive disorder (RR = 2.15) compared to data labelers/tech-support workers. Negative automatic thoughts (b = .39 to .74), ongoing stress (b = .27 to .55), and avoidant coping (b = .30 to .34) consistently predicted higher PTSD and depression severity across samples and at 3 month followup. Poorer perceived workplace culture was associated with higher depression (b = -.16 to -.32). These findings strongly implicate organizational context and related individual response styles, not exposure dose alone in shaping risk. We highlight structural and technological interventions such as limits on daily exposure, supportive team culture, interface features to reduce intrusive memories, and training of cognitive restructuring and adaptive coping to support mental health. We also connect implications to adjacent human in the loop data work (e.g., AI red teaming), where similar risks are emerging.", "AI": {"tldr": "\u5185\u5bb9\u5ba1\u6838\u5458\u9762\u4e34\u8f83\u9ad8\u7684\u5fc3\u7406\u5065\u5eb7\u98ce\u9669\uff0c\u9700\u901a\u8fc7\u6539\u8fdb\u5de5\u4f5c\u73af\u5883\u548c\u5e94\u5bf9\u7b56\u7565\u6765\u964d\u4f4e\u98ce\u9669\u3002", "motivation": "\u63a2\u7d22\u5185\u5bb9\u5ba1\u6838\u5458\u5728\u5de5\u4f5c\u73af\u5883\u4e2d\u63a5\u89e6\u75db\u82e6\u5185\u5bb9\u540e\uff0c\u5de5\u4f5c\u4e0a\u4e0b\u6587\u5bf9\u5176\u5fc3\u7406\u5065\u5eb7\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u4e86166\u540d\u56fd\u9645\u6837\u672c\u53ca45\u540d\u7f8e\u56fd\u6837\u672c\u5185\u5bb9\u5ba1\u6838\u5458\u7684\u5fc3\u7406\u5065\u5eb7\u72b6\u51b5\uff0c\u6bd4\u8f83\u4e86\u6570\u636e\u6807\u6ce8\u8005\u53ca\u6280\u672f\u652f\u6301\u5de5\u4f5c\u4eba\u5458\u7684\u6570\u636e\u3002", "result": "\u5185\u5bb9\u5ba1\u6838\u5458\u7684PTSD\u3001\u6291\u90c1\u75c7\u3001\u8eaf\u4f53\u75c7\u72b6\u548c\u9152\u7cbe\u8bef\u7528\u7684\u8bca\u65ad\u7387\u663e\u8457\u9ad8\u4e8e\u5bf9\u7167\u7ec4\uff0c\u5f71\u54cd\u56e0\u7d20\u5305\u62ec\u8d1f\u9762\u81ea\u52a8\u601d\u7ef4\u3001\u6301\u7eed\u538b\u529b\u548c\u9003\u907f\u5e94\u5bf9\u3002", "conclusion": "\u7ec4\u7ec7\u73af\u5883\u548c\u4e2a\u4eba\u5e94\u5bf9\u65b9\u5f0f\u5f71\u54cd\u5fc3\u7406\u5065\u5eb7\u98ce\u9669\uff0c\u5efa\u8bae\u91c7\u53d6\u7ed3\u6784\u6027\u548c\u6280\u672f\u6027\u5e72\u9884\u63aa\u65bd\u6765\u4fdd\u62a4\u5185\u5bb9\u5ba1\u6838\u5458\u7684\u5fc3\u7406\u5065\u5eb7\u3002"}}
{"id": "2511.09958", "categories": ["cs.RO", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.09958", "abs": "https://arxiv.org/abs/2511.09958", "authors": ["Xiangyi Wei", "Haotian Zhang", "Xinyi Cao", "Siyu Xie", "Weifeng Ge", "Yang Li", "Changbo Wang"], "title": "Audio-VLA: Adding Contact Audio Perception to Vision-Language-Action Model for Robotic Manipulation", "comment": null, "summary": "The Vision-Language-Action models (VLA) have achieved significant advances in robotic manipulation recently. However, vision-only VLA models create fundamental limitations, particularly in perceiving interactive and manipulation dynamic processes. This paper proposes Audio-VLA, a multimodal manipulation policy that leverages contact audio to perceive contact events and dynamic process feedback. Audio-VLA overcomes the vision-only constraints of VLA models. Additionally, this paper introduces the Task Completion Rate (TCR) metric to systematically evaluate dynamic operational processes. Audio-VLA employs pre-trained DINOv2 and SigLIP as visual encoders, AudioCLIP as the audio encoder, and Llama2 as the large language model backbone. We apply LoRA fine-tuning to these pre-trained modules to achieve robust cross-modal understanding of both visual and acoustic inputs. A multimodal projection layer aligns features from different modalities into the same feature space. Moreover RLBench and LIBERO simulation environments are enhanced by adding collision-based audio generation to provide realistic sound feedback during object interactions. Since current robotic manipulation evaluations focus on final outcomes rather than providing systematic assessment of dynamic operational processes, the proposed TCR metric measures how well robots perceive dynamic processes during manipulation, creating a more comprehensive evaluation metric. Extensive experiments on LIBERO, RLBench, and two real-world tasks demonstrate Audio-VLA's superior performance over vision-only comparative methods, while the TCR metric effectively quantifies dynamic process perception capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAudio-VLA\u6a21\u578b\uff0c\u901a\u8fc7\u878d\u5408\u97f3\u9891\u4fe1\u606f\u6765\u589e\u5f3a\u673a\u5668\u4eba\u7684\u52a8\u6001\u8fc7\u7a0b\u611f\u77e5\u80fd\u529b\uff0c\u5e76\u5f15\u5165TCR\u6307\u6807\u4ee5\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u64cd\u4f5c\u8fc7\u7a0b\u3002", "motivation": "\u76ee\u524d\u5927\u591a\u6570\u89c6\u89c9\u8bed\u8a00\u884c\u52a8\u6a21\u578b\u5728\u611f\u77e5\u4ea4\u4e92\u548c\u52a8\u6001\u8fc7\u7a0b\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u56e0\u6b64\u9700\u8981\u5f15\u5165\u97f3\u9891\u4fe1\u606f\u4ee5\u63d0\u5347\u52a8\u6001\u8fc7\u7a0b\u7684\u611f\u77e5\u80fd\u529b\u3002", "method": "\u91c7\u7528Audio-VLA\u6a21\u578b\uff0c\u7ed3\u5408\u89c6\u89c9\u7f16\u7801\u5668DINOv2\u3001SigLIP\u548c\u97f3\u9891\u7f16\u7801\u5668AudioCLIP\uff0c\u4ee5\u53ca\u5927\u8bed\u8a00\u6a21\u578bLlama2\uff0c\u901a\u8fc7LoRA\u5fae\u8c03\u5b9e\u73b0\u8de8\u6a21\u6001\u7406\u89e3\uff1b\u5e76\u5f15\u5165\u591a\u6a21\u6001\u6295\u5f71\u5c42\u5bf9\u4e0d\u540c\u6a21\u6001\u7279\u5f81\u8fdb\u884c\u5bf9\u9f50\u3002", "result": "\u5728LIBERO\u3001RLBench\u548c\u4e24\u4e2a\u771f\u5b9e\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cAudio-VLA\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u89c6\u89c9\u7684\u65b9\u6cd5\uff0c\u540c\u65f6TCR\u6307\u6807\u6709\u6548\u5730\u91cf\u5316\u4e86\u52a8\u6001\u8fc7\u7a0b\u7684\u611f\u77e5\u80fd\u529b\u3002", "conclusion": "Audio-VLA\u5728\u52a8\u6001\u8fc7\u7a0b\u611f\u77e5\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u4ec5\u4f9d\u8d56\u89c6\u89c9\u7684\u6a21\u578b\uff0c\u800cTCR\u6307\u6807\u80fd\u591f\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u64cd\u4f5c\u8fc7\u7a0b\u7684\u52a8\u6001\u6027\u3002"}}
{"id": "2511.09846", "categories": ["cs.HC", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.09846", "abs": "https://arxiv.org/abs/2511.09846", "authors": ["Mehedi Hasan Raju", "Oleg V. Komogortsev"], "title": "Real-Time Lightweight Gaze Privacy-Preservation Techniques Validated via Offline Gaze-Based Interaction Simulation", "comment": "11 pages, 2 Figures, Under Review", "summary": "This study examines the effectiveness of the real-time privacy-preserving techniques through an offline gaze-based interaction simulation framework. Those techniques aim to reduce the amount of identity-related information in eye-tracking data while improving the efficacy of the gaze-based interaction. Although some real-time gaze privatization methods were previously explored, their validation on the large dataset was not conducted. We propose a functional framework that allows to study the efficacy of real-time gaze privatization on an already collected offline dataset. The key metric used to assess the reduction of identity-related information is the identification rate, while improvements in gaze-based interactions are evaluated through signal quality during interaction. Our additional contribution is the employment of an extremely lightweight Kalman filter framework that reduces the amount of identity-related information in the gaze signal and improves gaze-based interaction performance.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u79bb\u7ebf\u6ce8\u89c6\u4ea4\u4e92\u6a21\u62df\u6846\u67b6\u8bc4\u4f30\u5b9e\u65f6\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u7684\u6709\u6548\u6027\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u529f\u80fd\u6846\u67b6\u6765\u6539\u8fdb\u6ce8\u89c6\u9690\u79c1\u5316\u6548\u7387\u3002", "motivation": "\u7814\u7a76\u5b9e\u65f6\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u7684\u6709\u6548\u6027\uff0c\u4ee5\u51cf\u5c11\u773c\u52a8\u6570\u636e\u4e2d\u7684\u8eab\u4efd\u76f8\u5173\u4fe1\u606f\uff0c\u540c\u65f6\u63d0\u9ad8\u57fa\u4e8e\u6ce8\u89c6\u7684\u4ea4\u4e92\u6548\u679c\u3002", "method": "\u901a\u8fc7\u4e00\u4e2a\u79bb\u7ebf\u773c\u52a8\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u8eab\u4efd\u8bc6\u522b\u7387\u548c\u4ea4\u4e92\u4fe1\u53f7\u8d28\u91cf\u6765\u9a8c\u8bc1\u5b9e\u65f6\u9690\u79c1\u5316\u6280\u672f\u7684\u6548\u679c\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u529f\u80fd\u6846\u67b6\uff0c\u53ef\u4ee5\u5728\u5df2\u6536\u96c6\u7684\u79bb\u7ebf\u6570\u636e\u96c6\u4e0a\u7814\u7a76\u5b9e\u65f6\u6ce8\u89c6\u9690\u79c1\u5316\u7684\u6709\u6548\u6027\uff0c\u5e76\u5f15\u5165\u8f7b\u91cf\u7ea7\u7684\u5361\u5c14\u66fc\u6ee4\u6ce2\u6846\u67b6\u3002", "conclusion": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u8fdb\u884c\u7684\u9690\u79c1\u4fdd\u62a4\u65b9\u6848\u6709\u6548\u964d\u4f4e\u4e86\u8eab\u4efd\u76f8\u5173\u4fe1\u606f\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u6ce8\u89c6\u4e92\u52a8\u6027\u80fd\u3002"}}
{"id": "2511.10008", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10008", "abs": "https://arxiv.org/abs/2511.10008", "authors": ["Xuancun Lu", "Jiaxiang Chen", "Shilin Xiao", "Zizhi Jin", "Zhangrui Chen", "Hanwen Yu", "Bohan Qian", "Ruochen Zhou", "Xiaoyu Ji", "Wenyuan Xu"], "title": "Phantom Menace: Exploring and Enhancing the Robustness of VLA Models against Physical Sensor Attacks", "comment": "Accepted by AAAI 2026", "summary": "Vision-Language-Action (VLA) models revolutionize robotic systems by enabling end-to-end perception-to-action pipelines that integrate multiple sensory modalities, such as visual signals processed by cameras and auditory signals captured by microphones. This multi-modality integration allows VLA models to interpret complex, real-world environments using diverse sensor data streams. Given the fact that VLA-based systems heavily rely on the sensory input, the security of VLA models against physical-world sensor attacks remains critically underexplored.\n  To address this gap, we present the first systematic study of physical sensor attacks against VLAs, quantifying the influence of sensor attacks and investigating the defenses for VLA models. We introduce a novel ``Real-Sim-Real'' framework that automatically simulates physics-based sensor attack vectors, including six attacks targeting cameras and two targeting microphones, and validates them on real robotic systems. Through large-scale evaluations across various VLA architectures and tasks under varying attack parameters, we demonstrate significant vulnerabilities, with susceptibility patterns that reveal critical dependencies on task types and model designs. We further develop an adversarial-training-based defense that enhances VLA robustness against out-of-distribution physical perturbations caused by sensor attacks while preserving model performance. Our findings expose an urgent need for standardized robustness benchmarks and mitigation strategies to secure VLA deployments in safety-critical environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u5730\u63a2\u8ba8\u4e86\u7269\u7406\u4f20\u611f\u5668\u653b\u51fb\u5bf9VLA\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u9632\u5fa1\u7b56\u7565\uff0c\u8868\u660e\u5728\u5b89\u5168\u5173\u952e\u73af\u5883\u4e2d\u9700\u8981\u5efa\u7acb\u6807\u51c6\u5316\u7684\u9c81\u68d2\u6027\u57fa\u51c6\u3002", "motivation": "VLA\u7cfb\u7edf\u4f9d\u8d56\u4e8e\u591a\u79cd\u4f20\u611f\u5668\u8f93\u5165\uff0c\u4f46\u5728\u9762\u5bf9\u7269\u7406\u4e16\u754c\u4f20\u611f\u5668\u653b\u51fb\u65f6\uff0c\u73b0\u6709\u7814\u7a76\u672a\u80fd\u5bf9\u5176\u5b89\u5168\u6027\u8fdb\u884c\u7cfb\u7edf\u6027\u63a2\u8ba8\uff0c\u56e0\u6b64\u4e9f\u9700\u5bf9\u5176\u8106\u5f31\u6027\u8fdb\u884c\u6df1\u5165\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u65b0\u9896\u7684\u201cReal-Sim-Real\u201d\u6846\u67b6\uff0c\u81ea\u52a8\u6a21\u62df\u57fa\u4e8e\u7269\u7406\u7684\u4f20\u611f\u5668\u653b\u51fb\u5411\u91cf\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\u8fd9\u4e9b\u653b\u51fb\uff0c\u8fdb\u884c\u5927\u89c4\u6a21\u8bc4\u4f30\uff0c\u4ee5\u91cf\u5316\u4e0d\u540cVLA\u67b6\u6784\u548c\u4efb\u52a1\u5728\u4e0d\u540c\u653b\u51fb\u53c2\u6570\u4e0b\u7684\u5f71\u54cd\u3002", "result": "\u5728\u5bf9\u4e0d\u540cVLA\u67b6\u6784\u548c\u4efb\u52a1\u7684\u5927\u89c4\u6a21\u8bc4\u4f30\u4e2d\uff0c\u53d1\u73b0\u4e86\u663e\u8457\u7684\u8106\u5f31\u6027\uff0c\u5e76\u8bc6\u522b\u51fa\u4efb\u52a1\u7c7b\u578b\u548c\u6a21\u578b\u8bbe\u8ba1\u7684\u5173\u952e\u4f9d\u8d56\u6027\uff0c\u540c\u65f6\u5f00\u53d1\u4e86\u57fa\u4e8e\u5bf9\u6297\u8bad\u7ec3\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u5bf9\u4f20\u611f\u5668\u653b\u51fb\u9020\u6210\u7684\u51fa\u754c\u7269\u7406\u6270\u52a8\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86VLA\u6a21\u578b\u5728\u7269\u7406\u4f20\u611f\u5668\u653b\u51fb\u4e0b\u7684\u663e\u8457\u8106\u5f31\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u5bf9\u6297\u8bad\u7ec3\u7684\u9632\u5fa1\u673a\u5236\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86\u5728\u5b89\u5168\u5173\u952e\u73af\u5883\u4e2d\u5efa\u7acb\u6807\u51c6\u5316\u9c81\u68d2\u6027\u57fa\u51c6\u548c\u7f13\u89e3\u7b56\u7565\u7684\u7d27\u8feb\u6027\u3002"}}
{"id": "2511.09867", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.09867", "abs": "https://arxiv.org/abs/2511.09867", "authors": ["Kamrul Hasan", "Dmytro Katrychuk", "Mehedi Hasan Raju", "Oleg V. Komogortsev"], "title": "Quantitative and Qualitative Comparison of Generative Models for Subject-Specific Gaze Synthesis: Diffusion vs GAN", "comment": "14 pages, 2 figures, 3 tables", "summary": "Recent advances in deep learning demonstrate the ability to generate synthetic gaze data. However, most approaches have primarily focused on generating data from random noise distributions or global, predefined latent embeddings, whereas individualized gaze sequence generation has been less explored. To address this gap, we revisit two recent approaches based on diffusion and generative adversarial networks (GANs) and introduce modifications that make both models explicitly subject-aware while improving accuracy and effectiveness. For the diffusion-based approach, we utilize compact user embeddings that emphasize per-subject traits. Moreover, for the GAN-based approach, we propose a subject-specific synthesis module that conditioned the generator to retain better idiosyncratic gaze information. Finally, we conduct a comprehensive assessment of these modified approaches utilizing standard eye-tracking signal quality metrics, including spatial accuracy and precision. This work helps define synthetic signal quality, realism, and subject specificity, thereby contributing to the potential development of gaze-based applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u6539\u8fdb\u4e86\u57fa\u4e8e\u6269\u6563\u548c GAN \u7684\u65b9\u6cd5\uff0c\u4f7f\u5176\u80fd\u591f\u751f\u6210\u4e2a\u4f53\u5316\u7684\u6ce8\u89c6\u5e8f\u5217\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u7528\u6237\u610f\u8bc6\u53ca\u51c6\u786e\u6027\u3002", "motivation": "\u63a2\u8ba8\u4e2a\u6027\u5316\u6ce8\u89c6\u5e8f\u5217\u751f\u6210\u7684\u4e0d\u8db3\uff0c\u6539\u8fdb\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u6709\u6548\u6027\u3002", "method": "\u91c7\u7528\u7d27\u51d1\u7684\u7528\u6237\u5d4c\u5165\u548c\u9488\u5bf9\u7279\u5b9a\u4e3b\u4f53\u7684\u5408\u6210\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u66f4\u4e3a\u4e2a\u6027\u5316\u7684\u6ce8\u89c6\u5e8f\u5217\u751f\u6210\u3002", "result": "\u63d0\u51fa\u4e86\u6539\u8fdb\u7684\u6269\u6563\u548c\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u65b9\u6cd5\uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u597d\u5730\u751f\u6210\u4e2a\u6027\u5316\u7684\u6ce8\u89c6\u6570\u636e\u3002", "conclusion": "\u901a\u8fc7\u4e25\u8c28\u7684\u8bc4\u4f30\uff0c\u8fd9\u4e9b\u6539\u8fdb\u6709\u52a9\u4e8e\u5b9a\u4e49\u5408\u6210\u4fe1\u53f7\u7684\u8d28\u91cf\u3001\u771f\u5b9e\u6027\u548c\u4e2a\u4f53\u7279\u6027\uff0c\u63a8\u52a8\u57fa\u4e8e\u6ce8\u89c6\u7684\u5e94\u7528\u53d1\u5c55\u3002"}}
{"id": "2511.10021", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.10021", "abs": "https://arxiv.org/abs/2511.10021", "authors": ["Egor Davydenko", "Andrei Volchenkov", "Vladimir Gerasimov", "Roman Gorbachev"], "title": "DecARt Leg: Design and Evaluation of a Novel Humanoid Robot Leg with Decoupled Actuation for Agile Locomotion", "comment": null, "summary": "In this paper, we propose a novel design of an electrically actuated robotic leg, called the DecARt (Decoupled Actuation Robot) Leg, aimed at performing agile locomotion. This design incorporates several new features, such as the use of a quasi-telescopic kinematic structure with rotational motors for decoupled actuation, a near-anthropomorphic leg appearance with a forward facing knee, and a novel multi-bar system for ankle torque transmission from motors placed above the knee. To analyze the agile locomotion capabilities of the design numerically, we propose a new descriptive metric, called the `Fastest Achievable Swing Time` (FAST), and perform a quantitative evaluation of the proposed design and compare it with other designs. Then we evaluate the performance of the DecARt Leg-based robot via extensive simulation and preliminary hardware experiments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7535\u52a8\u673a\u5668\u4eba\u817fDecARt\uff0c\u65e8\u5728\u63d0\u5347\u654f\u6377\u8fd0\u52a8\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u65b0\u6307\u6807FAST\u8fdb\u884c\u8bc4\u4f30\u3002", "motivation": "\u4e3a\u5b9e\u73b0\u654f\u6377\u8fd0\u52a8\uff0c\u8bbe\u8ba1\u4e00\u79cd\u65b0\u578b\u673a\u5668\u4eba\u817f\uff0c\u5177\u5907\u65b0\u7684\u529f\u80fd\u7279\u70b9\uff0c\u4ee5\u4f18\u5316\u8fd0\u52a8\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u7535\u52a8\u673a\u5668\u4eba\u817f\u8bbe\u8ba1\uff0c\u79f0\u4e3aDecARt\uff08Decoupled Actuation Robot\uff09\u817f\uff0c\u901a\u8fc7\u6570\u503c\u5206\u6790\u548c\u6bd4\u8f83\u8bc4\u4f30\u5176\u654f\u6377\u8fd0\u52a8\u80fd\u529b\u3002", "result": "\u63d0\u51fa\u4e86\u65b0\u7684\u63cf\u8ff0\u6027\u6307\u6807'\u5feb\u901f\u53ef\u8fbe\u6446\u52a8\u65f6\u95f4'(FAST)\uff0c\u5e76\u901a\u8fc7\u5b9a\u91cf\u8bc4\u4f30\u4e0e\u5176\u4ed6\u8bbe\u8ba1\u8fdb\u884c\u6bd4\u8f83\uff0c\u540c\u65f6\u8fdb\u884c\u5e7f\u6cdb\u7684\u4eff\u771f\u548c\u521d\u6b65\u786c\u4ef6\u5b9e\u9a8c\u3002", "conclusion": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DecARt\u817f\u7684\u6027\u80fd\uff0c\u8868\u660e\u5176\u5728\u654f\u6377\u8fd0\u52a8\u65b9\u9762\u7684\u4f18\u52bf\u3002"}}
{"id": "2511.10026", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.10026", "abs": "https://arxiv.org/abs/2511.10026", "authors": ["Yichen Gao", "Menghan Hu", "Gang Luo"], "title": "Grating haptic perception through touchscreen: Sighted vs. Visually Impaired", "comment": null, "summary": "Providing haptic feedback via smartphone touch screen may potentially offer blind people a capability to understand graphs. This study investigated the discrimination performance of haptic gratings in different frequencies, in both visually impaired (VI) and sighted (S) individuals. 6 VI participants and 10 S participants took part in two experiments designed to compare their ability to interpret grating images with a finger swiping across a smartphone touchscreen without vision. The swipe gesture activates phone vibration temporally synchronized with the black stripes. Their tasks were: (1) determining whether a grating pattern is presented on the touchscreen, (2) comparing two different grating frequencies and determining the wider one. Results demonstrated that the VI group exhibited superior tactile sensitivity compared to the S group, as evidenced by their significantly better performance in Experiment 1 (accuracy of 99.15\\% vs. 84.5\\%). Experiment 2 revealed that the peak performance of VI participants was approximately around 0.270 cycles per mm (83.3\\% accuracy), a frequency very similar to Braille dot spacing, while S group peaked around 0.963 cycles per mm (70\\% accuracy). The findings suggest that tactile stimulation coded with grating patterns could be potentially used to present interpretable graph for the visually impaired. Such an approach could offer a value to research in human-computer interaction and sensory adaptation.", "AI": {"tldr": "\u672c\u7814\u7a76\u53d1\u73b0\uff0c\u76f2\u4eba\u5728\u901a\u8fc7\u667a\u80fd\u624b\u673a\u89e6\u6478\u5c4f\u89e3\u8bfb\u89e6\u89c9\u683c\u7eb9\u65f6\u8868\u73b0\u4f18\u8d8a\uff0c\u8868\u660e\u53ef\u4ee5\u5229\u7528\u8fd9\u79cd\u89e6\u89c9\u523a\u6fc0\u6765\u5e2e\u52a9\u76f2\u4eba\u7406\u89e3\u56fe\u5f62\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u89e6\u89c9\u53cd\u9988\u5e2e\u52a9\u76f2\u4eba\u7406\u89e3\u56fe\u5f62\u4fe1\u606f\u3002", "method": "\u901a\u8fc7\u4e24\u9879\u5b9e\u9a8c\u6bd4\u8f83\u89c6\u89c9\u969c\u788d\u8005\u548c\u6b63\u5e38\u89c6\u529b\u8005\u5728\u667a\u80fd\u624b\u673a\u89e6\u6478\u5c4f\u4e0a\u8bc6\u522b\u89e6\u89c9\u683c\u7eb9\u7684\u80fd\u529b\u3002", "result": "\u89c6\u89c9\u969c\u788d\u8005\u5728\u89e6\u89c9\u654f\u611f\u5ea6\u4e0a\u4f18\u4e8e\u89c6\u529b\u6b63\u5e38\u8005\uff0c\u80fd\u591f\u66f4\u597d\u5730\u8bc6\u522b\u89e6\u89c9\u683c\u7eb9\u3002", "conclusion": "\u89e6\u89c9\u683c\u7eb9\u523a\u6fc0\u6709\u6f5c\u529b\u4e3a\u89c6\u89c9\u969c\u788d\u8005\u63d0\u4f9b\u53ef\u89e3\u8bfb\u7684\u56fe\u5f62\u5448\u73b0\u65b9\u5f0f\uff0c\u63a8\u52a8\u4eba\u673a\u4ea4\u4e92\u548c\u611f\u5b98\u9002\u5e94\u7814\u7a76\u3002"}}
{"id": "2511.10079", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10079", "abs": "https://arxiv.org/abs/2511.10079", "authors": ["Yizheng Wang", "Timon Rabczuk", "Yinghua Liu"], "title": "Physics-informed Machine Learning for Static Friction Modeling in Robotic Manipulators Based on Kolmogorov-Arnold Networks", "comment": null, "summary": "Friction modeling plays a crucial role in achieving high-precision motion control in robotic operating systems. Traditional static friction models (such as the Stribeck model) are widely used due to their simple forms; however, they typically require predefined functional assumptions, which poses significant challenges when dealing with unknown functional structures. To address this issue, this paper proposes a physics-inspired machine learning approach based on the Kolmogorov Arnold Network (KAN) for static friction modeling of robotic joints. The method integrates spline activation functions with a symbolic regression mechanism, enabling model simplification and physical expression extraction through pruning and attribute scoring, while maintaining both high prediction accuracy and interpretability. We first validate the method's capability to accurately identify key parameters under known functional models, and further demonstrate its robustness and generalization ability under conditions with unknown functional structures and noisy data. Experiments conducted on both synthetic data and real friction data collected from a six-degree-of-freedom industrial manipulator show that the proposed method achieves a coefficient of determination greater than 0.95 across various tasks and successfully extracts concise and physically meaningful friction expressions. This study provides a new perspective for interpretable and data-driven robotic friction modeling with promising engineering applicability.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7528\u4e8e\u673a\u5668\u4eba\u9759\u6001\u6469\u64e6\u5efa\u6a21\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5728\u591a\u79cd\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u826f\u597d\u7684\u9c81\u68d2\u6027\u4e0e\u63a8\u5e7f\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u9759\u6001\u6469\u64e6\u6a21\u578b\u5728\u5904\u7406\u672a\u77e5\u529f\u80fd\u7ed3\u6784\u65f6\u9762\u4e34\u663e\u8457\u6311\u6218\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u65b0\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u5347\u6469\u64e6\u5efa\u6a21\u7684\u7cbe\u5ea6\u4e0e\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u7ed3\u5408\u6837\u6761\u6fc0\u6d3b\u51fd\u6570\u4e0e\u7b26\u53f7\u56de\u5f52\u673a\u5236\uff0c\u8fdb\u884c\u5efa\u6a21\u7b80\u5316\u548c\u7269\u7406\u8868\u8fbe\u63d0\u53d6\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728\u5bf9\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u6469\u64e6\u6570\u636e\u8fdb\u884c\u5b9e\u9a8c\u4e2d\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u5927\u4e8e0.95\u7684\u51b3\u5b9a\u7cfb\u6570\uff0c\u5e76\u6210\u529f\u63d0\u53d6\u51fa\u7b80\u6d01\u4e14\u6709\u610f\u4e49\u7684\u6469\u64e6\u8868\u8fbe\u5f0f\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKolmogorov Arnold\u7f51\u7edc\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u5173\u8282\u7684\u9759\u6001\u6469\u64e6\u5efa\u6a21\uff0c\u5177\u6709\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u9002\u7528\u4e8e\u672a\u77e5\u529f\u80fd\u7ed3\u6784\u7684\u60c5\u51b5\uff0c\u5177\u6709\u5de5\u7a0b\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2511.10032", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.10032", "abs": "https://arxiv.org/abs/2511.10032", "authors": ["Vijay Keswani", "Cyrus Cousins", "Breanna Nguyen", "Vincent Conitzer", "Hoda Heidari", "Jana Schaich Borg", "Walter Sinnott-Armstrong"], "title": "Moral Change or Noise? On Problems of Aligning AI With Temporally Unstable Human Feedback", "comment": "To appear in the AAAI 2026 Alignment Track", "summary": "Alignment methods in moral domains seek to elicit moral preferences of human stakeholders and incorporate them into AI. This presupposes moral preferences as static targets, but such preferences often evolve over time. Proper alignment of AI to dynamic human preferences should ideally account for \"legitimate\" changes to moral reasoning, while ignoring changes related to attention deficits, cognitive biases, or other arbitrary factors. However, common AI alignment approaches largely neglect temporal changes in preferences, posing serious challenges to proper alignment, especially in high-stakes applications of AI, e.g., in healthcare domains, where misalignment can jeopardize the trustworthiness of the system and yield serious individual and societal harms. This work investigates the extent to which people's moral preferences change over time, and the impact of such changes on AI alignment. Our study is grounded in the kidney allocation domain, where we elicit responses to pairwise comparisons of hypothetical kidney transplant patients from over 400 participants across 3-5 sessions. We find that, on average, participants change their response to the same scenario presented at different times around 6-20% of the time (exhibiting \"response instability\"). Additionally, we observe significant shifts in several participants' retrofitted decision-making models over time (capturing \"model instability\"). The predictive performance of simple AI models decreases as a function of both response and model instability. Moreover, predictive performance diminishes over time, highlighting the importance of accounting for temporal changes in preferences during training. These findings raise fundamental normative and technical challenges relevant to AI alignment, highlighting the need to better understand the object of alignment (what to align to) when user preferences change significantly over time.", "AI": {"tldr": "\u9053\u5fb7\u504f\u597d\u4f1a\u968f\u65f6\u95f4\u53d8\u5316\uff0c\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u53d8\u5316\u7684\u8ba4\u8bc6\u4f1a\u5bf9AI\u5bf9\u9f50\u5e26\u6765\u6311\u6218\uff0c\u5c24\u5176\u5728\u533b\u7597\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u3002", "motivation": "\u968f\u7740\u9053\u5fb7\u504f\u597d\u7684\u53d8\u5316\uff0c\u786e\u4fddAI\u7cfb\u7edf\u7684\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\uff0c\u4ee5\u63d0\u5347\u5176\u53ef\u4fe1\u5ea6\uff0c\u5c24\u5176\u5728\u533b\u7597\u7b49\u91cd\u8981\u9886\u57df\u3002", "method": "\u901a\u8fc7\u5728\u80be\u810f\u5206\u914d\u9886\u57df\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6536\u96c6400\u591a\u540d\u53c2\u4e0e\u8005\u7684\u53cd\u5e94\uff0c\u5206\u6790\u5176\u9053\u5fb7\u504f\u597d\u7684\u53d8\u5316\u53ca\u5176\u5bf9AI\u6a21\u578b\u9884\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4eba\u7c7b\u9053\u5fb7\u504f\u597d\u7684\u52a8\u6001\u53d8\u5316\u53ca\u5176\u5bf9AI\u5bf9\u9f50\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\uff0c\u8fd9\u79cd\u53d8\u5316\u53ef\u80fd\u5bfc\u81f4\u7cfb\u7edf\u7684\u4e0d\u53ef\u4fe1\u4efb\u548c\u4e25\u91cd\u7684\u793e\u4f1a\u5371\u5bb3\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u8868\u660e\uff0cAI\u5728\u9762\u5bf9\u4eba\u7c7b\u9053\u5fb7\u504f\u597d\u53d8\u5316\u65f6\uff0c\u9700\u8981\u8003\u8651\u8fd9\u4e9b\u53d8\u5316\u7684\u6027\u8d28\u548c\u65f6\u95f4\u56e0\u7d20\uff0c\u4ee5\u6539\u5584AI\u5bf9\u9f50\u3002"}}
{"id": "2511.10087", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10087", "abs": "https://arxiv.org/abs/2511.10087", "authors": ["Haidong Huang", "Haiyue Zhu. Jiayu Song", "Xixin Zhao", "Yaohua Zhou", "Jiayi Zhang", "Yuze Zhai", "Xiaocong Li"], "title": "Opinion: Towards Unified Expressive Policy Optimization for Robust Robot Learning", "comment": "Accepted by NeurIPS 2025 Workshop on Embodied World Models for Decision Making", "summary": "Offline-to-online reinforcement learning (O2O-RL) has emerged as a promising paradigm for safe and efficient robotic policy deployment but suffers from two fundamental challenges: limited coverage of multimodal behaviors and distributional shifts during online adaptation. We propose UEPO, a unified generative framework inspired by large language model pretraining and fine-tuning strategies. Our contributions are threefold: (1) a multi-seed dynamics-aware diffusion policy that efficiently captures diverse modalities without training multiple models; (2) a dynamic divergence regularization mechanism that enforces physically meaningful policy diversity; and (3) a diffusion-based data augmentation module that enhances dynamics model generalization. On the D4RL benchmark, UEPO achieves +5.9\\% absolute improvement over Uni-O4 on locomotion tasks and +12.4\\% on dexterous manipulation, demonstrating strong generalization and scalability.", "AI": {"tldr": "UEPO\u662f\u4e00\u79cd\u65b0\u7684\u751f\u6210\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3O2O-RL\u4e2d\u7684\u591a\u6a21\u6001\u8868\u73b0\u4e0d\u8db3\u548c\u5728\u7ebf\u9002\u5e94\u95ee\u9898\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "O2O-RL\u9762\u4e34\u7684\u4e24\u4e2a\u6311\u6218\uff1a\u591a\u6a21\u6001\u884c\u4e3a\u7684\u8986\u76d6\u9762\u6709\u9650\u548c\u5728\u7ebf\u9002\u5e94\u8fc7\u7a0b\u4e2d\u7684\u5206\u5e03\u53d8\u5316\u3002", "method": "\u63d0\u51faUEPO\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u751f\u6210\u6846\u67b6\uff0c\u501f\u9274\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u7b56\u7565\u3002", "result": "\u5728D4RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUEPO\u5728\u8fd0\u52a8\u4efb\u52a1\u4e0a\u6bd4Uni-O4\u63d0\u9ad8\u4e865.9%\u7684\u7edd\u5bf9\u6027\u80fd\uff0c\u5728\u7075\u5de7\u64cd\u4f5c\u4e0a\u63d0\u9ad8\u4e8612.4%\u3002", "conclusion": "UEPO\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u5b89\u5168\u6709\u6548\u7684\u673a\u5668\u4eba\u653f\u7b56\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2511.10408", "categories": ["cs.HC", "cs.CY", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.10408", "abs": "https://arxiv.org/abs/2511.10408", "authors": ["Sahibzada Farhan Amin", "Sana Athar", "Anja Feldmann", "Ha Dao", "Mannat Kaur"], "title": "Navigating the Ethics of Internet Measurement: Researchers' Perspectives from a Case Study in the EU", "comment": null, "summary": "Internet measurement research is essential for understanding, improving, and securing Internet infrastructure. However, its methods often involve large-scale data collection and user observation, raising complex ethical questions. While recent research has identified ethical challenges in Internet measurement research and laid out best practices, little is known about how researchers actually make ethical decisions in their research practice. To understand how these practices take shape day-to-day from the perspective of Internet measurement researchers, we interviewed 16 researchers from an Internet measurement research group in the EU. Through thematic analysis, we find that researchers deal with five main ethical challenges: privacy and consent issues, the possibility of unintended harm, balancing transparency with security and accountability, uncertain ethical boundaries, and hurdles in the ethics review process. Researchers address these by lab testing, rate limiting, setting up clear communication channels, and relying heavily on mentors and colleagues for guidance. Researchers express that ethical requirements vary across institutions, jurisdictions and conferences, and ethics review boards often lack the technical knowledge to evaluate Internet measurement research. We also highlight the invisible labor of Internet measurement researchers and describe their ethics practices as craft knowledge, both of which are crucial in upholding responsible research practices in the Internet measurement community.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u4e92\u8054\u7f51\u6d4b\u91cf\u7814\u7a76\u4e2d\u7814\u7a76\u4eba\u5458\u5982\u4f55\u9762\u5bf9\u4f26\u7406\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5e94\u7684\u5e94\u5bf9\u7b56\u7565\u3002", "motivation": "\u4e86\u89e3\u4e92\u8054\u7f51\u6d4b\u91cf\u7814\u7a76\u4e2d\u6d89\u53ca\u7684\u4f26\u7406\u51b3\u7b56\uff0c\u4ee5\u4fdd\u969c\u7814\u7a76\u7684\u4f26\u7406\u6027\u548c\u6709\u6548\u6027\u3002", "method": "\u901a\u8fc7\u5bf9\u6b27\u76df\u4e00\u4e2a\u4e92\u8054\u7f51\u6d4b\u91cf\u7814\u7a76\u5c0f\u7ec4\u768416\u4f4d\u7814\u7a76\u4eba\u5458\u8fdb\u884c\u8bbf\u8c08\u5e76\u8fdb\u884c\u4e3b\u9898\u5206\u6790\u3002", "result": "\u901a\u8fc7\u5bf916\u4f4d\u7814\u7a76\u4eba\u5458\u7684\u8bbf\u8c08\uff0c\u5171\u540c\u8bc6\u522b\u51fa\u4e94\u4e2a\u4e3b\u8981\u7684\u4f26\u7406\u6311\u6218\uff0c\u5e76\u8ba8\u8bba\u4e86\u4ed6\u4eec\u7684\u5e94\u5bf9\u7b56\u7565\u3002", "conclusion": "\u4f26\u7406\u8981\u6c42\u5728\u4e0d\u540c\u673a\u6784\u548c\u5730\u533a\u5b58\u5728\u5dee\u5f02\uff0c\u4f26\u7406\u5ba1\u67e5\u59d4\u5458\u4f1a\u7684\u6280\u672f\u77e5\u8bc6\u4e0d\u8db3\u4ee5\u8bc4\u4f30\u4e92\u8054\u7f51\u6d4b\u91cf\u7814\u7a76\u7684\u590d\u6742\u6027\u3002"}}
{"id": "2511.10110", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.10110", "abs": "https://arxiv.org/abs/2511.10110", "authors": ["Kamil Dreczkowski", "Pietro Vitiello", "Vitalis Vosylius", "Edward Johns"], "title": "Learning a Thousand Tasks in a Day", "comment": "This is the author's version of the work. It is posted here by permission of the AAAS for personal use, not for redistribution. The definitive version was published in Science Robotics on 12 November 2025, DOI: https://www.science.org/doi/10.1126/scirobotics.adv7594. Link to project website: https://www.robot-learning.uk/learning-1000-tasks", "summary": "Humans are remarkably efficient at learning tasks from demonstrations, but today's imitation learning methods for robot manipulation often require hundreds or thousands of demonstrations per task. We investigate two fundamental priors for improving learning efficiency: decomposing manipulation trajectories into sequential alignment and interaction phases, and retrieval-based generalisation. Through 3,450 real-world rollouts, we systematically study this decomposition. We compare different design choices for the alignment and interaction phases, and examine generalisation and scaling trends relative to today's dominant paradigm of behavioural cloning with a single-phase monolithic policy. In the few-demonstrations-per-task regime (<10 demonstrations), decomposition achieves an order of magnitude improvement in data efficiency over single-phase learning, with retrieval consistently outperforming behavioural cloning for both alignment and interaction. Building on these insights, we develop Multi-Task Trajectory Transfer (MT3), an imitation learning method based on decomposition and retrieval. MT3 learns everyday manipulation tasks from as little as a single demonstration each, whilst also generalising to novel object instances. This efficiency enables us to teach a robot 1,000 distinct everyday tasks in under 24 hours of human demonstrator time. Through 2,200 additional real-world rollouts, we reveal MT3's capabilities and limitations across different task families. Videos of our experiments can be found on at https://www.robot-learning.uk/learning-1000-tasks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51faMT3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u7684\u6548\u7387\uff0c\u80fd\u5728<10\u6b21\u793a\u8303\u4e0b\u5b66\u4e60\u548c\u6cdb\u5316\u81f3\u65b0\u4efb\u52a1\u3002", "motivation": "\u63d0\u9ad8\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u7684\u6548\u7387\uff0c\u51cf\u5c11\u5bf9\u793a\u8303\u6570\u91cf\u7684\u4f9d\u8d56\u3002", "method": "\u7814\u7a76\u5206\u89e3\u64cd\u63a7\u8f68\u8ff9\u4e3a\u5bf9\u9f50\u548c\u4ea4\u4e92\u9636\u6bb5\uff0c\u7ed3\u5408\u68c0\u7d22\u65b9\u6cd5\u8fdb\u884c\u5b66\u4e60\u3002", "result": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5MT3\uff0c\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u548c\u68c0\u7d22\u6539\u5584\u673a\u5668\u4eba\u64cd\u63a7\u5b66\u4e60\u7684\u6548\u7387\u3002", "conclusion": "MT3\u5728\u6a21\u4eff\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u4ee5\u9ad8\u6548\u7684\u65b9\u5f0f\u5b66\u4e60\u548c\u6cdb\u5316\u591a\u6837\u5316\u7684\u65e5\u5e38\u4efb\u52a1\u3002"}}
{"id": "2511.10467", "categories": ["cs.HC", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.10467", "abs": "https://arxiv.org/abs/2511.10467", "authors": ["Maharshi Pathak", "SungKu Kang", "Vanessa C. Whittem", "Katherine Bassett", "Michael B. Kane", "David J. Fannon"], "title": "Motivations and Actions of Human-Building Interactions from Environmental Momentary Assessments", "comment": null, "summary": "The expansion of renewable electricity generation, growing demands due to electrification, greater prevalence of working from home, and increasing frequency and severity of extreme weather events, will place new demands on the electric supply and distribution grid. Broader adoption of demand response programs (DRPs) for the residential sector may help meet these challenges; however, experience shows that occupant overrides in DRPs compromises their effectiveness. There is a lack of formal understanding of how discomfort, routines, and other motivations affect DRP overrides and other related human building interactions (HBI). This paper reports preliminary findings from a study of 20 households in Colorado and Massachusetts, US over three months. Participants responded to ecological momentary assessments (EMA) triggered by thermostat interactions and at random times throughout the day. EMAs included Likert-scale questions of thermal preference, preference intensity, and changes to 7 different activity types that could affect thermal comfort, and an opened ended question about motivations of such actions. Twelve tags were developed to categorize motivation responses and analyzed statistically to identify associations between motivations, preferences, and HBI actions. Reactions to changes in the thermal environment were the most frequently observed motivation, 118 of 220 responses. On the other hand, 47% responses were at least partially motivated by non-thermal factors, suggesting limited utility for occupant behavior models founded solely on thermal comfort. Changes in activity level and clothing were less likely to be reported when EMAs were triggered by thermostat interactions, while fan interactions were more likely. Windows, shades, and portable heater interactions had no significant dependence on how the EMA was triggered.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f4f\u5b85\u9700\u6c42\u54cd\u5e94\u8ba1\u5212\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u4f4f\u6237\u7684\u884c\u4e3a\u53d7\u5230\u70ed\u73af\u5883\u53d8\u5316\u53ca\u975e\u70ed\u56e0\u7d20\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u4f4f\u5b85\u90e8\u95e8\u9700\u6c42\u54cd\u5e94\u8ba1\u5212\uff08DRPs\uff09\u7684\u6709\u6548\u6027\u53ca\u5176\u53d7\u5230\u4f4f\u6237\u884c\u4e3a\u5f71\u54cd\u7684\u673a\u5236\uff0c\u4ee5\u9002\u5e94\u53ef\u518d\u751f\u80fd\u6e90\u548c\u6c14\u5019\u53d8\u5316\u5e26\u6765\u7684\u65b0\u6311\u6218\u3002", "method": "\u91c7\u7528\u751f\u6001\u77ac\u65f6\u8bc4\u4f30\u6cd5\uff08EMA\uff09\u6536\u96c6\u4f4f\u6237\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u5bf9\u70ed\u73af\u5883\u53d8\u5316\u7684\u5373\u65f6\u53cd\u5e94\u53ca\u52a8\u673a\uff0c\u901a\u8fc7\u7edf\u8ba1\u5206\u6790\u8bc6\u522b\u52a8\u673a\u4e0e\u504f\u597d\u548c\u4f4f\u6237\u4ea4\u4e92\u4e4b\u95f4\u7684\u5173\u8054\u3002", "result": "\u901a\u8fc7\u5bf920\u4e2a\u5bb6\u5ead\u8fdb\u884c\u7684\u4e09\u4e2a\u6708\u89c2\u5bdf\uff0c\u5206\u6790\u4e86\u4f4f\u6237\u5728\u4e0d\u540c\u60c5\u51b5\u4e0b\u5bf9\u70ed\u73af\u5883\u7684\u53cd\u5e94\u53ca\u5176\u52a8\u673a\uff0c\u53d1\u73b0\u975e\u70ed\u56e0\u7d20\u5728\u4f4f\u6237\u884c\u4e3a\u4e2d\u5360\u91cd\u8981\u5730\u4f4d\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u5728\u9700\u6c42\u54cd\u5e94\u8ba1\u5212\u4e2d\u6574\u5408\u975e\u70ed\u56e0\u7d20\u7684\u91cd\u8981\u6027\uff0c\u547c\u5401\u5bf9\u73b0\u6709\u4f4f\u6237\u884c\u4e3a\u6a21\u578b\u8fdb\u884c\u91cd\u65b0\u8bc4\u4f30\u3002"}}
{"id": "2511.10276", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10276", "abs": "https://arxiv.org/abs/2511.10276", "authors": ["Konstantin Soshin", "Alexander Krapukhin", "Andrei Spiridonov", "Denis Shepelev", "Gregorii Bukhtuev", "Andrey Kuznetsov", "Vlad Shakhuro"], "title": "RoboBenchMart: Benchmarking Robots in Retail Environment", "comment": null, "summary": "Most existing robotic manipulation benchmarks focus on simplified tabletop scenarios, typically involving a stationary robotic arm interacting with various objects on a flat surface. To address this limitation, we introduce RoboBenchMart, a more challenging and realistic benchmark designed for dark store environments, where robots must perform complex manipulation tasks with diverse grocery items. This setting presents significant challenges, including dense object clutter and varied spatial configurations -- with items positioned at different heights, depths, and in close proximity. By targeting the retail domain, our benchmark addresses a setting with strong potential for near-term automation impact. We demonstrate that current state-of-the-art generalist models struggle to solve even common retail tasks. To support further research, we release the RoboBenchMart suite, which includes a procedural store layout generator, a trajectory generation pipeline, evaluation tools and fine-tuned baseline models.", "AI": {"tldr": "RoboBenchMart\u662f\u4e00\u4e2a\u65b0\u63d0\u51fa\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u57fa\u51c6\uff0c\u65e8\u5728\u5e94\u5bf9\u590d\u6742\u7684\u96f6\u552e\u73af\u5883\uff0c\u76ee\u524d\u7684\u901a\u7528\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u63d0\u4f9b\u4e86\u7814\u7a76\u6240\u9700\u7684\u5de5\u5177\u548c\u8d44\u6e90\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u64cd\u4f5c\u57fa\u51c6\u4e3b\u8981\u96c6\u4e2d\u5728\u7b80\u5316\u7684\u684c\u9762\u573a\u666f\uff0c\u7f3a\u4e4f\u5bf9\u590d\u6742\u96f6\u552e\u73af\u5883\u7684\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u66f4\u5177\u73b0\u5b9e\u610f\u4e49\u7684\u57fa\u51c6\u3002", "method": "\u901a\u8fc7\u521b\u5efaRoboBenchMart\u57fa\u51c6\uff0c\u63d0\u4f9b\u591a\u6837\u7684\u6742\u8d27\u5546\u54c1\u548c\u590d\u6742\u7684\u64cd\u4f5c\u4efb\u52a1\uff0c\u4ee5\u9002\u5e94\u73b0\u5b9e\u7684\u9ed1\u5e97\u73af\u5883\u3002", "result": "RoboBenchMart\u5c55\u793a\u4e86\u5f53\u524d\u901a\u7528\u6a21\u578b\u5728\u89e3\u51b3\u5e38\u89c1\u96f6\u552e\u4efb\u52a1\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u53d1\u5e03\u4e86\u5305\u62ec\u5546\u5e97\u5e03\u5c40\u751f\u6210\u5668\u3001\u8f68\u8ff9\u751f\u6210\u7ba1\u9053\u3001\u8bc4\u4f30\u5de5\u5177\u548c\u57fa\u7840\u6a21\u578b\u5728\u5185\u7684\u5b8c\u6574\u5957\u4ef6\u3002", "conclusion": "RoboBenchMart\u662f\u4e00\u4e2a\u9488\u5bf9\u590d\u6742\u96f6\u552e\u73af\u5883\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u57fa\u51c6\uff0c\u5c55\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u96f6\u552e\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\uff0c\u5e76\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\u7684\u8fdb\u884c\u3002"}}
{"id": "2511.10532", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.10532", "abs": "https://arxiv.org/abs/2511.10532", "authors": ["Jose Berengueres"], "title": "Preview, Accept or Discard? A Predictive Low-Motion Interaction Paradigm", "comment": null, "summary": "Repetitive strain injury (RSI) affects roughly one in five computer users and remains largely unresolved despite decades of ergonomic mouse redesign. All such devices share a fundamental limitation: they still require fine-motor motion to operate. This work investigates whether predictive, AI-assisted input can reduce that motion by replacing physical pointing with ranked on-screen suggestions. To preserve user agency, we introduce Preview Accept Discard (PAD), a zero-click interaction paradigm that lets users preview predicted GUI targets, cycle through a small set of ranked alternatives, and accept or discard them via key-release timing. We evaluate PAD in two settings: a browser-based email client and a ISO 9241-9 keyboard-prediction task under varying top-3 accuracies. Across both studies, PAD substantially reduces hand motion relative to trackpad use while maintaining comparable task times with the trackpad only when accuracies are similar to those of the best spell-checkers.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u9884\u6d4b\u8f93\u5165\u65b9\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u51cf\u5c11\u7cbe\u7ec6\u8fd0\u52a8\u6765\u89e3\u51b3\u91cd\u590d\u6027\u52b3\u635f\uff08RSI\uff09\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u7ecf\u8fc7\u591a\u5e74\u7684\u4eba\u4f53\u5de5\u7a0b\u5b66\u9f20\u6807\u8bbe\u8ba1\uff0cRSI\u95ee\u9898\u4ecd\u672a\u5f97\u5230\u89e3\u51b3\uff0c\u56e0\u6b64\u63a2\u8ba8\u662f\u5426\u901a\u8fc7\u9884\u6d4b\u8f93\u5165\u6765\u51cf\u8f7b\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u96f6\u70b9\u51fb\u4e92\u52a8\u8303\u5f0f\uff08PAD\uff09\uff0c\u5141\u8bb8\u7528\u6237\u9884\u89c8\u9884\u6d4b\u7684GUI\u76ee\u6807\uff0c\u901a\u8fc7\u65f6\u95f4\u91ca\u653e\u952e\u6765\u9009\u62e9\u6216\u4e22\u5f03\u8fd9\u4e9b\u76ee\u6807\u3002", "result": "\u5728\u6d4f\u89c8\u5668\u90ae\u7bb1\u5ba2\u6237\u7aef\u548cISO 9241-9\u952e\u76d8\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0cPAD\u76f8\u8f83\u4e8e\u6807\u51c6\u89e6\u63a7\u677f\u4f7f\u7528\u663e\u8457\u51cf\u5c11\u4e86\u624b\u90e8\u8fd0\u52a8\u3002", "conclusion": "PAD\u663e\u8457\u51cf\u5c11\u4e86\u624b\u90e8\u8fd0\u52a8\uff0c\u5e76\u5728\u7c7b\u4f3c\u6700\u4f73\u62fc\u5199\u68c0\u67e5\u5668\u7684\u51c6\u786e\u5ea6\u4e0b\u4fdd\u6301\u4e86\u4e0e\u89e6\u63a7\u677f\u76f8\u4f3c\u7684\u4efb\u52a1\u65f6\u95f4\u3002"}}
{"id": "2511.10403", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10403", "abs": "https://arxiv.org/abs/2511.10403", "authors": ["Mingxing Peng", "Ruoyu Yao", "Xusen Guo", "Jun Ma"], "title": "nuPlan-R: A Closed-Loop Planning Benchmark for Autonomous Driving via Reactive Multi-Agent Simulation", "comment": "8 pages, 3 figures", "summary": "Recent advances in closed-loop planning benchmarks have significantly improved the evaluation of autonomous vehicles. However, existing benchmarks still rely on rule-based reactive agents such as the Intelligent Driver Model (IDM), which lack behavioral diversity and fail to capture realistic human interactions, leading to oversimplified traffic dynamics. To address these limitations, we present nuPlan-R, a new reactive closed-loop planning benchmark that integrates learning-based reactive multi-agent simulation into the nuPlan framework. Our benchmark replaces the rule-based IDM agents with noise-decoupled diffusion-based reactive agents and introduces an interaction-aware agent selection mechanism to ensure both realism and computational efficiency. Furthermore, we extend the benchmark with two additional metrics to enable a more comprehensive assessment of planning performance. Extensive experiments demonstrate that our reactive agent model produces more realistic, diverse, and human-like traffic behaviors, leading to a benchmark environment that better reflects real-world interactive driving. We further reimplement a collection of rule-based, learning-based, and hybrid planning approaches within our nuPlan-R benchmark, providing a clearer reflection of planner performance in complex interactive scenarios and better highlighting the advantages of learning-based planners in handling complex and dynamic scenarios. These results establish nuPlan-R as a new standard for fair, reactive, and realistic closed-loop planning evaluation. We will open-source the code for the new benchmark.", "AI": {"tldr": "nuPlan-R\u662f\u4e00\u4e2a\u65b0\u578b\u7684\u95ed\u73af\u89c4\u5212\u57fa\u51c6\uff0c\u901a\u8fc7\u5f15\u5165\u5b66\u4e60\u57fa\u7840\u7684\u53cd\u5e94\u667a\u80fd\u4f53\uff0c\u63d0\u4f9b\u66f4\u771f\u5b9e\u7684\u4ea4\u901a\u884c\u4e3a\u8bc4\u4f30\uff0c\u6210\u4e3a\u65b0\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "motivation": "\u5f53\u524d\u57fa\u51c6\u4f9d\u8d56\u4e8e\u7f3a\u4e4f\u884c\u4e3a\u591a\u6837\u6027\u7684\u89c4\u5219\u57fa\u7840\u53cd\u5e94\u6a21\u578b\uff0c\u4e0d\u80fd\u51c6\u786e\u53cd\u6620\u771f\u5b9e\u4ea4\u901a\u52a8\u6001\u3002", "method": "\u63d0\u51fanuPlan-R\u57fa\u51c6\uff0c\u96c6\u6210\u5b66\u4e60\u57fa\u7840\u7684\u591a\u667a\u80fd\u4f53\u53cd\u5e94\u6a21\u62df\uff0c\u66ff\u6362\u4f20\u7edf\u7684\u89c4\u5219\u57fa\u7840\u53cd\u5e94\u6a21\u578b\uff0c\u6539\u5584\u4e86\u4e92\u52a8\u673a\u5236\u548c\u8bc4\u4f30\u6307\u6807\u3002", "result": "nuPlan-R\u63d0\u4f9b\u66f4\u771f\u5b9e\u3001\u591a\u6837\u5316\u7684\u4eba\u7c7b\u9a7e\u9a76\u884c\u4e3a\uff0c\u6539\u8fdb\u89c4\u5212\u6027\u80fd\u8bc4\u4f30\uff0c\u5e76\u5c55\u793a\u5b66\u4e60\u57fa\u7840\u89c4\u5212\u8005\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u4f18\u52bf\u3002", "conclusion": "nuPlan-R\u88ab\u786e\u7acb\u4e3a\u516c\u5e73\u3001\u5b9e\u65f6\u4e14\u771f\u5b9e\u7684\u95ed\u73af\u89c4\u5212\u8bc4\u4f30\u65b0\u6807\u51c6\u3002"}}
{"id": "2511.10544", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.10544", "abs": "https://arxiv.org/abs/2511.10544", "authors": ["Maximilian Eder", "Clemens Lechner", "Maurice Jakesch"], "title": "Bytes of a Feather: Personality and Opinion Alignment Effects in Human-AI Interaction", "comment": null, "summary": "Interactions with AI assistants are increasingly personalized to individual users. As AI personalization is dynamic and machine-learning-driven, we have limited understanding of how personalization affects interaction outcomes and user perceptions. We conducted a large-scale controlled experiment in which 1,000 participants interacted with AI assistants that took on certain personality traits and opinion stances. Our results show that participants consistently preferred to interact with models that shared their opinions. Participants also found opinion-aligned models more trustworthy, competent, warm, and persuasive, corroborating an AI-similarity-attraction hypothesis. In contrast, we observed no or only weak effects of AI personality alignment, with introvert models rated as less trustworthy and competent by introvert participants. These findings highlight opinion alignment as a central dimension of AI personalization and user preference, while underscoring the need for a more grounded discussion of the limits and risks of personalized AI.", "AI": {"tldr": "\u901a\u8fc7\u5bf91000\u540d\u53c2\u4e0e\u8005\u7684\u5927\u89c4\u6a21\u5b9e\u9a8c\uff0c\u6211\u4eec\u53d1\u73b0\u7528\u6237\u66f4\u503e\u5411\u4e8e\u4e0e\u5177\u6709\u76f8\u4f3c\u610f\u89c1\u7684AI\u52a9\u624b\u4e92\u52a8\uff0c\u5e76\u8ba4\u4e3a\u8fd9\u4e9b\u52a9\u624b\u66f4\u503c\u5f97\u4fe1\u8d56\u548c\u6709\u6548\u3002", "motivation": "to understand how AI personalization, driven by machine learning, affects user interactions and perceptions", "method": "conducting a large-scale controlled experiment with 1,000 participants interacting with AI assistants", "result": "participants preferred to interact with AI that shared their opinions and found them more trustworthy and competent; weak effects of AI personality alignment were observed", "conclusion": "\u610f\u89c1\u4e00\u81f4\u6027\u662fAI\u4e2a\u6027\u5316\u548c\u7528\u6237\u504f\u597d\u7684\u6838\u5fc3\u7ef4\u5ea6\uff0c\u540c\u65f6\u9700\u8981\u6df1\u5165\u8ba8\u8bba\u4e2a\u6027\u5316AI\u7684\u5c40\u9650\u6027\u548c\u98ce\u9669\u3002"}}
{"id": "2511.10411", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.10411", "abs": "https://arxiv.org/abs/2511.10411", "authors": ["Benjamin Stoler", "Jonathan Francis", "Jean Oh"], "title": "LongComp: Long-Tail Compositional Zero-Shot Generalization for Robust Trajectory Prediction", "comment": "8 pages, 3 figures", "summary": "Methods for trajectory prediction in Autonomous Driving must contend with rare, safety-critical scenarios that make reliance on real-world data collection alone infeasible. To assess robustness under such conditions, we propose new long-tail evaluation settings that repartition datasets to create challenging out-of-distribution (OOD) test sets. We first introduce a safety-informed scenario factorization framework, which disentangles scenarios into discrete ego and social contexts. Building on analogies to compositional zero-shot image-labeling in Computer Vision, we then hold out novel context combinations to construct challenging closed-world and open-world settings. This process induces OOD performance gaps in future motion prediction of 5.0% and 14.7% in closed-world and open-world settings, respectively, relative to in-distribution performance for a state-of-the-art baseline. To improve generalization, we extend task-modular gating networks to operate within trajectory prediction models, and develop an auxiliary, difficulty-prediction head to refine internal representations. Our strategies jointly reduce the OOD performance gaps to 2.8% and 11.5% in the two settings, respectively, while still improving in-distribution performance.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u65b0\u7684\u957f\u5c3e\u8bc4\u4f30\u8bbe\u7f6e\u548c\u6539\u8fdb\u7684\u6a21\u578b\u67b6\u6784\uff0c\u4ee5\u589e\u5f3a\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8f68\u8ff9\u9884\u6d4b\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u7a00\u6709\u548c\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u3002", "motivation": "\u9488\u5bf9\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u7a00\u6709\u5b89\u5168\u5173\u952e\u573a\u666f\uff0c\u63d0\u51fa\u957f\u5c3e\u8bc4\u4f30\u8bbe\u7f6e\u4ee5\u63d0\u5347\u6a21\u578b\u5728\u8fd9\u4e9b\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u5b89\u5168\u611f\u77e5\u573a\u666f\u5206\u89e3\u6846\u67b6\u548c\u4efb\u52a1\u6a21\u5757\u5316\u95e8\u63a7\u7f51\u7edc", "result": "\u5728\u95ed\u4e16\u754c\u548c\u5f00\u4e16\u754c\u8bbe\u7f6e\u4e0b\uff0c\u76f8\u6bd4\u4e8e\u57fa\u7ebf\uff0c\u672a\u6765\u8fd0\u52a8\u9884\u6d4b\u7684OOD\u6027\u80fd\u5dee\u8ddd\u5206\u522b\u4e3a5.0%\u548c14.7%\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6210\u529f\u51cf\u5c11\u4e86OOD\u6027\u80fd\u5dee\u8ddd\uff0c\u5e76\u63d0\u9ad8\u4e86\u5728\u5206\u5e03\u5185\u8868\u73b0\u3002"}}
{"id": "2511.10448", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.10448", "abs": "https://arxiv.org/abs/2511.10448", "authors": ["Lorenzo Pagliara", "Violeta Redondo", "Enrico Ferrentino", "Manuel Ferre", "Pasquale Chiacchio"], "title": "Improving dependability in robotized bolting operations", "comment": "10 pages, 9 figures", "summary": "Bolting operations are critical in industrial assembly and in the maintenance of scientific facilities, requiring high precision and robustness to faults. Although robotic solutions have the potential to improve operational safety and effectiveness, current systems still lack reliable autonomy and fault management capabilities. To address this gap, we propose a control framework for dependable robotized bolting tasks and instantiate it on a specific robotic system. The system features a control architecture ensuring accurate driving torque control and active compliance throughout the entire operation, enabling safe interaction even under fault conditions. By designing a multimodal human-robot interface (HRI) providing real-time visualization of relevant system information and supporting seamless transitions between automatic and manual control, we improve operator situation awareness and fault detection capabilities. A high-level supervisor (SV) coordinates the execution and manages transitions between control modes, ensuring consistency with the supervisory control (SVC) paradigm, while preserving the human operator's authority. The system is validated in a representative bolting operation involving pipe flange joining, under several fault conditions. The results demonstrate improved fault detection capabilities, enhanced operator situational awareness, and accurate and compliant execution of the bolting operation. However, they also reveal the limitations of relying on a single camera to achieve full situational awareness.", "AI": {"tldr": "\u7814\u7a76\u4e86\u673a\u5668\u4eba\u87ba\u6813\u64cd\u4f5c\u7684\u63a7\u5236\u6846\u67b6\uff0c\u63d0\u5347\u4e86\u6545\u969c\u68c0\u6d4b\u80fd\u529b\u548c\u64cd\u4f5c\u8005\u610f\u8bc6\uff0c\u663e\u793a\u51fa\u5355\u4e00\u89c6\u89c9\u7cfb\u7edf\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u673a\u5668\u4eba\u89e3\u51b3\u65b9\u6848\u5728\u5de5\u4e1a\u88c5\u914d\u548c\u79d1\u5b66\u8bbe\u65bd\u7ef4\u62a4\u4e2d\u7684\u9700\u6c42\u9ad8\uff0c\u4f46\u73b0\u6709\u7cfb\u7edf\u7f3a\u4e4f\u53ef\u9760\u7684\u81ea\u4e3b\u6027\u548c\u6545\u969c\u7ba1\u7406\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u591a\u6a21\u6001\u4eba\u673a\u4ea4\u4e92\u754c\u9762\u7684\u63a7\u5236\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u9ad8\u7b49\u7ea7\u76d1\u7763\u534f\u8c03\u6267\u884c\u548c\u63a7\u5236\u6a21\u5f0f\u8f6c\u6362\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u673a\u5668\u4eba\u87ba\u6813\u64cd\u4f5c\u7684\u63a7\u5236\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u9ad8\u64cd\u4f5c\u7684\u7cbe\u51c6\u5ea6\u548c\u5bb9\u9519\u80fd\u529b\u3002\u8be5\u7cfb\u7edf\u901a\u8fc7\u51c6\u786e\u63a7\u5236\u9a71\u52a8\u626d\u77e9\u548c\u6d3b\u52a8\u9075\u4ece\u6027\u6765\u589e\u5f3a\u5b89\u5168\u6027\uff0c\u5e76\u8bbe\u8ba1\u4e86\u591a\u6a21\u6001\u4eba\u673a\u63a5\u53e3\u4ee5\u6539\u5584\u64cd\u4f5c\u8005\u7684\u60c5\u51b5\u610f\u8bc6\u4e0e\u6545\u969c\u68c0\u6d4b\u80fd\u529b\u3002\u7cfb\u7edf\u5728\u7ba1\u6cd5\u5170\u8fde\u63a5\u7684\u5b9e\u9645\u64cd\u4f5c\u4e2d\u9a8c\u8bc1\uff0c\u7ed3\u679c\u663e\u793a\u51fa\u663e\u8457\u7684\u6539\u5584\uff0c\u4f46\u4e5f\u53d1\u73b0\u4e86\u5355\u4e00\u6444\u50cf\u5934\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u7cfb\u7edf\u5728\u4ee3\u8868\u6027\u7684\u87ba\u6813\u64cd\u4f5c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5e76\u6539\u5584\u4e86\u6545\u969c\u68c0\u6d4b\u548c\u64cd\u4f5c\u8005\u7684\u60c5\u51b5\u610f\u8bc6\uff0c\u4f46\u5355\u4e00\u6444\u50cf\u5934\u4e0d\u8db3\u4ee5\u5b9e\u73b0\u5168\u9762\u7684\u60c5\u51b5\u610f\u8bc6\u3002"}}
{"id": "2511.10580", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.10580", "abs": "https://arxiv.org/abs/2511.10580", "authors": ["Tianhui Han", "Shashwat Singh", "Sarvesh Patil", "Zeynep Temel"], "title": "From Fold to Function: Dynamic Modeling and Simulation-Driven Design of Origami Mechanisms", "comment": "8 Pages, 9 Figures, Submitted to IEEE RoboSoft", "summary": "Origami-inspired mechanisms can transform flat sheets into functional three-dimensional dynamic structures that are lightweight, compact, and capable of complex motion. These properties make origami increasingly valuable in robotic and deployable systems. However, accurately simulating their folding behavior and interactions with the environment remains challenging. To address this, we present a design framework for origami mechanism simulation that utilizes MuJoCo's deformable-body capabilities. In our approach, origami sheets are represented as graphs of interconnected deformable elements with user-specified constraints such as creases and actuation, defined through an intuitive graphical user interface (GUI). This framework allows users to generate physically consistent simulations that capture both the geometric structure of origami mechanisms and their interactions with external objects and surfaces. We demonstrate our method's utility through a case study on an origami catapult, where design parameters are optimized in simulation using the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) and validated experimentally on physical prototypes. The optimized structure achieves improved throwing performance, illustrating how our system enables rapid, simulation-driven origami design, optimization, and analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6298\u7eb8\u673a\u5236\u4eff\u771f\u8bbe\u8ba1\u6846\u67b6\uff0c\u6539\u5584\u4e86\u6298\u7eb8\u8bbe\u8ba1\u7684\u4f18\u5316\u6d41\u7a0b\uff0c\u63d0\u5347\u4e86\u529f\u80fd\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u4eff\u771f\u5de5\u5177\u5728\u6298\u7eb8\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u5c3d\u7ba1\u6298\u7eb8\u5728\u673a\u5668\u4eba\u548c\u53ef\u5c55\u5f00\u7cfb\u7edf\u4e2d\u5177\u6709\u6f5c\u5728\u4ef7\u503c\uff0c\u4f46\u51c6\u786e\u6a21\u62df\u5176\u6298\u53e0\u884c\u4e3a\u548c\u4e0e\u73af\u5883\u7684\u4ea4\u4e92\u4ecd\u5b58\u5728\u6311\u6218\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528MuJoCo\u53d8\u5f62\u4f53\u80fd\u529b\u7684\u6298\u7eb8\u673a\u5236\u4eff\u771f\u8bbe\u8ba1\u6846\u67b6\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u56fe\u5f62\u7528\u6237\u754c\u9762\u5b9a\u4e49\u7ea6\u675f\u3002", "result": "\u6211\u4eec\u7684\u6848\u4f8b\u7814\u7a76\u663e\u793a\uff0c\u901a\u8fc7\u4f18\u5316\u6298\u7eb8\u5f39\u5c04\u5668\u7684\u8bbe\u8ba1\u53c2\u6570\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6295\u63b7\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u4eff\u771f\u7ed3\u679c\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u5feb\u901f\u3001\u57fa\u4e8e\u4eff\u771f\u7684\u6298\u7eb8\u8bbe\u8ba1\u3001\u4f18\u5316\u548c\u5206\u6790\uff0c\u6210\u529f\u63d0\u5347\u4e86\u6298\u7eb8\u673a\u5236\u7684\u6027\u80fd\u3002"}}
{"id": "2511.10598", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.10598", "abs": "https://arxiv.org/abs/2511.10598", "authors": ["Raghav Adhikari", "Sachet Khatiwada", "Suman Poudel"], "title": "Optimizing the flight path for a scouting Uncrewed Aerial Vehicle", "comment": "This paper was prepared as an end of semester project for ME8710: Engineering Optimization, Clemson University. Consists of 7 pages and 8 figures", "summary": "Post-disaster situations pose unique navigation challenges. One of those challenges is the unstructured nature of the environment, which makes it hard to layout paths for rescue vehicles. We propose the use of Uncrewed Aerial Vehicle (UAV) in such scenario to perform reconnaissance across the environment. To accomplish this, we propose an optimization-based approach to plan a path for the UAV at optimal height where the sensors of the UAV can cover the most area and collect data with minimum uncertainty.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u5728\u540e\u707e\u96be\u60c5\u5883\u4e0b\uff0c\u4f7f\u7528\u65e0\u4eba\u9a7e\u9a76\u98de\u884c\u5668\u4f18\u5316\u8def\u5f84\u89c4\u5212\uff0c\u4ee5\u63d0\u9ad8\u73af\u5883\u4fa6\u5bdf\u6548\u7387\u3002", "motivation": "\u540e\u707e\u96be\u60c5\u5883\u4e0b\uff0c\u73af\u5883\u7684\u975e\u7ed3\u6784\u5316\u7279\u70b9\u7ed9\u6551\u63f4\u8f66\u8f86\u7684\u5bfc\u822a\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "\u4f7f\u7528\u4f18\u5316\u65b9\u6cd5\u4e3a\u65e0\u4eba\u673a\u89c4\u5212\u8def\u5f84\uff0c\u4ee5\u786e\u4fdd\u4f20\u611f\u5668\u8986\u76d6\u6700\u5927\u533a\u57df\u5e76\u6700\u5c0f\u5316\u6570\u636e\u6536\u96c6\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u63d0\u51fa\u4f7f\u7528\u65e0\u4eba\u9a7e\u9a76\u98de\u884c\u5668(UAV)\u8fdb\u884c\u73af\u5883\u4fa6\u5bdf\uff0c\u5e76\u7528\u4f18\u5316\u65b9\u6cd5\u89c4\u5212\u8def\u5f84\uff0c\u4ee5\u6700\u5c0f\u7684\u4e0d\u786e\u5b9a\u6027\u8986\u76d6\u6700\u5927\u533a\u57df\u3002", "conclusion": "\u65e0\u4eba\u673a\u5728\u707e\u540e\u73af\u5883\u4fa6\u5bdf\u4e2d\u5b9e\u73b0\u4e86\u6700\u4f18\u9ad8\u5ea6\u8def\u5f84\u89c4\u5212\uff0c\u53ef\u4ee5\u63d0\u9ad8\u6570\u636e\u91c7\u96c6\u7684\u8986\u76d6\u7387\u4e0e\u51c6\u786e\u6027\u3002"}}
{"id": "2511.10635", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10635", "abs": "https://arxiv.org/abs/2511.10635", "authors": ["Pascal Strauch", "David M\u00fcller", "Sammy Christen", "Agon Serifi", "Ruben Grandia", "Espen Knoop", "Moritz B\u00e4cher"], "title": "Robot Crash Course: Learning Soft and Stylized Falling", "comment": null, "summary": "Despite recent advances in robust locomotion, bipedal robots operating in the real world remain at risk of falling. While most research focuses on preventing such events, we instead concentrate on the phenomenon of falling itself. Specifically, we aim to reduce physical damage to the robot while providing users with control over a robot's end pose. To this end, we propose a robot agnostic reward function that balances the achievement of a desired end pose with impact minimization and the protection of critical robot parts during reinforcement learning. To make the policy robust to a broad range of initial falling conditions and to enable the specification of an arbitrary and unseen end pose at inference time, we introduce a simulation-based sampling strategy of initial and end poses. Through simulated and real-world experiments, our work demonstrates that even bipedal robots can perform controlled, soft falls.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5956\u52b1\u673a\u5236\uff0c\u4f7f\u53cc\u8db3\u673a\u5668\u4eba\u5728\u6454\u5012\u65f6\u80fd\u591f\u5b9e\u73b0\u53d7\u63a7\u548c\u67d4\u548c\u7684\u7740\u9646\uff0c\u964d\u4f4e\u7269\u7406\u635f\u5bb3\u3002", "motivation": "\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\uff0c\u53cc\u8db3\u673a\u5668\u4eba\u9762\u4e34\u6454\u5012\u7684\u98ce\u9669\uff0c\u5c3d\u7ba1\u5728\u7a33\u5065\u8fd0\u52a8\u65b9\u9762\u6709\u4e86\u8fdb\u5c55\u3002\u5927\u591a\u6570\u7814\u7a76\u96c6\u4e2d\u5728\u9884\u9632\u6454\u5012\u4e8b\u4ef6\u4e0a\uff0c\u4f46\u6211\u4eec\u8f6c\u800c\u5173\u6ce8\u6454\u5012\u672c\u8eab\u3002", "method": "\u91c7\u7528\u4e00\u79cd\u57fa\u4e8e\u6a21\u62df\u7684\u91c7\u6837\u7b56\u7565\uff0c\u9488\u5bf9\u521d\u59cb\u548c\u6700\u7ec8\u59ff\u6001\u8fdb\u884c\u91c7\u6837\uff0c\u4ee5\u63d0\u9ad8\u653f\u7b56\u7684\u9c81\u68d2\u6027\u5e76\u652f\u6301\u5728\u63a8\u65ad\u65f6\u6307\u5b9a\u4efb\u610f\u672a\u89c1\u7684\u7ec8\u6001\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0e\u673a\u5668\u4eba\u65e0\u5173\u7684\u5956\u52b1\u51fd\u6570\uff0c\u7528\u4e8e\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u5e73\u8861\u5b9e\u73b0\u6240\u9700\u7ec8\u6001\u4e0e\u51cf\u5c11\u51b2\u51fb\u3001\u4fdd\u62a4\u5173\u952e\u673a\u5668\u4eba\u90e8\u4ef6\uff0c\u6210\u529f\u5b9e\u73b0\u63a7\u5236\u7684\u8f6f\u7740\u9646\u3002", "conclusion": "\u901a\u8fc7\u6a21\u62df\u548c\u73b0\u5b9e\u4e16\u754c\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u53cc\u8db3\u673a\u5668\u4eba\u5728\u4e0d\u540c\u6454\u5012\u6761\u4ef6\u4e0b\u53ef\u5b9e\u73b0\u53d7\u63a7\u3001\u67d4\u548c\u7684\u7740\u9646\u3002"}}
