{"id": "2510.08777", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.08777", "abs": "https://arxiv.org/abs/2510.08777", "authors": ["Zekun Wu", "Anna Maria Feit"], "title": "Understanding and Predicting Temporal Visual Attention Influenced by Dynamic Highlights in Monitoring Task", "comment": null, "summary": "Monitoring interfaces are crucial for dynamic, highstakes tasks where\neffective user attention is essential. Visual highlights can guide attention\neffectively but may also introduce unintended disruptions. To investigate this,\nwe examined how visual highlights affect users' gaze behavior in a drone\nmonitoring task, focusing on when, how long, and how much attention they draw.\nWe found that highlighted areas exhibit distinct temporal characteristics\ncompared to non-highlighted ones, quantified using normalized saliency (NS)\nmetrics. Highlights elicited immediate responses, with NS peaking quickly, but\nthis shift came at the cost of reduced search efforts elsewhere, potentially\nimpacting situational awareness. To predict these dynamic changes and support\ninterface design, we developed the Highlight-Informed Saliency Model (HISM),\nwhich provides granular predictions of NS over time. These predictions enable\nevaluations of highlight effectiveness and inform the optimal timing and\ndeployment of highlights in future monitoring interface designs, particularly\nfor time-sensitive tasks."}
{"id": "2510.08783", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08783", "abs": "https://arxiv.org/abs/2510.08783", "authors": ["Reuben A. Luera", "Ryan Rossi", "Franck Dernoncourt", "Samyadeep Basu", "Sungchul Kim", "Subhojyoti Mukherjee", "Puneet Mathur", "Ruiyi Zhang", "Jihyung Kil", "Nedim Lipka", "Seunghyun Yoon", "Jiuxiang Gu", "Zichao Wang", "Cindy Xiong Bearfield", "Branislav Kveton"], "title": "MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces", "comment": null, "summary": "In an ideal design pipeline, user interface (UI) design is intertwined with\nuser research to validate decisions, yet studies are often resource-constrained\nduring early exploration. Recent advances in multimodal large language models\n(MLLMs) offer a promising opportunity to act as early evaluators, helping\ndesigners narrow options before formal testing. Unlike prior work that\nemphasizes user behavior in narrow domains such as e-commerce with metrics like\nclicks or conversions, we focus on subjective user evaluations across varied\ninterfaces. We investigate whether MLLMs can mimic human preferences when\nevaluating individual UIs and comparing them. Using data from a crowdsourcing\nplatform, we benchmark GPT-4o, Claude, and Llama across 30 interfaces and\nexamine alignment with human judgments on multiple UI factors. Our results show\nthat MLLMs approximate human preferences on some dimensions but diverge on\nothers, underscoring both their potential and limitations in supplementing\nearly UX research."}
{"id": "2510.08888", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.08888", "abs": "https://arxiv.org/abs/2510.08888", "authors": ["Yashodip Dharmendra Jagtap", "Aaditya Ganesh Bagul"], "title": "Green Grid: Smart Tech Meets E-Waste", "comment": "16 pages, 5 figures, 2 tables, includes system architecture diagrams\n  and performance analysis", "summary": "Electronic waste (e-waste) is a rapidly growing global problem caused by\nshorter device lifecycles and rising consumption. India ranks third globally in\ne-waste generation, producing over 1.7 million tonnes in 2023-24, of which less\nthan half is formally processed. To address this, we propose Green Grid, an\nintegrated AI-powered e-waste management platform combining IoT-enabled smart\ncollection, AI-based device classification, blockchain-based traceability, and\ngamified citizen engagement. The system features smart recycling bins with\nsensors for real-time monitoring, deep learning models for device\nidentification and sorting, a blockchain ledger for tamper-proof tracking, and\na reward-based mobile or web app to encourage user participation. Additionally,\nGreen Grid offers analytics dashboards and an eco-marketplace to support\npolicymakers and recyclers. By bridging technology, sustainability, and\ncommunity participation, the platform enhances transparency, increases formal\nrecycling rates, and advances India's transition toward a circular economy."}
{"id": "2510.08912", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.08912", "abs": "https://arxiv.org/abs/2510.08912", "authors": ["Jijie Zhou", "Yuhan Hu"], "title": "Beyond Words: Infusing Conversational Agents with Human-like Typing Behaviors", "comment": "Author's version of a paper published at CUI '24 (ACM Conversational\n  User Interfaces 2024)", "summary": "Recently, large language models have facilitated the emergence of highly\nintelligent conversational AI capable of engaging in human-like dialogues.\nHowever, a notable distinction lies in the fact that these AI models\npredominantly generate responses rapidly, often producing extensive content\nwithout emulating the thoughtful process characteristic of human cognition and\ntyping. This paper presents a design aimed at simulating human-like typing\nbehaviors, including patterns such as hesitation and self-editing, as well as a\npreliminary user experiment to understand whether and to what extent the agent\nwith human-like typing behaviors could potentially affect conversational\nengagement and its trustworthiness. We've constructed an interactive platform\nfeaturing user-adjustable parameters, allowing users to personalize the AI's\ncommunication style and thus cultivate a more enriching and immersive\nconversational experience. Our user experiment, involving interactions with\nthree types of agents - a baseline agent, one simulating hesitation, and\nanother integrating both hesitation and self-editing behaviors - reveals a\npreference for the agent that incorporates both behaviors, suggesting an\nimprovement in perceived naturalness and trustworthiness. Through the insights\nfrom our design process and both quantitative and qualitative feedback from\nuser experiments, this paper contributes to the multimodal interaction design\nand user experience for conversational AI, advocating for a more human-like,\nengaging, and trustworthy communication paradigm."}
{"id": "2510.08705", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08705", "abs": "https://arxiv.org/abs/2510.08705", "authors": ["Noah Steinkrüger", "Nisarga Nilavadi", "Wolfram Burgard", "Tanja Katharina Kaiser"], "title": "ConPoSe: LLM-Guided Contact Point Selection for Scalable Cooperative Object Pushing", "comment": null, "summary": "Object transportation in cluttered environments is a fundamental task in\nvarious domains, including domestic service and warehouse logistics. In\ncooperative object transport, multiple robots must coordinate to move objects\nthat are too large for a single robot. One transport strategy is pushing, which\nonly requires simple robots. However, careful selection of robot-object contact\npoints is necessary to push the object along a preplanned path. Although this\nselection can be solved analytically, the solution space grows combinatorially\nwith the number of robots and object size, limiting scalability. Inspired by\nhow humans rely on common-sense reasoning for cooperative transport, we propose\ncombining the reasoning capabilities of Large Language Models with local search\nto select suitable contact points. Our LLM-guided local search method for\ncontact point selection, ConPoSe, successfully selects contact points for a\nvariety of shapes, including cuboids, cylinders, and T-shapes. We demonstrate\nthat ConPoSe scales better with the number of robots and object size than the\nanalytical approach, and also outperforms pure LLM-based selection."}
{"id": "2510.08917", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.08917", "abs": "https://arxiv.org/abs/2510.08917", "authors": ["Brandon Lit", "Edward Crowder", "Daniel Vogel", "Hassan Khan"], "title": "\"I know it's not right, but that's what it said to do\": Investigating Trust in AI Chatbots for Cybersecurity Policy", "comment": null, "summary": "AI chatbots are an emerging security attack vector, vulnerable to threats\nsuch as prompt injection, and rogue chatbot creation. When deployed in domains\nsuch as corporate security policy, they could be weaponized to deliver guidance\nthat intentionally undermines system defenses. We investigate whether users can\nbe tricked by a compromised AI chatbot in this scenario. A controlled study\n(N=15) asked participants to use a chatbot to complete security-related tasks.\nWithout their knowledge, the chatbot was manipulated to give incorrect advice\nfor some tasks. The results show how trust in AI chatbots is related to task\nfamiliarity, and confidence in their ownn judgment. Additionally, we discuss\npossible reasons why people do or do not trust AI chatbots in different\nscenarios."}
{"id": "2510.08753", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.08753", "abs": "https://arxiv.org/abs/2510.08753", "authors": ["A. Wang", "C. Jiang", "M. Przystupa", "J. Valentine", "M. Jagersand"], "title": "Point and Go: Intuitive Reference Frame Reallocation in Mode Switching for Assistive Robotics", "comment": "7 Pages, 5 figures", "summary": "Operating high degree of freedom robots can be difficult for users of\nwheelchair mounted robotic manipulators. Mode switching in Cartesian space has\nseveral drawbacks such as unintuitive control reference frames, separate\ntranslation and orientation control, and limited movement capabilities that\nhinder performance. We propose Point and Go mode switching, which reallocates\nthe Cartesian mode switching reference frames into a more intuitive action\nspace comprised of new translation and rotation modes. We use a novel sweeping\nmotion to point the gripper, which defines the new translation axis along the\nrobot base frame's horizontal plane. This creates an intuitive `point and go'\ntranslation mode that allows the user to easily perform complex, human-like\nmovements without switching control modes. The system's rotation mode combines\nposition control with a refined end-effector oriented frame that provides\nprecise and consistent robot actions in various end-effector poses. We verified\nits effectiveness through initial experiments, followed by a three-task user\nstudy that compared our method to Cartesian mode switching and a state of the\nart learning method. Results show that Point and Go mode switching reduced\ncompletion times by 31\\%, pauses by 41\\%, and mode switches by 33\\%, while\nreceiving significantly favorable responses in user surveys."}
{"id": "2510.08930", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08930", "abs": "https://arxiv.org/abs/2510.08930", "authors": ["Ruixuan Sun", "Junyuan Wang", "Sanjali Roy", "Joseph A. Konstan"], "title": "Co-Authoring the Self: A Human-AI Interface for Interest Reflection in Recommenders", "comment": null, "summary": "Natural language-based user profiles in recommender systems have been\nexplored for their interpretability and potential to help users scrutinize and\nrefine their interests, thereby improving recommendation quality. Building on\nthis foundation, we introduce a human-AI collaborative profile for a movie\nrecommender system that presents editable personalized interest summaries of a\nuser's movie history. Unlike static profiles, this design invites users to\ndirectly inspect, modify, and reflect on the system's inferences. In an\neight-week online field deployment with 1775 active movie recommender users, we\nfind persistent gaps between user-perceived and system-inferred interests, show\nhow the profile encourages engagement and reflection, and identify design\ndirections for leveraging imperfect AI-powered user profiles to stimulate more\nuser intervention and build more transparent and trustworthy recommender\nexperiences."}
{"id": "2510.08754", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.08754", "abs": "https://arxiv.org/abs/2510.08754", "authors": ["David Nguyen", "Zulfiqar Zaidi", "Kevin Karol", "Jessica Hodgins", "Zhaoming Xie"], "title": "Whole Body Model Predictive Control for Spin-Aware Quadrupedal Table Tennis", "comment": "Submitted to appear in IEEE ICRA 2026", "summary": "Developing table tennis robots that mirror human speed, accuracy, and ability\nto predict and respond to the full range of ball spins remains a significant\nchallenge for legged robots. To demonstrate these capabilities we present a\nsystem to play dynamic table tennis for quadrupedal robots that integrates high\nspeed perception, trajectory prediction, and agile control. Our system uses\nexternal cameras for high-speed ball localization, physical models with learned\nresiduals to infer spin and predict trajectories, and a novel model predictive\ncontrol (MPC) formulation for agile full-body control. Notably, a continuous\nset of stroke strategies emerge automatically from different ball return\nobjectives using this control paradigm. We demonstrate our system in the real\nworld on a Spot quadruped, evaluate accuracy of each system component, and\nexhibit coordination through the system's ability to aim and return balls with\nvarying spin types. As a further demonstration, the system is able to rally\nwith human players."}
{"id": "2510.08991", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.08991", "abs": "https://arxiv.org/abs/2510.08991", "authors": ["Lucy Jiang", "Lotus Zhang", "Leah Findlater"], "title": "Creation, Critique, and Consumption: Exploring Generative AI Descriptions for Supporting Blind and Low Vision Professionals with Visual Tasks", "comment": "ASSETS 2025 Workshop Submission (AT @ Work: Intelligent Assistive\n  Technologies for Enabling Workplace Inclusion)", "summary": "Many blind and low vision (BLV) people are excluded from professional roles\nthat may involve visual tasks due to access barriers and persisting stigmas.\nAdvancing generative AI systems can support BLV people through providing\ncontextual and personalized visual descriptions for creation, critique, and\nconsumption. In this workshop paper, we provide design suggestions for how\nvisual descriptions can be better contextualized for multiple professional\ntasks. We conclude by discussing how these designs can improve autonomy,\ninclusion, and skill development over time."}
{"id": "2510.08787", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.08787", "abs": "https://arxiv.org/abs/2510.08787", "authors": ["Yiming Li", "Nael Darwiche", "Amirreza Razmjoo", "Sichao Liu", "Yilun Du", "Auke Ijspeert", "Sylvain Calinon"], "title": "Geometry-aware Policy Imitation", "comment": "21 pages, 13 figures. In submission", "summary": "We propose a Geometry-aware Policy Imitation (GPI) approach that rethinks\nimitation learning by treating demonstrations as geometric curves rather than\ncollections of state-action samples. From these curves, GPI derives distance\nfields that give rise to two complementary control primitives: a progression\nflow that advances along expert trajectories and an attraction flow that\ncorrects deviations. Their combination defines a controllable, non-parametric\nvector field that directly guides robot behavior. This formulation decouples\nmetric learning from policy synthesis, enabling modular adaptation across\nlow-dimensional robot states and high-dimensional perceptual inputs. GPI\nnaturally supports multimodality by preserving distinct demonstrations as\nseparate models and allows efficient composition of new demonstrations through\nsimple additions to the distance field. We evaluate GPI in simulation and on\nreal robots across diverse tasks. Experiments show that GPI achieves higher\nsuccess rates than diffusion-based policies while running 20 times faster,\nrequiring less memory, and remaining robust to perturbations. These results\nestablish GPI as an efficient, interpretable, and scalable alternative to\ngenerative approaches for robotic imitation learning. Project website:\nhttps://yimingli1998.github.io/projects/GPI/"}
{"id": "2510.09009", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.09009", "abs": "https://arxiv.org/abs/2510.09009", "authors": ["Leijie Wang", "Kathryn Yurechko", "Amy X. Zhang"], "title": "Promptimizer: User-Led Prompt Optimization for Personal Content Classification", "comment": "under review", "summary": "While LLMs now enable users to create content classifiers easily through\nnatural language, automatic prompt optimization techniques are often necessary\nto create performant classifiers. However, such techniques can fail to consider\nhow social media users want to evolve their filters over the course of usage,\nincluding desiring to steer them in different ways during initialization and\niteration. We introduce a user-centered prompt optimization technique,\nPromptimizer, that maintains high performance and ease-of-use but additionally\n(1) allows for user input into the optimization process and (2) produces final\nprompts that are interpretable. A lab experiment (n=16) found that users\nsignificantly preferred Promptimizer's human-in-the-loop optimization over a\nfully automatic approach. We further implement Promptimizer into Puffin, a tool\nto support YouTube content creators in creating and maintaining personal\nclassifiers to manage their comments. Over a 3-week deployment with 10\ncreators, participants successfully created diverse filters to better\nunderstand their audiences and protect their communities."}
{"id": "2510.08807", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08807", "abs": "https://arxiv.org/abs/2510.08807", "authors": ["Zhenyu Zhao", "Hongyi Jing", "Xiawei Liu", "Jiageng Mao", "Abha Jha", "Hanwen Yang", "Rong Xue", "Sergey Zakharor", "Vitor Guizilini", "Yue Wang"], "title": "Humanoid Everyday: A Comprehensive Robotic Dataset for Open-World Humanoid Manipulation", "comment": null, "summary": "From loco-motion to dextrous manipulation, humanoid robots have made\nremarkable strides in demonstrating complex full-body capabilities. However,\nthe majority of current robot learning datasets and benchmarks mainly focus on\nstationary robot arms, and the few existing humanoid datasets are either\nconfined to fixed environments or limited in task diversity, often lacking\nhuman-humanoid interaction and lower-body locomotion. Moreover, there are a few\nstandardized evaluation platforms for benchmarking learning-based policies on\nhumanoid data. In this work, we present Humanoid Everyday, a large-scale and\ndiverse humanoid manipulation dataset characterized by extensive task variety\ninvolving dextrous object manipulation, human-humanoid interaction,\nlocomotion-integrated actions, and more. Leveraging a highly efficient\nhuman-supervised teleoperation pipeline, Humanoid Everyday aggregates\nhigh-quality multimodal sensory data, including RGB, depth, LiDAR, and tactile\ninputs, together with natural language annotations, comprising 10.3k\ntrajectories and over 3 million frames of data across 260 tasks across 7 broad\ncategories. In addition, we conduct an analysis of representative policy\nlearning methods on our dataset, providing insights into their strengths and\nlimitations across different task categories. For standardized evaluation, we\nintroduce a cloud-based evaluation platform that allows researchers to\nseamlessly deploy their policies in our controlled setting and receive\nperformance feedback. By releasing Humanoid Everyday along with our policy\nlearning analysis and a standardized cloud-based evaluation platform, we intend\nto advance research in general-purpose humanoid manipulation and lay the\ngroundwork for more capable and embodied robotic agents in real-world\nscenarios. Our dataset, data collection code, and cloud evaluation website are\nmade publicly available on our project website."}
{"id": "2510.09242", "categories": ["cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09242", "abs": "https://arxiv.org/abs/2510.09242", "authors": ["Marco Siino", "Giuseppe Bonomo", "Rosario Sorbello", "Ilenia Tinnirello"], "title": "Investigating the Impact of Rational Dilated Wavelet Transform on Motor Imagery EEG Decoding with Deep Learning Models", "comment": null, "summary": "The present study investigates the impact of the Rational Discrete Wavelet\nTransform (RDWT), used as a plug-in preprocessing step for motor imagery\nelectroencephalographic (EEG) decoding prior to applying deep learning\nclassifiers. A systematic paired evaluation (with/without RDWT) is conducted on\nfour state-of-the-art deep learning architectures: EEGNet, ShallowConvNet,\nMBEEG\\_SENet, and EEGTCNet. This evaluation was carried out across three\nbenchmark datasets: High Gamma, BCI-IV-2a, and BCI-IV-2b. The performance of\nthe RDWT is reported with subject-wise averages using accuracy and Cohen's\nkappa, complemented by subject-level analyses to identify when RDWT is\nbeneficial. On BCI-IV-2a, RDWT yields clear average gains for EEGTCNet (+4.44\npercentage points, pp; kappa +0.059) and MBEEG\\_SENet (+2.23 pp; +0.030), with\nsmaller improvements for EEGNet (+2.08 pp; +0.027) and ShallowConvNet (+0.58\npp; +0.008). On BCI-IV-2b, the enhancements observed are modest yet consistent\nfor EEGNet (+0.21 pp; +0.044) and EEGTCNet (+0.28 pp; +0.077). On HGD, average\neffects are modest to positive, with the most significant gain observed for\nMBEEG\\_SENet (+1.65 pp; +0.022), followed by EEGNet (+0.76 pp; +0.010) and\nEEGTCNet (+0.54 pp; +0.008). Inspection of the subject material reveals\nsignificant enhancements in challenging recordings (e.g., non-stationary\nsessions), indicating that RDWT can mitigate localized noise and enhance\nrhythm-specific information. In conclusion, RDWT is shown to be a low-overhead,\narchitecture-aware preprocessing technique that can yield tangible gains in\naccuracy and agreement for deep model families and challenging subjects."}
{"id": "2510.08811", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.08811", "abs": "https://arxiv.org/abs/2510.08811", "authors": ["Jiurun Song", "Xiao Liang", "Minghui Zheng"], "title": "Adaptive Motion Planning via Contact-Based Intent Inference for Human-Robot Collaboration", "comment": null, "summary": "Human-robot collaboration (HRC) requires robots to adapt their motions to\nhuman intent to ensure safe and efficient cooperation in shared spaces.\nAlthough large language models (LLMs) provide high-level reasoning for\ninferring human intent, their application to reliable motion planning in HRC\nremains challenging. Physical human-robot interaction (pHRI) is intuitive but\noften relies on continuous kinesthetic guidance, which imposes burdens on\noperators. To address these challenges, a contact-informed adaptive\nmotion-planning framework is introduced to infer human intent directly from\nphysical contact and employ the inferred intent for online motion correction in\nHRC. First, an optimization-based force estimation method is proposed to infer\nhuman-intended contact forces and locations from joint torque measurements and\na robot dynamics model, thereby reducing cost and installation complexity while\nenabling whole-body sensitivity. Then, a torque-based contact detection\nmechanism with link-level localization is introduced to reduce the optimization\nsearch space and to enable real-time estimation. Subsequently, a\ncontact-informed adaptive motion planner is developed to infer human intent\nfrom contacts and to replan robot motion online, while maintaining smoothness\nand adapting to human corrections. Finally, experiments on a 7-DOF manipulator\nare conducted to demonstrate the accuracy of the proposed force estimation\nmethod and the effectiveness of the contact-informed adaptive motion planner\nunder perception uncertainty in HRC."}
{"id": "2510.09492", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.09492", "abs": "https://arxiv.org/abs/2510.09492", "authors": ["Sam Lau", "Kianoosh Boroojeni", "Harry Keeling", "Jenn Marroquin"], "title": "Barriers that Programming Instructors Face While Performing Emergency Pedagogical Design to Shape Student-AI Interactions with Generative AI Tools", "comment": "22 pages, 6 figures", "summary": "Generative AI (GenAI) tools are increasingly pervasive, pushing instructors\nto redesign how students use GenAI tools in coursework. We conceptualize this\nwork as emergency pedagogical design: reactive, indirect efforts by instructors\nto shape student-AI interactions without control over commercial interfaces. To\nunderstand practices of lead users conducting emergency pedagogical design, we\nconducted interviews (n=13) and a survey (n=169) of computing instructors.\nThese instructors repeatedly encountered five barriers: fragmented buy-in for\nrevising courses; policy crosswinds from non-prescriptive institutional\nguidance; implementation challenges as instructors attempt interventions;\nassessment misfit as student-AI interactions are only partially visible to\ninstructors; and lack of resources, including time, staffing, and paid tool\naccess. We use these findings to present emergency pedagogical design as a\ndistinct design setting for HCI and outline recommendations for HCI\nresearchers, academic institutions, and organizations to effectively support\ninstructors in adapting courses to GenAI."}
{"id": "2510.08812", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08812", "abs": "https://arxiv.org/abs/2510.08812", "authors": ["Grace Ra Kim", "Hailey Warner", "Duncan Eddy", "Evan Astle", "Zachary Booth", "Edward Balaban", "Mykel J. Kochenderfer"], "title": "Adaptive Science Operations in Deep Space Missions Using Offline Belief State Planning", "comment": "7 pages, 4 tables, 5 figures, accepted in IEEE ISPARO 2026", "summary": "Deep space missions face extreme communication delays and environmental\nuncertainty that prevent real-time ground operations. To support autonomous\nscience operations in communication-constrained environments, we present a\npartially observable Markov decision process (POMDP) framework that adaptively\nsequences spacecraft science instruments. We integrate a Bayesian network into\nthe POMDP observation space to manage the high-dimensional and uncertain\nmeasurements typical of astrobiology missions. This network compactly encodes\ndependencies among measurements and improves the interpretability and\ncomputational tractability of science data. Instrument operation policies are\ncomputed offline, allowing resource-aware plans to be generated and thoroughly\nvalidated prior to launch. We use the Enceladus Orbilander's proposed Life\nDetection Suite (LDS) as a case study, demonstrating how Bayesian network\nstructure and reward shaping influence system performance. We compare our\nmethod against the mission's baseline Concept of Operations (ConOps),\nevaluating both misclassification rates and performance in off-nominal sample\naccumulation scenarios. Our approach reduces sample identification errors by\nnearly 40%"}
{"id": "2510.09502", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.09502", "abs": "https://arxiv.org/abs/2510.09502", "authors": ["Trevor DePodesta", "Johanna Beyer"], "title": "LibraryLens: An Interactive Tool for Exploring and Arranging Digital Bookshelves", "comment": null, "summary": "Existing digital book management platforms often fail to capture the rich\nspatial and visual cues inherent to physical bookshelves, hindering users'\nability to fully engage with their collections. We present LibraryLens, a novel\nvisualization tool that addresses these shortcomings by enabling users to\ncreate, explore, and interact with immersive, two-dimensional representations\nof their personal libraries. The tool also caters to the growing trend of\nsocial sharing within online book communities, allowing users to create\nvisually appealing representations of their libraries that can be easily shared\non social platforms. Despite limitations inherent to the metadata being\nrendered, formative evaluations suggest that LibraryLens has the potential to\nlower the barrier to entry for users seeking to optimize their book\norganization without the constraints of physical space or manual labor,\nultimately fostering deeper engagement with their personal libraries."}
{"id": "2510.08851", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.08851", "abs": "https://arxiv.org/abs/2510.08851", "authors": ["Le Mao", "Andrew H. Liu", "Renos Zabounidis", "Zachary Kingston", "Joseph Campbell"], "title": "CDE: Concept-Driven Exploration for Reinforcement Learning", "comment": "Preprint", "summary": "Intelligent exploration remains a critical challenge in reinforcement\nlearning (RL), especially in visual control tasks. Unlike low-dimensional\nstate-based RL, visual RL must extract task-relevant structure from raw pixels,\nmaking exploration inefficient. We propose Concept-Driven Exploration (CDE),\nwhich leverages a pre-trained vision-language model (VLM) to generate\nobject-centric visual concepts from textual task descriptions as weak,\npotentially noisy supervisory signals. Rather than directly conditioning on\nthese noisy signals, CDE trains a policy to reconstruct the concepts via an\nauxiliary objective, using reconstruction accuracy as an intrinsic reward to\nguide exploration toward task-relevant objects. Because the policy internalizes\nthese concepts, VLM queries are only needed during training, reducing\ndependence on external models during deployment. Across five challenging\nsimulated visual manipulation tasks, CDE achieves efficient, targeted\nexploration and remains robust to noisy VLM predictions. Finally, we\ndemonstrate real-world transfer by deploying CDE on a Franka Research 3 arm,\nattaining an 80\\% success rate in a real-world manipulation task."}
{"id": "2510.09516", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.09516", "abs": "https://arxiv.org/abs/2510.09516", "authors": ["Rafael A. Calvo", "Dorian Peters"], "title": "Convivial Conversational Agents -- shifting toward relationships", "comment": null, "summary": "Conversational AI (CAI) systems offer opportunities to scale service\nprovision to unprecedented levels and governments and corporations are already\nbeginning to deploy them across services. The economic argument is similar\nacross domains: use CAI to automate the time-consuming conversations required\nfor customer, client or patient support. Herein we draw on our work in dementia\ncare to explore some of the challenges and opportunities for CAI, and how a new\nway of conceptualising these systems could help ensure essential aspects for\nhuman thriving are not lost in the process of automation."}
{"id": "2510.08880", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.08880", "abs": "https://arxiv.org/abs/2510.08880", "authors": ["Baoshan Song", "Xiao Xia", "Penggao Yan", "Yihan Zhong", "Weisong Wen", "Li-Ta Hsu"], "title": "Online IMU-odometer Calibration using GNSS Measurements for Autonomous Ground Vehicle Localization", "comment": "Submitted to IEEE Transactions on Intelligent Transportation Systems", "summary": "Accurate calibration of intrinsic (odometer scaling factors) and extrinsic\nparameters (IMU-odometer translation and rotation) is essential for autonomous\nground vehicle localization. Existing GNSS-aided approaches often rely on\npositioning results or raw measurements without ambiguity resolution, and their\nobservability properties remain underexplored. This paper proposes a tightly\ncoupled online calibration method that fuses IMU, odometer, and raw GNSS\nmeasurements (pseudo-range, carrier-phase, and Doppler) within an extendable\nfactor graph optimization (FGO) framework, incorporating outlier mitigation and\nambiguity resolution. Observability analysis reveals that two horizontal\ntranslation and three rotation parameters are observable under general motion,\nwhile vertical translation remains unobservable. Simulation and real-world\nexperiments demonstrate superior calibration and localization performance over\nstate-of-the-art loosely coupled methods. Specifically, the IMU-odometer\npositioning using our calibrated parameters achieves the absolute maximum error\nof 17.75 m while the one of LC method is 61.51 m, achieving up to 71.14 percent\nimprovement. To foster further research, we also release the first open-source\ndataset that combines IMU, 2D odometer, and raw GNSS measurements from both\nrover and base stations."}
{"id": "2510.09554", "categories": ["cs.HC", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.09554", "abs": "https://arxiv.org/abs/2510.09554", "authors": ["Thomas C. Smits", "Nikolay Akhmetov", "Tiffany S. Liaw", "Mark S. Keller", "Eric Mörth", "Nils Gehlenborg"], "title": "scellop: A Scalable Redesign of Cell Population Plots for Single-Cell Data", "comment": "8 pages, 1 figure, supplemental (2 pages, 1 figure) at the end", "summary": "Summary: Cell population plots are visualizations showing cell population\ndistributions in biological samples with single-cell data, traditionally shown\nwith stacked bar charts. Here, we address issues with this approach,\nparticularly its limited scalability with increasing number of cell types and\nsamples, and present scellop, a novel interactive cell population viewer\ncombining visual encodings optimized for common user tasks in studying\npopulations of cells across samples or conditions.\n  Availability and Implementation: Scellop is available under the MIT licence\nat https://github.com/hms-dbmi/scellop, and is available on PyPI\n(https://pypi.org/project/cellpop/) and NPM\n(https://www.npmjs.com/package/cellpop). A demo is available at\nhttps://scellop.netlify.app/."}
{"id": "2510.08884", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.08884", "abs": "https://arxiv.org/abs/2510.08884", "authors": ["Alexandre Lopes", "Catarina Barata", "Plinio Moreno"], "title": "Model-Based Lookahead Reinforcement Learning for in-hand manipulation", "comment": null, "summary": "In-Hand Manipulation, as many other dexterous tasks, remains a difficult\nchallenge in robotics by combining complex dynamic systems with the capability\nto control and manoeuvre various objects using its actuators. This work\npresents the application of a previously developed hybrid Reinforcement\nLearning (RL) Framework to In-Hand Manipulation task, verifying that it is\ncapable of improving the performance of the task. The model combines concepts\nof both Model-Free and Model-Based Reinforcement Learning, by guiding a trained\npolicy with the help of a dynamic model and value-function through trajectory\nevaluation, as done in Model Predictive Control. This work evaluates the\nperformance of the model by comparing it with the policy that will be guided.\nTo fully explore this, various tests are performed using both fully-actuated\nand under-actuated simulated robotic hands to manipulate different objects for\na given task. The performance of the model will also be tested for\ngeneralization tests, by changing the properties of the objects in which both\nthe policy and dynamic model were trained, such as density and size, and\nadditionally by guiding a trained policy in a certain object to perform the\nsame task in a different one. The results of this work show that, given a\npolicy with high average reward and an accurate dynamic model, the hybrid\nframework improves the performance of in-hand manipulation tasks for most test\ncases, even when the object properties are changed. However, this improvement\ncomes at the expense of increasing the computational cost, due to the\ncomplexity of trajectory evaluation."}
{"id": "2510.09570", "categories": ["cs.HC", "cs.GR", "cs.NE", "cs.RO", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2510.09570", "abs": "https://arxiv.org/abs/2510.09570", "authors": ["Nishant Gautam", "Somya Sharma", "Peter Corcoran", "Kaspar Althoefer"], "title": "Differential Analysis of Pseudo Haptic Feedback: Novel Comparative Study of Visual and Auditory Cue Integration for Psychophysical Evaluation", "comment": "17 Pages, 9 Figures", "summary": "Pseudo-haptics exploit carefully crafted visual or auditory cues to trick the\nbrain into \"feeling\" forces that are never physically applied, offering a\nlow-cost alternative to traditional haptic hardware. Here, we present a\ncomparative psychophysical study that quantifies how visual and auditory\nstimuli combine to evoke pseudo-haptic pressure sensations on a commodity\ntablet. Using a Unity-based Rollball game, participants (n = 4) guided a\nvirtual ball across three textured terrains while their finger forces were\ncaptured in real time with a Robotous RFT40 force-torque sensor. Each terrain\nwas paired with a distinct rolling-sound profile spanning 440 Hz - 4.7 kHz, 440\nHz - 13.1 kHz, or 440 Hz - 8.9 kHz; crevice collisions triggered additional\n\"knocking\" bursts to heighten realism. Average tactile forces increased\nsystematically with cue intensity: 0.40 N, 0.79 N and 0.88 N for visual-only\ntrials and 0.41 N, 0.81 N and 0.90 N for audio-only trials on Terrains 1-3,\nrespectively. Higher audio frequencies and denser visual textures both elicited\nstronger muscle activation, and their combination further reduced the force\nneeded to perceive surface changes, confirming multisensory integration. These\nresults demonstrate that consumer-grade isometric devices can reliably induce\nand measure graded pseudo-haptic feedback without specialized actuators,\nopening a path toward affordable rehabilitation tools, training simulators and\nassistive interfaces."}
{"id": "2510.08953", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.08953", "abs": "https://arxiv.org/abs/2510.08953", "authors": ["Cheng Ouyang", "Moeen Ul Islam", "Dong Chen", "Kaixiang Zhang", "Zhaojian Li", "Xiaobo Tan"], "title": "Direct Data-Driven Predictive Control for a Three-dimensional Cable-Driven Soft Robotic Arm", "comment": null, "summary": "Soft robots offer significant advantages in safety and adaptability, yet\nachieving precise and dynamic control remains a major challenge due to their\ninherently complex and nonlinear dynamics. Recently, Data-enabled Predictive\nControl (DeePC) has emerged as a promising model-free approach that bypasses\nexplicit system identification by directly leveraging input-output data. While\nDeePC has shown success in other domains, its application to soft robots\nremains underexplored, particularly for three-dimensional (3D) soft robotic\nsystems. This paper addresses this gap by developing and experimentally\nvalidating an effective DeePC framework on a 3D, cable-driven soft arm.\nSpecifically, we design and fabricate a soft robotic arm with a thick tubing\nbackbone for stability, a dense silicone body with large cavities for strength\nand flexibility, and rigid endcaps for secure termination. Using this platform,\nwe implement DeePC with singular value decomposition (SVD)-based dimension\nreduction for two key control tasks: fixed-point regulation and trajectory\ntracking in 3D space. Comparative experiments with a baseline model-based\ncontroller demonstrate DeePC's superior accuracy, robustness, and adaptability,\nhighlighting its potential as a practical solution for dynamic control of soft\nrobots."}
{"id": "2510.09605", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.09605", "abs": "https://arxiv.org/abs/2510.09605", "authors": ["Adam Coscia", "Alex Endert"], "title": "VisPile: A Visual Analytics System for Analyzing Multiple Text Documents With Large Language Models and Knowledge Graphs", "comment": "Accepted to HICSS 2026. 10 pages, 4 figures. For a demo video, see\n  https://youtu.be/vY6SqkkNeMQ. For a live demo, visit\n  https://adamcoscia.com/papers/vispile/demo/. The source code is available at\n  https://github.com/AdamCoscia/VisPile", "summary": "Intelligence analysts perform sensemaking over collections of documents using\nvarious visual and analytic techniques to gain insights from large amounts of\ntext. As data scales grow, our work explores how to leverage two AI\ntechnologies, large language models (LLMs) and knowledge graphs (KGs), in a\nvisual text analysis tool, enhancing sensemaking and helping analysts keep\npace. Collaborating with intelligence community experts, we developed a visual\nanalytics system called VisPile. VisPile integrates an LLM and a KG into\nvarious UI functions that assist analysts in grouping documents into piles,\nperforming sensemaking tasks like summarization and relationship mapping on\npiles, and validating LLM- and KG-generated evidence. Our paper describes the\ntool, as well as feedback received from six professional intelligence analysts\nthat used VisPile to analyze a text document corpus."}
{"id": "2510.08973", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.08973", "abs": "https://arxiv.org/abs/2510.08973", "authors": ["Bibekananda Patra", "Aditya Mahesh Kolte", "Sandipan Bandyopadhyay"], "title": "A geometrical approach to solve the proximity of a point to an axisymmetric quadric in space", "comment": null, "summary": "This paper presents the classification of a general quadric into an\naxisymmetric quadric (AQ) and the solution to the problem of the proximity of a\ngiven point to an AQ. The problem of proximity in $R^3$ is reduced to the same\nin $R^2$, which is not found in the literature. A new method to solve the\nproblem in $R^2$ is used based on the geometrical properties of the conics,\nsuch as sub-normal, length of the semi-major axis, eccentricity, slope and\nradius. Furthermore, the problem in $R^2$ is categorised into two and three\nmore sub-cases for parabola and ellipse/hyperbola, respectively, depending on\nthe location of the point, which is a novel approach as per the authors'\nknowledge. The proposed method is suitable for implementation in a common\nprogramming language, such as C and proved to be faster than a commercial\nlibrary, namely, Bullet."}
{"id": "2510.09080", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.09080", "abs": "https://arxiv.org/abs/2510.09080", "authors": ["Shannon Liu", "Maria Teresa Parreira", "Wendy Ju"], "title": "Training Models to Detect Successive Robot Errors from Human Reactions", "comment": "Accepted to NERC '25", "summary": "As robots become more integrated into society, detecting robot errors is\nessential for effective human-robot interaction (HRI). When a robot fails\nrepeatedly, how can it know when to change its behavior? Humans naturally\nrespond to robot errors through verbal and nonverbal cues that intensify over\nsuccessive failures-from confusion and subtle speech changes to visible\nfrustration and impatience. While prior work shows that human reactions can\nindicate robot failures, few studies examine how these evolving responses\nreveal successive failures. This research uses machine learning to recognize\nstages of robot failure from human reactions. In a study with 26 participants\ninteracting with a robot that made repeated conversational errors, behavioral\nfeatures were extracted from video data to train models for individual users.\nThe best model achieved 93.5% accuracy for detecting errors and 84.1% for\nclassifying successive failures. Modeling the progression of human reactions\nenhances error detection and understanding of repeated interaction breakdowns\nin HRI."}
{"id": "2510.09013", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.09013", "abs": "https://arxiv.org/abs/2510.09013", "authors": ["Daniel A. Williams", "Airlie Chapman", "Daniel R. Little", "Chris Manzie"], "title": "Trust Modeling and Estimation in Human-Autonomy Interactions", "comment": "10 pages. 13 figures", "summary": "Advances in the control of autonomous systems have accompanied an expansion\nin the potential applications for autonomous robotic systems. The success of\napplications involving humans depends on the quality of interaction between the\nautonomous system and the human supervisor, which is particularly affected by\nthe degree of trust that the supervisor places in the autonomous system. Absent\nfrom the literature are models of supervisor trust dynamics that can\naccommodate asymmetric responses to autonomous system performance and the\nintermittent nature of supervisor-autonomous system communication. This paper\nfocuses on formulating an estimated model of supervisor trust that incorporates\nboth of these features by employing a switched linear system structure with\nevent-triggered sampling of the model input and output. Trust response data\ncollected in a user study with 51 participants were then used identify\nparameters for a switched linear model-based observer of supervisor trust."}
{"id": "2510.09036", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09036", "abs": "https://arxiv.org/abs/2510.09036", "authors": ["Chuanrui Zhang", "Zhengxian Wu", "Guanxing Lu", "Yansong Tang", "Ziwei Wang"], "title": "iMoWM: Taming Interactive Multi-Modal World Model for Robotic Manipulation", "comment": null, "summary": "Learned world models hold significant potential for robotic manipulation, as\nthey can serve as simulator for real-world interactions. While extensive\nprogress has been made in 2D video-based world models, these approaches often\nlack geometric and spatial reasoning, which is essential for capturing the\nphysical structure of the 3D world. To address this limitation, we introduce\niMoWM, a novel interactive world model designed to generate color images, depth\nmaps, and robot arm masks in an autoregressive manner conditioned on actions.\nTo overcome the high computational cost associated with three-dimensional\ninformation, we propose MMTokenizer, which unifies multi-modal inputs into a\ncompact token representation. This design enables iMoWM to leverage large-scale\npretrained VideoGPT models while maintaining high efficiency and incorporating\nricher physical information. With its multi-modal representation, iMoWM not\nonly improves the visual quality of future predictions but also serves as an\neffective simulator for model-based reinforcement learning (MBRL) and\nfacilitates real-world imitation learning. Extensive experiments demonstrate\nthe superiority of iMoWM across these tasks, showcasing the advantages of\nmulti-modal world modeling for robotic manipulation. Homepage:\nhttps://xingyoujun.github.io/imowm/"}
{"id": "2510.09080", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.09080", "abs": "https://arxiv.org/abs/2510.09080", "authors": ["Shannon Liu", "Maria Teresa Parreira", "Wendy Ju"], "title": "Training Models to Detect Successive Robot Errors from Human Reactions", "comment": "Accepted to NERC '25", "summary": "As robots become more integrated into society, detecting robot errors is\nessential for effective human-robot interaction (HRI). When a robot fails\nrepeatedly, how can it know when to change its behavior? Humans naturally\nrespond to robot errors through verbal and nonverbal cues that intensify over\nsuccessive failures-from confusion and subtle speech changes to visible\nfrustration and impatience. While prior work shows that human reactions can\nindicate robot failures, few studies examine how these evolving responses\nreveal successive failures. This research uses machine learning to recognize\nstages of robot failure from human reactions. In a study with 26 participants\ninteracting with a robot that made repeated conversational errors, behavioral\nfeatures were extracted from video data to train models for individual users.\nThe best model achieved 93.5% accuracy for detecting errors and 84.1% for\nclassifying successive failures. Modeling the progression of human reactions\nenhances error detection and understanding of repeated interaction breakdowns\nin HRI."}
{"id": "2510.09089", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09089", "abs": "https://arxiv.org/abs/2510.09089", "authors": ["Jikai Wang", "Yunqi Cheng", "Kezhi Wang", "Zonghai Chen"], "title": "Robust Visual Teach-and-Repeat Navigation with Flexible Topo-metric Graph Map Representation", "comment": null, "summary": "Visual Teach-and-Repeat Navigation is a direct solution for mobile robot to\nbe deployed in unknown environments. However, robust trajectory repeat\nnavigation still remains challenged due to environmental changing and dynamic\nobjects. In this paper, we propose a novel visual teach-and-repeat navigation\nsystem, which consists of a flexible map representation, robust map matching\nand a map-less local navigation module. During the teaching process, the\nrecorded keyframes are formulated as a topo-metric graph and each node can be\nfurther extended to save new observations. Such representation also alleviates\nthe requirement of globally consistent mapping. To enhance the place\nrecognition performance during repeating process, instead of using\nframe-to-frame matching, we firstly implement keyframe clustering to aggregate\nsimilar connected keyframes into local map and perform place recognition based\non visual frame-tolocal map matching strategy. To promote the local goal\npersistent tracking performance, a long-term goal management algorithm is\nconstructed, which can avoid the robot getting lost due to environmental\nchanges or obstacle occlusion. To achieve the goal without map, a local\ntrajectory-control candidate optimization algorithm is proposed. Extensively\nexperiments are conducted on our mobile platform. The results demonstrate that\nour system is superior to the baselines in terms of robustness and\neffectiveness."}
{"id": "2510.09096", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09096", "abs": "https://arxiv.org/abs/2510.09096", "authors": ["Xinhu Li", "Ayush Jain", "Zhaojing Yang", "Yigit Korkmaz", "Erdem Bıyık"], "title": "When a Robot is More Capable than a Human: Learning from Constrained Demonstrators", "comment": null, "summary": "Learning from demonstrations enables experts to teach robots complex tasks\nusing interfaces such as kinesthetic teaching, joystick control, and\nsim-to-real transfer. However, these interfaces often constrain the expert's\nability to demonstrate optimal behavior due to indirect control, setup\nrestrictions, and hardware safety. For example, a joystick can move a robotic\narm only in a 2D plane, even though the robot operates in a higher-dimensional\nspace. As a result, the demonstrations collected by constrained experts lead to\nsuboptimal performance of the learned policies. This raises a key question: Can\na robot learn a better policy than the one demonstrated by a constrained\nexpert? We address this by allowing the agent to go beyond direct imitation of\nexpert actions and explore shorter and more efficient trajectories. We use the\ndemonstrations to infer a state-only reward signal that measures task progress,\nand self-label reward for unknown states using temporal interpolation. Our\napproach outperforms common imitation learning in both sample efficiency and\ntask completion time. On a real WidowX robotic arm, it completes the task in 12\nseconds, 10x faster than behavioral cloning, as shown in real-robot videos on\nhttps://sites.google.com/view/constrainedexpert ."}
{"id": "2510.09188", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.09188", "abs": "https://arxiv.org/abs/2510.09188", "authors": ["Zihao Mao", "Yunheng Wang", "Yunting Ji", "Yi Yang", "Wenjie Song"], "title": "Decentralized Multi-Robot Relative Navigation in Unknown, Structurally Constrained Environments under Limited Communication", "comment": null, "summary": "Multi-robot navigation in unknown, structurally constrained, and GPS-denied\nenvironments presents a fundamental trade-off between global strategic\nforesight and local tactical agility, particularly under limited communication.\nCentralized methods achieve global optimality but suffer from high\ncommunication overhead, while distributed methods are efficient but lack the\nbroader awareness to avoid deadlocks and topological traps. To address this, we\npropose a fully decentralized, hierarchical relative navigation framework that\nachieves both strategic foresight and tactical agility without a unified\ncoordinate system. At the strategic layer, robots build and exchange\nlightweight topological maps upon opportunistic encounters. This process\nfosters an emergent global awareness, enabling the planning of efficient,\ntrap-avoiding routes at an abstract level. This high-level plan then inspires\nthe tactical layer, which operates on local metric information. Here, a\nsampling-based escape point strategy resolves dense spatio-temporal conflicts\nby generating dynamically feasible trajectories in real time, concurrently\nsatisfying tight environmental and kinodynamic constraints. Extensive\nsimulations and real-world experiments demonstrate that our system\nsignificantly outperforms in success rate and efficiency, especially in\ncommunication-limited environments with complex topological structures."}
{"id": "2510.09204", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09204", "abs": "https://arxiv.org/abs/2510.09204", "authors": ["Simon Idoko", "Arun Kumar Singh"], "title": "Flow-Opt: Scalable Centralized Multi-Robot Trajectory Optimization with Flow Matching and Differentiable Optimization", "comment": null, "summary": "Centralized trajectory optimization in the joint space of multiple robots\nallows access to a larger feasible space that can result in smoother\ntrajectories, especially while planning in tight spaces. Unfortunately, it is\noften computationally intractable beyond a very small swarm size. In this\npaper, we propose Flow-Opt, a learning-based approach towards improving the\ncomputational tractability of centralized multi-robot trajectory optimization.\nSpecifically, we reduce the problem to first learning a generative model to\nsample different candidate trajectories and then using a learned\nSafety-Filter(SF) to ensure fast inference-time constraint satisfaction. We\npropose a flow-matching model with a diffusion transformer (DiT) augmented with\npermutation invariant robot position and map encoders as the generative model.\nWe develop a custom solver for our SF and equip it with a neural network that\npredicts context-specific initialization. The initialization network is trained\nin a self-supervised manner, taking advantage of the differentiability of the\nSF solver. We advance the state-of-the-art in the following respects. First, we\nshow that we can generate trajectories of tens of robots in cluttered\nenvironments in a few tens of milliseconds. This is several times faster than\nexisting centralized optimization approaches. Moreover, our approach also\ngenerates smoother trajectories orders of magnitude faster than competing\nbaselines based on diffusion models. Second, each component of our approach can\nbe batched, allowing us to solve a few tens of problem instances in a fraction\nof a second. We believe this is a first such result; no existing approach\nprovides such capabilities. Finally, our approach can generate a diverse set of\ntrajectories between a given set of start and goal locations, which can capture\ndifferent collision-avoidance behaviors."}
{"id": "2510.09209", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09209", "abs": "https://arxiv.org/abs/2510.09209", "authors": ["Yuki Kuroda", "Tomoya Takahashi", "Cristian C Beltran-Hernandez", "Masashi Hamaya", "Kazutoshi Tanaka"], "title": "PLEXUS Hand: Lightweight Four-Motor Prosthetic Hand Enabling Precision-Lateral Dexterous Manipulation", "comment": null, "summary": "Electric prosthetic hands should be lightweight to decrease the burden on the\nuser, shaped like human hands for cosmetic purposes, and have motors inside to\nprotect them from damage and dirt. In addition to the ability to perform daily\nactivities, these features are essential for everyday use of the hand. In-hand\nmanipulation is necessary to perform daily activities such as transitioning\nbetween different postures, particularly through rotational movements, such as\nreorienting cards before slot insertion and operating tools such as\nscrewdrivers. However, currently used electric prosthetic hands only achieve\nstatic grasp postures, and existing manipulation approaches require either many\nmotors, which makes the prosthesis heavy for daily use in the hand, or complex\nmechanisms that demand a large internal space and force external motor\nplacement, complicating attachment and exposing the components to damage.\nAlternatively, we combine a single-axis thumb and optimized thumb positioning\nto achieve basic posture and in-hand manipulation, that is, the reorientation\nbetween precision and lateral grasps, using only four motors in a lightweight\n(311 g) prosthetic hand. Experimental validation using primitive objects of\nvarious widths (5-30 mm) and shapes (cylinders and prisms) resulted in success\nrates of 90-100% for reorientation tasks. The hand performed seal stamping and\nUSB device insertion, as well as rotation to operate a screwdriver."}
{"id": "2510.09221", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09221", "abs": "https://arxiv.org/abs/2510.09221", "authors": ["Jingyuan Sun", "Chaoran Wang", "Mingyu Zhang", "Cui Miao", "Hongyu Ji", "Zihan Qu", "Han Sun", "Bing Wang", "Qingyi Si"], "title": "HANDO: Hierarchical Autonomous Navigation and Dexterous Omni-loco-manipulation", "comment": "4 pages, 2 figures, this paper has been accepted for the workshop\n  Perception and Planning for Mobile Manipulation in Changing Environments\n  (PM2CE) at IROS 2025", "summary": "Seamless loco-manipulation in unstructured environments requires robots to\nleverage autonomous exploration alongside whole-body control for physical\ninteraction. In this work, we introduce HANDO (Hierarchical Autonomous\nNavigation and Dexterous Omni-loco-manipulation), a two-layer framework\ndesigned for legged robots equipped with manipulators to perform human-centered\nmobile manipulation tasks. The first layer utilizes a goal-conditioned\nautonomous exploration policy to guide the robot to semantically specified\ntargets, such as a black office chair in a dynamic environment. The second\nlayer employs a unified whole-body loco-manipulation policy to coordinate the\narm and legs for precise interaction tasks-for example, handing a drink to a\nperson seated on the chair. We have conducted an initial deployment of the\nnavigation module, and will continue to pursue finer-grained deployment of\nwhole-body loco-manipulation."}
{"id": "2510.09229", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09229", "abs": "https://arxiv.org/abs/2510.09229", "authors": ["Yuyang Gao", "Haofei Ma", "Pai Zheng"], "title": "Glovity: Learning Dexterous Contact-Rich Manipulation via Spatial Wrench Feedback Teleoperation System", "comment": null, "summary": "We present Glovity, a novel, low-cost wearable teleoperation system that\nintegrates a spatial wrench (force-torque) feedback device with a haptic glove\nfeaturing fingertip Hall sensor calibration, enabling feedback-rich dexterous\nmanipulation. Glovity addresses key challenges in contact-rich tasks by\nproviding intuitive wrench and tactile feedback, while overcoming embodiment\ngaps through precise retargeting. User studies demonstrate significant\nimprovements: wrench feedback boosts success rates in book-flipping tasks from\n48% to 78% and reduces completion time by 25%, while fingertip calibration\nenhances thin-object grasping success significantly compared to commercial\nglove. Furthermore, incorporating wrench signals into imitation learning (via\nDP-R3M) achieves high success rate in novel contact-rich scenarios, such as\nadaptive page flipping and force-aware handovers. All hardware designs,\nsoftware will be open-sourced. Project website: https://glovity.github.io/"}
{"id": "2510.09254", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09254", "abs": "https://arxiv.org/abs/2510.09254", "authors": ["Dominik Urbaniak", "Alejandro Agostini", "Pol Ramon", "Jan Rosell", "Raúl Suárez", "Michael Suppa"], "title": "Obstacle Avoidance using Dynamic Movement Primitives and Reinforcement Learning", "comment": "8 pages, 7 figures", "summary": "Learning-based motion planning can quickly generate near-optimal\ntrajectories. However, it often requires either large training datasets or\ncostly collection of human demonstrations. This work proposes an alternative\napproach that quickly generates smooth, near-optimal collision-free 3D\nCartesian trajectories from a single artificial demonstration. The\ndemonstration is encoded as a Dynamic Movement Primitive (DMP) and iteratively\nreshaped using policy-based reinforcement learning to create a diverse\ntrajectory dataset for varying obstacle configurations. This dataset is used to\ntrain a neural network that takes as inputs the task parameters describing the\nobstacle dimensions and location, derived automatically from a point cloud, and\noutputs the DMP parameters that generate the trajectory. The approach is\nvalidated in simulation and real-robot experiments, outperforming a RRT-Connect\nbaseline in terms of computation and execution time, as well as trajectory\nlength, while supporting multi-modal trajectory generation for different\nobstacle geometries and end-effector dimensions. Videos and the implementation\ncode are available at https://github.com/DominikUrbaniak/obst-avoid-dmp-pi2."}
{"id": "2510.09267", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09267", "abs": "https://arxiv.org/abs/2510.09267", "authors": ["Amina Ferrad", "Johann Huber", "François Hélénon", "Julien Gleyze", "Mahdi Khoramshahi", "Stéphane Doncieux"], "title": "Placeit! A Framework for Learning Robot Object Placement Skills", "comment": "8 pages, 8 figures. Draft version", "summary": "Robotics research has made significant strides in learning, yet mastering\nbasic skills like object placement remains a fundamental challenge. A key\nbottleneck is the acquisition of large-scale, high-quality data, which is often\na manual and laborious process. Inspired by Graspit!, a foundational work that\nused simulation to automatically generate dexterous grasp poses, we introduce\nPlaceit!, an evolutionary-computation framework for generating valid placement\npositions for rigid objects. Placeit! is highly versatile, supporting tasks\nfrom placing objects on tables to stacking and inserting them. Our experiments\nshow that by leveraging quality-diversity optimization, Placeit! significantly\noutperforms state-of-the-art methods across all scenarios for generating\ndiverse valid poses. A pick&place pipeline built on our framework achieved a\n90% success rate over 120 real-world deployments. This work positions Placeit!\nas a powerful tool for open-environment pick-and-place tasks and as a valuable\nengine for generating the data needed to train simulation-based foundation\nmodels in robotics."}
{"id": "2510.09396", "categories": ["cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.09396", "abs": "https://arxiv.org/abs/2510.09396", "authors": ["Sajad Khatiri", "Francisco Eli Vina Barrientos", "Maximilian Wulf", "Paolo Tonella", "Sebastiano Panichella"], "title": "Bridging Research and Practice in Simulation-based Testing of Industrial Robot Navigation Systems", "comment": "12 pages, accepted for publication at IEEE/ACM International\n  Conference on Automated Software Engineering (ASE) 2025 - Industry Showcase\n  Track", "summary": "Ensuring robust robotic navigation in dynamic environments is a key\nchallenge, as traditional testing methods often struggle to cover the full\nspectrum of operational requirements. This paper presents the industrial\nadoption of Surrealist, a simulation-based test generation framework originally\nfor UAVs, now applied to the ANYmal quadrupedal robot for industrial\ninspection. Our method uses a search-based algorithm to automatically generate\nchallenging obstacle avoidance scenarios, uncovering failures often missed by\nmanual testing. In a pilot phase, generated test suites revealed critical\nweaknesses in one experimental algorithm (40.3% success rate) and served as an\neffective benchmark to prove the superior robustness of another (71.2% success\nrate). The framework was then integrated into the ANYbotics workflow for a\nsix-month industrial evaluation, where it was used to test five proprietary\nalgorithms. A formal survey confirmed its value, showing it enhances the\ndevelopment process, uncovers critical failures, provides objective benchmarks,\nand strengthens the overall verification pipeline."}
{"id": "2510.09459", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09459", "abs": "https://arxiv.org/abs/2510.09459", "authors": ["Ralf Römer", "Adrian Kobras", "Luca Worbis", "Angela P. Schoellig"], "title": "Failure Prediction at Runtime for Generative Robot Policies", "comment": "Accepted to NeurIPS 2025", "summary": "Imitation learning (IL) with generative models, such as diffusion and flow\nmatching, has enabled robots to perform complex, long-horizon tasks. However,\ndistribution shifts from unseen environments or compounding action errors can\nstill cause unpredictable and unsafe behavior, leading to task failure. Early\nfailure prediction during runtime is therefore essential for deploying robots\nin human-centered and safety-critical environments. We propose FIPER, a general\nframework for Failure Prediction at Runtime for generative IL policies that\ndoes not require failure data. FIPER identifies two key indicators of impending\nfailure: (i) out-of-distribution (OOD) observations detected via random network\ndistillation in the policy's embedding space, and (ii) high uncertainty in\ngenerated actions measured by a novel action-chunk entropy score. Both failure\nprediction scores are calibrated using a small set of successful rollouts via\nconformal prediction. A failure alarm is triggered when both indicators,\naggregated over short time windows, exceed their thresholds. We evaluate FIPER\nacross five simulation and real-world environments involving diverse failure\nmodes. Our results demonstrate that FIPER better distinguishes actual failures\nfrom benign OOD situations and predicts failures more accurately and earlier\nthan existing methods. We thus consider this work an important step towards\nmore interpretable and safer generative robot policies. Code, data and videos\nare available at https://tum-lsy.github.io/fiper_website."}
{"id": "2510.09483", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09483", "abs": "https://arxiv.org/abs/2510.09483", "authors": ["Lars Ohnemus", "Nils Hantke", "Max Weißer", "Kai Furmans"], "title": "FOGMACHINE -- Leveraging Discrete-Event Simulation and Scene Graphs for Modeling Hierarchical, Interconnected Environments under Partial Observations from Mobile Agents", "comment": "submitted to the IEEE for possible publication; 8 pages, 3 figures, 1\n  table", "summary": "Dynamic Scene Graphs (DSGs) provide a structured representation of\nhierarchical, interconnected environments, but current approaches struggle to\ncapture stochastic dynamics, partial observability, and multi-agent activity.\nThese aspects are critical for embodied AI, where agents must act under\nuncertainty and delayed perception. We introduce FOGMACHINE , an open-source\nframework that fuses DSGs with discrete-event simulation to model object\ndynamics, agent observations, and interactions at scale. This setup enables the\nstudy of uncertainty propagation, planning under limited perception, and\nemergent multi-agent behavior. Experiments in urban scenarios illustrate\nrealistic temporal and spatial patterns while revealing the challenges of\nbelief estimation under sparse observations. By combining structured\nrepresentations with efficient simulation, FOGMACHINE establishes an effective\ntool for benchmarking, model training, and advancing embodied AI in complex,\nuncertain environments."}
{"id": "2510.09497", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09497", "abs": "https://arxiv.org/abs/2510.09497", "authors": ["Noah Barnes", "Ji Woong Kim", "Lingyun Di", "Hannah Qu", "Anuruddha Bhattacharjee", "Miroslaw Janowski", "Dheeraj Gandhi", "Bailey Felix", "Shaopeng Jiang", "Olivia Young", "Mark Fuge", "Ryan D. Sochol", "Jeremy D. Brown", "Axel Krieger"], "title": "Autonomous Soft Robotic Guidewire Navigation via Imitation Learning", "comment": null, "summary": "In endovascular surgery, endovascular interventionists push a thin tube\ncalled a catheter, guided by a thin wire to a treatment site inside the\npatient's blood vessels to treat various conditions such as blood clots,\naneurysms, and malformations. Guidewires with robotic tips can enhance\nmaneuverability, but they present challenges in modeling and control.\nAutomation of soft robotic guidewire navigation has the potential to overcome\nthese challenges, increasing the precision and safety of endovascular\nnavigation. In other surgical domains, end-to-end imitation learning has shown\npromising results. Thus, we develop a transformer-based imitation learning\nframework with goal conditioning, relative action outputs, and automatic\ncontrast dye injections to enable generalizable soft robotic guidewire\nnavigation in an aneurysm targeting task. We train the model on 36 different\nmodular bifurcated geometries, generating 647 total demonstrations under\nsimulated fluoroscopy, and evaluate it on three previously unseen vascular\ngeometries. The model can autonomously drive the tip of the robot to the\naneurysm location with a success rate of 83% on the unseen geometries,\noutperforming several baselines. In addition, we present ablation and baseline\nstudies to evaluate the effectiveness of each design and data collection\nchoice. Project website: https://softrobotnavigation.github.io/"}
{"id": "2510.09526", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09526", "abs": "https://arxiv.org/abs/2510.09526", "authors": ["Chenghao Wang", "Kaushik Venkatesh Krishnamurthy", "Shreyansh Pitroda", "Adarsh Salagame", "Ioannis Mandralis", "Eric Sihite", "Alireza Ramezani", "Morteza Gharib"], "title": "Dynamic Quadrupedal Legged and Aerial Locomotion via Structure Repurposing", "comment": null, "summary": "Multi-modal ground-aerial robots have been extensively studied, with a\nsignificant challenge lying in the integration of conflicting requirements\nacross different modes of operation. The Husky robot family, developed at\nNortheastern University, and specifically the Husky v.2 discussed in this\nstudy, addresses this challenge by incorporating posture manipulation and\nthrust vectoring into multi-modal locomotion through structure repurposing.\nThis quadrupedal robot features leg structures that can be repurposed for\ndynamic legged locomotion and flight. In this paper, we present the hardware\ndesign of the robot and report primary results on dynamic quadrupedal legged\nlocomotion and hovering."}
{"id": "2510.09543", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09543", "abs": "https://arxiv.org/abs/2510.09543", "authors": ["Chenghao Wang", "Arjun Viswanathan", "Eric Sihite", "Alireza Ramezani"], "title": "Guiding Energy-Efficient Locomotion through Impact Mitigation Rewards", "comment": null, "summary": "Animals achieve energy-efficient locomotion by their implicit passive\ndynamics, a marvel that has captivated roboticists for decades.Recently,\nmethods incorporated Adversarial Motion Prior (AMP) and Reinforcement learning\n(RL) shows promising progress to replicate Animals' naturalistic motion.\nHowever, such imitation learning approaches predominantly capture explicit\nkinematic patterns, so-called gaits, while overlooking the implicit passive\ndynamics. This work bridges this gap by incorporating a reward term guided by\nImpact Mitigation Factor (IMF), a physics-informed metric that quantifies a\nrobot's ability to passively mitigate impacts. By integrating IMF with AMP, our\napproach enables RL policies to learn both explicit motion trajectories from\nanimal reference motion and the implicit passive dynamic. We demonstrate energy\nefficiency improvements of up to 32%, as measured by the Cost of Transport\n(CoT), across both AMP and handcrafted reward structure."}
{"id": "2510.09574", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09574", "abs": "https://arxiv.org/abs/2510.09574", "authors": ["Daria de tinguy", "Tim Verbelen", "Emilio Gamba", "Bart Dhoedt"], "title": "Zero-shot Structure Learning and Planning for Autonomous Robot Navigation using Active Inference", "comment": "yet to be submitted", "summary": "Autonomous navigation in unfamiliar environments requires robots to\nsimultaneously explore, localise, and plan under uncertainty, without relying\non predefined maps or extensive training. We present a biologically inspired,\nActive Inference-based framework, Active Inference MAPping and Planning\n(AIMAPP). This model unifies mapping, localisation, and decision-making within\na single generative model. Inspired by hippocampal navigation, it uses\ntopological reasoning, place-cell encoding, and episodic memory to guide\nbehaviour. The agent builds and updates a sparse topological map online, learns\nstate transitions dynamically, and plans actions by minimising Expected Free\nEnergy. This allows it to balance goal-directed and exploratory behaviours. We\nimplemented a ROS-compatible navigation system that is sensor and\nrobot-agnostic, capable of integrating with diverse hardware configurations. It\noperates in a fully self-supervised manner, is resilient to drift, and supports\nboth exploration and goal-directed navigation without any pre-training. We\ndemonstrate robust performance in large-scale real and simulated environments\nagainst state-of-the-art planning models, highlighting the system's\nadaptability to ambiguous observations, environmental changes, and sensor\nnoise. The model offers a biologically inspired, modular solution to scalable,\nself-supervised navigation in unstructured settings. AIMAPP is available at\nhttps://github.com/decide-ugent/AIMAPP."}
{"id": "2510.09570", "categories": ["cs.HC", "cs.GR", "cs.NE", "cs.RO", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2510.09570", "abs": "https://arxiv.org/abs/2510.09570", "authors": ["Nishant Gautam", "Somya Sharma", "Peter Corcoran", "Kaspar Althoefer"], "title": "Differential Analysis of Pseudo Haptic Feedback: Novel Comparative Study of Visual and Auditory Cue Integration for Psychophysical Evaluation", "comment": "17 Pages, 9 Figures", "summary": "Pseudo-haptics exploit carefully crafted visual or auditory cues to trick the\nbrain into \"feeling\" forces that are never physically applied, offering a\nlow-cost alternative to traditional haptic hardware. Here, we present a\ncomparative psychophysical study that quantifies how visual and auditory\nstimuli combine to evoke pseudo-haptic pressure sensations on a commodity\ntablet. Using a Unity-based Rollball game, participants (n = 4) guided a\nvirtual ball across three textured terrains while their finger forces were\ncaptured in real time with a Robotous RFT40 force-torque sensor. Each terrain\nwas paired with a distinct rolling-sound profile spanning 440 Hz - 4.7 kHz, 440\nHz - 13.1 kHz, or 440 Hz - 8.9 kHz; crevice collisions triggered additional\n\"knocking\" bursts to heighten realism. Average tactile forces increased\nsystematically with cue intensity: 0.40 N, 0.79 N and 0.88 N for visual-only\ntrials and 0.41 N, 0.81 N and 0.90 N for audio-only trials on Terrains 1-3,\nrespectively. Higher audio frequencies and denser visual textures both elicited\nstronger muscle activation, and their combination further reduced the force\nneeded to perceive surface changes, confirming multisensory integration. These\nresults demonstrate that consumer-grade isometric devices can reliably induce\nand measure graded pseudo-haptic feedback without specialized actuators,\nopening a path toward affordable rehabilitation tools, training simulators and\nassistive interfaces."}
