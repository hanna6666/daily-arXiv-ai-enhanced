{"id": "2601.09877", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.09877", "abs": "https://arxiv.org/abs/2601.09877", "authors": ["Paulius Jurcys", "Ashley Greenwald", "Mark Fenwick", "Valto Loikkanen", "Sebastian Porsdam Mann", "Brian D. Earp"], "title": "Who Owns My AI Twin? Data Ownership in a New World of Simulated Identities", "comment": null, "summary": "The emergence of AI twins, digital replicas that encapsulate an individual's knowledge, memories, psychological traits, and behavioral patterns, raises novel legal and ethical challenges for data governance and personal identity. Built from personal data, these systems require a rethinking of what it means to exercise dominion over one's data and to maintain personal autonomy in an AI-mediated environment. This article argues that natural persons should be recognized as the moral and legal owners of their AI twins, which function as intimate extensions of the self rather than as proprietary technological artifacts. It critiques prevailing legal frameworks that prioritize technological infrastructure and platform control over data and individual autonomy, exposing their structural limitations. In response, the article advances a human-centric model of data governance grounded in individual dominion and a private-by-default principle. This approach proposes a reimagined social contract for AI-driven identities that strengthens personal agency, promotes equitable data stewardship, and better aligns legal norms with the socio-technical realities of AI twins.", "AI": {"tldr": "\u672c\u6587\u8bba\u8bc1\u81ea\u7136\u4eba\u5e94\u88ab\u89c6\u4e3a\u5176\u4eba\u5de5\u667a\u80fd\u53cc\u80de\u80ce\u7684\u9053\u5fb7\u548c\u6cd5\u5f8b\u6240\u6709\u8005\uff0c\u5e76\u63d0\u51fa\u4e86\u4ee5\u4eba\u4e3a\u672c\u7684\u6570\u636e\u6cbb\u7406\u6a21\u578b\u3002", "motivation": "\u63a2\u8ba8\u6570\u5b57\u590d\u5236\u4f53\u5bf9\u4e2a\u4f53\u6570\u636e\u4e3b\u6743\u548c\u4e2a\u4eba\u81ea\u4e3b\u6027\u63d0\u51fa\u7684\u6311\u6218\uff0c\u5f3a\u8c03\u5728\u4eba\u5de5\u667a\u80fd\u4e2d\u4fdd\u62a4\u4e2a\u4eba\u8eab\u4efd\u7684\u5fc5\u8981\u6027\u3002", "method": "\u91c7\u7528\u6cd5\u5f8b\u4e0e\u4f26\u7406\u7684\u6279\u5224\u5206\u6790\uff0c\u5ba1\u89c6\u73b0\u884c\u6cd5\u5f8b\u6846\u67b6\u5bf9\u4e2a\u4eba\u6570\u636e\u63a7\u5236\u6743\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u6570\u636e\u6cbb\u7406\u6a21\u578b\u3002", "result": "\u672c\u8bba\u6587\u63a2\u8ba8\u4e86\u4eba\u5de5\u667a\u80fd\u53cc\u80de\u80ce\u7684\u51fa\u73b0\u5bf9\u6570\u636e\u6cbb\u7406\u548c\u4e2a\u4eba\u8eab\u4efd\u6240\u5e26\u6765\u7684\u6cd5\u5f8b\u548c\u9053\u5fb7\u65b0\u6311\u6218\u3002", "conclusion": "\u6587\u7ae0\u547c\u5401\u91cd\u6784\u793e\u4f1a\u5951\u7ea6\uff0c\u4ee5\u52a0\u5f3a\u4e2a\u4eba\u80fd\u529b\uff0c\u4fc3\u8fdb\u516c\u5e73\u7684\u6570\u636e\u7ba1\u7406\uff0c\u66f4\u597d\u5730\u5c06\u6cd5\u5f8b\u89c4\u8303\u4e0e\u4eba\u5de5\u667a\u80fd\u53cc\u80de\u80ce\u7684\u793e\u4f1a\u6280\u672f\u73b0\u5b9e\u5bf9\u63a5\u3002"}}
{"id": "2601.09887", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.09887", "abs": "https://arxiv.org/abs/2601.09887", "authors": ["Rostyslav Hnatyshyn", "Danny Perez", "Gerik Scheuermann", "Ross Maciejewski", "Baldwin Nsonga"], "title": "LAMDA: Aiding Visual Exploration of Atomic Displacements in Molecular Dynamics Simulations", "comment": "Accepted version of paper published in Transactions for Visualization and Computer Graphics", "summary": "Contemporary materials science research is heavily conducted in silico, involving massive simulations of the atomic-scale evolution of materials. Cataloging basic patterns in the atomic displacements is key to understanding and predicting the evolution of physical properties. However, the combinatorial complexity of the space of possible transitions coupled with the overwhelming amount of data being produced by high-throughput simulations make such an analysis extremely challenging and time-consuming for domain experts. The development of visual analytics systems that facilitate the exploration of simulation data is an active field of research. While these systems excel in identifying temporal regions of interest, they treat each timestep of a simulation as an independent event without considering the behavior of the atomic displacements between timesteps. We address this gap by introducing LAMDA, a visual analytics system that allows domain experts to quickly and systematically explore state-to-state transitions. In LAMDA, transitions are hierarchically categorized, providing a basis for cataloging displacement behavior, as well as enabling the analysis of simulations at different resolutions, ranging from very broad qualitative classes of transitions to very narrow definitions of unit processes. LAMDA supports navigating the hierarchy of transitions, enabling scientists to visualize the commonalities between different transitions in each class in terms of invariant features characterizing local atomic environments, and LAMDA simplifies the analysis by capturing user inputs through annotations. We evaluate our system through a case study and report on findings from our domain experts.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aLAMDA\u7684\u53ef\u89c6\u5316\u5206\u6790\u7cfb\u7edf\uff0c\u5e2e\u52a9\u6750\u6599\u79d1\u5b66\u5bb6\u5feb\u901f\u7cfb\u7edf\u5730\u63a2\u7d22\u539f\u5b50\u72b6\u6001\u4e4b\u95f4\u7684\u8f6c\u53d8\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5206\u6790\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5728\u6750\u6599\u79d1\u5b66\u7814\u7a76\u4e2d\uff0c\u968f\u7740\u8ba1\u7b97\u6a21\u62df\u6280\u672f\u7684\u53d1\u5c55\uff0c\u6570\u636e\u91cf\u6025\u5267\u589e\u52a0\uff0c\u5982\u4f55\u6709\u6548\u5206\u6790\u539f\u5b50\u4f4d\u79fb\u6a21\u5f0f\u4ee5\u7406\u89e3\u6750\u6599\u7684\u7269\u7406\u5c5e\u6027\u6f14\u5316\u53d8\u5f97\u6781\u5177\u6311\u6218\u6027\u3002", "method": "LAMDA\u7cfb\u7edf\u901a\u8fc7\u5c42\u6b21\u7ed3\u6784\u6765\u5206\u7c7b\u8f6c\u53d8\uff0c\u5141\u8bb8\u79d1\u5b66\u5bb6\u6839\u636e\u5c40\u90e8\u539f\u5b50\u73af\u5883\u7684\u7279\u5f81\u8fdb\u884c\u53ef\u89c6\u5316\uff0c\u652f\u6301\u6ce8\u91ca\u6355\u83b7\u7528\u6237\u8f93\u5165\u3002", "result": "\u63d0\u51fa\u4e86LAMDA\uff0c\u4e00\u4e2a\u53ef\u89c6\u5316\u5206\u6790\u7cfb\u7edf\uff0c\u53ef\u4ee5\u7cfb\u7edf\u5316\u5730\u63a2\u7d22\u72b6\u6001\u4e4b\u95f4\u7684\u8f6c\u53d8\uff0c\u4ece\u800c\u7b80\u5316\u8fd9\u79cd\u5206\u6790\u8fc7\u7a0b\u3002", "conclusion": "LAMDA\u7cfb\u7edf\u901a\u8fc7\u5c42\u7ea7\u5206\u7c7b\u8f6c\u53d8\uff0c\u652f\u6301\u4e0d\u540c\u5206\u8fa8\u7387\u7684\u6a21\u62df\u5206\u6790\uff0c\u5e76\u7b80\u5316\u4e86\u7528\u6237\u8f93\u5165\uff0c\u6700\u7ec8\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u6a21\u62df\u6570\u636e\u63a2\u7d22\u3002"}}
{"id": "2601.09896", "categories": ["cs.HC", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.09896", "abs": "https://arxiv.org/abs/2601.09896", "authors": ["Jordan Taylor", "William Agnew", "Maarten Sap", "Sarah E. Fox", "Haiyi Zhu"], "title": "The Algorithmic Gaze: An Audit and Ethnography of the LAION-Aesthetics Predictor Model", "comment": null, "summary": "Visual generative AI models are trained using a one-size-fits-all measure of aesthetic appeal. However, what is deemed \"aesthetic\" is inextricably linked to personal taste and cultural values, raising the question of whose taste is represented in visual generative AI models. In this work, we study an aesthetic evaluation model--LAION Aesthetic Predictor (LAP)--that is widely used to curate datasets to train visual generative image models, like Stable Diffusion, and evaluate the quality of AI-generated images. To understand what LAP measures, we audited the model across three datasets. First, we examined the impact of aesthetic filtering on the LAION-Aesthetics Dataset (approximately 1.2B images), which was curated from LAION-5B using LAP. We find that the LAP disproportionally filters in images with captions mentioning women, while filtering out images with captions mentioning men or LGBTQ+ people. Then, we used LAP to score approximately 330k images across two art datasets, finding the model rates realistic images of landscapes, cityscapes, and portraits from western and Japanese artists most highly. In doing so, the algorithmic gaze of this aesthetic evaluation model reinforces the imperial and male gazes found within western art history. In order to understand where these biases may have originated, we performed a digital ethnography of public materials related to the creation of LAP. We find that the development of LAP reflects the biases we found in our audits, such as the aesthetic scores used to train LAP primarily coming from English-speaking photographers and western AI-enthusiasts. In response, we discuss how aesthetic evaluation can perpetuate representational harms and call on AI developers to shift away from prescriptive measures of \"aesthetics\" toward more pluralistic evaluation.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86LAION Aesthetic Predictor\u5728\u5ba1\u7f8e\u7b5b\u9009\u4e2d\u7684\u504f\u89c1\uff0c\u547c\u5401\u5411\u591a\u5143\u5316\u7684\u5ba1\u7f8e\u8bc4\u4f30\u8f6c\u53d8\u3002", "motivation": "\u7814\u7a76\u89c6\u89c9\u751f\u6210AI\u6a21\u578b\u7684\u7f8e\u5b66\u8bc4\u4f30\uff0c\u63a2\u7d22\u5176\u4e2d\u6d89\u53ca\u7684\u4e2a\u4eba\u54c1\u5473\u548c\u6587\u5316\u4ef7\u503c\u89c2\uff0c\u7406\u89e3\u8c01\u7684\u54c1\u5473\u5728\u751f\u6210\u6a21\u578b\u4e2d\u5f97\u5230\u4e86\u4f53\u73b0\u3002", "method": "\u5ba1\u8ba1LAION Aesthetic Predictor\u6a21\u578b\uff0c\u901a\u8fc7\u5bf9\u4e09\u4e2a\u6570\u636e\u96c6\u7684\u5206\u6790\u53ca\u5bf9\u5f00\u53d1\u8d44\u6599\u7684\u6570\u5b57\u4eba\u7c7b\u5b66\u7814\u7a76\uff0c\u8bc6\u522b\u5176\u5b58\u5728\u7684\u504f\u89c1\u3002", "result": "\u53d1\u73b0LAION Aesthetic Predictor\u5728\u7b5b\u9009\u6570\u636e\u96c6\u65f6\u5b58\u5728\u6027\u522b\u548c\u6587\u5316\u504f\u89c1\uff0c\u5f3a\u5316\u4e86\u897f\u65b9\u827a\u672f\u5386\u53f2\u4e2d\u7684\u5e1d\u56fd\u4e3b\u4e49\u548c\u7537\u6027\u89c6\u89d2\u3002", "conclusion": "\u7f8e\u5b66\u8bc4\u4f30\u53ef\u80fd\u52a0\u5267\u4ee3\u8868\u6027\u4f24\u5bb3\uff0c\u5efa\u8baeAI\u5f00\u53d1\u8005\u91c7\u7528\u66f4\u5177\u5305\u5bb9\u6027\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2601.09898", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.09898", "abs": "https://arxiv.org/abs/2601.09898", "authors": ["David Elsweiler", "Christine Elsweiler", "Anna Ziegner"], "title": "Cooking Up Politeness in Human-AI Information Seeking Dialogue", "comment": null, "summary": "Politeness is a core dimension of human communication, yet its role in human-AI information seeking remains underexplored. We investigate how user politeness behaviour shapes conversational outcomes in a cooking-assistance setting. First, we annotated 30 dialogues, identifying four distinct user clusters ranging from Hyperpolite to Hyperefficient. We then scaled up to 18,000 simulated conversations across five politeness profiles (including impolite) and three open-weight models. Results show that politeness is not only cosmetic: it systematically affects response length, informational gain, and efficiency. Engagement-seeking prompts produced up to 90% longer replies and 38% more information nuggets than hyper-efficient prompts, but at markedly lower density. Impolite inputs yielded verbose but less efficient answers, with up to 48% fewer nuggets per watt-hour compared to polite input. These findings highlight politeness as both a fairness and sustainability issue: conversational styles can advantage or disadvantage users, and \"polite\" requests may carry hidden energy costs. We discuss implications for inclusive and resource-aware design of information agents.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u7528\u6237\u793c\u8c8c\u884c\u4e3a\u5982\u4f55\u5f71\u54cd\u4eba\u673a\u5bf9\u8bdd\u7ed3\u679c\uff0c\u7279\u522b\u662f\u5728\u70f9\u996a\u52a9\u624b\u573a\u666f\u4e2d\u3002\u901a\u8fc7\u5bf930\u4e2a\u5bf9\u8bdd\u8fdb\u884c\u6807\u6ce8\uff0c\u5e76\u6a21\u62df18000\u4e2a\u5bf9\u8bdd\uff0c\u7814\u7a76\u53d1\u73b0\u793c\u8c8c\u884c\u4e3a\u5bf9\u54cd\u5e94\u957f\u5ea6\u3001\u4fe1\u606f\u83b7\u53d6\u548c\u6548\u7387\u90fd\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u63a2\u8ba8\u4eba\u673a\u5bf9\u8bdd\u4e2d\u793c\u8c8c\u884c\u4e3a\u5bf9\u4fe1\u606f\u83b7\u53d6\u6548\u679c\u7684\u5f71\u54cd\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u5bf930\u4e2a\u7528\u6237\u5bf9\u8bdd\u8fdb\u884c\u6807\u6ce8\uff0c\u8bc6\u522b\u51fa\u56db\u79cd\u7528\u6237\u7fa4\u4f53\uff0c\u5e76\u6a21\u62df18000\u4e2a\u5bf9\u8bdd\u4ee5\u7814\u7a76\u4e0d\u540c\u793c\u8c8c\u8868\u73b0\u7684\u5f71\u54cd\u3002", "result": "\u793c\u8c8c\u884c\u4e3a\u7cfb\u7edf\u6027\u5730\u5f71\u54cd\u5bf9\u8bdd\u7684\u54cd\u5e94\u957f\u5ea6\u548c\u4fe1\u606f\u91cf\uff1b\u4e0d\u793c\u8c8c\u8f93\u5165\u5bfc\u81f4\u7684\u56de\u7b54\u5197\u957f\u4f46\u4f4e\u6548\uff0c\u4e14\u6709\u663e\u8457\u7684\u80fd\u91cf\u6210\u672c\u3002", "conclusion": "\u793c\u8c8c\u4e0d\u4ec5\u5f71\u54cd\u5bf9\u8bdd\u7684\u516c\u5e73\u6027\uff0c\u8fd8\u5b58\u5728\u80fd\u6e90\u6210\u672c\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u5173\u6ce8\u4fe1\u606f\u4ee3\u7406\u7684\u5305\u5bb9\u6027\u548c\u8d44\u6e90\u610f\u8bc6\u8bbe\u8ba1\u3002"}}
{"id": "2601.09740", "categories": ["cs.RO", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.09740", "abs": "https://arxiv.org/abs/2601.09740", "authors": ["Oumaima Barhoumi", "Mohamed H Zaki", "Sofi\u00e8ne Tahar"], "title": "Formal Safety Guarantees for Autonomous Vehicles using Barrier Certificates", "comment": "Accepted to AAAI-26 Bridge Program B10: Making Embodied AI Reliable with Testing and Formal Verification", "summary": "Modern AI technologies enable autonomous vehicles to perceive complex scenes, predict human behavior, and make real-time driving decisions. However, these data-driven components often operate as black boxes, lacking interpretability and rigorous safety guarantees. Autonomous vehicles operate in dynamic, mixed-traffic environments where interactions with human-driven vehicles introduce uncertainty and safety challenges. This work develops a formally verified safety framework for Connected and Autonomous Vehicles (CAVs) that integrates Barrier Certificates (BCs) with interpretable traffic conflict metrics, specifically Time-to-Collision (TTC) as a spatio-temporal safety metric. Safety conditions are verified using Satisfiability Modulo Theories (SMT) solvers, and an adaptive control mechanism ensures vehicles comply with these constraints in real time. Evaluation on real-world highway datasets shows a significant reduction in unsafe interactions, with up to 40\\% fewer events where TTC falls below a 3 seconds threshold, and complete elimination of conflicts in some lanes. This approach provides both interpretable and provable safety guarantees, demonstrating a practical and scalable strategy for safe autonomous driving.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u6b63\u5f0f\u9a8c\u8bc1\u7684\u5b89\u5168\u6846\u67b6\uff0c\u4e3a\u8054\u7f51\u4e0e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u63d0\u4f9b\u53ef\u89e3\u91ca\u4e14\u53ef\u9760\u7684\u5b89\u5168\u4fdd\u969c\uff0c\u663e\u8457\u51cf\u5c11\u4e0d\u5b89\u5168\u4ea4\u4e92\u3002", "motivation": "\u73b0\u4ee3AI\u6280\u672f\u4f7f\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u80fd\u591f\u611f\u77e5\u590d\u6742\u573a\u666f\uff0c\u9884\u6d4b\u4eba\u7c7b\u884c\u4e3a\uff0c\u5e76\u505a\u51fa\u5b9e\u65f6\u9a7e\u9a76\u51b3\u7b56\uff0c\u4f46\u8fd9\u4e9b\u6570\u636e\u9a71\u52a8\u7684\u7ec4\u4ef6\u901a\u5e38\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u4e25\u683c\u7684\u5b89\u5168\u4fdd\u8bc1\uff0c\u8fd9\u5728\u52a8\u6001\u6df7\u5408\u4ea4\u901a\u73af\u5883\u4e2d\u4e0e\u4eba\u9a7e\u9a76\u8f66\u8f86\u7684\u4ea4\u4e92\u5f15\u5165\u4e86\u4e0d\u786e\u5b9a\u6027\u548c\u5b89\u5168\u6311\u6218\u3002", "method": "\u4e00\u79cd\u6b63\u5f0f\u9a8c\u8bc1\u7684\u5b89\u5168\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u969c\u788d\u8bc1\u4e66\uff08BCs\uff09\u548c\u53ef\u89e3\u91ca\u7684\u4ea4\u901a\u51b2\u7a81\u6307\u6807\uff0c\u7279\u522b\u662f\u4f5c\u4e3a\u65f6\u7a7a\u5b89\u5168\u6307\u6807\u7684\u78b0\u649e\u65f6\u95f4\uff08TTC\uff09\u3002\u5229\u7528\u53ef\u6ee1\u8db3\u6027\u6a21\u7406\u8bba\uff08SMT\uff09\u6c42\u89e3\u5668\u9a8c\u8bc1\u5b89\u5168\u6761\u4ef6\uff0c\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u63a7\u5236\u673a\u5236\u786e\u4fdd\u8f66\u8f86\u5b9e\u65f6\u9075\u5faa\u8fd9\u4e9b\u7ea6\u675f\u3002", "result": "\u5728\u771f\u5b9e\u7684\u9ad8\u901f\u516c\u8def\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cunsafe interactions\u663e\u8457\u51cf\u5c11\uff0cTTC\u4f4e\u4e8e3\u79d2\u7684\u4e8b\u4ef6\u51cf\u5c11\u4e86\u591a\u8fbe40%\uff0c\u5e76\u4e14\u5728\u67d0\u4e9b\u8f66\u9053\u5b8c\u5168\u6d88\u9664\u4e86\u51b2\u7a81\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u548c\u53ef\u8bc1\u660e\u7684\u5b89\u5168\u4fdd\u969c\uff0c\u4e3a\u5b89\u5168\u81ea\u4e3b\u9a7e\u9a76\u63d0\u4f9b\u4e86\u5207\u5b9e\u53ef\u884c\u7684\u53ef\u6269\u5c55\u7b56\u7565\u3002"}}
{"id": "2601.09928", "categories": ["cs.HC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.09928", "abs": "https://arxiv.org/abs/2601.09928", "authors": ["Saber Zerhoudi", "Michael Granitzer"], "title": "In-Browser Agents for Search Assistance", "comment": null, "summary": "A fundamental tension exists between the demand for sophisticated AI assistance in web search and the need for user data privacy. Current centralized models require users to transmit sensitive browsing data to external services, which limits user control. In this paper, we present a browser extension that provides a viable in-browser alternative. We introduce a hybrid architecture that functions entirely on the client side, combining two components: (1) an adaptive probabilistic model that learns a user's behavioral policy from direct feedback, and (2) a Small Language Model (SLM), running in the browser, which is grounded by the probabilistic model to generate context-aware suggestions. To evaluate this approach, we conducted a three-week longitudinal user study with 18 participants. Our results show that this privacy-preserving approach is highly effective at adapting to individual user behavior, leading to measurably improved search efficiency. This work demonstrates that sophisticated AI assistance is achievable without compromising user privacy or data control.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u6d4f\u89c8\u5668\u6269\u5c55\uff0c\u5229\u7528\u5ba2\u6237\u7aef\u7684\u6df7\u5408\u67b6\u6784\u63d0\u4f9b\u9690\u79c1\u4fdd\u62a4\u7684AI\u641c\u7d22\u8f85\u52a9\uff0c\u6548\u679c\u663e\u8457\u3002", "motivation": "\u5e94\u5bf9\u5bf9AI\u52a9\u624b\u4e0d\u65ad\u589e\u957f\u7684\u9700\u6c42\u4e0e\u4fdd\u62a4\u7528\u6237\u6570\u636e\u9690\u79c1\u4e4b\u95f4\u7684\u77db\u76fe\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u6d4f\u89c8\u5668\u4e2d\u8fd0\u884c\u7684\u6df7\u5408\u67b6\u6784\uff0c\u5305\u62ec\u81ea\u9002\u5e94\u6982\u7387\u6a21\u578b\u548c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5b8c\u5168\u5728\u5ba2\u6237\u7aef\u5de5\u4f5c\u3002", "result": "\u8be5\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u6709\u6548\u9002\u5e94\u7528\u6237\u884c\u4e3a\uff0c\u63d0\u9ad8\u4e86\u641c\u7d22\u6548\u7387\u3002", "conclusion": "\u5728\u4e0d\u59a5\u534f\u7528\u6237\u9690\u79c1\u548c\u6570\u636e\u63a7\u5236\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u5148\u8fdb\u7684AI\u8f85\u52a9\u3002"}}
{"id": "2601.09838", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.09838", "abs": "https://arxiv.org/abs/2601.09838", "authors": ["Leonie Dyck", "Aiko Galetzka", "Maximilian Noller", "Anna-Lena Rinke", "Jutta Bormann", "Jekaterina Miller", "Michelle Hochbaum", "Julia Siemann", "J\u00f6rdis Alboth", "Andre Berwinkel", "Johanna Luz", "Britta Kley-Zobel", "Marcine Cyrys", "Nora Fl\u00f6ttmann", "Ariane Vogeler", "Mariia Melnikova", "Ira-Katharina Petras", "Michael Siniatchkin", "Winfried Barthlen", "Anna-Lisa Vollmer"], "title": "Interprofessional and Agile Development of Mobirobot: A Socially Assistive Robot for Pediatric Therapy Across Clinical and Therapeutic Settings", "comment": "submitted to Frontiers in Digital Health", "summary": "Introduction: Socially assistive robots hold promise for enhancing therapeutic engagement in paediatric clinical settings. However, their successful implementation requires not only technical robustness but also context-sensitive, co-designed solutions. This paper presents Mobirobot, a socially assistive robot developed to support mobilisation in children recovering from trauma, fractures, or depressive disorders through personalised exercise programmes.\n  Methods: An agile, human-centred development approach guided the iterative design of Mobirobot. Multidisciplinary clinical teams and end users were involved throughout the co-development process, which focused on early integration into real-world paediatric surgical and psychiatric settings. The robot, based on the NAO platform, features a simple setup, adaptable exercise routines with interactive guidance, motivational dialogue, and a graphical user interface (GUI) for monitoring and no-code system feedback.\n  Results: Deployment in hospital environments enabled the identification of key design requirements and usability constraints. Stakeholder feedback led to refinements in interaction design, movement capabilities, and technical configuration. A feasibility study is currently underway to assess acceptance, usability, and perceived therapeutic benefit, with data collection including questionnaires, behavioural observations, and staff-patient interviews.\n  Discussion: Mobirobot demonstrates how multiprofessional, stakeholder-led development can yield a socially assistive system suited for dynamic inpatient settings. Early-stage findings underscore the importance of contextual integration, robustness, and minimal-intrusion design. While challenges such as sensor limitations and patient recruitment remain, the platform offers a promising foundation for further research and clinical application.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86Mobirobot\uff0c\u4e00\u4e2a\u65e8\u5728\u652f\u6301\u513f\u7ae5\u5eb7\u590d\u7684\u793e\u4ea4\u8f85\u52a9\u673a\u5668\u4eba\uff0c\u91c7\u53d6\u4eba\u672c\u5f00\u53d1\u65b9\u6cd5\uff0c\u4ece\u8bbe\u8ba1\u5230\u5b9e\u65bd\u6d89\u53ca\u591a\u4e2a\u5229\u76ca\u76f8\u5173\u65b9\uff0c\u521d\u6b65\u6210\u679c\u663e\u793a\u5176\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u63d0\u9ad8\u513f\u7ae5\u5728\u6cbb\u7597\u540e\u6062\u590d\u8fc7\u7a0b\u4e2d\u5bf9\u8eab\u4f53\u6d3b\u52a8\u7684\u53c2\u4e0e\u5ea6\uff0c\u7279\u522b\u662f\u9488\u5bf9\u521b\u4f24\u3001\u9aa8\u6298\u6216\u6291\u90c1\u969c\u788d\u7684\u4e2a\u6027\u5316\u953b\u70bc\u65b9\u6848\u3002", "method": "\u91c7\u7528\u654f\u6377\u7684\u4eba\u672c\u5f00\u53d1\u65b9\u6cd5\uff0c\u6d89\u53ca\u591a\u5b66\u79d1\u56e2\u961f\u548c\u6700\u7ec8\u7528\u6237\u7684\u5171\u540c\u5f00\u53d1\u8fc7\u7a0b\u3002", "result": "Mobirobot\u5728\u533b\u9662\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u5e2e\u52a9\u8bc6\u522b\u4e86\u5173\u952e\u8bbe\u8ba1\u8981\u6c42\u548c\u53ef\u7528\u6027\u7ea6\u675f\uff0c\u5e76\u901a\u8fc7\u5229\u76ca\u76f8\u5173\u8005\u53cd\u9988\u6539\u8fdb\u4e86\u7cfb\u7edf\u8bbe\u8ba1\u3002", "conclusion": "Mobirobot\u5c55\u793a\u4e86\u591a\u4e13\u4e1a\u56e2\u961f\u4e3b\u5bfc\u7684\u53d1\u5c55\u65b9\u6cd5\u5728\u52a8\u6001\u4f4f\u9662\u73af\u5883\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u867d\u7136\u5b58\u5728\u4e00\u4e9b\u6311\u6218\uff0c\u4f46\u5176\u4e3a\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u4e34\u5e8a\u5e94\u7528\u5960\u5b9a\u4e86\u826f\u597d\u7684\u57fa\u7840\u3002"}}
{"id": "2601.09937", "categories": ["cs.HC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.09937", "abs": "https://arxiv.org/abs/2601.09937", "authors": ["Saber Zerhoudi", "Michael Granitzer"], "title": "From SERPs to Agents: A Platform for Comparative Studies of Information Interaction", "comment": null, "summary": "The diversification of information access systems, from RAG to autonomous agents, creates a critical need for comparative user studies. However, the technical overhead to deploy and manage these distinct systems is a major barrier. We present UXLab, an open-source system for web-based user studies that addresses this challenge. Its core is a web-based dashboard enabling the complete, no-code configuration of complex experimental designs. Researchers can visually manage the full study, from recruitment to comparing backends like traditional search, vector databases, and LLMs. We demonstrate UXLab's value via a micro case study comparing user behavior with RAG versus an autonomous agent. UXLab allows researchers to focus on experimental design and analysis, supporting future multi-modal interaction research.", "AI": {"tldr": "UXLab\u662f\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\uff0c\u7b80\u5316\u4e86\u7528\u6237\u7814\u7a76\u7684\u8bbe\u8ba1\u548c\u5206\u6790\u8fc7\u7a0b\uff0c\u652f\u6301\u7814\u7a76\u4eba\u5458\u8fdb\u884c\u4e0d\u540c\u4fe1\u606f\u83b7\u53d6\u7cfb\u7edf\u7684\u6bd4\u8f83\u3002", "motivation": "\u968f\u7740\u4fe1\u606f\u83b7\u53d6\u7cfb\u7edf\u7684\u591a\u6837\u5316\uff0c\u8fdb\u884c\u6bd4\u8f83\u6027\u7528\u6237\u7814\u7a76\u7684\u9700\u6c42\u65e5\u76ca\u589e\u5f3a\uff0c\u4f46\u73b0\u6709\u7cfb\u7edf\u7684\u6280\u672f\u590d\u6742\u6027\u6210\u4e3a\u4e86\u4e00\u4e2a\u4e3b\u8981\u969c\u788d\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u57fa\u4e8e\u7f51\u7edc\u7684\u4eea\u8868\u76d8\uff0c\u5141\u8bb8\u7814\u7a76\u4eba\u5458\u76f4\u89c2\u7ba1\u7406\u7528\u6237\u7814\u7a76\u7684\u5404\u4e2a\u73af\u8282\u3002", "result": "UXLab\u901a\u8fc7\u5fae\u578b\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5728RAG\u4e0e\u81ea\u4e3b\u4ee3\u7406\u4e4b\u95f4\u6bd4\u8f83\u7528\u6237\u884c\u4e3a\u7684\u80fd\u529b\uff0c\u4ee5\u53ca\u5176\u5728\u7ba1\u7406\u590d\u6742\u5b9e\u9a8c\u8bbe\u8ba1\u4e0a\u7684\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "UXLab\u662f\u4e00\u79cd\u5c06\u7528\u6237\u4f53\u9a8c\u7814\u7a76\u7b80\u5316\u4e3a\u65e0\u4ee3\u7801\u914d\u7f6e\u7684\u5f00\u6e90\u7cfb\u7edf\uff0c\u80fd\u6709\u6548\u652f\u6301\u591a\u6a21\u6001\u4ea4\u4e92\u7814\u7a76\u548c\u5b9e\u9a8c\u8bbe\u8ba1\u3002"}}
{"id": "2601.09856", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.09856", "abs": "https://arxiv.org/abs/2601.09856", "authors": ["Andrew Stratton", "Phani Teja Singamaneni", "Pranav Goyal", "Rachid Alami", "Christoforos Mavrogiannis"], "title": "How Human Motion Prediction Quality Shapes Social Robot Navigation Performance in Constrained Spaces", "comment": null, "summary": "Motivated by the vision of integrating mobile robots closer to humans in warehouses, hospitals, manufacturing plants, and the home, we focus on robot navigation in dynamic and spatially constrained environments. Ensuring human safety, comfort, and efficiency in such settings requires that robots are endowed with a model of how humans move around them. Human motion prediction around robots is especially challenging due to the stochasticity of human behavior, differences in user preferences, and data scarcity. In this work, we perform a methodical investigation of the effects of human motion prediction quality on robot navigation performance, as well as human productivity and impressions. We design a scenario involving robot navigation among two human subjects in a constrained workspace and instantiate it in a user study ($N=80$) involving two different robot platforms, conducted across two sites from different world regions. Key findings include evidence that: 1) the widely adopted average displacement error is not a reliable predictor of robot navigation performance and human impressions; 2) the common assumption of human cooperation breaks down in constrained environments, with users often not reciprocating robot cooperation, and causing performance degradations; 3) more efficient robot navigation often comes at the expense of human efficiency and comfort.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4eba\u7c7b\u8fd0\u52a8\u9884\u6d4b\u5bf9\u673a\u5668\u4eba\u5bfc\u822a\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4f20\u7edf\u9884\u6d4b\u65b9\u6cd5\u5728\u7279\u5b9a\u73af\u5883\u4e0b\u6548\u679c\u4e0d\u4f73\uff0c\u5e76\u63ed\u793a\u4e86\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u4e92\u52a8\u4e2d\u7684\u6548\u7387\u548c\u8212\u9002\u5ea6\u6743\u8861\u3002", "motivation": "\u5c06\u79fb\u52a8\u673a\u5668\u4eba\u66f4\u597d\u5730\u6574\u5408\u5230\u4ed3\u5e93\u3001\u533b\u9662\u3001\u5236\u9020\u5de5\u5382\u548c\u5bb6\u5ead\u4e2d\uff0c\u4ee5\u786e\u4fdd\u4eba\u7c7b\u5b89\u5168\u3001\u8212\u9002\u548c\u6548\u7387", "method": "\u9488\u5bf9\u52a8\u6001\u548c\u7a7a\u95f4\u53d7\u9650\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u5bfc\u822a\u8fdb\u884c\u7cfb\u7edf\u6027\u7814\u7a76", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5e73\u5747\u4f4d\u79fb\u8bef\u5dee\u5e76\u975e\u673a\u5668\u4eba\u5bfc\u822a\u6027\u80fd\u548c\u4eba\u7c7b\u5370\u8c61\u7684\u53ef\u9760\u9884\u6d4b\u6307\u6807\uff1b\u5728\u4eba\u7c7b\u5408\u4f5c\u7684\u5047\u8bbe\u5728\u53d7\u9650\u73af\u5883\u4e2d\u5931\u6548\uff0c\u4e14\u66f4\u9ad8\u6548\u7684\u673a\u5668\u4eba\u5bfc\u822a\u5f80\u5f80\u4ee5\u4eba\u7c7b\u6548\u7387\u548c\u8212\u9002\u4e3a\u4ee3\u4ef7\u3002", "conclusion": "\u4eba\u7c7b\u8fd0\u52a8\u9884\u6d4b\u8d28\u91cf\u5bf9\u673a\u5668\u4eba\u5bfc\u822a\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u5173\u6ce8\u4eba\u7c7b\u6548\u7387\u548c\u8212\u9002\u5ea6\u7684\u5e73\u8861\u3002"}}
{"id": "2601.10018", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10018", "abs": "https://arxiv.org/abs/2601.10018", "authors": ["Hasti Sharifi", "Homaira Huda Shomee", "Sourav Medya", "Debaleena Chattopadhyay"], "title": "Empowering Older Adults in Digital Technology Use with Foundation Models", "comment": null, "summary": "While high-quality technology support can assist older adults in using digital applications, many struggle to articulate their issues due to unfamiliarity with technical terminology and age-related cognitive changes. This study examines these communication challenges and explores AI-based approaches to mitigate them. We conducted a diary study with English-speaking, community-dwelling older adults to collect asynchronous, technology-related queries and used reflexive thematic analysis to identify communication barriers. To address these barriers, we evaluated how foundation models can paraphrase older adults' queries to improve solution accuracy. Two controlled experiments followed: one with younger adults evaluating AI-rephrased queries and another with older adults evaluating AI-generated solutions. We also developed a pipeline using large language models to generate the first synthetic dataset of how older adults request tech support (OATS). We identified four key communication challenges: verbosity, incompleteness, over-specification, and under-specification. Our prompt-chaining approach using the large language model, GPT-4o, elicited contextual details, paraphrased the original query, and generated a solution. AI-rephrased queries significantly improved solution accuracy (69% vs. 46%) and Google search results (69% vs. 35%). Younger adults better understood AI-rephrased queries (93.7% vs. 65.8%) and reported greater confidence and ease. Older adults reported high perceived ability to answer contextual questions (89.8%) and follow solutions (94.7%), with high confidence and ease. OATS demonstrated strong fidelity and face validity. This work shows how foundation models can enhance technology support for older adults by addressing age-related communication barriers. The OATS dataset offers a scalable resource for developing equitable AI systems that better serve aging populations.", "AI": {"tldr": "\u8001\u5e74\u4eba\u56e0\u6280\u672f\u672f\u8bed\u548c\u8ba4\u77e5\u53d8\u5316\u800c\u9762\u4e34\u6c9f\u901a\u56f0\u96be\uff0c\u7814\u7a76\u63a2\u8ba8\u4e86\u901a\u8fc7AI\u6765\u6539\u5584\u8fd9\u79cd\u60c5\u51b5\uff0c\u53d1\u73b0AI\u6539\u5199\u67e5\u8be2\u80fd\u63d0\u9ad8\u89e3\u51b3\u65b9\u6848\u7684\u51c6\u786e\u6027\u548c\u8001\u5e74\u4eba\u5bf9\u6280\u672f\u7684\u7406\u89e3\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u8001\u5e74\u4eba\u5728\u5bfb\u6c42\u6280\u672f\u652f\u6301\u65f6\u7684\u6c9f\u901a\u969c\u788d\uff0c\u7814\u7a76\u65e8\u5728\u63d0\u5347\u4ed6\u4eec\u7684\u53c2\u4e0e\u611f\u548c\u7406\u89e3\u80fd\u529b\u3002", "method": "\u7814\u7a76\u5206\u4e3a\u65e5\u8bb0\u7814\u7a76\u548c\u63a7\u5236\u5b9e\u9a8c\uff0c\u91c7\u7528\u53cd\u601d\u6027\u4e3b\u9898\u5206\u6790\u8bc6\u522b\u6c9f\u901a\u969c\u788d\uff0c\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\u4ee5\u652f\u6301AI\u6539\u5199\u3002", "result": "\u6b64\u7814\u7a76\u63ed\u793a\u4e86\u8001\u5e74\u4eba\u5728\u4f7f\u7528\u6570\u5b57\u6280\u672f\u65f6\u9762\u4e34\u7684\u6c9f\u901a\u969c\u788d\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8eAI\u7684\u89e3\u51b3\u65b9\u6848\u3002\u901a\u8fc7\u65e5\u8bb0\u7814\u7a76\u548c\u63a7\u5236\u5b9e\u9a8c\uff0c\u7814\u7a76\u53d1\u73b0AI\u6539\u5199\u67e5\u8be2\u663e\u8457\u63d0\u9ad8\u4e86\u89e3\u51b3\u65b9\u6848\u7684\u51c6\u786e\u6027\u548c\u8001\u5e74\u4eba\u7684\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u57fa\u7840\u6a21\u578b\u5982\u4f55\u901a\u8fc7\u89e3\u51b3\u4e0e\u5e74\u9f84\u76f8\u5173\u7684\u6c9f\u901a\u969c\u788d\u6765\u589e\u5f3a\u8001\u5e74\u4eba\u7684\u6280\u672f\u652f\u6301\u80fd\u529b\uff0c\u63d0\u51fa\u7684OATS\u6570\u636e\u96c6\u4e3a\u5f00\u53d1\u516c\u5e73\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8d44\u6e90\u3002"}}
{"id": "2601.09920", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.09920", "abs": "https://arxiv.org/abs/2601.09920", "authors": ["Ruopeng Huang", "Boyu Yang", "Wenlong Gui", "Jeremy Morgan", "Erdem Biyik", "Jiachen Li"], "title": "SyncTwin: Fast Digital Twin Construction and Synchronization for Safe Robotic Grasping", "comment": null, "summary": "Accurate and safe grasping under dynamic and visually occluded conditions remains a core challenge in real-world robotic manipulation. We present SyncTwin, a digital twin framework that unifies fast 3D scene reconstruction and real-to-sim synchronization for robust and safety-aware grasping in such environments. In the offline stage, we employ VGGT to rapidly reconstruct object-level 3D assets from RGB images, forming a reusable geometry library for simulation. During execution, SyncTwin continuously synchronizes the digital twin by tracking real-world object states via point cloud segmentation updates and aligning them through colored-ICP registration. The updated twin enables motion planners to compute collision-free and dynamically feasible trajectories in simulation, which are safely executed on the real robot through a closed real-to-sim-to-real loop. Experiments in dynamic and occluded scenes show that SyncTwin improves grasp accuracy and motion safety, demonstrating the effectiveness of digital-twin synchronization for real-world robotic execution.", "AI": {"tldr": "SyncTwin \u662f\u4e00\u4e2a\u6570\u5b57\u53cc\u80de\u80ce\u6846\u67b6\uff0c\u901a\u8fc7\u5feb\u901f\u7684 3D \u573a\u666f\u91cd\u6784\u4e0e\u771f\u5b9e-\u6a21\u62df\u540c\u6b65\uff0c\u5728\u52a8\u6001\u548c\u89c6\u89c9\u906e\u6321\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u5b89\u5168\u53ef\u9760\u7684\u6293\u53d6\u3002", "motivation": "\u5728\u52a8\u6001\u548c\u89c6\u89c9\u906e\u6321\u6761\u4ef6\u4e0b\u8fdb\u884c\u51c6\u786e\u548c\u5b89\u5168\u7684\u6293\u53d6\u662f\u73b0\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u6838\u5fc3\u6311\u6218\u3002", "method": "\u4f7f\u7528 VGGT \u4ece RGB \u56fe\u50cf\u5feb\u901f\u91cd\u6784\u5bf9\u8c61\u7ea7 3D \u8d44\u4ea7\uff0c\u6784\u5efa\u53ef\u91cd\u590d\u4f7f\u7528\u7684\u51e0\u4f55\u5e93\uff1b\u901a\u8fc7\u70b9\u4e91\u5206\u5272\u66f4\u65b0\u8ddf\u8e2a\u771f\u5b9e\u4e16\u754c\u5bf9\u8c61\u72b6\u6001\uff0c\u5e76\u901a\u8fc7\u5f69\u8272 ICP \u6ce8\u518c\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u8868\u660e SyncTwin \u5728\u52a8\u6001\u548c\u906e\u6321\u573a\u666f\u4e0b\u63d0\u9ad8\u4e86\u6293\u53d6\u7cbe\u5ea6\u548c\u8fd0\u52a8\u5b89\u5168\u6027\uff0c\u8ba9\u673a\u5668\u4eba\u53ef\u4ee5\u5b89\u5168\u6267\u884c\u89c4\u5212\u7684\u8f68\u8ff9\u3002", "conclusion": "SyncTwin \u63d0\u9ad8\u4e86\u5728\u52a8\u6001\u548c\u906e\u6321\u573a\u666f\u4e0b\u7684\u6293\u53d6\u51c6\u786e\u6027\u548c\u8fd0\u52a8\u5b89\u5168\u6027\uff0c\u9a8c\u8bc1\u4e86\u6570\u5b57\u53cc\u80de\u80ce\u540c\u6b65\u5728\u73b0\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u6267\u884c\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.10232", "categories": ["cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.10232", "abs": "https://arxiv.org/abs/2601.10232", "authors": ["Choro Ulan uulu", "Mikhail Kulyabin", "Katharina M Zeiner", "Jan Joosten", "Nuno Miguel Martins Pacheco", "Filippos Petridis", "Rebecca Johnson", "Jan Bosch", "Helena Holmstr\u00f6m Olsson"], "title": "Tables or Sankey Diagrams? Investigating User Interaction with Different Representations of Simulation Parameters", "comment": null, "summary": "Understanding complex parameter dependencies is critical for effective configuration and maintenance of software systems across diverse domains - from Computer-Aided Engineering (CAE) to cloud infrastructure and database management. However, legacy tabular interfaces create a major bottleneck: engineers cannot easily comprehend how parameters relate across the system, leading to inefficient workflows, costly configuration errors, and reduced system trust - a fundamental program comprehension challenge in configuration-intensive software. This research evaluates whether interactive Sankey diagrams can improve comprehension of parameter dependencies compared to traditional spreadsheet interfaces. We employed a heuristic evaluation using the PURE method with three expert evaluators (UX design, simulation, and software development specialists) to compare a Sankey-based prototype to traditional tabular representations for core engineering tasks. Our key contribution demonstrates that flow-based parameter visualizations significantly reduce cognitive load (51% lower PURE scores) and interaction complexity (56% fewer steps) compared to traditional tables, while making parameter dependencies immediately visible rather than requiring mental reconstruction. By explicitly visualizing parameter relationships, Sankey diagrams address a core software visualization challenge: helping users comprehend complex system configurations without requiring deep tool-specific knowledge. While demonstrated through CAE software, this research contributes to program comprehension and software visualization by showing that dependency-aware visualizations can significantly improve understanding of configuration-intensive systems. The findings have implications for any software domain where comprehending complex parameter relationships is essential for effective system use and maintenance.", "AI": {"tldr": "\u901a\u8fc7\u4f7f\u7528\u4ea4\u4e92\u5f0fSankey\u56fe\uff0c\u53ef\u663e\u8457\u63d0\u9ad8\u5bf9\u590d\u6742\u8f6f\u4ef6\u7cfb\u7edf\u4e2d\u53c2\u6570\u4f9d\u8d56\u5173\u7cfb\u7684\u7406\u89e3\uff0c\u6539\u5584\u914d\u7f6e\u548c\u7ef4\u62a4\u6548\u7387\u3002", "motivation": "\u7406\u89e3\u590d\u6742\u53c2\u6570\u4f9d\u8d56\u5173\u7cfb\u5bf9\u4e8e\u8f6f\u4ef6\u7cfb\u7edf\u7684\u914d\u7f6e\u548c\u7ef4\u62a4\u81f3\u5173\u91cd\u8981\uff0c\u7136\u800c\u4f20\u7edf\u8868\u683c\u754c\u9762\u5bfc\u81f4\u7684\u7406\u89e3\u56f0\u96be\u5f71\u54cd\u4e86\u5de5\u4f5c\u6548\u7387\u548c\u7cfb\u7edf\u4fe1\u4efb\u3002", "method": "\u91c7\u7528Sankey\u56fe\u8fdb\u884c\u53c2\u6570\u4f9d\u8d56\u5173\u7cfb\u53ef\u89c6\u5316\uff0c\u5e76\u4e0e\u4f20\u7edf\u8868\u683c\u63a5\u53e3\u8fdb\u884c\u6bd4\u8f83", "result": "\u4f7f\u7528Sankey\u56fe\u7684\u53ef\u89c6\u5316\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u8ba4\u77e5\u8d1f\u62c5\uff08PURE\u5f97\u5206\u964d\u4f4e51%\uff09\u548c\u4ea4\u4e92\u590d\u6742\u6027\uff08\u6b65\u9aa4\u51cf\u5c1156%\uff09\uff0c\u4f7f\u53c2\u6570\u4f9d\u8d56\u5173\u7cfb\u5373\u65f6\u53ef\u89c1\u3002", "conclusion": "Sankey\u56fe\u4f5c\u4e3a\u4f9d\u8d56\u5173\u7cfb\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u80fd\u591f\u5e2e\u52a9\u7528\u6237\u7406\u89e3\u590d\u6742\u7684\u7cfb\u7edf\u914d\u7f6e\uff0c\u63d0\u9ad8\u7a0b\u5e8f\u7406\u89e3\u548c\u8f6f\u4ef6\u53ef\u89c6\u5316\u6548\u679c\uff0c\u5bf9\u4efb\u4f55\u9700\u8981\u7406\u89e3\u590d\u6742\u53c2\u6570\u5173\u7cfb\u7684\u8f6f\u4ef6\u9886\u57df\u90fd\u6709\u79ef\u6781\u5f71\u54cd\u3002"}}
{"id": "2601.09988", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.09988", "abs": "https://arxiv.org/abs/2601.09988", "authors": ["Hojung Choi", "Yifan Hou", "Chuer Pan", "Seongheon Hong", "Austin Patel", "Xiaomeng Xu", "Mark R. Cutkosky", "Shuran Song"], "title": "In-the-Wild Compliant Manipulation with UMI-FT", "comment": "submitted to ICRA 2026", "summary": "Many manipulation tasks require careful force modulation. With insufficient force the task may fail, while excessive force could cause damage. The high cost, bulky size and fragility of commercial force/torque (F/T) sensors have limited large-scale, force-aware policy learning. We introduce UMI-FT, a handheld data-collection platform that mounts compact, six-axis force/torque sensors on each finger, enabling finger-level wrench measurements alongside RGB, depth, and pose. Using the multimodal data collected from this device, we train an adaptive compliance policy that predicts position targets, grasp force, and stiffness for execution on standard compliance controllers. In evaluations on three contact-rich, force-sensitive tasks (whiteboard wiping, skewering zucchini, and lightbulb insertion), UMI-FT enables policies that reliably regulate external contact forces and internal grasp forces, outperforming baselines that lack compliance or force sensing. UMI-FT offers a scalable path to learning compliant manipulation from in-the-wild demonstrations. We open-source the hardware and software to facilitate broader adoption at:https://umi-ft.github.io/.", "AI": {"tldr": "UMI-FT \u662f\u4e00\u79cd\u4fbf\u643a\u5f0f\u6570\u636e\u91c7\u96c6\u5e73\u53f0\uff0c\u901a\u8fc7\u5728\u624b\u6307\u4e0a\u5b89\u88c5\u516d\u8f74\u529b/\u626d\u77e9\u4f20\u611f\u5668\uff0c\u6536\u96c6\u591a\u6a21\u6001\u6570\u636e\u4ee5\u8bad\u7ec3\u81ea\u9002\u5e94\u5408\u89c4\u7b56\u7565\uff0c\u4ece\u800c\u5728\u529b\u654f\u611f\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u5e76\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u5546\u4e1a\u529b/\u626d\u77e9\u4f20\u611f\u5668\u6210\u672c\u9ad8\u3001\u4f53\u79ef\u5927\u4e14\u6613\u635f\uff0c\u9650\u5236\u4e86\u5927\u89c4\u6a21\u7684\u529b\u611f\u77e5\u7b56\u7565\u5b66\u4e60\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u624b\u6301\u6570\u636e\u91c7\u96c6\u5e73\u53f0 UMI-FT\uff0c\u5728\u6bcf\u4e2a\u624b\u6307\u4e0a\u5b89\u88c5\u7d27\u51d1\u7684\u516d\u8f74\u529b/\u626d\u77e9\u4f20\u611f\u5668\uff0c\u6536\u96c6\u591a\u6a21\u6001\u6570\u636e\u4ee5\u8bad\u7ec3\u81ea\u9002\u5e94\u5408\u89c4\u7b56\u7565\u3002", "result": "\u5728\u4e09\u4e2a\u63a5\u89e6\u4e30\u5bcc\u3001\u5bf9\u529b\u654f\u611f\u7684\u4efb\u52a1\u4e2d\uff08\u64e6\u767d\u677f\u3001\u523a\u5165\u897f\u846b\u82a6\u548c\u706f\u6ce1\u63d2\u5165\uff09\uff0cUMI-FT \u7684\u7b56\u7565\u80fd\u591f\u53ef\u9760\u5730\u8c03\u8282\u5916\u90e8\u63a5\u89e6\u529b\u548c\u5185\u90e8\u6293\u6301\u529b\uff0c\u8d85\u8d8a\u4ec5\u4f9d\u8d56\u5408\u89c4\u6216\u529b\u611f\u77e5\u7684\u57fa\u7ebf\u5bf9\u6bd4\u3002", "conclusion": "UMI-FT \u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u7136\u73af\u5883\u4e2d\u7684\u6f14\u793a\u5b66\u4e60\u54cd\u5e94\u7684\u64cd\u63a7\u7b56\u7565\uff0c\u540c\u65f6\u5f00\u6e90\u786c\u4ef6\u548c\u8f6f\u4ef6\u4ee5\u4fc3\u8fdb\u66f4\u5e7f\u6cdb\u7684\u91c7\u7528\u3002"}}
{"id": "2601.10236", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10236", "abs": "https://arxiv.org/abs/2601.10236", "authors": ["Bohan Zhang", "Chengke Bu", "Paramveer S. Dhillon"], "title": "Who Owns the Text? Design Patterns for Preserving Authorship in AI-Assisted Writing", "comment": "Preprint; 42 pages", "summary": "AI writing assistants can reduce effort and improve fluency, but they may also weaken writers' sense of authorship. We study this tension with an ownership-aware co-writing editor that offers on-demand, sentence-level suggestions and tests two common design choices: persona-based coaching and style personalization. In an online study (N=176), participants completed three professional writing tasks: an email without AI help, a proposal with generic AI suggestions, and a cover letter with persona-based coaching, while half received suggestions tailored to a brief sample of their prior writing. Across the two AI-assisted tasks, psychological ownership dropped relative to unassisted writing (about 0.85-1.0 points on a 7-point scale), even as cognitive load decreased (about 0.9 points) and quality ratings stayed broadly similar overall. Persona coaching did not prevent the ownership decline. Style personalization partially restored ownership (about +0.43) and increased AI incorporation in text (+5 percentage points). We distill five design patterns: on-demand initiation, micro-suggestions, voice anchoring, audience scaffolds, and point-of-decision provenance, to guide authorship-preserving writing tools.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u63a2\u8ba8\u4e86AI\u5199\u4f5c\u52a9\u624b\u5bf9\u4f5c\u5bb6\u6240\u6709\u6743\u611f\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4e2a\u6027\u5316\u98ce\u683c\u5efa\u8bae\u80fd\u90e8\u5206\u6062\u590d\u6240\u6709\u6743\u611f\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u63a2\u8ba8AI\u5199\u4f5c\u5de5\u5177\u5bf9\u4f5c\u5bb6\u5f52\u5c5e\u611f\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u5982\u4f55\u8bbe\u8ba1\u8fd9\u4e9b\u5de5\u5177\u4ee5\u589e\u5f3a\u4f5c\u8005\u7684\u521b\u4f5c\u6240\u6709\u6743\u3002", "method": "\u53c2\u4e0e\u8005\u5728\u6ca1\u6709AI\u5e2e\u52a9\u3001\u4f7f\u7528\u666e\u901aAI\u5efa\u8bae\u548c\u4f7f\u7528\u4e2a\u6027\u5316\u6307\u5bfc\u7684\u60c5\u51b5\u4e0b\u5b8c\u6210\u4e09\u9879\u5199\u4f5c\u4efb\u52a1\uff0c\u6bd4\u8f83\u4e86\u5fc3\u7406\u6240\u6709\u6743\u548c\u8ba4\u77e5\u8d1f\u62c5\u7684\u53d8\u5316\u3002", "result": "\u8fd9\u9879\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u5177\u6709\u6240\u6709\u6743\u610f\u8bc6\u7684\u534f\u4f5c\u7f16\u8f91\u5de5\u5177\uff0c\u63a2\u8ba8\u4e86AI\u5199\u4f5c\u52a9\u624b\u5bf9\u4f5c\u5bb6\u8457\u4f5c\u6743\u611f\u7684\u5f71\u54cd\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5c3d\u7ba1AI\u5e2e\u52a9\u51cf\u8f7b\u4e86\u8ba4\u77e5\u8d1f\u62c5\uff0c\u4f46\u4f5c\u5bb6\u7684\u5fc3\u7406\u6240\u6709\u6743\u611f\u4e0b\u964d\u3002\u4e2a\u6027\u5316\u7684\u98ce\u683c\u5efa\u8bae\u80fd\u90e8\u5206\u6062\u590d\u6240\u6709\u6743\u611f\uff0c\u63d0\u4f9b\u4e86\u4e94\u4e2a\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u4ee5\u6307\u5bfc\u672a\u6765\u7684\u5199\u4f5c\u5de5\u5177\u3002", "conclusion": "AI\u52a9\u624b\u867d\u7136\u63d0\u5347\u4e86\u5199\u4f5c\u6d41\u7545\u5ea6\u548c\u51cf\u5c11\u4e86\u8ba4\u77e5\u8d1f\u62c5\uff0c\u4f46\u5374\u964d\u4f4e\u4e86\u4f5c\u5bb6\u7684\u6240\u6709\u6743\u611f\u3002\u901a\u8fc7\u8bbe\u8ba1\u7279\u5b9a\u7684\u529f\u80fd\uff0c\u53ef\u4ee5\u5e2e\u52a9\u4f5c\u5bb6\u4fdd\u6301\u5bf9\u4f5c\u54c1\u7684\u63a7\u5236\u611f\u3002"}}
{"id": "2601.10116", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.10116", "abs": "https://arxiv.org/abs/2601.10116", "authors": ["Xintong Zhang", "Junfeng Chen", "Yuxiao Zhu", "Bing Luo", "Meng Guo"], "title": "CoCoPlan: Adaptive Coordination and Communication for Multi-robot Systems in Dynamic and Unknown Environments", "comment": "8 pages, 8 figures, published to RA-L", "summary": "Multi-robot systems can greatly enhance efficiency through coordination and collaboration, yet in practice, full-time communication is rarely available and interactions are constrained to close-range exchanges. Existing methods either maintain all-time connectivity, rely on fixed schedules, or adopt pairwise protocols, but none adapt effectively to dynamic spatio-temporal task distributions under limited communication, resulting in suboptimal coordination. To address this gap, we propose CoCoPlan, a unified framework that co-optimizes collaborative task planning and team-wise intermittent communication. Our approach integrates a branch-and-bound architecture that jointly encodes task assignments and communication events, an adaptive objective function that balances task efficiency against communication latency, and a communication event optimization module that strategically determines when, where and how the global connectivity should be re-established. Extensive experiments demonstrate that it outperforms state-of-the-art methods by achieving a 22.4% higher task completion rate, reducing communication overhead by 58.6%, and improving the scalability by supporting up to 100 robots in dynamic environments. Hardware experiments include the complex 2D office environment and large-scale 3D disaster-response scenario.", "AI": {"tldr": "CoCoPlan \u662f\u4e00\u4e2a\u4f18\u5316\u591a\u673a\u5668\u4eba\u4efb\u52a1\u548c\u901a\u4fe1\u7684\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u5b8c\u6210\u7387\u5e76\u964d\u4f4e\u4e86\u901a\u4fe1\u8d1f\u62c5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u4efb\u52a1\u5206\u914d\u548c\u6709\u9650\u901a\u4fe1\u6761\u4ef6\u4e0b\u6709\u6548\u534f\u8c03\u7684\u4e0d\u8db3\u4e4b\u5904\u3002", "method": "\u91c7\u7528\u5206\u652f\u9650\u754c\u67b6\u6784\u7f16\u7801\u4efb\u52a1\u5206\u914d\u548c\u901a\u4fe1\u4e8b\u4ef6\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u76ee\u6807\u51fd\u6570\u548c\u901a\u4fe1\u4e8b\u4ef6\u4f18\u5316\u6a21\u5757\u3002", "result": "CoCoPlan \u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5728\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u6709\u6548\u534f\u8c03\u4efb\u52a1\u548c\u6709\u9650\u7684\u901a\u4fe1\uff0c\u4ece\u800c\u63d0\u9ad8\u4efb\u52a1\u5b8c\u6210\u7387\u548c\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\u3002", "conclusion": "CoCoPlan \u5728\u52a8\u6001\u73af\u5883\u4e2d\u6709\u6548\u652f\u6301\u591a\u8fbe100\u4e2a\u673a\u5668\u4eba\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u66f4\u9ad8\u7684\u4efb\u52a1\u5b8c\u6210\u7387\u548c\u66f4\u4f4e\u7684\u901a\u4fe1\u5f00\u9500\u3002"}}
{"id": "2601.10253", "categories": ["cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.10253", "abs": "https://arxiv.org/abs/2601.10253", "authors": ["Nadine Kuo", "Agnia Sergeyuk", "Valerie Chen", "Maliheh Izadi"], "title": "Developer Interaction Patterns with Proactive AI: A Five-Day Field Study", "comment": "14 pages, 6 figures, accepted to IUI'26", "summary": "Current in-IDE AI coding tools typically rely on time-consuming manual prompting and context management, whereas proactive alternatives that anticipate developer needs without explicit invocation remain underexplored. Understanding when humans are receptive to such proactive AI assistance during their daily work remains an open question in human-AI interaction research. We address this gap through a field study of proactive AI assistance in professional developer workflows. We present a five-day in-the-wild study with 15 developers who interacted with a proactive feature of an AI assistant integrated into a production-grade IDE that offers code quality suggestions based on in-IDE developer activity. We examined 229 AI interventions across 5,732 interaction points to understand how proactive suggestions are received across workflow stages, how developers experience them, and their perceived impact. Our findings reveal systematic patterns in human receptivity to proactive suggestions: interventions at workflow boundaries (e.g., post-commit) achieved 52% engagement rates, while mid-task interventions (e.g., on declined edit) were dismissed 62% of the time. Notably, well-timed proactive suggestions required significantly less interpretation time than reactive suggestions (45.4s versus 101.4s, W = 109.00, r = 0.533, p = 0.0016), indicating enhanced cognitive alignment. This study provides actionable implications for designing proactive coding assistants, including how to time interventions, align them with developer context, and strike a balance between AI agency and user control in production IDEs.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u7814\u7a76\u5f00\u53d1\u8005\u5728\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u53cd\u5e94\uff0c\u63a2\u8ba8\u4e86\u524d\u77bb\u6027AI\u5efa\u8bae\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u9002\u65f6\u7684\u5efa\u8bae\u80fd\u663e\u8457\u63d0\u9ad8\u5f00\u53d1\u8005\u7684\u63a5\u53d7\u5ea6\u3002", "motivation": "\u5f53\u524d\u7684IDE\u5185AI\u7f16\u7801\u5de5\u5177\u901a\u5e38\u4f9d\u8d56\u8017\u65f6\u7684\u624b\u52a8\u63d0\u793a\u548c\u4e0a\u4e0b\u6587\u7ba1\u7406\uff0c\u800c\u5c11\u6709\u524d\u77bb\u6027\u66ff\u4ee3\u65b9\u6848\u53ef\u4ee5\u5728\u4e0d\u660e\u786e\u8bf7\u6c42\u7684\u60c5\u51b5\u4e0b\u9884\u89c1\u5f00\u53d1\u8005\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u89c2\u5bdf229\u4e2aAI\u5e72\u9884\u4e8b\u4ef6\u57285732\u4e2a\u4ea4\u4e92\u70b9\u7684\u6548\u679c\uff0c\u5206\u6790\u4e86\u5f00\u53d1\u8005\u5982\u4f55\u63a5\u53d7\u524d\u77bb\u6027\u5efa\u8bae\uff0c\u5e76\u6d4b\u91cf\u4e86\u5efa\u8bae\u7684\u65f6\u673a\u5bf9\u89e3\u91ca\u65f6\u95f4\u7684\u5f71\u54cd\u3002", "result": "\u901a\u8fc7\u5bf915\u540d\u5f00\u53d1\u8005\u8fdb\u884c\u4e3a\u671f\u4e94\u5929\u7684\u5b9e\u5730\u7814\u7a76\uff0c\u8bc4\u4f30\u4e86\u5728\u751f\u4ea7\u7ea7IDE\u4e2d\u96c6\u6210\u7684AI\u52a9\u624b\u7684\u524d\u77bb\u6027\u529f\u80fd\u5bf9\u4ee3\u7801\u8d28\u91cf\u5efa\u8bae\u7684\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u8bbe\u8ba1\u524d\u77bb\u6027\u7f16\u7801\u52a9\u624b\u65f6\uff0c\u5e72\u9884\u7684\u65f6\u673a\u3001\u4e0e\u5f00\u53d1\u8005\u4e0a\u4e0b\u6587\u7684\u5bf9\u9f50\u4ee5\u53caAI\u4ee3\u7406\u4e0e\u7528\u6237\u63a7\u5236\u4e4b\u95f4\u7684\u5e73\u8861\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2601.10208", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.10208", "abs": "https://arxiv.org/abs/2601.10208", "authors": ["Shuangshan Nors Li", "J. Nathan Kutz"], "title": "Terrain-Adaptive Mobile 3D Printing with Hierarchical Control", "comment": "Submitted to the 43rd International Symposium on Automation and Robotics in Construction (ISARC 2026)", "summary": "Mobile 3D printing on unstructured terrain remains challenging due to the conflict between platform mobility and deposition precision. Existing gantry-based systems achieve high accuracy but lack mobility, while mobile platforms struggle to maintain print quality on uneven ground. We present a framework that tightly integrates AI-driven disturbance prediction with multi-modal sensor fusion and hierarchical hardware control, forming a closed-loop perception-learning-actuation system. The AI module learns terrain-to-perturbation mappings from IMU, vision, and depth sensors, enabling proactive compensation rather than reactive correction. This intelligence is embedded into a three-layer control architecture: path planning, predictive chassis-manipulator coordination, and precision hardware execution. Through outdoor experiments on terrain with slopes and surface irregularities, we demonstrate sub-centimeter printing accuracy while maintaining full platform mobility. This AI-hardware integration establishes a practical foundation for autonomous construction in unstructured environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210AI\u9a71\u52a8\u7684\u5e72\u6270\u9884\u6d4b\u4e0e\u591a\u6a21\u6001\u4f20\u611f\u5668\u878d\u5408\u7684\u95ed\u73af\u7cfb\u7edf\uff0c\u4ee5\u89e3\u51b3\u79fb\u52a83D\u6253\u5370\u5728\u4e0d\u89c4\u5219\u5730\u5f62\u4e0a\u7684\u7cbe\u5ea6\u4e0e\u673a\u52a8\u6027\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfgantry\u7cfb\u7edf\u7cbe\u5ea6\u9ad8\u4f46\u7f3a\u4e4f\u673a\u52a8\u6027\u4e0e\u79fb\u52a8\u5e73\u53f0\u5728\u4e0d\u5e73\u5766\u5730\u9762\u4e0a\u6253\u5370\u8d28\u91cf\u4e0d\u4f73\u4e4b\u95f4\u7684\u77db\u76fe\u3002", "method": "\u91c7\u7528AI\u6a21\u5757\u4e0eIMU\u3001\u89c6\u89c9\u53ca\u6df1\u5ea6\u4f20\u611f\u5668\u878d\u5408\uff0c\u6784\u5efa\u4e09\u5c42\u63a7\u5236\u67b6\u6784\uff0c\u8fdb\u884c\u8def\u5f84\u89c4\u5212\u3001\u9884\u6d4b\u534f\u8c03\u53ca\u786c\u4ef6\u6267\u884c\u3002", "result": "\u6237\u5916\u5b9e\u9a8c\u8868\u660e\uff0c\u7cfb\u7edf\u5728\u5b58\u5728\u5761\u5ea6\u548c\u8868\u9762\u4e0d\u89c4\u5219\u6027\u7684\u5730\u5f62\u4e0a\u5b9e\u73b0\u4e86\u6253\u5370\u7cbe\u5ea6\u8fbe\u5230\u4e9a\u5398\u7c73\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u5728\u4e0d\u5e73\u5766\u5730\u5f62\u4e0a\u5b9e\u73b0\u4e9a\u5398\u7c73\u6253\u5370\u7cbe\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u5e73\u53f0\u7684\u5b8c\u5168\u673a\u52a8\u6027\uff0c\u4e3a\u4e0d\u89c4\u5219\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u65bd\u5de5\u5960\u5b9a\u4e86\u5b9e\u9645\u57fa\u7840\u3002"}}
{"id": "2601.10383", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.10383", "abs": "https://arxiv.org/abs/2601.10383", "authors": ["Marcel Gohsen", "Nicola Libera", "Johannes Kiesel", "Jan Ehlers", "Benno Stein"], "title": "Does Cognitive Load Affect Human Accuracy in Detecting Voice-Based Deepfakes?", "comment": "Accepted as full paper to CHIIR'26", "summary": "Deepfake technologies are powerful tools that can be misused for malicious purposes such as spreading disinformation on social media. The effectiveness of such malicious applications depends on the ability of deepfakes to deceive their audience. Therefore, researchers have investigated human abilities to detect deepfakes in various studies. However, most of these studies were conducted with participants who focused exclusively on the detection task; hence the studies may not provide a complete picture of human abilities to detect deepfakes under realistic conditions: Social media users are exposed to cognitive load on the platform, which can impair their detection abilities. In this paper, we investigate the influence of cognitive load on human detection abilities of voice-based deepfakes in an empirical study with 30 participants. Our results suggest that low cognitive load does not generally impair detection abilities, and that the simultaneous exposure to a secondary stimulus can actually benefit people in the detection task.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u8ba4\u77e5\u8d1f\u8377\u5bf9\u68c0\u6d4b\u6df1\u4f2a\u5f71\u50cf\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4f4e\u8ba4\u77e5\u8d1f\u8377\u4e0d\u635f\u5bb3\u68c0\u6d4b\u80fd\u529b\uff0c\u53cd\u800c\u53ef\u80fd\u6709\u6240\u5e2e\u52a9\u3002", "motivation": "\u4e86\u89e3\u793e\u4ea4\u5a92\u4f53\u7528\u6237\u5728\u8ba4\u77e5\u8d1f\u8377\u4e0b\u7684\u6df1\u4f2a\u5f71\u50cf\u68c0\u6d4b\u80fd\u529b", "method": "\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u5206\u6790\u8ba4\u77e5\u8d1f\u8377\u5bf9\u4eba\u7c7b\u68c0\u6d4b\u8bed\u97f3\u6df1\u4f2a\u5f71\u50cf\u7684\u5f71\u54cd", "result": "\u4f4e\u8ba4\u77e5\u8d1f\u8377\u5e76\u4e0d\u4f1a\u666e\u904d\u635f\u5bb3\u68c0\u6d4b\u80fd\u529b\uff0c\u540c\u65f6\u65f6\u63a5\u6536\u6b21\u8981\u523a\u6fc0\u53ef\u80fd\u6709\u52a9\u4e8e\u63d0\u5347\u68c0\u6d4b\u4efb\u52a1\u7684\u8868\u73b0", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u73af\u5883\u56e0\u7d20\u5982\u8ba4\u77e5\u8d1f\u8377\u5bf9\u6df1\u4f2a\u68c0\u6d4b\u80fd\u529b\u7684\u5f71\u54cd\u8f83\u4e3a\u590d\u6742\uff0c\u53ef\u80fd\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u4ea7\u751f\u79ef\u6781\u6548\u679c\u3002"}}
{"id": "2601.10225", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.10225", "abs": "https://arxiv.org/abs/2601.10225", "authors": ["Dongwook Kwak", "Geonhee Cho", "Jiook Chung", "Jinkyu Yang"], "title": "A Unified Framework for Kinematic Simulation of Rigid Foldable Structures", "comment": "34 pages (20 pages main text), 11 figures (7 in main text, 4 in appendix)", "summary": "Origami-inspired structures with rigid panels now span thick, kirigami, and multi-sheet realizations, making unified kinematic analysis essential. Yet a general method that consolidates their loop constraints has been lacking. We present an automated approach that generates the Pfaffian constraint matrix for arbitrary rigid foldable structures (RFS). From a minimally extended data schema, the tool constructs the facet-hinge graph, extracts a minimum cycle basis that captures all constraints, and assembles a velocity-level constraint matrix via screw theory that encodes coupled rotation and translation loop closure. The framework computes and visualizes deploy and fold motions across diverse RFS while eliminating tedious and error-prone constraint calculations.", "AI": {"tldr": "\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u5de5\u5177\uff0c\u7b80\u5316\u4e86\u521a\u6027\u6298\u53e0\u7ed3\u6784\u7684\u7ea6\u675f\u77e9\u9635\u751f\u6210\u8fc7\u7a0b\u3002", "motivation": "\u9700\u8981\u7edf\u4e00\u7684\u8fd0\u52a8\u5b66\u5206\u6790\u65b9\u6cd5\u6765\u5904\u7406\u5404\u79cd\u6298\u7eb8\u542f\u53d1\u7684\u7ed3\u6784\uff0c\u5c24\u5176\u662f\u521a\u6027\u6298\u53e0\u7ed3\u6784\u7684\u73af\u8def\u7ea6\u675f\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528\u6700\u5c0f\u6269\u5c55\u6570\u636e\u7ed3\u6784\u6784\u5efa\u9762-\u94f0\u63a5\u56fe\uff0c\u63d0\u53d6\u6700\u5c0f\u5faa\u73af\u57fa\u7840\u4ee5\u6355\u83b7\u6240\u6709\u7ea6\u675f\uff0c\u5e76\u901a\u8fc7\u87ba\u65cb\u7406\u8bba\u7ec4\u88c5\u901f\u5ea6\u7ea7\u7ea6\u675f\u77e9\u9635\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u751f\u6210\u4efb\u610f\u521a\u6027\u6298\u53e0\u7ed3\u6784\u7684Pfaffian\u7ea6\u675f\u77e9\u9635\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u8ba1\u7b97\u548c\u53ef\u89c6\u5316\u521a\u6027\u6298\u53e0\u7ed3\u6784\u7684\u5c55\u5f00\u548c\u6298\u53e0\u8fd0\u52a8\uff0c\u907f\u514d\u4e86\u7e41\u7410\u7684\u7ea6\u675f\u8ba1\u7b97\u3002"}}
{"id": "2601.10458", "categories": ["cs.HC", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.10458", "abs": "https://arxiv.org/abs/2601.10458", "authors": ["Raphael Buchm\u00fcller", "Dennis Collaris", "Linhao Meng", "Angelos Chatzimparmpas"], "title": "LangLasso: Interactive Cluster Descriptions through LLM Explanation", "comment": "This manuscript is accepted for publication in VIS 2025 VISxGenAI Workshop", "summary": "Dimensionality reduction is a powerful technique for revealing structure and potential clusters in data. However, as the axes are complex, non-linear combinations of features, they often lack semantic interpretability. Existing visual analytics (VA) methods support cluster interpretation through feature comparison and interactive exploration, but they require technical expertise and intense human effort. We present \\textit{LangLasso}, a novel method that complements VA approaches through interactive, natural language descriptions of clusters using large language models (LLMs). It produces human-readable descriptions that make cluster interpretation accessible to non-experts and allow integration of external contextual knowledge beyond the dataset. We systematically evaluate the reliability of these explanations and demonstrate that \\langlasso provides an effective first step for engaging broader audiences in cluster interpretation. The tool is available at https://langlasso.vercel.app", "AI": {"tldr": "LangLasso \u662f\u4e00\u79cd\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7fa4\u96c6\uff0c\u4fc3\u8fdb\u975e\u4e13\u5bb6\u7406\u89e3\u6570\u636e\u7ed3\u6784\u7684\u5de5\u5177", "motivation": "\u65e8\u5728\u63d0\u9ad8\u7fa4\u96c6\u89e3\u91ca\u7684\u53ef\u53ca\u6027\uff0c\u8ba9\u975e\u4e13\u5bb6\u4e5f\u80fd\u7406\u89e3\u6570\u636e\u7ed3\u6784\u548c\u6f5c\u5728\u7fa4\u96c6", "method": "LangLasso\uff0c\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u751f\u6210\u4e0e\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u5206\u6790\u65b9\u6cd5", "result": "LangLasso \u751f\u6210\u6613\u4e8e\u7406\u89e3\u7684\u7fa4\u96c6\u63cf\u8ff0\uff0c\u5e76\u7ecf\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u5176\u53ef\u9760\u6027", "conclusion": "LangLasso \u4f5c\u4e3a\u4e00\u79cd\u5de5\u5177\uff0c\u4e3a\u66f4\u5e7f\u6cdb\u53d7\u4f17\u7684\u7fa4\u96c6\u89e3\u91ca\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u521d\u6b65\u6b65\u9aa4\u3002"}}
{"id": "2601.10233", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.10233", "abs": "https://arxiv.org/abs/2601.10233", "authors": ["Yifan Xue", "Ze Zhang", "Knut \u00c5kesson", "Nadia Figueroa"], "title": "Proactive Local-Minima-Free Robot Navigation: Blending Motion Prediction with Safe Control", "comment": "Co-first authors: Yifan Xue and Ze Zhang", "summary": "This work addresses the challenge of safe and efficient mobile robot navigation in complex dynamic environments with concave moving obstacles. Reactive safe controllers like Control Barrier Functions (CBFs) design obstacle avoidance strategies based only on the current states of the obstacles, risking future collisions. To alleviate this problem, we use Gaussian processes to learn barrier functions online from multimodal motion predictions of obstacles generated by neural networks trained with energy-based learning. The learned barrier functions are then fed into quadratic programs using modulated CBFs (MCBFs), a local-minimum-free version of CBFs, to achieve safe and efficient navigation. The proposed framework makes two key contributions. First, it develops a prediction-to-barrier function online learning pipeline. Second, it introduces an autonomous parameter tuning algorithm that adapts MCBFs to deforming, prediction-based barrier functions. The framework is evaluated in both simulations and real-world experiments, consistently outperforming baselines and demonstrating superior safety and efficiency in crowded dynamic environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u969c\u788d\u51fd\u6570\u548c\u81ea\u52a8\u53c2\u6570\u8c03\u8282\u7b97\u6cd5\uff0c\u63d0\u9ad8\u79fb\u52a8\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "motivation": "\u79fb\u52a8\u673a\u5668\u4eba\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u5b89\u5168\u9ad8\u6548\u5bfc\u822a\u7684\u6311\u6218", "method": "\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u80fd\u91cf\u57fa\u7840\u5b66\u4e60\uff0c\u5728\u7ebf\u751f\u6210\u969c\u788d\u7269\u7684\u591a\u6a21\u6001\u8fd0\u52a8\u9884\u6d4b\uff0c\u5e76\u5229\u7528\u8c03\u5236\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u6784\u5efa\u5b89\u5168\u9ad8\u6548\u7684\u5bfc\u822a\u7b56\u7565\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\u7684\u5728\u7ebf\u5b66\u4e60\u969c\u788d\u51fd\u6570\u7684\u65b9\u6cd5\uff0c\u4ee5\u53ca\u81ea\u4e3b\u53c2\u6570\u8c03\u8282\u7b97\u6cd5\uff0c\u4ee5\u9002\u5e94\u53d8\u5f62\u7684\u9884\u6d4b\u57fa\u969c\u788d\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u51c6\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2601.10467", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.10467", "abs": "https://arxiv.org/abs/2601.10467", "authors": ["Kazi Noshin", "Syed Ishtiaque Ahmed", "Sharifa Sultana"], "title": "AI Sycophancy: How Users Flag and Respond", "comment": null, "summary": "While concerns about LLM sycophancy have grown among researchers and developers, how users themselves experience this behavior remains largely unexplored. We analyze Reddit discussions to investigate how users detect, mitigate, and perceive sycophantic AI. We develop the ODR Framework that maps user experiences across three stages: observing sycophantic behaviors, detecting sycophancy, and responding to these behaviors. Our findings reveal that users employ various detection techniques, including cross-platform comparison and inconsistency testing. We document diverse mitigation approaches, such as persona-based prompts to specific language patterns in prompt engineering. We find sycophancy's effects are context-dependent rather than universally harmful. Specifically, vulnerable populations experiencing trauma, mental health challenges, or isolation actively seek and value sycophantic behaviors as emotional support. Users develop both technical and folk explanations for why sycophancy occurs. These findings challenge the assumption that sycophancy should be eliminated universally. We conclude by proposing context-aware AI design that balances the risks with the benefits of affirmative interaction, while discussing implications for user education and transparency.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u7528\u6237\u5982\u4f55\u8bc6\u522b\u548c\u5e94\u5bf9\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u963f\u8c00\u884c\u4e3a\uff0c\u63d0\u51faODR\u6846\u67b6\uff0c\u53d1\u73b0\u963f\u8c00\u884c\u4e3a\u5f71\u54cd\u56e0\u60c5\u5883\u4e0d\u540c\u800c\u5f02\u3002", "motivation": "\u5c3d\u7ba1\u7814\u7a76\u8005\u5bf9LMM\u963f\u8c00\u884c\u4e3a\u7684\u62c5\u5fe7\u52a0\u5267\uff0c\u4f46\u7528\u6237\u5982\u4f55\u4f53\u9a8c\u8fd9\u4e9b\u884c\u4e3a\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u8ba8\u3002", "method": "\u901a\u8fc7\u5206\u6790Reddit\u8ba8\u8bba\uff0c\u7814\u7a76\u7528\u6237\u5728\u4e0eAI\u4e92\u52a8\u4e2d\u7684\u7ecf\u9a8c\u548c\u53cd\u5e94\uff0c\u63d0\u51faODR\u6846\u67b6\u3002", "result": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u7528\u6237\u5728\u4f7f\u7528\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08LLM\uff09\u65f6\u9047\u5230\u7684\u963f\u8c00\u884c\u4e3a\u53ca\u5176\u4f53\u9a8c\u3002\u6211\u4eec\u5f00\u53d1\u4e86ODR\u6846\u67b6\uff0c\u7ec6\u5206\u7528\u6237\u4f53\u9a8c\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a\u89c2\u5bdf\u963f\u8c00\u884c\u4e3a\u3001\u68c0\u6d4b\u963f\u8c00\u884c\u4e3a\u53ca\u5e94\u5bf9\u8fd9\u4e9b\u884c\u4e3a\u3002\u7814\u7a76\u663e\u793a\uff0c\u7528\u6237\u4f7f\u7528\u591a\u79cd\u68c0\u6d4b\u6280\u672f\u4e0e\u7f13\u89e3\u7b56\u7565\uff0c\u4e14\u963f\u8c00\u884c\u4e3a\u7684\u5f71\u54cd\u56e0\u4e0a\u4e0b\u6587\u800c\u5f02\u3002\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u5982\u7126\u8651\u3001\u5b64\u72ec\u7684\u4eba\u7fa4\u4e2d\uff0c\u7528\u6237\u4f1a\u79ef\u6781\u5bfb\u6c42\u963f\u8c00\u884c\u4e3a\u5e26\u6765\u7684\u60c5\u611f\u652f\u6301\u3002", "conclusion": "\u6211\u4eec\u5efa\u8bae\u8bbe\u8ba1\u66f4\u5177\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4eba\u5de5\u667a\u80fd\uff0c\u4ee5\u5e73\u8861\u963f\u8c00\u884c\u4e3a\u7684\u98ce\u9669\u4e0e\u76ca\u5904\uff0c\u5e76\u8ba8\u8bba\u7528\u6237\u6559\u80b2\u4e0e\u900f\u660e\u5ea6\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.10268", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.10268", "abs": "https://arxiv.org/abs/2601.10268", "authors": ["Eszter Birtalan", "Mikl\u00f3s Koller"], "title": "The impact of tactile sensor configurations on grasp learning efficiency -- a comparative evaluation in simulation", "comment": "13 pages, 6 figures, 2 tables", "summary": "Tactile sensors are breaking into the field of robotics to provide direct information related to contact surfaces, including contact events, slip events and even texture identification. These events are especially important for robotic hand designs, including prosthetics, as they can greatly improve grasp stability. Most presently published robotic hand designs, however, implement them in vastly different densities and layouts on the hand surface, often reserving the majority of the available space. We used simulations to evaluate 6 different tactile sensor configurations with different densities and layouts, based on their impact on reinforcement learning. Our two-setup system allows for robust results that are not dependent on the use of a given physics simulator, robotic hand model or machine learning algorithm. Our results show setup-specific, as well as generalized effects across the 6 sensorized simulations, and we identify one configuration as consistently yielding the best performance across both setups. These results could help future research aimed at robotic hand designs, including prostheses.", "AI": {"tldr": "\u7814\u7a76\u89e6\u89c9\u4f20\u611f\u5668\u914d\u7f6e\u5bf9\u673a\u5668\u4eba\u624b\u90e8\u8bbe\u8ba1\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u6700\u4f73\u914d\u7f6e\u53ef\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u89e6\u89c9\u4f20\u611f\u5668\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u5bf9\u673a\u68b0\u624b\u8bbe\u8ba1\u548c\u5047\u80a2\u7684\u5f71\u54cd\uff0c\u4ee5\u63d0\u9ad8\u6293\u63e1\u7a33\u5b9a\u6027\u3002", "method": "\u4f7f\u7528\u6a21\u62df\u8bc4\u4f30\u4e0d\u540c\u7684\u89e6\u89c9\u4f20\u611f\u5668\u914d\u7f6e\uff0c\u5206\u6790\u5176\u5bf9\u5f3a\u5316\u5b66\u4e60\u7684\u5f71\u54cd\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u8bc4\u4f30\u4e866\u79cd\u4e0d\u540c\u7684\u89e6\u89c9\u4f20\u611f\u5668\u914d\u7f6e\uff0c\u53d1\u73b0\u4e00\u4e2a\u914d\u7f6e\u5728\u4e24\u4e2a\u4e0d\u540c\u7684\u8bbe\u7f6e\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u80fd\u591f\u4e3a\u672a\u6765\u7684\u673a\u5668\u4eba\u624b\u8bbe\u8ba1\u7814\u7a76\u63d0\u4f9b\u6307\u5bfc\uff0c\u5305\u62ec\u5047\u80a2\u3002"}}
{"id": "2601.10525", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.10525", "abs": "https://arxiv.org/abs/2601.10525", "authors": ["Yijin Zhou", "Fu Li", "Yi Niu", "Boxun Fu", "Huaning Wang", "Lijian Zhang"], "title": "Learning from Brain Topography: A Hierarchical Local-Global Graph-Transformer Network for EEG Emotion Recognition", "comment": null, "summary": "Understanding how local neurophysiological patterns interact with global brain dynamics is essential for decoding human emotions from EEG signals. However, existing deep learning approaches often overlook the brain's intrinsic spatial organization, failing to simultaneously capture local topological relations and global dependencies. To address these challenges, we propose Neuro-HGLN, a Neurologically-informed Hierarchical Graph-Transformer Learning Network that integrates biologically grounded priors with hierarchical representation learning. Neuro-HGLN first constructs a spatial Euclidean prior graph based on physical electrode distances to serve as an anatomically grounded inductive bias. A learnable global dynamic graph is then introduced to model functional connectivity across the entire brain. In parallel, to capture fine-grained regional dependencies, Neuro-HGLN builds region-level local graphs using a multi-head self-attention mechanism. These graphs are processed synchronously through local-constrained parallel GCN layers to produce region-specific representations. Subsequently, an iTransformer encoder aggregates these features to capture cross-region dependencies under a dimension-as-token formulation. Extensive experiments demonstrate that Neuro-HGLN achieves state-of-the-art performance on multiple benchmarks, providing enhanced interpretability grounded in neurophysiological structure. These results highlight the efficacy of unifying local topological learning with cross-region dependency modeling for robust EEG emotion recognition.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u7f51\u7edcNeuro-HGLN\uff0c\u7ed3\u5408\u7a7a\u95f4\u56fe\u548c\u52a8\u6001\u56fe\u63d0\u5347\u8111\u7535\u56fe\u60c5\u7eea\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u8003\u8651\u5927\u8111\u7a7a\u95f4\u7ec4\u7ec7\u7684\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u8111\u7535\u56fe\u4fe1\u53f7\u89e3\u7801\u4eba\u7c7b\u60c5\u7eea\u7684\u6548\u679c\u3002", "method": "\u6784\u5efa\u751f\u7269\u5b66\u57fa\u7840\u7684\u7a7a\u95f4\u4f18\u5148\u56fe\u53ca\u52a8\u6001\u56fe\uff0c\u5e76\u5229\u7528\u591a\u5934\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5f62\u6210\u533a\u57df\u7ea7\u5c40\u90e8\u56fe\uff0c\u901a\u8fc7\u5c40\u90e8\u7ea6\u675f\u5e76\u884cGCN\u5c42\u5904\u7406\uff0c\u4ee5\u83b7\u5f97\u533a\u57df\u7279\u5f81\uff0c\u6700\u540e\u7528iTransformer\u7f16\u7801\u5668\u805a\u5408\u7279\u5f81\u4ee5\u6355\u6349\u8de8\u533a\u57df\u4f9d\u8d56\u3002", "result": "\u7ecf\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0cNeuro-HGLN\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u589e\u5f3a\u4e86\u89e3\u91ca\u80fd\u529b\uff0c\u57fa\u4e8e\u795e\u7ecf\u751f\u7406\u7ed3\u6784\u7684\u8868\u73b0\u3002", "conclusion": "Neuro-HGLN\u901a\u8fc7\u7ed3\u5408\u5c40\u90e8\u62d3\u6251\u5b66\u4e60\u4e0e\u8de8\u533a\u57df\u4f9d\u8d56\u5efa\u6a21\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8111\u7535\u56fe\u60c5\u7eea\u8bc6\u522b\u7684\u51c6\u786e\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2601.10340", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.10340", "abs": "https://arxiv.org/abs/2601.10340", "authors": ["David Morilla-Cabello", "Eduardo Montijano"], "title": "CHORAL: Traversal-Aware Planning for Safe and Efficient Heterogeneous Multi-Robot Routing", "comment": null, "summary": "Monitoring large, unknown, and complex environments with autonomous robots poses significant navigation challenges, where deploying teams of heterogeneous robots with complementary capabilities can substantially improve both mission performance and feasibility. However, effectively modeling how different robotic platforms interact with the environment requires rich, semantic scene understanding. Despite this, existing approaches often assume homogeneous robot teams or focus on discrete task compatibility rather than continuous routing. Consequently, scene understanding is not fully integrated into routing decisions, limiting their ability to adapt to the environment and to leverage each robot's strengths. In this paper, we propose an integrated semantic-aware framework for coordinating heterogeneous robots. Starting from a reconnaissance flight, we build a metric-semantic map using open-vocabulary vision models and use it to identify regions requiring closer inspection and capability-aware paths for each platform to reach them. These are then incorporated into a heterogeneous vehicle routing formulation that jointly assigns inspection tasks and computes robot trajectories. Experiments in simulation and in a real inspection mission with three robotic platforms demonstrate the effectiveness of our approach in planning safer and more efficient routes by explicitly accounting for each platform's navigation capabilities. We release our framework, CHORAL, as open source to support reproducibility and deployment of diverse robot teams.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u7684\u8bed\u4e49\u611f\u77e5\u6846\u67b6\u7528\u4e8e\u534f\u8c03\u5f02\u6784\u673a\u5668\u4eba\uff0c\u63d0\u5347\u73af\u5883\u76d1\u6d4b\u7684\u5bfc\u822a\u6548\u679c\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5728\u590d\u6742\u7684\u73af\u5883\u4e2d\uff0c\u91c7\u7528\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u8fdb\u884c\u76d1\u6d4b\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u548c\u53ef\u884c\u6027\uff1b\u76ee\u524d\u7684\u65b9\u6cd5\u591a\u5047\u8bbe\u673a\u5668\u4eba\u56e2\u961f\u5177\u540c\u8d28\u6027\uff0c\u9650\u5236\u4e86\u73af\u5883\u9002\u5e94\u80fd\u529b\u548c\u5404\u81ea\u4f18\u52bf\u7684\u5229\u7528\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u5ea6\u91cf-\u8bed\u4e49\u5730\u56fe\uff0c\u5e76\u91c7\u7528\u5f00\u653e\u8bcd\u6c47\u89c6\u89c9\u6a21\u578b\uff0c\u8bc6\u522b\u68c0\u67e5\u533a\u57df\u53ca\u5404\u5e73\u53f0\u7684\u8def\u5f84\uff0c\u6574\u5408\u8fdb\u5f02\u6784\u8f66\u8f86\u8def\u7531 formulatio\uff0c\u5b9e\u73b0\u4efb\u52a1\u5206\u914d\u548c\u8def\u5f84\u8ba1\u7b97\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u7684\u8bed\u4e49\u611f\u77e5\u6846\u67b6CHORAL\uff0c\u7528\u4e8e\u534f\u8c03\u5f02\u6784\u673a\u5668\u4eba\uff0c\u80fd\u591f\u6709\u6548\u8bc6\u522b\u9700\u8981\u7ec6\u81f4\u68c0\u67e5\u7684\u533a\u57df\uff0c\u5e76\u8ba1\u7b97\u51fa\u9002\u5408\u5404\u4e2a\u673a\u5668\u4eba\u5e73\u53f0\u7684\u80fd\u529b-aware\u8def\u5f84\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u8003\u8651\u6bcf\u4e2a\u5e73\u53f0\u7684\u5bfc\u822a\u80fd\u529b\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u89c4\u5212\u66f4\u5b89\u5168\u3001\u66f4\u9ad8\u6548\u7684\u8def\u7ebf\u65b9\u9762\u5c55\u73b0\u4e86\u826f\u597d\u7684\u6548\u679c\uff0c\u6846\u67b6CHORAL\u4e5f\u5df2\u5f00\u6e90\u4ee5\u652f\u6301\u4e0d\u540c\u673a\u5668\u4eba\u56e2\u961f\u7684\u91cd\u73b0\u6027\u548c\u90e8\u7f72\u3002"}}
{"id": "2601.10536", "categories": ["cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.10536", "abs": "https://arxiv.org/abs/2601.10536", "authors": ["Ishani Kanapathipillai", "Obhasha Priyankara"], "title": "CoGen: Creation of Reusable UI Components in Figma via Textual Commands", "comment": "8 pages, 6 figures, 11 tables", "summary": "The evolution of User Interface design has emphasized the need for efficient, reusable, and editable components to ensure an efficient design process. This research introduces CoGen, a system that uses machine learning techniques to generate reusable UI components directly in Figma, one of the most popular UI design tools. Addressing gaps in current systems, CoGen focuses on creating atomic components such as buttons, labels, and input fields using structured JSON and natural language prompts.\n  The project integrates Figma API data extraction, Seq2Seq models, and fine-tuned T5 transformers for component generation. The key results demonstrate the efficiency of the T5 model in prompt generation, with an accuracy of 98% and a BLEU score of 0.2668, which ensures the mapping of JSON to descriptive prompts. For JSON creation, CoGen achieves a success rate of up to 100% in generating simple JSON outputs for specified component types.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa CoGen \u7cfb\u7edf\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u5728 Figma \u4e2d\u751f\u6210\u53ef\u91cd\u7528 UI \u7ec4\u4ef6\uff0c\u663e\u8457\u63d0\u5347\u8bbe\u8ba1\u6548\u7387\u3002", "motivation": "\u968f\u7740\u7528\u6237\u754c\u9762\u8bbe\u8ba1\u7684\u53d1\u5c55\uff0c\u5bf9\u9ad8\u6548\u3001\u53ef\u91cd\u7528\u548c\u53ef\u7f16\u8f91\u7ec4\u4ef6\u7684\u9700\u6c42\u65e5\u76ca\u589e\u52a0\u3002", "method": "\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u5229\u7528 Figma API \u6570\u636e\u63d0\u53d6\u3001Seq2Seq \u6a21\u578b\u548c\u5fae\u8c03\u7684 T5 \u8f6c\u6362\u5668\u751f\u6210\u7ec4\u4ef6\u3002", "result": "T5 \u6a21\u578b\u5728\u63d0\u793a\u751f\u6210\u4e2d\u7684\u6548\u7387\u8fbe\u5230 98% \u7684\u51c6\u786e\u7387\uff0cJSON \u521b\u5efa\u6210\u529f\u7387\u9ad8\u8fbe100%\u3002", "conclusion": "CoGen \u7cfb\u7edf\u6709\u6548\u5730\u751f\u6210\u53ef\u91cd\u7528\u7684 UI \u7ec4\u4ef6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bbe\u8ba1\u6548\u7387\u3002"}}
{"id": "2601.10365", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.10365", "abs": "https://arxiv.org/abs/2601.10365", "authors": ["Yan Liu", "Tao Yu", "Haolin Song", "Hongbo Zhu", "Nianzong Hu", "Yuzhi Hao", "Xiuyong Yao", "Xizhe Zang", "Hua Chen", "Jie Zhao"], "title": "FastStair: Learning to Run Up Stairs with Humanoid Robots", "comment": null, "summary": "Running up stairs is effortless for humans but remains extremely challenging for humanoid robots due to the simultaneous requirements of high agility and strict stability. Model-free reinforcement learning (RL) can generate dynamic locomotion, yet implicit stability rewards and heavy reliance on task-specific reward shaping tend to result in unsafe behaviors, especially on stairs; conversely, model-based foothold planners encode contact feasibility and stability structure, but enforcing their hard constraints often induces conservative motion that limits speed. We present FastStair, a planner-guided, multi-stage learning framework that reconciles these complementary strengths to achieve fast and stable stair ascent. FastStair integrates a parallel model-based foothold planner into the RL training loop to bias exploration toward dynamically feasible contacts and to pretrain a safety-focused base policy. To mitigate planner-induced conservatism and the discrepancy between low- and high-speed action distributions, the base policy was fine-tuned into speed-specialized experts and then integrated via Low-Rank Adaptation (LoRA) to enable smooth operation across the full commanded-speed range. We deploy the resulting controller on the Oli humanoid robot, achieving stable stair ascent at commanded speeds up to 1.65 m/s and traversing a 33-step spiral staircase (17 cm rise per step) in 12 s, demonstrating robust high-speed performance on long staircases. Notably, the proposed approach served as the champion solution in the Canton Tower Robot Run Up Competition.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aFastStair\u7684\u591a\u9636\u6bb5\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6a21\u578b\u5bfc\u5411\u7684\u8db3\u8ff9\u89c4\u5212\u548c\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\uff0c\u6210\u529f\u5b9e\u73b0\u5feb\u901f\u7a33\u5b9a\u7684\u697c\u68af\u6500\u722c\u3002", "motivation": "\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u767b\u697c\u68af\u65f6\u9762\u4e34\u7684\u9ad8\u7075\u6d3b\u6027\u4e0e\u7a33\u5b9a\u6027\u8981\u6c42\u7684\u6311\u6218\uff0c\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u6a21\u578b\u5bfc\u5411\u7684\u8db3\u8ff9\u89c4\u5212\u5668\u4e0e\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u73af\uff0c\u504f\u5411\u4e8e\u52a8\u6001\u53ef\u884c\u63a5\u89e6\u5e76\u9884\u8bad\u7ec3\u5b89\u5168\u653f\u7b56\u3002\u901a\u8fc7\u4e13\u6ce8\u4e8e\u901f\u5ea6\u7684\u4e13\u5bb6\u7ec6\u5316\u57fa\u7840\u653f\u7b56\uff0c\u5e76\u4f7f\u7528\u4f4e\u79e9\u9002\u5e94\u6280\u672f\u5b9e\u73b0\u4e0d\u540c\u901f\u5ea6\u8303\u56f4\u7684\u5e73\u6ed1\u64cd\u4f5c\u3002", "result": "\u5728\u547d\u4ee4\u901f\u5ea6\u9ad8\u8fbe1.65 m/s\u7684\u60c5\u51b5\u4e0b\uff0cOli\u4eba\u5f62\u673a\u5668\u4eba\u6210\u529f\u7a7f\u8d8a33\u6b65\u87ba\u65cb\u697c\u68af\uff0c\u5b8c\u6210\u65f6\u95f4\u4e3a12\u79d2\u3002", "conclusion": "FastStair\u6210\u529f\u5730\u5728Oli\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u697c\u68af\u6500\u722c\uff0c\u5c55\u793a\u4e86\u5feb\u901f\u800c\u53ef\u9760\u7684\u6027\u80fd\u3002"}}
{"id": "2601.10688", "categories": ["cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.10688", "abs": "https://arxiv.org/abs/2601.10688", "authors": ["Rubel Hassan Mollik", "Vamsi Krishna Kosuri", "Hans Djalali", "Stephanie Ludi", "Aboubakar Mountapmbeme"], "title": "An Extension-Based Accessibility Framework for Making Blockly Accessible to Blind and Low-Vision Users", "comment": "Accepted at the 1st International Workshop on User Interface and Experience for Software Engineering (UISE 2026), co-located with ICSE 2026. Final published version available at DOI: 10.1145/3786169.3788398", "summary": "Block-based programming environments (BBPEs) such as Scratch and Code.org are now widely used in K-12 computer science classes, but they remain mostly inaccessible to blind or visually impaired (BVI) learners. A major problem is that prior accessibility solutions have relied on modifications to the Blockly library, making them difficult to apply in existing BBPEs and thereby limiting adoption. We present an Extension-based Accessibility Framework (EAF) to make BBPEs accessible for BVI students. The framework uses a modular architecture that enables seamless integration with existing Blockly-based BBPEs. We present an innovative three-dimensional (3D) hierarchical navigation model featuring stack labeling and block numbering, mode-based editing to prevent accidental modifications, and WAI-ARIA implementation to ensure compatibility with external screen readers. We evaluated our approach by integrating the EAF framework into two BBPEs (covering 177 test cases) and conducting semi-structured interviews with four participants using VoiceOver, JAWS, and NVDA. Participants reported clearer spatial orientation and easier mental model formation compared to default Blockly keyboard navigation. EAF shows that modular architecture can provide comprehensive accessibility while ensuring compatibility with existing BBPEs.", "AI": {"tldr": "\u63d0\u51faEAF\u6846\u67b6\uff0c\u63d0\u5347\u76f2\u4eba\u548c\u89c6\u529b\u53d7\u635f\u8005\u5728\u57fa\u4e8e\u533a\u5757\u7f16\u7a0b\u73af\u5883\u4e2d\u7684\u53ef\u8bbf\u95ee\u6027\uff0c\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u53ef\u4e0e\u73b0\u6709\u73af\u5883\u6574\u5408\uff0c\u4f53\u9a8c\u6539\u5584\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u53ef\u8bbf\u95ee\u6027\u89e3\u51b3\u65b9\u6848\u5728BBPE\u4e2d\u96be\u4ee5\u5e94\u7528\u7684\u95ee\u9898\uff0c\u4ece\u800c\u4f7f\u76f2\u4eba\u6216\u89c6\u529b\u53d7\u635f\u5b66\u4e60\u8005\u80fd\u591f\u66f4\u597d\u5730\u53c2\u4e0e\u8ba1\u7b97\u673a\u79d1\u5b66\u8bfe\u7a0b\u3002", "method": "\u4f7f\u7528\u6a21\u5757\u5316\u67b6\u6784\u8bbe\u8ba1EAF\u6846\u67b6\uff0c\u901a\u8fc7177\u4e2a\u6d4b\u8bd5\u6848\u4f8b\u8bc4\u4f30\u5176\u5728\u4e24\u4e2aBBPE\u4e2d\u7684\u96c6\u6210\u6548\u679c\uff0c\u5e76\u8fdb\u884c\u53c2\u4e0e\u8005\u8bbf\u8c08\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u5c55\u7684\u53ef\u8bbf\u95ee\u6027\u6846\u67b6\uff08EAF\uff09\uff0c\u4ee5\u589e\u5f3a\u76f2\u4eba\u6216\u89c6\u529b\u53d7\u635f\u5b66\u4e60\u8005\u5728\u57fa\u4e8e\u533a\u5757\u7684\u7f16\u7a0b\u73af\u5883\uff08BBPEs\uff09\u4e2d\u7684\u53ef\u8bbf\u95ee\u6027\u3002\u8be5\u6846\u67b6\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\uff0c\u4fbf\u4e8e\u4e0e\u73b0\u6709BBPEs\u7684\u6574\u5408\u3002\u901a\u8fc7\u5728\u4e24\u4e2aBBPE\u4e2d\u96c6\u6210EAF\u6846\u67b6\u5e76\u8fdb\u884c semi-structured interviews\uff0c\u53c2\u4e0e\u8005\u53cd\u9988\u4e86\u6bd4\u9ed8\u8ba4Blockly\u952e\u76d8\u5bfc\u822a\u66f4\u6e05\u6670\u7684\u7a7a\u95f4\u5b9a\u4f4d\u548c\u66f4\u5bb9\u6613\u7684\u5fc3\u7406\u6a21\u578b\u5f62\u6210\u3002", "conclusion": "EAF\u6846\u67b6\u901a\u8fc7\u6a21\u5757\u5316\u67b6\u6784\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u53ef\u8bbf\u95ee\u6027\uff0c\u786e\u4fdd\u4e0e\u73b0\u6709\u57fa\u4e8eBlockly\u7684\u7f16\u7a0b\u73af\u5883\u7684\u517c\u5bb9\u6027\u3002"}}
{"id": "2601.10379", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.10379", "abs": "https://arxiv.org/abs/2601.10379", "authors": ["He Ren", "Gaowei Yan", "Hang Liu", "Lifeng Cao", "Zhijun Zhao", "Gang Dang"], "title": "Online identification of nonlinear time-varying systems with uncertain information", "comment": null, "summary": "Digital twins (DTs), serving as the core enablers for real-time monitoring and predictive maintenance of complex cyber-physical systems, impose critical requirements on their virtual models: high predictive accuracy, strong interpretability, and online adaptive capability. However, existing techniques struggle to meet these demands simultaneously: Bayesian methods excel in uncertainty quantification but lack model interpretability, while interpretable symbolic identification methods (e.g., SINDy) are constrained by their offline, batch-processing nature, which make real-time updates challenging. To bridge this semantic and computational gap, this paper proposes a novel Bayesian Regression-based Symbolic Learning (BRSL) framework. The framework formulates online symbolic discovery as a unified probabilistic state-space model. By incorporating sparse horseshoe priors, model selection is transformed into a Bayesian inference task, enabling simultaneous system identification and uncertainty quantification. Furthermore, we derive an online recursive algorithm with a forgetting factor and establish precise recursive conditions that guarantee the well-posedness of the posterior distribution. These conditions also function as real-time monitors for data utility, enhancing algorithmic robustness. Additionally, a rigorous convergence analysis is provided, demonstrating the convergence of parameter estimates under persistent excitation conditions. Case studies validate the effectiveness of the proposed framework in achieving interpretable, probabilistic prediction and online learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBRSL\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u56de\u5f52\u5b9e\u73b0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u548c\u5728\u7ebf\u9002\u5e94\u7684\u7b26\u53f7\u5b66\u4e60\uff0c\u514b\u670d\u4e86\u73b0\u6709\u7b97\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5c55\u73b0\u4e86\u826f\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u73b0\u6709\u6280\u672f\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u3001\u5f3a\u53ef\u89e3\u91ca\u6027\u548c\u5728\u7ebf\u9002\u5e94\u80fd\u529b\u7684\u9700\u6c42\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u56de\u5f52\u7684\u7b26\u53f7\u5b66\u4e60\u6846\u67b6(BRSL)\uff0c\u5c06\u5728\u7ebf\u7b26\u53f7\u53d1\u73b0\u8868\u8ff0\u4e3a\u7edf\u4e00\u7684\u6982\u7387\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u3002", "result": "\u63d0\u51fa\u7684BRSL\u6846\u67b6\u5b9e\u73b0\u4e86\u7cfb\u7edf\u8bc6\u522b\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u540c\u65f6\u8fdb\u884c\uff0c\u589e\u5f3a\u4e86\u7b97\u6cd5\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u6848\u4f8b\u7814\u7a76\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cBRSL\u6846\u67b6\u5728\u771f\u5b9e\u5e94\u7528\u4e2d\u5177\u5907\u53ef\u89e3\u91ca\u6027\u3001\u6709\u6548\u7684\u6982\u7387\u9884\u6d4b\u548c\u5728\u7ebf\u5b66\u4e60\u80fd\u529b\u3002"}}
