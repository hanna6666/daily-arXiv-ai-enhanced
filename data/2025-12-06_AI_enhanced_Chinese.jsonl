{"id": "2512.04231", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04231", "abs": "https://arxiv.org/abs/2512.04231", "authors": ["Zhou Chen", "Joe Lin", "Carson Bulgin", "Sathyanarayanan N. Aakur"], "title": "CRAFT-E: A Neuro-Symbolic Framework for Embodied Affordance Grounding", "comment": "20 pages. 3 figures, 4 tables. Under Review", "summary": "Assistive robots operating in unstructured environments must understand not only what objects are, but what they can be used for. This requires grounding language-based action queries to objects that both afford the requested function and can be physically retrieved. Existing approaches often rely on black-box models or fixed affordance labels, limiting transparency, controllability, and reliability for human-facing applications. We introduce CRAFT-E, a modular neuro-symbolic framework that composes a structured verb-property-object knowledge graph with visual-language alignment and energy-based grasp reasoning. The system generates interpretable grounding paths that expose the factors influencing object selection and incorporates grasp feasibility as an integral part of affordance inference. We further construct a benchmark dataset with unified annotations for verb-object compatibility, segmentation, and grasp candidates, and deploy the full pipeline on a physical robot. CRAFT-E achieves competitive performance in static scenes, ImageNet-based functional retrieval, and real-world trials involving 20 verbs and 39 objects. The framework remains robust under perceptual noise and provides transparent, component-level diagnostics. By coupling symbolic reasoning with embodied perception, CRAFT-E offers an interpretable and customizable alternative to end-to-end models for affordance-grounded object selection, supporting trustworthy decision-making in assistive robotic systems.", "AI": {"tldr": "CRAFT-E\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u77e5\u8bc6\u56fe\u8c31\u548c\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50\uff0c\u6539\u5584\u4e86\u52a9\u7406\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bf9\u8c61\u9009\u62e9\u80fd\u529b\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6293\u53d6\u63a8\u7406\u3002", "motivation": "\u8f85\u52a9\u673a\u5668\u4eba\u9700\u8981\u7406\u89e3\u7269\u4f53\u7684\u529f\u80fd\u4ee5\u53ca\u5982\u4f55\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7269\u7406\u5730\u83b7\u53d6\u8fd9\u4e9b\u7269\u4f53\uff0c\u4ee5\u63d0\u9ad8\u4eba\u673a\u4ea4\u4e92\u7684\u900f\u660e\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "CRAFT-E\u7ed3\u5408\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u52a8\u8bcd-\u5c5e\u6027-\u5bf9\u8c61\u77e5\u8bc6\u56fe\u8c31\u4e0e\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u548c\u57fa\u4e8e\u80fd\u91cf\u7684\u6293\u53d6\u63a8\u7406\uff0c\u4ece\u800c\u6784\u5efa\u53ef\u89e3\u91ca\u7684\u5bf9\u8c61\u9009\u62e9\u8def\u5f84\u3002", "result": "CRAFT-E\u5728\u9759\u6001\u573a\u666f\u3001\u57fa\u4e8eImageNet\u7684\u529f\u80fd\u68c0\u7d22\u548c\u5b9e\u9645\u73af\u5883\u6d4b\u8bd5\u4e2d\uff0c\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u5728\u611f\u77e5\u566a\u58f0\u5e72\u6270\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u7ec4\u4ef6\u7ea7\u7684\u900f\u660e\u8bca\u65ad\u3002", "conclusion": "CRAFT-E\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u548c\u53ef\u5b9a\u5236\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u7528\u4e8e\u57fa\u4e8e\u53ef\u7528\u6027\u9009\u62e9\u5bf9\u8c61\uff0c\u6709\u52a9\u4e8e\u5728\u8f85\u52a9\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u652f\u6301\u53ef\u4fe1\u7684\u51b3\u7b56\u5236\u5b9a\u3002"}}
{"id": "2512.04249", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.04249", "abs": "https://arxiv.org/abs/2512.04249", "authors": ["Maksim Surov", "Leonid Freidovich"], "title": "Sliding Mode Control and Subspace Stabilization Methodology for the Orbital Stabilization of Periodic Trajectories", "comment": null, "summary": "This paper presents a combined sliding-mode control and subspace stabilization methodology for orbital stabilization of periodic trajectories in underactuated mechanical systems with one degree of underactuation. The approach starts with partial feedback linearization and stabilization. Then, transverse linearization along the reference orbit is computed, resulting in a periodic linear time-varying system with a stable subspace. Sliding-mode control drives trajectories toward this subspace. The proposed design avoids solving computationally intensive periodic LQR problems and improves robustness to matched disturbances. The methodology is validated through experiments on the Butterfly robot.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6ed1\u6a21\u63a7\u5236\u548c\u5b50\u7a7a\u95f4\u7a33\u5b9a\u5316\u7684\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u6b20\u9a71\u52a8\u673a\u68b0\u7cfb\u7edf\u4e2d\u5468\u671f\u8f68\u8ff9\u7684\u7a33\u5b9a\uff0c\u5e76\u5728\u8774\u8776\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u5728\u5177\u6709\u4e00\u4e2a\u6b20\u9a71\u52a8\u5ea6\u7684\u6b20\u9a71\u52a8\u673a\u68b0\u7cfb\u7edf\u4e2d\u7a33\u5b9a\u5468\u671f\u6027\u8f68\u8ff9\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002", "method": "\u7ed3\u5408\u6ed1\u6a21\u63a7\u5236\u548c\u5b50\u7a7a\u95f4\u7a33\u5b9a\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u90e8\u5206\u53cd\u9988\u7ebf\u6027\u5316\u548c\u7a33\u5b9a\u5316\uff0c\u8ba1\u7b97\u6cbf\u53c2\u8003\u8f68\u9053\u7684\u6a2a\u5411\u7ebf\u6027\u5316\uff0c\u5f62\u6210\u4e00\u4e2a\u5177\u6709\u7a33\u5b9a\u5b50\u7a7a\u95f4\u7684\u5468\u671f\u7ebf\u6027\u65f6\u53d8\u7cfb\u7edf\u3002", "result": "\u6ed1\u6a21\u63a7\u5236\u80fd\u591f\u5c06\u8f68\u8ff9\u5f15\u5bfc\u81f3\u8be5\u7a33\u5b9a\u5b50\u7a7a\u95f4\uff0c\u907f\u514d\u4e86\u6c42\u89e3\u8ba1\u7b97\u5bc6\u96c6\u7684\u5468\u671fLQR\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u5bf9\u5339\u914d\u5e72\u6270\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5df2\u5728\u8774\u8776\u673a\u5668\u4eba\u4e0a\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u3002"}}
{"id": "2512.04279", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.04279", "abs": "https://arxiv.org/abs/2512.04279", "authors": ["Feeza Khan Khanzada", "Jaerock Kwon"], "title": "Driving Beyond Privilege: Distilling Dense-Reward Knowledge into Sparse-Reward Policies", "comment": null, "summary": "We study how to exploit dense simulator-defined rewards in vision-based autonomous driving without inheriting their misalignment with deployment metrics. In realistic simulators such as CARLA, privileged state (e.g., lane geometry, infractions, time-to-collision) can be converted into dense rewards that stabilize and accelerate model-based reinforcement learning, but policies trained directly on these signals often overfit and fail to generalize when evaluated on sparse objectives such as route completion and collision-free overtaking. We propose reward-privileged world model distillation, a two-stage framework in which a teacher DreamerV3-style agent is first trained with a dense privileged reward, and only its latent dynamics are distilled into a student trained solely on sparse task rewards. Teacher and student share the same observation space (semantic bird's-eye-view images); privileged information enters only through the teacher's reward, and the student does not imitate the teacher's actions or value estimates. Instead, the student's world model is regularized to match the teacher's latent dynamics while its policy is learned from scratch on sparse success/failure signals. In CARLA lane-following and overtaking benchmarks, sparse-reward students outperform both dense-reward teachers and sparse-from-scratch baselines. On unseen lane-following routes, reward-privileged distillation improves success by about 23 percent relative to the dense teacher while maintaining comparable or better safety. On overtaking, students retain near-perfect performance on training routes and achieve up to a 27x improvement in success on unseen routes, with improved lane keeping. These results show that dense rewards can be leveraged to learn richer dynamics models while keeping the deployed policy optimized strictly for sparse, deployment-aligned objectives.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528\u6a21\u62df\u5668\u5bc6\u96c6\u5956\u52b1\u5728\u89c6\u89c9\u57fa\u7840\u7684\u81ea\u4e3b\u9a7e\u9a76\u4e2d\u63d0\u5347\u6027\u80fd\uff0c\u540c\u65f6\u907f\u514d\u4e0e\u90e8\u7f72\u76ee\u6807\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u73b0\u5b9e\u7684\u6a21\u62df\u5668\uff08\u5982CARLA\uff09\u4e2d\uff0c\u5bc6\u96c6\u5956\u52b1\u6709\u52a9\u4e8e\u52a0\u901f\u6a21\u578b\u8bad\u7ec3\uff0c\u4f46\u76f4\u63a5\u57fa\u4e8e\u8fd9\u4e9b\u5956\u52b1\u8bad\u7ec3\u7684\u7b56\u7565\u5728\u7a00\u758f\u76ee\u6807\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u4e24\u9636\u6bb5\u7684\u5956\u52b1\u7279\u6743\u4e16\u754c\u6a21\u578b\u84b8\u998f\u6846\u67b6\uff0c\u9996\u5148\u7528\u5bc6\u96c6\u5956\u52b1\u8bad\u7ec3\u6559\u5e08\u4ee3\u7406\uff0c\u7136\u540e\u7528\u7a00\u758f\u4efb\u52a1\u5956\u52b1\u8bad\u7ec3\u5b66\u751f\u4ee3\u7406\u3002", "result": "\u5728CARLA\u7684\u8f66\u9053\u8ddf\u968f\u548c\u8d85\u8f66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u7a00\u758f\u5956\u52b1\u7684\u5b66\u751f\u5728\u672a\u89c1\u8f66\u9053\u4e0a\u6210\u529f\u7387\u63d0\u9ad8\u7ea623%\uff0c\u8d85\u8f66\u65f6\u5728\u672a\u89c1\u8def\u7ebf\u4e0a\u7684\u6210\u529f\u7387\u63d0\u9ad8\u81f327\u500d\u3002", "conclusion": "\u5bc6\u96c6\u5956\u52b1\u53ef\u4ee5\u6709\u6548\u5730\u7528\u4e8e\u5b66\u4e60\u52a8\u6001\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u8bc1\u7b56\u7565\u4f18\u5316\u4e0e\u7a00\u758f\u76ee\u6807\u4e00\u81f4\u3002"}}
{"id": "2512.04308", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.04308", "abs": "https://arxiv.org/abs/2512.04308", "authors": ["Lei Zhang", "Ju Dong", "Kaixin Bai", "Minheng Ni", "Zoltan-Csaba Marton", "Zhaopeng Chen", "Jianwei Zhang"], "title": "ResponsibleRobotBench: Benchmarking Responsible Robot Manipulation using Multi-modal Large Language Models", "comment": "https://sites.google.com/view/responsible-robotbench", "summary": "Recent advances in large multimodal models have enabled new opportunities in embodied AI, particularly in robotic manipulation. These models have shown strong potential in generalization and reasoning, but achieving reliable and responsible robotic behavior in real-world settings remains an open challenge. In high-stakes environments, robotic agents must go beyond basic task execution to perform risk-aware reasoning, moral decision-making, and physically grounded planning. We introduce ResponsibleRobotBench, a systematic benchmark designed to evaluate and accelerate progress in responsible robotic manipulation from simulation to real world. This benchmark consists of 23 multi-stage tasks spanning diverse risk types, including electrical, chemical, and human-related hazards, and varying levels of physical and planning complexity. These tasks require agents to detect and mitigate risks, reason about safety, plan sequences of actions, and engage human assistance when necessary. Our benchmark includes a general-purpose evaluation framework that supports multimodal model-based agents with various action representation modalities. The framework integrates visual perception, context learning, prompt construction, hazard detection, reasoning and planning, and physical execution. It also provides a rich multimodal dataset, supports reproducible experiments, and includes standardized metrics such as success rate, safety rate, and safe success rate. Through extensive experimental setups, ResponsibleRobotBench enables analysis across risk categories, task types, and agent configurations. By emphasizing physical reliability, generalization, and safety in decision-making, this benchmark provides a foundation for advancing the development of trustworthy, real-world responsible dexterous robotic systems. https://sites.google.com/view/responsible-robotbench", "AI": {"tldr": "\u65b0\u63a8\u51fa\u7684ResponsibleRobotBench\u57fa\u51c6\u4e3a\u8bc4\u4f30\u548c\u63a8\u52a8\u8d23\u4efb\u611f\u5f3a\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u5de5\u5177\uff0c\u6d89\u53ca\u591a\u79cd\u98ce\u9669\u7c7b\u578b\u548c\u590d\u6742\u5ea6\uff0c\u65e8\u5728\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u5e94\u7528\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u6a21\u578b\u7684\u8fdb\u6b65\uff0c\u673a\u5668\u4eba\u64cd\u63a7\u9886\u57df\u6d8c\u73b0\u51fa\u65b0\u7684\u673a\u9047\uff0c\u5fc5\u987b\u589e\u5f3a\u673a\u5668\u4eba\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "ResponsibleRobotBench\u5305\u542b23\u4e2a\u6d89\u53ca\u591a\u79cd\u98ce\u9669\u7c7b\u578b\u7684\u591a\u9636\u6bb5\u4efb\u52a1\uff0c\u6d89\u53ca\u98ce\u9669\u68c0\u6d4b\u3001\u63a8\u7406\u53ca\u884c\u52a8\u89c4\u5212\u7b49\u5173\u952e\u80fd\u529b\uff0c\u5e76\u63d0\u4f9b\u591a\u6a21\u6001\u8bc4\u4f30\u6846\u67b6\u53ca\u4e30\u5bcc\u7684\u6570\u636e\u96c6\u3002", "result": "ResponsibleRobotBench\u57fa\u51c6\u8bc4\u4f30\u7cfb\u7edf\u5728\u8d1f\u8d23\u7684\u673a\u5668\u4eba\u64cd\u63a7\u65b9\u9762\u7684\u8fdb\u5c55\uff0c\u5c06\u6a21\u62df\u4e0e\u771f\u5b9e\u4e16\u754c\u7684\u6559\u5b66\u7ed3\u5408\uff0c\u4ee5\u786e\u4fdd\u673a\u5668\u4eba\u80fd\u591f\u7406\u89e3\u590d\u6742\u7684\u98ce\u9669\u5e76\u8fdb\u884c\u6709\u6548\u7684\u51b3\u7b56\u3002", "conclusion": "\u901a\u8fc7\u8d1f\u8d23\u673a\u5668\u4eba\u57fa\u51c6\uff0c\u7814\u7a76\u8005\u548c\u5f00\u53d1\u8005\u53ef\u4ee5\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u8bc4\u4f30\u548c\u63d0\u5347\u673a\u5668\u4eba\u7684\u8868\u73b0\uff0c\u786e\u4fdd\u5176\u5728\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u505a\u51fa\u5b89\u5168\u548c\u9053\u5fb7\u7684\u51b3\u7b56\u3002"}}
{"id": "2512.04316", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.04316", "abs": "https://arxiv.org/abs/2512.04316", "authors": ["Haoze Guo"], "title": "ConsentDiff at Scale: Longitudinal Audits of Web Privacy Policy Changes and UI Frictions", "comment": null, "summary": "Web privacy is experienced via two public artifacts: site utterances in policy texts, and the actions users are required to take during consent interfaces. In the extensive cross-section audits we've studied, there is a lack of longitudinal data detailing how these artifacts are changing together, and if interfaces are actually doing what they promise in policy. ConsentDiff provides that longitudinal view. We build a reproducible pipeline that snapshots sites every month, semantically aligns policy clauses to track clause-level churn, and classifies consent-UI patterns by pulling together DOM signals with cues provided by screenshots. We introduce a novel weighted claim-UI alignment score, connecting common policy claims to observable predicates, and enabling comparisons over time, regions, and verticals. Our measurements suggest continued policy churn, systematic changes to eliminate a higher-friction banner design, and significantly higher alignment where rejecting is visible and lower friction.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u91cd\u590d\u7684\u7ba1\u9053\uff0c\u65e8\u5728\u63d0\u4f9b\u7f51\u7ad9\u653f\u7b56\u6587\u672c\u548c\u7528\u6237\u540c\u610f\u754c\u9762\u7684\u957f\u671f\u53d8\u5316\u89c6\u89d2\uff0c\u63a2\u7a76\u4e8c\u8005\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u4e0e\u53d8\u5316\u3002", "motivation": "\u6211\u4eec\u5e0c\u671b\u586b\u8865\u5bf9\u7528\u6237\u9690\u79c1\u540c\u610f\u754c\u9762\u548c\u653f\u7b56\u6587\u672c\u4e4b\u95f4\u53d8\u5316\u7f3a\u4e4f\u7684\u7eb5\u5411\u6570\u636e\u7684\u7a7a\u767d\uff0c\u8bc4\u4f30\u8fd9\u4e9b\u754c\u9762\u662f\u5426\u771f\u6b63\u6267\u884c\u4e86\u6240\u627f\u8bfa\u7684\u653f\u7b56\u3002", "method": "\u4f7f\u7528\u5b9a\u671f\u7684\u7f51\u7ad9\u5feb\u7167\uff0c\u901a\u8fc7\u8bed\u4e49\u5bf9\u9f50\u653f\u7b56\u6761\u6b3e\uff0c\u8ddf\u8e2a\u6761\u6b3e\u7ea7\u522b\u7684\u53d8\u5316\uff0c\u540c\u65f6\u8fd0\u7528\u622a\u56fe\u63d0\u4f9b\u7684\u7ebf\u7d22\u5206\u7c7b\u540c\u610f\u7528\u6237\u754c\u9762\u6a21\u5f0f\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u6743\u91cd\u58f0\u660e-\u7528\u6237\u754c\u9762\u5bf9\u9f50\u5206\u6570\u3002", "result": "\u901a\u8fc7\u6211\u4eec\u7684\u6d4b\u91cf\uff0c\u53d1\u73b0\u653f\u7b56\u6301\u7eed\u53d8\u5316\uff0c\u663e\u8457\u6539\u53d8\u6d88\u9664\u4e86\u9ad8\u6469\u64e6\u7684\u6a2a\u5e45\u8bbe\u8ba1\uff0c\u5e76\u4e14\u5f53\u62d2\u7edd\u9009\u9879\u53ef\u89c1\u4e14\u6469\u64e6\u964d\u4f4e\u65f6\uff0c\u5b83\u4eec\u4e4b\u95f4\u7684\u5bf9\u9f50\u5ea6\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u8868\u660e\uff0c\u653f\u7b56\u6301\u7eed\u53d8\u5316\u3001\u7cfb\u7edf\u6027\u53d8\u5316\u5bfc\u81f4\u8f83\u9ad8\u6469\u64e6\u7684\u6a2a\u5e45\u8bbe\u8ba1\u88ab\u6d88\u9664\uff0c\u5e76\u4e14\u5728\u62d2\u7edd\u9009\u9879\u53ef\u89c1\u4e14\u6469\u64e6\u8f83\u4f4e\u65f6\uff0c\u653f\u7b56\u4e0e\u7528\u6237\u754c\u9762\u4e4b\u95f4\u7684\u5bf9\u9f50\u5ea6\u663e\u8457\u63d0\u9ad8\u3002"}}
{"id": "2512.04373", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.04373", "abs": "https://arxiv.org/abs/2512.04373", "authors": ["Hann Woei Ho", "Ye Zhou"], "title": "Vertical Planetary Landing on Sloped Terrain Using Optical Flow Divergence Estimates", "comment": "This paper is accepted at International Astronautical Congress (IAC 2025)", "summary": "Autonomous landing on sloped terrain poses significant challenges for small, lightweight spacecraft, such as rotorcraft and landers. These vehicles have limited processing capability and payload capacity, which makes advanced deep learning methods and heavy sensors impractical. Flying insects, such as bees, achieve remarkable landings with minimal neural and sensory resources, relying heavily on optical flow. By regulating flow divergence, a measure of vertical velocity divided by height, they perform smooth landings in which velocity and height decay exponentially together. However, adapting this bio-inspired strategy for spacecraft landings on sloped terrain presents two key challenges: global flow-divergence estimates obscure terrain inclination, and the nonlinear nature of divergence-based control can lead to instability when using conventional controllers. This paper proposes a nonlinear control strategy that leverages two distinct local flow divergence estimates to regulate both thrust and attitude during vertical landings. The control law is formulated based on Incremental Nonlinear Dynamic Inversion to handle the nonlinear flow divergence. The thrust control ensures a smooth vertical descent by keeping a constant average of the local flow divergence estimates, while the attitude control aligns the vehicle with the inclined surface at touchdown by exploiting their difference. The approach is evaluated in numerical simulations using a simplified 2D spacecraft model across varying slopes and divergence setpoints. Results show that regulating the average divergence yields stable landings with exponential decay of velocity and height, and using the divergence difference enables effective alignment with inclined terrain. Overall, the method offers a robust, low-resource landing strategy that enhances the feasibility of autonomous planetary missions with small spacecraft.", "AI": {"tldr": "\u9488\u5bf9\u5c0f\u578b\u822a\u5929\u5668\u5728\u5761\u9762\u7740\u9646\u7684\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u7ebf\u6027\u63a7\u5236\u7b56\u7565\uff0c\u57fa\u4e8e\u4e0d\u540c\u5c40\u90e8\u6d41\u53d1\u6563\u4f30\u8ba1\u6765\u8c03\u8282\u63a8\u529b\u548c\u59ff\u6001\uff0c\u786e\u4fdd\u5e73\u7a33\u7740\u9646\u3002", "motivation": "\u5c0f\u578b\u822a\u5929\u5668\u5982\u65cb\u7ffc\u673a\u548c\u7740\u9646\u5668\u5728\u5761\u9762\u7740\u9646\u65f6\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u4f20\u7edf\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u548c\u91cd\u578b\u4f20\u611f\u5668\u4e0d\u5207\u5b9e\u9645\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u4f4e\u8d44\u6e90\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u7ebf\u6027\u63a7\u5236\u7b56\u7565\uff0c\u5229\u7528\u4e24\u4e2a\u4e0d\u540c\u7684\u5c40\u90e8\u6d41\u53d1\u6563\u4f30\u8ba1\u6765\u8c03\u8282\u5782\u76f4\u7740\u9646\u65f6\u7684\u63a8\u529b\u548c\u59ff\u6001\uff0c\u8be5\u63a7\u5236\u6cd5\u57fa\u4e8e\u589e\u91cf\u975e\u7ebf\u6027\u52a8\u6001\u53cd\u6f14\u3002", "result": "\u901a\u8fc7\u5bf9\u7b80\u5316\u7684\u4e8c\u7ef4\u822a\u5929\u5668\u6a21\u578b\u5728\u4e0d\u540c\u5761\u5ea6\u548c\u53d1\u6563\u8bbe\u5b9a\u70b9\u4e0a\u7684\u6570\u503c\u6a21\u62df\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u63a8\u529b\u63a7\u5236\u7a33\u5b9a\u6027\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u901f\u5ea6\u548c\u9ad8\u5ea6\u7684\u6307\u6570\u8870\u51cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u4e14\u4f4e\u8d44\u6e90\u7684\u7740\u9646\u7b56\u7565\uff0c\u589e\u5f3a\u4e86\u5c0f\u578b\u822a\u5929\u5668\u81ea\u4e3b\u884c\u661f\u4efb\u52a1\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2512.04334", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.04334", "abs": "https://arxiv.org/abs/2512.04334", "authors": ["Chengke Liu", "Wei Xu"], "title": "Human-controllable AI: Meaningful Human Control", "comment": "52 pages", "summary": "Developing human-controllable artificial intelligence (AI) and achieving meaningful human control (MHC) has become a vital principle to address these challenges, ensuring ethical alignment and effective governance in AI. MHC is also a critical focus in human-centered AI (HCAI) research and application. This chapter systematically examines MHC in AI, articulating its foundational principles and future trajectory. MHC is not simply the right to operate, but the unity of human understanding, intervention, and the traceablity of responsibility in AI decision-making, which requires technological design, AI governance, and humans to play a role together. MHC ensures AI autonomy serves humans without constraining technological progress. The mode of human control needs to match the levels of technology, and human supervision should balance the trust and doubt of AI. For future AI systems, MHC mandates human controllability as a prerequisite, requiring: (1) technical architectures with embedded mechanisms for human control; (2) human-AI interactions optimized for better access to human understanding; and (3) the evolution of AI systems harmonizing intelligence and human controllability. Governance must prioritize HCAI strategies: policies balancing innovation and risk mitigation, human-centered participatory frameworks transcending technical elite dominance, and global promotion of MHC as a universal governance paradigm to safeguard HCAI development. Looking ahead, there is a need to strengthen interdisciplinary research on the controllability of AI systems, enhance ethical and legal awareness among stakeholders, moving beyond simplistic technology design perspectives, focus on the knowledge construction, complexity interpretation, and influencing factors surrounding human control. By fostering MHC, the development of human-controllable AI can be further advanced, delivering HCAI systems.", "AI": {"tldr": "\u6b64\u7ae0\u63a2\u8ba8\u4e86\u4eba\u7c7b\u53ef\u63a7AI\u7684\u610f\u4e49\u4e0e\u8981\u6c42\uff0c\u5f3a\u8c03\u4eba\u7c7b\u63a7\u5236\u3001\u6280\u672f\u8bbe\u8ba1\u548c\u6cbb\u7406\u4e4b\u95f4\u7684\u7edf\u4e00\uff0c\u547c\u5401\u52a0\u5f3a\u8de8\u5b66\u79d1\u7814\u7a76\u4ee5\u63a8\u52a8\u4eba\u672c\u5bfc\u5411\u7684\u4eba\u5de5\u667a\u80fd\u53d1\u5c55\u3002", "motivation": "\u5f00\u53d1\u53ef\u63a7\u7684\u4eba\u7c7b\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\uff0c\u4ee5\u786e\u4fdd\u4f26\u7406\u4e00\u81f4\u6027\u548c\u6709\u6548\u6cbb\u7406\uff0c\u5df2\u6210\u4e3a\u5e94\u5bf9\u6311\u6218\u7684\u91cd\u8981\u539f\u5219\u3002", "method": "\u7cfb\u7edf\u5ba1\u67e5\u4eba\u5de5\u667a\u80fd\u4e2d\u7684\u4eba\u7c7b\u63a7\u5236\uff0c\u9610\u660e\u5176\u57fa\u7840\u539f\u5219\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "result": "\u4eba\u7c7b\u63a7\u5236\u4e0d\u4ec5\u4ec5\u662f\u64cd\u4f5c\u6743\uff0c\u800c\u662f\u4eba\u7c7b\u7406\u89e3\u3001\u5e72\u9884\u548c\u8d23\u4efb\u53ef\u8ffd\u6eaf\u6027\u7684\u7edf\u4e00\uff0c\u8981\u6c42\u6280\u672f\u8bbe\u8ba1\u3001AI\u6cbb\u7406\u4e0e\u4eba\u7c7b\u5171\u540c\u53c2\u4e0e\u3002", "conclusion": "\u672a\u6765AI\u7cfb\u7edf\u9700\u4ee5\u4eba\u7c7b\u53ef\u63a7\u6027\u4e3a\u524d\u63d0\uff0c\u5f3a\u8c03\u6280\u672f\u67b6\u6784\u3001\u4eba\u673a\u4ea4\u4e92\u53caAI\u7cfb\u7edf\u7684\u6f14\u53d8\uff0c\u4ee5\u5b9e\u73b0\u4eba\u7c7b\u4e0eAI\u4e4b\u95f4\u7684\u548c\u8c10\u53d1\u5c55\uff0c\u63a8\u52a8\u4eba\u672c\u5bfc\u5411\u7684\u4eba\u5de5\u667a\u80fd\u6cbb\u7406\u653f\u7b56\u3002"}}
{"id": "2512.04381", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.04381", "abs": "https://arxiv.org/abs/2512.04381", "authors": ["Chengyang He", "Ge Sun", "Yue Bai", "Junkai Lu", "Jiadong Zhao", "Guillaume Sartoretti"], "title": "FALCON: Actively Decoupled Visuomotor Policies for Loco-Manipulation with Foundation-Model-Based Coordination", "comment": null, "summary": "We present FoundAtion-model-guided decoupled LoCO-maNipulation visuomotor policies (FALCON), a framework for loco-manipulation that combines modular diffusion policies with a vision-language foundation model as the coordinator. Our approach explicitly decouples locomotion and manipulation into two specialized visuomotor policies, allowing each subsystem to rely on its own observations. This mitigates the performance degradation that arise when a single policy is forced to fuse heterogeneous, potentially mismatched observations from locomotion and manipulation. Our key innovation lies in restoring coordination between these two independent policies through a vision-language foundation model, which encodes global observations and language instructions into a shared latent embedding conditioning both diffusion policies. On top of this backbone, we introduce a phase-progress head that uses textual descriptions of task stages to infer discrete phase and continuous progress estimates without manual phase labels. To further structure the latent space, we incorporate a coordination-aware contrastive loss that explicitly encodes cross-subsystem compatibility between arm and base actions. We evaluate FALCON on two challenging loco-manipulation tasks requiring navigation, precise end-effector placement, and tight base-arm coordination. Results show that it surpasses centralized and decentralized baselines while exhibiting improved robustness and generalization to out-of-distribution scenarios.", "AI": {"tldr": "FALCON\u7ed3\u5408\u6a21\u7ec4\u6269\u6563\u7b56\u7565\u548c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u89e3\u8026\u6b65\u6001\u4e0e\u64cd\u63a7\u7b56\u7565\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u63d0\u51fa\u4e00\u79cd\u6709\u6548\u7684\u6846\u67b6\u6765\u7ed3\u5408\u6b65\u6001\u548c\u64cd\u63a7\u80fd\u529b\uff0c\u89e3\u51b3\u5355\u4e00\u7b56\u7565\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u6a21\u7ec4\u6269\u6563\u7b56\u7565\u548c\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u534f\u8c03\u8005\uff0c\u5c06\u6b65\u6001\u548c\u64cd\u63a7\u7b56\u7565\u663e\u5f0f\u89e3\u8026\uff0c\u5e76\u901a\u8fc7\u76f8\u4e92\u517c\u5bb9\u7684\u5bf9\u6bd4\u635f\u5931\u7ed3\u6784\u5316\u6f5c\u5728\u7a7a\u95f4\u3002", "result": "\u5728\u4e24\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6b65\u6001\u64cd\u63a7\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u8d85\u8d8a\u4e86\u96c6\u4e2d\u548c\u5206\u6563\u57fa\u7ebf\uff0c\u8868\u73b0\u51fa\u66f4\u597d\u7684\u9c81\u68d2\u6027\u548c\u63a8\u5e7f\u80fd\u529b\u3002", "conclusion": "FALCON\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u5176\u4f18\u52bf\uff0c\u5e76\u7f13\u89e3\u4e86\u56e0\u5f02\u6784\u89c2\u5bdf\u800c\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u3002"}}
{"id": "2512.04398", "categories": ["cs.HC", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.04398", "abs": "https://arxiv.org/abs/2512.04398", "authors": ["E. Ch'ng"], "title": "What is Beyond Presence? Dimensionality, Control, and Information Spaces", "comment": "38 pages, accepted for Presence: Virtual and Augmented Reality 2026(37)", "summary": "What is after presence? Spatial presence, the sense of \"being there\", is becoming less of a primary objective and more of a baseline expectation of virtual reality. More than six decades after its invention, VR is shifting from a technical system into a cultural, social, and phenomenological medium, offering experiences that function as distinct modes of reality. Existing theories that focus primarily on perceptual illusions are no longer sufficient to account for these emerging forms of experience. A new framework is needed to guide the design and evaluation of immersive environments by identifying the key technical and abstract dimensions afforded by virtual worlds. These dimensions include spatial, placeness, temporal, social, cultural, cognitive, and psychological parameters. The central argument is that immersive environments must move beyond the technical dimension to leverage richer information channels that shape user experience. This shift from presence to experience orchestration invites creators across disciplines to contribute to the design and assessment of meaningful immersive worlds.", "AI": {"tldr": "\u968f\u7740\u865a\u62df\u73b0\u5b9e\u7684\u53d1\u5c55\uff0c\u6c89\u6d78\u4f53\u9a8c\u5fc5\u987b\u8d85\u8d8a\u6280\u672f\u7ef4\u5ea6\uff0c\u5efa\u7acb\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\u4ee5\u63d0\u5347\u4f53\u9a8c\uff0c\u5e76\u9f13\u52b1\u8de8\u5b66\u79d1\u521b\u4f5c\u8005\u7684\u5408\u4f5c\u3002", "motivation": "\u865a\u62df\u73b0\u5b9e\u6b63\u4ece\u4e00\u79cd\u6280\u672f\u7cfb\u7edf\u8f6c\u53d8\u4e3a\u6587\u5316\u548c\u793e\u4f1a\u5a92\u4ecb\uff0c\u73b0\u6709\u7406\u8bba\u65e0\u6cd5\u5b8c\u5168\u89e3\u91ca\u65b0\u5174\u7684\u4f53\u9a8c\u5f62\u5f0f\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u7406\u8bba\u6846\u67b6\u6765\u9002\u5e94\u8fd9\u4e00\u4e0b\u53d8\u5316\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u6846\u67b6\u4ee5\u6307\u5bfc\u6c89\u6d78\u73af\u5883\u7684\u8bbe\u8ba1\u548c\u8bc4\u4f30", "result": "\u63d0\u51fa\u4e86\u7a7a\u95f4\u3001\u573a\u6240\u6027\u3001\u65f6\u95f4\u3001\u793e\u4f1a\u3001\u6587\u5316\u3001\u8ba4\u77e5\u548c\u5fc3\u7406\u7b49\u7ef4\u5ea6\u4f5c\u4e3a\u865a\u62df\u4e16\u754c\u7684\u5173\u952e\u53c2\u6570\uff0c\u4ece\u800c\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002", "conclusion": "\u6c89\u6d78\u73af\u5883\u7684\u8bbe\u8ba1\u5e94\u8d85\u8d8a\u6280\u672f\u7ef4\u5ea6\uff0c\u5229\u7528\u4e30\u5bcc\u7684\u4fe1\u606f\u901a\u9053\u6765\u5851\u9020\u7528\u6237\u4f53\u9a8c\uff0c\u5f3a\u8c03\u8de8\u5b66\u79d1\u7684\u521b\u4f5c\u8005\u5728\u8bbe\u8ba1\u548c\u8bc4\u4f30\u6709\u610f\u4e49\u7684\u6c89\u6d78\u4e16\u754c\u65b9\u9762\u7684\u53c2\u4e0e\u3002"}}
{"id": "2512.04399", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.04399", "abs": "https://arxiv.org/abs/2512.04399", "authors": ["Haoqi Han", "Yi Yang", "Yifei Yu", "Yixuan Zhou", "Xiaohan Zhu", "Hesheng Wang"], "title": "Development of a 15-Degree-of-Freedom Bionic Hand with Cable-Driven Transmission and Distributed Actuation", "comment": null, "summary": "In robotic hand research, minimizing the number of actuators while maintaining human-hand-consistent dimensions and degrees of freedom constitutes a fundamental challenge. Drawing bio-inspiration from human hand kinematic configurations and muscle distribution strategies, this work proposes a novel 15-DoF dexterous robotic hand, with detailed analysis of its mechanical architecture, electrical system, and control system. The bionic hand employs a new tendon-driven mechanism, significantly reducing the number of motors required by traditional tendon-driven systems while enhancing motion performance and simplifying the mechanical structure. This design integrates five motors in the forearm to provide strong gripping force, while ten small motors are installed in the palm to support fine manipulation tasks. Additionally, a corresponding joint sensing and motor driving electrical system was developed to ensure efficient control and feedback. The entire system weighs only 1.4kg, combining lightweight and high-performance features. Through experiments, the bionic hand exhibited exceptional dexterity and robust grasping capabilities, demonstrating significant potential for robotic manipulation tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65b0\u578b15\u81ea\u7531\u5ea6\u4eff\u751f\u673a\u5668\u4eba\u624b\uff0c\u91c7\u7528\u8171\u9a71\u52a8\u673a\u5236\uff0c\u51cf\u5c11\u7535\u673a\u6570\u91cf\uff0c\u6027\u80fd\u4f18\u8d8a\uff0c\u5177\u6709\u826f\u597d\u7684\u7075\u6d3b\u6027\u548c\u6293\u53d6\u80fd\u529b\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u624b\u90e8\u7814\u7a76\u4e2d\uff0c\u51cf\u5c11\u4f3a\u670d\u5668\u6570\u91cf\u540c\u65f6\u4fdd\u6301\u4e0e\u4eba\u624b\u76f8\u4e00\u81f4\u7684\u5c3a\u5bf8\u548c\u81ea\u7531\u5ea6\u662f\u4e00\u9879\u57fa\u672c\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u768415\u81ea\u7531\u5ea6\u7075\u5de7\u673a\u5668\u4eba\u624b\uff0c\u5206\u6790\u4e86\u5176\u673a\u68b0\u7ed3\u6784\u3001\u7535\u6c14\u7cfb\u7edf\u548c\u63a7\u5236\u7cfb\u7edf\uff0c\u91c7\u7528\u65b0\u7684\u8171\u9a71\u52a8\u673a\u5236\uff0c\u51cf\u5c11\u4e86\u6240\u9700\u7535\u673a\u6570\u91cf\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u8fd0\u52a8\u6027\u80fd\u548c\u7b80\u5316\u4e86\u673a\u68b0\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u4eff\u751f\u624b\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u7075\u6d3b\u6027\u548c\u5f3a\u5927\u7684\u6293\u53d6\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u91cd\u5927\u6f5c\u529b\u3002", "conclusion": "\u8be5\u8bbe\u8ba1\u7ed3\u5408\u4e86\u8f7b\u91cf\u5316\u548c\u9ad8\u6027\u80fd\uff0c\u6574\u4e2a\u7cfb\u7edf\u91cd\u91cf\u4ec5\u4e3a1.4kg\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2512.04692", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.04692", "abs": "https://arxiv.org/abs/2512.04692", "authors": ["Mareike Daeglau", "Stephan Getzmann", "Moritz Bender", "Janina Fels", "Rainer Martin", "Alexander Raake", "Isabel S. Schiller", "Sabine J. Schlittmeier", "Katrin Schoenenberg", "Felix St\u00e4rz", "Leon O. H. Kroczek"], "title": "Interactive Communication -- cross-disciplinary perspectives from psychology, acoustics, and media technology", "comment": null, "summary": "Interactive communication (IC), i.e., the reciprocal exchange of information between two or more interactive partners, is a fundamental part of human nature. As such, it has been studied across multiple scientific disciplines with different goals and methods. This article provides a cross-disciplinary primer on contemporary IC that integrates psychological mechanisms with acoustic and media-technological constraints across theory, measurement, and applications. First, we outline theoretical frameworks that account for verbal, nonverbal and multimodal aspects of IC, including distinctions between face-to-face and computer-mediated communication. Second, we summarize key methodological approaches, including behavioral, cognitive, and experiential measures of communicative synchrony and acoustic signal quality. Third, we discuss selected applications, i.e. assistive listening technologies, conversational agents, alongside ethical considerations. Taken together, this review highlights how human capacities and technical systems jointly shape IC, consolidating concepts, findings, and challenges that have often been discussed in separate lines of research.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u4e92\u52a8\u6c9f\u901a\u7684\u5fc3\u7406\u673a\u5236\u3001\u58f0\u5b66\u548c\u5a92\u4f53\u6280\u672f\u9650\u5236\uff0c\u8de8\u5b66\u79d1\u6574\u5408\u7406\u8bba\u3001\u6d4b\u91cf\u548c\u5e94\u7528\u3002", "motivation": "\u63a2\u8ba8\u4e92\u52a8\u6c9f\u901a\u7684\u5404\u4e2a\u65b9\u9762\u548c\u5176\u91cd\u8981\u6027\uff0c\u6574\u5408\u4e0d\u540c\u5b66\u79d1\u7684\u7814\u7a76\u6210\u679c\u3002", "method": "\u6982\u8ff0\u4e92\u52a8\u6c9f\u901a\u7684\u7406\u8bba\u6846\u67b6\u3001\u65b9\u6cd5\u5b66\u65b9\u6cd5\u53ca\u5e94\u7528\uff0c\u6db5\u76d6\u9762\u8c08\u548c\u8ba1\u7b97\u673a\u5a92\u4ecb\u7684\u6c9f\u901a\u533a\u5206\u3002", "result": "\u603b\u7ed3\u4e86\u884c\u4e3a\u3001\u8ba4\u77e5\u548c\u4f53\u9a8c\u6d4b\u91cf\u7684\u5173\u952e\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u8f85\u52a9\u542c\u529b\u6280\u672f\u3001\u5bf9\u8bdd\u4ee3\u7406\u7b49\u5e94\u7528\u53ca\u4f26\u7406\u8003\u91cf\u3002", "conclusion": "\u5f3a\u8c03\u4eba\u7c7b\u80fd\u529b\u4e0e\u6280\u672f\u7cfb\u7edf\u5982\u4f55\u5171\u540c\u5851\u9020\u4e92\u52a8\u6c9f\u901a\uff0c\u5e76\u6574\u5408\u4e86\u4e0d\u540c\u7814\u7a76\u7ebf\u7684\u6982\u5ff5\u3001\u53d1\u73b0\u548c\u6311\u6218\u3002"}}
{"id": "2512.04404", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.04404", "abs": "https://arxiv.org/abs/2512.04404", "authors": ["Chaoran Wang", "Jingyuan Sun", "Yanhui Zhang", "Changju Wu"], "title": "Bridging Probabilistic Inference and Behavior Trees: An Interactive Framework for Adaptive Multi-Robot Cooperation", "comment": "34 pages, is submitted RAS Journal", "summary": "This paper proposes an Interactive Inference Behavior Tree (IIBT) framework that integrates behavior trees (BTs) with active inference under the free energy principle for distributed multi-robot decision-making. The proposed IIBT node extends conventional BTs with probabilistic reasoning, enabling online joint planning and execution across multiple robots. It remains fully compatible with standard BT architectures, allowing seamless integration into existing multi-robot control systems. Within this framework, multi-robot cooperation is formulated as a free-energy minimization process, where each robot dynamically updates its preference matrix based on perceptual inputs and peer intentions, thereby achieving adaptive coordination in partially observable and dynamic environments. The proposed approach is validated through both simulation and real-world experiments, including a multi-robot maze navigation and a collaborative manipulation task, compared against traditional BTs(https://youtu.be/KX_oT3IDTf4). Experimental results demonstrate that the IIBT framework reduces BT node complexity by over 70%, while maintaining robust, interpretable, and adaptive cooperative behavior under environmental uncertainty.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faIIBT\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u884c\u4e3a\u6811\u548c\u4e3b\u52a8\u63a8\u7406\uff0c\u63d0\u5347\u4e86\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u51b3\u7b56\u4e0e\u5408\u4f5c\u80fd\u529b\uff0c\u4e14\u6709\u6548\u964d\u4f4e\u4e86\u590d\u6742\u5ea6\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u51b3\u7b56\u5236\u5b9a\u4e2d\u7684\u5408\u4f5c\u4e0e\u9002\u5e94\u6027\u95ee\u9898\uff0c\u63d0\u5347\u884c\u4e3a\u6811\u7684\u5e94\u7528\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u4ea4\u4e92\u5f0f\u63a8\u7406\u884c\u4e3a\u6811(IIBT)\u6846\u67b6\uff0c\u5c06\u884c\u4e3a\u6811\u4e0e\u4e3b\u52a8\u63a8\u7406\u901a\u8fc7\u81ea\u7531\u80fd\u539f\u5219\u7ed3\u5408\uff0c\u652f\u6301\u5728\u7ebf\u8054\u5408\u89c4\u5212\u4e0e\u6267\u884c\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u9a8c\u8bc1IIBT\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5728\u591a\u673a\u5668\u4eba\u8ff7\u5bab\u5bfc\u822a\u548c\u534f\u4f5c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u6548\u679c\u663e\u8457\u3002", "conclusion": "IIBT\u6846\u67b6\u5728\u4fdd\u6301\u5f3a\u5065\u3001\u53ef\u89e3\u91ca\u548c\u9002\u5e94\u6027\u5408\u4f5c\u884c\u4e3a\u7684\u540c\u65f6\uff0cBT\u8282\u70b9\u590d\u6742\u5ea6\u964d\u4f4e\u4e8670%\u4ee5\u4e0a\u3002"}}
{"id": "2512.04843", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04843", "abs": "https://arxiv.org/abs/2512.04843", "authors": ["Amy Winecoff", "Kevin Klyman"], "title": "From Symptoms to Systems: An Expert-Guided Approach to Understanding Risks of Generative AI for Eating Disorders", "comment": null, "summary": "Generative AI systems may pose serious risks to individuals vulnerable to eating disorders. Existing safeguards tend to overlook subtle but clinically significant cues, leaving many risks unaddressed. To better understand the nature of these risks, we conducted semi-structured interviews with 15 clinicians, researchers, and advocates with expertise in eating disorders. Using abductive qualitative analysis, we developed an expert-guided taxonomy of generative AI risks across seven categories: (1) providing generalized health advice; (2) encouraging disordered behaviors; (3) supporting symptom concealment; (4) creating thinspiration; (5) reinforcing negative self-beliefs; (6) promoting excessive focus on the body; and (7) perpetuating narrow views about eating disorders. Our results demonstrate how certain user interactions with generative AI systems intersect with clinical features of eating disorders in ways that may intensify risk. We discuss implications of our work, including approaches for risk assessment, safeguard design, and participatory evaluation practices with domain experts.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u751f\u6210\u5f0fAI\u7cfb\u7edf\u5728\u4e0e\u996e\u98df\u5931\u8c03\u76f8\u5173\u7684\u7528\u6237\u4e92\u52a8\u4e2d\u53ef\u80fd\u52a0\u91cd\u98ce\u9669\uff0c\u9700\u9488\u5bf9\u5176\u7279\u70b9\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u98ce\u9669\u7ba1\u7406\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u7684\u4fdd\u62a4\u63aa\u65bd\u672a\u80fd\u5145\u5206\u8bc6\u522b\u5f71\u54cd\u996e\u98df\u5931\u8c03\u8005\u7684\u6f5c\u5728\u98ce\u9669\uff0c\u56e0\u6b64\u4e9f\u9700\u6df1\u5165\u4e86\u89e3\u8fd9\u4e9b\u98ce\u9669\u7684\u6027\u8d28\u3002", "method": "\u901a\u8fc7\u4e0e15\u4f4d\u996e\u98df\u5931\u8c03\u9886\u57df\u7684\u4e34\u5e8a\u533b\u751f\u3001\u7814\u7a76\u4eba\u5458\u548c\u5021\u5bfc\u8005\u8fdb\u884c\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\uff0c\u4f7f\u7528\u6f14\u7ece\u6027\u5b9a\u6027\u5206\u6790\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u751f\u6210\u5f0fAI\u7cfb\u7edf\u4e0e\u996e\u98df\u5931\u8c03\u7684\u4e34\u5e8a\u7279\u5f81\u4ea4\u53c9\uff0c\u53ef\u80fd\u52a0\u5267\u98ce\u9669\uff0c\u5e76\u63d0\u51fa\u4e86\u9488\u5bf9\u98ce\u9669\u8bc4\u4f30\u548c\u4fdd\u62a4\u8bbe\u8ba1\u7684\u5efa\u8bae\u3002", "conclusion": "\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u53ef\u80fd\u52a0\u5267\u996e\u98df\u5931\u8c03\u4eba\u7fa4\u7684\u98ce\u9669\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u98ce\u9669\u8bc4\u4f30\u548c\u4fdd\u62a4\u63aa\u65bd\u8bbe\u8ba1\u3002"}}
{"id": "2512.04415", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.04415", "abs": "https://arxiv.org/abs/2512.04415", "authors": ["Zhoufeng Wang", "Hang Zhao", "Juzhan Xu", "Shishun Zhang", "Zeyu Xiong", "Ruizhen Hu", "Chenyang Zhu", "Kai Xu"], "title": "RoboBPP: Benchmarking Robotic Online Bin Packing with Physics-based Simulation", "comment": "Under review at the International Journal of Robotics Research (IJRR)", "summary": "Physical feasibility in 3D bin packing is a key requirement in modern industrial logistics and robotic automation. With the growing adoption of industrial automation, online bin packing has gained increasing attention. However, inconsistencies in problem settings, test datasets, and evaluation metrics have hindered progress in the field, and there is a lack of a comprehensive benchmarking system. Direct testing on real hardware is costly, and building a realistic simulation environment is also challenging. To address these limitations, we introduce RoboBPP, a benchmarking system designed for robotic online bin packing. RoboBPP integrates a physics-based simulator to assess physical feasibility. In our simulation environment, we introduce a robotic arm and boxes at real-world scales to replicate real industrial packing workflows. By simulating conditions that arise in real industrial applications, we ensure that evaluated algorithms are practically deployable. In addition, prior studies often rely on synthetic datasets whose distributions differ from real-world industrial data. To address this issue, we collect three datasets from real industrial workflows, including assembly-line production, logistics packing, and furniture manufacturing. The benchmark comprises three carefully designed test settings and extends existing evaluation metrics with new metrics for structural stability and operational safety. We design a scoring system and derive a range of insights from the evaluation results. RoboBPP is fully open-source and is equipped with visualization tools and an online leaderboard, providing a reproducible and extensible foundation for future research and industrial applications (https://robot-bin-packing-benchmark.github.io).", "AI": {"tldr": "RoboBPP\u662f\u4e00\u4e2a\u9488\u5bf9\u673a\u5668\u4eba\u5728\u7ebf\u7bb1\u5b50\u5305\u88c5\u7684\u57fa\u51c6\u7cfb\u7edf\uff0c\u96c6\u6210\u7269\u7406\u6a21\u62df\u5668\uff0c\u65e8\u5728\u89e3\u51b3\u5de5\u4e1a\u7269\u6d41\u4e2d\u7684\u7269\u7406\u53ef\u884c\u6027\u95ee\u9898\uff0c\u63d0\u4f9b\u771f\u5b9e\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5728\u7ebf\u7bb1\u5b50\u5305\u88c5\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\u53ca\u7f3a\u4e4f\u6709\u6548\u57fa\u51c6\u7684\u6311\u6218\uff0c\u4ee5\u63d0\u5347\u5de5\u4e1a\u81ea\u52a8\u5316\u7684\u6548\u7387\u3002", "method": "\u63d0\u51faRoboBPP\uff0c\u4e00\u4e2a\u96c6\u6210\u7269\u7406\u57fa\u7840\u6a21\u62df\u5668\u7684\u57fa\u51c6\u7cfb\u7edf\uff0c\u7528\u4e8e\u8bc4\u4f30\u673a\u5668\u4eba\u5728\u7ebf\u7bb1\u5b50\u5305\u88c5\u7684\u7269\u7406\u53ef\u884c\u6027\u3002", "result": "\u901a\u8fc7\u6536\u96c6\u6765\u81ea\u771f\u5b9e\u5de5\u4e1a\u5de5\u4f5c\u6d41\u7a0b\u7684\u4e09\u79cd\u6570\u636e\u96c6\u5e76\u6a21\u62df\u5b9e\u9645\u5de5\u4e1a\u5e94\u7528\u7684\u6761\u4ef6\uff0c\u786e\u4fdd\u7b97\u6cd5\u53ef\u5b9e\u9645\u90e8\u7f72\u3002", "conclusion": "RoboBPP\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u57fa\u51c6\u7cfb\u7edf\uff0c\u63d0\u4f9b\u53ef\u89c6\u5316\u5de5\u5177\u548c\u5728\u7ebf\u6392\u884c\u699c\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u548c\u5de5\u4e1a\u5e94\u7528\u63d0\u4f9b\u4e00\u4e2a\u53ef\u91cd\u590d\u548c\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002"}}
{"id": "2512.05067", "categories": ["cs.HC", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.05067", "abs": "https://arxiv.org/abs/2512.05067", "authors": ["Lalitha A R"], "title": "Perceptually-Minimal Color Optimization for Web Accessibility: A Multi-Phase Constrained Approach", "comment": null, "summary": "Web accessibility guidelines require sufficient color contrast between text and backgrounds; yet, manually adjusting colors often necessitates significant visual deviation, compromising vital brand aesthetics. We present a novel, multi-phase optimization approach for automatically generating WCAG-compliant colors while minimizing perceptual change to original design choices.\n  Our method treats this as a constrained, non-linear optimization problem, utilizing the modern perceptually uniform OKLCH color space. Crucially, the optimization is constrained to preserve the original hue ($\\text{H}$) of the color, ensuring that modifications are strictly limited to necessary adjustments in lightness ($\\text{L}$) and chroma ($\\text{C}$). This is achieved through a three-phase sequence: binary search, gradient descent, and progressive constraint relaxation.\n  Evaluation on a dataset of 10,000 procedurally generated color pairs demonstrates that the algorithm successfully resolves accessibility violations in $77.22\\%$ of cases, with $88.51\\%$ of successful corrections exhibiting imperceptible color difference ($\u0394E_{2000} < 2.0$) as defined by standard perceptibility thresholds. The median perceptual change for successful adjustments is only $0.76\\ \u0394E_{2000}$, and the algorithm achieves this with a median processing time of $0.876\\text{ms}$ per color pair.\n  The approach demonstrates that accessibility compliance and visual design integrity can be achieved simultaneously through a computationally efficient, perceptually-aware optimization that respects brand identity. The algorithm is publicly implemented in the open-source cm-colors Python library.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u9636\u6bb5\u4f18\u5316\u65b9\u6cd5\uff0c\u81ea\u52a8\u751f\u6210\u7b26\u5408WCAG\u6807\u51c6\u7684\u989c\u8272\uff0c\u540c\u65f6\u5c3d\u91cf\u51cf\u5c11\u5bf9\u539f\u59cb\u8bbe\u8ba1\u7684\u89c6\u89c9\u53d8\u5316\u3002", "motivation": "\u89e3\u51b3\u7f51\u9875\u53ef\u8bbf\u95ee\u6027\u6307\u5357\u5bf9\u6587\u672c\u4e0e\u80cc\u666f\u4e4b\u95f4\u7684\u989c\u8272\u5bf9\u6bd4\u8981\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u54c1\u724c\u7f8e\u5b66\u4e0d\u53d7\u7834\u574f\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u4e00\u4e2a\u6709\u7ea6\u675f\u7684\u975e\u7ebf\u6027\u4f18\u5316\u95ee\u9898\uff0c\u5229\u7528OKLCH\u989c\u8272\u7a7a\u95f4\uff0c\u901a\u8fc7\u4e8c\u5206\u641c\u7d22\u3001\u68af\u5ea6\u4e0b\u964d\u548c\u9010\u6b65\u7ea6\u675f\u677e\u5f1b\u7684\u4e09\u9636\u6bb5\u5e8f\u5217\u8fdb\u884c\u5904\u7406\u3002", "result": "\u57281\u4e07\u5bf9\u7a0b\u5e8f\u751f\u6210\u7684\u989c\u8272\u5bf9\u4e2d\uff0c\u7b97\u6cd5\u6210\u529f\u89e3\u51b377.22%\u7684\u53ef\u8bbf\u95ee\u6027\u8fdd\u4f8b\uff0c\u5176\u4e2d88.51%\u7684\u6210\u529f\u4fee\u6b63\u8868\u73b0\u4e3a\u4e0d\u660e\u663e\u7684\u989c\u8272\u5dee\u5f02\uff0c\u611f\u77e5\u53d8\u5316\u4e2d\u4f4d\u6570\u4e3a0.76 \u0394E_{2000}\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u53ef\u8bbf\u95ee\u6027\u5408\u89c4\u548c\u89c6\u89c9\u8bbe\u8ba1\u5b8c\u6574\u6027\u7684\u7edf\u4e00\uff0c\u4e14\u5177\u6709\u9ad8\u6548\u7684\u8ba1\u7b97\u548c\u611f\u77e5\u610f\u8bc6\uff0c\u516c\u5f00\u5b9e\u73b0\u4e8ecm-colors Python\u5e93\u3002"}}
{"id": "2512.04446", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.04446", "abs": "https://arxiv.org/abs/2512.04446", "authors": ["Chang Liu", "Sibo Tian", "Sara Behdad", "Xiao Liang", "Minghui Zheng"], "title": "Vision-Language-Action Models for Selective Robotic Disassembly: A Case Study on Critical Component Extraction from Desktops", "comment": null, "summary": "Automating disassembly of critical components from end-of-life (EoL) desktops, such as high-value items like RAM modules and CPUs, as well as sensitive parts like hard disk drives, remains challenging due to the inherent variability and uncertainty of these products. Moreover, their disassembly requires sequential, precise, and dexterous operations, further increasing the complexity of automation. Current robotic disassembly processes are typically divided into several stages: perception, sequence planning, task planning, motion planning, and manipulation. Each stage requires explicit modeling, which limits generalization to unfamiliar scenarios. Recent development of vision-language-action (VLA) models has presented an end-to-end approach for general robotic manipulation tasks. Although VLAs have demonstrated promising performance on simple tasks, the feasibility of applying such models to complex disassembly remains largely unexplored. In this paper, we collected a customized dataset for robotic RAM and CPU disassembly and used it to fine-tune two well-established VLA approaches, OpenVLA and OpenVLA-OFT, as a case study. We divided the whole disassembly task into several small steps, and our preliminary experimental results indicate that the fine-tuned VLA models can faithfully complete multiple early steps but struggle with certain critical subtasks, leading to task failure. However, we observed that a simple hybrid strategy that combines VLA with a rule-based controller can successfully perform the entire disassembly operation. These findings highlight the current limitations of VLA models in handling the dexterity and precision required for robotic EoL product disassembly. By offering a detailed analysis of the observed results, this study provides insights that may inform future research to address current challenges and advance end-to-end robotic automated disassembly.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5229\u7528\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u8fdb\u884c\u7535\u5b50\u5e9f\u5f03\u7269\u62c6\u89e3\u7684\u53ef\u80fd\u6027\uff0c\u7ed3\u679c\u663e\u793a\u8fd9\u4e9b\u6a21\u578b\u5728\u7b80\u5355\u6b65\u9aa4\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u5374\u5728\u5173\u952e\u5b50\u4efb\u52a1\u4e2d\u9047\u5230\u56f0\u96be\u3002", "motivation": "\u968f\u7740\u7535\u5b50\u5e9f\u5f03\u7269\u7684\u589e\u52a0\uff0c\u81ea\u52a8\u62c6\u89e3\u9ad8\u4ef7\u503c\u548c\u654f\u611f\u7ec4\u4ef6\u7684\u9700\u6c42\u4e0d\u65ad\u4e0a\u5347\uff0c\u7136\u800c\u56e0\u4ea7\u54c1\u7684\u591a\u6837\u6027\u548c\u590d\u6742\u6027\uff0c\u73b0\u6709\u7684\u62c6\u89e3\u6280\u672f\u5b58\u5728\u660e\u663e\u4e0d\u8db3\u3002", "method": "\u6536\u96c6\u5b9a\u5236\u6570\u636e\u96c6\u6765\u5fae\u8c03\u4e24\u79cd\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff08OpenVLA\u548cOpenVLA-OFT\uff09\uff0c\u5e76\u5c06\u6574\u4e2a\u62c6\u89e3\u4efb\u52a1\u62c6\u5206\u4e3a\u591a\u4e2a\u5c0f\u6b65\u9aa4\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5fae\u8c03\u540e\u7684VLA\u6a21\u578b\u80fd\u591f\u5b8c\u6210\u591a\u4e2a\u65e9\u671f\u6b65\u9aa4\uff0c\u4f46\u5728\u67d0\u4e9b\u5173\u952e\u5b50\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u6700\u7ec8\u7ed3\u5408\u57fa\u4e8e\u89c4\u5219\u7684\u63a7\u5236\u5668\u5219\u80fd\u6210\u529f\u6267\u884c\u62c6\u89e3\u3002", "conclusion": "\u5f53\u524d\u7684VLA\u6a21\u578b\u5728\u5904\u7406\u62c6\u89e3\u5de5\u4f5c\u4e2d\u7684\u7075\u6d3b\u6027\u548c\u7cbe\u786e\u5ea6\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4f46\u7ed3\u5408\u57fa\u4e8e\u89c4\u5219\u7684\u63a7\u5236\u5668\u53ef\u4ee5\u6210\u529f\u5b8c\u6210\u6574\u4e2a\u62c6\u89e3\u64cd\u4f5c\u3002"}}
{"id": "2512.04453", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04453", "abs": "https://arxiv.org/abs/2512.04453", "authors": ["Debasmita Ghose", "Oz Gitelson", "Marynel Vazquez", "Brian Scassellati"], "title": "Open-Ended Goal Inference through Actions and Language for Human-Robot Collaboration", "comment": "Accepted to ACM/IEEE International Conference on Human-Robot Interaction, 2026 (HRI 2026), 10 pages, 4 figures", "summary": "To collaborate with humans, robots must infer goals that are often ambiguous, difficult to articulate, or not drawn from a fixed set. Prior approaches restrict inference to a predefined goal set, rely only on observed actions, or depend exclusively on explicit instructions, making them brittle in real-world interactions. We present BALI (Bidirectional Action-Language Inference) for goal prediction, a method that integrates natural language preferences with observed human actions in a receding-horizon planning tree. BALI combines language and action cues from the human, asks clarifying questions only when the expected information gain from the answer outweighs the cost of interruption, and selects supportive actions that align with inferred goals. We evaluate the approach in collaborative cooking tasks, where goals may be novel to the robot and unbounded. Compared to baselines, BALI yields more stable goal predictions and significantly fewer mistakes.", "AI": {"tldr": "BALI\u662f\u4e00\u79cd\u96c6\u6210\u81ea\u7136\u8bed\u8a00\u504f\u597d\u4e0e\u89c2\u5bdf\u5230\u7684\u4eba\u7c7b\u884c\u4e3a\u7684\u76ee\u6807\u9884\u6d4b\u65b9\u6cd5\uff0c\u5728\u5408\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u51cf\u5c11\u9519\u8bef\u3002", "motivation": "\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u534f\u4f5c\u65f6\uff0c\u9700\u8981\u6709\u6548\u63a8\u6d4b\u6a21\u7cca\u548c\u590d\u6742\u7684\u76ee\u6807\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5728\u8fd9\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "BALI\u7ed3\u5408\u4e86\u81ea\u7136\u8bed\u8a00\u548c\u884c\u52a8\u7ebf\u7d22\uff0c\u901a\u8fc7\u9012\u5f52\u89c4\u5212\u6811\u8fdb\u884c\u76ee\u6807\u63a8\u65ad\uff0c\u5e76\u5728\u4fe1\u606f\u589e\u76ca\u5927\u4e8e\u4e2d\u65ad\u6210\u672c\u65f6\u8be2\u95ee\u6f84\u6e05\u95ee\u9898\u3002", "result": "BALI\u5728\u5408\u4f5c\u70f9\u996a\u4efb\u52a1\u4e2d\u76f8\u8f83\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u7684\u76ee\u6807\u9884\u6d4b\uff0c\u9519\u8bef\u7387\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "BALI\u65b9\u6cd5\u5728\u5408\u4f5c\u70f9\u996a\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u7684\u76ee\u6807\u9884\u6d4b\u548c\u663e\u8457\u66f4\u5c11\u7684\u9519\u8bef\uff0c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2512.04502", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.04502", "abs": "https://arxiv.org/abs/2512.04502", "authors": ["Andres Arias", "Wei Zhang", "Haoyu Qian", "Jr-Shin Li", "Chuangchuang Sun"], "title": "One Ring to Rule Them All: Constrained Distributional Control for Massive-Scale Heterogeneous Robotic Ensemble Systems", "comment": "9 pages, 8 figures", "summary": "Ensemble control aims to steer a population of dynamical systems using a shared control input. This paper introduces a constrained ensemble control framework for parameterized, heterogeneous robotic systems operating under state and environmental constraints, such as obstacle avoidance. We develop a moment kernel transform that maps the parameterized ensemble dynamics to the moment system in a kernel space, enabling the characterization of population-level behavior. The state-space constraints, such as polyhedral waypoints to be visited and obstacles to be avoided, are also transformed into the moment space, leading to a unified formulation for safe, large-scale ensemble control. Expressive signal temporal logic specifications are employed to encode complex visit-avoid tasks, which are achieved through a single shared controller synthesized from our constrained ensemble control formulation. Simulation and hardware experiments demonstrate the effectiveness of the proposed approach in safely and efficiently controlling robotic ensembles within constrained environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7ea6\u675f\u96c6\u7fa4\u63a7\u5236\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5f02\u8d28\u673a\u5668\u4eba\u7fa4\u4f53\u7684\u5b89\u5168\u63a7\u5236\u3002", "motivation": "\u4e3a\u4e86\u5728\u5b58\u5728\u969c\u788d\u7269\u548c\u72b6\u6001\u7ea6\u675f\u7684\u73af\u5883\u4e2d\u9ad8\u6548\u5730\u63a7\u5236\u5f02\u8d28\u673a\u5668\u4eba\u7fa4\u4f53\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ea6\u675f\u7684\u96c6\u7fa4\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u5e93\u7a7a\u95f4\u6620\u5c04\u96c6\u7fa4\u52a8\u529b\u5b66\u5230\u77e9\u7cfb\u7edf\uff0c\u4ee5\u5b9e\u73b0\u7fa4\u4f53\u884c\u4e3a\u7684\u8868\u5f81\u3002", "result": "\u5728\u7ea6\u675f\u73af\u5883\u4e2d\uff0c\u5229\u7528\u5171\u4eab\u63a7\u5236\u8f93\u5165\u9ad8\u6548\u63a7\u5236\u5f02\u8d28\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u786e\u4fdd\u5b89\u5168\u7684\u4efb\u52a1\u6267\u884c\u3002", "conclusion": "\u901a\u8fc7\u7ea6\u675f\u7684\u96c6\u7fa4\u63a7\u5236\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u590d\u6742\u73af\u5883\u4e2d\u9ad8\u6548\u3001\u5b89\u5168\u5730\u63a7\u5236\u673a\u5668\u4eba\u96c6\u7fa4\u3002"}}
{"id": "2512.04731", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.04731", "abs": "https://arxiv.org/abs/2512.04731", "authors": ["Jian Tang", "Pu Pang", "Haowen Sun", "Chengzhong Ma", "Xingyu Chen", "Hua Huang", "Xuguang Lan"], "title": "Bridging Simulation and Reality: Cross-Domain Transfer with Semantic 2D Gaussian Splatting", "comment": null, "summary": "Cross-domain transfer in robotic manipulation remains a longstanding challenge due to the significant domain gap between simulated and real-world environments. Existing methods such as domain randomization, adaptation, and sim-real calibration often require extensive tuning or fail to generalize to unseen scenarios. To address this issue, we observe that if domain-invariant features are utilized during policy training in simulation, and the same features can be extracted and provided as the input to policy during real-world deployment, the domain gap can be effectively bridged, leading to significantly improved policy generalization. Accordingly, we propose Semantic 2D Gaussian Splatting (S2GS), a novel representation method that extracts object-centric, domain-invariant spatial features. S2GS constructs multi-view 2D semantic fields and projects them into a unified 3D space via feature-level Gaussian splatting. A semantic filtering mechanism removes irrelevant background content, ensuring clean and consistent inputs for policy learning. To evaluate the effectiveness of S2GS, we adopt Diffusion Policy as the downstream learning algorithm and conduct experiments in the ManiSkill simulation environment, followed by real-world deployment. Results demonstrate that S2GS significantly improves sim-to-real transferability, maintaining high and stable task performance in real-world scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8868\u793a\u65b9\u6cd5S2GS\uff0c\u901a\u8fc7\u63d0\u53d6\u5bf9\u8c61\u4e2d\u5fc3\u7684\u3001\u9886\u57df\u4e0d\u53d8\u7684\u7a7a\u95f4\u7279\u5f81\uff0c\u6709\u6548\u5730\u5f25\u8865\u4e86\u4eff\u771f\u4e0e\u73b0\u5b9e\u4e16\u754c\u4e4b\u95f4\u7684\u9886\u57df\u5dee\u8ddd\uff0c\u5e76\u6539\u5584\u4e86\u7b56\u7565\u6982\u5316\u80fd\u529b\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\uff0c\u4eff\u771f\u4e0e\u73b0\u5b9e\u4e16\u754c\u4e4b\u95f4\u7684\u9886\u57df\u5dee\u8ddd\u5bfc\u81f4\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5982\u9886\u57df\u968f\u673a\u5316\u548c\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8868\u793a\u65b9\u6cd5S2GS\uff0c\u901a\u8fc7\u6784\u5efa\u591a\u89c6\u89d2\u76842D\u8bed\u4e49\u573a\uff0c\u5e76\u901a\u8fc7\u7279\u5f81\u7ea7\u9ad8\u65af\u6563\u5c04\u5c06\u5176\u6295\u5f71\u5230\u7edf\u4e00\u76843D\u7a7a\u95f4\u4e2d\uff0c\u540c\u65f6\u91c7\u7528\u8bed\u4e49\u8fc7\u6ee4\u673a\u5236\u53bb\u9664\u4e0d\u76f8\u5173\u7684\u80cc\u666f\u5185\u5bb9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cS2GS\u5728ManiSkill\u4eff\u771f\u73af\u5883\u4e2d\u589e\u5f3a\u4e86\u7b56\u7565\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u8f6c\u79fb\u80fd\u529b\u3002", "conclusion": "S2GS\u663e\u8457\u63d0\u9ad8\u4e86\u4eff\u771f\u5230\u5b9e\u9645\u8f6c\u79fb\u7684\u80fd\u529b\uff0c\u5e76\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u4fdd\u6301\u4e86\u9ad8\u548c\u7a33\u5b9a\u7684\u4efb\u52a1\u8868\u73b0\u3002"}}
{"id": "2512.04770", "categories": ["cs.RO", "cs.AI", "cs.ET", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.04770", "abs": "https://arxiv.org/abs/2512.04770", "authors": ["Yuxing Wang", "Zhiyu Chen", "Tiantian Zhang", "Qiyue Yin", "Yongzhe Chang", "Zhiheng Li", "Liang Wang", "Xueqian Wang"], "title": "Embodied Co-Design for Rapidly Evolving Agents: Taxonomy, Frontiers, and Challenges", "comment": null, "summary": "Brain-body co-evolution enables animals to develop complex behaviors in their environments. Inspired by this biological synergy, embodied co-design (ECD) has emerged as a transformative paradigm for creating intelligent agents-from virtual creatures to physical robots-by jointly optimizing their morphologies and controllers rather than treating control in isolation. This integrated approach facilitates richer environmental interactions and robust task performance. In this survey, we provide a systematic overview of recent advances in ECD. We first formalize the concept of ECD and position it within related fields. We then introduce a hierarchical taxonomy: a lower layer that breaks down agent design into three fundamental components-controlling brain, body morphology, and task environment-and an upper layer that integrates these components into four major ECD frameworks: bi-level, single-level, generative, and open-ended. This taxonomy allows us to synthesize insights from more than one hundred recent studies. We further review notable benchmarks, datasets, and applications in both simulated and real-world scenarios. Finally, we identify significant challenges and offer insights into promising future research directions. A project associated with this survey has been created at https://github.com/Yuxing-Wang-THU/SurveyBrainBody.", "AI": {"tldr": "\u8eab\u4f53\u73b0\u8c61\u5171\u8bbe\u8ba1\uff08ECD\uff09\u901a\u8fc7\u4f18\u5316\u667a\u80fd\u4f53\u7684\u5f62\u6001\u548c\u63a7\u5236\u5668\uff0c\u589e\u5f3a\u4e86\u73af\u5883\u4ea4\u4e92\u548c\u4efb\u52a1\u8868\u73b0\u3002\u672c\u8c03\u67e5\u63d0\u4f9b\u4e86ECD\u7684\u7cfb\u7edf\u6982\u8ff0\uff0c\u63d0\u51fa\u4e86\u5206\u5c42\u5206\u7c7b\u6cd5\uff0c\u56de\u987e\u4e86\u76f8\u5173\u7814\u7a76\u3001\u57fa\u51c6\u548c\u5e94\u7528\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u63a2\u8ba8\u52a8\u7269\u5728\u73af\u5883\u4e2d\u590d\u6742\u884c\u4e3a\u7684\u5f62\u6210\u673a\u5236\uff0c\u4ee5\u53ca\u5982\u4f55\u901a\u8fc7\u751f\u7269\u542f\u793a\u6765\u6539\u8fdb\u667a\u80fd\u4f53\u7684\u8bbe\u8ba1\u3002", "method": "\u901a\u8fc7\u5efa\u7acb\u4e00\u4e2a\u5206\u5c42\u7684\u5206\u7c7b\u6cd5\uff0c\u5b9a\u4e49\u5e76\u9610\u8ff0\u4e86\u8eab\u4f53\u73b0\u8c61\u5171\u8bbe\u8ba1(ECD)\uff0c\u5e76\u8bc4\u4f30\u4e86\u76f8\u5173\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u3002", "result": "\u5bf9ECD\u7684\u7ec4\u6210\u90e8\u5206\u8fdb\u884c\u7cfb\u7edf\u5316\u5206\u7c7b\uff0c\u5e76\u56de\u987e\u4e86\u8d85\u8fc7\u4e00\u767e\u9879\u76f8\u5173\u7814\u7a76\u7684\u89c1\u89e3\u3002", "conclusion": "\u603b\u7ed3\u4e86\u5f53\u524dECD\u9886\u57df\u7684\u6311\u6218\u4e0e\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\uff0c\u540c\u65f6\u8fd8\u63d0\u4f9b\u4e86\u76f8\u5173\u9879\u76ee\u7684\u94fe\u63a5\u3002"}}
{"id": "2512.04772", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.04772", "abs": "https://arxiv.org/abs/2512.04772", "authors": ["Mauro Martini", "Marco Ambrosio", "Judith Vilella-Cantos", "Alessandro Navone", "Marcello Chiaberge"], "title": "TEMPO-VINE: A Multi-Temporal Sensor Fusion Dataset for Localization and Mapping in Vineyards", "comment": null, "summary": "In recent years, precision agriculture has been introducing groundbreaking innovations in the field, with a strong focus on automation. However, research studies in robotics and autonomous navigation often rely on controlled simulations or isolated field trials. The absence of a realistic common benchmark represents a significant limitation for the diffusion of robust autonomous systems under real complex agricultural conditions. Vineyards pose significant challenges due to their dynamic nature, and they are increasingly drawing attention from both academic and industrial stakeholders interested in automation. In this context, we introduce the TEMPO-VINE dataset, a large-scale multi-temporal dataset specifically designed for evaluating sensor fusion, simultaneous localization and mapping (SLAM), and place recognition techniques within operational vineyard environments. TEMPO-VINE is the first multi-modal public dataset that brings together data from heterogeneous LiDARs of different price levels, AHRS, RTK-GPS, and cameras in real trellis and pergola vineyards, with multiple rows exceeding 100 m in length. In this work, we address a critical gap in the landscape of agricultural datasets by providing researchers with a comprehensive data collection and ground truth trajectories in different seasons, vegetation growth stages, terrain and weather conditions. The sequence paths with multiple runs and revisits will foster the development of sensor fusion, localization, mapping and place recognition solutions for agricultural fields. The dataset, the processing tools and the benchmarking results will be available at the dedicated webpage upon acceptance.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a8\u51fa\u4e86TEMPO-VINE\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u519c\u4e1a\u9886\u57df\u73b0\u6709\u6570\u636e\u96c6\u7684\u4e0d\u8db3\uff0c\u4e3b\u8981\u5173\u6ce8\u4e8e\u4f20\u611f\u5668\u878d\u5408\u548cSLAM\u6280\u672f\u5728\u8461\u8404\u56ed\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u7531\u4e8e\u5bf9\u771f\u5b9e\u590d\u6742\u519c\u4e1a\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u81ea\u4e3b\u7cfb\u7edf\u7684\u9700\u6c42\u65e5\u76ca\u589e\u52a0\uff0c\u5c24\u5176\u662f\u5728\u52a8\u6001\u7684\u8461\u8404\u56ed\u73af\u5883\u4e2d\uff0c\u7814\u7a76\u4eba\u5458\u6025\u9700\u4e00\u4e2a\u771f\u5b9e\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "method": "\u5efa\u7acbTEMPO-VINE\u6570\u636e\u96c6\uff0c\u91c7\u96c6\u4e0d\u540c\u4ef7\u683c\u7ea7\u522b\u7684LiDAR\u3001AHRS\u3001RTK-GPS\u548c\u6444\u50cf\u5934\u7684\u6570\u636e\uff0c\u8986\u76d6\u591a\u4e2a\u751f\u957f\u9636\u6bb5\u548c\u6c14\u5019\u6761\u4ef6\u3002", "result": "TEMPO-VINE\u662f\u9996\u4e2a\u591a\u6a21\u6001\u516c\u5171\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u771f\u5b9e\u4f5c\u4e1a\u73af\u5883\u7684\u6570\u636e\uff0c\u7528\u4e8e\u4fc3\u8fdb\u519c\u4e1a\u9886\u57df\u4f20\u611f\u5668\u878d\u5408\u3001\u5b9a\u4f4d\u548c\u6620\u5c04\u6280\u672f\u7684\u53d1\u5c55\u3002", "conclusion": "TEMPO-VINE\u6570\u636e\u96c6\u586b\u8865\u4e86\u519c\u4e1a\u9886\u57df\u7f3a\u4e4f\u771f\u5b9e\u590d\u6742\u73af\u5883\u57fa\u51c6\u7684\u7a7a\u767d\uff0c\u4e3a\u673a\u5668\u4eba\u548c\u81ea\u52a8\u5316\u6280\u672f\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\u3002"}}
{"id": "2512.04773", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04773", "abs": "https://arxiv.org/abs/2512.04773", "authors": ["Giorgos Polychronis", "Foivos Pournaropoulos", "Christos D. Antonopoulos", "Spyros Lalis"], "title": "Using Machine Learning to Take Stay-or-Go Decisions in Data-driven Drone Missions", "comment": "19 pages, 3 figures, to appear in the proceedings of MobiQuitous 2025", "summary": "Drones are becoming indispensable in many application domains. In data-driven missions, besides sensing, the drone must process the collected data at runtime to decide whether additional action must be taken on the spot, before moving to the next point of interest. If processing does not reveal an event or situation that requires such an action, the drone has waited in vain instead of moving to the next point. If, however, the drone starts moving to the next point and it turns out that a follow-up action is needed at the previous point, it must spend time to fly-back. To take this decision, we propose different machine-learning methods based on branch prediction and reinforcement learning. We evaluate these methods for a wide range of scenarios where the probability of event occurrence changes with time. Our results show that the proposed methods consistently outperform the regression-based method proposed in the literature and can significantly improve the worst-case mission time by up to 4.1x. Also, the achieved median mission time is very close, merely up to 2.7% higher, to that of a method with perfect knowledge of the current underlying event probability at each point of interest.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u51b3\u7b56\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u65e0\u4eba\u673a\u5728\u52a8\u6001\u4e8b\u4ef6\u53d1\u751f\u6982\u7387\u53d8\u5316\u4e0b\u7684\u4efb\u52a1\u6267\u884c\u6548\u7387\uff0c\u7ed3\u679c\u663e\u793a\u5728\u591a\u4e2a\u573a\u666f\u4e2d\u6709\u663e\u8457\u6539\u5584\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u5728\u6570\u636e\u9a71\u52a8\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u5982\u4f55\u66f4\u6709\u6548\u5730\u5904\u7406\u5b9e\u65f6\u6570\u636e\u5e76\u4f5c\u51fa\u51b3\u7b56\u6210\u4e3a\u5173\u952e\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e0d\u540c\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5177\u4f53\u5305\u62ec\u57fa\u4e8e\u5206\u652f\u9884\u6d4b\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u8bc4\u4f30\u8fd9\u4e9b\u65b9\u6cd5\u5728\u591a\u79cd\u65f6\u95f4\u53d8\u5316\u60c5\u5883\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u6587\u732e\u4e2d\u57fa\u4e8e\u56de\u5f52\u7684\u65b9\u6cd5\uff0c\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u4efb\u52a1\u65f6\u95f4\u63d0\u9ad8\u4e864.1\u500d\uff0c\u800c\u4e2d\u4f4d\u4efb\u52a1\u65f6\u95f4\u4ec5\u6bd4\u5b8c\u7f8e\u77e5\u8bc6\u7684\u65b9\u6cd5\u9ad8\u51fa2.7%\u3002", "conclusion": "\u901a\u8fc7\u4f7f\u7528\u57fa\u4e8e\u5206\u652f\u9884\u6d4b\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u6211\u4eec\u7684\u7814\u7a76\u663e\u8457\u63d0\u9ad8\u4e86\u65e0\u4eba\u673a\u5728\u6570\u636e\u9a71\u52a8\u4efb\u52a1\u4e2d\u7684\u51b3\u7b56\u6548\u7387\uff0c\u51cf\u5c11\u4e86\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u3002"}}
{"id": "2512.04813", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.04813", "abs": "https://arxiv.org/abs/2512.04813", "authors": ["Huanqian Wang", "Chi Bene Chen", "Yang Yue", "Danhua Tao", "Tong Guo", "Shaoxuan Xie", "Denghang Huang", "Shiji Song", "Guocai Yao", "Gao Huang"], "title": "MOVE: A Simple Motion-Based Data Collection Paradigm for Spatial Generalization in Robotic Manipulation", "comment": "9 pages, 9 figures", "summary": "Imitation learning method has shown immense promise for robotic manipulation, yet its practical deployment is fundamentally constrained by the data scarcity. Despite prior work on collecting large-scale datasets, there still remains a significant gap to robust spatial generalization. We identify a key limitation: individual trajectories, regardless of their length, are typically collected from a \\emph{single, static spatial configuration} of the environment. This includes fixed object and target spatial positions as well as unchanging camera viewpoints, which significantly restricts the diversity of spatial information available for learning. To address this critical bottleneck in data efficiency, we propose \\textbf{MOtion-Based Variability Enhancement} (\\emph{MOVE}), a simple yet effective data collection paradigm that enables the acquisition of richer spatial information from dynamic demonstrations. Our core contribution is an augmentation strategy that injects motion into any movable objects within the environment for each demonstration. This process implicitly generates a dense and diverse set of spatial configurations within a single trajectory. We conduct extensive experiments in both simulation and real-world environments to validate our approach. For example, in simulation tasks requiring strong spatial generalization, \\emph{MOVE} achieves an average success rate of 39.1\\%, a 76.1\\% relative improvement over the static data collection paradigm (22.2\\%), and yields up to 2--5$\\times$ gains in data efficiency on certain tasks. Our code is available at https://github.com/lucywang720/MOVE.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u6570\u636e\u6536\u96c6\u65b9\u6cd5\uff0cMOtion-Based Variability Enhancement (MOVE)\uff0c\u901a\u8fc7\u5411\u6f14\u793a\u4e2d\u7684\u53ef\u79fb\u52a8\u5bf9\u8c61\u6ce8\u5165\u8fd0\u52a8\uff0c\u63d0\u9ad8\u6570\u636e\u7684\u7a7a\u95f4\u591a\u6837\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u5b66\u4e60\u6548\u7387\u548c\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u5e94\u7528\u53d7\u5230\u6570\u636e\u7a00\u7f3a\u7684\u9650\u5236\uff0c\u800c\u9759\u6001\u73af\u5883\u914d\u7f6e\u5bfc\u81f4\u6536\u96c6\u7684\u6570\u636e\u7f3a\u4e4f\u7a7a\u95f4\u591a\u6837\u6027\u3002", "method": "\u901a\u8fc7\u5728\u6f14\u793a\u4e2d\u5f15\u5165\u8fd0\u52a8\uff0c\u4f7f\u73af\u5883\u4e2d\u7684\u53ef\u79fb\u52a8\u5bf9\u8c61\u53d8\u5f97\u52a8\u6001\uff0c\u4ece\u800c\u751f\u6210\u66f4\u4e30\u5bcc\u548c\u591a\u6837\u7684\u7a7a\u95f4\u914d\u7f6e\uff0c\u63d0\u9ad8\u6570\u636e\u7684\u6709\u6548\u6027\u548c\u673a\u5668\u4eba\u5b66\u4e60\u80fd\u529b\u3002", "result": "MOtion-Based Variability Enhancement (MOVE) \u662f\u4e00\u79cd\u6570\u636e\u6536\u96c6\u8303\u5f0f\uff0c\u901a\u8fc7\u5728\u6bcf\u6b21\u6f14\u793a\u4e2d\u5411\u73af\u5883\u4e2d\u7684\u53ef\u79fb\u52a8\u5bf9\u8c61\u6ce8\u5165\u8fd0\u52a8\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u7a7a\u95f4\u4fe1\u606f\u7684\u4e30\u5bcc\u6027\u3002", "conclusion": "MOVE \u663e\u8457\u6539\u5584\u4e86\u6a21\u62df\u4efb\u52a1\u4e2d\u7684\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\uff0c\u53d6\u5f97\u4e8639.1%\u7684\u6210\u529f\u7387\uff0c\u76f8\u6bd4\u9759\u6001\u6570\u636e\u6536\u96c6\u65b9\u6cd5\u63d0\u5347\u4e8676.1%\u3002"}}
{"id": "2512.04884", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.04884", "abs": "https://arxiv.org/abs/2512.04884", "authors": ["Tim Engelbracht", "Ren\u00e9 Zurbr\u00fcgg", "Matteo Wohlrapp", "Martin B\u00fcchner", "Abhinav Valada", "Marc Pollefeys", "Hermann Blum", "Zuria Bauer"], "title": "Hoi! -- A Multimodal Dataset for Force-Grounded, Cross-View Articulated Manipulation", "comment": null, "summary": "We present a dataset for force-grounded, cross-view articulated manipulation that couples what is seen with what is done and what is felt during real human interaction. The dataset contains 3048 sequences across 381 articulated objects in 38 environments. Each object is operated under four embodiments - (i) human hand, (ii) human hand with a wrist-mounted camera, (iii) handheld UMI gripper, and (iv) a custom Hoi! gripper - where the tool embodiment provides synchronized end-effector forces and tactile sensing. Our dataset offers a holistic view of interaction understanding from video, enabling researchers to evaluate how well methods transfer between human and robotic viewpoints, but also investigate underexplored modalities such as force sensing and prediction.", "AI": {"tldr": "\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b3048\u4e2a\u4ea4\u4e92\u5e8f\u5217\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u529b\u611f\u77e5\uff0c\u4ee5\u652f\u6301\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u89c6\u89d2\u95f4\u7684\u7814\u7a76", "motivation": "\u63a8\u52a8\u4eba\u673a\u4ea4\u4e92\u7406\u89e3\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u548c\u52a8\u6001\u529b\u6570\u636e\uff0c\u7814\u7a76\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u4ea4\u4e92\u65b9\u5f0f", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u6570\u636e\u96c6\uff0c\u6db5\u76d63048\u4e2a\u5e8f\u5217\uff0c\u6d89\u53ca381\u4e2a\u5173\u8282\u7269\u4f53\u53ca38\u4e2a\u73af\u5883\uff0c\u4f7f\u7528\u591a\u79cd\u5de5\u5177\u64cd\u4f5c\u5e76\u540c\u6b65\u611f\u77e5\u529b\u548c\u89e6\u89c9", "result": "\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u4ece\u89c6\u9891\u4e2d\u7406\u89e3\u4ea4\u4e92\u7684\u6574\u4f53\u89c6\u89d2\uff0c\u652f\u6301\u7814\u7a76\u8005\u8bc4\u4f30\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u89c6\u89d2\u95f4\u7684\u65b9\u6cd5\u8fc1\u79fb", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u529b\u57fa\u7840\u7684\u8de8\u89c6\u89d2\u5173\u8282\u64cd\u4f5c\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u5e73\u53f0\uff0c\u62d3\u5c55\u4e86\u529b\u611f\u77e5\u4e0e\u9884\u6d4b\u7684\u7814\u7a76\u9886\u57df"}}
{"id": "2512.04917", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.04917", "abs": "https://arxiv.org/abs/2512.04917", "authors": ["Matteo Masoni", "Vincenzo Palermo", "Marco Gabiccini", "Martino Gulisano", "Giorgio Previati", "Massimiliano Gobbi", "Francesco Comolli", "Gianpiero Mastinu", "Massimo Guiggiani"], "title": "On Disturbance-Aware Minimum-Time Trajectory Planning: Evidence from Tests on a Dynamic Driving Simulator", "comment": "18 pages, 11 figures, 5 tables", "summary": "This work investigates how disturbance-aware, robustness-embedded reference trajectories translate into driving performance when executed by professional drivers in a dynamic simulator. Three planned reference trajectories are compared against a free-driving baseline (NOREF) to assess trade-offs between lap time (LT) and steering effort (SE): NOM, the nominal time-optimal trajectory; TLC, a track-limit-robust trajectory obtained by tightening margins to the track edges; and FLC, a friction-limit-robust trajectory obtained by tightening against axle and tire saturation. All trajectories share the same minimum lap-time objective with a small steering-smoothness regularizer and are evaluated by two professional drivers using a high-performance car on a virtual track. The trajectories derive from a disturbance-aware minimum-lap-time framework recently proposed by the authors, where worst-case disturbance growth is propagated over a finite horizon and used to tighten tire-friction and track-limit constraints, preserving performance while providing probabilistic safety margins. LT and SE are used as performance indicators, while RMS lateral deviation, speed error, and drift angle characterize driving style. Results show a Pareto-like LT-SE trade-off: NOM yields the shortest LT but highest SE; TLC minimizes SE at the cost of longer LT; FLC lies near the efficient frontier, substantially reducing SE relative to NOM with only a small LT increase. Removing trajectory guidance (NOREF) increases both LT and SE, confirming that reference trajectories improve pace and control efficiency. Overall, the findings highlight reference-based and disturbance-aware planning, especially FLC, as effective tools for training and for achieving fast yet stable trajectories.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5e72\u6270\u611f\u77e5\u7684\u53c2\u8003\u8f68\u8ff9\u5bf9\u4e13\u4e1a\u9a7e\u9a76\u5458\u5728\u52a8\u6001\u6a21\u62df\u5668\u4e2d\u9a7e\u9a76\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u6bd4\u8f83\u4e86\u4e09\u79cd\u53c2\u8003\u8f68\u8ff9\u548c\u81ea\u7531\u9a7e\u9a76\u57fa\u7ebf\uff0c\u7ed3\u679c\u663e\u793a\u53c2\u8003\u8f68\u8ff9\u80fd\u6709\u6548\u63d0\u9ad8\u9a7e\u9a76\u6548\u7387\u3002", "motivation": "\u63a2\u8ba8\u5728\u52a8\u6001\u6a21\u62df\u73af\u5883\u4e2d\uff0c\u5982\u4f55\u901a\u8fc7\u8bbe\u8ba1\u5e72\u6270\u611f\u77e5\u548c\u7a33\u5065\u6027\u7684\u53c2\u8003\u8f68\u8ff9\u6765\u63d0\u5347\u9a7e\u9a76\u6027\u80fd\u3002", "method": "\u6bd4\u8f83\u4e86\u4e09\u79cd\u53c2\u8003\u8f68\u8ff9\uff08NOM, TLC, FLC\uff09\u4e0e\u81ea\u7531\u9a7e\u9a76\u57fa\u7ebf\uff08NOREF\uff09\uff0c\u8bc4\u4f30\u5176\u5728\u5708\u901f\u548c\u8f6c\u5411\u52aa\u529b\u4e4b\u95f4\u7684\u6743\u8861\u3002", "result": "\u53d1\u73b0NOM\u5708\u901f\u6700\u77ed\u4f46\u8f6c\u5411\u52aa\u529b\u6700\u9ad8\uff0cTLC\u5728\u5708\u901f\u4e0e\u8f6c\u5411\u52aa\u529b\u4e4b\u95f4\u5212\u5206\u4e86\u826f\u597d\u7684\u5e73\u8861\uff0cFLC\u5728\u51cf\u5c11\u8f6c\u5411\u52aa\u529b\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u5bf9\u8f83\u77ed\u7684\u5708\u901f\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8e\u53c2\u8003\u7684\u5e72\u6270\u611f\u77e5\u89c4\u5212\u65b9\u6cd5\uff0c\u7279\u522b\u662fFLC\uff0c\u80fd\u591f\u6709\u6548\u7528\u4e8e\u8bad\u7ec3\u548c\u5b9e\u73b0\u5feb\u901f\u7a33\u5b9a\u7684\u9a7e\u9a76\u8f68\u8ff9\u3002"}}
{"id": "2512.04960", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.04960", "abs": "https://arxiv.org/abs/2512.04960", "authors": ["Jonne Van Haastregt", "Bastian Orthmann", "Michael C. Welle", "Yuchong Zhang", "Danica Kragic"], "title": "Hybrid-Diffusion Models: Combining Open-loop Routines with Visuomotor Diffusion Policies", "comment": null, "summary": "Despite the fact that visuomotor-based policies obtained via imitation learning demonstrate good performances in complex manipulation tasks, they usually struggle to achieve the same accuracy and speed as traditional control based methods. In this work, we introduce Hybrid-Diffusion models that combine open-loop routines with visuomotor diffusion policies. We develop Teleoperation Augmentation Primitives (TAPs) that allow the operator to perform predefined routines, such as locking specific axes, moving to perching waypoints, or triggering task-specific routines seamlessly during demonstrations. Our Hybrid-Diffusion method learns to trigger such TAPs during inference. We validate the method on challenging real-world tasks: Vial Aspiration, Open-Container Liquid Transfer, and container unscrewing. All experimental videos are available on the project's website: https://hybriddiffusion.github.io/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6269\u6563\u6a21\u578b\uff0c\u6709\u6548\u7ed3\u5408\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u548c\u9884\u5b9a\u4e49\u64cd\u4f5c\u7a0b\u5e8f\uff0c\u63d0\u5347\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u7684\u7cbe\u5ea6\u548c\u901f\u5ea6\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u89c6\u89c9\u8fd0\u52a8\u7684\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u5728\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u65e0\u6cd5\u4e0e\u4f20\u7edf\u63a7\u5236\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u901f\u5ea6\u4e0a\u5339\u654c\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408\u5f00\u653e\u5faa\u73af\u7a0b\u5e8f\u548c\u89c6\u89c9\u8fd0\u52a8\u6269\u6563\u7b56\u7565\uff0c\u5f00\u53d1\u4e86\u8fdc\u7a0b\u64cd\u4f5c\u589e\u5f3a\u539f\u8bed TAPs\uff0c\u4f7f\u5f97\u64cd\u4f5c\u5458\u80fd\u591f\u5728\u6f14\u793a\u8fc7\u7a0b\u4e2d\u65e0\u7f1d\u6267\u884c\u9884\u5b9a\u4e49\u7684\u4f8b\u7a0b\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u8fdb\u884c\u9a8c\u8bc1\uff0c\u5305\u62ec\u8bd5\u7ba1\u62bd\u53d6\u3001\u5f00\u653e\u5bb9\u5668\u6db2\u4f53\u8f6c\u79fb\u548c\u5bb9\u5668\u62e7\u677e\u3002", "conclusion": "\u6df7\u5408\u6269\u6563\u65b9\u6cd5\u901a\u8fc7\u5b66\u4e60\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u89e6\u53d1 TAPs\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u64cd\u4f5c\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2512.04973", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.04973", "abs": "https://arxiv.org/abs/2512.04973", "authors": ["Giuseppe Milazzo", "Manuel G. Catalano", "Antonio Bicchi", "Giorgio Grioli"], "title": "Preliminary Analysis and Simulation of a Compact Variable Stiffness Wrist", "comment": "This article has been accepted for publication in Springer Proceedings in Advanced Robotics, vol 31. Springer, Cham. This is the author's version, which has not been fully edited, and the content may change prior to final publication. Citation information: DOI https://doi.org/10.1007/978-3-031-64057-5_9", "summary": "Variable Stiffness Actuators prove invaluable for robotics applications in unstructured environments, fostering safe interactions and enhancing task adaptability. Nevertheless, their mechanical design inevitably results in larger and heavier structures compared to classical rigid actuators. This paper introduces a novel 3 Degrees of Freedom (DoFs) parallel wrist that achieves variable stiffness through redundant elastic actuation. Leveraging its parallel architecture, the device employs only four motors, rendering it compact and lightweight. This characteristic makes it particularly well-suited for applications in prosthetics or humanoid robotics. The manuscript delves into the theoretical model of the device and proposes a sophisticated control strategy for independent regulation of joint position and stiffness. Furthermore, it validates the proposed controller through simulation, utilizing a comprehensive analysis of the system dynamics. The reported results affirm the ability of the device to achieve high accuracy and disturbance rejection in rigid configurations while minimizing interaction forces with its compliant behavior.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u4e09\u81ea\u7531\u5ea6\u5e76\u884c\u8155\uff0c\u5229\u7528\u5197\u4f59\u5f39\u6027\u9a71\u52a8\u5b9e\u73b0\u53ef\u53d8\u521a\u5ea6\uff0c\u9002\u5408\u5047\u80a2\u548c\u7c7b\u4eba\u673a\u5668\u4eba\uff0c\u7ecf\u8fc7\u4eff\u771f\u9a8c\u8bc1\u63a7\u5236\u7cbe\u5ea6\u548c\u6297\u5e72\u6270\u80fd\u529b\u3002", "motivation": "\u53ef\u53d8\u521a\u5ea6\u9a71\u52a8\u5668\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u673a\u68b0\u8bbe\u8ba1\u5bfc\u81f4\u7ed3\u6784\u66f4\u5927\u66f4\u91cd\u3002", "method": "\u4ecb\u7ecd\u4e00\u79cd\u65b0\u578b\u7684\u4e09\u81ea\u7531\u5ea6\u5e76\u884c\u8155\u4f20\u52a8\u88c5\u7f6e\uff0c\u901a\u8fc7\u5197\u4f59\u5f39\u6027\u9a71\u52a8\u5b9e\u73b0\u53ef\u53d8\u521a\u5ea6\u3002", "result": "\u901a\u8fc7\u4f7f\u7528\u4ec5\u56db\u4e2a\u7535\u673a\uff0c\u8be5\u8bbe\u5907\u5728\u5e76\u884c\u67b6\u6784\u4e0b\u5b9e\u73b0\u4e86\u7d27\u51d1\u548c\u8f7b\u91cf\u5316\u7684\u8bbe\u8ba1\uff0c\u9002\u7528\u4e8e\u5047\u80a2\u6216\u7c7b\u4eba\u673a\u5668\u4eba\u5e94\u7528\u3002", "conclusion": "\u8be5\u63a7\u5236\u5668\u7ecf\u8fc7\u4eff\u771f\u9a8c\u8bc1\uff0c\u7cfb\u7edf\u52a8\u529b\u5b66\u5206\u6790\u8868\u660e\uff0c\u8bbe\u5907\u5728\u521a\u6027\u914d\u7f6e\u4e0b\u5177\u5907\u9ad8\u7cbe\u5ea6\u548c\u6297\u5e72\u6270\u80fd\u529b\uff0c\u540c\u65f6\u4ee5\u5176\u67d4\u6027\u884c\u4e3a\u6700\u5c0f\u5316\u4e86\u4ea4\u4e92\u529b\u3002"}}
{"id": "2512.04998", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.04998", "abs": "https://arxiv.org/abs/2512.04998", "authors": ["Giuseppe Milazzo", "Giorgio Grioli", "Antonio Bicchi", "Manuel G. Catalano"], "title": "Introducing V-Soft Pro: a Modular Platform for a Transhumeral Prosthesis with Controllable Stiffness", "comment": "This article has been accepted for publication in Proceedings of the International Conference On Rehabilitation Robotics (ICORR), 2025. This is the author's version, which has not been fully edited, and content may change prior to final publication. Citation information: DOI 10.1109/ICORR66766.2025.11062964", "summary": "Current upper limb prostheses aim to enhance user independence in daily activities by incorporating basic motor functions. However, they fall short of replicating the natural movement and interaction capabilities of the human arm. In contrast, human limbs leverage intrinsic compliance and actively modulate joint stiffness, enabling adaptive responses to varying tasks, impact absorption, and efficient energy transfer during dynamic actions. Inspired by this adaptability, we developed a transhumeral prosthesis with Variable Stiffness Actuators (VSAs) to replicate the controllable compliance found in biological joints. The proposed prosthesis features a modular design, allowing customization for different residual limb shapes and accommodating a range of independent control signals derived from users' biological cues. Integrated elastic elements passively support more natural movements, facilitate safe interactions with the environment, and adapt to diverse task requirements. This paper presents a comprehensive overview of the platform and its functionalities, highlighting its potential applications in the field of prosthetics.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5e26\u6709\u53ef\u53d8\u521a\u5ea6\u81f4\u52a8\u5668\u7684\u80b1\u9aa8\u5047\u80a2\uff0c\u65e8\u5728\u590d\u5236\u751f\u7269\u5173\u8282\u7684\u53ef\u63a7\u987a\u5e94\u6027\uff0c\u4ee5\u63d0\u5347\u5047\u80a2\u7684\u81ea\u7136\u79fb\u52a8\u548c\u73af\u5883\u4ea4\u4e92\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u4e0a\u80a2\u5047\u80a2\u5728\u63d0\u5347\u7528\u6237\u65e5\u5e38\u6d3b\u52a8\u72ec\u7acb\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u65e0\u6cd5\u5b8c\u5168\u590d\u5236\u4eba\u7c7b\u624b\u81c2\u7684\u81ea\u7136\u8fd0\u52a8\u548c\u4ea4\u4e92\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u5e26\u6709\u53ef\u53d8\u521a\u5ea6\u81f4\u52a8\u5668\u7684\u80b1\u9aa8\u5047\u80a2\uff0c\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u652f\u6301\u4e0d\u540c\u7684\u6b8b\u80a2\u5f62\u72b6\u548c\u7528\u6237\u7684\u751f\u7269\u4fe1\u53f7\u63a7\u5236\u3002", "result": "\u5047\u80a2\u6574\u5408\u7684\u5f39\u6027\u5143\u4ef6\u652f\u6301\u66f4\u81ea\u7136\u7684\u8fd0\u52a8\uff0c\u4fc3\u8fdb\u5b89\u5168\u7684\u73af\u5883\u4e92\u52a8\uff0c\u5e76\u9002\u5e94\u591a\u6837\u5316\u7684\u4efb\u52a1\u9700\u6c42\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u53ef\u53d8\u521a\u5ea6\u5047\u80a2\u5177\u6709\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u80fd\u591f\u9002\u5e94\u5404\u79cd\u6b8b\u80a2\u5f62\u72b6\uff0c\u5e76\u6839\u636e\u7528\u6237\u7684\u751f\u7269\u4fe1\u53f7\u8fdb\u884c\u72ec\u7acb\u63a7\u5236\uff0c\u5c55\u793a\u4e86\u5728\u5047\u80a2\u9886\u57df\u7684\u91cd\u8981\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2512.05008", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.05008", "abs": "https://arxiv.org/abs/2512.05008", "authors": ["Haroon Hublikar"], "title": "Contact-Implicit Modeling and Simulation of a Snake Robot on Compliant and Granular Terrain", "comment": null, "summary": "This thesis presents a unified modeling and simulation framework for analyzing sidewinding and tumbling locomotion of the COBRA snake robot across rigid, compliant, and granular terrains. A contact-implicit formulation is used to model distributed frictional interactions during sidewinding, and validated through MATLAB Simscape simulations and physical experiments on rigid ground and loose sand. To capture terrain deformation effects, Project Chrono's Soil Contact Model (SCM) is integrated with the articulated multibody dynamics, enabling prediction of slip, sinkage, and load redistribution that reduce stride efficiency on deformable substrates. For high-energy rolling locomotion on steep slopes, the Chrono DEM Engine is used to simulate particle-resolved granular interactions, revealing soil failure, intermittent lift-off, and energy dissipation mechanisms not captured by rigid models. Together, these methods span real-time control-oriented simulation and high-fidelity granular physics. Results demonstrate that rigid-ground models provide accurate short-horizon motion prediction, while continuum and particle-based terrain modeling becomes necessary for reliable mobility analysis in soft and highly dynamic environments. This work establishes a hierarchical simulation pipeline that advances robust, terrain-aware locomotion for robots operating in challenging unstructured settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u5efa\u6a21\u4e0e\u4eff\u771f\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790COBRA\u86c7\u5f62\u673a\u5668\u4eba\u5728\u4e0d\u540c\u5730\u5f62\u4e0b\u7684\u79fb\u52a8\u884c\u4e3a\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u5728\u590d\u6742\u73af\u5883\u4e2d\u673a\u5668\u4eba\u7684\u884c\u8d70\u80fd\u529b\uff0c\u63a2\u7d22\u4e0d\u540c\u5730\u5f62\u5bf9\u673a\u5668\u8fd0\u52a8\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u63a5\u89e6\u9690\u5f0f\u516c\u5f0f\u8fdb\u884c\u6469\u64e6\u4ea4\u4e92\u5efa\u6a21\uff0c\u7ed3\u5408MATLAB Simscape\u548c\u7269\u7406\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u540c\u65f6\u96c6\u6210\u4e86\u571f\u58e4\u63a5\u89e6\u6a21\u578b\u4ee5\u6355\u6349\u5730\u5f62\u53d8\u5f62\u6548\u5e94\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u521a\u6027\u5730\u9762\u6a21\u578b\u5bf9\u77ed\u671f\u8fd0\u52a8\u9884\u6d4b\u7cbe\u51c6\uff0c\u800c\u5bf9\u4e8e\u67d4\u8f6f\u548c\u52a8\u6001\u590d\u6742\u73af\u5883\u7684\u53ef\u9760\u79fb\u52a8\u5206\u6790\u5219\u9700\u8981\u8fde\u7eed\u4f53\u548c\u7c92\u5b50\u57fa\u7840\u7684\u5730\u5f62\u5efa\u6a21\u3002", "conclusion": "\u672c\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u4e2a\u5206\u5c42\u4eff\u771f\u7ba1\u9053\uff0c\u63a8\u52a8\u4e86\u673a\u5668\u4eba\u5728\u6311\u6218\u6027\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u7a33\u5065\u611f\u77e5\u79fb\u52a8\u80fd\u529b\u3002"}}
{"id": "2512.05094", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.05094", "abs": "https://arxiv.org/abs/2512.05094", "authors": ["James Ni", "Zekai Wang", "Wei Lin", "Amir Bar", "Yann LeCun", "Trevor Darrell", "Jitendra Malik", "Roei Herzig"], "title": "From Generated Human Videos to Physically Plausible Robot Trajectories", "comment": "For project website, see https://genmimic.github.io", "summary": "Video generation models are rapidly improving in their ability to synthesize human actions in novel contexts, holding the potential to serve as high-level planners for contextual robot control. To realize this potential, a key research question remains open: how can a humanoid execute the human actions from generated videos in a zero-shot manner? This challenge arises because generated videos are often noisy and exhibit morphological distortions that make direct imitation difficult compared to real video. To address this, we introduce a two-stage pipeline. First, we lift video pixels into a 4D human representation and then retarget to the humanoid morphology. Second, we propose GenMimic-a physics-aware reinforcement learning policy conditioned on 3D keypoints, and trained with symmetry regularization and keypoint-weighted tracking rewards. As a result, GenMimic can mimic human actions from noisy, generated videos. We curate GenMimicBench, a synthetic human-motion dataset generated using two video generation models across a spectrum of actions and contexts, establishing a benchmark for assessing zero-shot generalization and policy robustness. Extensive experiments demonstrate improvements over strong baselines in simulation and confirm coherent, physically stable motion tracking on a Unitree G1 humanoid robot without fine-tuning. This work offers a promising path to realizing the potential of video generation models as high-level policies for robot control.", "AI": {"tldr": "\u63d0\u51faGenMimic\uff0c\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u673a\u5668\u4eba\u63a7\u5236\u7b56\u7565\uff0c\u80fd\u591f\u5728\u96f6-shot\u60c5\u51b5\u4e0b\u4ece\u751f\u6210\u89c6\u9891\u4e2d\u6a21\u4eff\u4eba\u7c7b\u52a8\u4f5c\uff0c\u65e0\u9700\u5fae\u8c03\uff0c\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u8ba9\u7c7b\u4eba\u673a\u5668\u4eba\u5728\u96f6-shot\u60c5\u51b5\u4e0b\u6267\u884c\u751f\u6210\u89c6\u9891\u4e2d\u7684\u4eba\u7c7b\u52a8\u4f5c\uff0c\u4ee5\u5b9e\u73b0\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a\u9996\u5148\u5c06\u89c6\u9891\u50cf\u7d20\u63d0\u5347\u4e3a4D\u4eba\u7c7b\u8868\u793a\uff0c\u7136\u540e\u5bf9\u7c7b\u4eba\u5f62\u6001\u8fdb\u884c\u91cd\u5b9a\u5411\uff1b\u5176\u6b21\u63d0\u51faGenMimic\uff0c\u4e00\u4e2a\u57fa\u4e8e\u7269\u7406\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u6761\u4ef6\u4e3a3D\u5173\u952e\u70b9\uff0c\u5e76\u8bad\u7ec3\u65f6\u4f7f\u7528\u5bf9\u79f0\u6b63\u5219\u5316\u548c\u5173\u952e\u70b9\u52a0\u6743\u8ddf\u8e2a\u5956\u52b1\u3002", "result": "GenMimic\u80fd\u591f\u4ece\u5608\u6742\u7684\u751f\u6210\u89c6\u9891\u4e2d\u6a21\u4eff\u4eba\u7c7b\u52a8\u4f5c\uff0c\u4f7f\u7528\u7684GenMimicBench\u6570\u636e\u96c6\u4e3a\u96f6-shot\u6cdb\u5316\u548c\u7b56\u7565\u9c81\u68d2\u6027\u8bc4\u4f30\u5efa\u7acb\u4e86\u57fa\u51c6\u3002", "conclusion": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u5728\u6a21\u62df\u4e2d\u76f8\u8f83\u4e8e\u5f3a\u57fa\u7ebf\u6709\u663e\u8457\u6539\u8fdb\uff0c\u5e76\u5728Unitree G1\u7c7b\u4eba\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e00\u81f4\u3001\u7269\u7406\u7a33\u5b9a\u7684\u8fd0\u52a8\u8ddf\u8e2a\uff0c\u65e0\u9700\u5fae\u8c03\uff0c\u4e3a\u5b9e\u73b0\u89c6\u9891\u751f\u6210\u6a21\u578b\u4f5c\u4e3a\u673a\u5668\u4eba\u63a7\u5236\u7684\u9ad8\u5c42\u7b56\u7565\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002"}}
{"id": "2512.05107", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.05107", "abs": "https://arxiv.org/abs/2512.05107", "authors": ["Feng Xu", "Guangyao Zhai", "Xin Kong", "Tingzhong Fu", "Daniel F. N. Gordon", "Xueli An", "Benjamin Busam"], "title": "STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models", "comment": null, "summary": "Recent advances in Vision-Language-Action (VLA) models, powered by large language models and reinforcement learning-based fine-tuning, have shown remarkable progress in robotic manipulation. Existing methods often treat long-horizon actions as linguistic sequences and apply trajectory-level optimization methods such as Trajectory-wise Preference Optimization (TPO) or Proximal Policy Optimization (PPO), leading to coarse credit assignment and unstable training. However, unlike language, where a unified semantic meaning is preserved despite flexible sentence order, action trajectories progress through causally chained stages with different learning difficulties. This motivates progressive stage optimization. Thereby, we present Stage-Aware Reinforcement (STARE), a module that decomposes a long-horizon action trajectory into semantically meaningful stages and provides dense, interpretable, and stage-aligned reinforcement signals. Integrating STARE into TPO and PPO, we yield Stage-Aware TPO (STA-TPO) and Stage-Aware PPO (STA-PPO) for offline stage-wise preference and online intra-stage interaction, respectively. Further building on supervised fine-tuning as initialization, we propose the Imitation -> Preference -> Interaction (IPI), a serial fine-tuning pipeline for improving action accuracy in VLA models. Experiments on SimplerEnv and ManiSkill3 demonstrate substantial gains, achieving state-of-the-art success rates of 98.0 percent on SimplerEnv and 96.4 percent on ManiSkill3 tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSTARE\u6a21\u5757\u548cIPI\u7ba1\u9053\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6210\u529f\u7387\uff0c\u5c24\u5176\u662f\u5728\u957f\u65f6\u95f4\u52a8\u4f5c\u8f68\u8ff9\u7684\u4f18\u5316\u4e2d\u3002", "motivation": "\u4f20\u7edf\u7684\u957f\u8fdc\u52a8\u4f5c\u4f18\u5316\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u65f6\u5b58\u5728\u4fe1\u7528\u5206\u914d\u7c97\u7cd9\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u91c7\u7528\u9010\u6b65\u9636\u6bb5\u4f18\u5316\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86STARE\u6a21\u5757\uff0c\u5c06\u957f\u65f6\u95f4\u52a8\u4f5c\u8f68\u8ff9\u5206\u89e3\u4e3a\u8bed\u4e49\u660e\u786e\u7684\u9636\u6bb5\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230TPO\u548cPPO\u4e2d\uff0c\u5f62\u6210STA-TPO\u548cSTA-PPO\u3002\u540c\u65f6\uff0c\u8bbe\u8ba1\u4e86IPI\u7684\u5e8f\u5217\u5fae\u8c03\u7ba1\u9053\u6765\u63d0\u9ad8\u52a8\u4f5c\u51c6\u786e\u6027\u3002", "result": "\u5728SimplesEnv\u548cManiSkill3\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7ecf\u8fc7\u6539\u8fdb\u7684\u6a21\u578b\u5728\u8fd9\u4e24\u4e2a\u73af\u5883\u4e2d\u7684\u6210\u529f\u7387\u5206\u522b\u8fbe\u5230\u4e8698.0%\u548c96.4%\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165STARE\u6a21\u5757\u5e76\u7ed3\u5408\u7ba1\u9053IPI\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728\u590d\u6742\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6210\u529f\u7387\uff0c\u5c55\u793a\u4e86\u5728\u9010\u6b65\u9636\u6bb5\u4f18\u5316\u4e0b\u7684\u5f3a\u5316\u5b66\u4e60\u7684\u4f18\u52bf\u3002"}}
