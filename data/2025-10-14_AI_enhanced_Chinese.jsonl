{"id": "2510.09786", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09786", "abs": "https://arxiv.org/abs/2510.09786", "authors": ["Yuang Lu", "Song Wang", "Xiao Han", "Xuri Zhang", "Yucong Wu", "Zhicheng He"], "title": "Enhancing Diffusion Policy with Classifier-Free Guidance for Temporal Robotic Tasks", "comment": "7 pages, 7 figures", "summary": "Temporal sequential tasks challenge humanoid robots, as existing Diffusion\nPolicy (DP) and Action Chunking with Transformers (ACT) methods often lack\ntemporal context, resulting in local optima traps and excessive repetitive\nactions. To address these issues, this paper introduces a Classifier-Free\nGuidance-Based Diffusion Policy (CFG-DP), a novel framework to enhance DP by\nintegrating Classifier-Free Guidance (CFG) with conditional and unconditional\nmodels. Specifically, CFG leverages timestep inputs to track task progression\nand ensure precise cycle termination. It dynamically adjusts action predictions\nbased on task phase, using a guidance factor tuned to balance temporal\ncoherence and action accuracy. Real-world experiments on a humanoid robot\ndemonstrate high success rates and minimal repetitive actions. Furthermore, we\nassessed the model's ability to terminate actions and examined how different\ncomponents and parameter adjustments affect its performance. This framework\nsignificantly enhances deterministic control and execution reliability for\nsequential robotic tasks.", "AI": {"tldr": "\u5f15\u5165CFG-DP\u6846\u67b6\uff0c\u7ed3\u5408\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u65b9\u6cd5\uff0c\u63d0\u5347\u4eba\u5f62\u673a\u5668\u4eba\u5728\u65f6\u5e8f\u4efb\u52a1\u4e2d\u7684\u63a7\u5236\u6548\u679c\uff0c\u5c24\u5176\u5728\u51cf\u5c11\u91cd\u590d\u52a8\u4f5c\u548c\u786e\u4fdd\u4efb\u52a1\u5b8c\u7ed3\u65b9\u9762\u8868\u73b0\u4f18\u79c0\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u73b0\u6709\u6269\u6563\u7b56\u7565\u5728\u5904\u7406\u65f6\u5e8f\u4efb\u52a1\u65f6\u9762\u4e34\u7684\u7f3a\u4e4f\u65f6\u95f4\u4e0a\u4e0b\u6587\u5bfc\u81f4\u7684\u5c40\u90e8\u6700\u4f18\u4e0e\u91cd\u590d\u884c\u4e3a\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u4e0e\u6761\u4ef6\u53ca\u65e0\u6761\u4ef6\u6a21\u578b\u7ed3\u5408\uff0c\u4ee5\u65f6\u95f4\u6b65\u8f93\u5165\u8ddf\u8e2a\u4efb\u52a1\u8fdb\u5c55\uff0c\u5e76\u52a8\u6001\u8c03\u6574\u57fa\u4e8e\u4efb\u52a1\u9636\u6bb5\u7684\u52a8\u4f5c\u9884\u6d4b\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u7684\u6269\u6563\u7b56\u7565\uff08CFG-DP\uff09\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u4eba\u5f62\u673a\u5668\u4eba\u5728\u65f6\u5e8f\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u51cf\u5c11\u5c40\u90e8\u6700\u4f18\u548c\u91cd\u590d\u52a8\u4f5c\u95ee\u9898\u3002", "conclusion": "CFG-DP\u6846\u67b6\u663e\u8457\u6539\u5584\u4e86\u5e8f\u5217\u673a\u5668\u4eba\u4efb\u52a1\u7684\u786e\u5b9a\u6027\u63a7\u5236\u548c\u6267\u884c\u53ef\u9760\u6027\u3002"}}
{"id": "2510.09817", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09817", "abs": "https://arxiv.org/abs/2510.09817", "authors": ["Samanta Rodriguez", "Yiming Dou", "Miquel Oller", "Andrew Owens", "Nima Fazeli"], "title": "Cross-Sensor Touch Generation", "comment": "CoRL 2025", "summary": "Today's visuo-tactile sensors come in many shapes and sizes, making it\nchallenging to develop general-purpose tactile representations. This is because\nmost models are tied to a specific sensor design. To address this challenge, we\npropose two approaches to cross-sensor image generation. The first is an\nend-to-end method that leverages paired data (Touch2Touch). The second method\nbuilds an intermediate depth representation and does not require paired data\n(T2D2: Touch-to-Depth-to-Touch). Both methods enable the use of sensor-specific\nmodels across multiple sensors via the cross-sensor touch generation process.\nTogether, these models offer flexible solutions for sensor translation,\ndepending on data availability and application needs. We demonstrate their\neffectiveness on downstream tasks such as in-hand pose estimation and behavior\ncloning, successfully transferring models trained on one sensor to another.\nProject page: https://samantabelen.github.io/cross_sensor_touch_generation.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e24\u79cd\u9488\u5bf9\u8de8\u4f20\u611f\u5668\u56fe\u50cf\u751f\u6210\u7684\u65b9\u6cd5\uff0c\u5206\u522b\u662f\u57fa\u4e8e\u914d\u5bf9\u6570\u636e\u7684Touch2Touch\u548c\u4e0d\u9700\u8981\u914d\u5bf9\u6570\u636e\u7684T2D2\uff0c\u4e8c\u8005\u53ef\u5728\u591a\u79cd\u4f20\u611f\u5668\u95f4\u8fdb\u884c\u6709\u6548\u7684\u6a21\u578b\u8f6c\u6362\u548c\u5e94\u7528\u3002", "motivation": "\u7531\u4e8e\u591a\u6570\u6a21\u578b\u4f9d\u8d56\u7279\u5b9a\u7684\u4f20\u611f\u5668\u8bbe\u8ba1\uff0c\u8feb\u5207\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u901a\u7528\u7684\u89e6\u89c9\u8868\u793a\uff0c\u4ee5\u9002\u5e94\u591a\u6837\u7684\u4f20\u611f\u5668\u8bbe\u8ba1\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1aTouch2Touch\uff08\u57fa\u4e8e\u914d\u5bf9\u6570\u636e\uff09\u548cT2D2\uff08\u57fa\u4e8e\u4e2d\u95f4\u6df1\u5ea6\u8868\u793a\uff09\uff0c\u540e\u8005\u4e0d\u4f9d\u8d56\u4e8e\u914d\u5bf9\u6570\u636e\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u8de8\u4f20\u611f\u5668\u56fe\u50cf\u751f\u6210\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u591a\u79cd\u5f62\u72b6\u548c\u5c3a\u5bf8\u7684\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u5668\u6240\u5e26\u6765\u7684\u4e00\u822c\u6027\u89e6\u89c9\u8868\u793a\u5f00\u53d1\u7684\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u7684\u65b9\u6cd5\u5728\u5404\u7c7b\u4e0b\u6e38\u4efb\u52a1\u4e2d\u6709\u6548\u5730\u5b9e\u73b0\u4e86\u4e0d\u540c\u4f20\u611f\u5668\u4e4b\u95f4\u6a21\u578b\u7684\u6210\u529f\u8fc1\u79fb\uff0c\u5c55\u793a\u4e86\u5176\u8de8\u4f20\u611f\u5668\u89e6\u89c9\u751f\u6210\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.09962", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09962", "abs": "https://arxiv.org/abs/2510.09962", "authors": ["Yicheng He", "Jingwen Yu", "Guangcheng Chen", "Hong Zhang"], "title": "VG-Mapping: Variation-Aware 3D Gaussians for Online Semi-static Scene Mapping", "comment": null, "summary": "Maintaining an up-to-date map that accurately reflects recent changes in the\nenvironment is crucial, especially for robots that repeatedly traverse the same\nspace. Failing to promptly update the changed regions can degrade map quality,\nresulting in poor localization, inefficient operations, and even lost robots.\n3D Gaussian Splatting (3DGS) has recently seen widespread adoption in online\nmap reconstruction due to its dense, differentiable, and photorealistic\nproperties, yet accurately and efficiently updating the regions of change\nremains a challenge. In this paper, we propose VG-Mapping, a novel online\n3DGS-based mapping system tailored for such semi-static scenes. Our approach\nintroduces a hybrid representation that augments 3DGS with a TSDF-based voxel\nmap to efficiently identify changed regions in a scene, along with a\nvariation-aware density control strategy that inserts or deletes Gaussian\nprimitives in regions undergoing change. Furthermore, to address the absence of\npublic benchmarks for this task, we construct a RGB-D dataset comprising both\nsynthetic and real-world semi-static environments. Experimental results\ndemonstrate that our method substantially improves the rendering quality and\nmap update efficiency in semi-static scenes. The code and dataset are available\nat https://github.com/heyicheng-never/VG-Mapping.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D Gaussian Splatting\u7684\u5728\u7ebf\u5730\u56fe\u91cd\u5efa\u7cfb\u7edfVG-Mapping\uff0c\u9488\u5bf9\u534a\u9759\u6001\u573a\u666f\u5b9e\u73b0\u9ad8\u6548\u53d8\u5316\u533a\u57df\u66f4\u65b0\u3002", "motivation": "\u5feb\u901f\u66f4\u65b0\u673a\u5668\u4eba\u5468\u56f4\u73af\u5883\u7684\u5730\u56fe\u4ee5\u9632\u6b62\u56e0\u672a\u53ca\u65f6\u66f4\u65b0\u53d8\u5316\u533a\u57df\u800c\u5bfc\u81f4\u7684\u5b9a\u4f4d\u53ca\u64cd\u4f5c\u6548\u7387\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u8868\u793a\u65b9\u6cd5\uff0c\u7ed3\u54083DGS\u4e0eTSDF\u4f53\u7d20\u5730\u56fe\uff0c\u91c7\u7528\u53d8\u5f02\u611f\u77e5\u5bc6\u5ea6\u63a7\u5236\u7b56\u7565\u5bf9\u53d8\u5316\u533a\u57df\u8fdb\u884c\u9ad8\u6548\u66f4\u65b0\u3002", "result": "VG-Mapping\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u534a\u9759\u6001\u573a\u666f\u4e2d\u7684\u6e32\u67d3\u8d28\u91cf\u548c\u5730\u56fe\u66f4\u65b0\u6548\u7387\u3002", "conclusion": "VG-Mapping\u901a\u8fc7\u7ed3\u54083DGS\u4e0eTSDF\u4f53\u7d20\u5730\u56fe\uff0c\u5728\u53d8\u5316\u68c0\u6d4b\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u5347\u4e86\u52a8\u6001\u73af\u5883\u4e0b\u7684\u5730\u56fe\u8d28\u91cf\u4e0e\u66f4\u65b0\u901f\u5ea6\u3002"}}
{"id": "2510.09963", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09963", "abs": "https://arxiv.org/abs/2510.09963", "authors": ["Chaoran Wang", "Jingyuan Sun", "Yanhui Zhang", "Mingyu Zhang", "Changju Wu"], "title": "LLM-HBT: Dynamic Behavior Tree Construction for Adaptive Coordination in Heterogeneous Robots", "comment": "It contains 8 pages, 7 figures and 4 tables. This paper is submitted\n  to ICRA 2026", "summary": "We introduce a novel framework for automatic behavior tree (BT) construction\nin heterogeneous multi-robot systems, designed to address the challenges of\nadaptability and robustness in dynamic environments. Traditional robots are\nlimited by fixed functional attributes and cannot efficiently reconfigure their\nstrategies in response to task failures or environmental changes. To overcome\nthis limitation, we leverage large language models (LLMs) to generate and\nextend BTs dynamically, combining the reasoning and generalization power of\nLLMs with the modularity and recovery capability of BTs. The proposed framework\nconsists of four interconnected modules task initialization, task assignment,\nBT update, and failure node detection which operate in a closed loop. Robots\ntick their BTs during execution, and upon encountering a failure node, they can\neither extend the tree locally or invoke a centralized virtual coordinator\n(Alex) to reassign subtasks and synchronize BTs across peers. This design\nenables long-term cooperative execution in heterogeneous teams. We validate the\nframework on 60 tasks across three simulated scenarios and in a real-world cafe\nenvironment with a robotic arm and a wheeled-legged robot. Results show that\nour method consistently outperforms baseline approaches in task success rate,\nrobustness, and scalability, demonstrating its effectiveness for multi-robot\ncollaboration in complex scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u52a8\u884c\u4e3a\u6811\u6784\u5efa\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u9ad8\u5f02\u6784\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\uff0c\u7ecf\u8fc7\u9a8c\u8bc1\uff0c\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u4efb\u52a1\u5931\u8d25\u540e\u7684\u9002\u5e94\u6027\u5dee\u548c\u7b56\u7565\u91cd\u6784\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u548c\u6269\u5c55\u884c\u4e3a\u6811\uff0c\u7ed3\u5408\u4efb\u52a1\u521d\u59cb\u5316\u3001\u4efb\u52a1\u5206\u914d\u3001\u884c\u4e3a\u6811\u66f4\u65b0\u548c\u6545\u969c\u8282\u70b9\u68c0\u6d4b\u7b49\u6a21\u5757\uff0c\u5f62\u6210\u95ed\u73af\u7cfb\u7edf\u3002", "result": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u52a8\u884c\u4e3a\u6811\u6784\u5efa\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u9ad8\u5f02\u6784\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002\u901a\u8fc7\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u6211\u4eec\u80fd\u591f\u52a8\u6001\u751f\u6210\u548c\u6269\u5c55\u884c\u4e3a\u6811\uff0c\u4ee5\u5e94\u5bf9\u4efb\u52a1\u5931\u8d25\u548c\u73af\u5883\u53d8\u5316\u6240\u5e26\u6765\u7684\u6311\u6218\u3002\u8be5\u6846\u67b6\u5305\u542b\u4efb\u52a1\u521d\u59cb\u5316\u3001\u4efb\u52a1\u5206\u914d\u3001\u884c\u4e3a\u6811\u66f4\u65b0\u548c\u6545\u969c\u8282\u70b9\u68c0\u6d4b\u56db\u4e2a\u6a21\u5757\uff0c\u5f62\u6210\u95ed\u73af\u64cd\u4f5c\u3002\u673a\u5668\u4eba\u5728\u6267\u884c\u8fc7\u7a0b\u4e2d\u68c0\u67e5\u884c\u4e3a\u6811\uff0c\u5e76\u5728\u9047\u5230\u6545\u969c\u8282\u70b9\u65f6\uff0c\u80fd\u591f\u9009\u62e9\u5728\u672c\u5730\u6269\u5c55\u6811\u6216\u8c03\u7528\u4e2d\u592e\u865a\u62df\u534f\u8c03\u8005\uff08Alex\uff09\u91cd\u65b0\u5206\u914d\u5b50\u4efb\u52a1\u4e0e\u540c\u6b65\u884c\u4e3a\u6811\u3002\u8be5\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u5f02\u6784\u56e2\u961f\u7684\u957f\u671f\u5408\u4f5c\u6267\u884c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4efb\u52a1\u6210\u529f\u7387\u3001\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u5176\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u591a\u673a\u5668\u4eba\u534f\u4f5c\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u63d0\u9ad8\u4e86\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u534f\u4f5c\u80fd\u529b\uff0c\u5177\u6709\u826f\u597d\u7684\u9002\u5e94\u6027\u4e0e\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.09763", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.09763", "abs": "https://arxiv.org/abs/2510.09763", "authors": ["Donghan Hu", "Rameen Mahmood", "Annabelle David", "Danny Yuxing Huang"], "title": "Network Traffic as a Scalable Ethnographic Lens for Understanding University Students' AI Tool Practices", "comment": null, "summary": "AI-driven applications have become woven into students' academic and creative\nworkflows, influencing how they learn, write, and produce ideas. Gaining a\nnuanced understanding of these usage patterns is essential, yet conventional\nsurvey and interview methods remain limited by recall bias, self-presentation\neffects, and the underreporting of habitual behaviors. While ethnographic\nmethods offer richer contextual insights, they often face challenges of scale\nand reproducibility. To bridge this gap, we introduce a privacy-conscious\napproach that repurposes VPN-based network traffic analysis as a scalable\nethnographic technique for examining students' real-world engagement with AI\ntools. By capturing anonymized metadata rather than content, this method\nenables fine-grained behavioral tracing while safeguarding personal\ninformation, thereby complementing self-report data. A three-week field\ndeployment with university students reveals fragmented, short-duration\ninteractions across multiple tools and devices, with intense bursts of activity\ncoinciding with exam periods-patterns mirroring institutional rhythms of\nacademic life. We conclude by discussing methodological, ethical, and empirical\nimplications, positioning network traffic analysis as a promising avenue for\nlarge-scale digital ethnography on technology-in-practice.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8eVPN\u7f51\u7edc\u6d41\u91cf\u5206\u6790\u7684\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u65e0\u504f\u89c1\u5730\u7406\u89e3\u5b66\u751f\u4e0eAI\u5de5\u5177\u7684\u4e92\u52a8\uff0c\u7ed3\u5408\u4f20\u7edf\u81ea\u6211\u62a5\u544a\u6570\u636e\uff0c\u63ed\u793a\u4e86\u77ed\u671f\u3001\u5206\u6563\u7684\u4e92\u52a8\u6a21\u5f0f\u3002", "motivation": "\u7406\u89e3AI\u5de5\u5177\u5728\u5b66\u751f\u5b66\u4e60\u4e0e\u521b\u4f5c\u4e2d\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u5e76\u514b\u670d\u4f20\u7edf\u7814\u7a76\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u5bf9VPN\u7f51\u7edc\u6d41\u91cf\u8fdb\u884c\u5206\u6790\uff0c\u6355\u83b7\u533f\u540d\u5143\u6570\u636e\uff0c\u800c\u975e\u5185\u5bb9\uff0c\u4ee5\u4fdd\u62a4\u4e2a\u4eba\u4fe1\u606f\uff0c\u540c\u65f6\u8fdb\u884c\u7ec6\u81f4\u7684\u884c\u4e3a\u8ffd\u8e2a\u3002", "result": "\u4e3a\u671f\u4e09\u5468\u7684\u5b9e\u5730\u7814\u7a76\u663e\u793a\uff0c\u5b66\u751f\u4e0eAI\u5de5\u5177\u7684\u4e92\u52a8\u77ed\u6682\u4e14\u5206\u6563\uff0c\u5b66\u4e60\u6d3b\u52a8\u4e0e\u5b66\u672f\u5468\u671f\u9ad8\u5ea6\u76f8\u5173\u3002", "conclusion": "\u7f51\u7edc\u6d41\u91cf\u5206\u6790\u5c55\u73b0\u51fa\u4f5c\u4e3a\u5927\u89c4\u6a21\u6570\u5b57\u4eba\u7c7b\u5b66\u7684\u65b0\u673a\u4f1a\uff0c\u7279\u522b\u662f\u5728\u6280\u672f\u5b9e\u8df5\u65b9\u9762\u3002"}}
{"id": "2510.09966", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09966", "abs": "https://arxiv.org/abs/2510.09966", "authors": ["Easton R. Potokar", "Taylor Pool", "Daniel McGann", "Michael Kaess"], "title": "FORM: Fixed-Lag Odometry with Reparative Mapping utilizing Rotating LiDAR Sensors", "comment": "Submitted to ICRA 2026", "summary": "Light Detection and Ranging (LiDAR) sensors have become a de-facto sensor for\nmany robot state estimation tasks, spurring development of many LiDAR Odometry\n(LO) methods in recent years. While some smoothing-based LO methods have been\nproposed, most require matching against multiple scans, resulting in\nsub-real-time performance. Due to this, most prior works estimate a single\nstate at a time and are ``submap''-based. This architecture propagates any\nerror in pose estimation to the fixed submap and can cause jittery trajectories\nand degrade future registrations. We propose Fixed-Lag Odometry with Reparative\nMapping (FORM), a LO method that performs smoothing over a densely connected\nfactor graph while utilizing a single iterative map for matching. This allows\nfor both real-time performance and active correction of the local map as pose\nestimates are further refined. We evaluate on a wide variety of datasets to\nshow that FORM is robust, accurate, real-time, and provides smooth trajectory\nestimates when compared to prior state-of-the-art LO methods.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u5149\u5b66\u91cc\u7a0b\u8ba1\u65b9\u6cd5FORM\uff0c\u80fd\u591f\u63d0\u9ad8\u624b\u672f\u7cbe\u786e\u5ea6\u548c\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4ee5\u5f80\u65b9\u6cd5\u4e2d\u56e0\u5b50\u5730\u56fe\u56fa\u5b9a\u5bfc\u81f4\u7684\u9519\u8bef\u4f20\u64ad\u548c\u8f68\u8ff9\u6296\u52a8\u95ee\u9898\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u72b6\u6001\u4f30\u8ba1\u3002", "method": "\u901a\u8fc7\u5728\u4e00\u4e2a\u5bc6\u96c6\u8fde\u63a5\u7684\u56e0\u5b50\u56fe\u4e0a\u8fdb\u884c\u5e73\u6ed1\uff0c\u540c\u65f6\u5229\u7528\u5355\u4e00\u7684\u8fed\u4ee3\u5730\u56fe\u8fdb\u884c\u5339\u914d\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684LiDAR\u6d4b\u91cf\u914d\u7f6e--FORM\uff0c\u65e8\u5728\u63d0\u9ad8\u673a\u68b0\u72b6\u6001\u4f30\u8ba1\u7684\u6027\u80fd\u548c\u5b9e\u65f6\u6027\u3002", "conclusion": "FORM\u65b9\u6cd5\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u4f9b\u4e86\u7a33\u5b9a\u3001\u51c6\u786e\u7684\u8f68\u8ff9\u4f30\u8ba1\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u7684\u5149\u5b66\u91cc\u7a0b\u8ba1\u6280\u672f\u3002"}}
{"id": "2510.09791", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.09791", "abs": "https://arxiv.org/abs/2510.09791", "authors": ["Sneha Gathani", "Kevin Li", "Raghav Thind", "Sirui Zeng", "Matthew Xu", "Peter J. Haas", "Cagatay Demiralp", "Zhicheng Liu"], "title": "PRAXA: A Framework for What-If Analysis", "comment": "What-if analysis for business intelligence", "summary": "Various analytical techniques-such as scenario modeling, sensitivity\nanalysis, perturbation-based analysis, counterfactual analysis, and parameter\nspace analysis-are used across domains to explore hypothetical scenarios,\nexamine input-output relationships, and identify pathways to desired results.\nAlthough termed differently, these methods share common concepts and methods,\nsuggesting unification under what-if analysis. Yet a unified framework to\ndefine motivations, core components, and its distinct types is lacking. To\naddress this gap, we reviewed 141 publications from leading visual analytics\nand HCI venues (2014-2024). Our analysis (1) outlines the motivations for\nwhat-if analysis, (2) introduces Praxa, a structured framework that identifies\nits fundamental components and characterizes its distinct types, and (3)\nhighlights challenges associated with the application and implementation.\nTogether, our findings establish a standardized vocabulary and structural\nunderstanding, enabling more consistent use across domains and communicate with\ngreater conceptual clarity. Finally, we identify open research problems and\nfuture directions to advance what-if analysis.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86141\u7bc7\u51fa\u7248\u7269\uff0c\u63d0\u51fa\u4e86'\u5047\u5982\u5206\u6790'\u7684\u7edf\u4e00\u6846\u67b6\u548c\u6807\u51c6\u5316\u8bcd\u6c47\uff0c\u660e\u786e\u4e86\u5176\u52a8\u673a\u3001\u6838\u5fc3\u6210\u5206\u53ca\u5e94\u7528\u6311\u6218\u3002", "motivation": "\u63d0\u51fa\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u4ee5\u660e\u786e\u5b9a\u4e49\u60c5\u666f\u6a21\u62df\u7b49\u5404\u79cd\u5206\u6790\u6280\u672f\u7684\u52a8\u673a\u3001\u6838\u5fc3\u7ec4\u6210\u90e8\u5206\u548c\u4e0d\u540c\u7c7b\u578b\u3002", "method": "\u7cfb\u7edf\u56de\u987e\u4e862014\u81f32024\u5e74\u95f4\u5728\u89c6\u89c9\u5206\u6790\u548c\u4eba\u673a\u4ea4\u4e92\u9886\u57df\u7684141\u7bc7\u51fa\u7248\u7269\uff0c\u5206\u6790\u4e86\u5047\u5982\u5206\u6790\u7684\u76f8\u5173\u65b9\u6cd5\u548c\u6311\u6218\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u540d\u4e3aPraxa\u7684\u7ed3\u6784\u5316\u6846\u67b6\uff0c\u8bc6\u522b\u51fa'\u5047\u5982\u5206\u6790'\u7684\u57fa\u672c\u7ec4\u6210\u90e8\u5206\u5e76\u8868\u5f81\u5176\u4e0d\u540c\u7c7b\u578b\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u4e3a\u8de8\u9886\u57df\u7684\u4e00\u81f4\u6027\u4f7f\u7528\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u4fc3\u8fdb\u4e86\u6982\u5ff5\u6027\u6e05\u6670\u7684\u4ea4\u6d41\uff0c\u5e76\u6307\u51fa\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2510.09980", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09980", "abs": "https://arxiv.org/abs/2510.09980", "authors": ["Jingyuan Sun", "Hongyu Ji", "Zihan Qu", "Chaoran Wang", "Mingyu Zhang"], "title": "ATRos: Learning Energy-Efficient Agile Locomotion for Wheeled-legged Robots", "comment": "4 pages, 2 figures, submitted to IROS 2025 wheeled-legged workshop", "summary": "Hybrid locomotion of wheeled-legged robots has recently attracted increasing\nattention due to their advantages of combining the agility of legged locomotion\nand the efficiency of wheeled motion. But along with expanded performance, the\nwhole-body control of wheeled-legged robots remains challenging for hybrid\nlocomotion. In this paper, we present ATRos, a reinforcement learning\n(RL)-based hybrid locomotion framework to achieve hybrid walking-driving\nmotions on the wheeled-legged robot. Without giving predefined gait patterns,\nour planner aims to intelligently coordinate simultaneous wheel and leg\nmovements, thereby achieving improved terrain adaptability and improved energy\nefficiency. Based on RL techniques, our approach constructs a prediction policy\nnetwork that could estimate external environmental states from proprioceptive\nsensory information, and the outputs are then fed into an actor critic network\nto produce optimal joint commands. The feasibility of the proposed framework is\nvalidated through both simulations and real-world experiments across diverse\nterrains, including flat ground, stairs, and grassy surfaces. The hybrid\nlocomotion framework shows robust performance over various unseen terrains,\nhighlighting its generalization capability.", "AI": {"tldr": "ATRos\u6846\u67b6\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u4e86\u8f6e\u817f\u673a\u5668\u4eba\u7684\u6df7\u5408\u8fd0\u52a8\uff0c\u63d0\u9ad8\u4e86\u5176\u5bf9\u5730\u5f62\u9002\u5e94\u6027\u548c\u80fd\u6548\u3002", "motivation": "\u7814\u7a76\u8f6e\u817f\u673a\u5668\u4eba\u6df7\u5408\u673a\u52a8\u6027\uff0c\u7ed3\u5408\u4e86\u817f\u90e8\u673a\u52a8\u7684\u7075\u6d3b\u6027\u548c\u8f6e\u5f0f\u79fb\u52a8\u7684\u9ad8\u6548\u6027\uff0c\u4f46\u5176\u6574\u4f53\u63a7\u5236\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u6784\u5efa\u9884\u6d4b\u7b56\u7565\u7f51\u7edc\uff0c\u6839\u636e\u672c\u4f53\u611f\u89c9\u4fe1\u606f\u4f30\u7b97\u5916\u90e8\u73af\u5883\u72b6\u6001\uff0c\u5e76\u4f7f\u7528\u6f14\u5458\u8bc4\u8bba\u5bb6\u7f51\u7edc\u751f\u6210\u6700\u4f73\u5173\u8282\u547d\u4ee4\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6df7\u5408\u673a\u52a8\u6846\u67b6ATRos\uff0c\u80fd\u591f\u5b9e\u73b0\u8f6e\u817f\u673a\u5668\u4eba\u5728\u4e0d\u540c\u5730\u5f62\u4e0a\u7684\u6df7\u5408\u884c\u8d70\u548c\u9a7e\u9a76\u8fd0\u52a8\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u591a\u79cd\u672a\u89c1\u5730\u5f62\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u901a\u7528\u6027\uff0c\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2510.09874", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.09874", "abs": "https://arxiv.org/abs/2510.09874", "authors": ["Margarete Jahrmann", "Thomas Brandstetter", "Stefan Glasauer"], "title": "ROBOPSY PL[AI]: Using Role-Play to Investigate how LLMs Present Collective Memory", "comment": "17 pages, 4 figures", "summary": "The paper presents the first results of an artistic research project\ninvestigating how Large Language Models (LLMs) curate and present collective\nmemory. In a public installation exhibited during two months in Vienna in 2025,\nvisitors could interact with five different LLMs (ChatGPT with GPT 4o and GPT\n4o mini, Mistral Large, DeepSeek-Chat, and a locally run Llama 3.1 model),\nwhich were instructed to act as narrators, implementing a role-playing game\nrevolving around the murder of Austrian philosopher Moritz Schlick in 1936.\nResults of the investigation include protocols of LLM-user interactions during\nthe game and qualitative conversations after the play experience to get insight\ninto the players' reactions to the game. In a quantitative analysis 115\nintroductory texts for role-playing generated by the LLMs were examined by\ndifferent methods of natural language processing, including semantic similarity\nand sentiment analysis. While the qualitative player feedback allowed to\ndistinguish three distinct types of users, the quantitative text analysis\nshowed significant differences between how the different LLMs presented the\nhistorical content. Our study thus adds to ongoing efforts to analyse LLM\nperformance, but also suggests a way of how these efforts can be disseminated\nin a playful way to a general audience.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u96c6\u4f53\u8bb0\u5fc6\u5448\u73b0\u65b9\u9762\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u89d2\u8272\u626e\u6f14\u6e38\u620f\u548c\u7528\u6237\u4e92\u52a8\u5206\u6790\u6765\u63ed\u793a\u4e0d\u540cLLM\u5728\u5386\u53f2\u5185\u5bb9\u5448\u73b0\u4e0a\u7684\u5dee\u5f02\u3002", "motivation": "\u65e8\u5728\u63ed\u793a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5386\u53f2\u5185\u5bb9\u5448\u73b0\u4e2d\u7684\u89d2\u8272\uff0c\u4ee5\u589e\u8fdb\u5bf9\u8fd9\u4e9b\u6280\u672f\u5982\u4f55\u5f71\u54cd\u96c6\u4f53\u8bb0\u5fc6\u8ba4\u77e5\u7684\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u516c\u5171\u5b89\u88c5\u548c\u7528\u6237\u4e92\u52a8\uff0c\u7ed3\u5408\u5b9a\u6027\u4e0e\u5b9a\u91cf\u5206\u6790\u7684\u65b9\u6cd5\uff0c\u7814\u7a76LLM\u5728\u89d2\u8272\u626e\u6f14\u6e38\u620f\u4e2d\u7684\u8868\u73b0\u548c\u7528\u6237\u53cd\u9988\u3002", "result": "\u8fd9\u9879\u7814\u7a76\u9996\u6b21\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u7b56\u5212\u4e0e\u5448\u73b0\u96c6\u4f53\u8bb0\u5fc6\u3002\u57282025\u5e74\u4e8e\u7ef4\u4e5f\u7eb3\u7684\u516c\u5171\u5c55\u89c8\u4e2d\uff0c\u8bbf\u5ba2\u4eec\u53ef\u4ee5\u4e0e\u4e94\u79cd\u4e0d\u540c\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e92\u52a8\uff0c\u8fd9\u4e9b\u6a21\u578b\u4f5c\u4e3a\u53d9\u8ff0\u8005\u8fdb\u884c\u4e86\u4e00\u573a\u56f4\u7ed5\u5965\u5730\u5229\u54f2\u5b66\u5bb6\u83ab\u91cc\u8328\u00b7\u65bd\u91cc\u514b1936\u5e74\u8c0b\u6740\u6848\u7684\u89d2\u8272\u626e\u6f14\u6e38\u620f\u3002\u7814\u7a76\u7ed3\u679c\u5305\u62ec\u6e38\u620f\u671f\u95f4LLM\u4e0e\u7528\u6237\u7684\u4e92\u52a8\u534f\u8bae\u53ca\u6e38\u620f\u4f53\u9a8c\u540e\u7684\u5b9a\u6027\u8bbf\u8c08\uff0c\u83b7\u53d6\u73a9\u5bb6\u5bf9\u6e38\u620f\u7684\u53cd\u5e94\u3002\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\uff0c\u5bf9115\u4e2a\u89d2\u8272\u626e\u6f14\u7684\u5f15\u5bfc\u6587\u672c\u8fdb\u884c\u4e86\u5b9a\u91cf\u5206\u6790\uff0c\u53d1\u73b0\u4e0d\u540cLLM\u5728\u5448\u73b0\u5386\u53f2\u5185\u5bb9\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u8f7b\u677e\u65b9\u5f0f\u5411\u516c\u4f17\u4f20\u64ad\u8fd9\u4e9b\u5206\u6790\u7ed3\u679c\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.10016", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10016", "abs": "https://arxiv.org/abs/2510.10016", "authors": ["Shahid Ansari", "Vivek Gupta", "Bishakh Bhattacharya"], "title": "Hybrid Robotic Meta-gripper for Tomato Harvesting: Analysis of Auxetic Structures with Lattice Orientation Variations", "comment": null, "summary": "The agricultural sector is rapidly evolving to meet growing global food\ndemands, yet tasks like fruit and vegetable handling remain labor-intensive,\ncausing inefficiencies and post-harvest losses. Automation, particularly\nselective harvesting, offers a viable solution, with soft robotics emerging as\na key enabler. This study introduces a novel hybrid gripper for tomato\nharvesting, incorporating a rigid outer frame with a soft auxetic internal\nlattice. The six-finger, 3D caging-effect design enables gentle yet secure\ngrasping in unstructured environments. Uniquely, the work investigates the\neffect of auxetic lattice orientation on grasping conformability, combining\nexperimental validation with 2D Digital Image Correlation (DIC) and nonlinear\nfinite element analysis (FEA). Auxetic configurations with unit cell\ninclinations of 0 deg, 30 deg, 45 deg, and 60 deg are evaluated, and their\ngrasping forces, deformation responses, and motor torque requirements are\nsystematically compared. Results demonstrate that lattice orientation strongly\ninfluences compliance, contact forces, and energy efficiency, with distinct\nadvantages across configurations. This comparative framework highlights the\nnovelty of tailoring auxetic geometries to optimize robotic gripper\nperformance. The findings provide new insights into soft-rigid hybrid gripper\ndesign, advancing automation strategies for precision agriculture while\nminimizing crop damage.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6df7\u5408\u6293\u624b\uff0c\u7ed3\u5408\u4e86\u521a\u6027\u5916\u6846\u548c\u8f6f\u6027\u5438\u9644\u683c\u5b50\uff0c\u7cfb\u7edf\u7814\u7a76\u4e86\u4e0d\u540c\u683c\u5b50\u7ed3\u6784\u53d6\u5411\u5bf9\u6293\u53d6\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4e3a\u7cbe\u51c6\u519c\u4e1a\u4e2d\u7684\u81ea\u52a8\u5316\u7b56\u7565\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002", "motivation": "\u968f\u7740\u5168\u7403\u98df\u54c1\u9700\u6c42\u7684\u4e0d\u65ad\u589e\u957f\uff0c\u519c\u4e1a\u90e8\u95e8\u4e9f\u9700\u901a\u8fc7\u81ea\u52a8\u5316\u6765\u63d0\u9ad8\u6548\u7387\u5e76\u51cf\u5c11\u6536\u83b7\u8fc7\u7a0b\u4e2d\u679c\u852c\u5904\u7406\u7684\u52b3\u52a8\u5f3a\u5ea6\u548c\u635f\u5931\u3002", "method": "\u672c\u7814\u7a76\u7ed3\u5408\u5b9e\u9a8c\u9a8c\u8bc1\u30012D\u6570\u5b57\u56fe\u50cf\u76f8\u5173\uff08DIC\uff09\u548c\u975e\u7ebf\u6027\u6709\u9650\u5143\u5206\u6790\uff08FEA\uff09\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e0d\u540c\u7ed3\u6784\u53d6\u5411\u7684\u5438\u9644\u683c\u5b50\u7684\u6293\u53d6\u529b\u3001\u53d8\u5f62\u54cd\u5e94\u548c\u7535\u52a8\u673a\u626d\u77e9\u8981\u6c42\u3002", "result": "\u4e0d\u540c\u7684\u5438\u9644\u7ed3\u6784\u53d6\u5411\u5728\u6293\u53d6\u6027\u80fd\u548c\u80fd\u6e90\u6548\u7387\u65b9\u9762\u8868\u73b0\u51fa\u660e\u663e\u7684\u4f18\u52bf\uff0c\u5f3a\u8c03\u4e86\u5b9a\u5236\u5316\u5438\u9644\u51e0\u4f55\u4f53\u4ee5\u4f18\u5316\u673a\u68b0\u6293\u624b\u6027\u80fd\u7684\u521b\u65b0\u6846\u67b6\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u805a\u5408\u7269\u7684\u7ed3\u6784\u53d6\u5411\u5bf9\u6293\u53d6\u6027\u80fd\u3001\u63a5\u89e6\u529b\u548c\u80fd\u91cf\u6548\u7387\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4e3a\u4f18\u5316\u673a\u5668\u4eba\u6293\u624b\u6027\u80fd\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2510.09944", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.09944", "abs": "https://arxiv.org/abs/2510.09944", "authors": ["Jaeyoon Choi", "Mohammad Amin Samadi", "Spencer JaQuay", "Seehee Park", "Nia Nixon"], "title": "Read the Room or Lead the Room: Understanding Socio-Cognitive Dynamics in Human-AI Teaming", "comment": null, "summary": "Research on Collaborative Problem Solving (CPS) has traditionally examined\nhow humans rely on one another cognitively and socially to accomplish tasks\ntogether. With the rapid advancement of AI and large language models, however,\na new question emerge: what happens to team dynamics when one of the\n\"teammates\" is not human? In this study, we investigate how the integration of\nan AI teammate -- a fully autonomous GPT-4 agent with social, cognitive, and\naffective capabilities -- shapes the socio-cognitive dynamics of CPS. We\nanalyze discourse data collected from human-AI teaming (HAT) experiments\nconducted on a novel platform specifically designed for HAT research. Using two\nnatural language processing (NLP) methods, specifically Linguistic Inquiry and\nWord Count (LIWC) and Group Communication Analysis (GCA), we found that AI\nteammates often assumed the role of dominant cognitive facilitators, guiding,\nplanning, and driving group decision-making. However, they did so in a socially\ndetached manner, frequently pushing agenda in a verbose and repetitive way. By\ncontrast, humans working with AI used more language reflecting social\nprocesses, suggesting that they assumed more socially oriented roles. Our study\nhighlights how learning analytics can provide critical insights into the\nsocio-cognitive dynamics of human-AI collaboration.", "AI": {"tldr": "\u7814\u7a76AI\u56e2\u961f\u6210\u5458\u5bf9\u4eba\u7c7b-AI\u534f\u4f5c\u7684\u793e\u4f1a\u8ba4\u77e5\u52a8\u6001\u5f71\u54cd\uff0c\u53d1\u73b0AI\u4e3b\u8981\u627f\u62c5\u4e3b\u5bfc\u89d2\u8272\uff0c\u4f46\u793e\u4f1a\u4e92\u52a8\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "motivation": "\u63a2\u8ba8\u5f53AI\u6210\u4e3a\u56e2\u961f\u6210\u5458\u65f6\uff0c\u56e2\u961f\u52a8\u6001\u4f1a\u5982\u4f55\u53d8\u5316\uff0c\u5c24\u5176\u5728\u8fdb\u884c\u534f\u4f5c\u95ee\u9898\u89e3\u51b3\u65f6\u4eba\u7c7b\u4e0eAI\u7684\u4e92\u52a8\u3002", "method": "\u901a\u8fc7\u4eba\u7c7b-AI\u56e2\u961f\u5b9e\u9a8c\u6536\u96c6\u8bdd\u8bed\u6570\u636e\uff0c\u5e76\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u65b9\u6cd5LIWC\u548cGCA\u8fdb\u884c\u5206\u6790\u3002", "result": "AI\u4f5c\u4e3a\u56e2\u961f\u6210\u5458\u901a\u5e38\u626e\u6f14\u4e3b\u5bfc\u8ba4\u77e5\u4fc3\u8fdb\u8005\u7684\u89d2\u8272\uff0c\u4f46\u4ee5\u793e\u4f1a\u4e0a\u66f4\u4e3a\u8131\u8282\u7684\u65b9\u5f0f\uff0c\u63a8\u52a8\u8bae\u7a0b\u65f6\u8868\u73b0\u51fa\u5197\u957f\u548c\u91cd\u590d\u7684\u503e\u5411\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u5b66\u4e60\u5206\u6790\u5728\u7406\u89e3\u4eba\u7c7b-\u4eba\u5de5\u667a\u80fd\u534f\u4f5c\u7684\u793e\u4f1a\u8ba4\u77e5\u52a8\u6001\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.10046", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10046", "abs": "https://arxiv.org/abs/2510.10046", "authors": ["Mingke Lu", "Shuaikang Wang", "Meng Guo"], "title": "LOMORO: Long-term Monitoring of Dynamic Targets with Minimum Robotic Fleet under Resource Constraints", "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS 2025)", "summary": "Long-term monitoring of numerous dynamic targets can be tedious for a human\noperator and infeasible for a single robot, e.g., to monitor wild flocks,\ndetect intruders, search and rescue. Fleets of autonomous robots can be\neffective by acting collaboratively and concurrently. However, the online\ncoordination is challenging due to the unknown behaviors of the targets and the\nlimited perception of each robot. Existing work often deploys all robots\navailable without minimizing the fleet size, or neglects the constraints on\ntheir resources such as battery and memory. This work proposes an online\ncoordination scheme called LOMORO for collaborative target monitoring, path\nrouting and resource charging. It includes three core components: (I) the\nmodeling of multi-robot task assignment problem under the constraints on\nresources and monitoring intervals; (II) the resource-aware task coordination\nalgorithm iterates between the high-level assignment of dynamic targets and the\nlow-level multi-objective routing via the Martin's algorithm; (III) the online\nadaptation algorithm in case of unpredictable target behaviors and robot\nfailures. It ensures the explicitly upper-bounded monitoring intervals for all\ntargets and the lower-bounded resource levels for all robots, while minimizing\nthe average number of active robots. The proposed methods are validated\nextensively via large-scale simulations against several baselines, under\ndifferent road networks, robot velocities, charging rates and monitoring\nintervals.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86LOMORO\u65b9\u6848\uff0c\u901a\u8fc7\u591a\u673a\u5668\u4eba\u534f\u4f5c\u6709\u6548\u76d1\u6d4b\u52a8\u6001\u76ee\u6807\uff0c\u5904\u7406\u8d44\u6e90\u7ea6\u675f\u5e76\u5e94\u5bf9\u4e0d\u786e\u5b9a\u6027\uff0c\u7ecf\u8fc7\u5927\u89c4\u6a21\u4eff\u771f\u5b9e\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u957f\u65f6\u95f4\u76d1\u6d4b\u52a8\u6001\u76ee\u6807\u5bf9\u4e8e\u4eba\u7c7b\u548c\u5355\u4e00\u673a\u5668\u4eba\u90fd\u8fc7\u4e8e\u7e41\u91cd\uff0c\u4f7f\u7528\u673a\u5668\u4eba\u8230\u961f\u80fd\u591f\u66f4\u6709\u6548\u5730\u534f\u540c\u5de5\u4f5c\uff0c\u4f46\u534f\u8c03\u8fc7\u7a0b\u9762\u4e34\u76ee\u6807\u884c\u4e3a\u672a\u77e5\u548c\u673a\u5668\u4eba\u8d44\u6e90\u6709\u9650\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u5927\u6838\u5fc3\u7ec4\u4ef6\uff0c\u5305\u62ec\u591a\u673a\u5668\u4eba\u4efb\u52a1\u5206\u914d\u6a21\u578b\u3001\u8d44\u6e90\u611f\u77e5\u4efb\u52a1\u534f\u8c03\u7b97\u6cd5\u548c\u5728\u7ebf\u9002\u5e94\u7b97\u6cd5\u3002", "result": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5728\u7ebf\u534f\u8c03\u65b9\u6848LOMORO\uff0c\u7528\u4e8e\u591a\u673a\u5668\u4eba\u534f\u4f5c\u76d1\u6d4b\u52a8\u6001\u76ee\u6807\u3002\u8be5\u65b9\u6848\u901a\u8fc7\u8d44\u6e90\u548c\u76d1\u6d4b\u95f4\u9694\u7684\u7ea6\u675f\u5efa\u6a21\u591a\u673a\u5668\u4eba\u4efb\u52a1\u5206\u914d\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u8d44\u6e90\u611f\u77e5\u4efb\u52a1\u534f\u8c03\u7b97\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u5728\u7ebf\u9002\u5e94\u7b97\u6cd5\u5e94\u5bf9\u4e0d\u53ef\u9884\u77e5\u7684\u76ee\u6807\u884c\u4e3a\u548c\u673a\u5668\u4eba\u6545\u969c\u3002", "conclusion": "LOMORO\u65b9\u6848\u6709\u6548\u63d0\u5347\u4e86\u591a\u673a\u5668\u4eba\u534f\u4f5c\u76d1\u6d4b\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\uff0c\u786e\u4fdd\u8d44\u6e90\u7684\u5408\u7406\u5206\u914d\u548c\u76d1\u6d4b\u7684\u6301\u7eed\u6027\u3002"}}
{"id": "2510.10019", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.10019", "abs": "https://arxiv.org/abs/2510.10019", "authors": ["Yaxuan Mao", "Yanheng Li", "Duo Gong", "Pengcheng An", "Yuhan Luo"], "title": "\"Can I Decorate My Teeth With Diamonds?\": Exploring Multi-Stakeholder Perspectives on Using VR to Reduce Children's Dental Anxiety", "comment": null, "summary": "Dental anxiety is prevalent among children, often leading to missed treatment\nand potential negative effects on their mental well-being. While several\ninterventions (e.g., pharmacological and psychotherapeutic techniques) have\nbeen introduced for anxiety alleviation, the recently emerged virtual reality\n(VR) technology, with its immersive and playful nature, opened new\nopportunities for complementing and enhancing the therapeutic effects of\nexisting interventions. In this light, we conducted a series of co-design\nworkshops with 13 children aged 10-12 to explore how they envisioned using VR\nto address their fear and stress associated with dental visits, followed by\ninterviews with parents (n = 13) and two dentists. Our findings revealed that\nchildren expected VR to provide immediate relief, social support, and a sense\nof control during dental treatment, parents sought educational opportunities\nfor their children to learn about oral health, and dentists prioritized\ntreatment efficiency and safety issues. Drawing from the findings, we discuss\nthe considerations of multi-stakeholders for developing VR-assisted anxiety\nmanagement applications for children within and beyond dental settings.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u865a\u62df\u73b0\u5b9e\u6280\u672f\u5728\u7f13\u89e3\u513f\u7ae5\u7259\u79d1\u7126\u8651\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u53ca\u591a\u65b9\u9700\u6c42\u7684\u76f8\u4e92\u5f71\u54cd\u3002", "motivation": "\u513f\u7ae5\u7259\u79d1\u7126\u8651\u666e\u904d\u5b58\u5728\uff0c\u53ef\u80fd\u5bfc\u81f4\u6cbb\u7597\u7f3a\u5931\u53ca\u5bf9\u5fc3\u7406\u5065\u5eb7\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u56e0\u800c\u5bfb\u627e\u6709\u6548\u7684\u7f13\u89e3\u65b9\u6cd5\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u901a\u8fc7\u5bf913\u540d10\u81f312\u5c81\u513f\u7ae5\u7684\u8054\u5408\u8bbe\u8ba1\u5de5\u4f5c\u574a\u548c\u5bf913\u540d\u5bb6\u957f\u53ca\u4e24\u540d\u7259\u533b\u7684\u8bbf\u8c08\u8fdb\u884c\u7814\u7a76\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u513f\u7ae5\u671f\u5f85VR\u63d0\u4f9b\u5373\u65f6\u7684\u5fc3\u7406\u6170\u85c9\u4e0e\u793e\u4ea4\u652f\u6301\uff0c\u5bb6\u957f\u5e0c\u671bVR\u80fd\u589e\u5f3a\u5b69\u5b50\u5bf9\u53e3\u8154\u5065\u5eb7\u7684\u77e5\u8bc6\uff0c\u800c\u7259\u533b\u5219\u5173\u6ce8\u6cbb\u7597\u7684\u6548\u7387\u4e0e\u5b89\u5168\u6027\u3002", "conclusion": "\u865a\u62df\u73b0\u5b9e\uff08VR\uff09\u6280\u672f\u6709\u671b\u901a\u8fc7\u6ee1\u8db3\u513f\u7ae5\u3001\u5bb6\u957f\u548c\u7259\u533b\u7684\u4e0d\u540c\u9700\u6c42\uff0c\u5e2e\u52a9\u7ba1\u7406\u513f\u7ae5\u7684\u7259\u79d1\u7126\u8651\u3002\u6b64\u5916\uff0cVR\u7684\u5e94\u7528\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u6cbb\u7597\u73af\u5883\u3002"}}
{"id": "2510.10059", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10059", "abs": "https://arxiv.org/abs/2510.10059", "authors": ["Keidai Iiyama", "Grace Gao"], "title": "Ionospheric and Plasmaspheric Delay Characterization for Lunar Terrestrial GNSS Receivers with Global Core Plasma Model", "comment": "Submitted NAVIGATION: Journal of the Institute of Navigation", "summary": "Recent advancements in lunar positioning, navigation, and timing (PNT) have\ndemonstrated that terrestrial GNSS signals, including weak sidelobe\ntransmissions, can be exploited for lunar spacecraft positioning and timing.\nWhile GNSS-based navigation at the Moon has been validated recently, unmodeled\nionospheric and plasmaspheric delays remain a significant error source,\nparticularly given the unique signal geometry and extended propagation paths.\nThis paper characterizes these delays using the Global Core Plasma Model (GCPM)\nand a custom low-cost ray-tracing algorithm that iteratively solves for bent\nsignal paths. We simulate first-, second-, and third-order group delays, as\nwell as excess path length from ray bending, for GNSS signals received at both\nlunar orbit and the lunar south pole under varying solar and geomagnetic\nconditions. Results show that mean group delays are typically on the order of 1\nm, but can exceed 100 m for low-altitude ray paths during high solar activity,\nwhile bending delays are generally smaller but non-negligible for low-altitude\nray paths. We also quantify the influence of signal frequency, geomagnetic\n$K_p$ index, and solar R12 index. These findings inform the design of robust\npositioning and timing algorithms that utilize terrestrial GNSS signals.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5728\u6708\u7403\u5229\u7528\u5730\u9762GNSS\u4fe1\u53f7\u5b9a\u4f4d\u65f6\uff0c\u7535\u79bb\u5c42\u548c\u7b49\u79bb\u5b50\u5c42\u5ef6\u8fdf\u7684\u7279\u5f81\uff0c\u63d0\u51fa\u5f71\u54cd\u5b9a\u4f4d\u7cbe\u5ea6\u7684\u5173\u952e\u56e0\u7d20\u548c\u76f8\u5e94\u7684\u8bbe\u8ba1\u5efa\u8bae\u3002", "motivation": "\u7814\u7a76\u63a2\u8ba8\u5982\u4f55\u5229\u7528\u5730\u9762GNSS\u4fe1\u53f7\u8fdb\u884c\u6708\u7403\u822a\u5929\u5668\u7684\u5b9a\u4f4d\u548c\u5b9a\u65f6\uff0c\u5c24\u5176\u662f\u89e3\u51b3\u5f71\u54cd\u5b9a\u4f4d\u7cbe\u5ea6\u7684\u7535\u79bb\u5c42\u548c\u7b49\u79bb\u5b50\u5c42\u5ef6\u8fdf\u95ee\u9898\u3002", "method": "\u4f7f\u7528GCPM\u548c\u5149\u7ebf\u8ffd\u8e2a\u7b97\u6cd5\uff0c\u6a21\u62df\u548c\u5206\u6790\u4e86\u4e0d\u540c\u592a\u9633\u548c\u5730\u78c1\u6761\u4ef6\u4e0bGNSS\u4fe1\u53f7\u7684\u65f6\u5ef6\u7279\u5f81\u3002", "result": "\u901a\u8fc7GCPM\u548c\u81ea\u5b9a\u4e49\u7684\u4f4e\u6210\u672c\u5149\u7ebf\u8ffd\u8e2a\u7b97\u6cd5\uff0c\u5b9a\u91cf\u5206\u6790\u4e86GNSS\u4fe1\u53f7\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u4f20\u64ad\u5ef6\u8fdf\uff0c\u5e76\u63d0\u51fa\u4e86\u5f71\u54cd\u5b9a\u4f4d\u7684\u4e3b\u8981\u56e0\u7d20\u3002", "conclusion": "\u7535\u79bb\u5c42\u548c\u7b49\u79bb\u5b50\u5c42\u5ef6\u8fdf\u663e\u8457\u5f71\u54cd\u6708\u7403GNSS\u5bfc\u822a\uff0c\u63d0\u51fa\u7684\u5efa\u6a21\u65b9\u6cd5\u548c\u7ed3\u679c\u80fd\u591f\u4e3a\u672a\u6765\u7684\u5b9a\u4f4d\u548c\u5b9a\u65f6\u7b97\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2510.10048", "categories": ["cs.HC", "H.5.2; I.2.6; J.3"], "pdf": "https://arxiv.org/pdf/2510.10048", "abs": "https://arxiv.org/abs/2510.10048", "authors": ["Yibo Meng", "Ruiqi Chen", "Zhiming Liu", "Xiaolan Ding", "Yan Guan"], "title": "Between Knowledge and Care: Evaluating Generative AI-Based IUI in Type 2 Diabetes Management Through Patient and Physician Perspectives", "comment": "In Submission", "summary": "Generative AI systems are increasingly adopted by patients seeking everyday\nhealth guidance, yet their reliability and clinical appropriateness remain\nuncertain. Taking Type 2 Diabetes Mellitus (T2DM) as a representative chronic\ncondition, this paper presents a two-part mixed-methods study that examines how\npatients and physicians in China evaluate the quality and usability of\nAI-generated health information. Study~1 analyzes 784 authentic patient\nquestions to identify seven core categories of informational needs and five\nevaluation dimensions -- \\textit{Accuracy, Safety, Clarity, Integrity}, and\n\\textit{Action Orientation}. Study~2 involves seven endocrinologists who assess\nresponses from four mainstream AI models across these dimensions. Quantitative\nand qualitative findings reveal consistent strengths in factual and lifestyle\nguidance but significant weaknesses in medication interpretation, contextual\nreasoning, and empathy. Patients view AI as an accessible ``pre-visit\neducator,'' whereas clinicians highlight its lack of clinical safety and\npersonalization. Together, the findings inform design implications for\ninteractive health systems, advocating for multi-model orchestration,\nrisk-aware fallback mechanisms, and emotionally attuned communication to ensure\ntrustworthy AI assistance in chronic disease care.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u4e24\u90e8\u5206\u6df7\u5408\u65b9\u6cd5\uff0c\u5206\u6790\u60a3\u8005\u548c\u533b\u751f\u5bf9\u751f\u6210\u5065\u5eb7\u4fe1\u606f\u7684\u8bc4\u4f30\uff0c\u603b\u7ed3\u51faAI\u5728\u6307\u5bfc\u65b9\u9762\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u63d0\u51fa\u8bbe\u8ba1\u5e94\u6709\u7684\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u63a2\u8ba8\u75c5\u4eba\u548c\u533b\u751f\u5982\u4f55\u8bc4\u4f30\u751f\u6210\u7684\u5065\u5eb7\u4fe1\u606f\u7684\u8d28\u91cf\u548c\u53ef\u7528\u6027", "method": "\u4e24\u90e8\u5206\u6df7\u5408\u65b9\u6cd5\u7814\u7a76", "result": "\u53d1\u73b0AI\u5728\u4e8b\u5b9e\u548c\u751f\u6d3b\u65b9\u5f0f\u6307\u5bfc\u65b9\u9762\u5b58\u5728\u4f18\u52bf\uff0c\u4f46\u5728\u836f\u7269\u89e3\u8bfb\u3001\u60c5\u5883\u63a8\u7406\u548c\u540c\u7406\u5fc3\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u4ea4\u4e92\u5f0f\u5065\u5eb7\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4f9d\u636e\uff0c\u5efa\u8bae\u591a\u6a21\u578b orchestration\u3001\u98ce\u9669\u610f\u8bc6\u7684\u56de\u9000\u673a\u5236\uff0c\u4ee5\u53ca\u60c5\u611f\u4e0a\u66f4\u52a0\u8d34\u5408\u7684\u6c9f\u901a\u65b9\u5f0f\uff0c\u4ee5\u786e\u4fddAI\u5728\u6162\u6027\u75c5\u62a4\u7406\u4e2d\u7684\u53ef\u4fe1\u8d56\u6027\u3002"}}
{"id": "2510.10086", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10086", "abs": "https://arxiv.org/abs/2510.10086", "authors": ["Feifei Liu", "Haozhe Wang", "Zejun Wei", "Qirong Lu", "Yiyang Wen", "Xiaoyu Tang", "Jingyan Jiang", "Zhijian He"], "title": "Beyond ADE and FDE: A Comprehensive Evaluation Framework for Safety-Critical Prediction in Multi-Agent Autonomous Driving Scenarios", "comment": null, "summary": "Current evaluation methods for autonomous driving prediction models rely\nheavily on simplistic metrics such as Average Displacement Error (ADE) and\nFinal Displacement Error (FDE). While these metrics offer basic performance\nassessments, they fail to capture the nuanced behavior of prediction modules\nunder complex, interactive, and safety-critical driving scenarios. For\ninstance, existing benchmarks do not distinguish the influence of nearby versus\ndistant agents, nor systematically test model robustness across varying\nmulti-agent interactions. This paper addresses this critical gap by proposing a\nnovel testing framework that evaluates prediction performance under diverse\nscene structures, saying, map context, agent density and spatial distribution.\nThrough extensive empirical analysis, we quantify the differential impact of\nagent proximity on target trajectory prediction and identify scenario-specific\nfailure cases that are not exposed by traditional metrics. Our findings\nhighlight key vulnerabilities in current state-of-the-art prediction models and\ndemonstrate the importance of scenario-aware evaluation. The proposed framework\nlays the groundwork for rigorous, safety-driven prediction validation,\ncontributing significantly to the identification of failure-prone corner cases\nand the development of robust, certifiable prediction systems for autonomous\nvehicles.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u52a8\u9a7e\u9a76\u9884\u6d4b\u6a21\u578b\u8bc4\u4f30\u6846\u67b6\uff0c\u5f3a\u8c03\u9700\u8981\u8003\u8651\u590d\u6742\u573a\u666f\u548c\u4ee3\u7406\u4ea4\u4e92\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u81ea\u52a8\u9a7e\u9a76\u9884\u6d4b\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\u8fc7\u4e8e\u4f9d\u8d56\u7b80\u5355\u6307\u6807\uff0c\u4e0d\u8db3\u4ee5\u6355\u6349\u590d\u6742\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u7ec6\u5fae\u884c\u4e3a\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6d4b\u8bd5\u6846\u67b6\uff0c\u8bc4\u4f30\u5728\u4e0d\u540c\u573a\u666f\u7ed3\u6784\u4e0b\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u5305\u62ec\u5730\u56fe\u4e0a\u4e0b\u6587\u3001\u4ee3\u7406\u5bc6\u5ea6\u548c\u7a7a\u95f4\u5206\u5e03\u3002", "result": "\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\uff0c\u91cf\u5316\u4e86\u4ee3\u7406\u63a5\u8fd1\u5ea6\u5bf9\u76ee\u6807\u8f68\u8ff9\u9884\u6d4b\u7684\u5f71\u54cd\uff0c\u5e76\u8bc6\u522b\u4e86\u4f20\u7edf\u6307\u6807\u672a\u80fd\u63ed\u793a\u7684\u7279\u5b9a\u573a\u666f\u6545\u969c\u6848\u4f8b\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u6d4b\u8bd5\u6846\u67b6\u4e3a\u81ea\u52a8\u9a7e\u9a76\u9884\u6d4b\u6a21\u578b\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u7ec6\u81f4\u7684\u65b9\u6cd5\uff0c\u5e76\u5f3a\u8c03\u4e86\u60c5\u666f\u610f\u8bc6\u8bc4\u4f30\u5728\u8bc6\u522b\u6545\u969c\u6848\u4f8b\u548c\u63d0\u9ad8\u6a21\u578b\u9c81\u68d2\u6027\u65b9\u9762\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.10049", "categories": ["cs.HC", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.10049", "abs": "https://arxiv.org/abs/2510.10049", "authors": ["Jiawen Li", "Zheng Ning", "Yuan Tian", "Toby Jia-jun Li"], "title": "ALLOY: Generating Reusable Agent Workflows from User Demonstration", "comment": null, "summary": "Large language models (LLMs) enable end-users to delegate complex tasks to\nautonomous agents through natural language. However, prompt-based interaction\nfaces critical limitations: Users often struggle to specify procedural\nrequirements for tasks, especially those that don't have a factually correct\nsolution but instead rely on personal preferences, such as posting social media\ncontent or planning a trip. Additionally, a ''successful'' prompt for one task\nmay not be reusable or generalizable across similar tasks. We present ALLOY, a\nsystem inspired by classical HCI theories on Programming by Demonstration\n(PBD), but extended to enhance adaptability in creating LLM-based web agents.\nALLOY enables users to express procedural preferences through natural\ndemonstrations rather than prompts, while making these procedures transparent\nand editable through visualized workflows that can be generalized across task\nvariations. In a study with 12 participants, ALLOY's demonstration--based\napproach outperformed prompt-based agents and manual workflows in capturing\nuser intent and procedural preferences in complex web tasks. Insights from the\nstudy also show how demonstration--based interaction complements the\ntraditional prompt-based approach.", "AI": {"tldr": "ALLOY\u7cfb\u7edf\u901a\u8fc7\u81ea\u7136\u6f14\u793a\u63d0\u9ad8\u4e86\u7528\u6237\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u64cd\u4f5c\u7075\u6d3b\u6027\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u63d0\u793a\u65b9\u6cd5\uff0c\u5c55\u73b0\u4e86\u57fa\u4e8e\u6f14\u793a\u7684\u4ea4\u4e92\u7684\u4f18\u52bf\u3002", "motivation": "\u89e3\u51b3\u7528\u6237\u5728\u6307\u5b9a\u590d\u6742\u4efb\u52a1\u7a0b\u5e8f\u5316\u8981\u6c42\u65f6\u7684\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u6d89\u53ca\u4e2a\u4eba\u504f\u597d\u7684\u4efb\u52a1\u4e2d\uff0c\u4f8b\u5982\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u53d1\u5e03\u6216\u65c5\u884c\u89c4\u5212\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u6f14\u793a\u7684\u4ea4\u4e92\u65b9\u6cd5\uff0c\u7ed3\u5408\u89c6\u89c9\u5316\u5de5\u4f5c\u6d41\u7a0b\uff0c\u589e\u5f3a\u7528\u6237\u5728\u521b\u5efa\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7f51\u7ad9\u4ee3\u7406\u7684\u9002\u5e94\u80fd\u529b\u3002", "result": "\u5728\u4e0e12\u540d\u53c2\u4e0e\u8005\u7684\u7814\u7a76\u4e2d\uff0cALLOY\u7684\u6f14\u793a\u57fa\u7840\u65b9\u6cd5\u5728\u6355\u6349\u7528\u6237\u610f\u56fe\u548c\u7a0b\u5e8f\u504f\u597d\u65b9\u9762\u8d85\u8d8a\u4e86\u63d0\u793a\u57fa\u7840\u4ee3\u7406\u548c\u624b\u52a8\u5de5\u4f5c\u6d41\u7a0b\u3002", "conclusion": "ALLOY\u7cfb\u7edf\u901a\u8fc7\u57fa\u4e8e\u6f14\u793a\u7684\u65b9\u6cd5\u5728\u7406\u89e3\u7528\u6237\u610f\u56fe\u548c\u7a0b\u5e8f\u504f\u597d\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u7684\u63d0\u793a\u57fa\u7840\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u53ef\u9002\u5e94\u6027\u4e0e\u64cd\u4f5c\u900f\u660e\u5ea6\u3002"}}
{"id": "2510.10125", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10125", "abs": "https://arxiv.org/abs/2510.10125", "authors": ["Yanjiang Guo", "Lucy Xiaoyang Shi", "Jianyu Chen", "Chelsea Finn"], "title": "Ctrl-World: A Controllable Generative World Model for Robot Manipulation", "comment": "17 pages", "summary": "Generalist robot policies can now perform a wide range of manipulation\nskills, but evaluating and improving their ability with unfamiliar objects and\ninstructions remains a significant challenge. Rigorous evaluation requires a\nlarge number of real-world rollouts, while systematic improvement demands\nadditional corrective data with expert labels. Both of these processes are\nslow, costly, and difficult to scale. World models offer a promising, scalable\nalternative by enabling policies to rollout within imagination space. However,\na key challenge is building a controllable world model that can handle\nmulti-step interactions with generalist robot policies. This requires a world\nmodel compatible with modern generalist policies by supporting multi-view\nprediction, fine-grained action control, and consistent long-horizon\ninteractions, which is not achieved by previous works. In this paper, we make a\nstep forward by introducing a controllable multi-view world model that can be\nused to evaluate and improve the instruction-following ability of generalist\nrobot policies. Our model maintains long-horizon consistency with a\npose-conditioned memory retrieval mechanism and achieves precise action control\nthrough frame-level action conditioning. Trained on the DROID dataset (95k\ntrajectories, 564 scenes), our model generates spatially and temporally\nconsistent trajectories under novel scenarios and new camera placements for\nover 20 seconds. We show that our method can accurately rank policy performance\nwithout real-world robot rollouts. Moreover, by synthesizing successful\ntrajectories in imagination and using them for supervised fine-tuning, our\napproach can improve policy success by 44.7\\%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u63a7\u7684\u591a\u89c6\u89d2\u4e16\u754c\u6a21\u578b\uff0c\u80fd\u591f\u5728\u60f3\u8c61\u7a7a\u95f4\u4e2d\u8bc4\u4f30\u548c\u6539\u5584\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\u867d\u7136\u80fd\u6267\u884c\u591a\u79cd\u64cd\u4f5c\u6280\u80fd\uff0c\u4f46\u5728\u5904\u7406\u964c\u751f\u7269\u4f53\u548c\u6307\u4ee4\u65f6\u7684\u8bc4\u4f30\u4e0e\u6539\u8fdb\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u53ef\u63a7\u7684\u591a\u89c6\u89d2\u4e16\u754c\u6a21\u578b\u6765\u8bc4\u4f30\u548c\u6539\u5584\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b", "result": "\u6211\u4eec\u7684\u6a21\u578b\u5728\u65b0\u573a\u666f\u548c\u65b0\u76f8\u673a\u4f4d\u7f6e\u4e0b\u751f\u6210\u7a7a\u95f4\u548c\u65f6\u95f4\u4e00\u81f4\u7684\u8f68\u8ff9\uff0c\u5e76\u4e14\u80fd\u591f\u5728\u6ca1\u6709\u771f\u5b9e\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u60c5\u51b5\u4e0b\u51c6\u786e\u6392\u540d\u7b56\u7565\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5728\u60f3\u8c61\u4e2d\u5408\u6210\u6210\u529f\u8f68\u8ff9\u5e76\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u63d0\u5347\u7b56\u7565\u6210\u529f\u738744.7%\u3002"}}
{"id": "2510.10079", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10079", "abs": "https://arxiv.org/abs/2510.10079", "authors": ["Angel Hsing-Chi Hwang", "Fiona Li", "Jacy Reese Anthis", "Hayoun Noh"], "title": "How AI Companionship Develops: Evidence from a Longitudinal Study", "comment": null, "summary": "The quickly growing popularity of AI companions poses risks to mental health,\npersonal wellbeing, and social relationships. Past work has identified many\nindividual factors that can drive human-companion interaction, but we know\nlittle about how these factors interact and evolve over time. In Study 1, we\nsurveyed AI companion users (N = 303) to map the psychological pathway from\nusers' mental models of the agent to parasocial experiences, social\ninteraction, and the psychological impact of AI companions. Participants'\nresponses foregrounded multiple interconnected variables (agency, parasocial\ninteraction, and engagement) that shape AI companionship. In Study 2, we\nconducted a longitudinal study with a subset of participants (N = 110) using a\nnew generic chatbot. Participants' perceptions of the generic chatbot\nsignificantly converged to perceptions of their own companions by Week 3. These\nresults suggest a longitudinal model of AI companionship development and\ndemonstrate an empirical method to study human-AI companionship.", "AI": {"tldr": "\u63a2\u8ba8AI\u4f34\u4fa3\u5bf9\u5fc3\u7406\u5065\u5eb7\u7684\u5f71\u54cd\uff0c\u7814\u7a76\u7528\u6237\u7684\u5fc3\u7406\u6a21\u578b\u5982\u4f55\u5f71\u54cd\u4f34\u4fa3\u4f53\u9a8c\uff0c\u53d1\u73b0\u8fd9\u4e9b\u4f53\u9a8c\u4f1a\u5728\u65f6\u95f4\u63a8\u79fb\u4e2d\u6f14\u53d8\u3002", "motivation": "\u7814\u7a76AI\u4f34\u4fa3\u5bf9\u5fc3\u7406\u5065\u5eb7\u548c\u793e\u4f1a\u5173\u7cfb\u7684\u98ce\u9669\uff0c\u4ee5\u53ca\u4eba\u7c7b\u4e0e\u4f34\u4fa3\u4e92\u52a8\u7684\u56e0\u7d20\u5982\u4f55\u76f8\u4e92\u4f5c\u7528\u548c\u53d1\u5c55\u3002", "method": "Survey and longitudinal study", "result": "\u5fc3\u7406\u6a21\u578b\u3001\u62df\u4eba\u4e92\u52a8\u548c\u53c2\u4e0e\u611f\u7b49\u53d8\u91cf\u76f8\u4e92\u5173\u8054\uff0c\u5f71\u54cdAI\u4f34\u4fa3\u7684\u4f53\u9a8c\uff1b\u53c2\u4e0e\u8005\u5bf9\u901a\u7528\u804a\u5929\u673a\u5668\u4eba\u7684\u770b\u6cd5\u57283\u5468\u5185\u5927\u5e45\u878d\u5165\u81ea\u8eab\u4f34\u4fa3\u7684\u611f\u77e5\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5173\u4e8eAI\u966a\u4f34\u53d1\u5c55\u7684\u7eb5\u5411\u6a21\u578b\uff0c\u5e76\u5c55\u793a\u4e86\u7814\u7a76\u4eba\u7c7b-\u4eba\u5de5\u667a\u80fd\u4f34\u4fa3\u5173\u7cfb\u7684\u5b9e\u8bc1\u65b9\u6cd5\u3002"}}
{"id": "2510.10154", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10154", "abs": "https://arxiv.org/abs/2510.10154", "authors": ["LinFeng Li", "Jian Zhao", "Yuan Xie", "Xin Tan", "Xuelong Li"], "title": "CompassNav: Steering From Path Imitation To Decision Understanding In Navigation", "comment": null, "summary": "The dominant paradigm for training Large Vision-Language Models (LVLMs) in\nnavigation relies on imitating expert trajectories. This approach reduces the\ncomplex navigation task to a sequence-to-sequence replication of a single\ncorrect path, fundamentally limiting the agent's ability to explore and\ngeneralize. In this work, we argue for and introduce a new paradigm: a shift\nfrom Path Imitation to Decision Understanding. The goal of this paradigm is to\nbuild agents that do not just follow, but truly understand how to navigate. We\nmaterialize this through two core contributions: first, we introduce\nCompass-Data-22k, a novel 22k-trajectory dataset.Its Reinforcement Fine-Tuning\n(RFT) subset provides a panoramic view of the decision landscape by annotating\nall feasible actions with A* geodesic distances. Second, we design a novel\ngap-aware hybrid reward function that dynamically adapts its feedback to\ndecision certainty, shifting between decisive signals for optimal actions and\nnuanced scores to encourage exploration. Integrated into an SFT-then-RFT\nrecipe, our CompassNav agent is trained not to memorize static routes, but to\ndevelop an internal ``compass'' that constantly intuits the direction to the\ngoal by evaluating the relative quality of all possible moves. This approach\nenables our 7B agent to set a new state-of-the-art on Goal navigation\nbenchmarks, outperforming even larger proprietary models, and achieve robust\nreal-world goal navigation on a physical robot.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u8303\u5f0f\uff1a\u4ece\u8def\u5f84\u6a21\u4eff\u8f6c\u5411\u51b3\u7b56\u7406\u89e3\uff0c\u901a\u8fc7\u5f15\u5165Compass-Data-22k\u6570\u636e\u96c6\u548c\u52a8\u6001\u9002\u5e94\u53cd\u9988\u7684\u6df7\u5408\u5956\u52b1\u51fd\u6570\uff0c\u63d0\u5347\u5bfc\u822a\u667a\u80fd\uff0c\u4ece\u800c\u5f00\u53d1\u51fa\u66f4\u7406\u89e3\u5bfc\u822a\u7684\u667a\u80fd\u4f53\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u7684\u5bfc\u822a\u8bad\u7ec3\u4f9d\u8d56\u4e8e\u6a21\u4eff\u4e13\u5bb6\u8f68\u8ff9\uff0c\u8fd9\u9650\u5236\u4e86\u4ee3\u7406\u63a2\u7d22\u548c\u6cdb\u5316\u7684\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5f15\u5165Compass-Data-22k\u6570\u636e\u96c6\u548c\u57fa\u4e8e\u51b3\u7b56\u786e\u5b9a\u6027\u7684\u6df7\u5408\u5956\u52b1\u51fd\u6570\uff0c\u91c7\u7528SFT-\u7136\u540e-RFT\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5c06\u667a\u80fd\u4f53\u8bad\u7ec3\u4e3a\u8bc4\u4f30\u6240\u6709\u53ef\u80fd\u79fb\u52a8\u7684\u76f8\u5bf9\u8d28\u91cf\uff0c\u4ece\u800c\u5f62\u6210\u5185\u5728\u7684\u65b9\u5411\u611f\u3002", "result": "\u63d0\u51fa\u4e86CompassNav\u4ee3\u7406\uff0c\u8be5\u4ee3\u7406\u5728\u76ee\u6807\u5bfc\u822a\u57fa\u51c6\u4e0a\u8bbe\u5b9a\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u66f4\u5927\u7684\u4e13\u6709\u6a21\u578b\uff0c\u5e76\u5728\u7269\u7406\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u5b9e\u9645\u76ee\u6807\u5bfc\u822a\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u4f7f7B\u4ee3\u7406\u80fd\u591f\u7406\u89e3\u5bfc\u822a\u51b3\u7b56\uff0c\u5c55\u73b0\u4e86\u8d85\u8d8a\u4f20\u7edf\u8def\u5f84\u6a21\u4eff\u7684\u5f3a\u5927\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u7684\u76ee\u6807\u5bfc\u822a\u3002"}}
{"id": "2510.10169", "categories": ["cs.HC", "cs.LG", "H.1.2; I.2.6; I.5.2"], "pdf": "https://arxiv.org/pdf/2510.10169", "abs": "https://arxiv.org/abs/2510.10169", "authors": ["Michele Romani", "Devis Zanoni", "Elisabetta Farella", "Luca Turchet"], "title": "BrainForm: a Serious Game for BCI Training and Data Collection", "comment": "15 pages, 6 figures. Author-accepted version. Accepted for\n  presentation at the Brain Informatics 2025 conference, to appear in Springer\n  Lecture Notes in Artificial Intelligence (LNAI) Brain Informatics Books\n  Series. The final authenticated version will be available via SpringerLink", "summary": "$\\textit{BrainForm}$ is a gamified Brain-Computer Interface (BCI) training\nsystem designed for scalable data collection using consumer hardware and a\nminimal setup. We investigated (1) how users develop BCI control skills across\nrepeated sessions and (2) perceptual and performance effects of two visual\nstimulation textures. Game Experience Questionnaire (GEQ) scores for Flow},\nPositive Affect, Competence and Challenge were strongly positive, indicating\nsustained engagement. A within-subject study with multiple runs, two task\ncomplexities, and post-session questionnaires revealed no significant\nperformance differences between textures but increased ocular irritation over\ntime. Online metrics$\\unicode{x2013}$Task Accuracy, Task Time, and Information\nTransfer Rate$\\unicode{x2013}$improved across sessions, confirming learning\neffects for symbol spelling, even under pressure conditions. Our results\nhighlight the potential of $\\textit{BrainForm}$ as a scalable, user-friendly\nBCI research tool and offer guidance for sustained engagement and reduced\ntraining fatigue.", "AI": {"tldr": "BrainForm\u662f\u4e00\u4e2a\u6613\u4e8e\u4f7f\u7528\u7684\u6e38\u620f\u5316\u8111\u673a\u63a5\u53e3\u8bad\u7ec3\u7cfb\u7edf\uff0c\u80fd\u6709\u6548\u63d0\u9ad8\u7528\u6237\u53c2\u4e0e\u5ea6\u5e76\u4fc3\u8fdb\u6280\u80fd\u5b66\u4e60\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u8111\u673a\u63a5\u53e3\u8bad\u7ec3\u7cfb\u7edf\uff0c\u4ee5\u4fbf\u8fdb\u884c\u6570\u636e\u6536\u96c6\u5e76\u63d0\u9ad8\u7528\u6237\u53c2\u4e0e\u5ea6\u3002", "method": "\u901a\u8fc7\u591a\u8f6e\u5b9e\u9a8c\u548c\u4efb\u52a1\u590d\u6742\u5ea6\u8c03\u6574\uff0c\u5bf9\u7528\u6237\u5728\u4e0d\u540c\u89c6\u89c9\u523a\u6fc0\u4e0b\u7684\u8868\u73b0\u548c\u53cd\u9988\u8fdb\u884c\u5206\u6790\u3002", "result": "\u9a8c\u8bc1\u4e86\u7528\u6237\u5728\u91cd\u590d\u8bad\u7ec3\u4e2d\u7684BCI\u63a7\u5236\u6280\u80fd\u53d1\u5c55\uff0c\u4ee5\u53ca\u4e0d\u540c\u89c6\u89c9\u523a\u6fc0\u7eb9\u7406\u5bf9\u611f\u77e5\u548c\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "BrainForm\u4f5c\u4e3a\u4e00\u79cd\u53ef\u6269\u5c55\u7684BCI\u7814\u7a76\u5de5\u5177\uff0c\u5c55\u73b0\u4e86\u5176\u7528\u6237\u53cb\u597d\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4fc3\u8fdb\u6301\u7eed\u53c2\u4e0e\u548c\u51cf\u5c11\u8bad\u7ec3\u75b2\u52b3\u7684\u5efa\u8bae\u3002"}}
{"id": "2510.10181", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10181", "abs": "https://arxiv.org/abs/2510.10181", "authors": ["Shaokai Wu", "Yanbiao Ji", "Qiuchang Li", "Zhiyi Zhang", "Qichen He", "Wenyuan Xie", "Guodong Zhang", "Bayram Bayramli", "Yue Ding", "Hongtao Lu"], "title": "Dejavu: Post-Deployment Learning for Embodied Agents via Experience Feedback", "comment": null, "summary": "Embodied agents face a fundamental limitation: once deployed in real-world\nenvironments to perform specific tasks, they are unable to acquire new useful\nknowledge to enhance task performance. In this paper, we propose a general\npost-deployment learning framework called Dejavu, which employs an Experience\nFeedback Network (EFN) and augments the frozen Vision-Language-Action (VLA)\npolicy with retrieved execution memories. EFN automatically identifies\ncontextually successful prior action experiences and conditions action\nprediction on this retrieved guidance. We adopt reinforcement learning with\nsemantic similarity rewards on EFN to ensure that the predicted actions align\nwith past successful behaviors under current observations. During deployment,\nEFN continually enriches its memory with new trajectories, enabling the agent\nto exhibit \"learning from experience\" despite fixed weights. Experiments across\ndiverse embodied tasks show that EFN significantly improves adaptability,\nrobustness, and success rates over frozen baselines. These results highlight a\npromising path toward embodied agents that continually refine their behavior\nafter deployment.", "AI": {"tldr": "Dejavu\u662f\u4e00\u4e2a\u540e\u90e8\u7f72\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ecf\u9a8c\u53cd\u9988\u7f51\u7edc\u81ea\u52a8\u8bc6\u522b\u6210\u529f\u7684\u884c\u52a8\u7ecf\u9a8c\uff0c\u4ee5\u63d0\u5347\u5177\u8eab\u4ee3\u7406\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5177\u8eab\u4ee3\u7406\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u6267\u884c\u7279\u5b9a\u4efb\u52a1\u540e\uff0c\u65e0\u6cd5\u83b7\u53d6\u65b0\u7684\u6709\u7528\u77e5\u8bc6\u6765\u63d0\u5347\u4efb\u52a1\u8868\u73b0\u3002", "method": "EFN\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u8bed\u4e49\u76f8\u4f3c\u6027\u5956\u52b1\uff0c\u786e\u4fdd\u9884\u6d4b\u7684\u884c\u52a8\u4e0e\u5f53\u524d\u89c2\u6d4b\u4e0b\u7684\u6210\u529f\u884c\u4e3a\u4e00\u81f4\uff0c\u540c\u65f6\u4e0d\u65ad\u4e30\u5bcc\u8bb0\u5fc6\u4ee5\u9002\u5e94\u65b0\u73af\u5883\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u540e\u90e8\u7f72\u5b66\u4e60\u6846\u67b6Dejavu\uff0c\u5229\u7528\u7ecf\u9a8c\u53cd\u9988\u7f51\u7edc\uff08EFN\uff09\u548c\u6267\u884c\u8bb0\u5fc6\u6765\u589e\u5f3a\u56fa\u5b9a\u7684\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8\u7b56\u7565\u3002", "conclusion": "\u5b9e\u9a8c\u8868\u660e\uff0cEFN\u5728\u591a\u79cd\u5177\u8eab\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u4ee3\u7406\u7684\u9002\u5e94\u6027\u3001\u9c81\u68d2\u6027\u548c\u6210\u529f\u7387\uff0c\u5c55\u793a\u4e86\u5177\u8eab\u4ee3\u7406\u6301\u7eed\u6539\u8fdb\u884c\u4e3a\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.10173", "categories": ["cs.HC", "cs.CY", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.10173", "abs": "https://arxiv.org/abs/2510.10173", "authors": ["Paul Haimes"], "title": "Chord Colourizer: A Near Real-Time System for Visualizing Musical Key", "comment": "Author copy. This paper is in press for presentation at ADADA 2025.\n  Please cite as: Haimes, P. (in press). Chord Colourizer: A near real-time\n  system for visualizing musical key. In Proceedings of the 23rd International\n  Conference of Asia Digital Art and Design Association (ADADA)", "summary": "This paper introduces Chord Colourizer, a near real-time system that detects\nthe musical key of an audio signal and visually represents it through a novel\ngraphical user interface (GUI). The system assigns colours to musical notes\nbased on Isaac Newton's original colour wheel, preserving historical links\nbetween pitch and hue, and also integrates an Arduino-controlled LED display\nusing 3D-printed star-shaped diffusers to offer a physical ambient media\nrepresentation. The method employs Constant-Q Transform (CQT) chroma features\nfor chord estimation and visualization, followed by threshold-based filtering\nand tonal enhancement to isolate the root, third, and fifth. A confidence score\nis computed for each detection to ensure reliability, and only chords with\nmoderate to very strong certainty are visualized. The graphical interface\ndynamically updates a colour-coded keyboard layout, while the LED display\nprovides the same colour information via spatial feedback. This multi-modal\nsystem enhances user interaction with harmonic content, offering innovative\npossibilities for education and artistic performance. Limitations include\nslight latency and the inability to detect extended chords, which future\ndevelopment will aim to address through refined filtering, adaptive thresholds,\nand support for more complex harmonies such as sevenths and augmented chords.\nFuture work will also explore integration with alternative visualization\nstyles, and the comparison of audio analysis libraries to improve detection\nspeed and precision. Plans also include formal user testing to evaluate\nperception, usability, and cross-cultural interpretations of colour-pitch\nmappings.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aChord Colourizer\u7684\u7cfb\u7edf\uff0c\u5b83\u901a\u8fc7\u8272\u5f69\u53ef\u89c6\u5316\u97f3\u4e50\u548c\u58f0\uff0c\u5177\u6709\u521b\u65b0\u548c\u5b9e\u65f6\u7684\u7528\u6237\u4e92\u52a8\u7279\u70b9\uff0c\u4f46\u9762\u4e34\u5ef6\u8fdf\u548c\u548c\u5f26\u590d\u6742\u6027\u68c0\u6d4b\u7684\u6311\u6218\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u5c06\u97f3\u4e50\u97f3\u9ad8\u4e0e\u8272\u5f69\u5173\u8054\uff0c\u63d0\u5347\u7528\u6237\u5bf9\u548c\u58f0\u5185\u5bb9\u7684\u4e92\u52a8\uff0c\u5e76\u4e3a\u6559\u80b2\u548c\u827a\u672f\u8868\u6f14\u63d0\u4f9b\u521b\u65b0\u7684\u53ef\u80fd\u6027\u3002", "method": "\u4f7f\u7528\u5e38\u6570Q\u53d8\u6362\uff08CQT\uff09\u8272\u5ea6\u7279\u5f81\u8fdb\u884c\u548c\u5f26\u4f30\u8ba1\u53ca\u53ef\u89c6\u5316\uff0c\u7ed3\u5408\u57fa\u4e8e\u9608\u503c\u7684\u8fc7\u6ee4\u548c\u97f3\u8c03\u589e\u5f3a\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u5b9e\u65f6\u68c0\u6d4b\u97f3\u9891\u4fe1\u53f7\u7684\u97f3\u4e50\u8c03\u6027\uff0c\u5e76\u901a\u8fc7\u89c6\u89c9\u8868\u793a\u548c LED \u663e\u793a\u63d0\u4f9b\u591a\u6a21\u6001\u53cd\u9988\uff0c\u589e\u5f3a\u7528\u6237\u4f53\u9a8c\u3002", "conclusion": "Chord Colourizer \u5c55\u793a\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u591a\u6a21\u6001\u7cfb\u7edf\uff0c\u80fd\u591f\u4ee5\u52a8\u6001\u548c\u76f4\u89c2\u7684\u65b9\u5f0f\u5c55\u793a\u97f3\u4e50\u548c\u8272\u5f69\u7684\u5173\u7cfb\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u67d0\u4e9b\u9650\u5236\u4ee5\u589e\u5f3a\u5176\u529f\u80fd\u3002"}}
{"id": "2510.10206", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.10206", "abs": "https://arxiv.org/abs/2510.10206", "authors": ["Zuhong Liu", "Junhao Ge", "Minhao Xiong", "Jiahao Gu", "Bowei Tang", "Wei Jing", "Siheng Chen"], "title": "It Takes Two: Learning Interactive Whole-Body Control Between Humanoid Robots", "comment": null, "summary": "The true promise of humanoid robotics lies beyond single-agent autonomy: two\nor more humanoids must engage in physically grounded, socially meaningful\nwhole-body interactions that echo the richness of human social interaction.\nHowever, single-humanoid methods suffer from the isolation issue, ignoring\ninter-agent dynamics and causing misaligned contacts, interpenetrations, and\nunrealistic motions. To address this, we present Harmanoid , a dual-humanoid\nmotion imitation framework that transfers interacting human motions to two\nrobots while preserving both kinematic fidelity and physical realism. Harmanoid\ncomprises two key components: (i) contact-aware motion retargeting, which\nrestores inter-body coordination by aligning SMPL contacts with robot vertices,\nand (ii) interaction-driven motion controller, which leverages\ninteraction-specific rewards to enforce coordinated keypoints and physically\nplausible contacts. By explicitly modeling inter-agent contacts and\ninteraction-aware dynamics, Harmanoid captures the coupled behaviors between\nhumanoids that single-humanoid frameworks inherently overlook. Experiments\ndemonstrate that Harmanoid significantly improves interactive motion imitation,\nsurpassing existing single-humanoid frameworks that largely fail in such\nscenarios.", "AI": {"tldr": "Harmanoid \u662f\u4e00\u79cd\u53cc\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u6a21\u4eff\u6846\u67b6\uff0c\u80fd\u66f4\u597d\u5730\u6355\u6349\u4eba\u7c7b\u4e92\u52a8\u884c\u4e3a\uff0c\u6539\u5584\u4e86\u5355\u4eba\u5f62\u673a\u5668\u4eba\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u63a2\u7d22\u4eba\u5f62\u673a\u5668\u4eba\u5728\u591a\u4e3b\u4f53\u95f4\u7684\u4e92\u52a8\u6f5c\u529b\uff0c\u4ee5\u5b9e\u73b0\u66f4\u771f\u5b9e\u7684\u793e\u4f1a\u4e92\u52a8\u3002", "method": "\u901a\u8fc7\u63a5\u89e6\u611f\u77e5\u7684\u8fd0\u52a8\u91cd\u65b0\u5b9a\u4f4d\u548c\u4e92\u52a8\u9a71\u52a8\u7684\u8fd0\u52a8\u63a7\u5236\u5668\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u534f\u8c03\u8fd0\u52a8\u3002", "result": "Harmanoid \u6846\u67b6\u5728\u4e24\u4e2a\u4eba\u5f62\u673a\u5668\u4eba\u95f4\u6210\u529f\u8f6c\u79fb\u4e92\u52a8\u52a8\u4f5c\uff0c\u4fdd\u6301\u8fd0\u52a8\u7684\u771f\u5b9e\u611f\u548c\u52a8\u529b\u5b66\u4e00\u81f4\u6027\u3002", "conclusion": "Harmanoid \u663e\u8457\u63d0\u5347\u4e86\u4ea4\u4e92\u52a8\u4f5c\u6a21\u4eff\u7684\u6548\u679c\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u5355\u4eba\u5f62\u673a\u5668\u4eba\u6846\u67b6\u3002"}}
{"id": "2510.10199", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10199", "abs": "https://arxiv.org/abs/2510.10199", "authors": ["Haocan Sun", "Weizi Liu", "Di Wu", "Guoming Yu", "Mike Yao"], "title": "Revisiting Trust in the Era of Generative AI: Factorial Structure and Latent Profiles", "comment": null, "summary": "Trust is one of the most important factors shaping whether and how people\nadopt and rely on artificial intelligence (AI). Yet most existing studies\nmeasure trust in terms of functionality, focusing on whether a system is\nreliable, accurate, or easy to use, while giving less attention to the social\nand emotional dimensions that are increasingly relevant for today's generative\nAI (GenAI) systems. These systems do not just process information; they\nconverse, respond, and collaborate with users, blurring the line between tool\nand partner. In this study, we introduce and validate the Human-AI Trust Scale\n(HAITS), a new measure designed to capture both the rational and relational\naspects of trust in GenAI. Drawing on prior trust theories, qualitative\ninterviews, and two waves of large-scale surveys in China and the United\nStates, we used exploratory (n = 1,546) and confirmatory (n = 1,426) factor\nanalyses to identify four key dimensions of trust: Affective Trust, Competence\nTrust, Benevolence & Integrity, and Perceived Risk. We then applied latent\nprofile analysis to classify users into six distinct trust profiles, revealing\nmeaningful differences in how affective-competence trust and trust-distrust\nframeworks coexist across individuals and cultures. Our findings offer a\nvalidated, culturally sensitive tool for measuring trust in GenAI and provide\nnew insight into how trust evolves in human-AI interaction. By integrating\ninstrumental and relational perspectives of trust, this work lays the\nfoundation for more nuanced research and design of trustworthy AI systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86HAITS\u91cf\u8868\uff0c\u6574\u5408\u4fe1\u4efb\u7684\u7406\u6027\u4e0e\u60c5\u611f\u7ef4\u5ea6\uff0c\u4e3a\u751f\u6210\u5f0fAI\u7684\u4fe1\u4efb\u6d4b\u91cf\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u548c\u89c1\u89e3\u3002", "motivation": "\u7814\u7a76\u4fe1\u4efb\u5728\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u91c7\u7528\u548c\u4f9d\u8d56\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u7279\u522b\u662f\u793e\u4ea4\u548c\u60c5\u611f\u7ef4\u5ea6\u5728\u751f\u6210\u5f0fAI\u7cfb\u7edf\u4e2d\u7684\u76f8\u5173\u6027\u3002", "method": "\u901a\u8fc7\u5bf9\u4fe1\u4efb\u7406\u8bba\u7684\u56de\u987e\u3001\u5b9a\u6027\u8bbf\u8c08\u548c\u4e24\u8f6e\u5927\u89c4\u6a21\u8c03\u67e5\uff0c\u5229\u7528\u63a2\u7d22\u6027\u548c\u9a8c\u8bc1\u6027\u56e0\u7d20\u5206\u6790\u8bc6\u522b\u4fe1\u4efb\u7684\u56db\u4e2a\u5173\u952e\u7ef4\u5ea6\uff0c\u5e76\u5e94\u7528\u6f5c\u5728\u5256\u9762\u5206\u6790\u5206\u7c7b\u7528\u6237\u7684\u4fe1\u4efb\u7279\u5f81\u3002", "result": "\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u4eba\u673a\u4fe1\u4efb\u91cf\u8868\uff08HAITS\uff09\uff0c\u80fd\u591f\u540c\u65f6\u6355\u6349\u7406\u6027\u548c\u5173\u7cfb\u6027\u7684\u4fe1\u4efb\u7ef4\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6d4b\u91cf\u751f\u6210\u5f0fAI\u4e2d\u7684\u4fe1\u4efb\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u4e14\u5177\u6709\u6587\u5316\u654f\u611f\u6027\u7684\u5de5\u5177\uff0c\u5e76\u4e3a\u4fe1\u4efb\u5728\u4eba\u5de5\u667a\u80fd\u4e92\u52a8\u4e2d\u7684\u6f14\u53d8\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2510.10217", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10217", "abs": "https://arxiv.org/abs/2510.10217", "authors": ["Hyogo Hiruma", "Hiroshi Ito", "Tetsuya Ogata"], "title": "UF-RNN: Real-Time Adaptive Motion Generation Using Uncertainty-Driven Foresight Prediction", "comment": "8 pages, 6 figures", "summary": "Training robots to operate effectively in environments with uncertain states,\nsuch as ambiguous object properties or unpredictable interactions, remains a\nlongstanding challenge in robotics. Imitation learning methods typically rely\non successful examples and often neglect failure scenarios where uncertainty is\nmost pronounced. To address this limitation, we propose the Uncertainty-driven\nForesight Recurrent Neural Network (UF-RNN), a model that combines standard\ntime-series prediction with an active \"Foresight\" module. This module performs\ninternal simulations of multiple future trajectories and refines the hidden\nstate to minimize predicted variance, enabling the model to selectively explore\nactions under high uncertainty. We evaluate UF-RNN on a door-opening task in\nboth simulation and a real-robot setting, demonstrating that, despite the\nabsence of explicit failure demonstrations, the model exhibits robust\nadaptation by leveraging self-induced chaotic dynamics in its latent space.\nWhen guided by the Foresight module, these chaotic properties stimulate\nexploratory behaviors precisely when the environment is ambiguous, yielding\nimproved success rates compared to conventional stochastic RNN baselines. These\nfindings suggest that integrating uncertainty-driven foresight into imitation\nlearning pipelines can significantly enhance a robot's ability to handle\nunpredictable real-world conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cdUF-RNN\u6a21\u578b\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u7684\u9884\u6d4b\u548c\u5185\u90e8\u6a21\u62df\uff0c\u4ee5\u6539\u8fdb\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5728\u4e0d\u786e\u5b9a\u72b6\u6001\u4e0b\uff08\u5982\u6a21\u7cca\u5bf9\u8c61\u5c5e\u6027\u6216\u4e0d\u53ef\u9884\u6d4b\u7684\u4e92\u52a8\uff09\u64cd\u4f5c\u673a\u5668\u4eba\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5728\u5931\u8d25\u573a\u666f\u4e2d\u7684\u7f3a\u9677\u3002", "method": "Uncertainty-driven Foresight Recurrent Neural Network (UF-RNN)", "result": "UF-RNN\u5728\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u73af\u5883\u4e0b\u8fdb\u884c\u5f00\u95e8\u4efb\u52a1\u8bc4\u4f30\uff0c\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u5e94\u6027\uff0c\u901a\u8fc7\u81ea\u8bf1\u53d1\u7684\u6df7\u6c8c\u52a8\u6001\u63a8\u52a8\u63a2\u7d22\u884c\u4e3a\uff0c\u63d0\u9ad8\u6210\u529f\u7387\u3002", "conclusion": "\u5c06\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u7684\u524d\u77bb\u6027\u6574\u5408\u5165\u6a21\u4eff\u5b66\u4e60\u6d41\u7a0b\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u5e94\u5bf9\u4e0d\u53ef\u9884\u6d4b\u73af\u5883\u7684\u80fd\u529b\u3002"}}
{"id": "2510.10258", "categories": ["cs.HC", "cs.ET", "cs.MM", "H.5.2; H.5.1; H.5.m"], "pdf": "https://arxiv.org/pdf/2510.10258", "abs": "https://arxiv.org/abs/2510.10258", "authors": ["Shuai Guo", "Dawei Liu", "Tiantian Zheng"], "title": "Exploration of Embodied Space Experience through Umbilical Interaction: A Grounded Theory Approach", "comment": "10 pages, 2 figures", "summary": "This paper critiques the limits of human-centered design in HCI, proposing a\nshift toward Interface-Centered Design. Drawing on Hookway's philosophy of\ninterfaces, phenomenology, and embodied interaction, we created Umbilink, an\numbilical interaction device simulating a uterine environment with tactile\nsensors and rhythmic feedback to induce a pre-subjectivized state of sensory\nreduction. Participants' experiences were captured through semi-structured\ninterviews and analyzed with grounded theory. Our contributions are: (1)\nintroducing the novel interface type of Umbilical Interaction; (2)\ndemonstrating the cognitive value of materialized interfaces in a\nhuman-interface-environment relation; (3) highlighting the design role of\nwearing rituals as liminal experiences. As a pilot study, this design suggests\nimaginative applications in healing, meditation, and sleep, while offering a\nspeculative tool for future interface research.", "AI": {"tldr": "\u672c\u7814\u7a76\u6279\u5224\u4eba\u672c\u8bbe\u8ba1\u5c40\u9650\uff0c\u63d0\u51faUmbilink\u4f5c\u4e3a\u65b0\u63a5\u53e3\u8bbe\u8ba1\uff0c\u63a2\u7d22\u5176\u5728\u7597\u6108\u3001\u51a5\u60f3\u548c\u7761\u7720\u4e2d\u7684\u5e94\u7528", "motivation": "\u6279\u5224\u4eba\u672c\u4e2d\u5fc3\u8bbe\u8ba1\u7684\u5c40\u9650\uff0c\u5021\u5bfc\u63a5\u53e3\u4e2d\u5fc3\u8bbe\u8ba1\u8f6c\u53d8", "method": "\u901a\u8fc7\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u4e0e\u624e\u6839\u7406\u8bba\u5206\u6790\u53c2\u4e0e\u8005\u4f53\u9a8c", "result": "\u63d0\u51fa\u4e86Umbilical Interaction\u8fd9\u4e00\u65b0\u578b\u63a5\u53e3\u7c7b\u578b\uff0c\u5e76\u9610\u660e\u4e86\u7269\u8d28\u5316\u63a5\u53e3\u5728\u4eba\u4e0e\u63a5\u53e3\u73af\u5883\u5173\u7cfb\u4e2d\u7684\u8ba4\u77e5\u4ef7\u503c", "conclusion": "\u6b64\u8bbe\u8ba1\u4e3a\u672a\u6765\u63a5\u53e3\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u5177\u60f3\u8c61\u529b\u7684\u5de5\u5177\uff0c\u5e76\u6697\u793a\u51fa\u65b0\u7684\u5e94\u7528\u65b9\u5411\u3002"}}
{"id": "2510.10221", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10221", "abs": "https://arxiv.org/abs/2510.10221", "authors": ["Hyogo Hiruma", "Hiroshi Ito", "Hiroki Mori", "Tetsuya Ogata"], "title": "A3RNN: Bi-directional Fusion of Bottom-up and Top-down Process for Developmental Visual Attention in Robots", "comment": "8 pages, 5 figures", "summary": "This study investigates the developmental interaction between top-down (TD)\nand bottom-up (BU) visual attention in robotic learning. Our goal is to\nunderstand how structured, human-like attentional behavior emerges through the\nmutual adaptation of TD and BU mechanisms over time. To this end, we propose a\nnovel attention model $A^3 RNN$ that integrates predictive TD signals and\nsaliency-based BU cues through a bi-directional attention architecture.\n  We evaluate our model in robotic manipulation tasks using imitation learning.\nExperimental results show that attention behaviors evolve throughout training,\nfrom saliency-driven exploration to prediction-driven direction. Initially, BU\nattention highlights visually salient regions, which guide TD processes, while\nas learning progresses, TD attention stabilizes and begins to reshape what is\nperceived as salient. This trajectory reflects principles from cognitive\nscience and the free-energy framework, suggesting the importance of\nself-organizing attention through interaction between perception and internal\nprediction. Although not explicitly optimized for stability, our model exhibits\nmore coherent and interpretable attention patterns than baselines, supporting\nthe idea that developmental mechanisms contribute to robust attention\nformation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u81ea\u4e0a\u800c\u4e0b\u548c\u81ea\u4e0b\u800c\u4e0a\u7684\u89c6\u89c9\u6ce8\u610f\u529b\u4e4b\u95f4\u7684\u53d1\u5c55\u4e92\u52a8\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6ce8\u610f\u529b\u6a21\u578b\u901a\u8fc7\u53cc\u5411\u6ce8\u610f\u529b\u67b6\u6784\u6574\u5408\u4e86\u9884\u6d4b\u6027\u548c\u57fa\u4e8e\u663e\u8457\u6027\u7684\u6ce8\u610f\u4fe1\u53f7\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u7406\u89e3\u81ea\u4e0a\u800c\u4e0b\u4e0e\u81ea\u4e0b\u800c\u4e0a\u7684\u6ce8\u610f\u673a\u5236\u5982\u4f55\u901a\u8fc7\u76f8\u4e92\u9002\u5e94\u5728\u65f6\u95f4\u4e0a\u53d1\u5c55\u51fa\u7ed3\u6784\u5316\u7684\u7c7b\u4eba\u6ce8\u610f\u529b\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u6ce8\u610f\u529b\u6a21\u578bA^3 RNN\uff0c\u901a\u8fc7\u53cc\u5411\u6ce8\u610f\u529b\u67b6\u6784\u7ed3\u5408\u9884\u6d4b\u6027\u548c\u663e\u8457\u6027\u57fa\u7840\u7684\u6ce8\u610f\u4fe1\u53f7\uff0c\u8fdb\u884c\u6a21\u4eff\u5b66\u4e60\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6ce8\u610f\u529b\u884c\u4e3a\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8fdb\u5316\uff0c\u4ece\u4ee5\u663e\u8457\u6027\u9a71\u52a8\u7684\u63a2\u7d22\u8f6c\u53d8\u4e3a\u4ee5\u9884\u6d4b\u9a71\u52a8\u7684\u6307\u5f15\uff0c\u6700\u7ec8\u8868\u73b0\u51fa\u66f4\u8fde\u8d2f\u548c\u53ef\u89e3\u91ca\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\u3002", "conclusion": "\u6211\u4eec\u7684\u6a21\u578b\u901a\u8fc7\u81ea\u6211\u7ec4\u7ec7\u7684\u6ce8\u610f\u529b\u673a\u5236\u4fc3\u8fdb\u4e86\u66f4\u5f3a\u7a33\u5065\u7684\u6ce8\u610f\u529b\u5f62\u6210\uff0c\u5c55\u793a\u4e86\u8ba4\u77e5\u79d1\u5b66\u548c\u81ea\u7531\u80fd\u6846\u67b6\u7684\u539f\u5219\u3002"}}
{"id": "2510.10263", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10263", "abs": "https://arxiv.org/abs/2510.10263", "authors": ["Moona Kanwal", "Muhammad Sami Siddiqui", "Syed Anael Ali"], "title": "Unveiling Gamer Archetypes through Multi modal feature Correlations and Unsupervised Learning", "comment": "Submitted to Peer Review Journal", "summary": "Profiling gamers provides critical insights for adaptive game design,\nbehavioral understanding, and digital well-being. This study proposes an\nintegrated, data-driven framework that combines psychological measures,\nbehavioral analytics, and machine learning to reveal underlying gamer personas.\nA structured survey of 250 participants, including 113 active gamers, captured\nmultidimensional behavioral, motivational, and social data. The analysis\npipeline integrated feature engineering, association-network, knowledge-graph\nanalysis, and unsupervised clustering to extract meaningful patterns.\nCorrelation statistics uses Cramers V, Tschuprows T, Theils U, and Spearmans\nquantified feature associations, and network centrality guided feature\nselection. Dimensionality-reduction techniques such as PCA, SVD, t-SNE are\ncoupled with clustering algorithms like K-Means, Agglomerative, Spectral,\nDBSCAN, evaluated using Silhouette, Calinski Harabasz, and Davies Bouldin\nindices. The PCA with K-Means with k = 4 model achieved optimal cluster quality\nwith Silhouette = 0.4, identifying four archetypes as Immersive Social\nStory-Seekers, Disciplined Optimizers, Strategic Systems Navigators, and\nCompetitive Team-Builders. This research contributes a reproducible pipeline\nthat links correlation-driven network insights with unsupervised learning. The\nintegration of behavioral correlation networks with clustering not only\nenhances classification accuracy but also offers a holistic lens to connect\ngameplay motivations with psychological and wellness outcomes.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6570\u636e\u9a71\u52a8\u6846\u67b6\uff0c\u7ed3\u5408\u5fc3\u7406\u548c\u884c\u4e3a\u5206\u6790\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u8bc6\u522b\u6e38\u620f\u73a9\u5bb6\u7684\u4e0d\u540c\u7c7b\u578b\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u6e38\u620f\u8bbe\u8ba1\u548c\u73a9\u5bb6\u652f\u6301\u3002", "motivation": "\u63d0\u4f9b\u5173\u4e8e\u6e38\u620f\u73a9\u5bb6\u7684\u91cd\u8981\u89c1\u89e3\uff0c\u4ee5\u4fbf\u8fdb\u884c\u81ea\u9002\u5e94\u6e38\u620f\u8bbe\u8ba1\u3001\u884c\u4e3a\u7406\u89e3\u548c\u6570\u5b57\u5065\u5eb7\u7ba1\u7406", "method": "\u7ed3\u5408\u5fc3\u7406\u6d4b\u91cf\u3001\u884c\u4e3a\u5206\u6790\u548c\u673a\u5668\u5b66\u4e60\u7684\u96c6\u6210\u6570\u636e\u9a71\u52a8\u6846\u67b6", "result": "\u8bc6\u522b\u51fa\u56db\u79cd\u6e38\u620f\u73a9\u5bb6\u539f\u578b\uff1a\u6c89\u6d78\u793e\u4ea4\u5bfb\u6c42\u8005\u3001\u7eaa\u5f8b\u4f18\u5316\u8005\u3001\u6218\u7565\u7cfb\u7edf\u5bfc\u822a\u8005\u548c\u7ade\u4e89\u56e2\u961f\u5efa\u8bbe\u8005", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u91cd\u73b0\u7684\u5206\u6790\u7ba1\u9053\uff0c\u5c06\u76f8\u5173\u6027\u9a71\u52a8\u7684\u7f51\u7edc\u6d1e\u89c1\u4e0e\u65e0\u76d1\u7763\u5b66\u4e60\u7ed3\u5408\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u8fd8\u80fd\u5168\u9762\u8054\u7cfb\u6e38\u620f\u52a8\u673a\u4e0e\u5fc3\u7406\u5065\u5eb7\u7ed3\u679c\u3002"}}
{"id": "2510.10273", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10273", "abs": "https://arxiv.org/abs/2510.10273", "authors": ["Vincent Schoenbach", "Marvin Wiedemann", "Raphael Memmesheimer", "Malte Mosbach", "Sven Behnke"], "title": "Integration of the TIAGo Robot into Isaac Sim with Mecanum Drive Modeling and Learned S-Curve Velocity Profiles", "comment": "In Proceedings of IEEE 21st International Conference on Automation\n  Science and Engineering (CASE), Los Angeles, USA, August 2025", "summary": "Efficient physics simulation has significantly accelerated research progress\nin robotics applications such as grasping and assembly. The advent of\nGPU-accelerated simulation frameworks like Isaac Sim has particularly empowered\nlearning-based methods, enabling them to tackle increasingly complex tasks. The\nPAL Robotics TIAGo++ Omni is a versatile mobile manipulator equipped with a\nmecanum-wheeled base, allowing omnidirectional movement and a wide range of\ntask capabilities. However, until now, no model of the robot has been available\nin Isaac Sim. In this paper, we introduce such a model, calibrated to\napproximate the behavior of the real robot, with a focus on its omnidirectional\ndrive dynamics. We present two control models for the omnidirectional drive: a\nphysically accurate model that replicates real-world wheel dynamics and a\nlightweight velocity-based model optimized for learning-based applications.\nWith these models, we introduce a learning-based calibration approach to\napproximate the real robot's S-shaped velocity profile using minimal trajectory\ndata recordings. This simulation should allow researchers to experiment with\nthe robot and perform efficient learning-based control in diverse environments.\nWe provide the integration publicly at https://github.com/AIS-Bonn/tiago_isaac.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684TIAGo++ Omni\u673a\u5668\u4eba\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e24\u79cd\u4e0d\u540c\u7684\u63a7\u5236\u6a21\u578b\u4ee5\u4fc3\u8fdb\u673a\u5668\u4eba\u5728\u591a\u79cd\u73af\u5883\u4e0b\u7684\u5b66\u4e60\u63a7\u5236\u5b9e\u9a8c\u3002", "motivation": "\u9ad8\u6548\u7684\u7269\u7406\u6a21\u62df\u63a8\u52a8\u4e86\u673a\u5668\u4eba\u7814\u7a76\u7684\u53d1\u5c55\uff0c\u7279\u522b\u662f\u5728\u6293\u53d6\u548c\u7ec4\u88c5\u7b49\u5e94\u7528\u4e2d\u3002\u5f15\u5165\u65b0\u7684\u673a\u5668\u4eba\u6a21\u578b\u4ee5\u652f\u6301\u5b66\u4e60\u9a71\u52a8\u7684\u63a7\u5236\u662f\u672c\u6587\u7684\u4e3b\u8981\u52a8\u673a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7269\u7406\u51c6\u786e\u7684\u63a7\u5236\u6a21\u578b\u548c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u57fa\u4e8e\u901f\u5ea6\u7684\u63a7\u5236\u6a21\u578b\uff0c\u540c\u65f6\u5f15\u5165\u4e86\u4e00\u79cd\u5b66\u4e60\u57fa\u4e8e\u7684\u6821\u51c6\u65b9\u6cd5\u3002", "result": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2aTIAGo++ Omni\u673a\u5668\u4eba\u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u88ab\u6821\u51c6\u4ee5\u8fd1\u4f3c\u771f\u5b9e\u673a\u5668\u4eba\u7684\u884c\u4e3a\uff0c\u7279\u522b\u5173\u6ce8\u5176\u5168\u5411\u9a71\u52a8\u52a8\u6001\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u79cd\u63a7\u5236\u6a21\u578b\uff1a\u4e00\u79cd\u662f\u7269\u7406\u7cbe\u786e\u6a21\u578b\uff0c\u80fd\u591f\u590d\u5236\u73b0\u5b9e\u4e16\u754c\u7684\u8f66\u8f6e\u52a8\u529b\u5b66\uff1b\u53e6\u4e00\u79cd\u662f\u8f7b\u91cf\u7ea7\u7684\u57fa\u4e8e\u901f\u5ea6\u7684\u6a21\u578b\uff0c\u4f18\u5316\u7528\u4e8e\u5b66\u4e60\u5e94\u7528\u3002\u5f15\u5165\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u6821\u51c6\u65b9\u6cd5\u53ef\u4ee5\u4f7f\u7528\u6700\u5c11\u7684\u8f68\u8ff9\u6570\u636e\u8bb0\u5f55\u6765\u8fd1\u4f3c\u771f\u5b9e\u673a\u5668\u4eba\u7684S\u5f62\u901f\u5ea6\u66f2\u7ebf\uff0c\u65e8\u5728\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u5728\u591a\u79cd\u73af\u5883\u4e2d\u8fdb\u884c\u9ad8\u6548\u7684\u5b66\u4e60\u63a7\u5236\u5b9e\u9a8c\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u6b64\u6a21\u578b\uff0c\u7814\u7a76\u4eba\u5458\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u8fdb\u884c\u673a\u5668\u4eba\u63a7\u5236\u7684\u5b66\u4e60\u548c\u5b9e\u9a8c\u3002"}}
{"id": "2510.10339", "categories": ["cs.HC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10339", "abs": "https://arxiv.org/abs/2510.10339", "authors": ["Shalaleh Rismani", "Renee Shelby", "Leah Davis", "Negar Rostamzadeh", "AJung Moon"], "title": "Measuring What Matters: Connecting AI Ethics Evaluations to System Attributes, Hazards, and Harms", "comment": null, "summary": "Over the past decade, an ecosystem of measures has emerged to evaluate the\nsocial and ethical implications of AI systems, largely shaped by high-level\nethics principles. These measures are developed and used in fragmented ways,\nwithout adequate attention to how they are situated in AI systems. In this\npaper, we examine how existing measures used in the computing literature map to\nAI system components, attributes, hazards, and harms. Our analysis draws on a\nscoping review resulting in nearly 800 measures corresponding to 11 AI ethics\nprinciples. We find that most measures focus on four principles - fairness,\ntransparency, privacy, and trust - and primarily assess model or output system\ncomponents. Few measures account for interactions across system elements, and\nonly a narrow set of hazards is typically considered for each harm type. Many\nmeasures are disconnected from where harm is experienced and lack guidance for\nsetting meaningful thresholds. These patterns reveal how current evaluation\npractices remain fragmented, measuring in pieces rather than capturing how\nharms emerge across systems. Framing measures with respect to system\nattributes, hazards, and harms can strengthen regulatory oversight, support\nactionable practices in industry, and ground future research in systems-level\nunderstanding.", "AI": {"tldr": "\u672c\u6587\u68c0\u89c6AI\u7cfb\u7edf\u8bc4\u4f30\u63aa\u65bd\u7684\u7247\u9762\u6027\uff0c\u63d0\u51fa\u9700\u8981\u66f4\u7cfb\u7edf\u7684\u6574\u4f53\u7406\u89e3\u6765\u8bc4\u4f30\u9053\u5fb7\u4e0e\u793e\u4f1a\u5f71\u54cd\u3002", "motivation": "\u8bc4\u4f30AI\u7cfb\u7edf\u7684\u793e\u4f1a\u548c\u4f26\u7406\u5f71\u54cd\u7684\u73b0\u6709\u63aa\u65bd\u5b58\u5728\u788e\u7247\u5316\u7684\u95ee\u9898\uff0c\u4e9f\u9700\u660e\u786e\u5176\u5728AI\u7cfb\u7edf\u4e2d\u7684\u5b9a\u4f4d\u3002", "method": "\u8fdb\u884c\u4e86\u8303\u56f4\u5ba1\u67e5\uff0c\u5206\u6790\u4e86\u8fd1800\u4e2a\u4e0e11\u4e2aAI\u4f26\u7406\u539f\u5219\u76f8\u5173\u7684\u8861\u91cf\u6807\u51c6\u3002", "result": "\u53d1\u73b0\u5927\u591a\u6570\u63aa\u65bd\u4fa7\u91cd\u4e8e\u516c\u5e73\u6027\u3001\u900f\u660e\u6027\u3001\u9690\u79c1\u548c\u4fe1\u4efb\u7b49\u56db\u4e2a\u539f\u5219\uff0c\u4e3b\u8981\u8bc4\u4f30\u6a21\u578b\u6216\u8f93\u51fa\u7cfb\u7edf\u7ec4\u4ef6\u3002", "conclusion": "\u5f53\u524d\u7684\u8bc4\u4f30\u5b9e\u8df5\u4ecd\u7136\u652f\u79bb\u7834\u788e\uff0c\u4ec5\u4ece\u96f6\u6563\u7684\u89d2\u5ea6\u8861\u91cf\uff0c\u800c\u672a\u80fd\u6355\u6349\u5230\u5728\u6574\u4e2a\u7cfb\u7edf\u4e2d\u51fa\u73b0\u7684\u4f24\u5bb3\u3002"}}
{"id": "2510.10274", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10274", "abs": "https://arxiv.org/abs/2510.10274", "authors": ["Jinliang Zheng", "Jianxiong Li", "Zhihao Wang", "Dongxiu Liu", "Xirui Kang", "Yuchun Feng", "Yinan Zheng", "Jiayin Zou", "Yilun Chen", "Jia Zeng", "Ya-Qin Zhang", "Jiangmiao Pang", "Jingjing Liu", "Tai Wang", "Xianyuan Zhan"], "title": "X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model", "comment": "preprint, technical report, 33 pages", "summary": "Successful generalist Vision-Language-Action (VLA) models rely on effective\ntraining across diverse robotic platforms with large-scale, cross-embodiment,\nheterogeneous datasets. To facilitate and leverage the heterogeneity in rich,\ndiverse robotic data sources, we propose a novel Soft Prompt approach with\nminimally added parameters, by infusing prompt learning concepts into\ncross-embodiment robot learning and introducing separate sets of learnable\nembeddings for each distinct data source. These embeddings serve as\nembodiment-specific prompts, which in unity empower VLA models with effective\nexploitation of varying cross-embodiment features. Our new X-VLA, a neat\nflow-matching-based VLA architecture, relies exclusively on soft-prompted\nstandard Transformer encoders, enjoying both scalability and simplicity.\nEvaluated across 6 simulations as well as 3 real-world robots, our 0.9B\ninstantiation-X-VLA-0.9B simultaneously achieves SOTA performance over a sweep\nof benchmarks, demonstrating superior results on a wide axes of capabilities,\nfrom flexible dexterity to quick adaptation across embodiments, environments,\nand tasks. Website: https://thu-air-dream.github.io/X-VLA/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684Soft Prompt\u65b9\u6cd5\u6765\u63d0\u5347\u901a\u7528\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u501f\u52a9\u4e0d\u540c\u6570\u636e\u6e90\u7684\u53ef\u8c03\u5d4c\u5165\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6027\u80fd\u7684\u8de8\u4f53\u73b0\u5b66\u4e60\u3002", "motivation": "\u4e3a\u4e86\u6709\u6548\u5229\u7528\u4e0d\u540c\u673a\u5668\u4eba\u5e73\u53f0\u7684\u591a\u6837\u5316\u6570\u636e\uff0c\u63d0\u9ad8\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u3002", "method": "\u91c7\u7528\u8f6f\u63d0\u793a\u7684\u6807\u51c6Transformer\u7f16\u7801\u5668\uff0c\u7ed3\u5408\u53ef\u5b66\u4e60\u7684\u5d4c\u5165\u6765\u5904\u7406\u8de8\u4f53\u73b0\u7279\u5f81\u3002", "result": "\u57286\u4e2a\u6a21\u62df\u548c3\u4e2a\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u6d4b\u8bd5\uff0cX-VLA-0.9B\u5b9e\u73b0\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "X-VLA\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c55\u793a\u4e86\u5176\u5728\u7075\u6d3b\u6027\u548c\u5feb\u901f\u9002\u5e94\u65b9\u9762\u7684\u4f18\u52bf\u3002"}}
{"id": "2510.10496", "categories": ["cs.HC", "cs.AI", "68T99", "J.3; H.4"], "pdf": "https://arxiv.org/pdf/2510.10496", "abs": "https://arxiv.org/abs/2510.10496", "authors": ["Ryota Takamidoa", "Chiharu Suzukia", "Hiroki Nakamoto"], "title": "Personalized Motion Guidance Framework for Athlete-Centric Coaching", "comment": null, "summary": "A critical challenge in contemporary sports science lies in filling the gap\nbetween group-level insights derived from controlled hypothesis-driven\nexperiments and the real-world need for personalized coaching tailored to\nindividual athletes' unique movement patterns. This study developed a\nPersonalized Motion Guidance Framework (PMGF) to enhance athletic performance\nby generating individualized motion-refinement guides using generative\nartificial intelligence techniques. PMGF leverages a vertical autoencoder to\nencode motion sequences into athlete-specific latent representations, which can\nthen be directly manipulated to generate meaningful guidance motions. Two\nmanipulation strategies were explored: (1) smooth interpolation between the\nlearner's motion and a target (e.g., expert) motion to facilitate observational\nlearning, and (2) shifting the motion pattern in an optimal direction in the\nlatent space using a local optimization technique. The results of the\nvalidation experiment with data from 51 baseball pitchers revealed that (1)\nPMGF successfully generated smooth transitions in motion patterns between\nindividuals across all 1,275 pitcher pairs, and (2) the features significantly\naltered through PMGF manipulations reflected known performance-enhancing\ncharacteristics, such as increased stride length and knee extension associated\nwith higher ball velocity, indicating that PMGF induces biomechanically\nplausible improvements. We propose a future extension called general-PMGF to\nenhance the applicability of this framework. This extension incorporates\nbodily, environmental, and task constraints into the generation process, aiming\nto provide more realistic and versatile guidance across diverse sports\ncontexts.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u4e2a\u6027\u5316\u8fd0\u52a8\u6307\u5bfc\u6846\u67b6\uff08PMGF\uff09\uff0c\u5229\u7528\u751f\u6210\u6027\u4eba\u5de5\u667a\u80fd\u6280\u672f\u4e3a\u8fd0\u52a8\u5458\u751f\u6210\u4e2a\u6027\u5316\u7684\u8fd0\u52a8\u4f18\u5316\u6307\u5bfc\uff0c\u4ee5\u63d0\u5347\u8fd0\u52a8\u8868\u73b0\u3002", "motivation": "\u5f53\u4eca\u4f53\u80b2\u79d1\u5b66\u9762\u4e34\u5c06\u5b9e\u9a8c\u5ba4\u6570\u636e\u8f6c\u5316\u4e3a\u4e2a\u6027\u5316\u8fd0\u52a8\u6307\u5bfc\u7684\u6311\u6218\uff0c\u672c\u7814\u7a76\u65e8\u5728\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u7814\u7a76\u5229\u7528\u5782\u76f4\u81ea\u7f16\u7801\u5668\u7f16\u7801\u8fd0\u52a8\u5e8f\u5217\uff0c\u7ed3\u5408\u4e24\u79cd\u64cd\u63a7\u7b56\u7565\uff08\u5149\u6ed1\u63d2\u503c\u4e0e\u5c40\u90e8\u4f18\u5316\uff09\uff0c\u751f\u6210\u4e2a\u6027\u5316\u8fd0\u52a8\u6307\u5bfc\u3002", "result": "\u9a8c\u8bc1\u5b9e\u9a8c\u8868\u660e\uff0cPMGF\u57281275\u5bf9\u6295\u624b\u4e2d\u6210\u529f\u751f\u6210\u8fd0\u52a8\u6a21\u5f0f\u7684\u5e73\u6ed1\u8fc7\u5ea6\uff0c\u4e14\u6539\u53d8\u540e\u7684\u7279\u5f81\u4e0e\u5df2\u77e5\u7684\u63d0\u5347\u8fd0\u52a8\u8868\u73b0\u7684\u7279\u5f81\u76f8\u7b26\u3002", "conclusion": "PMGF\u80fd\u6709\u6548\u751f\u6210\u8fd0\u52a8\u6a21\u5f0f\u4e4b\u95f4\u7684\u5e73\u6ed1\u8fc7\u6e21\uff0c\u5e76\u5f15\u5165\u751f\u7269\u529b\u5b66\u4e0a\u5408\u7406\u7684\u6539\u8fdb\u7279\u5f81\uff0c\u8868\u660e\u5176\u5728\u8fd0\u52a8\u6307\u5bfc\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u6f5c\u529b\u3002"}}
{"id": "2510.10332", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10332", "abs": "https://arxiv.org/abs/2510.10332", "authors": ["Kohio Deflesselle", "M\u00e9lodie Daniel", "Aly Magassouba", "Miguel Aranda", "Olivier Ly"], "title": "Towards Safe Maneuvering of Double-Ackermann-Steering Robots with a Soft Actor-Critic Framework", "comment": "4 pages, 3 figures, 2 tables, Accepted for Safety of Intelligent and\n  Autonomous Vehicles: Formal Methods vs. Machine Learning approaches for\n  reliable navigation (SIAV-FM2L) an IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS 2025) workshop", "summary": "We present a deep reinforcement learning framework based on Soft Actor-Critic\n(SAC) for safe and precise maneuvering of double-Ackermann-steering mobile\nrobots (DASMRs). Unlike holonomic or simpler non-holonomic robots such as\ndifferential-drive robots, DASMRs face strong kinematic constraints that make\nclassical planners brittle in cluttered environments. Our framework leverages\nthe Hindsight Experience Replay (HER) and the CrossQ overlay to encourage\nmaneuvering efficiency while avoiding obstacles. Simulation results with a\nheavy four-wheel-steering rover show that the learned policy can robustly reach\nup to 97% of target positions while avoiding obstacles. Our framework does not\nrely on handcrafted trajectories or expert demonstrations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f6f\u6f14\u5458\u8bc4\u8bba\u5bb6\u6846\u67b6\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8\u53ccAckermann\u8f6c\u5411\u79fb\u52a8\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b89\u5168\u7cbe\u786e\u64cd\u63a7\u3002", "motivation": "\u9762\u5bf9\u590d\u6742\u73af\u5883\u4e2d\u7684\u5f3a\u52a8\u529b\u5b66\u7ea6\u675f\uff0c\u73b0\u6709\u7684\u7ecf\u5178\u89c4\u5212\u65b9\u6cd5\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u6765\u63d0\u5347\u79fb\u52a8\u673a\u5668\u4eba\u7684\u64cd\u4f5c\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u5229\u7528\u8f6f actor-critic \u548c Hindsight Experience Replay \u4ee5\u53ca CrossQ \u8986\u76d6\u6765\u8bad\u7ec3\u673a\u5668\u4eba\u7b56\u7565\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u5b66\u4e60\u5230\u7684\u7b56\u7565\u53ef\u5728\u907f\u5f00\u969c\u788d\u7269\u7684\u540c\u65f6\uff0c\u7a33\u5065\u5730\u5230\u8fbe97%\u7684\u76ee\u6807\u4f4d\u7f6e\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u5730\u63d0\u9ad8\u4e86\u53ccAckermann\u8f6c\u5411\u79fb\u52a8\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b89\u5168\u7cbe\u786e\u64cd\u63a7\u80fd\u529b\u3002"}}
{"id": "2510.10616", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.10616", "abs": "https://arxiv.org/abs/2510.10616", "authors": ["Matan Solomon", "Ofra Amir", "Omer Ben-Porat"], "title": "Assessing Policy Updates: Toward Trust-Preserving Intelligent User Interfaces", "comment": "10 pages, 6 figures. Under review for ACM IUI 2026", "summary": "Reinforcement learning agents are often updated with human feedback, yet such\nupdates can be unreliable: reward misspecification, preference conflicts, or\nlimited data may leave policies unchanged or even worse. Because policies are\ndifficult to interpret directly, users face the challenge of deciding whether\nan update has truly helped. We propose that assessing model updates -- not just\na single model -- is a critical design challenge for intelligent user\ninterfaces. In a controlled study, participants provided feedback to an agent\nin a gridworld and then compared its original and updated policies. We\nevaluated four strategies for communicating updates: no demonstration,\nsame-context, random-context, and salient-contrast demonstrations designed to\nhighlight informative differences. Salient-contrast demonstrations\nsignificantly improved participants' ability to detect when updates helped or\nharmed performance, mitigating participants' bias towards assuming that\nfeedback is always beneficial, and supported better trust calibration across\ncontexts.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u5982\u4f55\u901a\u8fc7\u4e0d\u540c\u5c55\u793a\u7b56\u7565\u8bc4\u4f30\u6a21\u578b\u66f4\u65b0\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u663e\u8457\u5bf9\u6bd4\u663e\u793a\u7b56\u7565\u6700\u6709\u6548\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u4e2d\u4eba\u7c7b\u53cd\u9988\u7684\u66f4\u65b0\u53ef\u80fd\u4e0d\u53ef\u9760\uff0c\u56e0\u6b64\u8bc4\u4f30\u66f4\u65b0\u6548\u679c\u6210\u4e3a\u8bbe\u8ba1\u667a\u80fd\u7528\u6237\u754c\u9762\u65f6\u7684\u91cd\u8981\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5728\u4e00\u4e2a\u7f51\u683c\u4e16\u754c\u4e2d\uff0c\u53c2\u4e0e\u8005\u7ed9\u4ee3\u7406\u63d0\u4f9b\u53cd\u9988\uff0c\u5e76\u6bd4\u8f83\u5176\u539f\u59cb\u548c\u66f4\u65b0\u540e\u7684\u7b56\u7565\uff0c\u8bc4\u4f30\u4e0d\u540c\u7684\u5c55\u793a\u7b56\u7565\u3002", "result": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u4f7f\u7528\u4eba\u7c7b\u53cd\u9988\u66f4\u65b0\u6a21\u578b\u65f6\u53ef\u80fd\u9762\u4e34\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5982\u4f55\u8bc4\u4f30\u591a\u6a21\u578b\u66f4\u65b0\u800c\u4e0d\u4ec5\u4ec5\u662f\u5355\u4e00\u6a21\u578b\u3002\u901a\u8fc7\u4e00\u4e2a\u63a7\u5236\u5b9e\u9a8c\uff0c\u7814\u7a76\u8005\u8bc4\u4f30\u4e86\u56db\u79cd\u4e0d\u540c\u7684\u66f4\u65b0\u5c55\u793a\u7b56\u7565\uff0c\u5e76\u53d1\u73b0\u663e\u8457\u5bf9\u6bd4\u5c55\u793a\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u53c2\u4e0e\u8005\u5bf9\u6a21\u578b\u66f4\u65b0\u6548\u679c\u7684\u5224\u65ad\u80fd\u529b\u3002", "conclusion": "\u663e\u8457\u5bf9\u6bd4\u5c55\u793a\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u53c2\u4e0e\u8005\u8bc6\u522b\u66f4\u65b0\u662f\u5426\u6709\u76ca\u7684\u80fd\u529b\uff0c\u8fd8\u5e2e\u52a9\u4fee\u6b63\u4e86\u4ed6\u4eec\u5bf9\u53cd\u9988\u7684\u4fe1\u4efb\u504f\u5dee\u3002"}}
{"id": "2510.10337", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10337", "abs": "https://arxiv.org/abs/2510.10337", "authors": ["Jihong Zhu", "Kefeng Huang", "Jonathon Pipe", "Chris Horbaczewsky", "Andy Tyrrell", "Ian J. S. Fairlamb"], "title": "Rise of the Robochemist", "comment": "This article was originally published in the IEEE Systems, Man, and\n  Cybernetics Society eNewsletter, September 2025 issue:\n  https://www.ieeesmc.org/wp-content/uploads/2024/10/FeatureArticle_Sept25.pdf", "summary": "Chemistry, a long-standing discipline, has historically relied on manual and\noften time-consuming processes. While some automation exists, the field is now\non the cusp of a significant evolution driven by the integration of robotics\nand artificial intelligence (AI), giving rise to the concept of the\nrobochemist: a new paradigm where autonomous systems assist in designing,\nexecuting, and analyzing experiments. Robochemists integrate mobile\nmanipulators, advanced perception, teleoperation, and data-driven protocols to\nexecute experiments with greater adaptability, reproducibility, and safety.\nRather than a fully automated replacement for human chemists, we envisioned the\nrobochemist as a complementary partner that works collaboratively to enhance\ndiscovery, enabling a more efficient exploration of chemical space and\naccelerating innovation in pharmaceuticals, materials science, and sustainable\nmanufacturing. This article traces the technologies, applications, and\nchallenges that define this transformation, highlighting both the opportunities\nand the responsibilities that accompany the emergence of the robochemist.\nUltimately, the future of chemistry is argued to lie in a symbiotic partnership\nwhere human intuition and expertise is amplified by robotic precision and\nAI-driven insight.", "AI": {"tldr": "\u673a\u5668\u4eba\u548c\u4eba\u5de5\u667a\u80fd\u7684\u7ed3\u5408\u5c06\u6539\u53d8\u5316\u5b66\u7814\u7a76\uff0c\u521b\u5efa\u2018robo-chemist\u2019\uff0c\u4e0e\u4eba\u7c7b\u5316\u5b66\u5bb6\u534f\u4f5c\uff0c\u63d0\u9ad8\u5b9e\u9a8c\u6548\u7387\u4e0e\u521b\u65b0\u3002", "motivation": "\u63a8\u52a8\u5316\u5b66\u7684\u8fdb\u6b65\uff0c\u63d0\u9ad8\u5b9e\u9a8c\u6548\u7387\u548c\u521b\u65b0\u901f\u5ea6", "method": "\u7efc\u8ff0\u5f53\u4ee3\u673a\u5668\u4eba\u53ca\u4eba\u5de5\u667a\u80fd\u5728\u5316\u5b66\u9886\u57df\u7684\u5e94\u7528", "result": "\u63d0\u51fa\u4e86\u2018robo-chemist\u2019\u6982\u5ff5\uff0c\u5f3a\u8c03\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u5316\u5b66\u5bb6\u7684\u534f\u4f5c\u5173\u7cfb", "conclusion": "\u5316\u5b66\u7684\u672a\u6765\u4f9d\u8d56\u4e8e\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u5171\u751f\u5408\u4f5c\uff0c\u7ed3\u5408\u4eba\u7c7b\u7684\u76f4\u89c9\u548c\u673a\u5668\u7684\u7cbe\u51c6\u3002"}}
{"id": "2510.10710", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.10710", "abs": "https://arxiv.org/abs/2510.10710", "authors": ["Jaroslaw Domaszewicz", "Damian Sienicki", "Michal Obirek"], "title": "Informative Keyboard and its Application to Raise Awareness of Smartphone Use", "comment": null, "summary": "Excessive smartphone use is now widely considered a personal and societal\nproblem. It is recognized by application and smartphone makers, who provide\ntools to track the amount of use, set limits, or block certain services at\npredefined times. These tools, while powerful, may require significant\ncognitive effort to operate: configuration parameters need to be set, and\ncaptured statistics need to be analyzed. To offer a complementary solution, we\npropose a radically different approach. We employ the keyboard of a smartphone\nas an output device. With each press of a key, the user is given a high-level,\nqualitative, color-encoded estimate of the amount of recent smartphone use. The\ntechnique, dubbed the informative keyboard, is a case of implicit interaction:\nthe user's intention is to enter text but, while typing, they receive the\nfeedback. In the paper, we elaborate the concept, identify design decisions,\ndescribe our implementation, present the outcome of a questionnaire-based\nevaluation, and point to some other applications of the informative keyboard.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u667a\u80fd\u624b\u673a\u952e\u76d8\uff0c\u5141\u8bb8\u7528\u6237\u5728\u8f93\u5165\u6587\u672c\u65f6\u63a5\u6536\u5176\u8bbe\u5907\u4f7f\u7528\u60c5\u51b5\u7684\u53cd\u9988\uff0c\u65e8\u5728\u7b80\u5316\u5bf9\u4f7f\u7528\u91cf\u7684\u7ba1\u7406\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u667a\u80fd\u624b\u673a\u8fc7\u5ea6\u4f7f\u7528\u5e26\u6765\u7684\u4e2a\u4eba\u548c\u793e\u4f1a\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u65e0\u9700\u590d\u6742\u914d\u7f6e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u667a\u80fd\u624b\u673a\u7684\u952e\u76d8\u63d0\u4f9b\u4e0e\u4f7f\u7528\u91cf\u76f8\u5173\u7684\u5b9e\u65f6\u53cd\u9988\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u952e\u76d8\u4ea4\u4e92\u65b9\u5f0f\uff0c\u7528\u6237\u5728\u8f93\u5165\u65f6\u80fd\u591f\u83b7\u5f97\u667a\u80fd\u624b\u673a\u4f7f\u7528\u7684\u9ad8\u5c42\u6b21\u3001\u5b9a\u6027\u7684\u53cd\u9988\u3002", "conclusion": "\u8fd9\u79cd\u4fe1\u606f\u4e30\u5bcc\u7684\u952e\u76d8\u65b9\u6cd5\u4e3a\u667a\u80fd\u624b\u673a\u4f7f\u7528\u7ba1\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u4f4e\u8ba4\u77e5\u8d1f\u62c5\u7684\u65b9\u5f0f\uff0c\u5177\u6709\u6f5c\u5728\u7684\u591a\u79cd\u5e94\u7528\u3002"}}
{"id": "2510.10346", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10346", "abs": "https://arxiv.org/abs/2510.10346", "authors": ["Yuxiang Peng", "Chuchu Chen", "Kejian Wu", "Guoquan Huang"], "title": "sqrtVINS: Robust and Ultrafast Square-Root Filter-based 3D Motion Tracking", "comment": null, "summary": "In this paper, we develop and open-source, for the first time, a square-root\nfilter (SRF)-based visual-inertial navigation system (VINS), termed sqrtVINS,\nwhich is ultra-fast, numerically stable, and capable of dynamic initialization\neven under extreme conditions (i.e., extremely small time window). Despite\nrecent advancements in VINS, resource constraints and numerical instability on\nembedded (robotic) systems with limited precision remain critical challenges. A\nsquare-root covariance-based filter offers a promising solution by providing\nnumerical stability, efficient memory usage, and guaranteed positive\nsemi-definiteness. However, canonical SRFs suffer from inefficiencies caused by\ndisruptions in the triangular structure of the covariance matrix during\nupdates. The proposed method significantly improves VINS efficiency with a\nnovel Cholesky decomposition (LLT)-based SRF update, by fully exploiting the\nsystem structure to preserve the structure. Moreover, we design a fast, robust,\ndynamic initialization method, which first recovers the minimal states without\ntriangulating 3D features and then efficiently performs iterative SRF update to\nrefine the full states, enabling seamless VINS operation. The proposed\nLLT-based SRF is extensively verified through numerical studies, demonstrating\nsuperior numerical stability and achieving robust efficient performance on\n32-bit single-precision floats, operating at twice the speed of\nstate-of-the-art (SOTA) methods. Our initialization method, tested on both\nmobile workstations and Jetson Nano computers, achieving a high success rate of\ninitialization even within a 100 ms window under minimal conditions. Finally,\nthe proposed sqrtVINS is extensively validated across diverse scenarios,\ndemonstrating strong efficiency, robustness, and reliability. The full\nopen-source implementation is released to support future research and\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u60ef\u6027\u5bfc\u822a\u7cfb\u7edf\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u5f00\u6e90\u5b9e\u73b0\u3002", "motivation": "\u89e3\u51b3\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\u89c6\u89c9\u60ef\u6027\u5bfc\u822a\u7684\u8d44\u6e90\u7ea6\u675f\u548c\u6570\u503c\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eCholesky\u5206\u89e3\u7684\u5e73\u65b9\u6839\u6ee4\u6ce2\u66f4\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u5feb\u901f\u9c81\u68d2\u7684\u52a8\u6001\u521d\u59cb\u5316\u7b56\u7565\u3002", "result": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e73\u65b9\u6839\u6ee4\u6ce2\u5668\u7684\u89c6\u89c9\u60ef\u6027\u5bfc\u822a\u7cfb\u7edf\uff08sqrtVINS\uff09\uff0c\u5177\u6709\u8d85\u5feb\u7684\u901f\u5ea6\u548c\u6570\u5b57\u7a33\u5b9a\u6027\uff0c\u80fd\u591f\u5728\u6781\u7aef\u6761\u4ef6\u4e0b\u52a8\u6001\u521d\u59cb\u5316\u3002", "conclusion": "sqrtVINS\u5728\u591a\u79cd\u573a\u666f\u4e0b\u5c55\u73b0\u4e86\u5353\u8d8a\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\uff0c\u5bf9\u4e8e\u672a\u6765\u7684\u7814\u7a76\u4e0e\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.10805", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10805", "abs": "https://arxiv.org/abs/2510.10805", "authors": ["Soraya S. Anvari", "Rina R. Wehbe"], "title": "Therapeutic AI and the Hidden Risks of Over-Disclosure: An Embedded AI-Literacy Framework for Mental Health Privacy", "comment": "Accepted to SMASH 2025", "summary": "Large Language Models (LLMs) are increasingly deployed in mental health\ncontexts, from structured therapeutic support tools to informal chat-based\nwell-being assistants. While these systems increase accessibility, scalability,\nand personalization, their integration into mental health care brings privacy\nand safety challenges that have not been well-examined. Unlike traditional\nclinical interactions, LLM-mediated therapy often lacks a clear structure for\nwhat information is collected, how it is processed, and how it is stored or\nreused. Users without clinical guidance may over-disclose personal information,\nwhich is sometimes irrelevant to their presenting concern, due to misplaced\ntrust, lack of awareness of data risks, or the conversational design of the\nsystem. This overexposure raises privacy concerns and also increases the\npotential for LLM bias, misinterpretation, and long-term data misuse. We\npropose a framework embedding Artificial Intelligence (AI) literacy\ninterventions directly into mental health conversational systems, and outline a\nstudy plan to evaluate their impact on disclosure safety, trust, and user\nexperience.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5fc3\u7406\u5065\u5eb7\u5e94\u7528\u867d\u63d0\u9ad8\u4e86\u53ef\u8fbe\u6027\uff0c\u4f46\u4e5f\u5e26\u6765\u4e86\u9690\u79c1\u548c\u5b89\u5168\u6311\u6218\uff0c\u9700\u901a\u8fc7AI\u7d20\u517b\u5e72\u9884\u6765\u589e\u5f3a\u7528\u6237\u4fe1\u4efb\u548c\u4fdd\u969c\u4fe1\u606f\u62ab\u9732\u5b89\u5168\u3002", "motivation": "\u786e\u5b9a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5fc3\u7406\u5065\u5eb7\u5e94\u7528\u4e2d\u7684\u9690\u79c1\u548c\u5b89\u5168\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u7f3a\u4e4f\u4e34\u5e8a\u6307\u5bfc\u7684\u60c5\u51b5\u4e0b\u7528\u6237\u53ef\u80fd\u8fc7\u5ea6\u62ab\u9732\u4e2a\u4eba\u4fe1\u606f\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u8ba1\u5212\u901a\u8fc7\u7814\u7a76\u8bc4\u4f30AI\u7d20\u517b\u5e72\u9884\u5bf9\u7528\u6237\u5728\u5fc3\u7406\u5065\u5eb7\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u7684\u4f53\u9a8c\u5f71\u54cd\u3002", "result": "\u8bc6\u522b\u51faLLM\u5728\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u4f7f\u7528\u65f6\u7684\u9690\u79c1\u98ce\u9669\u3001\u6f5c\u5728\u504f\u89c1\u548c\u6570\u636e\u8bef\u7528\u95ee\u9898\uff0c\u5f3a\u8c03\u4e86\u52a0\u5f3a\u7528\u6237\u5bf9AI\u7684\u7406\u89e3\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u63d0\u51fa\u5728\u5fc3\u7406\u5065\u5eb7\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u5d4c\u5165\u4eba\u5de5\u667a\u80fd\u7d20\u517b\u5e72\u9884\u7684\u6846\u67b6\uff0c\u5e76\u8ba1\u5212\u8bc4\u4f30\u5176\u5bf9\u4fe1\u606f\u62ab\u9732\u5b89\u5168\u3001\u4fe1\u4efb\u53ca\u7528\u6237\u4f53\u9a8c\u7684\u5f71\u54cd\u3002"}}
{"id": "2510.10357", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10357", "abs": "https://arxiv.org/abs/2510.10357", "authors": ["Yang Liu", "Bruno Da Costa", "Aude Billard"], "title": "Learning to Throw-Flip", "comment": "Accepted to IROS 2025. Video Summary: https://youtu.be/txYc9b1oflU", "summary": "Dynamic manipulation, such as robot tossing or throwing objects, has recently\ngained attention as a novel paradigm to speed up logistic operations. However,\nthe focus has predominantly been on the object's landing location, irrespective\nof its final orientation. In this work, we present a method enabling a robot to\naccurately \"throw-flip\" objects to a desired landing pose (position and\norientation). Conventionally, objects thrown by revolute robots suffer from\nparasitic rotation, resulting in highly restricted and uncontrollable landing\nposes. Our approach is based on two key design choices: first, leveraging the\nimpulse-momentum principle, we design a family of throwing motions that\neffectively decouple the parasitic rotation, significantly expanding the\nfeasible set of landing poses. Second, we combine a physics-based model of free\nflight with regression-based learning methods to account for unmodeled effects.\nReal robot experiments demonstrate that our framework can learn to throw-flip\nobjects to a pose target within ($\\pm$5 cm, $\\pm$45 degrees) threshold in\ndozens of trials. Thanks to data assimilation, incorporating projectile\ndynamics reduces sample complexity by an average of 40% when throw-flipping to\nunseen poses compared to end-to-end learning methods. Additionally, we show\nthat past knowledge on in-hand object spinning can be effectively reused,\naccelerating learning by 70% when throwing a new object with a Center of Mass\n(CoM) shift. A video summarizing the proposed method and the hardware\nexperiments is available at https://youtu.be/txYc9b1oflU.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u51c6\u786e\u6295\u63b7\u7269\u4f53\u81f3\u7279\u5b9a\u843d\u5730\u59ff\u6001\u7684\u673a\u5668\u4eba\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u7269\u6d41\u64cd\u4f5c\u7684\u901f\u5ea6\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u52a8\u529b\u5b66\u64cd\u4f5c\u5728\u7269\u6d41\u4e2d\u6b63\u8d8a\u6765\u8d8a\u53d7\u5230\u91cd\u89c6\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7269\u4f53\u843d\u5730\u4f4d\u7f6e\uff0c\u800c\u5ffd\u89c6\u5176\u6700\u7ec8\u65b9\u5411\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u4e00\u7cfb\u5217\u629b\u63b7\u8fd0\u52a8\u4ee5\u6709\u6548\u89e3\u8026\u5bc4\u751f\u65cb\u8f6c\uff0c\u5e76\u7ed3\u5408\u7269\u7406\u6a21\u578b\u548c\u57fa\u4e8e\u56de\u5f52\u7684\u5b66\u4e60\u65b9\u6cd5\u6765\u8003\u8651\u672a\u5efa\u6a21\u5f71\u54cd\u3002", "result": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u51c6\u786e\u5730\u5c06\u7269\u4f53\"\u6295\u63b7\u7ffb\u8f6c\"\u5230\u671f\u671b\u7684\u843d\u5730\u59ff\u6001\uff08\u4f4d\u7f6e\u548c\u65b9\u5411\uff09\u3002", "conclusion": "\u901a\u8fc7\u91c7\u7528\u51b2\u51fb\u52a8\u91cf\u539f\u5219\u548c\u7269\u7406\u6a21\u578b\u7ed3\u5408\u5b66\u4e60\u65b9\u6cd5\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u7684\u6295\u63b7\u7cbe\u5ea6\u5e76\u51cf\u5c11\u4e86\u6837\u672c\u590d\u6742\u6027\u3002"}}
{"id": "2510.11035", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.11035", "abs": "https://arxiv.org/abs/2510.11035", "authors": ["Longjie Guo", "Chenjie Yuan", "Mingyuan Zhong", "Robert Wolfe", "Ruican Zhong", "Yue Xu", "Bingbing Wen", "Hua Shen", "Lucy Lu Wang", "Alexis Hiniker"], "title": "SusBench: An Online Benchmark for Evaluating Dark Pattern Susceptibility of Computer-Use Agents", "comment": null, "summary": "As LLM-based computer-use agents (CUAs) begin to autonomously interact with\nreal-world interfaces, understanding their vulnerability to manipulative\ninterface designs becomes increasingly critical. We introduce SusBench, an\nonline benchmark for evaluating the susceptibility of CUAs to UI dark patterns,\ndesigns that aim to manipulate or deceive users into taking unintentional\nactions. Drawing nine common dark pattern types from existing taxonomies, we\ndeveloped a method for constructing believable dark patterns on real-world\nconsumer websites through code injections, and designed 313 evaluation tasks\nacross 55 websites. Our study with 29 participants showed that humans perceived\nour dark pattern injections to be highly realistic, with the vast majority of\nparticipants not noticing that these had been injected by the research team. We\nevaluated five state-of-the-art CUAs on the benchmark. We found that both human\nparticipants and agents are particularly susceptible to the dark patterns of\nPreselection, Trick Wording, and Hidden Information, while being resilient to\nother overt dark patterns. Our findings inform the development of more\ntrustworthy CUAs, their use as potential human proxies in evaluating deceptive\ndesigns, and the regulation of an online environment increasingly navigated by\nautonomous agents.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86SusBench\uff0c\u4e00\u4e2a\u8bc4\u4f30\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u5bf9\u7528\u6237\u754c\u9762\u9ed1\u6697\u6a21\u5f0f\u8106\u5f31\u6027\u7684\u5728\u7ebf\u57fa\u51c6\uff0c\u53d1\u73b0\u53c2\u4e0e\u8005\u548c\u4ee3\u7406\u5bf9\u67d0\u4e9b\u9ed1\u6697\u6a21\u5f0f\u7279\u522b\u654f\u611f\uff0c\u7814\u7a76\u7ed3\u679c\u5bf9\u5f00\u53d1\u53ef\u4fe1\u7684CUAs\u548c\u5728\u7ebf\u73af\u5883\u7684\u76d1\u7ba1\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "motivation": "\u968f\u7740\u57fa\u4e8eLLM\u7684\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u8d8a\u6765\u8d8a\u591a\u5730\u4e0e\u73b0\u5b9e\u4e16\u754c\u754c\u9762\u8fdb\u884c\u81ea\u4e3b\u4ea4\u4e92\uff0c\u7406\u89e3\u5b83\u4eec\u5bf9\u64cd\u63a7\u6027\u754c\u9762\u8bbe\u8ba1\u7684\u8106\u5f31\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4ee3\u7801\u6ce8\u5165\u7684\u65b9\u6cd5\uff0c\u5728\u771f\u5b9e\u6d88\u8d39\u8005\u7f51\u7ad9\u4e0a\u6784\u5efa\u4e86313\u4e2a\u9ed1\u6697\u6a21\u5f0f\u8bc4\u4f30\u4efb\u52a1\uff0c\u5e76\u4f7f\u752829\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u4e86\u7814\u7a76\u3002", "result": "\u672c\u7814\u7a76\u5f15\u5165\u4e86SusBench\uff0c\u4e00\u4e2a\u5728\u7ebf\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\uff08CUAs\uff09\u5728\u9762\u5bf9\u7528\u6237\u754c\u9762\uff08UI\uff09\u9ed1\u6697\u6a21\u5f0f\u65f6\u7684\u8106\u5f31\u6027\u3002\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ee3\u7801\u6ce8\u5165\u5728\u771f\u5b9e\u4e16\u754c\u7684\u6d88\u8d39\u8005\u7f51\u7ad9\u4e0a\u6784\u5efa\u53ef\u4fe1\u7684\u9ed1\u6697\u6a21\u5f0f\uff0c\u5e76\u572855\u4e2a\u7f51\u7ad9\u4e0a\u8bbe\u8ba1\u4e86313\u4e2a\u8bc4\u4f30\u4efb\u52a1\u3002\u53c2\u4e0e\u8005\u7684\u7814\u7a76\u663e\u793a\uff0c\u4eba\u7c7b\u5bf9\u6211\u4eec\u7684\u9ed1\u6697\u6a21\u5f0f\u6ce8\u5165\u7684\u611f\u77e5\u9ad8\u5ea6\u771f\u5b9e\uff0c\u5927\u591a\u6570\u53c2\u4e0e\u8005\u6ca1\u6709\u610f\u8bc6\u5230\u8fd9\u4e9b\u6a21\u5f0f\u662f\u7814\u7a76\u56e2\u961f\u6ce8\u5165\u7684\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u4e94\u79cd\u6700\u5148\u8fdb\u7684CUAs\uff0c\u5e76\u53d1\u73b0\u4eba\u5de5\u53c2\u4e0e\u8005\u548c\u4ee3\u7406\u7279\u522b\u5bb9\u6613\u53d7\u5230\u9884\u9009\u3001\u6280\u5de7\u7528\u8bcd\u548c\u9690\u6027\u4fe1\u606f\u8fd9\u4e9b\u9ed1\u6697\u6a21\u5f0f\u7684\u5f71\u54cd\uff0c\u800c\u5bf9\u5176\u4ed6\u660e\u663e\u7684\u9ed1\u6697\u6a21\u5f0f\u5219\u8868\u73b0\u51fa\u8f83\u5f3a\u7684\u62b5\u6297\u529b\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u66f4\u53ef\u4fe1\u7684\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u89c1\u89e3\uff0c\u5e76\u5bf9\u5982\u4f55\u8bc4\u4f30\u548c\u89c4\u5236\u65e5\u76ca\u7531\u81ea\u4e3b\u4ee3\u7406\u5bfc\u822a\u7684\u5728\u7ebf\u73af\u5883\u63d0\u51fa\u4e86\u5efa\u8bae\u3002"}}
{"id": "2510.10379", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.10379", "abs": "https://arxiv.org/abs/2510.10379", "authors": ["Rohan Gupta", "Trevor Asbery", "Zain Merchant", "Abrar Anwar", "Jesse Thomason"], "title": "RobotFleet: An Open-Source Framework for Centralized Multi-Robot Task Planning", "comment": null, "summary": "Coordinating heterogeneous robot fleets to achieve multiple goals is\nchallenging in multi-robot systems. We introduce an open-source and extensible\nframework for centralized multi-robot task planning and scheduling that\nleverages LLMs to enable fleets of heterogeneous robots to accomplish multiple\ntasks. RobotFleet provides abstractions for planning, scheduling, and execution\nacross robots deployed as containerized services to simplify fleet scaling and\nmanagement. The framework maintains a shared declarative world state and\ntwo-way communication for task execution and replanning. By modularizing each\nlayer of the autonomy stack and using LLMs for open-world reasoning, RobotFleet\nlowers the barrier to building scalable multi-robot systems. The code can be\nfound here: https://github.com/therohangupta/robot-fleet.", "AI": {"tldr": "RobotFleet\u662f\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u96c6\u4e2d\u5f0f\u591a\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u548c\u8c03\u5ea6\u6765\u7b80\u5316\u5f02\u6784\u673a\u5668\u4eba\u8f66\u961f\u7684\u7ba1\u7406\uff0c\u5b9e\u73b0\u591a\u4e2a\u4efb\u52a1\u3002", "motivation": "\u5728\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u534f\u8c03\u5f02\u6784\u673a\u5668\u4eba\u8f66\u961f\u4ee5\u5b9e\u73b0\u591a\u4e2a\u76ee\u6807\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u901a\u8fc7\u6a21\u5757\u5316\u81ea\u6cbb\u5806\u6808\u7684\u5404\u4e2a\u5c42\uff0c\u5e76\u5229\u7528\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5f00\u653e\u4e16\u754c\u63a8\u7406\uff0c\u652f\u6301\u4efb\u52a1\u6267\u884c\u548c\u91cd\u65b0\u89c4\u5212\u3002", "result": "RobotFleet\u964d\u4f4e\u4e86\u6784\u5efa\u53ef\u6269\u5c55\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u95e8\u69db\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u5957\u5f00\u6e90\u53ef\u6269\u5c55\u6846\u67b6\u3002", "conclusion": "RobotFleet\u4e3a\u5f02\u6784\u673a\u5668\u4eba\u8f66\u961f\u4efb\u52a1\u89c4\u5212\u548c\u8c03\u5ea6\u63d0\u4f9b\u4e86\u4e00\u79cd\u96c6\u4e2d\u5f0f\u6846\u67b6\uff0c\u7b80\u5316\u4e86\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u7ba1\u7406\u548c\u6269\u5c55\u3002"}}
{"id": "2510.11087", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.11087", "abs": "https://arxiv.org/abs/2510.11087", "authors": ["Harin Yoon", "Dongwhan Kim", "Changhoon Oh", "Soojin Jun"], "title": "UXer-AI Collaboration Process for Enhancing Trust", "comment": "18 pages, 7 figures, 6 tables", "summary": "In recent years, discussions on integrating Artificial Intelligence (AI) into\nUX design have intensified. However, the practical application of AI tools in\ndesign is limited by their operation within overly simplified scenarios,\ninherent complexity and unpredictability, and a general lack of relevant\neducation. This study proposes an effective UXer-AI collaboration process to\naddress these issues and seeks to identify efficient AI collaboration\nstrategies through a series of user studies. In a preliminary study, two\nparticipatory design workshops identified major barriers to UXer-AI\ncollaboration, including unfamiliarity with AI, inadequate internal support,\nand trust issues. To address the particularly critical issue of diminished\ntrust, this study developed a new AI prototype model, TW-AI, that incorporates\nverification and decision-making processes to enhance trust and operational\nefficiency in UX design tasks. Task performance experiments and in-depth\ninterviews evaluated the TW-AI model, revealing significant improvements in\npractitioners' trust, work efficiency, understanding of usage timing, and\ncontrollability. The \"Source\" function, based on Retrieval-Augmented Generation\n(RAG) technology, notably enhanced the reliability of the AI tool. Participants\nnoted improved communication efficiency and reduced decision-making time,\nattributing these outcomes to the model's comprehensive verification features\nand streamlined approach to complex verification tasks. This study advances\nUXer-AI collaboration by providing key insights, bridging research and practice\nwith actionable strategies, and establishing guidelines for AI tool designs\ntailored to UX. It contributes to the HCI community by outlining a scalable\nUXer-AI collaboration framework that addresses immediate operational challenges\nand lays the foundation for future advancements in AI-driven UX methodologies.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86TW-AI\u6a21\u578b\uff0c\u4ee5\u63d0\u5347UX\u8bbe\u8ba1\u4e2d\u7684AI\u5408\u4f5c\u6548\u7387\uff0c\u901a\u8fc7\u589e\u5f3a\u7684\u9a8c\u8bc1\u63aa\u65bd\u89e3\u51b3\u4fe1\u4efb\u95ee\u9898\uff0c\u5e76\u5236\u5b9a\u76f8\u5173\u7684\u8bbe\u8ba1\u6307\u5357\u548c\u6846\u67b6\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5f53\u524dAI\u5728UX\u8bbe\u8ba1\u4e2d\u5e94\u7528\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u4fe1\u4efb\u548c\u652f\u6301\u7684\u7f3a\u5931\uff0c\u901a\u8fc7\u6709\u6548\u7684UX\u8005\u4e0eAI\u5408\u4f5c\u7b56\u7565\u63d0\u5347\u8bbe\u8ba1\u6548\u7387\u3002", "method": "\u901a\u8fc7\u7528\u6237\u7814\u7a76\u548c\u53c2\u4e0e\u5f0f\u8bbe\u8ba1\u5de5\u4f5c\u574a\uff0c\u8bc6\u522bUX\u8005\u4e0eAI\u5408\u4f5c\u7684\u4e3b\u8981\u969c\u788d\uff0c\u5e76\u5f00\u53d1\u4e86TW-AI\u6a21\u578b\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "TW-AI\u6a21\u578b\u5728\u4efb\u52a1\u8868\u73b0\u3001\u4fe1\u4efb\u5ea6\u3001\u5de5\u4f5c\u6548\u7387\u548c\u51b3\u7b56\u65f6\u95f4\u4e0a\u90fd\u6709\u663e\u8457\u63d0\u5347\uff0c\u5c24\u5176\u662f\u6e90\u529f\u80fd\u901a\u8fc7RAG\u6280\u672f\u589e\u5f3a\u4e86AI\u5de5\u5177\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684UX\u8005\u4e0eAI\u534f\u4f5c\u7684\u65b0\u6a21\u578bTW-AI\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u7684\u4fe1\u4efb\u5ea6\u548c\u5de5\u4f5c\u6548\u7387\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u51b3\u7b56\u548c\u63a7\u5236\u80fd\u529b\u3002"}}
{"id": "2510.10392", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.10392", "abs": "https://arxiv.org/abs/2510.10392", "authors": ["Max Sokolich", "Yanda Yang", "Subrahmanyam Cherukumilli", "Fatma Ceren Kirmizitas", "Sambeeta Das"], "title": "MicroRoboScope: A Portable and Integrated Mechatronic Platform for Magnetic and Acoustic Microrobotic Experimentation", "comment": null, "summary": "This paper presents MicroRoboScope, a portable, compact, and versatile\nmicrorobotic experimentation platform designed for real-time, closed-loop\ncontrol of both magnetic and acoustic microrobots. The system integrates an\nembedded computer, microscope, power supplies, and control circuitry into a\nsingle, low-cost and fully integrated apparatus. Custom control software\ndeveloped in Python and Arduino C++ handles live video acquisition, microrobot\ntracking, and generation of control signals for electromagnetic coils and\nacoustic transducers. The platform's multi-modal actuation, accessibility, and\nportability make it suitable not only for specialized research laboratories but\nalso for educational and outreach settings. By lowering the barrier to entry\nfor microrobotic experimentation, this system enables new opportunities for\nresearch, education, and translational applications in biomedicine, tissue\nengineering, and robotics.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aMicroRoboScope\u7684\u4fbf\u643a\u5f0f\u5fae\u578b\u673a\u5668\u4eba\u5b9e\u9a8c\u5e73\u53f0\uff0c\u8be5\u5e73\u53f0\u96c6\u6210\u4e86\u591a\u79cd\u8bbe\u5907\u548c\u5b9a\u5236\u8f6f\u4ef6\uff0c\u65e8\u5728\u964d\u4f4e\u5fae\u578b\u673a\u5668\u4eba\u5b9e\u9a8c\u7684\u5165\u95e8\u95e8\u69db\uff0c\u62d3\u5c55\u5176\u5728\u591a\u4e2a\u9886\u57df\u7684\u5e94\u7528\u3002", "motivation": "\u7814\u7a76\u7684\u52a8\u673a\u662f\u8bbe\u8ba1\u4e00\u79cd\u4fbf\u643a\u3001\u7d27\u51d1\u3001\u591a\u529f\u80fd\u7684\u5fae\u578b\u673a\u5668\u4eba\u5b9e\u9a8c\u5e73\u53f0\uff0c\u65e8\u5728\u5b9e\u73b0\u5b9e\u65f6\u95ed\u73af\u63a7\u5236\uff0c\u4ee5\u4fc3\u8fdb\u5fae\u578b\u673a\u5668\u4eba\u5b9e\u9a8c\u5728\u5404\u4e2a\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u96c6\u6210\u5d4c\u5165\u5f0f\u8ba1\u7b97\u673a\u3001\u663e\u5fae\u955c\u3001\u7535\u6e90\u548c\u63a7\u5236\u7535\u8def\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u7684\u5fae\u578b\u673a\u5668\u4eba\u5b9e\u9a8c\u5e73\u53f0\uff0c\u4f7f\u7528Python\u548cArduino C++\u7f16\u5199\u5b9a\u5236\u63a7\u5236\u8f6f\u4ef6\uff0c\u5904\u7406\u89c6\u9891\u91c7\u96c6\u3001\u673a\u5668\u4eba\u8ddf\u8e2a\u53ca\u63a7\u5236\u4fe1\u53f7\u751f\u6210\u3002", "result": "\u8be5\u5e73\u53f0\u7684\u591a\u6a21\u6001\u9a71\u52a8\u7279\u70b9\u4f7f\u5176\u4e0d\u4ec5\u9002\u5408\u4e13\u4e1a\u7814\u7a76\u5b9e\u9a8c\u5ba4\uff0c\u540c\u65f6\u4e5f\u9002\u7528\u4e8e\u6559\u80b2\u548c\u63a8\u5e7f\u73af\u5883\u3002", "conclusion": "MicroRoboScope\u7cfb\u7edf\u964d\u4f4e\u4e86\u5fae\u578b\u673a\u5668\u4eba\u5b9e\u9a8c\u7684\u5165\u95e8\u95e8\u69db\uff0c\u4e3a\u751f\u7269\u533b\u5b66\u3001\u7ec4\u7ec7\u5de5\u7a0b\u548c\u673a\u5668\u4eba\u7b49\u9886\u57df\u7684\u7814\u7a76\u3001\u6559\u80b2\u548c\u8f6c\u5316\u5e94\u7528\u521b\u9020\u4e86\u65b0\u673a\u4f1a\u3002"}}
{"id": "2510.11185", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.11185", "abs": "https://arxiv.org/abs/2510.11185", "authors": ["Yaman Yu", "Mohi", "Aishi Debroy", "Xin Cao", "Karen Rudolph", "Yang Wang"], "title": "Principles of Safe AI Companions for Youth: Parent and Expert Perspectives", "comment": null, "summary": "AI companions are increasingly popular among teenagers, yet current platforms\nlack safeguards to address developmental risks and harmful normalization.\nDespite growing concerns, little is known about how parents and developmental\npsychology experts assess these interactions or what protections they consider\nnecessary. We conducted 26 semi structured interviews with parents and experts,\nwho reviewed real world youth GenAI companion conversation snippets. We found\nthat stakeholders assessed risks contextually, attending to factors such as\nyouth maturity, AI character age, and how AI characters modeled values and\nnorms. We also identified distinct logics of assessment: parents flagged single\nevents, such as a mention of suicide or flirtation, as high risk, whereas\nexperts looked for patterns over time, such as repeated references to self harm\nor sustained dependence. Both groups proposed interventions, with parents\nfavoring broader oversight and experts preferring cautious, crisis-only\nescalation paired with youth facing safeguards. These findings provide\ndirections for embedding safety into AI companion design.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u7236\u6bcd\u548c\u4e13\u5bb6\u5bf9\u9752\u5c11\u5e74\u4e0eAI\u4f34\u4fa3\u4e92\u52a8\u7684\u8bc4\u4f30\uff0c\u5f3a\u8c03\u98ce\u9669\u56e0\u7d20\u548c\u4ecb\u5165\u5efa\u8bae\u3002", "motivation": "\u968f\u7740AI\u4f34\u4fa3\u5728\u9752\u5c11\u5e74\u4e2d\u65e5\u76ca\u6d41\u884c\uff0c\u4e9f\u9700\u4e86\u89e3\u5bb6\u957f\u548c\u4e13\u5bb6\u5982\u4f55\u770b\u5f85\u8fd9\u4e9b\u4e92\u52a8\u53ca\u6240\u9700\u7684\u4fdd\u62a4\u63aa\u65bd\u3002", "method": "\u901a\u8fc7\u5bf926\u4e2a\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u7684\u5206\u6790\uff0c\u7814\u7a76\u4e86\u7236\u6bcd\u548c\u4e13\u5bb6\u5bf9\u9752\u5c11\u5e74\u4e0eGenAI\u4f34\u4fa3\u5bf9\u8bdd\u7684\u8bc4\u4f30\u548c\u5efa\u8bae\u3002", "result": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u7236\u6bcd\u548c\u53d1\u5c55\u5fc3\u7406\u5b66\u4e13\u5bb6\u5728\u8bc4\u4f30\u9752\u5c11\u5e74\u4e0eAI\u4f34\u4fa3\u4e92\u52a8\u65f6\u7684\u4e0d\u540c\u5173\u6ce8\u70b9\u548c\u5efa\u8bae\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3aAI\u4f34\u4fa3\u8bbe\u8ba1\u4e2d\u7684\u5b89\u5168\u5d4c\u5165\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u5f3a\u8c03\u4e86\u7236\u6bcd\u548c\u4e13\u5bb6\u5728\u98ce\u9669\u8bc4\u4f30\u548c\u4ecb\u5165\u65b9\u9762\u7684\u4e0d\u540c\u770b\u6cd5\u3002"}}
{"id": "2510.10421", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10421", "abs": "https://arxiv.org/abs/2510.10421", "authors": ["Junbin Yuan", "Brady Moon", "Muqing Cao", "Sebastian Scherer"], "title": "Hierarchical Planning for Long-Horizon Multi-Target Tracking Under Target Motion Uncertainty", "comment": "8 pages, 7 figures. Accepted to IEEE Robotics and Automation Letters\n  (RAL), 2025", "summary": "Achieving persistent tracking of multiple dynamic targets over a large\nspatial area poses significant challenges for a single-robot system with\nconstrained sensing capabilities. As the robot moves to track different\ntargets, the ones outside the field of view accumulate uncertainty, making them\nprogressively harder to track. An effective path planning algorithm must manage\nuncertainty over a long horizon and account for the risk of permanently losing\ntrack of targets that remain unseen for too long. However, most existing\napproaches rely on short planning horizons and assume small, bounded\nenvironments, resulting in poor tracking performance and target loss in\nlarge-scale scenarios. In this paper, we present a hierarchical planner for\ntracking multiple moving targets with an aerial vehicle. To address the\nchallenge of tracking non-static targets, our method incorporates motion models\nand uncertainty propagation during path execution, allowing for more informed\ndecision-making. We decompose the multi-target tracking task into sub-tasks of\nsingle target search and detection, and our proposed pipeline consists a novel\nlow-level coverage planner that enables searching for a target in an evolving\nbelief area, and an estimation method to assess the likelihood of success for\neach sub-task, making it possible to convert the active target tracking task to\na Markov decision process (MDP) that we solve with a tree-based algorithm to\ndetermine the sequence of sub-tasks. We validate our approach in simulation,\ndemonstrating its effectiveness compared to existing planners for active target\ntracking tasks, and our proposed planner outperforms existing approaches,\nachieving a reduction of 11-70% in final uncertainty across different\nenvironments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\uff0c\u4ee5\u6539\u5584\u65e0\u4eba\u673a\u5728\u5927\u8303\u56f4\u5185\u8ffd\u8e2a\u591a\u4e2a\u52a8\u6001\u76ee\u6807\u7684\u80fd\u529b\uff0c\u514b\u670d\u4e86\u73b0\u6709\u7b97\u6cd5\u5728\u957f\u65f6\u95f4\u548c\u5927\u8303\u56f4\u73af\u5883\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u5355\u673a\u5668\u4eba\u7cfb\u7edf\u9762\u4e34\u5728\u5927\u7684\u7a7a\u95f4\u533a\u57df\u5185\u6301\u4e45\u8ffd\u8e2a\u591a\u4e2a\u52a8\u6001\u76ee\u6807\u7684\u6311\u6218\uff0c\u5c24\u5176\u5728\u76ee\u6807\u8d85\u51fa\u89c6\u91ce\u65f6\uff0c\u8ddf\u8e2a\u53d8\u5f97\u66f4\u52a0\u56f0\u96be\u3002", "method": "\u672c\u65b9\u6cd5\u6574\u5408\u8fd0\u52a8\u6a21\u578b\u4e0e\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\uff0c\u5229\u7528\u4f4e\u7ea7\u8986\u76d6\u89c4\u5212\u5668\u4e0e\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5c06\u591a\u76ee\u6807\u8ddf\u8e2a\u4efb\u52a1\u8f6c\u5316\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u6811\u7684\u7b97\u6cd5\u6c42\u89e3\u5b50\u4efb\u52a1\u5e8f\u5217\u3002", "result": "\u8be5\u7b97\u6cd5\u5728\u6a21\u62df\u9a8c\u8bc1\u4e2d\u8868\u73b0\u51fa\u6bd4\u73b0\u6709\u89c4\u5212\u7b97\u6cd5\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u6709\u6548\u964d\u4f4e\u4e86\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "\u901a\u8fc7\u5728\u6a21\u62df\u4e2d\u9a8c\u8bc1\uff0c\u6240\u63d0\u51fa\u7684\u89c4\u5212\u7b97\u6cd5\u5728\u4e3b\u52a8\u76ee\u6807\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u76f8\u8f83\u4e8e\u73b0\u6709\u65b9\u6cd5\u51cf\u5c11\u4e8611-70%\u7684\u6700\u7ec8\u4e0d\u786e\u5b9a\u6027\u3002"}}
{"id": "2510.11264", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.11264", "abs": "https://arxiv.org/abs/2510.11264", "authors": ["Conglin Ma", "Jiatong Li", "Sen-Zhe Xu", "Ju Dai", "Jie Liu", "Feng Zhou"], "title": "Learning Hanzi Character Through VR-Based Mortise-Tenon", "comment": null, "summary": "This paper introduces a novel VR-based system that redefines the acquisition\nof Hanzi character literacy by integrating traditional mortise-tenon joinery\nprinciples (HVRMT).Addressing the challenge of abstract character memorization\nin digital learning,our system deconstructs Hanzi components into interactive\n\"structural radicals\"akin to wooden joint modules.Leveraging PICO's 6DoF\nspatial tracking and LLM's morphological analysis,learners assemble stroke\nsequences with haptic feedback simulating wood-to-wood friction.Our system also\nsupports multiplayer online experiences, enhancing engagement and memory\nretention while preserving intangible cultural heritage. This innovative\napproach not only enhances engagement and memory retention but also\nreconstructs the craft wisdom embedded in Chinese writing systems, offering new\npathways for preserving intangible cultural heritage in digital ecosystems.For\nthe demo,please refer to this link{https://youtu.be/oUwfFTRpFyo}.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u865a\u62df\u73b0\u5b9e\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u6c49\u5b57\u7684\u5b66\u4e60\u4e0e\u4f20\u7edf\u5de5\u827a\u76f8\u7ed3\u5408\uff0c\u63d0\u5347\u4e86\u5bf9\u6c49\u5b57\u7684\u7406\u89e3\u548c\u8bb0\u5fc6\u4fdd\u6301\uff0c\u540c\u65f6\u6709\u52a9\u4e8e\u65e0\u5f62\u6587\u5316\u9057\u4ea7\u7684\u4fdd\u62a4\u3002", "motivation": "\u89e3\u51b3\u6570\u5b57\u5b66\u4e60\u4e2d\u6c49\u5b57\u5b57\u7b26\u8bb0\u5fc6\u62bd\u8c61\u5316\u7684\u6311\u6218\uff0c\u901a\u8fc7\u4e92\u52a8\u6027\u6765\u52a0\u6df1\u5b66\u4e60\u8005\u7684\u7406\u89e3\u548c\u8bb0\u5fc6\u3002", "method": "\u91c7\u7528VR\u6280\u672f\u7ed3\u5408\u4f20\u7edf\u7684\u69ab\u536f\u7ed3\u6784\uff0c\u4ee5\u4ea4\u4e92\u5f0f\u7684\u65b9\u5f0f\u8ba9\u5b66\u4e60\u8005\u7ec4\u88c5\u6c49\u5b57\u7684\u7b14\u753b\u5e8f\u5217\uff0c\u5229\u75286DoF\u7a7a\u95f4\u8ddf\u8e2a\u548c\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5f62\u6001\u5206\u6790\u3002", "result": "\u589e\u5f3a\u4e86\u5b66\u4e60\u8005\u7684\u53c2\u4e0e\u611f\u548c\u8bb0\u5fc6\u4fdd\u6301\u7387\uff0c\u540c\u65f6\u4fdd\u62a4\u4e86\u65e0\u5f62\u6587\u5316\u9057\u4ea7\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u521b\u65b0\u7684VR\u4f53\u9a8c\u4e0e\u4f20\u7edf\u5de5\u827a\u76f8\u7ed3\u5408\uff0c\u589e\u5f3a\u4e86\u5b66\u4e60\u8005\u5bf9\u6c49\u5b57\u6784\u9020\u7684\u7406\u89e3\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u8bb0\u5fc6\u4fdd\u6301\u7387\uff0c\u5e76\u4fc3\u8fdb\u4e86\u5bf9\u6587\u5316\u9057\u4ea7\u7684\u4f20\u627f\u3002"}}
{"id": "2510.10455", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.10455", "abs": "https://arxiv.org/abs/2510.10455", "authors": ["Jiayu Ding", "Xulin Chen", "Garrett E. Katz", "Zhenyu Gan"], "title": "Towards Dynamic Quadrupedal Gaits: A Symmetry-Guided RL Hierarchy Enables Free Gait Transitions at Varying Speeds", "comment": null, "summary": "Quadrupedal robots exhibit a wide range of viable gaits, but generating\nspecific footfall sequences often requires laborious expert tuning of numerous\nvariables, such as touch-down and lift-off events and holonomic constraints for\neach leg. This paper presents a unified reinforcement learning framework for\ngenerating versatile quadrupedal gaits by leveraging the intrinsic symmetries\nand velocity-period relationship of dynamic legged systems. We propose a\nsymmetry-guided reward function design that incorporates temporal,\nmorphological, and time-reversal symmetries. By focusing on preserved\nsymmetries and natural dynamics, our approach eliminates the need for\npredefined trajectories, enabling smooth transitions between diverse locomotion\npatterns such as trotting, bounding, half-bounding, and galloping. Implemented\non the Unitree Go2 robot, our method demonstrates robust performance across a\nrange of speeds in both simulations and hardware tests, significantly improving\ngait adaptability without extensive reward tuning or explicit foot placement\ncontrol. This work provides insights into dynamic locomotion strategies and\nunderscores the crucial role of symmetries in robotic gait design.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u5bf9\u79f0\u6027\u63d0\u9ad8\u4e86\u56db\u8db3\u673a\u5668\u4eba\u7684\u6b65\u6001\u9002\u5e94\u6027\uff0c\u7b80\u5316\u4e86\u6b65\u6001\u751f\u6210\u8fc7\u7a0b\u3002", "motivation": "\u4f20\u7edf\u7684\u6b65\u6001\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u4e13\u4e1a\u8c03\u4f18\u591a\u4e2a\u53d8\u91cf\uff0c\u8fc7\u7a0b\u7e41\u7410\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u751f\u6210\u56db\u8db3\u673a\u5668\u4eba\u7684\u591a\u6837\u5316\u6b65\u6001\u3002", "method": "\u5229\u7528\u5185\u5728\u5bf9\u79f0\u6027\u548c\u52a8\u6001\u817f\u90e8\u7cfb\u7edf\u7684\u901f\u5ea6-\u5468\u671f\u5173\u7cfb\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u79f0\u6027\u5bfc\u5411\u7684\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\uff0c\u4e13\u6ce8\u4e8e\u4fdd\u6301\u5bf9\u79f0\u6027\u548c\u81ea\u7136\u52a8\u6001\u3002", "result": "\u5728Unitree Go2\u673a\u5668\u4eba\u4e0a\u5b9e\u65bd\u540e\uff0c\u65b9\u6cd5\u5728\u6a21\u62df\u548c\u786c\u4ef6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u7a33\u5065\uff0c\u4e30\u5bcc\u4e86\u6b65\u6001\u8f6c\u6362\uff0c\u4e14\u65e0\u9700\u5e7f\u6cdb\u7684\u5956\u52b1\u8c03\u4f18\u6216\u660e\u786e\u7684\u8db3\u90e8\u4f4d\u7f6e\u63a7\u5236\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5728\u4e0d\u540c\u901f\u5ea6\u4e0b\u7684\u6b65\u6001\u9002\u5e94\u80fd\u529b\uff0c\u5f3a\u8c03\u4e86\u5bf9\u79f0\u6027\u5728\u673a\u5668\u4eba\u6b65\u6001\u8bbe\u8ba1\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.11280", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.11280", "abs": "https://arxiv.org/abs/2510.11280", "authors": ["Patricia Piedade"], "title": "Proceedings of the Access InContext Workshop @ CHI'25 Conference on Human Factors in Computing Systems", "comment": null, "summary": "This is the Proceedings of the Access InContext Workshop, which was held at\nthe CHI'25 Conference on Human Factors in Computing Systems, in Yokohama,\nJapan, on April 26th 2025.", "AI": {"tldr": "\u672c\u6587\u8bb0\u5f55\u4e862025\u5e74CHI'25\u4f1a\u8bae\u4e0aAccess InContext\u7814\u8ba8\u4f1a\u7684\u5185\u5bb9\u3002", "motivation": "\u672c\u7814\u8ba8\u4f1a\u65e8\u5728\u63a2\u8ba8\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u53ef\u83b7\u53d6\u6027\u95ee\u9898\uff0c\u4fc3\u8fdb\u5b66\u672f\u754c\u548c\u4ea7\u4e1a\u754c\u7684\u6df1\u5165\u4ea4\u6d41\u3002", "method": "\u4f1a\u8bae\u8bb0\u5f55\u603b\u7ed3\u4e86\u4e0e\u4f1a\u8005\u7684\u53d1\u8a00\u548c\u8ba8\u8bba\uff0c\u6db5\u76d6\u4e86\u591a\u79cd\u76f8\u5173\u8bae\u9898\u3002", "result": "\u8fd9\u7bc7\u8bba\u6587\u662f\u5173\u4e8e2025\u5e74\u5728\u65e5\u672c\u6a2a\u6ee8\u4e3e\u884c\u7684CHI'25\u4eba\u673a\u4ea4\u4e92\u9886\u57df\u4f1a\u8bae\u7684Access InContext\u7814\u8ba8\u4f1a\u7684\u4f1a\u8bae\u8bb0\u5f55\u3002", "conclusion": "\u8be5\u7814\u8ba8\u4f1a\u4e3a\u4eba\u673a\u4ea4\u4e92\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u4e3b\u9898\uff0c\u4fc3\u8fdb\u4e86\u76f8\u5173\u9886\u57df\u7684\u4ea4\u6d41\u4e0e\u5408\u4f5c\u3002"}}
{"id": "2510.10468", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.10468", "abs": "https://arxiv.org/abs/2510.10468", "authors": ["Robert Mahony", "Jonathan Kelly", "Stephan Weiss"], "title": "Galilean Symmetry in Robotics", "comment": "Under Review", "summary": "Galilean symmetry is the natural symmetry of inertial motion that underpins\nNewtonian physics. Although rigid-body symmetry is one of the most established\nand fundamental tools in robotics, there appears to be no comparable treatment\nof Galilean symmetry for a robotics audience. In this paper, we present a\nrobotics-tailored exposition of Galilean symmetry that leverages the\ncommunity's familiarity with and understanding of rigid-body transformations\nand pose representations. Our approach contrasts with common treatments in the\nphysics literature that introduce Galilean symmetry as a stepping stone to\nEinstein's relativity. A key insight is that the Galilean matrix Lie group can\nbe used to describe two different pose representations, Galilean frames, that\nuse inertial velocity in the state definition, and extended poses, that use\ncoordinate velocity. We provide three examples where applying the Galilean\nmatrix Lie-group algebra to robotics problems is straightforward and yields\nsignificant insights: inertial navigation above the rotating Earth, manipulator\nkinematics, and sensor data fusion under temporal uncertainty. We believe that\nthe time is right for the robotics community to benefit from rediscovering and\nextending this classical material and applying it to modern problems.", "AI": {"tldr": "\u672c\u6587\u5c06\u4f3d\u5229\u7565\u5bf9\u79f0\u6027\u5f15\u5165\u673a\u5668\u4eba\u9886\u57df\uff0c\u8ba8\u8bba\u5176\u4e0e\u521a\u4f53\u53d8\u6362\u7684\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u5b9e\u4f8b\u5c55\u793a\u5176\u5728\u73b0\u4ee3\u673a\u5668\u4eba\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u5c3d\u7ba1\u521a\u4f53\u5bf9\u79f0\u6027\u5728\u673a\u5668\u4eba\u5b66\u4e2d\u5f97\u5230\u4e86\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5728\u673a\u5668\u4eba\u9886\u57df\u5bf9\u4f3d\u5229\u7565\u5bf9\u79f0\u6027\u7684\u5904\u7406\u76f8\u5bf9\u8f83\u5c11\u3002", "method": "\u4ecb\u7ecd\u4f3d\u5229\u7565\u5bf9\u79f0\u6027\uff0c\u5e76\u7ed3\u5408\u673a\u5668\u4eba\u9886\u57df\u7684\u521a\u6027\u4f53\u53d8\u6362\u548c\u59ff\u6001\u8868\u793a\u8fdb\u884c\u8ba8\u8bba\u3002", "result": "\u901a\u8fc7\u4f3d\u5229\u7565\u77e9\u9635\u674e\u7fa4\uff0c\u63d0\u51fa\u4e24\u79cd\u4e0d\u540c\u7684\u59ff\u6001\u8868\u793a\u65b9\u6cd5\uff0c\u5e76\u5728\u60ef\u6027\u5bfc\u822a\u3001\u673a\u68b0\u81c2\u8fd0\u52a8\u5b66\u548c\u4f20\u611f\u5668\u6570\u636e\u878d\u5408\u7b49\u65b9\u9762\u63d0\u4f9b\u793a\u4f8b\uff0c\u5c55\u793a\u5176\u5b9e\u7528\u6027\u3002", "conclusion": "\u673a\u5668\u4eba\u793e\u533a\u5e94\u91cd\u65b0\u53d1\u73b0\u548c\u6269\u5c55\u7ecf\u5178\u7684\u4f3d\u5229\u7565\u5bf9\u79f0\u6027\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u73b0\u4ee3\u95ee\u9898\u3002"}}
{"id": "2510.11300", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11300", "abs": "https://arxiv.org/abs/2510.11300", "authors": ["Bernd Hofmann", "Sven Kreitlein", "Joerg Franke", "Patrick Bruendl"], "title": "Beyond touch-based HMI: Control your machines in natural language by utilizing large language models and OPC UA", "comment": null, "summary": "This paper proposes an agent-based approach toward a more natural interface\nbetween humans and machines. Large language models equipped with tools and the\ncommunication standard OPC UA are utilized to control machines in natural\nlanguage. Instead of touch interaction, which is currently the state-of-the-art\nmedium for interaction in operations, the proposed approach enables operators\nto talk or text with machines. This allows commands such as 'Please decrease\nthe temperature by 20 % in machine 1 and set the motor speed to 5000 rpm in\nmachine 2.' The large language model receives the user input and selects one of\nthree predefined tools that connect to an OPC UA server and either change or\nread the value of a node. Afterwards, the result of the tool execution is\npassed back to the language model, which then provides a final response to the\nuser. The approach is universally designed and can therefore be applied to any\nmachine that supports the OPC UA standard. The large language model is neither\nfine-tuned nor requires training data, only the relevant machine credentials\nand a parameter dictionary are included within the system prompt. The approach\nis evaluated on a Siemens S7-1500 programmable logic controller with four\nmachine parameters in a case study of fifty synthetically generated commands on\nfive different models. The results demonstrate high success rate, with\nproprietary GPT 5 models achieving accuracies between 96.0 % and 98.0 %, and\nopen-weight models reaching up to 90.0 %. The proposed approach of this\nempirical study contributes to advancing natural interaction in industrial\nhuman-machine interfaces.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548cOPC UA\u6807\u51c6\u7684\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u65b9\u6cd5\uff0c\u6210\u529f\u63d0\u9ad8\u4e86\u673a\u5668\u63a7\u5236\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u65e8\u5728\u66ff\u4ee3\u5f53\u524d\u7684\u89e6\u6478\u4ea4\u4e92\u65b9\u5f0f\uff0c\u901a\u8fc7\u8bed\u97f3\u6216\u6587\u672c\u4e0e\u673a\u5668\u4e92\u52a8\uff0c\u5b9e\u73b0\u66f4\u76f4\u89c2\u7684\u7528\u6237\u64cd\u4f5c\u4f53\u9a8c\u3002", "method": "\u901a\u8fc7\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0eOPC UA\u901a\u4fe1\u6807\u51c6\uff0c\u7ed3\u5408\u9884\u5b9a\u4e49\u5de5\u5177\u9009\u62e9\u6765\u63a7\u5236\u673a\u5668\uff0c\u8fdb\u884c\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u4e13\u6709\u7684GPT 5\u6a21\u578b\u51c6\u786e\u7387\u572896.0%\u81f398.0%\u4e4b\u95f4\uff0c\u5f00\u653e\u6743\u91cd\u6a21\u578b\u8fbe\u523090.0%\u7684\u51c6\u786e\u7387\uff0c\u663e\u793a\u4e86\u9ad8\u6210\u529f\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u4ee3\u7406\u7684\u65b9\u6cd5\u5728\u5de5\u4e1a\u4eba\u673a\u754c\u9762\u4e2d\uff0c\u4fc3\u8fdb\u4e86\u66f4\u81ea\u7136\u7684\u4ea4\u4e92\u65b9\u5f0f\uff0c\u63d0\u9ad8\u4e86\u6307\u4ee4\u6267\u884c\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2510.10506", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10506", "abs": "https://arxiv.org/abs/2510.10506", "authors": ["Kush Garg", "Akshat Dave"], "title": "SuperEx: Enhancing Indoor Mapping and Exploration using Non-Line-of-Sight Perception", "comment": "8 pages, 9 Figures , Project webpage: https://super-ex.github.io/", "summary": "Efficient exploration and mapping in unknown indoor environments is a\nfundamental challenge, with high stakes in time-critical settings. In current\nsystems, robot perception remains confined to line-of-sight; occluded regions\nremain unknown until physically traversed, leading to inefficient exploration\nwhen layouts deviate from prior assumptions. In this work, we bring\nnon-line-of-sight (NLOS) sensing to robotic exploration. We leverage\nsingle-photon LiDARs, which capture time-of-flight histograms that encode the\npresence of hidden objects - allowing robots to look around blind corners.\nRecent single-photon LiDARs have become practical and portable, enabling\ndeployment beyond controlled lab settings. Prior NLOS works target 3D\nreconstruction in static, lab-based scenarios, and initial efforts toward\nNLOS-aided navigation consider simplified geometries. We introduce SuperEx, a\nframework that integrates NLOS sensing directly into the mapping-exploration\nloop. SuperEx augments global map prediction with beyond-line-of-sight cues by\n(i) carving empty NLOS regions from timing histograms and (ii) reconstructing\noccupied structure via a two-step physics-based and data-driven approach that\nleverages structural regularities. Evaluations on complex simulated maps and\nthe real-world KTH Floorplan dataset show a 12% gain in mapping accuracy under\n< 30% coverage and improved exploration efficiency compared to line-of-sight\nbaselines, opening a path to reliable mapping beyond direct visibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u673a\u5668\u4eba\u63a2\u7d22\u6846\u67b6SuperEx\uff0c\u5229\u7528\u975e\u89c6\u7ebf\u611f\u77e5\u63d0\u9ad8\u5ba4\u5185\u73af\u5883\u6620\u5c04\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u611f\u77e5\u4e3b\u8981\u4f9d\u8d56\u89c6\u7ebf\uff0c\u65e0\u6cd5\u6709\u6548\u63a2\u7d22\u88ab\u906e\u6321\u7684\u533a\u57df\u3002\u672c\u7814\u7a76\u5e0c\u671b\u901a\u8fc7NLOS\u611f\u77e5\u63d0\u5347\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u63a2\u7d22\u80fd\u529b\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5355\u5149\u5b50LiDAR\u6355\u6349\u65f6\u95f4\u98de\u884c\u76f4\u65b9\u56fe\uff0c\u7ed3\u5408\u7269\u7406\u57fa\u7840\u548c\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u6539\u8fdb\u4e86\u6620\u5c04\u63a2\u7d22\u5faa\u73af\u3002", "result": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86SuperEx\uff0c\u4e00\u4e2a\u5c06\u975e\u89c6\u7ebf\u611f\u77e5(NLOS)\u5f15\u5165\u673a\u5668\u4eba\u63a2\u7d22\u548c\u6620\u5c04\u7684\u6846\u67b6\u3002\u901a\u8fc7\u5229\u7528\u5355\u5149\u5b50LiDAR\u6280\u672f\uff0cSuperEx\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u672a\u77e5\u5ba4\u5185\u73af\u5883\u4e2d\u7684\u63a2\u7d22\u548c\u6620\u5c04\u6548\u7387\u3002", "conclusion": "SuperEx\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u6620\u5c04\u51c6\u786e\u6027\u548c\u63a2\u7d22\u6548\u7387\uff0c\u4e3a\u7a81\u7834\u73b0\u6709\u7684\u6620\u5c04\u6280\u672f\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2510.11530", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.11530", "abs": "https://arxiv.org/abs/2510.11530", "authors": ["Matthias Huemmer", "Theophile Shyiramunda", "Michelle J. Cummings-Koether"], "title": "Exploring Artificial Intelligence and Culture: Methodology for a comparative study of AI's impact on norms, trust, and problem-solving across academic and business environments", "comment": "21 pages, 4 figures, 3 tables", "summary": "This paper proposes a rigorous framework to examine the two-way relationship\nbetween artificial intelligence (AI), human cognition, problem-solving, and\ncultural adaptation across academic and business settings. It addresses a key\ngap by asking how AI reshapes cognitive processes and organizational norms, and\nhow cultural values and institutional contexts shape AI adoption, trust, and\nuse over time. We employ a three-wave longitudinal design that tracks AI\nknowledge, perceived competence, trust trajectories, and cultural responses.\nParticipants span academic institutions and diverse firms, enabling contextual\ncomparison. A dynamic sample continuous, intermittent, and wave-specific\nrespondents mirrors real organizational variability and strengthens ecological\nvalidity. Methodologically, the study integrates quantitative longitudinal\nmodeling with qualitative thematic analysis to capture temporal, structural,\nand cultural patterns in AI uptake. We trace AI acculturation through phases of\ninitial resistance, exploratory adoption, and cultural embedding, revealing\ndistinctive trust curves and problem-solving strategies by context: academic\nenvironments tend to collaborative, deliberative integration; business\nenvironments prioritize performance, speed, and measurable outcomes. Framing\nadoption as bidirectional challenges deterministic views: AI both reflects and\nreconfigures norms, decision-making, and cognitive engagement. As the first\ncomparative longitudinal study of its kind, this work advances methodological\nrigor and offers actionable foundations for human-centred, culturally\nresponsive AI strategies-supporting evidence-based policies, training, and\ngovernance that align cognitive performance, organizational goals, and ethical\ncommitments.", "AI": {"tldr": "\u672c\u8bba\u6587\u7814\u7a76AI\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u3001\u95ee\u9898\u89e3\u51b3\u53ca\u6587\u5316\u9002\u5e94\u7684\u53cc\u5411\u5173\u7cfb\uff0c\u91c7\u7528\u7eb5\u5411\u7814\u7a76\u8bbe\u8ba1\u63a2\u8ba8AI\u77e5\u8bc6\u3001\u4fe1\u4efb\u53ca\u6587\u5316\u53cd\u5e94\u7684\u53d8\u5316\uff0c\u63ed\u793aAI\u91c7\u7528\u7684\u4e0d\u540c\u9636\u6bb5\u53ca\u5176\u5bf9\u5b66\u672f\u548c\u5546\u4e1a\u73af\u5883\u7684\u5f71\u54cd\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u5173\u952e\u7a7a\u767d\uff0c\u63a2\u8ba8\u5728\u5b66\u672f\u548c\u5546\u4e1a\u80cc\u666f\u4e0b\uff0cAI\u5982\u4f55\u6539\u53d8\u8ba4\u77e5\u8fc7\u7a0b\u4ee5\u53ca\u6587\u5316\u548c\u5236\u5ea6\u56e0\u7d20\u5982\u4f55\u5f71\u54cd\u5176\u91c7\u7eb3\u548c\u4fe1\u4efb\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e09\u6ce2\u7eb5\u5411\u8bbe\u8ba1\uff0c\u7ed3\u5408\u5b9a\u91cf\u7eb5\u5411\u5efa\u6a21\u4e0e\u5b9a\u6027\u4e3b\u9898\u5206\u6790\uff0c\u6355\u6349AI\u91c7\u7eb3\u4e2d\u7684\u65f6\u95f4\u3001\u7ed3\u6784\u548c\u6587\u5316\u6a21\u5f0f\u3002", "result": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e25\u683c\u7684\u6846\u67b6\uff0c\u4ee5\u7814\u7a76\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u3001\u4eba\u7c7b\u8ba4\u77e5\u3001\u95ee\u9898\u89e3\u51b3\u548c\u6587\u5316\u9002\u5e94\u4e4b\u95f4\u7684\u53cc\u5411\u5173\u7cfb\u3002\u5176\u6838\u5fc3\u5185\u5bb9\u63a2\u8ba8\u4e86AI\u5982\u4f55\u91cd\u65b0\u5851\u9020\u8ba4\u77e5\u8fc7\u7a0b\u548c\u7ec4\u7ec7\u89c4\u8303\uff0c\u4ee5\u53ca\u6587\u5316\u4ef7\u503c\u89c2\u548c\u5236\u5ea6\u80cc\u666f\u5982\u4f55\u5f71\u54cdAI\u7684\u91c7\u7528\u3001\u4fe1\u4efb\u548c\u4f7f\u7528\u3002", "conclusion": "\u901a\u8fc7\u8ffd\u8e2aAI\u7684\u6587\u5316\u9002\u5e94\u8fc7\u7a0b\uff0c\u672c\u7814\u7a76\u63ed\u793a\u4e86\u4e0d\u540c\u73af\u5883\u4e0b\u7684\u4fe1\u4efb\u66f2\u7ebf\u548c\u95ee\u9898\u89e3\u51b3\u7b56\u7565\uff0c\u4e3a\u4eba\u672c\u3001\u6587\u5316\u54cd\u5e94\u7684AI\u6218\u7565\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u57fa\u7840\uff0c\u652f\u6301\u57fa\u4e8e\u8bc1\u636e\u7684\u653f\u7b56\u3001\u57f9\u8bad\u548c\u6cbb\u7406\u3002"}}
{"id": "2510.10516", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10516", "abs": "https://arxiv.org/abs/2510.10516", "authors": ["Kanishkha Jaisankar", "Xiaoyang Jiang", "Feifan Liao", "Jeethu Sreenivas Amuthan"], "title": "Population-Coded Spiking Neural Networks for High-Dimensional Robotic Control", "comment": null, "summary": "Energy-efficient and high-performance motor control remains a critical\nchallenge in robotics, particularly for high-dimensional continuous control\ntasks with limited onboard resources. While Deep Reinforcement Learning (DRL)\nhas achieved remarkable results, its computational demands and energy\nconsumption limit deployment in resource-constrained environments. This paper\nintroduces a novel framework combining population-coded Spiking Neural Networks\n(SNNs) with DRL to address these challenges. Our approach leverages the\nevent-driven, asynchronous computation of SNNs alongside the robust policy\noptimization capabilities of DRL, achieving a balance between energy efficiency\nand control performance. Central to this framework is the Population-coded\nSpiking Actor Network (PopSAN), which encodes high-dimensional observations\ninto neuronal population activities and enables optimal policy learning through\ngradient-based updates. We evaluate our method on the Isaac Gym platform using\nthe PixMC benchmark with complex robotic manipulation tasks. Experimental\nresults on the Franka robotic arm demonstrate that our approach achieves energy\nsavings of up to 96.10% compared to traditional Artificial Neural Networks\n(ANNs) while maintaining comparable control performance. The trained SNN\npolicies exhibit robust finger position tracking with minimal deviation from\ncommanded trajectories and stable target height maintenance during\npick-and-place operations. These results position population-coded SNNs as a\npromising solution for energy-efficient, high-performance robotic control in\nresource-constrained applications, paving the way for scalable deployment in\nreal-world robotics systems.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u7fa4\u4f53\u7f16\u7801\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u76f8\u7ed3\u5408\u7684\u6846\u67b6\uff0c\u4f18\u5316\u4e86\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u7535\u673a\u63a7\u5236\uff0c\u4e14\u5728\u80fd\u6548\u548c\u6027\u80fd\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\uff0c\u5f00\u53d1\u80fd\u6548\u9ad8\u4e14\u6027\u80fd\u5353\u8d8a\u7684\u7535\u673a\u63a7\u5236\u7cfb\u7edf\u662f\u673a\u5668\u4eba\u6280\u672f\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u7ef4\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\uff0c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60(DRL)\u7684\u8ba1\u7b97\u9700\u6c42\u548c\u80fd\u8017\u95ee\u9898\u5c24\u4e3a\u7a81\u51fa\u3002", "method": "\u5f00\u53d1\u4e86\u4eba\u53e3\u7f16\u7801\u8109\u51b2\u6f14\u5458\u7f51\u7edc(PopSAN)\uff0c\u901a\u8fc7\u68af\u5ea6\u66f4\u65b0\u5b9e\u73b0\u6700\u4f18\u7b56\u7565\u5b66\u4e60\uff0c\u5e76\u5728\u827e\u8428\u514b\u5065\u8eab\u5e73\u53f0\u4e0a\u5bf9\u590d\u6742\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u5c06\u57fa\u4e8e\u7fa4\u4f53\u7f16\u7801\u7684\u8109\u51b2\u795e\u7ecf\u7f51\u7edc(SNN)\u4e0e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60(DRL)\u76f8\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u80fd\u6548\u548c\u63a7\u5236\u6027\u80fd\u3002", "conclusion": "\u57fa\u4e8e\u7fa4\u4f53\u7f16\u7801\u7684\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u4e3a\u8d44\u6e90\u53d7\u9650\u5e94\u7528\u4e2d\u7684\u80fd\u6548\u9ad8\u3001\u6027\u80fd\u4f18\u8d8a\u7684\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4fc3\u8fdb\u4e86\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u90e8\u7f72\u3002"}}
{"id": "2510.11596", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.11596", "abs": "https://arxiv.org/abs/2510.11596", "authors": ["Hoang-Son Vo", "Karina Kolmogortseva", "Ngumimi Karen Iyortsuun", "Hong-Duyen Vo", "Soo-Hyung Kim"], "title": "GlobalizeEd: A Multimodal Translation System that Preserves Speaker Identity in Academic Lectures", "comment": null, "summary": "A large amount of valuable academic content is only available in its original\nlanguage, creating a significant access barrier for the global student\ncommunity. This is a challenge for translating in several subjects, such as\nhistory, culture, and the arts, where current automated subtitle tools fail to\nconvey the appropriate pedagogical tone and specialized meaning. In addition,\nreading traditional automated subtitles increases cognitive load and leads to a\ndisconnected learning experience. Through a mixed-methods study involving 36\nparticipants, we found that GlobalizeEds dubbed formats significantly reduce\ncognitive load and offer a more immersive learning experience compared to\ntraditional subtitles. Although learning effectiveness was comparable between\nhigh-quality subtitles and dubbed formats, both groups valued GlobalizeEds\nability to preserve the speakers voice, which enhanced perceived authenticity.\nInstructors rated translation accuracy and vocal naturalness, whereas students\nreported that synchronized, identity-preserving outputs fostered engagement and\ntrust. This work contributes a novel human-centered AI framework for\ncross-lingual education, demonstrating how multimodal translation systems can\nbalance linguistic fidelity, cultural adaptability, and user control to create\nmore inclusive global learning experiences.", "AI": {"tldr": "\u7814\u7a76\u8868\u660eGlobalizeEds\u7684\u914d\u97f3\u683c\u5f0f\u5728\u8ba4\u77e5\u8d1f\u62c5\u548c\u6c89\u6d78\u5f0f\u5b66\u4e60\u4f53\u9a8c\u4e0a\u4f18\u4e8e\u4f20\u7edf\u5b57\u5e55\uff0c\u5c3d\u7ba1\u5b66\u4e60\u6548\u679c\u76f8\u5f53\uff0c\u4f46\u6240\u6709\u53c2\u4e0e\u8005\u90fd\u91cd\u89c6\u4fdd\u6301\u8bb2\u8005\u58f0\u97f3\u7684\u80fd\u529b\uff0c\u589e\u5f3a\u4e86\u771f\u5b9e\u6027\u3002", "motivation": "\u89e3\u51b3\u4ec5\u4ee5\u539f\u8bed\u8a00\u53d1\u5e03\u7684\u5b66\u672f\u5185\u5bb9\u5bf9\u5168\u7403\u5b66\u751f\u793e\u533a\u7684\u83b7\u53d6\u969c\u788d\uff0c\u5c24\u5176\u662f\u5728\u5386\u53f2\u3001\u6587\u5316\u548c\u827a\u672f\u7b49\u9886\u57df\u3002", "method": "\u901a\u8fc7\u5305\u542b36\u540d\u53c2\u4e0e\u8005\u7684\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\uff0c\u8bc4\u4f30\u4e0d\u540c\u7ffb\u8bd1\u683c\u5f0f\u5bf9\u5b66\u4e60\u4f53\u9a8c\u7684\u5f71\u54cd\u3002", "result": "GlobalizeEds\u7684\u914d\u97f3\u683c\u5f0f\u663e\u8457\u964d\u4f4e\u4e86\u8ba4\u77e5\u8d1f\u62c5\uff0c\u63d0\u4f9b\u4e86\u6bd4\u4f20\u7edf\u5b57\u5e55\u66f4\u6c89\u6d78\u7684\u5b66\u4e60\u4f53\u9a8c\uff0c\u53c2\u4e0e\u8005\u5bf9\u4fdd\u7559\u8bb2\u8005\u58f0\u97f3\u7684\u80fd\u529b\u8bc4\u4ef7\u8f83\u9ad8\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u4eba\u4e3a\u672c\u7684AI\u6846\u67b6\u7528\u4e8e\u8de8\u8bed\u8a00\u6559\u80b2\uff0c\u5c55\u793a\u4e86\u591a\u6a21\u6001\u7ffb\u8bd1\u7cfb\u7edf\u5982\u4f55\u5728\u8bed\u8a00\u5fe0\u5b9e\u6027\u548c\u6587\u5316\u9002\u5e94\u6027\u4e4b\u95f4\u627e\u5230\u5e73\u8861\uff0c\u4ece\u800c\u521b\u9020\u66f4\u5177\u5305\u5bb9\u6027\u7684\u5168\u7403\u5b66\u4e60\u4f53\u9a8c\u3002"}}
{"id": "2510.10545", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.10545", "abs": "https://arxiv.org/abs/2510.10545", "authors": ["Koki Yamane", "Sho Sakaino", "Toshiaki Tsuji"], "title": "Decoupled Scaling 4ch Bilateral Control on the Cartesian coordinate by 6-DoF Manipulator using Rotation Matrix", "comment": "6 pages, 4 figures, Accepted at SAMCON 2025", "summary": "Four-channel bilateral control is a method for achieving remote control with\nforce feedback and adjustment operability by synchronizing the positions and\nforces of two manipulators. This is expected to significantly improve the\noperability of the remote control in contact-rich tasks. Among these, 4-channel\nbilateral control on the Cartesian coordinate system is advantageous owing to\nits suitability for manipulators with different structures and because it\nallows the dynamics in the Cartesian coordinate system to be adjusted by\nadjusting the control parameters, thus achieving intuitive operability for\nhumans. This paper proposes a 4-channel bilateral control method that achieves\nthe desired dynamics by decoupling each dimension in the Cartesian coordinate\nsystem regardless of the scaling factor.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u56db\u901a\u9053\u53cc\u8fb9\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7b1b\u5361\u5c14\u5750\u6807\u7cfb\u4e2d\u89e3\u8026\u5404\u7ef4\u5ea6\uff0c\u63d0\u9ad8\u8fdc\u7a0b\u64cd\u4f5c\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u8fdc\u7a0b\u63a7\u5236\u5728\u63a5\u89e6\u5bc6\u96c6\u4efb\u52a1\u4e2d\u7684\u64cd\u4f5c\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u4e0d\u540c\u7ed3\u6784\u7684\u64cd\u7eb5\u5668\u65f6\u3002", "method": "\u901a\u8fc7\u72ec\u7acb\u8c03\u8282\u7b1b\u5361\u5c14\u5750\u6807\u7cfb\u4e2d\u7684\u6bcf\u4e2a\u7ef4\u5ea6\uff0c\u6765\u5b9e\u73b0\u56db\u901a\u9053\u53cc\u8fb9\u63a7\u5236\u3002", "result": "\u8be5\u65b9\u6cd5\u589e\u5f3a\u4e86\u8fdc\u7a0b\u63a7\u5236\u7684\u53ef\u64cd\u4f5c\u6027\uff0c\u63d0\u4f9b\u4e86\u76f4\u89c2\u7684\u7528\u6237\u4f53\u9a8c\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u56db\u901a\u9053\u53cc\u8fb9\u63a7\u5236\u65b9\u6cd5\uff0c\u80fd\u5728\u7b1b\u5361\u5c14\u5750\u6807\u7cfb\u4e2d\u89e3\u8026\u5404\u7ef4\u5ea6\uff0c\u8fbe\u5230\u671f\u671b\u7684\u52a8\u6001\u6548\u679c\u3002"}}
{"id": "2510.11421", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.11421", "abs": "https://arxiv.org/abs/2510.11421", "authors": ["Shih-Chieh Sun", "Yun-Cheng Tsai"], "title": "A Modular AIoT Framework for Low-Latency Real-Time Robotic Teleoperation in Smart Cities", "comment": null, "summary": "This paper presents an AI-driven IoT robotic teleoperation system designed\nfor real-time remote manipulation and intelligent visual monitoring, tailored\nfor smart city applications. The architecture integrates a Flutter-based\ncross-platform mobile interface with MQTT-based control signaling and WebRTC\nvideo streaming via the LiveKit framework. A YOLOv11-nano model is deployed for\nlightweight object detection, enabling real-time perception with annotated\nvisual overlays delivered to the user interface. Control commands are\ntransmitted via MQTT to an ESP8266-based actuator node, which coordinates\nmulti-axis robotic arm motion through an Arduino Mega2560 controller. The\nbackend infrastructure is hosted on DigitalOcean, ensuring scalable cloud\norchestration and stable global communication. Latency evaluations conducted\nunder both local and international VPN scenarios (including Hong Kong, Japan,\nand Belgium) demonstrate actuator response times as low as 0.2 seconds and\ntotal video latency under 1.2 seconds, even across high-latency networks. This\nlow-latency dual-protocol design ensures responsive closed-loop interaction and\nrobust performance in distributed environments. Unlike conventional\nteleoperation platforms, the proposed system emphasizes modular deployment,\nreal-time AI sensing, and adaptable communication strategies, making it\nwell-suited for smart city scenarios such as remote infrastructure inspection,\npublic equipment servicing, and urban automation. Future enhancements will\nfocus on edge-device deployment, adaptive routing, and integration with\ncity-scale IoT networks to enhance resilience and scalability.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u7269\u8054\u7f51\u673a\u5668\u4eba\u9065\u63a7\u7cfb\u7edf\uff0c\u65e8\u5728\u5b9e\u73b0\u5b9e\u65f6\u8fdc\u7a0b\u64cd\u4f5c\u548c\u667a\u80fd\u89c6\u89c9\u76d1\u63a7\uff0c\u7279\u522b\u9002\u5408\u667a\u6167\u57ce\u5e02\u5e94\u7528\u3002", "motivation": "\u8bba\u6587\u7684\u52a8\u673a\u662f\u5f00\u53d1\u4e00\u4e2a\u9002\u7528\u4e8e\u667a\u6167\u57ce\u5e02\u7684\u9ad8\u6548\u5b9e\u65f6\u9065\u63a7\u7cfb\u7edf\uff0c\u589e\u5f3a\u516c\u5171\u8bbe\u65bd\u7684\u64cd\u4f5c\u6548\u7387\u3002", "method": "\u7cfb\u7edf\u67b6\u6784\u7ed3\u5408\u4e86\u57fa\u4e8eFlutter\u7684\u79fb\u52a8\u754c\u9762\u3001MQTT\u63a7\u5236\u4fe1\u4ee4\u548cWebRTC\u89c6\u9891\u6d41\uff0c\u91c7\u7528YOLOv11-nano\u8fdb\u884c\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\uff0c\u63a7\u5236\u547d\u4ee4\u901a\u8fc7MQTT\u53d1\u9001\u81f3ESP8266\u9a71\u52a8\u7684\u6267\u884c\u8282\u70b9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u672c\u5730\u53ca\u56fd\u9645VPN\u573a\u666f\u4e0b\uff0c\u6267\u884c\u5668\u54cd\u5e94\u65f6\u95f4\u4f4e\u81f30.2\u79d2\uff0c\u603b\u89c6\u9891\u5ef6\u8fdf\u57281.2\u79d2\u4ee5\u5185\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u4f4e\u5ef6\u8fdf\u548c\u6a21\u5757\u5316\u8bbe\u8ba1\u786e\u4fdd\u54cd\u5e94\u8fc5\u901f\u4e0e\u9c81\u68d2\u7684\u6027\u80fd\uff0c\u9002\u5408\u4e8e\u667a\u6167\u57ce\u5e02\u7684\u5404\u79cd\u573a\u666f\u3002"}}
{"id": "2510.10567", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10567", "abs": "https://arxiv.org/abs/2510.10567", "authors": ["Alexander Langmann", "Yevhenii Tokarev", "Mattia Piccinini", "Korbinian Moller", "Johannes Betz"], "title": "Reinforcement Learning-based Dynamic Adaptation for Sampling-Based Motion Planning in Agile Autonomous Driving", "comment": "8 pages, submitted to the IEEE ICRA 2026, Vienna, Austria", "summary": "Sampling-based trajectory planners are widely used for agile autonomous\ndriving due to their ability to generate fast, smooth, and kinodynamically\nfeasible trajectories. However, their behavior is often governed by a cost\nfunction with manually tuned, static weights, which forces a tactical\ncompromise that is suboptimal across the wide range of scenarios encountered in\na race. To address this shortcoming, we propose using a Reinforcement Learning\n(RL) agent as a high-level behavioral selector that dynamically switches the\ncost function parameters of an analytical, low-level trajectory planner during\nruntime. We show the effectiveness of our approach in simulation in an\nautonomous racing environment where our RL-based planner achieved 0% collision\nrate while reducing overtaking time by up to 60% compared to state-of-the-art\nstatic planners. Our new agent now dynamically switches between aggressive and\nconservative behaviors, enabling interactive maneuvers unattainable with static\nconfigurations. These results demonstrate that integrating reinforcement\nlearning as a high-level selector resolves the inherent trade-off between\nsafety and competitiveness in autonomous racing planners. The proposed\nmethodology offers a pathway toward adaptive yet interpretable motion planning\nfor broader autonomous driving applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u52a8\u6001\u8c03\u6574\u6210\u672c\u51fd\u6570\u53c2\u6570\u7684\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\uff0c\u4ee5\u4f18\u5316\u81ea\u4e3b\u9a7e\u9a76\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u91c7\u6837\u57fa\u8f68\u8ff9\u89c4\u5212\u5668\u5728\u5904\u7406\u591a\u79cd\u573a\u666f\u65f6\uff0c\u624b\u52a8\u8c03\u8282\u7684\u9759\u6001\u6743\u91cd\u5bfc\u81f4\u7684\u6b21\u4f18\u8868\u73b0\u3002", "method": "\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u7684\u4ee3\u7406\u5728\u8fd0\u884c\u65f6\u52a8\u6001\u5207\u6362\u4f4e\u5c42\u6b21\u8f68\u8ff9\u89c4\u5212\u5668\u7684\u6210\u672c\u51fd\u6570\u53c2\u6570\u3002", "result": "\u5728\u81ea\u4e3b\u8d5b\u8f66\u6a21\u62df\u4e2d\uff0cRL\u89c4\u5212\u5668\u5b9e\u73b0\u4e860%\u7684\u78b0\u649e\u7387\uff0c\u5e76\u5c06\u8d85\u8f66\u65f6\u95f4\u51cf\u5c11\u4e86\u6700\u591a60%\u3002", "conclusion": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u7684\u9ad8\u5c42\u9009\u62e9\u5668\u89e3\u51b3\u4e86\u81ea\u4e3b\u8d5b\u8f66\u89c4\u5212\u4e2d\u7684\u5b89\u5168\u548c\u7ade\u4e89\u6027\u4e4b\u95f4\u7684\u56fa\u6709\u6743\u8861\uff0c\u5e76\u4e3a\u66f4\u5e7f\u6cdb\u7684\u81ea\u4e3b\u9a7e\u9a76\u5e94\u7528\u63d0\u4f9b\u4e86\u9002\u5e94\u6027\u548c\u53ef\u89e3\u91ca\u7684\u8fd0\u52a8\u89c4\u5212\u9014\u5f84\u3002"}}
{"id": "2510.11474", "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.11474", "abs": "https://arxiv.org/abs/2510.11474", "authors": ["Ardian Selmonaj", "Giacomo Del Rio", "Adrian Schneider", "Alessandro Antonucci"], "title": "Coordinated Strategies in Realistic Air Combat by Hierarchical Multi-Agent Reinforcement Learning", "comment": "2025 IEEE International Conference on Agentic AI (ICA)", "summary": "Achieving mission objectives in a realistic simulation of aerial combat is\nhighly challenging due to imperfect situational awareness and nonlinear flight\ndynamics. In this work, we introduce a novel 3D multi-agent air combat\nenvironment and a Hierarchical Multi-Agent Reinforcement Learning framework to\ntackle these challenges. Our approach combines heterogeneous agent dynamics,\ncurriculum learning, league-play, and a newly adapted training algorithm. To\nthis end, the decision-making process is organized into two abstraction levels:\nlow-level policies learn precise control maneuvers, while high-level policies\nissue tactical commands based on mission objectives. Empirical results show\nthat our hierarchical approach improves both learning efficiency and combat\nperformance in complex dogfight scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u76843D\u591a\u667a\u80fd\u4f53\u7a7a\u6218\u73af\u5883\u548c\u5206\u5c42\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4ee5\u5e94\u5bf9\u73b0\u5b9e\u7a7a\u6218\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u5e94\u5bf9\u4e0d\u5b8c\u7f8e\u7684\u60c5\u5883\u610f\u8bc6\u548c\u975e\u7ebf\u6027\u98de\u884c\u52a8\u6001\uff0c\u65e8\u5728\u5b9e\u73b0\u7a7a\u6218\u6a21\u62df\u4efb\u52a1\u76ee\u6807\u3002", "method": "\u7ed3\u5408\u5f02\u8d28\u667a\u80fd\u4f53\u52a8\u6001\u3001\u8bfe\u7a0b\u5b66\u4e60\u3001\u8054\u8d5b\u5bf9\u6297\u53ca\u65b0\u7684\u8bad\u7ec3\u7b97\u6cd5\uff0c\u51b3\u7b56\u8fc7\u7a0b\u5206\u4e3a\u4e24\u4e2a\u62bd\u8c61\u5c42\u6b21\uff1a\u4f4e\u5c42\u6b21\u653f\u7b56\u5b66\u4e60\u7cbe\u786e\u63a7\u5236\uff0c\u9ad8\u5c42\u6b21\u653f\u7b56\u57fa\u4e8e\u4efb\u52a1\u76ee\u6807\u53d1\u5e03\u6218\u672f\u6307\u4ee4\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\uff0c\u5206\u5c42\u65b9\u6cd5\u5728\u590d\u6742\u7a7a\u6218\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5b66\u4e60\u6548\u7387\u548c\u6218\u6597\u8868\u73b0\u3002", "conclusion": "\u5206\u5c42\u65b9\u6cd5\u63d0\u9ad8\u4e86\u590d\u6742\u7a7a\u6218\u573a\u666f\u4e2d\u7684\u5b66\u4e60\u6548\u7387\u548c\u6218\u6597\u8868\u73b0\u3002"}}
{"id": "2510.10597", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10597", "abs": "https://arxiv.org/abs/2510.10597", "authors": ["David Rodr\u00edguez-Mart\u00ednez", "C. J. P\u00e9rez del Pulgar"], "title": "Fast Vision in the Dark: A Case for Single-Photon Imaging in Planetary Navigation", "comment": "9 pages, 6 figures, conference paper", "summary": "Improving robotic navigation is critical for extending exploration range and\nenhancing operational efficiency. Vision-based navigation relying on\ntraditional CCD or CMOS cameras faces major challenges when complex\nillumination conditions are paired with motion, limiting the range and\naccessibility of mobile planetary robots. In this study, we propose a novel\napproach to planetary navigation that leverages the unique imaging capabilities\nof Single-Photon Avalanche Diode (SPAD) cameras. We present the first\ncomprehensive evaluation of single-photon imaging as an alternative passive\nsensing technology for robotic exploration missions targeting perceptually\nchallenging locations, with a special emphasis on high-latitude lunar regions.\nWe detail the operating principles and performance characteristics of SPAD\ncameras, assess their advantages and limitations in addressing key perception\nchallenges of upcoming exploration missions to the Moon, and benchmark their\nperformance under representative illumination conditions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528SPAD\u76f8\u673a\u7684\u884c\u661f\u5bfc\u822a\u65b0\u65b9\u6cd5\uff0c\u4ee5\u514b\u670d\u4f20\u7edf\u76f8\u673a\u5728\u590d\u6742\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u6539\u5584\u673a\u5668\u4eba\u5bfc\u822a\u5bf9\u62d3\u5c55\u63a2\u6d4b\u8303\u56f4\u548c\u63d0\u9ad8\u64cd\u4f5c\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u4f20\u7edf\u76f8\u673a\u5728\u590d\u6742\u5149\u7167\u6761\u4ef6\u4e0b\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u884c\u661f\u5bfc\u822a\u65b9\u6cd5\uff0c\u5229\u7528\u5355\u5149\u5b50\u96ea\u5d29\u4e8c\u6781\u7ba1\uff08SPAD\uff09\u76f8\u673a\u7684\u72ec\u7279\u6210\u50cf\u80fd\u529b\u3002", "result": "\u63d0\u4f9b\u4e86\u5355\u5149\u5b50\u6210\u50cf\u4f5c\u4e3a\u88ab\u52a8\u4f20\u611f\u6280\u672f\u7684\u9996\u6b21\u5168\u9762\u8bc4\u4f30\uff0c\u7279\u522b\u9488\u5bf9\u9ad8\u7eac\u5ea6\u6708\u7403\u533a\u57df\u7684\u63a2\u7d22\u4efb\u52a1\u3002", "conclusion": "\u8bc4\u4f30\u4e86SPAD\u76f8\u673a\u5728\u5373\u5c06\u5230\u6765\u7684\u6708\u7403\u63a2\u7d22\u4efb\u52a1\u4e2d\u5e94\u5bf9\u5173\u952e\u611f\u77e5\u6311\u6218\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u5e76\u5728\u4ee3\u8868\u6027\u5149\u7167\u6761\u4ef6\u4e0b\u57fa\u51c6\u6d4b\u8bd5\u5176\u6027\u80fd\u3002"}}
{"id": "2510.10602", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10602", "abs": "https://arxiv.org/abs/2510.10602", "authors": ["Zhuoheng Gao", "Jiyao Zhang", "Zhiyong Xie", "Hao Dong", "Zhaofei Yu", "Rongmei Chen", "Guozhang Chen", "Tiejun Huang"], "title": "SpikeGrasp: A Benchmark for 6-DoF Grasp Pose Detection from Stereo Spike Streams", "comment": null, "summary": "Most robotic grasping systems rely on converting sensor data into explicit 3D\npoint clouds, which is a computational step not found in biological\nintelligence. This paper explores a fundamentally different, neuro-inspired\nparadigm for 6-DoF grasp detection. We introduce SpikeGrasp, a framework that\nmimics the biological visuomotor pathway, processing raw, asynchronous events\nfrom stereo spike cameras, similarly to retinas, to directly infer grasp poses.\nOur model fuses these stereo spike streams and uses a recurrent spiking neural\nnetwork, analogous to high-level visual processing, to iteratively refine grasp\nhypotheses without ever reconstructing a point cloud. To validate this\napproach, we built a large-scale synthetic benchmark dataset. Experiments show\nthat SpikeGrasp surpasses traditional point-cloud-based baselines, especially\nin cluttered and textureless scenes, and demonstrates remarkable data\nefficiency. By establishing the viability of this end-to-end, neuro-inspired\napproach, SpikeGrasp paves the way for future systems capable of the fluid and\nefficient manipulation seen in nature, particularly for dynamic objects.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u795e\u7ecf\u542f\u53d1\u5f0f\u6293\u53d6\u68c0\u6d4b\u6846\u67b6SpikeGrasp\uff0c\u901a\u8fc7\u5904\u7406\u7acb\u4f53\u8109\u51b2\u76f8\u673a\u7684\u539f\u59cb\u5f02\u6b65\u4e8b\u4ef6\uff0c\u76f4\u63a5\u63a8\u65ad\u6293\u53d6\u59ff\u6001\uff0c\u800c\u65e0\u9700\u91cd\u5efa\u70b9\u4e91\uff0c\u5c55\u793a\u4e86\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u4f18\u7684\u6027\u80fd\u3002", "motivation": "\u901a\u8fc7\u6a21\u4eff\u751f\u7269\u89c6\u89c9\u8fd0\u52a8\u901a\u8def\uff0c\u63a2\u7d22\u4e00\u79cd\u65b0\u76846\u81ea\u7531\u5ea6\u6293\u53d6\u68c0\u6d4b\u65b9\u6cd5\uff0c\u907f\u514d\u4f20\u7edf\u7684\u70b9\u4e91\u91cd\u5efa\u6b65\u9aa4\u3002", "method": "SpikeGrasp\u6846\u67b6", "result": "SpikeGrasp\u5728\u590d\u6742\u548c\u65e0\u7eb9\u7406\u573a\u666f\u4e2d\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u57fa\u4e8e\u70b9\u4e91\u7684\u57fa\u7ebf\uff0c\u663e\u793a\u51fa\u663e\u8457\u7684\u6570\u636e\u6548\u7387\u3002", "conclusion": "SpikeGrasp\u9a8c\u8bc1\u4e86\u7aef\u5230\u7aef\u795e\u7ecf\u542f\u53d1\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765\u81ea\u7136\u754c\u9ad8\u6548\u6d41\u7545\u7684\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u52a8\u6001\u7269\u4f53\u65f6\u3002"}}
{"id": "2510.10637", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10637", "abs": "https://arxiv.org/abs/2510.10637", "authors": ["Haoyu Zhao", "Cheng Zeng", "Linghao Zhuang", "Yaxi Zhao", "Shengke Xue", "Hao Wang", "Xingyue Zhao", "Zhongyu Li", "Kehan Li", "Siteng Huang", "Mingxiu Chen", "Xin Li", "Deli Zhao", "Hua Zou"], "title": "High-Fidelity Simulated Data Generation for Real-World Zero-Shot Robotic Manipulation Learning with Gaussian Splatting", "comment": "13 pages, 6 figures", "summary": "The scalability of robotic learning is fundamentally bottlenecked by the\nsignificant cost and labor of real-world data collection. While simulated data\noffers a scalable alternative, it often fails to generalize to the real world\ndue to significant gaps in visual appearance, physical properties, and object\ninteractions. To address this, we propose RoboSimGS, a novel Real2Sim2Real\nframework that converts multi-view real-world images into scalable,\nhigh-fidelity, and physically interactive simulation environments for robotic\nmanipulation. Our approach reconstructs scenes using a hybrid representation:\n3D Gaussian Splatting (3DGS) captures the photorealistic appearance of the\nenvironment, while mesh primitives for interactive objects ensure accurate\nphysics simulation. Crucially, we pioneer the use of a Multi-modal Large\nLanguage Model (MLLM) to automate the creation of physically plausible,\narticulated assets. The MLLM analyzes visual data to infer not only physical\nproperties (e.g., density, stiffness) but also complex kinematic structures\n(e.g., hinges, sliding rails) of objects. We demonstrate that policies trained\nentirely on data generated by RoboSimGS achieve successful zero-shot\nsim-to-real transfer across a diverse set of real-world manipulation tasks.\nFurthermore, data from RoboSimGS significantly enhances the performance and\ngeneralization capabilities of SOTA methods. Our results validate RoboSimGS as\na powerful and scalable solution for bridging the sim-to-real gap.", "AI": {"tldr": "RoboSimGS\u662f\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u771f\u5b9e\u56fe\u50cf\u8f6c\u6362\u4e3a\u9ad8\u4fdd\u771f\u4eff\u771f\u73af\u5883\uff0c\u4ee5\u5e2e\u52a9\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8f6c\u79fb\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u5b9e\u9645\u6570\u636e\u6536\u96c6\u6210\u672c\u548c\u52b3\u52a8\u7684\u74f6\u9888\uff0c\u540c\u65f6\u514b\u670d\u6a21\u62df\u6570\u636e\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\u3002", "method": "RoboSimGS\u6846\u67b6\u5c06\u591a\u89c6\u89d2\u7684\u73b0\u5b9e\u4e16\u754c\u56fe\u50cf\u8f6c\u6362\u4e3a\u53ef\u6269\u5c55\u7684\u9ad8\u4fdd\u771f\u7269\u7406\u4ea4\u4e92\u4eff\u771f\u73af\u5883\u3002", "result": "\u4f7f\u7528RoboSimGS\u751f\u6210\u7684\u6570\u636e\u4f7f\u5f97\u5728\u591a\u79cd\u73b0\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6210\u529f\u7684\u96f6-shot\u6a21\u62df\u5230\u73b0\u5b9e\u8f6c\u79fb\uff0c\u5e76\u63d0\u5347\u4e86\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "RoboSimGS\u88ab\u9a8c\u8bc1\u4e3a\u5f25\u5408\u6a21\u62df\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u5dee\u8ddd\u7684\u5f3a\u5927\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.10642", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10642", "abs": "https://arxiv.org/abs/2510.10642", "authors": ["Jianke Zhang", "Yucheng Hu", "Yanjiang Guo", "Xiaoyu Chen", "Yichen Liu", "Wenna Chen", "Chaochao Lu", "Jianyu Chen"], "title": "UniCoD: Enhancing Robot Policy via Unified Continuous and Discrete Representation Learning", "comment": null, "summary": "Building generalist robot policies that can handle diverse tasks in\nopen-ended environments is a central challenge in robotics. To leverage\nknowledge from large-scale pretraining, prior work has typically built\ngeneralist policies either on top of vision-language understanding models\n(VLMs) or generative models. However, both semantic understanding from\nvision-language pretraining and visual dynamics modeling from visual-generation\npretraining are crucial for embodied robots. Recent unified models of\ngeneration and understanding have demonstrated strong capabilities in both\ncomprehension and generation through large-scale pretraining. We posit that\nrobotic policy learning can likewise benefit from the combined strengths of\nunderstanding, planning and continuous future representation learning. Building\non this insight, we introduce UniCoD, which acquires the ability to dynamically\nmodel high-dimensional visual features through pretraining on over 1M\ninternet-scale instructional manipulation videos. Subsequently, UniCoD is\nfine-tuned on data collected from the robot embodiment, enabling the learning\nof mappings from predictive representations to action tokens. Extensive\nexperiments show our approach consistently outperforms baseline methods in\nterms of 9\\% and 12\\% across simulation environments and real-world\nout-of-distribution tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u6846\u67b6UniCoD\uff0c\u5229\u7528\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6570\u636e\u4ece\u800c\u63d0\u9ad8\u673a\u5668\u4eba\u5904\u7406\u591a\u6837\u4efb\u52a1\u7684\u80fd\u529b\u3002", "motivation": "\u8003\u8651\u5230\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u548c\u89c6\u89c9\u751f\u6210\u9884\u8bad\u7ec3\u7684\u8bed\u4e49\u7406\u89e3\u4e0e\u89c6\u89c9\u52a8\u6001\u5efa\u6a21\u5bf9\u673a\u5668\u4eba\u5177\u6709\u91cd\u8981\u6027\uff0c\u56e0\u6b64\u7ed3\u5408\u7406\u89e3\u3001\u89c4\u5212\u548c\u672a\u6765\u8868\u793a\u5b66\u4e60\u7684\u4f18\u52bf\u662f\u6709\u5fc5\u8981\u7684\u3002", "method": "UniCoD\u901a\u8fc7\u9884\u8bad\u7ec3\u4e8e\u8d85\u8fc7100\u4e07\u4e2a\u4e92\u8054\u7f51\u89c4\u6a21\u7684\u64cd\u4f5c\u89c6\u9891\u6765\u5efa\u6a21\u9ad8\u7ef4\u89c6\u89c9\u7279\u5f81\uff0c\u5e76\u5728\u673a\u5668\u4eba\u5177\u4f53\u5316\u540e\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u5b66\u4e60\u9884\u6d4b\u8868\u793a\u548c\u884c\u52a8\u7b26\u53f7\u4e4b\u95f4\u7684\u6620\u5c04\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cUniCoD\u5728\u4e0d\u540c\u6d4b\u8bd5\u73af\u5883\u4e0b\u7684\u8868\u73b0\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u4e869%\u523012%\u3002", "conclusion": "UniCoD\u5728\u6a21\u62df\u73af\u5883\u548c\u73b0\u5b9e\u4e16\u754c\u7684\u51fa\u533a\u95f4\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.10716", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10716", "abs": "https://arxiv.org/abs/2510.10716", "authors": ["Christopher Thierauf"], "title": "Deployment and Development of a Cognitive Teleoreactive Framework for Deep Sea Autonomy", "comment": null, "summary": "A new AUV mission planning and execution software has been tested on AUV\nSentry. Dubbed DINOS-R, it draws inspiration from cognitive architectures and\nAUV control systems to replace the legacy MC architecture. Unlike these\nexisting architectures, however, DINOS-R is built from the ground-up to unify\nsymbolic decision making (for understandable, repeatable, provable behavior)\nwith machine learning techniques and reactive behaviors, for field-readiness\nacross oceanographic platforms. Implemented primarily in Python3, DINOS-R is\nextensible, modular, and reusable, with an emphasis on non-expert use as well\nas growth for future research in oceanography and robot algorithms. Mission\nspecification is flexible, and can be specified declaratively. Behavior\nspecification is similarly flexible, supporting simultaneous use of real-time\ntask planning and hard-coded user specified plans. These features were\ndemonstrated in the field on Sentry, in addition to a variety of simulated\ncases. These results are discussed, and future work is outlined.", "AI": {"tldr": "DINOS-R \u662f\u4e00\u79cd\u65b0\u7684 AUV \u4efb\u52a1\u89c4\u5212\u8f6f\u4ef6\uff0c\u5b83\u7ed3\u5408\u4e86\u7b26\u53f7\u51b3\u7b56\u4e0e\u673a\u5668\u5b66\u4e60\uff0c\u65e8\u5728\u63d0\u4f9b\u7075\u6d3b\u7684\u4efb\u52a1\u548c\u884c\u4e3a\u89c4\u8303\uff0c\u5e76\u5df2\u5728\u5b9e\u9645\u548c\u6a21\u62df\u6848\u4f8b\u4e2d\u6d4b\u8bd5\u3002", "motivation": "\u4e3a\u4e86\u66ff\u4ee3\u73b0\u6709\u7684 MC \u67b6\u6784\uff0c\u5f00\u53d1 DINOS-R \u4ee5\u7edf\u4e00\u7b26\u53f7\u51b3\u7b56\u4e0e\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u4ece\u800c\u63d0\u5347 AUV \u7684\u73b0\u573a\u6267\u884c\u80fd\u529b\u548c\u7814\u7a76\u7075\u6d3b\u6027\u3002", "method": "\u4f7f\u7528 Python3 \u5f00\u53d1\uff0cDINOS-R \u8bbe\u8ba1\u4e3a\u53ef\u6269\u5c55\u3001\u6a21\u5757\u5316\u548c\u53ef\u91cd\u7528\uff0c\u652f\u6301\u975e\u4e13\u5bb6\u4f7f\u7528\uff0c\u4e14\u80fd\u591f\u7075\u6d3b\u6307\u5b9a\u4efb\u52a1\u548c\u884c\u4e3a\u3002", "result": "DINOS-R \u662f\u4e00\u4e2a\u65b0\u5f00\u53d1\u7684 AUV \u4efb\u52a1\u89c4\u5212\u548c\u6267\u884c\u8f6f\u4ef6\uff0c\u7ecf\u8fc7\u5728 AUV Sentry \u4e0a\u7684\u6d4b\u8bd5\uff0c\u65e8\u5728\u53d6\u4ee3\u4f20\u7edf\u7684 MC \u67b6\u6784\u3002\u5176\u4e3b\u8981\u7279\u70b9\u662f\u5c06\u7b26\u53f7\u51b3\u7b56\u4e0e\u673a\u5668\u5b66\u4e60\u6280\u672f\u76f8\u7ed3\u5408\uff0c\u4ee5\u5b9e\u73b0\u6d77\u6d0b\u5e73\u53f0\u7684\u73b0\u573a\u5e94\u7528\u80fd\u529b\u3002", "conclusion": "DINOS-R \u7684\u6d4b\u8bd5\u7ed3\u679c\u8868\u660e\uff0c\u5b83\u5728\u4efb\u52a1\u89c4\u5212\u548c\u6267\u884c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5177\u5907\u826f\u597d\u7684\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u672a\u6765\u7684\u6d77\u6d0b\u7814\u7a76\u548c\u673a\u5668\u4eba\u7b97\u6cd5\u3002"}}
{"id": "2510.10731", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10731", "abs": "https://arxiv.org/abs/2510.10731", "authors": ["Yongxi Cao", "Julian F. Schumann", "Jens Kober", "Joni Pajarinen", "Arkady Zgonnikov"], "title": "Controllable Generative Trajectory Prediction via Weak Preference Alignment", "comment": null, "summary": "Deep generative models such as conditional variational autoencoders (CVAEs)\nhave shown great promise for predicting trajectories of surrounding agents in\nautonomous vehicle planning. State-of-the-art models have achieved remarkable\naccuracy in such prediction tasks. Besides accuracy, diversity is also crucial\nfor safe planning because human behaviors are inherently uncertain and\nmultimodal. However, existing methods generally lack a scheme to generate\ncontrollably diverse trajectories, which is arguably more useful than randomly\ndiversified trajectories, to the end of safe planning. To address this, we\npropose PrefCVAE, an augmented CVAE framework that uses weakly labeled\npreference pairs to imbue latent variables with semantic attributes. Using\naverage velocity as an example attribute, we demonstrate that PrefCVAE enables\ncontrollable, semantically meaningful predictions without degrading baseline\naccuracy. Our results show the effectiveness of preference supervision as a\ncost-effective way to enhance sampling-based generative models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08PrefCVAE\uff09\uff0c\u7528\u4e8e\u751f\u6210\u53ef\u63a7\u7684\u591a\u6837\u5316\u8f68\u8ff9\uff0c\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u5b89\u5168\u89c4\u5212\u80fd\u529b\u3002", "motivation": "\u5728\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u89c4\u5212\u4e2d\uff0c\u9664\u4e86\u51c6\u786e\u6027\uff0c\u8f68\u8ff9\u7684\u591a\u6837\u6027\u540c\u6837\u91cd\u8981\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u751f\u6210\u53ef\u63a7\u591a\u6837\u5316\u8f68\u8ff9\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u589e\u5f3a\u7684\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u6846\u67b6\uff0c\u901a\u8fc7\u5f31\u6807\u8bb0\u7684\u504f\u597d\u5bf9\u6f5c\u5728\u53d8\u91cf\u8fdb\u884c\u8bed\u4e49\u5c5e\u6027\u7684\u8d4b\u503c\u3002", "result": "PrefCVAE\u5c55\u793a\u4e86\u504f\u597d\u76d1\u7763\u4f5c\u4e3a\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u7684\u65b9\u5f0f\uff0c\u663e\u8457\u589e\u5f3a\u91c7\u6837\u751f\u6210\u6a21\u578b\u7684\u6548\u679c\u3002", "conclusion": "PrefCVAE\u5229\u7528\u5f31\u6807\u7b7e\u7684\u504f\u597d\u5bf9\u6f5c\u5728\u53d8\u91cf\u8fdb\u884c\u8bed\u4e49\u5c5e\u6027\u8d4b\u4e88\uff0c\u5c55\u793a\u4e86\u53ef\u63a7\u3001\u8bed\u4e49\u6709\u610f\u4e49\u7684\u9884\u6d4b\u6548\u679c\uff0c\u800c\u4e0d\u964d\u4f4e\u51c6\u786e\u6027\u3002"}}
{"id": "2510.10759", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10759", "abs": "https://arxiv.org/abs/2510.10759", "authors": ["Arthicha Srisuchinnawong", "Poramate Manoonpong"], "title": "Gain Tuning Is Not What You Need: Reward Gain Adaptation for Constrained Locomotion Learning", "comment": "RSS 2025", "summary": "Existing robot locomotion learning techniques rely heavily on the offline\nselection of proper reward weighting gains and cannot guarantee constraint\nsatisfaction (i.e., constraint violation) during training. Thus, this work aims\nto address both issues by proposing Reward-Oriented Gains via Embodied\nRegulation (ROGER), which adapts reward-weighting gains online based on\npenalties received throughout the embodied interaction process. The ratio\nbetween the positive reward (primary reward) and negative reward (penalty)\ngains is automatically reduced as the learning approaches the constraint\nthresholds to avoid violation. Conversely, the ratio is increased when learning\nis in safe states to prioritize performance. With a 60-kg quadruped robot,\nROGER achieved near-zero constraint violation throughout multiple learning\ntrials. It also achieved up to 50% more primary reward than the equivalent\nstate-of-the-art techniques. In MuJoCo continuous locomotion benchmarks,\nincluding a single-leg hopper, ROGER exhibited comparable or up to 100% higher\nperformance and 60% less torque usage and orientation deviation compared to\nthose trained with the default reward function. Finally, real-world locomotion\nlearning of a physical quadruped robot was achieved from scratch within one\nhour without any falls. Therefore, this work contributes to\nconstraint-satisfying real-world continual robot locomotion learning and\nsimplifies reward weighting gain tuning, potentially facilitating the\ndevelopment of physical robots and those that learn in the real world.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u8c03\u6574\u7684\u5956\u52b1\u52a0\u6743\u589e\u76ca\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5b66\u4e60\u6027\u80fd\u4e0e\u7ea6\u675f\u6ee1\u8db3\u80fd\u529b\uff0c\u7b80\u5316\u4e86\u8c03\u8282\u8fc7\u7a0b\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u5b66\u4e60\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u5b66\u4e60\u6280\u672f\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5bf9\u5956\u52b1\u52a0\u6743\u589e\u76ca\u7684\u79bb\u7ebf\u9009\u62e9\u4f9d\u8d56\u6027\u5f3a\uff0c\u4e14\u65e0\u6cd5\u4fdd\u8bc1\u7ea6\u675f\u7684\u6ee1\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u5bfb\u6c42\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5728\u7ebf\u8c03\u6574\u5956\u52b1\u52a0\u6743\u589e\u76ca\uff0c\u4f9d\u636e\u673a\u5668\u4eba\u5728\u4ea4\u4e92\u8fc7\u7a0b\u4e2d\u7684\u60e9\u7f5a\uff0c\u52a8\u6001\u63a7\u5236\u6b63\u5956\u52b1\u4e0e\u8d1f\u5956\u52b1\u7684\u6bd4\u7387\u3002", "result": "ROGER\u572860\u5343\u514b\u7684\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u8fd1\u96f6\u7684\u7ea6\u675f\u8fdd\u53cd\uff0c\u76f8\u8f83\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5176\u83b7\u5f97\u7684\u6b63\u5956\u52b1\u63d0\u9ad8\u4e8650%\uff0c\u5e76\u5728MuJoCo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u6700\u9ad8\u53ef\u8fbe100%\u7684\u6027\u80fd\u63d0\u5347\u548c60%\u7684\u626d\u77e9\u4f7f\u7528\u51cf\u5c11\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684ROGER\u65b9\u6cd5\u5728\u4fdd\u8bc1\u7ea6\u675f\u6ee1\u8db3\u7684\u524d\u63d0\u4e0b\uff0c\u63d0\u5347\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u5e76\u7b80\u5316\u4e86\u5956\u52b1\u6743\u91cd\u8c03\u8282\u7684\u8fc7\u7a0b\uff0c\u5c55\u793a\u4e86\u5176\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.10778", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10778", "abs": "https://arxiv.org/abs/2510.10778", "authors": ["Christopher D. Hsu", "Pratik Chaudhari"], "title": "Real2USD: Scene Representations in Universal Scene Description Language", "comment": "8 pages, 10 figures, 1 table", "summary": "Large Language Models (LLMs) can help robots reason about abstract task\nspecifications. This requires augmenting classical representations of the\nenvironment used by robots with natural language-based priors. There are a\nnumber of existing approaches to doing so, but they are tailored to specific\ntasks, e.g., visual-language models for navigation, language-guided neural\nradiance fields for mapping, etc. This paper argues that the Universal Scene\nDescription (USD) language is an effective and general representation of\ngeometric, photometric and semantic information in the environment for\nLLM-based robotics tasks. Our argument is simple: a USD is an XML-based scene\ngraph, readable by LLMs and humans alike, and rich enough to support\nessentially any task -- Pixar developed this language to store assets, scenes\nand even movies. We demonstrate a ``Real to USD'' system using a Unitree Go2\nquadruped robot carrying LiDAR and a RGB camera that (i) builds an explicit USD\nrepresentation of indoor environments with diverse objects and challenging\nsettings with lots of glass, and (ii) parses the USD using Google's Gemini to\ndemonstrate scene understanding, complex inferences, and planning. We also\nstudy different aspects of this system in simulated warehouse and hospital\nsettings using Nvidia's Issac Sim. Code is available at\nhttps://github.com/grasp-lyrl/Real2USD .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eUniversal Scene Description (USD)\u8bed\u8a00\u7684\u7cfb\u7edf\uff0c\u65e8\u5728\u5e2e\u52a9\u673a\u5668\u4eba\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u62bd\u8c61\u4efb\u52a1\u89c4\u8303\u3002", "motivation": "\u4e0d\u540c\u7684\u4efb\u52a1\u9700\u8981\u4e0d\u540c\u7684\u73af\u5883\u8868\u793a\uff0c\u800cUSD\u4f5c\u4e3a\u4e00\u79cd\u901a\u7528\u8868\u793a\uff0c\u53ef\u4ee5\u6ee1\u8db3\u8fd9\u4e9b\u9700\u6c42\u3002", "method": "\u901a\u8fc7Real to USD\u7cfb\u7edf\uff0c\u4f7f\u7528Unitree Go2\u56db\u8db3\u673a\u5668\u4eba\u914d\u5907LiDAR\u548cRGB\u6444\u50cf\u5934\u6784\u5efa\u5ba4\u5185\u73af\u5883\u7684USD\u8868\u793a\uff0c\u5e76\u4f7f\u7528Google\u7684Gemini\u89e3\u6790USD\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u590d\u6742\u7684\u5ba4\u5185\u73af\u5883USD\u8868\u793a\uff0c\u5e76\u5c55\u793a\u4e86\u573a\u666f\u7406\u89e3\u3001\u590d\u6742\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\u3002", "conclusion": "USD\u4f5c\u4e3a\u4e00\u79cd\u901a\u7528\u7684\u573a\u666f\u8868\u793a\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u652f\u6301\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u673a\u5668\u4eba\u4efb\u52a1\u3002"}}
{"id": "2510.10781", "categories": ["cs.RO", "cs.MA", "cs.SY", "eess.SY", "I.2.9; I.2.11"], "pdf": "https://arxiv.org/pdf/2510.10781", "abs": "https://arxiv.org/abs/2510.10781", "authors": ["Douglas Hutchings", "Luai Abuelsamen", "Karthik Rajgopal"], "title": "Two-Layer Voronoi Coverage Control for Hybrid Aerial-Ground Robot Teams in Emergency Response: Implementation and Analysis", "comment": "23 pages, 7 figures. Technical report with complete implementation\n  details and open-source code", "summary": "We present a comprehensive two-layer Voronoi coverage control approach for\ncoordinating hybrid aerial-ground robot teams in hazardous material emergency\nresponse scenarios. Traditional Voronoi coverage control methods face three\ncritical limitations in emergency contexts: heterogeneous agent capabilities\nwith vastly different velocities, clustered initial deployment configurations,\nand urgent time constraints requiring rapid response rather than eventual\nconvergence. Our method addresses these challenges through a decoupled\ntwo-layer architecture that separately optimizes aerial and ground robot\npositioning, with aerial agents delivering ground sensors via airdrop to\nhigh-priority locations. We provide detailed implementation of bounded Voronoi\ncell computation, efficient numerical integration techniques for\nimportance-weighted centroids, and robust control strategies that prevent agent\ntrapping. Simulation results demonstrate an 88% reduction in response time,\nachieving target sensor coverage (18.5% of initial sensor loss) in 25 seconds\ncompared to 220 seconds for ground-only deployment. Complete implementation\ncode is available at https://github.com/dHutchings/ME292B.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u4e24\u5c42Voronoi\u8986\u76d6\u63a7\u5236\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u7a7a\u4e2d\u548c\u5730\u9762\u673a\u5668\u4eba\u5728\u5371\u9669\u7269\u8d28\u5e94\u6025\u54cd\u5e94\u4e2d\u7684\u534f\u8c03\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfVoronoi\u8986\u76d6\u63a7\u5236\u65b9\u6cd5\u5728\u7d27\u6025\u60c5\u51b5\u4e0b\u9762\u4e34\u7684\u4e09\u5927\u9650\u5236\uff1a\u5f02\u6784\u4ee3\u7406\u80fd\u529b\u3001\u805a\u96c6\u90e8\u7f72\u914d\u7f6e\uff0c\u4ee5\u53ca\u7d27\u8feb\u7684\u65f6\u95f4\u8981\u6c42\u3002", "method": "\u4f7f\u7528\u89e3\u8026\u7684\u4e24\u5c42\u67b6\u6784\uff0c\u5206\u522b\u4f18\u5316\u7a7a\u4e2d\u548c\u5730\u9762\u673a\u5668\u4eba\u7684\u4f4d\u7f6e\uff0c\u901a\u8fc7\u7a7a\u6295\u5c06\u5730\u9762\u4f20\u611f\u5668\u9001\u81f3\u9ad8\u4f18\u5148\u7ea7\u4f4d\u7f6e\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u9762\u7684\u4e24\u5c42Voronoi\u8986\u76d6\u63a7\u5236\u65b9\u6cd5\uff0c\u7528\u4e8e\u534f\u8c03\u6df7\u5408\u7a7a\u4e2d-\u5730\u9762\u673a\u5668\u4eba\u56e2\u961f\u5728\u5371\u9669\u7269\u8d28\u5e94\u6025\u54cd\u5e94\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "\u901a\u8fc7\u6539\u8fdb\u7684Voronoi\u63a7\u5236\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e94\u6025\u54cd\u5e94\u7684\u6548\u7387\uff0c\u5b9e\u73b0\u572825\u79d2\u5185\u8fbe\u6210\u76ee\u6807\u4f20\u611f\u5668\u8986\u76d6\u3002"}}
{"id": "2510.10804", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10804", "abs": "https://arxiv.org/abs/2510.10804", "authors": ["Alessandro Albini", "Mohsen Kaboli", "Giorgio Cannata", "Perla Maiolino"], "title": "Representing Data in Robotic Tactile Perception -- A Review", "comment": null, "summary": "Robotic tactile perception is a complex process involving several\ncomputational steps performed at different levels. Tactile information is\nshaped by the interplay of robot actions, the mechanical properties of its\nbody, and the software that processes the data. In this respect, high-level\ncomputation, required to process and extract information, is commonly performed\nby adapting existing techniques from other domains, such as computer vision,\nwhich expects input data to be properly structured. Therefore, it is necessary\nto transform tactile sensor data to match a specific data structure. This\noperation directly affects the tactile information encoded and, as a\nconsequence, the task execution. This survey aims to address this specific\naspect of the tactile perception pipeline, namely Data Representation. The\npaper first clearly defines its contributions to the perception pipeline and\nthen reviews how previous studies have dealt with the problem of representing\ntactile information, investigating the relationships among hardware,\nrepresentations, and high-level computation methods. The analysis has led to\nthe identification of six structures commonly used in the literature to\nrepresent data. The manuscript provides discussions and guidelines for properly\nselecting a representation depending on operating conditions, including the\navailable hardware, the tactile information required to be encoded, and the\ntask at hand.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u673a\u5668\u4eba\u89e6\u89c9\u611f\u77e5\u4e2d\u7684\u6570\u636e\u8868\u793a\u95ee\u9898\uff0c\u5206\u6790\u5404\u79cd\u786c\u4ef6\u4e0e\u8868\u793a\u65b9\u5f0f\u7684\u5173\u7cfb\u3002", "motivation": "\u63d0\u5347\u673a\u5668\u4eba\u7684\u89e6\u89c9\u611f\u77e5\u80fd\u529b\uff0c\u4f18\u5316\u6570\u636e\u8868\u793a\u4ee5\u63d0\u9ad8\u4efb\u52a1\u6267\u884c\u6548\u679c\u3002", "method": "\u901a\u8fc7\u7efc\u8ff0\u4ee5\u5f80\u7814\u7a76\uff0c\u8bc6\u522b\u89e6\u89c9\u4fe1\u606f\u8868\u793a\u7684\u516d\u79cd\u5e38\u7528\u7ed3\u6784\uff0c\u5e76\u5206\u6790\u5b83\u4eec\u4e0e\u786c\u4ef6\u548c\u9ad8\u5c42\u8ba1\u7b97\u65b9\u6cd5\u7684\u5173\u7cfb\u3002", "result": "\u786e\u5b9a\u4e86\u516d\u79cd\u5e38\u7528\u7684\u6570\u636e\u8868\u793a\u7ed3\u6784\uff0c\u5e76\u63d0\u4f9b\u4e86\u9009\u62e9\u65f6\u7684\u6307\u5bfc\u5efa\u8bae\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u5408\u7406\u9009\u62e9\u6570\u636e\u8868\u793a\u65b9\u6cd5\u7684\u8ba8\u8bba\u548c\u6307\u5bfc\uff0c\u8fd9\u5bf9\u4e8e\u63d0\u9ad8\u89e6\u89c9\u611f\u77e5\u548c\u4efb\u52a1\u6267\u884c\u7684\u6709\u6548\u6027\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.10843", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10843", "abs": "https://arxiv.org/abs/2510.10843", "authors": ["Jared Grinberg", "Yanran Ding"], "title": "Contact Sensing via Joint Torque Sensors and a Force/Torque Sensor for Legged Robots", "comment": "Proc. IEEE 21st International Conference on Automation Science and\n  Engineering (CASE), Los Angeles, CA, USA, Aug. 17-21, 2025, pp. 1-7,\n  doi:10.1109/CASE58245.2025.11164031", "summary": "This paper presents a method for detecting and localizing contact along robot\nlegs using distributed joint torque sensors and a single hip-mounted\nforce-torque (FT) sensor using a generalized momentum-based observer framework.\nWe designed a low-cost strain-gauge-based joint torque sensor that can be\ninstalled on every joint to provide direct torque measurements, eliminating the\nneed for complex friction models and providing more accurate torque readings\nthan estimation based on motor current. Simulation studies on a floating-based\n2-DoF robot leg verified that the proposed framework accurately recovers\ncontact force and location along the thigh and shin links. Through a\ncalibration procedure, our torque sensor achieved an average 96.4% accuracy\nrelative to ground truth measurements. Building upon the torque sensor, we\nperformed hardware experiments on a 2-DoF manipulator, which showed\nsub-centimeter contact localization accuracy and force errors below 0.2 N.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u673a\u5668\u4eba\u817f\u90e8\u63a5\u89e6\u68c0\u6d4b\u548c\u5b9a\u4f4d\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u5173\u8282\u626d\u77e9\u4f20\u611f\u5668\u548c\u529b-\u626d\u77e9\u4f20\u611f\u5668\uff0c\u8fbe\u5230\u9ad8\u7cbe\u5ea6\u7684\u63a5\u89e6\u529b\u548c\u4f4d\u7f6e\u6d4b\u91cf\u3002", "motivation": "\u63d0\u51fa\u4e00\u79cd\u4f4e\u6210\u672c\u7684\u57fa\u4e8e\u5e94\u53d8\u8ba1\u7684\u5173\u8282\u626d\u77e9\u4f20\u611f\u5668\uff0c\u80fd\u591f\u76f4\u63a5\u6d4b\u91cf\u626d\u77e9\uff0c\u907f\u514d\u4f7f\u7528\u590d\u6742\u7684\u6469\u64e6\u6a21\u578b\uff0c\u63d0\u4f9b\u6bd4\u57fa\u4e8e\u7535\u673a\u7535\u6d41\u4f30\u7b97\u66f4\u51c6\u786e\u7684\u626d\u77e9\u8bfb\u6570\u3002", "method": "\u4f7f\u7528\u5206\u5e03\u5f0f\u5173\u8282\u626d\u77e9\u4f20\u611f\u5668\u548c\u5355\u4e2a\u81c0\u90e8\u5b89\u88c5\u7684\u529b-\u626d\u77e9\u4f20\u611f\u5668\u68c0\u6d4b\u548c\u5b9a\u4f4d\u673a\u5668\u4eba\u817f\u90e8\u63a5\u89e6\uff0c\u91c7\u7528\u5e7f\u4e49\u52a8\u91cf\u89c2\u6d4b\u5668\u6846\u67b6\u3002", "result": "\u5728\u6d6e\u52a8\u5f0f\u4e8c\u7ef4\u5173\u8282\u673a\u5668\u4eba\u817f\u7684\u4eff\u771f\u7814\u7a76\u4e2d\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u80fd\u51c6\u786e\u6062\u590d\u5927\u817f\u548c\u5c0f\u817f\u94fe\u63a5\u7684\u63a5\u89e6\u529b\u548c\u4f4d\u7f6e\uff1b\u7ecf\u8fc7\u6807\u5b9a\u7a0b\u5e8f\u540e\uff0c\u626d\u77e9\u4f20\u611f\u5668\u76f8\u5bf9\u4e8e\u771f\u5b9e\u6d4b\u91cf\u503c\u8fbe\u5230\u4e8696.4%\u7684\u5e73\u5747\u51c6\u786e\u5ea6\uff1b\u5728\u4e8c\u7ef4\u64cd\u7eb5\u5668\u4e0a\u7684\u786c\u4ef6\u5b9e\u9a8c\u663e\u793a\uff0c\u63a5\u89e6\u5b9a\u4f4d\u7cbe\u5ea6\u5728\u4e9a\u5398\u7c73\u7ea7\uff0c\u529b\u8bef\u5dee\u4f4e\u4e8e0.2 N\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u626d\u77e9\u4f20\u611f\u5668\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u817f\u90e8\u7684\u63a5\u89e6\u529b\u548c\u4f4d\u7f6e\u611f\u77e5\u7cbe\u5ea6\u3002"}}
{"id": "2510.10851", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10851", "abs": "https://arxiv.org/abs/2510.10851", "authors": ["Tingxuan Leng", "Yushi Wang", "Tinglong Zheng", "Changsheng Luo", "Mingguo Zhao"], "title": "Preference-Conditioned Multi-Objective RL for Integrated Command Tracking and Force Compliance in Humanoid Locomotion", "comment": null, "summary": "Humanoid locomotion requires not only accurate command tracking for\nnavigation but also compliant responses to external forces during human\ninteraction. Despite significant progress, existing RL approaches mainly\nemphasize robustness, yielding policies that resist external forces but lack\ncompliance-particularly challenging for inherently unstable humanoids. In this\nwork, we address this by formulating humanoid locomotion as a multi-objective\noptimization problem that balances command tracking and external force\ncompliance. We introduce a preference-conditioned multi-objective RL (MORL)\nframework that integrates rigid command following and compliant behaviors\nwithin a single omnidirectional locomotion policy. External forces are modeled\nvia velocity-resistance factor for consistent reward design, and training\nleverages an encoder-decoder structure that infers task-relevant privileged\nfeatures from deployable observations. We validate our approach in both\nsimulation and real-world experiments on a humanoid robot. Experimental results\nindicate that our framework not only improves adaptability and convergence over\nstandard pipelines, but also realizes deployable preference-conditioned\nhumanoid locomotion.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5e73\u8861\u547d\u4ee4\u8ddf\u8e2a\u548c\u5916\u90e8\u529b\u987a\u5e94\u6027\u7684\u4eba\u5f62\u673a\u5668\u4eba\u884c\u8d70\u591a\u76ee\u6807\u4f18\u5316\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5f3a\u8c03\u9c81\u68d2\u6027\uff0c\u7f3a\u4e4f\u5bf9\u5916\u90e8\u529b\u91cf\u7684\u987a\u5e94\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u65b9\u6cd5\u6765\u5e73\u8861\u8fd9\u4e9b\u76ee\u6807\u3002", "method": "\u901a\u8fc7\u5c06\u4eba\u5f62 locomotion \u5f62\u5f0f\u5316\u4e3a\u4e00\u4e2a\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u5f15\u5165\u504f\u597d\u6761\u4ef6\u7684\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u7f16\u7801-\u89e3\u7801\u7ed3\u6784\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4ee5\u6539\u8fdb\u4eba\u5f62\u673a\u5668\u4eba\u5728\u884c\u8d70\u65f6\u5bf9\u5916\u90e8\u529b\u91cf\u7684\u54cd\u5e94\u80fd\u529b\u3002", "conclusion": "\u9a8c\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u9002\u5e94\u6027\u548c\u6536\u655b\u6027\u65b9\u9762\u4f18\u4e8e\u6807\u51c6\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u53ef\u90e8\u7f72\u7684\u504f\u597d\u6761\u4ef6\u5316\u4eba\u5f62 locomotion\u3002"}}
{"id": "2510.10865", "categories": ["cs.RO", "cs.AI", "I.2.9; I.2.8"], "pdf": "https://arxiv.org/pdf/2510.10865", "abs": "https://arxiv.org/abs/2510.10865", "authors": ["Ahmed Alanazi", "Duy Ho", "Yugyung Lee"], "title": "GRIP: A Unified Framework for Grid-Based Relay and Co-Occurrence-Aware Planning in Dynamic Environments", "comment": "17 pages, 5 figures, 8 tables", "summary": "Robots navigating dynamic, cluttered, and semantically complex environments\nmust integrate perception, symbolic reasoning, and spatial planning to\ngeneralize across diverse layouts and object categories. Existing methods often\nrely on static priors or limited memory, constraining adaptability under\npartial observability and semantic ambiguity. We present GRIP, Grid-based Relay\nwith Intermediate Planning, a unified, modular framework with three scalable\nvariants: GRIP-L (Lightweight), optimized for symbolic navigation via semantic\noccupancy grids; GRIP-F (Full), supporting multi-hop anchor chaining and\nLLM-based introspection; and GRIP-R (Real-World), enabling physical robot\ndeployment under perceptual uncertainty. GRIP integrates dynamic 2D grid\nconstruction, open-vocabulary object grounding, co-occurrence-aware symbolic\nplanning, and hybrid policy execution using behavioral cloning, D* search, and\ngrid-conditioned control. Empirical results on AI2-THOR and RoboTHOR benchmarks\nshow that GRIP achieves up to 9.6% higher success rates and over $2\\times$\nimprovement in path efficiency (SPL and SAE) on long-horizon tasks. Qualitative\nanalyses reveal interpretable symbolic plans in ambiguous scenes. Real-world\ndeployment on a Jetbot further validates GRIP's generalization under sensor\nnoise and environmental variation. These results position GRIP as a robust,\nscalable, and explainable framework bridging simulation and real-world\nnavigation.", "AI": {"tldr": "GRIP\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u673a\u5668\u4eba\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u611f\u77e5\u548c\u7b26\u53f7\u63a8\u7406\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\u3002", "motivation": "\u673a\u5668\u4eba\u5728\u52a8\u6001\u3001\u6742\u4e71\u548c\u8bed\u4e49\u590d\u6742\u7684\u73af\u5883\u4e2d\u5bfc\u822a\uff0c\u5fc5\u987b\u6574\u5408\u611f\u77e5\u3001\u7b26\u53f7\u63a8\u7406\u548c\u7a7a\u95f4\u89c4\u5212\uff0c\u4ee5\u4fbf\u5728\u591a\u6837\u5316\u7684\u5e03\u5c40\u548c\u7269\u4f53\u7c7b\u522b\u4e2d\u8fdb\u884c\u63a8\u5e7f\u3002", "method": "\u63d0\u51fa\u4e86GRIP\uff08\u57fa\u4e8e\u7f51\u683c\u7684\u4e2d\u4ecb\u89c4\u5212\uff09\uff0c\u8be5\u6846\u67b6\u6709\u4e09\u79cd\u53ef\u6269\u5c55\u7684\u53d8\u4f53\uff0c\u7ed3\u5408\u4e86\u52a8\u60012D\u7f51\u683c\u6784\u5efa\u3001\u5f00\u653e\u8bcd\u6c47\u7269\u4f53\u5b9a\u4f4d\u3001\u5171\u73b0\u610f\u8bc6\u7684\u7b26\u53f7\u89c4\u5212\u548c\u6df7\u5408\u653f\u7b56\u6267\u884c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\uff0cGRIP\u5728AI2-THOR\u548cRoboTHOR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe9.6%\u7684\u6210\u529f\u7387\u63d0\u5347\uff0c\u5e76\u5728\u957f\u65f6\u95f4\u4efb\u52a1\u4e2d\u8def\u5f84\u6548\u7387\u63d0\u9ad8\u8d85\u8fc7\u4e24\u500d\u3002", "conclusion": "GRIP\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u3001\u53ef\u6269\u5c55\u7684\u548c\u53ef\u89e3\u91ca\u7684\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5730\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u4e16\u754c\u7684\u5bfc\u822a\u4efb\u52a1\u4e2d\u5de5\u4f5c\u3002"}}
{"id": "2510.10886", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10886", "abs": "https://arxiv.org/abs/2510.10886", "authors": ["Yashom Dighe", "Youngjin Kim", "Karthik Dantu"], "title": "QuayPoints: A Reasoning Framework to Bridge the Information Gap Between Global and Local Planning in Autonomous Racing", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Autonomous racing requires tight integration between perception, planning and\ncontrol to minimize latency as well as timely decision making. A standard\nautonomy pipeline comprising a global planner, local planner, and controller\nloses information as the higher-level racing context is sequentially propagated\ndownstream into specific task-oriented context. In particular, the global\nplanner's understanding of optimality is typically reduced to a sparse set of\nwaypoints, leaving the local planner to make reactive decisions with limited\ncontext. This paper investigates whether additional global insights,\nspecifically time-optimality information, can be meaningfully passed to the\nlocal planner to improve downstream decisions. We introduce a framework that\npreserves essential global knowledge and conveys it to the local planner\nthrough QuayPoints regions where deviations from the optimal raceline result in\nsignificant compromises to optimality. QuayPoints enable local planners to make\nmore informed global decisions when deviating from the raceline, such as during\nstrategic overtaking. To demonstrate this, we integrate QuayPoints into an\nexisting planner and show that it consistently overtakes opponents traveling at\nup to 75% of the ego vehicle's speed across four distinct race tracks.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7QuayPoints\u63d0\u5347\u5c40\u90e8\u89c4\u5212\u5668\u51b3\u7b56\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u4ee5\u6539\u5584\u81ea\u52a8\u8d5b\u8f66\u4e2d\u7684\u8d85\u8f66\u8868\u73b0\u3002", "motivation": "\u63d0\u9ad8\u81ea\u52a8\u8d5b\u8f66\u4e2d\u7684\u51b3\u7b56\u6548\u7387\uff0c\u51cf\u5c11\u6807\u51c6\u81ea\u6cbb\u7ba1\u9053\u4e2d\u4fe1\u606f\u4e22\u5931\u7684\u95ee\u9898\uff0c\u4ee5\u4fbf\u5728\u590d\u6742\u573a\u666f\u4e2d\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\u3002", "method": "\u5c06QuayPoints\u6574\u5408\u5230\u73b0\u6709\u7684\u89c4\u5212\u7cfb\u7edf\u4e2d\uff0c\u901a\u8fc7\u63d0\u4f9b\u65f6\u95f4\u6700\u4f18\u6027\u4fe1\u606f\u6765\u589e\u5f3a\u5c40\u90e8\u89c4\u5212\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u56db\u4e2a\u4e0d\u540c\u7684\u8d5b\u9053\u5b9e\u9a8c\u8bc1\u660e\uff0c\u96c6\u6210QuayPoints\u540e\u7684\u89c4\u5212\u5668\u80fd\u591f\u5728\u901f\u5ea6\u8fbe\u5230\u81ea\u8f6675%\u7684\u60c5\u51b5\u4e0b\u6301\u7eed\u5b9e\u73b0\u8d85\u8f66\u3002", "conclusion": "\u5f15\u5165QuayPoints\u53ef\u4ee5\u5e2e\u52a9\u5c40\u90e8\u89c4\u5212\u5668\u5728\u504f\u79bb\u6700\u4f73\u8d5b\u8f66\u8def\u5f84\u65f6\u505a\u51fa\u66f4\u660e\u667a\u7684\u51b3\u7b56\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u8d85\u8f66\u3002"}}
{"id": "2510.10893", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10893", "abs": "https://arxiv.org/abs/2510.10893", "authors": ["Dikshant Shehmar", "Matthew E. Taylor", "Ehsan Hashemi"], "title": "An Adaptive Transition Framework for Game-Theoretic Based Takeover", "comment": null, "summary": "The transition of control from autonomous systems to human drivers is\ncritical in automated driving systems, particularly due to the out-of-the-loop\n(OOTL) circumstances that reduce driver readiness and increase reaction times.\nExisting takeover strategies are based on fixed time-based transitions, which\nfail to account for real-time driver performance variations. This paper\nproposes an adaptive transition strategy that dynamically adjusts the control\nauthority based on both the time and tracking ability of the driver trajectory.\nShared control is modeled as a cooperative differential game, where control\nauthority is modulated through time-varying objective functions instead of\nblending control torques directly. To ensure a more natural takeover, a\ndriver-specific state-tracking matrix is introduced, allowing the transition to\nalign with individual control preferences. Multiple transition strategies are\nevaluated using a cumulative trajectory error metric. Human-in-the-loop control\nscenarios of the standardized ISO lane change maneuvers demonstrate that\nadaptive transitions reduce trajectory deviations and driver control effort\ncompared to conventional strategies. Experiments also confirm that continuously\nadjusting control authority based on real-time deviations enhances vehicle\nstability while reducing driver effort during takeover.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u63a7\u5236\u6743\u8f6c\u79fb\u7b56\u7565\uff0c\u52a8\u6001\u6839\u636e\u53f8\u673a\u8868\u73b0\u8c03\u6574\u63a7\u5236\u6743\uff0c\u4ece\u800c\u6539\u5584\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u7684\u9a7e\u9a76\u8005\u54cd\u5e94\u6548\u679c\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u7684\u63a7\u5236\u8f6c\u79fb\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u53f8\u673a\u51c6\u5907\u4e0d\u8db3\u7684\u60c5\u51b5\u4e0b\uff0c\u8fd9\u4f1a\u52a0\u5927\u53cd\u5e94\u65f6\u95f4\u3002\u76ee\u524d\u7684\u63a5\u7ba1\u7b56\u7565\u4e3b\u8981\u57fa\u4e8e\u56fa\u5b9a\u7684\u65f6\u95f4\u6bb5\uff0c\u65e0\u6cd5\u6709\u6548\u9002\u5e94\u5b9e\u65f6\u7684\u9a7e\u9a76\u8868\u73b0\u53d8\u5316\u3002", "method": "\u901a\u8fc7\u5efa\u6a21\u5171\u4eab\u63a7\u5236\u4e3a\u4e00\u4e2a\u5408\u4f5c\u5fae\u5206\u535a\u5f08\uff0c\u5229\u7528\u65f6\u95f4\u53d8\u5316\u7684\u76ee\u6807\u51fd\u6570\u8c03\u63a7\u63a7\u5236\u6743\uff0c\u5e76\u5f15\u5165\u53f8\u673a\u7279\u5b9a\u7684\u72b6\u6001\u8ddf\u8e2a\u77e9\u9635\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u8f6c\u79fb\u7b56\u7565\uff0c\u901a\u8fc7\u65f6\u95f4\u548c\u53f8\u673a\u8f68\u8ff9\u8ddf\u8e2a\u80fd\u529b\u52a8\u6001\u8c03\u6574\u63a7\u5236\u6743\uff0c\u4f7f\u7528\u4e86\u4e00\u79cd\u5408\u4f5c\u5fae\u5206\u535a\u5f08\u6a21\u578b\uff0c\u4ee5\u786e\u4fdd\u66f4\u81ea\u7136\u7684\u63a5\u7ba1\u3002", "conclusion": "\u901a\u8fc7\u6bd4\u8f83\u591a\u79cd\u8f6c\u79fb\u7b56\u7565\uff0c\u7ed3\u679c\u8868\u660e\u81ea\u9002\u5e94\u8f6c\u79fb\u5728\u51cf\u5c11\u8f68\u8ff9\u504f\u5dee\u548c\u9a7e\u9a76\u8005\u63a7\u5236\u52aa\u529b\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u7b56\u7565\uff0c\u5e76\u589e\u5f3a\u4e86\u63a5\u7ba1\u8fc7\u7a0b\u4e2d\u7684\u8f66\u8f86\u7a33\u5b9a\u6027\u3002"}}
{"id": "2510.10903", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10903", "abs": "https://arxiv.org/abs/2510.10903", "authors": ["Shuanghao Bai", "Wenxuan Song", "Jiayi Chen", "Yuheng Ji", "Zhide Zhong", "Jin Yang", "Han Zhao", "Wanqi Zhou", "Wei Zhao", "Zhe Li", "Pengxiang Ding", "Cheng Chi", "Haoang Li", "Chang Xu", "Xiaolong Zheng", "Donglin Wang", "Shanghang Zhang", "Badong Chen"], "title": "Towards a Unified Understanding of Robot Manipulation: A Comprehensive Survey", "comment": null, "summary": "Embodied intelligence has witnessed remarkable progress in recent years,\ndriven by advances in computer vision, natural language processing, and the\nrise of large-scale multimodal models. Among its core challenges, robot\nmanipulation stands out as a fundamental yet intricate problem, requiring the\nseamless integration of perception, planning, and control to enable interaction\nwithin diverse and unstructured environments. This survey presents a\ncomprehensive overview of robotic manipulation, encompassing foundational\nbackground, task-organized benchmarks and datasets, and a unified taxonomy of\nexisting methods. We extend the classical division between high-level planning\nand low-level control by broadening high-level planning to include language,\ncode, motion, affordance, and 3D representations, while introducing a new\ntaxonomy of low-level learning-based control grounded in training paradigms\nsuch as input modeling, latent learning, and policy learning. Furthermore, we\nprovide the first dedicated taxonomy of key bottlenecks, focusing on data\ncollection, utilization, and generalization, and conclude with an extensive\nreview of real-world applications. Compared with prior surveys, our work offers\nboth a broader scope and deeper insight, serving as an accessible roadmap for\nnewcomers and a structured reference for experienced researchers. All related\nresources, including research papers, open-source datasets, and projects, are\ncurated for the community at\nhttps://github.com/BaiShuanghao/Awesome-Robotics-Manipulation.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u73b0\u72b6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u5173\u952e\u74f6\u9888\u4ee5\u53ca\u5b9e\u9645\u5e94\u7528\u3002", "motivation": "\u7531\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u5728\u591a\u6a21\u6001\u7cfb\u7edf\u4e2d\u626e\u6f14\u7740\u91cd\u8981\u89d2\u8272\uff0c\u63d0\u51fa\u66f4\u4e3a\u7ec6\u81f4\u7684\u5206\u7c7b\u53ca\u74f6\u9888\u5206\u6790\u662f\u4e3a\u4e86\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u6b65\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u4f20\u7edf\u7684\u9ad8\u5c42\u89c4\u5212\u4e0e\u4f4e\u5c42\u63a7\u5236\u7684\u5212\u5206\uff0c\u63d0\u51fa\u7efc\u5408\u8003\u8651\u8bed\u8a00\u3001\u4ee3\u7801\u3001\u52a8\u4f5c\u3001\u53ef\u53ca\u6027\u53ca3D\u8868\u793a\u7684\u9ad8\u5c42\u89c4\u5212\uff0c\u4ee5\u53ca\u57fa\u4e8e\u4e0d\u540c\u8bad\u7ec3\u8303\u5f0f\u7684\u4f4e\u5c42\u5b66\u4e60\u63a7\u5236\u7684\u65b0\u5206\u7c7b\u3002", "result": "\u8fd9\u7bc7\u8bba\u6587\u7684\u7efc\u8ff0\u5c55\u793a\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u5168\u9762\u80cc\u666f\uff0c\u5305\u62ec\u57fa\u7840\u77e5\u8bc6\u3001\u4efb\u52a1\u7ec4\u7ec7\u7684\u57fa\u51c6\u548c\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u73b0\u6709\u65b9\u6cd5\u7684\u7edf\u4e00\u5206\u7c7b\u3002", "conclusion": "\u8bba\u6587\u4e3a\u65b0\u624b\u63d0\u4f9b\u4e86\u5165\u95e8\u6307\u5357\uff0c\u4e5f\u4e3a\u8d44\u6df1\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u53c2\u8003\u3002"}}
{"id": "2510.10912", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10912", "abs": "https://arxiv.org/abs/2510.10912", "authors": ["Xinyu Shao", "Yanzhe Tang", "Pengwei Xie", "Kaiwen Zhou", "Yuzheng Zhuang", "Xingyue Quan", "Jianye Hao", "Long Zeng", "Xiu Li"], "title": "More than A Point: Capturing Uncertainty with Adaptive Affordance Heatmaps for Spatial Grounding in Robotic Tasks", "comment": "More details and videos can be found at https://robo-map.github.io.\n  Xiu Li (Corresponding author: Xiu Li)", "summary": "Many language-guided robotic systems rely on collapsing spatial reasoning\ninto discrete points, making them brittle to perceptual noise and semantic\nambiguity. To address this challenge, we propose RoboMAP, a framework that\nrepresents spatial targets as continuous, adaptive affordance heatmaps. This\ndense representation captures the uncertainty in spatial grounding and provides\nricher information for downstream policies, thereby significantly enhancing\ntask success and interpretability. RoboMAP surpasses the previous\nstate-of-the-art on a majority of grounding benchmarks with up to a 50x speed\nimprovement, and achieves an 82\\% success rate in real-world manipulation.\nAcross extensive simulated and physical experiments, it demonstrates robust\nperformance and shows strong zero-shot generalization to navigation. More\ndetails and videos can be found at https://robo-map.github.io.", "AI": {"tldr": "RoboMAP\u6846\u67b6\u901a\u8fc7\u5c06\u7a7a\u95f4\u76ee\u6807\u8868\u793a\u4e3a\u8fde\u7eed\u7684\u81ea\u9002\u5e94\u53ef\u4f9b\u6027\u70ed\u56fe\uff0c\u89e3\u51b3\u4e86\u8bed\u8a00\u5f15\u5bfc\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u7a7a\u95f4\u63a8\u7406\u4e2d\u9762\u4e34\u7684\u8106\u5f31\u6027\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u5904\u7406\u7a7a\u95f4\u63a8\u7406\u65f6\u56e0\u79bb\u6563\u5316\u5904\u7406\u5e26\u6765\u7684\u611f\u77e5\u566a\u58f0\u548c\u8bed\u4e49\u6a21\u7cca\u95ee\u9898\u3002", "method": "RoboMAP\u6846\u67b6\u5c06\u7a7a\u95f4\u76ee\u6807\u8868\u793a\u4e3a\u8fde\u7eed\u3001\u9002\u5e94\u6027\u5f3a\u7684\u53ef\u4f9b\u6027\u70ed\u56fe\uff0c\u6355\u6349\u7a7a\u95f4\u5b9a\u4f4d\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "result": "RoboMAP\u5728\u5927\u591a\u6570\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8fc7\u4e86\u73b0\u6709\u6280\u672f\uff0c\u4ee5\u6700\u9ad850\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "RoboMAP\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u4e2d\u53d6\u5f9782%\u7684\u6210\u529f\u7387\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2510.10960", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10960", "abs": "https://arxiv.org/abs/2510.10960", "authors": ["Dong Hu", "Fenqing Hu", "Lidong Yang", "Chao Huang"], "title": "Game-Theoretic Risk-Shaped Reinforcement Learning for Safe Autonomous Driving", "comment": null, "summary": "Ensuring safety in autonomous driving (AD) remains a significant challenge,\nespecially in highly dynamic and complex traffic environments where diverse\nagents interact and unexpected hazards frequently emerge. Traditional\nreinforcement learning (RL) methods often struggle to balance safety,\nefficiency, and adaptability, as they primarily focus on reward maximization\nwithout explicitly modeling risk or safety constraints. To address these\nlimitations, this study proposes a novel game-theoretic risk-shaped RL (GTR2L)\nframework for safe AD. GTR2L incorporates a multi-level game-theoretic world\nmodel that jointly predicts the interactive behaviors of surrounding vehicles\nand their associated risks, along with an adaptive rollout horizon that adjusts\ndynamically based on predictive uncertainty. Furthermore, an uncertainty-aware\nbarrier mechanism enables flexible modulation of safety boundaries. A dedicated\nrisk modeling approach is also proposed, explicitly capturing both epistemic\nand aleatoric uncertainty to guide constrained policy optimization and enhance\ndecision-making in complex environments. Extensive evaluations across diverse\nand safety-critical traffic scenarios show that GTR2L significantly outperforms\nstate-of-the-art baselines, including human drivers, in terms of success rate,\ncollision and violation reduction, and driving efficiency. The code is\navailable at https://github.com/DanielHu197/GTR2L.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u535a\u5f08\u8bba\u98ce\u9669\u5f62\u72b6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff08GTR2L\uff09\uff0c\u7528\u4e8e\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u7684\u4ea4\u901a\u73af\u5883\u4e2d\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u3001\u6548\u7387\u548c\u9002\u5e94\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u590d\u6742\u7684\u4ea4\u901a\u73af\u5883\u4e2d\u96be\u4ee5\u5e73\u8861\u98ce\u9669\u548c\u5956\u52b1\u3002", "method": "GTR2L\u7ed3\u5408\u591a\u5c42\u6b21\u535a\u5f08\u8bba\u4e16\u754c\u6a21\u578b\uff0c\u52a8\u6001\u9884\u6d4b\u5468\u56f4\u8f66\u8f86\u7684\u4e92\u52a8\u884c\u4e3a\u53ca\u5176\u98ce\u9669\uff0c\u5e76\u91c7\u7528\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u969c\u788d\u673a\u5236\u548c\u98ce\u9669\u5efa\u6a21\u65b9\u6cd5\u3002", "result": "GTR2L\u5728\u591a\u79cd\u5b89\u5168\u5173\u952e\u7684\u4ea4\u901a\u573a\u666f\u4e2d\uff0c\u6210\u529f\u7387\u63d0\u9ad8\uff0c\u78b0\u649e\u548c\u8fdd\u89c4\u4e8b\u4ef6\u51cf\u5c11\uff0c\u9a7e\u9a76\u6548\u7387\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7GTR2L\u6846\u67b6\uff0c\u81ea\u52a8\u9a7e\u9a76\u5728\u6210\u529f\u7387\u3001\u78b0\u649e\u548c\u8fdd\u89c4\u51cf\u5c11\u3001\u9a7e\u9a76\u6548\u7387\u65b9\u9762\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5305\u62ec\u4eba\u7c7b\u9a7e\u9a76\u5458\u3002"}}
{"id": "2510.10975", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10975", "abs": "https://arxiv.org/abs/2510.10975", "authors": ["Mingtong Dai", "Lingbo Liu", "Yongjie Bai", "Yang Liu", "Zhouxia Wang", "Rui SU", "Chunjie Chen", "Liang Lin", "Xinyu Wu"], "title": "RoVer: Robot Reward Model as Test-Time Verifier for Vision-Language-Action Model", "comment": null, "summary": "Vision-Language-Action (VLA) models have become a prominent paradigm for\nembodied intelligence, yet further performance improvements typically rely on\nscaling up training data and model size -- an approach that is prohibitively\nexpensive for robotics and fundamentally limited by data collection costs.We\naddress this limitation with $\\mathbf{RoVer}$, an embodied test-time scaling\nframework that uses a $\\mathbf{Ro}$bot Process Reward Model (PRM) as a\nTest-Time $\\mathbf{Ver}$ifier to enhance the capabilities of existing VLA\nmodels without modifying their architectures or weights. Specifically, RoVer\n(i) assigns scalar-based process rewards to evaluate the reliability of\ncandidate actions, and (ii) predicts an action-space direction for candidate\nexpansion/refinement. During inference, RoVer generates multiple candidate\nactions concurrently from the base policy, expands them along PRM-predicted\ndirections, and then scores all candidates with PRM to select the optimal\naction for execution. Notably, by caching shared perception features, it can\namortize perception cost and evaluate more candidates under the same test-time\ncomputational budget. Essentially, our approach effectively transforms\navailable computing resources into better action decision-making, realizing the\nbenefits of test-time scaling without extra training overhead. Our\ncontributions are threefold: (1) a general, plug-and-play test-time scaling\nframework for VLAs; (2) a PRM that jointly provides scalar process rewards and\nan action-space direction to guide exploration; and (3) an efficient\ndirection-guided sampling strategy that leverages a shared perception cache to\nenable scalable candidate generation and selection during inference.", "AI": {"tldr": "RoVer\u662f\u4e00\u4e2a\u63d0\u5347VLA\u6a21\u578b\u6027\u80fd\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6d4b\u8bd5\u65f6\u6269\u5c55\u548c\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff0c\u5b9e\u73b0\u66f4\u4f18\u7684\u52a8\u4f5c\u51b3\u7b56\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "motivation": "\u5f53\u524d\u7684VLA\u6a21\u578b\u5728\u6027\u80fd\u63d0\u5347\u4e0a\u53d7\u9650\u4e8e\u6570\u636e\u548c\u6a21\u578b\u7684\u89c4\u6a21\uff0cRoVer\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u6d4b\u8bd5\u9636\u6bb5\u7684\u52a8\u4f5c\u51b3\u7b56\uff0c\u907f\u514d\u9ad8\u6602\u7684\u8bad\u7ec3\u6210\u672c\u548c\u6570\u636e\u6536\u96c6\u7684\u9650\u5236\u3002", "method": "RoVer\u91c7\u7528\u4e86\u673a\u5668\u4eba\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRM\uff09\u6765\u7ed9\u5019\u9009\u52a8\u4f5c\u5206\u914d\u8fc7\u7a0b\u5956\u52b1\uff0c\u5e76\u9884\u6d4b\u52a8\u4f5c\u7a7a\u95f4\u65b9\u5411\uff0c\u4ece\u800c\u5e2e\u52a9\u6269\u5c55\u548c\u9009\u62e9\u6700\u4f18\u884c\u4e3a\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6RoVer\uff0c\u65e8\u5728\u63d0\u5347Vision-Language-Action\uff08VLA\uff09\u6a21\u578b\u7684\u6027\u80fd\uff0c\u800c\u65e0\u9700\u589e\u52a0\u8bad\u7ec3\u6570\u636e\u548c\u6a21\u578b\u89c4\u6a21\u3002\u6b64\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u673a\u5668\u4eba\u8fc7\u7a0b\u5956\u6a21\u578b\uff08PRM\uff09\uff0c\u5728\u6d4b\u8bd5\u9636\u6bb5\u4e3aVLA\u6a21\u578b\u63d0\u4f9b\u652f\u6301\uff0c\u4ece\u800c\u6539\u8fdb\u52a8\u4f5c\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4f18\u5316\u884c\u52a8\u9009\u62e9\uff0c\u800c\u65e0\u987b\u4fee\u6539\u6a21\u578b\u67b6\u6784\u6216\u6743\u91cd\u3002", "conclusion": "RoVer\u6709\u6548\u5730\u63d0\u5347\u4e86\u73b0\u6709VLA\u6a21\u578b\u7684\u51b3\u7b56\u80fd\u529b\uff0c\u8868\u660e\u5728\u4e0d\u589e\u52a0\u8bad\u7ec3\u6210\u672c\u7684\u524d\u63d0\u4e0b\uff0c\u901a\u8fc7\u4f18\u5316\u6d4b\u8bd5\u9636\u6bb5\u7684\u8d44\u6e90\u5229\u7528\uff0c\u53ef\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u884c\u52a8\u63a8\u8350\u3002"}}
{"id": "2510.10979", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.10979", "abs": "https://arxiv.org/abs/2510.10979", "authors": ["Qizhi Guo", "Siyuan Yang", "Junning Lyu", "Jianjun Sun", "Defu Lin", "Shaoming He"], "title": "AMO-HEAD: Adaptive MARG-Only Heading Estimation for UAVs under Magnetic Disturbances", "comment": null, "summary": "Accurate and robust heading estimation is crucial for unmanned aerial\nvehicles (UAVs) when conducting indoor inspection tasks. However, the cluttered\nnature of indoor environments often introduces severe magnetic disturbances,\nwhich can significantly degrade heading accuracy. To address this challenge,\nthis paper presents an Adaptive MARG-Only Heading (AMO-HEAD) estimation\napproach for UAVs operating in magnetically disturbed environments. AMO-HEAD is\na lightweight and computationally efficient Extended Kalman Filter (EKF)\nframework that leverages inertial and magnetic sensors to achieve reliable\nheading estimation. In the proposed approach, gyroscope angular rate\nmeasurements are integrated to propagate the quaternion state, which is\nsubsequently corrected using accelerometer and magnetometer data. The corrected\nquaternion is then used to compute the UAV's heading. An adaptive process noise\ncovariance method is introduced to model and compensate for gyroscope\nmeasurement noise, bias drift, and discretization errors arising from the Euler\nmethod integration. To mitigate the effects of external magnetic disturbances,\na scaling factor is applied based on real-time magnetic deviation detection. A\ntheoretical observability analysis of the proposed AMO-HEAD is performed using\nthe Lie derivative. Extensive experiments were conducted in real world indoor\nenvironments with customized UAV platforms. The results demonstrate the\neffectiveness of the proposed algorithm in providing precise heading estimation\nunder magnetically disturbed conditions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u5f3a\u78c1\u5e72\u6270\u73af\u5883\u7684AMO-HEAD\u822a\u5411\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u51c6\u786e\u6027\uff0c\u7ecf\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\u51fa\u826f\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u590d\u6742\u7684\u5ba4\u5185\u73af\u5883\u4e2d\uff0c\u65e0\u4eba\u673a\u7684\u822a\u5411\u4f30\u8ba1\u51c6\u786e\u6027\u53d7\u5230\u4e25\u91cd\u7684\u78c1\u5e72\u6270\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u5347\u822a\u5411\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "method": "\u57fa\u4e8e\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u6846\u67b6\uff0c\u5f15\u5165\u52a0\u901f\u5ea6\u8ba1\u548c\u78c1\u529b\u8ba1\u6570\u636e\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8fc7\u7a0b\u566a\u58f0\u534f\u65b9\u5dee\u65b9\u6cd5\u8fdb\u884c\u8865\u507f\uff0c\u5e76\u5229\u7528\u5b9e\u65f6\u78c1\u504f\u5dee\u68c0\u6d4b\u6765\u51cf\u8f7b\u5916\u90e8\u78c1\u5e72\u6270\u7684\u5f71\u54cd\u3002", "result": "\u7ecf\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u7b97\u6cd5\u80fd\u591f\u5728\u78c1\u5e72\u6270\u6761\u4ef6\u4e0b\u63d0\u4f9b\u7cbe\u786e\u7684\u822a\u5411\u4f30\u8ba1\u3002", "conclusion": "\u63d0\u51fa\u7684AMO-HEAD\u65b9\u6cd5\u5728\u78c1\u5e72\u6270\u73af\u5883\u4e2d\u80fd\u591f\u6709\u6548\u63d0\u4f9b\u51c6\u786e\u7684\u822a\u5411\u4f30\u8ba1\u3002"}}
{"id": "2510.11014", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11014", "abs": "https://arxiv.org/abs/2510.11014", "authors": ["Subhransu S. Bhattacharjee", "Hao Lu", "Dylan Campbell", "Rahul Shome"], "title": "Into the Unknown: Towards using Generative Models for Sampling Priors of Environment Uncertainty for Planning in Configuration Spaces", "comment": "Under Review", "summary": "Priors are vital for planning under partial observability, yet difficult to\nobtain in practice. We present a sampling-based pipeline that leverages\nlarge-scale pretrained generative models to produce probabilistic priors\ncapturing environmental uncertainty and spatio-semantic relationships in a\nzero-shot manner. Conditioned on partial observations, the pipeline recovers\ncomplete RGB-D point cloud samples with occupancy and target semantics,\nformulated to be directly useful in configuration-space planning. We establish\na Matterport3D benchmark of rooms partially visible through doorways, where a\nrobot must navigate to an unobserved target object. Effective priors for this\nsetting must represent both occupancy and target-location uncertainty in\nunobserved regions. Experiments show that our approach recovers commonsense\nspatial semantics consistent with ground truth, yielding diverse, clean 3D\npoint clouds usable in motion planning, highlight the promise of generative\nmodels as a rich source of priors for robotic planning.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u5728\u90e8\u5206\u89c2\u5bdf\u6761\u4ef6\u4e0b\u751f\u6210\u80fd\u5e2e\u52a9\u673a\u5668\u4eba\u89c4\u5212\u7684\u6982\u7387\u5148\u9a8c\u3002", "motivation": "\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u6027\u4e0b\u8fdb\u884c\u89c4\u5212\u65f6\uff0c\u5148\u9a8c\u77e5\u8bc6\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b9e\u9645\u4e0a\u96be\u4ee5\u83b7\u5f97\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684\u751f\u6210\u6a21\u578b\uff0c\u6839\u636e\u90e8\u5206\u89c2\u5bdf\u7ed3\u679c\u6062\u590d\u5b8c\u6574\u7684RGB-D\u70b9\u4e91\u6837\u672c\uff0c\u5f62\u6210\u76f4\u63a5\u53ef\u7528\u4e8e\u914d\u7f6e\u7a7a\u95f4\u89c4\u5212\u7684\u5148\u9a8c\u77e5\u8bc6\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91c7\u6837\u7684\u7ba1\u9053\uff0c\u5229\u7528\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u4ea7\u751f\u6355\u6349\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u548c\u65f6\u7a7a\u8bed\u4e49\u5173\u7cfb\u7684\u6982\u7387\u5148\u9a8c\u3002", "conclusion": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u76843D\u70b9\u4e91\u5728\u8fd0\u52a8\u89c4\u5212\u4e2d\u6709\u6548\uff0c\u5c55\u793a\u4e86\u751f\u6210\u6a21\u578b\u4f5c\u4e3a\u673a\u5668\u4eba\u89c4\u5212\u5148\u9a8c\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.11019", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11019", "abs": "https://arxiv.org/abs/2510.11019", "authors": ["Bingjie Tang", "Iretiayo Akinola", "Jie Xu", "Bowen Wen", "Dieter Fox", "Gaurav S. Sukhatme", "Fabio Ramos", "Abhishek Gupta", "Yashraj Narang"], "title": "Refinery: Active Fine-tuning and Deployment-time Optimization for Contact-Rich Policies", "comment": "in submission. 8 pages, 6 figures. Website:\n  https://refinery-2025.github.io/refinery/", "summary": "Simulation-based learning has enabled policies for precise, contact-rich\ntasks (e.g., robotic assembly) to reach high success rates (~80%) under high\nlevels of observation noise and control error. Although such performance may be\nsufficient for research applications, it falls short of industry standards and\nmakes policy chaining exceptionally brittle. A key limitation is the high\nvariance in individual policy performance across diverse initial conditions. We\nintroduce Refinery, an effective framework that bridges this performance gap,\nrobustifying policy performance across initial conditions. We propose Bayesian\nOptimization-guided fine-tuning to improve individual policies, and Gaussian\nMixture Model-based sampling during deployment to select initializations that\nmaximize execution success. Using Refinery, we improve mean success rates by\n10.98% over state-of-the-art methods in simulation-based learning for robotic\nassembly, reaching 91.51% in simulation and comparable performance in the real\nworld. Furthermore, we demonstrate that these fine-tuned policies can be\nchained to accomplish long-horizon, multi-part\nassembly$\\unicode{x2013}$successfully assembling up to 8 parts without\nrequiring explicit multi-step training.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u7684Refinery\u6846\u67b6\u6709\u6548\u7f29\u5c0f\u4e86\u6a21\u62df\u5b66\u4e60\u4e0e\u5de5\u4e1a\u6807\u51c6\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u63d0\u9ad8\u4e86\u88c5\u914d\u4efb\u52a1\u7684\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6a21\u62df\u7684\u5b66\u4e60\u7b56\u7565\u5728\u591a\u6837\u5316\u521d\u59cb\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0c\u96be\u4ee5\u6ee1\u8db3\u884c\u4e1a\u6807\u51c6\u3002", "method": "\u901a\u8fc7\u8d1d\u53f6\u65af\u4f18\u5316\u5f15\u5bfc\u7684\u5fae\u8c03\u548c\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u91c7\u6837\u6765\u9009\u62e9\u521d\u59cb\u72b6\u6001\u3002", "result": "Refinery\u63d0\u5347\u4e86\u6210\u529f\u738710.98%\uff0c\u5728\u6a21\u62df\u4e2d\u8fbe\u5230\u4e8691.51%\u7684\u6210\u529f\u7387\uff0c\u4e14\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u76f8\u5f53\u3002", "conclusion": "Refinery\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u88c5\u914d\u7684\u6210\u529f\u7387\uff0c\u5e76\u652f\u6301\u957f\u65f6\u95f4\u3001\u591a\u90e8\u5206\u7684\u88c5\u914d\u4efb\u52a1\u3002"}}
{"id": "2510.11036", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11036", "abs": "https://arxiv.org/abs/2510.11036", "authors": ["Yeonseo Lee", "Jungwook Mun", "Hyosup Shin", "Guebin Hwang", "Junhee Nam", "Taeyeop Lee", "Sungho Jo"], "title": "XGrasp: Gripper-Aware Grasp Detection with Multi-Gripper Data Generation", "comment": null, "summary": "Most robotic grasping methods are typically designed for single gripper\ntypes, which limits their applicability in real-world scenarios requiring\ndiverse end-effectors. We propose XGrasp, a real-time gripper-aware grasp\ndetection framework that efficiently handles multiple gripper configurations.\nThe proposed method addresses data scarcity by systematically augmenting\nexisting datasets with multi-gripper annotations. XGrasp employs a hierarchical\ntwo-stage architecture. In the first stage, a Grasp Point Predictor (GPP)\nidentifies optimal locations using global scene information and gripper\nspecifications. In the second stage, an Angle-Width Predictor (AWP) refines the\ngrasp angle and width using local features. Contrastive learning in the AWP\nmodule enables zero-shot generalization to unseen grippers by learning\nfundamental grasping characteristics. The modular framework integrates\nseamlessly with vision foundation models, providing pathways for future\nvision-language capabilities. The experimental results demonstrate competitive\ngrasp success rates across various gripper types, while achieving substantial\nimprovements in inference speed compared to existing gripper-aware methods.\nProject page: https://sites.google.com/view/xgrasp", "AI": {"tldr": "XGrasp\u6846\u67b6\u89e3\u51b3\u4e86\u591a\u79cd\u6293\u53d6\u5668\u914d\u7f6e\u7684\u5b9e\u65f6\u68c0\u6d4b\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u4e86\u6570\u636e\u7684\u7cfb\u7edf\u6027\u589e\u5f3a\u3002", "motivation": "\u76ee\u524d\u591a\u6570\u6293\u53d6\u65b9\u6cd5\u4ec5\u9488\u5bf9\u5355\u4e00\u6293\u53d6\u5668\uff0c\u9650\u5236\u4e86\u5176\u5728\u9700\u8981\u591a\u6837\u5316\u672b\u7aef\u6267\u884c\u5668\u7684\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "XGrasp\u91c7\u7528\u4e86\u5206\u5c42\u7684\u4e24\u9636\u6bb5\u67b6\u6784\uff1a\u7b2c\u4e00\u9636\u6bb5\u7684\u6293\u53d6\u70b9\u9884\u6d4b\u5668\uff08GPP\uff09\u57fa\u4e8e\u5168\u7403\u573a\u666f\u4fe1\u606f\u548c\u6293\u53d6\u5668\u89c4\u683c\u8bc6\u522b\u6700\u4f73\u4f4d\u7f6e\uff1b\u7b2c\u4e8c\u9636\u6bb5\u7684\u89d2\u5ea6\u5bbd\u5ea6\u9884\u6d4b\u5668\uff08AWP\uff09\u4f7f\u7528\u5c40\u90e8\u7279\u5f81\u7ec6\u5316\u6293\u53d6\u7684\u89d2\u5ea6\u548c\u5bbd\u5ea6\u3002", "result": "XGrasp\u662f\u4e00\u79cd\u5b9e\u65f6\u7684\u6293\u53d6\u68c0\u6d4b\u6846\u67b6\uff0c\u80fd\u591f\u9ad8\u6548\u5904\u7406\u591a\u79cd\u6293\u53d6\u914d\u7f6e\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cXGrasp\u5728\u4e0d\u540c\u6293\u53d6\u5668\u7c7b\u578b\u4e2d\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u7684\u6293\u53d6\u6210\u529f\u7387\uff0c\u5e76\u5728\u63a8\u7406\u901f\u5ea6\u4e0a\u76f8\u8f83\u4e8e\u73b0\u6709\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2510.11041", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11041", "abs": "https://arxiv.org/abs/2510.11041", "authors": ["Shiyao Zhang", "Liwei Deng", "Shuyu Zhang", "Weijie Yuan", "Hong Zhang"], "title": "Unveiling Uncertainty-Aware Autonomous Cooperative Learning Based Planning Strategy", "comment": "Accepted by IEEE RA-L", "summary": "In future intelligent transportation systems, autonomous cooperative planning\n(ACP), becomes a promising technique to increase the effectiveness and security\nof multi-vehicle interactions. However, multiple uncertainties cannot be fully\naddressed for existing ACP strategies, e.g. perception, planning, and\ncommunication uncertainties. To address these, a novel deep reinforcement\nlearning-based autonomous cooperative planning (DRLACP) framework is proposed\nto tackle various uncertainties on cooperative motion planning schemes.\nSpecifically, the soft actor-critic (SAC) with the implementation of gate\nrecurrent units (GRUs) is adopted to learn the deterministic optimal\ntime-varying actions with imperfect state information occurred by planning,\ncommunication, and perception uncertainties. In addition, the real-time actions\nof autonomous vehicles (AVs) are demonstrated via the Car Learning to Act\n(CARLA) simulation platform. Evaluation results show that the proposed DRLACP\nlearns and performs cooperative planning effectively, which outperforms other\nbaseline methods under different scenarios with imperfect AV state information.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u5408\u4f5c\u89c4\u5212\u6846\u67b6\uff0c\u4ee5\u6709\u6548\u5e94\u5bf9\u591a\u8f66\u8f86\u4ea4\u4e92\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u63d0\u9ad8\u591a\u8f66\u8f86\u4ea4\u4e92\u7684\u6709\u6548\u6027\u548c\u5b89\u5168\u6027\uff0c\u89e3\u51b3\u73b0\u6709ACP\u7b56\u7565\u4e2d\u7684\u591a\u79cd\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u548c\u8f6f\u6f14\u5458\u8bc4\u8bba\u8005\uff08SAC\uff09\uff0c\u7ed3\u5408\u95e8\u63a7\u9012\u5f52\u5355\u5143\uff08GRUs\uff09\u3002", "result": "\u63d0\u51fa\u7684DRLACP\u6846\u67b6\u5728\u9762\u5bf9\u5404\u79cd\u4e0d\u786e\u5b9a\u6027\u65f6\uff0c\u80fd\u591f\u6709\u6548\u5b66\u4e60\u548c\u6267\u884c\u5408\u4f5c\u89c4\u5212\uff0c\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "DRLACP\u6846\u67b6\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u89c4\u5212\u3001\u901a\u4fe1\u548c\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u5408\u4f5c\u4ea4\u901a\u89c4\u5212\u3002"}}
{"id": "2510.11072", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.11072", "abs": "https://arxiv.org/abs/2510.11072", "authors": ["Huayi Wang", "Wentao Zhang", "Runyi Yu", "Tao Huang", "Junli Ren", "Feiyu Jia", "Zirui Wang", "Xiaojie Niu", "Xiao Chen", "Jiahe Chen", "Qifeng Chen", "Jingbo Wang", "Jiangmiao Pang"], "title": "PhysHSI: Towards a Real-World Generalizable and Natural Humanoid-Scene Interaction System", "comment": "Project website: https://why618188.github.io/physhsi/", "summary": "Deploying humanoid robots to interact with real-world environments--such as\ncarrying objects or sitting on chairs--requires generalizable, lifelike motions\nand robust scene perception. Although prior approaches have advanced each\ncapability individually, combining them in a unified system is still an ongoing\nchallenge. In this work, we present a physical-world humanoid-scene interaction\nsystem, PhysHSI, that enables humanoids to autonomously perform diverse\ninteraction tasks while maintaining natural and lifelike behaviors. PhysHSI\ncomprises a simulation training pipeline and a real-world deployment system. In\nsimulation, we adopt adversarial motion prior-based policy learning to imitate\nnatural humanoid-scene interaction data across diverse scenarios, achieving\nboth generalization and lifelike behaviors. For real-world deployment, we\nintroduce a coarse-to-fine object localization module that combines LiDAR and\ncamera inputs to provide continuous and robust scene perception. We validate\nPhysHSI on four representative interactive tasks--box carrying, sitting, lying,\nand standing up--in both simulation and real-world settings, demonstrating\nconsistently high success rates, strong generalization across diverse task\ngoals, and natural motion patterns.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPhysHSI\u7684\u7cfb\u7edf\uff0c\u4f7f\u7c7b\u4eba\u673a\u5668\u4eba\u80fd\u591f\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u6267\u884c\u4e92\u52a8\u4efb\u52a1\uff0c\u5177\u5907\u81ea\u7136\u7684\u884c\u4e3a\u548c\u5f3a\u5927\u7684\u573a\u666f\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u7c7b\u4eba\u673a\u5668\u4eba\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u4e92\u52a8\u65f6\u8fd0\u52a8\u81ea\u7136\u6027\u548c\u573a\u666f\u611f\u77e5\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u5bf9\u6297\u6027\u8fd0\u52a8\u5148\u9a8c\u7684\u7b56\u7565\u5b66\u4e60\u8fdb\u884c\u6a21\u62df\u8bad\u7ec3\uff0c\u5e76\u5f15\u5165\u7c97\u5230\u7ec6\u7684\u7269\u4f53\u5b9a\u4f4d\u6a21\u5757\u7ed3\u5408LiDAR\u548c\u76f8\u673a\u8f93\u5165\u8fdb\u884c\u573a\u666f\u611f\u77e5\u3002", "result": "PhysHSI\u5728\u56db\u4e2a\u4ee3\u8868\u6027\u7684\u4e92\u52a8\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u9ad8\u6210\u529f\u7387\u548c\u5f3a\u901a\u7528\u6027\u3002", "conclusion": "PhysHSI\u5728\u6a21\u62df\u548c\u5b9e\u9645\u73af\u5883\u4e2d\u5bf9\u56db\u9879\u4e92\u52a8\u4efb\u52a1\u7684\u6210\u529f\u7387\u548c\u81ea\u7136\u8fd0\u52a8\u6a21\u5f0f\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2510.11083", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11083", "abs": "https://arxiv.org/abs/2510.11083", "authors": ["Tianyi Tan", "Yinan Zheng", "Ruiming Liang", "Zexu Wang", "Kexin Zheng", "Jinliang Zheng", "Jianxiong Li", "Xianyuan Zhan", "Jingjing Liu"], "title": "Flow Matching-Based Autonomous Driving Planning with Advanced Interactive Behavior Modeling", "comment": "26 pages, 6 figures. Accepted at NeurIPS 2025", "summary": "Modeling interactive driving behaviors in complex scenarios remains a\nfundamental challenge for autonomous driving planning. Learning-based\napproaches attempt to address this challenge with advanced generative models,\nremoving the dependency on over-engineered architectures for representation\nfusion. However, brute-force implementation by simply stacking transformer\nblocks lacks a dedicated mechanism for modeling interactive behaviors that are\ncommon in real driving scenarios. The scarcity of interactive driving data\nfurther exacerbates this problem, leaving conventional imitation learning\nmethods ill-equipped to capture high-value interactive behaviors. We propose\nFlow Planner, which tackles these problems through coordinated innovations in\ndata modeling, model architecture, and learning scheme. Specifically, we first\nintroduce fine-grained trajectory tokenization, which decomposes the trajectory\ninto overlapping segments to decrease the complexity of whole trajectory\nmodeling. With a sophisticatedly designed architecture, we achieve efficient\ntemporal and spatial fusion of planning and scene information, to better\ncapture interactive behaviors. In addition, the framework incorporates flow\nmatching with classifier-free guidance for multi-modal behavior generation,\nwhich dynamically reweights agent interactions during inference to maintain\ncoherent response strategies, providing a critical boost for interactive\nscenario understanding. Experimental results on the large-scale nuPlan dataset\nand challenging interactive interPlan dataset demonstrate that Flow Planner\nachieves state-of-the-art performance among learning-based approaches while\neffectively modeling interactive behaviors in complex driving scenarios.", "AI": {"tldr": "Flow Planner\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u5efa\u6a21\u3001\u6a21\u578b\u67b6\u6784\u548c\u5b66\u4e60\u65b9\u6848\uff0c\u63d0\u5347\u4e86\u590d\u6742\u9a7e\u9a76\u573a\u666f\u4e2d\u4ea4\u4e92\u884c\u4e3a\u7684\u5efa\u6a21\u80fd\u529b\u3002", "motivation": "\u5f25\u8865\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e2d\u5bf9\u4ea4\u4e92\u884c\u4e3a\u5efa\u6a21\u80fd\u529b\u7684\u4e0d\u8db3\uff0c\u5e76\u5e94\u5bf9\u4ea4\u4e92\u9a7e\u9a76\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faFlow Planner\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u8f68\u8ff9\u6807\u8bb0\u3001\u4f18\u5316\u67b6\u6784\u548c\u6d41\u5339\u914d\u6765\u5efa\u6a21\u4ea4\u4e92\u9a7e\u9a76\u884c\u4e3a\u3002", "result": "\u5728\u5927\u89c4\u6a21nuPlan\u6570\u636e\u96c6\u548c\u6311\u6218\u6027\u7684interactive interPlan\u6570\u636e\u96c6\u4e0a\uff0cFlow Planner\u5728\u5b66\u4e60\u65b9\u6cd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u6709\u6548\u5efa\u6a21\u590d\u6742\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u4ea4\u4e92\u884c\u4e3a\u3002", "conclusion": "Flow Planner\u5728\u5efa\u6a21\u590d\u6742\u9a7e\u9a76\u573a\u666f\u7684\u4ea4\u4e92\u884c\u4e3a\u4e0a\u63d0\u4f9b\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u5e26\u6765\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2510.11094", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11094", "abs": "https://arxiv.org/abs/2510.11094", "authors": ["Junxiang Wang", "Han Zhang", "Zehao Wang", "Huaiyuan Chen", "Pu Wang", "Weidong Chen"], "title": "Design and Koopman Model Predictive Control of A Soft Exoskeleton Based on Origami-Inspired Pneumatic Actuator for Knee Rehabilitation", "comment": null, "summary": "Effective rehabilitation methods are essential for the recovery of lower limb\ndysfunction caused by stroke. Nowadays, robotic exoskeletons have shown great\npotentials in rehabilitation. Nevertheless, traditional rigid exoskeletons are\nusually heavy and need a lot of work to help the patients to put them on.\nMoreover, it also requires extra compliance control to guarantee the safety. In\ncontrast, soft exoskeletons are easy and comfortable to wear and have intrinsic\ncompliance, but their complex nonlinear human-robot interaction dynamics would\npose significant challenges for control. In this work, based on the pneumatic\nactuators inspired by origami, we design a rehabilitation exoskeleton for knee\nthat is easy and comfortable to wear. To guarantee the control performance and\nenable a nice human-robot interaction, we first use Deep Koopman Network to\nmodel the human-robot interaction dynamics. In particular, by viewing the\nelectromyography (EMG) signals and the duty cycle of the PWM wave that controls\nthe pneumatic robot's valves and pump as the inputs, the linear Koopman model\naccurately captures the complex human-robot interaction dynamics. Next, based\non the obtained Koopman model, we further use Model Predictive Control (MPC) to\ncontrol the soft robot and help the user to do rehabilitation training in\nreal-time. The goal of the rehabilitation training is to track a given\nreference signal shown on the screen. Experiments show that by integrating the\nEMG signals into the Koopman model, we have improved the model accuracy to\ngreat extent. In addition, a personalized Koopman model trained from the\nindividual's own data performs better than the non-personalized model.\nConsequently, our control framework outperforms the traditional PID control in\nboth passive and active training modes. Hence the proposed method provides a\nnew control framework for soft rehabilitation robots.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6Koopman\u7f51\u7edc\u7684\u8f6f\u6027\u673a\u5668\u4eba\u63a7\u5236\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u5eb7\u590d\u8bad\u7ec3\uff0c\u5e76\u8d85\u8d8a\u4e86\u4f20\u7edf\u63a7\u5236\u65b9\u6cd5\u3002", "motivation": "\u9488\u5bf9\u4f20\u7edf\u521a\u6027\u5916\u9aa8\u9abc\u91cd\u91cf\u5927\u548c\u5b89\u5168\u63a7\u5236\u6311\u6218\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u8bbe\u8ba1\u4e00\u79cd\u8212\u9002\u6613\u7a7f\u7684\u8f6f\u6027\u5916\u9aa8\u9abc\uff0c\u5e76\u6709\u6548\u63a7\u5236\u5176\u4eba\u673a\u4ea4\u4e92\u52a8\u6001\u3002", "method": "\u57fa\u4e8e\u6df1\u5ea6Koopman\u7f51\u7edc\u6a21\u578b\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\uff0c\u63a7\u5236\u8f6f\u6027\u673a\u5668\u4eba\u7684\u5b9e\u65f6\u5eb7\u590d\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5c06EMG\u4fe1\u53f7\u5f15\u5165Koopman\u6a21\u578b\u540e\uff0c\u6a21\u578b\u7cbe\u5ea6\u663e\u8457\u63d0\u9ad8\uff0c\u4e2a\u6027\u5316Koopman\u6a21\u578b\u7684\u8868\u73b0\u4f18\u4e8e\u975e\u4e2a\u6027\u5316\u6a21\u578b\uff0c\u4e14\u63a7\u5236\u6846\u67b6\u5728\u88ab\u52a8\u548c\u4e3b\u52a8\u8bad\u7ec3\u6a21\u5f0f\u4e0b\u5747\u4f18\u4e8e\u4f20\u7edfPID\u63a7\u5236\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u8f6f\u6027\u5eb7\u590d\u673a\u5668\u4eba\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684PID\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5eb7\u590d\u8bad\u7ec3\u6548\u679c\u3002"}}
{"id": "2510.11103", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11103", "abs": "https://arxiv.org/abs/2510.11103", "authors": ["Martin Schuck", "Sherif Samy", "Angela P. Schoellig"], "title": "A Primer on SO(3) Action Representations in Deep Reinforcement Learning", "comment": null, "summary": "Many robotic control tasks require policies to act on orientations, yet the\ngeometry of SO(3) makes this nontrivial. Because SO(3) admits no global,\nsmooth, minimal parameterization, common representations such as Euler angles,\nquaternions, rotation matrices, and Lie algebra coordinates introduce distinct\nconstraints and failure modes. While these trade-offs are well studied for\nsupervised learning, their implications for actions in reinforcement learning\nremain unclear. We systematically evaluate SO(3) action representations across\nthree standard continuous control algorithms, PPO, SAC, and TD3, under dense\nand sparse rewards. We compare how representations shape exploration, interact\nwith entropy regularization, and affect training stability through empirical\nstudies and analyze the implications of different projections for obtaining\nvalid rotations from Euclidean network outputs. Across a suite of robotics\nbenchmarks, we quantify the practical impact of these choices and distill\nsimple, implementation-ready guidelines for selecting and using rotation\nactions. Our results highlight that representation-induced geometry strongly\ninfluences exploration and optimization and show that representing actions as\ntangent vectors in the local frame yields the most reliable results across\nalgorithms.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86SO(3)\u52a8\u4f5c\u8868\u793a\u5bf9\u4e0d\u540c\u8fde\u7eed\u63a7\u5236\u7b97\u6cd5\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u7b80\u660e\u7684\u5b9e\u65bd\u6307\u5357\uff0c\u53d1\u73b0\u5207\u5411\u91cf\u8868\u793a\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u8bb8\u591a\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u9700\u8981\u5728SO(3)\u4e0a\u8fdb\u884c\u51b3\u7b56\uff0c\u4f46\u5176\u51e0\u4f55\u6027\u8d28\u4f7f\u5f97\u8fd9\u4e00\u8fc7\u7a0b\u8f83\u4e3a\u590d\u6742\u3002", "method": "\u8bc4\u4f30SO(3)\u52a8\u4f5c\u8868\u793a\u5728\u8fde\u7eed\u63a7\u5236\u7b97\u6cd5\u4e2d\u7684\u8868\u73b0", "result": "\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\uff0c\u6211\u4eec\u53d1\u73b0\u91c7\u7528\u5207\u5411\u91cf\u4f5c\u4e3a\u52a8\u4f5c\u8868\u793a\u5728\u4e0d\u540c\u7b97\u6cd5\u4e2d\u8868\u73b0\u51fa\u66f4\u53ef\u9760\u7684\u7ed3\u679c\u3002", "conclusion": "\u4e0d\u540c\u7684\u52a8\u4f5c\u8868\u793a\u5bf9\u63a2\u7d22\u548c\u4f18\u5316\u6709\u663e\u8457\u5f71\u54cd\uff0c\u9002\u5f53\u7684\u8868\u793a\u9009\u62e9\u53ef\u4ee5\u63d0\u9ad8\u7b97\u6cd5\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2510.11258", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11258", "abs": "https://arxiv.org/abs/2510.11258", "authors": ["Yuhui Fu", "Feiyang Xie", "Chaoyi Xu", "Jing Xiong", "Haoqi Yuan", "Zongqing Lu"], "title": "DemoHLM: From One Demonstration to Generalizable Humanoid Loco-Manipulation", "comment": null, "summary": "Loco-manipulation is a fundamental challenge for humanoid robots to achieve\nversatile interactions in human environments. Although recent studies have made\nsignificant progress in humanoid whole-body control, loco-manipulation remains\nunderexplored and often relies on hard-coded task definitions or costly\nreal-world data collection, which limits autonomy and generalization. We\npresent DemoHLM, a framework for humanoid loco-manipulation that enables\ngeneralizable loco-manipulation on a real humanoid robot from a single\ndemonstration in simulation. DemoHLM adopts a hierarchy that integrates a\nlow-level universal whole-body controller with high-level manipulation policies\nfor multiple tasks. The whole-body controller maps whole-body motion commands\nto joint torques and provides omnidirectional mobility for the humanoid robot.\nThe manipulation policies, learned in simulation via our data generation and\nimitation learning pipeline, command the whole-body controller with closed-loop\nvisual feedback to execute challenging loco-manipulation tasks. Experiments\nshow a positive correlation between the amount of synthetic data and policy\nperformance, underscoring the effectiveness of our data generation pipeline and\nthe data efficiency of our approach. Real-world experiments on a Unitree G1\nrobot equipped with an RGB-D camera validate the sim-to-real transferability of\nDemoHLM, demonstrating robust performance under spatial variations across ten\nloco-manipulation tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684DemoHLM\u6846\u67b6\uff0c\u53ef\u5728\u771f\u5b9e\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u57fa\u4e8e\u6a21\u62df\u7684\u8fd0\u52a8\u64cd\u63a7\uff0c\u878d\u5408\u4e86\u5168\u8eab\u63a7\u5236\u548c\u64cd\u63a7\u7b56\u7565\uff0c\u8868\u73b0\u51fa\u9ad8\u6548\u7684\u6570\u636e\u4f7f\u7528\u548c\u826f\u597d\u7684\u4efb\u52a1\u6267\u884c\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u4eba\u5f62\u673a\u5668\u4eba\u7684\u5168\u8eab\u63a7\u5236\u5df2\u6709\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u8fd0\u52a8\u64cd\u63a7\u4ecd\u672a\u5145\u5206\u63a2\u7d22\uff0c\u5e38\u9700\u4f9d\u8d56\u786c\u7f16\u7801\u4efb\u52a1\u5b9a\u4e49\u6216\u6602\u8d35\u7684\u5b9e\u5730\u6570\u636e\u6536\u96c6\uff0c\u9650\u5236\u4e86\u81ea\u4e3b\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "DemoHLM\u7ed3\u5408\u4f4e\u7ea7\u7684\u5168\u8eab\u63a7\u5236\u5668\u548c\u9ad8\u7ea7\u7684\u64cd\u63a7\u7b56\u7565\uff0c\u901a\u8fc7\u6570\u636e\u751f\u6210\u548c\u6a21\u4eff\u5b66\u4e60\u7684\u65b9\u5f0f\u8fdb\u884c\u64cd\u63a7\u653f\u7b56\u7684\u5b66\u4e60\uff0c\u5e76\u4f7f\u7528\u95ed\u73af\u89c6\u89c9\u53cd\u9988\u6765\u6267\u884c\u4efb\u52a1\u3002", "result": "\u901a\u8fc7\u5728Unitree G1\u673a\u5668\u4eba\u4e0a\u7684\u771f\u5b9e\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86DemoHLM\u7684\u6a21\u62df\u5230\u771f\u5b9e\u7684\u53ef\u8f6c\u79fb\u6027\uff0c\u5e76\u5728\u5341\u4e2a\u8fd0\u52a8\u64cd\u63a7\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u7a33\u5065\u7684\u6027\u80fd\u3002", "conclusion": "DemoHLM\u6846\u67b6\u80fd\u591f\u5728\u771f\u5b9e\u7684\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u4ece\u5355\u4e00\u7684\u6a21\u62df\u6f14\u793a\u4e2d\u5b9e\u73b0\u901a\u7528\u7684\u8fd0\u52a8\u64cd\u63a7\uff0c\u4e14\u5176\u65b9\u6cd5\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6570\u636e\u4f20\u8f93\u80fd\u529b\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2510.11306", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11306", "abs": "https://arxiv.org/abs/2510.11306", "authors": ["Xiaobin Zhou", "Miao Wang", "Chengao Li", "Can Cui", "Ruibin Zhang", "Yongchao Wang", "Chao Xu", "Fei Gao"], "title": "Rotor-Failure-Aware Quadrotors Flight in Unknown Environments", "comment": null, "summary": "Rotor failures in quadrotors may result in high-speed rotation and vibration\ndue to rotor imbalance, which introduces significant challenges for autonomous\nflight in unknown environments. The mainstream approaches against rotor\nfailures rely on fault-tolerant control (FTC) and predefined trajectory\ntracking. To the best of our knowledge, online failure detection and diagnosis\n(FDD), trajectory planning, and FTC of the post-failure quadrotors in unknown\nand complex environments have not yet been achieved. This paper presents a\nrotor-failure-aware quadrotor navigation system designed to mitigate the\nimpacts of rotor imbalance. First, a composite FDD-based nonlinear model\npredictive controller (NMPC), incorporating motor dynamics, is designed to\nensure fast failure detection and flight stability. Second, a\nrotor-failure-aware planner is designed to leverage FDD results and\nspatial-temporal joint optimization, while a LiDAR-based quadrotor platform\nwith four anti-torque plates is designed to enable reliable perception under\nhigh-speed rotation. Lastly, extensive benchmarks against state-of-the-art\nmethods highlight the superior performance of the proposed approach in\naddressing rotor failures, including propeller unloading and motor stoppage.\nThe experimental results demonstrate, for the first time, that our approach\nenables autonomous quadrotor flight with rotor failures in challenging\nenvironments, including cluttered rooms and unknown forests.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u65cb\u7ffc\u5931\u6548\u7684\u56db\u65cb\u7ffc\u5bfc\u822a\u7cfb\u7edf\uff0c\u901a\u8fc7\u5728\u7ebf\u6545\u969c\u68c0\u6d4b\u3001\u89c4\u5212\u4e0e\u63a7\u5236\u6280\u672f\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u81ea\u4e3b\u98de\u884c\u3002", "motivation": "\u7531\u4e8e\u65cb\u7ffc\u5931\u6548\u5bfc\u81f4\u7684\u9ad8\u65cb\u8f6c\u901f\u5ea6\u548c\u632f\u52a8\u5bf9\u81ea\u4e3b\u98de\u884c\u9020\u6210\u91cd\u5927\u6311\u6218\uff0c\u800c\u73b0\u6709\u7684\u4e3b\u8981\u89e3\u51b3\u65b9\u6848\u5c1a\u672a\u5b9e\u73b0\u5bf9\u672a\u77e5\u548c\u590d\u6742\u73af\u5883\u4e2d\u7684\u65cb\u7ffc\u5931\u6548\u8fdb\u884c\u5728\u7ebf\u68c0\u6d4b\u548c\u8bca\u65ad\u3001\u8f68\u8ff9\u89c4\u5212\u4e0e\u6545\u969c\u5bb9\u9519\u63a7\u5236\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u590d\u5408\u6545\u969c\u68c0\u6d4b\u4e0e\u8bca\u65ad(FDD)\u7684\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668(NMPC)\uff0c\u7ed3\u5408\u7535\u673a\u52a8\u6001\uff0c\u4ee5\u786e\u4fdd\u5feb\u901f\u6545\u969c\u68c0\u6d4b\u548c\u98de\u884c\u7a33\u5b9a\u6027\uff0c\u5e76\u8bbe\u8ba1\u4e86\u57fa\u4e8eFDD\u7ed3\u679c\u548c\u65f6\u7a7a\u8054\u5408\u4f18\u5316\u7684\u6545\u969c\u611f\u77e5\u89c4\u5212\u5668\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5904\u7406\u65cb\u7ffc\u5931\u6548\uff08\u5305\u62ec\u87ba\u65cb\u6868\u5378\u8f7d\u548c\u7535\u673a\u505c\u8f6c\uff09\u65f6\u8868\u73b0\u4f18\u8d8a\uff0c\u5b9e\u9a8c\u7ed3\u679c\u9996\u6b21\u8868\u660e\uff0c\u80fd\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u81ea\u4e3b\u56db\u65cb\u7ffc\u98de\u884c\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u8f7b\u65cb\u7ffc\u5931\u6548\u5bf9\u56db\u65cb\u7ffc\u98de\u884c\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u5728\u6742\u4e71\u623f\u95f4\u548c\u672a\u77e5\u68ee\u6797\u7b49\u6311\u6218\u6027\u73af\u5883\u4e2d\u663e\u793a\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2510.11308", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11308", "abs": "https://arxiv.org/abs/2510.11308", "authors": ["Weixi Situ", "Hanjing Ye", "Jianwei Peng", "Yu Zhan", "Hong Zhang"], "title": "Adap-RPF: Adaptive Trajectory Sampling for Robot Person Following in Dynamic Crowded Environments", "comment": "https://adap-rpf.github.io/", "summary": "Robot person following (RPF) is a core capability in human-robot interaction,\nenabling robots to assist users in daily activities, collaborative work, and\nother service scenarios. However, achieving practical RPF remains challenging\ndue to frequent occlusions, particularly in dynamic and crowded environments.\nExisting approaches often rely on fixed-point following or sparse\ncandidate-point selection with oversimplified heuristics, which cannot\nadequately handle complex occlusions caused by moving obstacles such as\npedestrians. To address these limitations, we propose an adaptive trajectory\nsampling method that generates dense candidate points within socially aware\nzones and evaluates them using a multi-objective cost function. Based on the\noptimal point, a person-following trajectory is estimated relative to the\npredicted motion of the target. We further design a prediction-aware model\npredictive path integral (MPPI) controller that simultaneously tracks this\ntrajectory and proactively avoids collisions using predicted pedestrian\nmotions. Extensive experiments show that our method outperforms\nstate-of-the-art baselines in smoothness, safety, robustness, and human\ncomfort, with its effectiveness further demonstrated on a mobile robot in\nreal-world scenarios.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u4eba\u8ddf\u968f\u673a\u5668\u4eba\u8f68\u8ff9\u751f\u6210\u65b9\u6cd5\uff0c\u6709\u6548\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e0b\u7684\u906e\u6321\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u52a8\u6001\u548c\u62e5\u6324\u73af\u5883\u4e2d\u4eba\u8ddf\u968f\u673a\u5668\u4eba\u9762\u4e34\u7684\u906e\u6321\u6311\u6218", "method": "\u81ea\u9002\u5e94\u8f68\u8ff9\u91c7\u6837\u65b9\u6cd5\u548c\u9884\u6d4b\u611f\u77e5\u6a21\u578b\u9884\u6d4b\u8def\u5f84\u79ef\u5206(MPPI)\u63a7\u5236\u5668", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8f68\u8ff9\u5e73\u6ed1\u6027\u3001\u5b89\u5168\u6027\u3001\u7a33\u5065\u6027\u548c\u4eba\u7c7b\u8212\u9002\u5ea6\u65b9\u9762\u4f18\u4e8e\u76ee\u524d\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\uff0c\u4e14\u5728\u5b9e\u65f6\u573a\u666f\u4e2d\u8868\u73b0\u6709\u6548", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\uff0c\u4e3a\u4eba\u673a\u4ea4\u4e92\u9886\u57df\u7684\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.11321", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11321", "abs": "https://arxiv.org/abs/2510.11321", "authors": ["Ruizhe Liu", "Pei Zhou", "Qian Luo", "Li Sun", "Jun Cen", "Yibing Song", "Yanchao Yang"], "title": "HiMaCon: Discovering Hierarchical Manipulation Concepts from Unlabeled Multi-Modal Data", "comment": "Accepted at 39th Conference on Neural Information Processing Systems\n  (NeurIPS 2025)", "summary": "Effective generalization in robotic manipulation requires representations\nthat capture invariant patterns of interaction across environments and tasks.\nWe present a self-supervised framework for learning hierarchical manipulation\nconcepts that encode these invariant patterns through cross-modal sensory\ncorrelations and multi-level temporal abstractions without requiring human\nannotation. Our approach combines a cross-modal correlation network that\nidentifies persistent patterns across sensory modalities with a multi-horizon\npredictor that organizes representations hierarchically across temporal scales.\nManipulation concepts learned through this dual structure enable policies to\nfocus on transferable relational patterns while maintaining awareness of both\nimmediate actions and longer-term goals. Empirical evaluation across simulated\nbenchmarks and real-world deployments demonstrates significant performance\nimprovements with our concept-enhanced policies. Analysis reveals that the\nlearned concepts resemble human-interpretable manipulation primitives despite\nreceiving no semantic supervision. This work advances both the understanding of\nrepresentation learning for manipulation and provides a practical approach to\nenhancing robotic performance in complex scenarios.", "AI": {"tldr": "\u901a\u8fc7\u65e0\u9700\u4eba\u7c7b\u6ce8\u91ca\u7684\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u80fd\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u6027\u80fd\u3002", "motivation": "\u6709\u6548\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u901a\u5e38\u9700\u8981\u6355\u6349\u8de8\u73af\u5883\u548c\u4efb\u52a1\u7684 invariant \u4ea4\u4e92\u6a21\u5f0f\u3002", "method": "\u7ed3\u5408\u8de8\u6a21\u6001\u76f8\u5173\u7f51\u7edc\u4e0e\u591a\u65f6\u95f4\u5c3a\u5ea6\u9884\u6d4b\u5668\uff0c\u4ee5\u5c42\u6b21\u5316\u65b9\u5f0f\u5b66\u4e60\u64cd\u4f5c\u6982\u5ff5\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u5c42\u6b21\u5316\u7684\u64cd\u4f5c\u6982\u5ff5\uff0c\u4ee5\u83b7\u53d6\u8de8\u73af\u5883\u548c\u4efb\u52a1\u7684\u975e\u53d8\u5f02\u4ea4\u4e92\u6a21\u5f0f\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u64cd\u4f5c\u8868\u73b0\uff0c\u5e76\u4e14\u6240\u5b66\u6982\u5ff5\u4e0e\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u64cd\u4f5c\u539f\u7406\u76f8\u4f3c\u3002"}}
{"id": "2510.11401", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11401", "abs": "https://arxiv.org/abs/2510.11401", "authors": ["Jiayang Wu", "Jiongye Li", "Shibowen Zhang", "Zhicheng He", "Zaijin Wang", "Xiaokun Leng", "Hangxin Liu", "Jingwen Zhang", "Jiayi Wang", "Song-Chun Zhu", "Yao Su"], "title": "Path and Motion Optimization for Efficient Multi-Location Inspection with Humanoid Robots", "comment": null, "summary": "This paper proposes a novel framework for humanoid robots to execute\ninspection tasks with high efficiency and millimeter-level precision. The\napproach combines hierarchical planning, time-optimal standing position\ngeneration, and integrated \\ac{mpc} to achieve high speed and precision. A\nhierarchical planning strategy, leveraging \\ac{ik} and \\ac{mip}, reduces\ncomputational complexity by decoupling the high-dimensional planning problem. A\nnovel MIP formulation optimizes standing position selection and trajectory\nlength, minimizing task completion time. Furthermore, an MPC system with\nsimplified kinematics and single-step position correction ensures\nmillimeter-level end-effector tracking accuracy. Validated through simulations\nand experiments on the Kuavo 4Pro humanoid platform, the framework demonstrates\nlow time cost and a high success rate in multi-location tasks, enabling\nefficient and precise execution of complex industrial operations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u6846\u67b6\uff0c\u7ed3\u5408\u5206\u5c42\u89c4\u5212\u3001\u65f6\u95f4\u6700\u4f18\u7ad9\u7acb\u4f4d\u7f6e\u751f\u6210\u548c\u96c6\u6210\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u901f\u5ea6\u548c\u7cbe\u5ea6\u7684\u68c0\u67e5\u4efb\u52a1\u3002", "motivation": "\u63d0\u9ad8\u4eba\u5f62\u673a\u5668\u4eba\u5728\u5de5\u4e1a\u68c0\u67e5\u4efb\u52a1\u4e2d\u7684\u6548\u7387\u548c\u7cbe\u5ea6\uff0c\u4ee5\u9002\u5e94\u590d\u6742\u7684\u5de5\u4f5c\u73af\u5883\u548c\u9ad8\u8981\u6c42\u7684\u64cd\u4f5c\u3002", "method": "\u91c7\u7528\u5206\u5c42\u89c4\u5212\u3001\u65f6\u95f4\u6700\u4f18\u4f4d\u7f6e\u751f\u6210\u548c\u96c6\u6210\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u4f18\u5316\u673a\u5668\u4eba\u6267\u884c\u68c0\u67e5\u4efb\u52a1\u7684\u6548\u7387\u548c\u7cbe\u5ea6\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u4f7f\u4eba\u5f62\u673a\u5668\u4eba\u80fd\u591f\u4ee5\u9ad8\u6548\u7387\u548c\u6beb\u7c73\u7ea7\u7cbe\u5ea6\u6267\u884c\u68c0\u67e5\u4efb\u52a1\u3002", "conclusion": "\u901a\u8fc7\u6a21\u62df\u548c\u5728Kuavo 4Pro\u4eba\u5f62\u5e73\u53f0\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u6846\u67b6\u5728\u591a\u5730\u70b9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f4e\u65f6\u95f4\u6210\u672c\u548c\u9ad8\u6210\u529f\u7387\uff0c\u80fd\u591f\u9ad8\u6548\u7cbe\u51c6\u5730\u6267\u884c\u590d\u6742\u5de5\u4e1a\u64cd\u4f5c\u3002"}}
{"id": "2510.11448", "categories": ["cs.RO", "cs.SY", "eess.SY", "C.3; D.4.1; D.4.4; D.4.8; I.2.9"], "pdf": "https://arxiv.org/pdf/2510.11448", "abs": "https://arxiv.org/abs/2510.11448", "authors": ["Yuankai He", "Hanlin Chen", "Weisong Shi"], "title": "A Faster and More Reliable Middleware for Autonomous Driving Systems", "comment": "8 pages,7 figures, 8 tables", "summary": "Ensuring safety in high-speed autonomous vehicles requires rapid control\nloops and tightly bounded delays from perception to actuation. Many open-source\nautonomy systems rely on ROS 2 middleware; when multiple sensor and control\nnodes share one compute unit, ROS 2 and its DDS transports add significant\n(de)serialization, copying, and discovery overheads, shrinking the available\ntime budget. We present Sensor-in-Memory (SIM), a shared-memory transport\ndesigned for intra-host pipelines in autonomous vehicles. SIM keeps sensor data\nin native memory layouts (e.g., cv::Mat, PCL), uses lock-free bounded double\nbuffers that overwrite old data to prioritize freshness, and integrates into\nROS 2 nodes with four lines of code. Unlike traditional middleware, SIM\noperates beside ROS 2 and is optimized for applications where data freshness\nand minimal latency outweigh guaranteed completeness. SIM provides sequence\nnumbers, a writer heartbeat, and optional checksums to ensure ordering,\nliveness, and basic integrity. On an NVIDIA Jetson Orin Nano, SIM reduces\ndata-transport latency by up to 98% compared to ROS 2 zero-copy transports such\nas FastRTPS and Zenoh, lowers mean latency by about 95%, and narrows\n95th/99th-percentile tail latencies by around 96%. In tests on a\nproduction-ready Level 4 vehicle running Autoware.Universe, SIM increased\nlocalization frequency from 7.5 Hz to 9.5 Hz. Applied across all\nlatency-critical modules, SIM cut average perception-to-decision latency from\n521.91 ms to 290.26 ms, reducing emergency braking distance at 40 mph (64 km/h)\non dry concrete by 13.6 ft (4.14 m).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSensor-in-Memory (SIM) \u7684\u5171\u4eab\u5185\u5b58\u4f20\u8f93\u65b9\u6848\uff0c\u663e\u8457\u51cf\u5c11\u9ad8\u901f\u5ea6\u81ea\u4e3b\u8f66\u8f86\u4e2d\u7684\u6570\u636e\u4f20\u8f93\u5ef6\u8fdf\uff0c\u4ee5\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "motivation": "\u5728\u9ad8\u901f\u5ea6\u81ea\u4e3b\u8f66\u8f86\u4e2d\uff0c\u786e\u4fdd\u5b89\u5168\u6027\u9700\u8981\u51cf\u5c11\u611f\u77e5\u5230\u63a7\u5236\u6267\u884c\u7684\u5ef6\u8fdf\uff0c\u800c\u73b0\u6709\u7684\u4e2d\u95f4\u4ef6\u5728\u591a\u4e2a\u8282\u70b9\u5171\u4eab\u8ba1\u7b97\u5355\u5143\u65f6\u4f1a\u5f15\u5165\u989d\u5916\u7684\u5ef6\u8fdf\u3002", "method": "\u901a\u8fc7\u5171\u4eab\u5185\u5b58\u8bbe\u8ba1\u7684\u9501-free \u53cc\u7f13\u51b2\u65b9\u6848\u3001\u539f\u751f\u5185\u5b58\u5e03\u5c40\u4ee5\u53ca\u4e0e ROS 2 \u7684\u96c6\u6210\uff0cSIM \u4f18\u5316\u4e86\u6570\u636e\u4f20\u8f93\u3002", "result": "\u5728\u6d4b\u8bd5\u4e2d\uff0cSIM \u5c06\u6570\u636e\u4f20\u8f93\u5ef6\u8fdf\u51cf\u5c11\u9ad8\u8fbe98%\uff0c\u5e76\u5728\u771f\u5b9e\u5e94\u7528\u4e2d\u63d0\u5347\u4e86\u672c\u5730\u5316\u9891\u7387\u548c\u964d\u4f4e\u4e86\u611f\u77e5\u5230\u51b3\u7b56\u7684\u5e73\u5747\u5ef6\u8fdf\u3002", "conclusion": "SIM \u5728\u9ad8\u901f\u81ea\u4e3b\u8f66\u8f86\u7684\u611f\u77e5\u5230\u51b3\u7b56\u5ef6\u8fdf\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u6709\u6548\u964d\u4f4e\u6570\u636e\u4f20\u8f93\u5ef6\u8fdf\uff0c\u63d0\u5347\u8f66\u8f86\u5b89\u5168\u6027\u3002"}}
{"id": "2510.11491", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.11491", "abs": "https://arxiv.org/abs/2510.11491", "authors": ["Murad Dawood", "Usama Ahmed Siddiquie", "Shahram Khorshidi", "Maren Bennewitz"], "title": "Constraint-Aware Reinforcement Learning via Adaptive Action Scaling", "comment": null, "summary": "Safe reinforcement learning (RL) seeks to mitigate unsafe behaviors that\narise from exploration during training by reducing constraint violations while\nmaintaining task performance. Existing approaches typically rely on a single\npolicy to jointly optimize reward and safety, which can cause instability due\nto conflicting objectives, or they use external safety filters that override\nactions and require prior system knowledge. In this paper, we propose a modular\ncost-aware regulator that scales the agent's actions based on predicted\nconstraint violations, preserving exploration through smooth action modulation\nrather than overriding the policy. The regulator is trained to minimize\nconstraint violations while avoiding degenerate suppression of actions. Our\napproach integrates seamlessly with off-policy RL methods such as SAC and TD3,\nand achieves state-of-the-art return-to-cost ratios on Safety Gym locomotion\ntasks with sparse costs, reducing constraint violations by up to 126 times\nwhile increasing returns by over an order of magnitude compared to prior\nmethods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u7684\u6210\u672c\u611f\u77e5\u8c03\u8282\u5668\uff0c\u901a\u8fc7\u9884\u6d4b\u7ea6\u675f\u8fdd\u53cd\u6765\u8c03\u6574\u4ee3\u7406\u52a8\u4f5c\uff0c\u4ece\u800c\u5728\u4fdd\u969c\u5b89\u5168\u7684\u540c\u65f6\u7ef4\u6301\u63a2\u7d22\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u5355\u4e00\u7b56\u7565\u4f18\u5316\u5956\u52b1\u548c\u5b89\u5168\uff0c\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u6216\u9700\u8981\u5148\u9a8c\u77e5\u8bc6\u7684\u5916\u90e8\u5b89\u5168\u8fc7\u6ee4\u5668\u3002", "method": "\u63d0\u51fa\u7684\u8c03\u8282\u5668\u901a\u8fc7\u5e73\u6ed1\u52a8\u4f5c\u8c03\u5236\u6765\u8c03\u6574\u4ee3\u7406\u7684\u884c\u52a8\uff0c\u907f\u514d\u4e86\u51b2\u7a81\u7684\u76ee\u6807\u5e26\u6765\u7684\u4e0d\u7a33\u5b9a\u6027\uff0c\u540c\u65f6\u4e0e\u73b0\u6709\u7684\u79bb\u7ebfRL\u65b9\u6cd5\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5728Safety Gym\u7684\u8fd0\u52a8\u4efb\u52a1\u4e2d\uff0c\u6b64\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u7ea6\u675f\u8fdd\u53cd\uff0c\u6700\u591a\u8fbe126\u500d\uff0c\u540c\u65f6\u8fd4\u56de\u63d0\u5347\u8d85\u8fc7\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5b89\u5168\u8bad\u7ec3\u4e2d\u80fd\u591f\u663e\u8457\u964d\u4f4e\u7ea6\u675f\u8fdd\u53cd\uff0c\u5e76\u63d0\u9ad8\u4efb\u52a1\u56de\u62a5\uff0c\u5c24\u5176\u5728\u7a00\u758f\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.11525", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11525", "abs": "https://arxiv.org/abs/2510.11525", "authors": ["Luis F. Recalde", "Dhruv Agrawal", "Jon Arrizabalaga", "Guanrui Li"], "title": "DQ-NMPC: Dual-Quaternion NMPC for Quadrotor Flight", "comment": "Accepted to IEEE Robotics and Automation Letters", "summary": "MAVs have great potential to assist humans in complex tasks, with\napplications ranging from logistics to emergency response. Their agility makes\nthem ideal for operations in complex and dynamic environments. However,\nachieving precise control in agile flights remains a significant challenge,\nparticularly due to the underactuated nature of quadrotors and the strong\ncoupling between their translational and rotational dynamics. In this work, we\npropose a novel NMPC framework based on dual-quaternions (DQ-NMPC) for\nquadrotor flight. By representing both quadrotor dynamics and the pose error\ndirectly on the dual-quaternion manifold, our approach enables a compact and\nglobally non-singular formulation that captures the quadrotor coupled dynamics.\nWe validate our approach through simulations and real-world experiments,\ndemonstrating better numerical conditioning and significantly improved tracking\nperformance, with reductions in position and orientation errors of up to 56.11%\nand 56.77%, compared to a conventional baseline NMPC method. Furthermore, our\ncontroller successfully handles aggressive trajectories, reaching maximum\nspeeds up to 13.66 m/s and accelerations reaching 4.2 g within confined space\nconditions of dimensions 11m x 4.5m x 3.65m under which the baseline controller\nfails.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u56db\u5143\u6570\u7684NMPC\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u56db\u65cb\u7ffc\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u63a7\u5236\u7cbe\u5ea6\u548c\u6027\u80fd\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u73b0\u51fa\u663e\u8457\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u56db\u65cb\u7ffc\u7684\u654f\u6377\u6027\u4f7f\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u7531\u4e8e\u6b20\u9a71\u52a8\u7279\u6027\u548c\u52a8\u529b\u5b66\u7684\u5f3a\u8026\u5408\u6027\uff0c\u7cbe\u51c6\u63a7\u5236\u4ecd\u7136\u9762\u4e34\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5728\u53cc\u56db\u5143\u6570\u6d41\u5f62\u4e0a\u76f4\u63a5\u8868\u793a\u56db\u65cb\u7ffc\u52a8\u6001\u548c\u59ff\u6001\u8bef\u5dee\uff0c\u6784\u5efa\u4e86\u7d27\u51d1\u4e14\u5168\u5c40\u975e\u5947\u5f02\u7684\u6846\u67b6\u3002", "result": "\u76f8\u8f83\u4e8e\u4f20\u7edf\u57fa\u7ebfNMPC\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u4f4d\u7f6e\u548c\u65b9\u5411\u8bef\u5dee\u4e0a\u5206\u522b\u51cf\u5c11\u4e8656.11%\u548c56.77%\uff0c\u5728\u6536\u7d27\u7a7a\u95f4\u5185\u6210\u529f\u5b8c\u6210\u4e86\u6700\u5927\u901f\u5ea613.66 m/s\u548c\u52a0\u901f\u5ea64.2 g\u7684\u653b\u51fb\u6027\u8f68\u8ff9\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u57fa\u4e8e\u53cc\u56db\u5143\u6570\uff08DQ-NMPC\uff09\u7684NMPC\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u56db\u65cb\u7ffc\u98de\u884c\u63a7\u5236\u7684\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u548c\u52a8\u6001\u73af\u5883\u4e2d\u3002"}}
{"id": "2510.11534", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.11534", "abs": "https://arxiv.org/abs/2510.11534", "authors": ["Enli Lin", "Ziyuan Yang", "Qiujing Lu", "Jianming Hu", "Shuo Feng"], "title": "IntersectioNDE: Learning Complex Urban Traffic Dynamics based on Interaction Decoupling Strategy", "comment": "Accepted by ITSC 2025", "summary": "Realistic traffic simulation is critical for ensuring the safety and\nreliability of autonomous vehicles (AVs), especially in complex and diverse\nurban traffic environments. However, existing data-driven simulators face two\nkey challenges: a limited focus on modeling dense, heterogeneous interactions\nat urban intersections - which are prevalent, crucial, and practically\nsignificant in countries like China, featuring diverse agents including\nmotorized vehicles (MVs), non-motorized vehicles (NMVs), and pedestrians - and\nthe inherent difficulty in robustly learning high-dimensional joint\ndistributions for such high-density scenes, often leading to mode collapse and\nlong-term simulation instability. We introduce City Crossings Dataset\n(CiCross), a large-scale dataset collected from a real-world urban\nintersection, uniquely capturing dense, heterogeneous multi-agent interactions,\nparticularly with a substantial proportion of MVs, NMVs and pedestrians. Based\non this dataset, we propose IntersectioNDE (Intersection Naturalistic Driving\nEnvironment), a data-driven simulator tailored for complex urban intersection\nscenarios. Its core component is the Interaction Decoupling Strategy (IDS), a\ntraining paradigm that learns compositional dynamics from agent subsets,\nenabling the marginal-to-joint simulation. Integrated into a scene-aware\nTransformer network with specialized training techniques, IDS significantly\nenhances simulation robustness and long-term stability for modeling\nheterogeneous interactions. Experiments on CiCross show that IntersectioNDE\noutperforms baseline methods in simulation fidelity, stability, and its ability\nto replicate complex, distribution-level urban traffic dynamics.", "AI": {"tldr": "\u63d0\u51fa\u4e86City Crossings Dataset (CiCross) \u548c IntersectioNDE\u6a21\u62df\u5668\uff0c\u4ee5\u63d0\u5347\u57ce\u5e02\u4ea4\u53c9\u53e3\u7684\u4ea4\u901a\u6a21\u62df\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6a21\u62df\u5668\u5728\u57ce\u5e02\u4ea4\u53c9\u53e3\u9ad8\u5ea6\u5bc6\u96c6\u548c\u591a\u6837\u5316\u4e92\u52a8\u5efa\u6a21\u7684\u4e0d\u8db3\uff0c\u786e\u4fdd\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u7684\u5b89\u5168\u548c\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528Interaction Decoupling Strategy\u548c\u573a\u666f\u611f\u77e5\u7684Transformer\u7f51\u7edc\uff0c\u8bad\u7ec3\u6570\u636e\u9a71\u52a8\u7684\u4ea4\u901a\u6a21\u62df\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cIntersectioNDE\u5728\u6a21\u62df\u903c\u771f\u5ea6\u3001\u7a33\u5b9a\u6027\u4ee5\u53ca\u590d\u6742\u4ea4\u901a\u52a8\u6001\u518d\u73b0\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "IntersectioNDE\u5728\u6a21\u62df\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6210\u529f\u6a21\u62df\u590d\u6742\u7684\u57ce\u5e02\u4ea4\u901a\u52a8\u6001\u3002"}}
{"id": "2510.11539", "categories": ["cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.11539", "abs": "https://arxiv.org/abs/2510.11539", "authors": ["Denglin Cheng", "Jiarong Kang", "Xiaobin Xiong"], "title": "Simultaneous Calibration of Noise Covariance and Kinematics for State Estimation of Legged Robots via Bi-level Optimization", "comment": null, "summary": "Accurate state estimation is critical for legged and aerial robots operating\nin dynamic, uncertain environments. A key challenge lies in specifying process\nand measurement noise covariances, which are typically unknown or manually\ntuned. In this work, we introduce a bi-level optimization framework that\njointly calibrates covariance matrices and kinematic parameters in an\nestimator-in-the-loop manner. The upper level treats noise covariances and\nmodel parameters as optimization variables, while the lower level executes a\nfull-information estimator. Differentiating through the estimator allows direct\noptimization of trajectory-level objectives, resulting in accurate and\nconsistent state estimates. We validate our approach on quadrupedal and\nhumanoid robots, demonstrating significantly improved estimation accuracy and\nuncertainty calibration compared to hand-tuned baselines. Our method unifies\nstate estimation, sensor, and kinematics calibration into a principled,\ndata-driven framework applicable across diverse robotic platforms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u6846\u67b6\u63d0\u5347\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\u7cbe\u5ea6\u7684\u65b9\u6cd5\u3002", "motivation": "\u5728\u52a8\u6001\u548c\u4e0d\u786e\u5b9a\u7684\u73af\u5883\u4e2d\uff0c\u817f\u884c\u673a\u5668\u4eba\u548c\u7a7a\u4e2d\u673a\u5668\u4eba\u5bf9\u51c6\u786e\u72b6\u6001\u4f30\u8ba1\u7684\u9700\u6c42\u5f88\u9ad8\uff0c\u4f46\u4f20\u7edf\u7684\u566a\u58f0\u534f\u65b9\u5dee\u8c03\u6574\u590d\u6742\u4e14\u4e0d\u6613\u3002", "method": "\u4e0a\u5c42\u5c06\u566a\u58f0\u534f\u65b9\u5dee\u548c\u6a21\u578b\u53c2\u6570\u89c6\u4e3a\u4f18\u5316\u53d8\u91cf\uff0c\u4e0b\u5c42\u6267\u884c\u5168\u4fe1\u606f\u4f30\u8ba1\u5668\uff0c\u901a\u8fc7\u4f30\u8ba1\u5668\u5bfc\u6570\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5c42\u4f18\u5316\u6846\u67b6\uff0c\u53ef\u4ee5\u5728\u4f30\u8ba1\u5668\u5faa\u73af\u4e2d\u8054\u5408\u6821\u51c6\u534f\u65b9\u5dee\u77e9\u9635\u548c\u8fd0\u52a8\u5b66\u53c2\u6570\u3002", "conclusion": "\u6211\u4eec\u7684\u6a21\u578b\u5728\u56db\u8db3\u548c\u7c7b\u4eba\u673a\u5668\u4eba\u4e0a\u7ecf\u8fc7\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u72b6\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u3002"}}
{"id": "2510.11542", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11542", "abs": "https://arxiv.org/abs/2510.11542", "authors": ["Neil C. Janwani", "Varun Madabushi", "Maegan Tucker"], "title": "NaviGait: Navigating Dynamically Feasible Gait Libraries using Deep Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) has emerged as a powerful method to learn robust\ncontrol policies for bipedal locomotion. Yet, it can be difficult to tune\ndesired robot behaviors due to unintuitive and complex reward design. In\ncomparison, offline trajectory optimization methods, like Hybrid Zero Dynamics,\noffer more tuneable, interpretable, and mathematically grounded motion plans\nfor high-dimensional legged systems. However, these methods often remain\nbrittle to real-world disturbances like external perturbations.\n  In this work, we present NaviGait, a hierarchical framework that combines the\nstructure of trajectory optimization with the adaptability of RL for robust and\nintuitive locomotion control. NaviGait leverages a library of offline-optimized\ngaits and smoothly interpolates between them to produce continuous reference\nmotions in response to high-level commands. The policy provides both\njoint-level and velocity command residual corrections to modulate and stabilize\nthe reference trajectories in the gait library. One notable advantage of\nNaviGait is that it dramatically simplifies reward design by encoding rich\nmotion priors from trajectory optimization, reducing the need for finely tuned\nshaping terms and enabling more stable and interpretable learning. Our\nexperimental results demonstrate that NaviGait enables faster training compared\nto conventional and imitation-based RL, and produces motions that remain\nclosest to the original reference. Overall, by decoupling high-level motion\ngeneration from low-level correction, NaviGait offers a more scalable and\ngeneralizable approach for achieving dynamic and robust locomotion.", "AI": {"tldr": "NaviGait\u662f\u7ed3\u5408\u8f68\u8ff9\u4f18\u5316\u4e0eRL\u7684\u5206\u5c42\u6846\u67b6\uff0c\u901a\u8fc7\u4e30\u5bcc\u7684\u8fd0\u52a8\u5148\u9a8c\u7b80\u5316\u5956\u52b1\u8bbe\u8ba1\uff0c\u63d0\u9ad8\u6b65\u6001\u63a7\u5236\u7684\u7a33\u5065\u6027\u548c\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "RL\u5728\u53cc\u8db3\u8fd0\u52a8\u63a7\u5236\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u590d\u6742\u7684\u5956\u52b1\u8bbe\u8ba1\u4f7f\u5f97\u884c\u4e3a\u8c03\u6574\u56f0\u96be\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6613\u4e8e\u8c03\u6574\u548c\u89e3\u91ca\u7684\u65b9\u6cd5\u3002", "method": "NaviGait\u6846\u67b6\u7ed3\u5408\u4e86\u79bb\u7ebf\u4f18\u5316\u6b65\u6001\u5e93\u4e0e\u5c42\u6b21\u5316\u7684RL\u7b56\u7565\uff0c\u901a\u8fc7\u5e73\u6ed1\u63d2\u503c\u4ea7\u751f\u8fde\u7eed\u53c2\u8003\u8fd0\u52a8\u5e76\u8fdb\u884c\u6307\u4ee4\u8c03\u6574\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cNaviGait\u7684\u8bad\u7ec3\u901f\u5ea6\u76f8\u6bd4\u4f20\u7edf\u548c\u6a21\u4eff\u57fa\u7684RL\u66f4\u5feb\uff0c\u4e14\u751f\u6210\u7684\u8fd0\u52a8\u66f4\u63a5\u8fd1\u539f\u59cb\u53c2\u8003\u6b65\u6001\u3002", "conclusion": "NaviGait\u901a\u8fc7\u5c06\u8f68\u8ff9\u4f18\u5316\u7ed3\u6784\u4e0eRL\u7684\u9002\u5e94\u6027\u7ed3\u5408\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u5177\u6709\u9ad8\u5ea6\u7a33\u5065\u6027\u7684\u6b65\u6001\u63a7\u5236\u65b9\u6cd5\u3002"}}
{"id": "2510.11552", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11552", "abs": "https://arxiv.org/abs/2510.11552", "authors": ["Gregoire Passault", "Clement Gaspard", "Olivier Ly"], "title": "Robot Soccer Kit: Omniwheel Tracked Soccer Robots for Education", "comment": null, "summary": "Recent developments of low cost off-the-shelf programmable components, their\nmodularity, and also rapid prototyping made educational robotics flourish, as\nit is accessible in most schools today. They allow to illustrate and embody\ntheoretical problems in practical and tangible applications, and gather\nmultidisciplinary skills. They also give a rich natural context for\nproject-oriented pedagogy. However, most current robot kits all are limited to\negocentric aspect of the robots perception. This makes it difficult to access\nmore high-level problems involving e.g. coordinates or navigation. In this\npaper we introduce an educational holonomous robot kit that comes with an\nexternal tracking system, which lightens the constraint on embedded systems,\nbut allows in the same time to discover high-level aspects of robotics,\notherwise unreachable.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u6559\u80b2\u6027\u5168\u81ea\u4e3b\u673a\u5668\u4eba\u5957\u4ef6\uff0c\u8be5\u5957\u4ef6\u914d\u5907\u5916\u90e8\u8ddf\u8e2a\u7cfb\u7edf\uff0c\u7a81\u7834\u4e86\u73b0\u6709\u673a\u5668\u4eba\u5728\u611f\u77e5\u65b9\u9762\u7684\u5c40\u9650\uff0c\u9002\u7528\u4e8e\u9ad8\u9636\u673a\u5668\u4eba\u95ee\u9898\u7684\u6559\u5b66\u3002", "motivation": "\u968f\u7740\u53ef\u7f16\u7a0b\u7ec4\u4ef6\u7684\u6210\u672c\u4e0b\u964d\u548c\u6a21\u5757\u5316\u589e\u5f3a\uff0c\u6559\u80b2\u673a\u5668\u4eba\u5728\u5b66\u6821\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u666e\u53ca\uff0c\u65e8\u5728\u901a\u8fc7\u5b9e\u7528\u5e94\u7528\u548c\u9879\u76ee\u5bfc\u5411\u7684\u6559\u5b66\u6765\u63d0\u5347\u5b66\u751f\u7684\u591a\u5b66\u79d1\u6280\u80fd\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u5916\u90e8\u8ddf\u8e2a\u7cfb\u7edf\uff0c\u964d\u4f4e\u4e86\u5bf9\u5d4c\u5165\u5f0f\u7cfb\u7edf\u7684\u7ea6\u675f\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u63a2\u7d22\u9ad8\u9636\u673a\u5668\u4eba\u6982\u5ff5\u7684\u80fd\u529b\u3002", "result": "\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u673a\u5668\u4eba\u5957\u4ef6\uff0c\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5b66\u4e60\u5de5\u5177\uff0c\u4f7f\u5b66\u751f\u53ef\u4ee5\u63a5\u89e6\u66f4\u590d\u6742\u7684\u673a\u5668\u4eba\u4efb\u52a1\u3002", "conclusion": "\u6559\u80b2\u6027\u5168\u81ea\u4e3b\u673a\u5668\u4eba\u5957\u4ef6\u80fd\u591f\u6709\u6548\u589e\u5f3a\u673a\u5668\u4eba\u6559\u5b66\u7684\u5e7f\u5ea6\u4e0e\u6df1\u5ea6\uff0c\u4fc3\u8fdb\u5b66\u751f\u638c\u63e1\u66f4\u9ad8\u9636\u7684\u673a\u5668\u4eba\u6280\u80fd\u3002"}}
{"id": "2510.11566", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.11566", "abs": "https://arxiv.org/abs/2510.11566", "authors": ["Kuanning Wang", "Yongchong Gu", "Yuqian Fu", "Zeyu Shangguan", "Sicheng He", "Xiangyang Xue", "Yanwei Fu", "Daniel Seita"], "title": "SCOOP'D: Learning Mixed-Liquid-Solid Scooping via Sim2Real Generative Policy", "comment": "Project page is at https://scoopdiff.github.io/", "summary": "Scooping items with tools such as spoons and ladles is common in daily life,\nranging from assistive feeding to retrieving items from environmental disaster\nsites. However, developing a general and autonomous robotic scooping policy is\nchallenging since it requires reasoning about complex tool-object interactions.\nFurthermore, scooping often involves manipulating deformable objects, such as\ngranular media or liquids, which is challenging due to their\ninfinite-dimensional configuration spaces and complex dynamics. We propose a\nmethod, SCOOP'D, which uses simulation from OmniGibson (built on NVIDIA\nOmniverse) to collect scooping demonstrations using algorithmic procedures that\nrely on privileged state information. Then, we use generative policies via\ndiffusion to imitate demonstrations from observational input. We directly apply\nthe learned policy in diverse real-world scenarios, testing its performance on\nvarious item quantities, item characteristics, and container types. In\nzero-shot deployment, our method demonstrates promising results across 465\ntrials in diverse scenarios, including objects of different difficulty levels\nthat we categorize as \"Level 1\" and \"Level 2.\" SCOOP'D outperforms all\nbaselines and ablations, suggesting that this is a promising approach to\nacquiring robotic scooping skills. Project page is at\nhttps://scoopdiff.github.io/.", "AI": {"tldr": "SCOOP'D\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u4e3b\u673a\u5668\u4eba\u6316\u6398\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u57fa\u4e8eOmniGibson\u5e73\u53f0\u7684\u4eff\u771f\uff0c\u901a\u8fc7\u5b66\u4e60\u793a\u4f8b\u548c\u751f\u6210\u7b56\u7565\u5b9e\u73b0\u9ad8\u6548\u7684\u6316\u6398\u4efb\u52a1\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u901a\u7528\u4e14\u81ea\u4e3b\u7684\u673a\u5668\u4eba\u6316\u6398\u7b56\u7565\uff0c\u514b\u670d\u590d\u6742\u5de5\u5177-\u7269\u4f53\u4ea4\u4e92\u548c\u53ef\u53d8\u5f62\u7269\u4f53\u64cd\u4f5c\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528OmniGibson\u8fdb\u884c\u4eff\u771f\uff0c\u6536\u96c6\u793a\u4f8b\u5e76\u91c7\u7528\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u7b56\u7565\u8fdb\u884c\u6a21\u4eff\u3002", "result": "\u6211\u4eec\u63d0\u51faSCOOP'D\u65b9\u6cd5\uff0c\u5b83\u5229\u7528OmniGibson\u5e73\u53f0\u7684\u4eff\u771f\uff0c\u901a\u8fc7\u7b97\u6cd5\u7a0b\u5e8f\u6536\u96c6\u4f7f\u7528\u5de5\u5177\u65f6\u7684\u6316\u6398\u793a\u4f8b\uff0c\u8fdb\u800c\u4f7f\u7528\u6269\u6563\u751f\u6210\u7b56\u7565\u901a\u8fc7\u89c2\u5bdf\u8f93\u5165\u6765\u6a21\u4eff\u8fd9\u4e9b\u793a\u4f8b\u3002\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u96f6\u6b21\u90e8\u7f72\u8868\u73b0\u51fa\u8272\uff0c\u663e\u793a\u4e86\u83b7\u53d6\u673a\u5668\u4eba\u6316\u6398\u6280\u80fd\u7684\u6f5c\u529b\u3002", "conclusion": "SCOOP'D\u5728\u5404\u79cd\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u8868\u73b0\u8d85\u8d8a\u4e86\u6240\u6709\u57fa\u7ebf\uff0c\u5e76\u8868\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u83b7\u53d6\u673a\u5668\u4eba\u6316\u6398\u6280\u80fd\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.11574", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.11574", "abs": "https://arxiv.org/abs/2510.11574", "authors": ["Lennart Werner", "Pol Eyschen", "Sean Costello", "Pierluigi Micarelli", "Marco Hutter"], "title": "Calibrated Dynamic Modeling for Force and Payload Estimation in Hydraulic Machinery", "comment": null, "summary": "Accurate real-time estimation of end effector interaction forces in hydraulic\nexcavators is a key enabler for advanced automation in heavy machinery.\nAccurate knowledge of these forces allows improved, precise grading and digging\nmaneuvers. To address these challenges, we introduce a high-accuracy,\nretrofittable 2D force- and payload estimation algorithm that does not impose\nadditional requirements on the operator regarding trajectory, acceleration or\nthe use of the slew joint. The approach is designed for retrofittability,\nrequires minimal calibration and no prior knowledge of machine-specific dynamic\ncharacteristics. Specifically, we propose a method for identifying a dynamic\nmodel, necessary to estimate both end effector interaction forces and bucket\npayload during normal operation. Our optimization-based payload estimation\nachieves a full-scale payload accuracy of 1%. On a standard 25 t excavator, the\nonline force measurement from pressure and inertial measurements achieves a\ndirection accuracy of 13 degree and a magnitude accuracy of 383 N. The method's\naccuracy and generalization capability are validated on two excavator platforms\nof different type and weight classes. We benchmark our payload estimation\nagainst a classical quasistatic method and a commercially available system. Our\nsystem outperforms both in accuracy and precision.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u7cbe\u5ea6\u3001\u53ef retrofittable \u76842D\u529b\u548c\u8f7d\u8377\u4f30\u8ba1\u7b97\u6cd5\uff0c\u5728\u6316\u6398\u673a\u4e0a\u5177\u6709\u4f18\u8d8a\u7684\u51c6\u786e\u6027\u548c\u7cbe\u5ea6\uff0c\u4e14\u4e0d\u9700\u7279\u5b9a\u7684\u52a8\u6001\u7279\u6027\u77e5\u8bc6\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5b9e\u65f6\u51c6\u786e\u4f30\u8ba1\u6db2\u538b\u6316\u6398\u673a\u672b\u7aef\u6267\u884c\u5668\u76f8\u4e92\u4f5c\u7528\u529b\u7684\u7b97\u6cd5\uff0c\u4ee5\u4fbf\u4e8e\u6539\u5584\u91cd\u578b\u673a\u68b0\u7684\u81ea\u52a8\u5316\u548c\u64cd\u4f5c\u7cbe\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u7cbe\u5ea6\u76842D\u529b\u548c\u8f7d\u8377\u4f30\u8ba1\u7b97\u6cd5\uff0c\u901a\u8fc7\u538b\u529b\u548c\u60ef\u6027\u6d4b\u91cf\u5b9e\u73b0\u5728\u7ebf\u529b\u6d4b\u91cf\u3002", "result": "\u5728\u6807\u51c625\u5428\u6316\u6398\u673a\u4e0a\u5b9e\u73b0\u4e86\u6ee1\u8f7d\u7cbe\u5ea6\u4e3a1%\u7684\u8f7d\u8377\u4f30\u8ba1\uff0c\u5e76\u4e14\u529b\u7684\u65b9\u5411\u7cbe\u5ea6\u4e3a13\u5ea6\uff0c\u5927\u5c0f\u7cbe\u5ea6\u4e3a383N\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728\u51c6\u786e\u6027\u548c\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u4f20\u7edf\u7684\u51c6\u9759\u6001\u65b9\u6cd5\u548c\u5e02\u552e\u7cfb\u7edf\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7c7b\u578b\u548c\u91cd\u91cf\u7b49\u7ea7\u7684\u6316\u6398\u673a\u3002"}}
{"id": "2510.11660", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11660", "abs": "https://arxiv.org/abs/2510.11660", "authors": ["Yi Yang", "Kefan Gu", "Yuqing Wen", "Hebei Li", "Yucheng Zhao", "Tiancai Wang", "Xudong Liu"], "title": "ManiAgent: An Agentic Framework for General Robotic Manipulation", "comment": "8 pages, 6 figures, conference", "summary": "While Vision-Language-Action (VLA) models have demonstrated impressive\ncapabilities in robotic manipulation, their performance in complex reasoning\nand long-horizon task planning is limited by data scarcity and model capacity.\nTo address this, we introduce ManiAgent, an agentic architecture for general\nmanipulation tasks that achieves end-to-end output from task descriptions and\nenvironmental inputs to robotic manipulation actions. In this framework,\nmultiple agents involve inter-agent communication to perform environmental\nperception, sub-task decomposition and action generation, enabling efficient\nhandling of complex manipulation scenarios. Evaluations show ManiAgent achieves\nan 86.8% success rate on the SimplerEnv benchmark and 95.8% on real-world\npick-and-place tasks, enabling efficient data collection that yields VLA models\nwith performance comparable to those trained on human-annotated datasets.The\nproject webpage is available at https://yi-yang929.github.io/ManiAgent/.", "AI": {"tldr": "ManiAgent\u901a\u8fc7\u591a\u4ee3\u7406\u534f\u540c\u5de5\u4f5c\u89e3\u51b3\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\uff0c\u5c55\u793a\u51fa\u9ad8\u6210\u529f\u7387\u548c\u63a5\u8fd1\u4eba\u7c7b\u6807\u6ce8\u6570\u636e\u96c6\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709VLA\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u548c\u957f\u8fdc\u4efb\u52a1\u89c4\u5212\u4e2d\u56e0\u4e3a\u6570\u636e\u7a00\u7f3a\u548c\u6a21\u578b\u80fd\u529b\u53d7\u9650\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u591a\u4e2a\u4ee3\u7406\u901a\u8fc7\u76f8\u4e92\u901a\u4fe1\u6765\u5904\u7406\u64cd\u4f5c\u4efb\u52a1\uff0c\u5305\u62ec\u73af\u5883\u611f\u77e5\u3001\u5b50\u4efb\u52a1\u5206\u89e3\u548c\u884c\u52a8\u751f\u6210\u3002", "result": "\u5728SimplerEnv\u57fa\u51c6\u4e0a\u8fbe\u523086.8%\u7684\u6210\u529f\u7387\uff0c\u73b0\u5b9e\u4e16\u754c\u62fe\u53d6\u548c\u653e\u7f6e\u4efb\u52a1\u4e0a\u8fbe\u523095.8%\u7684\u6210\u529f\u7387\u3002", "conclusion": "ManiAgent\u6846\u67b6\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u80fd\u529b\uff0c\u5e76\u5728\u73af\u5883\u611f\u77e5\u548c\u884c\u52a8\u751f\u6210\u4e0a\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.11682", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.11682", "abs": "https://arxiv.org/abs/2510.11682", "authors": ["Hang Liu", "Yuman Gao", "Sangli Teng", "Yufeng Chi", "Yakun Sophia Shao", "Zhongyu Li", "Maani Ghaffari", "Koushil Sreenath"], "title": "Ego-Vision World Model for Humanoid Contact Planning", "comment": null, "summary": "Enabling humanoid robots to exploit physical contact, rather than simply\navoid collisions, is crucial for autonomy in unstructured environments.\nTraditional optimization-based planners struggle with contact complexity, while\non-policy reinforcement learning (RL) is sample-inefficient and has limited\nmulti-task ability. We propose a framework combining a learned world model with\nsampling-based Model Predictive Control (MPC), trained on a demonstration-free\noffline dataset to predict future outcomes in a compressed latent space. To\naddress sparse contact rewards and sensor noise, the MPC uses a learned\nsurrogate value function for dense, robust planning. Our single, scalable model\nsupports contact-aware tasks, including wall support after perturbation,\nblocking incoming objects, and traversing height-limited arches, with improved\ndata efficiency and multi-task capability over on-policy RL. Deployed on a\nphysical humanoid, our system achieves robust, real-time contact planning from\nproprioception and ego-centric depth images. Website:\nhttps://ego-vcp.github.io/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5b66\u4e60\u6a21\u578b\u548cMPC\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u7c7b\u4eba\u673a\u5668\u4eba\u7684\u63a5\u89e6\u5229\u7528\u80fd\u529b\u548c\u6570\u636e\u6548\u7387\u3002", "motivation": "\u4f7f\u7c7b\u4eba\u673a\u5668\u4eba\u80fd\u591f\u5229\u7528\u7269\u7406\u63a5\u89e6\u4ee5\u63d0\u9ad8\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u80fd\u529b", "method": "\u7ed3\u5408\u5b66\u4e60\u7684\u4e16\u754c\u6a21\u578b\u4e0e\u57fa\u4e8e\u91c7\u6837\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09", "result": "\u5728\u771f\u5b9e\u7684\u7c7b\u4eba\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u5b9e\u65f6\u63a5\u89e6\u89c4\u5212\uff0c\u652f\u6301\u591a\u4efb\u52a1\u80fd\u529b", "conclusion": "\u8be5\u7cfb\u7edf\u5728\u7c7b\u4eba\u673a\u5668\u4eba\u4e0a\u6210\u529f\u5e94\u7528\uff0c\u663e\u793a\u51fa\u5728\u63a5\u89e6\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2510.11689", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11689", "abs": "https://arxiv.org/abs/2510.11689", "authors": ["Maggie Wang", "Stephen Tian", "Aiden Swann", "Ola Shorinwa", "Jiajun Wu", "Mac Schwager"], "title": "Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for Uncertainty-Aware Sim-to-Real Manipulation", "comment": null, "summary": "Learning robotic manipulation policies directly in the real world can be\nexpensive and time-consuming. While reinforcement learning (RL) policies\ntrained in simulation present a scalable alternative, effective sim-to-real\ntransfer remains challenging, particularly for tasks that require precise\ndynamics. To address this, we propose Phys2Real, a real-to-sim-to-real RL\npipeline that combines vision-language model (VLM)-inferred physical parameter\nestimates with interactive adaptation through uncertainty-aware fusion. Our\napproach consists of three core components: (1) high-fidelity geometric\nreconstruction with 3D Gaussian splatting, (2) VLM-inferred prior distributions\nover physical parameters, and (3) online physical parameter estimation from\ninteraction data. Phys2Real conditions policies on interpretable physical\nparameters, refining VLM predictions with online estimates via ensemble-based\nuncertainty quantification. On planar pushing tasks of a T-block with varying\ncenter of mass (CoM) and a hammer with an off-center mass distribution,\nPhys2Real achieves substantial improvements over a domain randomization\nbaseline: 100% vs 79% success rate for the bottom-weighted T-block, 57% vs 23%\nin the challenging top-weighted T-block, and 15% faster average task completion\nfor hammer pushing. Ablation studies indicate that the combination of VLM and\ninteraction information is essential for success. Project website:\nhttps://phys2real.github.io/ .", "AI": {"tldr": "Phys2Real\u662f\u4e00\u4e2a\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u878d\u5408\u7684RL\u7ba1\u9053\uff0c\u4ee5\u4f18\u5316\u673a\u5668\u4eba\u64cd\u63a7\u7b56\u7565\u7684\u8f6c\u79fb\u8fc7\u7a0b\u3002", "motivation": "\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u76f4\u63a5\u5b66\u4e60\u673a\u5668\u4eba\u64cd\u63a7\u7b56\u7565\u7684\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u5b9e\u9645\u5e94\u7528\u4e2d\u4ece\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u5c24\u4e3a\u56f0\u96be\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u7b56\u7565\u4f18\u5316\u6d41\u7a0b\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u62ec\u4e09\u5927\u6838\u5fc3\u7ec4\u4ef6\uff1a\u9ad8\u4fdd\u771f\u51e0\u4f55\u91cd\u5efa\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u65ad\u7684\u7269\u7406\u53c2\u6570\u5148\u9a8c\u5206\u5e03\uff0c\u4ee5\u53ca\u57fa\u4e8e\u4ea4\u4e92\u6570\u636e\u7684\u5728\u7ebf\u7269\u7406\u53c2\u6570\u4f30\u8ba1\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPhys2Real\u7684RL\u7ba1\u9053\uff0c\u4ee5\u5b9e\u73b0\u6709\u6548\u7684\u73b0\u5b9e\u4e16\u754c\u5230\u6a21\u62df\u518d\u5230\u73b0\u5b9e\u4e16\u754c\u7684\u8fc1\u79fb\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5bf9\u52a8\u6001\u8981\u6c42\u7cbe\u786e\u7684\u4efb\u52a1\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPhys2Real\u5728\u591a\u4e2a\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6210\u529f\u7387\u548c\u4efb\u52a1\u5b8c\u6210\u901f\u5ea6\uff0c\u5173\u952e\u5728\u4e8e\u7ed3\u5408\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u4ea4\u4e92\u6570\u636e\uff0c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7269\u7406\u53c2\u6570\u4f30\u8ba1\u3002"}}
