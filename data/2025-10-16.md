<div id=toc></div>

# Table of Contents

- [cs.HC](#cs.HC) [Total: 10]
- [cs.RO](#cs.RO) [Total: 30]


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [1] [Changing Oneself by Teaching Others? Exploring the Protégé Effect in Digital Stress Self-Regulation](https://arxiv.org/abs/2510.12944)
*Sameha Alshakhsi,Ala Yankouskaya,Dena Al-Thani,Raian Ali*

Main category: cs.HC

TL;DR: 这项研究评估了一个基于顾问效应的干预措施，以管理数字压力，结果显示在行为改变方面存在困难。


<details>
  <summary>Details</summary>
Motivation: 探索顾问效应是否能支持新行为的获取，特别是在管理数字压力的情况下。

Method: 本研究对137名参与者进行了为期三周的干预，分为四组：两组为顾问型干预，涉及教导材料和准备内容，另一组为数字素养培训，最后一组为对照组。

Result: 这项研究表明，尽管顾问效应在知识和技能获取方面显示了潜力，但在行为改变方面的应用仍面临挑战。

Conclusion: 基于顾问效应的干预措施在行为改变方面的有效性有限，尤其是在数字压力背景下。

Abstract: The prot\'eg\'ee effect suggests that individuals learn more effectively when
they teach a subject. While this has shown potential for acquiring knowledge
and skills, can it also support acquiring a new behaviour? This study evaluated
a prot\'eg\'e-based intervention designed to manage digital stress. Over three
weeks, 137 participants with moderate to high digital stress were assigned to
four groups. Two were prot\'eg\'ee-based: a passive group, given material to
teach, and an active group, received headlines and had to search for and
prepare teaching content. Both groups completed three sessions, each focused on
one digital stress component: availability demand stress, approval anxiety, and
fear of missing out. A digital literacy group received similar content and
quizzes, and a control group. Outcomes measured included digital stress,
problematic social media use, word-of-mouth about its management, and issue
involvement. Findings highlight the challenge of translating cognitive
engagement into behavioural change, especially amid persistent digital habits
and socially reinforced stressors. Results offer insights into the limitations
of interventions based on the prot\'eg\'ee effect when applied to behaviour
change, particularly in the context of reflective digital wellbeing strategies.
Future research could explore interactive formats, such as peer engagement or
self-regulatory elements, to enhance motivation and impact.

</details>


### [2] [TaskAudit: Detecting Functiona11ity Errors in Mobile Apps via Agentic Task Execution](https://arxiv.org/abs/2510.12972)
*Mingyuan Zhong,Xia Chen,Davin Win Kyi,Chen Li,James Fogarty,Jacob O. Wobbrock*

Main category: cs.HC

TL;DR: TaskAudit通过模拟交互检测功能性可访问性错误，相较于现有工具更具效能。


<details>
  <summary>Details</summary>
Motivation: 当前的可访问性检查工具大多只评估静态或机械生成的上下文，无法捕获影响移动应用功能的常见可访问性错误。

Method: TaskAudit，采用模拟交互检测功能性错误的可访问性评估系统。

Result: TaskAudit在54个应用屏幕中检测到48个功能性错误，显著超越现有检查器的检测能力（4到20个错误）。

Conclusion: TaskAudit能有效识别标签功能不匹配、导航混乱和不当反馈等常见错误模式，提升了可访问性评估的实用性。

Abstract: Accessibility checkers are tools in support of accessible app development and
their use is encouraged by accessibility best practices. However, most current
checkers evaluate static or mechanically-generated contexts, failing to capture
common accessibility errors impacting mobile app functionality. We present
TaskAudit, an accessibility evaluation system that focuses on detecting
functiona11ity errors through simulated interactions. TaskAudit comprises three
components: a Task Generator that constructs interactive tasks from app
screens, a Task Executor that uses agents with a screen reader proxy to perform
these tasks, and an Accessibility Analyzer that detects and reports
accessibility errors by examining interaction traces. Evaluation on real-world
apps shows that our strategy detects 48 functiona11ity errors from 54 app
screens, compared to between 4 and 20 with existing checkers. Our analysis
demonstrates common error patterns that TaskAudit can detect in addition to
prior work, including label-functionality mismatch, cluttered navigation, and
inappropriate feedback.

</details>


### [3] [Behavioral Biometrics for Automatic Detection of User Familiarity in VR](https://arxiv.org/abs/2510.12988)
*Numan Zafar,Priyo Ranjan Kundu Prosun,Shafique Ahmad Chaudhry*

Main category: cs.HC

TL;DR: 研究通过分析手部运动模式，自动检测用户在VR中的熟悉程度，取得92.05%的最高准确率，为个性化和适应性VR体验铺平道路。


<details>
  <summary>Details</summary>
Motivation: 随着VR设备在日常生活中的广泛应用，许多用户在没有先前经验的情况下开始使用VR系统，自动检测用户熟悉程度有助于实时适应用户需求。

Method: 本研究使用尖端的深度分类器对用户在进行门锁输入任务时的手部运动模式进行分析，以实现自动识别用户的VR熟悉程度。

Result: 本研究探讨了通过分析手部运动模式来自动检测用户对虚拟现实（VR）设备的熟悉程度，以实时调整培训和界面，提高任务表现并减少用户挫败感。

Conclusion: 本研究结果表明，通过手部运动生物特征可以实时检测用户在关键VR应用中的熟悉程度，有助于提供个性化和适应性更强的VR体验。

Abstract: As virtual reality (VR) devices become increasingly integrated into everyday
settings, a growing number of users without prior experience will engage with
VR systems. Automatically detecting a user's familiarity with VR as an
interaction medium enables real-time, adaptive training and interface
adjustments, minimizing user frustration and improving task performance. In
this study, we explore the automatic detection of VR familiarity by analyzing
hand movement patterns during a passcode-based door-opening task, which is a
well-known interaction in collaborative virtual environments such as meeting
rooms, offices, and healthcare spaces. While novice users may lack prior VR
experience, they are likely to be familiar with analogous real-world tasks
involving keypad entry. We conducted a pilot study with 26 participants, evenly
split between experienced and inexperienced VR users, who performed tasks using
both controller-based and hand-tracking interactions. Our approach uses
state-of-the-art deep classifiers for automatic VR familiarity detection,
achieving the highest accuracies of 92.05% and 83.42% for hand-tracking and
controller-based interactions, respectively. In the cross-device evaluation,
where classifiers trained on controller data were tested using hand-tracking
data, the model achieved an accuracy of 78.89%. The integration of both
modalities in the mixed-device evaluation obtained an accuracy of 94.19%. Our
results underline the promise of using hand movement biometrics for the
real-time detection of user familiarity in critical VR applications, paving the
way for personalized and adaptive VR experiences.

</details>


### [4] [Deep Learning-Based Visual Fatigue Detection Using Eye Gaze Patterns in VR](https://arxiv.org/abs/2510.12994)
*Numan Zafar,Johnathan Locke,Shafique Ahmad Chaudhry*

Main category: cs.HC

TL;DR: 本文提出了一种基于深度学习的方法，通过连续眼动轨迹检测虚拟现实中的视觉疲劳，表现出高达94%的准确率，并为自适应人机交互提供了实际应用。


<details>
  <summary>Details</summary>
Motivation: 长期使用虚拟现实系统会导致视觉疲劳，影响用户的舒适度、性能和安全性，尤其是在高风险或长时间应用中。

Method: 通过利用深度学习分析在虚拟现实中记录的连续眼动轨迹，使用407名参与者的双目眼动追踪数据并评估六种深度分类器。

Result: EKYT模型在高视觉注意力任务（如视频观看和阅读文本）中最高可达94%的准确率，且发现疲劳与非疲劳状态之间存在显著的行为差异。

Conclusion: 眼动动态被确立为一种可靠且非侵入性的疲劳检测方式，具有在沉浸式虚拟现实中的实际应用潜力。

Abstract: Prolonged exposure to virtual reality (VR) systems leads to visual fatigue,
impairs user comfort, performance, and safety, particularly in high-stakes or
long-duration applications. Existing fatigue detection approaches rely on
subjective questionnaires or intrusive physiological signals, such as EEG,
heart rate, or eye-blink count, which limit their scalability and real-time
applicability. This paper introduces a deep learning-based study for detecting
visual fatigue using continuous eye-gaze trajectories recorded in VR. We use
the GazeBaseVR dataset comprising binocular eye-tracking data from 407
participants across five immersive tasks, extract cyclopean eye-gaze angles,
and evaluate six deep classifiers. Our results demonstrate that EKYT achieves
up to 94% accuracy, particularly in tasks demanding high visual attention, such
as video viewing and text reading. We further analyze gaze variance and
subjective fatigue measures, indicating significant behavioral differences
between fatigued and non-fatigued conditions. These findings establish eye-gaze
dynamics as a reliable and nonintrusive modality for continuous fatigue
detection in immersive VR, offering practical implications for adaptive
human-computer interactions.

</details>


### [5] [Developing and Validating the Arabic Version of the Attitudes Toward Large Language Models Scale](https://arxiv.org/abs/2510.13009)
*Basad Barajeeh,Ala Yankouskaya,Sameha AlShakhsi,Chun Sing Maxwell Ho,Guandong Xu,Raian Ali*

Main category: cs.HC

TL;DR: 本文翻译并验证了针对阿拉伯语人群的大型语言模型态度量表，结果显示该尺度在测量阿拉伯地区对LLMs的态度方面有效且可靠。


<details>
  <summary>Details</summary>
Motivation: 全球对大型语言模型（LLMs）的使用日益增长，需要适应当地文化和语言的工具，以准确测量公众态度。

Method: 将两种态度量表（AT-GLLM和AT-PLLM）翻译为阿拉伯语，并在249名阿拉伯语成人样本中进行验证。

Result: 翻译后的尺度在阿拉伯语中显示出良好的可靠性和有效性，并证实了两因素结构，且在性别间具有强测量不变性，内部可靠性良好。

Conclusion: 翻译并验证的阿拉伯语尺度是可靠有效的工具，有助于研究阿拉伯地区对大型语言模型（LLMs）的认知。

Abstract: As the use of large language models (LLMs) becomes increasingly global,
understanding public attitudes toward these systems requires tools that are
adapted to local contexts and languages. In the Arab world, LLM adoption has
grown rapidly with both globally dominant platforms and regional ones like
Fanar and Jais offering Arabic-specific solutions. This highlights the need for
culturally and linguistically relevant scales to accurately measure attitudes
toward LLMs in the region. Tools assessing attitudes toward artificial
intelligence (AI) can provide a base for measuring attitudes specific to LLMs.
The 5-item Attitudes Toward Artificial Intelligence (ATAI) scale, which
measures two dimensions, the AI Fear and the AI Acceptance, has been recently
adopted and adapted to develop new instruments in English using a sample from
the UK: the Attitudes Toward General LLMs (AT-GLLM) and Attitudes Toward
Primary LLM (AT-PLLM) scales. In this paper, we translate the two scales,
AT-GLLM and AT-PLLM, and validate them using a sample of 249 Arabic-speaking
adults. The results show that the scale, translated into Arabic, is a reliable
and valid tool that can be used for the Arab population and language.
Psychometric analyses confirmed a two-factor structure, strong measurement
invariance across genders, and good internal reliability. The scales also
demonstrated strong convergent and discriminant validity. Our scales will
support research in a non-Western context, a much-needed effort to help draw a
global picture of LLM perceptions, and will also facilitate localized research
and policy-making in the Arab region.

</details>


### [6] [Deliberate Lab: A Platform for Real-Time Human-AI Social Experiments](https://arxiv.org/abs/2510.13011)
*Crystal Qian,Vivian Tsai,Michael Behr,Nada Hussein,Léo Laugier,Nithum Thain,Lucas Dixon*

Main category: cs.HC

TL;DR: Deliberate Lab是一个开源平台，旨在促进人类与AI的合作研究，降低技术门槛，拓展实验方法。


<details>
  <summary>Details</summary>
Motivation: 社会和行为科学家希望研究人类与人工智能的互动、合作和决策，但实验基础设施不够完善，限制了相关研究的开展。

Method: 报告了平台的12个月公共部署，分析使用模式和工作流，并通过案例研究和深度访谈收集数据。

Result: 提出了Deliberate Lab，一个开源平台，支持大规模实时行为实验，包含人类参与者和基于大语言模型的代理。

Conclusion: 通过降低技术壁垒和标准化支持，Deliberate Lab为集体决策和以人为本的AI研究提供了新的方法论。

Abstract: Social and behavioral scientists increasingly aim to study how humans
interact, collaborate, and make decisions alongside artificial intelligence.
However, the experimental infrastructure for such work remains underdeveloped:
(1) few platforms support real-time, multi-party studies at scale; (2) most
deployments require bespoke engineering, limiting replicability and
accessibility, and (3) existing tools do not treat AI agents as first-class
participants. We present Deliberate Lab, an open-source platform for
large-scale, real-time behavioral experiments that supports both human
participants and large language model (LLM)-based agents. We report on a
12-month public deployment of the platform (N=88 experimenters, N=9195
experiment participants), analyzing usage patterns and workflows. Case studies
and usage scenarios are aggregated from platform users, complemented by
in-depth interviews with select experimenters. By lowering technical barriers
and standardizing support for hybrid human-AI experimentation, Deliberate Lab
expands the methodological repertoire for studying collective decision-making
and human-centered AI.

</details>


### [7] [Unmasking Hiring Bias: Platform Data Analysis and Controlled Experiments on Bias in Online Freelance Marketplaces via RAG-LLM Generated Contents](https://arxiv.org/abs/2510.13091)
*Wugeng Zheng,Guohou Shan*

Main category: cs.HC

TL;DR: 本文提出了一种新的方法，通过RAG与大型语言模型生成控制变量的虚假自由职业者档案，以研究在线自由职业市场中的性别和地区偏见。


<details>
  <summary>Details</summary>
Motivation: 随着在线自由职业市场的快速发展，尽管这些平台有助于减少传统招聘中的偏见，但用户个人信息的使用仍引发了持续的歧视担忧。

Method: 本文采用基于检索增强生成(RAG)与大型语言模型(LLM)的组合方法，创建虚假自由职业者档案用于模拟招聘实验，分析变量对招聘结果的影响。

Result: 使用基于RAG和大型语言模型创造的虚假自由职业者档案进行控制实验，得出性别和地区偏见对招聘过程的影响。

Conclusion: 尽管在初级招聘决策中性别偏见不显著，但女性自由职业者在项目后期更可能收到不佳评估，且美国自由职业者在模拟招聘中则表现出明显的偏好。

Abstract: Online freelance marketplaces, a rapidly growing part of the global labor
market, are creating a fair environment where professional skills are the main
factor for hiring. While these platforms can reduce bias from traditional
hiring, the personal information in user profiles raises concerns about ongoing
discrimination. Past studies on this topic have mostly used existing data,
which makes it hard to control for other factors and clearly see the effect of
things like gender or race. To solve these problems, this paper presents a new
method that uses Retrieval-Augmented Generation (RAG) with a Large Language
Model (LLM) to create realistic, artificial freelancer profiles for controlled
experiments. This approach effectively separates individual factors, enabling a
clearer statistical analysis of how different variables influence the
freelancer project process. In addition to analyzing extracted data with
traditional statistical methods for post-project stage analysis, our research
utilizes a dataset with highly controlled variables, generated by an RAG-LLM,
to conduct a simulated hiring experiment for pre-project stage analysis. The
results of our experiments show that, regarding gender, while no significant
preference emerged in initial hiring decisions, female freelancers are
substantially more likely to receive imperfect ratings post-project stage.
Regarding regional bias, a strong and consistent preference favoring US-based
freelancers shows that people are more likely to be selected in the simulated
experiments, perceived as more leader-like, and receive higher ratings on the
live platform.

</details>


### [8] [Adapting to the User: A Systematic Review of Personalized Interaction in VR](https://arxiv.org/abs/2510.13123)
*Tangyao Li,Yitong Zhu,Hai-Ning Liang,Yuyang Wang*

Main category: cs.HC

TL;DR: 本文探讨了个性化互动在虚拟现实中的应用，提出了统一适应机制的框架，并讨论了整合多模态传感器及其挑战。


<details>
  <summary>Details</summary>
Motivation: 探讨虚拟现实系统如何智能地响应和适应单个用户的状态、能力和偏好。

Method: 通过综合多项研究的发现，分析个性化互动中的适应性技术，并总结自适应机制在不同领域的应用。

Result: 提出了一个五阶段概念框架，以统一不同应用领域中的自适应机制，并总结了从多项研究中提取的发现。

Conclusion: 建议未来的研究方向，旨在开发更加以用户为中心的虚拟现实系统。

Abstract: As virtual reality (VR) systems become increasingly more advanced, they are
likewise expected to respond intelligently and adapt to individual user states,
abilities, and preferences. Recent work has explored how VR can be adapted and
tailored to individual users. However, existing reviews tend to address either
user-state sensing or adaptive interaction design in isolation, limiting our
understanding of their combined implementation in VR. Therefore, in this paper,
we examine the growing research on personalized interaction in VR, with a
particular focus on utilizing participants' immersion information and
adaptation mechanisms to modify virtual environments and enhance engagement,
performance, or a specific goal. We synthesize findings from studies that
employ adaptive techniques across diverse application domains and summarize a
five-stage conceptual framework that unifies adaptive mechanisms across
domains. Our analysis reveals emerging trends, including the integration of
multimodal sensors, an increasing reliance on user state inference, and the
challenge of balancing responsiveness with transparency. We conclude by
proposing future directions for developing more user-centered VR systems.

</details>


### [9] [Smart UX-design for Rescue Operations Wearable - A Knowledge Graph Informed Visualization Approach for Information Retrieval in Emergency Situations](https://arxiv.org/abs/2510.13539)
*Mubaris Nadeem,Johannes Zenkert,Christian Weber,Madjid Fathi,Muhammad Hamza*

Main category: cs.HC

TL;DR: 本论文提出了一种知识图谱驱动的智能UX设计方法，以提高急救情况下健康专业人员的信息检索和治疗建议能力。


<details>
  <summary>Details</summary>
Motivation: 在急救情况下，健康专业人员需要实时和准确的信息以提供更好的治疗，并提高救援效果。

Method: 采用知识图谱结合人工智能的方法来支持急救时的信息检索和治疗建议。

Result: 实现了一个系统，该系统可以通过上下文树立治疗帮助的推荐，从而提高急救响应效率。

Conclusion: 本研究提出的知识图谱驱动的智能用户体验设计有助于提高急救操作的有效性。

Abstract: This paper presents a knowledge graph-informed smart UX-design approach for
supporting information retrieval for a wearable, providing treatment
recommendations during emergency situations to health professionals. This paper
describes requirements that are unique to knowledge graph-based solutions, as
well as the direct requirements of health professionals. The resulting
implementation is provided for the project, which main goal is to improve
first-aid rescue operations by supporting artificial intelligence in situation
detection and knowledge graph representation via a contextual-based
recommendation for treatment assistance.

</details>


### [10] [Speculating a Tactile Grammar: Toward Task-Aligned Chart Design for Non-Visual Perception](https://arxiv.org/abs/2510.13731)
*Areen Khalaila,Dylan Cashman*

Main category: cs.HC

TL;DR: 本研究提出了一个触觉设计框架，旨在优化盲人和视力障碍者的触觉图表设计，强调触觉优先的方法，以提升数据分析任务的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的触觉图表设计主要以视觉为基础，未能有效支持盲人和低视力者的探索需求。

Method: 通过概念化的触觉设计框架和模拟样本探索图表设计，展示如何更好地支持触觉格式下的比较、趋势检测和比例估计。

Result: 框架展示了如何根据触觉策略和编码选择生成有效的触觉图表设计，以提升用户体验。

Conclusion: 提出的设计框架为触觉图表设计提供了指导，需通过后续的联合设计和任务评估进行验证。

Abstract: Tactile graphics are often adapted from visual chart designs, yet many of
these encodings do not translate effectively to non-visual exploration. Blind
and low-vision (BLV) people employ a variety of physical strategies such as
measuring lengths with fingers or scanning for texture differences to interpret
tactile charts. These observations suggest an opportunity to move beyond direct
visual translation and toward a tactile-first design approach. We outline a
speculative tactile design framework that explores how data analysis tasks may
align with tactile strategies and encoding choices. While this framework is not
yet validated, it offers a lens for generating tactile-first chart designs and
sets the stage for future empirical exploration. We present speculative mockups
to illustrate how the Tactile Perceptual Grammar might guide the design of an
accessible COVID-19 dashboard. This scenario illustrates how the grammar can
guide encoding choices that better support comparison, trend detection, and
proportion estimation in tactile formats. We conclude with design implications
and a discussion of future validation through co-design and task-based
evaluation.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [11] [Learning to Grasp Anything by Playing with Random Toys](https://arxiv.org/abs/2510.12866)
*Dantong Niu,Yuvan Sharma,Baifeng Shi,Rachel Ding,Matteo Gioia,Haoru Xue,Henry Tsai,Konstantinos Kallidromitis,Anirudh Pai,Shankar Shastry,Trevor Darrell,Jitendra Malik,Roei Herzig*

Main category: cs.RO

TL;DR: 本研究探索机器人是否可以通过学习简单玩具来实现对新物体的通用抓取能力，结果显示可以，且采用了一种新型的物体中心可视化表示方法。


<details>
  <summary>Details</summary>
Motivation: 探讨机器人在操作新颖物体时的能力局限，借鉴认知科学中儿童通过简单玩具学习来提升操作技能的原理。

Method: 通过随机组合的四种基础形状物体进行训练，提出了一种检测池化机制以实现物体中心的可视化表示，并在仿真及实际机器人上进行评估。

Result: 机器人能够通过学习由四种基础形状（球体、长方体、圆柱体和环）组成的随机组合物体，实现具有普遍性理解的抓取能力，并展示出在真实世界物体上的强大零-shot模型表现。

Conclusion: 本研究为机器人操作的可扩展性和普遍性学习提供了一条有前景的路径，且在YCB数据集上取得了67%的真实抓取成功率。

Abstract: Robotic manipulation policies often struggle to generalize to novel objects,
limiting their real-world utility. In contrast, cognitive science suggests that
children develop generalizable dexterous manipulation skills by mastering a
small set of simple toys and then applying that knowledge to more complex
items. Inspired by this, we study if similar generalization capabilities can
also be achieved by robots. Our results indicate robots can learn generalizable
grasping using randomly assembled objects that are composed from just four
shape primitives: spheres, cuboids, cylinders, and rings. We show that training
on these "toys" enables robust generalization to real-world objects, yielding
strong zero-shot performance. Crucially, we find the key to this generalization
is an object-centric visual representation induced by our proposed detection
pooling mechanism. Evaluated in both simulation and on physical robots, our
model achieves a 67% real-world grasping success rate on the YCB dataset,
outperforming state-of-the-art approaches that rely on substantially more
in-domain data. We further study how zero-shot generalization performance
scales by varying the number and diversity of training toys and the
demonstrations per toy. We believe this work offers a promising path to
scalable and generalizable learning in robotic manipulation. Demonstration
videos, code, checkpoints and our dataset are available on our project page:
https://lego-grasp.github.io/ .

</details>


### [12] [Gaussian Process Implicit Surfaces as Control Barrier Functions for Safe Robot Navigation](https://arxiv.org/abs/2510.12919)
*Mouhyemen Khan,Tatsuya Ibuki,Abhijit Chatterjee*

Main category: cs.RO

TL;DR: 论文提出了一种新方法，将高斯过程隐式表面应用于控制屏障函数，优化了安全边界的表示，并通过稀疏技术提高了数据处理效率。


<details>
  <summary>Details</summary>
Motivation: 提出一个统一框架，使隐式表面作为控制屏障函数，从而增强安全性和几何形状的表现力。

Method: 利用高斯过程隐式表面来构建安全边界，并通过传感器测量获得的安全样本来调整高斯过程。

Result: 开发一种称为稀疏高斯控制屏障函数的解决方案，以应对数据规模增长带来的挑战。

Conclusion: 通过对比实验验证了高斯控制屏障函数在安全交互和轨迹执行中的有效性。

Abstract: Level set methods underpin modern safety techniques such as control barrier
functions (CBFs), while also serving as implicit surface representations for
geometric shapes via distance fields. Inspired by these two paradigms, we
propose a unified framework where the implicit surface itself acts as a CBF. We
leverage Gaussian process (GP) implicit surface (GPIS) to represent the safety
boundaries, using safety samples which are derived from sensor measurements to
condition the GP. The GP posterior mean defines the implicit safety surface
(safety belief), while the posterior variance provides a robust safety margin.
Although GPs have favorable properties such as uncertainty estimation and
analytical tractability, they scale cubically with data. To alleviate this
issue, we develop a sparse solution called sparse Gaussian CBFs. To the best of
our knowledge, GPIS have not been explicitly used to synthesize CBFs. We
validate the approach on collision avoidance tasks in two settings: a simulated
7-DOF manipulator operating around the Stanford bunny, and a quadrotor
navigating in 3D around a physical chair. In both cases, Gaussian CBFs (with
and without sparsity) enable safe interaction and collision-free execution of
trajectories that would otherwise intersect the objects.

</details>


### [13] [Geometric Model Predictive Path Integral for Agile UAV Control with Online Collision Avoidance](https://arxiv.org/abs/2510.12924)
*Pavel Pochobradský,Ondřej Procházka,Robert Pěnička,Vojtěch Vonásek,Martin Saska*

Main category: cs.RO

TL;DR: 本文提出GMPPI控制器，能够在复杂环境中以高速度跟踪灵活轨迹并避开障碍物。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境下实现无人机高效避障和灵活轨迹跟踪的需求。

Method: GMPPI通过生成众多候选轨迹进行采样，并使用几何SE(3)控制来提高精度，结合变化的仿真时间步长和动态成本及噪声参数，以提升跟踪性能。

Result: GMPPI在模拟和现实环境中均能以高达13m/s的速度避障，且跟踪误差接近几何SE(3)控制器。

Conclusion: GMPPI控制器在模拟和实际实验中均表现出色，能够在保证灵活轨迹跟踪的同时，避免障碍物。

Abstract: In this letter, we introduce Geometric Model Predictive Path Integral
(GMPPI), a sampling-based controller capable of tracking agile trajectories
while avoiding obstacles. In each iteration, GMPPI generates a large number of
candidate rollout trajectories and then averages them to create a nominal
control to be followed by the Unmanned Aerial Vehicle (UAV). We propose using
geometric SE(3) control to generate part of the rollout trajectories,
significantly increasing precision in agile flight. Furthermore, we introduce
varying rollout simulation time step length and dynamic cost and noise
parameters, vastly improving tracking performance of smooth and low-speed
trajectories over an existing Model Predictive Path Integral (MPPI)
implementation. Finally, we propose an integration of GMPPI with a stereo depth
camera, enabling online obstacle avoidance at high speeds, a crucial step
towards autonomous UAV flights in complex environments. The proposed controller
can track simulated agile reference trajectories with position error similar to
the geometric SE(3) controller. However, the same configuration of the proposed
controller can avoid obstacles in a simulated forest environment at speeds of
up to 13m/s, surpassing the performance of a state-of-the-art obstacle-aware
planner. In real-world experiments, GMPPI retains the capability to track agile
trajectories and avoids obstacles at speeds of up to 10m/s.

</details>


### [14] [Enhancing Sampling-based Planning with a Library of Paths](https://arxiv.org/abs/2510.12962)
*Michal Minařík,Vojtěch Vonásek,Robert Pěnička*

Main category: cs.RO

TL;DR: 本文提出了一种路径规划方法，通过复用过往解决方案来显著提高狭窄通道中的效率，测试显示速度大幅提升，已开源。


<details>
  <summary>Details</summary>
Motivation: 解决机器人操作中在狭窄通道和环境中多物体运输时传统路径规划方法效率低下的问题。

Method: 采用基于库的过往解决方案来提高新对象路径规划的效率，并通过采样生成近似路径。

Result: 在多种狭窄通道场景中，提出的方法比OMPL库中的其他最先进方法显著提高了规划速度，时间需求减少了多达85%。

Conclusion: 本文提出了一种路径规划方法，通过重新利用过去的解决方案显著提高了在狭窄通道中的规划效率。

Abstract: Path planning for 3D solid objects is a challenging problem, requiring a
search in a six-dimensional configuration space, which is, nevertheless,
essential in many robotic applications such as bin-picking and assembly. The
commonly used sampling-based planners, such as Rapidly-exploring Random Trees,
struggle with narrow passages where the sampling probability is low, increasing
the time needed to find a solution. In scenarios like robotic bin-picking,
various objects must be transported through the same environment. However,
traditional planners start from scratch each time, losing valuable information
gained during the planning process. We address this by using a library of past
solutions, allowing the reuse of previous experiences even when planning for a
new, previously unseen object. Paths for a set of objects are stored, and when
planning for a new object, we find the most similar one in the library and use
its paths as approximate solutions, adjusting for possible mutual
transformations. The configuration space is then sampled along the approximate
paths. Our method is tested in various narrow passage scenarios and compared
with state-of-the-art methods from the OMPL library. Results show significant
speed improvements (up to 85% decrease in the required time) of our method,
often finding a solution in cases where the other planners fail. Our
implementation of the proposed method is released as an open-source package.

</details>


### [15] [The Omega Turn: A General Turning Template for Elongate Robots](https://arxiv.org/abs/2510.12970)
*Baxi Chong,Tianyu Wang,Kelimar Diaz,Christopher J. Pierce,Eva Erickson,Julian Whitman,Yuelin Deng,Esteban Flores,Ruijie Fu,Juntao He,Jianfeng Lin,Hang Lu,Guillaume Sartoretti,Howie Choset,Daniel I. Goldman*

Main category: cs.RO

TL;DR: 本研究设计了一种基于线虫omega转的控制器，以提高无肢机器人在拥挤环境中的转弯能力。


<details>
  <summary>Details</summary>
Motivation: 旨在改善无肢机器人的转弯策略，以增强其在搜索救援和工业检查等应用中的有效性和可靠性。

Method: 通过将omega转描述为两个传播波的叠加，设计了一个控制器来实现高效、稳健的转弯行为。

Result: 研究提出了一种基于微观线虫的omega转弯策略，用于让无肢机器人在复杂环境中高效机动。

Conclusion: 所提出的omega转弯控制器不仅适用于无肢机器人，还可以推广到多足机器人，提供了一种新的高级驱动转弯策略。

Abstract: Elongate limbless robots have the potential to locomote through tightly
packed spaces for applications such as search-and-rescue and industrial
inspections. The capability to effectively and robustly maneuver elongate
limbless robots is crucial to realize such potential. However, there has been
limited research on turning strategies for such systems. To achieve effective
and robust turning performance in cluttered spaces, we take inspiration from a
microscopic nematode, C. elegans, which exhibits remarkable maneuverability in
rheologically complex environments partially because of its ability to perform
omega turns. Despite recent efforts to analyze omega turn kinematics, it
remains unknown if there exists a wave equation sufficient to prescribe an
omega turn, let alone its reconstruction on robot platforms. Here, using a
comparative theory-biology approach, we prescribe the omega turn as a
superposition of two traveling waves. With wave equations as a guideline, we
design a controller for limbless robots enabling robust and effective turning
behaviors in lab and cluttered field environments. Finally, we show that such
omega turn controllers can also generalize to elongate multi-legged robots,
demonstrating an alternative effective body-driven turning strategy for
elongate robots, with and without limbs.

</details>


### [16] [Actron3D: Learning Actionable Neural Functions from Videos for Transferable Robotic Manipulation](https://arxiv.org/abs/2510.12971)
*Anran Zhang,Hanzhi Chen,Yannick Burkhardt,Yao Zhong,Johannes Betz,Helen Oleynikova,Stefan Leutenegger*

Main category: cs.RO

TL;DR: Actron3D框架通过少数人类视频赋予机器人可转移的6-DoF操控技能，显著提升成功率。


<details>
  <summary>Details</summary>
Motivation: 旨在使机器人能够通过人类视频快速学习多样化的操控技能，而不需要复杂的校准或大量的演示。

Method: 提出了Actron3D框架，通过少量单目、未校准的RGB人类视频，使机器人获得可转移的6个自由度操控技能。

Result: 在13个任务中，Actron3D相比于之前的方法显著提高了平均成功率，提升幅度达到了14.9个百分点，同时每个任务只需要2-3个演示视频。

Conclusion: 实验表明，Actron3D在模拟和现实环境中的表现优于现有方法，验证了其在机器人操控领域的有效性和高效性。

Abstract: We present Actron3D, a framework that enables robots to acquire transferable
6-DoF manipulation skills from just a few monocular, uncalibrated, RGB-only
human videos. At its core lies the Neural Affordance Function, a compact
object-centric representation that distills actionable cues from diverse
uncalibrated videos-geometry, visual appearance, and affordance-into a
lightweight neural network, forming a memory bank of manipulation skills.
During deployment, we adopt a pipeline that retrieves relevant affordance
functions and transfers precise 6-DoF manipulation policies via coarse-to-fine
optimization, enabled by continuous queries to the multimodal features encoded
in the neural functions. Experiments in both simulation and the real world
demonstrate that Actron3D significantly outperforms prior methods, achieving a
14.9 percentage point improvement in average success rate across 13 tasks while
requiring only 2-3 demonstration videos per task.

</details>


### [17] [UNCAP: Uncertainty-Guided Planning Using Natural Language Communication for Cooperative Autonomous Vehicles](https://arxiv.org/abs/2510.12992)
*Neel P. Bhatt,Po-han Li,Kushagra Gupta,Rohan Siva,Daniel Milan,Alexander T. Hogue,Sandeep P. Chinchali,David Fridovich-Keil,Zhangyang Wang,Ufuk Topcu*

Main category: cs.RO

TL;DR: UNCAP通过自然语言消息和感知不确定性的考虑，提升了自动驾驶车辆协作的效率与安全性。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖于高带宽原始传感器数据，要么忽视共享数据中的感知和规划不确定性，导致系统的可扩展性和安全性不足。因此，提出的UNCAP对这些问题进行了针对性的优化。

Method: 采用两阶段的通信协议：首先确定与信息交换最相关的车辆，然后这些车辆通过定量方式表达它们的感知不确定性。

Result: 提出了一种新方法UNCAP，旨在优化多辆自动驾驶车辆之间的通信，使其既高效又可解释，从而提高协作规划的安全性和可扩展性。

Conclusion: 通过实验证明，UNCAP能够显著降低通信带宽，提高驾驶安全性，减少决策不确定性，并在临近碰撞事件中增强安全距离。

Abstract: Safe large-scale coordination of multiple cooperative connected autonomous
vehicles (CAVs) hinges on communication that is both efficient and
interpretable. Existing approaches either rely on transmitting high-bandwidth
raw sensor data streams or neglect perception and planning uncertainties
inherent in shared data, resulting in systems that are neither scalable nor
safe. To address these limitations, we propose Uncertainty-Guided Natural
Language Cooperative Autonomous Planning (UNCAP), a vision-language model-based
planning approach that enables CAVs to communicate via lightweight natural
language messages while explicitly accounting for perception uncertainty in
decision-making. UNCAP features a two-stage communication protocol: (i) an ego
CAV first identifies the subset of vehicles most relevant for information
exchange, and (ii) the selected CAVs then transmit messages that quantitatively
express their perception uncertainty. By selectively fusing messages that
maximize mutual information, this strategy allows the ego vehicle to integrate
only the most relevant signals into its decision-making, improving both the
scalability and reliability of cooperative planning. Experiments across diverse
driving scenarios show a 63% reduction in communication bandwidth with a 31%
increase in driving safety score, a 61% reduction in decision uncertainty, and
a four-fold increase in collision distance margin during near-miss events.
Project website: https://uncap-project.github.io/

</details>


### [18] [Development of a Linear Guide-Rail Testbed for Physically Emulating ISAM Operations](https://arxiv.org/abs/2510.13005)
*Robert Muldrow,Channing Ludden,Christopher Petersen*

Main category: cs.RO

TL;DR: 本文提出了一种新的硬件在环测试平台，用于模拟和验证太空服务与组装操作，以应对复杂的动态测试挑战。


<details>
  <summary>Details</summary>
Motivation: 应对复杂的控制问题和在6自由度环境中验证机器人模型的挑战，以提高太空资产的使用寿命和功能。

Method: 设计和开发一个硬件在环（HIL）实验测试台，通过将6自由度的UR3e机器人臂与卫星总线结合，模拟ISAM操作。

Result: 建立了一个实验ISAM仿真系统，以探索和验证太空运动、串联机器人操作和接触力学模型。

Conclusion: 本研究提出了一种新的硬件在环实验平台，用于模拟太空服务、组装和制造（ISAM），解决了现有模型在实际太空中验证的挑战。

Abstract: In-Space Servicing, Assembly, and Manufacturing (ISAM) is a set of emerging
operations that provides several benefits to improve the longevity, capacity,
mo- bility, and expandability of existing and future space assets. Serial
robotic ma- nipulators are particularly vital in accomplishing ISAM operations,
however, the complex perturbation forces and motions associated with movement
of a robotic arm on a free-flying satellite presents a complex controls problem
requiring addi- tional study. While many dynamical models are developed,
experimentally test- ing and validating these models is challenging given that
the models operate in space, where satellites have six-degrees-of-freedom
(6-DOF). This paper attempts to resolve those challenges by presenting the
design and development of a new hardware-in-the-loop (HIL) experimental testbed
utilized to emulate ISAM. This emulation will be accomplished by means of a
6-DOF UR3e robotic arm attached to a satellite bus. This satellite bus is
mounted to a 1-DOF guide-rail system, en- abling the satellite bus and robotic
arm to move freely in one linear direction. This experimental ISAM emulation
system will explore and validate models for space motion, serial robot
manipulation, and contact mechanics.

</details>


### [19] [Kinematic Kitbashing for Modeling Functional Articulated Objects](https://arxiv.org/abs/2510.13048)
*Minghao Guo,Victor Zordan,Sheldon Andrews,Wojciech Matusik,Maneesh Agrawala,Hsueh-Ti Derek Liu*

Main category: cs.RO

TL;DR: 提出了一种自动化框架Kinematic Kitbashing，通过优化部件位置，合成功能性关节物体，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 通过重用现有模型的部件，合成具有功能性的关节物体

Method: Kinematic Kitbashing框架

Result: 生成广泛的组装关节形状，并在几何、运动学和功能指标上领先于现有基准

Conclusion: Kinematic Kitbashing有效结合了关节几何匹配和功能驱动优化，支持快速创建互动关节资产。

Abstract: We introduce Kinematic Kitbashing, an automatic framework that synthesizes
functionality-aware articulated objects by reusing parts from existing models.
Given a kinematic graph with a small collection of articulated parts, our
optimizer jointly solves for the spatial placement of every part so that (i)
attachments remain geometrically sound over the entire range of motion and (ii)
the assembled object satisfies user-specified functional goals such as
collision-free actuation, reachability, or trajectory following. At its core is
a kinematics-aware attachment energy that aligns vector distance function
features sampled across multiple articulation snapshots. We embed this
attachment term within an annealed Riemannian Langevin dynamics sampler that
treats functionality objectives as additional energies, enabling robust global
exploration while accommodating non-differentiable functionality objectives and
constraints. Our framework produces a wide spectrum of assembled articulated
shapes, from trash-can wheels grafted onto car bodies to multi-segment lamps,
gear-driven paddlers, and reconfigurable furniture, and delivers strong
quantitative improvements over state-of-the-art baselines across geometric,
kinematic, and functional metrics. By tightly coupling articulation-aware
geometry matching with functionality-driven optimization, Kinematic Kitbashing
bridges part-based shape modeling and functional assembly design, empowering
rapid creation of interactive articulated assets.

</details>


### [20] [VLA-0: Building State-of-the-Art VLAs with Zero Modification](https://arxiv.org/abs/2510.13054)
*Ankit Goyal,Hugo Hadfield,Xuning Yang,Valts Blukis,Fabio Ramos*

Main category: cs.RO

TL;DR: VLA-0模型通过将动作直接表示为文本，超越了复杂的现有模型，显示出强大的效能，尤其是在LIBERO基准测试上。


<details>
  <summary>Details</summary>
Motivation: 探索将动作直接表示为文本的策略，检验这种简化方法在VLA模型中的有效性。

Method: VLA-0模型通过将动作简单地表达为文本，而不是借助复杂的词汇或专门的动作头进行构建。

Result: 本文介绍的VLA-0模型在视觉-语言-动作(VLA)任务中显示出出色的性能。

Conclusion: VLA-0的设计简单却强大，能够在缺乏大规模特定于机器人的训练数据的情况下，依然 outperform 现有的多种复杂模型，证明了其优越性。

Abstract: Vision-Language-Action models (VLAs) hold immense promise for enabling
generalist robot manipulation. However, the best way to build them remains an
open question. Current approaches often add complexity, such as modifying the
existing vocabulary of a Vision-Language Model (VLM) with action tokens or
introducing special action heads. Curiously, the simplest strategy of
representing actions directly as text has remained largely unexplored. This
work introduces VLA-0 to investigate this idea. We find that VLA-0 is not only
effective; it is surprisingly powerful. With the right design, VLA-0
outperforms more involved models. On LIBERO, a popular benchmark for evaluating
VLAs, VLA-0 outperforms all existing methods trained on the same robotic data,
including $\pi_0.5$-KI, OpenVLA-OFT and SmolVLA. Furthermore, without
large-scale robotics-specific training, it outperforms methods trained on
large-scale robotic data, like $\pi_0.5$-KI, $\pi_0$, GR00T-N1 and MolmoAct.
These findings also translate to the real world, where VLA-0 outperforms
SmolVLA, a VLA model pre-trained on large-scale real data. This paper
summarizes our unexpected findings and spells out the specific techniques
required to unlock the high performance of this simple yet potent VLA design.
Visual results, code, and trained models are provided here:
https://vla0.github.io/.

</details>


### [21] [RoboHiMan: A Hierarchical Evaluation Paradigm for Compositional Generalization in Long-Horizon Manipulation](https://arxiv.org/abs/2510.13149)
*Yangtao Chen,Zixuan Chen,Nga Teng Chan,Junting Chen,Junhui Yin,Jieqi Shi,Yang Gao,Yong-Lu Li,Jing Huo*

Main category: cs.RO

TL;DR: 研究提出了RoboHiMan，一个新的分层评估框架，关注长期操作中的组合泛化能力，揭示了当前模型在技能组合上的局限性，并提出了改进方案。


<details>
  <summary>Details</summary>
Motivation: 旨在填补当前短缺的组合泛化、鲁棒性与计划执行之间相互作用的理解。

Method: 提出了一种分层评估方法，通过HiMan-Bench基准测试和多级训练数据集进行分析。

Result: 展示了不同模型在复杂扰动下的能力差距，提供了针对长期操作任务的改进方向。

Conclusion: 提出了RoboHiMan评估范式，强调技能组合的重要性，并揭示现有模型的能力缺口。

Abstract: Enabling robots to flexibly schedule and compose learned skills for novel
long-horizon manipulation under diverse perturbations remains a core challenge.
Early explorations with end-to-end VLA models show limited success, as these
models struggle to generalize beyond the training distribution. Hierarchical
approaches, where high-level planners generate subgoals for low-level policies,
bring certain improvements but still suffer under complex perturbations,
revealing limited capability in skill composition. However, existing benchmarks
primarily emphasize task completion in long-horizon settings, offering little
insight into compositional generalization, robustness, and the interplay
between planning and execution. To systematically investigate these gaps, we
propose RoboHiMan, a hierarchical evaluation paradigm for compositional
generalization in long-horizon manipulation. RoboHiMan introduces HiMan-Bench,
a benchmark of atomic and compositional tasks under diverse perturbations,
supported by a multi-level training dataset for analyzing progressive data
scaling, and proposes three evaluation paradigms (vanilla, decoupled, coupled)
that probe the necessity of skill composition and reveal bottlenecks in
hierarchical architectures. Experiments highlight clear capability gaps across
representative models and architectures, pointing to directions for advancing
models better suited to real-world long-horizon manipulation tasks. Videos and
open-source code can be found on our project website:
https://chenyt31.github.io/robo-himan.github.io/.

</details>


### [22] [ALOHA2 Robot Kitchen Application Scenario Reproduction Report](https://arxiv.org/abs/2510.13284)
*Haoyang Wu,Siheng Wu,William X. Liu,Fangui Zeng*

Main category: cs.RO

TL;DR: ALOHA2是一个改进版双臂遥控机器人，提供更高性能和更好的人体工学设计，能够进行多视角图像采集。


<details>
  <summary>Details</summary>
Motivation: 提高机械臂的性能、鲁棒性和人体工程学设计

Method: 使用双臂遥控机器人

Result: ALOHA2在设计上优于原始ALOHA，用户能够通过操作主机械臂来控制从机械臂，并实现多视角的RGB数据采集。

Conclusion: ALOHA2的设计提升了遥操作的效率和用户体验，有助于未来的机器人开发。

Abstract: ALOHA2 is an enhanced version of the dual-arm teleoperated robot ALOHA,
featuring higher performance and robustness compared to the original design,
while also being more ergonomic. Like ALOHA, ALOHA2 consists of two grippers
and two ViperX 6-DoF arms, as well as two smaller WidowX arms. Users control
the follower mechanical arms by operating the leader mechanical arms through
back-driving. The device also includes cameras that generate images from
multiple viewpoints, allowing for RGB data collection during teleoperation. The
robot is mounted on a 48-inch x 30-inch table, equipped with an aluminum frame
that provides additional mounting points for cameras and gravity compensation
systems.

</details>


### [23] [DAMM-LOAM: Degeneracy Aware Multi-Metric LiDAR Odometry and Mapping](https://arxiv.org/abs/2510.13287)
*Nishant Chandna,Akshat Kaushal*

Main category: cs.RO

TL;DR: 本文提出了一种新的DAMM-LOAM模块，通过点云分类和加权ICP算法改善LiDAR SLAM在特征稀疏条件下的性能。


<details>
  <summary>Details</summary>
Motivation: 解决目前LiDAR SLAM系统在稀疏特征和复杂环境中的局限性，提高里程计和地图生成的精度。

Method: 通过表面法线和邻域分析对点云进行分类，然后应用基于退化的加权最小二乘ICP算法进行里程计估计，并实施基于扫描上下文的后端支持。

Result: 提出了一种新的多度量激光雷达里程计和地图生成模块DAMM-LOAM，旨在解决当前LiDAR SLAM系统在特征稀疏、重复几何结构和高频运动情况下的姿态估计退化问题。通过对点云进行分类和基于退化的加权最小二乘ICP算法，该系统显著提高了里程计精度，尤其是在室内长走廊环境中。

Conclusion: DAMM-LOAM系统在室内环境中表现出显著的里程计精度提升，尤其适用于长走廊等场景。

Abstract: LiDAR Simultaneous Localization and Mapping (SLAM) systems are essential for
enabling precise navigation and environmental reconstruction across various
applications. Although current point-to-plane ICP algorithms perform effec-
tively in structured, feature-rich environments, they struggle in scenarios
with sparse features, repetitive geometric structures, and high-frequency
motion. This leads to degeneracy in 6- DOF pose estimation. Most
state-of-the-art algorithms address these challenges by incorporating
additional sensing modalities, but LiDAR-only solutions continue to face
limitations under such conditions. To address these issues, we propose a novel
Degeneracy-Aware Multi-Metric LiDAR Odometry and Map- ping (DAMM-LOAM) module.
Our system improves mapping accuracy through point cloud classification based
on surface normals and neighborhood analysis. Points are classified into
ground, walls, roof, edges, and non-planar points, enabling accurate
correspondences. A Degeneracy-based weighted least squares-based ICP algorithm
is then applied for accurate odom- etry estimation. Additionally, a Scan
Context based back-end is implemented to support robust loop closures.
DAMM-LOAM demonstrates significant improvements in odometry accuracy,
especially in indoor environments such as long corridors

</details>


### [24] [Tactile-Conditioned Diffusion Policy for Force-Aware Robotic Manipulation](https://arxiv.org/abs/2510.13324)
*Erik Helmut,Niklas Funk,Tim Schneider,Cristiana de Farias,Jan Peters*

Main category: cs.RO

TL;DR: FARM是一种模仿学习框架，通过高维触觉数据实现力信号推断，从而提高机器人在操控过程中对力的控制能力，特别是在处理复杂任务时。


<details>
  <summary>Details</summary>
Motivation: 研究联系丰富的操控技术，在处理脆弱或可变形物体时，如何通过正确的抓取力进行操控。

Method: 采用模仿学习框架，结合高维触觉数据推断触觉条件的力信号，从而定义力基础动作空间。

Result: FARM在应对高力、低力和动态力适应的三项任务中表现优异，证明了高维触觉观察以及基于力的控制空间的重要性。

Conclusion: FARM框架展示了在处理不同力要求的任务中，结合触觉反馈和基于力的控制空间的优势，超越了多种基准算法。

Abstract: Contact-rich manipulation depends on applying the correct grasp forces
throughout the manipulation task, especially when handling fragile or
deformable objects. Most existing imitation learning approaches often treat
visuotactile feedback only as an additional observation, leaving applied forces
as an uncontrolled consequence of gripper commands. In this work, we present
Force-Aware Robotic Manipulation (FARM), an imitation learning framework that
integrates high-dimensional tactile data to infer tactile-conditioned force
signals, which in turn define a matching force-based action space. We collect
human demonstrations using a modified version of the handheld Universal
Manipulation Interface (UMI) gripper that integrates a GelSight Mini visual
tactile sensor. For deploying the learned policies, we developed an actuated
variant of the UMI gripper with geometry matching our handheld version. During
policy rollouts, the proposed FARM diffusion policy jointly predicts robot
pose, grip width, and grip force. FARM outperforms several baselines across
three tasks with distinct force requirements -- high-force, low-force, and
dynamic force adaptation -- demonstrating the advantages of its two key
components: leveraging force-grounded, high-dimensional tactile observations
and a force-based control space. The codebase and design files are open-sourced
and available at https://tactile-farm.github.io .

</details>


### [25] [MODUR: A Modular Dual-reconfigurable Robot](https://arxiv.org/abs/2510.13356)
*Jie Gu,Tin Lun Lam,Chunxu Tian,Zhihao Xia,Yongheng Xing,Dan Zhang*

Main category: cs.RO

TL;DR: 开发了一种新的模块化自我重配置机器人MODUR，具备双层重配置能力，能在多个模块间进行高层次自我重配置，同时模块可改变形状执行基本动作。


<details>
  <summary>Details</summary>
Motivation: MSRR系统通过改变模块之间的拓扑关系，提供更高的适应性和鲁棒性，因此开发一种新型MSRR具有重要意义。

Method: 设计了一个紧凑的连接器和剪刀连杆组，形成并行机制，实现连接器运动解耦和相邻位置迁移能力。

Result: 通过对连接器的相互依赖进行全面分析，建立了基础运动设计的理论基础，并通过实验验证了MODUR的运动能力。

Conclusion: MODUR的运动通过一系列实验得到了验证，有效展示了其设计的可行性和灵活性。

Abstract: Modular Self-Reconfigurable Robot (MSRR) systems are a class of robots
capable of forming higher-level robotic systems by altering the topological
relationships between modules, offering enhanced adaptability and robustness in
various environments. This paper presents a novel MSRR called MODUR, featuring
dual-level reconfiguration capabilities designed to integrate reconfigurable
mechanisms into MSRR. Specifically, MODUR can perform high-level
self-reconfiguration among modules to create different configurations, while
each module is also able to change its shape to execute basic motions. The
design of MODUR primarily includes a compact connector and scissor linkage
groups that provide actuation, forming a parallel mechanism capable of
achieving both connector motion decoupling and adjacent position migration
capabilities. Furthermore, the workspace, considering the interdependent
connectors, is comprehensively analyzed, laying a theoretical foundation for
the design of the module's basic motion. Finally, the motion of MODUR is
validated through a series of experiments.

</details>


### [26] [Adversarial Fine-tuning in Offline-to-Online Reinforcement Learning for Robust Robot Control](https://arxiv.org/abs/2510.13358)
*Shingo Ayabe,Hiroshi Kera,Kazuhiko Kawamoto*

Main category: cs.RO

TL;DR: 提出了一种离线到在线的框架，通过对抗性微调提高策略的鲁棒性，从而无需风险的在线交互。


<details>
  <summary>Details</summary>
Motivation: 解决在动态环境中，基于静态数据集训练的政策在动作空间扰动下脆弱的问题。

Method: 使用清洁数据训练政策，然后通过对抗性微调注入干扰，以诱发补偿行为并提高鲁棒性，并采用性能感知课程调整干扰概率。

Result: 该研究提出了一种通过将离线强化学习与在线适应相结合的框架，增强了策略在动态环境中的自适应和鲁棒性。

Conclusion: 对抗性微调能够在不确定环境下实现自适应和鲁棒控制，成功弥合了离线效率与在线适应性之间的差距。

Abstract: Offline reinforcement learning enables sample-efficient policy acquisition
without risky online interaction, yet policies trained on static datasets
remain brittle under action-space perturbations such as actuator faults. This
study introduces an offline-to-online framework that trains policies on clean
data and then performs adversarial fine-tuning, where perturbations are
injected into executed actions to induce compensatory behavior and improve
resilience. A performance-aware curriculum further adjusts the perturbation
probability during training via an exponential-moving-average signal, balancing
robustness and stability throughout the learning process. Experiments on
continuous-control locomotion tasks demonstrate that the proposed method
consistently improves robustness over offline-only baselines and converges
faster than training from scratch. Matching the fine-tuning and evaluation
conditions yields the strongest robustness to action-space perturbations, while
the adaptive curriculum strategy mitigates the degradation of nominal
performance observed with the linear curriculum strategy. Overall, the results
show that adversarial fine-tuning enables adaptive and robust control under
uncertain environments, bridging the gap between offline efficiency and online
adaptability.

</details>


### [27] [Real-Time Knee Angle Prediction Using EMG and Kinematic Data with an Attention-Based CNN-LSTM Network and Transfer Learning Across Multiple Datasets](https://arxiv.org/abs/2510.13443)
*Mojtaba Mollahossein,Gholamreza Vossoughi,Mohammad Hossein Rohban*

Main category: cs.RO

TL;DR: 本研究提出了一种新的迁移学习框架，仅需少量步态周期即可预测膝关节角度，并展示了良好的通用性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有EMG信号预测方法中存在的实时性不足、测试条件不具代表性及数据集需求量大的问题，提出有效的迁移学习方法。

Method: 采用迁移学习框架，通过建立注意力机制的CNN-LSTM模型，利用EMG信号进行膝关节角度预测，并结合不同数据集进行训练与测试。

Result: 本文提出了一种用于膝关节角度预测的迁移学习框架，仅需少量步态周期便可适应新受试者。采用Georgia Tech、加州大学欧文分校(UCI)和Sharif机电实验室外骨骼(SMLE)的电肌图(EMG)数据集，开发了一种轻量级注意力基CNN-LSTM模型。实验结果表明，该模型在异常受试者的单步和50步预测中，NMAE分别为6.8%和13.7%。加入历史膝关节角度后，正常受试者的NMAE降低至3.1%和3.5%，异常受试者降低至2.8%和7.5%。在SMLE外骨骼的进一步适应中，模型分别实现了1.09%和3.1%的NMAE。

Conclusion: 研究表明迁移学习方法在不同受试者和情境下进行膝关节运动预测具有有效性，尤其适用于康复场景。

Abstract: Electromyography (EMG) signals are widely used for predicting body joint
angles through machine learning (ML) and deep learning (DL) methods. However,
these approaches often face challenges such as limited real-time applicability,
non-representative test conditions, and the need for large datasets to achieve
optimal performance. This paper presents a transfer-learning framework for knee
joint angle prediction that requires only a few gait cycles from new subjects.
Three datasets - Georgia Tech, the University of California Irvine (UCI), and
the Sharif Mechatronic Lab Exoskeleton (SMLE) - containing four EMG channels
relevant to knee motion were utilized. A lightweight attention-based CNN-LSTM
model was developed and pre-trained on the Georgia Tech dataset, then
transferred to the UCI and SMLE datasets. The proposed model achieved
Normalized Mean Absolute Errors (NMAE) of 6.8 percent and 13.7 percent for
one-step and 50-step predictions on abnormal subjects using EMG inputs alone.
Incorporating historical knee angles reduced the NMAE to 3.1 percent and 3.5
percent for normal subjects, and to 2.8 percent and 7.5 percent for abnormal
subjects. When further adapted to the SMLE exoskeleton with EMG, kinematic, and
interaction force inputs, the model achieved 1.09 percent and 3.1 percent NMAE
for one- and 50-step predictions, respectively. These results demonstrate
robust performance and strong generalization for both short- and long-term
rehabilitation scenarios.

</details>


### [28] [Bridge the Gap: Enhancing Quadruped Locomotion with Vertical Ground Perturbations](https://arxiv.org/abs/2510.13488)
*Maximilian Stasica,Arne Bick,Nico Bohlinger,Omid Mohseni,Max Johannes Alois Fritzsche,Clemens Hübler,Jan Peters,André Seyfarth*

Main category: cs.RO

TL;DR: 本文研究了四足机器人在振荡桥面上的运动训练，显著提高了机器人的运动稳定性和适应性，展示了基于模拟的强化学习在动态扰动环境中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 探索四足机器人在垂直地面扰动下的表现，特别是通过在振荡桥上的训练来增强其行走的鲁棒性。

Method: 使用强化学习和近端策略优化算法（PPO）在MuJoCo模拟环境中，针对15种不同的运动策略进行训练，结合五种步态和三种训练条件。

Result: 在振荡桥上训练的运动策略在稳定性和适应性上表现优于在刚性表面上训练的策略，并且这种框架使得机器人能够在动态地面扰动下进行运动。

Conclusion: 研究表明，在振荡桥上训练的四足机器人比在刚性表面上训练的表现更为优秀，具有更好的稳定性和适应性。这一框架使机器人在没有先前桥面暴露的情况下也能形成强大的步态模式。

Abstract: Legged robots, particularly quadrupeds, excel at navigating rough terrains,
yet their performance under vertical ground perturbations, such as those from
oscillating surfaces, remains underexplored. This study introduces a novel
approach to enhance quadruped locomotion robustness by training the Unitree Go2
robot on an oscillating bridge - a 13.24-meter steel-and-concrete structure
with a 2.0 Hz eigenfrequency designed to perturb locomotion. Using
Reinforcement Learning (RL) with the Proximal Policy Optimization (PPO)
algorithm in a MuJoCo simulation, we trained 15 distinct locomotion policies,
combining five gaits (trot, pace, bound, free, default) with three training
conditions: rigid bridge and two oscillating bridge setups with differing
height regulation strategies (relative to bridge surface or ground). Domain
randomization ensured zero-shot transfer to the real-world bridge. Our results
demonstrate that policies trained on the oscillating bridge exhibit superior
stability and adaptability compared to those trained on rigid surfaces. Our
framework enables robust gait patterns even without prior bridge exposure.
These findings highlight the potential of simulation-based RL to improve
quadruped locomotion during dynamic ground perturbations, offering insights for
designing robots capable of traversing vibrating environments.

</details>


### [29] [A Novel Robot Hand with Hoeckens Linkages and Soft Phalanges for Scooping and Self-Adaptive Grasping in Environmental Constraints](https://arxiv.org/abs/2510.13535)
*Wentao Guo,Yizhou Wang,Wenzeng Zhang*

Main category: cs.RO

TL;DR: 本文介绍了一种新型的欠驱动自适应机器人手，采用多种机械结构，实现多种抓取模式，且在不同环境中表现出良好的适应性和抓取稳定性。


<details>
  <summary>Details</summary>
Motivation: 为了提高机器人手在复杂环境下的抓取能力和适应性，设计了一种新型机器人手，能够灵活应对不同形状和大小的物体。

Method: 采用详细的运动学分析，优化推力角度和设计连杆长度；通过模拟验证设计，分析指尖运动，并确保不同抓取模式之间的平滑过渡；使用功率方程分析抓取力，以增强对系统性能的理解。

Result: 本文提出了一种新颖的欠驱动自适应机器人手——Hockens-A Hand，结合了Hoeckens机构、双平行四边形连杆和专门的四连杆结构，实现了三种自适应抓取模式：平行夹持、非对称铲取和包裹抓取。该手只需一个线性执行器，利用被动机械智能确保了在非结构化环境中的适应性和顺应性。

Conclusion: 实验验证表明，Hockens-A Hand在不同的环境约束下，能够稳定地实现三种抓取模式，具有广泛的适用性。

Abstract: This paper presents a novel underactuated adaptive robotic hand, Hockens-A
Hand, which integrates the Hoeckens mechanism, a double-parallelogram linkage,
and a specialized four-bar linkage to achieve three adaptive grasping modes:
parallel pinching, asymmetric scooping, and enveloping grasping. Hockens-A Hand
requires only a single linear actuator, leveraging passive mechanical
intelligence to ensure adaptability and compliance in unstructured
environments. Specifically, the vertical motion of the Hoeckens mechanism
introduces compliance, the double-parallelogram linkage ensures line contact at
the fingertip, and the four-bar amplification system enables natural
transitions between different grasping modes. Additionally, the inclusion of a
mesh-textured silicone phalanx further enhances the ability to envelop objects
of various shapes and sizes. This study employs detailed kinematic analysis to
optimize the push angle and design the linkage lengths for optimal performance.
Simulations validated the design by analyzing the fingertip motion and ensuring
smooth transitions between grasping modes. Furthermore, the grasping force was
analyzed using power equations to enhance the understanding of the system's
performance.Experimental validation using a 3D-printed prototype demonstrates
the three grasping modes of the hand in various scenarios under environmental
constraints, verifying its grasping stability and broad applicability.

</details>


### [30] [Hoecken-D Hand: A Novel Robotic Hand for Linear Parallel Pinching and Self-Adaptive Grasping](https://arxiv.org/abs/2510.13553)
*Wentao Guo,Wenzeng Zhang*

Main category: cs.RO

TL;DR: 本论文介绍了一款名为Hoecken-D Hand的下驱动机器人抓取器，结合了改进的Hoecken连杆和差动弹簧机制，能够在不同物体形状下实现有效的抓取，适用于非结构化环境。


<details>
  <summary>Details</summary>
Motivation: 研究目的是开发一种适应性强、成本低、能够抓取多种形状物体的机器人抓取器，以应对非结构化环境的挑战。

Method: 通过修改的Hoecken连杆与差动弹簧机制相结合，设计出一种在两种抓取模式下均表现出色的下驱动机器人抓取器。

Result: 原型展示了可靠的抓取性能，在多种物体形状下均表现出优势，具有200 mm的线性夹持范围。

Conclusion: Hoecken-D Hand是一种紧凑、适应性强且成本效益高的机器人抓取器，能够在非结构化环境中有效操控多种形状的物体。

Abstract: This paper presents the Hoecken-D Hand, an underactuated robotic gripper that
combines a modified Hoecken linkage with a differential spring mechanism to
achieve both linear parallel pinching and a mid-stroke transition to adaptive
envelope. The original Hoecken linkage is reconfigured by replacing one member
with differential links, preserving straight-line guidance while enabling
contact-triggered reconfiguration without additional actuators. A
double-parallelogram arrangement maintains fingertip parallelism during
conventional pinching, whereas the differential mechanism allows one finger to
wrap inward upon encountering an obstacle, improving stability on irregular or
thin objects. The mechanism can be driven by a single linear actuator,
minimizing complexity and cost; in our prototype, each finger is driven by its
own linear actuator for simplicity. We perform kinematic modeling and force
analysis to characterize grasp performance, including simulated grasping forces
and spring-opening behavior under varying geometric parameters. The design was
prototyped using PLA-based 3D printing, achieving a linear pinching span of
approximately 200 mm. Preliminary tests demonstrate reliable grasping in both
modes across a wide range of object geometries, highlighting the Hoecken-D Hand
as a compact, adaptable, and cost-effective solution for manipulation in
unstructured environments.

</details>


### [31] [Development of an Intuitive GUI for Non-Expert Teleoperation of Humanoid Robots](https://arxiv.org/abs/2510.13594)
*Austin Barret,Meng Cheng Lau*

Main category: cs.RO

TL;DR: 本研究旨在开发一个简单直观的图形用户界面，便于非专家操作人员通过FIRA规定的障碍课程控制人形机器人。


<details>
  <summary>Details</summary>
Motivation: 旨在解决目前人形机器人操控中界面设计的不足，使非专家用户也能轻松操作。

Method: 利用用户界面开发的常见实践，并结合人机交互领域的概念，设计一个可扩展的图形用户界面。

Result: 预计新开发的界面将提高非专家用户的操作能力和机器人的可用性。

Conclusion: 该研究开发的界面旨在让非专家用户更方便地操控机器人，促进人形机器人在实际应用中的普及。

Abstract: The operation of humanoid robotics is an essential field of research with
many practical and competitive applications. Many of these systems, however, do
not invest heavily in developing a non-expert-centered graphical user interface
(GUI) for operation. The focus of this research is to develop a scalable GUI
that is tailored to be simple and intuitive so non-expert operators can control
the robot through a FIRA-regulated obstacle course. Using common practices from
user interface development (UI) and understanding concepts described in
human-robot interaction (HRI) and other related concepts, we will develop a new
interface with the goal of a non-expert teleoperation system.

</details>


### [32] [Active Tactile Exploration for Rigid Body Pose and Shape Estimation](https://arxiv.org/abs/2510.13595)
*Ethan K. Gordon,Bruke Baraki,Hien Bui,Michael Posa*

Main category: cs.RO

TL;DR: 本研究提出一种基于触觉数据的框架，旨在高效学习未见物体的形状和位置，显著提高数据收集的效率。


<details>
  <summary>Details</summary>
Motivation: 在测试时学习一个物理精确的模型，以提高数据效率、可预测性和任务之间的重用性，同时克服触觉传感器在时间上的稀疏性。

Method: 提出一个仅使用触觉数据的学习与探索框架，以最小的机器人运动同时确定刚性物体的形状和位置。

Result: 在第一次接触后，只需少于10秒的随机采集数据即可学习长方体和凸多面体几何形状；探索方案显著加快了学习速度。

Conclusion: 该框架在模拟和实际机器人实验中表现出显著加速的学习效率，证明了在线探索和数据利用的有效性。

Abstract: General robot manipulation requires the handling of previously unseen
objects. Learning a physically accurate model at test time can provide
significant benefits in data efficiency, predictability, and reuse between
tasks. Tactile sensing can compliment vision with its robustness to occlusion,
but its temporal sparsity necessitates careful online exploration to maintain
data efficiency. Direct contact can also cause an unrestrained object to move,
requiring both shape and location estimation. In this work, we propose a
learning and exploration framework that uses only tactile data to
simultaneously determine the shape and location of rigid objects with minimal
robot motion. We build on recent advances in contact-rich system identification
to formulate a loss function that penalizes physical constraint violation
without introducing the numerical stiffness inherent in rigid-body contact.
Optimizing this loss, we can learn cuboid and convex polyhedral geometries with
less than 10s of randomly collected data after first contact. Our exploration
scheme seeks to maximize Expected Information Gain and results in significantly
faster learning in both simulated and real-robot experiments. More information
can be found at https://dairlab.github.io/activetactile

</details>


### [33] [PlanarMesh: Building Compact 3D Meshes from LiDAR using Incremental Adaptive Resolution Reconstruction](https://arxiv.org/abs/2510.13599)
*Jiahao Wang,Nived Chebrolu,Yifu Tao,Lintong Zhang,Ayoung Kim,Maurice Fallon*

Main category: cs.RO

TL;DR: 提出一种新型增量网格化LiDAR重建系统，结合平面建模与网格化技术，实时生成高精度和小文件大小的3D重建。


<details>
  <summary>Details</summary>
Motivation: 在线3D LiDAR映射系统需在细致表面重建和计算效率之间找到平衡。

Method: PlanarMesh，增量式网格化LiDAR重建系统

Result: PlanarMesh达到了与最先进技术相当或更高的重建精度，同时输出文件大小显著更小，并保持实时性能。

Conclusion: 通过采用多线程架构和Bounding Volume Hierarchy技术，PlanarMesh在精度与效率上均表现出色，适用于实时3D LiDAR映射。

Abstract: Building an online 3D LiDAR mapping system that produces a detailed surface
reconstruction while remaining computationally efficient is a challenging task.
In this paper, we present PlanarMesh, a novel incremental, mesh-based LiDAR
reconstruction system that adaptively adjusts mesh resolution to achieve
compact, detailed reconstructions in real-time. It introduces a new
representation, planar-mesh, which combines plane modeling and meshing to
capture both large surfaces and detailed geometry. The planar-mesh can be
incrementally updated considering both local surface curvature and free-space
information from sensor measurements. We employ a multi-threaded architecture
with a Bounding Volume Hierarchy (BVH) for efficient data storage and fast
search operations, enabling real-time performance. Experimental results show
that our method achieves reconstruction accuracy on par with, or exceeding,
state-of-the-art techniques-including truncated signed distance functions,
occupancy mapping, and voxel-based meshing-while producing smaller output file
sizes (10 times smaller than raw input and more than 5 times smaller than
mesh-based methods) and maintaining real-time performance (around 2 Hz for a
64-beam sensor).

</details>


### [34] [Efficient Force and Stiffness Prediction in Robotic Produce Handling with a Piezoresistive Pressure Sensor](https://arxiv.org/abs/2510.13616)
*Preston Fairchild,Claudia Chen,Xiaobo Tan*

Main category: cs.RO

TL;DR: 研究提出了一种低成本、易于制造的柔性压力传感器，用于提高机器人机械手在抓取农产品时的性能，能够实时估计物体的刚度和成熟度。


<details>
  <summary>Details</summary>
Motivation: 提高机器人对农业产品的处理能力，避免损伤，促进自动化农业的发展。

Method: 将柔性压力传感器与刚性机械手和气动软指结合，采用实时算法加速稳态输出估计。

Result: 这项研究开发了一种新型柔性压力传感器，与机械手臂集成，可用于精细处理农产品。

Conclusion: 该传感器的应用不仅提高了对不同形状、尺寸和刚度的物体的抓取能力，还能在质量控制和选择分发中发挥作用，促进未来农业自动化。

Abstract: Properly handling delicate produce with robotic manipulators is a major part
of the future role of automation in agricultural harvesting and processing.
Grasping with the correct amount of force is crucial in not only ensuring
proper grip on the object, but also to avoid damaging or bruising the product.
In this work, a flexible pressure sensor that is both low cost and easy to
fabricate is integrated with robotic grippers for working with produce of
varying shapes, sizes, and stiffnesses. The sensor is successfully integrated
with both a rigid robotic gripper, as well as a pneumatically actuated soft
finger. Furthermore, an algorithm is proposed for accelerated estimation of the
steady-state value of the sensor output based on the transient response data,
to enable real-time applications. The sensor is shown to be effective in
incorporating feedback to correctly grasp objects of unknown sizes and
stiffnesses. At the same time, the sensor provides estimates for these values
which can be utilized for identification of qualities such as ripeness levels
and bruising. It is also shown to be able to provide force feedback for objects
of variable stiffnesses. This enables future use not only for produce
identification, but also for tasks such as quality control and selective
distribution based on ripeness levels.

</details>


### [35] [Characterizing Lidar Point-Cloud Adversities Using a Vector Field Visualization](https://arxiv.org/abs/2510.13619)
*Daniel Choate,Jason Rife*

Main category: cs.RO

TL;DR: 提出了一种用于激光雷达数据分析的可视化方法，通过矢量场图揭示点云数据间的局部差异，帮助分析师识别并消除影响因素。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在改善激光雷达扫描数据的分析效率，帮助分析师更清晰地识别和处理数据中的逆境模式。

Method: 该方法生成一个矢量场图，展示配准点云之间的局部差异，辅助分析师识别潜在的逆境模式。

Result: 本论文介绍了一种可视化方法，用于帮助人类分析师分类影响激光雷达扫描匹配的逆境模式。

Conclusion: 通过应用此方法，分析师能够更好地处理和理解逆境机制，逐步去除影响因素。

Abstract: In this paper we introduce a visualization methodology to aid a human analyst
in classifying adversity modes that impact lidar scan matching. Our methodology
is intended for offline rather than real-time analysis. The method generates a
vector-field plot that characterizes local discrepancies between a pair of
registered point clouds. The vector field plot reveals patterns that would be
difficult for the analyst to extract from raw point-cloud data. After
introducing our methodology, we apply the process to two proof-of-concept
examples: one a simulation study and the other a field experiment. For both
data sets, a human analyst was able to reason about a series of adversity
mechanisms and iteratively remove those mechanisms from the raw data, to help
focus attention on progressively smaller discrepancies.

</details>


### [36] [A Modular Object Detection System for Humanoid Robots Using YOLO](https://arxiv.org/abs/2510.13625)
*Nicolas Pottier,Meng Cheng Lau*

Main category: cs.RO

TL;DR: 该研究提议利用YOLOv9提升机器人视觉系统的性能，测试结果显示该模型在精度上具有竞争力，但计算成本较高。


<details>
  <summary>Details</summary>
Motivation: 机器人领域的视觉系统效率低下，限制了技术进步。

Method: 提出一种基于YOLOv9的通用视觉模块，进行机器人电脑视觉任务的优化。

Result: 新的视觉模块在ROS1中实现，使用虚拟环境以实现YOLO兼容性，性能通过FPS和mAP评估。

Conclusion: YOLO模型在静态和动态环境下与几何模型相比，展示了可比精度与更高的鲁棒性。

Abstract: Within the field of robotics, computer vision remains a significant barrier
to progress, with many tasks hindered by inefficient vision systems. This
research proposes a generalized vision module leveraging YOLOv9, a
state-of-the-art framework optimized for computationally constrained
environments like robots. The model is trained on a dataset tailored to the
FIRA robotics Hurocup. A new vision module is implemented in ROS1 using a
virtual environment to enable YOLO compatibility. Performance is evaluated
using metrics such as frames per second (FPS) and Mean Average Precision (mAP).
Performance is then compared to the existing geometric framework in static and
dynamic contexts. The YOLO model achieved comparable precision at a higher
computational cost then the geometric model, while providing improved
robustness.

</details>


### [37] [LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models](https://arxiv.org/abs/2510.13626)
*Senyu Fei,Siyin Wang,Junhao Shi,Zihao Dai,Jikun Cai,Pengfang Qian,Li Ji,Xinzhe He,Shiduo Zhang,Zhaoye Fei,Jinlan Fu,Jingjing Gong,Xipeng Qiu*

Main category: cs.RO

TL;DR: VLA模型表现出色但在鲁棒性上存在严重问题，易受到多种扰动影响。


<details>
  <summary>Details</summary>
Motivation: 旨在揭示VLA模型在面对实际操作场景时的鲁棒性不足，并挑战现有的评估标准。

Method: 通过在七个维度引入受控扰动（包括物体布局、相机视角等）进行系统的脆弱性分析。

Result: 视觉-语言-动作（VLA）模型在机器人操作基准上表现出令人印象深刻的成功率，但这些结果可能掩盖了其在鲁棒性方面的根本弱点。

Conclusion: 我们的研究揭示了模型对扰动因素（如相机视角和机器人初始状态）的极度敏感性，性能在轻微扰动下可能会大幅下降，同时模型对语言指令的变化反应不敏感，这挑战了高基准分数与真正能力之间的假设。

Abstract: Visual-Language-Action (VLA) models report impressive success rates on
robotic manipulation benchmarks, yet these results may mask fundamental
weaknesses in robustness. We perform a systematic vulnerability analysis by
introducing controlled perturbations across seven dimensions: objects layout,
camera viewpoints, robot initial states, language instructions, light
conditions, background textures and sensor noise. We comprehensively analyzed
multiple state-of-the-art models and revealed consistent brittleness beneath
apparent competence. Our analysis exposes critical weaknesses: models exhibit
extreme sensitivity to perturbation factors, including camera viewpoints and
robot initial states, with performance dropping from 95% to below 30% under
modest perturbations. Surprisingly, models are largely insensitive to language
variations, with further experiments revealing that models tend to ignore
language instructions completely. Our findings challenge the assumption that
high benchmark scores equate to true competency and highlight the need for
evaluation practices that assess reliability under realistic variation.

</details>


### [38] [On Your Own: Pro-level Autonomous Drone Racing in Uninstrumented Arenas](https://arxiv.org/abs/2510.13644)
*Michael Bosello,Flavio Pinzarrone,Sara Kiade,Davide Aguiari,Yvo Keuter,Aaesha AlShehhi,Gyordan Caminati,Kei Long Wong,Ka Seng Chou,Junaid Halepota,Fares Alneyadi,Jacopo Panerati,Giovanni Pau*

Main category: cs.RO

TL;DR: 研究分析了无人机在不同环境中的自主驾驶性能，结果显示其可与专业人类飞行员相媲美，并公开了相关数据。


<details>
  <summary>Details</summary>
Motivation: 随着无人机技术在各行业的普及，提高其在真实、非结构化环境中的自主导航能力，提高无人机竞争力与应用范围。

Method: 在受控环境中通过外部追踪进行能力分析，并在没有地面真实测量的挑战性环境中进行性能展示。

Result: 本研究着眼于无人机技术，特别是基于视觉的自主驾驶能力，并在受控和不受控环境中进行性能分析。实验结果显示，该系统在两种环境中的表现达到了专业人类飞行员的水平，同时公开了飞行数据以供研究使用。

Conclusion: 本研究的结果表明该自主系统能够在受控和不受控环境中与专业人类飞行员的表现相匹配，是无人机技术向商业应用迈进的重要一步。

Abstract: Drone technology is proliferating in many industries, including agriculture,
logistics, defense, infrastructure, and environmental monitoring. Vision-based
autonomy is one of its key enablers, particularly for real-world applications.
This is essential for operating in novel, unstructured environments where
traditional navigation methods may be unavailable. Autonomous drone racing has
become the de facto benchmark for such systems. State-of-the-art research has
shown that autonomous systems can surpass human-level performance in racing
arenas. However, direct applicability to commercial and field operations is
still limited as current systems are often trained and evaluated in highly
controlled environments. In our contribution, the system's capabilities are
analyzed within a controlled environment -- where external tracking is
available for ground-truth comparison -- but also demonstrated in a
challenging, uninstrumented environment -- where ground-truth measurements were
never available. We show that our approach can match the performance of
professional human pilots in both scenarios. We also publicly release the data
from the flights carried out by our approach and a world-class human pilot.

</details>


### [39] [Hierarchical Discrete Lattice Assembly: An Approach for the Digital Fabrication of Scalable Macroscale Structures](https://arxiv.org/abs/2510.13686)
*Miana Smith,Paul Arthur Richard,Alexander Htet Kyaw,Neil Gershenfeld*

Main category: cs.RO

TL;DR: 本文提出了一种利用简单机器人和模块化组件高效制造大规模结构的方法，通过体素化目标结构及其实时数字双胞胎工具，来协调机器人组装。


<details>
  <summary>Details</summary>
Motivation: 当前大型结构的制造系统通常复杂、昂贵且不可靠，因此需要一种新的、更简化的制造方式。

Method: 使用简单机器人和相互锁定的网格建筑块的规模化宏结构制造方法

Result: 提出了一种新方法，通过将目标结构体素化并使用移动相对机器人来放置块，从而实现可扩展的大型结构制造。

Conclusion: 通过演示一系列米尺度对象的制造过程，验证了该系统的有效性，展现了其在大规模结构制造中的潜力。

Abstract: Although digital fabrication processes at the desktop scale have become
proficient and prolific, systems aimed at producing larger-scale structures are
still typically complex, expensive, and unreliable. In this work, we present an
approach for the fabrication of scalable macroscale structures using simple
robots and interlocking lattice building blocks. A target structure is first
voxelized so that it can be populated with an architected lattice. These voxels
are then grouped into larger interconnected blocks, which are produced using
standard digital fabrication processes, leveraging their capability to produce
highly complex geometries at a small scale. These blocks, on the size scale of
tens of centimeters, are then fed to mobile relative robots that are able to
traverse over the structure and place new blocks to form structures on the
meter scale. To facilitate the assembly of large structures, we introduce a
live digital twin simulation tool for controlling and coordinating assembly
robots that enables both global planning for a target structure and live user
design, interaction, or intervention. To improve assembly throughput, we
introduce a new modular assembly robot, designed for hierarchical voxel
handling. We validate this system by demonstrating the voxelization,
hierarchical blocking, path planning, and robotic fabrication of a set of
meter-scale objects.

</details>


### [40] [InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy](https://arxiv.org/abs/2510.13778)
*Xinyi Chen,Yilun Chen,Yanwei Fu,Ning Gao,Jiaya Jia,Weiyang Jin,Hao Li,Yao Mu,Jiangmiao Pang,Yu Qiao,Yang Tian,Bin Wang,Bolun Wang,Fangjing Wang,Hanqing Wang,Tai Wang,Ziqin Wang,Xueyuan Wei,Chao Wu,Shuai Yang,Jinhui Ye,Junqiu Yu,Jia Zeng,Jingjing Zhang,Jinyu Zhang,Shi Zhang,Feng Zheng,Bowen Zhou,Yangkun Zhu*

Main category: cs.RO

TL;DR: 提出InternVLA-M1框架，通过空间引导训练提高机器人指令遵循能力，实现可扩展通用智能。


<details>
  <summary>Details</summary>
Motivation: 推动遵循指令的机器人朝向可扩展的通用智能发展。

Method: 该框架包括两个阶段：一是基于空间推理数据的预训练，二是通过空间提示生成与实体相关的行为。

Result: InternVLA-M1在多个任务上表现出显著提升，尤其是在空间推理和指令遵循能力上。

Conclusion: 空间引导训练为可扩展且具韧性的通用机器人提供了统一的原则。

Abstract: We introduce InternVLA-M1, a unified framework for spatial grounding and
robot control that advances instruction-following robots toward scalable,
general-purpose intelligence. Its core idea is spatially guided
vision-language-action training, where spatial grounding serves as the critical
link between instructions and robot actions. InternVLA-M1 employs a two-stage
pipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning
data to determine ``where to act'' by aligning instructions with visual,
embodiment-agnostic positions, and (ii) spatially guided action post-training
to decide ``how to act'' by generating embodiment-aware actions through
plug-and-play spatial prompting. This spatially guided training recipe yields
consistent gains: InternVLA-M1 outperforms its variant without spatial guidance
by +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO
Franka, while demonstrating stronger spatial reasoning capability in box,
point, and trace prediction. To further scale instruction following, we built a
simulation engine to collect 244K generalizable pick-and-place episodes,
enabling a 6.2% average improvement across 200 tasks and 3K+ objects. In
real-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with
synthetic co-training, achieved +20.6% on unseen objects and novel
configurations. Moreover, in long-horizon reasoning-intensive scenarios, it
surpassed existing works by over 10%. These results highlight spatially guided
training as a unifying principle for scalable and resilient generalist robots.
Code and models are available at
https://github.com/InternRobotics/InternVLA-M1.

</details>
