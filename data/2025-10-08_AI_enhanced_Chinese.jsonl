{"id": "2510.05213", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05213", "abs": "https://arxiv.org/abs/2510.05213", "authors": ["Yixiao Wang", "Mingxiao Huo", "Zhixuan Liang", "Yushi Du", "Lingfeng Sun", "Haotian Lin", "Jinghuan Shang", "Chensheng Peng", "Mohit Bansal", "Mingyu Ding", "Masayoshi Tomizuka"], "title": "VER: Vision Expert Transformer for Robot Learning via Foundation Distillation and Dynamic Routing", "comment": null, "summary": "Pretrained vision foundation models (VFMs) advance robotic learning via rich\nvisual representations, yet individual VFMs typically excel only in specific\ndomains, limiting generality across tasks. Distilling multiple VFMs into a\nunified representation for policy can mitigate this limitation but often yields\ninflexible task-specific feature selection and requires costly full re-training\nto incorporate robot-domain knowledge. We propose VER, a Vision Expert\ntransformer for Robot learning. During pretraining, VER distills multiple VFMs\ninto a vision expert library. It then fine-tunes only a lightweight routing\nnetwork (fewer than 0.4% of parameters) to dynamically select task-relevant\nexperts from the pretrained library for downstream robot tasks. We further\nintroduce Patchwise Expert Routing with Curriculum Top-K Annealing to improve\nboth flexibility and precision of dynamic expert selection. Moreover, VER\nsupports parameter-efficient finetuning for scalable expert utilization and\nadaptive robot-domain knowledge integration. Across 17 diverse robotic tasks\nand multiple policy heads, VER achieves state-of-the-art performance. We find\nthat VER reduces large-norm outliers in task-irrelevant regions (e.g.,\nbackground) and concentrates on task-critical regions. Visualizations and codes\ncan be found in https://yixiaowang7.github.io/ver_page/.", "AI": {"tldr": "VER\u901a\u8fc7\u63d0\u70bc\u591a\u4e2a\u89c6\u89c9\u6a21\u578b\uff0c\u4f18\u5316\u673a\u5668\u4eba\u5b66\u4e60\u8fc7\u7a0b\uff0c\u652f\u6301\u7075\u6d3b\u3001\u9ad8\u6548\u7684\u4efb\u52a1\u9002\u5e94\u3002", "motivation": "\u89e3\u51b3\u4e2a\u4f53VFMs\u5728\u7279\u5b9a\u9886\u57df\u5185\u7684\u5c40\u9650\u6027\uff0c\u65e8\u5728\u63d0\u9ad8\u591a\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u9884\u8bad\u7ec3\u5c06\u591a\u4e2a\u89c6\u89c9\u57fa\u7840\u6a21\u578b(VFM)\u63d0\u70bc\u4e3a\u89c6\u89c9\u4e13\u5bb6\u5e93\uff0c\u7136\u540e\u5fae\u8c03\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u8def\u7531\u7f51\u7edc\u4ee5\u52a8\u6001\u9009\u62e9\u4efb\u52a1\u76f8\u5173\u4e13\u5bb6\u3002", "result": "VER\u6539\u8fdb\u4e86\u52a8\u6001\u4e13\u5bb6\u9009\u62e9\u7684\u7075\u6d3b\u6027\u548c\u7cbe\u786e\u6027\uff0c\u652f\u6301\u9ad8\u6548\u7684\u53c2\u6570\u5fae\u8c03\uff0c\u4fc3\u8fdb\u4e13\u5bb6\u7684\u53ef\u6269\u5c55\u5229\u7528\u548c\u81ea\u9002\u5e94\u673a\u5668\u4eba\u9886\u57df\u77e5\u8bc6\u6574\u5408\u3002", "conclusion": "VER\u572817\u9879\u591a\u6837\u5316\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u51cf\u5c0f\u4e86\u4efb\u52a1\u65e0\u5173\u533a\u57df\u7684\u5927\u8303\u6570\u79bb\u7fa4\u503c\u3002"}}
{"id": "2510.05330", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.05330", "abs": "https://arxiv.org/abs/2510.05330", "authors": ["Lu Yuanjie", "Mao Mingyang", "Xu Tong", "Wang Linji", "Lin Xiaomin", "Xiao Xuesu"], "title": "Adaptive Dynamics Planning for Robot Navigation", "comment": "8 pages, 4 figures", "summary": "Autonomous robot navigation systems often rely on hierarchical planning,\nwhere global planners compute collision-free paths without considering\ndynamics, and local planners enforce dynamics constraints to produce executable\ncommands. This discontinuity in dynamics often leads to trajectory tracking\nfailure in highly constrained environments. Recent approaches integrate\ndynamics within the entire planning process by gradually decreasing its\nfidelity, e.g., increasing integration steps and reducing collision checking\nresolution, for real-time planning efficiency. However, they assume that the\nfidelity of the dynamics should decrease according to a manually designed\nscheme. Such static settings fail to adapt to environmental complexity\nvariations, resulting in computational overhead in simple environments or\ninsufficient dynamics consideration in obstacle-rich scenarios. To overcome\nthis limitation, we propose Adaptive Dynamics Planning (ADP), a\nlearning-augmented paradigm that uses reinforcement learning to dynamically\nadjust robot dynamics properties, enabling planners to adapt across diverse\nenvironments. We integrate ADP into three different planners and further design\na standalone ADP-based navigation system, benchmarking them against other\nbaselines. Experiments in both simulation and real-world tests show that ADP\nconsistently improves navigation success, safety, and efficiency.", "AI": {"tldr": "\u81ea\u9002\u5e94\u52a8\u6001\u89c4\u5212\uff08ADP\uff09\u901a\u8fc7\u589e\u5f3a\u5b66\u4e60\u8c03\u6574\u52a8\u6001\u5c5e\u6027\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u548c\u5b89\u5168\u7684\u81ea\u4e3b\u5bfc\u822a\u3002", "motivation": "\u5f53\u524d\u81ea\u4e3b\u673a\u5668\u4eba\u5bfc\u822a\u7cfb\u7edf\u5728\u52a8\u6001\u89c4\u5212\u4e2d\u5b58\u5728\u4e0d\u8fde\u7eed\u6027\uff0c\u5bfc\u81f4\u5728\u9ad8\u5ea6\u7ea6\u675f\u73af\u5883\u4e0b\u7684\u8f68\u8ff9\u8ddf\u8e2a\u5931\u8d25\u3002", "method": "\u5c06ADP\u96c6\u6210\u5230\u4e09\u79cd\u4e0d\u540c\u7684\u89c4\u5212\u5668\u4e2d\uff0c\u5e76\u8bbe\u8ba1\u4e86\u72ec\u7acb\u7684ADP\u5bfc\u822a\u7cfb\u7edf\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u52a8\u6001\u89c4\u5212\uff08ADP\uff09\uff0c\u901a\u8fc7\u589e\u5f3a\u5b66\u4e60\u52a8\u6001\u8c03\u6574\u673a\u5668\u4eba\u52a8\u6001\u5c5e\u6027\uff0c\u63d0\u9ad8\u4e86\u5bfc\u822a\u6210\u529f\u7387\u3001\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u8f83\u4e8e\u5176\u4ed6\u57fa\u51c6\uff0cADP\u5728\u5bfc\u822a\u6210\u529f\u7387\u548c\u5b89\u5168\u6027\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2510.05382", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.05382", "abs": "https://arxiv.org/abs/2510.05382", "authors": ["Zhuowei Xu", "Zilin Si", "Kevin Zhang", "Oliver Kroemer", "Zeynep Temel"], "title": "A multi-modal tactile fingertip design for robotic hands to enhance dexterous manipulation", "comment": null, "summary": "Tactile sensing holds great promise for enhancing manipulation precision and\nversatility, but its adoption in robotic hands remains limited due to high\nsensor costs, manufacturing and integration challenges, and difficulties in\nextracting expressive and reliable information from signals. In this work, we\npresent a low-cost, easy-to-make, adaptable, and compact fingertip design for\nrobotic hands that integrates multi-modal tactile sensors. We use strain gauge\nsensors to capture static forces and a contact microphone sensor to measure\nhigh-frequency vibrations during contact. These tactile sensors are integrated\ninto a compact design with a minimal sensor footprint, and all sensors are\ninternal to the fingertip and therefore not susceptible to direct wear and tear\nfrom interactions. From sensor characterization, we show that strain gauge\nsensors provide repeatable 2D planar force measurements in the 0-5 N range and\nthe contact microphone sensor has the capability to distinguish contact\nmaterial properties. We apply our design to three dexterous manipulation tasks\nthat range from zero to full visual occlusion. Given the expressiveness and\nreliability of tactile sensor readings, we show that different tactile sensing\nmodalities can be used flexibly in different stages of manipulation, solely or\ntogether with visual observations to achieve improved task performance. For\ninstance, we can precisely count and unstack a desired number of paper cups\nfrom a stack with 100\\% success rate which is hard to achieve with vision only.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u6613\u4e8e\u5236\u4f5c\u7684\u9002\u5e94\u6027\u673a\u5668\u4eba\u624b\u6307\u5c16\u8bbe\u8ba1\uff0c\u96c6\u6210\u4e86\u591a\u6a21\u6001\u89e6\u89c9\u4f20\u611f\u5668\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u64cd\u4f5c\u7684\u7cbe\u786e\u5ea6\u548c\u591a\u6837\u6027\u3002", "motivation": "\u89e6\u89c9\u4f20\u611f\u6280\u672f\u5728\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7cbe\u5ea6\u548c\u7075\u6d3b\u6027\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u53d7\u9650\u4e8e\u9ad8\u4f20\u611f\u5668\u6210\u672c\u3001\u5236\u9020\u548c\u96c6\u6210\u6311\u6218\u4ee5\u53ca\u4fe1\u53f7\u63d0\u53d6\u7684\u56f0\u96be\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528\u5e94\u53d8\u8ba1\u4f20\u611f\u5668\u548c\u63a5\u89e6\u9ea6\u514b\u98ce\u4f20\u611f\u5668\u96c6\u6210\u5230\u7d27\u51d1\u7684\u673a\u5668\u4eba\u624b\u6307\u5c16\u8bbe\u8ba1\u4e2d\uff0c\u4ee5\u6355\u6349\u9759\u6001\u529b\u548c\u9ad8\u9891\u632f\u52a8\u3002", "result": "\u5e94\u53d8\u8ba1\u4f20\u611f\u5668\u57280-5 N\u8303\u56f4\u5185\u63d0\u4f9b\u53ef\u91cd\u590d\u76842D\u5e73\u9762\u529b\u6d4b\u91cf\uff0c\u63a5\u89e6\u9ea6\u514b\u98ce\u4f20\u611f\u5668\u80fd\u591f\u533a\u5206\u63a5\u89e6\u6750\u6599\u7279\u6027\u3002\u901a\u8fc7\u4e09\u4e2a\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u9a8c\u8bc1\u6211\u4eec\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6211\u4eec\u7684\u8bbe\u8ba1\u80fd\u591f\u5728\u4e0d\u540c\u64cd\u4f5c\u9636\u6bb5\u7075\u6d3b\u4f7f\u7528\u591a\u79cd\u89e6\u89c9\u4f20\u611f\u6280\u672f\uff0c\u663e\u8457\u63d0\u9ad8\u4efb\u52a1\u8868\u73b0\u3002"}}
{"id": "2510.05425", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.05425", "abs": "https://arxiv.org/abs/2510.05425", "authors": ["Marta Lagomarsino", "Francesco Tassi"], "title": "Towards Online Robot Interaction Adaptation to Human Upper-limb Mobility Impairments in Return-to-Work Scenarios", "comment": null, "summary": "Work environments are often inadequate and lack inclusivity for individuals\nwith upper-body disabilities. This paper presents a novel online framework for\nadaptive human-robot interaction (HRI) that accommodates users' arm mobility\nimpairments, ultimately aiming to promote active work participation. Unlike\ntraditional human-robot collaboration approaches that assume able-bodied users,\nour method integrates a mobility model for specific joint limitations into a\nhierarchical optimal controller. This allows the robot to generate reactive,\nmobility-aware behaviour online and guides the user's impaired limb to exploit\nresidual functional mobility. The framework was tested in handover tasks\ninvolving different upper-limb mobility impairments (i.e., emulated elbow and\nshoulder arthritis, and wrist blockage), under both standing and seated\nconfigurations with task constraints using a mobile manipulator, and\ncomplemented by quantitative and qualitative comparisons with state-of-the-art\nergonomic HRI approaches. Preliminary results indicated that the framework can\npersonalise the interaction to fit within the user's impaired range of motion\nand encourage joint usage based on the severity of their functional\nlimitations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7ebf\u6846\u67b6\uff0c\u901a\u8fc7\u9002\u5e94\u7528\u6237\u7684\u81c2\u90e8\u6b8b\u75be\uff0c\u6539\u8fdb\u4eba\u673a\u4ea4\u4e92\uff0c\u4ee5\u4fc3\u8fdb\u5de5\u4f5c\u53c2\u4e0e\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u73af\u5883\u672a\u80fd\u5145\u5206\u5305\u5bb9\u4e0a\u4e0b\u80a2\u6b8b\u75be\u4eba\u58eb\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u65b0\u7684\u65b9\u6cd5\u6765\u4fc3\u8fdb\u4ed6\u4eec\u7684\u5de5\u4f5c\u53c2\u4e0e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5728\u7ebf\u9002\u5e94\u6027\u4eba\u673a\u4ea4\u4e92\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u7279\u5b9a\u5173\u8282\u9650\u5236\u7684\u8fd0\u52a8\u6a21\u578b\u4e0e\u5206\u5c42\u6700\u4f18\u63a7\u5236\u5668\u3002", "result": "\u901a\u8fc7\u624b\u4f20\u4efb\u52a1\u6d4b\u8bd5\uff0c\u6846\u67b6\u80fd\u591f\u4e2a\u6027\u5316\u4e92\u52a8\uff0c\u9f13\u52b1\u7528\u6237\u6839\u636e\u5176\u529f\u80fd\u9650\u5236\u7a0b\u5ea6\u4f7f\u7528\u5173\u8282\uff0c\u5e76\u4e0e\u73b0\u6709\u7684\u4eba\u673a\u4ea4\u4e92\u65b9\u6cd5\u8fdb\u884c\u5b9a\u91cf\u548c\u5b9a\u6027\u6bd4\u8f83\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u6839\u636e\u7528\u6237\u7684\u6b8b\u75be\u9002\u5e94\u6027\u8c03\u6574\uff0c\u4f7f\u4e92\u52a8\u66f4\u52a0\u4e2a\u6027\u5316\u548c\u6709\u6548\u3002"}}
{"id": "2510.05249", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.05249", "abs": "https://arxiv.org/abs/2510.05249", "authors": ["Bhavya Matam", "Adamay Mann", "Kachina Studer", "Christian Gabbianelli", "Sonia Castelo", "John Liu", "Claudio Silva", "Dishita Turakhia"], "title": "CLAd-VR: Cognitive Load-based Adaptive Training for Machining Tasks in Virtual Reality", "comment": null, "summary": "With the growing need to effectively support workforce upskilling in the\nmanufacturing sector, virtual reality is gaining popularity as a scalable\ntraining solution. However, most current systems are designed as static,\nstep-by-step tutorials and do not adapt to a learner's needs or cognitive load,\nwhich is a critical factor in learning and longterm retention. We address this\nlimitation with CLAd-VR, an adaptive VR training system that integrates\nrealtime EEG-based sensing to measure the learner's cognitive load and adapt\ninstruction accordingly, specifically for domain-specific tasks in\nmanufacturing. The system features a VR training module for a precision\ndrilling task, designed with multimodal instructional elements including\nanimations, text, and video. Our cognitive load sensing pipeline uses a\nwearable EEG device to capture the trainee's neural activity, which is\nprocessed through an LSTM model to classify their cognitive load as low,\noptimal, or high in real time. Based on these classifications, the system\ndynamically adjusts task difficulty and delivers adaptive guidance using voice\nguidance, visual cues, or ghost hand animations. This paper introduces CLAd-VR\nsystem's architecture, including the EEG sensing hardware, real-time inference\nmodel, and adaptive VR interface.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aCLAd-VR\u7684\u81ea\u9002\u5e94\u865a\u62df\u73b0\u5b9e\u57f9\u8bad\u7cfb\u7edf\uff0c\u65e8\u5728\u9488\u5bf9\u5236\u9020\u884c\u4e1a\u7684\u5458\u5de5\u6280\u80fd\u63d0\u5347\uff0c\u5229\u7528\u5b9e\u65f6EEG\u611f\u77e5\u6765\u8c03\u6574\u8bad\u7ec3\u5185\u5bb9\uff0c\u589e\u52a0\u5b66\u4e60\u6548\u679c\u548c\u957f\u671f\u8bb0\u5fc6\u3002", "motivation": "\u968f\u7740\u5236\u9020\u4e1a\u5bf9\u5458\u5de5\u6280\u80fd\u63d0\u5347\u7684\u9700\u6c42\u589e\u52a0\uff0c\u73b0\u6709\u7684\u9759\u6001\u57f9\u8bad\u7cfb\u7edf\u65e0\u6cd5\u6ee1\u8db3\u4e2a\u6027\u5316\u5b66\u4e60\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u8005\u8ba4\u77e5\u8d1f\u8377\u7684\u81ea\u9002\u5e94\u57f9\u8bad\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7cfb\u7edf\u4f7f\u7528\u53ef\u7a7f\u6234EEG\u8bbe\u5907\u5b9e\u65f6\u6355\u6349\u5b66\u5458\u7684\u795e\u7ecf\u6d3b\u52a8\uff0c\u901a\u8fc7LSTM\u6a21\u578b\u5bf9\u8ba4\u77e5\u8d1f\u8377\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u6839\u636e\u5206\u7c7b\u7ed3\u679c\u8c03\u6574\u57f9\u8bad\u5185\u5bb9\u3002", "result": "\u7cfb\u7edf\u6210\u529f\u5730\u5b9e\u73b0\u4e86\u5bf9\u5b66\u5458\u8ba4\u77e5\u8d1f\u8377\u7684\u5b9e\u65f6\u5206\u7c7b\uff0c\u5e76\u80fd\u591f\u52a8\u6001\u8c03\u6574\u6559\u5b66\u96be\u5ea6\u548c\u63d0\u4f9b\u9002\u5f53\u6307\u5bfc\uff0c\u63d0\u5347\u4e86\u57f9\u8bad\u7684\u9002\u5e94\u6027\u548c\u6548\u679c\u3002", "conclusion": "CLAd-VR\u7cfb\u7edf\u901a\u8fc7\u5b9e\u65f6\u76d1\u6d4b\u5b66\u5458\u7684\u8ba4\u77e5\u8d1f\u8377\uff0c\u5b9e\u73b0\u4e86\u52a8\u6001\u8c03\u6574\u4efb\u52a1\u96be\u5ea6\u548c\u4e2a\u6027\u5316\u6307\u5bfc\uff0c\u63d0\u9ad8\u4e86\u57f9\u8bad\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.05430", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.05430", "abs": "https://arxiv.org/abs/2510.05430", "authors": ["Huayi Tang", "Pratik Chaudhari"], "title": "Active Semantic Perception", "comment": null, "summary": "We develop an approach for active semantic perception which refers to using\nthe semantics of the scene for tasks such as exploration. We build a compact,\nhierarchical multi-layer scene graph that can represent large, complex indoor\nenvironments at various levels of abstraction, e.g., nodes corresponding to\nrooms, objects, walls, windows etc. as well as fine-grained details of their\ngeometry. We develop a procedure based on large language models (LLMs) to\nsample plausible scene graphs of unobserved regions that are consistent with\npartial observations of the scene. These samples are used to compute an\ninformation gain of a potential waypoint for sophisticated spatial reasoning,\ne.g., the two doors in the living room can lead to either a kitchen or a\nbedroom. We evaluate this approach in complex, realistic 3D indoor environments\nin simulation. We show using qualitative and quantitative experiments that our\napproach can pin down the semantics of the environment quicker and more\naccurately than baseline approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u7684\u4e3b\u52a8\u611f\u77e5\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u573a\u666f\u56fe\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u589e\u5f3a\u5ba4\u5185\u73af\u5883\u63a2\u7d22\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u4e3b\u52a8\u5f0f\u8bed\u4e49\u611f\u77e5\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u573a\u666f\u7684\u8bed\u4e49\u8fdb\u884c\u63a2\u7d22\u7b49\u4efb\u52a1\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6765\u91c7\u6837\u4e0e\u90e8\u5206\u89c2\u6d4b\u4e00\u81f4\u7684\u672a\u89c2\u5bdf\u533a\u57df\u7684\u573a\u666f\u56fe\uff0c\u5e76\u8ba1\u7b97\u6f5c\u5728\u8def\u5f84\u70b9\u7684\u4fe1\u606f\u589e\u76ca\u3002", "result": "\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6b64\u65b9\u6cd5\u5728\u590d\u6742\u76843D\u5ba4\u5185\u73af\u5883\u4e2d\u76f8\u8f83\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u590d\u6742\u76843D\u5ba4\u5185\u73af\u5883\u4e2d\u80fd\u591f\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u66f4\u5feb\u66f4\u51c6\u786e\u5730\u786e\u5b9a\u73af\u5883\u7684\u8bed\u4e49\u3002"}}
{"id": "2510.05271", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.05271", "abs": "https://arxiv.org/abs/2510.05271", "authors": ["Prashanth Arun", "Vinita Vader", "Erya Xu", "Brent McCready-Branch", "Sarah Seabrook", "Kyle Scholz", "Ana Crisan", "Igor Grossmann", "Pascal Poupart"], "title": "Chrysalis: A Unified System for Comparing Active Teaching and Passive Learning with AI Agents in Education", "comment": "13 pages. Submitted to EAAI-26 symposium. Under review", "summary": "AI-assisted learning has seen a remarkable uptick over the last few years,\nmainly due to the rise in popularity of Large Language Models (LLMs). Their\nability to hold long-form, natural language interactions with users makes them\nexcellent resources for exploring school- and university-level topics in a\ndynamic, active manner. We compare students' experiences when interacting with\nan LLM companion in two capacities: tutored learning and learning-by-teaching.\nWe do this using Chrysalis, an LLM-based system that we have designed to\nsupport both AI tutors and AI teachable agents for any topic. Through a\nwithin-subject exploratory study with 36 participants, we present insights into\nstudent preferences between the two strategies and how constructs such as\nintellectual humility vary between these two interaction modes. To our\nknowledge, we are the first to conduct a direct comparison study on the effects\nof using an LLM as a tutor versus as a teachable agent on multiple topics. We\nhope that our work opens up new avenues for future research in this area.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u4f7f\u7528LLM\u4f5c\u4e3a\u5bfc\u5e08\u4e0e\u53ef\u6559\u4ee3\u7406\u7684\u5b66\u751f\u4f53\u9a8c\uff0c\u53d1\u73b0\u4e24\u8005\u5728\u4e92\u52a8\u6a21\u5f0f\u4e0e\u5b66\u4e60\u6548\u679c\u4e0a\u5b58\u5728\u5dee\u5f02\uff0c\u5e76\u5f3a\u8c03\u4e86\u672a\u6765\u7814\u7a76\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7531\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u666e\u53ca\uff0cAI\u8f85\u52a9\u5b66\u4e60\u7684\u9700\u6c42\u663e\u8457\u589e\u52a0\uff0c\u65e8\u5728\u63a2\u8ba8LLM\u5728\u6559\u80b2\u4e2d\u7684\u4e0d\u540c\u5e94\u7528\u65b9\u5f0f\u3002", "method": "\u901a\u8fc7\u5728Chrysalis\u7cfb\u7edf\u4e0a\u8fdb\u884c\u768436\u540d\u53c2\u4e0e\u8005\u7684\u63a2\u7a76\u6027\u7814\u7a76\uff0c\u63a2\u8ba8\u4e86\u5b66\u751f\u5728\u8f85\u5bfc\u5b66\u4e60\u548c\u6559\u5b66\u5b66\u4e60\u8fd9\u4e24\u79cd\u6a21\u5f0f\u4e0b\u7684\u4f53\u9a8c\u548c\u504f\u597d\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u5b66\u751f\u5728\u4e0eLLM\u4e92\u52a8\u65f6\u7684\u504f\u597d\uff0c\u5e76\u5206\u6790\u4e86\u5728\u8fd9\u4e24\u79cd\u4e92\u52a8\u6a21\u5f0f\u4e0b\u667a\u529b\u8c26\u900a\u7b49\u6784\u5ff5\u7684\u5dee\u5f02\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u9996\u6b21\u76f4\u63a5\u6bd4\u8f83\u4e86\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f5c\u4e3a\u5bfc\u5e08\u4e0e\u4f5c\u4e3a\u53ef\u6559\u4ee3\u7406\u4e4b\u95f4\u7684\u6548\u679c\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u6253\u5f00\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.05443", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.05443", "abs": "https://arxiv.org/abs/2510.05443", "authors": ["Shao-Yi Yu", "Jen-Wei Wang", "Maya Horii", "Vikas Garg", "Tarek Zohdi"], "title": "AD-NODE: Adaptive Dynamics Learning with Neural ODEs for Mobile Robots Control", "comment": null, "summary": "Mobile robots, such as ground vehicles and quadrotors, are becoming\nincreasingly important in various fields, from logistics to agriculture, where\nthey automate processes in environments that are difficult to access for\nhumans. However, to perform effectively in uncertain environments using\nmodel-based controllers, these systems require dynamics models capable of\nresponding to environmental variations, especially when direct access to\nenvironmental information is limited. To enable such adaptivity and facilitate\nintegration with model predictive control, we propose an adaptive dynamics\nmodel which bypasses the need for direct environmental knowledge by inferring\noperational environments from state-action history. The dynamics model is based\non neural ordinary equations, and a two-phase training procedure is used to\nlearn latent environment representations. We demonstrate the effectiveness of\nour approach through goal-reaching and path-tracking tasks on three robotic\nplatforms of increasing complexity: a 2D differential wheeled robot with\nchanging wheel contact conditions, a 3D quadrotor in variational wind fields,\nand the Sphero BOLT robot under two contact conditions for real-world\ndeployment. Empirical results corroborate that our method can handle temporally\nand spatially varying environmental changes in both simulation and real-world\nsystems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u4f7f\u79fb\u52a8\u673a\u5668\u4eba\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u80fd\u6709\u6548\u5de5\u4f5c\uff0c\u65e0\u9700\u76f4\u63a5\u4e86\u89e3\u73af\u5883\uff0c\u57fa\u4e8e\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\uff0c\u5e76\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u5b66\u4e60\u73af\u5883\u8868\u793a\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u673a\u5668\u4eba\u5e73\u53f0\u3002", "motivation": "\u79fb\u52a8\u673a\u5668\u4eba\u5728\u5404\u79cd\u9886\u57df\u7684\u91cd\u8981\u6027\u65e5\u76ca\u589e\u52a0\uff0c\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u9700\u8981\u6709\u6548\u7684\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u5c24\u5176\u662f\u5728\u7f3a\u4e4f\u76f4\u63a5\u73af\u5883\u4fe1\u606f\u65f6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\u7684\u81ea\u9002\u5e94\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u5b66\u4e60\u6f5c\u5728\u73af\u5883\u8868\u793a\u3002", "result": "\u901a\u8fc7\u5728\u4e09\u79cd\u590d\u6742\u6027\u9012\u589e\u7684\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u8fdb\u884c\u7684\u76ee\u6807\u5230\u8fbe\u548c\u8def\u5f84\u8ddf\u8e2a\u4efb\u52a1\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5904\u7406\u65f6\u7a7a\u53d8\u5316\u7684\u73af\u5883\u53d8\u5316\uff0c\u9002\u7528\u4e8e\u6a21\u62df\u548c\u73b0\u5b9e\u7cfb\u7edf\u3002"}}
{"id": "2510.05307", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.05307", "abs": "https://arxiv.org/abs/2510.05307", "authors": ["Jieyu Zhou", "Aryan Roy", "Sneh Gupta", "Daniel Weitekamp", "Christopher J. MacLellan"], "title": "When Should Users Check? A Decision-Theoretic Model of Confirmation Frequency in Multi-Step AI Agent Tasks", "comment": null, "summary": "Existing AI agents typically execute multi-step tasks autonomously and only\nallow user confirmation at the end. During execution, users have little\ncontrol, making the confirm-at-end approach brittle: a single error can cascade\nand force a complete restart. Confirming every step avoids such failures, but\nimposes tedious overhead. Balancing excessive interruptions against costly\nrollbacks remains an open challenge. We address this problem by modeling\nconfirmation as a minimum time scheduling problem. We conducted a formative\nstudy with eight participants, which revealed a recurring\nConfirmation-Diagnosis-Correction-Redo (CDCR) pattern in how users monitor\nerrors. Based on this pattern, we developed a decision-theoretic model to\ndetermine time-efficient confirmation point placement. We then evaluated our\napproach using a within-subjects study where 48 participants monitored AI\nagents and repaired their mistakes while executing tasks. Results show that 81\npercent of participants preferred our intermediate confirmation approach over\nthe confirm-at-end approach used by existing systems, and task completion time\nwas reduced by 13.54 percent.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u786e\u8ba4\u65b9\u6cd5\uff0c\u6539\u5584\u4e86AI\u4ee3\u7406\u7684\u7528\u6237\u4ea4\u4e92\u4f53\u9a8c\uff0c\u51cf\u5c11\u4e86\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u7684AI\u4ee3\u7406\u6267\u884c\u591a\u6b65\u9aa4\u4efb\u52a1\u65f6\uff0c\u7528\u6237\u5728\u6267\u884c\u8fc7\u7a0b\u4e2d\u63a7\u5236\u6709\u9650\uff0c\u5bfc\u81f4\u786e\u8ba4\u65b9\u6cd5\u8106\u5f31\u4e14\u6613\u51fa\u9519\u3002", "method": "\u901a\u8fc7\u5bf948\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u7684\u5728\u88ab\u8bd5\u7814\u7a76\uff0c\u8bc4\u4f30\u4e86\u57fa\u4e8eCDCR\u6a21\u5f0f\u7684\u51b3\u7b56\u7406\u8bba\u6a21\u578b\u3002", "result": "81%\u7684\u53c2\u4e0e\u8005\u66f4\u559c\u6b22\u4e2d\u95f4\u786e\u8ba4\u7684\u65b9\u6cd5\uff0c\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u51cf\u5c11\u4e8613.54%\u3002", "conclusion": "\u53c2\u4e0e\u8005\u66f4\u559c\u6b22\u4e2d\u95f4\u786e\u8ba4\u7684\u65b9\u6cd5\uff0c\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u51cf\u5c11\u4e8613.54%\u3002"}}
{"id": "2510.05536", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.05536", "abs": "https://arxiv.org/abs/2510.05536", "authors": ["Mahboubeh Zarei", "Robin Chhabra", "Farrokh Janabi-Sharifi"], "title": "Correlation-Aware Dual-View Pose and Velocity Estimation for Dynamic Robotic Manipulation", "comment": null, "summary": "Accurate pose and velocity estimation is essential for effective spatial task\nplanning in robotic manipulators. While centralized sensor fusion has\ntraditionally been used to improve pose estimation accuracy, this paper\npresents a novel decentralized fusion approach to estimate both pose and\nvelocity. We use dual-view measurements from an eye-in-hand and an eye-to-hand\nvision sensor configuration mounted on a manipulator to track a target object\nwhose motion is modeled as random walk (stochastic acceleration model). The\nrobot runs two independent adaptive extended Kalman filters formulated on a\nmatrix Lie group, developed as part of this work. These filters predict poses\nand velocities on the manifold $\\mathbb{SE}(3) \\times \\mathbb{R}^3 \\times\n\\mathbb{R}^3$ and update the state on the manifold $\\mathbb{SE}(3)$. The final\nfused state comprising the fused pose and velocities of the target is obtained\nusing a correlation-aware fusion rule on Lie groups. The proposed method is\nevaluated on a UFactory xArm 850 equipped with Intel RealSense cameras,\ntracking a moving target. Experimental results validate the effectiveness and\nrobustness of the proposed decentralized dual-view estimation framework,\nshowing consistent improvements over state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u53bb\u4e2d\u5fc3\u5316\u878d\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u89c6\u89d2\u6d4b\u91cf\u5b9e\u73b0\u5bf9\u673a\u5668\u4eba\u64cd\u4f5c\u624b\u7684\u59ff\u6001\u548c\u901f\u5ea6\u7684\u9ad8\u6548\u4f30\u8ba1\u3002", "motivation": "\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\uff0c\u51c6\u786e\u7684\u59ff\u6001\u548c\u901f\u5ea6\u4f30\u8ba1\u662f\u6709\u6548\u7a7a\u95f4\u4efb\u52a1\u89c4\u5212\u7684\u5fc5\u8981\u6761\u4ef6\u3002", "method": "\u4f7f\u7528\u53cc\u89c6\u89d2\u6d4b\u91cf\uff0c\u5728\u6bcf\u4e2a\u64cd\u4f5c\u624b\u4e0a\u8fd0\u884c\u4e24\u4e2a\u72ec\u7acb\u7684\u81ea\u9002\u5e94\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff0c\u5e76\u5728\u6d41\u5f62\u4e0a\u66f4\u65b0\u72b6\u6001\u3002", "result": "\u5728UFactory xArm 850\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6240\u63d0\u51fa\u65b9\u6cd5\u5bf9\u76ee\u6807\u8ddf\u8e2a\u5c55\u793a\u4e86\u663e\u8457\u7684\u6548\u679c\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u53bb\u4e2d\u5fc3\u5316\u53cc\u89c6\u89d2\u4f30\u8ba1\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e14\u76f8\u8f83\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u663e\u793a\u51fa\u4e00\u81f4\u7684\u6539\u8fdb\u3002"}}
{"id": "2510.05417", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05417", "abs": "https://arxiv.org/abs/2510.05417", "authors": ["Xinying Hou", "Ruiwei Xiao", "Runlong Ye", "Michael Liut", "John Stamper"], "title": "Exploring Student Choice and the Use of Multimodal Generative AI in Programming Learning", "comment": "7 pages, accepted to SIGCSE2026", "summary": "The broad adoption of Generative AI (GenAI) is impacting Computer Science\neducation, and recent studies found its benefits and potential concerns when\nstudents use it for programming learning. However, most existing explorations\nfocus on GenAI tools that primarily support text-to-text interaction. With\nrecent developments, GenAI applications have begun supporting multiple modes of\ncommunication, known as multimodality. In this work, we explored how\nundergraduate programming novices choose and work with multimodal GenAI tools,\nand their criteria for choices. We selected a commercially available multimodal\nGenAI platform for interaction, as it supports multiple input and output\nmodalities, including text, audio, image upload, and real-time screen-sharing.\nThrough 16 think-aloud sessions that combined participant observation with\nfollow-up semi-structured interviews, we investigated student modality choices\nfor GenAI tools when completing programming problems and the underlying\ncriteria for modality selections. With multimodal communication emerging as the\nfuture of AI in education, this work aims to spark continued exploration on\nunderstanding student interaction with multimodal GenAI in the context of CS\neducation.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u672c\u79d1\u751f\u5982\u4f55\u9009\u62e9\u548c\u4f7f\u7528\u591a\u6a21\u6001\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5de5\u5177\uff0c\u76ee\u7684\u662f\u7406\u89e3\u5176\u5728\u7f16\u7a0b\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u548c\u9009\u62e9\u6807\u51c6\u3002", "motivation": "\u867d\u7136\u5df2\u6709\u7814\u7a76\u63a2\u8ba8\u4e86\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5728\u7f16\u7a0b\u5b66\u4e60\u4e2d\u7684\u597d\u5904\u4e0e\u6f5c\u5728\u95ee\u9898\uff0c\u4f46\u5927\u591a\u6570\u96c6\u4e2d\u4e8e\u6587\u672c\u5230\u6587\u672c\u7684\u4ea4\u4e92\uff0c\u7f3a\u4e4f\u5bf9\u591a\u6a21\u6001\u5de5\u5177\u7684\u7814\u7a76\u3002", "method": "\u901a\u8fc716\u4e2a\u81ea\u8a00\u81ea\u8bed\u7684\u4f1a\u8bdd\uff0c\u7ed3\u5408\u53c2\u4e0e\u8005\u89c2\u5bdf\u548c\u540e\u7eed\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\uff0c\u7814\u7a76\u4e86\u5b66\u751f\u5728\u5b8c\u6210\u7f16\u7a0b\u95ee\u9898\u65f6\u5bf9\u591a\u6a21\u6001\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5de5\u5177\u7684\u9009\u62e9\u53ca\u5176\u4f9d\u636e\u3002", "result": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u672c\u79d1\u751f\u5728\u4f7f\u7528\u591a\u6a21\u6001\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5de5\u5177\u65f6\u7684\u9009\u62e9\u548c\u5de5\u4f5c\u65b9\u5f0f\uff0c\u4ee5\u53ca\u4ed6\u4eec\u7684\u9009\u62e9\u6807\u51c6\u3002", "conclusion": "\u968f\u7740\u591a\u6a21\u6001\u901a\u4fe1\u6210\u4e3a\u6559\u80b2\u9886\u57df\u7684\u8d8b\u52bf\uff0c\u672c\u7814\u7a76\u65e8\u5728\u6fc0\u53d1\u5bf9\u5b66\u751f\u5982\u4f55\u4e0e\u591a\u6a21\u6001\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u4e92\u52a8\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2510.05547", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.05547", "abs": "https://arxiv.org/abs/2510.05547", "authors": ["Eugene Vorobiov", "Ammar Jaleel Mahmood", "Salim Rezvani", "Robin Chhabra"], "title": "ARRC: Advanced Reasoning Robot Control - Knowledge-Driven Autonomous Manipulation Using Retrieval-Augmented Generation", "comment": null, "summary": "We present ARRC (Advanced Reasoning Robot Control), a practical system that\nconnects natural-language instructions to safe local robotic control by\ncombining Retrieval-Augmented Generation (RAG) with RGB-D perception and\nguarded execution on an affordable robot arm. The system indexes curated robot\nknowledge (movement patterns, task templates, and safety heuristics) in a\nvector database, retrieves task-relevant context for each instruction, and\nconditions a large language model (LLM) to produce JSON-structured action\nplans. Plans are executed on a UFactory xArm 850 fitted with a Dynamixel-driven\nparallel gripper and an Intel RealSense D435 camera. Perception uses AprilTag\ndetections fused with depth to produce object-centric metric poses. Execution\nis enforced via software safety gates: workspace bounds, speed and force caps,\ntimeouts, and bounded retries. We describe the architecture, knowledge design,\nintegration choices, and a reproducible evaluation protocol for tabletop scan,\napproach, and pick-place tasks. Experimental results demonstrate the efficacy\nof the proposed approach. Our design shows that RAG-based planning can\nsubstantially improve plan validity and adaptability while keeping perception\nand low-level control local to the robot.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86ARRC\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u901a\u8fc7\u7ed3\u5408RAG\u3001RGB-D\u611f\u77e5\u548c\u5b89\u5168\u6267\u884c\u673a\u5236\uff0c\u6709\u6548\u5730\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8f6c\u5316\u4e3a\u5b89\u5168\u7684\u673a\u5668\u4eba\u63a7\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u5212\u7684\u6709\u6548\u6027\u4e0e\u9002\u5e94\u6027\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u5b9e\u7528\u7684\u7cfb\u7edf\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4e0e\u672c\u5730\u673a\u5668\u4eba\u7684\u5b89\u5168\u63a7\u5236\u8fde\u63a5\u8d77\u6765\u3002", "method": "\u7ed3\u5408\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u3001RGB-D\u611f\u77e5\u548c\u53d7\u4fdd\u62a4\u7684\u6267\u884c\u673a\u5236\u7684\u673a\u5668\u4eba\u63a7\u5236\u7cfb\u7edf\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u73b0\u4e86RAG\u57fa\u7840\u8ba1\u5212\u5728\u63d0\u9ad8\u8ba1\u5212\u6709\u6548\u6027\u548c\u9002\u5e94\u6027\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5bf9\u673a\u5668\u4eba\u7684\u4f4e\u7ea7\u63a7\u5236\u548c\u611f\u77e5\u7684\u672c\u5730\u5316\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684ARRC\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u5730\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8f6c\u5316\u4e3a\u5b89\u5168\u7684\u673a\u5668\u4eba\u63a7\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u5212\u7684\u6709\u6548\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2510.05449", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.05449", "abs": "https://arxiv.org/abs/2510.05449", "authors": ["Matthew J\u00f6rke", "Defne Gen\u00e7", "Valentin Teutschbein", "Shardul Sapkota", "Sarah Chung", "Paul Schmiedmayer", "Maria Ines Campero", "Abby C. King", "Emma Brunskill", "James A. Landay"], "title": "Bloom: Designing for LLM-Augmented Behavior Change Interactions", "comment": null, "summary": "Large language models (LLMs) offer novel opportunities to support health\nbehavior change, yet existing work has narrowly focused on text-only\ninteractions. Building on decades of HCI research demonstrating the\neffectiveness of UI-based interactions, we present Bloom, an application for\nphysical activity promotion that integrates an LLM-based health coaching\nchatbot with established UI-based interactions. As part of Bloom's development,\nwe conducted a redteaming evaluation and contribute a safety benchmark dataset.\nIn a four-week randomized field study (N=54) comparing Bloom to a non-LLM\ncontrol, we observed important shifts in psychological outcomes: participants\nin the LLM condition reported stronger beliefs that activity was beneficial,\ngreater enjoyment, and more self-compassion. Both conditions significantly\nincreased physical activity levels, doubling the proportion of participants\nmeeting recommended weekly guidelines, though we observed no significant\ndifferences between conditions. Instead, our findings suggest that LLMs may be\nmore effective at shifting mindsets that precede longer-term behavior change.", "AI": {"tldr": "Bloom\u5e94\u7528\u7ed3\u5408LLM\u548c\u7528\u6237\u754c\u9762\u63d0\u5347\u4e86\u5065\u5eb7\u884c\u4e3a\u8f6c\u53d8\u7684\u5fc3\u6001\uff0c\u4fc3\u8fdb\u4e86\u8eab\u4f53\u6d3b\u52a8\uff0c\u4f46\u884c\u4e3a\u6539\u53d8\u6548\u679c\u5728LLM\u4e0e\u5bf9\u7167\u7ec4\u4e4b\u95f4\u65e0\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4fc3\u8fdb\u8eab\u4f53\u6d3b\u52a8\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u901a\u8fc7\u5f15\u5165\u66f4\u52a0\u591a\u6837\u5316\u7684\u4ea4\u4e92\u65b9\u5f0f\uff0c\u800c\u4e0d\u4ec5\u9650\u4e8e\u6587\u672c\u4ea4\u4e92\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aBloom\u7684\u5e94\u7528\u7a0b\u5e8f\uff0c\u901a\u8fc7\u6574\u5408\u57fa\u4e8eLLM\u7684\u5065\u5eb7\u8f85\u5bfc\u804a\u5929\u673a\u5668\u4eba\u4e0e\u6210\u719f\u7684\u7528\u6237\u754c\u9762\u4ea4\u4e92\uff0c\u8fdb\u884c\u4e86\u4e3a\u671f\u56db\u5468\u7684\u968f\u673a\u5bf9\u7167\u5b9e\u9a8c\u3002", "result": "LLM\u6761\u4ef6\u4e0b\u7684\u53c2\u4e0e\u8005\u5728\u5fc3\u7406\u7ed3\u679c\u4e0a\u663e\u793a\u51fa\u4fe1\u5ff5\u589e\u5f3a\u3001\u4eab\u53d7\u5ea6\u63d0\u9ad8\u548c\u81ea\u6211\u540c\u60c5\u611f\u589e\u52a0\uff0c\u5c3d\u7ba1\u4e24\u4e2a\u6761\u4ef6\u90fd\u63d0\u9ad8\u4e86\u8eab\u4f53\u6d3b\u52a8\u6c34\u5e73\uff0c\u4f46\u5728\u884c\u4e3a\u6539\u53d8\u4e0a\u672a\u89c1\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4fc3\u8fdb\u5065\u5eb7\u884c\u4e3a\u6539\u53d8\u65b9\u9762\u53ef\u80fd\u66f4\u52a0\u6709\u6548\u5730\u5f71\u54cd\u5fc3\u6001\uff0c\u800c\u975e\u7acb\u7aff\u89c1\u5f71\u5730\u6539\u53d8\u884c\u4e3a\u3002"}}
{"id": "2510.05553", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.05553", "abs": "https://arxiv.org/abs/2510.05553", "authors": ["Yan Rui Tan", "Wenqi Liu", "Wai Lun Leong", "John Guan Zhong Tan", "Wayne Wen Huei Yong", "Fan Shi", "Rodney Swee Huat Teo"], "title": "GO-Flock: Goal-Oriented Flocking in 3D Unknown Environments with Depth Maps", "comment": null, "summary": "Artificial Potential Field (APF) methods are widely used for reactive\nflocking control, but they often suffer from challenges such as deadlocks and\nlocal minima, especially in the presence of obstacles. Existing solutions to\naddress these issues are typically passive, leading to slow and inefficient\ncollective navigation. As a result, many APF approaches have only been\nvalidated in obstacle-free environments or simplified, pseudo 3D simulations.\nThis paper presents GO-Flock, a hybrid flocking framework that integrates\nplanning with reactive APF-based control. GO-Flock consists of an upstream\nPerception Module, which processes depth maps to extract waypoints and virtual\nagents for obstacle avoidance, and a downstream Collective Navigation Module,\nwhich applies a novel APF strategy to achieve effective flocking behavior in\ncluttered environments. We evaluate GO-Flock against passive APF-based\napproaches to demonstrate their respective merits, such as their flocking\nbehavior and the ability to overcome local minima. Finally, we validate\nGO-Flock through obstacle-filled environment and also hardware-in-the-loop\nexperiments where we successfully flocked a team of nine drones, six physical\nand three virtual, in a forest environment.", "AI": {"tldr": "GO-Flock\u6846\u67b6\u7ed3\u5408\u4e86\u89c4\u5212\u4e0eAPF\u63a7\u5236\uff0c\u6539\u5584\u4e86\u5728\u969c\u788d\u7269\u73af\u5883\u4e2d\u7fa4\u4f53\u6d3b\u52a8\u7684\u8868\u73b0\u3002", "motivation": "APF\u65b9\u6cd5\u5728\u7fa4\u4f53\u63a7\u5236\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5728\u969c\u788d\u7269\u5b58\u5728\u65f6\u5e38\u9762\u4e34\u6b7b\u9501\u548c\u5c40\u90e8\u6700\u4f18\u7b49\u6311\u6218\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5f80\u5f80\u6d88\u6781\uff0c\u5bfc\u81f4\u5bfc\u822a\u6548\u7387\u4f4e\u4e0b\u3002", "method": "GO-Flock\u5305\u542b\u611f\u77e5\u6a21\u5757\u7528\u4e8e\u63d0\u53d6\u8def\u5f84\u70b9\u548c\u865a\u62df\u4ee3\u7406\u4ee5\u907f\u514d\u969c\u788d\uff0c\u4ee5\u53ca\u96c6\u4f53\u5bfc\u822a\u6a21\u5757\u91c7\u7528\u65b0\u578bAPF\u7b56\u7565\u5b9e\u73b0\u6709\u6548\u7684\u7fa4\u4f53\u884c\u4e3a\u3002", "result": "\u63d0\u51fa\u4e86GO-Flock\u6846\u67b6\uff0c\u6709\u6548\u878d\u5408\u89c4\u5212\u4e0e\u53cd\u5e94\u5f0fAPF\u63a7\u5236\uff0c\u63d0\u5347\u4e86\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u7fa4\u4f53\u5bfc\u822a\u80fd\u529b\u3002", "conclusion": "GO-Flock\u5c55\u793a\u4e86\u5728\u969c\u788d\u73af\u5883\u4e2d\u66f4\u9ad8\u6548\u7684\u7fa4\u4f53\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.05510", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.05510", "abs": "https://arxiv.org/abs/2510.05510", "authors": ["Shunpei Norihama", "Yuka Iwane", "Jo Takezawa", "Simo Hosio", "Mari Hirano", "Naomi Yamashita", "Koji Yatani"], "title": "Two Modes of Reflection: How Temporal, Spatial, and Social Distances Affect Reflective Writing in Family Caregiving", "comment": null, "summary": "Writing about personal experiences can improve well-being, but for family\ncaregivers, fixed or user-initiated schedules often miss the right moments.\nDrawing on Construal Level Theory, we conducted a three-week field study with\n47 caregivers using a chatbot that delivered daily reflective writing prompts\nand captured temporal, spatial, and social contexts. We collected 958 writing\nentries, resulting in 5,412 coded segments. Our Analysis revealed two\nreflective modes. Under proximal conditions, participants produced detailed,\nemotion-rich, and care recipient-focused narratives that supported emotional\nrelease. Under distal conditions, they generated calmer, self-focused, and\nanalytic accounts that enabled objective reflection and cognitive reappraisal.\nParticipants described trade-offs: proximity preserved vivid detail but limited\nobjectivity, while distance enabled analysis but risked memory loss. This work\ncontributes empirical evidence of how psychological distances shape reflective\nwriting and proposes design implications for distance-aware Just-in-Time\nAdaptive Interventions for family caregivers' mental health support.", "AI": {"tldr": "\u5bb6\u5ead\u7167\u987e\u8005\u7684\u53cd\u601d\u5199\u4f5c\u53d7\u5fc3\u7406\u8ddd\u79bb\u5f71\u54cd\uff0c\u8fd1\u8ddd\u4fc3\u8fdb\u60c5\u611f\u8868\u8fbe\uff0c\u8fdc\u8ddd\u6709\u52a9\u4e8e\u5ba2\u89c2\u5206\u6790\uff0c\u8bbe\u8ba1\u5e94\u8003\u8651\u8fd9\u4e9b\u56e0\u7d20\u3002", "motivation": "\u63a2\u8ba8\u4e2a\u4eba\u7ecf\u5386\u5199\u4f5c\u5bf9\u5bb6\u5ead\u7167\u987e\u8005\u5fc3\u7406\u5065\u5eb7\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5982\u4f55\u5728\u9002\u5f53\u65f6\u673a\u8fdb\u884c\u53cd\u601d\u5199\u4f5c\u3002", "method": "\u901a\u8fc7\u4e3a\u671f\u4e09\u5468\u7684\u5b9e\u5730\u7814\u7a76\uff0c\u4f7f\u7528\u804a\u5929\u673a\u5668\u4eba\u6536\u96c6\u5e76\u5206\u679047\u4f4d\u7167\u987e\u8005\u7684\u53cd\u601d\u5199\u4f5c\uff0c\u83b7\u53d6958\u4e2a\u5199\u4f5c\u6761\u76ee\u548c5412\u4e2a\u7f16\u7801\u7247\u6bb5\u3002", "result": "\u53d1\u73b0\u4e24\u79cd\u53cd\u601d\u6a21\u5f0f\uff1a\u8fd1\u8ddd\u6761\u4ef6\u4e0b\u7684\u7ec6\u8282\u4e30\u5bcc\u548c\u60c5\u611f\u91ca\u653e\uff0c\u8fdc\u8ddd\u6761\u4ef6\u4e0b\u7684\u5e73\u9759\u5ba2\u89c2\u548c\u8ba4\u77e5\u91cd\u8bc4\u3002\u53c2\u4e0e\u8005\u9762\u4e34\u8fd1\u8ddd\u79bb\u548c\u8fdc\u8ddd\u79bb\u5e26\u6765\u7684\u6743\u8861\u3002", "conclusion": "\u5fc3\u7406\u8ddd\u79bb\u5f71\u54cd\u53cd\u601d\u5199\u4f5c\u7684\u65b9\u5f0f\uff0c\u8bbe\u8ba1\u5e94\u8003\u8651\u8fd9\u79cd\u5f71\u54cd\u4ee5\u652f\u6301\u5bb6\u5ead\u7167\u987e\u8005\u7684\u5fc3\u7406\u5065\u5eb7\u3002"}}
{"id": "2510.05662", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05662", "abs": "https://arxiv.org/abs/2510.05662", "authors": ["Taeyeop Lee", "Gyuree Kang", "Bowen Wen", "Youngho Kim", "Seunghyeok Back", "In So Kweon", "David Hyunchul Shim", "Kuk-Jin Yoon"], "title": "DeLTa: Demonstration and Language-Guided Novel Transparent Object Manipulation", "comment": "Project page: https://sites.google.com/view/DeLTa25/", "summary": "Despite the prevalence of transparent object interactions in human everyday\nlife, transparent robotic manipulation research remains limited to\nshort-horizon tasks and basic grasping capabilities.Although some methods have\npartially addressed these issues, most of them have limitations in\ngeneralizability to novel objects and are insufficient for precise long-horizon\nrobot manipulation. To address this limitation, we propose DeLTa (Demonstration\nand Language-Guided Novel Transparent Object Manipulation), a novel framework\nthat integrates depth estimation, 6D pose estimation, and vision-language\nplanning for precise long-horizon manipulation of transparent objects guided by\nnatural task instructions. A key advantage of our method is its\nsingle-demonstration approach, which generalizes 6D trajectories to novel\ntransparent objects without requiring category-level priors or additional\ntraining. Additionally, we present a task planner that refines the\nVLM-generated plan to account for the constraints of a single-arm, eye-in-hand\nrobot for long-horizon object manipulation tasks. Through comprehensive\nevaluation, we demonstrate that our method significantly outperforms existing\ntransparent object manipulation approaches, particularly in long-horizon\nscenarios requiring precise manipulation capabilities. Project page:\nhttps://sites.google.com/view/DeLTa25/", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684DeLTa\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6df1\u5ea6\u4f30\u8ba1\u548c\u89c6\u89c9\u8bed\u8a00\u89c4\u5212\uff0c\u63d0\u5347\u900f\u660e\u7269\u4f53\u64cd\u4f5c\u7684\u7cbe\u5ea6\u548c\u901a\u7528\u6027\u3002", "motivation": "\u5e94\u5bf9\u73b0\u6709\u6280\u672f\u5728\u5904\u7406\u65b0\u5bf9\u8c61\u548c\u957f\u65f6\u57df\u64cd\u63a7\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u7ed3\u5408\u6df1\u5ea6\u4f30\u8ba1\u30016D\u59ff\u6001\u4f30\u8ba1\u548c\u89c6\u89c9\u8bed\u8a00\u89c4\u5212\uff0c\u8fdb\u884c\u7cbe\u786e\u7684\u900f\u660e\u7269\u4f53\u64cd\u63a7\u3002", "result": "\u5168\u9762\u8bc4\u4f30\u8868\u660e\uff0cDeLTa\u5728\u957f\u65f6\u57df\u9700\u8981\u7cbe\u786e\u64cd\u63a7\u7684\u573a\u666f\u4e2d\u4f18\u4e8e\u5176\u4ed6\u5f53\u524d\u65b9\u6cd5\u3002", "conclusion": "\u6211\u4eec\u7684DeLTa\u65b9\u6cd5\u5728\u89e3\u51b3\u900f\u660e\u7269\u4f53\u957f\u65f6\u57df\u64cd\u63a7\u80fd\u529b\u65b9\u9762\u6bd4\u73b0\u6709\u65b9\u6cd5\u8868\u73b0\u663e\u8457\u66f4\u4f73\u3002"}}
{"id": "2510.05679", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.05679", "abs": "https://arxiv.org/abs/2510.05679", "authors": ["Rachel L. Franz", "Jacob O. Wobbrock"], "title": "Locability: An Ability-Based Ranking Model for Virtual Reality Locomotion Techniques", "comment": "36 pages, 11 figures", "summary": "There are over a hundred virtual reality (VR) locomotion techniques that\nexist today, with new ones being designed as VR technology evolves. The\ndifferent ways of controlling locomotion techniques (e.g., gestures, button\ninputs, body movements), along with the diversity of upper-body motor\nimpairments, can make it difficult for a user to know which locomotion\ntechnique is best suited to their particular abilities. Moreover,\ntrial-and-error can be difficult, time-consuming, and costly. Using machine\nlearning techniques and data from 20 people with and without upper-body motor\nimpairments, we developed a modeling approach to predict a ranked list of a\nuser's fastest techniques based on questionnaire and interaction data. We found\nthat a user's fastest technique could be predicted based on interaction data\nwith 92% accuracy and that predicted locomotion times were within 12% of\nobserved times. The model we trained could also rank six locomotion techniques\nbased on speed with 61% accuracy and that predictions were within 8% of\nobserved times. Our findings contribute to growing research in VR accessibility\nby taking an ability-based design approach to adapt systems to users'\nabilities.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u80fd\u591f\u6839\u636e\u7528\u6237\u7684\u4ea4\u4e92\u6570\u636e\u548c\u95ee\u5377\u4fe1\u606f\u9ad8\u6548\u9884\u6d4b\u9002\u5408\u7528\u6237\u7684\u865a\u62df\u73b0\u5b9e\u79fb\u52a8\u6280\u672f\uff0c\u7279\u522b\u662f\u9488\u5bf9\u4e0a\u534a\u8eab\u8fd0\u52a8\u969c\u788d\u7684\u7528\u6237\uff0c\u4fc3\u8fdb\u865a\u62df\u73b0\u5b9e\u7cfb\u7edf\u7684\u53ef\u53ca\u6027\u3002", "motivation": "\u968f\u7740\u865a\u62df\u73b0\u5b9e\u6280\u672f\u7684\u53d1\u5c55\uff0c\u5b58\u5728\u8bb8\u591a\u79fb\u52a8\u6280\u672f\uff0c\u7136\u800c\u7528\u6237\u5728\u9009\u62e9\u6700\u9002\u5408\u81ea\u5df1\u7684\u6280\u672f\u65f6\u9762\u4e34\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5b58\u5728\u4e0d\u540c\u7684\u8eab\u4f53\u80fd\u529b\u65f6\u3002", "method": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u5e76\u57fa\u4e8e20\u540d\u6709\u65e0\u4e0a\u534a\u8eab\u8fd0\u52a8\u969c\u788d\u7684\u7528\u6237\u7684\u6570\u636e\uff0c\u8fdb\u884c\u5efa\u6a21\u4ee5\u9884\u6d4b\u7528\u6237\u6700\u5feb\u7684\u79fb\u52a8\u6280\u672f\u6392\u540d\u3002", "result": "\u4ee592%\u7684\u51c6\u786e\u7387\u9884\u6d4b\u7528\u6237\u6700\u5feb\u7684\u79fb\u52a8\u6280\u672f\uff0c\u9884\u6d4b\u7684\u79fb\u52a8\u65f6\u95f4\u4e0e\u5b9e\u9645\u65f6\u95f4\u76f8\u5dee\u4e0d\u8d85\u8fc712%\uff1b\u80fd\u591f\u4ee561%\u7684\u51c6\u786e\u7387\u5bf9\u516d\u79cd\u79fb\u52a8\u6280\u672f\u8fdb\u884c\u901f\u5ea6\u6392\u540d\uff0c\u4e14\u9884\u6d4b\u4e0e\u89c2\u5bdf\u65f6\u95f4\u76f8\u5dee\u4e0d\u8d85\u8fc78%\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6280\u672f\u9884\u6d4b\u7528\u6237\u6700\u9002\u5408\u7684\u865a\u62df\u73b0\u5b9e\u79fb\u52a8\u6280\u672f\uff0c\u63d0\u9ad8\u4e86\u865a\u62df\u73b0\u5b9e\u7684\u53ef\u53ca\u6027\uff0c\u7279\u522b\u662f\u9488\u5bf9\u4e0a\u534a\u8eab\u8fd0\u52a8\u969c\u788d\u7528\u6237\u3002"}}
{"id": "2510.05681", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05681", "abs": "https://arxiv.org/abs/2510.05681", "authors": ["Suhyeok Jang", "Dongyoung Kim", "Changyeon Kim", "Youngsuk Kim", "Jinwoo Shin"], "title": "Verifier-free Test-Time Sampling for Vision Language Action Models", "comment": "14 pages; 3 figures", "summary": "Vision-Language-Action models (VLAs) have demonstrated remarkable performance\nin robot control. However, they remain fundamentally limited in tasks that\nrequire high precision due to their single-inference paradigm. While test-time\nscaling approaches using external verifiers have shown promise, they require\nadditional training and fail to generalize to unseen conditions. We propose\nMasking Distribution Guided Selection (MG-Select), a novel test-time scaling\nframework for VLAs that leverages the model's internal properties without\nrequiring additional training or external modules. Our approach utilizes KL\ndivergence from a reference action token distribution as a confidence metric\nfor selecting the optimal action from multiple candidates. We introduce a\nreference distribution generated by the same VLA but with randomly masked\nstates and language conditions as inputs, ensuring maximum uncertainty while\nremaining aligned with the target task distribution. Additionally, we propose a\njoint training strategy that enables the model to learn both conditional and\nunconditional distributions by applying dropout to state and language\nconditions, thereby further improving the quality of the reference\ndistribution. Our experiments demonstrate that MG-Select achieves significant\nperformance improvements, including a 28%/35% improvement in real-world\nin-distribution/out-of-distribution tasks, along with a 168% relative gain on\nRoboCasa pick-and-place tasks trained with 30 demonstrations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6d4b\u8bd5\u65f6\u95f4\u6269\u5c55\u6846\u67b6MG-Select\uff0c\u5229\u7528\u6a21\u578b\u5185\u90e8\u5c5e\u6027\u6765\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u9009\u62e9\u80fd\u529b\uff0c\u800c\u4e0d\u9700\u8981\u989d\u5916\u8bad\u7ec3\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u7cbe\u5bc6\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u6ca1\u6709\u989d\u5916\u8bad\u7ec3\u6216\u5916\u90e8\u6a21\u5757\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u6a21\u578b\u7684\u9009\u62e9\u80fd\u529b\u3002", "method": "Masking Distribution Guided Selection (MG-Select)", "result": "MG-Select\u5728\u771f\u5b9e\u4e16\u754c\u7684\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e8628%/35%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4ee5\u53ca\u5728RoboCasa\u6311\u9009\u548c\u653e\u7f6e\u4efb\u52a1\u4e2d\u76f8\u8f83\u4e8e30\u6b21\u6f14\u793a\u7684\u8bad\u7ec3\u53d6\u5f97\u4e86168%\u7684\u76f8\u5bf9\u589e\u76ca\u3002", "conclusion": "MG-Select\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u7cbe\u5ea6\u8981\u6c42\u9ad8\u7684\u573a\u666f\u4e2d\uff0c\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.05742", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.05742", "abs": "https://arxiv.org/abs/2510.05742", "authors": ["Yanwei Huang", "Wesley Hanwen Deng", "Sijia Xiao", "Motahhare Eslami", "Jason I. Hong", "Arpit Narechania", "Adam Perer"], "title": "Vipera: Blending Visual and LLM-Driven Guidance for Systematic Auditing of Text-to-Image Generative AI", "comment": "17 pages, 8 figures", "summary": "Despite their increasing capabilities, text-to-image generative AI systems\nare known to produce biased, offensive, and otherwise problematic outputs.\nWhile recent advancements have supported testing and auditing of generative AI,\nexisting auditing methods still face challenges in supporting effectively\nexplore the vast space of AI-generated outputs in a structured way. To address\nthis gap, we conducted formative studies with five AI auditors and synthesized\nfive design goals for supporting systematic AI audits. Based on these insights,\nwe developed Vipera, an interactive auditing interface that employs multiple\nvisual cues including a scene graph to facilitate image sensemaking and inspire\nauditors to explore and hierarchically organize the auditing criteria.\nAdditionally, Vipera leverages LLM-powered suggestions to facilitate\nexploration of unexplored auditing directions. Through a controlled experiment\nwith 24 participants experienced in AI auditing, we demonstrate Vipera's\neffectiveness in helping auditors navigate large AI output spaces and organize\ntheir analyses while engaging with diverse criteria.", "AI": {"tldr": "\u4e3a\u4e86\u6539\u5584\u5bf9\u751f\u6210AI\u7684\u5ba1\u8ba1\u65b9\u6cd5\uff0c\u7814\u7a76\u56e2\u961f\u5f00\u53d1\u4e86Vipera\uff0c\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u5ba1\u8ba1\u754c\u9762\uff0c\u5e2e\u52a9\u5ba1\u8ba1\u5458\u66f4\u6709\u6548\u5730\u5bfc\u822a\u548c\u5206\u6790AI\u751f\u6210\u7684\u8f93\u51fa\u3002", "motivation": "\u73b0\u6709\u7684AI\u5ba1\u8ba1\u65b9\u6cd5\u5728\u7ed3\u6784\u5316\u63a2\u7d22AI\u751f\u6210\u8f93\u51fa\u7684\u5e7f\u6cdb\u7a7a\u95f4\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002", "method": "\u8fdb\u884c\u4e86\u4e00\u9879\u63a7\u5236\u5b9e\u9a8c\uff0c\u6d89\u53ca24\u540d\u5177\u6709AI\u5ba1\u8ba1\u7ecf\u9a8c\u7684\u53c2\u4e0e\u8005\uff0c\u4ee5\u9a8c\u8bc1Vipera\u7684\u6709\u6548\u6027\u3002", "result": "Vipera\u662f\u4e00\u4e2a\u4e92\u52a8\u5ba1\u8ba1\u754c\u9762\uff0c\u5229\u7528\u591a\u4e2a\u89c6\u89c9\u7ebf\u7d22\uff08\u5305\u62ec\u573a\u666f\u56fe\uff09\u6765\u589e\u5f3a\u56fe\u50cf\u7406\u89e3\uff0c\u5e76\u6fc0\u52b1\u5ba1\u8ba1\u5458\u7ec4\u7ec7\u548c\u63a2\u7d22\u5ba1\u8ba1\u6807\u51c6\u3002", "conclusion": "Vipera\u6709\u6548\u5e2e\u52a9\u5ba1\u8ba1\u5458\u5728\u5e9e\u5927\u7684AI\u8f93\u51fa\u7a7a\u95f4\u4e2d\u5bfc\u822a\uff0c\u5e76\u5728\u4e0e\u591a\u79cd\u6807\u51c6\u4e92\u52a8\u65f6\u7ec4\u7ec7\u5206\u6790\u3002"}}
{"id": "2510.05692", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05692", "abs": "https://arxiv.org/abs/2510.05692", "authors": ["Yuhang Zhang", "Jiaping Xiao", "Chao Yan", "Mir Feroskhan"], "title": "Oracle-Guided Masked Contrastive Reinforcement Learning for Visuomotor Policies", "comment": null, "summary": "A prevailing approach for learning visuomotor policies is to employ\nreinforcement learning to map high-dimensional visual observations directly to\naction commands. However, the combination of high-dimensional visual inputs and\nagile maneuver outputs leads to long-standing challenges, including low sample\nefficiency and significant sim-to-real gaps. To address these issues, we\npropose Oracle-Guided Masked Contrastive Reinforcement Learning (OMC-RL), a\nnovel framework designed to improve the sample efficiency and asymptotic\nperformance of visuomotor policy learning. OMC-RL explicitly decouples the\nlearning process into two stages: an upstream representation learning stage and\na downstream policy learning stage. In the upstream stage, a masked Transformer\nmodule is trained with temporal modeling and contrastive learning to extract\ntemporally-aware and task-relevant representations from sequential visual\ninputs. After training, the learned encoder is frozen and used to extract\nvisual representations from consecutive frames, while the Transformer module is\ndiscarded. In the downstream stage, an oracle teacher policy with privileged\naccess to global state information supervises the agent during early training\nto provide informative guidance and accelerate early policy learning. This\nguidance is gradually reduced to allow independent exploration as training\nprogresses. Extensive experiments in simulated and real-world environments\ndemonstrate that OMC-RL achieves superior sample efficiency and asymptotic\npolicy performance, while also improving generalization across diverse and\nperceptually complex scenarios.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86OMC-RL\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u5b66\u4e60\u6765\u63d0\u5347\u6837\u672c\u6548\u7387\u4e0e\u653f\u7b56\u6027\u80fd\uff0c\u514b\u670d\u9ad8\u7ef4\u89c6\u89c9\u8f93\u5165\u4e0e\u52a8\u4f5c\u8f93\u51fa\u7ed3\u5408\u7684\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u9ad8\u7ef4\u89c6\u89c9\u8f93\u5165\u4e0e\u654f\u6377\u52a8\u4f5c\u8f93\u51fa\u7ed3\u5408\u5e26\u6765\u7684\u6837\u672c\u6548\u7387\u4f4e\u548csim-to-real\u5dee\u8ddd\u5927\u7684\u95ee\u9898\u3002", "method": "Oracle-Guided Masked Contrastive Reinforcement Learning (OMC-RL)", "result": "OMC-RL\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6837\u672c\u6548\u7387\u548c\u653f\u7b56\u6027\u80fd\uff0c\u540c\u65f6\u6539\u5584\u4e86\u5728\u591a\u6837\u548c\u611f\u77e5\u590d\u6742\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "OMC-RL\u6709\u6548\u63d0\u5347\u4e86\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387\u548c\u6027\u80fd\uff0c\u5e76\u5728\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.05833", "categories": ["cs.HC", "91E30, 62P25, 68T05", "H.5.2; H.1.2; H.5.0; J.4"], "pdf": "https://arxiv.org/pdf/2510.05833", "abs": "https://arxiv.org/abs/2510.05833", "authors": ["B. Sankar", "Devottama Sen", "Dibakar Sen"], "title": "The Interplay of Attention and Memory in Visual Enumeration", "comment": "51 pages, 21 figures", "summary": "Humans navigate and understand complex visual environments by subconsciously\nquantifying what they see, a process known as visual enumeration. However,\ntraditional studies using flat screens fail to capture the cognitive dynamics\nof this process over the large visual fields of real-world scenes. To address\nthis gap, we developed an immersive virtual reality system with integrated\neye-tracking to investigate the interplay between attention and memory during\ncomplex enumeration. We conducted a two-phase experiment where participants\nenumerated scenes of either simple abstract shapes or complex real-world\nobjects, systematically varying the task intent (e.g., selective vs. exhaustive\ncounting) and the spatial layout of items. Our results reveal that task intent\nis the dominant factor driving performance, with selective counting imposing a\nsignificant cognitive cost that was dramatically amplified by stimulus\ncomplexity. The semantic processing required for real-world objects reduced\naccuracy and suppressed memory recall, while the influence of spatial layout\nwas secondary and statistically non-significant when a higher-order cognitive\ntask intent was driving the human behaviour. We conclude that real-world\nenumeration is fundamentally constrained by the cognitive load of semantic\nprocessing, not just the mechanics of visual search. Our findings demonstrate\nthat under high cognitive demand, the effort to understand what we are seeing\ndirectly limits our capacity to remember it.", "AI": {"tldr": "\u4f7f\u7528\u6c89\u6d78\u5f0f\u865a\u62df\u73b0\u5b9e\u7cfb\u7edf\u7814\u7a76\u89c6\u89c9\u679a\u4e3e\u8fc7\u7a0b\u4e2d\u7684\u6ce8\u610f\u529b\u4e0e\u8bb0\u5fc6\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u4efb\u52a1\u610f\u56fe\u662f\u5f71\u54cd\u8868\u73b0\u7684\u4e3b\u8981\u56e0\u7d20\uff0c\u590d\u6742\u6027\u589e\u52a0\u663e\u8457\u964d\u4f4e\u51c6\u786e\u6027\u548c\u8bb0\u5fc6\u53ec\u56de\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u5e73\u9762\u5c4f\u5e55\u7814\u7a76\u65e0\u6cd5\u6355\u6349\u4eba\u7c7b\u5728\u5927\u89c6\u89c9\u573a\u666f\u4e2d\u6267\u884c\u89c6\u89c9\u679a\u4e3e\u7684\u8ba4\u77e5\u52a8\u6001\u3002", "method": "\u5f00\u53d1\u6c89\u6d78\u5f0f\u865a\u62df\u73b0\u5b9e\u7cfb\u7edf\uff0c\u7ed3\u5408\u773c\u52a8\u8ffd\u8e2a\uff0c\u8fdb\u884c\u4e24\u9636\u6bb5\u5b9e\u9a8c\uff0c\u53c2\u4e0e\u8005\u8fdb\u884c\u7b80\u5355\u548c\u590d\u6742\u7269\u4f53\u7684\u679a\u4e3e\uff0c\u6839\u636e\u4efb\u52a1\u610f\u56fe\u548c\u7269\u54c1\u7a7a\u95f4\u5e03\u5c40\u8fdb\u884c\u7cfb\u7edf\u53d8\u5316\u3002", "result": "\u53c2\u4e0e\u8005\u5728\u6267\u884c\u9009\u62e9\u6027\u8ba1\u6570\u65f6\u9762\u4e34\u663e\u8457\u7684\u8ba4\u77e5\u6210\u672c\uff0c\u590d\u6742\u523a\u6fc0\u663e\u8457\u964d\u4f4e\u51c6\u786e\u6027\u548c\u8bb0\u5fc6\u53ec\u56de\uff0c\u7a7a\u95f4\u5e03\u5c40\u7684\u5f71\u54cd\u5728\u9ad8\u8ba4\u77e5\u4efb\u52a1\u4e0b\u6b21\u8981\u4e14\u7edf\u8ba1\u4e0a\u4e0d\u663e\u8457\u3002", "conclusion": "\u771f\u5b9e\u4e16\u754c\u7684\u89c6\u89c9\u679a\u4e3e\u53d7\u5230\u8bed\u4e49\u5904\u7406\u7684\u8ba4\u77e5\u8d1f\u62c5\u9650\u5236\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u89c6\u89c9\u641c\u7d22\u7684\u673a\u68b0\u8fc7\u7a0b\u3002"}}
{"id": "2510.05707", "categories": ["cs.RO", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.05707", "abs": "https://arxiv.org/abs/2510.05707", "authors": ["David Boetius", "Abdelrahman Abdelnaby", "Ashok Kumar", "Stefan Leue", "Abdalla Swikir", "Fares J. Abu-Dakka"], "title": "Stable Robot Motions on Manifolds: Learning Lyapunov-Constrained Neural Manifold ODEs", "comment": "12 pages, 6 figures", "summary": "Learning stable dynamical systems from data is crucial for safe and reliable\nrobot motion planning and control. However, extending stability guarantees to\ntrajectories defined on Riemannian manifolds poses significant challenges due\nto the manifold's geometric constraints. To address this, we propose a general\nframework for learning stable dynamical systems on Riemannian manifolds using\nneural ordinary differential equations. Our method guarantees stability by\nprojecting the neural vector field evolving on the manifold so that it strictly\nsatisfies the Lyapunov stability criterion, ensuring stability at every system\nstate. By leveraging a flexible neural parameterisation for both the base\nvector field and the Lyapunov function, our framework can accurately represent\ncomplex trajectories while respecting manifold constraints by evolving\nsolutions directly on the manifold. We provide an efficient training strategy\nfor applying our framework and demonstrate its utility by solving Riemannian\nLASA datasets on the unit quaternion (S^3) and symmetric positive-definite\nmatrix manifolds, as well as robotic motions evolving on \\mathbb{R}^3 \\times\nS^3. We demonstrate the performance, scalability, and practical applicability\nof our approach through extensive simulations and by learning robot motions in\na real-world experiment.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u5229\u7528\u795e\u7ecf\u666e\u901a\u5fae\u5206\u65b9\u7a0b\u5728\u9ece\u66fc\u6d41\u5f62\u4e0a\u5b66\u4e60\u7a33\u5b9a\u52a8\u6001\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u63a2\u7d22\u5176\u5728\u673a\u5668\u4eba\u8fd0\u52a8\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u7a33\u5b9a\u7684\u52a8\u6001\u7cfb\u7edf\u5bf9\u4e8e\u5b89\u5168\u53ef\u9760\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u548c\u63a7\u5236\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u9ece\u66fc\u6d41\u5f62\u5b9a\u4e49\u7684\u8f68\u8ff9\u4e0a\u62d3\u5c55\u7a33\u5b9a\u6027\u4fdd\u8bc1\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002", "method": "\u4f7f\u7528\u795e\u7ecf\u666e\u901a\u5fae\u5206\u65b9\u7a0b\uff0c\u786e\u4fdd\u5728\u6d41\u5f62\u4e0a\u901a\u8fc7\u6295\u5f71\u795e\u7ecf\u77e2\u91cf\u573a\u6765\u6ee1\u8db3Lyapunov\u7a33\u5b9a\u6027\u6807\u51c6\uff0c\u4ee5\u5b9e\u73b0\u5728\u6bcf\u4e2a\u7cfb\u7edf\u72b6\u6001\u4e0b\u7684\u7a33\u5b9a\u6027\u3002", "result": "\u901a\u8fc7\u5728\u5355\u4f4d\u56db\u5143\u6570\uff08S^3\uff09\u548c\u5bf9\u79f0\u6b63\u5b9a\u77e9\u9635\u6d41\u5f62\u7b49\u9ece\u66fcLASA\u6570\u636e\u96c6\u4e0a\u7684\u5e94\u7528\uff0c\u4ee5\u53ca\u5728\textbf{R}^3 \times S^3\u4e0a\u6f14\u5316\u7684\u673a\u5668\u4eba\u8fd0\u52a8\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6027\u80fd\u3001\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u795e\u7ecf\u666e\u901a\u5fae\u5206\u65b9\u7a0b\u5728\u9ece\u66fc\u6d41\u5f62\u4e0a\u5b66\u4e60\u7a33\u5b9a\u7684\u52a8\u6001\u7cfb\u7edf\uff0c\u5e76\u5728\u5b9e\u9645\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u5176\u51fa\u8272\u7684\u6027\u80fd\u548c\u9002\u7528\u6027\u3002"}}
{"id": "2510.05844", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.05844", "abs": "https://arxiv.org/abs/2510.05844", "authors": ["Lonni Besan\u00e7on"], "title": "From \"Arbitrary Timberland\" To \"Skyline Charts\": Is Visualization At Risk From The Pollution of Scientific Literature?", "comment": null, "summary": "In this essay, I argue that, while visualization research does not seem to be\ndirectly at risk of being corrupted by the current massive wave of polluted\nresearch, certain visualization concepts are being used in fraudulent fashions\nand fields close to ours are being targeted. Worse, the society publishing our\nwork is overwhelmed by thousands of questionable papers that are being,\nunfortunately, published. As a community, and if we want our research to remain\nas good as it currently is, I argue that we should all get involved with our\nvariety of skills to help identify and correct the current scientific record. I\nthus aim to present a few questionable practices that are worth knowing about\nwhen reviewing for fields using visualization research, and hopefully will\nnever be useful when reviewing for our main venues. I also argue that our skill\nset could become particularly relevant in the future and invite scholars of the\nfields to try to get involved.", "AI": {"tldr": "\u53ef\u89c6\u5316\u7814\u7a76\u9762\u4e34\u9020\u5047\u95ee\u9898\uff0c\u7814\u7a76\u8005\u9700\u5171\u540c\u53c2\u4e0e\u7ef4\u62a4\u7814\u7a76\u8d28\u91cf\u3002", "motivation": "\u8b66\u60d5\u53ef\u89c6\u5316\u9886\u57df\u5185\u7684\u9020\u5047\u73b0\u8c61\u53ca\u5176\u5bf9\u5b66\u672f\u793e\u533a\u7684\u5f71\u54cd\u3002", "method": "\u5206\u6790\u53ef\u89c6\u5316\u7814\u7a76\u4e2d\u5b58\u5728\u7684\u6b3a\u8bc8\u6027\u505a\u6cd5\uff0c\u5e76\u63d0\u4f9b\u9884\u9632\u63aa\u65bd\u3002", "result": "\u8bc6\u522b\u51e0\u79cd\u503c\u5f97\u5173\u6ce8\u7684\u53ef\u7591\u505a\u6cd5\uff0c\u5e76\u547c\u5401\u5b66\u8005\u79ef\u6781\u53c2\u4e0e\u6539\u8fdb\u3002", "conclusion": "\u7814\u7a76\u8005\u5e94\u79ef\u6781\u53c2\u4e0e\u4fee\u6b63\u79d1\u5b66\u8bb0\u5f55\uff0c\u4ee5\u7ef4\u62a4\u53ef\u89c6\u5316\u7814\u7a76\u7684\u8d28\u91cf\u3002"}}
{"id": "2510.05713", "categories": ["cs.RO", "cs.AI", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.05713", "abs": "https://arxiv.org/abs/2510.05713", "authors": ["Wanli Ni", "Hui Tian", "Shuai Wang", "Chengyang Li", "Lei Sun", "Zhaohui Yang"], "title": "Federated Split Learning for Resource-Constrained Robots in Industrial IoT: Framework Comparison, Optimization Strategies, and Future Directions", "comment": "9 pages, 5 figures, submitted to the IEEE magazine", "summary": "Federated split learning (FedSL) has emerged as a promising paradigm for\nenabling collaborative intelligence in industrial Internet of Things (IoT)\nsystems, particularly in smart factories where data privacy, communication\nefficiency, and device heterogeneity are critical concerns. In this article, we\npresent a comprehensive study of FedSL frameworks tailored for\nresource-constrained robots in industrial scenarios. We compare synchronous,\nasynchronous, hierarchical, and heterogeneous FedSL frameworks in terms of\nworkflow, scalability, adaptability, and limitations under dynamic industrial\nconditions. Furthermore, we systematically categorize token fusion strategies\ninto three paradigms: input-level (pre-fusion), intermediate-level\n(intra-fusion), and output-level (post-fusion), and summarize their respective\nstrengths in industrial applications. We also provide adaptive optimization\ntechniques to enhance the efficiency and feasibility of FedSL implementation,\nincluding model compression, split layer selection, computing frequency\nallocation, and wireless resource management. Simulation results validate the\nperformance of these frameworks under industrial detection scenarios. Finally,\nwe outline open issues and research directions of FedSL in future smart\nmanufacturing systems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8d44\u6e90\u53d7\u9650\u673a\u5668\u4eba\u5728\u5de5\u4e1a\u573a\u666f\u4e2d\u5e94\u7528\u7684\u8054\u90a6\u5206\u5272\u5b66\u4e60\uff08FedSL\uff09\u6846\u67b6\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u6846\u67b6\u7684\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u4f18\u5316\u6280\u672f\uff0c\u5e76\u603b\u7ed3\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5728\u667a\u80fd\u5de5\u5382\u7684\u5de5\u4e1a\u7269\u8054\u7f51\u7cfb\u7edf\u4e2d\uff0c\u6570\u636e\u9690\u79c1\u3001\u901a\u4fe1\u6548\u7387\u548c\u8bbe\u5907\u5f02\u6784\u6027\u662f\u5173\u952e\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u9002\u5408\u8d44\u6e90\u53d7\u9650\u673a\u5668\u4eba\u5e94\u7528\u7684FedSL\u6846\u67b6\u3002", "method": "\u5bf9\u540c\u6b65\u3001\u5f02\u6b65\u3001\u5c42\u6b21\u548c\u5f02\u6784FedSL\u6846\u67b6\u8fdb\u884c\u6bd4\u8f83\uff0c\u63a2\u8ba8\u5176\u5728\u52a8\u6001\u5de5\u4e1a\u6761\u4ef6\u4e0b\u7684\u5de5\u4f5c\u6d41\u7a0b\u3001\u53ef\u6269\u5c55\u6027\u3001\u9002\u5e94\u6027\u548c\u5c40\u9650\u6027\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u6846\u67b6\u5728\u5de5\u4e1a\u68c0\u6d4b\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002\u672c\u6587\u8fd8\u7cfb\u7edf\u5206\u7c7b\u4e86\u4e09\u79cd\u6807\u8bb0\u878d\u5408\u7b56\u7565\uff0c\u5e76\u63d0\u51fa\u4e86\u589e\u5f3aFedSL\u5b9e\u65bd\u6548\u7387\u7684\u81ea\u9002\u5e94\u4f18\u5316\u6280\u672f\u3002", "conclusion": "\u672c\u6587\u603b\u7ed3\u4e86FedSL\u5728\u5de5\u4e1a\u573a\u666f\u4e2d\u7684\u5b9e\u65bd\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u548c\u5f00\u653e\u95ee\u9898\u3002"}}
{"id": "2510.06124", "categories": ["cs.HC", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06124", "abs": "https://arxiv.org/abs/2510.06124", "authors": ["Renee Shelby", "Fernando Diaz", "Vinodkumar Prabhakaran"], "title": "Taxonomy of User Needs and Actions", "comment": null, "summary": "The growing ubiquity of conversational AI highlights the need for frameworks\nthat capture not only users' instrumental goals but also the situated,\nadaptive, and social practices through which they achieve them. Existing\ntaxonomies of conversational behavior either overgeneralize, remain\ndomain-specific, or reduce interactions to narrow dialogue functions. To\naddress this gap, we introduce the Taxonomy of User Needs and Actions (TUNA),\nan empirically grounded framework developed through iterative qualitative\nanalysis of 1193 human-AI conversations, supplemented by theoretical review and\nvalidation across diverse contexts. TUNA organizes user actions into a\nthree-level hierarchy encompassing behaviors associated with information\nseeking, synthesis, procedural guidance, content creation, social interaction,\nand meta-conversation. By centering user agency and appropriation practices,\nTUNA enables multi-scale evaluation, supports policy harmonization across\nproducts, and provides a backbone for layering domain-specific taxonomies. This\nwork contributes a systematic vocabulary for describing AI use, advancing both\nscholarly understanding and practical design of safer, more responsive, and\nmore accountable conversational systems.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86TUNA\u6846\u67b6\uff0c\u65e8\u5728\u6355\u6349\u7528\u6237\u5728\u5bf9\u8bdd\u5f0fAI\u4e2d\u7684\u9700\u6c42\u548c\u884c\u4e3a\uff0c\u4fc3\u8fdb\u66f4\u5b89\u5168\u548c\u66f4\u8d1f\u8d23\u4efb\u7684\u5bf9\u8bdd\u7cfb\u7edf\u8bbe\u8ba1\u3002", "motivation": "\u968f\u7740\u5bf9\u8bdd\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u666e\u53ca\uff0c\u9700\u8981\u4e00\u79cd\u6846\u67b6\u6765\u6355\u6349\u7528\u6237\u7684\u9002\u5e94\u6027\u548c\u793e\u4f1a\u5b9e\u8df5\uff0c\u4ee5\u8d85\u8d8a\u73b0\u6709\u7684\u5bf9\u8bdd\u884c\u4e3a\u5206\u7c7b\u3002", "method": "\u901a\u8fc7\u5bf91193\u6b21\u4eba\u673a\u5bf9\u8bdd\u7684\u5b9a\u6027\u5206\u6790\uff0c\u4ee5\u53ca\u7406\u8bba\u56de\u987e\u548c\u4e0d\u540c\u9886\u57df\u7684\u9a8c\u8bc1\uff0c\u53d1\u5c55\u51faTUNA\u6846\u67b6\u3002", "result": "TUNA\u6846\u67b6\u5c06\u7528\u6237\u884c\u4e3a\u7ec4\u7ec7\u4e3a\u4e00\u4e2a\u4e09\u5c42\u6b21\u7684\u5c42\u7ea7\uff0c\u6db5\u76d6\u4fe1\u606f\u83b7\u53d6\u3001\u5408\u6210\u3001\u7a0b\u5e8f\u6307\u5bfc\u3001\u5185\u5bb9\u521b\u4f5c\u3001\u793e\u4ea4\u4e92\u52a8\u548c\u5143\u5bf9\u8bdd\u7b49\u65b9\u9762\u3002", "conclusion": "\u63d0\u51fa\u4e86TUNA\u6846\u67b6\uff0c\u4e3a\u7406\u89e3\u548c\u8bbe\u8ba1\u66f4\u5b89\u5168\u3001\u54cd\u5e94\u6027\u66f4\u5f3a\u4e14\u66f4\u5177\u95ee\u8d23\u6027\u7684\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u8bcd\u6c47\u548c\u5206\u7c7b\u5de5\u5177\u3002"}}
{"id": "2510.05729", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.05729", "abs": "https://arxiv.org/abs/2510.05729", "authors": ["Marc Kaufeld", "Johannes Betz"], "title": "Precise and Efficient Collision Prediction under Uncertainty in Autonomous Driving", "comment": "8 pages, submitted to the IEEE ICRA 2026, Vienna, Austria", "summary": "This research introduces two efficient methods to estimate the collision risk\nof planned trajectories in autonomous driving under uncertain driving\nconditions. Deterministic collision checks of planned trajectories are often\ninaccurate or overly conservative, as noisy perception, localization errors,\nand uncertain predictions of other traffic participants introduce significant\nuncertainty into the planning process. This paper presents two semi-analytic\nmethods to compute the collision probability of planned trajectories with\narbitrary convex obstacles. The first approach evaluates the probability of\nspatial overlap between an autonomous vehicle and surrounding obstacles, while\nthe second estimates the collision probability based on stochastic boundary\ncrossings. Both formulations incorporate full state uncertainties, including\nposition, orientation, and velocity, and achieve high accuracy at computational\ncosts suitable for real-time planning. Simulation studies verify that the\nproposed methods closely match Monte Carlo results while providing significant\nruntime advantages, enabling their use in risk-aware trajectory planning. The\ncollision estimation methods are available as open-source software:\nhttps://github.com/TUM-AVS/Collision-Probability-Estimation", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e24\u79cd\u534a\u89e3\u6790\u65b9\u6cd5\uff0c\u4ee5\u8ba1\u7b97\u81ea\u4e3b\u8f66\u8f86\u5728\u4e0d\u786e\u5b9a\u6761\u4ef6\u4e0b\u4e0e\u5468\u56f4\u969c\u788d\u7269\u76f8\u78b0\u649e\u7684\u6982\u7387\uff0c\u540c\u65f6\u517c\u987e\u5b9e\u65f6\u8ba1\u7b97\u4e4b\u95f4\u7684\u5e73\u8861\u3002", "motivation": "\u7531\u4e8e\u566a\u58f0\u611f\u77e5\u3001\u5b9a\u4f4d\u8bef\u5dee\u548c\u5176\u4ed6\u4ea4\u901a\u53c2\u4e0e\u8005\u7684\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u73b0\u6709\u7684\u786e\u5b9a\u6027\u78b0\u649e\u68c0\u67e5\u5e38\u5e38\u4e0d\u591f\u51c6\u786e\u6216\u8fc7\u4e8e\u4fdd\u5b88\uff0c\u4ee5\u81f3\u4e8e\u9700\u8981\u66f4\u597d\u7684\u98ce\u9669\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4e24\u79cd\u534a\u89e3\u6790\u65b9\u6cd5\u8ba1\u7b97\u4e0e\u4efb\u610f\u51f8\u969c\u788d\u7269\u7684\u78b0\u649e\u6982\u7387\uff0c\u5206\u522b\u901a\u8fc7\u7a7a\u95f4\u91cd\u53e0\u7684\u6982\u7387\u548c\u968f\u673a\u8fb9\u754c\u7a7f\u8d8a\u7684\u6982\u7387\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u5b8c\u6574\u8003\u8651\u4e86\u4f4d\u7f6e\u3001\u65b9\u5411\u548c\u901f\u5ea6\u7b49\u72b6\u6001\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e24\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u4f30\u8ba1\u81ea\u4e3b\u9a7e\u9a76\u4e2d\u5728\u4e0d\u786e\u5b9a\u9a7e\u9a76\u6761\u4ef6\u4e0b\u8ba1\u5212\u8f68\u8ff9\u7684\u78b0\u649e\u98ce\u9669\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u4ee5\u9ad8\u7cbe\u5ea6\u8ba1\u7b97\u78b0\u649e\u6982\u7387\uff0c\u5e76\u5927\u5e45\u5ea6\u964d\u4f4e\u8ba1\u7b97\u65f6\u95f4\uff0c\u9002\u7528\u4e8e\u98ce\u9669\u611f\u77e5\u8f68\u8ff9\u89c4\u5212\u3002"}}
{"id": "2510.06156", "categories": ["cs.HC", "H.5.2"], "pdf": "https://arxiv.org/pdf/2510.06156", "abs": "https://arxiv.org/abs/2510.06156", "authors": ["Guillaume Rivi\u00e8re"], "title": "Observing Interaction Rather Than Interfaces", "comment": "Written in French. 8 pages. Approximately 5000 words. No figures and\n  no tables", "summary": "The science of Human-Computer Interaction (HCI) is populated by isolated\nempirical findings, often tied to specific technologies, designs, and tasks.\nThis situation probably lies in observing the wrong object of study, that is to\nsay, observing interfaces rather than interaction. This paper proposes an\nexperimental methodology, powered by a research methodology, that enables\ntackling the ambition of observing interaction (rather than interfaces). These\nobservations are done during the treatment of applicative cases, allowing to\ngenerate and replicate results covering various experimental conditions,\nexpressed from the need of end users and the evolution of technologies.\nPerforming these observations when developing applicative prototypes\nillustrating novel technologies' utility allows, in the same time, to benefit\nfrom an optimization of these prototypes to better accomplish end users tasks.\nThis paper depicts a long term research direction, from generating the initial\nobservations of interaction properties and their replication, to their\nintegration, that would then lead to exploring the possible relations existing\nbetween those properties, to end toward the description of human-computer\ninteraction's physics.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b9e\u9a8c\u65b9\u6cd5\uff0c\u65e8\u5728\u89c2\u5bdf\u548c\u7814\u7a76\u4eba\u673a\u4ea4\u4e92\uff0c\u800c\u975e\u4ec5\u9650\u4e8e\u754c\u9762\uff0c\u63a8\u52a8HCI\u79d1\u5b66\u7684\u53d1\u5c55\u5e76\u4f18\u5316\u539f\u578b\u8bbe\u8ba1\u3002", "motivation": "\u5f53\u524dHCI\u9886\u57df\u5b58\u5728\u5b64\u7acb\u7684\u7ecf\u9a8c\u53d1\u73b0\uff0c\u4e3b\u8981\u96c6\u4e2d\u4e8e\u7279\u5b9a\u7684\u6280\u672f\u548c\u4efb\u52a1\uff0c\u56e0\u800c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u66f4\u597d\u5730\u7814\u7a76\u4eba\u673a\u4ea4\u4e92\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u9a8c\u65b9\u6cd5\u8bba\uff0c\u901a\u8fc7\u5e94\u7528\u6848\u4f8b\u7684\u6cbb\u7597\u8fdb\u884c\u89c2\u5bdf\uff0c\u4ee5\u4ea7\u751f\u548c\u590d\u5236\u8986\u76d6\u5404\u79cd\u5b9e\u9a8c\u6761\u4ef6\u7684\u7ed3\u679c\u3002", "result": "\u901a\u8fc7\u5728\u5f00\u53d1\u5e94\u7528\u539f\u578b\u65f6\u8fdb\u884c\u89c2\u5bdf\uff0c\u4f18\u5316\u8fd9\u4e9b\u539f\u578b\u4ee5\u66f4\u597d\u5730\u5b8c\u6210\u6700\u7ec8\u7528\u6237\u4efb\u52a1\uff0c\u6700\u7ec8\u76ee\u6807\u662f\u63cf\u8ff0\u4eba\u673a\u4ea4\u4e92\u7684\u7269\u7406\u7279\u6027\u3002", "conclusion": "\u672c\u8bba\u6587\u5c55\u793a\u4e86\u4e00\u79cd\u65b0\u7684\u5b9e\u9a8c\u65b9\u6cd5\u8bba\uff0c\u65e8\u5728\u901a\u8fc7\u89c2\u5bdf\u4eba\u673a\u4ea4\u4e92\u800c\u975e\u4ec5\u4ec5\u89c2\u5bdf\u754c\u9762\uff0c\u6765\u63a8\u52a8HCI\u79d1\u5b66\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.05780", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.05780", "abs": "https://arxiv.org/abs/2510.05780", "authors": ["Andreas Christou", "Andreas Sochopoulos", "Elliot Lister", "Sethu Vijayakumar"], "title": "Human-in-the-loop Optimisation in Robot-assisted Gait Training", "comment": null, "summary": "Wearable robots offer a promising solution for quantitatively monitoring gait\nand providing systematic, adaptive assistance to promote patient independence\nand improve gait. However, due to significant interpersonal and intrapersonal\nvariability in walking patterns, it is important to design robot controllers\nthat can adapt to the unique characteristics of each individual. This paper\ninvestigates the potential of human-in-the-loop optimisation (HILO) to deliver\npersonalised assistance in gait training. The Covariance Matrix Adaptation\nEvolution Strategy (CMA-ES) was employed to continuously optimise an\nassist-as-needed controller of a lower-limb exoskeleton. Six healthy\nindividuals participated over a two-day experiment. Our results suggest that\nwhile the CMA-ES appears to converge to a unique set of stiffnesses for each\nindividual, no measurable impact on the subjects' performance was observed\nduring the validation trials. These findings highlight the impact of\nhuman-robot co-adaptation and human behaviour variability, whose effect may be\ngreater than potential benefits of personalising rule-based assistive\ncontrollers. Our work contributes to understanding the limitations of current\npersonalisation approaches in exoskeleton-assisted gait rehabilitation and\nidentifies key challenges for effective implementation of human-in-the-loop\noptimisation in this domain.", "AI": {"tldr": "\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u53ef\u4ee5\u91cf\u5316\u76d1\u6d4b\u6b65\u6001\u5e76\u63d0\u4f9b\u9002\u5e94\u6027\u8f85\u52a9\uff0c\u4f46\u7531\u4e8e\u6b65\u6001\u7684\u4e2a\u4f53\u5dee\u5f02\uff0c\u8bbe\u8ba1\u80fd\u9002\u5e94\u4e2a\u4f53\u7279\u5f81\u7684\u63a7\u5236\u5668\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u4f7f\u7528\u4eba\u673a\u534f\u540c\u4f18\u5316\uff0c\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u63d0\u5347\u4e2a\u6027\u5316\u8f85\u52a9\uff0c\u4f46\u6ca1\u6709\u89c2\u5bdf\u5230\u5bf9\u53c2\u4e0e\u8005\u8868\u73b0\u7684\u663e\u8457\u5f71\u54cd\uff0c\u8fd9\u7a81\u663e\u4e86\u4eba\u673a\u5171\u9002\u5e94\u7684\u91cd\u8981\u6027\u4ee5\u53ca\u5f53\u524d\u4e2a\u6027\u5316\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u7528\u4e8e\u6b65\u6001\u76d1\u6d4b\u548c\u72ec\u7acb\u6027\u63d0\u9ad8\uff0c\u4f46\u6b65\u6001\u6a21\u5f0f\u7684\u663e\u8457\u4e2a\u4f53\u5dee\u5f02\u8981\u6c42\u8bbe\u8ba1\u80fd\u591f\u9002\u5e94\u6bcf\u4e2a\u4e2a\u4f53\u7279\u5f81\u7684\u673a\u5668\u4eba\u63a7\u5236\u5668\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u4e86\u534f\u65b9\u5dee\u77e9\u9635\u9002\u5e94\u8fdb\u5316\u7b56\u7565\uff08CMA-ES\uff09\u6765\u9488\u5bf9\u4e0b\u80a2\u5916\u9aa8\u9abc\u7684\u52a9\u529b\u63a7\u5236\u5668\u8fdb\u884c\u6301\u7eed\u7684\u4f18\u5316\uff0c\u5e76\u5728\u4e00\u4e2a\u4e3a\u671f\u4e24\u5929\u7684\u5b9e\u9a8c\u4e2d\u5bf9\u516d\u540d\u5065\u5eb7\u4e2a\u4f53\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "result": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u5728\u6b65\u6001\u8bad\u7ec3\u4e2d\u7684\u5e94\u7528\uff0c\u5c24\u5176\u662f\u5982\u4f55\u901a\u8fc7\u4eba\u673a\u534f\u540c\u4f18\u5316\u63d0\u4f9b\u4e2a\u6027\u5316\u7684\u5e2e\u52a9\u3002\u7814\u7a76\u8868\u660e\uff0c\u5c3d\u7ba1CMA-ES\u7b97\u6cd5\u80fd\u591f\u4e3a\u6bcf\u4e2a\u53c2\u4e0e\u8005\u4f18\u5316\u51fa\u72ec\u7279\u7684\u521a\u5ea6\u8bbe\u7f6e\uff0c\u4f46\u5728\u9a8c\u8bc1\u8bd5\u9a8c\u4e2d\u5e76\u672a\u5bf9\u53c2\u4e0e\u8005\u7684\u8868\u73b0\u4ea7\u751f\u660e\u663e\u5f71\u54cd\u3002\u8fd9\u8868\u660e\u4eba\u673a\u534f\u540c\u9002\u5e94\u548c\u4eba\u7c7b\u884c\u4e3a\u7684\u53d8\u5f02\u6027\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\u53ef\u80fd\u8d85\u8fc7\u4e86\u4e2a\u6027\u5316\u52a9\u529b\u63a7\u5236\u5668\u7684\u6f5c\u5728\u597d\u5904\uff0c\u56e0\u6b64\uff0c\u5bf9\u4e8e\u73b0\u6709\u4e2a\u6027\u5316\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u5e76\u786e\u5b9a\u4e86\u5728\u4eba\u673a\u534f\u540c\u4f18\u5316\u5b9e\u65bd\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002", "conclusion": "\u4eba\u673a\u534f\u540c\u9002\u5e94\u4e0e\u4eba\u7c7b\u884c\u4e3a\u53d8\u5f02\u6027\u53ef\u80fd\u5bf9\u6b65\u6001\u8bad\u7ec3\u7684\u6548\u679c\u5f71\u54cd\u663e\u8457\uff0c\u8d85\u51fa\u4e2a\u6027\u5316\u52a9\u529b\u63a7\u5236\u5668\u7684\u6f5c\u5728\u597d\u5904\u3002"}}
{"id": "2510.05827", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05827", "abs": "https://arxiv.org/abs/2510.05827", "authors": ["Haoran Zhang", "Shuanghao Bai", "Wanqi Zhou", "Yuedi Zhang", "Qi Zhang", "Pengxiang Ding", "Cheng Chi", "Donglin Wang", "Badong Chen"], "title": "VCoT-Grasp: Grasp Foundation Models with Visual Chain-of-Thought Reasoning for Language-driven Grasp Generation", "comment": null, "summary": "Robotic grasping is one of the most fundamental tasks in robotic\nmanipulation, and grasp detection/generation has long been the subject of\nextensive research. Recently, language-driven grasp generation has emerged as a\npromising direction due to its practical interaction capabilities. However,\nmost existing approaches either lack sufficient reasoning and generalization\ncapabilities or depend on complex modular pipelines. Moreover, current grasp\nfoundation models tend to overemphasize dialog and object semantics, resulting\nin inferior performance and restriction to single-object grasping. To maintain\nstrong reasoning ability and generalization in cluttered environments, we\npropose VCoT-Grasp, an end-to-end grasp foundation model that incorporates\nvisual chain-of-thought reasoning to enhance visual understanding for grasp\ngeneration. VCoT-Grasp adopts a multi-turn processing paradigm that dynamically\nfocuses on visual inputs while providing interpretable reasoning traces. For\ntraining, we refine and introduce a large-scale dataset, VCoT-GraspSet,\ncomprising 167K synthetic images with over 1.36M grasps, as well as 400+\nreal-world images with more than 1.2K grasps, annotated with intermediate\nbounding boxes. Extensive experiments on both VCoT-GraspSet and real robot\ndemonstrate that our method significantly improves grasp success rates and\ngeneralizes effectively to unseen objects, backgrounds, and distractors. More\ndetails can be found at https://zhanghr2001.github.io/VCoT-Grasp.github.io.", "AI": {"tldr": "VCoT-Grasp\u662f\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u63a8\u7406\u7684\u7aef\u5230\u7aef\u6293\u53d6\u6a21\u578b\uff0c\u80fd\u5728\u590d\u6742\u73af\u5883\u4e2d\u6709\u6548\u63d0\u5347\u6293\u53d6\u6210\u529f\u7387\u4e0e\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u7684\u6293\u53d6\u57fa\u7840\u6a21\u578b\u5728\u5bf9\u8bdd\u548c\u7269\u4f53\u8bed\u4e49\u4e0a\u8fc7\u5206\u5f3a\u8c03\uff0c\u5bfc\u81f4\u5355\u4e00\u7269\u4f53\u6293\u53d6\u7684\u6027\u80fd\u4e0d\u4f73\uff0c\u5e76\u4e14\u7f3a\u4e4f\u826f\u597d\u7684\u63a8\u7406\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u6293\u53d6\u57fa\u7840\u6a21\u578bVCoT-Grasp\uff0c\u7ed3\u5408\u4e86\u89c6\u89c9\u94fe\u601d\u7ef4\u63a8\u7406\u4ee5\u589e\u5f3a\u6293\u53d6\u751f\u6210\u7684\u89c6\u89c9\u7406\u89e3\u3002", "result": "VCoT-Grasp\u5728VCoT-GraspSet\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6293\u53d6\u6210\u529f\u7387\u3002", "conclusion": "VCoT-Grasp\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u4e86\u6293\u53d6\u6210\u529f\u7387\uff0c\u5e76\u80fd\u6709\u6548\u5730\u9002\u5e94\u672a\u89c1\u8fc7\u7684\u5bf9\u8c61\u3001\u80cc\u666f\u548c\u5e72\u6270\u7269\u3002"}}
{"id": "2510.05923", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.05923", "abs": "https://arxiv.org/abs/2510.05923", "authors": ["Aman Singh", "Aastha Mishra", "Deepak Kapa", "Suryank Joshi", "Shishir Kolathaya"], "title": "A Co-Design Framework for Energy-Aware Monoped Jumping with Detailed Actuator Modeling", "comment": "7 pages, 8 figures, 1 table, Accepted at IEEE-RAS 24th International\n  Conference on Humanoid Robots (Humanoids) 2025, Aman Singh, Aastha Mishra -\n  Authors contributed equally", "summary": "A monoped's jump height and energy consumption depend on both, its mechanical\ndesign and control strategy. Existing co-design frameworks typically optimize\nfor either maximum height or minimum energy, neglecting their trade-off. They\nalso often omit gearbox parameter optimization and use oversimplified actuator\nmass models, producing designs difficult to replicate in practice. In this\nwork, we introduce a novel three-stage co-design optimization framework that\njointly maximizes jump height while minimizing mechanical energy consumption of\na monoped. The proposed method explicitly incorporates realistic actuator mass\nmodels and optimizes mechanical design (including gearbox) and control\nparameters within a unified framework. The resulting design outputs are then\nused to automatically generate a parameterized CAD model suitable for direct\nfabrication, significantly reducing manual design iterations. Our experimental\nevaluations demonstrate a 50 percent reduction in mechanical energy consumption\ncompared to the baseline design, while achieving a jump height of 0.8m. Video\npresentation is available at http://y2u.be/XW8IFRCcPgM", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u4e09\u9636\u6bb5\u5171\u8bbe\u8ba1\u4f18\u5316\u6846\u67b6\uff0c\u6700\u5927\u5316\u5355\u811a\u8df3\u8dc3\u9ad8\u5ea6\u5e76\u964d\u4f4e\u80fd\u8017\uff0c\u51cf\u5c11\u8bbe\u8ba1\u8fed\u4ee3\u3002", "motivation": "\u73b0\u6709\u7684\u5171\u8bbe\u8ba1\u6846\u67b6\u591a\u5173\u6ce8\u4e8e\u6700\u5927\u5316\u9ad8\u5ea6\u6216\u6700\u5c0f\u5316\u80fd\u91cf\uff0c\u800c\u5ffd\u7565\u4e86\u4e24\u8005\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5bfc\u81f4\u8bbe\u8ba1\u96be\u4ee5\u590d\u73b0\u3002", "method": "\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u673a\u68b0\u8bbe\u8ba1\u3001\u63a7\u5236\u53c2\u6570\u4f18\u5316\u5e76\u5f15\u5165\u4e86\u73b0\u5b9e\u7684\u9a71\u52a8\u5668\u8d28\u91cf\u6a21\u578b\uff0c\u5f62\u6210\u4e86\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u6d41\u7a0b\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u4e09\u9636\u6bb5\u5171\u8bbe\u8ba1\u4f18\u5316\u6846\u67b6\uff0c\u65e8\u5728\u540c\u65f6\u6700\u5927\u5316\u5355\u811a\u8df3\u8dc3\u9ad8\u5ea6\u5e76\u6700\u5c0f\u5316\u673a\u68b0\u80fd\u8017\u3002\u901a\u8fc7\u5f15\u5165\u73b0\u5b9e\u7684\u9a71\u52a8\u5668\u8d28\u91cf\u6a21\u578b\uff0c\u8be5\u65b9\u6cd5\u5728\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u5185\u4f18\u5316\u673a\u68b0\u8bbe\u8ba1\uff08\u5305\u62ec\u51cf\u901f\u7bb1\uff09\u548c\u63a7\u5236\u53c2\u6570\u3002\u7ed3\u679c\u8bbe\u8ba1\u8f93\u51fa\u53ef\u4ee5\u81ea\u52a8\u751f\u6210\u9002\u5408\u76f4\u63a5\u5236\u9020\u7684\u53c2\u6570\u5316CAD\u6a21\u578b\uff0c\u663e\u8457\u51cf\u5c11\u624b\u52a8\u8bbe\u8ba1\u8fed\u4ee3\u3002\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u8bbe\u8ba1\u76f8\u6bd4\uff0c\u673a\u68b0\u80fd\u8017\u51cf\u5c11\u4e8650%\uff0c\u540c\u65f6\u8fbe\u52300.8\u7c73\u7684\u8df3\u8dc3\u9ad8\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u8df3\u8dc3\u6027\u80fd\u5e76\u51cf\u5c11\u4e86\u80fd\u8017\uff0c\u4e3a\u5355\u811a\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u8def\u5f84\u3002"}}
{"id": "2510.05957", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.05957", "abs": "https://arxiv.org/abs/2510.05957", "authors": ["Vaughn Gzenda", "Robin Chhabra"], "title": "Learning to Crawl: Latent Model-Based Reinforcement Learning for Soft Robotic Adaptive Locomotion", "comment": null, "summary": "Soft robotic crawlers are mobile robots that utilize soft body deformability\nand compliance to achieve locomotion through surface contact. Designing control\nstrategies for such systems is challenging due to model inaccuracies, sensor\nnoise, and the need to discover locomotor gaits. In this work, we present a\nmodel-based reinforcement learning (MB-RL) framework in which latent dynamics\ninferred from onboard sensors serve as a predictive model that guides an\nactor-critic algorithm to optimize locomotor policies. We evaluate the\nframework on a minimal crawler model in simulation using inertial measurement\nunits and time-of-flight sensors as observations. The learned latent dynamics\nenable short-horizon motion prediction while the actor-critic discovers\neffective locomotor policies. This approach highlights the potential of\nlatent-dynamics MB-RL for enabling embodied soft robotic adaptive locomotion\nbased solely on noisy sensor feedback.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u6f5c\u5728\u52a8\u6001\u4f18\u5316\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u7b56\u7565\uff0c\u4e14\u5728\u6a21\u62df\u4e2d\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u7531\u4e8e\u6a21\u578b\u4e0d\u51c6\u786e\u3001\u4f20\u611f\u5668\u566a\u58f0\u548c\u8fd0\u52a8\u6a21\u5f0f\u7684\u53d1\u73b0\uff0c\u63a7\u5236\u8f6f\u4f53\u673a\u5668\u4eba\u722c\u884c\u5668\u7684\u7b56\u7565\u8bbe\u8ba1\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u6539\u8fdb\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4f20\u611f\u5668\u83b7\u53d6\u7684\u9690\u542b\u52a8\u6001\u6765\u9884\u6d4b\u8fd0\u52a8\uff0c\u5e76\u901a\u8fc7\u52a8\u4f5c-\u8bc4\u8bba\u8005\u7b97\u6cd5\u4f18\u5316\u8fd0\u52a8\u7b56\u7565\u3002", "result": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u4f20\u611f\u5668\u83b7\u53d6\u7684\u6f5c\u5728\u52a8\u6001\u4fe1\u606f\u6765\u6307\u5bfc\u52a8\u4f5c-\u8bc4\u8bba\u8005\u7b97\u6cd5\u4f18\u5316\u8f6f\u4f53\u722c\u884c\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u7b56\u7565\u3002", "conclusion": "\u6f5c\u5728\u52a8\u6001\u7684\u6a21\u578b\u57fa\u7840\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u5728\u566a\u58f0\u4f20\u611f\u5668\u53cd\u9988\u4e0b\u7684\u81ea\u9002\u5e94\u8fd0\u52a8\uff0c\u663e\u793a\u4e86\u8f6f\u4f53\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u8fd0\u52a8\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.05981", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.05981", "abs": "https://arxiv.org/abs/2510.05981", "authors": ["Cristina Luna", "Alba Guerra", "Almudena Moreno", "Manuel Esquer", "Willy Roa", "Mateusz Krawczak", "Robert Popela", "Piotr Osica", "Davide Nicolis"], "title": "The DISTANT Design for Remote Transmission and Steering Systems for Planetary Robotics", "comment": "Paper for 18th Symposium on Advanced Space Technologies in Robotics\n  and Automation (ASTRA), presented on October 7th at Leiden, Netherlands", "summary": "Planetary exploration missions require robust locomotion systems capable of\noperating in extreme environments over extended periods. This paper presents\nthe DISTANT (Distant Transmission and Steering Systems) design, a novel\napproach for relocating rover traction and steering actuators from\nwheel-mounted positions to a thermally protected warm box within the rover\nbody. The design addresses critical challenges in long-distance traversal\nmissions by protecting sensitive components from thermal cycling, dust\ncontamination, and mechanical wear. A double wishbone suspension configuration\nwith cardan joints and capstan drive steering has been selected as the optimal\narchitecture following comprehensive trade-off analysis. The system enables\nindependent wheel traction, steering control, and suspension management whilst\nmaintaining all motorisation within the protected environment. The design meets\na 50 km traverse requirement without performance degradation, with integrated\ndust protection mechanisms and thermal management solutions. Testing and\nvalidation activities are planned for Q1 2026 following breadboard\nmanufacturing at 1:3 scale.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDISTANT\u7684\u65b0\u578b\u63a2\u6d4b\u8f66\u9a71\u52a8\u4e0e\u8f6c\u5411\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u65e8\u5728\u63d0\u5347\u63a2\u6d4b\u8f66\u5728\u6781\u7aef\u73af\u5883\u4e2d\u7684\u884c\u9a76\u80fd\u529b\u3002", "motivation": "\u5728\u6781\u7aef\u73af\u5883\u548c\u957f\u8ddd\u79bb\u63a2\u6d4b\u4efb\u52a1\u4e2d\uff0c\u4fdd\u62a4\u654f\u611f\u7ec4\u4ef6\u514d\u53d7\u70ed\u5faa\u73af\u3001\u5c18\u57c3\u6c61\u67d3\u548c\u673a\u68b0\u78e8\u635f\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u53cc\u53c9\u81c2\u60ac\u6302\u914d\u7f6e\uff0c\u7ed3\u5408\u4e07\u5411\u8282\u4e0e\u5377\u626c\u9a71\u52a8\u8f6c\u5411\u7cfb\u7edf\uff0c\u8fdb\u884c\u5168\u9762\u7684\u6743\u8861\u5206\u6790\u4ee5\u786e\u5b9a\u6700\u4f73\u67b6\u6784\u3002", "result": "\u8bbe\u8ba1\u80fd\u72ec\u7acb\u63a7\u5236\u8f66\u8f6e\u7275\u5f15\u3001\u8f6c\u5411\u548c\u60ac\u6302\uff0c\u786e\u4fdd\u6240\u6709\u7535\u673a\u5316\u7ec4\u4ef6\u4f4d\u4e8e\u53d7\u4fdd\u62a4\u7684\u73af\u5883\u5185\uff0c\u5e76\u8ba1\u5212\u57282026\u5e74Q1\u8fdb\u884c1:3\u6bd4\u4f8b\u7684\u9762\u5305\u677f\u5236\u9020\u6d4b\u8bd5\u4e0e\u9a8c\u8bc1\u3002", "conclusion": "DISTANT\u8bbe\u8ba1\u5177\u5907\u572850\u516c\u91cc\u7684\u8fdc\u7a0b\u7a7f\u8d8a\u4efb\u52a1\u4e2d\u7ef4\u6301\u6027\u80fd\u7684\u80fd\u529b\uff0c\u5e76\u5177\u5907\u9632\u5c18\u548c\u70ed\u7ba1\u7406\u65b9\u6848\uff0c\u9002\u5408\u957f\u671f\u63a2\u6d4b\u4efb\u52a1\u3002"}}
{"id": "2510.05985", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.05985", "abs": "https://arxiv.org/abs/2510.05985", "authors": ["Cristina Luna", "Robert Field", "Steven Kay"], "title": "AI-Enabled Capabilities to Facilitate Next-Generation Rover Surface Operations", "comment": "Paper for 18th Symposium on Advanced Space Technologies in Robotics\n  and Automation (ASTRA), presented on October 7th at Leiden, Netherlands", "summary": "Current planetary rovers operate at traverse speeds of approximately 10 cm/s,\nfundamentally limiting exploration efficiency. This work presents integrated AI\nsystems which significantly improve autonomy through three components: (i) the\nFASTNAV Far Obstacle Detector (FOD), capable of facilitating sustained 1.0 m/s\nspeeds via computer vision-based obstacle detection; (ii) CISRU, a multi-robot\ncoordination framework enabling human-robot collaboration for in-situ resource\nutilisation; and (iii) the ViBEKO and AIAXR deep learning-based terrain\nclassification studies. Field validation in Mars analogue environments\ndemonstrated these systems at Technology Readiness Level 4, providing\nmeasurable improvements in traverse speed, classification accuracy, and\noperational safety for next-generation planetary missions.", "AI": {"tldr": "\u672c\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u79cdAI\u6280\u672f\u7684\u7cfb\u7edf\uff0c\u63d0\u5347\u884c\u661f\u63a2\u6d4b\u5668\u7684\u63a2\u7d22\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u5f53\u524d\u884c\u661f\u63a2\u6d4b\u5668\u7684\u4f4e\u884c\u9a76\u901f\u5ea6\u9650\u5236\u4e86\u63a2\u7d22\u6548\u7387\uff0c\u56e0\u6b64\u63d0\u51fa\u6539\u8fdb\u65b9\u6848\u4ee5\u63d0\u9ad8\u5176\u81ea\u4e3b\u5bfc\u822a\u548c\u64cd\u4f5c\u80fd\u529b\u3002", "method": "\u96c6\u6210\u4e09\u79cd\u7ec4\u4ef6\uff0c\u5305\u62ec\u8fdc\u7a0b\u969c\u788d\u7269\u68c0\u6d4b\u3001\u591a\u4eba\u673a\u5668\u4eba\u534f\u4f5c\u6846\u67b6\u4ee5\u53ca\u6df1\u5ea6\u5b66\u4e60\u5730\u5f62\u5206\u7c7b\u7814\u7a76\u3002", "result": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u96c6\u6210AI\u7cfb\u7edf\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u884c\u661f\u63a2\u6d4b\u5668\u7684\u81ea\u4e3b\u6027\u548c\u6548\u7387\uff0c\u7a81\u7834\u5f53\u524d\u7ea610 cm/s\u7684\u901f\u5ea6\u9650\u5236\u3002", "conclusion": "\u7ecf\u8fc7\u5728\u706b\u661f\u7c7b\u73af\u5883\u4e2d\u7684\u73b0\u573a\u9a8c\u8bc1\uff0c\u8be5\u7cfb\u7edf\u5c55\u793a\u4e86\u53ef\u884c\u6027\uff0c\u5e76\u5728\u884c\u8fdb\u901f\u5ea6\u3001\u5206\u7c7b\u7cbe\u5ea6\u548c\u64cd\u4f5c\u5b89\u5168\u6027\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2510.05992", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.05992", "abs": "https://arxiv.org/abs/2510.05992", "authors": ["Tien-Dat Nguyen", "Thien-Minh Nguyen", "Vinh-Hao Nguyen"], "title": "Coordinate-Consistent Localization via Continuous-Time Calibration and Fusion of UWB and SLAM Observations", "comment": null, "summary": "Onboard simultaneous localization and mapping (SLAM) methods are commonly\nused to provide accurate localization information for autonomous robots.\nHowever, the coordinate origin of SLAM estimate often resets for each run. On\nthe other hand, UWB-based localization with fixed anchors can ensure a\nconsistent coordinate reference across sessions; however, it requires an\naccurate assignment of the anchor nodes' coordinates. To this end, we propose a\ntwo-stage approach that calibrates and fuses UWB data and SLAM data to achieve\ncoordinate-wise consistent and accurate localization in the same environment.\nIn the first stage, we solve a continuous-time batch optimization problem by\nusing the range and odometry data from one full run, incorporating height\npriors and anchor-to-anchor distance factors to recover the anchors' 3D\npositions. For the subsequent runs in the second stage, a sliding-window\noptimization scheme fuses the UWB and SLAM data, which facilitates accurate\nlocalization in the same coordinate system. Experiments are carried out on the\nNTU VIRAL dataset with six scenarios of UAV flight, and we show that\ncalibration using data in one run is sufficient to enable accurate localization\nin the remaining runs. We release our source code to benefit the community at\nhttps://github.com/ntdathp/slam-uwb-calibration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u901a\u8fc7\u878d\u5408UWB\u548cSLAM\u6570\u636e\uff0c\u5b9e\u73b0\u4e00\u81f4\u548c\u51c6\u786e\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u5b9a\u4f4d\u3002", "motivation": "\u89e3\u51b3SLAM\u6bcf\u6b21\u8fd0\u884c\u5750\u6807\u539f\u70b9\u91cd\u7f6e\u7684\u95ee\u9898\uff0c\u540c\u65f6\u7ed3\u5408UWB\u63d0\u4f9b\u7684\u56fa\u5b9a\u951a\u70b9\u4ee5\u786e\u4fdd\u8de8\u4f1a\u8bdd\u7684\u4e00\u81f4\u5750\u6807\u53c2\u8003\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\u7684\u65b9\u6cd5\uff0c\u6821\u51c6\u5e76\u878d\u5408UWB\u6570\u636e\u548cSLAM\u6570\u636e\uff0c\u4ee5\u5b9e\u73b0\u4e00\u81f4\u4e14\u51c6\u786e\u7684\u5b9a\u4f4d\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5728NTU VIRAL\u6570\u636e\u96c6\u7684\u516d\u79cd\u65e0\u4eba\u673a\u98de\u884c\u573a\u666f\u4e2d\uff0c\u4ec5\u4f7f\u7528\u4e00\u6b21\u8fd0\u884c\u7684\u6570\u636e\u8fdb\u884c\u6821\u51c6\uff0c\u5728\u540e\u7eed\u8fd0\u884c\u4e2d\u5b9e\u73b0\u4e86\u51c6\u786e\u7684\u5b9a\u4f4d\u3002", "conclusion": "\u6821\u51c6\u4e00\u6b21\u8fd0\u884c\u6570\u636e\u8db3\u4ee5\u652f\u6301\u540e\u7eed\u8fd0\u884c\u4e2d\u7684\u7cbe\u786e\u5b9a\u4f4d\uff0c\u4e14\u6e90\u4ee3\u7801\u5df2\u516c\u5f00\uff0c\u4ee5\u4fc3\u8fdb\u793e\u533a\u53d1\u5c55\u3002"}}
{"id": "2510.06068", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06068", "abs": "https://arxiv.org/abs/2510.06068", "authors": ["Heng Zhang", "Kevin Yuchen Ma", "Mike Zheng Shou", "Weisi Lin", "Yan Wu"], "title": "Cross-Embodiment Dexterous Hand Articulation Generation via Morphology-Aware Learning", "comment": null, "summary": "Dexterous grasping with multi-fingered hands remains challenging due to\nhigh-dimensional articulations and the cost of optimization-based pipelines.\nExisting end-to-end methods require training on large-scale datasets for\nspecific hands, limiting their ability to generalize across different\nembodiments. We propose an eigengrasp-based, end-to-end framework for\ncross-embodiment grasp generation. From a hand's morphology description, we\nderive a morphology embedding and an eigengrasp set. Conditioned on these,\ntogether with the object point cloud and wrist pose, an amplitude predictor\nregresses articulation coefficients in a low-dimensional space, which are\ndecoded into full joint articulations. Articulation learning is supervised with\na Kinematic-Aware Articulation Loss (KAL) that emphasizes fingertip-relevant\nmotions and injects morphology-specific structure. In simulation on unseen\nobjects across three dexterous hands, our model attains a 91.9% average grasp\nsuccess rate with less than 0.4 seconds inference per grasp. With few-shot\nadaptation to an unseen hand, it achieves 85.6% success on unseen objects in\nsimulation, and real-world experiments on this few-shot generalized hand\nachieve an 87% success rate. The code and additional materials will be made\navailable upon publication on our project website\nhttps://connor-zh.github.io/cross_embodiment_dexterous_grasping.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u672c\u5f81\u6293\u63e1\u751f\u6210\u9002\u7528\u4e8e\u4e0d\u540c\u624b\u578b\u7684\u6293\u53d6\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u4ee5\u5f80\u65b9\u6cd5\u4e2d\u5bf9\u5927\u578b\u6570\u636e\u96c6\u7684\u4f9d\u8d56\u548c\u7279\u5b9a\u624b\u578b\u7684\u9650\u5236\u3002", "motivation": "\u89e3\u51b3\u591a\u6307\u6293\u63e1\u4e2d\u7684\u9ad8\u7ef4\u5173\u8282\u4f18\u5316\u95ee\u9898\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u4e0d\u4f9d\u8d56\u4e8e\u7279\u5b9a\u624b\u578b\u4e14\u80fd\u591f\u5728\u4e0d\u540c\u624b\u578b\u95f4\u6cdb\u5316\u7684\u6293\u53d6\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u8003\u8651\u624b\u7684\u5f62\u6001\u63cf\u8ff0\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u901a\u8fc7\u672c\u5f81\u6293\u63e1\u96c6\u4e0e\u7269\u4f53\u70b9\u4e91\u548c\u624b\u8155\u59ff\u6001\u7ed3\u5408\uff0c\u56de\u5f52\u4f4e\u7ef4\u7a7a\u95f4\u7684\u5173\u8282\u7cfb\u6570\uff0c\u6700\u7ec8\u89e3\u7801\u4e3a\u5168\u5173\u8282\u7684\u8fd0\u52a8\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u672c\u5f81\u6293\u63e1\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7528\u4e8e\u8de8\u8eab\u4f53\u73b0\u7684\u6293\u53d6\u751f\u6210\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u6307\u624b\u7684\u6293\u53d6\u6210\u529f\u7387\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u5728\u4e09\u79cd\u7075\u5de7\u624b\u7684\u4eff\u771f\u6d4b\u8bd5\u4e2d\uff0c\u6293\u53d6\u6210\u529f\u7387\u8fbe\u523091.9%\u3002\u5728\u5bf9\u672a\u89c1\u624b\u7684\u5c11\u6837\u672c\u9002\u5e94\u4e2d\uff0c\u6210\u529f\u7387\u4e3a85.6%\u3002"}}
{"id": "2510.06085", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06085", "abs": "https://arxiv.org/abs/2510.06085", "authors": ["Roman Ibrahimov", "Jannik Matthias Heinen"], "title": "Multi-Robot Distributed Optimization for Exploration and Mapping of Unknown Environments using Bioinspired Tactile-Sensor", "comment": null, "summary": "This project proposes a bioinspired multi-robot system using Distributed\nOptimization for efficient exploration and mapping of unknown environments.\nEach robot explores its environment and creates a map, which is afterwards put\ntogether to form a global 2D map of the environment. Inspired by wall-following\nbehaviors, each robot autonomously explores its neighborhood based on a tactile\nsensor, similar to the antenna of a cockroach, mounted on the surface of the\nrobot. Instead of avoiding obstacles, robots log collision points when they\ntouch obstacles. This decentralized control strategy ensures effective task\nallocation and efficient exploration of unknown terrains, with applications in\nsearch and rescue, industrial inspection, and environmental monitoring. The\napproach was validated through experiments using e-puck robots in a simulated\n1.5 x 1.5 m environment with three obstacles. The results demonstrated the\nsystem's effectiveness in achieving high coverage, minimizing collisions, and\nconstructing accurate 2D maps.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u7269\u542f\u53d1\u7684\u591a\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u4f18\u5316\u5b9e\u73b0\u672a\u77e5\u73af\u5883\u7684\u6709\u6548\u63a2\u7d22\u4e0e\u6620\u5c04\uff0c\u7ed3\u679c\u663e\u793a\u5176\u5728\u591a\u4e2a\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5b9e\u73b0\u672a\u77e5\u73af\u5883\u7684\u6709\u6548\u63a2\u7d22\u4e0e\u6620\u5c04\uff0c\u63d0\u5347\u4efb\u52a1\u5206\u914d\u6548\u7387\u3002", "method": "\u91c7\u7528\u751f\u7269\u542f\u53d1\u7684\u591a\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u4f18\u5316\u8fdb\u884c\u73af\u5883\u63a2\u7d22\u548c\u6620\u5c04\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u8986\u76d6\u3001\u78b0\u649e\u7387\u548c\u5730\u56fe\u51c6\u786e\u6027\u65b9\u9762\u5177\u6709\u826f\u597d\u8868\u73b0\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5728\u6709\u6548\u8986\u76d6\u3001\u6700\u5c0f\u5316\u78b0\u649e\u548c\u6784\u5efa\u51c6\u786e\u76842D\u5730\u56fe\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.06127", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06127", "abs": "https://arxiv.org/abs/2510.06127", "authors": ["Xiao Liang", "Lu Shen", "Peihan Zhang", "Soofiyan Atar", "Florian Richter", "Michael Yip"], "title": "Towards Autonomous Tape Handling for Robotic Wound Redressing", "comment": null, "summary": "Chronic wounds, such as diabetic, pressure, and venous ulcers, affect over\n6.5 million patients in the United States alone and generate an annual cost\nexceeding \\$25 billion. Despite this burden, chronic wound care remains a\nroutine yet manual process performed exclusively by trained clinicians due to\nits critical safety demands. We envision a future in which robotics and\nautomation support wound care to lower costs and enhance patient outcomes. This\npaper introduces an autonomous framework for one of the most fundamental yet\nchallenging subtasks in wound redressing: adhesive tape manipulation.\nSpecifically, we address two critical capabilities: tape initial detachment\n(TID) and secure tape placement. To handle the complex adhesive dynamics of\ndetachment, we propose a force-feedback imitation learning approach trained\nfrom human teleoperation demonstrations. For tape placement, we develop a\nnumerical trajectory optimization method based to ensure smooth adhesion and\nwrinkle-free application across diverse anatomical surfaces. We validate these\nmethods through extensive experiments, demonstrating reliable performance in\nboth quantitative evaluations and integrated wound redressing pipelines. Our\nresults establish tape manipulation as an essential step toward practical\nrobotic wound care automation.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u4e3b\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u5584\u6162\u6027\u4f24\u53e3\u62a4\u7406\u4e2d\u7684\u80f6\u5e26\u64cd\u4f5c\uff0c\u5f3a\u8c03\u4e86\u80f6\u5e26\u521d\u59cb\u8131\u79bb\u548c\u5b89\u5168\u653e\u7f6e\u7684\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u81ea\u52a8\u5316\u4f24\u53e3\u62a4\u7406\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u6162\u6027\u4f24\u53e3\u62a4\u7406\u5bf9\u60a3\u8005\u9020\u6210\u91cd\u5927\u5f71\u54cd\uff0c\u73b0\u6709\u62a4\u7406\u8fc7\u7a0b\u7e41\u7410\u4e14\u9700\u4e13\u4e1a\u4eba\u5458\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u6280\u672f\u7684\u4ecb\u5165\u4ee5\u964d\u4f4e\u6210\u672c\u548c\u6539\u5584\u6cbb\u7597\u6548\u679c\u3002", "method": "\u91c7\u7528\u4eba\u7c7b\u9065\u64cd\u4f5c\u793a\u8303\u7684\u529b\u53cd\u9988\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u6765\u5904\u7406\u80f6\u5e26\u8131\u79bb\uff0c\u5e76\u4f7f\u7528\u6570\u503c\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\u786e\u4fdd\u80f6\u5e26\u7684\u5149\u6ed1\u7c98\u9644\u548c\u65e0\u76b1\u5e94\u7528\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u5b9a\u91cf\u8bc4\u4f30\u548c\u7efc\u5408\u4f24\u53e3\u5904\u7406\u6d41\u6c34\u7ebf\u4e2d\u5747\u8868\u73b0\u51fa\u53ef\u9760\u7684\u6027\u80fd\u3002", "conclusion": "\u80f6\u5e26\u64cd\u4f5c\u5728\u4f24\u53e3\u62a4\u7406\u81ea\u52a8\u5316\u4e2d\u662f\u4e00\u4e2a\u5173\u952e\u6b65\u9aa4\uff0c\u7814\u7a76\u8868\u660e\u6240\u63d0\u65b9\u6cd5\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u53ef\u9760\u3002"}}
{"id": "2510.06146", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06146", "abs": "https://arxiv.org/abs/2510.06146", "authors": ["Jaehwan Jeong", "Tuan-Anh Vu", "Radha Lahoti", "Jiawen Wang", "Vivek Alumootil", "Sangpil Kim", "M. Khalid Jawed"], "title": "Vision-Guided Targeted Grasping and Vibration for Robotic Pollination in Controlled Environments", "comment": null, "summary": "Robotic pollination offers a promising alternative to manual labor and\nbumblebee-assisted methods in controlled agriculture, where wind-driven\npollination is absent and regulatory restrictions limit the use of commercial\npollinators. In this work, we present and validate a vision-guided robotic\nframework that uses data from an end-effector mounted RGB-D sensor and combines\n3D plant reconstruction, targeted grasp planning, and physics-based vibration\nmodeling to enable precise pollination. First, the plant is reconstructed in 3D\nand registered to the robot coordinate frame to identify obstacle-free grasp\nposes along the main stem. Second, a discrete elastic rod model predicts the\nrelationship between actuation parameters and flower dynamics, guiding the\nselection of optimal pollination strategies. Finally, a manipulator with soft\ngrippers grasps the stem and applies controlled vibrations to induce pollen\nrelease. End-to-end experiments demonstrate a 92.5\\% main-stem grasping success\nrate, and simulation-guided optimization of vibration parameters further\nvalidates the feasibility of our approach, ensuring that the robot can safely\nand effectively perform pollination without damaging the flower. To our\nknowledge, this is the first robotic system to jointly integrate vision-based\ngrasping and vibration modeling for automated precision pollination.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u673a\u5668\u4eba\u6388\u7c89\u6846\u67b6\uff0c\u80fd\u5728\u53d7\u9650\u73af\u5883\u4e0b\u5b9e\u73b0\u9ad8\u6548\u3001\u7cbe\u51c6\u7684\u6388\u7c89\u64cd\u4f5c\u3002", "motivation": "\u5bfb\u6c42\u5728\u7f3a\u4e4f\u98ce\u529b\u6388\u7c89\u548c\u5546\u4e1a\u6388\u7c89\u8005\u88ab\u9650\u5236\u7684\u53d7\u63a7\u519c\u4e1a\u4e2d\uff0c\u5f00\u53d1\u9ad8\u6548\u7684\u6388\u7c89\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u7528RGB-D\u4f20\u611f\u5668\u8fdb\u884c3D\u690d\u7269\u91cd\u6784\uff0c\u9488\u5bf9\u6027\u6293\u53d6\u89c4\u5212\uff0c\u4ee5\u53ca\u57fa\u4e8e\u7269\u7406\u7684\u632f\u52a8\u5efa\u6a21\u6765\u5b9e\u73b0\u6388\u7c89\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e3b\u8981\u5e72\u7684\u6293\u53d6\u6210\u529f\u7387\u4e3a92.5%\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u5b89\u5168\u6027\u4e0e\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u6709\u6548\u96c6\u6210\u4e86\u89c6\u89c9\u6293\u53d6\u548c\u632f\u52a8\u5efa\u6a21\uff0c\u4e3a\u81ea\u52a8\u5316\u7cbe\u51c6\u6388\u7c89\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848\u3002"}}
{"id": "2510.06160", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06160", "abs": "https://arxiv.org/abs/2510.06160", "authors": ["Blake Romrell", "Abigail Austin", "Braden Meyers", "Ryan Anderson", "Carter Noh", "Joshua G. Mangelson"], "title": "A Preview of HoloOcean 2.0", "comment": "5 pages, 9 figures, submitted to the ICRA 2025 aq2uasim workshop", "summary": "Marine robotics simulators play a fundamental role in the development of\nmarine robotic systems. With increased focus on the marine robotics field in\nrecent years, there has been significant interest in developing higher\nfidelitysimulation of marine sensors, physics, and visual rendering\ncapabilities to support autonomous marine robot development and validation.\nHoloOcean 2.0, the next major release of HoloOcean, brings state-of-the-art\nfeatures under a general marine simulator capable of supporting a variety of\ntasks. New features in HoloOcean 2.0 include migration to Unreal Engine (UE)\n5.3, advanced vehicle dynamics using models from Fossen, and support for ROS2\nusing a custom bridge. Additional features are currently in development,\nincluding significantly more efficient ray tracing-based sidescan,\nforward-looking, and bathymetric sonar implementations; semantic sensors;\nenvironment generation tools; volumetric environmental effects; and realistic\nwaves.", "AI": {"tldr": "HoloOcean 2.0\u662f\u4e00\u4e2a\u5148\u8fdb\u7684\u6d77\u6d0b\u6a21\u62df\u5668\uff0c\u652f\u6301\u591a\u79cd\u4efb\u52a1\uff0c\u5e76\u5f15\u5165\u4e86\u591a\u4e2a\u65b0\u7279\u6027\uff0c\u5305\u62ec\u8fc1\u79fb\u5230Unreal Engine 5.3\u53caROS2\u652f\u6301\u3002", "motivation": "\u968f\u7740\u6d77\u6d0b\u673a\u5668\u4eba\u9886\u57df\u7684\u5173\u6ce8\u589e\u52a0\uff0c\u5f00\u53d1\u66f4\u9ad8\u4fdd\u771f\u5ea6\u7684\u6d77\u6d0b\u4eff\u771f\u7cfb\u7edf\u5bf9\u4e8e\u81ea\u4e3b\u6d77\u6d0b\u673a\u5668\u4eba\u5f00\u53d1\u548c\u9a8c\u8bc1\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u91c7\u7528Unreal Engine 5.3\u548cFossen\u7684\u8f66\u8f86\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u589e\u5f3a\u4e86\u6d77\u6d0b\u4f20\u611f\u5668\u3001\u7269\u7406\u548c\u89c6\u89c9\u6e32\u67d3\u7684\u4eff\u771f\u80fd\u529b\u3002", "result": "HoloOcean 2.0\u63a8\u51fa\u4e86\u4e00\u7cfb\u5217\u65b0\u7279\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6d77\u6d0b\u6a21\u62df\u7684\u529f\u80fd\u548c\u6548\u7387\uff0c\u5305\u62ec\u66f4\u9ad8\u6548\u7684\u58f0\u5450\u5b9e\u73b0\u548c\u73af\u5883\u751f\u6210\u5de5\u5177\u3002", "conclusion": "HoloOcean 2.0\u4e3a\u6d77\u6d0b\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u4eff\u771f\u7cbe\u5ea6\u548c\u591a\u6837\u6027\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u81ea\u4e3b\u6d77\u6d0b\u673a\u5668\u4eba\u7684\u8fdb\u6b65\u3002"}}
{"id": "2510.06199", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06199", "abs": "https://arxiv.org/abs/2510.06199", "authors": ["Chengyang Zhao", "Uksang Yoo", "Arkadeep Narayan Chaudhury", "Giljoo Nam", "Jonathan Francis", "Jeffrey Ichnowski", "Jean Oh"], "title": "DYMO-Hair: Generalizable Volumetric Dynamics Modeling for Robot Hair Manipulation", "comment": "Project page: https://chengyzhao.github.io/DYMOHair-web/", "summary": "Hair care is an essential daily activity, yet it remains inaccessible to\nindividuals with limited mobility and challenging for autonomous robot systems\ndue to the fine-grained physical structure and complex dynamics of hair. In\nthis work, we present DYMO-Hair, a model-based robot hair care system. We\nintroduce a novel dynamics learning paradigm that is suited for volumetric\nquantities such as hair, relying on an action-conditioned latent state editing\nmechanism, coupled with a compact 3D latent space of diverse hairstyles to\nimprove generalizability. This latent space is pre-trained at scale using a\nnovel hair physics simulator, enabling generalization across previously unseen\nhairstyles. Using the dynamics model with a Model Predictive Path Integral\n(MPPI) planner, DYMO-Hair is able to perform visual goal-conditioned hair\nstyling. Experiments in simulation demonstrate that DYMO-Hair's dynamics model\noutperforms baselines on capturing local deformation for diverse, unseen\nhairstyles. DYMO-Hair further outperforms baselines in closed-loop hair styling\ntasks on unseen hairstyles, with an average of 22% lower final geometric error\nand 42% higher success rate than the state-of-the-art system. Real-world\nexperiments exhibit zero-shot transferability of our system to wigs, achieving\nconsistent success on challenging unseen hairstyles where the state-of-the-art\nsystem fails. Together, these results introduce a foundation for model-based\nrobot hair care, advancing toward more generalizable, flexible, and accessible\nrobot hair styling in unconstrained physical environments. More details are\navailable on our project page: https://chengyzhao.github.io/DYMOHair-web/.", "AI": {"tldr": "DYMO-Hair\u662f\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u673a\u5668\u4eba\u53d1\u578b\u62a4\u7406\u7cfb\u7edf\uff0c\u91c7\u7528\u65b0\u9896\u7684\u52a8\u6001\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u76843D\u6f5c\u5728\u7a7a\u95f4\u5b9e\u73b0\u9ad8\u6548\u7684\u53d1\u578b\u8c03\u6574\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u53d1\u578b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u5907\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u7f8e\u53d1\u662f\u65e5\u5e38\u751f\u6d3b\u7684\u91cd\u8981\u6d3b\u52a8\uff0c\u4f46\u5bf9\u884c\u52a8\u80fd\u529b\u6709\u9650\u7684\u4eba\u58eb\u6765\u8bf4\u65e0\u6cd5\u83b7\u5f97\uff0c\u540c\u65f6\u5bf9\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\u6765\u8bf4\u4e5f\u5b58\u5728\u6311\u6218\u3002", "method": "\u8be5\u7cfb\u7edf\u57fa\u4e8e\u4e00\u79cd\u65b0\u9896\u7684\u52a8\u6001\u5b66\u4e60\u8303\u5f0f\uff0c\u7ed3\u5408\u52a8\u4f5c\u6761\u4ef6\u7684\u6f5c\u5728\u72b6\u6001\u7f16\u8f91\u673a\u5236\u53ca\u7d27\u51d1\u7684\u4e09\u7ef4\u6f5c\u5728\u7a7a\u95f4\uff0c\u4ee5\u63d0\u9ad8\u5bf9\u53d1\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "DYMO-Hair\u7684\u52a8\u6001\u6a21\u578b\u5728\u6a21\u62df\u4e2d\u8868\u73b0\u4f18\u4e8e\u591a\u4e2a\u57fa\u7ebf\uff0c\u5e76\u4e14\u5728\u672a\u89c1\u8fc7\u7684\u53d1\u578b\u6837\u672c\u4e0a\u6267\u884c\u5c01\u95ed\u73af\u53d1\u578b\u8bbe\u8ba1\u4efb\u52a1\u65f6\uff0c\u6700\u7ec8\u51e0\u4f55\u8bef\u5dee\u5e73\u5747\u964d\u4f4e22%\uff0c\u6210\u529f\u7387\u63d0\u9ad842%\u3002", "conclusion": "DYMO-Hair\u4e3a\u57fa\u4e8e\u6a21\u578b\u7684\u673a\u5668\u4eba\u62a4\u7406\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u81f4\u529b\u4e8e\u63a8\u8fdb\u66f4\u5177\u901a\u7528\u6027\u3001\u7075\u6d3b\u6027\u548c\u53ef\u53ca\u6027\u7684\u673a\u5668\u4eba\u53d1\u578b\u8bbe\u8ba1\u7cfb\u7edf\u3002"}}
{"id": "2510.06207", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06207", "abs": "https://arxiv.org/abs/2510.06207", "authors": ["Zefu Lin", "Rongxu Cui", "Chen Hanning", "Xiangyu Wang", "Junjia Xu", "Xiaojuan Jin", "Chen Wenbo", "Hui Zhou", "Lue Fan", "Wenling Li", "Zhaoxiang Zhang"], "title": "EmbodiedCoder: Parameterized Embodied Mobile Manipulation via Modern Coding Model", "comment": "Demo Page: https://anonymous.4open.science/w/Embodied-Coder/", "summary": "Recent advances in control robot methods, from end-to-end\nvision-language-action frameworks to modular systems with predefined\nprimitives, have advanced robots' ability to follow natural language\ninstructions. Nonetheless, many approaches still struggle to scale to diverse\nenvironments, as they often rely on large annotated datasets and offer limited\ninterpretability.In this work, we introduce EmbodiedCoder, a training-free\nframework for open-world mobile robot manipulation that leverages coding models\nto directly generate executable robot trajectories. By grounding high-level\ninstructions in code, EmbodiedCoder enables flexible object geometry\nparameterization and manipulation trajectory synthesis without additional data\ncollection or fine-tuning.This coding-based paradigm provides a transparent and\ngeneralizable way to connect perception with manipulation. Experiments on real\nmobile robots show that EmbodiedCoder achieves robust performance across\ndiverse long-term tasks and generalizes effectively to novel objects and\nenvironments.Our results demonstrate an interpretable approach for bridging\nhigh-level reasoning and low-level control, moving beyond fixed primitives\ntoward versatile robot intelligence. See the project page at:\nhttps://anonymous.4open.science/w/Embodied-Coder/", "AI": {"tldr": "EmbodiedCoder\u662f\u4e00\u4e2a\u65e0\u8bad\u7ec3\u8981\u6c42\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u6846\u67b6\uff0c\u5229\u7528\u7f16\u7801\u6a21\u578b\u751f\u6210\u6267\u884c\u8f68\u8ff9\uff0c\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u548c\u901a\u7528\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u673a\u5668\u4eba\u63a7\u5236\u65b9\u6cd5\u5728\u5904\u7406\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u65f6\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u4f9d\u8d56\u5927\u578b\u6807\u6ce8\u6570\u636e\u96c6\u548c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u901a\u8fc7\u5728\u4ee3\u7801\u4e2d\u57fa\u7840\u9ad8\u5c42\u6307\u4ee4\uff0cEmbodiedCoder\u5b9e\u73b0\u4e86\u7075\u6d3b\u7684\u5bf9\u8c61\u5904\u7406\u548c\u52a8\u4f5c\u5408\u6210\uff0c\u800c\u65e0\u9700\u989d\u5916\u7684\u6570\u636e\u6536\u96c6\u6216\u8c03\u6574\u3002", "result": "\u63d0\u51fa\u4e86EmbodiedCoder\uff0c\u4e00\u4e2a\u65e0\u987b\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u80fd\u591f\u76f4\u63a5\u751f\u6210\u53ef\u6267\u884c\u7684\u673a\u5668\u4eba\u8f68\u8ff9\uff0c\u5b9e\u73b0\u7075\u6d3b\u7684\u5bf9\u8c61\u51e0\u4f55\u53c2\u6570\u5316\u548c\u8f68\u8ff9\u5408\u6210\u3002", "conclusion": "EmbodiedCoder\u5728\u591a\u6837\u5316\u957f\u671f\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u63a8\u5e7f\u5230\u65b0\u7269\u4f53\u548c\u73af\u5883\uff0c\u5e76\u4e14\u63d0\u4f9b\u4e86\u89e3\u51b3\u9ad8\u5c42\u63a8\u7406\u4e0e\u4f4e\u5c42\u63a7\u5236\u4e4b\u95f4\u8054\u7cfb\u7684\u53ef\u89e3\u91ca\u65b9\u6cd5\u3002"}}
