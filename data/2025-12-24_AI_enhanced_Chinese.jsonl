{"id": "2512.19707", "categories": ["cs.HC", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.19707", "abs": "https://arxiv.org/abs/2512.19707", "authors": ["James K Ruffle", "Samia Mohinta", "Guilherme Pombo", "Asthik Biswas", "Alan Campbell", "Indran Davagnanam", "David Doig", "Ahmed Hamman", "Harpreet Hyare", "Farrah Jabeen", "Emma Lim", "Dermot Mallon", "Stephanie Owen", "Sophie Wilkinson", "Sebastian Brandner", "Parashkev Nachev"], "title": "Bidirectional human-AI collaboration in brain tumour assessments improves both expert human and AI agent performance", "comment": "38 pages, 6 figures, 7 supplementary figures", "summary": "The benefits of artificial intelligence (AI) human partnerships-evaluating how AI agents enhance expert human performance-are increasingly studied. Though rarely evaluated in healthcare, an inverse approach is possible: AI benefiting from the support of an expert human agent. Here, we investigate both human-AI clinical partnership paradigms in the magnetic resonance imaging-guided characterisation of patients with brain tumours. We reveal that human-AI partnerships improve accuracy and metacognitive ability not only for radiologists supported by AI, but also for AI agents supported by radiologists. Moreover, the greatest patient benefit was evident with an AI agent supported by a human one. Synergistic improvements in agent accuracy, metacognitive performance, and inter-rater agreement suggest that AI can create more capable, confident, and consistent clinical agents, whether human or model-based. Our work suggests that the maximal value of AI in healthcare could emerge not from replacing human intelligence, but from AI agents that routinely leverage and amplify it.", "AI": {"tldr": "\u672c\u7814\u7a76\u663e\u793a\u4eba\u7c7b\u4e0eAI\u7684\u5408\u4f5c\u53ef\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u663e\u8457\u63d0\u5347\u8111\u80bf\u7624\u60a3\u8005\u7684\u8bca\u65ad\u51c6\u786e\u6027\u4e0e\u5143\u8ba4\u77e5\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u4eba\u7c7b\u4e0eAI\u7684\u5408\u4f5c\u5982\u4f55\u63d0\u5347\u533b\u7597\u8bca\u65ad\u7684\u51c6\u786e\u6027\uff0c\u4ee5\u63a2\u7d22AI\u5728\u533b\u7597\u4e2d\u66f4\u6709\u6548\u7684\u5e94\u7528\u65b9\u5f0f\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u5206\u6790\u4eba\u7c7b\u653e\u5c04\u79d1\u533b\u751f\u4e0eAI\u4ee3\u7406\u5728\u8111\u80bf\u7624\u60a3\u8005\u5f71\u50cf\u8bca\u65ad\u4e2d\u7684\u8868\u73b0\uff0c\u8bc4\u4f30\u4e24\u8005\u4e4b\u95f4\u7684\u76f8\u4e92\u652f\u6301\u6548\u679c\u3002", "result": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4eba\u7c7b\u4e0e\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u7684\u5408\u4f5c\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u5bf9\u8111\u80bf\u7624\u60a3\u8005\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u65e0\u8bba\u662f\u4eba\u7c7b\u4e13\u5bb6\u8fd8\u662fAI\u4ee3\u7406\uff0c\u5728\u76f8\u4e92\u652f\u6301\u4e0b\uff0c\u53cc\u65b9\u7684\u51c6\u786e\u6027\u548c\u5143\u8ba4\u77e5\u80fd\u529b\u90fd\u6709\u6240\u63d0\u5347\u3002\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u4eba\u7c7b\u7684\u652f\u6301\uff0cAI\u5448\u73b0\u51fa\u6700\u9ad8\u7684\u6548\u76ca\u3002", "conclusion": "\u4eba\u5de5\u667a\u80fd\u5728\u533b\u7597\u9886\u57df\u7684\u6700\u5927\u4ef7\u503c\u53ef\u80fd\u5728\u4e8e\u589e\u5f3a\u4eba\u7c7b\u667a\u80fd\u800c\u975e\u53d6\u4ee3\u5b83\u3002"}}
{"id": "2512.19810", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.19810", "abs": "https://arxiv.org/abs/2512.19810", "authors": ["Diego Riofr\u00edo-Luzcando", "Jaime Ram\u00edrez", "Marta Berrocal-Lobo"], "title": "Predicting Student Actions in a Procedural Training Environment", "comment": "12 pages. Author Accepted Manuscript (AAM). \\c{opyright} 2017 IEEE. Final published version: https://doi.org/10.1109/TLT.2017.2658569", "summary": "Data mining is known to have a potential for predicting user performance. However, there are few studies that explore its potential for predicting student behavior in a procedural training environment. This paper presents a collective student model, which is built from past student logs. These logs are firstly grouped into clusters. Then an extended automaton is created for each cluster based on the sequences of events found in the cluster logs. The main objective of this model is to predict the actions of new students for improving the tutoring feedback provided by an intelligent tutoring system. The proposed model has been validated using student logs collected in a 3D virtual laboratory for teaching biotechnology. As a result of this validation, we concluded that the model can provide reasonably good predictions and can support tutoring feedback that is better adapted to each student type.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5386\u53f2\u65e5\u5fd7\u7684\u96c6\u4f53\u5b66\u751f\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u5b66\u751f\u884c\u4e3a\u5e76\u6539\u8fdb\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u7684\u53cd\u9988\u3002", "motivation": "\u6570\u636e\u6316\u6398\u5728\u7528\u6237\u8868\u73b0\u9884\u6d4b\u65b9\u9762\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u5229\u7528\u4e8e\u5b66\u751f\u884c\u4e3a\u9884\u6d4b\uff0c\u5c24\u5176\u5728\u7a0b\u5e8f\u57f9\u8bad\u73af\u5883\u4e2d\u3002", "method": "\u901a\u8fc7\u5c06\u5b66\u751f\u65e5\u5fd7\u5206\u7ec4\u4e3a\u7c07\uff0c\u5e76\u57fa\u4e8e\u7c07\u5185\u4e8b\u4ef6\u5e8f\u5217\u521b\u5efa\u6269\u5c55\u81ea\u52a8\u673a\uff0c\u6784\u5efa\u5b66\u751f\u884c\u4e3a\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u4f53\u5b66\u751f\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u6790\u5b66\u751f\u7684\u5386\u53f2\u65e5\u5fd7\u6765\u6784\u5efa\uff0c\u4ee5\u9884\u6d4b\u5b66\u751f\u5728\u7a0b\u5e8f\u57f9\u8bad\u73af\u5883\u4e2d\u7684\u884c\u4e3a\u3002", "conclusion": "\u9a8c\u8bc1\u7ed3\u679c\u663e\u793a\u8be5\u6a21\u578b\u80fd\u591f\u63d0\u4f9b\u5408\u7406\u7684\u9884\u6d4b\uff0c\u5e76\u63d0\u5347\u9488\u5bf9\u4e0d\u540c\u5b66\u751f\u7c7b\u578b\u7684\u8f85\u5bfc\u53cd\u9988\u6548\u679c\u3002"}}
{"id": "2512.19832", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.19832", "abs": "https://arxiv.org/abs/2512.19832", "authors": ["Mark D\u00edaz", "Renee Shelby", "Eric Corbett", "Andrew Smart"], "title": "How Tech Workers Contend with Hazards of Humanlikeness in Generative AI", "comment": null, "summary": "Generative AI's humanlike qualities are driving its rapid adoption in professional domains. However, this anthropomorphic appeal raises concerns from HCI and responsible AI scholars about potential hazards and harms, such as overtrust in system outputs. To investigate how technology workers navigate these humanlike qualities and anticipate emergent harms, we conducted focus groups with 30 professionals across six job functions (ML engineering, product policy, UX research and design, product management, technology writing, and communications). Our findings reveal an unsettled knowledge environment surrounding humanlike generative AI, where workers' varying perspectives illuminate a range of potential risks for individuals, knowledge work fields, and society. We argue that workers require comprehensive support, including clearer conceptions of ``humanlikeness'' to effectively mitigate these risks. To aid in mitigation strategies, we provide a conceptual map articulating the identified hazards and their connection to conflated notions of ``humanlikeness.''", "AI": {"tldr": "\u751f\u6210\u5f0fAI\u7684\u7c7b\u4eba\u7279\u5f81\u867d\u7136\u63a8\u52a8\u5176\u5feb\u901f\u91c7\u7528\uff0c\u4f46\u4e5f\u5f15\u53d1\u4e86\u5bf9\u6f5c\u5728\u98ce\u9669\u7684\u62c5\u5fe7\u3002", "motivation": "\u7814\u7a76\u751f\u6210\u5f0fAI\u5177\u6709\u4eba\u7c7b\u7279\u5f81\u6240\u5e26\u6765\u7684\u673a\u9047\u548c\u98ce\u9669", "method": "\u901a\u8fc7\u5bf930\u4f4d\u4e13\u4e1a\u4eba\u58eb\u8fdb\u884c\u7126\u70b9\u5c0f\u7ec4\u8bbf\u8c08", "result": "\u63ed\u793a\u51fa\u5bf9\u4eba\u7c7b\u7279\u5f81\u751f\u6210\u5f0fAI\u7684\u77e5\u8bc6\u73af\u5883\u4e0d\u7a33\u5b9a\uff0c\u4e13\u4e1a\u4eba\u58eb\u7684\u4e0d\u540c\u89c2\u70b9\u63ed\u793a\u51fa\u591a\u79cd\u6f5c\u5728\u98ce\u9669", "conclusion": "\u4e3a\u4e86\u6709\u6548\u964d\u4f4e\u98ce\u9669\uff0c\u4e13\u4e1a\u4eba\u58eb\u9700\u8981\u66f4\u5168\u9762\u7684\u652f\u6301\uff0c\u5305\u62ec\u66f4\u6e05\u6670\u7684\u4eba\u7c7b\u7279\u5f81\u6982\u5ff5\u3002"}}
{"id": "2512.19885", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.19885", "abs": "https://arxiv.org/abs/2512.19885", "authors": ["Diego Riofr\u00edo-Luzcando", "Jaime Ram\u00cdrez", "Cristian Moral", "Ang\u00e9lica de Antonio", "Marta Berrocal-Lobo"], "title": "Visualizing a Collective Student Model for Procedural Training Environments", "comment": "Preprint (not peer-reviewed). Version of Record: https://doi.org/10.1007/s11042-018-6641-x", "summary": "Visualization plays a relevant role for discovering patterns in big sets of data. In fact, the most common way to help a human with a pattern interpretation is through a graphic. In 2D/3D virtual environments for procedural training the student interaction is more varied and complex than in traditional e-learning environments. Therefore, the visualization and interpretation of students' behaviors becomes a challenge. This motivated us to design the visualization of a collective student model built from student logs taken from 2D/3D virtual environments for procedural training. This paper presents the design decisions that enable a suitable visualization of this model to instructors as well as a web tool that implements this visualization and is intended: to help instructors to improve their own teaching; and to enhance the tutoring strategy of an Intelligent Tutoring System. Then, this paper illustrates, with three detailed examples, how this tool can be used to those educational purposes. Next, the paper presents an experiment for validating the utility of the tool. In this experiment we show how the tool can help to modify the tutoring strategy of a 3D virtual laboratory. In this way, it is shown that the proposed visualization of the model can serve to improve the performance of students in 2D/3D virtual environments for procedural training.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u9762\u54112D/3D\u865a\u62df\u57f9\u8bad\u73af\u5883\u7684\u5b66\u751f\u6a21\u578b\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u65e8\u5728\u63d0\u9ad8\u6559\u5e08\u7684\u6559\u5b66\u6548\u7387\u548c\u5b66\u751f\u7684\u5b66\u4e60\u8868\u73b0\u3002", "motivation": "\u57282D/3D\u865a\u62df\u73af\u5883\u4e2d\uff0c\u5b66\u751f\u4e92\u52a8\u590d\u6742\uff0c\u9700\u8981\u6709\u6548\u7684\u53ef\u89c6\u5316\u6765\u5e2e\u52a9\u5206\u6790\u884c\u4e3a\u548c\u6539\u8fdb\u6559\u5b66\u7b56\u7565\u3002", "method": "\u8bbe\u8ba1\u5e08\u751f\u6a21\u578b\u7684\u53ef\u89c6\u5316\u4ee5\u53ca\u5f00\u53d1\u76f8\u5e94\u7684\u7f51\u9875\u5de5\u5177", "result": "\u5f00\u53d1\u7684\u53ef\u89c6\u5316\u5de5\u5177\u53ef\u4ee5\u5e2e\u52a9\u6559\u5e08\u6539\u5584\u6559\u5b66\uff0c\u589e\u5f3a\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u7684\u8f85\u5bfc\u7b56\u7565\uff0c\u5e76\u4e14\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u57283D\u865a\u62df\u5b9e\u9a8c\u5ba4\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u5de5\u5177\u6709\u6548\u63d0\u5347\u4e86\u6559\u5e08\u7684\u6559\u5b66\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u80fd\u6539\u5584\u5b66\u751f\u5728\u865a\u62df\u73af\u5883\u4e0b\u7684\u8868\u73b0\u3002"}}
{"id": "2512.19855", "categories": ["cs.RO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.19855", "abs": "https://arxiv.org/abs/2512.19855", "authors": ["Andrew Stirling", "Mykola Lukashchuk", "Dmitry Bagaev", "Wouter Kouw", "James R. Forbes"], "title": "Gaussian Variational Inference with Non-Gaussian Factors for State Estimation: A UWB Localization Case Study", "comment": null, "summary": "This letter extends the exactly sparse Gaussian variational inference (ESGVI) algorithm for state estimation in two complementary directions. First, ESGVI is generalized to operate on matrix Lie groups, enabling the estimation of states with orientation components while respecting the underlying group structure. Second, factors are introduced to accommodate heavy-tailed and skewed noise distributions, as commonly encountered in ultra-wideband (UWB) localization due to non-line-of-sight (NLOS) and multipath effects. Both extensions are shown to integrate naturally within the ESGVI framework while preserving its sparse and derivative-free structure. The proposed approach is validated in a UWB localization experiment with NLOS-rich measurements, demonstrating improved accuracy and comparable consistency. Finally, a Python implementation within a factor-graph-based estimation framework is made open-source (https://github.com/decargroup/gvi_ws) to support broader research use.", "AI": {"tldr": "\u672c\u7814\u7a76\u6269\u5c55\u4e86ESGVI\u7b97\u6cd5\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u77e9\u9635\u674e\u7fa4\u4e0a\u8fdb\u884c\u72b6\u6001\u4f30\u8ba1\uff0c\u5e76\u5f15\u5165\u56e0\u5b50\u5904\u7406\u91cd\u5c3e\u548c\u504f\u659c\u566a\u58f0\u5206\u5e03\uff0c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u5728UWB\u5b9a\u4f4d\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u63d0\u9ad8\u72b6\u6001\u4f30\u8ba1\u7684\u7cbe\u5ea6\u548c\u4e00\u81f4\u6027\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u975e\u89c6\u8ddd\u548c\u591a\u5f84\u6548\u5e94\u7684\u8d85\u5bbd\u5e26\u5b9a\u4f4d\u573a\u666f\u4e2d\u3002", "method": "\u5c06ESGVI\u7b97\u6cd5\u63a8\u5e7f\u5230\u77e9\u9635\u674e\u7fa4\uff0c\u6dfb\u52a0\u56e0\u5b50\u5904\u7406\u91cd\u5c3e\u548c\u504f\u659c\u566a\u58f0\uff0c\u4fdd\u7559\u4e86\u7b97\u6cd5\u7684\u7a00\u758f\u6027\u548c\u65e0\u5bfc\u6570\u7279\u6027\u3002", "result": "\u6269\u5c55\u7684ESGVI\u7b97\u6cd5\u5728UWB\u5b9a\u4f4d\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6539\u8fdb\u7684\u51c6\u786e\u6027\u548c\u53ef\u6bd4\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5177\u6709\u4e30\u5bccNLOS\u6d4b\u91cf\u7684UWB\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86ESGVI\u7684\u7a00\u758f\u548c\u65e0\u5bfc\u6570\u7ed3\u6784\u3002"}}
{"id": "2512.19898", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.19898", "abs": "https://arxiv.org/abs/2512.19898", "authors": ["Hanna Noyce", "Emily Olejniczak", "Vaskar Raychoudhury", "Roger O. Smith", "Md Osman Gani"], "title": "Free-Will vs Free-Wheel: Understanding Community Accessibility Requirements of Wheelchair Users through Interviews, Participatory Action, and Modeling", "comment": null, "summary": "Community participation is an important aspect of an individuals physical and mental well-being. This participation is often limited for persons with disabilities, especially those with ambulatory impairments due to the inability to optimally navigate the community. Accessibility is a multi-faceted problem and varies from person to person. Moreover, it depends on various personal and environmental factors. Despite significant research conducted to understand challenges faced by wheelchair users, developing an accessibility model for wheelchair users by identifying various characteristic features has not been thoroughly studied. In this research, we propose a three-dimensional model of accessibility and validate it through in-depth qualitative analysis involving semi-structured interviews and participatory action research. The outcomes of our studies validated many of our hypotheses about community access for wheelchair users and identified a need for more accessible path planning tools and resources. Overall, this research strengthened our three-dimensional User-Wheelchair-Environment model of accessibility.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u7528\u6237-\u8f6e\u6905-\u73af\u5883\u4e09\u7ef4\u65e0\u969c\u788d\u6a21\u578b\uff0c\u5f3a\u8c03\u5e94\u6539\u5584\u8f6e\u6905\u4f7f\u7528\u8005\u7684\u793e\u533a\u53c2\u4e0e\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u8bc6\u522b\u8f6e\u6905\u7528\u6237\u9762\u4e34\u7684\u65e0\u969c\u788d\u6311\u6218\uff0c\u8fdb\u800c\u5f00\u53d1\u65e0\u969c\u788d\u6a21\u578b\uff0c\u4ee5\u6539\u5584\u6b8b\u75be\u4eba\u58eb\u7684\u793e\u533a\u53c2\u4e0e\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u6df1\u5165\u7684\u5b9a\u6027\u5206\u6790\uff0c\u5305\u62ec\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u548c\u53c2\u4e0e\u5f0f\u884c\u52a8\u7814\u7a76\uff0c\u9a8c\u8bc1\u4e86\u4e09\u7ef4\u65e0\u969c\u788d\u6a21\u578b\u3002", "result": "\u7814\u7a76\u9a8c\u8bc1\u4e86\u591a\u4e2a\u5173\u4e8e\u8f6e\u6905\u7528\u6237\u793e\u533a\u8bbf\u95ee\u7684\u5047\u8bbe\uff0c\u5e76\u8bc6\u522b\u4e86\u5bf9\u66f4\u65e0\u969c\u788d\u8def\u5f84\u89c4\u5212\u5de5\u5177\u548c\u8d44\u6e90\u7684\u9700\u6c42\u3002", "conclusion": "\u672c\u7814\u7a76\u52a0\u5f3a\u4e86\u7528\u6237-\u8f6e\u6905-\u73af\u5883\u4e09\u7ef4\u65e0\u969c\u788d\u6a21\u578b\uff0c\u5e76\u5f3a\u8c03\u4e86\u5bf9\u8f6e\u6905\u4f7f\u7528\u8005\u793e\u533a\u901a\u884c\u7684\u6df1\u5165\u7406\u89e3\u3002"}}
{"id": "2512.19914", "categories": ["cs.RO", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.19914", "abs": "https://arxiv.org/abs/2512.19914", "authors": ["Sujan Warnakulasooriya", "Andreas Willig", "Xiaobing Wu"], "title": "A Time-efficient Prioritised Scheduling Algorithm to Optimise Initial Flock Formation of Drones", "comment": "35 pages", "summary": "Drone applications continue to expand across various domains, with flocking offering enhanced cooperative capabilities but introducing significant challenges during initial formation. Existing flocking algorithms often struggle with efficiency and scalability, particularly when potential collisions force drones into suboptimal trajectories. This paper presents a time-efficient prioritised scheduling algorithm that improves the initial formation process of drone flocks. The method assigns each drone a priority based on its number of potential collisions and its likelihood of reaching its target position without permanently obstructing other drones. Using this hierarchy, each drone computes an appropriate delay to ensure a collision-free path. Simulation results show that the proposed algorithm successfully generates collision-free trajectories for flocks of up to 5000 drones and outperforms the coupling-degree-based heuristic prioritised planning method (CDH-PP) in both performance and computational efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u95f4\u9ad8\u6548\u7684\u4f18\u5148\u7ea7\u8c03\u5ea6\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u65e0\u4eba\u673a\u5206\u914d\u4f18\u5148\u7ea7\u6765\u6539\u5584\u65e0\u4eba\u673a\u7f16\u961f\u7684\u521d\u59cb\u5f62\u6210\u8fc7\u7a0b\uff0c\u4ece\u800c\u751f\u6210\u65e0\u78b0\u649e\u7684\u8f68\u8ff9\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u5e94\u7528\u7684\u6269\u5c55\uff0c\u7f16\u961f\u80fd\u529b\u589e\u5f3a\u4f46\u5728\u521d\u59cb\u5f62\u6210\u8fc7\u7a0b\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u7b97\u6cd5\u5728\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u65e0\u4eba\u673a\u6839\u636e\u6f5c\u5728\u78b0\u649e\u6570\u91cf\u548c\u8fbe\u5230\u76ee\u6807\u4f4d\u7f6e\u7684\u53ef\u80fd\u6027\u5206\u914d\u4f18\u5148\u7ea7\uff0c\u5e76\u8ba1\u7b97\u9002\u5f53\u7684\u5ef6\u8fdf\u4ee5\u786e\u4fdd\u65e0\u78b0\u649e\u8def\u5f84\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u8026\u5408\u5ea6\u7684\u4f18\u5148\u7ea7\u89c4\u5212\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u751f\u6210\u65e0\u78b0\u649e\u8f68\u8ff9\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u591a\u8fbe5000\u67b6\u65e0\u4eba\u673a\u7684\u7f16\u961f\u3002"}}
{"id": "2512.19899", "categories": ["cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.19899", "abs": "https://arxiv.org/abs/2512.19899", "authors": ["Pa\u00fal Cumba-Armijos", "Diego Riofr\u00edo-Luzcando", "Ver\u00f3nica Rodr\u00edguez-Arboleda", "Joe Carri\u00f3n-Jumbo"], "title": "Detecting cyberbullying in Spanish texts through deep learning techniques", "comment": "Preprint (Author's Original Manuscript, AOM). Published version: https://doi.org/10.1504/IJDMMM.2022.125265", "summary": "Recent recollected data suggests that it is possible to automatically detect events that may negatively affect the most vulnerable parts of our society, by using any communication technology like social networks or messaging applications. This research consolidates and prepares a corpus with Spanish bullying expressions taken from Twitter in order to use them as an input to train a convolutional neuronal network through deep learning techniques. As a result of this training, a predictive model was created, which can identify Spanish cyberbullying expressions such as insults, racism, homophobic attacks, and so on.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u793e\u4ea4\u7f51\u7edc\u6536\u96c6\u897f\u73ed\u7259\u8bed\u6b3a\u51cc\u8868\u8fbe\uff0c\u5e76\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u6a21\u578b\uff0c\u4ee5\u81ea\u52a8\u68c0\u6d4b\u7f51\u7edc\u6b3a\u51cc\u4e8b\u4ef6\u3002", "motivation": "\u5e0c\u671b\u81ea\u52a8\u68c0\u6d4b\u53ef\u80fd\u5bf9\u793e\u4f1a\u5f31\u52bf\u7fa4\u4f53\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u7684\u4e8b\u4ef6\u3002", "method": "\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u901a\u8fc7\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u53ef\u4ee5\u8bc6\u522b\u4fae\u8fb1\u3001\u79cd\u65cf\u4e3b\u4e49\u3001\u6050\u540c\u653b\u51fb\u7b49\u897f\u73ed\u7259\u8bed\u7f51\u7edc\u6b3a\u51cc\u8868\u8fbe\u7684\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u6210\u529f\u521b\u5efa\u4e86\u4e00\u79cd\u9884\u6d4b\u6a21\u578b\uff0c\u53ef\u4ee5\u8bc6\u522b\u897f\u73ed\u7259\u8bed\u4e2d\u7684\u7f51\u7edc\u6b3a\u51cc\u8868\u8fbe\u3002"}}
{"id": "2512.20014", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20014", "abs": "https://arxiv.org/abs/2512.20014", "authors": ["Sangoh Lee", "Sangwoo Mo", "Wook-Shin Han"], "title": "Bring My Cup! Personalizing Vision-Language-Action Models with Visual Attentive Prompting", "comment": null, "summary": "While Vision-Language-Action (VLA) models generalize well to generic instructions, they struggle with personalized commands such as \"bring my cup\", where the robot must act on one specific instance among visually similar objects. We study this setting of manipulating personal objects, in which a VLA must identify and control a user-specific object unseen during training using only a few reference images. To address this challenge, we propose Visual Attentive Prompting (VAP), a simple-yet-effective training-free perceptual adapter that equips frozen VLAs with top-down selective attention. VAP treats the reference images as a non-parametric visual memory, grounds the personal object in the scene through open-vocabulary detection and embedding-based matching, and then injects this grounding as a visual prompt by highlighting the object and rewriting the instruction. We construct two simulation benchmarks, Personalized-SIMPLER and Personalized-VLABench, and a real-world tabletop benchmark to evaluate personalized manipulation across multiple robots and tasks. Experiments show that VAP consistently outperforms generic policies and token-learning baselines in both success rate and correct-object manipulation, helping to bridge the gap between semantic understanding and instance-level control.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u89c6\u89c9\u5173\u6ce8\u63d0\u793a\uff08VAP\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u5e2e\u52a9\u673a\u5668\u4eba\u51c6\u786e\u8bc6\u522b\u548c\u64cd\u4f5c\u7528\u6237\u7279\u5b9a\u7684\u4e2a\u4eba\u7269\u54c1\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8\uff08VLA\uff09\u6a21\u578b\u7684\u5c40\u9650\u3002", "motivation": "\u73b0\u6709\u7684VLA\u6a21\u578b\u5728\u5904\u7406\u4e2a\u6027\u5316\u6307\u4ee4\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5f53\u9700\u8981\u8bc6\u522b\u4e0e\u8bad\u7ec3\u96c6\u89c6\u89c9\u76f8\u4f3c\u7684\u7279\u5b9a\u5bf9\u8c61\u65f6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u5e94\u5bf9\u7528\u6237\u7279\u5b9a\u7269\u54c1\u7684\u64cd\u4f5c\u3002", "method": "VAP\u4f5c\u4e3a\u4e00\u79cd\u611f\u77e5\u9002\u914d\u5668\uff0c\u901a\u8fc7\u4f7f\u7528\u53c2\u8003\u56fe\u50cf\u4f5c\u4e3a\u975e\u53c2\u6570\u89c6\u89c9\u8bb0\u5fc6\uff0c\u5229\u7528\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u548c\u57fa\u4e8e\u5d4c\u5165\u7684\u5339\u914d\u6765\u5b9a\u4f4d\u4e2a\u4eba\u7269\u4f53\uff0c\u5e76\u901a\u8fc7\u9ad8\u4eae\u663e\u793a\u76ee\u6807\u7269\u4f53\u548c\u91cd\u5199\u6307\u4ee4\u6765\u6ce8\u5165\u89c6\u89c9\u63d0\u793a\u3002", "result": "\u901a\u8fc7\u6784\u5efa\u4e24\u4e2a\u4eba\u5de5\u6a21\u62df\u57fa\u51c6\u548c\u4e00\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u684c\u9762\u57fa\u51c6\uff0cVAP\u5728\u591a\u4e2a\u673a\u5668\u4eba\u548c\u4efb\u52a1\u4e2d\u7684\u4e2a\u6027\u5316\u64cd\u4f5c\u8868\u73b0\u51fa\u8272\uff0c\u6210\u529f\u7387\u548c\u6b63\u786e\u7269\u4f53\u64cd\u4f5c\u5747\u4f18\u4e8e\u5176\u4ed6\u57fa\u51c6\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cVAP\u65b9\u6cd5\u5728\u7279\u5b9a\u7269\u54c1\u64cd\u4f5c\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4f20\u7edf\u7b56\u7565\u548c\u57fa\u7840\u5b66\u4e60\u6a21\u578b\uff0c\u4ece\u800c\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7684\u4e2a\u6027\u5316\u7269\u54c1\u64cd\u4f5c\u80fd\u529b\u3002"}}
{"id": "2512.19926", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.19926", "abs": "https://arxiv.org/abs/2512.19926", "authors": ["Charlotte Brandebusemeyer", "Tobias Schimmer", "Bert Arnrich"], "title": "Developers' Experience with Generative AI -- First Insights from an Empirical Mixed-Methods Field Study", "comment": null, "summary": "With the rise of AI-powered coding assistants, firms and programmers are exploring how to optimize their interaction with them. Research has so far mainly focused on evaluating output quality and productivity gains, leaving aside the developers' experience during the interaction. In this study, we take a multimodal, developer-centered approach to gain insights into how professional developers experience the interaction with Generative AI (GenAI) in their natural work environment in a firm. The aim of this paper is (1) to demonstrate a feasible mixed-method study design with controlled and uncontrolled study periods within a firm setting, (2) to give first insights from complementary behavioral and subjective experience data on developers' interaction with GitHub Copilot and (3) to compare the impact of interaction types (no Copilot use, in-code suggestions, chat prompts or both in-code suggestions and chat prompts) on efficiency, accuracy and perceived workload whilst working on different task categories. Results of the controlled sessions in this study indicate that moderate use of either in-code suggestions or chat prompts improves efficiency (task duration) and reduces perceived workload compared to not using Copilot, while excessive or combined use lessens these benefits. Accuracy (task completion) profits from chat interaction. In general, subjective perception of workload aligns with objective behavioral data in this study. During the uncontrolled period of the study, both higher cognitive load and productivity were perceived when interacting with AI during everyday working tasks. This study motivates the use of comparable study designs, in e.g. workshop or hackathon settings, to evaluate GenAI tools holistically and realistically with a focus on the developers' experience.", "AI": {"tldr": "\u672c\u7814\u7a76\u91c7\u7528\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u5173\u6ce8\u5f00\u53d1\u8005\u4e0e\u751f\u6210\u6027AI\u7684\u4e92\u52a8\u4f53\u9a8c\u3002\u7ed3\u679c\u8868\u660e\uff0c\u9002\u5ea6\u4f7f\u7528AI\u52a9\u624b\u53ef\u63d0\u9ad8\u6548\u7387\u548c\u964d\u4f4e\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u4f46\u8fc7\u5ea6\u4f7f\u7528\u5219\u9002\u5f97\u5176\u53cd\u3002", "motivation": "\u63a2\u7d22\u7a0b\u5e8f\u5458\u4e0e\u751f\u6210\u6027\u4eba\u5de5\u667a\u80fd\uff08GenAI\uff09\u52a9\u624b\u7684\u4e92\u52a8\u4f53\u9a8c\uff0c\u4ee5\u4f18\u5316\u8fd9\u79cd\u5173\u7cfb\uff0c\u4f7f\u5176\u66f4\u7b26\u5408\u5f00\u53d1\u8005\u7684\u9700\u6c42\u548c\u5b9e\u9645\u5de5\u4f5c\u73af\u5883\u3002", "method": "\u901a\u8fc7\u5728\u516c\u53f8\u73af\u5883\u4e0b\u8bbe\u8ba1\u53d7\u63a7\u4e0e\u975e\u53d7\u63a7\u7684\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\uff0c\u6536\u96c6\u884c\u4e3a\u6570\u636e\u4e0e\u4e3b\u89c2\u4f53\u9a8c\u6570\u636e\uff0c\u4ee5\u6bd4\u8f83\u4e0d\u540c\u4ea4\u4e92\u7c7b\u578b\u5bf9\u6548\u7387\u3001\u51c6\u786e\u6027\u548c\u5de5\u4f5c\u8d1f\u8f7d\u611f\u77e5\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u663e\u793a\uff0c\u9002\u5ea6\u4f7f\u7528GitHub Copilot\u7684\u4ee3\u7801\u5efa\u8bae\u6216\u804a\u5929\u63d0\u793a\u53ef\u4ee5\u63d0\u9ad8\u5de5\u4f5c\u6548\u7387\uff0c\u5e76\u964d\u4f4e\u611f\u77e5\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u800c\u8fc7\u5ea6\u6216\u540c\u65f6\u4f7f\u7528\u8fd9\u4e24\u79cd\u65b9\u6cd5\u5219\u4f1a\u51cf\u5f31\u8fd9\u4e9b\u597d\u5904\u3002", "conclusion": "\u5728\u65e5\u5e38\u5de5\u4f5c\u4efb\u52a1\u4e2d\uff0c\u5f00\u53d1\u8005\u4e0e\u751f\u6210\u6027AI\u7684\u4e92\u52a8\u589e\u52a0\u4e86\u8ba4\u77e5\u8d1f\u8377\u548c\u751f\u4ea7\u529b\uff0c\u5efa\u8bae\u91c7\u7528\u7c7b\u4f3c\u7684\u7814\u7a76\u8bbe\u8ba1\u4ee5\u5168\u9762\u8bc4\u4ef7GenAI\u5de5\u5177\u7684\u4f7f\u7528\u4f53\u9a8c\u3002"}}
{"id": "2512.20166", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.20166", "abs": "https://arxiv.org/abs/2512.20166", "authors": ["Xiaofan Wang", "Xingyu Gao", "Jianlong Fu", "Zuolei Li", "Dean Fortier", "Galen Mullins", "Andrey Kolobov", "Baining Guo"], "title": "LoLA: Long Horizon Latent Action Learning for General Robot Manipulation", "comment": null, "summary": "The capability of performing long-horizon, language-guided robotic manipulation tasks critically relies on leveraging historical information and generating coherent action sequences. However, such capabilities are often overlooked by existing Vision-Language-Action (VLA) models. To solve this challenge, we propose LoLA (Long Horizon Latent Action Learning), a framework designed for robot manipulation that integrates long-term multi-view observations and robot proprioception to enable multi-step reasoning and action generation. We first employ Vision-Language Models to encode rich contextual features from historical sequences and multi-view observations. We further introduces a key module, State-Aware Latent Re-representation, which transforms visual inputs and language commands into actionable robot motion space. Unlike existing VLA approaches that merely concatenate robot proprioception (e.g., joint angles) with VL embeddings, this module leverages such robot states to explicitly ground VL representations in physical scale through a learnable \"embodiment-anchored\" latent space. We trained LoLA on diverse robotic pre-training datasets and conducted extensive evaluations on simulation benchmarks (SIMPLER and LIBERO), as well as two real-world tasks on Franka and Bi-Manual Aloha robots. Results show that LoLA significantly outperforms prior state-of-the-art methods (e.g., pi0), particularly in long-horizon manipulation tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6LoLA\uff0c\u901a\u8fc7\u957f\u671f\u7684\u591a\u89c6\u89d2\u89c2\u5bdf\u548c\u673a\u5668\u4eba\u672c\u4f53\u611f\u77e5\uff0c\u63d0\u5347\u673a\u5668\u4eba\u5728\u957f\u65f6\u95f4\u8303\u56f4\u5185\u7684\u64cd\u4f5c\u80fd\u529b\u3002", "motivation": "\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u957f\u65f6\u95f4\u8303\u56f4\u7684\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u89e3\u51b3\u73b0\u6709VLA\u6a21\u578b\u5bf9\u5386\u53f2\u4fe1\u606f\u7684\u5ffd\u89c6\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u4f7f\u7528Vision-Language\u6a21\u578b\u7f16\u7801\u4e0a\u4e0b\u6587\u7279\u5f81\u7684\u6846\u67b6\uff0c\u5e76\u5f15\u5165State-Aware Latent Re-representation\u6a21\u5757\uff0c\u5c06\u89c6\u89c9\u548c\u8bed\u8a00\u6307\u4ee4\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7684\u673a\u5668\u4eba\u52a8\u4f5c\u3002", "result": "LoLA\u6846\u67b6\u80fd\u6709\u6548\u5730\u5904\u7406\u957f\u65f6\u95f4\u8303\u56f4\u7684\u8bed\u8a00\u6307\u5bfc\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\uff0c\u6574\u5408\u4e86\u591a\u89c6\u89d2\u89c2\u5bdf\u548c\u673a\u5668\u4eba\u672c\u4f53\u611f\u77e5\uff0c\u4ee5\u751f\u6210\u8fde\u8d2f\u7684\u64cd\u4f5c\u5e8f\u5217\u3002", "conclusion": "LoLA\u5728\u957f\u65f6\u95f4\u8303\u56f4\u7684\u64cd\u4f5c\u4efb\u52a1\u4e2d\u663e\u8457\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2512.19999", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.19999", "abs": "https://arxiv.org/abs/2512.19999", "authors": ["Kexin Nie", "Xin Tang", "Mengyao Guo", "Ze Gao"], "title": "Stories That Teach: Eastern Wisdom for Human-AI Creative Partnerships", "comment": "4 pages, 1 figure", "summary": "This workshop explores innovative human-AI collaboration methodologies in HCI visual storytelling education through our established \"gap-and-fill\" approach. Drawing on Eastern aesthetic philosophies of intentional emptiness, including Chinese negative-space traditions, Japanese \"ma\" concepts, and contemporary design minimalism, we demonstrate how educators can teach students to maintain creative agency while strategically leveraging AI assistance. During this workshop, participants will experience a structured three-phase methodology: creating a human-led narrative foundation, identifying strategic gaps, and collaborating on AI enhancements. The workshop combines theoretical foundations with intensive hands-on practice, enabling participants to create compelling HCI visual narratives that demonstrate effective human-AI partnership. Through sequential art techniques, storyboarding exercises, and guided AI integration, attendees learn to communicate complex interactive concepts, accessibility solutions, and user experience flows while preserving narrative coherence and creative vision. Building on our successful workshops at ACM C&C 2025, this session specifically addresses the needs of the Chinese HCI community for culturally informed and pedagogically sound approaches to AI integration in creative education.", "AI": {"tldr": "\u901a\u8fc7\u201c\u7f3a\u53e3\u4e0e\u586b\u5145\u201d\u65b9\u6cd5\uff0c\u672c\u7814\u8ba8\u4f1a\u63a2\u8ba8\u4e86\u4eba\u673a\u5408\u4f5c\u5728HCI\u89c6\u89c9\u53d9\u4e8b\u6559\u80b2\u4e2d\u7684\u5e94\u7528\uff0c\u7ed3\u5408\u7406\u8bba\u4e0e\u5b9e\u8df5\uff0c\u5e2e\u52a9\u53c2\u4e0e\u8005\u4fdd\u6301\u521b\u9020\u6027\uff0c\u540c\u65f6\u5229\u7528AI\u589e\u5f3a\u53d9\u4e8b\u80fd\u529b\u3002", "motivation": "\u56de\u5e94\u4e2d\u56fdHCI\u793e\u533a\u5bf9\u6587\u5316\u77e5\u60c5\u548c\u6559\u80b2\u65b9\u6cd5\u7684\u9700\u6c42\uff0c\u4ee5\u4fc3\u8fdb\u521b\u9020\u6027\u6559\u80b2\u4e2d\u7684AI\u6574\u5408\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a\u5efa\u7acb\u53d9\u4e8b\u57fa\u7840\u3001\u8bc6\u522b\u7f3a\u53e3\u3001AI\u589e\u5f3a\u5b9e\u8df5\uff0c\u7ed3\u5408\u7406\u8bba\u548c\u5b9e\u8df5\u3002", "result": "\u672c\u6b21\u7814\u8ba8\u4f1a\u63a2\u8ba8\u4e86\u4eba\u673a\u5408\u4f5c\u5728HCI\u89c6\u89c9\u53d9\u4e8b\u6559\u80b2\u4e2d\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u91c7\u7528\u4e86\u6211\u4eec\u5efa\u7acb\u7684\u201c\u7f3a\u53e3\u4e0e\u586b\u5145\u201d\u65b9\u6cd5\u3002\u7ed3\u5408\u4e1c\u4e9a\u7f8e\u5b66\u54f2\u5b66\uff0c\u5982\u4e2d\u56fd\u7684\u8d1f\u7a7a\u95f4\u4f20\u7edf\u548c\u65e5\u672c\u7684\u201c\u95f4\u201d\uff08ma\uff09\u6982\u5ff5\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u5982\u4f55\u5728\u5229\u7528AI\u8f85\u52a9\u7684\u540c\u65f6\u4fdd\u6301\u5b66\u751f\u7684\u521b\u9020\u6027\u53d1\u6325\u3002\u7814\u8ba8\u4f1a\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a\u5efa\u7acb\u4ee5\u4eba\u4e3a\u4e3b\u7684\u53d9\u4e8b\u57fa\u7840\u3001\u8bc6\u522b\u6218\u7565\u6027\u7f3a\u53e3\uff0c\u4ee5\u53ca\u8fdb\u884cAI\u589e\u5f3a\u7684\u5408\u4f5c\u3002\u901a\u8fc7\u7406\u8bba\u4e0e\u5b9e\u8df5\u76f8\u7ed3\u5408\uff0c\u53c2\u4e0e\u8005\u80fd\u591f\u521b\u5efa\u5f15\u4eba\u5165\u80dc\u7684HCI\u89c6\u89c9\u53d9\u4e8b\uff0c\u5c55\u793a\u6709\u6548\u7684\u4eba\u673a\u5408\u4f5c\u3002\u53c2\u4e0e\u8005\u901a\u8fc7\u8fde\u7eed\u827a\u672f\u6280\u5de7\u3001\u6545\u4e8b\u677f\u7ec3\u4e60\u53ca\u5f15\u5bfc\u5f0fAI\u6574\u5408\uff0c\u5b66\u4e60\u5982\u4f55\u5728\u4fdd\u7559\u53d9\u4e8b\u8fde\u8d2f\u6027\u548c\u521b\u9020\u6027\u89c6\u91ce\u7684\u540c\u65f6\uff0c\u4f20\u8fbe\u590d\u6742\u7684\u4ea4\u4e92\u6982\u5ff5\u3001\u53ef\u8bbf\u95ee\u6027\u89e3\u51b3\u65b9\u6848\u548c\u7528\u6237\u4f53\u9a8c\u6d41\u7a0b\u3002", "conclusion": "\u672c\u6b21\u7814\u8ba8\u4f1a\u4e3a\u4e2d\u56fdHCI\u793e\u533a\u63d0\u4f9b\u4e86\u5207\u5408\u6587\u5316\u80cc\u666f\u548c\u6559\u80b2\u9700\u6c42\u7684\u4eba\u673a\u5408\u4f5cAI\u6574\u5408\u65b9\u6cd5\u3002"}}
{"id": "2512.20188", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20188", "abs": "https://arxiv.org/abs/2512.20188", "authors": ["Teqiang Zou", "Hongliang Zeng", "Yuxuan Nong", "Yifan Li", "Kehui Liu", "Haotian Yang", "Xinyang Ling", "Xin Li", "Lianyang Ma"], "title": "Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation", "comment": null, "summary": "Most Vision-Language-Action (VLA) systems integrate a Vision-Language Model (VLM) for semantic reasoning with an action expert generating continuous action signals, yet both typically run at a single unified frequency. As a result, policy performance is constrained by the low inference speed of large VLMs. This mandatory synchronous execution severely limits control stability and real-time performance in whole-body robotic manipulation, which involves more joints, larger motion spaces, and dynamically changing views. We introduce a truly asynchronous Fast-Slow VLA framework (DuoCore-FS), organizing the system into a fast pathway for high-frequency action generation and a slow pathway for rich VLM reasoning. The system is characterized by two key features. First, a latent representation buffer bridges the slow and fast systems. It stores instruction semantics and action-reasoning representation aligned with the scene-instruction context, providing high-level guidance to the fast pathway. Second, a whole-body action tokenizer provides a compact, unified representation of whole-body actions. Importantly, the VLM and action expert are still jointly trained end-to-end, preserving unified policy learning while enabling asynchronous execution. DuoCore-FS supports a 3B-parameter VLM while achieving 30 Hz whole-body action-chunk generation, approximately three times as fast as prior VLA models with comparable model sizes. Real-world whole-body manipulation experiments demonstrate improved task success rates and significantly enhanced responsiveness compared to synchronous Fast-Slow VLA baselines. The implementation of DuoCore-FS, including training, inference, and deployment, is provided to commercial users by Astribot as part of the Astribot robotic platform.", "AI": {"tldr": "DuoCore-FS\u662f\u4e00\u4e2a\u65b0\u7684\u5f02\u6b65\u6846\u67b6\uff0c\u901a\u8fc7\u8bbe\u7acb\u5feb\u6162\u901a\u9053\u6765\u63d0\u9ad8\u673a\u5668\u4eba\u64cd\u63a7\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\u548c\u53cd\u5e94\u901f\u5ea6\u3002", "motivation": "\u89e3\u51b3\u76ee\u524d\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8\u7cfb\u7edf\u5728\u6267\u884c\u8fc7\u7a0b\u4e2d\u7684\u540c\u6b65\u9650\u5236\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u5b9e\u65f6\u6027\u548c\u63a7\u5236\u7a33\u5b9a\u6027\u3002", "method": "DuoCore-FS\u91c7\u7528\u5feb\u6162\u901a\u9053\u8bbe\u8ba1\uff0c\u5229\u7528\u6f5c\u5728\u8868\u793a\u7f13\u51b2\u533a\u548c\u5168\u8eab\u52a8\u4f5c\u6807\u8bb0\u5668\u6765\u8fde\u63a5VLM\u548c\u52a8\u4f5c\u4e13\u5bb6\uff0c\u786e\u4fdd\u9ad8\u9891\u52a8\u4f5c\u751f\u6210\u4e0e\u8bed\u4e49\u63a8\u7406\u7684\u534f\u540c\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f02\u6b65\u7684Fast-Slow VLA\u6846\u67b6\uff08DuoCore-FS\uff09\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u52a8\u4f5c\u751f\u6210\u548c\u4e30\u5bcc\u7684\u63a8\u7406\u3002", "conclusion": "DuoCore-FS\u80fd\u6709\u6548\u5b9e\u73b0\u673a\u5668\u4eba\u52a8\u4f5c\u7684\u5feb\u901f\u751f\u6210\u4e0e\u8bed\u8a00\u63a8\u7406\uff0c\u540c\u65f6\u4fdd\u6301\u7edf\u4e00\u7684\u653f\u7b56\u5b66\u4e60\uff0c\u5c55\u793a\u4e86\u6bd4\u4ee5\u5f80\u6a21\u578b\u66f4\u4f18\u7684\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002"}}
{"id": "2512.20116", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.20116", "abs": "https://arxiv.org/abs/2512.20116", "authors": ["Yongchan Son", "Jahun Jang", "Been An", "Jimoon Kang", "Eunji Park"], "title": "/UnmuteAll: Modeling Verbal Communication Patterns of Collaborative Contexts in MOBA Games", "comment": null, "summary": "Team communication plays a vital role in supporting collaboration in multiplayer online games. Therefore, numerous studies were conducted to examine communication patterns in esports teams. While non-verbal communication has been extensively investigated, research on assessing voice-based verbal communication patterns remains relatively understudied. In this study, we propose a framework that automatically assesses verbal communication patterns by constructing networks with utterances transcribed from voice recordings. Through a data collection study, we obtained 84 game sessions from five League of Legends teams and subsequently investigated how verbal communication patterns varied across different conditions. As a result, we revealed that esports players exhibited broader and more balanced participation in collaborative situations, increased utterances over time with the largest rise in decision making, and team-level differences that were contingent on effective professional training. Building upon these findings, this study provides a generalizable tool for analyzing effective team communication.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u8bc4\u4f30\u7535\u5b50\u7ade\u6280\u56e2\u961f\u8bed\u97f3\u4ea4\u6d41\u6a21\u5f0f\u7684\u6846\u67b6\uff0c\u53d1\u73b0\u6709\u6548\u6c9f\u901a\u7684\u53c2\u4e0e\u548c\u9891\u7387\u968f\u56e2\u961f\u60c5\u51b5\u800c\u5f02\u3002", "motivation": "\u63a2\u8ba8\u5728\u7535\u5b50\u7ade\u6280\u56e2\u961f\u4e2d\u7684\u8bed\u97f3\u57fa\u7840\u6c9f\u901a\u6a21\u5f0f\uff0c\u4ee5\u586b\u8865\u76f8\u5173\u7814\u7a76\u7684\u7a7a\u767d", "method": "\u6784\u5efa\u7f51\u7edc\u8bc4\u4f30\u8bed\u97f3\u4ea4\u6d41\u6a21\u5f0f", "result": "\u53d1\u73b0\u7535\u5b50\u7ade\u6280\u9009\u624b\u5728\u5408\u4f5c\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u66f4\u5e7f\u6cdb\u548c\u66f4\u5e73\u8861\u7684\u53c2\u4e0e\uff0c\u4ee5\u53ca\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\u53d1\u8a00\u6b21\u6570\u7684\u589e\u52a0\uff0c\u51b3\u7b56\u65f6\u7684\u53d1\u8a00\u589e\u957f\u6700\u4e3a\u663e\u8457\uff0c\u540c\u65f6\u56e2\u961f\u95f4\u5b58\u5728\u53d7\u4e13\u4e1a\u57f9\u8bad\u5f71\u54cd\u7684\u5dee\u5f02", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u63a8\u5e7f\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u5206\u6790\u56e2\u961f\u6c9f\u901a\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2512.20224", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.20224", "abs": "https://arxiv.org/abs/2512.20224", "authors": ["Qijun Qin", "Ziqi Zhang", "Yihan Zhong", "Feng Huang", "Xikun Liu", "Runzhi Hu", "Hang Chen", "Wei Hu", "Dongzhe Su", "Jun Zhang", "Hoi-Fung Ng", "Weisong Wen"], "title": "UrbanV2X: A Multisensory Vehicle-Infrastructure Dataset for Cooperative Navigation in Urban Areas", "comment": "8 pages, 9 figures, IEEE ITSC 2025", "summary": "Due to the limitations of a single autonomous vehicle, Cellular Vehicle-to-Everything (C-V2X) technology opens a new window for achieving fully autonomous driving through sensor information sharing. However, real-world datasets supporting vehicle-infrastructure cooperative navigation in complex urban environments remain rare. To address this gap, we present UrbanV2X, a comprehensive multisensory dataset collected from vehicles and roadside infrastructure in the Hong Kong C-V2X testbed, designed to support research on smart mobility applications in dense urban areas. Our onboard platform provides synchronized data from multiple industrial cameras, LiDARs, 4D radar, ultra-wideband (UWB), IMU, and high-precision GNSS-RTK/INS navigation systems. Meanwhile, our roadside infrastructure provides LiDAR, GNSS, and UWB measurements. The entire vehicle-infrastructure platform is synchronized using the Precision Time Protocol (PTP), with sensor calibration data provided. We also benchmark various navigation algorithms to evaluate the collected cooperative data. The dataset is publicly available at https://polyu-taslab.github.io/UrbanV2X/.", "AI": {"tldr": "UrbanV2X\u6570\u636e\u96c6\u901a\u8fc7\u8f66\u8f86\u4e0e\u57fa\u7840\u8bbe\u65bd\u7684\u4f20\u611f\u5668\u4fe1\u606f\u5171\u4eab\uff0c\u652f\u6301\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u5b9e\u73b0\u81ea\u4e3b\u9a7e\u9a76\u7814\u7a76\u3002", "motivation": "\u63a8\u52a8\u5168\u9762\u81ea\u4e3b\u9a7e\u9a76\u4e0e\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u8f66\u8f86-\u57fa\u7840\u8bbe\u65bd\u5408\u4f5c\u5bfc\u822a\u7814\u7a76", "method": "\u6536\u96c6\u5e76\u540c\u6b65\u591a\u4f20\u611f\u5668\u6570\u636e", "result": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u591a\u4f20\u611f\u5668\u6570\u636e\u96c6UrbanV2X\uff0c\u6db5\u76d6\u8f66\u8f86\u4e0e\u8def\u8fb9\u57fa\u7840\u8bbe\u65bd\u7684\u6570\u636e\uff0c\u5e76\u516c\u5f00\u53ef\u7528", "conclusion": "UrbanV2X\u6570\u636e\u96c6\u4e3a\u667a\u80fd\u51fa\u884c\u5e94\u7528\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u57fa\u7840\uff0c\u5e76\u9f13\u52b1\u5728\u5bc6\u96c6\u57ce\u5e02\u73af\u5883\u4e2d\u5f00\u5c55\u8fdb\u4e00\u6b65\u7684\u63a2\u7d22\u3002"}}
{"id": "2512.20129", "categories": ["cs.HC", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20129", "abs": "https://arxiv.org/abs/2512.20129", "authors": ["Cyrus Vachha", "Yixiao Kang", "Zach Dive", "Ashwat Chidambaram", "Anik Gupta", "Eunice Jun", "Bjoern Hartmann"], "title": "Dreamcrafter: Immersive Editing of 3D Radiance Fields Through Flexible, Generative Inputs and Outputs", "comment": "CHI 2025, Project page: https://dream-crafter.github.io/", "summary": "Authoring 3D scenes is a central task for spatial computing applications. Competing visions for lowering existing barriers are (1) focus on immersive, direct manipulation of 3D content or (2) leverage AI techniques that capture real scenes (3D Radiance Fields such as, NeRFs, 3D Gaussian Splatting) and modify them at a higher level of abstraction, at the cost of high latency. We unify the complementary strengths of these approaches and investigate how to integrate generative AI advances into real-time, immersive 3D Radiance Field editing. We introduce Dreamcrafter, a VR-based 3D scene editing system that: (1) provides a modular architecture to integrate generative AI algorithms; (2) combines different levels of control for creating objects, including natural language and direct manipulation; and (3) introduces proxy representations that support interaction during high-latency operations. We contribute empirical findings on control preferences and discuss how generative AI interfaces beyond text input enhance creativity in scene editing and world building.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDreamcrafter\u7684VR\u57fa\u78403D\u573a\u666f\u7f16\u8f91\u7cfb\u7edf\uff0c\u6574\u5408\u4e86\u751f\u6210\u6027AI\u4e0e\u5b9e\u65f63D Radiance Field\u7f16\u8f91\uff0c\u65e8\u5728\u964d\u4f4e3D\u573a\u666f\u521b\u4f5c\u7684\u95e8\u69db\u3002", "motivation": "\u65e8\u5728\u6574\u5408\u6c89\u6d78\u5f0f3D\u5185\u5bb9\u76f4\u63a5\u64cd\u4f5c\u4e0eAI\u6280\u672f\u7684\u4f18\u52bf\uff0c\u964d\u4f4e\u521b\u5efa3D\u573a\u666f\u7684\u969c\u788d\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u67b6\u6784\uff0c\u96c6\u6210\u751f\u6210\u6027AI\u7b97\u6cd5\uff0c\u540c\u65f6\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u548c\u76f4\u63a5\u64cd\u4f5c\u7684\u591a\u79cd\u63a7\u5236\u65b9\u5f0f\u3002", "result": "\u5b9e\u8bc1\u7814\u7a76\u53d1\u73b0\uff0c\u751f\u6210\u6027AI\u63a5\u53e3\u5728\u573a\u666f\u7f16\u8f91\u548c\u4e16\u754c\u6784\u5efa\u65b9\u9762\u589e\u5f3a\u4e86\u521b\u9020\u6027\u3002", "conclusion": "Dreamcrafter\u901a\u8fc7\u7ed3\u5408\u751f\u6210\u6027AI\u4e0e\u7528\u6237\u63a7\u5236\u9009\u9879\uff0c\u63d0\u5347\u4e863D\u573a\u666f\u7f16\u8f91\u4e2d\u7684\u521b\u9020\u6027\u4e0e\u4ea4\u4e92\u6027\u3002"}}
{"id": "2512.20299", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20299", "abs": "https://arxiv.org/abs/2512.20299", "authors": ["Zhongyu Xia", "Wenhao Chen", "Yongtao Wang", "Ming-Hsuan Yang"], "title": "KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System", "comment": null, "summary": "Visual-language reasoning, driving knowledge, and value alignment are essential for advanced autonomous driving systems. However, existing approaches largely rely on data-driven learning, making it difficult to capture the complex logic underlying decision-making through imitation or limited reinforcement rewards. To address this, we propose KnowVal, a new autonomous driving system that enables visual-language reasoning through the synergistic integration of open-world perception and knowledge retrieval. Specifically, we construct a comprehensive driving knowledge graph that encodes traffic laws, defensive driving principles, and ethical norms, complemented by an efficient LLM-based retrieval mechanism tailored for driving scenarios. Furthermore, we develop a human-preference dataset and train a Value Model to guide interpretable, value-aligned trajectory assessment. Experimental results show that our method substantially improves planning performance while remaining compatible with existing architectures. Notably, KnowVal achieves the lowest collision rate on nuScenes and state-of-the-art results on Bench2Drive.", "AI": {"tldr": "KnowVal\u662f\u4e00\u4e2a\u5148\u8fdb\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f00\u653e\u4e16\u754c\u611f\u77e5\u548c\u77e5\u8bc6\u68c0\u7d22\u7684\u534f\u540c\u96c6\u6210\uff0c\u5b9e\u73b0\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u8fc7\u4e8e\u4f9d\u8d56\u6570\u636e\u9a71\u52a8\u5b66\u4e60\uff0c\u96be\u4ee5\u6355\u6349\u51b3\u7b56\u7684\u590d\u6742\u903b\u8f91\u3002", "method": "\u6784\u5efa\u5168\u9762\u7684\u9a7e\u9a76\u77e5\u8bc6\u56fe\uff0c\u7f16\u7801\u4ea4\u901a\u6cd5\u89c4\u3001\u9a7e\u9a76\u539f\u5219\u548c\u4f26\u7406\u89c4\u8303\uff0c\u5e76\u5f00\u53d1\u57fa\u4e8eLLM\u7684\u68c0\u7d22\u673a\u5236\u548c\u4eba\u7c7b\u504f\u597d\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cKnowVal\u5728nuScenes\u4e0a\u5b9e\u73b0\u4e86\u6700\u4f4e\u7684\u78b0\u649e\u7387\uff0c\u5e76\u5728Bench2Drive\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "KnowVal\u663e\u8457\u63d0\u9ad8\u4e86\u89c4\u5212\u6027\u80fd\uff0c\u5e76\u5728\u4fdd\u6301\u4e0e\u73b0\u6709\u67b6\u6784\u517c\u5bb9\u7684\u540c\u65f6\uff0c\u8fbe\u6210\u4e86\u6700\u4f4e\u7684\u78b0\u649e\u7387\u548c\u6700\u5148\u8fdb\u7684Bench2Drive\u7ed3\u679c\u3002"}}
{"id": "2512.20179", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.20179", "abs": "https://arxiv.org/abs/2512.20179", "authors": ["Dan Chen", "Heye Huang", "Tiantian Chen", "Zheng Li", "Yongji Li", "Yuhui Xu", "Sikai Chen"], "title": "RESPOND: Risk-Enhanced Structured Pattern for LLM-driven Online Node-level Decision-making", "comment": "28 pages, 8 figures", "summary": "Current LLM-based driving agents that rely on unstructured plain-text memory suffer from low-precision scene retrieval and inefficient reflection. To address this limitation, we present RESPOND, a structured decision-making framework for LLM-driven agents grounded in explicit risk patterns. RESPOND represents each ego-centric scene using a unified 5 by 3 matrix that encodes spatial topology and road constraints, enabling consistent and reliable retrieval of spatial risk configurations. Based on this representation, a hybrid rule and LLM decision pipeline is developed with a two-tier memory mechanism. In high-risk contexts, exact pattern matching enables rapid and safe reuse of verified actions, while in low-risk contexts, sub-pattern matching supports personalized driving style adaptation. In addition, a pattern-aware reflection mechanism abstracts tactical corrections from crash and near-miss frames to update structured memory, achieving one-crash-to-generalize learning. Extensive experiments demonstrate the effectiveness of RESPOND. In highway-env, RESPOND outperforms state-of-the-art LLM-based and reinforcement learning based driving agents while producing substantially fewer collisions. With step-wise human feedback, the agent acquires a Sporty driving style within approximately 20 decision steps through sub-pattern abstraction. For real-world validation, RESPOND is evaluated on 53 high-risk cut-in scenarios extracted from the HighD dataset. For each event, intervention is applied immediately before the cut-in and RESPOND re-decides the driving action. Compared to recorded human behavior, RESPOND reduces subsequent risk in 84.9 percent of scenarios, demonstrating its practical feasibility under real-world driving conditions. These results highlight RESPONDs potential for autonomous driving, personalized driving assistance, and proactive hazard mitigation.", "AI": {"tldr": "\u63d0\u51faRESPOND\uff0c\u4e00\u4e2a\u57fa\u4e8e\u7ed3\u6784\u5316\u51b3\u7b56\u7684\u9a7e\u9a76\u4ee3\u7406\uff0c\u6539\u8fdb\u4e86\u573a\u666f\u68c0\u7d22\u4e0e\u53cd\u601d\u6548\u7387\uff0c\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8eLLM\u7684\u9a7e\u9a76\u4ee3\u7406\u5728\u573a\u666f\u68c0\u7d22\u548c\u53cd\u601d\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "method": "\u91c7\u75285x3\u77e9\u9635\u6765\u8868\u793a\u573a\u666f\uff0c\u5e76\u7ed3\u5408\u6df7\u5408\u89c4\u5219\u4e0eLLM\u51b3\u7b56\u7ba1\u9053\uff0c\u5177\u6709\u4e24\u7ea7\u8bb0\u5fc6\u673a\u5236\u3002", "result": "RESPOND\u6846\u67b6\u5728\u9ad8\u901f\u516c\u8def\u73af\u5883\u4e2d\u4f18\u4e8e\u73b0\u6709\u7684LLM\u548c\u5f3a\u5316\u5b66\u4e60\u9a7e\u9a76\u4ee3\u7406\uff0c\u4e14\u78b0\u649e\u6b21\u6570\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "RESPOND\u5728\u81ea\u4e3b\u9a7e\u9a76\u3001\u4e2a\u6027\u5316\u9a7e\u9a76\u8f85\u52a9\u53ca\u4e3b\u52a8\u5371\u9669\u7f13\u89e3\u65b9\u9762\u5c55\u73b0\u4e86\u6f5c\u529b\uff0c\u80fd\u6709\u6548\u51cf\u5c11\u9a7e\u9a76\u4e2d\u7684\u98ce\u9669\u3002"}}
{"id": "2512.20322", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.20322", "abs": "https://arxiv.org/abs/2512.20322", "authors": ["Katsu Uchiyama", "Ryuma Niiyama"], "title": "Pneumatic bladder links with wide range of motion joints for articulated inflatable robots", "comment": "Accepted at IROS2024 (IEEE/RSJ International Conference on Intelligent Robots and Systems)", "summary": "Exploration of various applications is the frontier of research on inflatable robots. We proposed an articulated robots consisting of multiple pneumatic bladder links connected by rolling contact joints called Hillberry joints. The bladder link is made of a double-layered structure of tarpaulin sheet and polyurethane sheet, which is both airtight and flexible in shape. The integration of the Hilberry joint into an inflatable robot is also a new approach. The rolling contact joint allows wide range of motion of $\\pm 150 ^{\\circ}$, the largest among the conventional inflatable joints. Using the proposed mechanism for inflatable robots, we demonstrated moving a 500 g payload with a 3-DoF arm and lifting 3.4 kg and 5 kg payloads with 2-DoF and 1-DoF arms, respectively. We also experimented with a single 3-DoF inflatable leg attached to a dolly to show that the proposed structure worked for legged locomotion.", "AI": {"tldr": "\u7814\u7a76\u4e86\u53ef\u5145\u6c14\u673a\u5668\u4eba\u7684\u591a\u79cd\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u5173\u8282\u7ed3\u6784\uff0c\u8868\u73b0\u51fa\u826f\u597d\u7684\u627f\u8f7d\u80fd\u529b\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u63a2\u7d22\u53ef\u5145\u6c14\u673a\u5668\u4eba\u5728\u4e0d\u540c\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u6539\u5584\u5176\u8fd0\u52a8\u80fd\u529b\u548c\u627f\u8f7d\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7531\u591a\u4e2a\u6c14\u52a8\u6ce1\u56ca\u94fe\u63a5\u7ec4\u6210\u7684\u53ef\u5145\u6c14\u673a\u5668\u4eba\uff0c\u4f7f\u7528\u6eda\u52a8\u63a5\u89e6\u5173\u8282\uff08Hillberry joints\uff09\u4f5c\u4e3a\u8fde\u63a5\uff0c\u63d0\u5347\u4e86\u7075\u6d3b\u6027\u548c\u6d3b\u52a8\u8303\u56f4\u3002", "result": "\u901a\u8fc7\u8be5\u673a\u5236\u5b9e\u73b0\u4e863\u81ea\u7531\u5ea6\u624b\u81c2\u79fb\u52a8500\u514b\u8d1f\u8f7d\uff0c\u5e76\u75282\u81ea\u7531\u5ea6\u548c1\u81ea\u7531\u5ea6\u624b\u81c2\u63d0\u5347\u4e863.4\u516c\u65a4\u548c5\u516c\u65a4\u7684\u8d1f\u8f7d\uff0c\u540c\u65f6\u9a8c\u8bc1\u4e86\u817f\u90e8\u8fd0\u52a8\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u5145\u6c14\u673a\u5668\u4eba\u53ca\u5176\u65b0\u9896\u7684\u5173\u8282\u8bbe\u8ba1\u5c55\u73b0\u4e86\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u627f\u8f7d\u4e0d\u540c\u91cd\u91cf\u7684\u8d1f\u8f7d\u5e76\u5b9e\u73b0\u591a\u81ea\u7531\u5ea6\u7684\u8fd0\u52a8\u3002"}}
{"id": "2512.20181", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.20181", "abs": "https://arxiv.org/abs/2512.20181", "authors": ["Sadia Nasrin Tisha", "Md Nazmus Sakib", "Sanorita Dey"], "title": "Competing or Collaborating? The Role of Hackathon Formats in Shaping Team Dynamics and Project Choices", "comment": null, "summary": "Hackathons have emerged as dynamic platforms for fostering innovation, collaboration, and skill development in the technology sector. Structural differences across hackathon formats raise important questions about how event design can shape student learning experiences and engagement. This study examines two distinct hackathon formats: a gender-specific hackathon (GS) and a regular institutional hackathon (RI). Using a mixed-methods approach, we analyze variations in team dynamics, project themes, role assignments, and environmental settings. Our findings indicate that GS hackathon foster a collaborative and supportive atmosphere, emphasizing personal growth and community learning, with projects often centered on health and well-being. In contrast, RI hackathon tend to promote a competitive, outcome-driven environment, with projects frequently addressing entertainment and environmental sustainability. Based on these insights, we propose a hybrid hackathon model that combines the strengths of both formats to balance competition with inclusivity. This work contributes to the design of more engaging, equitable, and pedagogically effective hackathon experiences.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u6027\u522b\u7279\u5b9a\u548c\u5e38\u89c4\u9ed1\u5ba2\u9a6c\u62c9\u677e\u7684\u683c\u5f0f\u5dee\u5f02\uff0c\u53d1\u73b0\u524d\u8005\u5f3a\u8c03\u652f\u6301\u6027\u5b66\u4e60\u73af\u5883\uff0c\u800c\u540e\u8005\u5219\u66f4\u5177\u7ade\u4e89\u6027\uff0c\u5efa\u8bae\u7ed3\u5408\u4e24\u8005\u4f18\u70b9\u8bbe\u8ba1\u66f4\u5e73\u8861\u7684\u6df7\u5408\u578b\u9ed1\u5ba2\u9a6c\u62c9\u677e\u3002", "motivation": "\u7814\u7a76\u9ed1\u5ba2\u9a6c\u62c9\u677e\u7684\u8bbe\u8ba1\u5982\u4f55\u5f71\u54cd\u5b66\u751f\u7684\u5b66\u4e60\u4f53\u9a8c\u548c\u53c2\u4e0e\u5ea6\uff0c\u63a2\u7d22\u4e0d\u540c\u683c\u5f0f\u7684\u7ed3\u6784\u5dee\u5f02\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u5206\u6790\u4e24\u79cd\u4e0d\u540c\u7684\u9ed1\u5ba2\u9a6c\u62c9\u677e\u683c\u5f0f\uff08\u6027\u522b\u7279\u5b9a\u548c\u5e38\u89c4\uff09\u7684\u56e2\u961f\u52a8\u6001\u3001\u9879\u76ee\u4e3b\u9898\u3001\u89d2\u8272\u5206\u914d\u548c\u73af\u5883\u8bbe\u7f6e\u3002", "result": "\u53d1\u73b0\u6027\u522b\u7279\u5b9a\u9ed1\u5ba2\u9a6c\u62c9\u677e\uff08GS\uff09\u4fc3\u8fdb\u4e86\u534f\u4f5c\u548c\u652f\u6301\u7684\u6c1b\u56f4\uff0c\u800c\u5e38\u89c4\u9ed1\u5ba2\u9a6c\u62c9\u677e\uff08RI\uff09\u66f4\u503e\u5411\u4e8e\u7ade\u4e89\u9a71\u52a8\uff0c\u6709\u52a9\u4e8e\u8bbe\u8ba1\u66f4\u5177\u5438\u5f15\u529b\u548c\u516c\u5e73\u6027\u7684\u9ed1\u5ba2\u9a6c\u62c9\u677e\u4f53\u9a8c\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e24\u79cd\u9ed1\u5ba2\u9a6c\u62c9\u677e\u683c\u5f0f\u4f18\u70b9\u7684\u6df7\u5408\u6a21\u578b\uff0c\u4ee5\u5b9e\u73b0\u7ade\u4e89\u4e0e\u5305\u5bb9\u4e4b\u95f4\u7684\u5e73\u8861\uff0c\u4ece\u800c\u6539\u5584\u53c2\u4e0e\u4f53\u9a8c\u3002"}}
{"id": "2512.20355", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.20355", "abs": "https://arxiv.org/abs/2512.20355", "authors": ["Hao Wei", "Peiji Wang", "Qianhao Wang", "Tong Qin", "Fei Gao", "Yulin Si"], "title": "FAR-AVIO: Fast and Robust Schur-Complement Based Acoustic-Visual-Inertial Fusion Odometry with Sensor Calibration", "comment": null, "summary": "Underwater environments impose severe challenges to visual-inertial odometry systems, as strong light attenuation, marine snow and turbidity, together with weakly exciting motions, degrade inertial observability and cause frequent tracking failures over long-term operation. While tightly coupled acoustic-visual-inertial fusion, typically implemented through an acoustic Doppler Velocity Log (DVL) integrated with visual-inertial measurements, can provide accurate state estimation, the associated graph-based optimization is often computationally prohibitive for real-time deployment on resource-constrained platforms. Here we present FAR-AVIO, a Schur-Complement based, tightly coupled acoustic-visual-inertial odometry framework tailored for underwater robots. FAR-AVIO embeds a Schur complement formulation into an Extended Kalman Filter(EKF), enabling joint pose-landmark optimization for accuracy while maintaining constant-time updates by efficiently marginalizing landmark states. On top of this backbone, we introduce Adaptive Weight Adjustment and Reliability Evaluation(AWARE), an online sensor health module that continuously assesses the reliability of visual, inertial and DVL measurements and adaptively regulates their sigma weights, and we develop an efficient online calibration scheme that jointly estimates DVL-IMU extrinsics, without dedicated calibration manoeuvres. Numerical simulations and real-world underwater experiments consistently show that FAR-AVIO outperforms state-of-the-art underwater SLAM baselines in both localization accuracy and computational efficiency, enabling robust operation on low-power embedded platforms. Our implementation has been released as open source software at https://far-vido.gitbook.io/far-vido-docs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFAR-AVIO\u7684\u58f0\u5b66-\u89c6\u89c9-\u60ef\u6027\u6d4b\u7a0b\u6846\u67b6\uff0c\u9488\u5bf9\u6c34\u4e0b\u673a\u5668\u4eba\u8fdb\u884c\u4f18\u5316\uff0c\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u8ba1\u7b97\u8d1f\u62c5\u3002", "motivation": "\u6c34\u4e0b\u73af\u5883\u4e25\u5cfb\u6311\u6218\u89c6\u89c9-\u60ef\u6027\u6d4b\u7a0b\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u73b0\u6709\u7684\u58f0\u5b66-\u89c6\u89c9-\u60ef\u6027\u878d\u5408\u65b9\u6cd5\u867d\u7136\u51c6\u786e\uff0c\u4f46\u8ba1\u7b97\u5f00\u9500\u6781\u5927\uff0c\u8feb\u5207\u9700\u8981\u4f18\u5316\u4ee5\u9002\u5e94\u4f4e\u8d44\u6e90\u5e73\u53f0\u3002", "method": "\u57fa\u4e8eSchur\u4e92\u8865\u7684\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff0c\u5bf9\u59ff\u6001\u4e0e\u5730\u6807\u72b6\u6001\u8fdb\u884c\u8054\u5408\u4f18\u5316\uff0c\u540c\u65f6\u91c7\u7528\u81ea\u9002\u5e94\u52a0\u6743\u8c03\u6574\u548c\u53ef\u9760\u6027\u8bc4\u4f30\u6a21\u5757\uff0c\u4ee5\u63d0\u4f9b\u5728\u7ebf\u5065\u5eb7\u76d1\u6d4b\u4e0e\u6821\u51c6\u3002", "result": "FAR-AVIO\u5728\u6570\u503c\u6a21\u62df\u548c\u5b9e\u9645\u6c34\u4e0b\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c55\u73b0\u4e86\u9ad8\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "FAR-AVIO\u5728\u6c34\u4e0bSLAM\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4f4e\u529f\u8017\u5d4c\u5165\u5f0f\u5e73\u53f0\uff0c\u63a8\u52a8\u4e86\u6c34\u4e0b\u673a\u5668\u4eba\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.20221", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.20221", "abs": "https://arxiv.org/abs/2512.20221", "authors": ["Sung Park", "Daeho Yoon", "Jungmin Lee"], "title": "The Effect of Empathic Expression Levels in Virtual Human Interaction: A Controlled Experiment", "comment": null, "summary": "As artificial intelligence (AI) systems become increasingly embedded in everyday life, the ability of interactive agents to express empathy has become critical for effective human-AI interaction, particularly in emotionally sensitive contexts. Rather than treating empathy as a binary capability, this study examines how different levels of empathic expression in virtual human interaction influence user experience. We conducted a between-subject experiment (n = 70) in a counseling-style interaction context, comparing three virtual human conditions: a neutral dialogue-based agent, a dialogue-based empathic agent, and a video-based empathic agent that incorporates users' facial cues. Participants engaged in a 15-minute interaction and subsequently evaluated their experience using subjective measures of empathy and interaction quality. Results from analysis of variance (ANOVA) revealed significant differences across conditions in affective empathy, perceived naturalness of facial movement, and appropriateness of facial expression. The video-based empathic expression condition elicited significantly higher affective empathy than the neutral baseline (p < .001) and marginally higher levels than the dialogue-based condition (p < .10). In contrast, cognitive empathy did not differ significantly across conditions. These findings indicate that empathic expression in virtual humans should be conceptualized as a graded design variable, rather than a binary capability, with visually grounded cues playing a decisive role in shaping affective user experience.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u865a\u62df\u4eba\u4e92\u52a8\u4e2d\u4e0d\u540c\u540c\u7406\u8868\u8fbe\u6c34\u5e73\u5bf9\u7528\u6237\u4f53\u9a8c\u7684\u5f71\u54cd\uff0c\u7ed3\u679c\u8868\u660e\u89c6\u9891\u57fa\u7840\u7684\u540c\u7406\u8868\u8fbe\u66f4\u80fd\u5f15\u53d1\u60c5\u611f\u540c\u7406\u5fc3\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4ea4\u4e92\u4ee3\u7406\u8868\u73b0\u51fa\u540c\u7406\u5fc3\u7684\u80fd\u529b\u5bf9\u4e8e\u6709\u6548\u7684\u4eba\u673a\u4ea4\u4e92\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u60c5\u611f\u654f\u611f\u7684\u80cc\u666f\u4e2d\u3002", "method": "\u901a\u8fc7\u5728\u54a8\u8be2\u98ce\u683c\u4ea4\u4e92\u73af\u5883\u4e2d\u8fdb\u884c\u7684\u5b9e\u9a8c\uff0870\u540d\u53c2\u4e0e\u8005\uff09\uff0c\u6bd4\u8f83\u4e09\u79cd\u865a\u62df\u4eba\u6761\u4ef6\u7684\u7528\u6237\u4f53\u9a8c", "result": "\u89c6\u9891\u57fa\u7840\u7684\u540c\u7406\u8868\u8fbe\u6761\u4ef6\u5728\u60c5\u611f\u540c\u7406\u5fc3\u65b9\u9762\u663e\u8457\u9ad8\u4e8e\u4e2d\u7acb\u57fa\u7ebf\uff0c\u800c\u5728\u5bf9\u8bdd\u57fa\u7840\u7684\u6761\u4ef6\u4e0b\u8fb9\u9645\u9ad8\u3002", "conclusion": "\u865a\u62df\u4eba\u4e2d\u7684\u540c\u7406\u8868\u8fbe\u5e94\u89c6\u4e3a\u4e00\u4e2a\u6e10\u8fdb\u7684\u8bbe\u8ba1\u53d8\u91cf\uff0c\u89c6\u89c9\u7ebf\u7d22\u5728\u5851\u9020\u60c5\u611f\u7528\u6237\u4f53\u9a8c\u65b9\u9762\u8d77\u7740\u51b3\u5b9a\u6027\u4f5c\u7528\u3002"}}
{"id": "2512.20475", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.20475", "abs": "https://arxiv.org/abs/2512.20475", "authors": ["Maulana Bisyir Azhari", "Donghun Han", "Je In You", "Sungjun Park", "David Hyunchul Shim"], "title": "Drift-Corrected Monocular VIO and Perception-Aware Planning for Autonomous Drone Racing", "comment": null, "summary": "The Abu Dhabi Autonomous Racing League(A2RL) x Drone Champions League competition(DCL) requires teams to perform high-speed autonomous drone racing using only a single camera and a low-quality inertial measurement unit -- a minimal sensor set that mirrors expert human drone racing pilots. This sensor limitation makes the system susceptible to drift from Visual-Inertial Odometry (VIO), particularly during long and fast flights with aggressive maneuvers. This paper presents the system developed for the championship, which achieved a competitive performance. Our approach corrected VIO drift by fusing its output with global position measurements derived from a YOLO-based gate detector using a Kalman filter. A perception-aware planner generated trajectories that balance speed with the need to keep gates visible for the perception system. The system demonstrated high performance, securing podium finishes across multiple categories: third place in the AI Grand Challenge with top speed of 43.2 km/h, second place in the AI Drag Race with over 59 km/h, and second place in the AI Multi-Drone Race. We detail the complete architecture and present a performance analysis based on experimental data from the competition, contributing our insights on building a successful system for monocular vision-based autonomous drone flight.", "AI": {"tldr": "\u672c\u8bba\u6587\u5c55\u793a\u4e86\u4e00\u79cd\u80fd\u591f\u5728\u4f20\u611f\u5668\u9650\u5236\u4e0b\u8fdb\u884c\u9ad8\u6548\u65e0\u4eba\u673a\u6bd4\u8d5b\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u878d\u5408\u89c6\u89c9\u548c\u5b9a\u4f4d\u6570\u636e\u6765\u4fee\u6b63\u6f02\u79fb\uff0c\u5e76\u5b9e\u73b0\u4e86\u591a\u4e2a\u6bd4\u8d5b\u7c7b\u522b\u7684\u4f18\u5f02\u6210\u7ee9\u3002", "motivation": "\u89e3\u51b3\u65e0\u4eba\u673a\u5728\u5feb\u901f\u98de\u884c\u548c\u6fc0\u70c8\u673a\u52a8\u8fc7\u7a0b\u4e2d\u56e0\u4f20\u611f\u5668\u9650\u5236\u5bfc\u81f4\u7684\u6f02\u79fb\u95ee\u9898\uff0c\u786e\u4fdd\u9ad8\u6027\u80fd\u548c\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u5c06\u89c6\u89c9\u60ef\u6027\u6d4b\u91cf\u8f93\u51fa\u4e0eYOLO\u68c0\u6d4b\u5f97\u5230\u7684\u5168\u5c40\u4f4d\u7f6e\u878d\u5408\uff0c\u4f7f\u7528\u611f\u77e5\u610f\u8bc6\u89c4\u5212\u5668\u751f\u6210\u529f\u80fd\u660e\u786e\u7684\u8f68\u8ff9\u3002", "result": "\u672c\u8bba\u6587\u4ecb\u7ecd\u4e86\u5728\u963f\u5e03\u624e\u6bd4\u65e0\u4eba\u9a7e\u9a76\u8d5b\u8f66\u8054\u76df\uff08A2RL\uff09\u4e0e\u65e0\u4eba\u673a\u51a0\u519b\u8054\u8d5b\uff08DCL\uff09\u6bd4\u8d5b\u4e2d\uff0c\u4f7f\u7528\u5355\u4e00\u76f8\u673a\u548c\u4f4e\u8d28\u91cf\u60ef\u6027\u6d4b\u91cf\u5355\u5143\u8fdb\u884c\u9ad8\u901f\u5ea6\u81ea\u52a8\u9a7e\u9a76\u65e0\u4eba\u673a\u6bd4\u8d5b\u7684\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u6210\u529f\u5730\u901a\u8fc7\u5c06\u89c6\u89c9\u60ef\u6027\u6d4b\u91cf\u8f93\u51fa\u4e0e\u57fa\u4e8eYOLO\u7684\u7f51\u5173\u68c0\u6d4b\u5668\u7684\u5168\u5c40\u5b9a\u4f4d\u6d4b\u91cf\u7ed3\u5408\uff0c\u5e76\u5229\u7528\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u8fdb\u884c\u6f02\u79fb\u4fee\u6b63\u3002", "conclusion": "\u6211\u4eec\u7684\u7cfb\u7edf\u5728\u65f6\u95f4\u8303\u56f4\u5185\u8868\u73b0\u51fa\u8272\uff0c\u901a\u8fc7\u89c6\u89c9\u548c\u5168\u5c40\u4f4d\u7f6e\u7684\u878d\u5408\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u65e0\u4eba\u673a\u81ea\u4e3b\u98de\u884c\uff0c\u4e14\u5728\u591a\u4e2a\u6bd4\u8d5b\u4e2d\u83b7\u5f97\u5956\u9879\u3002"}}
{"id": "2512.20306", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.20306", "abs": "https://arxiv.org/abs/2512.20306", "authors": ["P\u00e9ter Ferenc Gyarmati", "Dominik Moritz", "Torsten M\u00f6ller", "Laura Koesten"], "title": "Structured Visualization Design Knowledge for Grounding Generative Reasoning and Situated Feedback", "comment": null, "summary": "Automated visualization design navigates a tension between symbolic systems and generative models. Constraint solvers enforce structural and perceptual validity, but the rules they require are difficult to author and too rigid to capture situated design knowledge. Large language models require no formal rules and can reason about contextual nuance, but they prioritize popular conventions over empirically grounded best practices. We address this tension by proposing a cataloging scheme that structures visualization design knowledge as natural-language guidelines with semantically typed metadata. This allows experts to author knowledge that machines can query. An expert study ($N=18$) indicates that practitioners routinely adapt heuristics to situational factors such as audience and communicative intent. To capture this reasoning, guideline sections specify not only advice but also the contexts where it applies, exceptions that invalidate it, and the sources from which it derives. We demonstrate the scheme's expressiveness by cataloging 744 guidelines drawn from cognitive science, accessibility standards, data journalism, and research on rhetorical aspects of visual communication. We embed guideline sections in a vector space, opening the knowledge itself to structural analysis. This reveals conflicting advice across sources and transferable principles between domains. Rather than replacing constraint-based tools, our scheme provides what they lack: situated guidance that generative systems can retrieve to ground their reasoning, users can verify against cited sources, and experts can author as knowledge evolves.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u76ee\u5f55\u65b9\u6848\uff0c\u5229\u7528\u81ea\u7136\u8bed\u8a00\u548c\u8bed\u4e49\u5143\u6570\u636e\u7ed3\u6784\u5316\u53ef\u89c6\u5316\u8bbe\u8ba1\u77e5\u8bc6\uff0c\u4ee5\u63d0\u4f9b\u7075\u6d3b\u7684\u8bbe\u8ba1\u6307\u5bfc\uff0c\u4fc3\u8fdb\u751f\u6210\u7cfb\u7edf\u7684\u63a8\u7406\u5e76\u9a8c\u8bc1\u4e13\u5bb6\u77e5\u8bc6\u3002", "motivation": "\u89e3\u51b3\u7b26\u53f7\u7cfb\u7edf\u4e0e\u751f\u6210\u6a21\u578b\u4e4b\u95f4\u7684\u5f20\u529b\uff0c\u901a\u8fc7\u63d0\u4f9b\u7075\u6d3b\u7684\u8bbe\u8ba1\u6307\u5bfc\u6765\u63d0\u5347\u81ea\u52a8\u5316\u53ef\u89c6\u5316\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u3002", "method": "\u901a\u8fc7\u5c06\u53ef\u89c6\u5316\u8bbe\u8ba1\u77e5\u8bc6\u7ed3\u6784\u5316\u4e3a\u5177\u6709\u8bed\u4e49\u7c7b\u578b\u5143\u6570\u636e\u7684\u81ea\u7136\u8bed\u8a00\u6307\u5357\uff0c\u8fdb\u884c\u4e13\u5bb6\u7814\u7a76\u4ee5\u89c2\u5bdf\u5b9e\u8df5\u8005\u5982\u4f55\u9002\u5e94\u4e0d\u540c\u60c5\u5883\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6848\u5c55\u793a\u4e86744\u6761\u6307\u5357\uff0c\u5e76\u63ed\u793a\u4e86\u4e0d\u540c\u6765\u6e90\u4e4b\u95f4\u7684\u51b2\u7a81\u5efa\u8bae\u4ee5\u53ca\u9886\u57df\u95f4\u7684\u53ef\u8f6c\u79fb\u539f\u5219\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u7684\u76ee\u5f55\u65b9\u6848\u80fd\u591f\u6355\u6349\u8bbe\u8ba1\u77e5\u8bc6\uff0c\u4f7f\u5f97\u4e13\u5bb6\u53ef\u4ee5\u521b\u9020\u7075\u6d3b\u4e14\u53ef\u67e5\u8be2\u7684\u81ea\u7136\u8bed\u8a00\u6307\u5357\u3002"}}
{"id": "2512.20591", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.20591", "abs": "https://arxiv.org/abs/2512.20591", "authors": ["Changyi Lin", "Boda Huo", "Mingyang Yu", "Emily Ruppel", "Bingqing Chen", "Jonathan Francis", "Ding Zhao"], "title": "LightTact: A Visual-Tactile Fingertip Sensor for Deformation-Independent Contact Sensing", "comment": null, "summary": "Contact often occurs without macroscopic surface deformation, such as during interaction with liquids, semi-liquids, or ultra-soft materials. Most existing tactile sensors rely on deformation to infer contact, making such light-contact interactions difficult to perceive robustly. To address this, we present LightTact, a visual-tactile fingertip sensor that makes contact directly visible via a deformation-independent, optics-based principle. LightTact uses an ambient-blocking optical configuration that suppresses both external light and internal illumination at non-contact regions, while transmitting only the diffuse light generated at true contacts. As a result, LightTact produces high-contrast raw images in which non-contact pixels remain near-black (mean gray value < 3) and contact pixels preserve the natural appearance of the contacting surface. Built on this, LightTact achieves accurate pixel-level contact segmentation that is robust to material properties, contact force, surface appearance, and environmental lighting. We further integrate LightTact on a robotic arm and demonstrate manipulation behaviors driven by extremely light contact, including water spreading, facial-cream dipping, and thin-film interaction. Finally, we show that LightTact's spatially aligned visual-tactile images can be directly interpreted by existing vision-language models, enabling resistor value reasoning for robotic sorting.", "AI": {"tldr": "LightTact\u662f\u4e00\u79cd\u65b0\u578b\u7684\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u5668\uff0c\u53ef\u5728\u65e0\u53d8\u5f62\u7684\u60c5\u51b5\u4e0b\u76f4\u63a5\u53ef\u89c6\u5316\u63a5\u89e6\uff0c\u9002\u7528\u4e8e\u8f7b\u89e6\u4ea4\u4e92\u3002", "motivation": "\u73b0\u6709\u89e6\u89c9\u4f20\u611f\u5668\u4e3b\u8981\u4f9d\u8d56\u53d8\u5f62\u6765\u63a8\u65ad\u63a5\u89e6\uff0c\u96be\u4ee5\u611f\u77e5\u8f7b\u63a5\u89e6\u4ea4\u4e92\uff0c\u56e0\u6b64\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u4f20\u611f\u5668\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "LightTact\u91c7\u7528\u73af\u5883\u5c4f\u853d\u7684\u5149\u5b66\u914d\u7f6e\uff0c\u6291\u5236\u975e\u63a5\u89e6\u533a\u57df\u7684\u5149\u7167\uff0c\u53ea\u4f20\u8f93\u771f\u5b9e\u63a5\u89e6\u4ea7\u751f\u7684\u6563\u5c04\u5149\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLightTact\u7684\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u5668\uff0c\u80fd\u591f\u5728\u65e0\u5b8f\u89c2\u8868\u9762\u53d8\u5f62\u7684\u60c5\u51b5\u4e0b\u68c0\u6d4b\u63a5\u89e6\u3002", "conclusion": "LightTact\u51ed\u501f\u5176\u5149\u5b66\u539f\u7406\uff0c\u80fd\u591f\u6709\u6548\u5730\u68c0\u6d4b\u8f7b\u5fae\u63a5\u89e6\uff0c\u5e76\u5728\u591a\u79cd\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2512.20584", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.20584", "abs": "https://arxiv.org/abs/2512.20584", "authors": ["Sharareh Mirzaei", "Stephanie Bunt", "Susan M Bogus"], "title": "A human-centered approach to reframing job satisfaction in the BIM-enabled construction industry", "comment": null, "summary": "As the construction industry undergoes rapid digital transformation, ensuring that new technologies enhance rather than hinder human experience has become essential. The inclusion of Building Information Modeling (BIM) plays a central role in this shift, yet its influence on job satisfaction remains underexplored. In response, this study developed a human-centered measurement model for evaluating job satisfaction in BIM work environments by adapting Hackman and Oldham's Job Characteristics Model for the architecture, engineering, and construction (AEC) industry to create a survey that captured industry perspectives on BIM use and job satisfaction. The model uses Partial Least Squares Structural Equation Modeling to analyze the survey results and identify what dimensions of BIM-related work affect job satisfaction. While it was hypothesized that BIM use increases job satisfaction, the results show that only some dimensions of BIM use positively impact BIM job satisfaction; the use of BIM does not guarantee an increase in overall job satisfaction. Additionally, more frequent BIM use was not associated with higher satisfaction levels. These findings suggest that in the AEC industry, sustainable job satisfaction depends less on technological autonomy and more on human-centric factors, particularly collaboration and meaningful engagement within digital workflows.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5efa\u7b51\u4fe1\u606f\u6a21\u578b(BIM)\u5728\u5efa\u7b51\u884c\u4e1a\u4e2d\u5bf9\u5de5\u4f5c\u6ee1\u610f\u5ea6\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u4e86BIM\u4f7f\u7528\u4e0e\u5de5\u4f5c\u6ee1\u610f\u5ea6\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\u3002", "motivation": "\u968f\u7740\u5efa\u7b51\u884c\u4e1a\u7684\u6570\u5b57\u5316\u8f6c\u578b\uff0c\u786e\u4fdd\u65b0\u6280\u672f\u63d0\u5347\u4eba\u7c7b\u4f53\u9a8c\u800c\u975e\u963b\u788d\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0cBIM\u7684\u5f71\u54cd\u7279\u5b9a\u4e8e\u5de5\u4f5c\u6ee1\u610f\u5ea6\u4e9f\u5f85\u7814\u7a76\u3002", "method": "\u91c7\u7528Hackman\u548cOldham\u7684\u5de5\u4f5c\u7279\u5f81\u6a21\u578b\uff0c\u521b\u5efa\u8c03\u67e5\u95ee\u5377\u5e76\u7528\u504f\u6700\u5c0f\u4e8c\u4e58\u7ed3\u6784\u65b9\u7a0b\u6a21\u578b\u5206\u6790\u8c03\u67e5\u7ed3\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cBIM\u7684\u4f7f\u7528\u5e76\u4e0d\u5fc5\u7136\u589e\u52a0\u6574\u4f53\u5de5\u4f5c\u6ee1\u610f\u5ea6\uff0c\u4ec5\u6709\u90e8\u5206BIM\u76f8\u5173\u7ef4\u5ea6\u5bf9\u5de5\u4f5c\u6ee1\u610f\u5ea6\u4ea7\u751f\u6b63\u9762\u5f71\u54cd\u3002", "conclusion": "\u5728AEC\u884c\u4e1a\uff0c\u6301\u7eed\u7684\u5de5\u4f5c\u6ee1\u610f\u5ea6\u66f4\u4f9d\u8d56\u4e8e\u4eba\u6027\u5316\u56e0\u7d20\uff0c\u5982\u5408\u4f5c\u548c\u5728\u6570\u5b57\u5de5\u4f5c\u6d41\u4e2d\u7684\u6709\u610f\u4e49\u7684\u53c2\u4e0e\uff0c\u800c\u975e\u6280\u672f\u81ea\u4e3b\u6027\u3002"}}
