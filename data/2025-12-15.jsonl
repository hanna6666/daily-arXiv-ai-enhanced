{"id": "2512.10960", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10960", "abs": "https://arxiv.org/abs/2512.10960", "authors": ["Ethan Obie Romero-Severson", "Tara Harvey", "Nick Generous", "Phillip M. Mach"], "title": "Measuring skill-based uplift from AI in a real biological laboratory", "comment": null, "summary": "Understanding how AI systems are used by people in real situations that mirror aspects of both legitimate and illegitimate use is key to predicting the risks and benefits of AI systems. This is especially true in biological applications, where skill rather than knowledge is often the primary barrier for an untrained person. The challenge is that these studies are difficult to execute well and can take months to plan and run.\n  Here we report the results of a pilot study that attempted to empirically measure the magnitude of \\emph{skills-based uplift} caused by access to an AI reasoning model, compared with a control group that had only internet access. Participants -- drawn from a diverse pool of Los Alamos National Laboratory employees with no prior wet-lab experience -- were asked to transform \\ecoli{} with a provided expression construct, induce expression of a reporter peptide, and have expression confirmed by mass spectrometry.\n  We recorded quantitative outcomes (e.g., successful completion of experimental segments) and qualitative observations about how participants interacted with the AI system, the internet, laboratory equipment, and one another. We present the results of the study and lessons learned in designing and executing this type of study, and we discuss these results in the context of future studies of the evolving relationship between AI and global biosecurity."}
{"id": "2512.10961", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10961", "abs": "https://arxiv.org/abs/2512.10961", "authors": ["Tao An"], "title": "AI as Cognitive Amplifier: Rethinking Human Judgment in the Age of Generative AI", "comment": "8 pages, 7 figures. Position paper based on field observations from training 500+ professionals since 2023", "summary": "Through extensive experience training professionals and individual users in AI tool adoption since the GPT-3 era, I have observed a consistent pattern: the same AI tool produces dramatically different results depending on who uses it. While some frame AI as a replacement for human intelligence, and others warn of cognitive decline, this position paper argues for a third perspective grounded in practical observation: AI as a cognitive amplifier that magnifies existing human capabilities rather than substituting for them. Drawing on research in human-computer interaction, cognitive augmentation theory, and educational technology, alongside field observations from corporate training across writing, software development, and data analysis domains, I present a framework positioning AI tools as intelligence amplification systems where output quality depends fundamentally on user expertise and judgment. Through analysis of empirical studies on expert-novice differences and systematic observations from professional training contexts, I demonstrate that domain knowledge, quality judgment, and iterative refinement capabilities create substantial performance gaps between users. I propose a three-level model of AI engagement -- from passive acceptance through iterative collaboration to cognitive direction -- and argue that the transition between levels requires not technical training but development of domain expertise and metacognitive skills. This position has critical implications for workforce development and AI system design. Rather than focusing solely on AI literacy or technical prompt engineering, I advocate for integrated approaches that strengthen domain expertise, evaluative judgment, and reflective practice."}
{"id": "2512.11065", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.11065", "abs": "https://arxiv.org/abs/2512.11065", "authors": ["Marcelo Fransoy", "Alejandro Hossian", "Hernán Merlino"], "title": "Immutable Explainability: Towards Verifiable and Auditable Affective AI", "comment": null, "summary": "Affective artificial intelligence has made substantial advances in recent years; yet two critical issues persist, particularly in sensitive applications. First, these systems frequently operate as 'black boxes', leaving their decision-making processes opaque. Second, audit logs often lack reliability, as the entity operating the system may alter them. In this work, we introduce the concept of Immutable Explainability, an architecture designed to address both challenges simultaneously. Our approach combines an interpretable inference engine - implemented through fuzzy logic to produce a transparent trace of each decision - with a cryptographic anchoring mechanism that records this trace on a blockchain, ensuring that it is tamper-evident and independently verifiable. To validate the approach, we implemented a heuristic pipeline integrating lexical and prosodic analysis within an explicit Mamdani-type multimodal fusion engine. Each inference generates an auditable record that is subsequently anchored on a public blockchain (Sepolia Testnet). We evaluated the system using the Spanish MEACorpus 2023, employing both the original corpus transcriptions and those generated by Whisper. The results show that our fuzzy-fusion approach outperforms baseline methods (linear and unimodal fusion). Beyond these quantitative outcomes, our primary objective is to establish a foundation for affective AI systems that offer transparent explanations, trustworthy audit trails, and greater user control over personal data."}
{"id": "2512.11096", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.11096", "abs": "https://arxiv.org/abs/2512.11096", "authors": ["Ben Wang", "Jiqun Liu"], "title": "Your plan may succeed, but what about failure? Investigating how people use ChatGPT for long-term life task planning", "comment": null, "summary": "Long-term life task planning is inherently complex and uncertain, yet little is known about how emerging AI systems support this process. This study investigates how people use ChatGPT for such planning tasks, focusing on user practices, uncertainties, and perceptions of AI assistance. We conducted an interview study with 14 participants who engaged in long-term planning activities using ChatGPT, combining analysis of their prompts and interview responses. The task topics across diverse domains, including personal well-being, event planning, and professional learning, along with prompts to initiate, refine, and contextualize plans. ChatGPT helped structure complex goals into manageable steps, generate ideas, and sustain motivation, serving as a reflective partner. Yet its outputs were often generic or idealized, lacking personalization, contextual realism, and adaptability, requiring users to actively adapt and verify results. Participants expressed a need for AI systems that provide adaptive and trustworthy guidance while acknowledging uncertainty and potential failure in long-term planning. Our findings show how AI supports long-term life task planning under evolving uncertainty and highlight design implications for systems that are adaptive, uncertainty-aware, and capable of supporting long-term planning as an evolving human-AI collaboration."}
{"id": "2512.11047", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11047", "abs": "https://arxiv.org/abs/2512.11047", "authors": ["Haoran Jiang", "Jin Chen", "Qingwen Bu", "Li Chen", "Modi Shi", "Yanjie Zhang", "Delong Li", "Chuanzhe Suo", "Chuang Wang", "Zhihui Peng", "Hongyang Li"], "title": "WholeBodyVLA: Towards Unified Latent VLA for Whole-Body Loco-Manipulation Control", "comment": null, "summary": "Humanoid robots require precise locomotion and dexterous manipulation to perform challenging loco-manipulation tasks. Yet existing approaches, modular or end-to-end, are deficient in manipulation-aware locomotion. This confines the robot to a limited workspace, preventing it from performing large-space loco-manipulation. We attribute this to: (1) the challenge of acquiring loco-manipulation knowledge due to the scarcity of humanoid teleoperation data, and (2) the difficulty of faithfully and reliably executing locomotion commands, stemming from the limited precision and stability of existing RL controllers. To acquire richer loco-manipulation knowledge, we propose a unified latent learning framework that enables Vision-Language-Action (VLA) system to learn from low-cost action-free egocentric videos. Moreover, an efficient human data collection pipeline is devised to augment the dataset and scale the benefits. To more precisely execute the desired locomotion commands, we present a loco-manipulation-oriented (LMO) RL policy specifically tailored for accurate and stable core loco-manipulation movements, such as advancing, turning, and squatting. Building on these components, we introduce WholeBodyVLA, a unified framework for humanoid loco-manipulation. To the best of our knowledge, WholeBodyVLA is one of its kind enabling large-space humanoid loco-manipulation. It is verified via comprehensive experiments on the AgiBot X2 humanoid, outperforming prior baseline by 21.3%. It also demonstrates strong generalization and high extensibility across a broad range of tasks."}
{"id": "2512.11105", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.11105", "abs": "https://arxiv.org/abs/2512.11105", "authors": ["Youngseung Jeon", "Christopher Hwang", "Ziwen Li", "Taylor Le Lievre", "Jesus J. Campagna", "Cohn Whitaker", "Varghese John", "Eunice Jun", "Xiang Anthony Chen"], "title": "Supporting Medicinal Chemists in Iterative Hypothesis Generation for Drug Target Identification", "comment": null, "summary": "While drug discovery is vital for human health, the process remains inefficient. Medicinal chemists must navigate a vast protein space to identify target proteins that meet three criteria: physical and functional interactions, therapeutic impact, and docking potential. Prior approaches have provided fragmented support for each criterion, limiting the generation of promising hypotheses for wet-lab experiments. We present HAPPIER, an AI-powered tool that supports hypothesis generation with integrated multi-criteria support for target identification. HAPPIER enables medicinal chemists to 1) efficiently explore and verify proteins in a single integrated graph component showing multi-criteria satisfaction and 2) validate AI suggestions with domain knowledge. These capabilities facilitate iterative cycles of divergent and convergent thinking, essential for hypothesis generation. We evaluated HAPPIER with ten medicinal chemists, finding that it increased the number of high-confidence hypotheses and support for the iterative cycle, and further demonstrated the relationship between engaging in such cycles and confidence in outputs."}
{"id": "2512.11080", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11080", "abs": "https://arxiv.org/abs/2512.11080", "authors": ["Cedric-Pascal Sommer", "Robert J. Wood", "Justin Werfel"], "title": "Taxonomy and Modular Tool System for Versatile and Effective Non-Prehensile Manipulations", "comment": "34 pages, 10 figures, 2 tables, supplementary videos: https://youtu.be/Hcefy53PY0M, https://youtu.be/nFF9k91hsfU, https://youtu.be/EulPLskNIZQ", "summary": "General-purpose robotic end-effectors of limited complexity, like the parallel-jaw gripper, are appealing for their balance of simplicity and effectiveness in a wide range of manipulation tasks. However, while many such manipulators offer versatility in grasp-like interactions, they are not optimized for non-prehensile actions like pressing, rubbing, or scraping -- manipulations needed for many common tasks. To perform such tasks, humans use a range of different body parts or tools with different rigidity, friction, etc., according to the properties most effective for a given task. Here, we discuss a taxonomy for the key properties of a non-actuated end-effector, laying the groundwork for a systematic understanding of the affordances of non-prehensile manipulators. We then present a modular tool system, based on the taxonomy, that can be used by a standard two-fingered gripper to extend its versatility and effectiveness in performing such actions. We demonstrate the application of the tool system in aerospace and household scenarios that require a range of non-prehensile and prehensile manipulations."}
{"id": "2512.11245", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.11245", "abs": "https://arxiv.org/abs/2512.11245", "authors": ["Zikang Chen", "Tan Xie", "Qinchuan Wang", "Heming Zheng", "Xudong Lu"], "title": "Breast-Rehab: A Postoperative Breast Cancer Rehabilitation Training Assessment System Based on Human Action Recognition", "comment": "has been accepted by CHIP 2025", "summary": "Postoperative upper limb dysfunction is prevalent among breast cancer survivors, yet their adherence to at-home rehabilitation exercises is low amidst limited nursing resources. The hardware overhead of commonly adopted VR-based mHealth solutions further hinders their widespread clinical application. Therefore, we developed Breast-Rehab, a novel, low-cost mHealth system to provide patients with out-of-hospital upper limb rehabilitation management. Breast-Rehab integrates a bespoke human action recognition algorithm with a retrieval-augmented generation (RAG) framework. By fusing visual and 3D skeletal data, our model accurately segments exercise videos recorded in uncontrolled home environments, outperforming standard models. These segmented clips, combined with a domain-specific knowledge base, guide a multi-modal large language model to generate clinically relevant assessment reports. This approach significantly reduces computational overhead and mitigates model hallucinations. We implemented the system as a WeChat Mini Program and a nurse-facing dashboard. A preliminary clinical study validated the system's feasibility and user acceptance, with patients achieving an average exercise frequency of 0.59 sessions/day over a two-week period. This work thus presents a complete, validated pipeline for AI-driven, at-home rehabilitation monitoring."}
{"id": "2512.11125", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.11125", "abs": "https://arxiv.org/abs/2512.11125", "authors": ["Benedictus C. G. Cinun", "Tua A. Tamba", "Immanuel R. Santjoko", "Xiaofeng Wang", "Michael A. Gunarso", "Bin Hu"], "title": "Design and Experimental Validation of Closed-Form CBF-Based Safe Control for Stewart Platform Under Multiple Constraints", "comment": "9 pages", "summary": "This letter presents a closed-form solution of Control Barrier Function (CBF) framework for enforcing safety constraints on a Stewart robotic platform. The proposed method simultaneously handles multiple position and velocity constraints through an explicit closed-form control law, eliminating the need to solve a Quadratic Program (QP) at every control step and enabling efficient real-time implementation. This letter derives necessary and sufficient conditions under which the closed-form expression remains non-singular, thereby ensuring well-posedness of the CBF solution to multi-constraint problem. The controller is validated in both simulation and hardware experiments on a custom-built Stewart platform prototype, demonstrating safetyguaranteed performance that is comparable to the QP-based formulation, while reducing computation time by more than an order of magnitude. The results confirm that the proposed approach provides a reliable and computationally lightweight framework for real-time safe control of parallel robotic systems. The experimental videos are available on the project website. (https://nail-uh.github.io/StewartPlatformSafeControl.github.io/)"}
{"id": "2512.11276", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11276", "abs": "https://arxiv.org/abs/2512.11276", "authors": ["Kellie Yu Hui Sim", "Pin Sym Foong", "Chenyu Zhao", "Melanie Yi Ning Quek", "Swarangi Subodh Mehta", "Kenny Tsu Wei Choo"], "title": "Words to Describe What I'm Feeling: Exploring the Potential of AI Agents for High Subjectivity Decisions in Advance Care Planning", "comment": "31 pages, 10 figures", "summary": "Serious illness can deprive patients of the capacity to speak for themselves. As populations age and caregiver networks shrink, the need for reliable support in Advance Care Planning (ACP) grows. To probe this fraught design space of using proxy agents for high-risk, high-subjectivity decisions, we built an experience prototype (\\acpagent{}) and asked 15 participants in 4 workshops to train it to be their personal proxy in ACP decisions. We analysed their coping strategies and feature requests and mapped the results onto axes of agent autonomy and human control. Our findings argue for a potential new role of AI in ACP where agents act as personal advocates for individuals, building mutual intelligibility over time. We conclude with design recommendations to balance the risks and benefits of such an agent."}
{"id": "2512.11173", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11173", "abs": "https://arxiv.org/abs/2512.11173", "authors": ["Tzu-Hsien Lee", "Fidan Mahmudova", "Karthik Desingh"], "title": "Learning Category-level Last-meter Navigation from RGB Demonstrations of a Single-instance", "comment": null, "summary": "Achieving precise positioning of the mobile manipulator's base is essential for successful manipulation actions that follow. Most of the RGB-based navigation systems only guarantee coarse, meter-level accuracy, making them less suitable for the precise positioning phase of mobile manipulation. This gap prevents manipulation policies from operating within the distribution of their training demonstrations, resulting in frequent execution failures. We address this gap by introducing an object-centric imitation learning framework for last-meter navigation, enabling a quadruped mobile manipulator robot to achieve manipulation-ready positioning using only RGB observations from its onboard cameras. Our method conditions the navigation policy on three inputs: goal images, multi-view RGB observations from the onboard cameras, and a text prompt specifying the target object. A language-driven segmentation module and a spatial score-matrix decoder then supply explicit object grounding and relative pose reasoning. Using real-world data from a single object instance within a category, the system generalizes to unseen object instances across diverse environments with challenging lighting and background conditions. To comprehensively evaluate this, we introduce two metrics: an edge-alignment metric, which uses ground truth orientation, and an object-alignment metric, which evaluates how well the robot visually faces the target. Under these metrics, our policy achieves 73.47% success in edge-alignment and 96.94% success in object-alignment when positioning relative to unseen target objects. These results show that precise last-meter navigation can be achieved at a category-level without depth, LiDAR, or map priors, enabling a scalable pathway toward unified mobile manipulation. Project page: https://rpm-lab-umn.github.io/category-level-last-meter-nav/"}
{"id": "2512.11295", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11295", "abs": "https://arxiv.org/abs/2512.11295", "authors": ["Nattaya Mairittha", "Gabriel Phorncharoenmusikul", "Sorawit Worapradidth"], "title": "AI Autonomy or Human Dependency? Defining the Boundary in Responsible AI with the $α$-Coefficient", "comment": null, "summary": "The integrity of contemporary AI systems is undermined by a critical design flaw: the misappropriation of Human-in-the-Loop (HITL) models to mask systems that are fundamentally reliant on human labor. We term this structural reliance Human-Instead-of-AI (HISOAI). HISOAI systems represent an ethical failure and an unsustainable economic dependency, where human workers function as hidden operational fallbacks rather than strategic collaborators. To rectify this, we propose the AI-First, Human-Empowered (AFHE) paradigm. AFHE mandates a technological design where the AI component must achieve a minimum, quantifiable level of functional independence prior to deployment. This standard is formalized through the AI Autonomy Coefficient (alpha), a metric that determines the proportion of tasks that the AI successfully processes without mandatory human substitution. We introduce the AFHE Deployment Algorithm, an algorithmic gate that requires the system to meet a specified alpha threshold across both offline and shadow testing. By enforcing this structural separation, the AFHE framework redefines the human's role to focus exclusively on high-value tasks, including ethical oversight, boundary pushing, and strategic model tuning, thereby ensuring true system transparency and operational independence. This work advocates for a critical shift toward metric-driven, structurally sound AI architecture, moving the industry beyond deceptive human dependency toward verifiable autonomy."}
{"id": "2512.11218", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11218", "abs": "https://arxiv.org/abs/2512.11218", "authors": ["Kechun Xu", "Zhenjie Zhu", "Anzhe Chen", "Shuqi Zhao", "Qing Huang", "Yifei Yang", "Haojian Lu", "Rong Xiong", "Masayoshi Tomizuka", "Yue Wang"], "title": "Seeing to Act, Prompting to Specify: A Bayesian Factorization of Vision Language Action Policy", "comment": null, "summary": "The pursuit of out-of-distribution generalization in Vision-Language-Action (VLA) models is often hindered by catastrophic forgetting of the Vision-Language Model (VLM) backbone during fine-tuning. While co-training with external reasoning data helps, it requires experienced tuning and data-related overhead. Beyond such external dependencies, we identify an intrinsic cause within VLA datasets: modality imbalance, where language diversity is much lower than visual and action diversity. This imbalance biases the model toward visual shortcuts and language forgetting. To address this, we introduce BayesVLA, a Bayesian factorization that decomposes the policy into a visual-action prior, supporting seeing-to-act, and a language-conditioned likelihood, enabling prompt-to-specify. This inherently preserves generalization and promotes instruction following. We further incorporate pre- and post-contact phases to better leverage pre-trained foundation models. Information-theoretic analysis formally validates our effectiveness in mitigating shortcut learning. Extensive experiments show superior generalization to unseen instructions, objects, and environments compared to existing methods. Project page is available at: https://xukechun.github.io/papers/BayesVLA."}
{"id": "2512.11472", "categories": ["cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11472", "abs": "https://arxiv.org/abs/2512.11472", "authors": ["David Wagmann", "Matti Krüger", "Chao Wang", "Jürgen Steimle"], "title": "Mirror Skin: In Situ Visualization of Robot Touch Intent on Robotic Skin", "comment": null, "summary": "Effective communication of robotic touch intent is a key factor in promoting safe and predictable physical human-robot interaction (pHRI). While intent communication has been widely studied, existing approaches lack the spatial specificity and semantic depth necessary to convey robot touch actions. We present Mirror Skin, a cephalopod-inspired concept that utilizes high-resolution, mirror-like visual feedback on robotic skin. By mapping in-situ visual representations of a human's body parts onto the corresponding robot's touch region, Mirror Skin communicates who shall initiate touch, where it will occur, and when it is imminent. To inform the design of Mirror Skin, we conducted a structured design exploration with experts in virtual reality (VR), iteratively refining six key dimensions. A subsequent controlled user study demonstrated that Mirror Skin significantly enhances accuracy and reduces response times for interpreting touch intent. These findings highlight the potential of visual feedback on robotic skin to communicate human-robot touch interactions."}
{"id": "2512.11249", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.11249", "abs": "https://arxiv.org/abs/2512.11249", "authors": ["Chandra Raskoti", "Weizi Li"], "title": "Elevation Aware 2D/3D Co-simulation Framework for Large-scale Traffic Flow and High-fidelity Vehicle Dynamics", "comment": null, "summary": "Reliable testing of autonomous driving systems requires simulation environments that combine large-scale traffic modeling with realistic 3D perception and terrain. Existing tools rarely capture real-world elevation, limiting their usefulness in cities with complex topography. This paper presents an automated, elevation-aware co-simulation framework that integrates SUMO with CARLA using a pipeline that fuses OpenStreetMap road networks and USGS elevation data into physically consistent 3D environments. The system generates smooth elevation profiles, validates geometric accuracy, and enables synchronized 2D-3D simulation across platforms. Demonstrations on multiple regions of San Francisco show the framework's scalability and ability to reproduce steep and irregular terrain. The result is a practical foundation for high-fidelity autonomous vehicle testing in realistic, elevation-rich urban settings."}
{"id": "2512.11564", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.11564", "abs": "https://arxiv.org/abs/2512.11564", "authors": ["Ziming Li", "Joffrey Guilmet", "Suzanne Sorli", "Hai-Ning Liang", "Diego Monteiro"], "title": "Say it or AI it: Evaluating Hands-Free Text Correction in Virtual Reality", "comment": null, "summary": "Text entry in Virtual Reality (VR) is challenging, even when accounting for the use of controllers. Prior work has tackled this challenge head-on, improving the efficiency of input methods. These techniques have the advantage of allowing for relatively straightforward text correction. However, text correction without the use of controllers is a topic that has not received the same amount of attention, even though it can be desirable in several scenarios, and can even be the source of frustration. Large language models have been adopted and evaluated as a corrective methodology, given their high power for predictions. Nevertheless, their predictions are not always correct, which can lead to lower usability. In this paper, we investigate whether, for text correction in VR that is hands-free, the use of AI could surpass in terms of usability and efficiency. We observed better usability for AI text correction when compared to voice input."}
{"id": "2512.11250", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11250", "abs": "https://arxiv.org/abs/2512.11250", "authors": ["Brock Marcinczyk", "Logan E. Beaver"], "title": "Optimal Control and Structurally-Informed Gradient Optimization of a Custom 4-DOF Rigid-Body Manipulator", "comment": "6 pages + 18 page appendix", "summary": "This work develops a control-centric framework for a custom 4-DOF rigid-body manipulator by coupling a reduced-order Pontryagin's Maximum Principle (PMP) controller with a physics-informed Gradient Descent stage. The reduced PMP model provides a closed-form optimal control law for the joint accelerations, while the Gradient Descent module determines the corresponding time horizons by minimizing a cost functional built directly from the full Rigid-Body Dynamics. Structural-mechanics reaction analysis is used only to initialize feasible joint velocities-most critically the azimuthal component-ensuring that the optimizer begins in a physically admissible region. The resulting kinematic trajectories and dynamically consistent time horizons are then supplied to the symbolic Euler-Lagrange model to yield closed-form inverse-dynamics inputs. This pipeline preserves a strict control-theoretic structure while embedding the physical constraints and loading behavior of the manipulator in a computationally efficient way."}
{"id": "2512.11661", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11661", "abs": "https://arxiv.org/abs/2512.11661", "authors": ["Brenda Nogueira", "Werner Geyer", "Andrew Anderson", "Toby Jia-Jun Li", "Dongwhi Kim", "Nuno Moniz", "Nitesh V. Chawla"], "title": "From Verification Burden to Trusted Collaboration: Design Goals for LLM-Assisted Literature Reviews", "comment": null, "summary": "Large Language Models (LLMs) are increasingly embedded in academic writing practices. Although numerous studies have explored how researchers employ these tools for scientific writing, their concrete implementation, limitations, and design challenges within the literature review process remain underexplored. In this paper, we report a user study with researchers across multiple disciplines to characterize current practices, benefits, and \\textit{pain points} in using LLMs to investigate related work. We identified three recurring gaps: (i) lack of trust in outputs, (ii) persistent verification burden, and (iii) requiring multiple tools. This motivates our proposal of six design goals and a high-level framework that operationalizes them through improved related papers visualization, verification at every step, and human-feedback alignment with generation-guided explanations. Overall, by grounding our work in the practical, day-to-day needs of researchers, we designed a framework that addresses these limitations and models real-world LLM-assisted writing, advancing trust through verifiable actions and fostering practical collaboration between researchers and AI systems."}
{"id": "2512.11275", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11275", "abs": "https://arxiv.org/abs/2512.11275", "authors": ["Suchang Chen", "Daqiang Guo"], "title": "Towards Logic-Aware Manipulation: A Knowledge Primitive for VLM-Based Assistants in Smart Manufacturing", "comment": "8 pages, 2 figures, submitted to the 2026 IFAC World Congress", "summary": "Existing pipelines for vision-language models (VLMs) in robotic manipulation prioritize broad semantic generalization from images and language, but typically omit execution-critical parameters required for contact-rich actions in manufacturing cells. We formalize an object-centric manipulation-logic schema, serialized as an eight-field tuple τ, which exposes object, interface, trajectory, tolerance, and force/impedance information as a first-class knowledge signal between human operators, VLM-based assistants, and robot controllers. We instantiate τ and a small knowledge base (KB) on a 3D-printer spool-removal task in a collaborative cell, and analyze τ-conditioned VLM planning using plan-quality metrics adapted from recent VLM/LLM planning benchmarks, while demonstrating how the same schema supports taxonomy-tagged data augmentation at training time and logic-aware retrieval-augmented prompting at test time as a building block for assistant systems in smart manufacturing enterprises."}
{"id": "2512.11674", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.11674", "abs": "https://arxiv.org/abs/2512.11674", "authors": ["Reza Shahriari", "Eric D. Ragan", "Jaime Ruiz"], "title": "Natural Language Interaction for Editing Visual Knowledge Graphs", "comment": null, "summary": "Knowledge graphs are often visualized using node-link diagrams that reveal relationships and structure. In many applications using graphs, it is desirable to allow users to edit graphs to ensure data accuracy or provides updates. Commonly in graph visualization, users can interact directly with the visual elements by clicking and typing updates to specific items through traditional interaction methods in the graphical user interface. However, it can become tedious to make many updates due to the need to individually select and change numerous items in a graph. Our research investigates natural language input as an alternative method for editing network graphs. We present a user study comparing GUI graph editing with two natural language alternatives to contribute novel empirical data of the trade-offs of the different interaction methods. The findings show natural language methods to be significantly more effective than traditional GUI interaction."}
{"id": "2512.11351", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.11351", "abs": "https://arxiv.org/abs/2512.11351", "authors": ["Steffen Schäfer", "Martin Cichon"], "title": "Incremental Validation of Automated Driving Functions using Generic Volumes in Micro- Operational Design Domains", "comment": null, "summary": "The validation of highly automated, perception-based driving systems must ensure that they function correctly under the full range of real-world conditions. Scenario-based testing is a prominent approach to addressing this challenge, as it involves the systematic simulation of objects and environments. Operational Design Domains (ODDs) are usually described using a taxonomy of qualitative designations for individual objects. However, the process of transitioning from taxonomy to concrete test cases remains unstructured, and completeness is theoretical. This paper introduces a structured method of subdividing the ODD into manageable sections, termed micro-ODDs (mODDs), and deriving test cases with abstract object representations. This concept is demonstrated using a one-dimensional, laterally guided manoeuvre involving a shunting locomotive within a constrained ODD. In this example, mODDs are defined and refined into narrow taxonomies that enable test case generation. Obstacles are represented as generic cubes of varying sizes, providing a simplified yet robust means of evaluating perception performance. A series of tests were conducted in a closed-loop, co-simulated virtual environment featuring photorealistic rendering and simulated LiDAR, GNSS and camera sensors. The results demonstrate how edge cases in obstacle detection can be systematically explored and how perception quality can be evaluated based on observed vehicle behaviour, using crash versus safe stop as the outcome metrics. These findings support the development of a standardised framework for safety argumentation and offer a practical step towards the validation and authorisation of automated driving functions."}
{"id": "2512.11724", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.11724", "abs": "https://arxiv.org/abs/2512.11724", "authors": ["Titaya Mairittha", "Tanakon Sawanglok", "Panuwit Raden", "Jirapast Buntub", "Thanapat Warunee", "Napat Asawachaisuvikrom", "Thanaphum Saiwongin"], "title": "From Signal to Turn: Interactional Friction in Modular Speech-to-Speech Pipelines", "comment": "6 pages, 1 figure", "summary": "While voice-based AI systems have achieved remarkable generative capabilities, their interactions often feel conversationally broken. This paper examines the interactional friction that emerges in modular Speech-to-Speech Retrieval-Augmented Generation (S2S-RAG) pipelines. By analyzing a representative production system, we move beyond simple latency metrics to identify three recurring patterns of conversational breakdown: (1) Temporal Misalignment, where system delays violate user expectations of conversational rhythm; (2) Expressive Flattening, where the loss of paralinguistic cues leads to literal, inappropriate responses; and (3) Repair Rigidity, where architectural gating prevents users from correcting errors in real-time. Through system-level analysis, we demonstrate that these friction points should not be understood as defects or failures, but as structural consequences of a modular design that prioritizes control over fluidity. We conclude that building natural spoken AI is an infrastructure design challenge, requiring a shift from optimizing isolated components to carefully choreographing the seams between them."}
{"id": "2512.11362", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11362", "abs": "https://arxiv.org/abs/2512.11362", "authors": ["Chao Xu", "Suyu Zhang", "Yang Liu", "Baigui Sun", "Weihong Chen", "Bo Xu", "Qi Liu", "Juncheng Wang", "Shujun Wang", "Shan Luo", "Jan Peters", "Athanasios V. Vasilakos", "Stefanos Zafeiriou", "Jiankang Deng"], "title": "An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges", "comment": null, "summary": "Vision-Language-Action (VLA) models are driving a revolution in robotics, enabling machines to understand instructions and interact with the physical world. This field is exploding with new models and datasets, making it both exciting and challenging to keep pace with. This survey offers a clear and structured guide to the VLA landscape. We design it to follow the natural learning path of a researcher: we start with the basic Modules of any VLA model, trace the history through key Milestones, and then dive deep into the core Challenges that define recent research frontier. Our main contribution is a detailed breakdown of the five biggest challenges in: (1) Representation, (2) Execution, (3) Generalization, (4) Safety, and (5) Dataset and Evaluation. This structure mirrors the developmental roadmap of a generalist agent: establishing the fundamental perception-action loop, scaling capabilities across diverse embodiments and environments, and finally ensuring trustworthy deployment-all supported by the essential data infrastructure. For each of them, we review existing approaches and highlight future opportunities. We position this paper as both a foundational guide for newcomers and a strategic roadmap for experienced researchers, with the dual aim of accelerating learning and inspiring new ideas in embodied intelligence. A live version of this survey, with continuous updates, is maintained on our \\href{https://suyuz1.github.io/Survery/}{project page}."}
{"id": "2512.11746", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.11746", "abs": "https://arxiv.org/abs/2512.11746", "authors": ["Hana Kopecka", "Jose Such"], "title": "The Influence of Human-like Appearance on Expected Robot Explanations", "comment": null, "summary": "A robot's appearance is a known factor influencing user's mental model and human-robot interaction, that has not been studied in the context of its influence in expected robot explanations. In this study, we investigate whether and to what extent the human-like appearance of robots elicits anthropomorphism, which is conceptualised as an attribution of mental capacities, and how the level of anthropomorphism is revealed in explanations that people expect to receive. We designed a between-subject study comprising conditions with visual stimuli of three domestic service robots with varying human-like appearance, and we prompted respondents to provide explanations they would expect to receive from the robot for the same robot actions. We found that most explanations were anthropomorphic across all conditions. However, there is a positive correlation between the anthropomorphic explanations and human-like appearance. We also report on more nuanced trends observed in non-anthropomorphic explanations and trends in robot descriptions."}
{"id": "2512.11551", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11551", "abs": "https://arxiv.org/abs/2512.11551", "authors": ["Jörg Gamerdinger", "Sven Teufel", "Simon Roller", "Oliver Bringmann"], "title": "CarlaNCAP: A Framework for Quantifying the Safety of Vulnerable Road Users in Infrastructure-Assisted Collective Perception Using EuroNCAP Scenarios", "comment": null, "summary": "The growing number of road users has significantly increased the risk of accidents in recent years. Vulnerable Road Users (VRUs) are particularly at risk, especially in urban environments where they are often occluded by parked vehicles or buildings. Autonomous Driving (AD) and Collective Perception (CP) are promising solutions to mitigate these risks. In particular, infrastructure-assisted CP, where sensor units are mounted on infrastructure elements such as traffic lights or lamp posts, can help overcome perceptual limitations by providing enhanced points of view, which significantly reduces occlusions. To encourage decision makers to adopt this technology, comprehensive studies and datasets demonstrating safety improvements for VRUs are essential. In this paper, we propose a framework for evaluating the safety improvement by infrastructure-based CP specifically targeted at VRUs including a dataset with safety-critical EuroNCAP scenarios (CarlaNCAP) with 11k frames. Using this dataset, we conduct an in-depth simulation study and demonstrate that infrastructure-assisted CP can significantly reduce accident rates in safety-critical scenarios, achieving up to 100% accident avoidance compared to a vehicle equipped with sensors with only 33%. Code is available at https://github.com/ekut-es/carla_ncap"}
{"id": "2512.11571", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11571", "abs": "https://arxiv.org/abs/2512.11571", "authors": ["Andreu Matoses Gimenez", "Nils Wilde", "Chris Pek", "Javier Alonso-Mora"], "title": "Cross-Entropy Optimization of Physically Grounded Task and Motion Plans", "comment": "Preprint", "summary": "Autonomously performing tasks often requires robots to plan high-level discrete actions and continuous low-level motions to realize them. Previous TAMP algorithms have focused mainly on computational performance, completeness, or optimality by making the problem tractable through simplifications and abstractions. However, this comes at the cost of the resulting plans potentially failing to account for the dynamics or complex contacts necessary to reliably perform the task when object manipulation is required. Additionally, approaches that ignore effects of the low-level controllers may not obtain optimal or feasible plan realizations for the real system. We investigate the use of a GPU-parallelized physics simulator to compute realizations of plans with motion controllers, explicitly accounting for dynamics, and considering contacts with the environment. Using cross-entropy optimization, we sample the parameters of the controllers, or actions, to obtain low-cost solutions. Since our approach uses the same controllers as the real system, the robot can directly execute the computed plans. We demonstrate our approach for a set of tasks where the robot is able to exploit the environment's geometry to move an object. Website and code: https://andreumatoses.github.io/research/parallel-realization"}
{"id": "2512.11609", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11609", "abs": "https://arxiv.org/abs/2512.11609", "authors": ["Tingyu Yuan", "Biaoliang Guan", "Wen Ye", "Ziyan Tian", "Yi Yang", "Weijie Zhou", "Yan Huang", "Peng Wang", "Chaoyang Zhao", "Jinqiao Wang"], "title": "UniBYD: A Unified Framework for Learning Robotic Manipulation Across Embodiments Beyond Imitation of Human Demonstrations", "comment": null, "summary": "In embodied intelligence, the embodiment gap between robotic and human hands brings significant challenges for learning from human demonstrations. Although some studies have attempted to bridge this gap using reinforcement learning, they remain confined to merely reproducing human manipulation, resulting in limited task performance. In this paper, we propose UniBYD, a unified framework that uses a dynamic reinforcement learning algorithm to discover manipulation policies aligned with the robot's physical characteristics. To enable consistent modeling across diverse robotic hand morphologies, UniBYD incorporates a unified morphological representation (UMR). Building on UMR, we design a dynamic PPO with an annealed reward schedule, enabling reinforcement learning to transition from imitation of human demonstrations to explore policies adapted to diverse robotic morphologies better, thereby going beyond mere imitation of human hands. To address the frequent failures of learning human priors in the early training stage, we design a hybrid Markov-based shadow engine that enables reinforcement learning to imitate human manipulations in a fine-grained manner. To evaluate UniBYD comprehensively, we propose UniManip, the first benchmark encompassing robotic manipulation tasks spanning multiple hand morphologies. Experiments demonstrate a 67.90% improvement in success rate over the current state-of-the-art. Upon acceptance of the paper, we will release our code and benchmark at https://github.com/zhanheng-creator/UniBYD."}
{"id": "2512.11620", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.11620", "abs": "https://arxiv.org/abs/2512.11620", "authors": ["Kanisorn Sangchai", "Methasit Boonpun", "Withawin Kraipetchara", "Paulo Garcia"], "title": "Architecting Large Action Models for Human-in-the-Loop Intelligent Robots", "comment": null, "summary": "The realization of intelligent robots, operating autonomously and interacting with other intelligent agents, human or artificial, requires the integration of environment perception, reasoning, and action. Classic Artificial Intelligence techniques for this purpose, focusing on symbolic approaches, have long-ago hit the scalability wall on compute and memory costs. Advances in Large Language Models in the past decade (neural approaches) have resulted in unprecedented displays of capability, at the cost of control, explainability, and interpretability. Large Action Models aim at extending Large Language Models to encompass the full perception, reasoning, and action cycle; however, they typically require substantially more comprehensive training and suffer from the same deficiencies in reliability. Here, we show it is possible to build competent Large Action Models by composing off-the-shelf foundation models, and that their control, interpretability, and explainability can be effected by incorporating symbolic wrappers and associated verification on their outputs, achieving verifiable neuro-symbolic solutions for intelligent robots. Our experiments on a multi-modal robot demonstrate that Large Action Model intelligence does not require massive end-to-end training, but can be achieved by integrating efficient perception models with a logic-driven core. We find that driving action execution through the generation of Planning Domain Definition Language (PDDL) code enables a human-in-the-loop verification stage that effectively mitigates action hallucinations. These results can support practitioners in the design and development of robotic Large Action Models across novel industries, and shed light on the ongoing challenges that must be addressed to ensure safety in the field."}
{"id": "2512.11736", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11736", "abs": "https://arxiv.org/abs/2512.11736", "authors": ["Ninghan Zhong", "Steven Caro", "Megnath Ramesh", "Rishi Bhatnagar", "Avraiem Iskandar", "Stephen L. Smith"], "title": "Bench-Push: Benchmarking Pushing-based Navigation and Manipulation Tasks for Mobile Robots", "comment": "Under review for ICRA 2026", "summary": "Mobile robots are increasingly deployed in cluttered environments with movable objects, posing challenges for traditional methods that prohibit interaction. In such settings, the mobile robot must go beyond traditional obstacle avoidance, leveraging pushing or nudging strategies to accomplish its goals. While research in pushing-based robotics is growing, evaluations rely on ad hoc setups, limiting reproducibility and cross-comparison. To address this, we present Bench-Push, the first unified benchmark for pushing-based mobile robot navigation and manipulation tasks. Bench-Push includes multiple components: 1) a comprehensive range of simulated environments that capture the fundamental challenges in pushing-based tasks, including navigating a maze with movable obstacles, autonomous ship navigation in ice-covered waters, box delivery, and area clearing, each with varying levels of complexity; 2) novel evaluation metrics to capture efficiency, interaction effort, and partial task completion; and 3) demonstrations using Bench-Push to evaluate example implementations of established baselines across environments. Bench-Push is open-sourced as a Python library with a modular design. The code, documentation, and trained models can be found at https://github.com/IvanIZ/BenchNPIN."}
{"id": "2512.11746", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.11746", "abs": "https://arxiv.org/abs/2512.11746", "authors": ["Hana Kopecka", "Jose Such"], "title": "The Influence of Human-like Appearance on Expected Robot Explanations", "comment": null, "summary": "A robot's appearance is a known factor influencing user's mental model and human-robot interaction, that has not been studied in the context of its influence in expected robot explanations. In this study, we investigate whether and to what extent the human-like appearance of robots elicits anthropomorphism, which is conceptualised as an attribution of mental capacities, and how the level of anthropomorphism is revealed in explanations that people expect to receive. We designed a between-subject study comprising conditions with visual stimuli of three domestic service robots with varying human-like appearance, and we prompted respondents to provide explanations they would expect to receive from the robot for the same robot actions. We found that most explanations were anthropomorphic across all conditions. However, there is a positive correlation between the anthropomorphic explanations and human-like appearance. We also report on more nuanced trends observed in non-anthropomorphic explanations and trends in robot descriptions."}
{"id": "2512.11769", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11769", "abs": "https://arxiv.org/abs/2512.11769", "authors": ["Xiaoyu Ma", "Zhengqing Yuan", "Zheyuan Zhang", "Kaiwen Shi", "Lichao Sun", "Yanfang Ye"], "title": "BLURR: A Boosted Low-Resource Inference for Vision-Language-Action Models", "comment": "10 pages, 3 figures. Code and integration scripts will be released at this http URL: https://github.com/JijiKing-Sam/BLURR-A-Boosted-Low-Resource-Inference-for-Vision-Language-Action-Model", "summary": "Vision-language-action (VLA) models enable impressive zero shot manipulation, but their inference stacks are often too heavy for responsive web demos or high frequency robot control on commodity GPUs. We present BLURR, a lightweight inference wrapper that can be plugged into existing VLA controllers without retraining or changing model checkpoints. Instantiated on the pi-zero VLA controller, BLURR keeps the original observation interfaces and accelerates control by combining an instruction prefix key value cache, mixed precision execution, and a single step rollout schedule that reduces per step computation. In our SimplerEnv based evaluation, BLURR maintains task success rates comparable to the original controller while significantly lowering effective FLOPs and wall clock latency. We also build an interactive web demo that allows users to switch between controllers and toggle inference options in real time while watching manipulation episodes. This highlights BLURR as a practical approach for deploying modern VLA policies under tight compute budgets."}
{"id": "2512.11773", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11773", "abs": "https://arxiv.org/abs/2512.11773", "authors": ["Britton Jordan", "Jordan Thompson", "Jesse F. d'Almeida", "Hao Li", "Nithesh Kumar", "Susheela Sharma Stern", "Ipek Oguz", "Robert J. Webster", "Daniel Brown", "Alan Kuntz", "James Ferguson"], "title": "ProbeMDE: Uncertainty-Guided Active Proprioception for Monocular Depth Estimation in Surgical Robotics", "comment": "9 pages, 5 figures", "summary": "Monocular depth estimation (MDE) provides a useful tool for robotic perception, but its predictions are often uncertain and inaccurate in challenging environments such as surgical scenes where textureless surfaces, specular reflections, and occlusions are common. To address this, we propose ProbeMDE, a cost-aware active sensing framework that combines RGB images with sparse proprioceptive measurements for MDE. Our approach utilizes an ensemble of MDE models to predict dense depth maps conditioned on both RGB images and on a sparse set of known depth measurements obtained via proprioception, where the robot has touched the environment in a known configuration. We quantify predictive uncertainty via the ensemble's variance and measure the gradient of the uncertainty with respect to candidate measurement locations. To prevent mode collapse while selecting maximally informative locations to propriocept (touch), we leverage Stein Variational Gradient Descent (SVGD) over this gradient map. We validate our method in both simulated and physical experiments on central airway obstruction surgical phantoms. Our results demonstrate that our approach outperforms baseline methods across standard depth estimation metrics, achieving higher accuracy while minimizing the number of required proprioceptive measurements."}
{"id": "2512.11781", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.11781", "abs": "https://arxiv.org/abs/2512.11781", "authors": ["Vineet Pasumarti", "Lorenzo Bianchi", "Antonio Loquercio"], "title": "Agile Flight Emerges from Multi-Agent Competitive Racing", "comment": null, "summary": "Through multi-agent competition and the sparse high-level objective of winning a race, we find that both agile flight (e.g., high-speed motion pushing the platform to its physical limits) and strategy (e.g., overtaking or blocking) emerge from agents trained with reinforcement learning. We provide evidence in both simulation and the real world that this approach outperforms the common paradigm of training agents in isolation with rewards that prescribe behavior, e.g., progress on the raceline, in particular when the complexity of the environment increases, e.g., in the presence of obstacles. Moreover, we find that multi-agent competition yields policies that transfer more reliably to the real world than policies trained with a single-agent progress-based reward, despite the two methods using the same simulation environment, randomization strategy, and hardware. In addition to improved sim-to-real transfer, the multi-agent policies also exhibit some degree of generalization to opponents unseen at training time. Overall, our work, following in the tradition of multi-agent competitive game-play in digital domains, shows that sparse task-level rewards are sufficient for training agents capable of advanced low-level control in the physical world.\n  Code: https://github.com/Jirl-upenn/AgileFlight_MultiAgent"}
{"id": "2512.11797", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11797", "abs": "https://arxiv.org/abs/2512.11797", "authors": ["Junjie Ye", "Rong Xue", "Basile Van Hoorick", "Pavel Tokmakov", "Muhammad Zubair Irshad", "Yue Wang", "Vitor Guizilini"], "title": "AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis", "comment": "Project page: https://jay-ye.github.io/AnchorDream/", "summary": "The collection of large-scale and diverse robot demonstrations remains a major bottleneck for imitation learning, as real-world data acquisition is costly and simulators offer limited diversity and fidelity with pronounced sim-to-real gaps. While generative models present an attractive solution, existing methods often alter only visual appearances without creating new behaviors, or suffer from embodiment inconsistencies that yield implausible motions. To address these limitations, we introduce AnchorDream, an embodiment-aware world model that repurposes pretrained video diffusion models for robot data synthesis. AnchorDream conditions the diffusion process on robot motion renderings, anchoring the embodiment to prevent hallucination while synthesizing objects and environments consistent with the robot's kinematics. Starting from only a handful of human teleoperation demonstrations, our method scales them into large, diverse, high-quality datasets without requiring explicit environment modeling. Experiments show that the generated data leads to consistent improvements in downstream policy learning, with relative gains of 36.4% in simulator benchmarks and nearly double performance in real-world studies. These results suggest that grounding generative world models in robot motion provides a practical path toward scaling imitation learning."}
{"id": "2512.11472", "categories": ["cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11472", "abs": "https://arxiv.org/abs/2512.11472", "authors": ["David Wagmann", "Matti Krüger", "Chao Wang", "Jürgen Steimle"], "title": "Mirror Skin: In Situ Visualization of Robot Touch Intent on Robotic Skin", "comment": null, "summary": "Effective communication of robotic touch intent is a key factor in promoting safe and predictable physical human-robot interaction (pHRI). While intent communication has been widely studied, existing approaches lack the spatial specificity and semantic depth necessary to convey robot touch actions. We present Mirror Skin, a cephalopod-inspired concept that utilizes high-resolution, mirror-like visual feedback on robotic skin. By mapping in-situ visual representations of a human's body parts onto the corresponding robot's touch region, Mirror Skin communicates who shall initiate touch, where it will occur, and when it is imminent. To inform the design of Mirror Skin, we conducted a structured design exploration with experts in virtual reality (VR), iteratively refining six key dimensions. A subsequent controlled user study demonstrated that Mirror Skin significantly enhances accuracy and reduces response times for interpreting touch intent. These findings highlight the potential of visual feedback on robotic skin to communicate human-robot touch interactions."}
