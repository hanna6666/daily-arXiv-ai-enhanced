<div id=toc></div>

# Table of Contents

- [cs.HC](#cs.HC) [Total: 19]
- [cs.RO](#cs.RO) [Total: 22]


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [1] [When LLMs Can't Help: Real-World Evaluation of LLMs in Nutrition](https://arxiv.org/abs/2511.20652)
*Karen Jia-Hui Li,Simone Balloccu,Ondrej Dusek,Ehud Reiter*

Main category: cs.HC

TL;DR: 本研究通过首个营养领域的随机对照试验评估LLMs的有效性，发现尽管在内在评估中表现良好，但在真实世界中的应用效果未能一致提升。


<details>
  <summary>Details</summary>
Motivation: 随着人们对大型语言模型（LLMs）及其在聊天机器人形式中的信任度提高，外部评估的缺乏仍然削弱了这种信任，尤其是在营养领域。专家要求采用随机对照试验（RCT），以确保基于证据的部署。

Method: 进行为期七周的随机对照试验（n=81），比较了两种不同的聊天机器人变种，测量其在饮食成果、情感福祉和参与度方面的效果。

Result: 我们的研究是第一个涉及LLMs在营养领域的随机对照试验，比较了集成与未集成LLMs的聊天机器人在饮食结果、情感福祉和参与度方面的表现。

Conclusion: 研究结果突显了内在评估与实际影响之间的关键差距，强调了跨学科、以人为本的方法的必要性。

Abstract: The increasing trust in large language models (LLMs), especially in the form of chatbots, is often undermined by the lack of their extrinsic evaluation. This holds particularly true in nutrition, where randomised controlled trials (RCTs) are the gold standard, and experts demand them for evidence-based deployment. LLMs have shown promising results in this field, but these are limited to intrinsic setups. We address this gap by running the first RCT involving LLMs for nutrition. We augment a rule-based chatbot with two LLM-based features: (1) message rephrasing for conversational variety and engagement, and (2) nutritional counselling through a fine-tuned model. In our seven-week RCT (n=81), we compare chatbot variants with and without LLM integration. We measure effects on dietary outcome, emotional well-being, and engagement. Despite our LLM-based features performing well in intrinsic evaluation, we find that they did not yield consistent benefits in real-world deployment. These results highlight critical gaps between intrinsic evaluations and real-world impact, emphasising the need for interdisciplinary, human-centred approaches.\footnote{We provide all of our code and results at: \\ \href{https://github.com/saeshyra/diet-chatbot-trial}{https://github.com/saeshyra/diet-chatbot-trial}}

</details>


### [2] [Domain-Grounded Evaluation of LLMs in International Student Knowledge](https://arxiv.org/abs/2511.20653)
*Claudinei Daitx,Haitham Amar*

Main category: cs.HC

TL;DR: 本研究评估了大型语言模型在留学咨询中的有效性，分析了其准确性和幻觉现象，并提出了可重用的审计协议以提高教育和咨询的质量。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在留学咨询中的应用日益广泛，对其可靠性的验证变得至关重要，尤其是在面对高风险问题时。

Method: 通过针对ApplyBoard的咨询工作流程设计的现实问题，使用明确的评分标准评估模型的准确性与幻觉现象。

Result: 通过比较不同模型的准确性和幻觉现象，识别出最可靠的模型，以及模型在答案完整性、主题偏离和不支持信息方面的常见失误。

Conclusion: 本研究提供了一种评估大型语言模型在留学咨询中的可靠性的方法，并指出其常见的失误模式，为教育和咨询中的应用提供了审计协议。

Abstract: Large language models (LLMs) are increasingly used to answer high-stakes study-abroad questions about admissions, visas, scholarships, and eligibility. Yet it remains unclear how reliably they advise students, and how often otherwise helpful answers drift into unsupported claims (``hallucinations'').
  This work provides a clear, domain-grounded overview of how current LLMs behave in this setting. Using realistic questions set drawn from ApplyBoard's advising workflows -- an EdTech platform that supports students from discovery to enrolment -- we evaluate two essentials side by side: accuracy (is the information correct and complete?) and hallucination (does the model add content not supported by the question or domain evidence). These questions are categorized by domain scope which can be a single-domain or multi-domain -- when it must integrate evidence across areas such as admissions, visas, and scholarships.
  To reflect real advising quality, we grade answers with a simple rubric which is correct, partial, or wrong. The rubric is domain-coverage-aware: an answer can be partial if it addresses only a subset of the required domains, and it can be over-scoped if it introduces extra, unnecessary domains; both patterns are captured in our scoring as under-coverage or reduced relevance/hallucination.
  We also report measures of faithfulness and answer relevance, alongside an aggregate hallucination score, to capture relevance and usefulness. All models are tested with the same questions for a fair, head-to-head comparison.
  Our goals are to: (1) give a clear picture of which models are most dependable for study-abroad advising, (2) surface common failure modes -- where answers are incomplete, off-topic, or unsupported, and (3) offer a practical, reusable protocol for auditing LLMs before deployment in education and advising contexts.

</details>


### [3] [CodeVaani: A Multilingual, Voice-Based Code Learning Assistant](https://arxiv.org/abs/2511.20654)
*Jayant Havare,Srikanth Tamilselvam,Ashish Mittal,Shalaka Thorat,Soham Jadia,Varsha Apte,Ganesh Ramakrishnan*

Main category: cs.HC

TL;DR: CodeVaani是一个多语言的语音助手，旨在帮助非英语母语的学生理解编程，提高编程教育的包容性。


<details>
  <summary>Details</summary>
Motivation: 针对来自多语言地区（如印度）的学生，解决编程教育中因英语能力和文本交互带来的障碍

Method: 通过集成Indic ASR、代码感知的转录模块和代码模型，开发了一种多语言语音助手

Result: CodeVaani在28名初学者中实现了75%的响应准确率，80%以上的参与者对体验给予了积极评价。

Conclusion: CodeVaani有助于降低编程教育的语言障碍，提高对多语言学生的支持，展示了声音驱动的AI系统在教育中的潜力。

Abstract: Programming education often assumes English proficiency and text-based interaction, creating barriers for students from multilingual regions such as India. We present CodeVaani, a multilingual speech-driven assistant for understanding code, built into Bodhitree [1], a Learning Management System developed at IIT Bombay. It is a voice-enabled assistant that helps learners explore programming concepts in their native languages. The system integrates Indic ASR, a codeaware transcription refinement module, and a code model for generating relevant answers. Responses are provided in both text and audio for natural interaction. In a study with 28 beginner programmers, CodeVaani achieved 75% response accuracy, with over 80% of participants rating the experience positively. Compared to classroom assistance, our framework offers ondemand availability, scalability to support many learners, and multilingual support that lowers the entry barrier for students with limited English proficiency. The demo will illustrate these capabilities and highlight how voice-based AI systems can make programming education more inclusive. Supplementary artifacts and demo video are also made available.

</details>


### [4] [Exploropleth: exploratory analysis of data binning methods in choropleth maps](https://arxiv.org/abs/2511.20655)
*Arpit Narechania,Alex Endert,Clio Andris*

Main category: cs.HC

TL;DR: Exploropleth是一个新的开源工具，支持多种数据分组方法的比较与定制，促进地图制作和教育。


<details>
  <summary>Details</summary>
Motivation: 为了展示不同分组方法对量化数据值的影响，帮助用户理解数据在地图上的表现，进而提高地图制作的效率与准确性。

Method: 通过与16名来自不同机构的制图师和地理信息系统专家的访谈，收集意见以优化工具，并提供多种数据分组方法的视图。

Result: 用户可以交互式探索已建立的数据分组方法，比较定制地图，并可即时重分类行政单位，赋予地图制作更大的灵活性。

Conclusion: Exploropleth是一种新型的开源网络地理空间可视化工具，能够有效地支持不同数据分组方法的比较与自定义，同时具备即时行政单位重分类的功能，对教育和专业地图制作均具有潜在价值。

Abstract: When creating choropleth maps, mapmakers often bin (i.e., group, classify) quantitative data values into groups to help show that certain areas fall within a similar range of values. For instance, a mapmaker may divide counties into groups of high, middle, and low life expectancy (measured in years). It is well known that different binning methods (e.g., natural breaks, quantile) yield different groupings, meaning the same data can be presented differently depending on how it is divided into bins. To help guide a wide variety of users, we present a new, open source, web-based, geospatial visualization tool, Exploropleth, that lets users interact with a catalog of established data binning methods, and subsequently compare, customize, and export custom maps. This tool advances the state of the art by providing multiple binning methods in one view and supporting administrative unit reclassification on-the-fly. We interviewed 16 cartographers and geographic information systems (GIS) experts from 13 government organizations, non-government organizations (NGOs), and federal agencies who identified opportunities to integrate Exploropleth into their existing mapmaking workflow, and found that the tool has potential to educate students as well as mapmakers with varying levels of experience. Exploropleth is open-source and publicly available at https://exploropleth.github.io.

</details>


### [5] [Context-Aware Visual Prompting: Automating Geospatial Web Dashboards with Large Language Models and Agent Self-Validation for Decision Support](https://arxiv.org/abs/2511.20656)
*Haowen Xu,Jose Tupayachi,Xiao-Ying Yu*

Main category: cs.HC

TL;DR: 提出了一种新框架，利用生成式AI和大型语言模型，自动创建交互式地理空间仪表板，提升风险分析的有效性与决策支持。


<details>
  <summary>Details</summary>
Motivation: 解决在风险分析和决策支持中可视化多维环境数据、实现复杂性和自动化程度有限的问题。

Method: 构建一个生成式AI框架，利用大型语言模型自动化创建交互式地理空间仪表板。

Result: 通过结构化知识图谱提升生成过程的准确性和上下文意识，结合上下文感知视觉提示机制。

Conclusion: 该框架通过集成的知识与视觉提示创新地提升了风险分析和决策支持的能力。

Abstract: The development of web-based geospatial dashboards for risk analysis and decision support is often challenged by the difficulty in visualization of big, multi-dimensional environmental data, implementation complexity, and limited automation. We introduce a generative AI framework that harnesses Large Language Models (LLMs) to automate the creation of interactive geospatial dashboards from user-defined inputs including UI wireframes, requirements, and data sources. By incorporating a structured knowledge graph, the workflow embeds domain knowledge into the generation process and enable accurate and context-aware code completions. A key component of our approach is the Context-Aware Visual Prompting (CAVP) mechanism, which extracts encodes and interface semantics from visual layouts to guide LLM driven generation of codes. The new framework also integrates a self-validation mechanism that uses an agent-based LLM and Pass@k evaluation alongside semantic metrics to assure output reliability. Dashboard snippets are paired with data visualization codebases and ontological representations, enabling a pipeline that produces scalable React-based completions using the MVVM architectural pattern. Our results demonstrate improved performance over baseline approaches and expanded functionality over third party platforms, while incorporating multi-page, fully functional interfaces. We successfully developed a framework to implement LLMs, demonstrated the pipeline for automated code generation, deployment, and performed chain-of-thought AI agents in self-validation. This integrative approach is guided by structured knowledge and visual prompts, providing an innovative geospatial solution in enhancing risk analysis and decision making.

</details>


### [6] [Intelligent Agents with Emotional Intelligence: Current Trends, Challenges, and Future Prospects](https://arxiv.org/abs/2511.20657)
*Raziyeh Zall,Alireza Kheyrkhah,Erik Cambria,Zahra Naseri,M. Reza Kangavari*

Main category: cs.HC

TL;DR: 本文综述了情感计算的核心组件，分析了面对的挑战，并指出生成技术在情感智能发展中的潜力。


<details>
  <summary>Details</summary>
Motivation: 情感智能代理在各个社会领域人机交互中扮演重要角色，然而全面的情感计算研究仍然有限。

Method: 调查和分析情感智能核心组件及其在人机交互中的应用

Result: 提供了情感理解、引发和表达的综合概述，分析了当前面临的主要挑战与方法，并探讨了生成技术的未来方向。

Conclusion: 本文强调了发展情感智能系统面临的挑战，并展望了在生成技术支持下，情感计算的未来发展方向。

Abstract: The development of agents with emotional intelligence is becoming increasingly vital due to their significant role in human-computer interaction and the growing integration of computer systems across various sectors of society. Affective computing aims to design intelligent systems that can recognize, evoke, and express human emotions, thereby emulating human emotional intelligence. While previous reviews have focused on specific aspects of this field, there has been limited comprehensive research that encompasses emotion understanding, elicitation, and expression, along with the related challenges. This survey addresses this gap by providing a holistic overview of core components of artificial emotion intelligence. It covers emotion understanding through multimodal data processing, as well as affective cognition, which includes cognitive appraisal, emotion mapping, and adaptive modulation in decision-making, learning, and reasoning. Additionally, it addresses the synthesis of emotional expression across text, speech, and facial modalities to enhance human-agent interaction. This paper identifies and analyzes the key challenges and issues encountered in the development of affective systems, covering state-of-the-art methodologies designed to address them. Finally, we highlight promising future directions, with particular emphasis on the potential of generative technologies to advance affective computing.

</details>


### [7] [Iteration and Co-design of a Physical Web Application for Outdoor Activities with Older Adults](https://arxiv.org/abs/2511.20659)
*Fatima Badmos,Emma Murphy,Michael Ward,Damon Berry*

Main category: cs.HC

TL;DR: 本研究提出利用Physical Web技术促进老年人参与户外活动，通过设计原型与用户反馈来优化应用。


<details>
  <summary>Details</summary>
Motivation: 探索科技如何促进老年人参与户外活动，提升康复动机，符合外部身体活动指导方针。

Method: 设计原型的创建与用户反馈收集

Result: 设计了一个初步原型并收集了12名老年人的反馈，分析他们对设计的看法和未来设计的想法。

Conclusion: 通过用户反馈推动设计改进，以提高老年人参与户外身体活动的积极性，促进健康。

Abstract: Existing research and physical activity guidelines highlight the benefits of outdoor physical activities for ageing populations. There is potential for technology to facilitate outdoor activity through Physical Web infrastructure. We proposed that embedding Physical Web applications that are engaging and interactive in public open spaces as part of interactive wellness parks can encourage older adults to participate in physical activities outdoors and motivate rehabilitation. We have created an initial design prototype based on design requirements generated from a qualitative field study with 24 older adults to explore their perceptions, experiences, and routines of outdoor physical activities. In this paper, we present an initial prototype and findings from a co-design session with 12 older adults, eliciting their feedback on the design and their ideas for future design iterations.

</details>


### [8] [Transforming Higher Education with AI-Powered Video Lectures](https://arxiv.org/abs/2511.20660)
*Dengsheng Zhang*

Main category: cs.HC

TL;DR: 本研究提出了一种半自动化的视频制作流程，使用AI生成脚本和语音，显示出良好的学习效果和较低的教师工作量，但仍需改善音质和虚拟形象。


<details>
  <summary>Details</summary>
Motivation: 探索AI在提高高等教育视频讲座生产中的潜力，尤其是在内容创作和可及性方面

Method: 使用Google Gemini生成脚本，使用Amazon Polly进行语音合成，使用Microsoft PowerPoint进行视频组装的半自动工作流程

Result: 通过案例研究展示AI生成的脚本与人类授课视频在学习成果上的可比性，学生对清晰度、一致性和可用性表示高度满意

Conclusion: AI辅助的视频制作可以降低教师的工作负担，提高可扩展性，并提供有效的学习资源，但未来在合成音和形象方面的改进将进一步增强学生的参与感。

Abstract: The integration of artificial intelligence (AI) into video lecture production has the potential to transform higher education by streamlining content creation and enhancing accessibility. This paper investigates a semi automated workflow that combines Google Gemini for script generation, Amazon Polly for voice synthesis, and Microsoft PowerPoint for video assembly. Unlike fully automated text to video platforms, this hybrid approach preserves pedagogical intent while ensuring script to slide synchronization, narrative coherence, and customization. Case studies demonstrate the effectiveness of Gemini in generating accurate and context-sensitive scripts for visually rich academic presentations, while Polly provides natural-sounding narration with controllable pacing. A two course pilot study was conducted to evaluate AI generated instructional videos (AIIV) against human instructional videos (HIV). Both qualitative and quantitative results indicate that AIIVs are comparable to HIVs in terms of learning outcomes, with students reporting high levels of clarity, coherence, and usability. However, limitations remain, particularly regarding audio quality and the absence of human-like avatars. The findings suggest that AI assisted video production can reduce instructor workload, improve scalability, and deliver effective learning resources, while future improvements in synthetic voices and avatars may further enhance learner engagement.

</details>


### [9] [Beyond the Legal Lens: A Sociotechnical Taxonomy of Lived Privacy Incidents and Harms](https://arxiv.org/abs/2511.20791)
*Kirsten Chapman,Garrett Smith,Kaitlyn Klabacka,Harrison Winslow,Louise Barkhuus,Cori Faklaris,Sauvik Das,Pamela Wisniewski,Bart Piet Knijnenburg,Heather Lipford,Xinru Page*

Main category: cs.HC

TL;DR: 本研究分析了隐私事件，提出了一个新的框架，揭示了数字隐私伤害的更广泛范围，强调了心理安全感的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有法律框架无法全面覆盖现代数字隐私相关的人类经验，因此需要更全面的隐私伤害表示。

Method: 对369个由公众报告的隐私事件进行分析。

Result: 发现隐私事件和伤害的范围超出了现有法律框架的考虑，且大多数隐私伤害源于恐惧和心理安全感丧失，而非具体的物质损害。

Conclusion: 本研究提出了一个新的框架，用于理解数字隐私伤害，适用于研究和实践领域。

Abstract: To understand how privacy incidents lead to harms, HCI researchers have historically leveraged legal frameworks. However, these frameworks expect acute, tangible harms and thus may not cover the full range of human experience relevant to modern-day digital privacy. To address this gap, our research builds upon these existing frameworks to develop a more comprehensive representation of people's lived experiences with privacy harms. We analyzed 369 privacy incidents reported by individuals from the general public. We found a broader range of privacy incidents and harms than accounted for in existing legal frameworks. The majority of reported privacy harms were not based on tangible harm, but on fear and loss of psychological safety. We also characterize the actors, motives, and information associated with various incidents. This work contributes a new framework for understanding digital privacy harms that can be utilized both in research and practice.

</details>


### [10] [PileUp: A Tufting Approach to Soft, Tactile, and Volumetric E-Textile Interfaces](https://arxiv.org/abs/2511.21000)
*Seoyoung Choi,Rashmi Balegar Mohan,Heather Jin Hee Kim,Jisoo Ha,Jeyeon Jo*

Main category: cs.HC

TL;DR: PileUp是一种通过绒毛状电子纺织品传感技术，利用导电纱线的结构实现多模态传感，能够检测压力、弯曲、应变和环境湿度。


<details>
  <summary>Details</summary>
Motivation: 探索柔性、富有表现力的绒毛织物在各种环境中的传感应用，推动电子纺织品的设计和实用化。

Method: 通过结合环形或剪切绒毛形态的导电纱线，构建出能感知多种机械变形及环境条件的三维柔性织物传感器。

Result: 提出了四个应用场景，展示了PileUp传感器在冥想地毯、运动容器和湿度监测墙面艺术中的有效集成。

Conclusion: PileUp展示了绒毛织物作为集成传感器制造方法的潜力，为柔性传感网络提供了新的可能性。

Abstract: We present PileUp, a tufted pile e-textile sensing approach that offers unique affordances through the tactile expressiveness and richness of its continuous, threaded-volume construction. By integrating conductive yarns in looped or cut pile forms, PileUp transforms soft 3-dimensional textiles into multimodal sensors capable of detecting mechanical deformations such as pressure, bending, and strain, as well as environmental conditions like moisture. We propose a design space that outlines the relationships between texture, form factor, and sensing affordances of tufted textiles. We characterize electrical responses under compression, bending, and strain, reporting sensor behaviors. To demonstrate versatility, we present three application scenarios in which PileUp sensors are seamlessly integrated into soft fabrics: a meditation rug with multi-zone sensing, a fleece sleeve that detects arm motion, and a moisture-sensing wall art. Our results establish tufting as an accessible yet expressive fabrication method for creating integrated sensing textiles, distinguishing our work from traditional flat textile sensors.

</details>


### [11] [LOOM: Personalized Learning Informed by Daily LLM Conversations Toward Long-Term Mastery via a Dynamic Learner Memory Graph](https://arxiv.org/abs/2511.21037)
*Justin Cui,Kevin Pu,Tovi Grossman*

Main category: cs.HC

TL;DR: LOOM系统通过理解学习者的当前需求与优先事项，提供个性化学习材料，推动学习进步和知识填补。


<details>
  <summary>Details</summary>
Motivation: 解决固定课程和迅速反应的学习系统之间的权衡，提供灵活且针对学习者当前需求的内容。

Method: 提出LOOM，一个基于学习者需求的个性化学习代理系统。

Result: LOOM能够根据LLM对话和学习者记忆图推断学习者需求，并生成能够链接相邻概念的个性化学习材料。

Conclusion: 设计更健壮的混合主动学习流程，整合结构化学习者建模与LLM日常互动的建议。

Abstract: Foundation models are increasingly used to personalize learning, yet many systems still assume fixed curricula or coarse progress signals, limiting alignment with learners' day-to-day needs. At the other extreme, lightweight incidental systems offer flexible, in-the-moment content but rarely guide learners toward mastery. Prior work privileges either continuity (maintaining a plan across sessions) or initiative (reacting to the moment), not both, leaving learners to navigate the trade-off between recency and trajectory-immediate relevance versus cumulative, goal-aligned progress. We present LOOM, an agentic pipeline that infers evolving learner needs from recent LLM conversations and a dynamic learner memory graph, then assembles coherent learning materials personalized to the learner's current needs, priorities, and understanding. These materials link adjacent concepts and surface gaps as tightly scoped modules that cumulatively advance broader goals, providing guidance and sustained progress while remaining responsive to new interests. We describe LOOM's end-to-end architecture and working prototype, including conversation summarization, topic planning, course generation, and graph-based progress tracking. In a formative study with ten participants, users reported that LOOM's generated lessons felt relevant to their recent activities and helped them recognize knowledge gaps, though they also highlighted needs for greater consistency and control. We conclude with design implications for more robust, mixed-initiative learning pipelines that integrate structured learner modelling with everyday LLM interactions.

</details>


### [12] [Human-Centered Artificial Social Intelligence (HC-ASI)](https://arxiv.org/abs/2511.21044)
*Hanxi Pan,Wei Xu,Mowei Shen,Zaifeng Gao*

Main category: cs.HC

TL;DR: 本章构建了以人为本的人工社会智能框架，探讨技术和人类中心设计原则，以及其在不同行业的实际应用。


<details>
  <summary>Details</summary>
Motivation: 在人工智能日益融入人类社会背景下，理解和参与复杂的社会互动是其关键能力。

Method: 提出以人为本的人工社会智能框架（HC-ASI）并系统性地探讨其技术基础和设计原则。

Result: 提供了HC-ASI框架，综合评估现有的人工社会智能（ASI）研究，通过案例研究展示其在多个领域的应用。

Conclusion: 提出了HC-ASI生态系统的发展挑战及未来的有希望的方向。

Abstract: As artificial intelligence systems become increasingly integrated into human social contexts, Artificial Social Intelligence (ASI) has emerged as a critical capability that enables AI to perceive, understand, and engage meaningfully in complex human social interactions. This chapter introduces a comprehensive framework for Human-Centered Artificial Social Intelligence (HC-ASI), built upon the Technology-Human Factors-Ethics (THE) Triangle, which systematically addresses both technical foundations and human-centered design principles necessary for developing socially intelligent AI systems. This chapter provides a comprehensive overview of current ASI research. This chapter begins by establishing the theoretical foundations of ASI, tracing its evolution from classical psychological theories of human social intelligence to contemporary computational models, then examines the mechanisms underlying human-AI social interaction with particular emphasis on establishing shared social understanding and appropriate role positioning. The chapter further explores ASI's practical implications for individuals and groups through comprehensive evaluation frameworks that combine technical benchmarks with human-centered experiential assessments, demonstrating real-world applications through detailed case studies spanning healthcare, companionship, education, and customer service domains. Building on the overview and the framework of HC -ASI, this chapter articulates core HC-ASI design principles and translates them into actionable methodologies and implementation guidelines that provide practical guidance for researchers and practitioners. This chapter concludes with a critical discussion of current challenges and promising directions for developing comprehensive HC-ASI ecosystems.

</details>


### [13] [Lattice Menu: A Low-Error Gaze-Based Marking Menu Utilizing Target-Assisted Gaze Gestures on a Lattice of Visual Anchors](https://arxiv.org/abs/2511.21131)
*Taejun Kim,Auejin Ham,Sunggeun Ahn,Geehyuk Lee*

Main category: cs.HC

TL;DR: Lattice Menu 是一种基于视线的菜单选择方式，通过视觉锚点提升选择准确性，显著降低错误率，用户偏好此方法并感受减少眼睛疲劳。


<details>
  <summary>Details</summary>
Motivation: 为了提高基于注视的菜单选择的准确性和效率，特别是在多级菜单环境中。

Method: 使用基于注视的标记菜单，通过视觉锚点提升菜单项选择的准确性。

Result: 实验结果显示 Lattice Menu 的错误率只有约1%，菜单选择时间为1.3-1.6秒，比传统菜单减少了约5倍的选择错误，且用户反馈更喜欢这种方法。

Conclusion: Lattice Menu 在专家使用时显示出显著下降的错误率和快速的菜单选择时间，且用户偏好该方法。

Abstract: We present Lattice Menu, a gaze-based marking menu utilizing a lattice of visual anchors that helps perform accurate gaze pointing for menu item selection. Users who know the location of the desired item can leverage target-assisted gaze gestures for multilevel item selection by looking at visual anchors over the gaze trajectories. Our evaluation showed that Lattice Menu exhibits a considerably low error rate (~1%) and a quick menu selection time (1.3-1.6 s) for expert usage across various menu structures (4 x 4 x 4 and 6 x 6 x 6) and sizes (8, 10 and 12°). In comparison with a traditional gaze-based marking menu that does not utilize visual targets, Lattice Menu showed remarkably (~5 times) fewer menu selection errors for expert usage. In a post-interview, all 12 subjects preferred Lattice Menu, and most subjects (8 out of 12) commented that the provisioning of visual targets facilitated more stable menu selections with reduced eye fatigue.

</details>


### [14] [STAR: Smartphone-analogous Typing in Augmented Reality](https://arxiv.org/abs/2511.21143)
*Taejun Kim,Amy Karlson,Aakar Gupta,Tovi Grossman,Jason Wu,Parastoo Abtahi,Christopher Collins,Michael Glueck,Hemant Bhaskar Surale*

Main category: cs.HC

TL;DR: 本文提出STAR，一种新的增强现实文本输入方法，利用用户对于智能手机键入的熟悉度，展示了该方法的输入速度和错误率数据。


<details>
  <summary>Details</summary>
Motivation: 当前增强现实应用中，文本输入效率和易用性仍然是一个开放的挑战。

Method: STAR方法，利用用户对于智能手机两指输入法的熟悉度，实现在增强现实（AR）中的虚拟QWERTY键盘输入。

Result: 参与者在使用STAR方法后，平均输入速度达到21.9词每分钟（WPM），为其智能手机输入速度的56%；同时，错误率为0.3%。

Conclusion: 进一步分析了STAR与智能手机输入之间的性能差距的主要因素，并讨论了缩小这一差距的方法。

Abstract: While text entry is an essential and frequent task in Augmented Reality (AR) applications, devising an efficient and easy-to-use text entry method for AR remains an open challenge. This research presents STAR, a smartphone-analogous AR text entry technique that leverages a user's familiarity with smartphone two-thumb typing. With STAR, a user performs thumb typing on a virtual QWERTY keyboard that is overlain on the skin of their hands. During an evaluation study of STAR, participants achieved a mean typing speed of 21.9 WPM (i.e., 56% of their smartphone typing speed), and a mean error rate of 0.3% after 30 minutes of practice. We further analyze the major factors implicated in the performance gap between STAR and smartphone typing, and discuss ways this gap could be narrowed.

</details>


### [15] [QuadStretcher: A Forearm-Worn Skin Stretch Display for Bare-Hand Interaction in AR/VR](https://arxiv.org/abs/2511.21157)
*Taejun Kim,Youngbo Aram Shim,Youngin Kim,Sunbum Kim,Jaeyeon Lee,Geehyuk Lee*

Main category: cs.HC

TL;DR: 本文提出QuadStretcher，一种用于增强AR/VR裸手交互的皮肤拉伸设备，并通过用户评估证实其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决在增强现实和虚拟现实中为裸手交互提供触觉反馈的挑战

Method: 设计并评估了一种名为QuadStretcher的皮肤拉伸显示设备

Result: QuadStretcher在表现力和现实感方面超越了基线方案Squeezer，尤其在3自由度的VR交互中效果显著

Conclusion: QuadStretcher为未来的前臂触觉系统提供了设计见解，推动AR/VR裸手体验的发展。

Abstract: The paradigm of bare-hand interaction has become increasingly prevalent in Augmented Reality (AR) and Virtual Reality (VR) environments, propelled by advancements in hand tracking technology. However, a significant challenge arises in delivering haptic feedback to users' hands, due to the necessity for the hands to remain bare. In response to this challenge, recent research has proposed an indirect solution of providing haptic feedback to the forearm. In this work, we present QuadStretcher, a skin stretch display featuring four independently controlled stretching units surrounding the forearm. While achieving rich haptic expression, our device also eliminates the need for a grounding base on the forearm by using a pair of counteracting tactors, thereby reducing bulkiness. To assess the effectiveness of QuadStretcher in facilitating immersive bare-hand experiences, we conducted a comparative user evaluation (n = 20) with a baseline solution, Squeezer. The results confirmed that QuadStretcher outperformed Squeezer in terms of expressing force direction and heightening the sense of realism, particularly in 3-DoF VR interactions such as pulling a rubber band, hooking a fishing rod, and swinging a tennis racket. We further discuss the design insights gained from qualitative user interviews, presenting key takeaways for future forearm-haptic systems aimed at advancing AR/VR bare-hand experiences.

</details>


### [16] [Generative AI Compensates for Age-Related Cognitive Decline in Decision Making: Preference-Aligned Recommendations Reduce Choice Difficulty](https://arxiv.org/abs/2511.21164)
*Sayaka Ishibashi,Kou Tamura,Ayana Goma,Kenta Yamamoto,Kouhei Masumoto*

Main category: cs.HC

TL;DR: 该研究探讨了生成型AI在决策中的应用，发现其可以减轻老年人的选择困难。


<details>
  <summary>Details</summary>
Motivation: 研究生成型AI能否在决策中提升老年人的选择满意度并减少选择困难

Method: 通过AI辅助的决策任务

Result: 使用AI时，老年人的选择困难显著降低，而选择满意度变化不大；AI帮助减轻了低认知功能与选择困难及满意度之间的关联。

Conclusion: 生成的选项推荐能够补偿老年人的信息搜索限制，降低选择感觉的困难，而不会影响满意度。

Abstract: Due to age-related declines in memory, processing speed, working memory, and executive functions, older adults experience difficulties in decision making when situations require novel choices, probabilistic judgments, rapid responses, or extensive information search. This study examined whether using generative AI during decision making enhances choice satisfaction and reduces choice difficulty among older adults. A total of 130 participants (younger: 56; older: 74) completed a music-selection task under AI-use and AI-nonuse conditions across two contexts: previously experienced (road trip) and not previously experienced (space travel). In the AI-nonuse condition, participants generated candidate options from memory; in the AI-use condition, GPT-4o presented options tailored to individual preferences. To assess cognitive function, we also administered the Wechsler Adult Intelligence Scale-Fourth Edition. Results revealed that in the AI-nonuse condition, older adults with lower cognitive function reported higher choice difficulty and lower choice satisfaction. Under the AI-use condition, choice satisfaction did not change significantly, but perceived choice difficulty decreased significantly in both age groups. Moreover, AI use attenuated the associations observed among older adults between lower cognitive function and both greater difficulty and lower satisfaction. These findings indicate that preference-aligned option recommendations generated by AI can compensate for age-related constraints on information search, thereby reducing perceived choice difficulty without diminishing satisfaction.

</details>


### [17] [TALES: A Taxonomy and Analysis of Cultural Representations in LLM-generated Stories](https://arxiv.org/abs/2511.21322)
*Kirti Bhagat,Shaily Bhatt,Athul Velagapudi,Aditya Vashistha,Shachi Dave,Danish Pruthi*

Main category: cs.HC

TL;DR: 本文提出TALES评估LLM生成故事的文化误表述，发现大部分生成故事存在文化 inaccuracies。


<details>
  <summary>Details</summary>
Motivation: 随着AI聊天机器人在创意需求上的广泛应用，研究其文化表现变得更加重要。

Method: TALES，一种评估LLM生成故事中文化误表述的方法

Result: 通过TALES-Tax评估了6个模型，发现88%的生成故事存在文化不准确性，尤其在中低资源语言和城市边缘故事中更为普遍。

Conclusion: 尽管模型生成的故事中存在文化误表述，但其在文化知识方面仍具有相应的能力。

Abstract: Millions of users across the globe turn to AI chatbots for their creative needs, inviting widespread interest in understanding how such chatbots represent diverse cultures. At the same time, evaluating cultural representations in open-ended tasks remains challenging and underexplored. In this work, we present TALES, an evaluation of cultural misrepresentations in LLM-generated stories for diverse Indian cultural identities. First, we develop TALES-Tax, a taxonomy of cultural misrepresentations by collating insights from participants with lived experiences in India through focus groups (N=9) and individual surveys (N=15). Using TALES-Tax, we evaluate 6 models through a large-scale annotation study spanning 2,925 annotations from 108 annotators with lived cultural experience from across 71 regions in India and 14 languages. Concerningly, we find that 88\% of the generated stories contain one or more cultural inaccuracies, and such errors are more prevalent in mid- and low-resourced languages and stories based in peri-urban regions in India. Lastly, we transform the annotations into TALES-QA, a standalone question bank to evaluate the cultural knowledge of foundational models. Through this evaluation, we surprisingly discover that models often possess the requisite cultural knowledge despite generating stories rife with cultural misrepresentations.

</details>


### [18] [Seeing Twice: How Side-by-Side T2I Comparison Changes Auditing Strategies](https://arxiv.org/abs/2511.21547)
*Matheus Kunzler Maldaner,Wesley Hanwen Deng,Jason I. Hong,Kenneth Holstein,Motahhare Eslami*

Main category: cs.HC

TL;DR: 本研究提出MIRAGE工具，通过并排比较多个文本到图像模型，帮助用户发现偏见，改善对生成性模型的理解。


<details>
  <summary>Details</summary>
Motivation: 随着生成性人工智能系统的普及，其潜在的有害输出限制了其可信度和效用，因此需要更多非AI专家用户参与审计这些系统。

Method: 设计并评估一个网页工具MIRAGE，用于生成文本到图像模型的比较与审计。

Result: 通过用户研究发现，大多数参与者在出现并排比较步骤后，从单一图像分析转向一般模型输出模式，并形成了对不同模型的'模型个性'的看法；双语参与者发现语言准确性存在差距。

Conclusion: 简单的比较界面能够加速偏见发现，并重新塑造人们对生成性模型的思考方式。

Abstract: While generative AI systems have gained popularity in diverse applications, their potential to produce harmful outputs limits their trustworthiness and utility. A small but growing line of research has explored tools and processes to better engage non-AI expert users in auditing generative AI systems. In this work, we present the design and evaluation of MIRAGE, a web-based tool exploring a "contrast-first" workflow that allows users to pick up to four different text-to-image (T2I) models, view their images side-by-side, and provide feedback on model performance on a single screen. In our user study with fifteen participants, we used four predefined models for consistency, with only a single model initially being shown. We found that most participants shifted from analyzing individual images to general model output patterns once the side-by-side step appeared with all four models; several participants coined persistent "model personalities" (e.g., cartoonish, saturated) that helped them form expectations about how each model would behave on future prompts. Bilingual participants also surfaced a language-fidelity gap, as English prompts produced more accurate images than Portuguese or Chinese, an issue often overlooked when dealing with a single model. These findings suggest that simple comparative interfaces can accelerate bias discovery and reshape how people think about generative models.

</details>


### [19] [MMA: A Momentum Mamba Architecture for Human Activity Recognition with Inertial Sensors](https://arxiv.org/abs/2511.21550)
*Thai-Khanh Nguyen,Uyen Vo,Tan M. Nguyen,Thieu N. Vo,Trung-Hieu Le,Cuong Pham*

Main category: cs.HC

TL;DR: 引入Momentum Mamba，一个增强动量的结构状态空间模型，提高了人类活动识别的准确性、稳定性和效率。


<details>
  <summary>Details</summary>
Motivation: 人类活动识别（HAR）在普适计算、移动健康和环境智能中至关重要，然而传统深度模型在处理长范围依赖性和计算成本上存在限制。

Method: 采用增强动量的结构状态空间模型（Momentum Mamba），结合二阶动力学和频率选择性记忆扩展。

Result: 提出了一种改进的结构状态空间模型Momentum Mamba，通过引入二阶动力学来增强信息流的稳定性和长序列建模能力。

Conclusion: Momentum Mamba模型在多个HAR基准实验中表现出一致的优势，证明其为HAR和更广泛序列建模应用提供了可伸缩的框架。

Abstract: Human activity recognition (HAR) from inertial sensors is essential for ubiquitous computing, mobile health, and ambient intelligence. Conventional deep models such as Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and transformers have advanced HAR but remain limited by vanishing or exloding gradients, high computational cost, and difficulty in capturing long-range dependencies. Structured state-space models (SSMs) like Mamba address these challenges with linear complexity and effective temporal modeling, yet they are restricted to first-order dynamics without stable longterm memory mechanisms. We introduce Momentum Mamba, a momentum-augmented SSM that incorporates second-order dynamics to improve stability of information flow across time steps, robustness, and long-sequence modeling. Two extensions further expand its capacity: Complex Momentum Mamba for frequency-selective memory scaling. Experiments on multiple HAR benchmarks demonstrate consistent gains over vanilla Mamba and Transformer baselines in accuracy, robustness, and convergence speed. With only moderate increases in training cost, momentum-augmented SSMs offer a favorable accuracy-efficiency balance, establishing them as a scalable paradigm for HAR and a promising principal framework for broader sequence modeling applications.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [20] [OVAL-Grasp: Open-Vocabulary Affordance Localization for Task Oriented Grasping](https://arxiv.org/abs/2511.20841)
*Edmond Tong,Advaith Balaji,Anthony Opipari,Stanley Lewis,Zhen Zeng,Odest Chadwicke Jenkins*

Main category: cs.RO

TL;DR: OVAL-Grasp是一种新型的任务导向抓取方法，利用大语言模型和视觉-语言模型，在抓取过程中准确识别和分割物体部分，性能超越了传统方法。


<details>
  <summary>Details</summary>
Motivation: 在非结构化环境中，机器人需要根据任务来操控对象，而传统的基于几何的方法在处理视觉定义部分、遮挡和未见对象时常常面临挑战。

Method: 提出了一种名为OVAL-Grasp的零-shot开放词汇任务导向抓取方法，结合大语言模型和视觉-语言模型，识别并分割需要抓取的物体部分。

Result: OVAL-Grasp在与20个家居物品及3个独特任务的实验中，成功识别和分割正确物体部分的准确率为95%，并且在真实世界实验中抓取正确可操作区域的准确率为78.3%。

Conclusion: OVAL-Grasp表现出在复杂场景中的有效性，尤其在部分遮挡情况下的选择成功率达到80%，显示了其在任务导向抓取中的潜力和优势。

Abstract: To manipulate objects in novel, unstructured environments, robots need task-oriented grasps that target object parts based on the given task. Geometry-based methods often struggle with visually defined parts, occlusions, and unseen objects. We introduce OVAL-Grasp, a zero-shot open-vocabulary approach to task-oriented, affordance based grasping that uses large-language models and vision-language models to allow a robot to grasp objects at the correct part according to a given task. Given an RGB image and a task, OVAL-Grasp identifies parts to grasp or avoid with an LLM, segments them with a VLM, and generates a 2D heatmap of actionable regions on the object. During our evaluations, we found that our method outperformed two task oriented grasping baselines on experiments with 20 household objects with 3 unique tasks for each. OVAL-Grasp successfully identifies and segments the correct object part 95% of the time and grasps the correct actionable area 78.3% of the time in real-world experiments with the Fetch mobile manipulator. Additionally, OVAL-Grasp finds correct object parts under partial occlusions, demonstrating a part selection success rate of 80% in cluttered scenes. We also demonstrate OVAL-Grasp's efficacy in scenarios that rely on visual features for part selection, and show the benefit of a modular design through our ablation experiments. Our project webpage is available at https://ekjt.github.io/OVAL-Grasp/

</details>


### [21] [NOIR 2.0: Neural Signal Operated Intelligent Robots for Everyday Activities](https://arxiv.org/abs/2511.20848)
*Tasha Kim,Yingke Wang,Hanvit Cho,Alex Hodges*

Main category: cs.RO

TL;DR: NOIR 2.0通过优化脑信号解码与少量学习算法，显著提高了机器人控制的效率。


<details>
  <summary>Details</summary>
Motivation: 开发一个能让人类通过脑信号控制机器人的系统，以提高日常任务的效率和便捷性。

Method: 采用脑电图(EEG)技术与少量学习算法，结合基础模型进行高效学习和适应。

Result: NOIR 2.0是NOIR的增强版，拥有更快更准的脑解码算法，并能适应个体用户的意图。

Conclusion: NOIR 2.0在任务完成时间和用户适应性方面表现优异，优化了人机交互体验。

Abstract: Neural Signal Operated Intelligent Robots (NOIR) system is a versatile brain-robot interface that allows humans to control robots for daily tasks using their brain signals. This interface utilizes electroencephalography (EEG) to translate human intentions regarding specific objects and desired actions directly into commands that robots can execute. We present NOIR 2.0, an enhanced version of NOIR. NOIR 2.0 includes faster and more accurate brain decoding algorithms, which reduce task completion time by 46%. NOIR 2.0 uses few-shot robot learning algorithms to adapt to individual users and predict their intentions. The new learning algorithms leverage foundation models for more sample-efficient learning and adaptation (15 demos vs. a single demo), significantly reducing overall human time by 65%.

</details>


### [22] [ACE-F: A Cross Embodiment Foldable System with Force Feedback for Dexterous Teleoperation](https://arxiv.org/abs/2511.20887)
*Rui Yan,Jiajian Fu,Shiqi Yang,Lars Paulsen,Xuxin Cheng,Xiaolong Wang*

Main category: cs.RO

TL;DR: ACE-F是一个集成功能反馈的可折叠遥控系统，通过创新的方法使复杂机器人操作更简单。


<details>
  <summary>Details</summary>
Motivation: 提升遥操作系统在复杂任务中的表现，解决缺乏集成功能反馈和可移植设计的问题。

Method: 提出一种具有集成功能反馈的跨形态可折叠遥控系统ACE-F，通过逆运动学和人机界面设计来捕获高质量的机器人演示数据。

Result: ACE-F系统显著简化了各种机器人操作的控制，使精巧的操作任务变得像使用电脑鼠标一样直观。

Conclusion: ACE-F在保证安全与精确运动控制的同时，推动了遥操作技术的发展，特别是在模仿学习中的应用。

Abstract: Teleoperation systems are essential for efficiently collecting diverse and high-quality robot demonstration data, especially for complex, contact-rich tasks. However, current teleoperation platforms typically lack integrated force feedback, cross-embodiment generalization, and portable, user-friendly designs, limiting their practical deployment. To address these limitations, we introduce ACE-F, a cross embodiment foldable teleoperation system with integrated force feedback. Our approach leverages inverse kinematics (IK) combined with a carefully designed human-robot interface (HRI), enabling users to capture precise and high-quality demonstrations effortlessly. We further propose a generalized soft-controller pipeline integrating PD control and inverse dynamics to ensure robot safety and precise motion control across diverse robotic embodiments. Critically, to achieve cross-embodiment generalization of force feedback without additional sensors, we innovatively interpret end-effector positional deviations as virtual force signals, which enhance data collection and enable applications in imitation learning. Extensive teleoperation experiments confirm that ACE-F significantly simplifies the control of various robot embodiments, making dexterous manipulation tasks as intuitive as operating a computer mouse. The system is open-sourced at: https://acefoldable.github.io/

</details>


### [23] [Efficient Greedy Algorithms for Feature Selection in Robot Visual Localization](https://arxiv.org/abs/2511.20894)
*Vivek Pandey,Amirhossein Mollaei,Nader Motee*

Main category: cs.RO

TL;DR: 本论文提出两种高效的特征选择算法，以改善机器人在未知环境中的定位性能，减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 图像帧中包含大量冗余或不信息的特征，处理所有特征会引入显著的计算延迟和低效，迫切需要智能特征选择。

Method: 提出两种快速且内存高效的特征选择算法，实时评估视觉特征的实用性。

Result: 所提算法在降低时间和内存复杂度的同时，实现了计算效率与定位精度之间的良好平衡。

Conclusion: 所提特征选择方法在实时性和内存效率方面表现优越，为机器人定位提供了重大的实用价值。

Abstract: Robot localization is a fundamental component of autonomous navigation in unknown environments. Among various sensing modalities, visual input from cameras plays a central role, enabling robots to estimate their position by tracking point features across image frames. However, image frames often contain a large number of features, many of which are redundant or uninformative for localization. Processing all features can introduce significant computational latency and inefficiency. This motivates the need for intelligent feature selection, identifying a subset of features that are most informative for localization over a prediction horizon. In this work, we propose two fast and memory-efficient feature selection algorithms that enable robots to actively evaluate the utility of visual features in real time. Unlike existing approaches with high computational and memory demands, the proposed methods are explicitly designed to reduce both time and memory complexity while achieving a favorable trade-off between computational efficiency and localization accuracy.

</details>


### [24] [Dynamic Test-Time Compute Scaling in Control Policy: Difficulty-Aware Stochastic Interpolant Policy](https://arxiv.org/abs/2511.20906)
*Inkook Chun,Seungjae Lee,Michael S. Albergo,Saining Xie,Eric Vanden-Eijnden*

Main category: cs.RO

TL;DR: DA-SIP框架通过动态调整控制步骤中的集成时间预算，实现了在各类机器人操控任务中更高效的计算，成功率与固定预算模型相当。


<details>
  <summary>Details</summary>
Motivation: 解决固定推理预算在不同任务复杂性下的计算效率问题，尤其在简单子任务和复杂任务表现不均的情况下。

Method: 利用困难分类器实时分析任务观察，动态选择步骤预算、最优求解变体和ODE/SDE集成。

Result: 引入了动态调整集成时间范围的困难感知随机插值策略（DA-SIP），实现了更高效的任务解决方案。

Conclusion: 通过自适应计算，DA-SIP将生成机器人控制器转变为高效且任务感知的系统，实现智能资源分配。

Abstract: Diffusion- and flow-based policies deliver state-of-the-art performance on long-horizon robotic manipulation and imitation learning tasks. However, these controllers employ a fixed inference budget at every control step, regardless of task complexity, leading to computational inefficiency for simple subtasks while potentially underperforming on challenging ones. To address these issues, we introduce Difficulty-Aware Stochastic Interpolant Policy (DA-SIP), a framework that enables robotic controllers to adaptively adjust their integration horizon in real time based on task difficulty. Our approach employs a difficulty classifier that analyzes observations to dynamically select the step budget, the optimal solver variant, and ODE/SDE integration at each control cycle. DA-SIP builds upon the stochastic interpolant formulation to provide a unified framework that unlocks diverse training and inference configurations for diffusion- and flow-based policies. Through comprehensive benchmarks across diverse manipulation tasks, DA-SIP achieves 2.6-4.4x reduction in total computation time while maintaining task success rates comparable to fixed maximum-computation baselines. By implementing adaptive computation within this framework, DA-SIP transforms generative robot controllers into efficient, task-aware systems that intelligently allocate inference resources where they provide the greatest benefit.

</details>


### [25] [AerialMind: Towards Referring Multi-Object Tracking in UAV Scenarios](https://arxiv.org/abs/2511.21053)
*Chenglizhao Chen,Shaofeng Liang,Runwei Guan,Xiaolou Sun,Haocheng Zhao,Haiyun Jiang,Tao Huang,Henghui Ding,Qing-Long Han*

Main category: cs.RO

TL;DR: 本研究提出了AerialMind，这是第一个针对无人机场景的大规模参考多目标跟踪基准，并开发了COALA框架和HETrack方法，以提高无机交互的智能系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有的参考多目标跟踪研究主要集中于地面场景，限制了智能系统在广泛场景中的应用能力，因此需要在无人机场景中开展新的研究。

Method: 开发了COALA框架用于半自动化协作标注，以及HETrack方法用于增强无人机场景的视觉-语言表征学习。

Result: 通过全面实验验证了数据集的挑战性及方法的有效性。

Conclusion: AerialMind为无人机领域的参考多目标跟踪提供了一个重要基准，COALA框架和HETrack方法有效提升了数据标注效率和视觉-语言表征学习能力。

Abstract: Referring Multi-Object Tracking (RMOT) aims to achieve precise object detection and tracking through natural language instructions, representing a fundamental capability for intelligent robotic systems. However, current RMOT research remains mostly confined to ground-level scenarios, which constrains their ability to capture broad-scale scene contexts and perform comprehensive tracking and path planning. In contrast, Unmanned Aerial Vehicles (UAVs) leverage their expansive aerial perspectives and superior maneuverability to enable wide-area surveillance. Moreover, UAVs have emerged as critical platforms for Embodied Intelligence, which has given rise to an unprecedented demand for intelligent aerial systems capable of natural language interaction. To this end, we introduce AerialMind, the first large-scale RMOT benchmark in UAV scenarios, which aims to bridge this research gap. To facilitate its construction, we develop an innovative semi-automated collaborative agent-based labeling assistant (COALA) framework that significantly reduces labor costs while maintaining annotation quality. Furthermore, we propose HawkEyeTrack (HETrack), a novel method that collaboratively enhances vision-language representation learning and improves the perception of UAV scenarios. Comprehensive experiments validated the challenging nature of our dataset and the effectiveness of our method.

</details>


### [26] [Dual-Agent Reinforcement Learning for Adaptive and Cost-Aware Visual-Inertial Odometry](https://arxiv.org/abs/2511.21083)
*Feiyang Pan,Shenghe Zheng,Chunyan Yin,Guangbin Dou*

Main category: cs.RO

TL;DR: 本研究提出一种结合强化学习的视觉惯性测程方法，优化了准确性与效率的平衡，确保在资源有限的环境中有效运行。


<details>
  <summary>Details</summary>
Motivation: 视觉惯性测程（VIO）在自主导航和增强现实中的应用至关重要，但当前的方法在效率和准确性之间面临权衡。

Method: 使用轻量级强化学习（RL）代理，通过两个关键设计选择（视觉前端的运行时机和对其输出的信任程度）来优化视觉惯性测程的性能。

Result: 在EuRoC MAV和TUM-VI数据集上的实验表明，所提出的方法在准确性、速度和内存使用上均表现出色，平均绝对轨迹误差（ATE）达到最佳，同时运行速度提高至1.77倍，GPU内存使用量减少。

Conclusion: 本研究提出的方法在准确性、效率和内存使用方面优于现有的GPU基础的视觉位置/视觉惯性系统，同时保持竞争的轨迹精度并显著减少计算负载。

Abstract: Visual-Inertial Odometry (VIO) is a critical component for robust ego-motion estimation, enabling foundational capabilities such as autonomous navigation in robotics and real-time 6-DoF tracking for augmented reality. Existing methods face a well-known trade-off: filter-based approaches are efficient but prone to drift, while optimization-based methods, though accurate, rely on computationally prohibitive Visual-Inertial Bundle Adjustment (VIBA) that is difficult to run on resource-constrained platforms. Rather than removing VIBA altogether, we aim to reduce how often and how heavily it must be invoked. To this end, we cast two key design choices in modern VIO, when to run the visual frontend and how strongly to trust its output, as sequential decision problems, and solve them with lightweight reinforcement learning (RL) agents. Our framework introduces a lightweight, dual-pronged RL policy that serves as our core contribution: (1) a Select Agent intelligently gates the entire VO pipeline based only on high-frequency IMU data; and (2) a composite Fusion Agent that first estimates a robust velocity state via a supervised network, before an RL policy adaptively fuses the full (p, v, q) state. Experiments on the EuRoC MAV and TUM-VI datasets show that, in our unified evaluation, the proposed method achieves a more favorable accuracy-efficiency-memory trade-off than prior GPU-based VO/VIO systems: it attains the best average ATE while running up to 1.77 times faster and using less GPU memory. Compared to classical optimization-based VIO systems, our approach maintains competitive trajectory accuracy while substantially reducing computational load.

</details>


### [27] [SocialNav: Training Human-Inspired Foundation Model for Socially-Aware Embodied Navigation](https://arxiv.org/abs/2511.21135)
*Ziyi Chen,Yingnan Guo,Zedong Chu,Minghua Luo,Yanfen Shen,Mingchao Sun,Junjun Hu,Shichao Xie,Kuan Yang,Pei Shi,Zhining Gu,Lu Liu,Honglin Han,Xiaolong Wu,Mu Xu,Yu Zhang*

Main category: cs.RO

TL;DR: 本研究提出了一种社交意识导航模型SocialNav，能够理解社会规范并生成符合这些规范的导航轨迹。


<details>
  <summary>Details</summary>
Motivation: 解决社会意识导航中遵循社会规范的研究挑战。

Method: 建立了一个包含710万样本的SocNav数据集，通过多阶段的训练流程，首先进行模仿学习注入导航技能和社交规范理解，然后通过SAFE-GRPO框架细化技能。

Result: SocialNav在成功率上提高了38%，在社会合规率上提高了46%。

Conclusion: SocialNav相比于现有的方法在导航成功率和社会合规性方面均有显著提高，展示了在社交意识导航领域的强大能力。

Abstract: Embodied navigation that adheres to social norms remains an open research challenge. Our \textbf{SocialNav} is a foundational model for socially-aware navigation with a hierarchical "brain-action" architecture, capable of understanding high-level social norms and generating low-level, socially compliant trajectories. To enable such dual capabilities, we construct the SocNav Dataset, a large-scale collection of 7 million samples, comprising (1) a Cognitive Activation Dataset providing social reasoning signals such as chain-of-thought explanations and social traversability prediction, and (2) an Expert Trajectories Pyramid aggregating diverse navigation demonstrations from internet videos, simulated environments, and real-world robots. A multi-stage training pipeline is proposed to gradually inject and refine navigation intelligence: we first inject general navigation skills and social norms understanding into the model via imitation learning, and then refine such skills through a deliberately designed Socially-Aware Flow Exploration GRPO (SAFE-GRPO), the first flow-based reinforcement learning framework for embodied navigation that explicitly rewards socially compliant behaviors. SocialNav achieves +38% success rate and +46% social compliance rate compared to the state-of-the-art method, demonstrating strong gains in both navigation performance and social compliance. Our project page: https://amap-eai.github.io/SocialNav/

</details>


### [28] [Maglev-Pentabot: Magnetic Levitation System for Non-Contact Manipulation using Deep Reinforcement Learning](https://arxiv.org/abs/2511.21149)
*Guoming Huang,Qingyi Zhou,Dianjing Liu,Shuai Zhang,Ming Zhou,Zongfu Yu*

Main category: cs.RO

TL;DR: 本研究提出Maglev-Pentabot系统，通过深度强化学习实现克级非接触操控，克服现有技术限制，并展示良好的泛化能力和扩展性。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有2D和3D非接触操控技术在处理克级对象时的局限性，提出新的系统以扩展非接触操控的应用范围。

Method: 本研究利用深度强化学习（DRL）开发复杂的控制策略，同时优化了电磁体排列以最大化可控空间，并引入了动作重映射方法解决样本稀疏问题。

Result: 实验结果表明该系统具备灵活的操控能力，并能推广到未经过明确训练的运输任务，同时能通过更大电磁体进行更重物体的操控。

Conclusion: 所提出的Maglev-Pentabot系统能有效地非接触操控克服了目前技术在微观尺度上的限制，并展示出在没有明确训练的情况下进行运输任务的泛化能力。

Abstract: Non-contact manipulation has emerged as a transformative approach across various industrial fields. However, current flexible 2D and 3D non-contact manipulation techniques are often limited to microscopic scales, typically controlling objects in the milligram range. In this paper, we present a magnetic levitation system, termed Maglev-Pentabot, designed to address this limitation. The Maglev-Pentabot leverages deep reinforcement learning (DRL) to develop complex control strategies for manipulating objects in the gram range. Specifically, we propose an electromagnet arrangement optimized through numerical analysis to maximize controllable space. Additionally, an action remapping method is introduced to address sample sparsity issues caused by the strong nonlinearity in magnetic field intensity, hence allowing the DRL controller to converge. Experimental results demonstrate flexible manipulation capabilities, and notably, our system can generalize to transport tasks it has not been explicitly trained for. Furthermore, our approach can be scaled to manipulate heavier objects using larger electromagnets, offering a reference framework for industrial-scale robotic applications.

</details>


### [29] [MarketGen: A Scalable Simulation Platform with Auto-Generated Embodied Supermarket Environments](https://arxiv.org/abs/2511.21161)
*Xu Hu,Yiyang Feng,Junran Peng,Jiawei He,Liyi Chen,Chuanchen Luo,Xucheng Yin,Qing Li,Zhaoxiang Zhang*

Main category: cs.RO

TL;DR: 本文推出了MarketGen，一个针对复杂商业环境的模拟平台，解决了现有机器人数据集中关于超市环境的不足，并提出了一种新基准用于评估超市代理。


<details>
  <summary>Details</summary>
Motivation: 由于现有机器人数据集缺乏多样性，主要集中于家庭等简单环境，本文旨在提供一种解决方案，以支持复杂商业环境的研究发展。

Method: 通过基于代理的程序内容生成框架，支持多模态输入，自动生成超市环境，并通过任务基准进行评估。

Result: 本文介绍了市场生成（MarketGen），一个用于复杂商业环境的可扩展模拟平台，以填补现有机器人数据集和基准测试的空白，后者主要集中在家庭或桌面环境中的短期任务。该平台具有新颖的基于代理的程序内容生成框架，支持文本和参考图像的多模态输入，并结合现实设计原则自动生成完整、结构化和真实的超市环境。此外，提供了一个广泛多样的3D资产库，包含1100多种超市商品和参数化设施资产。基于此生成基础，提出了一种新的基准，以评估超市代理，包含两个日常超市任务。通过广泛实验验证平台和基准，包括模块化代理系统的部署和成功的模拟到现实转移，MarketGen为复杂商业应用中的体现AI研究提供了全面框架。

Conclusion: MarketGen为复杂商业应用中的体现AI研究提供了一个系统的框架，促进了代理系统在超市环境中表现的评估和优化。

Abstract: The development of embodied agents for complex commercial environments is hindered by a critical gap in existing robotics datasets and benchmarks, which primarily focus on household or tabletop settings with short-horizon tasks. To address this limitation, we introduce MarketGen, a scalable simulation platform with automatic scene generation for complex supermarket environments. MarketGen features a novel agent-based Procedural Content Generation (PCG) framework. It uniquely supports multi-modal inputs (text and reference images) and integrates real-world design principles to automatically generate complete, structured, and realistic supermarkets. We also provide an extensive and diverse 3D asset library with a total of 1100+ supermarket goods and parameterized facilities assets. Building on this generative foundation, we propose a novel benchmark for assessing supermarket agents, featuring two daily tasks in a supermarket: (1) Checkout Unloading: long-horizon tabletop tasks for cashier agents, and (2) In-Aisle Item Collection: complex mobile manipulation tasks for salesperson agents. We validate our platform and benchmark through extensive experiments, including the deployment of a modular agent system and successful sim-to-real transfer. MarketGen provides a comprehensive framework to accelerate research in embodied AI for complex commercial applications.

</details>


### [30] [Kinematics-Aware Multi-Policy Reinforcement Learning for Force-Capable Humanoid Loco-Manipulation](https://arxiv.org/abs/2511.21169)
*Kaiyan Xiao,Zihan Xu,Cheng Zhe,Chengju Liu,Qijun Chen*

Main category: cs.RO

TL;DR: 本论文提出了一种新型的强化学习框架，改善了机器人在高负载工业环境中的灵巧性和主动力交互能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多足操作方法主要集中在灵巧操作上，而在高负载工业场景中，灵巧性和主动力交互的要求未得到满足，因此需要新的方法来解决这个问题。

Method: 采用解耦的三阶段训练流程，并设计启发式奖励函数以加快上肢训练，同时为下肢实施基于力的课程学习策略。

Result: 提出了一种基于强化学习的框架，该框架采用解耦的三阶段训练管道，分别针对上下肢，并包含一个增量命令策略，以优化训练过程。

Conclusion: 通过引入启发式奖励函数和基于力的课程学习策略，机器人能够在互动中更有效地调节和施加力，以满足工业应用的要求。

Abstract: Humanoid robots, with their human-like morphology, hold great potential for industrial applications. However, existing loco-manipulation methods primarily focus on dexterous manipulation, falling short of the combined requirements for dexterity and proactive force interaction in high-load industrial scenarios. To bridge this gap, we propose a reinforcement learning-based framework with a decoupled three-stage training pipeline, consisting of an upper-body policy, a lower-body policy, and a delta-command policy. To accelerate upper-body training, a heuristic reward function is designed. By implicitly embedding forward kinematics priors, it enables the policy to converge faster and achieve superior performance. For the lower body, a force-based curriculum learning strategy is developed, enabling the robot to actively exert and regulate interaction forces with the environment.

</details>


### [31] [Dual Preintegration for Relative State Estimation](https://arxiv.org/abs/2511.21189)
*Ruican Xia,Hailong Pei*

Main category: cs.RO

TL;DR: 本文提出了一种双重预积分的方法，通过将两个移动平台的IMU预积分结合起来，旨在提高相对状态估计的精度，尤其是在虚拟现实应用中。


<details>
  <summary>Details</summary>
Motivation: 移动代理的相对状态估计对于在六自由度运动下的互相定位至关重要，尤其是在虚拟现实等应用中，位置精度受到非线性旋转和线性化误差的显著影响。

Method: 本文的方法基于IMU预积分，采用双重预积分的技术作为运动约束，进行状态的相对估计，并通过仿真和实地实验进行算法评估。

Result: 提出了一种新颖的双重预积分方法，该方法通过整合两个平台的IMU预积分，提供运动约束并有效减小线性化误差，显著提高相对状态估计的精度和鲁棒性。

Conclusion: 实验结果表明，所提出的双重预积分方法在相对状态估计中比现有的先进算法具有更高的精度和鲁棒性，尤其在面对非线性旋转和背景纹理的干扰时。

Abstract: Relative State Estimation perform mutually localization between two mobile agents undergoing six-degree-of-freedom motion. Based on the principle of circular motion, the estimation accuracy is sensitive to nonlinear rotations of the reference platform, particularly under large inter-platform distances. This phenomenon is even obvious for linearized kinematics, because cumulative linearization errors significantly degrade precision. In virtual reality (VR) applications, this manifests as substantial positional errors in 6-DoF controller tracking during rapid rotations of the head-mounted display. The linearization errors introduce drift in the estimate and render the estimator inconsistent. In the field of odometry, IMU preintegration is proposed as a kinematic observation to enable efficient relinearization, thus mitigate linearized error. Building on this theory, we propose dual preintegration, a novel observation integrating IMU preintegration from both platforms. This method serves as kinematic constraints for consecutive relative state and supports efficient relinearization. We also perform observability analysis of the state and analytically formulate the accordingly null space. Algorithm evaluation encompasses both simulations and real-world experiments. Multiple nonlinear rotations on the reference platform are simulated to compare the precision of the proposed method with that of other state-of-the-art (SOTA) algorithms. The field test compares the proposed method and SOTA algorithms in the application of VR controller tracking from the perspectives of bias observability, nonlinear rotation, and background texture. The results demonstrate that the proposed method is more precise and robust than the SOTA algorithms.

</details>


### [32] [Transformer Driven Visual Servoing and Dual Arm Impedance Control for Fabric Texture Matching](https://arxiv.org/abs/2511.21203)
*Fuyuki Tokuda,Akira Seino,Akinari Kobayashi,Kai Tang,Kazuhiro Kosuge*

Main category: cs.RO

TL;DR: 提出了一种新方法，通过双臂操控器和灰度摄像头将一块织物精确对齐并放置在另一块织物上，以匹配表面纹理。


<details>
  <summary>Details</summary>
Motivation: 开发一种精确对齐和放置织物的方法，以实现织物表面纹理的匹配。

Method: 采用Transformer驱动的视觉伺服控制与双臂阻抗控制相结合，并引入了差异提取注意模块（DEAM）以提高姿态预测精度。

Result: 提出了一种基于Transformer的视觉伺服和双臂阻抗控制的控制方案。

Conclusion: 实验表明，所提系统能够准确对齐不同纹理的织物。

Abstract: In this paper, we propose a method to align and place a fabric piece on top of another using a dual-arm manipulator and a grayscale camera, so that their surface textures are accurately matched. We propose a novel control scheme that combines Transformer-driven visual servoing with dualarm impedance control. This approach enables the system to simultaneously control the pose of the fabric piece and place it onto the underlying one while applying tension to keep the fabric piece flat. Our transformer-based network incorporates pretrained backbones and a newly introduced Difference Extraction Attention Module (DEAM), which significantly enhances pose difference prediction accuracy. Trained entirely on synthetic images generated using rendering software, the network enables zero-shot deployment in real-world scenarios without requiring prior training on specific fabric textures. Real-world experiments demonstrate that the proposed system accurately aligns fabric pieces with different textures.

</details>


### [33] [Sampling-Based Optimization with Parallelized Physics Simulator for Bimanual Manipulation](https://arxiv.org/abs/2511.21264)
*Iryna Hurova,Alinjar Dan,Karl Kruusamäe,Arun Kumar Singh*

Main category: cs.RO

TL;DR: 本文提出一种新颖的基于采样的优化框架，利用GPU加速的物理仿真器处理复杂的双臂操作任务，克服现有学习方法在新场景中的泛化限制。


<details>
  <summary>Details</summary>
Motivation: 鉴于现有基于学习的方法难以在复杂环境中泛化，研究者们希望找到一种更可靠的替代方案。

Method: 通过自定义的模型预测路径积分控制（MPPI）算法，结合特定任务的成本函数，利用MuJoCo进行机器人与物体交互的高效评估。

Result: 该方法能够在静态障碍物的环境中成功解决更复杂的双臂操控任务，并且在PerAct²基准测试中表现出色。

Conclusion: 该方法在常见GPU上实现实时性能，并且成功实现了仿真到现实的转移，具备良好的样本复杂性和稳健性。

Abstract: In recent years, dual-arm manipulation has become an area of strong interest in robotics, with end-to-end learning emerging as the predominant strategy for solving bimanual tasks. A critical limitation of such learning-based approaches, however, is their difficulty in generalizing to novel scenarios, especially within cluttered environments. This paper presents an alternative paradigm: a sampling-based optimization framework that utilizes a GPU-accelerated physics simulator as its world model. We demonstrate that this approach can solve complex bimanual manipulation tasks in the presence of static obstacles. Our contribution is a customized Model Predictive Path Integral Control (MPPI) algorithm, \textbf{guided by carefully designed task-specific cost functions,} that uses GPU-accelerated MuJoCo for efficiently evaluating robot-object interaction. We apply this method to solve significantly more challenging versions of tasks from the PerAct$^{2}$ benchmark, such as requiring the point-to-point transfer of a ball through an obstacle course. Furthermore, we establish that our method achieves real-time performance on commodity GPUs and facilitates successful sim-to-real transfer by leveraging unique features within MuJoCo. The paper concludes with a statistical analysis of the sample complexity and robustness, quantifying the performance of our approach. The project website is available at: https://sites.google.com/view/bimanualakslabunitartu .

</details>


### [34] [Improvement of Collision Avoidance in Cut-In Maneuvers Using Time-to-Collision Metrics](https://arxiv.org/abs/2511.21280)
*Jamal Raiyn*

Main category: cs.RO

TL;DR: 本文提出一种新的基于深度学习的时间-碰撞(TTC)指标的碰撞避免策略，以应对自动驾驶车辆在切入场景中的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶车辆在插入场景中面临的碰撞风险

Method: 将深度学习与TTC计算相结合，预测碰撞并制定规避措施。

Result: 提出了一种基于深度学习的TTC碰撞避免策略，能够有效预测潜在碰撞并决定规避措施

Conclusion: 结合深度学习的TTC方法显著改善了传统TTC方法的表现，提升了自动驾驶的安全性。

Abstract: This paper proposes a new strategy for collision avoidance system leveraging Time-to-Collision (TTC) metrics for handling cut-in scenarios, which are particularly challenging for autonomous vehicles (AVs). By integrating a deep learning with TTC calculations, the system predicts potential collisions and determines appropriate evasive actions compared to traditional TTC -based approaches.

</details>


### [35] [Neural NMPC through Signed Distance Field Encoding for Collision Avoidance](https://arxiv.org/abs/2511.21312)
*Martin Jacquet,Marvin Harms,Kostas Alexis*

Main category: cs.RO

TL;DR: 本文提出了一种基于深度学习的NMPC框架，适用于未知环境中的飞行机器人，无需地图且能有效避免碰撞。


<details>
  <summary>Details</summary>
Motivation: 希望提高飞行机器人的自主导航能力，使其在未知和复杂的环境中有效地避免碰撞。

Method: 使用深度神经网络将范围图像编码为SDF，随后通过两个级联网络进行碰撞避免和速度跟踪。

Result: 提出了一种神经非线性模型预测控制（NMPC）框架，适用于在未知环境中进行无地图、无碰撞的飞行机器人导航，使用机载范围传感器。该方法通过深度神经网络将单个范围图像编码为有符号距离函数（SDF），并利用级联的神经网络结构进行碰撞避免和速度跟踪。

Conclusion: 实地实验表明，该神经NMPC在复杂环境中可以有效地实现碰撞避免，能够应对不稳定的速度参考输入和漂移的位置估计。

Abstract: This paper introduces a neural Nonlinear Model Predictive Control (NMPC) framework for mapless, collision-free navigation in unknown environments with Aerial Robots, using onboard range sensing. We leverage deep neural networks to encode a single range image, capturing all the available information about the environment, into a Signed Distance Function (SDF). The proposed neural architecture consists of two cascaded networks: a convolutional encoder that compresses the input image into a low-dimensional latent vector, and a Multi-Layer Perceptron that approximates the corresponding spatial SDF. This latter network parametrizes an explicit position constraint used for collision avoidance, which is embedded in a velocity-tracking NMPC that outputs thrust and attitude commands to the robot. First, a theoretical analysis of the contributed NMPC is conducted, verifying recursive feasibility and stability properties under fixed observations. Subsequently, we evaluate the open-loop performance of the learning-based components as well as the closed-loop performance of the controller in simulations and experiments. The simulation study includes an ablation study, comparisons with two state-of-the-art local navigation methods, and an assessment of the resilience to drifting odometry. The real-world experiments are conducted in forest environments, demonstrating that the neural NMPC effectively performs collision avoidance in cluttered settings against an adversarial reference velocity input and drifting position estimates.

</details>


### [36] [Hybrid Control for Robotic Nut Tightening Task](https://arxiv.org/abs/2511.21366)
*Dmitri Kovalenko*

Main category: cs.RO

TL;DR: 本文介绍了一种新型的自主机器人螺母拧紧系统，具有快速和低接触力的特点。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在开发一个高效、灵活的机器人螺母拧紧系统，以提高工业自动化过程的精度和效率。

Method: 该系统采用了分层运动原语规划器和控制切换方案，以在力控制和位置控制之间交替。

Result: 提出了一种自主机器人螺母拧紧系统，该系统适用于配备并联抓手的串行机械手。

Conclusion: 所提出的控制器可以在比基准快14%的速度下拧紧螺纹螺丝，同时施加的接触力降低至基准的40倍，且系统实施已开源。

Abstract: An autonomous robotic nut tightening system for a serial manipulator equipped with a parallel gripper is proposed. The system features a hierarchical motion-primitive-based planner and a control-switching scheme that alternates between force and position control. Extensive simulations demonstrate the system's robustness to variance in initial conditions. Additionally, the proposed controller tightens threaded screws 14% faster than the baseline while applying 40 times less contact force on manipulands. For the benefit of the research community, the system's implementation is open-sourced.

</details>


### [37] [$\mathcal{E}_0$: Enhancing Generalization and Fine-Grained Control in VLA Models via Continuized Discrete Diffusion](https://arxiv.org/abs/2511.21542)
*Zhihao Zhan,Jiaying Zhou,Likui Zhang,Qinhan Lv,Hao Liu,Jusheng Zhang,Weizheng Li,Ziliang Chen,Tianshui Chen,Keze Wang,Liang Lin,Guangrun Wang*

Main category: cs.RO

TL;DR: E0是一种基于离散扩散框架的VLA模型，优化了机器人动作生成与控制，改善了鲁棒性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前的VLA模型在复杂任务、场景和视角上泛化能力较差，本研究旨在提升模型的泛化能力和动作控制精确度。

Method: E0采用了连续离散扩散框架，将动作生成视为对量化动作标记的迭代去噪处理，结合了离散动作标记的优点以提升语义条件。

Result: E0模型在14个不同环境中展现出最先进的性能，相较于强基准平均提升10.7%。

Conclusion: E0模型通过离散扩散实现了更精确和稳定的机器人操控，展示了在通用VLA策略学习中的潜力。

Abstract: Vision-Language-Action (VLA) models offer a unified framework for robotic manipulation by integrating visual perception, language understanding, and control generation. Yet existing VLA models still struggle to generalize across diverse tasks, scenes, and camera viewpoints, and often produce coarse or unstable actions. We introduce E0, a continuized discrete diffusion framework that formulates action generation as iterative denoising over quantized action tokens. Compared with continuous diffusion policies, E0 offers two key advantages: (1) discrete action tokens align naturally with the symbolic structure of pretrained VLM/VLA backbones, enabling stronger semantic conditioning; and 2. discrete diffusion matches the true quantized nature of real-world robot control-whose hardware constraints (e.g., encoder resolution, control frequency, actuation latency) inherently discretize continuous signals-and therefore benefits from a Bayes-optimal denoiser that models the correct discrete action distribution, leading to stronger generalization. Compared with discrete autoregressive and mask-based discrete diffusion models, E0 supports a significantly larger and finer-grained action vocabulary and avoids the distributional mismatch introduced by masking-based corruptions-yielding more accurate fine-grained action control. We further introduce a spherical viewpoint perturbation augmentation method to improve robustness to camera shifts without additional data. Experiments on LIBERO, VLABench, and ManiSkill show that E0 achieves state-of-the-art performance across 14 diverse environments, outperforming strong baselines by 10.7% on average. Real-world evaluation on a Franka arm confirms that E0 delivers precise, robust, and transferable manipulation, establishing discrete diffusion as a promising direction for generalizable VLA policy learning.

</details>


### [38] [VacuumVLA: Boosting VLA Capabilities via a Unified Suction and Gripping Tool for Complex Robotic Manipulation](https://arxiv.org/abs/2511.21557)
*Hui Zhou,Siyuan Huang,Minxing Li,Hao Zhang,Lue Fan,Shaoshuai Shi*

Main category: cs.RO

TL;DR: 本文提出了一种新的硬件设计，结合两指夹持器和真空吸力，提升了机器人在复杂任务中的操作能力。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉语言动作模型在机器人操作中应用广泛，但常用的两指夹持器在某些任务中受限，无法有效完成如擦拭玻璃表面或打开无把手的抽屉等实际任务。

Method: 通过与DexVLA和Pi0两种最新的视觉语言动作框架的实验验证，展示了我们设计的效率和实用性。

Result: 我们提出了一种低成本的集成硬件设计，结合了机械两指夹持器和真空吸附单元，使得单一末端执行器能够实现双模式操作。

Conclusion: 实验结果表明，该混合末端执行器的应用使得机器人能够成功完成传统两指夹持器无法实现的多种复杂任务，增强了机器人操作的灵活性和范围。

Abstract: Vision Language Action models have significantly advanced general purpose robotic manipulation by harnessing large scale pretrained vision and language representations. Among existing approaches, a majority of current VLA systems employ parallel two finger grippers as their default end effectors. However, such grippers face inherent limitations in handling certain real world tasks such as wiping glass surfaces or opening drawers without handles due to insufficient contact area or lack of adhesion. To overcome these challenges, we present a low cost, integrated hardware design that combines a mechanical two finger gripper with a vacuum suction unit, enabling dual mode manipulation within a single end effector. Our system supports flexible switching or synergistic use of both modalities, expanding the range of feasible tasks. We validate the efficiency and practicality of our design within two state of the art VLA frameworks: DexVLA and Pi0. Experimental results demonstrate that with the proposed hybrid end effector, robots can successfully perform multiple complex tasks that are infeasible for conventional two finger grippers alone. All hardware designs and controlling systems will be released.

</details>


### [39] [Model-Based Policy Adaptation for Closed-Loop End-to-End Autonomous Driving](https://arxiv.org/abs/2511.21584)
*Haohong Lin,Yunzhi Zhang,Wenhao Ding,Jiajun Wu,Ding Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种基于模型的策略适应框架（MPA）来提升E2E自动驾驶模型在闭环环境中的鲁棒性和安全性，实验结果表明该方法有显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: E2E自动驾驶模型在开放循环评估中表现出色，但在闭环环境中却容易遭遇级联错误和泛化能力差的问题。因此需要一种新的框架来提高其鲁棒性和安全性。

Method: 提出了一种基于模型的策略适应方法（MPA），利用几何一致的仿真引擎生成多样的反事实轨迹，并训练扩散式政策适配器和多步Q值模型。

Result: 通过在nuScenes基准测试上的实验，MPA方法在领域内、领域外及安全关键场景上都取得了显著的性能提升，验证了反事实数据规模和推理时指导策略对整体效果的影响。

Conclusion: MPA框架显著提升了E2E自动驾驶模型在各种场景中的性能，尤其是在安全关键的应用中表现尤为突出。

Abstract: End-to-end (E2E) autonomous driving models have demonstrated strong performance in open-loop evaluations but often suffer from cascading errors and poor generalization in closed-loop settings. To address this gap, we propose Model-based Policy Adaptation (MPA), a general framework that enhances the robustness and safety of pretrained E2E driving agents during deployment. MPA first generates diverse counterfactual trajectories using a geometry-consistent simulation engine, exposing the agent to scenarios beyond the original dataset. Based on this generated data, MPA trains a diffusion-based policy adapter to refine the base policy's predictions and a multi-step Q value model to evaluate long-term outcomes. At inference time, the adapter proposes multiple trajectory candidates, and the Q value model selects the one with the highest expected utility. Experiments on the nuScenes benchmark using a photorealistic closed-loop simulator demonstrate that MPA significantly improves performance across in-domain, out-of-domain, and safety-critical scenarios. We further investigate how the scale of counterfactual data and inference-time guidance strategies affect overall effectiveness.

</details>


### [40] [Uncertainty Quantification for Visual Object Pose Estimation](https://arxiv.org/abs/2511.21666)
*Lorenzo Shaikewitz,Charis Georgiou,Luca Carlone*

Main category: cs.RO

TL;DR: 本研究提出了一种无分布假设的姿态不确定性估计方法SLUE，显著提高了不确定性边界的紧凑性，并在多个测试场景中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 在机器人控制与规划中，量化物体姿态估计的不确定性是关键，但传统方法依赖严格的分布假设，限制了应用的灵活性。

Method: 开发一种无分布假设的姿态不确定性边界，通过SLUE方法将复杂的非凸约束简化为单一的椭圆不确定性边界。

Result: SLUE在姿态估计数据集和无人机跟踪场景中表现优异，生成的平移和方向不确定性边界均优于已有方法。

Conclusion: SLUE提供了一种有效的方法来量化单目姿态估计中的不确定性，生成的边界相较于以往方法显著缩小，且具备高置信度。

Abstract: Quantifying the uncertainty of an object's pose estimate is essential for robust control and planning. Although pose estimation is a well-studied robotics problem, attaching statistically rigorous uncertainty is not well understood without strict distributional assumptions. We develop distribution-free pose uncertainty bounds about a given pose estimate in the monocular setting. Our pose uncertainty only requires high probability noise bounds on pixel detections of 2D semantic keypoints on a known object. This noise model induces an implicit, non-convex set of pose uncertainty constraints. Our key contribution is SLUE (S-Lemma Uncertainty Estimation), a convex program to reduce this set to a single ellipsoidal uncertainty bound that is guaranteed to contain the true object pose with high probability. SLUE solves a relaxation of the minimum volume bounding ellipsoid problem inspired by the celebrated S-lemma. It requires no initial guess of the bound's shape or size and is guaranteed to contain the true object pose with high probability. For tighter uncertainty bounds at the same confidence, we extend SLUE to a sum-of-squares relaxation hierarchy which is guaranteed to converge to the minimum volume ellipsoidal uncertainty bound for a given set of keypoint constraints. We show this pose uncertainty bound can easily be projected to independent translation and axis-angle orientation bounds. We evaluate SLUE on two pose estimation datasets and a real-world drone tracking scenario. Compared to prior work, SLUE generates substantially smaller translation bounds and competitive orientation bounds. We release code at https://github.com/MIT-SPARK/PoseUncertaintySets.

</details>


### [41] [TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos](https://arxiv.org/abs/2511.21690)
*Seungjae Lee,Yoonkyo Jung,Inkook Chun,Yao-Chih Lee,Zikui Cai,Hongjia Huang,Aayush Talreja,Tan Dat Dao,Yongyuan Liang,Jia-Bin Huang,Furong Huang*

Main category: cs.RO

TL;DR: 提出一种新气象的TraceGen模型，能够在小数据量的情况下实现跨表现与环境的机器人任务学习，具有较快的推理速度和高成功率。


<details>
  <summary>Details</summary>
Motivation: 在有限示范下学会新任务在新的平台和场景中仍然具有挑战性，尤其是在机器人和人类的视频应用中存在表现、相机和环境的差异。

Method: 提出了一种统一的符号表示方法和名为TraceGen的世界模型，训练数据处理管道TraceForge，将人类和机器人视频转化为一致的3D轨迹，从而实现跨表现、跨环境和跨任务的视频学习。

Result: TraceGen在123K视频和1.8M观察轨迹-语言三元组的预训练后，对目标机器人仅需五个视频即可在四个任务上达到80%的成功率，推理速度比现有视频模型快50-600倍，并且在仅有五个非校准的手机拍摄人类演示视频的情况下仍能达到67.5%的成功率。

Conclusion: TraceGen有效解决了小数据问题，展示了在多种条件下的适应能力，无需依赖对象检测或重的像素空间生成。

Abstract: Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging. While videos of other embodiments - humans and different robots - are abundant, differences in embodiment, camera, and environment hinder their direct use. We address the small-data problem by introducing a unifying, symbolic representation - a compact 3D "trace-space" of scene-level trajectories - that enables learning from cross-embodiment, cross-environment, and cross-task videos. We present TraceGen, a world model that predicts future motion in trace-space rather than pixel space, abstracting away appearance while retaining the geometric structure needed for manipulation. To train TraceGen at scale, we develop TraceForge, a data pipeline that transforms heterogeneous human and robot videos into consistent 3D traces, yielding a corpus of 123K videos and 1.8M observation-trace-language triplets. Pretraining on this corpus produces a transferable 3D motion prior that adapts efficiently: with just five target robot videos, TraceGen attains 80% success across four tasks while offering 50-600x faster inference than state-of-the-art video-based world models. In the more challenging case where only five uncalibrated human demonstration videos captured on a handheld phone are available, it still reaches 67.5% success on a real robot, highlighting TraceGen's ability to adapt across embodiments without relying on object detectors or heavy pixel-space generation.

</details>
