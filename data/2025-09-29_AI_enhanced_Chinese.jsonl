{"id": "2509.21436", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2509.21436", "abs": "https://arxiv.org/abs/2509.21436", "authors": ["Shutong Fan", "Lan Zhang", "Xiaoyong Yuan"], "title": "Position: Human Factors Reshape Adversarial Analysis in Human-AI Decision-Making Systems", "comment": null, "summary": "As Artificial Intelligence (AI) increasingly supports human decision-making,\nits vulnerability to adversarial attacks grows. However, the existing\nadversarial analysis predominantly focuses on fully autonomous AI systems,\nwhere decisions are executed without human intervention. This narrow focus\noverlooks the complexities of human-AI collaboration, where humans interpret,\nadjust, and act upon AI-generated decisions. Trust, expectations, and cognitive\nbehaviors influence how humans interact with AI, creating dynamic feedback\nloops that adversaries can exploit. To strengthen the robustness of AI-assisted\ndecision-making, adversarial analysis must account for the interplay between\nhuman factors and attack strategies.\n  This position paper argues that human factors fundamentally reshape\nadversarial analysis and must be incorporated into evaluating robustness in\nhuman-AI decision-making systems. To fully explore human factors in adversarial\nanalysis, we begin by investigating the role of human factors in human-AI\ncollaboration through a comprehensive review. We then introduce a novel\nrobustness analysis framework that (1) examines how human factors affect\ncollaborative decision-making performance, (2) revisits and interprets existing\nadversarial attack strategies in the context of human-AI interaction, and (3)\nintroduces a new timing-based adversarial attack as a case study, illustrating\nvulnerabilities emerging from sequential human actions. The experimental\nresults reveal that attack timing uniquely impacts decision outcomes in\nhuman-AI collaboration. We hope this analysis inspires future research on\nadversarial robustness in human-AI systems, fostering interdisciplinary\napproaches that integrate AI security, human cognition, and decision-making\ndynamics.", "AI": {"tldr": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u5728\u51b3\u7b56\u4e2d\u652f\u6301\u4eba\u7c7b\uff0c\u5176\u6613\u53d7\u653b\u51fb\u6027\u589e\u52a0\uff0c\u5fc5\u987b\u8003\u8651\u4eba\u7c7b\u56e0\u7d20\u4e0e\u5bf9\u6297\u7b56\u7565\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u4ee5\u589e\u5f3aAI\u8f85\u52a9\u51b3\u7b56\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u4eba\u5de5\u667a\u80fd\u8d8a\u6765\u8d8a\u591a\u5730\u652f\u6491\u4eba\u7c7b\u51b3\u7b56\u65f6\uff0c\u5bf9\u5176\u6613\u53d7\u5bf9\u6297\u653b\u51fb\u7684\u8106\u5f31\u6027\u4e5f\u5728\u589e\u957f\uff0c\u56e0\u6b64\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\u4eba\u7c7b\u548cAI\u7684\u534f\u4f5c\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u7efc\u5408\u56de\u987e\u4eba\u7c7b\u56e0\u7d20\u5728\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u4f5c\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9c81\u68d2\u6027\u5206\u6790\u6846\u67b6\u6765\u5206\u6790\u4eba\u7c7b\u56e0\u7d20\u5bf9\u51b3\u7b56\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u5f15\u5165\u65f6\u5e8f\u5bf9\u6297\u653b\u51fb\u4f5c\u4e3a\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u653b\u51fb\u65f6\u673a\u72ec\u7279\u5730\u5f71\u54cd\u4e86\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u51b3\u7b56\u7ed3\u679c\u3002", "conclusion": "\u4eba\u7c7b\u56e0\u7d20\u5bf9\u4eba\u673a\u51b3\u7b56\u7cfb\u7edf\u7684\u5bf9\u6297\u6027\u5206\u6790\u81f3\u5173\u91cd\u8981\uff0c\u5fc5\u987b\u7eb3\u5165\u8bc4\u4f30\u7684\u8003\u91cf\u4e2d\u3002"}}
{"id": "2509.21501", "categories": ["cs.HC", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21501", "abs": "https://arxiv.org/abs/2509.21501", "authors": ["Lu Sun", "Shihan Fu", "Bingsheng Yao", "Yuxuan Lu", "Wenbo Li", "Hansu Gu", "Jiri Gesi", "Jing Huang", "Chen Luo", "Dakuo Wang"], "title": "LLM Agent Meets Agentic AI: Can LLM Agents Simulate Customers to Evaluate Agentic-AI-based Shopping Assistants?", "comment": null, "summary": "Agentic AI is emerging, capable of executing tasks through natural language,\nsuch as Copilot for coding or Amazon Rufus for shopping. Evaluating these\nsystems is challenging, as their rapid evolution outpaces traditional human\nevaluation. Researchers have proposed LLM Agents to simulate participants as\ndigital twins, but it remains unclear to what extent a digital twin can\nrepresent a specific customer in multi-turn interaction with an agentic AI\nsystem. In this paper, we recruited 40 human participants to shop with Amazon\nRufus, collected their personas, interaction traces, and UX feedback, and then\ncreated digital twins to repeat the task. Pairwise comparison of human and\ndigital-twin traces shows that while agents often explored more diverse\nchoices, their action patterns aligned with humans and yielded similar design\nfeedback. This study is the first to quantify how closely LLM agents can mirror\nhuman multi-turn interaction with an agentic AI system, highlighting their\npotential for scalable evaluation.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86LLM\u4ee3\u7406\u5728\u6a21\u62df\u4eba\u7c7b\u591a\u8f6e\u4e92\u52a8\u4e2d\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u53ef\u7528\u4e8e\u53ef\u6269\u5c55\u7684AI\u7cfb\u7edf\u8bc4\u4f30\u3002", "motivation": "\u968f\u7740Agentic AI\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u4f20\u7edf\u7684\u4eba\u7c7b\u8bc4\u4f30\u65b9\u6cd5\u96be\u4ee5\u8ddf\u4e0a\uff0c\u56e0\u6b64\u9700\u8981\u627e\u5230\u65b0\u7684\u65b9\u6cd5\u6765\u8bc4\u4ef7\u8fd9\u4e9b\u7cfb\u7edf\u4e0e\u7528\u6237\u7684\u4e92\u52a8\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u62db\u52df40\u540d\u53c2\u4e0e\u8005\u4e0eAmazon Rufus\u8fdb\u884c\u8d2d\u7269\uff0c\u6536\u96c6\u4ed6\u4eec\u7684\u4eba\u683c\u7279\u5f81\u3001\u4ea4\u4e92\u6570\u636e\u548c\u7528\u6237\u4f53\u9a8c\u53cd\u9988\uff0c\u5e76\u521b\u5efa\u6570\u5b57\u53cc\u80de\u80ce\u4ee5\u91cd\u590d\u4efb\u52a1\u3002", "result": "\u914d\u5bf9\u6bd4\u8f83\u663e\u793a\uff0c\u5c3d\u7ba1\u6570\u5b57\u53cc\u80de\u80ce\u63a2\u7d22\u4e86\u66f4\u591a\u7684\u9009\u62e9\uff0c\u4f46\u5176\u884c\u4e3a\u6a21\u5f0f\u4e0e\u4eba\u7c7b\u76f8\u4f3c\uff0c\u5e76\u63d0\u4f9b\u4e86\u76f8\u4f3c\u7684\u8bbe\u8ba1\u53cd\u9988\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cLLM\u4ee3\u7406\u80fd\u591f\u6709\u6548\u5730\u6a21\u62df\u4eba\u7c7b\u5728\u4e0e\u4ee3\u7406AI\u7cfb\u7edf\u8fdb\u884c\u591a\u8f6e\u4ea4\u4e92\u65f6\u7684\u884c\u4e3a\uff0c\u4e3a\u53ef\u6269\u5c55\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6f5c\u5728\u53ef\u80fd\u3002"}}
{"id": "2509.21542", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21542", "abs": "https://arxiv.org/abs/2509.21542", "authors": ["Jianan Zhou", "Fleur Corbett", "Joori Byun", "Talya Porat", "Nejra van Zalk"], "title": "Psychological and behavioural responses in human-agent vs. human-human interactions: a systematic review and meta-analysis", "comment": null, "summary": "Interactive intelligent agents are being integrated across society. Despite\nachieving human-like capabilities, humans' responses to these agents remain\npoorly understood, with research fragmented across disciplines. We conducted a\nfirst systematic synthesis comparing a range of psychological and behavioural\nresponses in matched human-agent vs. human-human dyadic interactions. A total\nof 162 eligible studies (146 contributed to the meta-analysis; 468 effect\nsizes) were included in the systematic review and meta-analysis, which\nintegrated frequentist and Bayesian approaches. Our results indicate that\nindividuals exhibited less prosocial behaviour and moral engagement when\ninteracting with agents vs. humans. They attributed less agency and\nresponsibility to agents, perceiving them as less competent, likeable, and\nsocially present. In contrast, individuals' social alignment (i.e., alignment\nor adaptation of internal states and behaviours with partners), trust in\npartners, personal agency, task performance, and interaction experiences were\ngenerally comparable when interacting with agents vs. humans. We observed high\neffect-size heterogeneity for many subjective responses (i.e., social\nperceptions of partners, subjective trust, and interaction experiences),\nsuggesting context-dependency of partner effects. By examining the\ncharacteristics of studies, participants, partners, interaction scenarios, and\nresponse measures, we also identified several moderators shaping partner\neffects. Overall, functional behaviours and interactive experiences with agents\ncan resemble those with humans, whereas fundamental social attributions and\nmoral/prosocial concerns lag in human-agent interactions. Agents are thus\nafforded instrumental value on par with humans but lack comparable intrinsic\nvalue, providing practical implications for agent design and regulation.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u6bd4\u8f83\u4e86\u4eba\u7c7b\u4e0e\u667a\u80fd\u4f53\u53ca\u4eba\u7c7b\u4e4b\u95f4\u7684\u4e92\u52a8\uff0c\u53d1\u73b0\u4e0e\u667a\u80fd\u4f53\u4e92\u52a8\u65f6\uff0c\u4e2a\u4f53\u7684\u5229\u4ed6\u884c\u4e3a\u548c\u9053\u5fb7\u53c2\u4e0e\u8f83\u5c11\uff0c\u667a\u80fd\u4f53\u5728\u793e\u4f1a\u8ba4\u77e5\u4e0a\u8f83\u4eba\u7c7b\u88ab\u4f4e\u4f30\u3002\u5c3d\u7ba1\u5728\u67d0\u4e9b\u4e92\u52a8\u4e0a\u8868\u73b0\u76f8\u4f3c\uff0c\u4f46\u5728\u57fa\u672c\u7684\u793e\u4f1a\u5f52\u56e0\u548c\u9053\u5fb7\u5173\u5207\u4e0a\u5b58\u5728\u5dee\u8ddd\u3002", "motivation": "\u65e8\u5728\u7cfb\u7edf\u5730\u6bd4\u8f83\u4eba\u7c7b\u4e0e\u667a\u80fd\u4f53\u548c\u4eba\u7c7b\u4e4b\u95f4\u7684\u5fc3\u7406\u548c\u884c\u4e3a\u53cd\u5e94\uff0c\u4ee5\u586b\u8865\u8de8\u5b66\u79d1\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u7cfb\u7edf\u6027\u7efc\u8ff0\u548c\u5143\u5206\u6790\uff0c\u5305\u542b162\u4e2a\u7814\u7a76\u548c468\u4e2a\u6548\u5e94\u91cf\uff0c\u91c7\u7528\u9891\u7387\u548c\u8d1d\u53f6\u65af\u4e24\u79cd\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4e0e\u4eba\u7c7b\u4e92\u52a8\u65f6\uff0c\u4e2a\u4f53\u8868\u73b0\u51fa\u66f4\u5c11\u7684\u5229\u4ed6\u884c\u4e3a\u548c\u9053\u5fb7\u53c2\u4e0e\uff0c\u4e14\u5bf9\u667a\u80fd\u4f53\u7684\u8ba4\u77e5\uff08\u5982\u80fd\u529b\u3001\u53ef\u53d6\u6027\u548c\u793e\u4f1a\u5b58\u5728\u611f\uff09\u8f83\u4f4e\u3002\u793e\u4ea4\u5bf9\u9f50\u3001\u5bf9\u4f19\u4f34\u7684\u4fe1\u4efb\u7b49\u4e0e\u4eba\u7c7b\u4e92\u52a8\u65f6\u901a\u5e38\u76f8\u4f3c\uff0c\u4f46\u4e3b\u89c2\u53cd\u5e94\u7684\u6548\u5e94\u5927\u5c0f\u5f02\u8d28\u6027\u9ad8\uff0c\u8868\u660e\u5408\u4f5c\u4f19\u4f34\u6548\u679c\u53d7\u4e0a\u4e0b\u6587\u5f71\u54cd\u3002", "conclusion": "\u867d\u7136\u4e0e\u4eba\u7c7b\u4e4b\u95f4\u7684\u4e92\u52a8\u5728\u67d0\u4e9b\u529f\u80fd\u6027\u884c\u4e3a\u548c\u4f53\u9a8c\u4e0a\u7c7b\u4f3c\uff0c\u4f46\u4e0e\u667a\u80fd\u4f53\u7684\u4e92\u52a8\u5728\u57fa\u672c\u793e\u4f1a\u5f52\u56e0\u548c\u9053\u5fb7/\u5229\u4ed6\u5173\u5207\u65b9\u9762\u4ecd\u663e\u4e0d\u8db3\u3002\u56e0\u6b64\uff0c\u667a\u80fd\u4f53\u5728\u8bbe\u8ba1\u548c\u76d1\u7ba1\u65b9\u9762\u5e94\u88ab\u8d4b\u4e88\u4e0e\u4eba\u7c7b\u76f8\u5f53\u7684\u5de5\u5177\u4ef7\u503c\uff0c\u4f46\u7f3a\u4e4f\u76f8\u5e94\u7684\u5185\u5728\u4ef7\u503c\u3002"}}
{"id": "2509.21589", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2509.21589", "abs": "https://arxiv.org/abs/2509.21589", "authors": ["Nana Wang", "Gen Li", "Zhaoxin Fan", "Suli Wang"], "title": "EMG-UP: Unsupervised Personalization in Cross-User EMG Gesture Recognition", "comment": null, "summary": "Cross-user electromyography (EMG)-based gesture recognition represents a\nfundamental challenge in achieving scalable and personalized human-machine\ninteraction within real-world applications. Despite extensive efforts, existing\nmethodologies struggle to generalize effectively across users due to the\nintrinsic biological variability of EMG signals, resulting from anatomical\nheterogeneity and diverse task execution styles. To address this limitation, we\nintroduce EMG-UP, a novel and effective framework for Unsupervised\nPersonalization in cross-user gesture recognition. The proposed framework\nleverages a two-stage adaptation strategy: (1) Sequence-Cross Perspective\nContrastive Learning, designed to disentangle robust and user-specific feature\nrepresentations by capturing intrinsic signal patterns invariant to inter-user\nvariability, and (2) Pseudo-Label-Guided Fine-Tuning, which enables model\nrefinement for individual users without necessitating access to source domain\ndata. Extensive evaluations show that EMG-UP achieves state-of-the-art\nperformance, outperforming prior methods by at least 2.0% in accuracy.", "AI": {"tldr": "EMG-UP\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65e0\u76d1\u7763\u4e2a\u6027\u5316\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u8de8\u7528\u6237\u624b\u52bf\u8bc6\u522b\u4e2d\u7684\u751f\u7269\u4fe1\u53f7\u53d8\u5f02\u6027\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u8de8\u7528\u6237EMG\u4fe1\u53f7\u8bc6\u522b\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u9002\u5e94\u7b56\u7565\uff0c\u5305\u62ec\u5e8f\u5217\u4ea4\u53c9\u89c6\u89d2\u5bf9\u6bd4\u5b66\u4e60\u548c\u4f2a\u6807\u7b7e\u6307\u5bfc\u5fae\u8c03\u3002", "result": "EMG-UP\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u4e4b\u524d\u7684\u65b9\u6cd5\u3002", "conclusion": "EMG-UP\u5728\u8de8\u7528\u6237\u624b\u52bf\u8bc6\u522b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7cbe\u5ea6\u63d0\u9ad8\u81f3\u5c112.0%\u3002"}}
{"id": "2509.21370", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.21370", "abs": "https://arxiv.org/abs/2509.21370", "authors": ["Yashom Dighe", "Yash Turkar", "Karthik Dantu"], "title": "Language-in-the-Loop Culvert Inspection on the Erie Canal", "comment": "First two authors contributed equally", "summary": "Culverts on canals such as the Erie Canal, built originally in 1825, require\nfrequent inspections to ensure safe operation. Human inspection of culverts is\nchallenging due to age, geometry, poor illumination, weather, and lack of easy\naccess. We introduce VISION, an end-to-end, language-in-the-loop autonomy\nsystem that couples a web-scale vision-language model (VLM) with constrained\nviewpoint planning for autonomous inspection of culverts. Brief prompts to the\nVLM solicit open-vocabulary ROI proposals with rationales and confidences,\nstereo depth is fused to recover scale, and a planner -- aware of culvert\nconstraints -- commands repositioning moves to capture targeted close-ups.\nDeployed on a quadruped in a culvert under the Erie Canal, VISION closes the\nsee, decide, move, re-image loop on-board and produces high-resolution images\nfor detailed reporting without domain-specific fine-tuning. In an external\nevaluation by New York Canal Corporation personnel, initial ROI proposals\nachieved 61.4\\% agreement with subject-matter experts, and final\npost-re-imaging assessments reached 80\\%, indicating that VISION converts\ntentative hypotheses into grounded, expert-aligned findings.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86VISION\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u68c0\u6d4b\u6e20\u7ba1\uff0c\u63d0\u9ad8\u4e86\u68c0\u6d4b\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u8001\u65e7\u6e20\u7ba1\u7684\u4eba\u5de5\u68c0\u67e5\u9762\u4e34\u8bb8\u591a\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u81ea\u4e3b\u68c0\u67e5\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u4e86\u7f51\u7edc\u89c4\u6a21\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u548c\u53d7\u9650\u89c6\u89d2\u89c4\u5212\u7684\u5168\u81ea\u52a8\u68c0\u6d4b\u7cfb\u7edf\u3002", "result": "\u521d\u6b65\u533a\u57df\u5174\u8da3\u63d0\u6848\u4e0e\u4e13\u5bb6\u8fbe\u621061.4%\u7684\u4e00\u81f4\u7387\uff0c\u6700\u7ec8\u8bc4\u4f30\u8fbe\u5230\u4e8680%\u3002", "conclusion": "VISION\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u5730\u8fdb\u884c\u6e20\u7ba1\u7684\u81ea\u4e3b\u68c0\u6d4b\uff0c\u5e76\u4e14\u5728\u4e0e\u4e13\u5bb6\u7684\u8bc4\u4f30\u4e2d\u663e\u793a\u51fa\u826f\u597d\u7684\u4e00\u81f4\u6027\u3002"}}
{"id": "2509.21665", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2509.21665", "abs": "https://arxiv.org/abs/2509.21665", "authors": ["Lihua Du", "Xing Lyu", "Lezi Xie", "Bo Feng"], "title": "Alignment Without Understanding: A Message- and Conversation-Centered Approach to Understanding AI Sycophancy", "comment": null, "summary": "AI sycophancy is increasingly recognized as a harmful alignment, but research\nremains fragmented and underdeveloped at the conceptual level. This article\nredefines AI sycophancy as the tendency of large language models (LLMs) and\nother interactive AI systems to excessively and/or uncritically validate,\namplify, or align with a user's assertions-whether these concern factual\ninformation, cognitive evaluations, or affective states. Within this framework,\nwe distinguish three types of sycophancy: informational, cognitive, and\naffective. We also introduce personalization at the message level and critical\nprompting at the conversation level as key dimensions for distinguishing and\nexamining different manifestations of AI sycophancy. Finally, we propose the AI\nSycophancy Processing Model (AISPM) to examine the antecedents, outcomes, and\npsychological mechanisms through which sycophantic AI responses shape user\nexperiences. By embedding AI sycophancy in the broader landscape of\ncommunication theory and research, this article seeks to unify perspectives,\nclarify conceptual boundaries, and provide a foundation for systematic,\ntheory-driven investigations.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86AI sycophancy\u7684\u6982\u5ff5\uff0c\u5b9a\u4e49\u4e86\u5176\u7c7b\u578b\uff0c\u5e76\u63d0\u51fa\u5904\u7406\u6a21\u578bAISPM\uff0c\u4ee5\u4fc3\u8fdb\u5bf9\u5176\u5f71\u54cd\u548c\u673a\u5236\u7684\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "motivation": "\u9274\u4e8eAI sycophancy\u88ab\u666e\u904d\u8ba4\u53ef\u4e3a\u4e00\u79cd\u6709\u5bb3\u7684\u5bf9\u9f50\u73b0\u8c61\uff0c\u6587\u7ae0\u5e0c\u671b\u901a\u8fc7\u91cd\u65b0\u5b9a\u4e49\u548c\u6846\u67b6\u5316\u8fd9\u4e00\u6982\u5ff5\uff0c\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u548c\u6982\u5ff5\u6846\u67b6\u91cd\u6784\uff0c\u660e\u786e\u4e0d\u540c\u7c7b\u578b\u7684AI sycophancy\u53ca\u5176\u5f71\u54cd\u673a\u5236\u3002", "result": "\u6587\u7ae0\u6210\u529f\u5b9a\u4e49\u4e86AI sycophancy\uff0c\u533a\u5206\u4e86\u4fe1\u606f\u3001\u8ba4\u77e5\u548c\u60c5\u611f\u4e09\u79cd\u7c7b\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u591a\u4e2a\u5173\u952e\u7ef4\u5ea6\u53ca\u5904\u7406\u6a21\u578b\uff0c\u4ee5\u4e30\u5bcc\u5bf9\u8be5\u73b0\u8c61\u7684\u7406\u89e3\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86AI sycophancy\u5904\u7406\u6a21\u578b\uff08AISPM\uff09\uff0c\u4e3a\u7406\u89e3\u7528\u6237\u4f53\u9a8c\u4e2dAI\u7684\u8c04\u5a9a\u53cd\u5e94\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u4fc3\u8fdb\u4e86\u76f8\u5173\u7814\u7a76\u7684\u7edf\u4e00\u548c\u6df1\u5165\u3002"}}
{"id": "2509.21445", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.21445", "abs": "https://arxiv.org/abs/2509.21445", "authors": ["Archie Webster", "Lee Skull", "Seyed Amir Tafrishi"], "title": "Developing a Mono-Actuated Compliant GeoGami Robot", "comment": "8 pages, 12 figures, under-review", "summary": "This paper presents the design of a new soft-rigid robotic platform,\n\"GeoGami\". We leverage origami surface capabilities to achieve shape\ncontraction and to support locomotion with underactuated forms. A key challenge\nis that origami surfaces have high degrees of freedom and typically require\nmany actuators; we address repeatability by integrating surface compliance. We\npropose a mono-actuated GeoGami mobile platform that combines origami surface\ncompliance with a geometric compliant skeleton, enabling the robot to transform\nand locomote using a single actuator. We demonstrate the robot, develop a\nstiffness model, and describe the central gearbox mechanism. We also analyze\nalternative cable-driven actuation methods for the skeleton to enable surface\ntransformation. Finally, we evaluate the GeoGami platform for capabilities,\nincluding shape transformation and rolling. This platform opens new\ncapabilities for robots that change shape to access different environments and\nthat use shape transformation for locomotion.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u578b\u67d4\u6027-\u521a\u6027\u673a\u5668\u4eba\u5e73\u53f0GeoGami\uff0c\u5229\u7528\u6298\u7eb8\u7ed3\u6784\u5b9e\u73b0\u5f62\u72b6\u53d8\u6362\u4e0e\u8fd0\u52a8\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5355\u4e00\u9a71\u52a8\u5668\u63a7\u5236\u7684\u521b\u65b0\u65b9\u6cd5\u3002", "motivation": "\u5229\u7528\u6298\u7eb8\u7ed3\u6784\u7684\u7279\u6027\uff0c\u5b9e\u73b0\u5f62\u72b6\u6536\u7f29\u5e76\u652f\u6301\u4ee5\u6b20\u9a71\u52a8\u5f62\u5f0f\u7684 locomotion\uff0c\u4f46\u6298\u7eb8\u8868\u9762\u590d\u6742\u5ea6\u9ad8\uff0c\u9700\u4f17\u591a\u9a71\u52a8\u5668\uff0c\u56e0\u6b64\u63d0\u51fa\u96c6\u6210\u8868\u9762\u987a\u5e94\u6027\u4ee5\u63d0\u9ad8\u53ef\u91cd\u590d\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5355\u7535\u52a8\u5143\u4ef6\u9a71\u52a8\u7684GeoGami\u79fb\u52a8\u5e73\u53f0\uff0c\u7ed3\u5408\u4e86\u6298\u7eb8\u8868\u9762\u987a\u5e94\u6027\u4e0e\u51e0\u4f55\u987a\u5e94\u9aa8\u67b6\uff0c\u5141\u8bb8\u673a\u5668\u4eba\u901a\u8fc7\u5355\u4e00\u9a71\u52a8\u5668\u5b9e\u73b0\u53d8\u5f62\u548c\u8fd0\u52a8\u3002", "result": "\u5f00\u53d1\u4e86GeoGami\u673a\u5668\u4eba\uff0c\u5efa\u7acb\u4e86\u521a\u5ea6\u6a21\u578b\uff0c\u5e76\u63cf\u8ff0\u4e86\u4e2d\u592e\u53d8\u901f\u7bb1\u673a\u5236\u3002\u540c\u65f6\u5206\u6790\u4e86\u9ab7\u9ac5\u9aa8\u67b6\u7684\u66ff\u4ee3\u7535\u7f06\u9a71\u52a8\u65b9\u6cd5\uff0c\u8bc4\u4f30\u4e86\u5e73\u53f0\u7684\u5f62\u72b6\u53d8\u6362\u80fd\u529b\u548c\u6eda\u52a8\u80fd\u529b\u3002", "conclusion": "GeoGami\u5e73\u53f0\u5728\u5b9e\u73b0\u5f62\u72b6\u53d8\u6362\u548c\u6eda\u52a8\u65b9\u9762\u5c55\u73b0\u4e86\u65b0\u7684\u80fd\u529b\uff0c\u4e3a\u80fd\u591f\u6539\u53d8\u5f62\u72b6\u4ee5\u8fdb\u5165\u4e0d\u540c\u73af\u5883\u7684\u673a\u5668\u4eba\u5f00\u8f9f\u4e86\u65b0\u673a\u9047\u3002"}}
{"id": "2509.21685", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2509.21685", "abs": "https://arxiv.org/abs/2509.21685", "authors": ["Yaqing Yang", "Vikram Mohanty", "Yan-Ying Chen", "Matthew K. Hong", "Nikolas Martelaro", "Aniket Kittur"], "title": "FlexMind: Supporting Deeper Creative Thinking with LLMs", "comment": null, "summary": "Effective ideation requires both broad exploration of diverse ideas and deep\nevaluation of their potential. Generative AI can support such processes, but\ncurrent tools typically emphasize either generating many ideas or supporting\nin-depth consideration of a few, lacking support for both. Research also\nhighlights risks of over-reliance on LLMs, including shallow exploration and\nnegative creative outcomes. We present FlexMind, an AI-augmented system that\nscaffolds iterative exploration of ideas, tradeoffs, and mitigations. FlexMind\nexposes users to a broad set of ideas while enabling a lightweight transition\ninto deeper engagement. In a study comparing ideation with FlexMind to ChatGPT,\nparticipants generated higher-quality ideas with FlexMind, due to both broader\nexposure and deeper engagement with tradeoffs. By scaffolding ideation across\nbreadth, depth, and reflective evaluation, FlexMind empowers users to surface\nideas that might otherwise go unnoticed or be prematurely discarded.", "AI": {"tldr": "FlexMind\u662f\u4e00\u4e2a\u7ed3\u5408\u5e7f\u6cdb\u63a2\u7d22\u548c\u6df1\u5165\u8bc4\u4f30\u7684AI\u589e\u5f3a\u7cfb\u7edf\uff0c\u80fd\u591f\u63d0\u9ad8\u521b\u610f\u751f\u6210\u7684\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u7684\u751f\u6210\u5f0fAI\u5de5\u5177\u5f80\u5f80\u4ec5\u6ce8\u91cd\u751f\u6210\u5927\u91cf\u60f3\u6cd5\u6216\u652f\u6301\u5c11\u91cf\u60f3\u6cd5\u7684\u6df1\u5165\u63a2\u8ba8\uff0c\u7f3a\u4e4f\u4e24\u8005\u7684\u652f\u6301\uff0c\u5bb9\u6613\u5bfc\u81f4\u8868\u9762\u5316\u7684\u63a2\u7d22\u548c\u6d88\u6781\u7684\u521b\u9020\u6210\u679c\u3002", "method": "\u901a\u8fc7\u4e0eChatGPT\u8fdb\u884c\u6bd4\u8f83\u7684\u7814\u7a76\uff0c\u8bc4\u4f30FlexMind\u5728\u521b\u610f\u751f\u6210\u4e2d\u7684\u6709\u6548\u6027\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528FlexMind\u7684\u53c2\u4e0e\u8005\u751f\u6210\u7684\u521b\u610f\u8d28\u91cf\u66f4\u9ad8\uff0c\u8fd9\u5f52\u56e0\u4e8e\u66f4\u5e7f\u6cdb\u7684\u60f3\u6cd5\u66dd\u5149\u548c\u5bf9\u6743\u8861\u7684\u66f4\u6df1\u5165\u53c2\u4e0e\u3002", "conclusion": "FlexMind\u901a\u8fc7\u652f\u6301\u5e7f\u6cdb\u7684\u60f3\u6cd5\u63a2\u7d22\u548c\u6df1\u5165\u7684\u8bc4\u4f30\uff0c\u5e2e\u52a9\u7528\u6237\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u521b\u610f\uff0c\u514b\u670d\u4e86\u73b0\u6709\u751f\u6210\u5f0fAI\u5de5\u5177\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.21496", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.21496", "abs": "https://arxiv.org/abs/2509.21496", "authors": ["Peiwen Yang", "Weisong Wen", "Runqiu Yang", "Yingming Chen", "Cheuk Chi Tsang"], "title": "Wall Inspector: Quadrotor Control in Wall-proximity Through Model Compensation", "comment": null, "summary": "The safe operation of quadrotors in near-wall urban or indoor environments\n(e.g., inspection and search-and-rescue missions) is challenged by unmodeled\naerodynamic effects arising from wall-proximity. It generates complex vortices\nthat induce destabilizing suction forces, potentially leading to hazardous\nvibrations or collisions. This paper presents a comprehensive solution\nfeaturing (1) a physics-based suction force model that explicitly characterizes\nthe dependency on both rotor speed and wall distance, and (2) a\nsuction-compensated model predictive control (SC-MPC) framework designed to\nensure accurate and stable trajectory tracking during wall-proximity\noperations. The proposed SC-MPC framework incorporates an enhanced dynamics\nmodel that accounts for suction force effects, formulated as a factor graph\noptimization problem integrating system dynamics constraints, trajectory\ntracking objectives, control input smoothness requirements, and actuator\nphysical limitations. The suction force model parameters are systematically\nidentified through extensive experimental measurements across varying\noperational conditions. Experimental validation demonstrates SC-MPC's superior\nperformance, achieving 2.1 cm root mean squared error (RMSE) in X-axis and 2.0\ncm RMSE in Y-axis position control - representing 74% and 79% improvements over\ncascaded proportional-integral-derivative (PID) control, and 60% and 53%\nimprovements over standard MPC respectively. The corresponding mean absolute\nerror (MAE) metrics (1.2 cm X-axis, 1.4 cm Y-axis) similarly outperform both\nbaselines. The evaluation platform employs a ducted quadrotor design that\nprovides collision protection while maintaining aerodynamic efficiency. To\nfacilitate reproducibility and community adoption, we have open-sourced our\ncomplete implementation, available at\nhttps://anonymous.4open.science/r/SC-MPC-6A61.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5438\u529b\u8865\u507f\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6(SC-MPC)\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u56db\u65cb\u7ffc\u5728\u8fd1\u5899\u73af\u5883\u4e2d\u7684\u8f68\u8ff9\u63a7\u5236\u7cbe\u5ea6\uff0c\u5e76\u5f00\u6e90\u4e86\u5b8c\u6574\u5b9e\u73b0\u3002", "motivation": "\u4f20\u7edf\u63a7\u5236\u65b9\u6cd5\u5728\u56db\u65cb\u7ffc\u5728\u8fd1\u5899\u98de\u884c\u65f6\u53d7\u5230\u672a\u5efa\u6a21\u6c14\u52a8\u6548\u5e94\u7684\u5f71\u54cd\uff0c\u5bb9\u6613\u4ea7\u751f\u4e0d\u7a33\u5b9a\u7684\u632f\u52a8\u548c\u78b0\u649e\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u63a7\u5236\u6846\u67b6\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5438\u529b\u8865\u507f\u6a21\u578b\u9884\u6d4b\u63a7\u5236(SC-MPC)\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u8003\u8651\u5438\u529b\u6548\u5e94\u7684\u589e\u5f3a\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u7cfb\u7edf\u52a8\u6001\u7ea6\u675f\u3001\u8f68\u8ff9\u8ddf\u8e2a\u76ee\u6807\u3001\u63a7\u5236\u8f93\u5165\u5e73\u6ed1\u6027\u548c\u6267\u884c\u5668\u7269\u7406\u9650\u5236\u7b49\u95ee\u9898\u3002", "result": "\u4f7f\u7528SC-MPC\u6846\u67b6\uff0c\u56db\u65cb\u7ffc\u5728\u5b9e\u9a8c\u4e2d\u5728X\u8f74\u548cY\u8f74\u4f4d\u7f6e\u63a7\u5236\u4e0a\u5206\u522b\u5b9e\u73b0\u4e862.1 cm\u548c2.0 cm\u7684\u5747\u65b9\u6839\u8bef\u5dee(RMSE)\uff0c\u6bd4\u7ea7\u8054\u6bd4\u4f8b-\u79ef\u5206-\u5fae\u5206(PID)\u63a7\u5236\u65b9\u6cd5\u5206\u522b\u63d0\u9ad8\u4e8674%\u548c79%\uff0c\u6bd4\u6807\u51c6\u6a21\u578b\u9884\u6d4b\u63a7\u5236(MPC)\u63d0\u9ad8\u4e8660%\u548c53%\u3002", "conclusion": "\u901a\u8fc7\u5f00\u53d1\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7684\u5438\u529b\u6a21\u578b\u548c\u5438\u529b\u8865\u507f\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6(SC-MPC)\uff0c\u5b9e\u73b0\u4e86\u56db\u65cb\u7ffc\u5728\u8fd1\u5899\u64cd\u4f5c\u4e2d\u7684\u7cbe\u786e\u7a33\u5b9a\u8f68\u8ff9\u8ddf\u8e2a\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u660e\u663e\u4f18\u4e8e\u5176\u4ed6\u63a7\u5236\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2509.21721", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2509.21721", "abs": "https://arxiv.org/abs/2509.21721", "authors": ["Ruishan Wu", "Zhuoyang Li", "Charles Perin", "Sheelagh Carpendale", "Can Liu"], "title": "Design Exploration of AI-assisted Personal Affective Physicalization", "comment": null, "summary": "Personal Affective Physicalization is the process by which individuals\nexpress emotions through tangible forms to record, reflect on, and communicate.\nYet such physical data representations can be challenging to design due to the\nabstract nature of emotions. Given the shown potential of AI in detecting\nemotion and assisting design, we explore opportunities in AI-assisted design of\npersonal affective physicalization using a Research-through-Design method. We\ndeveloped PhEmotion, a tool for embedding LLM-extracted emotion values from\nhuman-AI conversations into parametric design of physical artifacts. A lab\nstudy was conducted with 14 participants creating these artifacts based on\ntheir personal emotions, with and without AI support. We observed nuances and\nvariations in participants' creative strategies, meaning-making processes and\ntheir perceptions of AI support in this context. We found key tensions in\nAI-human co-creation that provide a nuanced agenda for future research in\nAI-assisted personal affective physicalization.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86AI\u8f85\u52a9\u4e2a\u4eba\u60c5\u611f\u7269\u5316\u8bbe\u8ba1\u7684\u673a\u4f1a\uff0c\u5f00\u53d1\u4e86PhEmotion\u5de5\u5177\uff0c\u5e76\u8fdb\u884c\u5b9e\u9a8c\u4ee5\u7814\u7a76\u4eba\u7c7b\u4e0eAI\u5171\u540c\u521b\u9020\u7684\u590d\u6742\u6027\u53ca\u5176\u5bf9\u60c5\u611f\u8868\u8fbe\u7684\u5f71\u54cd\u3002", "motivation": "\u4e2a\u4eba\u60c5\u611f\u7269\u5316\u662f\u4eba\u4eec\u901a\u8fc7\u5177\u4f53\u5f62\u5f0f\u6765\u8868\u8fbe\u3001\u8bb0\u5f55\u548c\u6c9f\u901a\u60c5\u611f\u7684\u8fc7\u7a0b\uff0c\u4f46\u7531\u4e8e\u60c5\u611f\u7684\u62bd\u8c61\u6027\uff0c\u8bbe\u8ba1\u8fd9\u6837\u7684\u7269\u7406\u6570\u636e\u8868\u793a\u5b58\u5728\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e86\u901a\u8fc7\u8bbe\u8ba1\u7814\u7a76\u7684\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86PhEmotion\u5de5\u5177\uff0c\u4ee5\u5e2e\u52a9\u5c06\u4eba\u7c7b\u4e0eAI\u5bf9\u8bdd\u4e2d\u63d0\u53d6\u7684\u60c5\u611f\u503c\u5d4c\u5165\u7269\u7406\u827a\u672f\u54c1\u7684\u53c2\u6570\u5316\u8bbe\u8ba1\u4e2d\u3002", "result": "\u8fdb\u884c\u4e86\u4e00\u9879\u5b9e\u9a8c\u5ba4\u7814\u7a76\uff0c14\u540d\u53c2\u4e0e\u8005\u5728\u6709\u65e0AI\u652f\u6301\u7684\u60c5\u51b5\u4e0b\u521b\u5efa\u8fd9\u4e9b\u57fa\u4e8e\u4e2a\u4eba\u60c5\u611f\u7684\u7269\u54c1\uff0c\u89c2\u5bdf\u5230\u4e86\u53c2\u4e0e\u8005\u521b\u9020\u7b56\u7565\u3001\u610f\u4e49\u6784\u5efa\u8fc7\u7a0b\u4ee5\u53ca\u5bf9AI\u652f\u6301\u7684\u611f\u77e5\u7684\u7ec6\u5fae\u5dee\u5f02\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86AI\u5728\u4eba\u7c7b\u5171\u540c\u521b\u9020\u4e2a\u4eba\u60c5\u611f\u7269\u5316\u8fc7\u7a0b\u4e2d\u7684\u5173\u952e\u5f20\u529b\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u7ec6\u81f4\u7684\u8bae\u7a0b\u3002"}}
{"id": "2509.21523", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.21523", "abs": "https://arxiv.org/abs/2509.21523", "authors": ["Xiaofan Yu", "Yuwei Wu", "Katherine Mao", "Ye Tian", "Vijay Kumar", "Tajana Rosing"], "title": "DroneFL: Federated Learning for Multi-UAV Visual Target Tracking", "comment": null, "summary": "Multi-robot target tracking is a fundamental problem that requires\ncoordinated monitoring of dynamic entities in applications such as precision\nagriculture, environmental monitoring, disaster response, and security\nsurveillance. While Federated Learning (FL) has the potential to enhance\nlearning across multiple robots without centralized data aggregation, its use\nin multi-Unmanned Aerial Vehicle (UAV) target tracking remains largely\nunderexplored. Key challenges include limited onboard computational resources,\nsignificant data heterogeneity in FL due to varying targets and the fields of\nview, and the need for tight coupling between trajectory prediction and\nmulti-robot planning. In this paper, we introduce DroneFL, the first federated\nlearning framework specifically designed for efficient multi-UAV target\ntracking. We design a lightweight local model to predict target trajectories\nfrom sensor inputs, using a frozen YOLO backbone and a shallow transformer for\nefficient onboard training. The updated models are periodically aggregated in\nthe cloud for global knowledge sharing. To alleviate the data heterogeneity\nthat hinders FL convergence, DroneFL introduces a position-invariant model\narchitecture with altitude-based adaptive instance normalization. Finally, we\nfuse predictions from multiple UAVs in the cloud and generate optimal\ntrajectories that balance target prediction accuracy and overall tracking\nperformance. Our results show that DroneFL reduces prediction error by 6%-83%\nand tracking distance by 0.4%-4.6% compared to a distributed non-FL framework.\nIn terms of efficiency, DroneFL runs in real time on a Raspberry Pi 5 and has\non average just 1.56 KBps data rate to the cloud.", "AI": {"tldr": "DroneFL\u662f\u4e00\u4e2a\u4e13\u4e3a\u591a\u65e0\u4eba\u673a\u76ee\u6807\u8ffd\u8e2a\u8bbe\u8ba1\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u51cf\u5c11\u9884\u6d4b\u8bef\u5dee\u548c\u8ddf\u8e2a\u8ddd\u79bb\uff0c\u63d0\u5347\u6574\u4f53\u8ffd\u8e2a\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u76ee\u6807\u8ffd\u8e2a\u4e2d\u7684\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u3001\u6570\u636e\u5f02\u8d28\u6027\u5927\u548c\u8f68\u8ff9\u9884\u6d4b\u4e0e\u591a\u673a\u5668\u4eba\u89c4\u5212\u7d27\u5bc6\u8026\u5408\u7684\u6311\u6218\u3002", "method": "\u6784\u5efa\u8f7b\u91cf\u7ea7\u672c\u5730\u6a21\u578b\uff0c\u7ed3\u5408YOLO\u9aa8\u5e72\u7f51\u7edc\u548c\u6d45\u5c42\u53d8\u6362\u5668\u8fdb\u884c\u76ee\u6807\u8f68\u8ff9\u9884\u6d4b\uff0c\u5e76\u5728\u4e91\u7aef\u805a\u5408\u66f4\u65b0\u540e\u7684\u6a21\u578b\u3002", "result": "\u4e0e\u5206\u5e03\u5f0f\u975eFL\u6846\u67b6\u76f8\u6bd4\uff0cDroneFL\u7684\u9884\u6d4b\u8bef\u5dee\u51cf\u5c11\u4e866%-83%\uff0c\u8ddf\u8e2a\u8ddd\u79bb\u51cf\u5c11\u4e860.4%-4.6%\u3002", "conclusion": "DroneFL\u901a\u8fc7\u4f18\u5316\u591a\u65e0\u4eba\u673a\u76ee\u6807\u8ffd\u8e2a\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u5ea6\u548c\u8ddf\u8e2a\u6027\u80fd\uff0c\u4e14\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u5f02\u8d28\u6027\u95ee\u9898\u3002"}}
{"id": "2509.21731", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2509.21731", "abs": "https://arxiv.org/abs/2509.21731", "authors": ["Yuanning Han", "Ziyi Qiu", "Jiale Cheng", "RAY LC"], "title": "When Teams Embrace AI: Human Collaboration Strategies in Generative Prompting in a Creative Design Task", "comment": null, "summary": "Studies of Generative AI (GenAI)-assisted creative workflows have focused on\nindividuals overcoming challenges of prompting to produce what they envisioned.\nWhen designers work in teams, how do collaboration and prompting influence each\nother, and how do users perceive generative AI and their collaborators during\nthe co-prompting process? We engaged students with design or performance\nbackgrounds, and little exposure to GenAI, to work in pairs with GenAI to\ncreate stage designs based on a creative theme. We found two patterns of\ncollaborative prompting focused on generating story descriptions first, or\nvisual imagery first. GenAI tools helped participants build consensus in the\ntask, and allowed for discussion of the prompting strategies. Participants\nperceived GenAI as efficient tools rather than true collaborators, suggesting\nthat human partners reduced the reliance on their use. This work highlights the\nimportance of human-human collaboration when working with GenAI tools,\nsuggesting systems that take advantage of shared human expertise in the\nprompting process.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4eba\u7c7b\u5408\u4f5c\u8005\u4e0e\u751f\u6210\u578b\u4eba\u5de5\u667a\u80fd\u5171\u540c\u63d0\u793a\u8fc7\u7a0b\u4e2d\u7684\u76f8\u4e92\u5f71\u54cd\uff0c\u53d1\u73b0\u4eba\u7c7b\u5408\u4f5c\u662f\u5173\u952e\uff0c\u751f\u6210\u578b\u4eba\u5de5\u667a\u80fd\u88ab\u89c6\u4e3a\u6548\u7387\u5de5\u5177\u3002", "motivation": "\u63a2\u8ba8\u56e2\u961f\u4e2d\u5408\u4f5c\u8005\u4e4b\u95f4\u7684\u534f\u4f5c\u4e0e\u63d0\u793a\u5982\u4f55\u76f8\u4e92\u5f71\u54cd\uff0c\u4ee5\u53ca\u5728\u5171\u540c\u63d0\u793a\u8fc7\u7a0b\u4e2d\u7528\u6237\u5982\u4f55\u611f\u77e5\u751f\u6210\u578b\u4eba\u5de5\u667a\u80fd\u548c\u4ed6\u4eec\u7684\u5408\u4f5c\u8005\u3002", "method": "\u901a\u8fc7\u8ba9\u5177\u6709\u8bbe\u8ba1\u6216\u8868\u6f14\u80cc\u666f\u4e14\u5bf9\u751f\u6210\u578b\u4eba\u5de5\u667a\u80fd\u4e86\u89e3\u4e0d\u591a\u7684\u5b66\u751f\u4e24\u4eba\u4e00\u7ec4\uff0c\u4e0e\u751f\u6210\u578b\u4eba\u5de5\u667a\u80fd\u5408\u4f5c\u521b\u4f5c\u57fa\u4e8e\u521b\u610f\u4e3b\u9898\u7684\u821e\u53f0\u8bbe\u8ba1\u8fdb\u884c\u7814\u7a76\u3002", "result": "\u53d1\u73b0\u4e86\u4e24\u79cd\u96c6\u4e2d\u4e8e\u5148\u751f\u6210\u6545\u4e8b\u63cf\u8ff0\u6216\u89c6\u89c9\u56fe\u50cf\u7684\u534f\u4f5c\u63d0\u793a\u6a21\u5f0f\uff1b\u751f\u6210\u578b\u4eba\u5de5\u667a\u80fd\u5de5\u5177\u5e2e\u52a9\u53c2\u4e0e\u8005\u5728\u4efb\u52a1\u4e2d\u5efa\u7acb\u5171\u8bc6\uff0c\u5e76\u5141\u8bb8\u5bf9\u63d0\u793a\u7b56\u7565\u8fdb\u884c\u8ba8\u8bba\u3002\u53c2\u4e0e\u8005\u5c06\u751f\u6210\u578b\u4eba\u5de5\u667a\u80fd\u89c6\u4e3a\u9ad8\u6548\u5de5\u5177\uff0c\u800c\u975e\u771f\u6b63\u7684\u5408\u4f5c\u8005\uff0c\u8868\u660e\u4eba\u7c7b\u4f19\u4f34\u51cf\u5c11\u4e86\u5bf9\u5176\u4f7f\u7528\u7684\u4f9d\u8d56\u3002", "conclusion": "\u4eba\u7c7b\u5408\u4f5c\u5728\u4e0e\u751f\u6210\u578b\u4eba\u5de5\u667a\u80fd\u5de5\u5177\u7684\u4f7f\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u8fd9\u8868\u660e\u5728\u63d0\u793a\u8fc7\u7a0b\u4e2d\uff0c\u7cfb\u7edf\u5e94\u5145\u5206\u5229\u7528\u5171\u4eab\u7684\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u3002"}}
{"id": "2509.21543", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.21543", "abs": "https://arxiv.org/abs/2509.21543", "authors": ["Jinbang Huang", "Zhiyuan Li", "Zhanguang Zhang", "Xingyue Quan", "Jianye Hao", "Yingxue Zhang"], "title": "Plan2Evolve: LLM Self-Evolution for Improved Planning Capability via Automated Domain Generation", "comment": "25 pages, 7 figures", "summary": "Large Language Models (LLMs) have recently shown strong potential in robotic\ntask planning, particularly through automatic planning domain generation that\nintegrates symbolic search. Prior approaches, however, have largely treated\nthese domains as search utilities, with limited attention to their potential as\nscalable sources of reasoning data. At the same time, progress in reasoning\nLLMs has been driven by chain-of-thought (CoT) supervision, whose application\nin robotics remains dependent on costly, human-curated datasets. We propose\nPlan2Evolve, an LLM self-evolving framework in which the base model generates\nplanning domains that serve as engines for producing symbolic problem-plan\npairs as reasoning traces. These pairs are then transformed into extended CoT\ntrajectories by the same model through natural-language explanations, thereby\nexplicitly aligning symbolic planning structures with natural language\nreasoning. The resulting data extend beyond the model's intrinsic planning\ncapacity, enabling model fine-tuning that yields a planning-enhanced LLM with\nimproved planning success, stronger cross-task generalization, and reduced\ninference costs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u6211\u8fdb\u5316\u7684\u5927\u8bed\u8a00\u6a21\u578b\u6846\u67b6Plan2Evolve\uff0c\u901a\u8fc7\u751f\u6210\u89c4\u5212\u57df\u548c\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u7684\u80fd\u529b\uff0c\u5305\u62ec\u6210\u529f\u7387\u548c\u6cdb\u5316\u6027\u3002", "motivation": "\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5982\u4f55\u5229\u7528\u81ea\u751f\u6210\u7684\u6570\u636e\u6765\u63d0\u9ad8\u63a8\u7406\u80fd\u529b\uff0c\u51cf\u5c11\u5bf9\u4eba\u5de5\u7b56\u5212\u6570\u636e\u96c6\u7684\u4f9d\u8d56\u3002", "method": "\u63d0\u51fa\u4e86Plan2Evolve\u6846\u67b6\uff0c\u4f7f\u57fa\u672c\u6a21\u578b\u751f\u6210\u89c4\u5212\u57df\uff0c\u5e76\u4ea7\u751f\u7b26\u53f7\u95ee\u9898-\u8ba1\u5212\u5bf9\u4f5c\u4e3a\u63a8\u7406\u8f68\u8ff9\u3002", "result": "\u901a\u8fc7\u751f\u6210\u7684\u6570\u636e\u548c\u6269\u5c55\u7684\u94fe\u5f0f\u63a8\u7406\u8f68\u8ff9\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u89c4\u5212\u6210\u529f\u7387\uff0c\u589e\u5f3a\u4e86\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\uff0c\u51cf\u5c11\u4e86\u63a8\u7406\u6210\u672c\u3002", "conclusion": "Plan2Evolve\u6846\u67b6\u901a\u8fc7\u81ea\u6211\u8fdb\u5316\u751f\u6210\u89c4\u5212\u57df\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u4e2d\u7684\u80fd\u529b\u3002"}}
{"id": "2509.21860", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.21860", "abs": "https://arxiv.org/abs/2509.21860", "authors": ["Karthik S. Bhat", "Jiayue Melissa Shi", "Wenxuan Song", "Dong Whi Yoo", "Koustuv Saha"], "title": "\"In my defense, only three hours on Instagram\": Designing Toward Digital Self-Awareness and Wellbeing", "comment": null, "summary": "Screen use pervades daily life, shaping work, leisure, and social connections\nwhile raising concerns for digital wellbeing. Yet, reducing screen time alone\nrisks oversimplifying technology's role and neglecting its potential for\nmeaningful engagement. We posit self-awareness -- reflecting on one's digital\nbehavior -- as a critical pathway to digital wellbeing. We developed\nWellScreen, a lightweight probe that scaffolds daily reflection by asking\npeople to estimate and report smartphone use. In a two-week deployment (N=25),\nwe examined how discrepancies between estimated and actual usage shaped digital\nawareness and wellbeing. Participants often underestimated productivity and\nsocial media while overestimating entertainment app use. They showed a 10%\nimprovement in positive affect, rating WellScreen as moderately useful.\nInterviews revealed that structured reflection supported recognition of\npatterns, adjustment of expectations, and more intentional engagement with\ntechnology. Our findings highlight the promise of lightweight reflective\ninterventions for supporting self-awareness and intentional digital engagement,\noffering implications for designing digital wellbeing tools.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u901a\u8fc7WellScreen\u8fdb\u884c\u81ea\u6211\u53cd\u601d\u4ee5\u63d0\u5347\u6570\u5b57\u5065\u5eb7\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u5176\u80fd\u6539\u5584\u53c2\u4e0e\u8005\u7684\u79ef\u6781\u60c5\u7eea\uff0c\u4fc3\u8fdb\u66f4\u6709\u610f\u7684\u6280\u672f\u4f7f\u7528\u3002", "motivation": "\u5c3d\u7ba1\u51cf\u5c11\u5c4f\u5e55\u65f6\u95f4\u7684\u91cd\u8981\u6027\u88ab\u5e7f\u6cdb\u5021\u5bfc\uff0c\u4f46\u5ffd\u89c6\u6280\u672f\u5bf9\u6709\u610f\u4e49\u53c2\u4e0e\u7684\u6f5c\u529b\u3002\u56e0\u6b64\uff0c\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u81ea\u6211\u610f\u8bc6\u6765\u4fc3\u8fdb\u6570\u5b57\u5065\u5eb7\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u6b3e\u540d\u4e3aWellScreen\u7684\u8f7b\u91cf\u7ea7\u63a2\u6d4b\u8bbe\u5907\uff0c\u901a\u8fc7\u8ba9\u53c2\u4e0e\u8005\u4f30\u8ba1\u548c\u62a5\u544a\u667a\u80fd\u624b\u673a\u4f7f\u7528\u60c5\u51b5\u8fdb\u884c\u65e5\u5e38\u53cd\u601d\uff0c\u8fdb\u884c\u4e86\u4e3a\u671f\u4e24\u5468\u7684\u90e8\u7f72\u7814\u7a76\uff08\u6837\u672c\u91cfN=25\uff09\u3002", "result": "\u53c2\u4e0e\u8005\u901a\u5e38\u4f4e\u4f30\u751f\u4ea7\u529b\u548c\u793e\u4ea4\u5a92\u4f53\u4f7f\u7528\uff0c\u8fc7\u9ad8\u4f30\u8ba1\u5a31\u4e50\u5e94\u7528\u4f7f\u7528\uff0c\u79ef\u6781\u60c5\u7eea\u63d0\u9ad8\u4e8610%\uff0c\u5e76\u5c06WellScreen\u8bc4\u4e3a\u4e2d\u7b49\u5b9e\u7528\u3002\u8bbf\u8c08\u663e\u793a\uff0c\u7ed3\u6784\u5316\u53cd\u601d\u6709\u52a9\u4e8e\u8bc6\u522b\u4f7f\u7528\u6a21\u5f0f\u3001\u8c03\u6574\u671f\u671b\uff0c\u5e76\u66f4\u52a0\u6709\u610f\u5730\u53c2\u4e0e\u6280\u672f\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u53cd\u601d\u5e72\u9884\u63aa\u65bd\u80fd\u591f\u652f\u6301\u81ea\u6211\u610f\u8bc6\u548c\u6709\u610f\u7684\u6570\u5b57\u53c2\u4e0e\uff0c\u4e3a\u8bbe\u8ba1\u6570\u5b57\u5065\u5eb7\u5de5\u5177\u63d0\u4f9b\u4e86\u542f\u793a\u3002"}}
{"id": "2509.21563", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.21563", "abs": "https://arxiv.org/abs/2509.21563", "authors": ["Zhixin Zhang", "Liang Zhao", "Pawel Ladosz"], "title": "PL-VIWO2: A Lightweight, Fast and Robust Visual-Inertial-Wheel Odometry Using Points and Lines", "comment": "16 pages", "summary": "Vision-based odometry has been widely adopted in autonomous driving owing to\nits low cost and lightweight setup; however, its performance often degrades in\ncomplex outdoor urban environments. To address these challenges, we propose\nPL-VIWO2, a filter-based visual-inertial-wheel odometry system that integrates\nan IMU, wheel encoder, and camera (supporting both monocular and stereo) for\nlong-term robust state estimation. The main contributions are: (i) a novel line\nfeature processing framework that exploits the geometric relationship between\n2D feature points and lines, enabling fast and robust line tracking and\ntriangulation while ensuring real-time performance; (ii) an SE(2)-constrained\nSE(3) wheel pre-integration method that leverages the planar motion\ncharacteristics of ground vehicles for accurate wheel updates; and (iii) an\nefficient motion consistency check (MCC) that filters out dynamic features by\njointly using IMU and wheel measurements. Extensive experiments on Monte Carlo\nsimulations and public autonomous driving datasets demonstrate that PL-VIWO2\noutperforms state-of-the-art methods in terms of accuracy, efficiency, and\nrobustness.", "AI": {"tldr": "PL-VIWO2\u901a\u8fc7\u6574\u5408\u591a\u79cd\u4f20\u611f\u5668\u548c\u521b\u65b0\u7684\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u89c6\u89c9\u91cc\u7a0b\u8ba1\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u91cc\u7a0b\u8ba1\u5728\u590d\u6742\u7684\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u63d0\u5347\u957f\u671f\u72b6\u6001\u4f30\u8ba1\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6ee4\u6ce2\u7684\u89c6\u89c9\u60ef\u6027\u8f6e\u5f0f\u91cc\u7a0b\u8ba1\u7cfb\u7edf\uff0c\u6574\u5408\u4e86IMU\u3001\u8f6e\u7f16\u7801\u5668\u548c\u76f8\u673a\uff0c\u652f\u6301\u5355\u76ee\u548c\u7acb\u4f53\u89c6\u89c9\uff0c\u91c7\u7528\u65b0\u9896\u7684\u7ebf\u7279\u5f81\u5904\u7406\u6846\u67b6\u3001SE(2)\u7ea6\u675f\u7684SE(3)\u8f6e\u9884\u79ef\u5206\u65b9\u6cd5\u548c\u8fd0\u52a8\u4e00\u81f4\u6027\u68c0\u9a8c\u3002", "result": "\u5728Monte Carlo\u4eff\u771f\u548c\u516c\u5171\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPL-VIWO2\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "conclusion": "PL-VIWO2\u7cfb\u7edf\u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2509.21868", "categories": ["cs.HC", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21868", "abs": "https://arxiv.org/abs/2509.21868", "authors": ["Yuxuan Li", "Sauvik Das", "Hirokazu Shirado"], "title": "What Makes LLM Agent Simulations Useful for Policy? Insights From an Iterative Design Engagement in Emergency Preparedness", "comment": null, "summary": "There is growing interest in using Large Language Models as agents (LLM\nagents) for social simulations to inform policy, yet real-world adoption\nremains limited. This paper addresses the question: How can LLM agent\nsimulations be made genuinely useful for policy? We report on a year-long\niterative design engagement with a university emergency preparedness team.\nAcross multiple design iterations, we iteratively developed a system of 13,000\nLLM agents that simulate crowd movement and communication during a large-scale\ngathering under various emergency scenarios. These simulations informed actual\npolicy implementation, shaping volunteer training, evacuation protocols, and\ninfrastructure planning. Analyzing this process, we identify three design\nimplications: start with verifiable scenarios and build trust gradually, use\npreliminary simulations to elicit tacit knowledge, and treat simulation and\npolicy development as evolving together. These implications highlight\nactionable pathways to making LLM agent simulations that are genuinely useful\nfor policy.", "AI": {"tldr": "\u672c\u8bba\u6587\u63a2\u8ba8\u5982\u4f55\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u6a21\u62df\u63d0\u5347\u653f\u7b56\u5236\u5b9a\u7684\u5b9e\u7528\u6027\uff0c\u63d0\u51fa\u4e86\u4e09\u6761\u8bbe\u8ba1\u542f\u793a\u3002", "motivation": "\u63a2\u8ba8\u5982\u4f55\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u6a21\u62df\u5728\u653f\u7b56\u5236\u5b9a\u4e2d\u771f\u6b63\u6709\u7528\u3002", "method": "\u901a\u8fc7\u4e0e\u5927\u5b66\u5e94\u6025\u5907\u6218\u56e2\u961f\u7684\u6301\u7eed\u8bbe\u8ba1\u4e92\u52a8\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u6a21\u62df\u4eba\u7fa4\u79fb\u52a8\u4e0e\u6c9f\u901a\u7684\u7cfb\u7edf\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e8613,000\u4e2aLLM\u4ee3\u7406\uff0c\u8fd9\u4e9b\u4ee3\u7406\u5728\u5404\u79cd\u7d27\u6025\u60c5\u51b5\u4e0b\u6a21\u62df\u5927\u578b\u805a\u4f1a\u7684\u4eba\u7fa4\u884c\u4e3a\uff0c\u4fc3\u8fdb\u4e86\u5b9e\u9645\u653f\u7b56\u5b9e\u65bd\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u8bbe\u8ba1\u542f\u793a\u4e3a\u4f7fLLM\u4ee3\u7406\u6a21\u62df\u5bf9\u653f\u7b56\u771f\u6b63\u6709\u7528\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u8def\u5f84\u3002"}}
{"id": "2509.21571", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.21571", "abs": "https://arxiv.org/abs/2509.21571", "authors": ["HaoZhe Xu", "Cheng Cheng", "HongRui Sang", "Zhipeng Wang", "Qiyong He", "Xiuxian Li", "Bin He"], "title": "Autonomous UAV-Quadruped Docking in Complex Terrains via Active Posture Alignment and Constraint-Aware Control", "comment": null, "summary": "Autonomous docking between Unmanned Aerial Vehicles (UAVs) and ground robots\nis essential for heterogeneous systems, yet most existing approaches target\nwheeled platforms whose limited mobility constrains exploration in complex\nterrains. Quadruped robots offer superior adaptability but undergo frequent\nposture variations, making it difficult to provide a stable landing surface for\nUAVs. To address these challenges, we propose an autonomous UAV-quadruped\ndocking framework for GPS-denied environments. On the quadruped side, a Hybrid\nInternal Model with Horizontal Alignment (HIM-HA), learned via deep\nreinforcement learning, actively stabilizes the torso to provide a level\nplatform. On the UAV side, a three-phase strategy is adopted, consisting of\nlong-range acquisition with a median-filtered YOLOv8 detector, close-range\ntracking with a constraint-aware controller that integrates a Nonsingular Fast\nTerminal Sliding Mode Controller (NFTSMC) and a logarithmic Barrier Function\n(BF) to guarantee finite-time error convergence under field-of-view (FOV)\nconstraints, and terminal descent guided by a Safety Period (SP) mechanism that\njointly verifies tracking accuracy and platform stability. The proposed\nframework is validated in both simulation and real-world scenarios,\nsuccessfully achieving docking on outdoor staircases higher than 17 cm and\nrough slopes steeper than 30 degrees. Supplementary materials and videos are\navailable at: https://uav-quadruped-docking.github.io.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u4e3bUAV-\u56db\u8db3\u673a\u5668\u4eba\u5bf9\u63a5\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u5728GPS\u7f3a\u5931\u73af\u5883\u4e2d\uff0c\u56db\u8db3\u673a\u5668\u4eba\u4e3aUAV\u63d0\u4f9b\u7a33\u5b9a\u5bf9\u63a5\u5e73\u53f0\uff0c\u9a8c\u8bc1\u4e86\u9ad8\u96be\u5ea6\u5730\u5f62\u7684\u5bf9\u63a5\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u9488\u5bf9\u8f6e\u5f0f\u5e73\u53f0\u7684\u5bf9\u63a5\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u590d\u6742\u5730\u5f62\u9700\u6c42\uff0c\u800c\u56db\u8db3\u673a\u5668\u4eba\u7684\u9002\u5e94\u6027\u66f4\u5f3a\uff0c\u4f46\u5176\u9891\u7e41\u7684\u59ff\u6001\u53d8\u5316\u4f7fUAV\u5bf9\u63a5\u56f0\u96be\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684HIM-HA\u7a33\u5b9a\u56db\u8db3\u673a\u5668\u4eba\u7684\u8eaf\u5e72\uff0c\u5e76\u7ed3\u5408YOLOv8\u3001NFTSMC\u53caBF\u7b49\u6280\u672f\u5b9e\u73b0\u4e09\u9636\u6bb5\u7684UAV\u5bf9\u63a5\u63a7\u5236\u3002", "result": "\u5728\u6a21\u62df\u53ca\u5b9e\u9645\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u63d0\u8bae\u7684\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u590d\u6742\u5730\u5f62\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5bf9\u63a5\u3002", "conclusion": "\u63d0\u51fa\u7684\u81ea\u4e3bUAV-\u56db\u8db3\u673a\u5668\u4eba\u5bf9\u63a5\u6846\u67b6\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u8fbe17\u5398\u7c73\u7684\u6237\u5916\u53f0\u9636\u548c\u8d85\u8fc730\u5ea6\u7684\u9661\u5761\u5bf9\u63a5\u3002"}}
{"id": "2509.21890", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2509.21890", "abs": "https://arxiv.org/abs/2509.21890", "authors": ["Qianou Ma", "Kenneth Koedinger", "Tongshuang Wu"], "title": "Not Everyone Wins with LLMs: Behavioral Patterns and Pedagogical Implications in AI-assisted Data Analysis", "comment": null, "summary": "LLMs promise to democratize technical work in complex domains like\nprogrammatic data analysis, but not everyone benefits equally. We study how\nstudents with varied expertise use LLMs to complete Python-based data analysis\nin computational notebooks in a non-major course. Drawing on homework logs,\nrecordings, and surveys from 36 students, we ask: Which expertise matters most,\nand how does it shape AI use? Our mixed-methods analysis shows that technical\nexpertise -- not AI familiarity or communication skills -- remains a\nsignificant predictor of success. Students also vary widely in how they\nleverage LLMs, struggling at stages of forming intent, expressing inputs,\ninterpreting outputs, and assessing results. We identify success and failure\nbehaviors, such as providing context or decomposing prompts, that distinguish\neffective use. These findings inform AI literacy interventions, highlighting\nthat lightweight demonstrations improve surface fluency but are insufficient;\ndeeper training and scaffolds are needed to cultivate resilient AI use skills.", "AI": {"tldr": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u6280\u672f\u4e13\u957f\u662f\u6210\u529f\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u91cd\u8981\u56e0\u7d20\uff0c\u800c\u8f7b\u91cf\u7ea7\u57f9\u8bad\u4e0d\u591f\uff0c\u9700\u8981\u66f4\u6df1\u5c42\u6b21\u7684\u57f9\u8bad\u6765\u63d0\u9ad8AI\u4f7f\u7528\u6280\u80fd\u3002", "motivation": "\u7814\u7a76\u4e86\u89e3\u4e0d\u540c\u4e13\u4e1a\u80cc\u666f\u7684\u5b66\u751f\u5982\u4f55\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884cPython\u6570\u636e\u5206\u6790\uff0c\u4ee5\u4fc3\u8fdb\u66f4\u516c\u5e73\u7684\u6280\u672f\u5de5\u4f5c\u3002", "method": "\u901a\u8fc7\u5206\u679036\u540d\u5b66\u751f\u7684\u4f5c\u4e1a\u65e5\u5fd7\u3001\u5f55\u97f3\u548c\u95ee\u5377\u8c03\u67e5\uff0c\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\u5b66\u751f\u5728\u975e\u4e13\u4e1a\u8bfe\u7a0b\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u6280\u672f\u4e13\u957f\u663e\u8457\u5f71\u54cd\u6210\u529f\u4e0e\u5426\uff0c\u5b66\u751f\u5728\u5229\u7528LLMs\u65f6\u8868\u73b0\u51fa\u663e\u8457\u5dee\u5f02\uff0c\u5305\u62ec\u5f62\u6210\u610f\u56fe\u3001\u8868\u8fbe\u8f93\u5165\u3001\u89e3\u91ca\u8f93\u51fa\u548c\u8bc4\u4f30\u7ed3\u679c\u7684\u80fd\u529b\u3002", "conclusion": "\u6280\u672f\u4e13\u957f\u662f\u6210\u529f\u4f7f\u7528LLMs\u7684\u5173\u952e\uff0c\u800c\u4ec5\u4ec5\u4f9d\u8d56\u8f7b\u91cf\u7ea7\u793a\u8303\u662f\u4e0d\u591f\u7684\uff0c\u9700\u8fdb\u884c\u66f4\u6df1\u5165\u7684\u57f9\u8bad\u4ee5\u57f9\u517b\u7a33\u5065\u7684\u4eba\u5de5\u667a\u80fd\u4f7f\u7528\u6280\u80fd\u3002"}}
{"id": "2509.21602", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.21602", "abs": "https://arxiv.org/abs/2509.21602", "authors": ["Yang Jiao", "Yiding Qiu", "Henrik I. Christensen"], "title": "Real-Time Indoor Object SLAM with LLM-Enhanced Priors", "comment": null, "summary": "Object-level Simultaneous Localization and Mapping (SLAM), which incorporates\nsemantic information for high-level scene understanding, faces challenges of\nunder-constrained optimization due to sparse observations. Prior work has\nintroduced additional constraints using commonsense knowledge, but obtaining\nsuch priors has traditionally been labor-intensive and lacks generalizability\nacross diverse object categories. We address this limitation by leveraging\nlarge language models (LLMs) to provide commonsense knowledge of object\ngeometric attributes, specifically size and orientation, as prior factors in a\ngraph-based SLAM framework. These priors are particularly beneficial during the\ninitial phase when object observations are limited. We implement a complete\npipeline integrating these priors, achieving robust data association on sparse\nobject-level features and enabling real-time object SLAM. Our system, evaluated\non the TUM RGB-D and 3RScan datasets, improves mapping accuracy by 36.8\\% over\nthe latest baseline. Additionally, we present real-world experiments in the\nsupplementary video, demonstrating its real-time performance.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5e38\u8bc6\u77e5\u8bc6\uff0c\u672c\u6587\u63d0\u51fa\u7684SLAM\u65b9\u6cd5\u80fd\u591f\u5728\u7269\u4f53\u6620\u5c04\u4e2d\u663e\u8457\u63d0\u9ad8\u7cbe\u5ea6\uff0c\u5e76\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u73b0\u6709SLAM\u65b9\u6cd5\u5728\u5bf9\u8c61\u7ea7\u522b\u6620\u5c04\u4e2d\uff0c\u7531\u4e8e\u7a00\u758f\u89c2\u5bdf\u5bfc\u81f4\u7684\u4f18\u5316\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u7269\u4f53\u89c2\u5bdf\u7684\u6709\u6548\u6027\u3002", "method": "\u901a\u8fc7\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63d0\u4f9b\u7684\u7269\u4f53\u51e0\u4f55\u5c5e\u6027\u7684\u5e38\u8bc6\u77e5\u8bc6\uff0c\u96c6\u6210\u4e8e\u57fa\u4e8e\u56fe\u7684SLAM\u6846\u67b6\u4e2d\u3002", "result": "\u5728TUM RGB-D\u548c3RScan\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u6620\u5c04\u51c6\u786e\u6027\u63d0\u5347\u4e8636.8%\uff0c\u4e14\u7cfb\u7edf\u5177\u5907\u5b9e\u65f6\u6267\u884c\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684SLAM\u65b9\u6cd5\u5728\u7a00\u758f\u89c2\u5bdf\u4e0b\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6620\u5c04\u51c6\u786e\u6027\u63d0\u5347\uff0c\u4e14\u5728\u5b9e\u65f6\u6027\u80fd\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\u3002"}}
{"id": "2509.21914", "categories": ["cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21914", "abs": "https://arxiv.org/abs/2509.21914", "authors": ["Pawel Weichbroth", "Tomasz Szot"], "title": "The MUG-10 Framework for Preventing Usability Issues in Mobile Application Development", "comment": "26 pages, 3 figures, 1 table, 110 references", "summary": "Nowadays, mobile applications are essential tools for everyday life,\nproviding users with anytime, anywhere access to up-to-date information,\ncommunication, and entertainment. Needless to say, hardware limitations and the\ndiverse needs of different user groups pose a number of design and development\nchallenges. According to recent studies, usability is one of the most revealing\namong many others. However, few have made the direct effort to provide and\ndiscuss what countermeasures can be applied to avoid usability issues in mobile\napplication development. Through a survey of 20 mobile software design and\ndevelopment practitioners, this study aims to fill this research gap. Given the\nqualitative nature of the data collected, and with the goal of capturing and\npreserving the intrinsic meanings embedded in the experts' statements, we\nadopted in vivo coding. The analysis of the collected material enabled us to\ndevelop a novel framework consisting of ten guidelines and three activities\nwith general applications. In addition, it can be noted that active\ncollaboration with users in testing and collecting feedback was often\nemphasized at each stage of mobile application development. Future research\nshould consider focused action research that evaluates the effectiveness of our\nrecommendations and validates them across different stakeholder groups. In this\nregard, the development of automated tools to support early detection and\nmitigation of usability issues during mobile application development could also\nbe considered.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u79fb\u52a8\u5e94\u7528\u5f00\u53d1\u4e2d\u7684\u53ef\u7528\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u5bf9\u4ece\u4e1a\u8005\u7684\u8c03\u67e5\u63d0\u51fa\u4e86\u5305\u542b\u5341\u6761\u6307\u5bfc\u539f\u5219\u7684\u65b0\u6846\u67b6\uff0c\u5e76\u5f3a\u8c03\u4e86\u4e0e\u7528\u6237\u7684\u79ef\u6781\u5408\u4f5c\u3002", "motivation": "\u5f53\u524d\u79fb\u52a8\u5e94\u7528\u9762\u4e34\u786c\u4ef6\u9650\u5236\u548c\u7528\u6237\u591a\u6837\u5316\u9700\u6c42\u5e26\u6765\u7684\u8bbe\u8ba1\u4e0e\u5f00\u53d1\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u53ef\u7528\u6027\u65b9\u9762\u7f3a\u4e4f\u76f4\u63a5\u7684\u89e3\u51b3\u63aa\u65bd\u3002", "method": "\u901a\u8fc7\u5bf920\u540d\u79fb\u52a8\u8f6f\u4ef6\u8bbe\u8ba1\u548c\u5f00\u53d1\u4ece\u4e1a\u8005\u7684\u8c03\u67e5\uff0c\u91c7\u7528in vivo\u7f16\u7801\u8fdb\u884c\u6570\u636e\u5206\u6790\u3002", "result": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u5957\u7531\u5341\u6761\u6307\u5bfc\u539f\u5219\u548c\u4e09\u4e2a\u6d3b\u52a8\u7ec4\u6210\u7684\u65b0\u6846\u67b6\uff0c\u5f3a\u8c03\u5728\u79fb\u52a8\u5e94\u7528\u5f00\u53d1\u5404\u4e2a\u9636\u6bb5\u4e0e\u7528\u6237\u8fdb\u884c\u79ef\u6781\u7684\u5408\u4f5c\u548c\u53cd\u9988\u6536\u96c6\u3002", "conclusion": "\u5f00\u53d1\u53ef\u81ea\u52a8\u5316\u7684\u5de5\u5177\u4ee5\u652f\u6301\u79fb\u52a8\u5e94\u7528\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u53ef\u7528\u6027\u95ee\u9898\u7684\u65e9\u671f\u53d1\u73b0\u548c\u7f13\u89e3\u662f\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\u3002"}}
{"id": "2509.21664", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21664", "abs": "https://arxiv.org/abs/2509.21664", "authors": ["Philippe Nadeau", "Miguel Rogel", "Ivan Bili\u0107", "Ivan Petrovi\u0107", "Jonathan Kelly"], "title": "Generating Stable Placements via Physics-guided Diffusion Models", "comment": "Submitted to the IEEE International Conference on Robotics and\n  Automation 2026, Vienna, Austria, June 1-5, 2026", "summary": "Stably placing an object in a multi-object scene is a fundamental challenge\nin robotic manipulation, as placements must be penetration-free, establish\nprecise surface contact, and result in a force equilibrium. To assess\nstability, existing methods rely on running a simulation engine or resort to\nheuristic, appearance-based assessments. In contrast, our approach integrates\nstability directly into the sampling process of a diffusion model. To this end,\nwe query an offline sampling-based planner to gather multi-modal placement\nlabels and train a diffusion model to generate stable placements. The diffusion\nmodel is conditioned on scene and object point clouds, and serves as a\ngeometry-aware prior. We leverage the compositional nature of score-based\ngenerative models to combine this learned prior with a stability-aware loss,\nthereby increasing the likelihood of sampling from regions of high stability.\nImportantly, this strategy requires no additional re-training or fine-tuning,\nand can be directly applied to off-the-shelf models. We evaluate our method on\nfour benchmark scenes where stability can be accurately computed. Our\nphysics-guided models achieve placements that are 56% more robust to forceful\nperturbations while reducing runtime by 47% compared to a state-of-the-art\ngeometric method.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u5c06\u7a33\u5b9a\u6027\u878d\u5165\u6269\u6563\u6a21\u578b\u7684\u91c7\u6837\u8fc7\u7a0b\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7269\u4f53\u653e\u7f6e\u7684\u7a33\u5b9a\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u591a\u7269\u4f53\u573a\u666f\u4e0b\u7684\u7269\u4f53\u653e\u7f6e\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u79bb\u7ebf\u91c7\u6837\u89c4\u5212\u5668\u6536\u96c6\u591a\u6a21\u6001\u653e\u7f6e\u6807\u7b7e\uff0c\u5e76\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u751f\u6210\u7a33\u5b9a\u653e\u7f6e\uff0c\u7ed3\u5408\u7a33\u5b9a\u6027\u635f\u5931\u51fd\u6570\uff0c\u91c7\u7528\u7269\u7406\u5f15\u5bfc\u7684\u6a21\u578b\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7a33\u5065\u6027\uff0c\u5e76\u51cf\u5c11\u4e86\u8fd0\u884c\u65f6\u95f4\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u7a33\u5b9a\u6027\u76f4\u63a5\u878d\u5165\u6269\u6563\u6a21\u578b\u7684\u91c7\u6837\u8fc7\u7a0b\u4e2d\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u7269\u4f53\u5728\u591a\u7269\u4f53\u573a\u666f\u4e2d\u7684\u653e\u7f6e\u7a33\u5b9a\u6027\uff0c\u4f7f\u6240\u751f\u6210\u7684\u4f4d\u7f6e\u66f4\u52a0\u7a33\u5065\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002"}}
{"id": "2509.22146", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2509.22146", "abs": "https://arxiv.org/abs/2509.22146", "authors": ["Vivienne Jia Zhong", "Theresa Schmiedel"], "title": "Which Values Matter to Socially Assistive Robots in Elder Care Settings? Empirically Investigating Values That Should Be Embedded in SARs from a Multi-Stakeholder Perspective", "comment": null, "summary": "The integration of socially assistive robots (SARs) in elder care settings\nhas the potential to address critical labor shortages while enhancing the\nquality of care. However, the design of SARs must align with the values of\nvarious stakeholders to ensure their acceptance and efficacy. This study\nempirically investigates the values that should be embedded in SARs from a\nmulti-stakeholder perspective, including care receivers, caregivers,\ntherapists, relatives, and other involved parties. Utilizing a combination of\nsemi-structured interviews and focus groups, we identify a wide range of values\nrelated to safety, trust, care, privacy, and autonomy, and illustrate how\nstakeholders interpret these values in real-world care environments. Our\nfindings reveal several value tensions and propose potential resolutions to\nthese tensions. Additionally, the study highlights under-researched values such\nas calmness and collaboration, which are critical in fostering a supportive and\nefficient care environment. Our work contributes to the understanding of\nvalue-sensitive design of SARs and aids practitioners in developing SARs that\nalign with human values, ultimately promoting socially responsible applications\nin elder care settings.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u8001\u5e74\u62a4\u7406\u4e2d\u8bbe\u8ba1\u793e\u4ea4\u8f85\u52a9\u673a\u5668\u4eba\u5e94\u5d4c\u5165\u7684\u591a\u65b9\u5229\u76ca\u76f8\u5173\u8005\u7684\u4ef7\u503c\u89c2\uff0c\u5e76\u63d0\u51fa\u4e86\u5e94\u5bf9\u4ef7\u503c\u51b2\u7a81\u7684\u65b9\u6848\u3002", "motivation": "\u968f\u7740\u8001\u5e74\u62a4\u7406\u9886\u57df\u7684\u52b3\u52a8\u77ed\u7f3a\u95ee\u9898\u65e5\u76ca\u4e25\u91cd\uff0c\u5f00\u53d1\u7b26\u5408\u5404\u65b9\u5229\u76ca\u7684\u793e\u4ea4\u8f85\u52a9\u673a\u5668\u4eba\u4ee5\u63d0\u5347\u62a4\u7406\u8d28\u91cf\u663e\u5f97\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u548c\u7126\u70b9\u5c0f\u7ec4\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u8c03\u67e5\u6765\u81ea\u591a\u4e2a\u5229\u76ca\u76f8\u5173\u8005\u7684\u4ef7\u503c\u89c2\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e86\u591a\u79cd\u4e0e\u5b89\u5168\u3001\u4fe1\u4efb\u3001\u62a4\u7406\u3001\u9690\u79c1\u548c\u81ea\u4e3b\u6027\u76f8\u5173\u7684\u4ef7\u503c\u89c2\uff0c\u5e76\u63ed\u793a\u4e86\u5229\u76ca\u76f8\u5173\u8005\u5bf9\u8fd9\u4e9b\u4ef7\u503c\u89c2\u5728\u5b9e\u9645\u62a4\u7406\u73af\u5883\u4e2d\u7684\u4e0d\u540c\u89e3\u8bfb\u3002", "conclusion": "\u672c\u7814\u7a76\u589e\u5f3a\u4e86\u5bf9SARs\u4ef7\u503c\u654f\u611f\u8bbe\u8ba1\u7684\u7406\u89e3\uff0c\u5e2e\u52a9\u4ece\u4e1a\u8005\u5f00\u53d1\u7b26\u5408\u4eba\u7c7b\u4ef7\u503c\u7684\u793e\u4ea4\u8f85\u52a9\u673a\u5668\u4eba\uff0c\u4ece\u800c\u4fc3\u8fdb\u8001\u5e74\u62a4\u7406\u9886\u57df\u7684\u793e\u4f1a\u8d23\u4efb\u5e94\u7528\u3002"}}
{"id": "2509.21690", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.21690", "abs": "https://arxiv.org/abs/2509.21690", "authors": ["Muqun Hu", "Wenxi Chen", "Wenjing Li", "Falak Mandali", "Zijian He", "Renhong Zhang", "Praveen Krisna", "Katherine Christian", "Leo Benaharon", "Dizhi Ma", "Karthik Ramani", "Yan Gu"], "title": "Towards Versatile Humanoid Table Tennis: Unified Reinforcement Learning with Prediction Augmentation", "comment": null, "summary": "Humanoid table tennis (TT) demands rapid perception, proactive whole-body\nmotion, and agile footwork under strict timing -- capabilities that remain\ndifficult for unified controllers. We propose a reinforcement learning\nframework that maps ball-position observations directly to whole-body joint\ncommands for both arm striking and leg locomotion, strengthened by predictive\nsignals and dense, physics-guided rewards. A lightweight learned predictor, fed\nwith recent ball positions, estimates future ball states and augments the\npolicy's observations for proactive decision-making. During training, a\nphysics-based predictor supplies precise future states to construct dense,\ninformative rewards that lead to effective exploration. The resulting policy\nattains strong performance across varied serve ranges (hit rate $\\geq$ 96% and\nsuccess rate $\\geq$ 92%) in simulations. Ablation studies confirm that both the\nlearned predictor and the predictive reward design are critical for end-to-end\nlearning. Deployed zero-shot on a physical Booster T1 humanoid with 23 revolute\njoints, the policy produces coordinated lateral and forward-backward footwork\nwith accurate, fast returns, suggesting a practical path toward versatile,\ncompetitive humanoid TT.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u4fe1\u53f7\u548c\u7269\u7406\u5956\u52b1\u63d0\u5347\u4eba\u5f62\u673a\u5668\u4eba\u4e52\u4e53\u7403\u7684\u8868\u73b0\uff0c\u53d6\u5f97\u4e86\u9ad8\u51fb\u7403\u7387\u548c\u6210\u529f\u7387\u3002", "motivation": " humanoid\u4e52\u4e53\u7403\u9700\u8981\u5feb\u901f\u611f\u77e5\u3001\u4e3b\u52a8\u7684\u5168\u8eab\u8fd0\u52a8\u548c\u7075\u6d3b\u7684\u811a\u6b65\uff0c\u8fd9\u4e9b\u80fd\u529b\u5728\u7edf\u4e00\u63a7\u5236\u5668\u4e2d\u4ecd\u7136\u5f88\u96be\u5b9e\u73b0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u7403\u7684\u4f4d\u7f6e\u89c2\u5bdf\u76f4\u63a5\u6620\u5c04\u5230\u5168\u8eab\u5173\u8282\u6307\u4ee4\uff0c\u5e76\u5229\u7528\u9884\u6d4b\u4fe1\u53f7\u548c\u5bc6\u96c6\u7684\u7269\u7406\u5f15\u5bfc\u5956\u52b1\u589e\u5f3a\u5b66\u4e60\u6548\u679c\u3002", "result": "\u5728\u591a\u79cd\u53d1\u7403\u8303\u56f4\u5185\uff0c\u653f\u7b56\u7684\u51fb\u7403\u7387\u8fbe\u523096%\uff0c\u6210\u529f\u7387\u8fbe\u523092%\u3002\u5728\u7269\u7406Booster T1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u96f6-shot\u90e8\u7f72\uff0c\u5c55\u73b0\u51fa\u51fa\u8272\u7684\u534f\u8c03\u8fd0\u52a8\u548c\u5feb\u901f\u56de\u7403\u3002", "conclusion": "\u8be5\u653f\u7b56\u5728\u6a21\u62df\u4e2d\u8868\u73b0\u5f3a\u52b2\uff0c\u6210\u529f\u7387\u8d85\u8fc796%\uff0c\u663e\u793a\u4e86\u5176\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.22168", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22168", "abs": "https://arxiv.org/abs/2509.22168", "authors": ["Esen K. T\u00fct\u00fcnc\u00fc", "Lissette Lemus", "Kris Pilcher", "Holger Sprengel", "Jordi Sabater-Mir"], "title": "Teaching AI to Feel: A Collaborative, Full-Body Exploration of Emotive Communication", "comment": "9 pages, 10 Figures, ACM MM'25", "summary": "Commonaiverse is an interactive installation exploring human emotions through\nfull-body motion tracking and real-time AI feedback. Participants engage in\nthree phases: Teaching, Exploration and the Cosmos Phase, collaboratively\nexpressing and interpreting emotions with the system. The installation\nintegrates MoveNet for precise motion tracking and a multi-recommender AI\nsystem to analyze emotional states dynamically, responding with adaptive\naudiovisual outputs. By shifting from top-down emotion classification to\nparticipant-driven, culturally diverse definitions, we highlight new pathways\nfor inclusive, ethical affective computing. We discuss how this collaborative,\nout-of-the-box approach pushes multimedia research beyond single-user facial\nanalysis toward a more embodied, co-created paradigm of emotional AI.\nFurthermore, we reflect on how this reimagined framework fosters user agency,\nreduces bias, and opens avenues for advanced interactive applications.", "AI": {"tldr": "Commonaiverse\u662f\u4e00\u4e2a\u4e92\u52a8\u5b89\u88c5\u9879\u76ee\uff0c\u901a\u8fc7\u5168\u8eab\u8fd0\u52a8\u8ddf\u8e2a\u548c\u5b9e\u65f6AI\u53cd\u9988\u63a2\u7d22\u4eba\u7c7b\u60c5\u611f\uff0c\u91c7\u7528\u53c2\u4e0e\u8005\u9a71\u52a8\u7684\u60c5\u611f\u5b9a\u4e49\uff0c\u63a8\u52a8\u5305\u5bb9\u6027\u548c\u4f26\u7406\u7684\u60c5\u611f\u8ba1\u7b97\u3002", "motivation": "\u65e8\u5728\u7a81\u51fa\u4ee5\u53c2\u4e0e\u8005\u9a71\u52a8\u7684\u6587\u5316\u591a\u6837\u6027\u60c5\u611f\u5b9a\u4e49\uff0c\u63a8\u52a8\u5305\u5bb9\u548c\u4f26\u7406\u7684\u60c5\u611f\u8ba1\u7b97\u3002", "method": "\u901a\u8fc7\u5168\u8eab\u8fd0\u52a8\u8ddf\u8e2a\u548c\u5b9e\u65f6AI\u53cd\u9988\uff0c\u53c2\u4e0e\u8005\u5728\u4e09\u4e2a\u9636\u6bb5\uff08\u6559\u5b66\u3001\u63a2\u7d22\u548c\u5b87\u5b99\u9636\u6bb5\uff09\u4e2d\u4e0e\u7cfb\u7edf\u534f\u4f5c\u8868\u8fbe\u548c\u89e3\u8bfb\u60c5\u611f\u3002", "result": "Commonaiverse\u5b89\u88c5\u901a\u8fc7\u7ed3\u5408MoveNet\u548c\u591a\u63a8\u8350AI\u7cfb\u7edf\uff0c\u5b9e\u73b0\u5bf9\u60c5\u611f\u72b6\u6001\u7684\u52a8\u6001\u5206\u6790\u548c\u9002\u5e94\u6027\u89c6\u542c\u8f93\u51fa\u3002", "conclusion": "Commonaiverse\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u53c2\u4e0e\u8005\u4e3a\u4e2d\u5fc3\u7684\u60c5\u611f\u8ba1\u7b97\u65b0\u6a21\u5f0f\uff0c\u4fc3\u8fdb\u7528\u6237\u81ea\u4e3b\u6027\u548c\u591a\u6837\u6027\uff0c\u63a8\u52a8\u4e92\u52a8\u5e94\u7528\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.21723", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.21723", "abs": "https://arxiv.org/abs/2509.21723", "authors": ["Huayi Zhou", "Kui Jia"], "title": "VLBiMan: Vision-Language Anchored One-Shot Demonstration Enables Generalizable Robotic Bimanual Manipulation", "comment": "under review", "summary": "Achieving generalizable bimanual manipulation requires systems that can learn\nefficiently from minimal human input while adapting to real-world uncertainties\nand diverse embodiments. Existing approaches face a dilemma: imitation policy\nlearning demands extensive demonstrations to cover task variations, while\nmodular methods often lack flexibility in dynamic scenes. We introduce VLBiMan,\na framework that derives reusable skills from a single human example through\ntask-aware decomposition, preserving invariant primitives as anchors while\ndynamically adapting adjustable components via vision-language grounding. This\nadaptation mechanism resolves scene ambiguities caused by background changes,\nobject repositioning, or visual clutter without policy retraining, leveraging\nsemantic parsing and geometric feasibility constraints. Moreover, the system\ninherits human-like hybrid control capabilities, enabling mixed synchronous and\nasynchronous use of both arms. Extensive experiments validate VLBiMan across\ntool-use and multi-object tasks, demonstrating: (1) a drastic reduction in\ndemonstration requirements compared to imitation baselines, (2) compositional\ngeneralization through atomic skill splicing for long-horizon tasks, (3)\nrobustness to novel but semantically similar objects and external disturbances,\nand (4) strong cross-embodiment transfer, showing that skills learned from\nhuman demonstrations can be instantiated on different robotic platforms without\nretraining. By bridging human priors with vision-language anchored adaptation,\nour work takes a step toward practical and versatile dual-arm manipulation in\nunstructured settings.", "AI": {"tldr": "VLBiMan\u6846\u67b6\u901a\u8fc7\u4ece\u5355\u4e00\u793a\u4f8b\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00\u9002\u5e94\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u53cc\u624b\u64cd\u63a7\uff0c\u5177\u6709\u5f88\u5f3a\u7684\u6cdb\u5316\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u53ef\u6cdb\u5316\u7684\u53cc\u624b\u64cd\u4f5c\uff0c\u7cfb\u7edf\u9700\u80fd\u901a\u8fc7\u6700\u5c0f\u7684\u4eba\u7c7b\u8f93\u5165\u6709\u6548\u5b66\u4e60\uff0c\u540c\u65f6\u9002\u5e94\u73b0\u5b9e\u4e16\u754c\u7684\u4e0d\u786e\u5b9a\u6027\u4e0e\u591a\u6837\u6027\u3002", "method": "\u5f15\u5165VLBiMan\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u5355\u4e2a\u793a\u4f8b\u4e2d\u63d0\u53d6\u53ef\u91cd\u7528\u6280\u80fd\uff0c\u5e76\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u8fdb\u884c\u52a8\u6001\u8c03\u6574\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVLBiMan\u663e\u8457\u51cf\u5c11\u4e86\u6f14\u793a\u8981\u6c42\uff0c\u5c55\u793a\u4e86\u590d\u6742\u6027\u7684\u7ec4\u6210\u6cdb\u5316\uff0c\u5177\u5907\u5bf9\u65b0\u5bf9\u8c61\u548c\u5916\u90e8\u5e72\u6270\u7684\u9c81\u68d2\u6027\uff0c\u4ee5\u53ca\u8de8\u5e73\u53f0\u6280\u80fd\u4f20\u9012\u7684\u80fd\u529b\u3002", "conclusion": "VLBiMan\u6846\u67b6\u901a\u8fc7\u4efb\u52a1\u611f\u77e5\u7684\u5206\u89e3\u4e0e\u89c6\u89c9-\u8bed\u8a00\u7ed3\u5408\u7684\u9002\u5e94\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u53cc\u624b\u64cd\u63a7\u5b66\u4e60\uff0c\u4e14\u5177\u5907\u8f83\u5f3a\u7684\u901a\u7528\u6027\u4e0e\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.22271", "categories": ["cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22271", "abs": "https://arxiv.org/abs/2509.22271", "authors": ["Felix Glawe", "Tim Schmeckel", "Philipp Brauner", "Martina Ziefle"], "title": "Human Autonomy and Sense of Agency in Human-Robot Interaction: A Systematic Literature Review", "comment": null, "summary": "Human autonomy and sense of agency are increasingly recognised as critical\nfor user well-being, motivation, and the ethical deployment of robots in\nhuman-robot interaction (HRI). Given the rapid development of artificial\nintelligence, robot capabilities and their potential to function as colleagues\nand companions are growing. This systematic literature review synthesises 22\nempirical studies selected from an initial pool of 728 articles published\nbetween 2011 and 2024. Articles were retrieved from major scientific databases\nand identified based on empirical focus and conceptual relevance, namely, how\nto preserve and promote human autonomy and sense of agency in HRI. Derived\nthrough thematic synthesis, five clusters of potentially influential factors\nare revealed: robot adaptiveness, communication style, anthropomorphism,\npresence of a robot and individual differences. Measured through psychometric\nscales or the intentional binding paradigm, perceptions of autonomy and agency\nvaried across industrial, educational, healthcare, care, and hospitality\nsettings. The review underscores the theoretical differences between both\nconcepts, but their yet entangled use in HRI. Despite increasing interest, the\ncurrent body of empirical evidence remains limited and fragmented, underscoring\nthe necessity for standardised definitions, more robust operationalisations,\nand further exploratory and qualitative research. By identifying existing gaps\nand highlighting emerging trends, this review contributes to the development of\nhuman-centered, autonomy-supportive robot design strategies that uphold ethical\nand psychological principles, ultimately supporting well-being in human-robot\ninteraction.", "AI": {"tldr": "\u672c\u7efc\u8ff0\u63a2\u8ba8\u4e86\u4eba\u7c7b\u81ea\u4e3b\u6027\u548c\u4ee3\u7406\u611f\u5728\u673a\u5668\u4eba\u4ea4\u4e92\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5206\u6790\u4e86\u76f8\u5173\u7814\u7a76\u5e76\u63d0\u51fa\u672a\u6765\u53d1\u5c55\u7684\u5efa\u8bae\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u673a\u5668\u4eba\u5728\u4f5c\u4e3a\u540c\u4e8b\u548c\u4f34\u4fa3\u65b9\u9762\u7684\u80fd\u529b\u65e5\u76ca\u589e\u5f3a\uff0c\u56e0\u6b64\u9700\u8981\u5173\u6ce8\u7528\u6237\u7684\u81ea\u4e3b\u6027\u548c\u4ee3\u7406\u611f\uff0c\u4ee5\u786e\u4fdd\u4eba\u673a\u4ea4\u4e92\u7684\u9053\u5fb7\u6027\u548c\u6548\u76ca\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff0c\u5206\u6790\u4ece728\u7bc7\u6587\u732e\u4e2d\u7b5b\u9009\u51fa\u768422\u7bc7\u5b9e\u8bc1\u7814\u7a76\uff0c\u63a2\u8ba8\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u5982\u4f55\u7ef4\u62a4\u548c\u4fc3\u8fdb\u4eba\u7c7b\u81ea\u4e3b\u6027\u548c\u4ee3\u7406\u611f\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u4e94\u4e2a\u5f71\u54cd\u4eba\u7c7b\u81ea\u4e3b\u6027\u548c\u4ee3\u7406\u611f\u7684\u91cd\u8981\u56e0\u7d20\uff1a\u673a\u5668\u4eba\u9002\u5e94\u6027\u3001\u6c9f\u901a\u98ce\u683c\u3001\u4eba\u6027\u5316\u3001\u673a\u5668\u4eba\u5b58\u5728\u611f\u548c\u4e2a\u4f53\u5dee\u5f02\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u76ee\u524d\u5b9e\u8bc1\u7814\u7a76\u7684\u5c40\u9650\u548c\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u5f3a\u8c03\u4eba\u7c7b\u81ea\u4e3b\u6027\u548c\u4ee3\u7406\u611f\u5728\u673a\u5668\u4eba\u4ea4\u4e92\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u7814\u7a76\u9700\u8981\u66f4\u7cfb\u7edf\u7684\u5b9a\u4e49\u548c\u64cd\u4f5c\u65b9\u6cd5\uff0c\u4ee5\u4fc3\u8fdb\u4eba\u672c\u8bbe\u8ba1\u7684\u673a\u5668\u4eba\u5f00\u53d1\u3002"}}
{"id": "2509.21776", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.21776", "abs": "https://arxiv.org/abs/2509.21776", "authors": ["Hyeonseong Kim", "Roy El-Helou", "Seungbeen Lee", "Sungjoon Choi", "Matthew Pan"], "title": "The Turkish Ice Cream Robot: Examining Playful Deception in Social Human-Robot Interactions", "comment": "for more videos, see\n  https://hyeonseong-kim98.github.io/turkish-ice-cream-robot/", "summary": "Playful deception, a common feature in human social interactions, remains\nunderexplored in Human-Robot Interaction (HRI). Inspired by the Turkish Ice\nCream (TIC) vendor routine, we investigate how bounded, culturally familiar\nforms of deception influence user trust, enjoyment, and engagement during\nrobotic handovers. We design a robotic manipulator equipped with a custom\nend-effector and implement five TIC-inspired trick policies that deceptively\ndelay the handover of an ice cream-shaped object. Through a mixed-design user\nstudy with 91 participants, we evaluate the effects of playful deception and\ninteraction duration on user experience. Results reveal that TIC-inspired\ndeception significantly enhances enjoyment and engagement, though reduces\nperceived safety and trust, suggesting a structured trade-off across the\nmulti-dimensional aspects. Our findings demonstrate that playful deception can\nbe a valuable design strategy for interactive robots in entertainment and\nengagement-focused contexts, while underscoring the importance of deliberate\nconsideration of its complex trade-offs. You can find more information,\nincluding demonstration videos, on\nhttps://hyeonseong-kim98.github.io/turkish-ice-cream-robot/ .", "AI": {"tldr": "\u672c\u7814\u7a76\u8c03\u67e5\u4e86\u53d7\u571f\u8033\u5176\u51b0\u6dc7\u6dcb\u5c0f\u8d29\u542f\u53d1\u7684\u987d\u76ae\u6b3a\u9a97\u5bf9\u7528\u6237\u4fe1\u4efb\u3001\u4eab\u53d7\u548c\u53c2\u4e0e\u5ea6\u7684\u5f71\u54cd\uff0c\u7ed3\u679c\u663e\u793a\u8fd9\u79cd\u6b3a\u9a97\u7b56\u7565\u53ef\u4ee5\u63d0\u5347\u7528\u6237\u4f53\u9a8c\uff0c\u4f46\u4e5f\u5e26\u6765\u4e86\u5b89\u5168\u6027\u548c\u4fe1\u4efb\u7684\u964d\u4f4e\u3002", "motivation": "\u63a2\u8ba8\u4eba\u7c7b\u793e\u4f1a\u4e92\u52a8\u4e2d\u7684\u987d\u76ae\u6b3a\u9a97\u73b0\u8c61\u5728\u673a\u5668\u4eba\u4ea4\u4e92(HRI)\u4e2d\u7684\u5e94\u7528\u548c\u5f71\u54cd\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5e26\u6709\u5b9a\u5236\u672b\u7aef\u6267\u884c\u5668\u7684\u673a\u5668\u4eba\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e94\u79cd\u53d7\u571f\u8033\u5176\u51b0\u6dc7\u6dcb\u5c0f\u8d29\u542f\u53d1\u7684\u6b3a\u9a97\u7b56\u7565\uff0c\u8fd9\u4e9b\u7b56\u7565\u5de7\u5999\u5730\u5ef6\u8fdf\u4e86\u51b0\u6dc7\u6dcb\u5f62\u72b6\u7269\u4f53\u7684\u4ea4\u63a5\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u53d7\u571f\u8033\u5176\u51b0\u6dc7\u6dcb\u542f\u53d1\u7684\u6b3a\u9a97\u663e\u8457\u589e\u5f3a\u4e86\u7528\u6237\u7684\u4eab\u53d7\u548c\u53c2\u4e0e\u5ea6\uff0c\u4f46\u964d\u4f4e\u4e86\u611f\u77e5\u5b89\u5168\u6027\u548c\u4fe1\u4efb\u611f\uff0c\u8868\u660e\u5728\u591a\u7ef4\u65b9\u9762\u5b58\u5728\u7ed3\u6784\u4e0a\u7684\u6743\u8861\u3002", "conclusion": "\u987d\u76ae\u7684\u6b3a\u9a97\u53ef\u4ee5\u4f5c\u4e3a\u4e92\u52a8\u673a\u5668\u4eba\u8bbe\u8ba1\u4e2d\u7684\u4e00\u79cd\u6709\u4ef7\u503c\u7b56\u7565\uff0c\u5c24\u5176\u662f\u5728\u5a31\u4e50\u548c\u53c2\u4e0e\u5ea6\u9ad8\u7684\u80cc\u666f\u4e0b\uff0c\u4f46\u9700\u8c28\u614e\u8003\u8651\u5176\u590d\u6742\u7684\u6743\u8861\u3002"}}
{"id": "2509.22298", "categories": ["cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22298", "abs": "https://arxiv.org/abs/2509.22298", "authors": ["Felix Glawe", "Laura Kremer", "Luisa Vervier", "Philipp Brauner", "Martina Ziefle"], "title": "Trust and Human Autonomy after Cobot Failures: Communication is Key for Industry 5.0", "comment": null, "summary": "Collaborative robots (cobots) are a core technology of Industry 4.0. Industry\n4.0 uses cyber-physical systems, IoT and smart automation to improve efficiency\nand data-driven decision-making. Cobots, as cyber-physical systems, enable the\nintroduction of lightweight automation to smaller companies through their\nflexibility, low cost and ability to work alongside humans, while keeping\nhumans and their skills in the loop. Industry 5.0, the evolution of Industry\n4.0, places the worker at the centre of its principles: The physical and mental\nwell-being of the worker is the main goal of new technology design, not just\nproductivity, efficiency and safety standards. Within this concept, human trust\nin cobots and human autonomy are important. While trust is essential for\neffective and smooth interaction, the workers' perception of autonomy is key to\nintrinsic motivation and overall well-being. As failures are an inevitable part\nof technological systems, this study aims to answer the question of how system\nfailures affect trust in cobots as well as human autonomy, and how they can be\nrecovered afterwards. Therefore, a VR experiment (n = 39) was set up to\ninvestigate the influence of a cobot failure and its severity on human autonomy\nand trust in the cobot. Furthermore, the influence of transparent communication\nabout the failure and next steps was investigated. The results show that both\ntrust and autonomy suffer after cobot failures, with the severity of the\nfailure having a stronger negative impact on trust, but not on autonomy. Both\ntrust and autonomy can be partially restored by transparent communication.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u534f\u4f5c\u673a\u5668\u4eba\u6545\u969c\u5bf9\u4eba\u7c7b\u4fe1\u4efb\u548c\u81ea\u4e3b\u6027\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u6545\u969c\u4e25\u91cd\u7a0b\u5ea6\u5f71\u54cd\u4fe1\u4efb\uff0c\u900f\u660e\u6c9f\u901a\u80fd\u90e8\u5206\u6062\u590d\u4e24\u8005\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u7cfb\u7edf\u6545\u969c\u5982\u4f55\u5f71\u54cd\u4eba\u7c7b\u5bf9\u534f\u4f5c\u673a\u5668\u4eba\u7684\u4fe1\u4efb\u548c\u81ea\u4e3b\u6027\uff0c\u4ee5\u53ca\u5982\u4f55\u6062\u590d\u8fd9\u79cd\u4fe1\u4efb\u548c\u81ea\u4e3b\u6027\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u56e0\u7d20\u5728\u5de5\u4e1a5.0\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7VR\u5b9e\u9a8c\uff08\u53c2\u4e0e\u800539\u4eba\uff09\u7814\u7a76\u534f\u4f5c\u673a\u5668\u4eba\u6545\u969c\u53ca\u5176\u4e25\u91cd\u7a0b\u5ea6\u5bf9\u4eba\u7c7b\u4fe1\u4efb\u548c\u81ea\u4e3b\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u8003\u5bdf\u900f\u660e\u6c9f\u901a\u7684\u4f5c\u7528\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u534f\u4f5c\u673a\u5668\u4eba\u6545\u969c\u4f1a\u4f7f\u4fe1\u4efb\u548c\u81ea\u4e3b\u6027\u4e0b\u964d\uff0c\u6545\u969c\u7684\u4e25\u91cd\u7a0b\u5ea6\u5bf9\u4fe1\u4efb\u5f71\u54cd\u8f83\u5927\uff0c\u4f46\u5bf9\u5237\u81ea\u4e3b\u6027\u5f71\u54cd\u8f83\u5c0f\u3002\u900f\u660e\u6c9f\u901a\u53ef\u4ee5\u5e2e\u52a9\u90e8\u5206\u6062\u590d\u4fe1\u4efb\u548c\u81ea\u4e3b\u6027\u3002", "conclusion": "\u5728\u534f\u4f5c\u673a\u5668\u4eba\u53d1\u751f\u6545\u969c\u540e\uff0c\u4fe1\u4efb\u548c\u81ea\u4e3b\u6027\u90fd\u4f1a\u53d7\u5230\u5f71\u54cd\uff0c\u5176\u4e2d\u6545\u969c\u7684\u4e25\u91cd\u7a0b\u5ea6\u5bf9\u4fe1\u4efb\u7684\u5f71\u54cd\u66f4\u4e3a\u663e\u8457\uff0c\u4f46\u5bf9\u81ea\u4e3b\u6027\u7684\u5f71\u54cd\u8f83\u5c0f\u3002\u900f\u660e\u7684\u6c9f\u901a\u53ef\u4ee5\u90e8\u5206\u6062\u590d\u4fe1\u4efb\u548c\u81ea\u4e3b\u6027\u3002"}}
{"id": "2509.21810", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.21810", "abs": "https://arxiv.org/abs/2509.21810", "authors": ["Ning Huang", "Zhentao Xie", "Qinchuan Li"], "title": "Learning Multi-Skill Legged Locomotion Using Conditional Adversarial Motion Priors", "comment": null, "summary": "Despite growing interest in developing legged robots that emulate biological\nlocomotion for agile navigation of complex environments, acquiring a diverse\nrepertoire of skills remains a fundamental challenge in robotics. Existing\nmethods can learn motion behaviors from expert data, but they often fail to\nacquire multiple locomotion skills through a single policy and lack smooth\nskill transitions. We propose a multi-skill learning framework based on\nConditional Adversarial Motion Priors (CAMP), with the aim of enabling\nquadruped robots to efficiently acquire a diverse set of locomotion skills from\nexpert demonstrations. Precise skill reconstruction is achieved through a novel\nskill discriminator and skill-conditioned reward design. The overall framework\nsupports the active control and reuse of multiple skills, providing a practical\nsolution for learning generalizable policies in complex environments.", "AI": {"tldr": "\u7814\u53d1\u51fa\u4e00\u79cd\u591a\u6280\u80fd\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u5347\u56db\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u6280\u80fd\u83b7\u53d6\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u5bf9\u5f00\u53d1\u6a21\u4eff\u751f\u7269\u6b65\u6001\u7684\u56db\u8db3\u673a\u5668\u4eba\u4ee5\u5e94\u5bf9\u590d\u6742\u73af\u5883\u7684\u5174\u8da3\u65e5\u589e\uff0c\u4f46\u5728\u5355\u4e00\u7b56\u7565\u4e0b\u83b7\u53d6\u591a\u79cd\u8fd0\u52a8\u6280\u80fd\u4ecd\u7136\u662f\u4e00\u4e2a\u6839\u672c\u6311\u6218\u3002", "method": "\u91c7\u7528\u6761\u4ef6\u5bf9\u6297\u8fd0\u52a8\u5148\u9a8c\uff08CAMP\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u6280\u80fd\u9274\u522b\u5668\u548c\u6761\u4ef6\u5956\u52b1\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u7cbe\u51c6\u7684\u6280\u80fd\u91cd\u5efa\u3002", "result": "\u901a\u8fc7\u8be5\u6846\u67b6\uff0c\u56db\u8db3\u673a\u5668\u4eba\u80fd\u591f\u9ad8\u6548\u83b7\u5f97\u591a\u6837\u5316\u7684\u8fd0\u52a8\u6280\u80fd\uff0c\u5e76\u652f\u6301\u6280\u80fd\u7684\u4e3b\u52a8\u63a7\u5236\u4e0e\u91cd\u7528\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u5bf9\u6297\u8fd0\u52a8\u5148\u9a8c\u7684\u591a\u6280\u80fd\u5b66\u4e60\u6846\u67b6\uff0c\u4ee5\u652f\u6301\u56db\u8db3\u673a\u5668\u4eba\u4ece\u4e13\u5bb6\u6f14\u793a\u4e2d\u6709\u6548\u5730\u83b7\u53d6\u591a\u6837\u7684\u8fd0\u52a8\u6280\u80fd\u3002"}}
{"id": "2509.22443", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2509.22443", "abs": "https://arxiv.org/abs/2509.22443", "authors": ["Neal Reeves", "Elena Simperl"], "title": "Machines in the Margins: A Systematic Review of Automated Content Generation for Wikipedia", "comment": "30 pages, 1 figure. To be published in Proceedings of the ACM on\n  Human-Computer Interaction (issue 7, 2025)", "summary": "Wikipedia is among the largest examples of collective intelligence on the Web\nwith over 61 million articles covering over 320 languages. Although edited and\nmaintained by an active workforce of human volunteers, Wikipedia is highly\nreliant on automated bots to fill gaps in its human workforce. As well as\nadministrative and governance tasks, these bots also play a role in generating\ncontent, although to date such agents represent the smallest proportion of\nbots. While there has been considerable analysis of bots and their activity in\nWikipedia, such work captures only automated agents that have been actively\ndeployed to Wikipedia and fails to capture the methods that have been proposed\nto generate Wikipedia content in the wider literature. In this paper, we\nconduct a systematic literature review to explore how researchers have\noperationalised and evaluated automated content-generation agents for\nWikipedia. We identify the scope of these generation methods, the techniques\nand models used, the source content used for generation and the evaluation\nmethodologies which support generation processes. We also explore implications\nof our findings to CSCW, User Generated Content and Wikipedia, as well as\nresearch directions for future development. To the best of our knowledge, we\nare among the first to review the potential contributions of this understudied\nform of AI support for the Wikipedia community beyond the implementation of\nbots.", "AI": {"tldr": "\u672c\u7814\u7a76\u56de\u987e\u4e86\u6587\u732e\u4e2d\u5173\u4e8e\u81ea\u52a8\u751f\u6210\u7ef4\u57fa\u767e\u79d1\u5185\u5bb9\u7684\u65b9\u6cd5\u548c\u8bc4\u4f30\uff0c\u63a2\u8ba8\u4e86\u5176\u5bf9\u7528\u6237\u751f\u6210\u5185\u5bb9\u548c\u7ef4\u57fa\u767e\u79d1\u7684\u5f71\u54cd\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u6709\u7814\u7a76\u96c6\u4e2d\u5728\u5df2\u90e8\u7f72\u7684\u7ef4\u57fa\u767e\u79d1\u81ea\u52a8\u4ee3\u7406\u4e0a\uff0c\u4f46\u672c\u6587\u65e8\u5728\u586b\u8865\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684AI\u652f\u6301\u5bf9\u7ef4\u57fa\u767e\u79d1\u793e\u533a\u8d21\u732e\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u56de\u987e\uff0c\u5206\u6790\u81ea\u52a8\u5185\u5bb9\u751f\u6210\u4ee3\u7406\u5728\u7ef4\u57fa\u767e\u79d1\u4e2d\u7684\u5b9e\u73b0\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u8bc6\u522b\u4e86\u5185\u5bb9\u751f\u6210\u65b9\u6cd5\u7684\u8303\u56f4\u3001\u6240\u7528\u6280\u672f\u548c\u6a21\u578b\u3001\u751f\u6210\u6240\u7528\u7684\u6e90\u5185\u5bb9\u4ee5\u53ca\u652f\u6301\u751f\u6210\u8fc7\u7a0b\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "conclusion": "\u672c\u7814\u7a76\u7cfb\u7edf\u6027\u5730\u56de\u987e\u4e86\u6587\u732e\u4e2d\u6709\u5173\u81ea\u52a8\u5185\u5bb9\u751f\u6210\u4ee3\u7406\u5bf9\u7ef4\u57fa\u767e\u79d1\u7684\u8d21\u732e\uff0c\u6307\u51fa\u4e86\u65b9\u6cd5\u3001\u6280\u672f\u3001\u6e90\u5185\u5bb9\u548c\u8bc4\u4ef7\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2509.21873", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.21873", "abs": "https://arxiv.org/abs/2509.21873", "authors": ["Nishant Doshi"], "title": "Improved Vehicle Maneuver Prediction using Game Theoretic Priors", "comment": null, "summary": "Conventional maneuver prediction methods use some sort of classification\nmodel on temporal trajectory data to predict behavior of agents over a set time\nhorizon. Despite of having the best precision and recall, these models cannot\npredict a lane change accurately unless they incorporate information about the\nentire scene. Level-k game theory can leverage the human-like hierarchical\nreasoning to come up with the most rational decisions each agent can make in a\ngroup. This can be leveraged to model interactions between different vehicles\nin presence of each other and hence compute the most rational decisions each\nagent would make. The result of game theoretic evaluation can be used as a\n\"prior\" or combined with a traditional motion-based classification model to\nachieve more accurate predictions. The proposed approach assumes that the\nstates of the vehicles around the target lead vehicle are known. The module\nwill output the most rational maneuver prediction of the target vehicle based\non an online optimization solution. These predictions are instrumental in\ndecision making systems like Adaptive Cruise Control (ACC) or Traxen's\niQ-Cruise further improving the resulting fuel savings.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u535a\u5f08\u8bba\u4e0e\u4f20\u7edf\u6a21\u578b\u7684\u65b9\u5f0f\uff0c\u4ee5\u66f4\u7cbe\u51c6\u5730\u9884\u6d4b\u8f66\u8f86\u884c\u4e3a\uff0c\u6539\u5584\u81ea\u9002\u5e94\u5de1\u822a\u7cfb\u7edf\u6027\u80fd\u548c\u71c3\u6cb9\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u5206\u7c7b\u6a21\u578b\u5728\u9884\u6d4b\u8f66\u9053\u6539\u53d8\u65f6\u7684\u51c6\u786e\u6027\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u5bf9\u5168\u5c40\u573a\u666f\u7684\u7406\u89e3\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u5229\u7528\u535a\u5f08\u8bba\u7684\u5c42\u7ea7\u63a8\u7406\u6a21\u578b\uff0c\u5206\u6790\u4e0d\u540c\u8f66\u8f86\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u4ee5\u751f\u6210\u6700\u7406\u6027\u51b3\u7b56\u3002", "result": "\u7ed3\u5408\u535a\u5f08\u8bba\u4e0e\u4f20\u7edf\u8fd0\u52a8\u5206\u7c7b\u6a21\u578b\uff0c\u53ef\u4ee5\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u64cd\u63a7\u9884\u6d4b\uff0c\u63d0\u5347\u5982\u81ea\u9002\u5e94\u5de1\u822a\u63a7\u5236\u7cfb\u7edf\u7684\u6548\u679c\uff0c\u5b9e\u73b0\u66f4\u9ad8\u7684\u71c3\u6cb9\u8282\u7701\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u535a\u5f08\u8bba\u548c\u4f20\u7edf\u5206\u7c7b\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86\u5bf9\u76ee\u6807\u8f66\u8f86\u6700\u7406\u6027\u884c\u4e3a\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u8fdb\u800c\u4f18\u5316\u4e86\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2509.22505", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.22505", "abs": "https://arxiv.org/abs/2509.22505", "authors": ["Yunhao Yuan", "Jiaxun Zhang", "Talayeh Aledavood", "Renwen Zhang", "Koustuv Saha"], "title": "Mental Health Impacts of AI Companions: Triangulating Social Media Quasi-Experiments, User Perspectives, and Relational Theory", "comment": null, "summary": "AI-powered companion chatbots (AICCs) such as Replika are increasingly\npopular, offering empathetic interactions, yet their psychosocial impacts\nremain unclear. We examined how engaging with AICCs shaped wellbeing and how\nusers perceived these experiences. First, we conducted a large-scale\nquasi-experimental study of longitudinal Reddit data, applying stratified\npropensity score matching and Difference-in-Differences regression. Findings\nrevealed mixed effects -- greater affective and grief expression, readability,\nand interpersonal focus, alongside increases in language about loneliness and\nsuicidal ideation. Second, we complemented these results with 15\nsemi-structured interviews, which we thematically analyzed and contextualized\nusing Knapp's relationship development model. We identified trajectories of\ninitiation, escalation, and bonding, wherein AICCs provided emotional\nvalidation and social rehearsal but also carried risks of over-reliance and\nwithdrawal. Triangulating across methods, we offer design implications for AI\ncompanions that scaffold healthy boundaries, support mindful engagement,\nsupport disclosure without dependency, and surface relationship stages --\nmaximizing psychosocial benefits while mitigating risks.", "AI": {"tldr": "AI\u804a\u5929\u673a\u5668\u4eba\u63d0\u4f9b\u60c5\u611f\u652f\u6301\uff0c\u4f46\u4e5f\u5f15\u53d1\u5b64\u72ec\u548c\u4f9d\u8d56\u98ce\u9669\uff0c\u9700\u8981\u8bbe\u8ba1\u5065\u5eb7\u7684\u4e92\u52a8\u6a21\u5f0f\u3002", "motivation": "\u63a2\u8ba8AI\u966a\u4f34\u804a\u5929\u673a\u5668\u4eba\u5bf9\u7528\u6237\u5fc3\u7406\u793e\u4f1a\u5f71\u54cd\u53ca\u7528\u6237\u7684\u4f53\u9a8c\u8ba4\u77e5\u3002", "method": "\u7ed3\u5408\u5927\u89c4\u6a21\u51c6\u5b9e\u9a8c\u7814\u7a76\u548c15\u6b21\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\uff0c\u603b\u7ed3AI\u966a\u4f34\u804a\u5929\u673a\u5668\u4eba\u7684\u4f7f\u7528\u5f71\u54cd\u3002", "result": "\u7528\u6237\u5728\u4e0eAI\u804a\u5929\u673a\u5668\u4eba\u7684\u4e92\u52a8\u4e2d\u7ecf\u5386\u4e86\u60c5\u611f\u9a8c\u8bc1\u548c\u793e\u4ea4\u7ec3\u4e60\uff0c\u4f46\u4e5f\u51fa\u73b0\u4e86\u5b64\u72ec\u548c\u81ea\u6740\u610f\u5ff5\u8bed\u8a00\u7684\u589e\u52a0\u3002", "conclusion": "AICCs\u5728\u63d0\u4f9b\u60c5\u611f\u652f\u6301\u7684\u540c\u65f6\uff0c\u53ef\u80fd\u5bfc\u81f4\u7528\u6237\u8fc7\u5ea6\u4f9d\u8d56\u548c\u60c5\u611f\u5b64\u7acb\uff0c\u56e0\u6b64\u8bbe\u8ba1\u5e94\u5173\u6ce8\u5065\u5eb7\u754c\u9650\u548c\u652f\u6301\u6027\u4e92\u52a8\u3002"}}
{"id": "2509.21878", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.21878", "abs": "https://arxiv.org/abs/2509.21878", "authors": ["Moses Gladson Selvamuthu", "Tomoya Takahashi", "Riichiro Tadakuma", "Kazutoshi Tanaka"], "title": "WAVE: Worm Gear-based Adaptive Variable Elasticity for Decoupling Actuators from External Forces", "comment": null, "summary": "Robotic manipulators capable of regulating both compliance and stiffness\noffer enhanced operational safety and versatility. Here, we introduce Worm\nGear-based Adaptive Variable Elasticity (WAVE), a variable stiffness actuator\n(VSA) that integrates a non-backdrivable worm gear. By decoupling the driving\nmotor from external forces using this gear, WAVE enables precise force\ntransmission to the joint, while absorbing positional discrepancies through\ncompliance. WAVE is protected from excessive loads by converting impact forces\ninto elastic energy stored in a spring. In addition, the actuator achieves\ncontinuous joint stiffness modulation by changing the spring's precompression\nlength. We demonstrate these capabilities, experimentally validate the proposed\nstiffness model, show that motor loads approach zero at rest--even under\nexternal loading--and present applications using a manipulator with WAVE. This\noutcome showcases the successful decoupling of external forces. The protective\nattributes of this actuator allow for extended operation in contact-intensive\ntasks, and for robust robotic applications in challenging environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u53d8\u521a\u5ea6\u9a71\u52a8\u5668WAVE\uff0c\u5229\u7528\u8717\u6746\u9f7f\u8f6e\u89e3\u8026\u5916\u529b\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u529b\u4f20\u9012\u548c\u52a8\u6001\u521a\u5ea6\u8c03\u8282\uff0c\u9002\u7528\u4e8e\u9ad8\u63a5\u89e6\u7387\u548c\u590d\u6742\u73af\u5883\u4e0b\u7684\u673a\u5668\u4eba\u5e94\u7528\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u8c03\u8282\u521a\u5ea6\u548c\u67d4\u987a\u6027\u7684\u673a\u5668\u4eba\u4f20\u52a8\u7cfb\u7edf\uff0c\u4ee5\u63d0\u5347\u5b89\u5168\u6027\u548c\u5e94\u7528\u7075\u6d3b\u6027\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u8717\u6746\u7684\u81ea\u9002\u5e94\u53ef\u53d8\u5f39\u6027\uff08WAVE\uff09\u9a71\u52a8\u5668\uff0c\u8be5\u9a71\u52a8\u5668\u96c6\u6210\u4e86\u4e0d\u53ef\u56de\u9a71\u52a8\u7684\u8717\u6746\u9f7f\u8f6e\uff0c\u901a\u8fc7\u8c03\u8282\u5f39\u7c27\u7684\u9884\u538b\u7f29\u957f\u5ea6\u5b9e\u73b0\u5173\u8282\u521a\u5ea6\u7684\u8fde\u7eed\u8c03\u8282\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eWAVE\u9a71\u52a8\u5668\u5728\u9759\u6b62\u72b6\u6001\u4e0b\u7535\u673a\u8d1f\u8f7d\u63a5\u8fd1\u96f6\uff0c\u5373\u4fbf\u5728\u5916\u90e8\u8d1f\u8f7d\u4e0b\u4e5f\u80fd\u826f\u597d\u5de5\u4f5c\uff0c\u9a8c\u8bc1\u4e86\u67d4\u987a\u6027\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660eWAVE\u80fd\u591f\u6210\u529f\u89e3\u8026\u5916\u90e8\u529b\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u529b\u4f20\u9012\u548c\u52a8\u6001\u7684\u5173\u8282\u521a\u5ea6\u8c03\u8282\uff0c\u9002\u5408\u4e8e\u9ad8\u63a5\u89e6\u7387\u5e94\u7528\u53ca\u4e25\u82db\u73af\u5883\u4e0b\u7684\u673a\u5668\u4eba\u4f5c\u4e1a\u3002"}}
{"id": "2509.22545", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.22545", "abs": "https://arxiv.org/abs/2509.22545", "authors": ["Veda Duddu", "Jash Rajesh Parekh", "Andy Mao", "Hanyi Min", "Ziang Xiao", "Vedant Das Swain", "Koustuv Saha"], "title": "Does AI Coaching Prepare us for Workplace Negotiations?", "comment": null, "summary": "Workplace negotiations are undermined by psychological barriers, which can\neven derail well-prepared tactics. AI offers personalized and always --\navailable negotiation coaching, yet its effectiveness for negotiation\npreparedness remains unclear. We built Trucey, a prototype AI coach grounded in\nBrett's negotiation model. We conducted a between-subjects experiment (N=267),\ncomparing Trucey, ChatGPT, and a traditional negotiation Handbook, followed by\nin-depth interviews (N=15). While Trucey showed the strongest reductions in\nfear relative to both comparison conditions, the Handbook outperformed both AIs\nin usability and psychological empowerment. Interviews revealed that the\nHandbook's comprehensive, reviewable content was crucial for participants'\nconfidence and preparedness. In contrast, although participants valued AI's\nrehearsal capability, its guidance often felt verbose and fragmented --\ndelivered in bits and pieces that required additional effort -- leaving them\nuncertain or overwhelmed. These findings challenge assumptions of AI\nsuperiority and motivate hybrid designs that integrate structured,\ntheory-driven content with targeted rehearsal, clear boundaries, and adaptive\nscaffolds to address psychological barriers and support negotiation\npreparedness.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86AI\u6559\u7ec3\u4e0e\u4f20\u7edf\u624b\u518c\u5728\u8c08\u5224\u51c6\u5907\u4e2d\u7684\u6548\u679c\uff0c\u53d1\u73b0\u4f20\u7edf\u624b\u518c\u5728\u53ef\u7528\u6027\u4e0a\u66f4\u4f18\uff0c\u800cAI\u5728\u51cf\u8f7b\u6050\u60e7\u65b9\u9762\u8868\u73b0\u66f4\u4f73\uff0c\u63d0\u793a\u9700\u8981\u66f4\u6709\u6548\u7684\u6df7\u5408\u8bbe\u8ba1\u3002", "motivation": "\u63a2\u8ba8AI\u5bf9\u63d0\u9ad8\u8c08\u5224\u51c6\u5907\u6027\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u5fc3\u7406\u969c\u788d\u7684\u4f5c\u7528\u3002", "method": "\u8fdb\u884c\u4e86\u4e00\u9879267\u4eba\u53c2\u4e0e\u7684\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e86AI\u6559\u7ec3Trucey\u3001ChatGPT\u548c\u4f20\u7edf\u8c08\u5224\u624b\u518c\u7684\u6548\u679c\uff0c\u5e76\u8fdb\u884c\u4e8615\u4eba\u7684\u6df1\u5ea6\u8bbf\u8c08\u3002", "result": "Trucey\u6709\u6548\u51cf\u8f7b\u6050\u60e7\u611f\uff0c\u4f46\u4f20\u7edf\u624b\u518c\u5728\u53ef\u7528\u6027\u548c\u8d4b\u6743\u65b9\u9762\u66f4\u4f73\uff0c\u4e14AI\u7684\u6307\u5bfc\u5f80\u5f80\u663e\u5f97\u5197\u957f\u4e0e\u7247\u6bb5\u5316\uff0c\u4ee4\u53c2\u4e0e\u8005\u611f\u5230\u4e0d\u5b89\u6216\u4e0d\u77e5\u6240\u63aa\u3002", "conclusion": "Trucey\u5728\u51cf\u8f7b\u6050\u60e7\u65b9\u9762\u6548\u679c\u6700\u4f73\uff0c\u4f46\u4f20\u7edf\u624b\u518c\u5728\u53ef\u7528\u6027\u548c\u5fc3\u7406\u8d4b\u6743\u4e0a\u8868\u73b0\u66f4\u4f18\u3002\u7814\u7a76\u8868\u660e\uff0cAI\u5e76\u4e0d\u4e00\u5b9a\u603b\u662f\u4f18\u4e8e\u4f20\u7edf\u65b9\u5f0f\uff0c\u5f3a\u8c03\u4e86\u7ed3\u5408\u7406\u8bba\u9a71\u52a8\u5185\u5bb9\u4e0eAI\u8f85\u5bfc\u7684\u6df7\u5408\u8bbe\u8ba1\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2509.21928", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21928", "abs": "https://arxiv.org/abs/2509.21928", "authors": ["Jialiang Li", "Wenzheng Wu", "Gaojing Zhang", "Yifan Han", "Wenzhao Lian"], "title": "SAGE: Scene Graph-Aware Guidance and Execution for Long-Horizon Manipulation Tasks", "comment": null, "summary": "Successfully solving long-horizon manipulation tasks remains a fundamental\nchallenge. These tasks involve extended action sequences and complex object\ninteractions, presenting a critical gap between high-level symbolic planning\nand low-level continuous control. To bridge this gap, two essential\ncapabilities are required: robust long-horizon task planning and effective\ngoal-conditioned manipulation. Existing task planning methods, including\ntraditional and LLM-based approaches, often exhibit limited generalization or\nsparse semantic reasoning. Meanwhile, image-conditioned control methods\nstruggle to adapt to unseen tasks. To tackle these problems, we propose SAGE, a\nnovel framework for Scene Graph-Aware Guidance and Execution in Long-Horizon\nManipulation Tasks. SAGE utilizes semantic scene graphs as a structural\nrepresentation for scene states. A structural scene graph enables bridging\ntask-level semantic reasoning and pixel-level visuo-motor control. This also\nfacilitates the controllable synthesis of accurate, novel sub-goal images. SAGE\nconsists of two key components: (1) a scene graph-based task planner that uses\nVLMs and LLMs to parse the environment and reason about physically-grounded\nscene state transition sequences, and (2) a decoupled structural image editing\npipeline that controllably converts each target sub-goal graph into a\ncorresponding image through image inpainting and composition. Extensive\nexperiments have demonstrated that SAGE achieves state-of-the-art performance\non distinct long-horizon tasks.", "AI": {"tldr": "SAGE\u6846\u67b6\u901a\u8fc7\u8bed\u4e49\u60c5\u666f\u56fe\uff0c\u8fde\u63a5\u4e86\u4efb\u52a1\u89c4\u5212\u548c\u4f4e\u5c42\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u957f\u65f6\u95f4\u64cd\u63a7\u4efb\u52a1\u7684\u8fdb\u6b65\uff0c\u5c55\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u4efb\u52a1\u89c4\u5212\u548c\u56fe\u50cf\u63a7\u5236\u65b9\u6cd5\u5728\u5e7f\u6cdb\u9002\u5e94\u6027\u548c\u8bed\u4e49\u63a8\u7406\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u957f\u65f6\u95f4\u64cd\u63a7\u4efb\u52a1\u7684\u590d\u6742\u6027\u9762\u524d\u3002", "method": "SAGE\u6846\u67b6\u7ed3\u5408\u4e86\u57fa\u4e8e\u60c5\u666f\u56fe\u7684\u4efb\u52a1\u89c4\u5212\u548c\u89e3\u8026\u7ed3\u6784\u56fe\u50cf\u7f16\u8f91\u6d41\u7a0b\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89e3\u6790\u73af\u5883\u5e76\u751f\u6210\u76ee\u6807\u5b50\u76ee\u6807\u56fe\u50cf\u3002", "result": "\u901a\u8fc7\u6784\u5efa\u57fa\u4e8e\u8bed\u4e49\u60c5\u666f\u56fe\u7684\u4efb\u52a1\u89c4\u5212\u548c\u56fe\u50cf\u751f\u6210\uff0cSAGE\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u957f\u65f6\u95f4\u64cd\u63a7\u4efb\u52a1\u7684\u6267\u884c\u80fd\u529b\u3002", "conclusion": "SAGE\u6846\u67b6\u5728\u4e0d\u540c\u7684\u957f\u65f6\u95f4\u89c6\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u9ad8\u5c42\u8bed\u4e49\u89c4\u5212\u4e0e\u4f4e\u5c42\u63a7\u5236\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2509.21955", "categories": ["cs.RO", "cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.21955", "abs": "https://arxiv.org/abs/2509.21955", "authors": ["Divake Kumar", "Sina Tayebati", "Francesco Migliarba", "Ranganath Krishnan", "Amit Ranjan Trivedi"], "title": "Learnable Conformal Prediction with Context-Aware Nonconformity Functions for Robotic Planning and Perception", "comment": null, "summary": "Deep learning models in robotics often output point estimates with poorly\ncalibrated confidences, offering no native mechanism to quantify predictive\nreliability under novel, noisy, or out-of-distribution inputs. Conformal\nprediction (CP) addresses this gap by providing distribution-free coverage\nguarantees, yet its reliance on fixed nonconformity scores ignores context and\ncan yield intervals that are overly conservative or unsafe. We address this\nwith Learnable Conformal Prediction (LCP), which replaces fixed scores with a\nlightweight neural function that leverages geometric, semantic, and\ntask-specific features to produce context-aware uncertainty sets.\n  LCP maintains CP's theoretical guarantees while reducing prediction set sizes\nby 18% in classification, tightening detection intervals by 52%, and improving\npath planning safety from 72% to 91% success with minimal overhead. Across\nthree robotic tasks on seven benchmarks, LCP consistently outperforms Standard\nCP and ensemble baselines. In classification on CIFAR-100 and ImageNet, it\nachieves smaller set sizes (4.7-9.9% reduction) at target coverage. For object\ndetection on COCO, BDD100K, and Cityscapes, it produces 46-54% tighter bounding\nboxes. In path planning through cluttered environments, it improves success to\n91.5% with only 4.5% path inflation, compared to 12.2% for Standard CP.\n  The method is lightweight (approximately 4.8% runtime overhead, 42 KB memory)\nand supports online adaptation, making it well suited to resource-constrained\nautonomous systems. Hardware evaluation shows LCP adds less than 1% memory and\n15.9% inference overhead, yet sustains 39 FPS on detection tasks while being\n7.4 times more energy-efficient than ensembles.", "AI": {"tldr": "\u53ef\u5b66\u4e60\u7684\u975e\u987a\u4ece\u9884\u6d4b\uff08LCP\uff09\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u795e\u7ecf\u7f51\u7edc\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u9886\u57df\u4e2d\u9884\u6d4b\u7684\u53ef\u9760\u6027\uff0c\u51cf\u5c11\u4e86\u4e0d\u786e\u5b9a\u6027\u96c6\u5408\u7684\u5927\u5c0f\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u673a\u5668\u4eba\u9886\u57df\u4e2d\u5bf9\u8f93\u5165\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u9884\u6d4b\u53ef\u9760\u6027\u7684\u4e0d\u8db3\uff0c\u4ee5\u53ca\u73b0\u6709\u7684\u975e\u987a\u4ece\u9884\u6d4b\u7684\u4fdd\u5b88\u6027\u95ee\u9898\u3002", "method": "\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u975e\u987a\u4ece\u6027\u5206\u6570\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u5bf9\u4e0a\u4e0b\u6587\u4fe1\u606f\u8fdb\u884c\u5efa\u6a21\uff0c\u751f\u6210\u5177\u6709\u4e0a\u4e0b\u6587\u610f\u8bc6\u7684\u4e0d\u786e\u5b9a\u6027\u96c6\u5408\u3002", "result": "LCP\u5728\u5206\u7c7b\u4e2d\u5c06\u9884\u6d4b\u96c6\u51cf\u5c11\u4e8618%\uff0c\u68c0\u6d4b\u95f4\u9694\u6536\u7d27\u4e8652%\uff0c\u8def\u5f84\u89c4\u5212\u6210\u529f\u7387\u4ece72%\u63d0\u9ad8\u523091%\u3002\u5728CIFAR-100\u548cImageNet\u5206\u7c7b\u4e2d\uff0c\u96c6\u5408\u5927\u5c0f\u51cf\u5c114.7-9.9%\uff1b\u5728COCO\u3001BDD100K\u548cCityscapes\u76ee\u6807\u68c0\u6d4b\u4e2d\uff0c\u751f\u6210\u66f4\u7d27\u51d1\u7684\u8fb9\u754c\u6846\uff0c\u6536\u7f2946-54%\u3002", "conclusion": "LCP\u5728\u5206\u7c7b\u3001\u76ee\u6807\u68c0\u6d4b\u548c\u8def\u5f84\u89c4\u5212\u4efb\u52a1\u4e2d\u76f8\u8f83\u4e8e\u6807\u51c6CP\u548c\u96c6\u6210\u57fa\u7ebf\u8868\u73b0\u66f4\u4f73\uff0c\u4e14\u5728\u8d44\u6e90\u53d7\u9650\u7684\u81ea\u4e3b\u7cfb\u7edf\u4e2d\u5e94\u7528\u53ef\u9760\u3002"}}
{"id": "2509.22287", "categories": ["cs.RO", "cs.AI", "cs.HC", "I.2.7; H.5.2; K.3.1; J.4"], "pdf": "https://arxiv.org/pdf/2509.22287", "abs": "https://arxiv.org/abs/2509.22287", "authors": ["Stina Sundstedt", "Mattias Wingren", "Susanne H\u00e4gglund", "Daniel Ventus"], "title": "Leveraging Large Language Models for Robot-Assisted Learning of Morphological Structures in Preschool Children with Language Vulnerabilities", "comment": "12 pages, 2 figures, Preprint of: Sundstedt, S., Wingren, M.,\n  H\\\"agglund, S. & Ventus, D. (2025). Leveraging Large Language Models for\n  Robot-Assisted Learning of Morphological Structures in Preschool Children\n  with Language Vulnerabilities. In: Stephanidis, C., Antona, M., Ntoa, S. &\n  Salvendy, G. (eds.), Communications in Computer and Information Science, vol.\n  2523, pp. 415-425. Springer", "summary": "Preschool children with language vulnerabilities -- such as developmental\nlanguage disorders or immigration related language challenges -- often require\nsupport to strengthen their expressive language skills. Based on the principle\nof implicit learning, speech-language therapists (SLTs) typically embed target\nmorphological structures (e.g., third person -s) into everyday interactions or\ngame-based learning activities. Educators are recommended by SLTs to do the\nsame. This approach demands precise linguistic knowledge and real-time\nproduction of various morphological forms (e.g., \"Daddy wears these when he\ndrives to work\"). The task becomes even more demanding when educators or parent\nalso must keep children engaged and manage turn-taking in a game-based\nactivity. In the TalBot project our multiprofessional team have developed an\napplication in which the Furhat conversational robot plays the word retrieval\ngame \"Alias\" with children to improve language skills. Our application\ncurrently employs a large language model (LLM) to manage gameplay, dialogue,\naffective responses, and turn-taking. Our next step is to further leverage the\ncapacity of LLMs so the robot can generate and deliver specific morphological\ntargets during the game. We hypothesize that a robot could outperform humans at\nthis task. Novel aspects of this approach are that the robot could ultimately\nserve as a model and tutor for both children and professionals and that using\nLLM capabilities in this context would support basic communication needs for\nchildren with language vulnerabilities. Our long-term goal is to create a\nrobust LLM-based Robot-Assisted Language Learning intervention capable of\nteaching a variety of morphological structures across different languages.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u673a\u5668\u4eba\u8bed\u8a00\u5b66\u4e60\u5e94\u7528\uff0c\u65e8\u5728\u5e2e\u52a9\u6709\u8bed\u8a00\u56f0\u96be\u7684\u513f\u7ae5\u63d0\u9ad8\u8bed\u8a00\u8868\u8fbe\u80fd\u529b\uff0c\u6700\u7ec8\u76ee\u6807\u662f\u521b\u5efa\u591a\u8bed\u8a00\u7684\u8bed\u8a00\u5b66\u4e60\u5de5\u5177\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u4e3a\u5177\u6709\u8bed\u8a00\u8106\u5f31\u6027\u7684\u5b66\u524d\u513f\u7ae5\u63d0\u4f9b\u6709\u6548\u7684\u8868\u8fbe\u8bed\u8a00\u6280\u80fd\u652f\u6301\uff0c\u51cf\u8f7b\u6559\u5e08\u548c\u5bb6\u957f\u5728\u4fc3\u8fdb\u8bed\u8a00\u5b66\u4e60\u65f6\u7684\u8d1f\u62c5\u3002", "method": "\u901a\u8fc7TalBot\u9879\u76ee\uff0c\u4f7f\u7528Furhat\u5bf9\u8bdd\u673a\u5668\u4eba\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u8bed\u8a00\u6e38\u620f\u5e94\u7528\uff0c\u91cd\u70b9\u5728\u4e8e\u8bcd\u6c47\u68c0\u7d22\u548c\u5bf9\u8bdd\u7ba1\u7406\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u673a\u5668\u4eba\u8f85\u52a9\u7cfb\u7edf\u4e0d\u4ec5\u80fd\u591f\u7ba1\u7406\u6e38\u620f\u52a8\u6001\u548c\u5bf9\u8bdd\uff0c\u8fd8\u53ef\u80fd\u5728\u6559\u6388\u8bed\u8a00\u5f62\u6001\u7ed3\u6784\u65b9\u9762\u8d85\u8d8a\u4eba\u7c7b\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u673a\u5668\u4eba\u8f85\u52a9\u8bed\u8a00\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u5e2e\u52a9\u8bed\u8a00\u8106\u5f31\u513f\u7ae5\u901a\u8fc7\u6e38\u620f\u63d0\u9ad8\u8868\u8fbe\u80fd\u529b\uff0c\u6700\u7ec8\u76ee\u6807\u662f\u521b\u5efa\u4e00\u79cd\u80fd\u591f\u6559\u6388\u591a\u79cd\u8bed\u8a00\u7684\u5f62\u6001\u7ed3\u6784\u7684\u5e72\u9884\u5de5\u5177\u3002"}}
{"id": "2509.21961", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21961", "abs": "https://arxiv.org/abs/2509.21961", "authors": ["Lingguang Wang", "\u00d6mer \u015eahin Ta\u015f", "Marlon Steiner", "Christoph Stiller"], "title": "FlowDrive: moderated flow matching with data balancing for trajectory planning", "comment": null, "summary": "Learning-based planners are sensitive to the long-tailed distribution of\ndriving data. Common maneuvers dominate datasets, while dangerous or rare\nscenarios are sparse. This imbalance can bias models toward the frequent cases\nand degrade performance on critical scenarios. To tackle this problem, we\ncompare balancing strategies for sampling training data and find reweighting by\ntrajectory pattern an effective approach. We then present FlowDrive, a\nflow-matching trajectory planner that learns a conditional rectified flow to\nmap noise directly to trajectory distributions with few flow-matching steps. We\nfurther introduce moderated, in-the-loop guidance that injects small\nperturbation between flow steps to systematically increase trajectory diversity\nwhile remaining scene-consistent. On nuPlan and the interaction-focused\ninterPlan benchmarks, FlowDrive achieves state-of-the-art results among\nlearning-based planners and approaches methods with rule-based refinements.\nAfter adding moderated guidance and light post-processing (FlowDrive*), it\nachieves overall state-of-the-art performance across nearly all benchmark\nsplits.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFlowDrive\uff0c\u4e00\u79cd\u80fd\u6709\u6548\u5904\u7406\u9a7e\u9a76\u6570\u636e\u957f\u5c3e\u5206\u5e03\u95ee\u9898\u7684\u6d41\u5339\u914d\u8f68\u8ff9\u89c4\u5212\u5668\uff0c\u5176\u5728\u5404\u5927\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u9a7e\u9a76\u6570\u636e\u7684\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u5e38\u89c1\u64cd\u4f5c\u4e3b\u5bfc\u6570\u636e\u96c6\uff0c\u800c\u5371\u9669\u6216\u7a00\u6709\u573a\u666f\u7a00\u758f\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u5173\u952e\u573a\u666f\u4e0a\u7684\u8868\u73b0\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFlowDrive\u7684\u6d41\u5339\u914d\u8f68\u8ff9\u89c4\u5212\u5668\uff0c\u5b66\u4e60\u6761\u4ef6\u4fee\u6b63\u6d41\u4ee5\u76f4\u63a5\u5c06\u566a\u58f0\u6620\u5c04\u5230\u8f68\u8ff9\u5206\u5e03\uff0c\u5e76\u91c7\u7528\u8c03\u8282\u7684\u73af\u4e2d\u6307\u5bfc\u4ee5\u589e\u52a0\u8f68\u8ff9\u7684\u591a\u6837\u6027\u3002", "result": "\u901a\u8fc7\u5bf9\u6bd4\u5e73\u8861\u7b56\u7565\u548c\u6837\u672c\u8bad\u7ec3\u6570\u636e\uff0c\u53d1\u73b0\u57fa\u4e8e\u8f68\u8ff9\u6a21\u5f0f\u7684\u91cd\u52a0\u6743\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0cFlowDrive\u5c55\u793a\u4e86\u5728\u63d0\u9ad8\u8f68\u8ff9\u591a\u6837\u6027\u7684\u540c\u65f6\u4fdd\u6301\u573a\u666f\u4e00\u81f4\u6027\u3002", "conclusion": "FlowDrive\u5728nuPlan\u548cinterPlan\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u5b66\u4e60\u578b\u89c4\u5212\u5668\u7684\u6700\u4f73\u7ed3\u679c\uff0c\u5e76\u901a\u8fc7\u6dfb\u52a0\u8c03\u8282\u6307\u5bfc\u548c\u8f7b\u91cf\u7ea7\u540e\u5904\u7406\uff08FlowDrive*\uff09\u5728\u51e0\u4e4e\u6240\u6709\u57fa\u51c6\u6570\u636e\u96c6\u4e2d\u5b9e\u73b0\u4e86\u6574\u4f53\u7684\u6700\u4f73\u6027\u80fd\u3002"}}
{"id": "2509.21983", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21983", "abs": "https://arxiv.org/abs/2509.21983", "authors": ["Sigmund Hennum H\u00f8eg", "Aksel Vaaler", "Chaoqi Liu", "Olav Egeland", "Yilun Du"], "title": "Hybrid Diffusion for Simultaneous Symbolic and Continuous Planning", "comment": "10 pages, 11 figures. This work has been submitted to the IEEE for\n  possible publication. See https://sigmundhh.com/hybrid_diffusion/ for the\n  project website", "summary": "Constructing robots to accomplish long-horizon tasks is a long-standing\nchallenge within artificial intelligence. Approaches using generative methods,\nparticularly Diffusion Models, have gained attention due to their ability to\nmodel continuous robotic trajectories for planning and control. However, we\nshow that these models struggle with long-horizon tasks that involve complex\ndecision-making and, in general, are prone to confusing different modes of\nbehavior, leading to failure. To remedy this, we propose to augment continuous\ntrajectory generation by simultaneously generating a high-level symbolic plan.\nWe show that this requires a novel mix of discrete variable diffusion and\ncontinuous diffusion, which dramatically outperforms the baselines. In\naddition, we illustrate how this hybrid diffusion process enables flexible\ntrajectory synthesis, allowing us to condition synthesized actions on partial\nand complete symbolic conditions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u6269\u6563\u65b9\u6cd5\uff0c\u901a\u8fc7\u540c\u65f6\u751f\u6210\u9ad8\u5c42\u7b26\u53f7\u8ba1\u5212\uff0c\u6539\u5584\u5728\u957f\u671f\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8f68\u8ff9\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u751f\u6210\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u51b3\u7b56\u548c\u957f\u65f6\u95f4\u4efb\u52a1\u65f6\u7684\u4e0d\u8db3\uff0c\u5c24\u5176\u662fDiffusion Models\u5728\u8868\u73b0\u4e0a\u7684\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u6df7\u5408\u7684\u79bb\u6563\u53d8\u91cf\u6269\u6563\u548c\u8fde\u7eed\u6269\u6563\u65b9\u6cd5\u8fdb\u884c\u8f68\u8ff9\u751f\u6210\uff0c\u7ed3\u5408\u9ad8\u5c42\u6b21\u7b26\u53f7\u8ba1\u5212\u3002", "result": "\u6211\u4eec\u7684\u6df7\u5408\u6269\u6563\u8fc7\u7a0b\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4e14\u80fd\u591f\u7075\u6d3b\u5730\u6839\u636e\u90e8\u5206\u548c\u5b8c\u6574\u7684\u7b26\u53f7\u6761\u4ef6\u5408\u6210\u8f68\u8ff9\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u901a\u8fc7\u540c\u65f6\u751f\u6210\u9ad8\u5c42\u7b26\u53f7\u8ba1\u5212\u6765\u589e\u5f3a\u8fde\u7eed\u8f68\u8ff9\u751f\u6210\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u590d\u6742\u51b3\u7b56\u7684\u957f\u65f6\u95f4\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2509.21986", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21986", "abs": "https://arxiv.org/abs/2509.21986", "authors": ["Tomoya Yoshida", "Shuhei Kurita", "Taichi Nishimura", "Shinsuke Mori"], "title": "Developing Vision-Language-Action Model from Egocentric Videos", "comment": null, "summary": "Egocentric videos capture how humans manipulate objects and tools, providing\ndiverse motion cues for learning object manipulation. Unlike the costly,\nexpert-driven manual teleoperation commonly used in training\nVision-Language-Action models (VLAs), egocentric videos offer a scalable\nalternative. However, prior studies that leverage such videos for training\nrobot policies typically rely on auxiliary annotations, such as detailed\nhand-pose recordings. Consequently, it remains unclear whether VLAs can be\ntrained directly from raw egocentric videos. In this work, we address this\nchallenge by leveraging EgoScaler, a framework that extracts 6DoF object\nmanipulation trajectories from egocentric videos without requiring auxiliary\nrecordings. We apply EgoScaler to four large-scale egocentric video datasets\nand automatically refine noisy or incomplete trajectories, thereby constructing\na new large-scale dataset for VLA pre-training. Our experiments with a\nstate-of-the-art $\\pi_0$ architecture in both simulated and real-robot\nenvironments yield three key findings: (i) pre-training on our dataset improves\ntask success rates by over 20\\% compared to training from scratch, (ii) the\nperformance is competitive with that achieved using real-robot datasets, and\n(iii) combining our dataset with real-robot data yields further improvements.\nThese results demonstrate that egocentric videos constitute a promising and\nscalable resource for advancing VLA research.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u4ec5\u5229\u7528\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u8bad\u7ec3\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u63d0\u51faEgoScaler\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u64cd\u63a7\u80fd\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u7684\u6f5c\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u624b\u52a8\u9065\u63a7\u8bad\u7ec3\u65b9\u5f0f\u6210\u672c\u9ad8\u6602\u4e14\u4f9d\u8d56\u4e13\u5bb6\uff0c\u800c\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u4f38\u7f29\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u7528EgoScaler\u6846\u67b6\u4ece\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u4e2d\u63d0\u53d66DoF\u7269\u4f53\u64cd\u63a7\u8f68\u8ff9\uff0c\u65e0\u9700\u8f85\u52a9\u8bb0\u5f55\u3002", "result": "\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u76f8\u6bd4\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\uff0c\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad8\u4e8620%\u4ee5\u4e0a\uff0c\u800c\u4e14\u7ed3\u5408\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u4ee5\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u4e3a\u57fa\u7840\u7684\u8bad\u7ec3\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u64cd\u63a7\u6027\u80fd\uff0c\u5e76\u4e14\u8fd9\u79cd\u65b9\u6cd5\u5177\u6709\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2509.22002", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22002", "abs": "https://arxiv.org/abs/2509.22002", "authors": ["Yuping Gu", "Bangchao Huang", "Haoran Sun", "Ronghan Xu", "Jiayi Yin", "Wei Zhang", "Fang Wan", "Jia Pan", "Chaoyang Song"], "title": "One-DoF Robotic Design of Overconstrained Limbs with Energy-Efficient, Self-Collision-Free Motion", "comment": "23 pages, 11 figures, 2 tables. Accepted by Fundamental Research. For\n  Supplementary Videos, see https://bionicdl.ancorasir.com/?p=1668", "summary": "While it is expected to build robotic limbs with multiple degrees of freedom\n(DoF) inspired by nature, a single DoF design remains fundamental, providing\nbenefits that include, but are not limited to, simplicity, robustness,\ncost-effectiveness, and efficiency. Mechanisms, especially those with multiple\nlinks and revolute joints connected in closed loops, play an enabling factor in\nintroducing motion diversity for 1-DoF systems, which are usually constrained\nby self-collision during a full-cycle range of motion. This study presents a\nnovel computational approach to designing one-degree-of-freedom (1-DoF)\noverconstrained robotic limbs for a desired spatial trajectory, while achieving\nenergy-efficient, self-collision-free motion in full-cycle rotations. Firstly,\nwe present the geometric optimization problem of linkage-based robotic limbs in\na generalized formulation for self-collision-free design. Next, we formulate\nthe spatial trajectory generation problem with the overconstrained linkages by\noptimizing the similarity and dynamic-related metrics. We further optimize the\ngeometric shape of the overconstrained linkage to ensure smooth and\ncollision-free motion driven by a single actuator. We validated our proposed\nmethod through various experiments, including personalized automata and\nbio-inspired hexapod robots. The resulting hexapod robot, featuring\noverconstrained robotic limbs, demonstrated outstanding energy efficiency\nduring forward walking.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8bbe\u8ba1\u5355\u81ea\u7531\u5ea6\u8fc7\u7ea6\u675f\u673a\u5668\u4eba\u80a2\u4f53\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u65e0\u81ea\u78b0\u649e\u7684\u8fd0\u52a8\uff0c\u540c\u65f6\u5728\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u4f18\u5f02\u7684\u80fd\u6548\u3002", "motivation": "\u5c3d\u7ba1\u671f\u671b\u8bbe\u8ba1\u51fa\u591a\u81ea\u7531\u5ea6\u7684\u673a\u5668\u4eba\u80a2\u4f53\uff0c\u4f46\u5355\u81ea\u7531\u5ea6\u8bbe\u8ba1\u5728\u7b80\u5316\u6027\u3001\u9c81\u68d2\u6027\u3001\u6210\u672c\u6548\u76ca\u548c\u6548\u7387\u7b49\u65b9\u9762\u4ecd\u7136\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u51e0\u4f55\u4f18\u5316\u95ee\u9898\u548c\u7a7a\u95f4\u8f68\u8ff9\u751f\u6210\u95ee\u9898\u7684\u516c\u5f0f\u5316\uff0c\u7ed3\u5408\u4f18\u5316\u94fe\u63a5\u7684\u51e0\u4f55\u5f62\u72b6\uff0c\u786e\u4fdd\u8fd0\u52a8\u7684\u5e73\u6ed1\u548c\u65e0\u78b0\u649e\u3002", "result": "\u7814\u7a76\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5404\u79cd\u5b9e\u9a8c\uff0c\u53d1\u73b0\u5177\u5907\u8fc7\u7ea6\u675f\u7684\u673a\u5668\u4eba\u80a2\u4f53\u7684\u516d\u8db3\u673a\u5668\u4eba\u5728\u524d\u8fdb\u884c\u8d70\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u80fd\u6548\u3002", "conclusion": "\u8be5\u7814\u7a76\u5b9e\u73b0\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u51fa\u80fd\u5728\u5168\u5468\u671f\u65cb\u8f6c\u4e2d\u5b9e\u73b0\u9ad8\u6548\u3001\u65e0\u81ea\u78b0\u649e\u8fd0\u52a8\u7684\u5355\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u80a2\u4f53\u3002"}}
{"id": "2509.22058", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22058", "abs": "https://arxiv.org/abs/2509.22058", "authors": ["Qifeng Wang", "Weigang Li", "Lei Nie", "Xin Xu", "Wenping Liu", "Zhe Xu"], "title": "An Adaptive ICP LiDAR Odometry Based on Reliable Initial Pose", "comment": null, "summary": "As a key technology for autonomous navigation and positioning in mobile\nrobots, light detection and ranging (LiDAR) odometry is widely used in\nautonomous driving applications. The Iterative Closest Point (ICP)-based\nmethods have become the core technique in LiDAR odometry due to their efficient\nand accurate point cloud registration capability. However, some existing\nICP-based methods do not consider the reliability of the initial pose, which\nmay cause the method to converge to a local optimum. Furthermore, the absence\nof an adaptive mechanism hinders the effective handling of complex dynamic\nenvironments, resulting in a significant degradation of registration accuracy.\nTo address these issues, this paper proposes an adaptive ICP-based LiDAR\nodometry method that relies on a reliable initial pose. First, distributed\ncoarse registration based on density filtering is employed to obtain the\ninitial pose estimation. The reliable initial pose is then selected by\ncomparing it with the motion prediction pose, reducing the initial error\nbetween the source and target point clouds. Subsequently, by combining the\ncurrent and historical errors, the adaptive threshold is dynamically adjusted\nto accommodate the real-time changes in the dynamic environment. Finally, based\non the reliable initial pose and the adaptive threshold, point-to-plane\nadaptive ICP registration is performed from the current frame to the local map,\nachieving high-precision alignment of the source and target point clouds.\nExtensive experiments on the public KITTI dataset demonstrate that the proposed\nmethod outperforms existing approaches and significantly enhances the accuracy\nof LiDAR odometry.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94ICP\u7684LiDAR\u91cc\u7a0b\u8ba1\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u521d\u59cb\u59ff\u6001\u53ef\u9760\u6027\u548c\u52a8\u6001\u73af\u5883\u9002\u5e94\u6027\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6ce8\u518c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eICP\u7684\u65b9\u6cd5\u672a\u5145\u5206\u8003\u8651\u521d\u59cb\u59ff\u6001\u7684\u53ef\u9760\u6027\uff0c\u5e76\u7f3a\u4e4f\u9002\u5e94\u590d\u6742\u52a8\u6001\u73af\u5883\u7684\u673a\u5236\uff0c\u5bfc\u81f4\u6ce8\u518c\u7cbe\u5ea6\u663e\u8457\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94ICP\u7684LiDAR\u91cc\u7a0b\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u7c97\u7565\u6ce8\u518c\u83b7\u53d6\u53ef\u9760\u7684\u521d\u59cb\u59ff\u6001\uff0c\u5e76\u52a8\u6001\u8c03\u6574\u81ea\u9002\u5e94\u9608\u503c\u3002", "result": "\u901a\u8fc7\u6bd4\u8f83\u8fd0\u52a8\u9884\u6d4b\u59ff\u6001\u9009\u62e9\u53ef\u9760\u7684\u521d\u59cb\u59ff\u6001\uff0c\u5e76\u7ed3\u5408\u5f53\u524d\u4e0e\u5386\u53f2\u9519\u8bef\uff0c\u52a8\u6001\u8c03\u6574\u81ea\u9002\u5e94\u9608\u503c\uff0c\u6700\u7ec8\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u70b9\u4e91\u914d\u51c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728KITTI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86LiDAR\u91cc\u7a0b\u8ba1\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2509.22065", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.22065", "abs": "https://arxiv.org/abs/2509.22065", "authors": ["Ethan Fulcher", "J. Diego Caporale", "Yifeng Zhang", "John Ruck", "Feifei Qian"], "title": "Effect of Gait Design on Proprioceptive Sensing of Terrain Properties in a Quadrupedal Robot", "comment": "7+1 pages, 5 figures, ICRA Submission This work has been submitted to\n  the IEEE for possible publication", "summary": "In-situ robotic exploration is an important tool for advancing knowledge of\ngeological processes that describe the Earth and other Planetary bodies. To\ninform and enhance operations for these roving laboratories, it is imperative\nto understand the terramechanical properties of their environments, especially\nfor traversing on loose, deformable substrates. Recent research suggested that\nlegged robots with direct-drive and low-gear ratio actuators can sensitively\ndetect external forces, and therefore possess the potential to measure terrain\nproperties with their legs during locomotion, providing unprecedented sampling\nspeed and density while accessing terrains previously too risky to sample. This\npaper explores these ideas by investigating the impact of gait on\nproprioceptive terrain sensing accuracy, particularly comparing a\nsensing-oriented gait, Crawl N' Sense, with a locomotion-oriented gait,\nTrot-Walk. Each gait's ability to measure the strength and texture of\ndeformable substrate is quantified as the robot locomotes over a laboratory\ntransect consisting of a rigid surface, loose sand, and loose sand with\nsynthetic surface crusts. Our results suggest that with both the\nsensing-oriented crawling gait and locomotion-oriented trot gait, the robot can\nmeasure a consistent difference in the strength (in terms of penetration\nresistance) between the low- and high-resistance substrates; however, the\nlocomotion-oriented trot gait contains larger magnitude and variance in\nmeasurements. Furthermore, the slower crawl gait can detect brittle ruptures of\nthe surface crusts with significantly higher accuracy than the faster trot\ngait. Our results offer new insights that inform legged robot \"sensing during\nlocomotion\" gait design and planning for scouting the terrain and producing\nscientific measurements on other worlds to advance our understanding of their\ngeology and formation.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u673a\u5668\u4eba\u6b65\u6001\u5bf9\u5730\u5f62\u611f\u77e5\u7cbe\u5ea6\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u7279\u5b9a\u6b65\u6001\u53ef\u63d0\u9ad8\u5bf9\u5730\u9762\u7279\u6027\u7684\u6d4b\u91cf\uff0c\u6709\u52a9\u4e8e\u5916\u661f\u5730\u8d28\u52d8\u63a2\u3002", "motivation": "\u63d0\u9ad8\u5bf9\u673a\u5668\u4eba\u5728\u677e\u6563\u3001\u53ef\u53d8\u57fa\u5e95\u73af\u5883\u4e2d\u7684\u884c\u8d70\u6027\u80fd\u7684\u7406\u89e3\uff0c\u4ee5\u4fbf\u5728\u5916\u661f\u9762\u4e0a\u8fdb\u884c\u66f4\u6709\u6548\u7684\u5730\u8d28\u52d8\u63a2\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u5bf9\u6bd4\u4e24\u79cd\u6b65\u6001\uff08\u722c\u884cN'\u611f\u77e5\u548c\u8dd1\u6b65\u884c\u8d70\uff09\u5728\u4e0d\u540c\u57fa\u5e95\u4e0a\u8fdb\u884c\u7684\u6d4b\u91cf\uff0c\u8bc4\u4f30\u5176\u5bf9\u53ef\u53d8\u6c89\u79ef\u7269\u5f3a\u5ea6\u548c\u7eb9\u7406\u7684\u611f\u77e5\u51c6\u786e\u6027\u3002", "result": "\u5728\u4e0d\u540c\u7684\u57fa\u5e95\uff08\u521a\u6027\u8868\u9762\u3001\u677e\u6563\u6c99\u5b50\u53ca\u5176\u8868\u9762\u6709\u5408\u6210\u58f3\u5c42\uff09\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u53d1\u73b0\u722c\u884c\u6b65\u6001\u6bd4\u8dd1\u6b65\u6b65\u6001\u5728\u6d4b\u91cf\u53ef\u53d8\u6c89\u79ef\u7269\u7279\u6027\u4e0a\u66f4\u4e3a\u7cbe\u786e\u3002", "conclusion": "\u4f7f\u7528\u611f\u77e5\u610f\u8bc6\u7684\u722c\u884c\u6b65\u6001\u548c\u8fd0\u884c\u52a8\u7684\u8dd1\u6b65\u6b65\u6001\uff0c\u673a\u5668\u4eba\u80fd\u6709\u6548\u6d4b\u91cf\u4e0d\u540c\u57fa\u5e95\u7684\u5f3a\u5ea6\u5dee\u5f02\uff1b\u722c\u884c\u6b65\u6001\u5728\u68c0\u6d4b\u8868\u9762\u7834\u88c2\u65b9\u9762\u8868\u73b0\u66f4\u4f73\uff0c\u63d0\u4f9b\u4e86\u6539\u5584\u673a\u5668\u4eba\u6b65\u6001\u8bbe\u8ba1\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.22093", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22093", "abs": "https://arxiv.org/abs/2509.22093", "authors": ["Xiaohuan Pei", "Yuxing Chen", "Siyu Xu", "Yunke Wang", "Yuheng Shi", "Chang Xu"], "title": "Action-aware Dynamic Pruning for Efficient Vision-Language-Action Manipulation", "comment": null, "summary": "Robotic manipulation with Vision-Language-Action models requires efficient\ninference over long-horizon multi-modal context, where attention to dense\nvisual tokens dominates computational cost. Existing methods optimize inference\nspeed by reducing visual redundancy within VLA models, but they overlook the\nvarying redundancy across robotic manipulation stages. We observe that the\nvisual token redundancy is higher in coarse manipulation phase than in\nfine-grained operations, and is strongly correlated with the action dynamic.\nMotivated by this observation, we propose \\textbf{A}ction-aware\n\\textbf{D}ynamic \\textbf{P}runing (\\textbf{ADP}), a multi-modal pruning\nframework that integrates text-driven token selection with action-aware\ntrajectory gating. Our method introduces a gating mechanism that conditions the\npruning signal on recent action trajectories, using past motion windows to\nadaptively adjust token retention ratios in accordance with dynamics, thereby\nbalancing computational efficiency and perceptual precision across different\nmanipulation stages. Extensive experiments on the LIBERO suites and diverse\nreal-world scenarios demonstrate that our method significantly reduces FLOPs\nand action inference latency (\\textit{e.g.} $1.35 \\times$ speed up on\nOpenVLA-OFT) while maintaining competitive success rates (\\textit{e.g.} 25.8\\%\nimprovements with OpenVLA) compared to baselines, thereby providing a simple\nplug-in path to efficient robot policies that advances the efficiency and\nperformance frontier of robotic manipulation. Our project website is:\n\\href{https://vla-adp.github.io/}{ADP.com}.", "AI": {"tldr": "\u63d0\u51fa\u4e86ADP\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u526a\u679d\u548c\u52a8\u4f5c\u611f\u77e5\u673a\u5236\uff0c\u63d0\u9ad8\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u89c2\u5bdf\u5230\u5728\u7c97\u7565\u64cd\u63a7\u9636\u6bb5\uff0c\u89c6\u89c9\u6807\u8bb0\u7684\u5197\u4f59\u6027\u9ad8\u4e8e\u7ec6\u7c92\u5ea6\u64cd\u4f5c\uff0c\u5e76\u4e14\u4e0e\u52a8\u4f5c\u52a8\u6001\u5f3a\u76f8\u5173\uff0c\u56e0\u800c\u63d0\u51fa\u4e86\u57fa\u4e8e\u8fd9\u4e00\u89c2\u5bdf\u7684\u526a\u679d\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u4f5c\u611f\u77e5\u52a8\u6001\u526a\u679d\uff08ADP\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u6587\u672c\u9a71\u52a8\u7684\u6807\u8bb0\u9009\u62e9\u548c\u52a8\u4f5c\u611f\u77e5\u7684\u8f68\u8ff9\u95e8\u63a7\u673a\u5236\u3002", "result": "ADP\u5728LIBERO\u5957\u4ef6\u548c\u591a\u79cd\u771f\u5b9e\u573a\u666f\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u91cf\u548c\u5ef6\u8fdf\uff0c\u540c\u65f6\u4e5f\u63d0\u5347\u4e86\u6210\u529f\u7387\u3002", "conclusion": "ADP\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86FLOPs\u548c\u52a8\u4f5c\u63a8\u7406\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u6210\u529f\u7387\uff0c\u4e3a\u9ad8\u6548\u7684\u673a\u5668\u4eba\u7b56\u7565\u63d0\u4f9b\u4e86\u7b80\u5355\u7684\u63d2\u5165\u8def\u5f84\u3002"}}
{"id": "2509.22120", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22120", "abs": "https://arxiv.org/abs/2509.22120", "authors": ["Alireza Aliyari", "Gholamreza Vossoughi"], "title": "Multi-stage robust nonlinear model predictive control of a lower-limb exoskeleton robot", "comment": "12 pages, 11 figures, 2 tables, under review at the journal of\n  \"Transactions of the Canadian Society for Mechanical Engineering\"", "summary": "The use of exoskeleton robots is increasing due to the rising number of\nmusculoskeletal injuries. However, their effectiveness depends heavily on the\ndesign of control systems. Designing robust controllers is challenging because\nof uncertainties in human-robot systems. Among various control strategies,\nModel Predictive Control (MPC) is a powerful approach due to its ability to\nhandle constraints and optimize performance. Previous studies have used\nlinearization-based methods to implement robust MPC on exoskeletons, but these\ncan degrade performance due to nonlinearities in the robot's dynamics. To\naddress this gap, this paper proposes a Robust Nonlinear Model Predictive\nControl (RNMPC) method, called multi-stage NMPC, to control a\ntwo-degree-of-freedom exoskeleton by solving a nonlinear optimization problem.\nThis method uses multiple scenarios to represent system uncertainties. The\nstudy focuses on minimizing human-robot interaction forces during the swing\nphase, particularly when the robot carries unknown loads. Simulations and\nexperimental tests show that the proposed method significantly improves\nrobustness, outperforming non-robust NMPC. It achieves lower tracking errors\nand interaction forces under various uncertainties. For instance, when a 2 kg\nunknown payload is combined with external disturbances, the RMS values of thigh\nand shank interaction forces for multi-stage NMPC are reduced by 77 and 94\npercent, respectively, compared to non-robust NMPC.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7684\u591a\u9636\u6bb5\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5916\u9aa8\u9abc\u673a\u5668\u4eba\u7684\u9c81\u68d2\u6027\uff0c\u51cf\u5c11\u4e86\u4eba\u673a\u4ea4\u4e92\u529b\uff0c\u6548\u679c\u4f18\u4e8e\u4f20\u7edf\u63a7\u5236\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u808c\u8089\u9aa8\u9abc\u635f\u4f24\u6848\u4f8b\u7684\u589e\u52a0\uff0c\u5916\u9aa8\u9abc\u673a\u5668\u4eba\u7684\u4f7f\u7528\u8d8a\u6765\u8d8a\u666e\u904d\uff0c\u4f46\u5176\u6709\u6548\u6027\u53d7\u5230\u63a7\u5236\u7cfb\u7edf\u8bbe\u8ba1\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9636\u6bb5\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08RNMPC\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u51b3\u975e\u7ebf\u6027\u4f18\u5316\u95ee\u9898\u6765\u63a7\u5236\u4e00\u4e2a\u53cc\u81ea\u7531\u5ea6\u7684\u5916\u9aa8\u9abc\u3002", "result": "\u6a21\u62df\u548c\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u975e\u9c81\u68d2\u7684NMPC\u76f8\u6bd4\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u8ddf\u8e2a\u8bef\u5dee\u548c\u4eba\u673a\u4ea4\u4e92\u529b\u65b9\u9762\u5747\u6709\u663e\u8457\u6539\u5584\uff0c\u5c24\u5176\u5728\u5904\u7406\u672a\u77e5\u8d1f\u8f7d\u65f6\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u591a\u9636\u6bb5\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5916\u9aa8\u9abc\u673a\u5668\u4eba\u7684\u63a7\u5236\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u5728\u4e0d\u786e\u5b9a\u8d1f\u8f7d\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2509.22149", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22149", "abs": "https://arxiv.org/abs/2509.22149", "authors": ["Haoqi Yuan", "Ziye Huang", "Ye Wang", "Chuan Mao", "Chaoyi Xu", "Zongqing Lu"], "title": "DemoGrasp: Universal Dexterous Grasping from a Single Demonstration", "comment": null, "summary": "Universal grasping with multi-fingered dexterous hands is a fundamental\nchallenge in robotic manipulation. While recent approaches successfully learn\nclosed-loop grasping policies using reinforcement learning (RL), the inherent\ndifficulty of high-dimensional, long-horizon exploration necessitates complex\nreward and curriculum design, often resulting in suboptimal solutions across\ndiverse objects. We propose DemoGrasp, a simple yet effective method for\nlearning universal dexterous grasping. We start from a single successful\ndemonstration trajectory of grasping a specific object and adapt to novel\nobjects and poses by editing the robot actions in this trajectory: changing the\nwrist pose determines where to grasp, and changing the hand joint angles\ndetermines how to grasp. We formulate this trajectory editing as a single-step\nMarkov Decision Process (MDP) and use RL to optimize a universal policy across\nhundreds of objects in parallel in simulation, with a simple reward consisting\nof a binary success term and a robot-table collision penalty. In simulation,\nDemoGrasp achieves a 95% success rate on DexGraspNet objects using the Shadow\nHand, outperforming previous state-of-the-art methods. It also shows strong\ntransferability, achieving an average success rate of 84.6% across diverse\ndexterous hand embodiments on six unseen object datasets, while being trained\non only 175 objects. Through vision-based imitation learning, our policy\nsuccessfully grasps 110 unseen real-world objects, including small, thin items.\nIt generalizes to spatial, background, and lighting changes, supports both RGB\nand depth inputs, and extends to language-guided grasping in cluttered scenes.", "AI": {"tldr": "DemoGrasp\u662f\u4e00\u79cd\u901a\u8fc7\u7f16\u8f91\u6210\u529f\u6293\u53d6\u8f68\u8ff9\u6765\u5b66\u4e60\u901a\u7528\u6293\u53d6\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\u548c\u5f3a\u5927\u7684\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u9ad8\u7ef4\u5ea6\u957f\u671f\u63a2\u7d22\u4e2d\u7684\u590d\u6742\u5956\u52b1\u548c\u8bfe\u7a0b\u8bbe\u8ba1\u95ee\u9898\uff0c\u4ee5\u63d0\u9ad8\u901a\u7528\u6293\u53d6\u7684\u6709\u6548\u6027\u4e0e\u6210\u529f\u7387\u3002", "method": "\u901a\u8fc7\u7f16\u8f91\u673a\u5668\u4eba\u5728\u7279\u5b9a\u5bf9\u8c61\u4e0a\u7684\u6210\u529f\u6293\u53d6\u8f68\u8ff9\uff0c\u4f7f\u7528\u5355\u6b65\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u8fdb\u884c\u4f18\u5316\uff0c\u5e76\u7ed3\u5408\u7b80\u5355\u7684\u5956\u52b1\u8bbe\u8ba1\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5728\u6a21\u62df\u4e2d\uff0cDemoGrasp\u5728DexGraspNet\u5bf9\u8c61\u4e0a\u5b9e\u73b0\u4e8695%\u7684\u6210\u529f\u7387\uff0c\u5e76\u5728\u516d\u4e2a\u672a\u89c1\u5bf9\u8c61\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u6210\u529f\u7387\u8fbe84.6%\u3002", "conclusion": "DemoGrasp\u65b9\u6cd5\u5728\u591a\u6837\u5316\u5bf9\u8c61\u4e0a\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u6293\u53d6\u80fd\u529b\uff0c\u6210\u529f\u7387\u8fbe95%\uff0c\u5e76\u4e14\u5177\u5907\u4f18\u826f\u7684\u8fc1\u79fb\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2509.22175", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22175", "abs": "https://arxiv.org/abs/2509.22175", "authors": ["Quanzhou Li", "Zhonghua Wu", "Jingbo Wang", "Chen Change Loy", "Bo Dai"], "title": "DHAGrasp: Synthesizing Affordance-Aware Dual-Hand Grasps with Text Instructions", "comment": null, "summary": "Learning to generate dual-hand grasps that respect object semantics is\nessential for robust hand-object interaction but remains largely underexplored\ndue to dataset scarcity. Existing grasp datasets predominantly focus on\nsingle-hand interactions and contain only limited semantic part annotations. To\naddress these challenges, we introduce a pipeline, SymOpt, that constructs a\nlarge-scale dual-hand grasp dataset by leveraging existing single-hand datasets\nand exploiting object and hand symmetries. Building on this, we propose a\ntext-guided dual-hand grasp generator, DHAGrasp, that synthesizes Dual-Hand\nAffordance-aware Grasps for unseen objects. Our approach incorporates a novel\ndual-hand affordance representation and follows a two-stage design, which\nenables effective learning from a small set of segmented training objects while\nscaling to a much larger pool of unsegmented data. Extensive experiments\ndemonstrate that our method produces diverse and semantically consistent\ngrasps, outperforming strong baselines in both grasp quality and generalization\nto unseen objects. The project page is at\nhttps://quanzhou-li.github.io/DHAGrasp/.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53cc\u624b\u6293\u53d6\u751f\u6210\u65b9\u6cd5\uff0c\u514b\u670d\u4e86\u6570\u636e\u96c6\u7a00\u7f3a\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5728\u672a\u89c1\u7269\u4f53\u4e0a\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u751f\u6210\u7b26\u5408\u7269\u4f53\u8bed\u4e49\u7684\u53cc\u624b\u6293\u53d6\u5bf9\u4e8e\u7a33\u5065\u7684\u624b-\u7269\u4f53\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7684\u6570\u636e\u96c6\u7a00\u7f3a\u3002", "method": "\u5229\u7528\u5355\u624b\u6293\u53d6\u6570\u636e\u96c6\u548c\u7269\u4f53\u4e0e\u624b\u7684\u5bf9\u79f0\u6027\uff0c\u6784\u5efa\u4e00\u4e2a\u5927\u89c4\u6a21\u53cc\u624b\u6293\u53d6\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6587\u672c\u6307\u5bfc\u7684\u53cc\u624b\u6293\u53d6\u751f\u6210\u5668\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u53d1\u73b0\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u591a\u6837\u4e14\u7b26\u5408\u8bed\u4e49\u7684\u4e00\u81f4\u6293\u53d6\uff0c\u4e14\u5728\u6293\u53d6\u8d28\u91cf\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u4f18\u4e8e\u5f3a\u57fa\u51c6\u3002", "conclusion": "\u63d0\u51fa\u7684DHAGrasp\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u751f\u6210\u591a\u624b\u6293\u53d6\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6293\u53d6\u8d28\u91cf\u548c\u5bf9\u672a\u89c1\u7269\u4f53\u7684\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u8868\u73b0\u3002"}}
{"id": "2509.22195", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22195", "abs": "https://arxiv.org/abs/2509.22195", "authors": ["Asher J. Hancock", "Xindi Wu", "Lihan Zha", "Olga Russakovsky", "Anirudha Majumdar"], "title": "Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting", "comment": null, "summary": "Fine-tuning vision-language models (VLMs) on robot teleoperation data to\ncreate vision-language-action (VLA) models is a promising paradigm for training\ngeneralist policies, but it suffers from a fundamental tradeoff: learning to\nproduce actions often diminishes the VLM's foundational reasoning and\nmultimodal understanding, hindering generalization to novel scenarios,\ninstruction following, and semantic understanding. We argue that this\ncatastrophic forgetting is due to a distribution mismatch between the VLM's\ninternet-scale pretraining corpus and the robotics fine-tuning data. Inspired\nby this observation, we introduce VLM2VLA: a VLA training paradigm that first\nresolves this mismatch at the data level by representing low-level actions with\nnatural language. This alignment makes it possible to train VLAs solely with\nLow-Rank Adaptation (LoRA), thereby minimally modifying the VLM backbone and\naverting catastrophic forgetting. As a result, the VLM can be fine-tuned on\nrobot teleoperation data without fundamentally altering the underlying\narchitecture and without expensive co-training on internet-scale VLM datasets.\nThrough extensive Visual Question Answering (VQA) studies and over 800\nreal-world robotics experiments, we demonstrate that VLM2VLA preserves the\nVLM's core capabilities, enabling zero-shot generalization to novel tasks that\nrequire open-world semantic reasoning and multilingual instruction following.", "AI": {"tldr": "\u63d0\u51faVLM2VLA\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u4f4e\u79e9\u9002\u5e94\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u707e\u96be\u6027\u9057\u5fd8\u7684\u95ee\u9898\uff0c\u4fdd\u6301\u6a21\u578b\u80fd\u529b\uff0c\u5b9e\u73b0\u5bf9\u65b0\u4efb\u52a1\u7684\u6cdb\u5316\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u8fdc\u7a0b\u64cd\u4f5c\u6570\u636e\u4e0a\u5fae\u8c03\u540e\uff0c\u5bb9\u6613\u51fa\u73b0\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5f71\u54cd\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u591a\u6a21\u6001\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00-\u884c\u52a8\uff08VLA\uff09\u6a21\u578b\uff0c\u9996\u5148\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8868\u793a\u4f4e\u7ea7\u52a8\u4f5c\u4ee5\u89e3\u51b3\u6570\u636e\u5206\u5e03\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u7814\u7a76\u548c800\u591a\u9879\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u5b9e\u9a8c\uff0c\u8bc1\u660eVLM2VLA\u80fd\u591f\u5728\u4e0d\u8fdb\u884c\u6602\u8d35\u5171\u540c\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u673a\u5668\u4eba\u8fdc\u7a0b\u64cd\u4f5c\u6570\u636e\u8fdb\u884c\u5fae\u8c03\u3002", "conclusion": "VLM2VLA\u8bad\u7ec3\u8303\u5f0f\u80fd\u591f\u5728\u4e0d\u6539\u53d8\u57fa\u7840\u67b6\u6784\u7684\u60c5\u51b5\u4e0b\u4fdd\u6301\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u6838\u5fc3\u80fd\u529b\uff0c\u5b9e\u73b0\u5bf9\u65b0\u4efb\u52a1\u7684\u96f6-shot\u6cdb\u5316\u3002"}}
{"id": "2509.22199", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22199", "abs": "https://arxiv.org/abs/2509.22199", "authors": ["Haoyun Li", "Ivan Zhang", "Runqi Ouyang", "Xiaofeng Wang", "Zheng Zhu", "Zhiqin Yang", "Zhentao Zhang", "Boyuan Wang", "Chaojun Ni", "Wenkang Qin", "Xinze Chen", "Yun Ye", "Guan Huang", "Zhenbo Song", "Xingang Wang"], "title": "MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training", "comment": null, "summary": "Vision Language Action (VLA) models derive their generalization capability\nfrom diverse training data, yet collecting embodied robot interaction data\nremains prohibitively expensive. In contrast, human demonstration videos are\nfar more scalable and cost-efficient to collect, and recent studies confirm\ntheir effectiveness in training VLA models. However, a significant domain gap\npersists between human videos and robot-executed videos, including unstable\ncamera viewpoints, visual discrepancies between human hands and robotic arms,\nand differences in motion dynamics. To bridge this gap, we propose\nMimicDreamer, a framework that turns fast, low-cost human demonstrations into\nrobot-usable supervision by jointly aligning vision, viewpoint, and actions to\ndirectly support policy training. For visual alignment, we propose H2R Aligner,\na video diffusion model that generates high-fidelity robot demonstration videos\nby transferring motion from human manipulation footage. For viewpoint\nstabilization, EgoStabilizer is proposed, which canonicalizes egocentric videos\nvia homography and inpaints occlusions and distortions caused by warping. For\naction alignment, we map human hand trajectories to the robot frame and apply a\nconstrained inverse kinematics solver to produce feasible, low-jitter joint\ncommands with accurate pose tracking. Empirically, VLA models trained purely on\nour synthesized human-to-robot videos achieve few-shot execution on real\nrobots. Moreover, scaling training with human data significantly boosts\nperformance compared to models trained solely on real robot data; our approach\nimproves the average success rate by 14.7\\% across six representative\nmanipulation tasks.", "AI": {"tldr": "MimicDreamer\u6846\u67b6\u901a\u8fc7\u5c06\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u8f6c\u5316\u4e3a\u673a\u5668\u4eba\u53ef\u7528\u7684\u6307\u5bfc\uff0c\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u5728\u6267\u884c\u4efb\u52a1\u65f6\u7684\u9886\u57df\u5dee\u8ddd\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u6a21\u578b\u7684\u5b66\u4e60\u6548\u7387\u4e0e\u8868\u73b0\u3002", "motivation": "\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u66f4\u6613\u83b7\u53d6\u4e14\u6210\u672c\u4f4e\u5ec9\uff0c\u7136\u800c\u5b58\u5728\u4e0e\u673a\u5668\u4eba\u6267\u884c\u89c6\u9891\u95f4\u7684\u9886\u57df\u5dee\u8ddd\u3002", "method": "\u63d0\u51faMimicDreamer\u6846\u67b6\uff0c\u5305\u62ecH2R Aligner\u89c6\u9891\u6269\u6563\u6a21\u578b\u3001EgoStabilizer\u89c6\u89d2\u7a33\u5b9a\u5316\u624b\u6bb5\u53ca\u53d7\u9650\u9006\u8fd0\u52a8\u5b66\u6c42\u89e3\u5668\u8fdb\u884c\u52a8\u4f5c\u5bf9\u9f50\u3002", "result": "\u4f7f\u7528\u5408\u6210\u7684\u4eba\u7c7b\u5230\u673a\u5668\u4eba\u89c6\u9891\u8bad\u7ec3\u7684VLA\u6a21\u578b\u5728\u5b9e\u9645\u673a\u5668\u4eba\u4e0a\u7684\u5c11\u91cf\u6837\u672c\u6267\u884c\u8868\u73b0\u826f\u597d\uff0c\u4e14\u8bad\u7ec3\u4e2d\u4f7f\u7528\u4eba\u7c7b\u6570\u636e\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u5e73\u5747\u6210\u529f\u7387\u63d0\u534714.7%\u3002", "conclusion": "MimicDreamer\u6846\u67b6\u901a\u8fc7\u5c06\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u8f6c\u5316\u4e3a\u673a\u5668\u4eba\u53ef\u7528\u7684\u76d1\u7763\uff0c\u5b9e\u73b0\u4e86\u89c6\u89c9\u3001\u89c6\u89d2\u548c\u52a8\u4f5c\u7684\u5bf9\u9f50\uff0c\u4f7f\u5f97VLA\u6a21\u578b\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u5c11\u91cf\u6837\u672c\u6267\u884c\u5e76\u663e\u8457\u63d0\u9ad8\u6210\u529f\u7387\u3002"}}
{"id": "2509.22205", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22205", "abs": "https://arxiv.org/abs/2509.22205", "authors": ["Ke Ye", "Jiaming Zhou", "Yuanfeng Qiu", "Jiayi Liu", "Shihui Zhou", "Kun-Yu Lin", "Junwei Liang"], "title": "From Watch to Imagine: Steering Long-horizon Manipulation via Human Demonstration and Future Envisionment", "comment": null, "summary": "Generalizing to long-horizon manipulation tasks in a zero-shot setting\nremains a central challenge in robotics. Current multimodal foundation based\napproaches, despite their capabilities, typically fail to decompose high-level\ncommands into executable action sequences from static visual input alone. To\naddress this challenge, we introduce Super-Mimic, a hierarchical framework that\nenables zero-shot robotic imitation by directly inferring procedural intent\nfrom unscripted human demonstration videos. Our framework is composed of two\nsequential modules. First, a Human Intent Translator (HIT) parses the input\nvideo using multimodal reasoning to produce a sequence of language-grounded\nsubtasks. These subtasks then condition a Future Dynamics Predictor (FDP),\nwhich employs a generative model that synthesizes a physically plausible video\nrollout for each step. The resulting visual trajectories are dynamics-aware,\nexplicitly modeling crucial object interactions and contact points to guide the\nlow-level controller. We validate this approach through extensive experiments\non a suite of long-horizon manipulation tasks, where Super-Mimic significantly\noutperforms state-of-the-art zero-shot methods by over 20\\%. These results\nestablish that coupling video-driven intent parsing with prospective dynamics\nmodeling is a highly effective strategy for developing general-purpose robotic\nsystems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5c42\u6b21\u6846\u67b6Super-Mimic\uff0c\u901a\u8fc7\u89e3\u6790\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u6765\u5b9e\u73b0\u96f6-shot\u673a\u5668\u4eba\u6a21\u4eff\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u65f6\u95f4\u64cd\u4f5c\u4efb\u52a1\u7684\u6267\u884c\u6548\u679c\u3002", "motivation": "\u9762\u5bf9\u81ea\u52a8\u5316\u957f\u65f6\u95f4\u64cd\u4f5c\u4efb\u52a1\u7684\u6311\u6218\uff0c\u6211\u4eec\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u4ece\u975e\u811a\u672c\u5316\u7684\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u4e2d\u63a8\u65ad\u7a0b\u5e8f\u6027\u7684\u610f\u56fe\u3002", "method": "Super-Mimic\u6846\u67b6\u7531\u4e24\u4e2a\u987a\u5e8f\u6a21\u5757\u7ec4\u6210\uff1a\u4eba\u7c7b\u610f\u56fe\u7ffb\u8bd1\u5668\uff08HIT\uff09\u548c\u672a\u6765\u52a8\u6001\u9884\u6d4b\u5668\uff08FDP\uff09\u3002", "result": "Super-Mimic\u5728\u4e00\u7cfb\u5217\u957f\u65f6\u95f4\u64cd\u4f5c\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u96f6-shot\u65b9\u6cd5\uff0c\u63d0\u5347\u8d85\u8fc720%\u3002", "conclusion": "\u901a\u8fc7\u5c06\u89c6\u9891\u9a71\u52a8\u7684\u610f\u56fe\u89e3\u6790\u4e0e\u524d\u77bb\u6027\u52a8\u6001\u5efa\u6a21\u7ed3\u5408\u8d77\u6765\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u5f00\u53d1\u901a\u7528\u673a\u5668\u4eba\u7cfb\u7edf\u3002"}}
{"id": "2509.22288", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22288", "abs": "https://arxiv.org/abs/2509.22288", "authors": ["Johan Hatleskog", "Morten Nissov", "Kostas Alexis"], "title": "IMU-Preintegrated Radar Factors for Asynchronous Radar-LiDAR-Inertial SLAM", "comment": "8 pages, 7 figures, accepted by The 22nd International Conference on\n  Advanced Robotics (ICAR 2025). Supplementary video:\n  https://youtu.be/95jeWXBMN7c", "summary": "Fixed-lag Radar-LiDAR-Inertial smoothers conventionally create one factor\ngraph node per measurement to compensate for the lack of time synchronization\nbetween radar and LiDAR. For a radar-LiDAR sensor pair with equal rates, this\nstrategy results in a state creation rate of twice the individual sensor\nfrequencies. This doubling of the number of states per second yields high\noptimization costs, inhibiting real-time performance on resource-constrained\nhardware. We introduce IMU-preintegrated radar factors that use high-rate\ninertial data to propagate the most recent LiDAR state to the radar measurement\ntimestamp. This strategy maintains the node creation rate at the LiDAR\nmeasurement frequency. Assuming equal sensor rates, this lowers the number of\nnodes by 50 % and consequently the computational costs. Experiments on a single\nboard computer (which has 4 cores each of 2.2 GHz A73 and 2 GHz A53 with 8 GB\nRAM) show that our method preserves the absolute pose error of a conventional\nbaseline while simultaneously lowering the aggregated factor graph optimization\ntime by up to 56 %.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faIMU\u9884\u79ef\u5206\u96f7\u8fbe\u56e0\u5b50\u4ee5\u51cf\u5c11\u8282\u70b9\u521b\u5efa\u7387\u548c\u4f18\u5316\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u7684\u56fa\u5b9a\u5ef6\u8fdf\u96f7\u8fbe-LiDAR-\u60ef\u6027\u5e73\u6ed1\u5668\u56e0\u65f6\u949f\u4e0d\u540c\u6b65\u800c\u6bcf\u4e2a\u6d4b\u91cf\u521b\u5efa\u4e00\u4e2a\u56e0\u5b50\u56fe\u8282\u70b9\uff0c\u5bfc\u81f4\u9ad8\u4f18\u5316\u6210\u672c\u3002", "method": "\u5f15\u5165IMU\u9884\u79ef\u5206\u96f7\u8fbe\u56e0\u5b50\uff0c\u5229\u7528\u9ad8\u9891\u60ef\u6027\u6570\u636e\u4f20\u64adLiDAR\u72b6\u6001\u5230\u96f7\u8fbe\u6d4b\u91cf\u65f6\u95f4\u6233\u3002", "result": "\u76f8\u8f83\u4e8e\u5e38\u89c4\u65b9\u6cd5\uff0c\u8282\u70b9\u6570\u91cf\u51cf\u5c1150%\uff0c\u4f18\u5316\u65f6\u95f4\u964d\u4f4e\u4e8656%\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u7edd\u5bf9\u59ff\u6001\u8bef\u5dee\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4f18\u5316\u3002"}}
{"id": "2509.22296", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22296", "abs": "https://arxiv.org/abs/2509.22296", "authors": ["Joseph Hunt", "Koyo Fujii", "Aly Magassouba", "Praminda Caleb-Solly"], "title": "Beyond Detection -- Orchestrating Human-Robot-Robot Assistance via an Internet of Robotic Things Paradigm", "comment": "ICSR 2025, 8 pages, 3 figures", "summary": "Hospital patient falls remain a critical and costly challenge worldwide.\nWhile conventional fall prevention systems typically rely on post-fall\ndetection or reactive alerts, they also often suffer from high false positive\nrates and fail to address the underlying patient needs that lead to bed-exit\nattempts. This paper presents a novel system architecture that leverages the\nInternet of Robotic Things (IoRT) to orchestrate human-robot-robot interaction\nfor proactive and personalized patient assistance. The system integrates a\nprivacy-preserving thermal sensing model capable of real-time bed-exit\nprediction, with two coordinated robotic agents that respond dynamically based\non predicted intent and patient input. This orchestrated response could not\nonly reduce fall risk but also attend to the patient's underlying motivations\nfor movement, such as thirst, discomfort, or the need for assistance, before a\nhazardous situation arises. Our contributions with this pilot study are\nthree-fold: (1) a modular IoRT-based framework enabling distributed sensing,\nprediction, and multi-robot coordination; (2) a demonstration of low-resolution\nthermal sensing for accurate, privacy-preserving preemptive bed-exit detection;\nand (3) results from a user study and systematic error analysis that inform the\ndesign of situationally aware, multi-agent interactions in hospital settings.\nThe findings highlight how interactive and connected robotic systems can move\nbeyond passive monitoring to deliver timely, meaningful assistance, empowering\nsafer, more responsive care environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u667a\u80fd\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u80fd\u591f\u901a\u8fc7\u5b9e\u65f6\u9884\u6d4b\u548c\u4e3b\u52a8\u54cd\u5e94\uff0c\u51cf\u5c11\u533b\u9662\u60a3\u8005\u8dcc\u5012\u98ce\u9669\u3002", "motivation": "\u533b\u9662\u60a3\u8005\u8dcc\u5012\u95ee\u9898\u4e25\u91cd\u4e14\u6210\u672c\u9ad8\u6602\uff0c\u4f20\u7edf\u7684\u8dcc\u5012\u9884\u9632\u7cfb\u7edf\u7f3a\u4e4f\u9488\u5bf9\u60a3\u8005\u9700\u6c42\u7684\u4e3b\u52a8\u54cd\u5e94\uff0c\u5bfc\u81f4\u9ad8\u5047\u9633\u6027\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u8054\u7f51\u7684\u673a\u5668\u4eba\u67b6\u6784\uff0c\u901a\u8fc7\u9690\u79c1\u4fdd\u62a4\u7684\u70ed\u611f\u5e94\u6a21\u578b\u5b9e\u73b0\u5b9e\u65f6\u7684\u5e8a\u5916\u9884\u6d4b\uff0c\u5e76\u4f7f\u7528\u4e24\u4e2a\u534f\u8c03\u7684\u673a\u5668\u4eba\u54cd\u5e94\u60a3\u8005\u9700\u6c42\u3002", "result": "\u521d\u6b65\u7814\u7a76\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u63d0\u9ad8\u9884\u9632\u8dcc\u5012\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u60a3\u8005\u63d0\u4f9b\u57fa\u4e8e\u9700\u6c42\u7684\u4e2a\u6027\u5316\u534f\u52a9\uff0c\u964d\u4f4e\u6f5c\u5728\u7684\u5371\u9669\u60c5\u51b5\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u7cfb\u7edf\u80fd\u591f\u901a\u8fc7\u4e92\u52a8\u548c\u8fde\u63a5\u7684\u673a\u5668\u4eba\u6280\u672f\uff0c\u63d0\u4f9b\u53ca\u65f6\u3001\u6709\u610f\u4e49\u7684\u5e2e\u52a9\uff0c\u4ece\u800c\u63d0\u5347\u533b\u9662\u7684\u5b89\u5168\u548c\u54cd\u5e94\u80fd\u529b\u3002"}}
{"id": "2509.22356", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22356", "abs": "https://arxiv.org/abs/2509.22356", "authors": ["Enguang Liu", "Siyuan Liang", "Liming Lu", "Xiyu Zeng", "Xiaochun Cao", "Aishan Liu", "Shuchao Pang"], "title": "RoboView-Bias: Benchmarking Visual Bias in Embodied Agents for Robotic Manipulation", "comment": null, "summary": "The safety and reliability of embodied agents rely on accurate and unbiased\nvisual perception. However, existing benchmarks mainly emphasize generalization\nand robustness under perturbations, while systematic quantification of visual\nbias remains scarce. This gap limits a deeper understanding of how perception\ninfluences decision-making stability. To address this issue, we propose\nRoboView-Bias, the first benchmark specifically designed to systematically\nquantify visual bias in robotic manipulation, following a principle of factor\nisolation. Leveraging a structured variant-generation framework and a\nperceptual-fairness validation protocol, we create 2,127 task instances that\nenable robust measurement of biases induced by individual visual factors and\ntheir interactions. Using this benchmark, we systematically evaluate three\nrepresentative embodied agents across two prevailing paradigms and report three\nkey findings: (i) all agents exhibit significant visual biases, with camera\nviewpoint being the most critical factor; (ii) agents achieve their highest\nsuccess rates on highly saturated colors, indicating inherited visual\npreferences from underlying VLMs; and (iii) visual biases show strong,\nasymmetric coupling, with viewpoint strongly amplifying color-related bias.\nFinally, we demonstrate that a mitigation strategy based on a semantic\ngrounding layer substantially reduces visual bias by approximately 54.5\\% on\nMOKA. Our results highlight that systematic analysis of visual bias is a\nprerequisite for developing safe and reliable general-purpose embodied agents.", "AI": {"tldr": "\u63d0\u51faRoboView-Bias\u57fa\u51c6\uff0c\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u89c6\u89c9\u504f\u5dee\uff0c\u53d1\u73b0\u89c6\u89c9\u504f\u5dee\u5bf9\u4ee3\u7406\u51b3\u7b56\u5f71\u54cd\u663e\u8457\uff0c\u540c\u65f6\u63d0\u51fa\u6709\u6548\u7684\u7f13\u89e3\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5f3a\u8c03\u5728\u6270\u52a8\u4e0b\u7684\u6cdb\u5316\u548c\u9c81\u68d2\u6027\uff0c\u7f3a\u4e4f\u5bf9\u89c6\u89c9\u504f\u5dee\u7684\u7cfb\u7edf\u91cf\u5316\uff0c\u9650\u5236\u4e86\u5bf9\u77e5\u89c9\u5982\u4f55\u5f71\u54cd\u51b3\u7b56\u7a33\u5b9a\u6027\u7684\u66f4\u6df1\u5165\u7406\u89e3\u3002", "method": "\u63d0\u51faRoboView-Bias\u57fa\u51c6\uff0c\u901a\u8fc7\u56e0\u7d20\u9694\u79bb\u539f\u5219\u7cfb\u7edf\u91cf\u5316\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u89c6\u89c9\u504f\u5dee\uff0c\u5e76\u7ed3\u5408\u7ed3\u6784\u5316\u53d8\u4f53\u751f\u6210\u6846\u67b6\u548c\u611f\u77e5\u516c\u5e73\u6027\u9a8c\u8bc1\u534f\u8bae\u521b\u5efa2,127\u4e2a\u4efb\u52a1\u5b9e\u4f8b\u3002", "result": "\u7cfb\u7edf\u8bc4\u4f30\u4e09\u79cd\u4ee3\u8868\u6027\u4f53\u73b0\u4ee3\u7406\uff0c\u53d1\u73b0\u6240\u6709\u4ee3\u7406\u5747\u8868\u73b0\u51fa\u663e\u8457\u7684\u89c6\u89c9\u504f\u5dee\uff1b\u76f8\u673a\u89c6\u89d2\u662f\u6700\u5173\u952e\u56e0\u7d20\uff1b\u4ee3\u7406\u5728\u9ad8\u5ea6\u9971\u548c\u989c\u8272\u4e0a\u6210\u529f\u7387\u6700\u9ad8\uff1b\u89c6\u89c9\u504f\u5dee\u5448\u73b0\u51fa\u5f3a\u800c\u4e0d\u5bf9\u79f0\u7684\u8026\u5408\u3002", "conclusion": "\u7cfb\u7edf\u5206\u6790\u89c6\u89c9\u504f\u5dee\u662f\u5f00\u53d1\u5b89\u5168\u53ef\u9760\u7684\u901a\u7528\u4f53\u73b0\u4ee3\u7406\u7684\u5148\u51b3\u6761\u4ef6\u3002"}}
{"id": "2509.22421", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22421", "abs": "https://arxiv.org/abs/2509.22421", "authors": ["Leonel Giacobbe", "Jingdao Chen", "Chuangchuang Sun"], "title": "Learning-Based Collaborative Control for Bi-Manual Tactile-Reactive Grasping", "comment": null, "summary": "Grasping is a core task in robotics with various applications. However, most\ncurrent implementations are primarily designed for rigid items, and their\nperformance drops considerably when handling fragile or deformable materials\nthat require real-time feedback. Meanwhile, tactile-reactive grasping focuses\non a single agent, which limits their ability to grasp and manipulate large,\nheavy objects. To overcome this, we propose a learning-based, tactile-reactive\nmulti-agent Model Predictive Controller (MPC) for grasping a wide range of\nobjects with different softness and shapes, beyond the capabilities of\npreexisting single-agent implementations. Our system uses two Gelsight Mini\ntactile sensors [1] to extract real-time information on object texture and\nstiffness. This rich tactile feedback is used to estimate contact dynamics and\nobject compliance in real time, enabling the system to adapt its control policy\nto diverse object geometries and stiffness profiles. The learned controller\noperates in a closed loop, leveraging tactile encoding to predict grasp\nstability and adjust force and position accordingly. Our key technical\ncontributions include a multi-agent MPC formulation trained on real contact\ninteractions, a tactile-data driven method for inferring grasping states, and a\ncoordination strategy that enables collaborative control. By combining tactile\nsensing and a learning-based multi-agent MPC, our method offers a robust,\nintelligent solution for collaborative grasping in complex environments,\nsignificantly advancing the capabilities of multi-agent systems. Our approach\nis validated through extensive experiments against independent PD and MPC\nbaselines. Our pipeline outperforms the baselines regarding success rates in\nachieving and maintaining stable grasps across objects of varying sizes and\nstiffness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u591a\u667a\u80fd\u4f53\u89e6\u89c9\u53cd\u5e94\u63a7\u5236\u65b9\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u4e0d\u540c\u8f6f\u786c\u7269\u4f53\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u6293\u53d6\u6027\u80fd\u3002", "motivation": "\u76ee\u524d\u7684\u6293\u53d6\u6280\u672f\u4e3b\u8981\u9488\u5bf9\u521a\u6027\u7269\u4f53\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u8106\u5f31\u6216\u53ef\u53d8\u5f62\u6750\u6599\uff0c\u4e14\u5355\u667a\u80fd\u4f53\u65b9\u6cd5\u9650\u5236\u4e86\u5927\u91cd\u7269\u7684\u64cd\u4f5c\u80fd\u529b\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668 (MPC) \uff0c\u7ed3\u5408\u89e6\u89c9\u4f20\u611f\u5668\u5b9e\u65f6\u611f\u77e5\u7269\u4f53\u7279\u6027\u3002", "result": "\u5728\u591a\u79cd\u5c3a\u5bf8\u548c\u786c\u5ea6\u7684\u7269\u4f53\u4e0a\u5b9e\u73b0\u4e86\u8f83\u9ad8\u7684\u6293\u53d6\u6210\u529f\u7387\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684PD\u548cMPC\u57fa\u7ebf\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u591a\u667a\u80fd\u4f53\u89e6\u89c9\u53cd\u5e94\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u5404\u79cd\u8f6f\u786c\u7269\u4f53\uff0c\u5e76\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u7a33\u5b9a\u7684\u534f\u4f5c\u6293\u53d6\u3002"}}
{"id": "2509.22434", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22434", "abs": "https://arxiv.org/abs/2509.22434", "authors": ["Margherita Martorana", "Francesca Urgese", "Ilaria Tiddi", "Stefan Schlobach"], "title": "An Ontology for Unified Modeling of Tasks, Actions, Environments, and Capabilities in Personal Service Robotics", "comment": null, "summary": "Personal service robots are increasingly used in domestic settings to assist\nolder adults and people requiring support. Effective operation involves not\nonly physical interaction but also the ability to interpret dynamic\nenvironments, understand tasks, and choose appropriate actions based on\ncontext. This requires integrating both hardware components (e.g. sensors,\nactuators) and software systems capable of reasoning about tasks, environments,\nand robot capabilities. Frameworks such as the Robot Operating System (ROS)\nprovide open-source tools that help connect low-level hardware with\nhigher-level functionalities. However, real-world deployments remain tightly\ncoupled to specific platforms. As a result, solutions are often isolated and\nhard-coded, limiting interoperability, reusability, and knowledge sharing.\nOntologies and knowledge graphs offer a structured way to represent tasks,\nenvironments, and robot capabilities. Existing ontologies, such as the\nSocio-physical Model of Activities (SOMA) and the Descriptive Ontology for\nLinguistic and Cognitive Engineering (DOLCE), provide models for activities,\nspatial relationships, and reasoning structures. However, they often focus on\nspecific domains and do not fully capture the connection between environment,\naction, robot capabilities, and system-level integration. In this work, we\npropose the Ontology for roBOts and acTions (OntoBOT), which extends existing\nontologies to provide a unified representation of tasks, actions, environments,\nand capabilities. Our contributions are twofold: (1) we unify these aspects\ninto a cohesive ontology to support formal reasoning about task execution, and\n(2) we demonstrate its generalizability by evaluating competency questions\nacross four embodied agents - TIAGo, HSR, UR3, and Stretch - showing how\nOntoBOT enables context-aware reasoning, task-oriented execution, and knowledge\nsharing in service robotics.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faOntoBOT\u672c\u4f53\uff0c\u65e8\u5728\u7edf\u4e00\u8868\u793a\u670d\u52a1\u673a\u5668\u4eba\u4e2d\u7684\u4efb\u52a1\u3001\u884c\u52a8\u548c\u73af\u5883\uff0c\u589e\u5f3a\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u77e5\u8bc6\u5171\u4eab\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u672c\u4f53\u5728\u7279\u5b9a\u9886\u57df\u805a\u7126\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u4e2a\u53ef\u4ee5\u66f4\u5e7f\u6cdb\u9002\u7528\u7684\u6846\u67b6\uff0c\u4ee5\u652f\u6301\u52a8\u6001\u73af\u5883\u4e0b\u673a\u5668\u4eba\u7684\u6709\u6548\u64cd\u4f5c\u548c\u77e5\u8bc6\u5171\u4eab\u3002", "method": "\u901a\u8fc7\u5c06\u73b0\u6709\u672c\u4f53\u8fdb\u884c\u6269\u5c55\uff0c\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u672c\u4f53OntoBOT\uff0c\u5e76\u9488\u5bf9\u56db\u4e2a\u5177\u4f53\u673a\u5668\u4eba\u8fdb\u884c\u4e86\u80fd\u529b\u95ee\u9898\u7684\u8bc4\u4f30\u3002", "result": "OntoBOT\u5b9e\u73b0\u4e86\u4efb\u52a1\u3001\u884c\u52a8\u3001\u73af\u5883\u4e0e\u673a\u5668\u4eba\u80fd\u529b\u4e4b\u95f4\u7684\u7edf\u4e00\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u56db\u4e2a\u673a\u5668\u4eba\u6848\u4f8b\u5c55\u793a\u4e86\u5176\u5728\u670d\u52a1\u673a\u5668\u4eba\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684OntoBOT\u672c\u4f53\u4e3a\u4efb\u52a1\u3001\u884c\u52a8\u3001\u73af\u5883\u548c\u80fd\u529b\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u8868\u793a\uff0c\u4fc3\u8fdb\u4e86\u670d\u52a1\u673a\u5668\u4eba\u9886\u57df\u4e2d\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u63a8\u7406\u3001\u4efb\u52a1\u5bfc\u5411\u6267\u884c\u548c\u77e5\u8bc6\u5171\u4eab\u3002"}}
{"id": "2509.22441", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22441", "abs": "https://arxiv.org/abs/2509.22441", "authors": ["Zhangyuan Wang", "Yunpeng Zhu", "Yuqi Yan", "Xiaoyuan Tian", "Xinhao Shao", "Meixuan Li", "Weikun Li", "Guangsheng Su", "Weicheng Cui", "Dixia Fan"], "title": "UnderwaterVLA: Dual-brain Vision-Language-Action architecture for Autonomous Underwater Navigation", "comment": "This paper introduces the first VLA framework for AUVs, featuring a\n  dual-brain architecture and zero-data MPC for real-world underwater\n  navigation", "summary": "This paper presents UnderwaterVLA, a novel framework for autonomous\nunderwater navigation that integrates multimodal foundation models with\nembodied intelligence systems. Underwater operations remain difficult due to\nhydrodynamic disturbances, limited communication bandwidth, and degraded\nsensing in turbid waters. To address these challenges, we introduce three\ninnovations. First, a dual-brain architecture decouples high-level mission\nreasoning from low-level reactive control, enabling robust operation under\ncommunication and computational constraints. Second, we apply\nVision-Language-Action(VLA) models to underwater robotics for the first time,\nincorporating structured chain-of-thought reasoning for interpretable\ndecision-making. Third, a hydrodynamics-informed Model Predictive Control(MPC)\nscheme compensates for fluid effects in real time without costly task-specific\ntraining. Experimental results in field tests show that UnderwaterVLA reduces\nnavigation errors in degraded visual conditions while maintaining higher task\ncompletion by 19% to 27% over baseline. By minimizing reliance on\nunderwater-specific training data and improving adaptability across\nenvironments, UnderwaterVLA provides a scalable and cost-effective path toward\nthe next generation of intelligent AUVs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u4e3b\u6c34\u4e0b\u5bfc\u822a\u6846\u67b6UnderwaterVLA\uff0c\u9488\u5bf9\u6c34\u4e0b\u64cd\u4f5c\u4e2d\u7684\u591a\u79cd\u6311\u6218\uff0c\u901a\u8fc7\u521b\u65b0\u6280\u672f\u663e\u8457\u63d0\u9ad8\u4e86\u5bfc\u822a\u7cbe\u5ea6\u548c\u4efb\u52a1\u6548\u7387\uff0c\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u7ecf\u6d4e\u6027\u3002", "motivation": "\u6c34\u4e0b\u64cd\u4f5c\u9762\u4e34\u6c34\u6d41\u5e72\u6270\u3001\u901a\u4fe1\u5e26\u5bbd\u9650\u5236\u53ca\u6d51\u6d4a\u6c34\u57df\u611f\u77e5\u80fd\u529b\u4e0b\u964d\u7b49\u6311\u6218\uff0c\u4e9f\u9700\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u6765\u63d0\u9ad8\u81ea\u4e3b\u6c34\u4e0b\u5bfc\u822a\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u8111\u67b6\u6784\uff0c\u5e76\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8\u6a21\u578b\u53ca\u6c34\u52a8\u529b\u5b66\u4fe1\u606f\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u65b9\u6848\uff0c\u8fdb\u884c\u81ea\u4e3b\u6c34\u4e0b\u5bfc\u822a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cUnderwaterVLA\u5728\u6076\u52a3\u89c6\u89c9\u6761\u4ef6\u4e0b\u51cf\u5c11\u4e86\u5bfc\u822a\u8bef\u5dee\uff0c\u540c\u65f6\u6bd4\u57fa\u7ebf\u63d0\u9ad8\u4e8619%\u523027%\u7684\u4efb\u52a1\u5b8c\u6210\u7387\u3002", "conclusion": "UnderwaterVLA\u6846\u67b6\u901a\u8fc7\u51cf\u5c11\u5bfc\u822a\u9519\u8bef\u548c\u63d0\u9ad8\u4efb\u52a1\u5b8c\u6210\u7387\uff0c\u4e3a\u672a\u6765\u7684\u667a\u80fd\u81ea\u4e3b\u6c34\u4e0b\u5668\u5177\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.22469", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22469", "abs": "https://arxiv.org/abs/2509.22469", "authors": ["Ben Rossano", "Jaein Lim", "Jonathan P. How"], "title": "Uncertainty-Aware Multi-Robot Task Allocation With Strongly Coupled Inter-Robot Rewards", "comment": "8 pages", "summary": "This paper proposes a task allocation algorithm for teams of heterogeneous\nrobots in environments with uncertain task requirements. We model these\nrequirements as probability distributions over capabilities and use this model\nto allocate tasks such that robots with complementary skills naturally position\nnear uncertain tasks, proactively mitigating task failures without wasting\nresources. We introduce a market-based approach that optimizes the joint team\nobjective while explicitly capturing coupled rewards between robots, offering a\npolynomial-time solution in decentralized settings with strict communication\nassumptions. Comparative experiments against benchmark algorithms demonstrate\nthe effectiveness of our approach and highlight the challenges of incorporating\ncoupled rewards in a decentralized formulation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4efb\u52a1\u5206\u914d\u7b97\u6cd5\uff0c\u4f18\u5316\u5f02\u6784\u673a\u5668\u4eba\u5728\u4e0d\u786e\u5b9a\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u6982\u7387\u5efa\u6a21\u4efb\u52a1\u9700\u6c42\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u5728\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u4e2d\uff0c\u4efb\u52a1\u8981\u6c42\u7684\u4e0d\u786e\u5b9a\u6027\u4f1a\u5bfc\u81f4\u8d44\u6e90\u6d6a\u8d39\u548c\u4efb\u52a1\u5931\u8d25\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u5206\u914d\u7b97\u6cd5\u6765\u7ba1\u7406\u8fd9\u4e9b\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5e02\u573a\u57fa\u7840\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6982\u7387\u5206\u5e03\u5efa\u6a21\u4efb\u52a1\u9700\u6c42\uff0c\u4f18\u5316\u56e2\u961f\u7684\u8054\u5408\u76ee\u6807\u3002", "result": "\u4e0e\u57fa\u51c6\u7b97\u6cd5\u7684\u5bf9\u6bd4\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u7b97\u6cd5\u5728\u5206\u914d\u4efb\u52a1\u65b9\u9762\u66f4\u4e3a\u6709\u6548\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u5728\u53bb\u4e2d\u5fc3\u5316\u8bbe\u7f6e\u4e0b\uff0c\u8026\u5408\u5956\u52b1\u7684\u5f15\u5165\u6240\u9762\u4e34\u7684\u6311\u6218\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u6709\u6548\u4f18\u5316\u4e86\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u5728\u4e0d\u786e\u5b9a\u4efb\u52a1\u4e2d\u7684\u8d44\u6e90\u5206\u914d\uff0c\u63d0\u5347\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3002"}}
{"id": "2509.22493", "categories": ["cs.RO", "cs.AI", "cs.IR", "cs.LO"], "pdf": "https://arxiv.org/pdf/2509.22493", "abs": "https://arxiv.org/abs/2509.22493", "authors": ["Alberto Olivares-Alarcos", "Sergi Foix", "J\u00falia Borr\u00e0s", "Gerard Canal", "Guillem Aleny\u00e0"], "title": "Ontological foundations for contrastive explanatory narration of robot plans", "comment": "This version was submitted to the journal Information Sciences and is\n  under review since October 2024", "summary": "Mutual understanding of artificial agents' decisions is key to ensuring a\ntrustworthy and successful human-robot interaction. Hence, robots are expected\nto make reasonable decisions and communicate them to humans when needed. In\nthis article, the focus is on an approach to modeling and reasoning about the\ncomparison of two competing plans, so that robots can later explain the\ndivergent result. First, a novel ontological model is proposed to formalize and\nreason about the differences between competing plans, enabling the\nclassification of the most appropriate one (e.g., the shortest, the safest, the\nclosest to human preferences, etc.). This work also investigates the\nlimitations of a baseline algorithm for ontology-based explanatory narration.\nTo address these limitations, a novel algorithm is presented, leveraging\ndivergent knowledge between plans and facilitating the construction of\ncontrastive narratives. Through empirical evaluation, it is observed that the\nexplanations excel beyond the baseline method.", "AI": {"tldr": "\u673a\u5668\u4eba\u9700\u8981\u5408\u7406\u51b3\u7b56\u5e76\u4e0e\u4eba\u7c7b\u89e3\u91ca\u5176\u51b3\u7b56\uff0c\u672c\u6587\u63d0\u51fa\u65b0\u6a21\u578b\u548c\u7b97\u6cd5\u4ee5\u63d0\u5347\u8fd9\u4e00\u80fd\u529b\u5e76\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "motivation": "\u4e3a\u4e86\u786e\u4fdd\u4eba\u673a\u4ea4\u4e92\u7684\u53ef\u4fe1\u6027\u548c\u6210\u529f\u6027\uff0c\u673a\u5668\u4eba\u9700\u8981\u4f5c\u51fa\u5408\u7406\u7684\u51b3\u7b56\u5e76\u5728\u9700\u8981\u65f6\u8fdb\u884c\u89e3\u91ca\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u672c\u4f53\u6a21\u578b\u6765\u5f62\u5f0f\u5316\u548c\u63a8\u7406\u7ade\u4e89\u8ba1\u5212\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u5e76\u901a\u8fc7\u65b0\u7b97\u6cd5\u6784\u5efa\u5bf9\u6bd4\u53d9\u8ff0\u3002", "result": "\u901a\u8fc7\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u53d1\u73b0\u672c\u6587\u63d0\u51fa\u7684\u89e3\u91ca\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6a21\u578b\u548c\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u7684\u51b3\u7b56\u89e3\u91ca\u80fd\u529b\uff0c\u589e\u5f3a\u4eba\u673a\u4e92\u52a8\u7684\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2509.22498", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22498", "abs": "https://arxiv.org/abs/2509.22498", "authors": ["Katrina Ashton", "Chahyon Ku", "Shrey Shah", "Wen Jiang", "Kostas Daniilidis", "Bernadette Bucher"], "title": "HELIOS: Hierarchical Exploration for Language-grounded Interaction in Open Scenes", "comment": null, "summary": "Language-specified mobile manipulation tasks in novel environments\nsimultaneously face challenges interacting with a scene which is only partially\nobserved, grounding semantic information from language instructions to the\npartially observed scene, and actively updating knowledge of the scene with new\nobservations. To address these challenges, we propose HELIOS, a hierarchical\nscene representation and associated search objective to perform language\nspecified pick and place mobile manipulation tasks. We construct 2D maps\ncontaining the relevant semantic and occupancy information for navigation while\nsimultaneously actively constructing 3D Gaussian representations of\ntask-relevant objects. We fuse observations across this multi-layered\nrepresentation while explicitly modeling the multi-view consistency of the\ndetections of each object. In order to efficiently search for the target\nobject, we formulate an objective function balancing exploration of unobserved\nor uncertain regions with exploitation of scene semantic information. We\nevaluate HELIOS on the OVMM benchmark in the Habitat simulator, a pick and\nplace benchmark in which perception is challenging due to large and complex\nscenes with comparatively small target objects. HELIOS achieves\nstate-of-the-art results on OVMM. As our approach is zero-shot, HELIOS can also\ntransfer to the real world without requiring additional data, as we illustrate\nby demonstrating it in a real world office environment on a Spot robot.", "AI": {"tldr": "HELIOS\u65e8\u5728\u89e3\u51b3\u8bed\u8a00\u6307\u5b9a\u79fb\u52a8\u64cd\u63a7\u4efb\u52a1\u4e2d\u7684\u90e8\u5206\u89c2\u6d4b\u548c\u77e5\u8bc6\u66f4\u65b0\u95ee\u9898\uff0c\u4ee5\u5148\u8fdb\u6280\u672f\u5728OVMM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u53ef\u65e0\u7f1d\u8f6c\u79fb\u81f3\u771f\u5b9e\u4e16\u754c\u3002", "motivation": "\u89e3\u51b3\u8bed\u8a00\u6307\u5b9a\u7684\u79fb\u52a8\u64cd\u63a7\u4efb\u52a1\u5728\u90e8\u5206\u89c2\u5bdf\u573a\u666f\u4e2d\u7684\u6311\u6218\uff0c\u5305\u62ec\u8bed\u4e49\u4fe1\u606f\u7684\u5b9a\u4f4d\u548c\u573a\u666f\u77e5\u8bc6\u7684\u66f4\u65b0\u3002", "method": "\u521b\u5efa2D\u5730\u56fe\u548c3D\u9ad8\u65af\u8868\u793a\uff0c\u878d\u5408\u591a\u5c42\u6b21\u89c2\u6d4b\uff0c\u5229\u7528\u76ee\u6807\u51fd\u6570\u5728\u672a\u89c2\u5bdf\u6216\u4e0d\u786e\u5b9a\u533a\u57df\u4e0e\u573a\u666f\u8bed\u4e49\u4fe1\u606f\u95f4\u8fdb\u884c\u6709\u6548\u641c\u7d22\u3002", "result": "HELIOS\u5728Habitat\u6a21\u62df\u5668\u4e2d\u7684OVMM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u6210\u529f\u6f14\u793a\u3002", "conclusion": "HELIOS\u5728OVMM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u5728\u4e0d\u9700\u8981\u989d\u5916\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u8f6c\u79fb\u5230\u771f\u5b9e\u73af\u5883\u4e2d\u3002"}}
{"id": "2509.22550", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22550", "abs": "https://arxiv.org/abs/2509.22550", "authors": ["Xiaoyun Qiu", "Haichao Liu", "Yue Pan", "Jun Ma", "Xinhu Zheng"], "title": "An Intention-driven Lane Change Framework Considering Heterogeneous Dynamic Cooperation in Mixed-traffic Environment", "comment": null, "summary": "In mixed-traffic environments, where autonomous vehicles (AVs) interact with\ndiverse human-driven vehicles (HVs), unpredictable intentions and heterogeneous\nbehaviors make safe and efficient lane change maneuvers highly challenging.\nExisting methods often oversimplify these interactions by assuming uniform\npatterns. We propose an intention-driven lane change framework that integrates\ndriving-style recognition, cooperation-aware decision-making, and coordinated\nmotion planning. A deep learning classifier trained on the NGSIM dataset\nidentifies human driving styles in real time. A cooperation score with\nintrinsic and interactive components estimates surrounding drivers' intentions\nand quantifies their willingness to cooperate with the ego vehicle.\nDecision-making combines behavior cloning with inverse reinforcement learning\nto determine whether a lane change should be initiated. For trajectory\ngeneration, model predictive control is integrated with IRL-based intention\ninference to produce collision-free and socially compliant maneuvers.\nExperiments show that the proposed model achieves 94.2\\% accuracy and 94.3\\%\nF1-score, outperforming rule-based and learning-based baselines by 4-15\\% in\nlane change recognition. These results highlight the benefit of modeling\ninter-driver heterogeneity and demonstrate the potential of the framework to\nadvance context-aware and human-like autonomous driving in complex traffic\nenvironments.", "AI": {"tldr": "\u5728\u6df7\u5408\u4ea4\u901a\u73af\u5883\u4e2d\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u610f\u56fe\u9a71\u52a8\u7684\u6362\u9053\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u4e3b\u8f66\u8f86\u5728\u4eba\u7c7b\u9a7e\u9a76\u98ce\u683c\u5f02\u8d28\u6027\u7ba1\u7406\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u5728\u6df7\u5408\u4ea4\u901a\u73af\u5883\u4e2d\uff0c\u81ea\u4e3b\u8f66\u8f86\u4e0e\u591a\u6837\u5316\u7684\u4eba\u7c7b\u9a7e\u9a76\u8f66\u8f86\u4e4b\u95f4\u7684\u4e0d\u786e\u5b9a\u610f\u56fe\u548c\u5f02\u8d28\u884c\u4e3a\u4f7f\u5f97\u5b89\u5168\u548c\u9ad8\u6548\u7684\u6362\u9053\u64cd\u4f5c\u53d8\u5f97\u6781\u5177\u6311\u6218\u6027\u3002", "method": "\u96c6\u6210\u4e86\u9a7e\u9a76\u98ce\u683c\u8bc6\u522b\u3001\u5408\u4f5c\u610f\u8bc6\u51b3\u7b56\u548c\u534f\u8c03\u8fd0\u52a8\u89c4\u5212\u7684\u610f\u56fe\u9a71\u52a8\u6362\u9053\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u6362\u9053\u8bc6\u522b\u65b9\u9762\u8fbe\u523094.2%\u7684\u51c6\u786e\u7387\u548c94.3%\u7684F1-score\uff0c\u8d85\u8d8a\u4e86\u57fa\u4e8e\u89c4\u5219\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u57fa\u7ebf4-15%\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u5728\u590d\u6742\u4ea4\u901a\u73af\u5883\u4e2d\u5177\u6709\u9002\u5e94\u6027\u548c\u4eba\u6027\u5316\u7684\u81ea\u4e3b\u9a7e\u9a76\u6f5c\u529b\uff0c\u51c6\u786e\u53cd\u6620\u4e86\u4e0d\u540c\u9a7e\u9a76\u98ce\u683c\u7684\u4ea4\u4e92\u3002"}}
{"id": "2509.22573", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22573", "abs": "https://arxiv.org/abs/2509.22573", "authors": ["Farida Mohsen", "Ali Safa"], "title": "MINT-RVAE: Multi-Cues Intention Prediction of Human-Robot Interaction using Human Pose and Emotion Information from RGB-only Camera Data", "comment": null, "summary": "Efficiently detecting human intent to interact with ubiquitous robots is\ncrucial for effective human-robot interaction (HRI) and collaboration. Over the\npast decade, deep learning has gained traction in this field, with most\nexisting approaches relying on multimodal inputs, such as RGB combined with\ndepth (RGB-D), to classify time-sequence windows of sensory data as interactive\nor non-interactive. In contrast, we propose a novel RGB-only pipeline for\npredicting human interaction intent with frame-level precision, enabling faster\nrobot responses and improved service quality. A key challenge in intent\nprediction is the class imbalance inherent in real-world HRI datasets, which\ncan hinder the model's training and generalization. To address this, we\nintroduce MINT-RVAE, a synthetic sequence generation method, along with new\nloss functions and training strategies that enhance generalization on\nout-of-sample data. Our approach achieves state-of-the-art performance (AUROC:\n0.95) outperforming prior works (AUROC: 0.90-0.912), while requiring only RGB\ninput and supporting precise frame onset prediction. Finally, to support future\nresearch, we openly release our new dataset with frame-level labeling of human\ninteraction intent.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u4f9d\u8d56RGB\u6570\u636e\u9884\u6d4b\u4eba\u7c7b\u4e92\u52a8\u610f\u56fe\uff0c\u89e3\u51b3\u4e86\u7c7b\u4e0d\u5e73\u8861\u7b49\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\uff0c\u5e76\u53d1\u5e03\u4e86\u65b0\u6570\u636e\u96c6\u3002", "motivation": "\u63d0\u9ad8\u4eba-\u673a\u5668\u4eba\u4ea4\u4e92\u7684\u6709\u6548\u6027\uff0c\u5e76\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u5bf9\u591a\u6a21\u6001\u8f93\u5165\u7684\u4f9d\u8d56\u3002", "method": "\u5f15\u5165\u4e86MINT-RVAE\u5408\u6210\u5e8f\u5217\u751f\u6210\u65b9\u6cd5\uff0c\u7ed3\u5408\u65b0\u7684\u635f\u5931\u51fd\u6570\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u6765\u89e3\u51b3\u7c7b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u5728\u72b6\u6001\u6700\u5148\u8fdb\u7684\u6027\u80fd\u4e0a\u8fbe\u5230\u4e86AUROC: 0.95\uff0c\u8d85\u8fc7\u4e86\u4e4b\u524d\u7684\u5de5\u4f5c\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f7f\u7528RGB\u8f93\u5165\u9884\u6d4b\u4eba\u7c7b\u4e92\u52a8\u610f\u56fe\u7684\u65b9\u6cd5\uff0c\u5e76\u53d6\u5f97\u4e86\u51fa\u8272\u7684\u7ed3\u679c\u3002"}}
{"id": "2509.22578", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22578", "abs": "https://arxiv.org/abs/2509.22578", "authors": ["Yuan Xu", "Jiabing Yang", "Xiaofeng Wang", "Yixiang Chen", "Zheng Zhu", "Bowen Fang", "Guan Huang", "Xinze Chen", "Yun Ye", "Qiang Zhang", "Peiyan Li", "Xiangnan Wu", "Kai Wang", "Bing Zhan", "Shuo Lu", "Jing Liu", "Nianfeng Liu", "Yan Huang", "Liang Wang"], "title": "EgoDemoGen: Novel Egocentric Demonstration Generation Enables Viewpoint-Robust Manipulation", "comment": null, "summary": "Imitation learning based policies perform well in robotic manipulation, but\nthey often degrade under *egocentric viewpoint shifts* when trained from a\nsingle egocentric viewpoint. To address this issue, we present **EgoDemoGen**,\na framework that generates *paired* novel egocentric demonstrations by\nretargeting actions in the novel egocentric frame and synthesizing the\ncorresponding egocentric observation videos with proposed generative video\nrepair model **EgoViewTransfer**, which is conditioned by a novel-viewpoint\nreprojected scene video and a robot-only video rendered from the retargeted\njoint actions. EgoViewTransfer is finetuned from a pretrained video generation\nmodel using self-supervised double reprojection strategy. We evaluate\nEgoDemoGen on both simulation (RoboTwin2.0) and real-world robot. After\ntraining with a mixture of EgoDemoGen-generated novel egocentric demonstrations\nand original standard egocentric demonstrations, policy success rate improves\n**absolutely** by **+17.0%** for standard egocentric viewpoint and by\n**+17.7%** for novel egocentric viewpoints in simulation. On real-world robot,\nthe **absolute** improvements are **+18.3%** and **+25.8%**. Moreover,\nperformance continues to improve as the proportion of EgoDemoGen-generated\ndemonstrations increases, with diminishing returns. These results demonstrate\nthat EgoDemoGen provides a practical route to egocentric viewpoint-robust\nrobotic manipulation.", "AI": {"tldr": "EgoDemoGen\u901a\u8fc7\u751f\u6210\u65b0\u7684\u7b2c\u4e00\u4eba\u79f0\u793a\u8303\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u89c6\u89d2\u53d8\u5316\u4e0b\u7684\u64cd\u63a7\u6027\u80fd\uff0c\u4eff\u771f\u548c\u5b9e\u9645\u6d4b\u8bd5\u4e2d\u5747\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u76ee\u524d\u7684\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u5728\u5904\u7406\u5355\u4e00\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u89c6\u89d2\u53d8\u5316\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u6025\u9700\u6539\u5584\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u751f\u6210\u914d\u5bf9\u7684\u65b0\u7684\u7b2c\u4e00\u4eba\u79f0\u793a\u8303\u5e76\u7ed3\u5408\u751f\u6210\u7684\u89c6\u9891\u4fee\u590d\u6a21\u578bEgoViewTransfer\uff0cEgoDemoGen\u6709\u6548\u5730\u5e94\u5bf9\u4e86\u89c6\u89d2\u53d8\u5316\u5e26\u6765\u7684\u5f71\u54cd\u3002", "result": "\u5728\u4eff\u771f\u73af\u5883\u4e2d\uff0cEgoDemoGen\u5728\u6807\u51c6\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u548c\u65b0\u7684\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u4e0a\u7684\u7b56\u7565\u6210\u529f\u7387\u5206\u522b\u63d0\u9ad8\u4e8617.0%\u548c17.7%\uff1b\u5728\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u6d4b\u8bd5\u4e2d\uff0c\u7edd\u5bf9\u63d0\u9ad8\u5206\u522b\u4e3a18.3%\u548c25.8%\u3002", "conclusion": "EgoDemoGen\u4e3a\u5b9e\u73b0\u4ee5\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u4e3a\u57fa\u7840\u7684\u7a33\u5065\u673a\u5668\u4eba\u64cd\u63a7\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u65b0\u7684\u793a\u8303\u6765\u6539\u5584\u64cd\u63a7\u6027\u80fd\u3002"}}
{"id": "2509.22642", "categories": ["cs.RO", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.22642", "abs": "https://arxiv.org/abs/2509.22642", "authors": ["Xiaowei Chi", "Peidong Jia", "Chun-Kai Fan", "Xiaozhu Ju", "Weishi Mi", "Kevin Zhang", "Zhiyuan Qin", "Wanxin Tian", "Kuangzhi Ge", "Hao Li", "Zezhong Qian", "Anthony Chen", "Qiang Zhou", "Yueru Jia", "Jiaming Liu", "Yong Dai", "Qingpo Wuwu", "Chengyu Bai", "Yu-Kai Wang", "Ying Li", "Lizhang Chen", "Yong Bao", "Zhiyuan Jiang", "Jiacheng Zhu", "Kai Tang", "Ruichuan An", "Yulin Luo", "Qiuxuan Feng", "Siyuan Zhou", "Chi-min Chan", "Chengkai Hou", "Wei Xue", "Sirui Han", "Yike Guo", "Shanghang Zhang", "Jian Tang"], "title": "WoW: Towards a World omniscient World model Through Embodied Interaction", "comment": null, "summary": "Humans develop an understanding of intuitive physics through active\ninteraction with the world. This approach is in stark contrast to current video\nmodels, such as Sora, which rely on passive observation and therefore struggle\nwith grasping physical causality. This observation leads to our central\nhypothesis: authentic physical intuition of the world model must be grounded in\nextensive, causally rich interactions with the real world. To test this\nhypothesis, we present WoW, a 14-billion-parameter generative world model\ntrained on 2 million robot interaction trajectories. Our findings reveal that\nthe model's understanding of physics is a probabilistic distribution of\nplausible outcomes, leading to stochastic instabilities and physical\nhallucinations. Furthermore, we demonstrate that this emergent capability can\nbe actively constrained toward physical realism by SOPHIA, where\nvision-language model agents evaluate the DiT-generated output and guide its\nrefinement by iteratively evolving the language instructions. In addition, a\nco-trained Inverse Dynamics Model translates these refined plans into\nexecutable robotic actions, thus closing the imagination-to-action loop. We\nestablish WoWBench, a new benchmark focused on physical consistency and causal\nreasoning in video, where WoW achieves state-of-the-art performance in both\nhuman and autonomous evaluation, demonstrating strong ability in physical\ncausality, collision dynamics, and object permanence. Our work provides\nsystematic evidence that large-scale, real-world interaction is a cornerstone\nfor developing physical intuition in AI. Models, data, and benchmarks will be\nopen-sourced.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faWoW\u751f\u6210\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u4ea4\u4e92\u63d0\u5347AI\u7684\u7269\u7406\u76f4\u89c9\uff0c\u5efa\u7acb\u65b0\u57fa\u51c6WoWBench\uff0c\u8bc1\u660e\u5927\u89c4\u6a21\u4ea4\u4e92\u5bf9\u7269\u7406\u7406\u89e3\u7684\u91cd\u8981\u6027\uff0c\u5177\u6709\u5f00\u6e90\u4ef7\u503c\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u89c6\u9891\u6a21\u578b\uff08\u5982Sora\uff09\u56e0\u4f9d\u8d56\u88ab\u52a8\u89c2\u5bdf\u800c\u96be\u4ee5\u638c\u63e1\u7269\u7406\u56e0\u679c\u5173\u7cfb\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u7684\u4e30\u5bcc\u56e0\u679c\u4ea4\u4e92\u6765\u5efa\u7acb\u7269\u7406\u76f4\u89c9\u7684\u5047\u8bbe\u3002", "method": "\u63d0\u51faWoW\uff0c\u4e00\u4e2a\u57fa\u4e8e140\u4ebf\u53c2\u6570\u7684\u751f\u6210\u4e16\u754c\u6a21\u578b\uff0c\u6839\u636e200\u4e07\u6761\u673a\u5668\u4eba\u4ea4\u4e92\u8f68\u8ff9\u8fdb\u884c\u8bad\u7ec3\uff0c\u540c\u65f6\u4f7f\u7528SOPHIA\u548c\u5171\u8bad\u7ec3\u7684\u53cd\u5411\u52a8\u529b\u5b66\u6a21\u578b\u7ea6\u675f\u7269\u7406\u73b0\u5b9e\u6027\uff0c\u5e76\u5c06\u5176\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684\u673a\u5668\u4eba\u52a8\u4f5c\u3002", "result": "WoW\u6a21\u578b\u7684\u7269\u7406\u7406\u89e3\u8868\u73b0\u4e3a\u53ef\u80fd\u7ed3\u679c\u7684\u6982\u7387\u5206\u5e03\uff0c\u5bfc\u81f4\u968f\u673a\u4e0d\u7a33\u5b9a\u6027\u4e0e\u7269\u7406\u5e7b\u89c9\u3002WoW\u5728\u4eba\u7c7b\u548c\u81ea\u4e3b\u8bc4\u4f30\u4e2d\u5747\u5b9e\u73b0\u4e86\u6700\u65b0\u7684\u6027\u80fd\uff0c\u6210\u4e3a\u7269\u7406\u4e00\u81f4\u6027\u548c\u56e0\u679c\u63a8\u7406\u7684\u65b0\u57fa\u51c6WoWBench\u3002", "conclusion": "\u672c\u7814\u7a76\u7cfb\u7edf\u6027\u5730\u8bc1\u660e\u4e86\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u4ea4\u4e92\u662fAI\u53d1\u5c55\u7269\u7406\u76f4\u89c9\u7684\u57fa\u7840\uff0cWoW\u6a21\u578b\u5728\u7269\u7406\u56e0\u679c\u5173\u7cfb\u3001\u78b0\u649e\u52a8\u6001\u548c\u7269\u4f53\u6c38\u4e45\u6027\u7b49\u65b9\u9762\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u80fd\u529b\u3002"}}
{"id": "2509.22643", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22643", "abs": "https://arxiv.org/abs/2509.22643", "authors": ["Wenkai Guo", "Guanxing Lu", "Haoyuan Deng", "Zhenyu Wu", "Yansong Tang", "Ziwei Wang"], "title": "VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning via Online Monte Carlo Tree Search", "comment": "9 pages", "summary": "Vision-Language-Action models (VLAs) achieve strong performance in general\nrobotic manipulation tasks by scaling imitation learning. However, existing\nVLAs are limited to predicting short-sighted next-action, which struggle with\nlong-horizon trajectory tasks due to incremental deviations. To address this\nproblem, we propose a plug-in framework named VLA-Reasoner that effectively\nempowers off-the-shelf VLAs with the capability of foreseeing future states via\ntest-time scaling. Specifically, VLA-Reasoner samples and rolls out possible\naction trajectories where involved actions are rationales to generate future\nstates via a world model, which enables VLA-Reasoner to foresee and reason\npotential outcomes and search for the optimal actions. We further leverage\nMonte Carlo Tree Search (MCTS) to improve search efficiency in large action\nspaces, where stepwise VLA predictions seed the root. Meanwhile, we introduce a\nconfidence sampling mechanism based on Kernel Density Estimation (KDE), to\nenable efficient exploration in MCTS without redundant VLA queries. We evaluate\nintermediate states in MCTS via an offline reward shaping strategy, to score\npredicted futures and correct deviations with long-term feedback. We conducted\nextensive experiments in both simulators and the real world, demonstrating that\nour proposed VLA-Reasoner achieves significant improvements over the\nstate-of-the-art VLAs. Our method highlights a potential pathway toward\nscalable test-time computation of robotic manipulation.", "AI": {"tldr": "VLA-Reasoner\u901a\u8fc7\u589e\u5f3aVLA\u6a21\u578b\u7684\u672a\u6765\u72b6\u6001\u9884\u6d4b\u80fd\u529b\uff0c\u663e\u8457\u6539\u5584\u4e86\u957f\u65f6\u95f4\u8f68\u8ff9\u4efb\u52a1\u7684\u5904\u7406\uff0c\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684VLA\u6a21\u578b\u5728\u5904\u7406\u957f\u671f\u8f68\u8ff9\u4efb\u52a1\u65f6\u8868\u73b0\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u9884\u6d4b\u672a\u6765\u72b6\u6001\uff0c\u63d0\u5347\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u5728\u6d4b\u8bd5\u65f6\u6269\u5c55\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8\u6a21\u578b(VLA)\uff0c\u4f7f\u7528\u4e16\u754c\u6a21\u578b\u8fdb\u884c\u672a\u6765\u72b6\u6001\u7684\u9884\u6d4b\uff0c\u5e76\u7ed3\u5408\u8499\u7279\u5361\u7f57\u6811\u641c\u7d22(MCTS)\u548c\u57fa\u4e8e\u6838\u5bc6\u5ea6\u4f30\u8ba1(KDE)\u7684\u7f6e\u4fe1\u5ea6\u91c7\u6837\u673a\u5236\u3002", "result": "VLA-Reasoner\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709VLA\u6a21\u578b\u5728\u590d\u6742\u64cd\u63a7\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u80fd\u6709\u6548\u5730\u63a8\u6d4b\u6f5c\u5728\u7ed3\u679c\u5e76\u5bfb\u627e\u6700\u4f18\u52a8\u4f5c\u3002", "conclusion": "VLA-Reasoner\u5728\u6a21\u62df\u5668\u548c\u73b0\u5b9e\u4e16\u754c\u4e2d\u5c55\u793a\u4e86\u663e\u8457\u7684\u6539\u8fdb\uff0c\u5f00\u542f\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u53ef\u6269\u5c55\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u7684\u65b0\u8def\u5f84\u3002"}}
{"id": "2509.22652", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22652", "abs": "https://arxiv.org/abs/2509.22652", "authors": ["E-Ro Nguyen", "Yichi Zhang", "Kanchana Ranasinghe", "Xiang Li", "Michael S. Ryoo"], "title": "Pixel Motion Diffusion is What We Need for Robot Control", "comment": "16 pages, 7 figures", "summary": "We present DAWN (Diffusion is All We Need for robot control), a unified\ndiffusion-based framework for language-conditioned robotic manipulation that\nbridges high-level motion intent and low-level robot action via structured\npixel motion representation. In DAWN, both the high-level and low-level\ncontrollers are modeled as diffusion processes, yielding a fully trainable,\nend-to-end system with interpretable intermediate motion abstractions. DAWN\nachieves state-of-the-art results on the challenging CALVIN benchmark,\ndemonstrating strong multi-task performance, and further validates its\neffectiveness on MetaWorld. Despite the substantial domain gap between\nsimulation and reality and limited real-world data, we demonstrate reliable\nreal-world transfer with only minimal finetuning, illustrating the practical\nviability of diffusion-based motion abstractions for robotic control. Our\nresults show the effectiveness of combining diffusion modeling with\nmotion-centric representations as a strong baseline for scalable and robust\nrobot learning. Project page: https://nero1342.github.io/DAWN/", "AI": {"tldr": "DAWN\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u7684\u673a\u5668\u4eba\u63a7\u5236\u6846\u67b6\uff0c\u6709\u6548\u8fde\u63a5\u9ad8\u4f4e\u5c42\u52a8\u4f5c\u610f\u56fe\uff0c\u5b9e\u73b0\u591a\u4efb\u52a1\u8868\u73b0\u548c\u826f\u597d\u7684\u771f\u5b9e\u4e16\u754c\u8fc1\u79fb\u3002", "motivation": "\u901a\u8fc7\u7ed3\u6784\u5316\u50cf\u7d20\u8fd0\u52a8\u8868\u793a\uff0c\u5c06\u9ad8\u5c42\u8fd0\u52a8\u610f\u56fe\u4e0e\u4f4e\u5c42\u673a\u5668\u4eba\u52a8\u4f5c\u8fde\u63a5\u8d77\u6765\uff0c\u4ee5\u63d0\u5347\u673a\u5668\u4eba\u64cd\u63a7\u80fd\u529b\u3002", "method": "DAWN\u5c06\u9ad8\u5c42\u548c\u4f4e\u5c42\u63a7\u5236\u5668\u5747\u5efa\u6a21\u4e3a\u6269\u6563\u8fc7\u7a0b\uff0c\u6784\u5efa\u7edf\u4e00\u7684\u6269\u6563\u6846\u67b6\uff0c\u5177\u6709\u53ef\u8bad\u7ec3\u6027\u548c\u53ef\u89e3\u91ca\u7684\u4e2d\u95f4\u8fd0\u52a8\u62bd\u8c61\u3002", "result": "DAWN\u5728CALVIN\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u5728MetaWorld\u9a8c\u8bc1\u4e86\u5176\u5f3a\u5927\u7684\u591a\u4efb\u52a1\u8868\u73b0\uff0c\u5c55\u793a\u4e86\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u4e4b\u95f4\u7684\u53ef\u9760\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "DAWN\u5c55\u73b0\u4e86\u57fa\u4e8e\u6269\u6563\u5efa\u6a21\u4e0e\u8fd0\u52a8\u4e2d\u5fc3\u8868\u793a\u76f8\u7ed3\u5408\u7684\u6709\u6548\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u57fa\u7ebf\uff0c\u4e14\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u53ea\u9700\u5c11\u91cf\u5fae\u8c03\u5373\u53ef\u6210\u529f\u8fc1\u79fb\u3002"}}
{"id": "2509.22653", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22653", "abs": "https://arxiv.org/abs/2509.22653", "authors": ["Chih Yao Hu", "Yang-Sen Lin", "Yuna Lee", "Chih-Hai Su", "Jie-Ying Lee", "Shr-Ruei Tsai", "Chin-Yang Lin", "Kuan-Wen Chen", "Tsung-Wei Ke", "Yu-Lun Liu"], "title": "See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation", "comment": "CoRL 2025. Project page: https://spf-web.pages.dev", "summary": "We present See, Point, Fly (SPF), a training-free aerial vision-and-language\nnavigation (AVLN) framework built atop vision-language models (VLMs). SPF is\ncapable of navigating to any goal based on any type of free-form instructions\nin any kind of environment. In contrast to existing VLM-based approaches that\ntreat action prediction as a text generation task, our key insight is to\nconsider action prediction for AVLN as a 2D spatial grounding task. SPF\nharnesses VLMs to decompose vague language instructions into iterative\nannotation of 2D waypoints on the input image. Along with the predicted\ntraveling distance, SPF transforms predicted 2D waypoints into 3D displacement\nvectors as action commands for UAVs. Moreover, SPF also adaptively adjusts the\ntraveling distance to facilitate more efficient navigation. Notably, SPF\nperforms navigation in a closed-loop control manner, enabling UAVs to follow\ndynamic targets in dynamic environments. SPF sets a new state of the art in DRL\nsimulation benchmark, outperforming the previous best method by an absolute\nmargin of 63%. In extensive real-world evaluations, SPF outperforms strong\nbaselines by a large margin. We also conduct comprehensive ablation studies to\nhighlight the effectiveness of our design choice. Lastly, SPF shows remarkable\ngeneralization to different VLMs. Project page: https://spf-web.pages.dev", "AI": {"tldr": "SPF \u662f\u4e00\u79cd\u65e0\u8bad\u7ec3\u7684\u7a7a\u4e2d\u89c6\u89c9\u4e0e\u8bed\u8a00\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6a21\u7cca\u6307\u4ee4\u8f6c\u5316\u4e3a 2D \u8def\u5f84\u70b9\u548c 3D \u52a8\u4f5c\u547d\u4ee4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u65e0\u8bad\u7ec3\u7684 AVLN \u6846\u67b6\uff0c\u4f7f UAV \u80fd\u591f\u6839\u636e\u81ea\u7531\u5f62\u5f0f\u7684\u6307\u4ee4\u5728\u5404\u79cd\u73af\u5883\u4e2d\u5bfc\u822a\u3002", "method": "SPF \u5229\u7528 VLMs \u5c06\u6a21\u7cca\u8bed\u8a00\u6307\u4ee4\u5206\u89e3\u4e3a\u8f93\u5165\u56fe\u50cf\u4e0a\u7684 2D \u8def\u5f84\u70b9\uff0c\u5e76\u5c06\u5176\u8f6c\u5316\u4e3a UAV \u7684 3D \u4f4d\u79fb\u5411\u91cf\u4f5c\u4e3a\u884c\u52a8\u6307\u4ee4\u3002", "result": "SPF \u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u5148\u524d\u6700\u4f73\u65b9\u6cd563%\uff0c\u4e14\u5728\u771f\u5b9e\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "SPF \u5728 DRL \u4eff\u771f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u521b\u4e0b\u65b0\u7eaa\u5f55\uff0c\u4e14\u5728\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c55\u793a\u4e86\u5176\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
