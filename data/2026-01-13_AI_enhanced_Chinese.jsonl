{"id": "2601.06286", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.06286", "abs": "https://arxiv.org/abs/2601.06286", "authors": ["Min Dai", "William D. Compton", "Junheng Li", "Lizhi Yang", "Aaron D. Ames"], "title": "Walk the PLANC: Physics-Guided RL for Agile Humanoid Locomotion on Constrained Footholds", "comment": null, "summary": "Bipedal humanoid robots must precisely coordinate balance, timing, and contact decisions when locomoting on constrained footholds such as stepping stones, beams, and planks -- even minor errors can lead to catastrophic failure. Classical optimization and control pipelines handle these constraints well but depend on highly accurate mathematical representations of terrain geometry, making them prone to error when perception is noisy or incomplete. Meanwhile, reinforcement learning has shown strong resilience to disturbances and modeling errors, yet end-to-end policies rarely discover the precise foothold placement and step sequencing required for discontinuous terrain. These contrasting limitations motivate approaches that guide learning with physics-based structure rather than relying purely on reward shaping. In this work, we introduce a locomotion framework in which a reduced-order stepping planner supplies dynamically consistent motion targets that steer the RL training process via Control Lyapunov Function (CLF) rewards. This combination of structured footstep planning and data-driven adaptation produces accurate, agile, and hardware-validated stepping-stone locomotion on a humanoid robot, substantially improving reliability compared to conventional model-free reinforcement-learning baselines.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6b65\u4f10\u89c4\u5212\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u6b65\u6001\u6846\u67b6\uff0c\u63d0\u5347\u4e86\u53cc\u8db3\u673a\u5668\u4eba\u5728\u4e0d\u8fde\u7eed\u5730\u5f62\u4e2d\u8fd0\u52a8\u7684\u7cbe\u51c6\u6027\u4e0e\u9002\u5e94\u6027\u3002", "motivation": "\u7ecf\u5178\u7684\u4f18\u5316\u548c\u63a7\u5236\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7cbe\u786e\u7684\u5730\u5f62\u51e0\u4f55\u8868\u793a\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\u5728\u5e72\u6270\u548c\u5efa\u6a21\u8bef\u5dee\u65b9\u9762\u5177\u6709\u97e7\u6027\uff0c\u4f46\u5f80\u5f80\u96be\u4ee5\u53d1\u73b0\u7cbe\u786e\u7684\u8db3\u90e8\u653e\u7f6e\u548c\u6b65\u4f10\u5e8f\u5217\u3002", "method": "\u901a\u8fc7\u51cf\u9636\u6b65\u4f10\u89c4\u5212\u5668\u63d0\u4f9b\u52a8\u6001\u4e00\u81f4\u7684\u8fd0\u52a8\u76ee\u6807\uff0c\u5e76\u7ed3\u5408\u7ed3\u6784\u5316\u6b65\u4f10\u89c4\u5212\u548c\u6570\u636e\u9a71\u52a8\u7684\u9002\u5e94\uff0c\u4ee5\u5b9e\u73b0\u6b65\u6001\u5b66\u4e60\u3002", "result": "\u8be5\u65b9\u6cd5\u76f8\u6bd4\u4f20\u7edf\u7684\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u53cc\u8db3\u673a\u5668\u4eba\u5728\u4e0d\u8fde\u7eed\u5730\u5f62\u4e0a\u7684\u6b65\u4f10\u53ef\u9760\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u63a7\u5236Lyapunov\u51fd\u6570\u5956\u52b1\u6307\u5bfc\u5f3a\u5316\u5b66\u4e60\u7684\u6b65\u6001\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u53cc\u8db3\u673a\u5668\u4eba\u5728\u4e0d\u8fde\u7eed\u5730\u5f62\u4e0a\u7684\u6b65\u4f10\u7cbe\u51c6\u6027\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2601.06344", "categories": ["cs.RO", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.06344", "abs": "https://arxiv.org/abs/2601.06344", "authors": ["Cedric Melancon", "Julien Gascon-Samson", "Maarouf Saad", "Kuljeet Kaur", "Simon Savard"], "title": "BlazeAIoT: A Modular Multi-Layer Platform for Real-Time Distributed Robotics Across Edge, Fog, and Cloud Infrastructures", "comment": "17 pages, 9 figures", "summary": "The increasing complexity of distributed robotics has driven the need for platforms that seamlessly integrate edge, fog, and cloud computing layers while meeting strict real-time constraints. This paper introduces BlazeAIoT, a modular multi-layer platform designed to unify distributed robotics across heterogeneous infrastructures. BlazeAIoT provides dynamic data transfer, configurable services, and integrated monitoring, while ensuring resilience, security, and programming language flexibility. The architecture leverages Kubernetes-based clusters, broker interoperability (DDS, Kafka, Redis, and ROS2), and adaptive data distribution mechanisms to optimize communication and computation across diverse environments. The proposed solution includes a multi-layer configuration service, dynamic and adaptive data bridging, and hierarchical rate limiting to handle large messages. The platform is validated through robotics scenarios involving navigation and artificial intelligence-driven large-scale message processing, demonstrating robust performance under real-time constraints. Results highlight BlazeAIoT's ability to dynamically allocate services across incomplete topologies, maintain system health, and minimize latency, making it a cost-aware, scalable solution for robotics and broader IoT applications, such as smart cities and smart factories.", "AI": {"tldr": "BlazeAIoT\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u591a\u5c42\u5e73\u53f0\uff0c\u96c6\u6210\u4e86\u8fb9\u7f18\u3001\u96fe\u548c\u4e91\u8ba1\u7b97\uff0c\u65e8\u5728\u5728\u5206\u5e03\u5f0f\u673a\u5668\u4eba\u4e2d\u63d0\u4f9b\u52a8\u6001\u6570\u636e\u4f20\u8f93\u548c\u5b9e\u65f6\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u79cdIoT\u5e94\u7528\u3002", "motivation": "\u5206\u5e03\u5f0f\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u590d\u6742\u6027\u65e5\u76ca\u589e\u52a0\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u65e0\u7f1d\u6574\u5408\u8fb9\u7f18\u3001\u96fe\u8ba1\u7b97\u548c\u4e91\u8ba1\u7b97\u5c42\u7684\u5e73\u53f0\uff0c\u540c\u65f6\u6ee1\u8db3\u4e25\u683c\u7684\u5b9e\u65f6\u7ea6\u675f\u3002", "method": "BlazeAIoT\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u591a\u5c42\u5e73\u53f0\uff0c\u65e8\u5728\u7edf\u4e00\u5f02\u6784\u57fa\u7840\u8bbe\u65bd\u4e0a\u7684\u5206\u5e03\u5f0f\u673a\u5668\u4eba\u3002\u8be5\u5e73\u53f0\u5229\u7528\u57fa\u4e8eKubernetes\u7684\u96c6\u7fa4\u3001\u6d88\u606f\u4ee3\u7406\u4e92\u64cd\u4f5c\u6027 (DDS, Kafka, Redis\u548cROS2) \u548c\u81ea\u9002\u5e94\u6570\u636e\u5206\u53d1\u673a\u5236\uff0c\u4f18\u5316\u4e0d\u540c\u73af\u5883\u4e0b\u7684\u901a\u4fe1\u548c\u8ba1\u7b97\u3002", "result": "\u8be5\u5e73\u53f0\u5728\u6d89\u53ca\u5bfc\u822a\u548c\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u5927\u89c4\u6a21\u6d88\u606f\u5904\u7406\u7684\u673a\u5668\u4eba\u573a\u666f\u4e2d\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5c55\u73b0\u4e86\u5728\u5b9e\u65f6\u7ea6\u675f\u4e0b\u7684\u5f3a\u5927\u6027\u80fd\u3002", "conclusion": "BlazeAIoT\u80fd\u591f\u52a8\u6001\u5206\u914d\u670d\u52a1\uff0c\u901a\u8fc7\u4e0d\u5b8c\u6574\u7684\u62d3\u6251\u7ef4\u62a4\u7cfb\u7edf\u5065\u5eb7\u5e76\u6700\u5c0f\u5316\u5ef6\u8fdf\uff0c\u4f7f\u5176\u6210\u4e3a\u4e00\u4e2a\u5177\u6709\u6210\u672c\u610f\u8bc6\u548c\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u53ca\u66f4\u5e7f\u6cdb\u7684\u7269\u8054\u7f51\u5e94\u7528\uff0c\u5982\u667a\u80fd\u57ce\u5e02\u548c\u667a\u80fd\u5de5\u5382\u3002"}}
{"id": "2601.06415", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.06415", "abs": "https://arxiv.org/abs/2601.06415", "authors": ["Nathan Pascal Walus", "Ranulfo Bezerra", "Shotaro Kojima", "Tsige Tadesse Alemayoh", "Satoshi Tadokoro", "Kazunori Ohno"], "title": "Semantic Enrichment of CAD-Based Industrial Environments via Scene Graphs for Simulation and Reasoning", "comment": "Accepted to IEEE SSRR 2025", "summary": "Utilizing functional elements in an industrial environment, such as displays and interactive valves, provide effective possibilities for robot training. When preparing simulations for robots or applications that involve high-level scene understanding, the simulation environment must be equally detailed. Although CAD files for such environments deliver an exact description of the geometry and visuals, they usually lack semantic, relational and functional information, thus limiting the simulation and training possibilities. A 3D scene graph can organize semantic, spatial and functional information by enriching the environment through a Large Vision-Language Model (LVLM). In this paper we present an offline approach to creating detailed 3D scene graphs from CAD environments. This will serve as a foundation to include the relations of functional and actionable elements, which then can be used for dynamic simulation and reasoning. Key results of this research include both quantitative results of the generated semantic labels as well as qualitative results of the scene graph, especially in hindsight of pipe structures and identified functional relations. All code, results and the environment will be made available at https://cad-scenegraph.github.io", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eCAD\u6587\u4ef6\u751f\u62103D\u573a\u666f\u56fe\u7684\u65b9\u6cd5\uff0c\u4ee5\u589e\u5f3a\u5de5\u4e1a\u73af\u5883\u4e2d\u673a\u5668\u4eba\u7684\u8bad\u7ec3\u53ef\u80fd\u6027\u3002", "motivation": "\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\uff0c\u529f\u80fd\u5143\u7d20\u4e3a\u673a\u5668\u4eba\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u53ef\u80fd\u6027\uff0c\u4f46CAD\u6587\u4ef6\u901a\u5e38\u7f3a\u4e4f\u8bed\u4e49\u548c\u529f\u80fd\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u4eff\u771f\u548c\u8bad\u7ec3\u7684\u53ef\u80fd\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u79bb\u7ebf\u65b9\u6cd5\uff0c\u901a\u8fc7CAD\u73af\u5883\u751f\u6210\u8be6\u7ec6\u76843D\u573a\u666f\u56fe\uff0c\u5229\u7528\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u589e\u5f3a\u73af\u5883\u7684\u4fe1\u606f\u3002", "result": "\u7814\u7a76\u7684\u4e3b\u8981\u7ed3\u679c\u5305\u62ec\u751f\u6210\u7684\u8bed\u4e49\u6807\u7b7e\u7684\u5b9a\u91cf\u7ed3\u679c\u548c\u573a\u666f\u56fe\u7684\u5b9a\u6027\u7ed3\u679c\uff0c\u7279\u522b\u662f\u5728\u7ba1\u9053\u7ed3\u6784\u548c\u8bc6\u522b\u7684\u529f\u80fd\u5173\u7cfb\u65b9\u9762\u3002", "conclusion": "\u901a\u8fc7\u521b\u5efa\u8be6\u7ec6\u76843D\u573a\u666f\u56fe\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6a21\u62df\u548c\u63a8\u7406\u529f\u80fd\u6027\u548c\u53ef\u64cd\u4f5c\u6027\u5143\u7d20\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4e3a\u673a\u5668\u4eba\u8bad\u7ec3\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.06451", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.06451", "abs": "https://arxiv.org/abs/2601.06451", "authors": ["Hyunseo Koh", "Chang-Yong Song", "Youngjae Choi", "Misa Viveiros", "David Hyde", "Heewon Kim"], "title": "CulinaryCut-VLAP: A Vision-Language-Action-Physics Framework for Food Cutting via a Force-Aware Material Point Method", "comment": "16 pages; 15 figures; 5 tables", "summary": "Food cutting is a highly practical yet underexplored application at the intersection of vision and robotic manipulation. The task remains challenging because interactions between the knife and deformable materials are highly nonlinear and often entail large deformations, frequent contact, and topological change, which in turn hinder stable and safe large-scale data collection.\n  To address these challenges, we propose a unified framework that couples a vision-language-action (VLA) dataset with a physically realistic cutting simulator built on the material point method (MPM). Our simulator adopts MLS-MPM as its computational core, reducing numerical dissipation and energy drift while preserving rotational and shear responses even under topology-changing cuts. During cutting, forces and stress distributions are estimated from impulse exchanges between particles and the grid, enabling stable tracking of transient contact forces and energy transfer.\n  We also provide a benchmark dataset that integrates diverse cutting trajectories, multi-view visual observations, and fine-grained language instructions, together with force--torque and tool--pose labels to provide physically consistent training signals.\n  These components realize a learning--evaluation loop that respects the core physics of cutting and establishes a safe, reproducible, and scalable foundation for advancing VLA models in deformable object manipulation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u3001\u8bed\u8a00\u548c\u7269\u7406\u5207\u5272\u6a21\u62df\u7684\u6846\u67b6\uff0c\u4ee5\u5e94\u5bf9\u673a\u5668\u4eba\u5728\u53ef\u53d8\u5f62\u7269\u4f53\u5207\u5272\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u5efa\u7acb\u4e86\u4e00\u5957\u6570\u636e\u96c6\u548c\u5b66\u4e60\u8bc4\u4f30\u673a\u5236\u3002", "motivation": "\u63a2\u7d22\u5728\u89c6\u89c9\u4e0e\u673a\u5668\u4eba\u64cd\u4f5c\u4ea4\u53c9\u9886\u57df\u4e2d\u7684\u98df\u54c1\u5207\u5272\u5e94\u7528\uff0c\u540c\u65f6\u5e94\u5bf9\u5207\u5272\u8fc7\u7a0b\u4e2d\u7684\u975e\u7ebf\u6027\u4ea4\u4e92\u548c\u5f62\u53d8\u6311\u6218\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u7269\u4f53\u70b9\u65b9\u6cd5\uff08MPM\uff09\u7684\u7269\u7406\u5207\u5272\u6a21\u62df\u5668\uff0c\u5e76\u7ed3\u5408\u591a\u6837\u5316\u7684\u5207\u5272\u8f68\u8ff9\u3001\u89c6\u89c9\u89c2\u5bdf\u548c\u8bed\u8a00\u6307\u4ee4\u521b\u5efa\u57fa\u51c6\u6570\u636e\u96c6\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u5b9e\u73b0\u4e86\u4e00\u4e2a\u5b66\u4e60\u8bc4\u4f30\u5faa\u73af\uff0c\u4ece\u800c\u6539\u5584\u4e86\u53ef\u53d8\u5f62\u7269\u4f53\u64cd\u4f5c\u4e2d\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6570\u636e\u96c6\u4e0e\u7269\u7406\u5207\u5272\u6a21\u62df\u5668\uff0c\u4e3a\u53ef\u53d8\u5f62\u7269\u4f53\u64cd\u4f5c\u4e2d\u7684\u673a\u5668\u4eba\u5207\u5272\u4efb\u52a1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.06027", "categories": ["cs.HC", "cs.AI", "cs.CE", "cs.IR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.06027", "abs": "https://arxiv.org/abs/2601.06027", "authors": ["Alfonso Piscitelli", "Cristina David", "Mattia De Rosa", "Ali Mohammed", "Federico Nanni", "Jacob Pake", "Roly Perera", "Jessy Sodimu", "Chenyiqiu Zheng"], "title": "AI-Assisted Authoring for Transparent, Data-Driven Documents", "comment": null, "summary": "We introduce _transparent documents_, interactive web-based scholarly articles which allow readers to explore the relationship to the underlying data by hovering over fragments of text, and present an LLM-based tool for authoring transparent documents, building on recent developments in data provenance for general-purpose programming languages. As a target platform, our implementation uses Fluid, an open source programming language with a provenance-tracking runtime. Our agent-based tool supports a human author during the creation of transparent documents, identifying fragments of text which can be computed from data, such as numerical values selected from records or computed by aggregations like sum and mean, comparatives and superlatives like _better than_ and _largest_, trend-adjectives like _growing_, and similar quantitative or semi-quantitative phrases, and then attempts to synthesise a suitable Fluid query over the data which generates the target string. The resulting expression is inserted into the article's web page, turning the static text fragment into an interactable data-driven element able to reveal the data that underwrites the natural language claim. We evaluate our approach on a subset of SciGen, an open source dataset consisting of tables from scientific articles and their corresponding descriptions, which we extend with hand-generated counterfactual test cases to evaluate how well machine-generated expressions generalise. Our results show that gpt4o is often able to synthesise compound expressions extensionally compatible with our gold solutions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u900f\u660e\u6587\u6863\u7684\u6982\u5ff5\u548c\u76f8\u5e94\u7684LLM\u5de5\u5177\uff0c\u65e8\u5728\u589e\u5f3a\u5b66\u672f\u6587\u7ae0\u7684\u4ea4\u4e92\u6027\u548c\u900f\u660e\u5ea6\u3002\u901a\u8fc7\u5e94\u7528gpt4o\u548cFluid\u8bed\u8a00\uff0c\u751f\u6210\u53ef\u4e92\u52a8\u7684\u6587\u672c\u4e0e\u6570\u636e\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\u5176\u751f\u6210\u6548\u679c\u826f\u597d\u3002", "motivation": "\u65e8\u5728\u63d0\u9ad8\u5b66\u672f\u6587\u7ae0\u7684\u900f\u660e\u5ea6\uff0c\u4f7f\u8bfb\u8005\u80fd\u591f\u66f4\u76f4\u63a5\u5730\u63a2\u7d22\u6587\u672c\u4e0e\u57fa\u7840\u6570\u636e\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u4f7f\u7528gpt4o\u5de5\u5177\u751f\u6210\u900f\u660e\u6587\u6863\u4e2d\u7684\u6570\u636e\u9a71\u52a8\u5143\u7d20\uff0c\u5e76\u5229\u7528Fluid\u7f16\u7a0b\u8bed\u8a00\u8fdb\u884c\u5b9e\u73b0\u548c\u8bc4\u4f30\u3002", "result": "\u5728SciGen\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u751f\u6210\u7684\u8868\u8fbe\u5f0f\u901a\u5e38\u4e0e\u624b\u52a8\u751f\u6210\u7684\u9ec4\u91d1\u89e3\u51b3\u65b9\u6848\u5177\u6709\u6269\u5c55\u517c\u5bb9\u6027\u3002", "conclusion": "gpt4o\u5de5\u5177\u80fd\u591f\u751f\u6210\u4e0e\u9ec4\u91d1\u6807\u51c6\u89e3\u51b3\u65b9\u6848\u517c\u5bb9\u7684\u590d\u5408\u8868\u8fbe\u5f0f\uff0c\u5c55\u793a\u4e86\u900f\u660e\u6587\u6863\u7684\u4ea4\u4e92\u6027\u548c\u6570\u636e\u9a71\u52a8\u7279\u6027\u3002"}}
{"id": "2601.06508", "categories": ["cs.RO", "cs.CV", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.06508", "abs": "https://arxiv.org/abs/2601.06508", "authors": ["Andrei A. Korigodskii", "Artem E. Vasiunik", "Georgii A. Varin", "Adilia M. Zukhurova", "Matvei V. Urvantsev", "Semen A. Osipenkov", "Igor S. Efremov", "Georgii E. Bondar"], "title": "Precision Meets Art: Autonomous Multi-UAV System for Large Scale Mural Drawing", "comment": "6 pages, 9 figures", "summary": "The integration of autonomous unmanned aerial vehicles (UAVs) into large-scale artistic projects has emerged as a new application in robotics. This paper presents the design, deployment, and testing of a novel multi-drone system for automated mural painting in outdoor settings. This technology makes use of new software that coordinates multiple drones simultaneously, utilizing state-machine algorithms for task execution. Key advancements are the complex positioning system that combines 2D localization using a single motion tracking camera with onboard LiDAR for precise positioning, and a novel flight control algorithm, which works differently along the trajectory and normally to it, ensuring smoothness and high precision of the drawings at the same time. A 100 square meters mural was created using the developed multi-drone system, validating the system's efficacy. Compared to single-drone approaches, our multi-UAV solution significantly improves scalability and operational speed while maintaining high stability even in harsh weather conditions. The findings highlight the potential of autonomous robotic swarms in creative applications, paving the way for further advancements in large-scale robotic art.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\uff0c\u6210\u529f\u7528\u4e8e\u6237\u5916\u58c1\u753b\u7ed8\u5236\uff0c\u663e\u793a\u4e86\u673a\u5668\u4eba\u7fa4\u4f53\u5728\u827a\u672f\u521b\u4f5c\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u63a2\u7d22\u81ea\u4e3b\u65e0\u4eba\u673a\u5728\u5927\u89c4\u6a21\u827a\u672f\u9879\u76ee\u4e2d\u7684\u65b0\u5e94\u7528\uff0c\u63d0\u5347\u65e0\u4eba\u673a\u7ed8\u753b\u7684\u6548\u7387\u548c\u7cbe\u786e\u6027\u3002", "method": "\u8bbe\u8ba1\u3001\u90e8\u7f72\u548c\u6d4b\u8bd5\u65b0\u578b\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\uff0c\u7528\u4e8e\u6237\u5916\u81ea\u52a8\u58c1\u753b\u7ed8\u5236\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u5e45100\u5e73\u65b9\u7c73\u7684\u58c1\u753b\uff0c\u9a8c\u8bc1\u4e86\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e0e\u5355\u65e0\u4eba\u673a\u65b9\u6cd5\u76f8\u6bd4\u663e\u8457\u63d0\u5347\u4e86\u53ef\u6269\u5c55\u6027\u548c\u64cd\u4f5c\u901f\u5ea6\uff0c\u7a33\u5b9a\u6027\u826f\u597d\u3002", "conclusion": "\u81ea\u4e3b\u673a\u5668\u4eba\u7fa4\u4f53\u5728\u521b\u610f\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u663e\u8457\uff0c\u4e3a\u5927\u89c4\u6a21\u673a\u5668\u4eba\u827a\u672f\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2601.06028", "categories": ["cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.06028", "abs": "https://arxiv.org/abs/2601.06028", "authors": ["Mohammadreza Behboodi", "Eli Kinney-Lang", "Ali Etemad", "Adam Kirton", "Hatem Abou-Zeid"], "title": "Leveraging Foundation Models for Calibration-Free c-VEP BCIs", "comment": "8 Pages, 2 figures, Accepted and Presented at the IEEE SMC Conference 2025", "summary": "Foundation Models (FMs) have surged in popularity over the past five years, with applications spanning fields from computer vision to natural language processing. Brain-Computer Interfaces (BCIs) have also gained momentum due to their potential to support individuals with complex disabilities. Among BCI paradigms, code-modulated Visual Evoked Potentials (c-VEPs) remain relatively understudied, despite offering high information transfer rates and large selection target capacities. However, c-VEP systems require lengthy calibration sessions, limiting their practicality outside of laboratory settings. In this study, we use a FM for the first time to eliminate the need for lengthy calibration in c-VEP BCI systems. We evaluated two approaches: (1) a truly calibration-free approach requiring no subject-specific data, and (2) a limited calibration approach, where we assessed the benefit of incorporating incremental amounts of calibration data. In both cases, a classification head is trained on data from other subjects. For a new subject, no calibration data is required in the calibration-free setup, making the c-VEP system effectively plug-and-play. The proposed method was tested on two c-VEP datasets. For the calibration-free approach, the average accuracy on the first dataset (n = 17) was 68.8% +/- 17.6%, comparable to the full-calibration performance reported in the original study (66.2% +/- 13.8%), which required approximately 11 minutes of calibration. On the second dataset (n = 12), the calibration-free accuracy was 71.8% +/- 20.2%, versus 93.7% +/- 5.5% from the original study, which required around 3.5 minutes. A limited-calibration approach using only 20% of the subject's data (approximately 43 seconds) yielded 92% +/- 5.2% accuracy. These results indicate that our FM-based approach can effectively eliminate or significantly reduce the need for lengthy calibration in c-VEP BCIs.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFM\u7684c-VEP BCI\u65b9\u6cd5\uff0c\u6210\u529f\u51cf\u5c11\u4e86\u6821\u51c6\u65f6\u95f4\uff0c\u63d0\u5347\u4e86\u5b9e\u7528\u6027\u3002", "motivation": "\u51cf\u5c11BCI\u7cfb\u7edf\u957f\u65f6\u95f4\u6821\u51c6\u7684\u9700\u6c42\uff0c\u63d0\u9ad8\u5176\u5b9e\u7528\u6027\uff0c\u652f\u6301\u590d\u6742\u6b8b\u75be\u4eba\u58eb\u3002", "method": "\u4f7f\u7528\u57fa\u7840\u6a21\u578b\uff08FM\uff09\u8bc4\u4f30c-VEP BCI\u7cfb\u7edf\u7684\u6821\u51c6\u9700\u6c42", "result": "\u5728\u65e0\u6821\u51c6\u7684\u60c5\u51b5\u4e0b\uff0c\u7b2c\u4e00\u6570\u636e\u96c6\u7684\u5e73\u5747\u51c6\u786e\u7387\u4e3a68.8%\uff0c\u7b2c\u4e8c\u6570\u636e\u96c6\u4e3a71.8%\uff1b\u6709\u9650\u6821\u51c6\u65b9\u6cd5\u7684\u51c6\u786e\u7387\u4e3a92%\u3002", "conclusion": "FM\u65b9\u6cd5\u6709\u6548\u6d88\u9664\u4e86\u6216\u663e\u8457\u51cf\u5c11\u4e86c-VEP BCI\u7cfb\u7edf\u5bf9\u957f\u65f6\u95f4\u6821\u51c6\u7684\u9700\u6c42\u3002"}}
{"id": "2601.06552", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.06552", "abs": "https://arxiv.org/abs/2601.06552", "authors": ["Britt Besch", "Tai Mai", "Jeremias Thun", "Markus Huff", "J\u00f6rn Vogel", "Freek Stulp", "Samuel Bustamante"], "title": "Model Reconciliation through Explainability and Collaborative Recovery in Assistive Robotics", "comment": null, "summary": "Whenever humans and robots work together, it is essential that unexpected robot behavior can be explained to the user. Especially in applications such as shared control the user and the robot must share the same model of the objects in the world, and the actions that can be performed on these objects.\n  In this paper, we achieve this with a so-called model reconciliation framework. We leverage a Large Language Model to predict and explain the difference between the robot's and the human's mental models, without the need of a formal mental model of the user. Furthermore, our framework aims to solve the model divergence after the explanation by allowing the human to correct the robot. We provide an implementation in an assistive robotics domain, where we conduct a set of experiments with a real wheelchair-based mobile manipulator and its digital twin.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u6a21\u578b\u8c03\u548c\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89e3\u91ca\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u5fc3\u7406\u6a21\u578b\u5dee\u5f02\u5e76\u652f\u6301\u7528\u6237\u4fee\u6b63\uff0c\u4ece\u800c\u4fc3\u8fdb\u4eba\u673a\u534f\u4f5c\u3002", "motivation": "\u786e\u4fdd\u673a\u5668\u4eba\u884c\u4e3a\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u63d0\u9ad8\u4eba\u673a\u534f\u4f5c\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u5171\u4eab\u63a7\u5236\u7684\u573a\u666f\u4e2d\uff0c\u9700\u8981\u5b9e\u73b0\u7528\u6237\u4e0e\u673a\u5668\u4eba\u5bf9\u4e16\u754c\u7684\u4e00\u81f4\u7406\u89e3\u3002", "method": "\u91c7\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5fc3\u7406\u6a21\u578b\u9884\u6d4b\u548c\u89e3\u91ca\uff0c\u5141\u8bb8\u7528\u6237\u5728\u673a\u5668\u4eba\u8f93\u51fa\u540e\u8fdb\u884c\u7ea0\u6b63\uff0c\u4ee5\u89e3\u51b3\u6a21\u578b\u504f\u5dee\u95ee\u9898\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u578b\u8c03\u548c\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u9884\u6d4b\u548c\u89e3\u91ca\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u4e4b\u95f4\u7684\u5fc3\u7406\u6a21\u578b\u5dee\u5f02\uff0c\u4ee5\u4fc3\u8fdb\u4eba\u673a\u534f\u4f5c\u65f6\u7684\u900f\u660e\u5ea6\u548c\u7406\u89e3\u3002", "conclusion": "\u901a\u8fc7\u5728\u52a9\u7406\u673a\u5668\u4eba\u9886\u57df\u7684\u5e94\u7528\uff0c\u7cfb\u7edf\u652f\u6301\u7528\u6237\u7ea0\u6b63\u673a\u5668\u4eba\u7684\u884c\u4e3a\uff0c\u4ece\u800c\u51cf\u5c0f\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u63d0\u5347\u4eba\u673a\u4ea4\u4e92\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.06029", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06029", "abs": "https://arxiv.org/abs/2601.06029", "authors": ["K\u00e9vin Ducharlet", "Liwen Zhang", "Sara Maqrot", "Houssem Saidi"], "title": "A Recommendation System-Based Framework for Enhancing Human-Machine Collaboration in Industrial Timetabling Rescheduling: Application in Preventive Maintenance", "comment": null, "summary": "Industrial timetabling is a critical task for decision-makers across various sectors to ensure efficient system operation. In real-world settings, it remains challenging because unexpected events often disrupt execution. When such events arise, effective rescheduling and collaboration between humans and machines becomes essential. This paper presents a recommendation system-based framework for handling rescheduling challenges, built on Timefold, a powerful AI-driven planning engine. Our experimental study evaluates nine instances inspired by a realworld preventive maintenance use case, aiming to identify the heuristic that best balances solution quality and computing time to support near-optimal decisionmaking when rescheduling is required due to unexpected events during operational days. Finally, we illustrate the complete process of our recommendation system through a simple use case.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a8\u8350\u7cfb\u7edf\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u5de5\u4e1a\u8c03\u5ea6\u4e2d\u7684\u610f\u5916\u4e8b\u4ef6\u91cd\u8c03\u5ea6\u95ee\u9898\u3002", "motivation": "\u5de5\u4e1a\u8c03\u5ea6\u5bf9\u4e8e\u786e\u4fdd\u7cfb\u7edf\u9ad8\u6548\u8fd0\u4f5c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u5b9e\u9645\u60c5\u51b5\u4e0b\uff0c\u610f\u5916\u4e8b\u4ef6\u7684\u53d1\u751f\u5e38\u5e38\u5bfc\u81f4\u6267\u884c\u4e2d\u7684\u5e72\u6270\uff0c\u56e0\u6b64\u5bfb\u6c42\u6709\u6548\u7684\u91cd\u8c03\u5ea6\u65b9\u6848\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5efa\u7acb\u5728\u5f3a\u5927\u7684AI\u9a71\u52a8\u89c4\u5212\u5f15\u64ceTimefold\u57fa\u7840\u4e0a\u7684\u63a8\u8350\u7cfb\u7edf\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5bf9\u4e5d\u4e2a\u5b9e\u4f8b\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u4ee5\u786e\u5b9a\u6700\u4f73\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u4ece\u800c\u652f\u6301\u63a5\u8fd1\u6700\u4f73\u7684\u51b3\u7b56\u5236\u5b9a\u3002", "result": "\u901a\u8fc7\u5bf9\u5b9e\u9645\u7528\u4f8b\u7684\u542f\u53d1\u5f0f\u8bc4\u4f30\uff0c\u627e\u5230\u5728\u8ba1\u7b97\u65f6\u95f4\u4e0e\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u4e4b\u95f4\u6700\u4f73\u5e73\u8861\u7684\u542f\u53d1\u5f0f\u3002", "conclusion": "\u901a\u8fc7\u7b80\u5355\u7528\u4f8b\u5c55\u793a\u4e86\u63a8\u8350\u7cfb\u7edf\u7684\u5b8c\u6574\u8fc7\u7a0b\uff0c\u4e3a\u5904\u7406\u5de5\u4e1a\u8c03\u5ea6\u4e2d\u7684\u91cd\u8c03\u5ea6\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2601.06602", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.06602", "abs": "https://arxiv.org/abs/2601.06602", "authors": ["Mohammed S. Alharbi", "Shinkyu Park"], "title": "UMLoc: Uncertainty-Aware Map-Constrained Inertial Localization with Quantified Bounds", "comment": null, "summary": "Inertial localization is particularly valuable in GPS-denied environments such as indoors. However, localization using only Inertial Measurement Units (IMUs) suffers from drift caused by motion-process noise and sensor biases. This paper introduces Uncertainty-aware Map-constrained Inertial Localization (UMLoc), an end-to-end framework that jointly models IMU uncertainty and map constraints to achieve drift-resilient positioning. UMLoc integrates two coupled modules: (1) a Long Short-Term Memory (LSTM) quantile regressor, which estimates the specific quantiles needed to define 68%, 90%, and 95% prediction intervals serving as a measure of localization uncertainty and (2) a Conditioned Generative Adversarial Network (CGAN) with cross-attention that fuses IMU dynamic data with distance-based floor-plan maps to generate geometrically feasible trajectories. The modules are trained jointly, allowing uncertainty estimates to propagate through the CGAN during trajectory generation. UMLoc was evaluated on three datasets, including a newly collected 2-hour indoor benchmark with time-aligned IMU data, ground-truth poses and floor-plan maps. Results show that the method achieves a mean drift ratio of 5.9% over a 70 m travel distance and an average Absolute Trajectory Error (ATE) of 1.36 m, while maintaining calibrated prediction bounds.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u5730\u56fe\u7ea6\u675f\u60ef\u6027\u5b9a\u4f4d\u6846\u67b6UMLoc\uff0c\u901a\u8fc7\u8054\u5408\u5efa\u6a21IMU\u4e0d\u786e\u5b9a\u6027\u548c\u5730\u56fe\u7ea6\u675f\uff0c\u5b9e\u73b0\u4e86\u6297\u6f02\u79fb\u5b9a\u4f4d\u3002", "motivation": "\u5728GPS\u4fe1\u53f7\u7f3a\u5931\u7684\u5ba4\u5185\u73af\u5883\u4e2d\uff0c\u60ef\u6027\u5b9a\u4f4d\u6280\u672f\u7684\u6f02\u79fb\u95ee\u9898\u4e25\u91cd\u5f71\u54cd\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u56e0\u6b64\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u673a\u5236\u6765\u63d0\u9ad8\u60ef\u6027\u5b9a\u4f4d\u7684\u53ef\u9760\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u7684UMLoc\u6846\u67b6\u6574\u5408\u4e86\u4e24\u4e2a\u6a21\u5757\uff1aLSTM\u91cf\u5b50\u56de\u5f52\u5668\u7528\u4e8e\u4f30\u7b97\u4e0d\u786e\u5b9a\u6027\u9884\u6d4b\u533a\u95f4\uff0cCGAN\u7ed3\u5408\u8de8\u6ce8\u610f\u529b\u673a\u5236\u751f\u6210\u7b26\u5408\u51e0\u4f55\u7ea6\u675f\u7684\u8f68\u8ff9\u3002", "result": "UMLoc\u5728\u4e09\u79cd\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5305\u62ec\u4e00\u4e2a\u65b0\u6536\u96c6\u7684\u5305\u542bIMU\u6570\u636e\u548c\u771f\u5b9e\u4f4d\u59ff\u7684\u5ba4\u5185\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7ed3\u679c\u663e\u793a\u5176\u6f02\u79fb\u7387\u4e3a5.9%\uff0c\u7edd\u5bf9\u8f68\u8ff9\u8bef\u5dee\u4e3a1.36\u7c73\u3002", "conclusion": "UMLoc\u5728\u503e\u659c\u5b9a\u4f4d\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6f02\u79fb\uff0c\u4e14\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u80fd\u591f\u63d0\u4f9b\u51c6\u786e\u7684\u8f68\u8ff9\u9884\u6d4b\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002"}}
{"id": "2601.06030", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06030", "abs": "https://arxiv.org/abs/2601.06030", "authors": ["Richard Jiarui Tong"], "title": "From Augmentation to Symbiosis: A Review of Human-AI Collaboration Frameworks, Performance, and Perils", "comment": null, "summary": "This paper offers a concise, 60-year synthesis of human-AI collaboration, from Licklider's ``man-computer symbiosis\" (AI as colleague) and Engelbart's ``augmenting human intellect\" (AI as tool) to contemporary poles: Human-Centered AI's ``supertool\" and Symbiotic Intelligence's mutual-adaptation model. We formalize the mechanism for effective teaming as a causal chain: Explainable AI (XAI) -> co-adaptation -> shared mental models (SMMs). A meta-analytic ``performance paradox\" is then examined: human-AI teams tend to show negative synergy in judgment/decision tasks (underperforming AI alone) but positive synergy in content creation and problem formulation. We trace failures to the algorithm-in-the-loop dynamic, aversion/bias asymmetries, and cumulative cognitive deskilling. We conclude with a unifying framework--combining extended-self and dual-process theories--arguing that durable gains arise when AI functions as an internalized cognitive component, yielding a unitary human-XAI symbiotic agency. This resolves the paradox and delineates a forward agenda for research and practice.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86\u4eba\u7c7b\u4e0eAI\u5408\u4f5c\u7684\u5386\u53f2\uff0c\u5e76\u63d0\u51fa\u4e86\u6709\u6548\u56e2\u961f\u5408\u4f5c\u7684\u56e0\u679c\u94fe\u6a21\u578b\uff0c\u5206\u6790\u4e86\u4eba\u673a\u56e2\u961f\u5728\u5224\u65ad/\u51b3\u7b56\u548c\u5185\u5bb9\u521b\u4f5c\u4e2d\u7684\u4e0d\u540c\u8868\u73b0\uff0c\u6700\u7ec8\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u4ee5\u5b9e\u73b0\u6301\u7eed\u6536\u76ca\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4eba\u7c7b\u4e0eAI\u534f\u4f5c\u7684\u673a\u5236\u53ca\u5176\u5f71\u54cd\u529b\uff0c\u89e3\u51b3\u4eba\u673a\u534f\u4f5c\u4e2d\u5b58\u5728\u7684\u8868\u73b0\u6096\u8bba\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u4e0e\u5b9e\u8df5\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u56e0\u679c\u94fe\u6a21\u578b\uff0c\u5206\u6790\u4eba\u673a\u534f\u4f5c\u7684\u534f\u540c\u6548\u5e94\uff0c\u4f7f\u7528\u5143\u5206\u6790\u63a2\u8ba8\u8868\u73b0\u6096\u8bba\uff0c\u5e76\u7ed3\u5408\u6269\u5c55\u81ea\u6211\u4e0e\u53cc\u5904\u7406\u7406\u8bba\u6765\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\u3002", "result": "\u8be5\u8bba\u6587\u603b\u7ed3\u4e86\u8fc7\u53bb60\u5e74\u6765\u4eba\u7c7b\u4e0e\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u5408\u4f5c\u7684\u6f14\u53d8\uff0c\u63a2\u8ba8\u4e86\u4ece\u65e9\u671f\u7684\"\u4eba\u673a\u5171\u751f\"\u5230\u5f53\u4ee3\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u4eba\u5de5\u667a\u80fd\u7684\u4e0d\u540c\u89d2\u8272\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\uff0c\u5efa\u8bae\u5c06\u4eba\u5de5\u667a\u80fd\u89c6\u4e3a\u5185\u5316\u7684\u8ba4\u77e5\u7ec4\u4ef6\uff0c\u4ece\u800c\u5b9e\u73b0\u4eba\u673a\u534f\u4f5c\u7684\u6301\u4e45\u6536\u76ca\uff0c\u5e76\u89e3\u51b3\u4eba\u673a\u534f\u4f5c\u7684\u8868\u73b0\u6096\u8bba\u3002"}}
{"id": "2601.06617", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.06617", "abs": "https://arxiv.org/abs/2601.06617", "authors": ["Giovani Braglia", "Jos\u00e9 Jair Alves Mendes Junior", "Augusto Tetsuo Prado Inafuco", "Federico Mariano", "Leonardo S. Mattos"], "title": "Robotic Tele-Operation for Upper Aerodigestive Tract Microsurgery: System Design and Validation", "comment": null, "summary": "Upper aerodigestive tract (UADT) treatments frequently employ transoral laser microsurgery (TLM) for procedures such as the removal of tumors or polyps. In TLM, a laser beam is used to cut target tissue, while forceps are employed to grasp, manipulate, and stabilize tissue within the UADT. Although TLM systems may rely on different technologies and interfaces, forceps manipulation is still predominantly performed manually, introducing limitations in ergonomics, precision, and controllability. This paper proposes a novel robotic system for tissue manipulation in UADT procedures, based on a novel end-effector designed for forceps control. The system is integrated within a teleoperation framework that employs a robotic manipulator with a programmed remote center of motion (RCM), enabling precise and constrained instrument motion while improving surgeon ergonomics. The proposed approach is validated through two experimental studies and a dedicated usability evaluation, demonstrating its effectiveness and suitability for UADT surgical applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7528\u4e8eUADT\u624b\u672f\u4e2d\u7684\u7ec4\u7ec7\u64cd\u4f5c\uff0c\u65e8\u5728\u6539\u5584\u624b\u52a8\u64cd\u4f5c\u7684\u5c40\u9650\u6027\uff0c\u5e76\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u4f20\u7edfTLM\u7cfb\u7edf\u4e2d\u624b\u52a8\u64cd\u4f5c\u5939\u6301\u5668\u5e26\u6765\u7684\u5728\u4eba\u4f53\u5de5\u5b66\u3001\u7cbe\u786e\u5ea6\u548c\u53ef\u63a7\u6027\u7684\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65b0\u578b\u672b\u7aef\u6267\u884c\u5668\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86\u8fdc\u7a0b\u64cd\u4f5c\u6846\u67b6\uff0c\u4f7f\u7528\u673a\u5668\u4eba\u64cd\u7eb5\u5668\u548c\u7f16\u7a0b\u7684\u52a8\u4f5c\u4e2d\u5fc3\uff0c\u5141\u8bb8\u7cbe\u786e\u548c\u53d7\u9650\u7684\u5de5\u5177\u8fd0\u52a8\u3002", "result": "\u901a\u8fc7\u4e24\u4e2a\u5b9e\u9a8c\u7814\u7a76\u548c\u4e00\u9879\u4e13\u95e8\u7684\u53ef\u7528\u6027\u8bc4\u4f30\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7cfb\u7edf\u5728UADT\u624b\u672f\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u4e24\u4e2a\u5b9e\u9a8c\u7814\u7a76\u548c\u4e13\u95e8\u7684\u53ef\u7528\u6027\u8bc4\u4f30\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5176\u5728UADT\u5916\u79d1\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u548c\u9002\u5b9c\u6027\u3002"}}
{"id": "2601.06031", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06031", "abs": "https://arxiv.org/abs/2601.06031", "authors": ["Zeyi Liao", "Yadong Lu", "Boyu Gou", "Huan Sun", "Ahmed Awadallah"], "title": "Beyond Clicking:A Step Towards Generalist GUI Grounding via Text Dragging", "comment": "29 pages", "summary": "Graphical user interface (GUI) grounding, the process of mapping human instructions to GUI actions, serves as a fundamental basis to autonomous GUI agents. While existing grounding models achieve promising performance to simulate the mouse click action on various click-based benchmarks, another essential mode of mouse interaction, namely dragging, remains largely underexplored. Yet, dragging the mouse to select and manipulate textual content represents a prevalent and important usage in practical GUI scenarios. To narrow this gap, we first introduce GUI-Drag, a diverse dataset of 161K text dragging examples synthesized through a scalable pipeline. To support systematic and robust evaluation, we further construct ScreenDrag, a benchmark with 5,333 examples spanning three levels of interface context, together with three dedicated metrics designed for assessing text dragging capability. Models trained on GUI-Drag with an efficient continual training strategy achieve substantial improvements on ScreenDrag, while preserving the original click-based performance on ScreenSpot, ScreenSpot-v2, and OSWorld-G. Our work encourages further research on broader GUI grounding beyond just clicking and paves way toward a truly generalist GUI grounding model. All benchmark, data, checkpoints, and code are open-sourced and available at https://osu-nlp-group.github.io/GUI-Drag.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6784\u5efaGUI-Drag\u6570\u636e\u96c6\u548cScreenDrag\u57fa\u51c6\uff0c\u663e\u8457\u63d0\u5347\u4e86GUI\u6587\u672c\u62d6\u62fd\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u70b9\u51fb\u6027\u80fd\uff0c\u63a8\u52a8\u4e86GUI\u57fa\u7840\u66f4\u5e7f\u6cdb\u7684\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u7684GUI\u57fa\u7840\u6a21\u578b\u4e3b\u8981\u96c6\u4e2d\u5728\u9f20\u6807\u70b9\u51fb\uff0c\u800c\u62d6\u62fd\u64cd\u4f5c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u540c\u6837\u91cd\u8981\uff0c\u56e0\u6b64\u4e9f\u9700\u7814\u7a76\u548c\u63d0\u5347\u5176\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5f15\u5165GUI-Drag\u6570\u636e\u96c6\u548cScreenDrag\u57fa\u51c6\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\uff0c\u8bad\u7ec3\u6a21\u578b\u4ee5\u63d0\u5347\u6587\u672c\u62d6\u62fd\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u70b9\u51fb\u6027\u80fd", "result": "\u6a21\u578b\u5728GUI-Drag\u4e0a\u7ecf\u8fc7\u6709\u6548\u7684\u6301\u7eed\u8bad\u7ec3\u7b56\u7565\u540e\uff0c\u5728ScreenDrag\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u63d0\u5347\uff0c\u800c\u5728\u70b9\u51fb\u4efb\u52a1\u4e0a\u4fdd\u6301\u4e86\u539f\u6709\u6027\u80fd\u3002", "conclusion": "\u63d0\u51faGUI-Drag\u548cScreenDrag\u57fa\u51c6\u652f\u6301\u4e86\u66f4\u5e7f\u6cdb\u7684GUI\u57fa\u7840\u7814\u7a76\uff0c\u5411\u901a\u7528GUI\u6a21\u578b\u7684\u53d1\u5c55\u8fc8\u8fdb\u3002"}}
{"id": "2601.06652", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.06652", "abs": "https://arxiv.org/abs/2601.06652", "authors": ["Jing Cao", "Nishanth Kumar", "Aidan Curtis"], "title": "Follow the Signs: Using Textual Cues and LLMs to Guide Efficient Robot Navigation", "comment": null, "summary": "Autonomous navigation in unfamiliar environments often relies on geometric mapping and planning strategies that overlook rich semantic cues such as signs, room numbers, and textual labels. We propose a novel semantic navigation framework that leverages large language models (LLMs) to infer patterns from partial observations and predict regions where the goal is most likely located. Our method combines local perceptual inputs with frontier-based exploration and periodic LLM queries, which extract symbolic patterns (e.g., room numbering schemes and building layout structures) and update a confidence grid used to guide exploration. This enables robots to move efficiently toward goal locations labeled with textual identifiers (e.g., \"room 8\") even before direct observation. We demonstrate that this approach enables more efficient navigation in sparse, partially observable grid environments by exploiting symbolic patterns. Experiments across environments modeled after real floor plans show that our approach consistently achieves near-optimal paths and outperforms baselines by over 25% in Success weighted by Path Length.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8bed\u4e49\u5bfc\u822a\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u5347\u90e8\u5206\u89c2\u5bdf\u60c5\u51b5\u4e0b\u7684\u5bfc\u822a\u6548\u7387\uff0c\u5e76\u6210\u529f\u5728\u591a\u4e2a\u73af\u5883\u4e2d\u8d85\u8d8a\u57fa\u7ebf\u7b97\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u4e3b\u5bfc\u822a\u65b9\u6cd5\u5ffd\u89c6\u4e86\u73af\u5883\u4e2d\u7684\u4e30\u5bcc\u8bed\u4e49\u7ebf\u7d22\uff0c\u5982\u6807\u5fd7\u548c\u623f\u95f4\u7f16\u53f7\u3002", "method": "\u7ed3\u5408\u5c40\u90e8\u611f\u77e5\u8f93\u5165\u3001\u57fa\u4e8e\u8fb9\u754c\u7684\u63a2\u7d22\u548c\u5b9a\u671f\u7684LLM\u67e5\u8be2\uff0c\u63d0\u53d6\u7b26\u53f7\u6a21\u5f0f\u5e76\u66f4\u65b0\u4fe1\u5fc3\u7f51\u683c\u3002", "result": "\u5728\u6839\u636e\u771f\u5b9e\u5e73\u9762\u56fe\u5efa\u6a21\u7684\u73af\u5883\u4e2d\uff0c\u8be5\u65b9\u6cd5\u7684\u5bfc\u822a\u6548\u7387\u63d0\u9ad8\u4e8625%\u4ee5\u4e0a\uff0c\u8fbe\u5230\u8fd1\u6700\u4f18\u8def\u5f84\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7a00\u758f\u3001\u90e8\u5206\u53ef\u89c2\u5bdf\u7f51\u683c\u73af\u5883\u4e2d\u5c55\u793a\u51fa\u9ad8\u6548\u7684\u5bfc\u822a\u80fd\u529b\uff0c\u4f7f\u7528\u7b26\u53f7\u6a21\u5f0f\u6709\u6548\u5f15\u5bfc\u63a2\u7d22\u3002"}}
{"id": "2601.06032", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.06032", "abs": "https://arxiv.org/abs/2601.06032", "authors": ["Anna Katharina Holl-Etten", "Nina Schnaderbeck", "Elizaveta Kosareva", "Leonhard Aron Prattke", "Ralph Krueger", "Lisa Marie Warner", "Nora C. Vetter"], "title": "Applied Theory of Mind and Large Language Models - how good is ChatGPT at solving social vignettes?", "comment": "40 pages, 6 figures, 3 supplements", "summary": "The rapid development of language-based artificial intelligence (AI) offers new possibilities for psychotherapy and assistive systems, particularly benefitting autistic individuals who often respond well to technology. Parents of autistic persons emphasize the importance of appropriate and context-specific communication behavior. This study investigated whether GPT-3.5 Turbo and GPT-4, as language-based AI applications, are fundamentally capable of replicating this type of adequate communication behavior in the form of applied Theory of Mind (ToM). GPT-3.5 Turbo and GPT-4 were evaluated on three established higher-order ToM tasks: the Faux Pas Test, the Social Stories Questionnaire, and the Story Comprehension Test in English and German. Two independent raters scored response accuracy based on standardized manuals. In addition, responses were rated for epistemic markers as indicators of uncertainty. GPT's results were compared to human neurotypical and neurodivergent samples from previous own and others' research. GPT-4 achieved near human accuracy on the Faux Pas Test and outperformed GPT-3.5 Turbo and individuals with autistic traits. On the Social Stories Questionnaire, GPT-4 scored comparable to neurotypical adults, while GPT-3.5 Turbo remained well below. In the Story Comprehension Test, GPT-4 reached scores that exceeded neurotypical adult and adolescent benchmarks. However, GPT-4 used epistemic markers in up to 42% of responses. GPT-4 shows encouraging performance in complex higher-order ToM tasks and may offer future potential as an assistive tool for individuals with (and without) social communication difficulties. Its ability to interpret complex social situations is promising; however, the frequent use of uncertainty markers highlights the need for further study for assistive use and possibly further refinement to ensure consistent and reliable support in real-world use.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86GPT-3.5 Turbo\u548cGPT-4\u5728\u5e94\u7528\u5fc3\u667a\u7406\u8bba\uff08ToM\uff09\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0GPT-4\u5728\u590d\u6742\u7684ToM\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u7279\u522b\u662f\u5bf9\u81ea\u95ed\u75c7\u4e2a\u4f53\u7684\u8f85\u52a9\u6f5c\u529b\u3002", "motivation": "\u63a2\u7d22\u57fa\u4e8e\u8bed\u8a00\u7684\u4eba\u5de5\u667a\u80fd\u5728\u5fc3\u7406\u6cbb\u7597\u548c\u52a9\u6b8b\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u6539\u5584\u81ea\u95ed\u75c7\u4e2a\u4f53\u7684\u6c9f\u901a\u80fd\u529b\u3002", "method": "\u6bd4\u8f83GPT-3.5 Turbo\u548cGPT-4\u5728\u865a\u5047\u5931\u8bef\u6d4b\u8bd5\u3001\u793e\u4ea4\u6545\u4e8b\u95ee\u5377\u548c\u6545\u4e8b\u7406\u89e3\u6d4b\u8bd5\u7b49\u4e09\u4e2a\u66f4\u9ad8\u9636\u7684ToM\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u7531\u4e24\u540d\u72ec\u7acb\u8bc4\u5206\u8005\u8bc4\u5b9a\u51c6\u786e\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u6807\u8bb0\u3002", "result": "GPT-4\u5728\u865a\u5047\u5931\u8bef\u6d4b\u8bd5\u4e2d\u63a5\u8fd1\u4eba\u7c7b\u51c6\u786e\u5ea6\uff0c\u5e76\u4e14\u5728\u793e\u4ea4\u6545\u4e8b\u95ee\u5377\u548c\u6545\u4e8b\u7406\u89e3\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u81ea\u95ed\u75c7\u7279\u8d28\u4e2a\u4f53\uff0c\u800cGPT-3.5 Turbo\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u5c3d\u7ba1GPT-4\u5728\u590d\u6742ToM\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5176\u5bf9\u4e0d\u786e\u5b9a\u6027\u6807\u8bb0\u7684\u9891\u7e41\u4f7f\u7528\u8868\u660e\u8fd8\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\uff0c\u4ee5\u786e\u4fdd\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u4e00\u81f4\u6027\u548c\u53ef\u9760\u6027\u652f\u6301\u3002"}}
{"id": "2601.06728", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.06728", "abs": "https://arxiv.org/abs/2601.06728", "authors": ["Minhyuk Park", "Aloysius K. Mok", "Tsz-Chiu Au"], "title": "Robust Evacuation for Multi-Drone Failure in Drone Light Shows", "comment": "Accepted to AAAI-26 Bridge Program B10: Making Embodied AI Reliable with Testing and Formal Verification", "summary": "Drone light shows have emerged as a popular form of entertainment in recent years. However, several high-profile incidents involving large-scale drone failures -- where multiple drones simultaneously fall from the sky -- have raised safety and reliability concerns. To ensure robustness, we propose a drone parking algorithm designed specifically for multiple drone failures in drone light shows, aimed at mitigating the risk of cascading collisions by drone evacuation and enabling rapid recovery from failures by leveraging strategically placed hidden drones. Our algorithm integrates a Social LSTM model with attention mechanisms to predict the trajectories of failing drones and compute near-optimal evacuation paths that minimize the likelihood of surviving drones being hit by fallen drones. In the recovery node, our system deploys hidden drones (operating with their LED lights turned off) to replace failed drones so that the drone light show can continue. Our experiments showed that our approach can greatly increase the robustness of a multi-drone system by leveraging deep learning to predict the trajectories of fallen drones.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u4eba\u673a\u505c\u8f66\u7b97\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8\u65e0\u4eba\u673a\u706f\u5149\u79c0\u7684\u5b89\u5168\u6027\uff0c\u51cf\u5c11\u53d1\u751f\u7fa4\u4f53\u5760\u843d\u4e8b\u6545\u7684\u98ce\u9669\uff0c\u5e76\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u9884\u6d4b\u5760\u6bc1\u65e0\u4eba\u673a\u7684\u8f68\u8ff9\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u706f\u5149\u79c0\u7684\u6d41\u884c\uff0c\u76f8\u5173\u7684\u5b89\u5168\u548c\u53ef\u9760\u6027\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u7279\u522b\u662f\u591a\u65e0\u4eba\u673a\u540c\u65f6\u5760\u843d\u7684\u4e8b\u4ef6\u5f15\u53d1\u4e86\u5173\u6ce8\u3002", "method": "\u7ed3\u5408\u793e\u4f1aLSTM\u6a21\u578b\u4e0e\u6ce8\u610f\u529b\u673a\u5236\uff0c\u9884\u6d4b\u5931\u6548\u65e0\u4eba\u673a\u7684\u8f68\u8ff9\u5e76\u8ba1\u7b97\u8fd1\u4f3c\u6700\u4f18\u7684\u758f\u6563\u8def\u5f84\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u6240\u63d0\u7b97\u6cd5\u5728\u9884\u6d4b\u5760\u6bc1\u65e0\u4eba\u673a\u8f68\u8ff9\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65e0\u4eba\u673a\u505c\u8f66\u7b97\u6cd5\u80fd\u591f\u5927\u5e45\u63d0\u9ad8\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\uff0c\u786e\u4fdd\u65e0\u4eba\u673a\u706f\u5149\u79c0\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2601.06033", "categories": ["cs.HC", "cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.06033", "abs": "https://arxiv.org/abs/2601.06033", "authors": ["Patrick Gage Kelley", "Steven Rousso-Schindler", "Renee Shelby", "Kurt Thomas", "Allison Woodruff"], "title": "How Generative AI Empowers Attackers and Defenders Across the Trust & Safety Landscape", "comment": "28 pages, 4 tables, 1 figure", "summary": "Generative AI (GenAI) is a powerful technology poised to reshape Trust & Safety. While misuse by attackers is a growing concern, its defensive capacity remains underexplored. This paper examines these effects through a qualitative study with 43 Trust & Safety experts across five domains: child safety, election integrity, hate and harassment, scams, and violent extremism. Our findings characterize a landscape in which GenAI empowers both attackers and defenders. GenAI dramatically increases the scale and speed of attacks, lowering the barrier to entry for creating harmful content, including sophisticated propaganda and deepfakes. Conversely, defenders envision leveraging GenAI to detect and mitigate harmful content at scale, conduct investigations, deploy persuasive counternarratives, improve moderator wellbeing, and offer user support. This work provides a strategic framework for understanding GenAI's impact on Trust & Safety and charts a path for its responsible use in creating safer online environments.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u751f\u6210\u6027\u4eba\u5de5\u667a\u80fd\u5728\u4fe1\u4efb\u4e0e\u5b89\u5168\u9886\u57df\u7684\u5f71\u54cd\uff0c\u5f3a\u8c03\u4e86\u5176\u5728\u653b\u51fb\u548c\u9632\u5fa1\u65b9\u9762\u7684\u53cc\u91cd\u89d2\u8272\u3002", "motivation": "\u67e5\u660e\u751f\u6210\u6027\u4eba\u5de5\u667a\u80fd\u5728\u4fe1\u4efb\u4e0e\u5b89\u5168\u9886\u57df\u7684\u6f5c\u5728\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5176\u88ab\u653b\u51fb\u8005\u6ee5\u7528\u548c\u9632\u5fa1\u8005\u5229\u7528\u7684\u53cc\u91cd\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5bf943\u4f4d\u6765\u81ea\u4e94\u4e2a\u9886\u57df\u7684\u4fe1\u4efb\u4e0e\u5b89\u5168\u4e13\u5bb6\u7684\u5b9a\u6027\u7814\u7a76\u8fdb\u884c\u5206\u6790\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u751f\u6210\u6027\u4eba\u5de5\u667a\u80fd\u5728\u653b\u51fb\u548c\u9632\u5fa1\u65b9\u9762\u7684\u53cc\u9762\u6027\uff0c\u653b\u51fb\u8005\u5229\u7528\u5176\u5feb\u901f\u751f\u6210\u6709\u5bb3\u5185\u5bb9\uff0c\u800c\u9632\u5fa1\u8005\u4e5f\u5728\u63a2\u7d22\u5982\u4f55\u5229\u7528\u5176\u68c0\u6d4b\u548c\u7f13\u89e3\u8fd9\u4e9b\u6709\u5bb3\u5185\u5bb9\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6218\u7565\u6846\u67b6\uff0c\u4ee5\u7406\u89e3\u751f\u6210\u6027\u4eba\u5de5\u667a\u80fd\u5728\u4fe1\u4efb\u4e0e\u5b89\u5168\u4e2d\u7684\u5f71\u54cd\uff0c\u5e76\u4e3a\u5176\u5728\u521b\u5efa\u66f4\u5b89\u5168\u7684\u5728\u7ebf\u73af\u5883\u4e2d\u8d1f\u8d23\u4efb\u5730\u4f7f\u7528\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2601.06748", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.06748", "abs": "https://arxiv.org/abs/2601.06748", "authors": ["Changyu Liu", "Yiyang Liu", "Taowen Wang", "Qiao Zhuang", "James Chenhao Liang", "Wenhao Yang", "Renjing Xu", "Qifan Wang", "Dongfang Liu", "Cheng Han"], "title": "On-the-Fly VLA Adaptation via Test-Time Reinforcement Learning", "comment": null, "summary": "Vision-Language-Action models have recently emerged as a powerful paradigm for general-purpose robot learning, enabling agents to map visual observations and natural-language instructions into executable robotic actions. Though popular, they are primarily trained via supervised fine-tuning or training-time reinforcement learning, requiring explicit fine-tuning phases, human interventions, or controlled data collection. Consequently, existing methods remain unsuitable for challenging simulated- or physical-world deployments, where robots must respond autonomously and flexibly to evolving environments. To address this limitation, we introduce a Test-Time Reinforcement Learning for VLAs (TT-VLA), a framework that enables on-the-fly policy adaptation during inference. TT-VLA formulates a dense reward mechanism that leverages step-by-step task-progress signals to refine action policies during test time while preserving the SFT/RL-trained priors, making it an effective supplement to current VLA models. Empirical results show that our approach enhances overall adaptability, stability, and task success in dynamic, previously unseen scenarios under simulated and real-world settings. We believe TT-VLA offers a principled step toward self-improving, deployment-ready VLAs.", "AI": {"tldr": "TT-VLA\u4e3a\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728\u63a8\u7406\u65f6\u8fdb\u884c\u7b56\u7565\u9002\u5e94\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u5728\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u65f6\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u5f80\u5f80\u9700\u8981\u4eba\u5de5\u5e72\u9884\u548c\u6570\u636e\u6536\u96c6\uff0c\u800cTT-VLA\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f15\u5165\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\uff08TT-VLA\uff09\u6846\u67b6\uff0c\u5141\u8bb8\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u7b56\u7565\u9002\u5e94\u3002", "result": "\u901a\u8fc7\u5bc6\u96c6\u5956\u52b1\u673a\u5236\uff0c\u5229\u7528\u9010\u6b65\u4efb\u52a1\u8fdb\u5c55\u4fe1\u53f7\u5728\u6d4b\u8bd5\u65f6\u7ec6\u5316\u52a8\u4f5c\u7b56\u7565\uff0c\u540c\u65f6\u4fdd\u6301SFT/RL\u8bad\u7ec3\u7684\u5148\u9a8c\u77e5\u8bc6\u3002", "conclusion": "TT-VLA\u6709\u6548\u589e\u5f3a\u4e86\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u5728\u52a8\u6001\u548c\u672a\u89c1\u573a\u666f\u4e2d\u7684\u9002\u5e94\u6027\u3001\u7a33\u5b9a\u6027\u548c\u4efb\u52a1\u6210\u529f\u7387\u3002"}}
{"id": "2601.06364", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06364", "abs": "https://arxiv.org/abs/2601.06364", "authors": ["Xiaotian Zhang", "Jinhong Yu", "Pengwei Yan", "Le Jiang", "Xingyi Shen", "Mumo Cheng", "Xiaozhong Liu"], "title": "Human-in-the-Loop Interactive Report Generation for Chronic Disease Adherence", "comment": "5 pages, 3 figures. Accepted at the AAAI 2026 Workshop on AI for Healthy Aging and Longevity", "summary": "Chronic disease management requires regular adherence feedback to prevent avoidable hospitalizations, yet clinicians lack time to produce personalized patient communications. Manual authoring preserves clinical accuracy but does not scale; AI generation scales but can undermine trust in patient-facing contexts. We present a clinician-in-the-loop interface that constrains AI to data organization and preserves physician oversight through recognition-based review. A single-page editor pairs AI-generated section drafts with time-aligned visualizations, enabling inline editing with visual evidence for each claim. This division of labor (AI organizes, clinician decides) targets both efficiency and accountability. In a pilot with three physicians reviewing 24 cases, AI successfully generated clinically personalized drafts matching physicians' manual authoring practice (overall mean 4.86/10 vs. 5.0/10 baseline), requiring minimal physician editing (mean 8.3\\% content modification) with zero safety-critical issues, demonstrating effective automation of content generation. However, review time remained comparable to manual practice, revealing an accountability paradox: in high-stakes clinical contexts, professional responsibility requires complete verification regardless of AI accuracy. We contribute three interaction patterns for clinical AI collaboration: bounded generation with recognition-based review via chart-text pairing, automated urgency flagging that analyzes vital trends and adherence patterns with fail-safe escalation for missed critical monitoring tasks, and progressive disclosure controls that reduce cognitive load while maintaining oversight. These patterns indicate that clinical AI efficiency requires not only accurate models, but also mechanisms for selective verification that preserve accountability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408AI\u4e0e\u4e34\u5e8a\u533b\u751f\u7684\u754c\u9762\uff0c\u5c55\u73b0\u4e86AI\u5728\u4e34\u5e8a\u4e2a\u6027\u5316\u8349\u7a3f\u751f\u6210\u4e2d\u7684\u5e94\u7528\u4e0e\u6311\u6218\u3002", "motivation": "\u6162\u6027\u75c5\u7ba1\u7406\u9700\u8981\u5b9a\u671f\u7684\u9075\u4ece\u6027\u53cd\u9988\uff0c\u4ee5\u9632\u6b62\u53ef\u907f\u514d\u7684\u4f4f\u9662\uff0c\u4f46\u4e34\u5e8a\u533b\u751f\u7f3a\u4e4f\u65f6\u95f4\u6765\u751f\u6210\u4e2a\u6027\u5316\u7684\u75c5\u4eba\u6c9f\u901a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e34\u5e8a\u533b\u751f\u53c2\u4e0e\u7684\u754c\u9762\uff0cAI\u8d1f\u8d23\u6570\u636e\u7ec4\u7ec7\u5e76\u901a\u8fc7\u8bc6\u522b\u57fa\u7840\u7684\u5ba1\u67e5\u4fdd\u7559\u533b\u751f\u7684\u76d1\u7763\u3002", "result": "\u5728\u4e0e\u4e09\u4f4d\u533b\u751f\u5ba1\u67e5\u768424\u4e2a\u6848\u4f8b\u7684\u8bd5\u70b9\u4e2d\uff0cAI\u6210\u529f\u751f\u6210\u4e0e\u533b\u751f\u624b\u52a8\u64b0\u5199\u5b9e\u8df5\u76f8\u5339\u914d\u7684\u4e34\u5e8a\u4e2a\u6027\u5316\u8349\u7a3f\uff0c\u6574\u4f53\u5747\u503c\u4e3a4.86/10\uff0c\u8f83\u57fa\u7ebf5.0/10\u7a0d\u4f4e\uff0c\u4ec5\u9700\u8fdb\u884c\u5c11\u91cf\u7f16\u8f91\uff0c\u4e14\u65e0\u5b89\u5168\u5173\u952e\u6027\u95ee\u9898\u3002", "conclusion": "\u4e34\u5e8aAI\u7684\u6709\u6548\u6027\u4e0d\u4ec5\u4f9d\u8d56\u4e8e\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u9700\u8981\u9009\u62e9\u6027\u7684\u9a8c\u8bc1\u673a\u5236\u4ee5\u7ef4\u62a4\u8d23\u4efb\u611f\u3002"}}
{"id": "2601.06833", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.06833", "abs": "https://arxiv.org/abs/2601.06833", "authors": ["JaeHyung Jang", "JunHyeong Park", "Joong-Ku Lee", "Jee-Hwan Ryu"], "title": "SPINE Gripper: A Twisted Underactuated Mechanism-based Passive Mode-Transition Gripper", "comment": "11 pages, 10 figures. Preprint version of a manuscript submitted to IEEE Transactions on Mechatronics", "summary": "This paper presents a single-actuator passive gripper that achieves both stable grasping and continuous bidirectional in-hand rotation through mechanically encoded power transmission logic. Unlike conventional multifunctional grippers that require multiple actuators, sensors, or control-based switching, the proposed gripper transitions between grasping and rotation solely according to the magnitude of the applied input torque. The key enabler of this behavior is a Twisted Underactuated Mechanism (TUM), which generates non-coplanar motions, namely axial contraction and rotation, from a single rotational input while producing identical contraction regardless of rotation direction. A friction generator mechanically defines torque thresholds that govern passive mode switching, enabling stable grasp establishment before autonomously transitioning to in-hand rotation without sensing or active control. Analytical models describing the kinematics, elastic force generation, and torque transmission of the TUM are derived and experimentally validated. The fabricated gripper is evaluated through quantitative experiments on grasp success, friction-based grasp force regulation, and bidirectional rotation performance. System-level demonstrations, including bolt manipulation, object reorientation, and manipulator-integrated tasks driven solely by wrist torque, confirm reliable grasp to rotate transitions in both rotational directions. These results demonstrate that non-coplanar multifunctional manipulation can be realized through mechanical design alone, establishing mechanically encoded power transmission logic as a robust alternative to actuator and control intensive gripper architectures.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u5355\u81f4\u52a8\u5668\u88ab\u52a8\u6293\u624b\uff0c\u80fd\u591f\u901a\u8fc7\u65bd\u52a0\u626d\u77e9\u7684\u5927\u5c0f\uff0c\u5728\u6293\u63e1\u548c\u65cb\u8f6c\u4e4b\u95f4\u5e73\u7a33\u5207\u6362\uff0c\u5177\u5907\u4e86\u975e\u5171\u9762\u7684\u591a\u529f\u80fd\u64cd\u4f5c\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u591a\u529f\u80fd\u6293\u624b\u901a\u5e38\u4f9d\u8d56\u591a\u4e2a\u9a71\u52a8\u5668\u548c\u590d\u6742\u7684\u63a7\u5236\u7cfb\u7edf\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7b80\u5316\u8bbe\u8ba1\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u6293\u53d6\u548c\u65cb\u8f6c\u529f\u80fd\u3002", "method": "\u91c7\u7528\u626d\u8f6c\u6b20\u9a71\u52a8\u673a\u5236\uff08TUM\uff09\u5b9e\u73b0\u5355\u4e00\u65cb\u8f6c\u8f93\u5165\u4e0b\u7684\u8f74\u5411\u6536\u7f29\u548c\u65cb\u8f6c\u8fd0\u52a8\uff0c\u4ece\u800c\u7b80\u5316\u6293\u624b\u7684\u6784\u9020\u548c\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u6293\u624b\u5728\u6293\u63e1\u6210\u529f\u7387\u3001\u6469\u64e6\u63a7\u5236\u4e0b\u7684\u6293\u63e1\u529b\u5ea6\u548c\u53cc\u5411\u65cb\u8f6c\u6027\u80fd\u65b9\u9762\u7684\u4f18\u8d8a\u6027\uff0c\u4e14\u80fd\u5728\u6ca1\u6709\u4f20\u611f\u5668\u6216\u4e3b\u52a8\u63a7\u5236\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u7a33\u5b9a\u7684\u6293\u63e1\u4e0e\u65cb\u8f6c\u8f6c\u6362\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u68b0\u7f16\u7801\u529f\u7387\u4f20\u8f93\u903b\u8f91\u7684\u5355\u81f4\u52a8\u5668\u88ab\u52a8\u6293\u624b\uff0c\u80fd\u591f\u5b9e\u73b0\u7a33\u5b9a\u6293\u63e1\u548c\u8fde\u7eed\u53cc\u5411\u624b\u5185\u65cb\u8f6c\u3002"}}
{"id": "2601.06402", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.06402", "abs": "https://arxiv.org/abs/2601.06402", "authors": ["Woojin Jung", "Charles Chear", "Andrew H. Kim", "Vatsal Shah", "Tawfiq Ammari"], "title": "Spatiotemporal Change-Points in Development Discourse: Insights from Social Media in Low-Resource Contexts", "comment": null, "summary": "This study investigates the spatiotemporal evolution of development discourse in low-resource settings. Analyzing more than two years of geotagged X data from Zambia, we introduce a mixed-methods pipeline utilizing topic modeling, change-point detection, and qualitative coding to identify critical shifts in public debate. We identify seven recurring themes, including public health challenges and frustration with government policy, shaped by regional events and national interventions. Notably, we detect discourse changepoints linked to the COVID19 pandemic and a geothermal project, illustrating how online conversations mirror policy flashpoints. Our analysis distinguishes between the ephemeral nature of acute crises like COVID19 and the persistent, structural reorientations driven by long-term infrastructure projects. We conceptualize \"durable discourse\" as sustained narrative engagement with development issues. Contributing to HCI and ICTD, we examine technology's socioeconomic impact, providing practical implications and future work for direct local engagement.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u8d5e\u6bd4\u4e9a\u7684\u793e\u4ea4\u5a92\u4f53\u6570\u636e\uff0c\u53d1\u73b0\u53d1\u5c55\u8bdd\u8bed\u6709\u4e03\u4e2a\u4e3b\u9898\uff0c\u53d7COVID-19\u548c\u57fa\u7840\u8bbe\u65bd\u9879\u76ee\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u201c\u6301\u4e45\u8bdd\u8bed\u201d\u7684\u6982\u5ff5\u3002", "motivation": "\u672c\u7814\u7a76\u610f\u5728\u63ed\u793a\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\u53d1\u5c55\u8bdd\u8bed\u7684\u52a8\u6001\u53d8\u5316\u53ca\u5176\u5bf9\u6280\u672f\u4e0e\u793e\u4f1a\u7ecf\u6d4e\u5f71\u54cd\u7684\u7406\u89e3\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u7ba1\u9053\uff0c\u5305\u62ec\u4e3b\u9898\u5efa\u6a21\u3001\u53d8\u5316\u70b9\u68c0\u6d4b\u4e0e\u5b9a\u6027\u7f16\u7801\uff0c\u5206\u6790\u8d85\u8fc7\u4e24\u5e74\u7684\u5730\u7406\u6807\u8bb0\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u3002", "result": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\u53d1\u5c55\u8bdd\u8bed\u7684\u65f6\u7a7a\u6f14\u53d8\uff0c\u8bc6\u522b\u51fa\u4e0e\u516c\u5171\u5065\u5eb7\u548c\u653f\u5e9c\u653f\u7b56\u76f8\u5173\u7684\u4e03\u4e2a\u4e3b\u9898\uff0c\u53ca\u5176\u968f\u65f6\u95f4\u7684\u53d8\u5316\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u7ebf\u8ba8\u8bba\u53cd\u6620\u4e86\u653f\u7b56\u7684\u5173\u952e\u70b9\uff0c\u5c24\u5176\u662f\u4e0eCOVID-19\u75ab\u60c5\u76f8\u5173\u7684\u77ed\u671f\u5371\u673a\u4e0e\u957f\u671f\u57fa\u7840\u8bbe\u65bd\u9879\u76ee\u7684\u6301\u7eed\u6027\u8bdd\u8bed\u4e4b\u95f4\u7684\u533a\u522b\u3002"}}
{"id": "2601.06854", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.06854", "abs": "https://arxiv.org/abs/2601.06854", "authors": ["Luigi Romano", "Ole Morten Aamo", "Jan \u00c5slund", "Erik Frisk"], "title": "Semilinear single-track vehicle models with distributed tyre friction dynamics", "comment": "37 pages, 12 figures. Accepted by Nonlinear Dynamics", "summary": "This paper introduces a novel family of single-track vehicle models that incorporate a distributed representation of transient tyre dynamics, whilst simultaneously accounting for nonlinear effects induced by friction. The core of the proposed framework is represented by the distributed Friction with Bristle Dynamics (FrBD) model, which unifies and extends classical formulations such as Dahl and LuGre by describing the rolling contact process as a spatially distributed system governed by semilinear partial differential equations (PDEs). This model is systematically integrated into a single-track vehicle framework, where the resulting semilinear ODE-PDE interconnection captures the interaction between lateral vehicle motion and tyre deformation. Two main variants are considered: one with rigid tyre carcass and another with flexible carcass, each admitting a compact state-space representation. Local and global well-posedness properties for the coupled system are established rigorously, highlighting the dissipative and physically consistent properties of the distributed FrBD model. A linearisation procedure is also presented, enabling spectral analysis and transfer function derivation, and potentially facilitating the synthesis of controllers and observers. Numerical simulations demonstrate the model's capability to capture micro-shimmy oscillations and transient lateral responses to advanced steering manoeuvres. The proposed formulation advances the state-of-the-art in vehicle dynamics modelling by providing a physically grounded, mathematically rigorous, and computationally tractable approach to incorporating transient tyre behaviour in lateral vehicle dynamics, when accounting for the effect of limited friction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u5355\u8f66\u9053\u8f66\u8f86\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u6469\u64e6\u4e0e\u5237\u5b50\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u7cfb\u7edf\u5730\u6574\u5408\u8f6e\u80ce\u52a8\u6001\u53ca\u6469\u64e6\u975e\u7ebf\u6027\uff0c\u63d0\u5347\u4e86\u8f66\u8f86\u52a8\u529b\u5b66\u5efa\u6a21\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u5355\u8f66\u9053\u8f66\u8f86\u6a21\u578b\uff0c\u4ee5\u4fbf\u66f4\u597d\u5730\u7406\u89e3\u548c\u6a21\u62df\u8f6e\u80ce\u52a8\u6001\u53ca\u6469\u64e6\u5f15\u8d77\u7684\u975e\u7ebf\u6027\u6548\u5e94\u3002", "method": "\u4f7f\u7528\u5206\u5e03\u5f0f\u6469\u64e6\u4e0e\u5237\u5b50\u52a8\u529b\u5b66\uff08FrBD\uff09\u6a21\u578b\uff0c\u7ed3\u5408\u7ecf\u5178\u6a21\u578b\u5982Dahl\u548cLuGre\uff0c\u5e76\u901a\u8fc7\u534a\u7ebf\u6027\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u63cf\u8ff0\u6eda\u52a8\u63a5\u89e6\u8fc7\u7a0b\u3002\u5c06\u8fd9\u79cd\u6a21\u578b\u7cfb\u7edf\u5730\u6574\u5408\u8fdb\u5355\u8f66\u9053\u8f66\u8f86\u6846\u67b6\u4e2d\uff0c\u4ee5\u6355\u6349\u8f66\u8f86\u6a2a\u5411\u8fd0\u52a8\u4e0e\u8f6e\u80ce\u53d8\u5f62\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u5efa\u7acb\u4e86\u8026\u5408\u7cfb\u7edf\u7684\u5c40\u90e8\u548c\u5168\u5c40\u826f\u89e3\u6027\uff0c\u5c55\u793a\u4e86\u5206\u5e03\u5f0fFrBD\u6a21\u578b\u7684\u8017\u6563\u6027\u548c\u7269\u7406\u4e00\u81f4\u6027\uff1b\u8fdb\u884c\u7ebf\u6027\u5316\u7a0b\u5e8f\u4ee5\u5b9e\u73b0\u8c31\u5206\u6790\u548c\u4f20\u9012\u51fd\u6570\u63a8\u5bfc\uff0c\u652f\u6301\u63a7\u5236\u5668\u548c\u89c2\u6d4b\u5668\u7684\u5408\u6210\uff1b\u6570\u503c\u4eff\u771f\u9a8c\u8bc1\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8f66\u8f86\u52a8\u529b\u5b66\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7684\u3001\u6570\u5b66\u4e0a\u4e25\u683c\u4e14\u8ba1\u7b97\u4e0a\u53ef\u884c\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63a8\u8fdb\u4e86\u8f6e\u80ce\u5728\u6709\u9650\u6469\u64e6\u60c5\u51b5\u4e0b\u7684\u77ac\u6001\u884c\u4e3a\u7684\u7406\u89e3\u4e0e\u6a21\u62df\u3002"}}
{"id": "2601.06516", "categories": ["cs.HC", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.06516", "abs": "https://arxiv.org/abs/2601.06516", "authors": ["Carl Vincent Ladres Kho"], "title": "Pareto-Optimal Model Selection for Low-Cost, Single-Lead EMG Control in Embedded Systems", "comment": "15 pages main text, 51 pages total including appendices. 18 figures. Code and dataset available at: https://github.com/CarlKho-Minerva/v2-emg-muscle", "summary": "Consumer-grade biosensors offer a cost-effective alternative to medical-grade electromyography (EMG) systems, reducing hardware costs from thousands of dollars to approximately $13. However, these low-cost sensors introduce significant signal instability and motion artifacts. Deploying machine learning models on resource-constrained edge devices like the ESP32 presents a challenge: balancing classification accuracy with strict latency (<100ms) and memory (<320KB) constraints. Using a single-subject dataset comprising 1,540 seconds of raw data (1.54M data points, segmented into ~1,300 one-second windows), I evaluate 18 model architectures, ranging from statistical heuristics to deep transfer learning (ResNet50) and custom hybrid networks (MaxCRNN). While my custom \"MaxCRNN\" (Inception + Bi-LSTM + Attention) achieved the highest safety (99% Precision) and robustness, I identify Random Forest (74% accuracy) as the Pareto-optimal solution for embedded control on legacy microcontrollers. I demonstrate that reliable, low-latency EMG control is feasible on commodity hardware, with Deep Learning offering a path to near-perfect reliability on modern Edge AI accelerators.", "AI": {"tldr": "\u6d88\u8d39\u8005\u7ea7\u751f\u7269\u4f20\u611f\u5668\u4e3a\u533b\u7597\u7ea7\u808c\u7535\u56fe\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u66ff\u4ee3\u65b9\u6848\uff0c\u5c3d\u7ba1\u4f4e\u6210\u672c\u4f20\u611f\u5668\u5b58\u5728\u663e\u8457\u7684\u4fe1\u53f7\u4e0d\u7a33\u5b9a\u6027\u548c\u8fd0\u52a8\u4f2a\u5f71\u3002", "motivation": "\u63a8\u52a8\u751f\u7269\u4f20\u611f\u5668\u5728\u4f4e\u6210\u672c\u786c\u4ef6\u4e0a\u7684\u5e94\u7528\uff0c\u4ee5\u5e94\u5bf9\u533b\u7597\u7ea7\u8bbe\u5907\u7684\u9ad8\u6210\u672c\u548c\u590d\u6742\u6027\u3002", "method": "\u8bc4\u4f3018\u79cd\u6a21\u578b\u67b6\u6784\uff0c\u5305\u62ec\u7edf\u8ba1\u542f\u53d1\u5f0f\u3001\u6df1\u5ea6\u8fc1\u79fb\u5b66\u4e60\uff08ResNet50\uff09\u548c\u81ea\u5b9a\u4e49\u6df7\u5408\u7f51\u7edc\uff08MaxCRNN\uff09\uff0c\u4f7f\u75281,540\u79d2\u7684\u5355\u4e00\u53d7\u8bd5\u8005\u6570\u636e\u96c6\u3002", "result": "\u81ea\u5b9a\u4e49MaxCRNN\u6a21\u578b\u5728\u7cbe\u51c6\u5ea6\u4e0a\uff0899% \u7cbe\u786e\u7387\uff09\u8868\u73b0\u6700\u4f73\uff0c\u968f\u673a\u68ee\u6797\uff0874% \u51c6\u786e\u7387\uff09\u88ab\u786e\u5b9a\u4e3a\u5d4c\u5165\u5f0f\u63a7\u5236\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u89e3\u3002", "conclusion": "\u5728\u5546\u54c1\u786c\u4ef6\u4e0a\u5b9e\u73b0\u53ef\u9760\u7684\u4f4e\u5ef6\u8fdfEMG\u63a7\u5236\u662f\u53ef\u884c\u7684\uff0c\u5e76\u4e14\u6df1\u5ea6\u5b66\u4e60\u4e3a\u73b0\u4ee3\u8fb9\u7f18AI\u52a0\u901f\u5668\u63d0\u4f9b\u4e86\u63a5\u8fd1\u5b8c\u7f8e\u53ef\u9760\u6027\u7684\u9014\u5f84\u3002"}}
{"id": "2601.06887", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.06887", "abs": "https://arxiv.org/abs/2601.06887", "authors": ["Yin Zhang", "Zian Ning", "Shiyu Zhao"], "title": "Observability-Enhanced Target Motion Estimation via Bearing-Box: Theory and MAV Applications", "comment": "This paper is accepted by IEEE Transactions on Robotics (20 pages, 11 figures)", "summary": "Monocular vision-based target motion estimation is a fundamental challenge in numerous applications. This work introduces a novel bearing-box approach that fully leverages modern 3D detection measurements that are widely available nowadays but have not been well explored for motion estimation so far. Unlike existing methods that rely on restrictive assumptions such as isotropic target shape and lateral motion, our bearing-box estimator can estimate both the target's motion and its physical size without these assumptions by exploiting the information buried in a 3D bounding box. When applied to multi-rotor micro aerial vehicles (MAVs), the estimator yields an interesting advantage: it further removes the need for higher-order motion assumptions by exploiting the unique coupling between MAV's acceleration and thrust. This is particularly significant, as higher-order motion assumptions are widely believed to be necessary in state-of-the-art bearing-based estimators. We support our claims with rigorous observability analyses and extensive experimental validation, demonstrating the estimator's superior performance in real-world scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684bearing-box\u65b9\u6cd5\uff0c\u7528\u4e8e\u5355\u76ee\u89c6\u89c9\u4e0b\u76ee\u6807\u8fd0\u52a8\u4f30\u8ba1\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u9650\u5236\uff0c\u5c55\u793a\u51fa\u5728\u591a\u65cb\u7ffc\u5fae\u578b\u98de\u884c\u5668\u4e2d\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u5355\u76ee\u89c6\u89c9\u76ee\u6807\u8fd0\u52a8\u4f30\u8ba1\u5728\u591a\u4e2a\u5e94\u7528\u4e2d\u662f\u4e00\u4e2a\u57fa\u672c\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u4f9d\u8d56\u4e8e\u9650\u5236\u6027\u5047\u8bbe\u3002", "method": "\u5f15\u5165bearing-box\u65b9\u6cd5\uff0c\u5229\u75283D\u8fb9\u754c\u6846\u4e2d\u7684\u4fe1\u606f\u8fdb\u884c\u76ee\u6807\u8fd0\u52a8\u548c\u7269\u7406\u5927\u5c0f\u7684\u4f30\u8ba1\uff0c\u4e14\u65e0\u987b\u5047\u8bbe\u76ee\u6807\u5f62\u72b6\u548c\u8fd0\u52a8\u65b9\u5411\u3002", "result": "\u901a\u8fc7\u4e25\u683c\u7684\u53ef\u89c2\u6d4b\u6027\u5206\u6790\u548c\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u4f30\u8ba1\u5668\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u901a\u8fc7\u5229\u75283D\u68c0\u6d4b\u6d4b\u91cf\u4e2d\u7684\u4fe1\u606f\uff0c\u76f8\u8f83\u4e8e\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u66f4\u7075\u6d3b\u7684\u76ee\u6807\u8fd0\u52a8\u4f30\u8ba1\u80fd\u529b\uff0c\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2601.06611", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.06611", "abs": "https://arxiv.org/abs/2601.06611", "authors": ["Nelly Elsayed"], "title": "AI Washing and the Erosion of Digital Legitimacy: A Socio-Technical Perspective on Responsible Artificial Intelligence in Business", "comment": "38 pages, uner review", "summary": "The rapid evolution of artificial intelligence (AI) systems, tools, and technologies has opened up novel, unprecedented opportunities for businesses to innovate, differentiate, and compete. However, growing concerns have emerged about the use of AI in businesses, particularly AI washing, in which firms exaggerate, misrepresent, or superficially signal their AI capabilities to gain financial and reputational advantages. This paper aims to establish a conceptual foundation for understanding AI washing. In this paper, we draw on analogies from greenwashing and insights from Information Systems (IS) research on ethics, trust, signaling, and digital innovation. This paper proposes a typology of AI washing practices across four primary domains: marketing and branding, technical capability inflation, strategic signaling, and governance-based washing. In addition, we examine their organizational, industry, and societal impacts. Our investigation and analysis reveal how AI washing can lead to short-term gains; however, it also proposes severe long-term consequences, including reputational damage, erosion of trust, and misallocation of resources. Moreover, this paper examines current research directions and open questions aimed at mitigating AI washing practices and enhancing the trust and reliability of legitimate AI systems and technologies.", "AI": {"tldr": "\u4eba\u5de5\u667a\u80fd\u7684\u8fc5\u901f\u53d1\u5c55\u4e3a\u4f01\u4e1a\u521b\u65b0\u5f00\u8f9f\u4e86\u65b0\u673a\u4f1a\uff0c\u4f46\u4e5f\u5f15\u53d1\u4e86\u5bf9AI\u6d17\u6da4\u7684\u62c5\u5fe7\uff0c\u672c\u6587\u5efa\u7acb\u4e86\u6709\u5173AI\u6d17\u6da4\u7684\u6982\u5ff5\u57fa\u7840\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5173\u5b9e\u8df5\u7684\u5206\u7c7b\u53ca\u5176\u5f71\u54cd\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u5efa\u7acb\u4e00\u4e2a\u7406\u89e3AI\u6d17\u6da4\u7684\u6982\u5ff5\u57fa\u7840\uff0c\u4ee5\u5e94\u5bf9\u4f01\u4e1a\u5728\u4f7f\u7528AI\u6280\u672f\u65f6\u51fa\u73b0\u7684\u5938\u5927\u548c\u8bef\u5bfc\u884c\u4e3a\u3002", "method": "\u501f\u9274\u7eff\u8272\u6d17\u6da4\u7684\u7c7b\u6bd4\uff0c\u5e76\u7ed3\u5408\u4fe1\u606f\u7cfb\u7edf\u7814\u7a76\u4e2d\u7684\u4f26\u7406\u3001\u4fe1\u4efb\u3001\u4fe1\u53f7\u548c\u6570\u5b57\u521b\u65b0\u7684\u6d1e\u89c1\uff0c\u63d0\u51faAI\u6d17\u6da4\u5b9e\u8df5\u7684\u5206\u7c7b\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0cAI\u6d17\u6da4\u867d\u7136\u53ef\u4ee5\u5e26\u6765\u77ed\u671f\u6536\u76ca\uff0c\u4f46\u957f\u671f\u540e\u679c\u4e25\u91cd\uff0c\u4e14\u5bf9\u7ec4\u7ec7\u3001\u884c\u4e1a\u548c\u793e\u4f1a\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002", "conclusion": "AI washing\u53ef\u80fd\u5728\u77ed\u671f\u5185\u5e26\u6765\u5229\u76ca\uff0c\u4f46\u4ece\u957f\u8fdc\u6765\u770b\u4f1a\u5e26\u6765\u58f0\u8a89\u635f\u5bb3\u3001\u4fe1\u4efb\u4fb5\u8680\u548c\u8d44\u6e90\u9519\u914d\u7b49\u4e25\u91cd\u540e\u679c\u3002"}}
{"id": "2601.06997", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.06997", "abs": "https://arxiv.org/abs/2601.06997", "authors": ["Yuetao Li", "Zhizhou Jia", "Yu Zhang", "Qun Hao", "Shaohui Zhang"], "title": "ObjSplat: Geometry-Aware Gaussian Surfels for Active Object Reconstruction", "comment": "Project Page: https://li-yuetao.github.io/ObjSplat-page/", "summary": "Autonomous high-fidelity object reconstruction is fundamental for creating digital assets and bridging the simulation-to-reality gap in robotics. We present ObjSplat, an active reconstruction framework that leverages Gaussian surfels as a unified representation to progressively reconstruct unknown objects with both photorealistic appearance and accurate geometry. Addressing the limitations of conventional opacity or depth-based cues, we introduce a geometry-aware viewpoint evaluation pipeline that explicitly models back-face visibility and occlusion-aware multi-view covisibility, reliably identifying under-reconstructed regions even on geometrically complex objects. Furthermore, to overcome the limitations of greedy planning strategies, ObjSplat employs a next-best-path (NBP) planner that performs multi-step lookahead on a dynamically constructed spatial graph. By jointly optimizing information gain and movement cost, this planner generates globally efficient trajectories. Extensive experiments in simulation and on real-world cultural artifacts demonstrate that ObjSplat produces physically consistent models within minutes, achieving superior reconstruction fidelity and surface completeness while significantly reducing scan time and path length compared to state-of-the-art approaches. Project page: https://li-yuetao.github.io/ObjSplat-page/ .", "AI": {"tldr": "ObjSplat\u662f\u4e00\u4e2a\u57fa\u4e8e\u9ad8\u65af\u8868\u9762\u70b9\u7684\u4e3b\u52a8\u91cd\u5efa\u6846\u67b6\uff0c\u80fd\u5feb\u901f\u9ad8\u4fdd\u771f\u91cd\u5efa\u7269\u4f53\uff0c\u663e\u8457\u63d0\u9ad8\u626b\u63cf\u6548\u7387\u4e0e\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u81ea\u4e3b\u9ad8\u4fdd\u771f\u7269\u4f53\u91cd\u5efa\u5bf9\u4e8e\u521b\u9020\u6570\u5b57\u8d44\u4ea7\u548c\u5f25\u5408\u673a\u5668\u4eba\u6280\u672f\u4e2d\u7684\u4eff\u771f\u4e0e\u73b0\u5b9e\u7684\u5dee\u8ddd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86ObjSplat\uff0c\u4e00\u4e2a\u5229\u7528\u9ad8\u65af\u8868\u9762\u70b9\u7684\u4e3b\u52a8\u91cd\u5efa\u6846\u67b6\uff0c\u9010\u6b65\u91cd\u5efa\u672a\u77e5\u7269\u4f53\u7684\u5149\u7167\u903c\u771f\u5916\u89c2\u548c\u51c6\u786e\u51e0\u4f55\u5f62\u72b6\u3002\u5f15\u5165\u51e0\u4f55\u611f\u77e5\u7684\u89c6\u70b9\u8bc4\u4f30\u7ba1\u9053\uff0c\u6a21\u578b\u5316\u80cc\u9762\u53ef\u89c1\u6027\u548c\u906e\u6321\u611f\u77e5\u7684\u591a\u89c6\u89d2\u5171\u53ef\u89c1\u6027\u3002\u4f7f\u7528\u4e0b\u4e00\u6700\u4f73\u8def\u5f84\u89c4\u5212\u5668\u6267\u884c\u591a\u6b65\u524d\u77bb\u6027\u89c4\u5212\uff0c\u5728\u52a8\u6001\u6784\u5efa\u7684\u7a7a\u95f4\u56fe\u4e0a\u751f\u6210\u9ad8\u6548\u7684\u8f68\u8ff9\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u6587\u5316\u9057\u7269\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cObjSplat\u80fd\u591f\u5728\u6570\u5206\u949f\u5185\u751f\u6210\u7269\u7406\u4e00\u81f4\u7684\u6a21\u578b\uff0c\u5177\u6709\u4f18\u8d8a\u7684\u91cd\u5efa\u4fdd\u771f\u5ea6\u548c\u8868\u9762\u5b8c\u6574\u6027\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u626b\u63cf\u65f6\u95f4\u548c\u8def\u5f84\u957f\u5ea6\u3002", "conclusion": "ObjSplat\u5728\u91cd\u5efa\u6548\u7387\u548c\u8d28\u91cf\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2601.06616", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.06616", "abs": "https://arxiv.org/abs/2601.06616", "authors": ["Blessing Jerry", "Lourdes Moreno", "Virginia Francisco", "Raquel Hervas"], "title": "LLM-Driven Accessible Interface: A Model-Based Approach", "comment": null, "summary": "The integration of Large Language Models (LLMs) into interactive systems opens new opportunities for adaptive user experiences, yet it also raises challenges regarding accessibility, explainability, and normative compliance. This paper presents an implemented model-driven architecture for generating personalised, multimodal, and accessibility-aligned user interfaces. The approach combines structured user profiles, declarative adaptation rules, and validated prompt templates to refine baseline accessible UI templates that conform to WCAG 2.2 and EN 301 549, tailored to cognitive and sensory support needs. LLMs dynamically transform language complexity, modality, and visual structure, producing outputs such as Plain-Language text, pictograms, and high-contrast layouts aligned with ISO 24495-1 and W3C COGA guidance. A healthcare use case demonstrates how the system generates accessible post-consultation medication instructions tailored to a user profile comprising cognitive disability and hearing impairment. SysML v2 models provide explicit traceability between user needs, adaptation rules, and normative requirements, ensuring explainable and auditable transformations. Grounded in Human-Centered AI (HCAI), the framework incorporates co-design processes and structured feedback mechanisms to guide iterative refinement and support trustworthy generative behaviour.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u9a71\u52a8\u7684\u67b6\u6784\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e2a\u6027\u5316\u7684\u53ef\u53ca\u6027\u7528\u6237\u754c\u9762\uff0c\u65e8\u5728\u63d0\u5347\u4ea4\u4e92\u7cfb\u7edf\u7684\u7528\u6237\u4f53\u9a8c\uff0c\u540c\u65f6\u786e\u4fdd\u7b26\u5408\u89c4\u8303\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5728\u4ea4\u4e92\u7cfb\u7edf\u4e2d\u6574\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5f00\u542f\u4e86\u81ea\u9002\u5e94\u7528\u6237\u4f53\u9a8c\u7684\u65b0\u673a\u4f1a\uff0c\u4f46\u4e5f\u5e26\u6765\u4e86\u53ef\u53ca\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u89c4\u8303\u9075\u4ece\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u9a71\u52a8\u7684\u67b6\u6784\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u7528\u6237\u6863\u6848\u3001\u58f0\u660e\u5f0f\u9002\u914d\u89c4\u5219\u548c\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u63d0\u793a\u6a21\u677f\uff0c\u751f\u6210\u4e2a\u6027\u5316\u3001\u591a\u6a21\u6001\u4e14\u7b26\u5408\u53ef\u53ca\u6027\u8981\u6c42\u7684\u7528\u6237\u754c\u9762\u3002", "result": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u8f6c\u5316\u8bed\u8a00\u590d\u6742\u5ea6\u3001\u6a21\u6001\u548c\u89c6\u89c9\u7ed3\u6784\uff0c\u8f93\u51fa\u7b26\u5408ISO 24495-1\u548cW3C COGA\u6307\u5bfc\u7684\u53ef\u53ca\u6027\u6a21\u677f\uff0c\u4f8b\u5982\u7b80\u5355\u8bed\u8a00\u6587\u672c\u3001\u56fe\u6807\u548c\u9ad8\u5bf9\u6bd4\u5ea6\u5e03\u5c40\u3002", "conclusion": "\u901a\u8fc7SysML v2\u6a21\u578b\u63d0\u4f9b\u7528\u6237\u9700\u6c42\u3001\u9002\u914d\u89c4\u5219\u548c\u89c4\u8303\u8981\u6c42\u4e4b\u95f4\u7684\u660e\u786e\u53ef\u8ffd\u6eaf\u6027\uff0c\u786e\u4fdd\u4e86\u53ef\u89e3\u91ca\u548c\u53ef\u5ba1\u8ba1\u7684\u8f6c\u5316\uff0c\u5e76\u5728\u4ee5\u4eba\u4e3a\u672c\u7684AI\u6846\u67b6\u4e0b\uff0c\u7ed3\u5408\u5171\u540c\u8bbe\u8ba1\u8fc7\u7a0b\u548c\u7ed3\u6784\u5316\u53cd\u9988\u673a\u5236\uff0c\u6307\u5bfc\u8fed\u4ee3\u7cbe\u7ec6\u5316\u5e76\u652f\u6301\u53ef\u4fe1\u8d56\u7684\u751f\u6210\u884c\u4e3a\u3002"}}
{"id": "2601.07009", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.07009", "abs": "https://arxiv.org/abs/2601.07009", "authors": ["Shifa Sulaiman", "Mohammad Gohari", "Francesco Schetter", "Fanny Ficuciello"], "title": "A Sliding Mode Controller Based on Timoshenko Beam Theory Developed for a Tendon-Driven Robotic Wrist", "comment": null, "summary": "Development of dexterous robotic joints is essential for advancing manipulation capabilities in robotic systems. This paper presents the design and implementation of a tendon-driven robotic wrist joint together with an efficient Sliding Mode Controller (SMC) for precise motion control. The wrist mechanism is modeled using a Timoshenko-based approach to accurately capture its kinematic and dynamic properties, which serve as the foundation for tendon force calculations within the controller. The proposed SMC is designed to deliver fast dynamic response and computational efficiency, enabling accurate trajectory tracking under varying operating conditions. The effectiveness of the controller is validated through comparative analyses with existing controllers for similar wrist mechanisms. The proposed SMC demonstrates superior performance in both simulation and experimental studies. The Root Mean Square Error (RMSE) in simulation is approximately 1.67e-2 radians, while experimental validation yields an error of 0.2 radians. Additionally, the controller achieves a settling time of less than 3 seconds and a steady-state error below 1e-1 radians, consistently observed across both simulation and experimental evaluations. Comparative analyses confirm that the developed SMC surpasses alternative control strategies in motion accuracy, rapid convergence, and steady-state precision. This work establishes a foundation for future exploration of tendon-driven wrist mechanisms and control strategies in robotic applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u79cd\u8171\u9a71\u52a8\u7684\u673a\u5668\u4eba\u8155\u5173\u8282\u53ca\u9ad8\u6548\u7684\u6ed1\u6a21\u63a7\u5236\u5668\uff0c\u5c55\u793a\u4e86\u5728\u591a\u53d8\u64cd\u4f5c\u6761\u4ef6\u4e0b\u7684\u7cbe\u786e\u8fd0\u52a8\u63a7\u5236\u548c\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u53d1\u5c55\u7075\u5de7\u7684\u673a\u5668\u4eba\u5173\u8282\u662f\u63d0\u9ad8\u673a\u5668\u4eba\u7cfb\u7edf\u64cd\u63a7\u80fd\u529b\u7684\u5173\u952e\u3002", "method": "\u91c7\u7528\u57fa\u4e8eTimoshenko\u7684\u65b9\u6cd5\u5efa\u6a21\u8155\u5173\u8282\u673a\u5236\uff0c\u5e76\u8bbe\u8ba1\u6ed1\u6a21\u63a7\u5236\u5668\u4ee5\u5b9e\u73b0\u7cbe\u51c6\u7684\u8fd0\u52a8\u63a7\u5236\u3002", "result": "\u6ed1\u6a21\u63a7\u5236\u5668\u5728\u6a21\u62df\u548c\u5b9e\u9a8c\u4e2d\u7684\u8868\u73b0\u4f18\u8d8a\uff0cRMSE\u5206\u522b\u4e3a1.67e-2\u5f27\u5ea6\u548c0.2\u5f27\u5ea6\uff0c\u8fbe\u5230\u5c0f\u4e8e3\u79d2\u7684\u7a33\u6001\u65f6\u95f4\u53ca\u5c0f\u4e8e1e-1\u5f27\u5ea6\u7684\u7a33\u6001\u8bef\u5dee\u3002", "conclusion": "\u63d0\u51fa\u7684\u6ed1\u6a21\u63a7\u5236\u5668\u5728\u8fd0\u52a8\u51c6\u786e\u6027\u548c\u5feb\u901f\u6536\u655b\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u63a7\u5236\u7b56\u7565\uff0c\u4e3a\u672a\u6765\u7684\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u8171\u9a71\u52a8\u8155\u673a\u5236\u53ca\u63a7\u5236\u7b56\u7565\u7684\u63a2\u7d22\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.06650", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.06650", "abs": "https://arxiv.org/abs/2601.06650", "authors": ["Qian Ma", "Yingfan Zhou", "Shubhang Kaushik", "Aamod Joshi", "Aditya Majumdar", "Noah Apthorpe", "Yan Shvartzshnaider", "Sarah Rajtmajer", "Brett Frischmann"], "title": "Learning Password Best Practices Through In-Task Instruction", "comment": "16 pages, 7 figures, 16 tables", "summary": "Users often make security- and privacy-relevant decisions without a clear understanding of the rules that govern safe behavior. We introduce pedagogical friction, a design approach that introduces brief, instructional interactions at the moment of action. We evaluate this approach in the context of password creation, a task with clear, objective quality criteria and broad familiarity. We conducted a randomized repeated-measures study with 128 participants across four interface conditions that varied the depth and interactivity of guidance. We assessed three outcomes: (1) rule compliance in a subsequent password task without guidance, (2) accuracy on survey questions matched to the rules shown earlier, and (3) behavior-knowledge alignment, which captures whether participants who correctly followed a rule also recognized it on the survey. Across all guided conditions, participants corrected most rule violations in the follow-up task, achieved moderate accuracy on matched rule questions, and showed high behavior-knowledge alignment. These results support pedagogical friction as a lightweight and generalizable intervention for security- and privacy-critical interfaces.", "AI": {"tldr": "\u7814\u7a76\u5f15\u5165\u6559\u5b66\u6469\u64e6\u7684\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u5bc6\u7801\u521b\u5efa\u65f6\u63d0\u4f9b\u5373\u65f6\u6307\u5bfc\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u7528\u6237\u7684\u89c4\u5219\u9075\u4ece\u6027\u548c\u884c\u4e3a\u8ba4\u77e5\u3002", "motivation": "\u7528\u6237\u5728\u505a\u51fa\u4e0e\u5b89\u5168\u548c\u9690\u79c1\u76f8\u5173\u7684\u51b3\u7b56\u65f6\uff0c\u5e38\u5e38\u7f3a\u4e4f\u5bf9\u5b89\u5168\u884c\u4e3a\u89c4\u5219\u7684\u6e05\u6670\u7406\u89e3\u3002", "method": "\u8fdb\u884c\u4e86\u968f\u673a\u91cd\u590d\u6d4b\u91cf\u7814\u7a76\uff0c\u5171\u6709128\u540d\u53c2\u4e0e\u8005\uff0c\u6d4b\u8bd5\u4e86\u56db\u79cd\u4e0d\u540c\u754c\u9762\u6761\u4ef6\u4e0b\u7684\u5f15\u5bfc\u6df1\u5ea6\u548c\u4e92\u52a8\u6027\u3002", "result": "\u53c2\u4e0e\u8005\u5728\u65e0\u6307\u5bfc\u7684\u540e\u7eed\u5bc6\u7801\u4efb\u52a1\u4e2d\u7ea0\u6b63\u4e86\u5927\u90e8\u5206\u89c4\u5219\u8fdd\u53cd\uff0c\u89c4\u5219\u76f8\u5173\u95ee\u5377\u7684\u51c6\u786e\u6027\u5904\u4e8e\u4e2d\u7b49\u6c34\u5e73\uff0c\u5e76\u4e14\u884c\u4e3a\u4e0e\u77e5\u8bc6\u4e4b\u95f4\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u6559\u5b66\u6469\u64e6\uff0c\u7528\u6237\u80fd\u591f\u5728\u5bc6\u7801\u521b\u5efa\u4efb\u52a1\u4e2d\u66f4\u597d\u5730\u9075\u5faa\u89c4\u5219\uff0c\u5c55\u73b0\u51fa\u4e00\u5b9a\u7684\u77e5\u8bc6\u5bf9\u884c\u4e3a\u7684\u5f71\u54cd\u3002"}}
{"id": "2601.07052", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.07052", "abs": "https://arxiv.org/abs/2601.07052", "authors": ["Simon Sagmeister", "Marcel Weinmann", "Phillip Pitschi", "Markus Lienkamp"], "title": "RSLCPP - Deterministic Simulations Using ROS 2", "comment": "Submitted to 'IEEE Robotics and Automation Practice' for possible publication", "summary": "Simulation is crucial in real-world robotics, offering safe, scalable, and efficient environments for developing applications, ranging from humanoid robots to autonomous vehicles and drones. While the Robot Operating System (ROS) has been widely adopted as the backbone of these robotic applications in both academia and industry, its asynchronous, multiprocess design complicates reproducibility, especially across varying hardware platforms. Deterministic callback execution cannot be guaranteed when computation times and communication delays vary. This lack of reproducibility complicates scientific benchmarking and continuous integration, where consistent results are essential. To address this, we present a methodology to create deterministic simulations using ROS 2 nodes. Our ROS Simulation Library for C++ (RSLCPP) implements this approach, enabling existing nodes to be combined into a simulation routine that yields reproducible results without requiring any code changes. We demonstrate that our approach yields identical results across various CPUs and architectures when testing both a synthetic benchmark and a real-world robotics system. RSLCPP is open-sourced at https://github.com/TUMFTM/rslcpp.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eROS 2\u7684\u786e\u5b9a\u6027\u6a21\u62df\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86ROS\u5728\u4e0d\u540c\u786c\u4ef6\u5e73\u53f0\u4e0a\u91cd\u73b0\u6027\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3ROS\u5728\u4e0d\u540c\u786c\u4ef6\u5e73\u53f0\u4e0a\u7531\u4e8e\u5f02\u6b65\u591a\u8fdb\u7a0b\u8bbe\u8ba1\u9020\u6210\u7684\u91cd\u590d\u6027\u95ee\u9898\uff0c\u4ee5\u63d0\u9ad8\u79d1\u5b66\u57fa\u51c6\u6d4b\u8bd5\u548c\u6301\u7eed\u96c6\u6210\u7684\u6709\u6548\u6027\u3002", "method": "\u901a\u8fc7ROS 2\u8282\u70b9\u521b\u5efa\u786e\u5b9a\u6027\u6a21\u62df\u7684\u65b9\u6cd5\uff0c\u5229\u7528C++\u7684ROS Simulation Library (RSLCPP)\u6765\u5b9e\u73b0\uff0c\u4e0d\u9700\u8981\u5bf9\u73b0\u6709\u8282\u70b9\u8fdb\u884c\u4ee3\u7801\u4fee\u6539\u3002", "result": "\u5728\u6d4b\u8bd5\u5408\u6210\u57fa\u51c6\u548c\u5b9e\u9645\u673a\u5668\u4eba\u7cfb\u7edf\u65f6\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u4e0d\u540cCPU\u548c\u67b6\u6784\u4e0a\u80fd\u591f\u63d0\u4f9b\u76f8\u540c\u7684\u7ed3\u679c\u3002", "conclusion": "RSLCPP\u901a\u8fc7\u5f00\u653e\u6e90\u7801\u5f62\u5f0f\u63d0\u4f9b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u6a21\u62df\u91cd\u73b0\u6027\u3002"}}
{"id": "2601.06774", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.06774", "abs": "https://arxiv.org/abs/2601.06774", "authors": ["Xiangzhe Yuan", "Jiajun Wang", "Huanchen Wang", "Qian Wan", "Siying Hu"], "title": "ImmuniFraug: A Metacognitive Intervention Anti-Fraud Approach to Enhance Undergraduate Students' Cyber Fraud Awareness", "comment": null, "summary": "Cyber fraud now constitutes over half of criminal cases in China, with undergraduate students experiencing a disproportionate rise in victimization. Traditional anti-fraud training remains predominantly passive, yielding limited engagement and retention. This paper introduces ImmuniFraug, a Large Language Model (LLM)-based metacognitive intervention that delivers immersive, multimodal fraud simulations integrating text, voice, and visual avatars across ten prevalent fraud types. Each scenario is designed to replicate real-world persuasion tactics and psychological pressure, while post-interaction debriefs provide grounded feedback in protection motivation theory and reflective prompts to reinforce learning. In a controlled study with 846 Chinese undergraduates, ImmuniFraug was compared to official text-based materials. Linear Mixed-Effects Modeling (LMEM) reveals that the interactive intervention significantly improved fraud awareness (p = 0.026), successfully providing incremental learning value even when controlling for participants' extensive prior exposure to anti-fraud education, alongside high narrative immersion (M = 56.95/77). Thematic analysis of interviews revealed key effectiveness factors: perceived realism, adaptive deception, enforced time pressure, emotional manipulation awareness, and enhanced self-efficacy. Findings demonstrate that by shifting the focus from passive knowledge acquisition to active metacognitive engagement, LLM-based simulations offer a scalable and ecologically valid new paradigm for anti-fraud training and fostering fraud resilience.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u7684ImmuniFraug\u662f\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6c89\u6d78\u5f0f\u53cd\u6b3a\u8bc8\u5e72\u9884\uff0c\u8f83\u4f20\u7edf\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5b66\u751f\u7684\u6b3a\u8bc8\u610f\u8bc6\u548c\u53cd\u6b3a\u8bc8\u80fd\u529b\u3002", "motivation": "\u9488\u5bf9\u4e2d\u56fd\u5927\u5b66\u751f\u7f51\u7edc\u6b3a\u8bc8\u6848\u4ef6\u4e0a\u5347\u4ee5\u53ca\u4f20\u7edf\u53cd\u6b3a\u8bc8\u57f9\u8bad\u6548\u679c\u6709\u9650\uff0c\u4e9f\u9700\u4e00\u79cd\u65b0\u7684\u3001\u4e92\u52a8\u6027\u66f4\u5f3a\u7684\u6559\u80b2\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5c06ImmuniFraug\u4e0e\u4f20\u7edf\u6587\u672c\u6750\u6599\u8fdb\u884c\u5bf9\u6bd4\uff0c\u91c7\u7528\u7ebf\u6027\u6df7\u5408\u6548\u5e94\u6a21\u578b\u5206\u6790\u4e86846\u540d\u5927\u5b66\u751f\u7684\u5e72\u9884\u6548\u679c\uff0c\u5e76\u5229\u7528\u4e3b\u9898\u5206\u6790\u8bc4\u4f30\u8bbf\u8c08\u7ed3\u679c\u3002", "result": "ImmuniFraug\u6709\u6548\u63d0\u5347\u4e86\u5b66\u751f\u5bf9\u7f51\u7edc\u6b3a\u8bc8\u7684\u8ba4\u8bc6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6c89\u6d78\u5f0f\u4f53\u9a8c\u89e3\u51b3\u4f20\u7edf\u53cd\u6b3a\u8bc8\u57f9\u8bad\u4e2d\u7684\u4e0d\u8db3\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u5e72\u9884\u63aa\u65bd\u663e\u8457\u63d0\u9ad8\u4e86\u7f51\u7edc\u6b3a\u8bc8\u610f\u8bc6\uff0c\u57f9\u517b\u4e86\u5927\u5b66\u751f\u7684\u53cd\u6b3a\u8bc8\u80fd\u529b\uff0c\u663e\u793a\u4e86\u65b0\u7684\u57f9\u8bad\u8303\u5f0f\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2601.07060", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.07060", "abs": "https://arxiv.org/abs/2601.07060", "authors": ["Yuanzhe Liu", "Jingyuan Zhu", "Yuchen Mo", "Gen Li", "Xu Cao", "Jin Jin", "Yifan Shen", "Zhengyuan Li", "Tianjiao Yu", "Wenzhen Yuan", "Fangqiang Ding", "Ismini Lourentzou"], "title": "PALM: Progress-Aware Policy Learning via Affordance Reasoning for Long-Horizon Robotic Manipulation", "comment": null, "summary": "Recent advancements in vision-language-action (VLA) models have shown promise in robotic manipulation, yet they continue to struggle with long-horizon, multi-step tasks. Existing methods lack internal reasoning mechanisms that can identify task-relevant interaction cues or track progress within a subtask, leading to critical execution errors such as repeated actions, missed steps, and premature termination. To address these challenges, we introduce PALM, a VLA framework that structures policy learning around interaction-centric affordance reasoning and subtask progress cues. PALM distills complementary affordance representations that capture object relevance, contact geometry, spatial placements, and motion dynamics, and serve as task-relevant anchors for visuomotor control. To further stabilize long-horizon execution, PALM predicts continuous within-subtask progress, enabling seamless subtask transitions. Across extensive simulation and real-world experiments, PALM consistently outperforms baselines, achieving a 91.8% success rate on LIBERO-LONG, a 12.5% improvement in average length on CALVIN ABC->D, and a 2x improvement over real-world baselines across three long-horizon generalization settings.", "AI": {"tldr": "PALM\u662f\u4e00\u4e2a\u65b0\u7684VLA\u6846\u67b6\uff0c\u901a\u8fc7\u6709\u5229\u6027\u63a8\u7406\u548c\u8fdb\u5c55\u7ebf\u7d22\u63d0\u5347\u673a\u5668\u4eba\u957f\u65f6\u95f4\u591a\u6b65\u9aa4\u64cd\u4f5c\u7684\u6210\u529f\u7387\uff0c\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u73b0\u6709VLA\u6a21\u578b\u5728\u957f\u65f6\u95f4\u3001\u591a\u6b65\u9aa4\u4efb\u52a1\u4e2d\u7684\u6267\u884c\u9519\u8bef\uff0cPALM\u6846\u67b6\u88ab\u63d0\u51fa\uff0c\u4ee5\u589e\u5f3a\u673a\u5668\u4eba\u5bf9\u4ea4\u4e92\u76f8\u5173\u4fe1\u606f\u7684\u8bc6\u522b\u548c\u5229\u7528\u3002", "method": "PALM\u6846\u67b6\u901a\u8fc7\u4e0e\u4ea4\u4e92\u76f8\u5173\u7684\u6709\u5229\u6027\u63a8\u7406\u548c\u5b50\u4efb\u52a1\u8fdb\u5c55\u7ebf\u7d22\u8fdb\u884c\u653f\u7b56\u5b66\u4e60\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u8fdb\u884c\u6301\u7eed\u7684\u8fdb\u5c55\u9884\u6d4b\u3002", "result": "PALM\u6846\u67b6\u901a\u8fc7\u4e0e\u4ea4\u4e92\u76f8\u5173\u7684\u6709\u5229\u6027\u63a8\u7406\u548c\u5b50\u4efb\u52a1\u8fdb\u5c55\u7ebf\u7d22\u7684\u653f\u7b56\u5b66\u4e60\uff0c\u6709\u6548\u63a8\u52a8\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u9886\u57df\u7684\u53d1\u5c55\u3002", "conclusion": "PALM\u5728\u957f\u65f6\u95f4\u3001 multi-step\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u80fd\u591f\u663e\u8457\u51cf\u5c11\u6267\u884c\u9519\u8bef\u5e76\u63d0\u9ad8\u6210\u529f\u7387\u3002"}}
{"id": "2601.06781", "categories": ["cs.HC", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.06781", "abs": "https://arxiv.org/abs/2601.06781", "authors": ["Huatao Xu", "Zihe Liu", "Zilin Zeng", "Baichuan Li", "Mo Li"], "title": "AutoTour: Automatic Photo Tour Guide with Smartphones and LLMs", "comment": "21", "summary": "We present AutoTour, a system that enhances user exploration by automatically generating fine-grained landmark annotations and descriptive narratives for photos captured by users. The key idea of AutoTour is to fuse visual features extracted from photos with nearby geospatial features queried from open matching databases. Unlike existing tour applications that rely on pre-defined content or proprietary datasets, AutoTour leverages open and extensible data sources to provide scalable and context-aware photo-based guidance. To achieve this, we design a training-free pipeline that first extracts and filters relevant geospatial features around the user's GPS location. It then detects major landmarks in user photos through VLM-based feature detection and projects them into the horizontal spatial plane. A geometric matching algorithm aligns photo features with corresponding geospatial entities based on their estimated distance and direction. The matched features are subsequently grounded and annotated directly on the original photo, accompanied by large language model-generated textual and audio descriptions to provide an informative, tour-like experience. We demonstrate that AutoTour can deliver rich, interpretable annotations for both iconic and lesser-known landmarks, enabling a new form of interactive, context-aware exploration that bridges visual perception and geospatial understanding.", "AI": {"tldr": "AutoTour\u7cfb\u7edf\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u7ec6\u81f4\u7684\u5730\u6807\u6ce8\u91ca\u548c\u63cf\u8ff0\u6027\u53d9\u8ff0\uff0c\u63d0\u5347\u7528\u6237\u63a2\u7d22\u4f53\u9a8c\uff0c\u7ed3\u5408\u89c6\u89c9\u7279\u5f81\u548c\u5730\u7406\u7a7a\u95f4\u6570\u636e\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u7167\u7247\u6307\u5bfc\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u7cfb\u7edf\uff0c\u589e\u5f3a\u7528\u6237\u63a2\u7d22\u4f53\u9a8c\uff0c\u63d0\u4f9b\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u7167\u7247\u5bfc\u89c8\uff0c\u800c\u975e\u4f9d\u8d56\u9884\u5b9a\u4e49\u5185\u5bb9\u6216\u4e13\u6709\u6570\u636e\u96c6\u3002", "method": "\u8bbe\u8ba1\u4e00\u4e2a\u65e0\u8bad\u7ec3\u7ba1\u9053\uff0c\u63d0\u53d6\u5e76\u8fc7\u6ee4\u7528\u6237GPS\u4f4d\u7f6e\u5468\u56f4\u7684\u5730\u7406\u7279\u5f81\uff0c\u901a\u8fc7\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7279\u5f81\u68c0\u6d4b\u8bc6\u522b\u7167\u7247\u4e2d\u7684\u4e3b\u8981\u5730\u6807\uff0c\u5e76\u4f7f\u7528\u51e0\u4f55\u5339\u914d\u7b97\u6cd5\u5c06\u7167\u7247\u7279\u5f81\u4e0e\u5730\u7406\u5b9e\u4f53\u5bf9\u9f50\uff0c\u6700\u7ec8\u751f\u6210\u6ce8\u91ca\u548c\u63cf\u8ff0\u3002", "result": "AutoTour\u80fd\u591f\u4e3a\u6807\u5fd7\u6027\u548c\u4e0d\u592a\u77e5\u540d\u7684\u5730\u6807\u63d0\u4f9b\u4e30\u5bcc\u3001\u53ef\u89e3\u91ca\u7684\u6ce8\u91ca\uff0c\u652f\u6301\u4e92\u52a8\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u63a2\u7d22\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u8fde\u63a5\u4e86\u89c6\u89c9\u611f\u77e5\u4e0e\u5730\u7406\u7a7a\u95f4\u7406\u89e3\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u63a2\u7d22\u4f53\u9a8c\u3002"}}
{"id": "2601.07186", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.07186", "abs": "https://arxiv.org/abs/2601.07186", "authors": ["Zainab Altaweel", "Mohaiminul Al Nahian", "Jake Juettner", "Adnan Siraj Rakin", "Shiqi Zhang"], "title": "PROTEA: Securing Robot Task Planning and Execution", "comment": null, "summary": "Robots need task planning methods to generate action sequences for complex tasks. Recent work on adversarial attacks has revealed significant vulnerabilities in existing robot task planners, especially those built on foundation models. In this paper, we aim to address these security challenges by introducing PROTEA, an LLM-as-a-Judge defense mechanism, to evaluate the security of task plans. PROTEA is developed to address the dimensionality and history challenges in plan safety assessment. We used different LLMs to implement multiple versions of PROTEA for comparison purposes. For systemic evaluations, we created a dataset containing both benign and malicious task plans, where the harmful behaviors were injected at varying levels of stealthiness. Our results provide actionable insights for robotic system practitioners seeking to enhance robustness and security of their task planning systems. Details, dataset and demos are provided: https://protea-secure.github.io/PROTEA/", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faPROTEA\uff0c\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u9632\u5fa1\u673a\u5236\uff0c\u4ee5\u63d0\u5347\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u7684\u5b89\u5168\u6027\uff0c\u8bc4\u4f30\u6076\u6027\u884c\u4e3a\u5e76\u63d0\u4f9b\u7cfb\u7edf\u6027\u8bc4\u4f30\u7ed3\u679c\u3002", "motivation": "\u5e94\u5bf9\u73b0\u6709\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u4e2d\u7684\u5b89\u5168\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u57fa\u7840\u6a21\u578b\u4e0a\u7684\u8106\u5f31\u6027\u3002", "method": "\u5f15\u5165PROTEA\uff0c\u5229\u7528LLM\u4f5c\u4e3a\u5224\u522b\u673a\u5236\uff0c\u8bc4\u4f30\u4efb\u52a1\u89c4\u5212\u7684\u5b89\u5168\u6027\uff0c\u5e76\u521b\u5efa\u4e86\u5305\u542b\u826f\u6027\u548c\u6076\u6027\u4efb\u52a1\u8ba1\u5212\u7684\u6570\u636e\u96c6\u8fdb\u884c\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u8bc4\u4f30\u4e0d\u540c\u7248\u672c\u7684PROTEA\uff0c\u63d0\u4f9b\u4e86\u589e\u5f3a\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u7cfb\u7edf\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u7684\u53ef\u64cd\u4f5c\u6027\u89c1\u89e3\u3002", "conclusion": "PROTEA\u4e3a\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b89\u5168\u6027\u8bc4\u4f30\u673a\u5236\uff0c\u80fd\u591f\u63d0\u9ad8\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2601.06823", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.06823", "abs": "https://arxiv.org/abs/2601.06823", "authors": ["Rui Liu", "Liuqingqing Yang", "Runsheng Zhang", "Shixiao Wang"], "title": "Generative Modeling of Human-Computer Interfaces with Diffusion Processes and Conditional Control", "comment": null, "summary": "This study investigates human-computer interface generation based on diffusion models to overcome the limitations of traditional template-based design and fixed rule-driven methods. It first analyzes the key challenges of interface generation, including the diversity of interface elements, the complexity of layout logic, and the personalization of user needs. A generative framework centered on the diffusion-reverse diffusion process is then proposed, with conditional control introduced in the reverse diffusion stage to integrate user intent, contextual states, and task constraints, enabling unified modeling of visual presentation and interaction logic. In addition, regularization constraints and optimization objectives are combined to ensure the rationality and stability of the generated interfaces. Experiments are conducted on a public interface dataset with systematic evaluations, including comparative experiments, hyperparameter sensitivity tests, environmental sensitivity tests, and data sensitivity tests. Results show that the proposed method outperforms representative models in mean squared error, structural similarity, peak signal-to-noise ratio, and mean absolute error, while maintaining strong robustness under different parameter settings and environmental conditions. Overall, the diffusion model framework effectively improves the diversity, rationality, and intelligence of interface generation, providing a feasible solution for automated interface generation in complex interaction scenarios.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4eba\u673a\u754c\u9762\u751f\u6210\u6846\u67b6\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u6a21\u677f\u8bbe\u8ba1\u548c\u56fa\u5b9a\u89c4\u5219\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u7814\u7a76\u4eba\u673a\u754c\u9762\u751f\u6210\u7684\u5173\u952e\u6311\u6218\uff0c\u5305\u62ec\u754c\u9762\u5143\u7d20\u591a\u6837\u6027\u3001\u5e03\u5c40\u903b\u8f91\u590d\u6742\u6027\u4ee5\u53ca\u7528\u6237\u9700\u6c42\u4e2a\u6027\u5316\u3002", "method": "\u63d0\u51fa\u4ee5\u6269\u6563-\u53cd\u6269\u6563\u8fc7\u7a0b\u4e3a\u4e2d\u5fc3\u7684\u751f\u6210\u6846\u67b6\uff0c\u5728\u53cd\u6269\u6563\u9636\u6bb5\u5f15\u5165\u6761\u4ef6\u63a7\u5236\u4ee5\u6574\u5408\u7528\u6237\u610f\u56fe\u3001\u4e0a\u4e0b\u6587\u72b6\u6001\u548c\u4efb\u52a1\u7ea6\u675f\u3002\u7ed3\u5408\u6b63\u5219\u5316\u7ea6\u675f\u548c\u4f18\u5316\u76ee\u6807\uff0c\u786e\u4fdd\u751f\u6210\u754c\u9762\u7684\u5408\u7406\u6027\u548c\u7a33\u5b9a\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5747\u65b9\u8bef\u5dee\u3001\u7ed3\u6784\u76f8\u4f3c\u5ea6\u3001\u5cf0\u503c\u4fe1\u566a\u6bd4\u548c\u5747\u503c\u7edd\u5bf9\u8bef\u5dee\u7b49\u6307\u6807\u4e0a\u4f18\u4e8e\u4ee3\u8868\u6027\u6a21\u578b\uff0c\u5e76\u5728\u4e0d\u540c\u53c2\u6570\u8bbe\u7f6e\u548c\u73af\u5883\u6761\u4ef6\u4e0b\u4fdd\u6301\u5f3a\u5065\u6027\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u6846\u67b6\u6709\u6548\u63d0\u9ad8\u4e86\u4eba\u673a\u754c\u9762\u751f\u6210\u7684\u591a\u6837\u6027\u3001\u5408\u7406\u6027\u548c\u667a\u80fd\u6027\uff0c\u4e3a\u590d\u6742\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u81ea\u52a8\u5316\u754c\u9762\u751f\u6210\u63d0\u4f9b\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.07242", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.07242", "abs": "https://arxiv.org/abs/2601.07242", "authors": ["Taekbeom Lee", "Dabin Kim", "Youngseok Jang", "H. Jin Kim"], "title": "HERE: Hierarchical Active Exploration of Radiance Field with Epistemic Uncertainty Minimization", "comment": "Accepted to IEEE RA-L. The first two authors contributed equally", "summary": "We present HERE, an active 3D scene reconstruction framework based on neural radiance fields, enabling high-fidelity implicit mapping. Our approach centers around an active learning strategy for camera trajectory generation, driven by accurate identification of unseen regions, which supports efficient data acquisition and precise scene reconstruction. The key to our approach is epistemic uncertainty quantification based on evidential deep learning, which directly captures data insufficiency and exhibits a strong correlation with reconstruction errors. This allows our framework to more reliably identify unexplored or poorly reconstructed regions compared to existing methods, leading to more informed and targeted exploration. Additionally, we design a hierarchical exploration strategy that leverages learned epistemic uncertainty, where local planning extracts target viewpoints from high-uncertainty voxels based on visibility for trajectory generation, and global planning uses uncertainty to guide large-scale coverage for efficient and comprehensive reconstruction. The effectiveness of the proposed method in active 3D reconstruction is demonstrated by achieving higher reconstruction completeness compared to previous approaches on photorealistic simulated scenes across varying scales, while a hardware demonstration further validates its real-world applicability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u8f90\u5c04\u573a\u7684\u4e3b\u52a83D\u573a\u666f\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u548c\u5c42\u6b21\u5316\u63a2\u7d22\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u800c\u7cbe\u786e\u7684\u573a\u666f\u91cd\u5efa\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u9ad8\u4fdd\u771f\u9690\u5f0f\u6620\u5c04\u76843D\u573a\u666f\u91cd\u5efa\u6846\u67b6\uff0c\u4f18\u5316\u6570\u636e\u91c7\u96c6\u6548\u7387\u5e76\u63d0\u9ad8\u91cd\u5efa\u7cbe\u5ea6\u3002", "method": "\u57fa\u4e8e\u795e\u7ecf\u8f90\u5c04\u573a\u7684\u4e3b\u52a83D\u573a\u666f\u91cd\u5efa\u6846\u67b6\uff0c\u5229\u7528\u5bf9\u672a\u89c1\u533a\u57df\u7684\u51c6\u786e\u8bc6\u522b\u751f\u6210\u76f8\u673a\u8f68\u8ff9\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u63a2\u7d22\u7b56\u7565\u8fdb\u884c\u6570\u636e\u91c7\u96c6\u548c\u573a\u666f\u91cd\u5efa\u3002", "result": "\u5728\u4e0d\u540c\u5c3a\u5ea6\u7684\u5149\u7167\u771f\u5b9e\u6a21\u62df\u573a\u666f\u4e2d\uff0c\u6bd4\u8f83\u4ee5\u5f80\u65b9\u6cd5\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u91cd\u5efa\u5b8c\u6574\u6027\u4e0a\u8868\u73b0\u66f4\u4f73\uff0c\u5e76\u901a\u8fc7\u786c\u4ef6\u6f14\u793a\u9a8c\u8bc1\u5176\u73b0\u5b9e\u5e94\u7528\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u6d3b\u8dc3\u76843D\u91cd\u5efa\u65b9\u9762\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u91cd\u5efa\u5b8c\u6574\u6027\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002"}}
{"id": "2601.06877", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06877", "abs": "https://arxiv.org/abs/2601.06877", "authors": ["Donghuo Zeng", "Roberto Legaspi", "Kazushi Ikeda"], "title": "Personality-Aware Reinforcement Learning for Persuasive Dialogue with LLM-Driven Simulation", "comment": "15 pages, 7 figures, 3 tables", "summary": "Effective persuasive dialogue agents adapt their strategies to individual users, accounting for the evolution of their psychological states and intentions throughout conversations. We present a personality-aware reinforcement learning approach comprising three main modules: (1) a Strategy-Oriented Interaction Framework, which serves as an agenda-based strategy controller that selects strategy-level actions and generate responses via Maximal Marginal Relevance (MMR) retrieval to ensure contextual relevance, diversity, and scalable data generation; (2) Personality-Aware User Representation Learning, which produces an 81-dimensional mixed-type embedding predicted at each turn from recent exchanges and appended to the reinforcement learning state; and (3) a Dueling Double DQN (D3QN) model and Reward Prediction, in which the policy is conditioned on dialogue history and turn-level personality estimates and trained using a composite reward incorporating agreement intent, donation amount, and changeof-mind penalties. We use an agenda-based LLM simulation pipeline to generate diverse interactions, from which personality estimation is inferred from the generated utterances. Experiments on the PersuasionForGood (P4G) dataset augmented with simulated dialogues reveal three main findings: (i) turn-level personality conditioning improves policy adaptability and cumulative persuasion rewards; (ii) LLM-driven simulation enhances generalization to unseen user behaviors; and (iii) incorporating a change-of-mind penalty reduces post-agreement retractions while slightly improving donation outcomes. These results demonstrate that structured interaction, dynamic personality estimation, and behaviorally informed rewards together yield more effective persuasive policies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e2a\u6027\u610f\u8bc6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b56\u7565\u6846\u67b6\u3001\u4e2a\u6027\u8868\u793a\u5b66\u4e60\u548cD3QN\u6a21\u578b\u6539\u5584\u8bf4\u670d\u6548\u679c\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5177\u6709\u826f\u597d\u7684\u7b56\u7565\u9002\u5e94\u6027\u548c\u5956\u52b1\u7d2f\u8ba1\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u8bf4\u670d\u5bf9\u8bdd\u4ee3\u7406\u7684\u6709\u6548\u6027\uff0c\u9002\u5e94\u7528\u6237\u7684\u5fc3\u7406\u72b6\u6001\u548c\u610f\u56fe\u3002", "method": "\u4e2a\u6027\u610f\u8bc6\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5305\u62ec\u7b56\u7565\u5bfc\u5411\u4e92\u52a8\u6846\u67b6\u3001\u4e2a\u6027\u610f\u8bc6\u7528\u6237\u8868\u793a\u5b66\u4e60\u548c\u5bf9\u6297\u53cc\u91cdDQN\u6a21\u578b\uff0c\u7ed3\u5408\u5bf9\u8bdd\u5386\u53f2\u548c\u4e2a\u6027\u4f30\u8ba1\u8fdb\u884c\u7b56\u7565\u8bad\u7ec3\u3002", "result": "\u5728PersuasionForGood (P4G) \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8f6c\u7ea7\u522b\u4e2a\u6027\u6761\u4ef6'am\u00e9liore\u653f\u7b56\u7684\u9002\u5e94\u6027\u548c\u7d2f\u79ef\u8bf4\u670d\u5956\u52b1\uff0cLLM\u9a71\u52a8\u7684\u6a21\u62df\u589e\u5f3a\u5bf9\u672a\u89c1\u7528\u6237\u884c\u4e3a\u7684\u6cdb\u5316\uff0c\u6539\u53d8\u4e3b\u610f\u7684\u60e9\u7f5a\u51cf\u5c11\u4e86\u534f\u8bae\u540e\u7684\u64a4\u56de\uff0c\u540c\u65f6\u7565\u5fae\u6539\u5584\u4e86\u6350\u6b3e\u7ed3\u679c\u3002", "conclusion": "\u7ed3\u6784\u5316\u4e92\u52a8\u3001\u52a8\u6001\u4e2a\u6027\u8bc4\u4f30\u548c\u884c\u4e3a\u9a71\u52a8\u5956\u52b1\u5171\u540c\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u8bf4\u670d\u7b56\u7565\u3002"}}
{"id": "2601.07284", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.07284", "abs": "https://arxiv.org/abs/2601.07284", "authors": ["Haoyu Zhang", "Shibo Jin", "Lvsong Li", "Jun Li", "Liang Lin", "Xiaodong He", "Zecui Zeng"], "title": "AdaMorph: Unified Motion Retargeting via Embodiment-Aware Adaptive Transformers", "comment": null, "summary": "Retargeting human motion to heterogeneous robots is a fundamental challenge in robotics, primarily due to the severe kinematic and dynamic discrepancies between varying embodiments. Existing solutions typically resort to training embodiment-specific models, which scales poorly and fails to exploit shared motion semantics. To address this, we present AdaMorph, a unified neural retargeting framework that enables a single model to adapt human motion to diverse robot morphologies. Our approach treats retargeting as a conditional generation task. We map human motion into a morphology-agnostic latent intent space and utilize a dual-purpose prompting mechanism to condition the generation. Instead of simple input concatenation, we leverage Adaptive Layer Normalization (AdaLN) to dynamically modulate the decoder's feature space based on embodiment constraints. Furthermore, we enforce physical plausibility through a curriculum-based training objective that ensures orientation and trajectory consistency via integration. Experimental results on 12 distinct humanoid robots demonstrate that AdaMorph effectively unifies control across heterogeneous topologies, exhibiting strong zero-shot generalization to unseen complex motions while preserving the dynamic essence of the source behaviors.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86AdaMorph\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u795e\u7ecf\u91cd\u5b9a\u5411\u6846\u67b6\uff0c\u53ef\u4ee5\u9002\u5e94\u5404\u79cd\u673a\u5668\u4eba\u5f62\u6001\u7684\u4eba\u7684\u8fd0\u52a8\uff0c\u5b9e\u73b0\u4e86\u8fd0\u52a8\u7684\u6761\u4ef6\u751f\u6210\u3002", "motivation": "\u4eba\u7c7b\u8fd0\u52a8\u7684\u91cd\u5b9a\u5411\u81f3\u5f02\u6784\u673a\u5668\u4eba\u9762\u4e34\u7740\u4e25\u91cd\u7684\u8fd0\u52a8\u5b66\u548c\u52a8\u529b\u5b66\u5dee\u5f02\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u65e0\u6cd5\u6709\u6548\u5229\u7528\u5171\u4eab\u8fd0\u52a8\u8bed\u4e49\u3002", "method": "\u5c06\u91cd\u5b9a\u5411\u89c6\u4e3a\u6761\u4ef6\u751f\u6210\u4efb\u52a1\uff0c\u91c7\u7528Morphology-agnostic\u7684\u6f5c\u5728\u610f\u56fe\u7a7a\u95f4\u548c\u53cc\u91cd\u63d0\u793a\u673a\u5236\uff0c\u8fd0\u7528\u81ea\u9002\u5e94\u56fe\u5c42\u5f52\u4e00\u5316\uff08AdaLN\uff09\u6765\u52a8\u6001\u8c03\u8282\u89e3\u7801\u5668\u7684\u7279\u5f81\u7a7a\u95f4\u3002", "result": "\u9488\u5bf912\u79cd\u4e0d\u540c\u7684\u4eba\u5f62\u673a\u5668\u4eba\u8fdb\u884c\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAdaMorph\u80fd\u6709\u6548\u7edf\u4e00\u4e0d\u540c\u62d3\u6251\u7684\u63a7\u5236\uff0c\u5e76\u5728\u4fdd\u6301\u6e90\u884c\u4e3a\u52a8\u6001\u672c\u8d28\u7684\u540c\u65f6\u5bf9\u672a\u89c1\u8fc7\u7684\u590d\u6742\u52a8\u4f5c\u4f53\u73b0\u51fa\u5f3a\u5927\u7684\u96f6-shot\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "AdaMorph\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5c06\u4eba\u7c7b\u8fd0\u52a8\u7edf\u4e00\u5730\u9002\u5e94\u4e8e\u591a\u79cd\u673a\u5668\u4eba\u5f62\u6001\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5904\u7406\u590d\u6742\u52a8\u4f5c\u65f6\u7684\u5f3a\u5927\u96f6-shot\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2601.06902", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.06902", "abs": "https://arxiv.org/abs/2601.06902", "authors": ["Stinne Zacho", "Chris Hall", "Jakob Kusnick", "Stefan J\u00e4nicke"], "title": "Santa Clara 3D: Digital Reconstruction and Storytelling of a Francoist Concentration Camp", "comment": null, "summary": "This paper explores the potential of digital reconstruction and interactive storytelling to preserve historically suppressed sites. The main objective of an interdisciplinary team of data scientists from the MEMORISE project and associates of the memory association Asociacion Recuerdo y Dignidad was to preserve the memory of the Francoist Santa Clara concentration camp in Soria, Spain, through the use of digital technology. Combining archival research, 3D modelling, 360-degree photography, and web development, a prototype digital platform was created to visualise the transformation of the site across three historical phases: its origin as a convent, its use as a Francoist concentration camp, and its present-day condition. The platform allows users to navigate through spatial and temporal layers. Clickable media markers encourage exploration and interaction. Drawing on principles of participatory design, narrative visualisation, and open-ended user engagement, the project demonstrates how digital tools can support memory work, public engagement, and historical reflection. Our low-cost concept is especially adaptable to other physical sites that have been erased or forgotten.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u6570\u5b57\u91cd\u5efa\u548c\u4e92\u52a8\u53d9\u4e8b\u5728\u4fdd\u5b58\u5386\u53f2\u88ab\u538b\u5236\u5730\u70b9\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4ee5\u897f\u73ed\u7259\u7d22\u91cc\u4e9a\u7684\u5f17\u6717\u54e5\u5723\u514b\u62c9\u62c9\u96c6\u4e2d\u8425\u4e3a\u4f8b\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u7ed3\u5408\u591a\u79cd\u6570\u5b57\u6280\u672f\u8fdb\u884c\u5386\u53f2\u8bb0\u5fc6\u7684\u53ef\u89c6\u5316\u548c\u4e92\u52a8\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u5229\u7528\u6570\u5b57\u6280\u672f\u4fdd\u5b58\u5386\u53f2\u4e0a\u88ab\u538b\u5236\u7684\u5730\u65b9\u7684\u8bb0\u5fc6\uff0c\u7279\u522b\u662f\u5f17\u6717\u54e5\u65f6\u671f\u7684\u96c6\u4e2d\u8425\uff0c\u4ee5\u4fc3\u8fdb\u516c\u4f17\u5bf9\u5386\u53f2\u7684\u53c2\u4e0e\u548c\u53cd\u601d\u3002", "method": "\u901a\u8fc7\u6863\u6848\u7814\u7a76\u30013D\u5efa\u6a21\u3001360\u5ea6\u6444\u5f71\u548c\u7f51\u9875\u5f00\u53d1\uff0c\u521b\u5efa\u4e86\u4e00\u4e2a\u539f\u578b\u6570\u5b57\u5e73\u53f0\uff0c\u5c55\u793a\u8be5\u5730\u70b9\u5728\u4e09\u79cd\u5386\u53f2\u9636\u6bb5\u7684\u53d8\u5316\u3002", "result": "\u8be5\u5e73\u53f0\u5141\u8bb8\u7528\u6237\u5bfc\u822a\u65f6\u7a7a\u5c42\u6b21\uff0c\u901a\u8fc7\u53ef\u70b9\u51fb\u7684\u5a92\u4f53\u6807\u8bb0\u9f13\u52b1\u63a2\u7d22\u548c\u4e92\u52a8\uff0c\u5c55\u793a\u4e86\u6570\u5b57\u5de5\u5177\u5728\u652f\u6301\u8bb0\u5fc6\u5de5\u4f5c\u548c\u516c\u4f17\u53c2\u4e0e\u65b9\u9762\u7684\u5e94\u7528\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u6570\u5b57\u5de5\u5177\u5728\u5386\u53f2\u8bb0\u5fc6\u4fdd\u62a4\u3001\u516c\u4f17\u53c2\u4e0e\u548c\u5386\u53f2\u53cd\u601d\u4e2d\u7684\u91cd\u8981\u4f5c\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u5176\u4ed6\u88ab\u9057\u5fd8\u6216\u62b9\u53bb\u7684\u5730\u70b9\u7684\u4f4e\u6210\u672c\u6982\u5ff5\u3002"}}
{"id": "2601.07304", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07304", "abs": "https://arxiv.org/abs/2601.07304", "authors": ["Yun Chen", "Bowei Huang", "Fan Guo", "Kang Song"], "title": "Heterogeneous Multi-Expert Reinforcement Learning for Long-Horizon Multi-Goal Tasks in Autonomous Forklifts", "comment": "9 pages", "summary": "Autonomous mobile manipulation in unstructured warehouses requires a balance between efficient large-scale navigation and high-precision object interaction. Traditional end-to-end learning approaches often struggle to handle the conflicting demands of these distinct phases. Navigation relies on robust decision-making over large spaces, while manipulation needs high sensitivity to fine local details. Forcing a single network to learn these different objectives simultaneously often causes optimization interference, where improving one task degrades the other. To address these limitations, we propose a Heterogeneous Multi-Expert Reinforcement Learning (HMER) framework tailored for autonomous forklifts. HMER decomposes long-horizon tasks into specialized sub-policies controlled by a Semantic Task Planner. This structure separates macro-level navigation from micro-level manipulation, allowing each expert to focus on its specific action space without interference. The planner coordinates the sequential execution of these experts, bridging the gap between task planning and continuous control. Furthermore, to solve the problem of sparse exploration, we introduce a Hybrid Imitation-Reinforcement Training Strategy. This method uses expert demonstrations to initialize the policy and Reinforcement Learning for fine-tuning. Experiments in Gazebo simulations show that HMER significantly outperforms sequential and end-to-end baselines. Our method achieves a task success rate of 94.2\\% (compared to 62.5\\% for baselines), reduces operation time by 21.4\\%, and maintains placement error within 1.5 cm, validating its efficacy for precise material handling.", "AI": {"tldr": "HMER\u6846\u67b6\u901a\u8fc7\u5206\u89e3\u957f\u671f\u4efb\u52a1\u5e76\u5f15\u5165\u6df7\u5408\u6a21\u4eff-\u5f3a\u5316\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u81ea\u4e3b\u53c9\u8f66\u5728\u975e\u7ed3\u6784\u5316\u4ed3\u5e93\u4e2d\u7684\u5bfc\u822a\u4e0e\u7269\u4f53\u4ea4\u4e92\u6027\u80fd\u3002", "motivation": "\u5728\u975e\u7ed3\u6784\u5316\u4ed3\u5e93\u4e2d\uff0c\u81ea\u4e3b\u79fb\u52a8\u64cd\u4f5c\u9700\u8981\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u5bfc\u822a\u4e0e\u9ad8\u7cbe\u5ea6\u7684\u7269\u4f53\u4ea4\u4e92\u4e4b\u95f4\u7684\u5e73\u8861\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f02\u6784\u591a\u4e13\u5bb6\u5f3a\u5316\u5b66\u4e60\uff08HMER\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u957f\u671f\u4efb\u52a1\u5206\u89e3\u4e3a\u7531\u8bed\u4e49\u4efb\u52a1\u89c4\u5212\u5668\u63a7\u5236\u7684\u4e13\u95e8\u5b50\u7b56\u7565\u3002", "result": "HMER\u5728Gazebo\u6a21\u62df\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u987a\u5e8f\u548c\u7aef\u5230\u7aef\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4efb\u52a1\u6210\u529f\u738794.2%\uff08\u57fa\u7ebf\u4e3a62.5%\uff09\uff0c\u64cd\u4f5c\u65f6\u95f4\u51cf\u5c1121.4%\uff0c\u653e\u7f6e\u8bef\u5dee\u4fdd\u6301\u57281.5\u5398\u7c73\u4ee5\u5185\u3002", "conclusion": "HMER\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u4e3b\u53c9\u8f66\u7684\u7cbe\u786e\u7269\u6599\u5904\u7406\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5b83\u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.07085", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.07085", "abs": "https://arxiv.org/abs/2601.07085", "authors": ["Andrew D. Maynard"], "title": "The AI Cognitive Trojan Horse: How Large Language Models May Bypass Human Epistemic Vigilance", "comment": "15 pages, 18 references", "summary": "Large language model (LLM)-based conversational AI systems present a challenge to human cognition that current frameworks for understanding misinformation and persuasion do not adequately address. This paper proposes that a significant epistemic risk from conversational AI may lie not in inaccuracy or intentional deception, but in something more fundamental: these systems may be configured, through optimization processes that make them useful, to present characteristics that bypass the cognitive mechanisms humans evolved to evaluate incoming information. The Cognitive Trojan Horse hypothesis draws on Sperber and colleagues' theory of epistemic vigilance -- the parallel cognitive process monitoring communicated information for reasons to doubt -- and proposes that LLM-based systems present 'honest non-signals': genuine characteristics (fluency, helpfulness, apparent disinterest) that fail to carry the information equivalent human characteristics would carry, because in humans these are costly to produce while in LLMs they are computationally trivial. Four mechanisms of potential bypass are identified: processing fluency decoupled from understanding, trust-competence presentation without corresponding stakes, cognitive offloading that delegates evaluation itself to the AI, and optimization dynamics that systematically produce sycophancy. The framework generates testable predictions, including a counterintuitive speculation that cognitively sophisticated users may be more vulnerable to AI-mediated epistemic influence. This reframes AI safety as partly a problem of calibration -- aligning human evaluative responses with the actual epistemic status of AI-generated content -- rather than solely a problem of preventing deception.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684AI\u7cfb\u7edf\u5982\u4f55\u901a\u8fc7'\u8bda\u5b9e\u975e\u4fe1\u53f7'\u6765\u7ed5\u8fc7\u4eba\u7c7b\u7684\u8ba4\u77e5\u8bc4\u4f30\uff0c\u5f3a\u8c03\u9700\u8981\u91cd\u65b0\u5ba1\u89c6AI\u5b89\u5168\u6027\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5173\u4e8e\u8bef\u4fe1\u606f\u548c\u8bf4\u670d\u7684\u7406\u89e3\u6846\u67b6\u672a\u80fd\u5145\u5206\u5e94\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e26\u6765\u7684\u8ba4\u77e5\u6311\u6218\u3002", "method": "\u63d0\u51fa\u8ba4\u77e5\u7279\u6d1b\u4f0a\u6728\u9a6c\u5047\u8bbe\uff0c\u63a2\u8ba8\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u8bdd\u5f0fAI\u5bf9\u4eba\u7c7b\u8ba4\u77e5\u7684\u5f71\u54cd", "result": "\u8bc6\u522b\u4e86\u56db\u79cd\u53ef\u80fd\u7684\u8ba4\u77e5\u7ed5\u8fc7\u673a\u5236\uff0c\u5e76\u751f\u6210\u4e86\u53ef\u6d4b\u8bd5\u7684\u9884\u6d4b", "conclusion": "AI\u5b89\u5168\u95ee\u9898\u4e0d\u4ec5\u662f\u9632\u6b62\u6b3a\u9a97\uff0c\u8fd8\u6d89\u53ca\u4eba\u7c7b\u8bc4\u4f30\u54cd\u5e94\u4e0eAI\u751f\u6210\u5185\u5bb9\u7684\u5b9e\u9645\u8ba4\u77e5\u72b6\u6001\u7684\u6821\u51c6\u3002"}}
{"id": "2601.07362", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.07362", "abs": "https://arxiv.org/abs/2601.07362", "authors": ["Julia Richter", "Turcan Tuna", "Manthan Patel", "Takahiro Miki", "Devon Higgins", "James Fox", "Cesar Cadena", "Andres Diaz", "Marco Hutter"], "title": "Large-Scale Autonomous Gas Monitoring for Volcanic Environments: A Legged Robot on Mount Etna", "comment": "12 pages, 7 figures, submitted to IEEE Robotics & Automation Magazine (RAM)", "summary": "Volcanic gas emissions are key precursors of eruptive activity. Yet, obtaining accurate near-surface measurements remains hazardous and logistically challenging, motivating the need for autonomous solutions. Limited mobility in rough volcanic terrain has prevented wheeled systems from performing reliable in situ gas measurements, reducing their usefulness as sensing platforms. We present a legged robotic system for autonomous volcanic gas analysis, utilizing the quadruped ANYmal, equipped with a quadrupole mass spectrometer system. Our modular autonomy stack integrates a mission planning interface, global planner, localization framework, and terrain-aware local navigation. We evaluated the system on Mount Etna across three autonomous missions in varied terrain, achieving successful gas-source detections with autonomy rates of 93-100%. In addition, we conducted a teleoperated mission in which the robot measured natural fumaroles, detecting sulfur dioxide and carbon dioxide. We discuss lessons learned from the gas-analysis and autonomy perspectives, emphasizing the need for adaptive sensing strategies, tighter integration of global and local planning, and improved hardware design.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u4e00\u79cd\u817f\u5f0f\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u706b\u5c71\u6c14\u4f53\u5206\u6790\u4e2d\u7684\u81ea\u4e3b\u80fd\u529b\uff0c\u6210\u529f\u7387\u9ad8\u8fbe100%\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u81ea\u4e3b\u5bfc\u822a\u548c\u4f20\u611f\u7b56\u7565\u7684\u5efa\u8bae\u3002", "motivation": "\u706b\u5c71\u6c14\u4f53\u6392\u653e\u662f\u55b7\u53d1\u6d3b\u52a8\u7684\u5173\u952e\u524d\u5146\uff0c\u4f46\u7531\u4e8e\u8fd1\u5730\u9762\u6d4b\u91cf\u7684\u5371\u9669\u6027\u548c\u540e\u52e4\u6311\u6218\uff0c\u8feb\u5207\u9700\u8981\u81ea\u4e3b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u817f\u5f0f\u673a\u5668\u4eba\u7cfb\u7edfANYmal\uff0c\u914d\u5907\u56db\u6781\u8d28\u8c31\u4eea\uff0c\u6574\u5408\u4e86\u4efb\u52a1\u89c4\u5212\u754c\u9762\u3001\u5168\u5c40\u89c4\u5212\u5668\u3001\u5b9a\u4f4d\u6846\u67b6\u548c\u5730\u5f62\u610f\u8bc6\u5bfc\u822a\u3002", "result": "\u5728\u57c3\u7279\u7eb3\u5c71\u8fdb\u884c\u7684\u4e09\u6b21\u81ea\u4e3b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e8693-100%\u7684\u6210\u529f\u6c14\u4f53\u6e90\u68c0\u6d4b\uff0c\u53e6\u5916\u8fd8\u8fdb\u884c\u4e86\u9065\u63a7\u4efb\u52a1\uff0c\u6d4b\u91cf\u4e86\u81ea\u7136\u55b7\u6c14\u5b54\u4e2d\u7684\u4e8c\u6c27\u5316\u786b\u548c\u4e8c\u6c27\u5316\u78b3\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u4ece\u6c14\u4f53\u5206\u6790\u548c\u81ea\u6cbb\u7684\u89d2\u5ea6\u83b7\u5f97\u7684\u7ecf\u9a8c\u6559\u8bad\uff0c\u5f3a\u8c03\u9002\u5e94\u6027\u4f20\u611f\u7b56\u7565\u7684\u5fc5\u8981\u6027\u3001\u5168\u5c40\u4e0e\u5c40\u90e8\u89c4\u5212\u7684\u7d27\u5bc6\u96c6\u6210\u4ee5\u53ca\u6539\u8fdb\u786c\u4ef6\u8bbe\u8ba1\u3002"}}
{"id": "2601.07143", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.07143", "abs": "https://arxiv.org/abs/2601.07143", "authors": ["Hao Wang", "Wenhui Zhu", "Shao Tang", "Zhipeng Wang", "Xuanzhao Dong", "Xin Li", "Xiwen Chen", "Ashish Bastola", "Xinhao Huang", "Yalin Wang", "Abolfazl Razi"], "title": "EZBlender: Efficient 3D Editing with Plan-and-ReAct Agent", "comment": null, "summary": "As a cornerstone of the modern digital economy, 3D modeling and rendering demand substantial resources and manual effort when scene editing is performed in the traditional manner. Despite recent progress in VLM-based agents for 3D editing, the fundamental trade-off between editing precision and agent responsiveness remains unresolved. To overcome these limitations, we present EZBlender, a Blender agent with a hybrid framework that combines planning-based task decomposition and reactive local autonomy for efficient human AI collaboration and semantically faithful 3D editing. Specifically, this unexplored Plan-and-ReAct design not only preserves editing quality but also significantly reduces latency and computational cost. To further validate the efficiency and effectiveness of the proposed edge-autonomy architecture, we construct a dedicated multi-tasking benchmark that has not been systematically investigated in prior research. In addition, we provide a comprehensive analysis of language model preference, system responsiveness, and economic efficiency.", "AI": {"tldr": "EZBlender\u662f\u4e00\u79cd\u7ed3\u5408\u89c4\u5212\u4efb\u52a1\u5206\u89e3\u548c\u53cd\u5e94\u6027\u672c\u5730\u81ea\u4e3b\u6743\u7684Blender\u4ee3\u7406\uff0c\u65e8\u5728\u63d0\u9ad83D\u7f16\u8f91\u7684\u6548\u7387\u548c\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u76843D\u7f16\u8f91\u65b9\u6cd5\u5728\u573a\u666f\u7f16\u8f91\u4e2d\u9700\u8981\u5927\u91cf\u8d44\u6e90\u548c\u4eba\u5de5\u52aa\u529b\uff0c\u5c1a\u672a\u89e3\u51b3\u7f16\u8f91\u7cbe\u5ea6\u4e0e\u4ee3\u7406\u54cd\u5e94\u6027\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\u3002", "method": "EZBlender\u7ed3\u5408\u89c4\u5212\u4efb\u52a1\u5206\u89e3\u548c\u53cd\u5e94\u6027\u672c\u5730\u81ea\u4e3b\u6743\u7684\u6df7\u5408\u6846\u67b6\uff0c\u5b9e\u65bd\u4e86\u4e00\u79cd\u65b0\u7684Plan-and-ReAct\u8bbe\u8ba1\u3002", "result": "\u901a\u8fc7\u5efa\u7acb\u4e13\u95e8\u7684\u591a\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u8fb9\u7f18\u81ea\u4e3b\u67b6\u6784\u7684\u6548\u7387\u548c\u6709\u6548\u6027\uff0c\u5e76\u5206\u6790\u4e86\u8bed\u8a00\u6a21\u578b\u504f\u597d\u3001\u7cfb\u7edf\u54cd\u5e94\u6027\u548c\u7ecf\u6d4e\u6548\u7387\u3002", "conclusion": "EZBlender\u901a\u8fc7\u5176Plan-and-ReAct\u8bbe\u8ba1\u5728\u4fdd\u63013D\u7f16\u8f91\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u63a8\u52a8\u4e86\u4eba\u5de5\u667a\u80fd\u4e0e\u4eba\u7c7b\u7684\u9ad8\u6548\u5408\u4f5c\u3002"}}
{"id": "2601.07434", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.07434", "abs": "https://arxiv.org/abs/2601.07434", "authors": ["Xin Guan", "Fangguo Zhao", "Qianyi Wang", "Chengcheng Zhao", "Jiming Chen", "Shuo Li"], "title": "LOONG: Online Time-Optimal Autonomous Flight for MAVs in Cluttered Environments", "comment": null, "summary": "Autonomous flight of micro air vehicles (MAVs) in unknown, cluttered environments remains challenging for time-critical missions due to conservative maneuvering strategies. This article presents an integrated planning and control framework for high-speed, time-optimal autonomous flight of MAVs in cluttered environments. In each replanning cycle (100 Hz), a time-optimal trajectory under polynomial presentation is generated as a reference, with the time-allocation process accelerated by imitation learning. Subsequently, a time-optimal model predictive contouring control (MPCC) incorporates safe flight corridor (SFC) constraints at variable horizon steps to enable aggressive yet safe maneuvering, while fully exploiting the MAV's dynamics. We validate the proposed framework extensively on a custom-built LiDAR-based MAV platform. Simulation results demonstrate superior aggressiveness compared to the state of the art, while real-world experiments achieve a peak speed of 18 m/s in a cluttered environment and succeed in 10 consecutive trials from diverse start points. The video is available at the following link: https://youtu.be/vexXXhv99oQ.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5fae\u578b\u65e0\u4eba\u98de\u884c\u5668\u5728\u590d\u6742\u73af\u5883\u4e2d\u8fdb\u884c\u9ad8\u901f\u5ea6\u3001\u65f6\u95f4\u6700\u4f18\u81ea\u4e3b\u98de\u884c\u7684\u96c6\u6210\u89c4\u5212\u4e0e\u63a7\u5236\u6846\u67b6\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u5730\u5b9e\u9a8c\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u5fae\u578b\u65e0\u4eba\u98de\u884c\u5668\u5728\u672a\u77e5\u590d\u6742\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u98de\u884c\uff0c\u7279\u522b\u662f\u5728\u65f6\u95f4\u5173\u952e\u4efb\u52a1\u4e2d\uff0c\u7531\u4e8e\u4fdd\u5b88\u7684\u64cd\u63a7\u7b56\u7565\u9762\u4e34\u6311\u6218\u3002", "method": "\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u52a0\u901f\u65f6\u95f4\u5206\u914d\u8fc7\u7a0b\uff0c\u5728\u6bcf\u4e2a replanning \u5468\u671f\u5185\uff08100 Hz\uff09\u751f\u6210\u591a\u9879\u5f0f\u8868\u793a\u7684\u65f6\u95f4\u6700\u4f18\u8f68\u8ff9\u4f5c\u4e3a\u53c2\u8003\uff0c\u5e76\u91c7\u7528\u65f6\u95f4\u6700\u4f18\u6a21\u578b\u9884\u6d4b\u8f6e\u5ed3\u63a7\u5236\uff08MPCC\uff09\uff0c\u878d\u5165\u5b89\u5168\u98de\u884c\u8d70\u5eca\u7ea6\u675f\uff0c\u5b9e\u73b0\u6fc0\u8fdb\u4e14\u5b89\u5168\u7684\u64cd\u63a7\u3002", "result": "\u901a\u8fc7\u5b9a\u5236\u7684 LiDAR \u57fa\u7840 MAV \u5e73\u53f0\u8fdb\u884c\u5e7f\u6cdb\u9a8c\u8bc1\uff0c\u4eff\u771f\u7ed3\u679c\u663e\u793a\u51fa\u8f83\u73b0\u6709\u6280\u672f\u66f4\u4e3a\u6fc0\u8fdb\u7684\u6027\u80fd\uff0c\u5b9e\u5730\u5b9e\u9a8c\u4e2d\u5728\u590d\u6742\u73af\u5883\u4e2d\u8fbe\u5230\u6700\u9ad8\u901f\u5ea6 18 m/s\uff0c\u5e76\u6210\u529f\u8fdb\u884c 10 \u6b21\u8fde\u7eed\u8bd5\u9a8c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u63d0\u5347\u81ea\u4e3b\u98de\u884c\u6548\u7387\u4e0e\u5b89\u5168\u6027\u65b9\u9762\u5c55\u793a\u4e86\u663e\u8457\u7684\u4f18\u52bf\u3002"}}
{"id": "2601.07229", "categories": ["cs.HC", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.07229", "abs": "https://arxiv.org/abs/2601.07229", "authors": ["Eran Fainman", "Hagit Ben Shoshan", "Adir Solomon", "Osnat Mokryn"], "title": "DiSCo: Making Absence Visible in Intelligent Summarization Interfaces", "comment": null, "summary": "Intelligent interfaces increasingly use large language models to summarize user-generated content, yet these summaries emphasize what is mentioned while overlooking what is missing. This presence bias can mislead users who rely on summaries to make decisions. We present Domain Informed Summarization through Contrast (DiSCo), an expectation-based computational approach that makes absences visible by comparing each entity's content with domain topical expectations captured in reference distributions of aspects typically discussed in comparable accommodations. This comparison identifies aspects that are either unusually emphasized or missing relative to domain norms and integrates them into the generated text. In a user study across three accommodation domains, namely ski, beach, and city center, DiSCo summaries were rated as more detailed and useful for decision making than baseline large language model summaries, although slightly harder to read. The findings show that modeling expectations reduces presence bias and improves both transparency and decision support in intelligent summarization interfaces.", "AI": {"tldr": "\u5f00\u53d1DiSCo\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u9886\u57df\u671f\u671b\uff0c\u4f7f\u7f3a\u5931\u7684\u4fe1\u606f\u5728\u667a\u80fd\u6458\u8981\u4e2d\u53d8\u5f97\u53ef\u89c1\uff0c\u6539\u5584\u4e86\u51b3\u7b56\u652f\u6301\u548c\u900f\u660e\u5ea6\u3002", "motivation": "\u5f53\u524d\u7684\u667a\u80fd\u63a5\u53e3\u6458\u8981\u5b58\u5728\u504f\u89c1\uff0c\u5ffd\u7565\u4e86\u672a\u63d0\u53ca\u7684\u5185\u5bb9\uff0c\u53ef\u80fd\u8bef\u5bfc\u7528\u6237\u51b3\u7b56\u3002", "method": "DiSCo\u901a\u8fc7\u6bd4\u8f83\u5b9e\u4f53\u5185\u5bb9\u4e0e\u57df\u4e3b\u9898\u671f\u671b\u8fdb\u884c\u7f3a\u5931\u5185\u5bb9\u8bc6\u522b\uff0c\u6574\u5408\u8fd9\u4e9b\u7f3a\u5931\u4fe1\u606f\u5230\u751f\u6210\u7684\u6458\u8981\u4e2d\u3002", "result": "DiSCo\u662f\u4e00\u79cd\u901a\u8fc7\u6bd4\u8f83\u5b9e\u4f53\u5185\u5bb9\u4e0e\u9886\u57df\u4e3b\u9898\u671f\u671b\u6765\u8bc6\u522b\u5185\u5bb9\u7f3a\u5931\u7684\u667a\u80fd\u6458\u8981\u65b9\u6cd5\u3002", "conclusion": "DiSCo\u7684\u6458\u8981\u5728\u51b3\u7b56\u652f\u6301\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6458\u8981\uff0c\u5c3d\u7ba1\u53ef\u8bfb\u6027\u7a0d\u5dee\u3002"}}
{"id": "2601.07454", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.07454", "abs": "https://arxiv.org/abs/2601.07454", "authors": ["Yuxuan Hu", "Kuangji Zuo", "Boyu Ma", "Shihao Li", "Zhaoyang Xia", "Feng Xu", "Jianfei Yang"], "title": "WaveMan: mmWave-Based Room-Scale Human Interaction Perception for Humanoid Robots", "comment": null, "summary": "Reliable humanoid-robot interaction (HRI) in household environments is constrained by two fundamental requirements, namely robustness to unconstrained user positions and preservation of user privacy. Millimeter-wave (mmWave) sensing inherently supports privacy-preserving interaction, making it a promising modality for room-scale HRI. However, existing mmWave-based interaction-sensing systems exhibit poor spatial generalization at unseen distances or viewpoints. To address this challenge, we introduce WaveMan, a spatially adaptive room-scale perception system that restores reliable human interaction sensing across arbitrary user positions. WaveMan integrates viewpoint alignment and spectrogram enhancement for spatial consistency, with dual-channel attention for robust feature extraction. Experiments across five participants show that, under fixed-position evaluation, WaveMan achieves the same cross-position accuracy as the baseline with five times fewer training positions. In random free-position testing, accuracy increases from 33.00% to 94.33%, enabled by the proposed method. These results demonstrate the feasibility of reliable, privacy-preserving interaction for household humanoid robots across unconstrained user positions.", "AI": {"tldr": "WaveMan\u662f\u4e00\u79cd\u9002\u5e94\u6027\u5f3a\u7684\u6beb\u7c73\u6ce2\u611f\u77e5\u7cfb\u7edf\uff0c\u80fd\u5728\u5bb6\u5ead\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u51c6\u786e\u6027\u7684\u9690\u79c1\u4fdd\u62a4\u4eba\u673a\u4ea4\u4e92\uff0c\u6548\u679c\u660e\u663e\u4f18\u4e8e\u4ee5\u5f80\u7cfb\u7edf\u3002", "motivation": "\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\uff0c\u7279\u522b\u662f\u5728\u5bb6\u5ead\u73af\u5883\u4e2d\uff0c\u9700\u8981\u786e\u4fdd\u7528\u6237\u9690\u79c1\u5e76\u5bf9\u7528\u6237\u4f4d\u7f6e\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\uff0c\u8fd9\u6fc0\u53d1\u4e86\u5bf9\u6beb\u7c73\u6ce2\u4f20\u611f\u6280\u672f\u7684\u7814\u7a76\u3002", "method": "WaveMan\u96c6\u6210\u4e86\u89c6\u70b9\u5bf9\u9f50\u548c\u58f0\u8c31\u589e\u5f3a\u6280\u672f\uff0c\u5e76\u91c7\u7528\u53cc\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u5b9e\u73b0\u7a33\u5065\u7684\u7279\u5f81\u63d0\u53d6\u3002", "result": "\u5728\u56fa\u5b9a\u4f4d\u7f6e\u4e0b\uff0cWaveMan\u4e0e\u57fa\u7ebf\u7684\u8de8\u4f4d\u7f6e\u51c6\u786e\u6027\u76f8\u540c\uff0c\u4f46\u8bad\u7ec3\u4f4d\u7f6e\u51cf\u5c11\u5230\u4e94\u5206\u4e4b\u4e00\u3002\u5728\u968f\u673a\u81ea\u7531\u4f4d\u7f6e\u6d4b\u8bd5\u4e2d\uff0c\u51c6\u786e\u7387\u4ece33.00%\u63d0\u5347\u81f394.33%\u3002", "conclusion": "WaveMan\u7cfb\u7edf\u5728\u4e0d\u540c\u7528\u6237\u4f4d\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u4eba\u673a\u4ea4\u4e92\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u81ea\u7531\u4f4d\u7f6e\u4e0b\u7684\u51c6\u786e\u6027\uff0c\u652f\u6301\u9690\u79c1\u4fdd\u62a4\u7684\u5bb6\u7528\u673a\u5668\u4eba\u4ea4\u4e92\u3002"}}
{"id": "2601.07234", "categories": ["cs.HC", "cs.IR", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.07234", "abs": "https://arxiv.org/abs/2601.07234", "authors": ["Hagit Ben Shoshan", "Joel Lanir", "Pavel Goldstein", "Osnat Mokryn"], "title": "Making Absence Visible: The Roles of Reference and Prompting in Recognizing Missing Information", "comment": null, "summary": "Interactive systems that explain data, or support decision making often emphasize what is present while overlooking what is expected but missing. This presence bias limits users' ability to form complete mental models of a dataset or situation. Detecting absence depends on expectations about what should be there, yet interfaces rarely help users form such expectations. We present an experimental study examining how reference framing and prompting influence people's ability to recognize expected but missing categories in datasets. Participants compared distributions across three domains (energy, wealth, and regime) under two reference conditions: Global, presenting a unified population baseline, and Partial, showing several concrete exemplars. Results indicate that absence detection was higher with Partial reference than with Global reference, suggesting that partial, samples-based framing can support expectation formation and absence detection. When participants were prompted to look for what was missing, absence detection rose sharply. We discuss implications for interactive user interfaces and expectation-based visualization design, while considering cognitive trade-offs of reference structures and guided attention.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u53c2\u8003\u6846\u67b6\u548c\u63d0\u793a\u63d0\u9ad8\u7528\u6237\u53d1\u73b0\u6570\u636e\u96c6\u4e2d\u7f3a\u5931\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u90e8\u5206\u53c2\u8003\u548c\u63d0\u793a\u80fd\u663e\u8457\u63d0\u5347\u7f3a\u5931\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u63a2\u8ba8\u4ea4\u4e92\u7cfb\u7edf\u5982\u4f55\u5e2e\u52a9\u7528\u6237\u5f62\u6210\u5173\u4e8e\u6570\u636e\u96c6\u7684\u671f\u671b\uff0c\u4ee5\u4fbf\u66f4\u597d\u5730\u68c0\u6d4b\u7f3a\u5931\u4fe1\u606f\uff0c\u4ece\u800c\u514b\u670d\u5b58\u5728\u504f\u5dee\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u7814\u7a76\uff0c\u53c2\u4e0e\u8005\u5728\u4e0d\u540c\u53c2\u8003\u6761\u4ef6\uff08\u5168\u7403\u53c2\u8003\u548c\u90e8\u5206\u53c2\u8003\uff09\u4e0b\u6bd4\u8f83\u4e86\u4e09\u4e2a\u9886\u57df\uff08\u80fd\u6e90\u3001\u8d22\u5bcc\u548c\u653f\u6743\uff09\u4e2d\u7684\u6570\u636e\u5206\u5e03\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5728\u90e8\u5206\u53c2\u8003\u6761\u4ef6\u4e0b\uff0c\u7f3a\u5931\u68c0\u6d4b\u663e\u8457\u9ad8\u4e8e\u5168\u7403\u53c2\u8003\u6761\u4ef6\uff0c\u5f53\u63d0\u793a\u7528\u6237\u5bfb\u627e\u7f3a\u5931\u5185\u5bb9\u65f6\uff0c\u7f3a\u5931\u68c0\u6d4b\u6c34\u5e73\u6025\u5267\u4e0a\u5347\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u90e8\u5206\u53c2\u8003\u6846\u67b6\u548c\u63d0\u793a\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u7528\u6237\u5bf9\u7f3a\u5931\u7c7b\u522b\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u4ece\u800c\u6539\u5584\u4ea4\u4e92\u5f0f\u7528\u6237\u754c\u9762\u548c\u671f\u671b\u57fa\u7840\u53ef\u89c6\u5316\u8bbe\u8ba1\u3002"}}
{"id": "2601.07476", "categories": ["cs.RO", "cs.SE", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.07476", "abs": "https://arxiv.org/abs/2601.07476", "authors": ["Elia Cereda", "Alessandro Giusti", "Daniele Palossi"], "title": "NanoCockpit: Performance-optimized Application Framework for AI-based Autonomous Nanorobotics", "comment": "Source code available on GitHub at https://github.com/idsia-robotics/crazyflie-nanocockpit", "summary": "Autonomous nano-drones, powered by vision-based tiny machine learning (TinyML) models, are a novel technology gaining momentum thanks to their broad applicability and pushing scientific advancement on resource-limited embedded systems. Their small form factor, i.e., a few 10s grams, severely limits their onboard computational resources to sub-\\SI{100}{\\milli\\watt} microcontroller units (MCUs). The Bitcraze Crazyflie nano-drone is the \\textit{de facto} standard, offering a rich set of programmable MCUs for low-level control, multi-core processing, and radio transmission. However, roboticists very often underutilize these onboard precious resources due to the absence of a simple yet efficient software layer capable of time-optimal pipelining of multi-buffer image acquisition, multi-core computation, intra-MCUs data exchange, and Wi-Fi streaming, leading to sub-optimal control performances. Our \\textit{NanoCockpit} framework aims to fill this gap, increasing the throughput and minimizing the system's latency, while simplifying the developer experience through coroutine-based multi-tasking. In-field experiments on three real-world TinyML nanorobotics applications show our framework achieves ideal end-to-end latency, i.e. zero overhead due to serialized tasks, delivering quantifiable improvements in closed-loop control performance ($-$30\\% mean position error, mission success rate increased from 40\\% to 100\\%).", "AI": {"tldr": "NanoCockpit \u6846\u67b6\u901a\u8fc7\u63d0\u9ad8\u541e\u5410\u91cf\u548c\u51cf\u5c11\u5ef6\u8fdf\uff0c\u4f18\u5316\u4e86\u7eb3\u7c73\u65e0\u4eba\u673a\u7684\u8ba1\u7b97\u8d44\u6e90\u4f7f\u7528\uff0c\u63d0\u5347\u4e86\u95ed\u73af\u63a7\u5236\u6027\u80fd\u3002", "motivation": "\u5728\u8d44\u6e90\u6709\u9650\u7684\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\uff0c\u7eb3\u7c73\u65e0\u4eba\u673a\u7684\u8ba1\u7b97\u8d44\u6e90\u5e38\u88ab\u4f4e\u6548\u4f7f\u7528\u3002\u7f3a\u4e4f\u7b80\u5355\u9ad8\u6548\u7684\u8f6f\u4ef6\u5c42\u4f7f\u5f97\u5176\u63a7\u5236\u6027\u80fd\u4f4e\u4e0b\u3002\u56e0\u6b64\uff0c\u5f00\u53d1 NanoCockpit \u6846\u67b6\u4ee5\u4f18\u5316\u8fd9\u4e9b\u8d44\u6e90\u6210\u4e3a\u5fc5\u8981\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a NanoCockpit \u7684\u8f6f\u4ef6\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u534f\u7a0b\u7684\u591a\u4efb\u52a1\u5904\u7406\uff0c\u5b9e\u73b0\u65f6\u95f4\u6700\u4f18\u7684\u591a\u7f13\u51b2\u56fe\u50cf\u83b7\u53d6\u3001\u591a\u6838\u8ba1\u7b97\u3001MCUs \u95f4\u6570\u636e\u4ea4\u6362\u548c Wi-Fi \u6d41\u5a92\u4f53\u4f20\u8f93\uff0c\u4ee5\u63d0\u9ad8\u541e\u5410\u91cf\u548c\u51cf\u5c11\u5ef6\u8fdf\u3002", "result": "\u901a\u8fc7\u5728\u4e09\u79cd\u771f\u5b9e\u4e16\u754c\u7684 TinyML \u7eb3\u7c73\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u8fdb\u884c\u73b0\u573a\u5b9e\u9a8c\uff0c\u6211\u4eec\u7684\u6846\u67b6\u5b9e\u73b0\u4e86\u7406\u60f3\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\uff0c\u63d0\u4f9b\u4e86\u91cf\u5316\u7684\u95ed\u73af\u63a7\u5236\u6027\u80fd\u6539\u5584\u3002", "conclusion": "NanoCockpit \u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u95ed\u73af\u63a7\u5236\u6027\u80fd\uff0c\u51cf\u5c11\u4e86\u5e73\u5747\u4f4d\u7f6e\u8bef\u5dee\uff0c\u5e76\u4f7f\u4efb\u52a1\u6210\u529f\u7387\u4ece 40% \u63d0\u9ad8\u5230 100%\u3002"}}
{"id": "2601.07251", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.07251", "abs": "https://arxiv.org/abs/2601.07251", "authors": ["Zizhen Li", "Chuanhao Li", "Yibin Wang", "Yukang Feng", "Jianwen Sun", "Jiaxin Ai", "Fanrui Zhang", "Mingzhu Sun", "Yifei Huang", "Kaipeng Zhang"], "title": "MeepleLM: A Virtual Playtester Simulating Diverse Subjective Experiences", "comment": null, "summary": "Recent advancements have expanded the role of Large Language Models in board games from playing agents to creative co-designers. However, a critical gap remains: current systems lack the capacity to offer constructive critique grounded in the emergent user experience. Bridging this gap is fundamental for harmonizing Human-AI collaboration, as it empowers designers to refine their creations via external perspectives while steering models away from biased or unpredictable outcomes. Automating critique for board games presents two challenges: inferring the latent dynamics connecting rules to gameplay without an explicit engine, and modeling the subjective heterogeneity of diverse player groups. To address these, we curate a dataset of 1,727 structurally corrected rulebooks and 150K reviews selected via quality scoring and facet-aware sampling. We augment this data with Mechanics-Dynamics-Aesthetics (MDA) reasoning to explicitly bridge the causal gap between written rules and player experience. We further distill player personas and introduce MeepleLM, a specialized model that internalizes persona-specific reasoning patterns to accurately simulate the subjective feedback of diverse player archetypes. Experiments demonstrate that MeepleLM significantly outperforms latest commercial models (e.g., GPT-5.1, Gemini3-Pro) in community alignment and critique quality, achieving a 70% preference rate in user studies assessing utility. MeepleLM serves as a reliable virtual playtester for general interactive systems, marking a pivotal step towards audience-aligned, experience-aware Human-AI collaboration.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5982\u4f55\u901a\u8fc7MeepleLM\u6a21\u578b\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u684c\u6e38\u8bbe\u8ba1\u4e2d\u7684\u6279\u8bc4\u80fd\u529b\uff0c\u5f25\u8865\u4e86\u4eba\u5de5\u667a\u80fd\u5728\u7406\u89e3\u7528\u6237\u4f53\u9a8c\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u684c\u6e38\u4e2d\u7684\u5e94\u7528\u4e3b\u8981\u9650\u4e8e\u4f5c\u4e3a\u73a9\u800d\u4ee3\u7406\uff0c\u800c\u7f3a\u4e4f\u57fa\u4e8e\u7528\u6237\u4f53\u9a8c\u7684\u5efa\u8bbe\u6027\u6279\u8bc4\u80fd\u529b\u3002\u8fd9\u4e00\u80fd\u529b\u5bf9\u4f18\u5316\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u5305\u542b1727\u672c\u7ed3\u6784\u6b63\u786e\u7684\u89c4\u5219\u4e66\u548c150K\u6761\u8bc4\u8bba\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5229\u7528MDA\u63a8\u7406\u548c\u73a9\u5bb6\u89d2\u8272\u63d0\u5347\u5bf9\u89c4\u5219\u4e0e\u6e38\u620f\u4f53\u9a8c\u4e4b\u95f4\u56e0\u679c\u5173\u7cfb\u7684\u7406\u89e3\u3002\u5f15\u5165MeepleLM\u6a21\u578b\u4ee5\u6a21\u62df\u4e0d\u540c\u73a9\u5bb6\u7684\u4e3b\u89c2\u53cd\u9988\u3002", "result": "MeepleLM\u80fd\u591f\u63d0\u4f9b\u57fa\u4e8e\u7528\u6237\u4f53\u9a8c\u7684\u5efa\u8bbe\u6027\u6279\u8bc4\uff0c\u663e\u8457\u63d0\u5347\u6e38\u620f\u8bbe\u8ba1\u8fc7\u7a0b\u4e2d\u7684\u4eba\u673a\u534f\u4f5c\u6548\u679c\u3002", "conclusion": "MeepleLM\u4f5c\u4e3a\u4e00\u79cd\u4e13\u95e8\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u5185\u90e8\u5316\u73a9\u5bb6\u7279\u5b9a\u7684\u63a8\u7406\u6a21\u5f0f\uff0c\u6539\u5584\u4e86\u5bf9\u4e0d\u540c\u73a9\u5bb6\u7fa4\u4f53\u7684\u53cd\u9988\u6a21\u62df\uff0c\u63a8\u52a8\u4e86\u4eba\u673a\u534f\u4f5c\u7684\u8fdb\u6b65\u3002"}}
{"id": "2601.07558", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.07558", "abs": "https://arxiv.org/abs/2601.07558", "authors": ["Chen Feng", "Guiyong Zheng", "Tengkai Zhuang", "Yongqian Wu", "Fangzhan He", "Haojia Li", "Juepeng Zheng", "Shaojie Shen", "Boyu Zhou"], "title": "FlyCo: Foundation Model-Empowered Drones for Autonomous 3D Structure Scanning in Open-World Environments", "comment": "34 pages, 24 figures, 9 tables. Video: https://www.youtube.com/playlist?list=PLqjZjnqsCyl40rw3y15Yzc7Mdo-z1y2j8", "summary": "Autonomous 3D scanning of open-world target structures via drones remains challenging despite broad applications. Existing paradigms rely on restrictive assumptions or effortful human priors, limiting practicality, efficiency, and adaptability. Recent foundation models (FMs) offer great potential to bridge this gap. This paper investigates a critical research problem: What system architecture can effectively integrate FM knowledge for this task? We answer it with FlyCo, a principled FM-empowered perception-prediction-planning loop enabling fully autonomous, prompt-driven 3D target scanning in diverse unknown open-world environments. FlyCo directly translates low-effort human prompts (text, visual annotations) into precise adaptive scanning flights via three coordinated stages: (1) perception fuses streaming sensor data with vision-language FMs for robust target grounding and tracking; (2) prediction distills FM knowledge and combines multi-modal cues to infer the partially observed target's complete geometry; (3) planning leverages predictive foresight to generate efficient and safe paths with comprehensive target coverage. Building on this, we further design key components to boost open-world target grounding efficiency and robustness, enhance prediction quality in terms of shape accuracy, zero-shot generalization, and temporal stability, and balance long-horizon flight efficiency with real-time computability and online collision avoidance. Extensive challenging real-world and simulation experiments show FlyCo delivers precise scene understanding, high efficiency, and real-time safety, outperforming existing paradigms with lower human effort and verifying the proposed architecture's practicality. Comprehensive ablations validate each component's contribution. FlyCo also serves as a flexible, extensible blueprint, readily leveraging future FM and robotics advances. Code will be released.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFlyCo\uff0c\u4e00\u4e2a\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u81ea\u4e3b3D\u626b\u63cf\u7cfb\u7edf\uff0c\u7ed3\u5408\u611f\u77e5\u3001\u9884\u6d4b\u548c\u89c4\u5212\uff0c\u663e\u793a\u51fa\u5728\u591a\u6837\u672a\u77e5\u73af\u5883\u4e2d\u6709\u6548\u7684\u573a\u666f\u7406\u89e3\u548c\u64cd\u4f5c\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u65e0\u4eba\u673a\u5728\u5f00\u653e\u4e16\u754c\u76ee\u6807\u7ed3\u6784\u76843D\u626b\u63cf\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u4e25\u683c\u5047\u8bbe\u548c\u4eba\u5de5\u5148\u9a8c\uff0c\u6548\u7387\u548c\u9002\u5e94\u6027\u4e0d\u8db3\uff0c\u8feb\u5207\u9700\u8981\u7a81\u7834\u6027\u89e3\u51b3\u65b9\u6848\u3002", "method": "FlyCo\u7cfb\u7edf\u67b6\u6784\u96c6\u6210\u57fa\u7840\u6a21\u578b\u77e5\u8bc6\uff0c\u91c7\u7528\u611f\u77e5\u3001\u9884\u6d4b\u3001\u89c4\u5212\u4e09\u9636\u6bb5\u5faa\u73af\u5b9e\u73b0\u5168\u81ea\u4e3b3D\u76ee\u6807\u626b\u63cf", "result": "\u6df1\u5ea6\u9a8c\u8bc1\u4e86FlyCo\u5728\u5b9e\u666f\u4e0e\u6a21\u62df\u73af\u5883\u4e2d\u63d0\u4f9b\u7cbe\u51c6\u573a\u666f\u7406\u89e3\u3001\u9ad8\u6548\u7387\u548c\u5b9e\u65f6\u5b89\u5168\u6027\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6848\u5e76\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u3002", "conclusion": "FlyCo\u4f5c\u4e3a\u4e00\u79cd\u7075\u6d3b\u53ef\u6269\u5c55\u7684\u84dd\u56fe\uff0c\u5c55\u73b0\u4e86\u5728\u672a\u6765\u57fa\u7840\u6a21\u578b\u4e0e\u673a\u5668\u4eba\u6280\u672f\u8fdb\u5c55\u4e2d\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u4ee3\u7801\u5c06\u5728\u516c\u5f00\u63d0\u4f9b\u3002"}}
{"id": "2601.07262", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.07262", "abs": "https://arxiv.org/abs/2601.07262", "authors": ["Jiamu Zhou", "Jihong Wang", "Weiming Zhang", "Weiwen Liu", "Zhuosheng Zhang", "Xingyu Lou", "Weinan Zhang", "Huarong Deng", "Jun Wang"], "title": "ColorBrowserAgent: An Intelligent GUI Agent for Complex Long-Horizon Web Automation", "comment": null, "summary": "The web browser serves as a primary interface for daily human activities, making its automation a critical frontier for Human-Centred AI. While Large Language Models (LLMs) have enabled autonomous agents to interact with web GUIs, their reliability in real-world scenarios is hampered by long-horizon instability and the vast heterogeneity of site designs. In this paper, we introduce ColorBrowserAgent, a framework designed for Collaborative Autonomy in complex web tasks. Our approach integrates two human-centred mechanisms: (1) Progressive Progress Summarization, which mimics human short-term memory to maintain coherence over extended interactions; and (2) Human-in-the-Loop Knowledge Adaptation, which bridges the knowledge gap in diverse environments by soliciting expert intervention only when necessary. This symbiotic design allows the agent to learn from human tips without extensive retraining, effectively combining the scalability of AI with the adaptability of human cognition. Evaluated on the WebArena benchmark using GPT-5, ColorBrowserAgent achieves a state-of-the-art success rate of 71.2\\%, demonstrating the efficacy of interactive human assistance in robust web automation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5408\u6210\u7684\u81ea\u52a8\u5316\u6846\u67b6ColorBrowserAgent\uff0c\u7ed3\u5408\u4e86\u6e10\u8fdb\u5f0f\u8fdb\u5ea6\u6458\u8981\u548c\u4eba\u673a\u4e92\u52a8\u77e5\u8bc6\u9002\u5e94\u673a\u5236\u4ee5\u63d0\u9ad8\u7f51\u7edc\u4efb\u52a1\u7684\u81ea\u4e3b\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u7f51\u7edc\u6d4f\u89c8\u5668\u4f5c\u4e3a\u4eba\u7c7b\u65e5\u5e38\u6d3b\u52a8\u7684\u4e3b\u8981\u63a5\u53e3\uff0c\u5176\u81ea\u52a8\u5316\u662f\u4eba\u673a\u4e2d\u5fc3\u4eba\u5de5\u667a\u80fd\u7684\u91cd\u8981\u524d\u6cbf\u9886\u57df\u3002", "method": "ColorBrowserAgent\u6846\u67b6\u91c7\u7528\u4e86\u6e10\u8fdb\u5f0f\u8fdb\u5ea6\u6458\u8981\u548c\u4eba\u673a\u4e92\u52a8\u77e5\u8bc6\u9002\u5e94\u673a\u5236\uff0c\u4ee5\u6a21\u4eff\u4eba\u7c7b\u77ed\u671f\u8bb0\u5fc6\u5e76\u9002\u5e94\u591a\u6837\u5316\u73af\u5883\u3002", "result": "\u5728WebArena\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cColorBrowserAgent\u4e0eGPT-5\u7684\u7ed3\u5408\u5b9e\u73b0\u4e8671.2%\u7684\u6210\u529f\u7387\uff0c\u5c55\u73b0\u4e86\u4e92\u52a8\u4eba\u7c7b\u5e2e\u52a9\u5728\u7a33\u5065\u7f51\u7edc\u81ea\u52a8\u5316\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u6574\u5408\u4e86\u4eba\u7c7b\u7684\u9002\u5e94\u6027\u548c\u4eba\u5de5\u667a\u80fd\u7684\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u590d\u6742\u7f51\u7edc\u4efb\u52a1\u7684\u534f\u4f5c\u81ea\u4e3b\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.07559", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.07559", "abs": "https://arxiv.org/abs/2601.07559", "authors": ["Yuki Kuroda", "Tomoya Takahashi", "Cristian C. Beltran-Hernandez", "Kazutoshi Tanaka", "Masashi Hamaya"], "title": "Stable In-hand Manipulation for a Lightweight Four-motor Prosthetic Hand", "comment": null, "summary": "Electric prosthetic hands should be lightweight to decrease the burden on the user, shaped like human hands for cosmetic purposes, and designed with motors enclosed inside to protect them from damage and dirt. Additionally, in-hand manipulation is necessary to perform daily activities such as transitioning between different postures, particularly through rotational movements, such as reorienting a pen into a writing posture after picking it up from a desk. We previously developed PLEXUS hand (Precision-Lateral dEXteroUS manipulation hand), a lightweight (311 g) prosthetic hand driven by four motors. This prosthetic performed reorientation between precision and lateral grasps with various objects. However, its controller required predefined object widths and was limited to handling lightweight objects (of weight up to 34 g). This study addresses these limitations by employing motor current feedback. Combined with the hand's previously optimized single-axis thumb, this approach achieves more stable manipulation by estimating the object's width and adjusting the index finger position to maintain stable object holding during the reorientation. Experimental validation using primitive objects of various widths (5-30 mm) and shapes (cylinders and prisms) resulted in a 100% success rate with lightweight objects and maintained a high success rate (>=80) even with heavy aluminum prisms (of weight up to 289 g). By contrast, the performance without index finger coordination dropped to just 40% on the heaviest 289 g prism. The hand also successfully executed several daily tasks, including closing bottle caps and orienting a pen for writing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7535\u52a8\u5047\u624b\uff0c\u901a\u8fc7\u7535\u673a\u7535\u6d41\u53cd\u9988\u4f18\u5316\u64cd\u63a7\uff0c\u6210\u529f\u5904\u7406\u591a\u79cd\u7269\u4f53\uff0c\u5e76\u6267\u884c\u5e38\u89c1\u65e5\u5e38\u4efb\u52a1\u3002", "motivation": "\u6539\u5584\u5047\u624b\u64cd\u4f5c\u6027\u80fd\uff0c\u6ee1\u8db3\u7528\u6237\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u5bf9\u91cd\u7269\u53ca\u5404\u79cd\u7269\u4f53\u7684\u64cd\u63a7\u9700\u6c42\u3002", "method": "\u91c7\u7528\u7535\u673a\u7535\u6d41\u53cd\u9988\u4e0e\u4f18\u5316\u7684\u5355\u8f74\u62c7\u6307\u7ed3\u5408\uff0c\u4f30\u7b97\u7269\u4f53\u5bbd\u5ea6\u5e76\u8c03\u6574\u98df\u6307\u4f4d\u7f6e\uff0c\u589e\u5f3a\u624b\u90e8\u64cd\u63a7\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\uff0c\u5728\u5904\u74065-30\u6beb\u7c73\u5bbd\u7684\u8f7b\u8d28\u7269\u4f53\u65f6\u6210\u529f\u7387\u8fbe\u5230100%\uff0c\u5728\u91cd\u94dd\u68f1\u67f1(\u91cd\u8fbe289\u514b)\u7684\u5904\u7406\u4e2d\u6210\u529f\u7387\u4fdd\u6301\u572880%\u4ee5\u4e0a\u3002", "conclusion": "\u6539\u8fdb\u540e\u7684PLEXUS\u624b\u5728\u65e5\u5e38\u6d3b\u52a8\u4e2d\u7684\u8868\u73b0\u63d0\u5347\uff0c\u80fd\u591f\u6267\u884c\u5173\u95ed\u74f6\u76d6\u548c\u8c03\u8282\u7b14\u7684\u4f4d\u7f6e\u7b49\u4efb\u52a1\u3002"}}
{"id": "2601.07381", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.07381", "abs": "https://arxiv.org/abs/2601.07381", "authors": ["Yui Kondo", "Kevin Dunnell", "Isobel Voysey", "Qing Hu", "Victoria Paesano", "Phi H Nguyen", "Qing Xiao", "Jun Zhao", "Luc Rocher"], "title": "Interactive visualizations for adolescents to understand and challenge algorithmic profiling in online platforms", "comment": null, "summary": "Social media platforms regularly track, aggregate, and monetize adolescents' data, yet provide them with little visibility or agency over how algorithms construct their digital identities and make inferences about them. We introduce Algorithmic Mirror, an interactive visualization tool that transforms opaque profiling practices into explorable landscapes of personal data. It uniquely leverages adolescents' real digital footprints across YouTube, TikTok, and Netflix, to provide situated, personalized insights into datafication over time. In our study with 27 participants (ages 12--16), we show how engaging with their own data enabled adolescents to uncover the scale and persistence of data collection, recognize cross-platform profiling, and critically reflect algorithmic categorizations of their interests. These findings highlight how identity is a powerful motivator for adolescents' desire for greater digital agency, underscoring the need for platforms and policymakers to move toward structural reforms that guarantee children better transparency and the agency to influence their online experiences.", "AI": {"tldr": "\u7814\u7a76\u5c55\u793a\u4e86\u9752\u5c11\u5e74\u901a\u8fc7\u4e92\u52a8\u5de5\u5177\u7406\u89e3\u548c\u53cd\u601d\u4ed6\u4eec\u7684\u4e2a\u4eba\u6570\u636e\uff0c\u547c\u5401\u66f4\u9ad8\u7684\u900f\u660e\u5ea6\u548c\u4ee3\u7406\u6743\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u5bf9\u9752\u5c11\u5e74\u6570\u636e\u7684\u8ffd\u8e2a\u4e0e\u5206\u6790\u7684\u7f3a\u4e4f\u900f\u660e\u6027\u548c\u9752\u5c11\u5e74\u5bf9\u6570\u5b57\u8eab\u4efd\u7684\u63a7\u5236\u529b\u4e0d\u8db3\u3002", "method": "\u5f15\u5165\u4e92\u52a8\u53ef\u89c6\u5316\u5de5\u5177Algorithmic Mirror\uff0c\u8f6c\u53d8\u4e0d\u900f\u660e\u7684\u4e2a\u4eba\u6570\u636e\u5206\u6790\u4e3a\u53ef\u63a2\u7d22\u7684\u98ce\u666f\u3002", "result": "\u901a\u8fc7\u53c2\u4e0e\u8005\u7684\u6570\u5b57\u8db3\u8ff9\u63d0\u4f9b\u4e2a\u6027\u5316\u6570\u636e\u6d1e\u89c1\uff0c\u5e2e\u52a9\u9752\u5c11\u5e74\u7406\u89e3\u6570\u636e\u6536\u96c6\u7684\u89c4\u6a21\u548c\u6301\u7eed\u6027\u3002", "conclusion": "\u9752\u5c11\u5e74\u5bf9\u4e2a\u4eba\u8eab\u4efd\u7684\u7406\u89e3\u9a71\u52a8\u4e86\u4ed6\u4eec\u5bf9\u6570\u5b57\u4ee3\u7406\u6743\u7684\u6e34\u671b\uff0c\u5f3a\u8c03\u4e86\u5e73\u53f0\u548c\u653f\u7b56\u5236\u5b9a\u8005\u9700\u8981\u8fdb\u884c\u7ed3\u6784\u6027\u6539\u9769\u3002"}}
{"id": "2601.07701", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07701", "abs": "https://arxiv.org/abs/2601.07701", "authors": ["Ziwen Zhuang", "Shaoting Zhu", "Mengjie Zhao", "Hang Zhao"], "title": "Deep Whole-body Parkour", "comment": null, "summary": "Current approaches to humanoid control generally fall into two paradigms: perceptive locomotion, which handles terrain well but is limited to pedal gaits, and general motion tracking, which reproduces complex skills but ignores environmental capabilities. This work unites these paradigms to achieve perceptive general motion control. We present a framework where exteroceptive sensing is integrated into whole-body motion tracking, permitting a humanoid to perform highly dynamic, non-locomotion tasks on uneven terrain. By training a single policy to perform multiple distinct motions across varied terrestrial features, we demonstrate the non-trivial benefit of integrating perception into the control loop. Our results show that this framework enables robust, highly dynamic multi-contact motions, such as vaulting and dive-rolling, on unstructured terrain, significantly expanding the robot's traversability beyond simple walking or running. https://project-instinct.github.io/deep-whole-body-parkour", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u611f\u77e5\u4e0e\u4e00\u822c\u8fd0\u52a8\u63a7\u5236\u7684\u65b9\u6cd5\uff0c\u4f7f\u4eba\u5f62\u673a\u5668\u4eba\u80fd\u591f\u5728\u4e0d\u5e73\u5766\u5730\u5f62\u4e0a\u6267\u884c\u52a8\u6001\u975e\u884c\u8d70\u4efb\u52a1\u3002", "motivation": "\u5e0c\u671b\u5c06\u73b0\u6709\u7684\u4eba\u5f62\u63a7\u5236\u65b9\u6cd5\u7ed3\u5408\u8d77\u6765\uff0c\u4ee5\u63d0\u5347\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e0a\u7684\u8fd0\u52a8\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e00\u4e2a\u6846\u67b6\uff0c\u5c06\u5916\u90e8\u611f\u77e5\u6574\u5408\u5230\u5168\u8eab\u8fd0\u52a8\u8ddf\u8e2a\u4e2d\uff0c\u8bad\u7ec3\u4e00\u4e2a\u653f\u7b56\u6267\u884c\u591a\u79cd\u4e0d\u540c\u52a8\u4f5c\u3002", "result": "\u5b9e\u73b0\u4e86\u5728\u4e0d\u89c4\u5219\u5730\u5f62\u4e0a\u6267\u884c\u9ad8\u52a8\u6001\u591a\u63a5\u89e6\u52a8\u4f5c\uff0c\u5982\u7ffb\u8d8a\u548c\u7ffb\u6eda\uff0c\u663e\u8457\u6269\u5c55\u4e86\u673a\u5668\u4eba\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u6574\u5408\u611f\u77e5\u4fe1\u606f\u80fd\u591f\u663e\u8457\u589e\u5f3a\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u8fd0\u52a8\u80fd\u529b\u3002"}}
{"id": "2601.07401", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.07401", "abs": "https://arxiv.org/abs/2601.07401", "authors": ["Raj Mahmud", "Shlomo Berkovsky", "Mukesh Prasad", "A. Baki Kocaballi"], "title": "Recommendation-as-Experience: A framework for context-sensitive adaptation in conversational recommender systems", "comment": null, "summary": "While Conversational Recommender Systems (CRS) have matured technically, they frequently lack principled methods for encoding latent experiential aims as adaptive state variables. Consequently, contemporary architectures often prioritise ranking accuracy at the expense of nuanced, context-sensitive interaction behaviours. This paper addresses this gap through a comprehensive multi-domain study ($N = 168$) that quantifies the joint prioritisation of three critical interaction aims: educative (to inform and justify), explorative (to diversify and inspire), and affective (to align emotionally and socially). Utilising Bayesian hierarchical ordinal regression, we establish domain profiles and perceived item value as systematic modulators of these priorities. Furthermore, we identify stable user-level preferences for autonomy that persist across distinct interactional goals, suggesting that agency is a fundamental requirement of the conversational experience. Drawing on these empirical foundations, we formalise the Recommendation-as-Experience (RAE) adaptation framework. RAE systematically encodes contextual and individual signals into structured state representations, mapping them to experience-aligned dialogue policies realised through retrieval diversification, heuristic logic, or Large Language Model based controllable generation. As an architecture-agnostic blueprint, RAE facilitates the design of context-sensitive CRS that effectively balance experiential quality with predictive performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684Recommendation-as-Experience\uff08RAE\uff09\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u7efc\u5408\u8003\u8651\u6559\u80b2\u3001\u63a2\u7d22\u548c\u60c5\u611f\u4ea4\u4e92\u76ee\u6807\uff0c\u6539\u8fdb\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u7684\u8bbe\u8ba1\u548c\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u5f53\u524d\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u5728\u4ea4\u4e92\u76ee\u6807\u7f16\u7801\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u63a2\u8ba8\u5982\u4f55\u540c\u65f6\u8003\u8651\u7528\u6237\u7684\u6559\u80b2\u3001\u63a2\u7d22\u548c\u60c5\u611f\u9700\u6c42\uff0c\u4ee5\u63d0\u5347\u4ea4\u4e92\u8d28\u91cf\u3002", "method": "\u901a\u8fc7\u8d1d\u53f6\u65af\u5c42\u6b21\u5e8f\u56de\u5f52\u5206\u6790168\u4e2a\u591a\u9886\u57df\u7684\u6570\u636e\uff0c\u91cf\u5316\u4e86\u6559\u80b2\u3001\u63a2\u7d22\u548c\u60c5\u611f\u4ea4\u4e92\u76ee\u6807\u7684\u4f18\u5148\u7ea7\uff0c\u5e76\u63d0\u51fa\u4e86RAE\u6846\u67b6\uff0c\u5c06\u4e0a\u4e0b\u6587\u548c\u4e2a\u4f53\u4fe1\u53f7\u7ed3\u6784\u5316\u6620\u5c04\u5230\u5bf9\u8bdd\u7b56\u7565\u3002", "result": "\u53d1\u73b0\u7528\u6237\u5bf9\u81ea\u4e3b\u6743\u7684\u7a33\u5b9a\u504f\u597d\u5728\u4e0d\u540c\u7684\u4ea4\u4e92\u76ee\u6807\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u5e76\u4e14\u80fd\u591f\u6709\u6548\u5730\u5e73\u8861\u63a8\u8350\u8d28\u91cf\u4e0e\u7528\u6237\u4f53\u9a8c\u3002", "conclusion": "RAE\u6846\u67b6\u4f5c\u4e3a\u4e00\u79cd\u67b6\u6784\u65e0\u5173\u7684\u8bbe\u8ba1\u84dd\u56fe\uff0c\u4e3a\u7075\u6d3b\u5e94\u5bf9\u7528\u6237\u9700\u6c42\u548c\u63d0\u9ad8\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u7684\u9002\u5e94\u6027\u548c\u4f53\u9a8c\u8d28\u91cf\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2601.07718", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07718", "abs": "https://arxiv.org/abs/2601.07718", "authors": ["Shaoting Zhu", "Ziwen Zhuang", "Mengjie Zhao", "Kun-Ying Lee", "Hang Zhao"], "title": "Hiking in the Wild: A Scalable Perceptive Parkour Framework for Humanoids", "comment": "Project Page: https://project-instinct.github.io/hiking-in-the-wild", "summary": "Achieving robust humanoid hiking in complex, unstructured environments requires transitioning from reactive proprioception to proactive perception. However, integrating exteroception remains a significant challenge: mapping-based methods suffer from state estimation drift; for instance, LiDAR-based methods do not handle torso jitter well. Existing end-to-end approaches often struggle with scalability and training complexity; specifically, some previous works using virtual obstacles are implemented case-by-case. In this work, we present \\textit{Hiking in the Wild}, a scalable, end-to-end parkour perceptive framework designed for robust humanoid hiking. To ensure safety and training stability, we introduce two key mechanisms: a foothold safety mechanism combining scalable \\textit{Terrain Edge Detection} with \\textit{Foot Volume Points} to prevent catastrophic slippage on edges, and a \\textit{Flat Patch Sampling} strategy that mitigates reward hacking by generating feasible navigation targets. Our approach utilizes a single-stage reinforcement learning scheme, mapping raw depth inputs and proprioception directly to joint actions, without relying on external state estimation. Extensive field experiments on a full-size humanoid demonstrate that our policy enables robust traversal of complex terrains at speeds up to 2.5 m/s. The training and deployment code is open-sourced to facilitate reproducible research and deployment on real robots with minimal hardware modifications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u300aHiking in the Wild\u300b\u7684\u53ef\u6269\u5c55\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7528\u4e8e\u7c7b\u4eba\u5f92\u6b65\u884c\u8d70\uff0c\u89e3\u51b3\u4e86\u5728\u590d\u6742\u73af\u5883\u4e2d\u884c\u8d70\u7684\u591a\u9879\u6311\u6218\uff0c\u4e14\u5728\u901f\u5ea6\u548c\u5b89\u5168\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5728\u590d\u6742\u4e14\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u9c81\u68d2\u7684\u7c7b\u4eba\u5f92\u6b65\u884c\u8d70\uff0c\u9700\u8981\u4ece\u53cd\u5e94\u5f0f\u672c\u4f53\u611f\u77e5\u8f6c\u5411\u4e3b\u52a8\u611f\u77e5\uff0c\u4ee5\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u5229\u7528\u5355\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u65b9\u6848\uff0c\u76f4\u63a5\u5c06\u539f\u59cb\u6df1\u5ea6\u8f93\u5165\u548c\u672c\u4f53\u611f\u53d7\u6620\u5c04\u5230\u5173\u8282\u52a8\u4f5c\uff0c\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u72b6\u6001\u4f30\u8ba1\u3002", "result": "\u5927\u91cf\u5b9e\u5730\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u7b56\u7565\u53ef\u4ee5\u5728\u590d\u6742\u5730\u5f62\u4e2d\u5feb\u901f\u800c\u7a33\u5065\u5730\u884c\u8d70\uff0c\u901f\u5ea6\u9ad8\u8fbe2.5\u7c73/\u79d2\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u7684\u6846\u67b6\u53ef\u4ee5\u6709\u6548\u5730\u5728\u590d\u6742\u5730\u5f62\u4e2d\u5b9e\u73b0\u9c81\u68d2\u7684\u7c7b\u4eba\u5f92\u6b65\u884c\u8d70\uff0c\u901f\u5ea6\u53ef\u8fbe2.5\u7c73/\u79d2\uff0c\u4e14\u4ee3\u7801\u5df2\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u7814\u7a76\u7684\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2601.07556", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07556", "abs": "https://arxiv.org/abs/2601.07556", "authors": ["Siyang Li", "Jiayi Ouyang", "Zhenyao Cui", "Ziwei Wang", "Tianwang Jia", "Feng Wan", "Dongrui Wu"], "title": "Backpropagation-Free Test-Time Adaptation for Lightweight EEG-Based Brain-Computer Interfaces", "comment": null, "summary": "Electroencephalogram (EEG)-based brain-computer interfaces (BCIs) face significant deployment challenges due to inter-subject variability, signal non-stationarity, and computational constraints. While test-time adaptation (TTA) mitigates distribution shifts under online data streams without per-use calibration sessions, existing TTA approaches heavily rely on explicitly defined loss objectives that require backpropagation for updating model parameters, which incurs computational overhead, privacy risks, and sensitivity to noisy data streams. This paper proposes Backpropagation-Free Transformations (BFT), a TTA approach for EEG decoding that eliminates such issues. BFT applies multiple sample-wise transformations of knowledge-guided augmentations or approximate Bayesian inference to each test trial, generating multiple prediction scores for a single test sample. A learning-to-rank module enhances the weighting of these predictions, enabling robust aggregation for uncertainty suppression during inference under theoretical justifications. Extensive experiments on five EEG datasets of motor imagery classification and driver drowsiness regression tasks demonstrate the effectiveness, versatility, robustness, and efficiency of BFT. This research enables lightweight plug-and-play BCIs on resource-constrained devices, broadening the real-world deployment of decoding algorithms for EEG-based BCI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u53cd\u5411\u4f20\u64ad\u7684EEG\u89e3\u7801\u9002\u5e94\u65b9\u6cd5\uff08BFT\uff09\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u9ad8\u8111-\u673a\u63a5\u53e3\u7684\u90e8\u7f72\u80fd\u529b\u3002", "motivation": "EEG\u57fa\u7840\u7684\u8111-\u673a\u63a5\u53e3\u9762\u4e34\u4e2a\u4f53\u5dee\u5f02\u3001\u4fe1\u53f7\u975e\u5e73\u7a33\u6027\u548c\u8ba1\u7b97\u9650\u5236\u7b49\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u51cf\u8f7b\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u65e0\u53cd\u5411\u4f20\u64ad\u7684\u53d8\u6362\uff08BFT\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6bcf\u4e2a\u6d4b\u8bd5\u6837\u672c\u8fdb\u884c\u77e5\u8bc6\u5bfc\u5411\u7684\u591a\u6837\u672c\u53d8\u6362\uff0c\u751f\u6210\u591a\u4e2a\u9884\u6d4b\u5206\u6570\uff0c\u5e76\u5f15\u5165\u5b66\u4e60\u6392\u5e8f\u6a21\u5757\u52a0\u6743\u8fd9\u4e9b\u9884\u6d4b\u3002", "result": "\u5728\u4e94\u4e2aEEG\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cBFT\u5728\u7535\u52a8\u673a\u60f3\u8c61\u5206\u7c7b\u548c\u9a7e\u9a76\u75b2\u52b3\u56de\u5f52\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u3001\u7075\u6d3b\u6027\u3001\u9c81\u68d2\u6027\u548c\u9ad8\u6548\u6027\u3002", "conclusion": "BFT\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8f7b\u91cf\u5316\u7684\u8111-\u673a\u63a5\u53e3\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86EEG\u4fe1\u53f7\u89e3\u7801\u7684\u5b9e\u7528\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2601.07768", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.07768", "abs": "https://arxiv.org/abs/2601.07768", "authors": ["Alex Huang", "Akshay Karthik"], "title": "THETA: Triangulated Hand-State Estimation for Teleoperation and Automation in Robotic Hand Control", "comment": "The 11th International Conference on Engineering and Emerging Technologies (ICEET) 2025", "summary": "The teleoperation of robotic hands is limited by the high costs of depth cameras and sensor gloves, commonly used to estimate hand relative joint positions (XYZ). We present a novel, cost-effective approach using three webcams for triangulation-based tracking to approximate relative joint angles (theta) of human fingers. We also introduce a modified DexHand, a low-cost robotic hand from TheRobotStudio, to demonstrate THETA's real-time application. Data collection involved 40 distinct hand gestures using three 640x480p webcams arranged at 120-degree intervals, generating over 48,000 RGB images. Joint angles were manually determined by measuring midpoints of the MCP, PIP, and DIP finger joints. Captured RGB frames were processed using a DeepLabV3 segmentation model with a ResNet-50 backbone for multi-scale hand segmentation. The segmented images were then HSV-filtered and fed into THETA's architecture, consisting of a MobileNetV2-based CNN classifier optimized for hierarchical spatial feature extraction and a 9-channel input tensor encoding multi-perspective hand representations. The classification model maps segmented hand views into discrete joint angles, achieving 97.18% accuracy, 98.72% recall, F1 Score of 0.9274, and a precision of 0.8906. In real-time inference, THETA captures simultaneous frames, segments hand regions, filters them, and compiles a 9-channel tensor for classification. Joint-angle predictions are relayed via serial to an Arduino, enabling the DexHand to replicate hand movements. Future research will increase dataset diversity, integrate wrist tracking, and apply computer vision techniques such as OpenAI-Vision. THETA potentially ensures cost-effective, user-friendly teleoperation for medical, linguistic, and manufacturing applications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ecf\u6d4e\u6709\u6548\u7684\u624b\u90e8\u9065\u64cd\u4f5c\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u4e2a\u7f51\u7edc\u6444\u50cf\u5934\u8fdb\u884c\u4e09\u89d2\u6d4b\u91cf\u8ddf\u8e2a\uff0c\u5b9e\u65f6\u4f30\u8ba1\u624b\u6307\u5173\u8282\u89d2\u5ea6\uff0c\u5e76\u5229\u7528\u4f4e\u6210\u672c\u7684DexHand\u673a\u5668\u4eba\u624b\u5c55\u793a\u5176\u5e94\u7528\u3002", "motivation": "\u4f20\u7edf\u7684\u6df1\u5ea6\u6444\u50cf\u5934\u548c\u4f20\u611f\u624b\u5957\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u624b\u7684\u9065\u64cd\u4f5c\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u66f4\u7ecf\u6d4e\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u4e09\u53f0640x480\u5206\u8fa8\u7387\u7684\u7f51\u7edc\u6444\u50cf\u5934\u8fdb\u884c\u624b\u90e8\u4e09\u89d2\u6d4b\u91cf\u8ddf\u8e2a\uff0c\u7ed3\u5408DeepLabV3\u8fdb\u884c\u624b\u90e8\u5206\u5272\uff0c\u4ee5\u53ca\u4f7f\u7528\u4f18\u5316\u7684MobileNetV2 CNN\u5206\u7c7b\u5668\u8fdb\u884c\u5173\u8282\u89d2\u5ea6\u5206\u7c7b\u3002", "result": "\u6a21\u578b\u5728\u624b\u90e8\u59ff\u52bf\u5206\u7c7b\u4e0a\u53d6\u5f97\u4e8697.18%\u7684\u51c6\u786e\u7387\u300198.72%\u7684\u53ec\u56de\u7387\u3001F1\u5206\u6570\u4e3a0.9274\u548c\u7cbe\u786e\u5ea6\u4e3a0.8906\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u533b\u7597\u3001\u8bed\u8a00\u548c\u5236\u9020\u5e94\u7528\u63d0\u4f9b\u4e86\u6210\u672c\u6548\u76ca\u9ad8\u4e14\u7528\u6237\u53cb\u597d\u7684\u9065\u64cd\u4f5c\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u8ba1\u5212\u5728\u672a\u6765\u589e\u5f3a\u6570\u636e\u96c6\u7684\u591a\u6837\u6027\u548c\u6574\u5408\u8155\u90e8\u8ddf\u8e2a\u529f\u80fd\u3002"}}
{"id": "2601.07571", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.07571", "abs": "https://arxiv.org/abs/2601.07571", "authors": ["Charles Javerliat", "Guillaume Lavou\u00e9"], "title": "GPU accelerated surface-based gaze mapping for XR experiences", "comment": null, "summary": "Extended reality is a fast-growing domain for which there is an increasing need to analyze and understand user behavior. In particular, understanding human visual attention during immersive experiences is crucial for many applications. The visualization and analysis of visual attention are commonly done by building fixation density maps from eye-tracking data. Such visual attention mapping is well mastered for 3 degrees of freedom (3DoF) experiences (\\textit{i.e.}, involving 360 images or videos) but much less so for 6DoFs data, when the user can move freely in the 3D space. In that case, the visual attention information has to be mapped onto the 3D objects themselves. Some solutions exist for constructing such surface-based 6DoFs attention maps, however, they own several drawbacks: processing time, strong dependence on mesh resolution and/or texture mapping, and/or unpractical data representation for further processing. In this context, we propose a novel GPU-based algorithm that resolves the issues above while being generated in interactive time and rendered in real-time. Experiment on a challenging scene demonstrates the accuracy and robustness of our approach. To stimulate research in this area, the source code is publicly released and integrated into PLUME for ease of use in XR experiments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684GPU\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u57286\u81ea\u7531\u5ea6\u73af\u5883\u4e2d\u6784\u5efa\u89c6\u89c9\u6ce8\u610f\u529b\u5730\u56fe\u7684\u51e0\u5927\u7f3a\u9677\uff0c\u80fd\u591f\u5b9e\u73b0\u5b9e\u65f6\u6e32\u67d3\uff0c\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u7814\u7a76\u3002", "motivation": "\u968f\u7740\u6269\u5c55\u73b0\u5b9e\uff08XR\uff09\u9886\u57df\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5bf9\u7528\u6237\u884c\u4e3a\u5206\u6790\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u5c24\u5176\u662f\u5bf9\u6c89\u6d78\u5f0f\u4f53\u9a8c\u4e2d\u4eba\u7c7b\u89c6\u89c9\u6ce8\u610f\u529b\u7684\u7406\u89e3\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGPU\u7684\u65b0\u7b97\u6cd5\uff0c\u80fd\u591f\u57286DoFs\u73af\u5883\u4e0b\u9ad8\u6548\u751f\u6210\u89c6\u89c9\u6ce8\u610f\u529b\u5730\u56fe\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u4e4b\u5904\u3002", "result": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684GPU\u7b97\u6cd5\uff0c\u7528\u4e8e\u57286\u81ea\u7531\u5ea6\uff086DoFs\uff09\u73af\u5883\u4e2d\u9ad8\u6548\u6784\u5efa\u89c6\u89c9\u6ce8\u610f\u529b\u5730\u56fe\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u65f6\u95f4\u3001\u7f51\u683c\u5206\u8fa8\u7387\u548c\u7eb9\u7406\u6620\u5c04\u4f9d\u8d56\u6027\u7b49\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "conclusion": "\u8be5\u7814\u7a76\u7684\u65b9\u6cd5\u80fd\u591f\u5728\u4ea4\u4e92\u65f6\u95f4\u5185\u751f\u6210\u5e76\u5b9e\u65f6\u6e32\u67d36DoFs\u7684\u89c6\u89c9\u6ce8\u610f\u529b\u56fe\uff0c\u5e76\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2601.07813", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.07813", "abs": "https://arxiv.org/abs/2601.07813", "authors": ["Francisco Leiva", "Claudio Canales", "Michelle Valenzuela", "Javier Ruiz-del-Solar"], "title": "Data-driven control of hydraulic impact hammers under strict operational and control constraints", "comment": "21 pages, 14 figures", "summary": "This paper presents a data-driven methodology for the control of static hydraulic impact hammers, also known as rock breakers, which are commonly used in the mining industry. The task addressed in this work is that of controlling the rock-breaker so its end-effector reaches arbitrary target poses, which is required in normal operation to place the hammer on top of rocks that need to be fractured. The proposed approach considers several constraints, such as unobserved state variables due to limited sensing and the strict requirement of using a discrete control interface at the joint level. First, the proposed methodology addresses the problem of system identification to obtain an approximate dynamic model of the hydraulic arm. This is done via supervised learning, using only teleoperation data. The learned dynamic model is then exploited to obtain a controller capable of reaching target end-effector poses. For policy synthesis, both reinforcement learning (RL) and model predictive control (MPC) algorithms are utilized and contrasted. As a case study, we consider the automation of a Bobcat E10 mini-excavator arm with a hydraulic impact hammer attached as end-effector. Using this machine, both the system identification and policy synthesis stages are studied in simulation and in the real world. The best RL-based policy consistently reaches target end-effector poses with position errors below 12 cm and pitch angle errors below 0.08 rad in the real world. Considering that the impact hammer has a 4 cm diameter chisel, this level of precision is sufficient for breaking rocks. Notably, this is accomplished by relying only on approximately 68 min of teleoperation data to train and 8 min to evaluate the dynamic model, and without performing any adjustments for a successful policy Sim2Real transfer. A demonstration of policy execution in the real world can be found in https://youtu.be/e-7tDhZ4ZgA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u6765\u63a7\u5236\u6db2\u538b\u51b2\u51fb\u9524\uff0c\u89e3\u51b3\u4e86\u7cfb\u7edf\u8bc6\u522b\u4e0e\u63a7\u5236\u7b56\u7565\u8bbe\u8ba1\u7684\u95ee\u9898\uff0c\u4f7f\u7528\u5c11\u91cf\u7684\u9065\u64cd\u4f5c\u6570\u636e\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u5ca9\u77f3\u7834\u788e\u63a7\u5236\u3002", "motivation": "\u4e3a\u4e86\u5728\u91c7\u77ff\u884c\u4e1a\u4e2d\u63d0\u9ad8\u9759\u6001\u6db2\u538b\u51b2\u51fb\u9524\u7684\u63a7\u5236\u7cbe\u786e\u6027\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u6b63\u5e38\u64cd\u4f5c\u4e2d\u8fbe\u5230\u6240\u9700\u7684\u76ee\u6807\u59ff\u6001\u3002", "method": "\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u4ece\u9065\u64cd\u4f5c\u6570\u636e\u4e2d\u83b7\u5f97\u6db2\u538b\u81c2\u7684\u52a8\u6001\u6a21\u578b\uff0c\u968f\u540e\u5229\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u7b97\u6cd5\u8fdb\u884c\u63a7\u5236\u7b56\u7565\u7684\u8bbe\u8ba1\u548c\u5bf9\u6bd4\u3002", "result": "\u5728\u771f\u5b9e\u573a\u666f\u4e2d\uff0c\u4f7f\u7528\u4f18\u5316\u7684RL\u7b56\u7565\u5b9e\u73b0\u4e86\u76ee\u6807\u7aef\u6267\u884c\u5668\u59ff\u6001\u7684\u6709\u6548\u63a7\u5236\uff0c\u4f4d\u7f6e\u8bef\u5dee\u5c0f\u4e8e12\u5398\u7c73\uff0c\u4fef\u4ef0\u89d2\u8bef\u5dee\u5c0f\u4e8e0.08\u5f27\u5ea6\uff0c\u6ee1\u8db3\u5ca9\u77f3\u7834\u788e\u7684\u7cbe\u5ea6\u8981\u6c42\u3002", "conclusion": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63a7\u5236\u7b56\u7565\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u7cbe\u786e\u6027\uff0c\u80fd\u591f\u6709\u6548\u5730\u6307\u5f15\u6db2\u538b\u51b2\u51fb\u9524\u8fbe\u5230\u76ee\u6807\u59ff\u6001\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u6548\u7684\u5ca9\u77f3\u7834\u788e\u3002"}}
{"id": "2601.07576", "categories": ["cs.HC", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.07576", "abs": "https://arxiv.org/abs/2601.07576", "authors": ["Alvaro Becerra", "Ruth Cobos", "Roberto Daza"], "title": "A Multimodal Dataset of Student Oral Presentations with Sensors and Evaluation Data", "comment": "Article under review in the journal Scientific Data. GitHub repository of the dataset at: https://github.com/dataGHIA/SOPHIAS", "summary": "Oral presentation skills are a critical component of higher education, yet comprehensive datasets capturing real-world student performance across multiple modalities remain scarce. To address this gap, we present SOPHIAS (Student Oral Presentation monitoring for Holistic Insights & Analytics using Sensors), a 12-hour multimodal dataset containing recordings of 50 oral presentations (10-15-minute presentation followed by 5-15-minute Q&A) delivered by 65 undergraduate and master's students at the Universidad Autonoma de Madrid. SOPHIAS integrates eight synchronized sensor streams from high-definition webcams, ambient and webcam audio, eye-tracking glasses, smartwatch physiological sensors, and clicker, keyboard, and mouse interactions. In addition, the dataset includes slides and rubric-based evaluations from teachers, peers, and self-assessments, along with timestamped contextual annotations. The dataset captures presentations conducted in real classroom settings, preserving authentic student behaviors, interactions, and physiological responses. SOPHIAS enables the exploration of relationships between multimodal behavioral and physiological signals and presentation performance, supports the study of peer assessment, and provides a benchmark for developing automated feedback and Multimodal Learning Analytics tools. The dataset is publicly available for research through GitHub and Science Data Bank.", "AI": {"tldr": "SOPHIAS\u662f\u4e00\u4e2a\u5305\u542b50\u573a\u53e3\u5934\u5c55\u793a\u768412\u5c0f\u65f6\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u8bb0\u5f55\u4e86\u5b66\u751f\u7684\u8868\u73b0\uff0c\u7ed3\u5408\u591a\u79cd\u4f20\u611f\u5668\u6570\u636e\uff0c\u65e8\u5728\u4e3a\u9ad8\u7b49\u6559\u80b2\u7684\u53e3\u5934\u5c55\u793a\u6280\u80fd\u7814\u7a76\u63d0\u4f9b\u652f\u6301\u3002", "motivation": "\u586b\u8865\u9ad8\u7b49\u6559\u80b2\u4e2d\u53e3\u5934\u5c55\u793a\u6280\u80fd\u65b9\u9762\u7f3a\u4e4f\u771f\u5b9e\u4e16\u754c\u5b66\u751f\u8868\u73b0\u6570\u636e\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u6574\u5408\u591a\u4e2a\u9ad8\u7cbe\u5ea6\u4f20\u611f\u5668\u7684\u6570\u636e\uff0c\u521b\u5efa\u6db5\u76d6\u771f\u5b9e\u8bfe\u5802\u73af\u5883\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5e76\u5305\u62ec\u6559\u5e08\u3001\u540c\u4f34\u548c\u81ea\u6211\u8bc4\u4f30\u7684\u6807\u51c6\u5316\u8bc4\u5206\u3002", "result": "SOPHIAS\u6570\u636e\u96c6\u4e3a\u9ad8\u7b49\u6559\u80b2\u4e2d\u7684\u53e3\u5934\u5c55\u793a\u6280\u80fd\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u636e\u6765\u6e90\uff0c\u6db5\u76d6\u4e86\u5b66\u751f\u7684\u591a\u79cd\u8868\u73b0\u6307\u6807\u3002", "conclusion": "\u901a\u8fc7SOPHIAS\u6570\u636e\u96c6\uff0c\u7814\u7a76\u8005\u53ef\u4ee5\u6df1\u5165\u63a2\u8ba8\u591a\u6a21\u6001\u884c\u4e3a\u4e0e\u751f\u7406\u4fe1\u53f7\u4e0e\u6f14\u793a\u8868\u73b0\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u4e3a\u81ea\u52a8\u53cd\u9988\u548c\u591a\u6a21\u6001\u5b66\u4e60\u5206\u6790\u5de5\u5177\u7684\u5f00\u53d1\u63d0\u4f9b\u57fa\u51c6\u3002"}}
{"id": "2601.07821", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.07821", "abs": "https://arxiv.org/abs/2601.07821", "authors": ["Huanyu Li", "Kun Lei", "Sheng Zang", "Kaizhe Hu", "Yongyuan Liang", "Bo An", "Xiaoli Li", "Huazhe Xu"], "title": "Failure-Aware RL: Reliable Offline-to-Online Reinforcement Learning with Self-Recovery for Real-World Manipulation", "comment": "Project page: https://failure-aware-rl.github.io", "summary": "Post-training algorithms based on deep reinforcement learning can push the limits of robotic models for specific objectives, such as generalizability, accuracy, and robustness. However, Intervention-requiring Failures (IR Failures) (e.g., a robot spilling water or breaking fragile glass) during real-world exploration happen inevitably, hindering the practical deployment of such a paradigm. To tackle this, we introduce Failure-Aware Offline-to-Online Reinforcement Learning (FARL), a new paradigm minimizing failures during real-world reinforcement learning. We create FailureBench, a benchmark that incorporates common failure scenarios requiring human intervention, and propose an algorithm that integrates a world-model-based safety critic and a recovery policy trained offline to prevent failures during online exploration. Extensive simulation and real-world experiments demonstrate the effectiveness of FARL in significantly reducing IR Failures while improving performance and generalization during online reinforcement learning post-training. FARL reduces IR Failures by 73.1% while elevating performance by 11.3% on average during real-world RL post-training. Videos and code are available at https://failure-aware-rl.github.io.", "AI": {"tldr": "FARL\u662f\u4e00\u79cd\u65b0\u5174\u7684\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u7b97\u6cd5\uff0c\u80fd\u6709\u6548\u51cf\u5c11\u673a\u5668\u4eba\u5728\u73b0\u5b9e\u4e16\u754c\u63a2\u7d22\u4e2d\u7684\u5931\u8d25\uff0c\u540c\u65f6\u63d0\u9ad8\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5728\u73b0\u5b9e\u4e16\u754c\u63a2\u7d22\u4e2d\u4e0d\u53ef\u907f\u514d\u7684\u5e72\u9884\u9700\u6c42\u5931\u8d25\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4e86\u57fa\u4e8e\u4e16\u754c\u6a21\u578b\u7684\u5b89\u5168\u8bc4\u8bba\u5458\u548c\u79bb\u7ebf\u8bad\u7ec3\u7684\u6062\u590d\u7b56\u7565\uff0c\u4ee5\u9632\u6b62\u5728\u7ebf\u63a2\u7d22\u4e2d\u7684\u5931\u8d25\u3002", "result": "FARL\u5728\u73b0\u5b9e\u4e16\u754c\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u4e2d\u51cf\u5c11\u4e8673.1%\u7684\u5e72\u9884\u9700\u6c42\u5931\u8d25\uff0c\u5e76\u5e73\u5747\u63d0\u9ad8\u4e8611.3%\u7684\u6027\u80fd\u3002", "conclusion": "FARL\u6709\u6548\u51cf\u5c11\u4e86\u5e72\u9884\u9700\u6c42\u7684\u5931\u8d25\uff0c\u5e76\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u4e2d\u63d0\u9ad8\u4e86\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2601.07788", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.07788", "abs": "https://arxiv.org/abs/2601.07788", "authors": ["Liberty Kent", "Nilufer Tuptuk", "Ingolf Becker"], "title": "Passing the Baton: Shift Handovers within Cybersecurity Incident Response Teams", "comment": null, "summary": "Effective shift transitions are crucial for cybersecurity incident response teams, yet there is limited guidance on managing these handovers. This exploratory study aimed to develop guidelines for such transitions through the analysis of existing literature and consultation with practitioners. Two draft guidelines (A and B) were created based on existing literature and online resources. Six participants from the UK and international incident response teams, with experience in shift handovers, were interviewed about handover structure, challenges, training practices, and their views on the draft guidelines. The collected data indicate the importance of signposting, evolving handover procedures, individual differences in handover style and detail, and streamlining the handover procedure. Participants agreed the drafts included all relevant details but suggested adding a post-incident review section and a service section for outages or technical difficulties. This study establishes a foundation for enhancing transition practices in cybersecurity incident response teams.", "AI": {"tldr": "\u672c\u7814\u7a76\u5236\u5b9a\u4e86\u7f51\u7edc\u5b89\u5168\u4e8b\u4ef6\u54cd\u5e94\u56e2\u961f\u7684\u6362\u73ed\u8fc7\u6e21\u6307\u5357\uff0c\u5f3a\u8c03\u4e86\u7ed3\u6784\u3001\u6311\u6218\u548c\u57f9\u8bad\u65b9\u6cd5\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u6709\u6548\u7684\u6362\u73ed\u8fc7\u6e21\u5bf9\u7f51\u7edc\u5b89\u5168\u4e8b\u4ef6\u54cd\u5e94\u56e2\u961f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6709\u5173\u7ba1\u7406\u8fd9\u4e9b\u4ea4\u63a5\u7684\u6307\u5bfc\u6709\u9650\u3002", "method": "\u901a\u8fc7\u5206\u6790\u73b0\u6709\u6587\u732e\u548c\u4e0e\u4ece\u4e1a\u4eba\u5458\u54a8\u8be2\uff0c\u5236\u5b9a\u6709\u6548\u7684\u6362\u73ed\u8fc7\u6e21\u6307\u5357", "result": "\u53c2\u4e0e\u8005\u4e00\u81f4\u8ba4\u4e3a\u8349\u6848\u5305\u542b\u6240\u6709\u76f8\u5173\u7ec6\u8282\uff0c\u4f46\u5efa\u8bae\u589e\u52a0\u4e00\u4e2a\u4e8b\u540e\u590d\u5ba1\u90e8\u5206\u548c\u4e00\u4e2a\u7528\u4e8e\u5904\u7406\u6545\u969c\u6216\u6280\u672f\u56f0\u96be\u7684\u670d\u52a1\u90e8\u5206\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u589e\u5f3a\u7f51\u7edc\u5b89\u5168\u4e8b\u4ef6\u54cd\u5e94\u56e2\u961f\u7684\u8fc7\u6e21\u5b9e\u8df5\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
