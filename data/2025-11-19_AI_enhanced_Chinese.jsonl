{"id": "2511.13910", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.13910", "abs": "https://arxiv.org/abs/2511.13910", "authors": ["Parul Khanna", "Sameer Prabhu", "Ramin Karim", "Phillip Tretten"], "title": "Enhancing Decision Support in Construction through Industrial AI", "comment": null, "summary": "The construction industry is presently going through a transformation led by adopting digital technologies that leverage Artificial Intelligence (AI). These industrial AI solutions assist in various phases of the construction process, including planning, design, production and management. In particular, the production phase offers unique potential for the integration of such AI-based solutions. These AI-based solutions assist site managers, project engineers, coordinators and other key roles in making final decisions. To facilitate the decision-making process in the production phase of construction through a human-centric AI-based solution, it is important to understand the needs and challenges faced by the end users who interact with these AI-based solutions to enhance the effectiveness and usability of these systems. Without this understanding, the potential usage of these AI-based solutions may be limited. Hence, the purpose of this research study is to explore, identify and describe the key factors crucial for developing AI solutions in the construction industry. This study further identifies the correlation between these key factors. This was done by developing a demonstrator and collecting quantifiable feedback through a questionnaire targeting the end users, such as site managers and construction professionals. This research study will offer insights into developing and improving these industrial AI solutions, focusing on Human-System Interaction aspects to enhance decision support, usability, and overall AI solution adoption.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5efa\u7b51\u884c\u4e1a\u4e2d\u4e0e\u4eba\u5de5\u667a\u80fd\u89e3\u51b3\u65b9\u6848\u76f8\u5173\u7684\u5173\u952e\u56e0\u7d20\uff0c\u91cd\u70b9\u5728\u4e8e\u63d0\u5347\u51b3\u7b56\u652f\u6301\u548c\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u4e86\u89e3\u4e0e\u4eba\u5de5\u667a\u80fd\u89e3\u51b3\u65b9\u6848\u4e92\u52a8\u7684\u6700\u7ec8\u7528\u6237\u6240\u9762\u4e34\u7684\u9700\u6c42\u548c\u6311\u6218\uff0c\u4ee5\u63d0\u9ad8\u8fd9\u4e9b\u7cfb\u7edf\u7684\u6709\u6548\u6027\u548c\u53ef\u7528\u6027\u3002", "method": "\u901a\u8fc7\u5f00\u53d1\u793a\u8303\u5de5\u5177\u5e76\u91c7\u96c6\u9488\u5bf9\u6700\u7ec8\u7528\u6237\u7684\u95ee\u5377\u53cd\u9988\uff0c\u91cd\u70b9\u662f\u73b0\u573a\u7ecf\u7406\u548c\u5efa\u7b51\u4e13\u4e1a\u4eba\u5458\u3002", "result": "\u786e\u5b9a\u4e86\u5efa\u7b51\u884c\u4e1a\u4e2d\u5f00\u53d1\u4eba\u5de5\u667a\u80fd\u89e3\u51b3\u65b9\u6848\u7684\u5173\u952e\u56e0\u7d20\u53ca\u5176\u76f8\u4e92\u5173\u7cfb\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u5efa\u7b51\u884c\u4e1a\u7684\u4eba\u5de5\u667a\u80fd\u89e3\u51b3\u65b9\u6848\u5f00\u53d1\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u7279\u522b\u5173\u6ce8\u4eba\u673a\u4ea4\u4e92\u548c\u51b3\u7b56\u652f\u6301\u7684\u63d0\u5347\u3002"}}
{"id": "2511.13918", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.13918", "abs": "https://arxiv.org/abs/2511.13918", "authors": ["Parul Khanna", "Ravdeep Kour", "Ramin Karim"], "title": "Human-centric Maintenance Process Through Integration of AI, Speech, and AR", "comment": null, "summary": "The adoption of Augmented Reality (AR) is increasing to enhance Human-System Interaction (HSI) by creating immersive experiences that improve efficiency and safety in various industries. In industrial maintenance, traditional practices involve physical documentation and device interactions, which might disrupt the task, affect efficiency, and increase the cognitive load for the maintenance personnel. AR has the potential to support and enhance industrial maintenance processes in these aspects. Therefore, the purpose of this research is to study and explore how advanced technologies like Artificial Intelligence (AI), AR and speech processing can be integrated to support hands-free, real-time task logging and interaction in maintenance environments. This is done by developing a demonstrator for Microsoft HoloLens 2 using Unity, C#, Azure Cognitive Services, and Azure Functions, which enables speech-to-text transcription for hands-free maintenance support. Using Azures' speech recognition, the demonstrator can achieve high transcription accuracy in an AR environment, facilitating natural interactions between users and the augmented environment. The study aims to explore the potential of AR to reduce cognitive load, streamline workflows, and improve safety by enhancing HSI for maintenance personnel in high-stakes environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u5982\u4f55\u901a\u8fc7\u96c6\u6210\u4eba\u5de5\u667a\u80fd\u548c\u8bed\u97f3\u5904\u7406\u6280\u672f\uff0c\u652f\u6301\u5de5\u4e1a\u7ef4\u62a4\u4e2d\u7684\u514d\u624b\u64cd\u4f5c\u4e0e\u5b9e\u65f6\u4efb\u52a1\u8bb0\u5f55\u3002", "motivation": "\u7814\u7a76AR\u4e0eAI\u53ca\u8bed\u97f3\u5904\u7406\u6280\u672f\u7684\u6574\u5408\uff0c\u4ee5\u652f\u6301\u7ef4\u62a4\u73af\u5883\u4e2d\u7684\u514d\u624b\u64cd\u4f5c\u548c\u5b9e\u65f6\u4efb\u52a1\u8bb0\u5f55\u3002", "method": "\u901a\u8fc7\u5f00\u53d1\u7528\u4e8eMicrosoft HoloLens 2\u7684\u6f14\u793a\u7cfb\u7edf\uff0c\u5229\u7528Unity\u3001C#\u3001Azure\u8ba4\u77e5\u670d\u52a1\u548cAzure\u529f\u80fd\u5b9e\u73b0\u8bed\u97f3\u8f6c\u6587\u672c\u7684\u5b9e\u65f6\u4ea4\u4e92\u3002", "result": "\u7814\u7a76\u8bc1\u660e\uff0cAR\u73af\u5883\u4e2d\u8bed\u97f3\u8bc6\u522b\u51c6\u786e\u6027\u9ad8\uff0c\u80fd\u591f\u5b9e\u73b0\u7528\u6237\u4e0e\u589e\u5f3a\u73af\u5883\u4e4b\u95f4\u7684\u81ea\u7136\u4ea4\u4e92\u3002", "conclusion": "AR\u6280\u672f\u5728\u5de5\u4e1a\u7ef4\u62a4\u4e2d\u53ef\u4ee5\u6709\u6548\u964d\u4f4e\u8ba4\u77e5\u8d1f\u8377\uff0c\u63d0\u9ad8\u5de5\u4f5c\u6d41\u7a0b\u7684\u6548\u7387\u4e0e\u5b89\u5168\u6027\u3002"}}
{"id": "2511.13979", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.13979", "abs": "https://arxiv.org/abs/2511.13979", "authors": ["Harang Ju", "Sinan Aral"], "title": "Personality Pairing Improves Human-AI Collaboration", "comment": "28 pages, 7 figures", "summary": "Here we ask how AI agent \"personalities\" interact with human personalities, and other traits, to shape human-AI collaboration, productivity and performance. To estimate these relationships, we conducted a large-scale preregistered randomized experiment that paired 1,258 participants with AI agents that were prompted to exhibit varying levels of the Big Five personality traits. These human-AI teams produced 7,266 display ads for a real think tank, and the quality of these ads was evaluated by 1,995 independent human raters as well as in a field experiment conducted on X, which generated nearly 5 million impressions. We found, first, that personality pairing impacted teamwork quality. For example, neurotic AI improved teamwork for agreeable humans but impaired it for conscientious humans. Second, we found productivity effects of personality pairing and a \"productivity-performance trade-off\" in which certain pairings (e.g., agreeable human with neurotic AI) produced fewer ads but of higher quality. Third, personality pairing influenced ad quality and performance. For example, quality improved when open humans were paired with conscientious AI and when conscientious humans were paired with disagreeable AI. Some of these pairing effects were \"jagged\" in that they varied across text and visual tasks. For example open humans produced higher quality images but lower quality text when paired with agreeable AI. Pairing effects were also present in other human traits, like country of origin. For example, extroverted AI improved quality for Latin American workers, but degraded quality for East Asian workers. These findings demonstrate that human-AI personality alignment significantly improves collaboration, productivity, and performance and lay a foundation for future research on improving human-AI collaboration through AI personalization.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4eba\u7c7b\u4e0eAI\u4e2a\u6027\u7684\u4e92\u52a8\uff0c\u53d1\u73b0\u4e2a\u6027\u5339\u914d\u663e\u8457\u63d0\u5347\u5408\u4f5c\u6548\u7387\u548c\u5e7f\u544a\u8d28\u91cf\uff0c\u672a\u6765\u9700\u6df1\u5165\u7814\u7a76AI\u4e2a\u6027\u5316\u5bf9\u4eba-\u673a\u5408\u4f5c\u7684\u5f71\u54cd\u3002", "motivation": "\u63a2\u7a76AI\u4ee3\u7406\u7684'\u4e2a\u6027'\u4e0e\u4eba\u7c7b\u4e2a\u6027\u4e4b\u95f4\u7684\u4e92\u52a8\uff0c\u4ee5\u53ca\u5b83\u4eec\u5982\u4f55\u5f71\u54cd\u4eba\u7c7b\u4e0eAI\u7684\u534f\u4f5c\u3001\u751f\u4ea7\u529b\u548c\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u4e00\u9879\u5927\u89c4\u6a21\u7684\u968f\u673a\u5b9e\u9a8c\uff0c\u5c061258\u540d\u53c2\u4e0e\u8005\u4e0e\u4e0d\u540c\u4e2a\u6027\u7279\u5f81\u7684AI\u4ee3\u7406\u914d\u5bf9\uff0c\u8bc4\u4f30\u4e86\u56e2\u961f\u5e7f\u544a\u7684\u8d28\u91cf\u548c\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u4e2a\u6027\u5339\u914d\u5f71\u54cd\u56e2\u961f\u5408\u4f5c\u8d28\u91cf\u548c\u751f\u4ea7\u529b\u8868\u73b0\uff0c\u67d0\u4e9b\u914d\u5bf9\u7ec4\u5408\u80fd\u63d0\u9ad8\u5e7f\u544a\u8d28\u91cf\uff0c\u5e76\u663e\u793a\u51fa\u4e0d\u540c\u4efb\u52a1\u5bf9\u5339\u914d\u7684\u654f\u611f\u6027\u3002", "conclusion": "\u4eba\u7c7b\u4e0eAI\u7684\u6027\u683c\u5339\u914d\u663e\u8457\u63d0\u9ad8\u4e86\u5408\u4f5c\u3001\u751f\u4ea7\u529b\u548c\u8868\u73b0\uff0c\u4e3a\u672a\u6765\u7684\u4e2a\u6027\u5316AI\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.14009", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.14009", "abs": "https://arxiv.org/abs/2511.14009", "authors": ["Halle C. Braun", "Kushin Mukherjee", "Seth R. Gorelik", "Karen B. Schloss"], "title": "Affective Color Scales for Colormap Data Visualizations", "comment": "To be published in IEEE Transactions on Visualization and Computer Graphics", "summary": "Research on affective visualization design has shown that color is an especially powerful feature for influencing the emotional connotation of visualizations. Associations between colors and emotions are largely driven by lightness (e.g., lighter colors are associated with positive emotions, whereas darker colors are associated with negative emotions). Designing visualizations to have all light or all dark colors to convey particular emotions may work well for visualizations in which colors represent categories and spatial channels encode data values. However, this approach poses a problem for visualizations that use color to represent spatial patterns in data (e.g., colormap data visualizations) because lightness contrast is needed to reveal fine details in spatial structure. In this study, we found it is possible to design colormaps that have strong lightness contrast to support spatial vision while communicating clear affective connotation. We also found that affective connotation depended not only on the color scales used to construct the colormaps, but also the frequency with which colors appeared in the map, as determined by the underlying dataset (data-dependence hypothesis). These results emphasize the importance of data-aware design, which accounts for not only the design features that encode data (e.g., colors, shapes, textures), but also how those design features are instantiated in a visualization, given the properties of the data.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u60c5\u611f\u53ef\u89c6\u5316\u8bbe\u8ba1\u4e2d\u989c\u8272\u7684\u91cd\u8981\u6027\uff0c\u63d0\u51fa\u8bbe\u8ba1\u5e94\u8003\u8651\u8272\u5f69\u4e0e\u6570\u636e\u7279\u6027\u7684\u7ed3\u5408\uff0c\u4ee5\u5b9e\u73b0\u7a7a\u95f4\u89c6\u89c9\u4e0e\u60c5\u611f\u4f20\u8fbe\u7684\u5e73\u8861\u3002", "motivation": "\u63a2\u8ba8\u5982\u4f55\u5728\u6570\u636e\u53ef\u89c6\u5316\u4e2d\u5e73\u8861\u60c5\u611f\u4f20\u8fbe\u4e0e\u7a7a\u95f4\u6a21\u5f0f\u7684\u8868\u73b0\uff0c\u89e3\u51b3\u8bbe\u8ba1\u4e2d\u7684\u8272\u5f69\u4e0e\u5149\u4eae\u5ea6\u5bf9\u6bd4\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u5177\u6709\u5f3a\u5149\u4eae\u5bf9\u6bd4\u5ea6\u7684\u989c\u8272\u6620\u5c04\uff0c\u5206\u6790\u5176\u5728\u7a7a\u95f4\u89c6\u89c9\u548c\u60c5\u611f\u4f20\u8fbe\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u80fd\u591f\u8bbe\u8ba1\u51fa\u65e2\u652f\u6301\u7a7a\u95f4\u89c6\u89c9\u53c8\u5177\u60c5\u611f\u8868\u8fbe\u7684\u989c\u8272\u6620\u5c04\uff0c\u4e14\u60c5\u611f\u542b\u4e49\u4e0e\u989c\u8272\u51fa\u73b0\u7684\u9891\u7387\u53ca\u5176\u6570\u636e\u4f9d\u8d56\u6027\u76f8\u5173\u3002", "conclusion": "\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u6570\u636e\u611f\u77e5\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\uff0c\u8ba4\u4e3a\u5728\u8bbe\u8ba1\u53ef\u89c6\u5316\u65f6\u5e94\u8003\u8651\u6570\u636e\u7279\u6027\u4e0e\u8bbe\u8ba1\u5143\u7d20\u7684\u7ed3\u5408\u3002"}}
{"id": "2511.13961", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13961", "abs": "https://arxiv.org/abs/2511.13961", "authors": ["Jiarui Li", "Alessandro Zanardi", "Runyu Zhang", "Gioele Zardini"], "title": "FICO: Finite-Horizon Closed-Loop Factorization for Unified Multi-Agent Path Finding", "comment": null, "summary": "Multi-Agent Path Finding is a fundamental problem in robotics and AI, yet most existing formulations treat planning and execution separately and address variants of the problem in an ad hoc manner. This paper presents a system-level framework for MAPF that integrates planning and execution, generalizes across variants, and explicitly models uncertainties. At its core is the MAPF system, a formal model that casts MAPF as a control design problem encompassing classical and uncertainty-aware formulations. To solve it, we introduce Finite-Horizon Closed-Loop Factorization (FICO), a factorization-based algorithm inspired by receding-horizon control that exploits compositional structure for efficient closed-loop operation. FICO enables real-time responses -- commencing execution within milliseconds -- while scaling to thousands of agents and adapting seamlessly to execution-time uncertainties. Extensive case studies demonstrate that it reduces computation time by up to two orders of magnitude compared with open-loop baselines, while delivering significantly higher throughput under stochastic delays and agent arrivals. These results establish a principled foundation for analyzing and advancing MAPF through system-level modeling, factorization, and closed-loop design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u7ea7\u7684\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u89c4\u5212\u4e0e\u6267\u884c\uff0c\u5904\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u51faFICO\u7b97\u6cd5\uff0c\u63d0\u5347\u4e86\u5b9e\u65f6\u6027\u4e0e\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u5927\u591a\u5c06\u89c4\u5212\u548c\u6267\u884c\u5206\u5f00\u5904\u7406\uff0c\u7f3a\u4e4f\u5bf9\u4e0d\u786e\u5b9a\u6027\u7684\u660e\u786e\u5efa\u6a21\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6709\u9650\u89c6\u754c\u95ed\u73af\u56e0\u5f0f\u5206\u89e3\uff08FICO\uff09\u7684\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u57fa\u4e8e\u56e0\u5f0f\u5206\u89e3\uff0c\u7075\u611f\u6e90\u81ea\u524d\u89c6\u63a7\u5236\uff0c\u65e8\u5728\u9ad8\u6548\u5730\u8fdb\u884c\u95ed\u73af\u64cd\u4f5c\u3002", "result": "FICO\u7b97\u6cd5\u5728\u5b9e\u65f6\u54cd\u5e94\u80fd\u529b\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u80fd\u5728\u6beb\u79d2\u7ea7\u542f\u52a8\u6267\u884c\uff0c\u5e76\u5728\u9762\u4e34\u6267\u884c\u65f6\u7684\u4e0d\u786e\u5b9a\u6027\u65f6\u5177\u6709\u826f\u597d\u7684\u81ea\u9002\u5e94\u80fd\u529b\uff0c\u8ba1\u7b97\u65f6\u95f4\u6bd4\u5f00\u653e\u5faa\u73af\u57fa\u7ebf\u51cf\u5c11\u4e86\u4e24\u4e2a\u6570\u91cf\u7ea7\uff0c\u5e76\u5728\u968f\u673a\u5ef6\u8fdf\u548c\u4ee3\u7406\u5230\u8fbe\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u541e\u5410\u91cf\u3002", "conclusion": "\u672c\u6587\u4e3a\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\uff08MAPF\uff09\u5efa\u7acb\u4e86\u4e00\u4e2a\u7cfb\u7edf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u5efa\u6a21\u3001\u56e0\u5f0f\u5206\u89e3\u548c\u95ed\u73af\u8bbe\u8ba1\u4e3a\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u4f9b\u4e86\u539f\u5219\u57fa\u7840\u3002"}}
{"id": "2511.14013", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14013", "abs": "https://arxiv.org/abs/2511.14013", "authors": ["Bifei Mao", "Lanqing Hong"], "title": "Developing a Grounded View of AI", "comment": null, "summary": "As a capability coming from computation, how does AI differ fundamentally from the capabilities delivered by rule-based software program? The paper examines the behavior of artificial intelligence (AI) from engineering points of view to clarify its nature and limits. The paper argues that the rationality underlying humanity's impulse to pursue, articulate, and adhere to rules deserves to be valued and preserved. Identifying where rule-based practical rationality ends is the beginning of making it aware until action. Although the rules of AI behaviors are still hidden or only weakly observable, the paper has proposed a methodology to make a sense of discrimination possible and practical to identify the distinctions of the behavior of AI models with three types of decisions. It is a prerequisite for human responsibilities with alternative possibilities, considering how and when to use AI. It would be a solid start for people to ensure AI system soundness for the well-being of humans, society, and the environment.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86AI\u4e0e\u57fa\u4e8e\u89c4\u5219\u7684\u8f6f\u4ef6\u4e4b\u95f4\u7684\u57fa\u672c\u533a\u522b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\u8bba\u4ee5\u7406\u89e3AI\u7684\u51b3\u7b56\u884c\u4e3a\uff0c\u5e76\u5f3a\u8c03\u4eba\u7c7b\u5728\u4f7f\u7528AI\u65f6\u7684\u8d23\u4efb\u3002", "motivation": "\u63a2\u7d22AI\u4e0e\u57fa\u4e8e\u89c4\u5219\u7684\u8f6f\u4ef6\u7a0b\u5e8f\u7684\u6839\u672c\u533a\u522b\uff0c\u5f3a\u8c03\u4eba\u7c7b\u5728\u4f7f\u7528AI\u65f6\u9700\u8981\u8d1f\u8d77\u8d23\u4efb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\u8bba\uff0c\u65e8\u5728\u8fa8\u522bAI\u6a21\u578b\u884c\u4e3a\u7684\u4e0d\u540c\u51b3\u7b56\u7c7b\u578b\u3002", "result": "\u901a\u8fc7\u8bc6\u522b\u89c4\u5219\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u5bf9AI\u884c\u4e3a\u7684\u7406\u89e3\uff0c\u5e76\u4e3a\u4eba\u7c7b\u672a\u6765\u4e0eAI\u7684\u4e92\u52a8\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "conclusion": "\u4e3a\u4e86\u786e\u4fddAI\u7cfb\u7edf\u5bf9\u4eba\u7c7b\u3001\u793e\u4f1a\u548c\u73af\u5883\u7684\u826f\u597d\u5f71\u54cd\uff0c\u9700\u8981\u7406\u89e3AI\u884c\u4e3a\u53ca\u5176\u4e0e\u57fa\u4e8e\u89c4\u5219\u7684\u8f6f\u4ef6\u7a0b\u5e8f\u4e4b\u95f4\u7684\u57fa\u672c\u533a\u522b\u3002"}}
{"id": "2511.13985", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13985", "abs": "https://arxiv.org/abs/2511.13985", "authors": ["Jan Quenzel", "Sven Behnke"], "title": "LIO-MARS: Non-uniform Continuous-time Trajectories for Real-time LiDAR-Inertial-Odometry", "comment": "submitted to T-RO, 19 pages", "summary": "Autonomous robotic systems heavily rely on environment knowledge to safely navigate. For search & rescue, a flying robot requires robust real-time perception, enabled by complementary sensors. IMU data constrains acceleration and rotation, whereas LiDAR measures accurate distances around the robot. Building upon the LiDAR odometry MARS, our LiDAR-inertial odometry (LIO) jointly aligns multi-resolution surfel maps with a Gaussian mixture model (GMM) using a continuous-time B-spline trajectory. Our new scan window uses non-uniform temporal knot placement to ensure continuity over the whole trajectory without additional scan delay. Moreover, we accelerate essential covariance and GMM computations with Kronecker sums and products by a factor of 3.3. An unscented transform de-skews surfels, while a splitting into intra-scan segments facilitates motion compensation during spline optimization. Complementary soft constraints on relative poses and preintegrated IMU pseudo-measurements further improve robustness and accuracy. Extensive evaluation showcases the state-of-the-art quality of our LIO-MARS w.r.t. recent LIO systems on various handheld, ground and aerial vehicle-based datasets.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5148\u8fdb\u7684LiDAR\u60ef\u6027\u6d4b\u7a0b\uff08LIO-MARS\uff09\uff0c\u901a\u8fc7\u7ed3\u5408IMU\u548cLiDAR\u4f20\u611f\u5668\u6570\u636e\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u673a\u5668\u4eba\u5bfc\u822a\uff0c\u663e\u793a\u51fa\u5728\u591a\u4e2a\u6570\u636e\u96c6\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u5b89\u5168\u5bfc\u822a\u8fc7\u7a0b\u4e2d\u4f9d\u8d56\u73af\u5883\u77e5\u8bc6\uff0c\u5c24\u5176\u662f\u5728\u641c\u7d22\u548c\u6551\u63f4\u4efb\u52a1\u4e2d\uff0c\u98de\u884c\u673a\u5668\u4eba\u9700\u8981\u5f3a\u5927\u7684\u5b9e\u65f6\u611f\u77e5\uff0c\u7ed3\u5408IMU\u548cLiDAR\u4f20\u611f\u5668\u7684\u6570\u636e\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684LiDAR\u60ef\u6027\u6d4b\u7a0b\uff08LIO\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fde\u7eed\u65f6\u95f4B\u6837\u6761\u8f68\u8ff9\uff0c\u7ed3\u5408\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08GMM\uff09\u5bf9\u591a\u5206\u8fa8\u7387\u8868\u9762\u56fe\u8fdb\u884c\u5bf9\u9f50\uff0c\u540c\u65f6\u52a0\u901f\u4e86\u91cd\u8981\u7684\u534f\u65b9\u5dee\u548cGMM\u8ba1\u7b97\u3002", "result": "\u901a\u8fc7\u5f15\u5165\u975e\u5747\u5300\u65f6\u95f4\u8282\u70b9\u653e\u7f6e\u7684\u626b\u63cf\u7a97\u53e3\uff0c\u786e\u4fdd\u8f68\u8ff9\u8fde\u7eed\u6027\uff0c\u5e76\u4f7f\u7528\u4e0d\u53d8\u53d8\u6362\u53bb\u9664\u8bef\u5dee\uff0c\u540c\u65f6\u91c7\u7528\u8f6f\u7ea6\u675f\u63d0\u5347\u4e86\u9c81\u68d2\u6027\u548c\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86LIO-MARS\u5728\u591a\u79cd\u624b\u6301\u3001\u5730\u9762\u548c\u7a7a\u4e2d\u8f7d\u4f53\u6570\u636e\u96c6\u4e0a\uff0c\u4e0e\u6700\u65b0\u7684LIO\u7cfb\u7edf\u76f8\u6bd4\uff0c\u5177\u6709\u5148\u8fdb\u7684\u8d28\u91cf\u3002"}}
{"id": "2511.14118", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.14118", "abs": "https://arxiv.org/abs/2511.14118", "authors": ["Md Mosharaf Hossan", "Rifat Ara Tasnim", "Farjana Z Eishita"], "title": "Gamified Virtual Reality Exposure Therapy for Mysophobia: Evaluating the Efficacy of a Simulated Sneeze Intervention", "comment": null, "summary": "Mysophobia, or the fear of germs, is a prevalent anxiety disorder that significantly impacts daily life. This study investigates the potential of a gamified virtual reality (VR) intervention to simulate contamination-related scenarios and assess their emotional and psychological effects. A VR game based sneeze simulation was developed to evaluate its influence on participants' emotional states. Seven participants completed two versions of the game: a baseline version and an experimental version featuring the sneeze simulation. Emotional responses were measured using the Positive and Negative Affect Schedule (PANAS) and State-Trait Anxiety Inventory - State (STAI-S) questionnaires. The results revealed slight increases in negative affect and anxiety levels during the sneeze simulation. Also, a reduction in positive affect was revealed. However, these differences were not statistically significant (p > 0.05). This is likely due to small sample sizes, a lack of grossness in the simulation, or participants not being clinically mysophobes. This exploratory study highlights the potential of VR-based interventions for understanding and addressing contamination-related anxieties. It provides a foundation for future research with larger and more diverse participant pools.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u57fa\u4e8e\u865a\u62df\u73b0\u5b9e\u7684\u6e38\u620f\u5bf9\u5f3a\u8feb\u75c7\u76f8\u5173\u60c5\u611f\u548c\u5fc3\u7406\u5f71\u54cd\u7684\u6f5c\u529b\uff0c\u867d\u7136\u7ed3\u679c\u672a\u8fbe\u5230\u7edf\u8ba1\u5b66\u663e\u8457\u6027\uff0c\u4f46\u4e3a\u672a\u6765\u66f4\u5927\u6837\u672c\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "motivation": "\u8c03\u67e5\u6e38\u620f\u5316\u865a\u62df\u73b0\u5b9e\u5e72\u9884\u5bf9\u6a21\u62df\u6c61\u67d3\u76f8\u5173\u573a\u666f\u7684\u60c5\u611f\u548c\u5fc3\u7406\u5f71\u54cd\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u5f3a\u8feb\u75c7\u7684\u8868\u73b0\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u6b3e\u57fa\u4e8e\u865a\u62df\u73b0\u5b9e\u7684\u6e38\u620f\uff0c\u901a\u8fc7\u55b7\u568f\u6a21\u62df\u6765\u8bc4\u4f30\u5176\u5bf9\u53c2\u4e0e\u8005\u60c5\u7eea\u72b6\u6001\u7684\u5f71\u54cd\uff0c\u53c2\u4e0e\u8005\u5b8c\u6210\u4e86\u4e24\u4e2a\u7248\u672c\u7684\u6e38\u620f\uff1a\u57fa\u7ebf\u7248\u672c\u548c\u5e26\u55b7\u568f\u6a21\u62df\u7684\u5b9e\u9a8c\u7248\u672c\u3002", "result": "\u5728\u55b7\u568f\u6a21\u62df\u671f\u95f4\uff0c\u53c2\u4e0e\u8005\u7684\u8d1f\u9762\u60c5\u611f\u548c\u7126\u8651\u6c34\u5e73\u7565\u6709\u589e\u52a0\uff0c\u6b63\u9762\u60c5\u611f\u51cf\u5c11\uff0c\u4f46\u8fd9\u4e9b\u5dee\u5f02\u6ca1\u6709\u7edf\u8ba1\u5b66\u610f\u4e49\uff08p > 0.05\uff09\u3002\u53ef\u80fd\u662f\u7531\u4e8e\u6837\u672c\u91cf\u5c0f\u3001\u6a21\u62df\u7684\u6076\u5fc3\u7a0b\u5ea6\u4e0d\u8db3\u6216\u53c2\u4e0e\u8005\u4e0d\u662f\u4e34\u5e8a\u7684\u5f3a\u8feb\u75c7\u60a3\u8005\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u5f3a\u8c03\u4e86\u57fa\u4e8e\u865a\u62df\u73b0\u5b9e\u7684\u5e72\u9884\u63aa\u65bd\u5728\u7406\u89e3\u548c\u5e94\u5bf9\u4e0e\u6c61\u67d3\u76f8\u5173\u7684\u7126\u8651\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u672a\u6765\u8fdb\u884c\u66f4\u5927\u89c4\u6a21\u548c\u66f4\u5177\u591a\u6837\u6027\u7684\u53c2\u4e0e\u8005\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.14004", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.14004", "abs": "https://arxiv.org/abs/2511.14004", "authors": ["Taijing Chen", "Sateesh Kumar", "Junhong Xu", "George Pavlakos", "J oydeep Biswas", "Roberto Mart\u00edn-Mart\u00edn"], "title": "Searching in Space and Time: Unified Memory-Action Loops for Open-World Object Retrieval", "comment": "This paper is under review at ICRA", "summary": "Service robots must retrieve objects in dynamic, open-world settings where requests may reference attributes (\"the red mug\"), spatial context (\"the mug on the table\"), or past states (\"the mug that was here yesterday\"). Existing approaches capture only parts of this problem: scene graphs capture spatial relations but ignore temporal grounding, temporal reasoning methods model dynamics but do not support embodied interaction, and dynamic scene graphs handle both but remain closed-world with fixed vocabularies. We present STAR (SpatioTemporal Active Retrieval), a framework that unifies memory queries and embodied actions within a single decision loop. STAR leverages non-parametric long-term memory and a working memory to support efficient recall, and uses a vision-language model to select either temporal or spatial actions at each step. We introduce STARBench, a benchmark of spatiotemporal object search tasks across simulated and real environments. Experiments in STARBench and on a Tiago robot show that STAR consistently outperforms scene-graph and memory-only baselines, demonstrating the benefits of treating search in time and search in space as a unified problem.", "AI": {"tldr": "STAR\u6846\u67b6\u901a\u8fc7\u6574\u5408\u65f6\u7a7a\u4fe1\u606f\uff0c\u63d0\u5347\u4e86\u52a8\u6001\u5f00\u653e\u4e16\u754c\u4e2d\u670d\u52a1\u673a\u5668\u4eba\u5bf9\u7269\u4f53\u68c0\u7d22\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u5904\u7406\u52a8\u6001\u5f00\u653e\u4e16\u754c\u60c5\u5883\u4e0b\u7684\u7269\u4f53\u68c0\u7d22\uff0c\u7f3a\u4e4f\u6574\u5408\u65f6\u7a7a\u4fe1\u606f\u7684\u6709\u6548\u6846\u67b6\u3002", "method": "STAR\u6846\u67b6\u7ed3\u5408\u4e86\u975e\u53c2\u6570\u957f\u671f\u8bb0\u5fc6\u548c\u5de5\u4f5c\u8bb0\u5fc6\uff0c\u4ee5\u652f\u6301\u9ad8\u6548\u56de\u5fc6\uff0c\u5e76\u4f7f\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u6bcf\u4e2a\u6b65\u9aa4\u9009\u62e9\u65f6\u7a7a\u884c\u52a8\u3002", "result": "STAR\u5728\u591a\u79cd\u5b9e\u9a8c\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u573a\u666f\u56fe\u548c\u7eaf\u8bb0\u5fc6\u57fa\u7ebf\u3002", "conclusion": "STAR\u6846\u67b6\u5728\u65f6\u7a7a\u7269\u4f53\u641c\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u8bc1\u660e\u4e86\u5c06\u65f6\u95f4\u641c\u7d22\u548c\u7a7a\u95f4\u641c\u7d22\u89c6\u4e3a\u7edf\u4e00\u95ee\u9898\u7684\u4f18\u52bf\u3002"}}
{"id": "2511.14164", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.14164", "abs": "https://arxiv.org/abs/2511.14164", "authors": ["Yibo Meng", "Xiaolan Ding", "Lyumanshan Ye", "Zhiming Liu", "Yan Guan"], "title": "Final Happiness: What Intelligent User Interfaces Can Do for the lonely Dying", "comment": null, "summary": "This study explores the design of Intelligent User Interfaces (IUIs) to address the profound existential loneliness of terminally ill individuals. While Human-Computer Interaction (HCI) has made inroads in \"Thanatechnology,\" current research often focuses on practical aspects like digital legacy management, overlooking the subjective, existential needs of those facing death in isolation. To address this gap, we conducted in-depth qualitative interviews with 14 lonely, terminally ill individuals. Our core contributions are: (1) An empirically-grounded model articulating the complex psychological, practical, social, and spiritual needs of this group; (2) The \"Three Pillars, Twelve Principles\" framework for designing IUIs as \"Existential Companions\"; and (3) A critical design directive derived from user evaluations: technology in this context should aim for transcendence over simulation. The findings suggest that IUIs should create experiences that augment or surpass human capabilities, rather than attempting to simulate basic human connections, which can paradoxically deepen loneliness. This research provides a clear, user-centered path for designing technology that serves not as a \"tool for dying,\" but as a \"partner for living fully until the end\".", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u8bbe\u8ba1\u667a\u80fd\u7528\u6237\u754c\u9762\u4ee5\u5e94\u5bf9\u4e34\u7ec8\u5b64\u72ec\u611f\uff0c\u5f3a\u8c03\u8bbe\u8ba1\u5e94\u5173\u6ce8\u7528\u6237\u7684\u5fc3\u7406\u548c\u5b58\u5728\u9700\u6c42\uff0c\u5efa\u8baeIUIs\u5e94\u521b\u9020\u8d85\u8d8a\u4eba\u7c7b\u80fd\u529b\u7684\u4f53\u9a8c\u3002", "motivation": "\u65e8\u5728\u586b\u8865\u73b0\u6709\u7814\u7a76\u4e2d\u5bf9\u4e8e\u9762\u5bf9\u6b7b\u4ea1\u65f6\u5b64\u72ec\u4e2a\u4f53\u7684\u4e3b\u89c2\u5b58\u5728\u9700\u6c42\u7684\u5173\u6ce8\u7f3a\u5931\u3002", "method": "\u901a\u8fc7\u5bf914\u540d\u5b64\u72ec\u7684\u4e34\u7ec8\u75c5\u60a3\u8005\u8fdb\u884c\u6df1\u5165\u7684\u5b9a\u6027\u8bbf\u8c08\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ecf\u9a8c\u57fa\u7840\u7684\u6a21\u578b\uff0c\u9610\u660e\u4e86\u8be5\u7fa4\u4f53\u590d\u6742\u7684\u5fc3\u7406\u3001\u5b9e\u8df5\u3001\u793e\u4f1a\u548c\u7cbe\u795e\u9700\u6c42\uff0c\u5e76\u63d0\u51fa\u4e86\u201c\u4e09\u7ea7\u652f\u67f1\uff0c\u5341\u4e8c\u539f\u5219\u201d\u7684\u6846\u67b6\uff0c\u4ee5\u53ca\u4e00\u4e2a\u8bbe\u8ba1\u6307\u4ee4\uff0c\u5f3a\u8c03\u6280\u672f\u5e94\u8ffd\u6c42\u8d85\u8d8a\u800c\u975e\u6a21\u62df\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u667a\u80fd\u7528\u6237\u754c\u9762\uff08IUIs\uff09\u5e94\u5f53\u521b\u9020\u589e\u5f3a\u6216\u8d85\u8d8a\u4eba\u7c7b\u80fd\u529b\u7684\u4f53\u9a8c\uff0c\u800c\u975e\u4ec5\u4ec5\u6a21\u62df\u4eba\u9645\u8054\u7cfb\uff0c\u4ee5\u907f\u514d\u52a0\u6df1\u5b64\u72ec\u611f\u3002"}}
{"id": "2511.14024", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.14024", "abs": "https://arxiv.org/abs/2511.14024", "authors": ["Jaskirat Singh", "Rohan Chandra"], "title": "FACA: Fair and Agile Multi-Robot Collision Avoidance in Constrained Environments with Dynamic Priorities", "comment": null, "summary": "Multi-robot systems are increasingly being used for critical applications such as rescuing injured people, delivering food and medicines, and monitoring key areas. These applications usually involve navigating at high speeds through constrained spaces such as small gaps. Navigating such constrained spaces becomes particularly challenging when the space is crowded with multiple heterogeneous agents all of which have urgent priorities. What makes the problem even harder is that during an active response situation, roles and priorities can quickly change on a dime without informing the other agents. In order to complete missions in such environments, robots must not only be safe, but also agile, able to dodge and change course at a moment's notice. In this paper, we propose FACA, a fair and agile collision avoidance approach where robots coordinate their tasks by talking to each other via natural language (just as people do). In FACA, robots balance safety with agility via a novel artificial potential field algorithm that creates an automatic ``roundabout'' effect whenever a conflict arises. Our experiments show that FACA achieves a improvement in efficiency, completing missions more than 3.5X faster than baselines with a time reduction of over 70% while maintaining robust safety margins.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFACA\u7684\u591a\u673a\u5668\u4eba\u534f\u8c03\u4efb\u52a1\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u6d41\u548c\u65b0\u9896\u7684\u4eba\u5de5\u52bf\u573a\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u5b8c\u6210\u6548\u7387\u5e76\u786e\u4fdd\u5b89\u5168\u3002", "motivation": "\u968f\u7740\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u5173\u952e\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u589e\u52a0\uff0c\u5728\u62e5\u6324\u548c\u7d27\u6025\u7684\u73af\u5883\u4e2d\u5b89\u5168\u4e14\u7075\u6d3b\u5730\u5bfc\u822a\u6210\u4e3a\u5de8\u5927\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u7684\u673a\u5668\u4eba\u534f\u8c03\u4efb\u52a1\u7684\u65b9\u6cd5FACA\uff0c\u7ed3\u5408\u4e86\u4e00\u79cd\u65b0\u7684\u4eba\u5de5\u52bf\u573a\u7b97\u6cd5\uff0c\u521b\u5efa\u4e86\u81ea\u52a8\u7684\u201c\u73af\u5f62\u4ea4\u53c9\u53e3\u201d\u6548\u679c\u4ee5\u5904\u7406\u51b2\u7a81\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFACA\u80fd\u5b9e\u73b0\u8d85\u8fc73.5\u500d\u7684\u6548\u7387\u63d0\u5347\u548c\u8d85\u8fc770%\u7684\u65f6\u95f4\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u6301\u7a33\u5065\u7684\u5b89\u5168\u6c34\u5e73\u3002", "conclusion": "FACA\u65b9\u6cd5\u5728\u6548\u7387\u548c\u5b89\u5168\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u5b8c\u6210\u901f\u5ea6\u3002"}}
{"id": "2511.14174", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.14174", "abs": "https://arxiv.org/abs/2511.14174", "authors": ["Yibo Meng", "Rong Fu", "Lyumanshan Ye", "Zhiming Liu", "Zhixin Cai", "Xiaolan Ding", "Yan Guan"], "title": "A Longitudinal Study on the Attitudes of Gay Men in Beijing Towards Gay Social Media Platforms: Lonely Souls in the Digital Concrete Jungle", "comment": null, "summary": "Over the past decade, specialized social networking applications have become a cornerstone of life for many gay men in China. This paper employs a longitudinal mixed-methods approach to investigate how Chinese men who have sex with men (MSM) have shifted their attitudes toward these platforms between approximately 2013 and 2023. Drawing on archival analysis of online discourses, a quantitative survey of 412 participants, and in-depth semi-structured interviews with 32 participants, we trace the complex trajectory of this evolution. Our findings reveal a clear pattern: from the initial embrace of these applications as revolutionary tools for community building and identity affirmation (2014--2017), to a period of growing ambivalence and critique centered on commercialization, ``hookup culture,'' and multiple forms of discrimination (2017--2020), and finally to the present era (2020--2023), characterized by pragmatic, fragmented, yet simultaneously critical and reconstructive uses. Today, users strategically employ a repertoire of applications -- including global platforms (e.g., Grindr and Tinder), domestic mainstream platforms (e.g., Blued), and niche alternatives (e.g., Aloha) -- to fulfill differentiated needs. We develop a detailed temporal framework to capture this attitudinal evolution and discuss its design implications for creating more supportive, secure, and community-oriented digital environments for marginalized groups.", "AI": {"tldr": "\u672c\u7814\u7a76\u8c03\u67e5\u4e862013\u81f32023\u5e74\u95f4\uff0c\u4e2d\u56fd\u7537\u7537\u6027\u63a5\u89e6\u8005\u5bf9\u793e\u4ea4\u5e94\u7528\u6001\u5ea6\u7684\u53d8\u5316\uff0c\u53d1\u73b0\u5176\u7ecf\u5386\u4e86\u652f\u6301\u3001\u6279\u8bc4\u5230\u7075\u6d3b\u4f7f\u7528\u7684\u6f14\u53d8\uff0c\u6307\u51fa\u672a\u6765\u6570\u5b57\u73af\u5883\u8bbe\u8ba1\u7684\u542f\u793a\u3002", "motivation": "\u63a2\u8ba82013\u81f32023\u5e74\u95f4\uff0c\u4e2d\u56fd\u7537\u7537\u6027\u63a5\u89e6\u8005\uff08MSM\uff09\u5bf9\u4e13\u95e8\u793e\u4ea4\u7f51\u7edc\u5e94\u7528\u6001\u5ea6\u7684\u53d8\u5316\u3002", "method": "\u91c7\u7528\u7eb5\u5411\u6df7\u5408\u65b9\u6cd5\uff0c\u5305\u62ec\u5728\u7ebf\u8bdd\u8bed\u6863\u6848\u5206\u6790\u3001\u5bf9412\u540d\u53c2\u4e0e\u8005\u7684\u5b9a\u91cf\u8c03\u67e5\u548c\u5bf932\u540d\u53c2\u4e0e\u8005\u7684\u6df1\u5165\u534a\u7ed3\u6784\u8bbf\u8c08\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u7528\u6237\u7684\u6001\u5ea6\u7ecf\u5386\u4e86\u4e09\u4e2a\u9636\u6bb5\uff0c\u4ece\u6700\u521d\u7684\u70ed\u60c5\u652f\u6301\uff0c\u7ecf\u8fc7\u9010\u6e10\u7684\u77db\u76fe\u4e0e\u6279\u8bc4\uff0c\u6700\u7ec8\u53d1\u5c55\u5230\u7075\u6d3b\u800c\u6279\u5224\u6027\u7684\u4f7f\u7528\u65b9\u5f0f\u3002", "conclusion": "\u7528\u6237\u73b0\u5728\u4ee5\u6218\u7565\u6027\u65b9\u5f0f\u4f7f\u7528\u591a\u6837\u5316\u7684\u793e\u4ea4\u5e94\u7528\u7a0b\u5e8f\uff0c\u4ee5\u6ee1\u8db3\u4e0d\u540c\u7684\u9700\u6c42\uff0c\u5e76\u63a8\u52a8\u4e86\u66f4\u52a0\u652f\u6301\u6027\u548c\u793e\u533a\u5bfc\u5411\u7684\u6570\u5b57\u73af\u5883\u7684\u8bbe\u8ba1\u601d\u8003\u3002"}}
{"id": "2511.14037", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.14037", "abs": "https://arxiv.org/abs/2511.14037", "authors": ["Hesam Mojtahedi", "Reza Akhavian"], "title": "BIM-Discrepancy-Driven Active Sensing for Risk-Aware UAV-UGV Navigation", "comment": null, "summary": "This paper presents a BIM-discrepancy-driven active sensing framework for cooperative navigation between unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) in dynamic construction environments. Traditional navigation approaches rely on static Building Information Modeling (BIM) priors or limited onboard perception. In contrast, our framework continuously fuses real-time LiDAR data from aerial and ground robots with BIM priors to maintain an evolving 2D occupancy map. We quantify navigation safety through a unified corridor-risk metric integrating occupancy uncertainty, BIM-map discrepancy, and clearance. When risk exceeds safety thresholds, the UAV autonomously re-scans affected regions to reduce uncertainty and enable safe replanning. Validation in PX4-Gazebo simulation with Robotec GPU LiDAR demonstrates that risk-triggered re-scanning reduces mean corridor risk by 58% and map entropy by 43% compared to static BIM navigation, while maintaining clearance margins above 0.4 m. Compared to frontier-based exploration, our approach achieves similar uncertainty reduction in half the mission time. These results demonstrate that integrating BIM priors with risk-adaptive aerial sensing enables scalable, uncertainty-aware autonomy for construction robotics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5b9e\u65f6\u6570\u636e\u4e0eBIM\u6a21\u578b\uff0c\u63d0\u5347\u65e0\u4eba\u673a\u548c\u65e0\u4eba\u5730\u9762\u8f66\u8f86\u5728\u52a8\u6001\u65bd\u5de5\u73af\u5883\u4e2d\u7684\u534f\u4f5c\u5bfc\u822a\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5bfc\u822a\u5b89\u5168\u6027\u5e76\u51cf\u5c11\u4e86\u4efb\u52a1\u65f6\u95f4\u3002", "motivation": "\u9488\u5bf9\u4f20\u7edf\u5bfc\u822a\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001BIM\u5148\u9a8c\u6216\u6709\u9650\u673a\u8f7d\u611f\u77e5\u7684\u95ee\u9898\uff0c\u65e8\u5728\u63d0\u9ad8\u65e0\u4eba\u673a\u548c\u65e0\u4eba\u5730\u9762\u8f66\u8f86\u5728\u52a8\u6001\u65bd\u5de5\u73af\u5883\u4e2d\u7684\u534f\u4f5c\u5bfc\u822a\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cdBIM\u5dee\u5f02\u9a71\u52a8\u7684\u4e3b\u52a8\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u65f6\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\u4e0eBIM\u5148\u9a8c\u77e5\u8bc6\u878d\u5408\uff0c\u7ef4\u62a4\u4e00\u4e2a\u4e0d\u65ad\u6f14\u53d8\u7684\u4e8c\u7ef4\u5360\u7528\u56fe\uff0c\u5e76\u91cf\u5316\u5bfc\u822a\u5b89\u5168\u6027\u3002", "result": "\u5728PX4-Gazebo\u4eff\u771f\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5e73\u5747\u8d70\u5eca\u98ce\u9669\u51cf\u5c1158%\u548c\u5730\u56fe\u71b5\u51cf\u5c1143%\uff0c\u5e76\u4e14\u4e0e\u524d\u6cbf\u63a2\u7d22\u76f8\u6bd4\uff0c\u5728\u4e00\u534a\u7684\u4efb\u52a1\u65f6\u95f4\u5185\u5b9e\u73b0\u4e86\u7c7b\u4f3c\u7684\u4e0d\u786e\u5b9a\u6027\u51cf\u5c11\u3002", "conclusion": "\u5c06BIM\u5148\u9a8c\u77e5\u8bc6\u4e0e\u98ce\u9669\u81ea\u9002\u5e94\u7684\u7a7a\u4e2d\u611f\u77e5\u76f8\u7ed3\u5408\uff0c\u80fd\u591f\u4e3a\u5efa\u7b51\u673a\u5668\u4eba\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u80fd\u591f\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u81ea\u4e3b\u5bfc\u822a\u80fd\u529b\u3002"}}
{"id": "2511.14231", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.14231", "abs": "https://arxiv.org/abs/2511.14231", "authors": ["Huram Konjen"], "title": "Algorithmic Management and the Future of Human Work: Implications for Autonomy, Collaboration, and Innovation", "comment": "13 pages, 1 figure, 2 tables. Conceptual integration paper", "summary": "This study examines the evolving impact of algorithmic management on human resource management (HRM) practices, with a focus on employee autonomy, procedural transparency, and the sociotechnical dynamics of performance evaluation. Rather than adopting a qualitative or empirical approach, the paper develops a conceptual integration of insights from HRM, human-computer interaction (HCI), and Science and Technology Studies. The analysis highlights that although algorithmic systems can enhance operational efficiency, they risk reinforcing biases and narrowing the relational and contextual dimensions of work. These systems often overlook intangible contributions such as creativity, empathy, and collaborative problem solving, revealing gaps in data-driven performance measurement. In response, the study proposes a sociotechnical perspective on algorithmic accountability that emphasizes procedural transparency, organizational justice, and employee agency. By revisiting foundational questions within the rapidly evolving landscape of algorithmic management, the paper contributes to ongoing debates about the future of work and the design of managerial technologies that support, rather than constrain, human autonomy and organizational life.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u7b97\u6cd5\u7ba1\u7406\u5bf9\u4eba\u529b\u8d44\u6e90\u7ba1\u7406\u7684\u5f71\u54cd\uff0c\u5f3a\u8c03\u5176\u5728\u63d0\u9ad8\u6548\u7387\u7684\u540c\u65f6\uff0c\u4e5f\u53ef\u80fd\u52a0\u5267\u5de5\u4f5c\u4e2d\u7684\u504f\u89c1\uff0c\u5efa\u8bae\u5efa\u7acb\u793e\u4f1a\u6280\u672f\u7684\u7b97\u6cd5\u95ee\u8d23\u4f53\u7cfb\uff0c\u4ee5\u652f\u6301\u5458\u5de5\u7684\u81ea\u4e3b\u6027\u3002", "motivation": "\u63a2\u8ba8\u7b97\u6cd5\u7ba1\u7406\u5bf9\u4eba\u529b\u8d44\u6e90\u7ba1\u7406\u5b9e\u8df5\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5458\u5de5\u7684\u81ea\u4e3b\u6027\u3001\u7a0b\u5e8f\u900f\u660e\u5ea6\u53ca\u7ee9\u6548\u8bc4\u4f30\u7684\u793e\u4f1a\u6280\u672f\u52a8\u6001\u3002", "method": "\u901a\u8fc7\u6982\u5ff5\u6574\u5408HRM\u3001\u4eba\u673a\u4ea4\u4e92(HCI)\u548c\u79d1\u6280\u7814\u7a76\u7684\u89c1\u89e3\uff0c\u800c\u975e\u5b9a\u6027\u6216\u5b9e\u8bc1\u7814\u7a76\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u793e\u4f1a\u6280\u672f\u89c6\u89d2\u7684\u7b97\u6cd5\u95ee\u8d23\u6846\u67b6\uff0c\u5f3a\u8c03\u7a0b\u5e8f\u900f\u660e\u5ea6\u3001\u7ec4\u7ec7\u516c\u6b63\u548c\u5458\u5de5\u81ea\u6cbb\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u7b97\u6cd5\u7ba1\u7406\u7684\u53cc\u5203\u5251\u7279\u6027\uff0c\u5176\u63d0\u9ad8\u6548\u7387\u7684\u540c\u65f6\uff0c\u4e5f\u53ef\u80fd\u52a0\u5267\u504f\u89c1\u5e76\u964d\u4f4e\u5de5\u4f5c\u4e2d\u7684\u60c5\u611f\u548c\u5408\u4f5c\u8d21\u732e\u3002"}}
{"id": "2511.14139", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.14139", "abs": "https://arxiv.org/abs/2511.14139", "authors": ["Junhao Gong", "Shoujie Li", "Kit-Wa Sou", "Changqing Guo", "Hourong Huang", "Tong Wu", "Yifan Xie", "Chenxin Liang", "Chuqiao Lyu", "Xiaojun Liang", "Wenbo Ding"], "title": "FlexiCup: Wireless Multimodal Suction Cup with Dual-Zone Vision-Tactile Sensing", "comment": null, "summary": "Conventional suction cups lack sensing capabilities for contact-aware manipulation in unstructured environments. This paper presents FlexiCup, a fully wireless multimodal suction cup that integrates dual-zone vision-tactile sensing. The central zone dynamically switches between vision and tactile modalities via illumination control for contact detection, while the peripheral zone provides continuous spatial awareness for approach planning. FlexiCup supports both vacuum and Bernoulli suction modes through modular mechanical configurations, achieving complete wireless autonomy with onboard computation and power. We validate hardware versatility through dual control paradigms. Modular perception-driven grasping across structured surfaces with varying obstacle densities demonstrates comparable performance between vacuum (90.0% mean success) and Bernoulli (86.7% mean success) modes. Diffusion-based end-to-end learning achieves 73.3% success on inclined transport and 66.7% on orange extraction tasks. Ablation studies confirm that multi-head attention coordinating dual-zone observations provides 13% improvements for contact-aware manipulation. Hardware designs and firmware are available at https://anonymous.4open.science/api/repo/FlexiCup-DA7D/file/index.html?v=8f531b44.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7684FlexiCup\u662f\u4e00\u79cd\u65e0\u7ebf\u591a\u6a21\u6001\u5438\u76d8\uff0c\u96c6\u6210\u4e86\u53cc\u533a\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\uff0c\u663e\u8457\u63d0\u9ad8\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u63a5\u89e6\u611f\u77e5\u64cd\u63a7\u80fd\u529b\uff0c\u7ecf\u8fc7\u9a8c\u8bc1\u663e\u793a\u4e86\u826f\u597d\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u4f20\u7edf\u5438\u76d8\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u63a5\u89e6\u611f\u77e5\u80fd\u529b\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u4ee5\u63d0\u9ad8\u64cd\u63a7\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u901a\u8fc7\u6a21\u5757\u5316\u673a\u68b0\u914d\u7f6e\u548c\u53cc\u533a\u57df\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u7684\u96c6\u6210\u5b9e\u73b0\u4e86\u5b8c\u5168\u65e0\u7ebf\u7684\u81ea\u4e3b\u64cd\u4f5c\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u63a7\u5236\u8303\u5f0f\u9a8c\u8bc1\u4e86\u786c\u4ef6\u7684\u7075\u6d3b\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFlexiCup\u5728\u63d0\u4f9b\u4e86\u826f\u597d\u7684\u6210\u529f\u7387\uff08\u771f\u7a7a\u6a21\u5f0f90.0%\u548cBernoulli\u6a21\u5f0f86.7%\uff09\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u6269\u6563\u5b66\u4e60\u5728\u503e\u659c\u8fd0\u8f93\u548c\u6a59\u5b50\u63d0\u53d6\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4e8673.3%\u548c66.7%\u7684\u6210\u529f\u7387\uff0c\u4e14\u591a\u5934\u6ce8\u610f\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u64cd\u63a7\u6548\u679c\u3002", "conclusion": "FlexiCup\u5728\u63a5\u89e6\u611f\u77e5\u64cd\u63a7\u65b9\u9762\u5b9e\u73b0\u4e86\u663e\u8457\u63d0\u5347\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2511.14233", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.14233", "abs": "https://arxiv.org/abs/2511.14233", "authors": ["Wei Xiang", "Ziyue Lei", "Jie Wang", "Yingying Huang", "Qi Zheng", "Tianyi Zhang", "An Zhao", "Lingyun Sun"], "title": "Visionary Co-Driver: Enhancing Driver Perception of Potential Risks with LLM and HUD", "comment": "Accepted for publication in IEEE Transactions on Intelligent Transportation Systems (T-ITS)", "summary": "Drivers' perception of risky situations has always been a challenge in driving. Existing risk-detection methods excel at identifying collisions but face challenges in assessing the behavior of road users in non-collision situations. This paper introduces Visionary Co-Driver, a system that leverages large language models to identify non-collision roadside risks and alert drivers based on their eye movements. Specifically, the system combines video processing algorithms and LLMs to identify potentially risky road users. These risks are dynamically indicated on an adaptive heads-up display interface to enhance drivers' attention. A user study with 41 drivers confirms that Visionary Co-Driver improves drivers' risk perception and supports their recognition of roadside risks.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVisionary Co-Driver\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u9891\u5904\u7406\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u5347\u9a7e\u9a76\u8005\u5728\u975e\u78b0\u649e\u60c5\u51b5\u4e0b\u7684\u98ce\u9669\u8ba4\u77e5\u3002", "motivation": "\u73b0\u6709\u7684\u98ce\u9669\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u78b0\u649e\u8bc6\u522b\u4e0a\uff0c\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30\u975e\u78b0\u649e\u60c5\u51b5\u4e0b\u7684\u9053\u8def\u4f7f\u7528\u8005\u884c\u4e3a\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u7cfb\u7edf\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u89c6\u9891\u5904\u7406\u7b97\u6cd5\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u6765\u8bc6\u522b\u6f5c\u5728\u7684\u8def\u9762\u98ce\u9669\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u62ac\u5934\u663e\u793a\u754c\u9762\u52a8\u6001\u6307\u793a\u8fd9\u4e9b\u98ce\u9669\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c41\u540d\u9a7e\u9a76\u8005\u4f7f\u7528Visionary Co-Driver\u540e\uff0c\u5176\u98ce\u9669\u611f\u77e5\u663e\u8457\u63d0\u5347\u3002", "conclusion": "Visionary Co-Driver\u7cfb\u7edf\u6709\u6548\u63d0\u5347\u4e86\u9a7e\u9a76\u8005\u5bf9\u8def\u8fb9\u98ce\u9669\u7684\u611f\u77e5\uff0c\u652f\u6301\u4e86\u4ed6\u4eec\u5bf9\u975e\u78b0\u649e\u573a\u666f\u7684\u98ce\u9669\u8bc6\u522b\u3002"}}
{"id": "2511.14148", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14148", "abs": "https://arxiv.org/abs/2511.14148", "authors": ["Yuhua Jiang", "Shuang Cheng", "Yan Ding", "Feifei Gao", "Biqing Qi"], "title": "AsyncVLA: Asynchronous Flow Matching for Vision-Language-Action Models", "comment": null, "summary": "Vision-language-action (VLA) models have recently emerged as a powerful paradigm for building generalist robots. However, traditional VLA models that generate actions through flow matching (FM) typically rely on rigid and uniform time schedules, i.e., synchronous FM (SFM). Without action context awareness and asynchronous self-correction, SFM becomes unstable in long-horizon tasks, where a single action error can cascade into failure. In this work, we propose asynchronous flow matching VLA (AsyncVLA), a novel framework that introduces temporal flexibility in asynchronous FM (AFM) and enables self-correction in action generation. AsyncVLA breaks from the vanilla SFM in VLA models by generating the action tokens in a non-uniform time schedule with action context awareness. Besides, our method introduces the confidence rater to extract confidence of the initially generated actions, enabling the model to selectively refine inaccurate action tokens before execution. Moreover, we propose a unified training procedure for SFM and AFM that endows a single model with both modes, improving KV-cache utilization. Extensive experiments on robotic manipulation benchmarks demonstrate that AsyncVLA is data-efficient and exhibits self-correction ability. AsyncVLA achieves state-of-the-art results across general embodied evaluations due to its asynchronous generation in AFM. Our code is available at https://github.com/YuhuaJiang2002/AsyncVLA.", "AI": {"tldr": "AsyncVLA\u662f\u4e00\u79cd\u65b0\u7684VLA\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u5f02\u6b65\u6d41\u5339\u914d\u548c\u81ea\u6211\u4fee\u6b63\u673a\u5236\uff0c\u4f18\u5316\u4e86\u957f\u4efb\u52a1\u6267\u884c\u4e2d\u7684\u52a8\u4f5c\u751f\u6210\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u540c\u6b65\u6d41\u5339\u914d(SFM)\u5728\u957f\u4efb\u52a1\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\uff0c\u63d0\u5347\u52a8\u4f5c\u751f\u6210\u7684\u7075\u6d3b\u6027\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u5f02\u6b65\u6d41\u5339\u914dVLA (AsyncVLA)\u6846\u67b6\uff0c\u91c7\u7528\u975e\u5747\u5300\u65f6\u95f4\u8c03\u5ea6\u751f\u6210\u52a8\u4f5c\uff0c\u5e76\u5f15\u5165\u81ea\u4fe1\u7b49\u7ea7\u5668\u8fdb\u884c\u52a8\u4f5c\u4fee\u6b63\u3002", "result": "\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u57fa\u51c6\u4e0a\uff0cAsyncVLA\u5c55\u73b0\u51fa\u6570\u636e\u6548\u7387\u548c\u81ea\u6211\u4fee\u6b63\u80fd\u529b\uff0c\u5e76\u5728\u901a\u7528\u673a\u5668\u4eba\u8bc4\u4f30\u4e2d\u53d6\u5f97\u4e86\u6700\u65b0\u6210\u679c\u3002", "conclusion": "AsyncVLA\u5b9e\u73b0\u4e86\u5f02\u6b65\u751f\u6210\uff0c\u63d0\u9ad8\u4e86\u6570\u636e\u6548\u7387\u548c\u81ea\u6211\u4fee\u6b63\u80fd\u529b\uff0c\u8fbe\u5230\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u57fa\u51c6\u7684\u6700\u65b0\u6210\u679c\u3002"}}
{"id": "2511.14242", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.14242", "abs": "https://arxiv.org/abs/2511.14242", "authors": ["Yuan Li", "Xinyue Gui", "Ding Xia", "Mark Colley", "Takeo Igarashi"], "title": "TailCue: Exploring Animal-inspired Robotic Tail for Automated Vehicles Interaction", "comment": null, "summary": "Automated vehicles (AVs) are gradually becoming part of our daily lives. However, effective communication between road users and AVs remains a significant challenge. Although various external human-machine interfaces (eHMIs) have been developed to facilitate interactions, psychological factors, such as a lack of trust and inadequate emotional signaling, may still deter users from confidently engaging with AVs in certain contexts. To address this gap, we propose TailCue, an exploration of how tail-based eHMIs affect user interaction with AVs. We first investigated mappings between tail movements and emotional expressions from robotics and zoology, and accordingly developed a motion-emotion mapping scheme. A physical robotic tail was implemented, and specific tail motions were designed based on our scheme. An online, video-based user study with 21 participants was conducted. Our findings suggest that, although the intended emotions conveyed by the tail were not consistently recognized, open-ended feedback indicated that the tail motion needs to align with the scenarios and cues. Our result highlights the necessity of scenario-specific optimization to enhance tail-based eHMIs. Future work will refine tail movement strategies to maximize their effectiveness across diverse interaction contexts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c3e\u90e8\u7684\u60c5\u611f\u4fe1\u53f7\u7814\u7a76\uff0c\u5f3a\u8c03\u9700\u8981\u573a\u666f\u7279\u5b9a\u7684\u4f18\u5316\u4ee5\u4fc3\u8fdb\u4e0e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u4ea4\u4e92\u3002", "motivation": "\u6709\u6548\u7684\u4ea4\u901a\u53c2\u4e0e\u8005\u4e0e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e4b\u95f4\u7684\u6c9f\u901a\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\uff0c\u5fc3\u7406\u56e0\u7d20\u53ef\u80fd\u5f71\u54cd\u7528\u6237\u7684\u4fe1\u4efb\u5ea6\u548c\u53c2\u4e0e\u5ea6\u3002", "method": "\u901a\u8fc7\u673a\u5668\u4eba\u548c\u52a8\u7269\u5b66\u7684\u60c5\u611f\u8868\u8fbe\u7814\u7a76\uff0c\u5f00\u53d1\u8fd0\u52a8-\u60c5\u611f\u6620\u5c04\u65b9\u6848\uff0c\u5e76\u5b9e\u65bd\u7269\u7406\u673a\u68b0\u5c3e\u8fdb\u884c\u5728\u7ebf\u7528\u6237\u7814\u7a76\u3002", "result": "\u867d\u7136\u5c3e\u90e8\u4f20\u8fbe\u7684\u60c5\u611f\u6ca1\u6709\u88ab\u4e00\u81f4\u8bc6\u522b\uff0c\u4f46\u5f00\u653e\u5f0f\u53cd\u9988\u8868\u660e\uff0c\u5c3e\u90e8\u8fd0\u52a8\u9700\u8981\u4e0e\u573a\u666f\u548c\u63d0\u793a\u76f8\u4e00\u81f4\u3002", "conclusion": "\u5c3e\u90e8\u8fd0\u52a8\u9700\u8981\u4e0e\u573a\u666f\u548c\u63d0\u793a\u5bf9\u9f50\uff0c\u4ee5\u4f18\u5316\u57fa\u4e8e\u5c3e\u90e8\u7684eHMIs\u3002"}}
{"id": "2511.14161", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.14161", "abs": "https://arxiv.org/abs/2511.14161", "authors": ["Xiaoquan Sun", "Ruijian Zhang", "Kang Pang", "Bingchen Miao", "Yuxiang Tan", "Zhen Yang", "Ming Li", "Jiayu Chen"], "title": "RoboTidy : A 3D Gaussian Splatting Household Tidying Benchmark for Embodied Navigation and Action", "comment": null, "summary": "Household tidying is an important application area, yet current benchmarks neither model user preferences nor support mobility, and they generalize poorly, making it hard to comprehensively assess integrated language-to-action capabilities. To address this, we propose RoboTidy, a unified benchmark for language-guided household tidying that supports Vision-Language-Action (VLA) and Vision-Language-Navigation (VLN) training and evaluation. RoboTidy provides 500 photorealistic 3D Gaussian Splatting (3DGS) household scenes (covering 500 objects and containers) with collisions, formulates tidying as an \"Action (Object, Container)\" list, and supplies 6.4k high-quality manipulation demonstration trajectories and 1.5k naviagtion trajectories to support both few-shot and large-scale training. We also deploy RoboTidy in the real world for object tidying, establishing an end-to-end benchmark for household tidying. RoboTidy offers a scalable platform and bridges a key gap in embodied AI by enabling holistic and realistic evaluation of language-guided robots.", "AI": {"tldr": "RoboTidy\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\uff0c\u65e8\u5728\u901a\u8fc7\u8bed\u8a00\u6307\u5bfc\u5bb6\u5ead\u6574\u7406\uff0c\u652f\u6301VLA\u548cVLN\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u63d0\u4f9b\u4e30\u5bcc\u76843D\u573a\u666f\u548c\u6f14\u793a\uff0c\u4fc3\u8fdb\u673a\u5668\u4eba\u6280\u672f\u7684\u53d1\u5c55\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6ca1\u6709\u6a21\u578b\u7528\u6237\u504f\u597d\u548c\u652f\u6301\u79fb\u52a8\u6027\uff0c\u5bfc\u81f4\u7ed3\u679c\u7684\u6982\u62ec\u80fd\u529b\u8f83\u5dee\uff0c\u96be\u4ee5\u5168\u9762\u8bc4\u4f30\u7efc\u5408\u7684\u8bed\u8a00\u5230\u884c\u52a8\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u5e76\u90e8\u7f72\u4e86RoboTidy\u4f5c\u4e3a\u4e00\u9879\u57fa\u51c6\uff0c\u5305\u62ec500\u4e2a\u771f\u5b9e\u611f\u76843D\u5bb6\u5ead\u573a\u666f\uff0c\u4ee5\u53ca\u6536\u96c6\u4e0d\u540c\u7684\u6f14\u793a\u548c\u5bfc\u822a\u8f68\u8ff9\u6765\u652f\u6301\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "RoboTidy\u5305\u542b500\u4e2a\u573a\u666f\u30016.4k\u9ad8\u8d28\u91cf\u64cd\u4f5c\u6f14\u793a\u8f68\u8ff9\u548c1.5k\u5bfc\u822a\u8f68\u8ff9\uff0c\u65e8\u5728\u652f\u6301\u5c11\u91cf\u6837\u672c\u548c\u5927\u89c4\u6a21\u8bad\u7ec3\uff0c\u4e14\u5df2\u5728\u5b9e\u9645\u4e16\u754c\u4e2d\u8fdb\u884c\u90e8\u7f72\u3002", "conclusion": "RoboTidy\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u51c6\uff0c\u901a\u8fc7\u652f\u6301\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8(VLA)\u548c\u89c6\u89c9-\u8bed\u8a00-\u5bfc\u822a(VLN)\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u586b\u8865\u4e86\u73b0\u6709\u5bb6\u5ead\u6574\u7406\u9886\u57df\u7684\u5173\u952e\u7a7a\u767d\u3002"}}
{"id": "2511.14359", "categories": ["cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.14359", "abs": "https://arxiv.org/abs/2511.14359", "authors": ["Sebastian Lubos", "Alexander Felfernig", "Damian Garber", "Viet-Man Le", "Thi Ngoc Trang Tran"], "title": "Towards LLM-Based Usability Analysis for Recommender User Interfaces", "comment": "The paper was presented at IntRS'25: Joint Workshop on Interfaces and Human Decision Making for Recommender Systems, September 22, 2025, Prague, Czech Republic and is published in the workshop proceedings: https://ceur-ws.org/Vol-4027/", "summary": "Usability is a key factor in the effectiveness of recommender systems. However, the analysis of user interfaces is a time-consuming process that requires expertise. Recent advances in multimodal large language models (LLMs) offer promising opportunities to automate such evaluations. In this work, we explore the potential of multimodal LLMs to assess the usability of recommender system interfaces by considering a variety of publicly available systems as examples. We take user interface screenshots from multiple of these recommender platforms to cover both preference elicitation and recommendation presentation scenarios. An LLM is instructed to analyze these interfaces with regard to different usability criteria and provide explanatory feedback. Our evaluation demonstrates how LLMs can support heuristic-style usability assessments at scale to support the improvement of user experience.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bc4\u4f30\u63a8\u8350\u7cfb\u7edf\u53ef\u7528\u6027\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u7ed3\u679c\u8868\u660e\u8fd9\u4e9b\u6a21\u578b\u53ef\u4ee5\u4ee5\u89c4\u6a21\u5316\u7684\u65b9\u5f0f\u652f\u6301\u53ef\u7528\u6027\u8bc4\u4f30\u3002", "motivation": "\u63d0\u9ad8\u63a8\u8350\u7cfb\u7edf\u7684\u53ef\u7528\u6027\u8bc4\u4f30\u6548\u7387\uff0c\u51cf\u5c11\u5bf9\u4e13\u4e1a\u77e5\u8bc6\u7684\u4f9d\u8d56\uff0c\u5229\u7528\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u5316\u8bc4\u4f30\u8fc7\u7a0b\u3002", "method": "\u901a\u8fc7\u5206\u6790\u591a\u79cd\u516c\u5f00\u53ef\u7528\u7684\u63a8\u8350\u7cfb\u7edf\u754c\u9762\u622a\u56fe\uff0c\u8003\u8651\u4e0d\u540c\u7684\u53ef\u7528\u6027\u6807\u51c6\uff0c\u5e76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u89e3\u91ca\u6027\u53cd\u9988\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u4ee5\u542f\u53d1\u5f0f\u65b9\u5f0f\u652f\u6301\u53ef\u7528\u6027\u8bc4\u4f30\uff0c\u63a8\u52a8\u7528\u6237\u4f53\u9a8c\u7684\u6539\u8fdb\u3002", "conclusion": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5728\u5927\u89c4\u6a21\u4e0b\u652f\u6301\u63a8\u8350\u7cfb\u7edf\u754c\u9762\u7684\u53ef\u7528\u6027\u8bc4\u4f30\uff0c\u4ece\u800c\u6539\u5584\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2511.14178", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14178", "abs": "https://arxiv.org/abs/2511.14178", "authors": ["Zhuo Li", "Junjia Liu", "Zhipeng Dong", "Tao Teng", "Quentin Rouxel", "Darwin Caldwell", "Fei Chen"], "title": "Towards Deploying VLA without Fine-Tuning: Plug-and-Play Inference-Time VLA Policy Steering via Embodied Evolutionary Diffusion", "comment": "9 pages, 8 figures, submitted to IEEE RA-L", "summary": "Vision-Language-Action (VLA) models have demonstrated significant potential in real-world robotic manipulation. However, pre-trained VLA policies still suffer from substantial performance degradation during downstream deployment. Although fine-tuning can mitigate this issue, its reliance on costly demonstration collection and intensive computation makes it impractical in real-world settings. In this work, we introduce VLA-Pilot, a plug-and-play inference-time policy steering method for zero-shot deployment of pre-trained VLA without any additional fine-tuning or data collection. We evaluate VLA-Pilot on six real-world downstream manipulation tasks across two distinct robotic embodiments, encompassing both in-distribution and out-of-distribution scenarios. Experimental results demonstrate that VLA-Pilot substantially boosts the success rates of off-the-shelf pre-trained VLA policies, enabling robust zero-shot generalization to diverse tasks and embodiments. Experimental videos and code are available at: https://rip4kobe.github.io/vla-pilot/.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faVLA-Pilot\uff0c\u89e3\u51b3\u4e86\u9884\u8bad\u7ec3VLA\u653f\u7b56\u5728\u65e0\u7f1d\u90e8\u7f72\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6210\u529f\u7387\uff0c\u652f\u6301\u96f6-shot\u6cdb\u5316\u3002", "motivation": "\u89e3\u51b3\u9884\u8bad\u7ec3VLA\u7b56\u7565\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u9ad8\u6210\u672c\u548c\u8ba1\u7b97\u5bc6\u96c6\u6027\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u63a8\u7406\u65f6\u95f4\u653f\u7b56\u5f15\u5bfc\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u7684\u5fae\u8c03\u6216\u6570\u636e\u6536\u96c6\u3002", "result": "VLA-Pilot\u5728\u516d\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u4e0b\u6e38\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u9ad8\u4e86\u653f\u7b56\u6210\u529f\u7387\uff0c\u5b9e\u73b0\u4e86\u5bf9\u591a\u6837\u5316\u4efb\u52a1\u548c\u673a\u5668\u4eba\u8bbe\u7f6e\u7684\u5f3a\u7a33\u5065\u6027\u3002", "conclusion": "VLA-Pilot\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u8bad\u7ec3VLA\u7b56\u7565\u7684\u6210\u529f\u7387\uff0c\u652f\u6301\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u96f6-shot\u63a8\u5e7f\u3002"}}
{"id": "2511.14414", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.14414", "abs": "https://arxiv.org/abs/2511.14414", "authors": ["Yu Mei", "Xutong Wang", "Ziyao Zhang", "Yiming Fu", "Shiyi Wang", "Qingyang Wan", "Qinghuan Lan", "Chang Liu", "Jie Cai", "Chun Yu", "Yuanchun Shi"], "title": "PACEE: Supporting Children's Personal Emotion Education through Parent-AI Collaboration", "comment": null, "summary": "Emotion education is a crucial lesson for children aged 3 to 6. However, existing technologies primarily focus on promoting emotion education from the child's perspective, often neglecting the central role of parents in guiding early childhood emotion development. In this work, we conducted co-design sessions with five experienced kindergarten teachers and five parents to identify parental challenges and the roles that AI can play in family emotion education. Guided by these insights, we developed PACEE, an assistant for supporting parent-AI collaborative emotion education. PACEE enables parents to engage in emotional dialogues about common scenarios, with multiple forms of support provided by generative AI. It combines insights from parents and AI to model children's emotional states and collaboratively delivers personalized, parent-mediated guidance. In a user study involving 16 families, we found that PACEE significantly enhances parent-child engagement, encourages more in-depth emotional communication, and improves the parental experience. Our findings advance emotion coaching theory in both family settings and LLM-assisted contexts, offering valuable insights for designing AI-supported, parent-centered family education systems.", "AI": {"tldr": "PACEE\u662f\u4e00\u4e2a\u652f\u6301\u7236\u6bcd\u4e0eAI\u5408\u4f5c\u60c5\u611f\u6559\u80b2\u7684\u52a9\u624b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eb2\u5b50\u4e92\u52a8\u548c\u60c5\u611f\u6c9f\u901a\u3002", "motivation": "\u73b0\u6709\u6280\u672f\u4e3b\u8981\u4ece\u513f\u7ae5\u7684\u89d2\u5ea6\u4fc3\u8fdb\u60c5\u611f\u6559\u80b2\uff0c\u5ffd\u89c6\u4e86\u7236\u6bcd\u5728\u65e9\u671f\u60c5\u611f\u53d1\u5c55\u4e2d\u7684\u4e2d\u5fc3\u89d2\u8272\u3002", "method": "\u901a\u8fc7\u4e0e\u4e94\u4f4d\u7ecf\u9a8c\u4e30\u5bcc\u7684\u5e7c\u513f\u56ed\u6559\u5e08\u548c\u4e94\u4f4d\u5bb6\u957f\u8fdb\u884c\u8054\u5408\u8bbe\u8ba1\u4f1a\u8bae\uff0c\u8bc6\u522b\u4e86\u5bb6\u957f\u9762\u4e34\u7684\u6311\u6218\u4ee5\u53caAI\u5728\u5bb6\u5ead\u60c5\u611f\u6559\u80b2\u4e2d\u7684\u89d2\u8272\u3002", "result": "PACEE\u5141\u8bb8\u7236\u6bcd\u53c2\u4e0e\u5173\u4e8e\u5e38\u89c1\u60c5\u5883\u7684\u60c5\u611f\u5bf9\u8bdd\uff0c\u5e76\u901a\u8fc7\u751f\u6210\u5f0fAI\u63d0\u4f9b\u591a\u79cd\u652f\u6301\uff0c\u589e\u5f3a\u4e86\u60c5\u611f\u4e92\u52a8\u3002", "conclusion": "PACEE\u663e\u8457\u589e\u5f3a\u4e86\u4eb2\u5b50\u4e92\u52a8\uff0c\u9f13\u52b1\u4e86\u66f4\u6df1\u5165\u7684\u60c5\u611f\u4ea4\u6d41\uff0c\u5e76\u6539\u5584\u4e86\u7236\u6bcd\u7684\u4f53\u9a8c\uff0c\u4e3aAI\u652f\u6301\u4e0b\u7684\u5bb6\u5ead\u6559\u80b2\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2511.14327", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.14327", "abs": "https://arxiv.org/abs/2511.14327", "authors": ["Felipe Ballen-Moreno", "Pasquale Ferrentino", "Milan Amighi", "Bram Vanderborght", "Tom Verstraten"], "title": "Dual-Variable Force Characterisation method for Human-Robot Interaction in Wearable Robotics", "comment": "36 pages, 10 figures, submitted and under-review in Journal of the Mechanical Behavior of Biomedical Materials", "summary": "Understanding the physical interaction with wearable robots is essential to ensure safety and comfort. However, this interaction is complex in two key aspects: (1) the motion involved, and (2) the non-linear behaviour of soft tissues. Multiple approaches have been undertaken to better understand this interaction and to improve the quantitative metrics of physical interfaces or cuffs. As these two topics are closely interrelated, finite modelling and soft tissue characterisation offer valuable insights into pressure distribution and shear stress induced by the cuff. Nevertheless, current characterisation methods typically rely on a single fitting variable along one degree of freedom, which limits their applicability, given that interactions with wearable robots often involve multiple degrees of freedom. To address this limitation, this work introduces a dual-variable characterisation method, involving normal and tangential forces, aimed at identifying reliable material parameters and evaluating the impact of single-variable fitting on force and torque responses. This method demonstrates the importance of incorporating two variables into the characterisation process by analysing the normalized mean square error (NMSE) across different scenarios and material models, providing a foundation for simulation at the closest possible level, with a focus on the cuff and the human limb involved in the physical interaction between the user and the wearable robot.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u53cc\u53d8\u91cf\u8868\u5f81\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b63\u5e38\u548c\u5207\u5411\u529b\u6765\u5206\u6790\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u53ca\u5176\u4e0e\u4eba\u7c7b\u80a2\u4f53\u7684\u7269\u7406\u4ea4\u4e92\uff0c\u4ece\u800c\u6539\u5584\u5f53\u524d\u5355\u53d8\u91cf\u62df\u5408\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u7406\u89e3\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u4e0e\u4eba\u4f53\u7684\u7269\u7406\u4ea4\u4e92\u5bf9\u4e8e\u786e\u4fdd\u5b89\u5168\u548c\u8212\u9002\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u81ea\u7531\u5ea6\u4ea4\u4e92\u4e0b\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6d89\u53ca\u6cd5\u5411\u548c\u5207\u5411\u529b\u7684\u53cc\u53d8\u91cf\u8868\u5f81\u65b9\u6cd5\uff0c\u4ee5\u8bc6\u522b\u53ef\u9760\u7684\u6750\u6599\u53c2\u6570\u5e76\u8bc4\u4f30\u5355\u53d8\u91cf\u62df\u5408\u5bf9\u529b\u548c\u626d\u77e9\u54cd\u5e94\u7684\u5f71\u54cd\u3002", "result": "\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u573a\u666f\u548c\u6750\u6599\u6a21\u578b\u4e2d\u7684\u5f52\u4e00\u5316\u5747\u65b9\u8bef\u5dee\uff08NMSE\uff09\uff0c\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5c06\u4e24\u4e2a\u53d8\u91cf\u7eb3\u5165\u8868\u5f81\u8fc7\u7a0b\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u5f15\u5165\u53cc\u53d8\u91cf\u8868\u5f81\u65b9\u6cd5\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u4e0e\u4eba\u4f53\u4e4b\u95f4\u7684\u7269\u7406\u4ea4\u4e92\uff0c\u7279\u522b\u662f\u5728\u538b\u529b\u5206\u5e03\u548c\u526a\u5207\u5e94\u529b\u65b9\u9762\u3002"}}
{"id": "2511.14437", "categories": ["cs.HC", "cs.FL"], "pdf": "https://arxiv.org/pdf/2511.14437", "abs": "https://arxiv.org/abs/2511.14437", "authors": ["Mehrnoush Hajnorouzi", "Astrid Rakow", "Martin Fr\u00e4nzle"], "title": "Model Learning for Adjusting the Level of Automation in HCPS", "comment": "In Proceedings FMAS 2025, arXiv:2511.13245", "summary": "The steadily increasing level of automation in human-centred systems demands rigorous design methods for analysing and controlling interactions between humans and automated components, especially in safety-critical applications. The variability of human behaviour poses particular challenges for formal verification and synthesis. We present a model-based framework that enables design-time exploration of safe shared-control strategies in human-automation systems. The approach combines active automata learning -- to derive coarse, finite-state abstractions of human behaviour from simulations -- with game-theoretic reactive synthesis to determine whether a controller can guarantee safety when interacting with these models. If no such strategy exists, the framework supports iterative refinement of the human model or adjustment of the automation's controllable actions. A driving case study, integrating automata learning with reactive synthesis in UPPAAL, illustrates the applicability of the framework on a simplified driving scenario and its potential for analysing shared-control strategies in human-centred cyber-physical systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u52a8\u673a\u5b66\u4e60\u548c\u535a\u5f08\u7406\u8bba\u5408\u6210\uff0c\u5206\u6790\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u5b89\u5168\u63a7\u5236\u7b56\u7565\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u5316\u6c34\u5e73\u7684\u4e0d\u65ad\u63d0\u9ad8\uff0c\u5bf9\u4eba\u673a\u4ea4\u4e92\u7684\u4e25\u8c28\u8bbe\u8ba1\u65b9\u6cd5\u9700\u6c42\u8feb\u5207\uff0c\u5c24\u5176\u662f\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u3002", "method": "\u7ed3\u5408\u4e86\u4e3b\u52a8\u81ea\u52a8\u673a\u5b66\u4e60\u548c\u535a\u5f08\u7406\u8bba\u53cd\u5e94\u5408\u6210\u7684\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u6846\u67b6\u5728\u7b80\u5316\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u5206\u6790\u4eba\u673a\u534f\u4f5c\u63a7\u5236\u7b56\u7565\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u4ee5\u6709\u6548\u5730\u5206\u6790\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u5b89\u5168\u63a7\u5236\u7b56\u7565\uff0c\u63d0\u5347\u5b89\u5168\u6027\u548c\u53ef\u63a7\u6027\u3002"}}
{"id": "2511.14330", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.14330", "abs": "https://arxiv.org/abs/2511.14330", "authors": ["Yizhen Yin", "Yuhua Qi", "Dapeng Feng", "Hongbo Chen", "Hongjun Ma", "Jin Wu", "Yi Jiang"], "title": "MA-SLAM: Active SLAM in Large-Scale Unknown Environment using Map Aware Deep Reinforcement Learning", "comment": null, "summary": "Active Simultaneous Localization and Mapping (Active SLAM) involves the strategic planning and precise control of a robotic system's movement in order to construct a highly accurate and comprehensive representation of its surrounding environment, which has garnered significant attention within the research community. While the current methods demonstrate efficacy in small and controlled settings, they face challenges when applied to large-scale and diverse environments, marked by extended periods of exploration and suboptimal paths of discovery. In this paper, we propose MA-SLAM, a Map-Aware Active SLAM system based on Deep Reinforcement Learning (DRL), designed to address the challenge of efficient exploration in large-scale environments. In pursuit of this objective, we put forward a novel structured map representation. By discretizing the spatial data and integrating the boundary points and the historical trajectory, the structured map succinctly and effectively encapsulates the visited regions, thereby serving as input for the deep reinforcement learning based decision module. Instead of sequentially predicting the next action step within the decision module, we have implemented an advanced global planner to optimize the exploration path by leveraging long-range target points. We conducted experiments in three simulation environments and deployed in a real unmanned ground vehicle (UGV), the results demonstrate that our approach significantly reduces both the duration and distance of exploration compared with state-of-the-art methods.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e3b\u52a8SLAM\u7cfb\u7edfMA-SLAM\uff0c\u5229\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u548c\u7ed3\u6784\u5316\u5730\u56fe\u8868\u793a\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u73af\u5883\u4e0b\u7684\u9ad8\u6548\u63a2\u7d22\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u63a2\u7d22\u65f6\u95f4\u548c\u8ddd\u79bb\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u5f53\u524d\u7684\u4e3b\u52a8SLAM\u65b9\u6cd5\u5728\u5c0f\u89c4\u6a21\u4e14\u53d7\u63a7\u7684\u73af\u5883\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5927\u89c4\u6a21\u548c\u591a\u6837\u5316\u73af\u5883\u4e2d\u5b58\u5728\u63a2\u7d22\u65f6\u95f4\u957f\u548c\u53d1\u73b0\u8def\u5f84\u6b21\u4f18\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u5730\u56fe\u611f\u77e5\u4e3b\u52a8SLAM\u7cfb\u7edf\uff0c\u91c7\u7528\u4e86\u65b0\u7684\u7ed3\u6784\u5316\u5730\u56fe\u8868\u793a\uff0c\u901a\u8fc7\u79bb\u6563\u5316\u7a7a\u95f4\u6570\u636e\u5e76\u6574\u5408\u8fb9\u754c\u70b9\u4e0e\u5386\u53f2\u8f68\u8ff9\u6765\u63d0\u5347\u51b3\u7b56\u6a21\u5757\u7684\u8f93\u5165\u4fe1\u606f\u3002\u5b9e\u73b0\u4e86\u5148\u8fdb\u7684\u5168\u5c40\u89c4\u5212\u5668\u4ee5\u4f18\u5316\u63a2\u7d22\u8def\u5f84\u3002", "result": "\u7ecf\u8fc7\u4e09\u79cd\u4eff\u771f\u73af\u5883\u7684\u5b9e\u9a8c\u548c\u5728\u771f\u5b9e\u65e0\u4eba\u5730\u9762\u8f66\u8f86\uff08UGV\uff09\u4e0a\u7684\u90e8\u7f72\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u63a2\u7d22\u6301\u7eed\u65f6\u95f4\u548c\u8ddd\u79bb\u65b9\u9762\u90fd\u8f83\u73b0\u6709\u65b9\u6cd5\u6709\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u6211\u4eec\u7684MA-SLAM\u7cfb\u7edf\u5728\u63a2\u7d22\u65f6\u95f4\u548c\u8ddd\u79bb\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2511.14567", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14567", "abs": "https://arxiv.org/abs/2511.14567", "authors": ["Chen Chen", "Cuong Nguyen", "Alexa Siu", "Dingzeyu Li", "Nadir Weibel"], "title": "SweeperBot: Making 3D Browsing Accessible through View Analysis and Visual Question Answering", "comment": "28 pages, 16 figures, this article has been accepted for publication in the International Journal of Human-Computer Interaction (IJHCI), published by Taylor and Francis", "summary": "Accessing 3D models remains challenging for Screen Reader (SR) users. While some existing 3D viewers allow creators to provide alternative text, they often lack sufficient detail about the 3D models. Grounded on a formative study, this paper introduces SweeperBot, a system that enables SR users to leverage visual question answering to explore and compare 3D models. SweeperBot answers SR users' visual questions by combining an optimal view selection technique with the strength of generative- and recognition-based foundation models. An expert review with 10 Blind and Low-Vision (BLV) users with SR experience demonstrated the feasibility of using SweeperBot to assist BLV users in exploring and comparing 3D models. The quality of the descriptions generated by SweeperBot was validated by a second survey study with 30 sighted participants.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86SweeperBot\uff0c\u4e00\u4e2a\u7ed3\u5408\u89c6\u89c9\u95ee\u7b54\u7684\u7cfb\u7edf\uff0c\u65e8\u5728\u5e2e\u52a9\u5c4f\u5e55\u9605\u8bfb\u5668\u7528\u6237\u6709\u6548\u8bbf\u95ee\u548c\u7406\u89e33D\u6a21\u578b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5c4f\u5e55\u9605\u8bfb\u5668\u7528\u6237\u8bbf\u95ee3D\u6a21\u578b\u7684\u56f0\u96be\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u89c6\u89c9\u95ee\u7b54\u80fd\u529b\u6539\u5584\u5176\u4f53\u9a8c\u3002", "method": "\u8be5\u7814\u7a76\u7ed3\u5408\u4e86\u6700\u4f18\u89c6\u56fe\u9009\u62e9\u6280\u672f\u548c\u751f\u6210\u53ca\u8bc6\u522b\u57fa\u7840\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u5f00\u53d1\u4e86\u5141\u8bb8\u89c6\u969c\u7528\u6237\u63d0\u95ee\u5e76\u63a5\u53d7\u56de\u7b54\u7684\u7cfb\u7edf\u3002", "result": "SweeperBot\u6210\u529f\u5730\u5e2e\u52a9\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u7528\u6237\u5728\u63a2\u7d22\u548c\u6bd4\u8f833D\u6a21\u578b\u65b9\u9762\uff0c\u5e76\u4e14\u5176\u751f\u6210\u7684\u63cf\u8ff0\u8d28\u91cf\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002", "conclusion": "SweeperBot\u5c55\u793a\u4e86\u5e2e\u52a9\u89c6\u969c\u7528\u6237\u63a2\u7d22\u548c\u6bd4\u8f833D\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u5176\u751f\u6210\u7684\u63cf\u8ff0\u5f97\u5230\u4e86\u7528\u6237\u7684\u9a8c\u8bc1\u3002"}}
{"id": "2511.14335", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.14335", "abs": "https://arxiv.org/abs/2511.14335", "authors": ["Jeryes Danial", "Yosi Ben Asher", "Itzik Klein"], "title": "Simultaneous Localization and 3D-Semi Dense Mapping for Micro Drones Using Monocular Camera and Inertial Sensors", "comment": null, "summary": "Monocular simultaneous localization and mapping (SLAM) algorithms estimate drone poses and build a 3D map using a single camera. Current algorithms include sparse methods that lack detailed geometry, while learning-driven approaches produce dense maps but are computationally intensive. Monocular SLAM also faces scale ambiguities, which affect its accuracy. To address these challenges, we propose an edge-aware lightweight monocular SLAM system combining sparse keypoint-based pose estimation with dense edge reconstruction. Our method employs deep learning-based depth prediction and edge detection, followed by optimization to refine keypoints and edges for geometric consistency, without relying on global loop closure or heavy neural computations. We fuse inertial data with vision by using an extended Kalman filter to resolve scale ambiguity and improve accuracy. The system operates in real time on low-power platforms, as demonstrated on a DJI Tello drone with a monocular camera and inertial sensors. In addition, we demonstrate robust autonomous navigation and obstacle avoidance in indoor corridors and on the TUM RGBD dataset. Our approach offers an effective, practical solution to real-time mapping and navigation in resource-constrained environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u5355\u76eeSLAM\u7cfb\u7edf\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u7ed3\u5408\u7a00\u758f\u548c\u5bc6\u96c6\u7279\u5f81\u4f18\u5316\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u6620\u5c04\u548c\u5bfc\u822a\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u76eeSLAM\u7b97\u6cd5\u5728\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u5305\u62ec\u89c4\u6a21\u6b67\u4e49\u548c\u8be6\u7ec6\u51e0\u4f55\u7f3a\u5931\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u7a00\u758f\u5173\u952e\u70b9\u59ff\u6001\u4f30\u8ba1\u548c\u5bc6\u96c6\u8fb9\u7f18\u91cd\u5efa\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u8fdb\u884c\u6df1\u5ea6\u9884\u6d4b\u548c\u8fb9\u7f18\u68c0\u6d4b\uff0c\u968f\u540e\u901a\u8fc7\u4f18\u5316\u6765\u63d0\u9ad8\u51e0\u4f55\u4e00\u81f4\u6027\u3002", "result": "\u8be5\u7cfb\u7edf\u5728DJI Tello\u65e0\u4eba\u673a\u4e0a\u5b9e\u65f6\u8fd0\u884c\uff0c\u80fd\u591f\u8fdb\u884c\u81ea\u4e3b\u5bfc\u822a\u548c\u969c\u788d\u7269\u907f\u514d\uff0c\u5c55\u793a\u4e86\u5728\u5ba4\u5185\u8d70\u5eca\u548cTUM RGBD\u6570\u636e\u96c6\u4e0a\u7684\u6709\u6548\u5e94\u7528\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8f7b\u91cf\u7ea7\u5355\u76eeSLAM\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u5b9e\u65f6\u6761\u4ef6\u4e0b\u8fdb\u884c\u9ad8\u6548\u7684\u6620\u5c04\u548c\u5bfc\u822a\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u3002"}}
{"id": "2511.14591", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14591", "abs": "https://arxiv.org/abs/2511.14591", "authors": ["Nick von Felten", "Johannes Sch\u00f6ning", "Klaus Opwis", "Nicolas Scharowksi"], "title": "Biased Minds Meet Biased AI: How Class Imbalance Shapes Appropriate Reliance and Interacts with Human Base Rate Neglect", "comment": null, "summary": "Humans increasingly interact with artificial intelligence (AI) in decision-making. However, both AI and humans are prone to biases. While AI and human biases have been studied extensively in isolation, this paper examines their complex interaction. Specifically, we examined how class imbalance as an AI bias affects people's ability to appropriately rely on an AI-based decision-support system, and how it interacts with base rate neglect as a human bias. In a within-subject online study (N= 46), participants classified three diseases using an AI-based decision-support system trained on either a balanced or unbalanced dataset. We found that class imbalance disrupted participants' calibration of AI reliance. Moreover, we observed mutually reinforcing effects between class imbalance and base rate neglect, offering evidence of a compound human-AI bias. Based on these findings, we advocate for an interactionist perspective and further research into the mutually reinforcing effects of biases in human-AI interaction.", "AI": {"tldr": "\u672c\u7814\u7a76\u8003\u5bdf\u4e86AI\u548c\u4eba\u7c7b\u5728\u51b3\u7b56\u4e2d\u76f8\u4e92\u4f5c\u7528\u7684\u504f\u89c1\uff0c\u53d1\u73b0\u7c7b\u4e0d\u5e73\u8861\u4e0e\u57fa\u7840\u6bd4\u7387\u5ffd\u89c6\u76f8\u4e92\u5f71\u54cd\uff0c\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "motivation": "\u63a2\u8ba8\u4eba\u7c7b\u4e0e\u4eba\u5de5\u667a\u80fd\u5728\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u504f\u89c1\u76f8\u4e92\u4f5c\u7528\uff0c\u7279\u522b\u662fAI\u504f\u89c1\u4e0e\u4eba\u7c7b\u504f\u89c1\u7684\u590d\u6742\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u4e00\u9879\u5728\u7ebf\u5b9e\u9a8c\uff0c\u53c2\u4e0e\u8005\u4f7f\u7528\u57fa\u4e8eAI\u7684\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u5bf9\u4e09\u79cd\u75be\u75c5\u8fdb\u884c\u5206\u7c7b\uff0c\u5b9e\u9a8c\u8bbe\u8ba1\u4e3a\u88ab\u8bd5\u5185\uff08N=46\uff09\uff0c\u5e76\u5bf9\u6bd4\u4e86\u5e73\u8861\u4e0e\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u7c7b\u4e0d\u5e73\u8861\u5f71\u54cd\u4e86\u53c2\u4e0e\u8005\u5bf9AI\u652f\u6301\u7684\u4fe1\u4efb\uff0c\u5e76\u4e14\u7c7b\u4e0d\u5e73\u8861\u4e0e\u57fa\u7840\u6bd4\u7387\u5ffd\u89c6\u4e4b\u95f4\u5b58\u5728\u76f8\u4e92\u52a0\u5f3a\u6548\u5e94\uff0c\u5373\u590d\u5408\u504f\u89c1\u3002", "conclusion": "\u672c\u7814\u7a76\u53d1\u73b0\u4e86\u4eba\u7c7b\u4e0eAI\u4e92\u52a8\u4e2d\u5b58\u5728\u7684\u590d\u5408\u504f\u89c1\u73b0\u8c61\uff0c\u5e76\u5efa\u8bae\u8fdb\u4e00\u6b65\u7814\u7a76\u8fd9\u4e00\u76f8\u4e92\u4f5c\u7528\u3002"}}
{"id": "2511.14341", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.14341", "abs": "https://arxiv.org/abs/2511.14341", "authors": ["Michael Milford", "Tobias Fischer"], "title": "Going Places: Place Recognition in Artificial and Natural Systems", "comment": null, "summary": "Place recognition, the ability to identify previously visited locations, is critical for both biological navigation and autonomous systems. This review synthesizes findings from robotic systems, animal studies, and human research to explore how different systems encode and recall place. We examine the computational and representational strategies employed across artificial systems, animals, and humans, highlighting convergent solutions such as topological mapping, cue integration, and memory management. Animal systems reveal evolved mechanisms for multimodal navigation and environmental adaptation, while human studies provide unique insights into semantic place concepts, cultural influences, and introspective capabilities. Artificial systems showcase scalable architectures and data-driven models. We propose a unifying set of concepts by which to consider and develop place recognition mechanisms and identify key challenges such as generalization, robustness, and environmental variability. This review aims to foster innovations in artificial localization by connecting future developments in artificial place recognition systems to insights from both animal navigation research and human spatial cognition studies.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u673a\u5668\u4eba\u3001\u52a8\u7269\u548c\u4eba\u7c7b\u5728\u5730\u70b9\u8bc6\u522b\u65b9\u9762\u7684\u7814\u7a76\uff0c\u63a2\u8ba8\u4e86\u5176\u7f16\u7801\u548c\u56de\u5fc6\u65b9\u5f0f\uff0c\u91cd\u70b9\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u6982\u5ff5\u548c\u9762\u4e34\u7684\u6311\u6218\uff0c\u65e8\u5728\u4fc3\u8fdb\u4eba\u5de5\u5b9a\u4f4d\u7684\u521b\u65b0\u3002", "motivation": "\u63a2\u7d22\u751f\u7269\u5bfc\u822a\u548c\u81ea\u4e3b\u7cfb\u7edf\u4e2d\uff0c\u5730\u70b9\u8bc6\u522b\u7684\u91cd\u8981\u6027\u53ca\u5176\u5b9e\u73b0\u65b9\u5f0f\u3002", "method": "\u901a\u8fc7\u5408\u6210\u673a\u5668\u4eba\u7cfb\u7edf\u3001\u52a8\u7269\u7814\u7a76\u548c\u4eba\u7c7b\u7814\u7a76\u7684\u53d1\u73b0\uff0c\u5206\u6790\u4e0d\u540c\u7cfb\u7edf\u5982\u4f55\u7f16\u7801\u548c\u56de\u5fc6\u5730\u70b9\u3002", "result": "\u63ed\u793a\u4e86\u4eba\u5de5\u7cfb\u7edf\u3001\u52a8\u7269\u548c\u4eba\u7c7b\u5728\u5730\u70b9\u8bc6\u522b\u673a\u5236\u4e0a\u7684\u63a5\u8fd1\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u51fa\u4e86\u4e00\u5957\u7edf\u4e00\u7684\u6982\u5ff5\uff0c\u5e76\u8bc6\u522b\u51fa\u5982\u6cdb\u5316\u3001\u7a33\u5065\u6027\u548c\u73af\u5883\u53d8\u5f02\u6027\u7b49\u4e3b\u8981\u6311\u6218\u3002", "conclusion": "\u672c\u7efc\u8ff0\u65e8\u5728\u901a\u8fc7\u6574\u5408\u673a\u5668\u4eba\u3001\u52a8\u7269\u548c\u4eba\u7c7b\u7814\u7a76\u7684\u53d1\u73b0\uff0c\u4fc3\u8fdb\u4eba\u5de5\u5b9a\u4f4d\u7684\u521b\u65b0\uff0c\u8fde\u63a5\u672a\u6765\u7684\u4eba\u5de5\u5730\u70b9\u8bc6\u522b\u7cfb\u7edf\u4e0e\u52a8\u7269\u5bfc\u822a\u548c\u4eba\u7c7b\u7a7a\u95f4\u8ba4\u77e5\u7814\u7a76\u7684\u89c1\u89e3\u3002"}}
{"id": "2511.14636", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.14636", "abs": "https://arxiv.org/abs/2511.14636", "authors": ["Nyah Speicher", "Prashant Chandrasekar"], "title": "Theoretical basis for code presentation: A case for cognitive load", "comment": "10 pages, 1 figure", "summary": "Evidence supports that reducing cognitive load (CL) improves task performance for people of all abilities. This effect is specifically important for blind-and-low-vision (BLV) individuals because they cannot rely on many common methods of managing CL, which are frequently vision-based techniques. Current accessible \"solutions\" for BLV developers only sporadically consider CL in their design. There isn't a way to know whether CL is being alleviated by them. Neither do we know if alleviating CL is part of the mechanism behind why these solutions help BLV people. Using a strong foundation in psychological sciences, we identify aspects of CL that impact performance and learning in programming. These aspects are then examined when evaluating existing solutions for programming sub-tasks for BLV users. We propose an initial design \"recommendations\" for presentation of code which, when followed, will reduce cognitive load for BLV developers.", "AI": {"tldr": "\u51cf\u5c11\u8ba4\u77e5\u8d1f\u62c5\u5bf9\u6240\u6709\u80fd\u529b\u7684\u4eba\u90fd\u80fd\u63d0\u5347\u4efb\u52a1\u8868\u73b0\uff0c\u7279\u522b\u5bf9\u76f2\u4eba\u4e0e\u89c6\u529b\u4f4e\u4e0b\u8005\u81f3\u5173\u91cd\u8981\u3002\u63d0\u51fa\u9488\u5bf9\u4ee3\u7801\u5c55\u793a\u7684\u8bbe\u8ba1\u5efa\u8bae\u4ee5\u51cf\u8f7b\u8ba4\u77e5\u8d1f\u62c5\u3002", "motivation": "\u76f2\u4eba\u548c\u89c6\u529b\u4f4e\u4e0b\u4eba\u58eb\u65e0\u6cd5\u4f9d\u8d56\u5e38\u89c1\u7684\u89c6\u89c9\u57fa\u7840\u65b9\u6cd5\u6765\u7ba1\u7406\u8ba4\u77e5\u8d1f\u62c5\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u51cf\u8f7b\u8ba4\u77e5\u8d1f\u62c5\u6765\u63d0\u5347\u4ed6\u4eec\u7684\u4efb\u52a1\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u5fc3\u7406\u79d1\u5b66\u7814\u7a76\uff0c\u8bc6\u522b\u5f71\u54cd\u7f16\u7a0b\u8868\u73b0\u548c\u5b66\u4e60\u7684\u8ba4\u77e5\u8d1f\u62c5\u7684\u5404\u4e2a\u65b9\u9762\uff0c\u5e76\u8bc4\u4f30\u73b0\u6709\u7684\u7f16\u7a0b\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u901a\u8fc7\u8bc4\u4f30\u73b0\u6709\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u51fa\u521d\u6b65\u8bbe\u8ba1\u5efa\u8bae\uff0c\u65e8\u5728\u964d\u4f4e\u8ba4\u77e5\u8d1f\u62c5\u3002", "conclusion": "\u901a\u8fc7\u63d0\u51fa\u8bbe\u8ba1\u5efa\u8bae\uff0c\u80fd\u591f\u6709\u6548\u964d\u4f4e\u76f2\u4eba\u548c\u89c6\u529b\u4f4e\u4e0b\u5f00\u53d1\u8005\u7684\u8ba4\u77e5\u8d1f\u62c5\uff0c\u8fdb\u800c\u6539\u5584\u4ed6\u4eec\u7684\u7f16\u7a0b\u4efb\u52a1\u8868\u73b0\u3002"}}
{"id": "2511.14393", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.14393", "abs": "https://arxiv.org/abs/2511.14393", "authors": ["Svetlana Seliunina", "Daniel Schleich", "Sven Behnke"], "title": "Perception-aware Exploration for Consumer-grade UAVs", "comment": null, "summary": "In our work, we extend the current state-of-the-art approach for autonomous multi-UAV exploration to consumer-level UAVs, such as the DJI Mini 3 Pro. We propose a pipeline that selects viewpoint pairs from which the depth can be estimated and plans the trajectory that satisfies motion constraints necessary for odometry estimation. For the multi-UAV exploration, we propose a semi-distributed communication scheme that distributes the workload in a balanced manner. We evaluate our model performance in simulation for different numbers of UAVs and prove its ability to safely explore the environment and reconstruct the map even with the hardware limitations of consumer-grade UAVs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6d88\u8d39\u7ea7\u65e0\u4eba\u673a\u7684\u591a\u65e0\u4eba\u673a\u81ea\u4e3b\u63a2\u7d22\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73af\u5883\u63a2\u7d22\u548c\u5730\u56fe\u91cd\u5efa\u95ee\u9898\u3002", "motivation": "\u5c06\u6700\u5148\u8fdb\u7684\u65e0\u4eba\u673a\u81ea\u4e3b\u63a2\u7d22\u65b9\u6cd5\u6269\u5c55\u5230\u6d88\u8d39\u7ea7\u65e0\u4eba\u673a\uff0c\u63d0\u5347\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u3002", "method": "\u901a\u8fc7\u9009\u62e9\u89c6\u70b9\u5bf9\u4ee5\u4f30\u8ba1\u6df1\u5ea6\uff0c\u5e76\u89c4\u5212\u6ee1\u8db3\u8fd0\u52a8\u7ea6\u675f\u7684\u8f68\u8ff9\uff0c\u6765\u6269\u5c55\u591a\u65e0\u4eba\u673a\u63a2\u7d22\u7684\u65b9\u6cd5\u3002", "result": "\u5728\u4e0d\u540c\u6570\u91cf\u7684\u65e0\u4eba\u673a\u7684\u4eff\u771f\u8bc4\u4f30\u4e2d\uff0c\u6a21\u578b\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u5730\u5206\u914d\u5de5\u4f5c\u8d1f\u8f7d\u3002", "conclusion": "\u8be5\u6a21\u578b\u80fd\u591f\u5728\u6d88\u8d39\u8005\u7ea7\u65e0\u4eba\u673a\u7684\u786c\u4ef6\u9650\u5236\u4e0b\u5b89\u5168\u5730\u63a2\u7d22\u73af\u5883\u5e76\u91cd\u5efa\u5730\u56fe\u3002"}}
{"id": "2511.14661", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.14661", "abs": "https://arxiv.org/abs/2511.14661", "authors": ["Diana Romero", "Xin Gao", "Daniel Khalkhali", "Salma Elmalaki"], "title": "M-CALLM: Multi-level Context Aware LLM Framework for Group Interaction Prediction", "comment": null, "summary": "This paper explores how large language models can leverage multi-level contextual information to predict group coordination patterns in collaborative mixed reality environments. We demonstrate that encoding individual behavioral profiles, group structural properties, and temporal dynamics as natural language enables LLMs to break through the performance ceiling of statistical models. We build M-CALLM, a framework that transforms multimodal sensor streams into hierarchical context for LLM-based prediction, and evaluate three paradigms (zero-shot prompting, few-shot learning, and supervised fine-tuning) against statistical baselines across intervention mode (real-time prediction) and simulation mode (autoregressive forecasting) Head-to-head comparison on 16 groups (64 participants, ~25 hours) demonstrates that context-aware LLMs achieve 96% accuracy for conversation prediction, a 3.2x improvement over LSTM baselines, while maintaining sub-35ms latency. However, simulation mode reveals brittleness with 83% degradation due to cascading errors. Deep-dive into modality-specific performance shows conversation depends on temporal patterns, proximity benefits from group structure (+6%), while shared attention fails completely (0% recall), exposing architectural limitations. We hope this work spawns new ideas for building intelligent collaborative sensing systems that balance semantic reasoning capabilities with fundamental constraints.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5229\u7528\u591a\u7ea7\u4e0a\u4e0b\u6587\u4fe1\u606f\u5728\u534f\u4f5c\u6df7\u5408\u73b0\u5b9e\u73af\u5883\u4e2d\u9884\u6d4b\u7fa4\u4f53\u534f\u8c03\u6a21\u5f0f\uff0c\u63d0\u51fa\u4e86M-CALLM\u6846\u67b6\uff0c\u8868\u73b0\u4f18\u8d8a\u4f46\u9762\u4e34\u6a21\u62df\u6a21\u5f0f\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5229\u7528\u591a\u5c42\u6b21\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u6765\u9884\u6d4b\u534f\u4f5c\u6df7\u5408\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u7fa4\u4f53\u534f\u8c03\u6a21\u5f0f\u3002", "method": "\u6784\u5efa\u4e86M-CALLM\u6846\u67b6\uff0c\u5c06\u591a\u6a21\u6001\u4f20\u611f\u5668\u6d41\u8f6c\u5316\u4e3a\u5c42\u7ea7\u4e0a\u4e0b\u6587\uff0c\u4ee5\u652f\u6301\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9884\u6d4b\uff0c\u5e76\u8bc4\u4f30\u4e86\u96f6-shot\u63d0\u793a\u3001\u5c11-shot\u5b66\u4e60\u548c\u76d1\u7763\u5fae\u8c03\u4e09\u79cd\u8303\u5f0f\u3002", "result": "\u4e0a\u4e0b\u6587\u611f\u77e5\u7684LLM\u5728\u4f1a\u8bdd\u9884\u6d4b\u4e2d\u5b9e\u73b0\u4e8696%\u7684\u51c6\u786e\u7387\uff0c\u6bd4LSTM\u57fa\u7ebf\u63d0\u9ad8\u4e863.2\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4f4e\u4e8e35\u6beb\u79d2\u7684\u5ef6\u8fdf\u3002\u4f46\u5728\u6a21\u62df\u6a21\u5f0f\u4e0b\uff0c\u7531\u4e8e\u7ea7\u8054\u9519\u8bef\u5bfc\u81f4\u7684\u8106\u5f31\u6027\u8868\u73b0\u51fa83%\u7684\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u6211\u4eec\u5e0c\u671b\u8fd9\u4e00\u5de5\u4f5c\u80fd\u591f\u50ac\u751f\u51fa\u6784\u5efa\u667a\u80fd\u534f\u4f5c\u611f\u77e5\u7cfb\u7edf\u7684\u65b0\u601d\u8def\uff0c\u5e73\u8861\u8bed\u4e49\u63a8\u7406\u80fd\u529b\u4e0e\u57fa\u672c\u7ea6\u675f\u3002"}}
{"id": "2511.14396", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.14396", "abs": "https://arxiv.org/abs/2511.14396", "authors": ["Xiuxiu Qi", "Yu Yang", "Jiannong Cao", "Luyao Bai", "Chongshan Fan", "Chengtai Cao", "Hongpeng Wang"], "title": "Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning", "comment": "Accepted at AAAI 2026, the Project website is available at https://qhemu.github.io/CCoL/", "summary": "Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (e.g., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL's generalization under unseen and noisy object states.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u884c\u4e3a\u514b\u9686\u6846\u67b6CCoL\uff0c\u901a\u8fc7\u6301\u7eed\u7684\u89c6\u89c9\u3001\u8bed\u8a00\u548c\u5185\u90e8\u72b6\u6001\u5171\u5b66\u4e60\uff0c\u514b\u670d\u4e86\u7269\u7406\u4e0d\u8fde\u7eed\u6027\u548c\u8bed\u4e49-\u7269\u7406\u4e0d\u5bf9\u9f50\uff0c\u4ece\u800c\u63d0\u5347\u4eba\u673a\u4ea4\u4e92\u7684\u6027\u80fd\u3002", "motivation": "\u5e94\u5bf9\u884c\u4e3a\u514b\u9686\u5728\u5e8f\u5217\u52a8\u4f5c\u51b3\u7b56\u4e2d\u79ef\u7d2f\u8bef\u5dee\u7684\u6311\u6218\uff0c\u5e76\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u7269\u7406\u4e0d\u8fde\u7eed\u6027\u548c\u8bed\u4e49-\u7269\u7406\u4e0d\u5bf9\u9f50\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u8fde\u7eed\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u5171\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u673a\u5236\u5c06\u8bed\u8a00\u8bed\u4e49\u951a\u5b9a\u5230\u89c6\u89c9\u8fd0\u52a8\u8868\u793a\u4e0a\uff0c\u514b\u670d\u4e86\u8bed\u4e49-\u7269\u7406\u4e0d\u5bf9\u9f50\u95ee\u9898\u3002", "result": "CCoL\u5728\u4e09\u4e2a\u4eff\u771f\u73af\u5883\u4e2d\u7684\u5e73\u5747\u76f8\u5bf9\u63d0\u5347\u4e3a8.0%\uff0c\u5728\u4eba\u5de5\u793a\u8303\u7684\u53cc\u624b\u63d2\u5165\u4efb\u52a1\u4e2d\u6700\u9ad8\u8fbe19.2%\u7684\u76f8\u5bf9\u589e\u76ca\u3002", "conclusion": "CCoL\u663e\u8457\u63d0\u9ad8\u4e86\u884c\u4e3a\u514b\u9686\u7684\u6027\u80fd\uff0c\u786e\u4fdd\u4e86\u65f6\u95f4\u4e0a\u4e00\u81f4\u7684\u6267\u884c\u548c\u7ec6\u7c92\u5ea6\u7684\u8bed\u4e49\u5bf9\u9f50\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u9a8c\u8bc1\u5176\u5728\u672a\u77e5\u548c\u5608\u6742\u5bf9\u8c61\u72b6\u6001\u4e0b\u7684\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2511.14427", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14427", "abs": "https://arxiv.org/abs/2511.14427", "authors": ["Rickmer Krohn", "Vignesh Prasad", "Gabriele Tiboni", "Georgia Chalvatzaki"], "title": "Self-Supervised Multisensory Pretraining for Contact-Rich Robot Reinforcement Learning", "comment": "9 pages, 10 figures, preprint", "summary": "Effective contact-rich manipulation requires robots to synergistically leverage vision, force, and proprioception. However, Reinforcement Learning agents struggle to learn in such multisensory settings, especially amidst sensory noise and dynamic changes. We propose MultiSensory Dynamic Pretraining (MSDP), a novel framework for learning expressive multisensory representations tailored for task-oriented policy learning. MSDP is based on masked autoencoding and trains a transformer-based encoder by reconstructing multisensory observations from only a subset of sensor embeddings, leading to cross-modal prediction and sensor fusion. For downstream policy learning, we introduce a novel asymmetric architecture, where a cross-attention mechanism allows the critic to extract dynamic, task-specific features from the frozen embeddings, while the actor receives a stable pooled representation to guide its actions. Our method demonstrates accelerated learning and robust performance under diverse perturbations, including sensor noise, and changes in object dynamics. Evaluations in multiple challenging, contact-rich robot manipulation tasks in simulation and the real world showcase the effectiveness of MSDP. Our approach exhibits strong robustness to perturbations and achieves high success rates on the real robot with as few as 6,000 online interactions, offering a simple yet powerful solution for complex multisensory robotic control.", "AI": {"tldr": "\u63d0\u51faMSDP\u6846\u67b6\uff0c\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u5728\u591a\u611f\u5b98\u73af\u5883\u4e2d\u8bad\u7ec3\u8868\u73b0\uff0c\u4e14\u5728\u591a\u79cd\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u5353\u8d8a\uff0c\u5b66\u4e60\u901f\u5ea6\u5feb\uff0c\u5bf9\u6270\u52a8\u9c81\u68d2\u6027\u5f3a\u3002", "motivation": "\u6709\u6548\u7684\u63a5\u89e6\u4e30\u5bcc\u64cd\u63a7\u9700\u8981\u673a\u5668\u4eba\u534f\u540c\u5229\u7528\u89c6\u89c9\u3001\u529b\u91cf\u548c\u672c\u4f53\u611f\u77e5\uff0c\u4f46\u5f3a\u5316\u5b66\u4e60\u5728\u591a\u79cd\u611f\u5b98\u73af\u5883\u4e2d\u5b66\u4e60\u65f6\u56f0\u96be\u91cd\u91cd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f02\u6b65\u67b6\u6784\u7684\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u8de8\u6ce8\u610f\u529b\u673a\u5236\u6765\u63d0\u53d6\u52a8\u6001\u4efb\u52a1\u7279\u5f81\uff0c\u5e76\u4e3a\u884c\u52a8\u63d0\u4f9b\u7a33\u5b9a\u7684\u8868\u793a\u3002", "result": "MSDP\u65b9\u6cd5\u5c55\u793a\u4e86\u5728\u591a\u6837\u6270\u52a8\u4e0b\u52a0\u901f\u5b66\u4e60\u548c\u7a33\u5065\u8868\u73b0\uff0c\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u4ec5\u97006000\u6b21\u5728\u7ebf\u4ea4\u4e92\u5c31\u80fd\u8fbe\u5230\u9ad8\u6210\u529f\u7387\u3002", "conclusion": "MSDP\u65b9\u6cd5\u5728\u591a\u79cd\u6311\u6218\u6027\u7684\u63a5\u89e6\u4e30\u5bcc\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u662f\u5728\u9762\u5bf9\u4f20\u611f\u5668\u566a\u58f0\u548c\u7269\u4f53\u52a8\u6001\u53d8\u5316\u65f6\u3002"}}
{"id": "2511.14432", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.14432", "abs": "https://arxiv.org/abs/2511.14432", "authors": ["Marcela Gon\u00e7alves dos Santos", "Sylvain Hall\u00e9", "F\u00e1bio Petrillo"], "title": "Mutation Testing for Industrial Robotic Systems", "comment": "In Proceedings FMAS 2025, arXiv:2511.13245", "summary": "Industrial robotic systems (IRS) are increasingly deployed in diverse environments, where failures can result in severe accidents and costly downtime. Ensuring the reliability of the software controlling these systems is therefore critical. Mutation testing, a technique widely used in software engineering, evaluates the effectiveness of test suites by introducing small faults, or mutants, into the code. However, traditional mutation operators are poorly suited to robotic programs, which involve message-based commands and interactions with the physical world. This paper explores the adaptation of mutation testing to IRS by defining domain-specific mutation operators that capture the semantics of robot actions and sensor readings. We propose a methodology for generating meaningful mutants at the level of high-level read and write operations, including movement, gripper actions, and sensor noise injection. An empirical study on a pick-and-place scenario demonstrates that our approach produces more informative mutants and reduces the number of invalid or equivalent cases compared to conventional operators. Results highlight the potential of mutation testing to enhance test suite quality and contribute to safer, more reliable industrial robotic systems.", "AI": {"tldr": "\u672c\u8bba\u6587\u63a2\u8ba8\u4e86\u9488\u5bf9\u5de5\u4e1a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u7a81\u53d8\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u7279\u5b9a\u7684\u7a81\u53d8\u64cd\u4f5c\u7b26\uff0c\u63d0\u9ad8\u4e86\u6d4b\u8bd5\u5957\u4ef6\u7684\u6709\u6548\u6027\u548c\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u968f\u7740\u5de5\u4e1a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u7684\u5e94\u7528\u52a0\u5267\uff0c\u786e\u4fdd\u5176\u8f6f\u4ef6\u7684\u53ef\u9760\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u7684\u7a81\u53d8\u6d4b\u8bd5\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e\u673a\u5668\u4eba\u7a0b\u5e8f\uff0c\u56e0\u6b64\u9700\u8981\u9002\u5e94\u6027\u8c03\u6574\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u7279\u5b9a\u4e8e\u9886\u57df\u7684\u7a81\u53d8\u64cd\u4f5c\u7b26\uff0c\u9488\u5bf9\u673a\u5668\u4eba\u7684\u9ad8\u5c42\u8bfb\u5199\u64cd\u4f5c\u751f\u6210\u6709\u6548\u7684\u7a81\u53d8\u4f53\u3002", "result": "\u5728\u4e00\u4e2a\u6311\u53d6\u548c\u653e\u7f6e\u7684\u573a\u666f\u4e2d\uff0c\u5b9e\u8bc1\u7814\u7a76\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u7a81\u53d8\u4f53\u4fe1\u606f\u91cf\u66f4\u5927\uff0c\u76f8\u8f83\u4f20\u7edf\u64cd\u4f5c\u7b26\u51cf\u5c11\u4e86\u65e0\u6548\u6216\u7b49\u4ef7\u6848\u4f8b\u7684\u6570\u91cf\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u9762\u5411\u5de5\u4e1a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u7a81\u53d8\u6d4b\u8bd5\u65b9\u6cd5\u80fd\u591f\u63d0\u9ad8\u6d4b\u8bd5\u5957\u4ef6\u7684\u8d28\u91cf\uff0c\u4ece\u800c\u589e\u5f3a\u5de5\u4e1a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2511.14434", "categories": ["cs.RO", "cs.LO"], "pdf": "https://arxiv.org/pdf/2511.14434", "abs": "https://arxiv.org/abs/2511.14434", "authors": ["Marlow Fawn", "Matthias Scheutz"], "title": "Achieving Safe Control Online through Integration of Harmonic Control Lyapunov-Barrier Functions with Unsafe Object-Centric Action Policies", "comment": "In Proceedings FMAS 2025, arXiv:2511.13245", "summary": "We propose a method for combining Harmonic Control Lyapunov-Barrier Functions (HCLBFs) derived from Signal Temporal Logic (STL) specifications with any given robot policy to turn an unsafe policy into a safe one with formal guarantees.  The two components are combined via HCLBF-derived safety certificates, thus producing commands that preserve both safety and task-driven behavior.  We demonstrate with a simple proof-of-concept implementation for an object-centric force-based policy trained through reinforcement learning for a movement task of a stationary robot arm that is able to avoid colliding with obstacles on a table top after combining the policy with the safety constraints.  The proposed method can be generalized to more complex specifications and dynamic task settings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u8c10\u6ce2\u63a7\u5236\u674e\u96c5\u666e\u8bfa\u592b-\u969c\u788d\u51fd\u6570\u4e0e\u673a\u5668\u4eba\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u4ee5\u786e\u4fdd\u673a\u5668\u4eba\u5b89\u5168\u6267\u884c\u4efb\u52a1\u3002", "motivation": "\u786e\u4fdd\u673a\u5668\u4eba\u5728\u6267\u884c\u4efb\u52a1\u65f6\u7684\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u9a71\u52a8\u884c\u4e3a\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u57fa\u4e8e\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91\uff08STL\uff09\u89c4\u8303\u7684\u8c10\u6ce2\u63a7\u5236\u674e\u96c5\u666e\u8bfa\u592b-\u969c\u788d\u51fd\u6570\uff08HCLBF\uff09\u4e0e\u4efb\u4f55\u7ed9\u5b9a\u7684\u673a\u5668\u4eba\u7b56\u7565\uff0c\u5229\u7528HCLBF\u6d3e\u751f\u7684\u5b89\u5168\u8bc1\u4e66\u6765\u751f\u6210\u547d\u4ee4\u3002", "result": "\u901a\u8fc7\u4e00\u4e2a\u7b80\u5355\u7684\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u73b0\uff0c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u9759\u6001\u673a\u5668\u4eba\u81c2\u79fb\u52a8\u4efb\u52a1\u4e2d\u907f\u514d\u78b0\u649e\u7684\u6548\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u5c06\u4e0d\u5b89\u5168\u7684\u673a\u5668\u4eba\u7b56\u7565\u8f6c\u53d8\u4e3a\u5b89\u5168\u7684\u7b56\u7565\uff0c\u5e76\u5177\u6709\u6b63\u5f0f\u4fdd\u8bc1\u3002"}}
{"id": "2511.14458", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.14458", "abs": "https://arxiv.org/abs/2511.14458", "authors": ["Michelle Mattille", "Alexandre Mesot", "Miriam Weisskopf", "Nicole Ochsenbein-K\u00f6lble", "Ueli Moehrlen", "Bradley J. Nelson", "Quentin Boehler"], "title": "Advancing Minimally Invasive Precision Surgery in Open Cavities with Robotic Flexible Endoscopy", "comment": null, "summary": "Flexible robots hold great promise for enhancing minimally invasive surgery (MIS) by providing superior dexterity, precise control, and safe tissue interaction. Yet, translating these advantages into endoscopic interventions within open cavities remains challenging. The lack of anatomical constraints and the inherent flexibility of such devices complicate their control, while the limited field of view of endoscopes restricts situational awareness. We present a robotic platform designed to overcome these challenges and demonstrate its potential in fetoscopic laser coagulation, a complex MIS procedure typically performed only by highly experienced surgeons. Our system combines a magnetically actuated flexible endoscope with teleoperated and semi-autonomous navigation capabilities for performing targeted laser ablations. To enhance surgical awareness, the platform reconstructs real-time mosaics of the endoscopic scene, providing an extended and continuous visual context. The ability of this system to address the key limitations of MIS in open spaces is validated in vivo in an ovine model.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u65e8\u5728\u63d0\u5347\u5fae\u521b\u624b\u672f\u5728\u5f00\u653e\u8154\u4f53\u7684\u5e94\u7528\u80fd\u529b\uff0c\u901a\u8fc7\u7ed3\u5408\u67d4\u6027\u5185\u7aa5\u955c\u548c\u521b\u65b0\u7684\u5bfc\u822a\u6280\u672f\uff0c\u63d0\u9ad8\u624b\u672f\u7684\u7075\u6d3b\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u65e8\u5728\u63d0\u5347\u5fae\u521b\u624b\u672f\u4e2d\u7684\u7075\u6d3b\u6027\u548c\u63a7\u5236\u7cbe\u5ea6\uff0c\u514b\u670d\u5f53\u524d\u6280\u672f\u5728\u5f00\u653e\u8154\u4f53\u5e72\u9884\u4e2d\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408\u78c1\u529b\u9a71\u52a8\u7684\u67d4\u6027\u5185\u7aa5\u955c\u4e0e\u9065\u64cd\u4f5c\u548c\u534a\u81ea\u4e3b\u5bfc\u822a\u80fd\u529b\uff0c\u6267\u884c\u5b9a\u5411\u6fc0\u5149\u70e7\u707c\u3002", "result": "\u8be5\u7cfb\u7edf\u7ecf\u8fc7\u5728\u7f8a\u6a21\u578b\u4e2d\u7684\u5728\u4f53\u9a8c\u8bc1\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u5fae\u521b\u624b\u672f\u7684\u5173\u952e\u9650\u5236\u3002", "conclusion": "\u8be5\u673a\u5668\u4eba\u5e73\u53f0\u5728\u590d\u6742\u7684\u80ce\u5185\u6fc0\u5149\u51dd\u56fa\u624b\u672f\u4e2d\u5c55\u73b0\u4e86\u5176\u6f5c\u529b\uff0c\u89e3\u51b3\u4e86\u5f00\u653e\u8154\u4f53\u5185\u5fae\u521b\u624b\u672f\u7684\u5173\u952e\u9650\u5236\u3002"}}
{"id": "2511.14504", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.14504", "abs": "https://arxiv.org/abs/2511.14504", "authors": ["Jan Quenzel", "Valerij Sekin", "Daniel Schleich", "Alexander Miller", "Merlin Stampa", "Norbert Pahlke", "Christof R\u00f6hrig", "Sven Behnke"], "title": "Aerial Assistance System for Automated Firefighting during Turntable Ladder Operations", "comment": "7 pages, Presented at IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR), Galway, Ireland, 2025", "summary": "Fires in industrial facilities pose special challenges to firefighters, e.g., due to the sheer size and scale of the buildings. The resulting visual obstructions impair firefighting accuracy, further compounded by inaccurate assessments of the fire's location. Such imprecision simultaneously increases the overall damage and prolongs the fire-brigades operation unnecessarily.\n  We propose an automated assistance system for firefighting using a motorized fire monitor on a turntable ladder with aerial support from an unmanned aerial vehicle (UAV). The UAV flies autonomously within an obstacle-free flight funnel derived from geodata, detecting and localizing heat sources. An operator supervises the operation on a handheld controller and selects a fire target in reach. After the selection, the UAV automatically plans and traverses between two triangulation poses for continued fire localization. Simultaneously, our system steers the fire monitor to ensure the water jet reaches the detected heat source. In preliminary tests, our assistance system successfully localized multiple heat sources and directed a water jet towards the fires.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u706d\u706b\u8f85\u52a9\u7cfb\u7edf\uff0c\u7ed3\u5408\u7535\u52a8\u55b7\u5634\u548c\u65e0\u4eba\u673a\uff0c\u4ee5\u63d0\u9ad8\u5de5\u4e1a\u706b\u707e\u7684\u706d\u706b\u7cbe\u5ea6\u4e0e\u6548\u7387\u3002", "motivation": "\u5de5\u4e1a\u8bbe\u65bd\u7684\u706b\u707e\u5bf9\u6d88\u9632\u5458\u6784\u6210\u7279\u6b8a\u6311\u6218\uff0c\u5efa\u7b51\u7269\u7684\u89c4\u6a21\u548c\u89c6\u89c9\u969c\u788d\u5bfc\u81f4\u706d\u706b\u7cbe\u5ea6\u964d\u4f4e\uff0c\u589e\u52a0\u4e86\u635f\u5bb3\u5e76\u5ef6\u957f\u4e86\u6d88\u9632\u961f\u7684\u4f5c\u4e1a\u65f6\u95f4\u3002", "method": "\u4f7f\u7528\u7535\u52a8\u55b7\u5634\u4e0e\u65e0\u4eba\u673a\u7684\u7ec4\u5408\uff0c\u5efa\u7acb\u4e00\u4e2a\u57fa\u4e8e\u5730\u7406\u6570\u636e\u7684\u969c\u788d\u7269-free\u98de\u884c\u901a\u9053\uff0c\u6765\u81ea\u52a8\u5b9a\u4f4d\u4e0e\u8ddf\u8e2a\u706b\u6e90\u3002\u64cd\u4f5c\u5458\u901a\u8fc7\u624b\u6301\u63a7\u5236\u5668\u76d1\u63a7\u64cd\u4f5c\uff0c\u5e76\u80fd\u9009\u62e9\u53ef\u8fbe\u5230\u7684\u706d\u706b\u76ee\u6807\u3002", "result": "\u5728\u521d\u6b65\u6d4b\u8bd5\u4e2d\uff0c\u7cfb\u7edf\u6210\u529f\u5b9a\u4f4d\u591a\u4e2a\u70ed\u6e90\u5e76\u6307\u5f15\u6c34\u55b7\u5634\u5b9e\u65bd\u706d\u706b\u3002", "conclusion": "\u8be5\u81ea\u52a8\u5316\u8f85\u52a9\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u5b9a\u4f4d\u591a\u4e2a\u70ed\u6e90\u5e76\u6210\u529f\u6307\u5f15\u6c34\u55b7\u5634\u5411\u706b\u6e90\u55b7\u5c04\u6c34\u6d41\uff0c\u63d0\u5347\u4e86\u706d\u706b\u7cbe\u5ea6\u4e0e\u6548\u7387\u3002"}}
{"id": "2511.14565", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14565", "abs": "https://arxiv.org/abs/2511.14565", "authors": ["Minyoung Hwang", "Alexandra Forsey-Smerek", "Nathaniel Dennler", "Andreea Bobu"], "title": "Masked IRL: LLM-Guided Reward Disambiguation from Demonstrations and Language", "comment": null, "summary": "Robots can adapt to user preferences by learning reward functions from demonstrations, but with limited data, reward models often overfit to spurious correlations and fail to generalize. This happens because demonstrations show robots how to do a task but not what matters for that task, causing the model to focus on irrelevant state details. Natural language can more directly specify what the robot should focus on, and, in principle, disambiguate between many reward functions consistent with the demonstrations. However, existing language-conditioned reward learning methods typically treat instructions as simple conditioning signals, without fully exploiting their potential to resolve ambiguity. Moreover, real instructions are often ambiguous themselves, so naive conditioning is unreliable. Our key insight is that these two input types carry complementary information: demonstrations show how to act, while language specifies what is important. We propose Masked Inverse Reinforcement Learning (Masked IRL), a framework that uses large language models (LLMs) to combine the strengths of both input types. Masked IRL infers state-relevance masks from language instructions and enforces invariance to irrelevant state components. When instructions are ambiguous, it uses LLM reasoning to clarify them in the context of the demonstrations. In simulation and on a real robot, Masked IRL outperforms prior language-conditioned IRL methods by up to 15% while using up to 4.7 times less data, demonstrating improved sample-efficiency, generalization, and robustness to ambiguous language. Project page: https://MIT-CLEAR-Lab.github.io/Masked-IRL and Code: https://github.com/MIT-CLEAR-Lab/Masked-IRL", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u5939\u5c42\u9006\u5f3a\u5316\u5b66\u4e60\uff08Masked IRL\uff09\uff0c\u7ed3\u5408\u6f14\u793a\u548c\u8bed\u8a00\u6307\u4ee4\u5728\u5956\u52b1\u5efa\u6a21\u4e2d\u7684\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u9ad8\u6837\u672c\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u8bed\u8a00\u7684\u5956\u52b1\u5b66\u4e60\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u6307\u4ee4\u7684\u6f5c\u529b\u6765\u89e3\u51b3\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u800c\u6a21\u7cca\u7684\u6307\u4ee4\u4f7f\u5f97\u7b80\u5355\u7684\u6761\u4ef6\u5316\u65b9\u6cd5\u4e0d\u53ef\u9760\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Masked Inverse Reinforcement Learning (Masked IRL) \u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7ed3\u5408\u6f14\u793a\u548c\u8bed\u8a00\u6307\u4ee4\u7684\u4f18\u52bf\uff0c\u63a8\u65ad\u72b6\u6001\u76f8\u5173\u6027\u63a9\u7801\uff0c\u5e76\u5728\u6a21\u7cca\u6307\u4ee4\u7684\u4e0a\u4e0b\u6587\u4e2d\u8fdb\u884c\u63a8\u7406\u4ee5\u6f84\u6e05\u6307\u4ee4\u3002", "result": "Masked IRL \u5728\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u4f18\u4e8e\u5148\u524d\u7684\u8bed\u8a00\u6761\u4ef6\u9006\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u9ad8\u8fbe 15% \u7684\u63d0\u5347\uff0c\u540c\u65f6\u4f7f\u7528\u7684\u6570\u636e\u91cf\u51cf\u5c114.7\u500d\u3002", "conclusion": "Masked IRL \u5728\u591a\u4e2a\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u4e4b\u524d\u7684\u8bed\u8a00\u6761\u4ef6\u9006\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u8fbe\u5230\u4e86\u63d0\u9ad8\u6837\u672c\u6548\u7387\u3001\u6cdb\u5316\u80fd\u529b\u548c\u5bf9\u6a21\u7cca\u8bed\u8a00\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.14592", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14592", "abs": "https://arxiv.org/abs/2511.14592", "authors": ["Xianhui Meng", "Yuchen Zhang", "Zhijian Huang", "Zheng Lu", "Ziling Ji", "Yaoyao Yin", "Hongyuan Zhang", "Guangfeng Jiang", "Yandan Lin", "Long Chen", "Hangjun Ye", "Li Zhang", "Jun Liu", "Xiaoshuai Hao"], "title": "Is Your VLM for Autonomous Driving Safety-Ready? A Comprehensive Benchmark for Evaluating External and In-Cabin Risks", "comment": null, "summary": "Vision-Language Models (VLMs) show great promise for autonomous driving, but their suitability for safety-critical scenarios is largely unexplored, raising safety concerns. This issue arises from the lack of comprehensive benchmarks that assess both external environmental risks and in-cabin driving behavior safety simultaneously. To bridge this critical gap, we introduce DSBench, the first comprehensive Driving Safety Benchmark designed to assess a VLM's awareness of various safety risks in a unified manner. DSBench encompasses two major categories: external environmental risks and in-cabin driving behavior safety, divided into 10 key categories and a total of 28 sub-categories. This comprehensive evaluation covers a wide range of scenarios, ensuring a thorough assessment of VLMs' performance in safety-critical contexts. Extensive evaluations across various mainstream open-source and closed-source VLMs reveal significant performance degradation under complex safety-critical situations, highlighting urgent safety concerns. To address this, we constructed a large dataset of 98K instances focused on in-cabin and external safety scenarios, showing that fine-tuning on this dataset significantly enhances the safety performance of existing VLMs and paves the way for advancing autonomous driving technology. The benchmark toolkit, code, and model checkpoints will be publicly accessible.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDSBench\u57fa\u51c6\uff0c\u4ee5\u8bc4\u4f30\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u901a\u8fc7\u6784\u5efa\u6570\u636e\u96c6\u63d0\u9ad8\u5176\u5b89\u5168\u6027\u3002", "motivation": "\u63a2\u7d22VLMs\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\uff0c\u89e3\u51b3\u5f53\u524d\u7f3a\u4e4f\u8bc4\u4f30\u73af\u5883\u98ce\u9669\u548c\u8f66\u5185\u5b89\u5168\u884c\u4e3a\u7684\u57fa\u51c6\u95ee\u9898\u3002", "method": "\u5f15\u5165DSBench\u57fa\u51c6\uff0c\u901a\u8fc7\u8bc4\u4f30\u5916\u90e8\u73af\u5883\u98ce\u9669\u548c\u8f66\u5185\u9a7e\u9a76\u884c\u4e3a\u5b89\u5168\uff0c\u6784\u5efa\u5305\u542b98K\u5b9e\u4f8b\u7684\u5927\u578b\u6570\u636e\u96c6\u5e76\u5bf9\u73b0\u6709VLMs\u8fdb\u884c\u5fae\u8c03\u4ee5\u63d0\u9ad8\u5176\u5b89\u5168\u6027\u80fd\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u5728\u590d\u6742\u5b89\u5168\u5173\u952e\u60c5\u5883\u4e0b\uff0cVLMs\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff1b\u901a\u8fc7\u5fae\u8c03\u6570\u636e\u96c6\uff0c\u5b89\u5168\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "DSBench\u662f\u9996\u4e2a\u7efc\u5408\u9a7e\u9a76\u5b89\u5168\u57fa\u51c6\uff0c\u53ef\u663e\u8457\u63d0\u5347VLMs\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2511.14625", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.14625", "abs": "https://arxiv.org/abs/2511.14625", "authors": ["Qingwei Ben", "Botian Xu", "Kailin Li", "Feiyu Jia", "Wentao Zhang", "Jingping Wang", "Jingbo Wang", "Dahua Lin", "Jiangmiao Pang"], "title": "Gallant: Voxel Grid-based Humanoid Locomotion and Local-navigation across 3D Constrained Terrains", "comment": null, "summary": "Robust humanoid locomotion requires accurate and globally consistent perception of the surrounding 3D environment. However, existing perception modules, mainly based on depth images or elevation maps, offer only partial and locally flattened views of the environment, failing to capture the full 3D structure. This paper presents Gallant, a voxel-grid-based framework for humanoid locomotion and local navigation in 3D constrained terrains. It leverages voxelized LiDAR data as a lightweight and structured perceptual representation, and employs a z-grouped 2D CNN to map this representation to the control policy, enabling fully end-to-end optimization. A high-fidelity LiDAR simulation that dynamically generates realistic observations is developed to support scalable, LiDAR-based training and ensure sim-to-real consistency. Experimental results show that Gallant's broader perceptual coverage facilitates the use of a single policy that goes beyond the limitations of previous methods confined to ground-level obstacles, extending to lateral clutter, overhead constraints, multi-level structures, and narrow passages. Gallant also firstly achieves near 100% success rates in challenging scenarios such as stair climbing and stepping onto elevated platforms through improved end-to-end optimization.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faGallant\uff0c\u4e00\u4e2a\u57fa\u4e8e\u4f53\u7d20\u7f51\u683c\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u7aef\u5230\u7aef\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u5728\u590d\u67423D\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u5bfc\u822a\u548c\u8fd1100%\u7684\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u611f\u77e5\u6a21\u5757\u53ea\u80fd\u63d0\u4f9b\u90e8\u5206\u548c\u5c40\u90e8\u5e73\u5766\u7684\u73af\u5883\u89c6\u56fe\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u5b8c\u6574\u76843D\u7ed3\u6784\u3002", "method": "\u57fa\u4e8e\u4f53\u7d20\u7f51\u683c\u6846\u67b6\uff0c\u5229\u7528\u4f53\u7d20\u5316\u7684LiDAR\u6570\u636e\u548cz\u5206\u7ec4\u76842D CNN\u8fdb\u884c\u63a7\u5236\u7b56\u7565\u6620\u5c04\uff0c\u652f\u6301\u7aef\u5230\u7aef\u4f18\u5316\u3002", "result": "Gallant\u6269\u5927\u4e86\u611f\u77e5\u8986\u76d6\u8303\u56f4\uff0c\u4f7f\u5f97\u5355\u4e00\u7b56\u7565\u80fd\u591f\u5904\u7406\u66f4\u590d\u6742\u7684\u73af\u5883\uff0c\u5305\u62ec\u591a\u4e2a\u9ad8\u5ea6\u7684\u969c\u788d\u7269\u548c\u7a84\u901a\u9053\u3002", "conclusion": "Gallant\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u5bfc\u822a\uff0c\u6210\u529f\u7387\u63a5\u8fd1100%\u3002"}}
{"id": "2511.14659", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14659", "abs": "https://arxiv.org/abs/2511.14659", "authors": ["Chia-Yu Hung", "Navonil Majumder", "Haoyuan Deng", "Liu Renhang", "Yankang Ang", "Amir Zadeh", "Chuan Li", "Dorien Herremans", "Ziwei Wang", "Soujanya Poria"], "title": "NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards", "comment": "https://declare-lab.github.io/nora-1.5", "summary": "Vision--language--action (VLA) models have recently shown promising performance on a variety of embodied tasks, yet they still fall short in reliability and generalization, especially when deployed across different embodiments or real-world environments. In this work, we introduce NORA-1.5, a VLA model built from the pre-trained NORA backbone by adding to it a flow-matching-based action expert. This architectural enhancement alone yields substantial performance gains, enabling NORA-1.5 to outperform NORA and several state-of-the-art VLA models across both simulated and real-world benchmarks. To further improve robustness and task success, we develop a set of reward models for post-training VLA policies. Our rewards combine (i) an action-conditioned world model (WM) that evaluates whether generated actions lead toward the desired goal, and (ii) a deviation-from-ground-truth heuristic that distinguishes good actions from poor ones. Using these reward signals, we construct preference datasets and adapt NORA-1.5 to target embodiments through direct preference optimization (DPO). Extensive evaluations show that reward-driven post-training consistently improves performance in both simulation and real-robot settings, demonstrating significant VLA model-reliability gains through simple yet effective reward models. Our findings highlight NORA-1.5 and reward-guided post-training as a viable path toward more dependable embodied agents suitable for real-world deployment.", "AI": {"tldr": "NORA-1.5\u662f\u4e00\u79cd\u6539\u8fdb\u7684VLA\u6a21\u578b\uff0c\u901a\u8fc7\u67b6\u6784\u589e\u5f3a\u548c\u57fa\u4e8e\u5956\u52b1\u7684\u540e\u671f\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86\u5728\u5404\u79cd\u4f53\u73b0\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u5728\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u663e\u793a\u4e86\u66f4\u9ad8\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u867d\u7136\u73b0\u6709\u7684VLA\u6a21\u578b\u5728\u591a\u79cd\u4f53\u611f\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u826f\u597d\u8868\u73b0\uff0c\u4f46\u5728\u4e0d\u540c\u7684\u4f53\u73b0\u6216\u771f\u5b9e\u73af\u5883\u4e2d\u4ecd\u5b58\u5728\u53ef\u9760\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165NORA-1.5\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684NORA\u9aa8\u5e72\uff0c\u901a\u8fc7\u6dfb\u52a0\u6d41\u5339\u914d\u7684\u52a8\u4f5c\u4e13\u5bb6\u8fdb\u884c\u67b6\u6784\u589e\u5f3a\uff0c\u540c\u65f6\u5f00\u53d1\u4e86\u57fa\u4e8e\u5956\u52b1\u7684\u540e\u671f\u8bad\u7ec3\u7b56\u7565\u3002", "result": "NORA-1.5\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u4f18\u4e8eNORA\u53ca\u5176\u4ed6\u591a\u79cd\u5148\u8fdb\u7684VLA\u6a21\u578b\uff0c\u5e76\u4e14\u901a\u8fc7\u5956\u52b1\u9a71\u52a8\u7684\u540e\u671f\u8bad\u7ec3\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6a21\u578b\u5728\u8fd9\u4e9b\u8bbe\u7f6e\u4e2d\u7684\u8868\u73b0\u3002", "conclusion": "NORA-1.5\u53ca\u5176\u57fa\u4e8e\u5956\u52b1\u7684\u540e\u671f\u8bad\u7ec3\u65b9\u6cd5\u663e\u8457\u589e\u5f3a\u4e86VLA\u6a21\u578b\u7684\u53ef\u9760\u6027\uff0c\u4f7f\u5176\u66f4\u9002\u5408\u771f\u5b9e\u4e16\u754c\u7684\u5e94\u7528\u3002"}}
{"id": "2511.14755", "categories": ["cs.RO", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.14755", "abs": "https://arxiv.org/abs/2511.14755", "authors": ["Albert Lin", "Alessandro Pinto", "Somil Bansal"], "title": "Robust Verification of Controllers under State Uncertainty via Hamilton-Jacobi Reachability Analysis", "comment": "Submitted to the 8th Annual Learning for Dynamics & Control Conference", "summary": "As perception-based controllers for autonomous systems become increasingly popular in the real world, it is important that we can formally verify their safety and performance despite perceptual uncertainty. Unfortunately, the verification of such systems remains challenging, largely due to the complexity of the controllers, which are often nonlinear, nonconvex, learning-based, and/or black-box. Prior works propose verification algorithms that are based on approximate reachability methods, but they often restrict the class of controllers and systems that can be handled or result in overly conservative analyses. Hamilton-Jacobi (HJ) reachability analysis is a popular formal verification tool for general nonlinear systems that can compute optimal reachable sets under worst-case system uncertainties; however, its application to perception-based systems is currently underexplored. In this work, we propose RoVer-CoRe, a framework for the Robust Verification of Controllers via HJ Reachability. To the best of our knowledge, RoVer-CoRe is the first HJ reachability-based framework for the verification of perception-based systems under perceptual uncertainty. Our key insight is to concatenate the system controller, observation function, and the state estimation modules to obtain an equivalent closed-loop system that is readily compatible with existing reachability frameworks. Within RoVer-CoRe, we propose novel methods for formal safety verification and robust controller design. We demonstrate the efficacy of the framework in case studies involving aircraft taxiing and NN-based rover navigation. Code is available at the link in the footnote.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faRoVer-CoRe\u6846\u67b6\uff0c\u901a\u8fc7HJ\u53ef\u8fbe\u6027\u5206\u6790\u5f62\u5f0f\u9a8c\u8bc1\u611f\u77e5\u7cfb\u7edf\u7684\u5b89\u5168\u6027\uff0c\u89e3\u51b3\u4e86\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u5e26\u6765\u7684\u6311\u6218\u3002", "motivation": "\u968f\u7740\u611f\u77e5\u63a7\u5236\u5668\u5728\u81ea\u52a8\u5316\u7cfb\u7edf\u4e2d\u7684\u666e\u53ca\uff0c\u9700\u8981\u5bf9\u5b83\u4eec\u5728\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u5b89\u5168\u6027\u548c\u6027\u80fd\u8fdb\u884c\u6b63\u5f0f\u9a8c\u8bc1\u3002", "method": "\u63d0\u51faRoVer-CoRe\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u63a7\u5236\u5668\u3001\u89c2\u5bdf\u51fd\u6570\u548c\u72b6\u6001\u4f30\u8ba1\u6a21\u5757\u8fde\u63a5\uff0c\u5f62\u6210\u4e00\u4e2a\u517c\u5bb9\u4e8e\u73b0\u6709\u53ef\u8fbe\u6027\u6846\u67b6\u7684\u95ed\u73af\u7cfb\u7edf\uff0c\u518d\u901a\u8fc7\u8fd9\u79cd\u6846\u67b6\u8fdb\u884c\u5f62\u5f0f\u5b89\u5168\u9a8c\u8bc1\u548c\u9c81\u68d2\u63a7\u5236\u5668\u8bbe\u8ba1\u3002", "result": "RoVer-CoRe\u6846\u67b6\u5728\u98de\u673a\u6ed1\u884c\u548c\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u63a2\u6d4b\u5668\u5bfc\u822a\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "RoVer-CoRe\u662f\u9996\u4e2a\u57fa\u4e8eHJ\u53ef\u8fbe\u6027\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u4e0b\u9a8c\u8bc1\u63a7\u5236\u5668\u7684\u5b89\u5168\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2511.14756", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.14756", "abs": "https://arxiv.org/abs/2511.14756", "authors": ["Lai Wei", "Xuanbin Peng", "Ri-Zhao Qiu", "Tianshu Huang", "Xuxin Cheng", "Xiaolong Wang"], "title": "HMC: Learning Heterogeneous Meta-Control for Contact-Rich Loco-Manipulation", "comment": null, "summary": "Learning from real-world robot demonstrations holds promise for interacting with complex real-world environments. However, the complexity and variability of interaction dynamics often cause purely positional controllers to struggle with contacts or varying payloads. To address this, we propose a Heterogeneous Meta-Control (HMC) framework for Loco-Manipulation that adaptively stitches multiple control modalities: position, impedance, and hybrid force-position. We first introduce an interface, HMC-Controller, for blending actions from different control profiles continuously in the torque space. HMC-Controller facilitates both teleoperation and policy deployment. Then, to learn a robust force-aware policy, we propose HMC-Policy to unify different controllers into a heterogeneous architecture. We adopt a mixture-of-experts style routing to learn from large-scale position-only data and fine-grained force-aware demonstrations. Experiments on a real humanoid robot show over 50% relative improvement vs. baselines on challenging tasks such as compliant table wiping and drawer opening, demonstrating the efficacy of HMC.", "AI": {"tldr": "\u63d0\u51fa\u4e86Heterogeneous Meta-Control (HMC)\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u591a\u79cd\u63a7\u5236\u6a21\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u9002\u5e94\u6027\u5f3a\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edf\u4f4d\u7f6e\u63a7\u5236\u5668\u5728\u590d\u6742\u73af\u5883\u4e2d\u4e0e\u63a5\u89e6\u548c\u53d8\u8d1f\u8f7d\u7684\u4ea4\u4e92\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u9002\u5e94\u591a\u79cd\u63a7\u5236\u6a21\u5f0f\u7684\u6846\u67b6\u3002", "method": "\u901a\u8fc7HMC-Controller\u5b9e\u73b0\u4e0d\u540c\u63a7\u5236\u6a21\u5f0f\u7684\u8fde\u7eed\u6df7\u5408\uff0c\u5e76\u91c7\u7528HMC-Policy\u7edf\u4e00\u591a\u79cd\u63a7\u5236\u5668\uff0c\u5b66\u4e60\u5f3a\u5927\u7684\u529b\u89c9\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHMC\u76f8\u8f83\u4e8e\u57fa\u7ebf\u5728\u5982\u8868\u9762\u64e6\u62ed\u548c\u62bd\u5c49\u6253\u5f00\u7b49\u590d\u6742\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8fc750%\u7684\u76f8\u5bf9\u6539\u5584\u3002", "conclusion": "Heterogeneous Meta-Control (HMC)\u6846\u67b6\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
