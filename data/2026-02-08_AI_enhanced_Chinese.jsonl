{"id": "2602.04992", "categories": ["cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.04992", "abs": "https://arxiv.org/abs/2602.04992", "authors": ["Puqi Zhou", "Charles R. Twardy", "Cynthia Lum", "Myeong Lee", "David J. Porfirio", "Michael R. Hieb", "Chris Thomas", "Xuesu Xiao", "Sungsoo Ray Hong"], "title": "Applying Ground Robot Fleets in Urban Search: Understanding Professionals' Operational Challenges and Design Opportunities", "comment": "Under review", "summary": "Urban searches demand rapid, defensible decisions and sustained physical effort under high cognitive and situational load. Incident commanders must plan, coordinate, and document time-critical operations, while field searchers execute evolving tasks in uncertain environments. With recent advances in technology, ground-robot fleets paired with computer-vision-based situational awareness and LLM-powered interfaces offer the potential to ease these operational burdens. However, no dedicated studies have examined how public safety professionals perceive such technologies or envision their integration into existing practices, risking building technically sophisticated yet impractical solutions. To address this gap, we conducted focus-group sessions with eight police officers across five local departments in Virginia. Our findings show that ground robots could reduce professionals' reliance on paper references, mental calculations, and ad-hoc coordination, alleviating cognitive and physical strain in four key challenge areas: (1) partitioning the workforce across multiple search hypotheses, (2) retaining group awareness and situational awareness, (3) building route planning that fits the lost-person profile, and (4) managing cognitive and physical fatigue under uncertainty. We further identify four design opportunities and requirements for future ground-robot fleet integration in public-safety operations: (1) scalable multi-robot planning and control interfaces, (2) agency-specific route optimization, (3) real-time replanning informed by debrief updates, and (4) vision-assisted cueing that preserves operational trust while reducing cognitive workload. We conclude with design implications for deployable, accountable, and human-centered urban-search support systems", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u5730\u9762\u673a\u5668\u4eba\u5728\u57ce\u5e02\u641c\u7d22\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\uff0c\u901a\u8fc7\u4e0e\u8b66\u5bdf\u7684\u7126\u70b9\u5c0f\u7ec4\u8ba8\u8bba\uff0c\u63d0\u51fa\u4e86\u673a\u5668\u4eba\u96c6\u6210\u7684\u8bbe\u8ba1\u673a\u4f1a\u548c\u9700\u6c42\u3002", "motivation": "\u968f\u7740\u79d1\u6280\u7684\u8fdb\u6b65\uff0c\u5730\u9762\u673a\u5668\u4eba\u4e0e\u8ba1\u7b97\u673a\u89c6\u89c9\u53caLLM\u9a71\u52a8\u7684\u63a5\u53e3\u53ef\u4ee5\u51cf\u8f7b\u57ce\u5e02\u641c\u7d22\u4e2d\u516c\u5171\u5b89\u5168\u4e13\u4e1a\u4eba\u58eb\u7684\u64cd\u4f5c\u8d1f\u62c5\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u6280\u672f\u7684\u63a5\u53d7\u5ea6\u548c\u6574\u5408\u7684\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u4e0e\u6765\u81ea\u7ef4\u5409\u5c3c\u4e9a\u5dde\u4e94\u4e2a\u5730\u65b9\u90e8\u95e8\u7684\u516b\u540d\u8b66\u5bdf\u8fdb\u884c\u7126\u70b9\u5c0f\u7ec4\u8ba8\u8bba\uff0c\u83b7\u53d6\u6570\u636e\u548c\u89c1\u89e3\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5730\u9762\u673a\u5668\u4eba\u80fd\u964d\u4f4e\u4e13\u4e1a\u4eba\u58eb\u5bf9\u7eb8\u8d28\u53c2\u8003\u6750\u6599\u548c\u4e34\u65f6\u534f\u8c03\u7684\u4f9d\u8d56\uff0c\u51cf\u8f7b\u5728\u56db\u4e2a\u5173\u952e\u6311\u6218\u9886\u57df\u7684\u8ba4\u77e5\u548c\u8eab\u4f53\u8d1f\u62c5\u3002", "conclusion": "\u672c\u6587\u63a2\u8ba8\u4e86\u672a\u6765\u516c\u5171\u5b89\u5168\u64cd\u4f5c\u4e2d\u5730\u9762\u673a\u5668\u4eba\u961f\u4f0d\u6574\u5408\u7684\u8bbe\u8ba1\u542f\u793a\uff0c\u5f3a\u8c03\u4e86\u9700\u6c42\u53ef\u6269\u5c55\u7684\u591a\u673a\u5668\u4eba\u89c4\u5212\u63a5\u53e3\uff0c\u4ee5\u53ca\u589e\u5f3a\u884c\u52a8\u4fe1\u4efb\u548c\u51cf\u5c11\u8ba4\u77e5\u8d1f\u62c5\u7684\u8bbe\u8ba1\u8981\u6c42\u3002"}}
{"id": "2602.05016", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.05016", "abs": "https://arxiv.org/abs/2602.05016", "authors": ["Eryue Xu", "Tianshi Li"], "title": "From Fragmentation to Integration: Exploring the Design Space of AI Agents for Human-as-the-Unit Privacy Management", "comment": null, "summary": "Managing one's digital footprint is overwhelming, as it spans multiple platforms and involves countless context-dependent decisions. Recent advances in agentic AI offer ways forward by enabling holistic, contextual privacy-enhancing solutions. Building on this potential, we adopted a ''human-as-the-unit'' perspective and investigated users' cross-context privacy challenges through 12 semi-structured interviews. Results reveal that people rely on ad hoc manual strategies while lacking comprehensive privacy controls, highlighting nine privacy-management challenges across applications, temporal contexts, and relationships. To explore solutions, we generated nine AI agent concepts and evaluated them via a speed-dating survey with 116 US participants. The three highest-ranked concepts were all post-sharing management tools with half or full agent autonomy, with users expressing greater trust in AI accuracy than in their own efforts. Our findings highlight a promising design space where users see AI agents bridging the fragments in privacy management, particularly through automated, comprehensive post-sharing remediation of users' digital footprints.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u7528\u6237\u5728\u6570\u5b57\u8db3\u8ff9\u7ba1\u7406\u4e2d\u7684\u9690\u79c1\u6311\u6218\uff0c\u63d0\u51faAI\u4ee3\u7406\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\u4ee5\u63d0\u9ad8\u9690\u79c1\u7ba1\u7406\u7684\u6548\u7387\u548c\u6709\u6548\u6027\u3002", "motivation": "\u9762\u5bf9\u6570\u5b57\u8db3\u8ff9\u7ba1\u7406\u7684\u590d\u6742\u6027\u53ca\u7528\u6237\u5728\u591a\u5e73\u53f0\u4e0a\u9762\u4e34\u7684\u9690\u79c1\u6311\u6218\uff0c\u63a2\u7d22\u5229\u7528AI\u63d0\u4f9b\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u9690\u79c1\u589e\u5f3a\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc712\u6b21\u534a\u7ed3\u6784\u8bbf\u8c08\u63a2\u7d22\u7528\u6237\u7684\u8de8\u4e0a\u4e0b\u6587\u9690\u79c1\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u901f\u5ea6\u7ea6\u4f1a\u8c03\u67e5\u8bc4\u4f30\u4e86\u4e5d\u4e2aAI\u4ee3\u7406\u6982\u5ff5\u3002", "result": "\u7814\u7a76\u786e\u5b9a\u4e86\u4e5d\u4e2a\u9690\u79c1\u7ba1\u7406\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u4e5d\u4e2aAI\u4ee3\u7406\u6982\u5ff5\uff0c\u7279\u522b\u662f\u540e\u5206\u4eab\u7ba1\u7406\u5de5\u5177\u53d7\u5230\u4e86\u7528\u6237\u7684\u9ad8\u5ea6\u8ba4\u53ef\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u7528\u6237\u5bf9AI\u7684\u51c6\u786e\u6027\u8868\u73b0\u51fa\u66f4\u5927\u7684\u4fe1\u4efb\uff0c\u671f\u671bAI\u4ee3\u7406\u80fd\u591f\u81ea\u52a8\u3001\u5168\u9762\u5730\u4fee\u590d\u6570\u5b57\u8db3\u8ff9\uff0c\u6539\u5584\u9690\u79c1\u7ba1\u7406\u4f53\u9a8c\u3002"}}
{"id": "2602.05064", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.05064", "abs": "https://arxiv.org/abs/2602.05064", "authors": ["Yewon Kim", "Stephen Brade", "Alexander Wang", "David Zhou", "Haven Kim", "Bill Wang", "Sung-Ju Lee", "Hugo F Flores Garcia", "Cheng-Zhi Anna Huang", "Chris Donahue"], "title": "A Design Space for Live Music Agents", "comment": "Accepted as a full paper at ACM CHI 2026", "summary": "Live music provides a uniquely rich setting for studying creativity and interaction due to its spontaneous nature. The pursuit of live music agents--intelligent systems supporting real-time music performance and interaction--has captivated researchers across HCI, AI, and computer music for decades, and recent advancements in AI suggest unprecedented opportunities to evolve their design. However, the interdisciplinary nature of music has led to fragmented development across research communities, hindering effective communication and collaborative progress. In this work, we bring together perspectives from these diverse fields to map the current landscape of live music agents. Based on our analysis of 184 systems across both academic literature and video, we develop a comprehensive design space that categorizes dimensions spanning usage contexts, interactions, technologies, and ecosystems. By highlighting trends and gaps in live music agents, our design space offers researchers, designers, and musicians a structured lens to understand existing systems and shape future directions in real-time human-AI music co-creation. We release our annotated systems as a living artifact at https://live-music-agents.github.io.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86184\u4e2a\u73b0\u573a\u97f3\u4e50\u4ee3\u7406\u7cfb\u7edf\uff0c\u5efa\u7acb\u4e86\u4e00\u4e2a\u7efc\u5408\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u4ee5\u4fc3\u8fdb\u7814\u7a76\u8005\u3001\u8bbe\u8ba1\u5e08\u548c\u97f3\u4e50\u5bb6\u4e4b\u95f4\u7684\u6709\u6548\u6c9f\u901a\u548c\u534f\u4f5c\u3002", "motivation": "\u73b0\u573a\u97f3\u4e50\u56e0\u5176\u81ea\u53d1\u6027\u4e3a\u7814\u7a76\u521b\u9020\u529b\u548c\u4e92\u52a8\u63d0\u4f9b\u4e86\u72ec\u7279\u7684\u73af\u5883\uff0c\u7814\u7a76\u5b9e\u65f6\u97f3\u4e50\u4ee3\u7406\u7684\u667a\u80fd\u7cfb\u7edf\u652f\u6301\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u7136\u800c\uff0c\u8de8\u5b66\u79d1\u7684\u7279\u6027\u5bfc\u81f4\u4e86\u7814\u7a76\u793e\u533a\u7684\u53d1\u5c55\u788e\u7247\u5316\u3002", "method": "\u6211\u4eec\u5206\u6790\u4e86184\u4e2a\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e0d\u540c\u9886\u57df\u7684\u89c2\u70b9\uff0c\u5efa\u7acb\u4e86\u4e00\u4e2a\u7efc\u5408\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u5206\u7c7b\u7ef4\u5ea6\u6db5\u76d6\u4f7f\u7528\u60c5\u5883\u3001\u4ea4\u4e92\u3001\u6280\u672f\u548c\u751f\u6001\u7cfb\u7edf\u3002", "result": "\u901a\u8fc7\u6211\u4eec\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u6211\u4eec\u7a81\u51fa\u4e86\u73b0\u573a\u97f3\u4e50\u4ee3\u7406\u7684\u8d8b\u52bf\u548c\u7f3a\u53e3\uff0c\u8fdb\u800c\u4fc3\u8fdb\u5404\u9886\u57df\u4e4b\u95f4\u7684\u6709\u6548\u6c9f\u901a\u548c\u534f\u4f5c\u8fdb\u5c55\u3002", "conclusion": "\u6211\u4eec\u7684\u8bbe\u8ba1\u7a7a\u95f4\u4e3a\u7814\u7a76\u8005\u3001\u8bbe\u8ba1\u5e08\u548c\u97f3\u4e50\u5bb6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u89c6\u89d2\uff0c\u4ee5\u7406\u89e3\u73b0\u6709\u7cfb\u7edf\u5e76\u5851\u9020\u672a\u6765\u7684\u5b9e\u65f6\u4eba\u673a\u97f3\u4e50\u5171\u521b\u65b9\u5411\u3002"}}
{"id": "2602.05093", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.05093", "abs": "https://arxiv.org/abs/2602.05093", "authors": ["He Zhang", "Xinyang Li", "Xingyu Zhou", "Xinyi Fu"], "title": "VR Calm Plus: Coupling a Squeezable Tangible Interaction with Immersive VR for Stress Regulation", "comment": "This work has been conditionally accepted by the ACM CHI 2026 Conference", "summary": "While Virtual Reality (VR) is increasingly employed for stress management, most applications rely heavily on audio-visual stimuli and overlook the therapeutic potential of squeezing engagement. To address this gap, we introduce VR Calm Plus, a multimodal system that integrates a pressure-sensitive plush toy into an interactive VR environment. This interface allows users to dynamically modulate the virtual atmosphere through physical squeezing actions, fostering a deeper sense of embodied relaxation. We evaluated the system with 40 participants using PANAS-X surveys, subjective questionnaires, physiological measures (heart rate, skin conductance, pulse rate variability), and semi-structured interviews. Results demonstrate that, compared to a visual-only baseline, squeeze-based interaction significantly enhances positive affect and perceived relaxation. Physiological data further revealed a state of \"active relaxation\", characterized by greater reductions in heart rate and preserved autonomic flexibility (PRV), alongside sustained emotional engagement (GSR). Our findings highlight the value of coupling tangible input with immersive environments to support emotional well-being and offer design insights for future VR-based mental health tools.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faVR Calm Plus\uff0c\u901a\u8fc7\u7ed3\u5408\u89e6\u89c9\u8f93\u5165\u63d0\u5347VR\u5728\u538b\u529b\u7ba1\u7406\u4e2d\u7684\u5e94\u7528\uff0c\u521d\u6b65\u7ed3\u679c\u663e\u793a\u5176\u80fd\u589e\u5f3a\u653e\u677e\u4e0e\u79ef\u6781\u60c5\u611f\u3002", "motivation": "\u5927\u591a\u6570\u865a\u62df\u73b0\u5b9e\u5e94\u7528\u4f9d\u8d56\u4e8e\u97f3\u9891\u548c\u89c6\u89c9\u523a\u6fc0\uff0c\u4f46\u5ffd\u89c6\u4e86\u6324\u538b\u53c2\u4e0e\u7684\u6cbb\u7597\u6f5c\u529b\uff0c\u56e0\u6b64\u6211\u4eec\u63d0\u51fa\u4e86VR Calm Plus\uff0c\u96c6\u6210\u538b\u529b\u654f\u611f\u6bdb\u7ed2\u73a9\u5177\u548c\u4e92\u52a8VR\u73af\u5883\u3002", "method": "\u672c\u7814\u7a76\u5229\u7528PANAS-X\u95ee\u5377\u3001\u4e3b\u89c2\u95ee\u5377\u3001\u751f\u7406\u6307\u6807\uff08\u5fc3\u7387\u3001\u76ae\u80a4\u7535\u5bfc\u3001\u8109\u7387\u53d8\u5f02\u6027\uff09\u53ca\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u8bc4\u4f30\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "result": "\u4e0e\u4ec5\u89c6\u89c9\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u57fa\u4e8e\u6324\u538b\u7684\u4ea4\u4e92\u663e\u8457\u589e\u5f3a\u4e86\u79ef\u6781\u60c5\u611f\u548c\u611f\u77e5\u7684\u653e\u677e\uff0c\u540c\u65f6\u751f\u7406\u6570\u636e\u63ed\u793a\u4e86\u66f4\u5927\u7684\u5fc3\u7387\u51cf\u5c11\u548c\u4fdd\u6301\u81ea\u4e3b\u7075\u6d3b\u6027\u7684'\u4e3b\u52a8\u653e\u677e'\u72b6\u6001\u3002", "conclusion": "\u901a\u8fc7\u5c06\u53ef\u89e6\u6478\u7684\u8f93\u5165\u4e0e\u6c89\u6d78\u5f0f\u73af\u5883\u76f8\u7ed3\u5408\uff0c\u6211\u4eec\u7684\u7814\u7a76\u5f3a\u8c03\u4e86\u8be5\u65b9\u6cd5\u5bf9\u60c5\u611f\u5e78\u798f\u611f\u7684\u652f\u6301\u4ef7\u503c\uff0c\u5e76\u4e3a\u672a\u6765\u57fa\u4e8eVR\u7684\u5fc3\u7406\u5065\u5eb7\u5de5\u5177\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u89c1\u89e3\u3002"}}
{"id": "2602.05010", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.05010", "abs": "https://arxiv.org/abs/2602.05010", "authors": ["Maia Stiber", "Sameer Khan", "Russell Taylor", "Chien-Ming Huang"], "title": "Signal or 'Noise': Human Reactions to Robot Errors in the Wild", "comment": null, "summary": "In the real world, robots frequently make errors, yet little is known about people's social responses to errors outside of lab settings. Prior work has shown that social signals are reliable and useful for error management in constrained interactions, but it is unclear if this holds in the real world - especially with a non-social robot in repeated and group interactions with successive or propagated errors. To explore this, we built a coffee robot and conducted a public field deployment ($N = 49$). We found that participants consistently expressed varied social signals in response to errors and other stimuli, particularly during group interactions. Our findings suggest that social signals in the wild are rich (with participants volunteering information about the interaction), but \"noisy.\" We discuss lessons, benefits, and challenges for using social signals in real-world HRI.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4eba\u7c7b\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u5bf9\u673a\u5668\u4eba\u9519\u8bef\u7684\u793e\u4ea4\u53cd\u5e94\uff0c\u53d1\u73b0\u793e\u4ea4\u4fe1\u53f7\u4e30\u5bcc\u4f46\u5448\u73b0\u566a\u58f0\uff0c\u5f3a\u8c03\u4e86\u793e\u4ea4\u4fe1\u53f7\u5728\u73b0\u5b9e\u673a\u5668\u4eba-\u4eba\u7c7b\u4e92\u52a8\u4e2d\u7684\u6f5c\u529b\u4e0e\u6311\u6218\u3002", "motivation": "\u7814\u7a76\u4eba\u7c7b\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u5bf9\u673a\u5668\u4eba\u9519\u8bef\u7684\u793e\u4f1a\u53cd\u5e94\uff0c\u4ee5\u4e86\u89e3\u793e\u4ea4\u4fe1\u53f7\u5728\u9519\u8bef\u7ba1\u7406\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u5496\u5561\u673a\u5668\u4eba\u5e76\u5728\u516c\u5171\u573a\u6240\u8fdb\u884c\u90e8\u7f72\uff08\u6837\u672c\u91cf\u4e3a49\uff09\uff0c\u89c2\u5bdf\u53c2\u4e0e\u8005\u5bf9\u9519\u8bef\u7684\u793e\u4f1a\u53cd\u5e94\u3002", "result": "\u53c2\u4e0e\u8005\u5728\u7fa4\u4f53\u4e92\u52a8\u4e2d\u5bf9\u9519\u8bef\u8868\u73b0\u51fa\u5404\u79cd\u793e\u4ea4\u4fe1\u53f7\uff0c\u5e76\u81ea\u613f\u63d0\u4f9b\u6709\u5173\u4e92\u52a8\u7684\u4fe1\u606f\u3002", "conclusion": "\u793e\u4ea4\u4fe1\u53f7\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u8868\u73b0\u4e30\u5bcc\u4f46\u566a\u58f0\u8f83\u5927\uff0c\u4e3a\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u7684\u4e92\u52a8\u63d0\u51fa\u4e86\u53ef\u884c\u6027\u548c\u6311\u6218\u3002"}}
{"id": "2602.05111", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.05111", "abs": "https://arxiv.org/abs/2602.05111", "authors": ["Shri Harini Ramesh", "Foroozan Daneshzand", "Babak Rashidi", "Shriti Raj", "Hariharan Subramonyam", "Fateme Rajabiyazdi"], "title": "Metacognitive Demands and Strategies While Using Off-The-Shelf AI Conversational Agents for Health Information", "comment": null, "summary": "As Artificial Intelligence (AI) conversational agents become widespread, people are increasingly using them for health information seeking. The use of off-the-shelf conversational agents for health information seeking could place high metacognitive demands (the need for extensive monitoring and control of one's own thought process) on individuals, which could compromise their experience of seeking health information. However, currently, the specific demands that arise while using conversational agents for health information seeking, and the strategies people use to cope with those demands, remain unknown. To address these gaps, we conducted a think-aloud study with 15 participants as they sought health information using our off-the-shelf AI conversational agent. We identified the metacognitive demands such systems impose, the strategies people adopt in response, and propose considerations for designing beyond off-the-shelf interfaces to reduce these demands and support better user experiences and affordances in health information seeking.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86AI\u5bf9\u8bdd\u4ee3\u7406\u5728\u5065\u5eb7\u4fe1\u606f\u68c0\u7d22\u4e2d\u7684\u5143\u8ba4\u77e5\u8d1f\u62c5\uff0c\u8bc6\u522b\u5e94\u5bf9\u7b56\u7565\uff0c\u5e76\u63d0\u51fa\u4e86\u8bbe\u8ba1\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u5bf9\u8bdd\u4ee3\u7406\u7684\u666e\u53ca\uff0c\u4eba\u4eec\u8d8a\u6765\u8d8a\u591a\u5730\u4f7f\u7528\u5b83\u4eec\u641c\u7d22\u5065\u5eb7\u4fe1\u606f\uff0c\u4f46\u53ef\u80fd\u4f1a\u589e\u52a0\u4e2a\u4f53\u7684\u5143\u8ba4\u77e5\u8d1f\u62c5\u3002", "method": "\u8fdb\u884c\u4e86\u4e00\u9879\u601d\u7ef4\u5927\u58f0\u5b9e\u9a8c\uff0c\u9080\u8bf715\u540d\u53c2\u4e0e\u8005\u4f7f\u7528\u6211\u4eec\u7684AI\u5bf9\u8bdd\u4ee3\u7406\u8fdb\u884c\u5065\u5eb7\u4fe1\u606f\u68c0\u7d22\u3002", "result": "\u8bc6\u522b\u4e86\u4f7f\u7528\u6b64\u7c7b\u7cfb\u7edf\u65f6\u4ea7\u751f\u7684\u5143\u8ba4\u77e5\u9700\u6c42\u548c\u4eba\u4eec\u5e94\u5bf9\u8fd9\u4e9b\u9700\u6c42\u7684\u7b56\u7565\u3002", "conclusion": "\u63d0\u51fa\u4e86\u8bbe\u8ba1\u80fd\u51cf\u5c11\u8ba4\u77e5\u8d1f\u62c5\u5e76\u6539\u5584\u7528\u6237\u4f53\u9a8c\u7684\u5efa\u8bae\u3002"}}
{"id": "2602.05029", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05029", "abs": "https://arxiv.org/abs/2602.05029", "authors": ["Octavio Arriaga", "Proneet Sharma", "Jichen Guo", "Marc Otto", "Siddhant Kadwe", "Rebecca Adam"], "title": "Differentiable Inverse Graphics for Zero-shot Scene Reconstruction and Robot Grasping", "comment": "Submitted to IEEE Robotics and Automation Letters (RA-L) for review. This version includes the statement required by IEEE for preprints", "summary": "Operating effectively in novel real-world environments requires robotic systems to estimate and interact with previously unseen objects. Current state-of-the-art models address this challenge by using large amounts of training data and test-time samples to build black-box scene representations. In this work, we introduce a differentiable neuro-graphics model that combines neural foundation models with physics-based differentiable rendering to perform zero-shot scene reconstruction and robot grasping without relying on any additional 3D data or test-time samples. Our model solves a series of constrained optimization problems to estimate physically consistent scene parameters, such as meshes, lighting conditions, material properties, and 6D poses of previously unseen objects from a single RGBD image and bounding boxes. We evaluated our approach on standard model-free few-shot benchmarks and demonstrated that it outperforms existing algorithms for model-free few-shot pose estimation. Furthermore, we validated the accuracy of our scene reconstructions by applying our algorithm to a zero-shot grasping task. By enabling zero-shot, physically-consistent scene reconstruction and grasping without reliance on extensive datasets or test-time sampling, our approach offers a pathway towards more data efficient, interpretable and generalizable robot autonomy in novel environments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u5fae\u5206\u795e\u7ecf\u56fe\u5f62\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u5728\u65b0\u73af\u5883\u4e2d\u8fdb\u884c\u96f6-shot\u573a\u666f\u91cd\u5efa\u548c\u6293\u53d6\uff0c\u4e14\u4e0d\u4f9d\u8d56\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5c55\u73b0\u4e86\u4f18\u8d8a\u6027\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u673a\u5668\u4eba\u5728\u65b0\u73af\u5883\u4e2d\u4f30\u8ba1\u548c\u4ea4\u4e92\u4ee5\u524d\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u7684\u80fd\u529b\uff0c\u73b0\u6709\u7684\u4e3b\u6d41\u6a21\u578b\u4f9d\u8d56\u4e8e\u5927\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u548c\u6d4b\u8bd5\u6837\u672c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u5fae\u5206\u7684\u795e\u7ecf\u56fe\u5f62\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u7ed3\u5408\u4e86\u795e\u7ecf\u57fa\u7840\u6a21\u578b\u4e0e\u57fa\u4e8e\u7269\u7406\u7684\u53ef\u5fae\u5206\u6e32\u67d3\uff0c\u89e3\u51b3\u4e86\u4e00\u7cfb\u5217\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u4ee5\u4ece\u5355\u4e2aRGBD\u56fe\u50cf\u548c\u8fb9\u754c\u6846\u4e2d\u4f30\u8ba1\u573a\u666f\u53c2\u6570\u3002", "result": "\u8be5\u6a21\u578b\u5728\u6807\u51c6\u7684\u65e0\u6a21\u578b\u5c11\u6837\u672c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\uff0c\u5e76\u5728\u96f6-shot\u6293\u53d6\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u573a\u666f\u91cd\u5efa\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u65e0\u987b\u5927\u91cf\u6570\u636e\u6216\u6d4b\u8bd5\u6837\u672c\u7684\u96f6-shot\u573a\u666f\u91cd\u5efa\u548c\u6293\u53d6\uff0c\u4e3a\u65b0\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u81ea\u4e3b\u6027\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u8def\u5f84\u3002"}}
{"id": "2602.05128", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.05128", "abs": "https://arxiv.org/abs/2602.05128", "authors": ["Karla Felix Navarro", "Eugene Syriani", "Ian Arawjo"], "title": "Reporting and Reviewing LLM-Integrated Systems in HCI: Challenges and Considerations", "comment": "18 pages, 1 figure, 2 tables. For proposed reporting guidelines, see https://ianarawjo.github.io/Guidelines-for-Reporting-LLM-Integrated-Systems-in-HCI/", "summary": "What should HCI scholars consider when reporting and reviewing papers that involve LLM-integrated systems? We interview 18 authors of LLM-integrated system papers on their authoring and reviewing experiences. We find that norms of trust-building between authors and reviewers appear to be eroded by the uncertainty of LLM behavior and hyperbolic rhetoric surrounding AI. Authors perceive that reviewers apply uniquely skeptical and inconsistent standards towards papers that report LLM-integrated systems, and mitigate mistrust by adding technical evaluations, justifying usage, and de-emphasizing LLM presence. Authors' views challenge blanket directives to report all prompts and use open models, arguing that prompt reporting is context-dependent and justifying proprietary model usage despite ethical concerns. Finally, some tensions in peer review appear to stem from clashes between the norms and values of HCI and ML/NLP communities, particularly around what constitutes a contribution and an appropriate level of technical rigor. Based on our findings and additional feedback from six expert HCI researchers, we present a set of guidelines and considerations for authors, reviewers, and HCI communities around reporting and reviewing papers that involve LLM-integrated systems.", "AI": {"tldr": "HCI\u5b66\u8005\u5728\u62a5\u544a\u548c\u5ba1\u67e5\u6d89\u53caLLM\u96c6\u6210\u7cfb\u7edf\u7684\u8bba\u6587\u65f6\uff0c\u9700\u5173\u6ce8\u4fe1\u4efb\u7f3a\u5931\u3001\u6280\u672f\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u53caHCI\u4e0eML/NLP\u793e\u533a\u7684\u4ef7\u503c\u89c2\u5dee\u5f02\uff0c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u7cfb\u5217\u6307\u5bfc\u65b9\u9488.", "motivation": "\u7814\u7a76\u76ee\u7684\u662f\u5206\u6790HCI\u5b66\u8005\u5728\u62a5\u544a\u548c\u5ba1\u67e5\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u96c6\u6210\u7cfb\u7edf\u7684\u8bba\u6587\u65f6\u9700\u8003\u8651\u7684\u56e0\u7d20\uff0c\u4ee5\u53ca\u5f53\u524d\u5ba1\u67e5\u8fc7\u7a0b\u4e2d\u6240\u9762\u4e34\u7684\u6311\u6218\u4e0e\u77db\u76fe\u3002", "method": "\u901a\u8fc7\u5bf918\u4f4d\u5173\u4e8eLLM\u96c6\u6210\u7cfb\u7edf\u8bba\u6587\u7684\u4f5c\u8005\u8fdb\u884c\u8bbf\u8c08\uff0c\u63a2\u8ba8\u4ed6\u4eec\u5728\u5199\u4f5c\u548c\u5ba1\u5ba1\u4e2d\u7684\u7ecf\u9a8c\u4e0e\u770b\u6cd5\u3002", "result": "\u53d1\u73b0\u4f5c\u8005\u548c\u5ba1\u7a3f\u4eba\u4e4b\u95f4\u7684\u4fe1\u4efb\u5efa\u7acb\u89c4\u8303\u7531\u4e8eLLM\u884c\u4e3a\u7684\u4e0d\u786e\u5b9a\u6027\u4e0e\u56f4\u7ed5AI\u7684\u5938\u5927\u4fee\u8f9e\u800c\u53d7\u5230\u4fb5\u8680\uff0c\u540c\u65f6\u8bba\u6587\u5bf9LLM\u96c6\u6210\u7cfb\u7edf\u7684\u5ba1\u67e5\u6807\u51c6\u5b58\u5728\u6000\u7591\u548c\u4e0d\u4e00\u81f4\u6027\uff1b\u63d0\u51fa\u5efa\u8bae\u5305\u62ec\u6280\u672f\u8bc4\u4f30\u3001\u4f7f\u7528\u7406\u7531\u7684\u9610\u660e\u53ca\u5bf9LLM\u5b58\u5728\u611f\u7684\u964d\u4f4e\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u5957\u5173\u4e8e\u62a5\u544a\u548c\u5ba1\u67e5\u4e0eLLM\u96c6\u6210\u7cfb\u7edf\u76f8\u5173\u8bba\u6587\u7684\u6307\u5bfc\u65b9\u9488\uff0c\u5e2e\u52a9\u4f5c\u8005\u3001\u5ba1\u7a3f\u4eba\u53caHCI\u793e\u533a\u66f4\u597d\u5730\u7406\u89e3\u5e76\u5e94\u5bf9\u76f8\u5173\u6311\u6218\u3002"}}
{"id": "2602.05079", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05079", "abs": "https://arxiv.org/abs/2602.05079", "authors": ["Vinal Asodia", "Iman Sharifi", "Saber Fallah"], "title": "Reinforcement Learning Enhancement Using Vector Semantic Representation and Symbolic Reasoning for Human-Centered Autonomous Emergency Braking", "comment": "12 pages, 7 figures, 5 tables", "summary": "The problem with existing camera-based Deep Reinforcement Learning approaches is twofold: they rarely integrate high-level scene context into the feature representation, and they rely on rigid, fixed reward functions. To address these challenges, this paper proposes a novel pipeline that produces a neuro-symbolic feature representation that encompasses semantic, spatial, and shape information, as well as spatially boosted features of dynamic entities in the scene, with an emphasis on safety-critical road users. It also proposes a Soft First-Order Logic (SFOL) reward function that balances human values via a symbolic reasoning module. Here, semantic and spatial predicates are extracted from segmentation maps and applied to linguistic rules to obtain reward weights. Quantitative experiments in the CARLA simulation environment show that the proposed neuro-symbolic representation and SFOL reward function improved policy robustness and safety-related performance metrics compared to baseline representations and reward formulations across varying traffic densities and occlusion levels. The findings demonstrate that integrating holistic representations and soft reasoning into Reinforcement Learning can support more context-aware and value-aligned decision-making for autonomous driving.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u795e\u7ecf\u7b26\u53f7\u7279\u5f81\u8868\u793a\u548c\u8f6f\u4e00\u9636\u903b\u8f91\u5956\u52b1\u51fd\u6570\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u81ea\u4e3b\u9a7e\u9a76\u4e2d\u5b58\u5728\u7684\u95ee\u9898\uff0c\u7ed3\u679c\u663e\u793a\u63d0\u5347\u4e86\u653f\u7b56\u7a33\u5065\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u76f8\u673a\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u7279\u5f81\u8868\u793a\u4e2d\u5f88\u5c11\u6574\u5408\u9ad8\u5c42\u573a\u666f\u4e0a\u4e0b\u6587\uff0c\u5e76\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u7684\u5956\u52b1\u51fd\u6570\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u7279\u5f81\u8868\u793a\u548c\u8f6f\u4e00\u9636\u903b\u8f91\u5956\u52b1\u51fd\u6570.", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6240\u63d0\u51fa\u7684\u795e\u7ecf\u7b26\u53f7\u8868\u793a\u548cSFOL\u5956\u52b1\u51fd\u6570\u5728\u653f\u7b56\u7a33\u5065\u6027\u548c\u5b89\u5168\u76f8\u5173\u6027\u80fd\u6307\u6807\u4e0a\u8d85\u8fc7\u4e86\u57fa\u7ebf\u8868\u793a\u548c\u5956\u52b1\u516c\u5f0f\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u6574\u4f53\u8868\u793a\u548c\u8f6f\u63a8\u7406\uff0c\u589e\u5f3a\u4e86\u81ea\u4e3b\u9a7e\u9a76\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u4ef7\u503c\u5bf9\u9f50\u7684\u51b3\u7b56\u80fd\u529b\u3002"}}
{"id": "2602.05129", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.05129", "abs": "https://arxiv.org/abs/2602.05129", "authors": ["Xiaodan Hu", "Monica Perusqu\u00eda-Hern\u00e1ndez", "Mayra Donaji Barrera Machuca", "Anil Ufuk Batmaz", "Yan Zhang", "Wolfgang Stuerzlinger", "Kiyoshi Kiyokawa"], "title": "Varifocal Displays Reduce the Impact of the Vergence-Accommodation Conflict on 3D Pointing Performance in Augmented Reality Systems", "comment": "24 pages, 11 figures", "summary": "This paper investigates whether a custom varifocal display can improve 3D pointing performance in augmented reality (AR), where the vergence-accommodation conflict (VAC) is known to impair interaction. Varifocal displays have been hypothesized to alleviate the VAC by dynamically matching the focal distance to the user's gaze-defined target depth. Following prior work, we conducted a within-subject study with 24 participants performing an ISO 9241-411 pointing task under varifocal and fixed-focal viewing. Overall, varifocal viewing yielded significantly higher performance than the fixed-focal baseline across key interaction metrics, although the magnitude and even the direction of the benefit varied across individuals. In particular, participants' responses exhibited a baseline-dependent pattern, with smaller improvements (or occasional degradation) observed for those with better baseline performance. Our findings suggest that varifocal technology can improve AR pointing performance relative to fixed-focal viewing, while highlighting substantial individual differences that should be considered in design and evaluation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u53ef\u53d8\u7126\u663e\u793a\u5668\u5728\u589e\u5f3a\u73b0\u5b9e\u4e2d\u5bf93D\u6307\u5411\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5176\u6574\u4f53\u8868\u73b0\u4f18\u4e8e\u56fa\u5b9a\u7126\u70b9\uff0c\u4e14\u4e2a\u4f53\u5dee\u5f02\u663e\u8457\u3002", "motivation": "\u8c03\u67e5\u5b9a\u5236\u7684\u53ef\u53d8\u7126\u663e\u793a\u5668\u662f\u5426\u80fd\u6539\u5584\u589e\u5f3a\u73b0\u5b9e\u4e2d\u76843D\u6307\u5411\u6027\u80fd\uff0c\u5e76\u7f13\u89e3\u5df2\u77e5\u7684\u51dd\u89c6-\u8c03\u8282\u51b2\u7a81\uff08VAC\uff09\u5bf9\u4ea4\u4e92\u7684\u5f71\u54cd\u3002", "method": "\u8fdb\u884c\u4e86\u4e00\u9879\u5728\u88ab\u8bd5\u5185\u7684\u5b9e\u9a8c\u7814\u7a76\uff0c\u6d89\u53ca24\u540d\u53c2\u4e0e\u8005\u5728\u53ef\u53d8\u7126\u548c\u56fa\u5b9a\u7126\u70b9\u89c6\u56fe\u4e0b\u5b8c\u6210ISO 9241-411\u6307\u5411\u4efb\u52a1\u3002", "result": "\u6574\u4f53\u800c\u8a00\uff0c\u53ef\u53d8\u7126\u89c6\u56fe\u7684\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u56fa\u5b9a\u7126\u70b9\u57fa\u7ebf\u3002\u5728\u4e0d\u540c\u4e2a\u4f53\u4e2d\u7684\u6536\u76ca\u5e45\u5ea6\u548c\u65b9\u5411\u6709\u6240\u4e0d\u540c\uff0c\u4e14\u5728\u57fa\u7ebf\u8868\u73b0\u8f83\u597d\u7684\u53c2\u4e0e\u8005\u4e2d\u89c2\u5bdf\u5230\u6539\u5584\u5e45\u5ea6\u8f83\u5c0f\u6216\u5076\u5c14\u9000\u6b65\u3002", "conclusion": "\u53ef\u53d8\u7126\u6280\u672f\u76f8\u8f83\u4e8e\u56fa\u5b9a\u7126\u70b9\u89c6\u56fe\uff0c\u80fd\u591f\u63d0\u9ad8\u589e\u5f3a\u73b0\u5b9e\u4e2d\u7684\u6307\u5411\u6027\u80fd\uff0c\u4f46\u4e2a\u4f53\u5dee\u5f02\u663e\u8457\uff0c\u8bbe\u8ba1\u548c\u8bc4\u4f30\u65f6\u9700\u8003\u8651\u8fd9\u4e9b\u5dee\u5f02\u3002"}}
{"id": "2602.05092", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05092", "abs": "https://arxiv.org/abs/2602.05092", "authors": ["Thomas Cohn", "Lihan Tang", "Alexandre Amice", "Russ Tedrake"], "title": "A Framework for Combining Optimization-Based and Analytic Inverse Kinematics", "comment": "19 pages, 5 figures, 6 tables. Under submission", "summary": "Analytic and optimization methods for solving inverse kinematics (IK) problems have been deeply studied throughout the history of robotics. The two strategies have complementary strengths and weaknesses, but developing a unified approach to take advantage of both methods has proved challenging. A key challenge faced by optimization approaches is the complicated nonlinear relationship between the joint angles and the end-effector pose. When this must be handled concurrently with additional nonconvex constraints like collision avoidance, optimization IK algorithms may suffer high failure rates. We present a new formulation for optimization IK that uses an analytic IK solution as a change of variables, and is fundamentally easier for optimizers to solve. We test our methodology on three popular solvers, representing three different paradigms for constrained nonlinear optimization. Extensive experimental comparisons demonstrate that our new formulation achieves higher success rates than the old formulation and baseline methods across various challenging IK problems, including collision avoidance, grasp selection, and humanoid stability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u9006\u8fd0\u52a8\u5b66\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u89e3\u6790\u89e3\u4f5c\u4e3a\u53d8\u91cf\u53d8\u6362\uff0c\u63d0\u9ad8\u4e86\u5728\u590d\u6742\u7ea6\u675f\u4e0b\u7684\u6210\u529f\u7387\u3002", "motivation": "\u5f00\u53d1\u7edf\u4e00\u7684\u65b9\u6cd5\u4ee5\u540c\u65f6\u5229\u7528\u89e3\u6790\u548c\u4f18\u5316\u9006\u8fd0\u52a8\u5b66\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u89e3\u51b3\u9ad8\u5931\u8d25\u7387\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u89e3\u6790\u9006\u8fd0\u52a8\u5b66\u89e3\u4f5c\u4e3a\u53d8\u91cf\u53d8\u6362\u6765\u7b80\u5316\u4f18\u5316\u8fc7\u7a0b\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u89e3\u51b3\u78b0\u649e\u907f\u514d\u3001\u6293\u53d6\u9009\u62e9\u548c\u4eba\u5f62\u7a33\u5b9a\u6027\u7b49\u591a\u4e2a\u9006\u8fd0\u52a8\u5b66\u95ee\u9898\u4e2d\uff0c\u8868\u73b0\u51fa\u6bd4\u65e7\u65b9\u6cd5\u66f4\u9ad8\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u65b0\u63d0\u51fa\u7684\u4f18\u5316\u9006\u8fd0\u52a8\u5b66\u65b9\u6cd5\u5728\u5904\u7406\u591a\u79cd\u6311\u6218\u7684\u9006\u8fd0\u52a8\u5b66\u95ee\u9898\u65f6\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6210\u529f\u7387\u3002"}}
{"id": "2602.05299", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.05299", "abs": "https://arxiv.org/abs/2602.05299", "authors": ["Kashif Imteyaz", "Michael Muller", "Claudia Flores-Saviaga", "Saiph Savage"], "title": "Co-Designing Collaborative Generative AI Tools for Freelancers", "comment": null, "summary": "Most generative AI tools prioritize individual productivity and personalization, with limited support for collaboration. Designed for traditional workplaces, these tools do not fit freelancers' short-term teams or lack of shared institutional support, which can worsen their isolation and overlook freelancing platform dynamics. This mismatch means that, instead of empowering freelancers, current generative AI tools could reinforce existing precarity and make freelancer collaboration harder. To investigate how to design generative AI tools to support freelancer collaboration, we conducted co-design sessions with 27 freelancers. A key concern that emerged was the risk of AI systems compromising their creative agency and work identities when collaborating, especially when AI tools could reproduce content without attribution, threatening the authenticity and distinctiveness of their collaborative work. Freelancers proposed \"auxiliary AI\" systems, human-guided tools that support their creative agencies and identities, allowing for flexible freelancer-led collaborations that promote \"productive friction\". Drawing on Marcuse's concept of technological rationality, we argue that freelancers are resisting one-dimensional, efficiency-driven AI, and instead envisioning technologies that preserve their collective creative agencies. We conclude with design recommendations for collaborative generative AI tools for freelancers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u8bbe\u8ba1\u751f\u6210AI\u5de5\u5177\u4ee5\u652f\u6301\u81ea\u7531\u804c\u4e1a\u8005\u7684\u534f\u4f5c\uff0c\u5f3a\u8c03\u8981\u4fdd\u62a4\u5176\u521b\u9020\u4ee3\u7406\u548c\u5de5\u4f5c\u8eab\u4efd\uff0c\u63d0\u51fa\u4e86\u76f8\u5173\u7684\u8bbe\u8ba1\u5efa\u8bae\u3002", "motivation": "\u73b0\u6709\u7684\u751f\u6210AI\u5de5\u5177\u7531\u4e8e\u4fa7\u91cd\u4e2a\u4eba\u751f\u4ea7\u529b\u548c\u4e2a\u6027\u5316\uff0c\u672a\u80fd\u6709\u6548\u652f\u6301\u81ea\u7531\u804c\u4e1a\u8005\u7684\u534f\u4f5c\uff0c\u53ef\u80fd\u52a0\u5267\u5176\u5b64\u7acb\u611f\u548c\u4e0d\u7a33\u5b9a\u6027\u3002", "method": "\u901a\u8fc7\u4e0e27\u540d\u81ea\u7531\u804c\u4e1a\u8005\u8fdb\u884c\u5171\u521b\u4f1a\u8bae\uff0c\u63a2\u8ba8\u5982\u4f55\u8bbe\u8ba1\u751f\u6210AI\u5de5\u5177\u4ee5\u652f\u6301\u81ea\u7531\u804c\u4e1a\u8005\u7684\u534f\u4f5c\u3002", "result": "\u53d1\u73b0\u81ea\u7531\u804c\u4e1a\u8005\u62c5\u5fc3AI\u5de5\u5177\u53ef\u80fd\u4f1a\u4fb5\u72af\u5176\u521b\u9020\u4ee3\u7406\u548c\u5de5\u4f5c\u8eab\u4efd\uff0c\u5e76\u63d0\u51fa\u652f\u6301\u521b\u9020\u6027\u7684\u201c\u8f85\u52a9AI\u201d\u7cfb\u7edf\u8bbe\u8ba1\u5efa\u8bae\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u81ea\u7531\u804c\u4e1a\u8005\u7684\u534f\u4f5c\u751f\u6210AI\u5de5\u5177\u8bbe\u8ba1\u5efa\u8bae\uff0c\u4ee5\u4fdd\u62a4\u5176\u521b\u9020\u4ee3\u7406\u548c\u5de5\u4f5c\u8eab\u4efd\uff0c\u5e76\u652f\u6301\u81ea\u7531\u804c\u4e1a\u8005\u4e3b\u5bfc\u7684\u7075\u6d3b\u5408\u4f5c\u3002"}}
{"id": "2602.05156", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.05156", "abs": "https://arxiv.org/abs/2602.05156", "authors": ["Dong Ho Kang", "Aaron Kim", "Mingyo Seo", "Kazuto Yokoyama", "Tetsuya Narita", "Luis Sentis"], "title": "PLATO Hand: Shaping Contact Behavior with Fingernails for Precise Manipulation", "comment": null, "summary": "We present the PLATO Hand, a dexterous robotic hand with a hybrid fingertip that embeds a rigid fingernail within a compliant pulp. This design shapes contact behavior to enable diverse interaction modes across a range of object geometries. We develop a strain-energy-based bending-indentation model to guide the fingertip design and to explain how guided contact preserves local indentation while suppressing global bending. Experimental results show that the proposed robotic hand design demonstrates improved pinching stability, enhanced force observability, and successful execution of edge-sensitive manipulation tasks, including paper singulation, card picking, and orange peeling. Together, these results show that coupling structured contact geometry with a force-motion transparent mechanism provides a principled, physically embodied approach to precise manipulation.", "AI": {"tldr": "PLATO\u624b\u901a\u8fc7\u5c06\u521a\u6027\u6307\u7532\u4e0e\u67d4\u6027\u6307\u67c4\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u63d0\u9ad8\u5939\u6301\u7a33\u5b9a\u6027\u548c\u529b\u53ef\u89c2\u6d4b\u6027\u7684\u7075\u5de7\u64cd\u63a7\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u7269\u4f53\u64cd\u4f5c\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u79cd\u80fd\u591f\u5728\u5404\u79cd\u7269\u4f53\u51e0\u4f55\u5f62\u72b6\u4e0a\u5b9e\u73b0\u591a\u79cd\u4ea4\u4e92\u6a21\u5f0f\u7684\u7075\u5de7\u673a\u5668\u4eba\u624b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e94\u53d8\u80fd\u7684\u5f2f\u66f2-\u538b\u5165\u6a21\u578b\uff0c\u4ee5\u6307\u5bfc\u624b\u6307\u5c16\u8bbe\u8ba1\u5e76\u89e3\u91ca\u5982\u4f55\u4fdd\u6301\u5c40\u90e8\u538b\u5165\u540c\u65f6\u6291\u5236\u6574\u4f53\u5f2f\u66f2\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u673a\u5668\u4eba\u624b\u8bbe\u8ba1\u5728\u5939\u6301\u7a33\u5b9a\u6027\u3001\u529b\u53ef\u89c2\u6d4b\u6027\u548c\u8fb9\u7f18\u654f\u611f\u64cd\u4f5c\u4efb\u52a1\uff08\u5982\u7eb8\u5f20\u5206\u79bb\u3001\u5361\u7247\u62fe\u53d6\u548c\u6a59\u5b50\u5265\u76ae\uff09\u7684\u6267\u884c\u4e0a\u5747\u6709\u6240\u6539\u5584\u3002", "conclusion": "\u5c06\u7ed3\u6784\u5316\u63a5\u89e6\u51e0\u4f55\u4e0e\u529b-\u8fd0\u52a8\u900f\u660e\u673a\u5236\u76f8\u7ed3\u5408\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u3001\u7269\u7406\u4f53\u73b0\u7684\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u7cbe\u786e\u64cd\u4f5c\u3002"}}
{"id": "2602.05446", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.05446", "abs": "https://arxiv.org/abs/2602.05446", "authors": ["Rui Sheng", "Yukun Yang", "Chuhan Shi", "Yanna Lin", "Zixin Chen", "Huamin Qu", "Furui Cheng"], "title": "DiLLS: Interactive Diagnosis of LLM-based Multi-agent Systems via Layered Summary of Agent Behaviors", "comment": null, "summary": "Large language model (LLM)-based multi-agent systems have demonstrated impressive capabilities in handling complex tasks. However, the complexity of agentic behaviors makes these systems difficult to understand. When failures occur, developers often struggle to identify root causes and to determine actionable paths for improvement. Traditional methods that rely on inspecting raw log records are inefficient, given both the large volume and complexity of data. To address this challenge, we propose a framework and an interactive system, DiLLS, designed to reveal and structure the behaviors of multi-agent systems. The key idea is to organize information across three levels of query completion: activities, actions, and operations. By probing the multi-agent system through natural language, DiLLS derives and organizes information about planning and execution into a structured, multi-layered summary. Through a user study, we show that DiLLS significantly improves developers' effectiveness and efficiency in identifying, diagnosing, and understanding failures in LLM-based multi-agent systems.", "AI": {"tldr": "\u63d0\u51faDiLLS\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u7ec4\u7ec7\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u4fe1\u606f\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u66f4\u9ad8\u6548\u5730\u8bc6\u522b\u548c\u7406\u89e3\u6545\u969c\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u590d\u6742\u7684\u4ee3\u7406\u884c\u4e3a\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u66f4\u6709\u6548\u5730\u8bc6\u522b\u6545\u969c\u7684\u6839\u672c\u539f\u56e0\u53ca\u6539\u5584\u8def\u5f84\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u7cfb\u7edfDiLLS\uff0c\u7ec4\u7ec7\u591a\u4ee3\u7406\u7cfb\u7edf\u7684\u884c\u4e3a\u4fe1\u606f\uff0c\u5206\u4e3a\u6d3b\u52a8\u3001\u884c\u52a8\u548c\u64cd\u4f5c\u4e09\u4e2a\u5c42\u6b21\uff0c\u5229\u7528\u81ea\u7136\u8bed\u8a00\u8fdb\u884c\u67e5\u8be2\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cDiLLS\u663e\u8457\u63d0\u5347\u4e86\u5f00\u53d1\u8005\u5728\u5904\u7406\u6545\u969c\u65f6\u7684\u6548\u80fd\u4e0e\u6548\u7387\u3002", "conclusion": "DiLLS\u663e\u8457\u63d0\u9ad8\u4e86\u5f00\u53d1\u8005\u5728\u8bc6\u522b\u3001\u8bca\u65ad\u548c\u7406\u89e3\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u6545\u969c\u7684\u6548\u7387\u548c\u6548\u679c\u3002"}}
{"id": "2602.05198", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05198", "abs": "https://arxiv.org/abs/2602.05198", "authors": ["Kalvik Jakkala", "Saurav Agarwal", "Jason O'Kane", "Srinivas Akella"], "title": "Informative Path Planning with Guaranteed Estimation Uncertainty", "comment": "16 pages, 11 figures, preprint", "summary": "Environmental monitoring robots often need to reconstruct spatial fields (e.g., salinity, temperature, bathymetry) under tight distance and energy constraints. Classical boustrophedon lawnmower surveys provide geometric coverage guarantees but can waste effort by oversampling predictable regions. In contrast, informative path planning (IPP) methods leverage spatial correlations to reduce oversampling, yet typically offer no guarantees on reconstruction quality. This paper bridges these approaches by addressing informative path planning with guaranteed estimation uncertainty: computing the shortest path whose measurements ensure that the Gaussian-process (GP) posterior variance -- an intrinsic uncertainty measure that lower-bounds the mean-squared prediction error under the GP model -- falls below a user-specified threshold over the monitoring region.\n  We propose a three-stage approach: (i) learn a GP model from available prior information; (ii) transform the learned GP kernel into binary coverage maps for each candidate sensing location, indicating which locations' uncertainty can be reduced below a specified target; and (iii) plan a near-shortest route whose combined coverage satisfies the global uncertainty constraint. To address heterogeneous phenomena, we incorporate a nonstationary kernel that captures spatially varying correlation structure, and we accommodate non-convex environments with obstacles. Algorithmically, we present methods with provable approximation guarantees for sensing-location selection and for the joint selection-and-routing problem under a travel budget. Experiments on real-world topographic data show that our planners meet the uncertainty target using fewer sensing locations and shorter travel distances than a recent baseline, and field experiments with bathymetry-mapping autonomous surface and underwater vehicles demonstrate real-world feasibility.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u4fe1\u606f\u8def\u5f84\u89c4\u5212\u548c\u4e0d\u786e\u5b9a\u6027\u4fdd\u8bc1\uff0c\u4f18\u5316\u4e86\u73af\u5883\u76d1\u6d4b\u673a\u5668\u4eba\u7684\u8def\u5f84\u89c4\u5212\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u73af\u5883\u76d1\u6d4b\u673a\u5668\u4eba\u5728\u7a7a\u95f4\u573a\u91cd\u6784\u4e2d\u7684\u5e94\u7528\u9700\u6c42\u589e\u52a0\uff0c\u51cf\u5c11\u6d4b\u91cf\u91cd\u91c7\u6837\u5e76\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\u6210\u4e3a\u8feb\u5207\u9700\u6c42\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u9636\u6bb5\u65b9\u6cd5\uff0c\u5305\u62ec\u5b66\u4e60\u9ad8\u65af\u8fc7\u7a0b\u6a21\u578b\u3001\u751f\u6210\u8986\u76d6\u56fe\u4ee5\u53ca\u89c4\u5212\u8fd1\u6700\u4f18\u8def\u5f84\u4ee5\u6ee1\u8db3\u5168\u7403\u4e0d\u786e\u5b9a\u6027\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u6700\u8fd1\u7684\u57fa\u51c6\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6ee1\u8db3\u4e0d\u786e\u5b9a\u6027\u76ee\u6807\u7684\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528\u66f4\u5c11\u7684\u6d4b\u91cf\u4f4d\u7f6e\u548c\u66f4\u77ed\u7684\u65c5\u884c\u8ddd\u79bb\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u878d\u5408\u4fe1\u606f\u8def\u5f84\u89c4\u5212\u4e0e\u91cd\u5efa\u8d28\u91cf\u4fdd\u8bc1\uff0c\u4f18\u5316\u4e86\u73af\u5883\u76d1\u6d4b\u673a\u5668\u4eba\u7684\u6d4b\u91cf\u8def\u5f84\u3002"}}
{"id": "2602.05506", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.05506", "abs": "https://arxiv.org/abs/2602.05506", "authors": ["Xinrui Lin", "Heyan Huang", "Shumin Shi", "John Vines"], "title": "Relying on LLMs: Student Practices and Instructor Norms are Changing in Computer Science Education", "comment": "25 pages, 1 figure", "summary": "Prior research has raised concerns about students' over-reliance on large language models (LLMs) in higher education. This paper examines how Computer Science students and instructors engage with LLMs across five scenarios: \"Writing\", \"Quiz\", \"Programming\", \"Project-based learning\", and \"Information retrieval\". Through user studies with 16 students and 6 instructors, we identify 7 key intents, including increasingly complex student practices. Findings reveal varying levels of conflict between student practices and instructor norms, ranging from clear conflict in \"Writing-generation\" and \"(Programming) quiz-solving\", through partial conflict in \"Programming project-implementation\" and \"Project-based learning\", to broad agreement in \"Writing-revision & ideation\", \"(Programming) quiz-correction\" and \"Info-query & summary\". We document instructors are shifting from prohibiting to recognizing students' use of LLMs for high-quality work, integrating usage records into assessment grading. Finally, we propose LLM design guidelines: deploying default guardrails with game-like and empathetic interaction to prevent students from \"deserting\" LLMs, especially for \"Writing-generation\", while utilizing comprehension checks in low-conflict intents to promote learning.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u8ba1\u7b97\u673a\u79d1\u5b66\u5b66\u751f\u4e0e LLM \u7684\u4e92\u52a8\uff0c\u6307\u51fa\u4e86\u5b66\u751f\u4e0e\u6559\u5e08\u4e4b\u95f4\u7684\u51b2\u7a81\u548c\u5171\u8bc6\uff0c\u5e76\u63d0\u51fa\u4e86 LLM \u8bbe\u8ba1\u6307\u5357\u4ee5\u51cf\u5c11\u4f9d\u8d56\u548c\u4fc3\u8fdb\u5b66\u4e60\u3002", "motivation": "\u63a2\u8ba8\u5b66\u751f\u5728\u9ad8\u7b49\u6559\u80b2\u4e2d\u8fc7\u5ea6\u4f9d\u8d56\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5bf9 16 \u540d\u5b66\u751f\u548c 6 \u540d\u6559\u5e08\u7684\u7528\u6237\u7814\u7a76\uff0c\u5206\u6790\u4e86\u8ba1\u7b97\u673a\u79d1\u5b66\u5b66\u751f\u548c\u6559\u5e08\u5728\u4e94\u79cd\u60c5\u5883\u4e0b\u4e0e LLM \u7684\u4e92\u52a8\u3002", "result": "\u53d1\u73b0\u5b66\u751f\u5b9e\u8df5\u4e0e\u6559\u5e08\u89c4\u8303\u4e4b\u95f4\u5b58\u5728\u4e0d\u540c\u7a0b\u5ea6\u7684\u51b2\u7a81\uff0c\u4ee5\u53ca\u6559\u5e08\u5bf9 LLM \u4f7f\u7528\u7684\u6001\u5ea6\u6b63\u9010\u6e10\u4ece\u7981\u6b62\u8f6c\u5411\u8ba4\u53ef\u3002", "conclusion": "\u5efa\u8bae\u8bbe\u8ba1 LLM \u4ee5\u4fc3\u8fdb\u5b66\u751f\u5b66\u4e60\u5e76\u51cf\u5c11\u8fc7\u5ea6\u4f9d\u8d56\uff0c\u5c24\u5176\u662f\u5728\u5199\u4f5c\u751f\u6210\u9886\u57df\u3002"}}
{"id": "2602.05233", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05233", "abs": "https://arxiv.org/abs/2602.05233", "authors": ["Wenbo Wang", "Fangyun Wei", "QiXiu Li", "Xi Chen", "Yaobo Liang", "Chang Xu", "Jiaolong Yang", "Baining Guo"], "title": "MobileManiBench: Simplifying Model Verification for Mobile Manipulation", "comment": null, "summary": "Vision-language-action models have advanced robotic manipulation but remain constrained by reliance on the large, teleoperation-collected datasets dominated by the static, tabletop scenes. We propose a simulation-first framework to verify VLA architectures before real-world deployment and introduce MobileManiBench, a large-scale benchmark for mobile-based robotic manipulation. Built on NVIDIA Isaac Sim and powered by reinforcement learning, our pipeline autonomously generates diverse manipulation trajectories with rich annotations (language instructions, multi-view RGB-depth-segmentation images, synchronized object/robot states and actions). MobileManiBench features 2 mobile platforms (parallel-gripper and dexterous-hand robots), 2 synchronized cameras (head and right wrist), 630 objects in 20 categories, 5 skills (open, close, pull, push, pick) with over 100 tasks performed in 100 realistic scenes, yielding 300K trajectories. This design enables controlled, scalable studies of robot embodiments, sensing modalities, and policy architectures, accelerating research on data efficiency and generalization. We benchmark representative VLA models and report insights into perception, reasoning, and control in complex simulated environments.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86MobileManiBench\uff0c\u4e00\u4e2a\u5927\u578b\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u9a8c\u8bc1\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u79fb\u52a8\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u8868\u73b0\uff0c\u652f\u6301\u591a\u6837\u5316\u7684\u64cd\u4f5c\u7814\u7a76\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u53d7\u9650\u4e8e\u9759\u6001\u684c\u9762\u573a\u666f\u7684\u6570\u636e\u96c6\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5177\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u901a\u8fc7\u5728NVIDIA Isaac Sim\u4e0a\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff0c\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u7684\u64cd\u4f5c\u8f68\u8ff9\u5e76\u8fdb\u884c\u4e30\u5bcc\u7684\u6ce8\u91ca\u3002", "result": "MobileManiBench\u5305\u542b630\u79cd\u7269\u4f53\u548c100\u79cd\u73b0\u5b9e\u573a\u666f\uff0c\u63d0\u4f9b300K\u4e2a\u64cd\u4f5c\u8f68\u8ff9\uff0c\u652f\u6301\u5bf9\u4e0d\u540c\u673a\u5668\u4eba\u5f62\u6001\u548c\u611f\u77e5\u65b9\u5f0f\u7684\u7814\u7a76\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684MobileManiBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u52a0\u901f\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u6570\u636e\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u7814\u7a76\uff0c\u9a8c\u8bc1\u4e86\u4e0d\u540cVLA\u6a21\u578b\u5728\u590d\u6742\u6a21\u62df\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2602.05525", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.05525", "abs": "https://arxiv.org/abs/2602.05525", "authors": ["Fabrizio Fornari", "Eleonora Cova", "Niccol\u00f2 Vito Vacca", "Francesco Bocci", "Luigi Caputo"], "title": "Assessing Problem-Solving in HR Contexts: A Comparison Between Game-Based and Self-Report Measures", "comment": "24 pages, 2 figures", "summary": "Game-based assessments (GBAs) are increasingly adopted in recruitment contexts as tools to assess transversal skills through observable behavior. However, empirical evidence directly comparing game-based behavioral indicators with traditional self-report measures remains limited. This study adopts a method-comparison approach to explore the convergence between self-perceived and behaviorally enacted problem-solving competence, comparing a game-based assessment with the Problem Solving Inventory (PSI-B).\n  Seventy-eight participants completed both the PSI-B and a five-minute game-based problem-solving task, which classified performance into four behavioral proficiency levels. Results revealed no significant convergence between self-reported and behavior-based problem-solving scores, indicating a lack of convergence between the two measurement modalities.\n  Rather than indicating a lack of validity of the game-based assessment, these findings support the view that self-report and behavioral measures provide complementary information about problem-solving competence. The study highlights the risks of relying on a single assessment modality in personnel selection and underscores the value of integrating game-based tools within multi-method assessment frameworks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u81ea\u6211\u62a5\u544a\u548c\u57fa\u4e8e\u884c\u4e3a\u7684\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u4e24\u8005\u7f3a\u4e4f\u4e00\u81f4\u6027\uff0c\u5f3a\u8c03\u591a\u65b9\u6cd5\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5728\u62db\u8058\u73af\u5883\u4e2d\uff0c\u6e38\u620f\u5316\u8bc4\u4f30\u4f5c\u4e3a\u8bc4\u4f30\u8de8\u884c\u6280\u80fd\u7684\u5de5\u5177\u65e5\u76ca\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u5173\u4e8e\u6e38\u620f\u5316\u884c\u4e3a\u6307\u6807\u4e0e\u4f20\u7edf\u81ea\u6211\u62a5\u544a\u6d4b\u91cf\u4e4b\u95f4\u7684\u5b9e\u8bc1\u8bc1\u636e\u4ecd\u7136\u6709\u9650\u3002", "method": "\u91c7\u7528\u65b9\u6cd5\u6bd4\u8f83\u7684\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e86\u81ea\u6211\u611f\u77e5\u4e0e\u884c\u4e3a\u8868\u73b0\u7684\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u4f7f\u7528\u4e86\u95ee\u9898\u89e3\u51b3\u6e05\u5355\uff08PSI-B\uff09\u548c\u4e94\u5206\u949f\u7684\u6e38\u620f\u5316\u95ee\u9898\u89e3\u51b3\u4efb\u52a1\u3002", "result": "\u7ed3\u679c\u663e\u793a\u81ea\u6211\u62a5\u544a\u5f97\u5206\u4e0e\u57fa\u4e8e\u884c\u4e3a\u7684\u95ee\u9898\u89e3\u51b3\u5f97\u5206\u4e4b\u95f4\u6ca1\u6709\u663e\u8457\u7684\u4e00\u81f4\u6027\uff0c\u8868\u660e\u4e24\u79cd\u6d4b\u91cf\u65b9\u5f0f\u4e4b\u95f4\u7f3a\u4e4f\u6536\u655b\u6027\u3002", "conclusion": "\u81ea\u6211\u62a5\u544a\u4e0e\u884c\u4e3a\u8868\u73b0\u4e4b\u95f4\u7684\u6d4b\u91cf\u65b9\u5f0f\u7f3a\u4e4f\u4e00\u81f4\u6027\uff0c\u4f46\u8fd9\u5e76\u4e0d\u610f\u5473\u7740\u6e38\u620f\u5316\u8bc4\u4f30\u7684\u6709\u6548\u6027\u4e0d\u8db3\u3002\u8fd9\u4e9b\u53d1\u73b0\u652f\u6301\u81ea\u6211\u62a5\u544a\u548c\u884c\u4e3a\u6d4b\u91cf\u5728\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u8bc4\u4f30\u4e2d\u4e92\u4e3a\u8865\u5145\u7684\u89c2\u70b9\uff0c\u5f3a\u8c03\u4e86\u5728\u4eba\u5458\u9009\u62e9\u4e2d\u4f9d\u8d56\u5355\u4e00\u8bc4\u4f30\u65b9\u5f0f\u7684\u98ce\u9669\u3002"}}
{"id": "2602.05265", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05265", "abs": "https://arxiv.org/abs/2602.05265", "authors": ["Kalvik Jakkala", "Jason O'Kane"], "title": "Low-Cost Underwater In-Pipe Centering and Inspection Using a Minimal-Sensing Robot", "comment": null, "summary": "Autonomous underwater inspection of submerged pipelines is challenging due to confined geometries, turbidity, and the scarcity of reliable localization cues. This paper presents a minimal-sensing strategy that enables a free-swimming underwater robot to center itself and traverse a flooded pipe of known radius using only an IMU, a pressure sensor, and two sonars: a downward-facing single-beam sonar and a rotating 360 degree sonar. We introduce a computationally efficient method for extracting range estimates from single-beam sonar intensity data, enabling reliable wall detection in noisy and reverberant conditions. A closed-form geometric model leverages the two sonar ranges to estimate the pipe center, and an adaptive, confidence-weighted proportional-derivative (PD) controller maintains alignment during traversal. The system requires no Doppler velocity log, external tracking, or complex multi-sensor arrays. Experiments in a submerged 46 cm-diameter pipe using a Blue Robotics BlueROV2 heavy remotely operated vehicle demonstrate stable centering and successful full-pipe traversal despite ambient flow and structural deformations. These results show that reliable in-pipe navigation and inspection can be achieved with a lightweight, computationally efficient sensing and processing architecture, advancing the practicality of autonomous underwater inspection in confined environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6c34\u4e0b\u673a\u5668\u4eba\u5bfc\u822a\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u6c89\u6ca1\u7ba1\u9053\u7684\u81ea\u52a8\u68c0\u67e5\uff0c\u5c55\u793a\u4e86\u8be5\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e2d\u53ef\u9760\u8fd0\u884c\u7684\u80fd\u529b\u3002", "motivation": "\u81ea\u4e3b\u6c34\u4e0b\u68c0\u9a8c\u6c89\u6ca1\u7ba1\u9053\u9762\u4e34\u5f88\u5927\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u53d1\u5c55\u4e00\u79cd\u65b0\u7b56\u7565\u6765\u63d0\u9ad8\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\u3002", "method": "\u91c7\u7528\u6700\u5c0f\u611f\u77e5\u7b56\u7565\uff0c\u901a\u8fc7IMU\u3001\u538b\u529b\u4f20\u611f\u5668\u4ee5\u53ca\u5355\u675f\u58f0\u5450\u548c360\u5ea6\u58f0\u5450\u8fdb\u884c\u7ba1\u9053\u4e2d\u5fc3\u5b9a\u4f4d\u548c\u5de1\u822a\u3002", "result": "\u572846\u5398\u7c73\u76f4\u5f84\u7684\u6c89\u6ca1\u7ba1\u9053\u4e2d\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86\u7a33\u5b9a\u7684\u4e2d\u5fc3\u5b9a\u4f4d\u548c\u6210\u529f\u7684\u6574\u7ba1\u5de1\u822a\u80fd\u529b\uff0c\u5c3d\u7ba1\u5b58\u5728\u73af\u5883\u6d41\u52a8\u548c\u7ed3\u6784\u53d8\u5f62\u3002", "conclusion": "\u53ef\u9760\u7684\u7ba1\u9053\u5185\u5bfc\u822a\u4e0e\u68c0\u6d4b\u53ef\u4ee5\u901a\u8fc7\u8f7b\u91cf\u7ea7\u548c\u8ba1\u7b97\u9ad8\u6548\u7684\u4f20\u611f\u4e0e\u5904\u7406\u67b6\u6784\u5b9e\u73b0\uff0c\u4ece\u800c\u63d0\u5347\u81ea\u4e3b\u6c34\u4e0b\u68c0\u67e5\u5728\u6709\u9650\u73af\u5883\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2602.05628", "categories": ["cs.HC", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05628", "abs": "https://arxiv.org/abs/2602.05628", "authors": ["Alastair Howcroft", "Amber Bennett-Weston", "Ahmad Khan", "Joseff Griffiths", "Simon Gay", "Jeremy Howick"], "title": "AI chatbots versus human healthcare professionals: a systematic review and meta-analysis of empathy in patient care", "comment": "Open Access Invited Review. Systematic review and meta analysis of 15 studies 2023-2024. Published 20 October 2025", "summary": "Background: Empathy is widely recognized for improving patient outcomes, including reduced pain and anxiety and improved satisfaction, and its absence can cause harm. Meanwhile, use of artificial intelligence (AI)-based chatbots in healthcare is rapidly expanding, with one in five general practitioners using generative AI to assist with tasks such as writing letters. Some studies suggest AI chatbots can outperform human healthcare professionals (HCPs) in empathy, though findings are mixed and lack synthesis.\n  Sources of data: We searched multiple databases for studies comparing AI chatbots using large language models with human HCPs on empathy measures. We assessed risk of bias with ROBINS-I and synthesized findings using random-effects meta-analysis where feasible, whilst avoiding double counting.\n  Areas of agreement: We identified 15 studies (2023-2024). Thirteen studies reported statistically significantly higher empathy ratings for AI, with only two studies situated in dermatology favouring human responses. Of the 15 studies, 13 provided extractable data and were suitable for pooling. Meta-analysis of those 13 studies, all utilising ChatGPT-3.5/4, showed a standardized mean difference of 0.87 (95% CI, 0.54-1.20) favouring AI (P < .00001), roughly equivalent to a two-point increase on a 10-point scale.\n  Areas of controversy: Studies relied on text-based assessments that overlook non-verbal cues and evaluated empathy through proxy raters.\n  Growing points: Our findings indicate that, in text-only scenarios, AI chatbots are frequently perceived as more empathic than human HCPs.\n  Areas timely for developing research: Future research should validate these findings with direct patient evaluations and assess whether emerging voice-enabled AI systems can deliver similar empathic advantages.", "AI": {"tldr": "\u672c\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u6587\u672c\u573a\u666f\u4e2d\uff0cAI\u804a\u5929\u673a\u5668\u4eba\u88ab\u666e\u904d\u8ba4\u4e3a\u5177\u6709\u66f4\u9ad8\u7684\u540c\u7406\u5fc3\uff0c\u672a\u6765\u9700\u8981\u901a\u8fc7\u76f4\u63a5\u60a3\u8005\u8bc4\u4f30\u9a8c\u8bc1\u8fd9\u4e9b\u7ed3\u679c\uff0c\u5e76\u7814\u7a76\u8bed\u97f3\u4e92\u52a8AI\u7cfb\u7edf\u7684\u540c\u7406\u5fc3\u8868\u73b0\u3002", "motivation": "\u540c\u7406\u5fc3\u5bf9\u6539\u5584\u60a3\u8005\u7ed3\u679c\u81f3\u5173\u91cd\u8981\uff0c\u800cAI\u804a\u5929\u673a\u5668\u4eba\u5728\u533b\u7597\u4e2d\u7684\u5e94\u7528\u8fc5\u901f\u589e\u957f\uff0c\u6709\u7814\u7a76\u8868\u660e\u5176\u5728\u540c\u7406\u5fc3\u65b9\u9762\u53ef\u80fd\u4f18\u4e8e\u4eba\u7c7b\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u3002", "method": "\u5bf9\u591a\u4e2a\u6570\u636e\u5e93\u4e2d\u7684\u7814\u7a76\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u68c0\u7d22\uff0c\u6bd4\u8f83\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684AI\u804a\u5929\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u5728\u540c\u7406\u5fc3\u6d4b\u91cf\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u4f7f\u7528\u968f\u673a\u6548\u5e94\u5143\u5206\u6790\u6cd5\u8fdb\u884c\u6570\u636e\u5408\u6210\u3002", "result": "\u4ece15\u9879\u7814\u7a76\u4e2d\u83b7\u5f97\u7684\u6570\u636e\u8868\u660e\uff0c13\u9879\u7814\u7a76\u663e\u793aAI\u7684\u540c\u7406\u5fc3\u8bc4\u5206\u663e\u8457\u9ad8\u4e8e\u4eba\u7c7b\uff0c\u5143\u5206\u6790\u7ed3\u679c\u663e\u793a\u6807\u51c6\u5316\u5747\u503c\u5dee\u5f02\u4e3a0.87\uff0c\u8868\u660eAI\u5728\u540c\u7406\u5fc3\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u5728\u6587\u672c\u573a\u666f\u4e2d\uff0cAI\u804a\u5929\u673a\u5668\u4eba\u901a\u5e38\u88ab\u8ba4\u4e3a\u6bd4\u4eba\u7c7b\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u66f4\u5177\u540c\u7406\u5fc3\u3002"}}
{"id": "2602.05273", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05273", "abs": "https://arxiv.org/abs/2602.05273", "authors": ["Hengxuan Xu", "Fengbo Lan", "Zhixin Zhao", "Shengjie Wang", "Mengqiao Liu", "Jieqian Sun", "Yu Cheng", "Tao Zhang"], "title": "Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions", "comment": "14 pages, 10 figures, 8 tables", "summary": "Enabling robots to explore and act in unfamiliar environments under ambiguous human instructions by interactively identifying task-relevant objects (e.g., identifying cups or beverages for \"I'm thirsty\") remains challenging for existing vision-language model (VLM)-based methods. This challenge stems from inefficient reasoning and the lack of environmental interaction, which hinder real-time task planning and execution. To address this, We propose Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions (AIDE), a dual-stream framework that integrates interactive exploration with vision-language reasoning, where Multi-Stage Inference (MSI) serves as the decision-making stream and Accelerated Decision-Making (ADM) as the execution stream, enabling zero-shot affordance analysis and interpretation of ambiguous instructions. Extensive experiments in simulation and real-world environments show that AIDE achieves the task planning success rate of over 80\\% and more than 95\\% accuracy in closed-loop continuous execution at 10 Hz, outperforming existing VLM-based methods in diverse open-world scenarios.", "AI": {"tldr": "\u63d0\u51faAIDE\u6846\u67b6\u89e3\u51b3\u6a21\u7cca\u6307\u4ee4\u4e0b\u673a\u5668\u4eba\u63a2\u7d22\u548c\u6267\u884c\u4e2d\u7684\u6311\u6218\uff0c\u53d6\u5f97\u4f18\u79c0\u7684\u4efb\u52a1\u89c4\u5212\u548c\u6267\u884c\u7ed3\u679c\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8eVLM\u7684\u65b9\u6cd5\u5728\u5904\u7406\u6a21\u7cca\u4eba\u7c7b\u6307\u4ee4\u65f6\u9762\u4e34\u7684\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\u53ca\u7f3a\u4e4f\u73af\u5883\u4ea4\u4e92\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u53cc\u6d41\u6846\u67b6AIDE\uff0c\u5305\u62ec\u591a\u9636\u6bb5\u63a8\u7406\uff08MSI\uff09\u4f5c\u4e3a\u51b3\u7b56\u6d41\uff0c\u5feb\u901f\u51b3\u7b56\uff08ADM\uff09\u4f5c\u4e3a\u6267\u884c\u6d41\uff0c\u7ed3\u5408\u4e86\u4ea4\u4e92\u63a2\u7d22\u548c\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u3002", "result": "AIDE\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u73af\u5883\u4e2d\u7ecf\u8fc7\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4efb\u52a1\u89c4\u5212\u548c\u6267\u884c\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "AIDE\u5728\u591a\u79cd\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\u8d85\u8fc780%\u7684\u4efb\u52a1\u89c4\u5212\u6210\u529f\u7387\u548c95%\u4ee5\u4e0a\u7684\u95ed\u73af\u6301\u7eed\u6267\u884c\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8eVLM\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.05662", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.05662", "abs": "https://arxiv.org/abs/2602.05662", "authors": ["Swaroop Panda"], "title": "Making AI Agents Evaluate Misleading Charts without Nudging", "comment": null, "summary": "AI agents are increasingly used as low-cost proxies for early visualization evaluation. In an initial study of deliberately flawed charts, we test whether agents spontaneously penalise chart junk and misleading encodings without being prompted to look for errors. Using established scales (BeauVis and PREVis), the agent evaluated visualizations containing decorative clutter, manipulated axes, and distorted proportional cues. The ratings of aesthetic appeal and perceived readability often remained relatively high even when graphical integrity was compromised. These results suggest that un-nudged AI agent evaluation may underweight integrity-related defects unless such checks are explicitly elicited.", "AI": {"tldr": "AI\u4ee3\u7406\u5728\u8bc4\u4f30\u56fe\u8868\u65f6\uff0c\u5373\u4f7f\u5b58\u5728\u7f3a\u9677\u4e5f\u53ef\u80fd\u9ad8\u4f30\u5176\u7f8e\u89c2\u6027\u548c\u53ef\u8bfb\u6027\uff0c\u9664\u975e\u660e\u786e\u8981\u6c42\u5176\u68c0\u67e5\u5b8c\u6574\u6027\u95ee\u9898\u3002", "motivation": "\u63a2\u8ba8AI\u4ee3\u7406\u5728\u65e9\u671f\u53ef\u89c6\u5316\u8bc4\u4f30\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u6709\u610f\u8bbe\u8ba1\u7f3a\u9677\u7684\u56fe\u8868\u65f6\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u5df2\u786e\u7acb\u7684\u7f8e\u5b66\u8bc4\u4ef7\u5c3a\u5ea6\uff08BeauVis\u548cPREVis\uff09\u8bc4\u4f30\u5305\u542b\u56fe\u8868\u6742\u4e71\u3001\u64cd\u63a7\u8f74\u7ebf\u548c\u626d\u66f2\u6bd4\u4f8b\u7ebf\u7d22\u7684\u53ef\u89c6\u5316\u56fe\u8868\u3002", "result": "\u5c3d\u7ba1\u56fe\u5f62\u5b8c\u6574\u6027\u53d7\u5230\u635f\u5bb3\uff0cAI\u4ee3\u7406\u5bf9\u7f8e\u5b66\u5438\u5f15\u529b\u548c\u53ef\u8bfb\u6027\u7684\u8bc4\u5206\u901a\u5e38\u4ecd\u4fdd\u6301\u8f83\u9ad8\uff0c\u8fd9\u8868\u660e\u7f3a\u4e4f\u660e\u786e\u5f15\u5bfc\u65f6\uff0c\u4ee3\u7406\u53ef\u80fd\u4e0d\u4f1a\u5173\u6ce8\u5b8c\u6574\u6027\u7f3a\u9677\u3002", "conclusion": "AI\u4ee3\u7406\u5728\u6ca1\u6709\u63d0\u793a\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u80fd\u4f1a\u4f4e\u4f30\u4e0e\u56fe\u8868\u5b8c\u6574\u6027\u76f8\u5173\u7684\u7f3a\u9677\u3002"}}
{"id": "2602.05310", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05310", "abs": "https://arxiv.org/abs/2602.05310", "authors": ["Jipeng Kong", "Xinzhe Liu", "Yuhang Lin", "Jinrui Han", "S\u00f6ren Schwertfeger", "Chenjia Bai", "Xuelong Li"], "title": "Learning Soccer Skills for Humanoid Robots: A Progressive Perception-Action Framework", "comment": "13 pages, 9 figures, conference", "summary": "Soccer presents a significant challenge for humanoid robots, demanding tightly integrated perception-action capabilities for tasks like perception-guided kicking and whole-body balance control. Existing approaches suffer from inter-module instability in modular pipelines or conflicting training objectives in end-to-end frameworks. We propose Perception-Action integrated Decision-making (PAiD), a progressive architecture that decomposes soccer skill acquisition into three stages: motion-skill acquisition via human motion tracking, lightweight perception-action integration for positional generalization, and physics-aware sim-to-real transfer. This staged decomposition establishes stable foundational skills, avoids reward conflicts during perception integration, and minimizes sim-to-real gaps. Experiments on the Unitree G1 demonstrate high-fidelity human-like kicking with robust performance under diverse conditions-including static or rolling balls, various positions, and disturbances-while maintaining consistent execution across indoor and outdoor scenarios. Our divide-and-conquer strategy advances robust humanoid soccer capabilities and offers a scalable framework for complex embodied skill acquisition. The project page is available at https://soccer-humanoid.github.io/.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faPAiD\u67b6\u6784\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u7684\u6280\u80fd\u83b7\u53d6\uff0c\u589e\u5f3a\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u590d\u6742\u8db3\u7403\u4efb\u52a1\u4e2d\u7684\u611f\u77e5-\u884c\u52a8\u80fd\u529b\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u591a\u79cd\u73af\u5883\u4e0b\u5747\u80fd\u9ad8\u6548\u8868\u73b0\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6a21\u5757\u5316\u7ba1\u9053\u7684\u4e0d\u7a33\u5b9a\u6027\u548c\u7aef\u5230\u7aef\u6846\u67b6\u4e2d\u7684\u8bad\u7ec3\u76ee\u6807\u51b2\u7a81\uff0c\u4ee5\u63d0\u5347\u4eba\u5f62\u673a\u5668\u4eba\u5728\u8db3\u7403\u7b49\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u611f\u77e5-\u884c\u52a8\u80fd\u529b\u3002", "method": "\u91c7\u7528\u4e86\u5206\u9636\u6bb5\u7684\u6280\u80fd\u83b7\u53d6\uff0c\u5373\u901a\u8fc7\u4eba\u7c7b\u8fd0\u52a8\u8ddf\u8e2a\u83b7\u53d6\u8fd0\u52a8\u6280\u80fd\uff0c\u901a\u8fc7\u8f7b\u91cf\u5316\u7684\u611f\u77e5-\u884c\u52a8\u96c6\u6210\u5b9e\u73b0\u4f4d\u7f6e\u6cdb\u5316\uff0c\u6700\u540e\u5b9e\u73b0\u57fa\u4e8e\u7269\u7406\u7684\u6a21\u62df\u5230\u73b0\u5b9e\u8f6c\u79fb\u3002", "result": "\u5728Unitree G1\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPAiD\u67b6\u6784\u5728\u9759\u6001\u548c\u6eda\u52a8\u7403\u3001\u4e0d\u540c\u4f4d\u7f6e\u548c\u5e72\u6270\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u7684\u4eba\u7c7b\u8e22\u7403\u8868\u73b0\uff0c\u5e76\u5728\u5ba4\u5185\u548c\u5ba4\u5916\u573a\u666f\u4e2d\u6301\u7eed\u7a33\u5b9a\u6267\u884c\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684PAiD\u67b6\u6784\u901a\u8fc7\u5206\u9636\u6bb5\u7684\u6280\u80fd\u83b7\u53d6\u548c\u96c6\u6210\u65b9\u6cd5\uff0c\u6210\u529f\u63d0\u9ad8\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u8db3\u7403\u9886\u57df\u7684\u8868\u73b0\uff0c\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u9002\u5e94\u80fd\u529b\u548c\u7a33\u5b9a\u6027\u7684\u4f18\u52bf\u3002"}}
{"id": "2602.05671", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.05671", "abs": "https://arxiv.org/abs/2602.05671", "authors": ["Damien Rudaz", "Barbara Nino Carreras", "Sara Merlino", "Brian L. Due", "Barry Brown"], "title": "(Computer) Vision in Action: Comparing Remote Sighted Assistance and a Multimodal Voice Agent in Inspection Sequences", "comment": "Conditionally accepted at CHI 2026, 32 pages, 8 figures", "summary": "Does human-AI assistance unfold in the same way as human-human assistance? This research explores what can be learned from the expertise of blind individuals and sighted volunteers to inform the design of multimodal voice agents and address the enduring challenge of proactivity. Drawing on granular analysis of two representative fragments from a larger corpus, we contrast the practices co-produced by an experienced human remote sighted assistant and a blind participant-as they collaborate to find a stain on a blanket over the phone-with those achieved when the same participant worked with a multimodal voice agent on the same task, a few moments earlier. This comparison enables us to specify precisely which fundamental proactive practices the agent did not enact in situ. We conclude that, so long as multimodal voice agents cannot produce environmentally occasioned vision-based actions, they will lack a key resource relied upon by human remote sighted assistants.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u4eba\u7c7b\u52a9\u624b\u4e0e\u591a\u6a21\u6001\u8bed\u97f3\u52a9\u624b\u5728\u6267\u884c\u4efb\u52a1\u65f6\u7684\u8868\u73b0\uff0c\u6307\u51fa\u540e\u8005\u7f3a\u4e4f\u73af\u5883\u5f15\u53d1\u7684\u89c6\u89c9\u884c\u4e3a\uff0c\u5f71\u54cd\u5176\u4e3b\u52a8\u6027\u3002", "motivation": "\u4ece\u76f2\u4eba\u548c\u89c6\u529b\u6b63\u5e38\u5fd7\u613f\u8005\u7684\u4e13\u4e1a\u77e5\u8bc6\u4e2d\u6c72\u53d6\u7ecf\u9a8c\uff0c\u4ee5\u6307\u5bfc\u591a\u6a21\u6001\u8bed\u97f3\u52a9\u624b\u7684\u8bbe\u8ba1\uff0c\u5e76\u89e3\u51b3\u4e3b\u52a8\u6027\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u5206\u6790\u4e24\u4e2a\u4ee3\u8868\u6027\u7247\u6bb5\uff0c\u901a\u8fc7\u4e0e\u7ecf\u9a8c\u4e30\u5bcc\u7684\u4eba\u7c7b\u8fdc\u7a0b\u89c6\u529b\u52a9\u624b\u548c\u76f2\u4eba\u53c2\u4e0e\u8005\u7684\u5408\u4f5c\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u901a\u8fc7\u6bd4\u8f83\u4eba\u7c7b\u52a9\u624b\u4e0e\u591a\u6a21\u6001\u8bed\u97f3\u52a9\u624b\u5728\u76f8\u540c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u660e\u786e\u6307\u51fa\u4e86\u4ee3\u7406\u5728\u79ef\u6781\u4e3b\u52a8\u7684\u57fa\u672c\u5b9e\u8df5\u4e2d\u672a\u80fd\u5b9e\u73b0\u7684\u65b9\u9762\u3002", "conclusion": "\u591a\u6a21\u6001\u8bed\u97f3\u52a9\u624b\u65e0\u6cd5\u4ea7\u751f\u73af\u5883\u5f15\u53d1\u7684\u57fa\u4e8e\u89c6\u89c9\u7684\u884c\u4e3a\uff0c\u4ece\u800c\u7f3a\u4e4f\u4eba\u7c7b\u8fdc\u7a0b\u89c6\u529b\u52a9\u624b\u6240\u4f9d\u8d56\u7684\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2602.05325", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05325", "abs": "https://arxiv.org/abs/2602.05325", "authors": ["Jiacheng Fan", "Zhiyue Zhao", "Yiqian Zhang", "Chao Chen", "Peide Wang", "Hengdi Zhang", "Zhengxue Cheng"], "title": "RoboPaint: From Human Demonstration to Any Robot and Any View", "comment": "17 pages", "summary": "Acquiring large-scale, high-fidelity robot demonstration data remains a critical bottleneck for scaling Vision-Language-Action (VLA) models in dexterous manipulation. We propose a Real-Sim-Real data collection and data editing pipeline that transforms human demonstrations into robot-executable, environment-specific training data without direct robot teleoperation. Standardized data collection rooms are built to capture multimodal human demonstrations (synchronized 3 RGB-D videos, 11 RGB videos, 29-DoF glove joint angles, and 14-channel tactile signals). Based on these human demonstrations, we introduce a tactile-aware retargeting method that maps human hand states to robot dex-hand states via geometry and force-guided optimization. Then the retargeted robot trajectories are rendered in a photorealistic Isaac Sim environment to build robot training data. Real world experiments have demonstrated: (1) The retargeted dex-hand trajectories achieve an 84\\% success rate across 10 diverse object manipulation tasks. (2) VLA policies (Pi0.5) trained exclusively on our generated data achieve 80\\% average success rate on three representative tasks, i.e., pick-and-place, pushing and pouring. To conclude, robot training data can be efficiently \"painted\" from human demonstrations using our real-sim-real data pipeline. We offer a scalable, cost-effective alternative to teleoperation with minimal performance loss for complex dexterous manipulation.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6570\u636e\u7ba1\u9053\uff0c\u5c06\u4eba\u7c7b\u6f14\u793a\u8f6c\u5316\u4e3a\u673a\u5668\u4eba\u8bad\u7ec3\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u7075\u5de7\u64cd\u4f5c\u4e2d\u5927\u89c4\u6a21\u9ad8\u4fdd\u771f\u673a\u5668\u4eba\u6f14\u793a\u6570\u636e\u83b7\u53d6\u7684\u5173\u952e\u74f6\u9888\uff0c\u4ee5\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5b9e\u5883-\u4eff\u771f-\u5b9e\u5883\u7684\u6570\u636e\u6536\u96c6\u548c\u7f16\u8f91\u7ba1\u9053\uff0c\u5c06\u4eba\u7c7b\u6f14\u793a\u8f6c\u6362\u4e3a\u673a\u5668\u4eba\u53ef\u6267\u884c\u548c\u73af\u5883\u7279\u5b9a\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u91c7\u7528\u89e6\u89c9\u611f\u77e5\u91cd\u5b9a\u5411\u65b9\u6cd5\uff0c\u57fa\u4e8e\u51e0\u4f55\u548c\u529b\u5f15\u5bfc\u4f18\u5316\u5c06\u4eba\u7c7b\u624b\u7684\u72b6\u6001\u6620\u5c04\u5230\u673a\u5668\u4eba\u7075\u5de7\u624b\u7684\u72b6\u6001\u3002", "result": "\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8868\u660e\uff0c\u91cd\u5b9a\u5411\u7684\u7075\u5de7\u624b\u8f68\u8ff9\u572810\u4e2a\u4e0d\u540c\u7269\u4f53\u64cd\u4f5c\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e8684%\u7684\u6210\u529f\u7387\uff1b\u57fa\u4e8e\u751f\u6210\u6570\u636e\u8bad\u7ec3\u7684VLA\u7b56\u7565\u5728\u4e09\u9879\u4ee3\u8868\u6027\u4efb\u52a1\u4e0a\u5e73\u5747\u6210\u529f\u7387\u8fbe\u523080%\u3002", "conclusion": "\u6211\u4eec\u7684\u5b9e\u5883-\u4eff\u771f-\u5b9e\u5883\u6570\u636e\u7ba1\u9053\u80fd\u591f\u6709\u6548\u5730\u4ece\u4eba\u7c7b\u6f14\u793a\u4e2d\u751f\u6210\u673a\u5668\u4eba\u8bad\u7ec3\u6570\u636e\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u66ff\u4ee3\u65b9\u6848\u6765\u8fdb\u884c\u590d\u6742\u7684\u7075\u5de7\u64cd\u4f5c\u8bad\u7ec3\uff0c\u6027\u80fd\u635f\u5931\u6700\u5c0f\u3002"}}
{"id": "2602.05687", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05687", "abs": "https://arxiv.org/abs/2602.05687", "authors": ["Pavithren V S Pakianathan", "Rania Islambouli", "Diogo Branco", "Albrecht Schmidt", "Tiago Guerreiro", "Jan David Smeddinck"], "title": "Exploring AI-Augmented Sensemaking of Patient-Generated Health Data: A Mixed-Method Study with Healthcare Professionals in Cardiac Risk Reduction", "comment": null, "summary": "Individuals are increasingly generating substantial personal health and lifestyle data, e.g. through wearables and smartphones. While such data could transform preventative care, its integration into clinical practice is hindered by its scale, heterogeneity and the time pressure and data literacy of healthcare professionals (HCPs). We explore how large language models (LLMs) can support sensemaking of patient-generated health data (PGHD) with automated summaries and natural language data exploration. Using cardiovascular disease (CVD) risk reduction as a use case, 16 HCPs reviewed multimodal PGHD in a mixed-methods study with a prototype that integrated common charts, LLM-generated summaries, and a conversational interface. Findings show that AI summaries provided quick overviews that anchored exploration, while conversational interaction supported flexible analysis and bridged data-literacy gaps. However, HCPs raised concerns about transparency, privacy, and overreliance. We contribute empirical insights and sociotechnical design implications for integrating AI-driven summarization and conversation into clinical workflows to support PGHD sensemaking.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u652f\u6301\u60a3\u8005\u751f\u6210\u5065\u5eb7\u6570\u636e\u7684\u7406\u89e3\uff0c\u63d0\u51fa\u4e86\u5c06AI\u9a71\u52a8\u7684\u6458\u8981\u548c\u5bf9\u8bdd\u6574\u5408\u5230\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u5efa\u8bae\u3002", "motivation": "\u968f\u7740\u4e2a\u4eba\u5065\u5eb7\u548c\u751f\u6d3b\u65b9\u5f0f\u6570\u636e\u7684\u751f\u6210\u589e\u52a0\uff0c\u5982\u4f55\u6709\u6548\u6574\u5408\u8fd9\u4e9b\u6570\u636e\u4ee5\u6539\u5584\u9884\u9632\u4fdd\u5065\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\uff0c16\u540d\u533b\u7597\u4fdd\u5065\u4e13\u4e1a\u4eba\u5458\u8bc4\u4f30\u4e86\u4e00\u4e2a\u539f\u578b\uff0c\u8be5\u539f\u578b\u6574\u5408\u4e86\u5e38\u89c1\u56fe\u8868\u3001LLM\u751f\u6210\u7684\u6458\u8981\u548c\u5bf9\u8bdd\u754c\u9762\u3002", "result": "AI\u751f\u6210\u7684\u6458\u8981\u63d0\u4f9b\u4e86\u5feb\u901f\u7684\u6982\u8ff0\uff0c\u4fc3\u8fdb\u4e86\u7075\u6d3b\u7684\u5206\u6790\uff0c\u5e76\u5e2e\u52a9\u89e3\u51b3\u6570\u636e\u7d20\u517b\u5dee\u8ddd\u3002", "conclusion": "\u4eba\u5de5\u667a\u80fd\u751f\u6210\u7684\u6458\u8981\u548c\u5bf9\u8bdd\u754c\u9762\u5728\u652f\u6301\u60a3\u8005\u751f\u6210\u5065\u5eb7\u6570\u636e\u7684\u7406\u89e3\u4e2d\u5c55\u793a\u4e86\u6709\u6548\u6027\uff0c\u4f46\u4e5f\u5f15\u53d1\u4e86\u5bf9\u900f\u660e\u5ea6\u548c\u9690\u79c1\u7684\u62c5\u5fe7\u3002"}}
{"id": "2602.05441", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05441", "abs": "https://arxiv.org/abs/2602.05441", "authors": ["Dean Fortier", "Timothy Adamson", "Tess Hellebrekers", "Teresa LaScala", "Kofi Ennin", "Michael Murray", "Andrey Kolobov", "Galen Mullins"], "title": "Benchmarking Affordance Generalization with BusyBox", "comment": null, "summary": "Vision-Language-Action (VLA) models have been attracting the attention of researchers and practitioners thanks to their promise of generalization. Although single-task policies still offer competitive performance, VLAs are increasingly able to handle commands and environments unseen in their training set. While generalization in vision and language space is undoubtedly important for robust versatile behaviors, a key meta-skill VLAs need to possess is affordance generalization -- the ability to manipulate new objects with familiar physical features.\n  In this work, we present BusyBox, a physical benchmark for systematic semi-automatic evaluation of VLAs' affordance generalization. BusyBox consists of 6 modules with switches, sliders, wires, buttons, a display, and a dial. The modules can be swapped and rotated to create a multitude of BusyBox variations with different visual appearances but the same set of affordances. We empirically demonstrate that generalization across BusyBox variants is highly challenging even for strong open-weights VLAs such as $\u03c0_{0.5}$ and GR00T-N1.6. To encourage the research community to evaluate their own VLAs on BusyBox and to propose new affordance generalization experiments, we have designed BusyBox to be easy to build in most robotics labs. We release the full set of CAD files for 3D-printing its parts as well as a bill of materials for (optionally) assembling its electronics. We also publish a dataset of language-annotated demonstrations that we collected using the common bimanual Mobile Aloha robot on the canonical BusyBox configuration. All of the released materials are available at https://microsoft.github.io/BusyBox.", "AI": {"tldr": "BusyBox \u662f\u4e00\u4e2a\u65b0\u7684\u7269\u7406\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u884c\u52a8\u6a21\u578b\u5728\u6613\u64cd\u4f5c\u6027\u6cdb\u5316\u65b9\u9762\u7684\u80fd\u529b\uff0c\u63d0\u4f9b\u4e86\u65b9\u4fbf\u7684\u6784\u5efa\u65b9\u6cd5\u548c\u76f8\u5173\u6570\u636e\u96c6\u3002", "motivation": "\u968f\u7740\u7814\u7a76\u4eba\u5458\u5bf9 VLA \u6a21\u578b\u7684\u5173\u6ce8\u589e\u52a0\uff0c\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u6027\u7684\u5de5\u5177\u6765\u8bc4\u4f30\u5b83\u4eec\u5728\u5904\u7406\u672a\u77e5\u5bf9\u8c61\u65f6\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u53ef\u64cd\u4f5c\u6027\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u53ef\u4ea4\u6362\u548c\u65cb\u8f6c\u7684\u6a21\u5757\uff0c\u751f\u6210\u4e0d\u540c\u7684 BusyBox \u53d8\u4f53\uff0c\u8bc4\u4f30 VLAs \u7684\u4f20\u8fbe\u80fd\u529b\u3002", "result": "\u5373\u4f7f\u5bf9\u4e8e\u5f3a\u5927\u7684\u5f00\u653e\u6743\u91cd VLA \u6a21\u578b\uff0c\u5982 $\u03c0_{0.5}$ \u548c GR00T-N1.6\uff0c\u8de8 BusyBox \u53d8\u4f53\u7684\u6cdb\u5316\u4ecd\u7136\u975e\u5e38\u5177\u6709\u6311\u6218\u6027\u3002", "conclusion": "BusyBox \u662f\u4e00\u4e2a\u5b9e\u7528\u7684\u57fa\u51c6\uff0c\u65e8\u5728\u4fc3\u8fdb\u7814\u7a76\u4eba\u5458\u5bf9 VLA \u80fd\u529b\u7684\u8bc4\u4f30\u548c\u65b0\u5b9e\u9a8c\u7684\u63d0\u51fa\u3002"}}
{"id": "2602.05819", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.05819", "abs": "https://arxiv.org/abs/2602.05819", "authors": ["Yeon Su Park", "Nadia Azzahra Putri Arvi", "Seoyoung Kim", "Juho Kim"], "title": "Authorship Drift: How Self-Efficacy and Trust Evolve During LLM-Assisted Writing", "comment": "Conditionally accepted to CHI 2026", "summary": "Large language models (LLMs) are increasingly used as collaborative partners in writing. However, this raises a critical challenge of authorship, as users and models jointly shape text across interaction turns. Understanding authorship in this context requires examining users' evolving internal states during collaboration, particularly self-efficacy and trust. Yet, the dynamics of these states and their associations with users' prompting strategies and authorship outcomes remain underexplored. We examined these dynamics through a study of 302 participants in LLM-assisted writing, capturing interaction logs and turn-by-turn self-efficacy and trust ratings. Our analysis showed that collaboration generally decreased users' self-efficacy while increasing trust. Participants who lost self-efficacy were more likely to ask the LLM to edit their work directly, whereas those who recovered self-efficacy requested more review and feedback. Furthermore, participants with stable self-efficacy showed higher actual and perceived authorship of the final text. Based on these findings, we propose design implications for understanding and supporting authorship in human-LLM collaboration.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8f85\u52a9\u5199\u4f5c\u4e2d\uff0c\u7528\u6237\u81ea\u6211\u6548\u80fd\u611f\u548c\u4fe1\u4efb\u7684\u53d8\u5316\u53ca\u5176\u5bf9\u4f5c\u8005\u8eab\u4efd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u9700\u8981\u5173\u6ce8\u5982\u4f55\u652f\u6301\u81ea\u6211\u6548\u80fd\u611f\u7684\u7ef4\u6301\uff0c\u4ee5\u4fc3\u8fdb\u4eba\u7c7b\u4e0eLLM\u7684\u6709\u6548\u5408\u4f5c\u3002", "motivation": "\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u534f\u4f5c\u5199\u4f5c\u73af\u5883\u4e2d\u63a2\u8ba8\u7528\u6237\u7684\u81ea\u6211\u6548\u80fd\u611f\u548c\u4fe1\u4efb\u7684\u52a8\u6001\u53d8\u5316\uff0c\u4ee5\u4e86\u89e3\u5176\u5bf9\u4f5c\u8005\u8eab\u4efd\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5bf9302\u540d\u53c2\u4e0e\u8005\u7684LLM\u8f85\u52a9\u5199\u4f5c\u8fdb\u884c\u7814\u7a76\uff0c\u6536\u96c6\u4e92\u52a8\u65e5\u5fd7\u548c\u9010\u6b65\u7684\u81ea\u6211\u6548\u80fd\u611f\u53ca\u4fe1\u4efb\u8bc4\u7ea7\u3002", "result": "\u5408\u4f5c\u8fc7\u7a0b\u901a\u5e38\u4f1a\u964d\u4f4e\u7528\u6237\u7684\u81ea\u6211\u6548\u80fd\u611f\uff0c\u4f46\u63d0\u9ad8\u4fe1\u4efb\u5ea6\u3002\u5931\u53bb\u81ea\u6211\u6548\u80fd\u611f\u7684\u53c2\u4e0e\u8005\u66f4\u503e\u5411\u4e8e\u8bf7\u6c42LLM\u76f4\u63a5\u7f16\u8f91\u4ed6\u4eec\u7684\u4f5c\u54c1\uff0c\u800c\u81ea\u6211\u6548\u80fd\u611f\u6062\u590d\u7684\u53c2\u4e0e\u8005\u5219\u8bf7\u6c42\u66f4\u591a\u7684\u5ba1\u9605\u548c\u53cd\u9988\u3002", "conclusion": "\u7a33\u5b9a\u7684\u81ea\u6211\u6548\u80fd\u611f\u4e0e\u6700\u7ec8\u6587\u672c\u7684\u5b9e\u9645\u548c\u611f\u77e5\u4f5c\u8005\u8eab\u4efd\u8f83\u9ad8\u76f8\u5173\uff0c\u56e0\u6b64\u5728\u4fc3\u8fdb\u4eba\u7c7b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5408\u4f5c\u8fc7\u7a0b\u4e2d\uff0c\u9700\u8981\u5173\u6ce8\u7528\u6237\u81ea\u6211\u6548\u80fd\u611f\u7684\u53d8\u5316\u3002"}}
{"id": "2602.05456", "categories": ["cs.RO", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.05456", "abs": "https://arxiv.org/abs/2602.05456", "authors": ["Maksym Figat", "Ryan M. Mackey", "Michel D. Ingham"], "title": "Ontology-Driven Robotic Specification Synthesis", "comment": "8 pages, 9 figures, 3 tables, journal", "summary": "This paper addresses robotic system engineering for safety- and mission-critical applications by bridging the gap between high-level objectives and formal, executable specifications. The proposed method, Robotic System Task to Model Transformation Methodology (RSTM2) is an ontology-driven, hierarchical approach using stochastic timed Petri nets with resources, enabling Monte Carlo simulations at mission, system, and subsystem levels. A hypothetical case study demonstrates how the RSTM2 method supports architectural trades, resource allocation, and performance analysis under uncertainty. Ontological concepts further enable explainable AI-based assistants, facilitating fully autonomous specification synthesis. The methodology offers particular benefits to complex multi-robot systems, such as the NASA CADRE mission, representing decentralized, resource-aware, and adaptive autonomous systems of the future.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRSTM2\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u673a\u5668\u4eba\u7cfb\u7edf\u5de5\u7a0b\uff0c\u7279\u522b\u662f\u5728\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\uff0c\u63d0\u5347\u4efb\u52a1\u6267\u884c\u548c\u8d44\u6e90\u7ba1\u7406\u7684\u5b89\u5168\u6027\u4e0e\u6548\u679c\u3002", "motivation": "\u65e8\u5728\u586b\u8865\u9ad8\u5c42\u76ee\u6807\u548c\u5f62\u5f0f\u5316\u53ef\u6267\u884c\u89c4\u8303\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u63d0\u5347\u673a\u5668\u4eba\u7cfb\u7edf\u5de5\u7a0b\u5728\u5b89\u5168\u548c\u5173\u952e\u4efb\u52a1\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u91c7\u7528\u5206\u5c42\u7684\u672c\u4f53\u9a71\u52a8\u65b9\u6cd5\uff0c\u901a\u8fc7\u968f\u673a\u65f6\u5e8fPetri\u7f51\u4e0e\u8d44\u6e90\u7ed3\u5408\uff0c\u5b9e\u73b0\u4efb\u52a1\u5230\u6a21\u578b\u7684\u8f6c\u6362\uff0c\u5e76\u8fdb\u884cMonte Carlo\u6a21\u62df\u3002", "result": "\u901a\u8fc7\u5047\u8bbe\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86RSTM2\u65b9\u6cd5\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u652f\u6301\u67b6\u6784\u6743\u8861\u3001\u8d44\u6e90\u5206\u914d\u548c\u6027\u80fd\u5206\u6790\u7684\u80fd\u529b\uff0c\u5e76\u4fc3\u8fdb\u4e86\u89e3\u91ca\u6027AI\u52a9\u624b\u7684\u5e94\u7528\u3002", "conclusion": "\u672c\u65b9\u6cd5\u5bf9\u590d\u6742\u591a\u673a\u5668\u4eba\u7cfb\u7edf\uff08\u5982NASA CADRE\u4efb\u52a1\uff09\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8d44\u6e90\u7ba1\u7406\u5b9e\u73b0\u66f4\u597d\u7684\u5b89\u5168\u6027\u548c\u4efb\u52a1\u6267\u884c\u6548\u679c\u3002"}}
{"id": "2602.05825", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.05825", "abs": "https://arxiv.org/abs/2602.05825", "authors": ["Lena Hegemann", "Xinyi Wen", "Michael A. Hedderich", "Tarmo Nurmi", "Hariharan Subramonyam"], "title": "ToMigo: Interpretable Design Concept Graphs for Aligning Generative AI with Creative Intent", "comment": "18 pages, 10 figures, 2 tables", "summary": "Generative AI often produces results misaligned with user intentions, for example, resolving ambiguous prompts in unexpected ways. Despite existing approaches to clarify intent, a major challenge remains: understanding and influencing AI's interpretation of user intent through simple, direct inputs requiring no expertise or rigid procedures. We present ToMigo, representing intent as design concept graphs: nodes represent choices of purpose, content, or style, while edges link them with interpretable explanations. Applied to graphic design, ToMigo infers intent from reference images and text. We derived a schema of node types and edges from pre-study data, informing a multimodal large language model to generate graphs aligning nodes externally with user intent and internally toward a unified design goal. This structure enables users to explore AI reasoning and directly manipulate the design concept. In our user studies, ToMigo received high alignment ratings and captured most user intentions well. Users reported greater control and found interactive features-editable graphs, reflective chats, concept-design realignment-useful for evolving and realizing their design ideas.", "AI": {"tldr": "ToMigo\u901a\u8fc7\u8bbe\u8ba1\u6982\u5ff5\u56fe\u5e2e\u52a9\u7528\u6237\u66f4\u597d\u5730\u8868\u8fbe\u548c\u63a7\u5236\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u8bbe\u8ba1\u610f\u56fe\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u4e0e\u7528\u6237\u610f\u56fe\u4e4b\u95f4\u7684\u9519\u4f4d\uff0c\u901a\u8fc7\u7b80\u5355\u76f4\u63a5\u7684\u8f93\u5165\u7406\u89e3\u548c\u5f71\u54cdAI\u5bf9\u7528\u6237\u610f\u56fe\u7684\u89e3\u91ca\u3002", "method": "\u57fa\u4e8e\u56fe\u5f62\u8bbe\u8ba1\u7684\u7528\u6237\u7814\u7a76\uff0c\u7528\u4e8e\u9a8c\u8bc1ToMigo\u7684\u6709\u6548\u6027\uff0c\u91cd\u70b9\u5173\u6ce8\u7528\u6237\u610f\u56fe\u7684\u6355\u6349\u548c\u4ea4\u4e92\u7279\u6027\u3002", "result": "ToMigo\u6709\u6548\u5730\u63a8\u65ad\u7528\u6237\u610f\u56fe\uff0c\u5e76\u63d0\u4f9b\u4e86\u9ad8\u7b49\u7ea7\u7684\u5bf9\u9f50\u8bc4\u5206\uff0c\u7528\u6237\u89c9\u5f97\u63a7\u5236\u611f\u589e\u5f3a\uff0c\u4e14\u5176\u4e92\u52a8\u529f\u80fd\u5bf9\u8bbe\u8ba1\u601d\u8def\u7684\u6f14\u53d8\u548c\u5b9e\u73b0\u6709\u5e2e\u52a9\u3002", "conclusion": "\u7528\u6237\u5bf9ToMigo\u7684\u4e92\u52a8\u7279\u6027\u8868\u793a\u6ee1\u610f\uff0c\u8ba4\u4e3a\u53ef\u4ee5\u66f4\u597d\u5730\u63a7\u5236\u8bbe\u8ba1\u8fc7\u7a0b\uff0c\u5e76\u6709\u6548\u5730\u8868\u8fbe\u8bbe\u8ba1\u610f\u56fe\u3002"}}
{"id": "2602.05468", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05468", "abs": "https://arxiv.org/abs/2602.05468", "authors": ["Pranav Ponnivalavan", "Satoshi Funabashi", "Alexander Schmitz", "Tetsuya Ogata", "Shigeki Sugano"], "title": "TaSA: Two-Phased Deep Predictive Learning of Tactile Sensory Attenuation for Improving In-Grasp Manipulation", "comment": "8 pages, 8 figures, 8 tables, ICRA2026 accepted", "summary": "Humans can achieve diverse in-hand manipulations, such as object pinching and tool use, which often involve simultaneous contact between the object and multiple fingers. This is still an open issue for robotic hands because such dexterous manipulation requires distinguishing between tactile sensations generated by their self-contact and those arising from external contact. Otherwise, object/robot breakage happens due to contacts/collisions. Indeed, most approaches ignore self-contact altogether, by constraining motion to avoid/ignore self-tactile information during contact. While this reduces complexity, it also limits generalization to real-world scenarios where self-contact is inevitable. Humans overcome this challenge through self-touch perception, using predictive mechanisms that anticipate the tactile consequences of their own motion, through a principle called sensory attenuation, where the nervous system differentiates predictable self-touch signals, allowing novel object stimuli to stand out as relevant. Deriving from this, we introduce TaSA, a two-phased deep predictive learning framework. In the first phase, TaSA explicitly learns self-touch dynamics, modeling how a robot's own actions generate tactile feedback. In the second phase, this learned model is incorporated into the motion learning phase, to emphasize object contact signals during manipulation. We evaluate TaSA on a set of insertion tasks, which demand fine tactile discrimination: inserting a pencil lead into a mechanical pencil, inserting coins into a slot, and fixing a paper clip onto a sheet of paper, with various orientations, positions, and sizes. Across all tasks, policies trained with TaSA achieve significantly higher success rates than baseline methods, demonstrating that structured tactile perception with self-touch based on sensory attenuation is critical for dexterous robotic manipulation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86TaSA\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u81ea\u89e6\u52a8\u6001\uff0c\u6539\u8fdb\u673a\u5668\u4eba\u5bf9\u590d\u6742\u64cd\u63a7\u4efb\u52a1\u7684\u5904\u7406\uff0c\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u4eba\u7c7b\u80fd\u591f\u8fdb\u884c\u590d\u6742\u7684\u624b\u90e8\u64cd\u4f5c\uff0c\u901a\u8fc7\u9884\u6d4b\u673a\u5236\u514b\u670d\u81ea\u6211\u63a5\u89e6\u4e0e\u5916\u90e8\u63a5\u89e6\u7684\u533a\u5206\u95ee\u9898\uff0c\u800c\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u81ea\u63a5\u89e6\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u5728\u73b0\u5b9e\u573a\u666f\u4e0b\u7684\u9002\u5e94\u6027\u3002", "method": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aTaSA\u7684\u4e24\u9636\u6bb5\u6df1\u5ea6\u9884\u6d4b\u5b66\u4e60\u6846\u67b6\uff0c\u7b2c\u4e00\u9636\u6bb5\u5b66\u4e60\u81ea\u89e6\u52a8\u6001\uff0c\u7b2c\u4e8c\u9636\u6bb5\u5c06\u8fd9\u4e00\u6a21\u578b\u7eb3\u5165\u8fd0\u52a8\u5b66\u4e60\uff0c\u5f3a\u8c03\u7269\u4f53\u63a5\u89e6\u4fe1\u53f7\u3002", "result": "\u5728\u4e00\u7cfb\u5217\u63d2\u5165\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528TaSA\u8bad\u7ec3\u7684\u7b56\u7565\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u529f\u7387\uff0c\u663e\u793a\u51fa\u7ed3\u6784\u5316\u89e6\u89c9\u611f\u77e5\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165TaSA\u6846\u67b6\uff0c\u7814\u7a76\u8868\u660e\uff0c\u81ea\u89e6\u77e5\u89c9\u57fa\u4e8e\u611f\u5b98\u6291\u5236\u7684\u7ed3\u6784\u5316\u89e6\u89c9\u611f\u77e5\u5bf9\u7075\u5de7\u7684\u673a\u5668\u4eba\u64cd\u63a7\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2602.05826", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.05826", "abs": "https://arxiv.org/abs/2602.05826", "authors": ["Dongyijie Primo Pan", "Shuyue Li", "Yawei Zhao", "Junkun Long", "Hao Li", "Pan Hui"], "title": "Whispers of the Butterfly: A Research-through-Design Exploration of In-Situ Conversational AI Guidance in Large-Scale Outdoor MR Exhibitions", "comment": null, "summary": "Large-scale outdoor mixed reality (MR) art exhibitions distribute curated virtual works across open public spaces, but interpretation rarely scales without turning exploration into a scripted tour. Through Research-through-Design, we created Dream-Butterfly, an in-situ conversational AI docent embodied as a small non-human companion that visitors summon for multilingual, exhibition-grounded explanations. We deployed Dream-Butterfly in a large-scale outdoor MR exhibition at a public university campus in southern China, and conducted an in-the-wild between-subject study (N=24) comparing a primarily human-led tour with an AI-led tour while keeping staff for safety in both conditions. Combining questionnaires and semi-structured interviews, we characterize how shifting the primary explanation channel reshapes explanation access, perceived responsiveness, immersion, and workload, and how visitors negotiate responsibility handoffs among staff, the AI guide, and themselves. We distill transferable design implications for configuring mixed human-AI guiding roles and embodying conversational agents in mobile, safety-constrained outdoor MR exhibitions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u5728\u5927\u578b\u6237\u5916MR\u827a\u672f\u5c55\u89c8\u4e2d\u5f15\u5165AI\u5bfc\u89c8\u5458\u7684\u6548\u679c\u4e0e\u8bbe\u8ba1\u542f\u793a\uff0c\u53d1\u73b0AI\u5bfc\u89c8\u53ef\u6539\u5584\u8bbf\u5ba2\u4f53\u9a8c\u3002", "motivation": "\u5927\u578b\u6237\u5916\u6df7\u5408\u73b0\u5b9e\u827a\u672f\u5c55\u89c8\u4e2d\uff0c\u4f20\u7edf\u5bfc\u89c8\u65b9\u5f0f\u65e0\u6cd5\u6709\u6548\u6269\u5c55\uff0c\u63a2\u7d22\u4f53\u9a8c\u5f80\u5f80\u53d8\u6210\u4e86\u811a\u672c\u5316\u7684\u5bfc\u89c8\uff0c\u56e0\u6b64\u9700\u8981\u5bfb\u627e\u521b\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7Research-through-Design\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u5e76\u90e8\u7f72\u4e86\u4e00\u4e2aAI\u5bfc\u89c8\u5458\uff0c\u5e76\u8fdb\u884c\u4e86\u4e00\u9879\u670924\u540d\u53c2\u4e0e\u8005\u7684\u7530\u91ce\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4eba\u7c7b\u5bfc\u89c8\u4e0eAI\u5bfc\u89c8\u7684\u4f53\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cAI\u5bfc\u89c8\u63d0\u5347\u4e86\u5bf9\u4e8e\u89e3\u91ca\u4fe1\u606f\u7684\u83b7\u53d6\u3001\u53cd\u5e94\u7075\u654f\u5ea6\u548c\u6c89\u6d78\u611f\uff0c\u540c\u65f6\u51cf\u5c11\u53c2\u4e0e\u8005\u7684\u5de5\u4f5c\u8d1f\u62c5\uff1b\u5e76\u63ed\u793a\u4e86\u8bbf\u5ba2\u5728\u5bfc\u89c8\u8fc7\u7a0b\u4e2d\u5982\u4f55\u8fdb\u884c\u89d2\u8272\u8d23\u4efb\u7684\u4ea4\u63a5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684AI\u5bfc\u89c8\u5458Dream-Butterfly\uff0c\u901a\u8fc7\u5bf9\u6bd4\u4eba\u7c7b\u5bfc\u89c8\u4e0eAI\u5bfc\u89c8\uff0c\u5c55\u73b0\u4e86AI\u5728\u5927\u578b\u6237\u5916\u6df7\u5408\u73b0\u5b9e\u5c55\u89c8\u4e2d\u7684\u72ec\u7279\u4f18\u52bf\u548c\u8bbe\u8ba1\u542f\u793a\u3002"}}
{"id": "2602.05513", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05513", "abs": "https://arxiv.org/abs/2602.05513", "authors": ["Xukun Li", "Yu Sun", "Lei Zhang", "Bosheng Huang", "Yibo Peng", "Yuan Meng", "Haojun Jiang", "Shaoxuan Xie", "Guacai Yao", "Alois Knoll", "Zhenshan Bing", "Xinlong Wang", "Zhenguo Sun"], "title": "DECO: Decoupled Multimodal Diffusion Transformer for Bimanual Dexterous Manipulation with a Plugin Tactile Adapter", "comment": "17 pages, 8 figures", "summary": "Overview of the Proposed DECO Framework.} DECO is a DiT-based policy that decouples multimodal conditioning. Image and action tokens interact via joint self attention, while proprioceptive states and optional conditions are injected through adaptive layer normalization. Tactile signals are injected via cross attention, while a lightweight LoRA-based adapter is used to efficiently fine-tune the pretrained policy. DECO is also accompanied by DECO-50, a bimanual dexterous manipulation dataset with tactile sensing, consisting of 4 scenarios and 28 sub-tasks, covering more than 50 hours of data, approximately 5 million frames, and 8,000 successful trajectories.", "AI": {"tldr": "DECO\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u89e3\u8026\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548c\u6570\u636e\u96c6DECO-50\u6539\u5584\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u80fd\u529b\uff0cDECO\u6846\u67b6\u65e8\u5728\u6709\u6548\u96c6\u6210\u4e0d\u540c\u7684\u8f93\u5165\u6a21\u6001\uff08\u56fe\u50cf\u3001\u52a8\u4f5c\u3001\u89e6\u89c9\u4fe1\u53f7\u7b49\uff09\uff0c\u5e76\u6539\u5584\u5176\u81ea\u9002\u5e94\u80fd\u529b\u3002", "method": "DECO\u6846\u67b6\u91c7\u7528\u4e86\u57fa\u4e8eDiT\u7684\u7b56\u7565\uff0c\u7ed3\u5408\u4e86\u591a\u6a21\u6001\u6761\u4ef6\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u540c\u65f6\u5229\u7528\u8f7b\u91cf\u5316LoRA\u9002\u914d\u5668\u8fdb\u884c\u8c03\u4f18\u3002", "result": "DECO\u53ca\u5176\u914d\u5957\u7684\u6570\u636e\u96c6DECO-50\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u6570\u636e\uff0c\u786e\u4fdd\u4e86\u653f\u7b56\u7684\u6709\u6548\u6027\u548c\u7075\u6d3b\u6027\u3002", "conclusion": "DECO\u6846\u67b6\u901a\u8fc7\u591a\u6a21\u6001\u6761\u4ef6\u89e3\u8026\u548c\u521b\u65b0\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6539\u5584\u4e86\u673a\u5668\u4eba\u64cd\u63a7\u6027\u80fd\uff0c\u5e76\u751f\u6210\u4e86\u4e30\u5bcc\u7684\u6570\u636e\u96c6\u4ee5\u652f\u6301\u8be5\u653f\u7b56\u7684\u6d4b\u8bd5\u548c\u6539\u8fdb\u3002"}}
{"id": "2602.05837", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.05837", "abs": "https://arxiv.org/abs/2602.05837", "authors": ["Aashish Panta", "Giorgio Scorzelli", "Amy A. Gooch", "Werner Sun", "Katherine S. Shanks", "Suchismita Sarker", "Devin Bougie", "Keara Soloway", "Rolf Verberg", "Tracy Berman", "Glenn Tarcea", "John Allison", "Michela Taufer", "Valerio Pascucci"], "title": "Large Data Acquisition and Analytics at Synchrotron Radiation Facilities", "comment": "10 pages, 11 figures, Accepted at IEEE BigDataW 2025", "summary": "Synchrotron facilities like the Cornell High Energy Synchrotron Source (CHESS) generate massive data volumes from complex beamline experiments, but face challenges such as limited access time, the need for on-site experiment monitoring, and managing terabytes of data per user group. We present the design, deployment, and evaluation of a framework that addresses CHESS's data acquisition and management issues. Deployed on a secure CHESS server, our system provides real time, web-based tools for remote experiment monitoring and data quality assessment, improving operational efficiency. Implemented across three beamlines (ID3A, ID3B, ID4B), the framework managed 50-100 TB of data and over 10 million files in late 2024. Testing with 43 research groups and 86 dashboards showed reduced overhead, improved accessibility, and streamlined data workflows. Our paper highlights the development, deployment, and evaluation of our framework and its transformative impact on synchrotron data acquisition.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u65b0\u6846\u67b6\u7684\u8bbe\u8ba1\u548c\u8bc4\u4f30\uff0c\u8be5\u6846\u67b6\u6539\u5584\u4e86CHESS\u7684\u540c\u6b65\u8f90\u5c04\u6570\u636e\u91c7\u96c6\u548c\u7ba1\u7406\uff0c\u63d0\u5347\u4e86\u64cd\u4f5c\u6548\u7387\uff0c\u652f\u6301\u8fdc\u7a0b\u76d1\u6d4b\u548c\u6570\u636e\u8d28\u91cf\u8bc4\u4f30\u3002", "motivation": "\u9762\u5bf9\u6570\u636e\u83b7\u53d6\u548c\u7ba1\u7406\u6311\u6218\uff0c\u5305\u62ec\u6709\u9650\u7684\u8bbf\u95ee\u65f6\u95f4\u548c\u6570\u636e\u7ba1\u7406\u9700\u6c42\uff0c\u5f00\u53d1\u6846\u67b6\u4ee5\u63d0\u9ad8\u64cd\u4f5c\u6548\u7387\u3002", "method": "\u8bbe\u8ba1\u3001\u90e8\u7f72\u548c\u8bc4\u4f30\u4e86\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u540c\u6b65\u8f90\u5c04\u6570\u636e\u91c7\u96c6\u548c\u7ba1\u7406\uff0c\u63d0\u4f9b\u5b9e\u65f6\u7684\u8fdc\u7a0b\u5b9e\u9a8c\u76d1\u6d4b\u4e0e\u6570\u636e\u8d28\u91cf\u8bc4\u4f30\u5de5\u5177\u3002", "result": "\u8be5\u6846\u67b6\u57282024\u5e74\u5e95\u5728\u4e09\u6761\u5149\u675f\u7ebf\u6210\u529f\u90e8\u7f72\uff0c\u7ba1\u7406\u4e8650-100 TB\u6570\u636e\u548c\u8d85\u8fc71000\u4e07\u4e2a\u6587\u4ef6\uff0c\u6d4b\u8bd5\u663e\u793a\u964d\u4f4e\u4e86\u5f00\u9500\uff0c\u63d0\u9ad8\u4e86\u53ef\u8bbf\u95ee\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u540c\u6b65\u8f90\u5c04\u6570\u636e\u91c7\u96c6\u7684\u6548\u7387\u548c\u53ef\u8bbf\u95ee\u6027\uff0c\u5e76\u6210\u529f\u7ba1\u7406\u4e86\u5927\u91cf\u6570\u636e\u3002"}}
{"id": "2602.05516", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05516", "abs": "https://arxiv.org/abs/2602.05516", "authors": ["Runxiao Liu", "Pengda Mao", "Xiangli Le", "Shuang Gu", "Yapeng Chen", "Quan Quan"], "title": "Virtual-Tube-Based Cooperative Transport Control for Multi-UAV Systems in Constrained Environments", "comment": "10 pages, 8 figures", "summary": "This paper proposes a novel control framework for cooperative transportation of cable-suspended loads by multiple unmanned aerial vehicles (UAVs) operating in constrained environments. Leveraging virtual tube theory and principles from dissipative systems theory, the framework facilitates efficient multi-UAV collaboration for navigating obstacle-rich areas. The proposed framework offers several key advantages. (1) It achieves tension distribution and coordinated transportation within the UAV-cable-load system with low computational overhead, dynamically adapting UAV configurations based on obstacle layouts to facilitate efficient navigation. (2) By integrating dissipative systems theory, the framework ensures high stability and robustness, essential for complex multi-UAV operations. The effectiveness of the proposed approach is validated through extensive simulations, demonstrating its scalability for large-scale multi-UAV systems. Furthermore, the method is experimentally validated in outdoor scenarios, showcasing its practical feasibility and robustness under real-world conditions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u63a7\u5236\u6846\u67b6\uff0c\u5229\u7528\u865a\u62df\u7ba1\u548c\u8017\u6563\u7cfb\u7edf\u7406\u8bba\uff0c\u5b9e\u73b0\u4e86\u591a\u65e0\u4eba\u673a\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u5408\u4f5c\u8fd0\u8f93\uff0c\u5177\u6709\u826f\u597d\u7684\u5b9e\u7528\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u591a\u65e0\u4eba\u673a\u5728\u969c\u788d\u7269\u4e30\u5bcc\u7684\u73af\u5883\u4e2d\u8fdb\u884c\u8f7d\u91cd\u8fd0\u8f93\u7684\u6311\u6218\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u534f\u4f5c\u548c\u5bfc\u822a\u3002", "method": "\u5229\u7528\u865a\u62df\u7ba1\u7406\u8bba\u548c\u8017\u6563\u7cfb\u7edf\u7406\u8bba\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a7\u5236\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u591a\u65e0\u4eba\u673a\u5728\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5408\u4f5c\u8fd0\u8f93", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u6237\u5916\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u5927\u89c4\u6a21\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u65e0\u4eba\u673a\u5408\u4f5c\u8fd0\u8f93\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u53ef\u884c\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.05854", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.05854", "abs": "https://arxiv.org/abs/2602.05854", "authors": ["Yuying Tang", "Xinyi Chen", "Haotian Li", "Xing Xie", "Xiaojuan Ma", "Huamin Qu"], "title": "DuoDrama: Supporting Screenplay Refinement Through LLM-Assisted Human Reflection", "comment": "Accepted by CHI 2026", "summary": "AI has been increasingly integrated into screenwriting practice. In refinement, screenwriters expect AI to provide feedback that supports reflection across the internal perspective of characters and the external perspective of the overall story. However, existing AI tools cannot sufficiently coordinate the two perspectives to meet screenwriters' needs. To address this gap, we present DuoDrama, an AI system that generates feedback to assist screenwriters' reflection in refinement. To enable DuoDrama, based on performance theories and a formative study with nine professional screenwriters, we design the Experience-Grounded Feedback Generation Workflow for Human Reflection (ExReflect). In ExReflect, an AI agent adopts an experience role to generate experience and then shifts to an evaluation role to generate feedback based on the experience. A study with fourteen professional screenwriters shows that DuoDrama improves feedback quality and alignment and enhances the effectiveness, depth, and richness of reflection. We conclude by discussing broader implications and future directions.", "AI": {"tldr": "DuoDrama\u662f\u4e00\u4e2aAI\u7cfb\u7edf\uff0c\u53ef\u4ee5\u901a\u8fc7Experience-Grounded Feedback Generation Workflow\uff08ExReflect\uff09\u4e3a\u7f16\u5267\u63d0\u4f9b\u53cd\u601d\u652f\u6301\uff0c\u7814\u7a76\u663e\u793a\u5176\u6709\u6548\u63d0\u5347\u4e86\u53cd\u9988\u8d28\u91cf\u4e0e\u53cd\u601d\u6df1\u5ea6\u3002", "motivation": "\u73b0\u6709AI\u5de5\u5177\u65e0\u6cd5\u534f\u8c03\u89d2\u8272\u5185\u90e8\u89c6\u89d2\u548c\u6574\u4e2a\u6545\u4e8b\u7684\u5916\u90e8\u89c6\u89d2\uff0c\u65e0\u6cd5\u6ee1\u8db3\u7f16\u5267\u7684\u9700\u6c42\u3002", "method": "\u57fa\u4e8e\u8868\u6f14\u7406\u8bba\u548c\u5bf9\u4e5d\u4f4d\u4e13\u4e1a\u7f16\u5267\u7684\u5f62\u6210\u6027\u7814\u7a76\uff0c\u8bbe\u8ba1\u4e86\u4f53\u9a8c\u4e3a\u57fa\u7840\u7684\u53cd\u9988\u751f\u6210\u5de5\u4f5c\u6d41\u7a0b\uff08ExReflect\uff09\u3002", "result": "\u4e00\u9879\u9488\u5bf9\u5341\u56db\u4f4d\u4e13\u4e1a\u7f16\u5267\u7684\u7814\u7a76\u8868\u660e\uff0cDuoDrama\u6539\u5584\u4e86\u53cd\u9988\u8d28\u91cf\u548c\u4e00\u81f4\u6027\uff0c\u5e76\u589e\u5f3a\u4e86\u53cd\u601d\u7684\u6709\u6548\u6027\u3001\u6df1\u5ea6\u548c\u4e30\u5bcc\u6027\u3002", "conclusion": "DuoDrama\u63d0\u5347\u4e86\u53cd\u9988\u8d28\u91cf\u548c\u4e00\u81f4\u6027\uff0c\u589e\u5f3a\u4e86\u53cd\u601d\u7684\u6709\u6548\u6027\u3001\u6df1\u5ea6\u548c\u4e30\u5bcc\u6027\u3002"}}
{"id": "2602.05552", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.05552", "abs": "https://arxiv.org/abs/2602.05552", "authors": ["Bessie Dominguez-Dager", "Sergio Suescun-Ferrandiz", "Felix Escalona", "Francisco Gomez-Donoso", "Miguel Cazorla"], "title": "VLN-Pilot: Large Vision-Language Model as an Autonomous Indoor Drone Operator", "comment": null, "summary": "This paper introduces VLN-Pilot, a novel framework in which a large Vision-and-Language Model (VLLM) assumes the role of a human pilot for indoor drone navigation. By leveraging the multimodal reasoning abilities of VLLMs, VLN-Pilot interprets free-form natural language instructions and grounds them in visual observations to plan and execute drone trajectories in GPS-denied indoor environments. Unlike traditional rule-based or geometric path-planning approaches, our framework integrates language-driven semantic understanding with visual perception, enabling context-aware, high-level flight behaviors with minimal task-specific engineering. VLN-Pilot supports fully autonomous instruction-following for drones by reasoning about spatial relationships, obstacle avoidance, and dynamic reactivity to unforeseen events. We validate our framework on a custom photorealistic indoor simulation benchmark and demonstrate the ability of the VLLM-driven agent to achieve high success rates on complex instruction-following tasks, including long-horizon navigation with multiple semantic targets. Experimental results highlight the promise of replacing remote drone pilots with a language-guided autonomous agent, opening avenues for scalable, human-friendly control of indoor UAVs in tasks such as inspection, search-and-rescue, and facility monitoring. Our results suggest that VLLM-based pilots may dramatically reduce operator workload while improving safety and mission flexibility in constrained indoor environments.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86VLN-Pilot\uff0c\u4e00\u4e2a\u5229\u7528\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5ba4\u5185\u65e0\u4eba\u673a\u5bfc\u822a\u7684\u65b0\u6846\u67b6\uff0c\u5177\u6709\u8bed\u8a00\u9a71\u52a8\u7684\u8bed\u4e49\u7406\u89e3\u548c\u89c6\u89c9\u611f\u77e5\uff0c\u9a8c\u8bc1\u4e86\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u9ad8\u6210\u529f\u7387\u3002", "motivation": "\u5229\u7528\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u81ea\u52a8\u5316\u65e0\u4eba\u673a\u7684\u5ba4\u5185\u5bfc\u822a\u4efb\u52a1\uff0c\u7279\u522b\u662f\u5728\u7f3a\u4e4fGPS\u7684\u73af\u5883\u4e2d\u3002", "method": "\u901a\u8fc7\u5c06\u8bed\u8a00\u9a71\u52a8\u7684\u8bed\u4e49\u7406\u89e3\u4e0e\u89c6\u89c9\u611f\u77e5\u76f8\u7ed3\u5408\uff0c\u652f\u6301\u65e0\u4eba\u673a\u5728\u5ba4\u5185\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u3002", "result": "\u5728\u590d\u6742\u7684\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u8f83\u9ad8\u7684\u6210\u529f\u7387\uff0c\u5305\u62ec\u591a\u8bed\u4e49\u76ee\u6807\u7684\u957f\u65f6\u95f4\u5bfc\u822a\u3002", "conclusion": "VLLM\u9a71\u52a8\u7684\u65e0\u4eba\u673a\u98de\u884c\u5458\u53ef\u4ee5\u5927\u5e45\u51cf\u5c11\u64cd\u4f5c\u5458\u5de5\u4f5c\u8d1f\u62c5\uff0c\u540c\u65f6\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u4efb\u52a1\u7075\u6d3b\u6027\u3002"}}
{"id": "2602.05856", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.05856", "abs": "https://arxiv.org/abs/2602.05856", "authors": ["Zhiqi Gao", "Guo Zhu", "Huarui Luo", "Dongyijie Primo Pan", "Haoming Tang", "Bingquan Zhang", "Jiahuan Pei", "Jie Li", "Benyou Wang"], "title": "\"It Talks Like a Patient, But Feels Different\": Co-Designing AI Standardized Patients with Medical Learners", "comment": null, "summary": "Standardized patients (SPs) play a central role in clinical communication training but are costly, difficult to scale, and inconsistent. Large language model (LLM) based AI standardized patients (AI-SPs) promise flexible, on-demand practice, yet learners often report that they talk like a patient but feel different. We interviewed 12 clinical-year medical students and conducted three co-design workshops to examine how learners experience constraints of SP encounters and what they expect from AI-SPs. We identified six learner-centered needs, translated them into AI-SP design requirements, and synthesized a conceptual workflow. Our findings position AI-SPs as tools for deliberate practice and show that instructional usability, rather than conversational realism alone, drives learner trust, engagement, and educational value.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0cAI-SP\u53ef\u4ee5\u4f5c\u4e3a\u6709\u76ee\u7684\u7684\u7ec3\u4e60\u5de5\u5177\uff0c\u901a\u8fc7\u5173\u6ce8\u6559\u5b66\u53ef\u7528\u6027\u800c\u975e\u5355\u7eaf\u7684\u5bf9\u8bdd\u771f\u5b9e\u6027\uff0c\u63d0\u9ad8\u5b66\u4e60\u8005\u7684\u4fe1\u4efb\u5ea6\u548c\u53c2\u4e0e\u611f\u3002", "motivation": "\u63a2\u8ba8\u4f20\u7edf\u6807\u51c6\u5316\u60a3\u8005\u7684\u9ad8\u6210\u672c\u548c\u4e0d\u4e00\u81f4\u6027\uff0c\u4ee5\u53ca\u5b66\u4e60\u8005\u5bf9AI\u6807\u51c6\u5316\u60a3\u8005\u7684\u4e0d\u540c\u611f\u53d7\u3002", "method": "\u901a\u8fc7\u5bf912\u540d\u4e34\u5e8a\u5e74\u7684\u533b\u5b66\u5b66\u751f\u8fdb\u884c\u8bbf\u8c08\u548c\u4e09\u6b21\u5171\u540c\u8bbe\u8ba1\u7814\u8ba8\u4f1a\uff0c\u63a2\u8ba8\u5b66\u751f\u5bf9\u6807\u51c6\u5316\u60a3\u8005(SP)\u906d\u9047\u4e2d\u7684\u9650\u5236\u4ee5\u53ca\u4ed6\u4eec\u5bf9AI-SP\u7684\u671f\u671b\u3002", "result": "\u8bc6\u522b\u51fa\u516d\u4e2a\u4ee5\u5b66\u4e60\u8005\u4e3a\u4e2d\u5fc3\u7684\u9700\u6c42\uff0c\u5e76\u5c06\u5176\u8f6c\u5316\u4e3aAI-SP\u8bbe\u8ba1\u8981\u6c42\uff0c\u6700\u7ec8\u5f62\u6210\u4e00\u4e2a\u6982\u5ff5\u5de5\u4f5c\u6d41\u7a0b\u3002", "conclusion": "AI-SPs\u4f5c\u4e3a\u5de5\u5177\u53ef\u4ee5\u589e\u5f3a\u6709\u76ee\u7684\u7684\u7ec3\u4e60\uff0c\u5173\u6ce8\u6559\u5b66\u53ef\u7528\u6027\u6bd4\u5355\u7eaf\u7684\u5bf9\u8bdd\u771f\u5b9e\u6027\u66f4\u80fd\u63d0\u9ad8\u5b66\u4e60\u8005\u7684\u4fe1\u4efb\u5ea6\u3001\u53c2\u4e0e\u611f\u548c\u6559\u80b2\u4ef7\u503c\u3002"}}
{"id": "2602.05596", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05596", "abs": "https://arxiv.org/abs/2602.05596", "authors": ["Hokyun Lee", "Woo-Jeong Baek", "Junhyeok Cha", "Jaeheung Park"], "title": "TOLEBI: Learning Fault-Tolerant Bipedal Locomotion via Online Status Estimation and Fallibility Rewards", "comment": "Accepted for Publication at IEEE International Conference on Robotics and Automation (ICRA) 2026", "summary": "With the growing employment of learning algorithms in robotic applications, research on reinforcement learning for bipedal locomotion has become a central topic for humanoid robotics. While recently published contributions achieve high success rates in locomotion tasks, scarce attention has been devoted to the development of methods that enable to handle hardware faults that may occur during the locomotion process. However, in real-world settings, environmental disturbances or sudden occurrences of hardware faults might yield severe consequences. To address these issues, this paper presents TOLEBI (A faulT-tOlerant Learning framEwork for Bipedal locomotIon) that handles faults on the robot during operation. Specifically, joint locking, power loss and external disturbances are injected in simulation to learn fault-tolerant locomotion strategies. In addition to transferring the learned policy to the real robot via sim-to-real transfer, an online joint status module incorporated. This module enables to classify joint conditions by referring to the actual observations at runtime under real-world conditions. The validation experiments conducted both in real-world and simulation with the humanoid robot TOCABI highlight the applicability of the proposed approach. To our knowledge, this manuscript provides the first learning-based fault-tolerant framework for bipedal locomotion, thereby fostering the development of efficient learning methods in this field.", "AI": {"tldr": "TOLEBI\u662f\u4e00\u4e2a\u9488\u5bf9\u53cc\u8db3\u8fd0\u52a8\u7684\u6545\u969c\u5bb9\u5fcd\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u5728\u8fd0\u52a8\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u7684\u786c\u4ef6\u6545\u969c\uff0c\u652f\u6301\u5728\u7ebf\u72b6\u6001\u76d1\u6d4b\uff0c\u5e76\u901a\u8fc7\u4e00\u7cfb\u5217\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5c3d\u7ba1\u8fd1\u5e74\u6765\u5728\u53cc\u8db3\u8fd0\u52a8\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u9ad8\u6210\u529f\u7387\u7684\u6210\u679c\uff0c\u4f46\u5bf9\u5904\u7406\u8fd0\u52a8\u8fc7\u7a0b\u4e2d\u7684\u786c\u4ef6\u6545\u969c\u7684\u65b9\u6cd5\u7814\u7a76\u8f83\u5c11\uff0c\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u5e72\u6270\u6216\u786c\u4ef6\u6545\u969c\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u540e\u679c\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u76f8\u5e94\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5728\u4eff\u771f\u4e2d\u6ce8\u5165\u5173\u8282\u9501\u5b9a\u3001\u7535\u529b\u635f\u5931\u548c\u5916\u90e8\u5e72\u6270\uff0cTOLEBI\u5b66\u4e60\u6545\u969c\u5bb9\u5fcd\u7684\u8fd0\u52a8\u7b56\u7565\uff0c\u5e76\u5c06\u5b66\u4e60\u5230\u7684\u7b56\u7565\u901a\u8fc7\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8f6c\u79fb\u5230\u5b9e\u9645\u673a\u5668\u4eba\u4e0a\uff0c\u540c\u65f6\u96c6\u6210\u4e86\u5728\u7ebf\u5173\u8282\u72b6\u6001\u6a21\u5757\u4ee5\u5b9e\u65f6\u76d1\u6d4b\u5173\u8282\u72b6\u6001\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86TOLEBI\u5728\u73b0\u5b9e\u548c\u4eff\u771f\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u8fd0\u52a8\u4e2d\u7684\u6545\u969c\u65f6\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u672c\u8bba\u6587\u63d0\u51fa\u7684TOLEBI\u6846\u67b6\u662f\u9996\u4e2a\u57fa\u4e8e\u5b66\u4e60\u7684\u884c\u8d70\u6545\u969c\u5bb9\u5fcd\u6846\u67b6\uff0c\u63a8\u52a8\u4e86\u53cc\u8db3\u8fd0\u52a8\u9886\u57df\u9ad8\u6548\u5b66\u4e60\u65b9\u6cd5\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.05864", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.05864", "abs": "https://arxiv.org/abs/2602.05864", "authors": ["Mandi Yang", "Zhiqi Gao", "Yibo Meng", "Dongyijie Primo Pan"], "title": "Prompting Destiny: Negotiating Socialization and Growth in an LLM-Mediated Speculative Gameworld", "comment": null, "summary": "We present an LLM-mediated role-playing game that supports reflection on socialization, moral responsibility, and educational role positioning. Grounded in socialization theory, the game follows a four-season structure in which players guide a child prince through morally charged situations and compare the LLM-mediated NPC's differentiated responses across stages, helping them reason about how educational guidance shifts with socialization. To approximate real educational contexts and reduce score-chasing, the system hides real-time evaluative scores and provides delayed, end-of-stage growth feedback as reflective prompts. We conducted a user study (N=12) with gameplay logs and post-game interviews, analyzed via reflexive thematic analysis. Findings show how players negotiated responsibility and role positioning, and reveal an entry-load tension between open-ended expression and sustained engagement. We contribute design knowledge on translating sociological models of socialization into reflective AI-mediated game systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u4e00\u79cd\u652f\u6301\u793e\u4ea4\u5316\u548c\u9053\u5fb7\u8d23\u4efb\u53cd\u601d\u7684\u89d2\u8272\u626e\u6f14\u6e38\u620f\uff0c\u901a\u8fc7\u7528\u6237\u7814\u7a76\u63ed\u793a\u4e86\u73a9\u5bb6\u5728\u6e38\u620f\u4e2d\u7684\u8d23\u4efb\u611f\u548c\u89d2\u8272\u5b9a\u4f4d\u7684\u8c08\u5224\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u89d2\u8272\u626e\u6f14\u6e38\u620f\u652f\u6301\u5bf9\u793e\u4ea4\u5316\u3001\u9053\u5fb7\u8d23\u4efb\u611f\u548c\u6559\u80b2\u89d2\u8272\u5b9a\u4f4d\u7684\u53cd\u601d\u3002", "method": "\u901a\u8fc7\u7528\u6237\u7814\u7a76\uff08N=12\uff09\uff0c\u7ed3\u5408\u6e38\u620f\u8bb0\u5f55\u548c\u8d5b\u540e\u8bbf\u8c08\uff0c\u91c7\u7528\u53cd\u601d\u6027\u4e3b\u9898\u5206\u6790\u8fdb\u884c\u5206\u6790\u3002", "result": "\u6e38\u620f\u5e2e\u52a9\u73a9\u5bb6\u601d\u8003\u6559\u80b2\u6307\u5bfc\u5982\u4f55\u968f\u7740\u793e\u4ea4\u5316\u800c\u53d8\u5316\uff0c\u5e76\u63d0\u4f9b\u4e86\u5173\u4e8e\u5c06\u793e\u4f1a\u5b66\u6a21\u578b\u8f6c\u5316\u4e3a\u53cd\u601d\u6027AI\u4ecb\u5bfc\u6e38\u620f\u7cfb\u7edf\u7684\u8bbe\u8ba1\u77e5\u8bc6\u3002", "conclusion": "\u73a9\u5bb6\u5728\u6e38\u620f\u4e2d\u63a2\u8ba8\u8d23\u4efb\u611f\u548c\u89d2\u8272\u5b9a\u4f4d\uff0c\u53d1\u73b0\u4e86\u5f00\u653e\u8868\u8fbe\u4e0e\u6301\u7eed\u53c2\u4e0e\u4e4b\u95f4\u7684\u7d27\u5f20\u5173\u7cfb\u3002"}}
{"id": "2602.05608", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05608", "abs": "https://arxiv.org/abs/2602.05608", "authors": ["Yufei Zhu", "Shih-Min Yang", "Martin Magnusson", "Allan Wang"], "title": "HiCrowd: Hierarchical Crowd Flow Alignment for Dense Human Environments", "comment": "Accepted to the 2026 IEEE International Conference on Robotics and Automation (ICRA)", "summary": "Navigating through dense human crowds remains a significant challenge for mobile robots. A key issue is the freezing robot problem, where the robot struggles to find safe motions and becomes stuck within the crowd. To address this, we propose HiCrowd, a hierarchical framework that integrates reinforcement learning (RL) with model predictive control (MPC). HiCrowd leverages surrounding pedestrian motion as guidance, enabling the robot to align with compatible crowd flows. A high-level RL policy generates a follow point to align the robot with a suitable pedestrian group, while a low-level MPC safely tracks this guidance with short horizon planning. The method combines long-term crowd aware decision making with safe short-term execution. We evaluate HiCrowd against reactive and learning-based baselines in offline setting (replaying recorded human trajectories) and online setting (human trajectories are updated to react to the robot in simulation). Experiments on a real-world dataset and a synthetic crowd dataset show that our method outperforms in navigation efficiency and safety, while reducing freezing behaviors. Our results suggest that leveraging human motion as guidance, rather than treating humans solely as dynamic obstacles, provides a powerful principle for safe and efficient robot navigation in crowds.", "AI": {"tldr": "HiCrowd\u662f\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u63d0\u5347\u673a\u5668\u4eba\u5728\u62e5\u6324\u4eba\u7fa4\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u6548\u7387\u548c\u5b89\u5168\u6027\uff0c\u51cf\u5c11\u5361\u58f3\u73b0\u8c61\u3002", "motivation": "\u89e3\u51b3\u79fb\u52a8\u673a\u5668\u4eba\u5728\u5bc6\u96c6\u4eba\u7fa4\u4e2d\u5bfc\u822a\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u673a\u5668\u4eba\u5361\u58f3\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c42\u6b21\u5316\u6846\u67b6HiCrowd\uff0c\u7ed3\u5408\u4e86\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e0e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\uff0c\u901a\u8fc7\u5468\u56f4\u884c\u4eba\u7684\u8fd0\u52a8\u6570\u636e\u63d0\u4f9b\u6307\u5bfc\u3002", "result": "HiCrowd\u5728\u79bb\u7ebf\u548c\u5728\u7ebf\u5b9e\u9a8c\u8bbe\u7f6e\u4e2d\u5747\u663e\u793a\u51fa\u51fa\u8272\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5bfc\u822a\u6548\u7387\u548c\u5b89\u5168\u6027\u65b9\u9762\u3002", "conclusion": "HiCrowd\u6846\u67b6\u5728\u5bfc\u822a\u6548\u7387\u548c\u5b89\u5168\u6027\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u673a\u5668\u4eba\u5728\u62e5\u6324\u4eba\u7fa4\u4e2d\u7684\u5361\u58f3\u884c\u4e3a\u3002"}}
{"id": "2602.05987", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.05987", "abs": "https://arxiv.org/abs/2602.05987", "authors": ["Bingsheng Yao", "Chaoran Chen", "April Yi Wang", "Sherry Tongshuang Wu", "Toby Jia-jun Li", "Dakuo Wang"], "title": "From Human-Human Collaboration to Human-Agent Collaboration: A Vision, Design Philosophy, and an Empirical Framework for Achieving Successful Partnerships Between Humans and LLM Agents", "comment": null, "summary": "The emergence of Large Language Model (LLM) agents enables us to build agent-based intelligent systems that move beyond the role of a \"tool\" to become genuine collaborators with humans, thereby realizing a novel human-agent collaboration paradigm. Our vision is that LLM agents should resemble remote human collaborators, which allows HCI researchers to ground the future exploration in decades of research on trust, awareness, and common ground in remote human collaboration, while also revealing the unique opportunities and challenges that emerge when one or more partners are AI agents. This workshop establishes a foundational research agenda for the new era by posing the question: How can the rich understanding of remote human collaboration inspire and inform the design and study of human-agent collaboration? We will bring together an interdisciplinary group from HCI, CSCW, and AI to explore this critical transition. The 180-minute workshop will be highly interactive, featuring a keynote speaker, a series of invited lightning talks, and an exploratory group design session where participants will storyboard novel paradigms of human-agent partnership. Our goal is to enlighten the research community by cultivating a shared vocabulary and producing a research agenda that charts the future of collaborative agents.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4fc3\u8fdb\u4eba\u673a\u534f\u4f5c\u53d1\u5c55\uff0c\u7814\u8ba8\u4f1a\u63a2\u8ba8\u5982\u4f55\u501f\u9274\u8fdc\u7a0b\u4eba\u7c7b\u534f\u4f5c\u7684\u7814\u7a76\u4e3a\u4eba\u673a\u5408\u4f5c\u8bbe\u8ba1\u63d0\u4f9b\u542f\u793a\u3002", "motivation": "\u5e0c\u671b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u80fd\u591f\u6210\u4e3a\u4eba\u7c7b\u771f\u6b63\u7684\u5408\u4f5c\u4f19\u4f34\uff0c\u4ece\u800c\u63a8\u52a8\u4eba\u673a\u5408\u4f5c\u7684\u65b0\u8303\u5f0f\u3002", "method": "\u5c06\u4e3e\u529e180\u5206\u949f\u7684\u4e92\u52a8\u7814\u8ba8\u4f1a\uff0c\u5305\u542b\u4e3b\u9898\u6f14\u8bb2\u3001\u95ea\u7535\u6f14\u8bb2\u548c\u5c0f\u7ec4\u8bbe\u8ba1\u4f1a\u8bdd\u3002", "result": "\u9080\u8bf7\u6765\u81eaHCI\u3001CSCW\u548cAI\u9886\u57df\u7684\u8de8\u5b66\u79d1\u56e2\u961f\uff0c\u5171\u540c\u63a2\u8ba8\u4eba\u673a\u5408\u4f5c\u8bbe\u8ba1\u53ca\u5176\u672a\u6765\u6311\u6218\u4e0e\u673a\u9047\u3002", "conclusion": "\u672c\u7814\u8ba8\u4f1a\u65e8\u5728\u901a\u8fc7\u5efa\u7acb\u5171\u540c\u7684\u8bcd\u6c47\u548c\u7814\u7a76\u8bae\u7a0b\uff0c\u4e3a\u534f\u4f5c\u4ee3\u7406\u7684\u672a\u6765\u63cf\u7ed8\u84dd\u56fe\u3002"}}
{"id": "2602.05683", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.05683", "abs": "https://arxiv.org/abs/2602.05683", "authors": ["Chuwei Wang", "Eduardo Sebasti\u00e1n", "Amanda Prorok", "Anastasia Bizyaeva"], "title": "From Vision to Decision: Neuromorphic Control for Autonomous Navigation and Tracking", "comment": null, "summary": "Robotic navigation has historically struggled to reconcile reactive, sensor-based control with the decisive capabilities of model-based planners. This duality becomes critical when the absence of a predominant option among goals leads to indecision, challenging reactive systems to break symmetries without computationally-intense planners. We propose a parsimonious neuromorphic control framework that bridges this gap for vision-guided navigation and tracking. Image pixels from an onboard camera are encoded as inputs to dynamic neuronal populations that directly transform visual target excitation into egocentric motion commands. A dynamic bifurcation mechanism resolves indecision by delaying commitment until a critical point induced by the environmental geometry. Inspired by recently proposed mechanistic models of animal cognition and opinion dynamics, the neuromorphic controller provides real-time autonomy with a minimal computational burden, a small number of interpretable parameters, and can be seamlessly integrated with application-specific image processing pipelines. We validate our approach in simulation environments as well as on an experimental quadrotor platform.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u5f62\u6001\u63a7\u5236\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u7684\u51b3\u7b56\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u81ea\u4e3b\u63a7\u5236\u4e0e\u7b80\u5355\u53c2\u6570\u8bbe\u7f6e\u3002", "motivation": "\u89e3\u51b3\u53cd\u5e94\u6027\u63a7\u5236\u4e0e\u6a21\u578b\u89c4\u5212\u80fd\u529b\u4e4b\u95f4\u7684\u77db\u76fe\uff0c\u7279\u522b\u662f\u5728\u76ee\u6807\u4e0d\u660e\u663e\u65f6\u9020\u6210\u7684\u51b3\u7b56\u56f0\u96be\u3002", "method": "\u91c7\u7528\u7b80\u7ea6\u7684\u795e\u7ecf\u5f62\u6001\u63a7\u5236\u6846\u67b6\uff0c\u5229\u7528\u73af\u5883\u51e0\u4f55\u52a8\u6001\u5206\u5c94\u673a\u5236\u6765\u5ef6\u8fdf\u51b3\u7b56\uff0c\u76f4\u63a5\u5c06\u89c6\u89c9\u76ee\u6807\u6fc0\u52b1\u8f6c\u5316\u4e3a\u81ea\u6211\u4e2d\u5fc3\u8fd0\u52a8\u6307\u4ee4\u3002", "result": "\u901a\u8fc7\u6765\u81ea\u673a\u8f7d\u6444\u50cf\u5934\u7684\u56fe\u50cf\u50cf\u7d20\u7f16\u7801\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u51b3\u7b56\u5ef6\u8fdf\u673a\u5236\uff0c\u5e76\u5728\u6a21\u62df\u73af\u5883\u548c\u5b9e\u9a8c\u56db\u65cb\u7ffc\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u795e\u7ecf\u5f62\u6001\u63a7\u5236\u6846\u67b6\u5728\u89c6\u89c9\u5f15\u5bfc\u5bfc\u822a\u4e0e\u8ddf\u8e2a\u4e2d\u6709\u6548\u89e3\u51b3\u4e86\u53cd\u5e94\u6027\u7cfb\u7edf\u4e0e\u57fa\u4e8e\u6a21\u578b\u7684\u89c4\u5212\u4e4b\u95f4\u7684\u77db\u76fe\uff0c\u5177\u5907\u5b9e\u65f6\u81ea\u4e3b\u51b3\u7b56\u80fd\u529b\uff0c\u5e76\u4e14\u8ba1\u7b97\u8d1f\u62c5\u5c0f\u3002"}}
{"id": "2602.05760", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.05760", "abs": "https://arxiv.org/abs/2602.05760", "authors": ["Andreea Tulbure", "Carmen Scheidemann", "Elias Steiner", "Marco Hutter"], "title": "Task-Oriented Robot-Human Handovers on Legged Manipulators", "comment": "Accepted to 21st ACM/IEEE International Conference on Human-Robot Interaction (HRI) 2026", "summary": "Task-oriented handovers (TOH) are fundamental to effective human-robot collaboration, requiring robots to present objects in a way that supports the human's intended post-handover use. Existing approaches are typically based on object- or task-specific affordances, but their ability to generalize to novel scenarios is limited. To address this gap, we present AFT-Handover, a framework that integrates large language model (LLM)-driven affordance reasoning with efficient texture-based affordance transfer to achieve zero-shot, generalizable TOH. Given a novel object-task pair, the method retrieves a proxy exemplar from a database, establishes part-level correspondences via LLM reasoning, and texturizes affordances for feature-based point cloud transfer. We evaluate AFT-Handover across diverse task-object pairs, showing improved handover success rates and stronger generalization compared to baselines. In a comparative user study, our framework is significantly preferred over the current state-of-the-art, effectively reducing human regrasping before tool use. Finally, we demonstrate TOH on legged manipulators, highlighting the potential of our framework for real-world robot-human handovers.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86AFT-Handover\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408LLM\u9a71\u52a8\u7684\u63a8\u7406\u548c\u7eb9\u7406\u8f6c\u79fb\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u4eba\u673a\u4ea4\u63a5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u4ea4\u63a5\u65b9\u6cd5\u901a\u5e38\u5c40\u9650\u4e8e\u7279\u5b9a\u7684\u5bf9\u8c61\u6216\u4efb\u52a1\uff0c\u800c\u5176\u5728\u65b0\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u4ea4\u63a5\u6548\u7387\u3002", "method": "\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684\u53ef\u4f9b\u6027\u63a8\u7406\u548c\u57fa\u4e8e\u7eb9\u7406\u7684\u9ad8\u6548\u53ef\u4f9b\u6027\u8f6c\u79fb\uff0cAFT-Handover\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u7684\u53ef\u6cdb\u5316\u4efb\u52a1\u5bfc\u5411\u4ea4\u63a5\u3002", "result": "AFT-Handover\u5728\u591a\u6837\u5316\u7684\u4efb\u52a1-\u5bf9\u8c61\u7ec4\u5408\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5c55\u793a\u51fa\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u66f4\u9ad8\u7684\u4ea4\u63a5\u6210\u529f\u7387\u548c\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "AFT-Handover\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u5bfc\u5411\u4ea4\u63a5\u7684\u6210\u529f\u7387\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4e14\u5728\u7528\u6237\u7814\u7a76\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.05791", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05791", "abs": "https://arxiv.org/abs/2602.05791", "authors": ["Yufei Xue", "YunFeng Lin", "Wentao Dong", "Yang Tang", "Jingbo Wang", "Jiangmiao Pang", "Ming Zhou", "Minghuan Liu", "Weinan Zhang"], "title": "Scalable and General Whole-Body Control for Cross-Humanoid Locomotion", "comment": null, "summary": "Learning-based whole-body controllers have become a key driver for humanoid robots, yet most existing approaches require robot-specific training. In this paper, we study the problem of cross-embodiment humanoid control and show that a single policy can robustly generalize across a wide range of humanoid robot designs with one-time training. We introduce XHugWBC, a novel cross-embodiment training framework that enables generalist humanoid control through: (1) physics-consistent morphological randomization, (2) semantically aligned observation and action spaces across diverse humanoid robots, and (3) effective policy architectures modeling morphological and dynamical properties. XHugWBC is not tied to any specific robot. Instead, it internalizes a broad distribution of morphological and dynamical characteristics during training. By learning motion priors from diverse randomized embodiments, the policy acquires a strong structural bias that supports zero-shot transfer to previously unseen robots. Experiments on twelve simulated humanoids and seven real-world robots demonstrate the strong generalization and robustness of the resulting universal controller.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u8de8\u5f62\u4f53\u4eba\u5f62\u63a7\u5236\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86XHugWBC\u6846\u67b6\uff0c\u4f7f\u5f97\u5355\u4e00\u63a7\u5236\u7b56\u7565\u80fd\u591f\u5728\u591a\u79cd\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u6709\u6548\u63a7\u5236\uff0c\u65e0\u9700\u7279\u5b9a\u673a\u5668\u4eba\u8bad\u7ec3\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5b66\u4e60\u578b\u5168\u8eab\u63a7\u5236\u5668\u9700\u8981\u7279\u5b9a\u673a\u5668\u4eba\u7684\u8bad\u7ec3\u95ee\u9898\uff0c\u7814\u7a76\u8de8\u5f62\u4f53\u7684\u901a\u7528\u63a7\u5236\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86XHugWBC\u6846\u67b6\uff0c\u901a\u8fc7\u7269\u7406\u4e00\u81f4\u7684\u5f62\u6001\u968f\u673a\u5316\u3001\u8bed\u4e49\u5bf9\u9f50\u7684\u89c2\u5bdf\u548c\u52a8\u4f5c\u7a7a\u95f4\u53ca\u6709\u6548\u7684\u7b56\u7565\u67b6\u6784\u6765\u5b9e\u73b0\u8de8\u5f62\u4f53\u63a7\u5236\u3002", "result": "\u572812\u4e2a\u6a21\u62df\u4eba\u7c7b\u673a\u5668\u4eba\u548c7\u4e2a\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u901a\u7528\u63a7\u5236\u5668\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "XHugWBC\u80fd\u591f\u5728\u591a\u79cd\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u80fd\u591f\u5728\u672a\u89c1\u8fc7\u7684\u673a\u5668\u4eba\u4e0a\u4e5f\u80fd\u6709\u6548\u63a7\u5236\u3002"}}
{"id": "2602.05855", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05855", "abs": "https://arxiv.org/abs/2602.05855", "authors": ["Dennis Bank", "Joost Cordes", "Thomas Seel", "Simon F. G. Ehlers"], "title": "A Hybrid Autoencoder for Robust Heightmap Generation from Fused Lidar and Depth Data for Humanoid Robot Locomotion", "comment": null, "summary": "Reliable terrain perception is a critical prerequisite for the deployment of humanoid robots in unstructured, human-centric environments. While traditional systems often rely on manually engineered, single-sensor pipelines, this paper presents a learning-based framework that uses an intermediate, robot-centric heightmap representation. A hybrid Encoder-Decoder Structure (EDS) is introduced, utilizing a Convolutional Neural Network (CNN) for spatial feature extraction fused with a Gated Recurrent Unit (GRU) core for temporal consistency. The architecture integrates multimodal data from an Intel RealSense depth camera, a LIVOX MID-360 LiDAR processed via efficient spherical projection, and an onboard IMU. Quantitative results demonstrate that multimodal fusion improves reconstruction accuracy by 7.2% over depth-only and 9.9% over LiDAR-only configurations. Furthermore, the integration of a 3.2 s temporal context reduces mapping drift.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u63d0\u9ad8\u4eba\u5f62\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u5730\u5f62\u611f\u77e5\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u7cbe\u5ea6\u5e76\u964d\u4f4e\u5730\u56fe\u6f02\u79fb\u3002", "motivation": "\u63d0\u9ad8\u4eba\u5f62\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u4eba\u7c7b\u73af\u5883\u4e2d\u7684\u5730\u5f62\u611f\u77e5\u80fd\u529b\uff0c\u514b\u670d\u4f20\u7edf\u7cfb\u7edf\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6df7\u5408\u7f16\u7801-\u89e3\u7801\u7ed3\u6784\uff08EDS\uff09\uff0c\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u8fdb\u884c\u7a7a\u95f4\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u7ed3\u5408\u95e8\u63a7\u9012\u5f52\u5355\u5143\uff08GRU\uff09\u6838\u5fc3\u4ee5\u4fdd\u6301\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u591a\u6a21\u6001\u878d\u5408\u5728\u91cd\u5efa\u7cbe\u5ea6\u4e0a\u5206\u522b\u6bd4\u4ec5\u6df1\u5ea6\u548c\u4ec5\u6fc0\u5149\u96f7\u8fbe\u914d\u7f6e\u63d0\u9ad8\u4e867.2%\u548c9.9%\u3002", "conclusion": "\u591a\u6a21\u6001\u878d\u5408\u63d0\u9ad8\u4e86\u91cd\u5efa\u7cbe\u5ea6\uff0c\u5e76\u4e14\u901a\u8fc7\u6574\u5408\u65f6\u95f4\u4e0a\u4e0b\u6587\u964d\u4f4e\u4e86\u5730\u56fe\u6f02\u79fb\u73b0\u8c61\u3002"}}
{"id": "2602.05895", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05895", "abs": "https://arxiv.org/abs/2602.05895", "authors": ["Qi Li", "Karsten Berns"], "title": "Residual Reinforcement Learning for Waste-Container Lifting Using Large-Scale Cranes with Underactuated Tools", "comment": "12 pages", "summary": "This paper studies the container lifting phase of a waste-container recycling task in urban environments, performed by a hydraulic loader crane equipped with an underactuated discharge unit, and proposes a residual reinforcement learning (RRL) approach that combines a nominal Cartesian controller with a learned residual policy. All experiments are conducted in simulation, where the task is characterized by tight geometric tolerances between the discharge-unit hooks and the container rings relative to the overall crane scale, making precise trajectory tracking and swing suppression essential. The nominal controller uses admittance control for trajectory tracking and pendulum-aware swing damping, followed by damped least-squares inverse kinematics with a nullspace posture term to generate joint velocity commands. A PPO-trained residual policy in Isaac Lab compensates for unmodeled dynamics and parameter variations, improving precision and robustness without requiring end-to-end learning from scratch. We further employ randomized episode initialization and domain randomization over payload properties, actuator gains, and passive joint parameters to enhance generalization. Simulation results demonstrate improved tracking accuracy, reduced oscillations, and higher lifting success rates compared to the nominal controller alone.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u540d\u4e49\u63a7\u5236\u5668\u548c\u5b66\u4e60\u7684\u6b8b\u4f59\u7b56\u7565\u7684\u6b8b\u4f59\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u5e9f\u7269\u5bb9\u5668\u56de\u6536\u4efb\u52a1\u7684\u8f68\u8ff9\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u63d0\u5347\u6210\u529f\u7387\u3002", "motivation": "\u5728\u57ce\u5e02\u73af\u5883\u4e2d\u8fdb\u884c\u5e9f\u7269\u5bb9\u5668\u56de\u6536\u4efb\u52a1\u65f6\uff0c\u63d0\u5347\u5bb9\u5668\u9636\u6bb5\u5177\u6709\u4e25\u683c\u7684\u51e0\u4f55\u516c\u5dee\uff0c\u56e0\u6b64\u7cbe\u786e\u7684\u8f68\u8ff9\u8ddf\u8e2a\u548c\u6446\u52a8\u6291\u5236\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6b8b\u4f59\u5f3a\u5316\u5b66\u4e60\uff08RRL\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u540d\u4e49\u7b1b\u5361\u5c14\u63a7\u5236\u5668\u548c\u5b66\u4e60\u7684\u6b8b\u4f59\u7b56\u7565\uff0c\u5e76\u4f7f\u7528\u62df\u5408\u63a7\u5236\u3001\u963b\u5c3c\u6700\u5c0f\u4e8c\u4e58\u9006\u8fd0\u52a8\u5b66\u7b49\u6280\u672f\u3002", "result": "\u901a\u8fc7\u4f7f\u7528PPO\u8bad\u7ec3\u7684\u6b8b\u4f59\u7b56\u7565\uff0c\u901a\u8fc7\u8865\u507f\u672a\u5efa\u6a21\u7684\u52a8\u6001\u548c\u53c2\u6570\u53d8\u5316\uff0c\u6539\u8fdb\u4e86\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u901a\u7528\u6027\u3002", "conclusion": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u6bd4\u5355\u72ec\u4f7f\u7528\u540d\u4e49\u63a7\u5236\u5668\u5177\u6709\u66f4\u597d\u7684\u8f68\u8ff9\u8ddf\u8e2a\u7cbe\u5ea6\u3001\u51cf\u5c11\u4e86\u6446\u52a8\uff0c\u5e76\u63d0\u9ad8\u4e86\u63d0\u5347\u6210\u529f\u7387\u3002"}}
{"id": "2602.05922", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05922", "abs": "https://arxiv.org/abs/2602.05922", "authors": ["Aziz Mohamed Mili", "Louis Catar", "Paul G\u00e9rard", "Ilyass Tabiai", "David St-Onge"], "title": "From Bench to Flight: Translating Drone Impact Tests into Operational Safety Limits", "comment": null, "summary": "Indoor micro-aerial vehicles (MAVs) are increasingly used for tasks that require close proximity to people, yet practitioners lack practical methods to tune motion limits based on measured impact risk. We present an end-to-end, open toolchain that converts benchtop impact tests into deployable safety governors for drones. First, we describe a compact and replicable impact rig and protocol for capturing force-time profiles across drone classes and contact surfaces. Second, we provide data-driven models that map pre-impact speed to impulse and contact duration, enabling direct computation of speed bounds for a target force limit. Third, we release scripts and a ROS2 node that enforce these bounds online and log compliance, with support for facility-specific policies. We validate the workflow on multiple commercial off-the-shelf quadrotors and representative indoor assets, demonstrating that the derived governors preserve task throughput while meeting force constraints specified by safety stakeholders. Our contribution is a practical bridge from measured impacts to runtime limits, with shareable datasets, code, and a repeatable process that teams can adopt to certify indoor MAV operations near humans.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5de5\u5177\u94fe\uff0c\u5c06\u51b2\u51fb\u6d4b\u8bd5\u7ed3\u679c\u8f6c\u5316\u4e3a\u65e0\u4eba\u673a\u7684\u5b89\u5168\u9650\u5236\uff0c\u786e\u4fdd\u5ba4\u5185\u5fae\u578b\u98de\u884c\u5668\u5728\u9760\u8fd1\u4eba\u7c7b\u65f6\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u5ba4\u5185\u5fae\u578b\u98de\u884c\u5668\u7528\u4e8e\u63a5\u8fd1\u4eba\u7c7b\u7684\u4efb\u52a1\uff0c\u4f46\u7f3a\u4e4f\u57fa\u4e8e\u5b9e\u9645\u5f71\u54cd\u98ce\u9669\u8c03\u8282\u8fd0\u52a8\u9650\u5236\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u538b\u7f29\u4e14\u53ef\u590d\u5236\u7684\u51b2\u51fb\u8bbe\u5907\uff0c\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u6a21\u578b\uff0c\u8ba1\u7b97\u901f\u5ea6\u9650\u5236\uff0c\u5e76\u63d0\u4f9b\u5728\u7ebf\u5b9e\u65bd\u7684\u811a\u672c\u548cROS2\u8282\u70b9\u3002", "result": "\u7814\u7a76\u9a8c\u8bc1\u4e86\u8be5\u5de5\u4f5c\u6d41\u7a0b\u5728\u591a\u6b3e\u5546\u4e1a\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u548c\u5178\u578b\u5ba4\u5185\u8d44\u4ea7\u4e0a\u7684\u6709\u6548\u6027\uff0c\u786e\u4fdd\u4efb\u52a1\u901a\u8fc7\u7387\u7684\u540c\u65f6\u6ee1\u8db3\u5b89\u5168\u7ea6\u675f\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u4ece\u6d4b\u91cf\u5230\u5b9e\u65f6\u9650\u5236\u7684\u5b9e\u7528\u6865\u6881\uff0c\u4f7f\u5f97\u5ba4\u5185\u5fae\u578b\u98de\u884c\u5668\u80fd\u591f\u5b89\u5168\u5730\u5728\u4eba\u7c7b\u9644\u8fd1\u64cd\u4f5c\u3002"}}
{"id": "2602.06001", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.06001", "abs": "https://arxiv.org/abs/2602.06001", "authors": ["Carolina Higuera", "Sergio Arnaud", "Byron Boots", "Mustafa Mukadam", "Francois Robert Hogan", "Franziska Meier"], "title": "Visuo-Tactile World Models", "comment": "Preprint", "summary": "We introduce multi-task Visuo-Tactile World Models (VT-WM), which capture the physics of contact through touch reasoning. By complementing vision with tactile sensing, VT-WM better understands robot-object interactions in contact-rich tasks, avoiding common failure modes of vision-only models under occlusion or ambiguous contact states, such as objects disappearing, teleporting, or moving in ways that violate basic physics. Trained across a set of contact-rich manipulation tasks, VT-WM improves physical fidelity in imagination, achieving 33% better performance at maintaining object permanence and 29% better compliance with the laws of motion in autoregressive rollouts. Moreover, experiments show that grounding in contact dynamics also translates to planning. In zero-shot real-robot experiments, VT-WM achieves up to 35% higher success rates, with the largest gains in multi-step, contact-rich tasks. Finally, VT-WM demonstrates significant downstream versatility, effectively adapting its learned contact dynamics to a novel task and achieving reliable planning success with only a limited set of demonstrations.", "AI": {"tldr": "\u5f15\u5165\u591a\u4efb\u52a1\u89c6\u89c9-\u89e6\u89c9\u4e16\u754c\u6a21\u578b(VT-WM)\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u63a5\u89e6\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u5728\u96f6-shot\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u663e\u8457\u7684\u6210\u529f\u7387\u548c\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3\u673a\u5668\u4eba\u4e0e\u7269\u4f53\u4e4b\u95f4\u7684\u63a5\u89e6\u4e92\u52a8\uff0c\u907f\u514d\u89c6\u89c9\u6a21\u578b\u5728\u906e\u6321\u6216\u6a21\u7cca\u63a5\u89e6\u72b6\u6001\u4e0b\u7684\u5e38\u89c1\u5931\u8d25\u6a21\u5f0f\u3002", "method": "\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u65b9\u5f0f\uff0c\u5f15\u5165\u4e86\u591a\u6a21\u6001\u7684\u89c6\u89c9-\u89e6\u89c9\u4e16\u754c\u6a21\u578b(VT-WM)\uff0c\u5e76\u5728\u4e00\u7cfb\u5217\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u63a7\u4efb\u52a1\u4e2d\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "VT-WM\u5728\u7269\u7406\u771f\u5b9e\u6027\u7684\u60f3\u8c61\u80fd\u529b\u4e0a\u63d0\u9ad8\u4e8633%\uff0c\u5728\u9075\u5b88\u8fd0\u52a8\u5b9a\u5f8b\u65b9\u9762\u63d0\u9ad8\u4e8629%\u3002\u5b9e\u9a8c\u8868\u660e\u89e6\u89c9\u52a8\u6001\u7684\u57fa\u7840\u80fd\u591f\u8f6c\u5316\u4e3a\u6709\u6548\u7684\u89c4\u5212\u80fd\u529b\u3002", "conclusion": "VT-WM\u5728\u96f6-shot\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u4e86\u6700\u9ad835%\u7684\u6210\u529f\u7387\uff0c\u5e76\u4e14\u5728\u591a\u6b65\u9aa4\u3001\u63a5\u89e6\u4e30\u5bcc\u7684\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u5176\u5728\u65b0\u7684\u4efb\u52a1\u4e2d\u7684\u9002\u5e94\u80fd\u529b\u548c\u53ef\u9760\u7684\u89c4\u5212\u6210\u529f\u7387\u3002"}}
{"id": "2602.06038", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.06038", "abs": "https://arxiv.org/abs/2602.06038", "authors": ["Xiaopan Zhang", "Zejin Wang", "Zhixu Li", "Jianpeng Yao", "Jiachen Li"], "title": "CommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction", "comment": "IEEE International Conference on Robotics and Automation (ICRA 2026); Project Website: https://comm-cp.github.io/", "summary": "To complete assignments provided by humans in natural language, robots must interpret commands, generate and answer relevant questions for scene understanding, and manipulate target objects. Real-world deployments often require multiple heterogeneous robots with different manipulation capabilities to handle different assignments cooperatively. Beyond the need for specialized manipulation skills, effective information gathering is important in completing these assignments. To address this component of the problem, we formalize the information-gathering process in a fully cooperative setting as an underexplored multi-agent multi-task Embodied Question Answering (MM-EQA) problem, which is a novel extension of canonical Embodied Question Answering (EQA), where effective communication is crucial for coordinating efforts without redundancy. To address this problem, we propose CommCP, a novel LLM-based decentralized communication framework designed for MM-EQA. Our framework employs conformal prediction to calibrate the generated messages, thereby minimizing receiver distractions and enhancing communication reliability. To evaluate our framework, we introduce an MM-EQA benchmark featuring diverse, photo-realistic household scenarios with embodied questions. Experimental results demonstrate that CommCP significantly enhances the task success rate and exploration efficiency over baselines. The experiment videos, code, and dataset are available on our project website: https://comm-cp.github.io.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53bb\u4e2d\u5fc3\u5316\u901a\u4fe1\u6846\u67b6CommCP\uff0c\u9488\u5bf9\u591a\u673a\u5668\u4eba\u4efb\u52a1\u4e0b\u7684\u4fe1\u606f\u6536\u96c6\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u4e0e\u63a2\u7d22\u6548\u7387\u3002", "motivation": "\u4e3a\u4e86\u5728\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4e0b\u6267\u884c\u4efb\u52a1\uff0c\u673a\u5668\u4eba\u9700\u8981\u6709\u6548\u7684\u4fe1\u606f\u6536\u96c6\u4e0e\u534f\u8c03\uff0c\u5c24\u5176\u662f\u5728\u591a\u673a\u5668\u4eba\u73af\u5883\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53bb\u4e2d\u5fc3\u5316\u901a\u4fe1\u6846\u67b6CommCP\uff0c\u91c7\u7528\u4fdd\u5f62\u9884\u6d4b\u6765\u6821\u51c6\u751f\u6210\u6d88\u606f\u3002", "result": "CommCP\u5728\u591a\u4ee3\u7406\u591a\u4efb\u52a1\u7684\u73af\u5883\u4e0b\uff0c\u901a\u8fc7\u51cf\u5c11\u63a5\u6536\u8005\u5206\u5fc3\uff0c\u589e\u5f3a\u901a\u4fe1\u53ef\u9760\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u4efb\u52a1\u6267\u884c\u6548\u7387\u3002", "conclusion": "CommCP\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u548c\u63a2\u7d22\u6548\u7387\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u591a\u673a\u5668\u4eba\u591a\u4efb\u52a1\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
