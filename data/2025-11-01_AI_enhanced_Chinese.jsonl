{"id": "2510.25957", "categories": ["cs.HC", "H.5.1; I.3.7; H.5.2"], "pdf": "https://arxiv.org/pdf/2510.25957", "abs": "https://arxiv.org/abs/2510.25957", "authors": ["Radha Kumaran", "You-Jin Kim", "Anne E Milner", "Tom Bullock", "Barry Giesbrecht", "Tobias H\u00f6llerer"], "title": "The Impact of Navigation Aids on Search Performance and Object Recall in Wide-Area Augmented Reality", "comment": "Conference Paper, 17 pages. Published at the 2023 CHI Conference on\n  Human Factors in Computing Systems", "summary": "Head-worn augmented reality (AR) is a hotly pursued and increasingly feasible\ncontender paradigm for replacing or complementing smartphones and watches for\ncontinual information consumption. Here, we compare three different AR\nnavigation aids (on-screen compass, on-screen radar and in-world vertical\narrows) in a wide-area outdoor user study (n=24) where participants search for\nhidden virtual target items amongst physical and virtual objects. We analyzed\nparticipants' search task performance, movements, eye-gaze, survey responses\nand object recall. There were two key findings. First, all navigational aids\nenhanced search performance relative to a control condition, with some benefit\nand strongest user preference for in-world arrows. Second, users recalled fewer\nphysical objects than virtual objects in the environment, suggesting reduced\nawareness of the physical environment. Together, these findings suggest that\nwhile navigational aids presented in AR can enhance search task performance,\nusers may pay less attention to the physical environment, which could have\nundesirable side-effects.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e09\u79cdAR\u5bfc\u822a\u5de5\u5177\u7684\u6548\u679c\uff0c\u53d1\u73b0\u5b83\u4eec\u63d0\u5347\u4e86\u641c\u7d22\u6027\u80fd\uff0c\u4f46\u7528\u6237\u5bf9\u7269\u7406\u73af\u5883\u7684\u5173\u6ce8\u51cf\u5c11\u3002", "motivation": "\u7814\u7a76\u5934\u6234\u5f0f\u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u4f5c\u4e3a\u53d6\u4ee3\u6216\u8865\u5145\u667a\u80fd\u624b\u673a\u548c\u624b\u8868\u7684\u4fe1\u606f\u6d88\u8d39\u5de5\u5177\u7684\u6f5c\u529b\u3002", "method": "\u5728\u6237\u5916\u8fdb\u884c\u5e7f\u6cdb\u7528\u6237\u7814\u7a76\uff0c\u6bd4\u8f83\u4e86\u5c4f\u5e55\u4e0a\u7684\u6307\u5357\u9488\u3001\u96f7\u8fbe\u548c\u5b9e\u4e16\u754c\u7bad\u5934\u7684\u6548\u679c\uff0c\u5206\u6790\u4e86\u7528\u6237\u7684\u8868\u73b0\u3001\u52a8\u4f5c\u3001\u6ce8\u89c6\u548c\u53cd\u9988\u3002", "result": "\u6bd4\u8f83\u4e09\u79cd\u4e0d\u540c\u7684AR\u5bfc\u822a\u8f85\u52a9\u5de5\u5177\u5bf9\u7528\u6237\u641c\u7d22\u865a\u62df\u76ee\u6807\u7684\u5f71\u54cd\u3002", "conclusion": "AR\u5bfc\u822a\u8f85\u52a9\u5de5\u5177\u80fd\u63d0\u9ad8\u641c\u7d22\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u4f46\u53ef\u80fd\u5bfc\u81f4\u7528\u6237\u5bf9\u7269\u7406\u73af\u5883\u7684\u610f\u8bc6\u51cf\u5f31\uff0c\u9700\u6ce8\u610f\u6f5c\u5728\u7684\u4e0d\u826f\u5f71\u54cd\u3002"}}
{"id": "2510.25974", "categories": ["cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25974", "abs": "https://arxiv.org/abs/2510.25974", "authors": ["Mengtian Guo", "David Gotz", "Yue Wang"], "title": "Risks and Opportunities in Human-Machine Teaming in Operationalizing Machine Learning Target Variables", "comment": "23 pages, 6 figures", "summary": "Predictive modeling has the potential to enhance human decision-making.\nHowever, many predictive models fail in practice due to problematic problem\nformulation in cases where the prediction target is an abstract concept or\nconstruct and practitioners need to define an appropriate target variable as a\nproxy to operationalize the construct of interest. The choice of an appropriate\nproxy target variable is rarely self-evident in practice, requiring both domain\nknowledge and iterative data modeling. This process is inherently\ncollaborative, involving both domain experts and data scientists. In this work,\nwe explore how human-machine teaming can support this process by accelerating\niterations while preserving human judgment. We study the impact of two\nhuman-machine teaming strategies on proxy construction: 1) relevance-first:\nhumans leading the process by selecting relevant proxies, and 2)\nperformance-first: machines leading the process by recommending proxies based\non predictive performance. Based on a controlled user study of a proxy\nconstruction task (N = 20), we show that the performance-first strategy\nfacilitated faster iterations and decision-making, but also biased users\ntowards well-performing proxies that are misaligned with the application goal.\nOur study highlights the opportunities and risks of human-machine teaming in\noperationalizing machine learning target variables, yielding insights for\nfuture research to explore the opportunities and mitigate the risks.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u4eba\u673a\u534f\u4f5c\u5728\u4ee3\u7406\u53d8\u91cf\u6784\u5efa\u4e2d\u80fd\u52a0\u901f\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4f46\u9700\u8981\u8c28\u614e\u9009\u62e9\u4ee5\u907f\u514d\u504f\u79bb\u5e94\u7528\u76ee\u6807\u3002", "motivation": "\u9884\u6d4b\u5efa\u6a21\u80fd\u591f\u63d0\u5347\u4eba\u7c7b\u51b3\u7b56\u80fd\u529b\uff0c\u4f46\u5728\u5b9a\u4e49\u5408\u9002\u7684\u76ee\u6807\u53d8\u91cf\u4f5c\u4e3a\u4ee3\u7406\u65f6\u5b58\u5728\u96be\u9898\uff0c\u8fd9\u9700\u8981\u9886\u57df\u77e5\u8bc6\u548c\u8fed\u4ee3\u6570\u636e\u5efa\u6a21\u3002", "method": "\u901a\u8fc7\u5bf9\u4ee3\u7406\u6784\u5efa\u4efb\u52a1\u8fdb\u884c\u63a7\u5236\u7684\u7528\u6237\u7814\u7a76\uff0c\u6bd4\u8f83\u4e86\u4e24\u79cd\u4eba\u673a\u534f\u4f5c\u7b56\u7565\u7684\u5f71\u54cd\uff1a\u76f8\u5173\u6027\u4f18\u5148\u548c\u8868\u73b0\u4f18\u5148\u3002", "result": "\u8868\u73b0\u4f18\u5148\u7b56\u7565\u52a0\u5feb\u4e86\u8fed\u4ee3\u548c\u51b3\u7b56\uff0c\u4f46\u4e5f\u4f7f\u7528\u6237\u503e\u5411\u9009\u62e9\u4e0e\u5e94\u7528\u76ee\u6807\u4e0d\u4e00\u81f4\u7684\u9ad8\u6027\u80fd\u4ee3\u7406\u3002", "conclusion": "\u4eba\u673a\u534f\u4f5c\u5728\u673a\u5668\u5b66\u4e60\u76ee\u6807\u53d8\u91cf\u7684\u5b9e\u65bd\u4e2d\u5b58\u5728\u673a\u4f1a\u548c\u98ce\u9669\uff0c\u672a\u6765\u7814\u7a76\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u8fd9\u4e9b\u673a\u4f1a\u5e76\u51cf\u8f7b\u98ce\u9669\u3002"}}
{"id": "2510.25978", "categories": ["cs.HC", "H.5.1; I.3.7; H.5.2"], "pdf": "https://arxiv.org/pdf/2510.25978", "abs": "https://arxiv.org/abs/2510.25978", "authors": ["You-Jin Kim", "Radha Kumaran", "Jingjing Luo", "Tom Bullock", "Barry Giesbrecht", "Tobias H\u00f6llerer"], "title": "On the Go with AR: Attention to Virtual and Physical Targets while Varying Augmentation Density", "comment": "Conference Paper, 16 pages. Published at the 2025 CHI Conference on\n  Human Factors in Computing Systems", "summary": "Augmented reality is projected to be a primary mode of information\nconsumption on the go, seamlessly integrating virtual content into the physical\nworld. However, the potential perceptual demands of viewing virtual annotations\nwhile navigating a physical environment could impact user efficacy and safety,\nand the implications of these demands are not well understood. Here, we\ninvestigate the impact of virtual path guidance and augmentation density\n(visual clutter) on search performance and memory. Participants walked along a\npredefined path, searching for physical or virtual items. They experienced two\nlevels of augmentation density, and either walked freely or with enforced speed\nand path guidance. Augmentation density impacted behavior and reduced awareness\nof uncommon objects in the environment. Analysis of search task performance and\npost-experiment item recall revealed differing attention to physical and\nvirtual objects. On the basis of these findings we outline considerations for\nAR apps designed for use on the go.", "AI": {"tldr": "\u589e\u5f3a\u73b0\u5b9e\u7684\u865a\u62df\u8def\u5f84\u5f15\u5bfc\u548c\u89c6\u89c9\u6742\u4e71\u5ea6\u5f71\u54cd\u7528\u6237\u5728\u7269\u7406\u73af\u5883\u4e2d\u7684\u641c\u7d22\u8868\u73b0\u548c\u7269\u4f53\u8bb0\u5fc6\uff0c\u63d0\u793a\u9700\u5728AR\u5e94\u7528\u8bbe\u8ba1\u4e2d\u8003\u8651\u8fd9\u4e9b\u56e0\u7d20\u3002", "motivation": "\u7814\u7a76\u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u5bf9\u7528\u6237\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u5bfc\u822a\u65f6\u7684\u611f\u77e5\u9700\u6c42\u53ca\u5176\u5f71\u54cd\uff0c\u4ee5\u63d0\u9ad8\u7528\u6237\u6548\u679c\u548c\u5b89\u5168\u6027\u3002", "method": "\u901a\u8fc7\u8ba9\u53c2\u4e0e\u8005\u5728\u9884\u5b9a\u4e49\u8def\u5f84\u4e0a\u884c\u8d70\uff0c\u641c\u7d22\u7269\u7406\u6216\u865a\u62df\u7269\u4f53\uff0c\u63a7\u5236\u589e\u5f3a\u5bc6\u5ea6\u548c\u884c\u8d70\u65b9\u5f0f\uff0c\u5206\u6790\u5176\u5bf9\u641c\u7d22\u4efb\u52a1\u8868\u73b0\u548c\u7269\u4f53\u56de\u5fc6\u7684\u5f71\u54cd\u3002", "result": "\u865a\u62df\u8def\u5f84\u5f15\u5bfc\u548c\u589e\u5f3a\u5bc6\u5ea6\u5bf9\u641c\u7d22\u8868\u73b0\u548c\u8bb0\u5fc6\u6709\u663e\u8457\u5f71\u54cd\uff0c\u7528\u6237\u5bf9\u7269\u7406\u548c\u865a\u62df\u7269\u4f53\u7684\u6ce8\u610f\u529b\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u6839\u636e\u7814\u7a76\u7ed3\u679c\uff0c\u6211\u4eec\u4e3a\u7528\u4e8e\u79fb\u52a8\u573a\u666f\u7684\u589e\u5f3a\u73b0\u5b9e\u5e94\u7528\u63d0\u51fa\u4e86\u8bbe\u8ba1\u5efa\u8bae\u3002"}}
{"id": "2510.26015", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.26015", "abs": "https://arxiv.org/abs/2510.26015", "authors": ["Zhengtao Ma", "Rafael Gomez", "Togtokhtur Batbold", "Zishuo Zhu", "Yueteng Yu", "Ronald Schroeter"], "title": "Designing for Dignity while Driving: Interaction Needs of Blind and Low-Vision Passengers in Fully Automated Vehicles", "comment": null, "summary": "Fully automated vehicles (FAVs) hold promise for enhancing the mobility of\nblind and low-vision (BLV) individuals. To understand the situated interaction\nneeds of BLV passengers, we conducted six on-road, and in-lab focus groups with\n16 participants, immersing them in real-world driving conditions. Our thematic\nanalysis reveals that BLV participants express a high initial 'faith' in FAVs,\nbut require layered, value-sensitive information during the ride to cultivate\ntrust. The participants' modality preference for voice suggests re-evaluating\nthe role of haptics for BLV users in FAVs. Our findings show the importance of\na respectful interaction design in FAVs that both address BLV users' mobility\nchallenges and uphold their dignity. While others have advocated for a dignity\nlens, our contribution lies in grounding this framework in empirical findings\nand unpacking what it means to design for dignity in the context of FAVs.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5c3d\u7ba1BLV\u7528\u6237\u5bf9\u5168\u81ea\u52a8\u8f66\u8f86\u521d\u59cb\u4fe1\u4efb\u8f83\u9ad8\uff0c\u4f46\u4ed6\u4eec\u5728\u4e58\u5750\u8fc7\u7a0b\u4e2d\u9700\u8981\u66f4\u591a\u7684\u4ea4\u4e92\u4fe1\u606f\uff0c\u4e0d\u540c\u7684\u4ea4\u6d41\u65b9\u5f0f\u4e5f\u9700\u91cd\u65b0\u8bc4\u4f30\u3002", "motivation": "\u63a2\u7d22\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u8005\uff08BLV\uff09\u5728\u5168\u81ea\u52a8\u8f66\u8f86\uff08FAV\uff09\u4e2d\u7684\u4e92\u52a8\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u516d\u4e2a\u8def\u9762\u6d4b\u8bd5\u548c\u5b9e\u9a8c\u5ba4\u7126\u70b9\u5c0f\u7ec4\u7814\u7a76\u65b9\u5f0f\uff0c\u6536\u96c616\u540d\u53c2\u4e0e\u8005\u7684\u7ecf\u9a8c\u548c\u610f\u89c1\u3002", "result": "\u53d1\u73b0BLV\u53c2\u4e0e\u8005\u5728\u4f7f\u7528FAV\u65f6\u9700\u8981\u5206\u5c42\u7684\u3001\u6709\u4ef7\u503c\u7684\u4fe1\u606f\uff0c\u4ee5\u57f9\u517b\u5bf9FAV\u7684\u4fe1\u4efb\u3002", "conclusion": "\u8bbe\u8ba1FAV\u65f6\u9700\u5173\u6ce8\u5c0a\u91cd\u548c\u4ef7\u503c\u654f\u611f\u6027\u7684\u4ea4\u4e92\uff0c\u4ee5\u89e3\u51b3BLV\u7528\u6237\u7684\u51fa\u884c\u6311\u6218\uff0c\u5e76\u7ef4\u62a4\u4ed6\u4eec\u7684\u5c0a\u4e25\u3002"}}
{"id": "2510.25850", "categories": ["cs.RO", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.25850", "abs": "https://arxiv.org/abs/2510.25850", "authors": ["Kevin Qiu", "Marek Cygan"], "title": "Debate2Create: Robot Co-design via Large Language Model Debates", "comment": null, "summary": "Automating the co-design of a robot's morphology and control is a\nlong-standing challenge due to the vast design space and the tight coupling\nbetween body and behavior. We introduce Debate2Create (D2C), a framework in\nwhich large language model (LLM) agents engage in a structured dialectical\ndebate to jointly optimize a robot's design and its reward function. In each\nround, a design agent proposes targeted morphological modifications, and a\ncontrol agent devises a reward function tailored to exploit the new design. A\npanel of pluralistic judges then evaluates the design-control pair in\nsimulation and provides feedback that guides the next round of debate. Through\niterative debates, the agents progressively refine their proposals, producing\nincreasingly effective robot designs. Notably, D2C yields diverse and\nspecialized morphologies despite no explicit diversity objective. On a\nquadruped locomotion benchmark, D2C discovers designs that travel 73% farther\nthan the default, demonstrating that structured LLM-based debate can serve as a\npowerful mechanism for emergent robot co-design. Our results suggest that\nmulti-agent debate, when coupled with physics-grounded feedback, is a promising\nnew paradigm for automated robot design.", "AI": {"tldr": "D2C\u6846\u67b6\u5229\u7528\u591a\u667a\u80fd\u4f53\u7684\u8fa9\u8bba\u65b9\u5f0f\u81ea\u52a8\u4f18\u5316\u673a\u5668\u4eba\u7684\u5f62\u6001\u548c\u63a7\u5236\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u7684\u8bbe\u8ba1\u6548\u679c\u548c\u6548\u7387\u3002", "motivation": "\u81ea\u52a8\u5316\u673a\u5668\u4eba\u5f62\u6001\u548c\u63a7\u5236\u7684\u5171\u540c\u8bbe\u8ba1\u662f\u4e00\u4e2a\u957f\u671f\u4ee5\u6765\u7684\u6311\u6218\uff0c\u4e3b\u8981\u56e0\u4e3a\u8bbe\u8ba1\u7a7a\u95f4\u5e7f\u6cdb\u4e14\u5f62\u4f53\u4e0e\u884c\u4e3a\u4e4b\u95f4\u7d27\u5bc6\u8026\u5408\u3002", "method": "D2C\u5f15\u5165\u4e86\u4e00\u4e2a\u8bbe\u8ba1\u4ee3\u7406\u548c\u63a7\u5236\u4ee3\u7406\uff0c\u6bcf\u8f6e\u7531\u8bbe\u8ba1\u4ee3\u7406\u63d0\u51fa\u5f62\u6001\u4fee\u6539\uff0c\u63a7\u5236\u4ee3\u7406\u5236\u5b9a\u76f8\u5e94\u7684\u5956\u52b1\u529f\u80fd\uff0c\u901a\u8fc7\u8bc4\u5ba1\u673a\u6784\u7684\u53cd\u9988\u6765\u6307\u5bfc\u8bbe\u8ba1\u6539\u8fdb\u3002", "result": "D2C\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u8fa9\u8bba\uff0c\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u5171\u540c\u4f18\u5316\u673a\u5668\u4eba\u7684\u8bbe\u8ba1\u548c\u5956\u52b1\u673a\u5236\uff0c\u6700\u7ec8\u5728\u56db\u8db3\u884c\u8d70\u57fa\u51c6\u4e2d\u5b9e\u73b0\u4e86\u4e00\u79cd\u80fd\u591f\u65c5\u884c\u66f4\u8fdc\u7684\u673a\u5668\u4eba\u8bbe\u8ba1\u3002", "conclusion": "D2C\u8868\u660e\uff0c\u5f53\u4e0e\u7269\u7406\u57fa\u7840\u53cd\u9988\u76f8\u7ed3\u5408\u65f6\uff0c\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u662f\u4e00\u79cd\u6709\u524d\u9014\u7684\u65b0\u6a21\u5f0f\uff0c\u53ef\u4ee5\u7528\u4e8e\u673a\u5668\u4eba\u8bbe\u8ba1\u7684\u81ea\u52a8\u5316\u3002"}}
{"id": "2510.26041", "categories": ["cs.HC", "H.5.1; I.3.7; H.5.2"], "pdf": "https://arxiv.org/pdf/2510.26041", "abs": "https://arxiv.org/abs/2510.26041", "authors": ["Jamie Ngoc Dinh", "You-Jin Kim", "Myungin Lee"], "title": "FractalBrain: A Neuro-interactive Virtual Reality Experience using Electroencephalogram (EEG) for Mindfulness", "comment": "Extended Abstracts (Interactivity), 4 pages. Published at the 2024\n  CHI Conference on Human Factors in Computing Systems", "summary": "Mindfulness has been studied and practiced in enhancing psychological\nwell-being while reducing neuroticism and psychopathological indicators.\nHowever, practicing mindfulness with continuous attention is challenging,\nespecially for beginners. In the proposed system, FractalBrain, we utilize an\ninteractive audiovisual fractal with a geometric repetitive pattern that has\nbeen demonstrated to induce meditative effects. FractalBrain presents an\nexperience combining a surreal virtual reality (VR) program with an\nelectroencephalogram (EEG) interface. While viewing an ever-changing\nfractal-inspired artwork in an immersive environment, the user's EEG stream is\nanalyzed and mapped into VR. These EEG data adaptively manipulates the\naudiovisual parameters in real-time, generating a distinct experience for each\nuser. The pilot feedback suggests the potential of the FractalBrain to\nfacilitate mindfulness and enhance attention.", "AI": {"tldr": "FractalBrain\u662f\u4e00\u4e2a\u7ed3\u5408VR\u548c\u8111\u7535\u56fe\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u5b9e\u65f6\u5206\u6790\u7528\u6237\u7684\u8111\u6ce2\u6570\u636e\uff0c\u63d0\u4f9b\u4e2a\u6027\u5316\u7684\u6b63\u5ff5\u7ec3\u4e60\u4f53\u9a8c\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u521d\u5b66\u8005\u5728\u6301\u7eed\u6ce8\u610f\u529b\u6b63\u5ff5\u7ec3\u4e60\u4e2d\u7684\u6311\u6218\uff0c\u5229\u7528\u4ea4\u4e92\u5f0f\u7684\u89c6\u542c\u5206\u5f62\u589e\u5f3a\u5fc3\u7406\u5065\u5eb7\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u865a\u62df\u73b0\u5b9e\u7a0b\u5e8f\u4e0e\u8111\u7535\u56fe(EEG)\u63a5\u53e3\uff0c\u5b9e\u65f6\u5206\u6790\u7528\u6237\u7684EEG\u6570\u636e\u4ee5\u8c03\u6574\u89c6\u542c\u53c2\u6570\u3002", "result": "\u8bd5\u70b9\u53cd\u9988\u663e\u793aFractalBrain\u5bf9\u6b63\u5ff5\u7ec3\u4e60\u548c\u6ce8\u610f\u529b\u589e\u5f3a\u6709\u6f5c\u529b\u3002", "conclusion": "FractalBrain\u7cfb\u7edf\u80fd\u591f\u4fc3\u8fdb\u6b63\u5ff5\u7ec3\u4e60\u5e76\u589e\u5f3a\u6ce8\u610f\u529b\uff0c\u4ea7\u751f\u4e2a\u6027\u5316\u7684\u6c89\u6d78\u4f53\u9a8c\u3002"}}
{"id": "2510.25913", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.25913", "abs": "https://arxiv.org/abs/2510.25913", "authors": ["Gilbert Bahati", "Ryan M. Bena", "Meg Wilkinson", "Pol Mestres", "Ryan K. Cosner", "Aaron D. Ames"], "title": "Risk-Aware Safety Filters with Poisson Safety Functions and Laplace Guidance Fields", "comment": null, "summary": "Robotic systems navigating in real-world settings require a semantic\nunderstanding of their environment to properly determine safe actions. This\nwork aims to develop the mathematical underpinnings of such a representation --\nspecifically, the goal is to develop safety filters that are risk-aware. To\nthis end, we take a two step approach: encoding an understanding of the\nenvironment via Poisson's equation, and associated risk via Laplace guidance\nfields. That is, we first solve a Dirichlet problem for Poisson's equation to\ngenerate a safety function that encodes system safety as its 0-superlevel set.\nWe then separately solve a Dirichlet problem for Laplace's equation to\nsynthesize a safe \\textit{guidance field} that encodes variable levels of\ncaution around obstacles -- by enforcing a tunable flux boundary condition. The\nsafety function and guidance fields are then combined to define a safety\nconstraint and used to synthesize a risk-aware safety filter which, given a\nsemantic understanding of an environment with associated risk levels of\nenvironmental features, guarantees safety while prioritizing avoidance of\nhigher risk obstacles. We demonstrate this method in simulation and discuss how\n\\textit{a priori} understandings of obstacle risk can be directly incorporated\ninto the safety filter to generate safe behaviors that are risk-aware.", "AI": {"tldr": "\u8be5\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u98ce\u9669\u610f\u8bc6\u7684\u5b89\u5168\u8fc7\u6ee4\u5668\uff0c\u901a\u8fc7\u7406\u89e3\u73af\u5883\u6765\u4fdd\u969c\u673a\u5668\u4eba\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u5b89\u5168\u884c\u4e3a\u3002", "motivation": "\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u5bfc\u822a\u9700\u8981\u5bf9\u73af\u5883\u8fdb\u884c\u8bed\u4e49\u7406\u89e3\uff0c\u4ee5\u6b63\u786e\u5224\u65ad\u5b89\u5168\u884c\u4e3a\u3002", "method": "\u901a\u8fc7\u89e3Dirichlet\u95ee\u9898\uff0c\u5bf9\u4e8ePoisson\u65b9\u7a0b\u548cLaplace\u65b9\u7a0b\uff0c\u751f\u6210\u5b89\u5168\u51fd\u6570\u548c\u5b89\u5168\u6307\u5bfc\u573a\uff0c\u5408\u5e76\u4ee5\u5b9a\u4e49\u5b89\u5168\u7ea6\u675f\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u79cd\u98ce\u9669\u610f\u8bc6\u7684\u5b89\u5168\u8fc7\u6ee4\u5668\uff0c\u7ed3\u5408\u4e86\u5b89\u5168\u51fd\u6570\u548c\u6307\u5bfc\u573a\u4ee5\u786e\u4fdd\u5b89\u5168\u3002", "conclusion": "\u901a\u8fc7\u5c06\u98ce\u9669\u6c34\u5e73\u4e0e\u969c\u788d\u7269\u7279\u5f81\u5173\u8054\uff0c\u786e\u4fdd\u673a\u5668\u4eba\u7684\u5b89\u5168\u884c\u4e3a\u4f18\u5148\u907f\u5f00\u9ad8\u98ce\u9669\u969c\u788d\u7269\u3002"}}
{"id": "2510.26069", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.26069", "abs": "https://arxiv.org/abs/2510.26069", "authors": ["Leixian Shen", "Yifang Wang", "Huamin Qu", "Xing Xie", "Haotian Li"], "title": "Interaction-Augmented Instruction: Modeling the Synergy of Prompts and Interactions in Human-GenAI Collaboration", "comment": "26 pages", "summary": "Text prompt is the most common way for human-generative AI (GenAI)\ncommunication. Though convenient, it is challenging to convey fine-grained and\nreferential intent. One promising solution is to combine text prompts with\nprecise GUI interactions, like brushing and clicking. However, there lacks a\nformal model to model synergistic designs between prompts and interactions,\nhindering their comparison and innovation. To fill this gap, via an iterative\nand deductive process, we develop the Interaction-Augmented Instruction (IAI)\nmodel, a compact entity-relation graph formalizing how the combination of\ninteractions and text prompts enhances human-generative AI communication. With\nthe model, we distill twelve recurring and composable atomic interaction\nparadigms from prior tools, verifying our model's capability to facilitate\nsystematic design characterization and comparison. Case studies further\ndemonstrate the model's utility in applying, refining, and extending these\nparadigms. These results illustrate our IAI model's descriptive,\ndiscriminative, and generative power for shaping future GenAI systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4ea4\u4e92\u589e\u5f3a\u6307\u4ee4\uff08IAI\uff09\u6a21\u578b\uff0c\u4ee5\u4fc3\u8fdb\u6587\u672c\u63d0\u793a\u4e0eGUI\u4ea4\u4e92\u7684\u6709\u6548\u6574\u5408\uff0c\u4ece\u800c\u6539\u5584\u4eba\u7c7b\u4e0e\u751f\u6210\u6027AI\u4e4b\u95f4\u7684\u901a\u4fe1\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u63d0\u793a\u65b9\u5f0f\u96be\u4ee5\u4f20\u8fbe\u7ec6\u7c92\u5ea6\u548c\u6307\u5411\u6027\u7684\u610f\u56fe\uff0c\u7f3a\u4e4f\u5c06\u5176\u4e0eGUI\u4ea4\u4e92\u7ed3\u5408\u7684\u6b63\u5f0f\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u8fed\u4ee3\u548c\u63a8\u7406\u7684\u8fc7\u7a0b\uff0c\u5f00\u53d1\u4e86IAI\u6a21\u578b\uff0c\u5e76\u4ece\u4ee5\u5f80\u5de5\u5177\u4e2d\u63d0\u70bc\u51fa\u5341\u4e8c\u79cd\u539f\u5b50\u4ea4\u4e92\u8303\u5f0f\u3002", "result": "IAI\u6a21\u578b\u9a8c\u8bc1\u4e86\u5176\u5728\u8bbe\u8ba1\u7279\u5f81\u63cf\u8ff0\u3001\u6bd4\u8f83\u548c\u793a\u4f8b\u7814\u7a76\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u672a\u6765GenAI\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "IAI\u6a21\u578b\u80fd\u591f\u7cfb\u7edf\u6027\u5730\u63cf\u8ff0\u548c\u6bd4\u8f83\u4ea4\u4e92\u4e0e\u6587\u672c\u63d0\u793a\u7684\u7ec4\u5408\uff0c\u4fc3\u8fdb\u672a\u6765\u751f\u6210\u6027AI\u7cfb\u7edf\u7684\u8bbe\u8ba1\u521b\u65b0\u3002"}}
{"id": "2510.25965", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.25965", "abs": "https://arxiv.org/abs/2510.25965", "authors": ["Luoyan Zhong", "Heather Jin Hee Kim", "Dylan P. Losey", "Cara M. Nunez"], "title": "Curvature-Aware Calibration of Tactile Sensors for Accurate Force Estimation on Non-Planar Surfaces", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Flexible tactile sensors are increasingly used in real-world applications\nsuch as robotic grippers, prosthetic hands, wearable gloves, and assistive\ndevices, where they need to conform to curved and irregular surfaces. However,\nmost existing tactile sensors are calibrated only on flat substrates, and their\naccuracy and consistency degrade once mounted on curved geometries. This\nlimitation restricts their reliability in practical use. To address this\nchallenge, we develop a calibration model for a widely used resistive tactile\nsensor design that enables accurate force estimation on one-dimensional curved\nsurfaces. We then train a neural network (a multilayer perceptron) to predict\nlocal curvature from baseline sensor outputs recorded under no applied load,\nachieving an R2 score of 0.91. The proposed approach is validated on five daily\nobjects with varying curvatures under forces from 2 N to 8 N. Results show that\nthe curvature-aware calibration maintains consistent force accuracy across all\nsurfaces, while flat-surface calibration underestimates force as curvature\nincreases. Our results demonstrate that curvature-aware modeling improves the\naccuracy, consistency, and reliability of flexible tactile sensors, enabling\ndependable performance across real-world applications.", "AI": {"tldr": "\u7814\u7a76\u9488\u5bf9\u67d4\u6027\u89e6\u89c9\u4f20\u611f\u5668\u5728\u66f2\u9762\u4e0a\u7684\u6821\u51c6\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u529b\u6d4b\u91cf\u7684\u51c6\u786e\u6027\u4e0e\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u89e6\u89c9\u4f20\u611f\u5668\u591a\u53ea\u5728\u5e73\u9762\u4e0a\u8fdb\u884c\u6821\u51c6\uff0c\u5bfc\u81f4\u5728\u66f2\u9762\u4e0a\u7684\u5e94\u7528\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u964d\u4f4e\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u65b0\u7684\u6821\u51c6\u65b9\u6cd5\u4ee5\u63d0\u5347\u5176\u5b9e\u7528\u6027\u3002", "method": "\u91c7\u7528\u591a\u5c42\u611f\u77e5\u5668\u795e\u7ecf\u7f51\u7edc\uff0c\u6839\u636e\u4f20\u611f\u5668\u7684\u57fa\u7ebf\u8f93\u51fa\u9884\u6d4b\u5c40\u90e8\u66f2\u7387\uff0c\u5e76\u5728\u4e0d\u540c\u66f2\u7387\u7684\u7269\u4f53\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7528\u4e8e\u67d4\u6027\u89e6\u89c9\u4f20\u611f\u5668\u7684\u6821\u51c6\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86\u5176\u5728\u66f2\u9762\u4e0a\u7684\u529b\u4f30\u8ba1\u51c6\u786e\u6027\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u66f2\u7387\u611f\u77e5\u6821\u51c6\uff0c\u67d4\u6027\u89e6\u89c9\u4f20\u611f\u5668\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u5907\u4e86\u66f4\u597d\u7684\u53ef\u9760\u6027\u548c\u7cbe\u51c6\u5ea6\u3002"}}
{"id": "2510.26172", "categories": ["cs.HC", "cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.26172", "abs": "https://arxiv.org/abs/2510.26172", "authors": ["Shifu Chen", "Dazhen Deng", "Zhihong Xu", "Sijia Xu", "Tai-Quan Peng", "Yingcai Wu"], "title": "Linking Heterogeneous Data with Coordinated Agent Flows for Social Media Analysis", "comment": null, "summary": "Social media platforms generate massive volumes of heterogeneous data,\ncapturing user behaviors, textual content, temporal dynamics, and network\nstructures. Analyzing such data is crucial for understanding phenomena such as\nopinion dynamics, community formation, and information diffusion. However,\ndiscovering insights from this complex landscape is exploratory, conceptually\nchallenging, and requires expertise in social media mining and visualization.\nExisting automated approaches, though increasingly leveraging large language\nmodels (LLMs), remain largely confined to structured tabular data and cannot\nadequately address the heterogeneity of social media analysis. We present SIA\n(Social Insight Agents), an LLM agent system that links heterogeneous\nmulti-modal data -- including raw inputs (e.g., text, network, and behavioral\ndata), intermediate outputs, mined analytical results, and visualization\nartifacts -- through coordinated agent flows. Guided by a bottom-up taxonomy\nthat connects insight types with suitable mining and visualization techniques,\nSIA enables agents to plan and execute coherent analysis strategies. To ensure\nmulti-modal integration, it incorporates a data coordinator that unifies\ntabular, textual, and network data into a consistent flow. Its interactive\ninterface provides a transparent workflow where users can trace, validate, and\nrefine the agent's reasoning, supporting both adaptability and trustworthiness.\nThrough expert-centered case studies and quantitative evaluation, we show that\nSIA effectively discovers diverse and meaningful insights from social media\nwhile supporting human-agent collaboration in complex analytical tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSIA\u7cfb\u7edf\uff0c\u65e8\u5728\u901a\u8fc7\u534f\u8c03\u4ee3\u7406\u6d41\u7a0b\u5904\u7406\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u5f02\u6784\u591a\u6a21\u6001\u6570\u636e\uff0c\u652f\u6301\u4eba\u673a\u5408\u4f5c\uff0c\u5e76\u6709\u6548\u53d1\u73b0\u6709\u610f\u4e49\u7684\u6d1e\u5bdf\u3002", "motivation": "\u6df1\u5165\u7406\u89e3\u793e\u4ea4\u5a92\u4f53\u5206\u6790\u4e2d\u7684\u590d\u6742\u73b0\u8c61\uff0c\u5982\u8206\u8bba\u52a8\u6001\u548c\u4fe1\u606f\u6269\u6563\uff0c\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u5904\u7406\u5f02\u6784\u6570\u636e\uff0c\u6025\u9700\u65b0\u7684\u65b9\u6cd5\u8bba\u3002", "method": "\u63d0\u51faSIA\u7cfb\u7edf\uff0c\u901a\u8fc7\u534f\u8c03\u7684\u4ee3\u7406\u6d41\u7a0b\u8fde\u63a5\u5f02\u6784\u591a\u6a21\u6001\u6570\u636e\uff0c\u4ee5\u5b9e\u73b0\u4e00\u81f4\u7684\u6570\u636e\u6d41\u548c\u900f\u660e\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "SIA\u7cfb\u7edf\u901a\u8fc7\u4e13\u5bb6\u4e2d\u5fc3\u6848\u4f8b\u7814\u7a76\u548c\u5b9a\u91cf\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u5176\u5728\u793e\u4ea4\u5a92\u4f53\u5206\u6790\u4e2d\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u751f\u6210\u4e30\u5bcc\u7684\u6d1e\u5bdf\u3002", "conclusion": "SIA\uff08\u793e\u4ea4\u6d1e\u5bdf\u4ee3\u7406\uff09\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u5730\u53d1\u73b0\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u591a\u6837\u4e14\u6709\u610f\u4e49\u7684\u6d1e\u5bdf\uff0c\u540c\u65f6\u652f\u6301\u4eba\u7c7b\u548c\u4ee3\u7406\u5728\u590d\u6742\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u5408\u4f5c\u3002"}}
{"id": "2510.25985", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.25985", "abs": "https://arxiv.org/abs/2510.25985", "authors": ["Francisco M. F. R. Gon\u00e7alves", "Ryan M. Bena", "N\u00e9stor O. P\u00e9rez-Arancibia"], "title": "A New Type of Axis-Angle Attitude Control Law for Rotational Systems: Synthesis, Analysis, and Experiments", "comment": "2025 International Conference on Advanced Robotics (ICAR)", "summary": "Over the past few decades, continuous quaternion-based attitude control has\nbeen proven highly effective for driving rotational systems that can be modeled\nas rigid bodies, such as satellites and drones. However, methods rooted in this\napproach do not enforce the existence of a unique closed-loop (CL) equilibrium\nattitude-error quaternion (AEQ); and, for rotational errors about the\nattitude-error Euler axis larger than {\\pi}rad, their proportional-control\neffect diminishes as the system state moves away from the stable equilibrium of\nthe CL rotational dynamics. In this paper, we introduce a new type of attitude\ncontrol law that more effectively leverages the attitude-error Euler axis-angle\ninformation to guarantee a unique CL equilibrium AEQ and to provide greater\nflexibility in the use of proportional-control efforts. Furthermore, using two\ndifferent control laws as examples-through the construction of a strict\nLyapunov function for the CL dynamics-we demonstrate that the resulting unique\nequilibrium of the CL rotational system can be enforced to be uniformly\nasymptotically stable. To assess and demonstrate the functionality and\nperformance of the proposed approach, we performed numerical simulations and\nexecuted dozens of real-time tumble-recovery maneuvers using a small quadrotor.\nThese simulations and flight tests compellingly demonstrate that the proposed\naxis-angle-based method achieves superior flight performance-compared with that\nobtained using a high-performance quaternion-based controller-in terms of\nstabilization time.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u59ff\u6001\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u59ff\u6001\u8bef\u5dee\u7684\u63a7\u5236\uff0c\u4ee5\u4fdd\u8bc1\u552f\u4e00\u7684\u95ed\u73af\u5e73\u8861\uff0c\u5e76\u5728\u56db\u65cb\u7ffc\u7684\u4eff\u771f\u548c\u5b9e\u9645\u6d4b\u8bd5\u4e2d\u663e\u793a\u51fa\u66f4\u597d\u7684\u98de\u884c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u56db\u5143\u6570\u57fa\u7840\u59ff\u6001\u63a7\u5236\u65b9\u6cd5\u5728\u5904\u7406\u95ed\u73af\u5e73\u8861\u8bef\u5dee\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u59ff\u6001\u8bef\u5dee\u8f83\u5927\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u59ff\u6001\u63a7\u5236\u89c4\u5f8b\uff0c\u5229\u7528\u59ff\u6001\u8bef\u5dee\u6b27\u62c9\u8f74-\u89d2\u4fe1\u606f\uff0c\u6784\u9020\u4e25\u683c\u7684\u674e\u96c5\u752b\u8bfa\u592b\u51fd\u6570\u8fdb\u884c\u5206\u6790\u3002", "result": "\u91c7\u7528\u65b0\u7684\u63a7\u5236\u89c4\u5f8b\u540e\uff0c\u72ec\u7279\u7684\u95ed\u73af\u5e73\u8861\u59ff\u6001\u8bef\u5dee\u56db\u5143\u6570\u88ab\u4fdd\u8bc1\uff0c\u5e76\u4e14\u5b9e\u73b0\u4e86\u7edf\u4e00\u6e10\u8fd1\u7a33\u5b9a\u6027\u3002\u901a\u8fc7\u6570\u503c\u4eff\u771f\u548c\u5c0f\u578b\u56db\u65cb\u7ffc\u7684\u5b9e\u65f6\u6d4b\u8bd5\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8f74-\u89d2\u63a7\u5236\u65b9\u6cd5\u5728\u7a33\u5b9a\u65f6\u95f4\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\uff0c\u8d85\u8d8a\u4e86\u9ad8\u6027\u80fd\u7684\u56db\u5143\u6570\u63a7\u5236\u5668\u3002"}}
{"id": "2510.26251", "categories": ["cs.HC", "H.1.2"], "pdf": "https://arxiv.org/pdf/2510.26251", "abs": "https://arxiv.org/abs/2510.26251", "authors": ["Navid Ashrafi", "Philipp Graf", "Manuela Marquardt", "Francesco Vona", "Julia Schorlemmer", "Jan-Niklas Voigt-Antons"], "title": "Avatar Appearance Beyond Pixels -- User Ratings and Avatar Preferences within Health Applications", "comment": "17 pages, 3 figures, 1 table", "summary": "The appearance of a virtual avatar significantly influences its perceived\nappropriateness and the user's experience, particularly in healthcare\napplications. This study analyzed interactions with six avatars of varying\ncharacteristics in a patient-reported outcome measures (PROMs) application to\ninvestigate correlations between avatar ratings and user preferences.\nForty-seven participants completed a healthcare survey involving 30 PROMIS\nitems (Global Health and Physical Function) and then rated the avatars on\nwarmth, competence, attractiveness, and human-likeness, as well as their\nwillingness to share personal data. The results showed that competence was the\nmost critical factor in avatar selection, while human-likeness had minimal\nimpact on health data disclosure. Gender did not significantly affect the\nratings, but clothing style played a key role, with male avatars in\nprofessional attire rated higher in competence due to gender-stereotypical\nexpectations. In contrast, professional female avatars were rated lower in\nwarmth and attractiveness. These findings underline the importance of\nthoughtful avatar design in healthcare applications to enhance user experience\nand engagement.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4e0d\u540c\u7279\u5f81\u865a\u62df\u5934\u50cf\u5bf9\u7528\u6237\u4f53\u9a8c\u7684\u5f71\u54cd\uff0c\u7ed3\u679c\u663e\u793a\u80fd\u529b\u662f\u5934\u50cf\u9009\u62e9\u7684\u4e3b\u8981\u56e0\u7d20\uff0c\u5404\u6027\u522b\u5934\u50cf\u5728\u80fd\u529b\u3001\u6e29\u6696\u548c\u5438\u5f15\u529b\u65b9\u9762\u7684\u8bc4\u5206\u5b58\u5728\u5dee\u5f02\u3002", "motivation": "\u63a2\u7a76\u865a\u62df\u5934\u50cf\u5916\u89c2\u5982\u4f55\u5f71\u54cd\u5176\u9002\u7528\u6027\u53ca\u7528\u6237\u4f53\u9a8c\uff0c\u7279\u522b\u662f\u5728\u533b\u7597\u5e94\u7528\u573a\u666f\u4e2d", "method": "\u5206\u6790\u516d\u79cd\u4e0d\u540c\u7279\u5f81\u865a\u62df\u5934\u50cf\u4e0e\u7528\u6237\u504f\u597d\u7684\u4e92\u52a8", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u80fd\u529b\u662f\u5934\u50cf\u9009\u62e9\u4e2d\u6700\u5173\u952e\u7684\u56e0\u7d20\uff0c\u800c\u4eba\u6027\u5316\u5bf9\u5065\u5eb7\u6570\u636e\u5171\u4eab\u7684\u5f71\u54cd\u5fae\u4e4e\u5176\u5fae\u3002\u6027\u522b\u5bf9\u8bc4\u5206\u6ca1\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4f46\u670d\u88c5\u98ce\u683c\u5374\u8d77\u5230\u4e86\u5173\u952e\u89d2\u8272\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u533b\u7597\u5e94\u7528\u4e2d\u865a\u62df\u5934\u50cf\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u548c\u53c2\u4e0e\u5ea6\u3002"}}
{"id": "2510.26004", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26004", "abs": "https://arxiv.org/abs/2510.26004", "authors": ["Bai Li", "Achilleas Kourtellis", "Rong Cao", "Joseph Post", "Brian Porter", "Yu Zhang"], "title": "DARTS: A Drone-Based AI-Powered Real-Time Traffic Incident Detection System", "comment": "Preprint version. This manuscript is currently under review at\n  Transportation Research Part C: Emerging Technologies. The PDF corresponds to\n  the version submitted in June 2025. The main findings of this work were\n  recognized with the Best Intelligent Transportation Systems Paper Award at\n  the 2025 TRB Annual Meeting", "summary": "Rapid and reliable incident detection is critical for reducing crash-related\nfatalities, injuries, and congestion. However, conventional methods, such as\nclosed-circuit television, dashcam footage, and sensor-based detection,\nseparate detection from verification, suffer from limited flexibility, and\nrequire dense infrastructure or high penetration rates, restricting\nadaptability and scalability to shifting incident hotspots. To overcome these\nchallenges, we developed DARTS, a drone-based, AI-powered real-time traffic\nincident detection system. DARTS integrates drones' high mobility and aerial\nperspective for adaptive surveillance, thermal imaging for better\nlow-visibility performance and privacy protection, and a lightweight deep\nlearning framework for real-time vehicle trajectory extraction and incident\ndetection. The system achieved 99% detection accuracy on a self-collected\ndataset and supports simultaneous online visual verification, severity\nassessment, and incident-induced congestion propagation monitoring via a\nweb-based interface. In a field test on Interstate 75 in Florida, DARTS\ndetected and verified a rear-end collision 12 minutes earlier than the local\ntransportation management center and monitored incident-induced congestion\npropagation, suggesting potential to support faster emergency response and\nenable proactive traffic control to reduce congestion and secondary crash risk.\nCrucially, DARTS's flexible deployment architecture reduces dependence on\nfrequent physical patrols, indicating potential scalability and\ncost-effectiveness for use in remote areas and resource-constrained settings.\nThis study presents a promising step toward a more flexible and integrated\nreal-time traffic incident detection system, with significant implications for\nthe operational efficiency and responsiveness of modern transportation\nmanagement.", "AI": {"tldr": "DARTS\u662f\u4e00\u79cd\u521b\u65b0\u7684\u65e0\u4eba\u673a\u57fa\u7840AI\u5b9e\u65f6\u4ea4\u901a\u4e8b\u6545\u68c0\u6d4b\u7cfb\u7edf\uff0c\u5177\u6709\u9ad8\u51c6\u786e\u7387\u548c\u5b9e\u65f6\u76d1\u6d4b\u80fd\u529b\u3002", "motivation": "\u5feb\u901f\u4e14\u53ef\u9760\u7684\u4e8b\u6545\u68c0\u6d4b\u5bf9\u4e8e\u964d\u4f4e\u4e0e\u4e8b\u6545\u76f8\u5173\u7684\u6b7b\u4ea1\u7387\u3001\u4f24\u5bb3\u548c\u62e5\u5835\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3aDARTS\u7684\u65e0\u4eba\u673a\u57fa\u7840AI\u5b9e\u65f6\u4ea4\u901a\u4e8b\u6545\u68c0\u6d4b\u7cfb\u7edf\u3002", "result": "DARTS\u7cfb\u7edf\u5728\u81ea\u6536\u96c6\u7684\u6570\u636e\u96c6\u4e0a\u5b9e\u73b099%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u652f\u6301\u5b9e\u65f6\u8f66\u8f86\u8f68\u8ff9\u63d0\u53d6\u548c\u4e8b\u6545\u68c0\u6d4b\uff0c\u8fd8\u80fd\u8fdb\u884c\u5728\u7ebf\u53ef\u89c6\u5316\u9a8c\u8bc1\u3001\u4e25\u91cd\u6027\u8bc4\u4f30\u53ca\u4e8b\u6545\u9020\u6210\u7684\u62e5\u5835\u4f20\u64ad\u76d1\u6d4b\u3002", "conclusion": "DARTS\u7684\u7075\u6d3b\u90e8\u7f72\u67b6\u6784\u51cf\u5c11\u4e86\u5bf9\u9891\u7e41\u5b9e\u5730\u5de1\u903b\u7684\u4f9d\u8d56\uff0c\u8868\u660e\u5728\u8d44\u6e90\u532e\u4e4f\u7684\u73af\u5883\u4e2d\u5177\u6709\u6f5c\u5728\u7684\u53ef\u6269\u5c55\u6027\u548c\u6210\u672c\u6548\u76ca\uff0c\u652f\u6301\u73b0\u4ee3\u4ea4\u901a\u7ba1\u7406\u7684\u8fd0\u8425\u6548\u7387\u548c\u54cd\u5e94\u80fd\u529b\u3002"}}
{"id": "2510.26265", "categories": ["cs.HC", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.26265", "abs": "https://arxiv.org/abs/2510.26265", "authors": ["Ling-Long Zou", "Qiang Tong", "Er-Xia Luo", "Sen-Zhe Xu", "Song-Hai Zhang", "Fang-Lue Zhang"], "title": "Look at That Distractor: Dynamic Translation Gain under Low Perceptual Load in Virtual Reality", "comment": null, "summary": "Redirected walking utilizes gain adjustments within perceptual thresholds to\nallow natural navigation in large scale virtual environments within confined\nphysical environments. Previous research has found that when users are\ndistracted by some scene elements, they are less sensitive to gain values.\nHowever, the effects on detection thresholds have not been quantitatively\nmeasured. In this paper, we present a novel method that dynamically adjusts\ntranslation gain by leveraging visual distractors. We place distractors within\nthe user's field of view and apply a larger translation gain when their\nattention is drawn to them. Because the magnitude of gain adjustment depends on\nthe user's level of engagement with the distractors, the redirection process\nremains smooth and unobtrusive. To evaluate our method, we developed a task\noriented virtual environment for a user study. Results show that introducing\ndistractors in the virtual environment significantly raises users' translation\ngain thresholds. Furthermore, assessments using the Simulator Sickness\nQuestionnaire and Igroup Presence Questionnaire indicate that the method\nmaintains user comfort and acceptance, supporting its effectiveness for RDW\nsystems.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7528\u6237\u89c6\u91ce\u4e2d\u5f15\u5165\u5e72\u6270\u7269\u6765\u52a8\u6001\u8c03\u6574\u5e73\u79fb\u589e\u76ca\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u8005\u5e0c\u671b\u89e3\u51b3\u7528\u6237\u5728\u865a\u62df\u73af\u5883\u4e2d\u5bfc\u822a\u65f6\uff0c\u5982\u4f55\u5728\u7269\u7406\u7a7a\u95f4\u9650\u5236\u5185\u5b9e\u73b0\u66f4\u81ea\u7136\u7684\u884c\u8d70\u4f53\u9a8c\u3002", "method": "\u5728\u7528\u6237\u89c6\u91ce\u4e2d\u653e\u7f6e\u5e72\u6270\u7269\uff0c\u5e76\u6839\u636e\u7528\u6237\u5bf9\u5e72\u6270\u7269\u7684\u6ce8\u610f\u529b\u52a8\u6001\u8c03\u6574\u7ffb\u8bd1\u589e\u76ca\uff0c\u7528\u4e8e\u8bc4\u4f30\u5176\u5728\u865a\u62df\u73af\u5883\u4e2d\u7684\u6548\u679c\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u5e72\u6270\u7269\u52a8\u6001\u8c03\u6574\u5e73\u79fb\u589e\u76ca\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7528\u6237\u7684\u5e73\u79fb\u589e\u76ca\u9608\u503c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u5e73\u79fb\u589e\u76ca\u9608\u503c\uff0c\u540c\u65f6\u4fdd\u6301\u7528\u6237\u7684\u8212\u9002\u6027\u548c\u63a5\u53d7\u5ea6\uff0c\u9002\u7528\u4e8e\u91cd\u5b9a\u5411\u884c\u8d70\u7cfb\u7edf\u3002"}}
{"id": "2510.26018", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26018", "abs": "https://arxiv.org/abs/2510.26018", "authors": ["Petr Stibinger", "Tomas Baca", "Daniela Doubravova", "Jan Rusnak", "Jaroslav Solc", "Jan Jakubek", "Petr Stepan", "Martin Saska"], "title": "RADRON: Cooperative Localization of Ionizing Radiation Sources by MAVs with Compton Cameras", "comment": "8 pages, 9 figures, submitted for review to IEEE RA-L", "summary": "We present a novel approach to localizing radioactive material by cooperating\nMicro Aerial Vehicles (MAVs). Our approach utilizes a state-of-the-art\nsingle-detector Compton camera as a highly sensitive, yet miniature detector of\nionizing radiation. The detector's exceptionally low weight (40 g) opens up new\npossibilities of radiation detection by a team of cooperating agile MAVs. We\npropose a new fundamental concept of fusing the Compton camera measurements to\nestimate the position of the radiation source in real time even from extremely\nsparse measurements. The data readout and processing are performed directly\nonboard and the results are used in a dynamic feedback to drive the motion of\nthe vehicles. The MAVs are stabilized in a tightly cooperating swarm to\nmaximize the information gained by the Compton cameras, rapidly locate the\nradiation source, and even track a moving radiation source.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u901a\u8fc7\u534f\u4f5c\u7684\u5fae\u578b\u65e0\u4eba\u673a\u5229\u7528\u5eb7\u666e\u987f\u76f8\u673a\u5b9e\u65f6\u5b9a\u4f4d\u653e\u5c04\u6027\u6e90\u7684\u65b0\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u653e\u5c04\u6027\u7269\u8d28\u5b9a\u4f4d\uff0c\u5f3a\u8c03\u4e86\u8f7b\u91cf\u5316\u63a2\u6d4b\u5668\u548cMAVs\u7684\u5408\u4f5c\u3002", "method": "\u5229\u7528\u5355\u63a2\u6d4b\u5668\u5eb7\u666e\u987f\u76f8\u673a\u4f5c\u4e3a\u7075\u654f\u7684\u5c0f\u578b\u8f90\u5c04\u63a2\u6d4b\u5668\uff0cMAVs\u901a\u8fc7\u5408\u4f5c\u8fdb\u884c\u8f90\u5c04\u6e90\u5b9a\u4f4d\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u7cfb\u7edf\u80fd\u5728\u7a00\u758f\u7684\u6d4b\u91cf\u6761\u4ef6\u4e0b\u5b9e\u65f6\u4f30\u8ba1\u8f90\u5c04\u6e90\u4f4d\u7f6e\uff0c\u5e76\u80fd\u8ddf\u8e2a\u79fb\u52a8\u6e90\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u5408\u4f5c\u5fae\u578b\u65e0\u4eba\u673a\uff08MAVs\uff09\u80fd\u6709\u6548\u5b9a\u4f4d\u653e\u5c04\u6027\u7269\u8d28\uff0c\u5177\u6709\u91cd\u8981\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2510.26490", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.26490", "abs": "https://arxiv.org/abs/2510.26490", "authors": ["Alon Rosenbaum", "Yigal David", "Eran Kaufman", "Gilad Ravid", "Amit Ronen", "Assaf Krebs"], "title": "Scaffolding Creativity: How Divergent and Convergent LLM Personas Shape Human Machine Creative Problem-Solving", "comment": null, "summary": "Large language models (LLMs) are increasingly shaping creative work and\nproblem-solving; however, prior research suggests that they may diminish\nunassisted creativity. To address this tension, a coach-like LLM environment\nwas developed that embodies divergent and convergent thinking personas as two\ncomplementary processes. Effectiveness and user behavior were assessed through\na controlled experiment in which participants interacted with either persona,\nwhile a control group engaged with a standard LLM providing direct answers.\n  Notably, users' perceptions of which persona best supported their creativity\noften diverged from objective performance measures. Trait-based analyses\nrevealed that individual differences predict when people utilize divergent\nversus convergent personas, suggesting opportunities for adaptive sequencing.\nFurthermore, interaction patterns reflected the design thinking model,\ndemonstrating how persona-guided support shapes creative problem-solving.\n  Our findings provide design principles for creativity support systems that\nstrike a balance between exploration and convergence through persona-based\nguidance and personalization. These insights advance human-AI collaboration\ntools that scaffold rather than overshadow human creativity.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u89d2\u8272\u5f15\u5bfc\u7684\u5927\u8bed\u8a00\u6a21\u578b\u73af\u5883\uff0c\u8bc4\u4f30\u4e0d\u540c\u601d\u7ef4\u89d2\u8272\u5bf9\u521b\u9020\u529b\u652f\u6301\u7684\u5f71\u54cd\uff0c\u7ed3\u679c\u8868\u660e\u4e2a\u6027\u5316\u89d2\u8272\u6307\u5bfc\u6709\u52a9\u4e8e\u5e73\u8861\u63a2\u7d22\u4e0e\u6536\u655b\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u521b\u9020\u6027\u5de5\u4f5c\u548c\u89e3\u51b3\u95ee\u9898\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u5148\u524d\u7814\u7a76\u8868\u660e\uff0c\u5b83\u4eec\u53ef\u80fd\u4f1a\u524a\u5f31\u65e0\u8f85\u52a9\u521b\u9020\u529b\u3002", "method": "\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\u8bc4\u4f30\u7528\u6237\u884c\u4e3a\uff0c\u53c2\u4e0e\u8005\u4e0e\u4e24\u79cd\u89d2\u8272\uff08\u53d1\u6563\u601d\u7ef4\u548c\u6536\u655b\u601d\u7ef4\uff09\u8fdb\u884c\u4e92\u52a8\uff0c\u63a7\u5236\u7ec4\u5219\u4e0e\u63d0\u4f9b\u76f4\u63a5\u7b54\u6848\u7684\u6807\u51c6\u5927\u8bed\u8a00\u6a21\u578b\u4e92\u52a8\u3002", "result": "\u7528\u6237\u5bf9\u54ea\u79cd\u89d2\u8272\u66f4\u597d\u5730\u652f\u6301\u5176\u521b\u9020\u529b\u7684\u8ba4\u77e5\u4e0e\u5ba2\u89c2\u8868\u73b0\u6d4b\u91cf\u4e4b\u95f4\u5e38\u5e38\u5b58\u5728\u5dee\u5f02\uff1b\u4e2a\u6027\u7279\u5f81\u5206\u6790\u663e\u793a\uff0c\u4e2a\u4f53\u5dee\u5f02\u80fd\u591f\u9884\u6d4b\u4eba\u4eec\u4f55\u65f6\u5229\u7528\u53d1\u6563\u6216\u6536\u655b\u89d2\u8272\uff0c\u4ea4\u4e92\u6a21\u5f0f\u53cd\u6620\u4e86\u8bbe\u8ba1\u601d\u7ef4\u6a21\u578b\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u4e3a\u521b\u9020\u529b\u652f\u6301\u7cfb\u7edf\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u539f\u5219\uff0c\u901a\u8fc7\u57fa\u4e8e\u89d2\u8272\u7684\u6307\u5bfc\u548c\u4e2a\u6027\u5316\u5b9e\u73b0\u63a2\u7d22\u4e0e\u6536\u655b\u4e4b\u95f4\u7684\u5e73\u8861\u3002\u8fd9\u4e9b\u89c1\u89e3\u63a8\u52a8\u4e86\u4eba\u673a\u534f\u4f5c\u5de5\u5177\u7684\u53d1\u5c55\uff0c\u65e8\u5728\u8f85\u52a9\u800c\u975e\u538b\u5236\u4eba\u7c7b\u521b\u9020\u529b\u3002"}}
{"id": "2510.26040", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26040", "abs": "https://arxiv.org/abs/2510.26040", "authors": ["Emily Steiner", "Daniel van der Spuy", "Futian Zhou", "Afereti Pama", "Minas Liarokapis", "Henry Williams"], "title": "Accelerating Real-World Overtaking in F1TENTH Racing Employing Reinforcement Learning Methods", "comment": null, "summary": "While autonomous racing performance in Time-Trial scenarios has seen\nsignificant progress and development, autonomous wheel-to-wheel racing and\novertaking are still severely limited. These limitations are particularly\napparent in real-life driving scenarios where state-of-the-art algorithms\nstruggle to safely or reliably complete overtaking manoeuvres. This is\nimportant, as reliable navigation around other vehicles is vital for safe\nautonomous wheel-to-wheel racing. The F1Tenth Competition provides a useful\nopportunity for developing wheel-to-wheel racing algorithms on a standardised\nphysical platform. The competition format makes it possible to evaluate\novertaking and wheel-to-wheel racing algorithms against the state-of-the-art.\nThis research presents a novel racing and overtaking agent capable of learning\nto reliably navigate a track and overtake opponents in both simulation and\nreality. The agent was deployed on an F1Tenth vehicle and competed against\nopponents running varying competitive algorithms in the real world. The results\ndemonstrate that the agent's training against opponents enables deliberate\novertaking behaviours with an overtaking rate of 87% compared 56% for an agent\ntrained just to race.", "AI": {"tldr": "\u81ea\u4e3b\u8d5b\u8f66\u5728\u8d85\u8f66\u80fd\u529b\u65b9\u9762\u4ecd\u5b58\u5728\u5c40\u9650\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u667a\u80fd\u4f53\uff0c\u80fd\u591f\u63d0\u9ad8\u8d85\u8f66\u6210\u529f\u7387\u3002", "motivation": "\u63a2\u7d22\u63d0\u9ad8\u81ea\u4e3b\u8d5b\u8f66\u5728\u771f\u5b9e\u9a7e\u9a76\u573a\u666f\u4e2d\u8d85\u8f66\u80fd\u529b\u7684\u5fc5\u8981\u6027\uff0c\u7279\u522b\u662f\u5728\u8f6e\u5bf9\u8f6e\u8d5b\u8f66\u4e2d\u3002", "method": "\u5728F1Tenth\u7ade\u8d5b\u4e2d\u90e8\u7f72\u667a\u80fd\u4f53\uff0c\u9488\u5bf9\u4e0d\u540c\u5bf9\u624b\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u5b66\u4e60\u5e76\u6267\u884c\u6709\u6548\u7684\u8d85\u8f66\u7b56\u7565\u3002", "result": "\u5f00\u53d1\u51fa\u4e00\u79cd\u65b0\u578b\u7684\u8d5b\u8f66\u548c\u8d85\u8f66\u667a\u80fd\u4f53\uff0c\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u4e2d\u90fd\u80fd\u53ef\u9760\u5730\u5b8c\u6210\u8d85\u8f66\u3002", "conclusion": "\u8be5\u667a\u80fd\u4f53\u5728F1Tenth\u8f66\u8f86\u4e0a\u7ecf\u8fc7\u8bad\u7ec3\u540e\uff0c\u8d85\u8f66\u6210\u529f\u7387\u8fbe87%\uff0c\u663e\u793a\u51fa\u4e0e\u4f20\u7edf\u8bad\u7ec3\u65b9\u6cd5\u7684\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2510.26508", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.26508", "abs": "https://arxiv.org/abs/2510.26508", "authors": ["Clara Colombatto", "Sean Rintel", "Lev Tankelevitch"], "title": "Metacognition and Confidence Dynamics in Advice Taking from Generative AI", "comment": null, "summary": "Generative Artificial Intelligence (GenAI) can aid humans in a wide range of\ntasks, but its effectiveness critically depends on users being able to evaluate\nthe accuracy of GenAI outputs and their own expertise. Here we asked how\nconfidence in self and GenAI contributes to decisions to seek and rely on\nadvice from GenAI ('prospective confidence'), and how advice-taking in turn\nshapes this confidence ('retrospective confidence'). In a novel paradigm\ninvolving text generation, participants formulated plans for events, and could\nrequest advice from a GenAI (Study 1; N=200) or were randomly assigned to\nreceive advice (Study 2; N=300), which they could rely on or ignore. Advice\nrequests in Study 1 were related to higher prospective confidence in GenAI and\nlower confidence in self. Advice-seekers showed increased retrospective\nconfidence in GenAI, while those who declined advice showed increased\nconfidence in self. Random assignment in Study 2 revealed that advice exposure\nincreases confidence in GenAI and in self, suggesting that GenAI advice-taking\ncausally boosts retrospective confidence. These results were mirrored in advice\nreliance, operationalised as the textual similarity between GenAI advice and\nparticipants' responses, with reliance associated with increased retrospective\nconfidence in both GenAI and self. Critically, participants who chose to\nobtain/rely on advice provided more detailed responses (likely due to the\noutput's verbosity), but failed to check the output thoroughly, missing key\ninformation. These findings underscore a key role for confidence in\ninteractions with GenAI, shaped by both prior beliefs about oneself and the\nreliability of AI, and context-dependent exposure to advice.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u81ea\u4fe1\u5fc3\u4e0e\u751f\u6210AI (GenAI) \u8f93\u51fa\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u81ea\u4fe1\u548c\u5bf9GenAI\u7684\u4f9d\u8d56\u5f71\u54cd\u4e86\u7528\u6237\u7684\u51b3\u7b56\u4e0e\u4fe1\u5fc3\u3002", "motivation": "\u7406\u89e3\u4fe1\u5fc3\u5982\u4f55\u5f71\u54cd\u7528\u6237\u5728\u4e0e\u751f\u6210AI\u4e92\u52a8\u65f6\u7684\u51b3\u7b56\u4e0e\u5efa\u8bae\u7684\u4f9d\u8d56\u6027\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u4e24\u4e2a\u5b9e\u9a8c\u63a2\u8ba8\u7528\u6237\u5728\u5bfb\u6c42\u548c\u4f9d\u8d56GenAI\u5efa\u8bae\u8fc7\u7a0b\u4e2d\u7684\u81ea\u4fe1\u53d8\u5316\uff0c\u4ee5\u53ca\u5efa\u8bae\u5bf9\u81ea\u4fe1\u7684\u5f71\u54cd\u3002", "result": "\u5bfb\u6c42\u548c\u4f9d\u8d56GenAI\u5efa\u8bae\u4f7f\u7528\u6237\u5bf9\u81ea\u8eab\u548cGenAI\u7684\u4fe1\u5fc3\u589e\u52a0\uff0c\u4f46\u6709\u65f6\u4f1a\u5ffd\u89c6\u5173\u952e\u4fe1\u606f\u3002", "conclusion": "\u7528\u6237\u5bf9\u81ea\u8eab\u548cGenAI\u7684\u81ea\u4fe1\u5728\u4e92\u52a8\u4e2d\u8d77\u7740\u91cd\u8981\u4f5c\u7528\uff0c\u4e14\u5efa\u8bae\u7684\u4f9d\u8d56\u6027\u4f1a\u589e\u5f3a\u5bf9GenAI\u548c\u81ea\u8eab\u7684\u81ea\u4fe1\u3002"}}
{"id": "2510.26067", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.26067", "abs": "https://arxiv.org/abs/2510.26067", "authors": ["Chi Zhang", "Mingrui Li", "Wenzhe Tong", "Xiaonan Huang"], "title": "Morphology-Aware Graph Reinforcement Learning for Tensegrity Robot Locomotion", "comment": null, "summary": "Tensegrity robots combine rigid rods and elastic cables, offering high\nresilience and deployability but posing major challenges for locomotion control\ndue to their underactuated and highly coupled dynamics. This paper introduces a\nmorphology-aware reinforcement learning framework that integrates a graph\nneural network (GNN) into the Soft Actor-Critic (SAC) algorithm. By\nrepresenting the robot's physical topology as a graph, the proposed GNN-based\npolicy captures coupling among components, enabling faster and more stable\nlearning than conventional multilayer perceptron (MLP) policies. The method is\nvalidated on a physical 3-bar tensegrity robot across three locomotion\nprimitives, including straight-line tracking and bidirectional turning. It\nshows superior sample efficiency, robustness to noise and stiffness variations,\nand improved trajectory accuracy. Notably, the learned policies transfer\ndirectly from simulation to hardware without fine-tuning, achieving stable\nreal-world locomotion. These results demonstrate the advantages of\nincorporating structural priors into reinforcement learning for tensegrity\nrobot control.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5f20\u7d27\u7ed3\u6784\u673a\u5668\u4eba\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u63a7\u5236\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5f20\u7d27\u7ed3\u6784\u673a\u5668\u4eba\u5728\u8fd0\u52a8\u63a7\u5236\u4e2d\u7684\u6311\u6218\uff0c\u6539\u5584\u5176\u5728\u52a8\u6001\u4e0b\u7684\u5b66\u4e60\u80fd\u529b\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u5f62\u6001\u611f\u77e5\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5728Soft Actor-Critic (SAC)\u7b97\u6cd5\u4e2d\u6574\u5408\u56fe\u795e\u7ecf\u7f51\u7edc (GNN)\u3002", "result": "\u5728\u4e09\u79cd\u8fd0\u52a8\u539f\u578b\u4e0a\uff08\u5305\u62ec\u76f4\u7ebf\u8ddf\u8e2a\u548c\u53cc\u5411\u8f6c\u5411\uff09\u9a8c\u8bc1\u4e86\u65b9\u6cd5\uff0c\u663e\u793a\u51fa\u4f18\u8d8a\u7684\u6837\u672c\u6548\u7387\u3001\u5bf9\u566a\u58f0\u548c\u521a\u5ea6\u53d8\u5316\u7684\u9c81\u68d2\u6027\uff0c\u4ee5\u53ca\u6539\u5584\u7684\u8f68\u8ff9\u51c6\u786e\u6027\u3002", "conclusion": "\u7ed3\u5408\u7ed3\u6784\u5148\u9a8c\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5f20\u7d27\u7ed3\u6784\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u4e14\u80fd\u591f\u76f4\u63a5\u4ece\u4eff\u771f\u8fc1\u79fb\u5230\u786c\u4ef6\u3002"}}
{"id": "2510.26080", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.26080", "abs": "https://arxiv.org/abs/2510.26080", "authors": ["Fan Yang", "Renkai Ma", "Yaxin Hu", "Michael Rodgers", "Lingyao Li"], "title": "I don't Want You to Die: A Shared Responsibility Framework for Safeguarding Child-Robot Companionship", "comment": null, "summary": "Social robots like Moxie are designed to form strong emotional bonds with\nchildren, but their abrupt discontinuation can cause significant struggles and\ndistress to children. When these services end, the resulting harm raises\ncomplex questions of who bears responsibility when children's emotional bonds\nare broken. Using the Moxie shutdown as a case study through a qualitative\nsurvey of 72 U.S. participants, our findings show that the responsibility is\nviewed as a shared duty across the robot company, parents, developers, and\ngovernment. However, these attributions varied by political ideology and\nparental status of whether they have children. Participants' perceptions of\nwhether the robot service should continue are highly polarized; supporters\npropose technical, financial, and governmental pathways for continuity, while\nopponents cite business realities and risks of unhealthy emotional dependency.\nUltimately, this research contributes an empirically grounded shared\nresponsibility framework for safeguarding child-robot companionship by\ndetailing how accountability is distributed and contested, informing concrete\ndesign and policy implications to mitigate the emotional harm of robot\ndiscontinuation.", "AI": {"tldr": "\u793e\u4ea4\u673a\u5668\u4eba\u7684\u505c\u7528\u53ef\u80fd\u5bf9\u513f\u7ae5\u9020\u6210\u60c5\u611f\u56f0\u6270\uff0c\u8d23\u4efb\u5206\u914d\u5b58\u5728\u4e89\u8bae\uff0c\u5efa\u8bae\u5efa\u7acb\u8d23\u4efb\u6846\u67b6\u4ee5\u51cf\u8f7b\u5f71\u54cd\u3002", "motivation": "\u63a2\u8ba8\u793e\u4ea4\u673a\u5668\u4eba\u4e0e\u513f\u7ae5\u4e4b\u95f4\u7684\u60c5\u611f\u7ebd\u5e26\u5728\u7a81\u7136\u505c\u7528\u540e\u9020\u6210\u7684\u60c5\u611f\u56f0\u6270\u548c\u8d23\u4efb\u5f52\u5c5e\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5bf972\u4f4d\u7f8e\u56fd\u53c2\u4e0e\u8005\u7684\u5b9a\u6027\u8c03\u67e5\uff0c\u5206\u6790\u4ed6\u4eec\u5bf9\u673a\u5668\u4eba\u670d\u52a1\u505c\u7528\u7684\u770b\u6cd5\u53ca\u8d23\u4efb\u5f52\u5c5e\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u8d23\u4efb\u88ab\u89c6\u4e3a\u673a\u5668\u4eba\u516c\u53f8\u3001\u5bb6\u957f\u3001\u5f00\u53d1\u8005\u548c\u653f\u5e9c\u4e4b\u95f4\u7684\u5171\u540c\u8d23\u4efb\uff0c\u4f46\u8fd9\u79cd\u770b\u6cd5\u53d7\u653f\u6cbb\u610f\u8bc6\u5f62\u6001\u548c\u5bb6\u957f\u8eab\u4efd\u7684\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u513f\u7ae5\u4e0e\u673a\u5668\u4eba\u4f34\u4fa3\u5173\u7cfb\u7684\u8d23\u4efb\u6846\u67b6\u63d0\u4f9b\u5b9e\u8bc1\u652f\u6301\uff0c\u5e76\u8ba8\u8bba\u4e86\u5982\u4f55\u901a\u8fc7\u8bbe\u8ba1\u548c\u653f\u7b56\u5e94\u5bf9\u60c5\u611f\u4f24\u5bb3\u3002"}}
{"id": "2510.26082", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.26082", "abs": "https://arxiv.org/abs/2510.26082", "authors": ["Fan Yang", "Lingyao Li", "Yaxin Hu", "Michael Rodgers", "Renkai Ma"], "title": "Beyond the Uncanny Valley: A Mixed-Method Investigation of Anthropomorphism in Protective Responses to Robot Abuse", "comment": null, "summary": "Robots with anthropomorphic features are increasingly shaping how humans\nperceive and morally engage with them. Our research investigates how different\nlevels of anthropomorphism influence protective responses to robot abuse,\nextending the Computers as Social Actors (CASA) and uncanny valley theories\ninto a moral domain. In an experiment, we invite 201 participants to view\nvideos depicting abuse toward a robot with low (Spider), moderate (Two-Foot),\nor high (Humanoid) anthropomorphism. To provide a comprehensive analysis, we\ntriangulate three modalities: self-report surveys measuring emotions and\nuncanniness, physiological data from automated facial expression analysis, and\nqualitative reflections. Findings indicate that protective responses are not\nlinear. The moderately anthropomorphic Two-Foot robot, rated highest in\neeriness and \"spine-tingling\" sensations consistent with the uncanny valley,\nelicited the strongest physiological anger expressions. Self-reported anger and\nguilt are significantly higher for both the Two-Foot and Humanoid robots\ncompared to the Spider. Qualitative findings further reveal that as\nanthropomorphism increases, moral reasoning shifts from technical assessments\nof property damage to condemnation of the abuser's character, while governance\nproposals expand from property law to calls for quasi-animal rights and broader\nsocietal responsibility. These results suggest that the uncanny valley does not\ndampen moral concern but paradoxically heightens protective impulses, offering\ncritical implications for robot design, policy, and future legal frameworks.", "AI": {"tldr": "\u4e0d\u540c\u4eba\u5f62\u7279\u5f81\u7684\u673a\u5668\u4eba\u5982\u4f55\u5f71\u54cd\u4eba\u7c7b\u5bf9\u5176\u8650\u5f85\u7684\u4fdd\u62a4\u53cd\u5e94\uff0c\u53d1\u73b0\u4e2d\u7b49\u4eba\u5f62\u673a\u5668\u4eba\u5f15\u53d1\u6700\u5f3a\u70c8\u7684\u9053\u5fb7\u5173\u5207\u4e0e\u6124\u6012\u3002", "motivation": "\u63a2\u8ba8\u4eba\u5f62\u7279\u5f81\u5982\u4f55\u5f71\u54cd\u4eba\u7c7b\u5bf9\u673a\u5668\u4eba\u8650\u5f85\u7684\u9053\u5fb7\u53cd\u5e94\uff0c\u5ef6\u4f38CASA\u548c\u51b7\u6f20\u8c37\u7406\u8bba\u3002", "method": "\u5b9e\u9a8c\u7814\u7a76\uff0c\u4ee5\u89c6\u9891\u5c55\u793a\u4e0d\u540c\u4eba\u5f62\u7279\u5f81\u7684\u673a\u5668\u4eba\u906d\u53d7\u8650\u5f85\uff0c\u5206\u6790\u53c2\u4e0e\u8005\u7684\u4fdd\u62a4\u53cd\u5e94\u3002", "result": "\u53d1\u73b0\u4fdd\u62a4\u53cd\u5e94\u5e76\u975e\u7ebf\u6027\uff0c\u5177\u6709\u4e2d\u7b49\u4eba\u5f62\u7279\u5f81\u7684\u673a\u5668\u4eba\u5f15\u53d1\u5f3a\u70c8\u7684\u751f\u7406\u6124\u6012\u8868\u73b0\uff0c\u5e76\u4e14\u9053\u5fb7\u63a8\u7406\u968f\u7740\u4eba\u5f62\u7279\u5f81\u7684\u589e\u52a0\u800c\u8f6c\u53d8\u3002", "conclusion": "\u51b7\u6f20\u8c37\u6548\u679c\u5e76\u672a\u51cf\u5f31\u9053\u5fb7\u5173\u5207\uff0c\u53cd\u800c\u4fc3\u4f7f\u66f4\u5f3a\u70c8\u7684\u4fdd\u62a4\u51b2\u52a8\uff0c\u4e3a\u673a\u5668\u4eba\u8bbe\u8ba1\u3001\u653f\u7b56\u53ca\u6cd5\u5f8b\u6846\u67b6\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2510.26132", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.26132", "abs": "https://arxiv.org/abs/2510.26132", "authors": ["Nestor O. Perez-Arancibia"], "title": "Embodied Intelligence for Advanced Bioinspired Microrobotics: Examples and Insights", "comment": "8 pages, 7 figures, accepted to ICAR 2025", "summary": "The term embodied intelligence (EI) conveys the notion that body morphology,\nmaterial properties, interaction with the environment, and control strategies\ncan be purposefully integrated into the process of robotic design to generate\nintelligent behavior; in particular, locomotion and navigation. In this paper,\nwe discuss EI as a design principle for advanced microrobotics, with a\nparticular focus on co-design -- the simultaneous and interdependent\ndevelopment of physical structure and behavioral function. To illustrate the\ncontrast between EI-inspired systems and traditional architectures that\ndecouple sensing, computation, and actuation, we present and discuss a\ncollection of robots developed by the author and his team at the Autonomous\nMicrorobotic Systems Laboratory (AMSL). These robots exhibit intelligent\nbehavior that emerges from their structural dynamics and the physical\ninteraction between their components and with the environment. Platforms such\nas the Bee++, RoBeetle, SMALLBug, SMARTI, WaterStrider, VLEIBot+, and FRISSHBot\nexemplify how feedback loops, decision logics, sensing mechanisms, and smart\nactuation strategies can be embedded into the physical properties of the\nrobotic system itself. Along these lines, we contend that co-design is not only\na method for empirical optimization under constraints, but also an enabler of\nEI, offering a scalable and robust alternative to classical control for\nrobotics at the mm-to-cm-scale.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5728\u5fae\u578b\u673a\u5668\u4eba\u8bbe\u8ba1\u4e2d\u878d\u5165\u4f53\u73b0\u667a\u80fd\uff08EI\uff09\u539f\u5219\uff0c\u5f3a\u8c03\u5171\u8bbe\u8ba1\u7684\u5fc5\u8981\u6027\u53ca\u5176\u5728\u5fae\u578b\u673a\u5668\u4eba\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002", "motivation": "\u63a2\u8ba8\u8eab\u4f53\u5f62\u6001\u3001\u6750\u6599\u7279\u6027\u3001\u73af\u5883\u4ea4\u4e92\u548c\u63a7\u5236\u7b56\u7565\u5982\u4f55\u878d\u5165\u673a\u5668\u4eba\u8bbe\u8ba1\u8fc7\u7a0b\u4e2d\uff0c\u4ee5\u751f\u6210\u667a\u80fd\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5728\u8fd0\u52a8\u548c\u5bfc\u822a\u65b9\u9762\u3002", "method": "\u901a\u8fc7\u5c55\u793a\u4f5c\u8005\u53ca\u5176\u56e2\u961f\u5728\u81ea\u4e3b\u5fae\u578b\u673a\u5668\u4eba\u5b9e\u9a8c\u5ba4\u5f00\u53d1\u7684\u591a\u4e2a\u673a\u5668\u4eba\uff0c\u9610\u660e\u4e86EI\u4f5c\u4e3a\u8bbe\u8ba1\u539f\u5219\u7684\u5e94\u7528\u3002", "result": "\u5c55\u793a\u4e86\u4e00\u7cfb\u5217\u673a\u5668\u4eba\uff08\u5982Bee++\u3001RoBeetle\u7b49\uff09\uff0c\u8fd9\u4e9b\u673a\u5668\u4eba\u8868\u73b0\u51fa\u7684\u667a\u80fd\u884c\u4e3a\u6e90\u4e8e\u5176\u7ed3\u6784\u52a8\u6001\u4ee5\u53ca\u7ec4\u4ef6\u4e0e\u73af\u5883\u4e4b\u95f4\u7684\u7269\u7406\u4ea4\u4e92\u3002", "conclusion": "\u5171\u8bbe\u8ba1\u4e0d\u4ec5\u662f\u7ea6\u675f\u4e0b\u7684\u5b9e\u8bc1\u4f18\u5316\u65b9\u6cd5\uff0c\u8fd8\u662f\u4fc3\u8fdb\u4f53\u73b0\u667a\u80fd\u7684\u65b9\u5f0f\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u3001\u7a33\u5065\u7684\u66ff\u4ee3\u7ecf\u5178\u63a7\u5236\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u6beb\u7c73\u5230\u5398\u7c73\u7ea7\u7684\u673a\u5668\u4eba\u3002"}}
{"id": "2510.26139", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.26139", "abs": "https://arxiv.org/abs/2510.26139", "authors": ["Minseo Kwon", "Young J. Kim"], "title": "Kinodynamic Task and Motion Planning using VLM-guided and Interleaved Sampling", "comment": null, "summary": "Task and Motion Planning (TAMP) integrates high-level task planning with\nlow-level motion feasibility, but existing methods are costly in long-horizon\nproblems due to excessive motion sampling. While LLMs provide commonsense\npriors, they lack 3D spatial reasoning and cannot ensure geometric or dynamic\nfeasibility. We propose a kinodynamic TAMP framework based on a hybrid state\ntree that uniformly represents symbolic and numeric states during planning,\nenabling task and motion decisions to be jointly decided. Kinodynamic\nconstraints embedded in the TAMP problem are verified by an off-the-shelf\nmotion planner and physics simulator, and a VLM guides exploring a TAMP\nsolution and backtracks the search based on visual rendering of the states.\nExperiments on the simulated domains and in the real world show 32.14% -\n1166.67% increased average success rates compared to traditional and LLM-based\nTAMP planners and reduced planning time on complex problems, with ablations\nfurther highlighting the benefits of VLM guidance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u72b6\u6001\u6811\u7684TAMP\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u7b26\u53f7\u548c\u6570\u503c\u72b6\u6001\uff0c\u63d0\u9ad8\u4e86\u957f\u65f6\u95f4\u8de8\u5ea6\u95ee\u9898\u7684\u6210\u529f\u7387\u548c\u964d\u4f4e\u4e86\u89c4\u5212\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u7684\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u5728\u957f\u65f6\u95f4\u8de8\u5ea6\u95ee\u9898\u4e0a\u6210\u672c\u9ad8\u6602\uff0c\u4e3b\u8981\u662f\u7531\u4e8e\u8fc7\u591a\u7684\u8fd0\u52a8\u91c7\u6837\uff0c\u800c\u5f53\u524d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7f3a\u4e4f\u4e09\u7ef4\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u6cd5\u786e\u4fdd\u51e0\u4f55\u6216\u52a8\u6001\u53ef\u884c\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u72b6\u6001\u6811\u7684\u8fd0\u52a8\u4e0e\u4efb\u52a1\u89c4\u5212\u6846\u67b6\uff08TAMP\uff09\uff0c\u6574\u5408\u4e86\u7b26\u53f7\u4e0e\u6570\u503c\u72b6\u6001\uff0c\u5728\u89c4\u5212\u8fc7\u7a0b\u4e2d\u540c\u65f6\u8fdb\u884c\u4efb\u52a1\u548c\u8fd0\u52a8\u51b3\u7b56\u3002", "result": "\u76f8\u6bd4\u4e8e\u4f20\u7edf\u548c\u57fa\u4e8eLLM\u7684TAMP\u89c4\u5212\u5668\uff0c\u8be5\u6846\u67b6\u5728\u6a21\u62df\u57df\u548c\u73b0\u5b9e\u4e16\u754c\u7684\u5b9e\u9a8c\u4e2d\u663e\u793a\u51fa32.14%-1166.67%\u7684\u5e73\u5747\u6210\u529f\u7387\u63d0\u5347\uff0c\u5e76\u4e14\u5728\u590d\u6742\u95ee\u9898\u4e0a\u7684\u89c4\u5212\u65f6\u95f4\u51cf\u5c11\uff0c\u6d88\u878d\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u7a81\u51fa\u4e86VLM\u6307\u5bfc\u7684\u76ca\u5904\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5c06\u8fd0\u52a8\u89c4\u5212\u5668\u4e0e\u7269\u7406\u6a21\u62df\u5668\u76f8\u7ed3\u5408\uff0c\u9a8c\u8bc1\u4e86\u8fd0\u52a8\u7684\u52a8\u529b\u5b66\u7ea6\u675f\uff0c\u5e76\u901a\u8fc7\u53ef\u89c6\u5316\u72b6\u6001\u5f15\u5bfcTAMP\u89e3\u51b3\u65b9\u6848\u7684\u63a2\u7d22\u548c\u56de\u6eaf\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\u7684\u6548\u7387\u548c\u6548\u679c\u3002"}}
{"id": "2510.26142", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.26142", "abs": "https://arxiv.org/abs/2510.26142", "authors": ["Hahjin Lee", "Young J. Kim"], "title": "Adaptive Trajectory Refinement for Optimization-based Local Planning in Narrow Passages", "comment": null, "summary": "Trajectory planning for mobile robots in cluttered environments remains a\nmajor challenge due to narrow passages, where conventional methods often fail\nor generate suboptimal paths. To address this issue, we propose the adaptive\ntrajectory refinement algorithm, which consists of two main stages. First, to\nensure safety at the path-segment level, a segment-wise conservative collision\ntest is applied, where risk-prone trajectory path segments are recursively\nsubdivided until collision risks are eliminated. Second, to guarantee\npose-level safety, pose correction based on penetration direction and line\nsearch is applied, ensuring that each pose in the trajectory is collision-free\nand maximally clear from obstacles. Simulation results demonstrate that the\nproposed method achieves up to 1.69x higher success rates and up to 3.79x\nfaster planning times than state-of-the-art approaches. Furthermore, real-world\nexperiments confirm that the robot can safely pass through narrow passages\nwhile maintaining rapid planning performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u5e94\u6027\u8f68\u8ff9\u4f18\u5316\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u79fb\u52a8\u673a\u5668\u4eba\u5728\u72ed\u7a84\u901a\u9053\u4e2d\u7684\u89c4\u5212\u6210\u529f\u7387\u548c\u901f\u5ea6\u3002", "motivation": "\u5728\u62e5\u6324\u73af\u5883\u4e2d\u89c4\u5212\u8f68\u8ff9\u4ecd\u7136\u662f\u4e00\u5927\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u5e38\u5e38\u5931\u6548\u6216\u751f\u6210\u6b21\u4f18\u8def\u5f84\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5\u4ee5\u63d0\u9ad8\u901a\u8fc7\u72ed\u7a84\u901a\u9053\u7684\u6210\u529f\u7387\u548c\u6548\u7387\u3002", "method": "\u7b97\u6cd5\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u7ec6\u5206\u8def\u5f84\u6bb5\u6267\u884c\u4fdd\u5b88\u7684\u78b0\u649e\u6d4b\u8bd5\uff0c\u6d88\u9664\u78b0\u649e\u98ce\u9669\uff1b\u7b2c\u4e8c\u9636\u6bb5\u57fa\u4e8e\u6e17\u900f\u65b9\u5411\u548c\u7ebf\u641c\u7d22\u8fdb\u884c\u59ff\u6001\u4fee\u6b63\uff0c\u786e\u4fdd\u6240\u6709\u59ff\u6001\u5747\u4e3a\u65e0\u78b0\u649e\u72b6\u6001\u3002", "result": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u5e94\u6027\u8f68\u8ff9\u4f18\u5316\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u62e5\u6324\u73af\u5883\u4e2d\u4e3a\u79fb\u52a8\u673a\u5668\u4eba\u89c4\u5212\u8f68\u8ff9\uff0c\u5c24\u5176\u662f\u901a\u8fc7\u72ed\u7a84\u901a\u9053\u7684\u6311\u6218\u3002\u901a\u8fc7\u4e24\u4e2a\u4e3b\u8981\u9636\u6bb5\uff0c\u786e\u4fdd\u5728\u8def\u5f84\u6bb5\u548c\u59ff\u6001\u6c34\u5e73\u4e0a\u7684\u5b89\u5168\u6027\uff0c\u7b97\u6cd5\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u5b89\u5168\u901a\u8fc7\u72ed\u7a84\u7684\u901a\u9053\uff0c\u540c\u65f6\u4fdd\u6301\u5feb\u901f\u7684\u89c4\u5212\u6027\u80fd\u3002"}}
{"id": "2510.26170", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26170", "abs": "https://arxiv.org/abs/2510.26170", "authors": ["Satoshi Kikuch", "Masaya Kato", "Tsuyoshi Tasaki"], "title": "Self-localization on a 3D map by fusing global and local features from a monocular camera", "comment": null, "summary": "Self-localization on a 3D map by using an inexpensive monocular camera is\nrequired to realize autonomous driving. Self-localization based on a camera\noften uses a convolutional neural network (CNN) that can extract local features\nthat are calculated by nearby pixels. However, when dynamic obstacles, such as\npeople, are present, CNN does not work well. This study proposes a new method\ncombining CNN with Vision Transformer, which excels at extracting global\nfeatures that show the relationship of patches on whole image. Experimental\nresults showed that, compared to the state-of-the-art method (SOTA), the\naccuracy improvement rate in a CG dataset with dynamic obstacles is 1.5 times\nhigher than that without dynamic obstacles. Moreover, the self-localization\nerror of our method is 20.1% smaller than that of SOTA on public datasets.\nAdditionally, our robot using our method can localize itself with 7.51cm error\non average, which is more accurate than SOTA.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u7ed3\u5408CNN\u4e0e\u89c6\u89c9\u53d8\u6362\u5668\u7684\u65b0\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u5728\u52a8\u6001\u969c\u788d\u7269\u73af\u5883\u4e0b\u7684\u81ea\u6211\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u5728\u5b9e\u73b0\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5229\u7528\u4fbf\u5b9c\u7684\u5355\u76ee\u6444\u50cf\u5934\u8fdb\u884c3D\u5730\u56fe\u81ea\u6211\u5b9a\u4f4d\u662f\u5fc5\u8981\u7684\uff0c\u7136\u800c\u73b0\u6709\u7684\u57fa\u4e8eCNN\u7684\u65b9\u6cd5\u5728\u52a8\u6001\u969c\u788d\u7269\u5b58\u5728\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u89c6\u89c9\u53d8\u6362\u5668\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u4ee5\u63d0\u53d6\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\uff0c\u4ece\u800c\u63d0\u5347\u81ea\u6211\u5b9a\u4f4d\u6027\u80fd\u3002", "result": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408CNN\u548c\u89c6\u89c9\u53d8\u6362\u5668\uff08Vision Transformer\uff09\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u6709\u52a8\u6001\u969c\u788d\u7269\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u81ea\u6211\u5b9a\u4f4d\u7684\u7cbe\u5ea6\u3002", "conclusion": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\uff0c\u65b0\u65b9\u6cd5\u5728\u52a8\u6001\u969c\u788d\u7269\u73af\u5883\u4e0b\u7684\u51c6\u786e\u7387\u63d0\u5347\u8fbe1.5\u500d\uff0c\u4e14\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u81ea\u6211\u5b9a\u4f4d\u8bef\u5dee\u6bd4SOTA\u5c0f20.1%\u3002"}}
{"id": "2510.26236", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.26236", "abs": "https://arxiv.org/abs/2510.26236", "authors": ["Kyungmin Lee", "Sibeen Kim", "Minho Park", "Hyunseung Kim", "Dongyoon Hwang", "Hojoon Lee", "Jaegul Choo"], "title": "PHUMA: Physically-Grounded Humanoid Locomotion Dataset", "comment": null, "summary": "Motion imitation is a promising approach for humanoid locomotion, enabling\nagents to acquire humanlike behaviors. Existing methods typically rely on\nhigh-quality motion capture datasets such as AMASS, but these are scarce and\nexpensive, limiting scalability and diversity. Recent studies attempt to scale\ndata collection by converting large-scale internet videos, exemplified by\nHumanoid-X. However, they often introduce physical artifacts such as floating,\npenetration, and foot skating, which hinder stable imitation. In response, we\nintroduce PHUMA, a Physically-grounded HUMAnoid locomotion dataset that\nleverages human video at scale, while addressing physical artifacts through\ncareful data curation and physics-constrained retargeting. PHUMA enforces joint\nlimits, ensures ground contact, and eliminates foot skating, producing motions\nthat are both large-scale and physically reliable. We evaluated PHUMA in two\nsets of conditions: (i) imitation of unseen motion from self-recorded test\nvideos and (ii) path following with pelvis-only guidance. In both cases,\nPHUMA-trained policies outperform Humanoid-X and AMASS, achieving significant\ngains in imitating diverse motions. The code is available at\nhttps://davian-robotics.github.io/PHUMA.", "AI": {"tldr": "PHUMA\u662f\u4e00\u4e2a\u65b0\u7684\u4eba\u5f62 locomotion \u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5904\u7406\u7269\u7406\u4f2a\u5f71\uff0c\u63d0\u9ad8\u4e86\u8fd0\u52a8\u6a21\u4eff\u7684\u53ef\u9760\u6027\u4e0e\u591a\u6837\u6027\u3002", "motivation": "\u5f00\u53d1\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u6a21\u4eff\u80fd\u529b\uff0c\u5b9e\u73b0\u7c7b\u4eba\u884c\u4e3a\uff0c\u4f46\u73b0\u6709\u7684\u6570\u636e\u96c6\u7a00\u7f3a\u4e14\u6602\u8d35\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u591a\u6837\u6027\u3002", "method": "PHUMA\u5229\u7528\u4eba\u7c7b\u89c6\u9891\u8fdb\u884c\u6570\u636e\u6536\u96c6\uff0c\u901a\u8fc7\u4e25\u683c\u7684\u6570\u636e\u7b5b\u9009\u548c\u7269\u7406\u7ea6\u675f\u7684\u91cd\u5b9a\u5411\u6765\u6d88\u9664\u4f2a\u5f71\uff0c\u786e\u4fdd\u5173\u8282\u9650\u5236\u548c\u5730\u9762\u63a5\u89e6\u3002", "result": "PHUMA\u6570\u636e\u96c6\u901a\u8fc7\u5927\u89c4\u6a21\u4eba\u7c7b\u89c6\u9891\uff0c\u514b\u670d\u4e86\u6570\u636e\u91c7\u96c6\u56f0\u96be\uff0c\u5e76\u6d88\u9664\u8fd0\u52a8\u4e2d\u7684\u7269\u7406\u4f2a\u5f71\u3002", "conclusion": "\u7ecf\u8fc7\u8bc4\u4f30\uff0cPHUMA\u8bad\u7ec3\u7684\u7b56\u7565\u5728\u8fd0\u52a8\u6a21\u4eff\u548c\u8def\u5f84\u8ddf\u968f\u4e2d\u5747\u4f18\u4e8eHumanoid-X\u548cAMASS\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u6837\u6027\u8fd0\u52a8\u7684\u6a21\u4eff\u80fd\u529b\u3002"}}
{"id": "2510.26280", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.26280", "abs": "https://arxiv.org/abs/2510.26280", "authors": ["Gangyang Li", "Qing Shi", "Youhao Hu", "Jincheng Hu", "Zhongyuan Wang", "Xinlong Wang", "Shaqi Luo"], "title": "Thor: Towards Human-Level Whole-Body Reactions for Intense Contact-Rich Environments", "comment": null, "summary": "Humanoids hold great potential for service, industrial, and rescue\napplications, in which robots must sustain whole-body stability while\nperforming intense, contact-rich interactions with the environment. However,\nenabling humanoids to generate human-like, adaptive responses under such\nconditions remains a major challenge. To address this, we propose Thor, a\nhumanoid framework for human-level whole-body reactions in contact-rich\nenvironments. Based on the robot's force analysis, we design a force-adaptive\ntorso-tilt (FAT2) reward function to encourage humanoids to exhibit human-like\nresponses during force-interaction tasks. To mitigate the high-dimensional\nchallenges of humanoid control, Thor introduces a reinforcement learning\narchitecture that decouples the upper body, waist, and lower body. Each\ncomponent shares global observations of the whole body and jointly updates its\nparameters. Finally, we deploy Thor on the Unitree G1, and it substantially\noutperforms baselines in force-interaction tasks. Specifically, the robot\nachieves a peak pulling force of 167.7 N (approximately 48% of the G1's body\nweight) when moving backward and 145.5 N when moving forward, representing\nimprovements of 68.9% and 74.7%, respectively, compared with the\nbest-performing baseline. Moreover, Thor is capable of pulling a loaded rack\n(130 N) and opening a fire door with one hand (60 N). These results highlight\nThor's effectiveness in enhancing humanoid force-interaction capabilities.", "AI": {"tldr": "Thor\u662f\u4e00\u4e2a\u65b0\u7684\u7c7b\u4eba\u673a\u5668\u4eba\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u529b\u81ea\u9002\u5e94\u5956\u52b1\u63d0\u5347\u673a\u5668\u4eba\u5728\u9ad8\u63a5\u89e6\u5f3a\u5ea6\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u529b\u91cf\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u7c7b\u4eba\u673a\u5668\u4eba\u5728\u4e0e\u73af\u5883\u8fdb\u884c\u5f3a\u63a5\u89e6\u4ea4\u4e92\u65f6\uff0c\u5982\u4f55\u4ea7\u751f\u7c7b\u4f3c\u4eba\u7c7b\u7684\u9002\u5e94\u6027\u53cd\u5e94\u8fd9\u4e00\u4e3b\u8981\u6311\u6218\u3002", "method": "\u63d0\u51faThor\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e86\u529b\u81ea\u9002\u5e94\u8eaf\u5e72\u503e\u659c\u5956\u52b1\u51fd\u6570\uff0c\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u67b6\u6784\u5b9e\u73b0\u4e0a\u4e0b\u534a\u8eab\u7684\u89e3\u8026\u63a7\u5236\u3002", "result": "Thor\u5728\u529b\u91cf\u4ea4\u4e92\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6700\u5927\u540e\u9000\u62c9\u52a8\u529b\u4e3a167.7 N\uff0c\u524d\u8fdb\u4e3a145.5 N\uff0c\u5206\u522b\u6bd4\u6700\u4f18\u57fa\u7ebf\u63d0\u534768.9%\u548c74.7%\u3002", "conclusion": "Thor\u663e\u8457\u63d0\u5347\u4e86\u7c7b\u4eba\u673a\u5668\u4eba\u5728\u63a5\u89e6\u4e30\u5bcc\u73af\u5883\u4e2d\u7684\u529b\u4ea4\u4e92\u80fd\u529b\u3002"}}
{"id": "2510.26358", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26358", "abs": "https://arxiv.org/abs/2510.26358", "authors": ["Mirko Usuelli", "David Rapado-Rincon", "Gert Kootstra", "Matteo Matteucci"], "title": "AgriGS-SLAM: Orchard Mapping Across Seasons via Multi-View Gaussian Splatting SLAM", "comment": null, "summary": "Autonomous robots in orchards require real-time 3D scene understanding\ndespite repetitive row geometry, seasonal appearance changes, and wind-driven\nfoliage motion. We present AgriGS-SLAM, a Visual--LiDAR SLAM framework that\ncouples direct LiDAR odometry and loop closures with multi-camera 3D Gaussian\nSplatting (3DGS) rendering. Batch rasterization across complementary viewpoints\nrecovers orchard structure under occlusions, while a unified gradient-driven\nmap lifecycle executed between keyframes preserves fine details and bounds\nmemory. Pose refinement is guided by a probabilistic LiDAR-based depth\nconsistency term, back-propagated through the camera projection to tighten\ngeometry-appearance coupling. We deploy the system on a field platform in apple\nand pear orchards across dormancy, flowering, and harvesting, using a\nstandardized trajectory protocol that evaluates both training-view and\nnovel-view synthesis to reduce 3DGS overfitting in evaluation. Across seasons\nand sites, AgriGS-SLAM delivers sharper, more stable reconstructions and\nsteadier trajectories than recent state-of-the-art 3DGS-SLAM baselines while\nmaintaining real-time performance on-tractor. While demonstrated in orchard\nmonitoring, the approach can be applied to other outdoor domains requiring\nrobust multimodal perception.", "AI": {"tldr": "AgriGS-SLAM\u662f\u4e00\u4e2a\u7ed3\u5408\u89c6\u89c9\u548c\u6fc0\u5149\u96f7\u8fbe\u7684SLAM\u6846\u67b6\uff0c\u80fd\u591f\u5728\u679c\u56ed\u4e2d\u5b9e\u73b0\u5b9e\u65f63D\u573a\u666f\u7406\u89e3\uff0c\u514b\u670d\u590d\u6742\u73af\u5883\u5e26\u6765\u7684\u6311\u6218\u3002", "motivation": "\u5b9e\u73b0\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u679c\u56ed\u4e2d\u8fdb\u884c\u5b9e\u65f63D\u573a\u666f\u7406\u89e3\uff0c\u4ee5\u5e94\u5bf9\u91cd\u590d\u7684\u884c\u51e0\u4f55\u3001\u5b63\u8282\u6027\u5916\u89c2\u53d8\u5316\u548c\u98ce\u9a71\u52a8\u7684\u53f6\u5b50\u8fd0\u52a8\u3002", "method": "\u7ed3\u5408\u76f4\u63a5LiDAR\u91cc\u7a0b\u8ba1\u548c\u5faa\u73af\u95ed\u5408\u4e0e\u591a\u6444\u50cf\u673a3D\u9ad8\u65af\u6e85\u5c04\u6e32\u67d3\uff0c\u4f7f\u7528\u7edf\u4e00\u7684\u68af\u5ea6\u9a71\u52a8\u5730\u56fe\u751f\u547d\u5468\u671f\u548c\u6982\u7387LiDAR\u6df1\u5ea6\u4e00\u81f4\u6027\u5f15\u5bfc\u4f4d\u59ff\u4f18\u5316\u3002", "result": "AgriGS-SLAM\u5728\u591a\u4e2a\u5b63\u8282\u548c\u5730\u70b9\u7684\u6d4b\u8bd5\u4e2d\uff0c\u91cd\u5efa\u7ed3\u679c\u66f4\u6e05\u6670\u3001\u66f4\u7a33\u5b9a\uff0c\u8f68\u8ff9\u66f4\u5e73\u7a33\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u76843DGS-SLAM\u57fa\u7ebf\u3002", "conclusion": "AgriGS-SLAM\u5728\u679c\u56ed\u76d1\u6d4b\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u6e05\u6670\u7a33\u5b9a\u7684\u91cd\u5efa\u548c\u8f68\u8ff9\uff0c\u4e14\u5b9e\u65f6\u6027\u80fd\u4f18\u826f\uff0c\u9002\u7528\u4e8e\u5176\u4ed6\u6237\u5916\u573a\u666f\u7684\u591a\u6a21\u6001\u611f\u77e5\u3002"}}
{"id": "2510.26362", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.26362", "abs": "https://arxiv.org/abs/2510.26362", "authors": ["Tobias L\u00f6w", "Cem Bilaloglu", "Sylvain Calinon"], "title": "Cooperative Task Spaces for Multi-Arm Manipulation Control based on Similarity Transformations", "comment": null, "summary": "Many tasks in human environments require collaborative behavior between\nmultiple kinematic chains, either to provide additional support for carrying\nbig and bulky objects or to enable the dexterity that is required for in-hand\nmanipulation. Since these complex systems often have a very high number of\ndegrees of freedom coordinating their movements is notoriously difficult to\nmodel. In this article, we present the derivation of the theoretical\nfoundations for cooperative task spaces of multi-arm robotic systems based on\ngeometric primitives defined using conformal geometric algebra. Based on the\nsimilarity transformations of these cooperative geometric primitives, we derive\nan abstraction of complex robotic systems that enables representing these\nsystems in a way that directly corresponds to single-arm systems. By deriving\nthe associated analytic and geometric Jacobian matrices, we then show the\nstraightforward integration of our approach into classical control techniques\nrooted in operational space control. We demonstrate this using bimanual\nmanipulators, humanoids and multi-fingered hands in optimal control experiments\nfor reaching desired geometric primitives and in teleoperation experiments\nusing differential kinematics control. We then discuss how the geometric\nprimitives naturally embed nullspace structures into the controllers that can\nbe exploited for introducing secondary control objectives. This work,\nrepresents the theoretical foundations of this cooperative manipulation control\nframework, and thus the experiments are presented in an abstract way, while\ngiving pointers towards potential future applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u4f9b\u4e86\u57fa\u4e8e\u5171\u5f62\u51e0\u4f55\u4ee3\u6570\u7684\u591a\u81c2\u673a\u5668\u4eba\u5408\u4f5c\u4efb\u52a1\u7a7a\u95f4\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u5176\u5728\u63a7\u5236\u6280\u672f\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u534f\u8c03\u591a\u4e2a\u8fd0\u52a8\u94fe\u7684\u8fd0\u52a8\u5bf9\u590d\u6742\u7cfb\u7edf\u5efa\u6a21\u5341\u5206\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u5b9e\u9645\u4eba\u673a\u73af\u5883\u4e2d\u7684\u5408\u4f5c\u884c\u4e3a\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5171\u5f62\u51e0\u4f55\u4ee3\u6570\u7684\u591a\u81c2\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5408\u4f5c\u4efb\u52a1\u7a7a\u95f4\u7406\u8bba\u57fa\u7840", "result": "\u901a\u8fc7\u63a8\u5bfc\u590d\u6742\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u62bd\u8c61\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u5176\u4e0e\u5355\u81c2\u7cfb\u7edf\u76f4\u63a5\u5bf9\u5e94\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5728\u4f20\u7edf\u63a7\u5236\u6280\u672f\u4e2d\u7684\u7b80\u5355\u96c6\u6210\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u5408\u4f5c\u63a7\u5236\u6846\u67b6\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u4e3a\u672a\u6765\u5e94\u7528\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.26363", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.26363", "abs": "https://arxiv.org/abs/2510.26363", "authors": ["Ilya Kurinov", "Miroslav Ivanov", "Grzegorz Orzechowski", "Aki Mikkola"], "title": "Towards Reinforcement Learning Based Log Loading Automation", "comment": null, "summary": "Forestry forwarders play a central role in mechanized timber harvesting by\npicking up and moving logs from the felling site to a processing area or a\nsecondary transport vehicle. Forwarder operation is challenging and physically\nand mentally exhausting for the operator who must control the machine in remote\nareas for prolonged periods of time. Therefore, even partial automation of the\nprocess may reduce stress on the operator. This study focuses on continuing\nprevious research efforts in application of reinforcement learning agents in\nautomating log handling process, extending the task from grasping which was\nstudied in previous research to full log loading operation. The resulting agent\nwill be capable to automate a full loading procedure from locating and\ngrappling to transporting and delivering the log to a forestry forwarder bed.\nTo train the agent, a trailer type forestry forwarder simulation model in\nNVIDIA's Isaac Gym and a virtual environment for a typical log loading scenario\nwere developed. With reinforcement learning agents and a curriculum learning\napproach, the trained agent may be a stepping stone towards application of\nreinforcement learning agents in automation of the forestry forwarder. The\nagent learnt grasping a log in a random position from grapple's random position\nand transport it to the bed with 94% success rate of the best performing agent.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u81ea\u52a8\u5316\u68ee\u6797\u524d\u79fb\u5668\u7684\u6728\u6750\u88c5\u8f7d\u8fc7\u7a0b\uff0c\u6784\u5efa\u4e86\u6a21\u62df\u73af\u5883\u5e76\u53d6\u5f97\u4e8694%\u7684\u6210\u529f\u7387\u3002", "motivation": "\u4e3a\u964d\u4f4e\u64cd\u4f5c\u5458\u5728\u504f\u8fdc\u5730\u533a\u957f\u65f6\u95f4\u64cd\u4f5c\u673a\u68b0\u7684\u538b\u529b\uff0c\u901a\u8fc7\u90e8\u5206\u81ea\u52a8\u5316\u63d0\u5347\u6728\u6750\u88c5\u8f7d\u8fc7\u7a0b\u7684\u6548\u7387\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u548c\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7NVIDIA Isaac Gym\u5f00\u53d1\u7684\u6a21\u62df\u6a21\u578b\u8fdb\u884c\u57f9\u8bad\u3002", "result": "\u7ecf\u8fc7\u8bad\u7ec3\uff0c\u4ee3\u7406\u5728\u968f\u673a\u4f4d\u7f6e\u6293\u53d6\u6728\u6750\u5e76\u5c06\u5176\u8fd0\u8f93\u5230\u524d\u5411\u8f7d\u4f53\u5e8a\u4e0a\uff0c\u8868\u73b0\u51fa94%\u7684\u6700\u4f73\u6210\u529f\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u80fd\u591f\u6709\u6548\u5730\u81ea\u52a8\u5316\u6728\u6750\u88c5\u8f7d\u8fc7\u7a0b\uff0c\u8fbe\u523094%\u7684\u6210\u529f\u7387\uff0c\u4ece\u800c\u51cf\u8f7b\u64cd\u4f5c\u5458\u7684\u5de5\u4f5c\u538b\u529b\u3002"}}
{"id": "2510.26406", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26406", "abs": "https://arxiv.org/abs/2510.26406", "authors": ["Guanxing Lu", "Rui Zhao", "Haitao Lin", "He Zhang", "Yansong Tang"], "title": "Human-in-the-loop Online Rejection Sampling for Robotic Manipulation", "comment": "8 pages", "summary": "Reinforcement learning (RL) is widely used to produce robust robotic\nmanipulation policies, but fine-tuning vision-language-action (VLA) models with\nRL can be unstable due to inaccurate value estimates and sparse supervision at\nintermediate steps. In contrast, imitation learning (IL) is easy to train but\noften underperforms due to its offline nature. In this paper, we propose\nHi-ORS, a simple yet effective post-training method that utilizes rejection\nsampling to achieve both training stability and high robustness. Hi-ORS\nstabilizes value estimation by filtering out negatively rewarded samples during\nonline fine-tuning, and adopts a reward-weighted supervised training objective\nto provide dense intermediate-step supervision. For systematic study, we\ndevelop an asynchronous inference-training framework that supports flexible\nonline human-in-the-loop corrections, which serve as explicit guidance for\nlearning error-recovery behaviors. Across three real-world tasks and two\nembodiments, Hi-ORS fine-tunes a pi-base policy to master contact-rich\nmanipulation in just 1.5 hours of real-world training, outperforming RL and IL\nbaselines by a substantial margin in both effectiveness and efficiency.\nNotably, the fine-tuned policy exhibits strong test-time scalability by\nreliably executing complex error-recovery behaviors to achieve better\nperformance.", "AI": {"tldr": "Hi-ORS \u662f\u4e00\u79cd\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u62d2\u7edd\u91c7\u6837\u63d0\u9ad8\u4e86\u5f3a\u5316\u5b66\u4e60\u7684\u7a33\u5b9a\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5728\u77ed\u65f6\u95f4\u5185\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6548\u679c\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u5f3a\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u4e2d\u95f4\u6b65\u9aa4\u9636\u6bb5\u9762\u4e34\u7684\u4ef7\u503c\u4f30\u8ba1\u4e0d\u51c6\u786e\u548c\u7a00\u758f\u76d1\u7763\u95ee\u9898\uff0c\u4ee5\u53ca\u6a21\u4eff\u5b66\u4e60\uff08IL\uff09\u7531\u4e8e\u79bb\u7ebf\u7279\u6027\u5bfc\u81f4\u7684\u8868\u73b0\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u62d2\u7edd\u91c7\u6837\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fc7\u6ee4\u8d1f\u5956\u52b1\u6837\u672c\u548c\u91c7\u7528\u52a0\u6743\u5956\u52b1\u7684\u76d1\u7763\u8bad\u7ec3\u76ee\u6807\u6765\u5b9e\u73b0\u8bad\u7ec3\u7a33\u5b9a\u6027\u4e0e\u9ad8\u5ea6\u9c81\u68d2\u6027\u3002", "result": "\u5728\u4e09\u9879\u771f\u5b9e\u4efb\u52a1\u548c\u4e24\u4e2a\u5b9e\u73b0\u4e2d\uff0cHi-ORS \u5728\u4ec51.5\u5c0f\u65f6\u7684\u771f\u5b9e\u4e16\u754c\u8bad\u7ec3\u5185\uff0c\u901a\u8fc7\u5fae\u8c03pi-base\u7b56\u7565\uff0c\u6210\u529f\u638c\u63e1\u4e86\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u63a7\u4efb\u52a1\uff0c\u5e76\u8d85\u8fc7\u4e86\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u4eff\u5b66\u4e60\u7684\u57fa\u51c6\u3002", "conclusion": "Hi-ORS \u5728\u77ed\u65f6\u95f4\u5185\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u64cd\u63a7\u7684\u6548\u679c\u548c\u6548\u7387\uff0c\u5e76\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6d4b\u8bd5\u65f6\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2510.26536", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.26536", "abs": "https://arxiv.org/abs/2510.26536", "authors": ["Huajie Tan", "Cheng Chi", "Xiansheng Chen", "Yuheng Ji", "Zhongxia Zhao", "Xiaoshuai Hao", "Yaoxu Lyu", "Mingyu Cao", "Junkai Zhao", "Huaihai Lyu", "Enshen Zhou", "Ning Chen", "Yankai Fu", "Cheng Peng", "Wei Guo", "Dong Liang", "Zhuo Chen", "Mengsi Lyu", "Chenrui He", "Yulong Ao", "Yonghua Lin", "Pengwei Wang", "Zhongyuan Wang", "Shanghang Zhang"], "title": "RoboOS-NeXT: A Unified Memory-based Framework for Lifelong, Scalable, and Robust Multi-Robot Collaboration", "comment": null, "summary": "The proliferation of collaborative robots across diverse tasks and\nembodiments presents a central challenge: achieving lifelong adaptability,\nscalable coordination, and robust scheduling in multi-agent systems. Existing\napproaches, from vision-language-action (VLA) models to hierarchical\nframeworks, fall short due to their reliance on limited or dividual-agent\nmemory. This fundamentally constrains their ability to learn over long\nhorizons, scale to heterogeneous teams, or recover from failures, highlighting\nthe need for a unified memory representation. To address these limitations, we\nintroduce RoboOS-NeXT, a unified memory-based framework for lifelong, scalable,\nand robust multi-robot collaboration. At the core of RoboOS-NeXT is the novel\nSpatio-Temporal-Embodiment Memory (STEM), which integrates spatial scene\ngeometry, temporal event history, and embodiment profiles into a shared\nrepresentation. This memory-centric design is integrated into a\nbrain-cerebellum framework, where a high-level brain model performs global\nplanning by retrieving and updating STEM, while low-level controllers execute\nactions locally. This closed loop between cognition, memory, and execution\nenables dynamic task allocation, fault-tolerant collaboration, and consistent\nstate synchronization. We conduct extensive experiments spanning complex\ncoordination tasks in restaurants, supermarkets, and households. Our results\ndemonstrate that RoboOS-NeXT achieves superior performance across heterogeneous\nembodiments, validating its effectiveness in enabling lifelong, scalable, and\nrobust multi-robot collaboration. Project website:\nhttps://flagopen.github.io/RoboOS/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86RoboOS-NeXT\uff0c\u4e00\u4e2a\u57fa\u4e8e\u7edf\u4e00\u8bb0\u5fc6\u7684\u591a\u673a\u5668\u4eba\u534f\u4f5c\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u7684\u9002\u5e94\u6027\u548c\u534f\u540c\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u56e0\u4f9d\u8d56\u6709\u9650\u6216\u4e2a\u4f53\u4ee3\u7406\u8bb0\u5fc6\uff0c\u96be\u4ee5\u5b9e\u73b0\u957f\u671f\u5b66\u4e60\u3001\u9002\u5e94\u5f02\u6784\u56e2\u961f\u6216\u4ece\u5931\u8d25\u4e2d\u6062\u590d\u3002", "method": "\u63d0\u51faRoboOS-NeXT\u6846\u67b6\uff0c\u91c7\u7528\u7edf\u4e00\u7684\u8bb0\u5fc6\u6a21\u578b\u4ee5\u63d0\u5347\u591a\u673a\u5668\u4eba\u5408\u4f5c\u7684\u9002\u5e94\u6027\u3001\u534f\u8c03\u6027\u548c\u8c03\u5ea6\u80fd\u529b\u3002", "result": "RoboOS-NeXT\u901a\u8fc7\u6574\u5408\u7a7a\u95f4\u573a\u666f\u51e0\u4f55\u3001\u65f6\u95f4\u4e8b\u4ef6\u5386\u53f2\u548c\u4f53\u73b0\u7279\u5f81\uff0c\u5b9e\u73b0\u4e86\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u52a8\u6001\u4efb\u52a1\u5206\u914d\u548c\u5bb9\u9519\u5408\u4f5c\u3002\u5728\u590d\u6742\u573a\u666f\u4e2d\u663e\u793a\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "RoboOS-NeXT\u5728\u591a\u79cd\u590d\u6742\u534f\u8c03\u4efb\u52a1\u4e2d\u5c55\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u591a\u673a\u5668\u4eba\u534f\u4f5c\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.26551", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26551", "abs": "https://arxiv.org/abs/2510.26551", "authors": ["Prathamesh Kothavale", "Sravani Boddepalli"], "title": "Adaptive Inverse Kinematics Framework for Learning Variable-Length Tool Manipulation in Robotics", "comment": "10 pages, 5 figures. Demonstrates a reinforcement learning framework\n  for adaptive tool manipulation with variable-length extensions", "summary": "Conventional robots possess a limited understanding of their kinematics and\nare confined to preprogrammed tasks, hindering their ability to leverage tools\nefficiently. Driven by the essential components of tool usage - grasping the\ndesired outcome, selecting the most suitable tool, determining optimal tool\norientation, and executing precise manipulations - we introduce a pioneering\nframework. Our novel approach expands the capabilities of the robot's inverse\nkinematics solver, empowering it to acquire a sequential repertoire of actions\nusing tools of varying lengths. By integrating a simulation-learned action\ntrajectory with the tool, we showcase the practicality of transferring acquired\nskills from simulation to real-world scenarios through comprehensive\nexperimentation. Remarkably, our extended inverse kinematics solver\ndemonstrates an impressive error rate of less than 1 cm. Furthermore, our\ntrained policy achieves a mean error of 8 cm in simulation. Noteworthy, our\nmodel achieves virtually indistinguishable performance when employing two\ndistinct tools of different lengths. This research provides an indication of\npotential advances in the exploration of all four fundamental aspects of tool\nusage, enabling robots to master the intricate art of tool manipulation across\ndiverse tasks.", "AI": {"tldr": "\u901a\u8fc7\u65b0\u6846\u67b6\uff0c\u673a\u5668\u4eba\u80fd\u66f4\u597d\u5730\u7406\u89e3\u548c\u4f7f\u7528\u5de5\u5177\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u64cd\u4f5c\u3002", "motivation": "\u5e38\u89c4\u673a\u5668\u4eba\u5728\u8fd0\u52a8\u5b66\u7406\u89e3\u65b9\u9762\u6709\u9650\uff0c\u4ec5\u9650\u4e8e\u9884\u7f16\u7a0b\u4efb\u52a1\uff0c\u65e0\u6cd5\u6709\u6548\u5229\u7528\u5de5\u5177\u3002", "method": "\u901a\u8fc7\u96c6\u6210\u6a21\u62df\u5b66\u4e60\u7684\u52a8\u4f5c\u8f68\u8ff9\u548c\u5de5\u5177\uff0c\u5c55\u793a\u4e86\u4ece\u6a21\u62df\u5230\u73b0\u5b9e\u573a\u666f\u6280\u80fd\u8f6c\u79fb\u7684\u53ef\u884c\u6027\uff0c\u5e76\u8fdb\u884c\u4e86\u5168\u9762\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u63d0\u51fa\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u6269\u5c55\u673a\u5668\u4eba\u9006\u8fd0\u52a8\u5b66\u6c42\u89e3\u5668\u7684\u80fd\u529b\uff0c\u5b9e\u73b0\u5bf9\u5404\u79cd\u957f\u5ea6\u5de5\u5177\u7684\u987a\u5e8f\u64cd\u4f5c\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u673a\u5668\u4eba\u5728\u5de5\u5177\u4f7f\u7528\u56db\u4e2a\u57fa\u672c\u65b9\u9762\u7684\u6f5c\u5728\u8fdb\u5c55\uff0c\u4f7f\u5176\u638c\u63e1\u5de5\u5177\u64cd\u4f5c\u7684\u590d\u6742\u6280\u80fd\u3002"}}
{"id": "2510.26588", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.26588", "abs": "https://arxiv.org/abs/2510.26588", "authors": ["Gang Li", "Chunlei Zhai", "Teng Wang", "Shaun Li", "Shangsong Jiang", "Xiangwei Zhu"], "title": "FLYINGTRUST: A Benchmark for Quadrotor Navigation Across Scenarios and Vehicles", "comment": null, "summary": "Visual navigation algorithms for quadrotors often exhibit a large variation\nin performance when transferred across different vehicle platforms and scene\ngeometries, which increases the cost and risk of field deployment. To support\nsystematic early-stage evaluation, we introduce FLYINGTRUST, a high-fidelity,\nconfigurable benchmarking framework that measures how platform kinodynamics and\nscenario structure jointly affect navigation robustness. FLYINGTRUST models\nvehicle capability with two compact, physically interpretable indicators:\nmaximum thrust-to-weight ratio and axis-wise maximum angular acceleration. The\nbenchmark pairs a diverse scenario library with a heterogeneous set of real and\nvirtual platforms and prescribes a standardized evaluation protocol together\nwith a composite scoring method that balances scenario importance, platform\nimportance and performance stability. We use FLYINGTRUST to compare\nrepresentative optimization-based and learning-based navigation approaches\nunder identical conditions, performing repeated trials per platform-scenario\ncombination and reporting uncertainty-aware metrics. The results reveal\nsystematic patterns: navigation success depends predictably on platform\ncapability and scene geometry, and different algorithms exhibit distinct\npreferences and failure modes across the evaluated conditions. These\nobservations highlight the practical necessity of incorporating both platform\ncapability and scenario structure into algorithm design, evaluation, and\nselection, and they motivate future work on methods that remain robust across\ndiverse platforms and scenarios.", "AI": {"tldr": "FLYINGTRUST\u6846\u67b6\u5e2e\u52a9\u8bc4\u4f30\u548c\u4f18\u5316\u591a\u79cd\u56db\u65cb\u7ffc\u5bfc\u822a\u7b97\u6cd5\uff0c\u5f3a\u8c03\u5e73\u53f0\u80fd\u529b\u548c\u573a\u666f\u7ed3\u6784\u7684\u7ed3\u5408\u5bf9\u7b97\u6cd5\u8bbe\u8ba1\u548c\u9009\u62e9\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u5bfc\u822a\u7b97\u6cd5\u5728\u4e0d\u540c\u5e73\u53f0\u548c\u573a\u666f\u4e2d\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0cFLYINGTRUST\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u65e8\u5728\u964d\u4f4e\u90e8\u7f72\u98ce\u9669\u5e76\u63d0\u9ad8\u7b97\u6cd5\u6027\u80fd\u7684\u53ef\u9760\u6027\u3002", "method": "FLYINGTRUST\u6a21\u578b\u901a\u8fc7\u6700\u5927\u63a8\u91cd\u6bd4\u548c\u8f74\u5411\u6700\u5927\u89d2\u52a0\u901f\u5ea6\u4e24\u4e2a\u6307\u6807\u6765\u63cf\u8ff0\u5e73\u53f0\u80fd\u529b\uff0c\u5e76\u7ed3\u5408\u591a\u6837\u7684\u573a\u666f\u5e93\u4e0e\u771f\u5b9e\u53ca\u865a\u62df\u5e73\u53f0\u8fdb\u884c\u6807\u51c6\u5316\u8bc4\u4f30\u548c\u8bc4\u5206\u3002", "result": "FLYINGTRUST\u662f\u4e00\u79cd\u9ad8\u4fdd\u771f\u5ea6\u3001\u53ef\u914d\u7f6e\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u65e8\u5728\u8bc4\u4f30\u4e0d\u540c\u5e73\u53f0\u548c\u573a\u666f\u51e0\u4f55\u5bf9\u56db\u65cb\u7ffc\u89c6\u89c9\u5bfc\u822a\u7b97\u6cd5\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5e73\u53f0\u80fd\u529b\u548c\u573a\u666f\u51e0\u4f55\u5bf9\u5bfc\u822a\u6210\u529f\u6709\u53ef\u9884\u6d4b\u7684\u5f71\u54cd\uff0c\u4e0d\u540c\u7b97\u6cd5\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u504f\u597d\u548c\u5931\u8d25\u6a21\u5f0f\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.26623", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.26623", "abs": "https://arxiv.org/abs/2510.26623", "authors": ["Spencer Teetaert", "Sven Lilge", "Jessica Burgner-Kahrs", "Timothy D. Barfoot"], "title": "A Sliding-Window Filter for Online Continuous-Time Continuum Robot State Estimation", "comment": "8 pages, 6 figures. Submitted to IEEE-RAS International Conference on\n  Soft Robotics 2026", "summary": "Stochastic state estimation methods for continuum robots (CRs) often struggle\nto balance accuracy and computational efficiency. While several recent works\nhave explored sliding-window formulations for CRs, these methods are limited to\nsimplified, discrete-time approximations and do not provide stochastic\nrepresentations. In contrast, current stochastic filter methods must run at the\nspeed of measurements, limiting their full potential. Recent works in\ncontinuous-time estimation techniques for CRs show a principled approach to\naddressing this runtime constraint, but are currently restricted to offline\noperation. In this work, we present a sliding-window filter (SWF) for\ncontinuous-time state estimation of CRs that improves upon the accuracy of a\nfilter approach while enabling continuous-time methods to operate online, all\nwhile running at faster-than-real-time speeds. This represents the first\nstochastic SWF specifically designed for CRs, providing a promising direction\nfor future research in this area.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8fde\u7eed\u673a\u5668\u4eba\u7684\u6ed1\u52a8\u7a97\u53e3\u6ee4\u6ce2\u5668\uff08SWF\uff09\uff0c\u63d0\u5347\u4e86\u72b6\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u5b9e\u73b0\u4e86\u5728\u7ebf\u5b9e\u65f6\u64cd\u4f5c\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u8fde\u7eed\u673a\u5668\u4eba\uff08CRs\uff09\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u5b9e\u65f6\u64cd\u4f5c\u65f6\u7684\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6ed1\u52a8\u7a97\u53e3\u6ee4\u6ce2\u5668\uff08SWF\uff09\u7528\u4e8e\u8fde\u7eed\u65f6\u95f4\u7684\u72b6\u6001\u4f30\u8ba1\uff0c\u65e8\u5728\u63d0\u9ad8\u7cbe\u5ea6\u5e76\u5b9e\u73b0\u5b9e\u65f6\u8fd0\u884c\u3002", "result": "\u8fd9\u79cd\u65b0\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6ee4\u6ce2\u5668\u7684\u7cbe\u5ea6\uff0c\u5e76\u5141\u8bb8\u8fde\u7eed\u65f6\u95f4\u65b9\u6cd5\u4ee5\u8d85\u8fc7\u5b9e\u9645\u65f6\u95f4\u7684\u901f\u5ea6\u5728\u7ebf\u8fd0\u884c\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u672a\u6765\u8fde\u7eed\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002"}}
{"id": "2510.26638", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.26638", "abs": "https://arxiv.org/abs/2510.26638", "authors": ["Dave van der Meer", "Lo\u00efck P. Chovet", "Gabriel M. Garcia", "Abhishek Bera", "Miguel A. Olivares-Mendez"], "title": "REALMS2 -- Resilient Exploration And Lunar Mapping System 2 -- A Comprehensive Approach", "comment": "8 Pages, 8 Figures, Submitted and Accepted to IROS 2025", "summary": "The European Space Agency (ESA) and the European Space Resources Innovation\nCentre (ESRIC) created the Space Resources Challenge to invite researchers and\ncompanies to propose innovative solutions for Multi-Robot Systems (MRS) space\nprospection. This paper proposes the Resilient Exploration And Lunar Mapping\nSystem 2 (REALMS2), a MRS framework for planetary prospection and mapping.\nBased on Robot Operating System version 2 (ROS 2) and enhanced with Visual\nSimultaneous Localisation And Mapping (vSLAM) for map generation, REALMS2 uses\na mesh network for a robust ad hoc network. A single graphical user interface\n(GUI) controls all the rovers, providing a simple overview of the robotic\nmission. This system is designed for heterogeneous multi-robot exploratory\nmissions, tackling the challenges presented by extraterrestrial environments.\nREALMS2 was used during the second field test of the ESA-ESRIC Challenge and\nallowed to map around 60% of the area, using three homogeneous rovers while\nhandling communication delays and blackouts.", "AI": {"tldr": "REALMS2\u662f\u4e00\u4e2a\u57fa\u4e8eROS 2\u7684\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u6846\u67b6\uff0c\u65e8\u5728\u5e94\u5bf9\u5916\u661f\u73af\u5883\u7684\u63a2\u6d4b\u6311\u6218\uff0c\u5e76\u5728\u5b9e\u9645\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e8660%\u7684\u533a\u57df\u5730\u56fe\u7ed8\u5236\u3002", "motivation": "\u4e3a\u4e86\u9f13\u52b1\u7814\u7a76\u4eba\u5458\u548c\u516c\u53f8\u63d0\u51fa\u521b\u65b0\u7684\u591a\u673a\u5668\u4eba\u7cfb\u7edf\uff08MRS\uff09\u7a7a\u95f4\u63a2\u6d4b\u89e3\u51b3\u65b9\u6848\uff0cESA\u548cESRIC\u63a8\u51fa\u4e86\u7a7a\u95f4\u8d44\u6e90\u6311\u6218\u3002", "method": "\u57fa\u4e8eROS 2\uff0c\u7ed3\u5408vSLAM\u6280\u672f\u548c\u7f51\u72b6\u7f51\u7edc\uff0c\u5b9e\u73b0\u4e86\u4e00\u4e2a\u96c6\u4e2d\u5f0f\u56fe\u5f62\u7528\u6237\u754c\u9762\u63a7\u5236\u7684\u591a\u673a\u5668\u4eba\u6846\u67b6\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u6846\u67b6REALMS2\uff0c\u6210\u529f\u5730\u5b8c\u6210\u4e86\u7a7a\u95f4\u63a2\u6d4b\u548c\u6620\u5c04\u4efb\u52a1\u3002", "conclusion": "REALMS2\u5c55\u793a\u4e86\u5176\u5728\u5f02\u8d28\u591a\u673a\u5668\u4eba\u63a2\u6d4b\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5904\u7406\u901a\u4fe1\u5ef6\u8fdf\u548c\u4e2d\u65ad\u7b49\u95ee\u9898\u3002"}}
{"id": "2510.26646", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26646", "abs": "https://arxiv.org/abs/2510.26646", "authors": ["Xiaoyi He", "Danggui Chen", "Zhenshuo Zhang", "Zimeng Bai"], "title": "Hybrid DQN-TD3 Reinforcement Learning for Autonomous Navigation in Dynamic Environments", "comment": "6 pages, 5 figures; ROS+Gazebo (TurtleBot3) implementation;\n  evaluation with PathBench metrics; code (primary):\n  https://github.com/MayaCHEN-github/HierarchicalRL-robot-navigation; mirror\n  (for reproducibility): https://github.com/ShowyHe/DRL-robot-navigation", "summary": "This paper presents a hierarchical path-planning and control framework that\ncombines a high-level Deep Q-Network (DQN) for discrete sub-goal selection with\na low-level Twin Delayed Deep Deterministic Policy Gradient (TD3) controller\nfor continuous actuation. The high-level module selects behaviors and\nsub-goals; the low-level module executes smooth velocity commands. We design a\npractical reward shaping scheme (direction, distance, obstacle avoidance,\naction smoothness, collision penalty, time penalty, and progress), together\nwith a LiDAR-based safety gate that prevents unsafe motions. The system is\nimplemented in ROS + Gazebo (TurtleBot3) and evaluated with PathBench metrics,\nincluding success rate, collision rate, path efficiency, and re-planning\nefficiency, in dynamic and partially observable environments. Experiments show\nimproved success rate and sample efficiency over single-algorithm baselines\n(DQN or TD3 alone) and rule-based planners, with better generalization to\nunseen obstacle configurations and reduced abrupt control changes. Code and\nevaluation scripts are available at the project repository.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9ad8\u5c42DQN\u548c\u4f4e\u5c42TD3\u7684\u5c42\u6b21\u5316\u8def\u5f84\u89c4\u5212\u63a7\u5236\u6846\u67b6\uff0c\u4f18\u5316\u4e86\u51b3\u7b56\u548c\u6267\u884c.", "motivation": "\u4e3a\u4e86\u63d0\u5347\u8def\u5f84\u89c4\u5212\u548c\u63a7\u5236\u7684\u6548\u7387\u4e0e\u5b89\u5168\u6027\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u548c\u672a\u77e5\u73af\u5883\u4e2d\u3002", "method": "\u9ad8\u5c42\u4f7f\u7528DQN\u8fdb\u884c\u79bb\u6563\u5b50\u76ee\u6807\u9009\u62e9\uff0c\u4f4e\u5c42\u4f7f\u7528TD3\u63a7\u5236\u8fde\u7eed\u52a8\u4f5c\uff0c\u7ed3\u5408\u4e86\u591a\u79cd\u5956\u52b1\u673a\u5236\u548c\u5b89\u5168\u63aa\u65bd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6210\u529f\u7387\u548c\u6837\u672c\u6548\u7387\u663e\u8457\u63d0\u9ad8\uff0c\u4e14\u5bf9\u672a\u89c1\u969c\u788d\u7269\u914d\u7f6e\u7684\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\uff0c\u51cf\u5c11\u4e86\u6025\u5267\u63a7\u5236\u53d8\u5316\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u52a8\u6001\u548c\u90e8\u5206\u53ef\u89c2\u5bdf\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u4f18\u4e8e\u5355\u4e00\u7b97\u6cd5\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u89c4\u5212\u5668\u7684\u6027\u80fd\u3002"}}
{"id": "2510.26656", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26656", "abs": "https://arxiv.org/abs/2510.26656", "authors": ["Georgios Kamaras", "Craig Innes", "Subramanian Ramamoorthy"], "title": "Heuristic Adaptation of Potentially Misspecified Domain Support for Likelihood-Free Inference in Stochastic Dynamical Systems", "comment": null, "summary": "In robotics, likelihood-free inference (LFI) can provide the domain\ndistribution that adapts a learnt agent in a parametric set of deployment\nconditions. LFI assumes an arbitrary support for sampling, which remains\nconstant as the initial generic prior is iteratively refined to more\ndescriptive posteriors. However, a potentially misspecified support can lead to\nsuboptimal, yet falsely certain, posteriors. To address this issue, we propose\nthree heuristic LFI variants: EDGE, MODE, and CENTRE. Each interprets the\nposterior mode shift over inference steps in its own way and, when integrated\ninto an LFI step, adapts the support alongside posterior inference. We first\nexpose the support misspecification issue and evaluate our heuristics using\nstochastic dynamical benchmarks. We then evaluate the impact of heuristic\nsupport adaptation on parameter inference and policy learning for a dynamic\ndeformable linear object (DLO) manipulation task. Inference results in a finer\nlength and stiffness classification for a parametric set of DLOs. When the\nresulting posteriors are used as domain distributions for sim-based policy\nlearning, they lead to more robust object-centric agent performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e09\u79cd\u542f\u53d1\u5f0fLFI\u53d8\u4f53\uff0c\u901a\u8fc7\u6539\u8fdb\u540e\u9a8c\u63a8\u65ad\uff0c\u89e3\u51b3\u4e86\u652f\u6301\u9519\u8bef\u6307\u5b9a\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u52a8\u6001\u7269\u4f53\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u7684Likelihood-Free\u63a8\u65ad\u4e2d\uff0c\u652f\u6301\u7684\u9519\u8bef\u6307\u5b9a\u53ef\u80fd\u5bfc\u81f4\u4e0d\u7406\u60f3\u4f46\u5047\u88c5\u786e\u5b9a\u7684\u540e\u9a8c\u5206\u5e03\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u542f\u53d1\u5f0fLFI\u53d8\u4f53\uff08EDGE\u3001MODE\u548cCENTRE\uff09\u6765\u89e3\u51b3\u652f\u6301\u9519\u8bef\u6307\u5b9a\u95ee\u9898\uff0c\u9002\u5e94\u6027\u5730\u6574\u5408\u652f\u6301\u548c\u540e\u9a8c\u63a8\u7406\u3002", "result": "\u5728\u52a8\u6001\u53ef\u53d8\u7ebf\u6027\u7269\u4f53\uff08DLO\uff09\u64cd\u63a7\u4efb\u52a1\u4e2d\uff0c\u7ed3\u679c\u8868\u660e\u542f\u53d1\u5f0f\u652f\u6301\u81ea\u9002\u5e94\u53ef\u4ee5\u63d0\u9ad8\u53c2\u6570\u63a8\u65ad\u7684\u7cbe\u5ea6\uff0c\u540c\u65f6\u589e\u5f3a\u57fa\u4e8e\u4eff\u771f\u7684\u7b56\u7565\u5b66\u4e60\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u4f7f\u7528\u66f4\u7cbe\u786e\u7684\u540e\u9a8c\u5206\u5e03\u4f5c\u4e3a\u9886\u57df\u5206\u5e03\uff0c\u4eff\u771f\u7b56\u7565\u5b66\u4e60\u80fd\u591f\u5b9e\u73b0\u66f4\u5f3a\u5927\u7684\u9762\u5411\u5bf9\u8c61\u7684\u4ee3\u7406\u8868\u73b0\u3002"}}
{"id": "2510.26670", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.26670", "abs": "https://arxiv.org/abs/2510.26670", "authors": ["Qianyou Zhao", "Yuliang Shen", "Xuanran Zhai", "Ce Hao", "Duidi Wu", "Jin Qi", "Jie Hu", "Qiaojun Yu"], "title": "Hybrid Consistency Policy: Decoupling Multi-Modal Diversity and Real-Time Efficiency in Robotic Manipulation", "comment": null, "summary": "In visuomotor policy learning, diffusion-based imitation learning has become\nwidely adopted for its ability to capture diverse behaviors. However,\napproaches built on ordinary and stochastic denoising processes struggle to\njointly achieve fast sampling and strong multi-modality. To address these\nchallenges, we propose the Hybrid Consistency Policy (HCP). HCP runs a short\nstochastic prefix up to an adaptive switch time, and then applies a one-step\nconsistency jump to produce the final action. To align this one-jump\ngeneration, HCP performs time-varying consistency distillation that combines a\ntrajectory-consistency objective to keep neighboring predictions coherent and a\ndenoising-matching objective to improve local fidelity. In both simulation and\non a real robot, HCP with 25 SDE steps plus one jump approaches the 80-step\nDDPM teacher in accuracy and mode coverage while significantly reducing\nlatency. These results show that multi-modality does not require slow\ninference, and a switch time decouples mode retention from speed. It yields a\npractical accuracy efficiency trade-off for robot policies.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u4e00\u81f4\u6027\u7b56\u7565\uff08HCP\uff09\uff0c\u5728\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5b66\u4e60\u4e2d\u901a\u8fc7\u77ed\u671f\u968f\u673a\u524d\u7f00\u548c\u4e00\u81f4\u6027\u8df3\u8dc3\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u591a\u6a21\u6001\u6027\u548c\u901f\u5ea6\u95ee\u9898\u3002", "motivation": "\u5728\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5b66\u4e60\u4e2d\uff0c\u6269\u6563\u57fa\u7840\u6a21\u4eff\u5b66\u4e60\u88ab\u5e7f\u6cdb\u91c7\u7528\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5feb\u901f\u91c7\u6837\u548c\u5f3a\u591a\u6a21\u6001\u6027\u4e0a\u5b58\u5728\u6311\u6218\u3002", "method": "HCP\u901a\u8fc7\u77ed\u671f\u968f\u673a\u524d\u7f00\u548c\u4e00\u81f4\u6027\u8df3\u8dc3\uff0c\u7ed3\u5408\u65f6\u95f4\u53d8\u5316\u7684\u4e00\u81f4\u6027\u84b8\u998f\uff0c\u4f7f\u5f97\u9884\u6d4b\u7ed3\u679c\u5728\u90bb\u8fd1\u8f68\u8ff9\u4e2d\u4fdd\u6301\u4e00\u81f4\uff0c\u540c\u65f6\u63d0\u5347\u5c40\u90e8\u4fdd\u771f\u5ea6\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u4e00\u81f4\u6027\u7b56\u7565\uff08HCP\uff09\uff0c\u80fd\u591f\u5728\u51cf\u5c11\u5ef6\u8fdf\u7684\u540c\u65f6\u63a5\u8fd1\u6559\u5e08\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u6a21\u6001\u8986\u76d6\u7387\u3002", "conclusion": "\u6df7\u5408\u4e00\u81f4\u6027\u7b56\u7565\u4e3a\u673a\u5668\u4eba\u7b56\u7565\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u6743\u8861\uff0c\u8868\u660e\u591a\u6a21\u6001\u6027\u5e76\u4e0d\u9700\u8981\u7f13\u6162\u7684\u63a8\u7406\u8fc7\u7a0b\u3002"}}
{"id": "2510.26742", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.26742", "abs": "https://arxiv.org/abs/2510.26742", "authors": ["Yunchao Ma", "Yizhuang Zhou", "Yunhuan Yang", "Tiancai Wang", "Haoqiang Fan"], "title": "Running VLAs at Real-time Speed", "comment": "Code is available at https://github.com/Dexmal/realtime-vla", "summary": "In this paper, we show how to run pi0-level multi-view VLA at 30Hz frame rate\nand at most 480Hz trajectory frequency using a single consumer GPU. This\nenables dynamic and real-time tasks that were previously believed to be\nunattainable by large VLA models. To achieve it, we introduce a bag of\nstrategies to eliminate the overheads in model inference. The real-world\nexperiment shows that the pi0 policy with our strategy achieves a 100% success\nrate in grasping a falling pen task. Based on the results, we further propose a\nfull streaming inference framework for real-time robot control of VLA. Code is\navailable at https://github.com/Dexmal/realtime-vla.", "AI": {"tldr": "\u5229\u7528\u5355\u4e2aGPU\u548c\u7b56\u7565\u4f18\u5316\uff0c\u5b9e\u73b0\u5b9e\u65f6\u591a\u89c6\u89d2VLA\uff0c\u6210\u529f\u6293\u53d6\u4efb\u52a1\u3002", "motivation": "\u5b9e\u73b0\u52a8\u6001\u548c\u5b9e\u65f6\u4efb\u52a1\uff0c\u514b\u670d\u5927VLA\u6a21\u578b\u5e26\u6765\u7684\u9650\u5236", "method": "\u4f7f\u7528\u5355\u4e2a\u6d88\u8d39\u7ea7GPU\u8fdb\u884cpi0\u7ea7\u591a\u89c6\u89d2VLA\u8fd0\u884c", "result": "\u5728\u6293\u53d6\u4e0b\u843d\u94c5\u7b14\u4efb\u52a1\u4e2d\uff0cpi0\u7b56\u7565\u5b9e\u73b0\u4e86100%\u7684\u6210\u529f\u7387", "conclusion": "\u57fa\u4e8e\u5b9e\u9a8c\u7ed3\u679c\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u5b9e\u65f6\u673a\u5668\u4eba\u63a7\u5236\u7684\u5168\u6d41\u5a92\u4f53\u63a8\u7406\u6846\u67b6\u3002"}}
