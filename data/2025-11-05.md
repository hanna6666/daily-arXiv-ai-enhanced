<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 17]
- [cs.HC](#cs.HC) [Total: 13]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [TRACE: Textual Reasoning for Affordance Coordinate Extraction](https://arxiv.org/abs/2511.01999)
*Sangyun Park,Jin Kim,Yuchen Cui,Matthew S. Brown*

Main category: cs.RO

TL;DR: TRACE方法通过文本推理增强了VLM在机器人控制中的性能和解释性，提出了涉及大量推理数据的新数据集和学习策略。


<details>
  <summary>Details</summary>
Motivation: 针对现有视觉语言模型在将高层次指令转化为机器人操作所需精确空间可供性方面的不足，提出一种更有效的推理方法。

Method: 引入TRACE（文本推理用于可供性坐标提取）方法，利用文本推理链（CoR）整合到可供性预测过程中，并通过自主管道创建TRACE数据集，结合指令和明确的文本理由。

Result: TRACE调优后的模型在Where2Place基准测试上达到了48.1%的准确率（相对提高9.6%），在更具挑战性的W2P(h)子集上为55.0%，并且性能与推理数据量呈正相关。

Conclusion: 本研究提出的TRACE方法通过整合文本推理，显著提高了VLM在机器人控制中的精确性和可靠性，并通过TRACE数据集增强了解释性。

Abstract: Vision-Language Models (VLMs) struggle to translate high-level instructions
into the precise spatial affordances required for robotic manipulation. While
visual Chain-of-Thought (CoT) methods exist, they are often computationally
intensive. In this work, we introduce TRACE (Textual Reasoning for Affordance
Coordinate Extraction), a novel methodology that integrates a textual Chain of
Reasoning (CoR) into the affordance prediction process. We use this methodology
to create the TRACE dataset, a large-scale collection created via an autonomous
pipeline that pairs instructions with explicit textual rationales. By
fine-tuning a VLM on this data, our model learns to externalize its spatial
reasoning before acting. Our experiments show that our TRACE-tuned model
achieves state-of-the-art performance, reaching 48.1% accuracy on the primary
Where2Place (W2P) benchmark (a 9.6% relative improvement) and 55.0% on the more
challenging W2P(h) subset. Crucially, an ablation study demonstrates that
performance scales directly with the amount of reasoning data used, confirming
the CoR's effectiveness. Furthermore, analysis of the model's attention maps
reveals an interpretable reasoning process where focus shifts dynamically
across reasoning steps. This work shows that training VLMs to generate a
textual CoR is an effective and robust strategy for enhancing the precision,
reliability, and interpretability of VLM-based robot control. Our dataset and
code are available at https://github.com/jink-ucla/TRACE

</details>


### [2] [Stein-based Optimization of Sampling Distributions in Model Predictive Path Integral Control](https://arxiv.org/abs/2511.02015)
*Jace Aldrich,Odest Chadwicke Jenkins*

Main category: cs.RO

TL;DR: 本文提出的SOPPI结合SVGD提升MPPI效果，优化了样本生成，展示了在多种任务中的优越表现。


<details>
  <summary>Details</summary>
Motivation: MPPI传统方法依赖随机采样路径，存在样本不足的问题，导致轨迹空间的代表性不足和次优结果。

Method: 引入Stein Variational Gradient Descent (SVGD) 更新以优化MPPI控制的样本生成，形成Stein-Optimized Path-Integral Inference (SOPPI)算法。

Result: SOPPI在Cart-Pole和二维双足行走任务中表现突出，显示出在不同超参数下的改进性能。

Conclusion: SOPPI方法在多个任务中表现优于传统的MPPI，而且在较低粒子数下仍具可行性，且对高自由度系统和新型可微分模拟器具有潜在应用。

Abstract: This paper presents a novel method for Model Predictive Path Integral (MPPI)
control that optimizes sample generation towards an optimal trajectory through
Stein Variational Gradient Descent (SVGD). MPPI is traditionally reliant on
randomly sampled trajectories, often by a Gaussian distribution. The result can
lead to sample deprivation, under-representing the space of possible
trajectories, and yield suboptimal results. Through introducing SVGD updates in
between MPPI environment steps, we present Stein-Optimized Path-Integral
Inference (SOPPI), an MPPI/SVGD algorithm that can dynamically update noise
distributions at runtime to shape a more optimal representation without an
excessive increase in computational requirements. We demonstrate the efficacy
of our method systems ranging from a Cart-Pole to a two-dimensional bipedal
walking task, indicating improved performance above standard MPPI across a
range of hyper-parameters and demonstrate feasibility at lower particle counts.
We discuss the applicability of this MPPI/SVGD method to higher
degree-of-freedom systems, as well as its potential to new developments in
state-of-the-art differentiable simulators.

</details>


### [3] [TurboMap: GPU-Accelerated Local Mapping for Visual SLAM](https://arxiv.org/abs/2511.02036)
*Parsa Hosseininejad,Kimia Khabiri,Shishir Gopinath,Soudabeh Mohammadhashemi,Karthik Dantu,Steven Y. Ko*

Main category: cs.RO

TL;DR: TurboMap是一个为视觉SLAM系统设计的GPU加速和CPU优化的局部映射模块，提供显著加速。


<details>
  <summary>Details</summary>
Motivation: 识别视觉SLAM局部映射过程中的关键性能瓶颈，并通过有针对性的优化进行改进。

Method: 本研究通过将地图点三角测量和融合任务转移到GPU，并在CPU上加速冗余关键帧剔除以及集成GPU加速求解器来加速局部束调整来改进局部映射过程。

Result: TurboMap在EuRoC数据集上实现了1.3倍的平均加速，在TUM-VI数据集上实现了1.6倍的加速，同时保持了原始系统的准确性。

Conclusion: TurboMap通过优化GPU和CPU，在视觉SLAM的局部映射模块中实现了显著的加速，同时保持了系统的准确性。

Abstract: This paper presents TurboMap, a GPU-accelerated and CPU-optimized local
mapping module for visual SLAM systems. We identify key performance bottlenecks
in the local mapping process for visual SLAM and address them through targeted
GPU and CPU optimizations. Specifically, we offload map point triangulation and
fusion to the GPU, accelerate redundant keyframe culling on the CPU, and
integrate a GPU-accelerated solver to speed up local bundle adjustment. Our
implementation is built on top of ORB-SLAM3 and leverages CUDA for GPU
programming. The experimental results show that TurboMap achieves an average
speedup of 1.3x in the EuRoC dataset and 1.6x in the TUM-VI dataset in the
local mapping module, on both desktop and embedded platforms, while maintaining
the accuracy of the original system.

</details>


### [4] [TACO: Trajectory-Aware Controller Optimization for Quadrotors](https://arxiv.org/abs/2511.02060)
*Hersh Sanghvi,Spencer Folk,Vijay Kumar,Camillo Jose Taylor*

Main category: cs.RO

TL;DR: 本文提出TACO框架，通过实时在线调整四旋翼控制参数，优化轨迹跟踪性能，显著减少跟踪误差。


<details>
  <summary>Details</summary>
Motivation: 传统的固定参数调节方法限制了四旋翼在特定任务中的性能，急需一种更灵活和动态的控制参数调整方法。

Method: 通过在线调整控制器参数，基于预测模型和轻量级优化方案实现控制增益的实时优化。

Result: 在各种轨迹类型实验中，TACO显著超越了传统静态参数调节，并且相比黑盒优化基线，速度提升了几个数量级，实现了实时应用。

Conclusion: TACO显著提高了四旋翼在轨迹跟踪中的控制性能，减少了跟踪误差，并支持实时部署。

Abstract: Controller performance in quadrotor trajectory tracking depends heavily on
parameter tuning, yet standard approaches often rely on fixed, manually tuned
parameters that sacrifice task-specific performance. We present
Trajectory-Aware Controller Optimization (TACO), a framework that adapts
controller parameters online based on the upcoming reference trajectory and
current quadrotor state. TACO employs a learned predictive model and a
lightweight optimization scheme to optimize controller gains in real time with
respect to a broad class of trajectories, and can also be used to adapt
trajectories to improve dynamic feasibility while respecting smoothness
constraints. To enable large-scale training, we also introduce a parallelized
quadrotor simulator supporting fast data collection on diverse trajectories.
Experiments on a variety of trajectory types show that TACO outperforms
conventional, static parameter tuning while operating orders of magnitude
faster than black-box optimization baselines, enabling practical real-time
deployment on a physical quadrotor. Furthermore, we show that adapting
trajectories using TACO significantly reduces the tracking error obtained by
the quadrotor.

</details>


### [5] [A Step Toward World Models: A Survey on Robotic Manipulation](https://arxiv.org/abs/2511.02097)
*Peng-Fei Zhang,Ying Cheng,Xiaofan Sun,Shijie Wang,Lei Zhu,Heng Tao Shen*

Main category: cs.RO

TL;DR: 本研究调查了自主系统所需的世界模型，通过分析机器人操作的相关方法，确定了核心能力并提供了未来研究的方向。


<details>
  <summary>Details</summary>
Motivation: 随着自主代理在复杂和动态环境中执行任务的需求增加，理解世界基本机制和动态是必要的。

Method: 通过对机器人操作方法的文献综述，分析各种方法在感知、预测和控制方面的作用。

Result: 识别了关键的挑战和解决方案，提炼了真实世界模型应具备的核心组件、能力和功能。

Conclusion: 本研究为开发可推广和实用的世界模型提供了路线图。

Abstract: Autonomous agents are increasingly expected to operate in complex, dynamic,
and uncertain environments, performing tasks such as manipulation, navigation,
and decision-making. Achieving these capabilities requires agents to understand
the underlying mechanisms and dynamics of the world, moving beyond purely
reactive control or simple replication of observed states. This motivates the
development of world models as internal representations that encode
environmental states, capture dynamics, and enable prediction, planning, and
reasoning. Despite growing interest, the definition, scope, architectures, and
essential capabilities of world models remain ambiguous. In this survey, rather
than directly imposing a fixed definition and limiting our scope to methods
explicitly labeled as world models, we examine approaches that exhibit the core
capabilities of world models through a review of methods in robotic
manipulation. We analyze their roles across perception, prediction, and
control, identify key challenges and solutions, and distill the core
components, capabilities, and functions that a real world model should possess.
Building on this analysis, we aim to outline a roadmap for developing
generalizable and practical world models for robotics.

</details>


### [6] [Census-Based Population Autonomy For Distributed Robotic Teaming](https://arxiv.org/abs/2511.02147)
*Tyler M. Paine,Anastasia Bizyaeva,Michael R. Benjamin*

Main category: cs.RO

TL;DR: 提出了一种新的层次化多机器人自主模型，结合集体决策和个体优化，实验验证了其在不同海洋场景中的应用。


<details>
  <summary>Details</summary>
Motivation: 探讨如何建模、分析和设计多机器人系统，以实现协作的全部好处，特别是在海洋环境中。

Method: 引入分层的多机器人自主模型，利用邻居输入的加权计数进行集体决策，并使用区间规划进行个体行为的多目标优化。

Result: 通过实验验证了该模型在自适应采样、高价值单位保护及竞争性游戏等多个场景中的有效性。

Conclusion: 该模型在不同场景下验证了多机器人系统的有效性，展示了集体决策和个体行为优化的潜力。

Abstract: Collaborating teams of robots show promise due in their ability to complete
missions more efficiently and with improved robustness, attributes that are
particularly useful for systems operating in marine environments. A key issue
is how to model, analyze, and design these multi-robot systems to realize the
full benefits of collaboration, a challenging task since the domain of
multi-robot autonomy encompasses both collective and individual behaviors. This
paper introduces a layered model of multi-robot autonomy that uses the
principle of census, or a weighted count of the inputs from neighbors, for
collective decision-making about teaming, coupled with multi-objective behavior
optimization for individual decision-making about actions. The census component
is expressed as a nonlinear opinion dynamics model and the multi-objective
behavior optimization is accomplished using interval programming. This model
can be reduced to recover foundational algorithms in distributed optimization
and control, while the full model enables new types of collective behaviors
that are useful in real-world scenarios. To illustrate these points, a new
method for distributed optimization of subgroup allocation is introduced where
robots use a gradient descent algorithm to minimize portions of the cost
functions that are locally known, while being influenced by the opinion states
from neighbors to account for the unobserved costs. With this method the group
can collectively use the information contained in the Hessian matrix of the
total global cost. The utility of this model is experimentally validated in
three categorically different experiments with fleets of autonomous surface
vehicles: an adaptive sampling scenario, a high value unit protection scenario,
and a competitive game of capture the flag.

</details>


### [7] [Text to Robotic Assembly of Multi Component Objects using 3D Generative AI and Vision Language Models](https://arxiv.org/abs/2511.02162)
*Alexander Htet Kyaw,Richa Gupta,Dhruv Shah,Anoop Sinha,Kory Mathewson,Stefanie Pender,Sachin Chitta,Yotto Koga,Faez Ahmed,Lawrence Sass,Randall Davis*

Main category: cs.RO

TL;DR: 本研究提出了一种新的机器人组装管道，利用3D生成AI和视觉语言模型(VLM)从自然语言创建多组件对象，提高了用户对组件分配的偏好和控制能力。


<details>
  <summary>Details</summary>
Motivation: 尽管3D生成AI在从文本提示创建物理对象方面取得进展，但在涉及多种组件类型的对象创建上仍面临挑战。

Method: 通过将3D生成AI与视觉语言模型(VLMs)集成，构建一个可以从自然语言创建多组件对象的机器人组装管道。

Result: 用户对VLM生成的组件分配的偏好率为90.6%，而基于规则的分配为59.4%，随机分配仅为2.5%。

Conclusion: 该系统允许用户通过对话反馈来细化组件分配，增强人类在使用生成AI和机器人制作物理对象时的控制力和自主性。

Abstract: Advances in 3D generative AI have enabled the creation of physical objects
from text prompts, but challenges remain in creating objects involving multiple
component types. We present a pipeline that integrates 3D generative AI with
vision-language models (VLMs) to enable the robotic assembly of multi-component
objects from natural language. Our method leverages VLMs for zero-shot,
multi-modal reasoning about geometry and functionality to decompose
AI-generated meshes into multi-component 3D models using predefined structural
and panel components. We demonstrate that a VLM is capable of determining which
mesh regions need panel components in addition to structural components, based
on object functionality. Evaluation across test objects shows that users
preferred the VLM-generated assignments 90.6% of the time, compared to 59.4%
for rule-based and 2.5% for random assignment. Lastly, the system allows users
to refine component assignments through conversational feedback, enabling
greater human control and agency in making physical objects with generative AI
and robotics.

</details>


### [8] [Kinematic and Ergonomic Design of a Robotic Arm for Precision Laparoscopic Surgery](https://arxiv.org/abs/2511.02167)
*Tian Hao,Tong Lu,Che Chan*

Main category: cs.RO

TL;DR: 本研究开发了一种7自由度的机器人手臂，显著提升了微创手术的精度和效率，减轻了外科医生的疲劳，强调了运动学优化和人机工程学设计的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究机器人辅助微创手术的运动学与人机工程学设计原则，以提升手术精度并降低外科医生疲劳。

Method: 提出了一种7自由度（7-DOF）机器人手臂系统，结合了远程运动中心（RCM）和人机工程学设计，通过在通用机器人平台上实施来进行一系列模拟手术任务的评估。

Result: 实验结果表明，优化的机器人设计在靶向精度上提高（误差减少超过50%），任务完成时间缩短，同时显著降低操作者的肌肉疲劳和不适感。

Conclusion: 通过运动学优化和以人为中心的设计，优化的机器人设计显著提高了靶向精度和任务效率，同时降低了操作者肌肉疲劳和不适感，验证了这些设计原则在机器人辅助手术中的重要性。

Abstract: Robotic assistance in minimally invasive surgery can greatly enhance surgical
precision and reduce surgeon fatigue. This paper presents a focused
investigation on the kinematic and ergonomic design principles for a
laparoscopic surgical robotic arm aimed at high-precision tasks. We propose a
7-degree-of-freedom (7-DOF) robotic arm system that incorporates a remote
center of motion (RCM) at the instrument insertion point and ergonomic
considerations to improve surgeon interaction. The design is implemented on a
general-purpose robotic platform, and a series of simulated surgical tasks were
performed to evaluate targeting accuracy, task efficiency, and surgeon comfort
compared to conventional manual laparoscopy. Experimental results demonstrate
that the optimized robotic design achieves significantly improved targeting
accuracy (error reduced by over 50%) and shorter task completion times, while
substantially lowering operator muscle strain and discomfort. These findings
validate the importance of kinematic optimization (such as added articulations
and tremor filtering) and human-centered ergonomic design in enhancing the
performance of robot-assisted surgery. The insights from this work can guide
the development of next-generation surgical robots that improve surgical
outcomes and ergonomics for the operating team.

</details>


### [9] [A Quantitative Comparison of Centralised and Distributed Reinforcement Learning-Based Control for Soft Robotic Arms](https://arxiv.org/abs/2511.02192)
*Linxin Hou,Qirui Wu,Zhihang Qin,Neil Banerjee,Yongxin Guo,Cecilia Laschi*

Main category: cs.RO

TL;DR: 本研究比较中央与分布式MARL在软机器人控制中的表现，发现两者在样本效率和训练时间效率之间存在明显权衡，为未来的设计提供了指导。


<details>
  <summary>Details</summary>
Motivation: 研究中央和分布式多智能体强化学习架构在控制软机器人臂中的表现差异，以优化控制策略。

Method: 通过在仿真中使用PyElastica和OpenAI Gym接口，比较中心化的PPO控制器与多智能体PPO（MAPPO）在不同控制段数下的性能。

Result: 在软机器人臂控制中，随着控制段数的增加，分布式策略表现出更高的样本效率，但训练时间效率低于中心化策略。

Conclusion: 中心化和分布式策略在强化学习中的控制存在权衡，特别是在软机器人系统的设计指导中具有实际价值。

Abstract: This paper presents a quantitative comparison between centralised and
distributed multi-agent reinforcement learning (MARL) architectures for
controlling a soft robotic arm modelled as a Cosserat rod in simulation. Using
PyElastica and the OpenAI Gym interface, we train both a global Proximal Policy
Optimisation (PPO) controller and a Multi-Agent PPO (MAPPO) under identical
budgets. Both approaches are based on the arm having $n$ number of controlled
sections. The study systematically varies $n$ and evaluates the performance of
the arm to reach a fixed target in three scenarios: default baseline condition,
recovery from external disturbance, and adaptation to actuator failure.
Quantitative metrics used for the evaluation are mean action magnitude, mean
final distance, mean episode length, and success rate. The results show that
there are no significant benefits of the distributed policy when the number of
controlled sections $n\le4$. In very simple systems, when $n\le2$, the
centralised policy outperforms the distributed one. When $n$ increases to $4<
n\le 12$, the distributed policy shows a high sample efficiency. In these
systems, distributed policy promotes a stronger success rate, resilience, and
robustness under local observability and yields faster convergence given the
same sample size. However, centralised policies achieve much higher time
efficiency during training as it takes much less time to train the same size of
samples. These findings highlight the trade-offs between centralised and
distributed policy in reinforcement learning-based control for soft robotic
systems and provide actionable design guidance for future sim-to-real transfer
in soft rod-like manipulators.

</details>


### [10] [LACY: A Vision-Language Model-based Language-Action Cycle for Self-Improving Robotic Manipulation](https://arxiv.org/abs/2511.02239)
*Youngjin Hong,Houjian Yu,Mingen Li,Changhyun Choi*

Main category: cs.RO

TL;DR: LACY框架通过双向映射增强机器人行为的理解和语言解释，显著提高了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的单向语言到行动（L2A）模式缺乏深层次的上下文理解，限制了机器人的推广能力和行为解释能力，因此需要引入 действий — язык（A2L）能力。

Method: 采用统一的视角-语言模型，通过三个协同任务（L2A、A2L和L2C）进行联合训练，形成双向映射。

Result: 在真实和模拟的抓取调整任务中，LACY平均提高了任务成功率56.46%。

Conclusion: LACY框架显著提升了机器人操作任务的成功率，并增强了语言与操作之间的扎实关联。

Abstract: Learning generalizable policies for robotic manipulation increasingly relies
on large-scale models that map language instructions to actions (L2A). However,
this one-way paradigm often produces policies that execute tasks without deeper
contextual understanding, limiting their ability to generalize or explain their
behavior. We argue that the complementary skill of mapping actions back to
language (A2L) is essential for developing more holistic grounding. An agent
capable of both acting and explaining its actions can form richer internal
representations and unlock new paradigms for self-supervised learning. We
introduce LACY (Language-Action Cycle), a unified framework that learns such
bidirectional mappings within a single vision-language model. LACY is jointly
trained on three synergistic tasks: generating parameterized actions from
language (L2A), explaining observed actions in language (A2L), and verifying
semantic consistency between two language descriptions (L2C). This enables a
self-improving cycle that autonomously generates and filters new training data
through an active augmentation strategy targeting low-confidence cases, thereby
improving the model without additional human labels. Experiments on
pick-and-place tasks in both simulation and the real world show that LACY
improves task success rates by 56.46% on average and yields more robust
language-action grounding for robotic manipulation. Project page:
https://vla2026.github.io/LACY/

</details>


### [11] [SuckTac: Camera-based Tactile Sucker for Unstructured Surface Perception and Interaction](https://arxiv.org/abs/2511.02294)
*Ruiyong Yuan,Jieji Ren,Zhanxuan Peng,Feifei Chen,Guoying Gu*

Main category: cs.RO

TL;DR: 本研究提出了SuckTac，一种集成相机的智能吸盘，优化了结构和感知能力，改善了在复杂环境中对物体的操作和感知。


<details>
  <summary>Details</summary>
Motivation: 现有的吸盘缺乏高保真度的感知能力，限制了它们在复杂环境中的使用，研究旨在提升吸盘的感知和操作能力。

Method: 采用多材料集成铸造技术，优化吸盘结构，将相机和光源嵌入吸盘中，结合机械设计的优化，实现高密度的细节感知。

Result: 通过实验验证，SuckTac在复杂任务中表现卓越，展示了其在机器人技术领域的广泛适用性。

Conclusion: SuckTac是一种新型智能吸盘，通过集成相机式触觉传感器，显著提升了对复杂表面的感知能力和操控性能，适用于各种任务。

Abstract: Suckers are significant for robots in picking, transferring, manipulation and
locomotion on diverse surfaces. However, most of the existing suckers lack
high-fidelity perceptual and tactile sensing, which impedes them from resolving
the fine-grained geometric features and interaction status of the target
surface. This limits their robust performance with irregular objects and in
complex, unstructured environments. Inspired by the adaptive structure and
high-performance sensory capabilities of cephalopod suckers, in this paper, we
propose a novel, intelligent sucker, named SuckTac, that integrates a
camera-based tactile sensor directly within its optimized structure to provide
high-density perception and robust suction. Specifically, through joint
structure design and optimization and based on a multi-material integrated
casting technique, a camera and light source are embedded into the sucker,
which enables in-situ, high-density perception of fine details like surface
shape, texture and roughness. To further enhance robustness and adaptability,
the sucker's mechanical design is also optimized by refining its profile,
adding a compliant lip, and incorporating surface microstructure. Extensive
experiments, including challenging tasks such as robotic cloth manipulation and
soft mobile robot inspection, demonstrate the superior performance and broad
applicability of the proposed system.

</details>


### [12] [ZJUNlict Extended Team Description Paper 2025](https://arxiv.org/abs/2511.02315)
*Zifei Wu,Lijie Wang,Zhe Yang,Shijie Yang,Liang Wang,Haoran Fu,Yinliang Cai,Rong Xiong*

Main category: cs.RO

TL;DR: ZJUNlict团队在机器人技术上取得进展，通过IMU集成和软件模块优化，提升了游戏中的决策及适应性。


<details>
  <summary>Details</summary>
Motivation: 为了提高机器人的决策效率和适应快速游戏节奏的能力。

Method: 硬件方面，完成了IMU的集成以改进姿态精度和角速度规划；软件方面则优化了策略和CUDA模块。

Result: 通过硬件和软件的优化，提升了机器人的决策效率、球追逐预测和控球预测能力。

Conclusion: ZJUNlict团队在硬件和软件方面取得了显著进展，提升了机器人在动态游戏中的表现。

Abstract: This paper presents the ZJUNlict team's work over the past year, covering
both hardware and software advancements. In the hardware domain, the
integration of an IMU into the v2023 robot was completed to enhance posture
accuracy and angular velocity planning. On the software side, key modules were
optimized, including the strategy and CUDA modules, with significant
improvements in decision making efficiency, ball pursuit prediction, and ball
possession prediction to adapt to high-tempo game dynamics.

</details>


### [13] [Whole-body motion planning and safety-critical control for aerial manipulation](https://arxiv.org/abs/2511.02342)
*Lin Yang,Jinwoo Lee,Domenico Campolo,H. Jin Kim,Jeonghyun Byun*

Main category: cs.RO

TL;DR: 本研究提出了一种新的无人机操纵器整体运动规划及安全控制框架，通过超二次体建模和改进的路径规划技术，提高了在复杂环境中的路径安全性和动态可行性.


<details>
  <summary>Details</summary>
Motivation: 无人机操纵器在拥挤空间中执行复杂任务时，路径规划面临全身碰撞避免和几何抽象保守性的问题。

Method: 通过使用超二次体(SQ)建模，包括SQ加上代理的表示方法，结合Voronoi图和均衡流形，为无人机操纵器规划安全的、动态可行的路径。

Result: 在复杂环境中，提出的方法在生成平滑、碰撞感知路径的同时，优于基于采样的规划器，并在几何精度上超越了基于椭圆体的基线。

Conclusion: 提出的整体运动规划和安全控制框架在仿真和实际实验中均表现出优越的性能，证明了其在复杂环境中的可行性和鲁棒性。

Abstract: Aerial manipulation combines the maneuverability of multirotors with the
dexterity of robotic arms to perform complex tasks in cluttered spaces. Yet
planning safe, dynamically feasible trajectories remains difficult due to
whole-body collision avoidance and the conservativeness of common geometric
abstractions such as bounding boxes or ellipsoids. We present a whole-body
motion planning and safety-critical control framework for aerial manipulators
built on superquadrics (SQs). Using an SQ-plus-proxy representation, we model
both the vehicle and obstacles with differentiable, geometry-accurate surfaces.
Leveraging this representation, we introduce a maximum-clearance planner that
fuses Voronoi diagrams with an equilibrium-manifold formulation to generate
smooth, collision-aware trajectories. We further design a safety-critical
controller that jointly enforces thrust limits and collision avoidance via
high-order control barrier functions. In simulation, our approach outperforms
sampling-based planners in cluttered environments, producing faster, safer, and
smoother trajectories and exceeding ellipsoid-based baselines in geometric
fidelity. Actual experiments on a physical aerial-manipulation platform confirm
feasibility and robustness, demonstrating consistent performance across
simulation and hardware settings. The video can be found at
https://youtu.be/hQYKwrWf1Ak.

</details>


### [14] [Dexterous Robotic Piano Playing at Scale](https://arxiv.org/abs/2511.02504)
*Le Chen,Yi Zhao,Jan Schneider,Quankai Gao,Simon Guist,Cheng Qian,Juho Kannala,Bernhard Schölkopf,Joni Pajarinen,Dieter Büchler*

Main category: cs.RO

TL;DR: 本研究提出了 OmniPianist，一种使用自动指法、强化学习和模仿学习相结合的系统，能够独立学习和演奏近千首钢琴曲，推动了机器人灵巧演奏的发展。


<details>
  <summary>Details</summary>
Motivation: 实现机器人手具有人类水平的灵巧性，特别是在双手机器人钢琴演奏这一复杂任务中。

Method: 采用基于最优传输的自动指法策略、大规模强化学习以及流匹配变换器进行模仿学习。

Result: 开发了 OmniPianist 代理，能够不依赖人类演示学习钢琴演奏；使用超过一百万个轨迹的数据集进行训练。

Conclusion: OmniPianist 能够高效地演奏近千首音乐作品，展示了其在机器人钢琴演奏中的有效性和可扩展性。

Abstract: Endowing robot hands with human-level dexterity has been a long-standing goal
in robotics. Bimanual robotic piano playing represents a particularly
challenging task: it is high-dimensional, contact-rich, and requires fast,
precise control. We present OmniPianist, the first agent capable of performing
nearly one thousand music pieces via scalable, human-demonstration-free
learning. Our approach is built on three core components. First, we introduce
an automatic fingering strategy based on Optimal Transport (OT), allowing the
agent to autonomously discover efficient piano-playing strategies from scratch
without demonstrations. Second, we conduct large-scale Reinforcement Learning
(RL) by training more than 2,000 agents, each specialized in distinct music
pieces, and aggregate their experience into a dataset named RP1M++, consisting
of over one million trajectories for robotic piano playing. Finally, we employ
a Flow Matching Transformer to leverage RP1M++ through large-scale imitation
learning, resulting in the OmniPianist agent capable of performing a wide range
of musical pieces. Extensive experiments and ablation studies highlight the
effectiveness and scalability of our approach, advancing dexterous robotic
piano playing at scale.

</details>


### [15] [Non-Contact Manipulation of Induced Magnetic Dipoles](https://arxiv.org/abs/2511.02761)
*Seth Stewart,Joseph Pawelski,Steve Ward,Andrew J. Petruska*

Main category: cs.RO

TL;DR: 本研究实现了半浮力铝球的闭环位置控制，探讨了不同的力反转方法，推动了磁操控技术在导电非磁性物体中的应用。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在扩展磁操控领域到导电性和非磁性物体，特别关注通过振荡磁场回收空间碎片。

Method: 通过在实验室测试中实现半浮力铝球的闭环位置控制，并探索力反转的不同方法。

Result: 实验表明，闭环控制方法有效，能够实现铝球的定位控制。

Conclusion: 该研究展示了半浮力铝球的闭环位置控制，为诱导磁偶极子的三自由度位置控制的广泛应用奠定了基础。

Abstract: Extending the field of magnetic manipulation to conductive, non-magnetic
objects opens the door for a wide array of applications previously limited to
hard or soft magnetic materials. Of particular interest is the recycling of
space debris through the use of oscillating magnetic fields, which represent a
cache of raw materials in an environment particularly suited to the low forces
generated from inductive magnetic manipulation. Building upon previous work
that demonstrated 3D open-loop position control by leveraging the opposing
dipole moment created from induced eddy currents, this work demonstrates
closed-loop position control of a semi-buoyant aluminum sphere in lab tests,
and the efficacy of varying methods for force inversion is explored. The
closed-loop methods represent a critical first step towards wider applications
for 3-DOF position control of induced magnetic dipoles.

</details>


### [16] [XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations](https://arxiv.org/abs/2511.02776)
*Shichao Fan,Kun Wu,Zhengping Che,Xinhua Wang,Di Wu,Fei Liao,Ning Liu,Yixue Zhang,Zhen Zhao,Zhiyuan Xu,Meng Li,Qingjie Liu,Shanghang Zhang,Min Wan,Jian Tang*

Main category: cs.RO

TL;DR: XR-1是一个新的VLA学习框架，通过UVMC解决低级动作生成和数据源对齐问题，表现优越。


<details>
  <summary>Details</summary>
Motivation: 解决现有VLA模型在生成低级动作和跨领域数据源的对齐两大挑战。

Method: 提出了一个三阶段的训练范式，利用统一视觉运动编码（UVMC）连接视觉动态和机器人动作。

Result: XR-1在超过14,000个真实实验中超越多个先进基线模型，且对新颖物体和环境变化具有良好的适应性。

Conclusion: XR-1框架在多样化机器人和任务中表现优越，具备强大的泛化能力。

Abstract: Recent progress in large-scale robotic datasets and vision-language models
(VLMs) has advanced research on vision-language-action (VLA) models. However,
existing VLA models still face two fundamental challenges: (i) producing
precise low-level actions from high-dimensional observations, (ii) bridging
domain gaps across heterogeneous data sources, including diverse robot
embodiments and human demonstrations. Existing methods often encode latent
variables from either visual dynamics or robotic actions to guide policy
learning, but they fail to fully exploit the complementary multi-modal
knowledge present in large-scale, heterogeneous datasets. In this work, we
present X Robotic Model 1 (XR-1), a novel framework for versatile and scalable
VLA learning across diverse robots, tasks, and environments. XR-1 introduces
the \emph{Unified Vision-Motion Codes (UVMC)}, a discrete latent representation
learned via a dual-branch VQ-VAE that jointly encodes visual dynamics and
robotic motion. UVMC addresses these challenges by (i) serving as an
intermediate representation between the observations and actions, and (ii)
aligning multimodal dynamic information from heterogeneous data sources to
capture complementary knowledge. To effectively exploit UVMC, we propose a
three-stage training paradigm: (i) self-supervised UVMC learning, (ii)
UVMC-guided pretraining on large-scale cross-embodiment robotic datasets, and
(iii) task-specific post-training. We validate XR-1 through extensive
real-world experiments with more than 14,000 rollouts on six different robot
embodiments, spanning over 120 diverse manipulation tasks. XR-1 consistently
outperforms state-of-the-art baselines such as $\pi_{0.5}$, $\pi_0$, RDT,
UniVLA, and GR00T-N1.5 while demonstrating strong generalization to novel
objects, background variations, distractors, and illumination changes. Our
project is at https://xr-1-vla.github.io/.

</details>


### [17] [TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System](https://arxiv.org/abs/2511.02832)
*Yanjie Ze,Siheng Zhao,Weizhuo Wang,Angjoo Kanazawa,Rocky Duan,Pieter Abbeel,Guanya Shi,Jiajun Wu,C. Karen Liu*

Main category: cs.RO

TL;DR: TWIST2是一个便携式的无动捕人形机器人远程操作系统，实现了高效的数据收集和自主控制，成功展示多种复杂技能。


<details>
  <summary>Details</summary>
Motivation: 旨在克服现有的人形机器人远程操作系统的不足，提供可扩展的数据收集框架。

Method: 通过PICO4U VR获得实时全身人类动作，并利用定制的2自由度机器人颈部实现自我中心视角的控制。

Result: TWIST2系统能够在15分钟内收集100个演示，成功率接近100%，并展示了全身灵巧操作和动态踢球技能。

Conclusion: TWIST2系统实现了高效的人形机器人远程操作和数据收集，能够自主控制全身动作，并在各类任务上表现出色。

Abstract: Large-scale data has driven breakthroughs in robotics, from language models
to vision-language-action models in bimanual manipulation. However, humanoid
robotics lacks equally effective data collection frameworks. Existing humanoid
teleoperation systems either use decoupled control or depend on expensive
motion capture setups. We introduce TWIST2, a portable, mocap-free humanoid
teleoperation and data collection system that preserves full whole-body control
while advancing scalability. Our system leverages PICO4U VR for obtaining
real-time whole-body human motions, with a custom 2-DoF robot neck (cost around
$250) for egocentric vision, enabling holistic human-to-humanoid control. We
demonstrate long-horizon dexterous and mobile humanoid skills and we can
collect 100 demonstrations in 15 minutes with an almost 100% success rate.
Building on this pipeline, we propose a hierarchical visuomotor policy
framework that autonomously controls the full humanoid body based on egocentric
vision. Our visuomotor policy successfully demonstrates whole-body dexterous
manipulation and dynamic kicking tasks. The entire system is fully reproducible
and open-sourced at https://yanjieze.com/TWIST2 . Our collected dataset is also
open-sourced at https://twist-data.github.io .

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [18] [NeuResonance: Exploring Feedback Experiences for Fostering the Inter-brain Synchronization](https://arxiv.org/abs/2511.02079)
*Jamie Ngoc Dinh,Snehesh Shrestha,You-Jin Kim,Jun Nishida,Myungin Lee*

Main category: cs.HC

TL;DR: 本研究探讨了如何通过不同类型的反馈增强人际脑同步，结果表明反馈可以促进社交情感。


<details>
  <summary>Details</summary>
Motivation: 探索外部反馈如何进一步增强人际脑同步，以改善治疗、学习和艺术表演等需要良好人际关系的场景。

Method: 采用视觉、听觉和触觉反馈来增强人际脑同步的程度，并提供反馈系统的设计建议。

Result: 提供了三种不同类型的反馈：身体投影映射、和弦的声音化以及附在手腕上的振动带，研究其对人脑同步的影响。

Conclusion: 不同类型的反馈系统可以增强人际脑同步的强度，从而促进社交效果。

Abstract: When several individuals collaborate on a shared task, their brain activities
often synchronize. This phenomenon, known as Inter-brain Synchronization (IBS),
is notable for inducing prosocial outcomes such as enhanced interpersonal
feelings, including closeness, trust, empathy, and more. Further strengthening
the IBS with the aid of external feedback would be beneficial for scenarios
where those prosocial feelings play a vital role in interpersonal
communication, such as rehabilitation between a therapist and a patient, motor
skill learning between a teacher and a student, and group performance art. This
paper investigates whether visual, auditory, and haptic feedback of the IBS
level can further enhance its intensity, offering design recommendations for
feedback systems in IBS. We report findings when three different types of
feedback were provided: IBS level feedback by means of on-body projection
mapping, sonification using chords, and vibration bands attached to the wrist.

</details>


### [19] [AlloyLens: A Visual Analytics Tool for High-throughput Alloy Screening and Inverse Design](https://arxiv.org/abs/2511.02133)
*Suyang Li,Fernando Fajardo-Rojas,Diego Gomez-Gualdron,Remco Chang,Mingwei Li*

Main category: cs.HC

TL;DR: AlloyLens是一个新颖的交互式工具，帮助设计多功能合金，通过可视化和动态分析支持高维数据的探索。


<details>
  <summary>Details</summary>
Motivation: 当前工具在多维空间探索上有限，难以支持敏感性分析或多目标权衡推理，急需一种新的解决方案。

Method: 结合协调散点矩阵（SPLOM）、动态参数滑块、基于梯度的敏感性曲线和最近邻推荐，构建了一种综合性分析方法。

Result: 通过与领域专家合作开发的案例研究，验证了该系统在结构、热和电合金设计中的有效性。

Conclusion: AlloyLens是一种交互式视觉分析系统，可有效探索多功能合金的高维组成-结构-性能空间。

Abstract: Designing multi-functional alloys requires exploring high-dimensional
composition-structure-property spaces, yet current tools are limited to
low-dimensional projections and offer limited support for sensitivity or
multi-objective tradeoff reasoning. We introduce AlloyLens, an interactive
visual analytics system combining a coordinated scatterplot matrix (SPLOM),
dynamic parameter sliders, gradient-based sensitivity curves, and nearest
neighbor recommendations. This integrated approach reveals latent structure in
simulation data, exposes the local impact of compositional changes, and
highlights tradeoffs when exact matches are absent. We validate the system
through case studies co-developed with domain experts spanning structural,
thermal, and electrical alloy design.

</details>


### [20] [Learning Spatial Awareness for Laparoscopic Surgery with AI Assisted Visual Feedback](https://arxiv.org/abs/2511.02233)
*Songyang Liu,Yunpeng Tan,Shuai Li*

Main category: cs.HC

TL;DR: 本文介绍了一种新型AI辅助训练框架，通过结合2D和3D视觉，以改善腹腔镜手术培训中的空间感知问题。


<details>
  <summary>Details</summary>
Motivation: 传统的训练方法导致实习生在空间感知方面存在局限，因此需要一种新方式来改善这一不足。

Method: 开发一种AI辅助训练框架，结合标准的2D腹腔镜图像与同步的3D视觉反馈，通过混合现实界面供给实习生。

Result: 通过增加3D上下文，提升了实习生的深度感知、接触意识和工具方向理解。

Conclusion: 该AI辅助训练框架显著改善了外科实习生的空间感知和手术操作能力。

Abstract: Laparoscopic surgery constrains surgeons spatial awareness because procedures
are performed through a monocular, two-dimensional (2D) endoscopic view.
Conventional training methods using dry-lab models or recorded videos provide
limited depth cues, often leading trainees to misjudge instrument position and
perform ineffective or unsafe maneuvers. To address this limitation, we present
an AI-assisted training framework developed in NVIDIA Isaac Sim that couples
the standard 2D laparoscopic feed with synchronized three-dimensional (3D)
visual feedback delivered through a mixed-reality (MR) interface. While
trainees operate using the clinical 2D view, validated AI modules continuously
localize surgical instruments and detect instrument-tissue interactions in the
background. When spatial misjudgments are detected, 3D visual feedback are
displayed to trainees, while preserving the original operative perspective. Our
framework considers various surgical tasks including navigation, manipulation,
transfer, cutting, and suturing. Visually similar 2D cases can be disambiguated
through the added 3D context, improving depth perception, contact awareness,
and tool orientation understanding.

</details>


### [21] [The Pervasive Blind Spot: Benchmarking VLM Inference Risks on Everyday Personal Videos](https://arxiv.org/abs/2511.02367)
*Shuning Zhang,Zhaoxin Li,Changxi Wen,Ying Ma,Simin Li,Gengrui Zhang,Ziyi Zhang,Yibo Meng,Hantao Zhao,Xin Yi,Hewu Li*

Main category: cs.HC

TL;DR: 本研究揭示了VLM在推断个人视频中的隐私风险，模型表现超越人类，但解释能力存在问题。


<details>
  <summary>Details</summary>
Motivation: 探讨VLM在个人视频中的推断隐私威胁，特别是推断敏感个人属性的风险。

Method: 通过众包收集508个日常个人视频，并进行VLM推断能力的基准研究，与人类表现进行比较。

Result: 发现VLM在推断能力上超过人类，与视频特征和提示策略密切相关，且模型生成的解释不可靠。

Conclusion: VLM的推断能力超越人类，存在隐私风险，并显示出模型生成解释与证据之间的脱节。

Abstract: The proliferation of Vision-Language Models (VLMs) introduces profound
privacy risks from personal videos. This paper addresses the critical yet
unexplored inferential privacy threat, the risk of inferring sensitive personal
attributes over the data. To address this gap, we crowdsourced a dataset of 508
everyday personal videos from 58 individuals. We then conducted a benchmark
study evaluating VLM inference capabilities against human performance. Our
findings reveal three critical insights: (1) VLMs possess superhuman
inferential capabilities, significantly outperforming human evaluators,
leveraging a shift from object recognition to behavioral inference from
temporal streams. (2) Inferential risk is strongly correlated with factors such
as video characteristics and prompting strategies. (3) VLM-driven explanation
towards the inference is unreliable, as we revealed a disconnect between the
model-generated explanations and evidential impact, identifying ubiquitous
objects as misleading confounders.

</details>


### [22] [AI Credibility Signals Outrank Institutions and Engagement in Shaping News Perception on Social Media](https://arxiv.org/abs/2511.02370)
*Adnan Hoq,Matthew Facciani,Tim Weninger*

Main category: cs.HC

TL;DR: 研究表明AI生成的内容对用户对政治新闻的感知具有显著影响，需设计能够平衡信息影响力与用户自主性的策略。


<details>
  <summary>Details</summary>
Motivation: 探讨AI生成内容如何影响公众信任和信息的认知判断，填补相关研究的空白。

Method: 通过大规模混合设计实验（样本量为1000），研究AI生成的可信度评分对用户感知政治新闻的影响。

Result: 研究表明AI反馈显著调节了党派偏见和对机构的不信任，超越了传统参与信号的影响。

Conclusion: AI生成内容在塑造公众信任和认知判断方面具有显著影响，尤其是在政治新闻的用户感知中。

Abstract: AI-generated content is rapidly becoming a salient component of online
information ecosystems, yet its influence on public trust and epistemic
judgments remains poorly understood. We present a large-scale mixed-design
experiment (N = 1,000) investigating how AI-generated credibility scores affect
user perception of political news. Our results reveal that AI feedback
significantly moderates partisan bias and institutional distrust, surpassing
traditional engagement signals such as likes and shares. These findings
demonstrate the persuasive power of generative AI and suggest a need for design
strategies that balance epistemic influence with user autonomy.

</details>


### [23] [Revisiting put-that-there, context aware window interactions via LLMs](https://arxiv.org/abs/2511.02378)
*Riccardo Bovo,Daniele Giunchi,Pasquale Cascarano,Eric J. Gonzalez,Mar Gonzalez-Franco*

Main category: cs.HC

TL;DR: 本研究结合大型语言模型与XR技术，开发了一种新型交互系统，支持用户通过多种方式管理虚拟工作空间，提升了沉浸式环境中操作的流畅性。


<details>
  <summary>Details</summary>
Motivation: 重新思考Bolt的"放置概念"，使其适应现代头戴式显示设备，并提升用户体验。

Method: 将大型语言模型与XR技术栈结合，利用语义分割的3D环境、实时应用元数据及用户的语言和手势进行窗口管理。

Result: 用户可以通过明确的指令、指示性语言加手势或高阶目标进行窗口控制，系统支持一对多的动作映射和目标导向推理。

Conclusion: 该系统支持用户通过多种自然交互方式动态调整虚拟工作空间，提高了XR环境中的交互体验。

Abstract: We revisit Bolt's classic "Put-That-There" concept for modern head-mounted
displays by pairing Large Language Models (LLMs) with XR sensor and tech stack.
The agent fuses (i) a semantically segmented 3-D environment, (ii) live
application metadata, and (iii) users' verbal, pointing, and head-gaze cues to
issue JSON window-placement actions. As a result, users can manage a panoramic
workspace through: (1) explicit commands ("Place Google Maps on the coffee
table"), (2) deictic speech plus gestures ("Put that there"), or (3) high-level
goals ("I need to send a message"). Unlike traditional explicit interfaces, our
system supports one-to-many action mappings and goal-centric reasoning,
allowing the LLM to dynamically infer relevant applications and layout
decisions, including interrelationships across tools. This enables seamless,
intent-driven interaction without manual window juggling in immersive XR
environments.

</details>


### [24] [Can Conversational AI Counsel for Change? A Theory-Driven Approach to Supporting Dietary Intentions in Ambivalent Individuals](https://arxiv.org/abs/2511.02428)
*Michelle Bak,Kexin Quan,Tre Tomaszewski,Jessie Chin*

Main category: cs.HC

TL;DR: 本研究开发的CounselLLM通过结合TTM和动机访谈有效提升个体饮食改变意图，证明了理论驱动的大型语言模型在数字化咨询中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 健康饮食的遵守可以降低慢性疾病风险，但目前的遵守率仍然较低。

Method: 开发CounselLLM，使用基于TTM和动机访谈的个性化设计和领域特定的提示进行增强。

Result: CounselLLM在控制评估中显示出比人类顾问更强的TTM过程和MI肯定使用，且在互动咨询设置中能显著提高参与者的饮食变化意图。

Conclusion: 理论驱动的大型语言模型可以有效地吸引犹豫不决的个体，并提供数字化咨询的可扩展方法。

Abstract: Adherence to healthy diets reduces chronic illness risk, yet rates remain
low. Large Language Models (LLMs) are increasingly used for health
communication but often struggle to engage individuals with ambivalent
intentions at a pivotal stage of the Transtheoretical Model (TTM). We developed
CounselLLM, an open-source model enhanced through persona design and few-shot,
domain-specific prompts grounded in TTM and Motivational Interviewing (MI). In
controlled evaluations, CounselLLM showed stronger use of TTM subprocesses and
MI affirmations than human counselors, with comparable linguistic robustness
but expressed in more concrete terms. A user study then tested CounselLLM in an
interactive counseling setting against a baseline system. While knowledge and
perceptions did not change, participants' intentions for immediate dietary
change increased significantly after interacting with CounselLLM. Participants
also rated it as easy to use, understandable, and supportive. These findings
suggest theory-driven LLMs can effectively engage ambivalent individuals and
provide a scalable approach to digital counseling.

</details>


### [25] [OpenCourier: an Open Protocol for Building a Decentralized Ecosystem of Community-owned Delivery Platforms](https://arxiv.org/abs/2511.02455)
*Yuhan Liu,Varun Nagaraj Rao,Sohyeon Hwang,Janet Vertesi,Andrés Monroy-Hernández*

Main category: cs.HC

TL;DR: OpenCourier 是一个开放协议，旨在创建一个去中心化的配送平台生态系统，解决工人与平台之间的权力不平等和信息不对称问题，促进工人主体性和透明度。


<details>
  <summary>Details</summary>
Motivation: 尽管平台共享经济重塑了工作性质，但其集中化操作导致了对工人福祉的挑战。

Method: 提出一个开放协议，以规范去中心化配送平台生态系统内的沟通模式。

Result: OpenCourier 旨在解决平台与工人之间的权力不平衡、黑箱算法造成的信息不对称以及基础设施设计过程中的价值不对齐问题。

Conclusion: OpenCourier 提供了一种社区拥有的配送平台生态系统的蓝图，强调工人的主体性、透明度和自下而上的设计。

Abstract: Although the platform gig economy has reshaped the landscape of work, its
centralized operation by select actors has brought about challenges that
impedes workers' well-being. We present the architecture and design of
OpenCourier, an open protocol that defines communication patterns within a
decentralized ecosystem of delivery platforms. Through this protocol, we aim to
address three key challenges in the current economy: power imbalances between
the platform and workers, information asymmetries caused by black-boxed
algorithms and value misalignments in the infrastructure design process. With
the OpenCourier protocol, we outline a blueprint for community-owned ecosystem
of delivery platforms that centers worker agency, transparency, and bottom-up
design.

</details>


### [26] [HAGI++: Head-Assisted Gaze Imputation and Generation](https://arxiv.org/abs/2511.02468)
*Chuhan Jiao,Zhiming Hu,Andreas Bulling*

Main category: cs.HC

TL;DR: HAGI++ 是一种新颖的多模态扩散方法，能够改善注视数据插补，通过整合头部和手腕运动数据，实现更准确的注视记录和分析。


<details>
  <summary>Details</summary>
Motivation: 解决因眨眼、瞳孔检测错误或光照变化导致的注视数据缺失问题，以提高注视数据分析的有效性。

Method: 采用基于变换器的扩散模型来学习眼睛和头部运动之间的跨模态依赖关系，并整合额外的身体运动。

Result: HAGI++ 在大型数据集上的评估结果显示其在注视插补方面优于传统插值方法和基于深度学习的时间序列插补基线。

Conclusion: HAGI++ 改进了人类注视数据的插补，提供了更完整和准确的注视记录，并在多个应用领域具有重要潜力。

Abstract: Mobile eye tracking plays a vital role in capturing human visual attention
across both real-world and extended reality (XR) environments, making it an
essential tool for applications ranging from behavioural research to
human-computer interaction. However, missing values due to blinks, pupil
detection errors, or illumination changes pose significant challenges for
further gaze data analysis. To address this challenge, we introduce HAGI++ - a
multi-modal diffusion-based approach for gaze data imputation that, for the
first time, uses the integrated head orientation sensors to exploit the
inherent correlation between head and eye movements. HAGI++ employs a
transformer-based diffusion model to learn cross-modal dependencies between eye
and head representations and can be readily extended to incorporate additional
body movements. Extensive evaluations on the large-scale Nymeria, Ego-Exo4D,
and HOT3D datasets demonstrate that HAGI++ consistently outperforms
conventional interpolation methods and deep learning-based time-series
imputation baselines in gaze imputation. Furthermore, statistical analyses
confirm that HAGI++ produces gaze velocity distributions that closely match
actual human gaze behaviour, ensuring more realistic gaze imputations.
Moreover, by incorporating wrist motion captured from commercial wearable
devices, HAGI++ surpasses prior methods that rely on full-body motion capture
in the extreme case of 100% missing gaze data (pure gaze generation). Our
method paves the way for more complete and accurate eye gaze recordings in
real-world settings and has significant potential for enhancing gaze-based
analysis and interaction across various application domains.

</details>


### [27] [Emotional Contagion in Code: How GitHub Emoji Reactions Shape Developer Collaboration](https://arxiv.org/abs/2511.02515)
*Obada Kraishan*

Main category: cs.HC

TL;DR: 开发者社区表情符号反应影响技术讨论情感传播，57.4%的讨论呈积极情绪，结果挑战了技术讨论纯理性的假设。


<details>
  <summary>Details</summary>
Motivation: 开发者社区越来越依赖表情符号反应进行沟通，但关于这些情感信号如何传播和影响技术讨论的了解仍然有限。

Method: 分析了50个流行代码库中的2098个GitHub问题和拉取请求，研究了106,743个表情符号反应的模式。

Result: 57.4%的讨论具有积极情绪，积极情感传播与消极情感传播的比例为23:1，初始反应显著影响讨论轨迹。

Conclusion: 本研究表明，表情符号反应不仅仅是装饰，而是积极影响软件开发中协作结果的重要力量。

Abstract: Developer communities increasingly rely on emoji reactions to communicate,
but we know little about how these emotional signals spread and influence
technical discussions. We analyzed 2,098 GitHub issues and pull requests across
50 popular repositories, examining patterns in 106,743 emoji reactions to
understand emotional contagion in software development. Our findings reveal a
surprisingly positive emotional landscape: 57.4% of discussions carry positive
sentiment, with positive emotional cascades outnumbering negative ones 23:1. We
identified five distinct patterns, with "instant enthusiasm" affecting 45.6% of
items--nearly half receive immediate positive reinforcement. Statistical
analysis confirms strong emotional contagion (r=0.679, p<0.001) with a massive
effect size (d=2.393), suggesting that initial reactions powerfully shape
discussion trajectories. These findings challenge assumptions about technical
discourse being purely rational, demonstrating that even minimal emotional
signals create measurable ripple effects. Our work provides empirical evidence
that emoji reactions are not mere decoration but active forces shaping
collaborative outcomes in software development.

</details>


### [28] [SigmaCollab: An Application-Driven Dataset for Physically Situated Collaboration](https://arxiv.org/abs/2511.02560)
*Dan Bohus,Sean Andrist,Ann Paradiso,Nick Saw,Tim Schoonbeek,Maia Stiber*

Main category: cs.HC

TL;DR: SigmaCollab是一个新的人机协作数据集，包含丰富的多模态数据，旨在促进混合现实中的AI研究。


<details>
  <summary>Details</summary>
Motivation: 为了研究人机协作中新的挑战及其在混合现实中的应用，创建一个实际可用的数据集。

Method: 该研究通过混合现实助理AI代理指导未经训练的参与者完成物理世界中的程序任务，收集了多模态的数据流。

Result: SigmaCollab数据集包含85个会话和大约14小时的多模态数据，具有丰富的交互性和应用导向。

Conclusion: SigmaCollab数据集为物理环境下的人机协作研究提供了丰富的资源，并将促进AI模型在真实应用中进行更有效的测试与评估。

Abstract: We introduce SigmaCollab, a dataset enabling research on physically situated
human-AI collaboration. The dataset consists of a set of 85 sessions in which
untrained participants were guided by a mixed-reality assistive AI agent in
performing procedural tasks in the physical world. SigmaCollab includes a set
of rich, multimodal data streams, such as the participant and system audio,
egocentric camera views from the head-mounted device, depth maps, head, hand
and gaze tracking information, as well as additional annotations performed
post-hoc. While the dataset is relatively small in size (~ 14 hours), its
application-driven and interactive nature brings to the fore novel research
challenges for human-AI collaboration, and provides more realistic testing
grounds for various AI models operating in this space. In future work, we plan
to use the dataset to construct a set of benchmarks for physically situated
collaboration in mixed-reality task assistive scenarios. SigmaCollab is
available at https://github.com/microsoft/SigmaCollab.

</details>


### [29] [DropleX: Liquid sensing on tablet touchscreens](https://arxiv.org/abs/2511.02694)
*Siqi Zhang,Mayank Goel,Justin Chan*

Main category: cs.HC

TL;DR: DropleX是首个利用平板电脑触摸屏实现液体监测的系统，能够高效检测液体掺杂和污染，准确率高达99%。


<details>
  <summary>Details</summary>
Motivation: 为了实现通过普通平板电脑的电容触摸屏进行液体传感，检测微升液体样本是否被掺杂或污染。

Method: 利用物理启发机制禁用触摸屏的自适应过滤器，并通过信号处理和学习管道进行液体传感。

Result: DropleX系统在各种液体样本中检测掺杂和化学浓度的精确度分别为96-99%、93-96%及86-96%。

Conclusion: DropleX系统在日常设备上实现了液体传感的新机会，其检测精度高达96-99%。

Abstract: We present DropleX, the first system that enables liquid sensing using the
capacitive touchscreen of commodity tablets. DropleX detects microliter-scale
liquid samples, and performs non-invasive, through-container measurements to
detect whether a drink has been spiked or if a sealed liquid has been
contaminated. These capabilities are made possible by a physics-informed
mechanism that disables the touchscreen's built-in adaptive filters, originally
designed to reject the effects of liquid drops such as rain, without any
hardware modifications. We model the touchscreen's sensing capabilities,
limits, and non-idealities to inform the design of a signal processing and
learning-based pipeline for liquid sensing. Our system achieves 96-99% accuracy
in detecting microliter-scale adulteration in soda, wine, and milk, 93-96%
accuracy in threshold detection of trace chemical concentrations, and 86-96%
accuracy in through-container adulterant detection. Given the predominance of
touchscreens, these exploratory results can open new opportunities for liquid
sensing on everyday devices.

</details>


### [30] [Audience Amplified: Virtual Audiences in Asynchronously Performed AR Theater](https://arxiv.org/abs/2511.02807)
*You-Jin Kim,Misha Sra,Tobias Höllerer*

Main category: cs.HC

TL;DR: 本研究探讨了在增强现实舞蹈表演中虚拟观众的影响，结果显示虚拟观众能增强社交体验，但在某些情况下，缺乏虚拟观众反而带来更强的积极情感。


<details>
  <summary>Details</summary>
Motivation: 在任何时候都能享受增强现实体验，但大规模活动的观众可能无法实时聚集，因此需要提供一种替代方案以模拟大型活动的氛围。

Method: 进行了小规模研究，通过问卷和体验报告比较了有无虚拟观众的AR舞蹈表演对20名参与者的影响。

Result: 参与者在有虚拟观众的情况下表现出更强的兴趣和参与感，而一些参与者在没有虚拟观众时反而感受到更积极的情感。

Conclusion: 虚拟观众的存在增强了观众对表演的社交体验感，而在没有虚拟观众的情况下，一些观众则体验到更强的积极情感。

Abstract: Audience reactions can considerably enhance live experiences; conversely, in
anytime-anywhere augmented reality (AR) experiences, large crowds of people
might not always be available to congregate. To get closer to simulating live
events with large audiences, we created a mobile AR experience where users can
wander around naturally and engage in AR theater with virtual audiences trained
from real audiences using imitation learning. This allows us to carefully
capture the essence of human imperfections and behavior in artificial
intelligence (AI) audiences. The result is a novel mobile AR experience in
which solitary AR users experience an augmented performance in a physical space
with a virtual audience. Virtual dancers emerge from the surroundings,
accompanied by a digitally simulated audience, to provide a community
experience akin to immersive theater. In a pilot study, simulated human avatars
were vastly preferred over just audience audio commentary. We subsequently
engaged 20 participants as attendees of an AR dance performance, comparing a
no-audience condition with a simulated audience of six onlookers. Through
questionnaires and experience reports, we investigated user reactions and
behavior. Our results demonstrate that the presence of virtual audience members
caused attendees to perceive the performance as a social experience with
increased interest and involvement in the event. On the other hand, for some
attendees, the dance performances without the virtual audience evoked a
stronger positive sentiment.

</details>
