<div id=toc></div>

# Table of Contents

- [cs.HC](#cs.HC) [Total: 23]
- [cs.RO](#cs.RO) [Total: 52]


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [1] [AdvisingWise: Supporting Academic Advising in Higher Educations Through a Human-in-the-Loop Multi-Agent Framework](https://arxiv.org/abs/2511.05706)
*Wendan Jiang,Shiyuan Wang,Hiba Eltigani,Rukhshan Haroon,Abdullah Bin Faisal,Fahad Dogar*

Main category: cs.HC

TL;DR: AdvisingWise 是一个自动化学术指导系统，利用 LLM 提高咨询效率，经过评估显示能够产生可靠的个性化回复，增强了顾问的积极性。


<details>
  <summary>Details</summary>
Motivation: 高学生与顾问比率限制了顾问提供及时支持的能力，特别是在高峰期。

Method: 通过混合方法评估 AdvisingWise，包括专家评估、信息检索策略的 LLM 评判和用户研究。

Result: AdvisingWise 成功地自动化了信息检索和回复起草，产生准确的个性化回复，且经过人类顾问验证。

Conclusion: 人机协同在学术指导实践中的影响值得深入探讨，AdvisingWise 能够产生准确、个性化的回复，且提高了顾问的积极性。

Abstract: Academic advising is critical to student success in higher education, yet high student-to-advisor ratios limit advisors' capacity to provide timely support, particularly during peak periods. Recent advances in Large Language Models (LLMs) present opportunities to enhance the advising process. We present AdvisingWise, a multi-agent system that automates time-consuming tasks, such as information retrieval and response drafting, while preserving human oversight. AdvisingWise leverages authoritative institutional resources and adaptively prompts students about their academic backgrounds to generate reliable, personalized responses. All system responses undergo human advisor validation before delivery to students. We evaluate AdvisingWise through a mixed-methods approach: (1) expert evaluation on responses of 20 sample queries, (2) LLM-as-a-judge evaluation of the information retrieval strategy, and (3) a user study with 8 academic advisors to assess the system's practical utility. Our evaluation shows that AdvisingWise produces accurate, personalized responses. Advisors reported increasingly positive perceptions after using AdvisingWise, as their initial concerns about reliability and personalization diminished. We conclude by discussing the implications of human-AI synergy on the practice of academic advising.

</details>


### [2] [Home Environment and Student Creative Thinking: An Educational Data Science Analysis of PISA 2022](https://arxiv.org/abs/2511.05737)
*George X. Wang,Yuyang Shen*

Main category: cs.HC

TL;DR: 家庭环境中的文化、教育与数字资源与学生创造力表现密切相关，物理和数字刺激对创造性思维均有独立贡献。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨家庭环境对学生创造性思维表现的影响，特别关注文化、教育和数字资源的作用。

Method: 通过对来自60个国家的15425名学生的数据分析，应用高维回归和验证性因子分析，识别了家庭环境资源的潜在结构。

Result: 本研究表明，家庭环境中的物理和数字资源对青少年创造力有显著影响，二者在促进创造性思维的能力方面各自独立且互补。

Conclusion: 本研究强调，在家庭环境中促进创造性思维需要同时重视物理和数字资源的提供，二者是互为补充的因素。

Abstract: This study investigates how student exposure to resources in their home environments relates to creative thinking performance, using data from the PISA 2022 Creative Thinking assessment. It focuses on two primary questions: (1) How strongly is exposure to cultural, educational, and digital resources associated with creativity? (2) Do students perform better on divergent thinking tasks when physically engaged or digitally stimulated? Drawing on a sample of 15,425 students from 60 countries, the study applies high-dimensional regression and factor analysis to identify patterns across a wide range of exposure variables. To model the latent structure of home environment variables, we conducted a Confirmatory Factor Analysis. The analysis specified two latent factors: Physical Exposure and Digital Exposure. The model demonstrated excellent fit, with a Comparative Fit Index (CFI) of 0.971 and a Root Mean Square Error of Approximation (RMSEA) of 0.038. When both factors were entered together in the regression, physical and digital exposures each contributed unique explanatory power. There is no indication that one simply proxies the other; rather, they appear to be complementary dimensions of a creative home environment. This study offers compelling international evidence that both physical and digital resources in the home environment play significant, independent, and complementary roles in shaping adolescent creative thinking abilities. These findings have direct implications for efforts to promote creativity and equity in education.

</details>


### [3] [Adaptive Time Budgets for Safe and Comfortable Vehicle Control Transition in Conditionally Automated Driving](https://arxiv.org/abs/2511.05744)
*Kexin Liang,Simeon C. Calvert,J. W. C. van Lint*

Main category: cs.HC

TL;DR: 该研究通过驾驶模拟实验提出了一种适应性时间预算框架，发现5-6秒的接管缓冲时间有利于提升自动驾驶的安全和舒适性。


<details>
  <summary>Details</summary>
Motivation: 确保驾驶员在条件自动驾驶中能够及时恢复控制，以应对复杂的交通行为并提高混合交通系统的安全性与效率。

Method: 通过驾驶模拟器进行实验，评估不同接管缓冲时间对安全指标（如最小碰撞时间、最大减速度、方向盘角度）及主观评估（如时间充足感、风险感知和表现满意度）的影响。

Result: 该研究提出了一个适应性时间预算框架，以确保在条件自动驾驶中，驾驶员能够安全舒适地恢复对车辆的控制。通过驾驶模拟实验，评估了不同接管缓冲时间对安全指标和主观评估的影响。结果显示，5-6秒的接管缓冲时间最优，并且驾驶员倾向于保持相对稳定的接管缓冲时间。该框架结合了预测的接管时间和首选的接管缓冲，以动态分配过渡时间，这为自动驾驶的安全性和效率提供了新的思路。

Conclusion: 适应性时间预算框架可以根据特定情况下驾驶员的需求动态调整接管时间，从而提高控制过渡的可靠性并减少碰撞风险。

Abstract: Conditionally automated driving requires drivers to resume vehicle control promptly when automation reaches its operational limits. Ensuring smooth vehicle control transitions is critical for the safety and efficiency of mixed-traffic transportation systems, where complex interactions and variable traffic behaviors pose additional challenges. This study addresses this challenge by introducing an adaptive time budget framework that provides drivers with sufficient time to complete takeovers both safely and comfortably across diverse scenarios. We focus in particular on the takeover buffer, that is, the extra time available after drivers consciously resume control to complete evasive maneuvers. A driving simulator experiment is conducted to evaluate the influence of different takeover buffer lengths on safety-related indicators (minimum time-to-collision, maximum deceleration, and steering wheel angle) and subjective assessments (perceived time sufficiency, perceived risk, and performance satisfaction). Results show that (i) takeover buffers of about 5-6 seconds consistently lead to optimal safety and comfort; and (ii) drivers prefer relatively stable takeover buffers across varying traffic densities and n-back tasks. This study introduces an adaptive time budget framework that dynamically allocates transition time by incorporating a predicted takeover time and a preferred takeover buffer (piecewise function). This can serve as an important first step toward providing drivers with sufficient time to resume vehicle control across diverse scenarios, which needs to be validated in more diverse and real-world driving contexts. By aligning the provided time budget with driver needs under specific circumstances, the adaptive framework can improve reliability of control transitions, facilitate human-centered automated driving, reduce crash risk, and maintain overall traffic efficiency.

</details>


### [4] [Lived Experience in Dialogue: Co-designing Personalization in Large Language Models to Support Youth Mental Well-being](https://arxiv.org/abs/2511.05769)
*Kathleen W. Guan,Sarthak Giri,Mohammed Amara,Bernard J. Jansen,Enrico Liscio,Milena Esherick,Mohammed Al Owayyed,Ausrine Ratkute,Gayane Sedrakyan,Mark de Reuver,Joao Fernando Ferreira Goncalves,Caroline A. Figueroa*

Main category: cs.HC

TL;DR: 本研究探讨了如何通过社区视角促进大型语言模型的个性化设计，以更有效地支持青少年的心理健康。


<details>
  <summary>Details</summary>
Motivation: 由于青少年越来越依赖大型语言模型来支持心理健康，当前的个性化可能忽视了影响其需求的多样化生活经验。

Method: 进行了一项参与性研究，参与者包括青少年、家长和青少年护理工作者，共38人。

Result: 分析识别出三大主题，包括以人为本的情境化、明确的范围和离线转诊界限，以及促进反思和自主权的对话性支架，进而与劝服设计特征相结合。

Conclusion: 通过考虑青少年的生活经验，可以改进大型语言模型的个性化设计，以更有效地支持其心理健康。

Abstract: Youth increasingly turn to large language models (LLMs) for mental well-being support, yet current personalization in LLMs can overlook the heterogeneous lived experiences shaping their needs. We conducted a participatory study with youth, parents, and youth care workers (N=38), using co-created youth personas as scaffolds, to elicit community perspectives on how LLMs can facilitate more meaningful personalization to support youth mental well-being. Analysis identified three themes: person-centered contextualization responsive to momentary needs, explicit boundaries around scope and offline referral, and dialogic scaffolding for reflection and autonomy. We mapped these themes to persuasive design features for task suggestions, social facilitation, and system trustworthiness, and created corresponding dialogue extracts to guide LLM fine-tuning. Our findings demonstrate how lived experience can be operationalized to inform design features in LLMs, which can enhance the alignment of LLM-based interventions with the realities of youth and their communities, contributing to more effectively personalized digital well-being tools.

</details>


### [5] [TalkSketch: Multimodal Generative AI for Real-time Sketch Ideation with Speech](https://arxiv.org/abs/2511.05817)
*Weiyan Shi,Sunaya Upadhyay,Geraldine Quek,Kenny Tsu Wei Choo*

Main category: cs.HC

TL;DR: 提出了TalkSketch，一个结合手绘与语音输入的多模态AI草图系统，以改善设计师在创意生成过程中的流畅性和表达能力。


<details>
  <summary>Details</summary>
Motivation: 设计师在使用生成式AI进行创意生成时常面临文本提示不够有效和难以表达视觉概念的挑战，因此需要一种更灵活的工具来支持设计过程。

Method: 通过对6名设计师的形成性研究，探讨他们在构思时如何使用生成式AI，并分析了文本提示对创造性流程的影响。

Result: 本研究提出的TalkSketch系统通过将自由手绘与实时语音输入相结合，改善了设计师在构思阶段与生成式AI聊天机器人交互时的体验。

Conclusion: TalkSketch系统展示了生成式AI工具在设计过程中参与创造性思维的潜力，而不仅仅是关注最终产品。

Abstract: Sketching is a widely used medium for generating and exploring early-stage design concepts. While generative AI (GenAI) chatbots are increasingly used for idea generation, designers often struggle to craft effective prompts and find it difficult to express evolving visual concepts through text alone. In the formative study (N=6), we examined how designers use GenAI during ideation, revealing that text-based prompting disrupts creative flow. To address these issues, we developed TalkSketch, an embedded multimodal AI sketching system that integrates freehand drawing with real-time speech input. TalkSketch aims to support a more fluid ideation process through capturing verbal descriptions during sketching and generating context-aware AI responses. Our work highlights the potential of GenAI tools to engage the design process itself rather than focusing on output.

</details>


### [6] [Towards a Humanized Social-Media Ecosystem: AI-Augmented HCI Design Patterns for Safety, Agency & Well-Being](https://arxiv.org/abs/2511.05875)
*Mohd Ruhul Ameen,Akif Islam*

Main category: cs.HC

TL;DR: 提出了一种名为人类层AI（HL-AI）的技术，旨在增强用户在社交平台中的控制力，通过五种代表性框架和数学公式实现安全与 agency。


<details>
  <summary>Details</summary>
Motivation: 社交平台的算法往往只关注用户的参与度，并在某种程度上操控用户，导致用户面临压力、错误信息和失去控制感。

Method: 通过开发一个原型并实现五个主要功能框架，结合用户效用、自治成本和风险阈值的数学平衡，评估其技术准确性、可用性和行为结果。

Result: 提出了一种用户自有的、可解释的中介技术，旨在为用户提供实时控制。这一原型在Chrome和Edge浏览器中实现，帮助用户在面对潜在危害时采取预防措施。

Conclusion: 所提供的原型为今日的社交平台提供了一种可行的安全和控制解决方案，能够在文化多样性中进行用户评估。

Abstract: Social platforms connect billions of people, yet their engagement-first algorithms often work on users rather than with them, amplifying stress, misinformation, and a loss of control. We propose Human-Layer AI (HL-AI)--user-owned, explainable intermediaries that sit in the browser between platform logic and the interface. HL-AI gives people practical, moment-to-moment control without requiring platform cooperation. We contribute a working Chrome/Edge prototype implementing five representative pattern frameworks--Context-Aware Post Rewriter, Post Integrity Meter, Granular Feed Curator, Micro-Withdrawal Agent, and Recovery Mode--alongside a unifying mathematical formulation balancing user utility, autonomy costs, and risk thresholds. Evaluation spans technical accuracy, usability, and behavioral outcomes. The result is a suite of humane controls that help users rewrite before harm, read with integrity cues, tune feeds with intention, pause compulsive loops, and seek shelter during harassment, all while preserving agency through explanations and override options. This prototype offers a practical path to retrofit today's feeds with safety, agency, and well-being, inviting rigorous cross-cultural user evaluation.

</details>


### [7] [Pinching Visuo-haptic Display: Investigating Cross-Modal Effects of Visual Textures on Electrostatic Cloth Tactile Sensations](https://arxiv.org/abs/2511.05952)
*Takekazu Kitagishi,Chun-Wei Ooi,Yuichi Hiroi,Jun Rekimoto*

Main category: cs.HC

TL;DR: 本研究探讨了视觉纹理对触觉感知的影响，发现视觉粗糙度增强了感知到的摩擦力。


<details>
  <summary>Details</summary>
Motivation: 研究视觉纹理如何影响用电静态布料显示器进行的触觉感知。

Method: 通过用户研究考察视觉粗糙度与触觉摩擦之间的跨模态效应。

Result: 提出了一种可视-触觉系统，使用户能够捏和擦拭虚拟织物，同时感受到由电静态激活调制的真实摩擦感。

Conclusion: 视觉粗糙纹理增强了摩擦感，即使电静态刺激相同，这一发现为多模态纹理感知提供了新的理解，并为虚拟材料界面的触觉反馈设计提供了见解。

Abstract: This paper investigates how visual texture presentation influences tactile perception when interacting with electrostatic cloth displays. We propose a visuo-haptic system that allows users to pinch and rub virtual fabrics while feeling realistic frictional sensations modulated by electrostatic actuation. Through a user study, we examined the cross-modal effects between visual roughness and perceived tactile friction. The results demonstrate that visually rough textures amplify the perceived frictional force, even under identical electrostatic stimuli. These findings contribute to the understanding of multimodal texture perception and provide design insights for haptic feedback in virtual material interfaces.

</details>


### [8] [Towards Misinformation Resilience in Pakistan: A Participatory Study with Low-Socioeconomic Status Adults](https://arxiv.org/abs/2511.06147)
*Muhammad Abdullah Sohail,Amna Hassan,Shaheer Hammad,Salaar Masood,Suleman Shahid*

Main category: cs.HC

TL;DR: 数字错误信息对低社会经济地位（SES）人群影响显著，研究基于巴基斯坦低SES成年人，开发出支持模型以增强信息弹性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解低SES人群的信息习惯，以应对数字错误信息对其的影响。

Method: 通过三阶段的参与式研究，包括形成性访谈和共同设计会议，明确用户需求并转化为设计要求。

Result: 开发了Pehchaan原型，并进行可用性测试，结果显示其文化适应性强，用户接受度高，验证了基于文化的设计方法。

Conclusion: 研究证明了以用户为中心的方法在低SES背景下的有效性，并提出可重复的设计方法和信息韧性原则。

Abstract: Digital misinformation disproportionately affects low-socioeconomic status (SES) populations. While interventions for the Global South exist, they often report limited success, particularly among marginalized communities. Through a three-phase participatory study with 41 low-SES Pakistani adults, we conducted formative interviews to understand their information practices, followed by co-design sessions that translated these user-identified needs into concrete design requirements. Our findings reveal a sophisticated moral economy of sharing and a layered ecology of trust that prioritizes communal welfare. These insights inform the Scaffolded Support Model, a user-derived framework integrating on-demand assistance with gradual, inoculation-based skill acquisition. We instantiated this model in our prototype, "Pehchaan," and conducted usability testing (N=15), which confirmed its strong acceptance and cultural resonance, validating our culturally grounded approach. Our work contributes a foundational empirical account of non-Western misinformation practices, a replicable participatory methodology for inclusive design, and actionable principles for building information resilience in resource-constrained contexts.

</details>


### [9] [AI as intermediary in modern-day ritual: An immersive, interactive production of the roller disco musical Xanadu at UCLA](https://arxiv.org/abs/2511.06195)
*Mira Winick,Naisha Agarwal,Chiheb Boussema,Ingrid Lee,Camilo Vargas,Jeff Burke*

Main category: cs.HC

TL;DR: 本论文探讨了以仪式为框架的多用户人机互动，展示了AI在支持群体创造力和游戏中的作用，填补了单用户AI设计的空白。


<details>
  <summary>Details</summary>
Motivation: 当前大多数AI接口设计仅面向单用户，缺乏对多用户协作和创造力的考虑。

Method: 通过将仪式作为设计框架，分析了在2025年UCLA举办的音乐剧《Xanadu》中，观众互动如何影响AI生成的虚拟场景和舞蹈元素。

Result: 研究展示了观众的输入可以作为AI转化成仪式元素，体现了人机之间的互惠互动及其如何重塑虚拟空间。

Conclusion: 本研究探讨了在大型语言和生成媒体模型中，通过仪式框架促进人机多用户互动的可能性，强调了AI如何在组合性创作中发挥作用。

Abstract: Interfaces for contemporary large language, generative media, and perception AI models are often engineered for single user interaction. We investigate ritual as a design scaffold for developing collaborative, multi-user human-AI engagement. We consider the specific case of an immersive staging of the musical Xanadu performed at UCLA in Spring 2025. During a two-week run, over five hundred audience members contributed sketches and jazzercise moves that vision language models translated to virtual scenery elements and from choreographic prompts. This paper discusses four facets of interaction-as-ritual within the show: audience input as offerings that AI transforms into components of the ritual; performers as ritual guides, demonstrating how to interact with technology and sorting audience members into cohorts; AI systems as instruments "played" by the humans, in which sensing, generative components, and stagecraft create systems that can be mastered over time; and reciprocity of interaction, in which the show's AI machinery guides human behavior as well as being guided by humans, completing a human-AI feedback loop that visibly reshapes the virtual world. Ritual served as a frame for integrating linear narrative, character identity, music and interaction. The production explored how AI systems can support group creativity and play, addressing a critical gap in prevailing single user AI design paradigms.

</details>


### [10] [Decomate: Leveraging Generative Models for Co-Creative SVG Animation](https://arxiv.org/abs/2511.06297)
*Jihyeon Park,Jiyoon Myung,Seone Shin,Jungki Son,Joohyung Han*

Main category: cs.HC

TL;DR: Decomate 是一款基于自然语言的 SVG 动画工具，通过多模态语言模型重组 SVG 结构，简化动画创建过程，支持用户进行意图驱动的动画设计。


<details>
  <summary>Details</summary>
Motivation: 设计师在对静态 SVG 图形进行动画处理时常常遇到困难，尤其是在视觉结构与所需运动细节不匹配时，因此需要一种新的工具来提升其独立实验和迭代的能力。

Method: 使用多模态大语言模型将原始 SVG 结构重组为语义上有意义且适合动画的组件，并允许设计师通过文本提示指定各个组件的运动。

Result: 设计师能够通过自然语言交互进行迭代优化，使动画结果能直接反映用户的意图，促进创意工作流程中生成性 AI 的集成。

Conclusion: Decomate 为设计师提供了一种直观、基于自然语言的 SVG 动画创建方式，通过多模态大语言模型重构 SVG 图形，使其能够生成动画。

Abstract: Designers often encounter friction when animating static SVG graphics, especially when the visual structure does not match the desired level of motion detail. Existing tools typically depend on predefined groupings or require technical expertise, which limits designers' ability to experiment and iterate independently. We present Decomate, a system that enables intuitive SVG animation through natural language. Decomate leverages a multimodal large language model to restructure raw SVGs into semantically meaningful, animation-ready components. Designers can then specify motions for each component via text prompts, after which the system generates corresponding HTML/CSS/JS animations. By supporting iterative refinement through natural language interaction, Decomate integrates generative AI into creative workflows, allowing animation outcomes to be directly shaped by user intent.

</details>


### [11] [Personality over Precision: Exploring the Influence of Human-Likeness on ChatGPT Use for Search](https://arxiv.org/abs/2511.06447)
*Mert Yazan,Frederik Bungaran Ishak Situmeang,Suzan Verberne*

Main category: cs.HC

TL;DR: 本研究揭示了用户在对话搜索系统中倾向于相信和使用ChatGPT的原因，突出个性化和人性化在用户信任与使用决策中的重要性，也反映了用户在使用中可能存在的过度信任问题。


<details>
  <summary>Details</summary>
Motivation: 为了理解对话界面的采用因素，以及它如何创造个性化体验并导致过度信任的问题，因此展开了此研究。

Method: 通过对173名参与者进行调查，分析用户对信任、人性化和设计偏好的看法，并探讨用户在事实准确性与易用性、人性化之间的取舍意愿。

Result: DUB组用户更信任ChatGPT，倾向于以个性化和对话流畅性为重，而DUG组则对ChatGPT的信任度较低，但仍青睐无广告和响应互动。

Conclusion: 用户在使用对话搜索界面时更倾向于牺牲事实准确性以获得更具个性化和互动性的体验，尤其是对ChatGPT表现出更高的信任和人性化感知。这一研究强调了个性化和人性化在对话搜索系统中的重要性。

Abstract: Conversational search interfaces, like ChatGPT, offer an interactive, personalized, and engaging user experience compared to traditional search. On the downside, they are prone to cause overtrust issues where users rely on their responses even when they are incorrect. What aspects of the conversational interaction paradigm drive people to adopt it, and how it creates personalized experiences that lead to overtrust, is not clear. To understand the factors influencing the adoption of conversational interfaces, we conducted a survey with 173 participants. We examined user perceptions regarding trust, human-likeness (anthropomorphism), and design preferences between ChatGPT and Google. To better understand the overtrust phenomenon, we asked users about their willingness to trade off factuality for constructs like ease of use or human-likeness. Our analysis identified two distinct user groups: those who use both ChatGPT and Google daily (DUB), and those who primarily rely on Google (DUG). The DUB group exhibited higher trust in ChatGPT, perceiving it as more human-like, and expressed greater willingness to trade factual accuracy for enhanced personalization and conversational flow. Conversely, the DUG group showed lower trust toward ChatGPT but still appreciated aspects like ad-free experiences and responsive interactions. Demographic analysis further revealed nuanced patterns, with middle-aged adults using ChatGPT less frequently yet trusting it more, suggesting potential vulnerability to misinformation. Our findings contribute to understanding user segmentation, emphasizing the critical roles of personalization and human-likeness in conversational IR systems, and reveal important implications regarding users' willingness to compromise factual accuracy for more engaging interactions.

</details>


### [12] [Towards Attention-Aware Large Language Models: Integrating Real-Time Eye-Tracking and EEG for Adaptive AI Responses](https://arxiv.org/abs/2511.06468)
*Dan Zhang*

Main category: cs.HC

TL;DR: 该项目开发了一种关注度感知的LLM，通过整合EEG和眼动追踪，实现动态监测用户注意力。


<details>
  <summary>Details</summary>
Motivation: 该项目的动机是改善用户参与度，同时减轻认知负担，通过识别和响应用户的不同注意力状态。

Method: 项目方法是实时整合EEG和眼动追踪数据，并在基于LLM的互动系统中即时分类用户的注意力状态。

Result: 系统能够识别五种注意力状态，并针对每种状态做出相应反应，尤其关注如何处理注意力下降、干扰和认知超载的情况。

Conclusion: 该项目提出了一种关注度感知的大型语言模型（LLM），通过整合脑电图（EEG）和眼动追踪技术动态监测和测量用户注意力。

Abstract: This project proposes an attention-aware LLM that integrates EEG and eye tracking to monitor and measure user attention dynamically. To realize this, the project will integrate real-time EEG and eye-tracking data into an LLM-based interactive system and classify the user's attention state on the fly. The system can identify five attention states: High Attention, Stable Attention, Dropping Attention, Cognitive Overload, and Distraction. It responds accordingly to each state, with a particular focus on adapting to decreased attention, distraction, and cognitive overload to improve user engagement and reduce cognitive load.

</details>


### [13] [HugSense: Exploring the Sensing Capabilities of Inflatables](https://arxiv.org/abs/2511.06532)
*Klaus Stephan,Maximilian Eibl,Albrecht Kurze*

Main category: cs.HC

TL;DR: 本文探讨了如何利用充气物体的传感能力，提出将充气枕头转变为力传感器的方案，并展示其应用的潜力。


<details>
  <summary>Details</summary>
Motivation: HCI领域已经广泛使用充气物体作为执行器，但其传感能力却未得到充分利用。

Method: 通过结合充气装置和空气压力传感器，构建一个充气枕头的力传感器。

Result: 该充气装置能够提供准确且有趣的数据，显示出其作为传感器的有效性和应用潜力。

Conclusion: 充气物体作为传感器的应用潜力值得深入研究，并可能在实际应用中发挥重要作用。

Abstract: What information can we get using inflatables as sensors? While using inflatables as actuators for various interactions has been widely adopted in the HCI community, using the sensing capabilities of inflatables is much less common. Almost all inflatable setups include air pressure sensors as part of the automation when pressurizing or deflating, but the full potential of those sensors is rarely explored. This paper shows how to turn a complete pillow into a force sensor using an inflatable and a simple pneumatics setup including an air pressure sensor. We will show that this setup yields accurate and interesting data that warrants further exploration and elaborate on the potential for practical applications.

</details>


### [14] [Accessibility Gaps in U.S. Government Dashboards for Blind and Low-Vision Residents](https://arxiv.org/abs/2511.06688)
*Chadani Acharya*

Main category: cs.HC

TL;DR: 对美国政府公共仪表盘的审计显示其在可访问性方面存在不足，特别是对辅助技术的支持和数据的可读性。


<details>
  <summary>Details</summary>
Motivation: 通过提高公共仪表盘的可访问性，确保所有居民，特别是残障人士能够平等参与并满足ADA Title II对可访问性的要求。

Method: 基于屏幕阅读器需求和WCAG指南的评估标准，审核了六个政府公共仪表盘，检查主要指标的发现性、键盘无鼠标访问、清晰的语义标签、简洁的语言状态和趋势说明，以及数据的机器可读性。

Result: 审计发现多个仪表盘在可发现性和可读性方面存在问题，尤其是与辅助技术的兼容性，此外，加速运营仪表盘的可访问性通常低于慢速责任仪表盘。

Conclusion: 为了增强公众仪表盘的可访问性，建议增加状态和趋势说明、发布匹配的数据表或CSV、以及明确的可访问性承诺。

Abstract: Public dashboards are now a common way for US government agencies to share high stakes information with residents. We audited six live systems at federal, state, and city levels: CDC respiratory illness, HUD homelessness PIT and HIC, California HCD Annual Progress Report, New York City Mayor's Management Report, Houston Permitting, and Chicago public health and budget dashboards. Using a rubric based on screen reader needs and WCAG, we checked five items: (1) discoverability of key metrics by assistive tech, (2) keyboard access without mouse hover, (3) clear semantic labels for axes, series, and categories, (4) short plain language status and trend notes, and (5) machine readable tables or CSVs that mirror what sighted users see. Findings are mixed. Many charts fail basic discoverability or depend on hover, which blocks keyboard and screen reader use. Plain language summaries are common in CDC and Chicago, but rare in HUD and Houston. Machine readable data is strong for NYC, California, and HUD; it is weaker or unclear for Houston. Several sites promise service for the public or for customers yet do not name accessibility in their descriptions. Across systems we also observe urgency inversion: faster, operational dashboards tend to provide fewer accessible affordances than slower accountability dashboards. These patterns matter for equal participation and for ADA Title II compliance that references WCAG 2.1 AA. We propose three steps for any public dashboard: add a brief status and trend text at the same update cadence, publish a matching table or CSV of the visual metrics, and state an explicit accessibility commitment.

</details>


### [15] [HEDN: A Hard-Easy Dual Network with Task Difficulty Assessment for EEG Emotion Recognition](https://arxiv.org/abs/2511.06782)
*Qiang Wang,Liying Yang*

Main category: cs.HC

TL;DR: 通过HEDN网络识别源领域的转移难度，实现高效的EEG情绪识别，达成93.58%和79.82%的准确率。


<details>
  <summary>Details</summary>
Motivation: 应对跨学科EEG情绪识别中的个体差异，提升源领域到目标领域的适应性。

Method: 利用任务难度评估机制(TDA)动态识别源领域的难度，并建立专门的知识适应分支。

Result: 提出了HEDN，分别对困难源和简单源进行知识迁移，显著提高了情绪识别的准确率。

Conclusion: HEDN在跨学科EEG情绪识别上展现了优越的性能和广泛的适用性。

Abstract: Multi-source domain adaptation represents an effective approach to addressing individual differences in cross-subject EEG emotion recognition. However, existing methods treat all source domains equally, neglecting the varying transfer difficulties between different source domains and the target domain. This oversight can lead to suboptimal adaptation. To address this challenge, we propose a novel Hard-Easy Dual Network (HEDN), which dynamically identifies "Hard Source" and "Easy Source" through a Task Difficulty Assessment (TDA) mechanism and establishes two specialized knowledge adaptation branches. Specifically, the Hard Network is dedicated to handling "Hard Source" with higher transfer difficulty by aligning marginal distribution differences between source and target domains. Conversely, the Easy Network focuses on "Easy Source" with low transfer difficulty, utilizing a prototype classifier to model intra-class clustering structures while generating reliable pseudo-labels for the target domain through a prototype-guided label propagation algorithm. Extensive experiments on two benchmark datasets, SEED and SEED-IV, demonstrate that HEDN achieves state-of-the-art performance in cross-subject EEG emotion recognition, with average accuracies of 93.58\% on SEED and 79.82\% on SEED-IV, respectively. These results confirm the effectiveness and generalizability of HEDN in cross-subject EEG emotion recognition.

</details>


### [16] [AgentSUMO: An Agentic Framework for Interactive Simulation Scenario Generation in SUMO via Large Language Models](https://arxiv.org/abs/2511.06804)
*Minwoo Jeong,Jeeyun Chang,Yoonjin Yoon*

Main category: cs.HC

TL;DR: 本研究提出了AgentSUMO框架，通过大型语言模型为非专业用户生成交互式交通仿真场景，以解决城市交通系统日益复杂的问题。


<details>
  <summary>Details</summary>
Motivation: 随着城市交通系统变得越来越复杂，传统的仿真工具对非专业用户造成了很高的使用门槛，因此需要一种新的框架来简化这一过程。

Method: 通过引入自适应推理层，AgentSUMO能够解析用户意图、评估任务复杂性、推断缺失参数并制定可执行的仿真计划。

Result: 在首尔和曼哈顿的城市网络上的实验表明，AgentSUMO框架显著改善了交通流量指标，同时保持了对非专业用户的可访问性。

Conclusion: AgentSUMO成功将抽象的政策目标转化为可执行的仿真场景，提升了交通流量指标，并且对非专业用户保持了可操作性。

Abstract: The growing complexity of urban mobility systems has made traffic simulation indispensable for evidence-based transportation planning and policy evaluation. However, despite the analytical capabilities of platforms such as the Simulation of Urban MObility (SUMO), their application remains largely confined to domain experts. Developing realistic simulation scenarios requires expertise in network construction, origin-destination modeling, and parameter configuration for policy experimentation, creating substantial barriers for non-expert users such as policymakers, urban planners, and city officials. Moreover, the requests expressed by these users are often incomplete and abstract-typically articulated as high-level objectives, which are not well aligned with the imperative, sequential workflows employed in existing language-model-based simulation frameworks. To address these challenges, this study proposes AgentSUMO, an agentic framework for interactive simulation scenario generation via large language models. AgentSUMO departs from imperative, command-driven execution by introducing an adaptive reasoning layer that interprets user intents, assesses task complexity, infers missing parameters, and formulates executable simulation plans. The framework is structured around two complementary components, the Interactive Planning Protocol, which governs reasoning and user interaction, and the Model Context Protocol, which manages standardized communication and orchestration among simulation tools. Through this design, AgentSUMO converts abstract policy objectives into executable simulation scenarios. Experiments on urban networks in Seoul and Manhattan demonstrate that the agentic workflow achieves substantial improvements in traffic flow metrics while maintaining accessibility for non-expert users, successfully bridging the gap between policy goals and executable simulation workflows.

</details>


### [17] [A Low-Cost Embedded System for Automated Patient Queue and Health Data Management in Private Medical Chambers](https://arxiv.org/abs/2511.06914)
*Kawshik Kumar Paul,Mahdi Hasnat Siyam,Khandokar Md. Rahat Hossain*

Main category: cs.HC

TL;DR: 本论文设计并实现了一个基于微控制器的低成本系统，用于管理患者排队和收集初步健康数据。


<details>
  <summary>Details</summary>
Motivation: 为私立医疗机构提供便捷的患者登记和健康数据收集解决方案。

Method: 集成ATmega32微控制器、LM35温度传感器、XD-58C脉搏传感器、4x4矩阵键盘和16x2 LCD显示器。

Result: 实验评估表明，该系统在有限硬件条件下有效运行。

Conclusion: 该系统有效减少了手动劳动和基于接触的数据收集，适合发展地区的小型私人诊所。

Abstract: This paper presents the design and implementation of a low-cost microcontroller-based system for managing patient queues and preliminary health data collection in private medical chambers. Patient registration, queue management, and the collection of fundamental health metrics such as heart rate and body temperature are automated by the system. The proposed setup integrates an ATmega32 microcontroller, an LM35 temperature sensor, an XD-58C pulse sensor, 4x4 matrix keypads, and 16x2 LCD displays. The system separates patient-side input from doctor-side control, allowing doctors to call patients sequentially with a single button. Experimental evaluation conducted under limited hardware conditions demonstrates that the system reduces manual labor and contact-based data collection, making it feasible for small private practices in developing regions.

</details>


### [18] [Personalizing Emotion-aware Conversational Agents? Exploring User Traits-driven Conversational Strategies for Enhanced Interaction](https://arxiv.org/abs/2511.06954)
*Yuchong Zhang,Yong Ma,Di Fu,Stephanie Zubicueta Portales,Morten Fjeld,Danica Kragic*

Main category: cs.HC

TL;DR: 本研究探讨了用户的个性特征如何影响其与情感感知对话代理的互动，结果显示个性化设计能提升用户满意度。


<details>
  <summary>Details</summary>
Motivation: 探讨用户如何以不同的特征（如性别、个性和文化背景）调整与情感感知的对话代理（CA）之间的互动策略，以提升用户体验。

Method: 通过实证研究，使用表达五种不同情感的情感感知对话代理原型，分析用户在不同语音和情感上下文中的互动动态。

Result: 研究发现，用户参与度和对话策略因个体特征而异，强调个性化、情感敏感的互动价值。

Conclusion: 持续的研究是必要的，以设计能够识别并适应用户情感需求的对话代理，从而更有效支持多样化的用户群体。

Abstract: Conversational agents (CAs) are increasingly embedded in daily life, yet their ability to navigate user emotions efficiently is still evolving. This study investigates how users with varying traits -- gender, personality, and cultural background -- adapt their interaction strategies with emotion-aware CAs in specific emotional scenarios. Using an emotion-aware CA prototype expressing five distinct emotions (neutral, happy, sad, angry, and fear) through male and female voices, we examine how interaction dynamics shift across different voices and emotional contexts through empirical studies. Our findings reveal distinct variations in user engagement and conversational strategies based on individual traits, emphasizing the value of personalized, emotion-sensitive interactions. By analyzing both qualitative and quantitative data, we demonstrate that tailoring CAs to user characteristics can enhance user satisfaction and interaction quality. This work underscores the critical need for ongoing research to design CAs that not only recognize but also adaptively respond to emotional needs, ultimately supporting a diverse user groups more effectively.

</details>


### [19] [Achieving Effective Virtual Reality Interactions via Acoustic Gesture Recognition based on Large Language Models](https://arxiv.org/abs/2511.07085)
*Xijie Zhang,Fengliang He,Hong-Ning Dai*

Main category: cs.HC

TL;DR: 研究提出了一种新颖的LLM框架用于CIR手势识别，以解决VR/AR中小样本学习的问题，并证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决现有CIR手势识别方法在小样本情况下的局限性，满足VR场景的需求。

Method: 提出一个基于大语言模型的CIR手势识别框架，适用于VR/AR系统。

Result: 该框架在新构建的数据集上，使用LLM分类器达到了与经典机器学习基线相当的准确性，且无需领域特定的再训练。

Conclusion: 研究表明，基于大语言模型的手势识别框架在准确性和用户透明性方面具有优势，适用于现代VR/AR系统。

Abstract: Natural and efficient interaction remains a critical challenge for virtual reality and augmented reality (VR/AR) systems. Vision-based gesture recognition suffers from high computational cost, sensitivity to lighting conditions, and privacy leakage concerns. Acoustic sensing provides an attractive alternative: by emitting inaudible high-frequency signals and capturing their reflections, channel impulse response (CIR) encodes how gestures perturb the acoustic field in a low-cost and user-transparent manner. However, existing CIR-based gesture recognition methods often rely on extensive training of models on large labeled datasets, making them unsuitable for few-shot VR scenarios. In this work, we propose the first framework that leverages large language models (LLMs) for CIR-based gesture recognition in VR/AR systems. Despite LLMs' strengths, it is non-trivial to achieve few-shot and zero-shot learning of CIR gestures due to their inconspicuous features. To tackle this challenge, we collect differential CIR rather than original CIR data. Moreover, we construct a real-world dataset collected from 10 participants performing 15 gestures across three categories (digits, letters, and shapes), with 10 repetitions each. We then conduct extensive experiments on this dataset using an LLM-adopted classifier. Results show that our LLM-based framework achieves accuracy comparable to classical machine learning baselines, while requiring no domain-specific retraining.

</details>


### [20] [NoteEx: Interactive Visual Context Manipulation for LLM-Assisted Exploratory Data Analysis in Computational Notebooks](https://arxiv.org/abs/2511.07223)
*Mohammad Hasan Payandeh,Lin-Ping Yuan,Jian Zhao*

Main category: cs.HC

TL;DR: NoteEx是一种JupyterLab扩展，通过语义可视化帮助用户更有效地选择上下文，改善数据分析中的LLM应用。


<details>
  <summary>Details</summary>
Motivation: 解决用户在使用LLM辅助进行数据分析时，因上下文选择和心理模型维护不当引发的挫败感。

Method: 通过形成性研究和用户研究评估NoteEx的效果。

Result: NoteEx显著改善了用户的心理模型保持和上下文选择，提升了LLM回复的准确性和相关性。

Conclusion: NoteEx能够帮助用户更好地维护心理模型及上下文选择，促进更有效的数据分析和LLM交互。

Abstract: Computational notebooks have become popular for Exploratory Data Analysis (EDA), augmented by LLM-based code generation and result interpretation. Effective LLM assistance hinges on selecting informative context -- the minimal set of cells whose code, data, or outputs suffice to answer a prompt. As notebooks grow long and messy, users can lose track of the mental model of their analysis. They thus fail to curate appropriate contexts for LLM tasks, causing frustration and tedious prompt engineering. We conducted a formative study (n=6) that surfaced challenges in LLM context selection and mental model maintenance. Therefore, we introduce NoteEx, a JupyterLab extension that provides a semantic visualization of the EDA workflow, allowing analysts to externalize their mental model, specify analysis dependencies, and enable interactive selection of task-relevant contexts for LLMs. A user study (n=12) against a baseline shows that NoteEx improved mental model retention and context selection, leading to more accurate and relevant LLM responses.

</details>


### [21] [Designing Beyond Language: Sociotechnical Barriers in AI Health Technologies for Limited English Proficiency](https://arxiv.org/abs/2511.07277)
*Michelle Huang,Violeta J. Rodriguez,Koustuv Saha,Tal August*

Main category: cs.HC

TL;DR: 此研究探讨了人工智能如何在医疗中帮助有限英语能力患者，但同时也存在潜在的风险和障碍。


<details>
  <summary>Details</summary>
Motivation: 探讨如何借助人工智能技术改善有限英语能力（LEP）患者的医疗体验，尤其是西班牙语患者面临的语言与文化障碍。

Method: 通过对14名患者导航员进行故事板驱动访谈，以深入了解人工智能对西班牙语LEP患者的潜在影响。

Result: 识别了人工智能在医疗护理中可能带来的机遇与风险，包括语言和文化误解、隐私问题，以及对患者信任的影响。

Conclusion: 尽管人工智能工具可能帮助减轻社会障碍和制度限制，但仍需警惕虚假信息的传播和人际关系的破坏。

Abstract: Limited English proficiency (LEP) patients in the U.S. face systemic barriers to healthcare beyond language and interpreter access, encompassing procedural and institutional constraints. AI advances may support communication and care through on-demand translation and visit preparation, but also risk exacerbating existing inequalities. We conducted storyboard-driven interviews with 14 patient navigators to explore how AI could shape care experiences for Spanish-speaking LEP individuals. We identified tensions around linguistic and cultural misunderstandings, privacy concerns, and opportunities and risks for AI to augment care workflows. Participants highlighted structural factors that can undermine trust in AI systems, including sensitive information disclosure, unstable technology access, and low digital literacy. While AI tools can potentially alleviate social barriers and institutional constraints, there are risks of misinformation and uprooting human camaraderie. Our findings contribute design considerations for AI that support LEP patients and care teams via rapport-building, education, and language support, and minimizing disruptions to existing practices.

</details>


### [22] [People Perceive More Phantom Costs From Autonomous Agents When They Make Unreasonably Generous Offers](https://arxiv.org/abs/2511.07401)
*Benjamin Lebrun,Christoph Bartneck,David Kaber,Andrew Vonasch*

Main category: cs.HC

TL;DR: 人们因看似隐秘的缺陷而拒绝过于慷慨的报价。研究探讨了人类与机器人销售代理的不同影响，发现机器人因被视为较少自利而降低了隐性成本感知。


<details>
  <summary>Details</summary>
Motivation: 研究人们对过于慷慨报价的拒绝及其背后的心理机制，尤其是在不同代理类型的影响下。

Method: 通过一项汽车购买模拟实验，参与者在不同交易条件下与人类或机器人销售代理互动。

Result: 机器人比人类被认为更少自利，减少了隐性成本感知；大折扣虽然增加隐性成本，但也提高了购买意图，显示感知利益可以超过隐性成本。

Conclusion: 人们对责任的归属及其感知影响人类与机器人之间的互动，这对人工智能的伦理设计和营销策略有重要启示。

Abstract: People often reject offers that are too generous due to the perception of hidden drawbacks referred to as "phantom costs." We hypothesized that this perception and the decision-making vary based on the type of agent making the offer (human vs. robot) and the degree to which the agent is perceived to be autonomous or have the capacity for self-interest. To test this conjecture, participants (N = 855) engaged in a car-buying simulation where a human or robot sales agent, described as either autonomous or not, offered either a small (5%) or large (85%) discount. Results revealed that the robot was perceived as less self-interested than the human, which reduced the perception of phantom costs. While larger discounts increased phantom costs, they also increased purchase intentions, suggesting that perceived benefits can outweigh phantom costs. Importantly, phantom costs were not only attributed to the agent participants interacted with, but also to the product and the agent's manager, highlighting at least three sources of suspicion. These findings deepen our understanding of to whom people assign responsibility and how perceptions shape both human-human and human-robot interactions, with implications for ethical AI design and marketing strategies.

</details>


### [23] [Social-Physical Interactions with Virtual Characters: Evaluating the Impact of Physicality through Encountered-Type Haptics](https://arxiv.org/abs/2511.05683)
*Eric Godden,Jacquie Groenewegen,Michael Wheeler,Matthew K. X. J. Pan*

Main category: cs.HC

TL;DR: 研究表明，ETHOS增加物理接触能提升虚拟角色互动的沉浸感和 enjoyment。


<details>
  <summary>Details</summary>
Motivation: 探讨机器人介导的物理性如何影响与虚拟角色的社交物理互动感知。

Method: 通过用户研究，比较了无物理性、静态物理性和动态物理性三种条件下的互动体验。

Result: ETHOS（按需社交互动的遇到型触觉技术）的实验结果显示，增加物理性可以改善用户的沉浸感、真实感、享受度和连接感。

Conclusion: 这些发现展示了ETHOS的体验价值，并激励将遇到型触觉技术整合到社会互动的虚拟现实体验中。

Abstract: This work investigates how robot-mediated physicality influences the perception of social-physical interactions with virtual characters. ETHOS (Encountered-Type Haptics for On-demand Social interaction) is an encountered-type haptic display that integrates a torque-controlled manipulator and interchangeable props with a VR headset to enable three gestures: object handovers, fist bumps, and high fives. We conducted a user study to examine how ETHOS adds physicality to virtual character interactions and how this affects presence, realism, enjoyment, and connection metrics. Each participant experienced one interaction under three conditions: no physicality (NP), static physicality (SP), and dynamic physicality (DP). SP extended the purely virtual baseline (NP) by introducing tangible props for direct contact, while DP further incorporated motion and impact forces to emulate natural touch. Results show presence increased stepwise from NP to SP to DP. Realism, enjoyment, and connection also improved with added physicality, though differences between SP and DP were not significant. Comfort remained consistent across conditions, indicating no added psychological friction. These findings demonstrate the experiential value of ETHOS and motivate the integration of encountered-type haptics into socially meaningful VR experiences.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [24] [Lite VLA: Efficient Vision-Language-Action Control on CPU-Bound Edge Robots](https://arxiv.org/abs/2511.05642)
*Justin Williams,Kishor Datta Gupta,Roy George,Mrinmoy Sarkar*

Main category: cs.RO

TL;DR: 本研究展示了小型视觉语言模型在GPS受限环境下的机器人上的实时场景理解和推理能力，消除了对云连接的依赖，同时实现了高效的自主决策。


<details>
  <summary>Details</summary>
Motivation: 在GPS受限环境下，移动机器人需要进行本地且资源高效的推理，以实现自主运行。

Method: 通过集成紧凑的视觉语言模型和多模态感知，实施了一种允许移动和推理同时进行的新框架。

Result: 实验验证了系统在计算效率、任务准确性和响应性之间的平衡，并实现了小型视觉语言模型在边缘的成功部署。

Conclusion: 该研究实现了小型视觉语言模型在移动机器人上的成功应用，为边缘计算提供了可扩展的自主决策基础。

Abstract: The deployment of artificial intelligence models at the edge is increasingly critical for autonomous robots operating in GPS-denied environments where local, resource-efficient reasoning is essential. This work demonstrates the feasibility of deploying small Vision-Language Models (VLMs) on mobile robots to achieve real-time scene understanding and reasoning under strict computational constraints. Unlike prior approaches that separate perception from mobility, the proposed framework enables simultaneous movement and reasoning in dynamic environments using only on-board hardware. The system integrates a compact VLM with multimodal perception to perform contextual interpretation directly on embedded hardware, eliminating reliance on cloud connectivity. Experimental validation highlights the balance between computational efficiency, task accuracy, and system responsiveness. Implementation on a mobile robot confirms one of the first successful deployments of small VLMs for concurrent reasoning and mobility at the edge. This work establishes a foundation for scalable, assured autonomy in applications such as service robotics, disaster response, and defense operations.

</details>


### [25] [VLM-driven Skill Selection for Robotic Assembly Tasks](https://arxiv.org/abs/2511.05680)
*Jeong-Jung Kim,Doo-Yeol Koh,Chang-Hyun Kim*

Main category: cs.RO

TL;DR: 本论文提出了一种结合视觉-语言模型和模仿学习的机器人组装框架，用于更灵活的组装操作。


<details>
  <summary>Details</summary>
Motivation: 旨在通过集成先进的技术提升机器人在组装任务中的灵活性和适应能力。

Method: 将视觉感知、自然语言理解和学习的基本技能相结合，以实现灵活的机器人操作。

Result: 实验结果表明，该方法在组装任务中取得了高成功率。

Conclusion: 所提框架在组装场景中表现出高成功率，并保持了技能的可解释性。

Abstract: This paper presents a robotic assembly framework that combines Vision-Language Models (VLMs) with imitation learning for assembly manipulation tasks. Our system employs a gripper-equipped robot that moves in 3D space to perform assembly operations. The framework integrates visual perception, natural language understanding, and learned primitive skills to enable flexible and adaptive robotic manipulation. Experimental results demonstrate the effectiveness of our approach in assembly scenarios, achieving high success rates while maintaining interpretability through the structured primitive skill decomposition.

</details>


### [26] [A Unified Stochastic Mechanism Underlying Collective Behavior in Ants, Physical Systems, and Robotic Swarms](https://arxiv.org/abs/2511.05785)
*Lianhao Yin,Haiping Yu,Pascal Spino,Daniela Rus*

Main category: cs.RO

TL;DR: 本研究揭示了生物和物理系统中随机行为的共享统计机制，并建立了一个统一的随机模型以设计更智能的群体机器人。


<details>
  <summary>Details</summary>
Motivation: 生物群体如蚁群通过分散和随机的个体行为实现集体目标，而物理系统虽具随机粒子运动却未能实现集体目标。因此需要一个统一的框架来解释生物和物理系统中的随机行为。

Method: 基于对红火蚁（Formica polyctena）的实证研究，证明了在不同能量函数约束下的最大化共享统计机制。

Result: 展示了基于该原则的机器人群体能够表现出可扩展的、分散的合作，模拟具有最小个体计算的物理相似行为。

Conclusion: 建立了一个统一的随机模型，将生物、物理和机器人群体联系起来，为设计强大而智能的群体机器人提供了可扩展的原则。

Abstract: Biological swarms, such as ant colonies, achieve collective goals through decentralized and stochastic individual behaviors. Similarly, physical systems composed of gases, liquids, and solids exhibit random particle motion governed by entropy maximization, yet do not achieve collective objectives. Despite this analogy, no unified framework exists to explain the stochastic behavior in both biological and physical systems. Here, we present empirical evidence from \textit{Formica polyctena} ants that reveals a shared statistical mechanism underlying both systems: maximization under different energy function constraints. We further demonstrate that robotic swarms governed by this principle can exhibit scalable, decentralized cooperation, mimicking physical phase-like behaviors with minimal individual computation. These findings established a unified stochastic model linking biological, physical, and robotic swarms, offering a scalable principle for designing robust and intelligent swarm robotics.

</details>


### [27] [VLAD-Grasp: Zero-shot Grasp Detection via Vision-Language Models](https://arxiv.org/abs/2511.05791)
*Manav Kulshrestha,S. Talha Bukhari,Damon Conover,Aniket Bera*

Main category: cs.RO

TL;DR: 提出了一种新的机器人抓取方法VLAD-Grasp，通过视觉-语言模型生成目标图像并实现零样本抓取，性能优于许多监督学习模型。


<details>
  <summary>Details</summary>
Motivation: 提升机器人抓取能力，减少对大量标注数据的依赖，解决处理新物体时需要重训练的问题。

Method: 利用RGB-D图像，通过视觉-语言模型生成对应目标图像，结合深度和分割信息，将图像升维至3D，然后使用主成分分析和无对应优化对点云进行对齐，恢复可执行的抓取姿势。

Result: VLAD-Grasp在无训练的情况下，其性能与最先进的监督模型相当或优于之，实现了零样本推断。

Conclusion: VLAD-Grasp展示了视觉-语言基础模型在机器人操控中的强大潜力，能够实现对新物体的零样本泛化。

Abstract: Robotic grasping is a fundamental capability for autonomous manipulation; however, most existing methods rely on large-scale expert annotations and necessitate retraining to handle new objects. We present VLAD-Grasp, a Vision-Language model Assisted zero-shot approach for Detecting grasps. From a single RGB-D image, our method (1) prompts a large vision-language model to generate a goal image where a straight rod "impales" the object, representing an antipodal grasp, (2) predicts depth and segmentation to lift this generated image into 3D, and (3) aligns generated and observed object point clouds via principal component analysis and correspondence-free optimization to recover an executable grasp pose. Unlike prior work, our approach is training-free and does not rely on curated grasp datasets. Despite this, VLAD-Grasp achieves performance that is competitive with or superior to that of state-of-the-art supervised models on the Cornell and Jacquard datasets. We further demonstrate zero-shot generalization to novel real-world objects on a Franka Research 3 robot, highlighting vision-language foundation models as powerful priors for robotic manipulation.

</details>


### [28] [3D Mapping Using a Lightweight and Low-Power Monocular Camera Embedded inside a Gripper of Limbed Climbing Robots](https://arxiv.org/abs/2511.05816)
*Taku Okawara,Ryo Nishibe,Mao Kasano,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 本文提出了一种使用单目手眼相机的3D地形映射系统，适用于探索月球和火星的攀爬机器人。


<details>
  <summary>Details</summary>
Motivation: 传统的RGB-D相机虽然方便3D地形映射，但体积大、耗电多，限制了其在宇宙探测中的应用。

Method: 提出了一种将单目SLAM与肢体前向运动学结合的SLAM方法，通过因子图优化共同估计时序夹爪姿态和3D地图的全球度量尺度。

Result: 实验证明，该框架能实时构建带有度量缩放的3D地形地图，并通过单目相机实现自主抓取。

Conclusion: 该方法通过融合单目视觉约束和肢体前向运动学，实现了在实时构建3D地形地图的同时进行自主抓取。

Abstract: Limbed climbing robots are designed to explore challenging vertical walls, such as the skylights of the Moon and Mars. In such robots, the primary role of a hand-eye camera is to accurately estimate 3D positions of graspable points (i.e., convex terrain surfaces) thanks to its close-up views. While conventional climbing robots often employ RGB-D cameras as hand-eye cameras to facilitate straightforward 3D terrain mapping and graspable point detection, RGB-D cameras are large and consume considerable power.
  This work presents a 3D terrain mapping system designed for space exploration using limbed climbing robots equipped with a monocular hand-eye camera. Compared to RGB-D cameras, monocular cameras are more lightweight, compact structures, and have lower power consumption. Although monocular SLAM can be used to construct 3D maps, it suffers from scale ambiguity. To address this limitation, we propose a SLAM method that fuses monocular visual constraints with limb forward kinematics. The proposed method jointly estimates time-series gripper poses and the global metric scale of the 3D map based on factor graph optimization.
  We validate the proposed framework through both physics-based simulations and real-world experiments. The results demonstrate that our framework constructs a metrically scaled 3D terrain map in real-time and enables autonomous grasping of convex terrain surfaces using a monocular hand-eye camera, without relying on RGB-D cameras. Our method contributes to scalable and energy-efficient perception for future space missions involving limbed climbing robots. See the video summary here: https://youtu.be/fMBrrVNKJfc

</details>


### [29] [Gentle Manipulation Policy Learning via Demonstrations from VLM Planned Atomic Skills](https://arxiv.org/abs/2511.05855)
*Jiayu Zhou,Qiwei Wu,Jian Li,Zhe Chen,Xiaogang Xiong,Renjing Xu*

Main category: cs.RO

TL;DR: 本文提出一种新颖的框架，通过结合多种技术解决了自主操作长期任务中的成本和可扩展性挑战，实现了无昂贵示范的策略学习。


<details>
  <summary>Details</summary>
Motivation: 传统的自主执行长期、接触丰富的操作任务需要大量的真实世界数据和专家工程，存在成本和可扩展性挑战，因此提出此框架以克服这些限制。

Method: 该方法结合了分层语义分解、强化学习、视觉语言模型和知识蒸馏，提出了一种将复杂任务分解为原子技能的框架，并在仿真中训练策略。

Result: 通过大量的仿真实验和实际部署验证了该方法的有效性，展示了在不同任务中通过VLM引导的任务规划和技能蒸馏的优势。

Conclusion: 该方法实现了在没有昂贵人类示范的情况下对长期操作策略的学习，并且VLM引导的原子技能框架实现了对多种任务的可扩展性推广。

Abstract: Autonomous execution of long-horizon, contact-rich manipulation tasks traditionally requires extensive real-world data and expert engineering, posing significant cost and scalability challenges. This paper proposes a novel framework integrating hierarchical semantic decomposition, reinforcement learning (RL), visual language models (VLMs), and knowledge distillation to overcome these limitations. Complex tasks are decomposed into atomic skills, with RL-trained policies for each primitive exclusively in simulation. Crucially, our RL formulation incorporates explicit force constraints to prevent object damage during delicate interactions. VLMs perform high-level task decomposition and skill planning, generating diverse expert demonstrations. These are distilled into a unified policy via Visual-Tactile Diffusion Policy for end-to-end execution. We conduct comprehensive ablation studies exploring different VLM-based task planners to identify optimal demonstration generation pipelines, and systematically compare imitation learning algorithms for skill distillation. Extensive simulation experiments and physical deployment validate that our approach achieves policy learning for long-horizon manipulation without costly human demonstrations, while the VLM-guided atomic skill framework enables scalable generalization to diverse tasks.

</details>


### [30] [ViTaMIn-B: A Reliable and Efficient Visuo-Tactile Bimanual Manipulation Interface](https://arxiv.org/abs/2511.05858)
*Chuanyu Li,Chaoyi Liu,Daotan Wang,Shuyu Zhang,Lusong Li,Zecui Zeng,Fangchen Liu,Jing Xu,Rui Chen*

Main category: cs.RO

TL;DR: ViTaMIn-B是一个高效的手持数据收集系统，具有改进的触觉传感器和增强的姿态获取过程，适用于双手复杂操作。


<details>
  <summary>Details</summary>
Motivation: 目前的系统在复杂交互场景下缺乏可靠的触觉感知和姿态跟踪能力，ViTaMIn-B旨在填补这些空白，使得高质量演示的数据收集更加高效。

Method: 设计了一种名为DuoTact的新型合规视觉触觉传感器，以及基于Meta Quest控制器的统一6自由度双手姿态获取过程。

Result: ViTaMIn-B是一种新的手持数据收集系统，旨在改善复杂交互场景下的触觉感知和姿态跟踪性能，特别是在双手配合和接触频繁的任务中。

Conclusion: 用户研究和实验表明，ViTaMIn-B在新手和专家的操作中均表现出高效性和良好的可用性，且在多项任务中超越了现有系统。

Abstract: Handheld devices have opened up unprecedented opportunities to collect large-scale, high-quality demonstrations efficiently. However, existing systems often lack robust tactile sensing or reliable pose tracking to handle complex interaction scenarios, especially for bimanual and contact-rich tasks. In this work, we propose ViTaMIn-B, a more capable and efficient handheld data collection system for such tasks. We first design DuoTact, a novel compliant visuo-tactile sensor built with a flexible frame to withstand large contact forces during manipulation while capturing high-resolution contact geometry. To enhance the cross-sensor generalizability, we propose reconstructing the sensor's global deformation as a 3D point cloud and using it as the policy input. We further develop a robust, unified 6-DoF bimanual pose acquisition process using Meta Quest controllers, which eliminates the trajectory drift issue in common SLAM-based methods. Comprehensive user studies confirm the efficiency and high usability of ViTaMIn-B among novice and expert operators. Furthermore, experiments on four bimanual manipulation tasks demonstrate its superior task performance relative to existing systems.

</details>


### [31] [Fair and Safe: A Real-Time Hierarchical Control Framework for Intersections](https://arxiv.org/abs/2511.05886)
*Lei Shi,Yongju Kim,Xinzhi Zhong,Wissam Kontar,Qichao Liu,Soyoung Ahn*

Main category: cs.RO

TL;DR: 本论文提出了一个集成不平等厌恶的公平感知分层控制框架，以管理交叉口的自动驾驶车辆，结果显示该框架在实现安全的同时也能达到几乎完美的公平性。


<details>
  <summary>Details</summary>
Motivation: 为确保连接和自动化车辆在交叉口的协调公平性，提高社会接受度和系统长期效率，尤其是在安全关键的实时交通控制中，公平性仍然是一个未被充分探讨的领域。

Method: 提出了一个分层控制框架，顶层为集中分配模块负责控制权的分配，底层使用线性二次调节器和基于高阶控制屏障函数的安全过滤器执行轨迹。

Result: 该论文提出了一种公平感知的分层控制框架，用于管理交叉口的连接和自动驾驶车辆，旨在确保公平性并提高系统效率。

Conclusion: 通过仿真结果显示，该框架可以在不同交通需求下实现几乎完美的公平性，消除碰撞，降低平均延迟，同时保持实时可行性，证明了公平性可以在不牺牲安全性或性能的情况下系统化地纳入未来的自动驾驶交通系统。

Abstract: Ensuring fairness in the coordination of connected and automated vehicles at intersections is essential for equitable access, social acceptance, and long-term system efficiency, yet it remains underexplored in safety-critical, real-time traffic control. This paper proposes a fairness-aware hierarchical control framework that explicitly integrates inequity aversion into intersection management. At the top layer, a centralized allocation module assigns control authority (i.e., selects a single vehicle to execute its trajectory) by maximizing a utility that accounts for waiting time, urgency, control history, and velocity deviation. At the bottom layer, the authorized vehicle executes a precomputed trajectory using a Linear Quadratic Regulator (LQR) and applies a high-order Control Barrier Function (HOCBF)-based safety filter for real-time collision avoidance. Simulation results across varying traffic demands and demand distributions demonstrate that the proposed framework achieves near-perfect fairness, eliminates collisions, reduces average delay, and maintains real-time feasibility. These results highlight that fairness can be systematically incorporated without sacrificing safety or performance, enabling scalable and equitable coordination for future autonomous traffic systems.

</details>


### [32] [From Words to Safety: Language-Conditioned Safety Filtering for Robot Navigation](https://arxiv.org/abs/2511.05889)
*Zeyuan Feng,Haimingyue Zhang,Somil Bansal*

Main category: cs.RO

TL;DR: 本文提出了一种模块化框架，用于实现机器人导航中的语言条件安全，解决了传统方法在安全规范解释和适应性方面的局限。


<details>
  <summary>Details</summary>
Motivation: 随着机器人在开放世界和以人为中心的环境中日益集成，解读自然语言指令和遵循安全约束变得至关重要。

Method: 框架由三个核心模块组成：基于大型语言模型的安全规范转换模块、环境对象的3D表示感知模块以及基于模型预测控制的安全过滤器。

Result: 通过模拟研究和硬件实验评估，证明了该框架在不同环境和场景下能够稳健地解读和实施多样的语言规定约束。

Conclusion: 提出的模块化框架有效地将自然语言指令与安全约束结合，实现了机器人在多种环境下的安全导航。

Abstract: As robots become increasingly integrated into open-world, human-centered environments, their ability to interpret natural language instructions and adhere to safety constraints is critical for effective and trustworthy interaction. Existing approaches often focus on mapping language to reward functions instead of safety specifications or address only narrow constraint classes (e.g., obstacle avoidance), limiting their robustness and applicability. We propose a modular framework for language-conditioned safety in robot navigation. Our framework is composed of three core components: (1) a large language model (LLM)-based module that translates free-form instructions into structured safety specifications, (2) a perception module that grounds these specifications by maintaining object-level 3D representations of the environment, and (3) a model predictive control (MPC)-based safety filter that enforces both semantic and geometric constraints in real time. We evaluate the effectiveness of the proposed framework through both simulation studies and hardware experiments, demonstrating that it robustly interprets and enforces diverse language-specified constraints across a wide range of environments and scenarios.

</details>


### [33] [10 Open Challenges Steering the Future of Vision-Language-Action Models](https://arxiv.org/abs/2511.05936)
*Soujanya Poria,Navonil Majumder,Chia-Yu Hung,Amir Ali Bagherzadeh,Chuan Li,Kenneth Kwok,Ziwei Wang,Cheston Tan,Jiajun Wu,David Hsu*

Main category: cs.RO

TL;DR: 本文围绕VLA模型的发展讨论了10个里程碑和新兴趋势，旨在推动其更广泛的应用。


<details>
  <summary>Details</summary>
Motivation: 为了提升VLA模型在自然语言指令跟随方面的能力，推动其在实际应用中的广泛接受。

Method: 通过回顾当前发展中的10个主要里程碑，并探讨新兴趋势。

Result: 提出了围绕多模态性、推理等十个里程碑的讨论，并指出了空间理解等新兴趋势。

Conclusion: 本文讨论了多项重要里程碑和新兴趋势，为VLA模型的研发指明了方向。

Abstract: Due to their ability of follow natural language instructions, vision-language-action (VLA) models are increasingly prevalent in the embodied AI arena, following the widespread success of their precursors -- LLMs and VLMs. In this paper, we discuss 10 principal milestones in the ongoing development of VLA models -- multimodality, reasoning, data, evaluation, cross-robot action generalization, efficiency, whole-body coordination, safety, agents, and coordination with humans. Furthermore, we discuss the emerging trends of using spatial understanding, modeling world dynamics, post training, and data synthesis -- all aiming to reach these milestones. Through these discussions, we hope to bring attention to the research avenues that may accelerate the development of VLA models into wider acceptability.

</details>


### [34] [Robustness study of the bio-inspired musculoskeletal arm robot based on the data-driven iterative learning algorithm](https://arxiv.org/abs/2511.05995)
*Jianbo Yuan,Jing Dai,Yerui Fan,Yaxiong Wu,Yunpeng Liang,Weixin Yan*

Main category: cs.RO

TL;DR: 本研究设计了一种新型轻量化肌腱驱动的肌肉骨骼臂（LTDM-Arm），能够在负载干扰下有效完成预定轨迹跟踪任务。


<details>
  <summary>Details</summary>
Motivation: 模仿人类臂的灵活性、适应性和稳健性，以推动机器人技术的进步。

Method: 采用七自由度关节系统和模块化人工肌肉系统（MAMS），结合Hilly型肌肉模型和数据驱动的迭代学习控制（DDILC）优化激活信号。

Result: LTDM-Arm在仿真中承受20%负载干扰，实验中承受15%负载干扰，仍能有效进行轨迹跟踪。

Conclusion: LTDM-Arm系统为类人操作性能的高级机器人系统开发奠定了基础。

Abstract: The human arm exhibits remarkable capabilities, including both explosive power and precision, which demonstrate dexterity, compliance, and robustness in unstructured environments. Developing robotic systems that emulate human-like operational characteristics through musculoskeletal structures has long been a research focus. In this study, we designed a novel lightweight tendon-driven musculoskeletal arm (LTDM-Arm), featuring a seven degree-of-freedom (DOF) skeletal joint system and a modularized artificial muscular system (MAMS) with 15 actuators. Additionally, we employed a Hilly-type muscle model and data-driven iterative learning control (DDILC) to learn and refine activation signals for repetitive tasks within a finite time frame. We validated the anti-interference capabilities of the musculoskeletal system through both simulations and experiments. The results show that the LTDM-Arm system can effectively achieve desired trajectory tracking tasks, even under load disturbances of 20 % in simulation and 15 % in experiments. This research lays the foundation for developing advanced robotic systems with human-like operational performance.

</details>


### [35] [Development and testing of novel soft sleeve actuators](https://arxiv.org/abs/2511.06102)
*Mohammed Abboodi*

Main category: cs.RO

TL;DR: 本研究提出了一种软袖驱动架构，通过三个不同的软袖驱动器实现多轴运动，提升了力量传递，并降低了复杂附件的需求。


<details>
  <summary>Details</summary>
Motivation: 应对老龄化人口和神经及肌肉骨骼疾病日益增加的需求，开发有效、舒适且符合解剖学的可穿戴移动辅助设备。

Method: 开发三种软袖驱动器，采用热塑性弹性体使用定制的熔融纤维制造工艺，实验平台量化运动学和动力学输出，并进行参数研究。

Result: 研究开发了一种软袖驱动架构，能够有效传递力量和力矩，且适应肢体形状。

Conclusion: 研究为软袖驱动技术提供了统一和可制造的框架，能够实现紧凑和以用户为中心的辅助技术，具有增强的运动学和动力学性能。

Abstract: Aging populations and the rising prevalence of neurological and musculoskeletal disorders increase the demand for wearable mobility assistive devices that are effective, comfortable, and anatomically compatible. Many existing systems use rigid mechanisms and bulky interfaces that impede force transmission and reduce wearability. This study introduces a soft sleeve actuation architecture that conforms to the limb while transmitting forces and moments efficiently. We develop three soft sleeve actuators that produce linear, bending, and twisting motion, and an omnidirectional design that combines these motions in one device. Actuators are fabricated from thermoplastic elastomers using a customized fused filament fabrication process that produces airtight and compliant structures and resolves leakage observed with conventional methods. A dedicated experimental platform quantifies kinematic outputs such as displacement, angle, and twist, and kinetic outputs such as force and torque under low pneumatic pressures. A parametric study varies geometric features and material properties to determine their influence on performance. Results show reproducible multi axis motion with improved transfer of force to the limb and reduced need for complex attachment hardware. The work establishes a unified and manufacturable framework for soft sleeve actuation that enables compact and user centered assistive technologies with enhanced kinematic and kinetic performance.

</details>


### [36] [PlaCo: a QP-based robot planning and control framework](https://arxiv.org/abs/2511.06141)
*Marc Duclusaud,Grégoire Passault,Vincent Padois,Olivier Ly*

Main category: cs.RO

TL;DR: 本文介绍了PlaCo，一个旨在简化机器人系统的二次规划(QP)问题的规划和控制的软件框架。


<details>
  <summary>Details</summary>
Motivation: 为了提高机器人系统在规划和控制问题上的处理效率，对QP问题的解决进行了简化和抽象。

Method: 通过提供一个高层次的接口，PlaCo屏蔽了二次规划问题的低层数学表达，用户可以更容易地进行任务和约束的定义。

Result: PlaCo的设计使得用户可以快速原型开发，并在实时应用中实现出色的性能。

Conclusion: PlaCo为用户提供了一个高层接口，使得任务和约束的指定更加模块化和直观，同时支持Python和C++实现以满足不同需求。

Abstract: This article introduces PlaCo, a software framework designed to simplify the formulation and solution of Quadratic Programming (QP)-based planning and control problems for robotic systems. PlaCo provides a high-level interface that abstracts away the low-level mathematical formulation of QP problems, allowing users to specify tasks and constraints in a modular and intuitive manner. The framework supports both Python bindings for rapid prototyping and a C++ implementation for real-time performance.

</details>


### [37] [OpenVLN: Open-world aerial Vision-Language Navigation](https://arxiv.org/abs/2511.06182)
*Peican Lin,Gan Sun,Chenxi Liu,Fazeng Li,Weihong Ren,Yang Cong*

Main category: cs.RO

TL;DR: 提出一种新框架OpenVLN，提高无人机在复杂环境中的语言导航效率，实验结果显示性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决无人机在复杂户外环境中进行语言驱动导航时数据获取和长远轨迹规划的挑战。

Method: 提出一种数据高效的开放世界空中视觉语言导航框架（OpenVLN），使用强化学习优化VLM并引入长远规划器。

Result: 在TravelUAV基准上进行实验，表明该方法在成功率、oracle成功率和路径长度加权成功率上相较基线方法均有显著提升。

Conclusion: 该方法验证了在复杂空中环境中进行长远无人机导航的有效性。

Abstract: Vision-language models (VLMs) have been widely-applied in ground-based vision-language navigation (VLN). However, the vast complexity of outdoor aerial environments compounds data acquisition challenges and imposes long-horizon trajectory planning requirements on Unmanned Aerial Vehicles (UAVs), introducing novel complexities for aerial VLN. To address these challenges, we propose a data-efficient Open-world aerial Vision-Language Navigation (i.e., OpenVLN) framework, which could execute language-guided flight with limited data constraints and enhance long-horizon trajectory planning capabilities in complex aerial environments. Specifically, we reconfigure a reinforcement learning framework to optimize the VLM for UAV navigation tasks, which can efficiently fine-tune VLM by using rule-based policies under limited training data. Concurrently, we introduce a long-horizon planner for trajectory synthesis that dynamically generates precise UAV actions via value-based rewards. To the end, we conduct sufficient navigation experiments on the TravelUAV benchmark with dataset scaling across diverse reward settings. Our method demonstrates consistent performance gains of up to 4.34% in Success Rate, 6.19% in Oracle Success Rate, and 4.07% in Success weighted by Path Length over baseline methods, validating its deployment efficacy for long-horizon UAV navigation in complex aerial environments.

</details>


### [38] [ExpReS-VLA: Specializing Vision-Language-Action Models Through Experience Replay and Retrieval](https://arxiv.org/abs/2511.06202)
*Shahram Najam Syed,Yatharth Ahuja,Arthur Jakobsson,Jeff Ichnowski*

Main category: cs.RO

TL;DR: ExpReS-VLA是一种能有效适应新环境的Vision-Language-Action模型，通过经验重放与提取，成功提高了机器人操作的表现。


<details>
  <summary>Details</summary>
Motivation: 针对现有VLA模型在新环境适应性差的问题，我们提出了新方法以提高特定任务上的一致高性能。

Method: 采用经验重放和检索方法，结合阈值混合对比损失，有效指导模型的适应过程，同时减少了遗忘现象。

Result: ExpReS-VLA展示了在应用于机器人操作任务中的显著成功率提升，尤其是在特定环境中的适应性更强。

Conclusion: ExpReS-VLA在多个任务中展示了显著的成功率提高，证明了其在机器人适应性和记忆效率方面的有效性。

Abstract: Vision-Language-Action models such as OpenVLA show impressive zero-shot generalization across robotic manipulation tasks but often fail to adapt efficiently to new deployment environments. In many real-world applications, consistent high performance on a limited set of tasks is more important than broad generalization. We propose ExpReS-VLA, a method for specializing pre-trained VLA models through experience replay and retrieval while preventing catastrophic forgetting. ExpReS-VLA stores compact feature representations from the frozen vision backbone instead of raw image-action pairs, reducing memory usage by approximately 97 percent. During deployment, relevant past experiences are retrieved using cosine similarity and used to guide adaptation, while prioritized experience replay emphasizes successful trajectories. We also introduce Thresholded Hybrid Contrastive Loss, which enables learning from both successful and failed attempts. On the LIBERO simulation benchmark, ExpReS-VLA improves success rates from 82.6 to 93.1 percent on spatial reasoning tasks and from 61 to 72.3 percent on long-horizon tasks. On physical robot experiments with five manipulation tasks, it reaches 98 percent success on both seen and unseen settings, compared to 84.7 and 32 percent for naive fine-tuning. Adaptation takes 31 seconds using 12 demonstrations on a single RTX 5090 GPU, making the approach practical for real robot deployment.

</details>


### [39] [Affordance-Guided Coarse-to-Fine Exploration for Base Placement in Open-Vocabulary Mobile Manipulation](https://arxiv.org/abs/2511.06240)
*Tzu-Jung Lin,Jia-Fong Yeh,Hung-Ting Su,Chung-Yi Lin,Yi-Ting Chen,Winston H. Hsu*

Main category: cs.RO

TL;DR: 该研究提出了一种新颖的框架，以提高开放词汇移动操控任务中机器人的基座放置效能，结合视觉-语言模型的语义理解和几何可行性，通过迭代优化过程实现更优布局。


<details>
  <summary>Details</summary>
Motivation: 在开放词汇移动操控（OVMM）中，任务成功通常依赖于机器人基座放置的选择，而现有方法往往忽视了有效性，导致频繁的操作失败。

Method: 我们的办法通过构建跨模态表示（有效性RGB和障碍物地图+），利用VLM的粗语义先验来引导搜索，并在几何约束下细化布局，从而减少收敛到局部最优的风险。

Result: 我们提出了一种名为‘有效性引导的粗到细探索’的零样本框架，在五个多样的开放词汇移动操控任务中，取得85%的成功率，显著优于经典的几何规划和基于VLM的方法。

Conclusion: 这表明，考虑有效性和多模态推理在开放词汇移动操控中的规划具有潜在的广泛应用。

Abstract: In open-vocabulary mobile manipulation (OVMM), task success often hinges on the selection of an appropriate base placement for the robot. Existing approaches typically navigate to proximity-based regions without considering affordances, resulting in frequent manipulation failures. We propose Affordance-Guided Coarse-to-Fine Exploration, a zero-shot framework for base placement that integrates semantic understanding from vision-language models (VLMs) with geometric feasibility through an iterative optimization process. Our method constructs cross-modal representations, namely Affordance RGB and Obstacle Map+, to align semantics with spatial context. This enables reasoning that extends beyond the egocentric limitations of RGB perception. To ensure interaction is guided by task-relevant affordances, we leverage coarse semantic priors from VLMs to guide the search toward task-relevant regions and refine placements with geometric constraints, thereby reducing the risk of convergence to local optima. Evaluated on five diverse open-vocabulary mobile manipulation tasks, our system achieves an 85% success rate, significantly outperforming classical geometric planners and VLM-based methods. This demonstrates the promise of affordance-aware and multimodal reasoning for generalizable, instruction-conditioned planning in OVMM.

</details>


### [40] [Robust Differentiable Collision Detection for General Objects](https://arxiv.org/abs/2511.06267)
*Jiayi Chen,Wei Zhao,Liangwang Ruan,Baoquan Chen,He Wang*

Main category: cs.RO

TL;DR: 提出了一种高效的可微碰撞检测框架，支持复杂形状物体，实现了在抓取和操作中的优化。


<details>
  <summary>Details</summary>
Motivation: 解决传统碰撞检测算法在接触丰富任务中的梯度流动限制，特别是在抓取和操作任务中的优化问题。

Method: 提出了一种支持凸形和凹形物体的可微碰撞检测框架，通过距离基础的一阶随机平滑、适应性采样和等效梯度传输实现鲁棒且信息丰富的梯度计算。

Result: 在DexGraspNet和Objaverse复杂网格上的实验显示，所提出的方法显著优于现有基线。

Conclusion: 方法在复杂应用如灵巧抓取合成中表现出良好的效果，代码可供使用。

Abstract: Collision detection is a core component of robotics applications such as simulation, control, and planning. Traditional algorithms like GJK+EPA compute witness points (i.e., the closest or deepest-penetration pairs between two objects) but are inherently non-differentiable, preventing gradient flow and limiting gradient-based optimization in contact-rich tasks such as grasping and manipulation. Recent work introduced efficient first-order randomized smoothing to make witness points differentiable; however, their direction-based formulation is restricted to convex objects and lacks robustness for complex geometries. In this work, we propose a robust and efficient differentiable collision detection framework that supports both convex and concave objects across diverse scales and configurations. Our method introduces distance-based first-order randomized smoothing, adaptive sampling, and equivalent gradient transport for robust and informative gradient computation. Experiments on complex meshes from DexGraspNet and Objaverse show significant improvements over existing baselines. Finally, we demonstrate a direct application of our method for dexterous grasp synthesis to refine the grasp quality. The code is available at https://github.com/JYChen18/DiffCollision.

</details>


### [41] [External Photoreflective Tactile Sensing Based on Surface Deformation Measurement](https://arxiv.org/abs/2511.06311)
*Seiichi Yamamoto,Hiroki Ishizuka,Takumi Kawasetsu,Koh Hosoda,Takayuki Kameoka,Kango Yanagida,Takato Horii,Sei Ikeda,Osamu Oshiro*

Main category: cs.RO

TL;DR: 提出了一种通过外部光模块感知软体机器人接触力的新方法，具有高耐久性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 开发一种柔性且简化的软体机器人的触觉感应方法，避免传感器损坏与复杂的制造过程。

Method: 采用可外部附加的光反射模块，测量硅胶表面的变形来估计接触力，省去嵌入式触觉传感器。

Result: 实验验证了该方法的有效性，表现出一致的力输出关系，低迟滞性，高重复性，能够可靠地在软体抓取器上检测抓取事件。

Conclusion: 通过外部光学模块读取软体机器人的表面变形，提供了一种有效的接触力感知方法，兼顾了柔性和可制造性。

Abstract: We present a tactile sensing method enabled by the mechanical compliance of soft robots; an externally attachable photoreflective module reads surface deformation of silicone skin to estimate contact force without embedding tactile transducers. Locating the sensor off the contact interface reduces damage risk, preserves softness, and simplifies fabrication and maintenance. We first characterize the optical sensing element and the compliant skin, thendetermine the design of a prototype tactile sensor. Compression experiments validate the approach, exhibiting a monotonic force output relationship consistent with theory, low hysteresis, high repeatability over repeated cycles, and small response indentation speeds. We further demonstrate integration on a soft robotic gripper, where the module reliably detects grasp events. Compared with liquid filled or wireembedded tactile skins, the proposed modular add on architecture enhances durability, reduces wiring complexity, and supports straightforward deployment across diverse robot geometries. Because the sensing principle reads skin strain patterns, it also suggests extensions to other somatosensory cues such as joint angle or actuator state estimation from surface deformation. Overall, leveraging surface compliance with an external optical module provides a practical and robust route to equip soft robots with force perception while preserving structural flexibility and manufacturability, paving the way for robotic applications and safe human robot collaboration.

</details>


### [42] [Towards Adaptive Humanoid Control via Multi-Behavior Distillation and Reinforced Fine-Tuning](https://arxiv.org/abs/2511.06371)
*Yingnan Zhao,Xinmiao Wang,Dewei Wang,Xinzhe Liu,Dan Lu,Qilong Han,Peng Liu,Chenjia Bai*

Main category: cs.RO

TL;DR: 自适应人形控制（AHC）方法通过多行为蒸馏和在线反馈强化调优，实现了在多种情况和地形中自适应的人形机器人 locomotion 控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要为每个技能训练独立政策，导致控制器的泛化能力有限，对于不规则地形和不同情况下的表现脆弱。

Method: 采用两阶段框架，首先训练主要步态政策并进行多行为蒸馏，然后通过在线反馈进行强化微调，提升控制器在复杂地形上的适应性。

Result: 我们提出的自适应人形控制（AHC）方法在不同技能和地形中学习自适应的步态控制器，展现了强大的适应性。

Conclusion: 实验验证表明，所提方法在各种情况下和地形中都具有强大的适应能力。

Abstract: Humanoid robots are promising to learn a diverse set of human-like locomotion behaviors, including standing up, walking, running, and jumping. However, existing methods predominantly require training independent policies for each skill, yielding behavior-specific controllers that exhibit limited generalization and brittle performance when deployed on irregular terrains and in diverse situations. To address this challenge, we propose Adaptive Humanoid Control (AHC) that adopts a two-stage framework to learn an adaptive humanoid locomotion controller across different skills and terrains. Specifically, we first train several primary locomotion policies and perform a multi-behavior distillation process to obtain a basic multi-behavior controller, facilitating adaptive behavior switching based on the environment. Then, we perform reinforced fine-tuning by collecting online feedback in performing adaptive behaviors on more diverse terrains, enhancing terrain adaptability for the controller. We conduct experiments in both simulation and real-world experiments in Unitree G1 robots. The results show that our method exhibits strong adaptability across various situations and terrains. Project website: https://ahc-humanoid.github.io.

</details>


### [43] [ArtReg: Visuo-Tactile based Pose Tracking and Manipulation of Unseen Articulated Objects](https://arxiv.org/abs/2511.06378)
*Prajval Kumar Murali,Mohsen Kaboli*

Main category: cs.RO

TL;DR: 提出了一种新颖的基于视觉和触觉的跟踪方法ArtReg，能在不知道物体几何信息的情况下，实现对复杂关节物体的感知和操控。


<details>
  <summary>Details</summary>
Motivation: 面对未知复杂结构的物体，机器人需要在没有先前知识的情况下进行感知、跟踪和操控，这是机器人技术的一个基本挑战。

Method: 采用基于视觉-触觉的跟踪方法ArtReg，通过无前知识的方式整合点云数据，实现物体的动态跟踪和操控。

Result: 通过实际机器人实验，验证了该方法的有效性，特别是在复杂物体和具有挑战性环境下的表现优于现有方法，并实现了目标驱动的操控。

Conclusion: 该方法提高了机器人对未知复杂关节物体的识别和操控能力，展现了在各种挑战条件下的鲁棒性和准确性。

Abstract: Robots operating in real-world environments frequently encounter unknown objects with complex structures and articulated components, such as doors, drawers, cabinets, and tools. The ability to perceive, track, and manipulate these objects without prior knowledge of their geometry or kinematic properties remains a fundamental challenge in robotics. In this work, we present a novel method for visuo-tactile-based tracking of unseen objects (single, multiple, or articulated) during robotic interaction without assuming any prior knowledge regarding object shape or dynamics. Our novel pose tracking approach termed ArtReg (stands for Articulated Registration) integrates visuo-tactile point clouds in an unscented Kalman Filter formulation in the SE(3) Lie Group for point cloud registration. ArtReg is used to detect possible articulated joints in objects using purposeful manipulation maneuvers such as pushing or hold-pulling with a two-robot team. Furthermore, we leverage ArtReg to develop a closed-loop controller for goal-driven manipulation of articulated objects to move the object into the desired pose configuration. We have extensively evaluated our approach on various types of unknown objects through real robot experiments. We also demonstrate the robustness of our method by evaluating objects with varying center of mass, low-light conditions, and with challenging visual backgrounds. Furthermore, we benchmarked our approach on a standard dataset of articulated objects and demonstrated improved performance in terms of pose accuracy compared to state-of-the-art methods. Our experiments indicate that robust and accurate pose tracking leveraging visuo-tactile information enables robots to perceive and interact with unseen complex articulated objects (with revolute or prismatic joints).

</details>


### [44] [From Demonstrations to Safe Deployment: Path-Consistent Safety Filtering for Diffusion Policies](https://arxiv.org/abs/2511.06385)
*Ralf Römer,Julian Balletshofer,Jakob Thumm,Marco Pavone,Angela P. Schoellig,Matthias Althoff*

Main category: cs.RO

TL;DR: 本论文提出路径一致性安全过滤（PACS）方法，旨在为扩散政策提供安全保障，保持任务成功率，并显著优于传统反应安全方法。


<details>
  <summary>Details</summary>
Motivation: 解决扩散政策(DPs)在复杂操作任务中缺乏安全性的问题，以及外部安全机制带来的不可预测行为和性能下降。

Method: 提出路径一致性安全过滤（PACS）方法

Result: PACS提供了动态环境中的正式安全保障，保持了任务成功率，并在任务成功率上比反应安全方法（如控制屏障函数）提高了68%。

Conclusion: PACS有效地解决了扩散政策在复杂任务中的安全性和性能下降问题，适用于实时部署。

Abstract: Diffusion policies (DPs) achieve state-of-the-art performance on complex manipulation tasks by learning from large-scale demonstration datasets, often spanning multiple embodiments and environments. However, they cannot guarantee safe behavior, so external safety mechanisms are needed. These, however, alter actions in ways unseen during training, causing unpredictable behavior and performance degradation. To address these problems, we propose path-consistent safety filtering (PACS) for DPs. Our approach performs path-consistent braking on a trajectory computed from the sequence of generated actions. In this way, we keep execution consistent with the policy's training distribution, maintaining the learned, task-completing behavior. To enable a real-time deployment and handle uncertainties, we verify safety using set-based reachability analysis. Our experimental evaluation in simulation and on three challenging real-world human-robot interaction tasks shows that PACS (a) provides formal safety guarantees in dynamic environments, (b) preserves task success rates, and (c) outperforms reactive safety approaches, such as control barrier functions, by up to 68% in terms of task success. Videos are available at our project website: https://tum-lsy.github.io/pacs/.

</details>


### [45] [Whole-Body Control With Terrain Estimation of A 6-DoF Wheeled Bipedal Robot](https://arxiv.org/abs/2511.06397)
*Cong Wen,Yunfei Li,Kexin Liu,Yixin Qiu,Xuanhong Liao,Tianyu Wang,Dingchuan Liu,Tao Zhang,Ximin Lyu*

Main category: cs.RO

TL;DR: 本文开发了一种六自由度轮式双足机器人，通过完整的动态模型和全身控制框架结合地形估计，实现对不平坦地形的有效行驶。


<details>
  <summary>Details</summary>
Motivation: 现有研究简化计算，忽略腿部动态，限制了机器人的运动潜力，同时在不平坦地形上行驶面临挑战。

Method: 开发完整动态模型和全身控制框架结合地形估计

Result: 提出一种新型的六自由度轮式双足机器人，验证了地形估计算法的性能和鲁棒性，成功在不平坦地形上行驶。

Conclusion: 所提出的控制框架能够提高轮式双足机器人在复杂地形上的运动能力，具有良好的实际应用前景。

Abstract: Wheeled bipedal robots have garnered increasing attention in exploration and inspection. However, most research simplifies calculations by ignoring leg dynamics, thereby restricting the robot's full motion potential. Additionally, robots face challenges when traversing uneven terrain. To address the aforementioned issue, we develop a complete dynamics model and design a whole-body control framework with terrain estimation for a novel 6 degrees of freedom wheeled bipedal robot. This model incorporates the closed-loop dynamics of the robot and a ground contact model based on the estimated ground normal vector. We use a LiDAR inertial odometry framework and improved Principal Component Analysis for terrain estimation. Task controllers, including PD control law and LQR, are employed for pose control and centroidal dynamics-based balance control, respectively. Furthermore, a hierarchical optimization approach is used to solve the whole-body control problem. We validate the performance of the terrain estimation algorithm and demonstrate the algorithm's robustness and ability to traverse uneven terrain through both simulation and real-world experiments.

</details>


### [46] [Real Garment Benchmark (RGBench): A Comprehensive Benchmark for Robotic Garment Manipulation featuring a High-Fidelity Scalable Simulator](https://arxiv.org/abs/2511.06434)
*Wenkang Hu,Xincheng Tang,Yanzhi E,Yitong Li,Zhengjie Shu,Wei Li,Huamin Wang,Ruigang Yang*

Main category: cs.RO

TL;DR: 本论文提出了Real Garment Benchmark（RGBench），解决了机器人操作可变形物体的挑战，提供新的高效模拟工具和丰富的服装模型。


<details>
  <summary>Details</summary>
Motivation: 虽然在刚性物体的机器人操作上取得了进展，但对可变形物体的应用受限于缺乏相应的模型和模拟器。因此，本论文旨在填补这一空白。

Method: 利用高性能模拟器和大量实际服装动态数据进行实验和评估。

Result: 该论文提出了Real Garment Benchmark（RGBench），为机器人操作服装提供了一个全面的基准。该基准包括6000多个服装网格模型，新的高性能模拟器，以及评估服装模拟质量的综合协议。实验结果表明，所提模拟器在模拟精度和速度上均优于现有模拟器。

Conclusion: RGBench将促进机器人服装操作的未来研究，并提高模拟的真实感和准确性。

Abstract: While there has been significant progress to use simulated data to learn robotic manipulation of rigid objects, applying its success to deformable objects has been hindered by the lack of both deformable object models and realistic non-rigid body simulators. In this paper, we present Real Garment Benchmark (RGBench), a comprehensive benchmark for robotic manipulation of garments. It features a diverse set of over 6000 garment mesh models, a new high-performance simulator, and a comprehensive protocol to evaluate garment simulation quality with carefully measured real garment dynamics. Our experiments demonstrate that our simulator outperforms currently available cloth simulators by a large margin, reducing simulation error by 20% while maintaining a speed of 3 times faster. We will publicly release RGBench to accelerate future research in robotic garment manipulation. Website: https://rgbench.github.io/

</details>


### [47] [Sim-to-Real Transfer in Deep Reinforcement Learning for Bipedal Locomotion](https://arxiv.org/abs/2511.06465)
*Lingfan Bao,Tianhu Peng,Chengxu Zhou*

Main category: cs.RO

TL;DR: 本章探讨了深度强化学习在双足行走中的模拟到现实转移挑战，分析了主要差距来源，并提出了改进物理逼真度和增强策略鲁棒性的解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习在双足行走中模拟与现实之间的转移问题的挑战。

Method: 分析双足行走中深度强化学习中的模拟到现实转移问题及其解决方案。

Result: 提出了两种互补的解决策略：一种是通过提高模拟器的物理逼真度来缩小差距，另一种是通过在模拟中进行鲁棒性训练来增强策略的抗干扰性。

Conclusion: 通过整合这两种策略，建立了一套战略框架，为开发和评估强健的模拟到现实解决方案提供了清晰的路线图。

Abstract: This chapter addresses the critical challenge of simulation-to-reality (sim-to-real) transfer for deep reinforcement learning (DRL) in bipedal locomotion. After contextualizing the problem within various control architectures, we dissect the ``curse of simulation'' by analyzing the primary sources of sim-to-real gap: robot dynamics, contact modeling, state estimation, and numerical solvers. Building on this diagnosis, we structure the solutions around two complementary philosophies. The first is to shrink the gap through model-centric strategies that systematically improve the simulator's physical fidelity. The second is to harden the policy, a complementary approach that uses in-simulation robustness training and post-deployment adaptation to make the policy inherently resilient to model inaccuracies. The chapter concludes by synthesizing these philosophies into a strategic framework, providing a clear roadmap for developing and evaluating robust sim-to-real solutions.

</details>


### [48] [A Low-Rank Method for Vision Language Model Hallucination Mitigation in Autonomous Driving](https://arxiv.org/abs/2511.06496)
*Keke Long,Jiacheng Guo,Tianyun Zhang,Hongkai Yu,Xiaopeng Li*

Main category: cs.RO

TL;DR: 本文提出了一种新的低秩方法来自动评估视觉语言模型生成标题的幻觉水平，利用残差大小对其进行排序，实验表明方法有效且效率高。


<details>
  <summary>Details</summary>
Motivation: 在自主驾驶中，视觉语言模型可能会生成基于输入的虚假细节，因此需要一种有效的方法来检测和减轻这种幻觉现象。

Method: 通过构建句子嵌入矩阵，将其分解为低秩共识成分和稀疏残差，使用稀疏残差的大小来对候选说明进行排序。

Result: 本文提出了一种新的低秩方法，通过自动排名不同视觉语言模型生成的候选说明，来检测和减轻幻觉现象。该方法只使用说明文本，不需要外部参考或模型内部访问，涉及通过构建句子嵌入矩阵并将其分解为低秩共识成分和稀疏残差来实现。

Conclusion: 实验结果表明，提出的方法在识别幻觉-free标题的准确率上超过其他方法，且能显著减少推理时间，非常适合实时自主驾驶应用。

Abstract: Vision Language Models (VLMs) are increasingly used in autonomous driving to help understand traffic scenes, but they sometimes produce hallucinations, which are false details not grounded in the visual input. Detecting and mitigating hallucinations is challenging when ground-truth references are unavailable and model internals are inaccessible. This paper proposes a novel self-contained low-rank approach to automatically rank multiple candidate captions generated by multiple VLMs based on their hallucination levels, using only the captions themselves without requiring external references or model access. By constructing a sentence-embedding matrix and decomposing it into a low-rank consensus component and a sparse residual, we use the residual magnitude to rank captions: selecting the one with the smallest residual as the most hallucination-free. Experiments on the NuScenes dataset demonstrate that our approach achieves 87% selection accuracy in identifying hallucination-free captions, representing a 19% improvement over the unfiltered baseline and a 6-10% improvement over multi-agent debate method. The sorting produced by sparse error magnitudes shows strong correlation with human judgments of hallucinations, validating our scoring mechanism. Additionally, our method, which can be easily parallelized, reduces inference time by 51-67% compared to debate approaches, making it practical for real-time autonomous driving applications.

</details>


### [49] [Adaptive PID Control for Robotic Systems via Hierarchical Meta-Learning and Reinforcement Learning with Physics-Based Data Augmentation](https://arxiv.org/abs/2511.06500)
*JiaHao Wu,ShengWen Yu*

Main category: cs.RO

TL;DR: 提出了一种结合元学习和强化学习的分层控制框架，解决了PID控制器参数调节的挑战，通过物理数据增强策略提高了样本效率。实验表明该方法在不同平台上均有显著改进，且强化学习的效果依赖于元学习的质量和误差分布。


<details>
  <summary>Details</summary>
Motivation: 在工业机器人中，PID控制器因其简单性和可靠性仍然是主要选择，但手动调节PID参数耗时且需要广泛的领域专长。

Method: 建立了一种新的分层控制框架，结合了元学习进行PID初始化和强化学习进行在线适应，采用基于物理的数据增强策略来提高样本效率。

Result: 在Franka Panda（平均绝对误差为6.26°）上获得了16.6%的平均改善，在Laikago平台上发现优化天花板效应，且该方法在干扰下表现稳健，训练时间仅需10分钟。

Conclusion: 该方法的有效性高度依赖于元学习基准质量和误差分布，为分层控制系统的设计提供了重要指导。

Abstract: Proportional-Integral-Derivative (PID) controllers remain the predominant choice in industrial robotics due to their simplicity and reliability. However, manual tuning of PID parameters for diverse robotic platforms is time-consuming and requires extensive domain expertise. This paper presents a novel hierarchical control framework that combines meta-learning for PID initialization and reinforcement learning (RL) for online adaptation. To address the sample efficiency challenge, a \textit{physics-based data augmentation} strategy is introduced that generates virtual robot configurations by systematically perturbing physical parameters, enabling effective meta-learning with limited real robot data. The proposed approach is evaluated on two heterogeneous platforms: a 9-DOF Franka Panda manipulator and a 12-DOF Laikago quadruped robot. Experimental results demonstrate that the proposed method achieves 16.6\% average improvement on Franka Panda (6.26° MAE), with exceptional gains in high-load joints (J2: 80.4\% improvement from 12.36° to 2.42°). Critically, this work discovers the \textit{optimization ceiling effect}: RL achieves dramatic improvements when meta-learning exhibits localized high-error joints, but provides no benefit (0.0\%) when baseline performance is uniformly strong, as observed in Laikago. The method demonstrates robust performance under disturbances (parameter uncertainty: +19.2\%, no disturbance: +16.6\%, average: +10.0\%) with only 10 minutes of training time. Multi-seed analysis across 100 random initializations confirms stable performance (4.81+/-1.64\% average). These results establish that RL effectiveness is highly dependent on meta-learning baseline quality and error distribution, providing important design guidance for hierarchical control systems.

</details>


### [50] [Robotic versus Human Teleoperation for Remote Ultrasound](https://arxiv.org/abs/2511.07275)
*David Black,Septimiu Salcudean*

Main category: cs.RO

TL;DR: 本研究比较了人类和机器人远程操作在超声检查中的表现，发现人类远程操作在多方面更具优势，且性能相当


<details>
  <summary>Details</summary>
Motivation: 解决偏远地区缺乏专业超声检查人员的问题，促进超声技术的可及性

Method: 比较人类远程操作与机器人远程操作的差异

Result: 人类远程操作在完成时间、位置准确度上与机器人远程操作无显著差异，但提供了更一致的力应用

Conclusion: 人类远程操作以其更高的实用性和便捷性，成为偏远地区超声检查的有效选择。

Abstract: Diagnostic medical ultrasound is widely used, safe, and relatively low cost but requires a high degree of expertise to acquire and interpret the images. Personnel with this expertise are often not available outside of larger cities, leading to difficult, costly travel and long wait times for rural populations. To address this issue, tele-ultrasound techniques are being developed, including robotic teleoperation and recently human teleoperation, in which a novice user is remotely guided in a hand-over-hand manner through mixed reality to perform an ultrasound exam. These methods have not been compared, and their relative strengths are unknown. Human teleoperation may be more practical than robotics for small communities due to its lower cost and complexity, but this is only relevant if the performance is comparable. This paper therefore evaluates the differences between human and robotic teleoperation, examining practical aspects such as setup time and flexibility and experimentally comparing performance metrics such as completion time, position tracking, and force consistency. It is found that human teleoperation does not lead to statistically significant differences in completion time or position accuracy, with mean differences of 1.8% and 0.5%, respectively, and provides more consistent force application despite being substantially more practical and accessible.

</details>


### [51] [Koopman global linearization of contact dynamics for robot locomotion and manipulation enables elaborate control](https://arxiv.org/abs/2511.06515)
*Cormac O'Neill,Jasmine Terrones,H. Harry Asada*

Main category: cs.RO

TL;DR: 通过应用Koopman算子，该论文提出了一种新方法，克服了接触动态切换带来的控制困难，实现了机器人在与环境互动时的实时控制。


<details>
  <summary>Details</summary>
Motivation: 动态接触控制是机器人的一大挑战，接触边界的动力学切换使得控制变得复杂，因此需要一个有效的方法来实现控制。

Method: 使用Koopman算子将接触变化引起的分段动力学整合为统一的全局线性模型，应用于机器人控制。

Result: 该研究提出了一种新方法，利用Koopman算子将由于接触变化而分段的动力学合并为统一的全局线性模型，从而控制与环境动态接触的机器人。

Conclusion: 研究表明，采用Koopman算子的方法能够使机器人在多接触变化的时间范围内实时发现复杂的控制策略，该方法在机器人领域之外也具有广泛的应用潜力。

Abstract: Controlling robots that dynamically engage in contact with their environment is a pressing challenge. Whether a legged robot making-and-breaking contact with a floor, or a manipulator grasping objects, contact is everywhere. Unfortunately, the switching of dynamics at contact boundaries makes control difficult. Predictive controllers face non-convex optimization problems when contact is involved. Here, we overcome this difficulty by applying Koopman operators to subsume the segmented dynamics due to contact changes into a unified, globally-linear model in an embedding space. We show that viscoelastic contact at robot-environment interactions underpins the use of Koopman operators without approximation to control inputs. This methodology enables the convex Model Predictive Control of a legged robot, and the real-time control of a manipulator engaged in dynamic pushing. In this work, we show that our method allows robots to discover elaborate control strategies in real-time over time horizons with multiple contact changes, and the method is applicable to broad fields beyond robotics.

</details>


### [52] [CoFineLLM: Conformal Finetuning of LLMs for Language-Instructed Robot Planning](https://arxiv.org/abs/2511.06575)
*Jun Wang,Yevgeniy Vorobeychik,Yiannis Kantaros*

Main category: cs.RO

TL;DR: 提出一种新方法CoFineLLM，通过微调LLM以减少用户干预，提高规划的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长时间任务中的可靠性较低，容易产生过度自信的错误输出，因此需要提高其输出的准确性和减少用户的干预。

Method: 采用共形预测框架，对LLM进行微调，从而使模型能够生成更小的预测集合。

Result: 提出了一种名为CoFineLLM的框架，通过共形预测（CP）对大语言模型（LLM）进行微调，以减少预测集合的大小，提高自动化能力，降低用户干预。

Conclusion: CoFineLLM显著减小了LLM的预测集合大小，降低了对用户帮助的需求，提升了机器人规划任务的效果。

Abstract: Large Language Models (LLMs) have recently emerged as planners for language-instructed agents, generating sequences of actions to accomplish natural language tasks. However, their reliability remains a challenge, especially in long-horizon tasks, since they often produce overconfident yet wrong outputs. Conformal Prediction (CP) has been leveraged to address this issue by wrapping LLM outputs into prediction sets that contain the correct action with a user-defined confidence. When the prediction set is a singleton, the planner executes that action; otherwise, it requests help from a user. This has led to LLM-based planners that can ensure plan correctness with a user-defined probability. However, as LLMs are trained in an uncertainty-agnostic manner, without awareness of prediction sets, they tend to produce unnecessarily large sets, particularly at higher confidence levels, resulting in frequent human interventions limiting autonomous deployment. To address this, we introduce CoFineLLM (Conformal Finetuning for LLMs), the first CP-aware finetuning framework for LLM-based planners that explicitly reduces prediction-set size and, in turn, the need for user interventions. We evaluate our approach on multiple language-instructed robot planning problems and show consistent improvements over uncertainty-aware and uncertainty-agnostic finetuning baselines in terms of prediction-set size, and help rates. Finally, we demonstrate robustness of our method to out-of-distribution scenarios in hardware experiments.

</details>


### [53] [Underactuated Biomimetic Autonomous Underwater Vehicle for Ecosystem Monitoring](https://arxiv.org/abs/2511.06578)
*Kaustubh Singh,Shivam Kumar,Shashikant Pawar,Sandeep Manjanna*

Main category: cs.RO

TL;DR: 提出了一种适用于海洋和淡水生态监测的欠驱动仿生水下机器人，结合了机制设计和强化学习技术。


<details>
  <summary>Details</summary>
Motivation: 开发适合生态监测的水下机器人，提高水下探测能力和效率。

Method: 采用更新的机械设计，并利用强化学习技术学习最小驱动行为。

Result: 初步机械设计和游泳行为在FishGym模拟器上测试，展示出机器人在控制和游动方面的良好性能。

Conclusion: 该研究显示了新型鱼类机器人在生态监测中的潜力，强化学习能够有效提升其游泳行为。

Abstract: In this paper, we present an underactuated biomimetic underwater robot that is suitable for ecosystem monitoring in both marine and freshwater environments. We present an updated mechanical design for a fish-like robot and propose minimal actuation behaviors learned using reinforcement learning techniques. We present our preliminary mechanical design of the tail oscillation mechanism and illustrate the swimming behaviors on FishGym simulator, where the reinforcement learning techniques will be tested on

</details>


### [54] [How Do VLAs Effectively Inherit from VLMs?](https://arxiv.org/abs/2511.06619)
*Chuheng Zhang,Rushuai Yang,Xiaoyu Chen,Kaixin Wang,Li Zhao,Yi Chen,Jiang Bian*

Main category: cs.RO

TL;DR: 本文介绍了GrinningFace基准任务，评估VLA模型如何利用VLM的先验知识，通过任务的成功与否来观察知识转移的效果，并比较多种知识转移技术。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决VLAs如何有效继承VLM的先验知识这一关键问题，以实现更加可推广的具身控制能力。

Method: 在模拟环境和真实机器人上实施GrinningFace任务，比较了多种知识转移技术，包括参数高效微调、VLM冻结、协同训练和动作预测等。

Result: 该研究探讨了视觉-语言-行动(VLA)模型如何有效继承来自视觉-语言模型(VLM)的知识。通过引入一项名为GrinningFace的诊断基准，评价机器人在执行emoji表情图标的操作任务时的表现，旨在通过该任务来评估VLA模型的知识转移能力。

Conclusion: 研究表明，保持VLM先验知识对于VLA的泛化至关重要，为未来开发可泛化的具身AI系统提供指导。

Abstract: Vision-language-action (VLA) models hold the promise to attain generalizable embodied control. To achieve this, a pervasive paradigm is to leverage the rich vision-semantic priors of large vision-language models (VLMs). However, the fundamental question persists: How do VLAs effectively inherit the prior knowledge from VLMs? To address this critical question, we introduce a diagnostic benchmark, GrinningFace, an emoji tabletop manipulation task where the robot arm is asked to place objects onto printed emojis corresponding to language instructions. This task design is particularly revealing -- knowledge associated with emojis is ubiquitous in Internet-scale datasets used for VLM pre-training, yet emojis themselves are largely absent from standard robotics datasets. Consequently, they provide a clean proxy: successful task completion indicates effective transfer of VLM priors to embodied control. We implement this diagnostic task in both simulated environment and a real robot, and compare various promising techniques for knowledge transfer. Specifically, we investigate the effects of parameter-efficient fine-tuning, VLM freezing, co-training, predicting discretized actions, and predicting latent actions. Through systematic evaluation, our work not only demonstrates the critical importance of preserving VLM priors for the generalization of VLA but also establishes guidelines for future research in developing truly generalizable embodied AI systems.

</details>


### [55] [Rapidly Learning Soft Robot Control via Implicit Time-Stepping](https://arxiv.org/abs/2511.06667)
*Andrew Choi,Dezhong Tong*

Main category: cs.RO

TL;DR: 本研究展示了通过隐式时间步进实现快速软机器人策略学习，以及介绍了 delta 自然曲率控制方法，并证明了其在多个任务中的优越性。


<details>
  <summary>Details</summary>
Motivation: 填补软机器人模拟框架的缺口，提高软机器人策略学习的效率和可操作性。

Method: 使用通用的隐式软体模拟器DisMech和delta自然曲率控制方法进行软机器人策略学习。

Result: 使用隐式时间步进实现最大40倍的速度提升，同时保持模拟的准确性。

Conclusion: 隐式时间步进能够在不牺牲准确性的前提下，实现显著的速度提升，形式上为软机器人政策学习提供了有效的工具。

Abstract: With the explosive growth of rigid-body simulators, policy learning in simulation has become the de facto standard for most rigid morphologies. In contrast, soft robotic simulation frameworks remain scarce and are seldom adopted by the soft robotics community. This gap stems partly from the lack of easy-to-use, general-purpose frameworks and partly from the high computational cost of accurately simulating continuum mechanics, which often renders policy learning infeasible. In this work, we demonstrate that rapid soft robot policy learning is indeed achievable via implicit time-stepping. Our simulator of choice, DisMech, is a general-purpose, fully implicit soft-body simulator capable of handling both soft dynamics and frictional contact. We further introduce delta natural curvature control, a method analogous to delta joint position control in rigid manipulators, providing an intuitive and effective means of enacting control for soft robot learning. To highlight the benefits of implicit time-stepping and delta curvature control, we conduct extensive comparisons across four diverse soft manipulator tasks against one of the most widely used soft-body frameworks, Elastica. With implicit time-stepping, parallel stepping of 500 environments achieves up to 6x faster speeds for non-contact cases and up to 40x faster for contact-rich scenarios. Finally, a comprehensive sim-to-sim gap evaluation--training policies in one simulator and evaluating them in another--demonstrates that implicit time-stepping provides a rare free lunch: dramatic speedups achieved without sacrificing accuracy.

</details>


### [56] [Programmable Telescopic Soft Pneumatic Actuators for Deployable and Shape Morphing Soft Robots](https://arxiv.org/abs/2511.06673)
*Joel Kemp,Andre Farinha,David Howard,Krishna Manaswi Digumarti,Josh Pinskier*

Main category: cs.RO

TL;DR: 本文提出了一种新的参数化软气动驱动器（PTSPAs），通过参数化几何生成器定制驱动器，研究了设计空间，并在可部署的软四足动物中展示了其应用。


<details>
  <summary>Details</summary>
Motivation: 由于维度诅咒，目前没有有效的方式来直接利用软机器人设计的自由度，参数化的设计集为这种软机器人设计提供了一条可行的路径。

Method: 提出了一种参数化的几何生成器，用于从高层输入定制软驱动器模型，并通过半自动化实验和对关键参数的系统探索来研究新的设计空间。

Result: 使用PTSPAs，我们表征了驱动器的伸长/弯曲、扩展和刚度，并揭示了关键设计参数与性能之间的明确关系。还展示了这些驱动器在可部署的软四足动物中的应用，能够在狭小空间内自动适应行走。

Conclusion: PTSPAs为可部署和形状变换结构提供了一种新的设计范式，适用于需要大幅度长度变化的应用。

Abstract: Soft Robotics presents a rich canvas for free-form and continuum devices capable of exerting forces in any direction and transforming between arbitrary configurations. However, there is no current way to tractably and directly exploit the design freedom due to the curse of dimensionality. Parameterisable sets of designs offer a pathway towards tractable, modular soft robotics that appropriately harness the behavioural freeform of soft structures to create rich embodied behaviours. In this work, we present a parametrised class of soft actuators, Programmable Telescopic Soft Pneumatic Actuators (PTSPAs). PTSPAs expand axially on inflation for deployable structures and manipulation in challenging confined spaces. We introduce a parametric geometry generator to customise actuator models from high-level inputs, and explore the new design space through semi-automated experimentation and systematic exploration of key parameters. Using it we characterise the actuators' extension/bending, expansion, and stiffness and reveal clear relationships between key design parameters and performance. Finally we demonstrate the application of the actuators in a deployable soft quadruped whose legs deploy to walk, enabling automatic adaptation to confined spaces. PTSPAs present new design paradigm for deployable and shape morphing structures and wherever large length changes are required.

</details>


### [57] [Physically-Grounded Goal Imagination: Physics-Informed Variational Autoencoder for Self-Supervised Reinforcement Learning](https://arxiv.org/abs/2511.06745)
*Lan Thi Ha Nguyen,Kien Ton Manh,Anh Do Duc,Nam Pham Hai*

Main category: cs.RO

TL;DR: 本研究提出了一种物理信息引导的变体（PI-RIG），通过增强的VAE生成物理一致的目标，从而改善机器人在视觉操控任务中的技能获取效果。


<details>
  <summary>Details</summary>
Motivation: 自监督目标条件强化学习使得机器人能够在没有人类监督的情况下自主获取多样化的技能，但存在目标设定的问题，即机器人需要提出在当前环境中可实现且多样的目标。

Method: 结合物理约束进行VAE训练，明确分离潜在空间并强制执行物理一致性，以生成可实现的目标。

Result: 提出了一种物理信息引导的变体，该变体通过增强的物理信息变分自编码器生成物理一致且可实现的目标，大大提高了目标生成质量。

Conclusion: 经过广泛实验，物理信息引导的目标生成显著提高了提出目标的质量，从而提升了在各种视觉机器人操作任务中的探索效果和技能获取效率。

Abstract: Self-supervised goal-conditioned reinforcement learning enables robots to autonomously acquire diverse skills without human supervision. However, a central challenge is the goal setting problem: robots must propose feasible and diverse goals that are achievable in their current environment. Existing methods like RIG (Visual Reinforcement Learning with Imagined Goals) use variational autoencoder (VAE) to generate goals in a learned latent space but have the limitation of producing physically implausible goals that hinder learning efficiency. We propose Physics-Informed RIG (PI-RIG), which integrates physical constraints directly into the VAE training process through a novel Enhanced Physics-Informed Variational Autoencoder (Enhanced p3-VAE), enabling the generation of physically consistent and achievable goals. Our key innovation is the explicit separation of the latent space into physics variables governing object dynamics and environmental factors capturing visual appearance, while enforcing physical consistency through differential equation constraints and conservation laws. This enables the generation of physically consistent and achievable goals that respect fundamental physical principles such as object permanence, collision constraints, and dynamic feasibility. Through extensive experiments, we demonstrate that this physics-informed goal generation significantly improves the quality of proposed goals, leading to more effective exploration and better skill acquisition in visual robotic manipulation tasks including reaching, pushing, and pick-and-place scenarios.

</details>


### [58] [Semi-distributed Cross-modal Air-Ground Relative Localization](https://arxiv.org/abs/2511.06749)
*Weining Lu,Deer Bin,Lian Ma,Ming Ma,Zhihao Ma,Xiangyang Chen,Longfei Wang,Yixiao Feng,Zhouxian Jiang,Yongliang Shi,Bin Liang*

Main category: cs.RO

TL;DR: 提出一种有效的半分布式空地相对定位框架，利用UGV与UAV同时独立执行SLAM，能提高灵活性和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前的多机器人SLAM系统受限于所有机器人状态估计的耦合，影响灵活性和准确性。

Method: UGV和UAV独立执行SLAM，UGV利用LiDAR、相机和IMU进行局部束调整，优化相机姿态并估计UGV与UAV间的相对姿态。

Result: 实验结果表明，该方法在相对定位的准确性和效率上均表现优异，仅传输关键点像素及其描述符，有效控制通信带宽。

Conclusion: 该方法在准确性和效率方面表现出色，能够有效约束通信带宽，适用于空地协作任务。

Abstract: Efficient, accurate, and flexible relative localization is crucial in air-ground collaborative tasks. However, current approaches for robot relative localization are primarily realized in the form of distributed multi-robot SLAM systems with the same sensor configuration, which are tightly coupled with the state estimation of all robots, limiting both flexibility and accuracy. To this end, we fully leverage the high capacity of Unmanned Ground Vehicle (UGV) to integrate multiple sensors, enabling a semi-distributed cross-modal air-ground relative localization framework. In this work, both the UGV and the Unmanned Aerial Vehicle (UAV) independently perform SLAM while extracting deep learning-based keypoints and global descriptors, which decouples the relative localization from the state estimation of all agents. The UGV employs a local Bundle Adjustment (BA) with LiDAR, camera, and an IMU to rapidly obtain accurate relative pose estimates. The BA process adopts sparse keypoint optimization and is divided into two stages: First, optimizing camera poses interpolated from LiDAR-Inertial Odometry (LIO), followed by estimating the relative camera poses between the UGV and UAV. Additionally, we implement an incremental loop closure detection algorithm using deep learning-based descriptors to maintain and retrieve keyframes efficiently. Experimental results demonstrate that our method achieves outstanding performance in both accuracy and efficiency. Unlike traditional multi-robot SLAM approaches that transmit images or point clouds, our method only transmits keypoint pixels and their descriptors, effectively constraining the communication bandwidth under 0.3 Mbps. Codes and data will be publicly available on https://github.com/Ascbpiac/cross-model-relative-localization.git.

</details>


### [59] [SlotVLA: Towards Modeling of Object-Relation Representations in Robotic Manipulation](https://arxiv.org/abs/2511.06754)
*Taisei Hanyu,Nhat Chung,Huy Le,Toan Nguyen,Yuki Ikebe,Anthony Gunderman,Duy Nguyen Ho Minh,Khoa Vo,Tung Kieu,Kashu Yamazaki,Chase Rainwater,Anh Nguyen,Ngan Le*

Main category: cs.RO

TL;DR: 本研究提出LIBERO+数据集和SlotVLA框架，通过对象-关系中心的表示方法，提高机器人操作的效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 探讨紧凑、对象中心和对象-关系表示在多任务机器人操作中的潜力，解决现有模型的效率和可解释性问题。

Method: 研究对象-关系中心的表示方法，并提出了LIBERO+基准数据集与SlotVLA框架。

Result: 实验结果表明，对象中心和对象-关系代表显著减少了所需的视觉标记数量，同时提供了竞争性的泛化能力。

Conclusion: LIBERO+和SlotVLA为基于对象关系的机器人操作提供了紧凑、可解释和有效的基础，能够提升机器人的操控能力。

Abstract: Inspired by how humans reason over discrete objects and their relationships, we explore whether compact object-centric and object-relation representations can form a foundation for multitask robotic manipulation. Most existing robotic multitask models rely on dense embeddings that entangle both object and background cues, raising concerns about both efficiency and interpretability. In contrast, we study object-relation-centric representations as a pathway to more structured, efficient, and explainable visuomotor control. Our contributions are two-fold. First, we introduce LIBERO+, a fine-grained benchmark dataset designed to enable and evaluate object-relation reasoning in robotic manipulation. Unlike prior datasets, LIBERO+ provides object-centric annotations that enrich demonstrations with box- and mask-level labels as well as instance-level temporal tracking, supporting compact and interpretable visuomotor representations. Second, we propose SlotVLA, a slot-attention-based framework that captures both objects and their relations for action decoding. It uses a slot-based visual tokenizer to maintain consistent temporal object representations, a relation-centric decoder to produce task-relevant embeddings, and an LLM-driven module that translates these embeddings into executable actions. Experiments on LIBERO+ demonstrate that object-centric slot and object-relation slot representations drastically reduce the number of required visual tokens, while providing competitive generalization. Together, LIBERO+ and SlotVLA provide a compact, interpretable, and effective foundation for advancing object-relation-centric robotic manipulation.

</details>


### [60] [Human-Level Actuation for Humanoids](https://arxiv.org/abs/2511.06796)
*MD-Nazmus Sunbeam*

Main category: cs.RO

TL;DR: 本文提供了一种新的框架，使得类人机器人在激活能力上达成“人类水平”的量化和可比分析。


<details>
  <summary>Details</summary>
Motivation: 当前关于类人机器人的“人类水平”激活能力的声明多为定性，缺乏实际的定量分析和比较。这一研究希望填补这一空白。

Method: 采用运动学自由度图谱、定义人类等效包络和计算激活评分，结合动力学、功率监测和热测试等方法进行详细测量。

Result: 本文提出了一种综合框架，用于量化和比较类人机器人在“人类水平”激活方面的能力。通过建立运动学自由度（DoF）图谱、定义人类等效包络（HEE）和计算人类水平激活评分（HLAS），该框架能够明确机器人在执行特定动作时的扭矩、功率和耐力要求，并通过一系列实验提供可重复的测量协议。

Conclusion: 该框架为类人机器人的设计规范和激活系统的比较标准提供了基础，有助于进一步的发展和优化。

Abstract: Claims that humanoid robots achieve ``human-level'' actuation are common but rarely quantified. Peak torque or speed specifications tell us little about whether a joint can deliver the right combination of torque, power, and endurance at task-relevant postures and rates. We introduce a comprehensive framework that makes ``human-level'' measurable and comparable across systems. Our approach has three components. First, a kinematic \emph{DoF atlas} standardizes joint coordinate systems and ranges of motion using ISB-based conventions, ensuring that human and robot joints are compared in the same reference frames. Second, \emph{Human-Equivalence Envelopes (HEE)} define per-joint requirements by measuring whether a robot meets human torque \emph{and} power simultaneously at the same joint angle and rate $(q,ω)$, weighted by positive mechanical work in task-specific bands (walking, stairs, lifting, reaching, and hand actions). Third, the \emph{Human-Level Actuation Score (HLAS)} aggregates six physically grounded factors: workspace coverage (ROM and DoF), HEE coverage, torque-mode bandwidth, efficiency, and thermal sustainability. We provide detailed measurement protocols using dynamometry, electrical power monitoring, and thermal testing that yield every HLAS input from reproducible experiments. A worked example demonstrates HLAS computation for a multi-joint humanoid, showing how the score exposes actuator trade-offs (gearing ratio versus bandwidth and efficiency) that peak-torque specifications obscure. The framework serves as both a design specification for humanoid development and a benchmarking standard for comparing actuation systems, with all components grounded in published human biomechanics data.

</details>


### [61] [Vision-Aided Online A* Path Planning for Efficient and Safe Navigation of Service Robots](https://arxiv.org/abs/2511.06801)
*Praveen Kumar,Tushar Sandhan*

Main category: cs.RO

TL;DR: 本论文提出了一种新颖框架，解决了自主服务机器人在复杂人类环境中导航的感知与规划难题.


<details>
  <summary>Details</summary>
Motivation: 解决传统导航系统缺乏语义感知问题，提升自主服务机器人在复杂环境中的定位与导航能力.

Method: 通过将轻量级感知模块与在线A*规划器紧密集成，使用语义分割模型识别用户定义的视觉约束，进而指导机器人导航.

Result: 验证表明该框架在高保真仿真和实际机器人平台上的实时表现强大，能有效应对复杂环境。

Conclusion: 研究结果表明，使用该框架的机器人能够安全高效地应对复杂环境中的视觉线索，而不依赖传统规划方法.

Abstract: The deployment of autonomous service robots in human-centric environments is hindered by a critical gap in perception and planning. Traditional navigation systems rely on expensive LiDARs that, while geometrically precise, are seman- tically unaware, they cannot distinguish a important document on an office floor from a harmless piece of litter, treating both as physically traversable. While advanced semantic segmentation exists, no prior work has successfully integrated this visual intelligence into a real-time path planner that is efficient enough for low-cost, embedded hardware. This paper presents a frame- work to bridge this gap, delivering context-aware navigation on an affordable robotic platform. Our approach centers on a novel, tight integration of a lightweight perception module with an online A* planner. The perception system employs a semantic segmentation model to identify user-defined visual constraints, enabling the robot to navigate based on contextual importance rather than physical size alone. This adaptability allows an operator to define what is critical for a given task, be it sensitive papers in an office or safety lines in a factory, thus resolving the ambiguity of what to avoid. This semantic perception is seamlessly fused with geometric data. The identified visual constraints are projected as non-geometric obstacles onto a global map that is continuously updated from sensor data, enabling robust navigation through both partially known and unknown environments. We validate our framework through extensive experiments in high-fidelity simulations and on a real-world robotic platform. The results demonstrate robust, real-time performance, proving that a cost- effective robot can safely navigate complex environments while respecting critical visual cues invisible to traditional planners.

</details>


### [62] [Vision-Based System Identification of a Quadrotor](https://arxiv.org/abs/2511.06839)
*Selim Ahmet Iz,Mustafa Unel*

Main category: cs.RO

TL;DR: 本论文研究了基于视觉的系统识别技术在四旋翼模型和控制中的应用，结果显示这些技术能够提升四旋翼的建模精度和控制性能，展示了未来研究的潜力。


<details>
  <summary>Details</summary>
Motivation: 旨在解决四旋翼建模中复杂性和限制，尤其是推力和阻力系数方面的问题。

Method: 采用灰箱建模技术，结合从机载视觉系统获取的数据，设计了LQR控制器。

Result: 通过实验和分析，验证了基于视觉的系统识别的有效性，同时展示了模型之间的一致性。

Conclusion: 本研究显示了基于视觉的系统识别技术在四旋翼建模和控制中的有效性，突显了其在提升四旋翼性能和操作能力方面的潜力。

Abstract: This paper explores the application of vision-based system identification techniques in quadrotor modeling and control. Through experiments and analysis, we address the complexities and limitations of quadrotor modeling, particularly in relation to thrust and drag coefficients. Grey-box modeling is employed to mitigate uncertainties, and the effectiveness of an onboard vision system is evaluated. An LQR controller is designed based on a system identification model using data from the onboard vision system. The results demonstrate consistent performance between the models, validating the efficacy of vision based system identification. This study highlights the potential of vision-based techniques in enhancing quadrotor modeling and control, contributing to improved performance and operational capabilities. Our findings provide insights into the usability and consistency of these techniques, paving the way for future research in quadrotor performance enhancement, fault detection, and decision-making processes.

</details>


### [63] [Multi-Agent AI Framework for Road Situation Detection and C-ITS Message Generation](https://arxiv.org/abs/2511.06892)
*Kailin Tong,Selim Solmaz,Kenan Mujkic,Gottfried Allmer,Bo Leng*

Main category: cs.RO

TL;DR: 本研究介绍了一种结合多模态大语言模型与视觉感知的多智能体AI框架，用于改进道路情况监测，尽管在检测召回率上表现优异，但仍存在误报和性能下降的问题，指出了对专门领域模型微调的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有的道路情况检测方法在预定义场景中表现良好，但在未见过的案例中容易失败，且缺乏语义解释，这对可靠的交通推荐至关重要。

Method: 构建了一个多智能体AI框架，该框架结合了多模态大语言模型与基于视觉的感知，用于道路情况监测。

Result: 实验结果显示，在情况检测中实现了100%的召回率，消息格式正确性完美，但模型存在误报且在车道数量、行驶车道状态和原因代码方面的表现下降。Gemini-2.5-Flash尽管在一般任务上更强，但在检测准确性和语义理解方面表现不佳，并且延迟更高。

Conclusion: 该研究结果表明，需要对特定领域的LLM或MLLM进行微调，以提升智能交通应用的表现。

Abstract: Conventional road-situation detection methods achieve strong performance in predefined scenarios but fail in unseen cases and lack semantic interpretation, which is crucial for reliable traffic recommendations. This work introduces a multi-agent AI framework that combines multimodal large language models (MLLMs) with vision-based perception for road-situation monitoring. The framework processes camera feeds and coordinates dedicated agents for situation detection, distance estimation, decision-making, and Cooperative Intelligent Transport System (C-ITS) message generation. Evaluation is conducted on a custom dataset of 103 images extracted from 20 videos of the TAD dataset. Both Gemini-2.0-Flash and Gemini-2.5-Flash were evaluated. The results show 100\% recall in situation detection and perfect message schema correctness; however, both models suffer from false-positive detections and have reduced performance in terms of number of lanes, driving lane status and cause code. Surprisingly, Gemini-2.5-Flash, though more capable in general tasks, underperforms Gemini-2.0-Flash in detection accuracy and semantic understanding and incurs higher latency (Table II). These findings motivate further work on fine-tuning specialized LLMs or MLLMs tailored for intelligent transportation applications.

</details>


### [64] [Integration of Visual SLAM into Consumer-Grade Automotive Localization](https://arxiv.org/abs/2511.06919)
*Luis Diener,Jens Kalkkuhl,Markus Enzweiler*

Main category: cs.RO

TL;DR: 本研究提出将视觉SLAM应用于消费级车辆定位系统，以改善定位精度，实验结果显示该方法在陀螺仪校准和整体定位性能上表现出色。


<details>
  <summary>Details</summary>
Motivation: 提升消费级车辆的自我定位准确性，克服现有传感器的系统误差和校准问题。

Method: 将视觉SLAM与横向车辆动力学模型相结合，进行在线陀螺仪校准。

Result: 视觉SLAM的集成显著提高了陀螺仪校准的准确性，从而增强整体定位性能，优于现有的先进方法。

Conclusion: 融合视觉SLAM与车辆动力学模型的方法，为提高汽车定位准确性提供了有前景的解决方案。

Abstract: Accurate ego-motion estimation in consumer-grade vehicles currently relies on proprioceptive sensors, i.e. wheel odometry and IMUs, whose performance is limited by systematic errors and calibration. While visual-inertial SLAM has become a standard in robotics, its integration into automotive ego-motion estimation remains largely unexplored. This paper investigates how visual SLAM can be integrated into consumer-grade vehicle localization systems to improve performance. We propose a framework that fuses visual SLAM with a lateral vehicle dynamics model to achieve online gyroscope calibration under realistic driving conditions. Experimental results demonstrate that vision-based integration significantly improves gyroscope calibration accuracy and thus enhances overall localization performance, highlighting a promising path toward higher automotive localization accuracy. We provide results on both proprietary and public datasets, showing improved performance and superior localization accuracy on a public benchmark compared to state-of-the-art methods.

</details>


### [65] [Raspi$^2$USBL: An open-source Raspberry Pi-Based Passive Inverted Ultra-Short Baseline Positioning System for Underwater Robotics](https://arxiv.org/abs/2511.06998)
*Jin Huang,Yingqiang Wang,Ying Chen*

Main category: cs.RO

TL;DR: Raspi$^2$USBL是一个开源的Raspberry Pi基础的水下定位系统，提供了经济实惠且高精度的水下定位方案。


<details>
  <summary>Details</summary>
Motivation: 精确的水下定位是水下机器人面临的基本挑战，因为全球导航卫星系统（GNSS）信号无法穿透海面。

Method: 系统包括一个被动声纳接收器和一个主动信标，结合模块化硬件架构和开源软件框架，实现高精度时间同步和信号处理。

Result: 该系统在水下定位准确性上表现出色，提供了低成本的解决方案。

Conclusion: Raspi$^2$USBL通过开源硬件和软件，降低了水下机器人实验室的入门障碍，促进了可重复性和协作创新。

Abstract: Precise underwater positioning remains a fundamental challenge for underwater robotics since global navigation satellite system (GNSS) signals cannot penetrate the sea surface. This paper presents Raspi$^2$USBL, an open-source, Raspberry Pi-based passive inverted ultra-short baseline (piUSBL) positioning system designed to provide a low-cost and accessible solution for underwater robotic research. The system comprises a passive acoustic receiver and an active beacon. The receiver adopts a modular hardware architecture that integrates a hydrophone array, a multichannel preamplifier, an oven-controlled crystal oscillator (OCXO), a Raspberry Pi 5, and an MCC-series data acquisition (DAQ) board. Apart from the Pi 5, OCXO, and MCC board, the beacon comprises an impedance-matching network, a power amplifier, and a transmitting transducer. An open-source C++ software framework provides high-precision clock synchronization and triggering for one-way travel-time (OWTT) messaging, while performing real-time signal processing, including matched filtering, array beamforming, and adaptive gain control, to estimate the time of flight (TOF) and direction of arrival (DOA) of received signals. The Raspi$^2$USBL system was experimentally validated in an anechoic tank, freshwater lake, and open-sea trials. Results demonstrate a slant-range accuracy better than 0.1%, a bearing accuracy within 0.1$^\circ$, and stable performance over operational distances up to 1.3 km. These findings confirm that low-cost, reproducible hardware can deliver research-grade underwater positioning accuracy. By releasing both the hardware and software as open-source, Raspi$^2$USBL provides a unified reference platform that lowers the entry barrier for underwater robotics laboratories, fosters reproducibility, and promotes collaborative innovation in underwater acoustic navigation and swarm robotics.

</details>


### [66] [HDCNet: A Hybrid Depth Completion Network for Grasping Transparent and Reflective Objects](https://arxiv.org/abs/2511.07081)
*Guanghu Xie,Mingxu Li,Songwei Wu,Yang Liu,Zongwu Xie,Baoshi Cao,Hong Liu*

Main category: cs.RO

TL;DR: 提出HDCNet，解决透明和反射物体的深度感知问题，显著提高深度完成和抓取成功率。


<details>
  <summary>Details</summary>
Motivation: 传统深度传感器在透明和反射表面上的可靠性不足，限制了机器人在感知和抓取任务中的表现。

Method: 提出了一种新型的深度完成网络HDCNet，融合了Transformer、CNN和Mamba架构，采用双分支Transformer-CNN框架和混合融合模块。

Result: HDCNet在多个公共数据集上表现出SOTA的深度完成性能，并在抓取实验中提高了成功率，最多增加60%。

Conclusion: HDCNet显著提高了透明和反射物体的深度完成精度和机器人抓取成功率。

Abstract: Depth perception of transparent and reflective objects has long been a critical challenge in robotic manipulation.Conventional depth sensors often fail to provide reliable measurements on such surfaces, limiting the performance of robots in perception and grasping tasks. To address this issue, we propose a novel depth completion network,HDCNet,which integrates the complementary strengths of Transformer,CNN and Mamba architectures.Specifically,the encoder is designed as a dual-branch Transformer-CNN framework to extract modality-specific features. At the shallow layers of the encoder, we introduce a lightweight multimodal fusion module to effectively integrate low-level features. At the network bottleneck,a Transformer-Mamba hybrid fusion module is developed to achieve deep integration of high-level semantic and global contextual information, significantly enhancing depth completion accuracy and robustness. Extensive evaluations on multiple public datasets demonstrate that HDCNet achieves state-of-the-art(SOTA) performance in depth completion tasks.Furthermore,robotic grasping experiments show that HDCNet substantially improves grasp success rates for transparent and reflective objects,achieving up to a 60% increase.

</details>


### [67] [Dynamics-Decoupled Trajectory Alignment for Sim-to-Real Transfer in Reinforcement Learning for Autonomous Driving](https://arxiv.org/abs/2511.07155)
*Thomas Steinecker,Alexander Bienemann,Denis Trescher,Thorsten Luettel,Mirko Maehlisch*

Main category: cs.RO

TL;DR: 本文提出一种通过空间和时间对齐策略的框架，成功实现了强化学习在真实车辆上的应用，解决了模拟与现实之间的转移挑战。


<details>
  <summary>Details</summary>
Motivation: 虽然强强化学习在机器人领域有潜力，但由于车辆动力学复杂性和模拟与现实之间的不匹配，实际应用仍然面临挑战。

Method: 通过采用运动规划与控制解耦的框架，训练RL代理生成控制动作，并在真实车辆上进行验证。

Result: 本文提出了一种通过空间和时间对齐策略将运动规划与车辆控制解耦的框架，进而实现了强化学习在真实车辆上的应用。通过在模拟环境中训练RL代理并提取其行为，可以实现虚拟车辆与真实系统的同步，并最终验证了该方法在真实车辆上的有效性。

Conclusion: 该对齐策略促成了基于RL的运动规划从模拟到现实的无缝转移，成功实现了高层轨迹生成与低层车辆控制的解耦。

Abstract: Reinforcement learning (RL) has shown promise in robotics, but deploying RL on real vehicles remains challenging due to the complexity of vehicle dynamics and the mismatch between simulation and reality. Factors such as tire characteristics, road surface conditions, aerodynamic disturbances, and vehicle load make it infeasible to model real-world dynamics accurately, which hinders direct transfer of RL agents trained in simulation. In this paper, we present a framework that decouples motion planning from vehicle control through a spatial and temporal alignment strategy between a virtual vehicle and the real system. An RL agent is first trained in simulation using a kinematic bicycle model to output continuous control actions. Its behavior is then distilled into a trajectory-predicting agent that generates finite-horizon ego-vehicle trajectories, enabling synchronization between virtual and real vehicles. At deployment, a Stanley controller governs lateral dynamics, while longitudinal alignment is maintained through adaptive update mechanisms that compensate for deviations between virtual and real trajectories. We validate our approach on a real vehicle and demonstrate that the proposed alignment strategy enables robust zero-shot transfer of RL-based motion planning from simulation to reality, successfully decoupling high-level trajectory generation from low-level vehicle control.

</details>


### [68] [Automated Generation of Continuous-Space Roadmaps for Routing Mobile Robot Fleets](https://arxiv.org/abs/2511.07175)
*Marvin Rüdt,Constantin Enke,Kai Furmans*

Main category: cs.RO

TL;DR: 本研究提出了一种自动化的路线图生成方法，用于提高移动机器人车队在内部物流中的效率，克服了现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 移动机器人车队的高效路由对内部物流至关重要，现有方法在几何精度和实际约束方面存在不足。

Method: 结合自由空间离散化、需求驱动的K最短路优化和路径平滑技术，生成适合内部物流的路线图。

Result: 在多个案例中，所提方法相较于传统方法，展示了更低的结构复杂性、更高的冗余性和接近最优的路径长度。

Conclusion: 该方法在多种内部物流用例中表现优异，能够实现高效且稳健的移动机器人路由。

Abstract: Efficient routing of mobile robot fleets is crucial in intralogistics, where delays and deadlocks can substantially reduce system throughput. Roadmap design, specifying feasible transport routes, directly affects fleet coordination and computational performance. Existing approaches are either grid-based, compromising geometric precision, or continuous-space approaches that disregard practical constraints. This paper presents an automated roadmap generation approach that bridges this gap by operating in continuous-space, integrating station-to-station transport demand and enforcing minimum distance constraints for nodes and edges. By combining free space discretization, transport demand-driven $K$-shortest-path optimization, and path smoothing, the approach produces roadmaps tailored to intralogistics applications. Evaluation across multiple intralogistics use cases demonstrates that the proposed approach consistently outperforms established baselines (4-connected grid, 8-connected grid, and random sampling), achieving lower structural complexity, higher redundancy, and near-optimal path lengths, enabling efficient and robust routing of mobile robot fleets.

</details>


### [69] [PlanT 2.0: Exposing Biases and Structural Flaws in Closed-Loop Driving](https://arxiv.org/abs/2511.07292)
*Simon Gerstenecker,Andreas Geiger,Katrin Renz*

Main category: cs.RO

TL;DR: 本文提出了PlanT 2.0，用于深入分析自主驾驶模型的失效和偏见，强调数据驱动开发的重要性。


<details>
  <summary>Details</summary>
Motivation: 之前的工作缺乏对模型失效、偏见和快捷学习的深刻理解，导致只关注基准性能和方法创新。

Method: 通过对模型输入进行扰动并观察预测，进行系统性研究和分析。

Result: 引入了PlanT 2.0，并在CARLA Leaderboard 2.0上实现了最先进的性能，同时揭示了一些重要的失效，例如对低障碍物多样性的场景理解不足和对固定专家轨迹的过拟合。

Conclusion: 研究表明，寻找丰富、强大且更少偏见的数据集对自主驾驶的发展至关重要。

Abstract: Most recent work in autonomous driving has prioritized benchmark performance and methodological innovation over in-depth analysis of model failures, biases, and shortcut learning. This has led to incremental improvements without a deep understanding of the current failures. While it is straightforward to look at situations where the model fails, it is hard to understand the underlying reason. This motivates us to conduct a systematic study, where inputs to the model are perturbed and the predictions observed. We introduce PlanT 2.0, a lightweight, object-centric planning transformer designed for autonomous driving research in CARLA. The object-level representation enables controlled analysis, as the input can be easily perturbed (e.g., by changing the location or adding or removing certain objects), in contrast to sensor-based models. To tackle the scenarios newly introduced by the challenging CARLA Leaderboard 2.0, we introduce multiple upgrades to PlanT, achieving state-of-the-art performance on Longest6 v2, Bench2Drive, and the CARLA validation routes. Our analysis exposes insightful failures, such as a lack of scene understanding caused by low obstacle diversity, rigid expert behaviors leading to exploitable shortcuts, and overfitting to a fixed set of expert trajectories. Based on these findings, we argue for a shift toward data-centric development, with a focus on richer, more robust, and less biased datasets. We open-source our code and model at https://github.com/autonomousvision/plant2.

</details>


### [70] [Exact Smooth Reformulations for Trajectory Optimization Under Signal Temporal Logic Specifications](https://arxiv.org/abs/2511.07375)
*Shaohang Han,Joris Verhagen,Jana Tumova*

Main category: cs.RO

TL;DR: 研究信号时序逻辑下的运动规划，提出了一种精确的轨迹优化方法。


<details>
  <summary>Details</summary>
Motivation: 利用STL的鲁棒性语义以满足空间-时间要求。

Method: 通过引入对最大和最小运算符的精确重构，将STL合成视为轨迹优化问题。

Result: 该方法是精确的、平滑的，具有可靠性。

Conclusion: 提出的方法在数值模拟中验证了其实用性。

Abstract: We study motion planning under Signal Temporal Logic (STL), a useful formalism for specifying spatial-temporal requirements. We pose STL synthesis as a trajectory optimization problem leveraging the STL robustness semantics. To obtain a differentiable problem without approximation error, we introduce an exact reformulation of the max and min operators. The resulting method is exact, smooth, and sound. We validate it in numerical simulations, demonstrating its practical performance.

</details>


### [71] [Residual Rotation Correction using Tactile Equivariance](https://arxiv.org/abs/2511.07381)
*Yizhe Zhu,Zhang Ye,Boce Hu,Haibo Zhao,Yu Qi,Dian Wang,Robert Platt*

Main category: cs.RO

TL;DR: EquiTac框架提高了视觉触觉策略学习的样本效率和泛化能力，适用于接触丰富的操作任务。


<details>
  <summary>Details</summary>
Motivation: 受限于触觉数据收集的高成本，提升样本效率成为视觉触觉策略发展的关键要求。

Method: 该方法通过重建表面法线并利用SO(2)对称性进行策略学习，以提升样本效率和强化学习表现。

Result: EquiTac在真实机器人上实现了对未见手中方向的零-shot泛化，表现优异。

Conclusion: EquiTac在处理未知的手中对象方向时表现出色，且仅需少量训练样本，超越了现有基线。

Abstract: Visuotactile policy learning augments vision-only policies with tactile input, facilitating contact-rich manipulation. However, the high cost of tactile data collection makes sample efficiency the key requirement for developing visuotactile policies. We present EquiTac, a framework that exploits the inherent SO(2) symmetry of in-hand object rotation to improve sample efficiency and generalization for visuotactile policy learning. EquiTac first reconstructs surface normals from raw RGB inputs of vision-based tactile sensors, so rotations of the normal vector field correspond to in-hand object rotations. An SO(2)-equivariant network then predicts a residual rotation action that augments a base visuomotor policy at test time, enabling real-time rotation correction without additional reorientation demonstrations. On a real robot, EquiTac accurately achieves robust zero-shot generalization to unseen in-hand orientations with very few training samples, where baselines fail even with more training data. To our knowledge, this is the first tactile learning method to explicitly encode tactile equivariance for policy learning, yielding a lightweight, symmetry-aware module that improves reliability in contact-rich tasks.

</details>


### [72] [Unified Humanoid Fall-Safety Policy from a Few Demonstrations](https://arxiv.org/abs/2511.07407)
*Zhengjie Xu,Ye Li,Kwan-yee Lin,Stella X. Yu*

Main category: cs.RO

TL;DR: 本文提出了一种创新的方法，使人形机器人在面临摔倒时能够安全、自主地预防摔倒、减轻冲击并迅速恢复，实验表明该方法效果显著。


<details>
  <summary>Details</summary>
Motivation: 确保人形机器人在运动过程中的安全，尤其是在面临失去平衡和摔倒的风险时，提出一种综合的策略来应对摔倒和恢复的整个过程。

Method: 融合人类示范与强化学习，利用适应性扩散记忆学习全身行为，形成一种综合的应对摔倒的策略。

Result: 通过结合稀疏的人类示范、强化学习和适应性的基于扩散的安全反应记忆，学习到了一种统一的全身行为，实现了摔倒预防、冲击缓解和快速恢复。

Conclusion: 通过模拟和Unitree G1机器人进行了实验，验证了所提出方法的有效性，结果显示涉及摔倒的风险明显降低，恢复速度较快，未来有助于提升人形机器人的安全性和适应性。

Abstract: Falling is an inherent risk of humanoid mobility. Maintaining stability is thus a primary safety focus in robot control and learning, yet no existing approach fully averts loss of balance. When instability does occur, prior work addresses only isolated aspects of falling: avoiding falls, choreographing a controlled descent, or standing up afterward. Consequently, humanoid robots lack integrated strategies for impact mitigation and prompt recovery when real falls defy these scripts. We aim to go beyond keeping balance to make the entire fall-and-recovery process safe and autonomous: prevent falls when possible, reduce impact when unavoidable, and stand up when fallen. By fusing sparse human demonstrations with reinforcement learning and an adaptive diffusion-based memory of safe reactions, we learn adaptive whole-body behaviors that unify fall prevention, impact mitigation, and rapid recovery in one policy. Experiments in simulation and on a Unitree G1 demonstrate robust sim-to-real transfer, lower impact forces, and consistently fast recovery across diverse disturbances, pointing towards safer, more resilient humanoids in real environments. Videos are available at https://firm2025.github.io/.

</details>


### [73] [Using Vision Language Models as Closed-Loop Symbolic Planners for Robotic Applications: A Control-Theoretic Perspective](https://arxiv.org/abs/2511.07410)
*Hao Wang,Sathwik Karnik,Bea Lim,Somil Bansal*

Main category: cs.RO

TL;DR: 本文探讨了如何从控制理论的角度有效地将视觉语言模型（VLMs）应用于闭环符号规划，特别是在机器人应用中的表现及其改进建议。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）和视觉语言模型（VLMs）在符号规划中的应用广泛，但闭环符号规划的有效使用仍未得到充分探索。

Method: 设计并进行受控实验，以探讨VLM在闭环符号规划中的应用，特别关注控制视野和热启动对性能的影响。

Result: 研究表明，控制视野和热启动对VLM符号规划师的性能具有显著影响，并提供了一些提升其性能的建议。

Conclusion: 通过对控制视野和热启动对VLM符号规划师性能影响的研究，本文提出了一些改进建议，以增强VLM在高层机器人规划中的应用效果。

Abstract: Large Language Models (LLMs) and Vision Language Models (VLMs) have been widely used for embodied symbolic planning. Yet, how to effectively use these models for closed-loop symbolic planning remains largely unexplored. Because they operate as black boxes, LLMs and VLMs can produce unpredictable or costly errors, making their use in high-level robotic planning especially challenging. In this work, we investigate how to use VLMs as closed-loop symbolic planners for robotic applications from a control-theoretic perspective. Concretely, we study how the control horizon and warm-starting impact the performance of VLM symbolic planners. We design and conduct controlled experiments to gain insights that are broadly applicable to utilizing VLMs as closed-loop symbolic planners, and we discuss recommendations that can help improve the performance of VLM symbolic planners.

</details>


### [74] [Robot Learning from a Physical World Model](https://arxiv.org/abs/2511.07416)
*Jiageng Mao,Sicheng He,Hao-Ning Wu,Yang You,Shuyang Sun,Zhicheng Wang,Yanan Bao,Huizhong Chen,Leonidas Guibas,Vitor Guizilini,Howard Zhou,Yue Wang*

Main category: cs.RO

TL;DR: PhysWorld是一个框架，通过物理世界建模实现机器人从视频生成中学习，结合视频生成与物理世界重建，生成任务导向视频并可以将视频中的动作转化为物理可执行的机器人轨迹。


<details>
  <summary>Details</summary>
Motivation: 最近的视频生成模型能够根据语言指令和图像合成逼真的视觉演示，这是机器人训练的强大但尚未充分开发的信号来源，但传统方法未能考虑物理规律。

Method: 通过单张图像和任务指令生成任务导向的视频，同时重建视频中的物理世界，并通过物对象中心的残差强化学习将视频动作转化为物理可执行的操作。

Result: 在多种真实世界任务上的实验表明，PhysWorld相比之前的方法在操控准确性上有了显著提升。

Conclusion: PhysWorld显著提高了机器人操控的准确性，能够实现零样本泛化的机器人操控。

Abstract: We introduce PhysWorld, a framework that enables robot learning from video generation through physical world modeling. Recent video generation models can synthesize photorealistic visual demonstrations from language commands and images, offering a powerful yet underexplored source of training signals for robotics. However, directly retargeting pixel motions from generated videos to robots neglects physics, often resulting in inaccurate manipulations. PhysWorld addresses this limitation by coupling video generation with physical world reconstruction. Given a single image and a task command, our method generates task-conditioned videos and reconstructs the underlying physical world from the videos, and the generated video motions are grounded into physically accurate actions through object-centric residual reinforcement learning with the physical world model. This synergy transforms implicit visual guidance into physically executable robotic trajectories, eliminating the need for real robot data collection and enabling zero-shot generalizable robotic manipulation. Experiments on diverse real-world tasks demonstrate that PhysWorld substantially improves manipulation accuracy compared to previous approaches. Visit \href{https://pointscoder.github.io/PhysWorld_Web/}{the project webpage} for details.

</details>


### [75] [Lightning Grasp: High Performance Procedural Grasp Synthesis with Contact Fields](https://arxiv.org/abs/2511.07418)
*Zhao-Heng Yin,Pieter Abbeel*

Main category: cs.RO

TL;DR: 提出了一种名为Lightning Grasp的新算法，实现了对不规则工具状物体的实时多样化抓取合成，速度比现有方法快几个数量级。


<details>
  <summary>Details</summary>
Motivation: 旨在解决机器人和计算机图形学中灵巧手的实时多样化抓取合成问题。

Method: 提出了一种程序化抓取合成算法，利用Contact Field数据结构解耦几何计算和搜索过程。

Result: 该方法相比于现有技术，具有显著的速度提升，并且能够在无监督的情况下对不规则物体生成抓取。

Conclusion: Lightning Grasp通过引入Contact Field数据结构，显著提高了实时抓取合成的效率，并消除了对复杂能量函数和初始化的依赖。

Abstract: Despite years of research, real-time diverse grasp synthesis for dexterous hands remains an unsolved core challenge in robotics and computer graphics. We present Lightning Grasp, a novel high-performance procedural grasp synthesis algorithm that achieves orders-of-magnitude speedups over state-of-the-art approaches, while enabling unsupervised grasp generation for irregular, tool-like objects. The method avoids many limitations of prior approaches, such as the need for carefully tuned energy functions and sensitive initialization. This breakthrough is driven by a key insight: decoupling complex geometric computation from the search process via a simple, efficient data structure - the Contact Field. This abstraction collapses the problem complexity, enabling a procedural search at unprecedented speeds. We open-source our system to propel further innovation in robotic manipulation.

</details>
