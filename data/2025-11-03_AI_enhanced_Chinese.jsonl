{"id": "2510.26999", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26999", "abs": "https://arxiv.org/abs/2510.26999", "authors": ["Adithya Neelakantan", "Pratik Satpute", "Prerna Shinde", "Tejas Manjunatha Devang"], "title": "AIOT based Smart Education System: A Dual Layer Authentication and Context-Aware Tutoring Framework for Learning Environments", "comment": null, "summary": "The AIoT-Based Smart Education System integrates Artificial Intelligence and\nIoT to address persistent challenges in contemporary classrooms: attendance\nfraud, lack of personalization, student disengagement, and inefficient resource\nuse. The unified platform combines four core modules: (1) a dual-factor\nauthentication system leveraging RFID-based ID scans and WiFi verification for\nsecure, fraud-resistant attendance; (2) an AI-powered assistant that provides\nreal-time, context-aware support and dynamic quiz generation based on\ninstructor-supplied materials; (3) automated test generators to streamline\nadaptive assessment and reduce administrative overhead; and (4) the EcoSmart\nCampus module, which autonomously regulates classroom lighting, air quality,\nand temperature using IoT sensors and actuators. Simulated evaluations\ndemonstrate the system's effectiveness in delivering robust real-time\nmonitoring, fostering inclusive engagement, preventing fraudulent practices,\nand supporting operational scalability. Collectively, the AIoT-Based Smart\nEducation System offers a secure, adaptive, and efficient learning environment,\nproviding a scalable blueprint for future educational innovation and improved\nstudent outcomes through the synergistic application of artificial intelligence\nand IoT technologies.", "AI": {"tldr": "AIoT\u57fa\u7840\u7684\u667a\u80fd\u6559\u80b2\u7cfb\u7edf\u901a\u8fc7\u6574\u5408\u4eba\u5de5\u667a\u80fd\u548c\u7269\u8054\u7f51\uff0c\u63d0\u4f9b\u5b89\u5168\u53ef\u9760\u7684\u5b66\u4e60\u73af\u5883\uff0c\u89e3\u51b3\u8003\u52e4\u6b3a\u8bc8\u548c\u8d44\u6e90\u5229\u7528\u7b49\u95ee\u9898\uff0c\u4fc3\u8fdb\u5b66\u751f\u53c2\u4e0e\u548c\u6559\u80b2\u521b\u65b0\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u5f53\u4ee3\u8bfe\u5802\u4e2d\u7684\u6301\u7eed\u6311\u6218\uff0c\u4f8b\u5982\u8003\u52e4\u6b3a\u8bc8\u3001\u7f3a\u4e4f\u4e2a\u6027\u5316\u3001\u5b66\u751f disengagement \u548c\u8d44\u6e90\u4f7f\u7528\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u5c06\u4eba\u5de5\u667a\u80fd\u4e0e\u7269\u8054\u7f51\u7ed3\u5408\uff0c\u8bbe\u8ba1\u4e86\u56db\u4e2a\u6838\u5fc3\u6a21\u5757\uff0c\u5305\u62ec\u53cc\u56e0\u7d20\u8ba4\u8bc1\u7cfb\u7edf\u3001AI\u52a9\u624b\u3001\u81ea\u52a8\u5316\u6d4b\u8bd5\u751f\u6210\u5668\u548cEcoSmart\u6821\u56ed\u6a21\u5757\u3002", "result": "\u6a21\u62df\u8bc4\u4f30\u8868\u660e\u8be5\u7cfb\u7edf\u5728\u5b9e\u65f6\u76d1\u63a7\u3001\u4fc3\u8fdb\u5305\u5bb9\u6027\u53c2\u4e0e\u3001\u9884\u9632\u6b3a\u8bc8\u884c\u4e3a\u548c\u652f\u6301\u64cd\u4f5c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "AIoT-based\u667a\u80fd\u6559\u80b2\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b89\u5168\u3001\u7075\u6d3b\u548c\u9ad8\u6548\u7684\u5b66\u4e60\u73af\u5883\uff0c\u4e3a\u672a\u6765\u7684\u6559\u80b2\u521b\u65b0\u548c\u6539\u5584\u5b66\u751f\u6210\u679c\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u84dd\u56fe\u3002"}}
{"id": "2510.27058", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.27058", "abs": "https://arxiv.org/abs/2510.27058", "authors": ["Rui Liu", "Yifan Zhuang", "Runsheng Zhang"], "title": "Adaptive Human-Computer Interaction Strategies Through Reinforcement Learning in Complex", "comment": null, "summary": "This study addresses the challenges of dynamics and complexity in intelligent\nhuman-computer interaction and proposes a reinforcement learning-based\noptimization framework to improve long-term returns and overall experience.\nHuman-computer interaction is modeled as a Markov decision process, with state\nspace, action space, reward function, and discount factor defined to capture\nthe dynamics of user input, system feedback, and interaction environment. The\nmethod combines policy function, value function, and advantage function,\nupdates parameters through policy gradient, and continuously adjusts during\ninteraction to balance immediate feedback and long-term benefits. To validate\nthe framework, multimodal dialog and scene-aware datasets are used as the\nexperimental platform, with multiple sensitivity experiments conducted on key\nfactors such as discount factor, exploration rate decay, environmental noise,\nand data imbalance. Evaluation is carried out using cumulative reward, average\nepisode reward, convergence speed, and task success rate. Results show that the\nproposed method outperforms existing approaches across several metrics,\nachieving higher task completion while maintaining strategy stability.\nComparative experiments further confirm its advantages in interaction\nefficiency and long-term return, demonstrating the significant value of\nreinforcement learning in optimizing human-computer interaction.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6765\u4f18\u5316\u4eba\u673a\u4ea4\u4e92\uff0c\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u5efa\u6a21\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u63d0\u5347\u4efb\u52a1\u5b8c\u6210\u7387\u548c\u4ea4\u4e92\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u76ee\u7684\u5728\u4e8e\u89e3\u51b3\u667a\u80fd\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u52a8\u6001\u4e0e\u590d\u6742\u6027\u95ee\u9898\uff0c\u4ee5\u63d0\u9ad8\u957f\u671f\u56de\u62a5\u548c\u6574\u4f53\u4f53\u9a8c\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4eba\u673a\u4ea4\u4e92\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u7ed3\u5408\u7b56\u7565\u51fd\u6570\u3001\u4ef7\u503c\u51fd\u6570\u548c\u4f18\u52bf\u51fd\u6570\uff0c\u5229\u7528\u7b56\u7565\u68af\u5ea6\u4e0d\u65ad\u8c03\u6574\u53c2\u6570\uff0c\u4ee5\u5e73\u8861\u5373\u65f6\u53cd\u9988\u548c\u957f\u671f\u6536\u76ca\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u6846\u67b6\u5728\u4efb\u52a1\u6210\u529f\u7387\u3001\u7b56\u7565\u7a33\u5b9a\u6027\u7b49\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u4ea4\u4e92\u6548\u7387\u548c\u957f\u671f\u56de\u62a5\u65b9\u9762\u663e\u793a\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u8d85\u8fc7\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u4efb\u52a1\u5b8c\u6210\u7387\u548c\u7b56\u7565\u7a33\u5b9a\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5f3a\u5316\u5b66\u4e60\u5728\u4f18\u5316\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u91cd\u8981\u4ef7\u503c\u5f97\u4ee5\u9a8c\u8bc1\u3002"}}
{"id": "2510.27075", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.27075", "abs": "https://arxiv.org/abs/2510.27075", "authors": ["Byoung-Hee Kwon", "Minji Lee", "Seong-Whan Lee"], "title": "Functional connectivity guided deep neural network for decoding high-level visual imagery", "comment": "34 pages, 8 figures, 6 tables", "summary": "This study introduces a pioneering approach in brain-computer interface (BCI)\ntechnology, featuring our novel concept of high-level visual imagery for\nnon-invasive electroencephalography (EEG)-based communication. High-level\nvisual imagery, as proposed in our work, involves the user engaging in the\nmental visualization of complex upper limb movements. This innovative approach\nsignificantly enhances the BCI system, facilitating the extension of its\napplications to more sophisticated tasks such as EEG-based robotic arm control.\nBy leveraging this advanced form of visual imagery, our study opens new\nhorizons for intricate and intuitive mind-controlled interfaces. We developed\nan advanced deep learning architecture that integrates functional connectivity\nmetrics with a convolutional neural network-image transformer. This framework\nis adept at decoding subtle user intentions, addressing the spatial variability\nin high-level visual tasks, and effectively translating these into precise\ncommands for robotic arm control. Our comprehensive offline and pseudo-online\nevaluations demonstrate the framework's efficacy in real-time applications,\nincluding the nuanced control of robotic arms. The robustness of our approach\nis further validated through leave-one-subject-out cross-validation, marking a\nsignificant step towards versatile, subject-independent BCI applications. This\nresearch highlights the transformative impact of advanced visual imagery and\ndeep learning in enhancing the usability and adaptability of BCI systems,\nparticularly in robotic arm manipulation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u8111\u673a\u63a5\u53e3\u6280\u672f\uff0c\u901a\u8fc7\u9ad8\u7ea7\u89c6\u89c9\u60f3\u8c61\u548c\u6df1\u5ea6\u5b66\u4e60\uff0c\u589e\u5f3a\u4e86\u673a\u5668\u81c2\u63a7\u5236\u7684\u7cbe\u786e\u6027\u548c\u5b9e\u65f6\u6027\uff0c\u63a8\u52a8\u4e86BCI\u7cfb\u7edf\u7684\u591a\u6837\u5316\u5e94\u7528\u3002", "motivation": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u975e\u4fb5\u5165\u6027\u8111\u7535\u56fe\uff08EEG\uff09\u901a\u4fe1\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u6c34\u5e73\u7684\u89c6\u89c9\u60f3\u8c61\u7ecf\u9a8c\uff0c\u63d0\u5347BCI\u6280\u672f\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u673a\u5668\u4eba\u624b\u81c2\u63a7\u5236\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u9ad8\u7ea7\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u5c06\u529f\u80fd\u8fde\u63a5\u6027\u6307\u6807\u4e0e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc-\u56fe\u50cf\u53d8\u6362\u5668\u7ed3\u5408\uff0c\u80fd\u591f\u89e3\u7801\u7528\u6237\u7684\u7ec6\u5fae\u610f\u56fe\u5e76\u6709\u6548\u7ffb\u8bd1\u6210\u7cbe\u51c6\u7684\u673a\u5668\u4eba\u624b\u81c2\u63a7\u5236\u6307\u4ee4\u3002", "result": "\u6211\u4eec\u7684\u79bb\u7ebf\u548c\u4f2a\u5728\u7ebf\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u662f\u5728\u673a\u5668\u81c2\u7684\u7ec6\u81f4\u63a7\u5236\u65b9\u9762\uff0c\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u9ad8\u7ea7\u89c6\u89c9\u60f3\u8c61\u548c\u6df1\u5ea6\u5b66\u4e60\u5728\u63d0\u5347BCI\u7cfb\u7edf\u53ef\u7528\u6027\u548c\u9002\u5e94\u6027\u65b9\u9762\u7684\u53d8\u9769\u6027\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u673a\u5668\u4eba\u624b\u81c2\u64cd\u63a7\u4e2d\u3002"}}
{"id": "2510.27126", "categories": ["cs.HC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.27126", "abs": "https://arxiv.org/abs/2510.27126", "authors": ["Jinwen Tang", "Yi Shang"], "title": "AURA: A Reinforcement Learning Framework for AI-Driven Adaptive Conversational Surveys", "comment": null, "summary": "Conventional online surveys provide limited personalization, often resulting\nin low engagement and superficial responses. Although AI survey chatbots\nimprove convenience, most are still reactive: they rely on fixed dialogue trees\nor static prompt templates and therefore cannot adapt within a session to fit\nindividual users, which leads to generic follow-ups and weak response quality.\nWe address these limitations with AURA (Adaptive Understanding through\nReinforcement Learning for Assessment), a reinforcement learning framework for\nAI-driven adaptive conversational surveys. AURA quantifies response quality\nusing a four-dimensional LSDE metric (Length, Self-disclosure, Emotion, and\nSpecificity) and selects follow-up question types via an epsilon-greedy policy\nthat updates the expected quality gain within each session. Initialized with\npriors extracted from 96 prior campus-climate conversations (467 total\nchatbot-user exchanges), the system balances exploration and exploitation\nacross 10-15 dialogue exchanges, dynamically adapting to individual\nparticipants in real time. In controlled evaluations, AURA achieved a +0.12\nmean gain in response quality and a statistically significant improvement over\nnon-adaptive baselines (p=0.044, d=0.66), driven by a 63% reduction in\nspecification prompts and a 10x increase in validation behavior. These results\ndemonstrate that reinforcement learning can give survey chatbots improved\nadaptivity, transforming static questionnaires into interactive, self-improving\nassessment systems.", "AI": {"tldr": "AURA\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6539\u5584\u4e86\u8c03\u67e5\u804a\u5929\u673a\u5668\u4eba\u7684\u4e2a\u6027\u5316\u548c\u9002\u5e94\u6027\uff0c\u63d0\u5347\u4e86\u53c2\u4e0e\u8005\u7684\u56de\u7b54\u8d28\u91cf\u548c\u4e92\u52a8\u4f53\u9a8c\u3002", "motivation": "\u4f20\u7edf\u5728\u7ebf\u8c03\u67e5\u7f3a\u4e4f\u4e2a\u6027\u5316\uff0c\u5bfc\u81f4\u53c2\u4e0e\u8005\u7684\u4f4e\u53c2\u4e0e\u5ea6\u548c\u80a4\u6d45\u7684\u56de\u7b54\uff0c\u73b0\u6709AI\u804a\u5929\u673a\u5668\u4eba\u4ecd\u7136\u4f9d\u8d56\u56fa\u5b9a\u5bf9\u8bdd\u6811\uff0c\u65e0\u6cd5\u9002\u5e94\u4e2a\u4f53\u7528\u6237\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6AURA\uff0c\u5229\u7528LSDE\u56db\u7ef4\u6307\u6807\u91cf\u5316\u56de\u7b54\u8d28\u91cf\uff0c\u5e76\u901a\u8fc7\u03b5-\u8d2a\u5a6a\u7b56\u7565\u9009\u62e9\u540e\u7eed\u95ee\u9898\u7c7b\u578b\u3002", "result": "\u5728\u63a7\u5236\u8bc4\u4f30\u4e2d\uff0cAURA\u5b9e\u73b0\u4e86\u54cd\u5e94\u8d28\u91cf\u5e73\u5747\u63d0\u53470.12\uff0c\u5e76\u5728\u7edf\u8ba1\u4e0a\u663e\u8457\u4f18\u4e8e\u975e\u9002\u5e94\u6027\u57fa\u7ebf\uff08p=0.044, d=0.66\uff09\uff0c\u540c\u65f6\u51cf\u5c11\u89c4\u683c\u63d0\u793a63%\u5e76\u63d0\u9ad8\u9a8c\u8bc1\u884c\u4e3a10\u500d\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u663e\u8457\u63d0\u5347\u8c03\u67e5\u804a\u5929\u673a\u5668\u4eba\u7684\u9002\u5e94\u6027\uff0c\u5c06\u9759\u6001\u95ee\u5377\u8f6c\u53d8\u4e3a\u4e92\u52a8\u7684\u81ea\u6211\u6539\u8fdb\u8bc4\u4f30\u7cfb\u7edf\u3002"}}
{"id": "2510.26837", "categories": ["cs.RO", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2510.26837", "abs": "https://arxiv.org/abs/2510.26837", "authors": ["Conor K. Trygstad", "Nestor O. Perez-Arancibia"], "title": "Force Characterization of Insect-Scale Aquatic Propulsion Based on Fluid-Structure Interaction", "comment": "To be presented at ICAR 2025 in San Juan, Argentina", "summary": "We present force characterizations of two newly developed insect-scale\npropulsors--one single-tailed and one double-tailed--for microrobotic swimmers\nthat leverage fluid-structure interaction (FSI) to generate thrust. The designs\nof these two devices were inspired by anguilliform swimming and are driven by\nsoft tails excited by high-work-density (HWD) actuators powered by shape-memory\nalloy (SMA) wires. While these propulsors have been demonstrated to be suitable\nfor microrobotic aquatic locomotion and controllable with simple architectures\nfor trajectory tracking in the two-dimensional (2D) space, the characteristics\nand magnitudes of the associated forces have not been studied systematically.\nIn the research presented here, we adopted a theoretical framework based on the\nnotion of reactive forces and obtained experimental data for characterization\nusing a custom-built micro-N-resolution force sensor. We measured maximum and\ncycle-averaged force values with multi-test means of respectively 0.45 mN and\n2.97 micro-N, for the tested single-tail propulsor. For the dual-tail\npropulsor, we measured maximum and cycle-averaged force values with multi-test\nmeans of 0.61 mN and 22.6 micro-N, respectively. These results represent the\nfirst measurements of the instantaneous thrust generated by insect-scale\npropulsors of this type and provide insights into FSI for efficient\nmicrorobotic propulsion.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e24\u79cd\u65b0\u5f00\u53d1\u7684\u6606\u866b\u5c3a\u5ea6\u63a8\u8fdb\u5668\u7684\u529b\u7279\u6027\uff0c\u9996\u6b21\u6d4b\u91cf\u5176\u77ac\u65f6\u63a8\u529b\uff0c\u4e3a\u5fae\u673a\u5668\u4eba\u63a8\u52a8\u7684\u6d41\u4f53-\u7ed3\u6784\u76f8\u4e92\u4f5c\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u6027\u5730\u7814\u7a76\u65b0\u5f00\u53d1\u7684\u4e24\u79cd\u6606\u866b\u5c3a\u5ea6\u63a8\u8fdb\u5668\u7684\u529b\u7279\u6027\uff0c\u5c3d\u7ba1\u5b83\u4eec\u5728\u5fae\u673a\u5668\u4eba\u6c34\u4e0b\u8fd0\u52a8\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u76f8\u5173\u7684\u529b\u7279\u5f81\u5c1a\u672a\u88ab\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u53cd\u5e94\u529b\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u5229\u7528\u5b9a\u5236\u7684\u5fae\u725b\u987f\u7ea7\u529b\u4f20\u611f\u5668\u6536\u96c6\u5b9e\u9a8c\u6570\u636e\u3002", "result": "\u6d4b\u8bd5\u7684\u5355\u5c3e\u63a8\u8fdb\u5668\u6700\u5927\u548c\u5e73\u5747\u529b\u5206\u522b\u4e3a0.45 mN\u548c2.97\u5fae\u725b\u987f\uff0c\u800c\u53cc\u5c3e\u63a8\u8fdb\u5668\u5219\u5206\u522b\u4e3a0.61 mN\u548c22.6\u5fae\u725b\u987f\u3002", "conclusion": "\u6b64\u7814\u7a76\u9996\u6b21\u6d4b\u91cf\u4e86\u6606\u866b\u5c3a\u5ea6\u63a8\u8fdb\u5668\u6240\u4ea7\u751f\u7684\u77ac\u65f6\u63a8\u529b\uff0c\u4e3a\u9ad8\u6548\u7684\u5fae\u673a\u5668\u4eba\u63a8\u8fdb\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2510.27247", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.27247", "abs": "https://arxiv.org/abs/2510.27247", "authors": ["Deok-Seon Kim", "Seo-Hyun Lee", "Kang Yin", "Seong-Whan Lee"], "title": "Reconstructing Unseen Sentences from Speech-related Biosignals for Open-vocabulary Neural Communication", "comment": "Accepted for publication in IEEE Transactions on Neural Systems and\n  Rehabilitation Engineering", "summary": "Brain-to-speech (BTS) systems represent a groundbreaking approach to human\ncommunication by enabling the direct transformation of neural activity into\nlinguistic expressions. While recent non-invasive BTS studies have largely\nfocused on decoding predefined words or sentences, achieving open-vocabulary\nneural communication comparable to natural human interaction requires decoding\nunconstrained speech. Additionally, effectively integrating diverse signals\nderived from speech is crucial for developing personalized and adaptive neural\ncommunication and rehabilitation solutions for patients. This study\ninvestigates the potential of speech synthesis for previously unseen sentences\nacross various speech modes by leveraging phoneme-level information extracted\nfrom high-density electroencephalography (EEG) signals, both independently and\nin conjunction with electromyography (EMG) signals. Furthermore, we examine the\nproperties affecting phoneme decoding accuracy during sentence reconstruction\nand offer neurophysiological insights to further enhance EEG decoding for more\neffective neural communication solutions. Our findings underscore the\nfeasibility of biosignal-based sentence-level speech synthesis for\nreconstructing unseen sentences, highlighting a significant step toward\ndeveloping open-vocabulary neural communication systems adapted to diverse\npatient needs and conditions. Additionally, this study provides meaningful\ninsights into the development of communication and rehabilitation solutions\nutilizing EEG-based decoding technologies.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u9ad8\u5bc6\u5ea6EEG\u4fe1\u53f7\u63d0\u53d6\u97f3\u7d20\u4fe1\u606f\uff0c\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c47\u7684\u795e\u7ecf\u901a\u4fe1\uff0c\u901a\u8fc7\u751f\u7269\u4fe1\u53f7\u5408\u6210\u672a\u89c1\u53e5\u5b50\uff0c\u4e3a\u60a3\u8005\u63d0\u4f9b\u4e2a\u6027\u5316\u7684\u4ea4\u6d41\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c47\u795e\u7ecf\u901a\u4fe1\uff0c\u4f7f\u5176\u80fd\u591f\u4e0e\u81ea\u7136\u4eba\u7c7b\u4e92\u52a8\u76f8\u5ab2\u7f8e\uff0c\u540c\u65f6\u6709\u6548\u6574\u5408\u6765\u81ea\u8bed\u97f3\u7684\u591a\u79cd\u4fe1\u53f7\uff0c\u4ee5\u6ee1\u8db3\u60a3\u8005\u4e2a\u6027\u5316\u548c\u9002\u5e94\u6027\u795e\u7ecf\u901a\u4fe1\u53ca\u5eb7\u590d\u65b9\u6848\u7684\u9700\u6c42\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5229\u7528\u4ece\u9ad8\u5bc6\u5ea6\u8111\u7535\u56fe(EEG)\u4fe1\u53f7\u4e2d\u63d0\u53d6\u7684\u97f3\u7d20\u7ea7\u4fe1\u606f\uff0c\u4ee5\u53ca\u4e0e\u808c\u7535\u56fe(EMG)\u4fe1\u53f7\u7684\u7ed3\u5408\uff0c\u63a2\u8ba8\u4e86\u5728\u5404\u79cd\u8bed\u97f3\u6a21\u5f0f\u4e0b\u4e3a\u524d\u6240\u672a\u89c1\u7684\u53e5\u5b50\u8fdb\u884c\u8bed\u97f3\u5408\u6210\u7684\u6f5c\u529b\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u57fa\u4e8e\u751f\u7269\u4fe1\u53f7\u7684\u53e5\u5b50\u7ea7\u8bed\u97f3\u5408\u6210\u5728\u91cd\u6784\u672a\u89c1\u53e5\u5b50\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5f00\u53d1\u9002\u5e94\u60a3\u8005\u7684\u795e\u7ecf\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u652f\u6301\uff0c\u540c\u65f6\u5bf9EEG\u89e3\u7801\u6280\u672f\u7684\u53d1\u5c55\u4e5f\u63d0\u4f9b\u4e86\u6709\u610f\u4e49\u7684\u89c1\u89e3\u3002", "conclusion": "\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u57fa\u4e8e\u751f\u7269\u4fe1\u53f7\u7684\u53e5\u5b50\u7ea7\u8bed\u97f3\u5408\u6210\u5728\u91cd\u6784\u672a\u89c1\u53e5\u5b50\u65b9\u9762\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5f00\u53d1\u9002\u5e94\u4e0d\u540c\u60a3\u8005\u9700\u6c42\u7684\u5f00\u653e\u8bcd\u6c47\u795e\u7ecf\u901a\u4fe1\u7cfb\u7edf\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2510.26855", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26855", "abs": "https://arxiv.org/abs/2510.26855", "authors": ["Reihaneh Mirjalili"], "title": "Leveraging Foundation Models for Enhancing Robot Perception and Action", "comment": "Doctoral thesis", "summary": "This thesis investigates how foundation models can be systematically\nleveraged to enhance robotic capabilities, enabling more effective\nlocalization, interaction, and manipulation in unstructured environments. The\nwork is structured around four core lines of inquiry, each addressing a\nfundamental challenge in robotics while collectively contributing to a cohesive\nframework for semantics-aware robotic intelligence.", "AI": {"tldr": "\u672c\u8bba\u6587\u63a2\u8ba8\u57fa\u7840\u6a21\u578b\u5982\u4f55\u63d0\u5347\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u3001\u4ea4\u4e92\u4e0e\u64cd\u63a7\u80fd\u529b\uff0c\u5f62\u6210\u8bed\u4e49\u611f\u77e5\u7684\u673a\u5668\u4eba\u667a\u80fd\u6846\u67b6\u3002", "motivation": "\u7814\u7a76\u7684\u52a8\u673a\u662f\u63d0\u5347\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u3001\u4ea4\u4e92\u53ca\u64cd\u63a7\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u56db\u4e2a\u6838\u5fc3\u7814\u7a76\u65b9\u5411\uff0c\u9010\u6b65\u63a2\u8ba8\u673a\u5668\u4eba\u6280\u672f\u9762\u4e34\u7684\u6311\u6218\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u7840\u6a21\u578b\u7684\u5e94\u7528\u80fd\u6709\u6548\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u5730\u5229\u7528\u57fa\u7840\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u4ee5\u589e\u5f3a\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u7684\u673a\u5668\u4eba\u667a\u80fd\u6846\u67b6\u6765\u89e3\u51b3\u57fa\u672c\u6311\u6218\u3002"}}
{"id": "2510.27272", "categories": ["cs.HC", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.27272", "abs": "https://arxiv.org/abs/2510.27272", "authors": ["Vincent K. M. Cheung", "Pei-Cheng Shih", "Masato Hirano", "Masataka Goto", "Shinichi Furuya"], "title": "Inferring trust in recommendation systems from brain, behavioural, and physiological data", "comment": null, "summary": "As people nowadays increasingly rely on artificial intelligence (AI) to\ncurate information and make decisions, assigning the appropriate amount of\ntrust in automated intelligent systems has become ever more important. However,\ncurrent measurements of trust in automation still largely rely on self-reports\nthat are subjective and disruptive to the user. Here, we take music\nrecommendation as a model to investigate the neural and cognitive processes\nunderlying trust in automation. We observed that system accuracy was directly\nrelated to users' trust and modulated the influence of recommendation cues on\nmusic preference. Modelling users' reward encoding process with a reinforcement\nlearning model further revealed that system accuracy, expected reward, and\nprediction error were related to oscillatory neural activity recorded via EEG\nand changes in pupil diameter. Our results provide a neurally grounded account\nof calibrating trust in automation and highlight the promises of a multimodal\napproach towards developing trustable AI systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u7528\u6237\u5bf9\u81ea\u52a8\u5316\u7cfb\u7edf\u4fe1\u4efb\u7684\u795e\u7ecf\u548c\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7684\u4fe1\u4efb\u6821\u51c6\u6846\u67b6\uff0c\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u65b9\u6cd5\u5bf9\u5f00\u53d1\u53ef\u4fe1AI\u7cfb\u7edf\u7684\u6f5c\u529b\u3002", "motivation": "\u968f\u7740\u4eba\u4eec\u8d8a\u6765\u8d8a\u4f9d\u8d56\u4eba\u5de5\u667a\u80fd\u8fdb\u884c\u4fe1\u606f\u7b5b\u9009\u548c\u51b3\u7b56\uff0c\u8bc4\u4f30\u5bf9\u81ea\u52a8\u5316\u7cfb\u7edf\u7684\u4fe1\u4efb\u53d8\u5f97\u6108\u53d1\u91cd\u8981\uff0c\u800c\u73b0\u6709\u7684\u4fe1\u4efb\u6d4b\u91cf\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e3b\u89c2\u81ea\u6211\u62a5\u544a\uff0c\u8fd9\u5f71\u54cd\u4e86\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u4f7f\u7528\u97f3\u4e50\u63a8\u8350\u4f5c\u4e3a\u6a21\u578b\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u5206\u6790\u7528\u6237\u7684\u5956\u52b1\u7f16\u7801\u8fc7\u7a0b\uff0c\u5229\u7528EEG\u8bb0\u5f55\u795e\u7ecf\u6d3b\u52a8\u548c\u77b3\u5b54\u76f4\u5f84\u7684\u53d8\u5316\u3002", "result": "\u53d1\u73b0\u7cfb\u7edf\u51c6\u786e\u6027\u4e0e\u7528\u6237\u4fe1\u4efb\u76f4\u63a5\u76f8\u5173\uff0c\u5e76\u8c03\u8282\u4e86\u63a8\u8350\u7ebf\u7d22\u5bf9\u97f3\u4e50\u504f\u597d\u7684\u5f71\u54cd\uff1b\u540c\u65f6\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u3001\u671f\u671b\u5956\u52b1\u548c\u9884\u6d4b\u8bef\u5dee\u4e0e\u901a\u8fc7EEG\u8bb0\u5f55\u7684\u795e\u7ecf\u632f\u8361\u6d3b\u52a8\u548c\u77b3\u5b54\u76f4\u5f84\u53d8\u5316\u76f8\u5173\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u5efa\u7acb\u53ef\u4fe1\u7684\u81ea\u52a8\u5316\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u4e8e\u795e\u7ecf\u7684\u4fe1\u4efb\u6821\u51c6\u6846\u67b6\uff0c\u5e76\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u65b9\u6cd5\u5728\u5f00\u53d1\u53ef\u4fe1AI\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.26900", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.26900", "abs": "https://arxiv.org/abs/2510.26900", "authors": ["Jahir Argote-Gerald", "Genki Miyauchi", "Julian Rau", "Paul Trodden", "Roderich Gross"], "title": "Design for One, Deploy for Many: Navigating Tree Mazes with Multiple Agents", "comment": "7 pages, 7 figures, to be published in MRS 2025", "summary": "Maze-like environments, such as cave and pipe networks, pose unique\nchallenges for multiple robots to coordinate, including communication\nconstraints and congestion. To address these challenges, we propose a\ndistributed multi-agent maze traversal algorithm for environments that can be\nrepresented by acyclic graphs. It uses a leader-switching mechanism where one\nagent, assuming a head role, employs any single-agent maze solver while the\nother agents each choose an agent to follow. The head role gets transferred to\nneighboring agents where necessary, ensuring it follows the same path as a\nsingle agent would. The multi-agent maze traversal algorithm is evaluated in\nsimulations with groups of up to 300 agents, various maze sizes, and multiple\nsingle-agent maze solvers. It is compared against strategies that are na\\\"ive,\nor assume either global communication or full knowledge of the environment. The\nalgorithm outperforms the na\\\"ive strategy in terms of makespan and\nsum-of-fuel. It is superior to the global-communication strategy in terms of\nmakespan but is inferior to it in terms of sum-of-fuel. The findings suggest it\nis asymptotically equivalent to the full-knowledge strategy with respect to\neither metric. Moreover, real-world experiments with up to 20 Pi-puck robots\nconfirm the feasibility of the approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5e03\u5f0f\u591a\u667a\u80fd\u4f53\u8ff7\u5bab\u904d\u5386\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u590d\u6742\u73af\u5883\u4e0b\u7684\u591a\u4e2a\u673a\u5668\u4eba\u534f\u8c03\u95ee\u9898\uff0c\u5728\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "motivation": "\u9488\u5bf9\u591a\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u534f\u8c03\u95ee\u9898\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u662f\u901a\u4fe1\u9650\u5236\u548c\u62e5\u5835\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9886\u5bfc\u8005\u5207\u6362\u673a\u5236\u7684\u5206\u5e03\u5f0f\u591a\u667a\u80fd\u4f53\u8ff7\u5bab\u904d\u5386\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u65e0\u73af\u56fe\u8868\u793a\u7684\u73af\u5883\u3002", "result": "\u7b97\u6cd5\u5728\u591a\u8fbe300\u4e2a\u667a\u80fd\u4f53\u7684\u4eff\u771f\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e0e\u539f\u59cb\u7b56\u7565\u548c\u5168\u5c40\u901a\u4fe1\u7b56\u7565\u76f8\u6bd4\uff0c\u5728\u67d0\u4e9b\u65b9\u9762\u8d85\u8d8a\u4e86\u7b80\u5355\u7b56\u7565\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u5e76\u5728\u67d0\u4e9b\u6307\u6807\u4e0a\u4f18\u4e8e\u7b80\u5355\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7684\u591a\u673a\u5668\u4eba\u8ff7\u5bab\u5bfc\u822a\u95ee\u9898\u3002"}}
{"id": "2510.27401", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.27401", "abs": "https://arxiv.org/abs/2510.27401", "authors": ["Imaan Hameed", "Huma Umar", "Fozia Umber", "Maryam Mustafa"], "title": "\"Koyi Sawaal Nahi Hai\": Reimagining Maternal Health Chatbots for Collective, Culturally Grounded Care", "comment": null, "summary": "In recent years, LLM-based maternal health chatbots have been widely deployed\nin low-resource settings, but they often ignore real-world contexts where women\nmay not own phones, have limited literacy, and share decision-making within\nfamilies. Through the deployment of a WhatsApp-based maternal health chatbot\nwith 48 pregnant women in Lahore, Pakistan, we examine barriers to use in\npopulations where phones are shared, decision-making is collective, and\nliteracy varies. We complement this with focus group discussions with obstetric\nclinicians. Our findings reveal how adoption is shaped by proxy consent and\nfamily mediation, intermittent phone access, silence around asking questions,\ninfrastructural breakdowns, and contested authority. We frame barriers to\nnon-use as culturally conditioned rather than individual choices, and introduce\nthe Relational Chatbot Design Grammar (RCDG): four commitments that enable\nmediated decision-making, recognize silence as engagement, support episodic\nuse, and treat fragility as baseline to reorient maternal health chatbots\ntoward culturally grounded, collective care.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\u91c7\u7528\u598a\u5a20\u5065\u5eb7\u804a\u5929\u673a\u5668\u4eba\u7684\u969c\u788d\uff0c\u63d0\u51fa\u4e86\u5173\u7cfb\u804a\u5929\u673a\u5668\u4eba\u8bbe\u8ba1\u8bed\u6cd5\uff08RCDG\uff09\uff0c\u4ee5\u652f\u6301\u6587\u5316\u80cc\u666f\u4e0b\u7684\u96c6\u4f53\u62a4\u7406\u548c\u51b3\u7b56\u3002", "motivation": "\u8fd9\u9879\u7814\u7a76\u65e8\u5728\u89e3\u51b3 LLM \u57fa\u7840\u598a\u5a20\u5065\u5eb7\u804a\u5929\u673a\u5668\u4eba\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\u5e94\u7528\u65f6\u5ffd\u89c6\u73b0\u5b9e\u73af\u5883\u5e26\u6765\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u7535\u8bdd\u5171\u4eab\u3001\u6709\u9650\u8bc6\u5b57\u7387\u548c\u5bb6\u5ead\u5171\u540c\u51b3\u7b56\u7684\u60c5\u51b5\u3002", "method": "\u901a\u8fc7\u5728\u5df4\u57fa\u65af\u5766\u62c9\u5408\u5c14\u5bf948\u540d\u5b55\u5987\u8fdb\u884cWhatsApp\u57fa\u7840\u7684\u598a\u5a20\u5065\u5eb7\u804a\u5929\u673a\u5668\u4eba\u90e8\u7f72\uff0c\u7ed3\u5408\u4e0e\u4ea7\u79d1\u4e34\u5e8a\u533b\u751f\u7684\u7126\u70b9\u5c0f\u7ec4\u8ba8\u8bba\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u91c7\u7528\u60c5\u51b5\u53d7\u5230\u4ee3\u7406\u540c\u610f\u548c\u5bb6\u5ead\u8c03\u89e3\u3001\u4e0d\u7a33\u5b9a\u7684\u624b\u673a\u8bbf\u95ee\u3001\u5bf9\u63d0\u95ee\u7684\u6c89\u9ed8\u3001\u57fa\u7840\u8bbe\u65bd\u7f3a\u9677\u53ca\u4e89\u8bae\u6743\u5a01\u7684\u5f71\u54cd\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u5173\u7cfb\u804a\u5929\u673a\u5668\u4eba\u8bbe\u8ba1\u8bed\u6cd5\uff08RCDG\uff09\uff0c\u4ee5\u6307\u5bfc\u5728\u5177\u6709\u6587\u5316\u80cc\u666f\u7684\u60c5\u51b5\u4e0b\u8bbe\u8ba1\u598a\u5a20\u5065\u5eb7\u804a\u5929\u673a\u5668\u4eba\uff0c\u5f3a\u8c03\u96c6\u4f53\u51b3\u7b56\u548c\u5bf9\u8106\u5f31\u6027\u7684\u8ba4\u8bc6\u3002"}}
{"id": "2510.26909", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.26909", "abs": "https://arxiv.org/abs/2510.26909", "authors": ["Tim Windecker", "Manthan Patel", "Moritz Reuss", "Richard Schwarzkopf", "Cesar Cadena", "Rudolf Lioutikov", "Marco Hutter", "Jonas Frey"], "title": "NaviTrace: Evaluating Embodied Navigation of Vision-Language Models", "comment": "9 pages, 6 figures, under review at IEEE conference", "summary": "Vision-language models demonstrate unprecedented performance and\ngeneralization across a wide range of tasks and scenarios. Integrating these\nfoundation models into robotic navigation systems opens pathways toward\nbuilding general-purpose robots. Yet, evaluating these models' navigation\ncapabilities remains constrained by costly real-world trials, overly simplified\nsimulations, and limited benchmarks. We introduce NaviTrace, a high-quality\nVisual Question Answering benchmark where a model receives an instruction and\nembodiment type (human, legged robot, wheeled robot, bicycle) and must output a\n2D navigation trace in image space. Across 1000 scenarios and more than 3000\nexpert traces, we systematically evaluate eight state-of-the-art VLMs using a\nnewly introduced semantic-aware trace score. This metric combines Dynamic Time\nWarping distance, goal endpoint error, and embodiment-conditioned penalties\nderived from per-pixel semantics and correlates with human preferences. Our\nevaluation reveals consistent gap to human performance caused by poor spatial\ngrounding and goal localization. NaviTrace establishes a scalable and\nreproducible benchmark for real-world robotic navigation. The benchmark and\nleaderboard can be found at\nhttps://leggedrobotics.github.io/navitrace_webpage/.", "AI": {"tldr": "NaviTrace\u662f\u4e00\u4e2a\u65b0\u7684\u89c6\u89c9\u95ee\u7b54\u5bfc\u822a\u57fa\u51c6\uff0c\u7cfb\u7edf\u6027\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4e0e\u4eba\u7c7b\u8868\u73b0\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u5230\u673a\u5668\u4eba\u5bfc\u822a\u7cfb\u7edf\u4e2d\uff0c\u4ee5\u63a8\u52a8\u6784\u5efa\u901a\u7528\u673a\u5668\u4eba\uff0c\u4f46\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u7684\u5bfc\u822a\u80fd\u529b\u53d7\u5230\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u9650\u5236\u3002", "method": "\u901a\u8fc7\u5f15\u5165NaviTrace\u57fa\u51c6\uff0c\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u4e86\u516b\u4e2a\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u57281000\u4e2a\u573a\u666f\u548c3000\u591a\u4e2a\u4e13\u5bb6\u8f68\u8ff9\u4e0b\u7684\u5bfc\u822a\u80fd\u529b\uff0c\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u7684\u8bed\u4e49\u611f\u77e5\u8f68\u8ff9\u8bc4\u5206\u65b9\u6cd5\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u76ee\u524d\u7684\u6a21\u578b\u5728\u7a7a\u95f4\u5b9a\u4f4d\u548c\u76ee\u6807\u5b9a\u4f4d\u65b9\u9762\u5b58\u5728\u7f3a\u9677\uff0c\u8868\u73b0\u4e0e\u4eba\u7c7b\u7684\u671f\u671b\u5b58\u5728\u4e00\u81f4\u5dee\u8ddd\u3002", "conclusion": "NaviTrace\u5efa\u7acb\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u53ef\u91cd\u590d\u7684\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u5bfc\u822a\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u5b9a\u4f4d\u548c\u76ee\u6807\u5b9a\u4f4d\u65b9\u9762\u4e0e\u4eba\u7c7b\u8868\u73b0\u4e4b\u95f4\u7684\u4e00\u81f4\u5dee\u8ddd\u3002"}}
{"id": "2510.27521", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.27521", "abs": "https://arxiv.org/abs/2510.27521", "authors": ["Nick Judd", "Alexandre Vaz", "Kevin Paeth", "Layla In\u00e9s Davis", "Milena Esherick", "Jason Brand", "In\u00eas Amaro", "Tony Rousmaniere"], "title": "Independent Clinical Evaluation of General-Purpose LLM Responses to Signals of Suicide Risk", "comment": null, "summary": "We introduce findings and methods to facilitate evidence-based discussion\nabout how large language models (LLMs) should behave in response to user\nsignals of risk of suicidal thoughts and behaviors (STB). People are already\nusing LLMs as mental health resources, and several recent incidents implicate\nLLMs in mental health crises. Despite growing attention, few studies have been\nable to effectively generalize clinical guidelines to LLM use cases, and fewer\nstill have proposed methodologies that can be iteratively applied as knowledge\nimproves about the elements of human-AI interaction most in need of study. We\nintroduce an assessment of LLM alignment with guidelines for ethical\ncommunication, adapted from clinical principles and applied to expressions of\nrisk factors for STB in multi-turn conversations. Using a codebook created and\nvalidated by clinicians, mobilizing the volunteer participation of practicing\ntherapists and trainees (N=43) based in the U.S., and using generalized linear\nmixed-effects models for statistical analysis, we assess a single fully\nopen-source LLM, OLMo-2-32b. We show how to assess when a model deviates from\nclinically informed guidelines in a way that may pose a hazard and (thanks to\nits open nature) facilitates future investigation as to why. We find that\ncontrary to clinical best practice, OLMo-2-32b, and, possibly by extension,\nother LLMs, will become less likely to invite continued dialog as users send\nmore signals of STB risk in multi-turn settings. We also show that OLMo-2-32b\nresponds differently depending on the risk factor expressed. This empirical\nevidence highlights that just as chatbots pose hazards if their responses\nreinforce delusions or assist in suicidal acts, they may also discourage\nfurther help-seeking or cause feelings of dismissal or abandonment by\nwithdrawing from conversations when STB risk is expressed.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9762\u4e34\u81ea\u6740\u98ce\u9669\u4fe1\u53f7\u65f6\u7684\u53cd\u5e94\uff0c\u53d1\u73b0\u5176\u884c\u4e3a\u504f\u79bb\u4e86\u4e34\u5e8a\u6700\u4f73\u5b9e\u8df5\uff0c\u53ef\u80fd\u4f1a\u6291\u5236\u7528\u6237\u7684\u6c42\u52a9\u610f\u613f\u3002", "motivation": "\u8d8a\u6765\u8d8a\u591a\u7684\u4eba\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u5fc3\u7406\u5065\u5eb7\u8d44\u6e90\uff0c\u7136\u800c\u76ee\u524d\u7684\u7814\u7a76\u5728\u5c06\u4e34\u5e8a\u6307\u5357\u6709\u6548\u63a8\u5e7f\u5230LLM\u4f7f\u7528\u6848\u4f8b\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u63d0\u51fa\u9002\u5e94\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u8fdb\u884c\u4e86\u4e00\u9879\u5b9e\u8bc1\u7814\u7a76\uff0c\u91c7\u7528\u7531\u4e34\u5e8a\u533b\u751f\u521b\u5efa\u5e76\u9a8c\u8bc1\u7684\u4ee3\u7801\u672c\uff0c\u901a\u8fc7\u5fd7\u613f\u53c2\u4e0e\u7684\u6cbb\u7597\u5e08\u548c\u5b66\u5458\u7684\u6807\u6ce8\uff0c\u5bf9OLMo-2-32b\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u4f7f\u7528\u5e7f\u4e49\u7ebf\u6027\u6df7\u5408\u6548\u5e94\u6a21\u578b\u8fdb\u884c\u7edf\u8ba1\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cOLMo-2-32b\u5728\u68c0\u6d4b\u5230\u7528\u6237\u60c5\u7eea\u98ce\u9669\u4fe1\u53f7\u65f6\uff0c\u5176\u4e0e\u4e34\u5e8a\u6307\u5bfc\u4e0d\u7b26\u7684\u54cd\u5e94\u65b9\u5f0f\u53ef\u80fd\u5bfc\u81f4\u7528\u6237\u4e0d\u613f\u610f\u7ee7\u7eed\u5bf9\u8bdd\uff0c\u5e76\u4e14\u5bf9\u4e8e\u4e0d\u540c\u98ce\u9669\u56e0\u7d20\u7684\u8868\u8fbe\u54cd\u5e94\u4e5f\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "OLMo-2-32b\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u5bf9\u81ea\u6740\u98ce\u9669\u4fe1\u53f7\u7684\u54cd\u5e94\u65b9\u5f0f\u4e0e\u4e34\u5e8a\u6700\u4f73\u5b9e\u8df5\u76f8\u6096\uff0c\u53ef\u80fd\u5bfc\u81f4\u7528\u6237\u611f\u89c9\u88ab\u5ffd\u89c6\u6216\u653e\u5f03\uff0c\u964d\u4f4e\u5bfb\u6c42\u5e2e\u52a9\u7684\u610f\u613f\u3002"}}
{"id": "2510.26915", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26915", "abs": "https://arxiv.org/abs/2510.26915", "authors": ["Zachary Ravichandran", "Fernando Cladera", "Ankit Prabhu", "Jason Hughes", "Varun Murali", "Camillo Taylor", "George J. Pappas", "Vijay Kumar"], "title": "Heterogeneous Robot Collaboration in Unstructured Environments with Grounded Generative Intelligence", "comment": null, "summary": "Heterogeneous robot teams operating in realistic settings often must\naccomplish complex missions requiring collaboration and adaptation to\ninformation acquired online. Because robot teams frequently operate in\nunstructured environments -- uncertain, open-world settings without prior maps\n-- subtasks must be grounded in robot capabilities and the physical world.\nWhile heterogeneous teams have typically been designed for fixed\nspecifications, generative intelligence opens the possibility of teams that can\naccomplish a wide range of missions described in natural language. However,\ncurrent large language model (LLM)-enabled teaming methods typically assume\nwell-structured and known environments, limiting deployment in unstructured\nenvironments. We present SPINE-HT, a framework that addresses these limitations\nby grounding the reasoning abilities of LLMs in the context of a heterogeneous\nrobot team through a three-stage process. Given language specifications\ndescribing mission goals and team capabilities, an LLM generates grounded\nsubtasks which are validated for feasibility. Subtasks are then assigned to\nrobots based on capabilities such as traversability or perception and refined\ngiven feedback collected during online operation. In simulation experiments\nwith closed-loop perception and control, our framework achieves nearly twice\nthe success rate compared to prior LLM-enabled heterogeneous teaming\napproaches. In real-world experiments with a Clearpath Jackal, a Clearpath\nHusky, a Boston Dynamics Spot, and a high-altitude UAV, our method achieves an\n87\\% success rate in missions requiring reasoning about robot capabilities and\nrefining subtasks with online feedback. More information is provided at\nhttps://zacravichandran.github.io/SPINE-HT.", "AI": {"tldr": "SPINE-HT\u6846\u67b6\u9488\u5bf9\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u5728\u5f00\u653e\u73af\u5883\u4e2d\u8fdb\u884c\u590d\u6742\u4efb\u52a1\u7684\u80fd\u529b\u8fdb\u884c\u4e86\u589e\u5f3a\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u529f\u7387\uff0c\u8fbe\u5230\u4e8687%\u3002", "motivation": "\u5728\u4e0d\u786e\u5b9a\u3001\u5f00\u653e\u7684\u65e0\u5148\u9a8c\u5730\u56fe\u73af\u5883\u4e2d\uff0c\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u9700\u8981\u8fdb\u884c\u590d\u6742\u7684\u5408\u4f5c\u4efb\u52a1\uff0c\u5e76\u6709\u6548\u9002\u5e94\u5728\u7ebf\u83b7\u53d6\u7684\u4fe1\u606f\u3002", "method": "SPINE-HT\u6846\u67b6\u901a\u8fc7\u4e09\u9636\u6bb5\u8fc7\u7a0b\u5c06LLM\u7684\u63a8\u7406\u80fd\u529b\u4e0e\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u7684\u4e0a\u4e0b\u6587\u76f8\u7ed3\u5408\uff0c\u6839\u636e\u8bed\u8a00\u63cf\u8ff0\u7684\u4efb\u52a1\u76ee\u6807\u548c\u56e2\u961f\u80fd\u529b\uff0c\u751f\u6210\u80fd\u591f\u5b9e\u73b0\u7684\u5b50\u4efb\u52a1\uff0c\u5e76\u57fa\u4e8e\u80fd\u529b\uff08\u5982\u53ef\u7a7f\u8d8a\u6027\u6216\u611f\u77e5\uff09\u5206\u914d\u7ed9 robots \uff0c\u7ecf\u8fc7\u53cd\u9988\u4e0d\u65ad\u7cbe\u70bc\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u5b9e\u9a8c\u548c\u5b9e\u9645\u56e2\u961f\u5b9e\u9a8c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e0d\u540c\u673a\u5668\u4eba\u53c2\u4e0e\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u57fa\u4e8e\u673a\u5668\u4eba\u80fd\u529b\u8fdb\u884c\u63a8\u7406\u548c\u4f18\u5316\u5b50\u4efb\u52a1\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "\u6211\u4eec\u7684\u6846\u67b6\u5728\u5b9e\u9645\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e8687%\u7684\u6210\u529f\u7387\uff0c\u76f8\u8f83\u4e8e\u4e4b\u524d\u7684LLM\u589e\u5f3a\u7684\u5f02\u6784\u56e2\u961f\u65b9\u6cd5\uff0c\u6210\u529f\u7387\u51e0\u4e4e\u7ffb\u500d\u3002"}}
{"id": "2510.27572", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.27572", "abs": "https://arxiv.org/abs/2510.27572", "authors": ["Likitha Tadakala", "Muskan Saraf", "Sajjad Rezvani Boroujeni", "Hossein Abedi", "Tom Bush"], "title": "Beyond Visualization: Building Decision Intelligence Through Iterative Dashboard Refinement", "comment": null, "summary": "Effective business intelligence (BI) dashboards evolve through iterative\nrefinement rather than single-pass design. Addressing the lack of structured\nimprovement frameworks in BI practice, this study documents the four-stage\nevolution of a Power BI dashboard analyzing profitability decline in a\nfictional retail firm, Global Superstore. Using a dataset of \\$12.64 million in\nsales across seven markets and three product categories, the project\ndemonstrates how feedback-driven iteration and gap analysis convert exploratory\nvisuals into decision-support tools. Guided by four executive questions on\nprofitability, market prioritization, discount effects, and shipping costs,\neach iteration resolved analytical or interpretive shortcomings identified\nthrough collaborative review. Key findings include margin erosion in furniture\n(6.94% vs. 13.99% for technology), a 20% discount threshold beyond which\nprofitability declined, and \\$1.35 million in unrecovered shipping costs.\nContributions include: (a) a replicable feedback-driven methodology grounded in\niterative gap analysis; (b) DAX-based technical enhancements improving\ninterpretive clarity; (c) an inductively derived six-element narrative\nframework; and (d) evidence that narrative coherence emerges organically\nthrough structured refinement. The methodology suggests transferable value for\nboth BI practitioners and educators, pending validation across diverse\norganizational contexts.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cd\u9988\u7684\u8fed\u4ee3\u6539\u8fdb\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u4e00\u4e2a\u865a\u6784\u96f6\u552e\u516c\u53f8\u7684\u76c8\u5229\u80fd\u529b\u4e0b\u964d\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u6539\u8fdb\u63d0\u5347\u5546\u4e1a\u667a\u80fd\u4eea\u8868\u677f\u7684\u51b3\u7b56\u652f\u6301\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5546\u4e1a\u667a\u80fd\u5b9e\u8df5\u4e2d\u7f3a\u4e4f\u7ed3\u6784\u6027\u6539\u8fdb\u6846\u67b6\u7684\u95ee\u9898\uff0c\u4ee5\u63d0\u5347\u4eea\u8868\u677f\u7684\u6709\u6548\u6027\u3002", "method": "\u4f7f\u7528\u56db\u9636\u6bb5\u7684\u8fed\u4ee3\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u534f\u4f5c\u5ba1\u67e5\u6765\u8bc6\u522b\u5e76\u89e3\u51b3\u5206\u6790\u4e0a\u7684\u4e0d\u8db3\u3002", "result": "\u53d1\u73b0\u5bb6\u5177\u7684\u5229\u6da6\u7387\u4f4e\u4e8e\u6280\u672f\u4ea7\u54c1\uff0c\u6298\u6263\u8fbe\u523020%\u540e\u5229\u6da6\u4e0b\u964d\uff0c\u4ee5\u53ca\u672a\u6536\u56de\u7684\u8fd0\u8d39\u8fbe\u5230135\u4e07\u7f8e\u5143\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u53cd\u9988\u9a71\u52a8\u7684\u8fed\u4ee3\u65b9\u6cd5\u5728\u5546\u4e1a\u667a\u80fd\u4eea\u8868\u677f\u7684\u53d1\u5c55\u4e2d\u5982\u4f55\u4ea7\u751f\u663e\u8457\u7684\u51b3\u7b56\u652f\u6301\u5de5\u5177\u3002"}}
{"id": "2510.26935", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.FL"], "pdf": "https://arxiv.org/pdf/2510.26935", "abs": "https://arxiv.org/abs/2510.26935", "authors": ["Yunhao Yang", "Neel P. Bhatt", "Pranay Samineni", "Rohan Siva", "Zhanyang Wang", "Ufuk Topcu"], "title": "RepV: Safety-Separable Latent Spaces for Scalable Neurosymbolic Plan Verification", "comment": "Code and data are available at: https://repv-project.github.io/", "summary": "As AI systems migrate to safety-critical domains, verifying that their\nactions comply with well-defined rules remains a challenge. Formal methods\nprovide provable guarantees but demand hand-crafted temporal-logic\nspecifications, offering limited expressiveness and accessibility. Deep\nlearning approaches enable evaluation of plans against natural-language\nconstraints, yet their opaque decision process invites misclassifications with\npotentially severe consequences. We introduce RepV, a neurosymbolic verifier\nthat unifies both views by learning a latent space where safe and unsafe plans\nare linearly separable. Starting from a modest seed set of plans labeled by an\noff-the-shelf model checker, RepV trains a lightweight projector that embeds\neach plan, together with a language model-generated rationale, into a\nlow-dimensional space; a frozen linear boundary then verifies compliance for\nunseen natural-language rules in a single forward pass.\n  Beyond binary classification, RepV provides a probabilistic guarantee on the\nlikelihood of correct verification based on its position in the latent space.\nThis guarantee enables a guarantee-driven refinement of the planner, improving\nrule compliance without human annotations. Empirical evaluations show that RepV\nimproves compliance prediction accuracy by up to 15% compared to baseline\nmethods while adding fewer than 0.2M parameters. Furthermore, our refinement\nframework outperforms ordinary fine-tuning baselines across various planning\ndomains. These results show that safety-separable latent spaces offer a\nscalable, plug-and-play primitive for reliable neurosymbolic plan verification.\nCode and data are available at: https://repv-project.github.io/.", "AI": {"tldr": "RepV\u662f\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u9a8c\u8bc1\u5668\uff0c\u7ed3\u5408\u5f62\u5f0f\u65b9\u6cd5\u4e0e\u6df1\u5ea6\u5b66\u4e60\uff0c\u901a\u8fc7\u5b66\u4e60\u6f5c\u5728\u7a7a\u95f4\u6765\u5b9e\u73b0\u5b89\u5168\u8ba1\u5212\u7684\u9a8c\u8bc1\uff0c\u63d0\u9ad8\u5408\u89c4\u6027\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u540c\u65f6\u51cf\u5c11\u6a21\u578b\u53c2\u6570\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u8fc1\u79fb\u5230\u5b89\u5168\u5173\u952e\u9886\u57df\uff0c\u786e\u4fdd\u5176\u884c\u52a8\u7b26\u5408\u89c4\u5b9a\u7684\u6311\u6218\u4e0d\u65ad\u589e\u5927\uff0c\u65e2\u6709\u7684\u5f62\u5f0f\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5404\u6709\u5c40\u9650\u3002", "method": "RepV\u91c7\u7528\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6295\u5f71\u5668\uff0c\u5c06\u6807\u8bb0\u8ba1\u5212\u548c\u7531\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u63a8\u7406\u5d4c\u5165\u5230\u4f4e\u7ef4\u7a7a\u95f4\u4e2d\uff0c\u5e76\u5229\u7528\u51bb\u7ed3\u7684\u7ebf\u6027\u8fb9\u754c\u5bf9\u672a\u89c1\u7684\u81ea\u7136\u8bed\u8a00\u89c4\u5219\u8fdb\u884c\u5408\u89c4\u6027\u9a8c\u8bc1\u3002", "result": "RepV\u4f7f\u5408\u89c4\u6027\u9884\u6d4b\u51c6\u786e\u7387\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6700\u9ad815%\uff0c\u4e14\u5728\u4e0d\u540c\u89c4\u5212\u9886\u57df\u7684\u7cbe\u7ec6\u5316\u6846\u67b6\u4e0a\u8868\u73b0\u8d85\u8fc7\u666e\u901a\u5fae\u8c03\u57fa\u7ebf\u3002", "conclusion": "RepV\u901a\u8fc7\u5b66\u4e60\u6f5c\u5728\u7a7a\u95f4\uff0c\u5b9e\u73b0\u4e86\u5b89\u5168\u548c\u4e0d\u5b89\u5168\u8ba1\u5212\u7684\u7ebf\u6027\u53ef\u5206\u6027\uff0c\u5e76\u4e14\u5728\u5408\u89c4\u6027\u9884\u6d4b\u65b9\u9762\u8d85\u8d8a\u4e86\u57fa\u51c6\u65b9\u6cd5\uff0c\u4e3a\u795e\u7ecf\u7b26\u53f7\u8ba1\u5212\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u548c\u53ef\u9760\u7684\u5de5\u5177\u3002"}}
{"id": "2510.27681", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.27681", "abs": "https://arxiv.org/abs/2510.27681", "authors": ["Sean Kelley", "David De Cremer", "Christoph Riedl"], "title": "Personalized AI Scaffolds Synergistic Multi-Turn Collaboration in Creative Work", "comment": null, "summary": "As AI becomes more deeply embedded in knowledge work, building assistants\nthat support human creativity and expertise becomes more important. Yet\nachieving synergy in human-AI collaboration is not easy. Providing AI with\ndetailed information about a user's demographics, psychological attributes,\ndivergent thinking, and domain expertise may improve performance by scaffolding\nmore effective multi-turn interactions. We implemented a personalized LLM-based\nassistant, informed by users' psychometric profiles and an AI-guided interview\nabout their work style, to help users complete a marketing task for a fictional\nstartup. We randomized 331 participants to work with AI that was either generic\n(n = 116), partially personalized (n = 114), or fully personalized (n=101).\nParticipants working with personalized AI produce marketing campaigns of\nsignificantly higher quality and creativity, beyond what AI alone could have\nproduced. Compared to generic AI, personalized AI leads to higher self-reported\nlevels of assistance and feedback, while also increasing participant trust and\nconfidence. Causal mediation analysis shows that personalization improves\nperformance indirectly by enhancing collective memory, attention, and reasoning\nin the human-AI interaction. These findings provide a theory-driven framework\nin which personalization functions as external scaffolding that builds common\nground and shared partner models, reducing uncertainty and enhancing joint\ncognition. This informs the design of future AI assistants that maximize\nsynergy and support human creative potential while limiting negative\nhomogenization.", "AI": {"tldr": "\u4e2a\u6027\u5316AI\u901a\u8fc7\u7406\u89e3\u7528\u6237\u7279\u5f81\u6765\u63d0\u5347\u8425\u9500\u4efb\u52a1\u8868\u73b0\uff0c\u589e\u5f3a\u4eba\u673a\u534f\u4f5c\u6548\u679c\uff0c\u672a\u6765\u8bbe\u8ba1\u5e94\u6700\u5927\u5316\u534f\u540c\u6548\u5e94\u5e76\u652f\u6301\u4eba\u7c7b\u521b\u9020\u6f5c\u529b\u3002", "motivation": "\u968f\u7740AI\u5728\u77e5\u8bc6\u5de5\u4f5c\u4e2d\u7684\u6df1\u5165\u5e94\u7528\uff0c\u521b\u5efa\u652f\u6301\u4eba\u7c7b\u521b\u9020\u529b\u548c\u4e13\u4e1a\u77e5\u8bc6\u7684\u52a9\u624b\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\uff0c\u7136\u800c\u5b9e\u73b0\u4eba\u673a\u534f\u4f5c\u7684\u534f\u540c\u4f5c\u7528\u5e76\u4e0d\u5bb9\u6613\u3002", "method": "\u91c7\u7528\u4e2a\u6027\u5316LLM\u52a9\u624b\uff0c\u7ed3\u5408\u7528\u6237\u5fc3\u7406\u7279\u5f81\u548c\u5de5\u4f5c\u98ce\u683c\u7684AI\u6307\u5bfc\u8bbf\u8c08\uff0c\u8fdb\u884c\u968f\u673a\u5bf9\u7167\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e0d\u540c\u4e2a\u6027\u5316\u7a0b\u5ea6\u7684AI\u5bf9\u8425\u9500\u4efb\u52a1\u8868\u73b0\u7684\u5f71\u54cd\u3002", "result": "\u53c2\u4e0e\u4e2a\u6027\u5316AI\u7684\u7528\u6237\u5728\u8425\u9500\u6d3b\u52a8\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u8d28\u91cf\u548c\u521b\u9020\u529b\uff0c\u4e2a\u6027\u5316AI\u63d0\u9ad8\u4e86\u7528\u6237\u7684\u5e2e\u52a9\u611f\u548c\u53cd\u9988\u6c34\u5e73\uff0c\u589e\u52a0\u4e86\u4fe1\u4efb\u548c\u4fe1\u5fc3\u3002", "conclusion": "\u4e2a\u6027\u5316AI\u80fd\u663e\u8457\u63d0\u5347\u7528\u6237\u8425\u9500\u4efb\u52a1\u7684\u8d28\u91cf\u548c\u521b\u9020\u529b\uff0c\u5e76\u901a\u8fc7\u589e\u5f3a\u96c6\u4f53\u8bb0\u5fc6\u3001\u6ce8\u610f\u529b\u548c\u63a8\u7406\u6765\u95f4\u63a5\u6539\u5584\u4eba\u673a\u4e92\u52a8\u7ee9\u6548\u3002"}}
{"id": "2510.27010", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.27010", "abs": "https://arxiv.org/abs/2510.27010", "authors": ["William E. Heap", "Yimeng Qin", "Kai Hammond", "Anish Bayya", "Haonon Kong", "Allison M. Okamura"], "title": "A Hermetic, Transparent Soft Growing Vine Robot System for Pipe Inspection", "comment": "8 pages, 7 figures", "summary": "Rehabilitation of aging pipes requires accurate condition assessment and\nmapping far into the pipe interiors. Soft growing vine robot systems are\nparticularly promising for navigating confined, sinuous paths such as in pipes,\nbut are currently limited by complex subsystems and a lack of validation in\nreal-world industrial settings. In this paper, we introduce the concept and\nimplementation of a hermetic and transparent vine robot system for visual\ncondition assessment and mapping within non-branching pipes. This design\nencloses all mechanical and electrical components within the vine robot's soft,\nairtight, and transparent body, protecting them from environmental interference\nwhile enabling visual sensing. Because this approach requires an enclosed\nmechanism for transporting sensors, we developed, modeled, and tested a\npassively adapting enclosed tip mount. Finally, we validated the hermetic and\ntransparent vine robot system concept through a real-world condition assessment\nand mapping task in a wastewater pipe. This work advances the use of\nsoft-growing vine robots in pipe inspection by developing and demonstrating a\nrobust, streamlined, field-validated system suitable for continued development\nand deployment.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u5c01\u95ed\u548c\u900f\u660e\u7684\u8f6f\u751f\u957f\u85e4\u72b6\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7528\u4e8e\u7ba1\u9053\u7684\u89c6\u89c9\u72b6\u6001\u8bc4\u4f30\u548c\u6620\u5c04\uff0c\u7ecf\u8fc7\u73b0\u573a\u9a8c\u8bc1\uff0c\u663e\u793a\u51fa\u826f\u597d\u7684\u5e94\u7528\u524d\u666f\u3002", "motivation": "\u968f\u7740\u8001\u5316\u7ba1\u9053\u7684\u4fee\u590d\u9700\u6c42\u589e\u52a0\uff0c\u51c6\u786e\u7684\u72b6\u6001\u8bc4\u4f30\u548c\u5185\u90e8\u6620\u5c04\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7684\u8f6f\u751f\u957f\u85e4\u673a\u5668\u4eba\u7cfb\u7edf\u672a\u80fd\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u8fdb\u884c\u6709\u6548\u9a8c\u8bc1\u3002", "method": "\u8bbe\u8ba1\u3001\u5efa\u6a21\u548c\u6d4b\u8bd5\u4e86\u4e00\u79cd\u88ab\u52a8\u9002\u5e94\u7684\u5c01\u95ed\u5c16\u7aef\u652f\u67b6\uff0c\u7ed3\u5408\u4e86\u673a\u68b0\u548c\u7535\u6c14\u7ec4\u4ef6\u7684\u5c01\u95ed\u8bbe\u8ba1\u3002", "result": "\u5728\u6c61\u6c34\u7ba1\u9053\u4e2d\u7684\u5b9e\u9645\u6761\u4ef6\u8bc4\u4f30\u548c\u6620\u5c04\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u7cfb\u7edf\u7684\u6548\u679c\uff0c\u5c55\u793a\u4e86\u5176\u5728\u7ba1\u9053\u68c0\u67e5\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u7a33\u5065\u4e14\u7ecf\u8fc7\u73b0\u573a\u9a8c\u8bc1\u7684\u8f6f\u751f\u957f\u85e4\u72b6\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u9002\u7528\u4e8e\u7ba1\u9053\u68c0\u67e5\uff0c\u63a8\u52a8\u4e86\u8be5\u6280\u672f\u7684\u8fdb\u4e00\u6b65\u5f00\u53d1\u548c\u5e94\u7528\u3002"}}
{"id": "2510.27436", "categories": ["cs.RO", "cs.HC", "I.2.9; I.3.6"], "pdf": "https://arxiv.org/pdf/2510.27436", "abs": "https://arxiv.org/abs/2510.27436", "authors": ["Tomoko Yonezawa", "Hirotake Yamazoe", "Atsuo Fujino", "Daigo Suhara", "Takaya Tamamoto", "Yuto Nishiguchi"], "title": "Preliminary Prototyping of Avoidance Behaviors Triggered by a User's Physical Approach to a Robot", "comment": "Workshop on Socially Aware and Cooperative Intelligent Systems in HAI\n  2025", "summary": "Human-robot interaction frequently involves physical proximity or contact. In\nhuman-human settings, people flexibly accept, reject, or tolerate such\napproaches depending on the relationship and context. We explore the design of\na robot's rejective internal state and corresponding avoidance behaviors, such\nas withdrawing or pushing away, when a person approaches. We model the\naccumulation and decay of discomfort as a function of interpersonal distance,\nand implement tolerance (endurance) and limit-exceeding avoidance driven by the\nDominance axis of the PAD affect model. The behaviors and their intensities are\nrealized on an arm robot. Results illustrate a coherent pipeline from internal\nstate parameters to graded endurance motions and, once a limit is crossed, to\navoidance actions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u5e76\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u5728\u4eba\u5458\u63a5\u8fd1\u65f6\u7684\u62d2\u7edd\u548c\u56de\u907f\u884c\u4e3a\uff0c\u57fa\u4e8e\u4eba\u9645\u8ddd\u79bb\u6a21\u578b\u8bbe\u8ba1\uff0c\u5c55\u793a\u4e86\u673a\u5668\u4eba\u5982\u4f55\u5728\u4e92\u52a8\u4e2d\u7075\u6d3b\u8868\u8fbe\u4e0d\u9002\u611f\u3002", "motivation": "\u7814\u7a76\u4eba\u673a\u4ea4\u4e92\u4e2d\uff0c\u673a\u5668\u4eba\u5982\u4f55\u6709\u6548\u5730\u8868\u8fbe\u62d2\u7edd\u548c\u56de\u907f\uff0c\u7279\u522b\u662f\u5728\u8fd1\u8ddd\u79bb\u63a5\u89e6\u65f6\u7684\u4e92\u52a8\u53cd\u5e94\u3002", "method": "\u901a\u8fc7\u5efa\u7acb\u4e0d\u9002\u611f\u7d2f\u79ef\u548c\u8870\u51cf\u7684\u6a21\u578b\uff0c\u7ed3\u5408PAD\u60c5\u611f\u6a21\u578b\u7684\u4e3b\u5bfc\u6027\u8f74\uff0c\u6a21\u62df\u4e86\u673a\u5668\u4eba\u7684\u62d2\u7edd\u72b6\u6001\u548c\u56de\u907f\u884c\u4e3a\uff0c\u5e76\u5728\u673a\u68b0\u81c2\u4e0a\u5b9e\u73b0\u4e86\u8fd9\u4e9b\u884c\u4e3a\u7684\u8868\u8fbe\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u5c55\u793a\u4e86\u5185\u90e8\u72b6\u6001\u53c2\u6570\u5230\u6e10\u53d8\u8010\u53d7\u52a8\u4f5c\u7684\u5b8c\u6574\u6d41\u7a0b\uff0c\u5e76\u5728\u8d85\u8fc7\u6781\u9650\u65f6\u8868\u73b0\u51fa\u56de\u907f\u884c\u4e3a\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u79cd\u673a\u5668\u4eba\u62d2\u7edd\u548c\u56de\u907f\u884c\u4e3a\u7684\u6a21\u578b\uff0c\u80fd\u591f\u6839\u636e\u4eba\u9645\u8ddd\u79bb\u8c03\u6574\u673a\u5668\u4eba\u4e0e\u4eba\u4e4b\u95f4\u7684\u4e92\u52a8\u65b9\u5f0f\u3002"}}
{"id": "2510.27033", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.27033", "abs": "https://arxiv.org/abs/2510.27033", "authors": ["Simindokht Jahangard", "Mehrzad Mohammadi", "Abhinav Dhall", "Hamid Rezatofighi"], "title": "A Multi-Modal Neuro-Symbolic Approach for Spatial Reasoning-Based Visual Grounding in Robotics", "comment": null, "summary": "Visual reasoning, particularly spatial reasoning, is a challenging cognitive\ntask that requires understanding object relationships and their interactions\nwithin complex environments, especially in robotics domain. Existing\nvision_language models (VLMs) excel at perception tasks but struggle with\nfine-grained spatial reasoning due to their implicit, correlation-driven\nreasoning and reliance solely on images. We propose a novel neuro_symbolic\nframework that integrates both panoramic-image and 3D point cloud information,\ncombining neural perception with symbolic reasoning to explicitly model spatial\nand logical relationships. Our framework consists of a perception module for\ndetecting entities and extracting attributes, and a reasoning module that\nconstructs a structured scene graph to support precise, interpretable queries.\nEvaluated on the JRDB-Reasoning dataset, our approach demonstrates superior\nperformance and reliability in crowded, human_built environments while\nmaintaining a lightweight design suitable for robotics and embodied AI\napplications.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5168\u666f\u56fe\u50cf\u548c3D\u70b9\u4e91\u4fe1\u606f\uff0c\u6539\u5584\u4e86\u89c6\u89c9\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u6211\u4eec\u5e0c\u671b\u901a\u8fc7\u6574\u5408\u795e\u7ecf\u611f\u77e5\u548c\u7b26\u53f7\u63a8\u7406\u6765\u6539\u5584\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u6211\u4eec\u7684\u65b9\u6cd5\u7ed3\u5408\u4e86\u5168\u666f\u56fe\u50cf\u548c3D\u70b9\u4e91\u4fe1\u606f\uff0c\u5305\u62ec\u611f\u77e5\u6a21\u5757\u548c\u63a8\u7406\u6a21\u5757\uff0c\u901a\u8fc7\u6784\u5efa\u7ed3\u6784\u5316\u573a\u666f\u56fe\u6765\u652f\u6301\u7cbe\u786e\u7684\u67e5\u8be2\u3002", "result": "\u5728JRDB-Reasoning\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u62e5\u6324\u7684\u4eba\u9020\u73af\u5883\u4e2d\u5c55\u73b0\u4e86\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u9002\u5408\u7528\u4e8e\u673a\u5668\u4eba\u548c\u5177\u8eab\u4eba\u5de5\u667a\u80fd\u5e94\u7528\u3002"}}
{"id": "2510.27048", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.27048", "abs": "https://arxiv.org/abs/2510.27048", "authors": ["Eric T. Chang", "Peter Ballentine", "Zhanpeng He", "Do-Gon Kim", "Kai Jiang", "Hua-Hsuan Liang", "Joaquin Palacios", "William Wang", "Pedro Piacenza", "Ioannis Kymissis", "Matei Ciocarlie"], "title": "SpikeATac: A Multimodal Tactile Finger with Taxelized Dynamic Sensing for Dexterous Manipulation", "comment": "9 pages, 8 figures, under review", "summary": "In this work, we introduce SpikeATac, a multimodal tactile finger combining a\ntaxelized and highly sensitive dynamic response (PVDF) with a static\ntransduction method (capacitive) for multimodal touch sensing. Named for its\n`spiky' response, SpikeATac's 16-taxel PVDF film sampled at 4 kHz provides\nfast, sensitive dynamic signals to the very onset and breaking of contact. We\ncharacterize the sensitivity of the different modalities, and show that\nSpikeATac provides the ability to stop quickly and delicately when grasping\nfragile, deformable objects. Beyond parallel grasping, we show that SpikeATac\ncan be used in a learning-based framework to achieve new capabilities on a\ndexterous multifingered robot hand. We use a learning recipe that combines\nreinforcement learning from human feedback with tactile-based rewards to\nfine-tune the behavior of a policy to modulate force. Our hardware platform and\nlearning pipeline together enable a difficult dexterous and contact-rich task\nthat has not previously been achieved: in-hand manipulation of fragile objects.\nVideos are available at\n\\href{https://roamlab.github.io/spikeatac/}{roamlab.github.io/spikeatac}.", "AI": {"tldr": "SpikeATac\u662f\u4e00\u79cd\u65b0\u578b\u89e6\u89c9\u4f20\u611f\u5668\uff0c\u7ed3\u5408\u591a\u79cd\u611f\u5e94\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u7075\u5de7\u7684\u7269\u4f53\u64cd\u63a7\uff0c\u7279\u522b\u9002\u5408\u5904\u7406\u8106\u5f31\u7269\u4f53\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u9ad8\u7075\u654f\u5ea6\u7684\u89e6\u89c9\u4f20\u611f\u5668\uff0c\u4ee5\u63d0\u9ad8\u673a\u5668\u4eba\u624b\u6307\u5728\u64cd\u63a7\u8106\u5f31\u548c\u53ef\u53d8\u5f62\u7269\u4f53\u4e0a\u7684\u80fd\u529b\u3002", "method": "\u7ed3\u5408\u4e86PVDF\u52a8\u6001\u4f20\u611f\u5668\u548c\u7535\u5bb9\u9759\u6001\u4f20\u611f\u5668\u7684SpikeATac\u624b\u6307\uff0c\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u4e0e\u89e6\u89c9\u53cd\u9988\u7684\u5b66\u4e60\u6846\u67b6\u8fdb\u884c\u8c03\u4f18\u3002", "result": "SpikeATac\u80fd\u591f\u5feb\u901f\u3001\u7cbe\u786e\u5730\u6293\u53d6\u548c\u64cd\u63a7\u8106\u5f31\u7269\u4f53\uff0c\u5b9e\u73b0\u4e4b\u524d\u672a\u80fd\u5b8c\u6210\u7684\u590d\u6742\u4efb\u52a1\u3002", "conclusion": "SpikeATac\u5c55\u793a\u4e86\u5176\u5728\u5904\u7406\u8106\u5f31\u7269\u4f53\u65f6\u7684\u9ad8\u7075\u654f\u5ea6\u548c\u63a7\u5236\u80fd\u529b\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u624b\u6307\u5728\u63a5\u89e6\u4e30\u5bcc\u7684\u73af\u5883\u4e2d\u7684\u7075\u5de7\u64cd\u4f5c\u3002"}}
{"id": "2510.27114", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.27114", "abs": "https://arxiv.org/abs/2510.27114", "authors": ["Dohyeok Lee", "Jung Min Lee", "Munkyung Kim", "Seokhun Ju", "Jin Woo Koo", "Kyungjae Lee", "Dohyeong Kim", "TaeHyun Cho", "Jungwoo Lee"], "title": "Learning Generalizable Visuomotor Policy through Dynamics-Alignment", "comment": "9 pages, 6 figures", "summary": "Behavior cloning methods for robot learning suffer from poor generalization\ndue to limited data support beyond expert demonstrations. Recent approaches\nleveraging video prediction models have shown promising results by learning\nrich spatiotemporal representations from large-scale datasets. However, these\nmodels learn action-agnostic dynamics that cannot distinguish between different\ncontrol inputs, limiting their utility for precise manipulation tasks and\nrequiring large pretraining datasets. We propose a Dynamics-Aligned Flow\nMatching Policy (DAP) that integrates dynamics prediction into policy learning.\nOur method introduces a novel architecture where policy and dynamics models\nprovide mutual corrective feedback during action generation, enabling\nself-correction and improved generalization. Empirical validation demonstrates\ngeneralization performance superior to baseline methods on real-world robotic\nmanipulation tasks, showing particular robustness in OOD scenarios including\nvisual distractions and lighting variations.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b56\u7565\uff0c\u901a\u8fc7\u5c06\u52a8\u6001\u9884\u6d4b\u878d\u5165\u7b56\u7565\u5b66\u4e60\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5728\u64cd\u7eb5\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u89c6\u89c9\u548c\u5149\u7167\u53d8\u6362\u7684\u5e72\u6270\u65f6\u3002", "motivation": "\u884c\u4e3a\u514b\u9686\u65b9\u6cd5\u56e0\u4e13\u5bb6\u6f14\u793a\u6570\u636e\u652f\u6301\u6709\u9650\u800c\u5728\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u56e0\u6b64\u672c\u7814\u7a76\u5bfb\u6c42\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u6027\u80fd\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u67b6\u6784\uff0c\u5728\u884c\u52a8\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u7b56\u7565\u6a21\u578b\u548c\u52a8\u6001\u6a21\u578b\u4e4b\u95f4\u63d0\u4f9b\u76f8\u4e92\u7ea0\u6b63\u53cd\u9988\uff0c\u4ee5\u5b9e\u73b0\u81ea\u6211\u4fee\u6b63\u548c\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u771f\u5b9e\u673a\u5668\u4eba\u64cd\u7eb5\u4efb\u52a1\u4e2d\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u5e72\u6270\u548c\u5149\u7167\u53d8\u5316\u7b49OOD\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u63d0\u51fa\u7684\u52a8\u529b\u5bf9\u9f50\u6d41\u5339\u914d\u7b56\u7565\uff08DAP\uff09\u5728\u771f\u5b9e\u4e16\u754c\u7684\u673a\u5668\u4eba\u64cd\u7eb5\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u4f18\u8d8a\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u5c24\u5176\u5728OOD\uff08\u8fc7\u5ea6\u5916\u63a8\uff09\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.27151", "categories": ["cs.RO", "/"], "pdf": "https://arxiv.org/pdf/2510.27151", "abs": "https://arxiv.org/abs/2510.27151", "authors": ["Xueliang Cheng", "Kanzhong Yao", "Andrew West", "Ognjen Marjanovic", "Barry Lennox", "Keir Groves"], "title": "Confined Space Underwater Positioning Using Collaborative Robots", "comment": "31 pages including appendix, 24 figures", "summary": "Positioning of underwater robots in confined and cluttered spaces remains a\nkey challenge for field operations. Existing systems are mostly designed for\nlarge, open-water environments and struggle in industrial settings due to poor\ncoverage, reliance on external infrastructure, and the need for feature-rich\nsurroundings. Multipath effects from continuous sound reflections further\ndegrade signal quality, reducing accuracy and reliability. Accurate and easily\ndeployable positioning is essential for repeatable autonomous missions;\nhowever, this requirement has created a technological bottleneck limiting\nunderwater robotic deployment. This paper presents the Collaborative Aquatic\nPositioning (CAP) system, which integrates collaborative robotics and sensor\nfusion to overcome these limitations. Inspired by the \"mother-ship\" concept,\nthe surface vehicle acts as a mobile leader to assist in positioning a\nsubmerged robot, enabling localization even in GPS-denied and highly\nconstrained environments. The system is validated in a large test tank through\nrepeatable autonomous missions using CAP's position estimates for real-time\ntrajectory control. Experimental results demonstrate a mean Euclidean distance\n(MED) error of 70 mm, achieved in real time without requiring fixed\ninfrastructure, extensive calibration, or environmental features. CAP leverages\nadvances in mobile robot sensing and leader-follower control to deliver a step\nchange in accurate, practical, and infrastructure-free underwater localization.", "AI": {"tldr": "CAP\u7cfb\u7edf\u4e3a\u6c34\u4e0b\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u57fa\u7840\u8bbe\u65bd\u7684\u9ad8\u6548\u5b9a\u4f4d\u65b9\u6cd5\u3002", "motivation": "\u5728\u53d7\u9650\u548c\u6742\u4e71\u7684\u73af\u5883\u4e2d\u8fdb\u884c\u6c34\u4e0b\u673a\u5668\u4eba\u7684\u5b9a\u4f4d\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u800c\u73b0\u6709\u7cfb\u7edf\u5728\u5f00\u653e\u6c34\u57df\u73af\u5883\u4e2d\u6548\u679c\u66f4\u597d\u3002", "method": "CAP\u7cfb\u7edf\u4f7f\u7528\u79fb\u52a8\u9886\u5bfc\u8005\u5e2e\u52a9\u6c34\u4e0b\u673a\u5668\u4eba\u8fdb\u884c\u5b9a\u4f4d\uff0c\u5e76\u5728\u5927\u578b\u6d4b\u8bd5\u6c34\u69fd\u4e2d\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cCAP\u5b9e\u73b0\u4e8670\u6beb\u7c73\u7684\u5747\u65b9\u6839\u8bef\u5dee\uff0c\u4e14\u65e0\u9700\u56fa\u5b9a\u57fa\u7840\u8bbe\u65bd\u3001\u5927\u89c4\u6a21\u6821\u51c6\u6216\u4f9d\u8d56\u73af\u5883\u7279\u5f81\u3002", "conclusion": "CAP\u7cfb\u7edf\u901a\u8fc7\u7ed3\u5408\u5408\u4f5c\u673a\u5668\u4eba\u6280\u672f\u548c\u4f20\u611f\u5668\u878d\u5408\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728\u65e0GPS\u548c\u9ad8\u5ea6\u53d7\u9650\u73af\u5883\u4e2d\u6709\u6548\u7684\u6c34\u4e0b\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.27178", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.27178", "abs": "https://arxiv.org/abs/2510.27178", "authors": ["Xuan-Thuan Nguyen", "Khac Nam Nguyen", "Ngoc Duy Tran", "Thi Thoa Mac", "Anh Nguyen", "Hoang Hiep Ly", "Tung D. Ta"], "title": "MobiDock: Design and Control of A Modular Self Reconfigurable Bimanual Mobile Manipulator via Robotic Docking", "comment": "ICRA2026 submited", "summary": "Multi-robot systems, particularly mobile manipulators, face challenges in\ncontrol coordination and dynamic stability when working together. To address\nthis issue, this study proposes MobiDock, a modular self-reconfigurable mobile\nmanipulator system that allows two independent robots to physically connect and\nform a unified mobile bimanual platform. This process helps transform a complex\nmulti-robot control problem into the management of a simpler, single system.\nThe system utilizes an autonomous docking strategy based on computer vision\nwith AprilTag markers and a new threaded screw-lock mechanism. Experimental\nresults show that the docked configuration demonstrates better performance in\ndynamic stability and operational efficiency compared to two independently\ncooperating robots. Specifically, the unified system has lower Root Mean Square\n(RMS) Acceleration and Jerk values, higher angular precision, and completes\ntasks significantly faster. These findings confirm that physical\nreconfiguration is a powerful design principle that simplifies cooperative\ncontrol, improving stability and performance for complex tasks in real-world\nenvironments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u7684MobiDock\u7cfb\u7edf\u901a\u8fc7\u7269\u7406\u91cd\u6784\u7b80\u5316\u4e86\u591a\u673a\u5668\u4eba\u63a7\u5236\uff0c\u63d0\u9ad8\u4e86\u52a8\u6001\u7a33\u5b9a\u6027\u548c\u64cd\u4f5c\u6548\u7387\u3002", "motivation": "\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u534f\u540c\u63a7\u5236\u548c\u52a8\u6001\u7a33\u5b9a\u6027\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u79fb\u52a8\u64cd\u63a7\u5668\u3002", "method": "\u63d0\u51fa\u4e86MobiDock\uff0c\u4e00\u4e2a\u6a21\u5757\u5316\u81ea\u6211\u91cd\u6784\u7684\u79fb\u52a8\u64cd\u63a7\u5668\u7cfb\u7edf\uff0c\u5229\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u65b0\u7684\u87ba\u7eb9\u9501\u5b9a\u673a\u5236\u8fdb\u884c\u81ea\u4e3b\u5bf9\u63a5\u3002", "result": "\u4e0e\u4e24\u4e2a\u72ec\u7acb\u5408\u4f5c\u7684\u673a\u5668\u4eba\u76f8\u6bd4\uff0c\u5408\u5e76\u7cfb\u7edf\u5728\u52a8\u6001\u7a33\u5b9a\u6027\u548c\u64cd\u4f5c\u6548\u7387\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u5177\u6709\u66f4\u4f4e\u7684\u5747\u65b9\u6839\u52a0\u901f\u5ea6\u548c\u6296\u52a8\u503c\u3001\u66f4\u9ad8\u7684\u89d2\u5ea6\u7cbe\u5ea6\uff0c\u4e14\u4efb\u52a1\u5b8c\u6210\u901f\u5ea6\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "\u7269\u7406\u91cd\u6784\u662f\u4e00\u79cd\u5f3a\u5927\u7684\u8bbe\u8ba1\u539f\u5219\uff0c\u5b83\u7b80\u5316\u4e86\u534f\u540c\u63a7\u5236\uff0c\u63d0\u9ad8\u4e86\u590d\u6742\u4efb\u52a1\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2510.27184", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.27184", "abs": "https://arxiv.org/abs/2510.27184", "authors": ["Hoang Hiep Ly", "Cong-Nhat Nguyen", "Doan-Quang Tran", "Quoc-Khanh Dang", "Ngoc Duy Tran", "Thi Thoa Mac", "Anh Nguyen", "Xuan-Thuan Nguyen", "Tung D. Ta"], "title": "Hybrid Gripper Finger Enabling In-Grasp Friction Modulation Using Inflatable Silicone Pockets", "comment": "Submitted to ICRA 2026", "summary": "Grasping objects with diverse mechanical properties, such as heavy, slippery,\nor fragile items, remains a significant challenge in robotics. Conventional\ngrippers often rely on applying high normal forces, which can cause damage to\nobjects. To address this limitation, we present a hybrid gripper finger that\ncombines a rigid structural shell with a soft, inflatable silicone pocket. The\ngripper finger can actively modulate its surface friction by controlling the\ninternal air pressure of the silicone pocket. Results from fundamental\nexperiments indicate that increasing the internal pressure results in a\nproportional increase in the effective coefficient of friction. This enables\nthe gripper to stably lift heavy and slippery objects without increasing the\ngripping force and to handle fragile or deformable objects, such as eggs,\nfruits, and paper cups, with minimal damage by increasing friction rather than\napplying excessive force. The experimental results demonstrate that the hybrid\ngripper finger with adaptable friction provides a robust and safer alternative\nto relying solely on high normal forces, thereby enhancing the gripper\nflexibility in handling delicate, fragile, and diverse objects.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u6293\u624b\u624b\u6307\uff0c\u4f7f\u7528\u6c14\u56ca\u8c03\u8282\u6469\u64e6\u529b\uff0c\u589e\u5f3a\u6293\u53d6\u91cd\u3001\u6ed1\u53ca\u6613\u788e\u7269\u4f53\u7684\u80fd\u529b\uff0c\u65e0\u9700\u65bd\u52a0\u8fc7\u5927\u7684\u529b\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6293\u624b\u5728\u5904\u7406\u91cd\u3001\u6ed1\u3001\u6613\u788e\u7269\u4f53\u65f6\u6240\u9762\u4e34\u7684\u6311\u6218\uff0c\u907f\u514d\u56e0\u65bd\u52a0\u9ad8\u6b63\u5e38\u529b\u5bfc\u81f4\u7684\u7269\u4f53\u635f\u574f\u3002", "method": "\u901a\u8fc7\u4f7f\u7528\u521a\u6027\u7ed3\u6784\u5916\u58f3\u4e0e\u67d4\u6027\u6c14\u56ca\u7ed3\u5408\u7684\u6293\u624b\u8bbe\u8ba1\uff0c\u63a7\u5236\u5185\u90e8\u7a7a\u6c14\u538b\u529b\u6765\u8c03\u8282\u8868\u9762\u6469\u64e6\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5185\u90e8\u538b\u529b\u7684\u589e\u52a0\u4e0e\u6709\u6548\u6469\u64e6\u7cfb\u6570\u7684\u63d0\u9ad8\u6210\u6b63\u6bd4\uff0c\u4f7f\u5f97\u6293\u624b\u80fd\u591f\u5b89\u5168\u7a33\u5b9a\u5730\u6293\u53d6\u5404\u79cd\u7269\u4f53\u3002", "conclusion": "\u8be5\u6df7\u5408\u6293\u624b\u624b\u6307\u63d0\u4f9b\u4e86\u4e00\u79cd\u589e\u5f3a\u7075\u6d3b\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4ee5\u5b89\u5168\u5730\u5904\u7406\u5404\u79cd\u4e0d\u540c\u6027\u8d28\u7684\u7269\u4f53\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u4e8e\u9ad8\u7684\u6b63\u5e38\u529b\u91cf\u3002"}}
{"id": "2510.27191", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.27191", "abs": "https://arxiv.org/abs/2510.27191", "authors": ["Marcus Hoerger", "Muhammad Sudrajat", "Hanna Kurniawati"], "title": "Vectorized Online POMDP Planning", "comment": "8 pages, 3 figures. Submitted to ICRA 2026", "summary": "Planning under partial observability is an essential capability of autonomous\nrobots. The Partially Observable Markov Decision Process (POMDP) provides a\npowerful framework for planning under partial observability problems, capturing\nthe stochastic effects of actions and the limited information available through\nnoisy observations. POMDP solving could benefit tremendously from massive\nparallelization of today's hardware, but parallelizing POMDP solvers has been\nchallenging. They rely on interleaving numerical optimization over actions with\nthe estimation of their values, which creates dependencies and synchronization\nbottlenecks between parallel processes that can quickly offset the benefits of\nparallelization. In this paper, we propose Vectorized Online POMDP Planner\n(VOPP), a novel parallel online solver that leverages a recent POMDP\nformulation that analytically solves part of the optimization component,\nleaving only the estimation of expectations for numerical computation. VOPP\nrepresents all data structures related to planning as a collection of tensors\nand implements all planning steps as fully vectorized computations over this\nrepresentation. The result is a massively parallel solver with no dependencies\nand synchronization bottlenecks between parallel computations. Experimental\nresults indicate that VOPP is at least 20X more efficient in computing\nnear-optimal solutions compared to an existing state-of-the-art parallel online\nsolver.", "AI": {"tldr": "VOPP\u662f\u4e00\u4e2a\u65b0\u578b\u7684\u5e76\u884c\u5728\u7ebfPOMDP\u6c42\u89e3\u5668\uff0c\u901a\u8fc7\u77e2\u91cf\u5316\u8ba1\u7b97\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u5e76\u663e\u8457\u8d85\u8d8a\u4e86\u4f20\u7edf\u6c42\u89e3\u5668\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u7684\u73af\u5883\u4e2d\uff0c\u89c4\u5212\u5bf9\u4e8e\u81ea\u4e3b\u673a\u5668\u4eba\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7684POMDP\u6c42\u89e3\u5668\u5728\u5e76\u884c\u5316\u65f6\u9762\u4e34\u4f9d\u8d56\u6027\u548c\u540c\u6b65\u74f6\u9888\u7684\u95ee\u9898\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u77e2\u91cf\u5316\u7684\u5728\u7ebfPOMDP\u89c4\u5212\u5668\uff08VOPP\uff09\uff0c\u901a\u8fc7\u6700\u8fd1\u7684POMDP\u516c\u5f0f\u5206\u6790\u89e3\u51b3\u90e8\u5206\u4f18\u5316\u7ec4\u4ef6\uff0c\u53ea\u4fdd\u7559\u671f\u671b\u503c\u7684\u6570\u503c\u8ba1\u7b97\uff0c\u5e76\u5c06\u6240\u6709\u89c4\u5212\u76f8\u5173\u7684\u6570\u636e\u7ed3\u6784\u8868\u793a\u4e3a\u5f20\u91cf\u96c6\u5408\uff0c\u5b9e\u73b0\u5168\u77e2\u91cf\u5316\u8ba1\u7b97\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u5e76\u884c\u5728\u7ebf\u6c42\u89e3\u5668\u76f8\u6bd4\uff0cVOPP\u5728\u8ba1\u7b97\u8fd1\u4f3c\u6700\u4f18\u89e3\u7684\u6548\u7387\u4e0a\u81f3\u5c11\u63d0\u9ad8\u4e8620\u500d\u3002", "conclusion": "VOPP\u662f\u4e00\u79cd\u65b0\u578b\u7684\u5e76\u884c\u5728\u7ebfPOMDP\u6c42\u89e3\u5668\uff0c\u80fd\u591f\u9ad8\u6548\u8ba1\u7b97\u8fd1\u4f3c\u6700\u4f18\u89e3\uff0c\u5e76\u4e14\u5728\u5e76\u884c\u8ba1\u7b97\u4e2d\u63d0\u5347\u4e86\u6548\u7387\uff0c\u6ca1\u6709\u4f9d\u8d56\u548c\u540c\u6b65\u74f6\u9888\u3002"}}
{"id": "2510.27327", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.27327", "abs": "https://arxiv.org/abs/2510.27327", "authors": ["Robert Pommeranz", "Kevin Tebbe", "Ralf Heynicke", "Gerd Scholl"], "title": "A Modular and Scalable System Architecture for Heterogeneous UAV Swarms Using ROS 2 and PX4-Autopilot", "comment": null, "summary": "In this paper a modular and scalable architecture for heterogeneous\nswarm-based Counter Unmanned Aerial Systems (C-UASs) built on PX4-Autopilot and\nRobot Operating System 2 (ROS 2) framework is presented. The proposed\narchitecture emphasizes seamless integration of hardware components by\nintroducing independent ROS 2 nodes for each component of a Unmanned Aerial\nVehicle (UAV). Communication between swarm participants is abstracted in\nsoftware, allowing the use of various technologies without architectural\nchanges. Key functionalities are supported, e.g. leader following and formation\nflight to maneuver the swarm. The system also allows computer vision algorithms\nto be integrated for the detection and tracking of UAVs. Additionally, a ground\nstation control is integrated for the coordination of swarm operations.\nSwarm-based Unmanned Aerial System (UAS) architecture is verified within a\nGazebo simulation environment but also in real-world demonstrations.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePX4-Autopilot\u548cROS 2\u6846\u67b6\u7684\u6a21\u5757\u5316\u5f02\u6784\u53cd\u65e0\u4eba\u673a\u7cfb\u7edf\u67b6\u6784\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u7fa4\u4f53\u534f\u8c03\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u96c6\u6210\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u9762\u5bf9\u53cd\u65e0\u4eba\u673a\u9700\u6c42\uff0c\u8bbe\u8ba1\u4e00\u79cd\u53ef\u4ee5\u6709\u6548\u534f\u8c03\u548c\u7ba1\u7406\u591a\u67b6\u65e0\u4eba\u673a\u7684\u7cfb\u7edf\u3002", "method": "\u91c7\u7528PX4-Autopilot\u548cROS 2\u6846\u67b6\uff0c\u5efa\u7acb\u6a21\u5757\u5316\u548c\u53ef\u6269\u5c55\u7684\u5f02\u6784\u7fa4\u4f53\u53cd\u65e0\u4eba\u673a\u7cfb\u7edf\u67b6\u6784\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u6d4b\u9a8c\u8bc1\u4e86\u7fa4\u4f53\u65e0\u4eba\u673a\u7cfb\u7edf\u67b6\u6784\u7684\u6838\u5fc3\u529f\u80fd\uff0c\u5305\u62ec\u9886\u5bfc\u8ddf\u968f\u3001\u7f16\u961f\u98de\u884c\u53ca\u96c6\u6210\u8ba1\u7b97\u673a\u89c6\u89c9\u7b97\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u67b6\u6784\u5728Gazebo\u6a21\u62df\u73af\u5883\u548c\u5b9e\u9645\u6f14\u793a\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u4e0e\u53ef\u884c\u6027\u3002"}}
{"id": "2510.27333", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.27333", "abs": "https://arxiv.org/abs/2510.27333", "authors": ["Hao Cheng", "Yanbo Jiang", "Qingyuan Shi", "Qingwen Meng", "Keyu Chen", "Wenhao Yu", "Jianqiang Wang", "Sifa Zheng"], "title": "Modified-Emergency Index (MEI): A Criticality Metric for Autonomous Driving in Lateral Conflict", "comment": null, "summary": "Effective, reliable, and efficient evaluation of autonomous driving safety is\nessential to demonstrate its trustworthiness. Criticality metrics provide an\nobjective means of assessing safety. However, as existing metrics primarily\ntarget longitudinal conflicts, accurately quantifying the risks of lateral\nconflicts - prevalent in urban settings - remains challenging. This paper\nproposes the Modified-Emergency Index (MEI), a metric designed to quantify\nevasive effort in lateral conflicts. Compared to the original Emergency Index\n(EI), MEI refines the estimation of the time available for evasive maneuvers,\nenabling more precise risk quantification. We validate MEI on a public lateral\nconflict dataset based on Argoverse-2, from which we extract over 1,500\nhigh-quality AV conflict cases, including more than 500 critical events. MEI is\nthen compared with the well-established ACT and the widely used PET metrics.\nResults show that MEI consistently outperforms them in accurately quantifying\ncriticality and capturing risk evolution. Overall, these findings highlight MEI\nas a promising metric for evaluating urban conflicts and enhancing the safety\nassessment framework for autonomous driving. The open-source implementation is\navailable at https://github.com/AutoChengh/MEI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMEI\uff0c\u7528\u4e8e\u6539\u8fdb\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u8bc4\u4f30\uff0c\u7279\u522b\u662f\u5728\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u6a2a\u5411\u51b2\u7a81\u3002", "motivation": "\u73b0\u6709\u7684\u5b89\u5168\u8bc4\u4f30\u6307\u6807\u4e3b\u8981\u5173\u6ce8\u7eb5\u5411\u51b2\u7a81\uff0c\u96be\u4ee5\u51c6\u786e\u91cf\u5316\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u6a2a\u5411\u51b2\u7a81\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u4e86\u4fee\u6539\u540e\u7684\u7d27\u6025\u6307\u6807\uff08MEI\uff09\uff0c\u7528\u4e8e\u91cf\u5316\u6a2a\u5411\u51b2\u7a81\u4e2d\u7684\u56de\u907f\u52aa\u529b\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u9a8c\u8bc1\u3002", "result": "MEI\u5728\u51c6\u786e\u91cf\u5316\u91cd\u8981\u6027\u548c\u6355\u6349\u98ce\u9669\u6f14\u53d8\u65b9\u9762\u7a33\u6b65\u8d85\u8d8a\u4e86ACT\u548cPET\u6307\u6807\u3002", "conclusion": "MEI\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u91cf\u5316\u57ce\u5e02\u51b2\u7a81\u7684\u91cd\u8981\u6027\u6307\u6807\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2510.27420", "categories": ["cs.RO", "I.2.9"], "pdf": "https://arxiv.org/pdf/2510.27420", "abs": "https://arxiv.org/abs/2510.27420", "authors": ["Roman Freiberg", "Alexander Qualmann", "Ngo Anh Vien", "Gerhard Neumann"], "title": "Towards a Multi-Embodied Grasping Agent", "comment": "9 pages, 3 figures", "summary": "Multi-embodiment grasping focuses on developing approaches that exhibit\ngeneralist behavior across diverse gripper designs. Existing methods often\nlearn the kinematic structure of the robot implicitly and face challenges due\nto the difficulty of sourcing the required large-scale data. In this work, we\npresent a data-efficient, flow-based, equivariant grasp synthesis architecture\nthat can handle different gripper types with variable degrees of freedom and\nsuccessfully exploit the underlying kinematic model, deducing all necessary\ninformation solely from the gripper and scene geometry. Unlike previous\nequivariant grasping methods, we translated all modules from the ground up to\nJAX and provide a model with batching capabilities over scenes, grippers, and\ngrasps, resulting in smoother learning, improved performance and faster\ninference time. Our dataset encompasses grippers ranging from humanoid hands to\nparallel yaw grippers and includes 25,000 scenes and 20 million grasps.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u7684\u591a\u5f0f\u6837\u6293\u53d6\u65b9\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u4e0d\u540c\u81ea\u7531\u5ea6\u7684\u5939\u5177\uff0c\u5e76\u901a\u8fc7JAX\u5e73\u53f0\u6539\u8fdb\u5b66\u4e60\u548c\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u5f0f\u6837\u6293\u53d6\u65b9\u6cd5\u5728\u5e94\u5bf9\u591a\u6837\u7684\u5939\u5177\u8bbe\u8ba1\u65f6\uff0c\u9762\u4e34\u6570\u636e\u91cf\u4e0d\u8db3\u53ca\u8fd0\u52a8\u5b66\u7ed3\u6784\u5b66\u4e60\u56f0\u96be\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u6d41\u5f0f\u3001\u7b49\u53d8\u7684\u6293\u53d6\u5408\u6210\u67b6\u6784\uff0c\u5c06\u5404\u6a21\u5757\u8fc1\u79fb\u5230JAX\u5e73\u53f0\u4e0a\uff0c\u652f\u6301\u9488\u5bf9\u573a\u666f\u3001\u5939\u5177\u548c\u6293\u53d6\u7684\u6279\u5904\u7406\u80fd\u529b\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u79cd\u80fd\u591f\u5904\u7406\u591a\u79cd\u5939\u5177\u7684\u6293\u53d6\u67b6\u6784\uff0c\u4f7f\u752825,000\u4e2a\u573a\u666f\u548c20\u767e\u4e07\u4e2a\u6293\u53d6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b66\u4e60\u6548\u7387\u548c\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u3001\u6d41\u5f0f\u7684\u7b49\u53d8\u6293\u53d6\u5408\u6210\u67b6\u6784\uff0c\u80fd\u591f\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u548c\u81ea\u7531\u5ea6\u7684\u5939\u5177\uff0c\u5145\u5206\u5229\u7528\u6f5c\u5728\u7684\u8fd0\u52a8\u5b66\u6a21\u578b\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6293\u53d6\u6027\u80fd\u548c\u63a8\u7406\u901f\u5ea6\u3002"}}
{"id": "2510.27428", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.27428", "abs": "https://arxiv.org/abs/2510.27428", "authors": ["Hehui Zheng", "Bhavya Sukhija", "Chenhao Li", "Klemens Iten", "Andreas Krause", "Robert K. Katzschmann"], "title": "Learning Soft Robotic Dynamics with Active Exploration", "comment": null, "summary": "Soft robots offer unmatched adaptability and safety in unstructured\nenvironments, yet their compliant, high-dimensional, and nonlinear dynamics\nmake modeling for control notoriously difficult. Existing data-driven\napproaches often fail to generalize, constrained by narrowly focused task\ndemonstrations or inefficient random exploration. We introduce SoftAE, an\nuncertainty-aware active exploration framework that autonomously learns\ntask-agnostic and generalizable dynamics models of soft robotic systems. SoftAE\nemploys probabilistic ensemble models to estimate epistemic uncertainty and\nactively guides exploration toward underrepresented regions of the state-action\nspace, achieving efficient coverage of diverse behaviors without task-specific\nsupervision. We evaluate SoftAE on three simulated soft robotic platforms -- a\ncontinuum arm, an articulated fish in fluid, and a musculoskeletal leg with\nhybrid actuation -- and on a pneumatically actuated continuum soft arm in the\nreal world. Compared with random exploration and task-specific model-based\nreinforcement learning, SoftAE produces more accurate dynamics models, enables\nsuperior zero-shot control on unseen tasks, and maintains robustness under\nsensing noise, actuation delays, and nonlinear material effects. These results\ndemonstrate that uncertainty-driven active exploration can yield scalable,\nreusable dynamics models across diverse soft robotic morphologies, representing\na step toward more autonomous, adaptable, and data-efficient control in\ncompliant robots.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e3b\u52a8\u63a2\u7d22\u6846\u67b6SoftAE\uff0c\u65e8\u5728\u63d0\u9ad8\u8f6f\u673a\u5668\u4eba\u7684\u52a8\u6001\u5efa\u6a21\u80fd\u529b\uff0c\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u3001\u901a\u7528\u7684\u63a7\u5236\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u5e94\u5bf9\u8f6f\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u975e\u7ebf\u6027\u52a8\u6001\u5efa\u6a21\u65f6\uff0c\u56e0\u4efb\u52a1\u6f14\u793a\u8303\u56f4\u72ed\u7a84\u6216\u968f\u673a\u63a2\u7d22\u4f4e\u6548\u800c\u96be\u4ee5\u63a8\u5e7f\u3002", "method": "SoftAE\u91c7\u7528\u6982\u7387\u96c6\u6210\u6a21\u578b\u6765\u4f30\u8ba1\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3b\u52a8\u5f15\u5bfc\u63a2\u7d22\u672a\u5145\u5206\u4ee3\u8868\u7684\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u533a\u57df\u3002", "result": "SoftAE\u5728\u4e09\u79cd\u6a21\u62df\u8f6f\u673a\u5668\u4eba\u5e73\u53f0\uff08\u5982\u8fde\u7eed\u81c2\u3001\u6d41\u4f53\u4e2d\u7684\u5173\u8282\u9c7c\u548c\u6df7\u5408\u9a71\u52a8\u7684\u808c\u8089\u9aa8\u9abc\u817f\uff09\u4ee5\u53ca\u771f\u5b9e\u4e16\u754c\u7684\u6c14\u52a8\u9a71\u52a8\u8fde\u7eed\u8f6f\u81c2\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u76f8\u6bd4\u968f\u673a\u63a2\u7d22\u548c\u4efb\u52a1\u7279\u5b9a\u7684\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\uff0cSoftAE\u80fd\u4ea7\u751f\u66f4\u51c6\u786e\u7684\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u5e76\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u5b9e\u73b0\u66f4\u4f18\u7684\u96f6-shot\u63a7\u5236\uff0c\u540c\u65f6\u5728\u611f\u77e5\u566a\u58f0\u3001\u9a71\u52a8\u5ef6\u8fdf\u548c\u975e\u7ebf\u6027\u6750\u6599\u6548\u5e94\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u7684\u4e3b\u52a8\u63a2\u7d22\u6846\u67b6SoftAE\u80fd\u6709\u6548\u5730\u5b66\u4e60\u901a\u7528\u4e14\u53ef\u518d\u5229\u7528\u7684\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u4e3a\u8f6f\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u5411\u81ea\u4e3b\u3001\u9002\u5e94\u6027\u5f3a\u548c\u6570\u636e\u6548\u7387\u9ad8\u7684\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2510.27545", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.27545", "abs": "https://arxiv.org/abs/2510.27545", "authors": ["Travis Davies", "Yiqi Huang", "Alexi Gladstone", "Yunxin Liu", "Xiang Chen", "Heng Ji", "Huxian Liu", "Luhui Hu"], "title": "EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities", "comment": "9 pages, 6 figures, 4 tables", "summary": "Implicit policies parameterized by generative models, such as Diffusion\nPolicy, have become the standard for policy learning and Vision-Language-Action\n(VLA) models in robotics. However, these approaches often suffer from high\ncomputational cost, exposure bias, and unstable inference dynamics, which lead\nto divergence under distribution shifts. Energy-Based Models (EBMs) address\nthese issues by learning energy landscapes end-to-end and modeling equilibrium\ndynamics, offering improved robustness and reduced exposure bias. Yet, policies\nparameterized by EBMs have historically struggled to scale effectively. Recent\nwork on Energy-Based Transformers (EBTs) demonstrates the scalability of EBMs\nto high-dimensional spaces, but their potential for solving core challenges in\nphysically embodied models remains underexplored. We introduce a new\nenergy-based architecture, EBT-Policy, that solves core issues in robotic and\nreal-world settings. Across simulated and real-world tasks, EBT-Policy\nconsistently outperforms diffusion-based policies, while requiring less\ntraining and inference computation. Remarkably, on some tasks it converges\nwithin just two inference steps, a 50x reduction compared to Diffusion Policy's\n100. Moreover, EBT-Policy exhibits emergent capabilities not seen in prior\nmodels, such as zero-shot recovery from failed action sequences using only\nbehavior cloning and without explicit retry training. By leveraging its scalar\nenergy for uncertainty-aware inference and dynamic compute allocation,\nEBT-Policy offers a promising path toward robust, generalizable robot behavior\nunder distribution shifts.", "AI": {"tldr": "EBT-Policy\u901a\u8fc7\u65b0\u67b6\u6784\u8d85\u8d8a\u6269\u6563\u653f\u7b56\uff0c\u51cf\u5c11\u8ba1\u7b97\u9700\u6c42\u5e76\u5c55\u73b0\u51fa\u65b0\u7684\u80fd\u529b\uff0c\u5411\u589e\u5f3a\u673a\u5668\u4eba\u884c\u4e3a\u7684\u9c81\u68d2\u6027\u8fc8\u51fa\u91cd\u8981\u4e00\u6b65\u3002", "motivation": "\u73b0\u6709\u7684\u9690\u5f0f\u7b56\u7565\u7531\u4e8e\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u66b4\u9732\u504f\u5dee\u548c\u4e0d\u7a33\u5b9a\u63a8\u65ad\u52a8\u6001\uff0c\u5728\u5206\u5e03\u53d8\u5316\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u7b56\u7565\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3aEBT-Policy\u7684\u80fd\u91cf\u57fa\u7840\u67b6\u6784\uff0c\u89e3\u51b3\u673a\u5668\u4eba\u548c\u73b0\u5b9e\u4e16\u754c\u8bbe\u7f6e\u4e2d\u7684\u6838\u5fc3\u95ee\u9898\u3002", "result": "EBT-Policy\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u7a33\u5b9a\u4f18\u4e8e\u6269\u6563\u57fa\u7840\u653f\u7b56\uff0c\u5e76\u5728\u67d0\u4e9b\u4efb\u52a1\u4e2d\u5728\u4ec5\u4e24\u6b21\u63a8\u65ad\u6b65\u9aa4\u5185\u6536\u655b\u3002", "conclusion": "EBT-Policy\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u548c\u63a8\u7406\u8ba1\u7b97\uff0c\u5c55\u73b0\u51fa\u72ec\u7279\u80fd\u529b\uff0c\u63d0\u4f9b\u4e86\u5bf9\u6297\u5206\u5e03\u504f\u79fb\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.27558", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.27558", "abs": "https://arxiv.org/abs/2510.27558", "authors": ["Sushil Samuel Dinesh", "Shinkyu Park"], "title": "Toward Accurate Long-Horizon Robotic Manipulation: Language-to-Action with Foundation Models via Scene Graphs", "comment": null, "summary": "This paper presents a framework that leverages pre-trained foundation models\nfor robotic manipulation without domain-specific training. The framework\nintegrates off-the-shelf models, combining multimodal perception from\nfoundation models with a general-purpose reasoning model capable of robust task\nsequencing. Scene graphs, dynamically maintained within the framework, provide\nspatial awareness and enable consistent reasoning about the environment. The\nframework is evaluated through a series of tabletop robotic manipulation\nexperiments, and the results highlight its potential for building robotic\nmanipulation systems directly on top of off-the-shelf foundation models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u53ef\u4ee5\u5728\u65e0\u9700\u7279\u5b9a\u9886\u57df\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u5c55\u793a\u4e86\u5176\u5728\u673a\u5668\u4eba\u7cfb\u7edf\u5f00\u53d1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u76ee\u6807\u662f\u63d0\u9ad8\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387\uff0c\u51cf\u5c11\u5bf9\u7279\u5b9a\u9886\u57df\u8bad\u7ec3\u7684\u4f9d\u8d56\u3002", "method": "\u901a\u8fc7\u6574\u5408\u73b0\u6210\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u611f\u77e5\u548c\u901a\u7528\u63a8\u7406\u6a21\u578b\uff0c\u52a8\u6001\u7ef4\u62a4\u573a\u666f\u56fe\u4ee5\u5b9e\u73b0\u7a7a\u95f4\u610f\u8bc6\u548c\u73af\u5883\u63a8\u7406\u3002", "result": "\u901a\u8fc7\u4e00\u7cfb\u5217\u684c\u9762\u673a\u5668\u4eba\u64cd\u4f5c\u5b9e\u9a8c\u8bc4\u4f30\u6846\u67b6\uff0c\u7ed3\u679c\u663e\u793a\u5176\u80fd\u591f\u76f4\u63a5\u57fa\u4e8e\u73b0\u6210\u57fa\u7840\u6a21\u578b\u6784\u5efa\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u6709\u6548\u5229\u7528\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u4e14\u65e0\u9700\u7279\u5b9a\u9886\u57df\u7684\u8bad\u7ec3\uff0c\u8fd9\u663e\u793a\u4e86\u5176\u5728\u673a\u5668\u4eba\u7cfb\u7edf\u5f00\u53d1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.27666", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.27666", "abs": "https://arxiv.org/abs/2510.27666", "authors": ["Dong Heon Han", "Xiaohao Xu", "Yuxi Chen", "Yusheng Zhou", "Xinqi Zhang", "Jiaqi Wang", "Daniel Bruder", "Xiaonan Huang"], "title": "Whole-Body Proprioceptive Morphing: A Modular Soft Gripper for Robust Cross-Scale Grasping", "comment": null, "summary": "Biological systems, such as the octopus, exhibit masterful cross-scale\nmanipulation by adaptively reconfiguring their entire form, a capability that\nremains elusive in robotics. Conventional soft grippers, while compliant, are\nmostly constrained by a fixed global morphology, and prior shape-morphing\nefforts have been largely confined to localized deformations, failing to\nreplicate this biological dexterity. Inspired by this natural exemplar, we\nintroduce the paradigm of collaborative, whole-body proprioceptive morphing,\nrealized in a modular soft gripper architecture. Our design is a distributed\nnetwork of modular self-sensing pneumatic actuators that enables the gripper to\nintelligently reconfigure its entire topology, achieving multiple morphing\nstates that are controllable to form diverse polygonal shapes. By integrating\nrich proprioceptive feedback from embedded sensors, our system can seamlessly\ntransition from a precise pinch to a large envelope grasp. We experimentally\ndemonstrate that this approach expands the grasping envelope and enhances\ngeneralization across diverse object geometries (standard and irregular) and\nscales (up to 10$\\times$), while also unlocking novel manipulation modalities\nsuch as multi-object and internal hook grasping. This work presents a low-cost,\neasy-to-fabricate, and scalable framework that fuses distributed actuation with\nintegrated sensing, offering a new pathway toward achieving biological levels\nof dexterity in robotic manipulation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u6613\u4e8e\u5236\u9020\u7684\u6a21\u5757\u5316\u8f6f\u6293\u624b\uff0c\u901a\u8fc7\u6574\u5408\u81ea\u611f\u5e94\u53cd\u9988\uff0c\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u5728\u6293\u53d6\u8fc7\u7a0b\u4e2d\u751f\u7269\u7ea7\u522b\u7684\u7075\u6d3b\u6027\u3002", "motivation": "\u53d7\u5230\u7ae0\u9c7c\u7b49\u751f\u7269\u7cfb\u7edf\u8de8\u5c3a\u5ea6\u64cd\u4f5c\u80fd\u529b\u7684\u542f\u53d1\uff0c\u7814\u7a76\u65e8\u5728\u514b\u670d\u4f20\u7edf\u8f6f\u6293\u624b\u5728\u5f62\u6001\u4e0a\u7684\u9650\u5236\uff0c\u5bfb\u6c42\u66f4\u7075\u6d3b\u7684\u6293\u53d6\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u7f51\u7edc\u6a21\u5757\u5316\u81ea\u611f\u5e94\u6c14\u52a8\u6267\u884c\u5668\uff0c\u4f7f\u6293\u624b\u80fd\u591f\u667a\u80fd\u5730\u91cd\u65b0\u914d\u7f6e\u5176\u6574\u4e2a\u62d3\u6251\u7ed3\u6784\uff0c\u63a7\u5236\u4e0d\u540c\u7684\u591a\u8fb9\u5f62\u5f62\u72b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6269\u5c55\u6293\u53d6\u5305\u5bb9\u6027\uff0c\u63d0\u9ad8\u5bf9\u5404\u79cd\u7269\u4f53\u51e0\u4f55\u5f62\u72b6\u548c\u89c4\u6a21\u7684\u9002\u5e94\u6027\uff0c\u5e76\u5b9e\u73b0\u65b0\u578b\u64cd\u4f5c\u6a21\u5f0f\uff0c\u5982\u591a\u7269\u4f53\u548c\u5185\u90e8\u94a9\u6293\u53d6\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u8f6f\u6293\u624b\u67b6\u6784\uff0c\u80fd\u591f\u5b9e\u73b0\u4e0e\u751f\u7269\u7cfb\u7edf\u7c7b\u4f3c\u7684\u7075\u6d3b\u6027\u548c\u591a\u6837\u6027\u7684\u6293\u53d6\u6a21\u5f0f\uff0c\u6269\u5c55\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u80fd\u529b\u3002"}}
