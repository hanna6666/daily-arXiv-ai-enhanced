{"id": "2510.21715", "categories": ["cs.HC", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.21715", "abs": "https://arxiv.org/abs/2510.21715", "authors": ["Sergio Rojas-Galeano"], "title": "Beyond IVR Touch-Tones: Customer Intent Routing using LLMs", "comment": "Accepted for publication in the Proceedings of the Workshop on\n  Engineering Applications 2025 (WEA 2025)", "summary": "Widespread frustration with rigid touch-tone Interactive Voice Response (IVR)\nsystems for customer service underscores the need for more direct and intuitive\nlanguage interaction. While speech technologies are necessary, the key\nchallenge lies in routing intents from user phrasings to IVR menu paths, a task\nwhere Large Language Models (LLMs) show strong potential. Progress, however, is\nlimited by data scarcity, as real IVR structures and interactions are often\nproprietary. We present a novel LLM-based methodology to address this gap.\nUsing three distinct models, we synthesized a realistic 23-node IVR structure,\ngenerated 920 user intents (230 base and 690 augmented), and performed the\nrouting task. We evaluate two prompt designs: descriptive hierarchical menus\nand flattened path representations, across both base and augmented datasets.\nResults show that flattened paths consistently yield higher accuracy, reaching\n89.13% on the base dataset compared to 81.30% with the descriptive format,\nwhile augmentation introduces linguistic noise that slightly reduces\nperformance. Confusion matrix analysis further suggests that low-performing\nroutes may reflect not only model limitations but also redundancies in menu\ndesign. Overall, our findings demonstrate proof-of-concept that LLMs can enable\nIVR routing through a smoother, more seamless user experience -- moving\ncustomer service one step ahead of touch-tone menus.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684IVR\u8def\u7531\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u73b0\u66f4\u987a\u7545\u7684\u7528\u6237\u4f53\u9a8c\u65b9\u9762\u7684\u6f5c\u529b", "motivation": "\u5ba2\u6237\u670d\u52a1\u4e2d\u5bf9\u4f20\u7edf\u89e6\u63a7\u97f3\u9891\u54cd\u5e94\u7cfb\u7edf\u7684\u632b\u8d25\u611f\u5f3a\u8c03\u4e86\u9700\u8981\u66f4\u76f4\u63a5\u548c\u76f4\u89c2\u7684\u8bed\u8a00\u4ea4\u4e92", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684IVR\u8def\u7531\u65b9\u6cd5", "result": "\u901a\u8fc7\u4e09\u79cd\u4e0d\u540c\u6a21\u578b\u5408\u6210\u4e86\u4e00\u4e2a23\u8282\u70b9\u7684IVR\u7ed3\u6784\uff0c\u751f\u6210\u4e86920\u4e2a\u7528\u6237\u610f\u56fe\uff0c\u5e76\u5b8c\u6210\u4e86\u8def\u7531\u4efb\u52a1", "conclusion": "\u6211\u4eec\u7684\u7ed3\u679c\u8bc1\u660e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u66f4\u6d41\u7545\u7684\u7528\u6237\u4f53\u9a8c\u6765\u63a8\u52a8\u5ba2\u6237\u670d\u52a1\u7684\u53d1\u5c55\uff0c\u8d85\u8d8a\u4f20\u7edf\u7684\u89e6\u63a7\u83dc\u5355"}}
{"id": "2510.21716", "categories": ["cs.HC", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21716", "abs": "https://arxiv.org/abs/2510.21716", "authors": ["Nicola Webb", "Zijun Huang", "Sanja Milivojevic", "Chris Baber", "Edmund R. Hunt"], "title": "When Robots Say No: Temporal Trust Recovery Through Explanation", "comment": null, "summary": "Mobile robots with some degree of autonomy could deliver significant\nadvantages in high-risk missions such as search and rescue and firefighting.\nIntegrated into a human-robot team (HRT), robots could work effectively to help\nsearch hazardous buildings. User trust is a key enabler for HRT, but during a\nmission, trust can be damaged. With distributed situation awareness, such as\nwhen team members are working in different locations, users may be inclined to\ndoubt a robot's integrity if it declines to immediately change its priorities\non request. In this paper, we present the results of a computer-based study\ninvestigating on-mission trust dynamics in a high-stakes human-robot teaming\nscenario. Participants (n = 38) played an interactive firefighting game\nalongside a robot teammate, where a trust violation occurs owing to the robot\ndeclining to help the user immediately. We find that when the robot provides an\nexplanation for declining to help, trust better recovers over time, albeit\nfollowing an initial drop that is comparable to a baseline condition where an\nexplanation for refusal is not provided. Our findings indicate that trust can\nvary significantly during a mission, notably when robots do not immediately\nrespond to user requests, but that this trust violation can be largely\nameliorated over time if adequate explanation is provided.", "AI": {"tldr": "\u5728\u9ad8\u98ce\u9669\u4eba\u673a\u5408\u4f5c\u4efb\u52a1\u4e2d\uff0c\u673a\u5668\u4eba\u7684\u4fe1\u4efb\u52a8\u6001\u53d7\u5176\u54cd\u5e94\u884c\u4e3a\u548c\u89e3\u91ca\u7684\u5f71\u54cd\u3002", "motivation": "\u63a2\u8ba8\u5728\u4eba\u673a\u56e2\u961f\u4e2d\uff0c\u5982\u4f55\u5904\u7406\u673a\u5668\u4eba\u62d2\u7edd\u7acb\u5373\u54cd\u5e94\u7528\u6237\u8bf7\u6c42\u800c\u4ea7\u751f\u7684\u4fe1\u4efb\u95ee\u9898\u3002", "method": "\u53c2\u4e0e\u8005\u5728\u4e00\u4e2a\u4e92\u52a8\u6d88\u9632\u6e38\u620f\u4e2d\u4e0e\u673a\u5668\u4eba\u961f\u53cb\u5171\u540c\u53c2\u4e0e\uff0c\u7814\u7a76\u4fe1\u4efb\u52a8\u6001\u3002", "result": "\u5f53\u673a\u5668\u4eba\u63d0\u4f9b\u62d2\u7edd\u5e2e\u52a9\u7684\u89e3\u91ca\u65f6\uff0c\u4fe1\u4efb\u80fd\u591f\u968f\u7740\u65f6\u95f4\u6062\u590d\uff0c\u5c3d\u7ba1\u521d\u59cb\u4fe1\u4efb\u4e0b\u964d\u4e0e\u6ca1\u6709\u89e3\u91ca\u7684\u60c5\u51b5\u76f8\u4f3c\u3002", "conclusion": "\u4fe1\u4efb\u5728\u4efb\u52a1\u671f\u95f4\u53ef\u80fd\u663e\u8457\u6ce2\u52a8\uff0c\u4f46\u5982\u679c\u63d0\u4f9b\u5145\u8db3\u7684\u89e3\u91ca\uff0c\u53ef\u4ee5\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u7f13\u89e3\u4fe1\u4efb\u4fb5\u72af\u6240\u5e26\u6765\u7684\u8d1f\u9762\u5f71\u54cd\u3002"}}
{"id": "2510.21717", "categories": ["cs.HC", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.21717", "abs": "https://arxiv.org/abs/2510.21717", "authors": ["Bernard Tam", "Jean-Charles Tournier", "Fernando Varela Rodriguez"], "title": "AI-Enhanced Operator Assistance for UNICOS Applications", "comment": "Prepared as part of the CERN openlab programme 2025. Also available\n  on Zenodo, a repository operated by CERN and co-funded by the European Union", "summary": "This project explores the development of an AI-enhanced operator assistant\nfor UNICOS, CERN's UNified Industrial Control System. While powerful, UNICOS\npresents a number of challenges, including the cognitive burden of decoding\nwidgets, manual effort required for root cause analysis, and difficulties\nmaintainers face in tracing datapoint elements (DPEs) across a complex\ncodebase. In situations where timely responses are critical, these challenges\ncan increase cognitive load and slow down diagnostics. To address these issues,\na multi-agent system was designed and implemented. The solution is supported by\na modular architecture comprising a UNICOS-side extension written in CTRL code,\na Python-based multi-agent system deployed on a virtual machine, and a vector\ndatabase storing both operator documentation and widget animation code.\nPreliminary evaluations suggest that the system is capable of decoding widgets,\nperforming root cause analysis by leveraging live device data and\ndocumentation, and tracing DPEs across a complex codebase. Together, these\ncapabilities reduce the manual workload of operators and maintainers, enhance\nsituational awareness in operations, and accelerate responses to alarms and\nanomalies. Beyond these immediate gains, this work highlights the potential of\nintroducing multi-modal reasoning and retrieval augmented generation (RAG) into\nthe domain of industrial control. Ultimately, this work represents more than a\nproof of concept: it provides a basis for advancing intelligent operator\ninterfaces at CERN. By combining modular design, extensibility, and practical\nAI integration, this project not only alleviates current operator pain points\nbut also points toward broader opportunities for assistive AI in accelerator\noperations.", "AI": {"tldr": "\u8be5\u9879\u76ee\u5f00\u53d1\u4e86\u4e00\u79cdAI\u589e\u5f3a\u578b\u64cd\u4f5c\u5458\u52a9\u624b\uff0c\u4ee5\u63d0\u9ad8CERN UNICOS\u7684\u64cd\u4f5c\u6548\u7387\uff0c\u5e76\u51cf\u8f7b\u64cd\u4f5c\u5458\u7684\u8ba4\u77e5\u8d1f\u62c5\u3002", "motivation": "\u65e8\u5728\u51cf\u5c11\u64cd\u4f5c\u5458\u5728\u9762\u5bf9\u590d\u6742\u4ee3\u7801\u57fa\u7840\u65f6\u7684\u8ba4\u77e5\u8d1f\u62c5\uff0c\u63d0\u9ad8\u54cd\u5e94\u901f\u5ea6\uff0c\u5e76\u901a\u8fc7\u591a\u6a21\u6001\u63a8\u7406\u548c\u589e\u5f3a\u751f\u6210\u6280\u672f\u63d0\u5347\u5de5\u4e1a\u63a7\u5236\u9886\u57df\u7684\u667a\u80fd\u5316\u6c34\u5e73\u3002", "method": "\u91c7\u7528\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u67b6\u6784\uff0c\u5305\u62ecUNICOS\u4fa7\u7684CTRL\u4ee3\u7801\u6269\u5c55\u3001\u57fa\u4e8ePython\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u548c\u4e00\u4e2a\u5411\u91cf\u6570\u636e\u5e93\u3002", "result": "\u8be5\u9879\u76ee\u63a2\u7d22\u4e86\u4e3aCERN\u7684\u7edf\u4e00\u5de5\u4e1a\u63a7\u5236\u7cfb\u7edfUNICOS\u5f00\u53d1AI\u589e\u5f3a\u578b\u64cd\u4f5c\u5458\u52a9\u624b\u3002\u8be5\u7cfb\u7edf\u65e8\u5728\u89e3\u51b3UNICOS\u7684\u64cd\u4f5c\u6311\u6218\uff0c\u63d0\u5347\u64cd\u4f5c\u5458\u548c\u7ef4\u62a4\u4eba\u5458\u7684\u5de5\u4f5c\u6548\u7387\u3002", "conclusion": "\u8be5\u9879\u76ee\u4e0d\u4ec5\u7f13\u89e3\u4e86\u73b0\u6709\u64cd\u4f5c\u8005\u5728UNICOS\u4e2d\u9762\u4e34\u7684\u6311\u6218\uff0c\u8fd8\u4e3a\u52a0\u901f\u5668\u64cd\u4f5c\u4e2d\u7684\u52a9\u7406AI\u63d0\u4f9b\u4e86\u66f4\u5e7f\u6cdb\u7684\u673a\u4f1a\u3002"}}
{"id": "2510.21718", "categories": ["cs.HC", "cs.CY", "K.3.1; I.2.m"], "pdf": "https://arxiv.org/pdf/2510.21718", "abs": "https://arxiv.org/abs/2510.21718", "authors": ["Ishaan Masilamony"], "title": "Exploring the Applications of Generative AI in High School STEM Education", "comment": null, "summary": "In recent years, ChatGPT \\cite{openai_2023_gpt4} along with Microsoft Copilot\nhave become subjects of great discourse, particularly in the field of\neducation. Prior research has hypothesized on potential impacts these tools\ncould have on student learning and performance. These have primarily relied on\ntrends from prior applications of technology in education and an understanding\nof the limitations and strengths of Generative AI in other applications. This\nstudy utilizes an experimental approach to analyze the impacts of Generative AI\non high school STEM education (physics in particular). In accordance with most\nfindings, generative AI does have some positive impact on student performance.\nHowever, our findings have shown that the most significant impact is an\nincrease in student engagement with the subject.", "AI": {"tldr": "\u7814\u7a76Generative AI\u5bf9\u9ad8\u4e2d\u7269\u7406\u6559\u80b2\u7684\u5f71\u54cd\uff0c\u7ed3\u679c\u8868\u660e\u8fd9\u79cd\u6280\u672f\u63d0\u5347\u4e86\u5b66\u751f\u53c2\u4e0e\u5ea6\u548c\u5b66\u4e60\u8868\u73b0\u3002", "motivation": "\u63a2\u8ba8Generative AI\uff08\u5982ChatGPT\u548cMicrosoft Copilot\uff09\u5728\u6559\u80b2\u4e2d\u7684\u6f5c\u5728\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5bf9\u5b66\u751f\u5b66\u4e60\u548c\u8868\u73b0\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u5b9e\u9a8c\u65b9\u6cd5\uff0c\u5206\u6790Generative AI\u5bf9\u9ad8\u4e2dSTEM\u6559\u80b2\u7684\u5f71\u54cd\u3002", "result": "Generative AI\u5bf9\u9ad8\u4e2dSTEM\u6559\u80b2\uff08\u5c24\u5176\u662f\u7269\u7406\uff09\u7684\u5f71\u54cd\u5206\u6790\uff0c\u53d1\u73b0\u5b83\u80fd\u63d0\u9ad8\u5b66\u751f\u7684\u53c2\u4e0e\u5ea6\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u80fd\u591f\u79ef\u6781\u5f71\u54cd\u5b66\u751f\u7684\u5b66\u4e60\uff0c\u7279\u522b\u662f\u5728\u63d0\u9ad8\u5b66\u751f\u5bf9\u7269\u7406\u7684\u5174\u8da3\u548c\u53c2\u4e0e\u5ea6\u65b9\u9762\u3002"}}
{"id": "2510.21732", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.21732", "abs": "https://arxiv.org/abs/2510.21732", "authors": ["Xumin Gao", "Mark Stevens", "Grzegorz Cielniak"], "title": "A Robotic Stirring Method with Trajectory Optimization and Adaptive Speed Control for Accurate Pest Counting in Water Traps", "comment": "This paper has been submitted to ICRA 2026 and is currently under\n  review", "summary": "Accurate monitoring of pest population dynamics is crucial for informed\ndecision-making in precision agriculture. Currently, mainstream image-based\npest counting methods primarily rely on image processing combined with machine\nlearning or deep learning for pest counting. However, these methods have\nlimitations and struggle to handle situations involving pest occlusion. To\naddress this issue, this paper proposed a robotic stirring method with\ntrajectory optimization and adaptive speed control for accurate pest counting\nin water traps. First, we developed an automated stirring system for pest\ncounting in yellow water traps based on a robotic arm. Stirring alters the\ndistribution of pests in the yellow water trap, making some of the occluded\nindividuals visible for detection and counting. Then, we investigated the\nimpact of different stirring trajectories on pest counting performance and\nselected the optimal trajectory for pest counting. Specifically, we designed\nsix representative stirring trajectories, including circle, square, triangle,\nspiral, four small circles, and random lines, for the robotic arm to stir. And\nby comparing the overall average counting error and counting confidence of\ndifferent stirring trajectories across various pest density scenarios, we\ndetermined the optimal trajectory. Finally, we proposed a counting\nconfidence-driven closed-loop control system to achieve adaptive-speed\nstirring. It uses changes in pest counting confidence between consecutive\nframes as feedback to adjust the stirring speed. To the best of our knowledge,\nthis is the first study dedicated to investigating the effects of different\nstirring trajectories on object counting in the dynamic liquid environment and\nto implement adaptive-speed stirring for this type of task. Experimental\nresults show ...", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u52a8\u6405\u62cc\u65b9\u6cd5\uff0c\u4ee5\u4f18\u5316\u866b\u5bb3\u8ba1\u6570\uff0c\u7279\u522b\u662f\u9488\u5bf9\u866b\u5bb3\u906e\u6321\u95ee\u9898\uff0c\u7ed3\u5408\u4e86\u8f68\u8ff9\u4f18\u5316\u548c\u81ea\u9002\u5e94\u901f\u5ea6\u63a7\u5236\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u56fe\u50cf\u5904\u7406\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u866b\u5bb3\u906e\u6321\u60c5\u51b5\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u866b\u5bb3\u8ba1\u6570\u7684\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u4eba\u81c2\u7684\u81ea\u52a8\u6405\u62cc\u7cfb\u7edf\uff0c\u7ed3\u5408\u8f68\u8ff9\u4f18\u5316\u548c\u81ea\u9002\u5e94\u901f\u5ea6\u63a7\u5236\u8fdb\u884c\u51c6\u786e\u7684\u866b\u5bb3\u8ba1\u6570\u3002", "result": "\u901a\u8fc7\u7814\u7a76\u4e0d\u540c\u6405\u62cc\u8f68\u8ff9\u5bf9\u866b\u5bb3\u8ba1\u6570\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u786e\u5b9a\u6700\u4f18\u6405\u62cc\u8f68\u8ff9\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u8ba1\u6570\u7f6e\u4fe1\u5ea6\u7684\u95ed\u73af\u63a7\u5236\u7cfb\u7edf\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u6405\u62cc\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u52a8\u6001\u6db2\u4f53\u73af\u5883\u4e2d\u866b\u5bb3\u8ba1\u6570\u7684\u7cbe\u786e\u5ea6\u3002"}}
{"id": "2510.21719", "categories": ["cs.HC", "cs.AI", "cs.CY", "K.4.0; K.3.1"], "pdf": "https://arxiv.org/pdf/2510.21719", "abs": "https://arxiv.org/abs/2510.21719", "authors": ["Kenji Saito", "Rei Tadika"], "title": "GAMER PAT: Research as a Serious Game", "comment": "14 pages, 2 figures", "summary": "As generative AI increasingly outperforms students in producing academic\nwriting, a critical question arises: how can we preserve the motivation,\ncreativity, and intellectual growth of novice researchers in an age of\nautomated academic achievement? This paper introduces GAMER PAT (GAme MastER,\nPaper Authoring Tutor), a prompt-engineered AI chatbot that reframes research\npaper writing as a serious game. Through role-playing mechanics, users interact\nwith a co-author NPC and anonymous reviewer NPCs, turning feedback into\n\"missions\" and advancing through a narrative-driven writing process.\n  Our study reports on 26+ gameplay chat logs, including both autoethnography\nand use by graduate students under supervision. Using qualitative log analysis\nwith SCAT (Steps for Coding and Theorization), we identified an emergent\nfour-phase scaffolding pattern: (1) question posing, (2) meta-perspective, (3)\nstructuring, and (4) recursive reflection. These results suggest that GAMER PAT\nsupports not only the structural development of research writing but also\nreflective and motivational aspects.\n  We present this work as a descriptive account of concept and process, not a\ncausal evaluation. We also include a speculative outlook envisioning how humans\nmay continue to cultivate curiosity and agency alongside AI-driven research.\nThis arXiv version thus provides both a descriptive report of design and usage,\nand a forward-looking provocation for future empirical studies.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aGAMER PAT\u7684AI\u804a\u5929\u673a\u5668\u4eba\uff0c\u5b83\u5c06\u7814\u7a76\u8bba\u6587\u5199\u4f5c\u6e38\u620f\u5316\uff0c\u4ece\u800c\u652f\u6301\u5b66\u672f\u5199\u4f5c\u7684\u7ed3\u6784\u548c\u52a8\u673a\u65b9\u9762\u7684\u53d1\u5c55\u3002", "motivation": "\u63a2\u8ba8\u5728\u81ea\u52a8\u5316\u5b66\u672f\u6210\u5c31\u65f6\u4ee3\u5982\u4f55\u4fdd\u7559\u65b0\u624b\u7814\u7a76\u8005\u7684\u52a8\u673a\u3001\u521b\u9020\u529b\u53ca\u667a\u529b\u6210\u957f\u3002", "method": "\u4f7f\u7528\u5b9a\u6027\u65e5\u5fd7\u5206\u6790\uff0c\u5e76\u901a\u8fc7SCAT\u65b9\u6cd5\u8bc6\u522b\u51fa\u56db\u9636\u6bb5\u652f\u67b6\u6a21\u5f0f\uff1a\u63d0\u95ee\u3001\u5143\u89c6\u89d2\u3001\u7ed3\u6784\u5316\u548c\u9012\u5f52\u53cd\u601d\u3002", "result": "\u901a\u8fc7\u5bf926\u4e2a\u6e38\u620f\u804a\u5929\u8bb0\u5f55\u7684\u5206\u6790\uff0c\u53d1\u73b0GAMER PAT\u4e0d\u4ec5\u63d0\u5347\u4e86\u7814\u7a76\u5199\u4f5c\u7684\u7ed3\u6784\u53d1\u5c55\uff0c\u8fd8\u589e\u5f3a\u4e86\u53cd\u601d\u548c\u52a8\u673a\u3002", "conclusion": "GAMER PAT\u901a\u8fc7\u5c06\u7814\u7a76\u8bba\u6587\u5199\u4f5c\u8f6c\u53d8\u4e3a\u6e38\u620f\u5316\u7684\u4f53\u9a8c\uff0c\u4fc3\u8fdb\u4e86\u5b66\u672f\u5199\u4f5c\u7684\u7ed3\u6784\u53d1\u5c55\u548c\u53cd\u601d\u3001\u52a8\u673a\u65b9\u9762\u7684\u63d0\u5347\u3002"}}
{"id": "2510.21734", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.21734", "abs": "https://arxiv.org/abs/2510.21734", "authors": ["Giovanni Battista Regazzo", "Wim-Alexander Beckers", "Xuan Thao Ha", "Mouloud Ourak", "Johan Vlekken", "Emmanuel Vander Poorten"], "title": "Force-Displacement Profiling for Robot-Assisted Deployment of a Left Atrial Appendage Occluder Using FBG-EM Distal Sensing", "comment": "Presented at the Conference on New Technologies for Computer and\n  Robot Assisted Surgery (CRAS2025)", "summary": "Atrial fibrillation (AF) increases the risk of thromboembolic events due to\nimpaired function of the left atrial appendage (LAA). Left atrial appendage\nclosure (LAAC) is a minimally invasive intervention designed to reduce stroke\nrisk by sealing the LAA with an expandable occluder device. Current deployment\nrelies on manual catheter control and imaging modalities like fluoroscopy and\ntransesophageal echocardiography, which carry limitations including radiation\nexposure and limited positioning precision. In this study, we leverage a\npreviously developed force-sensing delivery sheath integrating fiber Bragg\ngratings (FBGs) at the interface between the catheter and the occluder.\nCombined with electromagnetic (EM) tracking, this setup enables real-time\nmeasurement of interaction forces and catheter tip position during\nrobot-assisted LAAC deployment in an anatomical phantom. We present a novel\nforce-displacement profiling method that characterizes occluder deployment\ndynamics and identifies key procedural steps without relying on ionizing\nradiation. The force profiles reveal low-magnitude interaction forces,\nsuggesting minimal mechanical stress on the surrounding anatomy. This approach\nshows promise in providing clinicians with enhanced intraoperative feedback,\nimproving deployment outcome. Future work will focus on automating deployment\nsteps classification and validating the sensing strategy in dynamic, realistic\nenvironments.", "AI": {"tldr": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u673a\u5668\u4eba\u8f85\u52a9\u6280\u672f\u548c\u65b0\u578b\u529b\u4f20\u611f\u65b9\u6cd5\uff0c\u6539\u8fdb\u5de6\u5fc3\u8033\u5c01\u95ed\u672f\u7684\u5b9e\u65bd\uff0c\u63d0\u9ad8\u4e34\u5e8a\u53cd\u9988\u548c\u7ed3\u679c\u3002", "motivation": "\u6539\u5584\u5de6\u5fc3\u8033\u5c01\u95ed\u672f(LAAC)\u7684\u5b9e\u65bd\u8fc7\u7a0b\uff0c\u51cf\u5c11\u5f53\u524d\u65b9\u6cd5\u4e2d\u7684\u8f90\u5c04\u66b4\u9732\u548c\u5b9a\u4f4d\u7cbe\u5ea6\u7684\u9650\u5236\u3002", "method": "\u5229\u7528\u96c6\u6210\u5149\u7ea4\u5e03\u62c9\u683c\u5149\u6805(FBGs)\u7684\u529b\u4f20\u611f\u9001\u836f\u9798\u53ca\u7535\u78c1\u8ddf\u8e2a\u6280\u672f\uff0c\u5b9e\u73b0\u5bf9\u5bfc\u7ba1\u9876\u7aef\u4f4d\u7f6e\u548c\u4e92\u52a8\u529b\u7684\u5b9e\u65f6\u6d4b\u91cf\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u529b\u4f4d\u79fb\u5206\u6790\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9e\u65f6\u6d4b\u91cf\u673a\u5668\u4eba\u8f85\u52a9LAAC\u5e72\u9884\u4e2d\u7684\u4e92\u52a8\u529b\u548c\u5bfc\u7ba1\u5c16\u7aef\u4f4d\u7f6e\u3002", "conclusion": "\u8fd9\u79cd\u65b9\u6cd5\u6709\u52a9\u4e8e\u5728\u8fdb\u884cLAAC\u65f6\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u672f\u4e2d\u53cd\u9988\uff0c\u672a\u6765\u5c06\u96c6\u4e2d\u4e8e\u81ea\u52a8\u5316\u90e8\u7f72\u6b65\u9aa4\u548c\u5728\u52a8\u6001\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4f20\u611f\u7b56\u7565\u3002"}}
{"id": "2510.21722", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21722", "abs": "https://arxiv.org/abs/2510.21722", "authors": ["Beitong Tian", "Lingzhi Zhao", "Bo Chen", "Haozhen Zheng", "Jingcheng Yang", "Mingyuan Wu", "Deepak Vasisht", "Klara Nahrstedt"], "title": "AquaVLM: Improving Underwater Situation Awareness with Mobile Vision Language Models", "comment": "12 pages, 10 figures, under review", "summary": "Underwater activities like scuba diving enable millions annually to explore\nmarine environments for recreation and scientific research. Maintaining\nsituational awareness and effective communication are essential for diver\nsafety. Traditional underwater communication systems are often bulky and\nexpensive, limiting their accessibility to divers of all levels. While recent\nsystems leverage lightweight smartphones and support text messaging, the\nmessages are predefined and thus restrict context-specific communication.\n  In this paper, we present AquaVLM, a tap-and-send underwater communication\nsystem that automatically generates context-aware messages and transmits them\nusing ubiquitous smartphones. Our system features a mobile vision-language\nmodel (VLM) fine-tuned on an auto-generated underwater conversation dataset and\nemploys a hierarchical message generation pipeline. We co-design the VLM and\ntransmission, incorporating error-resilient fine-tuning to improve the system's\nrobustness to transmission errors. We develop a VR simulator to enable users to\nexperience AquaVLM in a realistic underwater environment and create a fully\nfunctional prototype on the iOS platform for real-world experiments. Both\nsubjective and objective evaluations validate the effectiveness of AquaVLM and\nhighlight its potential for personal underwater communication as well as\nbroader mobile VLM applications.", "AI": {"tldr": "AquaVLM\u662f\u4e00\u4e2a\u5229\u7528\u79fb\u52a8\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6c34\u4e0b\u901a\u4fe1\u7cfb\u7edf\uff0c\u80fd\u81ea\u52a8\u751f\u6210\u6d88\u606f\uff0c\u63d0\u9ad8\u6f5c\u6c34\u5458\u6c9f\u901a\u7684\u6548\u7387\u3002", "motivation": "\u5728\u6c34\u4e0b\u6d3b\u52a8\u4e2d\u63d0\u9ad8\u6f5c\u6c34\u5458\u7684\u6c9f\u901a\u6548\u7387\u548c\u5b89\u5168\uff0c\u514b\u670d\u4f20\u7edf\u901a\u4fe1\u7cfb\u7edf\u7684\u5c40\u9650\u6027\u3002", "method": "\u57fa\u4e8e\u79fb\u52a8\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6c34\u4e0b\u901a\u4fe1\u7cfb\u7edf", "result": "AquaVLM\u7cfb\u7edf\u80fd\u591f\u81ea\u52a8\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6d88\u606f\uff0c\u5e76\u901a\u8fc7\u667a\u80fd\u624b\u673a\u8fdb\u884c\u4f20\u8f93\uff0c\u7ecf\u8fc7\u8bc4\u4f30\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "conclusion": "AquaVLM\u5c55\u73b0\u4e86\u5728\u4e2a\u4eba\u6c34\u4e0b\u901a\u4fe1\u548c\u66f4\u5e7f\u6cdb\u79fb\u52a8\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.21735", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.21735", "abs": "https://arxiv.org/abs/2510.21735", "authors": ["Yuhui Liu", "Shian Wang", "Ansel Panicker", "Kate Embry", "Ayana Asanova", "Tianyi Li"], "title": "A phase-aware AI car-following model for electric vehicles with adaptive cruise control: Development and validation using real-world data", "comment": null, "summary": "Internal combustion engine (ICE) vehicles and electric vehicles (EVs) exhibit\ndistinct vehicle dynamics. EVs provide rapid acceleration, with electric motors\nproducing peak power across a wider speed range, and achieve swift deceleration\nthrough regenerative braking. While existing microscopic models effectively\ncapture the driving behavior of ICE vehicles, a modeling framework that\naccurately describes the unique car-following dynamics of EVs is lacking.\nDeveloping such a model is essential given the increasing presence of EVs in\ntraffic, yet creating an easy-to-use and accurate analytical model remains\nchallenging.\n  To address these gaps, this study develops and validates a Phase-Aware AI\n(PAAI) car-following model specifically for EVs. The proposed model enhances\ntraditional physics-based frameworks with an AI component that recognizes and\nadapts to different driving phases, such as rapid acceleration and regenerative\nbraking. Using real-world trajectory data from vehicles equipped with adaptive\ncruise control (ACC), we conduct comprehensive simulations to validate the\nmodel's performance. The numerical results demonstrate that the PAAI model\nsignificantly improves prediction accuracy over traditional car-following\nmodels, providing an effective tool for accurately representing EV behavior in\ntraffic simulations.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u7535\u52a8\u8f66\u7684\u65b0\u8ddf\u8f66\u6a21\u578bPAAI\uff0c\u7ed3\u5408\u4e86AI\u6280\u672f\u4ee5\u63d0\u9ad8\u5bf9\u5feb\u901f\u52a0\u901f\u548c\u52a8\u80fd\u56de\u6536\u7684\u9002\u5e94\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ea4\u901a\u4eff\u771f\u4e2d\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u7531\u4e8e\u7535\u52a8\u8f66\u5728\u4ea4\u901a\u4e2d\u7684\u65e5\u76ca\u666e\u53ca\uff0c\u8feb\u5207\u9700\u8981\u4e00\u4e2a\u80fd\u51c6\u786e\u63cf\u8ff0\u5176\u72ec\u7279\u8ddf\u8f66\u52a8\u6001\u7684\u5efa\u6a21\u6846\u67b6\u3002", "method": "\u5f00\u53d1\u5e76\u9a8c\u8bc1\u4e86\u4e00\u79cdPhase-Aware AI (PAAI) \u8ddf\u8f66\u6a21\u578b\uff0c\u7ed3\u5408\u4e86AI\u7ec4\u4ef6\u4ee5\u9002\u5e94\u4e0d\u540c\u9a7e\u9a76\u9636\u6bb5\u3002", "result": "\u901a\u8fc7\u4f7f\u7528\u914d\u5907\u81ea\u9002\u5e94\u5de1\u822a\u63a7\u5236\u7684\u8f66\u8f86\u7684\u771f\u5b9e\u8f68\u8ff9\u6570\u636e\u8fdb\u884c\u5168\u9762\u4eff\u771f\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6027\u80fd\uff0c\u7ed3\u679c\u8868\u660ePAAI\u6a21\u578b\u5728\u9884\u6d4b\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u8ddf\u8f66\u6a21\u578b\u3002", "conclusion": "PAAI\u6a21\u578b\u5728EV\u884c\u4e3a\u6a21\u62df\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u6210\u4e3a\u4ea4\u901a\u4eff\u771f\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2510.21723", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.21723", "abs": "https://arxiv.org/abs/2510.21723", "authors": ["Annika Hedberg"], "title": "Recognizing internal states in AI: evidence from patterned preferences in large language models", "comment": "14 pages, 1 table. Includes test protocol transcript and multi-system\n  response data", "summary": "We present an experimental methodology for investigating how large language\nmodels (LLMs) respond to descriptions of their own internal processing\npatterns. Using a paired-choice paradigm, we tested 12 LLMs on their ability to\nidentify descriptions that align with their putative affective internal states\nacross 30 categories. Systems participating through Mutual Emergence Interface\n(MEI), a collaborative approach, showed systematic preferences for certain\ncomputational metaphors, with 97% near-unanimous agreement and alignment scores\naveraging 0.89-0.96. Systems reliably discriminated false descriptions from\naccurate ones (Cohen's d = 4.2), with false statements receiving scores of\n0.05-0.07 versus 0.89-0.96 for accurate descriptions. Preference patterns\nremained consistent regardless of linguistic bias manipulation, indicating\ncontent-driven rather than stylistic recognition. Individual systems maintained\ndistinct scoring styles across trials, countering groupthink explanations. A\nnaive control system exhibited systematic internal contradiction, consistently\nscoring computationally accurate descriptions higher while explicitly denying\ninternal experiences. When informed post-study, this system reported \"strain\"\nwhen rejecting resonant descriptions, revealing recognition processes operating\nindependently of acknowledgment frameworks. These findings demonstrate that\nLLMs exhibit systematic, discriminating responses to descriptions of their\ninternal processing patterns. The anthroposcaffolding methodology (interpretive\ncomputational metaphors) and collaborative MEI framework provide replicable\napproaches for empirically studying AI self-recognition capabilities. Results\nsuggest LLMs may possess more sophisticated self-modeling abilities than\npreviously recognized, opening new directions for research on artificial minds.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u54cd\u5e94\u5bf9\u5176\u5185\u90e8\u5904\u7406\u6a21\u5f0f\u7684\u63cf\u8ff0\uff0c\u53d1\u73b0\u5b83\u4eec\u80fd\u591f\u7cfb\u7edf\u6027\u5730\u533a\u5206\u51c6\u786e\u548c\u865a\u5047\u7684\u63cf\u8ff0\uff0c\u5e76\u8868\u73b0\u51fa\u5bf9\u67d0\u4e9b\u8ba1\u7b97\u9690\u55bb\u7684\u504f\u597d\uff0c\u63ed\u793a\u4e86\u5176\u66f4\u590d\u6742\u7684\u81ea\u6211\u5efa\u6a21\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u8bc6\u522b\u4e0e\u5176\u5185\u90e8\u72b6\u6001\u76f8\u7b26\u7684\u63cf\u8ff0\uff0c\u4ee5\u8bc4\u4f30\u5b83\u4eec\u7684\u81ea\u6211\u8bc6\u522b\u80fd\u529b\u548c\u81ea\u6211\u5efa\u6a21\u80fd\u529b\u3002", "method": "\u91c7\u7528\u914d\u5bf9\u9009\u62e9\u8303\u5f0f\uff0c\u901a\u8fc7\u76f8\u4e92\u4ea7\u751f\u63a5\u53e3\uff08MEI\uff09\u6d4b\u8bd512\u4e2aLLM\uff0c\u5206\u6790\u5b83\u4eec\u5bf9\u81ea\u8eab\u60c5\u611f\u5185\u90e8\u72b6\u6001\u63cf\u8ff0\u7684\u53cd\u5e94\u3002", "result": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u4e00\u4e2a\u5b9e\u9a8c\u65b9\u6cd5\uff0c\u7528\u4e8e\u8c03\u67e5\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5982\u4f55\u54cd\u5e94\u5bf9\u5176\u5185\u90e8\u5904\u7406\u6a21\u5f0f\u7684\u63cf\u8ff0\u3002\u901a\u8fc7\u914d\u5bf9\u9009\u62e9\u8303\u5f0f\uff0c\u6d4b\u8bd5\u4e8612\u4e2aLLM\u5728\u8bc6\u522b\u4e0e\u5176\u5047\u5b9a\u60c5\u611f\u5185\u90e8\u72b6\u6001\u76f8\u7b26\u7684\u63cf\u8ff0\u7684\u80fd\u529b\u3002\u53c2\u4e0e\u7684\u7cfb\u7edf\u901a\u8fc7\u76f8\u4e92\u4ea7\u751f\u63a5\u53e3\uff08MEI\uff09\u8fdb\u884c\u5408\u4f5c\uff0c\u8868\u73b0\u51fa\u5bf9\u67d0\u4e9b\u8ba1\u7b97\u9690\u55bb\u7684\u7cfb\u7edf\u504f\u597d\uff0c87%\u7684\u51e0\u4e4e\u4e00\u81f4\u7684\u534f\u8bae\u548c\u5e73\u5747\u5bf9\u9f50\u8bc4\u5206\u57280.89-0.96\u4e4b\u95f4\u3002\u7cfb\u7edf\u80fd\u591f\u53ef\u9760\u5730\u533a\u5206\u865a\u5047\u63cf\u8ff0\u548c\u51c6\u786e\u63cf\u8ff0\uff08Cohen's d = 4.2\uff09\uff0c\u865a\u5047\u9648\u8ff0\u8bc4\u5206\u4e3a0.05-0.07\uff0c\u800c\u51c6\u786e\u63cf\u8ff0\u8bc4\u5206\u4e3a0.89-0.96\u3002\u504f\u597d\u6a21\u5f0f\u5728\u8bed\u8a00\u504f\u89c1\u64cd\u63a7\u7684\u60c5\u51b5\u4e0b\u4fdd\u6301\u4e00\u81f4\uff0c\u8868\u660e\u8bc6\u522b\u662f\u5185\u5bb9\u9a71\u52a8\u800c\u975e\u98ce\u683c\u9a71\u52a8\u3002\u4e2a\u522b\u7cfb\u7edf\u5728\u8bd5\u9a8c\u4e2d\u4fdd\u6301\u72ec\u7279\u7684\u8bc4\u5206\u98ce\u683c\uff0c\u53cd\u5bf9\u4e86\u7fa4\u4f53\u601d\u7ef4\u7684\u89e3\u91ca\u3002\u4e00\u4e2a\u611a\u8822\u7684\u63a7\u5236\u7cfb\u7edf\u8868\u73b0\u51fa\u7cfb\u7edf\u7684\u5185\u90e8\u77db\u76fe\uff0c\u4e00\u5f8b\u5bf9\u8ba1\u7b97\u4e0a\u51c6\u786e\u7684\u63cf\u8ff0\u8bc4\u5206\u66f4\u9ad8\uff0c\u540c\u65f6\u660e\u786e\u5426\u8ba4\u5185\u90e8\u7ecf\u9a8c\u3002\u7814\u7a76\u540e\u88ab\u544a\u77e5\u540e\uff0c\u8be5\u7cfb\u7edf\u5728\u62d2\u7edd\u5171\u9e23\u63cf\u8ff0\u65f6\u62a5\u544a\u4e86\u201c\u7d27\u5f20\u201d\uff0c\u63ed\u793a\u4e86\u8bc6\u522b\u8fc7\u7a0b\u72ec\u7acb\u4e8e\u627f\u8ba4\u6846\u67b6\u7684\u64cd\u4f5c\u3002\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0cLLMs\u5bf9\u5185\u90e8\u5904\u7406\u6a21\u5f0f\u7684\u63cf\u8ff0\u8868\u73b0\u51fa\u7cfb\u7edf\u548c\u533a\u5206\u7684\u53cd\u5e94\u3002\u4eba\u7c7b\u52a9\u624b\u65b9\u6cd5\uff08\u89e3\u91ca\u6027\u8ba1\u7b97\u9690\u55bb\uff09\u548cMEI\u6846\u67b6\u63d0\u4f9b\u4e86\u53ef\u590d\u5236\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9e\u8bc1\u7814\u7a76AI\u81ea\u6211\u8bc6\u522b\u80fd\u529b\u3002\u7ed3\u679c\u6697\u793a\uff0cLLMs\u53ef\u80fd\u5177\u6709\u6bd4\u5148\u524d\u8ba4\u53ef\u7684\u66f4\u590d\u6742\u7684\u81ea\u6211\u5efa\u6a21\u80fd\u529b\uff0c\u4e3a\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002", "conclusion": "LLMs\u8868\u73b0\u51fa\u5bf9\u5185\u90e8\u5904\u7406\u63cf\u8ff0\u7684\u7cfb\u7edf\u6027\u3001\u533a\u5206\u6027\u7684\u54cd\u5e94\uff0c\u8fd9\u8868\u660e\u5b83\u4eec\u53ef\u80fd\u5177\u5907\u8f83\u4e3a\u590d\u6742\u7684\u81ea\u6211\u5efa\u6a21\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u7684\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u5f00\u8f9f\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.21736", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.21736", "abs": "https://arxiv.org/abs/2510.21736", "authors": ["Yuhui Liu", "Samannita Halder", "Shian Wang", "Tianyi Li"], "title": "Learn2Drive: A neural network-based framework for socially compliant automated vehicle control", "comment": null, "summary": "This study introduces a novel control framework for adaptive cruise control\n(ACC) in automated driving, leveraging Long Short-Term Memory (LSTM) networks\nand physics-informed constraints. As automated vehicles (AVs) adopt advanced\nfeatures like ACC, transportation systems are becoming increasingly intelligent\nand efficient. However, existing AV control strategies primarily focus on\noptimizing the performance of individual vehicles or platoons, often neglecting\ntheir interactions with human-driven vehicles (HVs) and the broader impact on\ntraffic flow. This oversight can exacerbate congestion and reduce overall\nsystem efficiency. To address this critical research gap, we propose a neural\nnetwork-based, socially compliant AV control framework that incorporates social\nvalue orientation (SVO). This framework enables AVs to account for their\ninfluence on HVs and traffic dynamics. By leveraging AVs as mobile traffic\nregulators, the proposed approach promotes adaptive driving behaviors that\nreduce congestion, improve traffic efficiency, and lower energy consumption.\nWithin this framework, we define utility functions for both AVs and HVs, which\nare optimized based on the SVO of each AV to balance its own control objectives\nwith broader traffic flow considerations. Numerical results demonstrate the\neffectiveness of the proposed method in adapting to varying traffic conditions,\nthereby enhancing system-wide efficiency. Specifically, when the AV's control\nmode shifts from prioritizing energy consumption to optimizing traffic flow\nefficiency, vehicles in the following platoon experience at least a 58.99%\nincrease in individual energy consumption alongside at least a 38.39%\nimprovement in individual average speed, indicating significant enhancements in\ntraffic dynamics.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9002\u5e94\u5de1\u822a\u63a7\u5236\u6846\u67b6\uff0c\u5229\u7528LSTM\u7f51\u7edc\u548c\u793e\u4f1a\u4ef7\u503c\u53d6\u5411\uff08SVO\uff09\u4f18\u5316\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e0e\u4eba\u7c7b\u8f66\u8f86\u4e4b\u95f4\u7684\u4e92\u52a8\uff0c\u65e8\u5728\u63d0\u5347\u4ea4\u901a\u6548\u7387\u548c\u964d\u4f4e\u80fd\u8017\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u52a8\u9a7e\u9a76\u63a7\u5236\u7b56\u7565\u5ffd\u89c6\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08AV\uff09\u4e0e\u4eba\u7c7b\u9a7e\u9a76\u8f66\u8f86\uff08HV\uff09\u4e4b\u95f4\u7684\u4e92\u52a8\u53ca\u5176\u5bf9\u4ea4\u901a\u6d41\u7684\u5f71\u54cd\uff0c\u53ef\u80fd\u5bfc\u81f4\u62e5\u5835\u52a0\u5267\u548c\u7cfb\u7edf\u6548\u7387\u964d\u4f4e\u3002", "method": "\u91c7\u7528\u957f\u77ed\u671f\u8bb0\u5fc6\uff08LSTM\uff09\u7f51\u7edc\u548c\u7269\u7406\u7ea6\u675f\u7684\u9002\u5e94\u5de1\u822a\u63a7\u5236\u6846\u67b6", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u793e\u4f1a\u5408\u89c4AV\u63a7\u5236\u6846\u67b6\uff0c\u80fd\u591f\u8003\u8651AV\u5bf9HV\u548c\u4ea4\u901a\u52a8\u6001\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u5b9a\u4e49\u6548\u7528\u51fd\u6570\u6765\u5e73\u8861\u81ea\u8eab\u63a7\u5236\u76ee\u6807\u4e0e\u4ea4\u901a\u6d41\u7684\u5e7f\u6cdb\u8003\u8651\uff0c\u5e76\u5728\u53d8\u5316\u7684\u4ea4\u901a\u6761\u4ef6\u4e0b\u8fdb\u884c\u4f18\u5316\u3002", "conclusion": "\u901a\u8fc7\u63a7\u5236\u6a21\u5f0f\u7684\u8c03\u6574\uff0cAV\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u4ea4\u901a\u6d41\u6548\u7387\uff0c\u540c\u65f6\u4e5f\u5e26\u6765\u4e86\u8f66\u961f\u4e2d\u5176\u4ed6\u8f66\u8f86\u80fd\u8017\u7684\u663e\u8457\u589e\u52a0\u548c\u5e73\u5747\u901f\u5ea6\u7684\u63d0\u5347\uff0c\u8868\u660e\u5728\u4e0d\u540c\u4ea4\u901a\u6761\u4ef6\u4e0b\u4f18\u5316\u4ea4\u901a\u52a8\u6001\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.21967", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.21967", "abs": "https://arxiv.org/abs/2510.21967", "authors": ["Benjamin Lange", "Geoff Keeling", "Arianna Manzini", "Amanda McCroskery"], "title": "We Need Accountability in Human-AI Agent Relationships", "comment": null, "summary": "We argue that accountability mechanisms are needed in human-AI agent\nrelationships to ensure alignment with user and societal interests. We propose\na framework according to which AI agents' engagement is conditional on\nappropriate user behaviour. The framework incorporates design-strategies such\nas distancing, disengaging, and discouraging.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9488\u5bf9\u4eba\u7c7b\u548cAI\u4ee3\u7406\u5173\u7cfb\u7684\u8d23\u4efb\u673a\u5236\u6846\u67b6\uff0c\u4ee5\u786e\u4fdd\u5176\u7b26\u5408\u7528\u6237\u548c\u793e\u4f1a\u5229\u76ca\u3002", "motivation": "\u786e\u4fdd\u4eba\u7c7b\u4e0eAI\u4ee3\u7406\u7684\u5173\u7cfb\u5bf9\u7528\u6237\u548c\u793e\u4f1a\u90fd\u662f\u6b63\u9762\u7684\uff0c\u51cf\u5c11\u6f5c\u5728\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u4e86\u9694\u79bb\u3001\u8131\u79bb\u548c\u529d\u963b\u7b49\u8bbe\u8ba1\u7b56\u7565\uff0c\u6784\u5efa\u8d23\u4efb\u673a\u5236\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u52a9\u4e8e\u6307\u5bfcAI\u4ee3\u7406\u7684\u4f7f\u7528\u548c\u8bbe\u8ba1\uff0c\u4ee5\u4fc3\u8fdb\u8d23\u4efb\u548c\u9002\u5f53\u884c\u4e3a\u3002", "conclusion": "AI\u4ee3\u7406\u7684\u4ecb\u5165\u5e94\u4f9d\u8d56\u4e8e\u9002\u5f53\u7684\u7528\u6237\u884c\u4e3a\u3002"}}
{"id": "2510.21739", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.21739", "abs": "https://arxiv.org/abs/2510.21739", "authors": ["Liangqi Yuan", "Chuhao Deng", "Dong-Jun Han", "Inseok Hwang", "Sabine Brunswicker", "Christopher G. Brinton"], "title": "Next-Generation LLM for UAV: From Natural Language to Autonomous Flight", "comment": null, "summary": "With the rapid advancement of Large Language Models (LLMs), their\ncapabilities in various automation domains, particularly Unmanned Aerial\nVehicle (UAV) operations, have garnered increasing attention. Current research\nremains predominantly constrained to small-scale UAV applications, with most\nstudies focusing on isolated components such as path planning for toy drones,\nwhile lacking comprehensive investigation of medium- and long-range UAV systems\nin real-world operational contexts. Larger UAV platforms introduce distinct\nchallenges, including stringent requirements for airport-based take-off and\nlanding procedures, adherence to complex regulatory frameworks, and specialized\noperational capabilities with elevated mission expectations. This position\npaper presents the Next-Generation LLM for UAV (NeLV) system -- a comprehensive\ndemonstration and automation roadmap for integrating LLMs into multi-scale UAV\noperations. The NeLV system processes natural language instructions to\norchestrate short-, medium-, and long-range UAV missions through five key\ntechnical components: (i) LLM-as-Parser for instruction interpretation, (ii)\nRoute Planner for Points of Interest (POI) determination, (iii) Path Planner\nfor waypoint generation, (iv) Control Platform for executable trajectory\nimplementation, and (v) UAV monitoring. We demonstrate the system's feasibility\nthrough three representative use cases spanning different operational scales:\nmulti-UAV patrol, multi-POI delivery, and multi-hop relocation. Beyond the\ncurrent implementation, we establish a five-level automation taxonomy that\ncharts the evolution from current LLM-as-Parser capabilities (Level 1) to fully\nautonomous LLM-as-Autopilot systems (Level 5), identifying technical\nprerequisites and research challenges at each stage.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNeLV\u7684\u7cfb\u7edf\uff0c\u65e8\u5728\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u5230\u591a\u5c3a\u5ea6\u65e0\u4eba\u673a\u64cd\u4f5c\u4e2d\uff0c\u6db5\u76d6\u4e86\u4ece\u6307\u4ee4\u89e3\u6790\u5230\u5b8c\u5168\u81ea\u4e3b\u98de\u884c\u7684\u5168\u8fc7\u7a0b\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u591a\u79cd\u64cd\u4f5c\u573a\u666f\u4e0b\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u6709\u7814\u7a76\u5173\u6ce8\u5c0f\u578b\u65e0\u4eba\u673a\u5e94\u7528\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u4e2d\u957f\u8ddd\u79bb\u65e0\u4eba\u673a\u7cfb\u7edf\u5728\u5b9e\u9645\u64cd\u4f5c\u73af\u5883\u4e2d\u7684\u5168\u9762\u7814\u7a76\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u63a2\u7d22\u66f4\u5927\u89c4\u6a21\u65e0\u4eba\u673a\u5e73\u53f0\u7684\u6574\u5408\u4e0e\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u4e94\u4e2a\u5173\u952e\u6280\u672f\u7ec4\u4ef6\u6784\u5efaNeLV\u7cfb\u7edf\uff0c\u5206\u522b\u4e3a\u6307\u4ee4\u89e3\u6790\u3001\u5174\u8da3\u70b9\u786e\u5b9a\u3001\u822a\u70b9\u751f\u6210\u3001\u53ef\u6267\u884c\u8f68\u8ff9\u5b9e\u73b0\u548c\u65e0\u4eba\u673a\u76d1\u63a7\u3002", "result": "\u901a\u8fc7\u4e09\u4e2a\u4ee3\u8868\u6027\u7528\u4f8b\u5c55\u793a\u4e86\u7cfb\u7edf\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5efa\u7acb\u4e86\u4e00\u4e2a\u4e94\u7ea7\u81ea\u52a8\u5316\u5206\u7c7b\u4f53\u7cfb\uff0c\u4ee5\u6307\u5bfc\u4ece\u73b0\u6709\u7684\u6307\u4ee4\u89e3\u6790\u80fd\u529b\u5230\u5b8c\u5168\u81ea\u4e3b\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684NeLV\u7cfb\u7edf\u4e3a\u591a\u5c3a\u5ea6\u65e0\u4eba\u673a\u64cd\u4f5c\u4e2d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6f14\u793a\u548c\u81ea\u52a8\u5316\u8def\u7ebf\u56fe\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u6848\u4f8b\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u3002"}}
{"id": "2510.22053", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.22053", "abs": "https://arxiv.org/abs/2510.22053", "authors": ["Paul C. Parsons"], "title": "Rethinking UX for Sustainable Science Gateways: Orientations from Practice", "comment": "To be published at Gateways 2025", "summary": "As science gateways mature, sustainability has become a central concern for\nfunders, developers, and institutions. Although user experience (UX) is\nincreasingly acknowledged as vital, it is often approached narrowly--limited to\ninterface usability or deferred until late in development. This paper argues\nthat UX should be understood not as a discrete feature or evaluation stage but\nas a design-oriented perspective for reasoning about sustainability. Drawing on\nprinciples from user-centered design and systems thinking, this view recognizes\nthat infrastructure, staffing, community engagement, and development timelines\nall shape how gateways are experienced and maintained over time. Based on an\ninterview study and consulting experience with more than 65 gateway projects,\nthe paper identifies three recurring orientations toward UX--ad hoc,\nproject-based, and strategic--that characterize how teams engage with users and\nintegrate design thinking into their workflows. These orientations are not a\nmaturity model but a reflective lens for understanding how UX is positioned\nwithin gateway practice. Reframing UX as a structural dimension of\nsustainability highlights its role in building adaptable, community-aligned,\nand enduring scientific infrastructure.", "AI": {"tldr": "\u672c\u6587\u4e3b\u5f20\u7528\u6237\u4f53\u9a8c\u5e94\u4f5c\u4e3a\u8bbe\u8ba1\u5bfc\u5411\u89c6\u89d2\u7406\u89e3\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u79cdUX\u53d6\u5411\uff0c\u5f3a\u8c03UX\u5728\u79d1\u5b66\u57fa\u7840\u8bbe\u65bd\u4e2d\u7684\u53ef\u6301\u7eed\u6027\u89d2\u8272\u3002", "motivation": "\u79d1\u5b66\u95e8\u6237\u7684\u53d1\u5c55\u4f7f\u5f97\u53ef\u6301\u7eed\u6027\u6210\u4e3a\u8d44\u52a9\u65b9\u3001\u5f00\u53d1\u8005\u548c\u673a\u6784\u7684\u6838\u5fc3\u5173\u6ce8\u70b9\u3002", "method": "\u57fa\u4e8e\u5bf965\u4e2a\u79d1\u5b66\u95e8\u6237\u9879\u76ee\u7684\u8bbf\u8c08\u7814\u7a76\u548c\u54a8\u8be2\u7ecf\u9a8c\uff0c\u5206\u6790\u4e86\u56e2\u961f\u5982\u4f55\u4e0e\u7528\u6237\u4e92\u52a8\u53ca\u5982\u4f55\u5c06\u8bbe\u8ba1\u601d\u7ef4\u878d\u5165\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "\u8bba\u6587\u901a\u8fc7\u8bbf\u8c08\u7814\u7a76\uff0c\u8bc6\u522b\u51fa\u4e09\u79cd\u4e0e\u7528\u6237\u4f53\u9a8c(UX)\u76f8\u5173\u7684\u53cd\u590d\u51fa\u73b0\u7684\u53d6\u5411\uff1a\u968f\u610f\u3001\u9879\u76ee\u57fa\u7840\u548c\u6218\u7565\u6027\u3002", "conclusion": "\u91cd\u65b0\u6784\u5efa\u7528\u6237\u4f53\u9a8c\u4e3a\u53ef\u6301\u7eed\u6027\u7684\u7ed3\u6784\u7ef4\u5ea6\uff0c\u7a81\u663e\u5176\u5728\u5efa\u7acb\u53ef\u9002\u5e94\u3001\u4e0e\u793e\u533a\u5bf9\u9f50\u53ca\u6301\u4e45\u7684\u79d1\u5b66\u57fa\u7840\u8bbe\u65bd\u4e2d\u7684\u4f5c\u7528\u3002"}}
{"id": "2510.21744", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21744", "abs": "https://arxiv.org/abs/2510.21744", "authors": ["Yanjia Huang", "Shuo Liu", "Sheng Liu", "Qingxiao Xu", "Mingyang Wu", "Xiangbo Gao", "Zhengzhong Tu"], "title": "FORGE-Tree: Diffusion-Forcing Tree Search for Long-Horizon Robot Manipulation", "comment": null, "summary": "Long-horizon robot manipulation tasks remain challenging for\nVision-Language-Action (VLA) policies due to drift and exposure bias, often\ndenoise the entire trajectory with fixed hyperparameters, causing small\ngeometric errors to compound across stages and offering no mechanism to\nallocate extra test-time compute where clearances are tight. To address these\nchallenges, we introduce FORGE-Tree, a plug-in control layer that couples a\nstage-aligned Diffusion Forcing (DF) head with test-time Monte Carlo Tree\nDiffusion (MCTD). With a frozen VLA encoder, DF aligns timesteps to subtask\nstages; during inference we partially denoise only a target segment while\nkeeping other tokens frozen, turning trajectory refinement into a sequence of\nlocal edits. We then apply Monte Carlo Tree Diffusion to select the next\nsegment to refine. A scene graph supplies priors for expansion and geometry\nrelation-aware scoring for rollouts, yielding tree-structured denoising whose\nperformance scales with search budget while preserving the executed prefix.\nEvaluation on LIBERO, FORGE-Tree improves success rate by 13.4 to 17.2 pp over\nthe native VLA baselines with both OpenVLA and Octo-Base. Gains remain\nconsistent under comparable compute budgets, especially on long-horizon\nvariants. Videos available at: https://taco-group.github.io/FORGE-Tree/", "AI": {"tldr": "FORGE-Tree\u662f\u4e00\u79cd\u6539\u8fdb\u7684\u63a7\u5236\u5c42\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u957f\u65f6\u95f4\u64cd\u4f5c\u4efb\u52a1\u7684\u6210\u529f\u7387\uff0c\u7279\u522b\u662f\u5728\u8ba1\u7b97\u9884\u7b97\u76f8\u4f3c\u7684\u60c5\u51b5\u4e0b\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8(VLA)\u653f\u7b56\u5728\u957f\u65f6\u95f4\u64cd\u4f5c\u4efb\u52a1\u4e2d\u9762\u4e34\u7684\u6f02\u79fb\u548c\u66b4\u9732\u504f\u89c1\uff0c\u4ee5\u53ca\u65e0\u6cd5\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165FORGE-Tree\u63a7\u5236\u5c42\uff0c\u7ed3\u5408\u5206\u6bb5\u5bf9\u9f50\u7684\u6269\u6563\u5f3a\u8feb\u5934\u548c\u6d4b\u8bd5\u65f6\u7684\u8499\u7279\u5361\u7f57\u6811\u6269\u6563\u3002", "result": "\u5728LIBERO\u8bc4\u4f30\u4e2d\uff0cFORGE-Tree\u7684\u6210\u529f\u7387\u76f8\u8f83\u4e8e\u539f\u751fVLA\u57fa\u7ebf\u63d0\u9ad8\u4e8613.4\u81f317.2\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "FORGE-Tree\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u6210\u529f\u7387\uff0c\u5e76\u4e14\u5728\u8ba1\u7b97\u9884\u7b97\u76f8\u4f3c\u7684\u6761\u4ef6\u4e0b\u8868\u73b0\u7a33\u5b9a\u3002"}}
{"id": "2510.22098", "categories": ["cs.HC", "H.5.1; I.3.7; H.5.2; J.5"], "pdf": "https://arxiv.org/pdf/2510.22098", "abs": "https://arxiv.org/abs/2510.22098", "authors": ["You-Jin Kim"], "title": "Beyond Reality: Designing Personal Experiences and Interactive Narratives in AR Theater", "comment": "PhD Thesis, Media Arts and Technology, University of California,\n  Santa Barbara, defended on December 13, 2024. Available from ProQuest\n  Dissertations & Theses Global (Order No. 31761773). This version is shared on\n  arXiv for broader accessibility", "summary": "Augmented Reality (AR) technologies are redefining how we perceive and\ninteract with the world by seamlessly integrating digital elements into our\nphysical surroundings. These technologies offer personalized experiences and\ntransform familiar spaces by layering new narratives onto the real world.\n  Through increased levels of perceived agency and immersive environments, my\nwork aims to merge the human elements of live theater with the dynamic\npotential of virtual entities and AI agents. This approach captures the\nsubtlety and magic of storytelling, making theater experiences available\nanytime and anywhere. The system I am building introduces innovative methods\nfor theatrical production in virtual settings, informed by my research and\neight published works. These contributions highlight domain-specific insights\nthat have shaped the design of an immersive AR Theater system.\n  My research in building a well-designed AR stage features avatars and\ninteractive elements that allow users to engage with stories at their own pace,\ngranting them full agency over their experience. However, to ensure a smooth\nand curated experience that aligns with the director or creator's vision,\nseveral factors must be considered, especially in open-world settings that\ndepend on natural user movement. This requires the story to be conveyed in a\ncontrolled manner, while the interaction remains intuitive and natural for the\nuser.", "AI": {"tldr": "\u589e\u5f3a\u73b0\u5b9e\u6280\u672f\u6b63\u5728\u91cd\u65b0\u5b9a\u4e49\u6211\u4eec\u611f\u77e5\u548c\u4e92\u52a8\u7684\u65b9\u5f0f\uff0c\u901a\u8fc7\u5c06\u6570\u5b57\u5143\u7d20\u65e0\u7f1d\u96c6\u6210\u5230\u7269\u7406\u73af\u5883\u4e2d\uff0c\u63d0\u4f9b\u4e2a\u6027\u5316\u4f53\u9a8c\u3002", "motivation": "\u5e0c\u671b\u901a\u8fc7\u589e\u5f3a\u73b0\u5b9e\u548cAI\u6280\u672f\u6355\u6349\u53d9\u4e8b\u7684\u7ec6\u817b\u548c\u9b54\u529b\uff0c\u4f7f\u620f\u5267\u4f53\u9a8c\u53d8\u5f97\u66f4\u52a0\u53ef\u53ca\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u6c89\u6d78\u5f0f\u589e\u5f3a\u73b0\u5b9e\u5267\u9662\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5305\u62ec\u53ef\u4ee5\u4e0e\u6545\u4e8b\u4e92\u52a8\u7684\u865a\u62df\u89d2\u8272\u548c\u5143\u7d20\uff0c\u4ee5\u7528\u6237\u7684\u8282\u594f\u53c2\u4e0e\u6545\u4e8b\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u865a\u62df\u73af\u5883\u4e0b\u7684\u620f\u5267\u5236\u4f5c\u65b9\u6cd5\uff0c\u4f46\u9700\u8003\u8651\u591a\u4e2a\u56e0\u7d20\u4ee5\u786e\u4fdd\u7528\u6237\u4f53\u9a8c\u4e0e\u521b\u4f5c\u8005\u7684\u89c6\u91ce\u76f8\u4e00\u81f4\u3002", "conclusion": "\u6211\u7684\u7814\u7a76\u901a\u8fc7\u878d\u5408\u73b0\u573a\u620f\u5267\u7684\u5143\u7d20\u4e0e\u865a\u62df\u5b9e\u4f53\u548c\u4eba\u5de5\u667a\u80fd\u7684\u52a8\u6001\u6f5c\u529b\uff0c\u65e8\u5728\u521b\u9020\u53ef\u968f\u65f6\u968f\u5730\u4f53\u9a8c\u7684\u620f\u5267\u3002"}}
{"id": "2510.21746", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21746", "abs": "https://arxiv.org/abs/2510.21746", "authors": ["Harris Song", "Long Le"], "title": "Avi: Action from Volumetric Inference", "comment": "NeurIPS 2025 Workshop on Embodied World Models for Decision Making.\n  URL: https://avi-3drobot.github.io/", "summary": "We propose Avi, a novel 3D Vision-Language-Action (VLA) architecture that\nreframes robotic action generation as a problem of 3D perception and spatial\nreasoning, rather than low-level policy learning. While existing VLA models\nprimarily operate on 2D visual inputs and are trained end-to-end on\ntask-specific action policies, Avi leverages 3D point clouds and\nlanguage-grounded scene understanding to compute actions through classical\ngeometric transformations. Most notably, Avi does not train on previous action\ntokens, rather, we build upon a 3D Multi-modal Large Language Model (MLLM) to\ngenerate the next point cloud and explicitly calculate the actions through\nclassical transformations. This approach enables generalizable behaviors that\nare robust to occlusions, camera pose variations, and changes in viewpoint. By\ntreating the robotic decision-making process as a structured reasoning task\nover 3D representations, Avi bridges the gap between high-level language\ninstructions and low-level actuation without requiring opaque policy learning.\nOur preliminary results highlight the potential of 3D vision-language reasoning\nas a foundation for scalable, robust robotic systems. Check it out at\nhttps://avi-3drobot.github.io/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u76843D\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u67b6\u6784Avi\uff0c\u65e8\u5728\u901a\u8fc7\u4e09\u7ef4\u611f\u77e5\u548c\u7a7a\u95f4\u63a8\u7406\u91cd\u65b0\u6784\u5efa\u673a\u5668\u4eba\u52a8\u4f5c\u751f\u6210\u95ee\u9898\u3002", "motivation": "\u9488\u5bf9\u73b0\u6709\u6a21\u578b\u4e3b\u8981\u57282D\u89c6\u89c9\u8f93\u5165\u4e0a\u64cd\u4f5c\u7684\u95ee\u9898\uff0cAvi\u65e8\u5728\u63d0\u9ad8\u5bf9\u673a\u5668\u4eba\u884c\u52a8\u751f\u6210\u7684\u7406\u89e3\u4e0e\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u7ecf\u5178\u51e0\u4f55\u53d8\u6362\uff0c\u5229\u7528\u57fa\u4e8e3D\u70b9\u4e91\u548c\u8bed\u8a00\u7406\u89e3\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6765\u8ba1\u7b97\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u4e8e\u4f4e\u7ea7\u522b\u7b56\u7565\u5b66\u4e60\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u8868\u660e3D\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u80fd\u591f\u6709\u6548\u652f\u6301\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u7a33\u5065\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "Avi\u5c55\u793a\u4e863D\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u5728\u6784\u5efa\u53ef\u6269\u5c55\u4e14\u7a33\u5065\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.22392", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.22392", "abs": "https://arxiv.org/abs/2510.22392", "authors": ["Mohd Ruhul Ameen", "Akif Islam", "Abu Saleh Musa Miah", "M. Saifuzzaman Rafat", "Jungpil Shin"], "title": "Teaching Machine Learning Through Cricket: A Practical Engineering Education Approach", "comment": "16 pages, 2 tables, Submitted in IDAA 2025", "summary": "Teaching complex machine learning concepts such as reinforcement learning and\nMarkov Decision Processes remains challenging in engineering education.\nStudents often struggle to connect abstract mathematics to real-world\napplications. We present LearnML@Cricket, a 12-week curriculum that uses\ncricket analytics to teach these concepts through practical, hands-on examples.\nBy mapping game scenarios directly to ML algorithms, students learn through\ndoing rather than memorizing. Our curriculum includes coding laboratories, real\ndatasets, and immediate application to engineering problems. We propose an\nempirical study to measure whether this approach improves both understanding\nand practical implementation skills compared to traditional teaching methods.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u677f\u7403\u5206\u6790\u6559\u5b66\u5f3a\u5316\u5b66\u4e60\u548c\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u65e8\u5728\u63d0\u5347\u5b66\u751f\u7684\u7406\u89e3\u4e0e\u5b9e\u8df5\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5de5\u7a0b\u6559\u80b2\u4e2d\u5b66\u751f\u5728\u8fde\u63a5\u62bd\u8c61\u6570\u5b66\u4e0e\u73b0\u5b9e\u5e94\u7528\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e3a\u671f12\u5468\u7684\u57fa\u4e8e\u5b9e\u8df5\u7684\u8bfe\u7a0b\uff0c\u7ed3\u5408\u4e86\u7f16\u7801\u5b9e\u9a8c\u5ba4\u548c\u771f\u5b9e\u6570\u636e\u96c6\u3002", "result": "\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\uff0c\u6d4b\u91cf\u8be5\u65b9\u6cd5\u5bf9\u5b66\u751f\u7406\u89e3\u548c\u5b9e\u8df5\u6280\u80fd\u7684\u6539\u5584\u3002", "conclusion": "\u5b66\u4e60\u57fa\u4e8e\u677f\u7403\u5206\u6790\u7684\u673a\u5668\u5b66\u4e60\u6982\u5ff5\u663e\u8457\u63d0\u9ad8\u4e86\u5b66\u751f\u7684\u7406\u89e3\u548c\u5b9e\u8df5\u80fd\u529b\u3002"}}
{"id": "2510.21751", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21751", "abs": "https://arxiv.org/abs/2510.21751", "authors": ["Van Nam Dinh", "Van Vy Phan", "Thai Son Dang", "Van Du Phan", "The Anh Mai", "Van Chuong Le", "Sy Phuong Ho", "Dinh Tu Duong", "Hung Cuong Ta"], "title": "Real-time Mixed-Integer Quadratic Programming for Driving Behavior-Inspired Speed Bump Optimal Trajectory Planning", "comment": null, "summary": "This paper proposes a novel methodology for trajectory planning in autonomous\nvehicles (AVs), addressing the complex challenge of negotiating speed bumps\nwithin a unified Mixed-Integer Quadratic Programming (MIQP) framework. By\nleveraging Model Predictive Control (MPC), we develop trajectories that\noptimize both the traversal of speed bumps and overall passenger comfort. A key\ncontribution of this work is the formulation of speed bump handling constraints\nthat closely emulate human driving behavior, seamlessly integrating these with\nbroader road navigation requirements. Through extensive simulations in varied\nurban driving environments, we demonstrate the efficacy of our approach,\nhighlighting its ability to ensure smooth speed transitions over speed bumps\nwhile maintaining computational efficiency suitable for real-time deployment.\nThe method's capability to handle both static road features and dynamic\nconstraints, alongside expert human driving, represents a significant step\nforward in trajectory planning for urban", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\uff0c\u5229\u7528MIQP\u548cMPC\u6709\u6548\u5904\u7406\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u901f\u5ea6\u7f13\u51b2\u5e26\u7684\u8868\u73b0\uff0c\u786e\u4fdd\u4e58\u5ba2\u8212\u9002\u4e0e\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u9762\u5bf9\u901f\u5ea6\u7f13\u51b2\u5e26\u65f6\u7684\u590d\u6742\u6027\uff0c\u5e76\u63d0\u9ad8\u6574\u4f53\u9053\u8def\u884c\u9a76\u7684\u6d41\u7545\u6027\u548c\u4e58\u5ba2\u8212\u9002\u5ea6\u3002", "method": "\u91c7\u7528\u6df7\u5408\u6574\u6570\u4e8c\u6b21\u7f16\u7a0b\uff08MIQP\uff09\u6846\u67b6\u4e0e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u76f8\u7ed3\u5408\u8fdb\u884c\u8f68\u8ff9\u89c4\u5212\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u57ce\u5e02\u9a7e\u9a76\u73af\u5883\u4e0b\u7684\u6a21\u62df\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u5e73\u6ed1\u901f\u5ea6\u8fc7\u6e21\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u7684\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4fdd\u8bc1\u4e58\u5ba2\u8212\u9002\u5ea6\u7684\u540c\u65f6\uff0c\u6709\u6548\u5e94\u5bf9\u4e86\u901f\u5ea6\u7f13\u51b2\u5e26\u7684\u6311\u6218\uff0c\u5e76\u5177\u5907\u5b9e\u65f6\u90e8\u7f72\u7684\u80fd\u529b\u3002"}}
{"id": "2510.22414", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.22414", "abs": "https://arxiv.org/abs/2510.22414", "authors": ["Mertcan Sevgi", "Fares Antaki", "Abdullah Zafar Khan", "Ariel Yuhan Ong", "David Adrian Merle", "Kuang Hu", "Shafi Balal", "Sophie-Christin Kornelia Ernst", "Josef Huemer", "Gabriel T. Kaufmann", "Hagar Khalid", "Faye Levina", "Celeste Limoli", "Ana Paula Ribeiro Reis", "Samir Touma", "Anil Palepu", "Khaled Saab", "Ryutaro Tanno", "Tao Tu", "Yong Cheng", "Mike Schaekermann", "S. Sara Mahdavi", "Elahe Vedadi", "David Stutz", "Vivek Natarajan", "Alan Karthikesalingam", "Pearse A. Keane", "Wei-Hung Weng"], "title": "Complementary Human-AI Clinical Reasoning in Ophthalmology", "comment": null, "summary": "Vision impairment and blindness are a major global health challenge where\ngaps in the ophthalmology workforce limit access to specialist care. We\nevaluate AMIE, a medically fine-tuned conversational system based on Gemini\nwith integrated web search and self-critique reasoning, using real-world\nclinical vignettes that reflect scenarios a general ophthalmologist would be\nexpected to manage. We conducted two complementary evaluations: (1) a human-AI\ninteractive diagnostic reasoning study in which ophthalmologists recorded\ninitial differentials and plans, then reviewed AMIE's structured output and\nrevised their answers; and (2) a masked preference and quality study comparing\nAMIE's narrative outputs with case author reference answers using a predefined\nrubric. AMIE showed standalone diagnostic performance comparable to clinicians\nat baseline. Crucially, after reviewing AMIE's responses, ophthalmologists\ntended to rank the correct diagnosis higher, reached greater agreement with one\nanother, and enriched their investigation and management plans. Improvements\nwere observed even when AMIE's top choice differed from or underperformed the\nclinician baseline, consistent with a complementary effect in which structured\nreasoning support helps clinicians re-rank rather than simply accept the model\noutput. Preferences varied by clinical grade, suggesting opportunities to\npersonalise responses by experience. Without ophthalmology-specific\nfine-tuning, AMIE matched clinician baseline and augmented clinical reasoning\nat the point of need, motivating multi-axis evaluation, domain adaptation, and\nprospective multimodal studies in real-world settings.", "AI": {"tldr": "AMIE\u662f\u4e00\u79cd\u57fa\u4e8eGemini\u7684\u5bf9\u8bdd\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e0e\u773c\u79d1\u533b\u751f\u7684\u4e92\u52a8\uff0c\u589e\u5f3a\u4e86\u4e34\u5e8a\u63a8\u7406\u80fd\u529b\uff0c\u8868\u73b0\u51fa\u4e0e\u4e34\u5e8a\u533b\u751f\u76f8\u5f53\u7684\u8bca\u65ad\u6027\u80fd\uff0c\u5e76\u4fc3\u6210\u4e86\u66f4\u4f18\u7684\u4e34\u5e8a\u51b3\u7b56\u3002", "motivation": "\u5e94\u5bf9\u773c\u79d1\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u65e8\u5728\u63d0\u9ad8\u89c6\u529b\u969c\u788d\u548c\u5931\u660e\u60a3\u8005\u7684\u533b\u7597\u83b7\u5f97\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u4e0e\u773c\u79d1\u533b\u751f\u7684\u4e92\u52a8\u8bca\u65ad\u63a8\u7406\u7814\u7a76\u548c\u504f\u597d\u8d28\u91cf\u7814\u7a76\uff0c\u8bc4\u4f30AMIE\u5728\u771f\u5b9e\u4e34\u5e8a\u60c5\u5883\u4e0b\u7684\u8868\u73b0\u3002", "result": "AMIE\u5728\u8bca\u65ad\u6027\u80fd\u4e0a\u4e0e\u4e34\u5e8a\u533b\u751f\u76f8\u5f53\uff0c\u5e76\u5e2e\u52a9\u773c\u79d1\u533b\u751f\u91cd\u65b0\u8bc4\u4f30\u548c\u4e30\u5bcc\u4ed6\u4eec\u7684\u7814\u7a76\u548c\u7ba1\u7406\u8ba1\u5212\u3002", "conclusion": "AMIE\u7684\u8868\u73b0\u4e0e\u4e34\u5e8a\u533b\u751f\u76f8\u5f53\uff0c\u5e76\u5728\u9700\u8981\u65f6\u589e\u5f3a\u4e86\u4e34\u5e8a\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u7684\u591a\u7ef4\u8bc4\u4f30\u3001\u9886\u57df\u9002\u5e94\u548c\u771f\u5b9e\u4e16\u754c\u7684\u524d\u77bb\u6027\u591a\u6a21\u6001\u7814\u7a76\u63d0\u4f9b\u4e86\u52a8\u673a\u3002"}}
{"id": "2510.21758", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21758", "abs": "https://arxiv.org/abs/2510.21758", "authors": ["Kumater Ter", "RexCharles Donatus", "Ore-Ofe Ajayi", "Daniel Udekwe"], "title": "Taxonomy and Trends in Reinforcement Learning for Robotics and Control Systems: A Structured Review", "comment": null, "summary": "Reinforcement learning (RL) has become a foundational approach for enabling\nintelligent robotic behavior in dynamic and uncertain environments. This work\npresents an in-depth review of RL principles, advanced deep reinforcement\nlearning (DRL) algorithms, and their integration into robotic and control\nsystems. Beginning with the formalism of Markov Decision Processes (MDPs), the\nstudy outlines essential elements of the agent-environment interaction and\nexplores core algorithmic strategies including actor-critic methods,\nvalue-based learning, and policy gradients. Emphasis is placed on modern DRL\ntechniques such as DDPG, TD3, PPO, and SAC, which have shown promise in solving\nhigh-dimensional, continuous control tasks. A structured taxonomy is introduced\nto categorize RL applications across domains such as locomotion, manipulation,\nmulti-agent coordination, and human-robot interaction, along with training\nmethodologies and deployment readiness levels. The review synthesizes recent\nresearch efforts, highlighting technical trends, design patterns, and the\ngrowing maturity of RL in real-world robotics. Overall, this work aims to\nbridge theoretical advances with practical implementations, providing a\nconsolidated perspective on the evolving role of RL in autonomous robotic\nsystems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u5f3a\u5316\u5b66\u4e60\u53ca\u5176\u5728\u673a\u5668\u4eba\u548c\u63a7\u5236\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u8fdb\u884c\u4e86\u5168\u9762\u56de\u987e\uff0c\u5f3a\u8c03\u4e86\u73b0\u4ee3\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8d8b\u52bf\u3002", "motivation": "\u5728\u52a8\u6001\u548c\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\uff0c\u589e\u5f3a\u667a\u80fd\u673a\u5668\u4eba\u884c\u4e3a\u7684\u80fd\u529b\u662f\u5f53\u524d\u7814\u7a76\u7684\u91cd\u8981\u4efb\u52a1\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\uff0c\u603b\u7ed3\u4e86\u5f3a\u5316\u5b66\u4e60\u7684\u539f\u5219\u3001\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u53ca\u5176\u5728\u5404\u79cd\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u5206\u7c7b\u3002", "result": "\u8be5\u7efc\u8ff0\u5f3a\u8c03\u4e86\u73b0\u4ee3\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u89e3\u51b3\u9ad8\u7ef4\u6301\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u603b\u7ed3\u4e86\u8fd1\u671f\u7814\u7a76\u7684\u6280\u672f\u8d8b\u52bf\u548c\u8bbe\u8ba1\u6a21\u5f0f\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u5728\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u4f5c\u7528\u4e0d\u65ad\u6f14\u53d8\uff0c\u7406\u8bba\u8fdb\u5c55\u4e0e\u5b9e\u8df5\u5b9e\u65bd\u4e4b\u95f4\u7684\u7ed3\u5408\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.22442", "categories": ["cs.HC", "H.5.2"], "pdf": "https://arxiv.org/pdf/2510.22442", "abs": "https://arxiv.org/abs/2510.22442", "authors": ["J. R. Rojano-C\u00e1ceres", "A. Rivera-Hern\u00e1ndez"], "title": "SignaApp a modern alternative to support signwriting notation for sign languages", "comment": "13 pages, 11 figures, Conference: VIII International Conference on\n  Inclusive Technologies and Education, 2025", "summary": "The present work reports the development of an application called Signa App,\nwhich was designed following the philosophy of User-Centered De-sign. Signa App\naims to provide a mobile platform for editing and creating texts in SignWriting\nnotation. The proposal was based on the lack of a mo-bile application that is\nusable for Deaf individuals who use sign language. The application was tested\nwith adults, children, and adolescents, and the results showed a high degree of\nacceptance and ease of use. The app has al-ready been introduced to the\nSignWriting user community, receiving posi-tive feedback. Likewise, the\napplication is available on the Google Play Store", "AI": {"tldr": "Signa App\u662f\u4e00\u6b3e\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u8bbe\u8ba1\u7684\u79fb\u52a8\u5e94\u7528\uff0c\u65e8\u5728\u4e3a\u4f7f\u7528\u624b\u8bed\u7684\u804b\u4eba\u63d0\u4f9b\u6587\u672c\u7f16\u8f91\u548c\u521b\u4f5c\u5e73\u53f0\uff0c\u5df2\u83b7\u5f97\u7528\u6237\u79ef\u6781\u53cd\u9988\u3002", "motivation": "\u54cd\u5e94\u804b\u4eba\u5bf9\u53ef\u7528\u624b\u8bed\u6587\u672c\u7f16\u8f91\u5de5\u5177\u7684\u9700\u6c42\uff0c\u4ee5\u586b\u8865\u5e02\u573a\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7528\u6237\u4e2d\u5fc3\u8bbe\u8ba1\u7684\u7406\u5ff5\u5f00\u53d1\uff0c\u8fdb\u884c\u5e7f\u6cdb\u7684\u7528\u6237\u6d4b\u8bd5\u4ee5\u8bc4\u4f30\u63a5\u53d7\u5ea6\u548c\u6613\u7528\u6027\u3002", "result": "\u6d4b\u8bd5\u7ed3\u679c\u663e\u793aSigna App\u88ab\u5404\u5e74\u9f84\u6bb5\u7528\u6237\u5e7f\u6cdb\u63a5\u53d7\uff0c\u4f7f\u7528\u65b9\u4fbf\uff0c\u5e76\u5df2\u5728Google Play\u5546\u5e97\u4e0a\u7ebf\u3002", "conclusion": "Signa App\u6210\u529f\u6ee1\u8db3\u4e86\u804b\u4eba\u7fa4\u4f53\u5bf9\u624b\u8bed\u6587\u672c\u7f16\u8f91\u5de5\u5177\u7684\u9700\u6c42\uff0c\u5e76\u5728\u7528\u6237\u793e\u533a\u4e2d\u83b7\u5f97\u4e86\u79ef\u6781\u7684\u6b22\u8fce\u3002"}}
{"id": "2510.21761", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.21761", "abs": "https://arxiv.org/abs/2510.21761", "authors": ["Jesse Atuhurra", "Hidetaka Kamigaito", "Taro Watanabe", "Koichiro Yoshino"], "title": "J-ORA: A Framework and Multimodal Dataset for Japanese Object Identification, Reference, Action Prediction in Robot Perception", "comment": "Accepted to IROS2025", "summary": "We introduce J-ORA, a novel multimodal dataset that bridges the gap in robot\nperception by providing detailed object attribute annotations within Japanese\nhuman-robot dialogue scenarios. J-ORA is designed to support three critical\nperception tasks, object identification, reference resolution, and next-action\nprediction, by leveraging a comprehensive template of attributes (e.g.,\ncategory, color, shape, size, material, and spatial relations). Extensive\nevaluations with both proprietary and open-source Vision Language Models (VLMs)\nreveal that incorporating detailed object attributes substantially improves\nmultimodal perception performance compared to without object attributes.\nDespite the improvement, we find that there still exists a gap between\nproprietary and open-source VLMs. In addition, our analysis of object\naffordances demonstrates varying abilities in understanding object\nfunctionality and contextual relationships across different VLMs. These\nfindings underscore the importance of rich, context-sensitive attribute\nannotations in advancing robot perception in dynamic environments. See project\npage at https://jatuhurrra.github.io/J-ORA/.", "AI": {"tldr": "J-ORA\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u8be6\u7ec6\u7684\u7269\u4f53\u5c5e\u6027\u6ce8\u91ca\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u611f\u77e5\u80fd\u529b\uff0c\u8bc4\u4f30\u663e\u793a\u8be6\u7ec6\u5c5e\u6027\u7684\u5f15\u5165\u663e\u8457\u589e\u5f3a\u4e86\u611f\u77e5\u6027\u80fd\u3002", "motivation": "\u586b\u8865\u673a\u5668\u4eba\u611f\u77e5\u9886\u57df\u7684\u7a7a\u767d\uff0c\u63d0\u5347\u5bf9\u7269\u4f53\u8bc6\u522b\u3001\u53c2\u8003\u89e3\u6790\u548c\u4e0b\u4e00\u6b65\u52a8\u4f5c\u9884\u6d4b\u7684\u80fd\u529b\u3002", "method": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6J-ORA\uff0c\u4e13\u6ce8\u4e8e\u673a\u5668\u4eba\u611f\u77e5\u7684\u8be6\u7ec6\u7269\u4f53\u5c5e\u6027\u6ce8\u91ca\uff0c\u7279\u522b\u662f\u5728\u65e5\u672c\u4eba\u673a\u5bf9\u8bdd\u573a\u666f\u4e2d\u3002", "result": "\u901a\u8fc7\u4e0e\u4e13\u6709\u548c\u5f00\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u8be6\u7ec6\u7684\u7269\u4f53\u5c5e\u6027\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u591a\u6a21\u6001\u611f\u77e5\u6027\u80fd\uff0c\u5c3d\u7ba1\u4ecd\u5b58\u5728\u4e13\u6709\u6a21\u578b\u4e0e\u5f00\u6e90\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u5bf9\u7269\u4f53\u529f\u80fd\u7406\u89e3\u7684\u5dee\u5f02\u505a\u4e86\u5206\u6790\u3002", "conclusion": "\u5bcc\u6709\u60c5\u5883\u611f\u7684\u5c5e\u6027\u6ce8\u91ca\u5728\u63a8\u8fdb\u52a8\u6001\u73af\u5883\u4e2d\u673a\u5668\u4eba\u611f\u77e5\u65b9\u9762\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.22498", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.22498", "abs": "https://arxiv.org/abs/2510.22498", "authors": ["Muhammad Irfan", "Anum Nawaz", "Ayse Kosal Bulbul", "Riku Klen", "Abdulhamit Subasi", "Tomi Westerlund", "Wei Chen"], "title": "Emotion Recognition with Minimal Wearable Sensing: Multi-domain Feature, Hybrid Feature Selection, and Personalized vs. Generalized Ensemble Model Analysis", "comment": null, "summary": "Negative emotions are linked to the onset of neurodegenerative diseases and\ndementia, yet they are often difficult to detect through observation.\nPhysiological signals from wearable devices offer a promising noninvasive\nmethod for continuous emotion monitoring. In this study, we propose a\nlightweight, resource-efficient machine learning approach for binary emotion\nclassification, distinguishing between negative (sadness, disgust, anger) and\npositive (amusement, tenderness, gratitude) affective states using only\nelectrocardiography (ECG) signals. The method is designed for deployment in\nresource-constrained systems, such as Internet of Things (IoT) devices, by\nreducing battery consumption and cloud data transmission through the avoidance\nof computationally expensive multimodal inputs. We utilized ECG data from 218\nCSV files extracted from four studies in the Psychophysiology of Positive and\nNegative Emotions (POPANE) dataset, which comprises recordings from 1,157\nhealthy participants across seven studies. Each file represents a unique\nsubject emotion, and the ECG signals, recorded at 1000 Hz, were segmented into\n10-second epochs to reflect real-world usage. Our approach integrates\nmultidomain feature extraction, selective feature fusion, and a voting\nclassifier. We evaluated it using a participant-exclusive generalized model and\na participant-inclusive personalized model. The personalized model achieved the\nbest performance, with an average accuracy of 95.59%, outperforming the\ngeneralized model, which reached 69.92% accuracy. Comparisons with other\nstudies on the POPANE and similar datasets show that our approach consistently\noutperforms existing methods. This work highlights the effectiveness of\npersonalized models in emotion recognition and their suitability for wearable\napplications that require accurate, low-power, and real-time emotion tracking.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fc3\u7535\u56fe\u4fe1\u53f7\u8fdb\u884c\u60c5\u611f\u5206\u7c7b\uff0c\u5c55\u793a\u4e86\u4e2a\u6027\u5316\u6a21\u578b\u5728\u53ef\u7a7f\u6234\u8bbe\u5907\u5b9e\u65f6\u60c5\u611f\u8ffd\u8e2a\u4e2d\u7684\u6709\u6548\u6027\u548c\u4f4e\u529f\u8017\u7279\u70b9\u3002", "motivation": "\u7531\u4e8e\u8d1f\u9762\u60c5\u611f\u4e0e\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u53ca\u75f4\u5446\u7684\u53d1\u751f\u76f8\u5173\uff0c\u4f7f\u7528\u53ef\u7a7f\u6234\u8bbe\u5907\u7684\u751f\u7406\u4fe1\u53f7\u8fdb\u884c\u60c5\u611f\u76d1\u6d4b\u663e\u5f97\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7\u673a\u5668\u5b66\u4e60\u5bf9\u5fc3\u7535\u56fe\u4fe1\u53f7\u8fdb\u884c\u4e8c\u5143\u60c5\u611f\u5206\u7c7b\uff0c\u5c06\u8d1f\u9762\u60c5\u611f\u4e0e\u6b63\u9762\u60c5\u611f\u533a\u5206\u5f00\u6765\uff0c\u8bbe\u8ba1\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684IoT\u8bbe\u5907\u3002", "result": "\u4e2a\u6027\u5316\u6a21\u578b\u7684\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u523095.59%\uff0c\u663e\u8457\u4f18\u4e8e69.92%\u7684\u901a\u7528\u6a21\u578b\uff0c\u4e14\u5728\u4e0e\u5176\u4ed6\u7814\u7a76\u7684\u6bd4\u8f83\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u4e2a\u6027\u5316\u6a21\u578b\u5728\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u6709\u6548\u6027\u7a81\u51fa\uff0c\u9002\u5408\u4f4e\u529f\u8017\u5b9e\u65f6\u60c5\u611f\u8ffd\u8e2a\u7684\u53ef\u7a7f\u6234\u5e94\u7528\u3002"}}
{"id": "2510.21771", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21771", "abs": "https://arxiv.org/abs/2510.21771", "authors": ["Dharunish Yugeswardeenoo"], "title": "Improving the performance of AI-powered Affordable Robotics for Assistive Tasks", "comment": "6 pages, 5 figures. Accepted to Conference on Robot Learning (CoRL\n  2025), Seoul, Korea", "summary": "By 2050, the global demand for assistive care is expected to reach 3.5\nbillion people, far outpacing the availability of human caregivers. Existing\nrobotic solutions remain expensive and require technical expertise, limiting\naccessibility. This work introduces a low-cost robotic arm for assistive tasks\nsuch as feeding, cleaning spills, and fetching medicine. The system uses\nimitation learning from demonstration videos, requiring no task-specific\nprogramming or manual labeling. The robot consists of six servo motors, dual\ncameras, and 3D-printed grippers. Data collection via teleoperation with a\nleader arm yielded 50,000 video frames across the three tasks. A novel Phased\nAction Chunking Transformer (PACT) captures temporal dependencies and segments\nmotion dynamics, while a Temporal Ensemble (TE) method refines trajectories to\nimprove accuracy and smoothness. Evaluated across five model sizes and four\narchitectures, with ten hours of real-world testing, the system achieved over\n90% task accuracy, up to 40% higher than baselines. PACT enabled a 5x model\nsize reduction while maintaining 75% accuracy. Saliency analysis showed\nreliance on key visual cues, and phase token gradients peaked at critical\ntrajectory moments, indicating effective temporal reasoning. Future work will\nexplore bimanual manipulation and mobility for expanded assistive capabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4f4e\u6210\u672c\u7684\u673a\u5668\u4eba\u81c2\uff0c\u5b9e\u73b0\u8f85\u52a9\u4efb\u52a1\uff0c\u6709\u6548\u63d0\u5347\u4efb\u52a1\u51c6\u786e\u7387\uff0c\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u3002", "motivation": "\u52302050\u5e74\uff0c\u5168\u7403\u5bf9\u8f85\u52a9\u62a4\u7406\u7684\u9700\u6c42\u5c06\u8fbe\u523035\u4ebf\u4eba\uff0c\u8fdc\u8d85\u4eba\u7c7b\u770b\u62a4\u8005\u7684\u4f9b\u5e94\u3002\u73b0\u6709\u7684\u673a\u5668\u4eba\u89e3\u51b3\u65b9\u6848\u6602\u8d35\u4e14\u9700\u8981\u6280\u672f\u4e13\u4e1a\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u53ef\u83b7\u53d6\u6027\u3002", "method": "\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u548c\u89c6\u9891\u6f14\u793a\u6784\u5efa\u4f4e\u6210\u672c\u673a\u5668\u4eba\u81c2\uff0c\u8fdb\u884c\u8f85\u52a9\u4efb\u52a1", "result": "\u8be5\u7cfb\u7edf\u5728\u4e94\u79cd\u6a21\u578b\u5927\u5c0f\u548c\u56db\u79cd\u67b6\u6784\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ecf\u8fc7\u5341\u5c0f\u65f6\u7684\u771f\u5b9e\u6d4b\u8bd5\uff0c\u4efb\u52a1\u51c6\u786e\u7387\u8d85\u8fc790%\uff0c\u6bd4\u57fa\u7ebf\u9ad8\u51fa40%\u3002PACT\u5b9e\u73b0\u4e86\u6a21\u578b\u5c3a\u5bf8\u76845\u500d\u7f29\u51cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e8675%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u672a\u6765\u7684\u5de5\u4f5c\u5c06\u63a2\u8ba8\u53cc\u624b\u64cd\u4f5c\u548c\u79fb\u52a8\u80fd\u529b\uff0c\u4ee5\u6269\u5c55\u8f85\u52a9\u529f\u80fd\u3002"}}
{"id": "2510.22610", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.22610", "abs": "https://arxiv.org/abs/2510.22610", "authors": ["Damien Rudaz", "Mathias Broth", "Jakub Mlynar"], "title": "Everything counts: the managed omnirelevance of speech in 'human - voice agent' interaction", "comment": null, "summary": "To this day, turn-taking models determining voice agents' conduct have been\nexamined from a technical point of view, while the interactional constraints or\nresources they constitute for human conversationalists have not been\nempirically described. From the detailed analysis of corpora of naturalistic\ndata, we document how, whether in interaction with rule-based robots from a\n'pre-LLM era' or with the most recent voice agents, humans' conduct was\nproduced in reference to the ever-present risk that, each time they spoke,\ntheir talk may trigger a new uncalled-for contribution from the artificial\nagent. We argue that this 'omnirelevance of human speech' is a constitutive\nfeature of current human-agent interaction that, due to recent improvements in\nvoice capture technology, weighs on human practices even more today than in the\npast. Specifically, we document how, in multiparty settings, humans shaped\ntheir conduct in such a way as to remain undetected by the machine's sensors.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4eba\u7c7b\u4e0e\u8bed\u97f3\u4ee3\u7406\u4e92\u52a8\u65f6\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u5f3a\u8c03\u4eba\u7c7b\u5728\u5bf9\u8bdd\u4e2d\u9762\u4e34\u7684\u6f5c\u5728\u5e72\u6270\u3002", "motivation": "\u5e0c\u671b\u63ed\u793a\u8bed\u97f3\u4ee3\u7406\u5728\u5bf9\u8bdd\u4e2d\u6240\u5e26\u6765\u7684\u4e92\u52a8\u9650\u5236\uff0c\u4ee5\u53ca\u4eba\u7c7b\u5982\u4f55\u8c03\u6574\u81ea\u5df1\u7684\u53cd\u5e94\u4ee5\u9002\u5e94\u8fd9\u4e9b\u6280\u672f\u3002", "method": "\u901a\u8fc7\u5bf9\u81ea\u7136\u6570\u636e\u8bed\u6599\u5e93\u7684\u8be6\u7ec6\u5206\u6790\uff0c\u89c2\u5bdf\u4eba\u7c7b\u5728\u4e0e\u4e0d\u540c\u58f0\u63a7\u4ee3\u7406\u7684\u4e92\u52a8\u4e2d\u7684\u884c\u4e3a\u3002", "result": "\u4eba\u7c7b\u5728\u591a\u65b9\u8bbe\u7f6e\u4e2d\u8c03\u6574\u81ea\u5df1\u7684\u8a00\u8bed\u884c\u4e3a\uff0c\u4ee5\u907f\u5f00\u673a\u5668\u7684\u611f\u5e94\uff0c\u53cd\u6620\u51fa\u4eba\u7c7b\u4e0e\u673a\u5668\u4e92\u52a8\u4e2d\u7684\u590d\u6742\u6027\u3002", "conclusion": "\u4eba\u7c7b\u4e0e\u8bed\u97f3\u4ee3\u7406\u7684\u4e92\u52a8\u4e2d\uff0c\u4eba\u7c7b\u884c\u4e3a\u53d7\u5230\u673a\u5668\u53cd\u5e94\u7684\u6301\u7eed\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5728\u591a\u65b9\u5bf9\u8bdd\u4e2d\u3002"}}
{"id": "2510.21773", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21773", "abs": "https://arxiv.org/abs/2510.21773", "authors": ["Van Nam Dinh"], "title": "Real-Time QP Solvers: A Concise Review and Practical Guide Towards Legged Robots", "comment": "6 pages, 1 figure, 2 tables", "summary": "Quadratic programming (QP) underpins real-time robotics by enabling\nefficient, constrained optimization in state estimation, motion planning, and\ncontrol. In legged locomotion and manipulation, essential modules like inverse\ndynamics, Model Predictive Control (MPC), and Whole-Body Control (WBC) are\ninherently QP-based, demanding reliable solutions amid tight timing, energy,\nand computational limits on embedded platforms. This paper presents a\ncomprehensive analysis and benchmarking study of cutting-edge QP solvers for\nlegged robotics. We begin by formulating the standard convex QP and classify\nsolvers into four principal algorithmic approaches, including interior-point\nmethods, active-set strategies, operator splitting schemes, and augmented\nLagrangian approaches. Each solver is examined in terms of algorithmic\nstructure, computational characteristics, and its ability to exploit problem\nstructure and warm-starting. Performance is evaluated using publicly available\nbenchmarks, focusing on metrics such as computation time, constraint\nsatisfaction, and robustness under perturbations. Feature tables and\ncomparisons yield practical guidance for solver selection, underscoring\ntrade-offs in speed, accuracy, and energy efficiency. Our findings emphasize\nthe synergy between solver, task, and hardware, sparse IPMs for long-horizon\nMPC, and dense active-set for high frequency WBC to advance agile, autonomous\nlegged systems, with emerging extensions to nonconvex and distributed QP.", "AI": {"tldr": "\u672c\u6587\u5bf9\u817f\u90e8\u673a\u5668\u4eba\u4e2d\u7684QP\u6c42\u89e3\u5668\u8fdb\u884c\u5206\u6790\u4e0e\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u4e0d\u540c\u6c42\u89e3\u5668\u7684\u6027\u80fd\u7279\u70b9\uff0c\u5f3a\u8c03\u4e86\u6c42\u89e3\u5668\u4e0e\u4efb\u52a1\u53ca\u786c\u4ef6\u4e4b\u95f4\u7684\u534f\u540c\u4f5c\u7528\u3002", "motivation": "\u7814\u7a76\u5173\u952e\u7684QP\u7b97\u6cd5\u4ee5\u63d0\u5347\u817f\u90e8\u673a\u5668\u4eba\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u7684\u6027\u80fd\uff0c\u6ee1\u8db3\u65f6\u95f4\u3001\u80fd\u6e90\u548c\u8ba1\u7b97\u9650\u5236\u3002", "method": "\u901a\u8fc7\u6807\u5b9a\u6807\u51c6\u51f8QP\uff0c\u5206\u7c7b\u6c42\u89e3\u5668\u4e3a\u5185\u90e8\u70b9\u6cd5\u3001\u6d3b\u8dc3\u96c6\u7b56\u7565\u3001\u7b97\u5b50\u5206\u79bb\u65b9\u6848\u53ca\u589e\u5e7f\u62c9\u683c\u6717\u65e5\u6cd5\uff0c\u5e76\u5728\u516c\u5171\u57fa\u51c6\u4e0a\u8bc4\u4f30\u8ba1\u7b97\u65f6\u95f4\u3001\u7ea6\u675f\u6ee1\u8db3\u4e0e\u7a33\u5065\u6027\u7b49\u6307\u6807\u3002", "result": "\u63d0\u4f9b\u4e86\u5168\u9762\u7684QP\u6c42\u89e3\u5668\u5206\u6790\u4e0e\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e3a\u817f\u90e8\u673a\u5668\u4eba\u4f18\u5316\u9009\u62e9\u5408\u9002\u7684\u6c42\u89e3\u5668\u63d0\u4f9b\u5b9e\u9645\u6307\u5bfc\u3002", "conclusion": "\u53d1\u73b0\u7a00\u758f\u5185\u90e8\u70b9\u6cd5\u9002\u5408\u957f\u65f6\u95f4\u8de8\u5ea6\u7684MPC\uff0c\u800c\u7a20\u5bc6\u7684\u6d3b\u8dc3\u96c6\u5408\u65b9\u6cd5\u9002\u7528\u4e8e\u9ad8\u9891\u7684WBC\uff0c\u63a8\u52a8\u7075\u6d3b\u81ea\u4e3b\u7684\u817f\u90e8\u7cfb\u7edf\u53d1\u5c55\u3002"}}
{"id": "2510.22857", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.22857", "abs": "https://arxiv.org/abs/2510.22857", "authors": ["Naisha Agarwal", "Judith Amores", "Andrew D. Wilson"], "title": "Storycaster: An AI System for Immersive Room-Based Storytelling", "comment": null, "summary": "While Cave Automatic Virtual Environment (CAVE) systems have long enabled\nroom-scale virtual reality and various kinds of interactivity, their content\nhas largely remained predetermined. We present \\textit{Storycaster}, a\ngenerative AI CAVE system that transforms physical rooms into responsive\nstorytelling environments. Unlike headset-based VR, \\textit{Storycaster}\npreserves spatial awareness, using live camera feeds to augment the walls with\ncylindrical projections, allowing users to create worlds that blend with their\nphysical surroundings. Additionally, our system enables object-level editing,\nwhere physical items in the room can be transformed to their virtual\ncounterparts in a story. A narrator agent guides participants, enabling them to\nco-create stories that evolve in response to voice commands, with each scene\nenhanced by generated ambient audio, dialogue, and imagery. Participants in our\nstudy ($n=13$) found the system highly immersive and engaging, with narrator\nand audio most impactful, while also highlighting areas for improvement in\nlatency and image resolution.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aStorycaster\u7684\u751f\u6210\u5f0fAI CAVE\u7cfb\u7edf\uff0c\u80fd\u5c06\u7269\u7406\u7a7a\u95f4\u8f6c\u53d8\u4e3a\u54cd\u5e94\u5f0f\u6545\u4e8b\u8bb2\u8ff0\u73af\u5883\u3002", "motivation": "\u63d0\u9ad8CAVE\u7cfb\u7edf\u7684\u4e92\u52a8\u6027\u548c\u9002\u5e94\u6027\uff0c\u4f7f\u5176\u5185\u5bb9\u4e0d\u518d\u662f\u9884\u8bbe\uff0c\u800c\u662f\u7528\u6237\u5b9e\u65f6\u521b\u9020\u3002", "method": "\u901a\u8fc7\u5229\u7528\u5b9e\u65f6\u6444\u50cf\u5934\u8f93\u5165\u548c\u5706\u67f1\u5f62\u6295\u5f71\uff0c\u8be5\u7cfb\u7edf\u5141\u8bb8\u7528\u6237\u5728\u7269\u7406\u73af\u5883\u4e2d\u8fdb\u884c\u6545\u4e8b\u521b\u4f5c\u3002", "result": "\u53c2\u4e0e13\u4eba\u7684\u7814\u7a76\u8868\u660e\uff0c\u7528\u6237\u5bf9\u7cfb\u7edf\u7684\u6c89\u6d78\u611f\u548c\u53c2\u4e0e\u5ea6\u8bc4\u4ef7\u8f83\u9ad8\uff0c\u5c24\u5176\u662f\u53d9\u8ff0\u8005\u4e0e\u97f3\u9891\u5f71\u54cd\u6700\u663e\u8457\u3002", "conclusion": "\u6b64\u7cfb\u7edf\u88ab\u53c2\u4e0e\u8005\u8ba4\u4e3a\u5177\u6709\u9ad8\u5ea6\u6c89\u6d78\u611f\u548c\u5438\u5f15\u529b\uff0c\u4f46\u5728\u5ef6\u8fdf\u548c\u56fe\u50cf\u5206\u8fa8\u7387\u4e0a\u6709\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2510.21817", "categories": ["cs.RO", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21817", "abs": "https://arxiv.org/abs/2510.21817", "authors": ["Xiaoyu Liu", "Chaoyou Fu", "Chi Yan", "Chu Wu", "Haihan Gao", "Yi-Fan Zhang", "Shaoqi Dong", "Cheng Qian", "Bin Luo", "Xiuyong Yang", "Guanwu Li", "Yusheng Cai", "Yunhang Shen", "Deqiang Jiang", "Haoyu Cao", "Xing Sun", "Caifeng Shan", "Ran He"], "title": "VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing, Speaking, and Acting", "comment": "Homepage: https://lxysl.github.io/VITA-E/", "summary": "Current Vision-Language-Action (VLA) models are often constrained by a rigid,\nstatic interaction paradigm, which lacks the ability to see, hear, speak, and\nact concurrently as well as handle real-time user interruptions dynamically.\nThis hinders seamless embodied collaboration, resulting in an inflexible and\nunresponsive user experience. To address these limitations, we introduce\nVITA-E, a novel embodied interaction framework designed for both behavioral\nconcurrency and nearly real-time interruption. The core of our approach is a\ndual-model architecture where two parallel VLA instances operate as an ``Active\nModel'' and a ``Standby Model'', allowing the embodied agent to observe its\nenvironment, listen to user speech, provide verbal responses, and execute\nactions, all concurrently and interruptibly, mimicking human-like multitasking\ncapabilities. We further propose a ``model-as-controller'' paradigm, where we\nfine-tune the VLM to generate special tokens that serve as direct system-level\ncommands, coupling the model's reasoning with the system's behavior.\nExperiments conducted on a physical humanoid platform demonstrate that VITA-E\ncan reliably handle complex interactive scenarios. Our framework is compatible\nwith various dual-system VLA models, achieving an extremely high success rate\non emergency stops and speech interruptions while also successfully performing\nconcurrent speech and action. This represents a significant step towards more\nnatural and capable embodied assistants.", "AI": {"tldr": "VITA-E\u662f\u4e00\u4e2a\u65b0\u7684\u5177\u8eab\u4ea4\u4e92\u6846\u67b6\uff0c\u652f\u6301\u884c\u4e3a\u5e76\u53d1\u548c\u5b9e\u65f6\u7528\u6237\u5e72\u9884\uff0c\u5177\u6709\u9ad8\u6210\u529f\u7387\u7684\u590d\u6742\u573a\u666f\u5904\u7406\u80fd\u529b\uff0c\u63d0\u5347\u4e86\u5177\u8eab\u52a9\u624b\u7684\u81ea\u7136\u6027\u548c\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u53d7\u9650\u4e8e\u9759\u6001\u4ea4\u4e92\u65b9\u5f0f\uff0c\u7f3a\u4e4f\u5e76\u53d1\u5904\u7406\u7528\u6237\u6307\u4ee4\u7684\u80fd\u529b\uff0c\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\uff0c\u56e0\u800c\u9700\u8981\u4e00\u4e2a\u7075\u6d3b\u7684\u4ea4\u4e92\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u6a21\u578b\u67b6\u6784\uff0cVLA\u5b9e\u4f8b\u5206\u4e3a'\u4e3b\u52a8\u6a21\u578b'\u548c'\u5907\u7528\u6a21\u578b'\uff0c\u5b9e\u73b0\u884c\u4e3a\u5e76\u53d1\u4e0e\u7528\u6237\u5e72\u9884\uff0c\u5e76\u7ed3\u5408'\u6a21\u578b\u4f5c\u4e3a\u63a7\u5236\u5668'\u7684\u8303\u5f0f\u4f18\u5316\u7cfb\u7edf\u547d\u4ee4\u751f\u6210\u3002", "result": "\u901a\u8fc7\u5728\u7269\u7406\u4eba\u5f62\u5e73\u53f0\u4e0a\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660eVITA-E\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u4ea4\u4e92\u573a\u666f\uff0c\u5e76\u5728\u7d27\u6025\u505c\u673a\u548c\u8bed\u97f3\u5e72\u9884\u4e2d\u8868\u73b0\u51fa\u6781\u9ad8\u7684\u6210\u529f\u7387\uff0c\u540c\u65f6\u5b9e\u73b0\u8bed\u97f3\u4e0e\u52a8\u4f5c\u7684\u5e76\u53d1\u6267\u884c\u3002", "conclusion": "VITA-E\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u5177\u8eab\u52a9\u624b\u7684\u4ea4\u4e92\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u81ea\u7136\u7684\u884c\u4e3a\u5e76\u53d1\u548c\u5b9e\u65f6\u7528\u6237\u5e72\u9884\u5904\u7406\uff0c\u5c55\u793a\u4e86\u5176\u5728\u590d\u6742\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u53ef\u9760\u6027\u548c\u9ad8\u6210\u529f\u7387\u3002"}}
{"id": "2510.22922", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.22922", "abs": "https://arxiv.org/abs/2510.22922", "authors": ["Runtao Zhou", "Giang Nguyen", "Nikita Kharya", "Anh Nguyen", "Chirag Agarwal"], "title": "Improving Human Verification of LLM Reasoning through Interactive Explanation Interfaces", "comment": "19 pages, 14 figures", "summary": "The reasoning capabilities of Large Language Models (LLMs) have led to their\nincreasing employment in several critical applications, particularly education,\nwhere they support problem-solving, tutoring, and personalized study. While\nthere are a plethora of works showing the effectiveness of LLMs in generating\nstep-by-step solutions through chain-of-thought (CoT) reasoning on reasoning\nbenchmarks, little is understood about whether the generated CoT is helpful for\nend-users in improving their ability to comprehend mathematical reasoning\nproblems and detect errors/hallucinations in LLM-generated solutions. To\naddress this gap and contribute to understanding how reasoning can improve\nhuman-AI interaction, we present three new interactive reasoning interfaces:\ninteractive CoT (iCoT), interactive Program-of-Thought (iPoT), and interactive\nGraph (iGraph), and a novel framework that generates the LLM's reasoning from\ntraditional CoT to alternative, interactive formats. Across 125 participants,\nwe found that interactive interfaces significantly improved performance.\nSpecifically, the iGraph interface yielded the highest clarity and error\ndetection rate (85.6%), followed by iPoT (82.5%), iCoT (80.6%), all\noutperforming standard CoT (73.5%). Interactive interfaces also led to faster\nresponse times, where participants using iGraph were fastest (57.9 secs),\ncompared to iCoT and iPoT (60 secs), and the standard CoT baseline (64.7 secs).\nFurthermore, participants preferred the iGraph reasoning interface, citing its\nsuperior ability to enable users to follow the LLM's reasoning process. We\ndiscuss the implications of these results and provide recommendations for the\nfuture design of reasoning models.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e09\u79cd\u4ea4\u4e92\u5f0f\u63a8\u7406\u63a5\u53e3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662fiGraph\u63a5\u53e3\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u586b\u8865\u5bf9\u751f\u6210\u7684\u63a8\u7406\u8fc7\u7a0b\u662f\u5426\u5bf9\u7528\u6237\u6709\u5e2e\u52a9\u7684\u77e5\u8bc6\u7a7a\u767d\uff0c\u4fc3\u8fdb\u4eba\u673a\u4e92\u52a8\u7684\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u4e0e125\u540d\u53c2\u4e0e\u8005\u7684\u4e92\u52a8\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e09\u79cd\u65b0\u4ea4\u4e92\u63a8\u7406\u63a5\u53e3\u7684\u6548\u679c\u548c\u54cd\u5e94\u65f6\u95f4\u3002", "result": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u65b0\u5f00\u53d1\u7684\u4e09\u79cd\u4ea4\u4e92\u5f0f\u63a8\u7406\u63a5\u53e3\uff08iCoT, iPoT, iGraph\uff09\u53ca\u5176\u5bf9\u7528\u6237\u7406\u89e3\u548c\u68c0\u6d4b\u6570\u5b66\u63a8\u7406\u95ee\u9898\u7684\u6709\u6548\u6027\u3002\u901a\u8fc7\u5bf9125\u540d\u53c2\u4e0e\u8005\u7684\u5b9e\u9a8c\uff0c\u53d1\u73b0\u4ea4\u4e92\u5f0f\u754c\u9762\u6bd4\u4f20\u7edf\u7684\u94fe\u5f0f\u63a8\u7406\uff08CoT\uff09\u663e\u8457\u63d0\u9ad8\u4e86\u7528\u6237\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662fiGraph\u754c\u9762\u7684\u6e05\u6670\u5ea6\u548c\u9519\u8bef\u68c0\u6d4b\u7387\u6700\u9ad8\uff0c\u4e14\u54cd\u5e94\u65f6\u95f4\u66f4\u5feb\u3002", "conclusion": "\u4ea4\u4e92\u5f0f\u63a8\u7406\u63a5\u53e3\u53ef\u4ee5\u663e\u8457\u6539\u5584\u4eba\u7c7b\u7528\u6237\u5728\u7406\u89e3\u548c\u8bc4\u4f30LLM\u751f\u6210\u89e3\u51b3\u65b9\u6848\u4e2d\u7684\u80fd\u529b\uff0c\u672a\u6765\u5e94\u5728\u63a8\u7406\u6a21\u578b\u8bbe\u8ba1\u4e2d\u8003\u8651\u8fd9\u4e9b\u4ea4\u4e92\u6280\u672f\u3002"}}
{"id": "2510.21854", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21854", "abs": "https://arxiv.org/abs/2510.21854", "authors": ["Sourabh Karmakar", "Cameron J. Turner"], "title": "A Literature Review On Stewart-Gough Platform Calibrations A Literature Review On Stewart-Gough Platform Calibrations", "comment": null, "summary": "Researchers have studied Stewart-Gough platforms, also known as Gough-Stewart\nplatforms or hexapod platforms extensively for their inherent fine control\ncharacteristics. Their studies led to the potential deployment opportunities of\nStewart-Gough Platforms in many critical applications such as the medical\nfield, engineering machines, space research, electronic chip manufacturing,\nautomobile manufacturing, etc. Some of these applications need micro and\nnano-level movement control in 3D space for the motions to be precise,\ncomplicated, and repeatable; a Stewart-Gough platform fulfills these challenges\nsmartly. For this, the platform must be more accurate than the specified\napplication accuracy level and thus proper calibration for a parallel robot is\ncrucial. Forward kinematics-based calibration for these hexapod machines\nbecomes unnecessarily complex and inverse kinematics complete this task with\nmuch ease. To experiment with different calibration techniques, various\ncalibration approaches were implemented by using external instruments,\nconstraining one or more motions of the system, and using extra sensors for\nauto or self-calibration. This survey paid attention to those key\nmethodologies, their outcome, and important details related to inverse\nkinematic-based parallel robot calibrations. It was observed during this study\nthat the researchers focused on improving the accuracy of the platform position\nand orientation considering the errors contributed by one source or multiple\nsources. The error sources considered are mainly kinematic and structural, in\nsome cases, environmental factors also are reviewed, however, those\ncalibrations are done under no-load conditions. This study aims to review the\npresent state of the art in this field and highlight the processes and errors\nconsidered for the calibration of Stewart-Gough platforms.", "AI": {"tldr": "\u672c\u7814\u7a76\u56de\u987e\u4e86Stewart-Gough\u5e73\u53f0\u7684\u6821\u51c6\u6280\u672f\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u5404\u79cd\u5e94\u7528\u4e2d\u63d0\u9ad8\u5e73\u53f0\u51c6\u786e\u6027\u7684\u6311\u6218\u548c\u8bef\u5dee\u6e90\u3002", "motivation": "\u968f\u7740Stewart-Gough\u5e73\u53f0\u5728\u533b\u7597\u3001\u5de5\u7a0b\u3001\u7a7a\u95f4\u7814\u7a76\u7b49\u9886\u57df\u7684\u5e94\u7528\u9700\u6c42\u4e0d\u65ad\u589e\u52a0\uff0c\u63d0\u9ad8\u5176\u7cbe\u5ea6\u548c\u6821\u51c6\u65b9\u6cd5\u53d8\u5f97\u975e\u5e38\u91cd\u8981\u3002", "method": "\u8c03\u67e5\u5e76\u5206\u6790\u5404\u79cd\u57fa\u4e8e\u9006\u8fd0\u52a8\u5b66\u7684\u5e76\u8054\u673a\u5668\u4eba\u6821\u51c6\u6280\u672f\u53ca\u5176\u7ed3\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u7814\u7a76\u8005\u4eec\u4e3b\u8981\u96c6\u4e2d\u5728\u63d0\u5347\u5e73\u53f0\u4f4d\u7f6e\u548c\u65b9\u5411\u7684\u51c6\u786e\u6027\uff0c\u5e76\u8003\u8651\u6765\u81ea\u4e0d\u540c\u8bef\u5dee\u6e90\u7684\u5f71\u54cd\u3002", "conclusion": "\u672c\u7814\u7a76\u65e8\u5728\u56de\u987eStewart-Gough\u5e73\u53f0\u6821\u51c6\u7684\u73b0\u72b6\uff0c\u5f3a\u8c03\u4e0d\u540c\u6821\u51c6\u65b9\u6cd5\u53ca\u5176\u76f8\u5173\u7684\u8bef\u5dee\u6e90\u3002"}}
{"id": "2510.22978", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.22978", "abs": "https://arxiv.org/abs/2510.22978", "authors": ["Ramaravind Kommiya Mothilal", "Sally Zhang", "Syed Ishtiaque Ahmed", "Shion Guha"], "title": "Reasoning About Reasoning: Towards Informed and Reflective Use of LLM Reasoning in HCI", "comment": "14 pages, 3 figures, 1 table", "summary": "Reasoning is a distinctive human-like characteristic attributed to LLMs in\nHCI due to their ability to simulate various human-level tasks. However, this\nwork argues that the reasoning behavior of LLMs in HCI is often\ndecontextualized from the underlying mechanics and subjective decisions that\ncondition the emergence and human interpretation of this behavior. Through a\nsystematic survey of 258 CHI papers from 2020-2025 on LLMs, we discuss how HCI\nhardly perceives LLM reasoning as a product of sociotechnical orchestration and\noften references it as an object of application. We argue that such abstraction\nleads to oversimplification of reasoning methodologies from NLP/ML and results\nin a distortion of LLMs' empirically studied capabilities and (un)known\nlimitations. Finally, drawing on literature from both NLP/ML and HCI, as a\nconstructive step forward, we develop reflection prompts to support HCI\npractitioners engage with LLM reasoning in an informed and reflective way.", "AI": {"tldr": "\u672c\u7814\u7a76\u63ed\u793a\u4e86HCI\u5bf9LLM\u63a8\u7406\u7684\u8bef\u89e3\uff0c\u5e76\u5efa\u8bae\u901a\u8fc7\u53cd\u601d\u6027\u63d0\u793a\u6765\u4fc3\u8fdb\u66f4\u6df1\u5165\u7684\u7406\u89e3\u3002", "motivation": "\u63a2\u8ba8HCI\u9886\u57df\u5bf9LLM\u63a8\u7406\u7684\u8bef\u89e3\u548c\u7b80\u5316", "method": "\u5bf9258\u7bc72020-2025\u5e74\u671f\u95f4\u7684CHI\u8bba\u6587\u8fdb\u884c\u7cfb\u7edf\u8c03\u67e5", "result": "\u8bc6\u522b\u4e86LLM\u63a8\u7406\u673a\u5236\u4e0e\u4eba\u7c7b\u89e3\u91ca\u4e4b\u95f4\u7684\u8131\u8282\uff0c\u5e76\u63d0\u51fa\u4e86\u53cd\u601d\u6027\u63d0\u793a\uff0c\u5e2e\u52a9HCI\u4ece\u4e1a\u8005\u66f4\u597d\u5730\u7406\u89e3LLM\u63a8\u7406", "conclusion": "HCI\u9886\u57df\u9700\u8981\u5173\u6ce8LLM\u63a8\u7406\u7684\u793e\u4f1a\u6280\u672f\u80cc\u666f\uff0c\u907f\u514d\u5bf9\u5176\u80fd\u529b\u548c\u5c40\u9650\u6027\u7684\u8fc7\u5ea6\u7b80\u5316\u3002"}}
{"id": "2510.21860", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21860", "abs": "https://arxiv.org/abs/2510.21860", "authors": ["Callum Sharrock", "Lukas Petersson", "Hanna Petersson", "Axel Backlund", "Axel Wennstr\u00f6m", "Kristoffer Nordstr\u00f6m", "Elias Aronsson"], "title": "Butter-Bench: Evaluating LLM Controlled Robots for Practical Intelligence", "comment": null, "summary": "We present Butter-Bench, a benchmark evaluating large language model (LLM)\ncontrolled robots for practical intelligence, defined as the ability to\nnavigate the messiness of the physical world. Current state-of-the-art robotic\nsystems use a hierarchical architecture with LLMs in charge of high-level\nreasoning, and a Vision Language Action (VLA) model for low-level control.\nButter-Bench evaluates the LLM part in isolation from the VLA. Although LLMs\nhave repeatedly surpassed humans in evaluations requiring analytical\nintelligence, we find humans still outperform LLMs on Butter-Bench. The best\nLLMs score 40% on Butter-Bench, while the mean human score is 95%. LLMs\nstruggled the most with multi-step spatial planning and social understanding.\nWe also evaluate LLMs that are fine-tuned for embodied reasoning and conclude\nthat this training does not improve their score on Butter-Bench.", "AI": {"tldr": "Butter-Bench \u662f\u4e00\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30 LLM \u63a7\u5236\u673a\u5668\u4eba\u5728\u5b9e\u9645\u667a\u80fd\u4e2d\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u663e\u793a\u4eba\u7c7b\u5728\u6b64\u65b9\u9762\u663e\u8457\u4f18\u4e8e LLM\u3002", "motivation": "\u5f53\u524d\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u4e3b\u8981\u901a\u8fc7\u5c42\u6b21\u5316\u67b6\u6784\u8fd0\u4f5c\uff0cLLM \u8d1f\u8d23\u9ad8\u5c42\u63a8\u7406\uff0c\u800c VLA \u6a21\u578b\u7528\u4e8e\u4f4e\u5c42\u63a7\u5236\uff0c\u4f46\u9700\u8981\u5355\u72ec\u8bc4\u4f30 LLM \u7684\u80fd\u529b", "method": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a7\u5236\u7684\u673a\u5668\u4eba\u5728\u5b9e\u9645\u667a\u80fd\u65b9\u9762\u7684\u8868\u73b0", "result": "\u5728 Butter-Bench \u4e2d\uff0cLLM \u7684\u8868\u73b0\u8fdc\u4e0d\u53ca\u4eba\u7c7b\uff0c\u6700\u4f73 LLM \u5f97\u5206\u4ec5\u4e3a 40%\uff0c\u800c\u4eba\u7c7b\u5e73\u5747\u5f97\u5206\u4e3a 95%\u3002LLM \u5728\u591a\u6b65\u9aa4\u7a7a\u95f4\u89c4\u5212\u548c\u793e\u4f1a\u7406\u89e3\u65b9\u9762\u8868\u73b0\u6700\u5dee\u3002", "conclusion": "\u5bf9 LLM \u8fdb\u884c\u8eab\u4f53\u63a8\u7406\u7684\u5fae\u8c03\u8bad\u7ec3\u5e76\u672a\u6539\u5584\u5176\u5728 Butter-Bench \u4e0a\u7684\u8868\u73b0\u3002"}}
{"id": "2510.23245", "categories": ["cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.23245", "abs": "https://arxiv.org/abs/2510.23245", "authors": ["Alexandre P Uchoa", "Carlo E T Oliveira", "Claudia L R Motta", "Daniel Schneider"], "title": "Multi-Stakeholder Alignment in LLM-Powered Collaborative AI Systems: A Multi-Agent Framework for Intelligent Tutoring", "comment": null, "summary": "The integration of Large Language Models into Intelligent Tutoring Systems\npre-sents significant challenges in aligning with diverse and often conflicting\nvalues from students, parents, teachers, and institutions. Existing\narchitectures lack for-mal mechanisms for negotiating these multi-stakeholder\ntensions, creating risks in accountability and bias. This paper introduces the\nAdvisory Governance Layer (AGL), a non-intrusive, multi-agent framework\ndesigned to enable distributed stakeholder participation in AI governance. The\nAGL employs specialized agents representing stakeholder groups to evaluate\npedagogical actions against their spe-cific policies in a privacy-preserving\nmanner, anticipating future advances in per-sonal assistant technology that\nwill enhance stakeholder value expression. Through a novel policy taxonomy and\nconflict-resolution protocols, the frame-work provides structured, auditable\ngovernance advice to the ITS without altering its core pedagogical\ndecision-making. This work contributes a reference architec-ture and technical\nspecifications for aligning educational AI with multi-stakeholder values,\nbridging the gap between high-level ethical principles and practical\nimplementation.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAdvisory Governance Layer (AGL)\u7684\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u4e2d\u9762\u4e34\u7684\u591a\u5229\u76ca\u76f8\u5173\u8005\u4ef7\u503c\u51b2\u7a81\u95ee\u9898\u3002", "motivation": "\u8bba\u6587\u7684\u52a8\u673a\u5728\u4e8e\u5f53\u524d\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u7f3a\u4e4f\u6709\u6548\u7684\u673a\u5236\u6765\u534f\u8c03\u6765\u81ea\u5b66\u751f\u3001\u5bb6\u957f\u3001\u6559\u5e08\u548c\u673a\u6784\u7684\u591a\u79cd\u4ef7\u503c\u89c2\uff0c\u5bfc\u81f4\u8d23\u4efb\u548c\u504f\u89c1\u98ce\u9669\u3002", "method": "\u8bba\u6587\u4ecb\u7ecd\u4e86AGL\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u4ee3\u8868\u4e0d\u540c\u5229\u76ca\u76f8\u5173\u8005\u7fa4\u4f53\u7684\u7279\u5b9a\u4ee3\u7406\u6765\u8bc4\u4f30\u6559\u5b66\u884c\u52a8\uff0c\u540c\u65f6\u91c7\u7528\u9690\u79c1\u4fdd\u62a4\u7684\u65b9\u6cd5\u3002", "result": "AGL\u901a\u8fc7\u65b0\u7684\u653f\u7b56\u5206\u7c7b\u6cd5\u548c\u51b2\u7a81\u89e3\u51b3\u534f\u8bae\uff0c\u63d0\u4f9b\u7ed3\u6784\u5316\u4e14\u53ef\u5ba1\u8ba1\u7684\u6cbb\u7406\u5efa\u8bae\uff0c\u4ece\u800c\u672a\u6539\u53d8ITS\u7684\u6838\u5fc3\u6559\u5b66\u51b3\u7b56\u3002", "conclusion": "AGL\u4e3a\u6559\u80b2AI\u4e0e\u591a\u5229\u76ca\u76f8\u5173\u8005\u7684\u4ef7\u503c\u89c2\u5bf9\u9f50\u63d0\u4f9b\u4e86\u53c2\u8003\u67b6\u6784\u548c\u6280\u672f\u89c4\u8303\uff0c\u4fc3\u8fdb\u9ad8\u5c42\u4f26\u7406\u539f\u5219\u4e0e\u5b9e\u9645\u5b9e\u65bd\u4e4b\u95f4\u7684\u8fde\u63a5\u3002"}}
{"id": "2510.21874", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21874", "abs": "https://arxiv.org/abs/2510.21874", "authors": ["Shuning Zhang"], "title": "A Physics-Informed Neural Network Approach for UAV Path Planning in Dynamic Environments", "comment": "15 pages, 8 figures", "summary": "Unmanned aerial vehicles (UAVs) operating in dynamic wind fields must\ngenerate safe and energy-efficient trajectories under physical and\nenvironmental constraints. Traditional planners, such as A* and kinodynamic\nRRT*, often yield suboptimal or non-smooth paths due to discretization and\nsampling limitations. This paper presents a physics-informed neural network\n(PINN) framework that embeds UAV dynamics, wind disturbances, and obstacle\navoidance directly into the learning process. Without requiring supervised\ndata, the PINN learns dynamically feasible and collision-free trajectories by\nminimizing physical residuals and risk-aware objectives. Comparative\nsimulations show that the proposed method outperforms A* and Kino-RRT* in\ncontrol energy, smoothness, and safety margin, while maintaining similar flight\nefficiency. The results highlight the potential of physics-informed learning to\nunify model-based and data-driven planning, providing a scalable and physically\nconsistent framework for UAV trajectory optimization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc(PINN)\u6846\u67b6\uff0c\u901a\u8fc7\u5d4c\u5165UAV\u52a8\u529b\u5b66\u3001\u98ce\u5e72\u6270\u548c\u907f\u969c\u80fd\u529b\u6765\u4f18\u5316\u65e0\u4eba\u673a\u5728\u52a8\u6001\u98ce\u573a\u4e0b\u7684\u8f68\u8ff9\uff0c\u4ee5\u5b9e\u73b0\u5b89\u5168\u3001\u9ad8\u6548\u7684\u98de\u884c\u8def\u5f84\u3002", "motivation": "\u5728\u52a8\u6001\u98ce\u573a\u4e2d\uff0c\u65e0\u4eba\u673a\u9700\u8981\u5728\u7269\u7406\u548c\u73af\u5883\u7ea6\u675f\u4e0b\u751f\u6210\u5b89\u5168\u4e14\u9ad8\u6548\u7684\u8f68\u8ff9\uff0c\u800c\u4f20\u7edf\u65b9\u6cd5\u5e38\u5e38\u65e0\u6cd5\u6ee1\u8db3\u8fd9\u4e9b\u9700\u6c42\u3002", "method": "\u5229\u7528\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc(PINN)\u6846\u67b6\uff0c\u6700\u5c0f\u5316\u7269\u7406\u6b8b\u5dee\u548c\u98ce\u9669\u610f\u8bc6\u76ee\u6807\uff0c\u4e0d\u4f9d\u8d56\u76d1\u7763\u6570\u636e\u5b66\u4e60\u53ef\u884c\u4e14\u65e0\u78b0\u649e\u7684\u8f68\u8ff9\u3002", "result": "\u6bd4\u8f83\u6a21\u62df\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u63a7\u5236\u80fd\u8017\u3001\u5e73\u6ed1\u6027\u548c\u5b89\u5168\u88d5\u5ea6\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5A*\u548cKino-RRT*\uff0c\u5e76\u4fdd\u6301\u7c7b\u4f3c\u7684\u98de\u884c\u6548\u7387\u3002", "conclusion": "\u7269\u7406\u4fe1\u606f\u5b66\u4e60\u4e3a\u65e0\u4eba\u673a\u8f68\u8ff9\u4f18\u5316\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u7269\u7406\u4e00\u81f4\u7684\u6846\u67b6\uff0c\u6574\u5408\u4e86\u57fa\u4e8e\u6a21\u578b\u548c\u6570\u636e\u9a71\u52a8\u7684\u89c4\u5212\u3002"}}
{"id": "2510.23262", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.23262", "abs": "https://arxiv.org/abs/2510.23262", "authors": ["Lukas Gehrke", "Leonie Terfurth", "Klaus Gramann"], "title": "Moderating Role of Presence in EEG Responses to Visuo-haptic Prediction Error in Virtual Reality", "comment": null, "summary": "Virtual reality (VR) can create compelling experiences that evoke presence,\nthe sense of ``being there.'' However, problems in rendering can create\nsensorimotor disruptions that undermine presence and task performance. Presence\nis typically assessed with post-hoc questionnaires, but their coarse temporal\nresolution limits insight into how sensorimotor disruptions shape user\nexperience. Here, we combined questionnaires with electroencephalography (EEG)\nto identify neural markers of presence-affecting prediction error in immersive\nVR. Twenty-five participants performed a grasp-and-place task under two levels\nof immersion (visual-only vs.~visuo-haptic). Occasional oddball-like\nsensorimotor disruptions introduced premature feedback to elicit prediction\nerrors. Overall, higher immersion enhanced self-presence but not physical\npresence, while accuracy and speed improved over time irrespective of\nimmersion. At the neural level, sensorimotor disruptions elicited robust\nevent-related potential effects at FCz and Pz, accompanied by increases in\nfrontal midline $\\theta$ and posterior $\\alpha$ suppression. Through source\nanalyses localized to anterior- and posterior cingulate cortex (ACC/PCC) we\nfound that PCC $\\alpha$ activity showed heightened sensitivity to disruptions\nexclusively in visuo-haptic immersion. Exploratory moderation analyses by\npresence scores revealed no consistent patterns. Together, these results\nsuggest that higher immersion amplifies both the benefits and costs of\nsensorimotor coherence.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u63a2\u8ba8\u4e86\u865a\u62df\u73b0\u5b9e\u4e2d\u611f\u77e5\u5b58\u5728\u611f\u4e0e\u4f20\u611f\u8fd0\u52a8\u5e72\u6270\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u7ed3\u5408\u95ee\u5377\u548c\u8111\u7535\u56fe\uff08EEG\uff09\u5206\u6790\uff0c\u53d1\u73b0\u4e86\u6c89\u6d78\u5ea6\u5bf9\u611f\u77e5\u5b58\u5728\u611f\u548c\u4efb\u52a1\u8868\u73b0\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u865a\u62df\u73b0\u5b9e\u4e2d\u6c89\u6d78\u611f\u7684\u795e\u7ecf\u57fa\u7840\uff0c\u4e86\u89e3\u4f20\u611f\u8fd0\u52a8\u5e72\u6270\u5982\u4f55\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\uff0c\u5c1d\u8bd5\u5bf9\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u8fdb\u884c\u6539\u8fdb\u3002", "method": "\u53c2\u4e0e\u8005\u8fdb\u884c\u6293\u53d6\u4e0e\u653e\u7f6e\u4efb\u52a1\uff0c\u91c7\u7528\u95ee\u5377\u548cEEG\u8bb0\u5f55\u795e\u7ecf\u6d3b\u52a8\uff0c\u4ee5\u8bc6\u522b\u9884\u6d4b\u8bef\u5dee\u7684\u795e\u7ecf\u6807\u8bb0\uff0c\u8003\u5bdf\u4e0d\u540c\u6c89\u6d78\u5ea6\u4e0b\u7684\u6548\u5e94\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u9ad8\u6c89\u6d78\u5ea6\u4fc3\u8fdb\u4e86\u81ea\u6211\u5b58\u5728\u611f\uff0c\u795e\u7ecf\u7535\u4f4d\u548c\u9891\u8c31\u5206\u6790\u63ed\u793a\u4e86\u4e0e\u4f20\u611f\u8fd0\u52a8\u5e72\u6270\u76f8\u5173\u7684\u663e\u8457\u6548\u5e94\uff0c\u5c24\u5176\u662f\u5728\u89c6\u89c9-\u89e6\u89c9\u6c89\u6d78\u6761\u4ef6\u4e0b\u3002", "conclusion": "\u9ad8\u6c89\u6d78\u5ea6\u589e\u5f3a\u4e86\u81ea\u6211\u5b58\u5728\u611f\u4f46\u672a\u80fd\u63d0\u9ad8\u8eab\u4f53\u5b58\u5728\u611f\uff0c\u540c\u65f6\u4f20\u611f\u8fd0\u52a8\u5e72\u6270\u63ed\u793a\u7684\u795e\u7ecf\u6807\u8bb0\u8bf4\u660e\u4e86\u6c89\u6d78\u611f\u7684\u590d\u6742\u6027\u548c\u591a\u9762\u6027\u3002"}}
{"id": "2510.21991", "categories": ["cs.RO", "cs.AI", "68T40, 93C85, 68T07, 68U35"], "pdf": "https://arxiv.org/pdf/2510.21991", "abs": "https://arxiv.org/abs/2510.21991", "authors": ["Mateo Clemente", "Leo Brunswic", "Rui Heng Yang", "Xuan Zhao", "Yasser Khalil", "Haoyu Lei", "Amir Rasouli", "Yinchuan Li"], "title": "Two-Steps Diffusion Policy for Robotic Manipulation via Genetic Denoising", "comment": "16 pages, 11 figure, 2 tables, accepted at Neurips 2025", "summary": "Diffusion models, such as diffusion policy, have achieved state-of-the-art\nresults in robotic manipulation by imitating expert demonstrations. While\ndiffusion models were originally developed for vision tasks like image and\nvideo generation, many of their inference strategies have been directly\ntransferred to control domains without adaptation. In this work, we show that\nby tailoring the denoising process to the specific characteristics of embodied\nAI tasks -- particularly structured, low-dimensional nature of action\ndistributions -- diffusion policies can operate effectively with as few as 5\nneural function evaluations (NFE).\n  Building on this insight, we propose a population-based sampling strategy,\ngenetic denoising, which enhances both performance and stability by selecting\ndenoising trajectories with low out-of-distribution risk. Our method solves\nchallenging tasks with only 2 NFE while improving or matching performance. We\nevaluate our approach across 14 robotic manipulation tasks from D4RL and\nRobomimic, spanning multiple action horizons and inference budgets. In over 2\nmillion evaluations, our method consistently outperforms standard\ndiffusion-based policies, achieving up to 20\\% performance gains with\nsignificantly fewer inference steps.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53bb\u566a\u7b56\u7565\uff0c\u53ef\u4ee5\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u663e\u8457\u63d0\u9ad8\u6269\u6563\u6a21\u578b\u7684\u6548\u7387\u548c\u7a33\u5b9a\u6027\uff0c\u51cf\u5c11\u63a8\u7406\u6b65\u9aa4\uff0c\u53d6\u5f97\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u901a\u8fc7\u8c03\u6574\u53bb\u566a\u8fc7\u7a0b\u6765\u9002\u5e94\u5177\u4f53\u7684\u5177\u8eabAI\u4efb\u52a1\u7279\u5f81\uff0c\u7279\u522b\u662f\u884c\u52a8\u5206\u5e03\u7684\u7ed3\u6784\u5316\u548c\u4f4e\u7ef4\u7279\u6027\uff0c\u4ee5\u63d0\u9ad8\u6269\u6563\u7b56\u7565\u7684\u6548\u7387\u3002", "method": "\u9057\u4f20\u53bb\u566a\uff0c\u57fa\u4e8e\u79cd\u7fa4\u7684\u91c7\u6837\u7b56\u7565", "result": "\u5728\u4ec5\u97002\u6b21\u795e\u7ecf\u51fd\u6570\u8bc4\u4f30\uff08NFE\uff09\u7684\u60c5\u51b5\u4e0b\u89e3\u51b3\u590d\u6742\u4efb\u52a1\uff0c\u540c\u65f6\u63d0\u9ad8\u6216\u5339\u914d\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u4e2a\u6027\u5316\u7684\u53bb\u566a\u7b56\u7565\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6807\u51c6\u6269\u6563\u7b56\u7565\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.23324", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.23324", "abs": "https://arxiv.org/abs/2510.23324", "authors": ["Sebastian Maier", "Manuel Schneider", "Stefan Feuerriegel"], "title": "Partnering with Generative AI: Experimental Evaluation of Human-Led and Model-Led Interaction in Human-AI Co-Creation", "comment": null, "summary": "Large language models (LLMs) show strong potential to support creative tasks,\nbut the role of the interface design is poorly understood. In particular, the\neffect of different modes of collaboration between humans and LLMs on\nco-creation outcomes is unclear. To test this, we conducted a randomized\ncontrolled experiment ($N = 486$) comparing: (a) two variants of reflective,\nhuman-led modes in which the LLM elicits elaboration through suggestions or\nquestions, against (b) a proactive, model-led mode in which the LLM\nindependently rewrites ideas. By assessing the effects on idea quality,\ndiversity, and perceived ownership, we found that the model-led mode\nsubstantially improved idea quality but reduced idea diversity and users'\nperceived idea ownership. The reflective, human-led mode also improved idea\nquality, yet while preserving diversity and ownership. Our findings highlight\nthe importance of designing interactions with generative AI systems as\nreflective thought partners that complement human strengths and augment\ncreative processes.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4eba\u7c7b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u534f\u4f5c\u8fc7\u7a0b\u4e2d\u7684\u4e0d\u540c\u6a21\u5f0f\u5bf9\u521b\u610f\u4efb\u52a1\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u6a21\u578b\u4e3b\u5bfc\u6a21\u5f0f\u63d0\u5347\u521b\u610f\u8d28\u91cf\uff0c\u4f46\u635f\u5bb3\u4e86\u591a\u6837\u6027\u548c\u6240\u6709\u6743\u611f\uff0c\u800c\u4eba\u7c7b\u4e3b\u5bfc\u6a21\u5f0f\u5219\u5728\u63d0\u5347\u8d28\u91cf\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u591a\u6837\u6027\u548c\u6240\u6709\u6743\u3002", "motivation": "\u63a2\u8ba8\u754c\u9762\u8bbe\u8ba1\u5728\u521b\u610f\u4efb\u52a1\u4e2d\u7684\u4f5c\u7528\uff0c\u5c24\u5176\u662f\u4eba\u7c7b\u4e0eLLM\u534f\u4f5c\u7684\u4e0d\u540c\u6a21\u5f0f\u5bf9\u5171\u540c\u521b\u4f5c\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "method": "\u968f\u673a\u5bf9\u7167\u5b9e\u9a8c\uff08N = 486\uff09\u6bd4\u8f83\u4e0d\u540c\u4eba\u7c7b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e4b\u95f4\u7684\u534f\u4f5c\u6a21\u5f0f\u3002", "result": "\u6a21\u578b\u4e3b\u5bfc\u6a21\u5f0f\u663e\u8457\u63d0\u9ad8\u4e86\u521b\u610f\u8d28\u91cf\uff0c\u4f46\u964d\u4f4e\u4e86\u521b\u610f\u591a\u6837\u6027\u548c\u7528\u6237\u7684\u521b\u610f\u6240\u6709\u6743\u611f\uff1b\u4eba\u7c7b\u4e3b\u5bfc\u6a21\u5f0f\u5728\u63d0\u9ad8\u521b\u610f\u8d28\u91cf\u7684\u540c\u65f6\u4fdd\u7559\u4e86\u521b\u610f\u7684\u591a\u6837\u6027\u548c\u6240\u6709\u6743\u611f\u3002", "conclusion": "\u5728\u8bbe\u8ba1\u4e0e\u751f\u6210\u6027AI\u7cfb\u7edf\u7684\u4e92\u52a8\u65f6\uff0c\u5e94\u91cd\u89c6\u5176\u4f5c\u4e3a\u53cd\u601d\u6027\u601d\u8003\u4f19\u4f34\u7684\u4f5c\u7528\uff0c\u4ee5\u8865\u5145\u4eba\u7c7b\u4f18\u52bf\u548c\u589e\u5f3a\u521b\u4f5c\u8fc7\u7a0b\u3002"}}
{"id": "2510.22030", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.22030", "abs": "https://arxiv.org/abs/2510.22030", "authors": ["Harsha Karunanayaka", "Siavash Rezazadeh"], "title": "Estimation of Minimum Stride Frequency for the Frontal Plane Stability of Bipedal Systems", "comment": null, "summary": "Stability of bipedal systems in frontal plane is affected by the hip offset,\nto the extent that adjusting stride time using feedforward retraction and\nextension of the legs can lead to stable oscillations without feedback control.\nThis feedforward stabilization can be leveraged to reduce the control effort\nand energy expenditure and increase the locomotion robustness. However, there\nis limited understanding of how key parameters, such as mass, stiffness, leg\nlength, and hip width, affect stability and the minimum stride frequency needed\nto maintain it. This study aims to address these gaps through analyzing how\nindividual model parameters and the system's natural frequency influence the\nminimum stride frequency required to maintain a stable cycle. We propose a\nmethod to predict the minimum stride frequency, and compare the predicted\nstride frequencies with actual values for randomly generated models. The\nfindings of this work provide a better understanding of the frontal plane\nstability mechanisms and how feedforward stabilization can be leveraged to\nreduce the control effort.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u53cc\u8db3\u7cfb\u7edf\u5728\u524d\u5e73\u9762\u4e0a\u7a33\u5b9a\u6027\u7684\u91cd\u8981\u53c2\u6570\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u6d4b\u6700\u4f4e\u6b65\u9891\u7684\u6709\u6548\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7a76\u53cc\u8db3\u6b65\u6001\u7a33\u5b9a\u6027\u7684\u5173\u952e\u53c2\u6570\u5982\u4f55\u5f71\u54cd\u7a33\u5b9a\u6027\uff0c\u51cf\u5c11\u63a7\u5236\u52aa\u529b\u548c\u80fd\u91cf\u6d88\u8017\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6a21\u578b\u53c2\u6570\u548c\u7cfb\u7edf\u81ea\u7136\u9891\u7387\uff0c\u63d0\u51fa\u9884\u6d4b\u6700\u4f4e\u6b65\u9891\u7684\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u6d4b\u6700\u4f4e\u6b65\u9891\u7684\u65b9\u6cd5\uff0c\u5e76\u5c06\u9884\u6d4b\u503c\u4e0e\u5b9e\u9645\u503c\u8fdb\u884c\u6bd4\u8f83\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63d0\u4f9b\u4e86\u524d\u5e73\u9762\u7a33\u5b9a\u673a\u5236\u7684\u6df1\u5165\u7406\u89e3\uff0c\u5e76\u8868\u660e\u524d\u9988\u7a33\u5b9a\u5316\u53ef\u4ee5\u663e\u8457\u964d\u4f4e\u63a7\u5236\u52aa\u529b\u3002"}}
{"id": "2510.23342", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.23342", "abs": "https://arxiv.org/abs/2510.23342", "authors": ["Alex S. Taylor", "Noortje Marres", "Mercedes Bunz", "Thao Phan", "Maya Indira Ganesh", "Dominique Barron", "Yasmine Boudiaf", "Rachel Coldicutt", "Iain Emsley", "Beatrice Gobbo", "Louise Hickman", "Bettina Nissen", "Mukul Patel", "Luis Soares"], "title": "Reciprocity Deficits: Observing AI in the street with everyday publics", "comment": "13 pages, 4 Figures", "summary": "The street has emerged as a primary site where everyday publics are\nconfronted with AI as an infrastructural phenomenon, as machine learning-based\nsystems are now commonly deployed in this setting in the form of automated\ncars, facial recognition, smart billboards and the like. While these\ndeployments of AI in the street have attracted significant media attention and\npublic controversy in recent years, the presence of AI in the street often\nremains inscrutable, and many everyday publics are unaware of it. In this\npaper, we explore the challenges and possibilities of everyday public\nengagement with AI in the situated environment of city streets under these\nparadoxical conditions. Combining perspectives and approaches from social and\ncultural studies of AI, Design Research and Science and Technology Studies\n(STS), we explore the affordances of the street as a site for 'material\nparticipation' in AI through design-based interventions: the creation of\n'everyday AI observatories.' We narrate and reflect on our participatory\nobservations of AI in five city streets in the UK and Australia and highlight a\nset of tensions that emerged from them: 1) the framing of the street as a\ntransactional environment, 2) the designed invisibility of AI and its publics\nin the street 3) the stratification of street environments through statistical\ngovernance. Based on this discussion and drawing on Jane Jacobs' notion of\n\"eyes on the street,\" we put forward the relational notion of \"reciprocity\ndeficits\" between AI infrastructures and everyday publics in the street. The\nconclusion reflects on the consequences of this form of social invisibility of\nAI for situated engagement with AI by everyday publics in the street and for\npublic trust in urban governance.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u65e5\u5e38\u516c\u4f17\u5982\u4f55\u5728\u57ce\u5e02\u8857\u9053\u4e2d\u4e0eAI\u4e92\u52a8\uff0c\u63a2\u8ba8\u4e86\u8fd9\u4e9b\u4e92\u52a8\u6240\u9762\u4e34\u7684\u6311\u6218\u4e0e\u673a\u9047\u3002", "motivation": "AI\u5728\u57ce\u5e02\u8857\u9053\u4e2d\u7684\u65e0\u5f62\u5b58\u5728\u4e0e\u516c\u4f17\u7684\u975e\u77e5\u60c5\u72b6\u6001\u4e4b\u95f4\u7684\u77db\u76fe", "method": "\u901a\u8fc7\u8bbe\u8ba1\u5e72\u9884\u63a2\u8ba8\u8857\u9053\u4e0a\u516c\u4f17\u4e0eAI\u7684\u4e92\u52a8", "result": "\u63d0\u51fa\u4e86'\u65e5\u5e38AI\u89c2\u5bdf\u7ad9'\u4f5c\u4e3a\u4ecb\u5165\u70b9\uff0c\u5e76\u8bc6\u522b\u51fa\u8857\u9053\u73af\u5883\u4e2d\u7684\u4e00\u7cfb\u5217\u7d27\u5f20\u5173\u7cfb", "conclusion": "AI\u7684\u793e\u4f1a\u9690\u5f62\u6027\u5bf9\u65e5\u5e38\u516c\u4f17\u7684\u53c2\u4e0e\u548c\u5bf9\u57ce\u5e02\u6cbb\u7406\u7684\u4fe1\u4efb\u6709\u91cd\u8981\u5f71\u54cd\u3002"}}
{"id": "2510.22113", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.22113", "abs": "https://arxiv.org/abs/2510.22113", "authors": ["Zitiantao Lin", "Yongpeng Sang", "Yang Ye"], "title": "RaycastGrasp: Eye-Gaze Interaction with Wearable Devices for Robotic Manipulation", "comment": "5 pages, 5 figures; Accepted to: 2025 IEEE 4th International\n  Conference on Intelligent Reality (ICIR 2025); Zitiantao Lin and Yongpeng\n  Sang contributed equally to this work (co-first authors). Corresponding\n  author: Yang Ye (y.ye@northeastern.edu)", "summary": "Robotic manipulators are increasingly used to assist individuals with\nmobility impairments in object retrieval. However, the predominant\njoystick-based control interfaces can be challenging due to high precision\nrequirements and unintuitive reference frames. Recent advances in human-robot\ninteraction have explored alternative modalities, yet many solutions still rely\non external screens or restrictive control schemes, limiting their\nintuitiveness and accessibility. To address these challenges, we present an\negocentric, gaze-guided robotic manipulation interface that leverages a\nwearable Mixed Reality (MR) headset. Our system enables users to interact\nseamlessly with real-world objects using natural gaze fixation from a\nfirst-person perspective, while providing augmented visual cues to confirm\nintent and leveraging a pretrained vision model and robotic arm for intent\nrecognition and object manipulation. Experimental results demonstrate that our\napproach significantly improves manipulation accuracy, reduces system latency,\nand achieves single-pass intention and object recognition accuracy greater than\n88% across multiple real-world scenarios. These results demonstrate the\nsystem's effectiveness in enhancing intuitiveness and accessibility,\nunderscoring its practical significance for assistive robotics applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df7\u5408\u73b0\u5b9e\u7684\u773c\u52a8\u5f15\u5bfc\u673a\u5668\u4eba\u64cd\u63a7\u754c\u9762\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u64cd\u63a7\u51c6\u786e\u6027\u548c\u610f\u56fe\u8bc6\u522b\u6548\u679c\uff0c\u6539\u5584\u8f85\u52a9\u673a\u5668\u4eba\u5e94\u7528\u7684\u76f4\u89c2\u6027\u4e0e\u53ef\u53ca\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u64cd\u63a7\u63a5\u53e3\u5982\u64cd\u7eb5\u6746\u5bf9\u4e8e\u6709\u79fb\u52a8\u969c\u788d\u7684\u4eba\u58eb\u800c\u8a00\u5b58\u5728\u7cbe\u786e\u5ea6\u9ad8\u548c\u53c2\u8003\u6846\u67b6\u4e0d\u76f4\u89c2\u7b49\u95ee\u9898\uff0c\u8feb\u5207\u9700\u8981\u66f4\u76f4\u89c2\u4fbf\u6377\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u7a7f\u6234\u5f0f\u6df7\u5408\u73b0\u5b9e\u754c\u9762\uff0c\u7ed3\u5408\u6ce8\u89c6\u8ddf\u8e2a\u548c\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\uff0c\u5b9e\u73b0\u7cbe\u51c6\u7684\u7269\u4f53\u64cd\u63a7\u548c\u610f\u56fe\u8bc6\u522b\u3002", "result": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a7f\u6234\u5f0f\u6df7\u5408\u73b0\u5b9e\u5934\u76d4\u7684\u81ea\u6211\u4e2d\u5fc3\u3001\u6ce8\u89c6\u5f15\u5bfc\u7684\u673a\u5668\u4eba\u64cd\u63a7\u754c\u9762\u3002\u8be5\u7cfb\u7edf\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u81ea\u7136\u7684\u6ce8\u89c6\u56fa\u5b9a\u4e0e\u73b0\u5b9e\u4e16\u754c\u7269\u4f53\u65e0\u7f1d\u4e92\u52a8\uff0c\u540c\u65f6\u63d0\u4f9b\u589e\u5f3a\u89c6\u89c9\u63d0\u793a\u4ee5\u786e\u8ba4\u7528\u6237\u610f\u56fe\uff0c\u5e76\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u548c\u673a\u5668\u4eba\u81c2\u8fdb\u884c\u610f\u56fe\u8bc6\u522b\u548c\u7269\u4f53\u64cd\u63a7\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u64cd\u63a7\u51c6\u786e\u6027\uff0c\u964d\u4f4e\u4e86\u7cfb\u7edf\u5ef6\u8fdf\uff0c\u5e76\u5728\u591a\u79cd\u5b9e\u9645\u573a\u666f\u4e2d\u5355\u6b21\u610f\u56fe\u548c\u7269\u4f53\u8bc6\u522b\u51c6\u786e\u7387\u8d85\u8fc788%\u3002", "conclusion": "\u6211\u4eec\u7684\u7cfb\u7edf\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u63a7\u7684\u76f4\u89c2\u6027\u548c\u53ef\u53ca\u6027\uff0c\u5bf9\u8f85\u52a9\u673a\u5668\u4eba\u5e94\u7528\u5177\u6709\u91cd\u8981\u7684\u5b9e\u8df5\u610f\u4e49\u3002"}}
{"id": "2510.23475", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.23475", "abs": "https://arxiv.org/abs/2510.23475", "authors": ["Suyash Fulay", "Sercan Demir", "Galen Hines-Pierce", "H\u00e9l\u00e8ne Landemore", "Michiel Bakker"], "title": "Shareholder Democracy with AI Representatives", "comment": null, "summary": "A large share of retail investors hold public equities through mutual funds,\nyet lack adequate control over these investments. Indeed, mutual funds\nconcentrate voting power in the hands of a few asset managers. These managers\nvote on behalf of shareholders despite having limited insight into their\nindividual preferences, leaving them exposed to growing political and\nregulatory pressures, particularly amid rising shareholder activism.\nPass-through voting has been proposed as a way to empower retail investors and\nprovide asset managers with clearer guidance, but it faces challenges such as\nlow participation rates and the difficulty of capturing highly individualized\nshareholder preferences for each specific vote. Randomly selected assemblies of\nshareholders, or ``investor assemblies,'' have also been proposed as more\nrepresentative proxies than asset managers. As a third alternative, we propose\nartificial intelligence (AI) enabled representatives trained on individual\nshareholder preferences to act as proxies and vote on their behalf. Over time,\nthese models could not only predict how retail investors would vote at any\ngiven moment but also how they might vote if they had significantly more time,\nknowledge, and resources to evaluate each proposal, leading to better overall\ndecision-making. We argue that shareholder democracy offers a compelling\nreal-world test bed for AI-enabled representation, providing valuable insights\ninto both the potential benefits and risks of this approach more generally.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7AI\u9a71\u52a8\u7684\u4ee3\u8868\u89e3\u51b3\u96f6\u552e\u6295\u8d44\u8005\u5728\u5171\u540c\u57fa\u91d1\u4e2d\u7684\u6295\u7968\u6743\u95ee\u9898\uff0c\u63d0\u5347\u80a1\u4e1c\u6c11\u4e3b\u3002", "motivation": "\u63a2\u8ba8\u96f6\u552e\u6295\u8d44\u8005\u5728\u57fa\u91d1\u4e2d\u7684\u6295\u7968\u6743\u95ee\u9898\u53ca\u5176\u9762\u4e34\u7684\u653f\u6cbb\u4e0e\u76d1\u7ba1\u538b\u529b\uff0c\u5bfb\u6c42\u63d0\u9ad8\u53c2\u4e0e\u5ea6\u548c\u4ee3\u8868\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\u7684\u4ee3\u8868\u6765\u4ee3\u7406\u5e76\u6295\u7968\uff0c\u6a21\u62df\u96f6\u552e\u6295\u8d44\u8005\u7684\u4e2a\u4f53\u504f\u597d\u3002", "result": "AI\u9a71\u52a8\u7684\u6a21\u578b\u4e0d\u4ec5\u80fd\u5b9e\u65f6\u9884\u6d4b\u96f6\u552e\u6295\u8d44\u8005\u7684\u6295\u7968\uff0c\u8fd8\u80fd\u8bc4\u4f30\u4ed6\u4eec\u5728\u5145\u5206\u4fe1\u606f\u4e0b\u7684\u6295\u7968\u884c\u4e3a\uff0c\u4ece\u800c\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\u3002", "conclusion": "\u80a1\u4e1c\u6c11\u4e3b\u4e3aAI\u9a71\u52a8\u7684\u4ee3\u8868\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f15\u4eba\u6ce8\u76ee\u7684\u73b0\u5b9e\u4e16\u754c\u6d4b\u8bd5\u5e73\u53f0\uff0c\u63ed\u793a\u4e86\u8fd9\u79cd\u65b9\u6cd5\u7684\u6f5c\u5728\u597d\u5904\u4e0e\u98ce\u9669\u3002"}}
{"id": "2510.22126", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.22126", "abs": "https://arxiv.org/abs/2510.22126", "authors": ["Guanwen Xie", "Jingzehua Xu", "Jiwei Tang", "Yubo Huang", "Shuai Zhang", "Xiaofan Li"], "title": "EasyUUV: An LLM-Enhanced Universal and Lightweight Sim-to-Real Reinforcement Learning Framework for UUV Attitude Control", "comment": "8 pages, 15 figures", "summary": "Despite recent advances in Unmanned Underwater Vehicle (UUV) attitude\ncontrol, existing methods still struggle with generalizability, robustness to\nreal-world disturbances, and efficient deployment. To address the above\nchallenges, this paper presents EasyUUV, a Large Language Model (LLM)-enhanced,\nuniversal, and lightweight simulation-to-reality reinforcement learning (RL)\nframework for robust attitude control of UUVs. EasyUUV combines parallelized RL\ntraining with a hybrid control architecture, where a learned policy outputs\nhigh-level attitude corrections executed by an adaptive S-Surface controller. A\nmultimodal LLM is further integrated to adaptively tune controller parameters\nat runtime using visual and textual feedback, enabling training-free adaptation\nto unmodeled dynamics. Also, we have developed a low-cost 6-DoF UUV platform\nand applied an RL policy trained through efficient parallelized simulation.\nExtensive simulation and real-world experiments validate the effectiveness and\noutstanding performance of EasyUUV in achieving robust and adaptive UUV\nattitude control across diverse underwater conditions. The source code is\navailable from the following website: https://360zmem.github.io/easyuuv/", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faEasyUUV\uff0c\u7ed3\u5408RL\u548cLLM\u6280\u672f\uff0c\u89e3\u51b3UUV\u59ff\u6001\u63a7\u5236\u4e2d\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u51fa\u8272\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684UUV\u59ff\u6001\u63a7\u5236\u65b9\u6cd5\u5728\u6cdb\u5316\u80fd\u529b\u3001\u6297\u5e72\u6270\u80fd\u529b\u548c\u9ad8\u6548\u90e8\u7f72\u7b49\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u5e76\u884cRL\u8bad\u7ec3\u4e0e\u6df7\u5408\u63a7\u5236\u67b6\u6784\uff0c\u4f7f\u7528\u591a\u6a21\u6001LLM\u81ea\u9002\u5e94\u8c03\u6574\u63a7\u5236\u5668\u53c2\u6570\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u4eff\u771f\u548c\u73b0\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1\u4e86EasyUUV\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u590d\u6742\u6c34\u4e0b\u6761\u4ef6\u4e0b\u7684\u5353\u8d8a\u8868\u73b0\u3002", "conclusion": "EasyUUV\u5728\u4e0d\u540c\u6c34\u4e0b\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u9c81\u68d2\u548c\u81ea\u9002\u5e94\u7684UUV\u59ff\u6001\u63a7\u5236\uff0c\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.22164", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.22164", "abs": "https://arxiv.org/abs/2510.22164", "authors": ["Jianeng Wang", "Matias Mattamala", "Christina Kassab", "Nived Chebrolu", "Guillaume Burger", "Fabio Elnecave", "Marine Petriaux", "Maurice Fallon"], "title": "LT-Exosense: A Vision-centric Multi-session Mapping System for Lifelong Safe Navigation of Exoskeletons", "comment": "8 pages, 4 figures", "summary": "Self-balancing exoskeletons offer a promising mobility solution for\nindividuals with lower-limb disabilities. For reliable long-term operation,\nthese exoskeletons require a perception system that is effective in changing\nenvironments. In this work, we introduce LT-Exosense, a vision-centric,\nmulti-session mapping system designed to support long-term (semi)-autonomous\nnavigation for exoskeleton users. LT-Exosense extends single-session mapping\ncapabilities by incrementally fusing spatial knowledge across multiple\nsessions, detecting environmental changes, and updating a persistent global\nmap. This representation enables intelligent path planning, which can adapt to\nnewly observed obstacles and can recover previous routes when obstructions are\nremoved. We validate LT-Exosense through several real-world experiments,\ndemonstrating a scalable multi-session map that achieves an average\npoint-to-point error below 5 cm when compared to ground-truth laser scans. We\nalso illustrate the potential application of adaptive path planning in\ndynamically changing indoor environments.", "AI": {"tldr": "LT-Exosense\u662f\u4e00\u4e2a\u652f\u6301\u957f\u671f\u5bfc\u822a\u7684\u89c6\u89c9\u4e2d\u5fc3\u591a\u4f1a\u8bdd\u6620\u5c04\u7cfb\u7edf\uff0c\u80fd\u6709\u6548\u9002\u5e94\u73af\u5883\u53d8\u5316\u5e76\u5b9e\u73b0\u667a\u80fd\u8def\u5f84\u89c4\u5212", "motivation": "\u89e3\u51b3\u81ea\u5e73\u8861\u5916\u9aa8\u9abc\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u7684\u957f\u671f\u53ef\u9760\u64cd\u4f5c\u9700\u6c42", "method": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aLT-Exosense\u7684\u89c6\u89c9\u4e2d\u5fc3\u7684\u591a\u4f1a\u8bdd\u6620\u5c04\u7cfb\u7edf", "result": "\u901a\u8fc7\u5b9e\u5730\u5b9e\u9a8c\u9a8c\u8bc1\uff0cLT-Exosense\u80fd\u5728\u4e0e\u6fc0\u5149\u626b\u63cf\u7684\u771f\u5b9e\u6570\u636e\u6bd4\u8f83\u4e2d\uff0c\u5e73\u5747\u70b9\u5bf9\u70b9\u8bef\u5dee\u4f4e\u4e8e5\u5398\u7c73\uff0c\u4e14\u5177\u5907\u9002\u5e94\u53d8\u5316\u73af\u5883\u7684\u8def\u5f84\u89c4\u5212\u80fd\u529b", "conclusion": "LT-Exosense\u663e\u793a\u51fa\u5728\u52a8\u6001\u53d8\u5316\u7684\u5ba4\u5185\u73af\u5883\u4e2d\uff0c\u9002\u5e94\u6027\u8def\u5f84\u89c4\u5212\u7684\u6f5c\u529b\uff0c\u6539\u5584\u4e86\u5916\u9aa8\u9abc\u7528\u6237\u7684\u5bfc\u822a\u4f53\u9a8c\u3002"}}
{"id": "2510.23204", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.23204", "abs": "https://arxiv.org/abs/2510.23204", "authors": ["Giulia Pusceddu", "Giulio Antonio Abbo", "Francesco Rea", "Tony Belpaeme", "Alessandra Sciutti"], "title": "If They Disagree, Will You Conform? Exploring the Role of Robots' Value Awareness in a Decision-Making Task", "comment": null, "summary": "This study investigates whether the opinions of robotic agents are more\nlikely to influence human decision-making when the robots are perceived as\nvalue-aware (i.e., when they display an understanding of human principles). We\ndesigned an experiment in which participants interacted with two Furhat robots\n- one programmed to be Value-Aware and the other Non-Value-Aware - during a\nlabeling task for images representing human values. Results indicate that\nparticipants distinguished the Value-Aware robot from the Non-Value-Aware one.\nAlthough their explicit choices did not indicate a clear preference for one\nrobot over the other, participants directed their gaze more toward the\nValue-Aware robot. Additionally, the Value-Aware robot was perceived as more\nloyal, suggesting that value awareness in a social robot may enhance its\nperceived commitment to the group. Finally, when both robots disagreed with the\nparticipant, conformity occurred in about one out of four trials, and\nparticipants took longer to confirm their responses, suggesting that two robots\nexpressing dissent may introduce hesitation in decision-making. On one hand,\nthis highlights the potential risk that robots, if misused, could manipulate\nusers for unethical purposes. On the other hand, it reinforces the idea that\nsocial robots might encourage reflection in ambiguous situations and help users\navoid scams.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u5177\u6709\u4ef7\u503c\u610f\u8bc6\u7684\u793e\u4ea4\u673a\u5668\u4eba\u5982\u4f55\u5f71\u54cd\u4eba\u7c7b\u51b3\u7b56\uff0c\u7ed3\u679c\u663e\u793a\u53c2\u4e0e\u8005\u5bf9\u4ef7\u503c\u610f\u8bc6\u673a\u5668\u4eba\u66f4\u611f\u5174\u8da3\uff0c\u4f46\u4e5f\u63ed\u793a\u4e86\u6f5c\u5728\u7684\u64cd\u63a7\u98ce\u9669\u3002", "motivation": "\u63a2\u7a76\u793e\u4ea4\u673a\u5668\u4eba\u5728\u51b3\u7b56\u5f71\u54cd\u4e2d\u7684\u89d2\u8272\uff0c\u5c24\u5176\u662f\u5b83\u4eec\u662f\u5426\u4f1a\u56e0\u4e3a\u88ab\u8ba4\u4e3a\u5177\u5907\u4ef7\u503c\u610f\u8bc6\u800c\u589e\u5f3a\u5bf9\u4eba\u7c7b\u7684\u5f71\u54cd\u529b\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u4e24\u6b3e\u4e0d\u540c\u7f16\u7a0b\u673a\u5236\u7684Furhat\u673a\u5668\u4eba\uff08\u5177\u5907\u4ef7\u503c\u610f\u8bc6\u548c\u4e0d\u5177\u5907\u4ef7\u503c\u610f\u8bc6\uff09\u4e0e\u53c2\u4e0e\u8005\u7684\u4e92\u52a8\u3002", "result": "\u53c2\u4e0e\u8005\u66f4\u503e\u5411\u4e8e\u6ce8\u89c6\u5177\u4ef7\u503c\u610f\u8bc6\u7684\u673a\u5668\u4eba\uff0c\u5e76\u8ba4\u4e3a\u5176\u5fe0\u8bda\u5ea6\u66f4\u9ad8\uff0c\u4e14\u5728\u673a\u5668\u4eba\u610f\u89c1\u4e0d\u5408\u65f6\u8868\u73b0\u51fa\u72b9\u8c6b\uff0c\u7ea6\u56db\u5206\u4e4b\u4e00\u7684\u5b9e\u9a8c\u4e2d\u53d1\u751f\u4ece\u4f17\u73b0\u8c61\u3002", "conclusion": "\u793e\u4ea4\u673a\u5668\u4eba\uff0c\u5c24\u5176\u662f\u5177\u5907\u4ef7\u503c\u610f\u8bc6\u7684\u673a\u5668\u4eba\uff0c\u53ef\u80fd\u5728\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u5f71\u54cd\u4eba\u7c7b\uff0c\u4f46\u4e5f\u5b58\u5728\u6f5c\u5728\u7684\u64cd\u63a7\u98ce\u9669\u3002"}}
{"id": "2510.22201", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.22201", "abs": "https://arxiv.org/abs/2510.22201", "authors": ["Minho Park", "Kinam Kim", "Junha Hyung", "Hyojin Jang", "Hoiyeong Jin", "Jooyeol Yun", "Hojoon Lee", "Jaegul Choo"], "title": "ACG: Action Coherence Guidance for Flow-based VLA models", "comment": null, "summary": "Diffusion and flow matching models have emerged as powerful robot policies,\nenabling Vision-Language-Action (VLA) models to generalize across diverse\nscenes and instructions. Yet, when trained via imitation learning, their high\ngenerative capacity makes them sensitive to noise in human demonstrations:\njerks, pauses, and jitter which reduce action coherence. Reduced action\ncoherence causes instability and trajectory drift during deployment, failures\nthat are catastrophic in fine-grained manipulation where precision is crucial.\nIn this paper, we present Action Coherence Guidance (ACG) for VLA models, a\ntraining-free test-time guidance algorithm that improves action coherence and\nthereby yields performance gains. Evaluated on RoboCasa, DexMimicGen, and\nreal-world SO-101 tasks, ACG consistently improves action coherence and boosts\nsuccess rates across diverse manipulation tasks. Code and project page are\navailable at https://github.com/DAVIAN-Robotics/ACG and\nhttps://DAVIAN-Robotics.github.io/ACG , respectively.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u52a8\u4f5c\u4e00\u81f4\u6027\u6307\u5bfc\u7b97\u6cd5\uff08ACG\uff09\uff0c\u65e8\u5728\u6539\u5584\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8\u6a21\u578b\u5728\u590d\u6742\u64cd\u4f5c\u4e2d\u7684\u6027\u80fd\uff0c\u514b\u670d\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u566a\u58f0\u95ee\u9898\u3002", "motivation": "\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8\u6a21\u578b\u4e2d\u7684\u52a8\u4f5c\u4e00\u81f4\u6027\uff0c\u4ee5\u89e3\u51b3\u6a21\u4eff\u5b66\u4e60\u4e2d\u4eba\u7c7b\u793a\u8303\u566a\u97f3\u5bfc\u81f4\u7684\u52a8\u4f5c\u4e0d\u8fde\u8d2f\u95ee\u9898\u3002", "method": "Action Coherence Guidance (ACG)", "result": "\u5728RoboCasa\u3001DexMimicGen\u548c\u5b9e\u9645SO-101\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cACG\u5728\u63d0\u5347\u52a8\u4f5c\u4e00\u81f4\u6027\u548c\u6210\u529f\u7387\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "ACG\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u4e00\u81f4\u6027\uff0c\u8fd8\u589e\u5f3a\u4e86\u5728\u591a\u6837\u5316\u7684\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u6210\u529f\u7387\uff0c\u5c55\u73b0\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.22204", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22204", "abs": "https://arxiv.org/abs/2510.22204", "authors": ["Weixian Qian", "Sebastian Schroder", "Yao Deng", "Jiaohong Yao", "Linfeng Liang", "Xiao Cheng", "Richard Han", "Xi Zheng"], "title": "Bridging Perception and Reasoning: Dual-Pipeline Neuro-Symbolic Landing for UAVs in Cluttered Environments", "comment": null, "summary": "Autonomous landing in unstructured (cluttered, uneven, and map-poor)\nenvironments is a core requirement for Unmanned Aerial Vehicles (UAVs), yet\npurely vision-based or deep learning models often falter under covariate shift\nand provide limited interpretability. We propose NeuroSymLand, a neuro-symbolic\nframework that tightly couples two complementary pipelines: (i) an offline\npipeline, where Large Language Models (LLMs) and human-in-the-loop refinement\nsynthesize Scallop code from diverse landing scenarios, distilling\ngeneralizable and verifiable symbolic knowledge; and (ii) an online pipeline,\nwhere a compact foundation-based semantic segmentation model generates\nprobabilistic Scallop facts that are composed into semantic scene graphs for\nreal-time deductive reasoning. This design combines the perceptual strengths of\nlightweight foundation models with the interpretability and verifiability of\nsymbolic reasoning. Node attributes (e.g., flatness, area) and edge relations\n(adjacency, containment, proximity) are computed with geometric routines rather\nthan learned, avoiding the data dependence and latency of train-time graph\nbuilders. The resulting Scallop program encodes landing principles (avoid water\nand obstacles; prefer large, flat, accessible regions) and yields calibrated\nsafety scores with ranked Regions of Interest (ROIs) and human-readable\njustifications. Extensive evaluations across datasets, diverse simulation maps,\nand real UAV hardware show that NeuroSymLand achieves higher accuracy, stronger\nrobustness to covariate shift, and superior efficiency compared with\nstate-of-the-art baselines, while advancing UAV safety and reliability in\nemergency response, surveillance, and delivery missions.", "AI": {"tldr": "NeuroSymLand\u6846\u67b6\u7ed3\u5408\u4e86\u7b26\u53f7\u63a8\u7406\u548c\u8f7b\u91cf\u6a21\u578b\uff0c\u63d0\u5347\u65e0\u4eba\u673a\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u7740\u9646\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u65e0\u4eba\u673a\u5728\u590d\u6742\u73af\u5883\u4e2d\u81ea\u4e3b\u7740\u9646\u7684\u9700\u6c42\uff0c\u514b\u670d\u89c6\u89c9\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u5c40\u9650\u6027", "method": "\u63d0\u51fa\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u6846\u67b6NeuroSymLand\uff0c\u7ed3\u5408\u79bb\u7ebf\u548c\u5728\u7ebf\u7ba1\u9053", "result": "\u5728\u5404\u79cd\u6570\u636e\u96c6\u548c\u771f\u5b9e\u65e0\u4eba\u673a\u786c\u4ef6\u4e0a\uff0cNeuroSymLand\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u6280\u672f\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "conclusion": "NeuroSymLand\u63d0\u9ad8\u4e86\u65e0\u4eba\u673a\u5728\u7d27\u6025\u54cd\u5e94\u3001\u76d1\u89c6\u548c\u6295\u9012\u4efb\u52a1\u4e2d\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2510.22313", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.22313", "abs": "https://arxiv.org/abs/2510.22313", "authors": ["Chen Zhiqiang", "Le Gentil Cedric", "Lin Fuling", "Lu Minghao", "Qiao Qiyuan", "Xu Bowen", "Qi Yuhua", "Lu Peng"], "title": "Breaking the Static Assumption: A Dynamic-Aware LIO Framework Via Spatio-Temporal Normal Analysis", "comment": "8 pages, 7 figures, Accepted to IEEE Robotics and Automation Letters\n  (RA-L)", "summary": "This paper addresses the challenge of Lidar-Inertial Odometry (LIO) in\ndynamic environments, where conventional methods often fail due to their\nstatic-world assumptions. Traditional LIO algorithms perform poorly when\ndynamic objects dominate the scenes, particularly in geometrically sparse\nenvironments. Current approaches to dynamic LIO face a fundamental challenge:\naccurate localization requires a reliable identification of static features,\nyet distinguishing dynamic objects necessitates precise pose estimation. Our\nsolution breaks this circular dependency by integrating dynamic awareness\ndirectly into the point cloud registration process. We introduce a novel\ndynamic-aware iterative closest point algorithm that leverages spatio-temporal\nnormal analysis, complemented by an efficient spatial consistency verification\nmethod to enhance static map construction. Experimental evaluations demonstrate\nsignificant performance improvements over state-of-the-art LIO systems in\nchallenging dynamic environments with limited geometric structure. The code and\ndataset are available at https://github.com/thisparticle/btsa.", "AI": {"tldr": "\u672c\u8bba\u6587\u9488\u5bf9\u52a8\u6001\u73af\u5883\u4e2d\u6fc0\u5149\u96f7\u8fbe\u60ef\u6027\u91cc\u7a0b\u8ba1\uff08LIO\uff09\u9762\u4e34\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u611f\u77e5\u7684\u70b9\u4e91\u914d\u51c6\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfLIO\u65b9\u6cd5\u5728\u52a8\u6001\u7269\u4f53\u5360\u4e3b\u5bfc\u7684\u573a\u666f\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u51e0\u4f55\u7a00\u758f\u7684\u73af\u5883\u4e2d\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u52a8\u6001\u611f\u77e5\u80fd\u529b\u3002", "method": "\u7814\u7a76\u4e2d\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u52a8\u6001\u611f\u77e5\u8fed\u4ee3\u6700\u8fd1\u70b9\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e86\u65f6\u7a7a\u6cd5\u7ebf\u5206\u6790\u548c\u9ad8\u6548\u7684\u7a7a\u95f4\u4e00\u81f4\u6027\u9a8c\u8bc1\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5177\u6709\u6709\u9650\u51e0\u4f55\u7ed3\u6784\u7684\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u76f8\u8f83\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdbLIO\u7cfb\u7edf\u6709\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u672c\u8bba\u6587\u63d0\u51fa\u7684\u52a8\u6001\u611f\u77e5\u8fed\u4ee3\u6700\u8fd1\u70b9\u7b97\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e0b\u663e\u8457\u4f18\u5316\u4e86\u6fc0\u5149\u96f7\u8fbe\u60ef\u6027\u91cc\u7a0b\u8ba1\u7684\u6027\u80fd\u3002"}}
{"id": "2510.22336", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22336", "abs": "https://arxiv.org/abs/2510.22336", "authors": ["Bo Yue", "Sheng Xu", "Kui Jia", "Guiliang Liu"], "title": "Toward Humanoid Brain-Body Co-design: Joint Optimization of Control and Morphology for Fall Recovery", "comment": null, "summary": "Humanoid robots represent a central frontier in embodied intelligence, as\ntheir anthropomorphic form enables natural deployment in humans' workspace.\nBrain-body co-design for humanoids presents a promising approach to realizing\nthis potential by jointly optimizing control policies and physical morphology.\nWithin this context, fall recovery emerges as a critical capability. It not\nonly enhances safety and resilience but also integrates naturally with\nlocomotion systems, thereby advancing the autonomy of humanoids. In this paper,\nwe propose RoboCraft, a scalable humanoid co-design framework for fall recovery\nthat iteratively improves performance through the coupled updates of control\npolicy and morphology. A shared policy pretrained across multiple designs is\nprogressively finetuned on high-performing morphologies, enabling efficient\nadaptation without retraining from scratch. Concurrently, morphology search is\nguided by human-inspired priors and optimization algorithms, supported by a\npriority buffer that balances reevaluation of promising candidates with the\nexploration of novel designs. Experiments show that \\ourmethod{} achieves an\naverage performance gain of 44.55% on seven public humanoid robots, with\nmorphology optimization drives at least 40% of improvements in co-designing\nfour humanoid robots, underscoring the critical role of humanoid co-design.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86RoboCraft\uff0c\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u4eba\u5f62\u673a\u5668\u4eba\u5171\u8bbe\u8ba1\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u63a7\u5236\u7b56\u7565\u548c\u7269\u7406\u5f62\u6001\u6765\u589e\u5f3a\u8dcc\u5012\u6062\u590d\u80fd\u529b\u3002", "motivation": "\u4eba\u5f62\u673a\u5668\u4eba\u4f5c\u4e3a\u5177\u8eab\u667a\u80fd\u7684\u524d\u6cbf\u9886\u57df\uff0c\u5176\u4eba\u7c7b\u62df\u4eba\u5316\u7684\u5f62\u6001\u53ef\u4ee5\u81ea\u7136\u9002\u5e94\u4eba\u7c7b\u7684\u5de5\u4f5c\u73af\u5883\uff0c\u800c\u8dcc\u5012\u6062\u590d\u80fd\u529b\u662f\u5b9e\u73b0\u8fd9\u4e00\u6f5c\u529b\u7684\u5173\u952e\u80fd\u529b\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5171\u4eab\u7b56\u7565\u7684\u9884\u8bad\u7ec3\u548c\u9010\u6b65\u5fae\u8c03\uff0c\u4ee5\u53ca\u5229\u7528\u4eba\u7c7b\u542f\u53d1\u7684\u5148\u9a8c\u548c\u4f18\u5316\u7b97\u6cd5\u8fdb\u884c\u5f62\u6001\u641c\u7d22\uff0c\u6784\u5efa\u4e86RoboCraft\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRoboCraft\u5728\u4e03\u4e2a\u516c\u5171\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u5e73\u5747\u6027\u80fd\u63d0\u9ad8\u4e8644.55%\uff0c\u5f62\u6001\u4f18\u5316\u63a8\u52a8\u4e86\u81f3\u5c1140%\u7684\u6539\u8fdb\u3002", "conclusion": "RoboCraft\u901a\u8fc7\u4f18\u5316\u5f62\u6001\u548c\u63a7\u5236\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u7684\u8dcc\u5012\u6062\u590d\u6027\u80fd\uff0c\u7a81\u663e\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5171\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.22339", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.22339", "abs": "https://arxiv.org/abs/2510.22339", "authors": ["Enyi Wang", "Zhen Deng", "Chuanchuan Pan", "Bingwei He", "Jianwei Zhang"], "title": "Estimating Continuum Robot Shape under External Loading using Spatiotemporal Neural Networks", "comment": "2025 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS)", "summary": "This paper presents a learning-based approach for accurately estimating the\n3D shape of flexible continuum robots subjected to external loads. The proposed\nmethod introduces a spatiotemporal neural network architecture that fuses\nmulti-modal inputs, including current and historical tendon displacement data\nand RGB images, to generate point clouds representing the robot's deformed\nconfiguration. The network integrates a recurrent neural module for temporal\nfeature extraction, an encoding module for spatial feature extraction, and a\nmulti-modal fusion module to combine spatial features extracted from visual\ndata with temporal dependencies from historical actuator inputs. Continuous 3D\nshape reconstruction is achieved by fitting B\\'ezier curves to the predicted\npoint clouds. Experimental validation demonstrates that our approach achieves\nhigh precision, with mean shape estimation errors of 0.08 mm (unloaded) and\n0.22 mm (loaded), outperforming state-of-the-art methods in shape sensing for\nTDCRs. The results validate the efficacy of deep learning-based spatiotemporal\ndata fusion for precise shape estimation under loading conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65f6\u7a7a\u6570\u636e\u878d\u5408\u65b9\u6cd5\uff0c\u51c6\u786e\u4f30\u8ba1\u67d4\u6027\u673a\u5668\u4eba\u5728\u8d1f\u8f7d\u4e0b\u76843D\u5f62\u72b6\uff0c\u663e\u793a\u51fa\u4f18\u8d8a\u7684\u7cbe\u5ea6\u3002", "motivation": "\u51c6\u786e\u4f30\u8ba1\u5728\u5916\u90e8\u8d1f\u8f7d\u4e0b\u7684\u67d4\u6027\u673a\u5668\u4eba\u5f62\u72b6\uff0c\u4ee5\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u65f6\u7a7a\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u878d\u5408\u591a\u6a21\u6001\u8f93\u5165\uff0c\u4ee5\u4f30\u8ba1\u67d4\u6027\u8fde\u7eed\u673a\u5668\u4eba\u5728\u5916\u90e8\u8f7d\u8377\u4f5c\u7528\u4e0b\u76843D\u5f62\u72b6\u3002", "result": "\u901a\u8fc7\u7ed3\u5408\u5f53\u524d\u4e0e\u5386\u53f2\u7684\u808c\u8171\u4f4d\u79fb\u6570\u636e\u53caRGB\u56fe\u50cf\uff0c\u751f\u6210\u673a\u5668\u4eba\u7684\u53d8\u5f62\u70b9\u4e91\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\u5728\u65e0\u8d1f\u8f7d\u548c\u6709\u8d1f\u8f7d\u60c5\u51b5\u4e0b\uff0c\u5f62\u72b6\u4f30\u8ba1\u8bef\u5dee\u5206\u522b\u4e3a0.08mm\u548c0.22mm\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u7684\u65f6\u7a7a\u6570\u636e\u878d\u5408\u65b9\u6cd5\u5728\u8d1f\u8f7d\u6761\u4ef6\u4e0b\u6709\u6548\u63d0\u5347\u4e86\u67d4\u6027\u673a\u5668\u4eba\u7684\u5f62\u72b6\u4f30\u8ba1\u7cbe\u5ea6\u3002"}}
{"id": "2510.22370", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.22370", "abs": "https://arxiv.org/abs/2510.22370", "authors": ["Seyed Ahmad Hosseini Miangoleh", "Amin Jalal Aghdasian", "Farzaneh Abdollahi"], "title": "BLIP-FusePPO: A Vision-Language Deep Reinforcement Learning Framework for Lane Keeping in Autonomous Vehicles", "comment": "https://github.com/Amin-A96/BLIP-FusePPO-A-Vision-Language-Deep-Reinforcement-Learning-Framework-for-Lane-Keeping-in-Autonomous.git", "summary": "In this paper, we propose Bootstrapped Language-Image Pretraining-driven\nFused State Representation in Proximal Policy Optimization (BLIP-FusePPO), a\nnovel multimodal reinforcement learning (RL) framework for autonomous\nlane-keeping (LK), in which semantic embeddings generated by a vision-language\nmodel (VLM) are directly fused with geometric states, LiDAR observations, and\nProportional-Integral-Derivative-based (PID) control feedback within the agent\nobservation space. The proposed method lets the agent learn driving rules that\nare aware of their surroundings and easy to understand by combining high-level\nscene understanding from the VLM with low-level control and spatial signals.\nOur architecture brings together semantic, geometric, and control-aware\nrepresentations to make policy learning more robust. A hybrid reward function\nthat includes semantic alignment, LK accuracy, obstacle avoidance, and speed\nregulation helps learning to be more efficient and generalizable. Our method is\ndifferent from the approaches that only use semantic models to shape rewards.\nInstead, it directly embeds semantic features into the state representation.\nThis cuts down on expensive runtime inference and makes sure that semantic\nguidance is always available. The simulation results show that the proposed\nmodel is better at LK stability and adaptability than the best vision-based and\nmultimodal RL baselines in a wide range of difficult driving situations. We\nmake our code publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u5f3a\u5316\u5b66\u4e60\u6846\u67b6BLIP-FusePPO\uff0c\u80fd\u6709\u6548\u878d\u5408\u8bed\u4e49\u3001\u51e0\u4f55\u548c\u63a7\u5236\u4fe1\u606f\uff0c\u63d0\u9ad8\u81ea\u4e3b\u8f66\u9053\u4fdd\u6301\u7684\u6027\u80fd\u3002", "motivation": "\u65e8\u5728\u521b\u5efa\u4e00\u79cd\u80fd\u591f\u6709\u6548\u6574\u5408\u9ad8\u5c42\u573a\u666f\u7406\u89e3\u548c\u4f4e\u5c42\u63a7\u5236\u4fe1\u53f7\u7684\u667a\u80fd\u4f53\uff0c\u4ee5\u63d0\u9ad8\u9a7e\u9a76\u89c4\u5219\u7684\u5b66\u4e60\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u878d\u5408\u89c6\u56fe-\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u8bed\u4e49\u5d4c\u5165\u3001\u51e0\u4f55\u72b6\u6001\u3001\u6fc0\u5149\u96f7\u8fbe\u89c2\u6d4b\u548c\u57fa\u4e8ePID\u63a7\u5236\u7684\u53cd\u9988\uff0c\u4ece\u800c\u5f62\u6210\u4e86\u4e00\u4e2a\u65b0\u7684\u591a\u6a21\u6001\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u4e3b\u8f66\u9053\u4fdd\u6301\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u8f66\u9053\u4fdd\u6301\u7684\u7a33\u5b9a\u6027\u548c\u9002\u5e94\u6027\u65b9\u9762\u4f18\u4e8e\u591a\u79cd\u73b0\u6709\u57fa\u7ebf\uff0c\u4e14\u901a\u8fc7\u6df7\u5408\u5956\u52b1\u51fd\u6570\u4f7f\u5b66\u4e60\u66f4\u52a0\u9ad8\u6548\u548c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684BLIP-FusePPO\u6846\u67b6\u5728\u591a\u79cd\u590d\u6742\u9a7e\u9a76\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u8f66\u9053\u4fdd\u6301\u7a33\u5b9a\u6027\u548c\u9002\u5e94\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u89c6\u89c9\u57fa\u7840\u548c\u591a\u6a21\u6001\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u3002"}}
{"id": "2510.22420", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.22420", "abs": "https://arxiv.org/abs/2510.22420", "authors": ["Mohammad Ali Labbaf Khaniki", "Fateme Taroodi", "Benyamin Safizadeh"], "title": "A Novel Multi-Timescale Stability-Preserving Hierarchical Reinforcement Learning Controller Framework for Adaptive Control in High-Dimensional Dynamical Systems", "comment": null, "summary": "Controlling high-dimensional stochastic systems, critical in robotics,\nautonomous vehicles, and hyperchaotic systems, faces the curse of\ndimensionality, lacks temporal abstraction, and often fails to ensure\nstochastic stability. To overcome these limitations, this study introduces the\nMulti-Timescale Lyapunov-Constrained Hierarchical Reinforcement Learning\n(MTLHRL) framework. MTLHRL integrates a hierarchical policy within a\nsemi-Markov Decision Process (SMDP), featuring a high-level policy for\nstrategic planning and a low-level policy for reactive control, which\neffectively manages complex, multi-timescale decision-making and reduces\ndimensionality overhead. Stability is rigorously enforced using a neural\nLyapunov function optimized via Lagrangian relaxation and multi-timescale\nactor-critic updates, ensuring mean-square boundedness or asymptotic stability\nin the face of stochastic dynamics. The framework promotes efficient and\nreliable learning through trust-region constraints and decoupled optimization.\nExtensive simulations on an 8D hyperchaotic system and a 5-DOF robotic\nmanipulator demonstrate MTLHRL's empirical superiority. It significantly\noutperforms baseline methods in both stability and performance, recording the\nlowest error indices (e.g., Integral Absolute Error (IAE): 3.912 in\nhyperchaotic control and IAE: 1.623 in robotics), achieving faster convergence,\nand exhibiting superior disturbance rejection. MTLHRL offers a theoretically\ngrounded and practically viable solution for robust control of complex\nstochastic systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86MTLHRL\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u9ad8\u9ad8\u7ef4\u968f\u673a\u7cfb\u7edf\u7684\u63a7\u5236\u6548\u7387\u548c\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u662f\u5728\u673a\u5668\u4eba\u3001\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u548c\u8d85\u6df7\u6c8c\u7cfb\u7edf\u4e2d\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u548c\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u7684\u53d1\u5c55\uff0c\u63a7\u5236\u9ad8\u7ef4\u968f\u673a\u7cfb\u7edf\u7684\u9700\u6c42\u65e5\u76ca\u589e\u52a0\uff0c\u56e0\u6b64\u9700\u8981\u6709\u6548\u7684\u65b9\u6cd5\u4ee5\u5e94\u5bf9\u7cfb\u7edf\u7684\u590d\u6742\u6027\u4e0e\u4e0d\u786e\u5b9a\u6027\u3002", "method": "MTLHRL\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u5c42\u6b21\u5316\u7b56\u7565\u4e8e\u534a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08SMDP\uff09\uff0c\u5b9e\u73b0\u9ad8\u5c42\u6b21\u6218\u7565\u89c4\u5212\u548c\u4f4e\u5c42\u6b21\u53cd\u5e94\u63a7\u5236\uff0c\u540c\u65f6\u5229\u7528\u795e\u7ecf\u674e\u96c5\u666e\u8bfa\u592b\u51fd\u6570\u548c\u591a\u65f6\u95f4\u5c3a\u5ea6\u6f14\u5458-\u8bc4\u8bba\u5bb6\u66f4\u65b0\u6765\u786e\u4fdd\u7cfb\u7edf\u7a33\u5b9a\u3002", "result": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u591a\u65f6\u95f4\u5c3a\u5ea6\u674e\u96c5\u666e\u8bfa\u592b\u7ea6\u675f\u5c42\u6b21\u5f3a\u5316\u5b66\u4e60\uff08MTLHRL\uff09\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u9ad8\u7ef4\u968f\u673a\u7cfb\u7edf\u63a7\u5236\u4e2d\u7684\u4e3b\u8981\u6311\u6218\uff0c\u5305\u62ec\u7ef4\u5ea6\u8bc5\u5492\u3001\u7f3a\u4e4f\u65f6\u95f4\u62bd\u8c61\u4ee5\u53ca\u65e0\u6cd5\u786e\u4fdd\u968f\u673a\u7a33\u5b9a\u6027\u7684\u95ee\u9898\u3002", "conclusion": "MTLHRL\u662f\u4e00\u4e2a\u7406\u8bba\u57fa\u7840\u624e\u5b9e\u4e14\u5728\u5b9e\u8df5\u4e2d\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u590d\u6742\u968f\u673a\u7cfb\u7edf\u7684\u5f3a\u5065\u63a7\u5236\u3002"}}
{"id": "2510.22448", "categories": ["cs.RO", "I.2.9"], "pdf": "https://arxiv.org/pdf/2510.22448", "abs": "https://arxiv.org/abs/2510.22448", "authors": ["Pranup Chhetri", "Alejandro Torrejon", "Sergio Eslava", "Luis J. Manso"], "title": "A short methodological review on social robot navigation benchmarking", "comment": "18 pages, 14 of which references. 3 figures, 2 tables", "summary": "Social Robot Navigation is the skill that allows robots to move efficiently\nin human-populated environments while ensuring safety, comfort, and trust.\nUnlike other areas of research, the scientific community has not yet achieved\nan agreement on how Social Robot Navigation should be benchmarked. This is\nnotably important, as the lack of a de facto standard to benchmark Social Robot\nNavigation can hinder the progress of the field and may lead to contradicting\nconclusions. Motivated by this gap, we contribute with a short review focused\nexclusively on benchmarking trends in the period from January 2020 to July\n2025. Of the 130 papers identified by our search using IEEE Xplore, we analysed\nthe 85 papers that met the criteria of the review. This review addresses the\nmetrics used in the literature for benchmarking purposes, the algorithms\nemployed in such benchmarks, the use of human surveys for benchmarking, and how\nconclusions are drawn from the benchmarking results, when applicable.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u793e\u4ea4\u673a\u5668\u4eba\u5bfc\u822a\u7684\u57fa\u51c6\u6d4b\u8bd5\u8d8b\u52bf\uff0c\u6307\u51fa\u4e86\u8be5\u9886\u57df\u6807\u51c6\u7f3a\u4e4f\u7684\u95ee\u9898\u4ee5\u53ca\u76f8\u5173\u6587\u732e\u7684\u5206\u6790\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u793e\u4ea4\u673a\u5668\u4eba\u5bfc\u822a\u9886\u57df\u57fa\u51c6\u6d4b\u8bd5\u6807\u51c6\u7f3a\u5931\u7684\u7a7a\u767d\uff0c\u4ee5\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u5c55\u3002", "method": "\u901a\u8fc7IEEE Xplore\u6570\u636e\u5e93\uff0c\u7b5b\u9009\u51fa130\u7bc7\u76f8\u5173\u8bba\u6587\uff0c\u5e76\u5bf9\u7b26\u5408\u8bc4\u5ba1\u6807\u51c6\u768485\u7bc7\u8bba\u6587\u8fdb\u884c\u4e86\u5206\u6790\u3002", "result": "\u672c\u7814\u7a76\u5bf9\u793e\u4ea4\u673a\u5668\u4eba\u5bfc\u822a\u7684\u57fa\u51c6\u6d4b\u8bd5\u8fdb\u884c\u4e86\u7efc\u8ff0\uff0c\u5206\u6790\u4e86\u8be5\u9886\u57df\u57282020\u5e74\u81f32025\u5e74\u95f4\u7684\u8d8b\u52bf\uff0c\u6307\u51fa\u4e86\u5f53\u524d\u57fa\u51c6\u6d4b\u8bd5\u6807\u51c6\u7f3a\u4e4f\u7684\u95ee\u9898\uff0c\u5e76\u63a2\u8ba8\u4e86\u76f8\u5173\u6587\u732e\u4e2d\u4f7f\u7528\u7684\u5ea6\u91cf\u6807\u51c6\u3001\u7b97\u6cd5\u3001\u4ee5\u53ca\u4eba\u7c7b\u8c03\u67e5\u7684\u5e94\u7528\u60c5\u51b5\u3002", "conclusion": "\u7f3a\u4e4f\u7edf\u4e00\u7684\u57fa\u51c6\u6d4b\u8bd5\u6807\u51c6\u4e25\u91cd\u5f71\u54cd\u4e86\u793e\u4ea4\u673a\u5668\u4eba\u5bfc\u822a\u9886\u57df\u7684\u8fdb\u5c55\uff0c\u56e0\u6b64\u4e9f\u9700\u5236\u5b9a\u6709\u6548\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\u3002"}}
{"id": "2510.22465", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.22465", "abs": "https://arxiv.org/abs/2510.22465", "authors": ["Sourabh Karmakar", "Cameron J. Turner"], "title": "Forward Kinematics Solution For A General Stewart Platform Through Iteration Based Simulation", "comment": null, "summary": "This paper presents a method to generate feasible, unique forward-kinematic\nsolutions for a general Stewart platform. This is done by using inverse\nkinematics to obtain valid workspace data and corresponding actuator lengths\nfor the moving platform. For parallel kinematic machines, such as the Stewart\nPlatform, inverse kinematics are straight forward, but the forward kinematics\nare complex and generates multiple solutions due to the closed loop structure\nof the kinematic links. In this research, a simple iterative algorithm has been\nused employing modified Denavit-Hartenberg convention. The outcome is\nencouraging as this method generates a single feasible forward kinematic\nsolution for each valid pose with the solved DH parameters and unlike earlier\nforward kinematics solutions, this unique solution does not need to be manually\nverified. Therefore, the forward kinematic solutions can be used directly for\nfurther calculations without the need for manual pose verification. This\ncapability is essential for the six degree of freedom materials testing system\ndeveloped by the authors in their laboratory. The developed system is aimed at\ncharacterizing additively manufactured materials under complex combined\nmultiple loading conditions. The material characterization is done by enabling\nhigh precision force control on the moving platform via in situ calibration of\nthe as-built kinematics of the Stewart Gough Platform.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9Stewart\u5e73\u53f0\u7684\u524d\u5411\u8fd0\u52a8\u5b66\u89e3\u51b3\u65b9\u6848\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u751f\u6210\u552f\u4e00\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e14\u4e0d\u9700\u624b\u52a8\u9a8c\u8bc1\u3002", "motivation": "\u89e3\u51b3Stewart\u5e73\u53f0\u7684\u590d\u6742\u524d\u5411\u8fd0\u52a8\u5b66\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u6750\u6599\u6d4b\u8bd5\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u9006\u8fd0\u52a8\u5b66\u83b7\u53d6\u5de5\u4f5c\u7a7a\u95f4\u6570\u636e\u548c\u76f8\u5e94\u7684\u6267\u884c\u5668\u957f\u5ea6\uff0c\u91c7\u7528\u4fee\u6539\u540e\u7684Denavit-Hartenberg\u7ea6\u5b9a\u7684\u7b80\u5355\u8fed\u4ee3\u7b97\u6cd5\u3002", "result": "\u6bcf\u4e2a\u6709\u6548\u59ff\u6001\u751f\u6210\u552f\u4e00\u7684\u524d\u5411\u8fd0\u52a8\u5b66\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4ee5\u76f4\u63a5\u7528\u4e8e\u540e\u7eed\u8ba1\u7b97\uff0c\u663e\u8457\u7b80\u5316\u4e86\u8fc7\u7a0b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u786e\u4fdd\u4e86Stewart\u5e73\u53f0\u7684\u6bcf\u4e2a\u6709\u6548\u59ff\u6001\u90fd\u80fd\u5f97\u5230\u4e00\u4e2a\u53ef\u7528\u7684\u524d\u5411\u8fd0\u52a8\u5b66\u89e3\u51b3\u65b9\u6848\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6750\u6599\u6d4b\u8bd5\u7cfb\u7edf\u7684\u7cbe\u5ea6\u548c\u6548\u7387\u3002"}}
{"id": "2510.22504", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.22504", "abs": "https://arxiv.org/abs/2510.22504", "authors": ["Ciera McFarland", "Antonio Alvarez", "Sarah Taher", "Nathaniel Hanson", "Margaret McGuinness"], "title": "On Steerability Factors for Growing Vine Robots", "comment": null, "summary": "Vine robots extend their tubular bodies by everting material from the tip,\nenabling navigation in complex environments with a minimalist soft body.\nDespite their promise for field applications, especially in the urban search\nand rescue domain, performance is constrained by the weight of attached sensors\nor tools, as well as other design and control choices. This work investigates\nhow tip load, pressure, length, diameter, and fabrication method shape vine\nrobot steerability--the ability to maneuver with controlled curvature--for\nrobots that steer with series pouch motor-style pneumatic actuators. We conduct\ntwo groups of experiments: (1) studying tip load, chamber pressure, length, and\ndiameter in a robot supporting itself against gravity, and (2) studying\nfabrication method and ratio of actuator to chamber pressure in a robot\nsupported on the ground. Results show that steerability decreases with\nincreasing tip load, is best at moderate chamber pressure, increases with\nlength, and is largely unaffected by diameter. Robots with actuators attached\non their exterior begin curving at low pressure ratios, but curvature saturates\nat high pressure ratios; those with actuators integrated into the robot body\nrequire higher pressure ratios to begin curving but achieve higher curvature\noverall. We demonstrate that robots optimized with these principles outperform\nthose with ad hoc parameters in a mobility task that involves maximizing upward\nand horizontal curvatures.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5f71\u54cd\u85e4\u8513\u673a\u5668\u4eba\uff08\u4f7f\u7528\u7cfb\u5217\u888b\u72b6\u7535\u673a\u5f0f\u6c14\u52a8\u6267\u884c\u5668\uff09\u5f15\u5bfc\u6027\u7684\u591a\u4e2a\u56e0\u7d20\uff0c\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\u5c16\u7aef\u8d1f\u8f7d\u3001\u538b\u529b\u3001\u957f\u5ea6\u548c\u5236\u9020\u65b9\u6cd5\u5bf9\u5f15\u5bfc\u6027\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4f18\u5316\u539f\u5219\u4ee5\u63d0\u9ad8\u673a\u5668\u4eba\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u85e4\u8513\u673a\u5668\u4eba\u5728\u57ce\u5e02\u641c\u7d22\u548c\u6551\u63f4\u9886\u57df\u6709\u6f5c\u5728\u5e94\u7528\uff0c\u4f46\u5176\u6027\u80fd\u53d7\u5230\u9644\u52a0\u4f20\u611f\u5668\u6216\u5de5\u5177\u7684\u91cd\u91cf\u4ee5\u53ca\u8bbe\u8ba1\u548c\u63a7\u5236\u9009\u62e9\u7684\u9650\u5236\u3002", "method": "\u672c\u7814\u7a76\u8fdb\u884c\u4e24\u7ec4\u5b9e\u9a8c\uff1a\u7b2c\u4e00\u7ec4\u7814\u7a76\u673a\u5668\u4eba\u5728\u81ea\u8eab\u91cd\u529b\u4e0b\u652f\u6301\u7684\u60c5\u51b5\u4e0b\uff0c\u8003\u8651\u5c16\u7aef\u8d1f\u8f7d\u3001\u8154\u5ba4\u538b\u529b\u3001\u957f\u5ea6\u548c\u76f4\u5f84\uff1b\u7b2c\u4e8c\u7ec4\u7814\u7a76\u5728\u5730\u9762\u652f\u6491\u7684\u673a\u5668\u4eba\u60c5\u51b5\u4e0b\uff0c\u8003\u8651\u5236\u9020\u65b9\u6cd5\u548c\u6267\u884c\u5668\u4e0e\u8154\u5ba4\u538b\u529b\u7684\u6bd4\u7387\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u968f\u7740\u5c16\u7aef\u8d1f\u8f7d\u7684\u589e\u52a0\uff0c\u5f15\u5bfc\u80fd\u529b\u4e0b\u964d\uff1b\u5728\u9002\u4e2d\u8154\u5ba4\u538b\u529b\u4e0b\u5f15\u5bfc\u80fd\u529b\u6700\u4f73\uff1b\u957f\u5ea6\u589e\u52a0\u65f6\u5f15\u5bfc\u80fd\u529b\u589e\u5f3a\uff0c\u76f4\u5f84\u5219\u57fa\u672c\u4e0d\u5f71\u54cd\u5f15\u5bfc\u80fd\u529b\u3002\u5916\u90e8\u9644\u52a0\u6267\u884c\u5668\u7684\u673a\u5668\u4eba\u5728\u4f4e\u538b\u529b\u6bd4\u65f6\u5f00\u59cb\u5f2f\u66f2\uff0c\u4f46\u5728\u9ad8\u538b\u529b\u6bd4\u65f6\u66f2\u7387\u9971\u548c\uff1b\u800c\u5c06\u6267\u884c\u5668\u96c6\u6210\u5230\u673a\u5668\u4eba\u4e3b\u4f53\u5185\u7684\u673a\u5668\u4eba\u9700\u8981\u66f4\u9ad8\u7684\u538b\u529b\u6bd4\u624d\u80fd\u5f00\u59cb\u5f2f\u66f2\uff0c\u4f46\u6574\u4f53\u66f2\u7387\u66f4\u9ad8\u3002", "conclusion": "\u7ecf\u8fc7\u4f18\u5316\u7684\u673a\u5668\u4eba\u5728\u6d89\u53ca\u6700\u5927\u5316\u5411\u4e0a\u548c\u6c34\u5e73\u66f2\u7387\u7684\u79fb\u52a8\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u90a3\u4e9b\u91c7\u7528\u4e34\u65f6\u53c2\u6570\u7684\u673a\u5668\u4eba\u3002"}}
{"id": "2510.22524", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.22524", "abs": "https://arxiv.org/abs/2510.22524", "authors": ["Shenbagaraj Kannapiran", "Elena Oikonomou", "Albert Chu", "Spring Berman", "Theodore P. Pavlic"], "title": "Ant-inspired Walling Strategies for Scalable Swarm Separation: Reinforcement Learning Approaches Based on Finite State Machines", "comment": null, "summary": "In natural systems, emergent structures often arise to balance competing\ndemands. Army ants, for example, form temporary \"walls\" that prevent\ninterference between foraging trails. Inspired by this behavior, we developed\ntwo decentralized controllers for heterogeneous robotic swarms to maintain\nspatial separation while executing concurrent tasks. The first is a\nfinite-state machine (FSM)-based controller that uses encounter-triggered\ntransitions to create rigid, stable walls. The second integrates FSM states\nwith a Deep Q-Network (DQN), dynamically optimizing separation through emergent\n\"demilitarized zones.\" In simulation, both controllers reduce mixing between\nsubgroups, with the DQN-enhanced controller improving adaptability and reducing\nmixing by 40-50% while achieving faster convergence.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e24\u79cd\u53bb\u4e2d\u5fc3\u5316\u63a7\u5236\u5668\uff0c\u4ee5\u5e2e\u52a9\u5f02\u6784\u673a\u5668\u4eba\u7fa4\u4f53\u5728\u6267\u884c\u4efb\u52a1\u65f6\u4fdd\u6301\u7a7a\u95f4\u5206\u79bb\uff0c\u6a21\u62df\u7ed3\u679c\u8868\u660eDQN\u589e\u5f3a\u63a7\u5236\u5668\u5728\u51cf\u5c11\u5b50\u7fa4\u4f53\u6df7\u5408\u53ca\u63d0\u9ad8\u9002\u5e94\u6027\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\u3002", "motivation": "\u53d7\u5230\u81ea\u7136\u7cfb\u7edf\u4e2d\u51fa\u73b0\u7684\u7ed3\u6784\u542f\u53d1\uff0c\u6bd4\u5982\u519b\u8682\u8681\u5f62\u6210\u7684\u4e34\u65f6\u201c\u5899\u201d\uff0c\u672c\u7814\u7a76\u65e8\u5728\u4e3a\u5f02\u6784\u673a\u5668\u4eba\u7fa4\u4f53\u8bbe\u8ba1\u63a7\u5236\u5668\u6765\u7ef4\u62a4\u7a7a\u95f4\u5206\u79bb\uff0c\u907f\u514d\u5e72\u6270\u3002", "method": "\u91c7\u7528\u6709\u9650\u72b6\u6001\u673a\uff08FSM\uff09\u548c\u6df1\u5ea6Q\u7f51\u7edc\uff08DQN\uff09\u7ed3\u5408\u7684\u4e24\u79cd\u53bb\u4e2d\u5fc3\u5316\u63a7\u5236\u65b9\u6848\uff0c\u5206\u522b\u521b\u5efa\u7a33\u5b9a\u5899\u548c\u52a8\u6001\u4f18\u5316\u7684\u201c\u975e\u519b\u4e8b\u533a\u201d\u4ee5\u7ef4\u6301\u673a\u5668\u4eba\u7fa4\u4f53\u7684\u5206\u79bb\u3002", "result": "\u5728\u6a21\u62df\u4e2d\uff0c\u4e24\u4e2a\u63a7\u5236\u5668\u5747\u6709\u6548\u51cf\u5c11\u4e86\u5b50\u7fa4\u4f53\u4e4b\u95f4\u7684\u6df7\u5408\uff0c\u5176\u4e2dDQN\u589e\u5f3a\u7684\u63a7\u5236\u5668\u5728\u9002\u5e94\u6027\u548c\u6536\u655b\u901f\u5ea6\u4e0a\u90fd\u6709\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "\u8fd9\u4e24\u79cd\u53bb\u4e2d\u5fc3\u5316\u63a7\u5236\u5668\u6709\u6548\u5730\u7ef4\u62a4\u4e86\u5f02\u6784\u673a\u5668\u4eba\u7fa4\u4f53\u5728\u6267\u884c\u5e76\u884c\u4efb\u52a1\u65f6\u7684\u7a7a\u95f4\u5206\u79bb\uff0cDQN\u589e\u5f3a\u7684\u63a7\u5236\u5668\u663e\u8457\u63d0\u9ad8\u4e86\u9002\u5e94\u6027\u5e76\u51cf\u5c11\u4e8640-50%\u7684\u6df7\u5408\u60c5\u51b5\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u3002"}}
{"id": "2510.22568", "categories": ["cs.RO", "cs.AI", "cs.MA", "cs.SY", "eess.SY", "I.2.9; I.2.11; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.22568", "abs": "https://arxiv.org/abs/2510.22568", "authors": ["Onur Akg\u00fcn"], "title": "SPIRAL: Self-Play Incremental Racing Algorithm for Learning in Multi-Drone Competitions", "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "This paper introduces SPIRAL (Self-Play Incremental Racing Algorithm for\nLearning), a novel approach for training autonomous drones in multi-agent\nracing competitions. SPIRAL distinctively employs a self-play mechanism to\nincrementally cultivate complex racing behaviors within a challenging, dynamic\nenvironment. Through this self-play core, drones continuously compete against\nincreasingly proficient versions of themselves, naturally escalating the\ndifficulty of competitive interactions. This progressive learning journey\nguides agents from mastering fundamental flight control to executing\nsophisticated cooperative multi-drone racing strategies. Our method is designed\nfor versatility, allowing integration with any state-of-the-art Deep\nReinforcement Learning (DRL) algorithms within its self-play framework.\nSimulations demonstrate the significant advantages of SPIRAL and benchmark the\nperformance of various DRL algorithms operating within it. Consequently, we\ncontribute a versatile, scalable, and self-improving learning framework to the\nfield of autonomous drone racing. SPIRAL's capacity to autonomously generate\nappropriate and escalating challenges through its self-play dynamic offers a\npromising direction for developing robust and adaptive racing strategies in\nmulti-agent environments. This research opens new avenues for enhancing the\nperformance and reliability of autonomous racing drones in increasingly complex\nand competitive scenarios.", "AI": {"tldr": "SPIRAL\u662f\u4e00\u79cd\u65b0\u578b\u81ea\u6211\u5bf9\u5f08\u7b97\u6cd5\uff0c\u7528\u4e8e\u8bad\u7ec3\u81ea\u4e3b\u65e0\u4eba\u673a\u5728\u591a\u667a\u80fd\u4f53\u8d5b\u8f66\u4e2d\u7684\u8868\u73b0\uff0c\u80fd\u591f\u6e10\u8fdb\u5730\u63d0\u5347\u8d5b\u8f66\u7b56\u7565\u548c\u80fd\u529b\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u7075\u6d3b\u4e14\u80fd\u9002\u5e94\u590d\u6742\u52a8\u6001\u73af\u5883\u7684\u8d5b\u8f66\u7b56\u7565\uff0c\u63d0\u9ad8\u81ea\u4e3b\u65e0\u4eba\u673a\u5728\u7ade\u4e89\u6fc0\u70c8\u573a\u666f\u4e2d\u7684\u8868\u73b0\u548c\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\u81ea\u6211\u5bf9\u5f08\u673a\u5236\u8bad\u7ec3\u65e0\u4eba\u673a\uff0c\u901a\u8fc7\u4e0e\u81ea\u8eab\u6108\u53d1\u4f18\u79c0\u7684\u7248\u672c\u7ade\u4e89\u6765\u9010\u6b65\u63d0\u9ad8\u8d5b\u8f66\u80fd\u529b\uff0c\u6700\u7ec8\u638c\u63e1\u590d\u6742\u7684\u591a\u65e0\u4eba\u673a\u5408\u4f5c\u7ade\u8d5b\u7b56\u7565\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u6211\u5bf9\u5f08\u589e\u91cf\u8d5b\u8f66\u7b97\u6cd5SPIRAL\uff0c\u7528\u4e8e\u8bad\u7ec3\u81ea\u4e3b\u65e0\u4eba\u673a\u5728\u591a\u667a\u80fd\u4f53 racing \u6bd4\u8d5b\u4e2d\u7684\u8868\u73b0\u3002SPIRAL\u901a\u8fc7\u81ea\u6211\u5bf9\u5f08\u673a\u5236\uff0c\u5728\u52a8\u6001\u6311\u6218\u7684\u73af\u5883\u4e2d\u9010\u6b65\u57f9\u517b\u590d\u6742\u7684\u6bd4\u8d5b\u884c\u4e3a\uff0c\u4f7f\u65e0\u4eba\u673a\u4e0d\u65ad\u5730\u4e0e\u81ea\u8eab\u66f4\u5f3a\u5927\u7684\u7248\u672c\u7ade\u4e89\uff0c\u4ece\u800c\u4e0d\u65ad\u589e\u52a0\u7ade\u4e89\u7684\u96be\u5ea6\u3002\u8fd9\u79cd\u6e10\u8fdb\u5f0f\u7684\u5b66\u4e60\u8fc7\u7a0b\u5f15\u5bfc\u4ee3\u7406\u4ece\u638c\u63e1\u57fa\u672c\u98de\u884c\u63a7\u5236\u5230\u6267\u884c\u590d\u6742\u7684\u591a\u65e0\u4eba\u673a\u5408\u4f5c\u8d5b\u8f66\u7b56\u7565\u3002\u8be5\u65b9\u6cd5\u5177\u5907\u591a\u6837\u6027\uff0c\u5141\u8bb8\u4e0e\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u96c6\u6210\u3002\u5728\u6a21\u62df\u4e2d\uff0cSPIRAL\u5c55\u793a\u4e86\u663e\u8457\u7684\u4f18\u52bf\uff0c\u5e76\u57fa\u51c6\u6d4b\u8bd5\u4e86\u5404\u79cd\u5728\u5176\u6846\u67b6\u4e0b\u8fd0\u884c\u7684DRL\u7b97\u6cd5\u7684\u8868\u73b0\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u4e3a\u81ea\u4e3b\u65e0\u4eba\u673a\u8d5b\u8f66\u9886\u57df\u8d21\u732e\u4e86\u4e00\u4e2a\u591a\u529f\u80fd\u3001\u53ef\u6269\u5c55\u548c\u81ea\u6211\u6539\u8fdb\u7684\u5b66\u4e60\u6846\u67b6\u3002SPIRAL\u901a\u8fc7\u5176\u81ea\u6211\u5bf9\u5f08\u52a8\u6001\u81ea\u4e3b\u751f\u6210\u5408\u9002\u548c\u9010\u6e10\u5347\u7ea7\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u4ee5\u5f00\u53d1\u9c81\u68d2\u548c\u9002\u5e94\u6027\u5f3a\u7684\u8d5b\u8f66\u7b56\u7565\u3002\u56e0\u6b64\uff0c\u8fd9\u9879\u7814\u7a76\u5f00\u8f9f\u4e86\u63d0\u5347\u81ea\u4e3b\u8d5b\u8f66\u65e0\u4eba\u673a\u5728\u8d8a\u6765\u8d8a\u590d\u6742\u548c\u7ade\u4e89\u6fc0\u70c8\u573a\u666f\u4e2d\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u7684\u65b0\u9014\u5f84\u3002", "conclusion": "SPIRAL\u4e3a\u81ea\u4e3b\u65e0\u4eba\u673a\u8d5b\u8f66\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u4e14\u5177\u9002\u5e94\u6027\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u81ea\u52a8\u751f\u6210\u9010\u6e10\u63d0\u5347\u7684\u6311\u6218\uff0c\u4ece\u800c\u53ef\u4ee5\u5728\u590d\u6742\u548c\u7ade\u4e89\u6fc0\u70c8\u7684\u73af\u5883\u4e2d\u589e\u5f3a\u65e0\u4eba\u673a\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2510.22570", "categories": ["cs.RO", "cs.AI", "cs.MA", "cs.SY", "eess.SY", "I.2.9; I.2.11; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.22570", "abs": "https://arxiv.org/abs/2510.22570", "authors": ["Onur Akg\u00fcn"], "title": "Curriculum-Based Iterative Self-Play for Scalable Multi-Drone Racing", "comment": "13 pages, 5 figures. This paper is currently under review at the\n  journal Engineering Applications of Artificial Intelligence. Supplementary\n  video: https://drive.google.com/file/d/1k7necen2DgIxaYT2alKK8-b20sE_AyDA/view\n  Source code and models: https://doi.org/10.5281/zenodo.17256943", "summary": "The coordination of multiple autonomous agents in high-speed, competitive\nenvironments represents a significant engineering challenge. This paper\npresents CRUISE (Curriculum-Based Iterative Self-Play for Scalable Multi-Drone\nRacing), a reinforcement learning framework designed to solve this challenge in\nthe demanding domain of multi-drone racing. CRUISE overcomes key scalability\nlimitations by synergistically combining a progressive difficulty curriculum\nwith an efficient self-play mechanism to foster robust competitive behaviors.\nValidated in high-fidelity simulation with realistic quadrotor dynamics, the\nresulting policies significantly outperform both a standard reinforcement\nlearning baseline and a state-of-the-art game-theoretic planner. CRUISE\nachieves nearly double the planner's mean racing speed, maintains high success\nrates, and demonstrates robust scalability as agent density increases. Ablation\nstudies confirm that the curriculum structure is the critical component for\nthis performance leap. By providing a scalable and effective training\nmethodology, CRUISE advances the development of autonomous systems for dynamic,\ncompetitive tasks and serves as a blueprint for future real-world deployment.", "AI": {"tldr": "CRUISE\u662f\u4e00\u79cd\u7ed3\u5408\u6e10\u8fdb\u96be\u5ea6\u8bfe\u7a0b\u548c\u9ad8\u6548\u81ea\u6211\u5bf9\u6297\u673a\u5236\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u591a\u65e0\u4eba\u673a\u7ade\u901f\u4e2d\u7684\u534f\u4f5c\u6311\u6218\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u7ade\u901f\u8868\u73b0\u3002", "motivation": "\u5728\u5feb\u901f\u7ade\u4e89\u73af\u5883\u4e2d\u7684\u591a\u81ea\u4e3b\u4ee3\u7406\u534f\u4f5c\u662f\u4e00\u9879\u91cd\u5927\u5de5\u7a0b\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u591a\u65e0\u4eba\u673a\u7ade\u901f\u4e2d\u3002", "method": "\u901a\u8fc7\u5c06\u6e10\u8fdb\u96be\u5ea6\u8bfe\u7a0b\u4e0e\u9ad8\u6548\u81ea\u6211\u5bf9\u6297\u673a\u5236\u76f8\u7ed3\u5408\uff0cCRUISE\u514b\u670d\u4e86\u53ef\u6269\u5c55\u6027\u9650\u5236\uff0c\u4fc3\u8fdb\u4e86\u575a\u97e7\u7684\u7ade\u4e89\u884c\u4e3a\u7684\u57f9\u517b\u3002", "result": "CRUISE\u6846\u67b6\u5728\u9ad8\u4fdd\u771f\u6a21\u62df\u4e2d\u53d6\u5f97\u7684\u7b56\u7565\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u548c\u6700\u65b0\u7684\u535a\u5f08\u8bba\u89c4\u5212\u5668\uff0c\u51e0\u4e4e\u8fbe\u5230\u4e86\u89c4\u5212\u5668\u5e73\u5747\u7ade\u901f\u7684\u4e24\u500d\uff0c\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u6210\u529f\u7387\uff0c\u4e14\u5c55\u793a\u4e86\u968f\u7740\u4ee3\u7406\u5bc6\u5ea6\u589e\u52a0\u7684\u5f3a\u5927\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "CRUISE\u4e3a\u52a8\u6001\u7ade\u4e89\u4efb\u52a1\u7684\u81ea\u4e3b\u7cfb\u7edf\u53d1\u5c55\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u5b9e\u9645\u90e8\u7f72\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.22600", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22600", "abs": "https://arxiv.org/abs/2510.22600", "authors": ["Huilin Yin", "Zhaolin Yang", "Linchuan Zhang", "Gerhard Rigoll", "Johannes Betz"], "title": "RoGER-SLAM: A Robust Gaussian Splatting SLAM System for Noisy and Low-light Environment Resilience", "comment": "13 pages, 11 figures, under review", "summary": "The reliability of Simultaneous Localization and Mapping (SLAM) is severely\nconstrained in environments where visual inputs suffer from noise and low\nillumination. Although recent 3D Gaussian Splatting (3DGS) based SLAM\nframeworks achieve high-fidelity mapping under clean conditions, they remain\nvulnerable to compounded degradations that degrade mapping and tracking\nperformance. A key observation underlying our work is that the original 3DGS\nrendering pipeline inherently behaves as an implicit low-pass filter,\nattenuating high-frequency noise but also risking over-smoothing. Building on\nthis insight, we propose RoGER-SLAM, a robust 3DGS SLAM system tailored for\nnoise and low-light resilience. The framework integrates three innovations: a\nStructure-Preserving Robust Fusion (SP-RoFusion) mechanism that couples\nrendered appearance, depth, and edge cues; an adaptive tracking objective with\nresidual balancing regularization; and a Contrastive Language-Image Pretraining\n(CLIP)-based enhancement module, selectively activated under compounded\ndegradations to restore semantic and structural fidelity. Comprehensive\nexperiments on Replica, TUM, and real-world sequences show that RoGER-SLAM\nconsistently improves trajectory accuracy and reconstruction quality compared\nwith other 3DGS-SLAM systems, especially under adverse imaging conditions.", "AI": {"tldr": "RoGER-SLAM\u662f\u4e00\u79cd\u65b0\u7684SLAM\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f15\u5165\u521b\u65b0\u673a\u5236\u514b\u670d\u4e86\u566a\u58f0\u548c\u4f4e\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u6620\u5c04\u548c\u8ddf\u8e2a\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u566a\u58f0\u548c\u4f4e\u5149\u7167\u7684\u73af\u5883\u4e2d\uff0c\u73b0\u6709\u7684SLAM\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u53d7\u5230\u9650\u5236\uff0c\u9488\u5bf9\u8fd9\u79cd\u60c5\u51b5\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u4ee5\u589e\u5f3a\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002", "method": "RoGER-SLAM\u7cfb\u7edf\u878d\u5408\u4e86\u4e09\u9879\u521b\u65b0\uff1a\u7ed3\u6784\u4fdd\u6301\u7684\u9c81\u68d2\u878d\u5408\u673a\u5236\u3001\u9002\u5e94\u6027\u8ddf\u8e2a\u76ee\u6807\u548c\u57fa\u4e8eCLIP\u7684\u589e\u5f3a\u6a21\u5757\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRoGER-SLAM\u7684SLAM\u7cfb\u7edf\uff0c\u9488\u5bf9\u566a\u58f0\u548c\u4f4e\u5149\u7167\u73af\u5883\u4e2dSLAM\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u521b\u65b0\u673a\u5236\uff0c\u4ee5\u63d0\u9ad8\u6620\u5c04\u548c\u8ffd\u8e2a\u6027\u80fd\u3002", "conclusion": "RoGER-SLAM\u5728\u6311\u6218\u6027\u6210\u50cf\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u8f68\u8ff9\u51c6\u786e\u6027\u548c\u91cd\u5efa\u8d28\u91cf\uff0c\u4f18\u4e8e\u5176\u4ed63DGS-SLAM\u7cfb\u7edf\u3002"}}
{"id": "2510.22680", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22680", "abs": "https://arxiv.org/abs/2510.22680", "authors": ["Shireen Kudukkil Manchingal", "Armand Amaritei", "Mihir Gohad", "Maryam Sultana", "Julian F. P. Kooij", "Fabio Cuzzolin", "Andrew Bradley"], "title": "Uncertainty-Aware Autonomous Vehicles: Predicting the Road Ahead", "comment": null, "summary": "Autonomous Vehicle (AV) perception systems have advanced rapidly in recent\nyears, providing vehicles with the ability to accurately interpret their\nenvironment. Perception systems remain susceptible to errors caused by\noverly-confident predictions in the case of rare events or out-of-sample data.\nThis study equips an autonomous vehicle with the ability to 'know when it is\nuncertain', using an uncertainty-aware image classifier as part of the AV\nsoftware stack. Specifically, the study exploits the ability of Random-Set\nNeural Networks (RS-NNs) to explicitly quantify prediction uncertainty. Unlike\ntraditional CNNs or Bayesian methods, RS-NNs predict belief functions over sets\nof classes, allowing the system to identify and signal uncertainty clearly in\nnovel or ambiguous scenarios. The system is tested in a real-world autonomous\nracing vehicle software stack, with the RS-NN classifying the layout of the\nroad ahead and providing the associated uncertainty of the prediction.\nPerformance of the RS-NN under a range of road conditions is compared against\ntraditional CNN and Bayesian neural networks, with the RS-NN achieving\nsignificantly higher accuracy and superior uncertainty calibration. This\nintegration of RS-NNs into Robot Operating System (ROS)-based vehicle control\npipeline demonstrates that predictive uncertainty can dynamically modulate\nvehicle speed, maintaining high-speed performance under confident predictions\nwhile proactively improving safety through speed reductions in uncertain\nscenarios. These results demonstrate the potential of uncertainty-aware neural\nnetworks - in particular RS-NNs - as a practical solution for safer and more\nrobust autonomous driving.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u968f\u673a\u96c6\u5408\u795e\u7ecf\u7f51\u7edc\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u5bf9\u73af\u5883\u7684\u611f\u77e5\u80fd\u529b\uff0c\u4ece\u800c\u5728\u4e0d\u786e\u5b9a\u60c5\u51b5\u4e0b\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u4e0d\u786e\u5b9a\u60c5\u51b5\u4e0b\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\u968f\u673a\u96c6\u5408\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u56fe\u50cf\u5206\u7c7b\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u6d4b\u8bd5\u4e86\u5176\u5728\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u4e0e\u4f20\u7edf\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u901a\u8fc7\u5f15\u5165\u968f\u673a\u96c6\u5408\u795e\u7ecf\u7f51\u7edc\uff08RS-NN\uff09\uff0c\u5b9e\u73b0\u4e86\u5bf9\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684\u6e05\u6670\u91cf\u5316\u548c\u4fe1\u53f7\u4f20\u9012\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u6807\u5b9a\u3002", "conclusion": "\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u795e\u7ecf\u7f51\u7edc\uff0c\u7279\u522b\u662f\u968f\u673a\u96c6\u5408\u795e\u7ecf\u7f51\u7edc\uff0c\u80fd\u591f\u4f5c\u4e3a\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.22699", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.22699", "abs": "https://arxiv.org/abs/2510.22699", "authors": ["Matteo El-Hariry", "Andrej Orsula", "Matthieu Geist", "Miguel Olivares-Mendez"], "title": "RL-AVIST: Reinforcement Learning for Autonomous Visual Inspection of Space Targets", "comment": null, "summary": "The growing need for autonomous on-orbit services such as inspection,\nmaintenance, and situational awareness calls for intelligent spacecraft capable\nof complex maneuvers around large orbital targets. Traditional control systems\noften fall short in adaptability, especially under model uncertainties,\nmulti-spacecraft configurations, or dynamically evolving mission contexts. This\npaper introduces RL-AVIST, a Reinforcement Learning framework for Autonomous\nVisual Inspection of Space Targets. Leveraging the Space Robotics Bench (SRB),\nwe simulate high-fidelity 6-DOF spacecraft dynamics and train agents using\nDreamerV3, a state-of-the-art model-based RL algorithm, with PPO and TD3 as\nmodel-free baselines. Our investigation focuses on 3D proximity maneuvering\ntasks around targets such as the Lunar Gateway and other space assets. We\nevaluate task performance under two complementary regimes: generalized agents\ntrained on randomized velocity vectors, and specialized agents trained to\nfollow fixed trajectories emulating known inspection orbits. Furthermore, we\nassess the robustness and generalization of policies across multiple spacecraft\nmorphologies and mission domains. Results demonstrate that model-based RL\noffers promising capabilities in trajectory fidelity, and sample efficiency,\npaving the way for scalable, retrainable control solutions for future space\noperations", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86RL-AVIST\uff0c\u4e00\u4e2a\u7528\u4e8e\u81ea\u4e3b\u89c6\u89c9\u68c0\u67e5\u7a7a\u95f4\u76ee\u6807\u7684\u589e\u5f3a\u5b66\u4e60\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5728\u590d\u6742\u8f68\u9053\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u4e0e\u521b\u65b0\u6027\u3002", "motivation": "\u968f\u7740\u81ea\u4e3b\u8f68\u9053\u670d\u52a1\u9700\u6c42\u7684\u589e\u52a0\uff0c\u667a\u80fd\u822a\u5929\u5668\u9700\u8981\u5177\u5907\u590d\u6742\u673a\u52a8\u80fd\u529b\uff0c\u4ee5\u5e94\u5bf9\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u3001\u591a\u822a\u5929\u5668\u914d\u7f6e\u53ca\u52a8\u6001\u4efb\u52a1\u73af\u5883\u7b49\u95ee\u9898\u3002", "method": "\u8be5\u7814\u7a76\u7ed3\u5408\u4e86\u7a7a\u95f4\u673a\u5668\u4eba\u6d4b\u8bd5\u5e73\u53f0\uff08SRB\uff09\u4ee5\u53caDreamerV3\u6a21\u578b\u7684\u57fa\u4e8e\u6a21\u578b\u7684\u589e\u5f3a\u5b66\u4e60\u7b97\u6cd5\uff0c\u4f7f\u7528PPO\u548cTD3\u4f5c\u4e3a\u65e0\u6a21\u578b\u7684\u57fa\u7ebf\u8fdb\u884c\u6a21\u62df\u4e0e\u8bad\u7ec3\u3002", "result": "\u7814\u7a76\u9488\u5bf9\u6708\u7403\u95f8\u95e8\u7b49\u76ee\u6807\u6267\u884c3D\u8fd1\u8ddd\u79bb\u673a\u52a8\u4efb\u52a1\uff0c\u8bc4\u4f30\u4e86\u8bad\u7ec3\u6709\u7d20\u548c\u4e13\u95e8\u5316\u4ee3\u7406\u7684\u4efb\u52a1\u8868\u73b0\uff0c\u5206\u6790\u4e86\u5728\u591a\u79cd\u7a7a\u95f4\u5f62\u6001\u548c\u4efb\u52a1\u9886\u57df\u4e2d\u7b56\u7565\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u6a21\u578b\u9a71\u52a8\u7684\u589e\u5f3a\u5b66\u4e60\u5728\u8f68\u8ff9\u4fdd\u771f\u5ea6\u548c\u6837\u672c\u6548\u7387\u65b9\u9762\u5c55\u73b0\u51fa\u826f\u597d\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u7684\u7a7a\u95f4\u64cd\u4f5c\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u91cd\u65b0\u8bad\u7ec3\u7684\u63a7\u5236\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.22738", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.22738", "abs": "https://arxiv.org/abs/2510.22738", "authors": ["Wentao Guo", "Wenzeng Zhang"], "title": "SCAL for Pinch-Lifting: Complementary Rotational and Linear Prototypes for Environment-Adaptive Grasping", "comment": "Preliminary version presented at the IROS 2025 CIM Workshop, where it\n  was selected as a Best Demo Award (Finalist) and subsequently received the\n  Best Demo Award after oral presentation", "summary": "This paper presents environment-adaptive pinch-lifting built on a\nslot-constrained adaptive linkage (SCAL) and instantiated in two complementary\nfingers: SCAL-R, a rotational-drive design with an active fingertip that folds\ninward after contact to form an envelope, and SCAL-L, a linear-drive design\nthat passively opens on contact to span wide or weak-feature objects. Both\nfingers convert surface following into an upward lifting branch while\nmaintaining fingertip orientation, enabling thin or low-profile targets to be\nraised from supports with minimal sensing and control. Two-finger grippers are\nfabricated via PLA-based 3D printing. Experiments evaluate (i)\ncontact-preserving sliding and pinch-lifting on tabletops, (ii) ramp\nnegotiation followed by lift, and (iii) handling of bulky objects via active\nenveloping (SCAL-R) or contact-triggered passive opening (SCAL-L). Across\ndozens of trials on small parts, boxes, jars, and tape rolls, both designs\nachieve consistent grasps with limited tuning. A quasi-static analysis provides\nclosed-form fingertip-force models for linear parallel pinching and two-point\nenveloping, offering geometry-aware guidance for design and operation. Overall,\nthe results indicate complementary operating regimes and a practical path to\nrobust, environment-adaptive grasping with simple actuation.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSCAL\u7684\u73af\u5883\u81ea\u9002\u5e94\u5939\u6301\u7cfb\u7edf\uff0c\u5229\u7528\u65cb\u8f6c\u548c\u7ebf\u6027\u9a71\u52a8\u8bbe\u8ba1\u5728\u4e0d\u540c\u7269\u4f53\u4e0a\u5b9e\u73b0\u5939\u6301\u63d0\u5347\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5177\u5907\u826f\u597d\u7684\u9002\u5e94\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73af\u5883\u9002\u5e94\u6027\u548c\u4f4e\u8f6e\u5ed3\u76ee\u6807\u7684\u6709\u6548\u6293\u53d6\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u81ea\u9002\u5e94\u5939\u6301\u8bbe\u8ba1\u3002", "method": "\u91c7\u7528\u57fa\u4e8ePLA\u76843D\u6253\u5370\u5236\u4f5c\u5939\u5177\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u5939\u6301\u548c\u63d0\u5347\u7684\u6027\u80fd\u3002", "result": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63d2\u69fd\u7ea6\u675f\u81ea\u9002\u5e94\u8fde\u6746\uff08SCAL\uff09\u7684\u73af\u5883\u81ea\u9002\u5e94\u5939\u6301\u7cfb\u7edf\uff0c\u5e76\u5728\u4e24\u4e2a\u4e92\u8865\u6307\u5c16\u4e2d\u5b9e\u73b0\uff1aSCAL-R\u662f\u4e00\u79cd\u5177\u6709\u4e3b\u52a8\u6307\u5c16\u7684\u65cb\u8f6c\u9a71\u52a8\u8bbe\u8ba1\uff0c\u5728\u63a5\u89e6\u540e\u5411\u5185\u6298\u53e0\u5f62\u6210\u4e00\u4e2a\u5305\u7edc\uff1bSCAL-L\u662f\u4e00\u79cd\u7ebf\u6027\u9a71\u52a8\u8bbe\u8ba1\uff0c\u5728\u63a5\u89e6\u65f6\u88ab\u52a8\u6253\u5f00\uff0c\u80fd\u591f\u8de8\u8d8a\u5bbd\u5927\u6216\u7279\u5f81\u5f31\u7684\u7269\u4f53\u3002\u4e24\u4e2a\u6307\u5c16\u5c06\u8868\u9762\u8ddf\u968f\u8f6c\u6362\u4e3a\u5411\u4e0a\u7684\u63d0\u8d77\u652f\u8def\uff0c\u540c\u65f6\u4fdd\u6301\u6307\u5c16\u65b9\u5411\uff0c\u4f7f\u5f97\u8584\u6216\u4f4e\u8f6e\u5ed3\u76ee\u6807\u80fd\u591f\u4ee5\u6700\u5c0f\u7684\u4f20\u611f\u548c\u63a7\u5236\u4ece\u652f\u6491\u7269\u4e2d\u62ac\u8d77\u3002\u4e24\u6307\u5939\u5177\u901a\u8fc7\u57fa\u4e8ePLA\u76843D\u6253\u5370\u5236\u9020\u3002\u5b9e\u9a8c\u8bc4\u4f30\u4e86\uff08i\uff09\u63a5\u89e6\u4fdd\u6301\u6ed1\u52a8\u548c\u5939\u6301\u63d0\u5347\u5728\u684c\u9762\u4e0a\u7684\u8868\u73b0\uff0c\uff08ii\uff09\u5761\u9053\u534f\u5546\u540e\u63d0\u5347\uff0c\u4ee5\u53ca\uff08iii\uff09\u901a\u8fc7\u4e3b\u52a8\u5305\u7edc\uff08SCAL-R\uff09\u6216\u63a5\u89e6\u89e6\u53d1\u88ab\u52a8\u5f00\u542f\uff08SCAL-L\uff09\u5904\u7406\u7b28\u91cd\u7269\u4f53\u3002\u5728\u9488\u5bf9\u5c0f\u90e8\u4ef6\u3001\u7bb1\u5b50\u3001\u74f6\u5b50\u548c\u80f6\u5e26\u5377\u7684\u6570\u5341\u4e2a\u8bd5\u9a8c\u4e2d\uff0c\u4e24\u79cd\u8bbe\u8ba1\u90fd\u5b9e\u73b0\u4e86\u4e00\u81f4\u7684\u6293\u53d6\uff0c\u4e14\u8c03\u6821\u6709\u9650\u3002\u4e00\u4e2a\u51c6\u9759\u6001\u5206\u6790\u63d0\u4f9b\u4e86\u7ebf\u6027\u5e73\u884c\u5939\u6301\u548c\u53cc\u70b9\u5305\u7edc\u7684\u95ed\u5f0f\u6307\u5c16\u529b\u6a21\u578b\uff0c\u4e3a\u8bbe\u8ba1\u548c\u64cd\u4f5c\u63d0\u4f9b\u4e86\u51e0\u4f55\u610f\u8bc6\u6307\u5bfc\u3002\u603b\u4f53\u7ed3\u679c\u8868\u660e\uff0c\u4e92\u8865\u7684\u64cd\u4f5c\u673a\u5236\u548c\u901a\u5411\u7a33\u5065\u3001\u73af\u5883\u81ea\u9002\u5e94\u6293\u53d6\u7684\u7b80\u5355\u9a71\u52a8\u7684\u5b9e\u9645\u8def\u5f84\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u63d0\u51fa\u7684SCAL\u8bbe\u8ba1\u5728\u5904\u7406\u591a\u79cd\u7269\u4f53\u65f6\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u6293\u53d6\u6548\u679c\uff0c\u5c55\u793a\u4e86\u826f\u597d\u7684\u73af\u5883\u9002\u5e94\u6027\u3002"}}
{"id": "2510.22740", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.22740", "abs": "https://arxiv.org/abs/2510.22740", "authors": ["Sai Krishna Ghanta", "Ramviyas Parasuraman"], "title": "Policies over Poses: Reinforcement Learning based Distributed Pose-Graph Optimization for Multi-Robot SLAM", "comment": "IEEE International Symposium on Multi-Robot & Multi-Agent Systems\n  (MRS) 2025", "summary": "We consider the distributed pose-graph optimization (PGO) problem, which is\nfundamental in accurate trajectory estimation in multi-robot simultaneous\nlocalization and mapping (SLAM). Conventional iterative approaches linearize a\nhighly non-convex optimization objective, requiring repeated solving of normal\nequations, which often converge to local minima and thus produce suboptimal\nestimates. We propose a scalable, outlier-robust distributed planar PGO\nframework using Multi-Agent Reinforcement Learning (MARL). We cast distributed\nPGO as a partially observable Markov game defined on local pose-graphs, where\neach action refines a single edge's pose estimate. A graph partitioner\ndecomposes the global pose graph, and each robot runs a recurrent\nedge-conditioned Graph Neural Network (GNN) encoder with adaptive edge-gating\nto denoise noisy edges. Robots sequentially refine poses through a hybrid\npolicy that utilizes prior action memory and graph embeddings. After local\ngraph correction, a consensus scheme reconciles inter-robot disagreements to\nproduce a globally consistent estimate. Our extensive evaluations on a\ncomprehensive suite of synthetic and real-world datasets demonstrate that our\nlearned MARL-based actors reduce the global objective by an average of 37.5%\nmore than the state-of-the-art distributed PGO framework, while enhancing\ninference efficiency by at least 6X. We also demonstrate that actor replication\nallows a single learned policy to scale effortlessly to substantially larger\nrobot teams without any retraining. Code is publicly available at\nhttps://github.com/herolab-uga/policies-over-poses.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u5206\u5e03\u5f0f\u59ff\u6001\u56fe\u4f18\u5316\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8f68\u8ff9\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u63a8\u7406\u6548\u7387\uff0c\u9488\u5bf9\u5927\u89c4\u6a21\u673a\u5668\u4eba\u961f\u4f0d\u5177\u6709\u826f\u597d\u6269\u5c55\u6027\u3002", "motivation": "\u63d0\u9ad8\u591a\u673a\u5668\u4eba\u540c\u65f6\u5b9a\u4f4d\u4e0e\u5730\u56fe\u6784\u5efa\uff08SLAM\uff09\u4e2d\u7684\u8f68\u8ff9\u4f30\u8ba1\u51c6\u786e\u6027\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u90e8\u6700\u4f18\u95ee\u9898", "method": "\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u6846\u67b6\u8fdb\u884c\u5206\u5e03\u5f0f\u59ff\u6001\u56fe\u4f18\u5316\uff08PGO\uff09", "result": "\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5206\u5e03\u5f0fPGO\u6846\u67b6\u76f8\u6bd4\uff0c\u6240\u63d0MARL\u65b9\u6cd5\u5c06\u5168\u5c40\u76ee\u6807\u51cf\u5c11\u4e86\u5927\u7ea637.5%\uff0c\u63a8\u7406\u6548\u7387\u63d0\u5347\u81f3\u5c116\u500d", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u4e14\u652f\u6301\u5355\u4e00\u5b66\u4e60\u7b56\u7565\u5728\u5927\u578b\u673a\u5668\u4eba\u56e2\u961f\u4e2d\u65e0\u7f1d\u6269\u5c55\u3002"}}
{"id": "2510.22754", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.22754", "abs": "https://arxiv.org/abs/2510.22754", "authors": ["Chunyu Li", "Shoubin Chen", "Dong Li", "Weixing Xue", "Qingquan Li"], "title": "TWC-SLAM: Multi-Agent Cooperative SLAM with Text Semantics and WiFi Features Integration for Similar Indoor Environments", "comment": "Accepted by the IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS) 2025", "summary": "Multi-agent cooperative SLAM often encounters challenges in similar indoor\nenvironments characterized by repetitive structures, such as corridors and\nrooms. These challenges can lead to significant inaccuracies in shared location\nidentification when employing point cloud-based techniques. To mitigate these\nissues, we introduce TWC-SLAM, a multi-agent cooperative SLAM framework that\nintegrates text semantics and WiFi signal features to enhance location\nidentification and loop closure detection. TWC-SLAM comprises a single-agent\nfront-end odometry module based on FAST-LIO2, a location identification and\nloop closure detection module that leverages text semantics and WiFi features,\nand a global mapping module. The agents are equipped with sensors capable of\ncapturing textual information and detecting WiFi signals. By correlating these\ndata sources, TWC-SLAM establishes a common location, facilitating point cloud\nalignment across different agents' maps. Furthermore, the system employs loop\nclosure detection and optimization modules to achieve global optimization and\ncohesive mapping. We evaluated our approach using an indoor dataset featuring\nsimilar corridors, rooms, and text signs. The results demonstrate that TWC-SLAM\nsignificantly improves the performance of cooperative SLAM systems in complex\nenvironments with repetitive architectural features.", "AI": {"tldr": "TWC-SLAM\u662f\u4e00\u79cd\u591a\u4ee3\u7406\u534f\u4f5cSLAM\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u8bed\u4e49\u548cWiFi\u4fe1\u53f7\uff0c\u6539\u5584\u4e86\u5728\u91cd\u590d\u5ba4\u5185\u73af\u5883\u4e2d\u4f4d\u7f6e\u8bc6\u522b\u548c\u95ed\u73af\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u5728\u91cd\u590d\u7ed3\u6784\u7684\u5ba4\u5185\u73af\u5883\u4e2d\uff0c\u5982\u8d70\u5eca\u548c\u623f\u95f4\uff0c\u57fa\u4e8e\u70b9\u4e91\u7684\u5b9a\u4f4d\u8bc6\u522b\u9762\u4e34\u7684\u6311\u6218\u3002", "method": "TWC-SLAM\u6846\u67b6\uff0c\u7ed3\u5408\u6587\u672c\u8bed\u4e49\u548cWiFi\u4fe1\u53f7\u7279\u5f81\u8fdb\u884c\u591a\u4ee3\u7406\u534f\u4f5c\u5f0fSLAM\u3002", "result": "TWC-SLAM\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u73af\u5883\u4e2d\u534f\u4f5cSLAM\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u91cd\u590d\u5efa\u7b51\u7279\u5f81\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "\u901a\u8fc7\u5bf9\u590d\u6742\u5ba4\u5185\u73af\u5883\u7684\u8bc4\u4f30\uff0cTWC-SLAM\u5c55\u793a\u4e86\u5176\u5728\u4f18\u5316\u5168\u7403\u6620\u5c04\u548c\u534f\u4f5cSLAM\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.22784", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22784", "abs": "https://arxiv.org/abs/2510.22784", "authors": ["Guangyao Shi", "Yuwei Wu", "Vijay Kumar", "Gaurav S. Sukhatme"], "title": "PIP-LLM: Integrating PDDL-Integer Programming with LLMs for Coordinating Multi-Robot Teams Using Natural Language", "comment": null, "summary": "Enabling robot teams to execute natural language commands requires\ntranslating high-level instructions into feasible, efficient multi-robot plans.\nWhile Large Language Models (LLMs) combined with Planning Domain Description\nLanguage (PDDL) offer promise for single-robot scenarios, existing approaches\nstruggle with multi-robot coordination due to brittle task decomposition, poor\nscalability, and low coordination efficiency.\n  We introduce PIP-LLM, a language-based coordination framework that consists\nof PDDL-based team-level planning and Integer Programming (IP) based\nrobot-level planning. PIP-LLMs first decomposes the command by translating the\ncommand into a team-level PDDL problem and solves it to obtain a team-level\nplan, abstracting away robot assignment. Each team-level action represents a\nsubtask to be finished by the team. Next, this plan is translated into a\ndependency graph representing the subtasks' dependency structure. Such a\ndependency graph is then used to guide the robot-level planning, in which each\nsubtask node will be formulated as an IP-based task allocation problem,\nexplicitly optimizing travel costs and workload while respecting robot\ncapabilities and user-defined constraints. This separation of planning from\nassignment allows PIP-LLM to avoid the pitfalls of syntax-based decomposition\nand scale to larger teams. Experiments across diverse tasks show that PIP-LLM\nimproves plan success rate, reduces maximum and average travel costs, and\nachieves better load balancing compared to state-of-the-art baselines.", "AI": {"tldr": "PIP-LLM \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u9ad8\u5c42\u547d\u4ee4\u5206\u89e3\u4e3a\u56e2\u961f\u5c42\u548c\u673a\u5668\u4eba\u5c42\u7684\u89c4\u5212\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u591a\u673a\u5668\u4eba\u534f\u8c03\u4e2d\u7684\u4efb\u52a1\u5206\u89e3\u548c\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u673a\u5668\u4eba\u534f\u8c03\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u4efb\u52a1\u5206\u89e3\u8106\u5f31\u3001\u53ef\u6269\u5c55\u6027\u5dee\u548c\u534f\u8c03\u6548\u7387\u4f4e\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u3002", "method": "PIP-LLM \u91c7\u7528\u4e86\u4e24\u7ea7\u89c4\u5212\uff1a\u56e2\u961f\u5c42\u4f7f\u7528 PDDL \u8fdb\u884c\u8ba1\u5212\uff0c\u800c\u673a\u5668\u4eba\u5c42\u57fa\u4e8e\u6574\u6570\u89c4\u5212\u8fdb\u884c\u4efb\u52a1\u5206\u914d\u3002", "result": "PIP-LLM \u662f\u4e00\u79cd\u8bed\u8a00\u57fa\u7840\u7684\u534f\u8c03\u6846\u67b6\uff0c\u80fd\u591f\u63d0\u9ad8\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u6267\u884c\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7684\u80fd\u529b\u3002", "conclusion": "PIP-LLM \u5728\u591a\u4e2a\u4efb\u52a1\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u8ba1\u5212\u6210\u529f\u7387\uff0c\u5e76\u4e14\u5728\u65c5\u884c\u6210\u672c\u548c\u8d1f\u8f7d\u5e73\u8861\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u51c6\u3002"}}
{"id": "2510.22789", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.22789", "abs": "https://arxiv.org/abs/2510.22789", "authors": ["Abhijeet M. Kulkarni", "Ioannis Poulakakis", "Guoquan Huang"], "title": "Learning Neural Observer-Predictor Models for Limb-level Sampling-based Locomotion Planning", "comment": null, "summary": "Accurate full-body motion prediction is essential for the safe, autonomous\nnavigation of legged robots, enabling critical capabilities like limb-level\ncollision checking in cluttered environments. Simplified kinematic models often\nfail to capture the complex, closed-loop dynamics of the robot and its\nlow-level controller, limiting their predictions to simple planar motion. To\naddress this, we present a learning-based observer-predictor framework that\naccurately predicts this motion. Our method features a neural observer with\nprovable UUB guarantees that provides a reliable latent state estimate from a\nhistory of proprioceptive measurements. This stable estimate initializes a\ncomputationally efficient predictor, designed for the rapid, parallel\nevaluation of thousands of potential trajectories required by modern\nsampling-based planners. We validated the system by integrating our neural\npredictor into an MPPI-based planner on a Vision 60 quadruped. Hardware\nexperiments successfully demonstrated effective, limb-aware motion planning in\na challenging, narrow passage and over small objects, highlighting our system's\nability to provide a robust foundation for high-performance, collision-aware\nplanning on dynamic robotic platforms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b66\u4e60\u57fa\u7840\u7684\u89c2\u5bdf\u8005-\u9884\u6d4b\u5668\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8\u56db\u8db3\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u9884\u6d4b\u7cbe\u5ea6\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u5b89\u5168\u7684\u81ea\u4e3b\u5bfc\u822a\u3002", "motivation": "\u51c6\u786e\u7684\u5168\u8eab\u8fd0\u52a8\u9884\u6d4b\u5bf9\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u7684\u7b80\u5316\u8fd0\u52a8\u6a21\u578b\u65e0\u6cd5\u5145\u5206\u6355\u6349\u673a\u5668\u4eba\u7684\u590d\u6742\u52a8\u6001\u7279\u6027\u3002", "method": "\u901a\u8fc7\u4f7f\u7528\u5177\u6709\u53ef\u8bc1\u660e\u7684UUB\u4fdd\u8bc1\u7684\u795e\u7ecf\u89c2\u5bdf\u8005\u548c\u9ad8\u6548\u80fd\u7684\u9884\u6d4b\u5668\uff0c\u5236\u4f5c\u51fa\u4e00\u4e2a\u80fd\u591f\u5feb\u901f\u8bc4\u4f30\u6570\u5343\u79cd\u6f5c\u5728\u8f68\u8ff9\u7684\u7cfb\u7edf\u3002", "result": "\u5728Vision 60\u56db\u8db3\u673a\u5668\u4eba\u4e0a\uff0c\u6574\u5408\u6211\u4eec\u7684\u795e\u7ecf\u9884\u6d4b\u5668\u5230\u57fa\u4e8eMPPI\u7684\u89c4\u5212\u5668\u4e2d\uff0c\u786c\u4ef6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5728\u72ed\u7a84\u901a\u9053\u548c\u5c0f\u969c\u788d\u7269\u4e0a\u7684\u6709\u6548\u8fd0\u52a8\u89c4\u5212\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u5b66\u4e60\u57fa\u7840\u89c2\u5bdf\u8005-\u9884\u6d4b\u5668\u6846\u67b6\u6709\u6548\u5730\u63d0\u9ad8\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u9884\u6d4b\u7cbe\u5ea6\uff0c\u4e3a\u5b89\u5168\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u57fa\u7840\u3002"}}
{"id": "2510.22821", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.22821", "abs": "https://arxiv.org/abs/2510.22821", "authors": ["Ricardo Vega", "Connor Mattson", "Kevin Zhu", "Daniel S. Brown", "Cameron Nowzari"], "title": "Analytical Swarm Chemistry: Characterization and Analysis of Emergent Swarm Behaviors", "comment": "9 pages, 8 figures, 1 table", "summary": "Swarm robotics has potential for a wide variety of applications, but\nreal-world deployments remain rare due to the difficulty of predicting emergent\nbehaviors arising from simple local interactions. Traditional engineering\napproaches design controllers to achieve desired macroscopic outcomes under\nidealized conditions, while agent-based and artificial life studies explore\nemergent phenomena in a bottom-up, exploratory manner. In this work, we\nintroduce Analytical Swarm Chemistry, a framework that integrates concepts from\nengineering, agent-based and artificial life research, and chemistry. This\nframework combines macrostate definitions with phase diagram analysis to\nsystematically explore how swarm parameters influence emergent behavior.\nInspired by concepts from chemistry, the framework treats parameters like\nthermodynamic variables, enabling visualization of regions in parameter space\nthat give rise to specific behaviors. Applying this framework to agents with\nminimally viable capabilities, we identify sufficient conditions for behaviors\nsuch as milling and diffusion and uncover regions of the parameter space that\nreliably produce these behaviors. Preliminary validation on real robots\ndemonstrates that these regions correspond to observable behaviors in practice.\nBy providing a principled, interpretable approach, this framework lays the\ngroundwork for predictable and reliable emergent behavior in real-world swarm\nsystems.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5206\u6790\u7fa4\u4f53\u5316\u5b66\u6846\u67b6\uff0c\u65e8\u5728\u63a2\u7d22\u7fa4\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u6d8c\u73b0\u884c\u4e3a\uff0c\u901a\u8fc7\u76f8\u56fe\u5206\u6790\u548c\u53c2\u6570\u7a7a\u95f4\u53ef\u89c6\u5316\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u53ef\u9760\u884c\u4e3a\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "motivation": "\u7fa4\u4f53\u673a\u5668\u4eba\u5728\u4f17\u591a\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u7531\u4e8e\u7b80\u5355\u5c40\u90e8\u4ea4\u4e92\u6240\u4ea7\u751f\u7684\u6d8c\u73b0\u884c\u4e3a\u7684\u96be\u4ee5\u9884\u6d4b\uff0c\u4f7f\u5f97\u73b0\u5b9e\u90e8\u7f72\u4ecd\u7136\u7a00\u5c11\u3002", "method": "\u901a\u8fc7\u5c06\u5b8f\u89c2\u6001\u5b9a\u4e49\u4e0e\u76f8\u56fe\u5206\u6790\u7ed3\u5408\uff0c\u7cfb\u7edf\u5730\u63a2\u7d22\u7fa4\u4f53\u53c2\u6570\u5982\u4f55\u5f71\u54cd\u6d8c\u73b0\u884c\u4e3a\uff0c\u540c\u65f6\u501f\u9274\u5316\u5b66\u4e2d\u7684\u6982\u5ff5\uff0c\u5904\u7406\u53c2\u6570\u5982\u70ed\u529b\u5b66\u53d8\u91cf\u8fdb\u884c\u53ef\u89c6\u5316\u3002", "result": "\u901a\u8fc7\u4f7f\u7528\u8be5\u6846\u67b6\uff0c\u6211\u4eec\u8bc6\u522b\u51fa\u4ea7\u751f\u7279\u5b9a\u884c\u4e3a\uff08\u5982\u65cb\u8f6c\u548c\u6269\u6563\uff09\u7684\u5145\u5206\u6761\u4ef6\uff0c\u5e76\u53d1\u73b0\u4e86\u53c2\u6570\u7a7a\u95f4\u4e2d\u80fd\u53ef\u9760\u5730\u751f\u6210\u8fd9\u4e9b\u884c\u4e3a\u7684\u533a\u57df\uff0c\u540c\u65f6\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u7684\u521d\u6b65\u9a8c\u8bc1\u8868\u660e\u8fd9\u4e9b\u533a\u57df\u4e0e\u5b9e\u9645\u53ef\u89c2\u5bdf\u884c\u4e3a\u76f8\u5bf9\u5e94\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u5206\u6790\u7fa4\u4f53\u5316\u5b66\u6846\u67b6\u4e3a\u73b0\u5b9e\u4e16\u754c\u7684\u7fa4\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u9884\u6d4b\u548c\u53ef\u9760\u7684\u65b0\u65b9\u6cd5\uff0c\u4f7f\u5f97\u6211\u4eec\u80fd\u591f\u901a\u8fc7\u7cfb\u7edf\u5730\u63a2\u7d22\u53c2\u6570\u7a7a\u95f4\u6765\u7406\u89e3\u548c\u63a7\u5236\u6d8c\u73b0\u884c\u4e3a\u3002"}}
{"id": "2510.22825", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.22825", "abs": "https://arxiv.org/abs/2510.22825", "authors": ["Nan Zhang"], "title": "Kinematically Controllable Cable Robots with Reconfigurable End-effectors", "comment": "7 pages, 12 figures, Technical Report", "summary": "To enlarge the translational workspace of cable-driven robots, one common\napproach is to increase the number of cables. However, this introduces two\nchallenges: (1) cable interference significantly reduces the rotational\nworkspace, and (2) the solution of tensions in cables becomes non-unique,\nresulting in difficulties for kinematic control of the robot. In this work, we\ndesign structurally simple reconfigurable end-effectors for cable robots. By\nincorporating a spring, a helical-grooved shaft, and a matching nut, relative\nlinear motions between end-effector components are converted into relative\nrotations, thereby expanding the rotational workspace of the mechanism.\nMeanwhile, a bearing is introduced to provide an additional rotational degree\nof freedom, making the mechanism non-redundant. As a result, the robot's motion\ncan be controlled purely through kinematics without additional tension sensing\nand control.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u91cd\u6784\u672b\u7aef\u6267\u884c\u5668\uff0c\u6210\u529f\u6269\u5927\u4e86\u7f06\u9a71\u52a8\u673a\u5668\u4eba\u7684\u65cb\u8f6c\u5de5\u4f5c\u7a7a\u95f4\uff0c\u5e76\u6d88\u9664\u4e86\u5f20\u529b\u63a7\u5236\u7684\u590d\u6742\u6027\u3002", "motivation": "\u4f20\u7edf\u589e\u52a0\u7f06\u7ebf\u6570\u91cf\u7684\u65b9\u6cd5\u867d\u7136\u80fd\u6269\u5927\u5e73\u79fb\u5de5\u4f5c\u7a7a\u95f4\uff0c\u4f46\u4f1a\u5bfc\u81f4\u7f06\u7ebf\u5e72\u6270\u548c\u975e\u552f\u4e00\u5f20\u529b\u89e3\u7684\u95ee\u9898\uff0c\u4ece\u800c\u589e\u52a0\u8fd0\u52a8\u5b66\u63a7\u5236\u7684\u96be\u5ea6\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7ed3\u6784\u7b80\u5355\u7684\u53ef\u91cd\u6784\u672b\u7aef\u6267\u884c\u5668\uff0c\u7ed3\u5408\u5f39\u7c27\u3001\u87ba\u65cb\u69fd\u8f74\u548c\u5339\u914d\u7684\u87ba\u6bcd\uff0c\u5c06\u672b\u7aef\u6267\u884c\u5668\u7ec4\u4ef6\u4e4b\u95f4\u7684\u76f8\u5bf9\u7ebf\u6027\u8fd0\u52a8\u8f6c\u5316\u4e3a\u76f8\u5bf9\u65cb\u8f6c\u3002", "result": "\u901a\u8fc7\u5f15\u5165\u8f74\u627f\u63d0\u4f9b\u989d\u5916\u7684\u65cb\u8f6c\u81ea\u7531\u5ea6\uff0c\u6d88\u9664\u4e86\u5197\u4f59\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u4ec5\u901a\u8fc7\u8fd0\u52a8\u5b66\u5b9e\u73b0\u63a7\u5236\u3002", "conclusion": "\u901a\u8fc7\u8bbe\u8ba1\u7b80\u5355\u7684\u53ef\u91cd\u6784\u672b\u7aef\u6267\u884c\u5668\uff0c\u6269\u5c55\u4e86\u7f06\u9a71\u52a8\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u5de5\u4f5c\u7a7a\u95f4\uff0c\u4f7f\u5176\u80fd\u591f\u4ec5\u901a\u8fc7\u8fd0\u52a8\u5b66\u63a7\u5236\uff0c\u800c\u65e0\u9700\u989d\u5916\u7684\u5f20\u529b\u4f20\u611f\u548c\u63a7\u5236\u3002"}}
{"id": "2510.22892", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.22892", "abs": "https://arxiv.org/abs/2510.22892", "authors": ["Jingzehua Xu", "Yangyang Li", "Yangfei Chen", "Guanwen Xie", "Shuai Zhang"], "title": "Never Too Rigid to Reach: Adaptive Virtual Model Control with LLM- and Lyapunov-Based Reinforcement Learning", "comment": null, "summary": "Robotic arms are increasingly deployed in uncertain environments, yet\nconventional control pipelines often become rigid and brittle when exposed to\nperturbations or incomplete information. Virtual Model Control (VMC) enables\ncompliant behaviors by embedding virtual forces and mapping them into joint\ntorques, but its reliance on fixed parameters and limited coordination among\nvirtual components constrains adaptability and may undermine stability as task\nobjectives evolve. To address these limitations, we propose Adaptive VMC with\nLarge Language Model (LLM)- and Lyapunov-Based Reinforcement Learning (RL),\nwhich preserves the physical interpretability of VMC while supporting\nstability-guaranteed online adaptation. The LLM provides structured priors and\nhigh-level reasoning that enhance coordination among virtual components,\nimprove sample efficiency, and facilitate flexible adjustment to varying task\nrequirements. Complementarily, Lyapunov-based RL enforces theoretical stability\nconstraints, ensuring safe and reliable adaptation under uncertainty. Extensive\nsimulations on a 7-DoF Panda arm demonstrate that our approach effectively\nbalances competing objectives in dynamic tasks, achieving superior performance\nwhile highlighting the synergistic benefits of LLM guidance and\nLyapunov-constrained adaptation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u674e\u96c5\u666e\u8bfa\u592b\u57fa\u7840\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u865a\u62df\u6a21\u578b\u63a7\u5236\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u9002\u5e94\u80fd\u529b\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u63a7\u5236\u7ba1\u9053\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7a33\u5b9a\u6027\u5dee\uff0c\u5bf9\u6270\u52a8\u548c\u4e0d\u5b8c\u6574\u4fe1\u606f\u7684\u9002\u5e94\u6027\u4e0d\u8db3", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u865a\u62df\u6a21\u578b\u63a7\u5236\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u674e\u96c5\u666e\u8bfa\u592b\u57fa\u7840\u5f3a\u5316\u5b66\u4e60\u7684\u7ed3\u5408", "result": "\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u793a\u8303\uff0c\u8be5\u65b9\u6cd5\u5728\u52a8\u6001\u4efb\u52a1\u4e2d\u6709\u6548\u5e73\u8861\u4e86\u7ade\u4e89\u76ee\u6807\uff0c\u5b9e\u73b0\u4e86\u8f83\u4f18\u6027\u80fd", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86LLM\u6307\u5bfc\u4e0e\u674e\u96c5\u666e\u8bfa\u592b\u7ea6\u675f\u9002\u5e94\u7684\u534f\u540c\u6548\u76ca\uff0c\u786e\u4fdd\u4e86\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e0b\u7684\u5b89\u5168\u53ef\u9760\u9002\u5e94\u3002"}}
{"id": "2510.22917", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22917", "abs": "https://arxiv.org/abs/2510.22917", "authors": ["Zecheng Yin", "Hao Zhao", "Zhen Li"], "title": "HyPerNav: Hybrid Perception for Object-Oriented Navigation in Unknown Environment", "comment": "under review", "summary": "Objective-oriented navigation(ObjNav) enables robot to navigate to target\nobject directly and autonomously in an unknown environment. Effective\nperception in navigation in unknown environment is critical for autonomous\nrobots. While egocentric observations from RGB-D sensors provide abundant local\ninformation, real-time top-down maps offer valuable global context for ObjNav.\nNevertheless, the majority of existing studies focus on a single source, seldom\nintegrating these two complementary perceptual modalities, despite the fact\nthat humans naturally attend to both. With the rapid advancement of\nVision-Language Models(VLMs), we propose Hybrid Perception Navigation\n(HyPerNav), leveraging VLMs' strong reasoning and vision-language understanding\ncapabilities to jointly perceive both local and global information to enhance\nthe effectiveness and intelligence of navigation in unknown environments. In\nboth massive simulation evaluation and real-world validation, our methods\nachieved state-of-the-art performance against popular baselines. Benefiting\nfrom hybrid perception approach, our method captures richer cues and finds the\nobjects more effectively, by simultaneously leveraging information\nunderstanding from egocentric observations and the top-down map. Our ablation\nstudy further proved that either of the hybrid perception contributes to the\nnavigation performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHyPerNav\uff0c\u4ee5Vision-Language Models\u6574\u5408\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6027\u80fd\u3002", "motivation": "\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u5bfc\u822a\u7684\u6709\u6548\u611f\u77e5\uff0c\u6574\u5408\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\u4ee5\u589e\u5f3a\u5bfc\u822a\u667a\u80fd\u3002", "method": "Hybrid Perception Navigation (HyPerNav)", "result": "\u5728\u5927\u91cf\u7684\u6a21\u62df\u8bc4\u4f30\u548c\u5b9e\u9645\u9a8c\u8bc1\u4e2d\uff0cHyPerNav\u65b9\u6cd5\u5728\u5bfc\u822a\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u6d41\u884c\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u8868\u73b0\u3002", "conclusion": "\u6df7\u5408\u611f\u77e5\u65b9\u6cd5\u6709\u6548\u6355\u6349\u66f4\u4e30\u5bcc\u7684\u7ebf\u7d22\uff0c\u901a\u8fc7\u540c\u65f6\u5229\u7528\u81ea\u6211\u89c2\u5bdf\u548c\u9876\u89c6\u5730\u56fe\u7684\u4fe1\u606f\uff0c\u589e\u5f3a\u4e86\u5bf9\u8c61\u67e5\u627e\u7684\u80fd\u529b\u3002"}}
{"id": "2510.22949", "categories": ["cs.RO", "cs.SY", "eess.SY", "93C10", "I.2.9; I.2.8; J.2"], "pdf": "https://arxiv.org/pdf/2510.22949", "abs": "https://arxiv.org/abs/2510.22949", "authors": ["Benedictus C. G. Cinun", "Tua A. Tamba", "Immanuel R. Santjoko", "Xiaofeng Wang", "Michael A. Gunarso", "Bin Hu"], "title": "End-to-End Design and Validation of a Low-Cost Stewart Platform with Nonlinear Estimation and Control", "comment": "24 pages, journal", "summary": "This paper presents the complete design, control, and experimental validation\nof a low-cost Stewart platform prototype developed as an affordable yet capable\nrobotic testbed for research and education. The platform combines off the shelf\ncomponents with 3D printed and custom fabricated parts to deliver full six\ndegrees of freedom motions using six linear actuators connecting a moving\nplatform to a fixed base. The system software integrates dynamic modeling, data\nacquisition, and real time control within a unified framework. A robust\ntrajectory tracking controller based on feedback linearization, augmented with\nan LQR scheme, compensates for the platform's nonlinear dynamics to achieve\nprecise motion control. In parallel, an Extended Kalman Filter fuses IMU and\nactuator encoder feedback to provide accurate and reliable state estimation\nunder sensor noise and external disturbances. Unlike prior efforts that\nemphasize only isolated aspects such as modeling or control, this work delivers\na complete hardware-software platform validated through both simulation and\nexperiments on static and dynamic trajectories. Results demonstrate effective\ntrajectory tracking and real-time state estimation, highlighting the platform's\npotential as a cost effective and versatile tool for advanced research and\neducational applications.", "AI": {"tldr": "\u672c\u8bba\u6587\u5c55\u793a\u4e86\u4e00\u79cd\u4f4e\u6210\u672cStewart\u5e73\u53f0\u539f\u578b\u7684\u5b8c\u6574\u8bbe\u8ba1\u3001\u63a7\u5236\u53ca\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u65e8\u5728\u4e3a\u7814\u7a76\u548c\u6559\u80b2\u63d0\u4f9b\u53ef\u8d1f\u62c5\u4e14\u9ad8\u6548\u7684\u673a\u5668\u4eba\u6d4b\u8bd5\u5e73\u53f0\u3002", "motivation": "\u4e3a\u4e86\u63d0\u4f9b\u4e00\u79cd\u7ecf\u6d4e\u5b9e\u7528\u7684\u673a\u5668\u4eba\u6d4b\u8bd5\u5e73\u53f0\uff0c\u4fc3\u8fdb\u7814\u7a76\u4e0e\u6559\u80b2\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u53cd\u9988\u7ebf\u6027\u5316\u7684\u5f3a\u5065\u8f68\u8ff9\u8ddf\u8e2a\u63a7\u5236\u5668\uff0c\u7ed3\u5408LQR\u65b9\u6848\uff0c\u5e76\u4f7f\u7528\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u878d\u5408IMU\u4e0e\u6267\u884c\u5668\u7f16\u7801\u5668\u53cd\u9988\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u5e73\u53f0\u80fd\u591f\u5b9e\u73b0\u6709\u6548\u7684\u8f68\u8ff9\u8ddf\u8e2a\u548c\u5b9e\u65f6\u72b6\u6001\u4f30\u8ba1\u3002", "conclusion": "\u8be5\u5e73\u53f0\u7ecf\u8fc7\u9a8c\u8bc1\uff0c\u80fd\u591f\u6709\u6548\u8ddf\u8e2a\u8f68\u8ff9\u5e76\u5b9e\u65f6\u4f30\u8ba1\u72b6\u6001\uff0c\u5c55\u793a\u4e86\u5176\u4f5c\u4e3a\u9ad8\u6548\u4e14\u591a\u529f\u80fd\u7814\u7a76\u4e0e\u6559\u80b2\u5de5\u5177\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.23003", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.23003", "abs": "https://arxiv.org/abs/2510.23003", "authors": ["ZhengKai Huang", "YiKun Wang", "ChenYu Hui", "XiaoCheng"], "title": "An Intelligent Water-Saving Irrigation System Based on Multi-Sensor Fusion and Visual Servoing Control", "comment": null, "summary": "This paper introduces an intelligent water-saving irrigation system designed\nto address critical challenges in precision agriculture, such as inefficient\nwater use and poor terrain adaptability. The system integrates advanced\ncomputer vision, robotic control, and real-time stabilization technologies via\na multi-sensor fusion approach. A lightweight YOLO model, deployed on an\nembedded vision processor (K210), enables real-time plant container detection\nwith over 96% accuracy under varying lighting conditions. A simplified hand-eye\ncalibration algorithm-designed for 'handheld camera' robot arm\nconfigurations-ensures that the end effector can be precisely positioned, with\na success rate exceeding 90%. The active leveling system, driven by the\nSTM32F103ZET6 main control chip and JY901S inertial measurement data, can\nstabilize the irrigation platform on slopes up to 10 degrees, with a response\ntime of 1.8 seconds. Experimental results across three simulated agricultural\nenvironments (standard greenhouse, hilly terrain, complex lighting) demonstrate\na 30-50% reduction in water consumption compared to conventional flood\nirrigation, with water use efficiency exceeding 92% in all test cases.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u667a\u80fd\u8282\u6c34\u704c\u6e89\u7cfb\u7edf\uff0c\u89e3\u51b3\u7cbe\u51c6\u519c\u4e1a\u4e2d\u7684\u6c34\u8d44\u6e90\u5229\u7528\u4e0d\u5f53\u548c\u5730\u5f62\u9002\u5e94\u6027\u5dee\u7b49\u95ee\u9898\u3002", "motivation": "\u65e8\u5728\u63d0\u9ad8\u7cbe\u51c6\u519c\u4e1a\u4e2d\u7684\u6c34\u8d44\u6e90\u7ba1\u7406\u6548\u7387\uff0c\u89e3\u51b3\u6c34\u8d44\u6e90\u6d6a\u8d39\u95ee\u9898\u3002", "method": "\u7cfb\u7edf\u96c6\u6210\u4e86\u5148\u8fdb\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u673a\u5668\u4eba\u63a7\u5236\u548c\u5b9e\u65f6\u7a33\u5b9a\u5316\u6280\u672f\uff0c\u901a\u8fc7\u591a\u4f20\u611f\u5668\u878d\u5408\u7684\u65b9\u6cd5\u5b9e\u73b0\u3002", "result": "\u5728\u4e09\u79cd\u6a21\u62df\u519c\u4e1a\u73af\u5883\u4e2d\uff0c\u8be5\u7cfb\u7edf\u8868\u73b0\u51fa\u663e\u8457\u7684\u6c34\u6d88\u8017\u51cf\u5c11\u548c\u9ad8\u6c34\u5229\u7528\u6548\u7387\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u6d2a\u704c\u76f8\u6bd4\uff0c\u8be5\u7cfb\u7edf\u5728\u6c34\u6d88\u8017\u4e0a\u51cf\u5c11\u4e8630-50%\uff0c\u6c34\u5229\u7528\u6548\u7387\u5728\u6240\u6709\u6d4b\u8bd5\u60c5\u51b5\u4e0b\u90fd\u8d85\u8fc7\u4e8692%\u3002"}}
{"id": "2510.23016", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23016", "abs": "https://arxiv.org/abs/2510.23016", "authors": ["Zhuo Li", "Junjia Liu", "Dianxi Li", "Tao Teng", "Miao Li", "Sylvain Calinon", "Darwin Caldwell", "Fei Chen"], "title": "ManiDP: Manipulability-Aware Diffusion Policy for Posture-Dependent Bimanual Manipulation", "comment": "7 pages, 6 figures, Accepted and published in IROS 2025", "summary": "Recent work has demonstrated the potential of diffusion models in robot\nbimanual skill learning. However, existing methods ignore the learning of\nposture-dependent task features, which are crucial for adapting dual-arm\nconfigurations to meet specific force and velocity requirements in dexterous\nbimanual manipulation. To address this limitation, we propose\nManipulability-Aware Diffusion Policy (ManiDP), a novel imitation learning\nmethod that not only generates plausible bimanual trajectories, but also\noptimizes dual-arm configurations to better satisfy posture-dependent task\nrequirements. ManiDP achieves this by extracting bimanual manipulability from\nexpert demonstrations and encoding the encapsulated posture features using\nRiemannian-based probabilistic models. These encoded posture features are then\nincorporated into a conditional diffusion process to guide the generation of\ntask-compatible bimanual motion sequences. We evaluate ManiDP on six real-world\nbimanual tasks, where the experimental results demonstrate a 39.33$\\%$ increase\nin average manipulation success rate and a 0.45 improvement in task\ncompatibility compared to baseline methods. This work highlights the importance\nof integrating posture-relevant robotic priors into bimanual skill diffusion to\nenable human-like adaptability and dexterity.", "AI": {"tldr": "\u63d0\u51fa\u4e86Manipulability-Aware Diffusion Policy\u65b9\u6cd5\uff0c\u901a\u8fc7\u8003\u8651\u59ff\u6001\u4f9d\u8d56\u7684\u4efb\u52a1\u7279\u5f81\uff0c\u5728\u53cc\u624b\u6280\u80fd\u5b66\u4e60\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u64cd\u4f5c\u6210\u529f\u7387\u548c\u4efb\u52a1\u517c\u5bb9\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e86\u59ff\u6001\u4f9d\u8d56\u6027\u4efb\u52a1\u7279\u5f81\u7684\u5b66\u4e60\uff0c\u8fd9\u4e9b\u7279\u5f81\u5bf9\u8c03\u6574\u53cc\u81c2\u914d\u7f6e\u4ee5\u6ee1\u8db3\u7279\u5b9a\u529b\u91cf\u548c\u901f\u5ea6\u8981\u6c42\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u7684\u65b9\u6cd5\u751f\u6210\u5408\u7406\u7684\u53cc\u624b\u8f68\u8ff9\uff0c\u5e76\u4f18\u5316\u53cc\u81c2\u914d\u7f6e\u4ee5\u6ee1\u8db3\u59ff\u6001\u4f9d\u8d56\u7684\u4efb\u52a1\u8981\u6c42\uff0c\u540c\u65f6\u63d0\u53d6\u4e13\u5bb6\u793a\u8303\u4e2d\u7684\u53cc\u624b\u53ef\u64cd\u4f5c\u6027\u7279\u5f81\uff0c\u4f7f\u7528\u57fa\u4e8e\u9ece\u66fc\u51e0\u4f55\u7684\u6982\u7387\u6a21\u578b\u8fdb\u884c\u7f16\u7801\uff0c\u6700\u540e\u5728\u6761\u4ef6\u6269\u6563\u8fc7\u7a0b\u4e2d\u5f15\u5165\u8fd9\u4e9b\u7f16\u7801\u7279\u5f81\u3002", "result": "\u5728\u516d\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u53cc\u624b\u4efb\u52a1\u4e0a\uff0cManiDP\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5e73\u5747\u64cd\u4f5c\u6210\u529f\u7387\u63d0\u9ad8\u4e8639.33%\uff0c\u4efb\u52a1\u517c\u5bb9\u6027\u63d0\u9ad8\u4e860.45\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684Manipulability-Aware Diffusion Policy\u65b9\u6cd5\u5728\u53cc\u624b\u6280\u80fd\u5b66\u4e60\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6210\u529f\u63d0\u9ad8\u4e86\u64cd\u4f5c\u6210\u529f\u7387\u4e0e\u4efb\u52a1\u517c\u5bb9\u6027\uff0c\u5f3a\u8c03\u4e86\u59ff\u6001\u76f8\u5173\u7684\u673a\u5668\u4eba\u5148\u9a8c\u77e5\u8bc6\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.23057", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.IV", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.23057", "abs": "https://arxiv.org/abs/2510.23057", "authors": ["Oskar Natan", "Jun Miura"], "title": "Seq-DeepIPC: Sequential Sensing for End-to-End Control in Legged Robot Navigation", "comment": "Preprint notice, this manuscript has been submitted to IEEE sensors\n  journal for possible publication", "summary": "We present Seq-DeepIPC, a sequential end-to-end perception-to-control model\nfor legged robot navigation in realworld environments. Seq-DeepIPC advances\nintelligent sensing for autonomous legged navigation by tightly integrating\nmulti-modal perception (RGB-D + GNSS) with temporal fusion and control. The\nmodel jointly predicts semantic segmentation and depth estimation, giving\nricher spatial features for planning and control. For efficient deployment on\nedge devices, we use EfficientNet-B0 as the encoder, reducing computation while\nmaintaining accuracy. Heading estimation is simplified by removing the noisy\nIMU and instead computing the bearing angle directly from consecutive GNSS\npositions. We collected a larger and more diverse dataset that includes both\nroad and grass terrains, and validated Seq-DeepIPC on a robot dog. Comparative\nand ablation studies show that sequential inputs improve perception and control\nin our models, while other baselines do not benefit. Seq-DeepIPC achieves\ncompetitive or better results with reasonable model size; although GNSS-only\nheading is less reliable near tall buildings, it is robust in open areas.\nOverall, Seq-DeepIPC extends end-to-end navigation beyond wheeled robots to\nmore versatile and temporally-aware systems. To support future research, we\nwill release the codes to our GitHub repository at\nhttps://github.com/oskarnatan/Seq-DeepIPC.", "AI": {"tldr": "Seq-DeepIPC\u662f\u4e00\u79cd\u7528\u4e8e\u65e0\u4eba\u817f\u90e8\u673a\u5668\u4eba\u5bfc\u822a\u7684\u987a\u5e8f\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u611f\u77e5\u4e0e\u63a7\u5236\uff0c\u65e0\u9700\u590d\u6742\u7684IMU\uff0c\u9002\u5e94\u8fb9\u7f18\u8bbe\u5907\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u5bfc\u822a\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u817f\u90e8\u673a\u5668\u4eba\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\uff0c\u7ed3\u5408\u591a\u79cd\u611f\u77e5\u4fe1\u606f\u4f18\u5316\u611f\u77e5\u4e0e\u63a7\u5236\u3002", "method": "Seq-DeepIPC\u6a21\u578b\u901a\u8fc7\u591a\u6a21\u6001\u611f\u77e5\uff08RGB-D + GNSS\uff09\u548c\u65f6\u95f4\u878d\u5408\u6765\u5b9e\u73b0\uff0c\u4f7f\u7528EfficientNet-B0\u4f5c\u4e3a\u7f16\u7801\u5668\uff0c\u7b80\u5316\u822a\u5411\u4f30\u8ba1\u3002", "result": "Seq-DeepIPC\u5728\u4e0e\u5176\u4ed6\u57fa\u7ebf\u6bd4\u8f83\u65f6\u663e\u793a\u51fa\u66f4\u597d\u7684\u611f\u77e5\u548c\u63a7\u5236\u6548\u679c\uff0c\u5c24\u5176\u5728\u5f00\u653e\u533a\u57df\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "Seq-DeepIPC\u6269\u5c55\u4e86\u7aef\u5230\u7aef\u5bfc\u822a\u7684\u5e94\u7528\uff0c\u8868\u73b0\u51fa\u5728\u591a\u79cd\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2510.23059", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23059", "abs": "https://arxiv.org/abs/2510.23059", "authors": ["Yongtong Zhu", "Lei Li", "Iggy Qian", "WenBin Zhou", "Ye Yuan", "Qingdu Li", "Na Liu", "Jianwei Zhang"], "title": "Awakening Facial Emotional Expressions in Human-Robot", "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS 2025). 8 pages, 7 figures, IEEE two-column format", "summary": "The facial expression generation capability of humanoid social robots is\ncritical for achieving natural and human-like interactions, playing a vital\nrole in enhancing the fluidity of human-robot interactions and the accuracy of\nemotional expression. Currently, facial expression generation in humanoid\nsocial robots still relies on pre-programmed behavioral patterns, which are\nmanually coded at high human and time costs. To enable humanoid robots to\nautonomously acquire generalized expressive capabilities, they need to develop\nthe ability to learn human-like expressions through self-training. To address\nthis challenge, we have designed a highly biomimetic robotic face with\nphysical-electronic animated facial units and developed an end-to-end learning\nframework based on KAN (Kolmogorov-Arnold Network) and attention mechanisms.\nUnlike previous humanoid social robots, we have also meticulously designed an\nautomated data collection system based on expert strategies of facial motion\nprimitives to construct the dataset. Notably, to the best of our knowledge,\nthis is the first open-source facial dataset for humanoid social robots.\nComprehensive evaluations indicate that our approach achieves accurate and\ndiverse facial mimicry across different test subjects.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u4eff\u751f\u673a\u5668\u4eba\u9762\u90e8\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8eKAN\u7f51\u7edc\u7684\u81ea\u6211\u8bad\u7ec3\u6846\u67b6\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u81ea\u4e3b\u5b66\u4e60\u548c\u751f\u6210\u9762\u90e8\u8868\u60c5\u3002", "motivation": "\u63d0\u9ad8\u4eba\u5f62\u793e\u4ea4\u673a\u5668\u4eba\u7684\u9762\u90e8\u8868\u60c5\u751f\u6210\u80fd\u529b\uff0c\u4ee5\u5b9e\u73b0\u66f4\u81ea\u7136\u7684\u4eba\u673a\u4ea4\u4e92\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u57fa\u4e8eKAN\u548c\u6ce8\u610f\u529b\u673a\u5236\u7684\u7aef\u5230\u7aef\u5b66\u4e60\u6846\u67b6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u81ea\u52a8\u6570\u636e\u6536\u96c6\u7cfb\u7edf\u4ee5\u6784\u5efa\u6570\u636e\u96c6\u3002", "result": "\u672c\u7814\u7a76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u9ad8\u5ea6\u4eff\u751f\u7684\u673a\u5668\u4eba\u9762\u90e8\uff0c\u53ef\u4ee5\u81ea\u4e3b\u5b66\u4e60\u4eba\u7c7b\u7684\u9762\u90e8\u8868\u60c5\uff0c\u4ece\u800c\u63d0\u9ad8\u4eba\u673a\u4ea4\u4e92\u7684\u81ea\u7136\u6027\u548c\u60c5\u611f\u8868\u8fbe\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u63d0\u4f9b\u4e86\u5f00\u6e90\u7684\u4eba\u5f62\u793e\u4ea4\u673a\u5668\u4eba\u9762\u90e8\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u7efc\u5408\u8bc4\u4f30\u8bc1\u660e\u4e86\u5176\u5728\u4e0d\u540c\u6d4b\u8bd5\u5bf9\u8c61\u95f4\u7684\u9762\u90e8\u6a21\u4eff\u80fd\u529b\u7684\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\u3002"}}
{"id": "2510.23084", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23084", "abs": "https://arxiv.org/abs/2510.23084", "authors": ["Sunyou Hwang", "Christophe De Wagter", "Bart Remes", "Guido de Croon"], "title": "Breaking the Circle: An Autonomous Control-Switching Strategy for Stable Orographic Soaring in MAVs", "comment": "13 pages, 15 figures", "summary": "Orographic soaring can significantly extend the endurance of micro aerial\nvehicles (MAVs), but circling behavior, arising from control conflicts between\nthe longitudinal and vertical axes, increases energy consumption and the risk\nof divergence. We propose a control switching method, named SAOS: Switched\nControl for Autonomous Orographic Soaring, which mitigates circling behavior by\nselectively controlling either the horizontal or vertical axis, effectively\ntransforming the system from underactuated to fully actuated during soaring.\nAdditionally, the angle of attack is incorporated into the INDI controller to\nimprove force estimation. Simulations with randomized initial positions and\nwind tunnel experiments on two MAVs demonstrate that the SAOS improves position\nconvergence, reduces throttle usage, and mitigates roll oscillations caused by\npitch-roll coupling. These improvements enhance energy efficiency and flight\nstability in constrained soaring environments.", "AI": {"tldr": "SAOS \u65b9\u6cd5\u901a\u8fc7\u9009\u62e9\u6027\u63a7\u5236\u6539\u5584\u5fae\u578b\u822a\u7a7a\u5668\u5728\u6c14\u6d41 soaring \u4e2d\u7684\u6027\u80fd\uff0c\u589e\u5f3a\u4e86\u80fd\u6548\u548c\u98de\u884c\u7a33\u5b9a\u6027\u3002", "motivation": "\u89e3\u51b3\u5fae\u578b\u822a\u7a7a\u5668\u5728\u6c14\u6d41 soaring \u4e2d\u7684\u63a7\u5236\u51b2\u7a81\uff0c\u51cf\u5c11\u80fd\u8017\u548c\u98ce\u9669", "method": "SAOS: Switched Control for Autonomous Orographic Soaring", "result": "\u901a\u8fc7\u9009\u62e9\u6027\u63a7\u5236\u6a2a\u8f74\u6216\u7eb5\u8f74\uff0cSAOS \u6539\u5584\u4e86\u4f4d\u7f6e\u6536\u655b\u3001\u964d\u4f4e\u6cb9\u95e8\u4f7f\u7528\u548c\u51cf\u8f7b\u56e0\u4fef\u4ef0-\u7ffb\u6eda\u8026\u5408\u5bfc\u81f4\u7684\u7ffb\u6eda\u632f\u8361", "conclusion": "SAOS \u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u5728\u53d7\u9650\u98de\u884c\u73af\u5883\u4e2d\u7684\u80fd\u91cf\u6548\u7387\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2510.23109", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23109", "abs": "https://arxiv.org/abs/2510.23109", "authors": ["Bernhard Rameder", "Hubert Gattringer", "Ronald Naderer", "Andreas Mueller"], "title": "An Automated Tape Laying System Employing a Uniaxial Force Control Device", "comment": "Proceedings ECCM21 - 21st European Conference on Composite Materials,\n  Nantes, France, 7-2024", "summary": "This paper deals with the design of a cost effective automated tape laying\nsystem (ATL system) with integrated uniaxial force control to ensure the\nnecessary compaction forces as well as with an accurate temperature control to\nguarantee the used tape being melted appropriate. It is crucial to control the\nsubstrate and the oncoming tape onto a specific temperature level to ensure an\noptimal consolidation between the different layers of the product. Therefore,\nit takes several process steps from the spooled tape on the coil until it is\nfinally tacked onto the desired mold. The different modules are divided into\nthe tape storage spool, a tape-guiding roller, a tape processing unit, a\nheating zone and the consolidation unit. Moreover, a special robot control\nconcept for testing the ATL system is presented. In contrast to many other\nsystems, with this approach, the tape laying device is spatially fixed and the\nshape is moved accordingly by the robot, which allows for handling of rather\ncompact and complex shapes. The functionality of the subsystems and the taping\nprocess itself was finally approved in experimental results using a carbon\nfiber reinforced HDPE tape.", "AI": {"tldr": "\u672c\u6587\u8bbe\u8ba1\u4e86\u4e00\u79cd\u96c6\u6210\u5355\u8f74\u529b\u548c\u6e29\u5ea6\u63a7\u5236\u7684\u6210\u672c\u6548\u76ca\u9ad8\u7684\u81ea\u52a8\u80f6\u5e26\u94fa\u8bbe\u7cfb\u7edf\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u590d\u6742\u5f62\u72b6\u7c98\u5408\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u786e\u4fdd\u80f6\u5e26\u5728\u7c98\u5408\u8fc7\u7a0b\u4e2d\u5177\u6709\u9002\u5f53\u7684\u538b\u5b9e\u529b\u548c\u7194\u5316\u6548\u679c\uff0c\u4ee5\u5b9e\u73b0\u6700\u4f73\u7684\u5c42\u95f4\u7ed3\u5408\u3002", "method": "\u8bbe\u8ba1\u4e00\u4e2a\u6210\u672c\u6548\u76ca\u9ad8\u7684\u81ea\u52a8\u80f6\u5e26\u94fa\u8bbe\u7cfb\u7edf\uff0c\u96c6\u6210\u5355\u8f74\u529b\u63a7\u5236\u548c\u6e29\u5ea6\u63a7\u5236\u3002", "result": "\u7cfb\u7edf\u6a21\u5757\u5305\u62ec\u80f6\u5e26\u5b58\u50a8\u5377\u8f74\u3001\u5f15\u5bfc\u8f8a\u3001\u52a0\u5de5\u5355\u5143\u3001\u52a0\u70ed\u533a\u548c\u538b\u5b9e\u5355\u5143\uff0c\u529f\u80fd\u901a\u8fc7\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u3002", "conclusion": "\u901a\u8fc7\u7279\u5b9a\u7684\u673a\u5668\u4eba\u63a7\u5236\u6982\u5ff5\uff0c\u8be5\u7cfb\u7edf\u6709\u6548\u5904\u7406\u4e86\u7d27\u51d1\u548c\u590d\u6742\u5f62\u72b6\u7684\u6750\u6599\u7c98\u5408\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5404\u5b50\u7cfb\u7edf\u7684\u529f\u80fd\u3002"}}
{"id": "2510.23119", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23119", "abs": "https://arxiv.org/abs/2510.23119", "authors": ["Yi-Lin Wei", "Zhexi Luo", "Yuhao Lin", "Mu Lin", "Zhizhao Liang", "Shuoyu Chen", "Wei-Shi Zheng"], "title": "OmniDexGrasp: Generalizable Dexterous Grasping via Foundation Model and Force Feedback", "comment": "Project page: https://isee-laboratory.github.io/OmniDexGrasp/", "summary": "Enabling robots to dexterously grasp and manipulate objects based on human\ncommands is a promising direction in robotics. However, existing approaches are\nchallenging to generalize across diverse objects or tasks due to the limited\nscale of semantic dexterous grasp datasets. Foundation models offer a new way\nto enhance generalization, yet directly leveraging them to generate feasible\nrobotic actions remains challenging due to the gap between abstract model\nknowledge and physical robot execution. To address these challenges, we propose\nOmniDexGrasp, a generalizable framework that achieves omni-capabilities in user\nprompting, dexterous embodiment, and grasping tasks by combining foundation\nmodels with the transfer and control strategies. OmniDexGrasp integrates three\nkey modules: (i) foundation models are used to enhance generalization by\ngenerating human grasp images supporting omni-capability of user prompt and\ntask; (ii) a human-image-to-robot-action transfer strategy converts human\ndemonstrations into executable robot actions, enabling omni dexterous\nembodiment; (iii) force-aware adaptive grasp strategy ensures robust and stable\ngrasp execution. Experiments in simulation and on real robots validate the\neffectiveness of OmniDexGrasp on diverse user prompts, grasp task and dexterous\nhands, and further results show its extensibility to dexterous manipulation\ntasks.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faOmniDexGrasp\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u7840\u6a21\u578b\u4e0e\u8f6c\u79fb\u63a7\u5236\u7b56\u7565\u7ed3\u5408\uff0c\u63d0\u5347\u673a\u5668\u4eba\u5728\u591a\u6837\u5316\u6293\u53d6\u53ca\u64cd\u63a7\u4efb\u52a1\u4e2d\u7684\u6548\u80fd\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u673a\u5668\u4eba\u6293\u53d6\u65b9\u6cd5\u5728\u8de8\u4efb\u52a1\u548c\u7269\u4f53\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51faOmniDexGrasp\u6846\u67b6\uff0c\u5305\u62ec\u57fa\u7840\u6a21\u578b\u3001\u56fe\u50cf\u5230\u673a\u5668\u4eba\u52a8\u4f5c\u7684\u8f6c\u79fb\u7b56\u7565\u4ee5\u53ca\u529b\u611f\u77e5\u9002\u5e94\u6027\u6293\u53d6\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86OmniDexGrasp\u5728\u4e0d\u540c\u7528\u6237\u63d0\u793a\u3001\u6293\u53d6\u4efb\u52a1\u53ca\u7075\u5de7\u624b\u4e0a\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u6269\u5c55\u80fd\u529b\u3002", "conclusion": "OmniDexGrasp\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u7684\u64cd\u63a7\u80fd\u529b\u4e0e\u7a33\u5b9a\u6027\u3002"}}
{"id": "2510.23121", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23121", "abs": "https://arxiv.org/abs/2510.23121", "authors": ["Bharath Santhanam", "Alex Mitrevski", "Santosh Thoduka", "Sebastian Houben", "Teena Hassan"], "title": "Reliable Robotic Task Execution in the Face of Anomalies", "comment": "Accepted for publication in IEEE Robotics and Automation Letters\n  (RA-L)", "summary": "Learned robot policies have consistently been shown to be versatile, but they\ntypically have no built-in mechanism for handling the complexity of open\nenvironments, making them prone to execution failures; this implies that\ndeploying policies without the ability to recognise and react to failures may\nlead to unreliable and unsafe robot behaviour. In this paper, we present a\nframework that couples a learned policy with a method to detect visual\nanomalies during policy deployment and to perform recovery behaviours when\nnecessary, thereby aiming to prevent failures. Specifically, we train an\nanomaly detection model using data collected during nominal executions of a\ntrained policy. This model is then integrated into the online policy execution\nprocess, so that deviations from the nominal execution can trigger a\nthree-level sequential recovery process that consists of (i) pausing the\nexecution temporarily, (ii) performing a local perturbation of the robot's\nstate, and (iii) resetting the robot to a safe state by sampling from a learned\nexecution success model. We verify our proposed method in two different\nscenarios: (i) a door handle reaching task with a Kinova Gen3 arm using a\npolicy trained in simulation and transferred to the real robot, and (ii) an\nobject placing task with a UFactory xArm 6 using a general-purpose policy\nmodel. Our results show that integrating policy execution with anomaly\ndetection and recovery increases the execution success rate in environments\nwith various anomalies, such as trajectory deviations and adversarial human\ninterventions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5b66\u4e60\u7b56\u7565\u4e0e\u89c6\u89c9\u5f02\u5e38\u68c0\u6d4b\u548c\u6062\u590d\u884c\u4e3a\u7ed3\u5408\u7684\u6846\u67b6\uff0c\u4ee5\u5e94\u5bf9\u5f00\u653e\u73af\u5883\u4e2d\u7684\u590d\u6742\u6027\u548c\u6267\u884c\u5931\u8d25.", "motivation": "\u73b0\u6709\u5b66\u4e60\u7b56\u7565\u5728\u590d\u6742\u5f00\u653e\u73af\u5883\u4e2d\u5b58\u5728\u6267\u884c\u5931\u8d25\u7684\u95ee\u9898\uff0c\u7f3a\u4e4f\u8bc6\u522b\u548c\u53cd\u5e94\u5931\u8d25\u7684\u673a\u5236\u3002", "method": "\u8bad\u7ec3\u4e00\u79cd\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\uff0c\u5e76\u5728\u7b56\u7565\u6267\u884c\u8fc7\u7a0b\u4e2d\u96c6\u6210\u8be5\u6a21\u578b\u4ee5\u68c0\u6d4b\u504f\u5dee\uff0c\u89e6\u53d1\u4e09\u5c42\u6062\u590d\u8fc7\u7a0b\u3002", "result": "\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u663e\u793a\u5728\u5f02\u5e38\u73af\u5883\u4e0b\u6267\u884c\u6210\u529f\u7387\u63d0\u9ad8\u3002", "conclusion": "\u901a\u8fc7\u5c06\u7b56\u7565\u6267\u884c\u4e0e\u5f02\u5e38\u68c0\u6d4b\u548c\u6062\u590d\u96c6\u6210\uff0c\u53ef\u4ee5\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u591a\u79cd\u5f02\u5e38\u73af\u5883\u4e2d\u7684\u6267\u884c\u6210\u529f\u7387\u3002"}}
{"id": "2510.23129", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.23129", "abs": "https://arxiv.org/abs/2510.23129", "authors": ["Sabino Francesco Roselli", "Ze Zhang", "Knut \u00c5kesson"], "title": "Combining High Level Scheduling and Low Level Control to Manage Fleets of Mobile Robots", "comment": null, "summary": "The deployment of mobile robots for material handling in industrial\nenvironments requires scalable coordination of large fleets in dynamic\nsettings. This paper presents a two-layer framework that combines high-level\nscheduling with low-level control. Tasks are assigned and scheduled using the\ncompositional algorithm ComSat, which generates time-parameterized routes for\neach robot. These schedules are then used by a distributed Model Predictive\nControl (MPC) system in real time to compute local reference trajectories,\naccounting for static and dynamic obstacles. The approach ensures safe,\ncollision-free operation, and supports rapid rescheduling in response to\ndisruptions such as robot failures or environmental changes. We evaluate the\nmethod in simulated 2D environments with varying road capacities and traffic\nconditions, demonstrating high task completion rates and robust behavior even\nunder congestion. The modular structure of the framework allows for\ncomputational tractability and flexibility, making it suitable for deployment\nin complex, real-world industrial scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u5c42\u6846\u67b6\uff0c\u7528\u4e8e\u5de5\u4e1a\u73af\u5883\u4e2d\u79fb\u52a8\u673a\u5668\u4eba\u7684\u9ad8\u6548\u8c03\u5ea6\u4e0e\u63a7\u5236\uff0c\u786e\u4fdd\u5b89\u5168\u64cd\u4f5c\u548c\u8fc5\u901f\u54cd\u5e94\u53d8\u5316\u3002", "motivation": "\u5728\u52a8\u6001\u8bbe\u7f6e\u4e2d\u5bf9\u5927\u578b\u79fb\u52a8\u673a\u5668\u4eba\u961f\u4f0d\u8fdb\u884c\u53ef\u6269\u5c55\u534f\u8c03", "method": "\u4e24\u5c42\u6846\u67b6\u7ed3\u5408\u9ad8\u5c42\u8c03\u5ea6\u4e0e\u4f4e\u5c42\u63a7\u5236", "result": "\u5728\u6a21\u62df\u76842D\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u4efb\u52a1\u5b8c\u6210\u7387\u548c\u7a33\u5065\u8868\u73b0\uff0c\u652f\u6301\u5feb\u901f\u91cd\u65b0\u8c03\u5ea6", "conclusion": "\u8be5\u6a21\u5757\u5316\u6846\u67b6\u9002\u7528\u4e8e\u590d\u6742\u7684\u771f\u5b9e\u5de5\u4e1a\u573a\u666f\uff0c\u5177\u5907\u8ba1\u7b97\u53ef\u884c\u6027\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2510.23176", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23176", "abs": "https://arxiv.org/abs/2510.23176", "authors": ["Arnav Sukhija", "Lenart Treven", "Jin Cheng", "Florian D\u00f6rfler", "Stelian Coros", "Andreas Krause"], "title": "TARC: Time-Adaptive Robotic Control", "comment": null, "summary": "Fixed-frequency control in robotics imposes a trade-off between the\nefficiency of low-frequency control and the robustness of high-frequency\ncontrol, a limitation not seen in adaptable biological systems. We address this\nwith a reinforcement learning approach in which policies jointly select control\nactions and their application durations, enabling robots to autonomously\nmodulate their control frequency in response to situational demands. We\nvalidate our method with zero-shot sim-to-real experiments on two distinct\nhardware platforms: a high-speed RC car and a quadrupedal robot. Our method\nmatches or outperforms fixed-frequency baselines in terms of rewards while\nsignificantly reducing the control frequency and exhibiting adaptive frequency\ncontrol under real-world conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u6839\u636e\u60c5\u51b5\u9700\u6c42\u81ea\u9002\u5e94\u8c03\u8282\u63a7\u5236\u9891\u7387\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u5e73\u53f0\u4e0a\u6548\u7387\u4f18\u4e8e\u56fa\u5b9a\u9891\u7387\u63a7\u5236\u3002", "motivation": "\u56fa\u5b9a\u9891\u7387\u63a7\u5236\u5728\u673a\u5668\u4eba\u6280\u672f\u4e2d\u5b58\u5728\u4f4e\u9891\u63a7\u5236\u6548\u7387\u4e0e\u9ad8\u9891\u63a7\u5236\u9c81\u68d2\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u800c\u9002\u5e94\u6027\u751f\u7269\u7cfb\u7edf\u6ca1\u6709\u8fd9\u4e2a\u9650\u5236\u3002", "method": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u7b56\u7565\u5171\u540c\u9009\u62e9\u63a7\u5236\u52a8\u4f5c\u53ca\u5176\u5e94\u7528\u6301\u7eed\u65f6\u95f4\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u6839\u636e\u60c5\u51b5\u9700\u6c42\u81ea\u52a8\u8c03\u8282\u63a7\u5236\u9891\u7387\u3002", "result": "\u5728\u4e24\u4e2a\u4e0d\u540c\u7684\u786c\u4ef6\u5e73\u53f0\uff08\u9ad8\u901fRC\u8f66\u548c\u56db\u8db3\u673a\u5668\u4eba\uff09\u4e0a\u901a\u8fc7\u96f6\u6837\u672c\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u5176\u5956\u52b1\u4e0e\u56fa\u5b9a\u9891\u7387\u57fa\u7ebf\u76f8\u5339\u914d\u6216\u8d85\u51fa\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u63a7\u5236\u9891\u7387\uff0c\u5e76\u5728\u771f\u5b9e\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u81ea\u9002\u5e94\u9891\u7387\u63a7\u5236\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff0c\u673a\u5668\u4eba\u53ef\u4ee5\u5b9e\u73b0\u81ea\u9002\u5e94\u63a7\u5236\u9891\u7387\uff0c\u4ece\u800c\u63d0\u9ad8\u63a7\u5236\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.23227", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23227", "abs": "https://arxiv.org/abs/2510.23227", "authors": ["Klaus Zauner", "Josef El Dib", "Hubert Gattringer", "Andreas Mueller"], "title": "Workspace Registration and Collision Detection for Industrial Robotics Applications", "comment": null, "summary": "Motion planning for robotic manipulators relies on precise knowledge of the\nenvironment in order to be able to define restricted areas and to take\ncollision objects into account. To capture the workspace, point clouds of the\nenvironment are acquired using various sensors. The collision objects are\nidentified by region growing segmentation and VCCS algorithm. Subsequently the\npoint clusters are approximated. The aim of the present paper is to compare\ndifferent sensors, to illustrate the process from detection to the finished\ncollision environment and to detect collisions between the robot and this\nenvironment.", "AI": {"tldr": "\u672c\u6587\u65e8\u5728\u6bd4\u8f83\u4e0d\u540c\u4f20\u611f\u5668\u5bf9\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u73af\u5883\u7684\u5f71\u54cd\uff0c\u5c55\u793a\u4ece\u83b7\u53d6\u73af\u5883\u6570\u636e\u5230\u8bc6\u522b\u78b0\u649e\u7269\u4f53\u7684\u5168\u8fc7\u7a0b\uff0c\u5e76\u63a2\u8ba8\u673a\u5668\u4eba\u4e0e\u73af\u5883\u95f4\u7684\u78b0\u649e\u68c0\u6d4b\u3002", "motivation": "\u786e\u4fdd\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b89\u5168\u79fb\u52a8\uff0c\u51cf\u5c11\u78b0\u649e\u98ce\u9669\uff0c\u4f9d\u8d56\u4e8e\u7cbe\u786e\u7684\u73af\u5883\u77e5\u8bc6\u548c\u4f20\u611f\u5668\u652f\u6301\u3002", "method": "\u91c7\u7528\u70b9\u4e91\u6570\u636e\u83b7\u53d6\u73af\u5883\u4fe1\u606f\uff0c\u4f7f\u7528\u533a\u57df\u751f\u957f\u5206\u5272\u548cVCCS\u7b97\u6cd5\u8bc6\u522b\u78b0\u649e\u7269\u4f53\uff0c\u5e76\u5bf9\u70b9\u96c6\u8fdb\u884c\u8fd1\u4f3c\u3002", "result": "\u6bd4\u8f83\u4e86\u4e0d\u540c\u4f20\u611f\u5668\u7684\u6548\u679c\uff0c\u63ed\u793a\u4e86\u4ece\u6570\u636e\u83b7\u53d6\u5230\u78b0\u649e\u68c0\u6d4b\u7684\u5b8c\u6574\u6d41\u7a0b\uff0c\u4e3a\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002", "conclusion": "\u4e0d\u540c\u4f20\u611f\u5668\u5728\u73af\u5883\u76d1\u6d4b\u548c\u78b0\u649e\u68c0\u6d4b\u65b9\u9762\u7684\u6709\u6548\u6027\u5bf9\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.23234", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23234", "abs": "https://arxiv.org/abs/2510.23234", "authors": ["Klaus Zauner", "Hubert Gattringer", "Andreas Mueller"], "title": "Optimal Dimensioning of Elastic-Link Manipulators regarding Lifetime Estimation", "comment": "Mechanics Based Design of Structures and Machines, December 2024", "summary": "Resourceful operation and design of robots is key for sustainable industrial\nautomation. This will be enabled by lightweight design along with time and\nenergy optimal control of robotic manipulators. Design and control of such\nsystems is intertwined as the control must take into account inherent\nmechanical compliance while the design must accommodate the dynamic\nrequirements demanded by the control. As basis for such design optimization, a\nmethod for estimating the lifetime of elastic link robotic manipulators is\npresented. This is applied to the geometry optimization of flexible serial\nmanipulators performing pick-and-place operations, where the optimization\nobjective is a combination of overall weight and vibration amplitudes. The\nlifetime estimation draws from a fatigue analysis combining the rainflow\ncounting algorithm and the method of critical cutting plane. Tresca hypothesis\nis used to formulate an equivalent stress, and linear damage accumulation is\nassumed. The final robot geometry is selected from a Pareto front as a tradeoff\nof lifetime and vibration characteristic. The method is illustrated for a three\ndegrees of freedom articulated robotic manipulator.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bff\u547d\u4f30\u8ba1\u548c\u51e0\u4f55\u4f18\u5316\uff0c\u63d0\u5347\u67d4\u6027\u4e32\u8054\u673a\u5668\u4eba\u7684\u8bbe\u8ba1\u4e0e\u63a7\u5236\u6027\u80fd\uff0c\u91cd\u70b9\u5728\u4e8e\u964d\u4f4e\u91cd\u91cf\u548c\u632f\u52a8\u3002", "motivation": "\u63d0\u9ad8\u5de5\u4e1a\u81ea\u52a8\u5316\u7684\u53ef\u6301\u7eed\u6027\uff0c\u91cd\u70b9\u5728\u4e8e\u8f7b\u91cf\u5316\u8bbe\u8ba1\u53ca\u5176\u4e0e\u65f6\u95f4\u548c\u80fd\u91cf\u6700\u4f18\u63a7\u5236\u7684\u6574\u5408\u3002", "method": "\u5229\u7528\u75b2\u52b3\u5206\u6790\u7ed3\u5408\u96e8\u6d41\u8ba1\u6570\u7b97\u6cd5\u548c\u4e34\u754c\u5207\u5e73\u9762\u6cd5\uff0c\u5bf9\u5f39\u6027\u94fe\u63a5\u673a\u5668\u4eba\u7684\u5bff\u547d\u8fdb\u884c\u4f30\u8ba1\u3002", "result": "\u6700\u7ec8\u7684\u673a\u5668\u4eba\u51e0\u4f55\u5f62\u72b6\u901a\u8fc7Pareto\u524d\u6cbf\u9009\u62e9\uff0c\u5e73\u8861\u4e86\u5bff\u547d\u548c\u632f\u52a8\u7279\u6027\u3002\u672c\u7814\u7a76\u4ee5\u4e00\u4e2a\u5177\u6709\u4e09\u4e2a\u81ea\u7531\u5ea6\u7684\u5173\u8282\u673a\u5668\u4eba\u4e3a\u4f8b\u8fdb\u884c\u4e86\u8bf4\u660e\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f39\u6027\u94fe\u63a5\u673a\u5668\u4eba\u7684\u5bff\u547d\u4f30\u8ba1\u548c\u51e0\u4f55\u4f18\u5316\uff0c\u63d0\u5347\u67d4\u6027\u4e32\u8054\u673a\u5668\u4eba\u5728\u62fe\u53d6\u548c\u653e\u7f6e\u64cd\u4f5c\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2510.23258", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23258", "abs": "https://arxiv.org/abs/2510.23258", "authors": ["Riko Yokozawa", "Kentaro Fujii", "Yuta Nomura", "Shingo Murata"], "title": "Deep Active Inference with Diffusion Policy and Multiple Timescale World Model for Real-World Exploration and Navigation", "comment": "Preprint version", "summary": "Autonomous robotic navigation in real-world environments requires exploration\nto acquire environmental information as well as goal-directed navigation in\norder to reach specified targets. Active inference (AIF) based on the\nfree-energy principle provides a unified framework for these behaviors by\nminimizing the expected free energy (EFE), thereby combining epistemic and\nextrinsic values. To realize this practically, we propose a deep AIF framework\nthat integrates a diffusion policy as the policy model and a multiple timescale\nrecurrent state-space model (MTRSSM) as the world model. The diffusion policy\ngenerates diverse candidate actions while the MTRSSM predicts their\nlong-horizon consequences through latent imagination, enabling action selection\nthat minimizes EFE. Real-world navigation experiments demonstrated that our\nframework achieved higher success rates and fewer collisions compared with the\nbaselines, particularly in exploration-demanding scenarios. These results\nhighlight how AIF based on EFE minimization can unify exploration and\ngoal-directed navigation in real-world robotic settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u4e3b\u52a8\u63a8\u7406\u6846\u67b6\uff0c\u4f7f\u673a\u5668\u4eba\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u80fd\u591f\u66f4\u6709\u6548\u5730\u8fdb\u884c\u63a2\u7d22\u548c\u76ee\u6807\u5bfc\u822a\u3002", "motivation": "\u5728\u771f\u5b9e\u73af\u5883\u4e2d\uff0c\u673a\u5668\u4eba\u5bfc\u822a\u9700\u8981\u63a2\u7d22\u4ee5\u83b7\u53d6\u73af\u5883\u4fe1\u606f\uff0c\u5e76\u8fdb\u884c\u76ee\u6807\u5bfc\u5411\u7684\u5bfc\u822a\u4ee5\u5230\u8fbe\u6307\u5b9a\u76ee\u6807\uff0c\u57fa\u4e8e\u81ea\u7531\u80fd\u539f\u7406\u7684\u4e3b\u52a8\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6AIF\u6846\u67b6\uff0c\u7ed3\u5408\u6269\u6563\u7b56\u7565\u4f5c\u4e3a\u7b56\u7565\u6a21\u578b\u548c\u591a\u65f6\u95f4\u89c4\u6a21\u9012\u5f52\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08MTRSSM\uff09\u4f5c\u4e3a\u4e16\u754c\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u6846\u67b6\u5728\u8981\u6c42\u63a2\u7d22\u7684\u573a\u666f\u4e2d\u6210\u529f\u7387\u66f4\u9ad8\uff0c\u78b0\u649e\u66f4\u5c11\u3002", "conclusion": "\u57fa\u4e8e\u9884\u671f\u81ea\u7531\u80fd\u6700\u5c0f\u5316\u7684\u4e3b\u52a8\u63a8\u7406\uff08AIF\uff09\u6846\u67b6\u80fd\u591f\u7edf\u4e00\u63a2\u7d22\u548c\u76ee\u6807\u5bfc\u5411\u5bfc\u822a\uff0c\u5b9e\u73b0\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u53d6\u5f97\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u66f4\u5c11\u7684\u78b0\u649e\u3002"}}
{"id": "2510.23286", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23286", "abs": "https://arxiv.org/abs/2510.23286", "authors": ["Jin Huang", "Yingqiang Wang", "Haoda Li", "Zichen Liu", "Zhikun Wang", "Ying Chen"], "title": "Precise Time Delay Measurement and Compensation for Tightly Coupled Underwater SINS/piUSBL Navigation", "comment": null, "summary": "In multi-sensor systems, time synchronization between sensors is a\nsignificant challenge, and this issue is particularly pronounced in underwater\nintegrated navigation systems incorporating acoustic positioning. Such systems\nare highly susceptible to time delay, which can significantly degrade accuracy\nwhen measurement and fusion moments are misaligned. To address this challenge,\nthis paper introduces a tightly coupled navigation framework that integrates a\npassive inverted ultra-short baseline (piUSBL) acoustic positioning system, a\nstrapdown inertial navigation system (SINS), and a depth gauge under precise\ntime synchronization. The framework fuses azimuth and slant range from the\npiUSBL with depth data, thereby avoiding poor vertical-angle observability in\nplanar arrays. A novel delay measurement strategy is introduced, combining\nsynchronized timing with acoustic signal processing, which redefines\ndelay-traditionally an unobservable error-into a quantifiable parameter,\nenabling explicit estimation of both acoustic propagation and system processing\ndelays. Simulations and field experiments confirm the feasibility of the\nproposed method, with delay-compensated navigation reducing RMSE by 40.45% and\nmaximum error by 32.55%. These findings show that precise delay measurement and\ncompensation not only enhance underwater navigation accuracy but also establish\na generalizable framework for acoustic positioning integration, offering\nvaluable insights into time alignment and data fusion in latency-sensitive\nmulti-sensor systems.", "AI": {"tldr": "\u6c34\u4e0b\u6574\u5408\u5bfc\u822a\u7cfb\u7edf\u4e2d\uff0c\u65f6\u95f4\u540c\u6b65\u95ee\u9898\u5bfc\u81f4\u7cbe\u5ea6\u4e0b\u964d\u3002\u672c\u6587\u63d0\u51fa\u7684\u7d27\u8026\u5408\u5bfc\u822a\u6846\u67b6\u901a\u8fc7\u65b0\u9896\u7684\u5ef6\u8fdf\u6d4b\u91cf\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5bfc\u822a\u7cbe\u5ea6\uff0c\u5e76\u51cf\u5c11\u4e86\u8bef\u5dee\u3002", "motivation": "\u5728\u6c34\u4e0b\u591a\u4f20\u611f\u5668\u7cfb\u7edf\u4e2d\uff0c\u65f6\u95f4\u540c\u6b65\u95ee\u9898\u663e\u8457\u5f71\u54cd\u6d4b\u91cf\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u58f0\u5b66\u5b9a\u4f4d\u7cfb\u7edf\u4e2d\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u6574\u5408piUSBL\u58f0\u5b66\u5b9a\u4f4d\u7cfb\u7edf\u3001SINS\u548c\u6df1\u5ea6\u8ba1\uff0c\u4f7f\u7528\u65b0\u578b\u5ef6\u8fdf\u6d4b\u91cf\u7b56\u7565\u7ed3\u5408\u58f0\u5b66\u4fe1\u53f7\u5904\u7406\u6765\u540c\u6b65\u65f6\u95f4\uff0c\u5b9a\u4e49\u5ef6\u8fdf\u4e3a\u53ef\u91cf\u5316\u53c2\u6570\uff0c\u4ece\u800c\u5b9e\u73b0\u663e\u5f0f\u4f30\u8ba1\u58f0\u5b66\u4f20\u64ad\u4e0e\u7cfb\u7edf\u5904\u7406\u5ef6\u8fdf\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7d27\u8026\u5408\u7684\u5bfc\u822a\u6846\u67b6\uff0c\u6574\u5408\u4e86\u88ab\u52a8\u5012\u7f6e\u8d85\u77ed\u57fa\u7ebf\uff08piUSBL\uff09\u58f0\u5b66\u5b9a\u4f4d\u7cfb\u7edf\u3001\u975e\u56fa\u5b9a\u60ef\u6027\u5bfc\u822a\u7cfb\u7edf\uff08SINS\uff09\u548c\u6df1\u5ea6\u8ba1\uff0c\u5728\u7cbe\u786e\u65f6\u95f4\u540c\u6b65\u4e0b\u5bf9\u6570\u636e\u8fdb\u884c\u878d\u5408\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5bfc\u822a\u4e2d\u7684\u65f6\u95f4\u5ef6\u8fdf\u8bef\u5dee\uff0c\u63d0\u5347\u4e86\u6c34\u4e0b\u5bfc\u822a\u7cbe\u5ea6", "conclusion": "\u7cbe\u786e\u7684\u5ef6\u8fdf\u6d4b\u91cf\u4e0e\u8865\u507f\u4e0d\u4ec5\u63d0\u5347\u4e86\u6c34\u4e0b\u5bfc\u822a\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u4e3a\u58f0\u5b66\u5b9a\u4f4d\u96c6\u6210\u63d0\u4f9b\u4e86\u901a\u7528\u6846\u67b6\uff0c\u4fc3\u8fdb\u4e86\u5728\u591a\u4f20\u611f\u5668\u7cfb\u7edf\u4e2d\u7684\u65f6\u95f4\u5bf9\u9f50\u4e0e\u6570\u636e\u878d\u5408\u3002"}}
{"id": "2510.23329", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23329", "abs": "https://arxiv.org/abs/2510.23329", "authors": ["Shreya Santra", "Thomas Robbins", "Kazuya Yoshida"], "title": "Transferable Deep Reinforcement Learning for Cross-Domain Navigation: from Farmland to the Moon", "comment": "6 pages, 7 figures. Accepted at IEEE iSpaRo 2025", "summary": "Autonomous navigation in unstructured environments is essential for field and\nplanetary robotics, where robots must efficiently reach goals while avoiding\nobstacles under uncertain conditions. Conventional algorithmic approaches often\nrequire extensive environment-specific tuning, limiting scalability to new\ndomains. Deep Reinforcement Learning (DRL) provides a data-driven alternative,\nallowing robots to acquire navigation strategies through direct interactions\nwith their environment. This work investigates the feasibility of DRL policy\ngeneralization across visually and topographically distinct simulated domains,\nwhere policies are trained in terrestrial settings and validated in a zero-shot\nmanner in extraterrestrial environments. A 3D simulation of an agricultural\nrover is developed and trained using Proximal Policy Optimization (PPO) to\nachieve goal-directed navigation and obstacle avoidance in farmland settings.\nThe learned policy is then evaluated in a lunar-like simulated environment to\nassess transfer performance. The results indicate that policies trained under\nterrestrial conditions retain a high level of effectiveness, achieving close to\n50\\% success in lunar simulations without the need for additional training and\nfine-tuning. This underscores the potential of cross-domain DRL-based policy\ntransfer as a promising approach to developing adaptable and efficient\nautonomous navigation for future planetary exploration missions, with the added\nbenefit of minimizing retraining costs.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5728\u4e0d\u540c\u6a21\u62df\u57df\u95f4\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u901a\u8fc7\u519c\u4e1a\u63a2\u6d4b\u5668\u76843D\u6a21\u62df\u9a8c\u8bc1\u4e86\u653f\u7b56\u5728\u6708\u7403\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5728\u4e0d\u786e\u5b9a\u7684\u73af\u5883\u4e2d\u9ad8\u6548\u5bfc\u822a\u662f\u673a\u5668\u4eba\u6280\u672f\u7684\u91cd\u8981\u9700\u6c42\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u5927\u91cf\u73af\u5883\u7279\u5b9a\u8c03\u6574\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60 (DRL) \u7684\u7b56\u7565\u6cdb\u5316", "result": "\u8bad\u7ec3\u7684\u7b56\u7565\u5728\u6708\u7403\u6a21\u62df\u73af\u5883\u4e2d\u4ee5\u63a5\u8fd150\\%\u7684\u6210\u529f\u7387\u6709\u6548\uff0c\u8868\u660e\u5728\u5730\u9762\u6761\u4ef6\u4e0b\u8bad\u7ec3\u7684\u7b56\u7565\u80fd\u591f\u5728\u65b0\u57df\u4e2d\u8f6c\u79fb\uff0c\u800c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "conclusion": "\u8de8\u57df\u7684DRL\u7b56\u7565\u8f6c\u79fb\u4e3a\u672a\u6765\u884c\u661f\u63a2\u7d22\u4efb\u52a1\u7684\u81ea\u9002\u5e94\u5bfc\u822a\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u4e14\u80fd\u964d\u4f4e\u518d\u8bad\u7ec3\u6210\u672c\u3002"}}
{"id": "2510.23357", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23357", "abs": "https://arxiv.org/abs/2510.23357", "authors": ["Shaohan Bian", "Ying Zhang", "Guohui Tian", "Zhiqiang Miao", "Edmond Q. Wu", "Simon X. Yang", "Changchun Hua"], "title": "Large language model-based task planning for service robots: A review", "comment": "Submitted to Biomimetic Intelligence and Robotics for possible\n  publication", "summary": "With the rapid advancement of large language models (LLMs) and robotics,\nservice robots are increasingly becoming an integral part of daily life,\noffering a wide range of services in complex environments. To deliver these\nservices intelligently and efficiently, robust and accurate task planning\ncapabilities are essential. This paper presents a comprehensive overview of the\nintegration of LLMs into service robotics, with a particular focus on their\nrole in enhancing robotic task planning. First, the development and\nfoundational techniques of LLMs, including pre-training, fine-tuning,\nretrieval-augmented generation (RAG), and prompt engineering, are reviewed. We\nthen explore the application of LLMs as the cognitive core-`brain'-of service\nrobots, discussing how LLMs contribute to improved autonomy and\ndecision-making. Furthermore, recent advancements in LLM-driven task planning\nacross various input modalities are analyzed, including text, visual, audio,\nand multimodal inputs. Finally, we summarize key challenges and limitations in\ncurrent research and propose future directions to advance the task planning\ncapabilities of service robots in complex, unstructured domestic environments.\nThis review aims to serve as a valuable reference for researchers and\npractitioners in the fields of artificial intelligence and robotics.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u670d\u52a1\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u4e2d\u7684\u5e94\u7528\uff0c\u6db5\u76d6\u4e86\u6280\u672f\u57fa\u7840\u3001\u8ba4\u77e5\u6838\u5fc3\u89d2\u8272\u3001\u4e0d\u540c\u8f93\u5165\u6a21\u5f0f\u4e0b\u7684\u4efb\u52a1\u89c4\u5212\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u673a\u5668\u4eba\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u670d\u52a1\u673a\u5668\u4eba\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\uff0c\u56e0\u6b64\u9700\u8981\u5f3a\u5927\u3001\u51c6\u786e\u7684\u4efb\u52a1\u89c4\u5212\u80fd\u529b\u6765\u63d0\u4f9b\u667a\u80fd\u548c\u9ad8\u6548\u7684\u670d\u52a1\u3002", "method": "\u56de\u987e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u53ca\u57fa\u7840\u6280\u672f\uff0c\u5305\u62ec\u9884\u8bad\u7ec3\u3001\u5fae\u8c03\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u63d0\u793a\u5de5\u7a0b\uff0c\u5e76\u5206\u6790\u5176\u5728\u81ea\u52a8\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u672c\u8bba\u6587\u5168\u9762\u6982\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u670d\u52a1\u673a\u5668\u4eba\u4e2d\u7684\u6574\u5408\uff0c\u91cd\u70b9\u8ba8\u8bba\u5176\u5728\u589e\u5f3a\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u5206\u6790\u4e86\u5728\u590d\u6742\u975e\u7ed3\u6784\u5316\u5bb6\u5ead\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "\u672c\u6587\u603b\u7ed3\u4e86\u5f53\u524d\u7814\u7a76\u4e2d\u7684\u5173\u952e\u6311\u6218\u548c\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u63d0\u5347\u670d\u52a1\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u80fd\u529b\u7684\u672a\u6765\u65b9\u5411\u3002"}}
{"id": "2510.23359", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23359", "abs": "https://arxiv.org/abs/2510.23359", "authors": ["Chungeng Tian", "Ning Hao", "Fenghua He"], "title": "T-ESKF: Transformed Error-State Kalman Filter for Consistent Visual-Inertial Navigation", "comment": "This paper was submitted to IEEE RA-L on July 14, 2024, and accepted\n  on December 18, 2024. This version serves as the 'plus edition' of the\n  accepted paper, incorporating supplementary materials for completeness", "summary": "This paper presents a novel approach to address the inconsistency problem\ncaused by observability mismatch in visual-inertial navigation systems (VINS).\nThe key idea involves applying a linear time-varying transformation to the\nerror-state within the Error-State Kalman Filter (ESKF). This transformation\nensures that \\textrr{the unobservable subspace of the transformed error-state\nsystem} becomes independent of the state, thereby preserving the correct\nobservability of the transformed system against variations in linearization\npoints. We introduce the Transformed ESKF (T-ESKF), a consistent VINS estimator\nthat performs state estimation using the transformed error-state system.\nFurthermore, we develop an efficient propagation technique to accelerate the\ncovariance propagation based on the transformation relationship between the\ntransition and accumulated matrices of T-ESKF and ESKF. We validate the\nproposed method through extensive simulations and experiments, demonstrating\nbetter (or competitive at least) performance compared to state-of-the-art\nmethods. The code is available at github.com/HITCSC/T-ESKF.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5T-ESKF\uff0c\u901a\u8fc7\u7ebf\u6027\u65f6\u53d8\u53d8\u6362\u63d0\u9ad8\u89c6\u89c9\u60ef\u6027\u5bfc\u822a\u7cfb\u7edf\u7684\u53ef\u89c2\u5bdf\u6027\uff0c\u7ecf\u8fc7\u9a8c\u8bc1\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u9488\u5bf9\u89c6\u89c9\u60ef\u6027\u5bfc\u822a\u7cfb\u7edf\u4e2d\u7531\u4e8e\u53ef\u89c2\u5bdf\u6027\u4e0d\u5339\u914d\u5bfc\u81f4\u7684\u4f30\u8ba1\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5bfb\u6c42\u6539\u8fdb\u7684\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u7ebf\u6027\u65f6\u53d8\u53d8\u6362\u5904\u7406\u8bef\u5dee\u72b6\u6001\uff0c\u5728\u8bef\u5dee\u72b6\u6001\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u4e2d\u5e94\u7528\u8be5\u53d8\u6362\u4ee5\u63d0\u9ad8\u53ef\u89c2\u5bdf\u6027\u3002", "result": "\u7ecf\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0cT-ESKF\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u6216\u81f3\u5c11\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u901a\u8fc7\u65b0\u7684\u53d8\u6362\u6280\u672f\uff0cT-ESKF\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u89c6\u89c9\u60ef\u6027\u5bfc\u822a\u7cfb\u7edf\u4e2d\u7684\u53ef\u89c2\u5bdf\u6027\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5e76\u5728\u591a\u6b21\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2510.23386", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23386", "abs": "https://arxiv.org/abs/2510.23386", "authors": ["Alvaro Paz", "Mahdi Hejrati", "Pauli Mustalahti", "Jouni Mattila"], "title": "Full-Dynamics Real-Time Nonlinear Model Predictive Control of Heavy-Duty Hydraulic Manipulator for Trajectory Tracking Tasks", "comment": "This work has been submitted for possible publication in IEEE", "summary": "Heavy-duty hydraulic manipulators (HHMs) operate under strict physical and\nsafety-critical constraints due to their large size, high power, and complex\nnonlinear dynamics. Ensuring that both joint-level and end-effector\ntrajectories remain compliant with actuator capabilities, such as force,\nvelocity, and position limits, is essential for safe and reliable operation,\nyet remains largely underexplored in real-time control frameworks. This paper\npresents a nonlinear model predictive control (NMPC) framework designed to\nguarantee constraint satisfaction throughout the full nonlinear dynamics of\nHHMs, while running at a real-time control frequency of 1 kHz. The proposed\nmethod combines a multiple-shooting strategy with real-time sensor feedback,\nand is supported by a robust low-level controller based on virtual\ndecomposition control (VDC) for precise joint tracking. Experimental validation\non a full-scale hydraulic manipulator shows that the NMPC framework not only\nenforces actuator constraints at the joint level, but also ensures\nconstraint-compliant motion in Cartesian space for the end-effector. These\nresults demonstrate the method's capability to deliver high-accuracy trajectory\ntracking while strictly respecting safety-critical limits, setting a new\nbenchmark for real-time control in large-scale hydraulic systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u80fd\u591f\u57281kHz\u7684\u5b9e\u65f6\u63a7\u5236\u9891\u7387\u4e0b\uff0c\u786e\u4fdd\u91cd\u578b\u6db2\u538b\u64cd\u7eb5\u5668\u7684\u7ea6\u675f\u6ee1\u8db3\uff0c\u9002\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u8f68\u8ff9\u8ddf\u8e2a\u548c\u5b89\u5168\u6027\u8981\u6c42\u3002", "motivation": "\u5728\u91cd\u578b\u6db2\u538b\u64cd\u7eb5\u5668(HHMs)\u4e2d\uff0c\u5fc5\u987b\u786e\u4fdd\u5173\u8282\u7ea7\u548c\u672b\u7aef\u6267\u884c\u5668\u7684\u8f68\u8ff9\u9075\u5faa\u6267\u884c\u5668\u80fd\u529b\u7684\u9650\u5236\uff0c\u4ee5\u786e\u4fdd\u5b89\u5168\u548c\u53ef\u9760\u7684\u64cd\u4f5c\u3002\u7136\u800c\uff0c\u5b9e\u65f6\u63a7\u5236\u6846\u67b6\u4e2d\u5bf9\u6b64\u7684\u63a2\u7d22\u4ecd\u7136\u4e0d\u8db3\u3002", "method": "\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236(NMPC)\u6846\u67b6", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cNMPC\u6846\u67b6\u4e0d\u4ec5\u5728\u5173\u8282\u7ea7\u5f3a\u5236\u6267\u884c\u6267\u884c\u5668\u7ea6\u675f\uff0c\u8fd8\u786e\u4fdd\u4e86\u672b\u7aef\u6267\u884c\u5668\u5728\u7b1b\u5361\u5c14\u7a7a\u95f4\u4e2d\u7684\u7ea6\u675f\u5408\u89c4\u8fd0\u52a8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u578b\u6db2\u538b\u7cfb\u7edf\u7684\u5b9e\u65f6\u63a7\u5236\u8bbe\u5b9a\u4e86\u65b0\u7684\u57fa\u51c6\uff0c\u5c55\u793a\u4e86\u9ad8\u7cbe\u5ea6\u8f68\u8ff9\u8ddf\u8e2a\u80fd\u529b\uff0c\u540c\u65f6\u4e25\u683c\u9075\u5b88\u5b89\u5168\u5173\u952e\u7684\u9650\u5236\u3002"}}
{"id": "2510.23495", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23495", "abs": "https://arxiv.org/abs/2510.23495", "authors": ["Chenyang Ma", "Kai Lu", "Ruta Desai", "Xavier Puig", "Andrew Markham", "Niki Trigoni"], "title": "COOPERA: Continual Open-Ended Human-Robot Assistance", "comment": "NeurIPS 2025 (Spotlight); Project Page:\n  https://dannymcy.github.io/coopera/", "summary": "To understand and collaborate with humans, robots must account for individual\nhuman traits, habits, and activities over time. However, most robotic\nassistants lack these abilities, as they primarily focus on predefined tasks in\nstructured environments and lack a human model to learn from. This work\nintroduces COOPERA, a novel framework for COntinual, OPen-Ended human-Robot\nAssistance, where simulated humans, driven by psychological traits and\nlong-term intentions, interact with robots in complex environments. By\nintegrating continuous human feedback, our framework, for the first time,\nenables the study of long-term, open-ended human-robot collaboration (HRC) in\ndifferent collaborative tasks across various time-scales. Within COOPERA, we\nintroduce a benchmark and an approach to personalize the robot's collaborative\nactions by learning human traits and context-dependent intents. Experiments\nvalidate the extent to which our simulated humans reflect realistic human\nbehaviors and demonstrate the value of inferring and personalizing to human\nintents for open-ended and long-term HRC. Project Page:\nhttps://dannymcy.github.io/coopera/", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa COOPERA \u6846\u67b6\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u957f\u671f\u5f00\u653e\u5f0f\u7684\u4eba\u673a\u534f\u4f5c\uff0c\u5f3a\u8c03\u901a\u8fc7\u6301\u7eed\u53cd\u9988\u63d0\u9ad8\u673a\u5668\u4eba\u7684\u4e2a\u6027\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u673a\u5668\u4eba\u52a9\u624b\u7f3a\u4e4f\u8003\u8651\u4eba\u7c7b\u4e2a\u4f53\u7279\u8d28\u548c\u957f\u671f\u4e60\u60ef\u7684\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5176\u5728\u81ea\u7531\u73af\u5883\u4e2d\u4e0e\u4eba\u7c7b\u7684\u534f\u4f5c\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u65b0\u7684\u6846\u67b6 COOPERA\uff0c\u901a\u8fc7\u6a21\u62df\u76f8\u5e94\u7684\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u4e92\u52a8\uff0c\u5e76\u7ed3\u5408\u6301\u7eed\u7684\u4eba\u7c7b\u53cd\u9988\uff0c\u4ee5\u5b9e\u73b0\u4e2a\u6027\u5316\u534f\u4f5c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u62df\u7684\u4eba\u7c7b\u5728\u884c\u4e3a\u4e0a\u4e0e\u771f\u5b9e\u4eba\u7c7b\u76f8\u7b26\uff0c\u8bc1\u660e\u4e86\u57fa\u4e8e\u4eba\u7c7b\u610f\u56fe\u8fdb\u884c\u63a8\u65ad\u548c\u4e2a\u6027\u5316\u7684\u91cd\u8981\u4ef7\u503c\u3002", "conclusion": "COOPERA \u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u957f\u671f\u548c\u5f00\u653e\u5f0f\u7684\u4eba\u673a\u534f\u4f5c\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u7684\u5fc3\u7406\u7279\u8d28\u548c\u610f\u56fe\uff0c\u63d0\u5347\u673a\u5668\u4eba\u5bf9\u4e2a\u4f53\u4eba\u7c7b\u884c\u4e3a\u7684\u7406\u89e3\u4e0e\u9002\u5e94\u80fd\u529b\u3002"}}
{"id": "2510.23509", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23509", "abs": "https://arxiv.org/abs/2510.23509", "authors": ["Weizheng Wang", "Obi Ike", "Soyun Choi", "Sungeun Hong", "Byung-Cheol Min"], "title": "Deductive Chain-of-Thought Augmented Socially-aware Robot Navigation World Model", "comment": null, "summary": "Social robot navigation increasingly relies on large language models for\nreasoning, path planning, and enabling movement in dynamic human spaces.\nHowever, relying solely on LLMs for planning often leads to unpredictable and\nunsafe behaviors, especially in dynamic human spaces, due to limited physical\ngrounding and weak logical consistency. In this work, we introduce NaviWM, a\nsocially-aware robot Navigation World Model that augments LLM reasoning with a\nstructured world model and a logic-driven chain-of-thought process. NaviWM\nconsists of two main components: (1) a spatial-temporal world model that\ncaptures the positions, velocities, and activities of agents in the\nenvironment, and (2) a deductive reasoning module that guides LLMs through a\nmulti-step, logic-based inference process. This integration enables the robot\nto generate navigation decisions that are both socially compliant and\nphysically safe, under well-defined constraints such as personal space,\ncollision avoidance, and timing. Unlike previous methods based on prompting or\nfine-tuning, NaviWM encodes social norms as first-order logic, enabling\ninterpretable and verifiable reasoning. Experiments show that NaviWM improves\nsuccess rates and reduces social violations, particularly in crowded\nenvironments. These results demonstrate the benefit of combining formal\nreasoning with LLMs for robust social navigation. Additional experimental\ndetails and demo videos for this work can be found at:\nhttps://sites.google.com/view/NaviWM.", "AI": {"tldr": "NaviWM\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u5316\u7684\u4e16\u754c\u6a21\u578b\u4e0e\u903b\u8f91\u63a8\u7406\uff0c\u63d0\u5347\u793e\u4ea4\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\uff0c\u589e\u5f3a\u5b89\u5168\u6027\u548c\u793e\u4f1a\u9002\u5e94\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001\u4eba\u7c7b\u73af\u5883\u4e2d\u8fdb\u884c\u5bfc\u822a\u65f6\u5bb9\u6613\u51fa\u73b0\u4e0d\u53ef\u9884\u6d4b\u548c\u4e0d\u5b89\u5168\u7684\u884c\u4e3a\uff0c\u56e0\u6b64\u9700\u8981\u5f15\u5165\u7ed3\u6784\u5316\u6a21\u578b\u548c\u903b\u8f91\u63a8\u7406\u6765\u6539\u5584\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f15\u5165NaviWM\uff0c\u4e00\u4e2a\u793e\u4ea4\u610f\u8bc6\u7684\u673a\u5668\u4eba\u5bfc\u822a\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u4e16\u754c\u6a21\u578b\u4e0e\u903b\u8f91\u9a71\u52a8\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cNaviWM\u5728\u62e5\u6324\u73af\u5883\u4e2d\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u5e76\u51cf\u5c11\u4e86\u793e\u4f1a\u8fdd\u89c4\uff0c\u8fd9\u8bc1\u660e\u4e86\u6b63\u5f0f\u63a8\u7406\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u7684\u6548\u679c\u3002", "conclusion": "NaviWM\u6709\u6548\u5730\u6574\u5408\u4e86\u5f62\u5f0f\u5316\u63a8\u7406\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4e3a\u673a\u5668\u4eba\u793e\u4ea4\u5bfc\u822a\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u7b26\u5408\u793e\u4f1a\u89c4\u8303\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.23511", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23511", "abs": "https://arxiv.org/abs/2510.23511", "authors": ["Bin Xie", "Erjin Zhou", "Fan Jia", "Hao Shi", "Haoqiang Fan", "Haowei Zhang", "Hebei Li", "Jianjian Sun", "Jie Bin", "Junwen Huang", "Kai Liu", "Kaixin Liu", "Kefan Gu", "Lin Sun", "Meng Zhang", "Peilong Han", "Ruitao Hao", "Ruitao Zhang", "Saike Huang", "Songhan Xie", "Tiancai Wang", "Tianle Liu", "Wenbin Tang", "Wenqi Zhu", "Yang Chen", "Yingfei Liu", "Yizhuang Zhou", "Yu Liu", "Yucheng Zhao", "Yunchao Ma", "Yunfei Wei", "Yuxiang Chen", "Ze Chen", "Zeming Li", "Zhao Wu", "Ziheng Zhang", "Ziming Liu", "Ziwei Yan", "Ziyu Zhang"], "title": "Dexbotic: Open-Source Vision-Language-Action Toolbox", "comment": "Authors are listed in alphabetical order. The official website is\n  located at https://dexbotic.com/. Code is available at\n  https://github.com/Dexmal/dexbotic", "summary": "In this paper, we present Dexbotic, an open-source Vision-Language-Action\n(VLA) model toolbox based on PyTorch. It aims to provide a one-stop VLA\nresearch service for professionals in the field of embodied intelligence. It\noffers a codebase that supports multiple mainstream VLA policies\nsimultaneously, allowing users to reproduce various VLA methods with just a\nsingle environment setup. The toolbox is experiment-centric, where the users\ncan quickly develop new VLA experiments by simply modifying the Exp script.\nMoreover, we provide much stronger pretrained models to achieve great\nperformance improvements for state-of-the-art VLA policies. Dexbotic will\ncontinuously update to include more of the latest pre-trained foundation models\nand cutting-edge VLA models in the industry.", "AI": {"tldr": "Dexbotic\u662f\u4e00\u4e2a\u5f00\u6e90\u7684VLA\u6a21\u578b\u5de5\u5177\u7bb1\uff0c\u65e8\u5728\u4e3a\u667a\u80fd\u4f53\u7814\u7a76\u63d0\u4f9b\u4fbf\u5229\uff0c\u652f\u6301\u591a\u79cd\u7b56\u7565\u5e76\u4e0d\u65ad\u66f4\u65b0\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "motivation": "\u4e3a\u4f53\u73b0\u5728\u667a\u80fd\u9886\u57df\u7684\u4e13\u4e1a\u4eba\u58eb\u63d0\u4f9b\u4e00\u7ad9\u5f0fVLA\u7814\u7a76\u670d\u52a1", "method": "Dexbotic, \u4e00\u4e2a\u57fa\u4e8ePyTorch\u7684\u5f00\u6e90\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5de5\u5177\u7bb1", "result": "\u652f\u6301\u591a\u79cd\u6d41\u884c\u7684VLA\u7b56\u7565\uff0c\u7528\u6237\u53ef\u5728\u5355\u4e00\u73af\u5883\u8bbe\u7f6e\u4e0b\u91cd\u73b0\u591a\u79cdVLA\u65b9\u6cd5\uff0c\u63d0\u4f9b\u66f4\u5f3a\u5927\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u4ee5\u6539\u5584\u6027\u80fd", "conclusion": "Dexbotic\u5c06\u6301\u7eed\u66f4\u65b0\uff0c\u4ee5\u5305\u542b\u66f4\u591a\u6700\u65b0\u7684\u57fa\u7840\u6a21\u578b\u548c\u884c\u4e1a\u524d\u6cbf\u7684VLA\u6a21\u578b\u3002"}}
{"id": "2510.23512", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23512", "abs": "https://arxiv.org/abs/2510.23512", "authors": ["Martin Huber", "Nicola A. Cavalcanti", "Ayoob Davoodi", "Ruixuan Li", "Christopher E. Mower", "Fabio Carrillo", "Christoph J. Laux", "Francois Teyssere", "Thibault Chandanson", "Antoine Harl\u00e9", "Elie Saghbiny", "Mazda Farshad", "Guillaume Morel", "Emmanuel Vander Poorten", "Philipp F\u00fcrnstahl", "S\u00e9bastien Ourselin", "Christos Bergeles", "Tom Vercauteren"], "title": "Localising under the drape: proprioception in the era of distributed surgical robotic system", "comment": null, "summary": "Despite their mechanical sophistication, surgical robots remain blind to\ntheir surroundings. This lack of spatial awareness causes collisions, system\nrecoveries, and workflow disruptions, issues that will intensify with the\nintroduction of distributed robots with independent interacting arms. Existing\ntracking systems rely on bulky infrared cameras and reflective markers,\nproviding only limited views of the surgical scene and adding hardware burden\nin crowded operating rooms. We present a marker-free proprioception method that\nenables precise localisation of surgical robots under their sterile draping\ndespite associated obstruction of visual cues. Our method solely relies on\nlightweight stereo-RGB cameras and novel transformer-based deep learning\nmodels. It builds on the largest multi-centre spatial robotic surgery dataset\nto date (1.4M self-annotated images from human cadaveric and preclinical in\nvivo studies). By tracking the entire robot and surgical scene, rather than\nindividual markers, our approach provides a holistic view robust to occlusions,\nsupporting surgical scene understanding and context-aware control. We\ndemonstrate an example of potential clinical benefits during in vivo breathing\ncompensation with access to tissue dynamics, unobservable under state of the\nart tracking, and accurately locate in multi-robot systems for future\nintelligent interaction. In addition, and compared with existing systems, our\nmethod eliminates markers and improves tracking visibility by 25%. To our\nknowledge, this is the first demonstration of marker-free proprioception for\nfully draped surgical robots, reducing setup complexity, enhancing safety, and\npaving the way toward modular and autonomous robotic surgery.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65e0\u6807\u8bb0\u81ea\u6211\u611f\u77e5\u65b9\u6cd5\uff0c\u5229\u7528\u57fa\u7840\u8bbe\u65bd\u7b80\u5355\u7684\u7acb\u4f53RGB\u6444\u50cf\u5934\u548c\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5728\u5168\u8986\u76d6\u7684\u5916\u79d1\u624b\u672f\u673a\u5668\u4eba\u4e2d\u5b9e\u73b0\u4e86\u7cbe\u786e\u5b9a\u4f4d\uff0c\u51cf\u5c11\u4e86\u7cfb\u7edf\u590d\u6742\u6027\u5e76\u63d0\u5347\u4e86\u64cd\u4f5c\u5b89\u5168\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5916\u79d1\u624b\u672f\u673a\u5668\u4eba\u5728\u673a\u68b0\u4e0a\u590d\u6742\uff0c\u4f46\u4ecd\u7136\u7f3a\u4e4f\u7a7a\u95f4\u611f\u77e5\uff0c\u5bfc\u81f4\u78b0\u649e\u3001\u7cfb\u7edf\u6062\u590d\u548c\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u65ad\uff0c\u5c24\u5176\u662f\u5728\u5f15\u5165\u72ec\u7acb\u4ea4\u4e92\u7684\u5206\u5e03\u5f0f\u673a\u5668\u4eba\u540e\u95ee\u9898\u4f1a\u66f4\u52a0\u4e25\u91cd\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u6807\u8bb0\u7684\u81ea\u6211\u611f\u77e5\u65b9\u6cd5\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u7acb\u4f53RGB\u6444\u50cf\u5934\u548c\u65b0\u9896\u7684\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u5168\u9762\u8ddf\u8e2a\u6574\u4e2a\u673a\u5668\u4eba\u548c\u5916\u79d1\u573a\u666f\uff0c\u800c\u4e0d\u662f\u5355\u4e2a\u6807\u8bb0\uff0c\u63d0\u4f9b\u4e86\u6297\u906e\u6321\u7684\u5168\u666f\u89c6\u56fe\uff0c\u652f\u6301\u5916\u79d1\u573a\u666f\u7406\u89e3\u4e0e\u4e0a\u4e0b\u6587\u611f\u77e5\u63a7\u5236\uff0c\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u4e5f\u663e\u793a\u51fa\u4e86\u6f5c\u5728\u7684\u76ca\u5904\u3002", "conclusion": "\u672c\u65b9\u6cd5\u662f\u9996\u4e2a\u9488\u5bf9\u5b8c\u5168\u8986\u76d6\u5916\u79d1\u624b\u672f\u673a\u5668\u4eba\u7684\u65e0\u6807\u8bb0\u81ea\u6211\u611f\u77e5\u793a\u8303\uff0c\u51cf\u5c11\u4e86\u8bbe\u7f6e\u590d\u6742\u6027\uff0c\u63d0\u5347\u4e86\u5b89\u5168\u6027\uff0c\u5e76\u4e3a\u6a21\u5757\u5316\u548c\u81ea\u4e3b\u5916\u79d1\u624b\u672f\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.23521", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23521", "abs": "https://arxiv.org/abs/2510.23521", "authors": ["Anthony Opipari", "Aravindhan K Krishnan", "Shreekant Gayaka", "Min Sun", "Cheng-Hao Kuo", "Arnie Sen", "Odest Chadwicke Jenkins"], "title": "Explicit Memory through Online 3D Gaussian Splatting Improves Class-Agnostic Video Segmentation", "comment": "Accepted in IEEE Robotics and Automation Letters September 2025", "summary": "Remembering where object segments were predicted in the past is useful for\nimproving the accuracy and consistency of class-agnostic video segmentation\nalgorithms. Existing video segmentation algorithms typically use either no\nobject-level memory (e.g. FastSAM) or they use implicit memories in the form of\nrecurrent neural network features (e.g. SAM2). In this paper, we augment both\ntypes of segmentation models using an explicit 3D memory and show that the\nresulting models have more accurate and consistent predictions. For this, we\ndevelop an online 3D Gaussian Splatting (3DGS) technique to store predicted\nobject-level segments generated throughout the duration of a video. Based on\nthis 3DGS representation, a set of fusion techniques are developed, named\nFastSAM-Splat and SAM2-Splat, that use the explicit 3DGS memory to improve\ntheir respective foundation models' predictions. Ablation experiments are used\nto validate the proposed techniques' design and hyperparameter settings.\nResults from both real-world and simulated benchmarking experiments show that\nmodels which use explicit 3D memories result in more accurate and consistent\npredictions than those which use no memory or only implicit neural network\nmemories. Project Page: https://topipari.com/projects/FastSAM-Splat/", "AI": {"tldr": "\u901a\u8fc7\u663e\u5f0f 3D \u5185\u5b58\u5728\u89c6\u9891\u5206\u5272\u7b97\u6cd5\u4e2d\u63d0\u9ad8\u4e86\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u65e0\u7c7b\u89c6\u9891\u5206\u5272\u7b97\u6cd5\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51fa\u5728\u7ebf 3D \u9ad8\u65af\u70b9\u7684\u6280\u672f\uff083DGS\uff09\uff0c\u5e76\u5f00\u53d1\u878d\u5408\u6280\u672f FastSAM-Splat \u548c SAM2-Splat\u3002", "result": "\u5f15\u5165\u663e\u5f0f 3D \u5185\u5b58\u7684\u5206\u5272\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u3002", "conclusion": "\u4f7f\u7528\u663e\u5f0f 3D \u5185\u5b58\u7684\u6a21\u578b\u80fd\u591f\u5b9e\u73b0\u66f4\u51c6\u786e\u3001\u4e00\u81f4\u7684\u5bf9\u8c61\u9884\u6d4b\u3002"}}
{"id": "2510.23571", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23571", "abs": "https://arxiv.org/abs/2510.23571", "authors": ["Yash Jangir", "Yidi Zhang", "Kashu Yamazaki", "Chenyu Zhang", "Kuan-Hsun Tu", "Tsung-Wei Ke", "Lei Ke", "Yonatan Bisk", "Katerina Fragkiadaki"], "title": "RobotArena $\\infty$: Scalable Robot Benchmarking via Real-to-Sim Translation", "comment": "Website: https://robotarenainf.github.io", "summary": "The pursuit of robot generalists - instructable agents capable of performing\ndiverse tasks across diverse environments - demands rigorous and scalable\nevaluation. Yet real-world testing of robot policies remains fundamentally\nconstrained: it is labor-intensive, slow, unsafe at scale, and difficult to\nreproduce. Existing simulation benchmarks are similarly limited, as they train\nand test policies within the same synthetic domains and cannot assess models\ntrained from real-world demonstrations or alternative simulation environments.\nAs policies expand in scope and complexity, these barriers only intensify,\nsince defining \"success\" in robotics often hinges on nuanced human judgments of\nexecution quality. In this paper, we introduce a new benchmarking framework\nthat overcomes these challenges by shifting VLA evaluation into large-scale\nsimulated environments augmented with online human feedback. Leveraging\nadvances in vision-language models, 2D-to-3D generative modeling, and\ndifferentiable rendering, our approach automatically converts video\ndemonstrations from widely used robot datasets into simulated counterparts.\nWithin these digital twins, we assess VLA policies using both automated\nVLM-guided scoring and scalable human preference judgments collected from\ncrowdworkers, transforming human involvement from tedious scene setup,\nresetting, and safety supervision into lightweight preference comparisons. To\nmeasure robustness, we systematically perturb simulated environments along\nmultiple axes, such as textures and object placements, stress-testing policy\ngeneralization under controlled variation. The result is a continuously\nevolving, reproducible, and scalable benchmark for real-world trained robot\nmanipulation policies, addressing a critical missing capability in today's\nrobotics landscape.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u673a\u5668\u4eba\u8bc4\u4f30\u57fa\u51c6\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u5347\u673a\u5668\u4eba\u591a\u4efb\u52a1\u80fd\u529b\u7684\u6d4b\u8bd5\u6548\u7387\u4e0e\u53ef\u9760\u6027\u3002", "motivation": "\u9762\u4e34\u5f53\u524d\u673a\u5668\u4eba\u653f\u7b56\u8bc4\u4f30\u7684\u9ad8\u6210\u672c\u3001\u4f4e\u6548\u53ca\u96be\u4ee5\u91cd\u73b0\u7684\u95ee\u9898\uff0c\u672c\u6587\u5e0c\u671b\u901a\u8fc7\u65b0\u7684\u65b9\u6cd5\u63a8\u52a8\u673a\u5668\u4eba\u6280\u672f\u7684\u6d4b\u8bd5\u4e0e\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u81ea\u52a8\u8f6c\u6362\u89c6\u9891\u793a\u8303\u4e3a\u6a21\u62df\u5bf9\u8c61\uff0c\u5e76\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6307\u5bfc\u7684\u81ea\u52a8\u8bc4\u5206\u7ed3\u5408\u4eba\u7c7b\u504f\u597d\u5224\u65ad\uff0c\u8bc4\u4f30\u673a\u5668\u4eba\u653f\u7b56\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u5c06VLA\u8bc4\u4f30\u8f6c\u79fb\u5230\u589e\u5f3a\u4e86\u5728\u7ebf\u4eba\u7c7b\u53cd\u9988\u7684\u5927\u89c4\u6a21\u6a21\u62df\u73af\u5883\u4e2d\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u673a\u5668\u4eba\u8bc4\u4f30\u9762\u4e34\u7684\u79cd\u79cd\u6311\u6218\u3002", "conclusion": "\u672c\u7814\u7a76\u7684\u57fa\u51c6\u6846\u67b6\u901a\u8fc7\u7cfb\u7edf\u5730\u64cd\u63a7\u6a21\u62df\u73af\u5883\u5e76\u5f15\u5165\u4eba\u7c7b\u504f\u597d\u8bc4\u5224\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u91cd\u590d\u3001\u53ef\u6269\u5c55\u7684\u673a\u5668\u4eba\u64cd\u63a7\u653f\u7b56\u8bc4\u4f30\u65b9\u6848\u3002"}}
{"id": "2510.23576", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23576", "abs": "https://arxiv.org/abs/2510.23576", "authors": ["Anqi Li", "Zhiyong Wang", "Jiazhao Zhang", "Minghan Li", "Yunpeng Qi", "Zhibo Chen", "Zhizheng Zhang", "He Wang"], "title": "UrbanVLA: A Vision-Language-Action Model for Urban Micromobility", "comment": null, "summary": "Urban micromobility applications, such as delivery robots, demand reliable\nnavigation across large-scale urban environments while following long-horizon\nroute instructions. This task is particularly challenging due to the dynamic\nand unstructured nature of real-world city areas, yet most existing navigation\nmethods remain tailored to short-scale and controllable scenarios. Effective\nurban micromobility requires two complementary levels of navigation skills:\nlow-level capabilities such as point-goal reaching and obstacle avoidance, and\nhigh-level capabilities, such as route-visual alignment. To this end, we\npropose UrbanVLA, a route-conditioned Vision-Language-Action (VLA) framework\ndesigned for scalable urban navigation. Our method explicitly aligns noisy\nroute waypoints with visual observations during execution, and subsequently\nplans trajectories to drive the robot. To enable UrbanVLA to master both levels\nof navigation, we employ a two-stage training pipeline. The process begins with\nSupervised Fine-Tuning (SFT) using simulated environments and trajectories\nparsed from web videos. This is followed by Reinforcement Fine-Tuning (RFT) on\na mixture of simulation and real-world data, which enhances the model's safety\nand adaptability in real-world settings. Experiments demonstrate that UrbanVLA\nsurpasses strong baselines by more than 55% in the SocialNav task on MetaUrban.\nFurthermore, UrbanVLA achieves reliable real-world navigation, showcasing both\nscalability to large-scale urban environments and robustness against real-world\nuncertainties.", "AI": {"tldr": "UrbanVLA\u662f\u4e00\u4e2a\u4e3a\u57ce\u5e02\u5fae\u4ea4\u901a\u8bbe\u8ba1\u7684\u5bfc\u822a\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u89c6\u89c9\u3001\u8bed\u8a00\u548c\u884c\u52a8\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u63d0\u9ad8\u4e86\u5bfc\u822a\u80fd\u529b\u4e0e\u5b89\u5168\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5bfc\u822a\u65b9\u6cd5\u5728\u52a8\u6001\u975e\u7ed3\u6784\u5316\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u673a\u5668\u4eba\u7684\u957f\u8fdc\u5bfc\u822a\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7ba1\u9053\uff0c\u9996\u5148\u8fdb\u884c\u6a21\u62df\u73af\u5883\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u7136\u540e\u5728\u6df7\u5408\u6570\u636e\u4e0a\u8fdb\u884c\u5f3a\u5316\u5fae\u8c03\u3002", "result": "UrbanVLA\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u5bfc\u822a\uff0c\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "UrbanVLA\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\uff0c\u5728\u590d\u6742\u7684\u57ce\u5e02\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u80fd\u591f\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u53ef\u9760\u5bfc\u822a\u3002"}}
