<div id=toc></div>

# Table of Contents

- [cs.HC](#cs.HC) [Total: 12]
- [cs.RO](#cs.RO) [Total: 31]


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [1] [A Platform for Interactive AI Character Experiences](https://arxiv.org/abs/2601.01027)
*Rafael Wampfler,Chen Yang,Dillon Elste,Nikola Kovacevic,Philine Witzig,Markus Gross*

Main category: cs.HC

TL;DR: 本文介绍了一个平台，旨在创建逼真的互动数字角色，尤其是以爱因斯坦为例，展示如何实现故事驱动的对话体验。


<details>
  <summary>Details</summary>
Motivation: 将人物角色融入互动故事驱动的对话中，吸引了各代人的想象，但实现这一愿景面临重大挑战。

Method: 结合基础模型的发展、提示工程和下游任务微调，提出了一种整合多种AI技术的系统，允许用户与数字化的爱因斯坦进行互动对话。

Result: 提出一个系统和平台，方便设计可信的数字角色，提供对话和故事驱动的体验，同时解决所有技术挑战。

Conclusion: 通过将各种AI组件统一到一个易于适应的平台上，我们的工作为沉浸式角色体验铺平了道路，使栩栩如生的故事互动成为现实。

Abstract: From movie characters to modern science fiction - bringing characters into interactive, story-driven conversations has captured imaginations across generations. Achieving this vision is highly challenging and requires much more than just language modeling. It involves numerous complex AI challenges, such as conversational AI, maintaining character integrity, managing personality and emotions, handling knowledge and memory, synthesizing voice, generating animations, enabling real-world interactions, and integration with physical environments. Recent advancements in the development of foundation models, prompt engineering, and fine-tuning for downstream tasks have enabled researchers to address these individual challenges. However, combining these technologies for interactive characters remains an open problem. We present a system and platform for conveniently designing believable digital characters, enabling a conversational and story-driven experience while providing solutions to all of the technical challenges. As a proof-of-concept, we introduce Digital Einstein, which allows users to engage in conversations with a digital representation of Albert Einstein about his life, research, and persona. While Digital Einstein exemplifies our methods for a specific character, our system is flexible and generalizes to any story-driven or conversational character. By unifying these diverse AI components into a single, easy-to-adapt platform, our work paves the way for immersive character experiences, turning the dream of lifelike, story-based interactions into a reality.

</details>


### [2] [SoulSeek: Exploring the Use of Social Cues in LLM-based Information Seeking](https://arxiv.org/abs/2601.01094)
*Yubo Shu,Peng Zhang,Meng Wu,Yan Chen,Haoxuan Zhou,Guanming Liu,Yu Zhang,Liuxin Zhang,Qianying Wang,Tun Lu,Ning Gu*

Main category: cs.HC

TL;DR: 研究社交线索在LLM基础搜索中的作用，结果显示社交线索改善了用户体验和信息行为


<details>
  <summary>Details</summary>
Motivation: 探讨现有LLM基础搜索系统与人类自然信息搜索中的社会认知之间的错位

Method: 通过设计研讨会、原型系统实现和被试间研究，以及混合方法分析，研究社交线索在LLM基础搜索中的影响

Result: 社交线索提升了用户的感知结果和体验，促进反思性信息行为，并揭示了当前LLM基础搜索的局限性

Conclusion: 提出改善社交知识理解、个性化线索设置和可控交互的设计建议。

Abstract: Social cues, which convey others' presence, behaviors, or identities, play a crucial role in human information seeking by helping individuals judge relevance and trustworthiness. However, existing LLM-based search systems primarily rely on semantic features, creating a misalignment with the socialized cognition underlying natural information seeking. To address this gap, we explore how the integration of social cues into LLM-based search influences users' perceptions, experiences, and behaviors. Focusing on social media platforms that are beginning to adopt LLM-based search, we integrate design workshops, the implementation of the prototype system (SoulSeek), a between-subjects study, and mixed-method analyses to examine both outcome- and process-level findings. The workshop informs the prototype's cue-integrated design. The study shows that social cues improve perceived outcomes and experiences, promote reflective information behaviors, and reveal limits of current LLM-based search. We propose design implications emphasizing better social-knowledge understanding, personalized cue settings, and controllable interactions.

</details>


### [3] [MotiBo: The Impact of Interactive Digital Storytelling Robots on Student Motivation through Self-Determination Theory](https://arxiv.org/abs/2601.01218)
*Ka Yan Fung,Tze Leung Rick Lui,Yuxing Tao,Kuen Fung Sin*

Main category: cs.HC

TL;DR: 本研究通过比较不同叙事方式，探讨了互动数字叙事系统对学生参与度和创造力的影响，结果发现机器人辅助叙事能显著提升学习效果。


<details>
  <summary>Details</summary>
Motivation: 在教育中越来越重视创造力，而传统叙事方法缺乏足够的互动元素来吸引学生。

Method: 采用准实验设计，比较三类叙事方法（纸质、PowerPoint和机器人辅助叙事MotiBo）对学生参与度的影响。

Result: 使用MotiBo的学生在行为和认知参与度上相比传统方法有显著改善。

Conclusion: 整合新技术能够有效提升学习体验，促进教育环境中的创造力和自我学习能力。

Abstract: Creativity is increasingly recognized as an important skill in education, and storytelling can enhance motivation and engagement among students. However, conventional storytelling methods often lack the interactive elements necessary to engage students. To this end, this study examines the impact of an interactive digital storytelling system incorporating a human-like robot on student engagement and creativity. The study aims to compare engagement levels across three modalities: paper-based, PowerPoint, and robot-assisted storytelling, MotiBo. Utilizing a quasi-experimental design, this work involves three groups of students who interact with the storytelling system over a five-day learning. Findings reveal that students using MotiBo exhibit statistically significant improvement in behavioural and cognitive engagement compared to those using traditional methods. These results suggest that the integration of novel technologies can effectively enhance the learning experience, ultimately promoting creativity and self-learning ability in educational settings. Future research will investigate the long-term effects of these technologies on learning outcomes and explore their potential for broader applications in diverse educational contexts.

</details>


### [4] [LiveBo: Empowering Non-Chinese Speaking Students through AI-Driven Real-Life Scenarios in Cantonese](https://arxiv.org/abs/2601.01227)
*Ka Yan Fung,Kwong Chiu Fung,Yuxing Tao,Tze Leung Rick Lui,Kuen Fung Sin*

Main category: cs.HC

TL;DR: 本研究探讨了将现实场景模拟与互动社交机器人相结合的语言学习方法对非中文母语学生的参与度和学习成就的影响，取得积极效果。


<details>
  <summary>Details</summary>
Motivation: 研究指出，语言学习过程复杂，不充足的词汇量会导致沟通障碍和学习动机下降。特别是对于非中文母语(NCS)学生，学习粤语面临诸多挑战。

Method: 采用准实验设计，让NCS学生与机器人辅助的语言学习系统进行交互，通过评估测试、问卷和半结构化访谈收集数据。

Result: 通过使用基于AI的机器人辅助语言学习系统LiveBo，NCS学生在行为和情感参与、动机与学习成果上获得积极改善。

Conclusion: 研究表明，互动和沉浸式学习体验在提升NCS学生的动机和语言学习效果方面具有重要意义。

Abstract: Language learning is a multifaceted process. Insufficient vocabulary can hinder communication and lead to demotivation. For non-Chinese speaking (NCS) students, learning Traditional Chinese (Cantonese) poses distinct challenges, particularly due to the complexity of converting spoken and written forms. To address this issue, this study examines the effectiveness of real-life scenario simulations integrated with interactive social robots in enhancing NCS student engagement and language acquisition. The research employs a quasi-experimental design involving NCS students who interact with an AI-driven, robot-assisted language learning system, LiveBo. The study aims to assess the impact of this innovative approach on active participation and motivation. Data are collected through proficiency tests, questionnaires and semi-structured interviews. Findings indicate that NCS students experience positive improvements in behavioural and emotional engagement, motivation and learning outcomes, highlighting the potential of integrating novel technologies in language education. We plan to compare with the control group in the future. This study highlights the significance of interactive and immersive learning experiences in promoting motivation and enhancing language acquisition among NCS students.

</details>


### [5] [Human-Centered Artificial Intelligence (HCAI): Foundations and Approaches](https://arxiv.org/abs/2601.01247)
*Wei Xu*

Main category: cs.HC

TL;DR: 人类中心的人工智能（HCAI）作为一种设计理念和方法论应运而生，聚焦人类利益，力求使人工智能技术服务于人类福祉，提供这一领域的综合框架和方法支持。


<details>
  <summary>Details</summary>
Motivation: 回应人工智能带来的双重挑战，强调人类的中心地位，以确保AI技术为人类服务，增进社会福祉。

Method: 通过分析HCAI的演变，介绍其关键概念、原则、方法和实践策略，探讨人工智能技术的特征与挑战。

Result: 本章确立了以人类为中心的人工智能（HCAI）的概念和方法基础，追溯其演变与最新进展，介绍了关键概念、框架和方法，并分析了人工智能技术面临的挑战，将HCAI定位为一种整体范式，旨在将人工智能创新与人类价值、社会福祉及可持续发展相结合。

Conclusion: 本章为《人类中心的人工智能手册》的内容奠定了理论和实践基础，促进HCAI原则与操作实施之间的联系。

Abstract: Artificial Intelligence (AI) is a transformative yet double-edged technology that can advance human welfare while also posing risks to humans and society. In response, the Human-Centered Artificial Intelligence (HCAI) approach has emerged as both a design philosophy and a methodological complement to prevailing technology-centered AI paradigms. Placing humans at the core, HCAI seeks to ensure that AI systems serve, augment, and empower humans rather than harm or replace them. This chapter establishes the conceptual and methodological foundations of HCAI by tracing its evolution and recent advancements. It introduces key HCAI concepts, frameworks, guiding principles, methodologies, and practical strategies that bridge philosophical HCAI principles with operational implementation. Through an analytical review of the emerging characteristics and challenges of AI technologies, the chapter positions HCAI as a holistic paradigm for aligning AI innovation with human values, societal well-being, and sustainable progress. Finally, this chapter outlines the structure and contributions of the Handbook of Human-Centered Artificial Intelligence. The purpose of this chapter is to provide an integrated foundation that connects HCAI conceptual frameworks, principles, methodology, and practices for this handbook, thereby paving the way for the content of subsequent chapters.

</details>


### [6] [Neural Digital Twins: Toward Next-Generation Brain-Computer Interfaces](https://arxiv.org/abs/2601.01539)
*Mohammad Mahdi Habibi Bina,Sepideh Baghernezhad,Mohammad Reza Daliri,Mohammad Hassan Moradi*

Main category: cs.HC

TL;DR: 神经数字双胞胎（NDT）是一种动态、个性化的计算模型，可以实时更新神经数据，以优化脑机接口的控制命令和解码算法。


<details>
  <summary>Details</summary>
Motivation: 解决BCI的频繁重校准、实时处理延迟、个性化不足等挑战，从而提升其长期可用性和用户体验。

Method: NDT通过实时神经数据更新，利用人工智能和神经科学数据采集技术，实现脑状态预测和控制命令优化。

Result: 本文提出了神经数字双胞胎（NDT）的概念，旨在解决当前脑机接口（BCI）面临的多个根本性挑战，并提高BCI的准确性、稳定性和长期可用性。

Conclusion: NDT有望通过提高神经技术的精确性、稳健性和个性化控制，推动下一代脑机接口和神经解码的应用。

Abstract: Current neural interfaces such as brain-computer interfaces (BCIs) face several fundamental challenges, including frequent recalibration due to neuroplasticity and session-to-session variability, real-time processing latency, limited personalization and generalization across subjects, hardware constraints, surgical risks in invasive systems, and cognitive burden in patients with neurological impairments. These limitations significantly affect the accuracy, stability, and long-term usability of BCIs. This article introduces the concept of the Neural Digital Twin (NDT) as an advanced solution to overcome these barriers. NDT represents a dynamic, personalized computational model of the brain-BCI system that is continuously updated with real-time neural data, enabling prediction of brain states, optimization of control commands, and adaptive tuning of decoding algorithms. The design of NDT draws inspiration from the application of Digital Twin technology in advanced industries such as aerospace and autonomous vehicles, and leverages recent advances in artificial intelligence and neuroscience data acquisition technologies. In this work, we discuss the structure and implementation of NDT and explore its potential applications in next-generation BCIs and neural decoding, highlighting its ability to enhance precision, robustness, and individualized control in neurotechnology.

</details>


### [7] [EdgeSSVEP: A Fully Embedded SSVEP BCI Platform for Low-Power Real-Time Applications](https://arxiv.org/abs/2601.01772)
*Manh-Dat Nguyen,Thomas Do,Nguyen Thanh Trung Le,Xuan-The Tran,Fred Chang,Chin-Teng Lin*

Main category: cs.HC

TL;DR: EdgeSSVEP 是一个低功耗的嵌入式 SSVE脑机接口系统，具备实时 EEG 处理功能，分类准确率高达 99.17%，适用于辅助通信和神经反馈。


<details>
  <summary>Details</summary>
Motivation: 为了克服传统脑机接口在实际应用中的体积和能源消耗问题，开发出一个紧凑且高效的 BCI 解决方案。

Method: 采用微控制器实现实时 EEG 获取、零相位滤波和分类，整合运动传感以增强鲁棒性。

Result: EdgeSSVEP 是一个基于微控制器的嵌入式稳态视觉诱发电位 (SSVEP) 脑机接口 (BCI) 平台，具备实时 EEG 获取、零相位滤波和在设备上分类的功能。它在仅耗电 222 毫瓦的 240 MHz MCU 上运行，集成了 8 通道 EEG 前端，支持 5 秒的刺激持续时间，完全本地执行 SSVEP 解码流程，消除了对 PC 处理的依赖。该平台在 10 名参与者的测试中实现了 99.17% 的分类准确率和 27.33 位/分钟的信息传输率 (ITR)，且功耗远低于传统桌面系统。EdgeSSVEP 还集成了运动传感器以支持伪影检测，增强了在实际环境中的鲁棒性和信号稳定性。此外，该系统为开发和调试提供了可选的 TCP 数据流功能，以便向外部客户发送数据。整体而言，EdgeSSVEP 提供了一个可扩展、节能和安全的嵌入式 BCI 平台，适用于辅助通信和神经反馈应用，并具有针对加速度计的伪影缓解和更广泛的现实世界部署的潜力。

Conclusion: EdgeSSVEP 提供了一个可扩展和高效的 BCI 平台，具有实际应用的潜力。

Abstract: Brain-Computer Interfaces (BCIs) enable users to interact with machines directly via neural activity, yet their real-world deployment is often hindered by bulky and powerhungry hardware. We present EdgeSSVEP, a fully embedded microcontroller-based Steady-State Visually Evoked Potential (SSVEP) BCI platform that performs real-time EEG acquisition, zero-phase filtering, and on-device classification within a lowpower 240 MHz MCU operating at only 222 mW. The system incorporates an 8-channel EEG front end, supports 5-second stimulus durations, and executes the entire SSVEP decoding pipeline locally, eliminating dependence on PC-based processing. EdgeSSVEP was evaluated using six stimulus frequencies (7, 8, 9, 11, 7.5, and 8.5 Hz) with 10 participants. The device achieved 99.17% classification accuracy and 27.33 bits/min Information Transfer Rate (ITR), while consuming substantially less power than conventional desktop-based systems. The system integrates motion sensing to support artifact detection and improve robustness and signal stability in practical environments. For development and debugging, the system also provides optional TCP data streaming to external clients. Overall, EdgeSSVEP offers a scalable, energy-efficient, and secure embedded BCI platform suitable for assistive communication and neurofeedback applications, with potential extensions to accelerometer-based artifact mitigation and broader real-world deployments.

</details>


### [8] [EyeLiveMetrics: Real-time Analysis of Online Reading with Eye Tracking](https://arxiv.org/abs/2601.02044)
*Daniel Hienert,Heiko Schmidt,Thomas Krämer,Dagmar Kern*

Main category: cs.HC

TL;DR: EyeLiveMetrics插件解决了现有眼动追踪软件在在线阅读分析中的局限，提供实时的眼动数据与文本的关联，支持研究和实时应用。


<details>
  <summary>Details</summary>
Motivation: 现有的眼动追踪软件在在线阅读监测方面存在局限，无法实时分析阅读行为，且无法将注视数据与网页内容建立内在联系。

Method: 开发了一款浏览器插件EyeLiveMetrics，自动将原始注视坐标实时映射到网页文本，并即时计算和存储注视、扫视及阅读指标。

Result: EyeLiveMetrics实时计算和存储阅读相关指标，经过比较评估验证其有效性。

Conclusion: EyeLiveMetrics是一款灵活的浏览器插件，能够实时追踪和分析在线阅读行为，为研究实验和实时应用提供了新的测量方式。

Abstract: Existing eye tracking software have certain limitations, especially with respect to monitoring reading online: (1) Most eye tracking software record eye tracking data as raw coordinates and stimuli as screen images/videos, but without inherent links between both. Analysts must draw areas of interest (AOIs) on webpage text for more fine-grained reading analysis. (2) The computation and analysis of fixation and reading metrics are done after the experiment and thus cannot be used for live applications. We present EyeLiveMetrics, a browser plugin that automatically maps raw gaze coordinates to text in real time. The plugin instantly calculates, stores, and provides fixation, saccade, and reading measures on words and paragraphs so that gaze behavior can be analyzed immediately. We also discuss the results of a comparative evaluation. EyeLiveMetrics offers a flexible way to measure reading on the web - for research experiments and live applications.

</details>


### [9] [Escaping the Filter Bubble: Evaluating Electroencephalographic Theta Band Synchronization as Indicator for Selective Exposure in Online News Reading](https://arxiv.org/abs/2601.02047)
*Thomas Krämer,Daniel Hienert,Francesco Chiossi,Thomas Kosch,Dagmar Kern*

Main category: cs.HC

TL;DR: 研究探讨了通过EEG和眼动追踪识别在线新闻选择性暴露的方法，发现能够帮助未来系统推荐多元信息。


<details>
  <summary>Details</summary>
Motivation: 用户倾向于选择符合自己信念的信息，从而产生过滤泡沫并限制多元视角。

Method: 通过实验收集参与者的EEG和眼动数据，分析他们对在线新闻的认同感及其与选择性暴露的关系。

Result: 通过实验验证了脑电图（EEG）和眼动追踪作为选择性暴露指标的有效性，发现参与者与新闻的认同感与脑电波的theta波功率正相关。

Conclusion: 未来的互动系统可以通过EEG和眼动追踪感知选择性暴露，提议更均衡的信息饮食。

Abstract: Selective exposure to online news occurs when users favor information that confirms their beliefs, creating filter bubbles and limiting diverse perspectives. Interactive systems can counter this by recommending different perspectives, but to achieve this, they need a real-time metric for selective exposure. We present an experiment where we evaluate Electroencephalography (EEG) and eye tracking as indicators for selective exposure by using eye tracking to recognize which textual parts participants read and using EEG to quantify the magnitude of selective exposure. Participants read online news while we collected EEG and eye movements with their agreement towards the news. We show that the agreement with news correlates positively with the theta band power in the parietal area. Our results indicate that future interactive systems can sense selective exposure using EEG and eye tracking to propose a more balanced information diet. This work presents an integrated experimental setup that identifies selective exposure using gaze and EEG-based metrics.

</details>


### [10] [Realistic adversarial scenario generation via human-like pedestrian model for autonomous vehicle control parameter optimisation](https://arxiv.org/abs/2601.02082)
*Yueyang Wang,Mehmet Dogar,Gustav Markkula*

Main category: cs.HC

TL;DR: 研究提出了一种新的行人模型，用于生成更具现实感的对抗场景，从而优化自动驾驶车辆的控制。


<details>
  <summary>Details</summary>
Motivation: 安全地部署自动驾驶车辆需要与其他道路用户（特别是行人）可靠互动，但直接在公共道路上进行测试既昂贵又不安全，因此探索仿真方法显得尤为重要。

Method: 采用认知启发的行人模型，通过模拟生成行为合理的对抗情境，以便在闭环测试中优化自动驾驶车辆控制器。

Result: 本文提出了一种基于认知启发的行人模型，在仿真测试中生成行为上合理的对抗情境，促进了自动驾驶车辆（AVs）控制的优化，并强调了人类变异性在AV测试中的重要性。

Conclusion: 通过将人类模型纳入仿真测试，能够提高自动驾驶车辆评估的可信度，并为基于行为的控制器优化提供实用基础。

Abstract: Autonomous vehicles (AVs) are rapidly advancing and are expected to play a central role in future mobility. Ensuring their safe deployment requires reliable interaction with other road users, not least pedestrians. Direct testing on public roads is costly and unsafe for rare but critical interactions, making simulation a practical alternative. Within simulation-based testing, adversarial scenarios are widely used to probe safety limits, but many prioritise difficulty over realism, producing exaggerated behaviours which may result in AV controllers that are overly conservative. We propose an alternative method, instead using a cognitively inspired pedestrian model featuring both inter-individual and intra-individual variability to generate behaviourally plausible adversarial scenarios. We provide a proof of concept demonstration of this method's potential for AV control optimisation, in closed-loop testing and tuning of an AV controller. Our results show that replacing the rule-based CARLA pedestrian with the human-like model yields more realistic gap acceptance patterns and smoother vehicle decelerations. Unsafe interactions occur only for certain pedestrian individuals and conditions, underscoring the importance of human variability in AV testing. Adversarial scenarios generated by this model can be used to optimise AV control towards safer and more efficient behaviour. Overall, this work illustrates how incorporating human-like road user models into simulation-based adversarial testing can enhance the credibility of AV evaluation and provide a practical basis to behaviourally informed controller optimisation.

</details>


### [11] [LocoScooter: Designing a Stationary Scooter-Based Locomotion System for Navigation in Virtual Reality](https://arxiv.org/abs/2601.02167)
*Wei He,Xiang Li,Per Ola Kristensson,Ge Lin Kan*

Main category: cs.HC

TL;DR: LocoScooter是一个低成本的虚拟移动接口，结合了脚滑和把手转向，提升了虚拟现实中的沉浸体验。


<details>
  <summary>Details</summary>
Motivation: 在虚拟现实中，特别是在空间有限的环境中，传统的行走方式存在挑战。

Method: 通过在一个包含14名参与者的实验中，比较了LocoScooter与传统操纵杆导航的效果。

Result: LocoScooter显著提高了沉浸感、乐趣和身体参与感，并与操纵杆导航相比，效率和可用性相当。

Conclusion: 尽管物理需求较高，用户没有报告疲劳感，表明熟悉的动作可以丰富虚拟现实导航体验。

Abstract: Virtual locomotion remains a challenge in VR, especially in space-limited environments where room-scale walking is impractical. We present LocoScooter, a low-cost, deployable locomotion interface combining foot-sliding on a compact treadmill with handlebar steering inspired by scooter riding. Built from commodity hardware, it supports embodied navigation through familiar, physically engaging movement. In a within-subject study (N = 14), LocoScooter significantly improved immersion, enjoyment, and bodily involvement over joystick navigation, while maintaining comparable efficiency and usability. Despite higher physical demand, users did not report increased fatigue, suggesting familiar movements can enrich VR navigation.

</details>


### [12] [Cooperation in Virtual Reality: Exploring Environmental Decision-Making through a Real-Effort Threshold Public Goods Game](https://arxiv.org/abs/2601.02214)
*Manuela Chessa,Michela Chessa,Lorenzo Gerini,Matteo Martini,Kaloyana Naneva,Fabio Solari*

Main category: cs.HC

TL;DR: 本研究探讨了虚拟现实中的头像社交表现如何影响阈值集体行动问题中的协调，发现简短的头像暴露能显著提高参与者的共同在场感和协调效果，进而增加社会福利。


<details>
  <summary>Details</summary>
Motivation: 随着数字平台的普及，如何协调地理分散用户的集体行动日益复杂，尤其是在需要关键参与人数的阈值情境中。

Method: 通过188名参与者的随机对照实验，比较了两种交互设置下的相应结果：在任务前与头像交互和在无头像交互的情况下单独完成任务。

Result: 研究发现，最小的头像暴露显著提高了感觉上的共同在场感，并改善了参与者的协调结果，参与者在此条件下成功优化了社会福利。

Conclusion: 简短的头像暴露可以有效提升用户的共同在场感和战略协调能力，增强集体行动的效益，尤其在可持续性倡议中具有实际应用价值。

Abstract: Digital platforms increasingly support collective action initiatives, yet coordinating geographically dispersed users through digital interfaces remains challenging, particularly in threshold settings where success requires critical mass participation. This study investigates how avatar-based social representation in Virtual Reality (VR) influences coordination in threshold collective action problems. Through a randomized controlled experiment with 188 participants organized in 94 pairs, we examine whether brief avatar exposure affects perceived co-presence and coordination outcomes in a two-player threshold public goods game implemented as a real-effort recycling task. We manipulate a single design feature: participants either briefly interact through avatars before the main task (Pre-Task Avatar treatment) or complete an equivalent activity individually without peer visibility (No Pre-Task Avatar treatment). Our findings reveal that minimal avatar exposure significantly increases perceived co-presence and improves strategic coordination, though not through increased contribution quantity. Participants exposed to peer avatars achieve higher social welfare by coordinating to avoid wasteful over-contribution beyond the threshold. Additionally, we identify VR presence-the sense of 'being there' in the virtual environment-as a stronger predictor of task performance than co-presence itself. This research contributes to Information Systems theory by establishing causal pathways from specific design features to presence to coordination outcomes, demonstrates VR as a rigorous experimental methodology for IS research, and provides actionable insights for designing collaborative platforms supporting sustainability initiatives and threshold collective action problems.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [13] [Value Vision-Language-Action Planning & Search](https://arxiv.org/abs/2601.00969)
*Ali Salamatian,Ke,Ren,Kieran Pattison,Cyrus Neary*

Main category: cs.RO

TL;DR: V-VLAPS框架通过增强MCTS的搜索能力，改进了机器人操作的成功率与效率。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在行为克隆过程中存在局限性，对分布转移敏感，因此需要引入基于值的规划与搜索方式。

Method: V-VLAPS结合了负载固定的VLA骨架的潜在表示，通过训练多层感知器(MLP)建立可学习的值函数来指导搜索。

Result: 提出了一种名为V-VLAPS的框架，通过引入轻量级的可学习值函数来增强MCTS，从而解决VLA政策在分布转移下的脆弱性。

Conclusion: V-VLAPS为机器人操作提供了一种更有效的决策策略，提高了成功率并减少了模拟次数。

Abstract: Vision-Language-Action (VLA) models have emerged as powerful generalist policies for robotic manipulation, yet they remain fundamentally limited by their reliance on behavior cloning, leading to brittleness under distribution shift. While augmenting pretrained models with test-time search algorithms like Monte Carlo Tree Search (MCTS) can mitigate these failures, existing formulations rely solely on the VLA prior for guidance, lacking a grounded estimate of expected future return. Consequently, when the prior is inaccurate, the planner can only correct action selection via the exploration term, which requires extensive simulation to become effective. To address this limitation, we introduce Value Vision-Language-Action Planning and Search (V-VLAPS), a framework that augments MCTS with a lightweight, learnable value function. By training a simple multilayer perceptron (MLP) on the latent representations of a fixed VLA backbone (Octo), we provide the search with an explicit success signal that biases action selection toward high-value regions. We evaluate V-VLAPS on the LIBERO robotic manipulation suite, demonstrating that our value-guided search improves success rates by over 5 percentage points while reducing the average number of MCTS simulations by 5-15 percent compared to baselines that rely only on the VLA prior.

</details>


### [14] [From Perception to Symbolic Task Planning: Vision-Language Guided Human-Robot Collaborative Structured Assembly](https://arxiv.org/abs/2601.00978)
*Yanyi Chen,Min Deng*

Main category: cs.RO

TL;DR: 提出一种面向人类的计划框架以增强人机协作的状态估计和任务规划。


<details>
  <summary>Details</summary>
Motivation: 解决结构装配中因噪声感知和人类干预导致的可靠状态估计和自适应任务规划的挑战。

Method: 通过两个模块进行设计：PSS模块使用视觉语言模型处理观测并生成符号状态，HPR模块进行多机器人任务分配和动态计划更新。

Result: PSS模块达到97%的状态合成准确率，HPR模块在多种HRC场景中保持任务可行性。

Conclusion: VLM与知识驱动的规划相结合，提高了动态条件下的状态估计和任务规划的稳健性。

Abstract: Human-robot collaboration (HRC) in structured assembly requires reliable state estimation and adaptive task planning under noisy perception and human interventions. To address these challenges, we introduce a design-grounded human-aware planning framework for human-robot collaborative structured assembly. The framework comprises two coupled modules. Module I, Perception-to-Symbolic State (PSS), employs vision-language models (VLMs) based agents to align RGB-D observations with design specifications and domain knowledge, synthesizing verifiable symbolic assembly states. It outputs validated installed and uninstalled component sets for online state tracking. Module II, Human-Aware Planning and Replanning (HPR), performs task-level multi-robot assignment and updates the plan only when the observed state deviates from the expected execution outcome. It applies a minimal-change replanning rule to selectively revise task assignments and preserve plan stability even under human interventions. We validate the framework on a 27-component timber-frame assembly. The PSS module achieves 97% state synthesis accuracy, and the HPR module maintains feasible task progression across diverse HRC scenarios. Results indicate that integrating VLM-based perception with knowledge-driven planning improves robustness of state estimation and task planning under dynamic conditions.

</details>


### [15] [Simulations of MRI Guided and Powered Ferric Applicators for Tetherless Delivery of Therapeutic Interventions](https://arxiv.org/abs/2601.00981)
*Wenhui Chu,Khang Tran,Nikolaos V. Tsekos*

Main category: cs.RO

TL;DR: 研究提出了一个用于MRI驱动的应用装置在血管内安全操作的计算平台，结合了MRI扫描、数据处理和虚拟固定区域生成。


<details>
  <summary>Details</summary>
Motivation: 探索MRI在术前规划和术中指导中的应用，尤其是在血管内干预中使用MRI机器人技术。

Method: 基于Qt框架，构建一个双向数据和指令管道，处理多切片MRI数据，生成虚拟通道，计算磁场梯度波形，并评估血管路径的安全性。

Result: 提出了一个计算平台，用于术前规划和MRI驱动的导管在血管内的建模，提供了安全操作的指导。

Conclusion: 该平台为未来实验研究提供了实时操作的可能性，增强了MRI机器人技术在临床应用中的安全性和有效性。

Abstract: Magnetic Resonance Imaging (MRI) is a well-established modality for pre-operative planning and is also explored for intra-operative guidance of procedures such as intravascular interventions. Among the experimental robot-assisted technologies, the magnetic field gradients of the MRI scanner are used to power and maneuver ferromagnetic applicators for accessing sites in the patient's body via the vascular network. In this work, we propose a computational platform for preoperative planning and modeling of MRI-powered applicators inside blood vessels. This platform was implemented as a two-way data and command pipeline that links the MRI scanner, the computational core, and the operator. The platform first processes multi-slice MR data to extract the vascular bed and then fits a virtual corridor inside the vessel. This corridor serves as a virtual fixture (VF), a forbidden region for the applicators to avoid vessel perforation or collision. The geometric features of the vessel centerline, the VF, and MRI safety compliance (dB/dt, max available gradient) are then used to generate magnetic field gradient waveforms. Different blood flow profiles can be user-selected, and those parameters are used for modeling the applicator's maneuvering. The modeling module further generates cues about whether the selected vascular path can be safely maneuvered. Given future experimental studies that require a real-time operation, the platform was implemented on the Qt framework (C/C++) with software modules performing specific tasks running on dedicated threads: PID controller, generation of VF, generation of MR gradient waveforms.

</details>


### [16] [Topological Mapping and Navigation using a Monocular Camera based on AnyLoc](https://arxiv.org/abs/2601.01067)
*Wenzheng Zhang,Yoshitaka Hara,Sousuke Nakamura*

Main category: cs.RO

TL;DR: 提出了一种基于单目相机的拓扑地图构建和导航方法，显著提高成功率并减少资源消耗。


<details>
  <summary>Details</summary>
Motivation: 简化路径规划和导航，通过用关键节点而非精确坐标表示环境，提供快速的地图构建和导航解决方案。

Method: 使用单目相机进行拓扑地图构建和导航的方法

Result: 在真实和仿真环境中，成功的闭环检测和导航实验，且较ResNet方法提高了60.2%的成功率，同时降低了时间和空间成本。

Conclusion: 该方法为机器人和人类在各种场景中的导航提供了一种轻量级解决方案。

Abstract: This paper proposes a method for topological mapping and navigation using a monocular camera. Based on AnyLoc, keyframes are converted into descriptors to construct topological relationships, enabling loop detection and map building. Unlike metric maps, topological maps simplify path planning and navigation by representing environments with key nodes instead of precise coordinates. Actions for visual navigation are determined by comparing segmented images with the image associated with target nodes. The system relies solely on a monocular camera, ensuring fast map building and navigation using key nodes. Experiments show effective loop detection and navigation in real and simulation environments without pre-training. Compared to a ResNet-based method, this approach improves success rates by 60.2% on average while reducing time and space costs, offering a lightweight solution for robot and human navigation in various scenarios.

</details>


### [17] [Towards reliable subsea object recovery: a simulation study of an auv with a suction-actuated end effector](https://arxiv.org/abs/2601.01106)
*Michele Grimaldi,Yosaku Maeda,Hitoshi Kakami,Ignacio Carlucho,Yvan Petillot,Tomoya Inoue*

Main category: cs.RO

TL;DR: 本研究利用Stonefish模拟器，展示了在极端深海条件下，HSV如何自主执行从水面到6000米的下潜、目标检测和物体回收。


<details>
  <summary>Details</summary>
Motivation: 自动化物体回收在极端深海环境中面临巨大的挑战，例如高水压、能见度低和水流复杂，精确操作变得尤为关键，而实际现场实验成本高且风险大。

Method: 本研究使用了Stonefish模拟器，结合PID控制器与基于逆运动学的机械臂控制器，并增强了加速度前馈，进行协调控制。

Result: 提出了一种基于仿真的方法，通过使用配备有三自由度机械臂和吸力末端执行器的深海小型车辆（HSV），成功进行了完整的自动化水下物体回收任务。

Conclusion: 高保真仿真为评估深海干预行为提供了有效且低风险的方法，有助于在实际部署前验证自动化行为。

Abstract: Autonomous object recovery in the hadal zone is challenging due to extreme hydrostatic pressure, limited visibility and currents, and the need for precise manipulation at full ocean depth. Field experimentation in such environments is costly, high-risk, and constrained by limited vehicle availability, making early validation of autonomous behaviors difficult. This paper presents a simulation-based study of a complete autonomous subsea object recovery mission using a Hadal Small Vehicle (HSV) equipped with a three-degree-of-freedom robotic arm and a suction-actuated end effector. The Stonefish simulator is used to model realistic vehicle dynamics, hydrodynamic disturbances, sensing, and interaction with a target object under hadal-like conditions. The control framework combines a world-frame PID controller for vehicle navigation and stabilization with an inverse-kinematics-based manipulator controller augmented by acceleration feed-forward, enabling coordinated vehicle - manipulator operation. In simulation, the HSV autonomously descends from the sea surface to 6,000 m, performs structured seafloor coverage, detects a target object, and executes a suction-based recovery. The results demonstrate that high-fidelity simulation provides an effective and low-risk means of evaluating autonomous deep-sea intervention behaviors prior to field deployment.

</details>


### [18] [Latent Space Reinforcement Learning for Multi-Robot Exploration](https://arxiv.org/abs/2601.01139)
*Sriram Rajasekar,Ashwini Ratnoo*

Main category: cs.RO

TL;DR: 本文提出了一种基于自动编码器和新颖程序生成算法的深度强化学习方法，旨在解决多代理系统在复杂环境中的自主映射问题。


<details>
  <summary>Details</summary>
Motivation: 解决多代理系统在未知环境映射中高效性与可扩展性不足的问题，尤其是在有限时间内的应用场景。

Method: 利用自动编码器进行维度减少和提出基于Perlin噪声的新颖程序生成算法，结合分层深度强化学习框架进行去中心化协调与加权共识机制。

Result: 提出的系统在代理数量增加时能够有效扩展，并且能够良好地泛化到不熟悉且结构上不同的环境，在通信受限的情况下也表现出强大的韧性。

Conclusion: 本文的方法有效解决了运动规划算法的可扩展性问题，并提升了多代理系统在复杂环境中的效率和鲁棒性。

Abstract: Autonomous mapping of unknown environments is a critical challenge, particularly in scenarios where time is limited. Multi-agent systems can enhance efficiency through collaboration, but the scalability of motion-planning algorithms remains a key limitation. Reinforcement learning has been explored as a solution, but existing approaches are constrained by the limited input size required for effective learning, restricting their applicability to discrete environments. This work addresses that limitation by leveraging autoencoders to perform dimensionality reduction, compressing high-fidelity occupancy maps into latent state vectors while preserving essential spatial information. Additionally, we introduce a novel procedural generation algorithm based on Perlin noise, designed to generate topologically complex training environments that simulate asteroid fields, caves and forests. These environments are used for training the autoencoder and the navigation algorithm using a hierarchical deep reinforcement learning framework for decentralized coordination. We introduce a weighted consensus mechanism that modulates reliance on shared data via a tuneable trust parameter, ensuring robustness to accumulation of errors. Experimental results demonstrate that the proposed system scales effectively with number of agents and generalizes well to unfamiliar, structurally distinct environments and is resilient in communication-constrained settings.

</details>


### [19] [VISO: Robust Underwater Visual-Inertial-Sonar SLAM with Photometric Rendering for Dense 3D Reconstruction](https://arxiv.org/abs/2601.01144)
*Shu Pan,Simon Archieri,Ahmet Cinar,Jonatan Scharff Willners,Ignacio Carlucho,Yvan Petillot*

Main category: cs.RO

TL;DR: 提出一种新的水下SLAM系统VISO，结合了立体相机、IMU和3D声纳，实现高精度定位和高保真密集重建。


<details>
  <summary>Details</summary>
Motivation: 应对水下环境的视觉挑战，提高基于视觉的定位和重建的准确性。

Method: 利用立体相机、惯性测量单元和3D声纳融合，采用粗到细的在线标定和光度渲染策略。

Result: 在实验中，VISO展示了优越的定位鲁棒性和准确性，以及与离线方法可比的实时3D重建表现。

Conclusion: VISO系统在水下定位和3D重建方面优于现有算法，具有实时性能。

Abstract: Visual challenges in underwater environments significantly hinder the accuracy of vision-based localisation and the high-fidelity dense reconstruction. In this paper, we propose VISO, a robust underwater SLAM system that fuses a stereo camera, an inertial measurement unit (IMU), and a 3D sonar to achieve accurate 6-DoF localisation and enable efficient dense 3D reconstruction with high photometric fidelity. We introduce a coarse-to-fine online calibration approach for extrinsic parameters estimation between the 3D sonar and the camera. Additionally, a photometric rendering strategy is proposed for the 3D sonar point cloud to enrich the sonar map with visual information. Extensive experiments in a laboratory tank and an open lake demonstrate that VISO surpasses current state-of-the-art underwater and visual-based SLAM algorithms in terms of localisation robustness and accuracy, while also exhibiting real-time dense 3D reconstruction performance comparable to the offline dense mapping method.

</details>


### [20] [ORION: Option-Regularized Deep Reinforcement Learning for Cooperative Multi-Agent Online Navigation](https://arxiv.org/abs/2601.01155)
*Zhang Shizhe,Liang Jingsong,Zhou Zhitao,Ye Shuhan,Wang Yizhuo,Tan Ming Siang Derek,Chiun Jimmy,Cao Yuhong,Sartoretti Guillaume*

Main category: cs.RO

TL;DR: 本文提出了ORION，一个新颖的深度强化学习框架，旨在提高多智能体在部分已知环境中的合作导航能力，特别是在动态地图环境中，ORION能自适应地调整智能体的导航策略以实现更高效的协作。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体导航方法通常假设环境完全已知，但对部分已知场景（如仓库或工厂地板）的支持有限，智能体需要在自己路径的最优性与收集及共享环境信息之间进行平衡。

Method: ORION利用共享图编码器来结合先验地图与在线感知，通过选项-评论家框架学习高层次合作模式，并采用双阶段合作策略来支援队友，从而降低整体任务完成时间。

Result: ORION框架在部分已知环境中的合作多智能体在线导航中表现出色，能够在动态地图差异下提供鲁棒状态嵌入，并实现高质量、实时的去中心化合作。

Conclusion: 通过大量模拟和实际机器人团队验证，ORION展示了其在真实世界合作导航中的鲁棒性和实用性。

Abstract: Existing methods for multi-agent navigation typically assume fully known environments, offering limited support for partially known scenarios such as warehouses or factory floors. There, agents may need to plan trajectories that balance their own path optimality with their ability to collect and share information about the environment that can help their teammates reach their own goals. To these ends, we propose ORION, a novel deep reinforcement learning framework for cooperative multi-agent online navigation in partially known environments. Starting from an imperfect prior map, ORION trains agents to make decentralized decisions, coordinate to reach their individual targets, and actively reduce map uncertainty by sharing online observations in a closed perception-action loop. We first design a shared graph encoder that fuses prior map with online perception into a unified representation, providing robust state embeddings under dynamic map discrepancies. At the core of ORION is an option-critic framework that learns to reason about a set of high-level cooperative modes that translate into sequences of low-level actions, allowing agents to switch between individual navigation and team-level exploration adaptively. We further introduce a dual-stage cooperation strategy that enables agents to assist teammates under map uncertainty, thereby reducing the overall makespan. Across extensive maze-like maps and large-scale warehouse environments, our simulation results show that ORION achieves high-quality, real-time decentralized cooperation over varying team sizes, outperforming state-of-the-art classical and learning-based baselines. Finally, we validate ORION on physical robot teams, demonstrating its robustness and practicality for real-world cooperative navigation.

</details>


### [21] [DST-Calib: A Dual-Path, Self-Supervised, Target-Free LiDAR-Camera Extrinsic Calibration Network](https://arxiv.org/abs/2601.01188)
*Zhiwei Huang,Yanwei Fu,Yi Zhou,Xieyuanli Chen,Qijun Chen,Rui Fan*

Main category: cs.RO

TL;DR: 本文提出了一种自监督的LiDAR-相机外部校准网络，采用双面数据增强，在线校准，并显著提升了模型的通用性与性能。


<details>
  <summary>Details</summary>
Motivation: 现有的LiDAR-相机外部校准方法依赖手工制作的校准目标或特定静态场景，限制了其在真实世界中的适应性和部署。

Method: 提出了一种双路径自监督校准框架，结合双面数据增强技术，改进了跨模态特征关联，从而减少了对高精度地面真值标签的依赖。

Result: 提出了第一种自监督的LiDAR-相机外部校准网络，能够在线工作，消除对特定校准目标的需求，并通过双面数据增强技术提高了校准的准确性和鲁棒性。

Conclusion: 通过大规模实验验证，所提方法在通用性和准确性上显著优于现有方法，并可实现完全自适应的在线校准。

Abstract: LiDAR-camera extrinsic calibration is essential for multi-modal data fusion in robotic perception systems. However, existing approaches typically rely on handcrafted calibration targets (e.g., checkerboards) or specific, static scene types, limiting their adaptability and deployment in real-world autonomous and robotic applications. This article presents the first self-supervised LiDAR-camera extrinsic calibration network that operates in an online fashion and eliminates the need for specific calibration targets. We first identify a significant generalization degradation problem in prior methods, caused by the conventional single-sided data augmentation strategy. To overcome this limitation, we propose a novel double-sided data augmentation technique that generates multi-perspective camera views using estimated depth maps, thereby enhancing robustness and diversity during training. Built upon this augmentation strategy, we design a dual-path, self-supervised calibration framework that reduces the dependence on high-precision ground truth labels and supports fully adaptive online calibration. Furthermore, to improve cross-modal feature association, we replace the traditional dual-branch feature extraction design with a difference map construction process that explicitly correlates LiDAR and camera features. This not only enhances calibration accuracy but also reduces model complexity. Extensive experiments conducted on five public benchmark datasets, as well as our own recorded dataset, demonstrate that the proposed method significantly outperforms existing approaches in terms of generalizability.

</details>


### [22] [EduSim-LLM: An Educational Platform Integrating Large Language Models and Robotic Simulation for Beginners](https://arxiv.org/abs/2601.01196)
*Shenqi Lu,Liangwei Zhang*

Main category: cs.RO

TL;DR: 本文提出EduSim-LLM，将大型语言模型应用于机器人控制，通过自然语言指令实现机器人行为转换，解决人机交互中的挑战，实验表明其高准确性。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言理解的发展，提高人机交互的直观性和可用性成为重要挑战，EduSim-LLM旨在解决这一挑战，促进机器人教育和实践的可及性。

Method: 通过设计直接控制和自主控制的人机交互模型，进行系统的仿真，评估LLMs在多机器人协作、运动规划和操作能力上的应用效果。

Result: 本论文提出了EduSim-LLM，这是一种将大型语言模型(LLMs)与机器人仿真结合的教育平台。EduSim-LLM旨在解决自然语言理解与机器人控制集成中的挑战，通过构建语言驱动控制模型，将自然语言指令转换为可执行的机器人行为序列。本文设计了直接控制和自主控制两种人机交互模型，并进行了系统的多语言模型仿真，评估机器人协作、运动规划和操作能力。实验结果表明，LLMs能够可靠地将自然语言转化为结构化的机器人动作，且经过提示工程模板后，指令解析的准确性显著提高。在任务复杂性增加的情况下，最高复杂度测试的整体准确率超过88.9%。

Conclusion: EduSim-LLM有效地解决了自然语言理解与机器人控制之间的障碍，提升了人机交互的直观性和实用性。

Abstract: In recent years, the rapid development of Large Language Models (LLMs) has significantly enhanced natural language understanding and human-computer interaction, creating new opportunities in the field of robotics. However, the integration of natural language understanding into robotic control is an important challenge in the rapid development of human-robot interaction and intelligent automation industries. This challenge hinders intuitive human control over complex robotic systems, limiting their educational and practical accessibility. To address this, we present the EduSim-LLM, an educational platform that integrates LLMs with robot simulation and constructs a language-drive control model that translates natural language instructions into executable robot behavior sequences in CoppeliaSim. We design two human-robot interaction models: direct control and autonomous control, conduct systematic simulations based on multiple language models, and evaluate multi-robot collaboration, motion planning, and manipulation capabilities. Experiential results show that LLMs can reliably convert natural language into structured robot actions; after applying prompt-engineering templates instruction-parsing accuracy improves significantly; as task complexity increases, overall accuracy rate exceeds 88.9% in the highest complexity tests.

</details>


### [23] [SAHA: Supervised Autonomous HArvester for selective forest thinning](https://arxiv.org/abs/2601.01282)
*Fang Nan,Meher Malladi,Qingqing Li,Fan Yang,Joonas Juola,Tiziano Guadagnino,Jens Behley,Cesar Cadena,Cyrill Stachniss,Marco Hutter*

Main category: cs.RO

TL;DR: 本文介绍了一款小型机器人收割机（SAHA），旨在实现选择性疏伐的自动化，经过一系列硬件改造及先进技术集成，已在北欧森林完成了自主试验。


<details>
  <summary>Details</summary>
Motivation: 林业在社会中发挥着重要作用，创造了显著的生态、经济和娱乐价值，然而高效的森林管理却需要人工密集且复杂的操作。

Method: 基于4.5吨的收割机平台，进行了关键硬件的改造及学习与模型基础的控制方法，确保液压执行器的精准控制和复杂环境下的导航。

Result: 提出了一种基于小型机器人收割机的解决方案（SAHA），能够在监督自主的情况下执行选择性疏伐任务。

Conclusion: 该机器人收割机能够在真实森林中操作，展现了在机器人森林管理领域的进步及表现分析。

Abstract: Forestry plays a vital role in our society, creating significant ecological, economic, and recreational value. Efficient forest management involves labor-intensive and complex operations. One essential task for maintaining forest health and productivity is selective thinning, which requires skilled operators to remove specific trees to create optimal growing conditions for the remaining ones. In this work, we present a solution based on a small-scale robotic harvester (SAHA) designed for executing this task with supervised autonomy. We build on a 4.5-ton harvester platform and implement key hardware modifications for perception and automatic control. We implement learning- and model-based approaches for precise control of hydraulic actuators, accurate navigation through cluttered environments, robust state estimation, and reliable semantic estimation of terrain traversability. Integrating state-of-the-art techniques in perception, planning, and control, our robotic harvester can autonomously navigate forest environments and reach targeted trees for selective thinning. We present experimental results from extensive field trials over kilometer-long autonomous missions in northern European forests, demonstrating the harvester's ability to operate in real forests. We analyze the performance and provide the lessons learned for advancing robotic forest management.

</details>


### [24] [Online Estimation and Manipulation of Articulated Objects](https://arxiv.org/abs/2601.01438)
*Russell Buchanan,Adrian Röfer,João Moura,Abhinav Valada,Sethu Vijayakumar*

Main category: cs.RO

TL;DR: 本研究提出一种新方法，通过结合视觉先验和自我感知，帮助机器人在操作未知物体时在线估计关节动作。


<details>
  <summary>Details</summary>
Motivation: 为了让服务机器人高效完成家务任务，必须使其能够操作任意关节物体。

Method: 使用因子图结合学习的视觉先验和自我感知进行在线关节估计，基于螺旋理论构建分析模型。

Result: 在机器人实验中，该方法实现了75%的未知关节物体自主打开成功率。

Conclusion: 机器人的初步关节预测和后续的快速更新有效提高了在现实中操控未知物体的成功率。

Abstract: From refrigerators to kitchen drawers, humans interact with articulated objects effortlessly every day while completing household chores. For automating these tasks, service robots must be capable of manipulating arbitrary articulated objects. Recent deep learning methods have been shown to predict valuable priors on the affordance of articulated objects from vision. In contrast, many other works estimate object articulations by observing the articulation motion, but this requires the robot to already be capable of manipulating the object. In this article, we propose a novel approach combining these methods by using a factor graph for online estimation of articulation which fuses learned visual priors and proprioceptive sensing during interaction into an analytical model of articulation based on Screw Theory. With our method, a robotic system makes an initial prediction of articulation from vision before touching the object, and then quickly updates the estimate from kinematic and force sensing during manipulation. We evaluate our method extensively in both simulations and real-world robotic manipulation experiments. We demonstrate several closed-loop estimation and manipulation experiments in which the robot was capable of opening previously unseen drawers. In real hardware experiments, the robot achieved a 75% success rate for autonomous opening of unknown articulated objects.

</details>


### [25] [AIMS: An Adaptive Integration of Multi-Sensor Measurements for Quadrupedal Robot Localization](https://arxiv.org/abs/2601.01561)
*Yujian Qiu,Yuqiu Mu,Wen Yang,Hao Zhu*

Main category: cs.RO

TL;DR: 本文提出一种新的适应性方法，旨在提升四足机器人在狭窄环境中的定位accuracy和robustness。


<details>
  <summary>Details</summary>
Motivation: 应对窄隧道环境中四足机器人定位精度低的问题。

Method: AIMS：自适应LiDAR-IMU-腿部里程计融合方法

Result: 在狭窄走廊环境中，提出的方法提高了定位精度和稳健性。

Conclusion: 通过实验验证，该方法在狭窄环境中优于现有技术，提升了定位准确性。

Abstract: This paper addresses the problem of accurate localization for quadrupedal robots operating in narrow tunnel-like environments. Due to the long and homogeneous characteristics of such scenarios, LiDAR measurements often provide weak geometric constraints, making traditional sensor fusion methods susceptible to accumulated motion estimation errors. To address these challenges, we propose AIMS, an adaptive LiDAR-IMU-leg odometry fusion method for robust quadrupedal robot localization in degenerate environments. The proposed method is formulated within an error-state Kalman filtering framework, where LiDAR and leg odometry measurements are integrated with IMU-based state prediction, and measurement noise covariance matrices are adaptively adjusted based on online degeneracy-aware reliability assessment. Experimental results obtained in narrow corridor environments demonstrate that the proposed method improves localization accuracy and robustness compared with state-of-the-art approaches.

</details>


### [26] [HanoiWorld : A Joint Embedding Predictive Architecture BasedWorld Model for Autonomous Vehicle Controller](https://arxiv.org/abs/2601.01577)
*Tran Tien Dat,Nguyen Hai An,Nguyen Khanh Viet Dung,Nguyen Duy Duc*

Main category: cs.RO

TL;DR: 本研究提出了一种基于联合嵌入预测架构（JEPA）的Hanoi-World世界模型，通过递归神经网络（RNN）进行长期规划，展示了在安全意识下高效的驾驶规划能力。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习在自主控制方面数据需求高且结果不稳定，且在安全性上表现不足，因此迫切需要一种新的方法。

Method: 本研究采用基于JEPA的世界模型和递归神经网络（RNN）进行长期规划和有效推理。

Result: 在Highway-Env包中进行的实验表明，Hanoi-World能够在安全意识下有效制定驾驶计划，其碰撞率显著低于现有的最先进基准。

Conclusion: Hanoi-World展示了在复杂环境中进行安全驾驶规划的有效性，并与现有基准相比表现出较低的碰撞率。

Abstract: Current attempts of Reinforcement Learning for Autonomous Controller are data-demanding while the results are under-performed, unstable, and unable to grasp and anchor on the concept of safety, and over-concentrating on noise features due to the nature of pixel reconstruction. While current Self-Supervised Learningapproachs that learning on high-dimensional representations by leveraging the JointEmbedding Predictive Architecture (JEPA) are interesting and an effective alternative, as the idea mimics the natural ability of the human brain in acquiring new skill usingimagination and minimal samples of observations. This study introduces Hanoi-World, a JEPA-based world model that using recurrent neural network (RNN) formaking longterm horizontal planning with effective inference time. Experimentsconducted on the Highway-Env package with difference enviroment showcase the effective capability of making a driving plan while safety-awareness, with considerablecollision rate in comparison with SOTA baselines

</details>


### [27] [Action-Sketcher: From Reasoning to Action via Visual Sketches for Long-Horizon Robotic Manipulation](https://arxiv.org/abs/2601.01618)
*Huajie Tan,Peterson Co,Yijie Xu,Shanyu Rong,Yuheng Ji,Cheng Chi,Xiansheng Chen,Qiongyu Zhang,Zhongxia Zhao,Pengwei Wang,Zhongyuan Wang,Shanghang Zhang*

Main category: cs.RO

TL;DR: 提出了一个新的机器人操作框架Action-Sketcher，利用视觉草图改善任务理解和执行，增强动态交互下的鲁棒性，与实际环境的适应性。


<details>
  <summary>Details</summary>
Motivation: 随着机器人操作在现实世界中的应用需求增加，需要在复杂环境中实现空间明确性和在动态互动下的时间韧性，但现有的方法在参考基础和任务分解等方面存在不足。

Method: 该框架采用循环的See-Think-Sketch-Act工作流程，并结合了一种自适应的token-gated策略，以增强推理、修正草图和执行行动的能力。通过跨模态的多阶段课程学习，提升了框架的训练和评估能力。

Result: 本研究提出了一种新颖的视觉语言行动框架Action-Sketcher，旨在改善长期机器人操作的效果，这一过程依赖于可视化草图来明确空间意图，并通过动态交互实现任务分解和因果解释。

Conclusion: Action-Sketcher通过利用视觉草图和多阶段学习策略，显著提升了机器人在复杂场景中的长期操作成功率、应对动态变化的鲁棒性，以及通过可编辑草图和逐步计划提升的可解释性。

Abstract: Long-horizon robotic manipulation is increasingly important for real-world deployment, requiring spatial disambiguation in complex layouts and temporal resilience under dynamic interaction. However, existing end-to-end and hierarchical Vision-Language-Action (VLA) policies often rely on text-only cues while keeping plan intent latent, which undermines referential grounding in cluttered or underspecified scenes, impedes effective task decomposition of long-horizon goals with close-loop interaction, and limits causal explanation by obscuring the rationale behind action choices. To address these issues, we first introduce Visual Sketch, an implausible visual intermediate that renders points, boxes, arrows, and typed relations in the robot's current views to externalize spatial intent, connect language to scene geometry. Building on Visual Sketch, we present Action-Sketcher, a VLA framework that operates in a cyclic See-Think-Sketch-Act workflow coordinated by adaptive token-gated strategy for reasoning triggers, sketch revision, and action issuance, thereby supporting reactive corrections and human interaction while preserving real-time action prediction. To enable scalable training and evaluation, we curate diverse corpus with interleaved images, text, Visual Sketch supervision, and action sequences, and train Action-Sketcher with a multi-stage curriculum recipe that combines interleaved sequence alignment for modality unification, language-to-sketch consistency for precise linguistic grounding, and imitation learning augmented with sketch-to-action reinforcement for robustness. Extensive experiments on cluttered scenes and multi-object tasks, in simulation and on real-world tasks, show improved long-horizon success, stronger robustness to dynamic scene changes, and enhanced interpretability via editable sketches and step-wise plans. Project website: https://action-sketcher.github.io

</details>


### [28] [DemoBot: Efficient Learning of Bimanual Manipulation with Dexterous Hands From Third-Person Human Videos](https://arxiv.org/abs/2601.01651)
*Yucheng Xu,Xiaofeng Mao,Elle Miller,Xinyu Yi,Yang Li,Zhibin Li,Robert B. Fisher*

Main category: cs.RO

TL;DR: DemoBot是一个学习框架，通过单个未标注RGB-D视频演示实现复杂操作技能的获取，采用强化学习改进运动轨迹。


<details>
  <summary>Details</summary>
Motivation: 解决学习复杂操作技能的挑战，无需从零开始学习，利用视频数据和强化学习来提高学习效率。

Method: 通过提取视频中的动作轨迹，结合强化学习进行技能提升和精细操作。

Result: DemoBot成功应用于长时间同步和异步的双手操作任务。

Conclusion: DemoBot提供了一种从人类视频中直接获取技能的可扩展方法，成功完成了长时间的双手组装任务。

Abstract: This work presents DemoBot, a learning framework that enables a dual-arm, multi-finger robotic system to acquire complex manipulation skills from a single unannotated RGB-D video demonstration. The method extracts structured motion trajectories of both hands and objects from raw video data. These trajectories serve as motion priors for a novel reinforcement learning (RL) pipeline that learns to refine them through contact-rich interactions, thereby eliminating the need to learn from scratch. To address the challenge of learning long-horizon manipulation skills, we introduce: (1) Temporal-segment based RL to enforce temporal alignment of the current state with demonstrations; (2) Success-Gated Reset strategy to balance the refinement of readily acquired skills and the exploration of subsequent task stages; and (3) Event-Driven Reward curriculum with adaptive thresholding to guide the RL learning of high-precision manipulation. The novel video processing and RL framework successfully achieved long-horizon synchronous and asynchronous bimanual assembly tasks, offering a scalable approach for direct skill acquisition from human videos.

</details>


### [29] [From Metrics to Meaning: Insights from a Mixed-Methods Field Experiment on Retail Robot Deployment](https://arxiv.org/abs/2601.01946)
*Sichao Song,Yuki Okafuji,Takuya Iwamoto,Jun Baba,Hiroshi Ishiguro*

Main category: cs.RO

TL;DR: 服务机器人提高了顾客停留率，但减少了员工参与的销售步骤，需更好地协调两者互动。


<details>
  <summary>Details</summary>
Motivation: 探讨服务机器人在零售环境中的有效性，以及其与顾客和员工的互动方式。

Method: 通过实际部署的现场实验，比较了无机器人、仅机器人及机器人+展示架三种条件下的顾客行为，并进行了员工访谈以深入理解定量结果。

Result: 服务机器人在吸引顾客方面表现出色，但对员工后续服务步骤产生负面影响。

Conclusion: 服务机器人能吸引顾客进入店内，但可能使员工服务变得不够主动，需优化人机协作。

Abstract: We report a mixed-methods field experiment of a conversational service robot deployed under everyday staffing discretion in a live bedding store. Over 12 days we alternated three conditions--Baseline (no robot), Robot-only, and Robot+Fixture--and video-annotated the service funnel from passersby to purchase. An explanatory sequential design then used six post-experiment staff interviews to interpret the quantitative patterns.
  Quantitatively, the robot increased stopping per passerby (highest with the fixture), yet clerk-led downstream steps per stopper--clerk approach, store entry, assisted experience, and purchase--decreased. Interviews explained this divergence: clerks avoided interrupting ongoing robot-customer talk, struggled with ambiguous timing amid conversational latency, and noted child-centered attraction that often satisfied curiosity at the doorway. The fixture amplified visibility but also anchored encounters at the threshold, creating a well-defined micro-space where needs could ``close'' without moving inside.
  We synthesize these strands into an integrative account from the initial show of interest on the part of a customer to their entering the store and derive actionable guidance. The results advance the understanding of interactions between customers, staff members, and the robot and offer practical recommendations for deploying service robots in high-touch retail.

</details>


### [30] [VisuoTactile 6D Pose Estimation of an In-Hand Object using Vision and Tactile Sensor Data](https://arxiv.org/abs/2601.01675)
*Snehal s. Dikhale,Karankumar Patel,Daksh Dhingra,Itoshi Naramura,Akinobu Hayashi,Soshi Iba,Nawid Jamali*

Main category: cs.RO

TL;DR: 该论文提出了一种结合视觉与触觉数据的6D物体位姿估计方法，能有效提高估计精度。


<details>
  <summary>Details</summary>
Motivation: 在物体抓取过程中，机器人夹爪的遮挡严重影响仅依赖视觉数据的方法，因此需要结合触觉数据来改进物体位姿估计。

Method: 提出了一种基于点云的网络架构，通过像素级密集融合将触觉数据与视觉数据结合。

Result: 实验结果表明，使用触觉数据可以提高6D位姿估计的准确性，并且网络能够有效地在合成和真实环境中工作。

Conclusion: 结合触觉数据与视觉数据可以显著提高物体的6D位姿估计精确度，并且网络可以成功地从合成训练迁移到真实机器人。

Abstract: Knowledge of the 6D pose of an object can benefit in-hand object manipulation. In-hand 6D object pose estimation is challenging because of heavy occlusion produced by the robot's grippers, which can have an adverse effect on methods that rely on vision data only. Many robots are equipped with tactile sensors at their fingertips that could be used to complement vision data. In this paper, we present a method that uses both tactile and vision data to estimate the pose of an object grasped in a robot's hand. To address challenges like lack of standard representation for tactile data and sensor fusion, we propose the use of point clouds to represent object surfaces in contact with the tactile sensor and present a network architecture based on pixel-wise dense fusion. We also extend NVIDIA's Deep Learning Dataset Synthesizer to produce synthetic photo-realistic vision data and corresponding tactile point clouds. Results suggest that using tactile data in addition to vision data improves the 6D pose estimate, and our network generalizes successfully from synthetic training to real physical robots.

</details>


### [31] [Explicit World Models for Reliable Human-Robot Collaboration](https://arxiv.org/abs/2601.01705)
*Kenneth Kwok,Basura Fernando,Qianli Xu,Vigneshwaran Subbaraju,Dongkyu Choi,Boon Kiat Quek*

Main category: cs.RO

TL;DR: 该论文探讨了在传感器噪声、模糊指令和人机交互下的鲁棒性问题，主张一种以人类期望为中心的人工智能可靠性方法。


<details>
  <summary>Details</summary>
Motivation: 在社交、多模态和流动的人类环境中，传统模型难以达到鲁棒性，因此需要重新审视人机交互的动态和主观特性。

Method: 研究方法致力于建立和更新一个可接近的"显性世界模型"，用于表示人类与AI之间的共同理解。

Result: 提出的方法能够更好地使机器人在复杂人际交互中理解和响应人类意图。

Conclusion: 研究表明，鲁棒性应在具体情境中考虑，以确保机器人行为与人类期望的一致性。

Abstract: This paper addresses the topic of robustness under sensing noise, ambiguous instructions, and human-robot interaction. We take a radically different tack to the issue of reliable embodied AI: instead of focusing on formal verification methods aimed at achieving model predictability and robustness, we emphasise the dynamic, ambiguous and subjective nature of human-robot interactions that requires embodied AI systems to perceive, interpret, and respond to human intentions in a manner that is consistent, comprehensible and aligned with human expectations. We argue that when embodied agents operate in human environments that are inherently social, multimodal, and fluid, reliability is contextually determined and only has meaning in relation to the goals and expectations of humans involved in the interaction. This calls for a fundamentally different approach to achieving reliable embodied AI that is centred on building and updating an accessible "explicit world model" representing the common ground between human and AI, that is used to align robot behaviours with human expectations.

</details>


### [32] [Simulations and Advancements in MRI-Guided Power-Driven Ferric Tools for Wireless Therapeutic Interventions](https://arxiv.org/abs/2601.01726)
*Wenhui Chu,Aobo Jin,Hardik A. Gohel*

Main category: cs.RO

TL;DR: 本研究开发了一个与MRI扫描仪集成的机器人系统，旨在提升血管内介入的精确性和安全性，显著推动医学成像与机器人技术的结合。


<details>
  <summary>Details</summary>
Motivation: 研究旨在提升MRI在医学成像中的应用，尤其是在指导血管内介入中的机器人辅助设备。

Method: 该系统基于Qt框架和C/C++，结合专用软件模块，用于与MRI扫描仪无缝集成。

Result: 系统能够创建针对不同血流特征的定制磁场梯度模式，评估导航预设血管路径的安全性和可行性。

Conclusion: 该系统对提高血管内介入程序的精确性和安全性具有重要意义。

Abstract: Designing a robotic system that functions effectively within the specific environment of a Magnetic Resonance Imaging (MRI) scanner requires solving numerous technical issues, such as maintaining the robot's precision and stability under strong magnetic fields. This research focuses on enhancing MRI's role in medical imaging, especially in its application to guide intravascular interventions using robot-assisted devices. A newly developed computational system is introduced, designed for seamless integration with the MRI scanner, including a computational unit and user interface. This system processes MR images to delineate the vascular network, establishing virtual paths and boundaries within vessels to prevent procedural damage. Key findings reveal the system's capability to create tailored magnetic field gradient patterns for device control, considering the vessel's geometry and safety norms, and adapting to different blood flow characteristics for finer navigation. Additionally, the system's modeling aspect assesses the safety and feasibility of navigating pre-set vascular paths. Conclusively, this system, based on the Qt framework and C/C++, with specialized software modules, represents a major step forward in merging imaging technology with robotic aid, significantly enhancing precision and safety in intravascular procedures.

</details>


### [33] [AlignDrive: Aligned Lateral-Longitudinal Planning for End-to-End Autonomous Driving](https://arxiv.org/abs/2601.01762)
*Yanhao Wu,Haoyang Zhang,Fei He,Rui Wu,Congpei Qiu,Liang Gao,Wei Ke,Tong Zhang*

Main category: cs.RO

TL;DR: 提出了一种新的级联框架，将纵向规划明确地与行驶路径条件结合，从而实现协调和以碰撞为意识的纵向和横向规划。


<details>
  <summary>Details</summary>
Motivation: 解决了当前端到端自动驾驶模型在规划阶段的协调失效和信息冗余问题。

Method: 提出了一种路径条件的形式，结合行驶路径进行纵向规划，并通过一种规划导向的数据增强策略模拟罕见的安全关键事件。

Result: 在Bench2Drive基准测试中获得89.07的驾驶评分和73.18%的成功率。

Conclusion: 该方法在Bench2Drive基准测试中设定了新的技术最前沿，显示出显著改善的协调性和安全性。

Abstract: End-to-end autonomous driving has rapidly progressed, enabling joint perception and planning in complex environments. In the planning stage, state-of-the-art (SOTA) end-to-end autonomous driving models decouple planning into parallel lateral and longitudinal predictions. While effective, this parallel design can lead to i) coordination failures between the planned path and speed, and ii) underutilization of the drive path as a prior for longitudinal planning, thus redundantly encoding static information. To address this, we propose a novel cascaded framework that explicitly conditions longitudinal planning on the drive path, enabling coordinated and collision-aware lateral and longitudinal planning. Specifically, we introduce a path-conditioned formulation that explicitly incorporates the drive path into longitudinal planning. Building on this, the model predicts longitudinal displacements along the drive path rather than full 2D trajectory waypoints. This design simplifies longitudinal reasoning and more tightly couples it with lateral planning. Additionally, we introduce a planning-oriented data augmentation strategy that simulates rare safety-critical events, such as vehicle cut-ins, by adding agents and relabeling longitudinal targets to avoid collision. Evaluated on the challenging Bench2Drive benchmark, our method sets a new SOTA, achieving a driving score of 89.07 and a success rate of 73.18%, demonstrating significantly improved coordination and safety

</details>


### [34] [DisCo-FLoc: Using Dual-Level Visual-Geometric Contrasts to Disambiguate Depth-Aware Visual Floorplan Localization](https://arxiv.org/abs/2601.01822)
*Shiyong Meng,Tao Zou,Bolei Chen,Chaoxu Mu,Jianxin Wang*

Main category: cs.RO

TL;DR: 本文提出了一种名为DisCo-FLoc的方法来解决视觉平面定位中的模糊问题，利用双层视觉-几何对比来消除深度意识视觉定位的歧义，无需额外的语义标签。


<details>
  <summary>Details</summary>
Motivation: 目前的视觉平面定位方法在处理具有重复结构的极简设计平面图时面临模糊性和语义标注限制的问题，因此需要一种新的方法来增强其适用性和准确性。

Method: 该方法首先使用深度估计技术的射线回归预测器来预测一系列FLoc候选点，并提出一种新的对比学习方法，通过位置级和方向级约束严格匹配深度感知视觉特征与对应的几何结构。

Result: 通过在两个标准视觉平面定位基准上进行广泛的比较研究，证明了DisCo-FLoc方法在增强鲁棒性和准确性方面的卓越性能。

Conclusion: DisCo-FLoc方法在多项标准视觉平面定位基准测试中优于现有的语义基础方法，显著提高了准确性和鲁棒性。

Abstract: Since floorplan data is readily available, long-term persistent, and robust to changes in visual appearance, visual Floorplan Localization (FLoc) has garnered significant attention. Existing methods either ingeniously match geometric priors or utilize sparse semantics to reduce FLoc uncertainty. However, they still suffer from ambiguous FLoc caused by repetitive structures within minimalist floorplans. Moreover, expensive but limited semantic annotations restrict their applicability. To address these issues, we propose DisCo-FLoc, which utilizes dual-level visual-geometric Contrasts to Disambiguate depth-aware visual Floc, without requiring additional semantic labels. Our solution begins with a ray regression predictor tailored for ray-casting-based FLoc, predicting a series of FLoc candidates using depth estimation expertise. In addition, a novel contrastive learning method with position-level and orientation-level constraints is proposed to strictly match depth-aware visual features with the corresponding geometric structures in the floorplan. Such matches can effectively eliminate FLoc ambiguity and select the optimal imaging pose from FLoc candidates. Exhaustive comparative studies on two standard visual Floc benchmarks demonstrate that our method outperforms the state-of-the-art semantic-based method, achieving significant improvements in both robustness and accuracy.

</details>


### [35] [CausalNav: A Long-term Embodied Navigation System for Autonomous Mobile Robots in Dynamic Outdoor Scenarios](https://arxiv.org/abs/2601.01872)
*Hongbo Duan,Shangyi Luo,Zhiyuan Deng,Yanbo Chen,Yuanhao Chiang,Yi Liu,Fangming Liu,Xueqian Wang*

Main category: cs.RO

TL;DR: CausalNav是首个针对动态户外环境的基于场景图的语义导航框架，通过多层次语义场景图实现高效导航。


<details>
  <summary>Details</summary>
Motivation: 移动机器人在大规模户外环境中的自主语言引导导航面临许多挑战，如语义推理、动态条件和长期稳定性。

Method: 使用大规模语言模型构建多层次语义场景图，结合实时感知与离线地图数据进行导航规划。

Result: 提出了一种新框架CausalNav，使用场景图进行语义导航。

Conclusion: 我们的框架在仿真及现实环境中表现出卓越的稳健性和效率。

Abstract: Autonomous language-guided navigation in large-scale outdoor environments remains a key challenge in mobile robotics, due to difficulties in semantic reasoning, dynamic conditions, and long-term stability. We propose CausalNav, the first scene graph-based semantic navigation framework tailored for dynamic outdoor environments. We construct a multi-level semantic scene graph using LLMs, referred to as the Embodied Graph, that hierarchically integrates coarse-grained map data with fine-grained object entities. The constructed graph serves as a retrievable knowledge base for Retrieval-Augmented Generation (RAG), enabling semantic navigation and long-range planning under open-vocabulary queries. By fusing real-time perception with offline map data, the Embodied Graph supports robust navigation across varying spatial granularities in dynamic outdoor environments. Dynamic objects are explicitly handled in both the scene graph construction and hierarchical planning modules. The Embodied Graph is continuously updated within a temporal window to reflect environmental changes and support real-time semantic navigation. Extensive experiments in both simulation and real-world settings demonstrate superior robustness and efficiency.

</details>


### [36] [Learning Diffusion Policy from Primitive Skills for Robot Manipulation](https://arxiv.org/abs/2601.01948)
*Zhihao Gu,Ming Yang,Difan Zou,Dong Xu*

Main category: cs.RO

TL;DR: SDP是一种技能条件的扩散政策，通过使用可解释的技能学习和条件动作规划，提升机器人在操控任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散政策依赖于全局指令，容易导致动作生成不匹配，研究人员希望通过使用细粒度的短时间操控技能提供更直观有效的机器人学习接口。

Method: 提出SDP，通过提取视觉观察和语言指令的离散表示，设计轻量路由网络，将复杂任务分解为一系列原始技能，生成对齐技能的动作。

Result: 在两个挑战性的仿真基准和真实机器人部署中，SDP始终优于最先进的方法。

Conclusion: SDP在各种任务中确保了一致的技能行为，并在复杂任务分解中表现优越，是技能基础的机器人学习新范式。

Abstract: Diffusion policies (DP) have recently shown great promise for generating actions in robotic manipulation. However, existing approaches often rely on global instructions to produce short-term control signals, which can result in misalignment in action generation. We conjecture that the primitive skills, referred to as fine-grained, short-horizon manipulations, such as ``move up'' and ``open the gripper'', provide a more intuitive and effective interface for robot learning. To bridge this gap, we propose SDP, a skill-conditioned DP that integrates interpretable skill learning with conditional action planning. SDP abstracts eight reusable primitive skills across tasks and employs a vision-language model to extract discrete representations from visual observations and language instructions. Based on them, a lightweight router network is designed to assign a desired primitive skill for each state, which helps construct a single-skill policy to generate skill-aligned actions. By decomposing complex tasks into a sequence of primitive skills and selecting a single-skill policy, SDP ensures skill-consistent behavior across diverse tasks. Extensive experiments on two challenging simulation benchmarks and real-world robot deployments demonstrate that SDP consistently outperforms SOTA methods, providing a new paradigm for skill-based robot learning with diffusion policies.

</details>


### [37] [What you reward is what you learn: Comparing rewards for online speech policy optimization in public HRI](https://arxiv.org/abs/2601.01969)
*Sichao Song,Yuki Okafuji,Kaito Ariu,Amy Koike*

Main category: cs.RO

TL;DR: 本研究探讨了如何在实际环境中优化社交机器人的语言策略，采用在线学习方法和多臂老虎机理论，提供了实施在线优化的设计经验。


<details>
  <summary>Details</summary>
Motivation: 为在开放和多样化环境中设计高效且可接受的会话服务机器人政策提出挑战。

Method: 在线学习与多臂老虎机问题结合，采用汤普森采样优化社交机器人语言策略。

Result: 通过对1400多次公共交互的现场部署，比较了三种补充的二元奖励并分析了机器人与用户的交互行为。

Conclusion: 研究总结了在实际公共人机交互中实施语言策略在线优化的可用设计经验。

Abstract: Designing policies that are both efficient and acceptable for conversational service robots in open and diverse environments is non-trivial. Unlike fixed, hand-tuned parameters, online learning can adapt to non-stationary conditions. In this paper, we study how to adapt a social robot's speech policy in the wild. During a 12-day in-situ deployment with over 1,400 public encounters, we cast online policy optimization as a multi-armed bandit problem and use Thompson sampling to select among six actions defined by speech rate (slow/normal/fast) and verbosity (concise/detailed). We compare three complementary binary rewards--Ru (user rating), Rc (conversation closure), and Rt (>=2 turns)--and show that each induces distinct arm distributions and interaction behaviors. We complement the online results with offline evaluations that analyze contextual factors (e.g., crowd level, group size) using video-annotated data. Taken together, we distill ready-to-use design lessons for deploying online optimization of speech policies in real public HRI settings.

</details>


### [38] [Deep Robust Koopman Learning from Noisy Data](https://arxiv.org/abs/2601.01971)
*Aditya Singh,Rajpal Singh,Jishnu Keshavan*

Main category: cs.RO

TL;DR: 提出了一种新颖的神经网络架构，通过联合学习提升函数和减小偏差的Koopman算子，克服了使用噪声数据时的限制。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据通常包含噪声，导致难以获得准确的Koopman算子近似；噪声引入的偏差严重影响预测和跟踪性能。

Method: 提出一种基于自编码器的神经网络架构，联合学习适当的提升函数和减少偏差的Koopman算子。

Result: 通过学习一致的Koopman基函数和合成减少偏差的Koopman算子，方法在多种噪声水平下展现出更强的鲁棒性。

Conclusion: 该方法通过理论分析和仿真实验证明了在训练噪声影响下的显著偏差减少，且在实际情况下展现出良好效果。

Abstract: Koopman operator theory has emerged as a leading data-driven approach that relies on a judicious choice of observable functions to realize global linear representations of nonlinear systems in the lifted observable space. However, real-world data is often noisy, making it difficult to obtain an accurate and unbiased approximation of the Koopman operator. The Koopman operator generated from noisy datasets is typically corrupted by noise-induced bias that severely degrades prediction and downstream tracking performance. In order to address this drawback, this paper proposes a novel autoencoder-based neural architecture to jointly learn the appropriate lifting functions and the reduced-bias Koopman operator from noisy data. The architecture initially learns the Koopman basis functions that are consistent for both the forward and backward temporal dynamics of the system. Subsequently, by utilizing the learned forward and backward temporal dynamics, the Koopman operator is synthesized with a reduced bias making the method more robust to noise compared to existing techniques. Theoretical analysis is used to demonstrate significant bias reduction in the presence of training noise. Dynamics prediction and tracking control simulations are conducted for multiple serial manipulator arms, including performance comparisons with leading alternative designs, to demonstrate its robustness under various noise levels. Experimental studies with the Franka FR3 7-DoF manipulator arm are further used to demonstrate the effectiveness of the proposed approach in a practical setting.

</details>


### [39] [Genie Sim 3.0 : A High-Fidelity Comprehensive Simulation Platform for Humanoid Robot](https://arxiv.org/abs/2601.02078)
*Chenghao Yin,Da Huang,Di Yang,Jichao Wang,Nanshu Zhao,Chen Xu,Wenjun Sun,Linjie Hou,Zhijun Li,Junhui Wu,Zhaobo Liu,Zhen Xiao,Sheng Zhang,Lei Bao,Rui Feng,Zhenquan Pang,Jiayu Li,Qian Wang,Maoqing Yao*

Main category: cs.RO

TL;DR: Genie Sim 3.0是一个新的机器人模拟平台，结合大语言模型和视觉语言模型，支持高效数据收集和评估，并提供开源数据集。


<details>
  <summary>Details</summary>
Motivation: 开发强大且具有普适性的机器人学习模型需要大规模、多样化的训练数据和可靠的评估基准，而在物理世界中收集数据面临高成本和可扩展性挑战。

Method: 基于大语言模型构建高保真场景的工具，利用视觉语言模型建立自动评估管道，并通过合成数据进行实验验证。

Result: 推出Genie Sim 3.0，一个统一的机器人操作模拟平台，及一个基于大语言模型的工具，用于从自然语言指令构建高保真场景。

Conclusion: 通过系统实验，本研究验证了开放源代码数据集的强大零-shot sim-to-real迁移能力，表明在控制条件下，合成数据可有效替代真实数据。

Abstract: The development of robust and generalizable robot learning models is critically contingent upon the availability of large-scale, diverse training data and reliable evaluation benchmarks. Collecting data in the physical world poses prohibitive costs and scalability challenges, and prevailing simulation benchmarks frequently suffer from fragmentation, narrow scope, or insufficient fidelity to enable effective sim-to-real transfer. To address these challenges, we introduce Genie Sim 3.0, a unified simulation platform for robotic manipulation. We present Genie Sim Generator, a large language model (LLM)-powered tool that constructs high-fidelity scenes from natural language instructions. Its principal strength resides in rapid and multi-dimensional generalization, facilitating the synthesis of diverse environments to support scalable data collection and robust policy evaluation. We introduce the first benchmark that pioneers the application of LLM for automated evaluation. It leverages LLM to mass-generate evaluation scenarios and employs Vision-Language Model (VLM) to establish an automated assessment pipeline. We also release an open-source dataset comprising more than 10,000 hours of synthetic data across over 200 tasks. Through systematic experimentation, we validate the robust zero-shot sim-to-real transfer capability of our open-source dataset, demonstrating that synthetic data can server as an effective substitute for real-world data under controlled conditions for scalable policy training. For code and dataset details, please refer to: https://github.com/AgibotTech/genie_sim.

</details>


### [40] [Vision-Based Early Fault Diagnosis and Self-Recovery for Strawberry Harvesting Robots](https://arxiv.org/abs/2601.02085)
*Meili Sun,Chunjiang Zhao,Lichao Yang,Hao Liu,Shimin Hu,Ya Xiong*

Main category: cs.RO

TL;DR: 本论文提出了一种集成多任务感知与自我修复的视觉故障诊断框架SRR-Net，有效解决了草莓收获机器人中的多个关键问题，显著提升了视觉感知精度与收获效率。


<details>
  <summary>Details</summary>
Motivation: 针对现有草莓收获机器人面临的视觉感知低集成度、果实抓取对齐问题和抓取不足等挑战，提出了一种新的框架以提升收获机器人性能。

Method: 使用SRR-Net作为核心模型进行多任务感知，包括草莓检测、分割和成熟度估计，同时集成了故障诊断和自我修复策略。

Result: 实验结果显示，SRR-Net在视觉感知方面具有高精度，草莓检测精度为0.895，召回率为0.813，同时支持多个任务并保持竞争力的推理速度。

Conclusion: SRR-Net在草莓收获机器人中有效提升了视觉感知和故障诊断能力，显著提高了收获的稳定性和效率。

Abstract: Strawberry harvesting robots faced persistent challenges such as low integration of visual perception, fruit-gripper misalignment, empty grasping, and strawberry slippage from the gripper due to insufficient gripping force, all of which compromised harvesting stability and efficiency in orchard environments. To overcome these issues, this paper proposed a visual fault diagnosis and self-recovery framework that integrated multi-task perception with corrective control strategies. At the core of this framework was SRR-Net, an end-to-end multi-task perception model that simultaneously performed strawberry detection, segmentation, and ripeness estimation, thereby unifying visual perception with fault diagnosis. Based on this integrated perception, a relative error compensation method based on the simultaneous target-gripper detection was designed to address positional misalignment, correcting deviations when error exceeded the tolerance threshold. To mitigate empty grasping and fruit-slippage faults, an early abort strategy was implemented. A micro-optical camera embedded in the end-effector provided real-time visual feedback, enabling grasp detection during the deflating stage and strawberry slip prediction during snap-off through MobileNet V3-Small classifier and a time-series LSTM classifier. Experiments demonstrated that SRR-Net maintained high perception accuracy. For detection, it achieved a precision of 0.895 and recall of 0.813 on strawberries, and 0.972/0.958 on hands. In segmentation, it yielded a precision of 0.887 and recall of 0.747 for strawberries, and 0.974/0.947 for hands. For ripeness estimation, SRR-Net attained a mean absolute error of 0.035, while simultaneously supporting multi-task perception and sustaining a competitive inference speed of 163.35 FPS.

</details>


### [41] [SingingBot: An Avatar-Driven System for Robotic Face Singing Performance](https://arxiv.org/abs/2601.02125)
*Zhuoxiong Xu,Xuanchen Li,Yuhao Cheng,Fei Xu,Yichao Yan,Xiaokang Yang*

Main category: cs.RO

TL;DR: 提出一种头像驱动的框架，以提升机器人的唱歌表现，通过视频生成模型合成生动的唱歌头像，显著提高情感表达和口型与音频的同步性。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人面部驱动研究主要集中在对话或模仿静态表情，无法满足连续情感表达和唱歌的一致性的高需求。

Method: 提出一种新颖的头像驱动框架，生成引人入胜的机器人唱歌表演。

Result: 通过情感动态范围指标的量化评估，表明广泛的情感谱对吸引人的表演至关重要。

Conclusion: 经过全面实验验证，我们的方法在情感表达方面表现出色，显著优于现有方案，提升了机器人唱歌的情感表现力。

Abstract: Equipping robotic faces with singing capabilities is crucial for empathetic Human-Robot Interaction. However, existing robotic face driving research primarily focuses on conversations or mimicking static expressions, struggling to meet the high demands for continuous emotional expression and coherence in singing. To address this, we propose a novel avatar-driven framework for appealing robotic singing. We first leverage portrait video generation models embedded with extensive human priors to synthesize vivid singing avatars, providing reliable expression and emotion guidance. Subsequently, these facial features are transferred to the robot via semantic-oriented mapping functions that span a wide expression space. Furthermore, to quantitatively evaluate the emotional richness of robotic singing, we propose the Emotion Dynamic Range metric to measure the emotional breadth within the Valence-Arousal space, revealing that a broad emotional spectrum is crucial for appealing performances. Comprehensive experiments prove that our method achieves rich emotional expressions while maintaining lip-audio synchronization, significantly outperforming existing approaches.

</details>


### [42] [Differential Barometric Altimetry for Submeter Vertical Localization and Floor Recognition Indoors](https://arxiv.org/abs/2601.02184)
*Yuhang Zhang,Sören Schwertfeger*

Main category: cs.RO

TL;DR: 本研究提出了一种结合差分气压传感的低成本框架，实现移动机器人在复杂环境中的准确垂直定位和楼层识别。


<details>
  <summary>Details</summary>
Motivation: 移动机器人在复杂多层环境中的定位和导航需要准确的高度估计和可靠的楼层识别。

Method: 利用差分气压传感器技术，集成在全ROS兼容的软件包中，实时发布高度数据。

Result: 提出的气压管道在艰难环境中测试表明，实现了亚米级垂直精度（RMSE：0.29 m）和100%的楼层识别率。

Conclusion: 该研究提出了一种低成本和高效的垂直估计框架，确保了移动机器人在多层环境中的准确定位和楼层识别。

Abstract: Accurate altitude estimation and reliable floor recognition are critical for mobile robot localization and navigation within complex multi-storey environments. In this paper, we present a robust, low-cost vertical estimation framework leveraging differential barometric sensing integrated within a fully ROS-compliant software package. Our system simultaneously publishes real-time altitude data from both a stationary base station and a mobile sensor, enabling precise and drift-free vertical localization. Empirical evaluations conducted in challenging scenarios -- such as fully enclosed stairwells and elevators, demonstrate that our proposed barometric pipeline achieves sub-meter vertical accuracy (RMSE: 0.29 m) and perfect (100%) floor-level identification. In contrast, our results confirm that standalone height estimates, obtained solely from visual- or LiDAR-based SLAM odometry, are insufficient for reliable vertical localization. The proposed ROS-compatible barometric module thus provides a practical and cost-effective solution for robust vertical awareness in real-world robotic deployments. The implementation of our method is released as open source at https://github.com/witsir/differential-barometric.

</details>


### [43] [CycleVLA: Proactive Self-Correcting Vision-Language-Action Models via Subtask Backtracking and Minimum Bayes Risk Decoding](https://arxiv.org/abs/2601.02295)
*Chenyang Ma,Guangyu Yang,Kai Lu,Shitong Xu,Bill Byrne,Niki Trigoni,Andrew Markham*

Main category: cs.RO

TL;DR: CycleVLA是一个具备主动自我纠正能力的系统，能够在故障发生前预测并恢复。


<details>
  <summary>Details</summary>
Motivation: 当前机器人故障检测与纠正工作通常是在故障发生后进行的，缺乏主动预防能力。

Method: CycleVLA整合了一个进度感知VLA、基于VLM的故障预测与规划，以及基于最小贝叶斯风险的测试时缩放策略。

Result: 循环VLA系统能够主动识别即将发生的故障并在执行过程中进行自我纠正。

Conclusion: 大量实验表明，CycleVLA提升了VLA的执行性能，且MBR解码策略在回退后提升了重试成功率。

Abstract: Current work on robot failure detection and correction typically operate in a post hoc manner, analyzing errors and applying corrections only after failures occur. This work introduces CycleVLA, a system that equips Vision-Language-Action models (VLAs) with proactive self-correction, the capability to anticipate incipient failures and recover before they fully manifest during execution. CycleVLA achieves this by integrating a progress-aware VLA that flags critical subtask transition points where failures most frequently occur, a VLM-based failure predictor and planner that triggers subtask backtracking upon predicted failure, and a test-time scaling strategy based on Minimum Bayes Risk (MBR) decoding to improve retry success after backtracking. Extensive experiments show that CycleVLA improves performance for both well-trained and under-trained VLAs, and that MBR serves as an effective zero-shot test-time scaling strategy for VLAs. Project Page: https://dannymcy.github.io/cyclevla/

</details>
