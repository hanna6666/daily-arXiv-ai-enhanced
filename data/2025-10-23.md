<div id=toc></div>

# Table of Contents

- [cs.HC](#cs.HC) [Total: 19]
- [cs.RO](#cs.RO) [Total: 25]


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [1] [LLM Bazaar: A Service Design for Supporting Collaborative Learning with an LLM-Powered Multi-Party Collaboration Infrastructure](https://arxiv.org/abs/2510.18877)
*Zhen Wu,Jiaxin Shi,R. Charles Murray,Carolyn Rosé,Micah San Andres*

Main category: cs.HC

TL;DR: 本论文探讨了大语言模型在协作学习中的应用，提出一种新的LLM代理架构以支持实时学习，并研究其对学习效果和互动的影响。


<details>
  <summary>Details</summary>
Motivation: 为了改善协作学习中的互动结构、组动力和学生参与度，利用最新的大语言模型技术。

Method: 在现有的开源协作支持架构Bazaar的基础上，集成了一个LLM代理接口，提供实时、上下文敏感的协作支持。

Result: 论文结果表明，集成LLM的代理接口能为小组学习提供有针对性的支持，从而可能提升学习效果和互动模式。

Conclusion: 该论文探讨了如何利用大语言模型（LLMs）增强协作学习的支持环境，并指出这一进展可能改变学习结果和互动模式。

Abstract: For nearly two decades, conversational agents have played a critical role in
structuring interactions in collaborative learning, shaping group dynamics, and
supporting student engagement. The recent integration of large language models
(LLMs) into these agents offers new possibilities for fostering critical
thinking and collaborative problem solving. In this work, we begin with an open
source collaboration support architecture called Bazaar and integrate an
LLM-agent shell that enables introduction of LLM-empowered, real time, context
sensitive collaborative support for group learning. This design and
infrastructure paves the way for exploring how tailored LLM-empowered
environments can reshape collaborative learning outcomes and interaction
patterns.

</details>


### [2] [CityAQVis: Integrated ML-Visualization Sandbox Tool for Pollutant Estimation in Urban Regions Using Multi-Source Data (Software Article)](https://arxiv.org/abs/2510.18878)
*Brij Bridhin Desai,Yukta Arvind,Aswathi Mundayatt,Jaya Sreevalsan-Nair*

Main category: cs.HC

TL;DR: CityAQVis是一个交互式机器学习工具，旨在利用多源数据预测和可视化城市空气污染物浓度，改善空气质量管理。


<details>
  <summary>Details</summary>
Motivation: 应对城市空气污染对公共健康和环境可持续性的重大风险，填补缺乏有效集成预测与可视化工具的空白。

Method: 采用多源数据，通过交互式机器学习工具CityAQVis预测和可视化地面污染物浓度。

Result: 开发的CityAQVis工具在案例研究中成功预测了城市地区的二氧化氮浓度，并展示了对不同污染物的适应性。

Conclusion: CityAQVis能够通过机器学习推动可视化分析，提升空气质量管理中的情境意识与数据驱动决策能力。

Abstract: Urban air pollution poses significant risks to public health, environmental
sustainability, and policy planning. Effective air quality management requires
predictive tools that can integrate diverse datasets and communicate complex
spatial and temporal pollution patterns. There is a gap in interactive tools
with seamless integration of forecasting and visualization of spatial
distributions of air pollutant concentrations. We present CityAQVis, an
interactive machine learning ML sandbox tool designed to predict and visualize
pollutant concentrations at the ground level using multi-source data, which
includes satellite observations, meteorological parameters, population density,
elevation, and nighttime lights. While traditional air quality visualization
tools often lack forecasting capabilities, CityAQVis enables users to build and
compare predictive models, visualizing the model outputs and offering insights
into pollution dynamics at the ground level. The pilot implementation of the
tool is tested through case studies predicting nitrogen dioxide (NO2)
concentrations in metropolitan regions, highlighting its adaptability to
various pollutants. Through an intuitive graphical user interface (GUI), the
user can perform comparative visualizations of the spatial distribution of
surface-level pollutant concentration in two different urban scenarios. Our
results highlight the potential of ML-driven visual analytics to improve
situational awareness and support data-driven decision-making in air quality
management.

</details>


### [3] [FIRETWIN: Digital Twin Advancing Multi-Modal Sensing, Interactive Analytics for Wildfire Response](https://arxiv.org/abs/2510.18879)
*Mayamin Hamid Raha,Ali Reza Tavakkoli,Chris Webb,Mobin Habibpour,Janice Coen,Eric Rowell,Fatemeh Afghah*

Main category: cs.HC

TL;DR: FIRETWIN是一个结合历史数据和实时数字可视化的数字双胞胎，可用于改善野火管理与响应。


<details>
  <summary>Details</summary>
Motivation: 目前的野火管理系统缺乏将历史数据与沉浸式数字表达相结合的集成虚拟环境，这影响了深入分析和有效决策的能力。

Method: FIRETWIN利用谷歌地图、虚幻引擎及CAWFE模型生成的输出，重建2014年加利福尼亚州金火的传播，提供高度真实和互动的环境。

Result: 本文介绍了FIRETWIN，这是一个旨在桥接复杂生态数据和高保真可视化的网络物理数字双胞胎，为可操作的事件响应提供支持。

Conclusion: 通过实时模拟和交互，FIRETWIN为用户提供了对火灾行为的深入理解和响应能力，极大地提升了野火管理的有效性。

Abstract: Current wildfire management systems lack integrated virtual environments that
combine historical data with immersive digital representations, hindering deep
analysis and effective decision making. This paper introduces FIRETWIN, a
cyber-physical Digital Twin (DT) designed to bridge complex ecological data and
operationally relevant, high-fidelity visualizations for actionable incident
response. FIRETWIN generates a dynamic 3D virtual globe that visualizes
evolving fire behavior in real time, driven by output from physics-based fire
models. The system supports multimodal perspectives, including satellite and
drone viewpoints comparable to NOAA GOES-18 imagery - enabling comprehensive
scenario analysis. Users interact with the environment to assess current fire
conditions, anticipate progression, and evaluate available resources.
Leveraging Google Maps, Unreal Engine, and pre-generated outputs from the CAWFE
coupled weather-wildland fire model, we reconstruct the spread of the 2014 King
Fire in California Eldorado National Forest. Procedural forest generation and
particle-level fire control enable a level of realism and interactivity not
possible in field training.

</details>


### [4] [Towards Better Health Conversations: The Benefits of Context-seeking](https://arxiv.org/abs/2510.18880)
*Rory Sayres,Yuexing Hao,Abbi Ward,Amy Wang,Beverly Freeman,Serena Zhan,Diego Ardila,Jimmy Li,I-Ching Lee,Anna Iurchenko,Siyi Kou,Kartikeya Badola,Jimmy Hu,Bhawesh Kumar,Keith Johnson,Supriya Vijay,Justin Krogue,Avinatan Hassidim,Yossi Matias,Dale R. Webster,Sunny Virmani,Yun Liu,Quang Duong,Mike Schaekermann*

Main category: cs.HC

TL;DR: 本研究探索了如何通过大型语言模型（LLMs）提高健康信息获取的体验，强调主动寻求背景信息的重要性，并提出相应的对话AI设计建议。


<details>
  <summary>Details</summary>
Motivation: 现代信息环境中，获取健康问题的信息可能很困难，LLMs可以提供定制的、便捷的信息，但也存在不准确和偏见的风险。

Method: 通过4项混合方法研究，对总共163名参与者进行分析，考察人们如何与大型语言模型（LLMs）互动以解答健康问题。

Result: 开发了一个“导航AI”，在随机双盲研究中，参与者对其好评，认为其比基础AI更有帮助、相关和针对性强。

Conclusion: 主动寻求背景信息对对话动态有显著影响，建议了用于健康主题的对话AI设计模式。

Abstract: Navigating health questions can be daunting in the modern information
landscape. Large language models (LLMs) may provide tailored, accessible
information, but also risk being inaccurate, biased or misleading. We present
insights from 4 mixed-methods studies (total N=163), examining how people
interact with LLMs for their own health questions. Qualitative studies revealed
the importance of context-seeking in conversational AIs to elicit specific
details a person may not volunteer or know to share. Context-seeking by LLMs
was valued by participants, even if it meant deferring an answer for several
turns. Incorporating these insights, we developed a "Wayfinding AI" to
proactively solicit context. In a randomized, blinded study, participants rated
the Wayfinding AI as more helpful, relevant, and tailored to their concerns
compared to a baseline AI. These results demonstrate the strong impact of
proactive context-seeking on conversational dynamics, and suggest design
patterns for conversational AI to help navigate health topics.

</details>


### [5] [Detecting AI-Assisted Cheating in Online Exams through Behavior Analytics](https://arxiv.org/abs/2510.18881)
*Gökhan Akçapınar*

Main category: cs.HC

TL;DR: 本研究探讨了学生在在线考试中可能出现的AI辅助作弊行为，并通过聚类方法分析了可疑行为与考试成绩之间的关系。


<details>
  <summary>Details</summary>
Motivation: 鉴于AI辅助作弊对在线考试的安全性造成威胁，本研究旨在通过行为分析发现可疑的学生群体。

Method: 采用k-Means聚类分析方法，分析了学生在监考下在线考试中的可疑行为，定义了选中文本、右键点击和失去页面关注度等行为。

Result: 本研究通过聚类分析，识别了在线考试中表现出可疑行为的学生群体，并比较了他们的考试成绩。大约33%的学生表现出不同程度的可疑行为，而这些学生的考试成绩平均高出30-40分。

Conclusion: 尽管需要进一步验证，这项初步研究为识别在线考试中的AI辅助作弊提供了重要的见解。

Abstract: AI-assisted cheating has emerged as a significant threat in the context of
online exams. Advanced browser extensions now enable large language models
(LLMs) to answer questions presented in online exams within seconds, thereby
compromising the security of these assessments. In this study, the behaviors of
students (N = 52) on an online exam platform during a proctored, face-to-face
exam were analyzed using clustering methods, with the aim of identifying groups
of students exhibiting suspicious behavior potentially associated with
cheating. Additionally, students in different clusters were compared in terms
of their exam scores. Suspicious exam behaviors in this study were defined as
selecting text within the question area, right-clicking, and losing focus on
the exam page. The total frequency of these behaviors performed by each student
during the exam was extracted, and k-Means clustering was employed for the
analysis. The findings revealed that students were classified into six clusters
based on their suspicious behaviors. It was found that students in four of the
six clusters, representing approximately 33% of the total sample, exhibited
suspicious behaviors at varying levels. When the exam scores of these students
were compared, it was observed that those who engaged in suspicious behaviors
scored, on average, 30-40 points higher than those who did not. Although
further research is necessary to validate these findings, this preliminary
study provides significant insights into the detection of AI-assisted cheating
in online exams using behavior analytics.

</details>


### [6] [Plural Voices, Single Agent: Towards Inclusive AI in Multi-User Domestic Spaces](https://arxiv.org/abs/2510.19008)
*Joydeep Chandra,Satyam Kumar Navneet*

Main category: cs.HC

TL;DR: 提出了一种新型家用AI框架PVM，旨在解决伦理和包容性问题，通过动态协商满足多用户需求，初步评价表明其性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 为应对家用AI在伦理、自治和包容性方面的挑战，特别是对儿童、老年人和神经多样性用户的关注。

Method: 通过采用公共数据集，使用人类 + 合成课程设计和公平意识场景，开发出一个动态协商用户需求的单代理框架。

Result: PVM在各项性能指标上超越了多代理基线，并通过设计创新展示了用户中心系统的潜力。

Conclusion: PVM在合规性、公平性、安全性和延迟方面表现优于多代理基线，为伦理和包容性家用AI提供了新方向。

Abstract: Domestic AI agents faces ethical, autonomy, and inclusion challenges,
particularly for overlooked groups like children, elderly, and Neurodivergent
users. We present the Plural Voices Model (PVM), a novel single-agent framework
that dynamically negotiates multi-user needs through real-time value alignment,
leveraging diverse public datasets on mental health, eldercare, education, and
moral reasoning. Using human+synthetic curriculum design with fairness-aware
scenarios and ethical enhancements, PVM identifies core values, conflicts, and
accessibility requirements to inform inclusive principles. Our privacy-focused
prototype features adaptive safety scaffolds, tailored interactions (e.g.,
step-by-step guidance for Neurodivergent users, simple wording for children),
and equitable conflict resolution. In preliminary evaluations, PVM outperforms
multi-agent baselines in compliance (76% vs. 70%), fairness (90% vs. 85%),
safety-violation rate (0% vs. 7%), and latency. Design innovations, including
video guidance, autonomy sliders, family hubs, and adaptive safety dashboards,
demonstrate new directions for ethical and inclusive domestic AI, for building
user-centered agentic systems in plural domestic contexts. Our Codes and Model
are been open sourced, available for reproduction:
https://github.com/zade90/Agora

</details>


### [7] [SocializeChat: A GPT-Based AAC Tool Grounded in Personal Memories to Support Social Communication](https://arxiv.org/abs/2510.19017)
*Wei Xiang,Yunkai Xu,Yuyang Fang,Zhuyu Teng,Zhaoqu Jiang,Beijia Hu,Jinguo Yang*

Main category: cs.HC

TL;DR: SocializeChat利用用户的个人记忆生成社交聊天建议，增强AAC工具的互动性和相关性。


<details>
  <summary>Details</summary>
Motivation: 老年人面临与他人进行有效社交沟通的困难，特别是在使用主要解决基本需求的辅助与替代沟通工具（AAC）时。

Method: 该系统结合了用户的主题偏好和人际关系的亲密程度，从而定制不同社交场景和对话伙伴的建议。

Result: SocializeChat 是一种AAC工具，通过用户的个人记忆记录生成句子建议，从而改善社交沟通。

Conclusion: 用户研究表明，SocializeChat在提升AAC支持的社交互动的包容性和相关性方面具有潜力。

Abstract: Elderly people with speech impairments often face challenges in engaging in
meaningful social communication, particularly when using Augmentative and
Alternative Communication (AAC) tools that primarily address basic needs.
Moreover, effective chats often rely on personal memories, which is hard to
extract and reuse. We introduce SocializeChat, an AAC tool that generates
sentence suggestions by drawing on users' personal memory records. By
incorporating topic preference and interpersonal closeness, the system reuses
past experience and tailors suggestions to different social contexts and
conversation partners. SocializeChat not only leverages past experiences to
support interaction, but also treats conversations as opportunities to create
new memories, fostering a dynamic cycle between memory and communication. A
user study shows its potential to enhance the inclusivity and relevance of
AAC-supported social interaction.

</details>


### [8] [Examining the Impact of Label Detail and Content Stakes on User Perceptions of AI-Generated Images on Social Media](https://arxiv.org/abs/2510.19024)
*Jingruo Chen,TungYen Wang,Marie Williams,Natalia Jordan,Mingyi Shao,Linda Zhang,Susan R. Fussell*

Main category: cs.HC

TL;DR: 本研究探讨了标签详细程度和内容风险对用户参与度及感知的影响，结果显示更详细的标签提升了透明度，但内容风险对用户参与度影响显著。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成图像在社交媒体上的日益普及，人们对信任和真实性的担忧日益增强，因此需要研究标签的详细程度和内容风险对用户行为的影响。

Method: 通过对105名参与者进行的组内实验研究，考察不同标签详细程度和内容风险对用户参与度以及对AI生成图像感知的影响。

Result: 该研究发现，提升标签的详细程度可以增强用户对标签透明度的感知，但未对用户参与度产生影响。内容的风险等级对用户参与度和信任感有显著影响，用户在低风险图像中表现出更高的参与度和信任感。

Conclusion: 社交媒体平台可以采用详细标签以提高透明度，而不影响用户参与度，建议AI生成内容的有效标签策略。

Abstract: AI-generated images are increasingly prevalent on social media, raising
concerns about trust and authenticity. This study investigates how different
levels of label detail (basic, moderate, maximum) and content stakes (high vs.
low) influence user engagement with and perceptions of AI-generated images
through a within-subjects experimental study with 105 participants. Our
findings reveal that increasing label detail enhances user perceptions of label
transparency but does not affect user engagement. However, content stakes
significantly impact user engagement and perceptions, with users demonstrating
higher engagement and trust in low-stakes images. These results suggest that
social media platforms can adopt detailed labels to improve transparency
without compromising user engagement, offering insights for effective labeling
strategies for AI-generated content.

</details>


### [9] [CLiVR: Conversational Learning System in Virtual Reality with AI-Powered Patients](https://arxiv.org/abs/2510.19031)
*Akilan Amithasagaran,Sagnik Dakshit,Bhavani Suryadevara,Lindsey Stockton*

Main category: cs.HC

TL;DR: CLiVR是一个虚拟现实系统，通过对话学习技术和情感分析，改进传统医学教育中的模拟训练。


<details>
  <summary>Details</summary>
Motivation: 传统的模拟患者培训方法资源消耗大，难以推广，亟需一种更高效、可拓展的训练方案。

Method: 使用Unity开发，基于综合症-症状数据库动态生成模拟场景，并进行情感分析以提供交流反馈。

Result: 本研究开发的CLiVR系统，在虚拟现实中结合大语言模型和3D化身，旨在提供更具可扩展性和沉浸感的医学教育训练。

Conclusion: CLiVR系统在医疗教育中显示出良好的用户接受度和良好的教育潜力，表明其作为标准化患者训练的可扩展补充。

Abstract: Simulations constitute a fundamental component of medical and nursing
education and traditionally employ standardized patients (SP) and high-fidelity
manikins to develop clinical reasoning and communication skills. However, these
methods require substantial resources, limiting accessibility and scalability.
In this study, we introduce CLiVR, a Conversational Learning system in Virtual
Reality that integrates large language models (LLMs), speech processing, and 3D
avatars to simulate realistic doctor-patient interactions. Developed in Unity
and deployed on the Meta Quest 3 platform, CLiVR enables trainees to engage in
natural dialogue with virtual patients. Each simulation is dynamically
generated from a syndrome-symptom database and enhanced with sentiment analysis
to provide feedback on communication tone. Through an expert user study
involving medical school faculty (n=13), we assessed usability, realism, and
perceived educational impact. Results demonstrated strong user acceptance, high
confidence in educational potential, and valuable feedback for improvement.
CLiVR offers a scalable, immersive supplement to SP-based training.

</details>


### [10] ["Over-the-Hood" AI Inclusivity Bugs and How 3 AI Product Teams Found and Fixed Them](https://arxiv.org/abs/2510.19033)
*Andrew Anderson,Fatima A. Moussaoui,Jimena Noa Guevara,Md Montaser Hamid,Margaret Burnett*

Main category: cs.HC

TL;DR: 本文研究了用户面对的AI产品中的包容性偏见，发现并修复了多种AI包容性错误，并提出一种新的设计方法。


<details>
  <summary>Details</summary>
Motivation: 研究AI产品中的用户面对的包容性偏见，特别是如何排除某些用户的解决问题方法的障碍。

Method: 通过与3个AI产品团队进行现场研究，探索用户面对的AI产品中的包容性错误及其修复方法。

Result: 识别出6种AI包容性错误，共出现83次，并修复了47个实例，同时开发了新的GenderMag-for-AI方法。

Conclusion: AI产品团队可以利用GenderMag-for-AI方法有效地识别和修复包容性错误。

Abstract: While much research has shown the presence of AI's "under-the-hood" biases
(e.g., algorithmic, training data, etc.), what about "over-the-hood"
inclusivity biases: barriers in user-facing AI products that disproportionately
exclude users with certain problem-solving approaches? Recent research has
begun to report the existence of such biases -- but what do they look like, how
prevalent are they, and how can developers find and fix them? To find out, we
conducted a field study with 3 AI product teams, to investigate what kinds of
AI inclusivity bugs exist uniquely in user-facing AI products, and whether/how
AI product teams might harness an existing (non-AI-oriented) inclusive design
method to find and fix them. The teams' work resulted in identifying 6 types of
AI inclusivity bugs arising 83 times, fixes covering 47 of these bug instances,
and a new variation of the GenderMag inclusive design method, GenderMag-for-AI,
that is especially effective at detecting certain kinds of AI inclusivity bugs.

</details>


### [11] [When Strings Tug at Algorithm: Human-AI Sovereignty and Entanglement in Nomadic Improvisational Music Performance as a Decolonial Exploration](https://arxiv.org/abs/2510.19086)
*Joshua Nijiati Alimujiang*

Main category: cs.HC

TL;DR: 研究人类与AI的权力动态，特别是在文化遗产背景下，强调创作者的自主性和去殖民化实践。


<details>
  <summary>Details</summary>
Motivation: 探讨人类与人工智能之间的权力动态，特别是在游牧即兴表演的背景下

Method: 实证研究与艺术创作

Result: 发现音乐家在创作过程中选择性地接受或拒绝算法建议，以维护他们的创作身份

Conclusion: 去殖民化的潜力要求重新设计文化生存工具，使技术成为去殖民实践的场所，而不仅仅是反馈环境。

Abstract: As emergent artificial intelligence technologies increasingly assert roles as
assistants within intangible cultural heritage contexts, researchers and
artists observe existing questions on the theme of agency negotiation, cultural
resistance, and technical critique. This research interrogates power dynamics
in human-AI sovereignty and entanglement for nomadic improvisational Dutar
performance, a living cultural heritage through a long-necked lute from the
Central Asia region. To investigate tensions between human agency and
computational hegemony, the researcher and artists examined and iterated a
feedback workflow that captures live performance data, processes digital
transformations, and creates a real-time interactive art experience via
immersive environments. Empirical data from artists and audience reveal
modulations where musicians selectively embrace or reject algorithmic
suggestions to preserve creative identity. The author concludes that decolonial
potential requires redesigning tools or systems for cultural survivance, where
technology becomes not merely a feedback environment but a site for decolonial
praxis, challenging computational hegemony in digital ecosystems.

</details>


### [12] [LLMartini: Seamless and Interactive Leveraging of Multiple LLMs through Comparison and Composition](https://arxiv.org/abs/2510.19252)
*Yingtian Shi,Jinda Yang,Yuhan Wang,Yiwen Yin,Haoyu Li,Kunyu Gao,Chun Yu*

Main category: cs.HC

TL;DR: LLMartini是一个互动系统，旨在简化多大型语言模型间的比较和组合，通过自动合并共识内容和高亮模型差异来提高效率，用户研究显示其效果显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 用户需要在不同大型语言模型之间比较和整合输出，以获得更高质量的响应，但手动整合输出效率低下，导致认知负担增加。

Method: LLMartini系统的开发与用户研究

Result: LLMartini在用户研究中显著超过传统手动方法，提升了任务完成时间、减少了认知负担，增加了用户满意度。

Conclusion: 该研究强调了以人为本的设计在提高多语言模型交互效率和创造力中的重要性，并对有效利用不同语言模型的互补优势提出了实际建议。

Abstract: The growing diversity of large language models (LLMs) means users often need
to compare and combine outputs from different models to obtain higher-quality
or more comprehensive responses. However, switching between separate interfaces
and manually integrating outputs is inherently inefficient, leading to a high
cognitive burden and fragmented workflows. To address this, we present
LLMartini, a novel interactive system that supports seamless comparison,
selection, and intuitive cross-model composition tools. The system decomposes
responses into semantically aligned segments based on task-specific criteria,
automatically merges consensus content, and highlights model differences
through color coding while preserving unique contributions. In a user study
(N=18), LLMartini significantly outperformed conventional manual methods across
all measured metrics, including task completion time, cognitive load, and user
satisfaction. Our work highlights the importance of human-centered design in
enhancing the efficiency and creativity of multi-LLM interactions and offers
practical implications for leveraging the complementary strengths of various
language models.

</details>


### [13] [Learning To Defer To A Population With Limited Demonstrations](https://arxiv.org/abs/2510.19351)
*Nilesh Ramgolam,Gustavo Carneiro,Hsiang-Ting,Chen*

Main category: cs.HC

TL;DR: 这项工作提出了一种新的框架，通过生成合成标签来提升L2D系统的适应性和可扩展性，解决数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 解决学习延迟(L2D)系统在实际部署中的数据稀缺问题

Method: 提出一种基于上下文的半监督框架，利用元学习生成专家特定的嵌入

Result: 在三种不同数据集上的实验结果表明，使用合成标签训练的模型迅速接近oracle水平的性能

Conclusion: 通过解决关键的训练瓶颈，此研究提高了自适应L2D系统的实用性和可扩展性，促进了人类与AI的协作。

Abstract: This paper addresses the critical data scarcity that hinders the practical
deployment of learning to defer (L2D) systems to the population. We introduce a
context-aware, semi-supervised framework that uses meta-learning to generate
expert-specific embeddings from only a few demonstrations. We demonstrate the
efficacy of a dual-purpose mechanism, where these embeddings are used first to
generate a large corpus of pseudo-labels for training, and subsequently to
enable on-the-fly adaptation to new experts at test-time. The experiment
results on three different datasets confirm that a model trained on these
synthetic labels rapidly approaches oracle-level performance, validating the
data efficiency of our approach. By resolving a key training bottleneck, this
work makes adaptive L2D systems more practical and scalable, paving the way for
human-AI collaboration in real-world environments. To facilitate
reproducibility and address implementation details not covered in the main
text, we provide our source code and training configurations at
https://github.com/nil123532/learning-to-defer-to-a-population-with-limited-demonstrations.

</details>


### [14] [Design Considerations for Human Oversight of AI: Insights from Co-Design Workshops and Work Design Theory](https://arxiv.org/abs/2510.19512)
*Cedric Faas,Sophie Kerstan,Richard Uth,Markus Langer,Anna Maria Feit*

Main category: cs.HC

TL;DR: AI系统的发展使得领域专家的角色转变为监督AI输出。本文通过与领域专家的合作，提出了用户在监督中的关键需求，并基于此设计了一个框架，以指导人机交互界面的设计。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统越来越自主，领域专家的监督角色变得至关重要，而目前对如何有效设计支持监督的接口了解甚少。

Method: 通过与心理学和计算机科学领域专家进行的四个共同设计工作坊，进行经验交流和界面原型设计。

Result: 本文探讨了在AI系统日益自主的背景下，领域专家在监督AI输出方面的作用，以及如何设计有效的界面来支持人类监督工作。通过与心理学和计算机科学领域专家进行的共同设计工作坊，提出了用户对监控任务的四个关键需求，并基于这些需求和SMART工作设计模型，开发了一个可推广的设计框架。

Conclusion: 本文提出了一个基于用户需求和工作设计理论的框架，期望能为人类监督AI系统的界面设计提供实质性建议，增强工作参与感与意义。

Abstract: As AI systems become increasingly capable and autonomous, domain experts'
roles are shifting from performing tasks themselves to overseeing AI-generated
outputs. Such oversight is critical, as undetected errors can have serious
consequences or undermine the benefits of AI. Effective oversight, however,
depends not only on detecting and correcting AI errors but also on the
motivation and engagement of the oversight personnel and the meaningfulness
they see in their work. Yet little is known about how domain experts approach
and experience the oversight task and what should be considered to design
effective and motivational interfaces that support human oversight. To address
these questions, we conducted four co-design workshops with domain experts from
psychology and computer science. We asked them to first oversee an AI-based
grading system, and then discuss their experiences and needs during oversight.
Finally, they collaboratively prototyped interfaces that could support them in
their oversight task. Our thematic analysis revealed four key user
requirements: understanding tasks and responsibilities, gaining insight into
the AI's decision-making, contributing meaningfully to the process, and
collaborating with peers and the AI. We integrated these empirical insights
with the SMART model of work design to develop a generalizable framework of
twelve design considerations. Our framework links interface characteristics and
user requirements to the psychological processes underlying effective and
satisfying work. Being grounded in work design theory, we expect these
considerations to be applicable across domains and discuss how they extend
existing guidelines for human-AI interaction and theoretical frameworks for
effective human oversight by providing concrete guidance on the design of
engaging and meaningful interfaces that support human oversight of AI systems.

</details>


### [15] [EasyVitessce: auto-magically adding interactivity to Scverse single-cell and spatial biology plots](https://arxiv.org/abs/2510.19532)
*Selena Luo,Mark S. Keller,Tabassum Kakar,Lisa Choy,Nils Gehlenborg*

Main category: cs.HC

TL;DR: EasyVitessce是一个Python包，通过简单的一行代码将静态图转化为交互式可视化，便于在计算笔记本和网页应用中使用。


<details>
  <summary>Details</summary>
Motivation: 提高Scverse Python绘图API的实用性，使用户能够更轻松地创建交互式图形

Method: 通过增加一行Python代码将静态Scanpy和SpatialData图转换为交互式可视化

Result: 成功实现了静态图形的互动性，用户只需简单配置即可使用Vitessce进行可视化

Conclusion: EasyVitessce简化了交互式可视化的创建过程，提升了数据可视化的便捷性和灵活性。

Abstract: EasyVitessce is a Python package that turns existing static Scanpy and
SpatialData plots into interactive visualizations by virtue of adding a single
line of Python code. The package uses Vitessce internally to render interactive
plots, and abstracts away technical details involved with configuration of
Vitessce. The resulting interactive plots can be viewed in computational
notebook environments or their configurations can be exported for usage in
other contexts such as web applications, enhancing the utility of popular
Scverse Python plotting APIs. EasyVitessce is released under the MIT License
and available on the Python Package Index (PyPI). The source code is publicly
available on GitHub.

</details>


### [16] [Unmanned Aerial Vehicles Control in a Digital Twin: Exploring the Effect of Different Points of View on User Experience in Virtual Reality](https://arxiv.org/abs/2510.19604)
*Francesco Vona,Mohamed Amer,Omar Abdellatif,Michelle Celina Hallmann,Maximilian Warsinke,Adriana-Simona Mihaita,Jan-Niklas Voigt-Antons*

Main category: cs.HC

TL;DR: 本研究探讨不同虚拟视角对无人机操控体验的影响，结果显示第一人称视角要求更高的注意力和努力，而第三人称视角更受用户青睐。


<details>
  <summary>Details</summary>
Motivation: 提高飞行员的情境意识和整体用户体验，减少人为错误。

Method: 在虚拟现实中使用数字双胞胎技术研究不同视角对无人机飞行控制的影响。

Result: 第一人称视角带来了更高的心理需求和工作量，但控制更流畅；第三人称视角最受欢迎。

Conclusion: 选择合适的视角对于提升UAV操作体验至关重要，第一人称和第三人称视角各有优缺点。

Abstract: Controlling Unmanned Aerial Vehicles (UAVs) is a cognitively demanding task,
with accidents often arising from insufficient situational awareness,
inadequate training, and poor user experiences. Providing more intuitive and
immersive visual feedback, particularly through Digital Twin technologies,
offers new opportunities to enhance pilot awareness and overall experience
quality. In this study, we investigate how different virtual points of view
(POVs) influence user experience and performance during UAV piloting in Virtual
Reality (VR), utilizing a digital twin that faithfully replicates the
real-world flight environment. We developed a VR application that enables
participants to control a physical DJI Mini 4 Pro drone while immersed in a
digital twin with four distinct camera perspectives: Baseline View (static
external), First-Person View, Chase View, and Third-Person View. Nineteen
participants completed a series of ring-based obstacle courses from each
perspective. In addition to objective flight data, we collected standardized
subjective assessments of user experience, presence, workload, cybersickness,
and situational awareness. Quantitative analyses revealed that the First-Person
View was associated with significantly higher mental demand and effort, greater
trajectory deviation, but smoother control inputs compared to the Third-Person
and Chase perspectives. Complementing these findings, preference data indicated
that the Third-Person View was most consistently favored, whereas the
First-Person View elicited polarized reactions.

</details>


### [17] [Sentiment Analysis of Social Media Data for Predicting Consumer Behavior Trends Using Machine Learning](https://arxiv.org/abs/2510.19656)
*S M Rakib Ul Karim,Rownak Ara Rasul,Tunazzina Sultana*

Main category: cs.HC

TL;DR: 本研究利用机器学习进行Twitter情感分析，揭示了消费者趋势，并开发出可扩展的框架以应对多语言数据和讽刺检测等挑战。


<details>
  <summary>Details</summary>
Motivation: 在快速技术进步的时代，社交媒体平台如Twitter成为获取消费者洞察和理解公众态度的重要工具。

Method: 使用先进的机器学习方法进行Twitter数据的情感分析，特别关注消费趋势的预测。

Result: 通过使用Sentiment140数据集，发现了以'car'为例的消费者偏好变化模式，BERT模型在情感分类和趋势预测中表现最佳。

Conclusion: LSTM和BERT模型有效捕获语言和上下文模式，提高了预测准确性，并提供了对消费者行为的洞察。

Abstract: In the era of rapid technological advancement, social media platforms such as
Twitter (X) have emerged as indispensable tools for gathering consumer
insights, capturing diverse opinions, and understanding public attitudes. This
research applies advanced machine learning methods for sentiment analysis on
Twitter data, with a focus on predicting consumer trends. Using the
Sentiment140 dataset, the study detects evolving patterns in consumer
preferences with "car" as an example. A structured workflow was used to clean
and prepare data for analysis. Machine learning models, including Support
Vector Machines (SVM), Naive Bayes, Long Short-Term Memory (LSTM) networks, and
Bidirectional Encoder Representations from Transformers (BERT), were employed
to classify sentiments and predict trends. Model performance was measured using
accuracy, precision, recall, and F1 score, with BERT achieving the highest
results (Accuracy: 83.48%, Precision: 79.37%, Recall: 90.60%, F1: 84.61).
Results show that LSTM and BERT effectively capture linguistic and contextual
patterns, improving prediction accuracy and providing insights into consumer
behavior. Temporal analysis revealed sentiment shifts across time, while Named
Entity Recognition (NER) identified related terms and themes. This research
addresses challenges like sarcasm detection and multilingual data processing,
offering a scalable framework for generating actionable consumer insights.

</details>


### [18] [Directive, Metacognitive or a Blend of Both? A Comparison of AI-Generated Feedback Types on Student Engagement, Confidence, and Outcomes](https://arxiv.org/abs/2510.19685)
*Omar Alsaiari,Nilufar Baghaei,Jason M. Lodge,Omid Noroozi,Dragan Gašević,Marie Boden,Hassan Khosravi*

Main category: cs.HC

TL;DR: 本研究探讨了指令性和元认知反馈对学生学习的影响，结果显示混合反馈能有效促进修订行为，结合即时改进指导与自我反思机会。


<details>
  <summary>Details</summary>
Motivation: 探索不同类型反馈对学生参与度、自信心和工作质量的影响，特别是在AI生成反馈的背景下。

Method: 进行了一学期的随机对照试验，涉及329名学生，比较了指令性反馈、元认知反馈及其混合反馈的效果。

Result: 混合反馈组的修订行为最为活跃，自信心评分普遍较高，而各组的资源质量结果相当。

Conclusion: 混合反馈在促进学生修订方面表现最佳，并能结合明确指导和自我反思的机会。

Abstract: Feedback is one of the most powerful influences on student learning, with
extensive research examining how best to implement it in educational settings.
Increasingly, feedback is being generated by artificial intelligence (AI),
offering scalable and adaptive responses. Two widely studied approaches are
directive feedback, which gives explicit explanations and reduces cognitive
load to speed up learning, and metacognitive feedback which prompts learners to
reflect, track their progress, and develop self-regulated learning (SRL)
skills. While both approaches have clear theoretical advantages, their
comparative effects on engagement, confidence, and quality of work remain
underexplored. This study presents a semester-long randomised controlled trial
with 329 students in an introductory design and programming course using an
adaptive educational platform. Participants were assigned to receive directive,
metacognitive, or hybrid AI-generated feedback that blended elements of both
directive and metacognitive feedback. Results showed that revision behaviour
differed across feedback conditions, with Hybrid prompting the most revisions
compared to Directive and Metacognitive. Confidence ratings were uniformly
high, and resource quality outcomes were comparable across conditions. These
findings highlight the promise of AI in delivering feedback that balances
clarity with reflection. Hybrid approaches, in particular, show potential to
combine actionable guidance for immediate improvement with opportunities for
self-reflection and metacognitive growth.

</details>


### [19] [LifeSync-Games: Toward a Video Game Paradigm for Promoting Responsible Gaming and Human Development](https://arxiv.org/abs/2510.19691)
*R. González-Ibáñez,J. Macías-Cáceres,M. Villalta-Paucar*

Main category: cs.HC

TL;DR: 本研究探讨了游戏在促进人类发展和福祉方面的潜力，提出了LifeSync-Games框架，以改进游戏的社会、心理和身体影响。


<details>
  <summary>Details</summary>
Motivation: 虽然游戏可以满足多种社会和心理需求，但其促进人类发展的潜力尚未得到充分利用，尤其是在公众讨论和监管中更多关注风险而非益处。

Method: 提出一个利用数字双胞胎的框架，结合理论基础、技术组件、设计指南和评估方法。

Result: 本研究提出了LifeSync-Games框架，以利用数字双胞胎连接虚拟游戏与现实生活活动，旨在增强游戏的开发价值，并促进身心和社会领域的成长。

Conclusion: 通过将虚拟游戏与真实活动相结合，LifeSync-Games框架能够更好地促进自我调节和全面发展。

Abstract: Technological advancements have made video games a central part of the
digital lives of nearly 3 billion people worldwide. Although games can address
various social, physical, and psychological needs, their potential to support
human development and well-being remains underutilized. Research highlights
both negative effects, such as addiction and isolation, and positive outcomes
like cognitive improvements and problem-solving skills. However, public
discourse and regulation often focus more on risks than benefits. To address
this imbalance, we present LifeSync-Games, a framework leveraging simplified
digital twins to connect virtual gameplay with real-life activities. This
reciprocal relationship aims to enhance the developmental value of gaming by
promoting self-regulation and fostering growth across physical, mental, and
social domains. We present the framework's theoretical foundations,
technological components, design guidelines, and evaluation approaches.
Additionally, we present early applications in both new and bestselling games
to demonstrate its versatility and practical relevance.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [20] [Towards Proprioceptive Terrain Mapping with Quadruped Robots for Exploration in Planetary Permanently Shadowed Regions](https://arxiv.org/abs/2510.18986)
*Alberto Sanchez-Delgado,João Carlos Virgolino Soares,Victor Barasuol,Claudio Semini*

Main category: cs.RO

TL;DR: 本文提出了一种对四足机器人在月球极地永久阴影区进行地形映射的新框架，能够通过内部传感器评估地形及其与机器人的交互，实现在模拟的月球环境中的有效应用。


<details>
  <summary>Details</summary>
Motivation: 鉴于永远阴影区可能含有水冰和地质记录，针对这些复杂地形，四足机器人能够有效获取原位数据，是未来探索的理想选择。

Method: 提出了一种基于内部传感器的地形映射框架，估算海拔、足部滑动、能量成本和稳定性边际，并集成到一个多层2.5D网格图中。

Result: 本文提出了一种四足机器人在月球极地永久阴影区进行地形映射的框架。

Conclusion: 该系统在模拟的月球环境中进行评估，验证了其在月球重力和地形条件下的一致映射性能。

Abstract: Permanently Shadowed Regions (PSRs) near the lunar poles are of interest for
future exploration due to their potential to contain water ice and preserve
geological records. Their complex, uneven terrain favors the use of legged
robots, which can traverse challenging surfaces while collecting in-situ data,
and have proven effective in Earth analogs, including dark caves, when equipped
with onboard lighting. While exteroceptive sensors like cameras and lidars can
capture terrain geometry and even semantic information, they cannot quantify
its physical interaction with the robot, a capability provided by
proprioceptive sensing. We propose a terrain mapping framework for quadruped
robots, which estimates elevation, foot slippage, energy cost, and stability
margins from internal sensing during locomotion. These metrics are
incrementally integrated into a multi-layer 2.5D gridmap that reflects terrain
interaction from the robot's perspective. The system is evaluated in a
simulator that mimics a lunar environment, using the 21 kg quadruped robot
Aliengo, showing consistent mapping performance under lunar gravity and terrain
conditions.

</details>


### [21] [Underwater Dense Mapping with the First Compact 3D Sonar](https://arxiv.org/abs/2510.18991)
*Chinmay Burgul,Yewei Huang,Michalis Chatzispyrou,Ioannis Rekleitis,Alberto Quattrini Li,Marios Xanthidis*

Main category: cs.RO

TL;DR: 本研究介绍紧凑型3D声纳在水下状态估计中的应用及挑战，提供了新型的映射和SLAM管道。


<details>
  <summary>Details</summary>
Motivation: 探讨在水下状态估计中采用紧凑型3D声纳的潜力和机会，因为现有的声学传感器在信息获取上存在限制。

Method: 本文提出声纳与摄像头间的标定流程，并研究不同表面和材料的声学响应，同时测试在复杂水下环境中的映射和SLAM管道。

Result: 首次评估紧凑型3D声纳的性能和容量，展示其在水下环境中的独特能力。

Conclusion: 3D声纳能够捕获一致的空间信息，便于详细重建和定位，但在声学传播方面仍面临挑战。

Abstract: In the past decade, the adoption of compact 3D range sensors, such as LiDARs,
has driven the developments of robust state-estimation pipelines, making them a
standard sensor for aerial, ground, and space autonomy. Unfortunately, poor
propagation of electromagnetic waves underwater, has limited the
visibility-independent sensing options of underwater state-estimation to
acoustic range sensors, which provide 2D information including, at-best,
spatially ambiguous information. This paper, to the best of our knowledge, is
the first study examining the performance, capacity, and opportunities arising
from the recent introduction of the first compact 3D sonar. Towards that
purpose, we introduce calibration procedures for extracting the extrinsics
between the 3D sonar and a camera and we provide a study on acoustic response
in different surfaces and materials. Moreover, we provide novel mapping and
SLAM pipelines tested in deployments in underwater cave systems and other
geometrically and acoustically challenging underwater environments. Our
assessment showcases the unique capacity of 3D sonars to capture consistent
spatial information allowing for detailed reconstructions and localization in
datasets expanding to hundreds of meters. At the same time it highlights
remaining challenges related to acoustic propagation, as found also in other
acoustic sensors. Datasets collected for our evaluations would be released and
shared with the community to enable further research advancements.

</details>


### [22] [SHRUMS: Sensor Hallucination for Real-time Underwater Motion Planning with a Compact 3D Sonar](https://arxiv.org/abs/2510.18996)
*Susheel Vadakkekuruppath,Herman B. Amundsen,Jason M. O'Kane,Marios Xanthidis*

Main category: cs.RO

TL;DR: 本论文提出了SHRUMS，一种新型水下3D导航方案，集成了3D声纳，能够在复杂环境中实现鲁棒导航。


<details>
  <summary>Details</summary>
Motivation: 解决水下机器人导航中的传感器缺乏和能见度极差的问题。

Method: 通过虚构传感器测量，并实时优化，同时处理复杂3D环境中的传感器数据流。

Result: SHRUMS在实际的3D声纳数据上得到了验证，并将在未来进行实际场地的部署。

Conclusion: SHRUMS是首个集成3D声纳的水下自主导航系统，能够在复杂的水下环境中展现出强大的鲁棒性。

Abstract: Autonomous navigation in 3D is a fundamental problem for autonomy. Despite
major advancements in terrestrial and aerial settings due to improved range
sensors including LiDAR, compact sensors with similar capabilities for
underwater robots have only recently become available, in the form of 3D
sonars. This paper introduces a novel underwater 3D navigation pipeline, called
SHRUMS (Sensor Hallucination for Robust Underwater Motion planning with 3D
Sonar). To the best of the authors' knowledge, SHRUMS is the first underwater
autonomous navigation stack to integrate a 3D sonar. The proposed pipeline
exhibits strong robustness while operating in complex 3D environments in spite
of extremely poor visibility conditions. To accommodate the intricacies of the
novel sensor data stream while achieving real-time locally optimal performance,
SHRUMS introduces the concept of hallucinating sensor measurements from
non-existent sensors with convenient arbitrary parameters, tailored to
application specific requirements. The proposed concepts are validated with
real 3D sonar sensor data, utilizing real inputs in challenging settings and
local maps constructed in real-time. Field deployments validating the proposed
approach in full are planned in the very near future.

</details>


### [23] [$\nabla$-SDF: Learning Euclidean Signed Distance Functions Online with Gradient-Augmented Octree Interpolation and Neural Residual](https://arxiv.org/abs/2510.18999)
*Zhirui Dai,Qihao Qian,Tianxing Fan,Nikolay Atanasov*

Main category: cs.RO

TL;DR: 提出一种新的混合方法 $
abla$-SDF，旨在提高符号距离函数的效率和准确性，以便在机器人和计算机视觉领域实现更好的性能。


<details>
  <summary>Details</summary>
Motivation: 从点云数据中估计符号距离函数（SDF）对机器人自主能力的提升十分必要，但现有方法在效率、持续性和大规模环境中存在局限。

Method: 该方法结合了显式先验和隐式神经残差，通过梯度增强的八叉树插值实现SDF重建。

Result: 提出了 $
abla$-SDF 方法，结合了基于梯度增强的八叉树插值的显式先验和隐式神经残差，实现了非截断的SDF重建，同时在计算和内存效率上与体素方法相当，在可微性和准确性上与神经网络方法相当。

Conclusion: $
abla$-SDF 方法在准确性和效率方面超越了现有技术，提供了一种可扩展的解决方案，适用于机器人和计算机视觉的下游任务。

Abstract: Estimation of signed distance functions (SDFs) from point cloud data has been
shown to benefit many robot autonomy capabilities, including localization,
mapping, motion planning, and control. Methods that support online and
large-scale SDF reconstruction tend to rely on discrete volumetric data
structures, which affect the continuity and differentiability of the SDF
estimates. Recently, using implicit features, neural network methods have
demonstrated high-fidelity and differentiable SDF reconstruction but they tend
to be less efficient, can experience catastrophic forgetting and memory
limitations in large environments, and are often restricted to truncated SDFs.
This work proposes $\nabla$-SDF, a hybrid method that combines an explicit
prior obtained from gradient-augmented octree interpolation with an implicit
neural residual. Our method achieves non-truncated (Euclidean) SDF
reconstruction with computational and memory efficiency comparable to
volumetric methods and differentiability and accuracy comparable to neural
network methods. Extensive experiments demonstrate that \methodname{}
outperforms the state of the art in terms of accuracy and efficiency, providing
a scalable solution for downstream tasks in robotics and computer vision.

</details>


### [24] [Motion Planning and Control of an Overactuated 4-Wheel Drive with Constrained Independent Steering](https://arxiv.org/abs/2510.19054)
*Shiyu Liu,Ilija Hadzic,Akshay Gupta,Aliasghar Arab*

Main category: cs.RO

TL;DR: 本论文针对具有转向约束的四轮驱动车辆，提出了一种改进的运动规划和控制方法，实现了平滑的路径跟踪和障碍物避让。


<details>
  <summary>Details</summary>
Motivation: 解决由于机械约束导致的运动不平滑和控制问题，提高四轮驱动独立转向系统的运动效率。

Method: 提出了一种运动规划和控制方法，针对具独立转向的四轮驱动（4WIS）车辆。在考虑机械约束下，该车辆的轮子无法执行完整的360度旋转。

Result: 提出的运动规划器能够实现精确的路径跟踪和障碍物避让，同时考虑了转向约束和速度转换的平滑性，并在仿真和实际机器人上得到验证。

Conclusion: 在ROS导航包的基础上实现了该运动规划器，实验结果表明该方法有效提升了运动的平滑性和效率。

Abstract: This paper addresses motion planning and con- trol of an overactuated 4-wheel
drive train with independent steering (4WIS) where mechanical constraints
prevent the wheels from executing full 360-degree rotations (swerve). The
configuration space of such a robot is constrained and contains discontinuities
that affect the smoothness of the robot motion. We introduce a mathematical
formulation of the steering constraints and derive discontinuity planes that
partition the velocity space into regions of smooth and efficient motion. We
further design the motion planner for path tracking and ob- stacle avoidance
that explicitly accounts for swerve constraints and the velocity transition
smoothness. The motion controller uses local feedback to generate actuation
from the desired velocity, while properly handling the discontinuity crossing
by temporarily stopping the motion and repositioning the wheels. We implement
the proposed motion planner as an extension to ROS Navigation package and
evaluate the system in simulation and on a physical robot.

</details>


### [25] [Convex Maneuver Planning for Spacecraft Collision Avoidance](https://arxiv.org/abs/2510.19058)
*Fausto Vega,Jon Arrizabalaga,Ryan Watson,Zachary Manchester*

Main category: cs.RO

TL;DR: 本文提出一种自动化算法，用于设计低推力的航天器碰撞避免机动，适应日益增长的低地球轨道卫星密度。


<details>
  <summary>Details</summary>
Motivation: 随着低地球轨道卫星数量增加，传统的航天器碰撞避免方法耗时且依赖人工，迫切需要一种自动化的方法来提高效率。

Method: 将问题建模为非凹二次约束二次规划（QCQP），并通过Shor松弛转化为凸半正定规划（SDP），从而获得原始问题的全局最优解。

Result: 提出了一种基于低推力的短期碰撞避免机动设计算法，并通过高保真模拟验证了其有效性。

Conclusion: 所提算法能够在确保碰撞概率的同时，实现最低能量和最低风险的碰撞避免策略，且模拟结果表明其有效性。

Abstract: Conjunction analysis and maneuver planning for spacecraft collision avoidance
remains a manual and time-consuming process, typically involving repeated
forward simulations of hand-designed maneuvers. With the growing density of
satellites in low-Earth orbit (LEO), autonomy is becoming essential for
efficiently evaluating and mitigating collisions. In this work, we present an
algorithm to design low-thrust collision-avoidance maneuvers for short-term
conjunction events. We first formulate the problem as a nonconvex
quadratically-constrained quadratic program (QCQP), which we then relax into a
convex semidefinite program (SDP) using Shor's relaxation. We demonstrate
empirically that the relaxation is tight, which enables the recovery of
globally optimal solutions to the original nonconvex problem. Our formulation
produces a minimum-energy solution while ensuring a desired probability of
collision at the time of closest approach. Finally, if the desired probability
of collision cannot be satisfied, we relax this constraint into a penalty,
yielding a minimum-risk solution. We validate our algorithm with a
high-fidelity simulation of a satellite conjunction in low-Earth orbit with a
simulated conjunction data message (CDM), demonstrating its effectiveness in
reducing collision risk.

</details>


### [26] [A Learning-based Model Reference Adaptive Controller Implemented on a Prosthetic Hand Wrist](https://arxiv.org/abs/2510.19068)
*Shifa Sulaiman,Mohammad Gohari,Francesco Schetter,Fanny Ficuciello*

Main category: cs.RO

TL;DR: 该研究提出了一种新型的神经网络控制器，能够显著提高假肢手腕的控制精度和实时性能，为可穿戴助力设备的发展提供了新的方向。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有控制策略在实时部署中缺乏适应性和高计算成本的问题，提升假肢手的功能性和自然运动能力。

Method: 提出了一种基于神经网络的模型参考自适应控制器，结合了顺应性腕机制和假肢手，利用Timoshenko梁理论进行动态建模。

Result: 仿真结果显示RMSE为$6.14 	imes 10^{-4}$ m，稳定时间为$3.2$s；实验验证表明平均RMSE为$5.66 	imes 10^{-3}$ m，稳态误差为$8.05 	imes 10^{-3}$ m，稳定时间为$1.58$s。

Conclusion: 该研究展示了一种基于神经网络的模型参考自适应控制器，能够提高软机器人手部的运动精度和响应能力，推动可穿戴辅助设备的智能控制整合。

Abstract: The functionality and natural motion of prosthetic hands remain limited by
the challenges in controlling compliant wrist mechanisms. Current control
strategies often lack adaptability and incur high computational costs, which
impedes real-time deployment in assistive robotics. To address this gap, this
study presents a computationally efficient Neural Network (NN)-based Model
Reference Adaptive Controller (MRAC) for a tendon-driven soft continuum wrist
integrated with a prosthetic hand. The dynamic modeling of the wrist is
formulated using Timoshenko beam theory, capturing both shear and bending
deformations. The proposed NN-MRAC estimates the required tendon forces from
deflection errors and minimizes deviation from a reference model through online
adaptation. Simulation results demonstrate improved precision with a root mean
square error (RMSE) of $6.14 \times 10^{-4}$ m and a settling time of $3.2$s.
Experimental validations confirm real-time applicability, with an average RMSE
of $5.66 \times 10^{-3}$ m, steady-state error of $8.05 \times 10^{-3}$ m, and
settling time of $1.58$ s. These results highlight the potential of the
controller to enhance motion accuracy and responsiveness in soft prosthetic
systems, thereby advancing the integration of adaptive intelligent control in
wearable assistive devices.

</details>


### [27] [Sample-Based Hybrid Mode Control: Asymptotically Optimal Switching of Algorithmic and Non-Differentiable Control Modes](https://arxiv.org/abs/2510.19074)
*Yilang Liu,Haoxiang You,Ian Abraham*

Main category: cs.RO

TL;DR: 本文提出一种样本基础的混合控制模式解决方案，适用于机器人任务，具有效能保证。


<details>
  <summary>Details</summary>
Motivation: 解决非可微和算法混合模式下的控制问题

Method: 基于样本的混合模式控制方法

Result: 通过整数优化选择控制模式、切换时机与持续时长，得到强有效的性能保证，适用于多种机器人任务。

Conclusion: 该方法在需要反应式切换的长时间规划和高频控制的实际机器人应用中证明了有效性。

Abstract: This paper investigates a sample-based solution to the hybrid mode control
problem across non-differentiable and algorithmic hybrid modes. Our approach
reasons about a set of hybrid control modes as an integer-based optimization
problem where we select what mode to apply, when to switch to another mode, and
the duration for which we are in a given control mode. A sample-based variation
is derived to efficiently search the integer domain for optimal solutions. We
find our formulation yields strong performance guarantees that can be applied
to a number of robotics-related tasks. In addition, our approach is able to
synthesize complex algorithms and policies to compound behaviors and achieve
challenging tasks. Last, we demonstrate the effectiveness of our approach in
real-world robotic examples that require reactive switching between long-term
planning and high-frequency control.

</details>


### [28] [Kinematic Analysis and Integration of Vision Algorithms for a Mobile Manipulator Employed Inside a Self-Driving Laboratory](https://arxiv.org/abs/2510.19081)
*Shifa Sulaiman,Tobias Busk Jensen,Stefan Hein Bengtson,Simon Bøgh*

Main category: cs.RO

TL;DR: 本研究开发了一种移动操控器，能够通过先进的视觉算法和运动学建模，在动态抓取和跟随任务中提高机器人对纹理物体的抓取能力，促进实验室的自主实验和人机协作。


<details>
  <summary>Details</summary>
Motivation: 提高实验室中机器人在自动化合成、可扩展反应工作流程和协作任务中的应用能力。

Method: 基于Denavit Hartenberg（DH）规则进行运动学建模，应用逆运动学解决方案以及视觉方法结合特征检测与单应性驱动的姿态估计。

Result: 开发了一种移动操控器，能够在自主实验室环境中协助人类操作员。

Conclusion: 该研究使得机器人能够在多样化环境中进行有效的自主操作，增强了下一代化学实验室的可扩展性和可重复性。

Abstract: Recent advances in robotics and autonomous systems have broadened the use of
robots in laboratory settings, including automated synthesis, scalable reaction
workflows, and collaborative tasks in self-driving laboratories (SDLs). This
paper presents a comprehensive development of a mobile manipulator designed to
assist human operators in such autonomous lab environments. Kinematic modeling
of the manipulator is carried out based on the Denavit Hartenberg (DH)
convention and inverse kinematics solution is determined to enable precise and
adaptive manipulation capabilities. A key focus of this research is enhancing
the manipulator ability to reliably grasp textured objects as a critical
component of autonomous handling tasks. Advanced vision-based algorithms are
implemented to perform real-time object detection and pose estimation, guiding
the manipulator in dynamic grasping and following tasks. In this work, we
integrate a vision method that combines feature-based detection with
homography-driven pose estimation, leveraging depth information to represent an
object pose as a $2$D planar projection within $3$D space. This adaptive
capability enables the system to accommodate variations in object orientation
and supports robust autonomous manipulation across diverse environments. By
enabling autonomous experimentation and human-robot collaboration, this work
contributes to the scalability and reproducibility of next-generation chemical
laboratories

</details>


### [29] [Safe Active Navigation and Exploration for Planetary Environments Using Proprioceptive Measurements](https://arxiv.org/abs/2510.19101)
*Matthew Jiang,Shipeng Liu,Feifei Qian*

Main category: cs.RO

TL;DR: SAEGT是一个四足机器人导航框架，能够在未知颗粒环境中安全探索，解决视觉传感器无法捕捉地形变形的问题。


<details>
  <summary>Details</summary>
Motivation: 旨在通过本体感知传感器提供比遥感更可靠的可通行性估计，以便在挑战性环境中引导轮式漫游者。

Method: 使用高斯过程回归来评估通过腿部与地面相互作用得到的安全区域和前沿区域，并配合一种实时的反应式控制器进行安全探索和导航。

Result: SAEGT在仿真中展示了能够安全探索和导航，且只依靠本体感知估计的可通行性来实现目标的能力。

Conclusion: SAEGT框架能够帮助四足机器人安全地探索未知的颗粒环境，特别是在视觉无法捕捉地形变形的情况下。

Abstract: Legged robots can sense terrain through force interactions during locomotion,
offering more reliable traversability estimates than remote sensing and serving
as scouts for guiding wheeled rovers in challenging environments. However, even
legged scouts face challenges when traversing highly deformable or unstable
terrain. We present Safe Active Exploration for Granular Terrain (SAEGT), a
navigation framework that enables legged robots to safely explore unknown
granular environments using proprioceptive sensing, particularly where visual
input fails to capture terrain deformability. SAEGT estimates the safe region
and frontier region online from leg-terrain interactions using Gaussian Process
regression for traversability assessment, with a reactive controller for
real-time safe exploration and navigation. SAEGT demonstrated its ability to
safely explore and navigate toward a specified goal using only proprioceptively
estimated traversability in simulation.

</details>


### [30] [A Cross-Environment and Cross-Embodiment Path Planning Framework via a Conditional Diffusion Model](https://arxiv.org/abs/2510.19128)
*Mehran Ghafarian Tamizi,Homayoun Honari,Amir Mehdi Soufi Enayati,Aleksey Nozdryn-Plotnicki,Homayoun Najjaran*

Main category: cs.RO

TL;DR: 本研究开发了一种新的路径规划框架GADGET，能够有效适应未知环境，不需重训练，实现安全的碰撞避免，且在多种机器人中表现出良好的迁移能力与性能。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决传统路径规划方法计算时间长、参数调节困难，以及现有学习基础方法无法有效泛化的问题。

Method: 介绍了GADGET，一种基于扩散模型的路径规划方法，采用混合双条件机制，将无分类器引导与分类器引导控制障碍函数（CBF）安全塑形相结合，实现环境感知与实时碰撞避免。

Result: 研究提出了一种新的路径规划框架GADGET，可以有效处理高维杂乱环境中的自主机器人路径规划问题。该方法能够在不同的环境和硬件上进行无须重训练的有效迁移，保持高成功率和低碰撞强度，且在各种实验环境中表现出良好的安全性和可靠性。

Conclusion: GADGET在新环境和多种机器人系统间具有良好的迁移能力，且在实际物理执行中能生成安全、无碰撞的轨迹，显示出其高效性与适应性。

Abstract: Path planning for a robotic system in high-dimensional cluttered environments
needs to be efficient, safe, and adaptable for different environments and
hardware. Conventional methods face high computation time and require extensive
parameter tuning, while prior learning-based methods still fail to generalize
effectively. The primary goal of this research is to develop a path planning
framework capable of generalizing to unseen environments and new robotic
manipulators without the need for retraining. We present GADGET (Generalizable
and Adaptive Diffusion-Guided Environment-aware Trajectory generation), a
diffusion-based planning model that generates joint-space trajectories
conditioned on voxelized scene representations as well as start and goal
configurations. A key innovation is GADGET's hybrid dual-conditioning mechanism
that combines classifier-free guidance via learned scene encoding with
classifier-guided Control Barrier Function (CBF) safety shaping, integrating
environment awareness with real-time collision avoidance directly in the
denoising process. This design supports zero-shot transfer to new environments
and robotic embodiments without retraining. Experimental results show that
GADGET achieves high success rates with low collision intensity in
spherical-obstacle, bin-picking, and shelf environments, with CBF guidance
further improving safety. Moreover, comparative evaluations indicate strong
performance relative to both sampling-based and learning-based baselines.
Furthermore, GADGET provides transferability across Franka Panda, Kinova Gen3
(6/7-DoF), and UR5 robots, and physical execution on a Kinova Gen3 demonstrates
its ability to generate safe, collision-free trajectories in real-world
settings.

</details>


### [31] [GRASPLAT: Enabling dexterous grasping through novel view synthesis](https://arxiv.org/abs/2510.19200)
*Matteo Bortolon,Nuno Ferreira Duarte,Plinio Moreno,Fabio Poiesi,José Santos-Victor,Alessio Del Bue*

Main category: cs.RO

TL;DR: 本文提出的新框架 GRASPLAT 利用 RGB 图像训练，实现了高效的机器人抓取，克服了 3D 数据获取的困难。


<details>
  <summary>Details</summary>
Motivation: 解决多指机器人手爪的灵活抓取问题，克服依赖完整 3D 扫描预测抓取姿势的现有方法的局限性。

Method: 通过合成真实物体与手部交互的高保真图像，利用 RGB 图像训练模型，结合 3D 高斯点云技术生成新的视图，并使用光度损失来优化抓取预测。

Result: GRASPLAT 在合成和真实世界抓取数据集上的实验表明，显著改善了抓取成功率。

Conclusion: GRASPLAT 提高了基于图像的方法的抓取成功率，最大提高了 36.9%。

Abstract: Achieving dexterous robotic grasping with multi-fingered hands remains a
significant challenge. While existing methods rely on complete 3D scans to
predict grasp poses, these approaches face limitations due to the difficulty of
acquiring high-quality 3D data in real-world scenarios. In this paper, we
introduce GRASPLAT, a novel grasping framework that leverages consistent 3D
information while being trained solely on RGB images. Our key insight is that
by synthesizing physically plausible images of a hand grasping an object, we
can regress the corresponding hand joints for a successful grasp. To achieve
this, we utilize 3D Gaussian Splatting to generate high-fidelity novel views of
real hand-object interactions, enabling end-to-end training with RGB data.
Unlike prior methods, our approach incorporates a photometric loss that refines
grasp predictions by minimizing discrepancies between rendered and real images.
We conduct extensive experiments on both synthetic and real-world grasping
datasets, demonstrating that GRASPLAT improves grasp success rates up to 36.9%
over existing image-based methods. Project page:
https://mbortolon97.github.io/grasplat/

</details>


### [32] [Hierarchical DLO Routing with Reinforcement Learning and In-Context Vision-language Models](https://arxiv.org/abs/2510.19268)
*Mingen Li,Houjian Yu,Yixuan Huang,Youngjin Hong,Changhyun Choi*

Main category: cs.RO

TL;DR: 本文提出一种自主的分层框架，通过视觉-语言模型和强化学习解决可变形线性物体的长时间路由任务，成功率为92.5%。


<details>
  <summary>Details</summary>
Motivation: 解决可变形线性物体(DLO)的长时间路由任务所面临的非线性动态和技能执行问题。

Method: 提出一种完全自主的分层框架，结合视觉-语言模型和强化学习实现低级技能执行。

Result: 与次最佳基线方法相比，该方法的表现提高了近50%。

Conclusion: 该方法在长时间路由任务中表现出色，总成功率达到92.5%。

Abstract: Long-horizon routing tasks of deformable linear objects (DLOs), such as
cables and ropes, are common in industrial assembly lines and everyday life.
These tasks are particularly challenging because they require robots to
manipulate DLO with long-horizon planning and reliable skill execution.
Successfully completing such tasks demands adapting to their nonlinear
dynamics, decomposing abstract routing goals, and generating multi-step plans
composed of multiple skills, all of which require accurate high-level reasoning
during execution. In this paper, we propose a fully autonomous hierarchical
framework for solving challenging DLO routing tasks. Given an implicit or
explicit routing goal expressed in language, our framework leverages
vision-language models~(VLMs) for in-context high-level reasoning to synthesize
feasible plans, which are then executed by low-level skills trained via
reinforcement learning. To improve robustness in long horizons, we further
introduce a failure recovery mechanism that reorients the DLO into
insertion-feasible states. Our approach generalizes to diverse scenes involving
object attributes, spatial descriptions, as well as implicit language commands.
It outperforms the next best baseline method by nearly 50% and achieves an
overall success rate of 92.5% across long-horizon routing scenarios.

</details>


### [33] [TARMAC: A Taxonomy for Robot Manipulation in Chemistry](https://arxiv.org/abs/2510.19289)
*Kefeng Huang,Jonathon Pipe,Alice E. Martin,Tianyuan Wang,Barnabas A. Franklin,Andy M. Tyrrell,Ian J. S. Fairlamb,Jihong Zhu*

Main category: cs.RO

TL;DR: 本文提出了一种名为TARMAC的分类法，用于提升化学实验室的自动化水平，减少人类干预，并提供灵活的机器人操作技能。


<details>
  <summary>Details</summary>
Motivation: 旨在减少化学实验室自动化中对人类干预的依赖，并提升现有机器人的独立性与适应性。

Method: 通过注释的教学实验室演示进行实验验证，介绍了一种针对化学操控的分类法。

Result: TARMAC框架成功分类并组织了实验室所需的核心操控技能，使机器人能够执行更复杂的任务。

Conclusion: 提出的TARMAC框架为化学实验室的自动化提供了系统化和可扩展的基础，促进了机器人技能的灵活使用和长期工作流程的整合。

Abstract: Chemistry laboratory automation aims to increase throughput, reproducibility,
and safety, yet many existing systems still depend on frequent human
intervention. Advances in robotics have reduced this dependency, but without a
structured representation of the required skills, autonomy remains limited to
bespoke, task-specific solutions with little capacity to transfer beyond their
initial design. Current experiment abstractions typically describe
protocol-level steps without specifying the robotic actions needed to execute
them. This highlights the lack of a systematic account of the manipulation
skills required for robots in chemistry laboratories. To address this gap, we
introduce TARMAC - a Taxonomy for Robot Manipulation in Chemistry - a
domain-specific framework that defines and organizes the core manipulations
needed in laboratory practice. Based on annotated teaching-lab demonstrations
and supported by experimental validation, TARMAC categorizes actions according
to their functional role and physical execution requirements. Beyond serving as
a descriptive vocabulary, TARMAC can be instantiated as robot-executable
primitives and composed into higher-level macros, enabling skill reuse and
supporting scalable integration into long-horizon workflows. These
contributions provide a structured foundation for more flexible and autonomous
laboratory automation. More information is available at
https://tarmac-paper.github.io/

</details>


### [34] [Imitation Learning Policy based on Multi-Step Consistent Integration Shortcut Model](https://arxiv.org/abs/2510.19356)
*Yu Fang,Xinyu Wang,Xuehe Zhang,Wanli Xue,Mingwei Zhang,Shengyong Chen,Jie Zhao*

Main category: cs.RO

TL;DR: 本文提出一种新方法，通过改进损失函数和优化算法来提高机器人模仿学习的推断速度和性能


<details>
  <summary>Details</summary>
Motivation: 为了解决流匹配方法在推断时间上高的效率问题

Method: 提出一种具有多步集成的单步快捷方法用于机器人模仿学习

Result: 通过扩展多步一致性损失，拆分单步损失并提高单步推断性能，同时提出自适应梯度分配方法增强学习过程的稳定性，验证了所提算法的有效性

Conclusion: 实验结果表明，所提出的方法在多种模拟基准和现实环境任务中有效提升了性能。

Abstract: The wide application of flow-matching methods has greatly promoted the
development of robot imitation learning. However, these methods all face the
problem of high inference time. To address this issue, researchers have
proposed distillation methods and consistency methods, but the performance of
these methods still struggles to compete with that of the original diffusion
models and flow-matching models. In this article, we propose a one-step
shortcut method with multi-step integration for robot imitation learning. To
balance the inference speed and performance, we extend the multi-step
consistency loss on the basis of the shortcut model, split the one-step loss
into multi-step losses, and improve the performance of one-step inference.
Secondly, to solve the problem of unstable optimization of the multi-step loss
and the original flow-matching loss, we propose an adaptive gradient allocation
method to enhance the stability of the learning process. Finally, we evaluate
the proposed method in two simulation benchmarks and five real-world
environment tasks. The experimental results verify the effectiveness of the
proposed algorithm.

</details>


### [35] [ProTerrain: Probabilistic Physics-Informed Rough Terrain World Modeling](https://arxiv.org/abs/2510.19364)
*Golnaz Raja,Ruslan Agishev,Miloš Prágr,Joni Pajarinen,Karel Zimmermann,Arun Kumar Singh,Reza Ghabcheloo*

Main category: cs.RO

TL;DR: 提出了一种新的概率框架，改进了不确定性估计和轨迹预测，适用于复杂的非结构化环境。


<details>
  <summary>Details</summary>
Motivation: 针对现有方法忽略3D空间数据局部相关性的不足，提出新的方法解决不确定性较高的非结构化环境中的机器人运动预测问题。

Method: 通过结构卷积运算符，结合可微分物理引擎，对地形参数的空间相关不确定性进行建模和传播，进行轨迹预测。

Result: 本文提出了一种高效的概率框架，用于明确建模地形参数的空间相关的真实性不确定性，并通过可微分物理引擎传播这种不确定性以进行轨迹预测。该方法能够在保证计算成本可控的情况下，提供高分辨率的多变量预测，并在实验证明中显示出显著改善的不确定性估计和轨迹预测准确性。

Conclusion: 该研究提供了一种新的方法来解决高不确定性环境中机器人运动预测的问题，显著提升了预测的可靠性和精度。

Abstract: Uncertainty-aware robot motion prediction is crucial for downstream
traversability estimation and safe autonomous navigation in unstructured,
off-road environments, where terrain is heterogeneous and perceptual
uncertainty is high. Most existing methods assume deterministic or spatially
independent terrain uncertainties, ignoring the inherent local correlations of
3D spatial data and often producing unreliable predictions. In this work, we
introduce an efficient probabilistic framework that explicitly models spatially
correlated aleatoric uncertainty over terrain parameters as a probabilistic
world model and propagates this uncertainty through a differentiable physics
engine for probabilistic trajectory forecasting. By leveraging structured
convolutional operators, our approach provides high-resolution multivariate
predictions at manageable computational cost. Experimental evaluation on a
publicly available dataset shows significantly improved uncertainty estimation
and trajectory prediction accuracy over aleatoric uncertainty estimation
baselines.

</details>


### [36] [Using Temperature Sampling to Effectively Train Robot Learning Policies on Imbalanced Datasets](https://arxiv.org/abs/2510.19373)
*Basavasagar Patil,Sydney Belt,Jayjun Lee,Nima Fazeli,Bernadette Bucher*

Main category: cs.RO

TL;DR: 本文提出了一种采样策略来解决机器人任务数据集中的操作不平衡问题，经过验证，该策略在低资源任务上显著提高了性能，同时保持高资源任务的表现。


<details>
  <summary>Details</summary>
Motivation: 随着机器人操作和传感器观察数据集的增大，现有的数据集在物理操作序列上存在不平衡性，这可能影响模型的训练效果。

Method: 通过几行代码集成到现有代码基础中，实施简单的采样策略，以改善战略训练。

Result: 提出了一种简单的采样策略，以缓解数据集中物理操作的不平衡性，增强模型的泛化能力。

Conclusion: 通过实验证明，该方法可有效提升多任务模型的能力，并在真实环境下验证了其有效性。

Abstract: Increasingly large datasets of robot actions and sensory observations are
being collected to train ever-larger neural networks. These datasets are
collected based on tasks and while these tasks may be distinct in their
descriptions, many involve very similar physical action sequences (e.g., 'pick
up an apple' versus 'pick up an orange'). As a result, many datasets of robotic
tasks are substantially imbalanced in terms of the physical robotic actions
they represent. In this work, we propose a simple sampling strategy for policy
training that mitigates this imbalance. Our method requires only a few lines of
code to integrate into existing codebases and improves generalization. We
evaluate our method in both pre-training small models and fine-tuning large
foundational models. Our results show substantial improvements on low-resource
tasks compared to prior state-of-the-art methods, without degrading performance
on high-resource tasks. This enables more effective use of model capacity for
multi-task policies. We also further validate our approach in a real-world
setup on a Franka Panda robot arm across a diverse set of tasks.

</details>


### [37] [Risk Assessment of an Autonomous Underwater Snake Robot in Confined Operations](https://arxiv.org/abs/2510.19415)
*Abdelrahman Sayed Sayed*

Main category: cs.RO

TL;DR: 本文提出一种贝叶斯方法来评估Eely在海洋探索中的失落风险，并通过敏感性分析识别影响失落的主要因素。


<details>
  <summary>Details</summary>
Motivation: 随着对海洋探索的兴趣日益增长，对受限和苛刻环境中的检查和干预需求也随之增加。

Method: 采用贝叶斯方法对Eely在复杂任务场景中的风险进行评估，并进行敏感性分析。

Result: 提出了一种贝叶斯方法来评估在两种任务场景中失去Eely的风险，旨在提高Eely的性能和任务成功的可能性。

Conclusion: 敏感性分析结果表明影响失去Eely的因素，可以帮助提高其在复杂环境中的操作能力。

Abstract: The growing interest in ocean discovery imposes a need for inspection and
intervention in confined and demanding environments. Eely's slender shape, in
addition to its ability to change its body configurations, makes articulated
underwater robots an adequate option for such environments. However, operation
of Eely in such environments imposes demanding requirements on the system, as
it must deal with uncertain and unstructured environments, extreme
environmental conditions, and reduced navigational capabilities. This paper
proposes a Bayesian approach to assess the risks of losing Eely during two
mission scenarios. The goal of this work is to improve Eely's performance and
the likelihood of mission success. Sensitivity analysis results are presented
in order to demonstrate the causes having the highest impact on losing Eely.

</details>


### [38] [GigaBrain-0: A World Model-Powered Vision-Language-Action Model](https://arxiv.org/abs/2510.19430)
*GigaBrain Team,Angen Ye,Boyuan Wang,Chaojun Ni,Guan Huang,Guosheng Zhao,Haoyun Li,Jie Li,Jiagang Zhu,Lv Feng,Peng Li,Qiuping Deng,Runqi Ouyang,Wenkang Qin,Xinze Chen,Xiaofeng Wang,Yang Wang,Yifan Li,Yilong Li,Yiran Ding,Yuan Xu,Yun Ye,Yukun Zhou,Zhehao Dong,Zhenan Wang,Zhichao Liu,Zheng Zhu*

Main category: cs.RO

TL;DR: GigaBrain-0是一个新型的视觉-语言-动作(VLA)基础模型，通过生成世界模型数据降低对真实机器人数据的依赖，提升跨任务的泛化能力与政策的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决当前VLA系统因需收集大规模真实机器人数据而导致的低效率和扩展性限制。

Method: 通过生成多样化的世界模型数据和实施RGBD输入建模，以及使用具身的思维链监督，提升模型的推理能力。

Result: GigaBrain-0在灵活、长期及移动操纵任务上实现了显著的实地性能提升，并且开发了一个轻量级的GigaBrain-0-Small版本，适用于高效运行于特定设备。

Conclusion: GigaBrain-0显著提高了机器人在复杂任务上的实地表现，并展示了在不同外观和环境条件下的优越泛化能力。

Abstract: Training Vision-Language-Action (VLA) models for generalist robots typically
requires large-scale real-world robot data, which is expensive and
time-consuming to collect. The inefficiency of physical data collection
severely limits the scalability, and generalization capacity of current VLA
systems. To address this challenge, we introduce GigaBrain-0, a novel VLA
foundation model empowered by world model-generated data (e.g., video
generation, real2real transfer, human transfer, view transfer, sim2real
transfer data). By leveraging world models to generate diverse data at scale,
GigaBrain-0 significantly reduces reliance on real robot data while improving
cross-task generalization. Our approach further improves policy robustness
through RGBD input modeling and embodied Chain-of-Thought (CoT) supervision,
enabling the model to reason about spatial geometry, object states, and
long-horizon dependencies during task execution. This leads to substantial
gains in real-world performance on dexterous, long-horizon, and mobile
manipulation tasks. Extensive experiments demonstrate that GigaBrain-0 achieves
superior generalization across variations in appearances (e.g., textures,
colors), object placements, and camera viewpoints. Additionally, we present
GigaBrain-0-Small, an optimized lightweight variant designed to run efficiently
on devices such as the NVIDIA Jetson AGX Orin.

</details>


### [39] [Using Non-Expert Data to Robustify Imitation Learning via Offline Reinforcement Learning](https://arxiv.org/abs/2510.19495)
*Kevin Huang,Rosario Scalise,Cleah Winston,Ayush Agrawal,Yunchu Zhang,Rohan Baijal,Markus Grotz,Byron Boots,Benjamin Burchfiel,Hongkai Dai,Masha Itkina,Paarth Shah,Abhishek Gupta*

Main category: cs.RO

TL;DR: 论文探讨了如何利用非专家数据提升模仿学习的性能，通过离线强化学习的方法来解决传统模仿学习对高质量专家数据的依赖问题。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法过于依赖高质量专家数据，限制了其在复杂真实场景下的适应性，而非专家数据的有效利用可以降低数据收集成本，提高学习效率。

Method: 提出了简单的算法修改，使得离线强化学习可以在稀疏数据覆盖环境中充分利用非专家数据。

Result: 通过引入非专家数据，显著提高了机器人在操作任务中的学习效果，扩大了成功的初始条件范围，提升了任务导向的策略性能。

Conclusion: 采用合适的算法设计，能够有效利用非专家数据，增强机器人模仿学习算法的鲁棒性和泛化能力。

Abstract: Imitation learning has proven effective for training robots to perform
complex tasks from expert human demonstrations. However, it remains limited by
its reliance on high-quality, task-specific data, restricting adaptability to
the diverse range of real-world object configurations and scenarios. In
contrast, non-expert data -- such as play data, suboptimal demonstrations,
partial task completions, or rollouts from suboptimal policies -- can offer
broader coverage and lower collection costs. However, conventional imitation
learning approaches fail to utilize this data effectively. To address these
challenges, we posit that with right design decisions, offline reinforcement
learning can be used as a tool to harness non-expert data to enhance the
performance of imitation learning policies. We show that while standard offline
RL approaches can be ineffective at actually leveraging non-expert data under
the sparse data coverage settings typically encountered in the real world,
simple algorithmic modifications can allow for the utilization of this data,
without significant additional assumptions. Our approach shows that broadening
the support of the policy distribution can allow imitation algorithms augmented
by offline RL to solve tasks robustly, showing considerably enhanced recovery
and generalization behavior. In manipulation tasks, these innovations
significantly increase the range of initial conditions where learned policies
are successful when non-expert data is incorporated. Moreover, we show that
these methods are able to leverage all collected data, including partial or
suboptimal demonstrations, to bolster task-directed policy performance. This
underscores the importance of algorithmic techniques for using non-expert data
for robust policy learning in robotics.

</details>


### [40] [Optimizing Prosthetic Wrist Movement: A Model Predictive Control Approach](https://arxiv.org/abs/2510.19541)
*Francesco Schetter,Shifa Sulaiman,Shoby George,Paolino De Risi,Fanny Ficuciello*

Main category: cs.RO

TL;DR: 本研究实施了模型预测控制策略，提高了假肢手的灵活性和直观性，优化了动态用户交互的运动调整。


<details>
  <summary>Details</summary>
Motivation: 提高假肢手的适应性和性能。

Method: 采用模型预测控制策略

Result: 通过模拟和实验验证，MPC显著优化了手腕关节运动和用户控制。

Conclusion: 该技术显著提升了假肢手的灵巧度，为智能假肢系统的发展提供了有希望的方向。

Abstract: The integration of advanced control strategies into prosthetic hands is
essential to improve their adaptability and performance. In this study, we
present an implementation of a Model Predictive Control (MPC) strategy to
regulate the motions of a soft continuum wrist section attached to a
tendon-driven prosthetic hand with less computational effort. MPC plays a
crucial role in enhancing the functionality and responsiveness of prosthetic
hands. By leveraging predictive modeling, this approach enables precise
movement adjustments while accounting for dynamic user interactions. This
advanced control strategy allows for the anticipation of future movements and
adjustments based on the current state of the prosthetic device and the
intentions of the user. Kinematic and dynamic modelings are performed using
Euler-Bernoulli beam and Lagrange methods respectively. Through simulation and
experimental validations, we demonstrate the effectiveness of MPC in optimizing
wrist articulation and user control. Our findings suggest that this technique
significantly improves the prosthetic hand dexterity, making movements more
natural and intuitive. This research contributes to the field of robotics and
biomedical engineering by offering a promising direction for intelligent
prosthetic systems.

</details>


### [41] [LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language Navigation in Continuous Environments](https://arxiv.org/abs/2510.19655)
*Hongyu Ding,Ziming Xu,Yudong Fang,You Wu,Zixuan Chen,Jieqi Shi,Jing Huo,Yifan Zhang,Yang Gao*

Main category: cs.RO

TL;DR: LaViRA是一个有效的零-shot视觉-语言导航框架，通过分解动作层级来解决环境泛化问题，显著提高了在VLN-CE基准测试中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决当前方法在场景泛化中的局限性，尤其是在同时利用大型模型推理能力与不依赖于环境特定的路径预测方面的权衡。

Method: 建立了一个将动作分解为粗到细的层级结构的零-shot框架，包含语言动作、视觉动作和机器人动作。

Result: LaViRA在未见环境中展示了更强的泛化能力，利用不同规模的多模态大型语言模型的优点来增强推理、定位和控制的能力。

Conclusion: LaViRA在VLN-CE基准测试中显著优于现有的最新方法，具备在未见环境中出色的泛化能力，同时保持了透明性和效率，适用于实际部署。

Abstract: Zero-shot Vision-and-Language Navigation in Continuous Environments (VLN-CE)
requires an agent to navigate unseen environments based on natural language
instructions without any prior training. Current methods face a critical
trade-off: either rely on environment-specific waypoint predictors that limit
scene generalization, or underutilize the reasoning capabilities of large
models during navigation. We introduce LaViRA, a simple yet effective zero-shot
framework that addresses this dilemma by decomposing action into a
coarse-to-fine hierarchy: Language Action for high-level planning, Vision
Action for perceptual grounding, and Robot Action for robust navigation. This
modular decomposition allows us to leverage the distinct strengths of different
scales of Multimodal Large Language Models (MLLMs) at each stage, creating a
system that is powerful in its reasoning, grounding and practical control.
LaViRA significantly outperforms existing state-of-the-art methods on the
VLN-CE benchmark, demonstrating superior generalization capabilities in unseen
environments, while maintaining transparency and efficiency for real-world
deployment.

</details>


### [42] [Fast Marker Detection for UV-Based Visual Relative Localisation in Agile UAV Swarms](https://arxiv.org/abs/2510.19663)
*Vojtěch Vrba,Viktor Walter,Petr Štěpán,Martin Saska*

Main category: cs.RO

TL;DR: 本论文介绍了一种用于快速检测孤立标记的创新方法，适用于灵活的无人机群体，以提升实时定位系统的性能。


<details>
  <summary>Details</summary>
Motivation: 随着灵活无人机群体技术的发展，实时定位系统中对孤立标记的快速检测变得愈加重要，亟需提升现有检测方法的性能。

Method: 论文中提出了一种三重创新，包括针对CPU的优化流程、GPU着色器程序和功能相同的FPGA流架构，显著加快了输入相机帧的处理速度。

Result: 本论文提出了一种新颖的方法，以实现快速在机载系统中检测孤立标记，从而进行多队友在灵活无人机（UAV）群体中的视觉相对定位。

Conclusion: 所提出的解决方案在多种嵌入式平台上经过评估，证明了其在低端无人机和微型无人机应用中的有效性和可行性，成为灵活无人机群体技术的重要支撑。

Abstract: A novel approach for the fast onboard detection of isolated markers for
visual relative localisation of multiple teammates in agile UAV swarms is
introduced in this paper. As the detection forms a key component of real-time
localisation systems, a three-fold innovation is presented, consisting of an
optimised procedure for CPUs, a GPU shader program, and a functionally
equivalent FPGA streaming architecture. For the proposed CPU and GPU solutions,
the mean processing time per pixel of input camera frames was accelerated by
two to three orders of magnitude compared to the state of the art. For the
localisation task, the proposed FPGA architecture offered the most significant
overall acceleration by minimising the total delay from camera exposure to
detection results. Additionally, the proposed solutions were evaluated on
various 32-bit and 64-bit embedded platforms to demonstrate their efficiency,
as well as their feasibility for applications using low-end UAVs and MAVs.
Thus, it has become a crucial enabling technology for agile UAV swarming.

</details>


### [43] [Learning Affordances at Inference-Time for Vision-Language-Action Models](https://arxiv.org/abs/2510.19752)
*Ameesh Shah,William Chen,Adwait Godbole,Federico Mora,Sanjit A. Seshia,Sergey Levine*

Main category: cs.RO

TL;DR: 本文提出LITEN，一种结合低层VLA和高层VLM的方法，能够在失败后通过反思调整行为，生成更有效的任务执行计划。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在失败后缺乏动态调整行为的能力，无法有效应对复杂控制任务

Method: 引入学习推理时执行（LITEN），将VLA低层策略与高层VLM连接

Result: 实验结果显示LITEN能够有效学习过去经验，生成使用高效指令的计划来完成长期任务

Conclusion: LITEN通过反思执行结果，能够在复杂的真实世界任务中有效学习和调整策略。

Abstract: Solving complex real-world control tasks often takes multiple tries: if we
fail at first, we reflect on what went wrong, and change our strategy
accordingly to avoid making the same mistake. In robotics,
Vision-Language-Action models (VLAs) offer a promising path towards solving
complex control tasks, but lack the ability to contextually and dynamically
readjust behavior when they fail to accomplish a task. In this work, we
introduce Learning from Inference-Time Execution (LITEN), which connects a VLA
low-level policy to a high-level VLM that conditions on past experiences by
including them in-context, allowing it to learn the affordances and
capabilities of the low-level VLA. Our approach iterates between a reasoning
phase that generates and executes plans for the low-level VLA, and an
assessment phase that reflects on the resulting execution and draws useful
conclusions to be included in future reasoning contexts. Unlike similar
approaches to self-refinement in non-robotics domains, LITEN must reflect on
unstructured real-world robot trajectories (e.g., raw videos), which requires
structured guiderails during assessment. Our experimental results demonstrate
LITEN is able to effectively learn from past experience to generate plans that
use high-affordance instructions to accomplish long-horizon tasks.

</details>


### [44] [SEA: Semantic Map Prediction for Active Exploration of Uncertain Areas](https://arxiv.org/abs/2510.19766)
*Hongyu Ding,Xinyue Liang,Yudong Fang,You Wu,Jieqi Shi,Jing Huo,Wenbin Li,Jing Wu,Yu-Kun Lai,Yang Gao*

Main category: cs.RO

TL;DR: 提出一种通过语义地图预测和强化学习的分层探索策略的机器人探索新方法，超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 借助对环境的长期理解，提高机器人的探索效率，填补现有学习方法在单步路径预测中的不足。

Method: 采用迭代预测-探索框架，基于当前观测显式预测地图缺失区域，并使用实际累积地图与预测地图的差异指导探索，同时结合强化学习更新长期探索策略。

Result: 本文提出了一种新的主动机器人探索方法SEA，该方法通过语义地图预测和基于强化学习的分层探索策略来实现。

Conclusion: 实验结果表明，所提出的方法在相同时间限制下显著超越了现有的最先进探索策略。

Abstract: In this paper, we propose SEA, a novel approach for active robot exploration
through semantic map prediction and a reinforcement learning-based hierarchical
exploration policy. Unlike existing learning-based methods that rely on
one-step waypoint prediction, our approach enhances the agent's long-term
environmental understanding to facilitate more efficient exploration. We
propose an iterative prediction-exploration framework that explicitly predicts
the missing areas of the map based on current observations. The difference
between the actual accumulated map and the predicted global map is then used to
guide exploration. Additionally, we design a novel reward mechanism that
leverages reinforcement learning to update the long-term exploration
strategies, enabling us to construct an accurate semantic map within limited
steps. Experimental results demonstrate that our method significantly
outperforms state-of-the-art exploration strategies, achieving superior
coverage ares of the global map within the same time constraints.

</details>
