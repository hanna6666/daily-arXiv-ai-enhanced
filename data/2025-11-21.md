<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 22]
- [cs.HC](#cs.HC) [Total: 9]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Gimballed Rotor Mechanism for Omnidirectional Quadrotors](https://arxiv.org/abs/2511.15909)
*J. Cristobal,A. Z. Zain Aldeen,M. Izadi,R. Faieghi*

Main category: cs.RO

TL;DR: 本文提出了一种万向旋翼机制，为构建全向四旋翼提供了模块化和高效的解决方案，支持六自由度独立运动，并通过控制算法和飞行测试验证了该方案的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有四旋翼在结构上受到限制，无法实现六自由度的独立运动。

Method: 设计一种万向旋翼机构用于构建全向四旋翼

Result: 提出的万向旋翼系统能保持轻量化及易集成的设计，且在四旋翼中心结构无重大改动的情况下，实现各旋翼独立倾斜。通过PX4自动驾驶仪开发了新的控制分配方案，并成功完成飞行测试。

Conclusion: 所提万向旋翼设计实现了轻量化与功能性的结合，为未来四旋翼的发展提供了新思路。

Abstract: This paper presents the design of a gimballed rotor mechanism as a modular and efficient solution for constructing omnidirectional quadrotors. Unlike conventional quadrotors, which are underactuated, this class of quadrotors achieves full actuation, enabling independent motion in all six degrees of freedom. While existing omnidirectional quadrotor designs often require significant structural modifications, the proposed gimballed rotor system maintains a lightweight and easy-to-integrate design by incorporating servo motors within the rotor platforms, allowing independent tilting of each rotor without major alterations to the central structure of a quadrotor. To accommodate this unconventional design, we develop a new control allocation scheme in PX4 Autopilot and present successful flight tests, validating the effectiveness of the proposed approach.

</details>


### [2] [I've Changed My Mind: Robots Adapting to Changing Human Goals during Collaboration](https://arxiv.org/abs/2511.15914)
*Debasmita Ghose,Oz Gitelson,Ryan Jin,Grace Abawe,Marynel Vazquez,Brian Scassellati*

Main category: cs.RO

TL;DR: 提出一种方法，通过监测候选行动序列及其可行性验证，提升机器人对人类目标变化的适应能力，尤其在合作烹饪任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 人类在任务执行中经常改变目标，而现有方法通常假设目标是固定的，导致目标预测被简化为一次性推断

Method: 监测候选行动序列，并通过策略库验证其可行性来检测目标变化

Result: 在合作烹饪环境中评估，方法优于所有基线，快速收敛到正确目标，减少任务完成时间，提高协作效率

Conclusion: 该方法显著提高了机器人与人类的协作效率，尤其是在面对动态变化的目标时。

Abstract: For effective human-robot collaboration, a robot must align its actions with human goals, even as they change mid-task. Prior approaches often assume fixed goals, reducing goal prediction to a one-time inference. However, in real-world scenarios, humans frequently shift goals, making it challenging for robots to adapt without explicit communication. We propose a method for detecting goal changes by tracking multiple candidate action sequences and verifying their plausibility against a policy bank. Upon detecting a change, the robot refines its belief in relevant past actions and constructs Receding Horizon Planning (RHP) trees to actively select actions that assist the human while encouraging Differentiating Actions to reveal their updated goal. We evaluate our approach in a collaborative cooking environment with up to 30 unique recipes and compare it to three comparable human goal prediction algorithms. Our method outperforms all baselines, quickly converging to the correct goal after a switch, reducing task completion time, and improving collaboration efficiency.

</details>


### [3] [The Role of Consequential and Functional Sound in Human-Robot Interaction: Toward Audio Augmented Reality Interfaces](https://arxiv.org/abs/2511.15956)
*Aliyah Smith,Monroe Kennedy*

Main category: cs.RO

TL;DR: 本研究探讨了功能性声音对人类感知和行为的影响，结果表明机器人声音设计可增强人机协作。


<details>
  <summary>Details</summary>
Motivation: 随着机器人在日常生活中的广泛应用，理解机器人与人类的声音交互变得至关重要。

Method: 通过定位和交接任务，考察了机器人发出的声音对人类感知的影响。

Result: 功能声音未对感知产生负面影响，空间声音能够传达任务相关信息并改善用户体验。

Conclusion: 功能性和变革性声音设计有助于改善人机协作，并为未来基于声音的交互策略提供指导。

Abstract: As robots become increasingly integrated into everyday environments, understanding how they communicate with humans is critical. Sound offers a powerful channel for interaction, encompassing both operational noises and intentionally designed auditory cues. In this study, we examined the effects of consequential and functional sounds on human perception and behavior, including a novel exploration of spatial sound through localization and handover tasks. Results show that consequential sounds of the Kinova Gen3 manipulator did not negatively affect perceptions, spatial localization is highly accurate for lateral cues but declines for frontal cues, and spatial sounds can simultaneously convey task-relevant information while promoting warmth and reducing discomfort. These findings highlight the potential of functional and transformative auditory design to enhance human-robot collaboration and inform future sound-based interaction strategies.

</details>


### [4] [PushingBots: Collaborative Pushing via Neural Accelerated Combinatorial Hybrid Optimization](https://arxiv.org/abs/2511.15995)
*Zili Tang,Ying Zhang,Meng Guo*

Main category: cs.RO

TL;DR: 本文提出一种基于组合混合优化的多机器人协作推送方法，能够有效应对复杂环境中的物体推送问题。


<details>
  <summary>Details</summary>
Motivation: 解决多机器人协作推送任意形状物体的问题，尤其是涉及复杂环境和不确定性的挑战。

Method: 基于组合混合优化的方法

Result: 方法在多种机器人和形状物体的情况下经过广泛验证，效果显著。

Conclusion: 所提方法在不同数量的机器人和泛形物体上经过模拟和实际实验验证，其效率和有效性得到了广泛认可。

Abstract: Many robots are not equipped with a manipulator and many objects are not suitable for prehensile manipulation (such as large boxes and cylinders). In these cases, pushing is a simple yet effective non-prehensile skill for robots to interact with and further change the environment. Existing work often assumes a set of predefined pushing modes and fixed-shape objects. This work tackles the general problem of controlling a robotic fleet to push collaboratively numerous arbitrary objects to respective destinations, within complex environments of cluttered and movable obstacles. It incorporates several characteristic challenges for multi-robot systems such as online task coordination under large uncertainties of cost and duration, and for contact-rich tasks such as hybrid switching among different contact modes, and under-actuation due to constrained contact forces. The proposed method is based on combinatorial hybrid optimization over dynamic task assignments and hybrid execution via sequences of pushing modes and associated forces. It consists of three main components: (I) the decomposition, ordering and rolling assignment of pushing subtasks to robot subgroups; (II) the keyframe guided hybrid search to optimize the sequence of parameterized pushing modes for each subtask; (III) the hybrid control to execute these modes and transit among them. Last but not least, a diffusion-based accelerator is adopted to predict the keyframes and pushing modes that should be prioritized during hybrid search; and further improve planning efficiency. The framework is complete under mild assumptions. Its efficiency and effectiveness under different numbers of robots and general-shaped objects are validated extensively in simulations and hardware experiments, as well as generalizations to heterogeneous robots, planar assembly and 6D pushing.

</details>


### [5] [Semantic Glitch: Agency and Artistry in an Autonomous Pixel Cloud](https://arxiv.org/abs/2511.16048)
*Qing Zhang,Jing Huang,Mingyang Xu,Jun Rekimoto*

Main category: cs.RO

TL;DR: 本论文提出“语义故障”，一种低保真飞行机器人艺术装置，使用多模态大语言模型进行导航，强调机器人个性而非效率。


<details>
  <summary>Details</summary>
Motivation: 在主流机器人追求精确性能的同时，探索一种创造性的低保真方法。

Method: 论文描述了一种新的自主导航管道，利用多模态大语言模型的语义理解进行导航，避免传统传感器。

Result: 本论文探索了一种低保真（lo-fi）机器人方法，展示了如何利用语义理解来创造有个性的飞行机器人艺术装置，称为“语义故障”。该装置的外形模仿3D像素风格的云，通过小说的自主导航管道，在不依赖传统传感器的情况下，仅利用多模态大语言模型的语义理解进行导航。通过为该机器人构建生物启发的个性，作者创建了一个“叙事思维”，与其具有弱历史负载的身体相辅相成。分析包括一个自行飞行的13分钟日志，并通过统计研究验证了该框架的鲁棒性，发掘出从标志物导航到“计划执行”缺口等诸多显现行为。最终，证明了这种低保真框架能够创造个性化的、不完美的机器人伴侣，其成功不在于效率，而在于个性。

Conclusion: 本研究展示了通过低保真框架创造个性化伴侣的潜力，强调其行为的不可预测性和叙事性。

Abstract: While mainstream robotics pursues metric precision and flawless performance, this paper explores the creative potential of a deliberately "lo-fi" approach. We present the "Semantic Glitch," a soft flying robotic art installation whose physical form, a 3D pixel style cloud, is a "physical glitch" derived from digital archaeology. We detail a novel autonomous pipeline that rejects conventional sensors like LiDAR and SLAM, relying solely on the qualitative, semantic understanding of a Multimodal Large Language Model to navigate. By authoring a bio-inspired personality for the robot through a natural language prompt, we create a "narrative mind" that complements the "weak," historically, loaded body. Our analysis begins with a 13-minute autonomous flight log, and a follow-up study statistically validates the framework's robustness for authoring quantifiably distinct personas. The combined analysis reveals emergent behaviors, from landmark-based navigation to a compelling "plan to execution" gap, and a character whose unpredictable, plausible behavior stems from a lack of precise proprioception. This demonstrates a lo-fi framework for creating imperfect companions whose success is measured in character over efficiency.

</details>


### [6] [Bi-AQUA: Bilateral Control-Based Imitation Learning for Underwater Robot Arms via Lighting-Aware Action Chunking with Transformers](https://arxiv.org/abs/2511.16050)
*Takeru Tsunoori,Masato Kobayashi,Yuki Uranishi*

Main category: cs.RO

TL;DR: Bi-AQUA是首个水下双边控制的模仿学习框架，集成光照感知视觉处理，利用三层级光照适应机制，在水下操作中表现优异。


<details>
  <summary>Details</summary>
Motivation: 在极端光照变化、色彩失真和可见度降低的条件下，水下机器人的操控面临重大挑战。

Method: Bi-AQUA采用三层级的光照适应机制，包括光照编码器、FiLM调制和显式光照令牌，进行视觉特征提取和任务条件设定。

Result: Bi-AQUA成功实现了水下双边控制基础的模仿学习，与传统模型相比，在多变的光照条件下表现出色。

Conclusion: Bi-AQUA通过有效的光照适应机制，促进了水下双边控制与操纵的融合，增强了水下机器人在复杂环境中的自适应能力。

Abstract: Underwater robotic manipulation is fundamentally challenged by extreme lighting variations, color distortion, and reduced visibility. We introduce Bi-AQUA, the first underwater bilateral control-based imitation learning framework that integrates lighting-aware visual processing for underwater robot arms. Bi-AQUA employs a hierarchical three-level lighting adaptation mechanism: a Lighting Encoder that extracts lighting representations from RGB images without manual annotation and is implicitly supervised by the imitation objective, FiLM modulation of visual backbone features for adaptive, lighting-aware feature extraction, and an explicit lighting token added to the transformer encoder input for task-aware conditioning. Experiments on a real-world underwater pick-and-place task under diverse static and dynamic lighting conditions show that Bi-AQUA achieves robust performance and substantially outperforms a bilateral baseline without lighting modeling. Ablation studies further confirm that all three lighting-aware components are critical. This work bridges terrestrial bilateral control-based imitation learning and underwater manipulation, enabling force-sensitive autonomous operation in challenging marine environments. For additional material, please check: https://mertcookimg.github.io/bi-aqua

</details>


### [7] [MagBotSim: Physics-Based Simulation and Reinforcement Learning Environments for Magnetic Robotics](https://arxiv.org/abs/2511.16158)
*Lara Bergmann,Cedric Grothues,Klaus Neumann*

Main category: cs.RO

TL;DR: MagBotSim是一个新开发的磁悬浮系统模拟平台，旨在提高工业自动化中的效率和灵活性，促进下一代制造系统的实现。


<details>
  <summary>Details</summary>
Motivation: 针对磁悬浮系统的开发，寻求提高工业自动化中的材料流动效率和灵活性。

Method: 通过提出MagBotSim，一个基于物理的磁悬浮系统模拟平台，框定了磁悬浮系统作为机器人群体。

Result: 开发出一套能够支持磁悬浮系统智能算法的模拟平台，展现出其在制造系统中的潜能。

Conclusion: MagBotSim为基于磁悬浮系统的智能算法开发提供了基础，推动了下一代制造系统的发展。

Abstract: Magnetic levitation is about to revolutionize in-machine material flow in industrial automation. Such systems are flexibly configurable and can include a large number of independently actuated shuttles (movers) that dynamically rebalance production capacity. Beyond their capabilities for dynamic transportation, these systems possess the inherent yet unexploited potential to perform manipulation. By merging the fields of transportation and manipulation into a coordinated swarm of magnetic robots (MagBots), we enable manufacturing systems to achieve significantly higher efficiency, adaptability, and compactness. To support the development of intelligent algorithms for magnetic levitation systems, we introduce MagBotSim (Magnetic Robotics Simulation): a physics-based simulation for magnetic levitation systems. By framing magnetic levitation systems as robot swarms and providing a dedicated simulation, this work lays the foundation for next generation manufacturing systems powered by Magnetic Robotics. MagBotSim's documentation, videos, experiments, and code are available at: https://ubi-coro.github.io/MagBotSim/

</details>


### [8] [PIPHEN: Physical Interaction Prediction with Hamiltonian Energy Networks](https://arxiv.org/abs/2511.16200)
*Kewei Chen,Yayu Long,Mingsheng Shang*

Main category: cs.RO

TL;DR: 提出PIPHEN框架，通过语义通信和物理交互预测网络，显著减小多机器人系统中的数据传输压力和决策延迟。


<details>
  <summary>Details</summary>
Motivation: 在复杂的物理协作中，多机器人系统面临"共享大脑困境"，高维多媒体数据的传输导致带宽瓶颈和决策延迟。

Method: PIPHEN通过语义蒸馏和两大组件（物理交互预测网络和哈密顿能量网络控制器）实现高维感知数据的压缩与协调行动的精确翻译。

Result: PIPHEN框架能够将信息表示压缩至原始数据量的不足5%，并将协作决策延迟从315毫秒减少到76毫秒，同时显著提高任务成功率。

Conclusion: 该研究为在资源受限的多机器人系统中解决"共享大脑困境"提供了一种高效的范式。

Abstract: Multi-robot systems in complex physical collaborations face a "shared brain dilemma": transmitting high-dimensional multimedia data (e.g., video streams at ~30MB/s) creates severe bandwidth bottlenecks and decision-making latency. To address this, we propose PIPHEN, an innovative distributed physical cognition-control framework. Its core idea is to replace "raw data communication" with "semantic communication" by performing "semantic distillation" at the robot edge, reconstructing high-dimensional perceptual data into compact, structured physical representations. This idea is primarily realized through two key components: (1) a novel Physical Interaction Prediction Network (PIPN), derived from large model knowledge distillation, to generate this representation; and (2) a Hamiltonian Energy Network (HEN) controller, based on energy conservation, to precisely translate this representation into coordinated actions. Experiments show that, compared to baseline methods, PIPHEN can compress the information representation to less than 5% of the original data volume and reduce collaborative decision-making latency from 315ms to 76ms, while significantly improving task success rates. This work provides a fundamentally efficient paradigm for resolving the "shared brain dilemma" in resource-constrained multi-robot systems.

</details>


### [9] [DynaMimicGen: A Data Generation Framework for Robot Learning of Dynamic Tasks](https://arxiv.org/abs/2511.16223)
*Vincenzo Pomponi,Paolo Franceschi,Stefano Baraldo,Loris Roveda,Oliver Avram,Luca Maria Gambardella,Anna Valente*

Main category: cs.RO

TL;DR: 本研究提出 DynaMimicGen (D-MG)，一种可扩展的数据集生成框架，通过少量人类示范在动态环境中训练机器人政策，显著提高了操控任务的学习效果。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中，收集大型多样化数据集通常耗时、劳动密集且不切实际。因此，开发一种在最少人类监督下生成数据集的框架是本研究的主要动机。

Method: D-MG 通过少量人类演示来生成可扩展的数据集，首先将演示分解为有意义的子任务，然后利用动态运动原语（DMP）适应和推广演示行为到新环境中。

Result: 训练基于 D-MG 生成数据的机器人代理表现出强大的性能，能够在长时间跨度和接触丰富的基准测试中完成任务，如立方体堆叠和将杯子放入抽屉，即便在不可预测的环境变化下也能表现良好。

Conclusion: DynaMimicGen (D-MG) 提供了一种高效的替代方案，支持动态任务设置下的机器学习，能够实现自动化的机器人学习，减少手动数据收集的需求。

Abstract: Learning robust manipulation policies typically requires large and diverse datasets, the collection of which is time-consuming, labor-intensive, and often impractical for dynamic environments. In this work, we introduce DynaMimicGen (D-MG), a scalable dataset generation framework that enables policy training from minimal human supervision while uniquely supporting dynamic task settings. Given only a few human demonstrations, D-MG first segments the demonstrations into meaningful sub-tasks, then leverages Dynamic Movement Primitives (DMPs) to adapt and generalize the demonstrated behaviors to novel and dynamically changing environments. Improving prior methods that rely on static assumptions or simplistic trajectory interpolation, D-MG produces smooth, realistic, and task-consistent Cartesian trajectories that adapt in real time to changes in object poses, robot states, or scene geometry during task execution. Our method supports different scenarios - including scene layouts, object instances, and robot configurations - making it suitable for both static and highly dynamic manipulation tasks. We show that robot agents trained via imitation learning on D-MG-generated data achieve strong performance across long-horizon and contact-rich benchmarks, including tasks like cube stacking and placing mugs in drawers, even under unpredictable environment changes. By eliminating the need for extensive human demonstrations and enabling generalization in dynamic settings, D-MG offers a powerful and efficient alternative to manual data collection, paving the way toward scalable, autonomous robot learning.

</details>


### [10] [FT-NCFM: An Influence-Aware Data Distillation Framework for Efficient VLA Models](https://arxiv.org/abs/2511.16233)
*Kewei Chen,Yayu Long,Shuai Li,Mingsheng Shang*

Main category: cs.RO

TL;DR: 本研究提出了一种新的数据蒸馏框架FT-NCFM，通过智能化地评估和合成数据，有效提升视觉-语言-行动模型的性能和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有的模型中心优化方法未能根本解决数据层面的挑战，限制了视觉-语言-行动模型的广泛应用。

Method: 引入了一种以数据为中心的生成数据蒸馏框架FT-NCFM，结合因果归因和程序对比验证进行样本评估。

Result: 在多个主流VLA基准测试中，训练仅需5%蒸馏核心集的模型与完整数据集的成功率相比达85-90%，且训练时间减少超过80%。

Conclusion: 智能数据蒸馏是一种有前景的新方法，可以有效构建高性能的视觉-语言-行动模型。

Abstract: The powerful generalization of Vision-Language-Action (VLA) models is bottlenecked by their heavy reliance on massive, redundant, and unevenly valued datasets, hindering their widespread application. Existing model-centric optimization paths, such as model compression (which often leads to performance degradation) or policy distillation (whose products are model-dependent and lack generality), fail to fundamentally address this data-level challenge. To this end, this paper introduces FT-NCFM, a fundamentally different, data-centric generative data distillation framework. Our framework employs a self-contained Fact-Tracing (FT) engine that combines causal attribution with programmatic contrastive verification to assess the intrinsic value of samples. Guided by these assessments, an adversarial NCFM process synthesizes a model-agnostic, information-dense, and reusable data asset. Experimental results on several mainstream VLA benchmarks show that models trained on just 5% of our distilled coreset achieve a success rate of 85-90% compared with training on the full dataset, while reducing training time by over 80%. Our work demonstrates that intelligent data distillation is a highly promising new path for building efficient, high-performance VLA models.

</details>


### [11] [How Robot Dogs See the Unseeable](https://arxiv.org/abs/2511.16262)
*Oliver Bimber,Karl Dietrich von Ellenrieder,Michael Haller,Rakesh John Amala Arokia Nathan,Gianni Lunardi,Marco Camurri,Mohamed Youssef,Santos Miguel Orozco Soto,Jeremy E. Niven*

Main category: cs.RO

TL;DR: 本研究探讨了动物窥视运动与合成孔径感应的关系，提出了一种有效克服遮挡的新方法，实现了高效的场景理解，对移动机器人具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 克服传统机器人摄像头在小光圈和大景深下带来的遮挡问题，提高机器人对复杂场景的理解能力。

Method: 通过模拟动物的窥视运动，机器人执行这种运动以描述一个宽合成孔径，然后对捕获的图像进行计算集成，以合成一个具有极浅景深的图像。

Result: 该方法能够实现快速、高分辨率的感知，改善场景理解并增强对高度多模态模型的视觉推理能力，且相较于传统方法具有更强的鲁棒性和计算效率。

Conclusion: 该研究表明，动物的窥视运动与合成孔径成像之间存在正式联系，能够有效克服机器人视觉中的部分遮挡问题，并提升复杂环境下的场景理解能力。

Abstract: Peering, a side-to-side motion used by animals to estimate distance through motion parallax, offers a powerful bio-inspired strategy to overcome a fundamental limitation in robotic vision: partial occlusion. Conventional robot cameras, with their small apertures and large depth of field, render both foreground obstacles and background objects in sharp focus, causing occluders to obscure critical scene information. This work establishes a formal connection between animal peering and synthetic aperture (SA) sensing from optical imaging. By having a robot execute a peering motion, its camera describes a wide synthetic aperture. Computational integration of the captured images synthesizes an image with an extremely shallow depth of field, effectively blurring out occluding elements while bringing the background into sharp focus. This efficient, wavelength-independent technique enables real-time, high-resolution perception across various spectral bands. We demonstrate that this approach not only restores basic scene understanding but also empowers advanced visual reasoning in large multimodal models, which fail with conventionally occluded imagery. Unlike feature-dependent multi-view 3D vision methods or active sensors like LiDAR, SA sensing via peering is robust to occlusion, computationally efficient, and immediately deployable on any mobile robot. This research bridges animal behavior and robotics, suggesting that peering motions for synthetic aperture sensing are a key to advanced scene understanding in complex, cluttered environments.

</details>


### [12] [Funabot-Upper: McKibben Actuated Haptic Suit Inducing Kinesthetic Perceptions in Trunk, Shoulder, Elbow, and Wrist](https://arxiv.org/abs/2511.16265)
*Haru Fukatsu,Ryoji Yasuda,Yuki Funabora,Shinji Doki*

Main category: cs.RO

TL;DR: 本文介绍了一款新型可穿戴触觉服Funabot-Upper，能够独立刺激多个上肢关节，提升动觉感知精度至94.6%。


<details>
  <summary>Details</summary>
Motivation: 开发能够感知多部位动作的可穿戴触觉设备以增强用户的运动感知能力。

Method: 采用新简化设计策略，独立刺激关节和肌肉，实验验证感知效果和刺激关系。

Result: Funabot-Upper成功诱发多关节的动觉感知，且减少了过往设计中的感知混淆。

Conclusion: Funabot-Upper在多关节感知上表现优越，具备广泛的未来应用潜力。

Abstract: This paper presents Funabot-Upper, a wearable haptic suit that enables users to perceive 14 upper-body motions, including those of the trunk, shoulder, elbow, and wrist. Inducing kinesthetic perception through wearable haptic devices has attracted attention, and various devices have been developed in the past. However, these have been limited to verifications on single body parts, and few have applied the same method to multiple body parts as well. In our previous study, we developed a technology that uses the contraction of artificial muscles to deform clothing in three dimensions. Using this technology, we developed a haptic suit that induces kinesthetic perception of 7 motions in multiple upper body. However, perceptual mixing caused by stimulating multiple human muscles has occurred between the shoulder and the elbow. In this paper, we established a new, simplified design policy and developed a novel haptic suit that induces kinesthetic perceptions in the trunk, shoulder, elbow, and wrist by stimulating joints and muscles independently. We experimentally demonstrated the induced kinesthetic perception and examined the relationship between stimulation and perceived kinesthetic perception under the new design policy. Experiments confirmed that Funabot-Upper successfully induces kinesthetic perception across multiple joints while reducing perceptual mixing observed in previous designs. The new suit improved recognition accuracy from 68.8% to 94.6% compared to the previous Funabot-Suit, demonstrating its superiority and potential for future haptic applications.

</details>


### [13] [InEKFormer: A Hybrid State Estimator for Humanoid Robots](https://arxiv.org/abs/2511.16306)
*Lasse Hohmeyer,Mihaela Popescu,Ivan Bergonzani,Dennis Mronga,Frank Kirchner*

Main category: cs.RO

TL;DR: 本研究提出了一种结合不变扩展卡尔曼滤波器和变压器网络的混合状态估计方法InEKFormer，显示了在类人机器人运动控制中的应用潜力与挑战。


<details>
  <summary>Details</summary>
Motivation: 类人机器人在各种应用中潜力巨大，但在不同行环境下的双足运动仍然充满挑战，状态估计对此至关重要。

Method: 提出了一种新型混合状态估计方法InEKFormer，结合了不变扩展卡尔曼滤波器（InEKF）和变压器网络。

Result: 与InEKF和KalmanNet方法的比较结果表明，尽管变压器在类人机器人状态估计方面具有潜力，但在这些高维问题中需要强自回归训练。

Conclusion: 提出的InEKFormer方法在类人机器人状态估计中展示了变压器网络的潜力，强调了高维问题中强自回归训练的必要性。

Abstract: Humanoid robots have great potential for a wide range of applications, including industrial and domestic use, healthcare, and search and rescue missions. However, bipedal locomotion in different environments is still a challenge when it comes to performing stable and dynamic movements. This is where state estimation plays a crucial role, providing fast and accurate feedback of the robot's floating base state to the motion controller. Although classical state estimation methods such as Kalman filters are widely used in robotics, they require expert knowledge to fine-tune the noise parameters. Due to recent advances in the field of machine learning, deep learning methods are increasingly used for state estimation tasks. In this work, we propose the InEKFormer, a novel hybrid state estimation method that incorporates an invariant extended Kalman filter (InEKF) and a Transformer network. We compare our method with the InEKF and the KalmanNet approaches on datasets obtained from the humanoid robot RH5. The results indicate the potential of Transformers in humanoid state estimation, but also highlight the need for robust autoregressive training in these high-dimensional problems.

</details>


### [14] [Safe and Optimal Variable Impedance Control via Certified Reinforcement Learning](https://arxiv.org/abs/2511.16330)
*Shreyas Kumar,Ravi Prakash*

Main category: cs.RO

TL;DR: 提出了一种新颖的RL框架C-GMS，通过稳定的增益调度学习DMP和VIC策略，确保了机器人在复杂环境中的安全与可靠交互。


<details>
  <summary>Details</summary>
Motivation: 在动态阻抗增益的时变特性下，模型无关的强化学习容易导致不稳定性和不安全的探索，因此需要一种更可靠的方法。

Method: 提出了一种新的轨迹中心RL框架C-GMS，通过学习结合DMP和VIC的策略来保证李雅普诺夫稳定性和执行器可行性。

Result: C-GMS确保每个策略的执行都是稳定和可物理实现的，并提供了在存在模型误差和不确定性时的跟踪误差界限理论保障。

Conclusion: C-GMS框架在模拟和真实机器人上的有效性得到了验证，为复杂环境中的可靠自主交互铺平了道路。

Abstract: Reinforcement learning (RL) offers a powerful approach for robots to learn complex, collaborative skills by combining Dynamic Movement Primitives (DMPs) for motion and Variable Impedance Control (VIC) for compliant interaction. However, this model-free paradigm often risks instability and unsafe exploration due to the time-varying nature of impedance gains. This work introduces Certified Gaussian Manifold Sampling (C-GMS), a novel trajectory-centric RL framework that learns combined DMP and VIC policies while guaranteeing Lyapunov stability and actuator feasibility by construction. Our approach reframes policy exploration as sampling from a mathematically defined manifold of stable gain schedules. This ensures every policy rollout is guaranteed to be stable and physically realizable, thereby eliminating the need for reward penalties or post-hoc validation. Furthermore, we provide a theoretical guarantee that our approach ensures bounded tracking error even in the presence of bounded model errors and deployment-time uncertainties. We demonstrate the effectiveness of C-GMS in simulation and verify its efficacy on a real robot, paving the way for reliable autonomous interaction in complex environments.

</details>


### [15] [Flow-Aided Flight Through Dynamic Clutters From Point To Motion](https://arxiv.org/abs/2511.16372)
*Bowen Xu,Zexuan Yan,Minghao Lu,Xiyu Fan,Yi Luo,Youshen Lin,Zhiqiang Chen,Yeke Chen,Qiyuan Qiao,Peng Lu*

Main category: cs.RO

TL;DR: 本文开发了一种使用单LiDAR传感器的强化学习方法，专注于动态环境中自主飞行的避障能力，显示出优越的成功率和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有的动态障碍物规避策略存在时间消耗和不可靠性，本文旨在简化此过程，利用RL和LiDAR传感器，提升在复杂动态场景中的决策能力。

Method: 通过编码原始点云生成固定形状、低分辨率且安全的深度感知距离图，采用多帧观测提取运动特征，并依赖改进的感知表示推进行为生成与政策优化。

Result: 本研究提出了一种基于单个LiDAR传感器的强化学习方法用于动态环境中的自主飞行系统，能够高效处理动态障碍物并生成规避行为。通过深度感知距离图和运动特征提取，该系统在复杂动态环境中的表现优越，展现了更高的成功率和适应性。

Conclusion: 结合简单直观的动态环境表征，该系统在真实环境中能够安全操作，展示了其有效性和适用性。

Abstract: Challenges in traversing dynamic clutters lie mainly in the efficient perception of the environmental dynamics and the generation of evasive behaviors considering obstacle movement. Previous solutions have made progress in explicitly modeling the dynamic obstacle motion for avoidance, but this key dependency of decision-making is time-consuming and unreliable in highly dynamic scenarios with occlusions. On the contrary, without introducing object detection, tracking, and prediction, we empower the reinforcement learning (RL) with single LiDAR sensing to realize an autonomous flight system directly from point to motion. For exteroception, a depth sensing distance map achieving fixed-shape, low-resolution, and detail-safe is encoded from raw point clouds, and an environment change sensing point flow is adopted as motion features extracted from multi-frame observations. These two are integrated into a lightweight and easy-to-learn representation of complex dynamic environments. For action generation, the behavior of avoiding dynamic threats in advance is implicitly driven by the proposed change-aware sensing representation, where the policy optimization is indicated by the relative motion modulated distance field. With the deployment-friendly sensing simulation and dynamics model-free acceleration control, the proposed system shows a superior success rate and adaptability to alternatives, and the policy derived from the simulator can drive a real-world quadrotor with safe maneuvers.

</details>


### [16] [Robot Metacognition: Decision Making with Confidence for Tool Invention](https://arxiv.org/abs/2511.16390)
*Ajith Anil Meera,Poppy Collis,Polina Arbuzova,Abián Torres,Paul F Kinghorn,Ricardo Sanz,Pablo Lanillos*

Main category: cs.RO

TL;DR: 作者提出了一种基于自信心的机器人元认知架构，以提高机器人在决策时的可靠性和在实际应用中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的机器人普遍缺乏反思能力，而这种能力对于学习、决策和解决问题至关重要。

Method: 借鉴神经科学的思想，构建以自信心为中心的元认知架构，并应用于自主工具发明的案例中。

Result: 本论文提出了一种机器人的元认知架构，重点在于自信心（对决策的二阶判断），以改进机器人在实际物理环境中的决策和行为表现。

Conclusion: 机器人通过自信心评估及行为监控能够更好地做出教学和决策，从而提升其在复杂环境中的适应能力。

Abstract: Robots today often miss a key ingredient of truly intelligent behavior: the ability to reflect on their own cognitive processes and decisions. In humans, this self-monitoring or metacognition is crucial for learning, decision making and problem solving. For instance, they can evaluate how confident they are in performing a task, thus regulating their own behavior and allocating proper resources. Taking inspiration from neuroscience, we propose a robot metacognition architecture centered on confidence (a second-order judgment on decisions) and we demonstrate it on the use case of autonomous tool invention. We propose the use of confidence as a metacognitive measure within the robot decision making scheme. Confidence-informed robots can evaluate the reliability of their decisions, improving their robustness during real-world physical deployment. This form of robotic metacognition emphasizes embodied action monitoring as a means to achieve better informed decisions. We also highlight potential applications and research directions for robot metacognition.

</details>


### [17] [Homogeneous Proportional-Integral-Derivative Controller in Mobile Robotic Manipulators](https://arxiv.org/abs/2511.16406)
*Luis Luna,Isaac Chairez,Andrey Polyakov*

Main category: cs.RO

TL;DR: 本研究提出了一种均质PID控制策略，显著提高了移动机器人操纵器的运动控制性能，尤其是在应对动态不确定性和外部干扰时。


<details>
  <summary>Details</summary>
Motivation: 移动机器人操纵器的控制面临非线性动态、欠驱动和基座与操纵子系统之间的耦合等重大挑战，因此需要一种新的控制策略来提高其运动控制的鲁棒性和协调性。

Method: 该方法基于均质控制理论设计hPID结构，利用Lyapunov方法进行稳定性分析，且通过实验验证了其在动态环境中的控制效果。

Result: 提出了一种新的均质比例-积分-微分（hPID）控制策略，并通过实验结果验证了其在高精度轨迹跟踪方面的有效性。

Conclusion: hPID控制器在确保全局渐近稳定性和有限时间收敛性的前提下，能够有效提升移动机器人操纵器的自主性和可靠性，优于传统线性PID控制器。

Abstract: Mobile robotic manipulators (MRMs), which integrate mobility and manipulation capabilities, present significant control challenges due to their nonlinear dynamics, underactuation, and coupling between the base and manipulator subsystems. This paper proposes a novel homogeneous Proportional-Integral-Derivative (hPID) control strategy tailored for MRMs to achieve robust and coordinated motion control. Unlike classical PID controllers, the hPID controller leverages the mathematical framework of homogeneous control theory to systematically enhance the stability and convergence properties of the closed-loop system, even in the presence of dynamic uncertainties and external disturbances involved into a system in a homogeneous way. A homogeneous PID structure is designed, ensuring improved convergence of tracking errors through a graded homogeneity approach that generalizes traditional PID gains to nonlinear, state-dependent functions. Stability analysis is conducted using Lyapunov-based methods, demonstrating that the hPID controller guarantees global asymptotic stability and finite-time convergence under mild assumptions. Experimental results on a representative MRM model validate the effectiveness of the hPID controller in achieving high-precision trajectory tracking for both the mobile base and manipulator arm, outperforming conventional linear PID controllers in terms of response time, steady-state error, and robustness to model uncertainties. This research contributes a scalable and analytically grounded control framework for enhancing the autonomy and reliability of next-generation mobile manipulation systems in structured and unstructured environments.

</details>


### [18] [LAOF: Robust Latent Action Learning with Optical Flow Constraints](https://arxiv.org/abs/2511.16407)
*Xizhou Bu,Jiexi Lyu,Fulei Sun,Ruichen Yang,Zhiqiang Ma,Wei Li*

Main category: cs.RO

TL;DR: LAOF利用光流约束以伪监督学习潜在动作表示，显著提升了学习效果，尤其在标签稀缺情况下。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理无关动作的干扰，传统的动作监督受限于动作标签稀缺，光流可以自然抑制背景元素，强调运动物体，因此有动机使用光流。

Method: LAOF是一种伪监督框架，利用光流作为动作驱动信号，学习对干扰物体鲁棒的潜在动作表示。

Result: 实验结果表明，LAOF在学习到的潜在表示上超越了现有方法，其光流约束稳定训练，提高了在极少标签情况下的潜在表示质量。

Conclusion: LAOF方法在下游模仿学习和强化学习任务中表现优越，即使在标签稀缺的情况下也能有效学习潜在动作表示。

Abstract: Learning latent actions from large-scale videos is crucial for the pre-training of scalable embodied foundation models, yet existing methods often struggle with action-irrelevant distractors. Although incorporating action supervision can alleviate these distractions, its effectiveness is restricted by the scarcity of available action labels. Optical flow represents pixel-level motion between consecutive frames, naturally suppressing background elements and emphasizing moving objects. Motivated by this, we propose robust Latent Action learning with Optical Flow constraints, called LAOF, a pseudo-supervised framework that leverages the agent's optical flow as an action-driven signal to learn latent action representations robust to distractors. Experimental results show that the latent representations learned by LAOF outperform existing methods on downstream imitation learning and reinforcement learning tasks. This superior performance arises from optical flow constraints, which substantially stabilize training and improve the quality of latent representations under extremely label-scarce conditions, while remaining effective as the proportion of action labels increases to 10 percent. Importantly, even without action supervision, LAOF matches or surpasses action-supervised methods trained with 1 percent of action labels.

</details>


### [19] [From Prompts to Printable Models: Support-Effective 3D Generation via Offset Direct Preference Optimization](https://arxiv.org/abs/2511.16434)
*Chenming Wu,Xiaofan Li,Chengkai Dai*

Main category: cs.RO

TL;DR: 本论文提出SEG框架，直接优化3D模型以减少支撑材料，提高3D打印的可持续性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有切片技术关注后处理优化，而不解决模型生成阶段的支撑设计需求。

Method: 引入SEG框架，结合直接偏好优化和偏移，优化3D模型以减少支撑材料的使用。

Result: SEG在两种基准数据集上表现优越，支持体积减少和可打印性显著提升。

Conclusion: SEG通过优化生成过程中的模型设计，具有转变3D打印和促进可持续数字制造的潜力。

Abstract: The transition from digital 3D models to physical objects via 3D printing often requires support structures to prevent overhanging features from collapsing during the fabrication process. While current slicing technologies offer advanced support strategies, they focus on post-processing optimizations rather than addressing the underlying need for support-efficient design during the model generation phase. This paper introduces SEG (\textit{\underline{S}upport-\underline{E}ffective \underline{G}eneration}), a novel framework that integrates Direct Preference Optimization with an Offset (ODPO) into the 3D generation pipeline to directly optimize models for minimal support material usage. By incorporating support structure simulation into the training process, SEG encourages the generation of geometries that inherently require fewer supports, thus reducing material waste and production time. We demonstrate SEG's effectiveness through extensive experiments on two benchmark datasets, Thingi10k-Val and GPT-3DP-Val, showing that SEG significantly outperforms baseline models such as TRELLIS, DPO, and DRO in terms of support volume reduction and printability. Qualitative results further reveal that SEG maintains high fidelity to input prompts while minimizing the need for support structures. Our findings highlight the potential of SEG to transform 3D printing by directly optimizing models during the generative process, paving the way for more sustainable and efficient digital fabrication practices.

</details>


### [20] [MiMo-Embodied: X-Embodied Foundation Model Technical Report](https://arxiv.org/abs/2511.16518)
*Xiaoshuai Hao,Lei Zhou,Zhijian Huang,Zhiwen Hou,Yingbo Tang,Lingfeng Zhang,Guang Li,Zheng Lu,Shuhuai Ren,Xianhui Meng,Yuchen Zhang,Jing Wu,Jinghui Lu,Chenxu Dang,Jiayi Guan,Jianhua Wu,Zhiyi Hou,Hanbing Li,Shumeng Xia,Mingliang Zhou,Yinan Zheng,Zihao Yue,Shuhao Gu,Hao Tian,Yuannan Shen,Jianwei Cui,Wen Zhang,Shaoqing Xu,Bing Wang,Haiyang Sun,Zeyu Zhu,Yuncheng Jiang,Zibin Guo,Chuhong Gong,Chaofan Zhang,Wenbo Ding,Kun Ma,Guang Chen,Rui Cai,Diyun Xiang,Heng Qu,Fuli Luo,Hangjun Ye,Long Chen*

Main category: cs.RO

TL;DR: MiMo-Embodied是首个跨领域基础模型，在自主驾驶和具身AI方面均取得了突破性成果。


<details>
  <summary>Details</summary>
Motivation: 为了推动自主驾驶和具身AI领域的发展，开发出综合性模型。

Method: 多阶段学习、精心构建数据和CoT/RL微调。

Result: 在17个具身AI基准和12个自主驾驶基准中，MiMo-Embodied均创下新纪录，显著优于现有基线。

Conclusion: 通过多阶段学习和数据构建，两个领域相互促进，显示出强大的正向转移效果。

Abstract: We open-source MiMo-Embodied, the first cross-embodied foundation model to successfully integrate and achieve state-of-the-art performance in both Autonomous Driving and Embodied AI. MiMo-Embodied sets new records across 17 embodied AI benchmarks in Task Planning, Affordance Prediction and Spatial Understanding, while also excelling in 12 autonomous driving benchmarks across Environmental Perception, Status Prediction, and Driving Planning. Across these tasks, MiMo-Embodied significantly outperforms existing open-source, closed-source, and specialized baselines. Our results indicate that through multi-stage learning, curated data construction, and CoT/RL fine-tuning, these two domains exhibit strong positive transfer and mutually reinforce one another. We provide a detailed analysis of our model design and training methodologies to facilitate further research. Code and models are available at https://github.com/XiaomiMiMo/MiMo-Embodied.

</details>


### [21] [InternData-A1: Pioneering High-Fidelity Synthetic Data for Pre-training Generalist Policy](https://arxiv.org/abs/2511.16651)
*Yang Tian,Yuyin Yang,Yiman Xie,Zetao Cai,Xu Shi,Ning Gao,Hangxu Liu,Xuekun Jiang,Zherui Qiu,Feng Yuan,Yaping Li,Ping Wang,Junhao Cai,Jia Zeng,Hao Dong,Jiangmiao Pang*

Main category: cs.RO

TL;DR: 本研究首次证明合成数据可独立于真实数据在Vision-Language-Action模型中实现高效预训练，并展示出出色的零-shot迁移能力。


<details>
  <summary>Details</summary>
Motivation: 旨在探索合成数据在提高Vision-Language-Action模型的普遍化能力方面的潜力，并填补合成与真实数据在机器人训练中的差距。

Method: 使用了高自主性、完全解耦的合成模拟管道生成InternData-A1数据集，并通过同一架构进行预训练。

Result: 合成数据模型在多项任务中表现与真实数据模型相当，实现了零-shot的域转移。数据集InternData-A1包含630k条轨迹和广泛的任务与场景。

Conclusion: 合成数据，尤其是InternData-A1，能够与传统真实数据匹敌，为机器人研究提供了新的数据生成途径。

Abstract: Recent works explore how real and synthetic data contribute to Vision-Language-Action (VLA) models' generalization. While current VLA models have shown the strong effectiveness of large-scale real-robot pre-training, synthetic data has not previously demonstrated comparable capability at scale. This paper provides the first evidence that synthetic data alone can match the performance of the strongest $π$-dataset in pre-training a VLA model, revealing the substantial value of large-scale simulation. The resulting model also exhibits surprisingly zero-shot sim-to-real transfer on several challenging tasks. Our synthetic dataset, InternData-A1, contains over 630k trajectories and 7,433 hours across 4 embodiments, 18 skills, 70 tasks, and 227 scenes, covering rigid, articulated, deformable, and fluid-object manipulation. It is generated through a highly autonomous, fully decoupled, and compositional simulation pipeline that enables long-horizon skill composition, flexible task assembly, and heterogeneous embodiments with minimal manual tuning. Using the same architecture as $π_0$, we pre-train a model entirely on InternData-A1 and find that it matches the official $π_0$ across 49 simulation tasks, 5 real-world tasks, and 4 long-horizon dexterous tasks. We release the dataset and will open-source the generation pipeline to broaden access to large-scale robotic data and to lower the barrier to scalable data creation for embodied AI research.

</details>


### [22] [Dexterity from Smart Lenses: Multi-Fingered Robot Manipulation with In-the-Wild Human Demonstrations](https://arxiv.org/abs/2511.16661)
*Irmak Guzey,Haozhi Qi,Julen Urain,Changhao Wang,Jessica Yin,Krishna Bodduluri,Mike Lambeta,Lerrel Pinto,Akshara Rai,Jitendra Malik,Tingfan Wu,Akash Sharma,Homanga Bharadhwaj*

Main category: cs.RO

TL;DR: 本研究提出AINA框架，利用轻便的Aria Gen 2眼镜收集人类数据，有望实现更高效的机器人操作策略学习。


<details>
  <summary>Details</summary>
Motivation: 从人类在自然环境中执行日常任务中学习机器人策略，以减少对人工数据收集的依赖，推动机器人操作的普适性。

Method: 使用AINA框架，通过Aria Gen 2眼镜收集人类数据，学习多指机器人操作策略。

Result: AINA框架能够从任何地方、任何人收集的数据中学习多指策略，在九个常见操作任务中展示了优异的性能。

Conclusion: AINA的实现表明，通过简单有效的硬件和框架，可以大幅度推动从人类活动中学习机器人策略的进程。

Abstract: Learning multi-fingered robot policies from humans performing daily tasks in natural environments has long been a grand goal in the robotics community. Achieving this would mark significant progress toward generalizable robot manipulation in human environments, as it would reduce the reliance on labor-intensive robot data collection. Despite substantial efforts, progress toward this goal has been bottle-necked by the embodiment gap between humans and robots, as well as by difficulties in extracting relevant contextual and motion cues that enable learning of autonomous policies from in-the-wild human videos. We claim that with simple yet sufficiently powerful hardware for obtaining human data and our proposed framework AINA, we are now one significant step closer to achieving this dream. AINA enables learning multi-fingered policies from data collected by anyone, anywhere, and in any environment using Aria Gen 2 glasses. These glasses are lightweight and portable, feature a high-resolution RGB camera, provide accurate on-board 3D head and hand poses, and offer a wide stereo view that can be leveraged for depth estimation of the scene. This setup enables the learning of 3D point-based policies for multi-fingered hands that are robust to background changes and can be deployed directly without requiring any robot data (including online corrections, reinforcement learning, or simulation). We compare our framework against prior human-to-robot policy learning approaches, ablate our design choices, and demonstrate results across nine everyday manipulation tasks. Robot rollouts are best viewed on our website: https://aina-robot.github.io.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [23] [A Crowdsourced Study of ChatBot Influence in Value-Driven Decision Making Scenarios](https://arxiv.org/abs/2511.15857)
*Anthony Wise,Xinyi Zhou,Martin Reimann,Anind Dey,Leilani Battle*

Main category: cs.HC

TL;DR: 研究显示，价值框架能够有效改变用户的决策，但若与用户价值观不一致，可能引发反效果


<details>
  <summary>Details</summary>
Motivation: 探讨使用框架而非偏见或虚假信息，是否能影响用户行为

Method: 进行众包研究，分析低级框架对用户决策的影响

Result: 在一个控制内容的实验中，价值框架的聊天机器人显著影响了参与者的预算选择

Conclusion: 价值框架的使用降低了操控LLM的门槛，提示出与明显偏见或虚假信息不同的风险。

Abstract: Similar to social media bots that shape public opinion, healthcare and financial decisions, LLM-based ChatBots like ChatGPT can persuade users to alter their behavior. Unlike prior work that persuades via overt-partisan bias or misinformation, we test whether framing alone suffices. We conducted a crowdsourced study, where 336 participants interacted with a neutral or one of two value-framed ChatBots while deciding to alter US defense spending. In this single policy domain with controlled content, participants exposed to value-framed ChatBots significantly changed their budget choices relative to the neutral control. When the frame misaligned with their values, some participants reinforced their original preference, revealing a potentially replicable backfire effect, originally considered rare in the literature. These findings suggest that value-framing alone lowers the barrier for manipulative uses of LLMs, revealing risks distinct from overt bias or misinformation, and clarifying risks to countering misinformation.

</details>


### [24] [Panel-by-Panel Souls: A Performative Workflow for Expressive Faces in AI-Assisted Manga Creation](https://arxiv.org/abs/2511.16038)
*Qing Zhang,Jing Huang,Yifei Huang,Jun Rekimoto*

Main category: cs.HC

TL;DR: 本研究提出了一种新的交互式工作流程，通过结合手动和自动工具，提升了文本到图像模型在漫画创作中的表现，增强艺术家的创作自由度。


<details>
  <summary>Details</summary>
Motivation: 当前的文本到图像模型在渲染复杂的面部表情以支持引人入胜的漫画叙事方面存在挑战，主要是由于语言本身的模糊性。

Method: 第一阶段结合基于地标的自动检测与手动框架工具，第二阶段使用LivePortrait引擎，通过视频输入实现细粒度控制。

Result: 提出了一个基于双混合管道的交互系统，结合了自动检测与手动框架工具，以实现艺术家中心的面部准备和流畅的视觉表达。

Conclusion: 这项工作展示了人机协作的新模型，为艺术家提供了更直接和直观的方式，将他们的创意转化为视觉表达。

Abstract: Current text-to-image models struggle to render the nuanced facial expressions required for compelling manga narratives, largely due to the ambiguity of language itself. To bridge this gap, we introduce an interactive system built on a novel, dual-hybrid pipeline. The first stage combines landmark-based auto-detection with a manual framing tool for robust, artist-centric face preparation. The second stage maps expressions using the LivePortrait engine, blending intuitive performative input from video for fine-grained control. Our case study analysis suggests that this integrated workflow can streamline the creative process and effectively translate narrative intent into visual expression. This work presents a practical model for human-AI co-creation, offering artists a more direct and intuitive means of ``infusing souls'' into their characters. Our primary contribution is not a new generative model, but a novel, interactive workflow that bridges the gap between artistic intent and AI execution.

</details>


### [25] [Heterogeneous Stroke: Using Unique Vibration Cues to Improve the Wrist-Worn Spatiotemporal Tactile Display](https://arxiv.org/abs/2511.16133)
*Taejun Kim,Youngbo Aram Shim,Geehyuk Lee*

Main category: cs.HC

TL;DR: 本研究提出了通过独特振动刺激提高腕部触觉显示器中触觉模式识别准确率的方法，取得了93.8%的字母和92.4%的数字识别准确率。


<details>
  <summary>Details</summary>
Motivation: 由于腕部有限的皮肤区域和空间分辨率，触觉器之间的混淆会导致识别准确率低，本研究旨在解决这一问题。

Method: 通过为每个触觉器分配独特的振动刺激，减少了触觉器之间的混淆。

Result: 本研究提出了一种名为“异构笔画”的设计概念，旨在提升腕部触觉显示器上空间时间触觉模式的识别准确率。通过为每个触觉器分配独特的振动刺激，以减少触觉器之间的混淆。该实现显示，在不同的手臂姿势中，字母和数字的传递准确率分别达到了93.8%和92.4%。

Conclusion: 异构笔画设计显著提高了腕部触觉显示器中信息传递的准确性，特别是在不同手臂姿势下。

Abstract: Beyond a simple notification of incoming calls or messages, more complex information such as alphabets and digits can be delivered through spatiotemporal tactile patterns (STPs) on a wrist-worn tactile display (WTD) with multiple tactors. However, owing to the limited skin area and spatial acuity of the wrist, frequent confusions occur between closely located tactors, resulting in a low recognition accuracy. Furthermore, the accuracies reported in previous studies have mostly been measured for a specific posture and could further decrease with free arm postures in real life. Herein, we present Heterogeneous Stroke, a design concept for improving the recognition accuracy of STPs on a WTD. By assigning unique vibrotactile stimuli to each tactor, the confusion between tactors can be reduced. Through our implementation of Heterogeneous Stroke, the alphanumeric characters could be delivered with high accuracy (93.8% for 26 alphabets and 92.4% for 10 digits) across different arm postures.

</details>


### [26] [Gaze Archive: Enhancing Human Memory through Active Visual Logging on Smart Glasses](https://arxiv.org/abs/2511.16214)
*Haoxin Ren,Feng Lu*

Main category: cs.HC

TL;DR: Gaze Archive通过智能眼镜的主动记录提升视觉记忆，证实其在用户交互中更轻松、更不干扰，展示了良好的实际应用前景。


<details>
  <summary>Details</summary>
Motivation: 应对传统记忆增强方法的局限性，如努力和干扰的特性，以及与用户意图的不一致。

Method: 通过GAHMA框架在智能眼镜上实现主动记录，利用人眼注视作为自然的注意力指示器，以实现意图精确捕捉和轻松无干扰的交互。

Result: GAHMA在意图精确记录方面优于非注视基线，并通过广泛的用户研究显示出其在不同场景中的优势。

Conclusion: Gaze Archive在用户记忆增强方面表现出显著优势，特别是在感知的轻松度、不干扰性和总体偏好方面，具有很强的现实应用潜力。

Abstract: People today are overwhelmed by massive amounts of information, leading to cognitive overload and memory burden. Traditional visual memory augmentation methods are either effortful and disruptive or fail to align with user intent. To address these limitations, we propose Gaze Archive, a novel visual memory enhancement paradigm through active logging on smart glasses. It leverages human gaze as a natural attention indicator, enabling both intent-precise capture and effortless-and-unobtrusive interaction. To implement Gaze Archive, we develop GAHMA, a technical framework that enables compact yet intent-aligned memory encoding and intuitive memory recall based on natural language queries. Quantitative experiments on our newly constructed GAVER dataset show that GAHMA achieves more intent-precise logging than non-gaze baselines. Through extensive user studies in both laboratory and real-world scenarios, we compare Gaze Archive with other existing memory augmentation methods. Results demonstrate its advantages in perceived effortlessness, unobtrusiveness and overall preference, showing strong potential for real-world deployment.

</details>


### [27] [When Less is More: A Story of Failing Bayesian Optimization Due to Additional Expert Knowledge](https://arxiv.org/abs/2511.16230)
*Dorina Weichert,Gunar Ernis,Marvin Worthmann,Peter Ryzko,Lukas Seifert*

Main category: cs.HC

TL;DR: 利用贝叶斯优化进行塑料废料回收材料的实验计划时，额外数据和知识的加入可能导致优化效果不佳，需谨慎处理。


<details>
  <summary>Details</summary>
Motivation: 应对塑料回收材料的实验规划及其属性控制挑战，提高实验的效率和效果。

Method: 使用贝叶斯优化和代理模型进行实验计划，并探讨历史数据和专家知识的引入。

Result: 发现额外数据和知识的加入可能对优化造成负面影响，并最终提出改进方案。

Conclusion: 额外的知识和数据在优化时只有在不复杂化基本目标的情况下才有益。

Abstract: The compounding of plastics with recycled material remains a practical challenge, as the properties of the processed material is not as easy to control as with completely new raw materials. For a data scientist, it makes sense to plan the necessary experiments in the development of new compounds using Bayesian Optimization, an optimization approach based on a surrogate model that is known for its data efficiency and is therefore well suited for data obtained from costly experiments. Furthermore, if historical data and expert knowledge are available, their inclusion in the surrogate model is expected to accelerate the convergence of the optimization. In this article, we describe a use case in which the addition of data and knowledge has impaired optimization. We also describe the unsuccessful methods that were used to remedy the problem before we found the reasons for the poor performance and achieved a satisfactory result. We conclude with a lesson learned: additional knowledge and data are only beneficial if they do not complicate the underlying optimization goal.

</details>


### [28] [Optimized User Experience for Labeling Systems for Predictive Maintenance Applications](https://arxiv.org/abs/2511.16236)
*Michelle Hallmann,Michael Stern,Francesco Vona,Ute Franke,Thomas Ostertag,Benjamin Schlueter,Jan-Niklas Voigt-Antons*

Main category: cs.HC

TL;DR: 本论文提出了一个用户友好的图形标签界面，用于提高列车维护和服务可靠性，采用经济高效的监控系统和机器学习算法。


<details>
  <summary>Details</summary>
Motivation: 目的是增强铁路运输的经济可行性和运行效率，强调成功的标签阶段对于训练有监督的机器学习系统的重要性。

Method: 本研究通过用户友好的标签界面设计，结合可用性启发式的最佳实践，以便与注释者的日常工作流程相结合，验证其可用性和用户体验。

Result: 本论文设计并实施了一个图形标签用户界面，用于监控和预测维护系统，面向德国乡村地区的列车及铁路基础设施。

Conclusion: 该研究旨在降低维护成本，提高铁路运输的服务可靠性，并为相关领域的研究提供实用见解。

Abstract: This paper presents the design and implementation of a graphical labeling user interface for a monitoring and predictive maintenance system for trains and rail infrastructure in a rural area of Germany. Aiming to enhance rail transportation's economic viability and operational efficiency, our project utilizes cost-effective wireless monitoring systems that combine affordable sensors and machine learning algorithms. Given that a successful labeling phase is indispensable for training a supervised machine learning system, we emphasize the importance of a user-friendly labeling user interface, which can be optimally integrated into the daily work routines of annotators. The labeling system has been designed based on best practices in usability heuristics and will be validated for usability and user experience through a study, the protocol for which is presented here. The value of this work lies in its potential to reduce maintenance costs and improve service reliability in rail transportation, contributing to the academic literature and offering practical insights for research on effective labeling user interfaces, as well as for the development of labeling systems in the industry. Upon completion of the study, we will share the results, refine the system as necessary, and explore its scalability in other areas of infrastructure maintenance.

</details>


### [29] [Optimizing Predictive Maintenance: Enhanced AI and Backend Integration](https://arxiv.org/abs/2511.16239)
*Michael Stern,Michelle Hallmann,Francesco Vona,Ute Franke,Thomas Ostertag,Benjamin Schlueter,Jan-Niklas Voigt-Antons*

Main category: cs.HC

TL;DR: 本文提出了一种无线监测系统，通过集成传感器和机器学习技术，以提高铁路运输的可靠性和效率。


<details>
  <summary>Details</summary>
Motivation: 铁路运输的成功依赖于高效的维护，以避免延误和故障，尤其是在资源有限的农村地区。

Method: 开发一个安全的数据管理系统，配备传感器以收集结构和环境数据，并使用机器学习模型进行分析。

Result: 提出了一种成本效益高的无线监测系统，集成了传感器和机器学习，旨在解决铁路维护挑战。

Conclusion: 该系统通过安全的数据管理，能够在潜在问题出现前进行预测性维护，确保铁路运输的安全与高效。

Abstract: Rail transportation success depends on efficient maintenance to avoid delays and malfunctions, particularly in rural areas with limited resources. We propose a cost-effective wireless monitoring system that integrates sensors and machine learning to address these challenges. We developed a secure data management system, equipping train cars and rail sections with sensors to collect structural and environmental data. This data supports Predictive Maintenance by identifying potential issues before they lead to failures. Implementing this system requires a robust backend infrastructure for secure data transfer, storage, and analysis. Designed collaboratively with stakeholders, including the railroad company and project partners, our system is tailored to meet specific requirements while ensuring data integrity and security. This article discusses the reasoning behind our design choices, including the selection of sensors, data handling protocols, and Machine Learning models. We propose a system architecture for implementing the solution, covering aspects such as network topology and data processing workflows. Our approach aims to enhance the reliability and efficiency of rail transportation through advanced technological integration.

</details>


### [30] [GazeInterpreter: Parsing Eye Gaze to Generate Eye-Body-Coordinated Narrations](https://arxiv.org/abs/2511.16245)
*Qing Chang,Zhiming Hu*

Main category: cs.HC

TL;DR: GazeInterpreter是一种利用眼动数据解析生成协调叙述的模型，显著提升了人类行为理解的性能。


<details>
  <summary>Details</summary>
Motivation: 全面理解人类行为，特别是眼动与身体运动之间的相互作用。

Method: GazeInterpreter采用符号化解析、层次结构和自我纠正机制，整合眼动与身体运动数据。

Result: 本研究提出了一种新颖的模型——GazeInterpreter，旨在通过解析眼动数据生成与身体运动协调的叙述，以全面理解人类行为。该方法结合了符号化眼动解析器、层次结构和自我纠正循环，有效提升了行为理解的准确性和完整性。

Conclusion: 解析眼动数据在理解人类行为方面具有重要潜力，并为未来研究提供了新方向。

Abstract: Comprehensively interpreting human behavior is a core challenge in human-aware artificial intelligence. However, prior works typically focused on body behavior, neglecting the crucial role of eye gaze and its synergy with body motion. We present GazeInterpreter - a novel large language model-based (LLM-based) approach that parses eye gaze data to generate eye-body-coordinated narrations. Specifically, our method features 1) a symbolic gaze parser that translates raw gaze signals into symbolic gaze events; 2) a hierarchical structure that first uses an LLM to generate eye gaze narration at semantic level and then integrates gaze with body motion within the same observation window to produce integrated narration; and 3) a self-correcting loop that iteratively refines the modality match, temporal coherence, and completeness of the integrated narration. This hierarchical and iterative processing can effectively align physical values and semantic text in the temporal and spatial domains. We validated the effectiveness of our eye-body-coordinated narrations on the text-driven motion generation task in the large-scale Nymeria benchmark. Moreover, we report significant performance improvements for the sample downstream tasks of action anticipation and behavior summarization. Taken together, these results reveal the significant potential of parsing eye gaze to interpret human behavior and open up a new direction for human behavior understanding.

</details>


### [31] [Optimized User Experience for Labeling Systems for Predictive Maintenance Applications (Extended)](https://arxiv.org/abs/2511.16266)
*Michelle Hallmann,Michael Stern,Juliane Henning,Ute Franke,Thomas Ostertag,Joao Paulo Javidi da Costa,Jan-Niklas Voigt-Antons*

Main category: cs.HC

TL;DR: 本文讨论了一种基于监督机器学习的预测性维护系统，该系统旨在提高铁路运输的效率和可靠性，同时优化用户界面以满足标注者的需求。


<details>
  <summary>Details</summary>
Motivation: 提升铁路运输的经济效益，减少延误与故障，确保用户在数据标注过程中的舒适性和高效性。

Method: 结合结构传播噪声测量与监督学习，利用无线传感器网络和分布式账本技术，实现对铁路车辆和基础设施的监测与维护建议。

Result: 驾驶员接口获得了优秀的可用性评价，而车间主任接口则被评为良好，表明该系统在日常工作流程整合方面的潜力。

Conclusion: 此预测性维护系统在铁路运输中具有广泛应用的潜力，特别是在数据标注效率和用户体验方面需进一步改善。

Abstract: The maintenance of rail vehicles and infrastructure plays a critical role in reducing delays, preventing malfunctions, and ensuring the economic efficiency of rail transportation companies. Predictive maintenance systems powered by supervised machine learning offer a promising approach by detecting failures before they occur, reducing unscheduled downtime, and improving operational efficiency. However, the success of such systems depends on high quality labeled data, necessitating user centered labeling interfaces tailored to annotators needs for Usability and User Experience. This study introduces a cost effective predictive maintenance system developed in the federally funded project DigiOnTrack, which combines structure borne noise measurement with supervised learning to provide monitoring and maintenance recommendations for rail vehicles and infrastructure in rural Germany. The system integrates wireless sensor networks, distributed ledger technology for secure data transfer, and a dockerized container infrastructure hosting the labeling interface and dashboard. Train drivers and workshop foremen labeled faults on infrastructure and vehicles to ensure accurate recommendations. The Usability and User Experience evaluation showed that the locomotive drivers interface achieved Excellent Usability, while the workshop foremans interface was rated as Good. These results highlight the systems potential for integration into daily workflows, particularly in labeling efficiency. However, areas such as Perspicuity require further optimization for more data intensive scenarios. The findings offer insights into the design of predictive maintenance systems and labeling interfaces, providing a foundation for future guidelines in Industry 4.0 applications, particularly in rail transportation.

</details>
