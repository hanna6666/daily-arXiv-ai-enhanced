<div id=toc></div>

# Table of Contents

- [cs.HC](#cs.HC) [Total: 16]
- [cs.RO](#cs.RO) [Total: 43]


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [1] [Assessing the Effectiveness of Driver Training Interventions in Improving Safe Engagement with Vehicle Automation Systems](https://arxiv.org/abs/2509.25364)
*Chengxin Zhang,Huizhong Guo,Zifei Wang,Fred Feng,Anuj Pradhan,Shan Bao*

Main category: cs.HC

TL;DR: 本研究探讨了如何通过不同形式的培训提高驾驶员使用自动驾驶系统的安全性，发现基于知识的培训效果最佳，特别对老年驾驶员的安全驾驶有积极影响。


<details>
  <summary>Details</summary>
Motivation: 研究针对自动驾驶安全性，探讨如何通过培训提高驾驶员与车辆自动化系统的互动。

Method: 通过对不同培训格式（业主手册、基于知识的培训和技能实践）进行比较，并使用混合效应和负二项模型分析安全相关结果。

Result: 基于知识的培训在系统理解和安全交互模式方面表现最佳，参与者的测验分数显著提高，且在实际交互中表现出更高的意识和频率。

Conclusion: 短期、有针对性的培训能够显著提高驾驶员安全有效地使用车辆自动化系统，特别是针对老年司机。

Abstract: This study investigates how targeted training interventions can improve safe
driver interaction with vehicle automation (VA) systems, focusing on Adaptive
Cruise Control (ACC) and Lane Keeping Assist (LKA), both safety-critical
advanced driver assistance systems (ADAS). Effective training reduces misuse
and enhances road safety by promoting correct knowledge and application.
  A review of multiple automakers' owners' manuals revealed inconsistencies in
describing ACC and LKA functions. Three training formats were compared: (1)
owners' manual (OM), (2) knowledge-based (KB) with summarized operational
guidelines and visual aids, and (3) skill-based hands-on practice in a driving
simulator (SIM). Thirty-six participants with no prior VA experience were
randomly assigned to one group. Safety-relevant outcomes - system comprehension
(quiz scores) and real-world engagement (frequency and duration of activations)
- were analyzed using mixed-effects and negative binomial models.
  KB training produced the greatest improvements in comprehension of system
limitations, as well as safer engagement patterns. Compared with OM
participants, KB participants achieved significantly higher quiz scores and
engaged LKA and ACC more often (1.4 and 1.45 times, respectively); they also
demonstrated greater awareness of scenarios requiring manual control,
indicating reduced risk of inappropriate reliance. Older drivers exhibited
longer activations overall, highlighting age-related differences in reliance
and potential safety implications.
  Short, targeted training can significantly improve safe and effective VA
system use, particularly for senior drivers. These results highlight training
as a proactive safety intervention to reduce human-automation mismatch and
enhance system reliability in real-world driving.

</details>


### [2] [Beyond the Pocket: A Large-Scale International Study on User Preferences on Bodily Placements of Commercial Wearables](https://arxiv.org/abs/2509.25383)
*Joanna Sorysz,Lars Krupp,Dominique Nshimyimana,Meagan B. Loerakker,Bo Zhou,Paul Lukowicz,Jakob Karolus*

Main category: cs.HC

TL;DR: 本研究通过问卷调查分析穿戴设备的放置习惯，提出用户中心的设计指导，促进更具包容性和适应性的穿戴系统设计。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴技术不断发展，集成到用户日常生活中导致了设计上的挑战，特别是缺乏关于用户使用习惯的大规模实证数据。

Method: 通过在多个国家进行问卷调查，收集与真实世界中穿戴设备放置相关的习惯数据。

Result: 320位参与者的结果表明，穿戴使用模式根据时间和上下文的变化而变化。

Conclusion: 本研究为穿戴设备的放置提供了以用户为中心的实践性指导，并探讨了这些指导与现有研究的异同。

Abstract: As wearable technologies continue to evolve-becoming smaller, more powerful,
and more deeply embedded in daily life-their integration into diverse user
contexts raises critical design challenges. There remains a notable gap in
large-scale empirical data on where users actually wear or carry these devices
throughout the day, systematically examining user preferences for wearable
placement across varied contexts and routines. In this work, we conducted a
questionnaire in several countries aimed at capturing real-world habits related
to wearable device placement. The results from n = 320 participants reveal how
wearable usage patterns shift depending on time of day and context. We propose
a set of practical, user-centered guidelines for sensor placement and discuss
how they align or diverge from assumptions seen in existing ISWC work. This
study contributes to ongoing efforts within the community to design more
inclusive, adaptable, and context-aware wearable systems.

</details>


### [3] [Human vs. AI Safety Perception? Decoding Human Safety Perception with Eye-Tracking Systems, Street View Images, and Explainable AI](https://arxiv.org/abs/2509.25457)
*Yuhao Kang,Junda Chen,Liu Liu,Kshitij Sharmad,Martina Mazzarello,Simone Mora,Fabio Duarte,Carlo Ratti*

Main category: cs.HC

TL;DR: 本研究通过目光追踪及深度学习方法深入分析居民在城市环境中对安全感知的影响因素，识别了关键视觉元素，并验证了可解释人工智能模型与实际人类注意力的一致性。


<details>
  <summary>Details</summary>
Motivation: 探讨居民如何感知安全，以便更好地理解和设计公共空间，从而提升居民对公共环境的安全感。

Method: 结合目光追踪系统与街景图像，使用深度学习技术的计算框架，分析人们在安全感知上的视觉关注点与时长。

Result: 识别出特定的环境视觉因素影响安全感知，如城市基础设施和公共空间特征，而天空对安全感知的影响较小，同时发现可解释人工智能模型与人类注意力模式的对比结果。

Conclusion: 本研究提供了对城市空间安全感知的更深层次理解，识别出特定的视觉环境因素影响人类的安全感知，并展示了人眼跟踪技术与可解释人工智能相结合的有效性。

Abstract: The way residents perceive safety plays an important role in how they use
public spaces. Studies have combined large-scale street view images and
advanced computer vision techniques to measure the perception of safety of
urban environments. Despite their success, such studies have often overlooked
the specific environmental visual factors that draw human attention and trigger
people's feelings of safety perceptions. In this study, we introduce a
computational framework that enriches the existing body of literature on place
perception by using eye-tracking systems with street view images and deep
learning approaches. Eye-tracking systems quantify not only what users are
looking at but also how long they engage with specific environmental elements.
This allows us to explore the nuance of which visual environmental factors
influence human safety perceptions. We conducted our research in Helsingborg,
Sweden, where we recruited volunteers outfitted with eye-tracking systems. They
were asked to indicate which of the two street view images appeared safer. By
examining participants' focus on specific features using Mean Object Ratio in
Highlighted Regions (MoRH) and Mean Object Hue (MoH), we identified key visual
elements that attract human attention when perceiving safe environments. For
instance, certain urban infrastructure and public space features draw more
human attention while the sky is less relevant in influencing safety
perceptions. These insights offer a more human-centered understanding of which
urban features influence human safety perceptions. Furthermore, we compared the
real human attention from eye-tracking systems with attention maps obtained
from eXplainable Artificial Intelligence (XAI) results. Several XAI models were
tested, and we observed that XGradCAM and EigenCAM most closely align with
human safety perceptual patterns.

</details>


### [4] ["Where Can I Park?" Understanding Human Perspectives and Scalably Detecting Disability Parking from Aerial Imagery](https://arxiv.org/abs/2509.25460)
*Jared Hwang,Chu Li,Hanbyul Kang,Maryam Hosseini,Jon E. Froehlich*

Main category: cs.HC

TL;DR: 本文通过访谈残疾人士，研究他们对残疾停车的使用和关注点，同时推出了AccessParkCV检测管道，以提高残疾停车位的检测和质量分析。


<details>
  <summary>Details</summary>
Motivation: 探讨残疾停车位的质量和分配问题，以及残疾人士对停车位使用的看法，填补美国在这些领域的研究空白。

Method: 通过与11名残疾人士进行半结构化访谈，了解他们对残疾停车位的使用、关注点和相关技术工具的看法，并开发了基于深度学习的检测管道。

Result: 我们发现残疾人士根据个人的流动需求适应残疾停车位的挑战，并重视可靠的实时可达性信息；我们的深度学习管道在检测残疾停车位和推断质量特征上取得了较高的精度，micro-F1达到了0.89。

Conclusion: 本文提出了一种新的检测残疾停车位的深度学习管道AccessParkCV，并提供了一个停车数据集，以支持今后的城市分析和用户工具。

Abstract: Accessible parking is critical for people with disabilities (PwDs), allowing
equitable access to destinations, independent mobility, and community
participation. Despite mandates, there has been no large-scale investigation of
the quality or allocation of disability parking in the US nor significant
research on PwD perspectives and uses of disability parking. In this paper, we
first present a semi-structured interview study with 11 PwDs to advance
understanding of disability parking uses, concerns, and relevant technology
tools. We find that PwDs often adapt to disability parking challenges according
to their personal mobility needs and value reliable, real-time accessibility
information. Informed by these findings, we then introduce a new deep learning
pipeline, called AccessParkCV, and parking dataset for automatically detecting
disability parking and inferring quality characteristics (e.g., width) from
orthorectified aerial imagery. We achieve a micro-F1=0.89 and demonstrate how
our pipeline can support new urban analytics and end-user tools. Together, we
contribute new qualitative understandings of disability parking, a novel
detection pipeline and open dataset, and design guidelines for future tools.

</details>


### [5] [LLM-Assisted News Discovery in High-Volume Information Streams: A Case Study](https://arxiv.org/abs/2509.25491)
*Nick Hagar,Ethan Silver,Clare Spencer,Nicholas Diakopoulos*

Main category: cs.HC

TL;DR: 大型语言模型能够有效过滤新闻线索，结合人工审查能提升监测能力，但在复杂判断上仍需人类参与。


<details>
  <summary>Details</summary>
Motivation: 应对记者在监测不断扩展的数字信息流中的挑战，并提升新闻内容的识别能力。

Method: 采用基于提示的方法将新闻价值编码到LLMs指令中，评估潜在故事线索。

Result: 在多个模型上进行验证，F1值达到0.94，新闻价值评估准确度达到92%，但对复杂的编辑判断存在困难。

Conclusion: 本文建议通过将大型语言模型(LLMs)与人工审查结合，增强新闻监测能力，同时保持编辑判断。

Abstract: Journalists face mounting challenges in monitoring ever-expanding digital
information streams to identify newsworthy content. While traditional
automation tools gather information at scale, they struggle with the editorial
judgment needed to assess newsworthiness. This paper investigates whether large
language models (LLMs) can serve as effective first-pass filters for
journalistic monitoring. We develop a prompt-based approach encoding
journalistic news values - timeliness, impact, controversy, and
generalizability - into LLM instructions to extract and evaluate potential
story leads. We validate our approach across multiple models against
expert-annotated ground truth, then deploy a real-world monitoring pipeline
that processes trade press articles daily. Our evaluation reveals strong
performance in extracting relevant leads from source material ($F1=0.94$) and
in coarse newsworthiness assessment ($\pm$1 accuracy up to 92%), but it
consistently struggles with nuanced editorial judgments requiring beat
expertise. The system proves most valuable as a hybrid tool combining automated
monitoring with human review, successfully surfacing novel, high-value leads
while filtering obvious noise. We conclude with practical recommendations for
integrating LLM-powered monitoring into newsroom workflows that preserves
editorial judgment while extending journalistic capacity.

</details>


### [6] [Botender: Supporting Communities in Collaboratively Designing AI Agents through Case-Based Provocations](https://arxiv.org/abs/2509.25492)
*Tzu-Sheng Kuo,Sophia Liu,Quan Ze Chen,Joseph Seering,Amy X. Zhang,Haiyi Zhu,Kenneth Holstein*

Main category: cs.HC

TL;DR: Botender允许社区成员无代码设计LLM驱动的机器人，通过互动场景促进反思和讨论，有效改善机器人行为。


<details>
  <summary>Details</summary>
Motivation: 为了让社区能够共同塑造社区机器人行为，解决外部设计者与社区需求不一致的问题。

Method: 通过Botender系统，社区成员能够无编码地提出、迭代和部署定制的机器人行为，并利用基于案例的刺激进行测试和迭代。

Result: Botender在六个Discord服务器上的五天部署中，帮助社区针对特定需求调整机器人行为，基于案例的刺激被证明比标准测试案例更有效。

Conclusion: Botender successfully enables communities to collaboratively design bots according to their specific needs, enhancing community engagement and satisfaction.

Abstract: AI agents, or bots, serve important roles in online communities. However,
they are often designed by outsiders or a few tech-savvy members, leading to
bots that may not align with the broader community's needs. How might
communities collectively shape the behavior of community bots? We present
Botender, a system that enables communities to collaboratively design
LLM-powered bots without coding. With Botender, community members can directly
propose, iterate on, and deploy custom bot behaviors tailored to community
needs. Botender facilitates testing and iteration on bot behavior through
case-based provocations: interaction scenarios generated to spark user
reflection and discussion around desirable bot behavior. A validation study
found these provocations more useful than standard test cases for revealing
improvement opportunities and surfacing disagreements. During a five-day
deployment across six Discord servers, Botender supported communities in
tailoring bot behavior to their specific needs, showcasing the usefulness of
case-based provocations in facilitating collaborative bot design.

</details>


### [7] [Atlas of Human-AI Interaction (v1): An Interactive Meta-Science Platform for Large-Scale Research Literature Sensemaking](https://arxiv.org/abs/2509.25499)
*Chayapatr Archiwaranguprok,Awu Chen,Sheer Karny,Hiroshi Ishii,Pattie Maes,Pat Pataranutaporn*

Main category: cs.HC

TL;DR: 本研究介绍了人机交互地图，通过交互式网页界面和AI知识提取，系统性地映射了1000多篇HCI论文的经验发现，揭示了因果关系，提供基于证据的设计框架。


<details>
  <summary>Details</summary>
Motivation: 人机交互研究者面临合成数千项实证研究的挑战，以理解AI对人类的影响并指导有效的设计。现有的文献综述方法忽略了重要的因果关系。

Method: 引入人机交互地图，一个互动网页界面，基于LLM-powered知识提取，对1000多篇HCI论文的经验发现进行系统映射。

Result: 提取了2037个经验发现，揭示了研究主题集群、共同主题和断开的领域；20名研究者的专家评估显示该系统在发现研究空白方面的有效性。

Conclusion: 本研究展示了AI如何转变文献合成，提供了基于证据的设计框架，为HCI及其他领域的计算元科学开辟新可能性。

Abstract: Human-AI interaction researchers face an overwhelming challenge: synthesizing
insights from thousands of empirical studies to understand how AI impacts
people and inform effective design. Existing approach for literature reviews
cluster papers by similarities, keywords or citations, missing the crucial
cause-and-effect relationships that reveal how design decisions impact user
outcomes. We introduce the Atlas of Human-AI Interaction, an interactive web
interface that provides the first systematic mapping of empirical findings
across 1,000+ HCI papers using LLM-powered knowledge extraction. Our approach
identifies causal relationships, and visualizes them through an AI-enabled
interactive web interface as a navigable knowledge graph. We extracted 2,037
empirical findings, revealing research topic clusters, common themes, and
disconnected areas. Expert evaluation with 20 researchers revealed the system's
effectiveness for discovering research gaps. This work demonstrates how AI can
transform literature synthesis itself, offering a scalable framework for
evidence-based design, opening new possibilities for computational meta-science
across HCI and beyond.

</details>


### [8] [XR Blocks: Accelerating Human-centered AI + XR Innovation](https://arxiv.org/abs/2509.25504)
*David Li,Nels Numan,Xun Qian,Yanhe Chen,Zhongyi Zhou,Evgenii Alekseev,Geonsun Lee,Alex Cooper,Min Xia,Scott Chung,Jeremy Nelson,Xiuxiu Yuan,Jolica Dias,Tim Bettridge,Benjamin Hersh,Michelle Huynh,Konrad Piascik,Ricardo Cabello,David Kim,Ruofei Du*

Main category: cs.HC

TL;DR: 本研究提出XR Blocks，一个跨平台框架，旨在加速人本中心的AI与XR创新，降低XR创作者的入门门槛，支持构建AI驱动的XR应用。


<details>
  <summary>Details</summary>
Motivation: 填补人工智能与扩展现实之间的生态系统差距，简化AI驱动的XR交互原型开发过程。

Method: 提供基于模块化架构的可插拔组件，结合WebXR、three.js、TensorFlow等技术构建工具包。

Result: 通过一系列开源模板、示例和高级演示，展示XR Blocks的实用性，帮助社区快速实现互动XR原型。

Conclusion: XR Blocks能够加速人本中心的人工智能与扩展现实创新，降低XR创作者的门槛，加速从概念到交互原型的实现。

Abstract: We are on the cusp where Artificial Intelligence (AI) and Extended Reality
(XR) are converging to unlock new paradigms of interactive computing. However,
a significant gap exists between the ecosystems of these two fields: while AI
research and development is accelerated by mature frameworks like JAX and
benchmarks like LMArena, prototyping novel AI-driven XR interactions remains a
high-friction process, often requiring practitioners to manually integrate
disparate, low-level systems for perception, rendering, and interaction. To
bridge this gap, we present XR Blocks, a cross-platform framework designed to
accelerate human-centered AI + XR innovation. XR Blocks strives to provide a
modular architecture with plug-and-play components for core abstraction in AI +
XR: user, world, peers; interface, context, and agents. Crucially, it is
designed with the mission of "reducing frictions from idea to reality", thus
accelerating rapid prototyping of AI + XR apps. Built upon accessible
technologies (WebXR, three.js, TensorFlow, Gemini), our toolkit lowers the
barrier to entry for XR creators. We demonstrate its utility through a set of
open-source templates, samples, and advanced demos, empowering the community to
quickly move from concept to interactive XR prototype. Site:
https://xrblocks.github.io

</details>


### [9] [User Prompting Strategies and ChatGPT Contextual Adaptation Shape Conversational Information-Seeking Experiences](https://arxiv.org/abs/2509.25513)
*Haoning Xue,Yoo Jung Oh,Xinyi Zhou,Xinyu Zhang,Berit Oxley*

Main category: cs.HC

TL;DR: 研究分析了937名美国成人与ChatGPT的对话行为，发现用户提示存在教育和政治倾向的不平等，同时揭示了ChatGPT在不同话题上的适应性。


<details>
  <summary>Details</summary>
Motivation: 探讨普通用户如何实际应用提示以及ChatGPT如何在现实世界的信息搜索中调整其回复。

Method: 对937名美国成人在多轮对话中与ChatGPT进行信息搜索进行分析，涵盖科学、健康和政策等领域的有争议和非争议话题。

Result: 只有19.1%的用户采用了提示策略，并且这些用户通常教育水平较高且倾向于民主党。同时，ChatGPT在各类话题上表现出语境适应性，有争议的话题回复更具认知复杂性和外部引用，虽然认知复杂的回复被认为不如简单的回复受欢迎，但在相关问题上，产生了更积极的态度。

Conclusion: 本研究揭示了用户促进行为的差异以及用户提示和AI回复如何共同影响信息搜索，尤其是在复杂和有争议的话题上。

Abstract: Conversational AI, such as ChatGPT, is increasingly used for information
seeking. However, little is known about how ordinary users actually prompt and
how ChatGPT adapts its responses in real-world conversational information
seeking (CIS). In this study, a nationally representative sample of 937 U.S.
adults engaged in multi-turn CIS with ChatGPT on both controversial and
non-controversial topics across science, health, and policy contexts. We
analyzed both user prompting strategies and the communication styles of ChatGPT
responses. The findings revealed behavioral signals of digital divide: only
19.1% of users employed prompting strategies, and these users were
disproportionately more educated and Democrat-leaning. Further, ChatGPT
demonstrated contextual adaptation: responses to controversial topics contain
more cognitive complexity and more external references than to
non-controversial topics. Notably, cognitively complex responses were perceived
as less favorable but produced more positive issue-relevant attitudes. This
study highlights disparities in user prompting behaviors and shows how user
prompts and AI responses together shape information-seeking with conversational
AI.

</details>


### [10] [Healthy Lifestyles and Self-Improvement Videos on YouTube: A Thematic Analysis of Teen-Targeted Social Media Content](https://arxiv.org/abs/2509.25537)
*Kyuha Jung,Tyler Kim,Yunan Chen*

Main category: cs.HC

TL;DR: 本研究通过分析YouTube视频和评论，探讨了青少年社交媒体内容的作用与影响，提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 青少年越来越依赖社交媒体获取健康信息，理解其内容价值变得重要。

Method: 对44个YouTube视频及66,901条评论进行主题分析。

Result: 这些视频使用引人入胜的叙事，帮助青少年应对挑战，同时也存在一些潜在的有害建议。

Conclusion: 社交媒体视频能为青少年提供多样的建议和社区支持，但也存在误导性信息。

Abstract: As teenagers increasingly turn to social media for health-related
information, understanding the values of teen-targeted content has become
important. Although videos on healthy lifestyles and self-improvement are
gaining popularity on social media platforms like YouTube, little is known
about how these videos benefit and engage with teenage viewers. To address
this, we conducted a thematic analysis of 44 YouTube videos and 66,901
comments. We found that these videos provide various advice on teenagers'
common challenges, use engaging narratives for authenticity, and foster
teen-centered communities through comments. However, a few videos also gave
misleading advice to adolescents that can be potentially harmful. Based on our
findings, we discuss design implications for creating relatable and intriguing
social media content for adolescents. Additionally, we suggest ways for social
media platforms to promote healthier and safer experiences for teenagers.

</details>


### [11] [Supporting Creative Ownership through Deep Learning-Based Music Variation](https://arxiv.org/abs/2509.25834)
*Stephen James Krol,Maria Teresa Llano,Jon McCormack*

Main category: cs.HC

TL;DR: 本文研究了个人拥有感在音乐AI设计中的重要性，表明工具应支持而非取代音乐创造力，强调了人类音乐表达的特性。


<details>
  <summary>Details</summary>
Motivation: 探讨个人拥有感在音乐人工智能设计中的重要性，帮助实践音乐家维护对创作过程的控制。

Method: 通过四周的生态评估，研究音乐变异工具在作曲环境中的运作。

Result: 研究发现，音乐工具依赖于音乐家的能力，促进了对创作过程和成果的个人拥有感，并揭示了技术能力与艺术身份之间的紧张关系。

Conclusion: 本文的研究表明，音乐人工智能工具应当支持而非取代人类创意，并强调了设计保持人类音乐表达特性的工具的重要性。

Abstract: This paper investigates the importance of personal ownership in musical AI
design, examining how practising musicians can maintain creative control over
the compositional process. Through a four-week ecological evaluation, we
examined how a music variation tool, reliant on the skill of musicians,
functioned within a composition setting. Our findings demonstrate that the
dependence of the tool on the musician's ability, to provide a strong initial
musical input and to turn moments into complete musical ideas, promoted
ownership of both the process and artefact. Qualitative interviews further
revealed the importance of this personal ownership, highlighting tensions
between technological capability and artistic identity. These findings provide
insight into how musical AI can support rather than replace human creativity,
highlighting the importance of designing tools that preserve the humanness of
musical expression.

</details>


### [12] [Photographic Conviviality: A Synchronic and Symbiotic Photographic Experience through a Body Paint Workshop](https://arxiv.org/abs/2509.25968)
*Chinatsu Ozawa,Tatsuya Minagawa,Yoichi Ochiai*

Main category: cs.HC

TL;DR: 本研究探讨了“照片纹身”的概念，将摄影与身体艺术结合，提出“摄影共生”的理念，通过即时摄影，促进个人表达和亲密体验。


<details>
  <summary>Details</summary>
Motivation: To explore the integration of photography and body ornamentation to enhance personal expression.

Method: Through workshops using an instant camera that prints onto mesh screens.

Result: The fusion creates dynamic experiences from static images, challenging traditional photography.

Conclusion: "Photo Tattooing" redefines photography's role in personal expression by merging it with body art, fostering intimacy.

Abstract: This study explores "Photo Tattooing," merging photography and body
ornamentation, and introduces the concept of "Photographic Conviviality." Using
our instant camera that prints images onto mesh screens for immediate body art,
we examine how this integration affects personal expression and challenges
traditional photography. Workshops revealed that this fusion redefines
photography's role, fostering intimacy and shared experiences, and opens new
avenues for self-expression by transforming static images into dynamic,
corporeal experiences.

</details>


### [13] [Dia-Lingle: A Gamified Interface for Dialectal Data Collection](https://arxiv.org/abs/2509.26210)
*Jiugeng Sun,Rita Sevastjanova,Sina Ahmadi,Rico Sennrich,Mennatallah El-Assady*

Main category: cs.HC

TL;DR: Dia-Lingle是一个游戏化平台，旨在解决方言数据收集难题，其设计促进用户参与，提高方言语料库的丰富性。


<details>
  <summary>Details</summary>
Motivation: 方言在计算文本资源上稀缺，主要以口语存在，且存在显著的地理多样性，数据收集面临重大挑战。

Method: 通过两大主要功能——用户改写方言句子和将句子与地理位置匹配，结合主动学习和游戏化难度策略来收集数据。

Result: 用户评价表明，Dia-Lingle的可用性高，用户满意度高，能有效扩展方言语料库。

Conclusion: Dia-Lingle是一种有效的工具，通过游戏化界面促进和改善方言数据的收集，表现出高用户满意度。

Abstract: Dialects suffer from the scarcity of computational textual resources as they
exist predominantly in spoken rather than written form and exhibit remarkable
geographical diversity. Collecting dialect data and subsequently integrating it
into current language technologies present significant obstacles. Gamification
has been proven to facilitate remote data collection processes with great ease
and on a substantially wider scale. This paper introduces Dia-Lingle, a
gamified interface aimed to improve and facilitate dialectal data collection
tasks such as corpus expansion and dialect labelling. The platform features two
key components: the first challenges users to rewrite sentences in their
dialects, identifies them through a classifier and solicits feedback, and the
other one asks users to match sentences to their geographical locations.
Dia-Lingle combines active learning with gamified difficulty levels,
strategically encouraging prolonged user engagement while efficiently enriching
the dialect corpus. Usability evaluation shows that our interface demonstrates
high levels of user satisfaction. We provide the link to Dia-Lingle:
https://dia-lingle.ivia.ch/, and demo video: https://youtu.be/0QyJsB8ym64.

</details>


### [14] [From Code to Concept: Evaluating Multiple Coordinated Views in Introductory Programming](https://arxiv.org/abs/2509.26466)
*Naaz Sibia,Valeria Ramirez Osorio,Jessica Wen,Rutwa Engineer,Angela Zavaleta Bernuy,Andrew Petersen,Michael Liut,Carolina Nobre*

Main category: cs.HC

TL;DR: 本研究探讨了多种可视化工具对新手程序员学习的影响，发现多重表现的可视化方式能提高参与度，且适应不同学习者的设计更为有效。


<details>
  <summary>Details</summary>
Motivation: 新手程序员在理解代码执行及形成抽象思维模型方面存在困难，这在背景、语言能力和经验多样的课堂中更加突出。

Method: 本研究在一门高人数的初级Python课程中进行，为期12周，比较了多重表现的可视化与单一可视化及文本方法的效果。

Result: 使用多重表现的可视化方式相比于单一可视化和文本方法，能够显著提高学生的参与度，尽管认知负荷在各个条件下并无显著差异。

Conclusion: 研究表明，多种外部表征与适应不同学习者的支撑结合，可更有效地支持初学者学习编程。

Abstract: Novice programmers often struggle to understand how code executes and to form
the abstract mental models necessary for effective problem-solving, challenges
that are amplified in large, diverse introductory courses where students'
backgrounds, language proficiencies, and prior experiences vary widely. This
study examines whether interactive, multi-representational visualizations,
combining synchronized code views, memory diagrams, and conceptual analogies,
can help manage cognitive load and foster engagement more effectively than
single-visual or text-only approaches. Over a 12-week deployment in a
high-enrolment introductory Python course (N = 829), students who relied solely
on text-based explanations reported significantly higher immediate mental
effort than those using visual aids, although overall cognitive load did not
differ significantly among conditions. The multi-representational approach
consistently yielded higher engagement than both single-visual and text-only
methods. Usage logs indicated that learners' interaction patterns varied with
topic complexity, and predictive modelling suggested that early experiences of
high cognitive load were associated with lower longer-term perceptions of
clarity and helpfulness. Individual differences, including language proficiency
and prior programming experience, moderated these patterns. By integrating
multiple external representations with scaffolded support adapted to diverse
learner profiles, our findings highlight design considerations for creating
visualization tools that more effectively support novices learning to program.

</details>


### [15] [The Invisible Mentor: Inferring User Actions from Screen Recordings to Recommend Better Workflows](https://arxiv.org/abs/2509.26557)
*Litao Yan,Andrew Head,Ken Milne,Vu Le,Sumit Gulwani,Chris Parnin,Emerson Murphy-Hill*

Main category: cs.HC

TL;DR: InvisibleMentor是一个新系统，通过屏幕录制自动识别低效工作流程并提供建议，超越了传统的AI助手。


<details>
  <summary>Details</summary>
Motivation: 许多用户在功能丰富的工具（如Excel）中难以察觉更高效的工作流程，现有的AI助手仅在用户描述目标或问题后提供帮助。

Method: 采用双阶段管道，使用视觉语言模型重建动作和上下文，再通过语言模型生成结构化建议。

Result: InvisibleMentor成功识别低效工作流程，其建议被参与者认为比基于提示的电子表格助手更加可操作、个性化和有助于学习和改进。

Conclusion: InvisibleMentor通过分析屏幕录制，准确识别低效工作流程，并提供可操作、个性化的建议，超越了传统的AI助手。

Abstract: Many users struggle to notice when a more efficient workflow exists in
feature-rich tools like Excel. Existing AI assistants offer help only after
users describe their goals or problems, which can be effortful and imprecise.
We present InvisibleMentor, a system that turns screen recordings of task
completion into vision-grounded reflections on tasks. It detects issues such as
repetitive edits and recommends more efficient alternatives based on observed
behavior. Unlike prior systems that rely on logs, APIs, or user prompts,
InvisibleMentor operates directly on screen recordings. It uses a two-stage
pipeline: a vision-language model reconstructs actions and context, and a
language model generates structured, high-fidelity suggestions. In evaluation,
InvisibleMentor accurately identified inefficient workflows, and participants
found its suggestions more actionable, tailored, and more helpful for learning
and improvement compared to a prompt-based spreadsheet assistant.

</details>


### [16] [Exploring Large Language Model as an Interactive Sports Coach: Lessons from a Single-Subject Half Marathon Preparation](https://arxiv.org/abs/2509.26593)
*Kichang Lee*

Main category: cs.HC

TL;DR: 这项研究探讨了大型语言模型作为虚拟教练在半程马拉松训练中的应用，展示了表现提升并提出未来系统设计需求。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLM）作为长期虚拟教练的潜力，以提升运动表现。

Method: 通过单一主体案例研究，记录LLM驱动的半程马拉松训练，为期两个月，利用文本互动和消费者应用日志。

Result: 参与者的表现从每公里7分54秒的2公里提升至每公里6分30秒的21.1公里，同时在频率、配速、心率耦合和效率指数趋势上有所进步。

Conclusion: 这项研究提供了证据和设计议程，推动大型语言模型（LLM）从传统顾问转变为闭环教练伴侣。

Abstract: Large language models (LLMs) are emerging as everyday assistants, but their
role as longitudinal virtual coaches is underexplored. This two-month single
subject case study documents LLM guided half marathon preparation
(July-September 2025). Using text based interactions and consumer app logs, the
LLM acted as planner, explainer, and occasional motivator. Performance improved
from sustaining 2 km at 7min 54sec per km to completing 21.1 km at 6min 30sec
per km, with gains in cadence, pace HR coupling, and efficiency index trends.
While causal attribution is limited without a control, outcomes demonstrate
safe, measurable progress. At the same time, gaps were evident, no realtime
sensor integration, text only feedback, motivation support that was user
initiated, and limited personalization or safety guardrails. We propose design
requirements for next generation systems, persistent athlete models with
explicit guardrails, multimodal on device sensing, audio, haptic, visual
feedback, proactive motivation scaffolds, and privacy-preserving
personalization. This study offers grounded evidence and a design agenda for
evolving LLMs from retrospective advisors to closed-loop coaching companions.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [17] [When and How to Express Empathy in Human-Robot Interaction Scenarios](https://arxiv.org/abs/2509.25200)
*Christian Arzate Cruz,Edwin C. Montiel-Vazquez,Chikara Maeda,Randy Gomez*

Main category: cs.RO

TL;DR: 本文提出了whEE框架，通过识别同理心信号来提高社交机器人的互动质量。


<details>
  <summary>Details</summary>
Motivation: 将同理心行为融入机器人中，以提高其社会效能和互动质量。

Method: 利用大型语言模型，whEE框架在社交机器人Haru的与人互动场景中进行评价。

Result: 研究结果表明，whEE能够有效识别和应对同理心信号。

Conclusion: whEE框架能够有效识别并响应同理心信号，为设计能够在不同互动上下文中自适应调整同理心水平的社交机器人提供了宝贵的见解。

Abstract: Incorporating empathetic behavior into robots can improve their social
effectiveness and interaction quality. In this paper, we present whEE (when and
how to express empathy), a framework that enables social robots to detect when
empathy is needed and generate appropriate responses. Using large language
models, whEE identifies key behavioral empathy cues in human interactions. We
evaluate it in human-robot interaction scenarios with our social robot, Haru.
Results show that whEE effectively identifies and responds to empathy cues,
providing valuable insights for designing social robots capable of adaptively
modulating their empathy levels across various interaction contexts.

</details>


### [18] [BEV-VLM: Trajectory Planning via Unified BEV Abstraction](https://arxiv.org/abs/2509.25249)
*Guancheng Chen,Sheng Yang,Tong Zhan,Jian Wang*

Main category: cs.RO

TL;DR: 本文提出了BEV-VLM框架，通过利用BEV特征图提高自主驾驶轨迹规划精度。


<details>
  <summary>Details</summary>
Motivation: 传统方法主要依赖原始视觉数据，缺乏处理压缩信息的能力。本研究试图利用BEV特征提升轨迹规划的准确性和可靠性。

Method: 该论文提出了BEV-VLM框架，结合多模态传感器数据和高清地图，生成BEV特征图用于轨迹规划。

Result: 实验结果显示，该方法在nuScenes数据集中实现了44.8%的规划精度提升，并有效避免碰撞。

Conclusion: 该研究表明，VLMs能有效处理BEV特征，以增强自主驾驶中的轨迹规划能力。

Abstract: This paper introduces BEV-VLM, a novel framework for trajectory planning in
autonomous driving that leverages Vision-Language Models (VLMs) with Bird's-Eye
View (BEV) feature maps as visual inputs. Unlike conventional approaches that
rely solely on raw visual data such as camera images, our method utilizes
highly compressed and informative BEV representations, which are generated by
fusing multi-modal sensor data (e.g., camera and LiDAR) and aligning them with
HD Maps. This unified BEV-HD Map format provides a geometrically consistent and
rich scene description, enabling VLMs to perform accurate trajectory planning.
Experimental results on the nuScenes dataset demonstrate 44.8% improvements in
planning accuracy and complete collision avoidance. Our work highlights that
VLMs can effectively interpret processed visual representations like BEV
features, expanding their applicability beyond raw images in trajectory
planning.

</details>


### [19] [SRMP: Search-Based Robot Motion Planning Library](https://arxiv.org/abs/2509.25352)
*Itamar Mishani,Yorai Shaoul,Ramkumar Natarajan,Jiaoyang Li,Maxim Likhachev*

Main category: cs.RO

TL;DR: SRMP 是一种新开发的软件框架，专为解决机器人操作中的运动规划问题，具有高度一致性和可靠性，特别适于多机器人任务，能与主流仿真器集成，并在工业应用中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有工具在高风险应用中的可预测性和可重复性不足，导致安全和高质量运动数据集的创建面临挑战。

Method: SRMP 是一个新的软件框架，专为机器人操作设计，通过生成一致和可靠的轨迹，提供运动规划算法，特别适用于多机器人操作任务。

Result: 通过广泛评估，SRMP 显示其在工业和安全关键应用中的出色表现，支持与多个主要仿真器的集成，并提供 MoveIt! 插件以便于部署。

Conclusion: SRMP 满足工业和安全关键应用的严格要求，并在多种机器人系统中设定了运动规划的新标准。

Abstract: Motion planning is a critical component in any robotic system. Over the
years, powerful tools like the Open Motion Planning Library (OMPL) have been
developed, offering numerous motion planning algorithms. However, existing
frameworks often struggle to deliver the level of predictability and
repeatability demanded by high-stakes applications -- ranging from ensuring
safety in industrial environments to the creation of high-quality motion
datasets for robot learning. Complementing existing tools, we introduce SRMP
(Search-based Robot Motion Planning), a new software framework tailored for
robotic manipulation. SRMP distinguishes itself by generating consistent and
reliable trajectories, and is the first software tool to offer motion planning
algorithms for multi-robot manipulation tasks. SRMP easily integrates with
major simulators, including MuJoCo, Sapien, Genesis, and PyBullet via a Python
and C++ API. SRMP includes a dedicated MoveIt! plugin that enables immediate
deployment on robot hardware and seamless integration with existing pipelines.
Through extensive evaluations, we demonstrate in this paper that SRMP not only
meets the rigorous demands of industrial and safety-critical applications but
also sets a new standard for consistency in motion planning across diverse
robotic systems. Visit srmp.readthedocs.io for SRMP documentation and
tutorials.

</details>


### [20] [SARM: Stage-Aware Reward Modeling for Long Horizon Robot Manipulation](https://arxiv.org/abs/2509.25358)
*Qianzhong Chen,Justin Yu,Mac Schwager,Pieter Abbeel,Fred Shentu,Philipp Wu*

Main category: cs.RO

TL;DR: 提出了一种视频基础的奖励建模框架，显著提升了机器人在长远操作中的成功率，验证了其在模仿学习中的关键作用。


<details>
  <summary>Details</summary>
Motivation: 解决大规模机器人学习在长时间、多接触操作任务中的不一致性和不稳定性。

Method: 引入了一种基于视频的奖励建模框架，通过自然语言子任务注释自动生成奖励标签，并结合奖励对行为克隆进行优化。

Result: 奖励模型在验证和真实机器人实验中均超过基线，结合RA-BC方法在折叠T恤时成功率显著提高。

Conclusion: 我们的方法在长时间操作中实现了显著的成功率，证明了奖励建模在大规模模仿学习中的重要性。

Abstract: Large-scale robot learning has recently shown promise for enabling robots to
perform complex tasks by integrating perception, control, and language
understanding. Yet, it struggles with long-horizon, contact-rich manipulation
such as deformable object handling, where demonstration quality is
inconsistent. Reward modeling offers a natural solution: by providing grounded
progress signals, it transforms noisy demonstrations into stable supervision
that generalizes across diverse trajectories. We introduce a stage-aware,
video-based reward modeling framework that jointly predicts high-level task
stages and fine-grained progress. Reward labels are automatically derived from
natural language subtask annotations, ensuring consistent progress estimation
across variable-length demonstrations. This design overcomes frame-index
labeling, which fails in variable-duration tasks like folding a T-shirt. Our
reward model demonstrates robustness to variability, generalization to
out-of-distribution settings, and strong utility for policy training. Building
on it, we propose Reward-Aligned Behavior Cloning (RA-BC), which filters
high-quality data and reweights samples by reward. Experiments show the reward
model alone outperforms baselines on validation and real robot rollouts.
Integrated into RA-BC, our approach achieves 83\% success on folding T-shirts
from the flattened state and 67\% from the crumpled state -- far surpassing
vanilla behavior cloning, which attains only 8\% and 0\% success. Overall, our
results highlight reward modeling as a key enabler for scalable,
annotation-efficient, and robust imitation learning in long-horizon
manipulation.

</details>


### [21] [Parallel Heuristic Search as Inference for Actor-Critic Reinforcement Learning Models](https://arxiv.org/abs/2509.25402)
*Hanlan Yang,Itamar Mishani,Luca Pivetti,Zachary Kingston,Maxim Likhachev*

Main category: cs.RO

TL;DR: 提出一种新型的高效并行Actor-Critic模型推理算法	extit{PACHs}，在机器人任务中证明了其提升效率与稳定性。


<details>
  <summary>Details</summary>
Motivation: 尽管已有很多研究关注训练稳定性和数据采样效率，但现有的执行策略仍然较为简单，主要依赖直接的演员策略展开。

Method: 提出了一种并行的最佳优先搜索算法，通过演员网络生成动作，同时利用评论家网络提供的成本估计指导搜索过程。

Result: 在机器人操作任务中，包括无碰撞运动规划和接触丰富的交互中，展示了我们方法的有效性。

Conclusion: 	extit{PACHs}在机器人学习任务中展示了有效性，显著提升了推理的效率和稳定性。

Abstract: Actor-Critic models are a class of model-free deep reinforcement learning
(RL) algorithms that have demonstrated effectiveness across various robot
learning tasks. While considerable research has focused on improving training
stability and data sampling efficiency, most deployment strategies have
remained relatively simplistic, typically relying on direct actor policy
rollouts. In contrast, we propose \pachs{} (\textit{P}arallel
\textit{A}ctor-\textit{C}ritic \textit{H}euristic \textit{S}earch), an
efficient parallel best-first search algorithm for inference that leverages
both components of the actor-critic architecture: the actor network generates
actions, while the critic network provides cost-to-go estimates to guide the
search. Two levels of parallelism are employed within the search -- actions and
cost-to-go estimates are generated in batches by the actor and critic networks
respectively, and graph expansion is distributed across multiple threads. We
demonstrate the effectiveness of our approach in robotic manipulation tasks,
including collision-free motion planning and contact-rich interactions such as
non-prehensile pushing. Visit p-achs.github.io for demonstrations and examples.

</details>


### [22] [CoTaP: Compliant Task Pipeline and Reinforcement Learning of Its Controller with Compliance Modulation](https://arxiv.org/abs/2509.25443)
*Zewen He,Chenyuan Chen,Dilshod Azizov,Yoshihiko Nakamura*

Main category: cs.RO

TL;DR: 提出一种新的类人机器人顺应性控制方法CoTaP，结合双智能体强化学习和模型顺应性控制，在模拟中展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了提升类人机器人的整体运动控制能力，解决现有学习控制方法在真实环境中缺乏柔顺性的问题。

Method: 提出了一种二阶段双智能体强化学习框架，并结合基于模型的顺应性控制，以实现类人机器人更好地应对真实环境中的互动。

Result: 通过训练一个基础策略和结合模型顺应性控制的上半身策略，优化了类人机器人的运动表现，确保系统稳定性。

Conclusion: 所提出的CoTaP策略在模拟中验证了其可行性，特别是在不同顺应性设置下对外部干扰的响应比较中表现良好。

Abstract: Humanoid whole-body locomotion control is a critical approach for humanoid
robots to leverage their inherent advantages. Learning-based control methods
derived from retargeted human motion data provide an effective means of
addressing this issue. However, because most current human datasets lack
measured force data, and learning-based robot control is largely
position-based, achieving appropriate compliance during interaction with real
environments remains challenging. This paper presents Compliant Task Pipeline
(CoTaP): a pipeline that leverages compliance information in the learning-based
structure of humanoid robots. A two-stage dual-agent reinforcement learning
framework combined with model-based compliance control for humanoid robots is
proposed. In the training process, first a base policy with a position-based
controller is trained; then in the distillation, the upper-body policy is
combined with model-based compliance control, and the lower-body agent is
guided by the base policy. In the upper-body control, adjustable task-space
compliance can be specified and integrated with other controllers through
compliance modulation on the symmetric positive definite (SPD) manifold,
ensuring system stability. We validated the feasibility of the proposed
strategy in simulation, primarily comparing the responses to external
disturbances under different compliance settings.

</details>


### [23] [Online Mapping for Autonomous Driving: Addressing Sensor Generalization and Dynamic Map Updates in Campus Environments](https://arxiv.org/abs/2509.25542)
*Zihan Zhang,Abhijit Ravichandran,Pragnya Korti,Luobin Wang,Henrik I. Christensen*

Main category: cs.RO

TL;DR: 本研究开发了一个基于校园高尔夫球车的平台，使用高级传感器进行在线HD地图生成，解决传统方法的问题，展示了其在自主驾驶中的实际应用。


<details>
  <summary>Details</summary>
Motivation: 解决传统HD地图生成 labor-intensive, expensive和在动态环境中难以维护的挑战。

Method: 在一个校园高尔夫球车平台上部署了配备双前摄像头和LiDAR传感器的在线映射系统。

Result: 通过使用校园特定数据进行微调，生成准确的地图预测，并支持持续更新。

Conclusion: 本研究展示了一个在线地图生成系统的实际应用，为自主驾驶提供了准确的HD地图，并支持持续更新，以适应动态环境的变化。

Abstract: High-definition (HD) maps are essential for autonomous driving, providing
precise information such as road boundaries, lane dividers, and crosswalks to
enable safe and accurate navigation. However, traditional HD map generation is
labor-intensive, expensive, and difficult to maintain in dynamic environments.
To overcome these challenges, we present a real-world deployment of an online
mapping system on a campus golf cart platform equipped with dual front cameras
and a LiDAR sensor. Our work tackles three core challenges: (1) labeling a 3D
HD map for campus environment; (2) integrating and generalizing the SemVecMap
model onboard; and (3) incrementally generating and updating the predicted HD
map to capture environmental changes. By fine-tuning with campus-specific data,
our pipeline produces accurate map predictions and supports continual updates,
demonstrating its practical value in real-world autonomous driving scenarios.

</details>


### [24] [Exhaustive-Serve-Longest Control for Multi-robot Scheduling Systems](https://arxiv.org/abs/2509.25556)
*Mohammad Merati,David Castañón*

Main category: cs.RO

TL;DR: 本研究提出了ESL策略用于优化多机器人调度，实验证明其在减少持有成本和平均队列长度方面的优越性，适合实际应用。


<details>
  <summary>Details</summary>
Motivation: 研究多机器人、多队列系统中的在线任务分配，考虑到随机到达和切换延迟的情况。

Method: 通过折扣成本的马尔可夫决策过程建模在线任务分配，并提出了一种名为Exhaustive-Serve-Longest（ESL）的实时策略。

Result: 在不同的服务器到位置比率和负载条件下，ESL策略 consistently yields lower discounted holding cost和更小的平均队列长度。

Conclusion: ESL策略在多机器人调度系统中表现出优越的实时性能，具有简单性和鲁棒性，适合作为实际应用的默认策略。

Abstract: We study online task allocation for multi-robot, multi-queue systems with
stochastic arrivals and switching delays. Time is slotted; each location can
host at most one robot per slot; service consumes one slot; switching between
locations incurs a one-slot travel delay; and arrivals are independent
Bernoulli processes. We formulate a discounted-cost Markov decision process and
propose Exhaustive-Serve-Longest (ESL), a simple real-time policy that serves
exhaustively when the current location is nonempty and, when idle, switches to
a longest unoccupied nonempty location, and we prove the optimality of this
policy. As baselines, we tune a fixed-dwell cyclic policy via a discrete-time
delay expression and implement a first-come-first-serve policy. Across
server-to-location ratios and loads, ESL consistently yields lower discounted
holding cost and smaller mean queue lengths, with action-time fractions showing
more serving and restrained switching. Its simplicity and robustness make ESL a
practical default for real-time multi-robot scheduling systems.

</details>


### [25] [Field Calibration of Hyperspectral Cameras for Terrain Inference](https://arxiv.org/abs/2509.25663)
*Nathaniel Hanson,Benjamin Pyatski,Samuel Hibbard,Gary Lvov,Oscar De La Garza,Charles DiMarzio,Kristen L. Dorsey,Taşkın Padır*

Main category: cs.RO

TL;DR: 本研究开发了HYPER DRIVE系统，通过移动机器人在不同光照条件下准确采集多波长高光谱图像，能够评估植被健康和土壤水分。


<details>
  <summary>Details</summary>
Motivation: 探讨地形内类间差异，如水分含量，如何影响车辆的地形通行能力，以及RGB视觉系统的局限性。

Method: 通过移动机器人收集和注册多波长高光谱图像，并在变化的光照条件下进行反射率校准。

Result: 成功开发了一个系统架构，通过高光谱成像技术实现了地形特性识别，并展示了其在实际应用中的有效性。

Conclusion: HYPER DRIVE系统能够在不同光照条件下准确收集和分析多波长高光谱图像，具有计算植被健康指数和土壤水分含量的能力。

Abstract: Intra-class terrain differences such as water content directly influence a
vehicle's ability to traverse terrain, yet RGB vision systems may fail to
distinguish these properties. Evaluating a terrain's spectral content beyond
red-green-blue wavelengths to the near infrared spectrum provides useful
information for intra-class identification. However, accurate analysis of this
spectral information is highly dependent on ambient illumination. We
demonstrate a system architecture to collect and register multi-wavelength,
hyperspectral images from a mobile robot and describe an approach to
reflectance calibrate cameras under varying illumination conditions. To
showcase the practical applications of our system, HYPER DRIVE, we demonstrate
the ability to calculate vegetative health indices and soil moisture content
from a mobile robot platform.

</details>


### [26] [dVLA: Diffusion Vision-Language-Action Model with Multimodal Chain-of-Thought](https://arxiv.org/abs/2509.25681)
*Junjie Wen,Minjie Zhu,Jiaming Liu,Zhiyuan Liu,Yicun Yang,Linfeng Zhang,Shanghang Zhang,Yichen Zhu,Yi Xu*

Main category: cs.RO

TL;DR: dVLA是一个新型的扩散基础的视觉-语言-动作机器人模型，展现了在多任务和真实环境中的高成功率，优化了性能并减少了推理时间。


<details>
  <summary>Details</summary>
Motivation: 旨在通过统一视觉感知、语言推理和机器人控制来构建下一代机器人系统，提升跨模态推理能力和适应新指令及物体的泛化能力。

Method: 通过联合优化感知、语言理解和动作，在单一扩散目标下进行训练，同时引入两种加速策略以减小推理延迟。

Result: 在LIBERO基准测试中，dVLA达到了96.4%的平均成功率，在真实的Franka机器人上成功完成多项任务，包括复杂的bin-picking任务。

Conclusion: dVLA展现了在实际应用中高性能的潜力，尤其是在复杂任务和真实世界环境下的表现出众。

Abstract: Vision-Language-Action (VLA) models are emerging as a next-generation
paradigm for robotics. We introduce dVLA, a diffusion-based VLA that leverages
a multimodal chain-of-thought to unify visual perception, language reasoning,
and robotic control in a single system. dVLA jointly optimizes perception,
language understanding, and action under a single diffusion objective, enabling
stronger cross-modal reasoning and better generalization to novel instructions
and objects. For practical deployment, we mitigate inference latency by
incorporating two acceleration strategies, a prefix attention mask and KV
caching, yielding up to around times speedup at test-time inference. We
evaluate dVLA in both simulation and the real world: on the LIBERO benchmark,
it achieves state-of-the-art performance with a 96.4% average success rate,
consistently surpassing both discrete and continuous action policies; on a real
Franka robot, it succeeds across a diverse task suite, including a challenging
bin-picking task that requires multi-step planning, demonstrating robust
real-world performance. Together, these results underscore the promise of
unified diffusion frameworks for practical, high-performance VLA robotics.

</details>


### [27] [Hierarchical Diffusion Motion Planning with Task-Conditioned Uncertainty-Aware Priors](https://arxiv.org/abs/2509.25685)
*Amelie Minji Kim,Anqi Wu,Ye Zhao*

Main category: cs.RO

TL;DR: 提出一种新的分层扩散规划方法，通过结构化高斯模型改进成功率和轨迹平滑度，实验证明优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统的扩散基础规划方法使用零均值、各向同性的高斯噪声，而我们的方法使用任务条件下的结构化高斯。

Method: 提出了一种新的分层扩散规划方法，直接在噪音模型中嵌入任务和运动结构，并将高斯过程运动规划（GPMP）产生的稀疏任务中心关键状态作为带噪观测。

Result: 在Maze2D目标到达和KUKA块堆叠实验中，与各向同性基线相比，成功率更高，轨迹更平滑，任务对齐度更强。

Conclusion: 我们的方法能够在可行、平滑且语义上有意义的轨迹附近集中概率质量，同时保持可处理性。

Abstract: We propose a novel hierarchical diffusion planner that embeds task and motion
structure directly in the noise model. Unlike standard diffusion-based planners
that use zero-mean, isotropic Gaussian noise, we employ a family of
task-conditioned structured Gaussians whose means and covariances are derived
from Gaussian Process Motion Planning (GPMP): sparse, task-centric key states
or their associated timings (or both) are treated as noisy observations to
produce a prior instance. We first generalize the standard diffusion process to
biased, non-isotropic corruption with closed-form forward and posterior
expressions. Building on this, our hierarchy separates prior instantiation from
trajectory denoising: the upper level instantiates a task-conditioned
structured Gaussian (mean and covariance), and the lower level denoises the
full trajectory under that fixed prior. Experiments on Maze2D goal-reaching and
KUKA block stacking show improved success rates, smoother trajectories, and
stronger task alignment compared to isotropic baselines. Ablation studies
indicate that explicitly structuring the corruption process offers benefits
beyond simply conditioning the neural network. Overall, our method concentrates
probability mass of prior near feasible, smooth, and semantically meaningful
trajectories while maintaining tractability. Our project page is available at
https://hta-diffusion.github.io.

</details>


### [28] [OmniNav: A Unified Framework for Prospective Exploration and Visual-Language Navigation](https://arxiv.org/abs/2509.25687)
*Xinda Xue,Junjun Hu,Minghua Luo,Xie Shichao,Jintao Chen,Zixun Xie,Quan Kuichen,Guo Wei,Mu Xu,Zedong Chu*

Main category: cs.RO

TL;DR: OmniNav是一个统一的导航框架，有效整合多种导航模式，结合快速和慢速模块，应用大型训练数据集，成功提升机器人导航的成功率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有模型在多样化的导航范式中表现不佳，成功率低且泛化能力有限，需要一个统一的框架来解决该问题。

Method: OmniNav采用快速-慢速系统设计，用于生成和选择导航路径，结合短期和长期观察，以及大规模的通用训练数据集，提升导航策略和普适性。

Result: OmniNav通过轻量级、低延迟的策略精准预测连续空间的路径点，显著提高成功率和鲁棒性，超越传统动作块方法，支持实际部署，提升路径效率和轨迹一致性。

Conclusion: OmniNav在多种导航基准测试中表现出色，并在实际应用中验证了其有效性，为具身导航提供了可行的见解，铺就了朝向多功能、高度可泛化的机器人智能的可扩展路径。

Abstract: Embodied navigation presents a core challenge for intelligent robots,
requiring the comprehension of visual environments, natural language
instructions, and autonomous exploration. Existing models often fall short in
offering a unified solution across diverse navigation paradigms, resulting in
low success rates and limited generalization. We introduce OmniNav, a unified
framework addressing instruct-goal, object-goal, point-goal navigation, and
frontier-based exploration within a single architecture. Our approach features
a lightweight, low-latency policy that accurately predicts continuous-space
waypoints (coordinates and orientations). This policy surpasses action-chunk
methods in precision and supports real-world deployment at control frequencies
up to 5 Hz. Architecturally, OmniNav employs a fast-slow system design: a fast
module generates waypoints using short-horizon visual context and subtasks,
while a slow module performs deliberative planning with long-horizon
observations and candidate frontiers to select subsequent subgoals and
subtasks. This collaboration enhances path efficiency and maintains trajectory
coherence, particularly in exploration and memory-intensive scenarios.
Crucially, we identify that the primary bottleneck isn't merely navigation
policy learning, but a robust understanding of general instructions and
objects. To boost generalization, OmniNav integrates large-scale,
general-purpose training datasets, including those for image captioning and
visual recognition, into a joint multi-task regimen. This significantly
improves success rates and robustness. Extensive experiments confirm OmniNav's
state-of-the-art performance across various navigation benchmarks, with
real-world deployment further validating its efficacy. OmniNav provides
practical insights for embodied navigation, charting a scalable path towards
versatile, highly generalizable robotic intelligence.

</details>


### [29] [VLA Model Post-Training via Action-Chunked PPO and Self Behavior Cloning](https://arxiv.org/abs/2509.25718)
*Si-Cheng Wang,Tian-Yu Xiang,Xiao-Hu Zhou,Mei-Jiang Gui,Xiao-Liang Xie,Shi-Qi Liu,Shuang-Yi Wang,Ao-Qun Jin,Zeng-Guang Hou*

Main category: cs.RO

TL;DR: 本研究通过引入动作块和动态更新机制，成功改进了视觉-语言-行动后训练中的强化学习应用，展示了高成功率和有效性。


<details>
  <summary>Details</summary>
Motivation: 解决在后训练阶段稀疏奖励和不稳定训练的问题，从而提升视觉-语言-行动模型的实际应用能力。

Method: 引入基于近端策略优化（PPO）的动作块，并结合自收集演示进行行为克隆，采用动态更新的演示缓冲区及辅助行为克隆损失。

Result: 在MetaWorld基准测试中表现优于监督微调，成功率达到0.93，成功所需步骤为42.17。

Conclusion: 该研究展示了强化学习在视觉-语言-行动后训练中的应用潜力，并为后续的VLA应用奠定基础。

Abstract: Reinforcement learning (RL) is a promising avenue for post-training
vision-language-action (VLA) models, but practical deployment is hindered by
sparse rewards and unstable training. This work mitigates these challenges by
introducing an action chunk based on proximal policy optimization (PPO) with
behavior cloning using self-collected demonstrations. Aggregating consecutive
actions into chunks improves the temporal consistency of the policy and the
density of informative feedback. In addition, an auxiliary behavior cloning
loss is applied with a dynamically updated demonstration buffer that
continually collects high-quality task trials during training. The relative
weight between the action-chunked PPO objective and the self behavior clone
auxiliary loss is adapted online to stabilize the post-training process.
Experiments on the MetaWorld benchmark indicate improved performance over
supervised fine-tuning, achieving a high success rate (0.93) and few steps to
success (42.17). These results demonstrate the viability of RL for VLA
post-training and help lay the groundwork for downstream VLA applications.

</details>


### [30] [TacRefineNet: Tactile-Only Grasp Refinement Between Arbitrary In-Hand Object Poses](https://arxiv.org/abs/2509.25746)
*Shuaijun Wang,Haoran Zhou,Diyun Xiang,Yangwei You*

Main category: cs.RO

TL;DR: TacRefineNet是首个仅利用多指触觉传感器实现任意物体手部姿态精细调整的方法，显著提升了抓取精度。


<details>
  <summary>Details</summary>
Motivation: 应对长时间任务中抓取执行阶段面临的姿态不准确性问题，提升整体性能。

Method: 使用多指尖触觉感知结合多支路策略网络进行精确控制更新。

Result: 通过大规模的仿真数据和少量的真实数据训练，TacRefineNet展示了卓越的抓取准确性。

Conclusion: TacRefineNet是一个仅依靠触觉反馈实现已知物体在任意目标姿态下的精细手部姿态调整的框架，达到了毫米级的抓取精度。

Abstract: Despite progress in both traditional dexterous grasping pipelines and recent
Vision-Language-Action (VLA) approaches, the grasp execution stage remains
prone to pose inaccuracies, especially in long-horizon tasks, which undermines
overall performance. To address this "last-mile" challenge, we propose
TacRefineNet, a tactile-only framework that achieves fine in-hand pose
refinement of known objects in arbitrary target poses using multi-finger
fingertip sensing. Our method iteratively adjusts the end-effector pose based
on tactile feedback, aligning the object to the desired configuration. We
design a multi-branch policy network that fuses tactile inputs from multiple
fingers along with proprioception to predict precise control updates. To train
this policy, we combine large-scale simulated data from a physics-based tactile
model in MuJoCo with real-world data collected from a physical system.
Comparative experiments show that pretraining on simulated data and fine-tuning
with a small amount of real data significantly improves performance over
simulation-only training. Extensive real-world experiments validate the
effectiveness of the method, achieving millimeter-level grasp accuracy using
only tactile input. To our knowledge, this is the first method to enable
arbitrary in-hand pose refinement via multi-finger tactile sensing alone.
Project website is available at https://sites.google.com/view/tacrefinenet

</details>


### [31] [Best of Sim and Real: Decoupled Visuomotor Manipulation via Learning Control in Simulation and Perception in Real](https://arxiv.org/abs/2509.25747)
*Jialei Huang,Zhaoheng Yin,Yingdong Hu,Shuo Wang,Xingyu Lin,Yang Gao*

Main category: cs.RO

TL;DR: 提出了一种分离感知与控制的框架，使得机器人从仿真到实际操作的转移更高效。


<details>
  <summary>Details</summary>
Motivation: 解决机器人操作中的仿真实际转移问题，特别是感知与控制的复杂性。

Method: 在仿真中训练控制策略，同时在实际部署中调整感知，形成一种分离的学习框架。

Result: 在只有10-20次实际演示的情况下，该方法在桌面操作任务中展示了优越的数据效率和超出分布的泛化能力。

Conclusion: 分离感知与控制的学习方法显著提升了仿真实际转移的效率和效果。

Abstract: Sim-to-real transfer remains a fundamental challenge in robot manipulation
due to the entanglement of perception and control in end-to-end learning. We
present a decoupled framework that learns each component where it is most
reliable: control policies are trained in simulation with privileged state to
master spatial layouts and manipulation dynamics, while perception is adapted
only at deployment to bridge real observations to the frozen control policy.
Our key insight is that control strategies and action patterns are universal
across environments and can be learned in simulation through systematic
randomization, while perception is inherently domain-specific and must be
learned where visual observations are authentic. Unlike existing end-to-end
approaches that require extensive real-world data, our method achieves strong
performance with only 10-20 real demonstrations by reducing the complex
sim-to-real problem to a structured perception alignment task. We validate our
approach on tabletop manipulation tasks, demonstrating superior data efficiency
and out-of-distribution generalization compared to end-to-end baselines. The
learned policies successfully handle object positions and scales beyond the
training distribution, confirming that decoupling perception from control
fundamentally improves sim-to-real transfer.

</details>


### [32] [SAC Flow: Sample-Efficient Reinforcement Learning of Flow-Based Policies via Velocity-Reparameterized Sequential Modeling](https://arxiv.org/abs/2509.25756)
*Yixian Zhang,Shu'ang Yu,Tonghe Zhang,Mo Guang,Haojia Hui,Kaiwen Long,Yu Wang,Chao Yu,Wenbo Ding*

Main category: cs.RO

TL;DR: 本研究通过重参数化速度网络和引入新算法，解决了流式政策训练中的不稳定性问题，达成了高效的学习成果。


<details>
  <summary>Details</summary>
Motivation: 针对流式回放在多步动作采样过程中导致的不稳定性，探索其与循环神经网络（RNN）梯度消失和爆炸的基本关联。

Method: 通过重参数化速度网络并引入基于噪声增强的回放，开发了一种实用的SAC算法，支持从头开始和离线到在线学习。

Result: 提出了两种稳定架构：Flow-G（带门控速度）和Flow-T（使用解码速度），并成功地实现了表达流基政策的训练。

Conclusion: 该方法在连续控制和机器人操控基准测试中实现了最先进的性能，消除了对常见变通方案的需求。

Abstract: Training expressive flow-based policies with off-policy reinforcement
learning is notoriously unstable due to gradient pathologies in the multi-step
action sampling process. We trace this instability to a fundamental connection:
the flow rollout is algebraically equivalent to a residual recurrent
computation, making it susceptible to the same vanishing and exploding
gradients as RNNs. To address this, we reparameterize the velocity network
using principles from modern sequential models, introducing two stable
architectures: Flow-G, which incorporates a gated velocity, and Flow-T, which
utilizes a decoded velocity. We then develop a practical SAC-based algorithm,
enabled by a noise-augmented rollout, that facilitates direct end-to-end
training of these policies. Our approach supports both from-scratch and
offline-to-online learning and achieves state-of-the-art performance on
continuous control and robotic manipulation benchmarks, eliminating the need
for common workarounds like policy distillation or surrogate objectives.

</details>


### [33] [Act to See, See to Act: Diffusion-Driven Perception-Action Interplay for Adaptive Policies](https://arxiv.org/abs/2509.25822)
*Jing Wang,Weiting Peng,Jing Tang,Zeyu Gong,Xihua Wang,Bo Tao,Li Cheng*

Main category: cs.RO

TL;DR: 本文提出的DP--AG模型通过显式建模感知与动作之间的动态相互作用，能够在多个任务中实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法脱离了感知与动作的因果关系，未能充分利用人类自然调节行为的能力。

Method: 通过概率潜在动态模型，使用纲要一致的对比损失促进感知与行动之间的双向学习。

Result: DP--AG在多个基准测试中表现优异，推动了感知与行动的统一表示学习。

Conclusion: DP--AG在模拟基准和现实世界UR5操作任务中显著优于现有最先进方法，为生物适应性和人工策略学习架起了桥梁。

Abstract: Existing imitation learning methods decouple perception and action, which
overlooks the causal reciprocity between sensory representations and action
execution that humans naturally leverage for adaptive behaviors. To bridge this
gap, we introduce Action--Guided Diffusion Policy (DP--AG), a unified
representation learning that explicitly models a dynamic interplay between
perception and action through probabilistic latent dynamics. DP--AG encodes
latent observations into a Gaussian posterior via variational inference and
evolves them using an action-guided SDE, where the Vector-Jacobian Product
(VJP) of the diffusion policy's noise predictions serves as a structured
stochastic force driving latent updates. To promote bidirectional learning
between perception and action, we introduce a cycle--consistent contrastive
loss that organizes the gradient flow of the noise predictor into a coherent
perception--action loop, enforcing mutually consistent transitions in both
latent updates and action refinements. Theoretically, we derive a variational
lower bound for the action-guided SDE, and prove that the contrastive objective
enhances continuity in both latent and action trajectories. Empirically, DP--AG
significantly outperforms state--of--the--art methods across simulation
benchmarks and real-world UR5 manipulation tasks. As a result, our DP--AG
offers a promising step toward bridging biological adaptability and artificial
policy learning.

</details>


### [34] [Reinforced Embodied Planning with Verifiable Reward for Real-World Robotic Manipulation](https://arxiv.org/abs/2509.25852)
*Zitong Bo,Yue Hu,Jinming Ma,Mingliang Zhou,Junhui Yin,Yachen Kang,Yuqi Liu,Tong Wu,Diyun Xiang,Hao Chen*

Main category: cs.RO

TL;DR: 本论文提出REVER框架，通过RoboFarseer模型，将视觉-语言模型应用于长时间操作任务，实现较高成功率和效果。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在进行长时间操作任务时，缺乏大规模序列操作数据和稠密的可解释奖励的问题，以提高操作计划的能力。

Method: 提出REVER框架，训练并发布RoboFarseer模型，以自然语言指令生成和验证长时间的操作计划，通过自动注释引擎将演示转换为视觉-指令-计划三元组，使用可验证奖励来评分生成的计划。

Result: RoboFarseer在性能上匹配或超过了大规模的专有模型，在开放式规划中超过最佳基线40%以上，整体成功率提升约60%。

Conclusion: REVER框架通过将视觉-语言模型应用于长时间操作计划，实现了显著的性能提升，能够在真实场景中生成和验证操作计划。

Abstract: Enabling robots to execute long-horizon manipulation tasks from free-form
language instructions remains a fundamental challenge in embodied AI. While
vision-language models (VLMs) have shown promise as high-level planners, their
deployment in the real world is hindered by two gaps: (i) the scarcity of
large-scale, sequential manipulation data that couples natural language with
multi-step action plans, and (ii) the absence of dense, interpretable rewards
for fine-tuning VLMs on planning objectives. To address these issues, we
propose REVER, a framework that empowers VLMs to generate and validate
long-horizon manipulation plans from natural language instructions in
real-world scenarios. Under REVER we train and release RoboFarseer, a VLM
incentivized to emit chain-of-thought that perform temporal and spatial
reasoning, ensuring physically plausible and logically coherent plans. To
obtain training data, we leverage the Universal Manipulation Interface
framework to capture hardware-agnostic demonstrations of atomic skills. An
automated annotation engine converts each demonstration into
vision-instruction-plan triplet. We introduce a verifiable reward that scores
the generated plan by its ordered bipartite matching overlap with the
ground-truth skill sequence. At run time, the fine-tuned VLM functions both as
a planner and as a monitor, verifying step-wise completion. RoboFarseer matches
or exceeds the performance of proprietary models that are orders of magnitude
larger, while on open-ended planning it surpasses the best baseline by more
than 40%. In real-world, long-horizon tasks, the complete system boosts overall
success by roughly 60% compared with the same low-level controller without the
planner. We will open-source both the dataset and the trained model upon
publication.

</details>


### [35] [State Estimation for Compliant and Morphologically Adaptive Robots](https://arxiv.org/abs/2509.25945)
*Valentin Yuryev,Max Polzin,Josie Hughes*

Main category: cs.RO

TL;DR: 提出了一种新的基于神经网络的状态估计方法，能有效提升顺应性机器人在复杂环境中的状态估计精度，显著增强了机器人在故障情况下的操作能力。


<details>
  <summary>Details</summary>
Motivation: 针对运动机器人在不确定场景下的状态估计难题，尤其是形态变化和顺应性相关的状态。

Method: 基于神经网络的状态估计器，利用历史状态数据和对不可靠传感器的直接影响机制进行状态预测。

Result: 在GOAT平台上进行实验，状态预测精度达到形态测量误差4.2%、速度误差6.3%和2.4%、方向误差1.5度，并在电机故障情况下实现300%的行驶范围增加。

Conclusion: 该方法在运动捕捉数据下实现了可观的状态估计精度，特别是在恶劣环境下展现了显著的效能提升。

Abstract: Locomotion robots with active or passive compliance can show robustness to
uncertain scenarios, which can be promising for agricultural, research and
environmental industries. However, state estimation for these robots is
challenging due to the lack of rigid-body assumptions and kinematic changes
from morphing. We propose a method to estimate typical rigid-body states
alongside compliance-related states, such as soft robot shape in different
morphologies and locomotion modes. Our neural network-based state estimator
uses a history of states and a mechanism to directly influence unreliable
sensors. We test our framework on the GOAT platform, a robot capable of passive
compliance and active morphing for extreme outdoor terrain. The network is
trained on motion capture data in a novel compliance-centric frame that
accounts for morphing-related states. Our method predicts shape-related
measurements within 4.2% of the robot's size, velocities within 6.3% and 2.4%
of the top linear and angular speeds, respectively, and orientation within 1.5
degrees. We also demonstrate a 300% increase in travel range during a motor
malfunction when using our estimator for closed-loop autonomous outdoor
operation.

</details>


### [36] [Towards Intuitive Human-Robot Interaction through Embodied Gesture-Driven Control with Woven Tactile Skins](https://arxiv.org/abs/2509.25951)
*ChunPing Lam,Xiangjia Chen,Chenming Wu,Hao Chen,Binzhi Sun,Guoxin Fang,Charlie C. L. Wang,Chengkai Dai,Yeung Yam*

Main category: cs.RO

TL;DR: 本文提出了一种新的人机交互框架，通过基于电容的织物触觉皮肤实现直观的手势控制，并在实时手势识别中取得接近100%的准确率。


<details>
  <summary>Details</summary>
Motivation: 开发一种直观的手势驱动控制框架，以缩小人类意图与机器人响应之间的差距。

Method: 使用了一种轻量级的卷积-变换器模型进行实时手势识别，定义了14种手势与机器人的命令映射。

Result: 实验表明，与传统的键盘面板和教学工具相比，该系统在机器人任务上的完成时间减少了多达57%。

Conclusion: 该框架展示了朝向更加自然和高效的人机交互的可行路径。

Abstract: This paper presents a novel human-robot interaction (HRI) framework that
enables intuitive gesture-driven control through a capacitance-based woven
tactile skin. Unlike conventional interfaces that rely on panels or handheld
devices, the woven tactile skin integrates seamlessly with curved robot
surfaces, enabling embodied interaction and narrowing the gap between human
intent and robot response. Its woven design combines fabric-like flexibility
with structural stability and dense multi-channel sensing through the
interlaced conductive threads. Building on this capability, we define a
gesture-action mapping of 14 single- and multi-touch gestures that cover
representative robot commands, including task-space motion and auxiliary
functions. A lightweight convolution-transformer model designed for gesture
recognition in real time achieves an accuracy of near-100%, outperforming prior
baseline approaches. Experiments on robot arm tasks, including pick-and-place
and pouring, demonstrate that our system reduces task completion time by up to
57% compared with keyboard panels and teach pendants. Overall, our proposed
framework demonstrates a practical pathway toward more natural and efficient
embodied HRI.

</details>


### [37] [MUVLA: Learning to Explore Object Navigation via Map Understanding](https://arxiv.org/abs/2509.25966)
*Peilong Han,Fan Jia,Min Zhang,Yutao Qiu,Hongyao Tang,Yan Zheng,Tiancai Wang,Jianye Hao*

Main category: cs.RO

TL;DR: MUVLA是一种新模型，旨在提升对象导航能力，通过强化学习和模仿多种示范，实现对空间的深入理解和探索策略的优化。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在通过语义地图抽象化和历史信息的统一，提升对象导航中的动作预测能力。

Method: MUVLA通过三阶段的训练管道实现，包括学习地图级空间理解、模仿混合质量的示范行为和奖励扩增。

Result: MUVLA在HM3D和Gibson基准测试中表现出色，能够从低质量或部分成功的轨迹中学习到有效的探索行为。

Conclusion: MUVLA模型在物体导航任务中实现卓越的泛化能力，能有效学习探索行为。

Abstract: In this paper, we present MUVLA, a Map Understanding Vision-Language-Action
model tailored for object navigation. It leverages semantic map abstractions to
unify and structure historical information, encoding spatial context in a
compact and consistent form. MUVLA takes the current and history observations,
as well as the semantic map, as inputs and predicts the action sequence based
on the description of goal object. Furthermore, it amplifies supervision
through reward-guided return modeling based on dense short-horizon progress
signals, enabling the model to develop a detailed understanding of action value
for reward maximization. MUVLA employs a three-stage training pipeline:
learning map-level spatial understanding, imitating behaviors from
mixed-quality demonstrations, and reward amplification. This strategy allows
MUVLA to unify diverse demonstrations into a robust spatial representation and
generate more rational exploration strategies. Experiments on HM3D and Gibson
benchmarks demonstrate that MUVLA achieves great generalization and learns
effective exploration behaviors even from low-quality or partially successful
trajectories.

</details>


### [38] [S$^3$E: Self-Supervised State Estimation for Radar-Inertial System](https://arxiv.org/abs/2509.25984)
*Shengpeng Wang,Yulong Xie,Qing Liao,Wei Wang*

Main category: cs.RO

TL;DR: 我们提出了一种新的自监督状态估计方法S$^3$E，通过融合雷达谱和惯性数据，以解决雷达点云稀疏性和角分辨率有限的问题，实验表明该方法提供了准确的定位能力。


<details>
  <summary>Details</summary>
Motivation: 现有的定位解决方案依赖于处理后的雷达点云，但雷达点云的稀疏性和多路径效应导致状态估计性能显著下降。

Method: 提出了一种自监督的状态估计方法S$^3$E，通过更丰富的雷达信号光谱绕过稀疏点，并融合互补的惯性信息。

Result: 实验结果表明，S$^3$E在状态估计方面表现稳健且准确，不依赖于定位真值监督。

Conclusion: S$^3$E方法通过自监督融合雷达谱和惯性数据，实现了无需定位真值监督的稳健和准确的状态估计。

Abstract: Millimeter-wave radar for state estimation is gaining significant attention
for its affordability and reliability in harsh conditions. Existing
localization solutions typically rely on post-processed radar point clouds as
landmark points. Nonetheless, the inherent sparsity of radar point clouds,
ghost points from multi-path effects, and limited angle resolution in
single-chirp radar severely degrade state estimation performance. To address
these issues, we propose S$^3$E, a \textbf{S}elf-\textbf{S}upervised
\textbf{S}tate \textbf{E}stimator that employs more richly informative radar
signal spectra to bypass sparse points and fuses complementary inertial
information to achieve accurate localization. S$^3$E fully explores the
association between \textit{exteroceptive} radar and \textit{proprioceptive}
inertial sensor to achieve complementary benefits. To deal with limited angle
resolution, we introduce a novel cross-fusion technique that enhances spatial
structure information by exploiting subtle rotational shift correlations across
heterogeneous data. The experimental results demonstrate our method achieves
robust and accurate performance without relying on localization ground truth
supervision. To the best of our knowledge, this is the first attempt to achieve
state estimation by fusing radar spectra and inertial data in a complementary
self-supervised manner.

</details>


### [39] [Emotionally Expressive Robots: Implications for Children's Behavior toward Robot](https://arxiv.org/abs/2509.25986)
*Elisabetta Zibetti,Sureya Waheed Palmer,Rebecca Stower,Salvatore M Anzalone*

Main category: cs.RO

TL;DR: 本研究探讨了机器人的情感表达对儿童行为的影响，发现儿童在游戏中表现出与机器人情感状态的对齐，但高水平的表达未能增强这种对齐。


<details>
  <summary>Details</summary>
Motivation: 研究机器人的情感表达在儿童社交行为中的潜在影响，特别是在与机器人互动时。

Method: 对22名7-11岁儿童进行的初步研究，探索不同情感表达方式对儿童行为的影响。

Result: 儿童的行为与机器人所表现的情感状态相一致，但表现的强度没有显著提高这种一致性。

Conclusion: 机器人的情感表达对儿童的社交情感行为有一定影响，但情感表达的强度并未显著提高这种影响。

Abstract: The growing development of robots with artificial emotional expressiveness
raises important questions about their persuasive potential in children's
behavior. While research highlights the pragmatic value of emotional
expressiveness in human social communication, the extent to which robotic
expressiveness can or should influence empathic responses in children is
grounds for debate. In a pilot study with 22 children (aged 7-11) we begin to
explore the ways in which different levels of embodied expressiveness (body
only, face only, body and face) of two basic emotions (happiness and sadness)
displayed by an anthropomorphic robot (QTRobot) might modify children's
behavior in a child-robot cooperative turn-taking game. We observed that
children aligned their behavior to the robot's inferred emotional state.
However, higher levels of expressiveness did not result in increased alignment.
The preliminary results reported here provide a starting point for reflecting
on robotic expressiveness and its role in shaping children's social-emotional
behavior toward robots as social peers in the near future.

</details>


### [40] [On the Conic Complementarity of Planar Contacts](https://arxiv.org/abs/2509.25999)
*Yann de Mont-Marin,Louis Montaut,Jean Ponce,Martial Hebert,Justin Carpentier*

Main category: cs.RO

TL;DR: 本文提出了一种平面Signorini条件，通过圆锥互补性公式链接了点接触法则和中心压力，进而改善了机器人接触模拟及控制算法。


<details>
  <summary>Details</summary>
Motivation: 连接机器人学中的两个基础原则，分别是点接触的Signorini法则和零矩点（中心压力）概念，促进接触动态的准确模拟和控制算法的设计。

Method: 通过构建一个圆锥互补性公式，来建模刚体之间的一般平面接触并证明其与全接触表面上施加点Signorini法则的等价性。

Result: 实现了离散接触模型与连续接触模型之间的桥接，揭示了三个物理状态（粘附、分离和倾斜）的统一结构，扩展了经典中心压力。

Conclusion: 本文提出的平面Signorini条件为刚体之间的一般平面接触提供了数学上一致且计算上可行的基础，具有重要的模拟和控制应用意义。

Abstract: We present a unifying theoretical result that connects two foundational
principles in robotics: the Signorini law for point contacts, which underpins
many simulation methods for preventing object interpenetration, and the center
of pressure (also known as the zero-moment point), a key concept used in, for
instance, optimization-based locomotion control. Our contribution is the planar
Signorini condition, a conic complementarity formulation that models general
planar contacts between rigid bodies. We prove that this formulation is
equivalent to enforcing the punctual Signorini law across an entire contact
surface, thereby bridging the gap between discrete and continuous contact
models. A geometric interpretation reveals that the framework naturally
captures three physical regimes -sticking, separating, and tilting-within a
unified complementarity structure. This leads to a principled extension of the
classical center of pressure, which we refer to as the extended center of
pressure. By establishing this connection, our work provides a mathematically
consistent and computationally tractable foundation for handling planar
contacts, with implications for both the accurate simulation of contact
dynamics and the design of advanced control and optimization algorithms in
locomotion and manipulation.

</details>


### [41] [Conflict-Based Search and Prioritized Planning for Multi-Agent Path Finding Among Movable Obstacles](https://arxiv.org/abs/2509.26050)
*Shaoli Hu,Shizhe Zhao,Zhongqiang Ren*

Main category: cs.RO

TL;DR: 本文探讨多代理路径寻找中的可移动障碍物（M-PAMO），首次尝试将 CBS、PP 与 PAMO* 结合以解决该问题，并比较各种方法的性能。


<details>
  <summary>Details</summary>
Motivation: M-PAMO 在物流和仓库中引发研究兴趣，尤其是移动机器人与意外可移动物体的交互问题。

Method: 通过适应和融合 CBS、PP 和 PAMO*，为 M-PAMO 提供解决方案，并进行性能比较。

Result: 比较在多达 20 个代理和数百个可移动障碍物下的多种方法性能，展示了这些方法的优缺点。

Conclusion: 本文首次尝试将流行的基于冲突的搜索（CBS）和优先规划（PP）与单代理可移动障碍物规划器 PAMO* 结合，以解决 M-PAMO 问题。

Abstract: This paper investigates Multi-Agent Path Finding Among Movable Obstacles
(M-PAMO), which seeks collision-free paths for multiple agents from their start
to goal locations among static and movable obstacles. M-PAMO arises in
logistics and warehouses where mobile robots are among unexpected movable
objects. Although Multi-Agent Path Finding (MAPF) and single-agent Path
planning Among Movable Obstacles (PAMO) were both studied, M-PAMO remains
under-explored. Movable obstacles lead to new fundamental challenges as the
state space, which includes both agents and movable obstacles, grows
exponentially with respect to the number of agents and movable obstacles. In
particular, movable obstacles often closely couple agents together spatially
and temporally. This paper makes a first attempt to adapt and fuse the popular
Conflict-Based Search (CBS) and Prioritized Planning (PP) for MAPF, and a
recent single-agent PAMO planner called PAMO*, together to address M-PAMO. We
compare their performance with up to 20 agents and hundreds of movable
obstacles, and show the pros and cons of these approaches.

</details>


### [42] [Evolutionary Continuous Adaptive RL-Powered Co-Design for Humanoid Chin-Up Performance](https://arxiv.org/abs/2509.26082)
*Tianyi Jin,Melya Boukheddimi,Rohit Kumar,Gabriele Fadini,Frank Kirchner*

Main category: cs.RO

TL;DR: 本研究提出EA-CoRL框架，通过强化学习与进化策略的结合，优化机器人设计与控制，显著提高了人形机器人执行动态任务的能力。


<details>
  <summary>Details</summary>
Motivation: 强调传统机器人设计与控制分开开发的不足，倡导并行优化以提升机器人性能。

Method: 通过进化算法和强化学习结合，进行硬件选择和控制策略的共同优化。

Result: EA-CoRL框架在RH5人形机器人动态 chin-up 任务中表现优越，超越了现有的共同设计方法。

Conclusion: EA-CoRL框架在动态行为任务上取得显著进步，展示了局部适应性对机器人共同设计的重要性。

Abstract: Humanoid robots have seen significant advancements in both design and
control, with a growing emphasis on integrating these aspects to enhance
overall performance. Traditionally, robot design has followed a sequential
process, where control algorithms are developed after the hardware is
finalized. However, this can be myopic and prevent robots to fully exploit
their hardware capabilities. Recent approaches advocate for co-design,
optimizing both design and control in parallel to maximize robotic
capabilities. This paper presents the Evolutionary Continuous Adaptive RL-based
Co-Design (EA-CoRL) framework, which combines reinforcement learning (RL) with
evolutionary strategies to enable continuous adaptation of the control policy
to the hardware. EA-CoRL comprises two key components: Design Evolution, which
explores the hardware choices using an evolutionary algorithm to identify
efficient configurations, and Policy Continuous Adaptation, which fine-tunes a
task-specific control policy across evolving designs to maximize performance
rewards. We evaluate EA-CoRL by co-designing the actuators (gear ratios) and
control policy of the RH5 humanoid for a highly dynamic chin-up task,
previously unfeasible due to actuator limitations. Comparative results against
state-of-the-art RL-based co-design methods show that EA-CoRL achieves higher
fitness score and broader design space exploration, highlighting the critical
role of continuous policy adaptation in robot co-design.

</details>


### [43] [Autonomous Multi-Robot Infrastructure for AI-Enabled Healthcare Delivery and Diagnostics](https://arxiv.org/abs/2509.26106)
*Nakhul Kalaivanan,Senthil Arumugam Muthukumaraswamy,Girish Balasubramanian*

Main category: cs.RO

TL;DR: 本研究展示了一个基于群体智能的多机器人系统，用于住院护理，并通过模拟环境验证了其在患者监测和药物投递等方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 旨在利用多机器人系统增强住院护理的自动化程度，提升监测效率和安全性。

Method: 通过仿真医院环境，并在控制自测中验证系统性能，未进行实地患者试验。

Result: 系统在传感器准确性、任务成功率和通信可靠性方面均表现出色，AI决策支持功能能够提前预警异常健康状态。

Conclusion: 该多机器人系统展示了在住院护理中应用群体智能原理的潜力，能够提高医院自动化和病人安全。

Abstract: This research presents a multi-robot system for inpatient care, designed
using swarm intelligence principles and incorporating wearable health sensors,
RF-based communication, and AI-driven decision support. Within a simulated
hospital environment, the system adopts a leader-follower swarm configuration
to perform patient monitoring, medicine delivery, and emergency assistance. Due
to ethical constraints, live patient trials were not conducted; instead,
validation was carried out through controlled self-testing with wearable
sensors. The Leader Robot acquires key physiological parameters, including
temperature, SpO2, heart rate, and fall detection, and coordinates other robots
when required. The Assistant Robot patrols corridors for medicine delivery,
while a robotic arm provides direct drug administration. The swarm-inspired
leader-follower strategy enhanced communication reliability and ensured
continuous monitoring, including automated email alerts to healthcare staff.
The system hardware was implemented using Arduino, Raspberry Pi, NRF24L01 RF
modules, and a HuskyLens AI camera. Experimental evaluation showed an overall
sensor accuracy above 94%, a 92% task-level success rate, and a 96%
communication reliability rate, demonstrating system robustness. Furthermore,
the AI-enabled decision support was able to provide early warnings of abnormal
health conditions, highlighting the potential of the system as a cost-effective
solution for hospital automation and patient safety.

</details>


### [44] [Side Scan Sonar-based SLAM for Autonomous Algae Farm Monitoring](https://arxiv.org/abs/2509.26121)
*Julian Valdez,Ignacio Torroba,John Folkesson,Ivan Stenius*

Main category: cs.RO

TL;DR: 本研究提出了一种创新的侧扫声纳SLAM框架，提高了自主水下航行器在海藻养殖中的导航和定位能力。


<details>
  <summary>Details</summary>
Motivation: 为实现海藻养殖业向工业规模的转型，关键在于通过自动化技术提升作物和基础设施的检测效率。

Method: 提出了一种基于侧扫声纳的SLAM框架，通过将结构绳索建模为单个地标序列，来提高自主水下航行器的实际应用。

Result: 该方法在真实海藻农场的硬件在环实验中超越了现有的先进解决方案。

Conclusion: 该框架在海藻养殖的自主水下航行器导航和定位中表现优越，为智能农业的发展提供了新的可能性。

Abstract: The transition of seaweed farming to an alternative food source on an
industrial scale relies on automating its processes through smart farming,
equivalent to land agriculture. Key to this process are autonomous underwater
vehicles (AUVs) via their capacity to automate crop and structural inspections.
However, the current bottleneck for their deployment is ensuring safe
navigation within farms, which requires an accurate, online estimate of the AUV
pose and map of the infrastructure. To enable this, we propose an efficient
side scan sonar-based (SSS) simultaneous localization and mapping (SLAM)
framework that exploits the geometry of kelp farms via modeling structural
ropes in the back-end as sequences of individual landmarks from each SSS ping
detection, instead of combining detections into elongated representations. Our
method outperforms state of the art solutions in hardware in the loop (HIL)
experiments on a real AUV survey in a kelp farm. The framework and dataset can
be found at https://github.com/julRusVal/sss_farm_slam.

</details>


### [45] [Terrain-Awared LiDAR-Inertial Odometry for Legged-Wheel Robots Based on Radial Basis Function Approximation](https://arxiv.org/abs/2509.26222)
*Yizhe Liu,Han Zhang*

Main category: cs.RO

TL;DR: 提出了一种地形感知LiDAR-惯性里程计框架，有效提高了不规则地形中的定位准确性。


<details>
  <summary>Details</summary>
Motivation: 在不规则地形（如坎坷道路和楼梯）上运行的腿轮机器人需要准确的里程计，以避免由于忽视地形几何而导致的姿态漂移。

Method: 提出了一种基于RBF的地形感知LiDAR-惯性里程计框架，使用自适应选择的中心和递归更新的权重来近似地形，产生光滑的地形流形以缓解$z$轴姿态漂移。

Result: 实验表明，我们的方法在非结构化地形上的定位准确性高于现有的最先进基准。

Conclusion: 我们的方法在具有连续高度变化或稀疏特征的非结构化地形中，尤其在突然高度变化时，达到了比当前最先进基准更高的定位准确性。

Abstract: An accurate odometry is essential for legged-wheel robots operating in
unstructured terrains such as bumpy roads and staircases. Existing methods
often suffer from pose drift due to their ignorance of terrain geometry. We
propose a terrain-awared LiDAR-Inertial odometry (LIO) framework that
approximates the terrain using Radial Basis Functions (RBF) whose centers are
adaptively selected and weights are recursively updated. The resulting smooth
terrain manifold enables ``soft constraints" that regularize the odometry
optimization and mitigates the $z$-axis pose drift under abrupt elevation
changes during robot's maneuver. To ensure the LIO's real-time performance, we
further evaluate the RBF-related terms and calculate the inverse of the sparse
kernel matrix with GPU parallelization. Experiments on unstructured terrains
demonstrate that our method achieves higher localization accuracy than the
state-of-the-art baselines, especially in the scenarios that have continuous
height changes or sparse features when abrupt height changes occur.

</details>


### [46] [ISyHand: A Dexterous Multi-finger Robot Hand with an Articulated Palm](https://arxiv.org/abs/2509.26236)
*Benjamin A. Richardson,Felix Grüninger,Lukas Mack,Joerg Stueckler,Katherine J. Kuchenbecker*

Main category: cs.RO

TL;DR: ISyHand 是一种低成本的开源机器人手，具有高灵活性和易制造性，通过强化学习在模拟中成功进行立方体重定向，同时在现实世界中也表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了开发一种既灵活又具有人性化设计的低成本开源机器人手，以满足现代机器人对灵巧操控的需求。

Method: 使用强化学习在模拟环境中训练 ISyHand 进行经典的手内操作任务：立方体重定向。

Result: ISyHand 在早期训练阶段优于两个可比手，而三者在策略收敛后表现相似，ISyHand 显著优于其固定掌版本，并成功在真实手上实施立方体重定向。

Conclusion: ISyHand 是一种高灵活性、低成本、易于制造的机器人手，能够在现实世界中成功进行灵巧操作。

Abstract: The rapid increase in the development of humanoid robots and customized
manufacturing solutions has brought dexterous manipulation to the forefront of
modern robotics. Over the past decade, several expensive dexterous hands have
come to market, but advances in hardware design, particularly in servo motors
and 3D printing, have recently facilitated an explosion of cheaper open-source
hands. Most hands are anthropomorphic to allow use of standard human tools, and
attempts to increase dexterity often sacrifice anthropomorphism. We introduce
the open-source ISyHand (pronounced easy-hand), a highly dexterous, low-cost,
easy-to-manufacture, on-joint servo-driven robot hand. Our hand uses
off-the-shelf Dynamixel motors, fasteners, and 3D-printed parts, can be
assembled within four hours, and has a total material cost of about 1,300 USD.
The ISyHands's unique articulated-palm design increases overall dexterity with
only a modest sacrifice in anthropomorphism. To demonstrate the utility of the
articulated palm, we use reinforcement learning in simulation to train the hand
to perform a classical in-hand manipulation task: cube reorientation. Our
novel, systematic experiments show that the simulated ISyHand outperforms the
two most comparable hands in early training phases, that all three perform
similarly well after policy convergence, and that the ISyHand significantly
outperforms a fixed-palm version of its own design. Additionally, we deploy a
policy trained on cube reorientation on the real hand, demonstrating its
ability to perform real-world dexterous manipulation.

</details>


### [47] [Anomaly detection for generic failure monitoring in robotic assembly, screwing and manipulation](https://arxiv.org/abs/2509.26308)
*Niklas Grambow,Lisa-Marie Fenner,Felipe Kempkes,Philip Hotz,Dingyuan Wan,Jörg Krüger,Kevin Haninger*

Main category: cs.RO

TL;DR: 本研究通过多模态时间序列数据对工业机器人任务中的异常检测进行了研究，发现基于自动编码器的方法在处理不同任务时具有良好的泛化能力，尽管对某些微妙故障的检测仍存在不足。


<details>
  <summary>Details</summary>
Motivation: 机器人操作中的异常状态会导致不可预测的行为和任务失败，因此需要有效的异常检测方法来提高成功率和减少损坏风险。

Method: 比较多个基于自动编码器的方法，评估其在不同机器人任务和控制策略下的泛化能力，包括扩散策略、位置控制和阻抗控制。

Result: 在电缆、拧螺丝和抛光等工业任务中，通过收集多模态时间序列数据并进行异常检测，验证了在复杂任务中集成AD的有效性。

Conclusion: 在多个工业机器人任务中，采用基于自动编码器的方法能够有效检测异常，且在电缆铺设和拧螺丝任务中的AUROC值超过0.93. 然而，抛光任务中只能可靠检测到严重故障，而较微妙的故障类型则未能检测到。

Abstract: Out-of-distribution states in robot manipulation often lead to unpredictable
robot behavior or task failure, limiting success rates and increasing risk of
damage. Anomaly detection (AD) can identify deviations from expected patterns
in data, which can be used to trigger failsafe behaviors and recovery
strategies. Prior work has applied data-driven AD to time series data in
specific robotic tasks, but its transferability across control strategies and
task types has not been shown. Leveraging time series data, such as
force/torque signals, allows to directly capture robot-environment
interactions, crucial for manipulation and online failure detection. Their
broad availability, high sampling rates, and low dimensionality enable high
temporal resolution and efficient processing. As robotic tasks can have widely
signal characteristics and requirements, AD methods which can be applied in the
same way to a wide range of tasks is needed, ideally with good data efficiency.
We examine three industrial robotic tasks, each presenting several anomalies.
Test scenarios in robotic cabling, screwing, and sanding are built, and
multimodal time series data is gathered. Several autoencoder-based methods are
compared, evaluating generalization across tasks and control methods (diffusion
policy, position, and impedance control). This allows us to validate the
integration of AD in complex tasks involving tighter tolerances and variation
from both the robot and its environment. Additionally, we evaluate data
efficiency, detection latency, and task characteristics which support robust
detection. The results indicate reliable detection with AUROC exceeding 0.93 in
failures in the cabling and screwing task, such as incorrect or misaligned
parts and obstructed targets. In the polishing task, only severe failures were
reliably detected, while more subtle failure types remained undetected.

</details>


### [48] [LLM-MCoX: Large Language Model-based Multi-robot Coordinated Exploration and Search](https://arxiv.org/abs/2509.26324)
*Ruiyang Wang,Haolun Tsu,David Hunt,Shaocheng Luo,Jiwoo Kim,Miroslav Pajic*

Main category: cs.RO

TL;DR: 本文提出LLM-MCoX框架，结合实时LiDAR处理与大型语言模型推理，提升多机器人系统在探索未知室内环境中的效率和协同能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法在多机器人系统中的探索和搜索效率较低，缺乏有效的机器人间协调。

Method: 通过实时LiDAR扫描处理和多模态大型语言模型推理来实现机器人协调任务分配。

Result: 在大规模环境中，LLM-MCoX比现有方法提高了22.7%的探索速度和50%的搜索效率。

Conclusion: LLM-MCoX框架显著提升了多机器人系统在探索和物体搜索中的效率与协同能力。

Abstract: Autonomous exploration and object search in unknown indoor environments
remain challenging for multi-robot systems (MRS). Traditional approaches often
rely on greedy frontier assignment strategies with limited inter-robot
coordination. In this work, we introduce LLM-MCoX (LLM-based Multi-robot
Coordinated Exploration and Search), a novel framework that leverages Large
Language Models (LLMs) for intelligent coordination of both homogeneous and
heterogeneous robot teams tasked with efficient exploration and target object
search. Our approach combines real-time LiDAR scan processing for frontier
cluster extraction and doorway detection with multimodal LLM reasoning (e.g.,
GPT-4o) to generate coordinated waypoint assignments based on shared
environment maps and robot states. LLM-MCoX demonstrates superior performance
compared to existing methods, including greedy and Voronoi-based planners,
achieving 22.7% faster exploration times and 50% improved search efficiency in
large environments with 6 robots. Notably, LLM-MCoX enables natural
language-based object search capabilities, allowing human operators to provide
high-level semantic guidance that traditional algorithms cannot interpret.

</details>


### [49] [Kinodynamic Motion Planning for Mobile Robot Navigation across Inconsistent World Models](https://arxiv.org/abs/2509.26339)
*Eric R. Damm,Thomas M. Howard*

Main category: cs.RO

TL;DR: 移动机器人在环境模型不确定下的路径规划研究，提出GEGRH方法，相比传统方法更高效可靠。


<details>
  <summary>Details</summary>
Motivation: 移动地面机器人需依赖传感器数据建立环境模型，以克服成本图中障碍物和自由空间的标记切换所带来的挑战。

Method: 论文讨论了三个主要的迭代方案：PEH、GEH和GEGRH，分别处理不同的世界模型历史和节点扩展。

Result: 初步结果表明，尽管PEH和GEH比VEH找到更乐观的解决方案，但都无法满足一秒内的规划需求，而GEGRH在非结构化的离路环境中能找到更低成本的轨迹且规划速度更快。

Conclusion: GEGRH方法比VEH方法能找到更低成本的轨迹，并且平均规划时间更快，与单假设搜索相比，GEGRH虽然平均规划时间略有增加，但能生成更保守的规划。

Abstract: Mobile ground robots lacking prior knowledge of an environment must rely on
sensor data to develop a model of their surroundings. In these scenarios,
consistent identification of obstacles and terrain features can be difficult
due to noise and algorithmic shortcomings, which can make it difficult for
motion planning systems to generate safe motions. One particular difficulty to
overcome is when regions of the cost map switch between being marked as
obstacles and free space through successive planning cycles. One potential
solution to this, which we refer to as Valid in Every Hypothesis (VEH), is for
the planning system to plan motions that are guaranteed to be safe through a
history of world models. Another approach is to track a history of world
models, and adjust node costs according to the potential penalty of needing to
reroute around previously hazardous areas. This work discusses three major
iterations on this idea. The first iteration, called PEH, invokes a sub-search
for every node expansion that crosses through a divergence point in the world
models. The second and third iterations, called GEH and GEGRH respectively,
defer the sub-search until after an edge expands into the goal region. GEGRH
uses an additional step to revise the graph based on divergent nodes in each
world. Initial results showed that, although PEH and GEH find more optimistic
solutions than VEH, they are unable to generate solutions in less than
one-second, which exceeds our requirements for field deployment. Analysis of
results from a field experiment in an unstructured, off-road environment on a
Clearpath Robotics Warthog UGV indicate that GEGRH finds lower cost
trajectories and has faster average planning times than VEH. Compared to
single-hypothesis (SH) search, where only the latest world model is considered,
GEGRH generates more conservative plans with a small increase in average
planning time.

</details>


### [50] [SDA-PLANNER: State-Dependency Aware Adaptive Planner for Embodied Task Planning](https://arxiv.org/abs/2509.26375)
*Zichao Shen,Chen Gao,Jiaqi Yuan,Tianchen Zhu,Xingcheng Fu,Qingyun Sun*

Main category: cs.RO

TL;DR: SDA-PLANNER是一种新的适应性规划方案，通过状态依赖图和误差处理机制来提升体现任务计划的能力，在多样化错误条件下表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的基于LLM的计划方法存在固定规划范式、缺乏行动序列约束和对错误的不敏感性，因此需要一种新的适应性规划方法来增强任务执行能力。

Method: 引入状态依赖图，建立行动前提和效果模型，同时使用误差自适应重规划策略，包括错误回溯、诊断和自适应行动子树生成。

Result: SDA-PLANNER通过引入状态依赖图和错误处理机制，提供了更全面的体现任务规划能力，特别是在处理执行错误时的动态修正。

Conclusion: SDA-PLANNER在成功率和目标完成方面，尤其是在多样化的错误条件下，显著优于基线方法。

Abstract: Embodied task planning requires agents to produce executable actions in a
close-loop manner within the environment. With progressively improving
capabilities of LLMs in task decomposition, planning, and generalization,
current embodied task planning methods adopt LLM-based architecture.However,
existing LLM-based planners remain limited in three aspects, i.e., fixed
planning paradigms, lack of action sequence constraints, and error-agnostic. In
this work, we propose SDA-PLANNER, enabling an adaptive planning paradigm,
state-dependency aware and error-aware mechanisms for comprehensive embodied
task planning. Specifically, SDA-PLANNER introduces a State-Dependency Graph to
explicitly model action preconditions and effects, guiding the dynamic
revision. To handle execution error, it employs an error-adaptive replanning
strategy consisting of Error Backtrack and Diagnosis and Adaptive Action
SubTree Generation, which locally reconstructs the affected portion of the plan
based on the current environment state. Experiments demonstrate that
SDA-PLANNER consistently outperforms baselines in success rate and goal
completion, particularly under diverse error conditions.

</details>


### [51] [Real-time Velocity Profile Optimization for Time-Optimal Maneuvering with Generic Acceleration Constraints](https://arxiv.org/abs/2509.26428)
*Mattia Piazza,Mattia Piccinini,Sebastiano Taddei,Francesco Biral,Enrico Bertolazzi*

Main category: cs.RO

TL;DR: FBGA是一种高效的前向-后向算法，能处理复杂的加速度限制，计算速度快且准确性高，适合轨迹规划。


<details>
  <summary>Details</summary>
Motivation: 在机器人轨迹规划中，计算沿规定路径的时间最优速度配置十分重要，尤其在自主赛车中有重大应用意义。

Method: 提出了一种新的前向-后向FBGA算法，使用短的离散路径段进行操作，满足用户定义的性能限制。

Result: 在五条赛道和两种车辆类型上测试时，FBGA的操控和圈速与最优控制基线相差极小（在0.11%-0.36%内），并且速度快了几个数量级。

Conclusion: FBGA算法在处理复杂的非凸加速度约束方面表现优异，且运行速度快，适用于在线多查询轨迹规划。

Abstract: The computation of time-optimal velocity profiles along prescribed paths,
subject to generic acceleration constraints, is a crucial problem in robot
trajectory planning, with particular relevance to autonomous racing. However,
the existing methods either support arbitrary acceleration constraints at high
computational cost or use conservative box constraints for computational
efficiency. We propose FBGA, a new \underline{F}orward-\underline{B}ackward
algorithm with \underline{G}eneric \underline{A}cceleration constraints, which
achieves both high accuracy and low computation time. FBGA operates forward and
backward passes to maximize the velocity profile in short, discretized path
segments, while satisfying user-defined performance limits. Tested on five
racetracks and two vehicle classes, FBGA handles complex, non-convex
acceleration constraints with custom formulations. Its maneuvers and lap times
closely match optimal control baselines (within $0.11\%$-$0.36\%$), while being
up to three orders of magnitude faster. FBGA maintains high accuracy even with
coarse discretization, making it well-suited for online multi-query trajectory
planning. Our open-source \texttt{C++} implementation is available at:
https://anonymous.4open.science/r/FB_public_RAL.

</details>


### [52] [Unwinding Rotations Reduces VR Sickness in Nonsimulated Immersive Telepresence](https://arxiv.org/abs/2509.26439)
*Filip Kulisiewicz,Basak Sakcak,Evan G. Center,Juho Kalliokoski,Katherine J. Mimnaugh,Steven M. LaValle,Timo Ojala*

Main category: cs.RO

TL;DR: 解除旋转方法在真实环境中有效改善VR体验，减少晕动症，且不影响用户任务表现。


<details>
  <summary>Details</summary>
Motivation: 研究旨在测试在真实摄像头视频流下，解除机器人旋转的假设是否仍然成立，以改善远程用户的体验。

Method: 进行了一个用户研究，样本量为36，将解除旋转方法与耦合旋转方法进行比较，任务是通过安装在机器人手臂上的全景摄像头完成的 inspection 任务。

Result: 结果显示，用户认为解除旋转方法更舒适且更受欢迎，且能够降低VR晕动症的发生。

Conclusion: 使用解除旋转的方法可以提高用户舒适度，减少虚拟现实（VR）晕动症，同时不显著影响任务表现。

Abstract: Immersive telepresence, when a user views the video stream of a $360^\circ$
camera in a remote environment using a Head Mounted Display (HMD), has great
potential to improve the sense of being in a remote environment. In most cases
of immersive robotic telepresence, the camera is mounted on a mobile robot
which increases the portion of the environment that the remote user can
explore. However, robot motions can induce unpleasant symptoms associated with
Virtual Reality (VR) sickness, degrading the overall user experience. Previous
research has shown that unwinding the rotations of the robot, that is,
decoupling the rotations that the camera undergoes due to robot motions from
what is seen by the user, can increase user comfort and reduce VR sickness.
However, that work considered a virtual environment and a simulated robot. In
this work, to test whether the same hypotheses hold when the video stream from
a real camera is used, we carried out a user study $(n=36)$ in which the
unwinding rotations method was compared against coupled rotations in a task
completed through a panoramic camera mounted on a robotic arm. Furthermore,
within an inspection task which involved translations and rotations in three
dimensions, we tested whether unwinding the robot rotations impacted the
performance of users. The results show that the users found the unwinding
rotations method to be more comfortable and preferable, and that a reduced
level of VR sickness can be achieved without a significant impact on task
performance.

</details>


### [53] [Analytic Conditions for Differentiable Collision Detection in Trajectory Optimization](https://arxiv.org/abs/2509.26459)
*Akshay Jaitly,Devesh K. Jha,Kei Ota,Yuki Shirai*

Main category: cs.RO

TL;DR: 本文提出了一种有效的非渗透约束实现方法，优化了碰撞-aware轨迹生成的效率，提供了数值实验支持。


<details>
  <summary>Details</summary>
Motivation: 现有优化方法在执行碰撞规避和接触规划时面临非渗透约束的问题，导致计算开销较大。

Method: 通过引入新颖的可微条件和解析表达式，来实现对非平滑物体之间的非碰撞约束，使用平滑半代数集近似多面体。

Result: 实验结果显示该方法性能优越，能有效处理复杂任务中的碰撞-aware轨迹优化问题，并与其他基线方法进行了比较。

Conclusion: 提出的方法有效地实现了对象间的非渗透约束，具有较好的性能和效率。

Abstract: Optimization-based methods are widely used for computing fast, diverse
solutions for complex tasks such as collision-free movement or planning in the
presence of contacts. However, most of these methods require enforcing
non-penetration constraints between objects, resulting in a non-trivial and
computationally expensive problem. This makes the use of optimization-based
methods for planning and control challenging. In this paper, we present a
method to efficiently enforce non-penetration of sets while performing
optimization over their configuration, which is directly applicable to problems
like collision-aware trajectory optimization. We introduce novel differentiable
conditions with analytic expressions to achieve this. To enforce non-collision
between non-smooth bodies using these conditions, we introduce a method to
approximate polytopes as smooth semi-algebraic sets. We present several
numerical experiments to demonstrate the performance of the proposed method and
compare the performance with other baseline methods recently proposed in the
literature.

</details>


### [54] [Learning from Hallucinating Critical Points for Navigation in Dynamic Environments](https://arxiv.org/abs/2509.26513)
*Saad Abdul Ghani,Kameron Lee,Xuesu Xiao*

Main category: cs.RO

TL;DR: LfH-CP 是一个新的自我监督框架，能够有效生成多样的动态障碍数据集，提升运动规划的成功率。


<details>
  <summary>Details</summary>
Motivation: 为了解决生成大规模多样的动态障碍数据集的挑战，避免昂贵的专家示范或试错探索。

Method: 采用了二阶段的幻觉因子化，即确定关键点和程序性生成轨迹。

Result: LfH-CP 生成了比现有基线更丰富的训练数据，训练出的规划者在模拟实验中成功率更高。

Conclusion: LfH-CP 提供了一个自我监督的框架，用于生成丰富的动态障碍数据集，从而实现更高效的运动规划。

Abstract: Generating large and diverse obstacle datasets to learn motion planning in
environments with dynamic obstacles is challenging due to the vast space of
possible obstacle trajectories. Inspired by hallucination-based data synthesis
approaches, we propose Learning from Hallucinating Critical Points (LfH-CP), a
self-supervised framework for creating rich dynamic obstacle datasets based on
existing optimal motion plans without requiring expensive expert demonstrations
or trial-and-error exploration. LfH-CP factorizes hallucination into two
stages: first identifying when and where obstacles must appear in order to
result in an optimal motion plan, i.e., the critical points, and then
procedurally generating diverse trajectories that pass through these points
while avoiding collisions. This factorization avoids generative failures such
as mode collapse and ensures coverage of diverse dynamic behaviors. We further
introduce a diversity metric to quantify dataset richness and show that LfH-CP
produces substantially more varied training data than existing baselines.
Experiments in simulation demonstrate that planners trained on LfH-CP datasets
achieves higher success rates compared to a prior hallucination method.

</details>


### [55] [Memory-Efficient 2D/3D Shape Assembly of Robot Swarms](https://arxiv.org/abs/2509.26518)
*Shuoyu Yue,Pengpeng Li,Yang Xu,Kunrui Ze,Xingjian Long,Huazi Cao,Guibin Sun*

Main category: cs.RO

TL;DR: 本文提出一种新型的内存高效树状地图表示法和分布式控制器，解决了均值漂移法在高分辨率/3D形状上的内存问题，验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 解决传统图像表示法在高分辨率和3D形状时产生的内存开销问题。

Method: 提出一种树状地图表示法和基于行为的分布式控制器，支持2D和3D形状的组装。

Result: 相较于先进的均值漂移算法，内存使用低1至2个数量级，形状进入速度提高2至3倍，同时保持相似的均匀性。

Conclusion: 提出了一种内存高效的树状地图表示法，结合基于行为的分布式控制器，实现了无分配的形状组装，具有良好的实用性。

Abstract: Mean-shift-based approaches have recently emerged as the most effective
methods for robot swarm shape assembly tasks. These methods rely on image-based
representations of target shapes to compute local density gradients and perform
mean-shift exploration, which constitute their core mechanism. However, such
image representations incur substantial memory overhead, which can become
prohibitive for high-resolution or 3D shapes. To overcome this limitation, we
propose a memory-efficient tree map representation that hierarchically encodes
user-specified shapes and is applicable to both 2D and 3D scenarios. Building
on this representation, we design a behavior-based distributed controller that
enables assignment-free shape assembly. Comparative 2D and 3D simulations
against a state-of-the-art mean-shift algorithm demonstrate one to two orders
of magnitude lower memory usage and two to three times faster shape entry while
maintaining comparable uniformity. Finally, we validate the framework through
physical experiments with 6 to 7 UAVs, confirming its real-world practicality.

</details>


### [56] [Radio-based Multi-Robot Odometry and Relative Localization](https://arxiv.org/abs/2509.26558)
*Andrés Martínez-Silva,David Alejo,Luis Merino,Fernando Caballero*

Main category: cs.RO

TL;DR: 本文提出了一种基于UWB和雷达的多机器人定位系统，利用低成本传感器，并通过优化算法实现高效的相对定位，表现优于传统方法，具备SLAM扩展性，代码和数据公开可供研究。


<details>
  <summary>Details</summary>
Motivation: 旨在解决机器人在复杂环境中的定位问题，尤其是在多机器人场景下，结合使用现有的低成本传感器提升定位精度。

Method: 该系统利用UWB和雷达技术，通过非线性优化框架进行三角测量，并使用姿态图优化框架整合多个机器人间的约束。

Result: 提出的系统在软件仿真和实地数据集中经过验证，相较于现有模型，提供了更好的定位性能，尤其在噪声环境中。

Conclusion: 所提出的相对定位模块在噪声鲁棒性方面优于现有的方法，并且具有可扩展性，便于进行同时定位与地图构建（SLAM）。

Abstract: Radio-based methods such as Ultra-Wideband (UWB) and RAdio Detection And
Ranging (radar), which have traditionally seen limited adoption in robotics,
are experiencing a boost in popularity thanks to their robustness to harsh
environmental conditions and cluttered environments. This work proposes a
multi-robot UGV-UAV localization system that leverages the two technologies
with inexpensive and readily-available sensors, such as Inertial Measurement
Units (IMUs) and wheel encoders, to estimate the relative position of an aerial
robot with respect to a ground robot. The first stage of the system pipeline
includes a nonlinear optimization framework to trilaterate the location of the
aerial platform based on UWB range data, and a radar pre-processing module with
loosely coupled ego-motion estimation which has been adapted for a multi-robot
scenario. Then, the pre-processed radar data as well as the relative
transformation are fed to a pose-graph optimization framework with odometry and
inter-robot constraints. The system, implemented for the Robotic Operating
System (ROS 2) with the Ceres optimizer, has been validated in
Software-in-the-Loop (SITL) simulations and in a real-world dataset. The
proposed relative localization module outperforms state-of-the-art closed-form
methods which are less robust to noise. Our SITL environment includes a custom
Gazebo plugin for generating realistic UWB measurements modeled after real
data. Conveniently, the proposed factor graph formulation makes the system
readily extensible to full Simultaneous Localization And Mapping (SLAM).
Finally, all the code and experimental data is publicly available to support
reproducibility and to serve as a common open dataset for benchmarking.

</details>


### [57] [Graphite: A GPU-Accelerated Mixed-Precision Graph Optimization Framework](https://arxiv.org/abs/2509.26581)
*Shishir Gopinath,Karthik Dantu,Steven Y. Ko*

Main category: cs.RO

TL;DR: Graphite 是一个 GPU 加速的非线性图优化框架，能有效减少内存使用并在多种设备上实现高效的优化任务。


<details>
  <summary>Details</summary>
Motivation: 为了提升图优化算法在资源受限环境中的性能，同时保持通用性和减少内存使用。

Method: 通过使用 CUDA C++ 接口构建非线性图优化框架，支持实时应用与优化任务之间的代码共享。

Result: 在知名的束调整问题上，Graphite 达到了与 MegBA 相似的性能，且内存使用更少，还在图像惯性束调整中相比 CPU 基线实现了高达 59 倍的加速。

Conclusion: Graphite 在大规模优化中表现优异，特别是在桌面和资源受限设备上，提供了较快的解决方案。

Abstract: We present Graphite, a GPU-accelerated nonlinear graph optimization
framework. It provides a CUDA C++ interface to enable the sharing of code
between a realtime application, such as a SLAM system, and its optimization
tasks. The framework supports techniques to reduce memory usage, including
in-place optimization, support for multiple floating point types and
mixed-precision modes, and dynamically computed Jacobians. We evaluate Graphite
on well-known bundle adjustment problems and find that it achieves similar
performance to MegBA, a solver specialized for bundle adjustment, while
maintaining generality and using less memory. We also apply Graphite to global
visual-inertial bundle adjustment on maps generated from stereo-inertial SLAM
datasets, and observe speed ups of up to 59x compared to a CPU baseline. Our
results indicate that our solver enables faster large-scale optimization on
both desktop and resource-constrained devices.

</details>


### [58] [OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction](https://arxiv.org/abs/2509.26633)
*Lujie Yang,Xiaoyu Huang,Zhen Wu,Angjoo Kanazawa,Pieter Abbeel,Carmelo Sferrazza,C. Karen Liu,Rocky Duan,Guanya Shi*

Main category: cs.RO

TL;DR: OmniRetarget是一种新型的数据生成引擎，能有效缩小人类与机器人之间的差距，提升机器人动作的可行性和交互性。


<details>
  <summary>Details</summary>
Motivation: 应对人类与机器人之间的显著表现差距，解决传统重定向方法在物理可行性和交互性上的不足

Method: 使用一种基于交互网格的数据生成引擎，最小化人类与机器人之间的Laplacian变形，并施加运动约束，从而生成运动轨迹

Result: 通过8小时以上的运动轨迹生成，OmniRetarget在运动约束满足和接触保持方面超越了传统基准，并成功在Unitree G1上执行长达30秒的复杂运动技能

Conclusion: OmniRetarget能有效生成高质量运动轨迹，从而提升人形机器人在复杂技能任务中的表现，且仅需少量奖励设置和简单的领域随机化

Abstract: A dominant paradigm for teaching humanoid robots complex skills is to
retarget human motions as kinematic references to train reinforcement learning
(RL) policies. However, existing retargeting pipelines often struggle with the
significant embodiment gap between humans and robots, producing physically
implausible artifacts like foot-skating and penetration. More importantly,
common retargeting methods neglect the rich human-object and human-environment
interactions essential for expressive locomotion and loco-manipulation. To
address this, we introduce OmniRetarget, an interaction-preserving data
generation engine based on an interaction mesh that explicitly models and
preserves the crucial spatial and contact relationships between an agent, the
terrain, and manipulated objects. By minimizing the Laplacian deformation
between the human and robot meshes while enforcing kinematic constraints,
OmniRetarget generates kinematically feasible trajectories. Moreover,
preserving task-relevant interactions enables efficient data augmentation, from
a single demonstration to different robot embodiments, terrains, and object
configurations. We comprehensively evaluate OmniRetarget by retargeting motions
from OMOMO, LAFAN1, and our in-house MoCap datasets, generating over 8-hour
trajectories that achieve better kinematic constraint satisfaction and contact
preservation than widely used baselines. Such high-quality data enables
proprioceptive RL policies to successfully execute long-horizon (up to 30
seconds) parkour and loco-manipulation skills on a Unitree G1 humanoid, trained
with only 5 reward terms and simple domain randomization shared by all tasks,
without any learning curriculum.

</details>


### [59] [MLA: A Multisensory Language-Action Model for Multimodal Understanding and Forecasting in Robotic Manipulation](https://arxiv.org/abs/2509.26642)
*Zhuoyang Liu,Jiaming Liu,Jiadong Xu,Nuowei Han,Chenyang Gu,Hao Chen,Kaichen Zhou,Renrui Zhang,Kai Chin Hsieh,Kun Wu,Zhengping Che,Jian Tang,Shanghang Zhang*

Main category: cs.RO

TL;DR: 提出了一种新的多感官语言-动作模型（MLA），通过无编码器的多模态对齐和后期训练策略提升机器人在复杂环境中的行动生成与理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型主要关注于如何生成动作，并未充分考虑机器人在空间物理世界的感知与互动，这需要对多感官信息进行综合理解，以实现复杂控制。

Method: 提出了一种无编码器的多模态对齐方案，将大型语言模型作为感知模块，直接对齐2D图像、3D点云和触觉信息。设计了未来多感官生成的后期训练策略以增强对物理动态的理解。

Result: MLA模型在复杂的真实世界任务中比以前的2D和3D VLA方法分别提高了12%和24%的性能，并且在未见配置上也展示了更好的泛化性。

Conclusion: MLA模型在复杂的接触丰富的真实世界任务中表现优于之前的前沿VLA方法，展现出更好的泛化能力。

Abstract: Vision-language-action models (VLAs) have shown generalization capabilities
in robotic manipulation tasks by inheriting from vision-language models (VLMs)
and learning action generation. Most VLA models focus on interpreting vision
and language to generate actions, whereas robots must perceive and interact
within the spatial-physical world. This gap highlights the need for a
comprehensive understanding of robotic-specific multisensory information, which
is crucial for achieving complex and contact-rich control. To this end, we
introduce a multisensory language-action (MLA) model that collaboratively
perceives heterogeneous sensory modalities and predicts future multisensory
objectives to facilitate physical world modeling. Specifically, to enhance
perceptual representations, we propose an encoder-free multimodal alignment
scheme that innovatively repurposes the large language model itself as a
perception module, directly interpreting multimodal cues by aligning 2D images,
3D point clouds, and tactile tokens through positional correspondence. To
further enhance MLA's understanding of physical dynamics, we design a future
multisensory generation post-training strategy that enables MLA to reason about
semantic, geometric, and interaction information, providing more robust
conditions for action generation. For evaluation, the MLA model outperforms the
previous state-of-the-art 2D and 3D VLA methods by 12% and 24% in complex,
contact-rich real-world tasks, respectively, while also demonstrating improved
generalization to unseen configurations. Project website:
https://sites.google.com/view/open-mla

</details>
