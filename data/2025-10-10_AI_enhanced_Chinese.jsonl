{"id": "2510.07417", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07417", "abs": "https://arxiv.org/abs/2510.07417", "authors": ["Corban Rivera", "Grayson Byrd", "Meghan Booker", "Bethany Kemp", "Allison Gaines", "Emma Holmes", "James Uplinger", "Celso M de Melo", "David Handelman"], "title": "FLEET: Formal Language-Grounded Scheduling for Heterogeneous Robot Teams", "comment": null, "summary": "Coordinating heterogeneous robot teams from free-form natural-language\ninstructions is hard. Language-only planners struggle with long-horizon\ncoordination and hallucination, while purely formal methods require\nclosed-world models. We present FLEET, a hybrid decentralized framework that\nturns language into optimized multi-robot schedules. An LLM front-end produces\n(i) a task graph with durations and precedence and (ii) a capability-aware\nrobot--task fitness matrix; a formal back-end solves a makespan-minimization\nproblem while the underlying robots execute their free-form subtasks with\nagentic closed-loop control. Across multiple free-form language-guided autonomy\ncoordination benchmarks, FLEET improves success over state of the art\ngenerative planners on two-agent teams across heterogeneous tasks. Ablations\nshow that mixed integer linear programming (MILP) primarily improves temporal\nstructure, while LLM-derived fitness is decisive for capability-coupled tasks;\ntogether they deliver the highest overall performance. We demonstrate the\ntranslation to real world challenges with hardware trials using a pair of\nquadruped robots with disjoint capabilities.", "AI": {"tldr": "FLEET\u662f\u4e00\u4e2a\u6df7\u5408\u7684\u53bb\u4e2d\u5fc3\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u5316\u4e3a\u4f18\u5316\u7684\u591a\u673a\u5668\u4eba\u8c03\u5ea6\uff0c\u63d0\u9ad8\u4e86\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u7684\u534f\u8c03\u6548\u7387\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u73b0\u6709\u8bed\u8a00\u9a71\u52a8\u89c4\u5212\u548c\u6b63\u5f0f\u65b9\u6cd5\u5728\u590d\u6742\u4efb\u52a1\u534f\u8c03\u4e2d\u7684\u4e0d\u8db3\uff0c\u63d0\u51faFLEET\u6846\u67b6\uff0c\u63d0\u5347\u591a\u673a\u5668\u4eba\u56e2\u961f\u7684\u534f\u4f5c\u6548\u7387\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b (LLM) \u63d0\u4f9b\u7684\u4efb\u52a1\u56fe\u548c\u80fd\u529b\u611f\u77e5\u7684\u673a\u5668\u4eba-\u4efb\u52a1\u9002\u5e94\u77e9\u9635\uff0c\u4ee5\u53ca\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212 (MILP) \u540e\u7aef\u89e3\u51b3\u8c03\u5ea6\u95ee\u9898\u3002", "result": "FLEET\u5728\u81ea\u7531\u5f62\u5f0f\u8bed\u8a00\u6307\u5bfc\u7684\u591a\u673a\u5668\u4eba\u534f\u8c03\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6210\u529f\u7387\u8d85\u8fc7\u5148\u8fdb\u7684\u751f\u6210\u89c4\u5212\u65b9\u6cd5\uff0c\u5177\u4f53\u4f53\u73b0\u5728\u5f02\u6784\u4efb\u52a1\u7684\u4e24\u4ee3\u7406\u56e2\u961f\u4e0a\u3002", "conclusion": "FLEET\u5728\u591a\u673a\u5668\u4eba\u534f\u8c03\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u6210\u529f\u7387\u9ad8\u4e8e\u73b0\u6709\u751f\u6210\u89c4\u5212\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u5f02\u6784\u4efb\u52a1\u548c\u80fd\u529b\u5339\u914d\u3002"}}
{"id": "2510.07447", "categories": ["cs.RO", "cs.LG", "math.DS"], "pdf": "https://arxiv.org/pdf/2510.07447", "abs": "https://arxiv.org/abs/2510.07447", "authors": ["Girolamo Oddo", "Roberto Nuca", "Matteo Parsani"], "title": "VeMo: A Lightweight Data-Driven Approach to Model Vehicle Dynamics", "comment": null, "summary": "Developing a dynamic model for a high-performance vehicle is a complex\nproblem that requires extensive structural information about the system under\nanalysis. This information is often unavailable to those who did not design the\nvehicle and represents a typical issue in autonomous driving applications,\nwhich are frequently developed on top of existing vehicles; therefore, vehicle\nmodels are developed under conditions of information scarcity. This paper\nproposes a lightweight encoder-decoder model based on Gate Recurrent Unit\nlayers to correlate the vehicle's future state with its past states, measured\nonboard, and control actions the driver performs. The results demonstrate that\nthe model achieves a maximum mean relative error below 2.6% in extreme dynamic\nconditions. It also shows good robustness when subject to noisy input data\nacross the interested frequency components. Furthermore, being entirely\ndata-driven and free from physical constraints, the model exhibits physical\nconsistency in the output signals, such as longitudinal and lateral\naccelerations, yaw rate, and the vehicle's longitudinal velocity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGRU\u5c42\u7684\u7f16\u7801-\u89e3\u7801\u6a21\u578b\uff0c\u5728\u4fe1\u606f\u7a00\u7f3a\u7684\u6761\u4ef6\u4e0b\uff0c\u6a21\u578b\u8868\u73b0\u51fa\u9ad8\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u81ea\u4e3b\u9a7e\u9a76\u8f66\u8f86\u3002", "motivation": "\u5728\u4fe1\u606f\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u5f00\u53d1\u9ad8\u6027\u80fd\u8f66\u8f86\u52a8\u6001\u6a21\u578b\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u81ea\u4e3b\u9a7e\u9a76\u5e94\u7528\u3002", "method": "\u57fa\u4e8e\u95e8\u63a7\u5faa\u73af\u5355\u5143\uff08GRU\uff09\u5c42\u7684\u8f7b\u91cf\u7f16\u7801-\u89e3\u7801\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u6781\u7aef\u52a8\u6001\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u6700\u9ad82.6%\u7684\u5747\u503c\u76f8\u5bf9\u8bef\u5dee\uff0c\u4e14\u5bf9\u566a\u58f0\u8f93\u5165\u6570\u636e\u5177\u6709\u826f\u597d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u6781\u7aef\u52a8\u6001\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u6700\u9ad8\u4e3a2.6%\u7684\u5747\u503c\u76f8\u5bf9\u8bef\u5dee\uff0c\u5c55\u73b0\u4e86\u826f\u597d\u7684\u9c81\u68d2\u6027\u548c\u7269\u7406\u4e00\u81f4\u6027\u3002"}}
{"id": "2510.07514", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07514", "abs": "https://arxiv.org/abs/2510.07514", "authors": ["Cael Yasutake", "Zachary Kingston", "Brian Plancher"], "title": "HJCD-IK: GPU-Accelerated Inverse Kinematics through Batched Hybrid Jacobian Coordinate Descent", "comment": null, "summary": "Inverse Kinematics (IK) is a core problem in robotics, in which joint\nconfigurations are found to achieve a desired end-effector pose. Although\nanalytical solvers are fast and efficient, they are limited to systems with low\ndegrees-of-freedom and specific topological structures. Numerical\noptimization-based approaches are more general, but suffer from high\ncomputational costs and frequent convergence to spurious local minima. Recent\nefforts have explored the use of GPUs to combine sampling and optimization to\nenhance both the accuracy and speed of IK solvers. We build on this recent\nliterature and introduce HJCD-IK, a GPU-accelerated, sampling-based hybrid\nsolver that combines an orientation-aware greedy coordinate descent\ninitialization scheme with a Jacobian-based polishing routine. This design\nenables our solver to improve both convergence speed and overall accuracy as\ncompared to the state-of-the-art, consistently finding solutions along the\naccuracy-latency Pareto frontier and often achieving order-of-magnitude gains.\nIn addition, our method produces a broad distribution of high-quality samples,\nyielding the lowest maximum mean discrepancy. We release our code open-source\nfor the benefit of the community.", "AI": {"tldr": "HJCD-IK\u662f\u4e00\u4e2a\u7ed3\u5408\u4e86GPU\u52a0\u901f\u7684\u91c7\u6837\u4e0e\u4f18\u5316\u7684\u6df7\u5408\u6c42\u89e3\u5668\uff0c\u63d0\u5347\u9006\u5411\u8fd0\u52a8\u5b66\u6c42\u89e3\u7684\u901f\u5ea6\u548c\u51c6\u786e\u6027\uff0c\u5e76\u91ca\u653e\u4e86\u5f00\u6e90\u4ee3\u7801\u3002", "motivation": "\u9006\u5411\u8fd0\u52a8\u5b66\u95ee\u9898\u5728\u673a\u5668\u4eba\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f20\u7edf\u7684\u89e3\u6790\u6c42\u89e3\u5668\u53d7\u9650\u4e8e\u81ea\u7531\u5ea6\u548c\u62d3\u6251\u7ed3\u6784\uff0c\u800c\u6570\u503c\u4f18\u5316\u65b9\u6cd5\u867d\u7136\u66f4\u901a\u7528\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u6613\u9677\u5165\u5c40\u90e8\u6781\u5c0f\u503c\u3002", "method": "HJCD-IK\u7ed3\u5408\u4e86\u65b9\u5411\u654f\u611f\u7684\u8d2a\u5fc3\u5750\u6807\u4e0b\u964d\u521d\u59cb\u5316\u65b9\u6848\u4e0e\u57fa\u4e8e\u96c5\u53ef\u6bd4\u7684\u629b\u5149\u4f8b\u7a0b\u3002", "result": "HJCD-IK\u5728\u51c6\u786e\u6027\u548c\u5ef6\u8fdf\u4e4b\u95f4\u8fbe\u5230\u4e86\u4f18\u826f\u7684\u5e73\u8861\uff0c\u63d0\u4f9b\u9ad8\u8d28\u91cf\u6837\u672c\u7684\u5e7f\u6cdb\u5206\u5e03\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u6c42\u89e3\u5668\u7684\u6027\u80fd\u3002", "conclusion": "HJCD-IK\u662f\u4e00\u79cdGPU\u52a0\u901f\u7684\u6df7\u5408\u6c42\u89e3\u5668\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9006\u5411\u8fd0\u52a8\u5b66\u6c42\u89e3\u7684\u6536\u655b\u901f\u5ea6\u548c\u6574\u4f53\u51c6\u786e\u6027\u3002"}}
{"id": "2510.07548", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07548", "abs": "https://arxiv.org/abs/2510.07548", "authors": ["Adam Hung", "Fan Yang", "Abhinav Kumar", "Sergio Aguilera Marinovic", "Soshi Iba", "Rana Soltani Zarrin", "Dmitry Berenson"], "title": "AVO: Amortized Value Optimization for Contact Mode Switching in Multi-Finger Manipulation", "comment": null, "summary": "Dexterous manipulation tasks often require switching between different\ncontact modes, such as rolling, sliding, sticking, or non-contact contact\nmodes. When formulating dexterous manipulation tasks as a trajectory\noptimization problem, a common approach is to decompose these tasks into\nsub-tasks for each contact mode, which are each solved independently.\nOptimizing each sub-task independently can limit performance, as optimizing\ncontact points, contact forces, or other variables without information about\nfuture sub-tasks can place the system in a state from which it is challenging\nto make progress on subsequent sub-tasks. Further, optimizing these sub-tasks\nis very computationally expensive. To address these challenges, we propose\nAmortized Value Optimization (AVO), which introduces a learned value function\nthat predicts the total future task performance. By incorporating this value\nfunction into the cost of the trajectory optimization at each planning step,\nthe value function gradients guide the optimizer toward states that minimize\nthe cost in future sub-tasks. This effectively bridges separately optimized\nsub-tasks, and accelerates the optimization by reducing the amount of online\ncomputation needed. We validate AVO on a screwdriver grasping and turning task\nin both simulation and real world experiments, and show improved performance\neven with 50% less computational budget compared to trajectory optimization\nwithout the value function.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5AVO\uff0c\u901a\u8fc7\u5f15\u5165\u5b66\u4e60\u7684\u4ef7\u503c\u51fd\u6570\u6765\u63d0\u9ad8\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u7684\u4f18\u5316\u6027\u80fd\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u8d1f\u62c5\u3002", "motivation": "\u5728\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u5207\u6362\u4e0d\u540c\u7684\u63a5\u89e6\u6a21\u5f0f\u662f\u5fc5\u9700\u7684\uff0c\u4f46\u72ec\u7acb\u4f18\u5316\u5b50\u4efb\u52a1\u4f1a\u9650\u5236\u6027\u80fd\u5e76\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u4f7f\u7528\u5b66\u4e60\u7684\u4ef7\u503c\u51fd\u6570\u7ed3\u5408\u8f68\u8ff9\u4f18\u5316\uff0c\u5728\u6bcf\u4e2a\u89c4\u5212\u6b65\u9aa4\u5f15\u5bfc\u4f18\u5316\u5668\u671d\u7740\u6700\u5c0f\u5316\u672a\u6765\u5b50\u4efb\u52a1\u6210\u672c\u7684\u72b6\u6001\u63a8\u8fdb\u3002", "result": "\u63d0\u51fa\u4e86Amortized Value Optimization (AVO)\uff0c\u5b9e\u73b0\u4e86\u901a\u8fc7\u5b66\u4e60\u7684\u4ef7\u503c\u51fd\u6570\u6765\u9884\u6d4b\u672a\u6765\u4efb\u52a1\u6027\u80fd\uff0c\u4ece\u800c\u6539\u5584\u4e86\u4f18\u5316\u8fc7\u7a0b\u3002", "conclusion": "\u901a\u8fc7\u6a21\u62df\u548c\u771f\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1\uff0cAVO\u5728\u6267\u884c\u87ba\u4e1d\u5200\u6293\u53d6\u548c\u8f6c\u52a8\u4efb\u52a1\u65f6\uff0c\u8868\u73b0\u51fa\u66f4\u4f73\u7684\u6027\u80fd\u548c\u964d\u4f4e\u7684\u8ba1\u7b97\u9884\u7b97\u3002"}}
{"id": "2510.07321", "categories": ["cs.HC", "econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2510.07321", "abs": "https://arxiv.org/abs/2510.07321", "authors": ["Antonios Stamatogiannakis", "Arsham Ghodsinia", "Sepehr Etminanrad", "Dilney Gon\u00e7alves", "David Santos"], "title": "How human is the machine? Evidence from 66,000 Conversations with Large Language Models", "comment": null, "summary": "When Artificial Intelligence (AI) is used to replace consumers (e.g.,\nsynthetic data), it is often assumed that AI emulates established consumers,\nand more generally human behaviors. Ten experiments with Large Language Models\n(LLMs) investigate if this is true in the domain of well-documented biases and\nheuristics. Across studies we observe four distinct types of deviations from\nhuman-like behavior. First, in some cases, LLMs reduce or correct biases\nobserved in humans. Second, in other cases, LLMs amplify these same biases.\nThird, and perhaps most intriguingly, LLMs sometimes exhibit biases opposite to\nthose found in humans. Fourth, LLMs' responses to the same (or similar) prompts\ntend to be inconsistent (a) within the same model after a time delay, (b)\nacross models, and (c) among independent research studies. Such inconsistencies\ncan be uncharacteristic of humans and suggest that, at least at one point,\nLLMs' responses differed from humans. Overall, unhuman-like responses are\nproblematic when LLMs are used to mimic or predict consumer behavior. These\nfindings complement research on synthetic consumer data by showing that sources\nof bias are not necessarily human-centric. They also contribute to the debate\nabout the tasks for which consumers, and more generally humans, can be replaced\nby AI.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5341\u4e2a\u5b9e\u9a8c\u63ed\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6a21\u62df\u4eba\u7c7b\u6d88\u8d39\u8005\u884c\u4e3a\u65f6\u7684\u591a\u6837\u504f\u5dee\uff0c\u6307\u51fa\u5b83\u4eec\u5728\u504f\u89c1\u8868\u73b0\u548c\u54cd\u5e94\u4e00\u81f4\u6027\u4e0a\u4e0e\u4eba\u7c7b\u5b58\u5728\u663e\u8457\u533a\u522b\u3002", "motivation": "\u63a2\u8ba8\u4eba\u5de5\u667a\u80fd\u5728\u66ff\u4ee3\u6d88\u8d39\u8005\u65f6\u662f\u5426\u80fd\u591f\u51c6\u786e\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5728\u504f\u89c1\u548c\u542f\u53d1\u5f0f\u51b3\u7b56\u65b9\u9762\u3002", "method": "\u901a\u8fc7\u5341\u4e2a\u5b9e\u9a8c\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u5728\u4e00\u4e9b\u5df2\u77e5\u504f\u89c1\u548c\u542f\u53d1\u5f0f\u9886\u57df\u7684\u884c\u4e3a\u3002", "result": "LLMs\u8868\u73b0\u51fa\u56db\u79cd\u660e\u663e\u4e0e\u4eba\u7c7b\u884c\u4e3a\u4e0d\u540c\u7684\u504f\u5dee\uff0c\u65e2\u5305\u62ec\u51cf\u5c11\u548c\u653e\u5927\u504f\u89c1\uff0c\u4e5f\u5305\u62ec\u8868\u73b0\u51fa\u4e0e\u4eba\u7c7b\u76f8\u53cd\u7684\u504f\u89c1\uff0c\u4ee5\u53ca\u54cd\u5e94\u7684\u4e00\u81f4\u6027\u95ee\u9898\u3002", "conclusion": "LLMs\u5728\u6a21\u4eff\u6216\u9884\u6d4b\u6d88\u8d39\u8005\u884c\u4e3a\u65f6\u8868\u73b0\u51fa\u4e0e\u4eba\u7c7b\u4e0d\u4e00\u81f4\u7684\u53cd\u5e94\uff0c\u8fd9\u53ef\u80fd\u4f7f\u5f97\u5b83\u4eec\u4e0d\u9002\u5408\u7528\u4f5c\u66ff\u4ee3\u6d88\u8d39\u8005\u7684\u6570\u636e\u6e90\u3002"}}
{"id": "2510.07611", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07611", "abs": "https://arxiv.org/abs/2510.07611", "authors": ["Jingyang You", "Hanna Kurniawati", "Lashika Medagoda"], "title": "Inspection Planning Primitives with Implicit Models", "comment": null, "summary": "The aging and increasing complexity of infrastructures make efficient\ninspection planning more critical in ensuring safety. Thanks to sampling-based\nmotion planning, many inspection planners are fast. However, they often require\nhuge memory. This is particularly true when the structure under inspection is\nlarge and complex, consisting of many struts and pillars of various geometry\nand sizes. Such structures can be represented efficiently using implicit\nmodels, such as neural Signed Distance Functions (SDFs). However, most\nprimitive computations used in sampling-based inspection planner have been\ndesigned to work efficiently with explicit environment models, which in turn\nrequires the planner to use explicit environment models or performs frequent\ntransformations between implicit and explicit environment models during\nplanning. This paper proposes a set of primitive computations, called\nInspection Planning Primitives with Implicit Models (IPIM), that enable\nsampling-based inspection planners to entirely use neural SDFs representation\nduring planning. Evaluation on three scenarios, including inspection of a\ncomplex real-world structure with over 92M triangular mesh faces, indicates\nthat even a rudimentary sampling-based planner with IPIM can generate\ninspection trajectories of similar quality to those generated by the\nstate-of-the-art planner, while using up to 70x less memory than the\nstate-of-the-art inspection planner.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u9690\u5f0f\u6a21\u578b\uff08\u795e\u7ecfSDFs\uff09\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u68c0\u67e5\u89c4\u5212\u7684\u5185\u5b58\u9700\u6c42\uff0c\u800c\u4fdd\u6301\u8f68\u8ff9\u8d28\u91cf\u3002", "motivation": "\u57fa\u7840\u8bbe\u65bd\u7684\u8001\u5316\u548c\u590d\u6742\u6027\u589e\u52a0\uff0c\u4f7f\u5f97\u9ad8\u6548\u7684\u68c0\u67e5\u89c4\u5212\u53d8\u5f97\u66f4\u52a0\u91cd\u8981\uff0c\u4ee5\u786e\u4fdd\u5b89\u5168\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u7ec4\u57fa\u672c\u8ba1\u7b97\uff0c\u5373\u9690\u5f0f\u6a21\u578b\u68c0\u67e5\u89c4\u5212\u57fa\u672c\u8ba1\u7b97\uff08IPIM\uff09\uff0c\u6765\u652f\u6301\u57fa\u4e8e\u91c7\u6837\u7684\u68c0\u67e5\u89c4\u5212\u5668\u5728\u89c4\u5212\u8fc7\u7a0b\u4e2d\u5b8c\u5168\u4f7f\u7528\u9690\u5f0f\u8868\u793a\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u5957\u88ab\u79f0\u4e3a\u9690\u5f0f\u6a21\u578b\u68c0\u67e5\u89c4\u5212\u57fa\u672c\u8ba1\u7b97\u7684\u65b9\u6cd5\uff08IPIM\uff09\uff0c\u4f7f\u5f97\u57fa\u4e8e\u91c7\u6837\u7684\u68c0\u67e5\u89c4\u5212\u5668\u80fd\u591f\u5b8c\u5168\u5229\u7528\u795e\u7ecf\u7b7e\u540d\u8ddd\u79bb\u51fd\u6570\uff08SDF\uff09\u8868\u793a\u3002", "conclusion": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528IPIM\u7684\u57fa\u672c\u91c7\u6837\u89c4\u5212\u5668\u80fd\u591f\u751f\u6210\u4e0e\u6700\u5148\u8fdb\u89c4\u5212\u5668\u76f8\u4f3c\u8d28\u91cf\u7684\u68c0\u67e5\u8f68\u8ff9\uff0c\u540c\u65f6\u4f7f\u7528\u7684\u5185\u5b58\u51cf\u5c11\u4e86\u591a\u8fbe70\u500d\u3002"}}
{"id": "2510.07322", "categories": ["cs.HC", "53-04, 68, 92", "H.4; J.3; J.2"], "pdf": "https://arxiv.org/pdf/2510.07322", "abs": "https://arxiv.org/abs/2510.07322", "authors": ["Hitesh Mohapatra"], "title": "A LoRa IoT Framework with Machine Learning for Remote Livestock Monitoring in Smart Agriculture", "comment": null, "summary": "This work presents AgroTrack, a LoRa-based IoT framework for remote livestock\nmonitoring in smart agriculture. The system is designed for low-power,\nlong-range communication and supports real-time tracking and basic health\nassessment of free-range livestock through GPS, motion, and temperature sensors\nintegrated into wearable collars. Data is collected and transmitted via LoRa to\ngateways and forwarded to a cloud platform for visualization, alerts, and\nanalytics. To enhance its practical deployment, AgroTrack incorporates advanced\nanalytics, including machine learning models for predictive health alerts and\nbehavioral anomaly detection. This integration transforms the framework from a\nbasic monitoring tool into an intelligent decision-support system, enabling\nfarmers to improve livestock management, operational efficiency, and\nsustainability in rural environments.", "AI": {"tldr": "AgroTrack\u662f\u4e00\u4e2a\u57fa\u4e8eLoRa\u7684IoT\u6846\u67b6\uff0c\u901a\u8fc7\u7a7f\u6234\u5f0f\u4f20\u611f\u5668\u5b9e\u73b0\u5bf9\u81ea\u7531\u653e\u517b\u7272\u755c\u7684\u5b9e\u65f6\u76d1\u6d4b\u548c\u5065\u5eb7\u8bc4\u4f30\uff0c\u96c6\u6210\u9ad8\u7ea7\u5206\u6790\u4ee5\u652f\u6301\u519c\u6c11\u66f4\u597d\u7684\u7ba1\u7406\u548c\u51b3\u7b56\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u519c\u6751\u667a\u80fd\u519c\u4e1a\u4e2d\u7684\u755c\u7267\u7ba1\u7406\u6548\u7387\u53ca\u53ef\u6301\u7eed\u53d1\u5c55\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4f4e\u529f\u8017\u3001\u957f\u8ddd\u79bb\u7684\u76d1\u6d4b\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528LoRa\u8fdb\u884c\u8fdc\u7a0b\u7272\u755c\u76d1\u6d4b\uff0c\u96c6\u6210GPS\u3001\u8fd0\u52a8\u548c\u6e29\u5ea6\u4f20\u611f\u5668\uff0c\u901a\u8fc7\u4e91\u5e73\u53f0\u8fdb\u884c\u6570\u636e\u53ef\u89c6\u5316\u548c\u5206\u6790\u3002", "result": "AgroTrack\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5b9e\u73b0\u4e86\u9884\u6d4b\u5065\u5eb7\u8b66\u62a5\u548c\u884c\u4e3a\u5f02\u5e38\u68c0\u6d4b\uff0c\u63d0\u5347\u4e86\u76d1\u6d4b\u6846\u67b6\u7684\u667a\u80fd\u5316\u3002", "conclusion": "AgroTrack\u662f\u4e00\u4e2a\u667a\u80fd\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff0c\u901a\u8fc7\u7269\u8054\u7f51\u6280\u672f\u63d0\u5347\u4e86\u519c\u6751\u73af\u5883\u4e0b\u7684\u755c\u7267\u7ba1\u7406\u548c\u53ef\u6301\u7eed\u6027\u3002"}}
{"id": "2510.07625", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.07625", "abs": "https://arxiv.org/abs/2510.07625", "authors": ["Alexander Du", "Emre Adabag", "Gabriel Bravo", "Brian Plancher"], "title": "GATO: GPU-Accelerated and Batched Trajectory Optimization for Scalable Edge Model Predictive Control", "comment": null, "summary": "While Model Predictive Control (MPC) delivers strong performance across\nrobotics applications, solving the underlying (batches of) nonlinear trajectory\noptimization (TO) problems online remains computationally demanding. Existing\nGPU-accelerated approaches typically (i) parallelize a single solve to meet\nreal-time deadlines, (ii) scale to very large batches at slower-than-real-time\nrates, or (iii) achieve speed by restricting model generality (e.g., point-mass\ndynamics or a single linearization). This leaves a large gap in solver\nperformance for many state-of-the-art MPC applications that require real-time\nbatches of tens to low-hundreds of solves. As such, we present GATO, an open\nsource, GPU-accelerated, batched TO solver co-designed across algorithm,\nsoftware, and computational hardware to deliver real-time throughput for these\nmoderate batch size regimes. Our approach leverages a combination of block-,\nwarp-, and thread-level parallelism within and across solves for ultra-high\nperformance. We demonstrate the effectiveness of our approach through a\ncombination of: simulated benchmarks showing speedups of 18-21x over CPU\nbaselines and 1.4-16x over GPU baselines as batch size increases; case studies\nhighlighting improved disturbance rejection and convergence behavior; and\nfinally a validation on hardware using an industrial manipulator. We open\nsource GATO to support reproducibility and adoption.", "AI": {"tldr": "\u63d0\u51faGATO\uff0c\u4e00\u4e2aGPU\u52a0\u901f\u7684\u6279\u5904\u7406\u8f68\u8ff9\u4f18\u5316\u6c42\u89e3\u5668\uff0c\u6ee1\u8db3\u5b9e\u65f6MPC\u5e94\u7528\u9700\u6c42\uff0c\u663e\u8457\u63d0\u9ad8\u6c42\u89e3\u6027\u80fd\u5e76\u5f00\u6e90\u4ee5\u652f\u6301\u518d\u73b0\u6027\u548c\u91c7\u7528", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8eGPU\u7684\u65b9\u6cd5\u5728\u5b9e\u65f6\u6027\u80fd\u548c\u6a21\u578b\u901a\u7528\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4ee5\u6ee1\u8db3\u73b0\u4ee3MPC\u5e94\u7528\u5bf9\u5b9e\u65f6\u6279\u5904\u7406\u7684\u9700\u6c42", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGATO\u7684\u5f00\u653e\u6e90\u4ee3\u7801\u3001GPU\u52a0\u901f\u7684\u6279\u5904\u7406\u8f68\u8ff9\u4f18\u5316\u6c42\u89e3\u5668", "result": "\u5728\u6279\u5904\u7406\u5927\u5c0f\u589e\u52a0\u65f6\uff0cGATO\u5728CPU\u57fa\u7ebf\u4e4b\u4e0a\u5b9e\u73b018-21\u500d\u7684\u52a0\u901f\uff0c\u5728GPU\u57fa\u7ebf\u4e4b\u4e0a\u5b9e\u73b01.4-16\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u6539\u5584\u4e86\u5e72\u6270\u6291\u5236\u548c\u6536\u655b\u7279\u6027\uff0c\u5e76\u5728\u5de5\u4e1a\u64cd\u7eb5\u5668\u4e0a\u8fdb\u884c\u4e86\u786c\u4ef6\u9a8c\u8bc1", "conclusion": "GATO\u6709\u6548\u63d0\u5347\u6279\u5904\u7406\u8f68\u8ff9\u4f18\u5316\u7684\u6c42\u89e3\u6027\u80fd\uff0c\u4f7f\u5176\u9002\u5408\u5b9e\u65f6\u5e94\u7528\uff0c\u5e76\u4e3a\u7814\u7a76\u8005\u548c\u5f00\u53d1\u8005\u63d0\u4f9b\u5f00\u653e\u7684\u5de5\u5177\u652f\u6301\u3002"}}
{"id": "2510.07609", "categories": ["cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07609", "abs": "https://arxiv.org/abs/2510.07609", "authors": ["Victor Victor", "Tania Krisanty", "Matthew McGinity", "Stefan Gumhold", "Uwe A\u00dfmann"], "title": "IGUANA: Immersive Guidance, Navigation, and Control for Consumer UAV", "comment": "This is the author's version of the work. The definitive Version of\n  Record was published in 31st ACM Symposium on Virtual Reality Software and\n  Technology (VRST '25)", "summary": "As the markets for unmanned aerial vehicles (UAVs) and mixed reality (MR)\nheadsets continue to grow, recent research has increasingly explored their\nintegration, which enables more intuitive, immersive, and situationally aware\ncontrol systems. We present IGUANA, an MR-based immersive guidance, navigation,\nand control system for consumer UAVs. IGUANA introduces three key elements\nbeyond conventional control interfaces: (1) a 3D terrain map interface with\ndraggable waypoint markers and live camera preview for high-level control, (2)\na novel spatial control metaphor that uses a virtual ball as a physical analogy\nfor low-level control, and (3) a spatial overlay that helps track the UAV when\nit is not visible with the naked eye or visual line of sight is interrupted. We\nconducted a user study to evaluate our design, both quantitatively and\nqualitatively, and found that (1) the 3D map interface is intuitive and easy to\nuse, relieving users from manual control and suggesting improved accuracy and\nconsistency with lower perceived workload relative to conventional dual-stick\ncontroller, (2) the virtual ball interface is intuitive but limited by the lack\nof physical feedback, and (3) the spatial overlay is very useful in enhancing\nthe users' situational awareness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faIGUANA\uff0c\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u73b0\u5b9e\u7684\u65e0\u4eba\u673a\u63a7\u5236\u7cfb\u7edf\uff0c\u7ed3\u54083D\u5730\u56fe\u3001\u865a\u62df\u7403\u63a7\u5236\u548c\u7a7a\u95f4\u8986\u76d6\uff0c\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u63a2\u7d22\u65e0\u4eba\u673a\u548c\u6df7\u5408\u73b0\u5b9e\u5934\u6234\u8bbe\u5907\u7684\u96c6\u6210\u4ee5\u63d0\u9ad8\u63a7\u5236\u7cfb\u7edf\u7684\u76f4\u89c2\u6027\u548c\u6c89\u6d78\u611f\u3002", "method": "\u7528\u6237\u7814\u7a76\u8bc4\u4f30IGUANA\u8bbe\u8ba1", "result": "\u53d1\u73b03D\u5730\u56fe\u754c\u9762\u76f4\u89c2\u6613\u7528\uff0c\u865a\u62df\u7403\u754c\u9762\u76f4\u89c2\u4f46\u7f3a\u4e4f\u7269\u7406\u53cd\u9988\uff0c\u7a7a\u95f4\u8986\u76d6\u589e\u5f3a\u7528\u6237\u7684\u60c5\u5883\u610f\u8bc6\u3002", "conclusion": "IGUANA\u6709\u6548\u63d0\u5347\u4e86\u7528\u6237\u5728\u63a7\u5236\u65e0\u4eba\u673a\u8fc7\u7a0b\u4e2d\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\uff0c\u8bf4\u660e\u4e86\u6df7\u5408\u73b0\u5b9e\u6280\u672f\u5728\u65e0\u4eba\u673a\u63a7\u5236\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.07674", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07674", "abs": "https://arxiv.org/abs/2510.07674", "authors": ["Lucas Chen", "Shrutheesh Raman Iyer", "Zachary Kingston"], "title": "Differentiable Particle Optimization for Fast Sequential Manipulation", "comment": "8 pages, 7 figures, 3 tables. Under review", "summary": "Sequential robot manipulation tasks require finding collision-free\ntrajectories that satisfy geometric constraints across multiple object\ninteractions in potentially high-dimensional configuration spaces. Solving\nthese problems in real-time and at large scales has remained out of reach due\nto computational requirements. Recently, GPU-based acceleration has shown\npromising results, but prior methods achieve limited performance due to CPU-GPU\ndata transfer overhead and complex logic that prevents full hardware\nutilization. To this end, we present SPaSM (Sampling Particle optimization for\nSequential Manipulation), a fully GPU-parallelized framework that compiles\nconstraint evaluation, sampling, and gradient-based optimization into optimized\nCUDA kernels for end-to-end trajectory optimization without CPU coordination.\nThe method consists of a two-stage particle optimization strategy: first\nsolving placement constraints through massively parallel sampling, then lifting\nsolutions to full trajectory optimization in joint space. Unlike hierarchical\napproaches, SPaSM jointly optimizes object placements and robot trajectories to\nhandle scenarios where motion feasibility constrains placement options.\nExperimental evaluation on challenging benchmarks demonstrates solution times\nin the realm of $\\textbf{milliseconds}$ with a 100% success rate; a\n$4000\\times$ speedup compared to existing approaches.", "AI": {"tldr": "SPaSM\u662f\u4e00\u4e2a\u5168GPU\u5e76\u884c\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f6\u4f18\u5316\u987a\u5e8f\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u5bfb\u627e\u65e0\u78b0\u649e\u4e14\u6ee1\u8db3\u591a\u7269\u4f53\u4ea4\u4e92\u51e0\u4f55\u7ea6\u675f\u7684\u8f68\u8ff9\uff0c\u4ee5\u4fbf\u5728\u9ad8\u7ef4\u914d\u7f6e\u7a7a\u95f4\u4e2d\u8fdb\u884c\u987a\u5e8f\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u89e3\u51b3\u5b9e\u65f6\u548c\u5927\u89c4\u6a21\u8ba1\u7b97\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u7c92\u5b50\u4f18\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u5e76\u884c\u91c7\u6837\u548c\u5173\u8282\u7a7a\u95f4\u7684\u8f68\u8ff9\u4f18\u5316\u5b9e\u73b0\u76ee\u6807\u7ea6\u675f\u7684\u8054\u5408\u4f18\u5316\u3002", "result": "SPaSM\u5728\u590d\u6742\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u9ad8\u8fbe4000\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u663e\u8457\u7684\u4f18\u52bf\u3002", "conclusion": "SPaSM\u5b9e\u73b0\u4e86\u5728\u6beb\u79d2\u7ea7\u522b\u7684\u89e3\u51b3\u65f6\u95f4\uff0c\u5e76\u5728\u590d\u6742\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86100%\u7684\u6210\u529f\u7387\u3002"}}
{"id": "2510.07610", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.07610", "abs": "https://arxiv.org/abs/2510.07610", "authors": ["Nate Laffan", "Ashley Hom", "Andrea Nadine Castillo", "Elizabeth Gitelman", "Rebecca Zhao", "Nikita Shenoy", "Kaia Rae Schweig", "Katherine Isbister"], "title": "The Slow Space Editor : Broadening Access to Restorative XR", "comment": null, "summary": "The Slow Space Editor is a 2D tool for creating 3D spaces. It was built as\npart of a research-through-design project that investigates how Virtual and\nMixed Reality (XR) environments might be used for reflection and attention\nrestoration. In this phase, we seek to radically simplify the creation of\nvirtual environments, thereby broadening the potential group of users who could\nbenefit from them. The research described in this paper has three aspects.\nFirst, we define the concept of \"slow space,\" situating it alongside existing\nresearch in HCI and environmental psychology. Second, we report on a series of\ninterviews with professional designers about how slow spaces are created in the\nphysical world. Third, we share the design of the tool itself, focussing on the\nbenefits of providing a simple method for users to control their environments.\nWe conclude with our findings from a 19-person qualitative study of the tool.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u65e8\u5728\u7b80\u5316\u865a\u62df\u73af\u5883\u521b\u5efa\u7684 2D \u5de5\u5177\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5bf9\u7528\u6237\u53cd\u601d\u548c\u6ce8\u610f\u529b\u6062\u590d\u7684\u5f71\u54cd\u3002", "motivation": "\u5e0c\u671b\u901a\u8fc7\u6781\u7b80\u5316\u865a\u62df\u73af\u5883\u7684\u521b\u5efa\u65b9\u5f0f\uff0c\u63d0\u5347\u7528\u6237\u7684\u53cd\u601d\u4e0e\u6ce8\u610f\u529b\u6062\u590d\u80fd\u529b\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u4e13\u4e1a\u8bbe\u8ba1\u5e08\u8bbf\u8c08\u548c\u5b9a\u6027\u7814\u7a76\u6765\u8bc4\u4f30\u5de5\u5177\u7684\u6548\u679c\u3002", "result": "Slow Space Editor \u662f\u4e00\u4e2a\u7528\u4e8e\u521b\u5efa 3D \u7a7a\u95f4\u7684 2D \u5de5\u5177\uff0c\u65e8\u5728\u7b80\u5316\u865a\u62df\u73af\u5883\u7684\u521b\u5efa\u8fc7\u7a0b\uff0c\u4f7f\u66f4\u591a\u7528\u6237\u53d7\u76ca\u3002", "conclusion": "\u901a\u8fc7\u5bf9\u5de5\u5177\u7684\u5b9a\u6027\u7814\u7a76\u53d1\u73b0\uff0c\u7b80\u5355\u7684\u73af\u5883\u63a7\u5236\u65b9\u6cd5\u53ef\u63d0\u9ad8\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2510.07700", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.07700", "abs": "https://arxiv.org/abs/2510.07700", "authors": ["Raghav Mishra", "Ian R. Manchester"], "title": "EB-MBD: Emerging-Barrier Model-Based Diffusion for Safe Trajectory Optimization in Highly Constrained Environments", "comment": null, "summary": "We propose enforcing constraints on Model-Based Diffusion by introducing\nemerging barrier functions inspired by interior point methods. We show that\nconstraints on Model-Based Diffusion can lead to catastrophic performance\ndegradation, even on simple 2D systems due to sample inefficiency in the Monte\nCarlo approximation of the score function. We introduce Emerging-Barrier\nModel-Based Diffusion (EB-MBD) which uses progressively introduced barrier\nconstraints to avoid these problems, significantly improving solution quality,\nwithout the need for computationally expensive operations such as projections.\nWe analyze the sampling liveliness of samples each iteration to inform barrier\nparameter scheduling choice. We demonstrate results for 2D collision avoidance\nand a 3D underwater manipulator system and show that our method achieves lower\ncost solutions than Model-Based Diffusion, and requires orders of magnitude\nless computation time than projection based methods.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165\u65b0\u5174\u969c\u788d\u51fd\u6570\uff0cEB-MBD\u663e\u8457\u6539\u5584\u4e86\u57fa\u4e8e\u6a21\u578b\u7684\u6269\u6563\u5728\u5904\u7406\u7ea6\u675f\u65f6\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u8ba1\u7b97\u6548\u7387\u548c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u4e0a\u3002", "motivation": "\u57fa\u4e8e\u6a21\u578b\u7684\u6269\u6563\u5b58\u5728\u7ea6\u675f\u65f6\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u5c24\u5176\u662f\u5728\u7b80\u5355\u7684\u4e8c\u7ef4\u7cfb\u7edf\u4e2d\uff0c\u4e3b\u8981\u7531\u4e8e\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u5728\u8bc4\u5206\u51fd\u6570\u8fd1\u4f3c\u4e2d\u7684\u6837\u672c\u4f4e\u6548\u6027\u3002", "method": "\u5f15\u5165\u65b0\u5174\u969c\u788d\u51fd\u6570\u4ee5\u7ea6\u675f\u57fa\u4e8e\u6a21\u578b\u7684\u6269\u6563\u8fc7\u7a0b", "result": "\u65b0\u5174\u969c\u788d\u6a21\u578b\u57fa\u4e8e\u6269\u6563\uff08EB-MBD\uff09\u901a\u8fc7\u9010\u6b65\u5f15\u5165\u969c\u788d\u7ea6\u675f\uff0c\u907f\u514d\u4e86\u6837\u672c\u4f4e\u6548\u6027\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u8ba1\u7b97\u590d\u6742\u7684\u6295\u5f71\u64cd\u4f5c\u3002", "conclusion": "EB-MBD\u65b9\u6cd5\u5728\u4e8c\u7ef4\u907f\u78b0\u548c\u4e09\u7ef4\u6c34\u4e0b\u64cd\u63a7\u7b49\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8f83\u4f4e\u7684\u6210\u672c\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e14\u8ba1\u7b97\u65f6\u95f4\u8fdc\u5c0f\u4e8e\u57fa\u4e8e\u6295\u5f71\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.07754", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.07754", "abs": "https://arxiv.org/abs/2510.07754", "authors": ["Yi-Chi Liao", "Jo\u00e3o Belo", "Hee-Seung Moon", "J\u00fcrgen Steimle", "Anna Maria Feit"], "title": "Human-in-the-Loop Optimization with Model-Informed Priors", "comment": null, "summary": "Human-in-the-loop optimization identifies optimal interface designs by\niteratively observing user performance. However, it often requires numerous\niterations due to the lack of prior information. While recent approaches have\naccelerated this process by leveraging previous optimization data, collecting\nuser data remains costly and often impractical. We present a conceptual\nframework, Human-in-the-Loop Optimization with Model-Informed Priors (HOMI),\nwhich augments human-in-the-loop optimization with a training phase where the\noptimizer learns adaptation strategies from diverse, synthetic user data\ngenerated with predictive models before deployment. To realize HOMI, we\nintroduce Neural Acquisition Function+ (NAF+), a Bayesian optimization method\nfeaturing a neural acquisition function trained with reinforcement learning.\nNAF+ learns optimization strategies from large-scale synthetic data, improving\nefficiency in real-time optimization with users. We evaluate HOMI and NAF+ with\nmid-air keyboard optimization, a representative VR input task. Our work\npresents a new approach for more efficient interface adaptation by bridging in\nsitu and in silico optimization processes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86HOMI\u6846\u67b6\u548c\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u4f18\u5316\u65b9\u6cd5NAF+\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u63d0\u9ad8\u4eba\u673a\u8fed\u4ee3\u4f18\u5316\u6548\u7387\u3002", "motivation": "\u4eba\u673a\u8fed\u4ee3\u4f18\u5316\u5e38\u5e38\u9700\u8981\u591a\u6b21\u8fed\u4ee3\u4ee5\u5bfb\u627e\u6700\u4f73\u754c\u9762\u8bbe\u8ba1\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u5148\u524d\u4fe1\u606f\uff0c\u8fd9\u4e00\u8fc7\u7a0b\u6548\u7387\u8f83\u4f4e\u3002\u540c\u65f6\uff0c\u6536\u96c6\u7528\u6237\u6570\u636e\u7684\u6210\u672c\u9ad8\u6602\u4e14\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u795e\u7ecf\u91c7\u96c6\u51fd\u6570NAF+\uff0c\u5e76\u5728\u4f7f\u7528\u5408\u6210\u6570\u636e\u7684\u57fa\u7840\u4e0a\u63d0\u5347\u5b9e\u65f6\u4f18\u5316\u6548\u7387\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6982\u5ff5\u6846\u67b6HOMI\uff0c\u901a\u8fc7\u5728\u771f\u5b9e\u7528\u6237\u90e8\u7f72\u524d\u8fdb\u884c\u8bad\u7ec3\u9636\u6bb5\uff0c\u5229\u7528\u751f\u6210\u7684\u5408\u6210\u7528\u6237\u6570\u636e\u589e\u5f3a\u4eba\u673a\u8fed\u4ee3\u4f18\u5316\u7684\u6548\u679c\u3002", "conclusion": "HOMI\u4e0eNAF+\u8868\u73b0\u51fa\u5728VR\u8f93\u5165\u4efb\u52a1\u4e2d\u7684\u9ad8\u6548\u6027\uff0c\u4e3a\u754c\u9762\u81ea\u9002\u5e94\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2510.07725", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07725", "abs": "https://arxiv.org/abs/2510.07725", "authors": ["Kasidit Muenprasitivej", "Ye Zhao", "Glen Chou"], "title": "Probabilistically-Safe Bipedal Navigation over Uncertain Terrain via Conformal Prediction and Contraction Analysis", "comment": "9 pages, 4 figures", "summary": "We address the challenge of enabling bipedal robots to traverse rough terrain\nby developing probabilistically safe planning and control strategies that\nensure dynamic feasibility and centroidal robustness under terrain uncertainty.\nSpecifically, we propose a high-level Model Predictive Control (MPC) navigation\nframework for a bipedal robot with a specified confidence level of safety that\n(i) enables safe traversal toward a desired goal location across a terrain map\nwith uncertain elevations, and (ii) formally incorporates uncertainty bounds\ninto the centroidal dynamics of locomotion control. To model the rough terrain,\nwe employ Gaussian Process (GP) regression to estimate elevation maps and\nleverage Conformal Prediction (CP) to construct calibrated confidence intervals\nthat capture the true terrain elevation. Building on this, we formulate\ncontraction-based reachable tubes that explicitly account for terrain\nuncertainty, ensuring state convergence and tube invariance. In addition, we\nintroduce a contraction-based flywheel torque control law for the reduced-order\nLinear Inverted Pendulum Model (LIPM), which stabilizes the angular momentum\nabout the center-of-mass (CoM). This formulation provides both probabilistic\nsafety and goal reachability guarantees. For a given confidence level, we\nestablish the forward invariance of the proposed torque control law by\ndemonstrating exponential stabilization of the actual CoM phase-space\ntrajectory and the desired trajectory prescribed by the high-level planner.\nFinally, we evaluate the effectiveness of our planning framework through\nphysics-based simulations of the Digit bipedal robot in MuJoCo.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u9488\u5bf9\u53cc\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e2d\u5b89\u5168\u884c\u8d70\u7684\u89c4\u5212\u548c\u63a7\u5236\u7b56\u7565\uff0c\u91c7\u7528\u9ad8\u5c42\u6b21\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u65b9\u6cd5\uff0c\u786e\u4fdd\u52a8\u6001\u53ef\u884c\u6027\u548c\u4e2d\u5fc3\u7a33\u5065\u6027\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u53cc\u8db3\u673a\u5668\u4eba\u5728\u4e0d\u540c\u5730\u5f62\u6761\u4ef6\u4e0b\u7684\u5b89\u5168\u884c\u8d70\u6311\u6218\uff0c\u786e\u4fdd\u52a8\u6001\u7a33\u5b9a\u6027\u548c\u5bfc\u822a\u7684\u53ef\u9760\u6027\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5951\u7ea6\u63a7\u5236\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u5408\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u548c\u7b26\u53f7\u9884\u6d4b\uff0c\u4ee5\u5b9e\u73b0\u5730\u5f62\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u3002", "result": "\u63d0\u51fa\u7684\u89c4\u5212\u7b56\u7565\u786e\u4fdd\u53cc\u8db3\u673a\u5668\u4eba\u5728\u6709\u4e0d\u786e\u5b9a\u6027\u7684\u5730\u5f62\u4e0a\u5b89\u5168\u884c\u8fdb\u5e76\u80fd\u591f\u8fbe\u5230\u9884\u5b9a\u76ee\u6807\u3002", "conclusion": "\u901a\u8fc7\u7269\u7406\u4eff\u771f\u9a8c\u8bc1\u4e86\u89c4\u5212\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5e76\u786e\u4fdd\u4e86\u673a\u5668\u4eba\u5728\u4e0d\u786e\u5b9a\u5730\u5f62\u4e2d\u7684\u5b89\u5168\u884c\u8fdb\u548c\u76ee\u6807\u5230\u8fbe\u3002"}}
{"id": "2510.07829", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07829", "abs": "https://arxiv.org/abs/2510.07829", "authors": ["Cathal Doyle"], "title": "The Rise of the Knowledge Sculptor: A New Archetype for Knowledge Work in the Age of Generative AI", "comment": "23 pages, 11 figures, preprint", "summary": "In the Generative Age, the nature of knowledge work is transforming.\nTraditional models that emphasise the organisation and retrieval of\npre-existing information are increasingly inadequate in the face of generative\nAI (GenAI) systems capable of autonomous content creation. This paper\nintroduces the Knowledge Sculptor (KS), a new professional archetype for\nHuman-GenAI collaboration that transforms raw AI output into trustworthy,\nactionable knowledge. Grounded in a socio-technical perspective, the KS is\nconceptualised through a framework of competencies, including architecting a\nvision, iterative dialogue, information sculpting, and curiosity-driven\nsynthesis. A practice-based vignette illustrates the KS role in action, and in\na self-referential approach, the paper itself serves as an artefact of the\nsculpting process it describes.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u77e5\u8bc6\u96d5\u523b\u8005\u8fd9\u4e00\u65b0\u804c\u4e1a\u89d2\u8272\uff0c\u65e8\u5728\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u77e5\u8bc6\u8f6c\u5316\u80fd\u529b\uff0c\u4ee5\u5e94\u5bf9\u751f\u6210\u6027AI\u5e26\u6765\u7684\u6311\u6218\u3002", "motivation": "\u968f\u7740\u751f\u6210\u6027\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\uff0c\u4f20\u7edf\u7684\u77e5\u8bc6\u7ba1\u7406\u6a21\u578b\u5df2\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\uff0c\u65b0\u7684\u804c\u4e1a\u89d2\u8272\u548c\u80fd\u529b\u6846\u67b6\u4e9f\u9700\u51fa\u73b0\uff0c\u4ee5\u4fc3\u8fdb\u4eba\u673a\u534f\u4f5c\u3002", "method": "\u672c\u6587\u91c7\u7528\u793e\u4f1a\u6280\u672f\u89c6\u89d2\uff0c\u7ed3\u5408\u80fd\u529b\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u8df5\u6848\u4f8b\u5c55\u793a\u77e5\u8bc6\u96d5\u523b\u8005\u7684\u5b9e\u9645\u5e94\u7528\u3002", "result": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u751f\u6210\u65f6\u4ee3\u4e2d\u77e5\u8bc6\u5de5\u4f5c\u7684\u8f6c\u53d8\uff0c\u5e76\u5f15\u5165\u4e86\u77e5\u8bc6\u96d5\u523b\u8005\u8fd9\u4e00\u65b0\u804c\u4e1a\u89d2\u8272\uff0c\u4ee5\u5e94\u5bf9\u751f\u6210\u6027\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u6311\u6218\u3002\u77e5\u8bc6\u96d5\u523b\u8005\u5728\u4eba\u4e0e\u751f\u6210AI\u7684\u534f\u4f5c\u4e2d\uff0c\u8d1f\u8d23\u5c06\u539f\u59cbAI\u8f93\u51fa\u8f6c\u5316\u4e3a\u53ef\u4fe1\u8d56\u3001\u53ef\u64cd\u4f5c\u7684\u77e5\u8bc6\u3002\u4ee5\u793e\u4f1a\u6280\u672f\u89c6\u89d2\u4e3a\u57fa\u7840\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u80fd\u529b\u6846\u67b6\uff0c\u5305\u62ec\u613f\u666f\u67b6\u6784\u3001\u8fed\u4ee3\u5bf9\u8bdd\u3001\u4fe1\u606f\u96d5\u523b\u548c\u597d\u5947\u9a71\u52a8\u7684\u7efc\u5408\u3002\u5728\u5b9e\u8df5\u6848\u4f8b\u4e2d\uff0c\u5c55\u793a\u4e86\u77e5\u8bc6\u96d5\u523b\u8005\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u89d2\u8272\u3002\u6b64\u5916\uff0c\u672c\u6587\u672c\u8eab\u4e5f\u4f5c\u4e3a\u5176\u63cf\u8ff0\u7684\u96d5\u523b\u8fc7\u7a0b\u7684\u4e00\u4e2a\u5b9e\u4f8b\u3002", "conclusion": "\u77e5\u8bc6\u96d5\u523b\u8005\u4f5c\u4e3a\u4e00\u79cd\u65b0\u804c\u4e1a\u89d2\u8272\uff0c\u901a\u8fc7\u5176\u80fd\u529b\u6846\u67b6\uff0c\u53ef\u6709\u6548\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u7684\u6210\u679c\uff0c\u8f6c\u5316\u751f\u6210AI\u7684\u8f93\u51fa\u4e3a\u53ef\u7528\u7684\u77e5\u8bc6\u3002"}}
{"id": "2510.07749", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07749", "abs": "https://arxiv.org/abs/2510.07749", "authors": ["Alexandre Moreira Nascimento", "Gabriel Kenji Godoy Shimanuki", "L\u00facio Flavio Vismari", "Jo\u00e3o Batista Camargo Jr", "Jorge Rady de Almeida Jr", "Paulo Sergio Cugnasca", "Anna Carolina Muller Queiroz", "Jeremy Noah Bailenson"], "title": "Injecting Hallucinations in Autonomous Vehicles: A Component-Agnostic Safety Evaluation Framework", "comment": "22 pages, 15 figures, 21 tables", "summary": "Perception failures in autonomous vehicles (AV) remain a major safety concern\nbecause they are the basis for many accidents. To study how these failures\naffect safety, researchers typically inject artificial faults into hardware or\nsoftware components and observe the outcomes. However, existing fault injection\nstudies often target a single sensor or machine perception (MP) module,\nresulting in siloed frameworks that are difficult to generalize or integrate\ninto unified simulation environments. This work addresses that limitation by\nreframing perception failures as hallucinations, false perceptions that distort\nan AV situational awareness and may trigger unsafe control actions. Since\nhallucinations describe only observable effects, this abstraction enables\nanalysis independent of specific sensors or algorithms, focusing instead on how\ntheir faults manifest along the MP pipeline. Building on this concept, we\npropose a configurable, component-agnostic hallucination injection framework\nthat induces six plausible hallucination types in an iterative open-source\nsimulator. More than 18,350 simulations were executed in which hallucinations\nwere injected while AVs crossed an unsignalized transverse street with traffic.\nThe results statistically validate the framework and quantify the impact of\neach hallucination type on collisions and near misses. Certain hallucinations,\nsuch as perceptual latency and drift, significantly increase the risk of\ncollision in the scenario tested, validating the proposed paradigm can stress\nthe AV system safety. The framework offers a scalable, statistically validated,\ncomponent agnostic, and fully interoperable toolset that simplifies and\naccelerates AV safety validations, even those with novel MP architectures and\ncomponents. It can potentially reduce the time-to-market of AV and lay the\nfoundation for future research on fault tolerance, and resilient AV design.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5e7b\u89c9\u6ce8\u5165\u6846\u67b6\uff0c\u4ee5\u589e\u5f3a\u5bf9\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u5b89\u5168\u6027\u7684\u9a8c\u8bc1\u548c\u7814\u7a76\uff0c\u5c24\u5176\u662f\u5728\u611f\u77e5\u5931\u8d25\u7684\u60c5\u5883\u4e0b\u3002", "motivation": "\u7814\u7a76\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u4e2d\u7684\u611f\u77e5\u5931\u8d25\u5bf9\u5b89\u5168\u6027\u7684\u5f71\u54cd\uff0c\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u9488\u5bf9\u5355\u4e00\u4f20\u611f\u5668\u6216\u673a\u5668\u611f\u77e5\u6a21\u5757\u7684\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u53ef\u914d\u7f6e\u3001\u7ec4\u4ef6\u65e0\u5173\u7684\u5e7b\u89c9\u6ce8\u5165\u6846\u67b6\uff0c\u5728\u5f00\u6e90\u6a21\u62df\u5668\u4e2d\u8fed\u4ee3\u8bf1\u53d1\u516d\u79cd\u53ef\u884c\u7684\u5e7b\u89c9\u7c7b\u578b\u3002", "result": "\u8be5\u6846\u67b6\u7ecf\u8fc7\u7edf\u8ba1\u9a8c\u8bc1\uff0c\u91cf\u5316\u4e86\u6bcf\u79cd\u5e7b\u89c9\u7c7b\u578b\u5bf9\u78b0\u649e\u548c\u63a5\u8fd1\u4e8b\u6545\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u67d0\u4e9b\u5e7b\u89c9\u663e\u8457\u589e\u52a0\u78b0\u649e\u98ce\u9669\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u7ecf\u8fc7\u7edf\u8ba1\u9a8c\u8bc1\u7684\u5de5\u5177\uff0c\u80fd\u591f\u52a0\u901f\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u7684\u5b89\u5168\u9a8c\u8bc1\uff0c\u4e3a\u672a\u6765\u7684\u6545\u969c\u5bb9\u5fcd\u6027\u548c\u97e7\u6027\u8bbe\u8ba1\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2510.07960", "categories": ["cs.HC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07960", "abs": "https://arxiv.org/abs/2510.07960", "authors": ["Emilio Estevan", "Mar\u00eda Sierra-Torralba", "Eduardo L\u00f3pez-Larraz", "Luis Montesano"], "title": "A Systematic Evaluation of Self-Supervised Learning for Label-Efficient Sleep Staging with Wearable EEG", "comment": "12 pages, 4 figures", "summary": "Wearable EEG devices have emerged as a promising alternative to\npolysomnography (PSG). As affordable and scalable solutions, their widespread\nadoption results in the collection of massive volumes of unlabeled data that\ncannot be analyzed by clinicians at scale. Meanwhile, the recent success of\ndeep learning for sleep scoring has relied on large annotated datasets.\nSelf-supervised learning (SSL) offers an opportunity to bridge this gap,\nleveraging unlabeled signals to address label scarcity and reduce annotation\neffort. In this paper, we present the first systematic evaluation of SSL for\nsleep staging using wearable EEG. We investigate a range of well-established\nSSL methods and evaluate them on two sleep databases acquired with the Ikon\nSleep wearable EEG headband: BOAS, a high-quality benchmark containing PSG and\nwearable EEG recordings with consensus labels, and HOGAR, a large collection of\nhome-based, self-recorded, and unlabeled recordings. Three evaluation scenarios\nare defined to study label efficiency, representation quality, and\ncross-dataset generalization. Results show that SSL consistently improves\nclassification performance by up to 10% over supervised baselines, with gains\nparticularly evident when labeled data is scarce. SSL achieves clinical-grade\naccuracy above 80% leveraging only 5% to 10% of labeled data, while the\nsupervised approach requires twice the labels. Additionally, SSL\nrepresentations prove robust to variations in population characteristics,\nrecording environments, and signal quality. Our findings demonstrate the\npotential of SSL to enable label-efficient sleep staging with wearable EEG,\nreducing reliance on manual annotations and advancing the development of\naffordable sleep monitoring systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u53ef\u7a7f\u6234EEG\u7761\u7720\u5206\u671f\u4e2d\u7684\u5e94\u7528\uff0c\u663e\u793a\u5176\u5728\u6570\u636e\u6807\u6ce8\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u6709\u6548\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u63a8\u52a8\u7761\u7720\u76d1\u6d4b\u7cfb\u7edf\u7684\u7ecf\u6d4e\u5316\u3002", "motivation": "\u9488\u5bf9\u53ef\u7a7f\u6234EEG\u8bbe\u5907\u4ea7\u751f\u7684\u5927\u91cf\u672a\u6807\u6ce8\u6570\u636e\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u89e3\u51b3\u6807\u6ce8\u7a00\u7f3a\u95ee\u9898\uff0c\u51cf\u5c11\u6807\u6ce8\u5de5\u4f5c\u91cf\u3002", "method": "\u91c7\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5bf9\u53ef\u7a7f\u6234EEG\u6570\u636e\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\uff0c\u5e76\u5728\u4e0d\u540c\u7684\u7761\u7720\u6570\u636e\u5e93\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "SSL\u65b9\u6cd5\u5728\u5206\u7c7b\u8868\u73b0\u4e0a\u63d0\u5347\u4e86\u6700\u9ad810%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5728\u4ec5\u4f7f\u75285%\u81f310%\u7684\u5df2\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u8fbe\u5230\u4e8680%\u4ee5\u4e0a\u7684\u4e34\u5e8a\u7ea7\u51c6\u786e\u7387\u3002", "conclusion": "\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u5728\u53ef\u7a7f\u6234EEG\u4e2d\u5c55\u793a\u4e86\u6709\u6548\u7684\u7761\u7720\u5206\u671f\u80fd\u529b\uff0c\u80fd\u591f\u5728\u8f83\u5c11\u6807\u6ce8\u6570\u636e\u7684\u57fa\u7840\u4e0a\u5b9e\u73b0\u4e34\u5e8a\u7ea7\u51c6\u786e\u6027\uff0c\u5e76\u63a8\u52a8\u4e86\u7ecf\u6d4e\u5b9e\u60e0\u7684\u7761\u7720\u76d1\u6d4b\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.07773", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07773", "abs": "https://arxiv.org/abs/2510.07773", "authors": ["YuHang Tang", "Yixuan Lou", "Pengfei Han", "Haoming Song", "Xinyi Ye", "Dong Wang", "Bin Zhao"], "title": "Trajectory Conditioned Cross-embodiment Skill Transfer", "comment": null, "summary": "Learning manipulation skills from human demonstration videos presents a\npromising yet challenging problem, primarily due to the significant embodiment\ngap between human body and robot manipulators. Existing methods rely on paired\ndatasets or hand-crafted rewards, which limit scalability and generalization.\nWe propose TrajSkill, a framework for Trajectory Conditioned Cross-embodiment\nSkill Transfer, enabling robots to acquire manipulation skills directly from\nhuman demonstration videos. Our key insight is to represent human motions as\nsparse optical flow trajectories, which serve as embodiment-agnostic motion\ncues by removing morphological variations while preserving essential dynamics.\nConditioned on these trajectories together with visual and textual inputs,\nTrajSkill jointly synthesizes temporally consistent robot manipulation videos\nand translates them into executable actions, thereby achieving cross-embodiment\nskill transfer. Extensive experiments are conducted, and the results on\nsimulation data (MetaWorld) show that TrajSkill reduces FVD by 39.6\\% and KVD\nby 36.6\\% compared with the state-of-the-art, and improves cross-embodiment\nsuccess rate by up to 16.7\\%. Real-robot experiments in kitchen manipulation\ntasks further validate the effectiveness of our approach, demonstrating\npractical human-to-robot skill transfer across embodiments.", "AI": {"tldr": "TrajSkill\u662f\u4e00\u79cd\u8de8\u4f53\u73b0\u6280\u80fd\u8f6c\u79fb\u6846\u67b6\uff0c\u901a\u8fc7\u7a00\u758f\u5149\u6d41\u8f68\u8ff9\u6574\u5408\u4eba\u7c7b\u64cd\u63a7\u6280\u80fd\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u6548\u7387\u53ca\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u4eba\u7c7b\u793a\u8303\u89c6\u9891\u4e0e\u673a\u5668\u4eba\u64cd\u4f5c\u4e4b\u95f4\u7684\u4f53\u73b0\u5dee\u8ddd\uff0c\u65e8\u5728\u63d0\u9ad8\u6280\u80fd\u8fc1\u79fb\u7684\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u4f7f\u7528\u7a00\u758f\u5149\u6d41\u8f68\u8ff9\u6765\u8868\u793a\u4eba\u7c7b\u52a8\u4f5c\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u8f93\u5165\uff0c\u5408\u6210\u673a\u5668\u4eba\u64cd\u63a7\u89c6\u9891\u5e76\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u52a8\u4f5c\u3002", "result": "\u5728MetaWorld\u4eff\u771f\u6570\u636e\u4e2d\uff0cTrajSkill\u76f8\u6bd4\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6280\u672f\u51cf\u5c11\u4e8639.6%\u7684FVD\u548c36.6%\u7684KVD\uff0c\u4e14\u63d0\u5347\u4e86\u8de8\u4f53\u73b0\u6210\u529f\u7387\u8fbe16.7%\u3002\u5728\u53a8\u623f\u64cd\u63a7\u4efb\u52a1\u7684\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "TrajSkill\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u4eba\u7c7b\u5230\u673a\u5668\u4eba\u6280\u80fd\u8f6c\u79fb\uff0c\u7279\u522b\u662f\u5728\u4e0d\u540c\u8eab\u4f53\u7ed3\u6784\u95f4\u7684\u8fc1\u79fb\u4e0a\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.07967", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.07967", "abs": "https://arxiv.org/abs/2510.07967", "authors": ["Yaning Li", "Ke Zhao", "Shucheng Zheng", "Xingyu Chen", "Chenyi Chen", "Wenxi Dai", "Weile Jiang", "Qi Dong", "Yiqing Zhao", "Meng Li", "Lin-Ping Yuan"], "title": "Pre/Absence: Prompting Cultural Awareness and Understanding for Lost Architectural Heritage in Virtual Reality", "comment": null, "summary": "Lost architectural heritage presents interpretive challenges due to vanished\nstructures and fragmented historical records. Using Hanyuan Hall of the Tang\ndynasty's Daming Palace as a case study, we conducted a formative investigation\nwith archaeologists, heritage administrators, and visitors to identify key\nissues in current interpretation practices. We found that these practices often\ncompress complex cultural layers into factual summaries and rely on linear\nnarratives that overlook the continuing reinterpretations following a site's\ndisappearance. In response, we designed Pre/Absence, a virtual reality\nexperience grounded in the presence-absence dialectic to interweave tangible\nand vanished aspects of heritage within a spatiotemporal narrative. A\nmixed-method study with 28 participants compared Pre/Absence to a paper-based\nexperience. Both improved users' factual understanding, but the VR experience\nmore strongly enhanced cultural awareness, evoked emotional engagement with\nloss, and encouraged critical reflection on the evolving social and political\nmeanings of heritage. The findings suggest that VR can move beyond static\nreconstruction to engage users as co-constructors of cultural meaning,\nproviding a nuanced framework for critical heritage narrative design in\nhuman-computer interaction.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u865a\u62df\u73b0\u5b9e\u4f53\u9a8c\u63a2\u8ba8\u4e86\u5931\u843d\u5efa\u7b51\u9057\u4ea7\u7684\u89e3\u8bfb\u6311\u6218\uff0c\u53d1\u73b0\u8fd9\u79cd\u65b9\u5f0f\u80fd\u591f\u589e\u5f3a\u6587\u5316\u610f\u8bc6\u548c\u60c5\u611f\u53c2\u4e0e\uff0c\u9f13\u52b1\u5bf9\u9057\u4ea7\u610f\u4e49\u7684\u6279\u5224\u6027\u53cd\u601d\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5931\u843d\u5efa\u7b51\u9057\u4ea7\u7684\u89e3\u8bfb\u6311\u6218\uff0c\u63d0\u51fa\u65b0\u7684\u65b9\u6cd5\u4ee5\u66f4\u5168\u9762\u3001\u66f4\u6df1\u5165\u5730\u7406\u89e3\u548c\u4f53\u9a8c\u6587\u5316\u9057\u4ea7\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\uff0c\u4e0e28\u540d\u53c2\u4e0e\u8005\u6bd4\u8f83\u4e86\u865a\u62df\u73b0\u5b9e\u4f53\u9a8c\u4e0e\u4f20\u7edf\u7eb8\u8d28\u4f53\u9a8c\u7684\u6548\u679c\u3002", "result": "\u672c\u7814\u7a76\u56f4\u7ed5\u5510\u4ee3\u5927\u660e\u5bab\u7684\u6c49\u962e\u6bbf\uff0c\u901a\u8fc7\u8003\u53e4\u5b66\u5bb6\u3001\u9057\u4ea7\u7ba1\u7406\u8005\u548c\u6e38\u5ba2\u7684\u89c6\u89d2\uff0c\u63a2\u8ba8\u4e86\u5982\u4f55\u5904\u7406\u5931\u843d\u7684\u5efa\u7b51\u9057\u4ea7\u53ca\u5176\u89e3\u8bfb\u4e2d\u7684\u6311\u6218\u3002\u6211\u4eec\u53d1\u73b0\u4f20\u7edf\u7684\u89e3\u8bfb\u65b9\u6cd5\u5f80\u5f80\u5c06\u590d\u6742\u7684\u6587\u5316\u5c42\u6b21\u7b80\u5316\u4e3a\u7b80\u5355\u4e8b\u5b9e\uff0c\u5e76\u5ffd\u7565\u4e86\u9057\u5740\u6d88\u5931\u540e\u4e0d\u65ad\u7684\u518d\u89e3\u91ca\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540d\u4e3a\u201c\u5728\u524d/\u7f3a\u5931\u201d\u7684\u865a\u62df\u73b0\u5b9e\u4f53\u9a8c\uff0c\u4ee5\u5728\u65f6\u7a7a\u53d9\u4e8b\u4e2d\u4e92\u7ec7\u5b9e\u9645\u5b58\u5728\u4e0e\u6d88\u5931\u7684\u9057\u4ea7\u3002\u901a\u8fc7\u4e0e\u4f20\u7edf\u7eb8\u8d28\u4f53\u9a8c\u7684\u5bf9\u6bd4\uff0c\u6211\u4eec\u7684\u6df7\u5408\u7814\u7a76\u65b9\u6cd5\u663e\u793a\u51fa\uff0c\u5c3d\u7ba1\u4e24\u8005\u5747\u63d0\u5347\u4e86\u7528\u6237\u7684\u4e8b\u5b9e\u7406\u89e3\uff0c\u4f46\u865a\u62df\u73b0\u5b9e\u4f53\u9a8c\u66f4\u5f3a\u70c8\u5730\u589e\u5f3a\u4e86\u6587\u5316\u610f\u8bc6\uff0c\u5f15\u53d1\u4e86\u5bf9\u5931\u843d\u7684\u60c5\u611f\u5171\u9e23\uff0c\u5e76\u9f13\u52b1\u4e86\u5bf9\u9057\u4ea7\u4e0d\u65ad\u6f14\u53d8\u7684\u793e\u4f1a\u548c\u653f\u6cbb\u610f\u4e49\u7684\u6279\u5224\u6027\u53cd\u601d\u3002\u7ed3\u679c\u8868\u660e\uff0c\u865a\u62df\u73b0\u5b9e\u53ef\u4ee5\u8d85\u8d8a\u9759\u6001\u91cd\u6784\uff0c\u9f13\u52b1\u7528\u6237\u4f5c\u4e3a\u6587\u5316\u610f\u4e49\u7684\u5171\u540c\u6784\u5efa\u8005\uff0c\u4ece\u800c\u4e3a\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u6279\u5224\u6027\u9057\u4ea7\u53d9\u4e8b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7ec6\u81f4\u7684\u6846\u67b6\u3002", "conclusion": "\u865a\u62df\u73b0\u5b9e\u4e0d\u4ec5\u6539\u5584\u4e86\u7528\u6237\u7684\u4e8b\u5b9e\u7406\u89e3\uff0c\u8fd8\u80fd\u66f4\u6709\u6548\u5730\u63d0\u5347\u6587\u5316\u610f\u8bc6\u548c\u60c5\u611f\u5171\u9e23\uff0c\u4e3a\u6279\u5224\u6027\u7684\u9057\u4ea7\u53d9\u4e8b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2510.07778", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07778", "abs": "https://arxiv.org/abs/2510.07778", "authors": ["Yandu Chen", "Kefan Gu", "Yuqing Wen", "Yucheng Zhao", "Tiancai Wang", "Liqiang Nie"], "title": "IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction", "comment": null, "summary": "Vision-Language-Action (VLA) models leverage pretrained vision-language\nmodels (VLMs) to couple perception with robotic control, offering a promising\npath toward general-purpose embodied intelligence. However, current SOTA VLAs\nare primarily pretrained on multimodal tasks with limited relevance to embodied\nscenarios, and then finetuned to map explicit instructions to actions.\nConsequently, due to the lack of reasoning-intensive pretraining and\nreasoning-guided manipulation, these models are unable to perform implicit\nhuman intention reasoning required for complex, real-world interactions. To\novercome these limitations, we propose \\textbf{IntentionVLA}, a VLA framework\nwith a curriculum training paradigm and an efficient inference mechanism. Our\nproposed method first leverages carefully designed reasoning data that combine\nintention inference, spatial grounding, and compact embodied reasoning,\nendowing the model with both reasoning and perception capabilities. In the\nfollowing finetuning stage, IntentionVLA employs the compact reasoning outputs\nas contextual guidance for action generation, enabling fast inference under\nindirect instructions. Experimental results show that IntentionVLA\nsubstantially outperforms $\\pi_0$, achieving 18\\% higher success rates with\ndirect instructions and 28\\% higher than ECoT under intention instructions. On\nout-of-distribution intention tasks, IntentionVLA achieves over twice the\nsuccess rate of all baselines, and further enables zero-shot human-robot\ninteraction with 40\\% success rate. These results highlight IntentionVLA as a\npromising paradigm for next-generation human-robot interaction (HRI) systems.", "AI": {"tldr": "IntentionVLA \u662f\u4e00\u79cd\u65b0\u9896\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u8bfe\u7a0b\u8bad\u7ec3\u548c\u9ad8\u6548\u63a8\u7406\u673a\u5236\uff0c\u589e\u5f3a\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u4eba\u7c7b\u610f\u56fe\u7406\u89e3\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4eba\u7c7b-\u673a\u5668\u4eba\u4ea4\u4e92\u7684\u6210\u529f\u7387\u3002", "motivation": "\u5f53\u524d\u7684 SOTA VLA \u6a21\u578b\u5728\u590d\u6742\u771f\u5b9e\u573a\u666f\u4e2d\u7f3a\u4e4f\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u6cd5\u5904\u7406\u9690\u542b\u7684\u4eba\u7c7b\u610f\u56fe\u63a8\u7406\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "IntentionVLA\u91c7\u7528\u8bfe\u7a0b\u8bad\u7ec3\u8303\u5f0f\uff0c\u6574\u5408\u4e86\u610f\u56fe\u63a8\u7406\u3001\u7a7a\u95f4\u5b9a\u4f4d\u548c\u7d27\u51d1\u7684\u5177\u8eab\u63a8\u7406\uff0c\u63d0\u5347\u6a21\u578b\u7684\u63a8\u7406\u548c\u611f\u77e5\u80fd\u529b\u3002", "result": "IntentionVLA\u5728\u76f4\u63a5\u6307\u4ee4\u4e0b\u6210\u529f\u7387\u63d0\u9ad8\u4e8618%\uff0c\u610f\u56fe\u6307\u4ee4\u4e0b\u63d0\u9ad8\u4e8628%\uff0c\u5728\u65e0\u76d1\u7763\u610f\u56fe\u4efb\u52a1\u4e0a\u6210\u529f\u7387\u662f\u6240\u6709\u57fa\u7ebf\u7684\u4e24\u500d\uff0c\u4e14\u5b9e\u73b0\u4e8640%\u7684\u96f6-shot \u4eba\u673a\u4ea4\u4e92\u6210\u529f\u7387\u3002", "conclusion": "IntentionVLA \u662f\u4e00\u79cd\u65b0\u5174\u7684 VLA \u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u7c7b-\u673a\u5668\u4eba\u4ea4\u4e92\u7684\u6210\u529f\u7387\uff0c\u5c24\u5176\u5728\u65e0\u76d1\u7763\u7684\u610f\u56fe\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.07987", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.07987", "abs": "https://arxiv.org/abs/2510.07987", "authors": ["Rachel L. Franz", "Jacob O. Wobbrock"], "title": "Quantifying Locomotion Differences Between Virtual Reality Users With and Without Motor Impairments", "comment": "34 pages, 24 figures", "summary": "Today's virtual reality (VR) systems and environments assume that users have\ntypical abilities, which can make VR inaccessible to people with physical\nimpairments. However, there is not yet an understanding of how inaccessible\nlocomotion techniques are, and which interactions make them inaccessible. To\nthis end, we conducted a study in which people with and without upper-body\nimpairments navigated a virtual environment with six locomotion techniques to\nquantify performance differences among groups. We found that groups performed\nsimilarly with Sliding Looking on all performance measures, suggesting that\nthis might be a good default locomotion technique for VR apps. To understand\nthe nature of performance differences with the other techniques, we collected\nlow-level interaction data from the controllers and headset and analyzed\ninteraction differences with a set of movement-, button-, and target-related\nmetrics. We found that movement-related metrics from headset data reveal\ndifferences among groups with all techniques, suggesting these are good metrics\nfor identifying whether a user has an upper-body impairment. We also identify\nmovement-, button, and target-related metrics that can explain performance\ndifferences between groups for particular locomotion techniques.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u865a\u62df\u73b0\u5b9e\u4e2d\u7684\u79fb\u52a8\u6280\u672f\u5bf9\u8eab\u4f53\u969c\u788d\u8005\u7684\u53ef\u8fbe\u6027\uff0c\u53d1\u73b0'\u6ed1\u52a8\u67e5\u770b'\u6280\u672f\u8868\u73b0\u4f18\u8d8a\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e9b\u53ef\u4ee5\u7528\u4e8e\u8bc6\u522b\u8eab\u4f53\u969c\u788d\u7684\u5ea6\u91cf\u6307\u6807\u3002", "motivation": "\u4e86\u89e3\u865a\u62df\u73b0\u5b9e\u4e2d\u7684\u79fb\u52a8\u6280\u672f\u5bf9\u8eab\u4f53\u6709\u969c\u788d\u7528\u6237\u7684\u53ef\u8fbe\u6027\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u8ba9\u8eab\u4f53\u6709\u969c\u788d\u548c\u6ca1\u6709\u969c\u788d\u7684\u4eba\u5728\u865a\u62df\u73af\u5883\u4e2d\u4f7f\u7528\u516d\u79cd\u79fb\u52a8\u6280\u672f\u8fdb\u884c\u5bfc\u822a\uff0c\u6536\u96c6\u5e76\u5206\u6790\u4f4e\u5c42\u4ea4\u4e92\u6570\u636e\u3002", "result": "\u901a\u8fc7\u91cf\u5316\u8eab\u4f53\u969c\u788d\u8005\u4e0e\u6b63\u5e38\u7528\u6237\u5728\u516d\u79cd\u79fb\u52a8\u6280\u672f\u4e0b\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u53d1\u73b0\u67d0\u4e9b\u6280\u672f\u53ef\u80fd\u66f4\u9002\u5408\u6240\u6709\u7528\u6237\u3002", "conclusion": "'\u6ed1\u52a8\u67e5\u770b'\u53ef\u80fd\u662f\u865a\u62df\u73b0\u5b9e\u5e94\u7528\u7684\u826f\u597d\u9ed8\u8ba4\u79fb\u52a8\u6280\u672f\uff0c\u5e76\u4e14\u901a\u8fc7\u4f4e\u7ea7\u4ea4\u4e92\u6570\u636e\u53ef\u4ee5\u6df1\u5165\u5206\u6790\u7528\u6237\u8868\u73b0\u7684\u5dee\u5f02\u3002"}}
{"id": "2510.07807", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07807", "abs": "https://arxiv.org/abs/2510.07807", "authors": ["Grace Cai", "Nithin Parepally", "Laura Zheng", "Ming C. Lin"], "title": "GM3: A General Physical Model for Micro-Mobility Vehicles", "comment": null, "summary": "Modeling the dynamics of micro-mobility vehicles (MMV) is becoming\nincreasingly important for training autonomous vehicle systems and building\nurban traffic simulations. However, mainstream tools rely on variants of the\nKinematic Bicycle Model (KBM) or mode-specific physics that miss tire slip,\nload transfer, and rider/vehicle lean. To our knowledge, no unified,\nphysics-based model captures these dynamics across the full range of common\nMMVs and wheel layouts. We propose the \"Generalized Micro-mobility Model\"\n(GM3), a tire-level formulation based on the tire brush representation that\nsupports arbitrary wheel configurations, including single/double track and\nmulti-wheel platforms. We introduce an interactive model-agnostic simulation\nframework that decouples vehicle/layout specification from dynamics to compare\nthe GM3 with the KBM and other models, consisting of fixed step RK4\nintegration, human-in-the-loop and scripted control, real-time trajectory\ntraces and logging for analysis. We also empirically validate the GM3 on the\nStanford Drone Dataset's deathCircle (roundabout) scene for biker, skater, and\ncart classes.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u6a21\u578bGM3\uff0c\u4ee5\u66f4\u597d\u5730\u6a21\u62df\u5fae\u51fa\u884c\u8f66\u8f86\u7684\u52a8\u529b\u5b66\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u6355\u6349\u5fae\u51fa\u884c\u8f66\u8f86\u7684\u52a8\u6001\u7279\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u66f4\u5168\u9762\u7684\u52a8\u529b\u5b66\u6a21\u578b\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u8f6e\u80ce\u5237\u8868\u793a\u7684\u8f6e\u80ce\u7ea7\u522b\u516c\u5f0f\uff0c\u5e76\u5efa\u7acb\u4e86\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u4ea4\u4e92\u5f0f\u4eff\u771f\u6846\u67b6\u3002", "result": "\u901a\u8fc7\u4e0eKBM\u548c\u5176\u4ed6\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\uff0cGM3\u5728\u65af\u5766\u798f\u65e0\u4eba\u673a\u6570\u636e\u96c6\u4e2d\u5f97\u5230\u4e86\u7ecf\u9a8c\u9a8c\u8bc1\u3002", "conclusion": "GM3\u4e3a\u5fae\u51fa\u884c\u8f66\u8f86\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u3001\u57fa\u4e8e\u7269\u7406\u7684\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u80fd\u591f\u8de8\u8d8a\u4e0d\u540c\u7684\u8f66\u8f86\u914d\u7f6e\u8fdb\u884c\u7cbe\u51c6\u6a21\u62df\u3002"}}
{"id": "2510.08104", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08104", "abs": "https://arxiv.org/abs/2510.08104", "authors": ["Joshua Holstein", "Gerhard Satzger"], "title": "Development of Mental Models in Human-AI Collaboration: A Conceptual Framework", "comment": "Preprint version. Accepted for presentation at the International\n  Conference on Information Systems (ICIS 2025). Please cite the published\n  version when available", "summary": "Artificial intelligence has become integral to organizational decision-making\nand while research has explored many facets of this human-AI collaboration, the\nfocus has mainly been on designing the AI agent(s) and the way the\ncollaboration is set up - generally assuming a human decision-maker to be\n\"fixed\". However, it has largely been neglected that decision-makers' mental\nmodels evolve through their continuous interaction with AI systems. This paper\naddresses this gap by conceptualizing how the design of human-AI collaboration\ninfluences the development of three complementary and interdependent mental\nmodels necessary for this collaboration. We develop an integrated\nsocio-technical framework that identifies the mechanisms driving the mental\nmodel evolution: data contextualization, reasoning transparency, and\nperformance feedback. Our work advances human-AI collaboration literature\nthrough three key contributions: introducing three distinct mental models\n(domain, information processing, complementarity-awareness); recognizing the\ndynamic nature of mental models; and establishing mechanisms that guide the\npurposeful design of effective human-AI collaboration.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4eba\u7c7b\u4e0eAI\u7684\u5408\u4f5c\u5982\u4f55\u5f71\u54cd\u51b3\u7b56\u8005\u5fc3\u667a\u6a21\u578b\u7684\u6f14\u53d8\uff0c\u63d0\u51fa\u4e86\u8bbe\u8ba1\u6709\u6548\u5408\u4f5c\u7684\u673a\u5236", "motivation": "\u63a2\u8ba8\u4eba\u7c7b\u4e0e\u4eba\u5de5\u667a\u80fd\u7684\u5408\u4f5c\u4e2d\u51b3\u7b56\u8005\u5fc3\u667a\u6a21\u578b\u7684\u6f14\u53d8\u4ee5\u53ca\u5176\u8bbe\u8ba1\u5f71\u54cd", "method": "\u6784\u5efa\u4e00\u4e2a\u7efc\u5408\u7684\u793e\u4f1a\u6280\u672f\u6846\u67b6", "result": "\u63d0\u51fa\u4e09\u4e2a\u4e92\u8865\u4e14\u76f8\u4e92\u4f9d\u8d56\u7684\u5fc3\u667a\u6a21\u578b\uff0c\u5e76\u8bc6\u522b\u5f71\u54cd\u5176\u53d1\u5c55\u7684\u673a\u5236", "conclusion": "\u672c\u6587\u5f3a\u8c03\u4e86\u5fc3\u667a\u6a21\u578b\u7684\u52a8\u6001\u7279\u6027\u53ca\u5176\u5728\u6709\u6548\u4eba\u673a\u534f\u4f5c\u8bbe\u8ba1\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.07865", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07865", "abs": "https://arxiv.org/abs/2510.07865", "authors": ["Guowei Zou", "Haitao Wang", "Hejun Wu", "Yukun Qian", "Yuhang Wang", "Weibing Li"], "title": "DM1: MeanFlow with Dispersive Regularization for 1-Step Robotic Manipulation", "comment": "Website with code: https://guowei-zou.github.io/dm1/", "summary": "The ability to learn multi-modal action distributions is indispensable for\nrobotic manipulation policies to perform precise and robust control. Flow-based\ngenerative models have recently emerged as a promising solution to learning\ndistributions of actions, offering one-step action generation and thus\nachieving much higher sampling efficiency compared to diffusion-based methods.\nHowever, existing flow-based policies suffer from representation collapse, the\ninability to distinguish similar visual representations, leading to failures in\nprecise manipulation tasks. We propose DM1 (MeanFlow with Dispersive\nRegularization for One-Step Robotic Manipulation), a novel flow matching\nframework that integrates dispersive regularization into MeanFlow to prevent\ncollapse while maintaining one-step efficiency. DM1 employs multiple dispersive\nregularization variants across different intermediate embedding layers,\nencouraging diverse representations across training batches without introducing\nadditional network modules or specialized training procedures. Experiments on\nRoboMimic benchmarks show that DM1 achieves 20-40 times faster inference (0.07s\nvs. 2-3.5s) and improves success rates by 10-20 percentage points, with the\nLift task reaching 99% success over 85% of the baseline. Real-robot deployment\non a Franka Panda further validates that DM1 transfers effectively from\nsimulation to the physical world. To the best of our knowledge, this is the\nfirst work to leverage representation regularization to enable flow-based\npolicies to achieve strong performance in robotic manipulation, establishing a\nsimple yet powerful approach for efficient and robust manipulation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6d41\u5339\u914d\u6846\u67b6DM1\uff0c\u5229\u7528\u5206\u6563\u6b63\u5219\u5316\u6765\u9632\u6b62\u8868\u793a\u5d29\u6e83\uff0c\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6548\u679c\u4e0e\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6d41\u7684\u7b56\u7565\u9762\u4e34\u8868\u793a\u5d29\u6e83\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u7cbe\u786e\u64cd\u4f5c\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u6765\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "method": "DM1\u7ed3\u5408\u4e86\u5206\u6563\u6b63\u5219\u5316\u4e0eMeanFlow\uff0c\u91c7\u7528\u591a\u4e2a\u6b63\u5219\u5316\u53d8\u4f53\u4ee5\u9f13\u52b1\u8bad\u7ec3\u6279\u6b21\u95f4\u7684\u591a\u6837\u5316\u8868\u793a\u3002", "result": "DM1\u5728RoboMimic\u57fa\u51c6\u4e0a\u6bd4\u57fa\u7ebf\u63d0\u9ad8\u4e8610-20\u4e2a\u767e\u5206\u70b9\u7684\u6210\u529f\u7387\uff0c\u5e76\u5728Lift\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e8699%\u7684\u6210\u529f\u7387\u3002", "conclusion": "DM1\u901a\u8fc7\u5206\u6563\u6b63\u5219\u5316\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u7a33\u5065\u7684\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u8bc1\u660e\u4e86\u4ece\u6a21\u62df\u5230\u5b9e\u9645\u5e94\u7528\u7684\u6709\u6548\u8f6c\u79fb\u3002"}}
{"id": "2510.08202", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.ET"], "pdf": "https://arxiv.org/pdf/2510.08202", "abs": "https://arxiv.org/abs/2510.08202", "authors": ["Lirui Guo", "Michael G. Burke", "Wynita M. Griggs"], "title": "Sentiment Matters: An Analysis of 200 Human-SAV Interactions", "comment": "Accepted for presentation at IEEE ITSC 2025 and for publication in\n  its Proceedings. \\c{opyright} 2025 IEEE. Personal use permitted; other uses\n  require permission from IEEE, including reprinting, republishing, or reuse of\n  any copyrighted component of this work", "summary": "Shared Autonomous Vehicles (SAVs) are likely to become an important part of\nthe transportation system, making effective human-SAV interactions an important\narea of research. This paper introduces a dataset of 200 human-SAV interactions\nto further this area of study. We present an open-source human-SAV\nconversational dataset, comprising both textual data (e.g., 2,136 human-SAV\nexchanges) and empirical data (e.g., post-interaction survey results on a range\nof psychological factors). The dataset's utility is demonstrated through two\nbenchmark case studies: First, using random forest modeling and chord diagrams,\nwe identify key predictors of SAV acceptance and perceived service quality,\nhighlighting the critical influence of response sentiment polarity (i.e.,\nperceived positivity). Second, we benchmark the performance of an LLM-based\nsentiment analysis tool against the traditional lexicon-based TextBlob method.\nResults indicate that even simple zero-shot LLM prompts more closely align with\nuser-reported sentiment, though limitations remain. This study provides novel\ninsights for designing conversational SAV interfaces and establishes a\nfoundation for further exploration into advanced sentiment modeling, adaptive\nuser interactions, and multimodal conversational systems.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5305\u542b200\u4e2aSAV\u4ea4\u4e92\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728SAV\u63a5\u53d7\u5ea6\u548c\u60c5\u611f\u5206\u6790\u7684\u5e94\u7528\u3002", "motivation": "\u968f\u7740\u5171\u4eab\u81ea\u4e3b\u8f66\u8f86\uff08SAV\uff09\u6210\u4e3a\u4ea4\u901a\u7cfb\u7edf\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u6709\u6548\u7684\u4eba\u7c7b\u4e0eSAV\u4e4b\u95f4\u7684\u4ea4\u4e92\u7814\u7a76\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5305\u542b200\u4e2a\u4eba\u7c7b\u4e0e\u5171\u4eab\u81ea\u4e3b\u8f66\u8f86\uff08SAV\uff09\u4ea4\u4e92\u7684\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u57fa\u51c6\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6548\u7528\u3002", "result": "\u8bc6\u522b\u51fa\u5f71\u54cdSAV\u63a5\u53d7\u5ea6\u548c\u611f\u77e5\u670d\u52a1\u8d28\u91cf\u7684\u5173\u952e\u9884\u6d4b\u56e0\u7d20\uff0c\u5e76\u6bd4\u8f83\u4e86\u4ee5LLM\u4e3a\u57fa\u7840\u7684\u60c5\u611f\u5206\u6790\u5de5\u5177\u4e0e\u4f20\u7edf\u6587\u672c\u5206\u6790\u65b9\u6cd5TextBlob\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u8bbe\u8ba1\u5bf9\u8bdd\u5f0fSAV\u754c\u9762\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u5e76\u4e3a\u8fdb\u4e00\u6b65\u63a2\u7d22\u9ad8\u7ea7\u60c5\u611f\u5efa\u6a21\u3001\u81ea\u9002\u5e94\u7528\u6237\u4ea4\u4e92\u548c\u591a\u6a21\u6001\u5bf9\u8bdd\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.07869", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07869", "abs": "https://arxiv.org/abs/2510.07869", "authors": ["Junwen Gu", "Zhiheng wu", "Pengxuan Si", "Shuang Qiu", "Yukai Feng", "Luoyang Sun", "Laien Luo", "Lianyi Yu", "Jian Wang", "Zhengxing Wu"], "title": "USIM and U0: A Vision-Language-Action Dataset and Model for General Underwater Robots", "comment": null, "summary": "Underwater environments present unique challenges for robotic operation,\nincluding complex hydrodynamics, limited visibility, and constrained\ncommunication. Although data-driven approaches have advanced embodied\nintelligence in terrestrial robots and enabled task-specific autonomous\nunderwater robots, developing underwater intelligence capable of autonomously\nperforming multiple tasks remains highly challenging, as large-scale,\nhigh-quality underwater datasets are still scarce. To address these\nlimitations, we introduce USIM, a simulation-based multi-task\nVision-Language-Action (VLA) dataset for underwater robots. USIM comprises over\n561K frames from 1,852 trajectories, totaling approximately 15.6 hours of\nBlueROV2 interactions across 20 tasks in 9 diverse scenarios, ranging from\nvisual navigation to mobile manipulation. Building upon this dataset, we\npropose U0, a VLA model for general underwater robots, which integrates\nbinocular vision and other sensor modalities through multimodal fusion, and\nfurther incorporates a convolution-attention-based perception focus enhancement\nmodule (CAP) to improve spatial understanding and mobile manipulation. Across\ntasks such as inspection, obstacle avoidance, scanning, and dynamic tracking,\nthe framework achieves a success rate of 80%, while in challenging mobile\nmanipulation tasks, it reduces the distance to the target by 21.2% compared\nwith baseline methods, demonstrating its effectiveness. USIM and U0 show that\nVLA models can be effectively applied to underwater robotic applications,\nproviding a foundation for scalable dataset construction, improved task\nautonomy, and the practical realization of intelligent general underwater\nrobots.", "AI": {"tldr": "\u672c\u8bba\u6587\u4ecb\u7ecd\u4e86USIM\u6570\u636e\u96c6\u53ca\u5176\u4e0a\u7684U0\u6a21\u578b\uff0c\u4ee5\u514b\u670d\u6c34\u4e0b\u673a\u5668\u4eba\u5728\u591a\u4efb\u52a1\u6267\u884c\u4e2d\u7684\u56f0\u96be\u3002", "motivation": "\u6c34\u4e0b\u73af\u5883\u5bf9\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u51fa\u4e86\u72ec\u7279\u7684\u6311\u6218\uff0c\u5305\u62ec\u590d\u6742\u7684\u6c34\u52a8\u529b\u5b66\u3001\u6709\u9650\u7684\u53ef\u89c6\u6027\u548c\u53d7\u9650\u7684\u901a\u4fe1\u3002\u4f20\u7edf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5c1a\u96be\u4ee5\u6ee1\u8db3\u591a\u4efb\u52a1\u7684\u6c34\u4e0b\u667a\u80fd\u673a\u5668\u4eba\u9700\u6c42\uff0c\u56e0\u6b64\u5f00\u53d1\u53ef\u4ee5\u81ea\u4e3b\u7ba1\u7406\u591a\u4efb\u52a1\u7684\u6c34\u4e0b\u667a\u80fd\u673a\u5668\u4eba\u4ecd\u6781\u5177\u6311\u6218\u6027\u3002", "method": "USIM\u5305\u542b561K\u5e27\u6570\u636e\uff0c\u6db5\u76d620\u4e2a\u4efb\u52a1\u4e0e9\u79cd\u573a\u666f\uff1bU0\u7ed3\u5408\u53cc\u76ee\u89c6\u89c9\u4e0e\u4f20\u611f\u5668\u878d\u5408\uff0c\u901a\u8fc7\u5377\u79ef\u6ce8\u610f\u529b\u6a21\u5757\u63d0\u5347\u7a7a\u95f4\u7406\u89e3\u548c\u79fb\u52a8\u64cd\u4f5c\u80fd\u529b\u3002", "result": "\u63d0\u51fa\u4e86USIM\uff0c\u4e00\u4e2a\u57fa\u4e8e\u4eff\u771f\u7684\u591a\u4efb\u52a1\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6570\u636e\u96c6\uff0c\u4ee5\u53caU0\uff0c\u4e00\u4e2a\u96c6\u6210\u53cc\u76ee\u89c6\u89c9\u548c\u5176\u4ed6\u4f20\u611f\u5668\u7684VLA\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u572880%\u6210\u529f\u7387\u7684\u57fa\u7840\u4e0a\uff0c\u63d0\u5347\u79fb\u52a8\u64cd\u4f5c\u7684\u6709\u6548\u6027\u3002", "conclusion": "USIM\u548cU0\u8868\u660e\uff0cVLA\u6a21\u578b\u53ef\u6709\u6548\u5e94\u7528\u4e8e\u6c34\u4e0b\u673a\u5668\u4eba\u9886\u57df\uff0c\u4e3a\u53ef\u6269\u5c55\u6570\u636e\u96c6\u5efa\u8bbe\u548c\u667a\u80fd\u673a\u5668\u4eba\u81ea\u4e3b\u6267\u884c\u4efb\u52a1\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.08227", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.08227", "abs": "https://arxiv.org/abs/2510.08227", "authors": ["Mariana Fernandez-Espinosa", "Kai Zhang", "Jad Bendarkawi", "Ashley Ponce", "Sean Chidozie Mata", "Aminah Aliu", "Lei Zhang", "Francisco Fernandez Medina", "Elena Mangione-Lora", "Andres Monroy-Hernandez", "Diego Gomez-Zara"], "title": "Practicing a Second Language Without Fear: Mixed Reality Agents for Interactive Group Conversation", "comment": "22 pages", "summary": "Developing speaking proficiency in a second language can be cognitively\ndemanding and emotionally taxing, often triggering fear of making mistakes or\nbeing excluded from larger groups. While current learning tools show promise\nfor speaking practice, most focus on dyadic, scripted scenarios, limiting\nopportunities for dynamic group interactions. To address this gap, we present\nConversAR, a Mixed Reality system that leverages Generative AI and XR to\nsupport situated and personalized group conversations. It integrates embodied\nAI agents, scene recognition, and generative 3D props anchored to real-world\nsurroundings. Based on a formative study with experts in language acquisition,\nwe developed and tested this system with a user study with 21 second-language\nlearners. Results indicate that the system enhanced learner engagement,\nincreased willingness to communicate, and offered a safe space for speaking. We\ndiscuss the implications for integrating Generative AI and XR into the design\nof future language learning applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3aConversAR\u7684\u6df7\u5408\u73b0\u5b9e\u7cfb\u7edf\uff0c\u4ee5\u589e\u5f3a\u7b2c\u4e8c\u8bed\u8a00\u5b66\u4e60\u8005\u7684\u53e3\u8bed\u80fd\u529b\uff0c\u63d0\u4f9b\u52a8\u6001\u5c0f\u7ec4\u5bf9\u8bdd\u673a\u4f1a\uff0c\u63d0\u5347\u5b66\u4e60\u8005\u7684\u53c2\u4e0e\u611f\u548c\u6c9f\u901a\u610f\u613f\u3002", "motivation": "\u5b66\u4e60\u7b2c\u4e8c\u8bed\u8a00\u7684\u53e3\u8bed\u80fd\u529b\u5f80\u5f80\u5177\u6709\u8f83\u9ad8\u7684\u8ba4\u77e5\u8d1f\u8377\u548c\u60c5\u611f\u538b\u529b\uff0c\u73b0\u6709\u5de5\u5177\u8fc7\u4e8e\u5173\u6ce8\u5bf9\u8bdd\u7684\u5355\u4e00\u573a\u666f\uff0c\u7f3a\u4e4f\u52a8\u6001\u4ea4\u4e92\uff0c\u8feb\u5207\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u6765\u6ee1\u8db3\u5b66\u4e60\u8005\u7684\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u4e0e\u8bed\u8a00\u4e60\u5f97\u9886\u57df\u7684\u4e13\u5bb6\u8fdb\u884c\u521d\u6b65\u7814\u7a76\uff0c\u5f00\u53d1\u5e76\u6d4b\u8bd5\u4e86ConversAR\u7cfb\u7edf\uff0c\u968f\u540e\u572821\u540d\u7b2c\u4e8c\u8bed\u8a00\u5b66\u4e60\u8005\u4e2d\u8fdb\u884c\u7528\u6237\u7814\u7a76\u3002", "result": "\u672c\u7814\u7a76\u63d0\u51fa\u7684ConversAR\u7cfb\u7edf\uff0c\u901a\u8fc7\u6df7\u5408\u73b0\u5b9e\u6280\u672f\u3001\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u4ee5\u53ca\u589e\u5f3a\u73b0\u5b9e\uff0c\u65e8\u5728\u63d0\u5347\u7b2c\u4e8c\u8bed\u8a00\u7684\u53e3\u8bed\u80fd\u529b\uff0c\u8be5\u7cfb\u7edf\u5177\u6709\u52a8\u6001\u5c0f\u7ec4\u5bf9\u8bdd\u7684\u80fd\u529b\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u5b66\u4e60\u5de5\u5177\u7684\u9650\u5236\u3002", "conclusion": "ConversAR\u7cfb\u7edf\u6709\u6548\u63d0\u5347\u4e86\u5b66\u4e60\u8005\u7684\u53c2\u4e0e\u5ea6\u3001\u6c9f\u901a\u610f\u613f\uff0c\u5e76\u4e3a\u4ed6\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b89\u5168\u7684\u4ea4\u6d41\u7a7a\u95f4\uff0c\u672a\u6765\u8bed\u8a00\u5b66\u4e60\u5e94\u7528\u5e94\u8003\u8651\u6574\u5408\u751f\u6210\u5f0fAI\u4e0eXR\u6280\u672f\u3002"}}
{"id": "2510.07871", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07871", "abs": "https://arxiv.org/abs/2510.07871", "authors": ["Erjia Xiao", "Lingfeng Zhang", "Yingbo Tang", "Hao Cheng", "Renjing Xu", "Wenbo Ding", "Lei Zhou", "Long Chen", "Hangjun Ye", "Xiaoshuai Hao"], "title": "Team Xiaomi EV-AD VLA: Learning to Navigate Socially Through Proactive Risk Perception -- Technical Report for IROS 2025 RoboSense Challenge Social Navigation Track", "comment": null, "summary": "In this report, we describe the technical details of our submission to the\nIROS 2025 RoboSense Challenge Social Navigation Track. This track focuses on\ndeveloping RGBD-based perception and navigation systems that enable autonomous\nagents to navigate safely, efficiently, and socially compliantly in dynamic\nhuman-populated indoor environments. The challenge requires agents to operate\nfrom an egocentric perspective using only onboard sensors including RGB-D\nobservations and odometry, without access to global maps or privileged\ninformation, while maintaining social norm compliance such as safe distances\nand collision avoidance. Building upon the Falcon model, we introduce a\nProactive Risk Perception Module to enhance social navigation performance. Our\napproach augments Falcon with collision risk understanding that learns to\npredict distance-based collision risk scores for surrounding humans, which\nenables the agent to develop more robust spatial awareness and proactive\ncollision avoidance behaviors. The evaluation on the Social-HM3D benchmark\ndemonstrates that our method improves the agent's ability to maintain personal\nspace compliance while navigating toward goals in crowded indoor scenes with\ndynamic human agents, achieving 2nd place among 16 participating teams in the\nchallenge.", "AI": {"tldr": "\u672c\u62a5\u544a\u4ecb\u7ecd\u4e86\u4e00\u79cd\u589e\u5f3a\u793e\u4f1a\u5bfc\u822a\u6027\u80fd\u7684RGBD\u611f\u77e5\u4e0e\u5bfc\u822a\u7cfb\u7edf\uff0c\u5e76\u5728IROS 2025\u6311\u6218\u8d5b\u4e2d\u83b7\u5f97\u7b2c\u4e8c\u540d\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u5f00\u53d1\u4e00\u79cd\u81ea\u4e3b\u4ee3\u7406\u80fd\u591f\u5728\u590d\u6742\u7684\u5ba4\u5185\u73af\u5883\u4e2d\u5b89\u5168\u3001\u9ad8\u6548\u5730\u5bfc\u822a\uff0c\u540c\u65f6\u9075\u5faa\u793e\u4f1a\u89c4\u8303\uff0c\u4f8b\u5982\u4fdd\u6301\u5b89\u5168\u8ddd\u79bb\u548c\u907f\u514d\u78b0\u649e\u3002", "method": "\u672c\u7814\u7a76\u57fa\u4e8eFalcon\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u524d\u77bb\u6027\u98ce\u9669\u611f\u77e5\u6a21\u5757\uff0c\u7528\u4e8e\u5b66\u4e60\u9884\u6d4b\u5468\u56f4\u4eba\u7c7b\u7684\u57fa\u4e8e\u8ddd\u79bb\u7684\u78b0\u649e\u98ce\u9669\u5206\u6570\uff0c\u4ece\u800c\u63d0\u9ad8\u7a7a\u95f4\u611f\u77e5\u548c\u4e3b\u52a8\u907f\u78b0\u884c\u4e3a\u3002", "result": "\u672c\u62a5\u544a\u63cf\u8ff0\u4e86\u6211\u4eec\u5728IROS 2025 RoboSense\u6311\u6218\u8d5b\u793e\u4f1a\u5bfc\u822a\u8d5b\u9053\u4e2d\u7684\u63d0\u4ea4\u5185\u5bb9\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRGBD\u7684\u611f\u77e5\u4e0e\u5bfc\u822a\u7cfb\u7edf\uff0c\u65e8\u5728\u4f7f\u81ea\u4e3b\u4ee3\u7406\u80fd\u591f\u5728\u52a8\u6001\u4eba\u7fa4\u5bc6\u96c6\u7684\u5ba4\u5185\u73af\u5883\u4e2d\u5b89\u5168\u3001\u9ad8\u6548\u5e76\u7b26\u5408\u793e\u4f1a\u89c4\u8303\u5730\u5bfc\u822a\u3002\u6211\u4eec\u6839\u636eFalcon\u6a21\u578b\u5f15\u5165\u4e86\u4e00\u79cd\u524d\u77bb\u6027\u98ce\u9669\u611f\u77e5\u6a21\u5757\uff0c\u4ee5\u589e\u5f3a\u793e\u4f1a\u5bfc\u822a\u7684\u6027\u80fd\u3002\u901a\u8fc7\u8bc4\u4f30\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728Social-HM3D\u57fa\u51c6\u4e0a\u63d0\u9ad8\u4e86\u4ee3\u7406\u4eba\u5458\u7684\u4e2a\u4eba\u7a7a\u95f4\u5408\u89c4\u80fd\u529b\uff0c\u5e76\u5728\u6311\u6218\u4e2d\u83b7\u5f97\u4e8616\u652f\u53c2\u8d5b\u961f\u4f0d\u4e2d\u7684\u7b2c\u4e8c\u540d\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u524d\u77bb\u6027\u98ce\u9669\u611f\u77e5\u6a21\u5757\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u81ea\u4e3b\u4ee3\u7406\u5728\u52a8\u6001\u4eba\u7fa4\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u4e2a\u4eba\u7a7a\u95f4\u5408\u89c4\u3002"}}
{"id": "2510.08242", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.08242", "abs": "https://arxiv.org/abs/2510.08242", "authors": ["Mohammed Almutairi", "Charles Chiang", "Haoze Guo", "Matthew Belcher", "Nandini Banerjee", "Maria Milkowski", "Svitlana Volkova", "Daniel Nguyen", "Tim Weninger", "Michael Yankoski", "Trenton W. Ford", "Diego Gomez-Zara"], "title": "Simulating Teams with LLM Agents: Interactive 2D Environments for Studying Human-AI Dynamics", "comment": "29 pages", "summary": "Enabling users to create their own simulations offers a powerful way to study\nteam dynamics and performance. We introduce VirTLab, a system that allows\nresearchers and practitioners to design interactive, customizable simulations\nof team dynamics with LLM-based agents situated in 2D spatial environments.\nUnlike prior frameworks that restrict scenarios to predefined or static tasks,\nour approach enables users to build scenarios, assign roles, and observe how\nagents coordinate, move, and adapt over time. By bridging team cognition\nbehaviors with scalable agent-based modeling, our system provides a testbed for\ninvestigating how environments influence coordination, collaboration, and\nemergent team behaviors. We demonstrate its utility by aligning simulated\noutcomes with empirical evaluations and a user study, underscoring the\nimportance of customizable environments for advancing research on multi-agent\nsimulations. This work contributes to making simulations accessible to both\ntechnical and non-technical users, supporting the design, execution, and\nanalysis of complex multi-agent experiments.", "AI": {"tldr": "VirTLab\u662f\u4e00\u79cd\u5141\u8bb8\u7528\u6237\u521b\u5efa\u5b9a\u5236\u5316\u56e2\u961f\u52a8\u6001\u6a21\u62df\u7684\u65b0\u7cfb\u7edf\uff0c\u5229\u7528\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u57282D\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u9a8c\u3002", "motivation": "\u5728\u56e2\u961f\u52a8\u6001\u548c\u8868\u73b0\u7814\u7a76\u4e2d\uff0c\u5141\u8bb8\u7528\u6237\u521b\u5efa\u81ea\u5df1\u6a21\u62df\u7684\u80fd\u529b\u662f\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u73b0\u6709\u7684\u6846\u67b6\u5f80\u5f80\u53d7\u9650\u4e8e\u9884\u5b9a\u4e49\u573a\u666f\u6216\u9759\u6001\u4efb\u52a1\u3002", "method": "VirTLab\u5141\u8bb8\u7528\u6237\u8bbe\u8ba1\u4ea4\u4e92\u5f0f\u3001\u53ef\u5b9a\u5236\u7684\u56e2\u961f\u52a8\u6001\u6a21\u62df\uff0c\u901a\u8fc7\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u57282D\u7a7a\u95f4\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u8be5\u7cfb\u7edf\u5c55\u793a\u4e86\u5b9a\u5236\u73af\u5883\u5728\u63a8\u8fdb\u591a\u667a\u80fd\u4f53\u6a21\u62df\u7814\u7a76\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e14\u901a\u8fc7\u4e0e\u5b9e\u8bc1\u8bc4\u4f30\u548c\u7528\u6237\u7814\u7a76\u7684\u5bf9\u9f50\u8bc1\u660e\u4e86\u5176\u5b9e\u7528\u6027\u3002", "conclusion": "VirTLab\u4e3a\u7814\u7a76\u56e2\u961f\u52a8\u6001\u63d0\u4f9b\u4e86\u4e00\u4e2a\u91cd\u8981\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u901a\u8fc7\u53ef\u5b9a\u5236\u7684\u73af\u5883\u4fc3\u8fdb\u4e86\u591a\u667a\u80fd\u4f53\u5b9e\u9a8c\u7684\u8bbe\u8ba1\u548c\u5206\u6790\u3002"}}
{"id": "2510.07882", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07882", "abs": "https://arxiv.org/abs/2510.07882", "authors": ["Boyu Li", "Siyuan He", "Hang Xu", "Haoqi Yuan", "Yu Zang", "Liwei Hu", "Junpeng Yue", "Zhenxiong Jiang", "Pengbo Hu", "B\u00f6rje F. Karlsson", "Yehui Tang", "Zongqing Lu"], "title": "Towards Proprioception-Aware Embodied Planning for Dual-Arm Humanoid Robots", "comment": null, "summary": "In recent years, Multimodal Large Language Models (MLLMs) have demonstrated\nthe ability to serve as high-level planners, enabling robots to follow complex\nhuman instructions. However, their effectiveness, especially in long-horizon\ntasks involving dual-arm humanoid robots, remains limited. This limitation\narises from two main challenges: (i) the absence of simulation platforms that\nsystematically support task evaluation and data collection for humanoid robots,\nand (ii) the insufficient embodiment awareness of current MLLMs, which hinders\nreasoning about dual-arm selection logic and body positions during planning. To\naddress these issues, we present DualTHOR, a new dual-arm humanoid simulator,\nwith continuous transition and a contingency mechanism. Building on this\nplatform, we propose Proprio-MLLM, a model that enhances embodiment awareness\nby incorporating proprioceptive information with motion-based position\nembedding and a cross-spatial encoder. Experiments show that, while existing\nMLLMs struggle in this environment, Proprio-MLLM achieves an average\nimprovement of 19.75% in planning performance. Our work provides both an\nessential simulation platform and an effective model to advance embodied\nintelligence in humanoid robotics. The code is available at\nhttps://anonymous.4open.science/r/DualTHOR-5F3B.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86DualTHOR\u4eff\u771f\u5e73\u53f0\u548cProprio-MLLM\u6a21\u578b\uff0c\u6709\u6548\u589e\u5f3a\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u6267\u884c\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u89c4\u5212\u80fd\u529b\u3002", "motivation": "\u9488\u5bf9\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u53cc\u81c2\u4eba\u5f62\u673a\u5668\u4eba\u957f\u65f6\u95f4\u4efb\u52a1\u6267\u884c\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u6211\u4eec\u65e8\u5728\u63d0\u9ad8\u4eff\u771f\u8bc4\u4ef7\u548c\u6570\u636e\u6536\u96c6\u7684\u80fd\u529b\uff0c\u540c\u65f6\u589e\u5f3a\u6a21\u578b\u5bf9\u8eab\u4f53\u4f4d\u7f6e\u7684\u611f\u77e5\u3002", "method": "\u901a\u8fc7\u521b\u5efaDualTHOR\uff0c\u4e00\u4e2a\u65b0\u578b\u7684\u53cc\u81c2\u4eba\u5f62\u673a\u5668\u4eba\u4eff\u771f\u5e73\u53f0\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u51faProprio-MLLM\u6a21\u578b\uff0c\u7ed3\u5408\u672c\u4f53\u611f\u77e5\u4fe1\u606f\u4ee5\u589e\u5f3a\u8eab\u4f53\u610f\u8bc6\u3002", "result": "Proprio-MLLM\u5728\u53cc\u81c2\u4eba\u5f62\u673a\u5668\u4eba\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u5e73\u574719.75%\u7684\u89c4\u5212\u6027\u80fd\u63d0\u5347\uff0c\u800c\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6b64\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86DualTHOR\u4eff\u771f\u5e73\u53f0\u548cProprio-MLLM\u6a21\u578b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u89c4\u5212\u6027\u80fd\u3002"}}
{"id": "2510.08326", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.08326", "abs": "https://arxiv.org/abs/2510.08326", "authors": ["Yaning Li", "Yutong Chen", "Yihan Hou", "Chenyi Chen", "Yihan Han", "Jingxuan Han", "Wenxi Dai", "Youyou Li", "Xinke Tang", "Meng Li", "Qi Dong", "Hongwei Li"], "title": "LacAIDes: Generative AI-Supported Creative Interactive Circuits Crafting to Enliven Traditional Lacquerware", "comment": null, "summary": "Lacquerware, a representative craft of Chinese intangible cultural heritage,\nis renowned for its layered aesthetics and durability but faces declining\nengagement. While prior human-computer interaction research has explored\nembedding interactive circuits to transform lacquerware into responsive\nartifacts, most studies have focused on fabrication techniques rather than\nsupporting makers in creatively designing such interactions at a low threshold.\nTo address this gap, we present LacAIDes, a Generative AI powered\ncreativity-support tool built on a multi-agent workflow aligned with the double\ndiamond model of design thinking. LacAIDes enables exploration and creation of\nculturally grounded interactive circuits without requiring prior technical\nexpertise. We evaluated LacAIDes in a longitudinal workshop with 34\nparticipants using a mixed-method approach. Results show that LacAIDes\ndemonstrated high usability, enhanced creative engagement in craft making, and\nencouraged critical reflection on the role of Generative AI in digital craft\npractices. This work contributes to human-computer interaction by introducing a\nnovel creativity-support tool and providing empirical insights into\nrevitalizing traditional craft making through Generative AI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u5f0fAI\u5de5\u5177LacAIDes\uff0c\u65e8\u5728\u5e2e\u52a9\u975e\u6280\u672f\u80cc\u666f\u7684\u5236\u4f5c\u8005\u63a2\u7d22\u548c\u521b\u5efa\u5177\u6709\u6587\u5316\u6839\u57fa\u7684\u4ea4\u4e92\u7535\u8def\uff0c\u63d0\u5347\u4f20\u7edf\u6f06\u5668\u5de5\u827a\u7684\u6d3b\u529b\u3002", "motivation": "\u63a8\u52a8\u4e2d\u56fd\u4f20\u7edf\u6f06\u5668\u5de5\u827a\u7684\u521b\u65b0\u8bbe\u8ba1\u4e0e\u521b\u4f5c\uff0c\u89e3\u51b3\u4e0e\u4eba\u673a\u4ea4\u4e92\u76f8\u5173\u7684\u6280\u672f\u95e8\u69db\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4e0e34\u540d\u53c2\u4e0e\u8005\u7684\u7eb5\u5411\u5de5\u4f5c\u574a\u8fdb\u884c\u6df7\u5408\u65b9\u6cd5\u8bc4\u4f30\uff0c\u9a8c\u8bc1LacAIDes\u7684\u53ef\u7528\u6027\u548c\u5e94\u7528\u6548\u679c\u3002", "result": "\u5f00\u53d1\u5e76\u8bc4\u4f30\u4e86\u4e00\u4e2a\u540d\u4e3aLacAIDes\u7684\u751f\u6210\u5f0fAI\u521b\u610f\u652f\u6301\u5de5\u5177\uff0c\u4fc3\u8fdb\u4e86\u7528\u6237\u5728\u6f06\u5668\u5236\u4f5c\u4e2d\u7684\u521b\u9020\u6027\u53c2\u4e0e\u3002", "conclusion": "LacAIDes\u5de5\u5177\u63d0\u9ad8\u4e86\u7528\u6237\u5728\u6f06\u5668\u5236\u4f5c\u4e2d\u7684\u53c2\u4e0e\u611f\u548c\u521b\u9020\u529b\uff0c\u5e76\u4fc3\u4f7f\u5bf9\u751f\u6210\u5f0fAI\u5728\u6570\u5b57\u5de5\u827a\u5b9e\u8df5\u4e2d\u4f5c\u7528\u7684\u53cd\u601d\u3002"}}
{"id": "2510.07975", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07975", "abs": "https://arxiv.org/abs/2510.07975", "authors": ["Mingyang Sun", "Jiude Wei", "Qichen He", "Donglin Wang", "Cewu Lu", "Jianhua Sun"], "title": "Executable Analytic Concepts as the Missing Link Between VLM Insight and Precise Manipulation", "comment": null, "summary": "Enabling robots to perform precise and generalized manipulation in\nunstructured environments remains a fundamental challenge in embodied AI. While\nVision-Language Models (VLMs) have demonstrated remarkable capabilities in\nsemantic reasoning and task planning, a significant gap persists between their\nhigh-level understanding and the precise physical execution required for\nreal-world manipulation. To bridge this \"semantic-to-physical\" gap, we\nintroduce GRACE, a novel framework that grounds VLM-based reasoning through\nexecutable analytic concepts (EAC)-mathematically defined blueprints that\nencode object affordances, geometric constraints, and semantics of\nmanipulation. Our approach integrates a structured policy scaffolding pipeline\nthat turn natural language instructions and visual information into an\ninstantiated EAC, from which we derive grasp poses, force directions and plan\nphysically feasible motion trajectory for robot execution. GRACE thus provides\na unified and interpretable interface between high-level instruction\nunderstanding and low-level robot control, effectively enabling precise and\ngeneralizable manipulation through semantic-physical grounding. Extensive\nexperiments demonstrate that GRACE achieves strong zero-shot generalization\nacross a variety of articulated objects in both simulated and real-world\nenvironments, without requiring task-specific training.", "AI": {"tldr": "GRACE\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u5229\u7528\u53ef\u6267\u884c\u7684\u89e3\u6790\u6982\u5ff5\u5c06\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e0e\u673a\u5668\u4eba\u64cd\u4f5c\u8fde\u63a5\u8d77\u6765\uff0c\u589e\u5f3a\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u80fd\u529b\u3002", "motivation": "\u8feb\u5207\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u89e3\u51b3\u673a\u5668\u4eba\u9ad8\u5c42\u6b21\u7406\u89e3\u4e0e\u7269\u7406\u64cd\u4f5c\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4ee5\u4fbf\u5728\u590d\u6742\u73af\u5883\u4e2d\u8fdb\u884c\u6709\u6548\u7684\u64cd\u4f5c\u3002", "method": "GRACE\u96c6\u6210\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u7b56\u7565\u642d\u5efa\u7ba1\u9053\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u548c\u89c6\u89c9\u4fe1\u606f\u751f\u6210\u53ef\u6267\u884c\u7684\u89e3\u6790\u6982\u5ff5\uff0c\u4ee5\u6b64\u63a8\u5bfc\u6293\u53d6\u59ff\u52bf\u3001\u65bd\u529b\u65b9\u5411\u548c\u8fd0\u52a8\u8f68\u8ff9\u3002", "result": "GRACE\u6846\u67b6\u901a\u8fc7\u53ef\u6267\u884c\u7684\u89e3\u6790\u6982\u5ff5(EAC)\u5f25\u5408\u4e86\u8bed\u4e49\u7406\u89e3\u4e0e\u5b9e\u9645\u6267\u884c\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u7cbe\u786e\u548c\u901a\u7528\u7684\u64cd\u4f5c\u3002", "conclusion": "GRACE\u5b9e\u73b0\u4e86\u9ad8\u5c42\u6b21\u6307\u4ee4\u7406\u89e3\u4e0e\u4f4e\u5c42\u6b21\u673a\u5668\u4eba\u63a7\u5236\u4e4b\u95f4\u7684\u7edf\u4e00\u63a5\u53e3\uff0c\u5c55\u73b0\u4e86\u5728\u591a\u4e2a\u7269\u4f53\u4e0a\u7684\u96f6-shot\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.08328", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.08328", "abs": "https://arxiv.org/abs/2510.08328", "authors": ["Kalyan Ramana Gattoz", "Prasad S. Onkar"], "title": "Motion Exploration of Articulated Product Concepts in Interactive Sketching Environment", "comment": null, "summary": "In the early stages of engineering design, it is essential to know how a\nproduct behaves, especially how it moves. As designers must keep adjusting the\nmotion until it meets the intended requirements, this process is often\nrepetitive and time-consuming. Although the physics behind these motions is\nusually based on simple equations, manually working through them can be tedious\nand inefficient. To ease this burden, some tasks are now handled by computers.\nOne common method involves converting hand-drawn sketches into models using CAD\nor CAE software. However, this approach can be time- and resource-intensive.\nAdditionally, product sketches are usually best understood only by the\ndesigners who created them. Others may struggle to interpret them correctly,\nrelying heavily on intuition and prior experience. Since sketches are static,\nthey fail to show how a product moves, limiting their usefulness. This paper\npresents a new approach that addresses these issues by digitising the natural\nact of sketching. It allows designers to create, simulate, and test the motion\nof mechanical concepts in a more interactive way. An application was developed\nto evaluate this method, focusing on user satisfaction and mental workload\nduring a design task. The results showed a 77% reduction in cognitive effort\ncompared to traditional methods, with users reporting high satisfaction. Future\nwork will focus on expanding this approach from 2D (planar) to full 3D\n(spatial) design environments, enabling more complex product concept\ndevelopment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u5b57\u5316\u8349\u56fe\u8bbe\u8ba1\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bbe\u8ba1\u8fc7\u7a0b\u4e2d\u7684\u8ba4\u77e5\u8d1f\u62c5\u3002", "motivation": "\u8bbe\u8ba1\u8fc7\u7a0b\u4e2d\u9700\u8981\u9891\u7e41\u8c03\u6574\u7269\u4f53\u8fd0\u52a8\uff0c\u4f20\u7edf\u65b9\u6cd5\u8017\u65f6\u4e14\u4e0d\u591f\u76f4\u89c2\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u6b3e\u5e94\u7528\u7a0b\u5e8f\u6765\u8bc4\u4f30\u6570\u5b57\u5316\u8349\u56fe\u8bbe\u8ba1\u7684\u4ea4\u4e92\u5f0f\u6a21\u62df\u65b9\u6cd5\u3002", "result": "\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8ba4\u77e5\u52aa\u529b\u964d\u4f4e\u4e8677%\uff0c\u7528\u6237\u6ee1\u610f\u5ea6\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86\u8bbe\u8ba1\u5e08\u7684\u8ba4\u77e5\u52aa\u529b\uff0c\u4e14\u7528\u6237\u6ee1\u610f\u5ea6\u9ad8\uff0c\u672a\u6765\u5c06\u6269\u5c55\u5230\u66f4\u591a\u7ef4\u5ea6\u7684\u8bbe\u8ba1\u73af\u5883\u3002"}}
{"id": "2510.07986", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07986", "abs": "https://arxiv.org/abs/2510.07986", "authors": ["Gaofeng Li", "Peisen Xu", "Ruize Wang", "Qi Ye", "Jiming Chen", "Dezhen Song", "Yanlong Huang"], "title": "Orientation Learning and Adaptation towards Simultaneous Incorporation of Multiple Local Constraints", "comment": null, "summary": "Orientation learning plays a pivotal role in many tasks. However, the\nrotation group SO(3) is a Riemannian manifold. As a result, the distortion\ncaused by non-Euclidean geometric nature introduces difficulties to the\nincorporation of local constraints, especially for the simultaneous\nincorporation of multiple local constraints. To address this issue, we propose\nthe Angle-Axis Space-based orientation representation method to solve several\norientation learning problems, including orientation adaptation and\nminimization of angular acceleration. Specifically, we propose a weighted\naverage mechanism in SO(3) based on the angle-axis representation method. Our\nmain idea is to generate multiple trajectories by considering different local\nconstraints at different basepoints. Then these multiple trajectories are fused\nto generate a smooth trajectory by our proposed weighted average mechanism,\nachieving the goal to incorporate multiple local constraints simultaneously.\nCompared with existing solution, ours can address the distortion issue and make\nthe off-theshelf Euclidean learning algorithm be re-applicable in non-Euclidean\nspace. Simulation and Experimental evaluations validate that our solution can\nnot only adapt orientations towards arbitrary desired via-points and cope with\nangular acceleration constraints, but also incorporate multiple local\nconstraints simultaneously to achieve extra benefits, e.g., achieving smaller\nacceleration costs.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8eAngle-Axis\u8868\u793a\u6cd5\u7684\u65b9\u5411\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u65b9\u5411\u5b66\u4e60\u4e2d\u7684\u591a\u4e2a\u5c40\u90e8\u7ea6\u675f\u6574\u5408\u95ee\u9898\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u975e\u6b27\u51e0\u91cc\u5f97\u51e0\u4f55\u6027\u8d28\u5e26\u6765\u7684\u626d\u66f2\u95ee\u9898\uff0c\u4ee5\u4fbf\u5728\u65b9\u5411\u5b66\u4e60\u4e2d\u6709\u6548\u6574\u5408\u591a\u4e2a\u5c40\u90e8\u7ea6\u675f\u3002", "method": "\u91c7\u7528Angle-Axis\u8868\u793a\u6cd5\uff0c\u5728SO(3)\u4e2d\u63d0\u51fa\u52a0\u6743\u5e73\u5747\u673a\u5236\uff0c\u901a\u8fc7\u751f\u6210\u591a\u6761\u8f68\u8ff9\u5e76\u878d\u5408\u4ee5\u5b9e\u73b0\u5e73\u6ed1\u8f68\u8ff9\u3002", "result": "\u5df2\u9a8c\u8bc1\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u9002\u5e94\u4efb\u610f\u76ee\u6807\u70b9\u65b9\u5411\uff0c\u5e76\u5e94\u5bf9\u89d2\u52a0\u901f\u5ea6\u7ea6\u675f\uff0c\u540c\u65f6\u6709\u6548\u6574\u5408\u591a\u4e2a\u5c40\u90e8\u7ea6\u675f\uff0c\u964d\u4f4e\u52a0\u901f\u5ea6\u6210\u672c\u3002", "conclusion": "\u63d0\u51fa\u7684Angle-Axis Space\u57fa\u4e8e\u65b9\u5411\u8868\u793a\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u65b9\u5411\u5b66\u4e60\u4e2d\u7684\u626d\u66f2\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u591a\u4e2a\u5c40\u90e8\u7ea6\u675f\u7684\u540c\u65f6\u6574\u5408\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.08332", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.08332", "abs": "https://arxiv.org/abs/2510.08332", "authors": ["Mengdi Chu", "Zefeng Qiu", "Meng Ling", "Shuning Jiang", "Robert S. Laramee", "Michael Sedlmair", "Jian Chen"], "title": "What Makes a Visualization Complex?", "comment": "9+20 pages, 9+18 figures. Accepted at IEEE VIS 2025", "summary": "We investigate the perceived visual complexity (VC) in data visualizations\nusing objective image-based metrics. We collected VC scores through a\nlarge-scale crowdsourcing experiment involving 349 participants and 1,800\nvisualization images. We then examined how these scores align with 12\nimage-based metrics spanning information-theoretic, clutter, color, and our two\nobject-based metrics. Our results show that both low-level image properties and\nthe high-level elements affect perceived VC in visualization images; The number\nof corners and distinct colors are robust metrics across visualizations.\nSecond, feature congestion, an information-theoretic metric capturing\nstatistical patterns in color and texture, is the strongest predictor of\nperceived complexity in visualizations rich in the same stimuli; edge density\neffectively explains VC in node-link diagrams. Additionally, we observe a\nbell-curve effect for text annotations: increasing text-to-ink ratio (TiR)\ninitially reduces complexity, reaching an optimal point, beyond which further\ntext increases perceived complexity. Our quantification pipeline is also\ninterpretable, enabling metric-based explanations, grounded in the\nVisComplexity2K dataset, bridging computational metrics with human perceptual\nresponses. osf.io/5xe8a has the preregistration and osf.io/bdet6 has the\nVisComplexity2K dataset, source code, and all Apdx. and figures.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u4f17\u5305\u5b9e\u9a8c\u63a2\u8ba8\u6570\u636e\u53ef\u89c6\u5316\u4e2d\u7684\u89c6\u89c9\u590d\u6742\u5ea6\uff0c\u53d1\u73b0\u4f4e\u7ea7\u548c\u9ad8\u7ea7\u7279\u5f81\u7686\u5bf9\u89c6\u89c9\u590d\u6742\u5ea6\u4ea7\u751f\u5f71\u54cd\uff0c\u5e76\u63d0\u4f9b\u4e86\u57fa\u4e8e\u6307\u6807\u7684\u91cf\u5316\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u7814\u7a76\u6570\u636e\u53ef\u89c6\u5316\u4e2d\u7684\u89c6\u89c9\u590d\u6742\u5ea6\uff0c\u4ee5\u4e86\u89e3\u5f71\u54cd\u4eba\u7c7b\u611f\u77e5\u7684\u5ba2\u89c2\u6307\u6807\u3002", "method": "\u901a\u8fc7\u5927\u578b\u4f17\u5305\u5b9e\u9a8c\u6536\u96c6\u548c\u5206\u6790\u6570\u636e\u53ef\u89c6\u5316\u4e2d\u7684\u89c6\u89c9\u590d\u6742\u5ea6\u8bc4\u5206\uff0c\u5e76\u4e0e\u591a\u79cd\u56fe\u50cf\u57fa\u7840\u6307\u6807\u8fdb\u884c\u5bf9\u6bd4\u7814\u7a76\u3002", "result": "\u53d1\u73b0\u4f4e\u7ea7\u56fe\u50cf\u7279\u6027\u548c\u9ad8\u7ea7\u5143\u7d20\u5747\u5bf9\u53ef\u89c6\u5316\u56fe\u50cf\u7684\u89c6\u89c9\u590d\u6742\u5ea6\u4ea7\u751f\u5f71\u54cd\u3002\u89d2\u7684\u6570\u91cf\u548c\u4e0d\u540c\u989c\u8272\u7684\u6570\u76ee\u662f\u5f3a\u6709\u529b\u7684\u6307\u6807\uff1b\u7279\u5f81\u62e5\u6324\u5ea6\u6700\u80fd\u9884\u6d4b\u590d\u6742\u5ea6\uff1b\u6587\u672c\u6ce8\u91ca\u7684\u6587\u672c\u4e0e\u58a8\u6c34\u6bd4\u4f8b\u5b58\u5728\u949f\u5f62\u66f2\u7ebf\u6548\u5e94\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u53ef\u89c6\u5316\u590d\u6742\u5ea6\u7684\u7406\u89e3\u63d0\u4f9b\u4e86\u91cf\u5316\u548c\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\uff0c\u5f3a\u8c03\u4e86\u56fe\u50cf\u7279\u5f81\u5728\u89c6\u89c9\u590d\u6742\u5ea6\u611f\u77e5\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.08022", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08022", "abs": "https://arxiv.org/abs/2510.08022", "authors": ["Kehui Liu", "Zhongjie Jia", "Yang Li", "Zhaxizhuoma", "Pengan Chen", "Song Liu", "Xin Liu", "Pingrui Zhang", "Haoming Song", "Xinyi Ye", "Nieqing Cao", "Zhigang Wang", "Jia Zeng", "Dong Wang", "Yan Ding", "Bin Zhao", "Xuelong Li"], "title": "FastUMI-100K: Advancing Data-driven Robotic Manipulation with a Large-scale UMI-style Dataset", "comment": null, "summary": "Data-driven robotic manipulation learning depends on large-scale,\nhigh-quality expert demonstration datasets. However, existing datasets, which\nprimarily rely on human teleoperated robot collection, are limited in terms of\nscalability, trajectory smoothness, and applicability across different robotic\nembodiments in real-world environments. In this paper, we present FastUMI-100K,\na large-scale UMI-style multimodal demonstration dataset, designed to overcome\nthese limitations and meet the growing complexity of real-world manipulation\ntasks. Collected by FastUMI, a novel robotic system featuring a modular,\nhardware-decoupled mechanical design and an integrated lightweight tracking\nsystem, FastUMI-100K offers a more scalable, flexible, and adaptable solution\nto fulfill the diverse requirements of real-world robot demonstration data.\nSpecifically, FastUMI-100K contains over 100K+ demonstration trajectories\ncollected across representative household environments, covering 54 tasks and\nhundreds of object types. Our dataset integrates multimodal streams, including\nend-effector states, multi-view wrist-mounted fisheye images and textual\nannotations. Each trajectory has a length ranging from 120 to 500 frames.\nExperimental results demonstrate that FastUMI-100K enables high policy success\nrates across various baseline algorithms, confirming its robustness,\nadaptability, and real-world applicability for solving complex, dynamic\nmanipulation challenges. The source code and dataset will be released in this\nlink https://github.com/MrKeee/FastUMI-100K.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FastUMI-100K\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u591a\u6a21\u6001\u793a\u8303\u6570\u636e\u96c6\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u7684\u5c40\u9650\u6027\uff0c\u6ee1\u8db3\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u7684\u9700\u6c42\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u636e\u96c6\u5728\u89c4\u6a21\u3001\u5e73\u6ed1\u5ea6\u548c\u9002\u7528\u6027\u65b9\u9762\u6709\u9650\uff0c\u4e0d\u8db3\u4ee5\u6ee1\u8db3\u590d\u6742\u73b0\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u7684\u9700\u6c42\u3002", "method": "\u901a\u8fc7FastUMI\u7cfb\u7edf\u6536\u96c6\u548c\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\uff0c\u6784\u5efaFastUMI-100K\u6570\u636e\u96c6\u3002", "result": "FastUMI-100K\u5305\u542b10\u4e07\u6761\u793a\u8303\u8f68\u8ff9\uff0c\u8986\u76d654\u4e2a\u4efb\u52a1\u548c\u6570\u767e\u79cd\u7269\u54c1\u7c7b\u578b\uff0c\u80fd\u591f\u652f\u6301\u9ad8\u7b56\u7565\u6210\u529f\u7387\u3002", "conclusion": "FastUMI-100K\u5728\u9ad8\u653f\u7b56\u6210\u529f\u7387\u548c\u73b0\u5b9e\u5e94\u7528\u6027\u65b9\u9762\u5c55\u73b0\u51fa\u5176\u5f3a\u5927\u7684\u9002\u5e94\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2510.08044", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08044", "abs": "https://arxiv.org/abs/2510.08044", "authors": ["Shiyuan Yin", "Chenjia Bai", "Zihao Zhang", "Junwei Jin", "Xinxin Zhang", "Chi Zhang", "Xuelong Li"], "title": "Towards Reliable LLM-based Robot Planning via Combined Uncertainty Estimation", "comment": null, "summary": "Large language models (LLMs) demonstrate advanced reasoning abilities,\nenabling robots to understand natural language instructions and generate\nhigh-level plans with appropriate grounding. However, LLM hallucinations\npresent a significant challenge, often leading to overconfident yet potentially\nmisaligned or unsafe plans. While researchers have explored uncertainty\nestimation to improve the reliability of LLM-based planning, existing studies\nhave not sufficiently differentiated between epistemic and intrinsic\nuncertainty, limiting the effectiveness of uncertainty estimation. In this\npaper, we present Combined Uncertainty estimation for Reliable Embodied\nplanning (CURE), which decomposes the uncertainty into epistemic and intrinsic\nuncertainty, each estimated separately. Furthermore, epistemic uncertainty is\nsubdivided into task clarity and task familiarity for more accurate evaluation.\nThe overall uncertainty assessments are obtained using random network\ndistillation and multi-layer perceptron regression heads driven by LLM\nfeatures. We validated our approach in two distinct experimental settings:\nkitchen manipulation and tabletop rearrangement experiments. The results show\nthat, compared to existing methods, our approach yields uncertainty estimates\nthat are more closely aligned with the actual execution outcomes.", "AI": {"tldr": "CURE\u65b9\u6cd5\u901a\u8fc7\u5c06\u4e0d\u786e\u5b9a\u6027\u5206\u89e3\u4e3a\u5916\u5728\u548c\u5185\u5728\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u5347\u4e86\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u89c4\u5212\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89c4\u5212\u4e2d\u5f15\u53d1\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u9ad8\u89c4\u5212\u7684\u5b89\u5168\u6027\u548c\u4e00\u81f4\u6027\u3002", "method": "\u4f7f\u7528\u968f\u673a\u7f51\u7edc\u84b8\u998f\u548c\u591a\u5c42\u611f\u77e5\u673a\u56de\u5f52\u5934\uff0c\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7279\u5f81\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u8bc4\u4f30\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u65b9\u6cd5CURE\uff0c\u65e8\u5728\u63d0\u9ad8\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u673a\u5668\u4eba\u89c4\u5212\u7684\u53ef\u9760\u6027\u3002", "conclusion": "CURE\u65b9\u6cd5\u6709\u6548\u533a\u5206\u4e86\u5916\u5728\u548c\u5185\u5728\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5728\u4e24\u4e2a\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2510.08106", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.08106", "abs": "https://arxiv.org/abs/2510.08106", "authors": ["Zihan Li", "Yixiao Xu", "Lei Zhang", "Taiyu Han", "Xinshan Yang", "Yingni Wang", "Mingxuan Liu", "Shenghai Xin", "Linxun Liu", "Hongen Liao", "Guochen Ning"], "title": "Beyond hospital reach: Autonomous lightweight ultrasound robot for liver sonography", "comment": null, "summary": "Liver disease is a major global health burden. While ultrasound is the\nfirst-line diagnostic tool, liver sonography requires locating multiple\nnon-continuous planes from positions where target structures are often not\nvisible, for biometric assessment and lesion detection, requiring significant\nexpertise. However, expert sonographers are severely scarce in resource-limited\nregions. Here, we develop an autonomous lightweight ultrasound robot comprising\nan AI agent that integrates multi-modal perception with memory attention for\nlocalization of unseen target structures, and a 588-gram 6-degrees-of-freedom\ncable-driven robot. By mounting on the abdomen, the system enhances robustness\nagainst motion. Our robot can autonomously acquire expert-level standard liver\nultrasound planes and detect pathology in patients, including two from Xining,\na 2261-meter-altitude city with limited medical resources. Our system performs\neffectively on rapid-motion individuals and in wilderness environments. This\nwork represents the first demonstration of autonomous sonography across\nmultiple challenging scenarios, potentially transforming access to expert-level\ndiagnostics in underserved regions.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u81ea\u4e3b\u8d85\u58f0\u673a\u5668\u4eba\uff0c\u80fd\u5728\u590d\u6742\u73af\u5883\u4e2d\u6709\u6548\u8fdb\u884c\u809d\u810f\u8d85\u58f0\u68c0\u67e5\uff0c\u7279\u522b\u9002\u5408\u8d44\u6e90\u6709\u9650\u7684\u5730\u533a\u3002", "motivation": "\u809d\u810f\u75be\u75c5\u662f\u5168\u7403\u4e3b\u8981\u7684\u5065\u5eb7\u8d1f\u62c5\uff0c\u73b0\u6709\u7684\u8d85\u58f0\u8bca\u65ad\u5de5\u5177\u4f9d\u8d56\u4e8e\u4e13\u5bb6\uff0c\u5c24\u5176\u5728\u8d44\u6e90\u6709\u9650\u7684\u5730\u533a\u4e13\u5bb6\u7a00\u7f3a\u3002", "method": "\u8be5\u7cfb\u7edf\u7ed3\u5408\u4e86\u591a\u6a21\u6001\u611f\u77e5\u4e0e\u8bb0\u5fc6\u6ce8\u610f\u529b\uff0c\u4ee5\u5b9a\u4f4d\u4e0d\u53ef\u89c1\u7684\u76ee\u6807\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7\u4e00\u4e2a588\u514b\u7684\u516d\u81ea\u7531\u5ea6\u7535\u7f06\u9a71\u52a8\u673a\u5668\u4eba\u6765\u5b8c\u6210\u8d85\u58f0\u626b\u63cf\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u79cd\u81ea\u4e3b\u8f7b\u91cf\u5316\u8d85\u58f0\u673a\u5668\u4eba\uff0c\u80fd\u591f\u5728\u5404\u79cd\u590d\u6742\u573a\u666f\u4e2d\u8fdb\u884c\u809d\u810f\u8d85\u58f0\u626b\u63cf\uff0c\u63d0\u5347\u4e86\u8bca\u65ad\u7684\u53ef\u53ca\u6027\u3002", "conclusion": "\u8fd9\u4e00\u5de5\u4f5c\u5c55\u793a\u4e86\u5728\u591a\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u73af\u5883\u4e2d\u8fdb\u884c\u81ea\u4e3b\u8d85\u58f0\u68c0\u67e5\u7684\u53ef\u80fd\u6027\uff0c\u80fd\u663e\u8457\u6539\u5584\u6b20\u53d1\u8fbe\u5730\u533a\u7684\u533b\u7597\u8bca\u65ad\u53ef\u53ca\u6027\u3002"}}
{"id": "2510.08118", "categories": ["cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.08118", "abs": "https://arxiv.org/abs/2510.08118", "authors": ["Massimiliano de Leoni", "Faizan Ahmed Khan", "Simone Agostinelli"], "title": "Accurate and Noise-Tolerant Extraction of Routine Logs in Robotic Process Automation (Extended Version)", "comment": "16 pages, 5 figures", "summary": "Robotic Process Mining focuses on the identification of the routine types\nperformed by human resources through a User Interface. The ultimate goal is to\ndiscover routine-type models to enable robotic process automation. The\ndiscovery of routine-type models requires the provision of a routine log.\nUnfortunately, the vast majority of existing works do not directly focus on\nenabling the model discovery, limiting themselves to extracting the set of\nactions that are part of the routines. They were also not evaluated in\nscenarios characterized by inconsistent routine execution, hereafter referred\nto as noise, which reflects natural variability and occasional errors in human\nperformance. This paper presents a clustering-based technique that aims to\nextract routine logs. Experiments were conducted on nine UI logs from the\nliterature with different levels of injected noise. Our technique was compared\nwith existing techniques, most of which are not meant to discover routine logs\nbut were adapted for the purpose. The results were evaluated through standard\nstate-of-the-art metrics, showing that we can extract more accurate routine\nlogs than what the state of the art could, especially in the presence of noise.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6280\u672f\uff0c\u901a\u8fc7\u805a\u7c7b\u5206\u6790\u63d0\u53d6\u4eba\u7c7b\u64cd\u4f5c\u7684\u4f8b\u884c\u65e5\u5fd7\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u6267\u884c\u566a\u97f3\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u63d0\u53d6\u884c\u52a8\u96c6\u5408\uff0c\u800c\u975e\u76f4\u63a5\u652f\u6301\u6a21\u578b\u53d1\u73b0\uff0c\u5c24\u5176\u7f3a\u4e4f\u5bf9\u6267\u884c\u4e0d\u4e00\u81f4\u60c5\u5883\u7684\u8bc4\u4f30\u3002", "method": "\u57fa\u4e8e\u805a\u7c7b\u7684\u6280\u672f\uff0c\u9488\u5bf9\u4e0d\u540c\u6c34\u5e73\u566a\u97f3\u6ce8\u5165\u7684\u7528\u6237\u754c\u9762\u65e5\u5fd7\u8fdb\u884c\u5b9e\u9a8c\u6bd4\u8f83\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u805a\u7c7b\u7684\u6280\u672f\u6765\u63d0\u53d6\u4f8b\u884c\u65e5\u5fd7\uff0c\u80fd\u591f\u5728\u9762\u5bf9\u566a\u97f3\u65f6\u63d0\u53d6\u66f4\u51c6\u786e\u7684\u65e5\u5fd7\u3002", "conclusion": "\u65b0\u6280\u672f\u5728\u566a\u97f3\u5f71\u54cd\u4e0b\u4f9d\u7136\u80fd\u6709\u6548\u63d0\u53d6\u9ad8\u8d28\u91cf\u4f8b\u884c\u65e5\u5fd7\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.08173", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08173", "abs": "https://arxiv.org/abs/2510.08173", "authors": ["Haolin Yang", "Yuxing Long", "Zhuoyuan Yu", "Zihan Yang", "Minghan Wang", "Jiapeng Xu", "Yihan Wang", "Ziyan Yu", "Wenzhe Cai", "Lei Kang", "Hao Dong"], "title": "NavSpace: How Navigation Agents Follow Spatial Intelligence Instructions", "comment": null, "summary": "Instruction-following navigation is a key step toward embodied intelligence.\nPrior benchmarks mainly focus on semantic understanding but overlook\nsystematically evaluating navigation agents' spatial perception and reasoning\ncapabilities. In this work, we introduce the NavSpace benchmark, which contains\nsix task categories and 1,228 trajectory-instruction pairs designed to probe\nthe spatial intelligence of navigation agents. On this benchmark, we\ncomprehensively evaluate 22 navigation agents, including state-of-the-art\nnavigation models and multimodal large language models. The evaluation results\nlift the veil on spatial intelligence in embodied navigation. Furthermore, we\npropose SNav, a new spatially intelligent navigation model. SNav outperforms\nexisting navigation agents on NavSpace and real robot tests, establishing a\nstrong baseline for future work.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a8\u51faNavSpace\u57fa\u51c6\uff0c\u4ee5\u7cfb\u7edf\u8bc4\u4f30\u5bfc\u822a\u4ee3\u7406\u7684\u7a7a\u95f4\u667a\u80fd\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u5bfc\u822a\u6a21\u578bSNav\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u4ee3\u7406\u3002", "motivation": "\u6307\u5bfc\u6027\u5bfc\u822a\u662f\u5b9e\u73b0\u5177\u8eab\u667a\u80fd\u7684\u5173\u952e\u6b65\u9aa4\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u8bed\u4e49\u7406\u89e3\uff0c\u672a\u7cfb\u7edf\u8bc4\u4f30\u5bfc\u822a\u4ee3\u7406\u7684\u7a7a\u95f4\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u5f15\u5165NavSpace\u57fa\u51c6\uff0c\u5305\u542b\u516d\u4e2a\u4efb\u52a1\u7c7b\u522b\u548c1,228\u4e2a\u8f68\u8ff9-\u6307\u4ee4\u5bf9\uff0c\u5bf922\u4e2a\u5bfc\u822a\u4ee3\u7406\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "NavSpace\u57fa\u51c6\u7684\u5f15\u5165\u53ca\u5176\u5bf922\u4e2a\u5bfc\u822a\u4ee3\u7406\u7684\u5168\u9762\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u5177\u8eab\u5bfc\u822a\u4e2d\u7684\u7a7a\u95f4\u667a\u80fd\u3002", "conclusion": "SNav\u5728NavSpace\u548c\u771f\u5b9e\u673a\u5668\u4eba\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u5bfc\u822a\u4ee3\u7406\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u5f3a\u57fa\u7ebf\u3002"}}
{"id": "2510.08270", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.08270", "abs": "https://arxiv.org/abs/2510.08270", "authors": ["Damir Nurtdinov", "Aliaksei Korshuk", "Alexei Kornaev", "Alexander Maloletov"], "title": "Evaluation of a Robust Control System in Real-World Cable-Driven Parallel Robots", "comment": null, "summary": "This study evaluates the performance of classical and modern control methods\nfor real-world Cable-Driven Parallel Robots (CDPRs), focusing on\nunderconstrained systems with limited time discretization. A comparative\nanalysis is conducted between classical PID controllers and modern\nreinforcement learning algorithms, including Deep Deterministic Policy Gradient\n(DDPG), Proximal Policy Optimization (PPO), and Trust Region Policy\nOptimization (TRPO). The results demonstrate that TRPO outperforms other\nmethods, achieving the lowest root mean square (RMS) errors across various\ntrajectories and exhibiting robustness to larger time intervals between control\nupdates. TRPO's ability to balance exploration and exploitation enables stable\ncontrol in noisy, real-world environments, reducing reliance on high-frequency\nsensor feedback and computational demands. These findings highlight TRPO's\npotential as a robust solution for complex robotic control tasks, with\nimplications for dynamic environments and future applications in sensor fusion\nor hybrid control strategies.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0cTRPO\u5728\u63a7\u5236\u7535\u7f06\u9a71\u52a8\u5e76\u884c\u673a\u5668\u4eba\u65b9\u9762\u6548\u679c\u6700\u4f18\uff0c\u9002\u5408\u52a8\u6001\u548c\u566a\u58f0\u73af\u5883\uff0c\u51cf\u5c11\u5bf9\u9ad8\u9891\u4f20\u611f\u5668\u53cd\u9988\u7684\u4f9d\u8d56\u3002", "motivation": "\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u7535\u7f06\u9a71\u52a8\u5e76\u884c\u673a\u5668\u4eba\uff08CDPR\uff09\u63a7\u5236\u9762\u4e34\u65f6\u95f4\u79bb\u6563\u9650\u5236\u548c\u7cfb\u7edf\u6b20\u7ea6\u675f\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u8bc4\u4f30\u4e0d\u540c\u63a7\u5236\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "method": "\u6bd4\u8f83\u5206\u6790\u7ecf\u5178PID\u63a7\u5236\u5668\u4e0e\u73b0\u4ee3\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff08\u5982DDPG\u3001PPO\u548cTRPO\uff09\uff0c\u4ee5\u8bc4\u4f30\u5176\u5728\u7535\u7f06\u9a71\u52a8\u5e76\u884c\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u8868\u73b0\u3002", "result": "TRPO\u65b9\u6cd5\u5728\u591a\u4e2a\u8f68\u8ff9\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u6700\u4f4e\u7684\u5747\u65b9\u6839\uff08RMS\uff09\u8bef\u5dee\uff0c\u5e76\u4e14\u5177\u5907\u8f83\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "TRPO\u4f5c\u4e3a\u590d\u6742\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u7684\u5f3a\u5927\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u5728\u4f20\u611f\u5668\u878d\u5408\u548c\u6df7\u5408\u63a7\u5236\u7b56\u7565\u4e2d\u53d1\u6325\u91cd\u8981\u4f5c\u7528\u3002"}}
{"id": "2510.08381", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08381", "abs": "https://arxiv.org/abs/2510.08381", "authors": ["Baoyang Chen", "Xian Xu", "Huamin Qu"], "title": "Airy: Reading Robot Intent through Height and Sky", "comment": null, "summary": "As industrial robots move into shared human spaces, their opaque decision\nmaking threatens safety, trust, and public oversight. This artwork, Airy, asks\nwhether complex multi agent AI can become intuitively understandable by staging\na competition between two reinforcement trained robot arms that snap a bedsheet\nskyward. Building on three design principles, competition as a clear metric\n(who lifts higher), embodied familiarity (audiences recognize fabric snapping),\nand sensor to sense mapping (robot cooperation or rivalry shown through forest\nand weather projections), the installation gives viewers a visceral way to read\nmachine intent. Observations from five international exhibitions indicate that\naudiences consistently read the robots' strategies, conflict, and cooperation\nin real time, with emotional reactions that mirror the system's internal state.\nThe project shows how sensory metaphors can turn a black box into a public\ninterface.", "AI": {"tldr": "\u672c\u4f5c\u54c1\u901a\u8fc7\u673a\u5668\u4eba\u7ade\u4e89\u5c55\u793a\u5176\u610f\u56fe\uff0c\u4f7f\u89c2\u4f17\u80fd\u76f4\u89c2\u7406\u89e3AI\u7684\u884c\u4e3a\u548c\u72b6\u6001\u3002", "motivation": "\u5de5\u4e1a\u673a\u5668\u4eba\u8fdb\u5165\u4eba\u7c7b\u5171\u4eab\u7a7a\u95f4\uff0c\u5bfc\u81f4\u5176\u51b3\u7b56\u8fc7\u7a0b\u4e0d\u900f\u660e\uff0c\u5a01\u80c1\u5230\u5b89\u5168\u3001\u4fe1\u4efb\u548c\u516c\u4f17\u76d1\u7763\u3002", "method": "\u4f5c\u54c1\u57fa\u4e8e\u4e09\u4e2a\u8bbe\u8ba1\u539f\u5219\uff1a\u7ade\u4e89\u4f5c\u4e3a\u660e\u786e\u6307\u6807\u3001\u5177\u8eab\u719f\u6089\u611f\u548c\u4f20\u611f\u5668\u6620\u5c04\uff0c\u5c55\u793a\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u4e92\u52a8\u3002", "result": "\u8be5\u827a\u672f\u4f5c\u54c1\u901a\u8fc7\u4e24\u4e2a\u5f3a\u5316\u8bad\u7ec3\u7684\u673a\u5668\u4eba\u624b\u81c2\u4e4b\u95f4\u7684\u7ade\u4e89\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u76f4\u89c2\u7406\u89e3\u590d\u6742\u591a\u667a\u80fd\u4f53AI\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u9879\u76ee\u8868\u660e\uff0c\u611f\u5b98\u9690\u55bb\u53ef\u4ee5\u5c06\u9ed1\u7bb1\u53d8\u6210\u516c\u4f17\u63a5\u53e3\uff0c\u4ece\u800c\u589e\u5f3a\u4eba\u4eec\u5bf9\u673a\u5668\u4eba\u884c\u4e3a\u7684\u7406\u89e3\u3002"}}
{"id": "2510.08406", "categories": ["cs.RO", "cs.SY", "eess.SY", "math.OC", "I.2.9; G.1.6; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.08406", "abs": "https://arxiv.org/abs/2510.08406", "authors": ["Filip Be\u010danovi\u0107", "Kosta Jovanovi\u0107", "Vincent Bonnet"], "title": "Reliability of Single-Level Equality-Constrained Inverse Optimal Control", "comment": "8 pages, 3 figures", "summary": "Inverse optimal control (IOC) allows the retrieval of optimal cost function\nweights, or behavioral parameters, from human motion. The literature on IOC\nuses methods that are either based on a slow bilevel process or a fast but\nnoise-sensitive minimization of optimality condition violation. Assuming\nequality-constrained optimal control models of human motion, this article\npresents a faster but robust approach to solving IOC using a single-level\nreformulation of the bilevel method and yields equivalent results. Through\nnumerical experiments in simulation, we analyze the robustness to noise of the\nproposed single-level reformulation to the bilevel IOC formulation with a\nhuman-like planar reaching task that is used across recent studies. The\napproach shows resilience to very large levels of noise and reduces the\ncomputation time of the IOC on this task by a factor of 15 when compared to a\nclassical bilevel implementation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5355\u7ea7\u9006\u6700\u4f18\u63a7\u5236\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u5feb\u7684\u8ba1\u7b97\u901f\u5ea6\u548c\u66f4\u5f3a\u7684\u566a\u58f0\u9c81\u68d2\u6027\u3002", "motivation": "\u9006\u6700\u4f18\u63a7\u5236\uff08IOC\uff09\u7528\u4e8e\u4ece\u4eba\u7c7b\u8fd0\u52a8\u4e2d\u63d0\u53d6\u6700\u4f18\u6210\u672c\u51fd\u6570\u6743\u91cd\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901f\u5ea6\u6162\u4e14\u5bf9\u566a\u58f0\u654f\u611f\u3002", "method": "\u91c7\u7528\u5355\u7ea7\u91cd\u6784\u7684\u9006\u6700\u4f18\u63a7\u5236\u65b9\u6cd5\uff0c\u5206\u6790\u566a\u58f0\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u6781\u5927\u566a\u58f0\u4e0b\u4ecd\u7136\u6709\u6548\uff0c\u5e76\u5c06\u8ba1\u7b97\u65f6\u95f4\u6bd4\u4f20\u7edf\u53cc\u5c42\u65b9\u6cd5\u51cf\u5c11\u4e8615\u500d\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5904\u7406\u5927\u566a\u58f0\u65f6\u8868\u73b0\u51fa\u5f3a\u97e7\u6027\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u65f6\u95f4\u3002"}}
{"id": "2510.08408", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.08408", "abs": "https://arxiv.org/abs/2510.08408", "authors": ["Bibekananda Patra", "Rajeevlochana G. Chittawadigi", "Sandipan Bandyopadhyay"], "title": "Validation of collision-free spheres of Stewart-Gough platforms for constant orientations using the Application Programming Interface of a CAD software", "comment": null, "summary": "This paper presents a method of validation of the size of the largest\ncollision-free sphere (CFS) of a 6-6 Stewart-Gough platform manipulator (SGPM)\nfor a given orientation of its moving platform (MP) using the Application\nProgramming Interface (API) of a CAD software. The position of the MP is\nupdated via the API in an automated manner over a set of samples within a shell\nenclosing the surface of the CFS. For each pose of the manipulator, each pair\nof legs is investigated for mutual collisions. The CFS is considered safe or\nvalidated iff none of the points falling inside the CFS lead to a collision\nbetween any pair of legs. This approach can not only validate the safety of a\nprecomputed CFS, but also estimate the same for any spatial parallel\nmanipulator.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7CAD\u8f6f\u4ef6API\u9a8c\u8bc16-6\u65af\u56fe\u5c14\u7279-\u9ad8\u592b\u5e73\u53f0\u6700\u5927\u65e0\u78b0\u649e\u7403\u4f53\u5927\u5c0f\u7684\u65b9\u6cd5\uff0c\u4ece\u800c\u786e\u4fdd\u673a\u68b0\u624b\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u786e\u4fdd\u5728\u7279\u5b9a\u59ff\u6001\u4e0b\uff0c\u673a\u68b0\u624b\u7684\u8fd0\u52a8\u5b89\u5168\u6027\uff0c\u907f\u514d\u5173\u8282\u4e4b\u95f4\u7684\u78b0\u649e\u3002", "method": "\u4f7f\u7528 CAD \u8f6f\u4ef6\u7684 API \u81ea\u52a8\u66f4\u65b06-6\u65af\u56fe\u5c14\u7279-\u9ad8\u592b\u5e73\u53f0\u673a\u68b0\u624b\uff08SGPM\uff09\u8fd0\u52a8\u5e73\u53f0\uff08MP\uff09\u7684\u4f4d\u7f6e\uff0c\u9a8c\u8bc1\u5176\u6700\u5927\u65e0\u78b0\u649e\u7403\u4f53\u7684\u5927\u5c0f\u3002", "result": "\u5728\u6bcf\u4e2a\u52a8\u4f5c\u4e2d\uff0c\u8c03\u67e5\u6bcf\u5bf9\u817f\u4e4b\u95f4\u7684\u4e92\u76f8\u78b0\u649e\uff0c\u4ece\u800c\u9a8c\u8bc1\u65e0\u78b0\u649e\u7403\u4f53\u7684\u5b89\u5168\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u9a8c\u8bc1\u9884\u5148\u8ba1\u7b97\u7684\u65e0\u78b0\u649e\u7403\u4f53\u7684\u5b89\u5168\u6027\uff0c\u8fd8\u53ef\u4f30\u7b97\u4efb\u4f55\u7a7a\u95f4\u5e76\u884c\u673a\u68b0\u624b\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2510.08464", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08464", "abs": "https://arxiv.org/abs/2510.08464", "authors": ["Jason Jabbour", "Dong-Ki Kim", "Max Smith", "Jay Patrikar", "Radhika Ghosal", "Youhui Wang", "Ali Agha", "Vijay Janapa Reddi", "Shayegan Omidshafiei"], "title": "Don't Run with Scissors: Pruning Breaks VLA Models but They Can Be Recovered", "comment": null, "summary": "Vision-Language-Action (VLA) models have advanced robotic capabilities but\nremain challenging to deploy on resource-limited hardware. Pruning has enabled\nefficient compression of large language models (LLMs), yet it is largely\nunderstudied in robotics. Surprisingly, we observe that pruning VLA models\nleads to drastic degradation and increased safety violations. We introduce\nGLUESTICK, a post-pruning recovery method that restores much of the original\nmodel's functionality while retaining sparsity benefits. Our method performs a\none-time interpolation between the dense and pruned models in weight-space to\ncompute a corrective term. This correction is used during inference by each\npruned layer to recover lost capabilities with minimal overhead. GLUESTICK\nrequires no additional training, is agnostic to the pruning algorithm, and\nintroduces a single hyperparameter that controls the tradeoff between\nefficiency and accuracy. Across diverse VLA architectures and tasks in\nmanipulation and navigation, GLUESTICK achieves competitive memory efficiency\nwhile substantially recovering success rates and reducing safety violations.\nAdditional material can be found at: https://gluestick-vla.github.io/.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86GLUESTICK\u540e\u4fee\u526a\u6062\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e00\u79cd\u7b80\u5355\u7684\u63d2\u503c\u6280\u672f\uff0c\u5728\u4e0d\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u4f7f\u5f97VLA\u6a21\u578b\u5728\u4fee\u526a\u540e\u80fd\u591f\u6062\u590d\u529f\u80fd\u3002", "motivation": "VLA\u6a21\u578b\u5728\u8d44\u6e90\u6709\u9650\u7684\u786c\u4ef6\u4e0a\u90e8\u7f72\u5b58\u5728\u6311\u6218\uff0c\u5c3d\u7ba1\u4fee\u526a\u53ef\u4ee5\u6709\u6548\u538b\u7f29\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u63d0\u51faGLUESTICK\uff0c\u4e00\u79cd\u540e\u4fee\u526a\u6062\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6743\u91cd\u7a7a\u95f4\u4e2d\u8fdb\u884c\u7a00\u758f\u6a21\u578b\u548c\u7a20\u5bc6\u6a21\u578b\u4e4b\u95f4\u7684\u63d2\u503c\uff0c\u8ba1\u7b97\u6821\u6b63\u9879\u4ee5\u6062\u590d\u529f\u80fd\u3002", "result": "GLUESTICK\u5728\u5404\u7c7bVLA\u67b6\u6784\u548c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u7684\u5185\u5b58\u6548\u7387\uff0c\u540c\u65f6\u5927\u5e45\u6062\u590d\u6210\u529f\u7387\uff0c\u51cf\u5c11\u5b89\u5168\u8fdd\u89c4\u3002", "conclusion": "GLUESTICK\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u5bf9\u4fee\u526a\u7b97\u6cd5\u65e0\u504f\u89c1\uff0c\u53ea\u5f15\u5165\u4e00\u4e2a\u8d85\u53c2\u6570\u6765\u5e73\u8861\u6548\u7387\u4e0e\u51c6\u786e\u6027\uff0c\u80fd\u6709\u6548\u63d0\u5347\u4fee\u526a\u540eVLA\u6a21\u578b\u7684\u529f\u80fd\u3002"}}
{"id": "2510.08475", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08475", "abs": "https://arxiv.org/abs/2510.08475", "authors": ["Jhen Hsieh", "Kuan-Hsun Tu", "Kuo-Han Hung", "Tsung-Wei Ke"], "title": "DexMan: Learning Bimanual Dexterous Manipulation from Human and Generated Videos", "comment": "Video results are available at:\n  https://embodiedai-ntu.github.io/dexman/index.html", "summary": "We present DexMan, an automated framework that converts human visual\ndemonstrations into bimanual dexterous manipulation skills for humanoid robots\nin simulation. Operating directly on third-person videos of humans manipulating\nrigid objects, DexMan eliminates the need for camera calibration, depth\nsensors, scanned 3D object assets, or ground-truth hand and object motion\nannotations. Unlike prior approaches that consider only simplified floating\nhands, it directly controls a humanoid robot and leverages novel contact-based\nrewards to improve policy learning from noisy hand-object poses estimated from\nin-the-wild videos.\n  DexMan achieves state-of-the-art performance in object pose estimation on the\nTACO benchmark, with absolute gains of 0.08 and 0.12 in ADD-S and VSD.\nMeanwhile, its reinforcement learning policy surpasses previous methods by 19%\nin success rate on OakInk-v2. Furthermore, DexMan can generate skills from both\nreal and synthetic videos, without the need for manual data collection and\ncostly motion capture, and enabling the creation of large-scale, diverse\ndatasets for training generalist dexterous manipulation.", "AI": {"tldr": "DexMan\u662f\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u5c06\u4eba\u7c7b\u89c6\u89c9\u6f14\u793a\u8f6c\u6362\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u6280\u80fd\uff0c\u4e0d\u9700\u8981\u590d\u6742\u7684\u6570\u636e\u6536\u96c6\uff0c\u8868\u73b0\u4f18\u8d8a\u3002", "motivation": "\u65e8\u5728\u6d88\u9664\u5bf9\u76f8\u673a\u6807\u5b9a\u3001\u6df1\u5ea6\u4f20\u611f\u5668\u548c\u6602\u8d35\u8fd0\u52a8\u6355\u6349\u7684\u4f9d\u8d56\uff0c\u4ee5\u7b80\u5316\u548c\u63d0\u5347\u53cc\u624b\u7075\u5de7\u64cd\u4f5c\u6280\u80fd\u7684\u751f\u6210\u8fc7\u7a0b\u3002", "method": "DexMan\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u7b2c\u4e09\u65b9\u4eba\u7c7b\u64cd\u4f5c\u89c6\u9891\uff0c\u76f4\u63a5\u63a7\u5236\u4eff\u4eba\u673a\u5668\u4eba\uff0c\u5e76\u4f7f\u7528\u65b0\u9896\u7684\u57fa\u4e8e\u63a5\u89e6\u7684\u5956\u52b1\u6765\u63d0\u9ad8\u653f\u7b56\u5b66\u4e60\u3002", "result": "\u5728TACO\u57fa\u51c6\u4e0a\uff0cDexMan\u5728\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5728OakInk-v2\u4e0a\uff0c\u5176\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u6210\u529f\u7387\u63d0\u5347\u4e8619%\u3002", "conclusion": "DexMan\u901a\u8fc7\u5229\u7528\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u81ea\u52a8\u5316\u751f\u6210\u673a\u68b0\u624b\u9700\u8981\u7684\u53cc\u624b\u7075\u5de7\u64cd\u4f5c\u6280\u80fd\uff0c\u8868\u73b0\u51fa\u8272\u4e14\u65e0\u9700\u590d\u6742\u7684\u6570\u636e\u51c6\u5907\u3002"}}
{"id": "2510.08547", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08547", "abs": "https://arxiv.org/abs/2510.08547", "authors": ["Xiuwei Xu", "Angyuan Ma", "Hankun Li", "Bingyao Yu", "Zheng Zhu", "Jie Zhou", "Jiwen Lu"], "title": "R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation", "comment": "Project page: https://r2rgen.github.io/", "summary": "Towards the aim of generalized robotic manipulation, spatial generalization\nis the most fundamental capability that requires the policy to work robustly\nunder different spatial distribution of objects, environment and agent itself.\nTo achieve this, substantial human demonstrations need to be collected to cover\ndifferent spatial configurations for training a generalized visuomotor policy\nvia imitation learning. Prior works explore a promising direction that\nleverages data generation to acquire abundant spatially diverse data from\nminimal source demonstrations. However, most approaches face significant\nsim-to-real gap and are often limited to constrained settings, such as\nfixed-base scenarios and predefined camera viewpoints. In this paper, we\npropose a real-to-real 3D data generation framework (R2RGen) that directly\naugments the pointcloud observation-action pairs to generate real-world data.\nR2RGen is simulator- and rendering-free, thus being efficient and\nplug-and-play. Specifically, given a single source demonstration, we introduce\nan annotation mechanism for fine-grained parsing of scene and trajectory. A\ngroup-wise augmentation strategy is proposed to handle complex multi-object\ncompositions and diverse task constraints. We further present camera-aware\nprocessing to align the distribution of generated data with real-world 3D\nsensor. Empirically, R2RGen substantially enhances data efficiency on extensive\nexperiments and demonstrates strong potential for scaling and application on\nmobile manipulation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u76843D\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u63a7\u4e2d\u7684\u7a7a\u95f4\u666e\u904d\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5b9e\u6d4b\u589e\u5f3a\u6570\u636e\u6548\u7387\uff0c\u4fc3\u8fdb\u79fb\u52a8\u64cd\u4f5c\u7684\u5e94\u7528\u3002", "motivation": "\u9488\u5bf9\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u666e\u904d\u5316\u9700\u6c42\uff0c\u9700\u8981\u6536\u96c6\u5927\u91cf\u4eba\u7c7b\u793a\u8303\u4ee5\u8986\u76d6\u4e0d\u540c\u7684\u7a7a\u95f4\u914d\u7f6e\uff0c\u4ece\u800c\u8bad\u7ec3\u901a\u7528\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u6d4b\u5230\u5b9e\u6d4b\u76843D\u6570\u636e\u751f\u6210\u6846\u67b6R2RGen\uff0c\u91c7\u7528\u4e86\u7fa4\u4f53\u589e\u5f3a\u7b56\u7565\u548c\u57fa\u4e8e\u6444\u50cf\u673a\u7684\u5904\u7406\u3002", "result": "R2RGen\u901a\u8fc7\u76f4\u63a5\u589e\u5f3a\u70b9\u4e91\u89c2\u5bdf-\u52a8\u4f5c\u5bf9\uff0c\u751f\u6210\u771f\u5b9e\u4e16\u754c\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u8fc7\u53bb\u65b9\u6cd5\u4e2d\u5b58\u5728\u7684sim-to-real\u5dee\u8ddd\u548c\u9650\u5236\u3002", "conclusion": "R2RGen\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u6548\u7387\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u79fb\u52a8\u64cd\u4f5c\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2510.08556", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08556", "abs": "https://arxiv.org/abs/2510.08556", "authors": ["Xueyi Liu", "He Wang", "Li Yi"], "title": "DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via Joint-Wise Neural Dynamics Model", "comment": "Project Website: https://meowuu7.github.io/DexNDM/ Video:\n  https://youtu.be/tU2Mv8vWftU", "summary": "Achieving generalized in-hand object rotation remains a significant challenge\nin robotics, largely due to the difficulty of transferring policies from\nsimulation to the real world. The complex, contact-rich dynamics of dexterous\nmanipulation create a \"reality gap\" that has limited prior work to constrained\nscenarios involving simple geometries, limited object sizes and aspect ratios,\nconstrained wrist poses, or customized hands. We address this sim-to-real\nchallenge with a novel framework that enables a single policy, trained in\nsimulation, to generalize to a wide variety of objects and conditions in the\nreal world. The core of our method is a joint-wise dynamics model that learns\nto bridge the reality gap by effectively fitting limited amount of real-world\ncollected data and then adapting the sim policy's actions accordingly. The\nmodel is highly data-efficient and generalizable across different whole-hand\ninteraction distributions by factorizing dynamics across joints, compressing\nsystem-wide influences into low-dimensional variables, and learning each\njoint's evolution from its own dynamic profile, implicitly capturing these net\neffects. We pair this with a fully autonomous data collection strategy that\ngathers diverse, real-world interaction data with minimal human intervention.\nOur complete pipeline demonstrates unprecedented generality: a single policy\nsuccessfully rotates challenging objects with complex shapes (e.g., animals),\nhigh aspect ratios (up to 5.33), and small sizes, all while handling diverse\nwrist orientations and rotation axes. Comprehensive real-world evaluations and\na teleoperation application for complex tasks validate the effectiveness and\nrobustness of our approach. Website: https://meowuu7.github.io/DexNDM/", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5173\u8282\u52a8\u6001\u6a21\u578b\u89e3\u51b3\u673a\u5668\u4eba\u624b\u4e2d\u7269\u4f53\u65cb\u8f6c\u5728\u771f\u5b9e\u4e0e\u6a21\u62df\u95f4\u8f6c\u79fb\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u5355\u4e00\u7b56\u7565\u5728\u591a\u6837\u5bf9\u8c61\u53ca\u6761\u4ef6\u4e0b\u7684\u901a\u7528\u6027\uff0c\u5c55\u793a\u4e86\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u673a\u5668\u4eba\u624b\u4e2d\u7269\u4f53\u65cb\u8f6c\u7684\u73b0\u5b9e\u4e0e\u6a21\u62df\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4ee5\u5b9e\u73b0\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u573a\u666f\u548c\u63d0\u9ad8\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u7075\u6d3b\u6027\u4e0e\u6548\u7387\u3002", "method": "\u57fa\u4e8e\u5173\u8282\u52a8\u6001\u6a21\u578b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6709\u6548\u5229\u7528\u5c11\u91cf\u771f\u5b9e\u4e16\u754c\u6570\u636e\u6539\u8fdb\u6a21\u62df\u7b56\u7565\uff0c\u5e76\u5b9e\u73b0\u81ea\u4e3b\u7ba1\u7406\u7684\u6570\u636e\u6536\u96c6\u3002", "result": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u673a\u5668\u4eba\u624b\u4e2d\u7269\u4f53\u65cb\u8f6c\u7684\u901a\u7528\u6027\u95ee\u9898\u3002\u8be5\u6846\u67b6\u53ef\u4ee5\u8ba9\u5728\u6a21\u62df\u4e2d\u8bad\u7ec3\u7684\u5355\u4e00\u7b56\u7565\u6709\u6548\u8f6c\u79fb\u5230\u771f\u5b9e\u73af\u5883\u4e2d\uff0c\u6210\u529f\u5904\u7406\u590d\u6742\u5f62\u72b6\u3001\u9ad8\u957f\u5bbd\u6bd4\u548c\u5c0f\u5c3a\u5bf8\u7684\u7269\u4f53\uff0c\u540c\u65f6\u9002\u5e94\u591a\u79cd\u624b\u8155\u59ff\u6001\u548c\u65cb\u8f6c\u8f74\u3002\u6838\u5fc3\u65b9\u6cd5\u662f\u4e00\u4e2a\u5173\u8282\u52a8\u6001\u6a21\u578b\uff0c\u901a\u8fc7\u5c11\u91cf\u771f\u5b9e\u4e16\u754c\u6570\u636e\u9002\u5e94\u6a21\u62df\u7b56\u7565\u7684\u52a8\u4f5c\uff0c\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u901a\u7528\u6027\u548c\u6570\u636e\u6548\u7387\u3002", "conclusion": "\u6b64\u7814\u7a76\u6210\u529f\u5730\u5c55\u793a\u4e86\u5728\u591a\u6837\u590d\u6742\u7269\u4f53\u65cb\u8f6c\u4efb\u52a1\u4e2d\uff0c\u5229\u7528\u5355\u4e00\u6a21\u62df\u7b56\u7565\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u6709\u6548\u5e94\u7528\uff0c\u540c\u65f6\u901a\u8fc7\u9ad8\u6548\u6570\u636e\u6536\u96c6\u7b56\u7565\u63d0\u5347\u4e86\u6a21\u578b\u7684\u901a\u7528\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2510.08568", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08568", "abs": "https://arxiv.org/abs/2510.08568", "authors": ["Hongyu Li", "Lingfeng Sun", "Yafei Hu", "Duy Ta", "Jennifer Barry", "George Konidaris", "Jiahui Fu"], "title": "NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos", "comment": null, "summary": "Enabling robots to execute novel manipulation tasks zero-shot is a central\ngoal in robotics. Most existing methods assume in-distribution tasks or rely on\nfine-tuning with embodiment-matched data, limiting transfer across platforms.\nWe present NovaFlow, an autonomous manipulation framework that converts a task\ndescription into an actionable plan for a target robot without any\ndemonstrations. Given a task description, NovaFlow synthesizes a video using a\nvideo generation model and distills it into 3D actionable object flow using\noff-the-shelf perception modules. From the object flow, it computes relative\nposes for rigid objects and realizes them as robot actions via grasp proposals\nand trajectory optimization. For deformable objects, this flow serves as a\ntracking objective for model-based planning with a particle-based dynamics\nmodel. By decoupling task understanding from low-level control, NovaFlow\nnaturally transfers across embodiments. We validate on rigid, articulated, and\ndeformable object manipulation tasks using a table-top Franka arm and a Spot\nquadrupedal mobile robot, and achieve effective zero-shot execution without\ndemonstrations or embodiment-specific training. Project website:\nhttps://novaflow.lhy.xyz/.", "AI": {"tldr": "NovaFlow\u662f\u4e00\u4e2a\u65e0\u9700\u6f14\u793a\u7684\u81ea\u4e3b\u64cd\u7eb5\u6846\u67b6\uff0c\u80fd\u591f\u5c06\u4efb\u52a1\u63cf\u8ff0\u8f6c\u5316\u4e3a\u884c\u52a8\u8ba1\u5212\uff0c\u5e76\u5728\u4e0d\u540c\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u6709\u6548\u8f6c\u79fb\u3002", "motivation": "\u5b9e\u73b0\u673a\u5668\u4eba\u96f6-shot\u6267\u884c\u65b0\u578b\u64cd\u7eb5\u4efb\u52a1\uff0c\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "NovaFlow\u6846\u67b6\u5c06\u4efb\u52a1\u63cf\u8ff0\u8f6c\u6362\u4e3a\u76ee\u6807\u673a\u5668\u4eba\u7684\u53ef\u64cd\u4f5c\u8ba1\u5212\uff0c\u65e0\u9700\u4efb\u4f55\u6f14\u793a\u3002", "result": "\u5728\u521a\u6027\u3001\u5173\u8282\u548c\u53ef\u53d8\u5f62\u7269\u4f53\u64cd\u7eb5\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86NovaFlow\uff0c\u6210\u529f\u5b9e\u73b0\u6709\u6548\u7684\u96f6-shot\u6267\u884c\u3002", "conclusion": "\u901a\u8fc7\u5c06\u4efb\u52a1\u7406\u89e3\u4e0e\u4f4e\u7ea7\u63a7\u5236\u89e3\u8026\uff0cNovaFlow\u5b9e\u73b0\u4e86\u8de8\u5e73\u53f0\u7684\u81ea\u7136\u8fc1\u79fb\u3002"}}
{"id": "2510.08571", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08571", "abs": "https://arxiv.org/abs/2510.08571", "authors": ["Animikh Aich", "Adwait Kulkarni", "Eshed Ohn-Bar"], "title": "Scalable Offline Metrics for Autonomous Driving", "comment": "Accepted at IROS 2025 (IEEE/RSJ International Conference on\n  Intelligent Robots and Systems)", "summary": "Real-World evaluation of perception-based planning models for robotic\nsystems, such as autonomous vehicles, can be safely and inexpensively conducted\noffline, i.e., by computing model prediction error over a pre-collected\nvalidation dataset with ground-truth annotations. However, extrapolating from\noffline model performance to online settings remains a challenge. In these\nsettings, seemingly minor errors can compound and result in test-time\ninfractions or collisions. This relationship is understudied, particularly\nacross diverse closed-loop metrics and complex urban maneuvers. In this work,\nwe revisit this undervalued question in policy evaluation through an extensive\nset of experiments across diverse conditions and metrics. Based on analysis in\nsimulation, we find an even worse correlation between offline and online\nsettings than reported by prior studies, casting doubts on the validity of\ncurrent evaluation practices and metrics for driving policies. Next, we bridge\nthe gap between offline and online evaluation. We investigate an offline metric\nbased on epistemic uncertainty, which aims to capture events that are likely to\ncause errors in closed-loop settings. The resulting metric achieves over 13%\nimprovement in correlation compared to previous offline metrics. We further\nvalidate the generalization of our findings beyond the simulation environment\nin real-world settings, where even greater gains are observed.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u611f\u77e5\u57fa\u7840\u89c4\u5212\u6a21\u578b\u5728\u79bb\u7ebf\u4e0e\u5728\u7ebf\u8bc4\u4f30\u4e2d\u7684\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba4\u8bc6\u4e0d\u786e\u5b9a\u6027\u7684\u79bb\u7ebf\u6307\u6807\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4e0e\u5728\u7ebf\u6027\u80fd\u7684\u76f8\u5173\u6027\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u7cfb\u7edf\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff09\u4e2d\uff0c\u79bb\u7ebf\u8bc4\u4f30\u611f\u77e5\u57fa\u7840\u7684\u89c4\u5212\u6a21\u578b\u975e\u5e38\u53ef\u884c\u548c\u7ecf\u6d4e\uff0c\u4f46\u5c06\u79bb\u7ebf\u6027\u80fd\u63a8\u65ad\u5230\u5728\u7ebf\u73af\u5883\u4e2d\u5374\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e0b\u7684\u95ed\u73af\u5ea6\u91cf\u548c\u7b56\u7565\u8bc4\u4f30\u662f\u4e0d\u591f\u5145\u5206\u7684\u3002", "method": "\u901a\u8fc7\u4e00\u7cfb\u5217\u6a21\u62df\u5b9e\u9a8c\uff0c\u5206\u6790\u79bb\u7ebf\u4e0e\u5728\u7ebf\u8bbe\u7f6e\u7684\u53cd\u9988\uff0c\u5e76\u901a\u8fc7\u65b0\u7684\u79bb\u7ebf\u6307\u6807\u8bc4\u4f30\u7b56\u7565\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u6709\u6548\u6027\u3002", "result": "\u57fa\u4e8e\u5b9e\u9a8c\u5206\u6790\uff0c\u53d1\u73b0\u79bb\u7ebf\u4e0e\u5728\u7ebf\u8bbe\u7f6e\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u6bd4\u5148\u524d\u7814\u7a76\u6240\u62a5\u544a\u7684\u8981\u5dee\uff0c\u8fd9\u5bf9\u5f53\u524d\u7684\u8bc4\u4f30\u5b9e\u8df5\u548c\u9a7e\u9a76\u7b56\u7565\u7684\u6709\u6548\u6027\u63d0\u51fa\u4e86\u7591\u95ee\u3002\u540c\u65f6\uff0c\u57fa\u4e8e\u8ba4\u8bc6\u4e0d\u786e\u5b9a\u6027\u7684\u65b0\u79bb\u7ebf\u6307\u6807\u63d0\u9ad8\u4e86\u76f8\u5173\u6027\uff0c\u5177\u6709\u8d85\u8fc713%\u7684\u6539\u8fdb\u3002", "conclusion": "\u6211\u4eec\u7684\u53d1\u73b0\u8868\u660e\uff0c\u5f53\u524d\u7684\u9a7e\u9a76\u653f\u7b56\u8bc4\u4f30\u5b9e\u8df5\u5b58\u5728\u7f3a\u9677\uff0c\u65b0\u63d0\u51fa\u7684\u6307\u6807\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u8868\u73b0\u66f4\u4f73\uff0c\u5f3a\u8c03\u4e86\u66f4\u7cbe\u786e\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.08572", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08572", "abs": "https://arxiv.org/abs/2510.08572", "authors": ["Rocktim Jyoti Das", "Harsh Singh", "Diana Turmakhan", "Muhammad Abdullah Sohail", "Mingfei Han", "Preslav Nakov", "Fabio Pizzati", "Ivan Laptev"], "title": "BLAZER: Bootstrapping LLM-based Manipulation Agents with Zero-Shot Data Generation", "comment": "11 pages, 8 figures", "summary": "Scaling data and models has played a pivotal role in the remarkable progress\nof computer vision and language. Inspired by these domains, recent efforts in\nrobotics have similarly focused on scaling both data and model size to develop\nmore generalizable and robust policies. However, unlike vision and language,\nrobotics lacks access to internet-scale demonstrations across diverse robotic\ntasks and environments. As a result, the scale of existing datasets typically\nsuffers from the need for manual data collection and curation. To address this\nproblem, here we propose BLAZER, a framework that learns manipulation policies\nfrom automatically generated training data. We build on the zero-shot\ncapabilities of LLM planners and automatically generate demonstrations for\ndiverse manipulation tasks in simulation. Successful examples are then used to\nfinetune an LLM and to improve its planning capabilities without human\nsupervision. Notably, while BLAZER training requires access to the simulator's\nstate, we demonstrate direct transfer of acquired skills to sensor-based\nmanipulation. Through extensive experiments, we show BLAZER to significantly\nimprove zero-shot manipulation in both simulated and real environments.\nMoreover, BLAZER improves on tasks outside of its training pool and enables\ndownscaling of LLM models. Our code and data will be made publicly available on\nthe project page.", "AI": {"tldr": "BLAZER\u6846\u67b6\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u5229\u7528LLM\u7684\u96f6-shot\u80fd\u529b\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u64cd\u63a7\u7684\u6709\u6548\u6027\uff0c\u4e14\u53ef\u4ee5\u5728\u4e0d\u4f9d\u8d56\u4eba\u5de5\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u6a21\u578b\u7ec6\u8c03\u3002", "motivation": "\u673a\u5668\u4eba\u9886\u57df\u76ee\u524d\u7f3a\u4e4f\u4e92\u8054\u7f51\u89c4\u6a21\u7684\u591a\u6837\u5316\u6f14\u793a\u6570\u636e\uff0c\u73b0\u6709\u6570\u636e\u96c6\u901a\u5e38\u9700\u8981\u4eba\u5de5\u6536\u96c6\u548c\u6574\u7406\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u63d0\u5347\u6570\u636e\u89c4\u6a21\u4ee5\u8bad\u7ec3\u66f4\u5f3a\u5927\u7684\u64cd\u63a7\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u5229\u7528LLM\u89c4\u5212\u5668\u7684\u96f6-shot\u80fd\u529b\u6765\u5b66\u4e60\u64cd\u63a7\u7b56\u7565\uff0c\u5e76\u5728\u8fd9\u4e9b\u7b56\u7565\u6210\u529f\u793a\u4f8b\u7684\u57fa\u7840\u4e0a\u7ec6\u8c03LLM\u3002", "result": "BLAZER\u901a\u8fc7\u5728\u6a21\u62df\u4e2d\u81ea\u52a8\u751f\u6210\u6f14\u793a\uff0c\u80fd\u6709\u6548\u63d0\u9ad8\u96f6-shot\u64cd\u63a7\u80fd\u529b\uff0c\u5e76\u6210\u529f\u5c06\u83b7\u5f97\u7684\u6280\u80fd\u8f6c\u79fb\u5230\u57fa\u4e8e\u4f20\u611f\u5668\u7684\u64cd\u4f5c\u4e2d\u3002", "conclusion": "BLAZER\u663e\u8457\u63d0\u5347\u4e86\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u96f6-shot\u64cd\u4f5c\u80fd\u529b\uff0c\u5e76\u80fd\u5728\u8bad\u7ec3\u6c60\u5916\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u652f\u6301LLM\u6a21\u578b\u7684\u4e0b\u7f29\u3002"}}
