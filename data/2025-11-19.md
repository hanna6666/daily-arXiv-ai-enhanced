<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 26]
- [cs.HC](#cs.HC) [Total: 18]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [FICO: Finite-Horizon Closed-Loop Factorization for Unified Multi-Agent Path Finding](https://arxiv.org/abs/2511.13961)
*Jiarui Li,Alessandro Zanardi,Runyu Zhang,Gioele Zardini*

Main category: cs.RO

TL;DR: 本文提出了一种系统级的多智能体路径规划框架，集成了规划与执行，处理不确定性，提出FICO算法，提升了实时性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体路径规划大多将规划和执行分开处理，缺乏对不确定性的明确建模。

Method: 提出了一种名为有限视界闭环因式分解（FICO）的算法，该算法基于因式分解，灵感源自前视控制，旨在高效地进行闭环操作。

Result: FICO算法在实时响应能力上表现优越，能在毫秒级启动执行，并在面临执行时的不确定性时具有良好的自适应能力，计算时间比开放循环基线减少了两个数量级，并在随机延迟和代理到达的情况下显著提高了吞吐量。

Conclusion: 本文为多智能体路径规划（MAPF）建立了一个系统级框架，通过系统建模、因式分解和闭环设计为进一步研究提供了原则基础。

Abstract: Multi-Agent Path Finding is a fundamental problem in robotics and AI, yet most existing formulations treat planning and execution separately and address variants of the problem in an ad hoc manner. This paper presents a system-level framework for MAPF that integrates planning and execution, generalizes across variants, and explicitly models uncertainties. At its core is the MAPF system, a formal model that casts MAPF as a control design problem encompassing classical and uncertainty-aware formulations. To solve it, we introduce Finite-Horizon Closed-Loop Factorization (FICO), a factorization-based algorithm inspired by receding-horizon control that exploits compositional structure for efficient closed-loop operation. FICO enables real-time responses -- commencing execution within milliseconds -- while scaling to thousands of agents and adapting seamlessly to execution-time uncertainties. Extensive case studies demonstrate that it reduces computation time by up to two orders of magnitude compared with open-loop baselines, while delivering significantly higher throughput under stochastic delays and agent arrivals. These results establish a principled foundation for analyzing and advancing MAPF through system-level modeling, factorization, and closed-loop design.

</details>


### [2] [LIO-MARS: Non-uniform Continuous-time Trajectories for Real-time LiDAR-Inertial-Odometry](https://arxiv.org/abs/2511.13985)
*Jan Quenzel,Sven Behnke*

Main category: cs.RO

TL;DR: 本论文提出了一种先进的LiDAR惯性测程（LIO-MARS），通过结合IMU和LiDAR传感器数据，实现了高效准确的机器人导航，显示出在多个数据集的优越性能。


<details>
  <summary>Details</summary>
Motivation: 自主机器人系统在安全导航过程中依赖环境知识，尤其是在搜索和救援任务中，飞行机器人需要强大的实时感知，结合IMU和LiDAR传感器的数据。

Method: 论文提出了一种新的LiDAR惯性测程（LIO）方法，通过连续时间B样条轨迹，结合高斯混合模型（GMM）对多分辨率表面图进行对齐，同时加速了重要的协方差和GMM计算。

Result: 通过引入非均匀时间节点放置的扫描窗口，确保轨迹连续性，并使用不变变换去除误差，同时采用软约束提升了鲁棒性和精度。

Conclusion: 该研究展示了LIO-MARS在多种手持、地面和空中载体数据集上，与最新的LIO系统相比，具有先进的质量。

Abstract: Autonomous robotic systems heavily rely on environment knowledge to safely navigate. For search & rescue, a flying robot requires robust real-time perception, enabled by complementary sensors. IMU data constrains acceleration and rotation, whereas LiDAR measures accurate distances around the robot. Building upon the LiDAR odometry MARS, our LiDAR-inertial odometry (LIO) jointly aligns multi-resolution surfel maps with a Gaussian mixture model (GMM) using a continuous-time B-spline trajectory. Our new scan window uses non-uniform temporal knot placement to ensure continuity over the whole trajectory without additional scan delay. Moreover, we accelerate essential covariance and GMM computations with Kronecker sums and products by a factor of 3.3. An unscented transform de-skews surfels, while a splitting into intra-scan segments facilitates motion compensation during spline optimization. Complementary soft constraints on relative poses and preintegrated IMU pseudo-measurements further improve robustness and accuracy. Extensive evaluation showcases the state-of-the-art quality of our LIO-MARS w.r.t. recent LIO systems on various handheld, ground and aerial vehicle-based datasets.

</details>


### [3] [Searching in Space and Time: Unified Memory-Action Loops for Open-World Object Retrieval](https://arxiv.org/abs/2511.14004)
*Taijing Chen,Sateesh Kumar,Junhong Xu,George Pavlakos,J oydeep Biswas,Roberto Martín-Martín*

Main category: cs.RO

TL;DR: STAR框架通过整合时空信息，提升了动态开放世界中服务机器人对物体检索的能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法充分处理动态开放世界情境下的物体检索，缺乏整合时空信息的有效框架。

Method: STAR框架结合了非参数长期记忆和工作记忆，以支持高效回忆，并使用视觉-语言模型在每个步骤选择时空行动。

Result: STAR在多种实验和真实环境中表现优于现有的场景图和纯记忆基线。

Conclusion: STAR框架在时空物体搜索任务中表现优越，证明了将时间搜索和空间搜索视为统一问题的优势。

Abstract: Service robots must retrieve objects in dynamic, open-world settings where requests may reference attributes ("the red mug"), spatial context ("the mug on the table"), or past states ("the mug that was here yesterday"). Existing approaches capture only parts of this problem: scene graphs capture spatial relations but ignore temporal grounding, temporal reasoning methods model dynamics but do not support embodied interaction, and dynamic scene graphs handle both but remain closed-world with fixed vocabularies. We present STAR (SpatioTemporal Active Retrieval), a framework that unifies memory queries and embodied actions within a single decision loop. STAR leverages non-parametric long-term memory and a working memory to support efficient recall, and uses a vision-language model to select either temporal or spatial actions at each step. We introduce STARBench, a benchmark of spatiotemporal object search tasks across simulated and real environments. Experiments in STARBench and on a Tiago robot show that STAR consistently outperforms scene-graph and memory-only baselines, demonstrating the benefits of treating search in time and search in space as a unified problem.

</details>


### [4] [FACA: Fair and Agile Multi-Robot Collision Avoidance in Constrained Environments with Dynamic Priorities](https://arxiv.org/abs/2511.14024)
*Jaskirat Singh,Rohan Chandra*

Main category: cs.RO

TL;DR: 提出了一种名为FACA的多机器人协调任务方法，通过自然语言交流和新颖的人工势场算法，显著提高了任务完成效率并确保安全。


<details>
  <summary>Details</summary>
Motivation: 随着多机器人系统在关键应用中的使用增加，在拥挤和紧急的环境中安全且灵活地导航成为巨大挑战。

Method: 提出了一种使用自然语言的机器人协调任务的方法FACA，结合了一种新的人工势场算法，创建了自动的“环形交叉口”效果以处理冲突。

Result: 实验表明，FACA能实现超过3.5倍的效率提升和超过70%的时间减少，同时保持稳健的安全水平。

Conclusion: FACA方法在效率和安全性方面表现优异，能够显著提高多机器人系统在复杂环境中的任务完成速度。

Abstract: Multi-robot systems are increasingly being used for critical applications such as rescuing injured people, delivering food and medicines, and monitoring key areas. These applications usually involve navigating at high speeds through constrained spaces such as small gaps. Navigating such constrained spaces becomes particularly challenging when the space is crowded with multiple heterogeneous agents all of which have urgent priorities. What makes the problem even harder is that during an active response situation, roles and priorities can quickly change on a dime without informing the other agents. In order to complete missions in such environments, robots must not only be safe, but also agile, able to dodge and change course at a moment's notice. In this paper, we propose FACA, a fair and agile collision avoidance approach where robots coordinate their tasks by talking to each other via natural language (just as people do). In FACA, robots balance safety with agility via a novel artificial potential field algorithm that creates an automatic ``roundabout'' effect whenever a conflict arises. Our experiments show that FACA achieves a improvement in efficiency, completing missions more than 3.5X faster than baselines with a time reduction of over 70% while maintaining robust safety margins.

</details>


### [5] [BIM-Discrepancy-Driven Active Sensing for Risk-Aware UAV-UGV Navigation](https://arxiv.org/abs/2511.14037)
*Hesam Mojtahedi,Reza Akhavian*

Main category: cs.RO

TL;DR: 本文提出了一种新的框架，通过整合实时数据与BIM模型，提升无人机和无人地面车辆在动态施工环境中的协作导航能力，显著提高了导航安全性并减少了任务时间。


<details>
  <summary>Details</summary>
Motivation: 针对传统导航方法依赖静态BIM先验或有限机载感知的问题，旨在提高无人机和无人地面车辆在动态施工环境中的协作导航能力。

Method: 提出了一种BIM差异驱动的主动感知框架，通过实时激光雷达数据与BIM先验知识融合，维护一个不断演变的二维占用图，并量化导航安全性。

Result: 在PX4-Gazebo仿真中，该方法实现了平均走廊风险减少58%和地图熵减少43%，并且与前沿探索相比，在一半的任务时间内实现了类似的不确定性减少。

Conclusion: 将BIM先验知识与风险自适应的空中感知相结合，能够为建筑机器人提供可扩展、能够感知不确定性的自主导航能力。

Abstract: This paper presents a BIM-discrepancy-driven active sensing framework for cooperative navigation between unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) in dynamic construction environments. Traditional navigation approaches rely on static Building Information Modeling (BIM) priors or limited onboard perception. In contrast, our framework continuously fuses real-time LiDAR data from aerial and ground robots with BIM priors to maintain an evolving 2D occupancy map. We quantify navigation safety through a unified corridor-risk metric integrating occupancy uncertainty, BIM-map discrepancy, and clearance. When risk exceeds safety thresholds, the UAV autonomously re-scans affected regions to reduce uncertainty and enable safe replanning. Validation in PX4-Gazebo simulation with Robotec GPU LiDAR demonstrates that risk-triggered re-scanning reduces mean corridor risk by 58% and map entropy by 43% compared to static BIM navigation, while maintaining clearance margins above 0.4 m. Compared to frontier-based exploration, our approach achieves similar uncertainty reduction in half the mission time. These results demonstrate that integrating BIM priors with risk-adaptive aerial sensing enables scalable, uncertainty-aware autonomy for construction robotics.

</details>


### [6] [FlexiCup: Wireless Multimodal Suction Cup with Dual-Zone Vision-Tactile Sensing](https://arxiv.org/abs/2511.14139)
*Junhao Gong,Shoujie Li,Kit-Wa Sou,Changqing Guo,Hourong Huang,Tong Wu,Yifan Xie,Chenxin Liang,Chuqiao Lyu,Xiaojun Liang,Wenbo Ding*

Main category: cs.RO

TL;DR: 本文提出的FlexiCup是一种无线多模态吸盘，集成了双区视觉触觉传感，显著提高在非结构化环境中的接触感知操控能力，经过验证显示了良好的实用性。


<details>
  <summary>Details</summary>
Motivation: 传统吸盘在非结构化环境中的接触感知能力不足，因此需要一种新的解决方案以提高操控准确性和效率。

Method: 通过模块化机械配置和双区域视觉触觉传感的集成实现了完全无线的自主操作，并通过多种控制范式验证了硬件的灵活性。

Result: 实验表明，FlexiCup在提供了良好的成功率（真空模式90.0%和Bernoulli模式86.7%）的同时，通过扩散学习在倾斜运输和橙子提取任务上表现出了73.3%和66.7%的成功率，且多头注意机制显著提升了操控效果。

Conclusion: FlexiCup在接触感知操控方面实现了显著提升，尤其是在复杂环境中的表现优异。

Abstract: Conventional suction cups lack sensing capabilities for contact-aware manipulation in unstructured environments. This paper presents FlexiCup, a fully wireless multimodal suction cup that integrates dual-zone vision-tactile sensing. The central zone dynamically switches between vision and tactile modalities via illumination control for contact detection, while the peripheral zone provides continuous spatial awareness for approach planning. FlexiCup supports both vacuum and Bernoulli suction modes through modular mechanical configurations, achieving complete wireless autonomy with onboard computation and power. We validate hardware versatility through dual control paradigms. Modular perception-driven grasping across structured surfaces with varying obstacle densities demonstrates comparable performance between vacuum (90.0% mean success) and Bernoulli (86.7% mean success) modes. Diffusion-based end-to-end learning achieves 73.3% success on inclined transport and 66.7% on orange extraction tasks. Ablation studies confirm that multi-head attention coordinating dual-zone observations provides 13% improvements for contact-aware manipulation. Hardware designs and firmware are available at https://anonymous.4open.science/api/repo/FlexiCup-DA7D/file/index.html?v=8f531b44.

</details>


### [7] [AsyncVLA: Asynchronous Flow Matching for Vision-Language-Action Models](https://arxiv.org/abs/2511.14148)
*Yuhua Jiang,Shuang Cheng,Yan Ding,Feifei Gao,Biqing Qi*

Main category: cs.RO

TL;DR: AsyncVLA是一种新的VLA框架，通过引入异步流匹配和自我修正机制，优化了长任务执行中的动作生成和上下文感知。


<details>
  <summary>Details</summary>
Motivation: 解决传统同步流匹配(SFM)在长任务中的不稳定性，提升动作生成的灵活性和上下文感知能力。

Method: 提出异步流匹配VLA (AsyncVLA)框架，采用非均匀时间调度生成动作，并引入自信等级器进行动作修正。

Result: 在机器人操作基准上，AsyncVLA展现出数据效率和自我修正能力，并在通用机器人评估中取得了最新成果。

Conclusion: AsyncVLA实现了异步生成，提高了数据效率和自我修正能力，达到了机器人操作基准的最新成果。

Abstract: Vision-language-action (VLA) models have recently emerged as a powerful paradigm for building generalist robots. However, traditional VLA models that generate actions through flow matching (FM) typically rely on rigid and uniform time schedules, i.e., synchronous FM (SFM). Without action context awareness and asynchronous self-correction, SFM becomes unstable in long-horizon tasks, where a single action error can cascade into failure. In this work, we propose asynchronous flow matching VLA (AsyncVLA), a novel framework that introduces temporal flexibility in asynchronous FM (AFM) and enables self-correction in action generation. AsyncVLA breaks from the vanilla SFM in VLA models by generating the action tokens in a non-uniform time schedule with action context awareness. Besides, our method introduces the confidence rater to extract confidence of the initially generated actions, enabling the model to selectively refine inaccurate action tokens before execution. Moreover, we propose a unified training procedure for SFM and AFM that endows a single model with both modes, improving KV-cache utilization. Extensive experiments on robotic manipulation benchmarks demonstrate that AsyncVLA is data-efficient and exhibits self-correction ability. AsyncVLA achieves state-of-the-art results across general embodied evaluations due to its asynchronous generation in AFM. Our code is available at https://github.com/YuhuaJiang2002/AsyncVLA.

</details>


### [8] [RoboTidy : A 3D Gaussian Splatting Household Tidying Benchmark for Embodied Navigation and Action](https://arxiv.org/abs/2511.14161)
*Xiaoquan Sun,Ruijian Zhang,Kang Pang,Bingchen Miao,Yuxiang Tan,Zhen Yang,Ming Li,Jiayu Chen*

Main category: cs.RO

TL;DR: RoboTidy是一个新的基准，旨在通过语言指导家庭整理，支持VLA和VLN的训练和评估，提供丰富的3D场景和演示，促进机器人技术的发展。


<details>
  <summary>Details</summary>
Motivation: 现有基准没有模型用户偏好和支持移动性，导致结果的概括能力较差，难以全面评估综合的语言到行动能力。

Method: 设计并部署了RoboTidy作为一项基准，包括500个真实感的3D家庭场景，以及收集不同的演示和导航轨迹来支持训练和评估。

Result: RoboTidy包含500个场景、6.4k高质量操作演示轨迹和1.5k导航轨迹，旨在支持少量样本和大规模训练，且已在实际世界中进行部署。

Conclusion: RoboTidy提供了一个统一的基准，通过支持视觉-语言-行动(VLA)和视觉-语言-导航(VLN)训练和评估，填补了现有家庭整理领域的关键空白。

Abstract: Household tidying is an important application area, yet current benchmarks neither model user preferences nor support mobility, and they generalize poorly, making it hard to comprehensively assess integrated language-to-action capabilities. To address this, we propose RoboTidy, a unified benchmark for language-guided household tidying that supports Vision-Language-Action (VLA) and Vision-Language-Navigation (VLN) training and evaluation. RoboTidy provides 500 photorealistic 3D Gaussian Splatting (3DGS) household scenes (covering 500 objects and containers) with collisions, formulates tidying as an "Action (Object, Container)" list, and supplies 6.4k high-quality manipulation demonstration trajectories and 1.5k naviagtion trajectories to support both few-shot and large-scale training. We also deploy RoboTidy in the real world for object tidying, establishing an end-to-end benchmark for household tidying. RoboTidy offers a scalable platform and bridges a key gap in embodied AI by enabling holistic and realistic evaluation of language-guided robots.

</details>


### [9] [Towards Deploying VLA without Fine-Tuning: Plug-and-Play Inference-Time VLA Policy Steering via Embodied Evolutionary Diffusion](https://arxiv.org/abs/2511.14178)
*Zhuo Li,Junjia Liu,Zhipeng Dong,Tao Teng,Quentin Rouxel,Darwin Caldwell,Fei Chen*

Main category: cs.RO

TL;DR: 本研究提出VLA-Pilot，解决了预训练VLA政策在无缝部署中的性能下降问题，显著提升了成功率，支持零-shot泛化。


<details>
  <summary>Details</summary>
Motivation: 解决预训练VLA策略在实际部署中的性能下降问题，尤其是微调过程中的高成本和计算密集性。

Method: 引入了一种即插即用的推理时间政策引导方法，无需额外的微调或数据收集。

Result: VLA-Pilot在六个真实世界的下游操作任务中表现出色，提高了政策成功率，实现了对多样化任务和机器人设置的强稳健性。

Conclusion: VLA-Pilot显著提高了预训练VLA策略的成功率，支持在不同任务和机器人上进行零-shot推广。

Abstract: Vision-Language-Action (VLA) models have demonstrated significant potential in real-world robotic manipulation. However, pre-trained VLA policies still suffer from substantial performance degradation during downstream deployment. Although fine-tuning can mitigate this issue, its reliance on costly demonstration collection and intensive computation makes it impractical in real-world settings. In this work, we introduce VLA-Pilot, a plug-and-play inference-time policy steering method for zero-shot deployment of pre-trained VLA without any additional fine-tuning or data collection. We evaluate VLA-Pilot on six real-world downstream manipulation tasks across two distinct robotic embodiments, encompassing both in-distribution and out-of-distribution scenarios. Experimental results demonstrate that VLA-Pilot substantially boosts the success rates of off-the-shelf pre-trained VLA policies, enabling robust zero-shot generalization to diverse tasks and embodiments. Experimental videos and code are available at: https://rip4kobe.github.io/vla-pilot/.

</details>


### [10] [Dual-Variable Force Characterisation method for Human-Robot Interaction in Wearable Robotics](https://arxiv.org/abs/2511.14327)
*Felipe Ballen-Moreno,Pasquale Ferrentino,Milan Amighi,Bram Vanderborght,Tom Verstraten*

Main category: cs.RO

TL;DR: 本研究提出一种双变量表征方法，通过正常和切向力来分析可穿戴机器人及其与人类肢体的物理交互，从而改善当前单变量拟合的局限性。


<details>
  <summary>Details</summary>
Motivation: 理解可穿戴机器人与人体的物理交互对于确保安全和舒适至关重要，现有方法在多自由度交互下存在局限性。

Method: 提出了一种涉及法向和切向力的双变量表征方法，以识别可靠的材料参数并评估单变量拟合对力和扭矩响应的影响。

Result: 通过分析不同场景和材料模型中的归一化均方误差（NMSE），该方法展示了将两个变量纳入表征过程的重要性。

Conclusion: 引入双变量表征方法可以更好地理解可穿戴机器人与人体之间的物理交互，特别是在压力分布和剪切应力方面。

Abstract: Understanding the physical interaction with wearable robots is essential to ensure safety and comfort. However, this interaction is complex in two key aspects: (1) the motion involved, and (2) the non-linear behaviour of soft tissues. Multiple approaches have been undertaken to better understand this interaction and to improve the quantitative metrics of physical interfaces or cuffs. As these two topics are closely interrelated, finite modelling and soft tissue characterisation offer valuable insights into pressure distribution and shear stress induced by the cuff. Nevertheless, current characterisation methods typically rely on a single fitting variable along one degree of freedom, which limits their applicability, given that interactions with wearable robots often involve multiple degrees of freedom. To address this limitation, this work introduces a dual-variable characterisation method, involving normal and tangential forces, aimed at identifying reliable material parameters and evaluating the impact of single-variable fitting on force and torque responses. This method demonstrates the importance of incorporating two variables into the characterisation process by analysing the normalized mean square error (NMSE) across different scenarios and material models, providing a foundation for simulation at the closest possible level, with a focus on the cuff and the human limb involved in the physical interaction between the user and the wearable robot.

</details>


### [11] [MA-SLAM: Active SLAM in Large-Scale Unknown Environment using Map Aware Deep Reinforcement Learning](https://arxiv.org/abs/2511.14330)
*Yizhen Yin,Yuhua Qi,Dapeng Feng,Hongbo Chen,Hongjun Ma,Jin Wu,Yi Jiang*

Main category: cs.RO

TL;DR: 本论文提出了一种新的主动SLAM系统MA-SLAM，利用深度强化学习和结构化地图表示解决了大规模环境下的高效探索问题，实验证明该方法在探索时间和距离上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 当前的主动SLAM方法在小规模且受控的环境中表现良好，但在大规模和多样化环境中存在探索时间长和发现路径次优的问题。

Method: 提出了一种基于深度强化学习（DRL）的地图感知主动SLAM系统，采用了新的结构化地图表示，通过离散化空间数据并整合边界点与历史轨迹来提升决策模块的输入信息。实现了先进的全局规划器以优化探索路径。

Result: 经过三种仿真环境的实验和在真实无人地面车辆（UGV）上的部署，验证了该方法在探索持续时间和距离方面都较现有方法有显著降低。

Conclusion: 我们的MA-SLAM系统在探索时间和距离上显著优于现有先进方法。

Abstract: Active Simultaneous Localization and Mapping (Active SLAM) involves the strategic planning and precise control of a robotic system's movement in order to construct a highly accurate and comprehensive representation of its surrounding environment, which has garnered significant attention within the research community. While the current methods demonstrate efficacy in small and controlled settings, they face challenges when applied to large-scale and diverse environments, marked by extended periods of exploration and suboptimal paths of discovery. In this paper, we propose MA-SLAM, a Map-Aware Active SLAM system based on Deep Reinforcement Learning (DRL), designed to address the challenge of efficient exploration in large-scale environments. In pursuit of this objective, we put forward a novel structured map representation. By discretizing the spatial data and integrating the boundary points and the historical trajectory, the structured map succinctly and effectively encapsulates the visited regions, thereby serving as input for the deep reinforcement learning based decision module. Instead of sequentially predicting the next action step within the decision module, we have implemented an advanced global planner to optimize the exploration path by leveraging long-range target points. We conducted experiments in three simulation environments and deployed in a real unmanned ground vehicle (UGV), the results demonstrate that our approach significantly reduces both the duration and distance of exploration compared with state-of-the-art methods.

</details>


### [12] [Simultaneous Localization and 3D-Semi Dense Mapping for Micro Drones Using Monocular Camera and Inertial Sensors](https://arxiv.org/abs/2511.14335)
*Jeryes Danial,Yosi Ben Asher,Itzik Klein*

Main category: cs.RO

TL;DR: 本研究提出了一种轻量级的单目SLAM系统，通过深度学习结合稀疏和密集特征优化，在资源受限环境中实现实时映射和导航。


<details>
  <summary>Details</summary>
Motivation: 现有的单目SLAM算法在精度和计算效率方面存在挑战，包括规模歧义和详细几何缺失，亟需一种更高效的解决方案。

Method: 结合稀疏关键点姿态估计和密集边缘重建，利用深度学习进行深度预测和边缘检测，随后通过优化来提高几何一致性。

Result: 该系统在DJI Tello无人机上实时运行，能够进行自主导航和障碍物避免，展示了在室内走廊和TUM RGBD数据集上的有效应用。

Conclusion: 本研究提出了一种新颖的轻量级单目SLAM系统，能够在实时条件下进行高效的映射和导航，特别适用于资源受限的环境。

Abstract: Monocular simultaneous localization and mapping (SLAM) algorithms estimate drone poses and build a 3D map using a single camera. Current algorithms include sparse methods that lack detailed geometry, while learning-driven approaches produce dense maps but are computationally intensive. Monocular SLAM also faces scale ambiguities, which affect its accuracy. To address these challenges, we propose an edge-aware lightweight monocular SLAM system combining sparse keypoint-based pose estimation with dense edge reconstruction. Our method employs deep learning-based depth prediction and edge detection, followed by optimization to refine keypoints and edges for geometric consistency, without relying on global loop closure or heavy neural computations. We fuse inertial data with vision by using an extended Kalman filter to resolve scale ambiguity and improve accuracy. The system operates in real time on low-power platforms, as demonstrated on a DJI Tello drone with a monocular camera and inertial sensors. In addition, we demonstrate robust autonomous navigation and obstacle avoidance in indoor corridors and on the TUM RGBD dataset. Our approach offers an effective, practical solution to real-time mapping and navigation in resource-constrained environments.

</details>


### [13] [Going Places: Place Recognition in Artificial and Natural Systems](https://arxiv.org/abs/2511.14341)
*Michael Milford,Tobias Fischer*

Main category: cs.RO

TL;DR: 本文综述了机器人、动物和人类在地点识别方面的研究，探讨了其编码和回忆方式，重点提出了统一的概念和面临的挑战，旨在促进人工定位的创新。


<details>
  <summary>Details</summary>
Motivation: 探索生物导航和自主系统中，地点识别的重要性及其实现方式。

Method: 通过合成机器人系统、动物研究和人类研究的发现，分析不同系统如何编码和回忆地点。

Result: 揭示了人工系统、动物和人类在地点识别机制上的接近解决方案，提出了一套统一的概念，并识别出如泛化、稳健性和环境变异性等主要挑战。

Conclusion: 本综述旨在通过整合机器人、动物和人类研究的发现，促进人工定位的创新，连接未来的人工地点识别系统与动物导航和人类空间认知研究的见解。

Abstract: Place recognition, the ability to identify previously visited locations, is critical for both biological navigation and autonomous systems. This review synthesizes findings from robotic systems, animal studies, and human research to explore how different systems encode and recall place. We examine the computational and representational strategies employed across artificial systems, animals, and humans, highlighting convergent solutions such as topological mapping, cue integration, and memory management. Animal systems reveal evolved mechanisms for multimodal navigation and environmental adaptation, while human studies provide unique insights into semantic place concepts, cultural influences, and introspective capabilities. Artificial systems showcase scalable architectures and data-driven models. We propose a unifying set of concepts by which to consider and develop place recognition mechanisms and identify key challenges such as generalization, robustness, and environmental variability. This review aims to foster innovations in artificial localization by connecting future developments in artificial place recognition systems to insights from both animal navigation research and human spatial cognition studies.

</details>


### [14] [Perception-aware Exploration for Consumer-grade UAVs](https://arxiv.org/abs/2511.14393)
*Svetlana Seliunina,Daniel Schleich,Sven Behnke*

Main category: cs.RO

TL;DR: 本文提出了一种针对消费级无人机的多无人机自主探索方法，有效解决了环境探索和地图重建问题。


<details>
  <summary>Details</summary>
Motivation: 将最先进的无人机自主探索方法扩展到消费级无人机，提升其在实际应用中的可行性。

Method: 通过选择视点对以估计深度，并规划满足运动约束的轨迹，来扩展多无人机探索的方法。

Result: 在不同数量的无人机的仿真评估中，模型表现出色，能够有效地分配工作负载。

Conclusion: 该模型能够在消费者级无人机的硬件限制下安全地探索环境并重建地图。

Abstract: In our work, we extend the current state-of-the-art approach for autonomous multi-UAV exploration to consumer-level UAVs, such as the DJI Mini 3 Pro. We propose a pipeline that selects viewpoint pairs from which the depth can be estimated and plans the trajectory that satisfies motion constraints necessary for odometry estimation. For the multi-UAV exploration, we propose a semi-distributed communication scheme that distributes the workload in a balanced manner. We evaluate our model performance in simulation for different numbers of UAVs and prove its ability to safely explore the environment and reconstruct the map even with the hardware limitations of consumer-grade UAVs.

</details>


### [15] [Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning](https://arxiv.org/abs/2511.14396)
*Xiuxiu Qi,Yu Yang,Jiannong Cao,Luyao Bai,Chongshan Fan,Chengtai Cao,Hongpeng Wang*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的行为克隆框架CCoL，通过持续的视觉、语言和内部状态共学习，克服了物理不连续性和语义-物理不对齐，从而提升人机交互的性能。


<details>
  <summary>Details</summary>
Motivation: 应对行为克隆在序列动作决策中积累误差的挑战，并解决现有方法中的物理不连续性和语义-物理不对齐问题。

Method: 提出了连续视觉-语言-动作共学习的框架，通过跨注意力机制将语言语义锚定到视觉运动表示上，克服了语义-物理不对齐问题。

Result: CCoL在三个仿真环境中的平均相对提升为8.0%，在人工示范的双手插入任务中最高达19.2%的相对增益。

Conclusion: CCoL显著提高了行为克隆的性能，确保了时间上一致的执行和细粒度的语义对齐，并在真实世界中验证其在未知和嘈杂对象状态下的广泛适用性。

Abstract: Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (e.g., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL's generalization under unseen and noisy object states.

</details>


### [16] [Self-Supervised Multisensory Pretraining for Contact-Rich Robot Reinforcement Learning](https://arxiv.org/abs/2511.14427)
*Rickmer Krohn,Vignesh Prasad,Gabriele Tiboni,Georgia Chalvatzaki*

Main category: cs.RO

TL;DR: 提出MSDP框架，有效提升机器人在多感官环境中训练表现，且在多种复杂操作任务中表现卓越，学习速度快，对扰动鲁棒性强。


<details>
  <summary>Details</summary>
Motivation: 有效的接触丰富操控需要机器人协同利用视觉、力量和本体感知，但强化学习在多种感官环境中学习时困难重重。

Method: 提出了一种异步架构的策略学习方法，结合了跨注意力机制来提取动态任务特征，并为行动提供稳定的表示。

Result: MSDP方法展示了在多样扰动下加速学习和稳健表现，在真实机器人上仅需6000次在线交互就能达到高成功率。

Conclusion: MSDP方法在多种挑战性的接触丰富机器人操作任务中展现出高效性和鲁棒性，尤其是在面对传感器噪声和物体动态变化时。

Abstract: Effective contact-rich manipulation requires robots to synergistically leverage vision, force, and proprioception. However, Reinforcement Learning agents struggle to learn in such multisensory settings, especially amidst sensory noise and dynamic changes. We propose MultiSensory Dynamic Pretraining (MSDP), a novel framework for learning expressive multisensory representations tailored for task-oriented policy learning. MSDP is based on masked autoencoding and trains a transformer-based encoder by reconstructing multisensory observations from only a subset of sensor embeddings, leading to cross-modal prediction and sensor fusion. For downstream policy learning, we introduce a novel asymmetric architecture, where a cross-attention mechanism allows the critic to extract dynamic, task-specific features from the frozen embeddings, while the actor receives a stable pooled representation to guide its actions. Our method demonstrates accelerated learning and robust performance under diverse perturbations, including sensor noise, and changes in object dynamics. Evaluations in multiple challenging, contact-rich robot manipulation tasks in simulation and the real world showcase the effectiveness of MSDP. Our approach exhibits strong robustness to perturbations and achieves high success rates on the real robot with as few as 6,000 online interactions, offering a simple yet powerful solution for complex multisensory robotic control.

</details>


### [17] [Mutation Testing for Industrial Robotic Systems](https://arxiv.org/abs/2511.14432)
*Marcela Gonçalves dos Santos,Sylvain Hallé,Fábio Petrillo*

Main category: cs.RO

TL;DR: 本论文探讨了针对工业机器人系统的突变测试方法，通过引入特定的突变操作符，提高了测试套件的有效性和机器人系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着工业机器人系统在多样化环境中的应用加剧，确保其软件的可靠性变得至关重要。传统的突变测试方法不适用于机器人程序，因此需要适应性调整。

Method: 通过定义特定于领域的突变操作符，针对机器人的高层读写操作生成有效的突变体。

Result: 在一个挑取和放置的场景中，实证研究显示，该方法生成的突变体信息量更大，相较传统操作符减少了无效或等价案例的数量。

Conclusion: 本研究表明，面向工业机器人系统的突变测试方法能够提高测试套件的质量，从而增强工业机器人系统的安全性和可靠性。

Abstract: Industrial robotic systems (IRS) are increasingly deployed in diverse environments, where failures can result in severe accidents and costly downtime. Ensuring the reliability of the software controlling these systems is therefore critical. Mutation testing, a technique widely used in software engineering, evaluates the effectiveness of test suites by introducing small faults, or mutants, into the code. However, traditional mutation operators are poorly suited to robotic programs, which involve message-based commands and interactions with the physical world. This paper explores the adaptation of mutation testing to IRS by defining domain-specific mutation operators that capture the semantics of robot actions and sensor readings. We propose a methodology for generating meaningful mutants at the level of high-level read and write operations, including movement, gripper actions, and sensor noise injection. An empirical study on a pick-and-place scenario demonstrates that our approach produces more informative mutants and reduces the number of invalid or equivalent cases compared to conventional operators. Results highlight the potential of mutation testing to enhance test suite quality and contribute to safer, more reliable industrial robotic systems.

</details>


### [18] [Achieving Safe Control Online through Integration of Harmonic Control Lyapunov-Barrier Functions with Unsafe Object-Centric Action Policies](https://arxiv.org/abs/2511.14434)
*Marlow Fawn,Matthias Scheutz*

Main category: cs.RO

TL;DR: 提出一种结合谐波控制李雅普诺夫-障碍函数与机器人策略的方法，以确保机器人安全执行任务。


<details>
  <summary>Details</summary>
Motivation: 确保机器人在执行任务时的安全性，同时保持任务驱动行为。

Method: 通过结合基于信号时序逻辑（STL）规范的谐波控制李雅普诺夫-障碍函数（HCLBF）与任何给定的机器人策略，利用HCLBF派生的安全证书来生成命令。

Result: 通过一个简单的概念验证实现，展示了该方法在静态机器人臂移动任务中避免碰撞的效果。

Conclusion: 提出的方法可以将不安全的机器人策略转变为安全的策略，并具有正式保证。

Abstract: We propose a method for combining Harmonic Control Lyapunov-Barrier Functions (HCLBFs) derived from Signal Temporal Logic (STL) specifications with any given robot policy to turn an unsafe policy into a safe one with formal guarantees.  The two components are combined via HCLBF-derived safety certificates, thus producing commands that preserve both safety and task-driven behavior.  We demonstrate with a simple proof-of-concept implementation for an object-centric force-based policy trained through reinforcement learning for a movement task of a stationary robot arm that is able to avoid colliding with obstacles on a table top after combining the policy with the safety constraints.  The proposed method can be generalized to more complex specifications and dynamic task settings.

</details>


### [19] [Advancing Minimally Invasive Precision Surgery in Open Cavities with Robotic Flexible Endoscopy](https://arxiv.org/abs/2511.14458)
*Michelle Mattille,Alexandre Mesot,Miriam Weisskopf,Nicole Ochsenbein-Kölble,Ueli Moehrlen,Bradley J. Nelson,Quentin Boehler*

Main category: cs.RO

TL;DR: 研究提出了一种新的机器人平台，旨在提升微创手术在开放腔体的应用能力，通过结合柔性内窥镜和创新的导航技术，提高手术的灵活性和安全性。


<details>
  <summary>Details</summary>
Motivation: 旨在提升微创手术中的灵活性和控制精度，克服当前技术在开放腔体干预中的挑战。

Method: 结合磁力驱动的柔性内窥镜与遥操作和半自主导航能力，执行定向激光烧灼。

Result: 该系统经过在羊模型中的在体验证，能够有效应对微创手术的关键限制。

Conclusion: 该机器人平台在复杂的胎内激光凝固手术中展现了其潜力，解决了开放腔体内微创手术的关键限制。

Abstract: Flexible robots hold great promise for enhancing minimally invasive surgery (MIS) by providing superior dexterity, precise control, and safe tissue interaction. Yet, translating these advantages into endoscopic interventions within open cavities remains challenging. The lack of anatomical constraints and the inherent flexibility of such devices complicate their control, while the limited field of view of endoscopes restricts situational awareness. We present a robotic platform designed to overcome these challenges and demonstrate its potential in fetoscopic laser coagulation, a complex MIS procedure typically performed only by highly experienced surgeons. Our system combines a magnetically actuated flexible endoscope with teleoperated and semi-autonomous navigation capabilities for performing targeted laser ablations. To enhance surgical awareness, the platform reconstructs real-time mosaics of the endoscopic scene, providing an extended and continuous visual context. The ability of this system to address the key limitations of MIS in open spaces is validated in vivo in an ovine model.

</details>


### [20] [Aerial Assistance System for Automated Firefighting during Turntable Ladder Operations](https://arxiv.org/abs/2511.14504)
*Jan Quenzel,Valerij Sekin,Daniel Schleich,Alexander Miller,Merlin Stampa,Norbert Pahlke,Christof Röhrig,Sven Behnke*

Main category: cs.RO

TL;DR: 本论文提出了一种自动化灭火辅助系统，结合电动喷嘴和无人机，以提高工业火灾的灭火精度与效率。


<details>
  <summary>Details</summary>
Motivation: 工业设施的火灾对消防员构成特殊挑战，建筑物的规模和视觉障碍导致灭火精度降低，增加了损害并延长了消防队的作业时间。

Method: 使用电动喷嘴与无人机的组合，建立一个基于地理数据的障碍物-free飞行通道，来自动定位与跟踪火源。操作员通过手持控制器监控操作，并能选择可达到的灭火目标。

Result: 在初步测试中，系统成功定位多个热源并指引水喷嘴实施灭火。

Conclusion: 该自动化辅助系统能够有效定位多个热源并成功指引水喷嘴向火源喷射水流，提升了灭火精度与效率。

Abstract: Fires in industrial facilities pose special challenges to firefighters, e.g., due to the sheer size and scale of the buildings. The resulting visual obstructions impair firefighting accuracy, further compounded by inaccurate assessments of the fire's location. Such imprecision simultaneously increases the overall damage and prolongs the fire-brigades operation unnecessarily.
  We propose an automated assistance system for firefighting using a motorized fire monitor on a turntable ladder with aerial support from an unmanned aerial vehicle (UAV). The UAV flies autonomously within an obstacle-free flight funnel derived from geodata, detecting and localizing heat sources. An operator supervises the operation on a handheld controller and selects a fire target in reach. After the selection, the UAV automatically plans and traverses between two triangulation poses for continued fire localization. Simultaneously, our system steers the fire monitor to ensure the water jet reaches the detected heat source. In preliminary tests, our assistance system successfully localized multiple heat sources and directed a water jet towards the fires.

</details>


### [21] [Masked IRL: LLM-Guided Reward Disambiguation from Demonstrations and Language](https://arxiv.org/abs/2511.14565)
*Minyoung Hwang,Alexandra Forsey-Smerek,Nathaniel Dennler,Andreea Bobu*

Main category: cs.RO

TL;DR: 该论文提出了夹层逆强化学习（Masked IRL），结合演示和语言指令在奖励建模中的优势，显著提高样本效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基于语言的奖励学习方法未能充分利用指令的潜力来解决模糊性问题，而模糊的指令使得简单的条件化方法不可靠。

Method: 提出了一种名为 Masked Inverse Reinforcement Learning (Masked IRL) 的框架，该框架利用大型语言模型（LLMs）结合演示和语言指令的优势，推断状态相关性掩码，并在模糊指令的上下文中进行推理以澄清指令。

Result: Masked IRL 在模拟和真实机器人上优于先前的语言条件逆强化学习方法，高达 15% 的提升，同时使用的数据量减少4.7倍。

Conclusion: Masked IRL 在多个实验中表现优于之前的语言条件逆强化学习方法，达到了提高样本效率、泛化能力和对模糊语言的鲁棒性。

Abstract: Robots can adapt to user preferences by learning reward functions from demonstrations, but with limited data, reward models often overfit to spurious correlations and fail to generalize. This happens because demonstrations show robots how to do a task but not what matters for that task, causing the model to focus on irrelevant state details. Natural language can more directly specify what the robot should focus on, and, in principle, disambiguate between many reward functions consistent with the demonstrations. However, existing language-conditioned reward learning methods typically treat instructions as simple conditioning signals, without fully exploiting their potential to resolve ambiguity. Moreover, real instructions are often ambiguous themselves, so naive conditioning is unreliable. Our key insight is that these two input types carry complementary information: demonstrations show how to act, while language specifies what is important. We propose Masked Inverse Reinforcement Learning (Masked IRL), a framework that uses large language models (LLMs) to combine the strengths of both input types. Masked IRL infers state-relevance masks from language instructions and enforces invariance to irrelevant state components. When instructions are ambiguous, it uses LLM reasoning to clarify them in the context of the demonstrations. In simulation and on a real robot, Masked IRL outperforms prior language-conditioned IRL methods by up to 15% while using up to 4.7 times less data, demonstrating improved sample-efficiency, generalization, and robustness to ambiguous language. Project page: https://MIT-CLEAR-Lab.github.io/Masked-IRL and Code: https://github.com/MIT-CLEAR-Lab/Masked-IRL

</details>


### [22] [Is Your VLM for Autonomous Driving Safety-Ready? A Comprehensive Benchmark for Evaluating External and In-Cabin Risks](https://arxiv.org/abs/2511.14592)
*Xianhui Meng,Yuchen Zhang,Zhijian Huang,Zheng Lu,Ziling Ji,Yaoyao Yin,Hongyuan Zhang,Guangfeng Jiang,Yandan Lin,Long Chen,Hangjun Ye,Li Zhang,Jun Liu,Xiaoshuai Hao*

Main category: cs.RO

TL;DR: 本文提出DSBench基准，以评估视觉-语言模型在安全关键场景中的表现，并通过构建数据集提高其安全性。


<details>
  <summary>Details</summary>
Motivation: 探索VLMs在安全关键场景中的适用性，解决当前缺乏评估环境风险和车内安全行为的基准问题。

Method: 引入DSBench基准，通过评估外部环境风险和车内驾驶行为安全，构建包含98K实例的大型数据集并对现有VLMs进行微调以提高其安全性能。

Result: 评估显示在复杂安全关键情境下，VLMs性能显著下降；通过微调数据集，安全性能显著提升。

Conclusion: DSBench是首个综合驾驶安全基准，可显著提升VLMs在安全关键场景中的表现。

Abstract: Vision-Language Models (VLMs) show great promise for autonomous driving, but their suitability for safety-critical scenarios is largely unexplored, raising safety concerns. This issue arises from the lack of comprehensive benchmarks that assess both external environmental risks and in-cabin driving behavior safety simultaneously. To bridge this critical gap, we introduce DSBench, the first comprehensive Driving Safety Benchmark designed to assess a VLM's awareness of various safety risks in a unified manner. DSBench encompasses two major categories: external environmental risks and in-cabin driving behavior safety, divided into 10 key categories and a total of 28 sub-categories. This comprehensive evaluation covers a wide range of scenarios, ensuring a thorough assessment of VLMs' performance in safety-critical contexts. Extensive evaluations across various mainstream open-source and closed-source VLMs reveal significant performance degradation under complex safety-critical situations, highlighting urgent safety concerns. To address this, we constructed a large dataset of 98K instances focused on in-cabin and external safety scenarios, showing that fine-tuning on this dataset significantly enhances the safety performance of existing VLMs and paves the way for advancing autonomous driving technology. The benchmark toolkit, code, and model checkpoints will be publicly accessible.

</details>


### [23] [Gallant: Voxel Grid-based Humanoid Locomotion and Local-navigation across 3D Constrained Terrains](https://arxiv.org/abs/2511.14625)
*Qingwei Ben,Botian Xu,Kailin Li,Feiyu Jia,Wentao Zhang,Jingping Wang,Jingbo Wang,Dahua Lin,Jiangmiao Pang*

Main category: cs.RO

TL;DR: 本论文提出Gallant，一个基于体素网格的框架，通过改进的端到端优化，实现了机器人在复杂3D环境中的高效导航和近100%的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的感知模块只能提供部分和局部平坦的环境视图，无法有效捕捉完整的3D结构。

Method: 基于体素网格框架，利用体素化的LiDAR数据和z分组的2D CNN进行控制策略映射，支持端到端优化。

Result: Gallant扩大了感知覆盖范围，使得单一策略能够处理更复杂的环境，包括多个高度的障碍物和窄通道。

Conclusion: Gallant实现了机器人在复杂环境中的高效导航，成功率接近100%。

Abstract: Robust humanoid locomotion requires accurate and globally consistent perception of the surrounding 3D environment. However, existing perception modules, mainly based on depth images or elevation maps, offer only partial and locally flattened views of the environment, failing to capture the full 3D structure. This paper presents Gallant, a voxel-grid-based framework for humanoid locomotion and local navigation in 3D constrained terrains. It leverages voxelized LiDAR data as a lightweight and structured perceptual representation, and employs a z-grouped 2D CNN to map this representation to the control policy, enabling fully end-to-end optimization. A high-fidelity LiDAR simulation that dynamically generates realistic observations is developed to support scalable, LiDAR-based training and ensure sim-to-real consistency. Experimental results show that Gallant's broader perceptual coverage facilitates the use of a single policy that goes beyond the limitations of previous methods confined to ground-level obstacles, extending to lateral clutter, overhead constraints, multi-level structures, and narrow passages. Gallant also firstly achieves near 100% success rates in challenging scenarios such as stair climbing and stepping onto elevated platforms through improved end-to-end optimization.

</details>


### [24] [NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards](https://arxiv.org/abs/2511.14659)
*Chia-Yu Hung,Navonil Majumder,Haoyuan Deng,Liu Renhang,Yankang Ang,Amir Zadeh,Chuan Li,Dorien Herremans,Ziwei Wang,Soujanya Poria*

Main category: cs.RO

TL;DR: NORA-1.5是一种改进的VLA模型，通过架构增强和基于奖励的后期训练显著提升了在各种体现任务中的表现，尤其在真实世界环境中显示了更高的可靠性。


<details>
  <summary>Details</summary>
Motivation: 虽然现有的VLA模型在多种体感任务上取得了良好表现，但在不同的体现或真实环境中仍存在可靠性和泛化能力不足的问题。

Method: 引入NORA-1.5模型，该模型基于预训练的NORA骨干，通过添加流匹配的动作专家进行架构增强，同时开发了基于奖励的后期训练策略。

Result: NORA-1.5在模拟和现实世界基准测试中均优于NORA及其他多种先进的VLA模型，并且通过奖励驱动的后期训练，进一步提升了模型在这些设置中的表现。

Conclusion: NORA-1.5及其基于奖励的后期训练方法显著增强了VLA模型的可靠性，使其更适合真实世界的应用。

Abstract: Vision--language--action (VLA) models have recently shown promising performance on a variety of embodied tasks, yet they still fall short in reliability and generalization, especially when deployed across different embodiments or real-world environments. In this work, we introduce NORA-1.5, a VLA model built from the pre-trained NORA backbone by adding to it a flow-matching-based action expert. This architectural enhancement alone yields substantial performance gains, enabling NORA-1.5 to outperform NORA and several state-of-the-art VLA models across both simulated and real-world benchmarks. To further improve robustness and task success, we develop a set of reward models for post-training VLA policies. Our rewards combine (i) an action-conditioned world model (WM) that evaluates whether generated actions lead toward the desired goal, and (ii) a deviation-from-ground-truth heuristic that distinguishes good actions from poor ones. Using these reward signals, we construct preference datasets and adapt NORA-1.5 to target embodiments through direct preference optimization (DPO). Extensive evaluations show that reward-driven post-training consistently improves performance in both simulation and real-robot settings, demonstrating significant VLA model-reliability gains through simple yet effective reward models. Our findings highlight NORA-1.5 and reward-guided post-training as a viable path toward more dependable embodied agents suitable for real-world deployment.

</details>


### [25] [Robust Verification of Controllers under State Uncertainty via Hamilton-Jacobi Reachability Analysis](https://arxiv.org/abs/2511.14755)
*Albert Lin,Alessandro Pinto,Somil Bansal*

Main category: cs.RO

TL;DR: 本研究提出RoVer-CoRe框架，通过HJ可达性分析形式验证感知系统的安全性，解决了感知不确定性带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着感知控制器在自动化系统中的普及，需要对它们在感知不确定性下的安全性和性能进行正式验证。

Method: 提出RoVer-CoRe框架，通过将控制器、观察函数和状态估计模块连接，形成一个兼容于现有可达性框架的闭环系统，再通过这种框架进行形式安全验证和鲁棒控制器设计。

Result: RoVer-CoRe框架在飞机滑行和基于神经网络的探测器导航的案例研究中展示了其有效性。

Conclusion: RoVer-CoRe是首个基于HJ可达性的方法，用于在感知不确定性下验证控制器的安全性和性能。

Abstract: As perception-based controllers for autonomous systems become increasingly popular in the real world, it is important that we can formally verify their safety and performance despite perceptual uncertainty. Unfortunately, the verification of such systems remains challenging, largely due to the complexity of the controllers, which are often nonlinear, nonconvex, learning-based, and/or black-box. Prior works propose verification algorithms that are based on approximate reachability methods, but they often restrict the class of controllers and systems that can be handled or result in overly conservative analyses. Hamilton-Jacobi (HJ) reachability analysis is a popular formal verification tool for general nonlinear systems that can compute optimal reachable sets under worst-case system uncertainties; however, its application to perception-based systems is currently underexplored. In this work, we propose RoVer-CoRe, a framework for the Robust Verification of Controllers via HJ Reachability. To the best of our knowledge, RoVer-CoRe is the first HJ reachability-based framework for the verification of perception-based systems under perceptual uncertainty. Our key insight is to concatenate the system controller, observation function, and the state estimation modules to obtain an equivalent closed-loop system that is readily compatible with existing reachability frameworks. Within RoVer-CoRe, we propose novel methods for formal safety verification and robust controller design. We demonstrate the efficacy of the framework in case studies involving aircraft taxiing and NN-based rover navigation. Code is available at the link in the footnote.

</details>


### [26] [HMC: Learning Heterogeneous Meta-Control for Contact-Rich Loco-Manipulation](https://arxiv.org/abs/2511.14756)
*Lai Wei,Xuanbin Peng,Ri-Zhao Qiu,Tianshu Huang,Xuxin Cheng,Xiaolong Wang*

Main category: cs.RO

TL;DR: 提出了Heterogeneous Meta-Control (HMC)框架，通过融合多种控制模式，显著提升了机器人在复杂任务中的性能，适应性强。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统位置控制器在复杂环境中与接触和变负载的交互问题，提出了一种能够适应多种控制模式的框架。

Method: 通过HMC-Controller实现不同控制模式的连续混合，并采用HMC-Policy统一多种控制器，学习强大的力觉策略。

Result: 实验表明，HMC相较于基线在如表面擦拭和抽屉打开等复杂任务上实现了超过50%的相对改善。

Conclusion: Heterogeneous Meta-Control (HMC)框架在复杂任务中表现出显著的性能提升，证明了其有效性。

Abstract: Learning from real-world robot demonstrations holds promise for interacting with complex real-world environments. However, the complexity and variability of interaction dynamics often cause purely positional controllers to struggle with contacts or varying payloads. To address this, we propose a Heterogeneous Meta-Control (HMC) framework for Loco-Manipulation that adaptively stitches multiple control modalities: position, impedance, and hybrid force-position. We first introduce an interface, HMC-Controller, for blending actions from different control profiles continuously in the torque space. HMC-Controller facilitates both teleoperation and policy deployment. Then, to learn a robust force-aware policy, we propose HMC-Policy to unify different controllers into a heterogeneous architecture. We adopt a mixture-of-experts style routing to learn from large-scale position-only data and fine-grained force-aware demonstrations. Experiments on a real humanoid robot show over 50% relative improvement vs. baselines on challenging tasks such as compliant table wiping and drawer opening, demonstrating the efficacy of HMC.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [27] [Enhancing Decision Support in Construction through Industrial AI](https://arxiv.org/abs/2511.13910)
*Parul Khanna,Sameer Prabhu,Ramin Karim,Phillip Tretten*

Main category: cs.HC

TL;DR: 本研究探讨了建筑行业中与人工智能解决方案相关的关键因素，重点在于提升决策支持和用户体验。


<details>
  <summary>Details</summary>
Motivation: 了解与人工智能解决方案互动的最终用户所面临的需求和挑战，以提高这些系统的有效性和可用性。

Method: 通过开发示范工具并采集针对最终用户的问卷反馈，重点是现场经理和建筑专业人员。

Result: 确定了建筑行业中开发人工智能解决方案的关键因素及其相互关系。

Conclusion: 本研究为建筑行业的人工智能解决方案开发提供了见解，特别关注人机交互和决策支持的提升。

Abstract: The construction industry is presently going through a transformation led by adopting digital technologies that leverage Artificial Intelligence (AI). These industrial AI solutions assist in various phases of the construction process, including planning, design, production and management. In particular, the production phase offers unique potential for the integration of such AI-based solutions. These AI-based solutions assist site managers, project engineers, coordinators and other key roles in making final decisions. To facilitate the decision-making process in the production phase of construction through a human-centric AI-based solution, it is important to understand the needs and challenges faced by the end users who interact with these AI-based solutions to enhance the effectiveness and usability of these systems. Without this understanding, the potential usage of these AI-based solutions may be limited. Hence, the purpose of this research study is to explore, identify and describe the key factors crucial for developing AI solutions in the construction industry. This study further identifies the correlation between these key factors. This was done by developing a demonstrator and collecting quantifiable feedback through a questionnaire targeting the end users, such as site managers and construction professionals. This research study will offer insights into developing and improving these industrial AI solutions, focusing on Human-System Interaction aspects to enhance decision support, usability, and overall AI solution adoption.

</details>


### [28] [Human-centric Maintenance Process Through Integration of AI, Speech, and AR](https://arxiv.org/abs/2511.13918)
*Parul Khanna,Ravdeep Kour,Ramin Karim*

Main category: cs.HC

TL;DR: 本研究探索了增强现实（AR）如何通过集成人工智能和语音处理技术，支持工业维护中的免手操作与实时任务记录。


<details>
  <summary>Details</summary>
Motivation: 研究AR与AI及语音处理技术的整合，以支持维护环境中的免手操作和实时任务记录。

Method: 通过开发用于Microsoft HoloLens 2的演示系统，利用Unity、C#、Azure认知服务和Azure功能实现语音转文本的实时交互。

Result: 研究证明，AR环境中语音识别准确性高，能够实现用户与增强环境之间的自然交互。

Conclusion: AR技术在工业维护中可以有效降低认知负荷，提高工作流程的效率与安全性。

Abstract: The adoption of Augmented Reality (AR) is increasing to enhance Human-System Interaction (HSI) by creating immersive experiences that improve efficiency and safety in various industries. In industrial maintenance, traditional practices involve physical documentation and device interactions, which might disrupt the task, affect efficiency, and increase the cognitive load for the maintenance personnel. AR has the potential to support and enhance industrial maintenance processes in these aspects. Therefore, the purpose of this research is to study and explore how advanced technologies like Artificial Intelligence (AI), AR and speech processing can be integrated to support hands-free, real-time task logging and interaction in maintenance environments. This is done by developing a demonstrator for Microsoft HoloLens 2 using Unity, C#, Azure Cognitive Services, and Azure Functions, which enables speech-to-text transcription for hands-free maintenance support. Using Azures' speech recognition, the demonstrator can achieve high transcription accuracy in an AR environment, facilitating natural interactions between users and the augmented environment. The study aims to explore the potential of AR to reduce cognitive load, streamline workflows, and improve safety by enhancing HSI for maintenance personnel in high-stakes environments.

</details>


### [29] [Personality Pairing Improves Human-AI Collaboration](https://arxiv.org/abs/2511.13979)
*Harang Ju,Sinan Aral*

Main category: cs.HC

TL;DR: 本研究探讨人类与AI个性的互动，发现个性匹配显著提升合作效率和广告质量，未来需深入研究AI个性化对人-机合作的影响。


<details>
  <summary>Details</summary>
Motivation: 探究AI代理的'个性'与人类个性之间的互动，以及它们如何影响人类与AI的协作、生产力和表现。

Method: 通过一项大规模的随机实验，将1258名参与者与不同个性特征的AI代理配对，评估了团队广告的质量和表现。

Result: 发现个性匹配影响团队合作质量和生产力表现，某些配对组合能提高广告质量，并显示出不同任务对匹配的敏感性。

Conclusion: 人类与AI的性格匹配显著提高了合作、生产力和表现，为未来的个性化AI研究奠定了基础。

Abstract: Here we ask how AI agent "personalities" interact with human personalities, and other traits, to shape human-AI collaboration, productivity and performance. To estimate these relationships, we conducted a large-scale preregistered randomized experiment that paired 1,258 participants with AI agents that were prompted to exhibit varying levels of the Big Five personality traits. These human-AI teams produced 7,266 display ads for a real think tank, and the quality of these ads was evaluated by 1,995 independent human raters as well as in a field experiment conducted on X, which generated nearly 5 million impressions. We found, first, that personality pairing impacted teamwork quality. For example, neurotic AI improved teamwork for agreeable humans but impaired it for conscientious humans. Second, we found productivity effects of personality pairing and a "productivity-performance trade-off" in which certain pairings (e.g., agreeable human with neurotic AI) produced fewer ads but of higher quality. Third, personality pairing influenced ad quality and performance. For example, quality improved when open humans were paired with conscientious AI and when conscientious humans were paired with disagreeable AI. Some of these pairing effects were "jagged" in that they varied across text and visual tasks. For example open humans produced higher quality images but lower quality text when paired with agreeable AI. Pairing effects were also present in other human traits, like country of origin. For example, extroverted AI improved quality for Latin American workers, but degraded quality for East Asian workers. These findings demonstrate that human-AI personality alignment significantly improves collaboration, productivity, and performance and lay a foundation for future research on improving human-AI collaboration through AI personalization.

</details>


### [30] [Affective Color Scales for Colormap Data Visualizations](https://arxiv.org/abs/2511.14009)
*Halle C. Braun,Kushin Mukherjee,Seth R. Gorelik,Karen B. Schloss*

Main category: cs.HC

TL;DR: 本研究探讨了情感可视化设计中颜色的重要性，提出设计应考虑色彩与数据特性的结合，以实现空间视觉与情感传达的平衡。


<details>
  <summary>Details</summary>
Motivation: 探讨如何在数据可视化中平衡情感传达与空间模式的表现，解决设计中的色彩与光亮度对比问题。

Method: 通过设计具有强光亮对比度的颜色映射，分析其在空间视觉和情感传达中的表现。

Result: 研究发现，能够设计出既支持空间视觉又具情感表达的颜色映射，且情感含义与颜色出现的频率及其数据依赖性相关。

Conclusion: 本研究强调了数据感知设计的重要性，认为在设计可视化时应考虑数据特性与设计元素的结合。

Abstract: Research on affective visualization design has shown that color is an especially powerful feature for influencing the emotional connotation of visualizations. Associations between colors and emotions are largely driven by lightness (e.g., lighter colors are associated with positive emotions, whereas darker colors are associated with negative emotions). Designing visualizations to have all light or all dark colors to convey particular emotions may work well for visualizations in which colors represent categories and spatial channels encode data values. However, this approach poses a problem for visualizations that use color to represent spatial patterns in data (e.g., colormap data visualizations) because lightness contrast is needed to reveal fine details in spatial structure. In this study, we found it is possible to design colormaps that have strong lightness contrast to support spatial vision while communicating clear affective connotation. We also found that affective connotation depended not only on the color scales used to construct the colormaps, but also the frequency with which colors appeared in the map, as determined by the underlying dataset (data-dependence hypothesis). These results emphasize the importance of data-aware design, which accounts for not only the design features that encode data (e.g., colors, shapes, textures), but also how those design features are instantiated in a visualization, given the properties of the data.

</details>


### [31] [Developing a Grounded View of AI](https://arxiv.org/abs/2511.14013)
*Bifei Mao,Lanqing Hong*

Main category: cs.HC

TL;DR: 本文探讨了AI与基于规则的软件之间的基本区别，提出了一种方法论以理解AI的决策行为，并强调人类在使用AI时的责任。


<details>
  <summary>Details</summary>
Motivation: 探索AI与基于规则的软件程序的根本区别，强调人类在使用AI时需要负起责任。

Method: 提出了一种方法论，旨在辨别AI模型行为的不同决策类型。

Result: 通过识别规则的局限性，提出了对AI行为的理解，并为人类未来与AI的互动提供了基础。

Conclusion: 为了确保AI系统对人类、社会和环境的良好影响，需要理解AI行为及其与基于规则的软件程序之间的基本区别。

Abstract: As a capability coming from computation, how does AI differ fundamentally from the capabilities delivered by rule-based software program? The paper examines the behavior of artificial intelligence (AI) from engineering points of view to clarify its nature and limits. The paper argues that the rationality underlying humanity's impulse to pursue, articulate, and adhere to rules deserves to be valued and preserved. Identifying where rule-based practical rationality ends is the beginning of making it aware until action. Although the rules of AI behaviors are still hidden or only weakly observable, the paper has proposed a methodology to make a sense of discrimination possible and practical to identify the distinctions of the behavior of AI models with three types of decisions. It is a prerequisite for human responsibilities with alternative possibilities, considering how and when to use AI. It would be a solid start for people to ensure AI system soundness for the well-being of humans, society, and the environment.

</details>


### [32] [Gamified Virtual Reality Exposure Therapy for Mysophobia: Evaluating the Efficacy of a Simulated Sneeze Intervention](https://arxiv.org/abs/2511.14118)
*Md Mosharaf Hossan,Rifat Ara Tasnim,Farjana Z Eishita*

Main category: cs.HC

TL;DR: 本研究探讨了基于虚拟现实的游戏对强迫症相关情感和心理影响的潜力，虽然结果未达到统计学显著性，但为未来更大样本的研究提供了基础。


<details>
  <summary>Details</summary>
Motivation: 调查游戏化虚拟现实干预对模拟污染相关场景的情感和心理影响，以更好地理解强迫症的表现。

Method: 开发了一款基于虚拟现实的游戏，通过喷嚏模拟来评估其对参与者情绪状态的影响，参与者完成了两个版本的游戏：基线版本和带喷嚏模拟的实验版本。

Result: 在喷嚏模拟期间，参与者的负面情感和焦虑水平略有增加，正面情感减少，但这些差异没有统计学意义（p > 0.05）。可能是由于样本量小、模拟的恶心程度不足或参与者不是临床的强迫症患者。

Conclusion: 这项研究强调了基于虚拟现实的干预措施在理解和应对与污染相关的焦虑方面的潜力，并为未来进行更大规模和更具多样性的参与者研究奠定了基础。

Abstract: Mysophobia, or the fear of germs, is a prevalent anxiety disorder that significantly impacts daily life. This study investigates the potential of a gamified virtual reality (VR) intervention to simulate contamination-related scenarios and assess their emotional and psychological effects. A VR game based sneeze simulation was developed to evaluate its influence on participants' emotional states. Seven participants completed two versions of the game: a baseline version and an experimental version featuring the sneeze simulation. Emotional responses were measured using the Positive and Negative Affect Schedule (PANAS) and State-Trait Anxiety Inventory - State (STAI-S) questionnaires. The results revealed slight increases in negative affect and anxiety levels during the sneeze simulation. Also, a reduction in positive affect was revealed. However, these differences were not statistically significant (p > 0.05). This is likely due to small sample sizes, a lack of grossness in the simulation, or participants not being clinically mysophobes. This exploratory study highlights the potential of VR-based interventions for understanding and addressing contamination-related anxieties. It provides a foundation for future research with larger and more diverse participant pools.

</details>


### [33] [Final Happiness: What Intelligent User Interfaces Can Do for the lonely Dying](https://arxiv.org/abs/2511.14164)
*Yibo Meng,Xiaolan Ding,Lyumanshan Ye,Zhiming Liu,Yan Guan*

Main category: cs.HC

TL;DR: 该研究探讨了设计智能用户界面以应对临终孤独感，强调设计应关注用户的心理和存在需求，建议IUIs应创造超越人类能力的体验。


<details>
  <summary>Details</summary>
Motivation: 旨在填补现有研究中对于面对死亡时孤独个体的主观存在需求的关注缺失。

Method: 通过对14名孤独的临终病患者进行深入的定性访谈。

Result: 提出了一个经验基础的模型，阐明了该群体复杂的心理、实践、社会和精神需求，并提出了“三级支柱，十二原则”的框架，以及一个设计指令，强调技术应追求超越而非模拟。

Conclusion: 研究表明，智能用户界面（IUIs）应当创造增强或超越人类能力的体验，而非仅仅模拟人际联系，以避免加深孤独感。

Abstract: This study explores the design of Intelligent User Interfaces (IUIs) to address the profound existential loneliness of terminally ill individuals. While Human-Computer Interaction (HCI) has made inroads in "Thanatechnology," current research often focuses on practical aspects like digital legacy management, overlooking the subjective, existential needs of those facing death in isolation. To address this gap, we conducted in-depth qualitative interviews with 14 lonely, terminally ill individuals. Our core contributions are: (1) An empirically-grounded model articulating the complex psychological, practical, social, and spiritual needs of this group; (2) The "Three Pillars, Twelve Principles" framework for designing IUIs as "Existential Companions"; and (3) A critical design directive derived from user evaluations: technology in this context should aim for transcendence over simulation. The findings suggest that IUIs should create experiences that augment or surpass human capabilities, rather than attempting to simulate basic human connections, which can paradoxically deepen loneliness. This research provides a clear, user-centered path for designing technology that serves not as a "tool for dying," but as a "partner for living fully until the end".

</details>


### [34] [A Longitudinal Study on the Attitudes of Gay Men in Beijing Towards Gay Social Media Platforms: Lonely Souls in the Digital Concrete Jungle](https://arxiv.org/abs/2511.14174)
*Yibo Meng,Rong Fu,Lyumanshan Ye,Zhiming Liu,Zhixin Cai,Xiaolan Ding,Yan Guan*

Main category: cs.HC

TL;DR: 本研究调查了2013至2023年间，中国男男性接触者对社交应用态度的变化，发现其经历了支持、批评到灵活使用的演变，指出未来数字环境设计的启示。


<details>
  <summary>Details</summary>
Motivation: 探讨2013至2023年间，中国男男性接触者（MSM）对专门社交网络应用态度的变化。

Method: 采用纵向混合方法，包括在线话语档案分析、对412名参与者的定量调查和对32名参与者的深入半结构访谈。

Result: 研究发现用户的态度经历了三个阶段，从最初的热情支持，经过逐渐的矛盾与批评，最终发展到灵活而批判性的使用方式。

Conclusion: 用户现在以战略性方式使用多样化的社交应用程序，以满足不同的需求，并推动了更加支持性和社区导向的数字环境的设计思考。

Abstract: Over the past decade, specialized social networking applications have become a cornerstone of life for many gay men in China. This paper employs a longitudinal mixed-methods approach to investigate how Chinese men who have sex with men (MSM) have shifted their attitudes toward these platforms between approximately 2013 and 2023. Drawing on archival analysis of online discourses, a quantitative survey of 412 participants, and in-depth semi-structured interviews with 32 participants, we trace the complex trajectory of this evolution. Our findings reveal a clear pattern: from the initial embrace of these applications as revolutionary tools for community building and identity affirmation (2014--2017), to a period of growing ambivalence and critique centered on commercialization, ``hookup culture,'' and multiple forms of discrimination (2017--2020), and finally to the present era (2020--2023), characterized by pragmatic, fragmented, yet simultaneously critical and reconstructive uses. Today, users strategically employ a repertoire of applications -- including global platforms (e.g., Grindr and Tinder), domestic mainstream platforms (e.g., Blued), and niche alternatives (e.g., Aloha) -- to fulfill differentiated needs. We develop a detailed temporal framework to capture this attitudinal evolution and discuss its design implications for creating more supportive, secure, and community-oriented digital environments for marginalized groups.

</details>


### [35] [Algorithmic Management and the Future of Human Work: Implications for Autonomy, Collaboration, and Innovation](https://arxiv.org/abs/2511.14231)
*Huram Konjen*

Main category: cs.HC

TL;DR: 本研究探讨算法管理对人力资源管理的影响，强调其在提高效率的同时，也可能加剧工作中的偏见，建议建立社会技术的算法问责体系，以支持员工的自主性。


<details>
  <summary>Details</summary>
Motivation: 探讨算法管理对人力资源管理实践的影响，特别是员工的自主性、程序透明度及绩效评估的社会技术动态。

Method: 通过概念整合HRM、人机交互(HCI)和科技研究的见解，而非定性或实证研究。

Result: 提出了一个社会技术视角的算法问责框架，强调程序透明度、组织公正和员工自治。

Conclusion: 研究强调了算法管理的双刃剑特性，其提高效率的同时，也可能加剧偏见并降低工作中的情感和合作贡献。

Abstract: This study examines the evolving impact of algorithmic management on human resource management (HRM) practices, with a focus on employee autonomy, procedural transparency, and the sociotechnical dynamics of performance evaluation. Rather than adopting a qualitative or empirical approach, the paper develops a conceptual integration of insights from HRM, human-computer interaction (HCI), and Science and Technology Studies. The analysis highlights that although algorithmic systems can enhance operational efficiency, they risk reinforcing biases and narrowing the relational and contextual dimensions of work. These systems often overlook intangible contributions such as creativity, empathy, and collaborative problem solving, revealing gaps in data-driven performance measurement. In response, the study proposes a sociotechnical perspective on algorithmic accountability that emphasizes procedural transparency, organizational justice, and employee agency. By revisiting foundational questions within the rapidly evolving landscape of algorithmic management, the paper contributes to ongoing debates about the future of work and the design of managerial technologies that support, rather than constrain, human autonomy and organizational life.

</details>


### [36] [Visionary Co-Driver: Enhancing Driver Perception of Potential Risks with LLM and HUD](https://arxiv.org/abs/2511.14233)
*Wei Xiang,Ziyue Lei,Jie Wang,Yingying Huang,Qi Zheng,Tianyi Zhang,An Zhao,Lingyun Sun*

Main category: cs.HC

TL;DR: 本论文提出了一种名为Visionary Co-Driver的系统，通过结合视频处理和大型语言模型，提升驾驶者在非碰撞情况下的风险认知。


<details>
  <summary>Details</summary>
Motivation: 现有的风险检测方法主要集中在碰撞识别上，无法有效评估非碰撞情况下的道路使用者行为，因此需要一种新的系统来解决这一问题。

Method: 结合视频处理算法和大型语言模型(LLMs)来识别潜在的路面风险，并通过自适应抬头显示界面动态指示这些风险。

Result: 用户研究表明，41名驾驶者使用Visionary Co-Driver后，其风险感知显著提升。

Conclusion: Visionary Co-Driver系统有效提升了驾驶者对路边风险的感知，支持了他们对非碰撞场景的风险识别。

Abstract: Drivers' perception of risky situations has always been a challenge in driving. Existing risk-detection methods excel at identifying collisions but face challenges in assessing the behavior of road users in non-collision situations. This paper introduces Visionary Co-Driver, a system that leverages large language models to identify non-collision roadside risks and alert drivers based on their eye movements. Specifically, the system combines video processing algorithms and LLMs to identify potentially risky road users. These risks are dynamically indicated on an adaptive heads-up display interface to enhance drivers' attention. A user study with 41 drivers confirms that Visionary Co-Driver improves drivers' risk perception and supports their recognition of roadside risks.

</details>


### [37] [TailCue: Exploring Animal-inspired Robotic Tail for Automated Vehicles Interaction](https://arxiv.org/abs/2511.14242)
*Yuan Li,Xinyue Gui,Ding Xia,Mark Colley,Takeo Igarashi*

Main category: cs.HC

TL;DR: 本文提出了一种基于尾部的情感信号研究，强调需要场景特定的优化以促进与自动驾驶车辆的交互。


<details>
  <summary>Details</summary>
Motivation: 有效的交通参与者与自动驾驶车辆之间的沟通是一个重要挑战，心理因素可能影响用户的信任度和参与度。

Method: 通过机器人和动物学的情感表达研究，开发运动-情感映射方案，并实施物理机械尾进行在线用户研究。

Result: 虽然尾部传达的情感没有被一致识别，但开放式反馈表明，尾部运动需要与场景和提示相一致。

Conclusion: 尾部运动需要与场景和提示对齐，以优化基于尾部的eHMIs。

Abstract: Automated vehicles (AVs) are gradually becoming part of our daily lives. However, effective communication between road users and AVs remains a significant challenge. Although various external human-machine interfaces (eHMIs) have been developed to facilitate interactions, psychological factors, such as a lack of trust and inadequate emotional signaling, may still deter users from confidently engaging with AVs in certain contexts. To address this gap, we propose TailCue, an exploration of how tail-based eHMIs affect user interaction with AVs. We first investigated mappings between tail movements and emotional expressions from robotics and zoology, and accordingly developed a motion-emotion mapping scheme. A physical robotic tail was implemented, and specific tail motions were designed based on our scheme. An online, video-based user study with 21 participants was conducted. Our findings suggest that, although the intended emotions conveyed by the tail were not consistently recognized, open-ended feedback indicated that the tail motion needs to align with the scenarios and cues. Our result highlights the necessity of scenario-specific optimization to enhance tail-based eHMIs. Future work will refine tail movement strategies to maximize their effectiveness across diverse interaction contexts.

</details>


### [38] [Towards LLM-Based Usability Analysis for Recommender User Interfaces](https://arxiv.org/abs/2511.14359)
*Sebastian Lubos,Alexander Felfernig,Damian Garber,Viet-Man Le,Thi Ngoc Trang Tran*

Main category: cs.HC

TL;DR: 本研究探讨了多模态大语言模型在评估推荐系统可用性方面的潜力，结果表明这些模型可以以规模化的方式支持可用性评估。


<details>
  <summary>Details</summary>
Motivation: 提高推荐系统的可用性评估效率，减少对专业知识的依赖，利用先进的多模态大语言模型自动化评估过程。

Method: 通过分析多种公开可用的推荐系统界面截图，考虑不同的可用性标准，并利用大型语言模型提供解释性反馈。

Result: 研究表明，大型语言模型可以以启发式方式支持可用性评估，推动用户体验的改进。

Conclusion: 多模态大型语言模型能够在大规模下支持推荐系统界面的可用性评估，从而改善用户体验。

Abstract: Usability is a key factor in the effectiveness of recommender systems. However, the analysis of user interfaces is a time-consuming process that requires expertise. Recent advances in multimodal large language models (LLMs) offer promising opportunities to automate such evaluations. In this work, we explore the potential of multimodal LLMs to assess the usability of recommender system interfaces by considering a variety of publicly available systems as examples. We take user interface screenshots from multiple of these recommender platforms to cover both preference elicitation and recommendation presentation scenarios. An LLM is instructed to analyze these interfaces with regard to different usability criteria and provide explanatory feedback. Our evaluation demonstrates how LLMs can support heuristic-style usability assessments at scale to support the improvement of user experience.

</details>


### [39] [PACEE: Supporting Children's Personal Emotion Education through Parent-AI Collaboration](https://arxiv.org/abs/2511.14414)
*Yu Mei,Xutong Wang,Ziyao Zhang,Yiming Fu,Shiyi Wang,Qingyang Wan,Qinghuan Lan,Chang Liu,Jie Cai,Chun Yu,Yuanchun Shi*

Main category: cs.HC

TL;DR: PACEE是一个支持父母与AI合作情感教育的助手，显著提升了亲子互动和情感沟通。


<details>
  <summary>Details</summary>
Motivation: 现有技术主要从儿童的角度促进情感教育，忽视了父母在早期情感发展中的中心角色。

Method: 通过与五位经验丰富的幼儿园教师和五位家长进行联合设计会议，识别了家长面临的挑战以及AI在家庭情感教育中的角色。

Result: PACEE允许父母参与关于常见情境的情感对话，并通过生成式AI提供多种支持，增强了情感互动。

Conclusion: PACEE显著增强了亲子互动，鼓励了更深入的情感交流，并改善了父母的体验，为AI支持下的家庭教育系统设计提供了重要见解。

Abstract: Emotion education is a crucial lesson for children aged 3 to 6. However, existing technologies primarily focus on promoting emotion education from the child's perspective, often neglecting the central role of parents in guiding early childhood emotion development. In this work, we conducted co-design sessions with five experienced kindergarten teachers and five parents to identify parental challenges and the roles that AI can play in family emotion education. Guided by these insights, we developed PACEE, an assistant for supporting parent-AI collaborative emotion education. PACEE enables parents to engage in emotional dialogues about common scenarios, with multiple forms of support provided by generative AI. It combines insights from parents and AI to model children's emotional states and collaboratively delivers personalized, parent-mediated guidance. In a user study involving 16 families, we found that PACEE significantly enhances parent-child engagement, encourages more in-depth emotional communication, and improves the parental experience. Our findings advance emotion coaching theory in both family settings and LLM-assisted contexts, offering valuable insights for designing AI-supported, parent-centered family education systems.

</details>


### [40] [Model Learning for Adjusting the Level of Automation in HCPS](https://arxiv.org/abs/2511.14437)
*Mehrnoush Hajnorouzi,Astrid Rakow,Martin Fränzle*

Main category: cs.HC

TL;DR: 提出了一种基于模型的框架，结合自动机学习和博弈理论合成，分析人机协作中的安全控制策略。


<details>
  <summary>Details</summary>
Motivation: 随着自动化水平的不断提高，对人机交互的严谨设计方法需求迫切，尤其是在安全关键应用中。

Method: 结合了主动自动机学习和博弈理论反应合成的方法。

Result: 通过案例研究，展示了框架在简化驾驶场景中的适用性，并验证了其在分析人机协作控制策略方面的潜力。

Conclusion: 该框架可以有效地分析人机协作中的安全控制策略，提升安全性和可控性。

Abstract: The steadily increasing level of automation in human-centred systems demands rigorous design methods for analysing and controlling interactions between humans and automated components, especially in safety-critical applications. The variability of human behaviour poses particular challenges for formal verification and synthesis. We present a model-based framework that enables design-time exploration of safe shared-control strategies in human-automation systems. The approach combines active automata learning -- to derive coarse, finite-state abstractions of human behaviour from simulations -- with game-theoretic reactive synthesis to determine whether a controller can guarantee safety when interacting with these models. If no such strategy exists, the framework supports iterative refinement of the human model or adjustment of the automation's controllable actions. A driving case study, integrating automata learning with reactive synthesis in UPPAAL, illustrates the applicability of the framework on a simplified driving scenario and its potential for analysing shared-control strategies in human-centred cyber-physical systems.

</details>


### [41] [SweeperBot: Making 3D Browsing Accessible through View Analysis and Visual Question Answering](https://arxiv.org/abs/2511.14567)
*Chen Chen,Cuong Nguyen,Alexa Siu,Dingzeyu Li,Nadir Weibel*

Main category: cs.HC

TL;DR: 本研究介绍了SweeperBot，一个结合视觉问答的系统，旨在帮助屏幕阅读器用户有效访问和理解3D模型。


<details>
  <summary>Details</summary>
Motivation: 为了解决屏幕阅读器用户访问3D模型的困难，研究旨在通过视觉问答能力改善其体验。

Method: 该研究结合了最优视图选择技术和生成及识别基础模型的优势，开发了允许视障用户提问并接受回答的系统。

Result: SweeperBot成功地帮助盲人和低视力用户在探索和比较3D模型方面，并且其生成的描述质量得到了验证。

Conclusion: SweeperBot展示了帮助视障用户探索和比较3D模型的可行性，其生成的描述得到了用户的验证。

Abstract: Accessing 3D models remains challenging for Screen Reader (SR) users. While some existing 3D viewers allow creators to provide alternative text, they often lack sufficient detail about the 3D models. Grounded on a formative study, this paper introduces SweeperBot, a system that enables SR users to leverage visual question answering to explore and compare 3D models. SweeperBot answers SR users' visual questions by combining an optimal view selection technique with the strength of generative- and recognition-based foundation models. An expert review with 10 Blind and Low-Vision (BLV) users with SR experience demonstrated the feasibility of using SweeperBot to assist BLV users in exploring and comparing 3D models. The quality of the descriptions generated by SweeperBot was validated by a second survey study with 30 sighted participants.

</details>


### [42] [Biased Minds Meet Biased AI: How Class Imbalance Shapes Appropriate Reliance and Interacts with Human Base Rate Neglect](https://arxiv.org/abs/2511.14591)
*Nick von Felten,Johannes Schöning,Klaus Opwis,Nicolas Scharowksi*

Main category: cs.HC

TL;DR: 本研究考察了AI和人类在决策中相互作用的偏见，发现类不平衡与基础比率忽视相互影响，支持进一步研究。


<details>
  <summary>Details</summary>
Motivation: 探讨人类与人工智能在决策过程中的偏见相互作用，特别是AI偏见与人类偏见的复杂关系。

Method: 通过一项在线实验，参与者使用基于AI的决策支持系统对三种疾病进行分类，实验设计为被试内（N=46），并对比了平衡与不平衡数据集的影响。

Result: 发现类不平衡影响了参与者对AI支持的信任，并且类不平衡与基础比率忽视之间存在相互加强效应，即复合偏见。

Conclusion: 本研究发现了人类与AI互动中存在的复合偏见现象，并建议进一步研究这一相互作用。

Abstract: Humans increasingly interact with artificial intelligence (AI) in decision-making. However, both AI and humans are prone to biases. While AI and human biases have been studied extensively in isolation, this paper examines their complex interaction. Specifically, we examined how class imbalance as an AI bias affects people's ability to appropriately rely on an AI-based decision-support system, and how it interacts with base rate neglect as a human bias. In a within-subject online study (N= 46), participants classified three diseases using an AI-based decision-support system trained on either a balanced or unbalanced dataset. We found that class imbalance disrupted participants' calibration of AI reliance. Moreover, we observed mutually reinforcing effects between class imbalance and base rate neglect, offering evidence of a compound human-AI bias. Based on these findings, we advocate for an interactionist perspective and further research into the mutually reinforcing effects of biases in human-AI interaction.

</details>


### [43] [Theoretical basis for code presentation: A case for cognitive load](https://arxiv.org/abs/2511.14636)
*Nyah Speicher,Prashant Chandrasekar*

Main category: cs.HC

TL;DR: 减少认知负担对所有能力的人都能提升任务表现，特别对盲人与视力低下者至关重要。提出针对代码展示的设计建议以减轻认知负担。


<details>
  <summary>Details</summary>
Motivation: 盲人和视力低下人士无法依赖常见的视觉基础方法来管理认知负担，因此需要探索如何通过减轻认知负担来提升他们的任务表现。

Method: 通过心理科学研究，识别影响编程表现和学习的认知负担的各个方面，并评估现有的编程解决方案。

Result: 通过评估现有解决方案，提出初步设计建议，旨在降低认知负担。

Conclusion: 通过提出设计建议，能够有效降低盲人和视力低下开发者的认知负担，进而改善他们的编程任务表现。

Abstract: Evidence supports that reducing cognitive load (CL) improves task performance for people of all abilities. This effect is specifically important for blind-and-low-vision (BLV) individuals because they cannot rely on many common methods of managing CL, which are frequently vision-based techniques. Current accessible "solutions" for BLV developers only sporadically consider CL in their design. There isn't a way to know whether CL is being alleviated by them. Neither do we know if alleviating CL is part of the mechanism behind why these solutions help BLV people. Using a strong foundation in psychological sciences, we identify aspects of CL that impact performance and learning in programming. These aspects are then examined when evaluating existing solutions for programming sub-tasks for BLV users. We propose an initial design "recommendations" for presentation of code which, when followed, will reduce cognitive load for BLV developers.

</details>


### [44] [M-CALLM: Multi-level Context Aware LLM Framework for Group Interaction Prediction](https://arxiv.org/abs/2511.14661)
*Diana Romero,Xin Gao,Daniel Khalkhali,Salma Elmalaki*

Main category: cs.HC

TL;DR: 本文研究了大型语言模型如何利用多级上下文信息在协作混合现实环境中预测群体协调模式，提出了M-CALLM框架，表现优越但面临模拟模式下的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型如何利用多层次的上下文信息来预测协作混合现实环境中的群体协调模式。

Method: 构建了M-CALLM框架，将多模态传感器流转化为层级上下文，以支持大语言模型的预测，并评估了零-shot提示、少-shot学习和监督微调三种范式。

Result: 上下文感知的LLM在会话预测中实现了96%的准确率，比LSTM基线提高了3.2倍，同时保持了低于35毫秒的延迟。但在模拟模式下，由于级联错误导致的脆弱性表现出83%的性能下降。

Conclusion: 我们希望这一工作能够催生出构建智能协作感知系统的新思路，平衡语义推理能力与基本约束。

Abstract: This paper explores how large language models can leverage multi-level contextual information to predict group coordination patterns in collaborative mixed reality environments. We demonstrate that encoding individual behavioral profiles, group structural properties, and temporal dynamics as natural language enables LLMs to break through the performance ceiling of statistical models. We build M-CALLM, a framework that transforms multimodal sensor streams into hierarchical context for LLM-based prediction, and evaluate three paradigms (zero-shot prompting, few-shot learning, and supervised fine-tuning) against statistical baselines across intervention mode (real-time prediction) and simulation mode (autoregressive forecasting) Head-to-head comparison on 16 groups (64 participants, ~25 hours) demonstrates that context-aware LLMs achieve 96% accuracy for conversation prediction, a 3.2x improvement over LSTM baselines, while maintaining sub-35ms latency. However, simulation mode reveals brittleness with 83% degradation due to cascading errors. Deep-dive into modality-specific performance shows conversation depends on temporal patterns, proximity benefits from group structure (+6%), while shared attention fails completely (0% recall), exposing architectural limitations. We hope this work spawns new ideas for building intelligent collaborative sensing systems that balance semantic reasoning capabilities with fundamental constraints.

</details>
