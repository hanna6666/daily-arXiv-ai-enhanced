{"id": "2601.08953", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08953", "abs": "https://arxiv.org/abs/2601.08953", "authors": ["Le Liu", "Bangguo Yu", "Nynke Vellinga", "Ming Cao"], "title": "Fairness risk and its privacy-enabled solution in AI-driven robotic applications", "comment": null, "summary": "Complex decision-making by autonomous machines and algorithms could underpin the foundations of future society. Generative AI is emerging as a powerful engine for such transitions. However, we show that Generative AI-driven developments pose a critical pitfall: fairness concerns. In robotic applications, although intuitions about fairness are common, a precise and implementable definition that captures user utility and inherent data randomness is missing. Here we provide a utility-aware fairness metric for robotic decision making and analyze fairness jointly with user-data privacy, deriving conditions under which privacy budgets govern fairness metrics. This yields a unified framework that formalizes and quantifies fairness and its interplay with privacy, which is tested in a robot navigation task. In view of the fact that under legal requirements, most robotic systems will enforce user privacy, the approach shows surprisingly that such privacy budgets can be jointly used to meet fairness targets. Addressing fairness concerns in the creative combined consideration of privacy is a step towards ethical use of AI and strengthens trust in autonomous robots deployed in everyday environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6548\u7528\u5bfc\u5411\u7684\u516c\u5e73\u6027\u5ea6\u91cf\u6807\u51c6\uff0c\u5206\u6790\u4e86\u673a\u5668\u4eba\u51b3\u7b56\u4e2d\u7684\u516c\u5e73\u6027\u4e0e\u7528\u6237\u9690\u79c1\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u9690\u79c1\u9884\u7b97\u53ef\u52a9\u529b\u5b9e\u73b0\u516c\u5e73\u6027\u76ee\u6807\uff0c\u4e3aAI\u7684\u4f26\u7406\u4f7f\u7528\u94fa\u5e73\u9053\u8def\u3002", "motivation": "\u63a2\u8ba8\u81ea\u4e3b\u673a\u5668\u548c\u7b97\u6cd5\u7684\u590d\u6742\u51b3\u7b56\u5bf9\u672a\u6765\u793e\u4f1a\u7684\u57fa\u7840\u6027\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u751f\u6210\u5f0fAI\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u6548\u7528\u7684\u516c\u5e73\u6027\u5ea6\u91cf\u6807\u51c6\uff0c\u5e76\u5206\u6790\u7528\u6237\u6570\u636e\u9690\u79c1\u4e0e\u516c\u5e73\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5efa\u7acb\u7edf\u4e00\u6846\u67b6\uff0c\u5b9a\u91cf\u5316\u516c\u5e73\u6027\u53ca\u5176\u4e0e\u9690\u79c1\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u901a\u8fc7\u673a\u5668\u4eba\u5bfc\u822a\u4efb\u52a1\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u53d1\u73b0\u9690\u79c1\u9884\u7b97\u53ef\u4ee5\u4e0e\u516c\u5e73\u6027\u76ee\u6807\u8054\u5408\u4f7f\u7528\uff0c\u80fd\u591f\u6ee1\u8db3\u6cd5\u5f8b\u8981\u6c42\u53ca\u63d0\u5347\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u7528\u6237\u4fe1\u4efb\u3002", "conclusion": "\u5728\u4f26\u7406\u4f7f\u7528AI\u7684\u8fc7\u7a0b\u4e2d\uff0c\u7efc\u5408\u8003\u8651\u516c\u5e73\u6027\u548c\u9690\u79c1\u6027\u662f\u63d0\u9ad8\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u65e5\u5e38\u73af\u5883\u4e2d\u4fe1\u4efb\u5ea6\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2601.09031", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09031", "abs": "https://arxiv.org/abs/2601.09031", "authors": ["Xuetao Li", "Wenke Huang", "Mang Ye", "Jifeng Xuan", "Bo Du", "Sheng Liu", "Miao Li"], "title": "Generalizable Geometric Prior and Recurrent Spiking Feature Learning for Humanoid Robot Manipulation", "comment": null, "summary": "Humanoid robot manipulation is a crucial research area for executing diverse human-level tasks, involving high-level semantic reasoning and low-level action generation. However, precise scene understanding and sample-efficient learning from human demonstrations remain critical challenges, severely hindering the applicability and generalizability of existing frameworks. This paper presents a novel RGMP-S, Recurrent Geometric-prior Multimodal Policy with Spiking features, facilitating both high-level skill reasoning and data-efficient motion synthesis. To ground high-level reasoning in physical reality, we leverage lightweight 2D geometric inductive biases to enable precise 3D scene understanding within the vision-language model. Specifically, we construct a Long-horizon Geometric Prior Skill Selector that effectively aligns the semantic instructions with spatial constraints, ultimately achieving robust generalization in unseen environments. For the data efficiency issue in robotic action generation, we introduce a Recursive Adaptive Spiking Network. We parameterize robot-object interactions via recursive spiking for spatiotemporal consistency, fully distilling long-horizon dynamic features while mitigating the overfitting issue in sparse demonstration scenarios. Extensive experiments are conducted across the Maniskill simulation benchmark and three heterogeneous real-world robotic systems, encompassing a custom-developed humanoid, a desktop manipulator, and a commercial robotic platform. Empirical results substantiate the superiority of our method over state-of-the-art baselines and validate the efficacy of the proposed modules in diverse generalization scenarios. To facilitate reproducibility, the source code and video demonstrations are publicly available at https://github.com/xtli12/RGMP-S.git.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faRGMP-S\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cbe\u786e\u76843D\u573a\u666f\u7406\u89e3\u4e0e\u9ad8\u6548\u7684\u8fd0\u52a8\u5408\u6210\uff0c\u89e3\u51b3\u4e86\u4eba\u7c7b\u7ea7\u522b\u4efb\u52a1\u4e2d\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4eba\u7c7b\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u590d\u6742\u6027\u8981\u6c42\u63d0\u5347\u573a\u666f\u7406\u89e3\u4e0e\u4ece\u793a\u8303\u4e2d\u5b66\u4e60\u7684\u6548\u7387\uff0c\u4ee5\u63d0\u9ad8\u73b0\u6709\u6846\u67b6\u7684\u9002\u7528\u6027\u548c\u666e\u904d\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684RGMP-S\u65b9\u6cd5\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u4e8c\u7ef4\u51e0\u4f55\u5f52\u7eb3\u504f\u89c1\u5b9e\u73b03D\u573a\u666f\u7406\u89e3\uff0c\u7ed3\u5408\u957f\u65f6\u95f4\u51e0\u4f55\u4f18\u5148\u6280\u80fd\u9009\u62e9\u5668\u4e0e\u9012\u5f52\u81ea\u9002\u5e94\u8109\u51b2\u7f51\u7edc\uff0c\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u9ad8\u6548\u6027\u4e0e\u51c6\u786e\u6027\u3002", "result": "\u5728Maniskill\u4eff\u771f\u57fa\u51c6\u548c\u4e09\u79cd\u4e0d\u540c\u7684\u771f\u5b9e\u673a\u5668\u4eba\u7cfb\u7edf\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u6cdb\u5316\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "RGMP-S\u65b9\u6cd5\u80fd\u591f\u5728\u672a\u89c1\u73af\u5883\u4e0b\u5b9e\u73b0\u9c81\u68d2\u6027\u6cdb\u5316\uff0c\u5145\u5206\u5229\u7528\u4eba\u7c7b\u6f14\u793a\u8fdb\u884c\u9ad8\u6548\u5b66\u4e60\uff0c\u5e76\u5177\u5907\u51fa\u8272\u7684\u6570\u636e\u6548\u7387\u4e0e\u8fd0\u52a8\u5408\u6210\u80fd\u529b\u3002"}}
{"id": "2601.09104", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.09104", "abs": "https://arxiv.org/abs/2601.09104", "authors": ["Ko Yamamoto", "Kyosuke Ishibashi", "Hiroki Ishikawa", "Osamu Azami"], "title": "Design Methodology of Hydraulically-driven Soft Robotic Gripper for a Large and Heavy Object", "comment": null, "summary": "This paper presents a design methodology of a hydraulically-driven soft robotic gripper for grasping a large and heavy object -- approximately 10 - 20 kg with 20 - 30 cm diameter. Most existing soft grippers are pneumatically actuated with several hundred kPa pressure, and cannot generate output force sufficient for such a large and heavy object. Instead of pneumatic actuation, hydraulic actuation has a potential to generate much larger power by several MPa pressure. In this study, we develop a hydraulically-driven soft gripper, in which its basic design parameters are determined based on a mathematical model that represents the relationship among the driving pressure, bending angle, object mass and grasping force. Moreover, we selected materials suitable for grasping a heavier object, based on the finite element analysis result of the detailed design. We report experimental results on a 20 kg object grasping and closed-loop control of the finger bending angle.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u6db2\u538b\u9a71\u52a8\u7684\u8f6f\u673a\u5668\u4eba\u6293\u624b\uff0c\u80fd\u591f\u6293\u53d6\u91cd\u8fbe20\u516c\u65a4\u7684\u7269\u4f53\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6293\u53d6\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u6c14\u52a8\u9a71\u52a8\u8f6f\u6293\u624b\u5728\u6293\u53d6\u5927\u91cd\u7269\u65f6\u529b\u4e0d\u8db3\uff0c\u8feb\u5207\u9700\u8981\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e00\u79cd\u6db2\u538b\u9a71\u52a8\u7684\u8f6f\u673a\u5668\u4eba\u6293\u624b\uff0c\u57fa\u4e8e\u6570\u5b66\u6a21\u578b\u786e\u5b9a\u8bbe\u8ba1\u53c2\u6570\uff0c\u5e76\u8fdb\u884c\u6750\u6599\u9009\u62e9\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u6210\u529f\u6293\u53d620\u516c\u65a4\u91cd\u7684\u7269\u4f53\uff0c\u5e76\u5b9e\u73b0\u624b\u6307\u5f2f\u66f2\u89d2\u5ea6\u7684\u95ed\u73af\u63a7\u5236\u3002", "conclusion": "\u6db2\u538b\u9a71\u52a8\u8f6f\u6293\u624b\u80fd\u591f\u6709\u6548\u6293\u53d6\u91cd\u7269\uff0c\u5177\u6709\u4f18\u4e8e\u6c14\u52a8\u9a71\u52a8\u6293\u624b\u7684\u8f93\u51fa\u529b\u3002"}}
{"id": "2601.09163", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.09163", "abs": "https://arxiv.org/abs/2601.09163", "authors": ["Tong Wu", "Shoujie Li", "Junhao Gong", "Changqing Guo", "Xingting Li", "Shilong Mu", "Wenbo Ding"], "title": "CEI: A Unified Interface for Cross-Embodiment Visuomotor Policy Learning in 3D Space", "comment": null, "summary": "Robotic foundation models trained on large-scale manipulation datasets have shown promise in learning generalist policies, but they often overfit to specific viewpoints, robot arms, and especially parallel-jaw grippers due to dataset biases. To address this limitation, we propose Cross-Embodiment Interface (\\CEI), a framework for cross-embodiment learning that enables the transfer of demonstrations across different robot arm and end-effector morphologies. \\CEI introduces the concept of \\textit{functional similarity}, which is quantified using Directional Chamfer Distance. Then it aligns robot trajectories through gradient-based optimization, followed by synthesizing observations and actions for unseen robot arms and end-effectors. In experiments, \\CEI transfers data and policies from a Franka Panda robot to \\textbf{16} different embodiments across \\textbf{3} tasks in simulation, and supports bidirectional transfer between a UR5+AG95 gripper robot and a UR5+Xhand robot across \\textbf{6} real-world tasks, achieving an average transfer ratio of 82.4\\%. Finally, we demonstrate that \\CEI can also be extended with spatial generalization and multimodal motion generation capabilities using our proposed techniques. Project website: https://cross-embodiment-interface.github.io/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4ea4\u53c9\u4f53\u73b0\u63a5\u53e3\uff08CEI\uff09\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u652f\u6301\u4e0d\u540c\u673a\u5668\u4eba\u7ed3\u6784\u4e4b\u95f4\u7684\u8fd0\u52a8\u7b56\u7565\u548c\u6570\u636e\u8fc1\u79fb\u3002", "motivation": "\u5f53\u524d\u7684\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u5e38\u5e38\u56e0\u6570\u636e\u96c6\u504f\u89c1\u800c\u8fc7\u62df\u5408\u4e8e\u7279\u5b9a\u7684\u89c6\u89d2\u548c\u6293\u53d6\u5de5\u5177\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u63d0\u9ad8\u6a21\u578b\u7684\u5e7f\u6cdb\u9002\u5e94\u6027\u548c\u8fc1\u79fb\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4ea4\u53c9\u4f53\u73b0\u63a5\u53e3\uff08CEI\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u529f\u80fd\u76f8\u4f3c\u6027\u91cf\u5316\u3001\u68af\u5ea6\u4f18\u5316\u5bf9\u673a\u5668\u4eba\u8f68\u8ff9\u8fdb\u884c\u5bf9\u9f50\uff0c\u5e76\u4e3a\u672a\u89c1\u8fc7\u7684\u673a\u5668\u4eba\u624b\u81c2\u548c\u672b\u7aef\u6267\u884c\u5668\u5408\u6210\u89c2\u5bdf\u4e0e\u52a8\u4f5c\u3002", "result": "CEI\u5728\u4eff\u771f\u5b9e\u9a8c\u4e2d\u6210\u529f\u5730\u5c06Franka Panda\u673a\u5668\u4eba\u7684\u6570\u636e\u548c\u7b56\u7565\u8fc1\u79fb\u523016\u79cd\u4e0d\u540c\u7684\u673a\u5668\u4eba\u7ed3\u6784\uff0c\u57286\u9879\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e8682.4%\u7684\u5e73\u5747\u8fc1\u79fb\u6bd4\u7387\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u4ea4\u53c9\u4f53\u73b0\u5b66\u4e60\u6846\u67b6\uff0c\textit{CEI} \u6709\u6548\u5730\u89e3\u51b3\u4e86\u56e0\u6570\u636e\u96c6\u504f\u89c1\u5bfc\u81f4\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u8fd0\u52a8\u7b56\u7565\u548c\u6570\u636e\u5728\u4e0d\u540c\u673a\u5668\u4eba\u7ed3\u6784\u4e4b\u95f4\u7684\u6210\u529f\u8fc1\u79fb\u3002"}}
{"id": "2601.08954", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.08954", "abs": "https://arxiv.org/abs/2601.08954", "authors": ["Sumin Hong", "Jewoong Moon", "Taeyeon Eom", "Juno Hwang", "Jibeom Seo"], "title": "Leveraging learning analytics to enhance immersive teacher simulations: Challenges and opportunities", "comment": "30 pages, 10 figures. This chapter examines immersive teacher simulations and multimodal analytics. Project website: https://teachergenai.github.io/", "summary": "This chapter examines how data analytics can be leveraged to enhance immersive teacher simulations, situating this inquiry within the broader learning sciences discourse on embodied cognition, data-informed feedback, and teacher professional learning. It explores both conceptual foundations and empirical cases to illustrate how analytics serve as mediational tools that connect immersive experiences with reflective teaching practice. The chapter unfolds in multiple sections: (1) The Innovation Journey: An Overview of Immersive Teacher Simulations outlines the evolution from traditional simulations to XR-based environments, highlighting the need for professional decision-making under realistic constraints. (2) Innovation in Existing Research and Practice situates teacher analytics within the trajectory from descriptive observation to multimodal and predictive modeling. (3) Study Approach and Design details how multimodal data-discourse, behavior, and gaze-from the TeacherGen@i simulation were collected and organized to reveal cognitive distribution of pedagogical discourse and interaction patterns. (4) Findings present the cognitive distribution of preservice teachers' pedagogical discourse and the sequential interaction patterns that emerge in exchange, illustrating how multimodal analytics make pedagogical reasoning processes visible within immersive simulations. (5) Understanding Innovative Practices in Teacher Education examines teaching analytics to enhance immersive teacher simulation based on the findings of the study. (6) Key Takeaways of the Innovation Journey identifies research challenges and design implications for scalable, analytics-enhanced teacher education. Together, these sections position immersive teacher simulations as a pivotal testbed for aligning learning analytics, professional learning, and next-generation immersive learning environment design.", "AI": {"tldr": "\u672c\u7ae0\u63a2\u8ba8\u5982\u4f55\u5229\u7528\u6570\u636e\u5206\u6790\u63d0\u5347\u6c89\u6d78\u5f0f\u6559\u5e08\u6a21\u62df\uff0c\u5f3a\u8c03\u5206\u6790\u5de5\u5177\u5728\u6559\u80b2\u53cd\u601d\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u603b\u7ed3\u4e86\u6559\u5e08\u6559\u80b2\u4e2d\u7684\u7814\u7a76\u6311\u6218\u548c\u8bbe\u8ba1\u610f\u4e49\u3002", "motivation": "\u63a2\u8ba8\u6570\u636e\u5206\u6790\u5728\u6c89\u6d78\u5f0f\u6559\u5e08\u6a21\u62df\u4e2d\u5982\u4f55\u4e3a\u6559\u5e08\u4e13\u4e1a\u5b66\u4e60\u63d0\u4f9b\u652f\u6301\uff0c\u8fdb\u800c\u6539\u8fdb\u6559\u5b66\u5b9e\u8df5\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6765\u81eaTeacherGen@i\u6a21\u62df\u7684\u591a\u6a21\u6001\u6570\u636e\uff0c\u5c55\u793a\u4e86\u6559\u80b2\u8bdd\u8bed\u548c\u4e92\u52a8\u6a21\u5f0f\u7684\u8ba4\u77e5\u5206\u5e03\u3002", "result": "\u901a\u8fc7\u6570\u636e\u5206\u6790\u63d0\u5347\u6c89\u6d78\u5f0f\u6559\u5e08\u6a21\u62df\u7684\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u5206\u6790\u5de5\u5177\u662f\u5982\u4f55\u5c06\u6a21\u62df\u4f53\u9a8c\u4e0e\u53cd\u601d\u6559\u5b66\u5b9e\u8df5\u76f8\u8fde\u63a5\u7684\u3002", "conclusion": "\u6c89\u6d78\u5f0f\u6559\u5e08\u6a21\u62df\u4e3a\u5bf9\u5b66\u4e60\u5206\u6790\u548c\u6559\u5e08\u4e13\u4e1a\u5b66\u4e60\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u5b9e\u9a8c\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u6559\u5e08\u6559\u80b2\u4e2d\u9762\u4e34\u7684\u7814\u7a76\u6311\u6218\u548c\u8bbe\u8ba1\u610f\u4e49\u3002"}}
{"id": "2601.09178", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.09178", "abs": "https://arxiv.org/abs/2601.09178", "authors": ["Paul Brunzema", "Thomas Lew", "Ray Zhang", "Takeru Shirasawa", "John Subosits", "Marcus Greiff"], "title": "Vision-Conditioned Variational Bayesian Last Layer Dynamics Models", "comment": "9 pages, 7 figures, currently under review", "summary": "Agile control of robotic systems often requires anticipating how the environment affects system behavior. For example, a driver must perceive the road ahead to anticipate available friction and plan actions accordingly. Achieving such proactive adaptation within autonomous frameworks remains a challenge, particularly under rapidly changing conditions. Traditional modeling approaches often struggle to capture abrupt variations in system behavior, while adaptive methods are inherently reactive and may adapt too late to ensure safety. We propose a vision-conditioned variational Bayesian last-layer dynamics model that leverages visual context to anticipate changes in the environment. The model first learns nominal vehicle dynamics and is then fine-tuned with feature-wise affine transformations of latent features, enabling context-aware dynamics prediction. The resulting model is integrated into an optimal controller for vehicle racing. We validate our method on a Lexus LC500 racing through water puddles. With vision-conditioning, the system completed all 12 attempted laps under varying conditions. In contrast, all baselines without visual context consistently lost control, demonstrating the importance of proactive dynamics adaptation in high-performance applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89c6\u89c9\u6761\u4ef6\u7684\u52a8\u6001\u9884\u6d4b\u6a21\u578b\uff0c\u80fd\u5728\u53d8\u5316\u73af\u5883\u4e0b\u4f18\u5316\u81ea\u52a8\u9a7e\u9a76\u63a7\u5236\u5668\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u8fc5\u901f\u53d8\u5316\u7684\u73af\u5883\u4e2d\u7ef4\u6301\u5b89\u5168\u6027\u4e0e\u6027\u80fd\u7684\u6311\u6218\uff0c\u5f3a\u8c03\u4e86\u4e3b\u52a8\u9002\u5e94\u7684\u91cd\u8981\u6027\u3002", "method": "\u5229\u7528\u89c6\u89c9\u4fe1\u606f\u8bad\u7ec3\u53d8\u5206\u8d1d\u53f6\u65af\u6a21\u578b\uff0c\u96c6\u6210\u5165\u8f66\u8f86\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u7279\u5f81\u7684\u4eff\u5c04\u53d8\u6362\u63d0\u5347\u52a8\u6001\u9884\u6d4b\u80fd\u529b\u3002", "result": "\u5728\u53d8\u5316\u73af\u5883\u4e0b\uff0c\u57fa\u4e8e\u89c6\u89c9\u7684\u53d8\u5206\u8d1d\u53f6\u65af\u52a8\u6001\u6a21\u578b\u80fd\u591f\u6709\u6548\u9884\u5224\u73af\u5883\u53d8\u5316\uff0c\u4ece\u800c\u5b9e\u73b0\u4e3b\u52a8\u9002\u5e94\uff0c\u4fdd\u8bc1\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u96c6\u6210\u89c6\u89c9\u6761\u4ef6\u7684\u52a8\u6001\u9884\u6d4b\u6a21\u578b\uff0c\u4f18\u5316\u63a7\u5236\u5668\u53ef\u4ee5\u5728\u9ad8\u6027\u80fd\u6761\u4ef6\u4e0b\u4fdd\u6301\u8f66\u8f86\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u5feb\u901f\u53d8\u5316\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.09033", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.09033", "abs": "https://arxiv.org/abs/2601.09033", "authors": ["Yejoon Song", "Bandi Kim", "Yeju Kwon", "Sung Park"], "title": "Exploring the Effects of Generative AI Assistance on Writing Self-Efficacy", "comment": null, "summary": "Generative AI (GenAI) is increasingly used in academic writing, yet its effects on students' writing self-efficacy remain contingent on how assistance is configured. This pilot study investigates how ideation-level, sentence-level, full-process, and no AI support differentially shape undergraduate writers' self-efficacy using a 2 by 2 experimental design with Korean undergraduates completing argumentative writing tasks. Results indicate that AI assistance does not uniformly enhance self-efficacy full AI support produced high but stable self-efficacy alongside signs of reduced ownership, sentence-level AI support led to consistent self-efficacy decline, and ideation-level AI support was associated with both high self-efficacy and positive longitudinal change. These findings suggest that the locus of AI intervention, rather than the amount of assistance, is critical in fostering writing self-efficacy while preserving learner agency.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4e0d\u540c\u7ea7\u522b\u7684\u751f\u6210\u6027AI\u652f\u6301\u5bf9\u5927\u5b66\u751f\u5199\u4f5c\u81ea\u6211\u6548\u80fd\u7684\u5f71\u54cd\uff0c\u7ed3\u679c\u663e\u793a\uff0cAI\u652f\u6301\u7684\u914d\u7f6e\u65b9\u5f0f\u5bf9\u81ea\u6211\u6548\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u652f\u6301\u7684\u6570\u91cf\u3002", "motivation": "\u968f\u7740\u751f\u6210\u6027AI\u5728\u5b66\u672f\u5199\u4f5c\u4e2d\u7684\u65e5\u76ca\u666e\u53ca\uff0c\u7406\u89e3\u5176\u5bf9\u5b66\u751f\u5199\u4f5c\u81ea\u6211\u6548\u80fd\u7684\u5f71\u54cd\u53d8\u5f97\u91cd\u8981\u3002", "method": "\u91c7\u75282x2\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u9488\u5bf9\u97e9\u56fd\u672c\u79d1\u751f\u5728\u5b8c\u6210\u8bba\u8ff0\u6027\u5199\u4f5c\u4efb\u52a1\u65f6\u8fdb\u884c\u4e0d\u540c\u7c7b\u578b\u7684AI\u652f\u6301\uff1a\u521b\u610f\u9636\u6bb5\u3001\u53e5\u5b50\u9636\u6bb5\u3001\u5b8c\u6574\u8fc7\u7a0b\u548c\u65e0\u652f\u6301\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cAI\u652f\u6301\u5bf9\u81ea\u6211\u6548\u80fd\u7684\u5f71\u54cd\u5e76\u4e0d\u4e00\u81f4\uff0c\u5168\u9762AI\u652f\u6301\u4ea7\u751f\u9ad8\u4f46\u7a33\u5b9a\u7684\u81ea\u6211\u6548\u80fd\uff0c\u53e5\u5b50\u7ea7AI\u652f\u6301\u5bfc\u81f4\u81ea\u6211\u6548\u80fd\u6301\u7eed\u4e0b\u964d\uff0c\u800c\u521b\u610f\u7ea7AI\u652f\u6301\u5219\u4e0e\u9ad8\u81ea\u6211\u6548\u80fd\u548c\u79ef\u6781\u7684\u7eb5\u5411\u53d8\u5316\u76f8\u5173\u3002", "conclusion": "AI\u5e72\u9884\u7684\u4f4d\u7f6e\uff0c\u800c\u4e0d\u662f\u652f\u6301\u7684\u6570\u91cf\uff0c\u5728\u63d0\u5347\u5199\u4f5c\u81ea\u6211\u6548\u80fd\u548c\u4fdd\u7559\u5b66\u4e60\u8005\u81ea\u4e3b\u6027\u65b9\u9762\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2601.09231", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.09231", "abs": "https://arxiv.org/abs/2601.09231", "authors": ["Shuoye Li", "Zhiyuan Song", "Yulin Li", "Zhihai Bi", "Jun Ma"], "title": "Online Trajectory Optimization for Arbitrary-Shaped Mobile Robots via Polynomial Separating Hypersurfaces", "comment": null, "summary": "An emerging class of trajectory optimization methods enforces collision avoidance by jointly optimizing the robot's configuration and a separating hyperplane. However, as linear separators only apply to convex sets, these methods require convex approximations of both the robot and obstacles, which becomes an overly conservative assumption in cluttered and narrow environments. In this work, we unequivocally remove this limitation by introducing nonlinear separating hypersurfaces parameterized by polynomial functions. We first generalize the classical separating hyperplane theorem and prove that any two disjoint bounded closed sets in Euclidean space can be separated by a polynomial hypersurface, serving as the theoretical foundation for nonlinear separation of arbitrary geometries. Building on this result, we formulate a nonlinear programming (NLP) problem that jointly optimizes the robot's trajectory and the coefficients of the separating polynomials, enabling geometry-aware collision avoidance without conservative convex simplifications. The optimization remains efficiently solvable using standard NLP solvers. Simulation and real-world experiments with nonconvex robots demonstrate that our method achieves smooth, collision-free, and agile maneuvers in environments where convex-approximation baselines fail.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u7ebf\u6027\u5206\u79bb\u8d85\u66f2\u9762\uff0c\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u4e8e\u4fdd\u5b88\u51f8\u8fd1\u4f3c\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u8f68\u8ff9\u4f18\u5316\u4e0e\u78b0\u649e\u907f\u514d\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u7ebf\u6027\u5206\u79bb\u5668\u7684\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u4fdd\u5b88\u6027\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u7684\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u7531\u591a\u9879\u5f0f\u51fd\u6570\u53c2\u6570\u5316\u7684\u975e\u7ebf\u6027\u5206\u79bb\u8d85\u66f2\u9762\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u4e00\u4e2a\u8054\u5408\u4f18\u5316\u7684\u95ee\u9898\uff0c\u4f18\u5316\u673a\u5668\u4eba\u7684\u8f68\u8ff9\u548c\u5206\u79bb\u591a\u9879\u5f0f\u7684\u7cfb\u6570\u3002", "result": "\u7ecf\u8fc7\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u975e\u51f8\u673a\u5668\u4eba\u53ca\u5176\u64cd\u4f5c\u73af\u5883\u4e2d\u80fd\u591f\u5b9e\u73b0\u6d41\u7545\u3001\u65e0\u78b0\u649e\u7684\u673a\u52a8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5730\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u5e73\u6ed1\u3001\u65e0\u78b0\u649e\u7684\u673a\u52a8\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u4fdd\u5b88\u7684\u51f8\u8fd1\u4f3c\u3002"}}
{"id": "2601.09045", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.09045", "abs": "https://arxiv.org/abs/2601.09045", "authors": ["Hasan Tarik Akbaba", "Efe Bozkir", "Anna Puhl", "S\u00fcleyman \u00d6zdel", "Enkelejda Kasneci"], "title": "Exploring Organizational Readiness and Ecosystem Coordination for Industrial XR", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Extended Reality (XR) offers transformative potential for industrial support, training, and maintenance; yet, widespread adoption lags despite demonstrated occupational value and hardware maturity. Organizations successfully implement XR in isolated pilots, yet struggle to scale these into sustained operational deployment, a phenomenon we characterize as the ``Pilot Trap.'' This study examines this phenomenon through a qualitative ecosystem analysis of 17 expert interviews across technology providers, solution integrators, and industrial adopters. We identify a ``Great Inversion'' in adoption barriers: critical constraints have shifted from technological maturity to organizational readiness (e.g., change management, key performance indicator alignment, and political resistance). While hardware ergonomics and usability remain relevant, our findings indicate that systemic misalignments between stakeholder incentives are the primary cause of friction preventing enterprise integration. We conclude that successful industrial XR adoption requires a shift from technology-centric piloting to a problem-first, organizational transformation approach, necessitating explicit ecosystem-level coordination.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u5de5\u4e1aXR\u63a8\u5e7f\u4e2d\u7684\"Pilot Trap\"\u73b0\u8c61\uff0c\u53d1\u73b0\u7ec4\u7ec7\u51c6\u5907\u5ea6\u662f\u4e3b\u8981\u969c\u788d\uff0c\u5efa\u8bae\u91c7\u53d6\u7ec4\u7ec7\u53d8\u9769\u65b9\u6cd5\u4ee5\u63a8\u52a8XR\u5e94\u7528\u3002", "motivation": "\u5c3d\u7ba1XR\u5728\u5de5\u4e1a\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5e7f\u6cdb\u91c7\u7eb3\u4ecd\u7136\u6ede\u540e\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5f71\u54cdXR\u666e\u53ca\u7684\u969c\u788d\u548c\u9a71\u52a8\u56e0\u7d20\u3002", "method": "\u901a\u8fc7\u5bf917\u4f4d\u4e13\u5bb6\u7684\u5b9a\u6027\u8bbf\u8c08\uff0c\u8fdb\u884c\u751f\u6001\u7cfb\u7edf\u5206\u6790\u4ee5\u8bc6\u522bXR\u91c7\u7eb3\u4e2d\u7684\u5173\u952e\u969c\u788d\u548c\u4fc3\u8fdb\u56e0\u7d20\u3002", "result": "\u672c\u7814\u7a76\u901a\u8fc7\u5bf917\u4f4d\u4e13\u5bb6\u7684\u8bbf\u8c08\uff0c\u8bc6\u522b\u5e76\u5206\u6790\u4e86\u5de5\u4e1a\u6269\u5c55\u73b0\u5b9e\uff08XR\uff09\u5728\u63a8\u5e7f\u4e2d\u7684\"Pilot Trap\"\u73b0\u8c61\uff0c\u6307\u51fa\u6210\u529f\u7684XR\u573a\u666f\u5f80\u5f80\u96be\u4ee5\u5728\u7ec4\u7ec7\u4e2d\u5927\u89c4\u6a21\u5b9e\u65bd\u3002\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u5173\u952e\u7684\u969c\u788d\u5df2\u4ece\u6280\u672f\u6210\u719f\u5ea6\u8f6c\u5411\u7ec4\u7ec7\u51c6\u5907\u5ea6\uff0c\u5982\u53d8\u66f4\u7ba1\u7406\u548c\u653f\u6cbb\u963b\u529b\u7b49\u3002", "conclusion": "\u6210\u529f\u7684\u5de5\u4e1aXR\u91c7\u7eb3\u9700\u8981\u4ece\u4ee5\u6280\u672f\u4e3a\u4e2d\u5fc3\u7684\u8bd5\u70b9\u8f6c\u5411\u4ee5\u95ee\u9898\u4e3a\u5bfc\u5411\u7684\u7ec4\u7ec7\u53d8\u9769\u65b9\u6cd5\uff0c\u5f3a\u8c03\u751f\u6001\u7cfb\u7edf\u5c42\u9762\u7684\u534f\u8c03\u3002"}}
{"id": "2601.09318", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.09318", "abs": "https://arxiv.org/abs/2601.09318", "authors": ["Ro'i Lang", "Elon Rimon"], "title": "Feedback-Based Mobile Robot Navigation in 3-D Environments Using Artificial Potential Functions Technical Report", "comment": null, "summary": "This technical report presents the construction and analysis of polynomial navigation functions for motion planning in 3-D workspaces populated by spherical and cylindrical obstacles. The workspace is modeled as a bounded spherical region, and obstacles are encoded using smooth polynomial implicit functions. We establish conditions under which the proposed navigation functions admit a unique non-degenerate minimum at the target while avoiding local minima, including in the presence of pairwise intersecting obstacles. Gradient and Hessian analyses are provided, and the theoretical results are validated through numerical simulations in obstacle rich 3-D environments.", "AI": {"tldr": "\u672c\u62a5\u544a\u63a2\u8ba8\u4e86\u5728\u590d\u6742\u4e09\u7ef4\u73af\u5883\u4e2d\u4f7f\u7528\u591a\u9879\u5f0f\u5bfc\u822a\u51fd\u6570\u8fdb\u884c\u8fd0\u52a8\u89c4\u5212\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u907f\u514d\u5c40\u90e8\u6700\u5c0f\u503c\u7684\u6761\u4ef6\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u7406\u8bba\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u4e09\u7ef4\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u969c\u788d\u7269\uff08\u7403\u5f62\u548c\u67f1\u5f62\uff09\u5b58\u5728\u4e0b\u7684\u6709\u6548\u8def\u5f84\u89c4\u5212\u3002", "method": "\u6784\u5efa\u5149\u6ed1\u591a\u9879\u5f0f\u9690\u5f0f\u51fd\u6570\u7684\u5bfc\u822a\u51fd\u6570\uff0c\u5e76\u5206\u6790\u5176\u5728\u76ee\u6807\u4f4d\u7f6e\u7684\u552f\u4e00\u975e\u9000\u5316\u6700\u5c0f\u503c\u7684\u6761\u4ef6\uff0c\u4f7f\u7528\u68af\u5ea6\u548c\u6d77\u68ee\u77e9\u9635\u5206\u6790\u3002", "result": "\u5728\u542b\u6709\u4ea4\u53c9\u969c\u788d\u7269\u7684\u60c5\u51b5\u4e0b\uff0c\u63a8\u52a8\u5bfc\u822a\u51fd\u6570\u5728\u76ee\u6807\u4f4d\u7f6e\u8fbe\u5230\u552f\u4e00\u975e\u9000\u5316\u6700\u5c0f\u503c\uff0c\u907f\u514d\u5c40\u90e8\u6700\u5c0f\u503c\u3002\u901a\u8fc7\u6570\u503c\u4eff\u771f\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u591a\u9879\u5f0f\u5bfc\u822a\u51fd\u6570\u80fd\u6709\u6548\u5b9e\u73b0\u590d\u6742\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\uff0c\u5e76\u4e14\u5728\u9047\u5230\u969c\u788d\u7269\u65f6\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2601.09048", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.09048", "abs": "https://arxiv.org/abs/2601.09048", "authors": ["Yuki Kobayashi", "Koichi Toida"], "title": "Immersive XR That Moves People: How XR Advertising Transforms Comprehension, Empathy, and Behavioural Intention", "comment": "8 pages, 3 figures. Experimental study comparing immersive XR and non-immersive 2D advertising", "summary": "Extended Reality (XR) affords an enhanced sense of bodily presence that supports experiential modes of comprehension and affective engagement which exceed the possibilities of conventional information delivery. Nevertheless, the psychological processes engendered by XR, and the manner in which these processes inform subsequent behavioural intentions, remain only partially delineated. The present study addresses this issue within an applied context by comparing non-immersive 2D viewing advertising with immersive XR experiential advertising. We examined whether XR strengthens internal responses to a product, specifically perceived comprehension and empathy, and whether these responses, in turn, influence the behavioural outcome of purchase intention. A repeated-measures two-way ANOVA demonstrated a significant main effect of advertising modality, with XR yielding higher ratings on all evaluative dimensions. Mediation analysis further indicated that the elevation in purchase intention was mediated by empathy, whereas no significant mediating effect was observed for comprehension within the scope of this study. These findings suggest that immersive XR experiences augment empathic engagement with virtual products, and that this enhanced empathy plays a pivotal role in shaping subsequent behavioural intentions.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u6c89\u6d78\u5f0f\u7684XR\u5e7f\u544a\u6bd4\u4f20\u7edf2D\u5e7f\u544a\u80fd\u66f4\u597d\u5730\u63d0\u9ad8\u7528\u6237\u5bf9\u4ea7\u54c1\u7684\u5171\u60c5\uff0c\u8fd9\u79cd\u5171\u60c5\u8fdb\u4e00\u6b65\u6fc0\u52b1\u4e86\u8d2d\u4e70\u610f\u56fe\u3002", "motivation": "\u63a2\u8ba8XR\u6240\u5f15\u53d1\u7684\u5fc3\u7406\u8fc7\u7a0b\u4ee5\u53ca\u8fd9\u4e9b\u8fc7\u7a0b\u5982\u4f55\u5f71\u54cd\u540e\u7eed\u7684\u8d2d\u4e70\u610f\u56fe\uff0c\u586b\u8865\u4f20\u7edf\u4fe1\u606f\u4f20\u9012\u65b9\u5f0f\u4e0eXR\u95f4\u7a7a\u767d\u7684\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u91cd\u590d\u6d4b\u91cf\u7684\u53cc\u56e0\u7d20\u65b9\u5dee\u5206\u6790\uff08ANOVA\uff09\u6bd4\u8f83\u975e\u6c89\u6d78\u5f0f\u76842D\u5e7f\u544a\u548c\u6c89\u6d78\u5f0fXR\u5e7f\u544a; \u8fdb\u884c\u4e2d\u4ecb\u5206\u6790\u9a8c\u8bc1\u5171\u9e23\u548c\u7406\u89e3\u5728\u8d2d\u4e70\u610f\u56fe\u4e2d\u7684\u4f5c\u7528\u3002", "result": "XR\u5e7f\u544a\u5728\u6240\u6709\u8bc4\u4f30\u7ef4\u5ea6\u4e0a\u7ed9\u4e88\u4e86\u66f4\u9ad8\u7684\u8bc4\u5206\uff0c\u5176\u4e2d\u540c\u60c5\u5fc3\u5728\u8d2d\u4e70\u610f\u56fe\u7684\u63d0\u5347\u4e2d\u8d77\u5230\u4e86\u4e2d\u4ecb\u4f5c\u7528\uff0c\u800c\u7406\u89e3\u5728\u6b64\u6b21\u7814\u7a76\u4e2d\u672a\u663e\u793a\u51fa\u663e\u8457\u7684\u4e2d\u4ecb\u6548\u679c\u3002", "conclusion": "\u6c89\u6d78\u5f0f\u7684XR\u4f53\u9a8c\u80fd\u591f\u589e\u5f3a\u5bf9\u865a\u62df\u4ea7\u54c1\u7684\u5171\u9e23\uff0c\u800c\u8fd9\u79cd\u589e\u5f3a\u7684\u5171\u9e23\u5728\u5851\u9020\u540e\u7eed\u7684\u884c\u4e3a\u610f\u56fe\u4e2d\u626e\u6f14\u4e86\u5173\u952e\u89d2\u8272\u3002"}}
{"id": "2601.09377", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.09377", "abs": "https://arxiv.org/abs/2601.09377", "authors": ["Xuemei Yao", "Xiao Yang", "Jianbin Sun", "Liuwei Xie", "Xuebin Shao", "Xiyu Fang", "Hang Su", "Kewei Yang"], "title": "ReflexDiffusion: Reflection-Enhanced Trajectory Planning for High-lateral-acceleration Scenarios in Autonomous Driving", "comment": "Accepted by AAAI 2026", "summary": "Generating safe and reliable trajectories for autonomous vehicles in long-tail scenarios remains a significant challenge, particularly for high-lateral-acceleration maneuvers such as sharp turns, which represent critical safety situations. Existing trajectory planners exhibit systematic failures in these scenarios due to data imbalance. This results in insufficient modelling of vehicle dynamics, road geometry, and environmental constraints in high-risk situations, leading to suboptimal or unsafe trajectory prediction when vehicles operate near their physical limits. In this paper, we introduce ReflexDiffusion, a novel inference-stage framework that enhances diffusion-based trajectory planners through reflective adjustment. Our method introduces a gradient-based adjustment mechanism during the iterative denoising process: after each standard trajectory update, we compute the gradient between the conditional and unconditional noise predictions to explicitly amplify critical conditioning signals, including road curvature and lateral vehicle dynamics. This amplification enforces strict adherence to physical constraints, particularly improving stability during high-lateral-acceleration maneuvers where precise vehicle-road interaction is paramount. Evaluated on the nuPlan Test14-hard benchmark, ReflexDiffusion achieves a 14.1% improvement in driving score for high-lateral-acceleration scenarios over the state-of-the-art (SOTA) methods. This demonstrates that inference-time trajectory optimization can effectively compensate for training data sparsity by dynamically reinforcing safety-critical constraints near handling limits. The framework's architecture-agnostic design enables direct deployment to existing diffusion-based planners, offering a practical solution for improving autonomous vehicle safety in challenging driving conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7684ReflexDiffusion\u901a\u8fc7\u53cd\u5c04\u8c03\u6574\u673a\u5236\u63d0\u9ad8\u4e86\u57fa\u4e8e\u6269\u6563\u7684\u8f68\u8ff9\u89c4\u5212\u5668\u7684\u6027\u80fd\uff0c\u5728\u9ad8\u6a2a\u5411\u52a0\u901f\u5ea6\u7684\u573a\u666f\u4e0b\uff0c\u6210\u529f\u5f25\u8865\u4e86\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8f66\u8f86\u5b89\u5168\u6027\u3002", "motivation": "\u5728\u957f\u5c3e\u573a\u666f\u4e2d\uff0c\u4e3a\u81ea\u4e3b\u8f66\u8f86\u751f\u6210\u5b89\u5168\u4e14\u53ef\u9760\u7684\u8f68\u8ff9\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u6a2a\u5411\u52a0\u901f\u5ea6\u64cd\u4f5c\u4e0b\uff0c\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u8f68\u8ff9\u89c4\u5212\u5668\u5728\u8fd9\u4e9b\u5173\u952e\u5b89\u5168\u60c5\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u8f68\u8ff9\u9884\u6d4b\u4e0d\u51c6\u786e\u6216\u4e0d\u5b89\u5168\u3002", "method": "ReflexDiffusion\u6846\u67b6\u901a\u8fc7\u5728\u8fed\u4ee3\u53bb\u566a\u8fc7\u7a0b\u4e2d\u6267\u884c\u57fa\u4e8e\u68af\u5ea6\u7684\u8c03\u6574\u673a\u5236\uff0c\u589e\u5f3a\u4e86\u57fa\u4e8e\u6269\u6563\u7684\u8f68\u8ff9\u89c4\u5212\u5668\u7684\u6027\u80fd\u3002\u6bcf\u6b21\u6807\u51c6\u8f68\u8ff9\u66f4\u65b0\u540e\uff0c\u8ba1\u7b97\u6761\u4ef6\u566a\u58f0\u548c\u65e0\u6761\u4ef6\u566a\u58f0\u9884\u6d4b\u4e4b\u95f4\u7684\u68af\u5ea6\uff0c\u4ee5\u5f3a\u8c03\u5173\u952e\u7684\u8c03\u8282\u4fe1\u53f7\u3002", "result": "\u5728nuPlan Test14-hard\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cReflexDiffusion\u5728\u9ad8\u6a2a\u5411\u52a0\u901f\u5ea6\u573a\u666f\u4e2d\u6bd4\u73b0\u6709\u7684\u6700\u5148\u8fdb\uff08SOTA\uff09\u65b9\u6cd5\u63d0\u9ad8\u4e8614.1%\u7684\u9a7e\u9a76\u5f97\u5206\u3002", "conclusion": "ReflexDiffusion\u5728\u9ad8\u6a2a\u5411\u52a0\u901f\u5ea6\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5728\u63d0\u5347\u81ea\u4e3b\u8f66\u8f86\u5b89\u5168\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.09053", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.09053", "abs": "https://arxiv.org/abs/2601.09053", "authors": ["Haiyi Li", "Yutong Li", "Yiheng Chi", "Alison Deslandes", "Mathew Leonardi", "Shay Freger", "Yuan Zhang", "Jodie Avery", "M. Louise Hull", "Hsiang-Ting Chen"], "title": "Evaluating local large language models for structured extraction from endometriosis-specific transvaginal ultrasound reports", "comment": null, "summary": "In this study, we evaluate a locally-deployed large-language model (LLM) to convert unstructured endometriosis transvaginal ultrasound (eTVUS) scan reports into structured data for imaging informatics workflows. Across 49 eTVUS reports, we compared three LLMs (7B/8B and a 20B-parameter model) against expert human extraction. The 20B model achieved a mean accuracy of 86.02%, substantially outperforming smaller models and confirming the importance of scale in handling complex clinical text. Crucially, we identified a highly complementary error profile: the LLM excelled at syntactic consistency (e.g., date/numeric formatting) where humans faltered, while human experts provided superior semantic and contextual interpretation. We also found that the LLM's semantic errors were fundamental limitations that could not be mitigated by simple prompt engineering. These findings strongly support a human-in-the-loop (HITL) workflow in which the on-premise LLM serves as a collaborative tool, not a full replacement. It automates routine structuring and flags potential human errors, enabling imaging specialists to focus on high-level semantic validation. We discuss implications for structured reporting and interactive AI systems in clinical practice.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e00\u79cd\u672c\u5730\u90e8\u7f72\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5c06\u975e\u7ed3\u6784\u5316\u7684\u5185\u819c\u5f02\u4f4d\u75c7\u7ecf\u9634\u9053\u8d85\u58f0\u626b\u63cf\u62a5\u544a\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u6570\u636e\u300220B\u6a21\u578b\u8fbe\u5230\u4e8686.02%\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u8f83\u5c0f\u7684\u6a21\u578b\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86\u4eba\u673a\u534f\u4f5c\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u968f\u7740\u5f71\u50cf\u4fe1\u606f\u5b66\u5de5\u4f5c\u6d41\u7684\u9700\u6c42\u589e\u52a0\uff0c\u9700\u5c06\u975e\u7ed3\u6784\u5316\u7684\u4e34\u5e8a\u6587\u672c\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u6570\u636e\uff0c\u4ee5\u63d0\u5347\u6570\u636e\u7684\u53ef\u7528\u6027\u548c\u5206\u6790\u80fd\u529b\u3002", "method": "\u7814\u7a76\u4e2d\u6bd4\u8f83\u4e86\u4e09\u79cd\u4e0d\u540c\u53c2\u6570\u7684\u6a21\u578b\uff087B/8B\u548c20B\uff09\u572849\u4efdeTVUS\u62a5\u544a\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u7684\u63d0\u53d6\u7ed3\u679c\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "20B\u6a21\u578b\u5728\u8bed\u6cd5\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u4eba\u7c7b\u4e13\u5bb6\uff0c\u6574\u4f53\u51c6\u786e\u7387\u4e3a86.02%\u3002\u7136\u800c\uff0cLLM\u5728\u8bed\u4e49\u7406\u89e3\u4e0a\u5b58\u5728\u57fa\u672c\u5c40\u9650\uff0c\u9700\u7ed3\u5408\u4eba\u7c7b\u4e13\u5bb6\u8fdb\u884c\u9a8c\u8bc1\u3002", "conclusion": "\u672c\u7814\u7a76\u652f\u6301\u4eba\u673a\u534f\u4f5c\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5176\u4e2d\u672c\u5730\u90e8\u7f72\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u534f\u4f5c\u5de5\u5177\uff0c\u800c\u975e\u5b8c\u5168\u66ff\u4ee3\u3002"}}
{"id": "2601.09444", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.09444", "abs": "https://arxiv.org/abs/2601.09444", "authors": ["Lauri Suomela", "Naoki Takahata", "Sasanka Kuruppu Arachchige", "Harry Edelman", "Joni-Kristian K\u00e4m\u00e4r\u00e4inen"], "title": "Data Scaling for Navigation in Unknown Environments", "comment": null, "summary": "Generalization of imitation-learned navigation policies to environments unseen in training remains a major challenge. We address this by conducting the first large-scale study of how data quantity and data diversity affect real-world generalization in end-to-end, map-free visual navigation. Using a curated 4,565-hour crowd-sourced dataset collected across 161 locations in 35 countries, we train policies for point goal navigation and evaluate their closed-loop control performance on sidewalk robots operating in four countries, covering 125 km of autonomous driving.\n  Our results show that large-scale training data enables zero-shot navigation in unknown environments, approaching the performance of policies trained with environment-specific demonstrations. Critically, we find that data diversity is far more important than data quantity. Doubling the number of geographical locations in a training set decreases navigation errors by ~15%, while performance benefit from adding data from existing locations saturates with very little data. We also observe that, with noisy crowd-sourced data, simple regression-based models outperform generative and sequence-based architectures. We release our policies, evaluation setup and example videos on the project page.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u6570\u636e\u91cf\u4e0e\u591a\u6837\u6027\u5bf9\u5b9e\u9645\u73af\u5883\u4e2d\u89c6\u89c9\u5bfc\u822a\u653f\u7b56\u7684\u4e00\u4f53\u5316\u5b66\u4e60\u7684\u5f71\u54cd\uff0c\u8868\u660e\u591a\u6837\u6027\u6bd4\u6570\u91cf\u66f4\u91cd\u8981\u3002", "motivation": "\u89e3\u51b3\u6a21\u4eff\u5b66\u4e60\u5bfc\u822a\u653f\u7b56\u5728\u672a\u89c1\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u8de8161\u4e2a\u5730\u70b9\u641c\u96c6\u76844,565\u5c0f\u65f6\u7684\u4f17\u5305\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u70b9\u76ee\u6807\u5bfc\u822a\u653f\u7b56\uff0c\u5e76\u8bc4\u4f30\u5176\u5728\u56db\u4e2a\u56fd\u5bb6\u7684\u81ea\u9a7e\u673a\u5668\u4eba\u4e0a\u7684\u95ed\u73af\u63a7\u5236\u6027\u80fd\u3002", "result": "\u901a\u8fc7\u5927\u89c4\u6a21\u7684\u6570\u636e\u8bad\u7ec3\uff0c\u5bfc\u822a\u653f\u7b56\u80fd\u591f\u5728\u672a\u77e5\u73af\u5883\u4e2d\u5b9e\u73b0\u96f6-shot\u5bfc\u822a\uff0c\u63a5\u8fd1\u4e8e\u4f7f\u7528\u7279\u5b9a\u73af\u5883\u793a\u8303\u8bad\u7ec3\u7684\u653f\u7b56\u7684\u6027\u80fd\u3002", "conclusion": "\u5927\u6570\u636e\u7684\u591a\u6837\u6027\u5bf9\u5bfc\u822a\u653f\u7b56\u7684\u6cdb\u5316\u80fd\u529b\u5f71\u54cd\u66f4\u4e3a\u663e\u8457\uff0c\u589e\u52a0\u65b0\u5730\u7406\u4f4d\u7f6e\u7684\u6570\u636e\u4f1a\u663e\u8457\u964d\u4f4e\u5bfc\u822a\u8bef\u5dee\uff0c\u800c\u73b0\u6709\u4f4d\u7f6e\u7684\u6570\u636e\u91cf\u589e\u52a0\u7684\u6548\u679c\u5219\u9010\u6e10\u51cf\u5f31\u3002"}}
{"id": "2601.09150", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.09150", "abs": "https://arxiv.org/abs/2601.09150", "authors": ["Jianwen Sun", "Yukang Feng", "Kaining Ying", "Chuanhao Li", "Zizhen Li", "Fanrui Zhang", "Jiaxin Ai", "Yifan Chang", "Yu Dai", "Yifei Huang", "Kaipeng Zhang"], "title": "World Craft: Agentic Framework to Create Visualizable Worlds via Text", "comment": null, "summary": "Large Language Models (LLMs) motivate generative agent simulation (e.g., AI Town) to create a ``dynamic world'', holding immense value across entertainment and research. However, for non-experts, especially those without programming skills, it isn't easy to customize a visualizable environment by themselves. In this paper, we introduce World Craft, an agentic world creation framework to create an executable and visualizable AI Town via user textual descriptions. It consists of two main modules, World Scaffold and World Guild. World Scaffold is a structured and concise standardization to develop interactive game scenes, serving as an efficient scaffolding for LLMs to customize an executable AI Town-like environment. World Guild is a multi-agent framework to progressively analyze users' intents from rough descriptions, and synthesizes required structured contents (\\eg environment layout and assets) for World Scaffold . Moreover, we construct a high-quality error-correction dataset via reverse engineering to enhance spatial knowledge and improve the stability and controllability of layout generation, while reporting multi-dimensional evaluation metrics for further analysis. Extensive experiments demonstrate that our framework significantly outperforms existing commercial code agents (Cursor and Antigravity) and LLMs (Qwen3 and Gemini-3-Pro). in scene construction and narrative intent conveyance, providing a scalable solution for the democratization of environment creation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u7528\u6237\u6587\u672c\u63cf\u8ff0\u521b\u5efa\u53ef\u89c6\u5316AI\u57ce\u9547\u7684\u6846\u67b6World Craft\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u6548\u7387\u548c\u8d28\u91cf\u3002", "motivation": "\u4e3a\u4e86\u5e2e\u52a9\u975e\u4e13\u4e1a\u4eba\u58eb\u548c\u7f3a\u4e4f\u7f16\u7a0b\u6280\u80fd\u7684\u7528\u6237\u66f4\u8f7b\u677e\u5730\u81ea\u5b9a\u4e49\u52a8\u6001\u4e16\u754c\u3002", "method": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aWorld Craft\u7684\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u7528\u6237\u6587\u672c\u63cf\u8ff0\u521b\u5efa\u53ef\u6267\u884c\u548c\u53ef\u89c6\u5316\u7684AI\u57ce\u9547\u3002", "result": "\u901a\u8fc7\u4e24\u4e2a\u4e3b\u8981\u6a21\u5757World Scaffold\u548cWorld Guild\uff0c\u52a0\u4e0a\u9ad8\u8d28\u91cf\u7684\u9519\u8bef\u4fee\u6b63\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5e03\u5c40\u751f\u6210\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u63a7\u6027\uff0c\u5e76\u5728\u573a\u666f\u6784\u5efa\u548c\u53d9\u4e8b\u610f\u56fe\u4f20\u8fbe\u65b9\u9762\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u5546\u4e1a\u4ee3\u7801\u4ee3\u7406\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u73af\u5883\u521b\u5efa\u7684\u6c11\u4e3b\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u8ba9\u975e\u4e13\u5bb6\u7528\u6237\u80fd\u591f\u8f7b\u677e\u5b9a\u5236\u89c6\u89c9\u5316\u73af\u5883\u3002"}}
{"id": "2601.09512", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.09512", "abs": "https://arxiv.org/abs/2601.09512", "authors": ["Ralf R\u00f6mer", "Yi Zhang", "Angela P. Schoellig"], "title": "CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion", "comment": "Project page: https://tum-lsy.github.io/clare. 9 pages, 5 figures", "summary": "To teach robots complex manipulation tasks, it is now a common practice to fine-tune a pre-trained vision-language-action model (VLA) on task-specific data. However, since this recipe updates existing representations, it is unsuitable for long-term operation in the real world, where robots must continually adapt to new tasks and environments while retaining the knowledge they have already acquired. Existing continual learning methods for robotics commonly require storing previous data (exemplars), struggle with long task sequences, or rely on task identifiers for deployment. To address these limitations, we propose CLARE, a general, parameter-efficient framework for exemplar-free continual learning with VLAs. CLARE introduces lightweight modular adapters into selected feedforward layers and autonomously expands the model only where necessary when learning a new task, guided by layer-wise feature similarity. During deployment, an autoencoder-based routing mechanism dynamically activates the most relevant adapters without requiring task labels. Through extensive experiments on the LIBERO benchmark, we show that CLARE achieves high performance on new tasks without catastrophic forgetting of earlier tasks, significantly outperforming even exemplar-based methods. Code and data are available at https://tum-lsy.github.io/clare.", "AI": {"tldr": "CLARE\u662f\u4e00\u4e2a\u65e0\u793a\u4f8b\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u5728\u65b0\u4efb\u52a1\u4e2d\u4fdd\u6301\u9ad8\u6027\u80fd\u4e14\u4e0d\u4f1a\u9057\u5fd8\u65e7\u77e5\u8bc6\u3002", "motivation": "\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u89c6-\u8bed-\u884c\u52a8\u6a21\u578b\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u5fae\u8c03\u4e0d\u9002\u5408\u957f\u671f\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u64cd\u4f5c\uff0c\u56e0\u5176\u65e0\u6cd5\u6301\u7eed\u9002\u5e94\u65b0\u4efb\u52a1\u548c\u73af\u5883\u800c\u4e0d\u5fd8\u8bb0\u5df2\u5b66\u77e5\u8bc6\u3002", "method": "\u63d0\u51faCLARE\uff0c\u4e00\u4e2a\u901a\u7528\u7684\u3001\u53c2\u6570\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u5b9e\u73b0\u65e0\u793a\u4f8b\u6301\u7eed\u5b66\u4e60\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u6a21\u5757\u9002\u914d\u5668\u548c\u81ea\u52a8\u7f16\u7801\u5668\u8def\u7531\u673a\u5236\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u663e\u793a\uff0cCLARE\u5728\u65b0\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u4e14\u672a\u51fa\u73b0\u707e\u96be\u6027\u9057\u5fd8\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u793a\u4f8b\u7684\u65b9\u6cd5\u3002", "conclusion": "CLARE\u6709\u6548\u89e3\u51b3\u957f\u671f\u64cd\u4f5c\u4e2d\u7684\u6301\u7eed\u5b66\u4e60\u95ee\u9898\uff0c\u5177\u6709\u8f83\u597d\u7684\u9002\u5e94\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2601.09208", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09208", "abs": "https://arxiv.org/abs/2601.09208", "authors": ["Miki Ueno"], "title": "Mikasa: A Character-Driven Emotional AI Companion Inspired by Japanese Oshi Culture", "comment": "11 pages, 1 figure", "summary": "Recent progress in large language models and multimodal interaction has made it possible to develop AI companions that can have fluent and emotionally expressive conversations. However, many of these systems have problems keeping users satisfied and engaged over long periods. This paper argues that these problems do not come mainly from weak models, but from poor character design and unclear definitions of the user-AI relationship. I present Mikasa, an emotional AI companion inspired by Japanese Oshi culture-specifically its emphasis on long-term, non-exclusive commitment to a stable character-as a case study of character-driven companion design. Mikasa does not work as a general-purpose assistant or a chatbot that changes roles. Instead, Mikasa is designed as a coherent character with a stable personality and a clearly defined relationship as a partner. This relationship does not force exclusivity or obligation. Rather, it works as a reference point that stabilizes interaction norms and reduces the work users must do to keep redefining the relationship. Through an exploratory evaluation, I see that users describe their preferences using surface-level qualities such as conversational naturalness, but they also value relationship control and imaginative engagement in ways they do not state directly. These results suggest that character coherence and relationship definition work as latent structural elements that shape how good the interaction feels, without users recognizing them as main features. The contribution of this work is to show that character design is a functional part of AI companion systems, not just decoration. Mikasa is one example based on a specific cultural context, but the design principles-commitment to a consistent personality and clear relationship definition-can be used for many emotionally grounded AI companions.", "AI": {"tldr": "\u63d0\u51faMikasa\uff0c\u5f3a\u8c03\u60c5\u611fAI\u966a\u4f34\u8005\u7684\u89d2\u8272\u4e00\u81f4\u6027\u548c\u5173\u7cfb\u5b9a\u4e49\u5bf9\u7528\u6237\u4f53\u9a8c\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfAI\u966a\u4f34\u8005\u5728\u957f\u671f\u4e92\u52a8\u4e2d\u7528\u6237\u6ee1\u610f\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u91cd\u70b9\u653e\u5728\u89d2\u8272\u8bbe\u8ba1\u548c\u7528\u6237-AI\u5173\u7cfb\u7684\u6e05\u6670\u5b9a\u4e49\u4e0a\u3002", "method": "\u6848\u4f8b\u7814\u7a76\uff0c\u8bbe\u8ba1\u4ee5\u89d2\u8272\u4e3a\u9a71\u52a8\u7684AI\u966a\u4f34\u8005\uff0c\u5e76\u901a\u8fc7\u63a2\u7d22\u6027\u8bc4\u4f30\u5206\u6790\u7528\u6237\u4e92\u52a8\u4f53\u9a8c\u3002", "result": "\u63d0\u51faMikasa\uff0c\u4e00\u4e2a\u60c5\u611fAI\u966a\u4f34\u8005\uff0c\u5c55\u793a\u89d2\u8272\u4e00\u81f4\u6027\u548c\u5173\u7cfb\u5b9a\u4e49\u5982\u4f55\u5f71\u54cd\u4ea4\u4e92\u8d28\u91cf\u3002", "conclusion": "\u89d2\u8272\u8bbe\u8ba1\u662fAI\u966a\u4f34\u7cfb\u7edf\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u4e0d\u4ec5\u662f\u88c5\u9970\uff0c\u80fd\u663e\u8457\u63d0\u9ad8\u7528\u6237\u6ee1\u610f\u5ea6\u548c\u53c2\u4e0e\u5ea6\u3002"}}
{"id": "2601.09518", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.09518", "abs": "https://arxiv.org/abs/2601.09518", "authors": ["Wei-Jin Huang", "Yue-Yi Zhang", "Yi-Lin Wei", "Zhi-Wei Xia", "Juantao Tan", "Yuan-Ming Li", "Zhilin Zhao", "Wei-Shi Zheng"], "title": "Learning Whole-Body Human-Humanoid Interaction from Human-Human Demonstrations", "comment": null, "summary": "Enabling humanoid robots to physically interact with humans is a critical frontier, but progress is hindered by the scarcity of high-quality Human-Humanoid Interaction (HHoI) data. While leveraging abundant Human-Human Interaction (HHI) data presents a scalable alternative, we first demonstrate that standard retargeting fails by breaking the essential contacts. We address this with PAIR (Physics-Aware Interaction Retargeting), a contact-centric, two-stage pipeline that preserves contact semantics across morphology differences to generate physically consistent HHoI data. This high-quality data, however, exposes a second failure: conventional imitation learning policies merely mimic trajectories and lack interactive understanding. We therefore introduce D-STAR (Decoupled Spatio-Temporal Action Reasoner), a hierarchical policy that disentangles when to act from where to act. In D-STAR, Phase Attention (when) and a Multi-Scale Spatial module (where) are fused by the diffusion head to produce synchronized whole-body behaviors beyond mimicry. By decoupling these reasoning streams, our model learns robust temporal phases without being distracted by spatial noise, leading to responsive, synchronized collaboration. We validate our framework through extensive and rigorous simulations, demonstrating significant performance gains over baseline approaches and a complete, effective pipeline for learning complex whole-body interactions from HHI data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPAIR\u7684\u7269\u7406\u611f\u77e5\u4e92\u52a8\u91cd\u5b9a\u5411\u65b9\u6cd5\u548cD-STAR\u7684\u5206\u89e3\u65f6\u7a7a\u52a8\u4f5c\u63a8\u7406\u653f\u7b56\uff0c\u4ee5\u6539\u8fdb\u4eba\u5f62\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u7684\u4ea4\u4e92\u80fd\u529b\uff0c\u5e76\u5728\u6a21\u62df\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u9ad8\u8d28\u91cf\u7684\u4eba-\u4eba\u5f62\u4e92\u52a8\u6570\u636e\u7a00\u7f3a\uff0c\u4f20\u7edf\u7684\u91cd\u5b9a\u5411\u65b9\u6cd5\u672a\u80fd\u4fdd\u7559\u5173\u952e\u63a5\u89e6\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u65b0\u7684\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u4eba\u5f62\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u7684\u4e92\u52a8\u80fd\u529b\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u4e86\u4e24\u9636\u6bb5\u7ba1\u9053PAIR\uff0c\u4e13\u6ce8\u4e8e\u63a5\u89e6\u8bed\u4e49\uff0c\u5e76\u5f15\u5165D-STAR\u4ee5\u5206\u89e3\u4e92\u52a8\u4e2d\u7684\u65f6\u7a7a\u884c\u4e3a\uff0c\u4f7f\u5f97\u673a\u5668\u4eba\u80fd\u591f\u66f4\u597d\u5730\u8fdb\u884c\u540c\u6b65\u534f\u4f5c\u3002", "result": "\u901a\u8fc7\u7cfb\u7edf\u7684\u6a21\u62df\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u6846\u67b6\u5728\u590d\u6742\u5168\u8eab\u4e92\u52a8\u7684\u5b66\u4e60\u4e0a\u6709\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u6211\u4eec\u7684\u6846\u67b6\u901a\u8fc7\u5e7f\u6cdb\u7684\u6a21\u62df\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u76f8\u8f83\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7684\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u4ece\u4eba\u7c7b\u4e92\u52a8\u6570\u636e\u4e2d\u5b66\u4e60\u590d\u6742\u5168\u8eab\u4e92\u52a8\u7684\u5b8c\u6574\u7ba1\u9053\u3002"}}
{"id": "2601.09610", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.09610", "abs": "https://arxiv.org/abs/2601.09610", "authors": ["Marie Luisa Fiedler", "Christian Merz", "Jonathan Tschanter", "Carolin Wienrich", "Marc Erich Latoschik"], "title": "Technological Advances in Two Generations of Consumer-Grade VR Systems: Effects on User Experience and Task Performance", "comment": "12 pages, 4 figures, 7 tables", "summary": "Integrated VR (IVR) systems consist of a head-mounted display (HMD) and body-tracking capabilities. They enable users to translate their physical movements into corresponding avatar movements in real-time, allowing them to perceive their avatars via the displays. Consumer-grade IVR systems have been available for 10 years, significantly fostering VR research worldwide. However, the effects of even apparently significant technological advances of IVR systems on user experience and the overall validity of prior embodiment research using such systems often remain unclear. We ran a user-centered study comparing two comparable IVR generations: a nearly 10-year-old hardware (HTC Vive, 6-point tracking) and a modern counterpart (HTC Vive Pro 2, 6-point tracking). To ensure ecological validity, we evaluated the systems in their commercially available, as-is configurations. In a 2x5 mixed design, participants completed five tasks covering different use cases on either the old or new system. We assessed presence, sense of embodiment, appearance and behavior plausibility, workload, task performance, and gathered qualitative feedback. Results showed no significant system differences, with only small effect sizes. Bayesian analysis further supported the null hypothesis, suggesting that the investigated generational hardware improvements offer limited benefits for user experience and task performance. For the 10-year generational step examined here, excluding potential technological progress in the necessary software components, this supports the validity of conclusions from prior work and underscores the applicability of older configurations for research in embodied VR.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u4e24\u4ee3\u96c6\u6210\u865a\u62df\u73b0\u5b9e\u7cfb\u7edf\uff0c\u7ed3\u679c\u8868\u660e\u786c\u4ef6\u8fdb\u6b65\u5bf9\u7528\u6237\u4f53\u9a8c\u5f71\u54cd\u6709\u9650\uff0c\u5148\u524d\u7814\u7a76\u7684\u6709\u6548\u6027\u5f97\u4ee5\u652f\u6301\u3002", "motivation": "\u5c3d\u7ba1\u6d88\u8d39\u8005\u7ea7\u96c6\u6210\u865a\u62df\u73b0\u5b9e\u7cfb\u7edf\u5df2\u7ecf\u5b58\u5728\u5341\u5e74\uff0c\u4f46\u5176\u5bf9\u7528\u6237\u4f53\u9a8c\u7684\u5b9e\u9645\u5f71\u54cd\u4ecd\u4e0d\u660e\u786e\uff0c\u6fc0\u52b1\u4e86\u5bf9\u5176\u91cd\u8981\u6027\u7684\u7814\u7a76\u3002", "method": "\u8fdb\u884c\u7528\u6237\u4e2d\u5fc3\u7684\u7814\u7a76\uff0c\u6bd4\u8f83\u4e24\u4ee3\u76f8\u4f3c\u7684\u96c6\u6210\u865a\u62df\u73b0\u5b9e\u7cfb\u7edf\u6027\u80fd", "result": "\u901a\u8fc7\u8bc4\u4f30\u7528\u6237\u7684\u5b58\u5728\u611f\u3001\u5316\u8eab\u611f\u3001\u5916\u89c2\u4e0e\u884c\u4e3a\u7684\u53ef\u4fe1\u5ea6\u3001\u5de5\u4f5c\u8d1f\u8f7d\u3001\u4efb\u52a1\u8868\u73b0\u7b49\uff0c\u53d1\u73b0\u4e24\u4ee3\u7cfb\u7edf\u6ca1\u6709\u663e\u8457\u5dee\u5f02\uff0c\u53ea\u6709\u5c0f\u7684\u6548\u5e94\u5927\u5c0f\u3002", "conclusion": "10\u5e74\u6280\u672f\u8fdb\u6b65\u7684\u786c\u4ef6\u6539\u5584\u5bf9\u7528\u6237\u4f53\u9a8c\u548c\u4efb\u52a1\u8868\u73b0\u7684\u597d\u5904\u6709\u9650\uff0c\u8fd9\u652f\u6301\u4e86\u5148\u524d\u5de5\u4f5c\u7684\u6709\u6548\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u8f83\u65e7\u914d\u7f6e\u5728\u5316\u8eab\u865a\u62df\u73b0\u5b9e\u7814\u7a76\u4e2d\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2601.09578", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.09578", "abs": "https://arxiv.org/abs/2601.09578", "authors": ["Jiajun Sun", "Yangyi Ou", "Haoyuan Zheng", "Chao yang", "Yue Ma"], "title": "Multimodal Signal Processing For Thermo-Visible-Lidar Fusion In Real-time 3D Semantic Mapping", "comment": "5 pages,7 figures. Under review", "summary": "In complex environments, autonomous robot navigation and environmental perception pose higher requirements for SLAM technology. This paper presents a novel method for semantically enhancing 3D point cloud maps with thermal information. By first performing pixel-level fusion of visible and infrared images, the system projects real-time LiDAR point clouds onto this fused image stream. It then segments heat source features in the thermal channel to instantly identify high temperature targets and applies this temperature information as a semantic layer on the final 3D map. This approach generates maps that not only have accurate geometry but also possess a critical semantic understanding of the environment, making it highly valuable for specific applications like rapid disaster assessment and industrial preventive maintenance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u53ef\u89c1\u5149\u548c\u7ea2\u5916\u56fe\u50cf\u8fdb\u884c\u50cf\u7d20\u7ea7\u878d\u5408\uff0c\u589e\u5f3a\u4e09\u7ef4\u70b9\u4e91\u5730\u56fe\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u5229\u7528\u70ed\u6e90\u7279\u5f81\u8bc6\u522b\u9ad8\u6e29\u76ee\u6807\uff0c\u63d0\u5347\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u548c\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u5728\u590d\u6742\u73af\u5883\u4e2d\uff0c\u81ea\u4e3b\u673a\u5668\u4eba\u7684\u5bfc\u822a\u548c\u73af\u5883\u611f\u77e5\u5bf9SLAM\u6280\u672f\u63d0\u51fa\u4e86\u66f4\u9ad8\u8981\u6c42\uff0c\u4e9f\u9700\u65b0\u7684\u65b9\u6cd5\u6765\u589e\u5f3a\u5730\u56fe\u8bed\u4e49\u4fe1\u606f\u3002", "method": "\u8be5\u65b9\u6cd5\u9996\u5148\u8fdb\u884c\u53ef\u89c1\u5149\u548c\u7ea2\u5916\u56fe\u50cf\u7684\u50cf\u7d20\u7ea7\u878d\u5408\uff0c\u7136\u540e\u5c06\u5b9e\u65f6\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u6295\u5f71\u5230\u878d\u5408\u7684\u56fe\u50cf\u6d41\u4e0a\uff0c\u6700\u540e\u5728\u70ed\u901a\u9053\u4e2d\u5206\u5272\u70ed\u6e90\u7279\u5f81\u4ee5\u8bc6\u522b\u9ad8\u6e29\u76ee\u6807\uff0c\u5e76\u5c06\u5176\u6e29\u5ea6\u4fe1\u606f\u6dfb\u52a0\u4e3a\u4e09\u7ef4\u5730\u56fe\u7684\u8bed\u4e49\u5c42\u3002", "result": "\u4f7f\u7528\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u5730\u56fe\u4e0d\u4ec5\u5177\u5907\u51c6\u786e\u7684\u51e0\u4f55\u4fe1\u606f\uff0c\u8fd8\u62e5\u6709\u5bf9\u73af\u5883\u7684\u5173\u952e\u8bed\u4e49\u7406\u89e3\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u590d\u6742\u73af\u5883\u4e2d\u673a\u5668\u4eba\u7684\u8868\u73b0\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u7ed3\u5408\u70ed\u4fe1\u606f\u7684\u4e09\u7ef4\u70b9\u4e91\u5730\u56fe\u663e\u8457\u63d0\u9ad8\u4e86\u73af\u5883\u611f\u77e5\u80fd\u529b\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5feb\u901f\u707e\u540e\u8bc4\u4f30\u548c\u5de5\u4e1a\u9884\u9632\u7ef4\u62a4\u7b49\u5e94\u7528\u3002"}}
{"id": "2601.09620", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09620", "abs": "https://arxiv.org/abs/2601.09620", "authors": ["Pooja Prajod", "Hannes Cools", "Thomas R\u00f6ggla", "Karthikeya Puttur Venkatraj", "Amber Kusters", "Alia ElKattan", "Pablo Cesar", "Abdallah El Ali"], "title": "Full Disclosure, Less Trust? How the Level of Detail about AI Use in News Writing Affects Readers' Trust", "comment": null, "summary": "As artificial intelligence (AI) is increasingly integrated into news production, calls for transparency about the use of AI have gained considerable traction. Recent studies suggest that AI disclosures can lead to a ``transparency dilemma'', where disclosure reduces readers' trust. However, little is known about how the \\textit{level of detail} in AI disclosures influences trust and contributes to this dilemma within the news context. In this 3$\\times$2$\\times$2 mixed factorial study with 40 participants, we investigate how three levels of AI disclosures (none, one-line, detailed) across two types of news (politics and lifestyle) and two levels of AI involvement (low and high) affect news readers' trust. We measured trust using the News Media Trust questionnaire, along with two decision behaviors: source-checking and subscription decisions. Questionnaire responses and subscription rates showed a decline in trust only for detailed AI disclosures, whereas source-checking behavior increased for both one-line and detailed disclosures, with the effect being more pronounced for detailed disclosures. Insights from semi-structured interviews suggest that source-checking behavior was primarily driven by interest in the topic, followed by trust, whereas trust was the main factor influencing subscription decisions. Around two-thirds of participants expressed a preference for detailed disclosures, while most participants who preferred one-line indicated a need for detail-on-demand disclosure formats. Our findings show that not all AI disclosures lead to a transparency dilemma, but instead reflect a trade-off between readers' desire for more transparency and their trust in AI-assisted news content.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86AI\u62ab\u9732\u7684\u8be6\u7ec6\u7a0b\u5ea6\u5bf9\u65b0\u95fb\u8bfb\u8005\u4fe1\u4efb\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u7ec6\u8282\u62ab\u9732\u53ef\u80fd\u5bfc\u81f4\u4fe1\u4efb\u4e0b\u964d\uff0c\u800c\u4e00\u884c\u62ab\u9732\u63d0\u5347\u4e86\u4fe1\u606f\u6838\u5b9e\u884c\u4e3a\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u878d\u5165\u65b0\u95fb\u751f\u4ea7\uff0c\u516c\u4f17\u5bf9\u900f\u660e\u5ea6\u7684\u547c\u58f0\u65e5\u76ca\u589e\u9ad8\uff0c\u4f46AI\u62ab\u9732\u7684\u6548\u679c\u5c1a\u4e0d\u660e\u786e\uff0c\u7279\u522b\u662f\u5728\u4fe1\u4efb\u95ee\u9898\u4e0a\u3002", "method": "\u91c7\u75283\u00d72\u00d72\u7684\u6df7\u5408\u56e0\u5b50\u8bbe\u8ba1\uff0c\u8fdb\u884c\u4e86\u4e00\u9879\u5305\u542b40\u540d\u53c2\u4e0e\u8005\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u4e0d\u540c\u7ea7\u522b\u7684AI\u62ab\u9732\u5bf9\u653f\u6cbb\u548c\u751f\u6d3b\u65b9\u5f0f\u65b0\u95fb\u4e2d\u4fe1\u4efb\u53ca\u51b3\u7b56\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "result": "\u672c\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u65b0\u95fb\u5185\u5bb9\u4e2d\uff0c\u5f53AI\u62ab\u9732\u8be6\u7ec6\u4fe1\u606f\u65f6\uff0c\u8bfb\u8005\u7684\u4fe1\u4efb\u5ea6\u4e0b\u964d\u3002\u4f46\u5728\u63d0\u4f9b\u4e00\u884c\u63cf\u8ff0\u65f6\uff0c\u8bfb\u8005\u5bf9\u6d88\u606f\u6e90\u7684\u6838\u5b9e\u884c\u4e3a\u589e\u52a0\uff0c\u540c\u65f6\uff0c\u5176\u4fe1\u4efb\u5ea6\u672a\u660e\u663e\u4e0b\u964d\u3002 ", "conclusion": "\u4e0d\u540c\u8be6\u7ec6\u7a0b\u5ea6\u7684AI\u62ab\u9732\u5bf9\u65b0\u95fb\u8bfb\u8005\u7684\u4fe1\u4efb\u548c\u51b3\u7b56\u884c\u4e3a\u6709\u663e\u8457\u5f71\u54cd\uff0c\u8be6\u7ec6\u62ab\u9732\u53ef\u80fd\u5bfc\u81f4\u4fe1\u4efb\u4e0b\u964d\uff0c\u800c\u7b80\u77ed\u62ab\u9732\u523a\u6fc0\u4e86\u6838\u5b9e\u884c\u4e3a\u3002"}}
{"id": "2601.09632", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.09632", "abs": "https://arxiv.org/abs/2601.09632", "authors": ["Rose Connolly", "Victor Zordan", "Rachel McDonnell"], "title": "Perceptually-Guided Adjusted Teleporting: Perceptual Thresholds for Teleport Displacements in Virtual Environments", "comment": "9 pages. to be published in IEEE VR conference proceedings 2026", "summary": "Teleportation is one of the most common locomotion techniques in virtual reality, yet its perceptual properties remain underexplored. While redirected walking research has shown that users' movements can be subtly manipulated without detection, similar imperceptible adjustments for teleportation have not been systematically investigated. This study examines the thresholds at which teleportation displacements become noticeable to users. We conducted a repeated-measures experiment in which participants' selected teleport destinations were altered in both direction (forwards, backwards) and at different ranges (small, large). Detection thresholds for these positional adjustments were estimated using a psychophysical staircase method with a two-alternative forced choice (2AFC) task. Results show that teleport destinations can be shifted without detection, with larger tolerances for backward adjustments and across longer teleport ranges. These findings establish baseline perceptual limits for redirected teleportation and highlight its potential as a design technique. Applications include supporting interpersonal distance management in social VR, guiding players toward objectives in games, and assisting novice users with navigation. By identifying the limits of imperceptible teleportation adjustments, this work extends redirection principles beyond walking to teleportation and opens new opportunities for adaptive and socially aware VR locomotion systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u865a\u62df\u73b0\u5b9e\u4e2d\u4f20\u9001\u6280\u672f\u7684\u611f\u77e5\u7279\u6027\uff0c\u53d1\u73b0\u7528\u6237\u5bf9\u4f20\u9001\u76ee\u6807\u4f4d\u7f6e\u7684\u8c03\u6574\u5177\u6709\u4e00\u5b9a\u7684\u672a\u5bdf\u89c9\u9608\u503c\u3002", "motivation": "\u7814\u7a76\u865a\u62df\u73b0\u5b9e\u4e2d\u4f20\u9001\u65b9\u5f0f\u7684\u611f\u77e5\u5c5e\u6027\u4ee5\u63d0\u9ad8\u7528\u6237\u4f53\u9a8c\u548c\u8bbe\u8ba1\u7075\u6d3b\u6027\u3002", "method": "\u901a\u8fc7\u91cd\u590d\u6d4b\u91cf\u5b9e\u9a8c\uff0c\u91c7\u7528\u5fc3\u7406\u7269\u7406\u9636\u68af\u6cd5\u548c\u4e8c\u9009\u4e00\u5f3a\u5236\u9009\u62e9\u4efb\u52a1\u8bc4\u4f30\u4f20\u9001\u76ee\u6807\u4f4d\u7f6e\u7684\u8c03\u6574\u9608\u503c\u3002", "result": "\u53d1\u73b0\u7528\u6237\u5bf9\u4f20\u9001\u76ee\u6807\u7684\u8c03\u6574\u53ef\u5728\u4e0d\u88ab\u5bdf\u89c9\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\uff0c\u4e14\u5bf9\u5411\u540e\u8c03\u6574\u548c\u957f\u8ddd\u79bb\u4f20\u9001\u7684\u5bb9\u5fcd\u5ea6\u66f4\u9ad8\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u91cd\u65b0\u5b9a\u5411\u4f20\u9001\u8bbe\u5b9a\u4e86\u611f\u77e5\u6781\u9650\uff0c\u4e3a\u865a\u62df\u73b0\u5b9e\u4e2d\u7684\u9002\u5e94\u6027\u548c\u793e\u4ea4\u610f\u8bc6\u79fb\u52a8\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002"}}
