{"id": "2510.23840", "categories": ["cs.HC", "H.5.1; I.3.7; H.5.2"], "pdf": "https://arxiv.org/pdf/2510.23840", "abs": "https://arxiv.org/abs/2510.23840", "authors": ["You-Jin Kim", "Andrew D. Wilson", "Jennifer Jacobs", "Tobias Höllerer"], "title": "Reality Distortion Room: A Study of User Locomotion Responses to Spatial Augmented Reality Effects", "comment": "Conference Paper, 10 pages. Published at the 2023 IEEE International\n  Symposium on Mixed and Augmented Reality (ISMAR)", "summary": "Reality Distortion Room (RDR) is a proof-of-concept augmented reality system\nusing projection mapping and unencumbered interaction with the Microsoft\nRoomAlive system to study a user's locomotive response to visual effects that\nseemingly transform the physical room the user is in. This study presents five\neffects that augment the appearance of a physical room to subtly encourage user\nmotion. Our experiment demonstrates users' reactions to the different\ndistortion and augmentation effects in a standard living room, with the\ndistortion effects projected as wall grids, furniture holograms, and small\nparticles in the air. The augmented living room can give the impression of\nbecoming elongated, wrapped, shifted, elevated, and enlarged. The study results\nsupport the implementation of AR experiences in limited physical spaces by\nproviding an initial understanding of how users can be subtly encouraged to\nmove throughout a room."}
{"id": "2510.23848", "categories": ["cs.HC", "H.5.1; I.3.7; H.5.2"], "pdf": "https://arxiv.org/pdf/2510.23848", "abs": "https://arxiv.org/abs/2510.23848", "authors": ["You-Jin Kim", "Myungin Lee", "Marko Peljhan", "JoAnn Kuchera-Morin", "Tobias Höllerer"], "title": "Spatial Orchestra: Locomotion Music Instruments through Spatial Exploration", "comment": "Extended Abstracts (Interactivity), 5 pages. Published at the 2024\n  CHI Conference on Human Factors in Computing Systems", "summary": "Spatial Orchestra demonstrates how easy it is to play musical instruments\nusing basic input like natural locomotion, which is accessible to most. Unlike\nmany musical instruments, our work allows individuals of all skill levels to\neffortlessly create music by walking into virtual bubbles. Our Augmented\nReality experience involves interacting with ever-shifting sound bubbles that\nthe user engages with by stepping into color-coded bubbles within the assigned\narea using a standalone AR headset. Each bubble corresponds to a cello note,\nand omits sound from the center of the bubble, and lets the user hear and\nexpress in spatial audio, effectively transforming participants into musicians.\nThis interactive element enables users to explore the intersection of spatial\nawareness, musical rhythm that extends to bodily expression through playful\nmovements and dance-like gestures within the bubble-filled environment. This\nunique experience illuminates the intricate relationship between spatial\nawareness and the art of musical performance."}
{"id": "2510.23875", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.23875", "abs": "https://arxiv.org/abs/2510.23875", "authors": ["Eswari Jayakumar", "Niladri Sekhar Dash", "Debasmita Mukherjee"], "title": "Large Language Model Agent Personality and Response Appropriateness: Evaluation by Human Linguistic Experts, LLM-as-Judge, and Natural Language Processing Model", "comment": null, "summary": "While Large Language Model (LLM)-based agents can be used to create highly\nengaging interactive applications through prompting personality traits and\ncontextual data, effectively assessing their personalities has proven\nchallenging. This novel interdisciplinary approach addresses this gap by\ncombining agent development and linguistic analysis to assess the prompted\npersonality of LLM-based agents in a poetry explanation task. We developed a\nnovel, flexible question bank, informed by linguistic assessment criteria and\nhuman cognitive learning levels, offering a more comprehensive evaluation than\ncurrent methods. By evaluating agent responses with natural language processing\nmodels, other LLMs, and human experts, our findings illustrate the limitations\nof purely deep learning solutions and emphasize the critical role of\ninterdisciplinary design in agent development."}
{"id": "2510.23887", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.23887", "abs": "https://arxiv.org/abs/2510.23887", "authors": ["Sumin Hong", "Xavier Briggs", "Qingxiao Zheng", "Yao Du", "Jinjun Xiong", "Toby Jia-jun Li"], "title": "MORA: AI-Mediated Story-Based practice for Speech Sound Disorder from Clinic to Home", "comment": null, "summary": "Speech sound disorder is among the most common communication challenges in\npreschool children. Home-based practice is essential for effective therapy and\nfor acquiring generalization of target sounds, yet sustaining engaging and\nconsistent practice remains difficult. Existing story-based activities, despite\ntheir potential for sound generalization and educational benefits, are often\nunderutilized due to limited interactivity. Moreover, many practice tools fail\nto sufficiently integrate speech-language pathologists into the process,\nresulting in weak alignment with clinical treatment plans. To address these\nlimitations, we present MORA, an interactive story-based practice system. MORA\nintroduces three key innovations. First, it embeds target sounds and vocabulary\ninto dynamic, character-driven conversational narratives, requiring children to\nactively produce speech to progress the story, thereby creating natural\nopportunities for exposure, repetition, and generalization. Second, it provides\nvisual cues, explicit instruction, and feedback, allowing children to practice\neffectively either independently or with caregivers. Third, it supports an\nAI-in-the-loop workflow, enabling SLPs to configure target materials, review\nlogged speech with phoneme-level scoring, and adapt therapy plans\nasynchronously -- bridging the gap between clinic and home practice while\nrespecting professional expertise. A formative study with six licensed SLPs\ninformed the system's design rationale, and an expert review with seven SLPs\ndemonstrated strong alignment with established articulation-based treatments,\nas well as potential to enhance children's engagement and literacy.\nFurthermore, discussions highlight the design considerations for professional\nsupport and configurability, adaptive and multimodal child interaction, while\nhighlighting MORA's broader applicability across speech and language disorders."}
{"id": "2510.23763", "categories": ["cs.RO", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23763", "abs": "https://arxiv.org/abs/2510.23763", "authors": ["Siyin Wang", "Jinlan Fu", "Feihong Liu", "Xinzhe He", "Huangxuan Wu", "Junhao Shi", "Kexin Huang", "Zhaoye Fei", "Jingjing Gong", "Zuxuan Wu", "Yugang Jiang", "See-Kiong Ng", "Tat-Seng Chua", "Xipeng Qiu"], "title": "RoboOmni: Proactive Robot Manipulation in Omni-modal Context", "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid\nprogress in Vision-Language-Action (VLA) models for robotic manipulation.\nAlthough effective in many scenarios, current approaches largely rely on\nexplicit instructions, whereas in real-world interactions, humans rarely issue\ninstructions directly. Effective collaboration requires robots to infer user\nintentions proactively. In this work, we introduce cross-modal contextual\ninstructions, a new setting where intent is derived from spoken dialogue,\nenvironmental sounds, and visual cues rather than explicit commands. To address\nthis new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor\nframework based on end-to-end omni-modal LLMs that unifies intention\nrecognition, interaction confirmation, and action execution. RoboOmni fuses\nauditory and visual signals spatiotemporally for robust intention recognition,\nwhile supporting direct speech interaction. To address the absence of training\ndata for proactive intention recognition in robotic manipulation, we build\nOmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640\nbackgrounds, and six contextual instruction types. Experiments in simulation\nand real-world settings show that RoboOmni surpasses text- and ASR-based\nbaselines in success rate, inference speed, intention recognition, and\nproactive assistance."}
{"id": "2510.23904", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.23904", "abs": "https://arxiv.org/abs/2510.23904", "authors": ["Kexin Quan", "Dina Albassam", "Mengke Wu", "Zijian Ding", "Jessie Chin"], "title": "Towards AI as Colleagues: Multi-Agent System Improves Structured Professional Ideation", "comment": null, "summary": "Most AI systems today are designed to manage tasks and execute predefined\nsteps. This makes them effective for process coordination but limited in their\nability to engage in joint problem-solving with humans or contribute new ideas.\nWe introduce MultiColleagues, a multi-agent conversational system that shows\nhow AI agents can act as colleagues by conversing with each other, sharing new\nideas, and actively involving users in collaborative ideation. In a\nwithin-subjects study with 20 participants, we compared MultiColleagues to a\nsingle-agent baseline. Results show that MultiColleagues fostered stronger\nperceptions of social presence, produced ideas rated significantly higher in\nquality and novelty, and encouraged deeper elaboration. These findings\ndemonstrate the potential of AI agents to move beyond process partners toward\ncolleagues that share intent, strengthen group dynamics, and collaborate with\nhumans to advance ideas."}
{"id": "2510.23860", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23860", "abs": "https://arxiv.org/abs/2510.23860", "authors": ["Hyung Chan Cho", "Go-Eum Cha", "Yanfu Liu", "Sooyeon Jeong"], "title": "Motivating Students' Self-study with Goal Reminder and Emotional Support", "comment": "RO-MAN 2025 accepted paper", "summary": "While the efficacy of social robots in supporting people in learning tasks\nhas been extensively investigated, their potential impact in assisting students\nin self-studying contexts has not been investigated much. This study explores\nhow a social robot can act as a peer study companion for college students\nduring self-study tasks by delivering task-oriented goal reminder and positive\nemotional support. We conducted an exploratory Wizard-of-Oz study to explore\nhow these robotic support behaviors impacted students' perceived focus,\nproductivity, and engagement in comparison to a robot that only provided\nphysical presence (control). Our study results suggest that participants in the\ngoal reminder and the emotional support conditions reported greater ease of\nuse, with the goal reminder condition additionally showing a higher willingness\nto use the robot in future study sessions. Participants' satisfaction with the\nrobot was correlated with their perception of the robot as a social other, and\nthis perception was found to be a predictor for their level of goal achievement\nin the self-study task. These findings highlight the potential of socially\nassistive robots to support self-study through both functional and emotional\nengagement."}
{"id": "2510.23947", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.23947", "abs": "https://arxiv.org/abs/2510.23947", "authors": ["Zihan Liu", "Parisa Rabbani", "Veda Duddu", "Kyle Fan", "Madison Lee", "Yun Huang"], "title": "Toward Socially-Aware LLMs: A Survey of Multimodal Approaches to Human Behavior Understanding", "comment": null, "summary": "LLM-powered multimodal systems are increasingly used to interpret human\nsocial behavior, yet how researchers apply the models' 'social competence'\nremains poorly understood. This paper presents a systematic literature review\nof 176 publications across different application domains (e.g., healthcare,\neducation, and entertainment). Using a four-dimensional coding framework\n(application, technical, evaluative, and ethical), we find (1) frequent use of\npattern recognition and information extraction from multimodal sources, but\nlimited support for adaptive, interactive reasoning; (2) a dominant\n'modality-to-text' pipeline that privileges language over rich audiovisual\ncues, striping away nuanced social cues; (3) evaluation practices reliant on\nstatic benchmarks, with socially grounded, human-centered assessments rare; and\n(4) Ethical discussions focused mainly on legal and rights-related risks (e.g.,\nprivacy), leaving societal risks (e.g., deception) overlooked--or at best\nacknowledged but left unaddressed. We outline a research agenda for evaluating\nsocially competent, ethically informed, and interaction-aware multi-modal\nsystems."}
{"id": "2510.23902", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23902", "abs": "https://arxiv.org/abs/2510.23902", "authors": ["Jans Solano", "Diego Quiroz"], "title": "Stand, Walk, Navigate: Recovery-Aware Visual Navigation on a Low-Cost Wheeled Quadruped", "comment": "Accepted at the IROS 2025 Workshop on Wheeled-Legged Robots", "summary": "Wheeled-legged robots combine the efficiency of wheels with the obstacle\nnegotiation of legs, yet many state-of-the-art systems rely on costly actuators\nand sensors, and fall-recovery is seldom integrated, especially for\nwheeled-legged morphologies. This work presents a recovery-aware\nvisual-inertial navigation system on a low-cost wheeled quadruped. The proposed\nsystem leverages vision-based perception from a depth camera and deep\nreinforcement learning policies for robust locomotion and autonomous recovery\nfrom falls across diverse terrains. Simulation experiments show agile mobility\nwith low-torque actuators over irregular terrain and reliably recover from\nexternal perturbations and self-induced failures. We further show goal directed\nnavigation in structured indoor spaces with low-cost perception. Overall, this\napproach lowers the barrier to deploying autonomous navigation and robust\nlocomotion policies in budget-constrained robotic platforms."}
{"id": "2510.24004", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.24004", "abs": "https://arxiv.org/abs/2510.24004", "authors": ["Shane Dirksen", "Radha Kumaran", "You-Jin Kim", "Yilin Wang", "Tobias Höllerer"], "title": "Modeling Object Attention in Mobile AR for Intrinsic Cognitive Security", "comment": "Conference Paper, 5 pages. Published at the 2025 ACM the\n  International Symposium on Theory, Algorithmic Foundations, and Protocol\n  Design for Mobile Networks and Mobile Computing (MobiHoc)", "summary": "We study attention in mobile Augmented Reality (AR) using object recall as a\nproxy outcome. We observe that the ability to recall an object (physical or\nvirtual) that was encountered in a mobile AR experience depends on many\npossible impact factors and attributes, with some objects being readily\nrecalled while others are not, and some people recalling objects overall much\nbetter or worse than others. This opens up a potential cognitive attack in\nwhich adversaries might create conditions that make an AR user not recall\ncertain potentially mission-critical objects. We explore whether a calibrated\npredictor of object recall can help shield against such cognitive attacks. We\npool data from four mobile AR studies (with a total of 1,152 object recall\nprobes) and fit a Partial Least Squares Structural Equation Model (PLS-SEM)\nwith formative Object, Scene, and User State composites predicting recall, also\nbenchmarking against Random Forest and multilayer perceptron classifiers.\nPLS-SEM attains the best F1 score in three of four studies. Additionally, path\nestimates identify lighting, augmentation density, AR registration stability,\ncognitive load, and AR familiarity as primary drivers. The model outputs\nper-object recall probabilities that can drive interface adjustments when\npredicted recall falls. Overall, PLS-SEM provides competitive accuracy with\ninterpretable levers for design and evaluation in mobile AR."}
{"id": "2510.23928", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23928", "abs": "https://arxiv.org/abs/2510.23928", "authors": ["Raman Jha", "Yang Zhou", "Giuseppe Loianno"], "title": "Adaptive Keyframe Selection for Scalable 3D Scene Reconstruction in Dynamic Environments", "comment": "Under Review for ROBOVIS 2026", "summary": "In this paper, we propose an adaptive keyframe selection method for improved\n3D scene reconstruction in dynamic environments. The proposed method integrates\ntwo complementary modules: an error-based selection module utilizing\nphotometric and structural similarity (SSIM) errors, and a momentum-based\nupdate module that dynamically adjusts keyframe selection thresholds according\nto scene motion dynamics. By dynamically curating the most informative frames,\nour approach addresses a key data bottleneck in real-time perception. This\nallows for the creation of high-quality 3D world representations from a\ncompressed data stream, a critical step towards scalable robot learning and\ndeployment in complex, dynamic environments. Experimental results demonstrate\nsignificant improvements over traditional static keyframe selection strategies,\nsuch as fixed temporal intervals or uniform frame skipping. These findings\nhighlight a meaningful advancement toward adaptive perception systems that can\ndynamically respond to complex and evolving visual scenes. We evaluate our\nproposed adaptive keyframe selection module on two recent state-of-the-art 3D\nreconstruction networks, Spann3r and CUT3R, and observe consistent improvements\nin reconstruction quality across both frameworks. Furthermore, an extensive\nablation study confirms the effectiveness of each individual component in our\nmethod, underlining their contribution to the overall performance gains."}
{"id": "2510.24011", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.24011", "abs": "https://arxiv.org/abs/2510.24011", "authors": ["Hiroki Nakano", "Jo Takezawa", "Fabrice Matulic", "Chi-Lan Yang", "Koji Yatani"], "title": "Understanding Reader Perception Shifts upon Disclosure of AI Authorship", "comment": null, "summary": "As AI writing support becomes ubiquitous, how disclosing its use affects\nreader perception remains a critical, underexplored question. We conducted a\nstudy with 261 participants to examine how revealing varying levels of AI\ninvolvement shifts author impressions across six distinct communicative acts.\nOur analysis of 990 responses shows that disclosure generally erodes\nperceptions of trustworthiness, caring, competence, and likability, with the\nsharpest declines in social and interpersonal writing. A thematic analysis of\nparticipants' feedback links these negative shifts to a perceived loss of human\nsincerity, diminished author effort, and the contextual inappropriateness of\nAI. Conversely, we find that higher AI literacy mitigates these negative\nperceptions, leading to greater tolerance or even appreciation for AI use. Our\nresults highlight the nuanced social dynamics of AI-mediated authorship and\ninform design implications for creating transparent, context-sensitive writing\nsystems that better preserve trust and authenticity."}
{"id": "2510.23954", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23954", "abs": "https://arxiv.org/abs/2510.23954", "authors": ["Pejman Kheradmand", "Behnam Moradkhani", "Raghavasimhan Sankaranarayanan", "Kent K. Yamamoto", "Tanner J. Zachem", "Patrick J. Codd", "Yash Chitalia", "Pierre E. Dupont"], "title": "A Comprehensive General Model of Tendon-Actuated Concentric Tube Robots with Multiple Tubes and Tendons", "comment": null, "summary": "Tendon-actuated concentric tube mechanisms combine the advantages of\ntendon-driven continuum robots and concentric tube robots while addressing\ntheir respective limitations. They overcome the restricted degrees of freedom\noften seen in tendon-driven designs, and mitigate issues such as snapping\ninstability associated with concentric tube robots. However, a complete and\ngeneral mechanical model for these systems remains an open problem. In this\nwork, we propose a Cosserat rod-based framework for modeling the general case\nof $n$ concentric tubes, each actuated by $m_i$ tendons, where $i = \\{1,\n\\ldots, n\\}$. The model allows each tube to twist and elongate while enforcing\na shared centerline for bending. We validate the proposed framework through\nexperiments with two-tube and three tube assemblies under various tendon\nrouting configurations, achieving tip prediction errors $<4\\%$ of the robot's\ntotal length. We further demonstrate the model's generality by applying it to\nexisting robots in the field, where maximum tip deviations remain around $5\\%$\nof the total length. This model provides a foundation for accurate shape\nestimation and control of advanced tendon-actuated concentric tube robots."}
{"id": "2510.24057", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.24057", "abs": "https://arxiv.org/abs/2510.24057", "authors": ["Qirong Zhu", "Ansheng Wang", "Shinji Tanaka", "Yasutoshi Makino", "Hiroyuki Shinoda"], "title": "VR-Assisted Guide Dog Training: A 360° PanoHaptic System for Right-Hand Commands Analysis", "comment": "9 pages, 9 figures", "summary": "This paper presents a VR-based guide dog training system designed to assist\nnovice trainers in understanding guide dog behavior and issuing appropriate\ntraining commands. Guide dogs play a vital role in supporting independent\nmobility for visually impaired individuals, yet the limited number of skilled\ntrainers restricts their availability. Training is highly demanding, requiring\naccurate observation of the dog's status and precise command issuance,\nespecially through right-hand gestures. While the trainer's left hand holds the\nharness to perceive haptic cues, the right hand is used to indicate directions,\nmaintain attention, and provide comfort, with motion patterns varying by\nscenario and the dog's progress. Currently, novices learn mainly by observing\nexperts or watching videos, which lacks immersion and makes it difficult to\nadopt the trainer's perspective for understanding behavior or synchronizing\ncommand timing.\n  To address these limitations, the proposed system introduces a VR-based\nassistive platform integrating panoramic visuals and haptic feedback to create\nan immersive training environment. The visual module provides contextual\nguidance, including cues for command execution and real-time comparison of the\nuser's posture with standard actions, while the haptic module delivers tactile\nfeedback for command gestures. Users can re-experience training sessions across\ndiverse scenarios and dog proficiency levels, allowing independent and repeated\npractice. By improving the timing, accuracy, and expressiveness of right-hand\ncommands, the system aims to accelerate skill acquisition, enhance training\nquality, and mitigate the shortage of qualified trainers, ultimately increasing\nthe availability of guide dogs for visually impaired individuals."}
{"id": "2510.23963", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23963", "abs": "https://arxiv.org/abs/2510.23963", "authors": ["Hiroki Ishikawa", "Kyosuke Ishibashi", "Ko Yamamoto"], "title": "Adaptive-twist Soft Finger Mechanism for Grasping by Wrapping", "comment": null, "summary": "This paper presents a soft robot finger capable of adaptive-twist deformation\nto grasp objects by wrapping them. For a soft hand to grasp and pick-up one\nobject from densely contained multiple objects, a soft finger requires the\nadaptive-twist deformation function in both in-plane and out-of-plane\ndirections. The function allows the finger to be inserted deeply into a limited\ngap among objects. Once inserted, the soft finger requires appropriate control\nof grasping force normal to contact surface, thereby maintaining the twisted\ndeformation. In this paper, we refer to this type of grasping as grasping by\nwrapping. To achieve these two functions by a single actuation source, we\npropose a variable stiffness mechanism that can adaptively change the stiffness\nas the pressure is higher. We conduct a finite element analysis (FEA) on the\nproposed mechanism and determine its design parameter based on the FEA result.\nUsing the developed soft finger, we report basic experimental results and\ndemonstrations on grasping various objects."}
{"id": "2510.24070", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.24070", "abs": "https://arxiv.org/abs/2510.24070", "authors": ["Jingyi Xie", "Chuhao Wu", "Ge Wang", "Rui Yu", "He Zhang", "Ronald Metoyer", "Si Chen"], "title": "Building AI Literacy at Home: How Families Navigate Children's Self-Directed Learning with AI", "comment": null, "summary": "As generative AI becomes embedded in children's learning spaces, families\nface new challenges in guiding its use. Middle childhood (ages 7-13) is a\ncritical stage where children seek autonomy even as parental influence remains\nstrong. Using self-directed learning (SDL) as a lens, we examine how parents\nperceive and support children's developing AI literacy through focus groups\nwith 13 parent-child pairs. Parents described evolving phases of engagement\ndriven by screen time, self-motivation, and growing knowledge. While many\nframed AI primarily as a study tool, few considered its non-educational roles\nor risks, such as privacy and infrastructural embedding. Parents also noted\ngaps in their own AI understanding, often turning to joint exploration and\nengagement as a form of co-learning. Our findings reveal how families\nco-construct children's AI literacy, exposing tensions between practical\nexpectations and critical literacies, and provide design implications that\nfoster SDL while balancing autonomy and oversight."}
{"id": "2510.23988", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23988", "abs": "https://arxiv.org/abs/2510.23988", "authors": ["Phuc Nguyen Xuan", "Thanh Nguyen Canh", "Huu-Hung Nguyen", "Nak Young Chong", "Xiem HoangVan"], "title": "A Survey on Collaborative SLAM with 3D Gaussian Splatting", "comment": null, "summary": "This survey comprehensively reviews the evolving field of multi-robot\ncollaborative Simultaneous Localization and Mapping (SLAM) using 3D Gaussian\nSplatting (3DGS). As an explicit scene representation, 3DGS has enabled\nunprecedented real-time, high-fidelity rendering, ideal for robotics. However,\nits use in multi-robot systems introduces significant challenges in maintaining\nglobal consistency, managing communication, and fusing data from heterogeneous\nsources. We systematically categorize approaches by their architecture --\ncentralized, distributed -- and analyze core components like multi-agent\nconsistency and alignment, communication-efficient, Gaussian representation,\nsemantic distillation, fusion and pose optimization, and real-time scalability.\nIn addition, a summary of critical datasets and evaluation metrics is provided\nto contextualize performance. Finally, we identify key open challenges and\nchart future research directions, including lifelong mapping, semantic\nassociation and mapping, multi-model for robustness, and bridging the Sim2Real\ngap."}
{"id": "2510.24227", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.24227", "abs": "https://arxiv.org/abs/2510.24227", "authors": ["Senuri Wijenayake", "Joanne Gray", "Asangi Jayatilaka", "Louise La Sala", "Nalin Arachchilage", "Ryan M. Kelly", "Sanchari Das"], "title": "Advancing Interdisciplinary Approaches to Online Safety Research", "comment": null, "summary": "The growing prevalence of negative experiences in online spaces demands\nurgent attention from the human-computer interaction (HCI) community. However,\nresearch on online safety remains fragmented across different HCI subfields,\nwith limited communication and collaboration between disciplines. This siloed\napproach risks creating ineffective responses, including design solutions that\nfail to meet the diverse needs of users, and policy efforts that overlook\ncritical usability concerns. This workshop aims to foster interdisciplinary\ndialogue on online safety by bringing together researchers from within and\nbeyond HCI - including but not limited to Social Computing, Digital Design,\nInternet Policy, Cybersecurity, Ethics, and Social Sciences. By uniting\nresearchers, policymakers, industry practitioners, and community advocates we\naim to identify shared challenges in online safety research, highlight gaps in\ncurrent knowledge, and establish common research priorities. The workshop will\nsupport the development of interdisciplinary research plans and establish\ncollaborative environments - both within and beyond Australia - to action them."}
{"id": "2510.23997", "categories": ["cs.RO", "I.2.9"], "pdf": "https://arxiv.org/pdf/2510.23997", "abs": "https://arxiv.org/abs/2510.23997", "authors": ["Stanley Wu", "Mohamad H. Danesh", "Simon Li", "Hanna Yurchyk", "Amin Abyaneh", "Anas El Houssaini", "David Meger", "Hsiu-Chin Lin"], "title": "VOCALoco: Viability-Optimized Cost-aware Adaptive Locomotion", "comment": "Accepted in IEEE Robotics and Automation Letters (RAL), 2025. 8\n  pages, 9 figures", "summary": "Recent advancements in legged robot locomotion have facilitated traversal\nover increasingly complex terrains. Despite this progress, many existing\napproaches rely on end-to-end deep reinforcement learning (DRL), which poses\nlimitations in terms of safety and interpretability, especially when\ngeneralizing to novel terrains. To overcome these challenges, we introduce\nVOCALoco, a modular skill-selection framework that dynamically adapts\nlocomotion strategies based on perceptual input. Given a set of pre-trained\nlocomotion policies, VOCALoco evaluates their viability and energy-consumption\nby predicting both the safety of execution and the anticipated cost of\ntransport over a fixed planning horizon. This joint assessment enables the\nselection of policies that are both safe and energy-efficient, given the\nobserved local terrain. We evaluate our approach on staircase locomotion tasks,\ndemonstrating its performance in both simulated and real-world scenarios using\na quadrupedal robot. Empirical results show that VOCALoco achieves improved\nrobustness and safety during stair ascent and descent compared to a\nconventional end-to-end DRL policy"}
{"id": "2510.24594", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.24594", "abs": "https://arxiv.org/abs/2510.24594", "authors": ["Dapeng Zhang", "Marina Katoh", "Weiping Pei"], "title": "Detecting the Use of Generative AI in Crowdsourced Surveys: Implications for Data Integrity", "comment": "Accepted by CSCW 2025 workshop Beyond Information: Online\n  Participatory Culture and Information Disorder", "summary": "The widespread adoption of generative AI (GenAI) has introduced new\nchallenges in crowdsourced data collection, particularly in survey-based\nresearch. While GenAI offers powerful capabilities, its unintended use in\ncrowdsourcing, such as generating automated survey responses, threatens the\nintegrity of empirical research and complicates efforts to understand public\nopinion and behavior. In this study, we investigate and evaluate two approaches\nfor detecting AI-generated responses in online surveys: LLM-based detection and\nsignature-based detection. We conducted experiments across seven survey\nstudies, comparing responses collected before 2022 with those collected after\nthe release of ChatGPT. Our findings reveal a significant increase in\nAI-generated responses in the post-2022 studies, highlighting how GenAI may\nsilently distort crowdsourced data. This work raises broader concerns about\nevolving landscape of data integrity, where GenAI can compromise data quality,\nmislead researchers, and influence downstream findings in fields such as\nhealth, politics, and social behavior. By surfacing detection strategies and\nempirical evidence of GenAI's impact, we aim to contribute to ongoing\nconversation about safeguarding research integrity and supporting scholars\nnavigating these methodological and ethical challenges."}
{"id": "2510.24029", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY", "q-bio.NC", "I.2.9; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.24029", "abs": "https://arxiv.org/abs/2510.24029", "authors": ["Andrew Gerstenslager", "Bekarys Dukenbaev", "Ali A. Minai"], "title": "Improved Accuracy of Robot Localization Using 3-D LiDAR in a Hippocampus-Inspired Model", "comment": "8 pages, 9 figures, Presented at the 2025 International Joint\n  Conference on Neural Networks, Rome, July 2025", "summary": "Boundary Vector Cells (BVCs) are a class of neurons in the brains of\nvertebrates that encode environmental boundaries at specific distances and\nallocentric directions, playing a central role in forming place fields in the\nhippocampus. Most computational BVC models are restricted to two-dimensional\n(2D) environments, making them prone to spatial ambiguities in the presence of\nhorizontal symmetries in the environment. To address this limitation, we\nincorporate vertical angular sensitivity into the BVC framework, thereby\nenabling robust boundary detection in three dimensions, and leading to\nsignificantly more accurate spatial localization in a biologically-inspired\nrobot model.\n  The proposed model processes LiDAR data to capture vertical contours, thereby\ndisambiguating locations that would be indistinguishable under a purely 2D\nrepresentation. Experimental results show that in environments with minimal\nvertical variation, the proposed 3D model matches the performance of a 2D\nbaseline; yet, as 3D complexity increases, it yields substantially more\ndistinct place fields and markedly reduces spatial aliasing. These findings\nshow that adding a vertical dimension to BVC-based localization can\nsignificantly enhance navigation and mapping in real-world 3D spaces while\nretaining performance parity in simpler, near-planar scenarios."}
{"id": "2510.24638", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.24638", "abs": "https://arxiv.org/abs/2510.24638", "authors": ["Sabrina Haque", "Kyle Henry", "Troyee Saha", "Kimberly Vanhoose", "Jobaidul Boni", "Samantha Moss", "Kate Hyun", "Kathy Siepker", "Xiangli Gu", "Angela Liegey-Dougall", "Stephen Mattingly", "Christoph Csallner"], "title": "What Does It Take? Developing a Smartphone App that Motivates Older Adults to be Physically Active", "comment": "28 pages", "summary": "Maintaining physical activity is essential for older adults' health and\nwell-being, yet participation remains low. Traditional paper-based and\nin-person interventions have been effective but face scalability issues.\nSmartphone apps offer a potential solution, but their effectiveness in\nreal-world use remains underexplored. Most prior studies take place in\ncontrolled environments, use specialized hardware, or rely on in-person\ntraining sessions or researcher-led setup. This study examines the feasibility\nand engagement of Senior Fit, a standalone mobile fitness app designed for\nolder adults. We conducted continuous testing with 25 participants aged 65-85,\nrefining the app based on their feedback to improve usability and\naccessibility. Our findings underscore both the potential and key challenges in\ndesigning digital health interventions. Older adults valued features such as\nvideo demonstrations and reminders that made activity feel accessible and\nmotivating, yet some expressed frustration with manual logging and limited\npersonalization. The Facebook group provided encouragement for some but\nexcluded others unfamiliar with the platform. These results highlight the need\nfor fitness apps that integrate flexible tracking, clear feedback, and\nlow-barrier social support. We contribute design recommendations for creating\ninclusive mobile fitness tools that align with older adults' routines and\ncapabilities, offering insights for future long-term, real-world deployments."}
{"id": "2510.24052", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24052", "abs": "https://arxiv.org/abs/2510.24052", "authors": ["Jongsuk Kim", "Jaeyoung Lee", "Gyojin Han", "Dongjae Lee", "Minki Jeong", "Junmo Kim"], "title": "SynAD: Enhancing Real-World End-to-End Autonomous Driving Models through Synthetic Data Integration", "comment": null, "summary": "Recent advancements in deep learning and the availability of high-quality\nreal-world driving datasets have propelled end-to-end autonomous driving.\nDespite this progress, relying solely on real-world data limits the variety of\ndriving scenarios for training. Synthetic scenario generation has emerged as a\npromising solution to enrich the diversity of training data; however, its\napplication within E2E AD models remains largely unexplored. This is primarily\ndue to the absence of a designated ego vehicle and the associated sensor\ninputs, such as camera or LiDAR, typically provided in real-world scenarios. To\naddress this gap, we introduce SynAD, the first framework designed to enhance\nreal-world E2E AD models using synthetic data. Our method designates the agent\nwith the most comprehensive driving information as the ego vehicle in a\nmulti-agent synthetic scenario. We further project path-level scenarios onto\nmaps and employ a newly developed Map-to-BEV Network to derive bird's-eye-view\nfeatures without relying on sensor inputs. Finally, we devise a training\nstrategy that effectively integrates these map-based synthetic data with real\ndriving data. Experimental results demonstrate that SynAD effectively\nintegrates all components and notably enhances safety performance. By bridging\nsynthetic scenario generation and E2E AD, SynAD paves the way for more\ncomprehensive and robust autonomous driving models."}
{"id": "2510.24055", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24055", "abs": "https://arxiv.org/abs/2510.24055", "authors": ["Xiucheng Zhang", "Yang Jiang", "Hongwei Qing", "Jiashuo Bai"], "title": "Language-Conditioned Representations and Mixture-of-Experts Policy for Robust Multi-Task Robotic Manipulation", "comment": "8 pages", "summary": "Perceptual ambiguity and task conflict limit multitask robotic manipulation\nvia imitation learning. We propose a framework combining a Language-Conditioned\nVisual Representation (LCVR) module and a Language-conditioned\nMixture-ofExperts Density Policy (LMoE-DP). LCVR resolves perceptual\nambiguities by grounding visual features with language instructions, enabling\ndifferentiation between visually similar tasks. To mitigate task conflict,\nLMoE-DP uses a sparse expert architecture to specialize in distinct, multimodal\naction distributions, stabilized by gradient modulation. On real-robot\nbenchmarks, LCVR boosts Action Chunking with Transformers (ACT) and Diffusion\nPolicy (DP) success rates by 33.75% and 25%, respectively. The full framework\nachieves a 79% average success, outperforming the advanced baseline by 21%. Our\nwork shows that combining semantic grounding and expert specialization enables\nrobust, efficient multi-task manipulation"}
{"id": "2510.24067", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24067", "abs": "https://arxiv.org/abs/2510.24067", "authors": ["Tianyi Ding", "Ronghao Zheng", "Senlin Zhang", "Meiqin Liu"], "title": "Balanced Collaborative Exploration via Distributed Topological Graph Voronoi Partition", "comment": null, "summary": "This work addresses the collaborative multi-robot autonomous online\nexploration problem, particularly focusing on distributed exploration planning\nfor dynamically balanced exploration area partition and task allocation among a\nteam of mobile robots operating in obstacle-dense non-convex environments.\n  We present a novel topological map structure that simultaneously\ncharacterizes both spatial connectivity and global exploration completeness of\nthe environment. The topological map is updated incrementally to utilize known\nspatial information for updating reachable spaces, while exploration targets\nare planned in a receding horizon fashion under global coverage guidance.\n  A distributed weighted topological graph Voronoi algorithm is introduced\nimplementing balanced graph space partitions of the fused topological maps.\nTheoretical guarantees are provided for distributed consensus convergence and\nequitable graph space partitions with constant bounds.\n  A local planner optimizes the visitation sequence of exploration targets\nwithin the balanced partitioned graph space to minimize travel distance, while\ngenerating safe, smooth, and dynamically feasible motion trajectories.\n  Comprehensive benchmarking against state-of-the-art methods demonstrates\nsignificant improvements in exploration efficiency, completeness, and workload\nbalance across the robot team."}
{"id": "2510.24069", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24069", "abs": "https://arxiv.org/abs/2510.24069", "authors": ["Sangmin Kim", "Hajun Kim", "Gijeong Kim", "Min-Gyu Kim", "Hae-Won Park"], "title": "Dynamically-Consistent Trajectory Optimization for Legged Robots via Contact Point Decomposition", "comment": "8 pages, 4 figures, IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT\n  VERSION. ACCEPTED OCTOBER, 2025", "summary": "To generate reliable motion for legged robots through trajectory\noptimization, it is crucial to simultaneously compute the robot's path and\ncontact sequence, as well as accurately consider the dynamics in the problem\nformulation. In this paper, we present a phase-based trajectory optimization\nthat ensures the feasibility of translational dynamics and friction cone\nconstraints throughout the entire trajectory. Specifically, our approach\nleverages the superposition properties of linear differential equations to\ndecouple the translational dynamics for each contact point, which operates\nunder different phase sequences. Furthermore, we utilize the differentiation\nmatrix of B{\\'e}zier polynomials to derive an analytical relationship between\nthe robot's position and force, thereby ensuring the consistent satisfaction of\ntranslational dynamics. Additionally, by exploiting the convex closure property\nof B{\\'e}zier polynomials, our method ensures compliance with friction cone\nconstraints. Using the aforementioned approach, the proposed trajectory\noptimization framework can generate dynamically reliable motions with various\ngait sequences for legged robots. We validate our framework using a quadruped\nrobot model, focusing on the feasibility of dynamics and motion generation."}
{"id": "2510.24108", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24108", "abs": "https://arxiv.org/abs/2510.24108", "authors": ["Zhenxin Li", "Wenhao Yao", "Zi Wang", "Xinglong Sun", "Jingde Chen", "Nadine Chang", "Maying Shen", "Jingyu Song", "Zuxuan Wu", "Shiyi Lan", "Jose M. Alvarez"], "title": "ZTRS: Zero-Imitation End-to-end Autonomous Driving with Trajectory Scoring", "comment": null, "summary": "End-to-end autonomous driving maps raw sensor inputs directly into\nego-vehicle trajectories to avoid cascading errors from perception modules and\nto leverage rich semantic cues. Existing frameworks largely rely on Imitation\nLearning (IL), which can be limited by sub-optimal expert demonstrations and\ncovariate shift during deployment. On the other hand, Reinforcement Learning\n(RL) has recently shown potential in scaling up with simulations, but is\ntypically confined to low-dimensional symbolic inputs (e.g. 3D objects and\nmaps), falling short of full end-to-end learning from raw sensor data. We\nintroduce ZTRS (Zero-Imitation End-to-End Autonomous Driving with Trajectory\nScoring), a framework that combines the strengths of both worlds: sensor inputs\nwithout losing information and RL training for robust planning. To the best of\nour knowledge, ZTRS is the first framework that eliminates IL entirely by only\nlearning from rewards while operating directly on high-dimensional sensor data.\nZTRS utilizes offline reinforcement learning with our proposed Exhaustive\nPolicy Optimization (EPO), a variant of policy gradient tailored for enumerable\nactions and rewards. ZTRS demonstrates strong performance across three\nbenchmarks: Navtest (generic real-world open-loop planning), Navhard (open-loop\nplanning in challenging real-world and synthetic scenarios), and HUGSIM\n(simulated closed-loop driving). Specifically, ZTRS achieves the\nstate-of-the-art result on Navhard and outperforms IL-based baselines on\nHUGSIM. Code will be available at https://github.com/woxihuanjiangguo/ZTRS."}
{"id": "2510.24109", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24109", "abs": "https://arxiv.org/abs/2510.24109", "authors": ["Wenbin Ding", "Jun Chen", "Mingjia Chen", "Fei Xie", "Qi Mao", "Philip Dames"], "title": "PFEA: An LLM-based High-Level Natural Language Planning and Feedback Embodied Agent for Human-Centered AI", "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has marked a\nsignificant breakthrough in Artificial Intelligence (AI), ushering in a new era\nof Human-centered Artificial Intelligence (HAI). HAI aims to better serve human\nwelfare and needs, thereby placing higher demands on the intelligence level of\nrobots, particularly in aspects such as natural language interaction, complex\ntask planning, and execution. Intelligent agents powered by LLMs have opened up\nnew pathways for realizing HAI. However, existing LLM-based embodied agents\noften lack the ability to plan and execute complex natural language control\ntasks online. This paper explores the implementation of intelligent robotic\nmanipulating agents based on Vision-Language Models (VLMs) in the physical\nworld. We propose a novel embodied agent framework for robots, which comprises\na human-robot voice interaction module, a vision-language agent module and an\naction execution module. The vision-language agent itself includes a\nvision-based task planner, a natural language instruction converter, and a task\nperformance feedback evaluator. Experimental results demonstrate that our agent\nachieves a 28\\% higher average task success rate in both simulated and real\nenvironments compared to approaches relying solely on LLM+CLIP, significantly\nimproving the execution success rate of high-level natural language instruction\ntasks."}
{"id": "2510.24118", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24118", "abs": "https://arxiv.org/abs/2510.24118", "authors": ["Haotian Zhou", "Xiaole Wang", "He Li", "Fusheng Sun", "Shengyu Guo", "Guolei Qi", "Jianghuan Xu", "Huijing Zhao"], "title": "LagMemo: Language 3D Gaussian Splatting Memory for Multi-modal Open-vocabulary Multi-goal Visual Navigation", "comment": null, "summary": "Navigating to a designated goal using visual information is a fundamental\ncapability for intelligent robots. Most classical visual navigation methods are\nrestricted to single-goal, single-modality, and closed set goal settings. To\naddress the practical demands of multi-modal, open-vocabulary goal queries and\nmulti-goal visual navigation, we propose LagMemo, a navigation system that\nleverages a language 3D Gaussian Splatting memory. During exploration, LagMemo\nconstructs a unified 3D language memory. With incoming task goals, the system\nqueries the memory, predicts candidate goal locations, and integrates a local\nperception-based verification mechanism to dynamically match and validate goals\nduring navigation. For fair and rigorous evaluation, we curate GOAT-Core, a\nhigh-quality core split distilled from GOAT-Bench tailored to multi-modal\nopen-vocabulary multi-goal visual navigation. Experimental results show that\nLagMemo's memory module enables effective multi-modal open-vocabulary goal\nlocalization, and that LagMemo outperforms state-of-the-art methods in\nmulti-goal visual navigation. Project page:\nhttps://weekgoodday.github.io/lagmemo"}
{"id": "2510.24194", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24194", "abs": "https://arxiv.org/abs/2510.24194", "authors": ["Ev Zisselman", "Mirco Mutti", "Shelly Francis-Meretzki", "Elisei Shafer", "Aviv Tamar"], "title": "Blindfolded Experts Generalize Better: Insights from Robotic Manipulation and Videogames", "comment": null, "summary": "Behavioral cloning is a simple yet effective technique for learning\nsequential decision-making from demonstrations. Recently, it has gained\nprominence as the core of foundation models for the physical world, where\nachieving generalization requires countless demonstrations of a multitude of\ntasks. Typically, a human expert with full information on the task demonstrates\na (nearly) optimal behavior. In this paper, we propose to hide some of the\ntask's information from the demonstrator. This ``blindfolded'' expert is\ncompelled to employ non-trivial exploration to solve the task. We show that\ncloning the blindfolded expert generalizes better to unseen tasks than its\nfully-informed counterpart. We conduct experiments of real-world robot peg\ninsertion tasks with (limited) human demonstrations, alongside videogames from\nthe Procgen benchmark. Additionally, we support our findings with theoretical\nanalysis, which confirms that the generalization error scales with\n$\\sqrt{I/m}$, where $I$ measures the amount of task information available to\nthe demonstrator, and $m$ is the number of demonstrated tasks. Both theory and\npractice indicate that cloning blindfolded experts generalizes better with\nfewer demonstrated tasks. Project page with videos and code:\nhttps://sites.google.com/view/blindfoldedexperts/home"}
{"id": "2510.24257", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24257", "abs": "https://arxiv.org/abs/2510.24257", "authors": ["Ziqi Ma", "Changda Tian", "Yue Gao"], "title": "Manipulate as Human: Learning Task-oriented Manipulation Skills by Adversarial Motion Priors", "comment": null, "summary": "In recent years, there has been growing interest in developing robots and\nautonomous systems that can interact with human in a more natural and intuitive\nway. One of the key challenges in achieving this goal is to enable these\nsystems to manipulate objects and tools in a manner that is similar to that of\nhumans. In this paper, we propose a novel approach for learning human-style\nmanipulation skills by using adversarial motion priors, which we name HMAMP.\nThe approach leverages adversarial networks to model the complex dynamics of\ntool and object manipulation, as well as the aim of the manipulation task. The\ndiscriminator is trained using a combination of real-world data and simulation\ndata executed by the agent, which is designed to train a policy that generates\nrealistic motion trajectories that match the statistical properties of human\nmotion. We evaluated HMAMP on one challenging manipulation task: hammering, and\nthe results indicate that HMAMP is capable of learning human-style manipulation\nskills that outperform current baseline methods. Additionally, we demonstrate\nthat HMAMP has potential for real-world applications by performing real robot\narm hammering tasks. In general, HMAMP represents a significant step towards\ndeveloping robots and autonomous systems that can interact with humans in a\nmore natural and intuitive way, by learning to manipulate tools and objects in\na manner similar to how humans do."}
{"id": "2510.24261", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24261", "abs": "https://arxiv.org/abs/2510.24261", "authors": ["Jingyi Tian", "Le Wang", "Sanping Zhou", "Sen Wang", "Jiayi Li", "Gang Hua"], "title": "DynaRend: Learning 3D Dynamics via Masked Future Rendering for Robotic Manipulation", "comment": "Accepted to NeurIPS 2025", "summary": "Learning generalizable robotic manipulation policies remains a key challenge\ndue to the scarcity of diverse real-world training data. While recent\napproaches have attempted to mitigate this through self-supervised\nrepresentation learning, most either rely on 2D vision pretraining paradigms\nsuch as masked image modeling, which primarily focus on static semantics or\nscene geometry, or utilize large-scale video prediction models that emphasize\n2D dynamics, thus failing to jointly learn the geometry, semantics, and\ndynamics required for effective manipulation. In this paper, we present\nDynaRend, a representation learning framework that learns 3D-aware and\ndynamics-informed triplane features via masked reconstruction and future\nprediction using differentiable volumetric rendering. By pretraining on\nmulti-view RGB-D video data, DynaRend jointly captures spatial geometry, future\ndynamics, and task semantics in a unified triplane representation. The learned\nrepresentations can be effectively transferred to downstream robotic\nmanipulation tasks via action value map prediction. We evaluate DynaRend on two\nchallenging benchmarks, RLBench and Colosseum, as well as in real-world robotic\nexperiments, demonstrating substantial improvements in policy success rate,\ngeneralization to environmental perturbations, and real-world applicability\nacross diverse manipulation tasks."}
{"id": "2510.24315", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24315", "abs": "https://arxiv.org/abs/2510.24315", "authors": ["Baozhe Zhang", "Xinwei Chen", "Qingcheng Chen", "Chao Xu", "Fei Gao", "Yanjun Cao"], "title": "Global-State-Free Obstacle Avoidance for Quadrotor Control in Air-Ground Cooperation", "comment": null, "summary": "CoNi-MPC provides an efficient framework for UAV control in air-ground\ncooperative tasks by relying exclusively on relative states, eliminating the\nneed for global state estimation. However, its lack of environmental\ninformation poses significant challenges for obstacle avoidance. To address\nthis issue, we propose a novel obstacle avoidance algorithm, Cooperative\nNon-inertial frame-based Obstacle Avoidance (CoNi-OA), designed explicitly for\nUAV-UGV cooperative scenarios without reliance on global state estimation or\nobstacle prediction. CoNi-OA uniquely utilizes a single frame of raw LiDAR data\nfrom the UAV to generate a modulation matrix, which directly adjusts the\nquadrotor's velocity to achieve obstacle avoidance. This modulation-based\nmethod enables real-time generation of collision-free trajectories within the\nUGV's non-inertial frame, significantly reducing computational demands (less\nthan 5 ms per iteration) while maintaining safety in dynamic and unpredictable\nenvironments. The key contributions of this work include: (1) a\nmodulation-based obstacle avoidance algorithm specifically tailored for UAV-UGV\ncooperation in non-inertial frames without global states; (2) rapid, real-time\ntrajectory generation based solely on single-frame LiDAR data, removing the\nneed for obstacle modeling or prediction; and (3) adaptability to both static\nand dynamic environments, thus extending applicability to featureless or\nunknown scenarios."}
{"id": "2510.24335", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24335", "abs": "https://arxiv.org/abs/2510.24335", "authors": ["Mingyu Jeong", "Eunsung Kim", "Sehun Park", "Andrew Jaeyong Choi"], "title": "NVSim: Novel View Synthesis Simulator for Large Scale Indoor Navigation", "comment": "9 pages, 10 figures", "summary": "We present NVSim, a framework that automatically constructs large-scale,\nnavigable indoor simulators from only common image sequences, overcoming the\ncost and scalability limitations of traditional 3D scanning. Our approach\nadapts 3D Gaussian Splatting to address visual artifacts on sparsely observed\nfloors a common issue in robotic traversal data. We introduce Floor-Aware\nGaussian Splatting to ensure a clean, navigable ground plane, and a novel\nmesh-free traversability checking algorithm that constructs a topological graph\nby directly analyzing rendered views. We demonstrate our system's ability to\ngenerate valid, large-scale navigation graphs from real-world data. A video\ndemonstration is avilable at https://youtu.be/tTiIQt6nXC8"}
{"id": "2510.24457", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.24457", "abs": "https://arxiv.org/abs/2510.24457", "authors": ["Jorge Vicente-Martinez", "Edgar Ramirez-Laboreo"], "title": "Flatness-based trajectory planning for 3D overhead cranes with friction compensation and collision avoidance", "comment": "8 pages, 11 figures", "summary": "This paper presents an optimal trajectory generation method for 3D overhead\ncranes by leveraging differential flatness. This framework enables the direct\ninclusion of complex physical and dynamic constraints, such as nonlinear\nfriction and collision avoidance for both payload and rope. Our approach allows\nfor aggressive movements by constraining payload swing only at the final point.\nA comparative simulation study validates our approach, demonstrating that\nneglecting dry friction leads to actuator saturation and collisions. The\nresults show that friction modeling is a fundamental requirement for fast and\nsafe crane trajectories."}
{"id": "2510.24508", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24508", "abs": "https://arxiv.org/abs/2510.24508", "authors": ["Haoying Li", "Yifan Peng", "Junfeng Wu"], "title": "Supervisory Measurement-Guided Noise Covariance Estimation", "comment": null, "summary": "Reliable state estimation hinges on accurate specification of sensor noise\ncovariances, which weigh heterogeneous measurements. In practice, these\ncovariances are difficult to identify due to environmental variability,\nfront-end preprocessing, and other reasons. We address this by formulating\nnoise covariance estimation as a bilevel optimization that, from a Bayesian\nperspective, factorizes the joint likelihood of so-called odometry and\nsupervisory measurements, thereby balancing information utilization with\ncomputational efficiency. The factorization converts the nested Bayesian\ndependency into a chain structure, enabling efficient parallel computation: at\nthe lower level, an invariant extended Kalman filter with state augmentation\nestimates trajectories, while a derivative filter computes analytical gradients\nin parallel for upper-level gradient updates. The upper level refines the\ncovariance to guide the lower-level estimation. Experiments on synthetic and\nreal-world datasets show that our method achieves higher efficiency over\nexisting baselines."}
{"id": "2510.24515", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24515", "abs": "https://arxiv.org/abs/2510.24515", "authors": ["Malintha Fernando", "Petter Ögren", "Silun Zhang"], "title": "Stochastic Prize-Collecting Games: Strategic Planning in Multi-Robot Systems", "comment": "Submitted to IEEE Robotics and Automation Letters", "summary": "The Team Orienteering Problem (TOP) generalizes many real-world multi-robot\nscheduling and routing tasks that occur in autonomous mobility, aerial\nlogistics, and surveillance applications. While many flavors of the TOP exist\nfor planning in multi-robot systems, they assume that all the robots cooperate\ntoward a single objective; thus, they do not extend to settings where the\nrobots compete in reward-scarce environments. We propose Stochastic\nPrize-Collecting Games (SPCG) as an extension of the TOP to plan in the\npresence of self-interested robots operating on a graph, under energy\nconstraints and stochastic transitions. A theoretical study on complete and\nstar graphs establishes that there is a unique pure Nash equilibrium in SPCGs\nthat coincides with the optimal routing solution of an equivalent TOP given a\nrank-based conflict resolution rule. This work proposes two algorithms: Ordinal\nRank Search (ORS) to obtain the ''ordinal rank'' --one's effective rank in\ntemporarily-formed local neighborhoods during the games' stages, and Fictitious\nOrdinal Response Learning (FORL) to obtain best-response policies against one's\nsenior-rank opponents. Empirical evaluations conducted on road networks and\nsynthetic graphs under both dynamic and stationary prize distributions show\nthat 1) the state-aliasing induced by OR-conditioning enables learning policies\nthat scale more efficiently to large team sizes than those trained with the\nglobal index, and 2) Policies trained with FORL generalize better to imbalanced\nprize distributions than those with other multi-agent training methods.\nFinally, the learned policies in the SPCG achieved between 87% and 95%\noptimality compared to an equivalent TOP solution obtained by mixed-integer\nlinear programming."}
{"id": "2510.24533", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24533", "abs": "https://arxiv.org/abs/2510.24533", "authors": ["Yuan Shen", "Yuze Hong", "Guangyang Zeng", "Tengfei Zhang", "Pui Yi Chui", "Ziyang Hong", "Junfeng Wu"], "title": "GeVI-SLAM: Gravity-Enhanced Stereo Visua Inertial SLAM for Underwater Robots", "comment": null, "summary": "Accurate visual inertial simultaneous localization and mapping (VI SLAM) for\nunderwater robots remains a significant challenge due to frequent visual\ndegeneracy and insufficient inertial measurement unit (IMU) motion excitation.\nIn this paper, we present GeVI-SLAM, a gravity-enhanced stereo VI SLAM system\ndesigned to address these issues. By leveraging the stereo camera's direct\ndepth estimation ability, we eliminate the need to estimate scale during IMU\ninitialization, enabling stable operation even under low acceleration dynamics.\nWith precise gravity initialization, we decouple the pitch and roll from the\npose estimation and solve a 4 degrees of freedom (DOF) Perspective-n-Point\n(PnP) problem for pose tracking. This allows the use of a minimal 3-point\nsolver, which significantly reduces computational time to reject outliers\nwithin a Random Sample Consensus framework. We further propose a\nbias-eliminated 4-DOF PnP estimator with provable consistency, ensuring the\nrelative pose converges to the true value as the feature number increases. To\nhandle dynamic motion, we refine the full 6-DOF pose while jointly estimating\nthe IMU covariance, enabling adaptive weighting of the gravity prior. Extensive\nexperiments on simulated and real-world data demonstrate that GeVI-SLAM\nachieves higher accuracy and greater stability compared to state-of-the-art\nmethods."}
{"id": "2510.24554", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24554", "abs": "https://arxiv.org/abs/2510.24554", "authors": ["Vignesh Kottayam Viswanathan", "Yifan Bai", "Scott Fredriksson", "Sumeet Satpute", "Christoforos Kanellakis", "George Nikolakopoulos"], "title": "An Adaptive Inspection Planning Approach Towards Routine Monitoring in Uncertain Environments", "comment": "Submitted for ICRA 2026", "summary": "In this work, we present a hierarchical framework designed to support robotic\ninspection under environment uncertainty. By leveraging a known environment\nmodel, existing methods plan and safely track inspection routes to visit points\nof interest. However, discrepancies between the model and actual site\nconditions, caused by either natural or human activities, can alter the surface\nmorphology or introduce path obstructions. To address this challenge, the\nproposed framework divides the inspection task into: (a) generating the initial\nglobal view-plan for region of interests based on a historical map and (b)\nlocal view replanning to adapt to the current morphology of the inspection\nscene. The proposed hierarchy preserves global coverage objectives while\nenabling reactive adaptation to the local surface morphology. This enables the\nlocal autonomy to remain robust against environment uncertainty and complete\nthe inspection tasks. We validate the approach through deployments in\nreal-world subterranean mines using quadrupedal robot."}
{"id": "2510.24571", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24571", "abs": "https://arxiv.org/abs/2510.24571", "authors": ["Hongxu Zhao", "Guangyang Zeng", "Yunling Shao", "Tengfei Zhang", "Junfeng Wu"], "title": "Spatiotemporal Calibration of Doppler Velocity Logs for Underwater Robots", "comment": null, "summary": "The calibration of extrinsic parameters and clock offsets between sensors for\nhigh-accuracy performance in underwater SLAM systems remains insufficiently\nexplored. Existing methods for Doppler Velocity Log (DVL) calibration are\neither constrained to specific sensor configurations or rely on oversimplified\nassumptions, and none jointly estimate translational extrinsics and time\noffsets. We propose a Unified Iterative Calibration (UIC) framework for general\nDVL sensor setups, formulated as a Maximum A Posteriori (MAP) estimation with a\nGaussian Process (GP) motion prior for high-fidelity motion interpolation. UIC\nalternates between efficient GP-based motion state updates and gradient-based\ncalibration variable updates, supported by a provably statistically consistent\nsequential initialization scheme. The proposed UIC can be applied to IMU,\ncameras and other modalities as co-sensors. We release an open-source\nDVL-camera calibration toolbox. Beyond underwater applications, several aspects\nof UIC-such as the integration of GP priors for MAP-based calibration and the\ndesign of provably reliable initialization procedures-are broadly applicable to\nother multi-sensor calibration problems. Finally, simulations and real-world\ntests validate our approach."}
{"id": "2510.24584", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24584", "abs": "https://arxiv.org/abs/2510.24584", "authors": ["Jørgen Anker Olsen", "Lars Rønhaug Pettersen", "Kostas Alexis"], "title": "Towards Quadrupedal Jumping and Walking for Dynamic Locomotion using Reinforcement Learning", "comment": "8 pages", "summary": "This paper presents a curriculum-based reinforcement learning framework for\ntraining precise and high-performance jumping policies for the robot `Olympus'.\nSeparate policies are developed for vertical and horizontal jumps, leveraging a\nsimple yet effective strategy. First, we densify the inherently sparse jumping\nreward using the laws of projectile motion. Next, a reference state\ninitialization scheme is employed to accelerate the exploration of dynamic\njumping behaviors without reliance on reference trajectories. We also present a\nwalking policy that, when combined with the jumping policies, unlocks versatile\nand dynamic locomotion capabilities. Comprehensive testing validates walking on\nvaried terrain surfaces and jumping performance that exceeds previous works,\neffectively crossing the Sim2Real gap. Experimental validation demonstrates\nhorizontal jumps up to 1.25 m with centimeter accuracy and vertical jumps up to\n1.0 m. Additionally, we show that with only minor modifications, the proposed\nmethod can be used to learn omnidirectional jumping."}
{"id": "2510.24623", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24623", "abs": "https://arxiv.org/abs/2510.24623", "authors": ["Nicolai Steinke", "Daniel Goehring"], "title": "GroundLoc: Efficient Large-Scale Outdoor LiDAR-Only Localization", "comment": null, "summary": "In this letter, we introduce GroundLoc, a LiDAR-only localization pipeline\ndesigned to localize a mobile robot in large-scale outdoor environments using\nprior maps. GroundLoc employs a Bird's-Eye View (BEV) image projection focusing\non the perceived ground area and utilizes the place recognition network R2D2,\nor alternatively, the non-learning approach Scale-Invariant Feature Transform\n(SIFT), to identify and select keypoints for BEV image map registration. Our\nresults demonstrate that GroundLoc outperforms state-of-the-art methods on the\nSemanticKITTI and HeLiPR datasets across various sensors. In the multi-session\nlocalization evaluation, GroundLoc reaches an Average Trajectory Error (ATE)\nwell below 50 cm on all Ouster OS2 128 sequences while meeting online runtime\nrequirements. The system supports various sensor models, as evidenced by\nevaluations conducted with Velodyne HDL-64E, Ouster OS2 128, Aeva Aeries II,\nand Livox Avia sensors. The prior maps are stored as 2D raster image maps,\nwhich can be created from a single drive and require only 4 MB of storage per\nsquare kilometer. The source code is available at\nhttps://github.com/dcmlr/groundloc."}
{"id": "2510.24671", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24671", "abs": "https://arxiv.org/abs/2510.24671", "authors": ["Li Li", "Tobias Brinkmann", "Till Temmen", "Markus Eisenbarth", "Jakob Andert"], "title": "Multi-Agent Scenario Generation in Roundabouts with a Transformer-enhanced Conditional Variational Autoencoder", "comment": null, "summary": "With the increasing integration of intelligent driving functions into\nserial-produced vehicles, ensuring their functionality and robustness poses\ngreater challenges. Compared to traditional road testing, scenario-based\nvirtual testing offers significant advantages in terms of time and cost\nefficiency, reproducibility, and exploration of edge cases. We propose a\nTransformer-enhanced Conditional Variational Autoencoder (CVAE-T) model for\ngenerating multi-agent traffic scenarios in roundabouts, which are\ncharacterized by high vehicle dynamics and complex layouts, yet remain\nrelatively underexplored in current research. The results show that the\nproposed model can accurately reconstruct original scenarios and generate\nrealistic, diverse synthetic scenarios. Besides, two Key-Performance-Indicators\n(KPIs) are employed to evaluate the interactive behavior in the generated\nscenarios. Analysis of the latent space reveals partial disentanglement, with\nseveral latent dimensions exhibiting distinct and interpretable effects on\nscenario attributes such as vehicle entry timing, exit timing, and velocity\nprofiles. The results demonstrate the model's capability to generate scenarios\nfor the validation of intelligent driving functions involving multi-agent\ninteractions, as well as to augment data for their development and iterative\nimprovement."}
{"id": "2510.24676", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.24676", "abs": "https://arxiv.org/abs/2510.24676", "authors": ["Jiaxuan Zhang", "Yuquan Leng", "Yixuan Guo", "Chenglong Fu"], "title": "Feature Matching-Based Gait Phase Prediction for Obstacle Crossing Control of Powered Transfemoral Prosthesis", "comment": "6 pages, conference", "summary": "For amputees with powered transfemoral prosthetics, navigating obstacles or\ncomplex terrain remains challenging. This study addresses this issue by using\nan inertial sensor on the sound ankle to guide obstacle-crossing movements. A\ngenetic algorithm computes the optimal neural network structure to predict the\nrequired angles of the thigh and knee joints. A gait progression prediction\nalgorithm determines the actuation angle index for the prosthetic knee motor,\nultimately defining the necessary thigh and knee angles and gait progression.\nResults show that when the standard deviation of Gaussian noise added to the\nthigh angle data is less than 1, the method can effectively eliminate noise\ninterference, achieving 100\\% accuracy in gait phase estimation under 150 Hz,\nwith thigh angle prediction error being 8.71\\% and knee angle prediction error\nbeing 6.78\\%. These findings demonstrate the method's ability to accurately\npredict gait progression and joint angles, offering significant practical value\nfor obstacle negotiation in powered transfemoral prosthetics."}
{"id": "2510.24680", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24680", "abs": "https://arxiv.org/abs/2510.24680", "authors": ["Zishuo Wang", "Joel Loo", "David Hsu"], "title": "Fare: Failure Resilience in Learned Visual Navigation Control", "comment": null, "summary": "While imitation learning (IL) enables effective visual navigation, IL\npolicies are prone to unpredictable failures in out-of-distribution (OOD)\nscenarios. We advance the notion of failure-resilient policies, which not only\ndetect failures but also recover from them automatically. Failure recognition\nthat identifies the factors causing failure is key to informing recovery: e.g.\npinpointing image regions triggering failure detections can provide cues to\nguide recovery. We present Fare, a framework to construct failure-resilient IL\npolicies, embedding OOD-detection and recognition in them without using\nexplicit failure data, and pairing them with recovery heuristics. Real-world\nexperiments show that Fare enables failure recovery across two different policy\narchitectures, enabling robust long-range navigation in complex environments."}
{"id": "2510.24683", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24683", "abs": "https://arxiv.org/abs/2510.24683", "authors": ["Caleb Escobedo", "Nataliya Nechyporenko", "Shreyas Kadekodi", "Alessandro Roncone"], "title": "A Framework for the Systematic Evaluation of Obstacle Avoidance and Object-Aware Controllers", "comment": null, "summary": "Real-time control is an essential aspect of safe robot operation in the real\nworld with dynamic objects. We present a framework for the analysis of\nobject-aware controllers, methods for altering a robot's motion to anticipate\nand avoid possible collisions. This framework is focused on three design\nconsiderations: kinematics, motion profiles, and virtual constraints.\nAdditionally, the analysis in this work relies on verification of robot\nbehaviors using fundamental robot-obstacle experimental scenarios. To showcase\nthe effectiveness of our method we compare three representative object-aware\ncontrollers. The comparison uses metrics originating from the design\nconsiderations. From the analysis, we find that the design of object-aware\ncontrollers often lacks kinematic considerations, continuity of control points,\nand stability in movement profiles. We conclude that this framework can be used\nin the future to design, compare, and benchmark obstacle avoidance methods."}
{"id": "2510.24692", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24692", "abs": "https://arxiv.org/abs/2510.24692", "authors": ["Jun Wang", "Ziyang Zhou", "Ardalan Kahak", "Suyi Li"], "title": "Embodying Physical Computing into Soft Robots", "comment": null, "summary": "Softening and onboarding computers and controllers is one of the final\nfrontiers in soft robotics towards their robustness and intelligence for\neveryday use. In this regard, embodying soft and physical computing presents\nexciting potential. Physical computing seeks to encode inputs into a mechanical\ncomputing kernel and leverage the internal interactions among this kernel's\nconstituent elements to compute the output. Moreover, such input-to-output\nevolution can be re-programmable. This perspective paper proposes a framework\nfor embodying physical computing into soft robots and discusses three unique\nstrategies in the literature: analog oscillators, physical reservoir computing,\nand physical algorithmic computing. These embodied computers enable the soft\nrobot to perform complex behaviors that would otherwise require CMOS-based\nelectronics -- including coordinated locomotion with obstacle avoidance,\npayload weight and orientation classification, and programmable operation based\non logical rules. This paper will detail the working principles of these\nembodied physical computing methods, survey the current state-of-the-art, and\npresent a perspective for future development."}
