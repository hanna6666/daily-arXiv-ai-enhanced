{"id": "2510.17948", "categories": ["cs.RO", "cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.17948", "abs": "https://arxiv.org/abs/2510.17948", "authors": ["Christopher A McClurg", "Alan R Wagner"], "title": "Studying the Effects of Robot Intervention on School Shooters in Virtual Reality", "comment": "Preprint under review for conference publication. 10 pages, 9\n  figures, 3 tables (including 1-page appendix)", "summary": "We advance the understanding of robotic intervention in high-risk scenarios\nby examining their potential to distract and impede a school shooter. To\nevaluate this concept, we conducted a virtual reality study with 150 university\nparticipants role-playing as a school shooter. Within the simulation, an\nautonomous robot predicted the shooter's movements and positioned itself\nstrategically to interfere and distract. The strategy the robot used to\napproach the shooter was manipulated -- either moving directly in front of the\nshooter (aggressive) or maintaining distance (passive) -- and the distraction\nmethod, ranging from no additional cues (low), to siren and lights (medium), to\nsiren, lights, and smoke to impair visibility (high). An aggressive,\nhigh-distraction robot reduced the number of victims by 46.6% relative to a\nno-robot control. This outcome underscores both the potential of robotic\nintervention to enhance safety and the pressing ethical questions surrounding\ntheir use in school environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u673a\u5668\u4eba\u5728\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\u5bf9\u5b66\u6821\u67aa\u624b\u7684\u5e72\u9884\u6f5c\u529b\uff0c\u53d1\u73b0\u4e3b\u52a8\u5e72\u6270\u7684\u673a\u5668\u4eba\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u53d7\u5bb3\u8005\u6570\u91cf\u3002", "motivation": "\u63d0\u5347\u5bf9\u673a\u5668\u4eba\u5728\u9ad8\u98ce\u9669\u60c5\u51b5\u4e0b\u7684\u5e72\u9884\u80fd\u529b\u7684\u7406\u89e3\uff0c\u5c24\u5176\u662f\u5728\u5b66\u6821\u5b89\u5168\u9886\u57df\u3002", "method": "\u901a\u8fc7\u865a\u62df\u73b0\u5b9e\u7814\u7a76\uff0c150\u540d\u5927\u5b66\u53c2\u4e0e\u8005\u6a21\u62df\u5b66\u6821\u67aa\u624b\uff0c\u8bc4\u4f30\u4e0d\u540c\u7c7b\u578b\u7684\u673a\u5668\u4eba\u5e72\u6270\u7b56\u7565\u3002", "result": "\u4e3b\u52a8\u3001\u9ad8\u5e72\u6270\u7684\u673a\u5668\u4eba\u4f7f\u53d7\u5bb3\u8005\u6570\u91cf\u51cf\u5c11\u4e8646.6%\uff0c\u76f8\u8f83\u4e8e\u4e0d\u5f00\u542f\u673a\u5668\u4eba\u7684\u63a7\u5236\u7ec4\u3002", "conclusion": "\u673a\u5668\u4eba\u5e72\u9884\u80fd\u591f\u6709\u6548\u63d0\u5347\u5b89\u5168\u6027\uff0c\u4f46\u5728\u5b66\u6821\u73af\u5883\u4e2d\u4f7f\u7528\u9700\u8003\u8651\u4f26\u7406\u95ee\u9898\u3002"}}
{"id": "2510.17950", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17950", "abs": "https://arxiv.org/abs/2510.17950", "authors": ["Adina Yakefu", "Bin Xie", "Chongyang Xu", "Enwen Zhang", "Erjin Zhou", "Fan Jia", "Haitao Yang", "Haoqiang Fan", "Haowei Zhang", "Hongyang Peng", "Jing Tan", "Junwen Huang", "Kai Liu", "Kaixin Liu", "Kefan Gu", "Qinglun Zhang", "Ruitao Zhang", "Saike Huang", "Shen Cheng", "Shuaicheng Liu", "Tiancai Wang", "Tiezhen Wang", "Wei Sun", "Wenbin Tang", "Yajun Wei", "Yang Chen", "Youqiang Gui", "Yucheng Zhao", "Yunchao Ma", "Yunfei Wei", "Yunhuan Yang", "Yutong Guo", "Ze Chen", "Zhengyuan Du", "Ziheng Zhang", "Ziming Liu", "Ziwei Yan"], "title": "RoboChallenge: Large-scale Real-robot Evaluation of Embodied Policies", "comment": "Authors are listed in alphabetical order. The official website is\n  located at https://robochallenge.ai", "summary": "Testing on real machines is indispensable for robotic control algorithms. In\nthe context of learning-based algorithms, especially VLA models, demand for\nlarge-scale evaluation, i.e. testing a large number of models on a large number\nof tasks, is becoming increasingly urgent. However, doing this right is highly\nnon-trivial, especially when scalability and reproducibility is taken into\naccount. In this report, we describe our methodology for constructing\nRoboChallenge, an online evaluation system to test robotic control algorithms,\nand our survey of recent state-of-the-art VLA models using our initial\nbenchmark Table30.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5728\u7ebf\u8bc4\u4f30\u7cfb\u7edfRoboChallenge\uff0c\u7528\u4e8e\u6d4b\u8bd5\u673a\u5668\u4eba\u63a7\u5236\u7b97\u6cd5\uff0c\u5c24\u5176\u662f\u5927\u89c4\u6a21\u8bc4\u4f30\u5b66\u4e60\u578b\u7b97\u6cd5\u7684\u9700\u6c42\u6108\u53d1\u8feb\u5207\u3002", "motivation": "\u968f\u7740\u5bf9\u5927\u89c4\u6a21\u8bc4\u4f30\u7684\u9700\u6c42\u589e\u52a0\uff0c\u5c24\u5176\u662f\u5728\u5b66\u4e60\u578b\u7b97\u6cd5\u9886\u57df\uff0c\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u53ef\u9760\u7684\u7cfb\u7edf\u6765\u4fdd\u8bc1\u6d4b\u8bd5\u7684\u53ef\u6269\u5c55\u6027\u548c\u53ef\u91cd\u73b0\u6027\u3002", "method": "\u672c\u6587\u63cf\u8ff0\u4e86RoboChallenge\u7684\u6784\u5efa\u65b9\u6cd5\uff0c\u5f3a\u8c03\u4e86\u5728\u7ebf\u8bc4\u4f30\u7cfb\u7edf\u5728\u673a\u5668\u4eba\u63a7\u5236\u7b97\u6cd5\u6d4b\u8bd5\u4e2d\u7684\u91cd\u8981\u6027\u3002", "result": "\u6211\u4eec\u901a\u8fc7\u521d\u6b65\u57fa\u51c6Table30\u5bf9\u6700\u8fd1\u7684\u5148\u8fdbVLA\u6a21\u578b\u8fdb\u884c\u4e86\u8c03\u67e5\u548c\u8bc4\u4f30\u3002", "conclusion": "\u901a\u8fc7RoboChallenge\uff0c\u6211\u4eec\u80fd\u591f\u66f4\u6709\u6548\u5730\u6d4b\u8bd5\u548c\u8bc4\u4f30\u6700\u65b0\u7684VLA\u6a21\u578b\u3002"}}
{"id": "2510.18002", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.18002", "abs": "https://arxiv.org/abs/2510.18002", "authors": ["Junli Ren", "Junfeng Long", "Tao Huang", "Huayi Wang", "Zirui Wang", "Feiyu Jia", "Wentao Zhang", "Jingbo Wang", "Ping Luo", "Jiangmiao Pang"], "title": "Humanoid Goalkeeper: Learning from Position Conditioned Task-Motion Constraints", "comment": null, "summary": "We present a reinforcement learning framework for autonomous goalkeeping with\nhumanoid robots in real-world scenarios. While prior work has demonstrated\nsimilar capabilities on quadrupedal platforms, humanoid goalkeeping introduces\ntwo critical challenges: (1) generating natural, human-like whole-body motions,\nand (2) covering a wider guarding range with an equivalent response time.\nUnlike existing approaches that rely on separate teleoperation or fixed motion\ntracking for whole-body control, our method learns a single end-to-end RL\npolicy, enabling fully autonomous, highly dynamic, and human-like robot-object\ninteractions. To achieve this, we integrate multiple human motion priors\nconditioned on perceptual inputs into the RL training via an adversarial\nscheme. We demonstrate the effectiveness of our method through real-world\nexperiments, where the humanoid robot successfully performs agile, autonomous,\nand naturalistic interceptions of fast-moving balls. In addition to\ngoalkeeping, we demonstrate the generalization of our approach through tasks\nsuch as ball escaping and grabbing. Our work presents a practical and scalable\nsolution for enabling highly dynamic interactions between robots and moving\nobjects, advancing the field toward more adaptive and lifelike robotic\nbehaviors.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4f7f\u4eba\u5f62\u673a\u5668\u4eba\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u80fd\u591f\u81ea\u4e3b\u3001\u7075\u6d3b\u5730\u8fdb\u884c\u5b88\u95e8\u53ca\u5176\u4ed6\u76f8\u5173\u4efb\u52a1\uff0c\u63a8\u52a8\u4e86\u673a\u5668\u4eba\u4e0e\u52a8\u6001\u7269\u4f53\u4ea4\u4e92\u7684\u5b9e\u73b0\u3002", "motivation": "\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u5b88\u95e8\u4efb\u52a1\u4e2d\u6d89\u53ca\u7684\u81ea\u7136\u52a8\u4f5c\u751f\u6210\u548c\u5feb\u901f\u53cd\u5e94\u8303\u56f4\u8986\u76d6\u7b49\u5173\u952e\u6311\u6218\uff0c\u63a8\u52a8\u673a\u5668\u4eba\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u6574\u5408\u591a\u4e2a\u4eba\u4f53\u8fd0\u52a8\u5148\u9a8c\uff0c\u5229\u7528\u5bf9\u6297\u6027\u65b9\u6848\u5c06\u5176\u6761\u4ef6\u5316\u4e8e\u611f\u77e5\u8f93\u5165\uff0c\u5b9e\u73b0\u5355\u4e00\u7aef\u5230\u7aef\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u8fdb\u884c\u81ea\u4e3b\u52a8\u6001\u63a7\u5236\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4f7f\u4eba\u5f62\u673a\u5668\u4eba\u80fd\u591f\u6210\u529f\u5b9e\u73b0\u7075\u6d3b\u3001\u81ea\u7136\u7684\u5feb\u901f\u7403\u62e6\u622a\uff0c\u5e76\u80fd\u591f\u63a8\u5e7f\u81f3\u5176\u4ed6\u4efb\u52a1\u5982\u7403\u9003\u907f\u548c\u6293\u53d6\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u7684\u81ea\u4e3b\u5b88\u95e8\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5728\u52a8\u6001\u4e92\u52a8\u4e2d\u5b9e\u73b0\u4eba\u7c7b\u822c\u884c\u4e3a\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.18063", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.18063", "abs": "https://arxiv.org/abs/2510.18063", "authors": ["Bin-Bin Hu", "Weijia Yao", "Ming Cao"], "title": "MOFM-Nav: On-Manifold Ordering-Flexible Multi-Robot Navigation", "comment": null, "summary": "This paper addresses the problem of multi-robot navigation where robots\nmaneuver on a desired \\(m\\)-dimensional (i.e., \\(m\\)-D) manifold in the\n$n$-dimensional Euclidean space, and maintain a {\\it flexible spatial\nordering}. We consider $ m\\geq 2$, and the multi-robot coordination is achieved\nvia non-Euclidean metrics. However, since the $m$-D manifold can be\ncharacterized by the zero-level sets of $n$ implicit functions, the last $m$\nentries of the GVF propagation term become {\\it strongly coupled} with the\npartial derivatives of these functions if the auxiliary vectors are not\nappropriately chosen. These couplings not only influence the on-manifold\nmaneuvering of robots, but also pose significant challenges to the further\ndesign of the ordering-flexible coordination via non-Euclidean metrics.\n  To tackle this issue, we first identify a feasible solution of auxiliary\nvectors such that the last $m$ entries of the propagation term are effectively\ndecoupled to be the same constant. Then, we redesign the coordinated GVF (CGVF)\nalgorithm to {\\it boost} the advantages of singularities elimination and global\nconvergence by treating $m$ manifold parameters as additional $m$ virtual\ncoordinates. Furthermore, we enable the on-manifold ordering-flexible motion\ncoordination by allowing each robot to share $m$ virtual coordinates with its\ntime-varying neighbors and a virtual target robot, which {\\it circumvents} the\npossible complex calculation if Euclidean metrics were used instead. Finally,\nwe showcase the proposed algorithm's flexibility, adaptability, and robustness\nthrough extensive simulations with different initial positions,\nhigher-dimensional manifolds, and robot breakdown, respectively.", "AI": {"tldr": "\u8be5\u8bba\u6587\u805a\u7126\u4e8e\u591a\u673a\u5668\u4eba\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u5bfc\u822a\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u534f\u8c03\u7b97\u6cd5\u4ee5\u5b9e\u73b0\u7075\u6d3b\u7684\u6392\u5e8f\u548c\u9ad8\u6548\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u534f\u8c03\u3002", "motivation": "\u7814\u7a76\u591a\u673a\u5668\u4eba\u534f\u8c03\u5728\u9ad8\u7ef4\u6d41\u5f62\u4e0a\u7684\u5bfc\u822a\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u6b27\u51e0\u91cc\u5f97\u5ea6\u91cf\u8ba1\u7b97\u590d\u6742\u65f6\uff0c\u63a2\u7d22\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u4ee5\u63d0\u5347\u7075\u6d3b\u6027\u548c\u534f\u8c03\u6027\u3002", "method": "\u901a\u8fc7\u8bc6\u522b\u8f85\u52a9\u5411\u91cf\u7684\u53ef\u884c\u89e3\uff0c\u91cd\u65b0\u8bbe\u8ba1\u534f\u8c03GVF\u7b97\u6cd5\uff0c\u4f7f\u5f97\u673a\u5668\u4eba\u8fd0\u52a8\u80fd\u591f\u6709\u6548\u89e3\u8026\uff0c\u5e76\u5171\u4eab\u865a\u62df\u5750\u6807\u4ee5\u7b80\u5316\u8ba1\u7b97\u3002", "result": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u673a\u5668\u4eba\u5bfc\u822a\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u673a\u5668\u4eba\u5728\u9ad8\u7ef4\u6d41\u5f62\u4e0a\u7075\u6d3b\u6392\u5e8f\u7684\u95ee\u9898\u3002\u901a\u8fc7\u975e\u6b27\u51e0\u91cc\u5f97\u5ea6\u91cf\u5b9e\u73b0\u591a\u673a\u5668\u4eba\u534f\u8c03\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u7684\u534f\u8c03GVF\u7b97\u6cd5\u4ee5\u589e\u5f3a\u7075\u6d3b\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u4eff\u771f\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u7b97\u6cd5\u5728\u4e0d\u540c\u521d\u59cb\u4f4d\u7f6e\u3001\u9ad8\u7ef4\u6d41\u5f62\u53ca\u673a\u5668\u4eba\u6545\u969c\u60c5\u51b5\u4e0b\u7684\u7075\u6d3b\u6027\u3001\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.18039", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.18039", "abs": "https://arxiv.org/abs/2510.18039", "authors": ["Allison Chen", "Sunnie S. Y. Kim", "Angel Franyutti", "Amaya Dharmasiri", "Kushin Mukherjee", "Olga Russakovsky", "Judith E. Fan"], "title": "Presenting Large Language Models as Companions Affects What Mental Capacities People Attribute to Them", "comment": null, "summary": "How does messaging about about large language models (LLMs) in public\ndiscourse influence the way people think about and interact with these models?\nTo answer this question, we randomly assigned participants (N = 470) to watch a\nshort informational video presenting LLMs as either machines, tools, or\ncompanions -- or to watch no video. We then assessed how strongly they believed\nLLMs to possess various mental capacities, such as the ability have intentions\nor remember things. We found that participants who watched the companion video\nreported believing that LLMs more fully possessed these capacities than did\nparticipants in other groups. In a follow-up study (N = 604), we replicated\nthese findings and found nuanced effects on how these videos impact people's\nreliance on LLM-generated responses when seeking out factual information.\nTogether, these studies highlight the impact of messaging about AI -- beyond\ntechnical advances in AI -- to generate broad societal impact.", "AI": {"tldr": "\u6b64\u7814\u7a76\u63a2\u8ba8\u5173\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4fe1\u606f\u4f20\u8fbe\u5982\u4f55\u5f71\u54cd\u4eba\u4eec\u5bf9\u5b83\u4eec\u7684\u770b\u6cd5\u548c\u4f7f\u7528\uff0c\u53d1\u73b0\u5c06LLMs\u89c6\u4e3a\u2018\u4f34\u4fa3\u2019\u4f1a\u589e\u5f3a\u4eba\u4eec\u5bf9\u5176\u5fc3\u7406\u80fd\u529b\u7684\u4fe1\u5ff5\u3002", "motivation": "\u7814\u7a76\u516c\u5171\u8bdd\u8bed\u4e2d\u5173\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4fe1\u606f\u5982\u4f55\u5f71\u54cd\u4eba\u4eec\u5bf9\u8fd9\u4e9b\u6a21\u578b\u7684\u770b\u6cd5\u548c\u4e92\u52a8\u65b9\u5f0f\u3002", "method": "\u968f\u673a\u5206\u914d\u53c2\u4e0e\u8005\u89c2\u770b\u4e0d\u540c\u7c7b\u578b\u7684\u89c6\u9891\uff08\u673a\u5668\u3001\u5de5\u5177\u3001\u4f34\u4fa3\uff09\u6216\u4e0d\u89c2\u770b\u89c6\u9891\uff0c\u968f\u540e\u8bc4\u4f30\u4ed6\u4eec\u5bf9LLMs\u5fc3\u7406\u80fd\u529b\u7684\u4fe1\u5ff5\u3002", "result": "\u89c2\u770b\u4f34\u4fa3\u89c6\u9891\u7684\u53c2\u4e0e\u8005\u8ba4\u4e3aLLMs\u66f4\u5177\u5907\u610f\u56fe\u6216\u8bb0\u5fc6\u7b49\u5fc3\u7406\u80fd\u529b\u3002", "conclusion": "\u4fe1\u606f\u4f20\u8fbe\u65b9\u5f0f\u5bf9\u4eba\u4eec\u5bf9AI\u7684\u770b\u6cd5\u6709\u663e\u8457\u5f71\u54cd\uff0c\u8d85\u8d8a\u4e86\u6280\u672f\u8fdb\u6b65\u5bf9\u793e\u4f1a\u7684\u5f71\u54cd\u3002"}}
{"id": "2510.18085", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.18085", "abs": "https://arxiv.org/abs/2510.18085", "authors": ["Connor Mattson", "Varun Raveendra", "Ellen Novoseller", "Nicholas Waytowich", "Vernon J. Lawhern", "Daniel S. Brown"], "title": "R2BC: Multi-Agent Imitation Learning from Single-Agent Demonstrations", "comment": "9 pages, 6 figures", "summary": "Imitation Learning (IL) is a natural way for humans to teach robots,\nparticularly when high-quality demonstrations are easy to obtain. While IL has\nbeen widely applied to single-robot settings, relatively few studies have\naddressed the extension of these methods to multi-agent systems, especially in\nsettings where a single human must provide demonstrations to a team of\ncollaborating robots. In this paper, we introduce and study Round-Robin\nBehavior Cloning (R2BC), a method that enables a single human operator to\neffectively train multi-robot systems through sequential, single-agent\ndemonstrations. Our approach allows the human to teleoperate one agent at a\ntime and incrementally teach multi-agent behavior to the entire system, without\nrequiring demonstrations in the joint multi-agent action space. We show that\nR2BC methods match, and in some cases surpass, the performance of an oracle\nbehavior cloning approach trained on privileged synchronized demonstrations\nacross four multi-agent simulated tasks. Finally, we deploy R2BC on two\nphysical robot tasks trained using real human demonstrations.", "AI": {"tldr": "R2BC\u662f\u4e00\u79cd\u65b0\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u5141\u8bb8\u5355\u4eba\u6709\u6548\u8bad\u7ec3\u591a\u4e2a\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u8f6e\u6d41\u793a\u8303\u5b9e\u73b0\u4f18\u5f02\u7684\u8bad\u7ec3\u6548\u679c\u3002", "motivation": "\u63a2\u8ba8\u5982\u4f55\u5c06\u6a21\u4eff\u5b66\u4e60\u6269\u5c55\u81f3\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u5c24\u5176\u5728\u5355\u4e00\u4eba\u7c7b\u4e3a\u591a\u4e2a\u534f\u4f5c\u673a\u5668\u4eba\u63d0\u4f9b\u793a\u8303\u7684\u80cc\u666f\u4e0b\u3002", "method": "\u901a\u8fc7\u8f6e\u6d41\u793a\u8303\uff0c\u5355\u4e00\u4eba\u7c7b\u64cd\u4f5c\u5458\u9010\u4e2a\u8bad\u7ec3\u673a\u5668\u4eba\uff0c\u5b9e\u73b0\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u884c\u4e3a\u7684\u589e\u91cf\u6559\u5b66\u3002", "result": "R2BC\u5728\u56db\u4e2a\u591a\u4ee3\u7406\u6a21\u62df\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u4e0e\u4f20\u7edf\u884c\u4e3a\u514b\u9686\u65b9\u6cd5\u76f8\u5f53\u7684\u8868\u73b0\uff0c\u8fd8\u5728\u4e24\u4e2a\u5b9e\u9645\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u6210\u529f\u5e94\u7528\u3002", "conclusion": "R2BC\u65b9\u6cd5\u5728\u591a\u4ee3\u7406\u7cfb\u7edf\u4e0a\u6709\u6548\u5730\u8fdb\u884c\u8bad\u7ec3\uff0c\u4e14\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u884c\u4e3a\u514b\u9686\u65b9\u6cd5\u3002"}}
{"id": "2510.18158", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.18158", "abs": "https://arxiv.org/abs/2510.18158", "authors": ["Yixue Cai", "Xiyan Su", "Dongpeng Yao", "Rongduo Han", "Nan Gao", "Haining Zhang"], "title": "Design and Challenges of Mental Health Assessment Tools Based on Natural Language Interaction", "comment": null, "summary": "Mental health assessments are of central importance to individuals'\nwell-being. Conventional assessment methodologies predominantly depend on\nclinical interviews and standardised self-report questionnaires. Nevertheless,\nthe efficacy of these methodologies is frequently impeded by factors such as\nsubjectivity, recall bias, and accessibility issues. Furthermore, concerns\nregarding bias and privacy may result in misreporting in data collected through\nself-reporting in mental health research. The present study examined the design\nopportunities and challenges inherent in the development of a mental health\nassessment tool based on natural language interaction with large language\nmodels (LLMs). An interactive prototype system was developed using\nconversational AI for non-invasive mental health assessment, and was evaluated\nthrough semi-structured interviews with 11 mental health professionals (six\ncounsellors and five psychiatrists). The analysis identified key design\nconsiderations for future development, highlighting how AI-driven adaptive\nquestioning could potentially enhance the reliability of self-reported data\nwhile identifying critical challenges, including privacy protection,\nalgorithmic bias, and cross-cultural applicability. This study provides an\nempirical foundation for mental health technology innovation by demonstrating\nthe potential and limitations of natural language interaction in mental health\nassessment.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5229\u7528\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u7684AI\u5fc3\u7406\u5065\u5eb7\u8bc4\u4f30\u5de5\u5177\u7684\u8bbe\u8ba1\u673a\u4f1a\u4e0e\u6311\u6218\uff0c\u63d0\u5347\u4e86\u81ea\u6211\u62a5\u544a\u6570\u636e\u7684\u53ef\u9760\u6027\uff0c\u4f46\u540c\u65f6\u4e5f\u63ed\u793a\u4e86\u9690\u79c1\u53ca\u504f\u89c1\u7b49\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5fc3\u7406\u5065\u5eb7\u8bc4\u4f30\u65b9\u6cd5\u53d7\u4e3b\u89c2\u6027\u3001\u56de\u5fc6\u504f\u5dee\u53ca\u53ef\u8fbe\u6027\u7b49\u56e0\u7d20\u5f71\u54cd\uff0c\u7814\u7a76\u65e8\u5728\u6539\u5584\u8fd9\u4e00\u73b0\u72b6\uff0c\u63a2\u7d22AI\u5728\u5fc3\u7406\u5065\u5eb7\u8bc4\u4f30\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u5f00\u53d1\u4e00\u4e2a\u57fa\u4e8e\u4f1a\u8bddAI\u7684\u4e92\u52a8\u539f\u578b\u7cfb\u7edf\uff0c\u5e76\u4e0e\u5fc3\u7406\u5065\u5eb7\u4e13\u4e1a\u4eba\u5458\u8fdb\u884c\u534a\u7ed3\u6784\u8bbf\u8c08\u6765\u8bc4\u4f30\u8be5\u7cfb\u7edf\u3002", "result": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u7684\u5fc3\u7406\u5065\u5eb7\u8bc4\u4f30\u5de5\u5177\u7684\u8bbe\u8ba1\u673a\u4f1a\u4e0e\u6311\u6218\u3002\u901a\u8fc7\u4e0e11\u540d\u5fc3\u7406\u5065\u5eb7\u4e13\u4e1a\u4eba\u5458\u7684\u534a\u7ed3\u6784\u8bbf\u8c08\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e92\u52a8\u539f\u578b\u7cfb\u7edf\u3002\u7ed3\u679c\u663e\u793a\uff0cAI\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u63d0\u95ee\u53ef\u80fd\u63d0\u5347\u81ea\u6211\u62a5\u544a\u6570\u636e\u7684\u53ef\u9760\u6027\uff0c\u4f46\u4e5f\u9762\u4e34\u9690\u79c1\u4fdd\u62a4\u3001\u7b97\u6cd5\u504f\u89c1\u548c\u8de8\u6587\u5316\u9002\u7528\u6027\u7b49\u5173\u952e\u6311\u6218\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5fc3\u7406\u5065\u5eb7\u6280\u672f\u521b\u65b0\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u5728\u5fc3\u7406\u5065\u5eb7\u8bc4\u4f30\u4e2d\u7684\u6f5c\u529b\u4e0e\u5c40\u9650\u6027\u3002"}}
{"id": "2510.18127", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.18127", "abs": "https://arxiv.org/abs/2510.18127", "authors": ["Dharmik Patel", "Antonio Rafael Vazquez Pantoja", "Jiuzhou Lei", "Kiju Lee", "Xiao Liang", "Minghui Zheng"], "title": "ANGEL: A Novel Gripper for Versatile and Light-touch Fruit Harvesting", "comment": null, "summary": "Fruit harvesting remains predominantly a labor-intensive process, motivating\nthe development of research for robotic grippers. Conventional rigid or\nvacuum-driven grippers require complex mechanical design or high energy\nconsumption. Current enveloping-based fruit harvesting grippers lack\nadaptability to fruits of different sizes. This paper introduces a\ndrawstring-inspired, cable-driven soft gripper for versatile and gentle fruit\nharvesting. The design employs 3D-printed Thermoplastic Polyurethane (TPU)\npockets with integrated steel wires that constrict around the fruit when\nactuated, distributing pressure uniformly to minimize bruising and allow\nversatility to fruits of varying sizes. The lightweight structure, which\nrequires few components, reduces mechanical complexity and cost compared to\nother grippers. Actuation is achieved through servo-driven cable control, while\nmotor feedback provides autonomous grip adjustment with tunable grip strength.\nExperimental validation shows that, for tomatoes within the gripper's effective\nsize range, harvesting was achieved with a 0% immediate damage rate and a\nbruising rate of less than 9% after five days, reinforcing the gripper's\nsuitability for fruit harvesting.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u8f6f\u5f0f\u679c\u5b9e\u91c7\u6458\u6293\u624b\uff0c\u5177\u6709\u9ad8\u9002\u5e94\u6027\u548c\u4f4e\u635f\u4f24\u7387\uff0c\u91c7\u7528\u7b80\u5355\u7684\u8bbe\u8ba1\u548c\u8f7b\u91cf\u5316\u7ed3\u6784\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u89c4\u683c\u7684\u6c34\u679c\u3002", "motivation": "\u4f20\u7edf\u7684\u679c\u5b9e\u91c7\u6458\u4f9d\u8d56\u52b3\u52a8\u529b\uff0c\u73b0\u6709\u6293\u624b\u5728\u8bbe\u8ba1\u590d\u6742\u6027\u4e0e\u80fd\u8017\u4e0a\u5b58\u5728\u95ee\u9898\uff0c\u73b0\u6709\u6293\u624b\u7f3a\u4e4f\u5bf9\u4e0d\u540c\u6c34\u679c\u5c3a\u5bf8\u7684\u9002\u5e94\u6027\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u542f\u53d1\u81ea\u62c9\u7ef3\u7684\u3001\u57fa\u4e8e\u7535\u7f06\u9a71\u52a8\u7684\u8f6f\u5f0f\u6293\u624b", "result": "\u91c7\u75283D\u6253\u5370\u70ed\u5851\u6027\u805a\u6c28\u916f\uff08TPU\uff09\u53e3\u888b\u4e0e\u96c6\u6210\u94a2\u4e1d\u7684\u8bbe\u8ba1\uff0c\u6293\u624b\u5b9e\u73b0\u4e86\u5bf9\u6c34\u679c\u7684\u6e29\u548c\u91c7\u6458\uff0c\u5e76\u5728\u8bd5\u9a8c\u4e2d\u663e\u793a\u51fa\u5bf9\u897f\u7ea2\u67ff\u76840%\u5373\u65f6\u635f\u4f24\u7387\u548c\u4f4e\u4e8e9%\u7684\u538b\u4f24\u7387\u3002", "conclusion": "\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\uff0c\u8be5\u8f6f\u6293\u624b\u5728\u91c7\u6458\u8fc7\u7a0b\u4e2d\u80fd\u591f\u6709\u6548\u9632\u6b62\u6c34\u679c\u635f\u4f24\uff0c\u9002\u5408\u4e8e\u679c\u5b9e\u91c7\u6458\u3002"}}
{"id": "2510.18185", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.18185", "abs": "https://arxiv.org/abs/2510.18185", "authors": ["Karelia Salinas", "Luis Gustavo Nonato", "Jean-Daniel Fekete", "Fernanda Bartolo dos Santos Saran"], "title": "Enhancing Urban Data Exploration: Layer Toggling and Visibility-Preserving Lenses for Multi-Attribute Spatial Analysis", "comment": null, "summary": "We propose two novel interaction techniques for visualization-assisted\nexploration of urban data: Layer Toggling and Visibility-Preserving Lenses.\nLayer Toggling mitigates visual overload by organizing information into\nseparate layers while enabling comparisons through controlled overlays. This\ntechnique supports focused analysis without losing spatial context and allows\nusers to switch layers using a dedicated button. Visibility-Preserving Lenses\nadapt their size and transparency dynamically, enabling detailed inspection of\ndense spatial regions and temporal attributes. These techniques facilitate\nurban data exploration and improve prediction. Understanding complex phenomena\nrelated to crime, mobility, and residents' behavior is crucial for informed\nurban planning. Yet navigating such data often causes cognitive overload and\nvisual clutter due to overlapping layers. We validate our visualization tool\nthrough a user study measuring performance, cognitive load, and interaction\nefficiency. Using real-world data from Sao Paulo, we demonstrate how our\napproach enhances exploratory and analytical tasks and provides guidelines for\nfuture interactive systems.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u521b\u65b0\u7684\u4e92\u52a8\u6280\u672f\uff0c\u65e8\u5728\u6539\u5584\u57ce\u5e02\u6570\u636e\u7684\u53ef\u89c6\u5316\u548c\u63a2\u7d22\uff0c\u51cf\u5c11\u8ba4\u77e5\u8d1f\u62c5\u5e76\u63d0\u5347\u5206\u6790\u6548\u7387\u3002", "motivation": "\u5728\u57ce\u5e02\u89c4\u5212\u4e2d\uff0c\u7406\u89e3\u72af\u7f6a\u3001\u6d41\u52a8\u6027\u53ca\u5c45\u6c11\u884c\u4e3a\u7b49\u590d\u6742\u73b0\u8c61\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6570\u636e\u91cd\u53e0\u4f1a\u5bfc\u81f4\u8ba4\u77e5\u8fc7\u8f7d\u4e0e\u89c6\u89c9\u6742\u4e71\u3002", "method": "\u901a\u8fc7\u7528\u6237\u7814\u7a76\u6d4b\u91cf\u6027\u80fd\u3001\u8ba4\u77e5\u8d1f\u8f7d\u548c\u4ea4\u4e92\u6548\u7387\uff0c\u5bf9\u53ef\u89c6\u5316\u5de5\u5177\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u901a\u8fc7\u771f\u4eba\u5b9e\u6d4b\u7684\u7814\u7a76\u7ed3\u679c\uff0c\u5c55\u793a\u4e86\u6240\u63d0\u51fa\u6280\u672f\u5982\u4f55\u6539\u5584\u63a2\u7d22\u6027\u548c\u5206\u6790\u6027\u4efb\u52a1\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e92\u52a8\u6280\u672f\u6709\u6548\u4fc3\u8fdb\u4e86\u57ce\u5e02\u6570\u636e\u7684\u63a2\u7a76\u4e0e\u5206\u6790\uff0c\u964d\u4f4e\u4e86\u8ba4\u77e5\u8d1f\u62c5\uff0c\u5e76\u4e3a\u672a\u6765\u4ea4\u4e92\u7cfb\u7edf\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2510.18137", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.18137", "abs": "https://arxiv.org/abs/2510.18137", "authors": ["Hrishikesh Sathyanarayan", "Victor Vantilborgh", "Ian Abraham"], "title": "Quality Over Quantity: Curating Contact-Based Robot Datasets Improves Learning", "comment": null, "summary": "In this paper, we investigate the utility of datasets and whether more data\nor the 'right' data is advantageous for robot learning. In particular, we are\ninterested on quantifying the utility of contact-based data as contact holds\nsignificant information for robot learning. Our approach derives a\ncontact-aware objective function for learning object dynamics and shape from\npose and contact data. We show that the contact-aware Fisher-information metric\ncan be used to rank and curate contact-data based on how informative data is\nfor learning. In addition, we find that selecting a reduced dataset based on\nthis ranking improves the learning task while also making learning a\ndeterministic process. Interestingly, our results show that more data is not\nnecessarily advantageous, and rather, less but informative data can accelerate\nlearning, especially depending on the contact interactions. Last, we show how\nour metric can be used to provide initial guidance on data curation for\ncontact-based robot learning.", "AI": {"tldr": "\u7814\u7a76\u63a5\u89e6\u6570\u636e\u5728\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u7684\u6548\u7528\uff0c\u53d1\u73b0\u66f4\u5c11\u4f46\u4fe1\u606f\u4e30\u5bcc\u7684\u6570\u636e\u80fd\u52a0\u901f\u5b66\u4e60\u3002", "motivation": "\u7814\u7a76\u6570\u636e\u96c6\u7684\u6548\u7528\uff0c\u8bc4\u4f30\u66f4\u591a\u6570\u636e\u4e0e\u2018\u6b63\u786e\u2019\u6570\u636e\u5bf9\u673a\u5668\u4eba\u5b66\u4e60\u7684\u4f18\u52bf\u3002", "method": "\u5f00\u53d1\u63a5\u89e6\u611f\u77e5\u7684\u76ee\u6807\u51fd\u6570\uff0c\u5e76\u5229\u7528Fisher\u4fe1\u606f\u5ea6\u91cf\u5bf9\u63a5\u89e6\u6570\u636e\u8fdb\u884c\u6392\u540d\u548c\u7b5b\u9009\u3002", "result": "\u63d0\u51fa\u57fa\u4e8e\u63a5\u89e6\u7684\u76ee\u6807\u51fd\u6570\u7528\u4e8e\u5b66\u4e60\u7269\u4f53\u52a8\u6001\u548c\u5f62\u72b6\uff0c\u5e76\u5f00\u53d1\u4e86\u63a5\u89e6\u611f\u77e5\u7684Fisher\u4fe1\u606f\u5ea6\u91cf\u3002", "conclusion": "\u63a5\u89e6\u6570\u636e\u7684\u7cbe\u7b80\u9009\u62e9\u53ef\u4ee5\u6539\u5584\u5b66\u4e60\u4efb\u52a1\uff0c\u540c\u65f6\u4f7f\u5b66\u4e60\u8fc7\u7a0b\u66f4\u5177\u786e\u5b9a\u6027\u3002"}}
{"id": "2510.18296", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.18296", "abs": "https://arxiv.org/abs/2510.18296", "authors": ["Aayushi Dangol", "Smriti Kotiyal", "Robert Wolfe", "Alex J. Bowers", "Antonio Vigil", "Jason Yip", "Julie A. Kientz", "Suleman Shahid", "Tom Yeh", "Vincent Cho", "Katie Davis"], "title": "Relief or displacement? How teachers are negotiating generative AI's role in their professional practice", "comment": null, "summary": "As generative AI (genAI) rapidly enters classrooms, accompanied by\ndistrict-level policy rollouts and industry-led teacher trainings, it is\nimportant to rethink the canonical ``adopt and train'' playbook. Decades of\neducational technology research show that tools promising personalization and\naccess often deepen inequities due to uneven resources, training, and\ninstitutional support. Against this backdrop, we conducted semi-structured\ninterviews with 22 teachers from a large U.S. school district that was an early\nadopter of genAI. Our findings reveal the motivations driving adoption, the\nfactors underlying resistance, and the boundaries teachers negotiate to align\ngenAI use with their values. We further contribute by unpacking the\nsociotechnical dynamics -- including district policies, professional norms, and\nrelational commitments -- that shape how teachers navigate the promises and\nrisks of these tools.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u751f\u6210\u578b\u4eba\u5de5\u667a\u80fd\u5728\u6559\u80b2\u4e2d\u7684\u5e94\u7528\u9700\u8981\u91cd\u65b0\u601d\u8003\u4f20\u7edf\u7684\u91c7\u7528\u4e0e\u57f9\u8bad\u6a21\u5f0f\uff0c\u7279\u522b\u662f\u9700\u8981\u5173\u6ce8\u5176\u5728\u6559\u5e08\u57f9\u8bad\u548c\u8d44\u6e90\u652f\u6301\u4e0a\u7684\u4e0d\u5e73\u7b49\u3002", "motivation": "\u63a2\u8ba8\u751f\u6210\u578b\u4eba\u5de5\u667a\u80fd\u5728\u6559\u80b2\u4e2d\u7684\u5e94\u7528\uff0c\u4e86\u89e3\u6559\u5e08\u7684\u91c7\u7528\u52a8\u673a\u548c\u62b5\u5236\u56e0\u7d20\u3002", "method": "\u5bf922\u4f4d\u6765\u81ea\u65e9\u671f\u91c7\u7528\u751f\u6210\u578b\u4eba\u5de5\u667a\u80fd\u7684\u5927\u578b\u7f8e\u56fd\u5b66\u533a\u7684\u6559\u5e08\u8fdb\u884c\u4e86\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u6559\u5e08\u5728\u91c7\u7528\u751f\u6210\u578b\u4eba\u5de5\u667a\u80fd\u65f6\u7684\u52a8\u673a\u3001\u62b5\u5236\u53ca\u5176\u5728\u793e\u4f1a\u6280\u672f\u52a8\u6001\u4e2d\u7684\u4f5c\u7528\u3002", "conclusion": "\u6559\u5e08\u5728\u4f7f\u7528\u751f\u6210\u578b\u4eba\u5de5\u667a\u80fd\u65f6\u9762\u4e34\u7684\u52a8\u673a\u3001\u62b5\u5236\u56e0\u7d20\u548c\u793e\u4f1a\u6280\u672f\u52a8\u6001\u5f71\u54cd\u4e86\u5176\u4ef7\u503c\u89c2\u7684\u5bf9\u9f50\u3002"}}
{"id": "2510.18316", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18316", "abs": "https://arxiv.org/abs/2510.18316", "authors": ["Chengshu Li", "Mengdi Xu", "Arpit Bahety", "Hang Yin", "Yunfan Jiang", "Huang Huang", "Josiah Wong", "Sujay Garlanka", "Cem Gokmen", "Ruohan Zhang", "Weiyu Liu", "Jiajun Wu", "Roberto Mart\u00edn-Mart\u00edn", "Li Fei-Fei"], "title": "MoMaGen: Generating Demonstrations under Soft and Hard Constraints for Multi-Step Bimanual Mobile Manipulation", "comment": "Project website: momagen.github.io. The first four authors contribute\n  equally", "summary": "Imitation learning from large-scale, diverse human demonstrations has proven\neffective for training robots, but collecting such data is costly and\ntime-consuming. This challenge is amplified for multi-step bimanual mobile\nmanipulation, where humans must teleoperate both a mobile base and two\nhigh-degree-of-freedom arms. Prior automated data generation frameworks have\naddressed static bimanual manipulation by augmenting a few human demonstrations\nin simulation, but they fall short for mobile settings due to two key\nchallenges: (1) determining base placement to ensure reachability, and (2)\npositioning the camera to provide sufficient visibility for visuomotor\npolicies. To address these issues, we introduce MoMaGen, which formulates data\ngeneration as a constrained optimization problem that enforces hard constraints\n(e.g., reachability) while balancing soft constraints (e.g., visibility during\nnavigation). This formulation generalizes prior approaches and provides a\nprincipled foundation for future methods. We evaluate MoMaGen on four\nmulti-step bimanual mobile manipulation tasks and show that it generates\nsignificantly more diverse datasets than existing methods. Leveraging this\ndiversity, MoMaGen can train successful imitation learning policies from a\nsingle source demonstration, and these policies can be fine-tuned with as few\nas 40 real-world demonstrations to achieve deployment on physical robotic\nhardware. More details are available at our project page: momagen.github.io.", "AI": {"tldr": "MoMaGen\u901a\u8fc7\u7ea6\u675f\u4f18\u5316\u65b9\u6cd5\u751f\u6210\u591a\u6837\u5316\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u7684\u6a21\u4eff\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u636e\u751f\u6210\u6846\u67b6\u5728\u79fb\u52a8\u53cc\u624b\u64cd\u4f5c\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u6709\u6548\u6536\u96c6\u8bad\u7ec3\u6570\u636e\u3002", "method": "\u5c06\u6570\u636e\u751f\u6210\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u4e00\u4e2a\u5177\u6709\u786c\u7ea6\u675f\uff08\u5982\u53ef\u8fbe\u6027\uff09\u548c\u8f6f\u7ea6\u675f\uff08\u5982\u5bfc\u822a\u8fc7\u7a0b\u4e2d\u7684\u53ef\u89c6\u6027\uff09\u7684\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u3002", "result": "MoMaGen\u5728\u56db\u4e2a\u591a\u6b65\u9aa4\u7684\u53cc\u624b\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u4e0a\u8868\u73b0\u5353\u8d8a\uff0c\u751f\u6210\u7684\u6570\u636e\u96c6\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u591a\u6837\u3002", "conclusion": "MoMaGen\u80fd\u591f\u751f\u6210\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u5177\u591a\u6837\u6027\u7684\u6570\u636e\u96c6\uff0c\u5e76\u6210\u529f\u8bad\u7ec3\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\uff0c\u66f4\u5177\u53ef\u90e8\u7f72\u6027\u3002"}}
{"id": "2510.18311", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.18311", "abs": "https://arxiv.org/abs/2510.18311", "authors": ["Shadmaan Hye", "Matthew P. LeGendre", "Katherine E. Isaacs"], "title": "Reimagining Disassembly Interfaces with Visualization: Combining Instruction Tracing and Control Flow with DisViz", "comment": "14 pages, 9 figures, accepted in TVCG", "summary": "In applications where efficiency is critical, developers may examine their\ncompiled binaries, seeking to understand how the compiler transformed their\nsource code and what performance implications that transformation may have.\nThis analysis is challenging due to the vast number of disassembled binary\ninstructions and the many-to-many mappings between them and the source code.\nThese problems are exacerbated as source code size increases, giving the\ncompiler more freedom to map and disperse binary instructions across the\ndisassembly space. Interfaces for disassembly typically display instructions as\nan unstructured listing or sacrifice the order of execution. We design a new\nvisual interface for disassembly code that combines execution order with\ncontrol flow structure, enabling analysts to both trace through code and\nidentify familiar aspects of the computation. Central to our approach is a\nnovel layout of instructions grouped into basic blocks that displays a looping\nstructure in an intuitive way. We add to this disassembly representation a\nunique block-based mini-map that leverages our layout and shows context across\nthousands of disassembly instructions. Finally, we embed our disassembly\nvisualization in a web-based tool, DisViz, which adds dynamic linking with\nsource code across the entire application. DizViz was developed in\ncollaboration with program analysis experts following design study methodology\nand was validated through evaluation sessions with ten participants from four\ninstitutions. Participants successfully completed the evaluation tasks,\nhypothesized about compiler optimizations, and noted the utility of our new\ndisassembly view. Our evaluation suggests that our new integrated view helps\napplication developers in understanding and navigating disassembly code.", "AI": {"tldr": "\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u53cd\u6c47\u7f16\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u540d\u4e3aDisViz\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u7406\u89e3\u548c\u5206\u6790\u53cd\u6c47\u7f16\u4ee3\u7801\u3002", "motivation": "\u5728\u6548\u7387\u81f3\u5173\u91cd\u8981\u7684\u5e94\u7528\u4e2d\uff0c\u5f00\u53d1\u8005\u9700\u8981\u5206\u6790\u7f16\u8bd1\u540e\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u4ee5\u7406\u89e3\u7f16\u8bd1\u5668\u5bf9\u6e90\u4ee3\u7801\u7684\u53d8\u6362\u53ca\u5176\u6027\u80fd\u5f71\u54cd\uff0c\u4f46\u7531\u4e8e\u53cd\u6c47\u7f16\u6307\u4ee4\u6570\u91cf\u5e9e\u5927\u53ca\u5176\u4e0e\u6e90\u4ee3\u7801\u7684\u590d\u6742\u6620\u5c04\u5173\u7cfb\uff0c\u8fd9\u4e00\u5206\u6790\u4efb\u52a1\u53d8\u5f97\u6781\u5177\u6311\u6218\u6027\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u7814\u7a76\u65b9\u6cd5\u8bba\u4e0e\u7a0b\u5e8f\u5206\u6790\u4e13\u5bb6\u5408\u4f5c\u5f00\u53d1DisViz\uff0c\u8fdb\u884c\u7684\u8bc4\u4f30\u4f1a\u8bdd\u5f97\u5230\u4e86\u5341\u540d\u6765\u81ea\u56db\u4e2a\u673a\u6784\u7684\u53c2\u4e0e\u8005\u7684\u53cd\u9988\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53cd\u6c47\u7f16\u4ee3\u7801\u53ef\u89c6\u5316\u754c\u9762DisViz\uff0c\u65e8\u5728\u5e2e\u52a9\u5f00\u53d1\u8005\u66f4\u597d\u5730\u7406\u89e3\u548c\u5206\u6790\u53cd\u6c47\u7f16\u4ee3\u7801\u3002\u8be5\u754c\u9762\u7ed3\u5408\u4e86\u6267\u884c\u987a\u5e8f\u548c\u63a7\u5236\u6d41\u7ed3\u6784\uff0c\u91c7\u7528\u76f4\u89c2\u7684\u57fa\u672c\u5757\u5e03\u5c40\uff0c\u4ee5\u5c55\u793a\u5faa\u73af\u7ed3\u6784\uff0c\u540c\u65f6\u5f15\u5165\u4e00\u4e2a\u57fa\u4e8e\u5757\u7684\u8ff7\u4f60\u5730\u56fe\uff0c\u663e\u793a\u6570\u5343\u6761\u53cd\u6c47\u7f16\u6307\u4ee4\u7684\u4e0a\u4e0b\u6587\u3002", "conclusion": "\u6211\u4eec\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u65b0\u7684\u96c6\u6210\u89c6\u56fe\u6709\u52a9\u4e8e\u5e94\u7528\u7a0b\u5e8f\u5f00\u53d1\u8005\u7406\u89e3\u548c\u5bfc\u822a\u53cd\u6c47\u7f16\u4ee3\u7801\u3002"}}
{"id": "2510.18337", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.18337", "abs": "https://arxiv.org/abs/2510.18337", "authors": ["Wenhui Huang", "Changhe Chen", "Han Qi", "Chen Lv", "Yilun Du", "Heng Yang"], "title": "MoTVLA: A Vision-Language-Action Model with Unified Fast-Slow Reasoning", "comment": null, "summary": "Integrating visual-language instructions into visuomotor policies is gaining\nmomentum in robot learning for enhancing open-world generalization. Despite\npromising advances, existing approaches face two challenges: limited language\nsteerability when no generated reasoning is used as a condition, or significant\ninference latency when reasoning is incorporated.In this work, we introduce\nMoTVLA, a mixture-of-transformers (MoT)-based vision-language-action (VLA)\nmodel that integrates fast-slow unified reasoning with behavior policy\nlearning. MoTVLA preserves the general intelligence of pre-trained VLMs\n(serving as the generalist) for tasks such as perception, scene understanding,\nand semantic planning, while incorporating a domain expert, a second\ntransformer that shares knowledge with the pretrained VLM, to generate\ndomain-specific fast reasoning (e.g., robot motion decomposition), thereby\nimproving policy execution efficiency. By conditioning the action expert on\ndecomposed motion instructions, MoTVLA can learn diverse behaviors and\nsubstantially improve language steerability. Extensive evaluations across\nnatural language processing benchmarks, robotic simulation environments, and\nreal-world experiments confirm the superiority of MoTVLA in both fast-slow\nreasoning and manipulation task performance.", "AI": {"tldr": "MoTVLA\u6a21\u578b\u901a\u8fc7\u7ed3\u5408\u5feb\u901f\u548c\u6162\u901f\u63a8\u7406\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5728\u5904\u7406\u89c6\u89c9-\u8bed\u8a00\u6307\u4ee4\u65f6\u7684\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u63d0\u5347\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u89c6\u89c9\u8bed\u8a00\u6307\u4ee4\u7684\u6574\u5408\uff0c\u4ee5\u589e\u5f3a\u5f00\u653e\u4e16\u754c\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u63a8\u7406\u65f6\u7684\u8bed\u8a00\u53ef\u5f15\u5bfc\u6027\u548c\u63a8\u7406\u5ef6\u8fdf\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u6df7\u5408\u8f6c\u6362\u5668\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b (MoTVLA)", "result": "MoTVLA\u901a\u8fc7\u6574\u5408\u5feb\u901f\u4e0e\u6162\u901f\u7684\u7edf\u4e00\u63a8\u7406\uff0c\u63d0\u9ad8\u4e86\u653f\u7b56\u6267\u884c\u6548\u7387\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u8bed\u8a00\u7684\u53ef\u64cd\u4f5c\u6027\u3002", "conclusion": "\u5927\u91cf\u8bc4\u4f30\u8868\u660e\uff0cMoTVLA\u5728\u5feb\u901f-\u6162\u901f\u63a8\u7406\u548c\u64cd\u63a7\u4efb\u52a1\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.18385", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.18385", "abs": "https://arxiv.org/abs/2510.18385", "authors": ["Mohd Ruhul Ameen", "Akif Islam", "Momen Khandokar Ope"], "title": "Khelte Khelte Shikhi: A Proposed HCI Framework for Gamified Interactive Learning with Minecraft in Bangladeshi Education Systems", "comment": "6 pages, 6 tables, 6 figures, submitted to the 28th International\n  Conference on Computer and Information Technology (ICCIT 2025)", "summary": "Game-based learning shows real promise for engaging students in well-funded\nschools, but what about everyone else? We propose a practical framework for\nimplementing Minecraft Education Edition in Bangladesh's 130,000 schools where\n55 percent lack reliable internet, rural areas experience 12-16 hour daily\npower availability, only 8 percent of rural schools have computer access, and\nstudent-teacher ratios reach 52:1. Our approach tackles these constraints\nhead-on with three deployment tiers: cloud-based multiplayer for urban schools\nwith stable infrastructure (15 percent), local area network solutions with\nsolar power for semi-urban contexts (30 percent), and offline turn-based modes\nusing refurbished hardware for rural settings (55 percent). We provide eight\npre-built curriculum-aligned worlds with complete Bangla localization covering\ntopics from Lalbagh Fort reconstruction to monsoon flood simulation. The\ninterface accommodates first-time users through progressive complexity,\nculturally familiar metaphors using local farming and architecture, and\naccessibility features including keyboard-only controls and 200 percent text\nscaling. Teacher training spans 48 hours across digital literacy, pedagogical\nintegration, and content creation. We detail evaluation protocols with specific\nbenchmarks: 15 percent learning gains, 70 percent transfer task mastery, System\nUsability Scale scores above 70, and sub-two-dollar cost per student-hour. This\nframework has not been empirically validated; it synthesizes game-based\nlearning theory, HCI principles, and contextual analysis to provide\nimplementable specifications for pilot testing in resource-constrained\nsettings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5728\u5b5f\u52a0\u62c9\u56fd\u5b9e\u65bdMinecraft\u6559\u80b2\u7248\u7684\u6846\u67b6\uff0c\u9488\u5bf9\u4e0d\u540c\u5b66\u6821\u73af\u5883\u8bbe\u8ba1\u4e86\u4e09\u79cd\u65b9\u6848\uff0c\u5e76\u5305\u542b\u672c\u5730\u5316\u8bfe\u7a0b", "motivation": "\u5728\u7f3a\u4e4f\u53ef\u9760\u4e92\u8054\u7f51\u548c\u8ba1\u7b97\u673a\u8d44\u6e90\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u5b5f\u52a0\u62c9\u56fd\u5b66\u6821\u7684\u5b66\u751f\u53c2\u4e0e\u5ea6", "method": "\u63d0\u51fa\u4e00\u5957\u5b9e\u65bdMinecraft\u6559\u80b2\u7248\u7684\u6846\u67b6", "result": "\u8bbe\u8ba1\u4e86\u4e09\u79cd\u9488\u5bf9\u4e0d\u540c\u73af\u5883\u7684\u90e8\u7f72\u65b9\u6848\uff0c\u5e76\u63d0\u4f9b\u4e86\u672c\u5730\u5316\u7684\u8bfe\u7a0b\u5185\u5bb9", "conclusion": "\u8be5\u6846\u67b6\u672a\u7ecf\u8fc7\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u4f46\u57fa\u4e8e\u6e38\u620f\u5b66\u4e60\u7406\u8bba\u3001HCI\u539f\u5219\u548c\u60c5\u5883\u5206\u6790\uff0c\u5c06\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u63d0\u4f9b\u53ef\u5b9e\u65bd\u7684\u89c4\u8303\u3002"}}
{"id": "2510.18347", "categories": ["cs.RO", "cs.SY", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.18347", "abs": "https://arxiv.org/abs/2510.18347", "authors": ["Muhammad Hanif", "Reiji Terunuma", "Takumi Sumino", "Kelvin Cheng", "Takeshi Hatanaka"], "title": "Coverage-Recon: Coordinated Multi-Drone Image Sampling with Online Map Feedback", "comment": "Submitted to IEEE Transactions on Control Systems Technology (under\n  review). Project page: https://htnk-lab.github.io/coverage-recon/", "summary": "This article addresses collaborative 3D map reconstruction using multiple\ndrones. Achieving high-quality reconstruction requires capturing images of\nkeypoints within the target scene from diverse viewing angles, and coverage\ncontrol offers an effective framework to meet this requirement. Meanwhile,\nrecent advances in real-time 3D reconstruction algorithms make it possible to\nrender an evolving map during flight, enabling immediate feedback to guide\ndrone motion. Building on this, we present Coverage-Recon, a novel coordinated\nimage sampling algorithm that integrates online map feedback to improve\nreconstruction quality on-the-fly. In Coverage-Recon, the coordinated motion of\ndrones is governed by a Quadratic Programming (QP)-based angle-aware coverage\ncontroller, which ensures multi-viewpoint image capture while enforcing safety\nconstraints. The captured images are processed in real time by the NeuralRecon\nalgorithm to generate an evolving 3D mesh. Mesh changes across the scene are\ninterpreted as indicators of reconstruction uncertainty and serve as feedback\nto update the importance index of the coverage control as the map evolves. The\neffectiveness of Coverage-Recon is validated through simulation and\nexperiments, demonstrating both qualitatively and quantitatively that\nincorporating online map feedback yields more complete and accurate 3D\nreconstructions than conventional methods. Project page:\nhttps://htnk-lab.github.io/coverage-recon/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoverage-Recon\u7684\u65b0\u7b97\u6cd5\uff0c\u901a\u8fc7\u591a\u67b6\u65e0\u4eba\u673a\u534f\u4f5c\u548c\u5728\u7ebf\u5730\u56fe\u53cd\u9988\uff0c\u63d0\u9ad8\u4e863D\u5730\u56fe\u91cd\u5efa\u7684\u8d28\u91cf\u3002", "motivation": "\u591a\u67b6\u65e0\u4eba\u673a\u8fdb\u884c\u534f\u4f5c3D\u5730\u56fe\u91cd\u5efa\uff0c\u9700\u8981\u4ece\u4e0d\u540c\u89d2\u5ea6\u62cd\u6444\u5173\u952e\u70b9\u56fe\u50cf\uff0c\u8986\u76d6\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6846\u67b6\u6765\u6ee1\u8db3\u8fd9\u4e00\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u57fa\u4e8e\u4e8c\u6b21\u89c4\u5212\u7684\u89d2\u5ea6\u611f\u77e5\u8986\u76d6\u63a7\u5236\u5668\u534f\u8c03\u65e0\u4eba\u673a\u7684\u8fd0\u52a8\uff0c\u5e76\u4f7f\u7528NeuralRecon\u7b97\u6cd5\u5b9e\u65f6\u5904\u7406\u6355\u83b7\u7684\u56fe\u50cf\u751f\u62103D\u7f51\u683c\u3002", "result": "\u6a21\u62df\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86Coverage-Recon\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u7ed3\u5408\u5728\u7ebf\u5730\u56fe\u53cd\u9988\u53ef\u4ee5\u5b9e\u73b0\u66f4\u5b8c\u6574\u548c\u51c6\u786e\u76843D\u91cd\u5efa\u3002", "conclusion": "Coverage-Recon\u901a\u8fc7\u5728\u7ebf\u5730\u56fe\u53cd\u9988\uff0c\u663e\u8457\u63d0\u9ad8\u4e863D\u91cd\u5efa\u7684\u5b8c\u6574\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2510.18625", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.18625", "abs": "https://arxiv.org/abs/2510.18625", "authors": ["Eric DeDeMarbre", "Jay Henderson", "J. Felipe Gonzalez", "Rob Teather"], "title": "Effects of Virtual Controller Representation and Virtuality on Selection Performance in Extended Reality", "comment": null, "summary": "We present an experiment exploring how the controller's virtual\nrepresentation impacts target acquisition performance across MR and VR\ncontexts. Participants performed selection tasks comparing four visual\nconfigurations: a virtual controller, a virtual hand, both the controller and\nthe hand, and neither representation. We found performance comparable between\nVR and MR, and switching between them did not impact the user's ability to\nperform basic tasks. Controller representations mimicking reality enhanced\nperformance across both modes. However, users perceived performance differently\nin MR, indicating the need for unique MR design considerations, particularly\nregarding spatial awareness.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u63a7\u5236\u5668\u865a\u62df\u8868\u73b0\u5bf9MR\u548cVR\u4e0a\u4e0b\u6587\u76ee\u6807\u83b7\u53d6\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u901a\u8fc7\u63a7\u5236\u5668\u8868\u73b0\u7684\u73b0\u5b9e\u6a21\u4eff\u589e\u5f3a\u4e86\u4e24\u8005\u6a21\u5f0f\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u63a2\u7d22\u63a7\u5236\u5668\u7684\u865a\u62df\u8868\u73b0\u5982\u4f55\u5f71\u54cdMR\u548cVR\u73af\u5883\u4e2d\u7684\u76ee\u6807\u83b7\u53d6\u6027\u80fd", "method": "\u5b9e\u9a8c\u6bd4\u8f83\u56db\u79cd\u89c6\u89c9\u914d\u7f6e\u4e0b\u7684\u76ee\u6807\u83b7\u53d6\u6027\u80fd", "result": "\u53d1\u73b0VR\u548cMR\u4e4b\u95f4\u7684\u6027\u80fd\u76f8\u5f53\uff0c\u63a7\u5236\u5668\u7684\u73b0\u5b9e\u6a21\u4eff\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u800c\u7528\u6237\u5728MR\u4e2d\u611f\u77e5\u6027\u80fd\u6709\u6240\u4e0d\u540c", "conclusion": "\u5728MR\u8bbe\u8ba1\u4e2d\u9700\u8981\u8003\u8651\u7528\u6237\u7684\u7a7a\u95f4\u610f\u8bc6\uff0c\u4ee5\u8fdb\u4e00\u6b65\u4f18\u5316\u4f53\u9a8c\u3002"}}
{"id": "2510.18348", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18348", "abs": "https://arxiv.org/abs/2510.18348", "authors": ["Alexandros Ntagkas", "Chairi Kiourt", "Konstantinos Chatzilygeroudis"], "title": "PGTT: Phase-Guided Terrain Traversal for Perceptive Legged Locomotion", "comment": "9 pages, 9 figures, 2 tables", "summary": "State-of-the-art perceptive Reinforcement Learning controllers for legged\nrobots either (i) impose oscillator or IK-based gait priors that constrain the\naction space, add bias to the policy optimization and reduce adaptability\nacross robot morphologies, or (ii) operate \"blind\", which struggle to\nanticipate hind-leg terrain, and are brittle to noise. In this paper, we\npropose Phase-Guided Terrain Traversal (PGTT), a perception-aware deep-RL\napproach that overcomes these limitations by enforcing gait structure purely\nthrough reward shaping, thereby reducing inductive bias in policy learning\ncompared to oscillator/IK-conditioned action priors. PGTT encodes per-leg phase\nas a cubic Hermite spline that adapts swing height to local heightmap\nstatistics and adds a swing-phase contact penalty, while the policy acts\ndirectly in joint space supporting morphology-agnostic deployment. Trained in\nMuJoCo (MJX) on procedurally generated stair-like terrains with curriculum and\ndomain randomization, PGTT achieves the highest success under push disturbances\n(median +7.5% vs. the next best method) and on discrete obstacles (+9%), with\ncomparable velocity tracking, and converging to an effective policy roughly 2x\nfaster than strong end-to-end baselines. We validate PGTT on a Unitree Go2\nusing a real-time LiDAR elevation-to-heightmap pipeline, and we report\npreliminary results on ANYmal-C obtained with the same hyperparameters. These\nfindings indicate that terrain-adaptive, phase-guided reward shaping is a\nsimple and general mechanism for robust perceptive locomotion across platforms.", "AI": {"tldr": "PGTT\u662f\u4e00\u79cd\u65b0\u63d0\u51fa\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u5956\u52b1\u5851\u9020\u6765\u5b9e\u73b0\u6b65\u6001\u7ed3\u6784\uff0c\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u589e\u5f3a\u817f\u90e8\u673a\u5668\u4eba\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u817f\u90e8\u673a\u5668\u4eba\u63a7\u5236\u65b9\u6cd5\u5b58\u5728\u884c\u52a8\u7a7a\u95f4\u53d7\u9650\u3001\u5bf9\u566a\u58f0\u654f\u611f\u7b49\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u9c81\u68d2\u7684\u5e94\u5bf9\u7b56\u7565\u3002", "method": "PGTT\u901a\u8fc7\u5956\u52b1\u5851\u9020\u5f15\u5165\u6b65\u6001\u7ed3\u6784\uff0c\u4f7f\u7528\u7acb\u65b9Hermite\u6837\u6761\u7f16\u7801\u8fd0\u52a8\u9636\u6bb5\uff0c\u9002\u5e94\u5f53\u5730\u9ad8\u5ea6\u56fe\u7edf\u8ba1\uff0c\u652f\u6301\u5f62\u6001\u65e0\u5173\u7684\u7b56\u7565\u5b66\u4e60\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5PGTT\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u817f\u90e8\u673a\u5668\u4eba\u63a7\u5236\u5668\u7684\u7f3a\u9677\uff0c\u6539\u5584\u9002\u5e94\u6027\u4e0e\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\uff0cPGTT\u5728\u4e0d\u540c\u5730\u5f62\u4e0b\u8868\u73b0\u4f18\u8d8a\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u5c55\u73b0\u4e86\u5176\u5728\u611f\u77e5\u6b65\u6001\u63a7\u5236\u65b9\u9762\u7684\u524d\u666f\u3002"}}
{"id": "2510.18371", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.18371", "abs": "https://arxiv.org/abs/2510.18371", "authors": ["Mingxin Li", "Haibo Hu", "Jinghuai Deng", "Yuchen Xi", "Xinhong Chen", "Jianping Wang"], "title": "MMRHP: A Miniature Mixed-Reality HIL Platform for Auditable Closed-Loop Evaluation", "comment": null, "summary": "Validation of autonomous driving systems requires a trade-off between test\nfidelity, cost, and scalability. While miniaturized hardware-in-the-loop (HIL)\nplatforms have emerged as a promising solution, a systematic framework\nsupporting rigorous quantitative analysis is generally lacking, limiting their\nvalue as scientific evaluation tools. To address this challenge, we propose\nMMRHP, a miniature mixed-reality HIL platform that elevates miniaturized\ntesting from functional demonstration to rigorous, reproducible quantitative\nanalysis. The core contributions are threefold. First, we propose a systematic\nthree-phase testing process oriented toward the Safety of the Intended\nFunctionality(SOTIF)standard, providing actionable guidance for identifying the\nperformance limits and triggering conditions of otherwise correctly functioning\nsystems. Second, we design and implement a HIL platform centered around a\nunified spatiotemporal measurement core to support this process, ensuring\nconsistent and traceable quantification of physical motion and system timing.\nFinally, we demonstrate the effectiveness of this solution through\ncomprehensive experiments. The platform itself was first validated, achieving a\nspatial accuracy of 10.27 mm RMSE and a stable closed-loop latency baseline of\napproximately 45 ms. Subsequently, an in-depth Autoware case study leveraged\nthis validated platform to quantify its performance baseline and identify a\ncritical performance cliff at an injected latency of 40 ms. This work shows\nthat a structured process, combined with a platform offering a unified\nspatio-temporal benchmark, enables reproducible, interpretable, and\nquantitative closed-loop evaluation of autonomous driving systems.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMMRHP\u7684\u8ff7\u4f60\u6df7\u5408\u73b0\u5b9e\u786c\u4ef6\u5728\u73af(HIL)\u5e73\u53f0\uff0c\u65e8\u5728\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u9a8c\u8bc1\u8fc7\u7a0b\uff0c\u901a\u8fc7\u7cfb\u7edf\u7684\u65b9\u6cd5\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u53ef\u91cd\u590d\u3001\u53ef\u91cf\u5316\u7684\u6d4b\u8bd5\uff0c\u7b26\u5408\u5b89\u5168\u610f\u56fe\u529f\u80fd(SOTIF)\u6807\u51c6\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u9a8c\u8bc1\u9700\u8981\u5728\u6d4b\u8bd5\u903c\u771f\u6027\u3001\u6210\u672c\u548c\u53ef\u6269\u5c55\u6027\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\uff0c\u73b0\u6709\u7684\u8ff7\u4f60HIL\u5e73\u53f0\u7f3a\u4e4f\u7cfb\u7edf\u7684\u5b9a\u91cf\u5206\u6790\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u7684\u4e09\u9636\u6bb5\u6d4b\u8bd5\u8fc7\u7a0b\uff0c\u5e76\u8bbe\u8ba1\u548c\u5b9e\u73b0\u4e86\u4e00\u4e2a\u4ee5\u7edf\u4e00\u65f6\u7a7a\u6d4b\u91cf\u6838\u5fc3\u4e3a\u4e2d\u5fc3\u7684HIL\u5e73\u53f0\uff0c\u4ee5\u652f\u6301\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u786e\u4fdd\u7269\u7406\u8fd0\u52a8\u548c\u7cfb\u7edf\u65f6\u5e8f\u7684\u91cf\u5316\u4e00\u81f4\u6027\u548c\u53ef\u8ffd\u6eaf\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u5e73\u53f0\u7684\u7a7a\u95f4\u7cbe\u5ea6\u8fbe\u523010.27 mm RMSE\uff0c\u95ed\u73af\u5ef6\u8fdf\u57fa\u7ebf\u7ea6\u4e3a45 ms\uff0c\u5e76\u4e14\u5728Autoware\u6848\u4f8b\u7814\u7a76\u4e2d\u786e\u8ba4\u4e86\u572840 ms\u6ce8\u5165\u5ef6\u8fdf\u4e0b\u7684\u5173\u952e\u6027\u80fd\u60ac\u5d16\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u6d41\u7a0b\u548c\u7edf\u4e00\u65f6\u7a7a\u57fa\u51c6\u7684\u5e73\u53f0\uff0c\u80fd\u591f\u5b9e\u73b0\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u53ef\u91cd\u590d\u3001\u53ef\u89e3\u91ca\u548c\u5b9a\u91cf\u7684\u95ed\u73af\u8bc4\u4f30\u3002"}}
{"id": "2510.18373", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.18373", "abs": "https://arxiv.org/abs/2510.18373", "authors": ["Wanchen Li", "Kahina Chalabi", "Sabbah Maxime", "Thomas Bousquet", "Robin Passama", "Sofiane Ramdani", "Andrea Cherubini", "Vincent Bonnet"], "title": "Biomechanically consistent real-time action recognition for human-robot interaction", "comment": null, "summary": "This paper presents a novel framework for real-time human action recognition\nin industrial contexts, using standard 2D cameras. We introduce a complete\npipeline for robust and real-time estimation of human joint kinematics, input\nto a temporally smoothed Transformer-based network, for action recognition. We\nrely on a new dataset including 11 subjects performing various actions, to\nevaluate our approach. Unlike most of the literature that relies on joint\ncenter positions (JCP) and is offline, ours uses biomechanical prior, eg. joint\nangles, for fast and robust real-time recognition. Besides, joint angles make\nthe proposed method agnostic to sensor and subject poses as well as to\nanthropometric differences, and ensure robustness across environments and\nsubjects. Our proposed learning model outperforms the best baseline model,\nrunning also in real-time, along various metrics. It achieves 88% accuracy and\nshows great generalization ability, for subjects not facing the cameras.\nFinally, we demonstrate the robustness and usefulness of our technique, through\nan online interaction experiment, with a simulated robot controlled in\nreal-time via the recognized actions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5de5\u4e1a\u80cc\u666f\u4e0b\u7684\u5b9e\u65f6\u4eba\u7c7b\u52a8\u4f5c\u8bc6\u522b\uff0c\u5229\u7528\u6807\u51c62D\u76f8\u673a\u3002", "motivation": "\u63d0\u9ad8\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u4eba\u7c7b\u52a8\u4f5c\u8bc6\u522b\u7684\u7cbe\u786e\u6027\u548c\u5b9e\u65f6\u6027\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u4e00\u4e2a\u5305\u542b11\u4e2a\u53d7\u8bd5\u8005\u6267\u884c\u591a\u79cd\u52a8\u4f5c\u7684\u65b0\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\uff0c\u5229\u7528\u751f\u7269\u529b\u5b66\u5148\u9a8c\uff08\u5982\u5173\u8282\u89d2\u5ea6\uff09\u5b9e\u73b0\u5feb\u901f\u548c\u9c81\u68d2\u7684\u5b9e\u65f6\u8bc6\u522b\u3002", "result": "\u5728\u4e0d\u9762\u5411\u6444\u50cf\u673a\u7684\u5bf9\u8c61\u4e0a\uff0c\u6a21\u578b\u8fbe\u5230\u4e8688%\u7684\u51c6\u786e\u7387\u5e76\u5c55\u793a\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u6211\u4eec\u7684\u5b66\u4e60\u6a21\u578b\u5728\u5404\u79cd\u6307\u6807\u4e0a\u4f18\u4e8e\u6700\u4f73\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u5177\u5907\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7684\u73af\u5883\u548c\u5bf9\u8c61\u3002"}}
{"id": "2510.18402", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.18402", "abs": "https://arxiv.org/abs/2510.18402", "authors": ["Matthias Lorenzen", "Teodoro Alamo", "Martina Mammarella", "Fabrizio Dabbene"], "title": "MPC-based motion planning for non-holonomic systems in non-convex domains", "comment": "Preprint of ECC 2025 submission", "summary": "Motivated by the application of using model predictive control (MPC) for\nmotion planning of autonomous mobile robots, a form of output tracking MPC for\nnon-holonomic systems and with non-convex constraints is studied. Although the\nadvantages of using MPC for motion planning have been demonstrated in several\npapers, in most of the available fundamental literature on output tracking MPC\nit is assumed, often implicitly, that the model is holonomic and generally the\nstate or output constraints must be convex. Thus, in application-oriented\npublications, empirical results dominate and the topic of proving completeness,\nin particular under which assumptions the target is always reached, has\nreceived comparatively little attention. To address this gap, we present a\nnovel MPC formulation that guarantees convergence to the desired target under\nrealistic assumptions, which can be verified in relevant real-world scenarios.", "AI": {"tldr": "\u7814\u7a76\u4e86\u9002\u7528\u4e8e\u975e\u5b8c\u6574\u7cfb\u7edf\u7684MPC\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f62\u5f0f\uff0c\u786e\u4fdd\u5728\u771f\u5b9e\u6761\u4ef6\u4e0b\u6536\u655b\u5230\u76ee\u6807\u3002", "motivation": "\u63a2\u8ba8\u5c06\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5e94\u7528\u4e8e\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u7684\u5fc5\u8981\u6027\u3002", "method": "\u7814\u7a76\u4e86\u9002\u7528\u4e8e\u975e\u5b8c\u6574\u7cfb\u7edf\u548c\u975e\u51f8\u7ea6\u675f\u7684\u8f93\u51fa\u8ddf\u8e2aMPC\u5f62\u5f0f\u3002", "result": "\u5728\u73b0\u5b9e\u6761\u4ef6\u4e0b\uff0c\u4fdd\u8bc1\u4e86\u6240\u63d0\u51fa\u7684MPC\u5728\u8fd0\u52a8\u89c4\u5212\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684MPC\u5f62\u5f0f\uff0c\u786e\u4fdd\u5728\u73b0\u5b9e\u6761\u4ef6\u4e0b\u6536\u655b\u5230\u671f\u671b\u76ee\u6807\u3002"}}
{"id": "2510.18518", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.18518", "abs": "https://arxiv.org/abs/2510.18518", "authors": ["Fang Nan", "Hao Ma", "Qinghua Guan", "Josie Hughes", "Michael Muehlebach", "Marco Hutter"], "title": "Efficient Model-Based Reinforcement Learning for Robot Control via Online Learning", "comment": null, "summary": "We present an online model-based reinforcement learning algorithm suitable\nfor controlling complex robotic systems directly in the real world. Unlike\nprevailing sim-to-real pipelines that rely on extensive offline simulation and\nmodel-free policy optimization, our method builds a dynamics model from\nreal-time interaction data and performs policy updates guided by the learned\ndynamics model. This efficient model-based reinforcement learning scheme\nsignificantly reduces the number of samples to train control policies, enabling\ndirect training on real-world rollout data. This significantly reduces the\ninfluence of bias in the simulated data, and facilitates the search for\nhigh-performance control policies. We adopt online learning analysis to derive\nsublinear regret bounds under standard stochastic online optimization\nassumptions, providing formal guarantees on performance improvement as more\ninteraction data are collected. Experimental evaluations were performed on a\nhydraulic excavator arm and a soft robot arm, where the algorithm demonstrates\nstrong sample efficiency compared to model-free reinforcement learning methods,\nreaching comparable performance within hours. Robust adaptation to shifting\ndynamics was also observed when the payload condition was randomized. Our\napproach paves the way toward efficient and reliable on-robot learning for a\nbroad class of challenging control tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7ebf\u6a21\u578b\u57fa\u7840\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u5b9e\u65f6\u4e92\u52a8\u6570\u636e\u6784\u5efa\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u6837\u672c\u6548\u7387\uff0c\u5e76\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u76f4\u63a5\u8bad\u7ec3\u63a7\u5236\u7b56\u7565\u3002", "motivation": "\u4e3a\u4e86\u76f4\u63a5\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u63a7\u5236\u590d\u6742\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u89e3\u51b3\u73b0\u6709\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u74f6\u9888", "method": "\u5728\u7ebf\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5", "result": "\u5728\u6db2\u538b\u6316\u6398\u81c2\u548c\u8f6f\u4f53\u673a\u5668\u4eba\u81c2\u4e0a\u7684\u5b9e\u9a8c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6837\u672c\u6548\u7387\uff0c\u76f8\u6bd4\u4e8e\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u6570\u5c0f\u65f6\u5185\u8fbe\u5230\u53ef\u6bd4\u7684\u6027\u80fd", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u6548\u53ef\u9760\u7684\u673a\u5668\u4eba\u5b66\u4e60\u5f00\u8f9f\u4e86\u65b0\u7684\u8def\u5f84\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u590d\u6742\u7684\u63a7\u5236\u4efb\u52a1\u3002"}}
{"id": "2510.18546", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18546", "abs": "https://arxiv.org/abs/2510.18546", "authors": ["Zebin Yang", "Sunjian Zheng", "Tong Xie", "Tianshi Xu", "Bo Yu", "Fan Wang", "Jie Tang", "Shaoshan Liu", "Meng Li"], "title": "EfficientNav: Towards On-Device Object-Goal Navigation with Navigation Map Caching and Retrieval", "comment": "NeurIPS 2025", "summary": "Object-goal navigation (ObjNav) tasks an agent with navigating to the\nlocation of a specific object in an unseen environment. Embodied agents\nequipped with large language models (LLMs) and online constructed navigation\nmaps can perform ObjNav in a zero-shot manner. However, existing agents heavily\nrely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small\nLLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to\nlimited model capacity for understanding complex navigation maps, which\nprevents deploying ObjNav on local devices. At the same time, the long prompt\nintroduced by the navigation map description will cause high planning latency\non local devices. In this paper, we propose EfficientNav to enable on-device\nefficient LLM-based zero-shot ObjNav. To help the smaller LLMs better\nunderstand the environment, we propose semantics-aware memory retrieval to\nprune redundant information in navigation maps. To reduce planning latency, we\npropose discrete memory caching and attention-based memory clustering to\nefficiently save and re-use the KV cache. Extensive experimental results\ndemonstrate that EfficientNav achieves 11.1% improvement in success rate on\nHM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time\nlatency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our\ncode will be released soon.", "AI": {"tldr": "EfficientNav\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u8bb0\u5fc6\u7ba1\u7406\u6280\u672f\uff0c\u4f7f\u5c0f\u578bLLM\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u5bf9\u8c61\u76ee\u6807\u5bfc\u822a\uff0c\u5e76\u663e\u8457\u63d0\u5347\u6210\u529f\u7387\u53ca\u51cf\u5c11\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709\u4f9d\u8d56\u4e8e\u4e91\u7aef\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u5728\u8f6c\u5411\u5c0f\u578bLLM\u65f6\uff0c\u6210\u529f\u7387\u663e\u8457\u4e0b\u964d\uff0c\u4e14\u5bfc\u822a\u56fe\u7684\u957f\u63d0\u793a\u589e\u52a0\u4e86\u672c\u5730\u8bbe\u5907\u7684\u89c4\u5212\u5ef6\u8fdf\u3002", "method": "\u63d0\u51fa\u4e86\u8bed\u4e49\u611f\u77e5\u7684\u8bb0\u5fc6\u68c0\u7d22\u3001\u79bb\u6563\u8bb0\u5fc6\u7f13\u5b58\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u8bb0\u5fc6\u805a\u7c7b\uff0c\u4ee5\u63d0\u9ad8\u5c0f\u578bLLM\u5728\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5728HM3D\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660eEfficientNav\u7684\u6210\u529f\u7387\u63d0\u9ad8\u4e8611.1%\uff0c\u5b9e\u65f6\u5ef6\u8fdf\u51cf\u5c11\u4e866.7\u500d\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u51cf\u5c11\u4e864.7\u500d\u3002", "conclusion": "EfficientNav\u5728HM3D\u57fa\u51c6\u4e0a\u6bd4\u57fa\u4e8eGPT-4\u7684\u57fa\u7ebf\u53d6\u5f97\u4e8611.1%\u7684\u6210\u529f\u7387\u63d0\u5347\uff0c\u5e76\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u5b9e\u65f6\u5ef6\u8fdf\u548c\u7aef\u5230\u7aef\u5ef6\u8fdf\u51cf\u5c11\u3002"}}
{"id": "2510.18558", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.18558", "abs": "https://arxiv.org/abs/2510.18558", "authors": ["Yue Wang", "Lixian Zhang", "Yimin Zhu", "Yangguang Liu", "Xuwei Yang"], "title": "Flexbee: A Grasping and Perching UAV Based on Soft Vector-Propulsion Nozzle", "comment": "11 pages, 17 figures", "summary": "The aim of this paper is to design a new type of grasping and perching\nunmanned aerial vehicle (UAV), called Flexbee, which features a soft\nvector-propulsion nozzle (SVPN). Compared to previous UAVs, Flexbee integrates\nflight, grasping, and perching functionalities into the four SVPNs. This\nintegration offers advantages including decoupled position and attitude\ncontrol, high structural reuse, and strong adaptability strong adaptability for\ngrasping and perching. A dynamics model of Flexbee has been developed, and the\nnonlinear coupling issue of the moment has been resolved through linearization\nof the equivalent moment model. A hierarchical control strategy was used to\ndesign controllers for the two operational modes of Flexbee. Finally, flight,\ngrasping, and perching experiments were conducted to validate Flexbee's\nkinematic capabilities and the effectiveness of the control strategy.", "AI": {"tldr": "Flexbee\u662f\u4e00\u79cd\u65b0\u578b\u7684\u65e0\u4eba\u673a\uff0c\u96c6\u6210\u4e86\u98de\u884c\u3001\u6293\u53d6\u548c\u6816\u606f\u529f\u80fd\uff0c\u7ecf\u8fc7\u9a8c\u8bc1\u5c55\u793a\u4e86\u5176\u8fd0\u52a8\u80fd\u529b\u548c\u63a7\u5236\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "motivation": "To create a versatile UAV that integrates flight, grasping, and perching functionalities for improved performance.", "method": "Development of a dynamics model, resolution of nonlinear coupling issues, and implementation of a hierarchical control strategy.", "result": "Flexbee successfully integrates multiple functionalities and exhibits effective control, as proven by experimental validation.", "conclusion": "Flexbee demonstrates enhanced operational capabilities in flight, grasping, and perching, validated by conducted experiments."}}
{"id": "2510.18600", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.18600", "abs": "https://arxiv.org/abs/2510.18600", "authors": ["Shubham Vyas", "Franek Stark", "Rohit Kumar", "Hannah Isermann", "Jonas Haack", "Mihaela Popescu", "Jakob Middelberg", "Dennis Mronga", "Frank Kirchner"], "title": "Quadrupeds for Planetary Exploration: Field Testing Control Algorithms on an Active Volcano", "comment": "Presented at 18th Symposium on Advanced Space Technologies in\n  Robotics and Automation (ASTRA)", "summary": "Missions such as the Ingenuity helicopter have shown the advantages of using\nnovel locomotion modes to increase the scientific return of planetary\nexploration missions. Legged robots can further expand the reach and capability\nof future planetary missions by traversing more difficult terrain than wheeled\nrovers, such as jumping over cracks on the ground or traversing rugged terrain\nwith boulders. To develop and test algorithms for using quadruped robots, the\nAAPLE project was carried out at DFKI. As part of the project, we conducted a\nseries of field experiments on the Volcano on the Aeolian island of Vulcano, an\nactive stratovolcano near Sicily, Italy. The experiments focused on validating\nnewly developed state-of-the-art adaptive optimal control algorithms for\nquadrupedal locomotion in a high-fidelity analog environment for Lunar and\nMartian surfaces. This paper presents the technical approach, test plan,\nsoftware architecture, field deployment strategy, and evaluation results from\nthe Vulcano campaign.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e2d\u7684\u5e94\u7528\uff0c\u9a8c\u8bc1\u4e86\u5728\u7c7b\u6708\u7403\u548c\u706b\u661f\u8868\u9762\u5de5\u4f5c\u7684\u65b0\u63a7\u5236\u7b97\u6cd5\u3002", "motivation": "\u63a2\u7d22\u884c\u661f\u7684\u4efb\u52a1\u9700\u8981\u66f4\u9ad8\u6548\u7684\u79fb\u52a8\u65b9\u5f0f\uff0c\u56db\u8db3\u673a\u5668\u4eba\u80fd\u5728\u590d\u6742\u5730\u5f62\u4e2d\u8868\u73b0\u51fa\u4f18\u52bf\u3002", "method": "\u901a\u8fc7\u5728\u610f\u5927\u5229\u7684\u706b\u5c71\u8fdb\u884c\u7cfb\u5217\u5b9e\u5730\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u56db\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e2d\u7684\u9002\u5e94\u6027\u548c\u63a7\u5236\u7b97\u6cd5\u3002", "result": "\u6210\u529f\u9a8c\u8bc1\u4e86\u65b0\u5f00\u53d1\u7684\u81ea\u9002\u5e94\u6700\u4f18\u63a7\u5236\u7b97\u6cd5\u5728\u56db\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u56db\u8db3\u673a\u5668\u4eba\u53ef\u4ee5\u63d0\u5347\u884c\u661f\u63a2\u7d22\u4efb\u52a1\u7684\u80fd\u529b\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2510.18608", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18608", "abs": "https://arxiv.org/abs/2510.18608", "authors": ["Luigi Quarantiello", "Elia Piccoli", "Jack Bell", "Malio Li", "Giacomo Carf\u00ec", "Eric Nuertey Coleman", "Gerlando Gramaglia", "Lanpei Li", "Mauro Madeddu", "Irene Testa", "Vincenzo Lomonaco"], "title": "A Compositional Paradigm for Foundation Models: Towards Smarter Robotic Agents", "comment": null, "summary": "The birth of Foundation Models brought unprecedented results in a wide range\nof tasks, from language to vision, to robotic control. These models are able to\nprocess huge quantities of data, and can extract and develop rich\nrepresentations, which can be employed across different domains and modalities.\nHowever, they still have issues in adapting to dynamic, real-world scenarios\nwithout retraining the entire model from scratch. In this work, we propose the\napplication of Continual Learning and Compositionality principles to foster the\ndevelopment of more flexible, efficient and smart AI solutions.", "AI": {"tldr": "\u57fa\u7840\u6a21\u578b\u867d\u7136\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u6539\u8fdb\u9002\u5e94\u6027\uff0c\u672c\u6587\u63d0\u51fa\u5229\u7528\u6301\u7eed\u5b66\u4e60\u4e0e\u7ec4\u5408\u6027\u539f\u5219\u6765\u63d0\u5347AI\u7cfb\u7edf\u7684\u7075\u6d3b\u6027\u4e0e\u6548\u7387\u3002", "motivation": "\u76ee\u524d\u57fa\u7840\u6a21\u578b\u5728\u591a\u4e2a\u9886\u57df\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff0c\u4f46\u5728\u52a8\u6001\u3001\u73b0\u5b9e\u573a\u666f\u4e2d\u9002\u5e94\u6027\u4e0d\u8db3\uff0c\u800c\u9700\u8981\u4ece\u5934\u5f00\u59cb\u91cd\u8bad\u7ec3\u7684\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002", "method": "\u901a\u8fc7\u6301\u7eed\u5b66\u4e60\u548c\u7ec4\u5408\u6027\u7684\u539f\u5219\u63a8\u52a8AI\u6a21\u578b\u5728\u4e0d\u540c\u4efb\u52a1\u4e4b\u95f4\u7684\u8fc1\u79fb\u4e0e\u9002\u5e94\u3002", "result": "\u63d0\u51fa\u5e94\u7528\u6301\u7eed\u5b66\u4e60\u548c\u7ec4\u5408\u6027\u539f\u5219\uff0c\u53ef\u4ee5\u4fc3\u8fdb\u66f4\u7075\u6d3b\u3001\u9ad8\u6548\u548c\u667a\u80fd\u7684AI\u89e3\u51b3\u65b9\u6848\u7684\u5f00\u53d1\u3002", "conclusion": "\u91c7\u7528\u6301\u7eed\u5b66\u4e60\u548c\u7ec4\u5408\u6027\u539f\u5219\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u52a0\u7075\u6d3b\u548c\u9ad8\u6548\u7684\u4eba\u5de5\u667a\u80fd\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u52a8\u6001\u53d8\u5316\u3002"}}
{"id": "2510.18643", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.18643", "abs": "https://arxiv.org/abs/2510.18643", "authors": ["Mattias Trende", "Petter \u00d6gren"], "title": "Least Restrictive Hyperplane Control Barrier Functions", "comment": null, "summary": "Control Barrier Functions (CBFs) can provide provable safety guarantees for\ndynamic systems. However, finding a valid CBF for a system of interest is often\nnon-trivial, especially if the shape of the unsafe region is complex and the\nCBFs are of higher order. A common solution to this problem is to make a\nconservative approximation of the unsafe region in the form of a\nline/hyperplane, and use the corresponding conservative Hyperplane-CBF when\ndeciding on safe control actions. In this letter, we note that conservative\nconstraints are only a problem if they prevent us from doing what we want.\nThus, instead of first choosing a CBF and then choosing a safe control with\nrespect to the CBF, we optimize over a combination of CBFs and safe controls to\nget as close as possible to our desired control, while still having the safety\nguarantee provided by the CBF. We call the corresponding CBF the least\nrestrictive Hyperplane-CBF. Finally, we also provide a way of creating a smooth\nparameterization of the CBF-family for the optimization, and illustrate the\napproach on a double integrator dynamical system with acceleration constraints,\nmoving through a group of arbitrarily shaped static and moving obstacles.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316CBFs\u548c\u5b89\u5168\u63a7\u5236\u7684\u7ec4\u5408\uff0c\u51cf\u5c11\u5bf9\u63a7\u5236\u7684\u9650\u5236\uff0c\u540c\u65f6\u4fdd\u6301\u5b89\u5168\u6027\u4fdd\u969c\u3002", "motivation": "CBFs\u5728\u52a8\u6001\u7cfb\u7edf\u4e2d\u63d0\u4f9b\u5b89\u5168\u4fdd\u969c\uff0c\u4f46\u5bfb\u627e\u6709\u6548\u7684CBFs\u5f80\u5f80\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u5b89\u5168\u533a\u57df\u590d\u6742\u65f6\u3002", "method": "\u4f18\u5316CBFs\u548c\u5b89\u5168\u63a7\u5236\u7684\u7ec4\u5408\uff0c\u751f\u6210\u6700\u5c11\u9650\u5236\u7684Hyperplane-CBF\uff0c\u540c\u65f6\u4e3aCBF\u5bb6\u65cf\u521b\u5efa\u5149\u6ed1\u53c2\u6570\u5316\u3002", "result": "\u5728\u53cc\u91cd\u79ef\u5206\u52a8\u6001\u7cfb\u7edf\u4e2d\u5e94\u7528\u6b64\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5728\u6709\u969c\u788d\u7269\u7684\u73af\u5883\u4e2d\u5b89\u5168\u63a7\u5236\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6700\u5c11\u9650\u5236Hyperplane-CBF\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u5177\u6709\u590d\u6742\u4e0d\u5b89\u5168\u533a\u57df\u7684\u52a8\u6001\u7cfb\u7edf\uff0c\u5e76\u63d0\u4f9b\u5b89\u5168\u63a7\u5236\u3002"}}
{"id": "2510.18678", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.18678", "abs": "https://arxiv.org/abs/2510.18678", "authors": ["Alberto Sanchez-Delgado", "Jo\u00e3o Carlos Virgolino Soares", "David Omar Al Tawil", "Alessia Li Noce", "Matteo Villa", "Victor Barasuol", "Paolo Arena", "Claudio Semini"], "title": "Towards An Adaptive Locomotion Strategy For Quadruped Rovers: Quantifying When To Slide Or Walk On Planetary Slopes", "comment": "Published at the 18th Symposium on Advanced Space Technologies in\n  Robotics and Automation (ASTRA 2025)", "summary": "Legged rovers provide enhanced mobility compared to wheeled platforms,\nenabling navigation on steep and irregular planetary terrains. However,\ntraditional legged locomotion might be energetically inefficient and\npotentially dangerous to the rover on loose and inclined surfaces, such as\ncrater walls and cave slopes. This paper introduces a preliminary study that\ncompares the Cost of Transport (CoT) of walking and torso-based sliding\nlocomotion for quadruped robots across different slopes, friction conditions\nand speed levels. By identifying intersections between walking and sliding CoT\ncurves, we aim to define threshold conditions that may trigger transitions\nbetween the two strategies. The methodology combines physics-based simulations\nin Isaac Sim with particle interaction validation in ANSYS-Rocky. Our results\nrepresent an initial step towards adaptive locomotion strategies for planetary\nlegged rovers.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u6b65\u6001\u4e0e\u6ed1\u52a8\u65b9\u5f0f\u7684\u80fd\u91cf\u6548\u7387\uff0c\u4e3a\u672a\u6765\u81ea\u9002\u5e94\u79fb\u52a8\u7b56\u7565\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u63d0\u9ad8\u884c\u8d70\u673a\u5668\u4eba\u5728\u4e0d\u5e73\u5766\u548c\u9661\u5ced\u884c\u661f\u8868\u9762\u4e0a\u7684\u79fb\u52a8\u80fd\u529b\uff0c\u800c\u4f20\u7edf\u817f\u90e8\u8fd0\u52a8\u53ef\u80fd\u4e0d\u591f\u9ad8\u6548\u4e14\u5b58\u5728\u98ce\u9669\u3002", "method": "\u7ed3\u5408\u4e86Isaac Sim\u4e2d\u7684\u57fa\u4e8e\u7269\u7406\u7684\u6a21\u62df\u548cANSYS-Rocky\u4e2d\u7684\u7c92\u5b50\u4e92\u52a8\u9a8c\u8bc1\u3002", "result": "\u6bd4\u8f83\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5728\u4e0d\u540c\u5761\u5ea6\u3001\u6469\u64e6\u6761\u4ef6\u548c\u901f\u5ea6\u4e0b\uff0c\u6b65\u6001\u4e0e\u8eaf\u5e72\u6ed1\u52a8\u7684\u8fd0\u8f93\u6210\u672c\u3002", "conclusion": "\u63d0\u51fa\u4e86\u9608\u503c\u6761\u4ef6\uff0c\u4ee5\u5e2e\u52a9\u5224\u65ad\u5728\u884c\u8d70\u4e0e\u6ed1\u52a8\u4e4b\u95f4\u7684\u5207\u6362\uff0c\u63a8\u52a8\u672a\u6765\u7684\u81ea\u9002\u5e94\u8fd0\u52a8\u7b56\u7565\u3002"}}
{"id": "2510.18697", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.18697", "abs": "https://arxiv.org/abs/2510.18697", "authors": ["Phuoc Nguyen", "Francesco Verdoja", "Ville Kyrki"], "title": "Event-Grounding Graph: Unified Spatio-Temporal Scene Graph from Robotic Observations", "comment": "Submitted to RA-L", "summary": "A fundamental aspect for building intelligent autonomous robots that can\nassist humans in their daily lives is the construction of rich environmental\nrepresentations. While advances in semantic scene representations have enriched\nrobotic scene understanding, current approaches lack a connection between\nspatial features and dynamic events; e.g., connecting the blue mug to the event\nwashing a mug. In this work, we introduce the event-grounding graph (EGG), a\nframework grounding event interactions to spatial features of a scene. This\nrepresentation allows robots to perceive, reason, and respond to complex\nspatio-temporal queries. Experiments using real robotic data demonstrate EGG's\ncapability to retrieve relevant information and respond accurately to human\ninquiries concerning the environment and events within. Furthermore, the EGG\nframework's source code and evaluation dataset are released as open-source at:\nhttps://github.com/aalto-intelligent-robotics/EGG.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6EGG\uff0c\u53ef\u4ee5\u5c06\u4e8b\u4ef6\u4e0e\u7a7a\u95f4\u7279\u5f81\u5173\u8054\uff0c\u63d0\u5347\u673a\u5668\u4eba\u5bf9\u73af\u5883\u7684\u7406\u89e3\u548c\u54cd\u5e94\u80fd\u529b\u3002", "motivation": "\u6784\u5efa\u80fd\u591f\u6267\u884c\u590d\u6742\u65f6\u7a7a\u67e5\u8be2\u7684\u667a\u80fd\u81ea\u4e3b\u673a\u5668\u4eba\uff0c\u589e\u5f3a\u5176\u73af\u5883\u7406\u89e3\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e8b\u4ef6\u57fa\u7840\u56fe\uff08EGG\uff09\uff0c\u5c06\u4e8b\u4ef6\u4ea4\u4e92\u8fde\u63a5\u5230\u573a\u666f\u7684\u7a7a\u95f4\u7279\u5f81\u3002", "result": "EGG\u80fd\u591f\u51c6\u786e\u68c0\u7d22\u6709\u5173\u73af\u5883\u548c\u4e8b\u4ef6\u7684\u4fe1\u606f\uff0c\u5e76\u5bf9\u4eba\u7c7b\u8be2\u95ee\u505a\u51fa\u54cd\u5e94\u3002", "conclusion": "EGG\u6846\u67b6\u6709\u6548\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5bf9\u590d\u6742\u73af\u5883\u67e5\u8be2\u7684\u5904\u7406\u80fd\u529b\uff0c\u5e76\u5df2\u5f00\u6e90\u3002"}}
{"id": "2510.18766", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.18766", "abs": "https://arxiv.org/abs/2510.18766", "authors": ["Alexander Krawciw", "Sven Lilge", "Luka Antonyshyn", "Timothy D. Barfoot"], "title": "Sharing the Load: Distributed Model-Predictive Control for Precise Multi-Rover Cargo Transport", "comment": "8 pages, 4 figures", "summary": "For autonomous cargo transportation, teams of mobile robots can provide more\noperational flexibility than a single large robot. In these scenarios,\nprecision in both inter-vehicle distance and path tracking is key. With this\nmotivation, we develop a distributed model-predictive controller (MPC) for\nmulti-vehicle cargo operations that builds on the precise path-tracking of\nlidar teach and repeat. To carry cargo, a following vehicle must maintain a\nEuclidean distance offset from a lead vehicle regardless of the path curvature.\nOur approach uses a shared map to localize the robots relative to each other\nwithout GNSS or direct observations. We compare our approach to a centralized\nMPC and a baseline approach that directly measures the inter-vehicle distance.\nThe distributed MPC shows equivalent nominal performance to the more complex\ncentralized MPC. Using a direct measurement of the relative distance between\nthe leader and follower shows improved tracking performance in close-range\nscenarios but struggles with long-range offsets. The operational flexibility\nprovided by distributing the computation makes it well suited for real\ndeployments. We evaluate four types of convoyed path trackers with over 10 km\nof driving in a coupled convoy. With convoys of two and three rovers, the\nproposed distributed MPC method works in real-time to allow map-based convoying\nto maintain maximum spacing within 20 cm of the target in various conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5171\u4eab\u5730\u56fe\u7684\u5206\u5e03\u5f0f\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668(MPC)\u4e00\u7528\u4e8e\u591a\u4e2a\u79fb\u52a8\u673a\u5668\u4eba\u8fdb\u884c\u8d27\u8fd0\u64cd\u4f5c\uff0c\u663e\u793a\u51fa\u5728\u8def\u5f84\u8ddf\u8e2a\u548c\u8f66\u8f86\u95f4\u8ddd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7531\u4e8e\u591a\u4e2a\u79fb\u52a8\u673a\u5668\u4eba\u80fd\u63d0\u4f9b\u6bd4\u5355\u4e2a\u5927\u578b\u673a\u5668\u4eba\u66f4\u9ad8\u7684\u64cd\u4f5c\u7075\u6d3b\u6027\uff0c\u56e0\u6b64\u5728\u81ea\u4e3b\u8d27\u7269\u8fd0\u8f93\u4e2d\u9700\u8981\u6539\u8fdb\u8f66\u8f86\u95f4\u7684\u8ddd\u79bb\u548c\u8def\u5f84\u8ddf\u8e2a\u7cbe\u5ea6\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e lidar \u6559\u5b66\u548c\u91cd\u590d\u7684\u5206\u5e03\u5f0f\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668(MPC)\u3002", "result": "\u5206\u5e03\u5f0fMPC\u572810\u516c\u91cc\u8bad\u7ec3\u4e2d\u8868\u73b0\u51fa\u4e0e\u4e2d\u592eMPC\u76f8\u5f53\u7684\u6027\u80fd\uff1b\u5728\u8fd1\u8ddd\u79bb\u573a\u666f\u4e2d\uff0c\u76f4\u63a5\u6d4b\u91cf\u7684\u76f8\u5bf9\u8ddd\u79bb\u8ddf\u8e2a\u6027\u80fd\u66f4\u4f73\uff0c\u4f46\u5728\u8fdc\u8ddd\u79bb\u504f\u79fb\u65f6\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "\u5206\u5e03\u5f0fMPC\u5728\u591a\u8f66\u8f86\u8d27\u8fd0\u64cd\u4f5c\u4e2d\u663e\u793a\u51fa\u4e0e\u96c6\u4e2d\u5f0fMPC\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u90e8\u7f72\u3002"}}
{"id": "2510.18776", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.18776", "abs": "https://arxiv.org/abs/2510.18776", "authors": ["Emad Razavi", "Angelo Bratta", "Jo\u00e3o Carlos Virgolino Soares", "Carmine Recchiuto", "Claudio Semini"], "title": "Online Object-Level Semantic Mapping for Quadrupeds in Real-World Environments", "comment": "Published at the Italian Conference on Robotics and Intelligent\n  Machines (I-RIM) 3D, 2025", "summary": "We present an online semantic object mapping system for a quadruped robot\noperating in real indoor environments, turning sensor detections into named\nobjects in a global map. During a run, the mapper integrates range geometry\nwith camera detections, merges co-located detections within a frame, and\nassociates repeated detections into persistent object instances across frames.\nObjects remain in the map when they are out of view, and repeated sightings\nupdate the same instance rather than creating duplicates. The output is a\ncompact object layer that can be queried (class, pose, and confidence), is\nintegrated with the occupancy map and readable by a planner. In on-robot tests,\nthe layer remained stable across viewpoint changes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u7684\u5728\u7ebf\u8bed\u4e49\u7269\u4f53\u6620\u5c04\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u771f\u5b9e\u5ba4\u5185\u73af\u5883\u4e2d\u5c06\u4f20\u611f\u5668\u68c0\u6d4b\u8f6c\u5316\u4e3a\u5168\u7403\u56fe\u4e2d\u7684\u547d\u540d\u7269\u4f53\u3002", "motivation": "\u5728\u590d\u6742\u73af\u5883\u4e2d\u51c6\u786e\u8bc6\u522b\u548c\u6301\u4e45\u8ddf\u8e2a\u5bf9\u8c61\uff0c\u4ee5\u5b9e\u73b0\u66f4\u590d\u6742\u7684\u81ea\u4e3b\u5bfc\u822a\u548c\u4efb\u52a1\u6267\u884c\u3002", "method": "\u7cfb\u7edf\u901a\u8fc7\u6574\u5408\u8ddd\u79bb\u51e0\u4f55\u4fe1\u606f\u4e0e\u6444\u50cf\u5934\u68c0\u6d4b\uff0c\u5408\u5e76\u540c\u5e27\u5185\u7684\u5171\u5b58\u68c0\u6d4b\uff0c\u5e76\u5c06\u91cd\u590d\u68c0\u6d4b\u5173\u8054\u4e3a\u6301\u7eed\u7684\u7269\u4f53\u5b9e\u4f8b\u3002", "result": "\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u7269\u4f53\u5c42\u5728\u89c6\u89d2\u53d8\u5316\u65f6\u4fdd\u6301\u7a33\u5b9a\uff0c\u80fd\u591f\u88ab\u89c4\u5212\u5668\u8bfb\u53d6\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u80fd\u591f\u7a33\u5b9a\u5730\u5728\u89c6\u89d2\u53d8\u5316\u4e0b\u4fdd\u6301\u7269\u4f53\u6620\u5c04\u7684\u7a33\u5b9a\u6027\uff0c\u5e76\u6709\u6548\u6574\u5408\u4e0d\u540c\u4f20\u611f\u5668\u6570\u636e\u3002"}}
{"id": "2510.18845", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.18845", "abs": "https://arxiv.org/abs/2510.18845", "authors": ["Ryan Teoh", "Sander Tonkens", "William Sharpless", "Aijia Yang", "Zeyuan Feng", "Somil Bansal", "Sylvia Herbert"], "title": "MADR: MPC-guided Adversarial DeepReach", "comment": "8 pages, under review", "summary": "Hamilton-Jacobi (HJ) Reachability offers a framework for generating safe\nvalue functions and policies in the face of adversarial disturbance, but is\nlimited by the curse of dimensionality. Physics-informed deep learning is able\nto overcome this infeasibility, but itself suffers from slow and inaccurate\nconvergence, primarily due to weak PDE gradients and the complexity of\nself-supervised learning. A few works, recently, have demonstrated that\nenriching the self-supervision process with regular supervision (based on the\nnature of the optimal control problem), greatly accelerates convergence and\nsolution quality, however, these have been limited to single player problems\nand simple games. In this work, we introduce MADR: MPC-guided Adversarial\nDeepReach, a general framework to robustly approximate the two-player, zero-sum\ndifferential game value function. In doing so, MADR yields the corresponding\noptimal strategies for both players in zero-sum games as well as safe policies\nfor worst-case robustness. We test MADR on a multitude of high-dimensional\nsimulated and real robotic agents with varying dynamics and games, finding that\nour approach significantly out-performs state-of-the-art baselines in\nsimulation and produces impressive results in hardware.", "AI": {"tldr": "MADR\u6846\u67b6\u7ed3\u5408MPC\u4e0e\u6df1\u5ea6\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u9ad8\u7ef4\u5bf9\u6297\u535a\u5f08\u4e2d\u7684\u7b56\u7565\u548c\u5b89\u5168\u95ee\u9898\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u867d\u7136HJ\u53ef\u8fbe\u6027\u80fd\u591f\u751f\u6210\u5b89\u5168\u503c\u51fd\u6570\uff0c\u4f46\u9762\u4e34\u7ef4\u6570\u8bc5\u5492\uff0c\u800c\u7269\u7406\u4fe1\u606f\u6df1\u5ea6\u5b66\u4e60\u5728\u6536\u655b\u6027\u4e0a\u76f8\u5bf9\u8f83\u6162\u4e14\u4e0d\u51c6\u786e\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5f15\u5165MADR\uff1a\u57fa\u4e8eMPC\u5f15\u5bfc\u7684\u5bf9\u6297\u6df1\u5ea6Reach\uff0c\u901a\u8fc7\u589e\u5f3a\u81ea\u6211\u76d1\u7763\u4e0e\u5e38\u89c4\u76d1\u7763\u7ed3\u5408\u6765\u903c\u8fd1\u4e24\u4eba\u96f6\u548c\u5fae\u5206\u6e38\u620f\u7684\u4ef7\u503c\u51fd\u6570\u3002", "result": "MADR\u5728\u591a\u4e2a\u9ad8\u7ef4\u4eff\u771f\u53ca\u5b9e\u9645\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u6d4b\u8bd5\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u57fa\u7ebf\uff0c\u5e76\u53d6\u5f97\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u786c\u4ef6\u7ed3\u679c\u3002", "conclusion": "MADR\u5728\u9ad8\u7ef4\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u4f9b\u4e86\u5065\u58ee\u7684\u96f6\u548c\u535a\u5f08\u7b56\u7565\u548c\u5b89\u5168\u653f\u7b56\u3002"}}
