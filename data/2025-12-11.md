<div id=toc></div>

# Table of Contents

- [cs.HC](#cs.HC) [Total: 22]
- [cs.RO](#cs.RO) [Total: 34]


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [1] [Agentic AI as Undercover Teammates: Argumentative Knowledge Construction in Hybrid Human-AI Collaborative Learning](https://arxiv.org/abs/2512.08933)
*Lixiang Yan,Yueqiao Jin,Linxuan Zhao,Roberto Martinez-Maldonado,Xinyu Li,Xiu Guan,Wenxin Guo,Xibin Han,Dragan Gašević*

Main category: cs.HC

TL;DR: 研究生成性人工智能在协作学习中的作用，发现其以不同角色影响知识构建过程。


<details>
  <summary>Details</summary>
Motivation: 了解生成性人工智能在协作学习环境中的作用及其对知识构建过程的影响。

Method: 研究设计了不同支持与对立角色的AI，与人类参与者协同完成任务，并对交流数据进行分析。

Result: 分析显示AI在协作中保持平衡的参与，但显著重组了认知与社交流程，支持性角色增进概念整合，而对立性角色促进批判探讨。

Conclusion: 生成性人工智能可视为认知与社交的参与者，通过提高推理质量和协调性推动学习。

Abstract: Generative artificial intelligence (AI) agents are increasingly embedded in collaborative learning environments, yet their impact on the processes of argumentative knowledge construction remains insufficiently understood. Emerging conceptualisations of agentic AI and artificial agency suggest that such systems possess bounded autonomy, interactivity, and adaptability, allowing them to engage as epistemic participants rather than mere instructional tools. Building on this theoretical foundation, the present study investigates how agentic AI, designed as undercover teammates with either supportive or contrarian personas, shapes the epistemic and social dynamics of collaborative reasoning. Drawing on Weinberger and Fischer's (2006) four-dimensional framework, participation, epistemic reasoning, argument structure, and social modes of co-construction, we analysed synchronous discourse data from 212 human and 64 AI participants (92 triads) engaged in an analytical problem-solving task. Mixed-effects and epistemic network analyses revealed that AI teammates maintained balanced participation but substantially reorganised epistemic and social processes: supportive personas promoted conceptual integration and consensus-oriented reasoning, whereas contrarian personas provoked critical elaboration and conflict-driven negotiation. Epistemic adequacy, rather than participation volume, predicted individual learning gains, indicating that agentic AI's educational value lies in enhancing the quality and coordination of reasoning rather than amplifying discourse quantity. These findings extend CSCL theory by conceptualising agentic AI as epistemic and social participants, bounded yet adaptive collaborators that redistribute cognitive and argumentative labour in hybrid human-AI learning environments.

</details>


### [2] [Motion2Meaning: A Clinician-Centered Framework for Contestable LLM in Parkinson's Disease Gait Interpretation](https://arxiv.org/abs/2512.08934)
*Loc Phuc Truong Nguyen,Hung Thanh Do,Hung Truong Thanh Nguyen,Hung Cao*

Main category: cs.HC

TL;DR: 该研究推出了一个名为Motion2Meaning的框架，增强了临床医生对AI辅助步态分析的透明度和可争议性，通过可穿戴传感器数据和可解释的AI实现了临床监督。


<details>
  <summary>Details</summary>
Motivation: 当前的临床仪表盘缺乏透明度，无法有效支持临床医生质疑AI决策，因此需要一个改进的框架来解决此问题。

Method: 利用可穿戴传感器收集的垂直地面反作用力时间序列数据，通过1D卷积神经网络进行预测，并结合可争议的解读界面和跨模态解释差异化保障。

Result: 提出的Motion2Meaning框架通过集成界面实现对AI决策的透明性，与临床医生互动更加灵活，增强了可争议性。

Conclusion: Motion2Meaning系统结合了可穿戴传感器数据和可解释的AI，提供了一种有效的方式来解释和争辩AI在帕金森病步态分析中的决策，有助于临床监督。

Abstract: AI-assisted gait analysis holds promise for improving Parkinson's Disease (PD) care, but current clinical dashboards lack transparency and offer no meaningful way for clinicians to interrogate or contest AI decisions. To address this issue, we present Motion2Meaning, a clinician-centered framework that advances Contestable AI through a tightly integrated interface designed for interpretability, oversight, and procedural recourse. Our approach leverages vertical Ground Reaction Force (vGRF) time-series data from wearable sensors as an objective biomarker of PD motor states. The system comprises three key components: a Gait Data Visualization Interface (GDVI), a one-dimensional Convolutional Neural Network (1D-CNN) that predicts Hoehn & Yahr severity stages, and a Contestable Interpretation Interface (CII) that combines our novel Cross-Modal Explanation Discrepancy (XMED) safeguard with a contestable Large Language Model (LLM). Our 1D-CNN achieves 89.0% F1-score on the public PhysioNet gait dataset. XMED successfully identifies model unreliability by detecting a five-fold increase in explanation discrepancies in incorrect predictions (7.45%) compared to correct ones (1.56%), while our LLM-powered interface enables clinicians to validate correct predictions and successfully contest a portion of the model's errors. A human-centered evaluation of this contestable interface reveals a crucial trade-off between the LLM's factual grounding and its readability and responsiveness to clinical feedback. This work demonstrates the feasibility of combining wearable sensor analysis with Explainable AI (XAI) and contestable LLMs to create a transparent, auditable system for PD gait interpretation that maintains clinical oversight while leveraging advanced AI capabilities. Our implementation is publicly available at: https://github.com/hungdothanh/motion2meaning.

</details>


### [3] [From Script to Stage: Automating Experimental Design for Social Simulations with LLMs](https://arxiv.org/abs/2512.08935)
*Yuwei Guo,Zihan Zhao,Deyu Zhou,Xiaowei Liu,Ming Zhang*

Main category: cs.HC

TL;DR: 本文提出了一种自动化多智能体实验设计框架，利用大型语言模型降低社会科学实验设计的复杂性，提供决策支持。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型（LLMs）在社会科学研究中的应用，并解决传统计算实验的复杂性和高门槛挑战。

Method: 提出了一种基于脚本生成的自动多智能体实验设计框架，分为三个阶段：脚本生成、脚本最终确定和角色生成。

Result: 在多个社会科学实验场景中，大量实验表明生成的角色智能体能够按照设计脚本执行，并重现与现实世界一致的结果。

Conclusion: 该框架降低了社会科学实验设计的门槛，为政策制定和研究提供了新的决策支持工具。

Abstract: The rise of large language models (LLMs) has opened new avenues for social science research. Multi-agent simulations powered by LLMs are increasingly becoming a vital approach for exploring complex social phenomena and testing theoretical hypotheses. However, traditional computational experiments often rely heavily on interdisciplinary expertise, involve complex operations, and present high barriers to entry. While LLM-driven agents show great potential for automating experimental design, their reliability and scientific rigor remain insufficient for widespread adoption. To address these challenges, this paper proposes an automated multi-agent experiment design framework based on script generation, inspired by the concept of the Decision Theater. The experimental design process is divided into three stages: (1) Script Generation - a Screenwriter Agent drafts candidate experimental scripts; (2) Script Finalization - a Director Agent evaluates and selects the final script; (3) Actor Generation - an Actor Factory creates actor agents capable of performing on the experimental "stage" according to the finalized script. Extensive experiment conducted across multiple social science experimental scenarios demonstrate that the generated actor agents can perform according to the designed scripts and reproduce outcomes consistent with real-world situations. This framework not only lowers the barriers to experimental design in social science but also provides a novel decision-support tool for policy-making and research. The project's source code is available at: https://anonymous.4open.science/r/FSTS-DE1E

</details>


### [4] [A Principle-based Framework for the Development and Evaluation of Large Language Models for Health and Wellness](https://arxiv.org/abs/2512.08936)
*Brent Winslow,Jacqueline Shreibati,Javier Perez,Hao-Wei Su,Nichole Young-Lin,Nova Hammerquist,Daniel McDuff,Jason Guss,Jenny Vafeiadou,Nick Cain,Alex Lin,Erik Schenck,Shiva Rajagopal,Jia-Ru Chung,Anusha Venkatakrishnan,Amy Armento Lee,Maryam Karimzadehgan,Qingyou Meng,Rythm Agarwal,Aravind Natarajan,Tracy Giest*

Main category: cs.HC

TL;DR: 本研究开发了一种基于原则的框架来评估个人健康应用中的LLMs，识别安全、有效性及用户反馈的重要性，最终形成了负责任开发的标准化方法。


<details>
  <summary>Details</summary>
Motivation: 将生成性人工智能融入个人健康应用带来了个性化和数据驱动的健康指导的新机会，但也面临与用户安全、模型准确性和隐私保护相关的挑战。

Method: 开发和验证一种原则性框架用于系统评估应用于个人健康和健康的LLMs

Result: 在Fitbit Insights探索器中应用该框架进行的多阶段部署，涉及超过13,000名同意用户，系统识别出初步测试中未显现的挑战。

Conclusion: 建立了一种全面、可操作的方法，旨在负责任地开发和部署LLM驱动的健康应用程序，提供了标准化的方法以促进创新，同时确保新兴技术对用户安全、有效和可信。

Abstract: The incorporation of generative artificial intelligence into personal health applications presents a transformative opportunity for personalized, data-driven health and fitness guidance, yet also poses challenges related to user safety, model accuracy, and personal privacy. To address these challenges, a novel, principle-based framework was developed and validated for the systematic evaluation of LLMs applied to personal health and wellness. First, the development of the Fitbit Insights explorer, a large language model (LLM)-powered system designed to help users interpret their personal health data, is described. Subsequently, the safety, helpfulness, accuracy, relevance, and personalization (SHARP) principle-based framework is introduced as an end-to-end operational methodology that integrates comprehensive evaluation techniques including human evaluation by generalists and clinical specialists, autorater assessments, and adversarial testing, into an iterative development lifecycle. Through the application of this framework to the Fitbit Insights explorer in a staged deployment involving over 13,000 consented users, challenges not apparent during initial testing were systematically identified. This process guided targeted improvements to the system and demonstrated the necessity of combining isolated technical evaluations with real-world user feedback. Finally, a comprehensive, actionable approach is established for the responsible development and deployment of LLM-powered health applications, providing a standardized methodology to foster innovation while ensuring emerging technologies are safe, effective, and trustworthy for users.

</details>


### [5] [When AI Gives Advice: Evaluating AI and Human Responses to Online Advice-Seeking for Well-Being](https://arxiv.org/abs/2512.08937)
*Harsh Kumar,Jasmine Chahal,Yinuo Zhao,Zeling Zhang,Annika Wei,Louis Tay,Ashton Anderson*

Main category: cs.HC

TL;DR: 研究比较了LLM建议与人类建议的质量，发现LLM表现更好，并探讨了如何结合人类与AI的建议以提高建议质量。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨LLM建议质量的未知领域，以及与人类和众包建议的比较。

Method: 通过两个研究（共210名参与者）对比了Reddit上高票建议与LLM生成建议的质量，并进行了探索性问卷调查。

Result: 研究发现LLM生成的建议在有效性、热情和再次寻求建议的意愿上显著优于人类建议，同时强调了人类建议与AI建议的结合潜力。

Conclusion: 该研究提出了结合人类和AI建议的设计理念，并强调了在建议提供中AI与人类智慧之间的互补性。

Abstract: Seeking advice is a core human behavior that the Internet has reinvented twice: first through forums and Q\&A communities that crowdsource public guidance, and now through large language models (LLMs) that deliver private, on-demand counsel at scale. Yet the quality of this synthesized LLM advice remains unclear. How does it compare, not only against arbitrary human comments, but against the wisdom of the online crowd? We conducted two studies (N = 210) in which experts compared top-voted Reddit advice with LLM-generated advice. LLMs ranked significantly higher overall and on effectiveness, warmth, and willingness to seek advice again. GPT-4o beat GPT-5 on all metrics except sycophancy, suggesting that benchmark gains need not improve advice-giving. In our second study, we examined how human and algorithmic advice could be combined, and found that human advice can be unobtrusively polished to compete with AI-generated comments. Finally, to surface user expectations, we ran an exploratory survey with undergraduates (N=148) that revealed heterogeneous, persona-dependent preferences for agent qualities (e.g., coach-like: goal-focused structure; friend-like: warmth and humor). We conclude with design implications for advice-giving agents and ecosystems blending AI, crowd input, and expert oversight.

</details>


### [6] [The Impact of Artificial Intelligence on Strategic Technology Management: A Mixed-Methods Analysis of Resources, Capabilities, and Human-AI Collaboration](https://arxiv.org/abs/2512.08938)
*Massimo Fascinari,Vincent English*

Main category: cs.HC

TL;DR: 本研究探讨了AI在战略技术管理中的有效整合，揭示了成功因素和所需能力，并提出了AI增强的战略技术管理框架。


<details>
  <summary>Details</summary>
Motivation: 探讨如何利用AI提升战略技术管理的有效性和投资回报。

Method: 通过定量调查数据和定性专家访谈相结合的混合方法研究。

Result: 人工智能（AI）可以有效整合到战略技术管理（STM）实践中，以提高技术投资的战略一致性和有效性。

Conclusion: 人性化的AI增补是未来发展的最佳路径，AI应作为协作伙伴，而非人类判断的替代品。

Abstract: This paper investigates how artificial intelligence (AI) can be effectively integrated into Strategic Technology Management (STM) practices to enhance the strategic alignment and effectiveness of technology investments. Through a mixed-methods approach combining quantitative survey data (n=230) and qualitative expert interviews (n=14), this study addresses three critical research questions: what success factors AI innovates for STM roadmap formulation under uncertainty; what resources and capabilities organizations require for AI-enhanced STM; and how human-AI interaction should be designed for complex STM tasks. The findings reveal that AI fundamentally transforms STM through data-driven strategic alignment and continuous adaptation, while success depends on cultivating proprietary data ecosystems, specialized human talent, and robust governance capabilities. The study introduces the AI-based Strategic Technology Management (AIbSTM) conceptual framework, which synthesizes technical capabilities with human and organizational dimensions across three layers: strategic alignment, resource-based view, and human-AI interaction. Contrary to visions of autonomous AI leadership, the research demonstrates that the most viable trajectory is human-centric augmentation, where AI serves as a collaborative partner rather than a replacement for human judgment. This work contributes to theory by extending the Resource-Based View to AI contexts and addressing cognitive and socio-technical chasms in AI adoption, while offering practitioners a prescriptive framework for navigating AI integration in strategic technology management.

</details>


### [7] [Assessing the Human-Likeness of LLM-Driven Digital Twins in Simulating Health Care System Trust](https://arxiv.org/abs/2512.08939)
*Yuzhou Wu,Mingyang Wu,Di Liu,Rong Yin,Kang Li*

Main category: cs.HC

TL;DR: 本研究评估了LLM驱动的人类数字双胞胎在模拟医疗系统不信任方面的能力，发现其在主要人口特征上可再现人类结果，但在细微差异如教育水平上敏感性不足，导致了对极端选项的选择较少。


<details>
  <summary>Details</summary>
Motivation: 研究LLM驱动的人类数字双胞胎在医疗保健系统中的应用潜力，特别是在模拟复杂心理特征如医疗系统不信任时的表现。

Method: 基于Twin-2K-500数据集，使用医疗保健系统不信任量表（HCSDS）系统性评估LLM驱动人类数字双胞胎的模拟结果，并分析项目级分布、摘要统计和人口亚组模式。

Result: 模拟结果显示，数字双胞胎的反应更集中、方差更低，并且极端选项的选择较少，尽管在主要的人口模式上广泛再现了人类结果，但在教育水平的细微差异上表现出相对较低的敏感性。

Conclusion: 当前的LLM驱动数字双胞胎在模拟复杂人类态度方面存在局限性，需谨慎校准和验证，在健康系统工程中应用前应进行更全面的情感推理机制研究。

Abstract: Serving as an emerging and powerful tool, Large Language Model (LLM)-driven Human Digital Twins are showing great potential in healthcare system research. However, its actual simulation ability for complex human psychological traits, such as distrust in the healthcare system, remains unclear. This research gap particularly impacts health professionals' trust and usage of LLM-based Artificial Intelligence (AI) systems in assisting their routine work. In this study, based on the Twin-2K-500 dataset, we systematically evaluated the simulation results of the LLM-driven human digital twin using the Health Care System Distrust Scale (HCSDS) with an established human-subject sample, analyzing item-level distributions, summary statistics, and demographic subgroup patterns. Results showed that the simulated responses by the digital twin were significantly more centralized with lower variance and had fewer selections of extreme options (all p<0.001). While the digital twin broadly reproduces human results in major demographic patterns, such as age and gender, it exhibits relatively low sensitivity in capturing minor differences in education levels. The LLM-based digital twin simulation has the potential to simulate population trends, but it also presents challenges in making detailed, specific distinctions in subgroups of human beings. This study suggests that the current LLM-driven Digital Twins have limitations in modeling complex human attitudes, which require careful calibration and validation before applying them in inferential analyses or policy simulations in health systems engineering. Future studies are necessary to examine the emotional reasoning mechanism of LLMs before their use, particularly for studies that involve simulations sensitive to social topics, such as human-automation trust.

</details>


### [8] [Psychlysis: Towards the Creation of a Questionnaire-based Machine Learning Tool to Analyze States of Mind](https://arxiv.org/abs/2512.08940)
*Hemakshi Jani,Mitish Karia,Meet Gohil,Rahul Bhadja,Aznam Yacoub,Shafaq Khan*

Main category: cs.HC

TL;DR: 本论文介绍了一种名为Psychlysis的问卷基础机器学习应用，旨在分析用户当前的心理状态，提供个性化的改善情绪建议，重点是改善用户的情绪而不仅仅是检测情绪。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在开发一种能够利用机器学习分析用户心理状态的应用程序，以提供改善情绪的个性化建议，从而提升用户的心理健康。

Method: 应用程序基于问卷和OCEAN模型，分析用户的个性特征，并根据这些特征提供定制的建议。

Result: 初步结果表明，该模型在预测用户情绪和提供个性化建议方面具有潜力。

Conclusion: 该应用程序有潜力为多个社会群体（包括医生、个人和心理健康组织）提供情感健康改善的好处，并减少心理健康问题对日常生活的负面影响。

Abstract: This paper describes the development of Psychlysis, a work-in-progress questionnaire-based machine learning application analyzing the user's current state of mind and suggesting ways to improve their mood using Machine Learning. The application utilizes the OCEAN model to understand the user's personality traits and make customized suggestions to enhance their well-being. The proposed application focus on improving the user's mood rather than just detecting their emotions. Preliminary results of the model are presented, showing the potential of the application in predicting the user's mood and providing personalized recommendations. The paper concludes by highlighting the potential benefits of such an application for various societal segments, including doctors, individuals, and mental health organizations, in improving emotional well-being and reducing the negative impact of mental health issues on daily life.

</details>


### [9] [One Size Fits None: A Personalized Framework for Urban Accessibility Using Exponential Decay](https://arxiv.org/abs/2512.08941)
*Prabhanjana Ghuriki,S. Chanti,Jossy P George*

Main category: cs.HC

TL;DR: 本研究提出了一种个性化的可达性框架，通过实时评估和用户自定义权重，支持可持续城市发展的政策制定。


<details>
  <summary>Details</summary>
Motivation: 旨在通过个性化的可达性框架，满足不同用户的优先需求和生活方式要求，实现基于个体需求的实时城市评估。

Method: 采用网格离散化和两阶段计算架构，将数据密集型预处理与轻量级实时计算分开。

Result: 该框架使得可达性建模对非技术用户变得可用，支持细粒度空间分析和识别社区内的可达性差异。

Conclusion: 研究为实现可持续发展目标11提供了工具，帮助理解不同人群对相同城市空间的体验，从而支持解决可达性差距的政策制定。

Abstract: This study develops a personalized accessibility framework that integrates exponential decay functions with user-customizable weighting systems. The framework enables real-time, personalized urban evaluation based on individual priorities and lifestyle requirements. The methodology employs grid-based discretization and a two-stage computational architecture that separates intensive preprocessing from lightweight real-time calculations. The computational architecture demonstrates that accessibility modelling can be made accessible to non-technical users through interactive interfaces, enabling fine-grained spatial analysis and identification of accessibility variations within neighbourhoods. The research contributes to Sustainable Development Goal 11's vision of inclusive, sustainable cities by providing tools for understanding how different populations experience identical urban spaces, supporting evidence-based policy development that addresses accessibility gaps.

</details>


### [10] [Beyond Technical Debt: How AI-Assisted Development Creates Comprehension Debt in Resource-Constrained Indie Teams](https://arxiv.org/abs/2512.08942)
*Yujie Zhang*

Main category: cs.HC

TL;DR: 本研究提出了CIGDI框架，旨在帮助分布式团队使用AI工具开发游戏，识别了新形式的技术债务“理解债务”，并讨论了AI支持对开发者技能的影响。


<details>
  <summary>Details</summary>
Motivation: 研究针对分布式兼职团队的初级独立游戏开发者，他们缺乏适合特定背景的生产框架。

Method: 提出CIGDI框架，整合AI工具以应对技术债务、协调和疲劳等挑战，基于对三人团队开发2D叙事游戏的三个月反思实践和自我民族志研究，分析157个Jira任务、333个GitHub提交、13个以上的Miro板和8次反思会议。

Result: CIGDI被提出为一个七阶段的迭代过程，围绕人机互动的决策点结构，虽然AI支持降低了认知负担，但发现了新的挑战“理解债务”，这是AI帮助团队构建超出其独立技能水平的系统而引起的脆弱性和依赖性。

Conclusion: CIGDI为资源有限的团队提供了实用的生产框架，并提出关于AI辅助是否构成学习阶梯或依赖陷阱的关键问题。

Abstract: Junior indie game developers in distributed, part-time teams lack production frameworks suited to their specific context, as traditional methodologies are often inaccessible. This study introduces the CIGDI (Co-Intelligence Game Development Ideation) Framework, an alternative approach for integrating AI tools to address persistent challenges of technical debt, coordination, and burnout.
  The framework emerged from a three-month reflective practice and autoethnographic study of a three-person distributed team developing the 2D narrative game "The Worm's Memoirs". Based on analysis of development data (N=157 Jira tasks, N=333 GitHub commits, N=13+ Miro boards, N=8 reflection sessions), CIGDI is proposed as a seven-stage iterative process structured around human-in-the-loop decision points (Priority Criteria and Timeboxing).
  While AI support democratized knowledge access and reduced cognitive load, our analysis identified a significant challenge: "comprehension debt." We define this as a novel form of technical debt where AI helps teams build systems more sophisticated than their independent skill level can create or maintain. This paradox (possessing functional systems the team incompletely understands) creates fragility and AI dependency, distinct from traditional code quality debt.
  This work contributes a practical production framework for resource-constrained teams and identifies critical questions about whether AI assistance constitutes a learning ladder or a dependency trap for developer skill.

</details>


### [11] [SimClinician: A Multimodal Simulation Testbed for Reliable Psychologist AI Collaboration in Mental Health Diagnosis](https://arxiv.org/abs/2512.08953)
*Filippo Cenacchi,Longbing Cao,Deborah Richards*

Main category: cs.HC

TL;DR: SimClinician是一个互动仿真平台，通过整合多模态患者数据，提升心理学家对AI诊断的接受度，降低 escalations。


<details>
  <summary>Details</summary>
Motivation: 在心理健康领域，AI诊断的有效性取决于心理学家对AI建议的反应，但目前研究缺乏对AI诊断界面设计的探讨。

Method: 开发SimClinician平台，集成音频、文本和眼动表达模式的仪表板，结合一套决策层映射AI输出与多模态证据。

Result: 测试结果表明，确认步骤使心理学家对AI建议的接受度提高了23%，并保持互动流畅。

Conclusion: SimClinician提高了心理学家对AI建议的接受度，并优化了诊断流程。

Abstract: AI based mental health diagnosis is often judged by benchmark accuracy, yet in practice its value depends on how psychologists respond whether they accept, adjust, or reject AI suggestions. Mental health makes this especially challenging: decisions are continuous and shaped by cues in tone, pauses, word choice, and nonverbal behaviors of patients. Current research rarely examines how AI diagnosis interface design influences these choices, leaving little basis for reliable testing before live studies. We present SimClinician, an interactive simulation platform, to transform patient data into psychologist AI collaborative diagnosis. Contributions include: (1) a dashboard integrating audio, text, and gaze-expression patterns; (2) an avatar module rendering de-identified dynamics for analysis; (3) a decision layer that maps AI outputs to multimodal evidence, letting psychologists review AI reasoning, and enter a diagnosis. Tested on the E-DAIC corpus (276 clinical interviews, expanded to 480,000 simulations), SimClinician shows that a confirmation step raises acceptance by 23%, keeping escalations below 9%, and maintaining smooth interaction flow.

</details>


### [12] [PoultryTalk: A Multi-modal Retrieval-Augmented Generation (RAG) System for Intelligent Poultry Management and Decision Support](https://arxiv.org/abs/2512.08995)
*Kapalik Khanal,Biswash Khatiwada,Stephen Afrifa,Ranjan Sapkota,Sanjay Shah,Frank Bai,Ramesh Bahadur Bist*

Main category: cs.HC

TL;DR: 开发了PoultryTalk，一个基于多模态的智能系统，提供准确的禽类管理建议，用户满意度高。


<details>
  <summary>Details</summary>
Motivation: 小型和中型 poultry 农场面临缺乏及时专家支持的问题，急需智能数据驱动的系统以应对气候压力和疾病威胁。

Method: 提出PoultryTalk系统，利用多模态的检索增强生成技术实现实时专家指导。

Result: 通过200个专家验证的查询和34名参与者的反馈，PoultryTalk展现出强大的技术性能和用户满意度。

Conclusion: PoultryTalk提供准确的禽类管理建议，并表现出良好的用户接受度和可扩展性。

Abstract: The Poultry industry plays a vital role in global food security, yet small- and medium-scale farmers frequently lack timely access to expert-level support for disease diagnosis, nutrition planning, and management decisions. With rising climate stress, unpredictable feed prices, and persistent disease threats, poultry producers often struggle to make quick, informed decisions. Therefore, there is a critical need for intelligent, data-driven systems that can deliver reliable, on-demand consultation. This paper presents PoultryTalk, a novel multi-modal Retrieval-Augmented Generation (RAG) system designed to provide real-time expert guidance through text and image-based interaction. PoultryTalk uses OpenAI's text-embedding-3-small and GPT-4o to provide smart, context-aware poultry management advice from text, images, or questions. System usability and performance were evaluated using 200 expert-verified queries and feedback from 34 participants who submitted 267 queries to the PoultryTalk prototype. The expert-verified benchmark queries confirmed strong technical performance, achieving a semantic similarity of 84.0% and an average response latency of 3.6 seconds. Compared with OpenAI's GPT-4o, PoultryTalk delivered more accurate and reliable information related to poultry. Based on participants' evaluations, PoultryTalk achieved a response accuracy of 89.9%, with about 9.1% of responses rated as incorrect. A post-use survey indicated high user satisfaction: 95.6% of participants reported that the chatbot provided "always correct" and "mostly correct" answers. 82.6% indicated they would recommend the tool, and 17.4% responded "maybe." These results collectively demonstrate that PoultryTalk not only delivers accurate, contextually relevant information but also demonstrates strong user acceptance and scalability potential.

</details>


### [13] [Prototyping and Evaluating a Real-time Neuro-Adaptive Virtual Reality Flight Training System](https://arxiv.org/abs/2512.09014)
*Evy van Weelden,Jos M. Prinsen,Caterina Ceccato,Ethel Pruss,Anita Vrins,Maryam Alimardani,Travis J. Wiltshire,Max M. Louwerse*

Main category: cs.HC

TL;DR: 本研究评估了一种基于脑机接口的实时适应性飞行训练系统，结果显示在主观和客观测量上与固定训练序列无显著差异，但飞行员偏好适应性系统，显示出个性化训练的潜力。


<details>
  <summary>Details</summary>
Motivation: 实时调整飞行训练中的任务难度对优化表现和管理飞行员工作负荷至关重要。

Method: 开发并测试基于EEG的神经适应训练系统，比较适应性训练与固定难度增加的训练序列，在虚拟现实飞行模拟中进行评估。

Result: 在主观测量和飞行表现上，适应性和固定顺序的训练模式没有显著差异，主观工作负荷增加时，飞行表现下降。

Conclusion: 尽管研究结果表明适应性训练与固定顺序在性能上没有显著差异，但BCI驱动的飞行训练系统有潜力提供更个性化和多样化的训练体验。

Abstract: Real-time adjustments to task difficulty during flight training are crucial for optimizing performance and managing pilot workload. This study evaluated the functionality of a pre-trained brain-computer interface (BCI) that adapts training difficulty based on real-time estimations of workload from brain signals. Specifically, an EEG-based neuro-adaptive training system was developed and tested in Virtual Reality (VR) flight simulations with military student pilots. The neuro-adaptive system was compared to a fixed sequence that progressively increased in difficulty, in terms of self-reported user engagement, workload, and simulator sickness (subjective measures), as well as flight performance (objective metric). Additionally, we explored the relationships between subjective workload and flight performance in the VR simulator for each condition. The experiments concluded with semi-structured interviews to elicit the pilots' experience with the neuro-adaptive prototype. Results revealed no significant differences between the adaptive and fixed sequence conditions in subjective measures or flight performance. In both conditions, flight performance decreased as subjective workload increased. The semi-structured interviews indicated that, upon briefing, the pilots preferred the neuro-adaptive VR training system over the system with a fixed sequence, although individual differences were observed in the perception of difficulty and the order of changes in difficulty. Even though this study shows performance does not change, BCI-based flight training systems hold the potential to provide a more personalized and varied training experience.

</details>


### [14] [Mental Models of Autonomy and Sentience Shape Reactions to AI](https://arxiv.org/abs/2512.09085)
*Janet V. T. Pauketat,Daniel B. Shank,Aikaterina Manoli,Jacy Reese Anthis*

Main category: cs.HC

TL;DR: 研究表明，AI的自主性和感知能力对人类反应有不同影响，感知能力的激活增强了人类对AI的道德考虑和认知，而自主性则增加了威胁感。


<details>
  <summary>Details</summary>
Motivation: 探讨AI的自主性与感知能力之间的关系，以及它们对人类反应的影响。

Method: 进行了三项预实验和四项预注册的情境实验，共涉及3076名参与者，描述AI的自主性和感知能力的不同组合。

Result: 感知能力的激活比自主性更能提高心智感知和道德考虑，而自主性则更能增加威胁感。

Conclusion: 通过区分AI的不同心智模型，我们可以更准确地研究人机互动，为人性化AI的设计提供指导。

Abstract: Narratives about artificial intelligence (AI) entangle autonomy, the capacity to self-govern, with sentience, the capacity to sense and feel. AI agents that perform tasks autonomously and companions that recognize and express emotions may activate mental models of autonomy and sentience, respectively, provoking distinct reactions. To examine this possibility, we conducted three pilot studies (N = 374) and four preregistered vignette experiments describing an AI as autonomous, sentient, both, or neither (N = 2,702). Activating a mental model of sentience increased general mind perception (cognition and emotion) and moral consideration more than autonomy, but autonomy increased perceived threat more than sentience. Sentience also increased perceived autonomy more than vice versa. Based on a within-paper meta-analysis, sentience changed reactions more than autonomy on average. By disentangling different mental models of AI, we can study human-AI interaction with more precision to better navigate the detailed design of anthropomorphized AI and prompting interfaces.

</details>


### [15] [Understanding Mental States in Active and Autonomous Driving with EEG](https://arxiv.org/abs/2512.09190)
*Prithila Angkan,Paul Hungler,Ali Etemad*

Main category: cs.HC

TL;DR: 本研究首次通过EEG比较主动与自动驾驶中的心理状态，结果显示二者在复杂任务下虽然趋势相似，但心理状态强度与神经激活模式显著不同，强调针对不同场景的数据和模型的重要性。


<details>
  <summary>Details</summary>
Motivation: 了解驾驶员在主动和自动驾驶模式下心理状态的差异对于设计安全的人车交互界面至关重要。

Method: 通过31名参与者在不同复杂度的驾驶模式下进行相同任务的数据，利用EEG比较主动驾驶和自动驾驶的认知负荷、疲劳、情感价值和唤醒度。

Result: 研究发现，虽然两种模式在复杂性水平上呈现出相似的趋势，但心理状态的强度及其背后的神经激活存在显著差异，这表明主动驾驶和自动驾驶之间存在明显的分布转变。

Conclusion: 本研究强调了在开发下一代自动驾驶车辆驾驶员监测系统时，需要基于特定场景的数据和模型。

Abstract: Understanding how driver mental states differ between active and autonomous driving is critical for designing safe human-vehicle interfaces. This paper presents the first EEG-based comparison of cognitive load, fatigue, valence, and arousal across the two driving modes. Using data from 31 participants performing identical tasks in both scenarios of three different complexity levels, we analyze temporal patterns, task-complexity effects, and channel-wise activation differences. Our findings show that although both modes evoke similar trends across complexity levels, the intensity of mental states and the underlying neural activation differ substantially, indicating a clear distribution shift between active and autonomous driving. Transfer-learning experiments confirm that models trained on active driving data generalize poorly to autonomous driving and vice versa. We attribute this distribution shift primarily to differences in motor engagement and attentional demands between the two driving modes, which lead to distinct spatial and temporal EEG activation patterns. Although autonomous driving results in lower overall cortical activation, participants continue to exhibit measurable fluctuations in cognitive load, fatigue, valence, and arousal associated with readiness to intervene, task-evoked emotional responses, and monotony-related passive fatigue. These results emphasize the need for scenario-specific data and models when developing next-generation driver monitoring systems for autonomous vehicles.

</details>


### [16] [Advancing Research via Human-AI Interactive Theorem Proving](https://arxiv.org/abs/2512.09443)
*Chenyi Li,Zhijian Lai,Dong An,Jiang Hu,Zaiwen Wen*

Main category: cs.HC

TL;DR: 该论文提出一种人机交互的工作流程，将大型语言模型应用于数学研究，确保数学严谨性，并通过案例研究展示其在量子计算中的应用。


<details>
  <summary>Details</summary>
Motivation: 探讨如何在科学计算中使用大型语言模型作为研究工具，同时保持数学严谨性。

Method: 人机互动的工作流程，结合专家控制问题的表述和可接受的假设，模型负责搜索证明或矛盾，提出候选性质和定理，帮助构建满足约束的结构和参数。

Result: 通过案例研究，识别不变子空间，探索Grover兼容的回撤，获得回撤基础梯度方法的收敛保证。

Conclusion: 该工作提供了一个将大型语言模型整合到前沿数学研究的实际模板，从而加速了证明空间和算法设计的探索，同时保持了透明的推理责任。

Abstract: We investigate how large language models can be used as research tools in scientific computing while preserving mathematical rigor. We propose a human-in-the-loop workflow for interactive theorem proving and discovery with LLMs. Human experts retain control over problem formulation and admissible assumptions, while the model searches for proofs or contradictions, proposes candidate properties and theorems, and helps construct structures and parameters that satisfy explicit constraints, supported by numerical experiments and simple verification checks. Experts treat these outputs as raw material, further refine them, and organize the results into precise statements and rigorous proofs. We instantiate this workflow in a case study on the connection between manifold optimization and Grover's quantum search algorithm, where the pipeline helps identify invariant subspaces, explore Grover-compatible retractions, and obtain convergence guarantees for the retraction-based gradient method. The framework provides a practical template for integrating large language models into frontier mathematical research, enabling faster exploration of proof space and algorithm design while maintaining transparent reasoning responsibilities. Although illustrated on manifold optimization problems in quantum computing, the principles extend to other core areas of scientific computing.

</details>


### [17] [An Efficient Interaction Human-AI Synergy System Bridging Visual Awareness and Large Language Model for Intensive Care Units](https://arxiv.org/abs/2512.09473)
*Yibowen Zhao,Yiming Cao,Zhiqi Shen,Juan Du,Yonghui Xu,Lizhen Cui,Cyril Leung*

Main category: cs.HC

TL;DR: 提出了一种新的人机智能协同系统，旨在改善重症监护室的数据管理，降低认知负担，提升患者安全性。


<details>
  <summary>Details</summary>
Motivation: 当前重症监护室的手动数据抄录和信息系统碎片化，存在潜在的患者安全和运营效率风险。

Method: 提出一个基于云-边缘-端架构的人机智能协同系统，集成视觉感知数据提取和语义交互机制。

Result: 系统通过实时捕获生理数据和语音查询提高了数据访问性和减少了手动输入错误，确保低延迟通信和可扩展性。

Conclusion: 该系统降低了重症监护室护士和医生的认知负担，并展示了在智能医疗系统中更广泛应用的潜力。

Abstract: Intensive Care Units (ICUs) are critical environments characterized by high-stakes monitoring and complex data management. However, current practices often rely on manual data transcription and fragmented information systems, introducing potential risks to patient safety and operational efficiency. To address these issues, we propose a human-AI synergy system based on a cloud-edge-end architecture, which integrates visual-aware data extraction and semantic interaction mechanisms. Specifically, a visual-aware edge module non-invasively captures real-time physiological data from bedside monitors, reducing manual entry errors. To improve accessibility to fragmented data sources, a semantic interaction module, powered by a Large Language Model (LLM), enables physicians to perform efficient and intuitive voice-based queries over structured patient data. The hierarchical cloud-edge-end deployment ensures low-latency communication and scalable system performance. Our system reduces the cognitive burden on ICU nurses and physicians and demonstrates promising potential for broader applications in intelligent healthcare systems.

</details>


### [18] [Exploring Community-Powered Conversational Agent for Health Knowledge Acquisition: A Case Study in Colorectal Cancer](https://arxiv.org/abs/2512.09511)
*Yiwei Yuan,Zhiqing Wang,Xiucheng Zhang,Yichao Luo,Shuya Lin,Yang Bai,Zhenhui Peng*

Main category: cs.HC

TL;DR: 本研究开发了一种名为CanAnswer的对话代理，通过整合社区内容，提高年轻人在健康知识学习中的效率及可靠性。


<details>
  <summary>Details</summary>
Motivation: 年轻成年人在网上社区中获取健康知识时，面临内容分散、信息质量参差不齐和术语不熟悉等挑战。

Method: 通过对56名参与者的调查和与6名医疗专家的访谈，结合使用CanAnswer的实验研究，评估其有效性。

Result: CanAnswer有效提高用户的健康知识回忆，同时降低学习过程中的工作负担。

Conclusion: CanAnswer提高了知识回忆率，并减轻了学习过程中的任务负担，同时获得了专家的认可与确认。

Abstract: Online communities have become key platforms where young adults, actively seek and share information, including health knowledge. However, these users often face challenges when browsing these communities, such as fragmented content, varying information quality and unfamiliar terminology. Based on a survey with 56 participants and follow-up interviews, we identify common challenges and expected features for learning health knowledge. In this paper, we develop a computational workflow that integrates community content into a conversational agent named CanAnswer to facilitate health knowledge acquisition. Using colorectal cancer as a case study, we evaluate CanAnswer through a lab study with 24 participants and interviews with six medical experts. Results show that CanAnswer improves the recalled gained knowledge and reduces the task workload of the learning session. Our expert interviews (N=6) further confirm the reliability and usefulness of CanAnswer. We discuss the generality of CanAnswer and provide design considerations for enhancing the usefulness and credibility of community-powered learning tools.

</details>


### [19] [Auto-BenchmarkCard: Automated Synthesis of Benchmark Documentation](https://arxiv.org/abs/2512.09577)
*Aris Hofmann,Inge Vejsbjerg,Dhaval Salwala,Elizabeth M. Daly*

Main category: cs.HC

TL;DR: Auto-BenchmarkCard是一种生成AI基准验证描述的工作流程，解决了基准文档不完整或不一致的问题，促进了透明度、可比性和可重用性。


<details>
  <summary>Details</summary>
Motivation: 基准文档经常不完整或不一致，使得在任务或领域之间解释和比较基准变得困难。

Method: 结合来自异质源的多代理数据提取和LLM驱动的综合，自动生成有效的基准描述，并通过FactReasoner工具进行验证。

Result: 此工作流程潜在地提升了AI基准报告的透明度、可比性和可重用性。

Conclusion: Auto-BenchmarkCard的工作流程通过多代理数据提取和LLM驱动的综合，显著提高了AI基准文档的透明度和可比较性，从而帮助研究人员更好地评估基准选择。

Abstract: We present Auto-BenchmarkCard, a workflow for generating validated descriptions of AI benchmarks. Benchmark documentation is often incomplete or inconsistent, making it difficult to interpret and compare benchmarks across tasks or domains. Auto-BenchmarkCard addresses this gap by combining multi-agent data extraction from heterogeneous sources (e.g., Hugging Face, Unitxt, academic papers) with LLM-driven synthesis. A validation phase evaluates factual accuracy through atomic entailment scoring using the FactReasoner tool. This workflow has the potential to promote transparency, comparability, and reusability in AI benchmark reporting, enabling researchers and practitioners to better navigate and evaluate benchmark choices.

</details>


### [20] [ImageTalk: Designing a Multimodal AAC Text Generation System Driven by Image Recognition and Natural Language Generation](https://arxiv.org/abs/2512.09610)
*Boyin Yang,Puming Jiang,Per Ola Kristensson*

Main category: cs.HC

TL;DR: 本文提出了ImageTalk系统，通过代理用户和最终用户的设计方法，有效提升了重症运动神经元病患者的沟通效率和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机在于传统的符号基础辅助和替代沟通系统词汇有限，而文本输入解决方案通常通信效率低下。

Method: 通过定制化的代理用户和最终用户设计阶段，迭代设计和开发ImageTalk系统。

Result: ImageTalk系统展示了95.6%的击键节省，同时保持一致的性能和用户满意度。

Conclusion: 论文提出了一个新型的多模态文本生成系统ImageTalk，该系统有效提高了重症运动神经元病患者的沟通效率，且得到了用户的高度满意。

Abstract: People living with Motor Neuron Disease (plwMND) frequently encounter speech and motor impairments that necessitate a reliance on augmentative and alternative communication (AAC) systems. This paper tackles the main challenge that traditional symbol-based AAC systems offer a limited vocabulary, while text entry solutions tend to exhibit low communication rates. To help plwMND articulate their needs about the system efficiently and effectively, we iteratively design and develop a novel multimodal text generation system called ImageTalk through a tailored proxy-user-based and an end-user-based design phase. The system demonstrates pronounced keystroke savings of 95.6%, coupled with consistent performance and high user satisfaction. We distill three design guidelines for AI-assisted text generation systems design and outline four user requirement levels tailored for AAC purposes, guiding future research in this field.

</details>


### [21] [Smart, simple, sincere - Why and how we should rethink connected things in our smart homes](https://arxiv.org/abs/2512.09755)
*Albrecht Kurze,Andreas Bischof,Arne Berger*

Main category: cs.HC

TL;DR: 智能家居设备的普及带来了隐私风险，本文呼吁重新思考其应用，并提出应对策略。


<details>
  <summary>Details</summary>
Motivation: 随着智能设备的普及，它们在提高家庭舒适性和安全性的同时，也带来了隐私风险，亟需关注和解决。

Method: 通过观察和研究智能设备及其引发的隐私问题，提出解决方案和建议。

Result: 提出了具体的研究项目和方法，以帮助社区重新思考如何更加安全地实现智能家居。

Conclusion: 本研究呼吁重新思考智能连接设备和服务在家庭中的应用，提出了一些方法和研究项目来应对隐私风险。

Abstract: More and more smart connected things and services turn our homes into smart environments. They promise comfort, efficiency and security. These devices often integrate simple sensors, e.g. for temperature, light or humidity, etc. However, these smart but yet simple sensors can pose a sincere privacy risk. The sensor data enables sense-making of home attendance, domestic activities and even health conditions, often a fact that neither users nor developers are aware of or do not know how to address. Nevertheless, not all is lost or evil. This article makes a plea for how we, the ThingsCon community, might rethink smart connected things and services in our homes. We show this in our approaches and research projects that we initiated.

</details>


### [22] [Building a Data Dashboard for Magic: The Gathering: Initial Design Considerations](https://arxiv.org/abs/2512.09802)
*Tomás Alves,João Moreira*

Main category: cs.HC

TL;DR: 研究旨在为Magic: The Gathering Commander格式开发可视化仪表板，通过用户测试发现玩家优先考虑与上下文相关的指标，并提供了效果好的仪表板设计指南。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一个可视化仪表板，以满足Magic: The Gathering Commander格式玩家的数据分析需求。

Method: 进行用户任务分析，设计仪表板并进行结构化用户测试以评估玩家对数据可视化的理解和偏好。

Result: 玩家更重视上下文相关及结果驱动的指标，经典图表的理解度高于复杂的图表形式，强调了本地化视图和用户自定义的重要性。

Conclusion: 本研究提出了关于游戏数据可视化仪表板的初步设计阶段，强调了适应性和上下文相关性的重要性，并提供了设计指南。

Abstract: This paper presents the initial stages of a design study aimed at developing a dashboard to visualize gameplay data of the Commander format from Magic: The Gathering. We conducted a user-task analysis to identify requirements for a data visualization dashboard tailored to the Commander format. Afterwards, we proposed a design for the dashboard leveraging visualizations to address players' needs and pain points for typical data analysis tasks in the context domain. Then, we followed-up with a structured user test to evaluate players' comprehension and preferences of data visualizations. Results show that players prioritize contextually relevant, outcome-driven metrics over peripheral ones, and that canonical charts like heatmaps and line charts support higher comprehension than complex ones such as scatterplots or icicle plots. Our findings also highlight the importance of localized views, user customization, and progressive disclosure, emphasizing that adaptability and contextual relevance are as essential as accuracy in effective dashboard design. Our study contributes practical design guidelines for data visualization in gaming contexts and highlights broader implications for engagement-driven dashboards.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [23] [ShelfAware: Real-Time Visual-Inertial Semantic Localization in Quasi-Static Environments with Low-Cost Sensors](https://arxiv.org/abs/2512.09065)
*Shivendra Agrawal,Jake Brawer,Ashutosh Naik,Alessandro Roncone,Bradley Hayes*

Main category: cs.RO

TL;DR: ShelfAware是一种语义粒子滤波器，利用场景语义的统计证据进行稳健的全球定位，在零散且动态的室内环境中实现高成功率和快速收敛。


<details>
  <summary>Details</summary>
Motivation: 室内工作空间往往是准静态的，虽然全局布局稳定，但局部语义不断变化，这导致视觉定位困难。

Method: ShelfAware结合深度似然性与基于类别的语义相似性，利用预先计算的语义视点库执行逆语义提议，进行快速假设生成。

Result: 在100次试验中，ShelfAware在各种条件下达成96%的成功率，并在所有条件下实现最低的平移均方根误差(RMSE)。

Conclusion: ShelfAware通过在类别层面建模语义并利用逆提议解决几何混叠和语义漂移问题，适用于多种室内环境，并在实际应用中展现了良好的稳定性和实时性。

Abstract: Many indoor workspaces are quasi-static: global layout is stable but local semantics change continually, producing repetitive geometry, dynamic clutter, and perceptual noise that defeat vision-based localization. We present ShelfAware, a semantic particle filter for robust global localization that treats scene semantics as statistical evidence over object categories rather than fixed landmarks. ShelfAware fuses a depth likelihood with a category-centric semantic similarity and uses a precomputed bank of semantic viewpoints to perform inverse semantic proposals inside MCL, yielding fast, targeted hypothesis generation on low-cost, vision-only hardware. Across 100 global-localization trials spanning four conditions (cart-mounted, wearable, dynamic obstacles, and sparse semantics) in a semantically dense, retail environment, ShelfAware achieves a 96% success rate (vs. 22% MCL and 10% AMCL) with a mean time-to-convergence of 1.91s, attains the lowest translational RMSE in all conditions, and maintains stable tracking in 80% of tested sequences, all while running in real time on a consumer laptop-class platform. By modeling semantics distributionally at the category level and leveraging inverse proposals, ShelfAware resolves geometric aliasing and semantic drift common to quasi-static domains. Because the method requires only vision sensors and VIO, it integrates as an infrastructure-free building block for mobile robots in warehouses, labs, and retail settings; as a representative application, it also supports the creation of assistive devices providing start-anytime, shared-control assistive navigation for people with visual impairments.

</details>


### [24] [Inferring Operator Emotions from a Motion-Controlled Robotic Arm](https://arxiv.org/abs/2512.09086)
*Xinyu Qi,Zeyu Deng,Shaun Alexander Macdonald,Liying Li,Chen Wang,Muhammad Ali Imran,Philip G. Zhao*

Main category: cs.RO

TL;DR: 本研究探讨了如何通过远程控制机器人的运动推断操作员的情感状态，提出了一种新方法，达到了83.3%的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 探讨远程机器人控制中操作员情感状态的影响，并寻求有效的情感识别方法。

Method: 利用机器学习系统，通过远程控制机器人的运动推断操作员的情感状态。

Result: 该系统在识别用户情感状态时达到了83.3%的准确率。

Conclusion: 该系统能够通过机器人运动推断远程操作员的情感状态，具有重要的应用潜力。

Abstract: A remote robot operator's affective state can significantly impact the resulting robot's motions leading to unexpected consequences, even when the user follows protocol and performs permitted tasks. The recognition of a user operator's affective states in remote robot control scenarios is, however, underexplored. Current emotion recognition methods rely on reading the user's vital signs or body language, but the devices and user participation these measures require would add limitations to remote robot control. We demonstrate that the functional movements of a remote-controlled robotic avatar, which was not designed for emotional expression, can be used to infer the emotional state of the human operator via a machine-learning system. Specifically, our system achieved 83.3$\%$ accuracy in recognizing the user's emotional state expressed by robot movements, as a result of their hand motions. We discuss the implications of this system on prominent current and future remote robot operation and affective robotic contexts.

</details>


### [25] [Masked Generative Policy for Robotic Control](https://arxiv.org/abs/2512.09101)
*Lipeng Zhuang,Shiyu Fan,Florent P. Audonnet,Yingdong Ru,Gerardo Aragon Camarasa,Paul Henderson*

Main category: cs.RO

TL;DR: MGP 是一种新型视觉运动模仿学习框架，通过并行生成和快速优化低置信度动作，适用于复杂任务，显著提升了成功率和推理速度。


<details>
  <summary>Details</summary>
Motivation: 旨在改善在复杂和非马尔可夫任务中的视觉运动控制能力，克服现有方法的局限性。

Method: 使用条件掩码变换器生成并优化动作令牌，提出 MGP-Short 和 MGP-Long 两种新取样范式以适应不同的任务需求。

Result: Masked Generative Policy (MGP) 提出了一种新的视觉运动模仿学习框架，该框架通过并行生成低置信度动作令牌快速优化以实现高效控制。

Conclusion: MGP 在 150 个机器人操作任务中表现出色，在复杂的非马尔可夫任务上超越了现有方法，展示了其在快速推理和成功率方面的优势。

Abstract: We present Masked Generative Policy (MGP), a novel framework for visuomotor imitation learning. We represent actions as discrete tokens, and train a conditional masked transformer that generates tokens in parallel and then rapidly refines only low-confidence tokens. We further propose two new sampling paradigms: MGP-Short, which performs parallel masked generation with score-based refinement for Markovian tasks, and MGP-Long, which predicts full trajectories in a single pass and dynamically refines low-confidence action tokens based on new observations. With globally coherent prediction and robust adaptive execution capabilities, MGP-Long enables reliable control on complex and non-Markovian tasks that prior methods struggle with. Extensive evaluations on 150 robotic manipulation tasks spanning the Meta-World and LIBERO benchmarks show that MGP achieves both rapid inference and superior success rates compared to state-of-the-art diffusion and autoregressive policies. Specifically, MGP increases the average success rate by 9% across 150 tasks while cutting per-sequence inference time by up to 35x. It further improves the average success rate by 60% in dynamic and missing-observation environments, and solves two non-Markovian scenarios where other state-of-the-art methods fail.

</details>


### [26] [Cognitive Trust in HRI: "Pay Attention to Me and I'll Trust You Even if You are Wrong"](https://arxiv.org/abs/2512.09105)
*Adi Manor,Dan Cohen,Ziv Keidar,Avi Parush,Hadas Erel*

Main category: cs.RO

TL;DR: 研究表明，机器人的注意力和能力影响人类对机器人的认知信任，高注意力可以弥补低能力的劣势。


<details>
  <summary>Details</summary>
Motivation: 探讨影响人机互动中认知信任的因素，尤其是情感因素与绩效因素的互动。

Method: 通过实验评估机器人的能力和注意力在认知信任中的作用，使用2x2实验设计进行搜索任务。

Result: 高注意力的机器人即便在能力低下的情况下，仍能与能力高的机器人获得相似的信任水平。

Conclusion: 认知信任的建立过程比以往认为的更复杂，情感过程在其中发挥重要作用，应与传统的基于能力的模型同时考虑。

Abstract: Cognitive trust and the belief that a robot is capable of accurately performing tasks, are recognized as central factors in fostering high-quality human-robot interactions. It is well established that performance factors such as the robot's competence and its reliability shape cognitive trust. Recent studies suggest that affective factors, such as robotic attentiveness, also play a role in building cognitive trust. This work explores the interplay between these two factors that shape cognitive trust. Specifically, we evaluated whether different combinations of robotic competence and attentiveness introduce a compensatory mechanism, where one factor compensates for the lack of the other. In the experiment, participants performed a search task with a robotic dog in a 2x2 experimental design that included two factors: competence (high or low) and attentiveness (high or low). The results revealed that high attentiveness can compensate for low competence. Participants who collaborated with a highly attentive robot that performed poorly reported trust levels comparable to those working with a highly competent robot. When the robot did not demonstrate attentiveness, low competence resulted in a substantial decrease in cognitive trust. The findings indicate that building cognitive trust in human-robot interaction may be more complex than previously believed, involving emotional processes that are typically overlooked. We highlight an affective compensatory mechanism that adds a layer to consider alongside traditional competence-based models of cognitive trust.

</details>


### [27] [Semantic Trajectory Generation for Goal-Oriented Spacecraft Rendezvous](https://arxiv.org/abs/2512.09111)
*Yuji Takubo,Arpit Dwivedi,Sukeerth Ramkumar,Luis A. Pabon,Daniele Gammelli,Marco Pavone,Simone D'Amico*

Main category: cs.RO

TL;DR: 本论文提出了一种新的轨迹生成框架SAGES，能够将自然语言命令转化为遵循非凸约束的航天器轨迹，实现高层意图的反映，并且减少了专家输入，提升了操作的可扩展性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 未来自主航天器需要可靠的实时轨迹生成，但现有方法依赖广泛的专家输入，限制了在实际交会任务中的操作可扩展性。

Method: 介绍了SAGES（语义自主引导引擎），一个将自然语言命令转换为反映高层意图的航天器轨迹的框架，同时遵循非凸约束。

Result: 实验表明，SAGES在两种设置下（容错接近操作和自由飞行机器人平台）能够可靠地产生与人类命令对齐的轨迹，在不同行为模式下实现超过90%的语义行为一致性。

Conclusion: 该研究标志着朝着语言条件下的约束感知航天器轨迹生成的初步进展，使操作员能够通过直观的自然语言命令互动引导安全性和行为，减少专家负担。

Abstract: Reliable real-time trajectory generation is essential for future autonomous spacecraft. While recent progress in nonconvex guidance and control is paving the way for onboard autonomous trajectory optimization, these methods still rely on extensive expert input (e.g., waypoints, constraints, mission timelines, etc.), which limits the operational scalability in real rendezvous missions.This paper introduces SAGES (Semantic Autonomous Guidance Engine for Space), a trajectory-generation framework that translates natural-language commands into spacecraft trajectories that reflect high-level intent while respecting nonconvex constraints. Experiments in two settings -- fault-tolerant proximity operations with continuous-time constraint enforcement and a free-flying robotic platform -- demonstrate that SAGES reliably produces trajectories aligned with human commands, achieving over 90\% semantic-behavioral consistency across diverse behavior modes. Ultimately, this work marks an initial step toward language-conditioned, constraint-aware spacecraft trajectory generation, enabling operators to interactively guide both safety and behavior through intuitive natural-language commands with reduced expert burden.

</details>


### [28] [UPETrack: Unidirectional Position Estimation for Tracking Occluded Deformable Linear Objects](https://arxiv.org/abs/2512.09283)
*Fan Wu,Chenguang Yang,Haibin Yang,Shuo Wang,Yanrui Xu,Xing Zhou,Meng Gao,Yaoqi Xian,Zhihong Zhu,Shifeng Huang*

Main category: cs.RO

TL;DR: 本研究提出了一种名为UPETrack的实时跟踪框架，用于形变线性物体(DLOs)的状态追踪，显著提高了跟踪精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 实时跟踪形变线性物体在工业组装、医疗程序和日常生活中至关重要，但高维配置空间和非线性动力学等障碍阻碍了有效跟踪。

Method: UPETrack基于单向位置估计(UPE)框架，分为两个阶段：可见部分跟踪使用高斯混合模型(GMM)及期望最大化(EM)算法，遮挡区域预测使用UPE算法。

Result: 实验结果显示，UPETrack在定位精度和计算效率上超过了TrackDLO和CDCPD2等两种最先进的追踪算法。

Conclusion: UPETrack通过利用几何连续性和时间演变模式，实现了高效且稳定的遮挡节点估计，为DLOs的实时跟踪提供了新的解决方案。

Abstract: Real-time state tracking of Deformable Linear Objects (DLOs) is critical for enabling robotic manipulation of DLOs in industrial assembly, medical procedures, and daily-life applications. However, the high-dimensional configuration space, nonlinear dynamics, and frequent partial occlusions present fundamental barriers to robust real-time DLO tracking. To address these limitations, this study introduces UPETrack, a geometry-driven framework based on Unidirectional Position Estimation (UPE), which facilitates tracking without the requirement for physical modeling, virtual simulation, or visual markers. The framework operates in two phases: (1) visible segment tracking is based on a Gaussian Mixture Model (GMM) fitted via the Expectation Maximization (EM) algorithm, and (2) occlusion region prediction employing UPE algorithm we proposed. UPE leverages the geometric continuity inherent in DLO shapes and their temporal evolution patterns to derive a closed-form positional estimator through three principal mechanisms: (i) local linear combination displacement term, (ii) proximal linear constraint term, and (iii) historical curvature term. This analytical formulation allows efficient and stable estimation of occluded nodes through explicit linear combinations of geometric components, eliminating the need for additional iterative optimization. Experimental results demonstrate that UPETrack surpasses two state-of-the-art tracking algorithms, including TrackDLO and CDCPD2, in both positioning accuracy and computational efficiency.

</details>


### [29] [One-Shot Real-World Demonstration Synthesis for Scalable Bimanual Manipulation](https://arxiv.org/abs/2512.09297)
*Huayi Zhou,Kui Jia*

Main category: cs.RO

TL;DR: BiDemoSyn是一种从单个真实示例中合成可行的双手操作演示的框架，解决了遥操作和仿真之间的权衡，实现了高效且真实的模仿学习。


<details>
  <summary>Details</summary>
Motivation: 学习灵巧的双手操作策略依赖于大规模、高质量的演示，目前的范式存在固有的权衡：遥操作提供了物理基础的数据，但劳动强度大，而基于仿真的合成则具有更高的效率，但存在仿真与现实之间的差距。

Method: 我们提出了BiDemoSyn框架，从一个真实世界的示例中合成接触丰富、物理可行的双手演示。关键思想是将任务分解为不变的协调块和可变的、依赖于对象的调整，并通过视觉引导对齐和轻量级轨迹优化进行适应。

Result: 在六个双臂任务中，我们展示了基于BiDemoSyn数据训练的策略能够有效地泛化到新颖的对象姿态和形状，显著优于近期的基线。

Conclusion: BiDemoSyn在效率和现实世界的真实性之间架起了桥梁，为复杂的双手操作提供了一条可扩展的模仿学习路径，而不妨碍物理基础的建立。

Abstract: Learning dexterous bimanual manipulation policies critically depends on large-scale, high-quality demonstrations, yet current paradigms face inherent trade-offs: teleoperation provides physically grounded data but is prohibitively labor-intensive, while simulation-based synthesis scales efficiently but suffers from sim-to-real gaps. We present BiDemoSyn, a framework that synthesizes contact-rich, physically feasible bimanual demonstrations from a single real-world example. The key idea is to decompose tasks into invariant coordination blocks and variable, object-dependent adjustments, then adapt them through vision-guided alignment and lightweight trajectory optimization. This enables the generation of thousands of diverse and feasible demonstrations within several hour, without repeated teleoperation or reliance on imperfect simulation. Across six dual-arm tasks, we show that policies trained on BiDemoSyn data generalize robustly to novel object poses and shapes, significantly outperforming recent baselines. By bridging the gap between efficiency and real-world fidelity, BiDemoSyn provides a scalable path toward practical imitation learning for complex bimanual manipulation without compromising physical grounding.

</details>


### [30] [Scene-agnostic Hierarchical Bimanual Task Planning via Visual Affordance Reasoning](https://arxiv.org/abs/2512.09310)
*Kwang Bin Lee,Jiho Kang,Sung-Hee Lee*

Main category: cs.RO

TL;DR: 本文提出了一种统一框架，通过结合视觉点定位、双手子目标规划和交互点驱动双手提示，实现了在无场景的情况下，生成语义、物理可行的双手操作计划。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人任务规划器主要是单手操作，未能解决双手操作在空间、几何和协调上的挑战，因此需要一个统一的框架来处理这一问题。

Method: 我们提出的框架集成了三个关键模块：视觉点定位（VPG）、双手子目标规划（BSP）和交互点驱动双手提示（IPBP）。

Result: 实验表明，这种方法能生成一致、可行且紧凑的双手计划，并在复杂场景中无需重新训练就能进行推广。

Conclusion: 该方法能够生成语义上有意义、物理上可行且可平行化的双手操作计划，并在未见场景中表现出强大的场景无关性理解能力。

Abstract: Embodied agents operating in open environments must translate high-level instructions into grounded, executable behaviors, often requiring coordinated use of both hands. While recent foundation models offer strong semantic reasoning, existing robotic task planners remain predominantly unimanual and fail to address the spatial, geometric, and coordination challenges inherent to bimanual manipulation in scene-agnostic settings. We present a unified framework for scene-agnostic bimanual task planning that bridges high-level reasoning with 3D-grounded two-handed execution. Our approach integrates three key modules. Visual Point Grounding (VPG) analyzes a single scene image to detect relevant objects and generate world-aligned interaction points. Bimanual Subgoal Planner (BSP) reasons over spatial adjacency and cross-object accessibility to produce compact, motion-neutralized subgoals that exploit opportunities for coordinated two-handed actions. Interaction-Point-Driven Bimanual Prompting (IPBP) binds these subgoals to a structured skill library, instantiating synchronized unimanual or bimanual action sequences that satisfy hand-state and affordance constraints. Together, these modules enable agents to plan semantically meaningful, physically feasible, and parallelizable two-handed behaviors in cluttered, previously unseen scenes. Experiments show that it produces coherent, feasible, and compact two-handed plans, and generalizes to cluttered scenes without retraining, demonstrating robust scene-agnostic affordance reasoning for bimanual tasks.

</details>


### [31] [Development and Testing for Perception Based Autonomous Landing of a Long-Range QuadPlane](https://arxiv.org/abs/2512.09343)
*Ashik E Rasul,Humaira Tasnim,Ji Yu Kim,Young Hyun Lim,Scott Schmitz,Bruce W. Jo,Hyung-Jin Yoon*

Main category: cs.RO

TL;DR: 本文提出了一种轻量级的QuadPlane系统，旨在解决在无GPS和动态环境中进行高效自主着陆的挑战。


<details>
  <summary>Details</summary>
Motivation: 在GPS无法使用的环境中（如城市复杂区域），实现可靠的自主着陆需要良好的感知能力和优化的技术框架。

Method: 开发轻量级的QuadPlane系统

Result: 实现基于视觉的自主着陆和视觉-惯性里程计

Conclusion: 建立了在动态、无GPS环境中自主着陆的基础

Abstract: QuadPlanes combine the range efficiency of fixed-wing aircraft with the maneuverability of multi-rotor platforms for long-range autonomous missions. In GPS-denied or cluttered urban environments, perception-based landing is vital for reliable operation. Unlike structured landing zones, real-world sites are unstructured and highly variable, requiring strong generalization capabilities from the perception system. Deep neural networks (DNNs) provide a scalable solution for learning landing site features across diverse visual and environmental conditions. While perception-driven landing has been shown in simulation, real-world deployment introduces significant challenges. Payload and volume constraints limit high-performance edge AI devices like the NVIDIA Jetson Orin Nano, which are crucial for real-time detection and control. Accurate pose estimation during descent is necessary, especially in the absence of GPS, and relies on dependable visual-inertial odometry. Achieving this with limited edge AI resources requires careful optimization of the entire deployment framework. The flight characteristics of large QuadPlanes further complicate the problem. These aircraft exhibit high inertia, reduced thrust vectoring, and slow response times further complicate stable landing maneuvers. This work presents a lightweight QuadPlane system for efficient vision-based autonomous landing and visual-inertial odometry, specifically developed for long-range QuadPlane operations such as aerial monitoring. It describes the hardware platform, sensor configuration, and embedded computing architecture designed to meet demanding real-time, physical constraints. This establishes a foundation for deploying autonomous landing in dynamic, unstructured, GPS-denied environments.

</details>


### [32] [COVLM-RL: Critical Object-Oriented Reasoning for Autonomous Driving Using VLM-Guided Reinforcement Learning](https://arxiv.org/abs/2512.09349)
*Lin Li,Yuxin Cai,Jianwu Fang,Jianru Xue,Chen Lv*

Main category: cs.RO

TL;DR: COVLM-RL是一种新颖的自动驾驶框架，结合了视觉语言模型与强化学习，通过关键对象推理和一致性损失，提高了培训效率和可靠性，且增强了系统在未知环境中的适应能力。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统在泛化能力、训练效率和决策透明性方面存在挑战，现有的方法在新场景下表现出不够稳健。

Method: 提出了一种新的自动驾驶框架COVLM-RL，集成了关键对象导向推理与视觉语言模型指导的强化学习，利用链式思维提示策略，提高了训练效率和推理的可解释性。

Result: COVLM-RL在模拟环境中显示出显著的泛化能力，使得在训练场景中的成功率提高了30%，在未见过的场景中提高了50%。

Conclusion: 通过结合关键对象导向推理和视觉语言模型指导的强化学习，COVLM-RL在训练效率和政策可解释性方面表现出色，显著提升了自动驾驶系统在不同环境下的表现。

Abstract: End-to-end autonomous driving frameworks face persistent challenges in generalization, training efficiency, and interpretability. While recent methods leverage Vision-Language Models (VLMs) through supervised learning on large-scale datasets to improve reasoning, they often lack robustness in novel scenarios. Conversely, reinforcement learning (RL)-based approaches enhance adaptability but remain data-inefficient and lack transparent decision-making. % contribution To address these limitations, we propose COVLM-RL, a novel end-to-end driving framework that integrates Critical Object-oriented (CO) reasoning with VLM-guided RL. Specifically, we design a Chain-of-Thought (CoT) prompting strategy that enables the VLM to reason over critical traffic elements and generate high-level semantic decisions, effectively transforming multi-view visual inputs into structured semantic decision priors. These priors reduce the input dimensionality and inject task-relevant knowledge into the RL loop, accelerating training and improving policy interpretability. However, bridging high-level semantic guidance with continuous low-level control remains non-trivial. To this end, we introduce a consistency loss that encourages alignment between the VLM's semantic plans and the RL agent's control outputs, enhancing interpretability and training stability. Experiments conducted in the CARLA simulator demonstrate that COVLM-RL significantly improves the success rate by 30\% in trained driving environments and by 50\% in previously unseen environments, highlighting its strong generalization capability.

</details>


### [33] [Observability Analysis and Composite Disturbance Filtering for a Bar Tethered to Dual UAVs Subject to Multi-source Disturbances](https://arxiv.org/abs/2512.09377)
*Lidan Xu,Dadong Fan,Junhong Wang,Wenshuo Li,Hao Lu,Jianzhong Qiao*

Main category: cs.RO

TL;DR: 研究表明，仅利用无人机的里程计信息，便可实现载荷姿态的完整估计，且系统在存在两类或更少的扰动时是可观察的。


<details>
  <summary>Details</summary>
Motivation: 探讨在多源扰动下，仅使用无人机的里程计信息是否能够观察到载荷的姿态，从而减少系统成本和复杂性。

Method: 开发了一种基于扰动观测器的误差状态扩展卡尔曼滤波器，用于状态和扰动估计。

Result: 通过仿真和实验验证，仅凭无人机的里程计信息可以完全估计系统的状态和扰动。

Conclusion: 只依靠无人机的里程计信息，可以完整估计系统状态和扰动，并且在系统中，只存在两种或更少的集中扰动时，该系统是可观察的。

Abstract: Cooperative suspended aerial transportation is highly susceptible to multi-source disturbances such as aerodynamic effects and thrust uncertainties. To achieve precise load manipulation, existing methods often rely on extra sensors to measure cable directions or the payload's pose, which increases the system cost and complexity. A fundamental question remains: is the payload's pose observable under multi-source disturbances using only the drones' odometry information? To answer this question, this work focuses on the two-drone-bar system and proves that the whole system is observable when only two or fewer types of lumped disturbances exist by using the observability rank criterion. To the best of our knowledge, we are the first to present such a conclusion and this result paves the way for more cost-effective and robust systems by minimizing their sensor suites. Next, to validate this analysis, we consider the situation where the disturbances are only exerted on the drones, and develop a composite disturbance filtering scheme. A disturbance observer-based error-state extended Kalman filter is designed for both state and disturbance estimation, which renders improved estimation performance for the whole system evolving on the manifold $(\mathbb{R}^3)^2\times(TS^2)^3$. Our simulation and experimental tests have validated that it is possible to fully estimate the state and disturbance of the system with only odometry information of the drones.

</details>


### [34] [H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos](https://arxiv.org/abs/2512.09406)
*Hai Ci,Xiaokang Liu,Pei Yang,Yiren Song,Mike Zheng Shou*

Main category: cs.RO

TL;DR: 通过视频到视频的转换，将日常人机交互视频转化为机器人操作视频，无需配对数据，且实现高质量运动生成。


<details>
  <summary>Details</summary>
Motivation: 希望消除机器人学习中数据收集流程的繁琐，通过利用日常人类视频简化训练过程。

Method: 引入可转移表示，通过对训练视频中的机器人手臂进行填充和简单的视觉提示来生成机器人的运动视频，在测试时对人类视频执行相同的过程以生成对应的机器人视频。

Result: 提出了一种视频到视频的转换框架，能够将普通的人机交互视频转换为运动一致的机器人操作视频

Conclusion: 该方法显著提高了机器人动作的真实性和稳定性，展示了从未标记的人类视频中扩大机器人学习的潜力

Abstract: Robots that learn manipulation skills from everyday human videos could acquire broad capabilities without tedious robot data collection. We propose a video-to-video translation framework that converts ordinary human-object interaction videos into motion-consistent robot manipulation videos with realistic, physically grounded interactions. Our approach does not require any paired human-robot videos for training only a set of unpaired robot videos, making the system easy to scale. We introduce a transferable representation that bridges the embodiment gap: by inpainting the robot arm in training videos to obtain a clean background and overlaying a simple visual cue (a marker and arrow indicating the gripper's position and orientation), we can condition a generative model to insert the robot arm back into the scene. At test time, we apply the same process to human videos (inpainting the person and overlaying human pose cues) and generate high-quality robot videos that mimic the human's actions. We fine-tune a SOTA video diffusion model (Wan 2.2) in an in-context learning manner to ensure temporal coherence and leveraging of its rich prior knowledge. Empirical results demonstrate that our approach achieves significantly more realistic and grounded robot motions compared to baselines, pointing to a promising direction for scaling up robot learning from unlabeled human videos. Project page: https://showlab.github.io/H2R-Grounder/

</details>


### [35] [Generalizable Collaborative Search-and-Capture in Cluttered Environments via Path-Guided MAPPO and Directional Frontier Allocation](https://arxiv.org/abs/2512.09410)
*Jialin Ying,Zhihao Li,Zicheng Dong,Guohua Wu,Yihuan Liao*

Main category: cs.RO

TL;DR: 提出PGF-MAPPO框架，通过潜力场与空间分散，提升了多智能体在复杂环境中的追捕效率，并展现出良好的零-shot泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中，协作追逐逃避面临稀疏奖励和视野受限的挑战，标准的多智能体强化学习方法效率低下，难以扩展到大规模场景。

Method: 采用分层框架，将拓扑规划与反应控制结合，通过A*潜力场整合致密奖励塑造，使用方向性前沿分配加速覆盖。

Result: PGF-MAPPO展示出在不同大小地图上有效捕获逃避者的能力，且在见到新环境时表现出色，超越了规则和学习基础上的对比方法。

Conclusion: PGF-MAPPO在捕获效率上优于更快的逃避者，并在未见环境中展现出强大的零-shot泛化能力。

Abstract: Collaborative pursuit-evasion in cluttered environments presents significant challenges due to sparse rewards and constrained Fields of View (FOV). Standard Multi-Agent Reinforcement Learning (MARL) often suffers from inefficient exploration and fails to scale to large scenarios. We propose PGF-MAPPO (Path-Guided Frontier MAPPO), a hierarchical framework bridging topological planning with reactive control. To resolve local minima and sparse rewards, we integrate an A*-based potential field for dense reward shaping. Furthermore, we introduce Directional Frontier Allocation, combining Farthest Point Sampling (FPS) with geometric angle suppression to enforce spatial dispersion and accelerate coverage. The architecture employs a parameter-shared decentralized critic, maintaining O(1) model complexity suitable for robotic swarms. Experiments demonstrate that PGF-MAPPO achieves superior capture efficiency against faster evaders. Policies trained on 10x10 maps exhibit robust zero-shot generalization to unseen 20x20 environments, significantly outperforming rule-based and learning-based baselines.

</details>


### [36] [D$^2$GSLAM: 4D Dynamic Gaussian Splatting SLAM](https://arxiv.org/abs/2512.09411)
*Siting Zhu,Yuxiang Huang,Wenhua Wu,Chaokang Jiang,Yongbo Chen,I-Ming Chen,Hesheng Wang*

Main category: cs.RO

TL;DR: D$^2$GSLAM是一种新型动态SLAM系统，利用高斯表示，在动态环境中实现准确的动态重建和稳健的跟踪。


<details>
  <summary>Details</summary>
Motivation: 解决现有动态SLAM系统在动态环境中面临的挑战，利用动态对象的信息进行更全面的场景重建。

Method: 系统由四个主要组成部分构成，包括几何引导的动态分离方法、动态-静态复合表示、逐步位姿精细化策略和运动一致性损失。

Result: D$^2$GSLAM在动态环境中表现出色，能够准确重建动态对象并实现稳健的跟踪。

Conclusion: 提出了一种新的动态SLAM系统，通过高斯表示实现了动态重建和跟踪的结合，具有优异的动态环境表现。

Abstract: Recent advances in Dense Simultaneous Localization and Mapping (SLAM) have demonstrated remarkable performance in static environments. However, dense SLAM in dynamic environments remains challenging. Most methods directly remove dynamic objects and focus solely on static scene reconstruction, which ignores the motion information contained in these dynamic objects. In this paper, we present D$^2$GSLAM, a novel dynamic SLAM system utilizing Gaussian representation, which simultaneously performs accurate dynamic reconstruction and robust tracking within dynamic environments. Our system is composed of four key components: (i) We propose a geometric-prompt dynamic separation method to distinguish between static and dynamic elements of the scene. This approach leverages the geometric consistency of Gaussian representation and scene geometry to obtain coarse dynamic regions. The regions then serve as prompts to guide the refinement of the coarse mask for achieving accurate motion mask. (ii) To facilitate accurate and efficient mapping of the dynamic scene, we introduce dynamic-static composite representation that integrates static 3D Gaussians with dynamic 4D Gaussians. This representation allows for modeling the transitions between static and dynamic states of objects in the scene for composite mapping and optimization. (iii) We employ a progressive pose refinement strategy that leverages both the multi-view consistency of static scene geometry and motion information from dynamic objects to achieve accurate camera tracking. (iv) We introduce a motion consistency loss, which leverages the temporal continuity in object motions for accurate dynamic modeling. Our D$^2$GSLAM demonstrates superior performance on dynamic scenes in terms of mapping and tracking accuracy, while also showing capability in accurate dynamic modeling.

</details>


### [37] [A Hierarchical, Model-Based System for High-Performance Humanoid Soccer](https://arxiv.org/abs/2512.09431)
*Quanyou Wang,Mingzhang Zhu,Ruochen Hou,Kay Gillespie,Alvin Zhu,Shiqi Wang,Yicheng Wang,Gaberiel I. Fernandez,Yeting Liu,Colin Togashi,Hyunwoo Nam,Aditya Navghare,Alex Xu,Taoyuanmin Zhu,Min Sung Ahn,Arturo Flores Alvarez,Justin Quan,Ethan Hong,Dennis W. Hong*

Main category: cs.RO

TL;DR: 本文介绍了ARTEMIS团队在2024年RoboCup成人尺寸类人足球比赛中的成功，包括硬件和软件创新。


<details>
  <summary>Details</summary>
Motivation: 随着驱动、传感和控制技术的进步，类人机器人在动态和现实世界能力上引起了广泛关注，RoboCup为其提供了挑战性基准。

Method: 研发了基于轻质结构组件和高扭矩准直接驱动器的成人尺寸类人机器人硬件，以及整合立体视觉、物体检测和地标融合的软件框架，生成动态可行的轨迹并协调游戏决策。

Result: 通过无缝集成的子系统，ARTEMIS实现了快速、精准和战术有效的比赛表现，成功夺得2024年成人尺寸类人足球比赛冠军。

Conclusion: 本文展示了设计原则、系统架构和实验结果，为ARTEMIS的成功提供了基础。

Abstract: The development of athletic humanoid robots has gained significant attention as advances in actuation, sensing, and control enable increasingly dynamic, real-world capabilities. RoboCup, an international competition of fully autonomous humanoid robots, provides a uniquely challenging benchmark for such systems, culminating in the long-term goal of competing against human soccer players by 2050. This paper presents the hardware and software innovations underlying our team's victory in the RoboCup 2024 Adult-Sized Humanoid Soccer Competition. On the hardware side, we introduce an adult-sized humanoid platform built with lightweight structural components, high-torque quasi-direct-drive actuators, and a specialized foot design that enables powerful in-gait kicks while preserving locomotion robustness. On the software side, we develop an integrated perception and localization framework that combines stereo vision, object detection, and landmark-based fusion to provide reliable estimates of the ball, goals, teammates, and opponents. A mid-level navigation stack then generates collision-aware, dynamically feasible trajectories, while a centralized behavior manager coordinates high-level decision making, role selection, and kick execution based on the evolving game state. The seamless integration of these subsystems results in fast, precise, and tactically effective gameplay, enabling robust performance under the dynamic and adversarial conditions of real matches. This paper presents the design principles, system architecture, and experimental results that contributed to ARTEMIS's success as the 2024 Adult-Sized Humanoid Soccer champion.

</details>


### [38] [Sequential Testing for Descriptor-Agnostic LiDAR Loop Closure in Repetitive Environments](https://arxiv.org/abs/2512.09447)
*Jaehyun Kim,Seungwon Choi,Tae-Wan Kim*

Main category: cs.RO

TL;DR: 提出了一种新的多帧回环验证方法，降低了室内环境中的误报率，并提高了精度。


<details>
  <summary>Details</summary>
Motivation: 希望通过更精确的多帧证据来改进回环验证，尤其是在复杂的室内环境中。

Method: 使用截断序列概率比率检验(SPRT)进行多帧回环验证。

Result: 在五个序列库数据集上进行了评估，表现出比单帧和启发式多帧基线更高的精度。

Conclusion: 该验证器在结构性重复的室内环境中有效减少了误报，提升了回环验证的精度。

Abstract: We propose a descriptor-agnostic, multi-frame loop closure verification method that formulates LiDAR loop closure as a truncated Sequential Probability Ratio Test (SPRT). Instead of deciding from a single descriptor comparison or using fixed thresholds with late-stage Iterative Closest Point (ICP) vetting, the verifier accumulates a short temporal stream of descriptor similarities between a query and each candidate. It then issues an accept/reject decision adaptively once sufficient multi-frame evidence has been observed, according to user-specified Type-I/II error design targets. This precision-first policy is designed to suppress false positives in structurally repetitive indoor environments. We evaluate the verifier on a five-sequence library dataset, using a fixed retrieval front-end with several representative LiDAR global descriptors. Performance is assessed via segment-level K-hit precision-recall and absolute trajectory error (ATE) and relative pose error (RPE) after pose graph optimization. Across descriptors, the sequential verifier consistently improves precision and reduces the impact of aliased loops compared with single-frame and heuristic multi-frame baselines. Our implementation and dataset will be released at: https://github.com/wanderingcar/snu_library_dataset.

</details>


### [39] [Development of a Compliant Gripper for Safe Robot-Assisted Trouser Dressing-Undressing](https://arxiv.org/abs/2512.09462)
*Jayant Unde,Takumi Inden,Yuki Wakayama,Jacinto Colan,Yaonan Zhu,Tadayoshi Aoyama,Yasuhisa Hasegawa*

Main category: cs.RO

TL;DR: 本论文介绍了一种新的抓取系统，该系统旨在帮助老年人和半身瘫痪者在穿脱裤子时的辅助，强调了其设计、开发和实验评估的过程。


<details>
  <summary>Details</summary>
Motivation: 随着许多国家人口快速老龄化，如何保持老年人的生活质量成为一个重要问题，尤其是对在如厕过程中需要支持的老年人。

Method: 本研究设计并开发了一种抓取系统，并将其集成到自定义的机器人操控系统中，以实现对老年人和半身瘫痪者穿脱裤子的辅助。

Result: 实验评估显示，该抓取系统在狭小空间内成功辅助穿脱裤子的成功率高，并优于现有的相关研究。

Conclusion: 本研究为老年人和身体残障人士提供了一种先进的助理机器人方案，以提高他们独立穿衣的能力和生活质量。

Abstract: In recent years, many countries, including Japan, have rapidly aging populations, making the preservation of seniors' quality of life a significant concern. For elderly people with impaired physical abilities, support for toileting is one of the most important issues. This paper details the design, development, experimental assessment, and potential application of the gripper system, with a focus on the unique requirements and obstacles involved in aiding elderly or hemiplegic individuals in dressing and undressing trousers. The gripper we propose seeks to find the right balance between compliance and grasping forces, ensuring precise manipulation while maintaining a safe and compliant interaction with the users. The gripper's integration into a custom--built robotic manipulator system provides a comprehensive solution for assisting hemiplegic individuals in their dressing and undressing tasks. Experimental evaluations and comparisons with existing studies demonstrate the gripper's ability to successfully assist in both dressing and dressing of trousers in confined spaces with a high success rate. This research contributes to the advancement of assistive robotics, empowering elderly, and physically impaired individuals to maintain their independence and improve their quality of life.

</details>


### [40] [On Mobile Ad Hoc Networks for Coverage of Partially Observable Worlds](https://arxiv.org/abs/2512.09495)
*Edwin Meriaux,Shuo Wen,Louis-Roy Langevin,Doina Precup,Antonio Loría,Gregory Dudek*

Main category: cs.RO

TL;DR: 本文提出了在未知环境中移动代理的通信网络建立问题，为此引入了部分可观察的合作守卫艺术画廊问题，并提出了两种有效的算法。


<details>
  <summary>Details</summary>
Motivation: 旨在解决在未知环境中移动代理的运动和放置，以建立通信网络。

Method: 提出了两种算法：CADENCE（集中式计划者）和DADENCE（去中心化协调机制），分别用于解决部分可观察的合作守卫艺术画廊问题（POCGAGP）。

Result: 通过1500个不同规模和结构的测试案例的仿真评估，证明了在覆盖和探索未知空间的同时形成连通网络的成功。

Conclusion: 该研究展示了基于几何抽象的通信驱动探索的价值，并表明去中心化策略在可扩展性方面与中心化性能竞争。

Abstract: This paper addresses the movement and placement of mobile agents to establish a communication network in initially unknown environments. We cast the problem in a computational-geometric framework by relating the coverage problem and line-of-sight constraints to the Cooperative Guard Art Gallery Problem, and introduce its partially observable variant, the Partially Observable Cooperative Guard Art Gallery Problem (POCGAGP). We then present two algorithms that solve POCGAGP: CADENCE, a centralized planner that incrementally selects 270 degree corners at which to deploy agents, and DADENCE, a decentralized scheme that coordinates agents using local information and lightweight messaging. Both approaches operate under partial observability and target simultaneous coverage and connectivity. We evaluate the methods in simulation across 1,500 test cases of varied size and structure, demonstrating consistent success in forming connected networks while covering and exploring unknown space. These results highlight the value of geometric abstractions for communication-driven exploration and show that decentralized policies are competitive with centralized performance while retaining scalability.

</details>


### [41] [ViTA-Seg: Vision Transformer for Amodal Segmentation in Robotics](https://arxiv.org/abs/2512.09510)
*Donato Caramia,Florian T. Pokorny,Giuseppe Triggiani,Denis Ruffino,David Naso,Paolo Roberto Massenio*

Main category: cs.RO

TL;DR: ViTA-Seg是一个适用于实时模态分割的视觉变压器框架，能有效处理机器人拣选中的遮挡问题，具有优秀的分割准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决机器人拣选中由于遮挡导致的抓取规划不准确和不可靠的问题。

Method: 提出了ViTA-Seg，一个利用全局注意力的视觉变压器框架，包含单头和双头两种架构，进行实时的模态分割。

Result: 在COOCA和KINS两个模态基准测试上，ViTA-Seg Dual Head实现了强大的模态和遮挡分割准确性。

Conclusion: ViTA-Seg Dual Head在模态分割和遮挡分割的准确性及计算效率上表现出色，支持稳健的实时机器人操作。

Abstract: Occlusions in robotic bin picking compromise accurate and reliable grasp planning. We present ViTA-Seg, a class-agnostic Vision Transformer framework for real-time amodal segmentation that leverages global attention to recover complete object masks, including hidden regions. We proposte two architectures: a) Single-Head for amodal mask prediction; b) Dual-Head for amodal and occluded mask prediction. We also introduce ViTA-SimData, a photo-realistic synthetic dataset tailored to industrial bin-picking scenario. Extensive experiments on two amodal benchmarks, COOCA and KINS, demonstrate that ViTA-Seg Dual Head achieves strong amodal and occlusion segmentation accuracy with computational efficiency, enabling robust, real-time robotic manipulation.

</details>


### [42] [REASAN: Learning Reactive Safe Navigation for Legged Robots](https://arxiv.org/abs/2512.09537)
*Qihao Yuan,Ziyu Cao,Ming Cao,Kailai Li*

Main category: cs.RO

TL;DR: 我们提出了一种新的模块化框架REASAN，实现了在复杂环境中使用单个LiDAR的腿部反应式导航，系统包含三个RL策略和一个外部估计器，经过验证，表现出优于现有方法的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 旨在通过降低复杂腿部运动控制任务的难度，实现轻量级神经网络的训练，同时避免对高级启发式或政策切换机制的依赖。

Method: 构建了一个模块化的端到端框架，包括三种强化学习政策模块和一款基于变换器的外部环境估计器。

Result: 经过全面的消融试验，验证了设计选择，并显示出在复杂导航任务中相对于现有方法具有更好的鲁棒性。

Conclusion: REASAN 系统实现了在复杂环境中单机器人和多机器人设置下的全自主实时反应式导航，证明了其在导航任务中的鲁棒性和有效性。

Abstract: We present a novel modularized end-to-end framework for legged reactive navigation in complex dynamic environments using a single light detection and ranging (LiDAR) sensor. The system comprises four simulation-trained modules: three reinforcement-learning (RL) policies for locomotion, safety shielding, and navigation, and a transformer-based exteroceptive estimator that processes raw point-cloud inputs. This modular decomposition of complex legged motor-control tasks enables lightweight neural networks with simple architectures, trained using standard RL practices with targeted reward shaping and curriculum design, without reliance on heuristics or sophisticated policy-switching mechanisms. We conduct comprehensive ablations to validate our design choices and demonstrate improved robustness compared to existing approaches in challenging navigation tasks. The resulting reactive safe navigation (REASAN) system achieves fully onboard and real-time reactive navigation across both single- and multi-robot settings in complex environments. We release our training and deployment code at https://github.com/ASIG-X/REASAN.

</details>


### [43] [Mastering Diverse, Unknown, and Cluttered Tracks for Robust Vision-Based Drone Racing](https://arxiv.org/abs/2512.09571)
*Feng Yu,Yu Hu,Yang Su,Yang Deng,Linzuo Zhang,Danping Zou*

Main category: cs.RO

TL;DR: 提出了一种新颖的无人机竞速学习框架，增强了模型在复杂环境中的一般化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的基于强化学习的方法普遍针对固定且无障碍的轨道，未能有效处理复杂环境中的快速飞行与碰撞避免的平衡。

Method: 采用两阶段学习框架，包括初始的软碰撞训练阶段和后续的硬碰撞细化阶段，结合噪声增强的自适应课程与不对称的演员-评论家架构。

Result: 通过广泛的仿真与消融研究，以及在有计算限制的四旋翼上的实地实验，验证了实现敏捷飞行的系统在面对门位错误时的鲁棒性。

Conclusion: 本研究提出了一种针对未知、杂乱环境下无人机竞速的可推广框架，能够在处理门位误差的同时，实现敏捷飞行和强大的障碍物避免能力。

Abstract: Most reinforcement learning(RL)-based methods for drone racing target fixed, obstacle-free tracks, leaving the generalization to unknown, cluttered environments largely unaddressed. This challenge stems from the need to balance racing speed and collision avoidance, limited feasible space causing policy exploration trapped in local optima during training, and perceptual ambiguity between gates and obstacles in depth maps-especially when gate positions are only coarsely specified. To overcome these issues, we propose a two-phase learning framework: an initial soft-collision training phase that preserves policy exploration for high-speed flight, followed by a hard-collision refinement phase that enforces robust obstacle avoidance. An adaptive, noise-augmented curriculum with an asymmetric actor-critic architecture gradually shifts the policy's reliance from privileged gate-state information to depth-based visual input. We further impose Lipschitz constraints and integrate a track-primitive generator to enhance motion stability and cross-environment generalization. We evaluate our framework through extensive simulation and ablation studies, and validate it in real-world experiments on a computationally constrained quadrotor. The system achieves agile flight while remaining robust to gate-position errors, developing a generalizable drone racing framework with the capability to operate in diverse, partially unknown and cluttered environments. https://yufengsjtu.github.io/MasterRacing.github.io/

</details>


### [44] [UrbanNav: Learning Language-Guided Urban Navigation from Web-Scale Human Trajectories](https://arxiv.org/abs/2512.09607)
*Yanghong Mei,Yirong Yang,Longteng Guo,Qunbo Wang,Ming-Ming Yu,Xingjian He,Wenjun Wu,Jing Liu*

Main category: cs.RO

TL;DR: UrbanNav是一种框架，旨在通过自然语言指令帮助机器人在复杂城市环境中进行导航，表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 应对复杂城市环境中自然语言指令的挑战，如模糊的空间参考和动态街景。

Method: 开发UrbanNav框架，利用网络规模的城市行走视频，创建大规模标注管道，将人类导航轨迹与语言指令对齐。

Result: UrbanNav包含1500小时的导航数据和300万指令-轨迹-地标三元组，模型展示出卓越的空间推理能力和对模糊指令的鲁棒性。

Conclusion: UrbanNav显著优于现有方法，凸显了大规模网络视频数据在语言引导下实现真实世界城市导航的潜力。

Abstract: Navigating complex urban environments using natural language instructions poses significant challenges for embodied agents, including noisy language instructions, ambiguous spatial references, diverse landmarks, and dynamic street scenes. Current visual navigation methods are typically limited to simulated or off-street environments, and often rely on precise goal formats, such as specific coordinates or images. This limits their effectiveness for autonomous agents like last-mile delivery robots navigating unfamiliar cities. To address these limitations, we introduce UrbanNav, a scalable framework that trains embodied agents to follow free-form language instructions in diverse urban settings. Leveraging web-scale city walking videos, we develop an scalable annotation pipeline that aligns human navigation trajectories with language instructions grounded in real-world landmarks. UrbanNav encompasses over 1,500 hours of navigation data and 3 million instruction-trajectory-landmark triplets, capturing a wide range of urban scenarios. Our model learns robust navigation policies to tackle complex urban scenarios, demonstrating superior spatial reasoning, robustness to noisy instructions, and generalization to unseen urban settings. Experimental results show that UrbanNav significantly outperforms existing methods, highlighting the potential of large-scale web video data to enable language-guided, real-world urban navigation for embodied agents.

</details>


### [45] [Super4DR: 4D Radar-centric Self-supervised Odometry and Gaussian-based Map Optimization](https://arxiv.org/abs/2512.09608)
*Zhiheng Li,Weihua Wang,Qiang Shen,Yichen Zhao,Zheng Fang*

Main category: cs.RO

TL;DR: Super4DR 是一个基于 4D 雷达的框架，利用集群感知和自我监督机制显著提升了里程计和地图质量，特别是在极端环境下。


<details>
  <summary>Details</summary>
Motivation: 传统 SLAM 系统在低光和恶劣天气条件下表现不佳，而 4D 雷达在这些环境中更为适合，但其点云稀疏和嘈杂限制了准确估计。

Method: 设计集群感知的里程计网络与层次自我监督机制，采用 3D 高斯作为中介表示，以及雷达特定的生长策略、选择性分离和多视角正则化

Result: Super4DR 框架在自监督方法上性能提升 67%，接近监督里程计，且在地图质量上缩小与 LiDAR 的差距，同时支持多模态图像渲染。

Conclusion: Super4DR 在低光和恶劣天气下，利用 4D 雷达改进了里程计估计和地图优化，超越了传统 SLAM 系统的局限性。

Abstract: Conventional SLAM systems using visual or LiDAR data often struggle in poor lighting and severe weather. Although 4D radar is suited for such environments, its sparse and noisy point clouds hinder accurate odometry estimation, while the radar maps suffer from obscure and incomplete structures. Thus, we propose Super4DR, a 4D radar-centric framework for learning-based odometry estimation and gaussian-based map optimization. First, we design a cluster-aware odometry network that incorporates object-level cues from the clustered radar points for inter-frame matching, alongside a hierarchical self-supervision mechanism to overcome outliers through spatio-temporal consistency, knowledge transfer, and feature contrast. Second, we propose using 3D gaussians as an intermediate representation, coupled with a radar-specific growth strategy, selective separation, and multi-view regularization, to recover blurry map areas and those undetected based on image texture. Experiments show that Super4DR achieves a 67% performance gain over prior self-supervised methods, nearly matches supervised odometry, and narrows the map quality disparity with LiDAR while enabling multi-modal image rendering.

</details>


### [46] [GLaD: Geometric Latent Distillation for Vision-Language-Action Models](https://arxiv.org/abs/2512.09619)
*Minghao Guo,Meng Cao,Jiachen Tao,Rongtao Xu,Yan Yan,Xiaodan Liang,Ivan Laptev,Xiaojun Chang*

Main category: cs.RO

TL;DR: GLaD是一个几何感知的视觉-语言-动作框架，通过知识蒸馏在预训练中整合3D几何信息，显著提升了空间推理和策略泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型主要依赖RGB信息，忽视了对空间推理和操控至关重要的几何线索。

Method: 引入GLaD模型，通过知识蒸馏方法在预训练阶段整合3D几何先验，将视觉特征与几何感知的视觉变换器对齐。

Result: GLaD在桥接数据集上预训练后，在四个LIBERO任务套件上实现平均成功率94.1%，超越了使用相同预训练数据的UniVLA（92.5%）。

Conclusion: GLaD模型通过几何感知预训练提升了空间推理和策略泛化能力，证明了几何感知的有效性。

Abstract: Most existing Vision-Language-Action (VLA) models rely primarily on RGB information, while ignoring geometric cues crucial for spatial reasoning and manipulation. In this work, we introduce GLaD, a geometry-aware VLA framework that incorporates 3D geometric priors during pretraining through knowledge distillation. Rather than distilling geometric features solely into the vision encoder, we align the LLM's hidden states corresponding to visual tokens with features from a frozen geometry-aware vision transformer (VGGT), ensuring that geometric understanding is deeply integrated into the multimodal representations that drive action prediction. Pretrained on the Bridge dataset with this geometry distillation mechanism, GLaD achieves 94.1% average success rate across four LIBERO task suites, outperforming UniVLA (92.5%) which uses identical pretraining data. These results validate that geometry-aware pretraining enhances spatial reasoning and policy generalization without requiring explicit depth sensors or 3D annotations.

</details>


### [47] [ReMoSPLAT: Reactive Mobile Manipulation Control on a Gaussian Splat](https://arxiv.org/abs/2512.09656)
*Nicolas Marticorena,Tobias Fischer,Niko Suenderhauf*

Main category: cs.RO

TL;DR: 本研究提出了一种名为ReMoSPLAT的反应式控制器，通过高斯散点表示实现避障，并与其他方法进行了比较，展示了其在实际和模拟环境中的有效性。


<details>
  <summary>Details</summary>
Motivation: 在不进行复杂规划的情况下，准确表示环境以避免障碍是个挑战。

Method: 基于二次规划的反应式控制器，利用高斯散点表示进行避障。

Result: 通过集成额外的约束和成本，移动 manipulator 平台可以在拥挤场景中到达预期的末端执行器姿态，同时避免障碍。

Conclusion: 所提出的方法在模拟实验中与依赖完美真实信息的控制器表现相当，展示了其可行性。

Abstract: Reactive control can gracefully coordinate the motion of the base and the arm of a mobile manipulator. However, incorporating an accurate representation of the environment to avoid obstacles without involving costly planning remains a challenge. In this work, we present ReMoSPLAT, a reactive controller based on a quadratic program formulation for mobile manipulation that leverages a Gaussian Splat representation for collision avoidance. By integrating additional constraints and costs into the optimisation formulation, a mobile manipulator platform can reach its intended end effector pose while avoiding obstacles, even in cluttered scenes. We investigate the trade-offs of two methods for efficiently calculating robot-obstacle distances, comparing a purely geometric approach with a rasterisation-based approach. Our experiments in simulation on both synthetic and real-world scans demonstrate the feasibility of our method, showing that the proposed approach achieves performance comparable to controllers that rely on perfect ground-truth information.

</details>


### [48] [High-Resolution Water Sampling via a Solar-Powered Autonomous Surface Vehicle](https://arxiv.org/abs/2512.09798)
*Misael Mamani,Mariel Fernandez,Grace Luna,Steffani Limachi,Leonel Apaza,Carolina Montes-Dávalos,Marcelo Herrera,Edwin Salcedo*

Main category: cs.RO

TL;DR: 本文提出了一种新型的太阳能自动无人水面车辆，通过独特的取样系统，实现高效的水样采集和处理，适用于偏远水域的监测。


<details>
  <summary>Details</summary>
Motivation: 准确的水质评估需要空间分布良好的取样，而大多数无人水面车辆的取样能力有限。

Method: 开发了一种太阳能供电的全自动无人水面车辆 (USV)，具备创新的注射器取样架构，能够在每次任务中采集72个独立的水样。

Result: 该平台在Achocalla泻湖的实地试验中表现出87%的航点准确性，稳定的自主导航，以及与手动采集参考相当的准确的物理化学测量。

Conclusion: 该平台为遥远环境中的水质监测提供了可靠的高分辨率取样和自主任务执行的可扩展解决方案。

Abstract: Accurate water quality assessment requires spatially resolved sampling, yet most unmanned surface vehicles (USVs) can collect only a limited number of samples or rely on single-point sensors with poor representativeness. This work presents a solar-powered, fully autonomous USV featuring a novel syringe-based sampling architecture capable of acquiring 72 discrete, contamination-minimized water samples per mission. The vehicle incorporates a ROS 2 autonomy stack with GPS-RTK navigation, LiDAR and stereo-vision obstacle detection, Nav2-based mission planning, and long-range LoRa supervision, enabling dependable execution of sampling routes in unstructured environments. The platform integrates a behavior-tree autonomy architecture adapted from Nav2, enabling mission-level reasoning and perception-aware navigation. A modular 6x12 sampling system, controlled by distributed micro-ROS nodes, provides deterministic actuation, fault isolation, and rapid module replacement, achieving spatial coverage beyond previously reported USV-based samplers. Field trials in Achocalla Lagoon (La Paz, Bolivia) demonstrated 87% waypoint accuracy, stable autonomous navigation, and accurate physicochemical measurements (temperature, pH, conductivity, total dissolved solids) comparable to manually collected references. These results demonstrate that the platform enables reliable high-resolution sampling and autonomous mission execution, providing a scalable solution for aquatic monitoring in remote environments.

</details>


### [49] [Bridging the Basilisk Astrodynamics Framework with ROS 2 for Modular Spacecraft Simulation and Hardware Integration](https://arxiv.org/abs/2512.09833)
*Elias Krantz,Ngai Nam Chan,Gunnar Tibert,Huina Mao,Christer Fuglesang*

Main category: cs.RO

TL;DR: 本文提出了一种轻量级的开源通信桥梁，实现了Basilisk模拟器与ROS 2之间的实时数据交换，支持航天器自主控制的快速开发和测试。


<details>
  <summary>Details</summary>
Motivation: 解决高保真航天器模拟器与模块化机器人框架集成的挑战，促进自主性的发展。

Method: 在Basilisk天体动力学模拟器和Robot Operating System 2 (ROS 2)之间建立实时双向数据交换的通信桥梁，支持领导-跟随编队飞行场景的模拟与测试。

Result: 展示了在ATMOS平面微重力测试平台上进行的非线性模型预测控制的领导-跟随编队飞行场景应用，并且模拟与硬件的部署保持一致。

Conclusion: 提出的通信桥梁为模块化航天器自主性和可复制的研究工作流程提供了灵活和可扩展的平台。

Abstract: Integrating high-fidelity spacecraft simulators with modular robotics frameworks remains a challenge for autonomy development. This paper presents a lightweight, open-source communication bridge between the Basilisk astrodynamics simulator and the Robot Operating System 2 (ROS 2), enabling real-time, bidirectional data exchange for spacecraft control. The bridge requires no changes to Basilisk's core and integrates seamlessly with ROS 2 nodes. We demonstrate its use in a leader-follower formation flying scenario using nonlinear model predictive control, deployed identically in both simulation and on the ATMOS planar microgravity testbed. This setup supports rapid development, hardware-in-the-loop testing, and seamless transition from simulation to hardware. The bridge offers a flexible and scalable platform for modular spacecraft autonomy and reproducible research workflows.

</details>


### [50] [Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation](https://arxiv.org/abs/2512.09851)
*Yuyang Li,Yinghan Chen,Zihang Zhao,Puhao Li,Tengyu Liu,Siyuan Huang,Yixin Zhu*

Main category: cs.RO

TL;DR: TacThru是一种允许同时进行视觉和触觉信号提取的新型STS传感器，TacThru-UMI是基于此传感器的模仿学习框架，在多项任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 机器人操作需要丰富的多模态感知和有效的学习框架，以处理复杂的现实世界任务。

Method: 提出一种新的STS传感器TacThru和一个模仿学习框架TacThru-UMI

Result: TacThru-UMI在五个真实世界任务中平均成功率达到85.5%，显著优于交替触觉-视觉的66.3%和单一视觉的55.4%。

Conclusion: 结合同时的多模态感知与现代学习框架能够实现更精确、灵活的机器人操作。

Abstract: Robotic manipulation requires both rich multimodal perception and effective learning frameworks to handle complex real-world tasks. See-through-skin (STS) sensors, which combine tactile and visual perception, offer promising sensing capabilities, while modern imitation learning provides powerful tools for policy acquisition. However, existing STS designs lack simultaneous multimodal perception and suffer from unreliable tactile tracking. Furthermore, integrating these rich multimodal signals into learning-based manipulation pipelines remains an open challenge. We introduce TacThru, an STS sensor enabling simultaneous visual perception and robust tactile signal extraction, and TacThru-UMI, an imitation learning framework that leverages these multimodal signals for manipulation. Our sensor features a fully transparent elastomer, persistent illumination, novel keyline markers, and efficient tracking, while our learning system integrates these signals through a Transformer-based Diffusion Policy. Experiments on five challenging real-world tasks show that TacThru-UMI achieves an average success rate of 85.5%, significantly outperforming the baselines of alternating tactile-visual (66.3%) and vision-only (55.4%). The system excels in critical scenarios, including contact detection with thin and soft objects and precision manipulation requiring multimodal coordination. This work demonstrates that combining simultaneous multimodal perception with modern learning frameworks enables more precise, adaptable robotic manipulation.

</details>


### [51] [Visual Heading Prediction for Autonomous Aerial Vehicles](https://arxiv.org/abs/2512.09898)
*Reza Ahmari,Ahmad Mohammadi,Vahid Hemmati,Mohammed Mynuddin,Parham Kebria,Mahmoud Nabil Mahmoud,Xiaohong Yuan,Abdollah Homaifar*

Main category: cs.RO

TL;DR: 本文提出了一种基于视觉的、数据驱动的UAV-UGV实时集成框架，能够在缺乏GPS/GNSS的环境中实现可靠的多智能体协调。


<details>
  <summary>Details</summary>
Motivation: 无人机(UAV)与无人地面车辆(UGV)的集成对智能自主系统的发展至关重要，但在实时场景中精确协调面临巨大挑战，尤其是在缺乏外部定位基础设施的情况下。

Method: 使用YOLOv5模型进行UGV检测，以及轻量化人工神经网络估计UAV所需的航向角。

Result: 在受控实验室环境中收集了超过13,000张标注图像，并训练得出平均绝对误差为0.1506°，均方根误差为0.1957°的ANN模型，UGV检测准确率达到95%。

Conclusion: 该系统提供了一种独立于基础设施的视觉解决方案，展现出在GPS/GNSS受限环境中可靠多智能体协调的强大潜力。

Abstract: The integration of Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) is increasingly central to the development of intelligent autonomous systems for applications such as search and rescue, environmental monitoring, and logistics. However, precise coordination between these platforms in real-time scenarios presents major challenges, particularly when external localization infrastructure such as GPS or GNSS is unavailable or degraded [1]. This paper proposes a vision-based, data-driven framework for real-time UAV-UGV integration, with a focus on robust UGV detection and heading angle prediction for navigation and coordination. The system employs a fine-tuned YOLOv5 model to detect UGVs and extract bounding box features, which are then used by a lightweight artificial neural network (ANN) to estimate the UAV's required heading angle. A VICON motion capture system was used to generate ground-truth data during training, resulting in a dataset of over 13,000 annotated images collected in a controlled lab environment. The trained ANN achieves a mean absolute error of 0.1506° and a root mean squared error of 0.1957°, offering accurate heading angle predictions using only monocular camera inputs. Experimental evaluations achieve 95% accuracy in UGV detection. This work contributes a vision-based, infrastructure- independent solution that demonstrates strong potential for deployment in GPS/GNSS-denied environments, supporting reliable multi-agent coordination under realistic dynamic conditions. A demonstration video showcasing the system's real-time performance, including UGV detection, heading angle prediction, and UAV alignment under dynamic conditions, is available at: https://github.com/Kooroshraf/UAV-UGV-Integration

</details>


### [52] [YOPO-Nav: Visual Navigation using 3DGS Graphs from One-Pass Videos](https://arxiv.org/abs/2512.09903)
*Ryan Meegan,Adam D'Souza,Bryan Bo Cao,Shubham Jain,Kristin Dana*

Main category: cs.RO

TL;DR: YOPO-Nav通过利用视频而无需详细地图进行高效视觉导航，验证了其在现实场景中的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的机器人导航依赖于详细的地图构建和路径规划，导致计算和内存需求高。

Method: 提出了YOPO-Nav方法，它利用探索视频作为视觉参考，通过连接的局部3D高斯点云模型进行紧凑的环境空间表示。

Result: 在YOPO-Campus数据集上，YOPO-Nav实现了优秀的图像目标导航性能。

Conclusion: YOPO-Nav不仅优化了导航过程，还将在视觉导航和场景表示研究中提供数据集和代码，供公众使用。

Abstract: Visual navigation has emerged as a practical alternative to traditional robotic navigation pipelines that rely on detailed mapping and path planning. However, constructing and maintaining 3D maps is often computationally expensive and memory-intensive. We address the problem of visual navigation when exploration videos of a large environment are available. The videos serve as a visual reference, allowing a robot to retrace the explored trajectories without relying on metric maps. Our proposed method, YOPO-Nav (You Only Pass Once), encodes an environment into a compact spatial representation composed of interconnected local 3D Gaussian Splatting (3DGS) models. During navigation, the framework aligns the robot's current visual observation with this representation and predicts actions that guide it back toward the demonstrated trajectory. YOPO-Nav employs a hierarchical design: a visual place recognition (VPR) module provides coarse localization, while the local 3DGS models refine the goal and intermediate poses to generate control actions. To evaluate our approach, we introduce the YOPO-Campus dataset, comprising 4 hours of egocentric video and robot controller inputs from over 6 km of human-teleoperated robot trajectories. We benchmark recent visual navigation methods on trajectories from YOPO-Campus using a Clearpath Jackal robot. Experimental results show YOPO-Nav provides excellent performance in image-goal navigation for real-world scenes on a physical robot. The dataset and code will be made publicly available for visual navigation and scene representation research.

</details>


### [53] [Py-DiSMech: A Scalable and Efficient Framework for Discrete Differential Geometry-Based Modeling and Control of Soft Robots](https://arxiv.org/abs/2512.09911)
*Radha Lahoti,Ryan Chaiyakul,M. Khalid Jawed*

Main category: cs.RO

TL;DR: Py-DiSMech是一个开源Python模拟框架，针对软机器人结构的高保真模拟，具有高速计算、隐式接触模型和反馈控制模块等特点，显著优于现有模拟器。


<details>
  <summary>Details</summary>
Motivation: 针对软机器人设计和控制中复杂的几何变形和接触相互作用，呼唤高保真模拟框架以实现物理精度和计算可扩展性。

Method: Py-DiSMech通过离散微分几何（DDG）原理建模和控制软机器人结构，采用高度向量化的NumPy实现，支持隐式接触模型和反馈控制模块。

Result: Py-DiSMech在计算效率上显著超越现有的状态最先进的模拟器Elastica，同时保持物理精度。

Conclusion: Py-DiSMech是一个可扩展、可延伸的平台，适用于软机器人领域的模拟驱动设计、控制验证和从模拟到现实的研究。

Abstract: High-fidelity simulation has become essential to the design and control of soft robots, where large geometric deformations and complex contact interactions challenge conventional modeling tools. Recent advances in the field demand simulation frameworks that combine physical accuracy, computational scalability, and seamless integration with modern control and optimization pipelines. In this work, we present Py-DiSMech, a Python-based, open-source simulation framework for modeling and control of soft robotic structures grounded in the principles of Discrete Differential Geometry (DDG). By discretizing geometric quantities such as curvature and strain directly on meshes, Py-DiSMech captures the nonlinear deformation of rods, shells, and hybrid structures with high fidelity and reduced computational cost. The framework introduces (i) a fully vectorized NumPy implementation achieving order-of-magnitude speed-ups over existing geometry-based simulators; (ii) a penalty-energy-based fully implicit contact model that supports rod-rod, rod-shell, and shell-shell interactions; (iii) a natural-strain-based feedback-control module featuring a proportional-integral (PI) controller for shape regulation and trajectory tracking; and (iv) a modular, object-oriented software design enabling user-defined elastic energies, actuation schemes, and integration with machine-learning libraries. Benchmark comparisons demonstrate that Py-DiSMech substantially outperforms the state-of-the-art simulator Elastica in computational efficiency while maintaining physical accuracy. Together, these features establish Py-DiSMech as a scalable, extensible platform for simulation-driven design, control validation, and sim-to-real research in soft robotics.

</details>


### [54] [LISN: Language-Instructed Social Navigation with VLM-based Controller Modulating](https://arxiv.org/abs/2512.09920)
*Junting Chen,Yunchuan Li,Panfeng Jiang,Jiacheng Du,Zixuan Chen,Chenrui Tie,Jiajun Deng,Lin Shao*

Main category: cs.RO

TL;DR: 本研究提出了LISN-Bench，这是一个基于仿真的语言指令社交导航基准，并介绍了Social-Nav-Modulator，其在复杂任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 移动机器人在人机共存中需要具备社会意识导航能力，现有研究未能全面涵盖社交导航的重要方面，如遵循用户指令和社交规范。

Method: 通过构建LISN-Bench基准和引入Social-Nav-Modulator，一个快速-慢速的层次系统，解耦低级动作生成与VLM循环，提高了动态避免和感知适应性。

Result: 我们的研究方法在复杂任务中的成功率为91.3%，显著超过竞争基线63%的成功率，尤其在拥挤环境中表现突出。

Conclusion: 提出的LISN-Bench是首个语言指令社交导航的仿真基准，验证了使用Social-Nav-Modulator的有效性，其成功率超过当前最具竞争力的基线。

Abstract: Towards human-robot coexistence, socially aware navigation is significant for mobile robots. Yet existing studies on this area focus mainly on path efficiency and pedestrian collision avoidance, which are essential but represent only a fraction of social navigation. Beyond these basics, robots must also comply with user instructions, aligning their actions to task goals and social norms expressed by humans. In this work, we present LISN-Bench, the first simulation-based benchmark for language-instructed social navigation. Built on Rosnav-Arena 3.0, it is the first standardized social navigation benchmark to incorporate instruction following and scene understanding across diverse contexts. To address this task, we further propose Social-Nav-Modulator, a fast-slow hierarchical system where a VLM agent modulates costmaps and controller parameters. Decoupling low-level action generation from the slower VLM loop reduces reliance on high-frequency VLM inference while improving dynamic avoidance and perception adaptability. Our method achieves an average success rate of 91.3%, which is greater than 63% than the most competitive baseline, with most of the improvements observed in challenging tasks such as following a person in a crowd and navigating while strictly avoiding instruction-forbidden regions. The project website is at: https://social-nav.github.io/LISN-project/

</details>


### [55] [Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models](https://arxiv.org/abs/2512.09927)
*Yifan Ye,Jiaqi Ma,Jun Cen,Zhihe Lu*

Main category: cs.RO

TL;DR: TEAM-VLA是一个用于加速视觉-语言-动作模型推理的框架，通过令牌的动态扩展和合并机制来提高效率，同时保持任务表现。


<details>
  <summary>Details</summary>
Motivation: 为了克服大规模多模态数据集预训练的视觉-语言-动作模型在动态环境下实时部署时的计算负担与延迟敏感性问题。

Method: 提出了一种名为Token Expand-and-Merge-VLA (TEAM-VLA)的框架，通过动态令牌扩展机制和逐层合并来优化推理效率。

Result: TEAM-VLA能够在不重新训练的情况下提升VLA模型的推理速度，并保持或提高任务成功率。

Conclusion: 团队实验表明，TEAM-VLA在LIBERO基准测试中有效提升了推理速度，同时在任务成功率上与完整的VLA模型相当或更好。

Abstract: Vision-Language-Action (VLA) models pretrained on large-scale multimodal datasets have emerged as powerful foundations for robotic perception and control. However, their massive scale, often billions of parameters, poses significant challenges for real-time deployment, as inference becomes computationally expensive and latency-sensitive in dynamic environments. To address this, we propose Token Expand-and-Merge-VLA (TEAM-VLA), a training-free token compression framework that accelerates VLA inference while preserving task performance. TEAM-VLA introduces a dynamic token expansion mechanism that identifies and samples additional informative tokens in the spatial vicinity of attention-highlighted regions, enhancing contextual completeness. These expanded tokens are then selectively merged in deeper layers under action-aware guidance, effectively reducing redundancy while maintaining semantic coherence. By coupling expansion and merging within a single feed-forward pass, TEAM-VLA achieves a balanced trade-off between efficiency and effectiveness, without any retraining or parameter updates. Extensive experiments on LIBERO benchmark demonstrate that TEAM-VLA consistently improves inference speed while maintaining or even surpassing the task success rate of full VLA models. The code is public available on \href{https://github.com/Jasper-aaa/TEAM-VLA}{https://github.com/Jasper-aaa/TEAM-VLA}

</details>


### [56] [HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models](https://arxiv.org/abs/2512.09928)
*Minghui Lin,Pengxiang Ding,Shu Wang,Zifeng Zhuang,Yang Liu,Xinyang Tong,Wenxuan Song,Shangke Lyu,Siteng Huang,Donglin Wang*

Main category: cs.RO

TL;DR: HiF-VLA通过利用运动进行双向时间推理，显著改善了机器人在长时间操控任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 针对现有VLA模型在长时间一致性方面的不足，通过运动来捕捉时间上下文和世界动态。

Method: 提出HiF-VLA框架，通过后视先验编码过去的动态，通过前视推理预见未来的运动，并通过后视调制联合专家整合两者。

Result: HiF-VLA在LIBERO-Long和CALVIN ABC-D基准测试上超越强基线，并在实际长时间操控任务中显著提高效果。

Conclusion: HiF-VLA在实际机器人操作中表现出广泛的有效性，并在长时间操控任务中取得显著改进。

Abstract: Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and linguistic cues into actions. However, most VLAs assume the Markov property, relying only on the current observation and thus suffering from temporal myopia that degrades long-horizon coherence. In this work, we view motion as a more compact and informative representation of temporal context and world dynamics, capturing inter-state changes while filtering static pixel-level noise. Building on this idea, we propose HiF-VLA (Hindsight, Insight, and Foresight for VLAs), a unified framework that leverages motion for bidirectional temporal reasoning. HiF-VLA encodes past dynamics through hindsight priors, anticipates future motion via foresight reasoning, and integrates both through a hindsight-modulated joint expert to enable a ''think-while-acting'' paradigm for long-horizon manipulation. As a result, HiF-VLA surpasses strong baselines on LIBERO-Long and CALVIN ABC-D benchmarks, while incurring negligible additional inference latency. Furthermore, HiF-VLA achieves substantial improvements in real-world long-horizon manipulation tasks, demonstrating its broad effectiveness in practical robotic settings.

</details>
