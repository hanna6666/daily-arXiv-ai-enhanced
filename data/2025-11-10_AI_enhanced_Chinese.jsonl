{"id": "2511.04758", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.04758", "abs": "https://arxiv.org/abs/2511.04758", "authors": ["Caelan Garrett", "Fabio Ramos"], "title": "ScheduleStream: Temporal Planning with Samplers for GPU-Accelerated Multi-Arm Task and Motion Planning & Scheduling", "comment": "Project website: https://schedulestream.github.io", "summary": "Bimanual and humanoid robots are appealing because of their human-like\nability to leverage multiple arms to efficiently complete tasks. However,\ncontrolling multiple arms at once is computationally challenging due to the\ngrowth in the hybrid discrete-continuous action space. Task and Motion Planning\n(TAMP) algorithms can efficiently plan in hybrid spaces but generally produce\nplans, where only one arm is moving at a time, rather than schedules that allow\nfor parallel arm motion. In order to extend TAMP to produce schedules, we\npresent ScheduleStream, the first general-purpose framework for planning &\nscheduling with sampling operations. ScheduleStream models temporal dynamics\nusing hybrid durative actions, which can be started asynchronously and persist\nfor a duration that's a function of their parameters. We propose\ndomain-independent algorithms that solve ScheduleStream problems without any\napplication-specific mechanisms. We apply ScheduleStream to Task and Motion\nPlanning & Scheduling (TAMPAS), where we use GPU acceleration within samplers\nto expedite planning. We compare ScheduleStream algorithms to several ablations\nin simulation and find that they produce more efficient solutions. We\ndemonstrate ScheduleStream on several real-world bimanual robot tasks at\nhttps://schedulestream.github.io.", "AI": {"tldr": "\u63d0\u51fa\u4e86ScheduleStream\u6846\u67b6\uff0c\u4ee5\u6539\u8fdb\u591a\u81c2\u673a\u5668\u4eba\u7684\u4efb\u52a1\u548c\u8fd0\u52a8\u89c4\u5212\uff0c\u5141\u8bb8\u5e76\u884c\u8fd0\u52a8\u5e76\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u591a\u81c2\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u8ba1\u7b97\u6311\u6218\uff0c\u63d0\u9ad8\u4efb\u52a1\u548c\u8fd0\u52a8\u89c4\u5212\u7684\u6548\u7387\u548c\u7075\u6d3b\u6027\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u5e76\u884c\u52a8\u4f5c\u7684\u573a\u666f\u4e2d\u3002", "method": "\u5f15\u5165ScheduleStream\u6846\u67b6\uff0c\u5229\u7528\u6df7\u5408\u6301\u7eed\u6027\u52a8\u4f5c\u5efa\u6a21\u65f6\u95f4\u52a8\u6001\uff0c\u5e76\u8bbe\u8ba1\u57df\u65e0\u5173\u7b97\u6cd5\u4ee5\u89e3\u51b3\u8c03\u5ea6\u95ee\u9898\uff0c\u540c\u65f6\u901a\u8fc7GPU\u52a0\u901f\u63d0\u5347\u89c4\u5212\u901f\u5ea6\u3002", "result": "ScheduleStream\u5728\u6a21\u62df\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u591a\u4e2a\u5b9e\u9645\u7684\u53cc\u81c2\u673a\u5668\u4eba\u4efb\u52a1\u3002", "conclusion": "ScheduleStream\u6210\u529f\u5730\u6269\u5c55\u4e86\u4efb\u52a1\u548c\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u591a\u81c2\u673a\u5668\u4eba\u4e2d\u5b9e\u73b0\u5e76\u884c\u8c03\u5ea6\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.04769", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04769", "abs": "https://arxiv.org/abs/2511.04769", "authors": ["Phat Nguyen", "Tsun-Hsuan Wang", "Zhang-Wei Hong", "Erfan Aasi", "Andrew Silva", "Guy Rosman", "Sertac Karaman", "Daniela Rus"], "title": "ReGen: Generative Robot Simulation via Inverse Design", "comment": null, "summary": "Simulation plays a key role in scaling robot learning and validating\npolicies, but constructing simulations remains a labor-intensive process. This\npaper introduces ReGen, a generative simulation framework that automates\nsimulation design via inverse design. Given a robot's behavior -- such as a\nmotion trajectory or an objective function -- and its textual description,\nReGen infers plausible scenarios and environments that could have caused the\nbehavior. ReGen leverages large language models to synthesize scenarios by\nexpanding a directed graph that encodes cause-and-effect relationships,\nrelevant entities, and their properties. This structured graph is then\ntranslated into a symbolic program, which configures and executes a robot\nsimulation environment. Our framework supports (i) augmenting simulations based\non ego-agent behaviors, (ii) controllable, counterfactual scenario generation,\n(iii) reasoning about agent cognition and mental states, and (iv) reasoning\nwith distinct sensing modalities, such as braking due to faulty GPS signals. We\ndemonstrate ReGen in autonomous driving and robot manipulation tasks,\ngenerating more diverse, complex simulated environments compared to existing\nsimulations with high success rates, and enabling controllable generation for\ncorner cases. This approach enhances the validation of robot policies and\nsupports data or simulation augmentation, advancing scalable robot learning for\nimproved generalization and robustness. We provide code and example videos at:\nhttps://regen-sim.github.io/", "AI": {"tldr": "ReGen\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u751f\u6210\u6a21\u62df\u73af\u5883\u7684\u6846\u67b6\uff0c\u80fd\u57fa\u4e8e\u673a\u5668\u4eba\u884c\u4e3a\u548c\u6587\u672c\u63cf\u8ff0\u63a8\u65ad\u51fa\u573a\u666f\uff0c\u589e\u5f3a\u673a\u5668\u4eba\u5b66\u4e60\u9a8c\u8bc1\u3002", "motivation": "\u6784\u5efa\u673a\u5668\u4eba\u5b66\u4e60\u548c\u9a8c\u8bc1\u7b56\u7565\u7684\u6a21\u62df\u662f\u4e00\u4e2a\u52b3\u52a8\u5bc6\u96c6\u7684\u8fc7\u7a0b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u65b9\u6cd5\u6765\u51cf\u5c11\u8fd9\u4e2a\u8fc7\u7a0b\u7684\u590d\u6742\u6027\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6269\u5c55\u7f16\u7801\u56e0\u679c\u5173\u7cfb\u7684\u6709\u5411\u56fe\uff0c\u5f62\u6210\u7b26\u53f7\u7a0b\u5e8f\uff0c\u4ece\u800c\u914d\u7f6e\u548c\u6267\u884c\u673a\u5668\u4eba\u6a21\u62df\u73af\u5883\u3002", "result": "\u63d0\u51fa\u4e86ReGen\uff0c\u4e00\u4e2a\u901a\u8fc7\u9006\u5411\u8bbe\u8ba1\u81ea\u52a8\u5316\u6a21\u62df\u8bbe\u8ba1\u7684\u751f\u6210\u6027\u6a21\u62df\u6846\u67b6\u3002", "conclusion": "ReGen\u5728\u81ea\u4e3b\u9a7e\u9a76\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u751f\u6210\u4e86\u66f4\u590d\u6742\u591a\u6837\u7684\u6a21\u62df\u73af\u5883\uff0c\u4ece\u800c\u6539\u5584\u4e86\u673a\u5668\u4eba\u7b56\u7565\u7684\u9a8c\u8bc1\u548c\u5b66\u4e60\u3002"}}
{"id": "2511.04812", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04812", "abs": "https://arxiv.org/abs/2511.04812", "authors": ["Zixuan Huang", "Huaidian Hou", "Dmitry Berenson"], "title": "Unified Multimodal Diffusion Forcing for Forceful Manipulation", "comment": "Project website: https://unified-df.github.io", "summary": "Given a dataset of expert trajectories, standard imitation learning\napproaches typically learn a direct mapping from observations (e.g., RGB\nimages) to actions. However, such methods often overlook the rich interplay\nbetween different modalities, i.e., sensory inputs, actions, and rewards, which\nis crucial for modeling robot behavior and understanding task outcomes. In this\nwork, we propose Multimodal Diffusion Forcing, a unified framework for learning\nfrom multimodal robot trajectories that extends beyond action generation.\nRather than modeling a fixed distribution, MDF applies random partial masking\nand trains a diffusion model to reconstruct the trajectory. This training\nobjective encourages the model to learn temporal and cross-modal dependencies,\nsuch as predicting the effects of actions on force signals or inferring states\nfrom partial observations. We evaluate MDF on contact-rich, forceful\nmanipulation tasks in simulated and real-world environments. Our results show\nthat MDF not only delivers versatile functionalities, but also achieves strong\nperformance, and robustness under noisy observations. More visualizations can\nbe found on our website https://unified-df.github.io", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6269\u6563\u5f3a\u8feb\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u591a\u6a21\u6001\u673a\u5668\u4eba\u8f68\u8ff9\u4e2d\u5b66\u4e60\uff0c\u8d85\u8d8a\u4e86\u7b80\u5355\u7684\u52a8\u4f5c\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u5ffd\u89c6\u4e0d\u540c\u6a21\u6001\u4e4b\u95f4\u7684\u4e30\u5bcc\u4ea4\u4e92\uff0c\u800c\u8fd9\u79cd\u4ea4\u4e92\u5bf9\u4e8e\u5efa\u6a21\u673a\u5668\u4eba\u884c\u4e3a\u548c\u7406\u89e3\u4efb\u52a1\u7ed3\u679c\u81f3\u5173\u91cd\u8981\u3002", "method": "Multimodal Diffusion Forcing (MDF)", "result": "MDF\u5728\u591a\u4e2a\u63a5\u89e6\u4e30\u5bcc\u548c\u5f3a\u52b2\u64cd\u4f5c\u7684\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u5f3a\u5927\u7684\u6027\u80fd\u548c\u7a33\u5065\u6027\uff0c\u5e76\u80fd\u591f\u6709\u6548\u5730\u5b66\u4e60\u591a\u6a21\u6001\u673a\u5668\u4eba\u8f68\u8ff9\u3002", "conclusion": "MDF\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u591a\u79cd\u529f\u80fd\uff0c\u8fd8\u5728\u566a\u58f0\u89c2\u6d4b\u4e0b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.04964", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.04964", "abs": "https://arxiv.org/abs/2511.04964", "authors": ["Lingyu Zhang", "Mitchell Wang", "Boyuan Chen"], "title": "Scientific judgment drifts over time in AI ideation", "comment": null, "summary": "Scientific discovery begins with ideas, yet evaluating early-stage research\nconcepts is a subtle and subjective human judgment. As large language models\n(LLMs) are increasingly tasked with generating scientific hypotheses, most\nsystems assume that scientists' evaluations form a fixed gold standard, and\nthat scientists' judgments do not change. Here we challenge this assumption. In\na two-wave study with 7,182 ratings from 57 active researchers across six\nscientific departments, each participant repeatedly evaluated a constant\n\"control\" research idea alongside AI-generated ideas. We show that scientists'\nratings of the very same idea systematically drift over time: overall quality\nscores increased by 0.61 points on a 0-10 scale (P = 0.005), and test-retest\nreliability was only moderate across core dimensions of scientific value,\nrevealing systematic temporal drift in perceived idea quality. Yet the internal\nstructure of judgment remained stable, such as the relative importance placed\non originality, feasibility, clarity. We then aligned an LLM-based ideation\nsystem to first-wave human ratings and used it to select new ideas. Although\nalignment improved agreement with Wave-1 evaluations, its apparent gains\ndisappeared once drift in human standards was accounted for. Thus, tuning to a\nfixed human snapshot produced improvements that were transient rather than\npersistent. These findings reveal that human evaluation of scientific ideas is\nnot static but a dynamic process with stable priorities and requires shifting\ncalibration. Treating one-time human ratings as immutable ground truth risks\noverstating progress in AI-assisted ideation and obscuring the challenge of\nco-evolving with changing expert standards. Drift-aware evaluation protocols\nand longitudinal benchmarks may therefore be essential for building AI systems\nthat reliably augment, rather than overfit to, human scientific judgment.", "AI": {"tldr": "\u79d1\u5b66\u5bb6\u5bf9\u7814\u7a76\u521b\u610f\u7684\u8bc4\u5224\u6807\u51c6\u5e76\u975e\u9759\u6001\uff0c\u800c\u662f\u4f1a\u968f\u65f6\u95f4\u53d8\u5316\uff0c\u663e\u793a\u51fa\u8d28\u91cf\u8bc4\u5206\u7684\u7cfb\u7edf\u6f02\u79fb\u3002AI\u5728\u79d1\u5b66\u521b\u610f\u751f\u6210\u4e2d\u7684\u5e94\u7528\u5e94\u8003\u8651\u8fd9\u79cd\u52a8\u6001\u6027\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u53c2\u4e0e\u79d1\u7814\u5047\u8bbe\u7684\u751f\u6210\uff0c\u7406\u89e3\u79d1\u5b66\u5bb6\u8bc4\u5224\u6807\u51c6\u7684\u53d8\u5316\u5bf9\u4e8e\u63d0\u9ad8AI\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u6709\u6548\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u4e24\u6ce2\u7814\u7a76\uff0c\u6536\u96c6\u4e8657\u540d\u7814\u7a76\u4eba\u5458\u5bf9\u63a7\u5236\u7814\u7a76\u521b\u610f\u548cAI\u751f\u6210\u521b\u610f\u76847182\u4e2a\u8bc4\u5206\uff0c\u4ee5\u5206\u6790\u5176\u8bc4\u4ef7\u6807\u51c6\u7684\u53d8\u52a8\u3002", "result": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u79d1\u5b66\u8bc4\u4f30\u7684\u52a8\u6001\u6027\uff0c\u53d1\u73b0\u79d1\u5b66\u5bb6\u5bf9\u7814\u7a76\u521b\u610f\u7684\u8bc4\u5224\u6807\u51c6\u5e76\u975e\u56fa\u5b9a\uff0c\u800c\u662f\u968f\u65f6\u95f4\u53d8\u5316\u3002\u7814\u7a76\u8868\u660e\uff0c\u5c3d\u7ba1\u5bf9\u67d0\u4e00\u7814\u7a76\u521b\u610f\u7684\u8bc4\u4ef7\u7a33\u5b9a\uff0c\u4f46\u6574\u4f53\u8d28\u91cf\u8bc4\u5206\u5728\u65f6\u95f4\u4e0a\u51fa\u73b0\u7cfb\u7edf\u6027\u7684\u6f02\u79fb\u3002\u8fd9\u4e00\u53d1\u73b0\u5bf9\u4eba\u5de5\u667a\u80fd\u751f\u6210\u79d1\u5b66\u5047\u8bbe\u7684\u7cfb\u7edf\u5177\u6709\u91cd\u8981\u542f\u793a\uff0c\u5f3a\u8c03\u4e86\u8bc4\u4f30\u6807\u51c6\u9700\u968f\u65f6\u95f4\u8c03\u6574\u3002", "conclusion": "\u79d1\u5b66\u8bc4\u4f30\u662f\u52a8\u6001\u7684\uff0c\u800c\u975e\u56fa\u5b9a\u7684\uff1b\u5ffd\u89c6\u8fd9\u4e00\u70b9\u53ef\u80fd\u5bfc\u81f4\u5bf9AI\u8f85\u52a9\u521b\u610f\u8fdb\u5c55\u7684\u8bef\u89e3\uff0c\u9700\u5efa\u7acb\u66f4\u6709\u6548\u7684\u8bc4\u4f30\u534f\u8bae\u3002"}}
{"id": "2511.04827", "categories": ["cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.04827", "abs": "https://arxiv.org/abs/2511.04827", "authors": ["Tobias Fischer", "Wolf Vollprecht", "Bas Zalmstra", "Ruben Arts", "Tim de Jager", "Alejandro Fontan", "Adam D Hines", "Michael Milford", "Silvio Traversaro", "Daniel Claes", "Scarlett Raine"], "title": "Pixi: Unified Software Development and Distribution for Robotics and AI", "comment": "20 pages, 3 figures, 11 code snippets", "summary": "The reproducibility crisis in scientific computing constrains robotics\nresearch. Existing studies reveal that up to 70% of robotics algorithms cannot\nbe reproduced by independent teams, while many others fail to reach deployment\nbecause creating shareable software environments remains prohibitively complex.\nThese challenges stem from fragmented, multi-language, and hardware-software\ntoolchains that lead to dependency hell. We present Pixi, a unified\npackage-management framework that addresses these issues by capturing exact\ndependency states in project-level lockfiles, ensuring bit-for-bit\nreproducibility across platforms. Its high-performance SAT solver achieves up\nto 10x faster dependency resolution than comparable tools, while integration of\nthe conda-forge and PyPI ecosystems removes the need for multiple managers.\nAdopted in over 5,300 projects since 2023, Pixi reduces setup times from hours\nto minutes and lowers technical barriers for researchers worldwide. By enabling\nscalable, reproducible, collaborative research infrastructure, Pixi accelerates\nprogress in robotics and AI.", "AI": {"tldr": "Pixi\u662f\u4e00\u4e2a\u7528\u4e8e\u89e3\u51b3\u79d1\u5b66\u8ba1\u7b97\u4e2d\u53ef\u91cd\u590d\u6027\u5371\u673a\u7684\u7edf\u4e00\u5305\u7ba1\u7406\u6846\u67b6\uff0c\u5df2\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u6280\u672f\u7814\u7a76\u4e2d\u3002", "motivation": "\u89e3\u51b3\u79d1\u5b66\u8ba1\u7b97\u4e2d\u73b0\u6709\u673a\u5668\u4eba\u7b97\u6cd5\u53ef\u91cd\u590d\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u514b\u670d\u8f6f\u4ef6\u73af\u5883\u8bbe\u7f6e\u7684\u590d\u6742\u6027\u3002", "method": "\u4f7f\u7528\u9ad8\u6027\u80fdSAT\u6c42\u89e3\u5668\u6765\u5b9e\u73b0\u4f9d\u8d56\u5173\u7cfb\u7684\u5feb\u901f\u89e3\u6790\uff0c\u7ed3\u5408\u9879\u76ee\u7ea7\u9501\u6587\u4ef6\u4fdd\u969c\u53ef\u91cd\u590d\u6027\uff0c\u540c\u65f6\u6574\u5408\u591a\u4e2a\u5305\u7ba1\u7406\u751f\u6001\u7cfb\u7edf\u3002", "result": "Pixi\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u5305\u7ba1\u7406\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u79d1\u5b66\u8ba1\u7b97\u4e2d\u7684\u53ef\u91cd\u590d\u6027\u5371\u673a\uff0c\u7279\u522b\u662f\u5728\u673a\u5668\u4eba\u6280\u672f\u7814\u7a76\u4e2d\u3002\u5176\u901a\u8fc7\u9879\u76ee\u7ea7\u9501\u6587\u4ef6\u6355\u6349\u7cbe\u786e\u7684\u4f9d\u8d56\u72b6\u6001\uff0c\u5b9e\u73b0\u8de8\u5e73\u53f0\u4f4d\u5bf9\u4f4d\u7684\u53ef\u91cd\u590d\u6027\u3002\u5176\u9ad8\u6027\u80fdSAT\u6c42\u89e3\u5668\u4f7f\u5f97\u4f9d\u8d56\u89e3\u6790\u901f\u5ea6\u6bd4\u7c7b\u4f3c\u5de5\u5177\u5feb10\u500d\uff0c\u5e76\u901a\u8fc7\u6574\u5408conda-forge\u548cPyPI\u751f\u6001\u7cfb\u7edf\uff0c\u6d88\u9664\u4e86\u591a\u4e2a\u7ba1\u7406\u5de5\u5177\u7684\u9700\u6c42\u3002Pixi\u5df2\u57282023\u5e74\u540e\u5e94\u7528\u4e8e\u8d85\u8fc75300\u4e2a\u9879\u76ee\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4ece\u51e0\u5c0f\u65f6\u5230\u51e0\u5206\u949f\u7684\u8bbe\u7f6e\u65f6\u95f4\uff0c\u964d\u4f4e\u4e86\u5168\u7403\u7814\u7a76\u4eba\u5458\u7684\u6280\u672f\u95e8\u69db\u3002", "conclusion": "Pixi\u901a\u8fc7\u63d0\u4f9b\u53ef\u9760\u7684\u5305\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u4fc3\u8fdb\u4e86\u673a\u5668\u4eba\u548c\u4eba\u5de5\u667a\u80fd\u9886\u57df\u7684\u53ef\u91cd\u590d\u6027\u548c\u534f\u4f5c\u7814\u7a76\uff0c\u63a8\u52a8\u4e86\u76f8\u5173\u9886\u57df\u7684\u8fdb\u6b65\u3002"}}
{"id": "2511.04995", "categories": ["cs.HC", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04995", "abs": "https://arxiv.org/abs/2511.04995", "authors": ["Amol Harsh", "Brainerd Prince", "Siddharth Siddharth", "Deepan Raj Prabakar Muthirayan", "Kabir S Bhalla", "Esraaj Sarkar Gupta", "Siddharth Sahu"], "title": "Enhancing Public Speaking Skills in Engineering Students Through AI", "comment": null, "summary": "This research-to-practice full paper was inspired by the persistent challenge\nin effective communication among engineering students. Public speaking is a\nnecessary skill for future engineers as they have to communicate technical\nknowledge with diverse stakeholders. While universities offer courses or\nworkshops, they are unable to offer sustained and personalized training to\nstudents. Providing comprehensive feedback on both verbal and non-verbal\naspects of public speaking is time-intensive, making consistent and\nindividualized assessment impractical. This study integrates research on verbal\nand non-verbal cues in public speaking to develop an AI-driven assessment model\nfor engineering students. Our approach combines speech analysis, computer\nvision, and sentiment detection into a multi-modal AI system that provides\nassessment and feedback. The model evaluates (1) verbal communication (pitch,\nloudness, pacing, intonation), (2) non-verbal communication (facial\nexpressions, gestures, posture), and (3) expressive coherence, a novel\nintegration ensuring alignment between speech and body language. Unlike\nprevious systems that assess these aspects separately, our model fuses multiple\nmodalities to deliver personalized, scalable feedback. Preliminary testing\ndemonstrated that our AI-generated feedback was moderately aligned with expert\nevaluations. Among the state-of-the-art AI models evaluated, all of which were\nLarge Language Models (LLMs), including Gemini and OpenAI models, Gemini Pro\nemerged as the best-performing, showing the strongest agreement with human\nannotators. By eliminating reliance on human evaluators, this AI-driven public\nspeaking trainer enables repeated practice, helping students naturally align\ntheir speech with body language and emotion, crucial for impactful and\nprofessional communication.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u878d\u5408\u8a00\u8bed\u5206\u6790\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u60c5\u611f\u68c0\u6d4b\u7684AI\u6a21\u578b\uff0c\u4ee5\u5e2e\u52a9\u5de5\u7a0b\u5b66\u751f\u6539\u5584\u516c\u5171\u6f14\u8bb2\u6280\u80fd\u3002", "motivation": "\u9762\u5bf9\u5de5\u7a0b\u5b66\u751f\u6c9f\u901a\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5f00\u53d1\u4e00\u79cd\u53ef\u6301\u7eed\u548c\u4e2a\u6027\u5316\u7684\u8bc4\u4f30\u5de5\u5177\u6765\u63d0\u5347\u4ed6\u4eec\u7684\u516c\u5171\u6f14\u8bb2\u80fd\u529b\u3002", "method": "\u6a21\u578b\u5229\u7528\u8a00\u8bed\u5206\u6790\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u60c5\u611f\u68c0\u6d4b\uff0c\u5bf9\u5b66\u751f\u7684\u8a00\u8bed\u4e0e\u975e\u8a00\u8bed\u4ea4\u6d41\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\u3002", "result": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cdAI\u9a71\u52a8\u7684\u516c\u5171\u6f14\u8bb2\u8bc4\u4f30\u6a21\u578b\uff0c\u901a\u8fc7\u6574\u5408\u8a00\u8bed\u4e0e\u975e\u8a00\u8bed\u4ea4\u6d41\u65b9\u5f0f\u7684\u7814\u7a76\uff0c\u4e3a\u5de5\u7a0b\u5b66\u751f\u63d0\u4f9b\u4e2a\u6027\u5316\u7684\u53cd\u9988\u548c\u8bc4\u4f30\u3002", "conclusion": "\u8fd9\u4e00AI\u9a71\u52a8\u7684\u516c\u5171\u6f14\u8bb2\u8bad\u7ec3\u5de5\u5177\u80fd\u591f\u5e2e\u52a9\u5b66\u751f\u53cd\u590d\u7ec3\u4e60\uff0c\u63d0\u5347\u4ed6\u4eec\u7684\u4e13\u4e1a\u6c9f\u901a\u80fd\u529b\u3002"}}
{"id": "2511.04831", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04831", "abs": "https://arxiv.org/abs/2511.04831", "authors": ["NVIDIA", ":", "Mayank Mittal", "Pascal Roth", "James Tigue", "Antoine Richard", "Octi Zhang", "Peter Du", "Antonio Serrano-Mu\u00f1oz", "Xinjie Yao", "Ren\u00e9 Zurbr\u00fcgg", "Nikita Rudin", "Lukasz Wawrzyniak", "Milad Rakhsha", "Alain Denzler", "Eric Heiden", "Ales Borovicka", "Ossama Ahmed", "Iretiayo Akinola", "Abrar Anwar", "Mark T. Carlson", "Ji Yuan Feng", "Animesh Garg", "Renato Gasoto", "Lionel Gulich", "Yijie Guo", "M. Gussert", "Alex Hansen", "Mihir Kulkarni", "Chenran Li", "Wei Liu", "Viktor Makoviychuk", "Grzegorz Malczyk", "Hammad Mazhar", "Masoud Moghani", "Adithyavairavan Murali", "Michael Noseworthy", "Alexander Poddubny", "Nathan Ratliff", "Welf Rehberg", "Clemens Schwarke", "Ritvik Singh", "James Latham Smith", "Bingjie Tang", "Ruchik Thaker", "Matthew Trepte", "Karl Van Wyk", "Fangzhou Yu", "Alex Millane", "Vikram Ramasamy", "Remo Steiner", "Sangeeta Subramanian", "Clemens Volk", "CY Chen", "Neel Jawale", "Ashwin Varghese Kuruttukulam", "Michael A. Lin", "Ajay Mandlekar", "Karsten Patzwaldt", "John Welsh", "Huihua Zhao", "Fatima Anes", "Jean-Francois Lafleche", "Nicolas Mo\u00ebnne-Loccoz", "Soowan Park", "Rob Stepinski", "Dirk Van Gelder", "Chris Amevor", "Jan Carius", "Jumyung Chang", "Anka He Chen", "Pablo de Heras Ciechomski", "Gilles Daviet", "Mohammad Mohajerani", "Julia von Muralt", "Viktor Reutskyy", "Michael Sauter", "Simon Schirm", "Eric L. Shi", "Pierre Terdiman", "Kenny Vilella", "Tobias Widmer", "Gordon Yeoman", "Tiffany Chen", "Sergey Grizan", "Cathy Li", "Lotus Li", "Connor Smith", "Rafael Wiltz", "Kostas Alexis", "Yan Chang", "David Chu", "Linxi \"Jim\" Fan", "Farbod Farshidian", "Ankur Handa", "Spencer Huang", "Marco Hutter", "Yashraj Narang", "Soha Pouya", "Shiwei Sheng", "Yuke Zhu", "Miles Macklin", "Adam Moravanszky", "Philipp Reist", "Yunrong Guo", "David Hoeller", "Gavriel State"], "title": "Isaac Lab: A GPU-Accelerated Simulation Framework for Multi-Modal Robot Learning", "comment": "Code and documentation are available here:\n  https://github.com/isaac-sim/IsaacLab", "summary": "We present Isaac Lab, the natural successor to Isaac Gym, which extends the\nparadigm of GPU-native robotics simulation into the era of large-scale\nmulti-modal learning. Isaac Lab combines high-fidelity GPU parallel physics,\nphotorealistic rendering, and a modular, composable architecture for designing\nenvironments and training robot policies. Beyond physics and rendering, the\nframework integrates actuator models, multi-frequency sensor simulation, data\ncollection pipelines, and domain randomization tools, unifying best practices\nfor reinforcement and imitation learning at scale within a single extensible\nplatform. We highlight its application to a diverse set of challenges,\nincluding whole-body control, cross-embodiment mobility, contact-rich and\ndexterous manipulation, and the integration of human demonstrations for skill\nacquisition. Finally, we discuss upcoming integration with the differentiable,\nGPU-accelerated Newton physics engine, which promises new opportunities for\nscalable, data-efficient, and gradient-based approaches to robot learning. We\nbelieve Isaac Lab's combination of advanced simulation capabilities, rich\nsensing, and data-center scale execution will help unlock the next generation\nof breakthroughs in robotics research.", "AI": {"tldr": "Isaac Lab\u662fIsaac Gym\u7684\u81ea\u7136\u7ee7\u627f\u8005\uff0c\u901a\u8fc7\u9ad8\u7ea7\u4eff\u771f\u548c\u4e30\u5bcc\u7684\u4f20\u611f\u80fd\u529b\uff0c\u65e8\u5728\u63a8\u52a8\u5927\u89c4\u6a21\u673a\u5668\u4eba\u5b66\u4e60\u548c\u7814\u7a76\u7684\u8fdb\u5c55\u3002", "motivation": "\u5ef6\u7eedIsaac Gym\u7684\u7406\u5ff5\uff0c\u63a8\u52a8\u5927\u89c4\u6a21\u591a\u6a21\u6001\u5b66\u4e60\u65f6\u4ee3\u7684\u673a\u5668\u4eba\u6a21\u62df\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u4e14\u53ef\u7ec4\u5408\u7684\u67b6\u6784\uff0c\u96c6\u6210\u9ad8\u4fdd\u771fGPU\u5e76\u884c\u7269\u7406\u3001\u7167\u7247\u7ea7\u6e32\u67d3\u3001\u9a71\u52a8\u6a21\u578b\u3001\u591a\u9891\u4f20\u611f\u5668\u4eff\u771f\u7b49\u529f\u80fd\u3002", "result": "\u8bc1\u660e\u4e86\u5176\u5728\u5168\u8eab\u63a7\u5236\u3001\u8de8\u89d2\u8272\u79fb\u52a8\u3001\u63a5\u89e6\u4e30\u5bcc\u7684\u7cbe\u7ec6\u64cd\u63a7\u548c\u4eba\u7c7b\u793a\u8303\u6280\u80fd\u83b7\u53d6\u7b49\u591a\u6837\u5316\u6311\u6218\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "Isaac Lab\u5c06\u63a8\u52a8\u673a\u5668\u4eba\u7814\u7a76\u7684\u4e0b\u4e00\u4ee3\u7a81\u7834\uff0c\u7ed3\u5408\u4e86\u9ad8\u7ea7\u4eff\u771f\u80fd\u529b\u3001\u4e30\u5bcc\u7684\u4f20\u611f\u529f\u80fd\u548c\u6570\u636e\u4e2d\u5fc3\u7ea7\u7684\u6267\u884c\u3002"}}
{"id": "2511.04997", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.04997", "abs": "https://arxiv.org/abs/2511.04997", "authors": ["Walter L. Leite", "Huibin Zhang", "Shibani Rana", "Yide Hao", "Amber D. Hatch", "Lingchen Kong", "Huan Kuang"], "title": "Do intelligent tutoring systems benefit K-12 students? A meta-analysis and evaluation of heterogeneity of treatment effects in the U.S", "comment": null, "summary": "To expand the use of intelligent tutoring systems (ITS) in K-12 schools, it\nis essential to understand the conditions under which their use is most\nbeneficial. This meta-analysis evaluated the heterogeneity of ITS effects\nacross studies focusing on elementary, middle, and high schools in the U.S. It\nincluded 18 studies with 77 effect sizes across 11 ITS. Overall, there was a\nsignificant positive effect size of ITS on U.S. K-12 students' learning\noutcomes (g=0.271, SE=0.011, p=0.001). Furthermore, effect sizes were similar\nacross elementary and middle schools, and for low-achieving students, but were\nlower in studies including rural schools. A MetaForest analysis showed that\nproviding worked-out examples, intervention duration, intervention condition,\ntype of learning outcome, and immediate measurement were the most important\nmoderators of treatment effects.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u835f\u8403\u5206\u6790\u53d1\u73b0\uff0c\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u5728K-12\u6559\u80b2\u4e2d\u5bf9\u5b66\u4e60\u6210\u679c\u6709\u79ef\u6781\u5f71\u54cd\uff0c\u5c24\u5176\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u3002", "motivation": "\u7406\u89e3\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u5728K-12\u5b66\u6821\u4e2d\u4f7f\u7528\u7684\u6700\u4f73\u6761\u4ef6\uff0c\u4ee5\u6269\u5927\u5176\u5e94\u7528\u3002", "method": "\u835f\u8403\u5206\u679018\u9879\u7814\u7a76\uff0c\u8bc4\u4f3011\u79cdITS\u7684\u5f71\u54cd\u6548\u679c\u3002", "result": "\u901a\u8fc7\u5bf918\u9879\u7814\u7a76\u7684\u835f\u8403\u5206\u6790\uff0c\u663e\u793aITS\u5bf9\u7f8e\u56fdK-12\u5b66\u751f\u5b66\u4e60\u6210\u679c\u7684\u663e\u8457\u6b63\u9762\u5f71\u54cd\u3002", "conclusion": "ITS\u5728\u63d0\u5347K-12\u5b66\u751f\u5b66\u4e60\u6548\u679c\u65b9\u9762\u6548\u679c\u663e\u8457\uff0c\u4f46\u5728\u4e0d\u540c\u80cc\u666f\u548c\u6761\u4ef6\u4e0b\u7684\u6548\u679c\u5b58\u5728\u5dee\u5f02\u3002"}}
{"id": "2511.04835", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04835", "abs": "https://arxiv.org/abs/2511.04835", "authors": ["Shubham Natraj", "Bruno Sinopoli", "Yiannis Kantaros"], "title": "Conformalized Non-uniform Sampling Strategies for Accelerated Sampling-based Motion Planning", "comment": null, "summary": "Sampling-based motion planners (SBMPs) are widely used to compute dynamically\nfeasible robot paths. However, their reliance on uniform sampling often leads\nto poor efficiency and slow planning in complex environments. We introduce a\nnovel non-uniform sampling strategy that integrates into existing SBMPs by\nbiasing sampling toward `certified' regions. These regions are constructed by\n(i) generating an initial, possibly infeasible, path using any heuristic path\npredictor (e.g., A* or vision-language models) and (ii) applying conformal\nprediction to quantify the predictor's uncertainty. This process yields\nprediction sets around the initial-guess path that are guaranteed, with\nuser-specified probability, to contain the optimal solution. To our knowledge,\nthis is the first non-uniform sampling approach for SBMPs that provides such\nprobabilistically correct guarantees on the sampling regions. Extensive\nevaluations demonstrate that our method consistently finds feasible paths\nfaster and generalizes better to unseen environments than existing baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u975e\u5747\u5300\u91c7\u6837\u7b56\u7565\uff0c\u7528\u4e8e\u63d0\u9ad8\u91c7\u6837\u57fa\u7840\u8fd0\u52a8\u89c4\u5212\u5668\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6548\u7387\u548c\u89c4\u5212\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u5747\u5300\u91c7\u6837\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u6548\u7387\u4f4e\u4e0b\uff0c\u6025\u9700\u6539\u8fdb\u3002", "method": "\u901a\u8fc7\u751f\u6210\u521d\u59cb\u8def\u5f84\u5e76\u5e94\u7528\u4fdd\u5f62\u9884\u6d4b\u6765\u6784\u5efa\u201c\u8ba4\u8bc1\u201d\u533a\u57df\uff0c\u504f\u5411\u4e8e\u8fd9\u4e9b\u533a\u57df\u8fdb\u884c\u975e\u5747\u5300\u91c7\u6837\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u591a\u9879\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u66f4\u5feb\u627e\u5230\u53ef\u884c\u8def\u5f84\u3001\u5728\u672a\u89c1\u8fc7\u7684\u73af\u5883\u4e2d\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8fd9\u79cd\u65b0\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u5c55\u793a\u51fa\u66f4\u5feb\u7684\u8def\u5f84\u627e\u5230\u80fd\u529b\u548c\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.05025", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05025", "abs": "https://arxiv.org/abs/2511.05025", "authors": ["Hala Sheta"], "title": "8bit-GPT: Exploring Human-AI Interaction on Obsolete Macintosh Operating Systems", "comment": "NeurIPS Creative AI Track 2025: Humanity", "summary": "The proliferation of assistive chatbots offering efficient, personalized\ncommunication has driven widespread over-reliance on them for decision-making,\ninformation-seeking and everyday tasks. This dependence was found to have\nadverse consequences on information retention as well as lead to superficial\nemotional attachment. As such, this work introduces 8bit-GPT; a language model\nsimulated on a legacy Macintosh Operating System, to evoke reflection on the\nnature of Human-AI interaction and the consequences of anthropomorphic\nrhetoric. Drawing on reflective design principles such as slow-technology and\ncounterfunctionality, this work aims to foreground the presence of chatbots as\na tool by defamiliarizing the interface and prioritizing inefficient\ninteraction, creating a friction between the familiar and not.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e868bit-GPT\u6a21\u578b\uff0c\u65e8\u5728\u63a2\u8ba8\u4eba\u673a\u4ea4\u4e92\u53ca\u5176\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u804a\u5929\u673a\u5668\u4eba\u4f7f\u7528\u4e2d\u5f15\u53d1\u7684\u4f9d\u8d56\u548c\u60c5\u611f\u8054\u7cfb\u95ee\u9898\u3002", "motivation": "To address over-reliance on assistive chatbots and its negative effects on information retention and emotional attachment.", "method": "Introduce 8bit-GPT, a language model on a legacy system, to explore Human-AI interaction and anthropomorphic rhetoric.", "result": "Developed a tool that promotes reflection on interactions with chatbots through a defamiliarized interface and inefficient interaction.", "conclusion": "\u901a\u8fc7\u53cd\u601d\u6027\u8bbe\u8ba1\u539f\u5219\uff0c8bit-GPT\u610f\u5728\u91cd\u65b0\u5ba1\u89c6\u804a\u5929\u673a\u5668\u4eba\u7684\u89d2\u8272\uff0c\u5e76\u63d0\u9192\u7528\u6237\u6ce8\u610f\u4e0e\u4e4b\u4e92\u52a8\u65f6\u53ef\u80fd\u4ea7\u751f\u7684\u4f9d\u8d56\u611f\u3002"}}
{"id": "2511.04837", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04837", "abs": "https://arxiv.org/abs/2511.04837", "authors": ["Cameron Robinson", "Ganghee Jang"], "title": "Design Exploration for Protection and Cleaning of Solar Panels with Case Studies for Space Missions", "comment": "4 pages, 3 figures (5 assets)", "summary": "Solar energy is used for many mission-critical applications including space\nexploration, sensor systems to monitor wildfires, etc. Their operation can be\nlimited or even terminated if solar panels are covered with dust or hit by\nspace debris. To address this issue, we designed panel cleaning mechanisms and\ntested protective materials. For cleaning mechanisms, we designed and compared\na wiper system and a rail system. For protective materials, we found through\ncollision tests that polycarbonate was very promising, though the most\nimportant factor was layering a soft material between the panel's surface and a\nhard material. In the cleaning system comparisons, the wiper-based system was\nmore efficient than the rail-based system in terms of cost, cleaning speed, and\ntotal power consumption.", "AI": {"tldr": "\u7814\u7a76\u4e86\u592a\u9633\u80fd\u9762\u677f\u6e05\u6d01\u673a\u5236\u548c\u4fdd\u62a4\u6750\u6599\uff0c\u53d1\u73b0\u522e\u677f\u7cfb\u7edf\u5728\u6210\u672c\u3001\u6e05\u6d01\u901f\u5ea6\u548c\u603b\u7535\u529b\u6d88\u8017\u65b9\u9762\u4f18\u4e8e\u8f68\u9053\u7cfb\u7edf\uff0c\u540c\u65f6\u805a\u78b3\u9178\u916f\u6750\u6599\u5728\u78b0\u649e\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u826f\u7684\u4fdd\u62a4\u6027\uff0c\u7279\u522b\u662f\u9700\u8981\u5728\u786c\u6750\u6599\u4e0e\u9762\u677f\u8868\u9762\u4e4b\u95f4\u5c42\u53e0\u8f6f\u6750\u6599\u3002", "motivation": "\u592a\u9633\u80fd\u5728\u7a7a\u95f4\u63a2\u7d22\u548c\u76d1\u6d4b\u91ce\u706b\u7b49\u5173\u952e\u4efb\u52a1\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4fc3\u4f7f\u6211\u4eec\u89e3\u51b3\u9762\u677f\u53d7\u5c18\u57c3\u548c\u788e\u7247\u5f71\u54cd\u7684\u6e05\u6d01\u548c\u4fdd\u62a4\u95ee\u9898\u3002", "method": "\u6211\u4eec\u8bbe\u8ba1\u5e76\u6bd4\u8f83\u4e86\u522e\u677f\u7cfb\u7edf\u548c\u8f68\u9053\u7cfb\u7edf\u7684\u6e05\u6d01\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u78b0\u649e\u6d4b\u8bd5\u8bc4\u4f30\u4e0d\u540c\u4fdd\u62a4\u6750\u6599\u7684\u6548\u679c\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u592a\u9633\u80fd\u9762\u677f\u56e0\u5c18\u57c3\u8986\u76d6\u6216\u592a\u7a7a\u788e\u7247\u51b2\u51fb\u800c\u5bfc\u81f4\u7684\u64cd\u4f5c\u9650\u5236\u6216\u7ec8\u6b62\u95ee\u9898\u7684\u65b9\u6848\uff0c\u901a\u8fc7\u8bbe\u8ba1\u6e05\u6d01\u673a\u5236\u548c\u6d4b\u8bd5\u4fdd\u62a4\u6750\u6599\u6765\u5b9e\u73b0\u3002", "conclusion": "\u6e05\u6d01\u7cfb\u7edf\u4e2d\u7684\u522e\u677f\u5f0f\u7cfb\u7edf\u6bd4\u8f68\u9053\u5f0f\u7cfb\u7edf\u5728\u591a\u9879\u6307\u6807\u4e0a\u8868\u73b0\u66f4\u4f73\uff0c\u800c\u5728\u6750\u6599\u4fdd\u62a4\u65b9\u9762\uff0c\u805a\u78b3\u9178\u916f\u548c\u8f6f\u6750\u6599\u5c42\u53e0\u7684\u7ec4\u5408\u4e3a\u5173\u952e\u3002"}}
{"id": "2511.05066", "categories": ["cs.HC", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.05066", "abs": "https://arxiv.org/abs/2511.05066", "authors": ["Philipp Schaad", "Tal Ben-Nun", "Torsten Hoefler"], "title": "VEIL: Reading Control Flow Graphs Like Code", "comment": null, "summary": "Control flow graphs (CFGs) are essential tools for understanding program\nbehavior, yet the size of real-world CFGs makes them difficult to interpret.\nWith thousands of nodes and edges, sophisticated graph drawing algorithms are\nrequired to present them on screens in ways that make them readable and\nunderstandable. However, being designed for general graphs, these algorithms\nfrequently break the natural flow of execution, placing later instructions\nbefore earlier ones and obscuring critical program structures. In this paper,\nwe introduce a set of criteria specifically tailored for CFG visualization,\nfocusing on preserving execution order and making complex structures easier to\nfollow. Building on these criteria, we present VEIL, a new layout algorithm\nthat uses dominator analysis to produce clearer, more intuitive CFG layouts.\nThrough a study of CFGs from real-world applications, we show how our method\nimproves readability and provides improved layout performance compared to state\nof the art graph drawing techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684CFG\u53ef\u89c6\u5316\u7b97\u6cd5VEIL\uff0c\u65e8\u5728\u63d0\u9ad8\u7a0b\u5e8f\u63a7\u5236\u6d41\u56fe\u7684\u53ef\u8bfb\u6027\u3002", "motivation": "\u63d0\u9ad8\u73b0\u5b9e\u4e16\u754cCFG\u7684\u53ef\u8bfb\u6027\u548c\u7406\u89e3\u6027", "method": "\u63d0\u51faVEIL\u7b97\u6cd5", "result": "VEIL\u7b97\u6cd5\u63d0\u4f9b\u4e86\u66f4\u6e05\u6670\u3001\u76f4\u89c2\u7684CFG\u5e03\u5c40\uff0c\u63d0\u9ad8\u4e86\u53ef\u8bfb\u6027\u548c\u5e03\u5c40\u6027\u80fd\u3002", "conclusion": "VEIL\u901a\u8fc7\u4fdd\u7559\u6267\u884c\u987a\u5e8f\u548c\u7b80\u5316\u590d\u6742\u7ed3\u6784\uff0c\u663e\u8457\u6539\u5584\u4e86CFG\u7684\u53ef\u89c6\u5316\u6548\u679c\u3002"}}
{"id": "2511.04976", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04976", "abs": "https://arxiv.org/abs/2511.04976", "authors": ["Xin Nie", "Zhiyuan Cheng", "Yuan Zhang", "Chao Ji", "Jiajia Wu", "Yuhan Zhang", "Jia Pan"], "title": "iFlyBot-VLM Technical Report", "comment": null, "summary": "We introduce iFlyBot-VLM, a general-purpose Vision-Language Model (VLM) used\nto improve the domain of Embodied Intelligence. The central objective of\niFlyBot-VLM is to bridge the cross-modal semantic gap between high-dimensional\nenvironmental perception and low-level robotic motion control. To this end, the\nmodel abstracts complex visual and spatial information into a body-agnostic and\ntransferable Operational Language, thereby enabling seamless perception-action\nclosed-loop coordination across diverse robotic platforms. The architecture of\niFlyBot-VLM is systematically designed to realize four key functional\ncapabilities essential for embodied intelligence: 1) Spatial Understanding and\nMetric Reasoning; 2) Interactive Target Grounding; 3) Action Abstraction and\nControl Parameter Generation; 4) Task Planning and Skill Sequencing. We\nenvision iFlyBot-VLM as a scalable and generalizable foundation model for\nembodied AI, facilitating the progression from specialized task-oriented\nsystems toward generalist, cognitively capable agents. We conducted evaluations\non 10 current mainstream embodied intelligence-related VLM benchmark datasets,\nsuch as Blink and Where2Place, and achieved optimal performance while\npreserving the model's general capabilities. We will publicly release both the\ntraining data and model weights to foster further research and development in\nthe field of Embodied Intelligence.", "AI": {"tldr": "iFlyBot-VLM\u662f\u4e00\u79cd\u65b0\u578b\u901a\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u9ad8\u4f53\u73b0\u667a\u80fd\u7684\u4efb\u52a1\u8868\u73b0\uff0c\u901a\u8fc7\u56db\u4e2a\u5173\u952e\u529f\u80fd\u652f\u6301\u611f\u77e5-\u884c\u52a8\u534f\u8c03\u3002", "motivation": "\u63a8\u52a8\u4f53\u73b0\u667a\u80fd\u7684\u53d1\u5c55\uff0c\u5e76\u5b9e\u73b0\u4efb\u52a1\u7cfb\u7edf\u5230\u901a\u7528\u4ee3\u7406\u7684\u8f6c\u53d8\uff0c\u63d0\u5347\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u6a21\u578b\u901a\u8fc7\u62bd\u8c61\u590d\u6742\u89c6\u89c9\u548c\u7a7a\u95f4\u4fe1\u606f\uff0c\u751f\u6210\u4e0e\u8eab\u4f53\u65e0\u5173\u3001\u53ef\u8fc1\u79fb\u7684\u64cd\u4f5c\u8bed\u8a00\uff0c\u652f\u6301\u611f\u77e5-\u884c\u52a8\u95ed\u73af\u534f\u8c03\u3002", "result": "\u672c\u6587\u4ecb\u7ecd\u4e86iFlyBot-VLM\uff0c\u4e00\u79cd\u7528\u4e8e\u63d0\u5347\u4f53\u73b0\u667a\u80fd\u9886\u57df\u7684\u901a\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u3002\u8be5\u6a21\u578b\u7684\u6838\u5fc3\u76ee\u6807\u662f\u5f25\u5408\u9ad8\u7ef4\u73af\u5883\u611f\u77e5\u4e0e\u4f4e\u7ea7\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\u4e4b\u95f4\u7684\u8de8\u6a21\u6001\u8bed\u4e49\u5dee\u8ddd\u3002", "conclusion": "iFlyBot-VLM\u88ab\u89c6\u4e3a\u4f53\u73b0\u4eba\u5de5\u667a\u80fd\u7684\u53ef\u6269\u5c55\u57fa\u7840\u6a21\u578b\uff0c\u6709\u52a9\u4e8e\u4ece\u4e13\u95e8\u4efb\u52a1\u7cfb\u7edf\u5411\u666e\u9002\u8ba4\u77e5\u80fd\u529b\u4ee3\u7406\u7684\u53d1\u5c55\u3002"}}
{"id": "2511.05094", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.05094", "abs": "https://arxiv.org/abs/2511.05094", "authors": ["Zhaoyang Li", "Shangzhuo Xie", "Qianqian Yang"], "title": "FM4Com: Foundation Model for Scene-Adaptive Communication Strategy Optimization", "comment": null, "summary": "The emergence of sixth-generation (6G) networks heralds an intelligent\ncommunication ecosystem driven by AI-native air interfaces. However, current\nphysical-layer designs-typically following modular and isolated optimization\nparadigms-fail to achieve global end-to-end optimality due to neglected\ninter-module dependencies. Although large language models (LLMs) have recently\nbeen applied to communication tasks such as beam prediction and resource\nallocation, existing studies remain limited to single-task or single-modality\nscenarios and lack the ability to jointly reason over communication states and\nuser intents for personalized strategy adaptation. To address these\nlimitations, this paper proposes a novel multimodal communication\ndecision-making model based on reinforcement learning. The proposed model\nsemantically aligns channel state information (CSI) and textual user\ninstructions, enabling comprehensive understanding of both physical-layer\nconditions and communication intents. It then generates physically realizable,\nuser-customized link construction strategies that dynamically adapt to changing\nenvironments and preference tendencies. A two-stage reinforcement learning\nframework is employed: the first stage expands the experience pool via\nheuristic exploration and behavior cloning to obtain a near-optimal\ninitialization, while the second stage fine-tunes the model through\nmulti-objective reinforcement learning considering bit error rate, throughput,\nand complexity. Experimental results demonstrate that the proposed model\nsignificantly outperforms conventional planning-based algorithms under\nchallenging channel conditions, achieving robust, efficient, and personalized\n6G link construction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u6a21\u6001\u901a\u4fe1\u51b3\u7b56\u6a21\u578b\uff0c\u80fd\u591f\u66f4\u597d\u5730\u9002\u5e946G\u7f51\u7edc\u7684\u590d\u6742\u73af\u5883\u548c\u7528\u6237\u9700\u6c42\u3002", "motivation": "\u5f53\u524d\u7269\u7406\u5c42\u8bbe\u8ba1\u7684\u6a21\u5757\u5316\u4f18\u5316\u65e0\u6cd5\u5b9e\u73b0\u5168\u5c40\u6700\u4f73\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u901a\u4fe1\u72b6\u6001\u548c\u7528\u6237\u610f\u56fe\u7684\u8054\u5408\u63a8\u7406\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u6a21\u6001\u901a\u4fe1\u51b3\u7b56\u6a21\u578b", "result": "\u63d0\u51fa\u7684\u6a21\u578b\u5728\u6076\u52a3\u4fe1\u9053\u6761\u4ef6\u4e0b\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u57fa\u4e8e\u89c4\u5212\u7684\u7b97\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u7a33\u5065\u3001\u9ad8\u6548\u548c\u4e2a\u6027\u5316\u76846G\u94fe\u8def\u6784\u5efa\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u878d\u5408\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u548c\u7528\u6237\u6307\u4ee4\u7684\u6a21\u578b\u80fd\u591f\u6709\u6548\u63d0\u53476G\u901a\u4fe1\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u4e2a\u6027\u5316\u7b56\u7565\u3002"}}
{"id": "2511.04992", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04992", "abs": "https://arxiv.org/abs/2511.04992", "authors": ["Bibekananda Patra", "Sandipan Bandyopadhyay"], "title": "A semi-analytical approach for computing the largest singularity-free spheres of a class of 6-6 Stewart-Gough platforms for specified orientation workspaces", "comment": null, "summary": "This article presents a method for computing the largest singularity-free\nsphere (SFS) of a 6-6 Stewart-Gough platform manipulator (SGPM) over a\nspecified orientation workspace. For a fixed orientation of the moving\nplatform, the SFS is computed analytically. This process is repeated over a set\nof samples generated within the orientation workspace, and the smallest among\nthem is designated as the desired SFS for the given orientation workspace.\nNumerical experiments are performed on four distinct architectures of the SGPM\nto understand their relative performances w.r.t. SFS volumes over the same\norientation workspace. This study demonstrates the potential utility of the\nproposed computational method both in analysis and design of SGPMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b976-6 SGPM\u6700\u5927\u65e0\u5947\u70b9\u7403\u7684\u5206\u6790\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u786e\u5b9a\u7ed9\u5b9a\u65b9\u5411\u5de5\u4f5c\u7a7a\u95f4\u4e2dSGPM\u7684\u6700\u4f73\u8bbe\u8ba1\u548c\u6027\u80fd\u3002", "method": "\u5206\u6790\u8ba1\u7b976-6\u65af\u56fe\u5c14\u7279-\u6208\u592b\u5e73\u53f0\u64cd\u7eb5\u5668\uff08SGPM\uff09\u7684\u6700\u5927\u65e0\u5947\u70b9\u7403\uff08SFS\uff09\u3002", "result": "\u901a\u8fc7\u5bf9\u56db\u79cd\u4e0d\u540c\u67b6\u6784\u7684SGPM\u8fdb\u884c\u6570\u503c\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e86\u5b83\u4eec\u5728\u76f8\u540c\u65b9\u5411\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u7684SFS\u4f53\u79ef\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728SGPM\u7684\u5206\u6790\u548c\u8bbe\u8ba1\u4e2d\u5177\u6709\u6f5c\u5728\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.05136", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.05136", "abs": "https://arxiv.org/abs/2511.05136", "authors": ["Patrice Labedan", "Nicolas Drougard"], "title": "Interface Homme-Machine pour l'Identification des Liaisons de Coins", "comment": "21 pages, in French language. 25 figures, projet ACCADIL (Ancient\n  Coin Classification Algorithms for DIe Links), Travaux connexes:\n  arXiv:2502.01186", "summary": "ACCADIL is a project that led to the development of software tools for the\nidentification of coin die links from coin photographs. It provides a\ncomputational algorithm based on computer vision and classification techniques,\nalong with an online interface for the interactive verification of results.\nThis guide briefly describes the algorithmic principles, the preparation of\ndata prior to analysis, and the features offered by the interface: dataset\naddition, visualization modes (overlay, side-by-side, magnifier, transparency),\nresult export, and distance visualization. ACCADIL thus provides numismatists\nwith a comprehensive tool for the analysis of die links within a coin\ncollection.", "AI": {"tldr": "ACCADIL\u9879\u76ee\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u8f6f\u4ef6\u5de5\u5177\uff0c\u5e2e\u52a9\u94b1\u5e01\u5b66\u5bb6\u8bc6\u522b\u94b1\u5e01\u6a21\u5177\u94fe\u63a5\uff0c\u5e76\u63d0\u4f9b\u591a\u79cd\u53ef\u89c6\u5316\u548c\u6570\u636e\u5904\u7406\u529f\u80fd\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u94b1\u5e01\u6a21\u5177\u94fe\u63a5\u8bc6\u522b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5f00\u53d1\u4e86\u8fd9\u4e00\u8f6f\u4ef6\u5de5\u5177\u3002", "method": "\u4f7f\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u5206\u7c7b\u6280\u672f\u7684\u8ba1\u7b97\u7b97\u6cd5", "result": "\u5f00\u53d1\u4e86\u5728\u7ebf\u63a5\u53e3\uff0c\u652f\u6301\u4ea4\u4e92\u5f0f\u7ed3\u679c\u9a8c\u8bc1\uff0c\u5e76\u63d0\u4f9b\u591a\u79cd\u6570\u636e\u53ef\u89c6\u5316\u548c\u5bfc\u51fa\u529f\u80fd\u3002", "conclusion": "ACCADIL\u4e3a\u94b1\u5e01\u5b66\u5bb6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u5206\u6790\u5de5\u5177\uff0c\u5e2e\u52a9\u8bc6\u522b\u94b1\u5e01\u7684\u6a21\u5177\u94fe\u63a5\u3002"}}
{"id": "2511.04994", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04994", "abs": "https://arxiv.org/abs/2511.04994", "authors": ["Xingyuan Zhou", "Peter Paik", "S. Farokh Atashzar"], "title": "Encoding Biomechanical Energy Margin into Passivity-based Synchronization for Networked Telerobotic Systems", "comment": null, "summary": "Maintaining system stability and accurate position tracking is imperative in\nnetworked robotic systems, particularly for haptics-enabled human-robot\ninteraction. Recent literature has integrated human biomechanics into the\nstabilizers implemented for teleoperation, enhancing force preservation while\nguaranteeing convergence and safety. However, position desynchronization due to\nimperfect communication and non-passive behaviors remains a challenge. This\npaper proposes a two-port biomechanics-aware passivity-based synchronizer and\nstabilizer, referred to as TBPS2. This stabilizer optimizes position\nsynchronization by leveraging human biomechanics while reducing the\nstabilizer's conservatism in its activation. We provide the mathematical design\nsynthesis of the stabilizer and the proof of stability. We also conducted a\nseries of grid simulations and systematic experiments, comparing their\nperformance with that of state-of-the-art solutions under varying time delays\nand environmental conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u57fa\u4e8e\u751f\u7269\u529b\u5b66\u7684\u7a33\u5b9a\u5668TBPS2\uff0c\u65e8\u5728\u89e3\u51b3\u7f51\u7edc\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u4f4d\u7f6e\u540c\u6b65\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5728\u7f51\u7edc\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7279\u522b\u662f\u5728\u5b9e\u73b0\u4eba\u673a\u4ea4\u4e92\u7684\u8fc7\u7a0b\u4e2d\uff0c\u7ef4\u62a4\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u7684\u4f4d\u7f6e\u8ddf\u8e2a\u81f3\u5173\u91cd\u8981\uff0c\u76ee\u524d\u6587\u732e\u5bf9\u751f\u7269\u529b\u5b66\u7684\u5e94\u7528\u6709\u6240\u8fdb\u5c55\uff0c\u4f46\u4ecd\u9762\u4e34\u7740\u901a\u4fe1\u4e0d\u5b8c\u7f8e\u5e26\u6765\u7684\u4f4d\u7f6e\u4e0d\u540c\u6b65\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6570\u5b66\u8bbe\u8ba1\u5408\u6210\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u4eba\u673a\u751f\u7269\u529b\u5b66\u7684\u4e24\u7aef\u53e3\u88ab\u52a8\u540c\u6b65\u5668\u548c\u7a33\u5b9a\u5668\uff0c\u5e76\u8fdb\u884c\u4e86\u7a33\u5b9a\u6027\u8bc1\u660e\u3002", "result": "\u901a\u8fc7\u4e0e\u73b0\u6709\u6280\u672f\u7684\u6bd4\u8f83\uff0cTBPS2\u5728\u4e0d\u540c\u65f6\u95f4\u5ef6\u8fdf\u548c\u73af\u5883\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u4ee3\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u4eba\u673a\u751f\u7269\u529b\u5b66\u6574\u5408\u7684\u6f5c\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684TBPS2\u7a33\u5b9a\u5668\u5728\u591a\u79cd\u73af\u5883\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u66f4\u4f18\u7684\u540c\u6b65\u6027\u80fd\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7531\u4e8e\u901a\u4fe1\u4e0d\u5b8c\u5584\u5bfc\u81f4\u7684\u4f4d\u7f6e\u4fe1\u6b65\u91c7\u4e0d\u540c\u6b65\u7684\u95ee\u9898\u3002"}}
{"id": "2511.05304", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.05304", "abs": "https://arxiv.org/abs/2511.05304", "authors": ["Akhil Ajikumar", "Sahil Mayenkar", "Steven Yoo", "Sakib Reza", "Mohsen Moghaddam"], "title": "psiUnity: A Platform for Multimodal Data-Driven XR", "comment": null, "summary": "Extended reality (XR) research increasingly relies on the ability to stream\nand synchronize multimodal data between headsets and immersive applications for\ndata-driven interaction and experimentation. However, developers face a\ncritical gap: the Platform for Situated Intelligence (psi), which excels at\ndeterministic temporal alignment and multimodal data management, has been\nlargely inaccessible to the dominant Unity/MRTK ecosystem used for HoloLens\ndevelopment. We introduce psiUnity, an open-source C# integration that bridges\npsi's .NET libraries with Unity 2022.3 and MRTK3 for HoloLens 2. psiUnity\nenables bidirectional, real-time streaming of head pose, hand tracking, gaze,\nIMU, audio, and depth sensor data (AHAT and long-throw) with microsecond-level\ntemporal precision, allowing Unity applications to both consume and produce\nsynchronized multimodal data streams. By embedding psi's native serialization,\nlogging, and temporal coordination directly within Unity's architecture,\npsiUnity extends psi beyond its previous StereoKit limitations and empowers the\nHRI, HCI, and embodied-AI communities to develop reproducible, data-driven XR\ninteractions and experiments within the familiar Unity environment. The\nintegration is available at https://github.com/sailgt/psiUnity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fapsiUnity\uff0c\u4e00\u4e2a\u5f00\u6e90\u6574\u5408\u5de5\u5177\uff0c\u8fde\u63a5psi\u4e0eUnity\uff0c\u652f\u6301\u5b9e\u65f6\u591a\u6a21\u6001\u6570\u636e\u7684\u53cc\u5411\u6d41\u52a8\uff0c\u4fc3\u8fdbXR\u4ea4\u4e92\u5b9e\u9a8c\u3002", "motivation": "\u73b0\u6709\u7684HoloLens\u5f00\u53d1\u53d7\u9650\u4e8eUnity/MRTK\u751f\u6001\u7cfb\u7edf\u4e0epsi\u4e4b\u95f4\u7684\u8fde\u63a5\u95ee\u9898\uff0c\u65e0\u6cd5\u6709\u6548\u8fdb\u884c\u591a\u6a21\u6001\u6570\u636e\u7684\u540c\u6b65\u548c\u7ba1\u7406\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2aC#\u96c6\u6210\u5de5\u5177psiUnity\uff0c\u652f\u6301\u5b9e\u65f6\u53cc\u5411\u6d41\u5f0f\u4f20\u8f93\u6570\u636e\uff0c\u589e\u5f3aUnity\u4e0epsi\u7684\u8fde\u63a5\u3002", "result": "\u672c\u6587\u4ecb\u7ecd\u4e86psiUnity\uff0c\u4e00\u4e2a\u5f00\u6e90\u7684C#\u96c6\u6210\u5de5\u5177\uff0c\u65e8\u5728\u5f25\u8865\u73b0\u6709Unity/MRTK\u751f\u6001\u7cfb\u7edf\u4e0ePlatform for Situated Intelligence\uff08psi\uff09\u4e4b\u95f4\u7684\u5173\u952e\u5dee\u8ddd\uff0c\u4f7f\u5f97HoloLens\u5f00\u53d1\u8005\u80fd\u591f\u9ad8\u6548\u540c\u6b65\u548c\u6d41\u5f0f\u4f20\u8f93\u591a\u6a21\u6001\u6570\u636e\u3002", "conclusion": "psiUnity\u901a\u8fc7\u5728Unity\u67b6\u6784\u4e2d\u5d4c\u5165psi\u7684\u672c\u5730\u5e8f\u5217\u5316\u548c\u65f6\u95f4\u534f\u8c03\u529f\u80fd\uff0c\u7a81\u7834\u4e86StereoKit\u7684\u5c40\u9650\u6027\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8HRI\u3001HCI\u548c\u5177\u8eabAI\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u3002"}}
{"id": "2511.05007", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05007", "abs": "https://arxiv.org/abs/2511.05007", "authors": ["Baiye Cheng", "Tianhai Liang", "Suning Huang", "Maanping Shao", "Feihong Zhang", "Botian Xu", "Zhengrong Xue", "Huazhe Xu"], "title": "MoE-DP: An MoE-Enhanced Diffusion Policy for Robust Long-Horizon Robotic Manipulation with Skill Decomposition and Failure Recovery", "comment": null, "summary": "Diffusion policies have emerged as a powerful framework for robotic\nvisuomotor control, yet they often lack the robustness to recover from subtask\nfailures in long-horizon, multi-stage tasks and their learned representations\nof observations are often difficult to interpret. In this work, we propose the\nMixture of Experts-Enhanced Diffusion Policy (MoE-DP), where the core idea is\nto insert a Mixture of Experts (MoE) layer between the visual encoder and the\ndiffusion model. This layer decomposes the policy's knowledge into a set of\nspecialized experts, which are dynamically activated to handle different phases\nof a task. We demonstrate through extensive experiments that MoE-DP exhibits a\nstrong capability to recover from disturbances, significantly outperforming\nstandard baselines in robustness. On a suite of 6 long-horizon simulation\ntasks, this leads to a 36% average relative improvement in success rate under\ndisturbed conditions. This enhanced robustness is further validated in the real\nworld, where MoE-DP also shows significant performance gains. We further show\nthat MoE-DP learns an interpretable skill decomposition, where distinct experts\ncorrespond to semantic task primitives (e.g., approaching, grasping). This\nlearned structure can be leveraged for inference-time control, allowing for the\nrearrangement of subtasks without any re-training.Our video and code are\navailable at the https://moe-dp-website.github.io/MoE-DP-Website/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7684\u6269\u6563\u653f\u7b56\u6846\u67b6MoE-DP\uff0c\u5229\u7528\u4e13\u5bb6\u6df7\u5408\u5c42\u63d0\u9ad8\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u7684\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6269\u6563\u653f\u7b56\u5728\u957f\u65f6\u95f4\u591a\u9636\u6bb5\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u4e14\u5b66\u4e60\u5230\u7684\u89c2\u5bdf\u8868\u793a\u96be\u4ee5\u89e3\u91ca\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "\u5728\u89c6\u89c9\u7f16\u7801\u5668\u548c\u6269\u6563\u6a21\u578b\u4e4b\u95f4\u63d2\u5165\u4e00\u4e2a\u4e13\u5bb6\u6df7\u5408\u5c42\uff0c\u52a8\u6001\u6fc0\u6d3b\u4e0d\u540c\u4e13\u5bb6\u5904\u7406\u4efb\u52a1\u7684\u4e0d\u540c\u9636\u6bb5\u3002", "result": "\u57286\u4e2a\u957f\u65f6\u95f4\u6a21\u62df\u4efb\u52a1\u4e2d\uff0cMoE-DP\u5728\u6270\u52a8\u6761\u4ef6\u4e0b\u6210\u529f\u7387\u5e73\u5747\u63d0\u9ad8\u4e8636%\u3002", "conclusion": "MoE-DP\u5728\u957f\u65f6\u95f4\u6a21\u62df\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u9c81\u68d2\u6027\uff0c\u63d0\u9ad8\u4e86\u6210\u529f\u7387\uff0c\u5e76\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2511.05346", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.05346", "abs": "https://arxiv.org/abs/2511.05346", "authors": ["Olaf V. Adan", "Dimitra Dritsa", "Steven Houben"], "title": "Semantic Interactivity: leveraging NLP to enable a shared interaction approach for joint activities", "comment": "15 pages, 14 figures", "summary": "Collocated collaboration, where individuals work together in the same\nphysical space and time, remains a cornerstone of effective teamwork. However,\nmost collaborative systems are designed to support individual tasks rather than\njoint activities; they enable interactions for users to complete tasks rather\nthan interactivity to engage in shared experiences. In this work, we introduce\nan NLP-driven mechanism that enables semantic interactivity through a shared\ninteraction mechanism. This mechanism was developed as part of CollEagle, an\ninteractive tabletop system that supports shared externalisation practices by\noffering a low-effort way for users to create, curate, organise, and structure\ninformation to capture the essence of collaborative discussions. Our\npreliminary study highlights the potential for semantic interactivity to\nmediate group interactions, suggesting that the interaction approach paves the\nway for designing novel collaborative interfaces. We contribute our\nimplementation and offer insights for future research to enable semantic\ninteractivity in systems that support joint activities.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u673a\u5236\uff0c\u901a\u8fc7\u5171\u4eab\u4ea4\u4e92\u589e\u5f3a\u8bed\u4e49\u4e92\u52a8\uff0c\u652f\u6301\u56e2\u961f\u534f\u4f5c\u3002", "motivation": "\u63a2\u8ba8\u4f20\u7edf\u534f\u4f5c\u7cfb\u7edf\u5b64\u7acb\u89e3\u51b3\u4e2a\u4eba\u4efb\u52a1\u7684\u5c40\u9650\u6027\uff0c\u81f4\u529b\u4e8e\u901a\u8fc7\u5171\u4eab\u4f53\u9a8c\u63d0\u5347\u56e2\u961f\u5408\u4f5c\u7684\u6709\u6548\u6027\u3002", "method": "\u91c7\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\uff0c\u5206\u6790\u56e2\u961f\u4e92\u52a8\u4e2d\u7684\u5171\u4eab\u4fe1\u606f\u7ed3\u6784\uff0c\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86CollEagle\u4ea4\u4e92\u684c\u9762\u7cfb\u7edf\u3002", "result": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u673a\u5236\uff0c\u65e8\u5728\u901a\u8fc7\u5171\u4eab\u4ea4\u4e92\u673a\u5236\u4fc3\u8fdb\u8bed\u4e49\u4ea4\u4e92\u3002", "conclusion": "\u6211\u4eec\u7684\u521d\u6b65\u7814\u7a76\u8868\u660e\uff0c\u8bed\u4e49\u4ea4\u4e92\u80fd\u591f\u6709\u6548\u8c03\u8282\u7fa4\u4f53\u4e92\u52a8\uff0c\u6709\u52a9\u4e8e\u8bbe\u8ba1\u65b0\u578b\u534f\u4f5c\u754c\u9762\u3002"}}
{"id": "2511.05026", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05026", "abs": "https://arxiv.org/abs/2511.05026", "authors": ["Xingyuan Zhou", "Peter Paik", "S. Farokh Atashzar"], "title": "Tunable Passivity Control for Centralized Multiport Networked Systems", "comment": null, "summary": "Centralized Multiport Networked Dynamic (CMND) systems have emerged as a key\narchitecture with applications in several complex network systems, such as\nmultilateral telerobotics and multi-agent control. These systems consist of a\nhub node/subsystem connecting with multiple remote nodes/subsystems via a\nnetworked architecture. One challenge for this system is stability, which can\nbe affected by non-ideal network artifacts. Conventional passivity-based\napproaches can stabilize the system under specialized applications like\nsmall-scale networked systems. However, those conventional passive stabilizers\nhave several restrictions, such as distributing compensation across subsystems\nin a decentralized manner, limiting flexibility, and, at the same time, relying\non the restrictive assumptions of node passivity. This paper synthesizes a\ncentralized optimal passivity-based stabilization framework for CMND systems.\nIt consists of a centralized passivity observer monitoring overall energy flow\nand an optimal passivity controller that distributes the just-needed\ndissipation among various nodes, guaranteeing strict passivity and, thus, L2\nstability. The proposed data-driven model-free approach, i.e., Tunable\nCentralized Optimal Passivity Control (TCoPC), optimizes total performance\nbased on the prescribed dissipation distribution strategy while ensuring\nstability. The controller can put high dissipation loads on some sub-networks\nwhile relaxing the dissipation on other nodes. Simulation results demonstrate\nthe proposed frameworks performance in a complex task under different\ntime-varying delay scenarios while relaxing the remote nodes minimum phase and\npassivity assumption, enhancing the scalability and generalizability.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u4e2d\u5f0f\u6700\u4f73\u88ab\u52a8\u63a7\u5236\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3CMND\u7cfb\u7edf\u4e2d\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4f18\u5316\u6027\u80fd\u5e76\u589e\u5f3a\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u89e3\u51b3CMND\u7cfb\u7edf\u4e2d\u7531\u4e8e\u975e\u7406\u60f3\u7f51\u7edc\u73af\u5883\u5bfc\u81f4\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u514b\u670d\u4f20\u7edf\u88ab\u52a8\u7a33\u5b9a\u5668\u7684\u5c40\u9650\u6027\u3002", "method": "Tunable Centralized Optimal Passivity Control (TCoPC)", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u96c6\u4e2d\u5f0f\u6700\u4f73\u88ab\u52a8\u63a7\u5236\u6846\u67b6\uff0c\u53ef\u4ee5\u6839\u636e\u9700\u6c42\u5728\u5404\u4e2a\u8282\u70b9\u95f4\u5206\u914d\u5fc5\u8981\u7684\u8017\u6563\uff0c\u786e\u4fddL2\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4e0d\u540c\u65f6\u95f4\u5ef6\u8fdf\u60c5\u51b5\u4e0b\u7684\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u79c0\uff0c\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2511.05400", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.05400", "abs": "https://arxiv.org/abs/2511.05400", "authors": ["Ma Xiaofan", "Yan Lirong", "Zhao Weijia", "Zeng Weiping", "Wu Huiyue"], "title": "Designing Hierarchical Exploratory Experiences for Ethnic Costumes: A Cultural Gene-Based Perspective", "comment": null, "summary": "Ethnic clothing is a vital carrier of cultural identity, yet its digital\npreservation often results in static displays that fail to convey deep cultural\nmeaning or foster user engagement. Existing practices lack a systematic design\nframework for translating the hierarchical cultural connotations of these\ngarments into dynamic, personalized, and identity-promoting digital\nexperiences. To address this gap, this paper proposes a Three-Layer Cultural\nGene Framework that systematically decodes ethnic costumes from their\nsurface-level visual symbols, through their mid-level socio-cultural contexts,\nto their inner-layer spiritual core. Based on this framework, we designed and\nimplemented an interactive digital platform featuring two key innovations: a\n\"gene-first\" exploratory path that encourages curiosity-driven discovery, and\nan AI-powered co-creation experience. This generative feature allows users to\nco-create personalized narratives and images based on their understanding of\nthe \"inner-layer\" genes, transforming them from passive observers into active\nco-creators. A mixed-methods user study (N=24) was conducted to evaluate the\nplatform. The findings demonstrate that our approach effectively enhances\nusers' cultural cognition, deepens their affective connection, and\nsignificantly promotes their sense of cultural identity. This research\ncontributes a validated framework and a practical exemplar for designing\ngenerative, identity-building digital experiences for cultural heritage,\noffering a new pathway for its preservation and revitalization in the digital\nage.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e09\u5c42\u6587\u5316\u57fa\u56e0\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4e92\u52a8\u6570\u5b57\u5e73\u53f0\uff0c\u589e\u5f3a\u7528\u6237\u7684\u6587\u5316\u8ba4\u77e5\u548c\u8eab\u4efd\u611f\uff0c\u63a8\u52a8\u6587\u5316\u9057\u4ea7\u7684\u6570\u5b57\u4fdd\u62a4\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u5b57\u5316\u4fdd\u62a4\u5b9e\u8df5\u7f3a\u4e4f\u4e00\u4e2a\u7cfb\u7edf\u7684\u8bbe\u8ba1\u6846\u67b6\uff0c\u4ee5\u52a8\u6001\u548c\u4e2a\u6027\u5316\u7684\u65b9\u5f0f\u4f20\u8fbe\u6c11\u65cf\u670d\u9970\u7684\u6df1\u5c42\u6587\u5316\u610f\u4e49\uff0c\u63d0\u5347\u7528\u6237\u53c2\u4e0e\u5ea6\u3002", "method": "\u8fdb\u884c\u4e86\u4e00\u9879\u6df7\u5408\u65b9\u6cd5\u7684\u7528\u6237\u7814\u7a76\uff08N=24\uff09\u6765\u8bc4\u4f30\u6211\u4eec\u8bbe\u8ba1\u7684\u5e73\u53f0\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u5e73\u53f0\u6709\u6548\u589e\u5f3a\u4e86\u7528\u6237\u7684\u6587\u5316\u8ba4\u77e5\uff0c\u6df1\u5316\u4e86\u60c5\u611f\u8fde\u63a5\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u6587\u5316\u8eab\u4efd\u611f\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u6846\u67b6\u548c\u5b9e\u9645\u793a\u4f8b\uff0c\u7528\u4e8e\u8bbe\u8ba1\u751f\u6210\u6027\u3001\u5851\u9020\u8eab\u4efd\u7684\u6587\u5316\u9057\u4ea7\u6570\u5b57\u4f53\u9a8c\uff0c\u4e3a\u6570\u5b57\u65f6\u4ee3\u7684\u4fdd\u62a4\u4e0e\u590d\u5174\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84\u3002"}}
{"id": "2511.05033", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05033", "abs": "https://arxiv.org/abs/2511.05033", "authors": ["Jennifer K. Leestma", "Siddharth R. Nathella", "Christoph P. O. Nuesslein", "Snehil Mathur", "Gregory S. Sawicki", "Aaron J. Young"], "title": "Epically Powerful: An open-source software and mechatronics infrastructure for wearable robotic systems", "comment": "11 pages, 5 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "Epically Powerful is an open-source robotics infrastructure that streamlines\nthe underlying framework of wearable robotic systems - managing communication\nprotocols, clocking, actuator commands, visualization, sensor data acquisition,\ndata logging, and more - while also providing comprehensive guides for hardware\nselection, system assembly, and controller implementation. Epically Powerful\ncontains a code base enabling simplified user implementation via Python that\nseamlessly interfaces with various commercial state-of-the-art quasi-direct\ndrive (QDD) actuators, single-board computers, and common sensors, provides\nexample controllers, and enables real-time visualization. To further support\ndevice development, the package also includes a recommended parts list and\ncompatibility guide and detailed documentation on hardware and software\nimplementation. The goal of Epically Powerful is to lower the barrier to\ndeveloping and deploying custom wearable robotic systems without a\npre-specified form factor, enabling researchers to go from raw hardware to\nmodular, robust devices quickly and effectively. Though originally designed\nwith wearable robotics in mind, Epically Powerful is broadly applicable to\nother robotic domains that utilize QDD actuators, single-board computers, and\nsensors for closed-loop control.", "AI": {"tldr": "Epically Powerful\u662f\u4e00\u4e2a\u65e8\u5728\u7b80\u5316\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u7cfb\u7edf\u5f00\u53d1\u7684\u5f00\u6e90\u6846\u67b6\uff0c\u652f\u6301\u591a\u79cd\u786c\u4ef6\u548c\u8f6f\u4ef6\u529f\u80fd\uff0c\u5e76\u63d0\u4f9b\u8be6\u7ec6\u6587\u6863\u548c\u5efa\u8bae\u914d\u4ef6\u5217\u8868\u3002", "motivation": "\u65e8\u5728\u5feb\u901f\u6709\u6548\u5730\u5c06\u539f\u59cb\u786c\u4ef6\u8f6c\u53d8\u4e3a\u6a21\u5757\u5316\u4e14\u5f3a\u5927\u7684\u8bbe\u5907\uff0c\u652f\u6301\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u548c\u90e8\u7f72\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7Python\u7f16\u5199\u4ee3\u7801\uff0c\u652f\u6301\u4e0eQDD\u6267\u884c\u5668\u3001\u5355\u677f\u8ba1\u7b97\u673a\u548c\u5e38\u89c1\u4f20\u611f\u5668\u65e0\u7f1d\u63a5\u53e3\uff0c\u5b9e\u73b0\u5b9e\u65f6\u53ef\u89c6\u5316\u548c\u793a\u4f8b\u63a7\u5236\u5668\u3002", "result": "Epically Powerful\u662f\u4e00\u4e2a\u5f00\u6e90\u673a\u5668\u4eba\u57fa\u7840\u8bbe\u65bd\uff0c\u65e8\u5728\u7cbe\u7b80\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5e95\u5c42\u6846\u67b6\uff0c\u901a\u8fc7\u7ba1\u7406\u901a\u4fe1\u534f\u8bae\u3001\u65f6\u949f\u3001\u6267\u884c\u5668\u547d\u4ee4\u3001\u53ef\u89c6\u5316\u3001\u4f20\u611f\u5668\u6570\u636e\u91c7\u96c6\u3001\u6570\u636e\u8bb0\u5f55\u7b49\u529f\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u786c\u4ef6\u9009\u62e9\u3001\u7cfb\u7edf\u7ec4\u88c5\u548c\u63a7\u5236\u5668\u5b9e\u73b0\u7684\u5168\u9762\u6307\u5357\u3002", "conclusion": "Epically Powerful\u964d\u4f4e\u4e86\u5f00\u53d1\u548c\u90e8\u7f72\u5b9a\u5236\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u95e8\u69db\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u673a\u5668\u4eba\u9886\u57df\u3002"}}
{"id": "2511.05410", "categories": ["cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.05410", "abs": "https://arxiv.org/abs/2511.05410", "authors": ["Justin D. Weisz", "Michael Muller", "Kush R. Varshney"], "title": "Story Arena: A Multi-Agent Environment for Envisioning the Future of Software Engineering", "comment": "8 pages. Appeared in the 2025 Workshop on The End of Programming (as\n  we know it): Envisioning Radical Re-Conceptualizations of Co-Coding with AI,\n  held in conjunction with the Aarhus 2025 Decennial Conference, August 18-22,\n  2025", "summary": "What better way to understand the impact of AI on software engineering than\nto ask AI itself? We constructed Story Arena, a multi-agent \"writer's room\" in\nwhich multiple AI agents, independently imbued with a position statement on the\nfuture of software engineering, converse with each other to develop a shared\nvision. They then use this shared vision to collaboratively construct a design\nfiction that depicts this vision in narrative form. We present \"The Code of\nTrust,\" a short fiction that investigates themes of human comprehension, trust,\ncontent ownership, augmentation vs. replacement, and uncertain futures in\nhuman-AI co-creation.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6784\u5efa\u591a\u667a\u80fd\u4f53\u73af\u5883Story Arena\uff0c\u8ba9AI\u63a2\u8ba8\u8f6f\u4ef6\u5de5\u7a0b\u7684\u672a\u6765\uff0c\u5e76\u521b\u4f5c\u4e86\u540d\u4e3a\u300a\u4fe1\u4efb\u7684\u6cd5\u5219\u300b\u7684\u77ed\u7bc7\u5c0f\u8bf4\u3002", "motivation": "\u5e0c\u671b\u901a\u8fc7AI\u7684\u89c6\u89d2\u7406\u89e3\u5176\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u8ba8\u4eba\u7c7b\u4e0eAI\u7684\u5171\u540c\u521b\u9020\u8fc7\u7a0b\u4e2d\u7684\u4fe1\u4efb\u4e0e\u6240\u6709\u6743\u7b49\u4e3b\u9898\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u591a\u667a\u80fd\u4f53\u5bf9\u8bdd\u73af\u5883\uff0c\u8ba9AI\u6839\u636e\u5404\u81ea\u7acb\u573a\u8fdb\u884c\u8ba8\u8bba\uff0c\u5e76\u5171\u540c\u521b\u4f5c\u5185\u5bb9\u3002", "result": "\u672c\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aStory Arena\u7684\u591a\u667a\u80fd\u4f53\"\u5199\u4f5c\u5ba4\"\uff0c\u5728\u6b64\u73af\u5883\u4e2d\uff0c\u591a\u4e2a\u4eba\u5de5\u667a\u80fd\u4f53\u4ee5\u72ec\u7acb\u7684\u7acb\u573a\u8fdb\u884c\u5bf9\u8bdd\uff0c\u63a2\u8ba8\u8f6f\u4ef6\u5de5\u7a0b\u7684\u672a\u6765\u3002\u5728\u5f62\u6210\u5171\u540c\u613f\u666f\u540e\uff0c\u5b83\u4eec\u5408\u4f5c\u521b\u4f5c\u4e86\u4e00\u90e8\u53d9\u4e8b\u5f62\u5f0f\u7684\u8bbe\u8ba1\u5c0f\u8bf4\u300a\u4fe1\u4efb\u7684\u6cd5\u5219\u300b\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u4eba\u5de5\u667a\u80fd\u5728\u8f6f\u4ef6\u5de5\u7a0b\u672a\u6765\u63a2\u7d22\u4e2d\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u5408\u4f5c\u521b\u4f5c\u548c\u53d9\u4e8b\u6784\u5efa\u65b9\u9762\u3002"}}
{"id": "2511.05052", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.05052", "abs": "https://arxiv.org/abs/2511.05052", "authors": ["Zihao Li", "Yiming Zhu", "Zhe Zhong", "Qinyuan Ren", "Yijiang Huang"], "title": "TAPOM: Task-Space Topology-Guided Motion Planning for Manipulating Elongated Object in Cluttered Environments", "comment": null, "summary": "Robotic manipulation in complex, constrained spaces is vital for widespread\napplications but challenging, particularly when navigating narrow passages with\nelongated objects. Existing planning methods often fail in these low-clearance\nscenarios due to the sampling difficulties or the local minima. This work\nproposes Topology-Aware Planning for Object Manipulation (TAPOM), which\nexplicitly incorporates task-space topological analysis to enable efficient\nplanning. TAPOM uses a high-level analysis to identify critical pathways and\ngenerate guiding keyframes, which are utilized in a low-level planner to find\nfeasible configuration space trajectories. Experimental validation demonstrates\nsignificantly high success rates and improved efficiency over state-of-the-art\nmethods on low-clearance manipulation tasks. This approach offers broad\nimplications for enhancing manipulation capabilities of robots in complex\nreal-world environments.", "AI": {"tldr": "TAPOM\u901a\u8fc7\u4efb\u52a1\u7a7a\u95f4\u7684\u62d3\u6251\u5206\u6790\u6765\u63d0\u9ad8\u4f4e\u6e05\u6670\u5ea6\u73af\u5883\u4e0b\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u6548\u7387\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u6210\u529f\u7387\u548c\u6548\u7387\u663e\u8457\u9ad8\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u590d\u6742\u53d7\u9650\u7a7a\u95f4\u4e2d\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u5bf9\u4e8e\u5e7f\u6cdb\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u72ed\u7a84\u901a\u9053\u4e2d\u64cd\u4f5c\u62c9\u957f\u7269\u4f53\u65f6\uff0c\u73b0\u6709\u89c4\u5212\u65b9\u6cd5\u5e38\u5e38\u5931\u6548\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u89c4\u5212\u6846\u67b6\u3002", "method": "TAPOM\u7ed3\u5408\u9ad8\u5c42\u6b21\u62d3\u6251\u5206\u6790\u8bc6\u522b\u5173\u952e\u901a\u9053\uff0c\u751f\u6210\u5f15\u5bfc\u5173\u952e\u5e27\u5e76\u5728\u4f4e\u5c42\u6b21\u89c4\u5212\u4e2d\u5bfb\u627e\u53ef\u884c\u7684\u914d\u7f6e\u7a7a\u95f4\u8f68\u8ff9\u3002", "result": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u62d3\u6251\u611f\u77e5\u89c4\u5212\u7684\u5bf9\u8c61\u64cd\u4f5c\u65b9\u6cd5\uff08TAPOM\uff09\uff0c\u65e8\u5728\u89e3\u51b3\u5728\u590d\u6742\u53d7\u9650\u7a7a\u95f4\u4e2d\u8fdb\u884c\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u7a84\u901a\u9053\u5185\u4f7f\u7528\u62c9\u957f\u7269\u4f53\u65f6\u3002", "conclusion": "TAPOM\u65b9\u6cd5\u6709\u6548\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u53d7\u9650\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u80fd\u529b\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2511.05129", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05129", "abs": "https://arxiv.org/abs/2511.05129", "authors": ["Bin Fan", "Jianjian Jiang", "Zhuohao Li", "Yixiang He", "Xiaoming Wu", "Yihan Yang", "Shengbang Liu", "Weishi Zheng"], "title": "Decomposed Object Manipulation via Dual-Actor Policy", "comment": "9 pages, 7 figures, 5 tables", "summary": "Object manipulation, which focuses on learning to perform tasks on similar\nparts across different types of objects, can be divided into an approaching\nstage and a manipulation stage. However, previous works often ignore this\ncharacteristic of the task and rely on a single policy to directly learn the\nwhole process of object manipulation. To address this problem, we propose a\nnovel Dual-Actor Policy, termed DAP, which explicitly considers different\nstages and leverages heterogeneous visual priors to enhance each stage.\nSpecifically, we introduce an affordance-based actor to locate the functional\npart in the manipulation task, thereby improving the approaching process.\nFollowing this, we propose a motion flow-based actor to capture the movement of\nthe component, facilitating the manipulation process. Finally, we introduce a\ndecision maker to determine the current stage of DAP and select the\ncorresponding actor. Moreover, existing object manipulation datasets contain\nfew objects and lack the visual priors needed to support training. To address\nthis, we construct a simulated dataset, the Dual-Prior Object Manipulation\nDataset, which combines the two visual priors and includes seven tasks,\nincluding two challenging long-term, multi-stage tasks. Experimental results on\nour dataset, the RoboTwin benchmark and real-world scenarios illustrate that\nour method consistently outperforms the SOTA method by 5.55%, 14.7% and 10.4%\non average respectively.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u6f14\u5458\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7269\u4f53\u64cd\u4f5c\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u4e00\u4e9b\u5c40\u9650\u3002", "motivation": "\u7269\u4f53\u64cd\u4f5c\u4efb\u52a1\u88ab\u5212\u5206\u4e3a\u63a5\u8fd1\u9636\u6bb5\u548c\u64cd\u4f5c\u9636\u6bb5\uff0c\u4f20\u7edf\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u8fd9\u4e00\u533a\u5206\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u6539\u8fdb\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u6f14\u5458\u7b56\u7565\uff0c\u5206\u522b\u5904\u7406\u7269\u4f53\u63a5\u8fd1\u548c\u64cd\u4f5c\u7684\u4e0d\u540c\u9636\u6bb5\uff0c\u91c7\u7528\u57fa\u4e8e\u529f\u80fd\u7684\u6f14\u5458\u548c\u57fa\u4e8e\u8fd0\u52a8\u6d41\u7684\u6f14\u5458\uff0c\u5e76\u5f15\u5165\u51b3\u7b56\u8005\u786e\u5b9a\u5f53\u524d\u9636\u6bb5\u3002", "result": "\u5728\u6a21\u62df\u6570\u636e\u96c6\u3001RoboTwin\u57fa\u51c6\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5e73\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u65b9\u6cd55.55%\u300114.7%\u548c10.4%\u3002", "conclusion": "\u63d0\u51fa\u7684\u53cc\u6f14\u5458\u7b56\u7565\u5728\u7269\u4f53\u64cd\u4f5c\u4e2d\u7684\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u793a\u4e86\u5176\u5728\u5206\u9636\u6bb5\u5b66\u4e60\u548c\u5229\u7528\u4e0d\u540c\u89c6\u89c9\u5148\u9a8c\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.05158", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05158", "abs": "https://arxiv.org/abs/2511.05158", "authors": ["Sahar Salimpour", "Iacopo Catalano", "Tomi Westerlund", "Mohsen Falahi", "Jorge Pe\u00f1a Queralta"], "title": "Follow-Me in Micro-Mobility with End-to-End Imitation Learning", "comment": null, "summary": "Autonomous micro-mobility platforms face challenges from the perspective of\nthe typical deployment environment: large indoor spaces or urban areas that are\npotentially crowded and highly dynamic. While social navigation algorithms have\nprogressed significantly, optimizing user comfort and overall user experience\nover other typical metrics in robotics (e.g., time or distance traveled) is\nunderstudied. Specifically, these metrics are critical in commercial\napplications. In this paper, we show how imitation learning delivers smoother\nand overall better controllers, versus previously used manually-tuned\ncontrollers. We demonstrate how DAAV's autonomous wheelchair achieves\nstate-of-the-art comfort in follow-me mode, in which it follows a human\noperator assisting persons with reduced mobility (PRM). This paper analyzes\ndifferent neural network architectures for end-to-end control and demonstrates\ntheir usability in real-world production-level deployments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u81ea\u4e3b\u8f6e\u6905\u5728\u8ddf\u968f\u6a21\u5f0f\u4e0b\u7684\u8212\u9002\u5ea6\uff0c\u63d0\u51fa\u6a21\u4eff\u5b66\u4e60\u4f5c\u4e3a\u6539\u5584\u5fae\u79fb\u52a8\u5e73\u53f0\u63a7\u5236\u5668\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u5927\u578b\u5ba4\u5185\u7a7a\u95f4\u6216\u57ce\u5e02\u73af\u5883\u4e2d\uff0c\u4f18\u5316\u81ea\u4e3b\u5fae\u79fb\u52a8\u5e73\u53f0\u7684\u7528\u6237\u8212\u9002\u5ea6\u548c\u6574\u4f53\u7528\u6237\u4f53\u9a8c\uff0c\u5c24\u5176\u5728\u5546\u4e1a\u5e94\u7528\u4e2d\u7684\u91cd\u8981\u6027\u3002", "method": "\u4f7f\u7528\u6a21\u4eff\u5b66\u4e60\u6280\u672f\uff0c\u5bf9\u4e0d\u540c\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u8fdb\u884c\u5206\u6790\uff0c\u4ee5\u5b9e\u73b0\u7aef\u5230\u7aef\u63a7\u5236\u3002", "result": "\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\uff0c\u5f00\u53d1\u51fa\u6bd4\u624b\u52a8\u8c03\u4f18\u7684\u63a7\u5236\u5668\u66f4\u52a0\u5e73\u7a33\u548c\u66f4\u597d\u7684\u63a7\u5236\u65b9\u6848\u3002", "conclusion": "\u4e0d\u540c\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u88ab\u5206\u6790\u5e76\u8bc1\u660e\u5728\u5b9e\u9645\u751f\u4ea7\u7ea7\u90e8\u7f72\u4e2d\u7684\u53ef\u7528\u6027\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u81ea\u4e3b\u8f6e\u6905\u7684\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2511.05185", "categories": ["cs.RO", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.05185", "abs": "https://arxiv.org/abs/2511.05185", "authors": ["Adri\u00e1n Campazas-Vega", "Claudia \u00c1lvarez-Aparicio", "David Sobr\u00edn-Hidalgo", "Laura Inyesto-Alonso", "Francisco Javier Rodr\u00edguez-Lera", "Vicente Matell\u00e1n-Olivera", "\u00c1ngel Manuel Guerrero-Higueras"], "title": "Procedimiento de auditor\u00eda de ciberseguridad para sistemas aut\u00f3nomos: metodolog\u00eda, amenazas y mitigaciones", "comment": "32 pages, in Spanish language, 7 tables, 12 Figures. White paper\n  under the TESCAC project", "summary": "The deployment of autonomous systems has experienced remarkable growth in\nrecent years, driven by their integration into sectors such as industry,\nmedicine, logistics, and domestic environments. This expansion is accompanied\nby a series of security issues that entail significant risks due to the\ncritical nature of autonomous systems, especially those operating in\nhuman-interaction environments. Furthermore, technological advancement and the\nhigh operational and architectural complexity of autonomous systems have\nresulted in an increased attack surface. This article presents a specific\nsecurity auditing procedure for autonomous systems, based on a layer-structured\nmethodology, a threat taxonomy adapted to the robotic context, and a set of\nconcrete mitigation measures. The validity of the proposed approach is\ndemonstrated through four practical case studies applied to representative\nrobotic platforms: the Vision 60 military quadruped from Ghost Robotics, the A1\nrobot from Unitree Robotics, the UR3 collaborative arm from Universal Robots,\nand the Pepper social robot from Aldebaran Robotics.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b89\u5168\u5ba1\u8ba1\u7a0b\u5e8f\uff0c\u65e8\u5728\u89e3\u51b3\u81ea\u4e3b\u7cfb\u7edf\u9762\u4e34\u7684\u5b89\u5168\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6848\u4f8b\u4e2d\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u968f\u7740\u81ea\u4e3b\u7cfb\u7edf\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5176\u5728\u5404\u79cd\u9886\u57df\u7684\u5e94\u7528\u4ee5\u53ca\u7531\u6b64\u5f15\u53d1\u7684\u5b89\u5168\u95ee\u9898\u4fc3\u4f7f\u7814\u7a76\u9ad8\u6548\u7684\u5b89\u5168\u5ba1\u8ba1\u7a0b\u5e8f\u3002", "method": "\u4f7f\u7528\u5206\u5c42\u7ed3\u6784\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u9002\u5e94\u4e8e\u673a\u5668\u4eba\u4e0a\u4e0b\u6587\u7684\u5a01\u80c1\u5206\u7c7b\u6cd5\uff0c\u5e76\u63d0\u4f9b\u5177\u4f53\u7684\u7f13\u89e3\u63aa\u65bd\u3002", "result": "\u901a\u8fc7\u5bf9\u56db\u4e2a\u4ee3\u8868\u6027\u673a\u5668\u4eba\u5e73\u53f0\u7684\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u7ed3\u6784\u7684\u65b9\u6cd5\u7684\u5b89\u5168\u5ba1\u8ba1\u7a0b\u5e8f\uff0c\u4ee5\u5e94\u5bf9\u81ea\u4e3b\u7cfb\u7edf\u7684\u5b89\u5168\u95ee\u9898\u3002"}}
{"id": "2511.05199", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05199", "abs": "https://arxiv.org/abs/2511.05199", "authors": ["Yichen Zhu", "Feifei Feng"], "title": "Let Me Show You: Learning by Retrieving from Egocentric Video for Robotic Manipulation", "comment": "Accepted by IROS 2025", "summary": "Robots operating in complex and uncertain environments face considerable\nchallenges. Advanced robotic systems often rely on extensive datasets to learn\nmanipulation tasks. In contrast, when humans are faced with unfamiliar tasks,\nsuch as assembling a chair, a common approach is to learn by watching video\ndemonstrations. In this paper, we propose a novel method for learning robot\npolicies by Retrieving-from-Video (RfV), using analogies from human\ndemonstrations to address manipulation tasks. Our system constructs a video\nbank comprising recordings of humans performing diverse daily tasks. To enrich\nthe knowledge from these videos, we extract mid-level information, such as\nobject affordance masks and hand motion trajectories, which serve as additional\ninputs to enhance the robot model's learning and generalization capabilities.\nWe further feature a dual-component system: a video retriever that taps into an\nexternal video bank to fetch task-relevant video based on task specification,\nand a policy generator that integrates this retrieved knowledge into the\nlearning cycle. This approach enables robots to craft adaptive responses to\nvarious scenarios and generalize to tasks beyond those in the training data.\nThrough rigorous testing in multiple simulated and real-world settings, our\nsystem demonstrates a marked improvement in performance over conventional\nrobotic systems, showcasing a significant breakthrough in the field of\nrobotics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u89c6\u9891\u83b7\u53d6\u5b66\u4e60\u673a\u5668\u4eba\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e2d\u95f4\u4fe1\u606f\u63d0\u5347\u673a\u5668\u4eba\u7684\u5b66\u4e60\u80fd\u529b\uff0c\u4ece\u800c\u5728\u591a\u573a\u666f\u4e0b\u5c55\u793a\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002", "motivation": "\u4eba\u7c7b\u9762\u5bf9\u4e0d\u719f\u6089\u4efb\u52a1\u65f6\u901a\u5e38\u901a\u8fc7\u89c2\u770b\u89c6\u9891\u5b66\u4e60\uff0c\u672c\u6587\u65e8\u5728\u501f\u9274\u8fd9\u4e00\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u9002\u5e94\u6027\u548c\u7075\u6d3b\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u6784\u5efa\u4e86\u4e00\u4e2a\u89c6\u9891\u5e93\uff0c\u63d0\u53d6\u4e86\u4e2d\u95f4\u5c42\u4fe1\u606f\uff0c\u5e76\u4f7f\u7528\u4e00\u4e2a\u53cc\u7ec4\u4ef6\u7cfb\u7edf\uff0c\u5305\u62ec\u89c6\u9891\u68c0\u7d22\u5668\u548c\u7b56\u7565\u751f\u6210\u5668\uff0c\u4ee5\u589e\u5f3a\u5b66\u4e60\u548c\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u673a\u5668\u4eba\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c6\u9891\u5b66\u4e60\u4e0e\u4eba\u7c7b\u7684\u793a\u8303\u8fdb\u884c\u7c7b\u6bd4\uff0c\u6765\u89e3\u51b3\u590d\u6742\u7684\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u4efb\u52a1\u3002", "conclusion": "\u7ecf\u8fc7\u591a\u79cd\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u7684\u4e25\u683c\u6d4b\u8bd5\uff0c\u8be5\u7cfb\u7edf\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u6807\u5fd7\u7740\u673a\u5668\u4eba\u6280\u672f\u9886\u57df\u7684\u91cd\u5927\u7a81\u7834\u3002"}}
{"id": "2511.05203", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05203", "abs": "https://arxiv.org/abs/2511.05203", "authors": ["Linus Nwankwo", "Bj\u00f6rn Ellensohn", "Christian Rauch", "Elmar Rueckert"], "title": "Beyond Master and Apprentice: Grounding Foundation Models for Symbiotic Interactive Learning in a Shared Latent Space", "comment": null, "summary": "Today's autonomous agents can understand free-form natural language\ninstructions and execute long-horizon tasks in a manner akin to human-level\nreasoning. These capabilities are mostly driven by large-scale pre-trained\nfoundation models (FMs). However, the approaches with which these models are\ngrounded for human-robot interaction (HRI) perpetuate a master-apprentice\nmodel, where the apprentice (embodied agent) passively receives and executes\nthe master's (human's) commands without reciprocal learning. This reactive\ninteraction approach does not capture the co-adaptive dynamics inherent in\neveryday multi-turn human-human interactions. To address this, we propose a\nSymbiotic Interactive Learning (SIL) approach that enables both the master and\nthe apprentice to co-adapt through mutual, bidirectional interactions. We\nformalised SIL as a co-adaptation process within a shared latent task space,\nwhere the agent and human maintain joint belief states that evolve based on\ninteraction history. This enables the agent to move beyond reactive execution\nto proactive clarification, adaptive suggestions, and shared plan refinement.\nTo realise these novel behaviours, we leveraged pre-trained FMs for spatial\nperception and reasoning, alongside a lightweight latent encoder that grounds\nthe models' outputs into task-specific representations. Furthermore, to ensure\nstability as the tasks evolve, we augment SIL with a memory architecture that\nprevents the forgetting of learned task-space representations. We validate SIL\non both simulated and real-world embodied tasks, including instruction\nfollowing, information retrieval, query-oriented reasoning, and interactive\ndialogues. Demos and resources are public\nat:~\\href{https://linusnep.github.io/SIL/}{https://linusnep.github.io/SIL/}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5171\u751f\u4e92\u52a8\u5b66\u4e60\uff08SIL\uff09\u65b9\u6cd5\uff0c\u5141\u8bb8\u81ea\u4e3b\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u901a\u8fc7\u53cc\u5411\u4e92\u52a8\u5b9e\u73b0\u5171\u540c\u9002\u5e94\uff0c\u8d85\u8d8a\u4f20\u7edf\u7684\u88ab\u52a8\u6267\u884c\u6a21\u5f0f\u3002", "motivation": "\u5f53\u524d\u7684\u81ea\u4e3b\u4ee3\u7406\u80fd\u591f\u7406\u89e3\u81ea\u7531\u5f62\u5f0f\u7684\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5e76\u6267\u884c\u957f\u65f6\u95f4\u4efb\u52a1\uff0c\u4f46\u4e0e\u4eba\u7c7b\u7684\u4e92\u52a8\u7f3a\u4e4f\u53cc\u5411\u5b66\u4e60\uff0c\u4e3b\u8981\u4fdd\u6301\u88ab\u52a8\u63a5\u53d7\u6307\u4ee4\u7684\u5173\u7cfb\u3002\u8fd9\u79cd\u53cd\u5e94\u5f0f\u4e92\u52a8\u65e0\u6cd5\u4f53\u73b0\u65e5\u5e38\u591a\u8f6e\u4eba\u9645\u4ea4\u6d41\u4e2d\u7684\u5171\u9002\u5e94\u52a8\u6001\u3002", "method": "\u5c06\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\u4e0e\u8f7b\u91cf\u7ea7\u6f5c\u5728\u7f16\u7801\u5668\u7ed3\u5408\uff0c\u4ee5\u7a7a\u95f4\u611f\u77e5\u548c\u63a8\u7406\u4e3a\u57fa\u7840\uff0c\u786e\u4fdd\u5728\u4efb\u52a1\u6f14\u53d8\u8fc7\u7a0b\u4e2d\u7ef4\u62a4\u5b66\u4e60\u7684\u4efb\u52a1\u7a7a\u95f4\u8868\u793a\uff0c\u8fdb\u800c\u5b9e\u73b0\u5171\u4eab\u7684\u6f84\u6e05\u548c\u5efa\u8bae\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5171\u751f\u4e92\u52a8\u5b66\u4e60\uff08SIL\uff09\u7684\u65b9\u6cd5\uff0c\u4f7f\u5f97\u4eba\u7c7b\u548c\u4ee3\u7406\u80fd\u591f\u901a\u8fc7\u53cc\u5411\u4e92\u4ea4\u8fdb\u884c\u5171\u540c\u9002\u5e94\uff0c\u5e76\u5728\u5171\u4eab\u7684\u6f5c\u5728\u4efb\u52a1\u7a7a\u95f4\u5185\u7ef4\u6301\u5171\u540c\u7684\u4fe1\u5ff5\u72b6\u6001\u3002\u800c\u4e14\uff0cSIL\u80fd\u591f\u652f\u6301\u4ee3\u7406\u4ece\u88ab\u52a8\u6267\u884c\u8f6c\u53d8\u4e3a\u4e3b\u52a8\u6f84\u6e05\u3001\u9002\u5e94\u6027\u5efa\u8bae\u548c\u5171\u540c\u8ba1\u5212\u7684\u7ec6\u5316\u3002", "conclusion": "\u901a\u8fc7\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e0a\u9a8c\u8bc1SIL\uff0c\u672c\u7814\u7a76\u5c55\u793a\u4e86\u5176\u5728\u6307\u4ee4\u9075\u5faa\u3001\u4fe1\u606f\u68c0\u7d22\u3001\u67e5\u8be2\u5bfc\u5411\u63a8\u7406\u548c\u4e92\u52a8\u5bf9\u8bdd\u7b49\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u516c\u5f00\u7684\u6f14\u793a\u548c\u8d44\u6e90\u3002"}}
{"id": "2511.05234", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05234", "abs": "https://arxiv.org/abs/2511.05234", "authors": ["Philipp Dahlinger", "Niklas Freymuth", "Tai Hoang", "Tobias W\u00fcrth", "Michael Volpp", "Luise K\u00e4rger", "Gerhard Neumann"], "title": "Context-aware Learned Mesh-based Simulation via Trajectory-Level Meta-Learning", "comment": "35 pages. Submitted to Transactions on Machine Learning Research\n  (TMLR)", "summary": "Simulating object deformations is a critical challenge across many scientific\ndomains, including robotics, manufacturing, and structural mechanics. Learned\nGraph Network Simulators (GNSs) offer a promising alternative to traditional\nmesh-based physics simulators. Their speed and inherent differentiability make\nthem particularly well suited for applications that require fast and accurate\nsimulations, such as robotic manipulation or manufacturing optimization.\nHowever, existing learned simulators typically rely on single-step\nobservations, which limits their ability to exploit temporal context. Without\nthis information, these models fail to infer, e.g., material properties.\nFurther, they rely on auto-regressive rollouts, which quickly accumulate error\nfor long trajectories. We instead frame mesh-based simulation as a\ntrajectory-level meta-learning problem. Using Conditional Neural Processes, our\nmethod enables rapid adaptation to new simulation scenarios from limited\ninitial data while capturing their latent simulation properties. We utilize\nmovement primitives to directly predict fast, stable and accurate simulations\nfrom a single model call. The resulting approach, Movement-primitive\nMeta-MeshGraphNet (M3GN), provides higher simulation accuracy at a fraction of\nthe runtime cost compared to state-of-the-art GNSs across several tasks.", "AI": {"tldr": "M3GN \u901a\u8fc7\u6761\u4ef6\u795e\u7ecf\u8fc7\u7a0b\u548c\u8fd0\u52a8\u539f\u8bed\u6846\u67b6\uff0c\u63d0\u4f9b\u4e86\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u4eff\u771f\u51c6\u786e\u6027\u548c\u964d\u4f4e\u8fd0\u884c\u65f6\u95f4\u6210\u672c\u7684\u521b\u65b0\u4eff\u771f\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u5b66\u4e60\u4eff\u771f\u5668\u4f9d\u8d56\u4e8e\u5355\u6b65\u89c2\u5bdf\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u65f6\u95f4\u4e0a\u4e0b\u6587\u5e76\u5bb9\u6613\u5728\u957f\u671f\u8f68\u8ff9\u4e2d\u79ef\u7d2f\u8bef\u5dee\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u6539\u8fdb\u4eff\u771f\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "M3GN \u65b9\u6cd5\u5229\u7528\u6761\u4ef6\u795e\u7ecf\u8fc7\u7a0b\uff0c\u5c06\u57fa\u4e8e\u7f51\u683c\u7684\u4eff\u771f\u6846\u67b6\u89c6\u4e3a\u8f68\u8ff9\u7ea7\u522b\u7684\u5143\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u8fd0\u52a8\u539f\u8bed\u76f4\u63a5\u9884\u6d4b\u5feb\u901f\u3001\u7a33\u5b9a\u548c\u7cbe\u786e\u7684\u4eff\u771f\u3002", "result": "M3GN \u5728\u8fd0\u884c\u65f6\u95f4\u6210\u672c\u4e0a\u76f8\u8f83\u4e8e\u6700\u5148\u8fdb\u7684 GNS \u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u4eff\u771f\u51c6\u786e\u6027\uff0c\u80fd\u591f\u4ece\u6709\u9650\u7684\u521d\u59cb\u6570\u636e\u5feb\u901f\u9002\u5e94\u65b0\u7684\u4eff\u771f\u573a\u666f\u3002", "conclusion": "M3GN \u65b9\u6cd5\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u4eff\u771f\u51c6\u786e\u6027\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u8fd0\u884c\u65f6\u95f4\u6210\u672c\u3002"}}
{"id": "2511.05275", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05275", "abs": "https://arxiv.org/abs/2511.05275", "authors": ["Hokyun Im", "Euijin Jeong", "Jianlong Fu", "Andrey Kolobov", "Youngwoon Lee"], "title": "TwinVLA: Data-Efficient Bimanual Manipulation with Twin Single-Arm Vision-Language-Action Models", "comment": "Project webpage : https://jellyho.github.io/TwinVLA/", "summary": "Vision-language-action models (VLAs) trained on large-scale robotic datasets\nhave demonstrated strong performance on manipulation tasks, including bimanual\ntasks. However, because most public datasets focus on single-arm\ndemonstrations, adapting VLAs for bimanual tasks typically requires substantial\nadditional bimanual data and fine-tuning. To address this challenge, we\nintroduce TwinVLA, a modular framework that composes two copies of a pretrained\nsingle-arm VLA into a coordinated bimanual VLA. Unlike monolithic\ncross-embodiment models trained on mixtures of single-arm and bimanual data,\nTwinVLA improves both data efficiency and performance by composing pretrained\nsingle-arm policies. Across diverse bimanual tasks in real-world and simulation\nsettings, TwinVLA outperforms a comparably-sized monolithic RDT-1B model\nwithout requiring any bimanual pretraining. Furthermore, it narrows the gap to\nstate-of-the-art model, $\\pi_0$ which rely on extensive proprietary bimanual\ndata and compute cost. These results establish our modular composition approach\nas a data-efficient and scalable path toward high-performance bimanual\nmanipulation, leveraging public single-arm data.", "AI": {"tldr": "TwinVLA\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u7ec4\u5408\u9884\u8bad\u7ec3\u7684\u5355\u81c2\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86\u53cc\u81c2\u64cd\u63a7\u4efb\u52a1\u7684\u6570\u636e\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u53cc\u81c2\u64cd\u63a7\u4efb\u52a1\u4e0a\u9700\u8981\u5927\u91cf\u989d\u5916\u6570\u636e\u548c\u8c03\u6574\u7684\u95ee\u9898", "method": "TwinVLA\u5c06\u4e24\u4e2a\u9884\u8bad\u7ec3\u7684\u5355\u81c2\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7ec4\u5408\u6210\u4e00\u4e2a\u534f\u8c03\u7684\u53cc\u81c2\u6a21\u578b\u3002", "result": "TwinVLA\u5728\u591a\u6837\u7684\u53cc\u81c2\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u6bd4\u8f83\u76f8\u5f53\u89c4\u6a21\u7684\u5355\u4e00\u5927\u578b\u6a21\u578b\uff0c\u800c\u65e0\u9700\u53cc\u81c2\u9884\u8bad\u7ec3", "conclusion": "\u6211\u4eec\u7684\u6a21\u5757\u5316\u7ec4\u5408\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u6761\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6027\u80fd\u7684\u53cc\u81c2\u64cd\u63a7\uff0c\u5145\u5206\u5229\u7528\u516c\u5171\u7684\u5355\u81c2\u6570\u636e\u3002"}}
{"id": "2511.05307", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.05307", "abs": "https://arxiv.org/abs/2511.05307", "authors": ["Akua K. Dickson", "Juan C. Pacheco Garcia", "Andrew P. Sabelhaus"], "title": "Force-Safe Environment Maps and Real-Time Detection for Soft Robot Manipulators", "comment": null, "summary": "Soft robot manipulators have the potential for deployment in delicate\nenvironments to perform complex manipulation tasks. However, existing obstacle\ndetection and avoidance methods do not consider limits on the forces that\nmanipulators may exert upon contact with delicate obstacles. This work\nintroduces a framework that maps force safety criteria from task space (i.e.\npositions along the robot's body) to configuration space (i.e. the robot's\njoint angles) and enables real-time force safety detection. We incorporate\nlimits on allowable environmental contact forces for given task-space\nobstacles, and map them into configuration space (C-space) through the\nmanipulator's forward kinematics. This formulation ensures that configurations\nclassified as safe are provably below the maximum force thresholds, thereby\nallowing us to determine force-safe configurations of the soft robot\nmanipulator in real-time. We validate our approach in simulation and hardware\nexperiments on a two-segment pneumatic soft robot manipulator. Results\ndemonstrate that the proposed method accurately detects force safety during\ninteractions with deformable obstacles, thereby laying the foundation for\nreal-time safe planning of soft manipulators in delicate, cluttered\nenvironments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u68c0\u6d4b\u8f6f\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u4e0e\u8106\u5f31\u969c\u788d\u7269\u4ea4\u4e92\u65f6\u529b\u5b89\u5168\u7684\u65b0\u6846\u67b6\uff0c\u6709\u6548\u907f\u514d\u8fc7\u5927\u63a5\u89e6\u529b\u9020\u6210\u7684\u635f\u5bb3\u3002", "motivation": "\u73b0\u6709\u969c\u788d\u7269\u68c0\u6d4b\u65b9\u6cd5\u672a\u8003\u8651\u673a\u5668\u4eba\u4e0e\u8106\u5f31\u969c\u788d\u7269\u63a5\u89e6\u65f6\u65bd\u52a0\u7684\u529b\u9650\u5236\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u786e\u4fdd\u673a\u5668\u4eba\u5b89\u5168\u64cd\u4f5c\u3002", "method": "\u901a\u8fc7\u5c06\u4efb\u52a1\u7a7a\u95f4\u4e2d\u7684\u529b\u5b89\u5168\u6807\u51c6\u6620\u5c04\u5230\u914d\u7f6e\u7a7a\u95f4\uff0c\u7ed3\u5408\u524d\u5411\u8fd0\u52a8\u5b66\u5b9e\u73b0\u529b\u5b89\u5168\u68c0\u6d4b\u3002", "result": "\u5728\u6a21\u62df\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7ed3\u679c\u8868\u660e\u5176\u80fd\u591f\u51c6\u786e\u68c0\u6d4b\u4e0e\u53ef\u53d8\u5f62\u969c\u788d\u7269\u7684\u529b\u5b89\u5168\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u5b9e\u65f6\u68c0\u6d4b\u8f6f\u673a\u5668\u4eba\u5728\u4e0e\u53ef\u53d8\u5f62\u969c\u788d\u7269\u76f8\u4e92\u4f5c\u7528\u65f6\u7684\u529b\u5b89\u5168\u6027\uff0c\u4e3a\u6b64\u7c7b\u673a\u5668\u4eba\u7684\u5b89\u5168\u89c4\u5212\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2511.05379", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.05379", "abs": "https://arxiv.org/abs/2511.05379", "authors": ["Eric Godden", "Jacquie Groenewegen", "Matthew K. X. J. Pan"], "title": "ETHOS: A Robotic Encountered-Type Haptic Display for Social Interaction in Virtual Reality", "comment": "8 pages", "summary": "We present ETHOS (Encountered-Type Haptics for On-demand Social Interaction),\na dynamic encountered-type haptic display (ETHD) that enables natural physical\ncontact in virtual reality (VR) during social interactions such as handovers,\nfist bumps, and high-fives. The system integrates a torque-controlled robotic\nmanipulator with interchangeable passive props (silicone hand replicas and a\nbaton), marker-based physical-virtual registration via a ChArUco board, and a\nsafety monitor that gates motion based on the user's head and hand pose. We\nintroduce two control strategies: (i) a static mode that presents a stationary\nprop aligned with its virtual counterpart, consistent with prior ETHD\nbaselines, and (ii) a dynamic mode that continuously updates prop position by\nexponentially blending an initial mid-point trajectory with real-time hand\ntracking, generating a unique contact point for each interaction. Bench tests\nshow static colocation accuracy of 5.09 +/- 0.94 mm, while user interactions\nachieved temporal alignment with an average contact latency of 28.53 +/- 31.21\nms across all interaction and control conditions. These results demonstrate the\nfeasibility of recreating socially meaningful haptics in VR. By incorporating\nessential safety and control mechanisms, ETHOS establishes a practical\nfoundation for high-fidelity, dynamic interpersonal interactions in virtual\nenvironments.", "AI": {"tldr": "ETHOS\u7cfb\u7edf\u901a\u8fc7\u81ea\u7136\u7269\u7406\u63a5\u89e6\u63d0\u5347\u4e86\u865a\u62df\u73b0\u5b9e\u4e2d\u7684\u793e\u4ea4\u4e92\u52a8\uff0c\u5c55\u793a\u4e86\u53ef\u884c\u7684\u9ad8\u4fdd\u771f\u89e6\u89c9\u518d\u73b0\u6280\u672f\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u589e\u5f3a\u865a\u62df\u73b0\u5b9e\u4e2d\u7684\u793e\u4ea4\u4e92\u52a8\u4f53\u9a8c\uff0c\u5c24\u5176\u662f\u901a\u8fc7\u81ea\u7136\u7684\u7269\u7406\u63a5\u89e6\u6765\u63d0\u5347\u4e92\u52a8\u7684\u771f\u5b9e\u611f\u3002", "method": "ETHOS\u7cfb\u7edf\u96c6\u6210\u4e86\u4e00\u4e2a\u626d\u77e9\u63a7\u5236\u7684\u673a\u5668\u4eba\u64cd\u63a7\u5668\u4e0e\u53ef\u66f4\u6362\u7684\u88ab\u52a8\u9053\u5177\uff0c\u540c\u65f6\u4f7f\u7528\u57fa\u4e8e\u6807\u8bb0\u7684\u7269\u7406-\u865a\u62df\u6ce8\u518c\u548c\u5b89\u5168\u76d1\u63a7\u673a\u5236\u3002", "result": "\u6d4b\u8bd5\u663e\u793a\uff0c\u9759\u6001\u5bf9\u4f4d\u7cbe\u5ea6\u4e3a5.09\u00b10.94\u6beb\u7c73\uff0c\u7528\u6237\u4ea4\u4e92\u7684\u5e73\u5747\u63a5\u89e6\u5ef6\u8fdf\u4e3a28.53\u00b131.21\u6beb\u79d2\uff0c\u8868\u660e\u53ef\u4ee5\u6709\u6548\u5730\u5728VR\u4e2d\u91cd\u73b0\u793e\u4ea4\u89e6\u89c9\u3002", "conclusion": "ETHOS\u4e3a\u9ad8\u4fdd\u771f\u52a8\u6001\u4eba\u9645\u4ea4\u4e92\u5728\u865a\u62df\u73af\u5883\u4e2d\u5960\u5b9a\u4e86\u5b9e\u7528\u57fa\u7840\uff0c\u4f7f\u5f97\u793e\u4ea4\u89e6\u89c9\u5728VR\u4e2d\u5f97\u4ee5\u91cd\u73b0\u3002"}}
{"id": "2511.05397", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05397", "abs": "https://arxiv.org/abs/2511.05397", "authors": ["Samarth Chopra", "Alex McMoil", "Ben Carnovale", "Evan Sokolson", "Rajkumar Kubendran", "Samuel Dickerson"], "title": "EveryDayVLA: A Vision-Language-Action Model for Affordable Robotic Manipulation", "comment": "Submitted to ICRA 2026", "summary": "While Vision-Language-Action (VLA) models map visual inputs and language\ninstructions directly to robot actions, they often rely on costly hardware and\nstruggle in novel or cluttered scenes. We introduce EverydayVLA, a 6-DOF\nmanipulator that can be assembled for under $300, capable of modest payloads\nand workspace. A single unified model jointly outputs discrete and continuous\nactions, and our adaptive-horizon ensemble monitors motion uncertainty to\ntrigger on-the-fly re-planning for safe, reliable operation. On LIBERO,\nEverydayVLA matches state-of-the-art success rates, and in real-world tests it\noutperforms prior methods by 49% in-distribution and 34.9% out-of-distribution.\nBy combining a state-of-the-art VLA with cost-effective hardware, EverydayVLA\ndemocratizes access to a robotic foundation model and paves the way for\neconomical use in homes and research labs alike. Experiment videos and details:\nhttps://everydayvla.github.io/", "AI": {"tldr": "EverydayVLA\u662f\u4e00\u6b3e\u4f4e\u6210\u672c\u7684\u673a\u5668\u4eba\u64cd\u7eb5\u5668\uff0c\u7ed3\u5408\u5148\u8fdb\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u7684\u52a8\u529b\u5728\u4e8e\u964d\u4f4e\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6210\u672c\u548c\u590d\u6742\u6027\uff0c\u4f7f\u5f97\u66f4\u591a\u7528\u6237\u80fd\u591f\u63a5\u89e6\u548c\u4f7f\u7528\u673a\u5668\u4eba\u6280\u672f\uff0c\u7279\u522b\u662f\u5728\u5bb6\u5ead\u548c\u79d1\u7814\u73af\u5883\u4e2d\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u8054\u5408\u8f93\u51fa\u79bb\u6563\u548c\u8fde\u7eed\u52a8\u4f5c\u7684\u7edf\u4e00\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u65f6\u95f4\u6c34\u5e73\u96c6\u6210\u76d1\u63a7\u8fd0\u52a8\u4e0d\u786e\u5b9a\u6027\uff0c\u4ece\u800c\u5b9e\u73b0\u5b89\u5168\u53ef\u9760\u7684\u64cd\u4f5c\u548c\u5b9e\u65f6\u91cd\u65b0\u89c4\u5212\u3002", "result": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86EverydayVLA\uff0c\u8fd9\u662f\u4e00\u4e2a\u6210\u672c\u4f4e\u4e8e300\u7f8e\u5143\u76846-DOF\u64cd\u7eb5\u5668\uff0c\u80fd\u591f\u5c06\u89c6\u89c9\u8f93\u5165\u548c\u8bed\u8a00\u6307\u4ee4\u6620\u5c04\u5230\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u65b0\u573a\u666f\u548c\u6742\u4e71\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\u3002", "conclusion": "EverydayVLA\u901a\u8fc7\u5c06\u9ad8\u6548\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u4e0e\u7ecf\u6d4e\u5b9e\u60e0\u7684\u786c\u4ef6\u76f8\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u5728\u5bb6\u5ead\u548c\u7814\u7a76\u5b9e\u9a8c\u5ba4\u7684\u53ef\u53ca\u6027\u3002"}}
{"id": "2511.05402", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05402", "abs": "https://arxiv.org/abs/2511.05402", "authors": ["Muhammad Saud Ul Hassan", "Derek Vasquez", "Hamza Asif", "Christian Hubicki"], "title": "Stable and Robust SLIP Model Control via Energy Conservation-Based Feedback Cancellation for Quadrupedal Applications", "comment": null, "summary": "In this paper, we present an energy-conservation based control architecture\nfor stable dynamic motion in quadruped robots. We model the robot as a\nSpring-loaded Inverted Pendulum (SLIP), a model well-suited to represent the\nbouncing motion characteristic of running gaits observed in various biological\nquadrupeds and bio-inspired robotic systems. The model permits leg-orientation\ncontrol during flight and leg-length control during stance, a design choice\ninspired by natural quadruped behaviors and prevalent in robotic quadruped\nsystems. Our control algorithm uses the reduced-order SLIP dynamics of the\nquadruped to track a stable parabolic spline during stance, which is calculated\nusing the principle of energy conservation. Through simulations based on the\ndesign specifications of an actual quadruped robot, Ghost Robotics Minitaur, we\ndemonstrate that our control algorithm generates stable bouncing gaits.\nAdditionally, we illustrate the robustness of our controller by showcasing its\nability to maintain stable bouncing even when faced with up to a 10% error in\nsensor measurements.", "AI": {"tldr": "\u672c\u7814\u7a76\u4e3a\u56db\u8db3\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u80fd\u91cf\u5b88\u6052\u7684\u63a7\u5236\u65b9\u6cd5\uff0c\u6210\u529f\u6a21\u62df\u4e86\u81ea\u7136\u56db\u8db3\u751f\u7269\u7684\u8df3\u8dc3\u8fd0\u52a8\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u57fa\u4e8e\u80fd\u91cf\u5b88\u6052\u7684\u63a7\u5236\u67b6\u6784\uff0c\u4ee5\u5b9e\u73b0\u56db\u8db3\u673a\u5668\u4eba\u7684\u7a33\u5b9a\u52a8\u6001\u8fd0\u52a8\u3002", "method": "\u5c06\u56db\u8db3\u673a\u5668\u4eba\u5efa\u6a21\u4e3a\u5f39\u7c27\u8d1f\u8f7d\u5012\u6446\uff08SLIP\uff09\uff0c\u4f7f\u7528\u7b80\u5316\u7684SLIP\u52a8\u529b\u5b66\uff0c\u901a\u8fc7\u80fd\u91cf\u5b88\u6052\u539f\u7406\u8ddf\u8e2a\u7a33\u5b9a\u7684\u629b\u7269\u7ebf\u6837\u6761\u3002", "result": "\u63d0\u51fa\u7684\u63a7\u5236\u7b97\u6cd5\u80fd\u591f\u751f\u6210\u7a33\u5b9a\u7684\u8df3\u8dc3\u6b65\u6001\uff0c\u4e14\u5728\u4f20\u611f\u5668\u6d4b\u91cf\u8bef\u5dee\u8fbe\u523010%\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u4fdd\u6301\u7a33\u5b9a\u3002", "conclusion": "\u901a\u8fc7\u5bf9Ghost Robotics Minitaur\u7684\u4eff\u771f\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u63a7\u5236\u7b97\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u4e0e\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.05426", "categories": ["cs.RO", "J.2"], "pdf": "https://arxiv.org/pdf/2511.05426", "abs": "https://arxiv.org/abs/2511.05426", "authors": ["Luca Girardi", "Gabriel Maquignaz", "Stefano Mintchev"], "title": "Bioinspired Soft Quadrotors Jointly Unlock Agility, Squeezability, and Collision Resilience", "comment": "26 pages, 12 figures, 2 tables, 9 videos (not yet disclosed, awaiting\n  peer review)", "summary": "Natural flyers use soft wings to seamlessly enable a wide range of flight\nbehaviours, including agile manoeuvres, squeezing through narrow passageways,\nand withstanding collisions. In contrast, conventional quadrotor designs rely\non rigid frames that support agile flight but inherently limit collision\nresilience and squeezability, thereby constraining flight capabilities in\ncluttered environments. Inspired by the anisotropic stiffness and distributed\nmass-energy structures observed in biological organisms, we introduce\nFlexiQuad, a soft-frame quadrotor design approach that limits this trade-off.\nWe demonstrate a 405-gram FlexiQuad prototype, three orders of magnitude more\ncompliant than conventional quadrotors, yet capable of acrobatic manoeuvres\nwith peak speeds above 80 km/h and linear and angular accelerations exceeding 3\ng and 300 rad/s$^2$, respectively. Analysis demonstrates it can replicate\naccelerations of rigid counterparts up to a thrust-to-weight ratio of 8.\nSimultaneously, FlexiQuad exhibits fourfold higher collision resilience,\nsurviving frontal impacts at 5 m/s without damage and reducing destabilising\nforces in glancing collisions by a factor of 39. Its frame can fully compress,\nenabling flight through gaps as narrow as 70% of its nominal width. Our\nanalysis identifies an optimal structural softness range, from 0.006 to 0.77\nN/mm, comparable to that of natural flyers' wings, whereby agility,\nsqueezability, and collision resilience are jointly achieved for FlexiQuad\nmodels from 20 to 3000 grams. FlexiQuad expands hovering drone capabilities in\ncomplex environments, enabling robust physical interactions without\ncompromising flight performance.", "AI": {"tldr": "FlexiQuad\u662f\u4e00\u79cd\u65b0\u578b\u8f6f\u6846\u67b6\u56db\u65cb\u7ffc\uff0c\u5177\u5907\u9ad8\u7075\u6d3b\u6027\u3001\u78b0\u649e\u97e7\u6027\u4e0e\u53ef\u901a\u8fc7\u7a84\u9699\u98de\u884c\u7684\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u56db\u65cb\u7ffc\u5728\u78b0\u649e\u97e7\u6027\u548c\u901a\u8fc7\u72ed\u7a84\u7a7a\u95f4\u98de\u884c\u80fd\u529b\u4e0a\u53d7\u5230\u9650\u5236\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u751f\u7269\u542f\u53d1\u7684\u8bbe\u8ba1\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u751f\u7269\u4f53\u7684\u7ed3\u6784\u7279\u6027\uff0c\u5728\u8bbe\u8ba1\u4e0a\u5b9e\u73b0\u5404\u5411\u5f02\u6027\u521a\u5ea6\u7684\u67d4\u6027\u56db\u65cb\u7ffc\u539f\u578b\uff0c\u6d4b\u8bd5\u5176\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002", "result": "FlexiQuad\u662f\u4e00\u79cd\u8f6f\u6846\u67b6\u56db\u65cb\u7ffc\u8bbe\u8ba1\uff0c\u7ed3\u5408\u4e86\u751f\u7269\u4f53\u7684\u5404\u5411\u5f02\u6027\u521a\u5ea6\u4e0e\u5206\u5e03\u8d28\u91cf\u80fd\u91cf\u7ed3\u6784\uff0c\u63d0\u5347\u4e86\u98de\u884c\u80fd\u529b\u3002\u5176\u539f\u578b\u91cd405\u514b\uff0c\u7b26\u5408\u4f20\u7edf\u56db\u65cb\u7ffc\u6027\u80fd\u4e14\u5177\u66f4\u9ad8\u7684\u67d4\u97e7\u6027\u548c\u78b0\u649e\u6297\u6027\uff0c\u80fd\u5728\u590d\u6742\u73af\u5883\u4e2d\u66f4\u81ea\u5982\u5730\u98de\u884c\u3002", "conclusion": "FlexiQuad\u901a\u8fc7\u5f15\u5165\u8f6f\u6846\u67b6\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u56db\u65cb\u7ffc\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u98de\u884c\u80fd\u529b\u4e0e\u5b89\u5168\u6027\uff0c\u662f\u63d0\u9ad8\u65e0\u4eba\u673a\u8868\u73b0\u7684\u6709\u6548\u9014\u5f84\u3002"}}
