<div id=toc></div>

# Table of Contents

- [cs.HC](#cs.HC) [Total: 16]
- [cs.RO](#cs.RO) [Total: 35]


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [1] [How human is the machine? Evidence from 66,000 Conversations with Large Language Models](https://arxiv.org/abs/2510.07321)
*Antonios Stamatogiannakis,Arsham Ghodsinia,Sepehr Etminanrad,Dilney Gonçalves,David Santos*

Main category: cs.HC

TL;DR: 该研究通过十个实验揭示了大型语言模型在模拟人类消费者行为时的多样偏差，指出它们在偏见表现和响应一致性上与人类存在显著区别。


<details>
  <summary>Details</summary>
Motivation: 探讨人工智能在替代消费者时是否能够准确模拟人类行为，特别是在偏见和启发式决策方面。

Method: 通过十个实验研究大型语言模型(LLMs)在一些已知偏见和启发式领域的行为。

Result: LLMs表现出四种明显与人类行为不同的偏差，既包括减少和放大偏见，也包括表现出与人类相反的偏见，以及响应的一致性问题。

Conclusion: LLMs在模仿或预测消费者行为时表现出与人类不一致的反应，这可能使得它们不适合用作替代消费者的数据源。

Abstract: When Artificial Intelligence (AI) is used to replace consumers (e.g.,
synthetic data), it is often assumed that AI emulates established consumers,
and more generally human behaviors. Ten experiments with Large Language Models
(LLMs) investigate if this is true in the domain of well-documented biases and
heuristics. Across studies we observe four distinct types of deviations from
human-like behavior. First, in some cases, LLMs reduce or correct biases
observed in humans. Second, in other cases, LLMs amplify these same biases.
Third, and perhaps most intriguingly, LLMs sometimes exhibit biases opposite to
those found in humans. Fourth, LLMs' responses to the same (or similar) prompts
tend to be inconsistent (a) within the same model after a time delay, (b)
across models, and (c) among independent research studies. Such inconsistencies
can be uncharacteristic of humans and suggest that, at least at one point,
LLMs' responses differed from humans. Overall, unhuman-like responses are
problematic when LLMs are used to mimic or predict consumer behavior. These
findings complement research on synthetic consumer data by showing that sources
of bias are not necessarily human-centric. They also contribute to the debate
about the tasks for which consumers, and more generally humans, can be replaced
by AI.

</details>


### [2] [A LoRa IoT Framework with Machine Learning for Remote Livestock Monitoring in Smart Agriculture](https://arxiv.org/abs/2510.07322)
*Hitesh Mohapatra*

Main category: cs.HC

TL;DR: AgroTrack是一个基于LoRa的IoT框架，通过穿戴式传感器实现对自由放养牲畜的实时监测和健康评估，集成高级分析以支持农民更好的管理和决策。


<details>
  <summary>Details</summary>
Motivation: 为了提高农村智能农业中的畜牧管理效率及可持续发展，设计了一种低功耗、长距离的监测系统。

Method: 使用LoRa进行远程牲畜监测，集成GPS、运动和温度传感器，通过云平台进行数据可视化和分析。

Result: AgroTrack通过机器学习模型实现了预测健康警报和行为异常检测，提升了监测框架的智能化。

Conclusion: AgroTrack是一个智能决策支持系统，通过物联网技术提升了农村环境下的畜牧管理和可持续性。

Abstract: This work presents AgroTrack, a LoRa-based IoT framework for remote livestock
monitoring in smart agriculture. The system is designed for low-power,
long-range communication and supports real-time tracking and basic health
assessment of free-range livestock through GPS, motion, and temperature sensors
integrated into wearable collars. Data is collected and transmitted via LoRa to
gateways and forwarded to a cloud platform for visualization, alerts, and
analytics. To enhance its practical deployment, AgroTrack incorporates advanced
analytics, including machine learning models for predictive health alerts and
behavioral anomaly detection. This integration transforms the framework from a
basic monitoring tool into an intelligent decision-support system, enabling
farmers to improve livestock management, operational efficiency, and
sustainability in rural environments.

</details>


### [3] [IGUANA: Immersive Guidance, Navigation, and Control for Consumer UAV](https://arxiv.org/abs/2510.07609)
*Victor Victor,Tania Krisanty,Matthew McGinity,Stefan Gumhold,Uwe Aßmann*

Main category: cs.HC

TL;DR: 本文提出IGUANA，一种基于混合现实的无人机控制系统，结合3D地图、虚拟球控制和空间覆盖，提升用户体验。


<details>
  <summary>Details</summary>
Motivation: 探索无人机和混合现实头戴设备的集成以提高控制系统的直观性和沉浸感。

Method: 用户研究评估IGUANA设计

Result: 发现3D地图界面直观易用，虚拟球界面直观但缺乏物理反馈，空间覆盖增强用户的情境意识。

Conclusion: IGUANA有效提升了用户在控制无人机过程中的准确性和一致性，说明了混合现实技术在无人机控制中的潜力。

Abstract: As the markets for unmanned aerial vehicles (UAVs) and mixed reality (MR)
headsets continue to grow, recent research has increasingly explored their
integration, which enables more intuitive, immersive, and situationally aware
control systems. We present IGUANA, an MR-based immersive guidance, navigation,
and control system for consumer UAVs. IGUANA introduces three key elements
beyond conventional control interfaces: (1) a 3D terrain map interface with
draggable waypoint markers and live camera preview for high-level control, (2)
a novel spatial control metaphor that uses a virtual ball as a physical analogy
for low-level control, and (3) a spatial overlay that helps track the UAV when
it is not visible with the naked eye or visual line of sight is interrupted. We
conducted a user study to evaluate our design, both quantitatively and
qualitatively, and found that (1) the 3D map interface is intuitive and easy to
use, relieving users from manual control and suggesting improved accuracy and
consistency with lower perceived workload relative to conventional dual-stick
controller, (2) the virtual ball interface is intuitive but limited by the lack
of physical feedback, and (3) the spatial overlay is very useful in enhancing
the users' situational awareness.

</details>


### [4] [The Slow Space Editor : Broadening Access to Restorative XR](https://arxiv.org/abs/2510.07610)
*Nate Laffan,Ashley Hom,Andrea Nadine Castillo,Elizabeth Gitelman,Rebecca Zhao,Nikita Shenoy,Kaia Rae Schweig,Katherine Isbister*

Main category: cs.HC

TL;DR: 本文介绍了一个旨在简化虚拟环境创建的 2D 工具，并探讨了其对用户反思和注意力恢复的影响。


<details>
  <summary>Details</summary>
Motivation: 希望通过极简化虚拟环境的创建方式，提升用户的反思与注意力恢复能力。

Method: 研究通过专业设计师访谈和定性研究来评估工具的效果。

Result: Slow Space Editor 是一个用于创建 3D 空间的 2D 工具，旨在简化虚拟环境的创建过程，使更多用户受益。

Conclusion: 通过对工具的定性研究发现，简单的环境控制方法可提高用户体验。

Abstract: The Slow Space Editor is a 2D tool for creating 3D spaces. It was built as
part of a research-through-design project that investigates how Virtual and
Mixed Reality (XR) environments might be used for reflection and attention
restoration. In this phase, we seek to radically simplify the creation of
virtual environments, thereby broadening the potential group of users who could
benefit from them. The research described in this paper has three aspects.
First, we define the concept of "slow space," situating it alongside existing
research in HCI and environmental psychology. Second, we report on a series of
interviews with professional designers about how slow spaces are created in the
physical world. Third, we share the design of the tool itself, focussing on the
benefits of providing a simple method for users to control their environments.
We conclude with our findings from a 19-person qualitative study of the tool.

</details>


### [5] [Human-in-the-Loop Optimization with Model-Informed Priors](https://arxiv.org/abs/2510.07754)
*Yi-Chi Liao,João Belo,Hee-Seung Moon,Jürgen Steimle,Anna Maria Feit*

Main category: cs.HC

TL;DR: 本文提出了HOMI框架和基于神经网络的优化方法NAF+，通过合成数据提高人机迭代优化效率。


<details>
  <summary>Details</summary>
Motivation: 人机迭代优化常常需要多次迭代以寻找最佳界面设计，但由于缺乏先前信息，这一过程效率较低。同时，收集用户数据的成本高昂且不切实际。

Method: 引入基于强化学习训练的神经采集函数NAF+，并在使用合成数据的基础上提升实时优化效率。

Result: 提出了一种新的概念框架HOMI，通过在真实用户部署前进行训练阶段，利用生成的合成用户数据增强人机迭代优化的效果。

Conclusion: HOMI与NAF+表现出在VR输入任务中的高效性，为界面自适应提供了一种新方法。

Abstract: Human-in-the-loop optimization identifies optimal interface designs by
iteratively observing user performance. However, it often requires numerous
iterations due to the lack of prior information. While recent approaches have
accelerated this process by leveraging previous optimization data, collecting
user data remains costly and often impractical. We present a conceptual
framework, Human-in-the-Loop Optimization with Model-Informed Priors (HOMI),
which augments human-in-the-loop optimization with a training phase where the
optimizer learns adaptation strategies from diverse, synthetic user data
generated with predictive models before deployment. To realize HOMI, we
introduce Neural Acquisition Function+ (NAF+), a Bayesian optimization method
featuring a neural acquisition function trained with reinforcement learning.
NAF+ learns optimization strategies from large-scale synthetic data, improving
efficiency in real-time optimization with users. We evaluate HOMI and NAF+ with
mid-air keyboard optimization, a representative VR input task. Our work
presents a new approach for more efficient interface adaptation by bridging in
situ and in silico optimization processes.

</details>


### [6] [The Rise of the Knowledge Sculptor: A New Archetype for Knowledge Work in the Age of Generative AI](https://arxiv.org/abs/2510.07829)
*Cathal Doyle*

Main category: cs.HC

TL;DR: 本文介绍了知识雕刻者这一新职业角色，旨在提升人机协作中的知识转化能力，以应对生成性AI带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着生成性人工智能的发展，传统的知识管理模型已无法满足需求，新的职业角色和能力框架亟需出现，以促进人机协作。

Method: 本文采用社会技术视角，结合能力框架，通过实践案例展示知识雕刻者的实际应用。

Result: 本文探讨了在生成时代中知识工作的转变，并引入了知识雕刻者这一新职业角色，以应对生成性人工智能系统的挑战。知识雕刻者在人与生成AI的协作中，负责将原始AI输出转化为可信赖、可操作的知识。以社会技术视角为基础，本文提出了一系列能力框架，包括愿景架构、迭代对话、信息雕刻和好奇驱动的综合。在实践案例中，展示了知识雕刻者在实际应用中的角色。此外，本文本身也作为其描述的雕刻过程的一个实例。

Conclusion: 知识雕刻者作为一种新职业角色，通过其能力框架，可有效提升人机协作的成果，转化生成AI的输出为可用的知识。

Abstract: In the Generative Age, the nature of knowledge work is transforming.
Traditional models that emphasise the organisation and retrieval of
pre-existing information are increasingly inadequate in the face of generative
AI (GenAI) systems capable of autonomous content creation. This paper
introduces the Knowledge Sculptor (KS), a new professional archetype for
Human-GenAI collaboration that transforms raw AI output into trustworthy,
actionable knowledge. Grounded in a socio-technical perspective, the KS is
conceptualised through a framework of competencies, including architecting a
vision, iterative dialogue, information sculpting, and curiosity-driven
synthesis. A practice-based vignette illustrates the KS role in action, and in
a self-referential approach, the paper itself serves as an artefact of the
sculpting process it describes.

</details>


### [7] [A Systematic Evaluation of Self-Supervised Learning for Label-Efficient Sleep Staging with Wearable EEG](https://arxiv.org/abs/2510.07960)
*Emilio Estevan,María Sierra-Torralba,Eduardo López-Larraz,Luis Montesano*

Main category: cs.HC

TL;DR: 本研究首次系统评估自监督学习在可穿戴EEG睡眠分期中的应用，显示其在数据标注稀缺的情况下，能有效提高分类准确率，推动睡眠监测系统的经济化。


<details>
  <summary>Details</summary>
Motivation: 针对可穿戴EEG设备产生的大量未标注数据，通过自监督学习方法解决标注稀缺问题，减少标注工作量。

Method: 采用自监督学习方法对可穿戴EEG数据进行系统评估，并在不同的睡眠数据库上进行测试。

Result: SSL方法在分类表现上提升了最高10%的准确率，并在仅使用5%至10%的已标注数据的情况下，达到了80%以上的临床级准确率。

Conclusion: 自监督学习（SSL）在可穿戴EEG中展示了有效的睡眠分期能力，能够在较少标注数据的基础上实现临床级准确性，并推动了经济实惠的睡眠监测系统的发展。

Abstract: Wearable EEG devices have emerged as a promising alternative to
polysomnography (PSG). As affordable and scalable solutions, their widespread
adoption results in the collection of massive volumes of unlabeled data that
cannot be analyzed by clinicians at scale. Meanwhile, the recent success of
deep learning for sleep scoring has relied on large annotated datasets.
Self-supervised learning (SSL) offers an opportunity to bridge this gap,
leveraging unlabeled signals to address label scarcity and reduce annotation
effort. In this paper, we present the first systematic evaluation of SSL for
sleep staging using wearable EEG. We investigate a range of well-established
SSL methods and evaluate them on two sleep databases acquired with the Ikon
Sleep wearable EEG headband: BOAS, a high-quality benchmark containing PSG and
wearable EEG recordings with consensus labels, and HOGAR, a large collection of
home-based, self-recorded, and unlabeled recordings. Three evaluation scenarios
are defined to study label efficiency, representation quality, and
cross-dataset generalization. Results show that SSL consistently improves
classification performance by up to 10% over supervised baselines, with gains
particularly evident when labeled data is scarce. SSL achieves clinical-grade
accuracy above 80% leveraging only 5% to 10% of labeled data, while the
supervised approach requires twice the labels. Additionally, SSL
representations prove robust to variations in population characteristics,
recording environments, and signal quality. Our findings demonstrate the
potential of SSL to enable label-efficient sleep staging with wearable EEG,
reducing reliance on manual annotations and advancing the development of
affordable sleep monitoring systems.

</details>


### [8] [Pre/Absence: Prompting Cultural Awareness and Understanding for Lost Architectural Heritage in Virtual Reality](https://arxiv.org/abs/2510.07967)
*Yaning Li,Ke Zhao,Shucheng Zheng,Xingyu Chen,Chenyi Chen,Wenxi Dai,Weile Jiang,Qi Dong,Yiqing Zhao,Meng Li,Lin-Ping Yuan*

Main category: cs.HC

TL;DR: 本研究通过虚拟现实体验探讨了失落建筑遗产的解读挑战，发现这种方式能够增强文化意识和情感参与，鼓励对遗产意义的批判性反思。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决失落建筑遗产的解读挑战，提出新的方法以更全面、更深入地理解和体验文化遗产。

Method: 本研究使用混合方法研究，与28名参与者比较了虚拟现实体验与传统纸质体验的效果。

Result: 本研究围绕唐代大明宫的汉阮殿，通过考古学家、遗产管理者和游客的视角，探讨了如何处理失落的建筑遗产及其解读中的挑战。我们发现传统的解读方法往往将复杂的文化层次简化为简单事实，并忽略了遗址消失后不断的再解释。为此，我们设计了一个名为“在前/缺失”的虚拟现实体验，以在时空叙事中互织实际存在与消失的遗产。通过与传统纸质体验的对比，我们的混合研究方法显示出，尽管两者均提升了用户的事实理解，但虚拟现实体验更强烈地增强了文化意识，引发了对失落的情感共鸣，并鼓励了对遗产不断演变的社会和政治意义的批判性反思。结果表明，虚拟现实可以超越静态重构，鼓励用户作为文化意义的共同构建者，从而为人机交互中的批判性遗产叙事设计提供了细致的框架。

Conclusion: 虚拟现实不仅改善了用户的事实理解，还能更有效地提升文化意识和情感共鸣，为批判性的遗产叙事设计提供了新的可能性。

Abstract: Lost architectural heritage presents interpretive challenges due to vanished
structures and fragmented historical records. Using Hanyuan Hall of the Tang
dynasty's Daming Palace as a case study, we conducted a formative investigation
with archaeologists, heritage administrators, and visitors to identify key
issues in current interpretation practices. We found that these practices often
compress complex cultural layers into factual summaries and rely on linear
narratives that overlook the continuing reinterpretations following a site's
disappearance. In response, we designed Pre/Absence, a virtual reality
experience grounded in the presence-absence dialectic to interweave tangible
and vanished aspects of heritage within a spatiotemporal narrative. A
mixed-method study with 28 participants compared Pre/Absence to a paper-based
experience. Both improved users' factual understanding, but the VR experience
more strongly enhanced cultural awareness, evoked emotional engagement with
loss, and encouraged critical reflection on the evolving social and political
meanings of heritage. The findings suggest that VR can move beyond static
reconstruction to engage users as co-constructors of cultural meaning,
providing a nuanced framework for critical heritage narrative design in
human-computer interaction.

</details>


### [9] [Quantifying Locomotion Differences Between Virtual Reality Users With and Without Motor Impairments](https://arxiv.org/abs/2510.07987)
*Rachel L. Franz,Jacob O. Wobbrock*

Main category: cs.HC

TL;DR: 本研究探讨了虚拟现实中的移动技术对身体障碍者的可达性，发现'滑动查看'技术表现优越，并提出了一些可以用于识别身体障碍的度量指标。


<details>
  <summary>Details</summary>
Motivation: 了解虚拟现实中的移动技术对身体有障碍用户的可达性影响。

Method: 通过让身体有障碍和没有障碍的人在虚拟环境中使用六种移动技术进行导航，收集并分析低层交互数据。

Result: 通过量化身体障碍者与正常用户在六种移动技术下的表现差异，发现某些技术可能更适合所有用户。

Conclusion: '滑动查看'可能是虚拟现实应用的良好默认移动技术，并且通过低级交互数据可以深入分析用户表现的差异。

Abstract: Today's virtual reality (VR) systems and environments assume that users have
typical abilities, which can make VR inaccessible to people with physical
impairments. However, there is not yet an understanding of how inaccessible
locomotion techniques are, and which interactions make them inaccessible. To
this end, we conducted a study in which people with and without upper-body
impairments navigated a virtual environment with six locomotion techniques to
quantify performance differences among groups. We found that groups performed
similarly with Sliding Looking on all performance measures, suggesting that
this might be a good default locomotion technique for VR apps. To understand
the nature of performance differences with the other techniques, we collected
low-level interaction data from the controllers and headset and analyzed
interaction differences with a set of movement-, button-, and target-related
metrics. We found that movement-related metrics from headset data reveal
differences among groups with all techniques, suggesting these are good metrics
for identifying whether a user has an upper-body impairment. We also identify
movement-, button, and target-related metrics that can explain performance
differences between groups for particular locomotion techniques.

</details>


### [10] [Development of Mental Models in Human-AI Collaboration: A Conceptual Framework](https://arxiv.org/abs/2510.08104)
*Joshua Holstein,Gerhard Satzger*

Main category: cs.HC

TL;DR: 本文探讨了人类与AI的合作如何影响决策者心智模型的演变，提出了设计有效合作的机制


<details>
  <summary>Details</summary>
Motivation: 探讨人类与人工智能的合作中决策者心智模型的演变以及其设计影响

Method: 构建一个综合的社会技术框架

Result: 提出三个互补且相互依赖的心智模型，并识别影响其发展的机制

Conclusion: 本文强调了心智模型的动态特性及其在有效人机协作设计中的重要性。

Abstract: Artificial intelligence has become integral to organizational decision-making
and while research has explored many facets of this human-AI collaboration, the
focus has mainly been on designing the AI agent(s) and the way the
collaboration is set up - generally assuming a human decision-maker to be
"fixed". However, it has largely been neglected that decision-makers' mental
models evolve through their continuous interaction with AI systems. This paper
addresses this gap by conceptualizing how the design of human-AI collaboration
influences the development of three complementary and interdependent mental
models necessary for this collaboration. We develop an integrated
socio-technical framework that identifies the mechanisms driving the mental
model evolution: data contextualization, reasoning transparency, and
performance feedback. Our work advances human-AI collaboration literature
through three key contributions: introducing three distinct mental models
(domain, information processing, complementarity-awareness); recognizing the
dynamic nature of mental models; and establishing mechanisms that guide the
purposeful design of effective human-AI collaboration.

</details>


### [11] [Sentiment Matters: An Analysis of 200 Human-SAV Interactions](https://arxiv.org/abs/2510.08202)
*Lirui Guo,Michael G. Burke,Wynita M. Griggs*

Main category: cs.HC

TL;DR: 本文介绍了一个包含200个SAV交互的数据集，并展示了其在SAV接受度和情感分析的应用。


<details>
  <summary>Details</summary>
Motivation: 随着共享自主车辆（SAV）成为交通系统的重要组成部分，有效的人类与SAV之间的交互研究变得尤为重要。

Method: 介绍了一个包含200个人类与共享自主车辆（SAV）交互的数据集，并通过基准案例研究验证了其效用。

Result: 识别出影响SAV接受度和感知服务质量的关键预测因素，并比较了以LLM为基础的情感分析工具与传统文本分析方法TextBlob的性能。

Conclusion: 本研究为设计对话式SAV界面提供了新的见解，并为进一步探索高级情感建模、自适应用户交互和多模态对话系统奠定了基础。

Abstract: Shared Autonomous Vehicles (SAVs) are likely to become an important part of
the transportation system, making effective human-SAV interactions an important
area of research. This paper introduces a dataset of 200 human-SAV interactions
to further this area of study. We present an open-source human-SAV
conversational dataset, comprising both textual data (e.g., 2,136 human-SAV
exchanges) and empirical data (e.g., post-interaction survey results on a range
of psychological factors). The dataset's utility is demonstrated through two
benchmark case studies: First, using random forest modeling and chord diagrams,
we identify key predictors of SAV acceptance and perceived service quality,
highlighting the critical influence of response sentiment polarity (i.e.,
perceived positivity). Second, we benchmark the performance of an LLM-based
sentiment analysis tool against the traditional lexicon-based TextBlob method.
Results indicate that even simple zero-shot LLM prompts more closely align with
user-reported sentiment, though limitations remain. This study provides novel
insights for designing conversational SAV interfaces and establishes a
foundation for further exploration into advanced sentiment modeling, adaptive
user interactions, and multimodal conversational systems.

</details>


### [12] [Practicing a Second Language Without Fear: Mixed Reality Agents for Interactive Group Conversation](https://arxiv.org/abs/2510.08227)
*Mariana Fernandez-Espinosa,Kai Zhang,Jad Bendarkawi,Ashley Ponce,Sean Chidozie Mata,Aminah Aliu,Lei Zhang,Francisco Fernandez Medina,Elena Mangione-Lora,Andres Monroy-Hernandez,Diego Gomez-Zara*

Main category: cs.HC

TL;DR: 本研究开发了一种名为ConversAR的混合现实系统，以增强第二语言学习者的口语能力，提供动态小组对话机会，提升学习者的参与感和沟通意愿。


<details>
  <summary>Details</summary>
Motivation: 学习第二语言的口语能力往往具有较高的认知负荷和情感压力，现有工具过于关注对话的单一场景，缺乏动态交互，迫切需要新的解决方案来满足学习者的需求。

Method: 通过与语言习得领域的专家进行初步研究，开发并测试了ConversAR系统，随后在21名第二语言学习者中进行用户研究。

Result: 本研究提出的ConversAR系统，通过混合现实技术、生成式人工智能以及增强现实，旨在提升第二语言的口语能力，该系统具有动态小组对话的能力，克服了传统学习工具的限制。

Conclusion: ConversAR系统有效提升了学习者的参与度、沟通意愿，并为他们提供了一个安全的交流空间，未来语言学习应用应考虑整合生成式AI与XR技术。

Abstract: Developing speaking proficiency in a second language can be cognitively
demanding and emotionally taxing, often triggering fear of making mistakes or
being excluded from larger groups. While current learning tools show promise
for speaking practice, most focus on dyadic, scripted scenarios, limiting
opportunities for dynamic group interactions. To address this gap, we present
ConversAR, a Mixed Reality system that leverages Generative AI and XR to
support situated and personalized group conversations. It integrates embodied
AI agents, scene recognition, and generative 3D props anchored to real-world
surroundings. Based on a formative study with experts in language acquisition,
we developed and tested this system with a user study with 21 second-language
learners. Results indicate that the system enhanced learner engagement,
increased willingness to communicate, and offered a safe space for speaking. We
discuss the implications for integrating Generative AI and XR into the design
of future language learning applications.

</details>


### [13] [Simulating Teams with LLM Agents: Interactive 2D Environments for Studying Human-AI Dynamics](https://arxiv.org/abs/2510.08242)
*Mohammed Almutairi,Charles Chiang,Haoze Guo,Matthew Belcher,Nandini Banerjee,Maria Milkowski,Svitlana Volkova,Daniel Nguyen,Tim Weninger,Michael Yankoski,Trenton W. Ford,Diego Gomez-Zara*

Main category: cs.HC

TL;DR: VirTLab是一种允许用户创建定制化团队动态模拟的新系统，利用基于LLM的代理在2D环境中进行实验。


<details>
  <summary>Details</summary>
Motivation: 在团队动态和表现研究中，允许用户创建自己模拟的能力是强大的工具，现有的框架往往受限于预定义场景或静态任务。

Method: VirTLab允许用户设计交互式、可定制的团队动态模拟，通过基于LLM的代理在2D空间环境中进行实验。

Result: 该系统展示了定制环境在推进多智能体模拟研究中的重要性，并且通过与实证评估和用户研究的对齐证明了其实用性。

Conclusion: VirTLab为研究团队动态提供了一个重要的测试平台，通过可定制的环境促进了多智能体实验的设计和分析。

Abstract: Enabling users to create their own simulations offers a powerful way to study
team dynamics and performance. We introduce VirTLab, a system that allows
researchers and practitioners to design interactive, customizable simulations
of team dynamics with LLM-based agents situated in 2D spatial environments.
Unlike prior frameworks that restrict scenarios to predefined or static tasks,
our approach enables users to build scenarios, assign roles, and observe how
agents coordinate, move, and adapt over time. By bridging team cognition
behaviors with scalable agent-based modeling, our system provides a testbed for
investigating how environments influence coordination, collaboration, and
emergent team behaviors. We demonstrate its utility by aligning simulated
outcomes with empirical evaluations and a user study, underscoring the
importance of customizable environments for advancing research on multi-agent
simulations. This work contributes to making simulations accessible to both
technical and non-technical users, supporting the design, execution, and
analysis of complex multi-agent experiments.

</details>


### [14] [LacAIDes: Generative AI-Supported Creative Interactive Circuits Crafting to Enliven Traditional Lacquerware](https://arxiv.org/abs/2510.08326)
*Yaning Li,Yutong Chen,Yihan Hou,Chenyi Chen,Yihan Han,Jingxuan Han,Wenxi Dai,Youyou Li,Xinke Tang,Meng Li,Qi Dong,Hongwei Li*

Main category: cs.HC

TL;DR: 本文提出了一种生成式AI工具LacAIDes，旨在帮助非技术背景的制作者探索和创建具有文化根基的交互电路，提升传统漆器工艺的活力。


<details>
  <summary>Details</summary>
Motivation: 推动中国传统漆器工艺的创新设计与创作，解决与人机交互相关的技术门槛问题。

Method: 通过与34名参与者的纵向工作坊进行混合方法评估，验证LacAIDes的可用性和应用效果。

Result: 开发并评估了一个名为LacAIDes的生成式AI创意支持工具，促进了用户在漆器制作中的创造性参与。

Conclusion: LacAIDes工具提高了用户在漆器制作中的参与感和创造力，并促使对生成式AI在数字工艺实践中作用的反思。

Abstract: Lacquerware, a representative craft of Chinese intangible cultural heritage,
is renowned for its layered aesthetics and durability but faces declining
engagement. While prior human-computer interaction research has explored
embedding interactive circuits to transform lacquerware into responsive
artifacts, most studies have focused on fabrication techniques rather than
supporting makers in creatively designing such interactions at a low threshold.
To address this gap, we present LacAIDes, a Generative AI powered
creativity-support tool built on a multi-agent workflow aligned with the double
diamond model of design thinking. LacAIDes enables exploration and creation of
culturally grounded interactive circuits without requiring prior technical
expertise. We evaluated LacAIDes in a longitudinal workshop with 34
participants using a mixed-method approach. Results show that LacAIDes
demonstrated high usability, enhanced creative engagement in craft making, and
encouraged critical reflection on the role of Generative AI in digital craft
practices. This work contributes to human-computer interaction by introducing a
novel creativity-support tool and providing empirical insights into
revitalizing traditional craft making through Generative AI.

</details>


### [15] [Motion Exploration of Articulated Product Concepts in Interactive Sketching Environment](https://arxiv.org/abs/2510.08328)
*Kalyan Ramana Gattoz,Prasad S. Onkar*

Main category: cs.HC

TL;DR: 提出了一种数字化草图设计的新方法，显著降低了设计过程中的认知负担。


<details>
  <summary>Details</summary>
Motivation: 设计过程中需要频繁调整物体运动，传统方法耗时且不够直观。

Method: 开发了一款应用程序来评估数字化草图设计的交互式模拟方法。

Result: 与传统方法相比，认知努力降低了77%，用户满意度高。

Conclusion: 该方法有效减少了设计师的认知努力，且用户满意度高，未来将扩展到更多维度的设计环境。

Abstract: In the early stages of engineering design, it is essential to know how a
product behaves, especially how it moves. As designers must keep adjusting the
motion until it meets the intended requirements, this process is often
repetitive and time-consuming. Although the physics behind these motions is
usually based on simple equations, manually working through them can be tedious
and inefficient. To ease this burden, some tasks are now handled by computers.
One common method involves converting hand-drawn sketches into models using CAD
or CAE software. However, this approach can be time- and resource-intensive.
Additionally, product sketches are usually best understood only by the
designers who created them. Others may struggle to interpret them correctly,
relying heavily on intuition and prior experience. Since sketches are static,
they fail to show how a product moves, limiting their usefulness. This paper
presents a new approach that addresses these issues by digitising the natural
act of sketching. It allows designers to create, simulate, and test the motion
of mechanical concepts in a more interactive way. An application was developed
to evaluate this method, focusing on user satisfaction and mental workload
during a design task. The results showed a 77% reduction in cognitive effort
compared to traditional methods, with users reporting high satisfaction. Future
work will focus on expanding this approach from 2D (planar) to full 3D
(spatial) design environments, enabling more complex product concept
development.

</details>


### [16] [What Makes a Visualization Complex?](https://arxiv.org/abs/2510.08332)
*Mengdi Chu,Zefeng Qiu,Meng Ling,Shuning Jiang,Robert S. Laramee,Michael Sedlmair,Jian Chen*

Main category: cs.HC

TL;DR: 本研究通过众包实验探讨数据可视化中的视觉复杂度，发现低级和高级特征皆对视觉复杂度产生影响，并提供了基于指标的量化和可解释性。


<details>
  <summary>Details</summary>
Motivation: 研究数据可视化中的视觉复杂度，以了解影响人类感知的客观指标。

Method: 通过大型众包实验收集和分析数据可视化中的视觉复杂度评分，并与多种图像基础指标进行对比研究。

Result: 发现低级图像特性和高级元素均对可视化图像的视觉复杂度产生影响。角的数量和不同颜色的数目是强有力的指标；特征拥挤度最能预测复杂度；文本注释的文本与墨水比例存在钟形曲线效应。

Conclusion: 本研究为可视化复杂度的理解提供了量化和可解释的方法，强调了图像特征在视觉复杂度感知中的重要性。

Abstract: We investigate the perceived visual complexity (VC) in data visualizations
using objective image-based metrics. We collected VC scores through a
large-scale crowdsourcing experiment involving 349 participants and 1,800
visualization images. We then examined how these scores align with 12
image-based metrics spanning information-theoretic, clutter, color, and our two
object-based metrics. Our results show that both low-level image properties and
the high-level elements affect perceived VC in visualization images; The number
of corners and distinct colors are robust metrics across visualizations.
Second, feature congestion, an information-theoretic metric capturing
statistical patterns in color and texture, is the strongest predictor of
perceived complexity in visualizations rich in the same stimuli; edge density
effectively explains VC in node-link diagrams. Additionally, we observe a
bell-curve effect for text annotations: increasing text-to-ink ratio (TiR)
initially reduces complexity, reaching an optimal point, beyond which further
text increases perceived complexity. Our quantification pipeline is also
interpretable, enabling metric-based explanations, grounded in the
VisComplexity2K dataset, bridging computational metrics with human perceptual
responses. osf.io/5xe8a has the preregistration and osf.io/bdet6 has the
VisComplexity2K dataset, source code, and all Apdx. and figures.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [17] [FLEET: Formal Language-Grounded Scheduling for Heterogeneous Robot Teams](https://arxiv.org/abs/2510.07417)
*Corban Rivera,Grayson Byrd,Meghan Booker,Bethany Kemp,Allison Gaines,Emma Holmes,James Uplinger,Celso M de Melo,David Handelman*

Main category: cs.RO

TL;DR: FLEET是一个混合的去中心化框架，通过将自然语言转化为优化的多机器人调度，提高了异构机器人团队的协调效率，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有语言驱动规划和正式方法在复杂任务协调中的不足，提出FLEET框架，提升多机器人团队的协作效率。

Method: 该方法结合了大规模语言模型 (LLM) 提供的任务图和能力感知的机器人-任务适应矩阵，以及混合整数线性规划 (MILP) 后端解决调度问题。

Result: FLEET在自由形式语言指导的多机器人协调基准测试中，成功率超过先进的生成规划方法，具体体现在异构任务的两代理团队上。

Conclusion: FLEET在多机器人协调中表现优越，成功率高于现有生成规划方法，能够有效处理异构任务和能力匹配。

Abstract: Coordinating heterogeneous robot teams from free-form natural-language
instructions is hard. Language-only planners struggle with long-horizon
coordination and hallucination, while purely formal methods require
closed-world models. We present FLEET, a hybrid decentralized framework that
turns language into optimized multi-robot schedules. An LLM front-end produces
(i) a task graph with durations and precedence and (ii) a capability-aware
robot--task fitness matrix; a formal back-end solves a makespan-minimization
problem while the underlying robots execute their free-form subtasks with
agentic closed-loop control. Across multiple free-form language-guided autonomy
coordination benchmarks, FLEET improves success over state of the art
generative planners on two-agent teams across heterogeneous tasks. Ablations
show that mixed integer linear programming (MILP) primarily improves temporal
structure, while LLM-derived fitness is decisive for capability-coupled tasks;
together they deliver the highest overall performance. We demonstrate the
translation to real world challenges with hardware trials using a pair of
quadruped robots with disjoint capabilities.

</details>


### [18] [VeMo: A Lightweight Data-Driven Approach to Model Vehicle Dynamics](https://arxiv.org/abs/2510.07447)
*Girolamo Oddo,Roberto Nuca,Matteo Parsani*

Main category: cs.RO

TL;DR: 提出了一种基于GRU层的编码-解码模型，在信息稀缺的条件下，模型表现出高性能和鲁棒性，适用于自主驾驶车辆。


<details>
  <summary>Details</summary>
Motivation: 在信息稀缺的情况下开发高性能车辆动态模型，特别是对于自主驾驶应用。

Method: 基于门控循环单元（GRU）层的轻量编码-解码模型。

Result: 模型在极端动态条件下实现了最高2.6%的均值相对误差，且对噪声输入数据具有良好的鲁棒性。

Conclusion: 该模型在极端动态条件下实现了最高为2.6%的均值相对误差，展现了良好的鲁棒性和物理一致性。

Abstract: Developing a dynamic model for a high-performance vehicle is a complex
problem that requires extensive structural information about the system under
analysis. This information is often unavailable to those who did not design the
vehicle and represents a typical issue in autonomous driving applications,
which are frequently developed on top of existing vehicles; therefore, vehicle
models are developed under conditions of information scarcity. This paper
proposes a lightweight encoder-decoder model based on Gate Recurrent Unit
layers to correlate the vehicle's future state with its past states, measured
onboard, and control actions the driver performs. The results demonstrate that
the model achieves a maximum mean relative error below 2.6% in extreme dynamic
conditions. It also shows good robustness when subject to noisy input data
across the interested frequency components. Furthermore, being entirely
data-driven and free from physical constraints, the model exhibits physical
consistency in the output signals, such as longitudinal and lateral
accelerations, yaw rate, and the vehicle's longitudinal velocity.

</details>


### [19] [HJCD-IK: GPU-Accelerated Inverse Kinematics through Batched Hybrid Jacobian Coordinate Descent](https://arxiv.org/abs/2510.07514)
*Cael Yasutake,Zachary Kingston,Brian Plancher*

Main category: cs.RO

TL;DR: HJCD-IK是一个结合了GPU加速的采样与优化的混合求解器，提升逆向运动学求解的速度和准确性，并释放了开源代码。


<details>
  <summary>Details</summary>
Motivation: 逆向运动学问题在机器人领域至关重要，传统的解析求解器受限于自由度和拓扑结构，而数值优化方法虽然更通用但计算成本高且易陷入局部极小值。

Method: HJCD-IK结合了方向敏感的贪心坐标下降初始化方案与基于雅可比的抛光例程。

Result: HJCD-IK在准确性和延迟之间达到了优良的平衡，提供高质量样本的广泛分布，并显著提高了求解器的性能。

Conclusion: HJCD-IK是一种GPU加速的混合求解器，显著提高了逆向运动学求解的收敛速度和整体准确性。

Abstract: Inverse Kinematics (IK) is a core problem in robotics, in which joint
configurations are found to achieve a desired end-effector pose. Although
analytical solvers are fast and efficient, they are limited to systems with low
degrees-of-freedom and specific topological structures. Numerical
optimization-based approaches are more general, but suffer from high
computational costs and frequent convergence to spurious local minima. Recent
efforts have explored the use of GPUs to combine sampling and optimization to
enhance both the accuracy and speed of IK solvers. We build on this recent
literature and introduce HJCD-IK, a GPU-accelerated, sampling-based hybrid
solver that combines an orientation-aware greedy coordinate descent
initialization scheme with a Jacobian-based polishing routine. This design
enables our solver to improve both convergence speed and overall accuracy as
compared to the state-of-the-art, consistently finding solutions along the
accuracy-latency Pareto frontier and often achieving order-of-magnitude gains.
In addition, our method produces a broad distribution of high-quality samples,
yielding the lowest maximum mean discrepancy. We release our code open-source
for the benefit of the community.

</details>


### [20] [AVO: Amortized Value Optimization for Contact Mode Switching in Multi-Finger Manipulation](https://arxiv.org/abs/2510.07548)
*Adam Hung,Fan Yang,Abhinav Kumar,Sergio Aguilera Marinovic,Soshi Iba,Rana Soltani Zarrin,Dmitry Berenson*

Main category: cs.RO

TL;DR: 本研究提出了一种新方法AVO，通过引入学习的价值函数来提高灵巧操作任务的优化性能，显著减少计算负担。


<details>
  <summary>Details</summary>
Motivation: 在灵巧操作任务中，切换不同的接触模式是必需的，但独立优化子任务会限制性能并增加计算成本。

Method: 使用学习的价值函数结合轨迹优化，在每个规划步骤引导优化器朝着最小化未来子任务成本的状态推进。

Result: 提出了Amortized Value Optimization (AVO)，实现了通过学习的价值函数来预测未来任务性能，从而改善了优化过程。

Conclusion: 通过模拟和真实实验验证，AVO在执行螺丝刀抓取和转动任务时，表现出更佳的性能和降低的计算预算。

Abstract: Dexterous manipulation tasks often require switching between different
contact modes, such as rolling, sliding, sticking, or non-contact contact
modes. When formulating dexterous manipulation tasks as a trajectory
optimization problem, a common approach is to decompose these tasks into
sub-tasks for each contact mode, which are each solved independently.
Optimizing each sub-task independently can limit performance, as optimizing
contact points, contact forces, or other variables without information about
future sub-tasks can place the system in a state from which it is challenging
to make progress on subsequent sub-tasks. Further, optimizing these sub-tasks
is very computationally expensive. To address these challenges, we propose
Amortized Value Optimization (AVO), which introduces a learned value function
that predicts the total future task performance. By incorporating this value
function into the cost of the trajectory optimization at each planning step,
the value function gradients guide the optimizer toward states that minimize
the cost in future sub-tasks. This effectively bridges separately optimized
sub-tasks, and accelerates the optimization by reducing the amount of online
computation needed. We validate AVO on a screwdriver grasping and turning task
in both simulation and real world experiments, and show improved performance
even with 50% less computational budget compared to trajectory optimization
without the value function.

</details>


### [21] [Inspection Planning Primitives with Implicit Models](https://arxiv.org/abs/2510.07611)
*Jingyang You,Hanna Kurniawati,Lashika Medagoda*

Main category: cs.RO

TL;DR: 本研究提出了一种新方法，通过使用隐式模型（神经SDFs）可以显著减少检查规划的内存需求，而保持轨迹质量。


<details>
  <summary>Details</summary>
Motivation: 基础设施的老化和复杂性增加，使得高效的检查规划变得更加重要，以确保安全。

Method: 本文提出了一组基本计算，即隐式模型检查规划基本计算（IPIM），来支持基于采样的检查规划器在规划过程中完全使用隐式表示。

Result: 提出了一套被称为隐式模型检查规划基本计算的方法（IPIM），使得基于采样的检查规划器能够完全利用神经签名距离函数（SDF）表示。

Conclusion: 实验表明，使用IPIM的基本采样规划器能够生成与最先进规划器相似质量的检查轨迹，同时使用的内存减少了多达70倍。

Abstract: The aging and increasing complexity of infrastructures make efficient
inspection planning more critical in ensuring safety. Thanks to sampling-based
motion planning, many inspection planners are fast. However, they often require
huge memory. This is particularly true when the structure under inspection is
large and complex, consisting of many struts and pillars of various geometry
and sizes. Such structures can be represented efficiently using implicit
models, such as neural Signed Distance Functions (SDFs). However, most
primitive computations used in sampling-based inspection planner have been
designed to work efficiently with explicit environment models, which in turn
requires the planner to use explicit environment models or performs frequent
transformations between implicit and explicit environment models during
planning. This paper proposes a set of primitive computations, called
Inspection Planning Primitives with Implicit Models (IPIM), that enable
sampling-based inspection planners to entirely use neural SDFs representation
during planning. Evaluation on three scenarios, including inspection of a
complex real-world structure with over 92M triangular mesh faces, indicates
that even a rudimentary sampling-based planner with IPIM can generate
inspection trajectories of similar quality to those generated by the
state-of-the-art planner, while using up to 70x less memory than the
state-of-the-art inspection planner.

</details>


### [22] [GATO: GPU-Accelerated and Batched Trajectory Optimization for Scalable Edge Model Predictive Control](https://arxiv.org/abs/2510.07625)
*Alexander Du,Emre Adabag,Gabriel Bravo,Brian Plancher*

Main category: cs.RO

TL;DR: 提出GATO，一个GPU加速的批处理轨迹优化求解器，满足实时MPC应用需求，显著提高求解性能并开源以支持再现性和采用


<details>
  <summary>Details</summary>
Motivation: 解决现有基于GPU的方法在实时性能和模型通用性方面的不足，以满足现代MPC应用对实时批处理的需求

Method: 提出了一种名为GATO的开放源代码、GPU加速的批处理轨迹优化求解器

Result: 在批处理大小增加时，GATO在CPU基线之上实现18-21倍的加速，在GPU基线之上实现1.4-16倍的加速，同时改善了干扰抑制和收敛特性，并在工业操纵器上进行了硬件验证

Conclusion: GATO有效提升批处理轨迹优化的求解性能，使其适合实时应用，并为研究者和开发者提供开放的工具支持。

Abstract: While Model Predictive Control (MPC) delivers strong performance across
robotics applications, solving the underlying (batches of) nonlinear trajectory
optimization (TO) problems online remains computationally demanding. Existing
GPU-accelerated approaches typically (i) parallelize a single solve to meet
real-time deadlines, (ii) scale to very large batches at slower-than-real-time
rates, or (iii) achieve speed by restricting model generality (e.g., point-mass
dynamics or a single linearization). This leaves a large gap in solver
performance for many state-of-the-art MPC applications that require real-time
batches of tens to low-hundreds of solves. As such, we present GATO, an open
source, GPU-accelerated, batched TO solver co-designed across algorithm,
software, and computational hardware to deliver real-time throughput for these
moderate batch size regimes. Our approach leverages a combination of block-,
warp-, and thread-level parallelism within and across solves for ultra-high
performance. We demonstrate the effectiveness of our approach through a
combination of: simulated benchmarks showing speedups of 18-21x over CPU
baselines and 1.4-16x over GPU baselines as batch size increases; case studies
highlighting improved disturbance rejection and convergence behavior; and
finally a validation on hardware using an industrial manipulator. We open
source GATO to support reproducibility and adoption.

</details>


### [23] [Differentiable Particle Optimization for Fast Sequential Manipulation](https://arxiv.org/abs/2510.07674)
*Lucas Chen,Shrutheesh Raman Iyer,Zachary Kingston*

Main category: cs.RO

TL;DR: SPaSM是一个全GPU并行化框架，用于实时优化顺序机器人操作任务，显著提高了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 寻找无碰撞且满足多物体交互几何约束的轨迹，以便在高维配置空间中进行顺序机器人操作，解决实时和大规模计算的挑战。

Method: 提出了一种两阶段粒子优化策略，通过并行采样和关节空间的轨迹优化实现目标约束的联合优化。

Result: SPaSM在复杂基准测试中展示了高达4000倍的速度提升，与现有方法相比具有显著的优势。

Conclusion: SPaSM实现了在毫秒级别的解决时间，并在复杂基准测试中取得了100%的成功率。

Abstract: Sequential robot manipulation tasks require finding collision-free
trajectories that satisfy geometric constraints across multiple object
interactions in potentially high-dimensional configuration spaces. Solving
these problems in real-time and at large scales has remained out of reach due
to computational requirements. Recently, GPU-based acceleration has shown
promising results, but prior methods achieve limited performance due to CPU-GPU
data transfer overhead and complex logic that prevents full hardware
utilization. To this end, we present SPaSM (Sampling Particle optimization for
Sequential Manipulation), a fully GPU-parallelized framework that compiles
constraint evaluation, sampling, and gradient-based optimization into optimized
CUDA kernels for end-to-end trajectory optimization without CPU coordination.
The method consists of a two-stage particle optimization strategy: first
solving placement constraints through massively parallel sampling, then lifting
solutions to full trajectory optimization in joint space. Unlike hierarchical
approaches, SPaSM jointly optimizes object placements and robot trajectories to
handle scenarios where motion feasibility constrains placement options.
Experimental evaluation on challenging benchmarks demonstrates solution times
in the realm of $\textbf{milliseconds}$ with a 100% success rate; a
$4000\times$ speedup compared to existing approaches.

</details>


### [24] [EB-MBD: Emerging-Barrier Model-Based Diffusion for Safe Trajectory Optimization in Highly Constrained Environments](https://arxiv.org/abs/2510.07700)
*Raghav Mishra,Ian R. Manchester*

Main category: cs.RO

TL;DR: 通过引入新兴障碍函数，EB-MBD显著改善了基于模型的扩散在处理约束时的性能，特别是在计算效率和解决方案质量上。


<details>
  <summary>Details</summary>
Motivation: 基于模型的扩散存在约束时可能导致性能显著下降，尤其是在简单的二维系统中，主要由于蒙特卡洛方法在评分函数近似中的样本低效性。

Method: 引入新兴障碍函数以约束基于模型的扩散过程

Result: 新兴障碍模型基于扩散（EB-MBD）通过逐步引入障碍约束，避免了样本低效性的问题，显著提高了解决方案质量，并且不需要计算复杂的投影操作。

Conclusion: EB-MBD方法在二维避碰和三维水下操控等应用中表现出较低的成本解决方案，并且计算时间远小于基于投影的方法。

Abstract: We propose enforcing constraints on Model-Based Diffusion by introducing
emerging barrier functions inspired by interior point methods. We show that
constraints on Model-Based Diffusion can lead to catastrophic performance
degradation, even on simple 2D systems due to sample inefficiency in the Monte
Carlo approximation of the score function. We introduce Emerging-Barrier
Model-Based Diffusion (EB-MBD) which uses progressively introduced barrier
constraints to avoid these problems, significantly improving solution quality,
without the need for computationally expensive operations such as projections.
We analyze the sampling liveliness of samples each iteration to inform barrier
parameter scheduling choice. We demonstrate results for 2D collision avoidance
and a 3D underwater manipulator system and show that our method achieves lower
cost solutions than Model-Based Diffusion, and requires orders of magnitude
less computation time than projection based methods.

</details>


### [25] [Probabilistically-Safe Bipedal Navigation over Uncertain Terrain via Conformal Prediction and Contraction Analysis](https://arxiv.org/abs/2510.07725)
*Kasidit Muenprasitivej,Ye Zhao,Glen Chou*

Main category: cs.RO

TL;DR: 本研究开发了一种针对双足机器人在复杂地形中安全行走的规划和控制策略，采用高层次模型预测控制框架和不确定性建模方法，确保动态可行性和中心稳健性。


<details>
  <summary>Details</summary>
Motivation: 旨在解决双足机器人在不同地形条件下的安全行走挑战，确保动态稳定性和导航的可靠性。

Method: 构建了一种基于契约控制的模型预测控制框架，结合高斯过程回归和符号预测，以实现地形的不确定性建模。

Result: 提出的规划策略确保双足机器人在有不确定性的地形上安全行进并能够达到预定目标。

Conclusion: 通过物理仿真验证了规划框架的有效性，并确保了机器人在不确定地形中的安全行进和目标到达。

Abstract: We address the challenge of enabling bipedal robots to traverse rough terrain
by developing probabilistically safe planning and control strategies that
ensure dynamic feasibility and centroidal robustness under terrain uncertainty.
Specifically, we propose a high-level Model Predictive Control (MPC) navigation
framework for a bipedal robot with a specified confidence level of safety that
(i) enables safe traversal toward a desired goal location across a terrain map
with uncertain elevations, and (ii) formally incorporates uncertainty bounds
into the centroidal dynamics of locomotion control. To model the rough terrain,
we employ Gaussian Process (GP) regression to estimate elevation maps and
leverage Conformal Prediction (CP) to construct calibrated confidence intervals
that capture the true terrain elevation. Building on this, we formulate
contraction-based reachable tubes that explicitly account for terrain
uncertainty, ensuring state convergence and tube invariance. In addition, we
introduce a contraction-based flywheel torque control law for the reduced-order
Linear Inverted Pendulum Model (LIPM), which stabilizes the angular momentum
about the center-of-mass (CoM). This formulation provides both probabilistic
safety and goal reachability guarantees. For a given confidence level, we
establish the forward invariance of the proposed torque control law by
demonstrating exponential stabilization of the actual CoM phase-space
trajectory and the desired trajectory prescribed by the high-level planner.
Finally, we evaluate the effectiveness of our planning framework through
physics-based simulations of the Digit bipedal robot in MuJoCo.

</details>


### [26] [Injecting Hallucinations in Autonomous Vehicles: A Component-Agnostic Safety Evaluation Framework](https://arxiv.org/abs/2510.07749)
*Alexandre Moreira Nascimento,Gabriel Kenji Godoy Shimanuki,Lúcio Flavio Vismari,João Batista Camargo Jr,Jorge Rady de Almeida Jr,Paulo Sergio Cugnasca,Anna Carolina Muller Queiroz,Jeremy Noah Bailenson*

Main category: cs.RO

TL;DR: 提出一种新的幻觉注入框架，以增强对自动驾驶汽车安全性的验证和研究，尤其是在感知失败的情境下。


<details>
  <summary>Details</summary>
Motivation: 研究自动驾驶汽车中的感知失败对安全性的影响，解决现有研究针对单一传感器或机器感知模块的限制。

Method: 提出一种可配置、组件无关的幻觉注入框架，在开源模拟器中迭代诱发六种可行的幻觉类型。

Result: 该框架经过统计验证，量化了每种幻觉类型对碰撞和接近事故的影响，发现某些幻觉显著增加碰撞风险。

Conclusion: 该框架提供了一种可扩展且经过统计验证的工具，能够加速自动驾驶汽车的安全验证，为未来的故障容忍性和韧性设计研究奠定基础。

Abstract: Perception failures in autonomous vehicles (AV) remain a major safety concern
because they are the basis for many accidents. To study how these failures
affect safety, researchers typically inject artificial faults into hardware or
software components and observe the outcomes. However, existing fault injection
studies often target a single sensor or machine perception (MP) module,
resulting in siloed frameworks that are difficult to generalize or integrate
into unified simulation environments. This work addresses that limitation by
reframing perception failures as hallucinations, false perceptions that distort
an AV situational awareness and may trigger unsafe control actions. Since
hallucinations describe only observable effects, this abstraction enables
analysis independent of specific sensors or algorithms, focusing instead on how
their faults manifest along the MP pipeline. Building on this concept, we
propose a configurable, component-agnostic hallucination injection framework
that induces six plausible hallucination types in an iterative open-source
simulator. More than 18,350 simulations were executed in which hallucinations
were injected while AVs crossed an unsignalized transverse street with traffic.
The results statistically validate the framework and quantify the impact of
each hallucination type on collisions and near misses. Certain hallucinations,
such as perceptual latency and drift, significantly increase the risk of
collision in the scenario tested, validating the proposed paradigm can stress
the AV system safety. The framework offers a scalable, statistically validated,
component agnostic, and fully interoperable toolset that simplifies and
accelerates AV safety validations, even those with novel MP architectures and
components. It can potentially reduce the time-to-market of AV and lay the
foundation for future research on fault tolerance, and resilient AV design.

</details>


### [27] [Trajectory Conditioned Cross-embodiment Skill Transfer](https://arxiv.org/abs/2510.07773)
*YuHang Tang,Yixuan Lou,Pengfei Han,Haoming Song,Xinyi Ye,Dong Wang,Bin Zhao*

Main category: cs.RO

TL;DR: TrajSkill是一种跨体现技能转移框架，通过稀疏光流轨迹整合人类操控技能，显著提升机器人操作效率及成功率。


<details>
  <summary>Details</summary>
Motivation: 解决人类示范视频与机器人操作之间的体现差距，旨在提高技能迁移的可扩展性和泛化能力。

Method: 通过使用稀疏光流轨迹来表示人类动作，结合视觉和文本输入，合成机器人操控视频并转化为可执行动作。

Result: 在MetaWorld仿真数据中，TrajSkill相比于当前最先进的技术减少了39.6%的FVD和36.6%的KVD，且提升了跨体现成功率达16.7%。在厨房操控任务的真实机器人实验中验证了方法的有效性。

Conclusion: TrajSkill实现了有效的人类到机器人技能转移，特别是在不同身体结构间的迁移上表现出色。

Abstract: Learning manipulation skills from human demonstration videos presents a
promising yet challenging problem, primarily due to the significant embodiment
gap between human body and robot manipulators. Existing methods rely on paired
datasets or hand-crafted rewards, which limit scalability and generalization.
We propose TrajSkill, a framework for Trajectory Conditioned Cross-embodiment
Skill Transfer, enabling robots to acquire manipulation skills directly from
human demonstration videos. Our key insight is to represent human motions as
sparse optical flow trajectories, which serve as embodiment-agnostic motion
cues by removing morphological variations while preserving essential dynamics.
Conditioned on these trajectories together with visual and textual inputs,
TrajSkill jointly synthesizes temporally consistent robot manipulation videos
and translates them into executable actions, thereby achieving cross-embodiment
skill transfer. Extensive experiments are conducted, and the results on
simulation data (MetaWorld) show that TrajSkill reduces FVD by 39.6\% and KVD
by 36.6\% compared with the state-of-the-art, and improves cross-embodiment
success rate by up to 16.7\%. Real-robot experiments in kitchen manipulation
tasks further validate the effectiveness of our approach, demonstrating
practical human-to-robot skill transfer across embodiments.

</details>


### [28] [IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction](https://arxiv.org/abs/2510.07778)
*Yandu Chen,Kefan Gu,Yuqing Wen,Yucheng Zhao,Tiancai Wang,Liqiang Nie*

Main category: cs.RO

TL;DR: IntentionVLA 是一种新颖的模型，通过课程训练和高效推理机制，增强了机器人在复杂场景中的人类意图理解能力，显著提高了人类-机器人交互的成功率。


<details>
  <summary>Details</summary>
Motivation: 当前的 SOTA VLA 模型在复杂真实场景中缺乏推理能力，无法处理隐含的人类意图推理，因此需要改进。

Method: IntentionVLA采用课程训练范式，整合了意图推理、空间定位和紧凑的具身推理，提升模型的推理和感知能力。

Result: IntentionVLA在直接指令下成功率提高了18%，意图指令下提高了28%，在无监督意图任务上成功率是所有基线的两倍，且实现了40%的零-shot 人机交互成功率。

Conclusion: IntentionVLA 是一种新兴的 VLA 框架，显著提升了人类-机器人交互的成功率，尤其在无监督的意图任务中表现优异。

Abstract: Vision-Language-Action (VLA) models leverage pretrained vision-language
models (VLMs) to couple perception with robotic control, offering a promising
path toward general-purpose embodied intelligence. However, current SOTA VLAs
are primarily pretrained on multimodal tasks with limited relevance to embodied
scenarios, and then finetuned to map explicit instructions to actions.
Consequently, due to the lack of reasoning-intensive pretraining and
reasoning-guided manipulation, these models are unable to perform implicit
human intention reasoning required for complex, real-world interactions. To
overcome these limitations, we propose \textbf{IntentionVLA}, a VLA framework
with a curriculum training paradigm and an efficient inference mechanism. Our
proposed method first leverages carefully designed reasoning data that combine
intention inference, spatial grounding, and compact embodied reasoning,
endowing the model with both reasoning and perception capabilities. In the
following finetuning stage, IntentionVLA employs the compact reasoning outputs
as contextual guidance for action generation, enabling fast inference under
indirect instructions. Experimental results show that IntentionVLA
substantially outperforms $\pi_0$, achieving 18\% higher success rates with
direct instructions and 28\% higher than ECoT under intention instructions. On
out-of-distribution intention tasks, IntentionVLA achieves over twice the
success rate of all baselines, and further enables zero-shot human-robot
interaction with 40\% success rate. These results highlight IntentionVLA as a
promising paradigm for next-generation human-robot interaction (HRI) systems.

</details>


### [29] [GM3: A General Physical Model for Micro-Mobility Vehicles](https://arxiv.org/abs/2510.07807)
*Grace Cai,Nithin Parepally,Laura Zheng,Ming C. Lin*

Main category: cs.RO

TL;DR: 提出一种新模型GM3，以更好地模拟微出行车辆的动力学。


<details>
  <summary>Details</summary>
Motivation: 现有模型在捕捉微出行车辆的动态特性方面存在不足，因此需要一个更全面的动力学模型。

Method: 采用基于轮胎刷表示的轮胎级别公式，并建立了一个模型无关的交互式仿真框架。

Result: 通过与KBM和其他模型进行比较，GM3在斯坦福无人机数据集中得到了经验验证。

Conclusion: GM3为微出行车辆提供了一种统一的、基于物理的动力学模型，能够跨越不同的车辆配置进行精准模拟。

Abstract: Modeling the dynamics of micro-mobility vehicles (MMV) is becoming
increasingly important for training autonomous vehicle systems and building
urban traffic simulations. However, mainstream tools rely on variants of the
Kinematic Bicycle Model (KBM) or mode-specific physics that miss tire slip,
load transfer, and rider/vehicle lean. To our knowledge, no unified,
physics-based model captures these dynamics across the full range of common
MMVs and wheel layouts. We propose the "Generalized Micro-mobility Model"
(GM3), a tire-level formulation based on the tire brush representation that
supports arbitrary wheel configurations, including single/double track and
multi-wheel platforms. We introduce an interactive model-agnostic simulation
framework that decouples vehicle/layout specification from dynamics to compare
the GM3 with the KBM and other models, consisting of fixed step RK4
integration, human-in-the-loop and scripted control, real-time trajectory
traces and logging for analysis. We also empirically validate the GM3 on the
Stanford Drone Dataset's deathCircle (roundabout) scene for biker, skater, and
cart classes.

</details>


### [30] [DM1: MeanFlow with Dispersive Regularization for 1-Step Robotic Manipulation](https://arxiv.org/abs/2510.07865)
*Guowei Zou,Haitao Wang,Hejun Wu,Yukun Qian,Yuhang Wang,Weibing Li*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的流匹配框架DM1，利用分散正则化来防止表示崩溃，提升机器人操作的效果与效率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于流的策略面临表示崩溃的问题，影响精确操作，因此需要一种新的框架来解决此问题。

Method: DM1结合了分散正则化与MeanFlow，采用多个正则化变体以鼓励训练批次间的多样化表示。

Result: DM1在RoboMimic基准上比基线提高了10-20个百分点的成功率，并在Lift任务中取得了99%的成功率。

Conclusion: DM1通过分散正则化实现了高效且稳健的机器人操作，证明了从模拟到实际应用的有效转移。

Abstract: The ability to learn multi-modal action distributions is indispensable for
robotic manipulation policies to perform precise and robust control. Flow-based
generative models have recently emerged as a promising solution to learning
distributions of actions, offering one-step action generation and thus
achieving much higher sampling efficiency compared to diffusion-based methods.
However, existing flow-based policies suffer from representation collapse, the
inability to distinguish similar visual representations, leading to failures in
precise manipulation tasks. We propose DM1 (MeanFlow with Dispersive
Regularization for One-Step Robotic Manipulation), a novel flow matching
framework that integrates dispersive regularization into MeanFlow to prevent
collapse while maintaining one-step efficiency. DM1 employs multiple dispersive
regularization variants across different intermediate embedding layers,
encouraging diverse representations across training batches without introducing
additional network modules or specialized training procedures. Experiments on
RoboMimic benchmarks show that DM1 achieves 20-40 times faster inference (0.07s
vs. 2-3.5s) and improves success rates by 10-20 percentage points, with the
Lift task reaching 99% success over 85% of the baseline. Real-robot deployment
on a Franka Panda further validates that DM1 transfers effectively from
simulation to the physical world. To the best of our knowledge, this is the
first work to leverage representation regularization to enable flow-based
policies to achieve strong performance in robotic manipulation, establishing a
simple yet powerful approach for efficient and robust manipulation.

</details>


### [31] [USIM and U0: A Vision-Language-Action Dataset and Model for General Underwater Robots](https://arxiv.org/abs/2510.07869)
*Junwen Gu,Zhiheng wu,Pengxuan Si,Shuang Qiu,Yukai Feng,Luoyang Sun,Laien Luo,Lianyi Yu,Jian Wang,Zhengxing Wu*

Main category: cs.RO

TL;DR: 本论文介绍了USIM数据集及其上的U0模型，以克服水下机器人在多任务执行中的困难。


<details>
  <summary>Details</summary>
Motivation: 水下环境对机器人操作提出了独特的挑战，包括复杂的水动力学、有限的可视性和受限的通信。传统数据驱动方法尚难以满足多任务的水下智能机器人需求，因此开发可以自主管理多任务的水下智能机器人仍极具挑战性。

Method: USIM包含561K帧数据，涵盖20个任务与9种场景；U0结合双目视觉与传感器融合，通过卷积注意力模块提升空间理解和移动操作能力。

Result: 提出了USIM，一个基于仿真的多任务视觉-语言-动作（VLA）数据集，以及U0，一个集成双目视觉和其他传感器的VLA模型，展示了在80%成功率的基础上，提升移动操作的有效性。

Conclusion: USIM和U0表明，VLA模型可有效应用于水下机器人领域，为可扩展数据集建设和智能机器人自主执行任务提供了基础。

Abstract: Underwater environments present unique challenges for robotic operation,
including complex hydrodynamics, limited visibility, and constrained
communication. Although data-driven approaches have advanced embodied
intelligence in terrestrial robots and enabled task-specific autonomous
underwater robots, developing underwater intelligence capable of autonomously
performing multiple tasks remains highly challenging, as large-scale,
high-quality underwater datasets are still scarce. To address these
limitations, we introduce USIM, a simulation-based multi-task
Vision-Language-Action (VLA) dataset for underwater robots. USIM comprises over
561K frames from 1,852 trajectories, totaling approximately 15.6 hours of
BlueROV2 interactions across 20 tasks in 9 diverse scenarios, ranging from
visual navigation to mobile manipulation. Building upon this dataset, we
propose U0, a VLA model for general underwater robots, which integrates
binocular vision and other sensor modalities through multimodal fusion, and
further incorporates a convolution-attention-based perception focus enhancement
module (CAP) to improve spatial understanding and mobile manipulation. Across
tasks such as inspection, obstacle avoidance, scanning, and dynamic tracking,
the framework achieves a success rate of 80%, while in challenging mobile
manipulation tasks, it reduces the distance to the target by 21.2% compared
with baseline methods, demonstrating its effectiveness. USIM and U0 show that
VLA models can be effectively applied to underwater robotic applications,
providing a foundation for scalable dataset construction, improved task
autonomy, and the practical realization of intelligent general underwater
robots.

</details>


### [32] [Team Xiaomi EV-AD VLA: Learning to Navigate Socially Through Proactive Risk Perception -- Technical Report for IROS 2025 RoboSense Challenge Social Navigation Track](https://arxiv.org/abs/2510.07871)
*Erjia Xiao,Lingfeng Zhang,Yingbo Tang,Hao Cheng,Renjing Xu,Wenbo Ding,Lei Zhou,Long Chen,Hangjun Ye,Xiaoshuai Hao*

Main category: cs.RO

TL;DR: 本报告介绍了一种增强社会导航性能的RGBD感知与导航系统，并在IROS 2025挑战赛中获得第二名。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机在于开发一种自主代理能够在复杂的室内环境中安全、高效地导航，同时遵循社会规范，例如保持安全距离和避免碰撞。

Method: 本研究基于Falcon模型，提出了一种前瞻性风险感知模块，用于学习预测周围人类的基于距离的碰撞风险分数，从而提高空间感知和主动避碰行为。

Result: 本报告描述了我们在IROS 2025 RoboSense挑战赛社会导航赛道中的提交内容。我们提出了一种基于RGBD的感知与导航系统，旨在使自主代理能够在动态人群密集的室内环境中安全、高效并符合社会规范地导航。我们根据Falcon模型引入了一种前瞻性风险感知模块，以增强社会导航的性能。通过评估，我们的方法在Social-HM3D基准上提高了代理人员的个人空间合规能力，并在挑战中获得了16支参赛队伍中的第二名。

Conclusion: 通过引入前瞻性风险感知模块，我们的方法显著提升了自主代理在动态人群环境中的导航能力，实现了个人空间合规。

Abstract: In this report, we describe the technical details of our submission to the
IROS 2025 RoboSense Challenge Social Navigation Track. This track focuses on
developing RGBD-based perception and navigation systems that enable autonomous
agents to navigate safely, efficiently, and socially compliantly in dynamic
human-populated indoor environments. The challenge requires agents to operate
from an egocentric perspective using only onboard sensors including RGB-D
observations and odometry, without access to global maps or privileged
information, while maintaining social norm compliance such as safe distances
and collision avoidance. Building upon the Falcon model, we introduce a
Proactive Risk Perception Module to enhance social navigation performance. Our
approach augments Falcon with collision risk understanding that learns to
predict distance-based collision risk scores for surrounding humans, which
enables the agent to develop more robust spatial awareness and proactive
collision avoidance behaviors. The evaluation on the Social-HM3D benchmark
demonstrates that our method improves the agent's ability to maintain personal
space compliance while navigating toward goals in crowded indoor scenes with
dynamic human agents, achieving 2nd place among 16 participating teams in the
challenge.

</details>


### [33] [Towards Proprioception-Aware Embodied Planning for Dual-Arm Humanoid Robots](https://arxiv.org/abs/2510.07882)
*Boyu Li,Siyuan He,Hang Xu,Haoqi Yuan,Yu Zang,Liwei Hu,Junpeng Yue,Zhenxiong Jiang,Pengbo Hu,Börje F. Karlsson,Yehui Tang,Zongqing Lu*

Main category: cs.RO

TL;DR: 研究提出了DualTHOR仿真平台和Proprio-MLLM模型，有效增强了人形机器人在执行复杂任务中的规划能力。


<details>
  <summary>Details</summary>
Motivation: 针对当前多模态大语言模型在双臂人形机器人长时间任务执行中的局限性，我们旨在提高仿真评价和数据收集的能力，同时增强模型对身体位置的感知。

Method: 通过创建DualTHOR，一个新型的双臂人形机器人仿真平台，并在此基础上提出Proprio-MLLM模型，结合本体感知信息以增强身体意识。

Result: Proprio-MLLM在双臂人形机器人环境中实现了平均19.75%的规划性能提升，而现有的多模态大语言模型在此环境中表现不佳。

Conclusion: 本研究提出了DualTHOR仿真平台和Proprio-MLLM模型，有效提升了人形机器人在复杂任务中的规划性能。

Abstract: In recent years, Multimodal Large Language Models (MLLMs) have demonstrated
the ability to serve as high-level planners, enabling robots to follow complex
human instructions. However, their effectiveness, especially in long-horizon
tasks involving dual-arm humanoid robots, remains limited. This limitation
arises from two main challenges: (i) the absence of simulation platforms that
systematically support task evaluation and data collection for humanoid robots,
and (ii) the insufficient embodiment awareness of current MLLMs, which hinders
reasoning about dual-arm selection logic and body positions during planning. To
address these issues, we present DualTHOR, a new dual-arm humanoid simulator,
with continuous transition and a contingency mechanism. Building on this
platform, we propose Proprio-MLLM, a model that enhances embodiment awareness
by incorporating proprioceptive information with motion-based position
embedding and a cross-spatial encoder. Experiments show that, while existing
MLLMs struggle in this environment, Proprio-MLLM achieves an average
improvement of 19.75% in planning performance. Our work provides both an
essential simulation platform and an effective model to advance embodied
intelligence in humanoid robotics. The code is available at
https://anonymous.4open.science/r/DualTHOR-5F3B.

</details>


### [34] [Executable Analytic Concepts as the Missing Link Between VLM Insight and Precise Manipulation](https://arxiv.org/abs/2510.07975)
*Mingyang Sun,Jiude Wei,Qichen He,Donglin Wang,Cewu Lu,Jianhua Sun*

Main category: cs.RO

TL;DR: GRACE是一个新框架，利用可执行的解析概念将视觉-语言模型与机器人操作连接起来，增强了机器人在复杂环境中的操作能力。


<details>
  <summary>Details</summary>
Motivation: 迫切需要一种方法来解决机器人高层次理解与物理操作之间的差距，以便在复杂环境中进行有效的操作。

Method: GRACE集成了一个结构化的策略搭建管道，通过自然语言指令和视觉信息生成可执行的解析概念，以此推导抓取姿势、施力方向和运动轨迹。

Result: GRACE框架通过可执行的解析概念(EAC)弥合了语义理解与实际执行之间的差距，使机器人能够在非结构化环境中实现精确和通用的操作。

Conclusion: GRACE实现了高层次指令理解与低层次机器人控制之间的统一接口，展现了在多个物体上的零-shot泛化能力。

Abstract: Enabling robots to perform precise and generalized manipulation in
unstructured environments remains a fundamental challenge in embodied AI. While
Vision-Language Models (VLMs) have demonstrated remarkable capabilities in
semantic reasoning and task planning, a significant gap persists between their
high-level understanding and the precise physical execution required for
real-world manipulation. To bridge this "semantic-to-physical" gap, we
introduce GRACE, a novel framework that grounds VLM-based reasoning through
executable analytic concepts (EAC)-mathematically defined blueprints that
encode object affordances, geometric constraints, and semantics of
manipulation. Our approach integrates a structured policy scaffolding pipeline
that turn natural language instructions and visual information into an
instantiated EAC, from which we derive grasp poses, force directions and plan
physically feasible motion trajectory for robot execution. GRACE thus provides
a unified and interpretable interface between high-level instruction
understanding and low-level robot control, effectively enabling precise and
generalizable manipulation through semantic-physical grounding. Extensive
experiments demonstrate that GRACE achieves strong zero-shot generalization
across a variety of articulated objects in both simulated and real-world
environments, without requiring task-specific training.

</details>


### [35] [Orientation Learning and Adaptation towards Simultaneous Incorporation of Multiple Local Constraints](https://arxiv.org/abs/2510.07986)
*Gaofeng Li,Peisen Xu,Ruize Wang,Qi Ye,Jiming Chen,Dezhen Song,Yanlong Huang*

Main category: cs.RO

TL;DR: 研究提出了一种新颖的基于Angle-Axis表示法的方向表示方法，用于解决方向学习中的多个局部约束整合问题，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决非欧几里得几何性质带来的扭曲问题，以便在方向学习中有效整合多个局部约束。

Method: 采用Angle-Axis表示法，在SO(3)中提出加权平均机制，通过生成多条轨迹并融合以实现平滑轨迹。

Result: 已验证提出的方法可适应任意目标点方向，并应对角加速度约束，同时有效整合多个局部约束，降低加速度成本。

Conclusion: 提出的Angle-Axis Space基于方向表示方法可以有效解决方向学习中的扭曲问题，适用于多个局部约束的同时整合，并在实验中验证了其有效性。

Abstract: Orientation learning plays a pivotal role in many tasks. However, the
rotation group SO(3) is a Riemannian manifold. As a result, the distortion
caused by non-Euclidean geometric nature introduces difficulties to the
incorporation of local constraints, especially for the simultaneous
incorporation of multiple local constraints. To address this issue, we propose
the Angle-Axis Space-based orientation representation method to solve several
orientation learning problems, including orientation adaptation and
minimization of angular acceleration. Specifically, we propose a weighted
average mechanism in SO(3) based on the angle-axis representation method. Our
main idea is to generate multiple trajectories by considering different local
constraints at different basepoints. Then these multiple trajectories are fused
to generate a smooth trajectory by our proposed weighted average mechanism,
achieving the goal to incorporate multiple local constraints simultaneously.
Compared with existing solution, ours can address the distortion issue and make
the off-theshelf Euclidean learning algorithm be re-applicable in non-Euclidean
space. Simulation and Experimental evaluations validate that our solution can
not only adapt orientations towards arbitrary desired via-points and cope with
angular acceleration constraints, but also incorporate multiple local
constraints simultaneously to achieve extra benefits, e.g., achieving smaller
acceleration costs.

</details>


### [36] [FastUMI-100K: Advancing Data-driven Robotic Manipulation with a Large-scale UMI-style Dataset](https://arxiv.org/abs/2510.08022)
*Kehui Liu,Zhongjie Jia,Yang Li,Zhaxizhuoma,Pengan Chen,Song Liu,Xin Liu,Pingrui Zhang,Haoming Song,Xinyi Ye,Nieqing Cao,Zhigang Wang,Jia Zeng,Dong Wang,Yan Ding,Bin Zhao,Xuelong Li*

Main category: cs.RO

TL;DR: 本文提出了FastUMI-100K，一个大规模的多模态示范数据集，以解决现有数据集的局限性，满足复杂操作任务的需求。


<details>
  <summary>Details</summary>
Motivation: 现有的数据集在规模、平滑度和适用性方面有限，不足以满足复杂现实世界操作任务的需求。

Method: 通过FastUMI系统收集和整合多模态数据，构建FastUMI-100K数据集。

Result: FastUMI-100K包含10万条示范轨迹，覆盖54个任务和数百种物品类型，能够支持高策略成功率。

Conclusion: FastUMI-100K在高政策成功率和现实应用性方面展现出其强大的适应性和可靠性。

Abstract: Data-driven robotic manipulation learning depends on large-scale,
high-quality expert demonstration datasets. However, existing datasets, which
primarily rely on human teleoperated robot collection, are limited in terms of
scalability, trajectory smoothness, and applicability across different robotic
embodiments in real-world environments. In this paper, we present FastUMI-100K,
a large-scale UMI-style multimodal demonstration dataset, designed to overcome
these limitations and meet the growing complexity of real-world manipulation
tasks. Collected by FastUMI, a novel robotic system featuring a modular,
hardware-decoupled mechanical design and an integrated lightweight tracking
system, FastUMI-100K offers a more scalable, flexible, and adaptable solution
to fulfill the diverse requirements of real-world robot demonstration data.
Specifically, FastUMI-100K contains over 100K+ demonstration trajectories
collected across representative household environments, covering 54 tasks and
hundreds of object types. Our dataset integrates multimodal streams, including
end-effector states, multi-view wrist-mounted fisheye images and textual
annotations. Each trajectory has a length ranging from 120 to 500 frames.
Experimental results demonstrate that FastUMI-100K enables high policy success
rates across various baseline algorithms, confirming its robustness,
adaptability, and real-world applicability for solving complex, dynamic
manipulation challenges. The source code and dataset will be released in this
link https://github.com/MrKeee/FastUMI-100K.

</details>


### [37] [Towards Reliable LLM-based Robot Planning via Combined Uncertainty Estimation](https://arxiv.org/abs/2510.08044)
*Shiyuan Yin,Chenjia Bai,Zihao Zhang,Junwei Jin,Xinxin Zhang,Chi Zhang,Xuelong Li*

Main category: cs.RO

TL;DR: CURE方法通过将不确定性分解为外在和内在不确定性，提升了基于大型语言模型的规划的可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在规划中引发的幻觉问题，提高规划的安全性和一致性。

Method: 使用随机网络蒸馏和多层感知机回归头，通过大型语言模型特征进行不确定性评估。

Result: 提出了一种结合不确定性估计的方法CURE，旨在提高基于大型语言模型的机器人规划的可靠性。

Conclusion: CURE方法有效区分了外在和内在不确定性，并在两个实验中验证了其优越性。

Abstract: Large language models (LLMs) demonstrate advanced reasoning abilities,
enabling robots to understand natural language instructions and generate
high-level plans with appropriate grounding. However, LLM hallucinations
present a significant challenge, often leading to overconfident yet potentially
misaligned or unsafe plans. While researchers have explored uncertainty
estimation to improve the reliability of LLM-based planning, existing studies
have not sufficiently differentiated between epistemic and intrinsic
uncertainty, limiting the effectiveness of uncertainty estimation. In this
paper, we present Combined Uncertainty estimation for Reliable Embodied
planning (CURE), which decomposes the uncertainty into epistemic and intrinsic
uncertainty, each estimated separately. Furthermore, epistemic uncertainty is
subdivided into task clarity and task familiarity for more accurate evaluation.
The overall uncertainty assessments are obtained using random network
distillation and multi-layer perceptron regression heads driven by LLM
features. We validated our approach in two distinct experimental settings:
kitchen manipulation and tabletop rearrangement experiments. The results show
that, compared to existing methods, our approach yields uncertainty estimates
that are more closely aligned with the actual execution outcomes.

</details>


### [38] [Beyond hospital reach: Autonomous lightweight ultrasound robot for liver sonography](https://arxiv.org/abs/2510.08106)
*Zihan Li,Yixiao Xu,Lei Zhang,Taiyu Han,Xinshan Yang,Yingni Wang,Mingxuan Liu,Shenghai Xin,Linxun Liu,Hongen Liao,Guochen Ning*

Main category: cs.RO

TL;DR: 本研究开发了一种轻量级的自主超声机器人，能在复杂环境中有效进行肝脏超声检查，特别适合资源有限的地区。


<details>
  <summary>Details</summary>
Motivation: 肝脏疾病是全球主要的健康负担，现有的超声诊断工具依赖于专家，尤其在资源有限的地区专家稀缺。

Method: 该系统结合了多模态感知与记忆注意力，以定位不可见的目标结构，并通过一个588克的六自由度电缆驱动机器人来完成超声扫描。

Result: 开发了一种自主轻量化超声机器人，能够在各种复杂场景中进行肝脏超声扫描，提升了诊断的可及性。

Conclusion: 这一工作展示了在多个具有挑战性的环境中进行自主超声检查的可能性，能显著改善欠发达地区的医疗诊断可及性。

Abstract: Liver disease is a major global health burden. While ultrasound is the
first-line diagnostic tool, liver sonography requires locating multiple
non-continuous planes from positions where target structures are often not
visible, for biometric assessment and lesion detection, requiring significant
expertise. However, expert sonographers are severely scarce in resource-limited
regions. Here, we develop an autonomous lightweight ultrasound robot comprising
an AI agent that integrates multi-modal perception with memory attention for
localization of unseen target structures, and a 588-gram 6-degrees-of-freedom
cable-driven robot. By mounting on the abdomen, the system enhances robustness
against motion. Our robot can autonomously acquire expert-level standard liver
ultrasound planes and detect pathology in patients, including two from Xining,
a 2261-meter-altitude city with limited medical resources. Our system performs
effectively on rapid-motion individuals and in wilderness environments. This
work represents the first demonstration of autonomous sonography across
multiple challenging scenarios, potentially transforming access to expert-level
diagnostics in underserved regions.

</details>


### [39] [Accurate and Noise-Tolerant Extraction of Routine Logs in Robotic Process Automation (Extended Version)](https://arxiv.org/abs/2510.08118)
*Massimiliano de Leoni,Faizan Ahmed Khan,Simone Agostinelli*

Main category: cs.RO

TL;DR: 本文提出了一种新技术，通过聚类分析提取人类操作的例行日志，特别是在存在执行噪音的情况下表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于提取行动集合，而非直接支持模型发现，尤其缺乏对执行不一致情境的评估。

Method: 基于聚类的技术，针对不同水平噪音注入的用户界面日志进行实验比较。

Result: 提出了一种基于聚类的技术来提取例行日志，能够在面对噪音时提取更准确的日志。

Conclusion: 新技术在噪音影响下依然能有效提取高质量例行日志，优于现有方法。

Abstract: Robotic Process Mining focuses on the identification of the routine types
performed by human resources through a User Interface. The ultimate goal is to
discover routine-type models to enable robotic process automation. The
discovery of routine-type models requires the provision of a routine log.
Unfortunately, the vast majority of existing works do not directly focus on
enabling the model discovery, limiting themselves to extracting the set of
actions that are part of the routines. They were also not evaluated in
scenarios characterized by inconsistent routine execution, hereafter referred
to as noise, which reflects natural variability and occasional errors in human
performance. This paper presents a clustering-based technique that aims to
extract routine logs. Experiments were conducted on nine UI logs from the
literature with different levels of injected noise. Our technique was compared
with existing techniques, most of which are not meant to discover routine logs
but were adapted for the purpose. The results were evaluated through standard
state-of-the-art metrics, showing that we can extract more accurate routine
logs than what the state of the art could, especially in the presence of noise.

</details>


### [40] [NavSpace: How Navigation Agents Follow Spatial Intelligence Instructions](https://arxiv.org/abs/2510.08173)
*Haolin Yang,Yuxing Long,Zhuoyuan Yu,Zihan Yang,Minghan Wang,Jiapeng Xu,Yihan Wang,Ziyan Yu,Wenzhe Cai,Lei Kang,Hao Dong*

Main category: cs.RO

TL;DR: 该研究推出NavSpace基准，以系统评估导航代理的空间智能，并提出新的导航模型SNav，表现优于现有代理。


<details>
  <summary>Details</summary>
Motivation: 指导性导航是实现具身智能的关键步骤，但现有基准主要关注语义理解，未系统评估导航代理的空间感知和推理能力。

Method: 引入NavSpace基准，包含六个任务类别和1,228个轨迹-指令对，对22个导航代理进行评估。

Result: NavSpace基准的引入及其对22个导航代理的全面评估，揭示了具身导航中的空间智能。

Conclusion: SNav在NavSpace和真实机器人测试中超越现有导航代理，为未来研究奠定了强基线。

Abstract: Instruction-following navigation is a key step toward embodied intelligence.
Prior benchmarks mainly focus on semantic understanding but overlook
systematically evaluating navigation agents' spatial perception and reasoning
capabilities. In this work, we introduce the NavSpace benchmark, which contains
six task categories and 1,228 trajectory-instruction pairs designed to probe
the spatial intelligence of navigation agents. On this benchmark, we
comprehensively evaluate 22 navigation agents, including state-of-the-art
navigation models and multimodal large language models. The evaluation results
lift the veil on spatial intelligence in embodied navigation. Furthermore, we
propose SNav, a new spatially intelligent navigation model. SNav outperforms
existing navigation agents on NavSpace and real robot tests, establishing a
strong baseline for future work.

</details>


### [41] [Evaluation of a Robust Control System in Real-World Cable-Driven Parallel Robots](https://arxiv.org/abs/2510.08270)
*Damir Nurtdinov,Aliaksei Korshuk,Alexei Kornaev,Alexander Maloletov*

Main category: cs.RO

TL;DR: 研究表明，TRPO在控制电缆驱动并行机器人方面效果最优，适合动态和噪声环境，减少对高频传感器反馈的依赖。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，电缆驱动并行机器人（CDPR）控制面临时间离散限制和系统欠约束的问题，因此需要评估不同控制方法的性能。

Method: 比较分析经典PID控制器与现代强化学习算法（如DDPG、PPO和TRPO），以评估其在电缆驱动并行机器人控制中的表现。

Result: TRPO方法在多个轨迹上表现出色，具有最低的均方根（RMS）误差，并且具备较强的鲁棒性。

Conclusion: TRPO作为复杂机器人控制任务的强大解决方案，有望在传感器融合和混合控制策略中发挥重要作用。

Abstract: This study evaluates the performance of classical and modern control methods
for real-world Cable-Driven Parallel Robots (CDPRs), focusing on
underconstrained systems with limited time discretization. A comparative
analysis is conducted between classical PID controllers and modern
reinforcement learning algorithms, including Deep Deterministic Policy Gradient
(DDPG), Proximal Policy Optimization (PPO), and Trust Region Policy
Optimization (TRPO). The results demonstrate that TRPO outperforms other
methods, achieving the lowest root mean square (RMS) errors across various
trajectories and exhibiting robustness to larger time intervals between control
updates. TRPO's ability to balance exploration and exploitation enables stable
control in noisy, real-world environments, reducing reliance on high-frequency
sensor feedback and computational demands. These findings highlight TRPO's
potential as a robust solution for complex robotic control tasks, with
implications for dynamic environments and future applications in sensor fusion
or hybrid control strategies.

</details>


### [42] [Airy: Reading Robot Intent through Height and Sky](https://arxiv.org/abs/2510.08381)
*Baoyang Chen,Xian Xu,Huamin Qu*

Main category: cs.RO

TL;DR: 本作品通过机器人竞争展示其意图，使观众能直观理解AI的行为和状态。


<details>
  <summary>Details</summary>
Motivation: 工业机器人进入人类共享空间，导致其决策过程不透明，威胁到安全、信任和公众监督。

Method: 作品基于三个设计原则：竞争作为明确指标、具身熟悉感和传感器映射，展示机器人之间的互动。

Result: 该艺术作品通过两个强化训练的机器人手臂之间的竞争，提供了一种直观理解复杂多智能体AI的方法。

Conclusion: 该项目表明，感官隐喻可以将黑箱变成公众接口，从而增强人们对机器人行为的理解。

Abstract: As industrial robots move into shared human spaces, their opaque decision
making threatens safety, trust, and public oversight. This artwork, Airy, asks
whether complex multi agent AI can become intuitively understandable by staging
a competition between two reinforcement trained robot arms that snap a bedsheet
skyward. Building on three design principles, competition as a clear metric
(who lifts higher), embodied familiarity (audiences recognize fabric snapping),
and sensor to sense mapping (robot cooperation or rivalry shown through forest
and weather projections), the installation gives viewers a visceral way to read
machine intent. Observations from five international exhibitions indicate that
audiences consistently read the robots' strategies, conflict, and cooperation
in real time, with emotional reactions that mirror the system's internal state.
The project shows how sensory metaphors can turn a black box into a public
interface.

</details>


### [43] [Reliability of Single-Level Equality-Constrained Inverse Optimal Control](https://arxiv.org/abs/2510.08406)
*Filip Bečanović,Kosta Jovanović,Vincent Bonnet*

Main category: cs.RO

TL;DR: 本文提出了一种新的单级逆最优控制方法，具有更快的计算速度和更强的噪声鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 逆最优控制（IOC）用于从人类运动中提取最优成本函数权重，但现有方法速度慢且对噪声敏感。

Method: 采用单级重构的逆最优控制方法，分析噪声对结果的影响。

Result: 实验结果显示，该方法在极大噪声下仍然有效，并将计算时间比传统双层方法减少了15倍。

Conclusion: 所提出的方法在处理大噪声时表现出强韧性，并显著降低了计算时间。

Abstract: Inverse optimal control (IOC) allows the retrieval of optimal cost function
weights, or behavioral parameters, from human motion. The literature on IOC
uses methods that are either based on a slow bilevel process or a fast but
noise-sensitive minimization of optimality condition violation. Assuming
equality-constrained optimal control models of human motion, this article
presents a faster but robust approach to solving IOC using a single-level
reformulation of the bilevel method and yields equivalent results. Through
numerical experiments in simulation, we analyze the robustness to noise of the
proposed single-level reformulation to the bilevel IOC formulation with a
human-like planar reaching task that is used across recent studies. The
approach shows resilience to very large levels of noise and reduces the
computation time of the IOC on this task by a factor of 15 when compared to a
classical bilevel implementation.

</details>


### [44] [Validation of collision-free spheres of Stewart-Gough platforms for constant orientations using the Application Programming Interface of a CAD software](https://arxiv.org/abs/2510.08408)
*Bibekananda Patra,Rajeevlochana G. Chittawadigi,Sandipan Bandyopadhyay*

Main category: cs.RO

TL;DR: 提出了一种通过CAD软件API验证6-6斯图尔特-高夫平台最大无碰撞球体大小的方法，从而确保机械手的安全性。


<details>
  <summary>Details</summary>
Motivation: 确保在特定姿态下，机械手的运动安全性，避免关节之间的碰撞。

Method: 使用 CAD 软件的 API 自动更新6-6斯图尔特-高夫平台机械手（SGPM）运动平台（MP）的位置，验证其最大无碰撞球体的大小。

Result: 在每个动作中，调查每对腿之间的互相碰撞，从而验证无碰撞球体的安全性。

Conclusion: 该方法不仅能验证预先计算的无碰撞球体的安全性，还可估算任何空间并行机械手的安全性。

Abstract: This paper presents a method of validation of the size of the largest
collision-free sphere (CFS) of a 6-6 Stewart-Gough platform manipulator (SGPM)
for a given orientation of its moving platform (MP) using the Application
Programming Interface (API) of a CAD software. The position of the MP is
updated via the API in an automated manner over a set of samples within a shell
enclosing the surface of the CFS. For each pose of the manipulator, each pair
of legs is investigated for mutual collisions. The CFS is considered safe or
validated iff none of the points falling inside the CFS lead to a collision
between any pair of legs. This approach can not only validate the safety of a
precomputed CFS, but also estimate the same for any spatial parallel
manipulator.

</details>


### [45] [Don't Run with Scissors: Pruning Breaks VLA Models but They Can Be Recovered](https://arxiv.org/abs/2510.08464)
*Jason Jabbour,Dong-Ki Kim,Max Smith,Jay Patrikar,Radhika Ghosal,Youhui Wang,Ali Agha,Vijay Janapa Reddi,Shayegan Omidshafiei*

Main category: cs.RO

TL;DR: 研究提出了GLUESTICK后修剪恢复方法，通过一种简单的插值技术，在不额外训练的情况下，使得VLA模型在修剪后能够恢复功能。


<details>
  <summary>Details</summary>
Motivation: VLA模型在资源有限的硬件上部署存在挑战，尽管修剪可以有效压缩大型语言模型，但在机器人领域的研究较少。

Method: 提出GLUESTICK，一种后修剪恢复方法，通过在权重空间中进行稀疏模型和稠密模型之间的插值，计算校正项以恢复功能。

Result: GLUESTICK在各类VLA架构和操作任务中实现了竞争性的内存效率，同时大幅恢复成功率，减少安全违规。

Conclusion: GLUESTICK方法无需额外训练，对修剪算法无偏见，只引入一个超参数来平衡效率与准确性，能有效提升修剪后VLA模型的功能。

Abstract: Vision-Language-Action (VLA) models have advanced robotic capabilities but
remain challenging to deploy on resource-limited hardware. Pruning has enabled
efficient compression of large language models (LLMs), yet it is largely
understudied in robotics. Surprisingly, we observe that pruning VLA models
leads to drastic degradation and increased safety violations. We introduce
GLUESTICK, a post-pruning recovery method that restores much of the original
model's functionality while retaining sparsity benefits. Our method performs a
one-time interpolation between the dense and pruned models in weight-space to
compute a corrective term. This correction is used during inference by each
pruned layer to recover lost capabilities with minimal overhead. GLUESTICK
requires no additional training, is agnostic to the pruning algorithm, and
introduces a single hyperparameter that controls the tradeoff between
efficiency and accuracy. Across diverse VLA architectures and tasks in
manipulation and navigation, GLUESTICK achieves competitive memory efficiency
while substantially recovering success rates and reducing safety violations.
Additional material can be found at: https://gluestick-vla.github.io/.

</details>


### [46] [DexMan: Learning Bimanual Dexterous Manipulation from Human and Generated Videos](https://arxiv.org/abs/2510.08475)
*Jhen Hsieh,Kuan-Hsun Tu,Kuo-Han Hung,Tsung-Wei Ke*

Main category: cs.RO

TL;DR: DexMan是一个新的框架，可以将人类视觉演示转换为机器人操作技能，不需要复杂的数据收集，表现优越。


<details>
  <summary>Details</summary>
Motivation: 旨在消除对相机标定、深度传感器和昂贵运动捕捉的依赖，以简化和提升双手灵巧操作技能的生成过程。

Method: DexMan是一个自动化框架，通过第三方人类操作视频，直接控制仿人机器人，并使用新颖的基于接触的奖励来提高政策学习。

Result: 在TACO基准上，DexMan在物体姿态估计上达到最先进的性能，同时在OakInk-v2上，其强化学习策略成功率提升了19%。

Conclusion: DexMan通过利用人类演示视频自动化生成机械手需要的双手灵巧操作技能，表现出色且无需复杂的数据准备。

Abstract: We present DexMan, an automated framework that converts human visual
demonstrations into bimanual dexterous manipulation skills for humanoid robots
in simulation. Operating directly on third-person videos of humans manipulating
rigid objects, DexMan eliminates the need for camera calibration, depth
sensors, scanned 3D object assets, or ground-truth hand and object motion
annotations. Unlike prior approaches that consider only simplified floating
hands, it directly controls a humanoid robot and leverages novel contact-based
rewards to improve policy learning from noisy hand-object poses estimated from
in-the-wild videos.
  DexMan achieves state-of-the-art performance in object pose estimation on the
TACO benchmark, with absolute gains of 0.08 and 0.12 in ADD-S and VSD.
Meanwhile, its reinforcement learning policy surpasses previous methods by 19%
in success rate on OakInk-v2. Furthermore, DexMan can generate skills from both
real and synthetic videos, without the need for manual data collection and
costly motion capture, and enabling the creation of large-scale, diverse
datasets for training generalist dexterous manipulation.

</details>


### [47] [R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation](https://arxiv.org/abs/2510.08547)
*Xiuwei Xu,Angyuan Ma,Hankun Li,Bingyao Yu,Zheng Zhu,Jie Zhou,Jiwen Lu*

Main category: cs.RO

TL;DR: 本研究提出了一种新的3D数据生成方法，旨在解决机器人操控中的空间普遍化问题，通过实测增强数据效率，促进移动操作的应用。


<details>
  <summary>Details</summary>
Motivation: 针对机器人操作的普遍化需求，需要收集大量人类示范以覆盖不同的空间配置，从而训练通用的视觉运动策略。

Method: 提出了一种实测到实测的3D数据生成框架R2RGen，采用了群体增强策略和基于摄像机的处理。

Result: R2RGen通过直接增强点云观察-动作对，生成真实世界数据，解决了过去方法中存在的sim-to-real差距和限制。

Conclusion: R2RGen显著提升了数据效率，并展示了在移动操作中的广泛应用潜力。

Abstract: Towards the aim of generalized robotic manipulation, spatial generalization
is the most fundamental capability that requires the policy to work robustly
under different spatial distribution of objects, environment and agent itself.
To achieve this, substantial human demonstrations need to be collected to cover
different spatial configurations for training a generalized visuomotor policy
via imitation learning. Prior works explore a promising direction that
leverages data generation to acquire abundant spatially diverse data from
minimal source demonstrations. However, most approaches face significant
sim-to-real gap and are often limited to constrained settings, such as
fixed-base scenarios and predefined camera viewpoints. In this paper, we
propose a real-to-real 3D data generation framework (R2RGen) that directly
augments the pointcloud observation-action pairs to generate real-world data.
R2RGen is simulator- and rendering-free, thus being efficient and
plug-and-play. Specifically, given a single source demonstration, we introduce
an annotation mechanism for fine-grained parsing of scene and trajectory. A
group-wise augmentation strategy is proposed to handle complex multi-object
compositions and diverse task constraints. We further present camera-aware
processing to align the distribution of generated data with real-world 3D
sensor. Empirically, R2RGen substantially enhances data efficiency on extensive
experiments and demonstrates strong potential for scaling and application on
mobile manipulation.

</details>


### [48] [DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via Joint-Wise Neural Dynamics Model](https://arxiv.org/abs/2510.08556)
*Xueyi Liu,He Wang,Li Yi*

Main category: cs.RO

TL;DR: 本论文提出了一种通过关节动态模型解决机器人手中物体旋转在真实与模拟间转移的挑战，实现了单一策略在多样对象及条件下的通用性，展示了在复杂任务中的有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决当前机器人手中物体旋转的现实与模拟之间的差距，以实现更广泛的应用场景和提高机器人操作的灵活性与效率。

Method: 基于关节动态模型的框架，通过有效利用少量真实世界数据改进模拟策略，并实现自主管理的数据收集。

Result: 本研究提出了一种新框架，旨在解决机器人手中物体旋转的通用性问题。该框架可以让在模拟中训练的单一策略有效转移到真实环境中，成功处理复杂形状、高长宽比和小尺寸的物体，同时适应多种手腕姿态和旋转轴。核心方法是一个关节动态模型，通过少量真实世界数据适应模拟策略的动作，展现出卓越的通用性和数据效率。

Conclusion: 此研究成功地展示了在多样复杂物体旋转任务中，利用单一模拟策略在真实环境中的有效应用，同时通过高效数据收集策略提升了模型的通用性和适应性。

Abstract: Achieving generalized in-hand object rotation remains a significant challenge
in robotics, largely due to the difficulty of transferring policies from
simulation to the real world. The complex, contact-rich dynamics of dexterous
manipulation create a "reality gap" that has limited prior work to constrained
scenarios involving simple geometries, limited object sizes and aspect ratios,
constrained wrist poses, or customized hands. We address this sim-to-real
challenge with a novel framework that enables a single policy, trained in
simulation, to generalize to a wide variety of objects and conditions in the
real world. The core of our method is a joint-wise dynamics model that learns
to bridge the reality gap by effectively fitting limited amount of real-world
collected data and then adapting the sim policy's actions accordingly. The
model is highly data-efficient and generalizable across different whole-hand
interaction distributions by factorizing dynamics across joints, compressing
system-wide influences into low-dimensional variables, and learning each
joint's evolution from its own dynamic profile, implicitly capturing these net
effects. We pair this with a fully autonomous data collection strategy that
gathers diverse, real-world interaction data with minimal human intervention.
Our complete pipeline demonstrates unprecedented generality: a single policy
successfully rotates challenging objects with complex shapes (e.g., animals),
high aspect ratios (up to 5.33), and small sizes, all while handling diverse
wrist orientations and rotation axes. Comprehensive real-world evaluations and
a teleoperation application for complex tasks validate the effectiveness and
robustness of our approach. Website: https://meowuu7.github.io/DexNDM/

</details>


### [49] [NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos](https://arxiv.org/abs/2510.08568)
*Hongyu Li,Lingfeng Sun,Yafei Hu,Duy Ta,Jennifer Barry,George Konidaris,Jiahui Fu*

Main category: cs.RO

TL;DR: NovaFlow是一个无需演示的自主操纵框架，能够将任务描述转化为行动计划，并在不同机器人平台上有效转移。


<details>
  <summary>Details</summary>
Motivation: 实现机器人零-shot执行新型操纵任务，克服现有方法的局限性。

Method: NovaFlow框架将任务描述转换为目标机器人的可操作计划，无需任何演示。

Result: 在刚性、关节和可变形物体操纵任务中验证了NovaFlow，成功实现有效的零-shot执行。

Conclusion: 通过将任务理解与低级控制解耦，NovaFlow实现了跨平台的自然迁移。

Abstract: Enabling robots to execute novel manipulation tasks zero-shot is a central
goal in robotics. Most existing methods assume in-distribution tasks or rely on
fine-tuning with embodiment-matched data, limiting transfer across platforms.
We present NovaFlow, an autonomous manipulation framework that converts a task
description into an actionable plan for a target robot without any
demonstrations. Given a task description, NovaFlow synthesizes a video using a
video generation model and distills it into 3D actionable object flow using
off-the-shelf perception modules. From the object flow, it computes relative
poses for rigid objects and realizes them as robot actions via grasp proposals
and trajectory optimization. For deformable objects, this flow serves as a
tracking objective for model-based planning with a particle-based dynamics
model. By decoupling task understanding from low-level control, NovaFlow
naturally transfers across embodiments. We validate on rigid, articulated, and
deformable object manipulation tasks using a table-top Franka arm and a Spot
quadrupedal mobile robot, and achieve effective zero-shot execution without
demonstrations or embodiment-specific training. Project website:
https://novaflow.lhy.xyz/.

</details>


### [50] [Scalable Offline Metrics for Autonomous Driving](https://arxiv.org/abs/2510.08571)
*Animikh Aich,Adwait Kulkarni,Eshed Ohn-Bar*

Main category: cs.RO

TL;DR: 本研究分析了感知基础规划模型在离线与在线评估中的差异，并提出了一种基于认识不确定性的离线指标，显著提高了与在线性能的相关性。


<details>
  <summary>Details</summary>
Motivation: 在机器人系统（如自动驾驶车辆）中，离线评估感知基础的规划模型非常可行和经济，但将离线性能推断到在线环境中却面临挑战，特别是在复杂城市环境下的闭环度量和策略评估是不够充分的。

Method: 通过一系列模拟实验，分析离线与在线设置的反馈，并通过新的离线指标评估策略的性能，验证了其在真实世界中的有效性。

Result: 基于实验分析，发现离线与在线设置之间的相关性比先前研究所报告的要差，这对当前的评估实践和驾驶策略的有效性提出了疑问。同时，基于认识不确定性的新离线指标提高了相关性，具有超过13%的改进。

Conclusion: 我们的发现表明，当前的驾驶政策评估实践存在缺陷，新提出的指标在真实环境中表现更佳，强调了更精确评估的重要性。

Abstract: Real-World evaluation of perception-based planning models for robotic
systems, such as autonomous vehicles, can be safely and inexpensively conducted
offline, i.e., by computing model prediction error over a pre-collected
validation dataset with ground-truth annotations. However, extrapolating from
offline model performance to online settings remains a challenge. In these
settings, seemingly minor errors can compound and result in test-time
infractions or collisions. This relationship is understudied, particularly
across diverse closed-loop metrics and complex urban maneuvers. In this work,
we revisit this undervalued question in policy evaluation through an extensive
set of experiments across diverse conditions and metrics. Based on analysis in
simulation, we find an even worse correlation between offline and online
settings than reported by prior studies, casting doubts on the validity of
current evaluation practices and metrics for driving policies. Next, we bridge
the gap between offline and online evaluation. We investigate an offline metric
based on epistemic uncertainty, which aims to capture events that are likely to
cause errors in closed-loop settings. The resulting metric achieves over 13%
improvement in correlation compared to previous offline metrics. We further
validate the generalization of our findings beyond the simulation environment
in real-world settings, where even greater gains are observed.

</details>


### [51] [BLAZER: Bootstrapping LLM-based Manipulation Agents with Zero-Shot Data Generation](https://arxiv.org/abs/2510.08572)
*Rocktim Jyoti Das,Harsh Singh,Diana Turmakhan,Muhammad Abdullah Sohail,Mingfei Han,Preslav Nakov,Fabio Pizzati,Ivan Laptev*

Main category: cs.RO

TL;DR: BLAZER框架通过自动生成训练数据，利用LLM的零-shot能力，提高了机器人操控的有效性，且可以在不依赖人工的情况下进行模型细调。


<details>
  <summary>Details</summary>
Motivation: 机器人领域目前缺乏互联网规模的多样化演示数据，现有数据集通常需要人工收集和整理，因此需要一种新方法来提升数据规模以训练更强大的操控模型。

Method: 通过自动生成的训练数据，利用LLM规划器的零-shot能力来学习操控策略，并在这些策略成功示例的基础上细调LLM。

Result: BLAZER通过在模拟中自动生成演示，能有效提高零-shot操控能力，并成功将获得的技能转移到基于传感器的操作中。

Conclusion: BLAZER显著提升了在模拟和真实环境中的零-shot操作能力，并能在训练池外的任务上表现出色，同时支持LLM模型的下缩。

Abstract: Scaling data and models has played a pivotal role in the remarkable progress
of computer vision and language. Inspired by these domains, recent efforts in
robotics have similarly focused on scaling both data and model size to develop
more generalizable and robust policies. However, unlike vision and language,
robotics lacks access to internet-scale demonstrations across diverse robotic
tasks and environments. As a result, the scale of existing datasets typically
suffers from the need for manual data collection and curation. To address this
problem, here we propose BLAZER, a framework that learns manipulation policies
from automatically generated training data. We build on the zero-shot
capabilities of LLM planners and automatically generate demonstrations for
diverse manipulation tasks in simulation. Successful examples are then used to
finetune an LLM and to improve its planning capabilities without human
supervision. Notably, while BLAZER training requires access to the simulator's
state, we demonstrate direct transfer of acquired skills to sensor-based
manipulation. Through extensive experiments, we show BLAZER to significantly
improve zero-shot manipulation in both simulated and real environments.
Moreover, BLAZER improves on tasks outside of its training pool and enables
downscaling of LLM models. Our code and data will be made publicly available on
the project page.

</details>
