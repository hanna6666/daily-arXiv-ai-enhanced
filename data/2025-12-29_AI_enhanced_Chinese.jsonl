{"id": "2512.21551", "categories": ["cs.HC", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21551", "abs": "https://arxiv.org/abs/2512.21551", "authors": ["Hua Shen", "Tiffany Knearem", "Divy Thakkar", "Pat Pataranutaporn", "Anoop Sinha", "Yike", "Shi", "Jenny T. Liang", "Lama Ahmad", "Tanu Mitra", "Brad A. Myers", "Yang Li"], "title": "Human-AI Interaction Alignment: Designing, Evaluating, and Evolving Value-Centered AI For Reciprocal Human-AI Futures", "comment": "CHI 2026 BiAlign Workshop", "summary": "The rapid integration of generative AI into everyday life underscores the need to move beyond unidirectional alignment models that only adapt AI to human values. This workshop focuses on bidirectional human-AI alignment, a dynamic, reciprocal process where humans and AI co-adapt through interaction, evaluation, and value-centered design. Building on our past CHI 2025 BiAlign SIG and ICLR 2025 Workshop, this workshop will bring together interdisciplinary researchers from HCI, AI, social sciences and more domains to advance value-centered AI and reciprocal human-AI collaboration. We focus on embedding human and societal values into alignment research, emphasizing not only steering AI toward human values but also enabling humans to critically engage with and evolve alongside AI systems. Through talks, interdisciplinary discussions, and collaborative activities, participants will explore methods for interactive alignment, frameworks for societal impact evaluation, and strategies for alignment in dynamic contexts. This workshop aims to bridge the disciplines' gaps and establish a shared agenda for responsible, reciprocal human-AI futures.", "AI": {"tldr": "\u672c\u7814\u8ba8\u4f1a\u5173\u6ce8\u4eba\u7c7b\u4e0eAI\u7684\u53cc\u5411\u5bf9\u9f50\uff0c\u9f13\u52b1\u8de8\u5b66\u79d1\u5408\u4f5c\u4ee5\u6784\u5efa\u8d1f\u8d23\u4efb\u7684AI\u4e0e\u4eba\u7c7b\u5408\u4f5c\u672a\u6765\u3002", "motivation": "\u5728\u751f\u6210AI\u8fc5\u901f\u878d\u5165\u65e5\u5e38\u751f\u6d3b\u7684\u80cc\u666f\u4e0b\uff0c\u4e9f\u9700\u8d85\u8d8a\u5355\u5411\u5bf9\u9f50\u6a21\u578b\uff0c\u91cd\u89c6\u53cc\u5411\u4eba\u7c7b\u4e0eAI\u7684\u4e92\u52a8\u3002", "method": "\u901a\u8fc7\u6f14\u8bb2\u3001\u8de8\u5b66\u79d1\u8ba8\u8bba\u548c\u5408\u4f5c\u6d3b\u52a8\uff0c\u53c2\u4e0e\u8005\u5c06\u5171\u540c\u63a2\u8ba8\u76f8\u5173\u65b9\u6cd5\u4e0e\u7b56\u7565\u3002", "result": "\u8be5\u7814\u8ba8\u4f1a\u5f3a\u8c03\u4e86\u4eba\u7c7b\u4e0eAI\u4e4b\u95f4\u7684\u53cc\u5411\u5bf9\u9f50\uff0c\u65e8\u5728\u4fc3\u8fdb\u4eba\u7c7b\u4e0eAI\u7684\u5171\u540c\u9002\u5e94\u3002", "conclusion": "\u901a\u8fc7\u8de8\u5b66\u79d1\u8ba8\u8bba\u4e0e\u5408\u4f5c\u6d3b\u52a8\uff0c\u53c2\u4e0e\u8005\u5c06\u63a2\u7d22\u4e92\u52a8\u5bf9\u9f50\u7684\u65b9\u6cd5\u3001\u793e\u4f1a\u5f71\u54cd\u8bc4\u4f30\u6846\u67b6\u548c\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5bf9\u9f50\u7b56\u7565\u3002"}}
{"id": "2512.21589", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.21589", "abs": "https://arxiv.org/abs/2512.21589", "authors": ["Masaaki Yamauchi", "Yiyuan Liang", "Hiroko Hara", "Hideyuki Shimonishi", "Masayuki Murata"], "title": "Emotion-Aware Smart Home Automation Based on the eBICA Model", "comment": "Accepted at IEEE ICCE 2026", "summary": "Smart home automation that adapts to a user's emotional state can enhance psychological safety in daily living environments. This study proposes an emotion-aware automation framework guided by the emotional Biologically Inspired Cognitive Architecture (eBICA), which integrates appraisal, somatic responses, and behavior selection. We conducted a proof-of-concept experiment in a pseudo-smart-home environment, where participants were exposed to an anxiety-inducing event followed by a comfort-inducing automation. State anxiety (STAI-S) was measured throughout the task sequence. The results showed a significant reduction in STAI-S immediately after introducing the avoidance automation, demonstrating that emotion-based control can effectively promote psychological safety. Furthermore, an analysis of individual characteristics suggested that personality and anxiety-related traits modulate the degree of relief, indicating the potential for personalized emotion-adaptive automation. Overall, this study provides empirical evidence that eBICA-based emotional control can function effectively in smart home environments and offers a foundation for next-generation affective home automation systems.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u60c5\u611f\u611f\u77e5\u7684\u667a\u80fd\u5bb6\u5c45\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u5e76\u8bc1\u5b9e\u5176\u80fd\u591f\u6709\u6548\u51cf\u5c11\u7528\u6237\u7684\u7126\u8651\uff0c\u63d0\u9ad8\u5fc3\u7406\u5b89\u5168\u611f\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u5bb6\u5c45\u6280\u672f\u7684\u53d1\u5c55\uff0c\u5982\u4f55\u4f7f\u8fd9\u4e9b\u7cfb\u7edf\u66f4\u52a0\u4eba\u6027\u5316\u548c\u80fd\u591f\u7406\u89e3\u7528\u6237\u60c5\u611f\u72b6\u6001\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u60c5\u611f\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u6765\u63d0\u5347\u7528\u6237\u5728\u5bb6\u5ead\u73af\u5883\u4e2d\u7684\u5fc3\u7406\u5b89\u5168\u611f\u3002", "method": "\u7814\u7a76\u5728\u4f2a\u667a\u80fd\u5bb6\u5c45\u73af\u5883\u4e2d\u8fdb\u884c\uff0c\u901a\u8fc7\u60c5\u5883\u8bbe\u7f6e\u5f15\u53d1\u53c2\u4e0e\u8005\u7684\u7126\u8651\uff0c\u7136\u540e\u5f15\u5165\u8212\u9002\u611f\u81ea\u52a8\u5316\u8fdb\u884c\u5e72\u9884\uff0c\u540c\u65f6\u6d4b\u91cf\u72b6\u6001\u7126\u8651\u4ee5\u8bc4\u4f30\u81ea\u52a8\u5316\u7684\u6548\u679c\u3002", "result": "\u8be5\u7814\u7a76\u9a8c\u8bc1\u4e86\u60c5\u611f\u9a71\u52a8\u7684\u667a\u80fd\u5bb6\u5c45\u81ea\u52a8\u5316\u5728\u589e\u5f3a\u7528\u6237\u5fc3\u7406\u5b89\u5168\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u652f\u6301\uff0c\u5c55\u793a\u4e86\u4e2a\u6027\u5316\u60c5\u611f\u9002\u5e94\u81ea\u52a8\u5316\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8e\u60c5\u611f\u7684\u63a7\u5236\u53ef\u4ee5\u6709\u6548\u964d\u4f4e\u7126\u8651\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u60c5\u611f\u5316\u5bb6\u5ead\u81ea\u52a8\u5316\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.21649", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.21649", "abs": "https://arxiv.org/abs/2512.21649", "authors": ["ATM Mizanur Rahman", "Sharifa Sultana"], "title": "Ghostcrafting AI: Under the Rug of Platform Labor", "comment": null, "summary": "Platform laborers play an indispensable yet hidden role in building and sustaining AI systems. Drawing on an eight-month ethnography of Bangladesh's platform labor industry and inspired by Gray and Suri, we conceptualize Ghostcrafting AI to describe how workers materially enable AI while remaining invisible or erased from recognition. Workers pursue platform labor as a path to prestige and mobility but sustain themselves through resourceful, situated learning - renting cyber-cafe computers, copying gig templates, following tutorials in unfamiliar languages, and relying on peer networks. At the same time, they face exploitative wages, unreliable payments, biased algorithms, and governance structures that make their labor precarious and invisible. To cope, they develop tactical repertoires such as identity masking, bypassing platform fees, and pirated tools. These practices reveal both AI's dependency on ghostcrafted labor and the urgent need for design, policy, and governance interventions that ensure fairness, recognition, and sustainability in platform futures.", "AI": {"tldr": "\u5e73\u53f0\u52b3\u52a8\u8005\u5728\u6784\u5efa\u548c\u7ef4\u6301AI\u7cfb\u7edf\u4e2d\u53d1\u6325\u7740\u91cd\u8981\u800c\u9690\u853d\u7684\u4f5c\u7528\uff0c\u4ed6\u4eec\u9762\u4e34\u7740\u5265\u524a\u548c\u4e0d\u5e73\u7b49\u7684\u6311\u6218\uff0c\u9700\u8981\u901a\u8fc7\u6218\u672f\u6027\u7684\u65b9\u6cd5\u6765\u5e94\u5bf9\u81ea\u8eab\u7684\u8106\u5f31\u72b6\u51b5\uff0c\u63ed\u793a\u4e86AI\u5bf9Ghostcrafting\u52b3\u52a8\u7684\u4f9d\u8d56\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63ed\u793a\u5e73\u53f0\u52b3\u52a8\u8005\u5728AI\u7cfb\u7edf\u4e2d\u7684\u91cd\u8981\u4f5c\u7528\uff0c\u5c3d\u7ba1\u4ed6\u4eec\u7684\u52b3\u52a8\u5e38\u5e38\u88ab\u5ffd\u89c6\u548c\u4f4e\u4f30\u3002", "method": "\u901a\u8fc7\u5bf9\u5b5f\u52a0\u62c9\u56fd\u5e73\u53f0\u52b3\u52a8\u4ea7\u4e1a\u7684\u516b\u4e2a\u6708\u6c11\u65cf\u5fd7\u7814\u7a76\uff0c\u5206\u6790\u52b3\u52a8\u8005\u7684\u5de5\u4f5c\u5b9e\u8df5\u548c\u6311\u6218\u3002", "result": "\u53d1\u73b0\u5e73\u53f0\u52b3\u52a8\u8005\u901a\u8fc7\u8d44\u6e90\u5b66\u4e60\u548c\u6218\u672f\u624b\u6bb5\u5728\u8270\u96be\u73af\u5883\u4e2d\u751f\u5b58\uff0c\u53cd\u6620\u4e86AI\u5bf9\u52b3\u52a8\u8005\u7684\u4f9d\u8d56\u548c\u4ea7\u4e1a\u4e2d\u5b58\u5728\u7684\u5265\u524a\u6027\u95ee\u9898\u3002", "conclusion": "AI\u7cfb\u7edf\u7684\u5efa\u8bbe\u548c\u7ef4\u6301\u4f9d\u8d56\u4e8e\u9690\u5f62\u7684\u5e73\u53f0\u52b3\u52a8\u8005\uff0c\u5fc5\u987b\u8fdb\u884c\u8bbe\u8ba1\u3001\u653f\u7b56\u548c\u6cbb\u7406\u5e72\u9884\uff0c\u4ee5\u786e\u4fdd\u672a\u6765\u5e73\u53f0\u7684\u516c\u5e73\u6027\u3001\u8ba4\u53ef\u548c\u53ef\u6301\u7eed\u6027\u3002"}}
{"id": "2512.21747", "categories": ["cs.HC", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.21747", "abs": "https://arxiv.org/abs/2512.21747", "authors": ["Gourav Siddhad", "Anurag Singh", "Rajkumar Saini", "Partha Pratim Roy"], "title": "Modified TSception for Analyzing Driver Drowsiness and Mental Workload from EEG", "comment": "8 Pages, 3 Figures, 1 Table", "summary": "Driver drowsiness remains a primary cause of traffic accidents, necessitating the development of real-time, reliable detection systems to ensure road safety. This study presents a Modified TSception architecture designed for the robust assessment of driver fatigue using Electroencephalography (EEG). The model introduces a novel hierarchical architecture that surpasses the original TSception by implementing a five-layer temporal refinement strategy to capture multi-scale brain dynamics. A key innovation is the use of Adaptive Average Pooling, which provides the structural flexibility to handle varying EEG input dimensions, and a two - stage fusion mechanism that optimizes the integration of spatiotemporal features for improved stability. When evaluated on the SEED-VIG dataset and compared against established methods - including SVM, Transformer, EEGNet, ConvNeXt, LMDA-Net, and the original TSception - the Modified TSception achieves a comparable accuracy of 83.46% (vs. 83.15% for the original). Critically, the proposed model exhibits a substantially reduced confidence interval (0.24 vs. 0.36), signifying a marked improvement in performance stability. Furthermore, the architecture's generalizability is validated on the STEW mental workload dataset, where it achieves state-of-the-art results with 95.93% and 95.35% accuracy for 2-class and 3-class classification, respectively. These improvements in consistency and cross-task generalizability underscore the effectiveness of the proposed modifications for reliable EEG-based monitoring of drowsiness and mental workload.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684TSception\u67b6\u6784\uff0c\u7528\u4e8e\u901a\u8fc7\u8111\u7535\u56fe\uff08EEG\uff09\u5b9e\u65f6\u68c0\u6d4b\u9a7e\u9a76\u5458\u75b2\u52b3\uff0c\u5177\u6709\u6bd4\u539f\u59cb\u6a21\u578b\u66f4\u597d\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u9a7e\u9a76\u5458\u56f0\u5026\u662f\u4ea4\u901a\u4e8b\u6545\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u5b9e\u65f6\u3001\u53ef\u9760\u7684\u68c0\u6d4b\u7cfb\u7edf\u4ee5\u786e\u4fdd\u9053\u8def\u5b89\u5168\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684TSception\u67b6\u6784\uff0c\u91c7\u7528\u4e94\u5c42\u65f6\u5e8f\u7cbe\u70bc\u7b56\u7565\u4ee5\u53ca\u81ea\u9002\u5e94\u5e73\u5747\u6c60\u5316\u548c\u4e24\u9636\u6bb5\u878d\u5408\u673a\u5236\uff0c\u4ee5\u6355\u6349\u591a\u5c3a\u5ea6\u8111\u52a8\u6001\u3002", "result": "\u5728SEED-VIG\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u540e\uff0c\u6539\u8fdb\u7684TSception\u6a21\u578b\u5b9e\u73b0\u4e8683.46%\u7684\u51c6\u786e\u7387\uff0c\u4fe1\u5fc3\u533a\u95f4\u663e\u8457\u7f29\u5c0f\uff0c\u4e14\u5728STEW\u5fc3\u7406\u8d1f\u8377\u6570\u636e\u96c6\u4e0a\u4e5f\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u5206\u7c7b\u7ed3\u679c\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u7684\u6539\u8fdb\u548c\u67b6\u6784\u663e\u793a\u51fa\u5728\u57fa\u4e8eEEG\u7684\u75b2\u52b3\u548c\u5fc3\u7406\u8d1f\u8377\u76d1\u6d4b\u4e0a\u7684\u6709\u6548\u6027\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u7a33\u5b9a\u6027\u548c\u8de8\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2512.21375", "categories": ["cs.RO", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.21375", "abs": "https://arxiv.org/abs/2512.21375", "authors": ["Yuanshuang Fu", "Qianyao Wang", "Qihao Wang", "Bonan Zhang", "Jiaxin Zhao", "Yiming Cao", "Zhijun Li"], "title": "Safe Path Planning and Observation Quality Enhancement Strategy for Unmanned Aerial Vehicles in Water Quality Monitoring Tasks", "comment": null, "summary": "Unmanned Aerial Vehicle (UAV) spectral remote sensing technology is widely used in water quality monitoring. However, in dynamic environments, varying illumination conditions, such as shadows and specular reflection (sun glint), can cause severe spectral distortion, thereby reducing data availability. To maximize the acquisition of high-quality data while ensuring flight safety, this paper proposes an active path planning method for dynamic light and shadow disturbance avoidance. First, a dynamic prediction model is constructed to transform the time-varying light and shadow disturbance areas into three-dimensional virtual obstacles. Second, an improved Interfered Fluid Dynamical System (IFDS) algorithm is introduced, which generates a smooth initial obstacle avoidance path by building a repulsive force field. Subsequently, a Model Predictive Control (MPC) framework is employed for rolling-horizon path optimization to handle flight dynamics constraints and achieve real-time trajectory tracking. Furthermore, a Dynamic Flight Altitude Adjustment (DFAA) mechanism is designed to actively reduce the flight altitude when the observable area is narrow, thereby enhancing spatial resolution. Simulation results show that, compared with traditional PID and single obstacle avoidance algorithms, the proposed method achieves an obstacle avoidance success rate of 98% in densely disturbed scenarios, significantly improves path smoothness, and increases the volume of effective observation data by approximately 27%. This research provides an effective engineering solution for precise UAV water quality monitoring in complex illumination environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u52a8\u6001\u5149\u5f71\u5e72\u6270\u7684\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u6c34\u8d28\u76d1\u6d4b\u7684\u6570\u636e\u8d28\u91cf\u548c\u98de\u884c\u5b89\u5168\u3002", "motivation": "\u5728\u53d8\u5316\u7684\u73af\u5883\u4e2d\uff0c\u65e0\u4eba\u673a\u5728\u6c34\u8d28\u76d1\u6d4b\u4e2d\u906d\u9047\u5149\u5f71\u5e72\u6270\uff0c\u5bfc\u81f4\u6570\u636e\u53ef\u7528\u6027\u964d\u4f4e\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u6784\u5efa\u52a8\u6001\u9884\u6d4b\u6a21\u578b\u5c06\u5149\u5f71\u5e72\u6270\u8f6c\u5316\u4e3a\u4e09\u7ef4\u865a\u62df\u969c\u788d\uff0c\u5e94\u7528\u6539\u8fdb\u7684\u5e72\u6270\u6d41\u4f53\u52a8\u529b\u7cfb\u7edf\u7b97\u6cd5\u751f\u6210\u521d\u59cb\u907f\u969c\u8def\u5f84\uff0c\u5e76\u91c7\u7528\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\u8fdb\u884c\u8def\u5f84\u4f18\u5316\uff0c\u8bbe\u8ba1\u52a8\u6001\u98de\u884c\u9ad8\u5ea6\u8c03\u6574\u673a\u5236\u4ee5\u63d0\u5347\u7a7a\u95f4\u5206\u8fa8\u7387\u3002", "result": "\u5728\u590d\u6742\u7167\u660e\u73af\u5883\u4e0b\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u5bc6\u96c6\u5e72\u6270\u573a\u666f\u4e2d\u5b9e\u73b0\u4e8698%\u7684\u907f\u969c\u6210\u529f\u7387\uff0c\u8def\u5f84\u5e73\u6ed1\u5ea6\u663e\u8457\u63d0\u9ad8\uff0c\u6709\u6548\u89c2\u6d4b\u6570\u636e\u91cf\u589e\u52a0\u7ea627%\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u590d\u6742\u5149\u7167\u73af\u5883\u4e0b\u7684\u7cbe\u786e\u65e0\u4eba\u673a\u6c34\u8d28\u76d1\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5de5\u7a0b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.21796", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.21796", "abs": "https://arxiv.org/abs/2512.21796", "authors": ["Hye-Young Jo", "Ada Zhao", "Xiaoan Liu", "Ryo Suzuki"], "title": "Generative Lecture: Making Lecture Videos Interactive with LLMs and AI Clone Instructors", "comment": "14 pages, 16 figures", "summary": "We introduce Generative Lecture, a concept that makes existing lecture videos interactive through generative AI and AI clone instructors. By leveraging interactive avatars powered by HeyGen, ElevenLabs, and GPT-5, we embed an AI instructor into the video and augment the video content in response to students' questions. This allows students to personalize the lecture material, directly ask questions in the video, and receive tailored explanations generated and delivered by the AI-cloned instructor. From a design elicitation study (N=8), we identified four goals that guided the development of eight system features: 1) on-demand clarification, 2) enhanced visuals, 3) interactive example, 4) personalized explanation, 5) adaptive quiz, 6) study summary, 7) automatic highlight, and 8) adaptive break. We then conducted a user study (N=12) to evaluate the usability and effectiveness of the system and collected expert feedback (N=5). The results suggest that our system enables effective two-way communication and supports personalized learning.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e92\u52a8\u8bb2\u5ea7\u6982\u5ff5\uff0c\u901a\u8fc7\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u548cAI\u514b\u9686\u8bb2\u5e08\u4f7f\u73b0\u6709\u8bb2\u5ea7\u89c6\u9891\u53d8\u5f97\u4e92\u52a8\uff0c\u652f\u6301\u4e2a\u6027\u5316\u5b66\u4e60\u3002", "motivation": "\u6b7b\u677f\u7684\u6559\u5b66\u89c6\u9891\u96be\u4ee5\u6ee1\u8db3\u4e2a\u4f53\u5b66\u4e60\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u901a\u8fc7AI\u6280\u672f\u589e\u5f3a\u89c6\u9891\u4e92\u52a8\u6027\u548c\u4e2a\u6027\u5316\u3002", "method": "\u4f7f\u7528HeyGen\u3001ElevenLabs\u548cGPT-5\u6280\u672f\uff0c\u5c06AI\u8bb2\u5e08\u5d4c\u5165\u89c6\u9891\u4e2d\uff0c\u540c\u65f6\u6839\u636e\u5b66\u751f\u63d0\u95ee\u589e\u5f3a\u89c6\u9891\u5185\u5bb9\uff0c\u8bbe\u8ba1\u4e86\u5305\u62ec\u516b\u4e2a\u7cfb\u7edf\u529f\u80fd\u7684\u7528\u6237\u7814\u7a76\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u53cc\u5411\u6c9f\u901a\uff0c\u652f\u6301\u4e2a\u6027\u5316\u5b66\u4e60\uff0c\u5e76\u6536\u96c6\u4e86\u4e13\u5bb6\u53cd\u9988\u3002", "conclusion": "\u672c\u7cfb\u7edf\u63a8\u52a8\u4e86\u4e2a\u6027\u5316\u5b66\u4e60\u548c\u4e92\u52a8\u6559\u5b66\u7684\u53d1\u5c55\uff0c\u662f\u672a\u6765\u6559\u80b2\u6280\u672f\u7684\u4e00\u79cd\u91cd\u8981\u8d8b\u52bf\u3002"}}
{"id": "2512.21398", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.21398", "abs": "https://arxiv.org/abs/2512.21398", "authors": ["Rahul Moorthy Mahesh", "Oguzhan Goktug Poyrazoglu", "Yukang Cao", "Volkan Isler"], "title": "Fast Navigation Through Occluded Spaces via Language-Conditioned Map Prediction", "comment": null, "summary": "In cluttered environments, motion planners often face a trade-off between safety and speed due to uncertainty caused by occlusions and limited sensor range. In this work, we investigate whether co-pilot instructions can help robots plan more decisively while remaining safe. We introduce PaceForecaster, as an approach that incorporates such co-pilot instructions into local planners. PaceForecaster takes the robot's local sensor footprint (Level-1) and the provided co-pilot instructions as input and predicts (i) a forecasted map with all regions visible from Level-1 (Level-2) and (ii) an instruction-conditioned subgoal within Level-2. The subgoal provides the planner with explicit guidance to exploit the forecasted environment in a goal-directed manner. We integrate PaceForecaster with a Log-MPPI controller and demonstrate that using language-conditioned forecasts and goals improves navigation performance by 36% over a local-map-only baseline while in polygonal environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u534f\u52a9\u6307\u4ee4\u7684\u8fd0\u52a8\u89c4\u5212\u5668PaceForecaster\uff0c\u53ef\u5728\u590d\u6742\u73af\u5883\u4e2d\u66f4\u5b89\u5168\u3001\u8fc5\u901f\u5730\u89c4\u5212\u8def\u5f84\u3002", "motivation": "\u5728\u590d\u6742\u73af\u5883\u4e2d\uff0c\u8fd0\u52a8\u89c4\u5212\u9762\u4e34\u5b89\u5168\u4e0e\u901f\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u906e\u6321\u548c\u4f20\u611f\u5668\u8303\u56f4\u9650\u5236\u7684\u60c5\u51b5\u4e0b\u3002", "method": "PaceForecaster\u7ed3\u5408\u673a\u5668\u4eba\u5c40\u90e8\u4f20\u611f\u5668\u4fe1\u606f\u548c\u534f\u52a9\u6307\u4ee4\uff0c\u9884\u6d4b\u53ef\u89c1\u533a\u57df\u5730\u56fe\uff0c\u5e76\u751f\u6210\u76ee\u6807\u6307\u5411\u7684\u5b50\u76ee\u6807\uff0c\u4ee5\u6307\u5bfc\u89c4\u5212\u3002", "result": "\u4e0e\u4ec5\u4f7f\u7528\u5c40\u90e8\u5730\u56fe\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u96c6\u6210PaceForecaster\u7684\u5bfc\u822a\u6027\u80fd\u5728\u591a\u8fb9\u5f62\u73af\u5883\u4e2d\u63d0\u5347\u4e8636%\u3002", "conclusion": "\u5f15\u5165\u8bed\u8a00\u6307\u4ee4\u7684\u9884\u6d4b\u548c\u76ee\u6807\u663e\u8457\u6539\u5584\u4e86\u673a\u5668\u4eba\u7684\u5bfc\u822a\u80fd\u529b\u3002"}}
{"id": "2512.21968", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.21968", "abs": "https://arxiv.org/abs/2512.21968", "authors": ["Kureha Hamagashira", "Miyuki Azuma", "Sotaro Shimada"], "title": "Positive Narrativity Enhances Sense of Agency toward a VR Avatar", "comment": "11 pages,4 figures", "summary": "The full-body illusion (FBI) refers to the experience of perceiving a virtual avatar as one's own body. In virtual reality (VR) environments, inducing the FBI has been shown to modulate users' bodily experiences and behavior. Previous studies have demonstrated that embodying avatars with specific characteristics can influence users' actions, largely through the activation of implicit stereotypes. However, few studies have explicitly manipulated users' impressions of an avatar by introducing narrative context. The present study investigated how avatar narrativity, induced through contextual narratives, affects the FBI. Healthy participants embodied a powerful artificial lifeform avatar in VR after listening to either a positive narrative, in which the avatar used its abilities to protect others, or a negative narrative, in which it misused its power. Participants' impressions of the avatar and indices of bodily self-consciousness were subsequently assessed. The results showed that positive narratives significantly enhanced the sense of agency (SoA), and that SoA was positively correlated with participants' perceived personal familiarity with the avatar. These findings suggest that the avatar narrativity can modulate embodiment in VR.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u865a\u62df\u73b0\u5b9e\u4e2d\uff0c\u901a\u8fc7\u53d9\u4e8b\u80cc\u666f\u5f71\u54cd\u7528\u6237\u5bf9\u4eba\u5de5\u751f\u547d\u4f53\u5934\u50cf\u7684\u5370\u8c61\uff0c\u4ece\u800c\u8c03\u8282\u5168\u8eab\u5e7b\u89c9\u7684\u53ef\u80fd\u6027\u3002\u6b63\u9762\u53d9\u4e8b\u589e\u5f3a\u4e86\u7528\u6237\u7684\u4ee3\u7406\u611f\uff0c\u5e76\u4e0e\u5176\u5bf9\u5934\u50cf\u7684\u719f\u6089\u611f\u76f8\u5173\u3002", "motivation": "\u7814\u7a76\u5168\u8eab\u5e7b\u89c9(FBI)\u5728\u865a\u62df\u73b0\u5b9e\u4e2d\u5bf9\u7528\u6237\u8eab\u4f53\u4f53\u9a8c\u548c\u884c\u4e3a\u7684\u8c03\u8282\u4f5c\u7528\uff0c\u7279\u522b\u662f\u5982\u4f55\u901a\u8fc7\u53d9\u4e8b\u80cc\u666f\u5f71\u54cd\u7528\u6237\u5bf9\u5934\u50cf\u7684\u5370\u8c61\u3002", "method": "\u53c2\u4e0e\u8005\u5728\u865a\u62df\u73b0\u5b9e\u4e2d\u4f53\u9a8c\u4e00\u4e2a\u5f3a\u5927\u7684\u4eba\u5de5\u751f\u547d\u4f53\u5934\u50cf\uff0c\u542c\u53d6\u6b63\u9762\u6216\u8d1f\u9762\u53d9\u4e8b\u540e\u8fdb\u884c\u5224\u65ad\u3002", "result": "\u6b63\u9762\u53d9\u4e8b\u663e\u8457\u589e\u5f3a\u4e86\u53c2\u4e0e\u8005\u7684\u4ee3\u7406\u611f(SoA)\uff0c\u800cSoA\u4e0e\u53c2\u4e0e\u8005\u5bf9\u5934\u50cf\u7684\u4e2a\u4eba\u719f\u6089\u611f\u5448\u6b63\u76f8\u5173\u3002", "conclusion": "\u5934\u50cf\u7684\u53d9\u4e8b\u6027\u53ef\u4ee5\u8c03\u8282\u865a\u62df\u73b0\u5b9e\u4e2d\u7684\u8eab\u4f53\u5316\u4f53\u9a8c\u3002"}}
{"id": "2512.21425", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.21425", "abs": "https://arxiv.org/abs/2512.21425", "authors": ["Hang Zhou", "Yuhui Zhai", "Shiyu Shen", "Yanfeng Ouyang", "Xiaowei Shi", "Xiaopeng"], "title": "Developing a Fundamental Diagram for Urban Air Mobility Based on Physical Experiments", "comment": null, "summary": "Urban Air Mobility (UAM) is an emerging application of unmanned aerial vehicles (UAVs) that promises to reduce travel time and alleviate congestion in urban transportation systems. As drone density increases, UAM operations are expected to experience congestion similar to that in ground traffic. However, the fundamental characteristics of UAM traffic flow, particularly under real-world operating conditions, remain poorly understood. This study proposes a general framework for constructing the fundamental diagram (FD) of UAM traffic by integrating theoretical analysis with physical experiments. To the best of our knowledge, this is the first study to derive a UAM FD using real-world physical test data. On the theoretical side, we design two drone control laws for collision avoidance and develop simulation-based traffic generation methods to produce diverse UAM traffic scenarios. Based on Edie's definition, traffic flow theory is then applied to construct the FD and characterize the macroscopic properties of UAM traffic. To account for real-world disturbances and modeling uncertainties, we further conduct physical experiments on a reduced-scale testbed using Bitcraze Crazyflie drones. Both simulation and physical test trajectory data are collected and organized into the UAMTra2Flow dataset, which is analyzed using the proposed framework. Preliminary results indicate that classical FD structures for ground transportation are also applicable to UAM systems. Notably, FD curves obtained from physical experiments exhibit deviations from simulation-based results, highlighting the importance of experimental validation. Finally, results from the reduced-scale testbed are scaled to realistic operating conditions to provide practical insights for future UAM traffic systems. The dataset and code for this paper are publicly available at https://github.com/CATS-Lab/UAM-FD.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u6784\u5efaUAM\u4ea4\u901a\u57fa\u672c\u56fe\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7406\u8bba\u4e0e\u5b9e\u9a8c\u7ed3\u5408\uff0c\u786e\u8ba4\u4f20\u7edfFD\u7ed3\u6784\u9002\u7528\u4e8eUAM\uff0c\u5e76\u5f3a\u8c03\u5b9e\u9a8c\u9a8c\u8bc1\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u5bc6\u5ea6\u7684\u589e\u52a0\uff0cUAM\u8fd0\u8425\u53ef\u80fd\u4f1a\u7ecf\u5386\u7c7b\u4f3c\u5730\u9762\u4ea4\u901a\u7684\u62e5\u5835\uff0c\u4f46\u76ee\u524dUAM\u4ea4\u901a\u6d41\u7684\u57fa\u672c\u7279\u5f81\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u6574\u5408\u7406\u8bba\u5206\u6790\u4e0e\u7269\u7406\u5b9e\u9a8c\u6784\u5efaUAM\u4ea4\u901a\u7684\u57fa\u672c\u56fe\uff0c\u5e76\u8bbe\u8ba1\u65e0\u4eba\u673a\u63a7\u5236\u6cd5\u5219\u4e0e\u57fa\u4e8e\u4eff\u771f\u7684\u4ea4\u901a\u751f\u6210\u65b9\u6cd5\u3002", "result": "\u6536\u96c6\u7684\u4eff\u771f\u548c\u7269\u7406\u6d4b\u8bd5\u8f68\u8ff9\u6570\u636e\u88ab\u7ec4\u7ec7\u6210UAMTra2Flow\u6570\u636e\u96c6\uff0c\u521d\u6b65\u7ed3\u679c\u8868\u660e\u4f20\u7edf\u5730\u9762\u4ea4\u901a\u7684FD\u7ed3\u6784\u540c\u6837\u9002\u7528\u4e8eUAM\u7cfb\u7edf\uff0c\u5e76\u4e14\u4ece\u7269\u7406\u5b9e\u9a8c\u4e2d\u5f97\u5230\u7684FD\u66f2\u7ebf\u4e0e\u4eff\u771f\u7ed3\u679c\u5b58\u5728\u504f\u5dee\u3002", "conclusion": "\u6d89\u53ca\u5b9e\u9a8c\u9a8c\u8bc1\u7684\u91cd\u8981\u6027\uff0c\u6700\u7ec8\u5c06\u7f29\u5c0f\u89c4\u6a21\u6d4b\u8bd5\u7684\u6570\u636e\u7ed3\u679c\u6269\u5c55\u81f3\u73b0\u5b9e\u64cd\u4f5c\u6761\u4ef6\uff0c\u4e3a\u672a\u6765\u7684UAM\u4ea4\u901a\u7cfb\u7edf\u63d0\u4f9b\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2512.22016", "categories": ["cs.HC", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.22016", "abs": "https://arxiv.org/abs/2512.22016", "authors": ["Xiangwen Zhang", "Xiaowei Dai", "Runnan Chen", "Xiaoming Chen", "Zeke Zexi Hu"], "title": "SketchPlay: Intuitive Creation of Physically Realistic VR Content with Gesture-Driven Sketching", "comment": null, "summary": "Creating physically realistic content in VR often requires complex modeling tools or predefined 3D models, textures, and animations, which present significant barriers for non-expert users. In this paper, we propose SketchPlay, a novel VR interaction framework that transforms humans' air-drawn sketches and gestures into dynamic, physically realistic scenes, making content creation intuitive and playful like drawing. Specifically, sketches capture the structure and spatial arrangement of objects and scenes, while gestures convey physical cues such as velocity, direction, and force that define movement and behavior. By combining these complementary forms of input, SketchPlay captures both the structure and dynamics of user-created content, enabling the generation of a wide range of complex physical phenomena, such as rigid body motion, elastic deformation, and cloth dynamics. Experimental results demonstrate that, compared to traditional text-driven methods, SketchPlay offers significant advantages in expressiveness, and user experience. By providing an intuitive and engaging creation process, SketchPlay lowers the entry barrier for non-expert users and shows strong potential for applications in education, art, and immersive storytelling.", "AI": {"tldr": "SketchPlay\u662f\u4e00\u79cd\u72ec\u7279\u7684\u865a\u62df\u73b0\u5b9e\u4ea4\u4e92\u6846\u67b6\uff0c\u901a\u8fc7\u7528\u6237\u7684\u624b\u7ed8\u8349\u56fe\u548c\u624b\u52bf\u6765\u521b\u5efa\u52a8\u6001\u4e14\u7269\u7406\u4e0a\u771f\u5b9e\u7684\u573a\u666f\uff0c\u964d\u4f4e\u4e86\u975e\u4e13\u5bb6\u7528\u6237\u7684\u521b\u4f5c\u95e8\u69db\u3002", "motivation": "\u4f20\u7edf\u76843D\u5efa\u6a21\u548c\u52a8\u753b\u5de5\u5177\u5bf9\u975e\u4e13\u4e1a\u7528\u6237\u5b58\u5728\u663e\u8457\u969c\u788d\u3002", "method": "SketchPlay\u6846\u67b6\u7ed3\u5408\u624b\u7ed8\u8349\u56fe\u548c\u624b\u52bf\u8f93\u5165\uff0c\u6355\u6349\u5bf9\u8c61\u548c\u573a\u666f\u7684\u7ed3\u6784\u53ca\u7a7a\u95f4\u6392\u5217\uff0c\u5e76\u901a\u8fc7\u624b\u52bf\u4f20\u8fbe\u7269\u7406\u4fe1\u606f\uff0c\u751f\u6210\u590d\u6742\u7684\u7269\u7406\u73b0\u8c61\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSketchPlay\u5728\u8868\u73b0\u529b\u548c\u7528\u6237\u4f53\u9a8c\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u6587\u672c\u9a71\u52a8\u7684\u65b9\u6cd5\u3002", "conclusion": "SketchPlay\u63d0\u4f9b\u4e86\u76f4\u89c2\u4e14\u5f15\u4eba\u5165\u80dc\u7684\u521b\u4f5c\u8fc7\u7a0b\uff0c\u663e\u793a\u51fa\u5728\u6559\u80b2\u3001\u827a\u672f\u548c\u6c89\u6d78\u5f0f\u53d9\u4e8b\u7b49\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2512.21430", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.21430", "abs": "https://arxiv.org/abs/2512.21430", "authors": ["Yusuf Ali", "Gryphon Patlin", "Karthik Kothuri", "Muhammad Zubair Irshad", "Wuwei Liang", "Zsolt Kira"], "title": "EVE: A Generator-Verifier System for Generative Policies", "comment": null, "summary": "Visuomotor policies based on generative architectures such as diffusion and flow-based matching have shown strong performance but degrade under distribution shifts, demonstrating limited recovery capabilities without costly finetuning. In the language modeling domain, test-time compute scaling has revolutionized reasoning capabilities of modern LLMs by leveraging additional inference-time compute for candidate solution refinement. These methods typically leverage foundation models as verification modules in a zero-shot manner to synthesize improved candidate solutions. In this work, we hypothesize that generative policies can similarly benefit from additional inference-time compute that employs zero-shot VLM-based verifiers. A systematic analysis of improving policy performance through the generation-verification framework remains relatively underexplored in the current literature. To this end, we introduce EVE - a modular, generator-verifier interaction framework - that boosts the performance of pretrained generative policies at test time, with no additional training. EVE wraps a frozen base policy with multiple zero-shot, VLM-based verifier agents. Each verifier proposes action refinements to the base policy candidate actions, while an action incorporator fuses the aggregated verifier output into the base policy action prediction to produce the final executed action. We study design choices for generator-verifier information interfacing across a system of verifiers with distinct capabilities. Across a diverse suite of manipulation tasks, EVE consistently improves task success rates without any additional policy training. Through extensive ablations, we isolate the contribution of verifier capabilities and action incorporator strategies, offering practical guidelines to build scalable, modular generator-verifier systems for embodied control.", "AI": {"tldr": "EVE\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u751f\u6210-\u9a8c\u8bc1\u4ea4\u4e92\u6846\u67b6\uff0c\u901a\u8fc7\u96f6-shot\u9a8c\u8bc1\u8005\u5728\u6d4b\u8bd5\u65f6\u589e\u5f3a\u751f\u6210\u653f\u7b56\u7684\u6027\u80fd\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5229\u7528\u751f\u6210-\u9a8c\u8bc1\u6846\u67b6\u63d0\u5347\u751f\u6210\u653f\u7b56\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u9762\u5bf9\u5206\u5e03\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\uff0c\u964d\u4f4e\u6210\u672c\u548c\u51cf\u5c11\u5fae\u8c03\u9700\u6c42\u3002", "method": "\u5f15\u5165EVE\u6846\u67b6\uff0c\u5c06\u9884\u8bad\u7ec3\u751f\u6210\u653f\u7b56\u4e0e\u591a\u4e2a\u96f6-shot VLM\u57fa\u7840\u7684\u9a8c\u8bc1\u8005\u4ee3\u7406\u76f8\u7ed3\u5408\uff0c\u4ee5\u5728\u6d4b\u8bd5\u65f6\u63d0\u5347\u653f\u7b56\u8868\u73b0\u3002", "result": "\u5728\u5404\u79cd\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0cEVE\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u529f\u7387\uff0c\u5e76\u5728\u8bbe\u8ba1\u9009\u62e9\u4e0a\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u751f\u6210-\u9a8c\u8bc1\u7cfb\u7edf\u6784\u5efa\u6307\u5357\u3002", "conclusion": "EVE\u901a\u8fc7\u65e0\u989d\u5916\u8bad\u7ec3\u63d0\u5347\u4e86\u9884\u8bad\u7ec3\u751f\u6210\u653f\u7b56\u5728\u6d4b\u8bd5\u65f6\u7684\u8868\u73b0\uff0c\u4e14\u5728\u591a\u79cd\u64cd\u4f5c\u4efb\u52a1\u4e2d consistently\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3002"}}
{"id": "2512.22032", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2512.22032", "abs": "https://arxiv.org/abs/2512.22032", "authors": ["Ziyan Zhang", "Nan Gao", "Zhiqiang Nie", "Shantanu Pal", "Haining Zhang"], "title": "Context-Aware Intelligent Chatbot Framework Leveraging Mobile Sensing", "comment": "Accepted at Companion of the 2025 ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp Companion '25), Espoo, Finland", "summary": "With the rapid advancement of large language models (LLMs), intelligent conversational assistants have demonstrated remarkable capabilities across various domains. However, they still mainly rely on explicit textual input and do not know the real world behaviors of users. This paper proposes a context-sensitive conversational assistant framework grounded in mobile sensing data. By collecting user behavior and environmental data through smartphones, we abstract these signals into 16 contextual scenarios and translate them into natural language prompts, thus improving the model's understanding of the user's state. We design a structured prompting system to guide the LLM in generating a more personalized and contextually relevant dialogue. This approach integrates mobile sensing with large language models, demonstrating the potential of passive behavioral data in intelligent conversation and offering a viable path toward digital health and personalized interaction.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u79fb\u52a8\u4f20\u611f\u6570\u636e\u7684\u4e0a\u4e0b\u6587\u654f\u611f\u5bf9\u8bdd\u52a9\u624b\u6846\u67b6\uff0c\u4ee5\u589e\u5f3a\u5bf9\u8bdd\u4e2a\u6027\u5316\u548c\u76f8\u5173\u6027\u3002", "motivation": "\u4e3a\u4e86\u4f7f\u667a\u80fd\u5bf9\u8bdd\u52a9\u624b\u80fd\u7406\u89e3\u7528\u6237\u7684\u771f\u5b9e\u4e16\u754c\u884c\u4e3a\uff0c\u4ece\u800c\u63d0\u4f9b\u66f4\u4e2a\u6027\u5316\u7684\u5bf9\u8bdd\u4f53\u9a8c\u3002", "method": "\u7ed3\u5408\u79fb\u52a8\u4f20\u611f\u6570\u636e\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u8bbe\u8ba1\u7ed3\u6784\u5316\u63d0\u793a\u7cfb\u7edf\u6765\u63d0\u5347\u5bf9\u8bdd\u7684\u4e2a\u6027\u5316\u548c\u76f8\u5173\u6027\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u7528\u6237\u884c\u4e3a\u548c\u73af\u5883\u6570\u636e\u8f6c\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\u63d0\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u8bdd\u52a9\u624b\u5bf9\u7528\u6237\u72b6\u6001\u7684\u7406\u89e3\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u5c55\u793a\u4e86\u901a\u8fc7\u79fb\u52a8\u4f20\u611f\u6570\u636e\u6539\u8fdb\u667a\u80fd\u5bf9\u8bdd\u52a9\u624b\u4e2a\u6027\u5316\u5bf9\u8bdd\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.21438", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.21438", "abs": "https://arxiv.org/abs/2512.21438", "authors": ["Marvin Chanc\u00e1n", "Avijit Banerjee", "George Nikolakopoulos"], "title": "Planetary Terrain Datasets and Benchmarks for Rover Path Planning", "comment": null, "summary": "Planetary rover exploration is attracting renewed interest with several upcoming space missions to the Moon and Mars. However, a substantial amount of data from prior missions remain underutilized for path planning and autonomous navigation research. As a result, there is a lack of space mission-based planetary datasets, standardized benchmarks, and evaluation protocols. In this paper, we take a step towards coordinating these three research directions in the context of planetary rover path planning. We propose the first two large planar benchmark datasets, MarsPlanBench and MoonPlanBench, derived from high-resolution digital terrain images of Mars and the Moon. In addition, we set up classical and learned path planning algorithms, in a unified framework, and evaluate them on our proposed datasets and on a popular planning benchmark. Through comprehensive experiments, we report new insights on the performance of representative path planning algorithms on planetary terrains, for the first time to the best of our knowledge. Our results show that classical algorithms can achieve up to 100% global path planning success rates on average across challenging terrains such as Moon's north and south poles. This suggests, for instance, why these algorithms are used in practice by NASA. Conversely, learning-based models, although showing promising results in less complex environments, still struggle to generalize to planetary domains. To serve as a starting point for fundamental path planning research, our code and datasets will be released at: https://github.com/mchancan/PlanetaryPathBench.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MarsPlanBench\u548cMoonPlanBench\u4e24\u4e2a\u5927\u578b\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u4f20\u7edf\u548c\u5b66\u4e60\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\u5728\u884c\u661f\u5730\u5f62\u4e0a\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u663e\u793a\u4f20\u7edf\u7b97\u6cd5\u5728\u590d\u6742\u5730\u5f62\u4e2d\u6210\u529f\u7387\u9ad8\u3002", "motivation": "\u5f53\u524d\u884c\u661f\u63a2\u6d4b\u4efb\u52a1\u7684\u6570\u636e\u672a\u88ab\u5145\u5206\u5229\u7528\uff0c\u7f3a\u4e4f\u9488\u5bf9\u8def\u5f84\u89c4\u5212\u7684\u884c\u661f\u6570\u636e\u96c6\u3001\u6807\u51c6\u5316\u57fa\u51c6\u548c\u8bc4\u4f30\u534f\u8bae\u3002", "method": "\u901a\u8fc7\u9ad8\u5206\u8fa8\u7387\u6570\u5b57\u5730\u5f62\u56fe\u6784\u5efaMarsPlanBench\u548cMoonPlanBench\u6570\u636e\u96c6\uff0c\u5e76\u5728\u7edf\u4e00\u6846\u67b6\u4e0b\u8bc4\u4f30\u7ecf\u5178\u548c\u5b66\u4e60\u7684\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\u3002", "result": "\u4f20\u7edf\u7b97\u6cd5\u5728\u6708\u7403\u5317\u6781\u548c\u5357\u6781\u7b49\u590d\u6742\u5730\u5f62\u4e0a\u7684\u5168\u7403\u8def\u5f84\u89c4\u5212\u6210\u529f\u7387\u53ef\u8fbe100%\uff0c\u800c\u5b66\u4e60\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u666e\u9002\u6027\u4ecd\u9700\u63d0\u9ad8\u3002", "conclusion": "\u4f20\u7edf\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\u5728\u6708\u7403\u6781\u5730\u7b49\u590d\u6742\u5730\u5f62\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u800c\u57fa\u4e8e\u5b66\u4e60\u7684\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e2d\u5c1a\u9700\u6539\u8fdb\u3002"}}
{"id": "2512.21497", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.21497", "abs": "https://arxiv.org/abs/2512.21497", "authors": ["Siddhartha Upadhyay", "Ratnangshu Das", "Pushpak Jagtap"], "title": "Spatiotemporal Tubes for Probabilistic Temporal Reach-Avoid-Stay Task in Uncertain Dynamic Environment", "comment": null, "summary": "In this work, we extend the Spatiotemporal Tube (STT) framework to address Probabilistic Temporal Reach-Avoid-Stay (PrT-RAS) tasks in dynamic environments with uncertain obstacles. We develop a real-time tube synthesis procedure that explicitly accounts for time-varying uncertain obstacles and provides formal probabilistic safety guarantees. The STT is formulated as a time-varying ball in the state space whose center and radius evolve online based on uncertain sensory information. We derive a closed-form, approximation-free control law that confines the system trajectory within the tube, ensuring both probabilistic safety and task satisfaction. Our method offers a formal guarantee for probabilistic avoidance and finite-time task completion. The resulting controller is model-free, approximation-free, and optimization-free, enabling efficient real-time execution while guaranteeing convergence to the target. The effectiveness and scalability of the framework are demonstrated through simulation studies and hardware experiments on mobile robots, a UAV, and a 7-DOF manipulator navigating in cluttered and uncertain environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6269\u5c55\u4e86\u65f6\u95f4\u7a7a\u95f4\u7ba1\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u52a8\u6001\u73af\u5883\u4e0b\u7684\u6982\u7387\u65f6\u95f4\u5230\u8fbe-\u907f\u5f00-\u505c\u7559\u4efb\u52a1\uff0c\u63d0\u4f9b\u4e86\u6b63\u5f0f\u7684\u6982\u7387\u5b89\u5168\u4fdd\u8bc1\u3002", "motivation": "\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u5b58\u5728\u4e0d\u786e\u5b9a\u969c\u788d\u7269\u5e26\u6765\u7684\u5b89\u5168\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u4fdd\u8bc1\u4efb\u52a1\u7684\u987a\u5229\u5b8c\u6210\u4e0e\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u7ba1\u9053\u5408\u6210\u7a0b\u5e8f\uff0c\u901a\u8fc7\u65f6\u95f4\u53d8\u5316\u7684\u4e0d\u786e\u5b9a\u969c\u788d\u7269\u548c\u611f\u77e5\u4fe1\u606f\uff0c\u5f62\u6210\u4e00\u4e2a\u65f6\u95f4\u53d8\u5316\u7684\u72b6\u6001\u7a7a\u95f4\u7403\u4f53\uff0c\u5e76\u5bfc\u51fa\u4e86\u4e0d\u9700\u8981\u8fd1\u4f3c\u7684\u63a7\u5236\u5f8b\u3002", "result": "\u8be5\u65b9\u6cd5\u786e\u4fdd\u7cfb\u7edf\u8f68\u8ff9\u5728\u7ba1\u9053\u5185\uff0c\u4fdd\u8bc1\u4e86\u6982\u7387\u5b89\u5168\u6027\u548c\u4efb\u52a1\u5c65\u884c\u3002\u901a\u8fc7\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u662f\u65e0\u6a21\u578b\u3001\u65e0\u8fd1\u4f3c\u548c\u65e0\u4f18\u5316\u7684\uff0c\u80fd\u6709\u6548\u5730\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u65f6\u8fd0\u884c\u5e76\u786e\u4fdd\u6536\u655b\u5230\u76ee\u6807\u3002"}}
{"id": "2512.21534", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.21534", "abs": "https://arxiv.org/abs/2512.21534", "authors": ["Congrui Bai", "Zhenting Du", "Weibang Bai"], "title": "A Novel Robotic Variable Stiffness Mechanism Based on Helically Wound Structured Electrostatic Layer Jamming", "comment": null, "summary": "This paper introduces a novel variable stiffness mechanism termed Helically Wound Structured Electrostatic Layer Jamming (HWS-ELJ) and systematically investigates its potential applications in variable stiffness robotic finger design. The proposed method utilizes electrostatic attraction to enhance interlayer friction, thereby suppressing relative sliding and enabling tunable stiffness. Compared with conventional planar ELJ, the helical configuration of HWS-ELJ provides exponentially increasing stiffness adjustment with winding angle, achieving significantly greater stiffness enhancement for the same electrode contact area while reducing the required footprint under equivalent stiffness conditions. Considering the practical advantage of voltage-based control, a series of experimental tests under different initial force conditions were conducted to evaluate the stiffness modulation characteristics of HWS-ELJ. The results demonstrated its rational design and efficacy, with outcomes following the deduced theoretical trends. Furthermore, a robotic finger prototype integrating HWS-ELJ was developed, demonstrating voltage-driven stiffness modulation and confirming the feasibility of the proposed robotic variable stiffness mechanism.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u53d8\u521a\u5ea6\u673a\u5236HWS-ELJ\uff0c\u5e76\u5c55\u793a\u5176\u5728\u673a\u5668\u4eba\u624b\u6307\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u65b0\u578b\u7684\u53ef\u53d8\u521a\u5ea6\u673a\u5236\u4ee5\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u624b\u6307\u8bbe\u8ba1\u3002", "method": "\u63d0\u51fa\u4e86\u87ba\u65cb\u7f20\u7ed5\u7ed3\u6784\u7535\u9759\u7535\u5c42\u7c98\u5f39\u6027\u673a\u5236\uff08HWS-ELJ\uff09\uff0c\u5229\u7528\u7535\u9759\u7535\u5438\u5f15\u589e\u5f3a\u5c42\u95f4\u6469\u64e6\uff0c\u4ece\u800c\u63a7\u5236\u521a\u5ea6\u8c03\u6574\u3002", "result": "\u901a\u8fc7\u4e0d\u540c\u521d\u59cb\u529b\u6761\u4ef6\u4e0b\u7684\u5b9e\u9a8c\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86HWS-ELJ\u7684\u521a\u5ea6\u8c03\u8282\u7279\u6027\uff0c\u7ed3\u679c\u7b26\u5408\u7406\u8bba\u8d8b\u52bf\u3002", "conclusion": "\u5f00\u53d1\u7684\u96c6\u6210HWS-ELJ\u7684\u673a\u5668\u4eba\u624b\u6307\u539f\u578b\u5c55\u793a\u4e86\u7535\u538b\u9a71\u52a8\u7684\u521a\u5ea6\u8c03\u8282\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u8bae\u7684\u53ef\u53d8\u521a\u5ea6\u673a\u5236\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2512.21573", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.21573", "abs": "https://arxiv.org/abs/2512.21573", "authors": ["Zhangzheng Tu", "Kailun Su", "Shaolong Zhu", "Yukun Zheng"], "title": "World-Coordinate Human Motion Retargeting via SAM 3D Body", "comment": null, "summary": "Recovering world-coordinate human motion from monocular videos with humanoid robot retargeting is significant for embodied intelligence and robotics. To avoid complex SLAM pipelines or heavy temporal models, we propose a lightweight, engineering-oriented framework that leverages SAM 3D Body (3DB) as a frozen perception backbone and uses the Momentum HumanRig (MHR) representation as a robot-friendly intermediate. Our method (i) locks the identity and skeleton-scale parameters of per tracked subject to enforce temporally consistent bone lengths, (ii) smooths per-frame predictions via efficient sliding-window optimization in the low-dimensional MHR latent space, and (iii) recovers physically plausible global root trajectories with a differentiable soft foot-ground contact model and contact-aware global optimization. Finally, we retarget the reconstructed motion to the Unitree G1 humanoid using a kinematics-aware two-stage inverse kinematics pipeline. Results on real monocular videos show that our method has stable world trajectories and reliable robot retargeting, indicating that structured human representations with lightweight physical constraints can yield robot-ready motion from monocular input.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u4eba\u4f53\u8868\u793a\u548c\u7269\u7406\u7ea6\u675f\uff0c\u4ece\u5355\u76ee\u89c6\u9891\u4e2d\u6062\u590d\u7a33\u5b9a\u7684\u4eba\u4f53\u8fd0\u52a8\u5e76\u5b9e\u73b0\u53ef\u9760\u7684\u673a\u5668\u4eba\u91cd\u5b9a\u5411\u3002", "motivation": "\u4ece\u5355\u76ee\u89c6\u9891\u4e2d\u6062\u590d\u4e16\u754c\u5750\u6807\u7684\u4eba\u4f53\u8fd0\u52a8\u5bf9\u4e8e\u5177\u8eab\u667a\u80fd\u548c\u673a\u5668\u4eba\u6280\u672f\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u5de5\u7a0b\u5bfc\u5411\u6846\u67b6\uff0c\u5229\u7528SAM 3D Body\u4f5c\u4e3a\u611f\u77e5\u540e\u7aef\uff0c\u5e76\u4f7f\u7528Momentum HumanRig\u4f5c\u4e3a\u673a\u5668\u4eba\u53cb\u597d\u7684\u4e2d\u95f4\u8868\u793a\u3002", "result": "\u5728\u771f\u5b9e\u5355\u76ee\u89c6\u9891\u4e0a\u7684\u7ed3\u679c\u8868\u660e\uff0c\u65b9\u6cd5\u5177\u6709\u7a33\u5b9a\u7684\u4e16\u754c\u8f68\u8ff9\u548c\u53ef\u9760\u7684\u673a\u5668\u4eba\u91cd\u5b9a\u5411\u3002", "conclusion": "\u7ed3\u6784\u5316\u7684\u4eba\u4f53\u8868\u793a\u4e0e\u8f7b\u91cf\u7269\u7406\u7ea6\u675f\u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u4ece\u5355\u76ee\u8f93\u5165\u4e2d\u751f\u6210\u9002\u5408\u673a\u5668\u4eba\u4f7f\u7528\u7684\u8fd0\u52a8\u3002"}}
{"id": "2512.21627", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.21627", "abs": "https://arxiv.org/abs/2512.21627", "authors": ["Botao Ren", "Junjun Hu", "Xinda Xue", "Minghua Luo", "Jintao Chen", "Haochen Bai", "Liangliang You", "Mu Xu"], "title": "AstraNav-Memory: Contexts Compression for Long Memory", "comment": null, "summary": "Lifelong embodied navigation requires agents to accumulate, retain, and exploit spatial-semantic experience across tasks, enabling efficient exploration in novel environments and rapid goal reaching in familiar ones. While object-centric memory is interpretable, it depends on detection and reconstruction pipelines that limit robustness and scalability. We propose an image-centric memory framework that achieves long-term implicit memory via an efficient visual context compression module end-to-end coupled with a Qwen2.5-VL-based navigation policy. Built atop a ViT backbone with frozen DINOv3 features and lightweight PixelUnshuffle+Conv blocks, our visual tokenizer supports configurable compression rates; for example, under a representative 16$\\times$ compression setting, each image is encoded with about 30 tokens, expanding the effective context capacity from tens to hundreds of images. Experimental results on GOAT-Bench and HM3D-OVON show that our method achieves state-of-the-art navigation performance, improving exploration in unfamiliar environments and shortening paths in familiar ones. Ablation studies further reveal that moderate compression provides the best balance between efficiency and accuracy. These findings position compressed image-centric memory as a practical and scalable interface for lifelong embodied agents, enabling them to reason over long visual histories and navigate with human-like efficiency.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u56fe\u50cf\u4e2d\u5fc3\u7684\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u89c6\u89c9\u4e0a\u4e0b\u6587\u538b\u7f29\u6a21\u5757\u4e0e\u5bfc\u822a\u7b56\u7565\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u957f\u671f\u9690\u5f0f\u8bb0\u5fc6\uff0c\u63d0\u9ad8\u4e86\u5728\u964c\u751f\u73af\u5883\u4e2d\u7684\u63a2\u7d22\u80fd\u529b\u548c\u5728\u719f\u6089\u73af\u5883\u4e2d\u7684\u8def\u5f84\u7f29\u77ed\u6548\u679c\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u5bf9\u8c61\u4e2d\u5fc3\u8bb0\u5fc6\u5728\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u7684\u9650\u5236\uff0c\u901a\u8fc7\u5f15\u5165\u56fe\u50cf\u4e2d\u5fc3\u8bb0\u5fc6\u6846\u67b6\uff0c\u63d0\u5347\u4ee3\u7406\u5728\u5404\u79cd\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eQwen2.5-VL\u7684\u5bfc\u822a\u7b56\u7565\u4e0e\u89c6\u89c9\u4e0a\u4e0b\u6587\u538b\u7f29\u6a21\u5757\u76f8\u7ed3\u5408\u7684\u56fe\u50cf\u4e2d\u5fc3\u8bb0\u5fc6\u6846\u67b6\uff0c\u5efa\u7acb\u5728ViT\u4e3b\u5e72\u4e0a\uff0c\u5229\u7528\u51bb\u7ed3\u7684DINOv3\u7279\u5f81\u4e0e\u8f7b\u91cf\u7ea7\u7684PixelUnshuffle+Conv\u6a21\u5757\u3002", "result": "\u5728GOAT-Bench\u548cHM3D-OVON\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5bfc\u822a\u6027\u80fd\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u6539\u5584\u4e86\u964c\u751f\u73af\u5883\u4e2d\u7684\u63a2\u7d22\u5e76\u7f29\u77ed\u4e86\u719f\u6089\u73af\u5883\u4e2d\u7684\u8def\u5f84\u3002", "conclusion": "\u538b\u7f29\u7684\u56fe\u50cf\u4e2d\u5fc3\u8bb0\u5fc6\u4e3a\u7ec8\u8eab\u5177\u8eab\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u9645\u548c\u53ef\u6269\u5c55\u7684\u63a5\u53e3\uff0c\u4f7f\u5176\u80fd\u591f\u5bf9\u957f\u65f6\u95f4\u7684\u89c6\u89c9\u5386\u53f2\u8fdb\u884c\u63a8\u7406\u5e76\u4ee5\u7c7b\u4eba\u6548\u7387\u8fdb\u884c\u5bfc\u822a\u3002"}}
{"id": "2512.21654", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21654", "abs": "https://arxiv.org/abs/2512.21654", "authors": ["Zikun Guo", "Adeyinka P. Adedigba", "Rammohan Mallipeddi", "Heoncheol Lee"], "title": "Structural Induced Exploration for Balanced and Scalable Multi-Robot Path Planning", "comment": "20pages, 6Figues", "summary": "Multi-robot path planning is a fundamental yet challenging problem due to its combinatorial complexity and the need to balance global efficiency with fair task allocation among robots. Traditional swarm intelligence methods, although effective on small instances, often converge prematurely and struggle to scale to complex environments. In this work, we present a structure-induced exploration framework that integrates structural priors into the search process of the ant colony optimization (ACO). The approach leverages the spatial distribution of the task to induce a structural prior at initialization, thereby constraining the search space. The pheromone update rule is then designed to emphasize structurally meaningful connections and incorporates a load-aware objective to reconcile the total travel distance with individual robot workload. An explicit overlap suppression strategy further ensures that tasks remain distinct and balanced across the team. The proposed framework was validated on diverse benchmark scenarios covering a wide range of instance sizes and robot team configurations. The results demonstrate consistent improvements in route compactness, stability, and workload distribution compared to representative metaheuristic baselines. Beyond performance gains, the method also provides a scalable and interpretable framework that can be readily applied to logistics, surveillance, and search-and-rescue applications where reliable large-scale coordination is essential.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u6846\u67b6\uff0c\u5229\u7528\u7ed3\u6784\u5148\u9a8c\u63d0\u9ad8\u4efb\u52a1\u5206\u914d\u7684\u516c\u5e73\u6027\u548c\u8def\u5f84\u89c4\u5212\u7684\u6574\u4f53\u6548\u7387\uff0c\u5e76\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u591a\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u662f\u4e00\u4e2a\u590d\u6742\u7684\u7ec4\u5408\u95ee\u9898\uff0c\u6d89\u53ca\u5728\u9ad8\u6548\u6027\u4e0e\u516c\u5e73\u6027\u4e4b\u95f4\u7684\u5e73\u8861\uff0c\u73b0\u6709\u7b97\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b58\u5728\u6536\u655b\u8fc7\u65e9\u548c\u89c4\u6a21\u6269\u5c55\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u7ed3\u6784\u8bf1\u5bfc\u7684\u63a2\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u521d\u59cb\u5316\u65f6\u5229\u7528\u4efb\u52a1\u7684\u7a7a\u95f4\u5206\u5e03\u6765\u7ea6\u675f\u641c\u7d22\u7a7a\u95f4\uff0c\u5e76\u5728\u4fe1\u606f\u7d20\u66f4\u65b0\u89c4\u5219\u4e2d\u5f3a\u8c03\u7ed3\u6784\u4e0a\u6709\u610f\u4e49\u7684\u8fde\u63a5\uff0c\u540c\u65f6\u8003\u8651\u4efb\u52a1\u7684\u8d1f\u8f7d\u60c5\u51b5\u3002", "result": "\u901a\u8fc7\u5f15\u5165\u7ed3\u6784\u5148\u9a8c\u7684\u591a\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u6846\u67b6\u63d0\u9ad8\u4e86\u8def\u5f84\u7684\u7d27\u51d1\u6027\u3001\u7a33\u5b9a\u6027\u548c\u4efb\u52a1\u5206\u914d\u7684\u516c\u5e73\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u591a\u79cd\u57fa\u51c6\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u4f20\u7edf\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u6709\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u5177\u5907\u8f83\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2512.21722", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.21722", "abs": "https://arxiv.org/abs/2512.21722", "authors": ["Zishuo Wang", "Xinyu Zhang", "Zhuonan Liu", "Tomohito Kawabata", "Daeun Song", "Xuesu Xiao", "Ling Xiao"], "title": "MAction-SocialNav: Multi-Action Socially Compliant Navigation via Reasoning-enhanced Prompt Tuning", "comment": null, "summary": "Socially compliant navigation requires robots to move safely and appropriately in human-centered environments by respecting social norms. However, social norms are often ambiguous, and in a single scenario, multiple actions may be equally acceptable. Most existing methods simplify this problem by assuming a single correct action, which limits their ability to handle real-world social uncertainty. In this work, we propose MAction-SocialNav, an efficient vision language model for socially compliant navigation that explicitly addresses action ambiguity, enabling generating multiple plausible actions within one scenario. To enhance the model's reasoning capability, we introduce a novel meta-cognitive prompt (MCP) method. Furthermore, to evaluate the proposed method, we curate a multi-action socially compliant navigation dataset that accounts for diverse conditions, including crowd density, indoor and outdoor environments, and dual human annotations. The dataset contains 789 samples, each with three-turn conversation, split into 710 training samples and 79 test samples through random selection. We also design five evaluation metrics to assess high-level decision precision, safety, and diversity. Extensive experiments demonstrate that the proposed MAction-SocialNav achieves strong social reasoning performance while maintaining high efficiency, highlighting its potential for real-world human robot navigation. Compared with zero-shot GPT-4o and Claude, our model achieves substantially higher decision quality (APG: 0.595 vs. 0.000/0.025) and safety alignment (ER: 0.264 vs. 0.642/0.668), while maintaining real-time efficiency (1.524 FPS, over 3x faster).", "AI": {"tldr": "MAction-SocialNav\u662f\u4e00\u79cd\u65b0\u578b\u7684\u673a\u5668\u4eba\u793e\u4ea4\u5408\u89c4\u5bfc\u822a\u6a21\u578b\uff0c\u901a\u8fc7\u5904\u7406\u884c\u4e3a\u6a21\u7cca\u6027\u548c\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u5728\u51b3\u7b56\u8d28\u91cf\u548c\u5b89\u5168\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u793e\u4ea4\u5408\u89c4\u5bfc\u822a\uff0c\u4ee5\u786e\u4fdd\u673a\u5668\u4eba\u5728\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u73af\u5883\u4e2d\u5b89\u5168\u4e14\u5408\u9002\u5730\u79fb\u52a8\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578bMAction-SocialNav\uff0c\u89e3\u51b3\u884c\u4e3a\u6a21\u7cca\u6027\uff0c\u901a\u8fc7\u65b0\u7684\u5143\u8ba4\u77e5\u63d0\u793a\u65b9\u6cd5\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u591a\u52a8\u4f5c\u793e\u4ea4\u5408\u89c4\u5bfc\u822a\u6570\u636e\u96c6\uff0c\u5305\u542b789\u4e2a\u6837\u672c\uff0c\u8bbe\u8ba1\u4e94\u4e2a\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u8bc1\u660eMAction-SocialNav\u5728\u793e\u4ea4\u63a8\u7406\u6027\u80fd\u548c\u6548\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u76f8\u8f83\u4e8e\u73b0\u6709\u6a21\u578b\uff0cMAction-SocialNav\u5728\u51b3\u7b56\u8d28\u91cf\u548c\u5b89\u5168\u5bf9\u9f50\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b9e\u65f6\u6548\u7387\u3002"}}
{"id": "2512.21723", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.21723", "abs": "https://arxiv.org/abs/2512.21723", "authors": ["Alexandr V. Korchemnyi", "Anatoly O. Onishchenko", "Eva A. Bakaeva", "Alexey K. Kovalev", "Aleksandr I. Panov"], "title": "HELP: Hierarchical Embodied Language Planner for Household Tasks", "comment": null, "summary": "Embodied agents tasked with complex scenarios, whether in real or simulated environments, rely heavily on robust planning capabilities. When instructions are formulated in natural language, large language models (LLMs) equipped with extensive linguistic knowledge can play this role. However, to effectively exploit the ability of such models to handle linguistic ambiguity, to retrieve information from the environment, and to be based on the available skills of an agent, an appropriate architecture must be designed. We propose a Hierarchical Embodied Language Planner, called HELP, consisting of a set of LLM-based agents, each dedicated to solving a different subtask. We evaluate the proposed approach on a household task and perform real-world experiments with an embodied agent. We also focus on the use of open source LLMs with a relatively small number of parameters, to enable autonomous deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5c42\u6b21\u5316\u8bed\u8a00\u89c4\u5212\u5668HELP\uff0c\u901a\u8fc7\u591a\u4e2aLLM\u4ee3\u7406\u6765\u89e3\u51b3\u590d\u6742\u7684\u5b9e\u9645\u4efb\u52a1\uff0c\u5e76\u5728\u5bb6\u5ead\u573a\u666f\u4e2d\u8fdb\u884c\u4e86\u6709\u6548\u6027\u8bc4\u4f30\u3002", "motivation": "\u968f\u7740\u590d\u6742\u573a\u666f\u7684\u589e\u591a\uff0c\u73b0\u6709\u7684\u89c4\u5212\u80fd\u529b\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\uff0c\u56e0\u6b64\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u67b6\u6784\u4ee5\u5145\u5206\u5229\u7528LLM\u5728\u8bed\u8a00\u7406\u89e3\u4e0a\u7684\u4f18\u52bf\u3002", "method": "Hierarchical Embodied Language Planner (HELP)", "result": "\u901a\u8fc7\u4e00\u4e2a\u96c6\u6210\u591a\u79cd\u4efb\u52a1\u7684LLM\u4ee3\u7406\u6765\u89e3\u51b3\u590d\u6742\u573a\u666f\u4e2d\u7684\u4efb\u52a1", "conclusion": "\u63d0\u51fa\u7684HELP\u67b6\u6784\u5728\u5bb6\u5ead\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u5e76\u652f\u6301\u5c0f\u53c2\u6570\u7684\u5f00\u6e90LLM\u8fdb\u884c\u81ea\u4e3b\u5e94\u7528"}}
{"id": "2512.21853", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21853", "abs": "https://arxiv.org/abs/2512.21853", "authors": ["Kentaro Uno", "Elian Neppel", "Gustavo H. Diaz", "Ashutosh Mishra", "Shamistan Karimov", "A. Sejal Jain", "Ayesha Habib", "Pascal Pama", "Hazal Gozbasi", "Shreya Santra", "Kazuya Yoshida"], "title": "MoonBot: Modular and On-Demand Reconfigurable Robot Toward Moon Base Construction", "comment": "This is the authors' version of a paper accepted for publication in IEEE Transactions on Field Robotics, (c) IEEE. The final published version is available at https://doi.org/10.1109/TFR.2025.3624346", "summary": "The allure of lunar surface exploration and development has recently captured widespread global attention. Robots have proved to be indispensable for exploring uncharted terrains, uncovering and leveraging local resources, and facilitating the construction of future human habitats. In this article, we introduce the modular and on-demand reconfigurable robot (MoonBot), a modular and reconfigurable robotic system engineered to maximize functionality while operating within the stringent mass constraints of lunar payloads and adapting to varying environmental conditions and task requirements. This article details the design and development of MoonBot and presents a preliminary field demonstration that validates the proof of concept through the execution of milestone tasks simulating the establishment of lunar infrastructure. These tasks include essential civil engineering operations, infrastructural component transportation and deployment, and assistive operations with inflatable modules. Furthermore, we systematically summarize the lessons learned during testing, focusing on the connector design and providing valuable insights for the advancement of modular robotic systems in future lunar missions.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u7684\u6708\u7403\u63a2\u7d22\u673a\u5668\u4ebaMoonBot\uff0c\u5e76\u901a\u8fc7\u521d\u6b65\u7684\u73b0\u573a\u6f14\u793a\u9a8c\u8bc1\u4e86\u5176\u6982\u5ff5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u63a2\u7d22\u548c\u5f00\u53d1\u6708\u7403\u8868\u9762\u5f15\u8d77\u4e86\u5168\u7403\u7684\u5e7f\u6cdb\u5173\u6ce8\uff0c\u673a\u5668\u4eba\u5728\u63a2\u7d22\u672a\u77e5\u5730\u5f62\u548c\u5efa\u8bbe\u4eba\u7c7b\u5c45\u4f4f\u73af\u5883\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4ecb\u7ecd\u4e86MoonBot\uff0c\u8fd9\u662f\u4e00\u79cd\u6a21\u5757\u5316\u548c\u6309\u9700\u53ef\u91cd\u6784\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u65e8\u5728\u5728\u4e25\u683c\u7684\u8d28\u91cf\u7ea6\u675f\u4e0b\u6700\u5927\u5316\u529f\u80fd\u3002", "result": "MoonBot\u5728\u6a21\u62df\u5efa\u7acb\u6708\u7403\u57fa\u7840\u8bbe\u65bd\u7684\u8bd5\u9a8c\u4efb\u52a1\u4e2d\u8bc1\u660e\u4e86\u5176\u6982\u5ff5\u7684\u6709\u6548\u6027\uff0c\u5305\u62ec\u571f\u6728\u5de5\u7a0b\u64cd\u4f5c\u3001\u57fa\u7840\u8bbe\u65bd\u7ec4\u4ef6\u8fd0\u8f93\u548c\u90e8\u7f72\u4ee5\u53ca\u52a9\u529b\u5145\u6c14\u6a21\u5757\u7684\u64cd\u4f5c\u3002", "conclusion": "\u6d4b\u8bd5\u603b\u7ed3\u4e86\u5b9d\u8d35\u7684\u7ecf\u9a8c\u6559\u8bad\uff0c\u5c24\u5176\u662f\u5728\u8fde\u63a5\u5668\u8bbe\u8ba1\u65b9\u9762\uff0c\u4e3a\u672a\u6765\u7684\u6708\u7403\u4efb\u52a1\u4e2d\u7684\u6a21\u5757\u5316\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u8fdb\u6b65\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2512.21882", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.21882", "abs": "https://arxiv.org/abs/2512.21882", "authors": ["Kenta Iizuka", "Akiyoshi Uchida", "Kentaro Uno", "Kazuya Yoshida"], "title": "Optimal Trajectory Planning for Orbital Robot Rendezvous and Docking", "comment": "Author's version of a manuscript accepted at the International Conference on Space Robotics 2025 (iSpaRo 2025). (c) IEEE", "summary": "Approaching a tumbling target safely is a critical challenge in space debris removal missions utilizing robotic manipulators onboard servicing satellites. In this work, we propose a trajectory planning method based on nonlinear optimization for a close-range rendezvous to bring a free-floating, rotating debris object in a two-dimensional plane into the manipulator's workspace, as a preliminary step for its capture. The proposed method introduces a dynamic keep-out sphere that adapts depending on the approach conditions, allowing for closer and safer access to the target. Furthermore, a control strategy is developed to reproduce the optimized trajectory using discrete ON/OFF thrusters, considering practical implementation constraints.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u975e\u7ebf\u6027\u4f18\u5316\u7684\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\uff0c\u65e8\u5728\u5b89\u5168\u6709\u6548\u5730\u63a5\u8fd1\u5e76\u6355\u83b7\u65cb\u8f6c\u7684\u7a7a\u95f4 debris\u3002", "motivation": "\u89e3\u51b3\u592a\u7a7a debris \u79fb\u9664\u4efb\u52a1\u4e2d\uff0c\u5b89\u5168\u63a5\u8fd1\u7ffb\u6eda\u76ee\u6807\u7684\u96be\u9898\u3002", "method": "\u57fa\u4e8e\u975e\u7ebf\u6027\u4f18\u5316\u7684\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\uff0c\u7ed3\u5408\u52a8\u6001\u4fdd\u6301\u7403\u4ee5\u9002\u5e94\u63a5\u8fd1\u6761\u4ef6\uff0c\u548c\u63a7\u5236\u7b56\u7565\u4f7f\u7528\u79bb\u6563 ON/OFF \u63a8\u8fdb\u5668\u590d\u73b0\u4f18\u5316\u8f68\u8ff9\u3002", "result": "\u5b9e\u73b0\u4e86\u5728\u63a5\u8fd1\u7ffb\u6eda debris \u76ee\u6807\u65f6\u7684\u66f4\u8fd1\u548c\u66f4\u5b89\u5168\u7684\u8bbf\u95ee\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u65cb\u8f6c\u76ee\u6807\u7684\u6355\u83b7\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u8003\u8651\u4e86\u5b9e\u9645\u5b9e\u65bd\u7684\u7ea6\u675f\u3002"}}
{"id": "2512.21886", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.21886", "abs": "https://arxiv.org/abs/2512.21886", "authors": ["Akiyoshi Uchida", "Antonine Richard", "Kentaro Uno", "Miguel Olivares-Mendez", "Kazuya Yoshida"], "title": "Online Inertia Parameter Estimation for Unknown Objects Grasped by a Manipulator Towards Space Applications", "comment": "Author's version of a manuscript accepted at the International Conference on Space Robotics 2025 (iSpaRo 2025). (c) IEEE", "summary": "Knowing the inertia parameters of a grasped object is crucial for dynamics-aware manipulation, especially in space robotics with free-floating bases. This work addresses the problem of estimating the inertia parameters of an unknown target object during manipulation. We apply and extend an existing online identification method by incorporating momentum conservation, enabling its use for the floating-base robots. The proposed method is validated through numerical simulations, and the estimated parameters are compared with ground-truth values. Results demonstrate accurate identification in the scenarios, highlighting the method's applicability to on-orbit servicing and other space missions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5728\u7ebf\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u91cf\u5b88\u6052\u6765\u6539\u5584\u60ef\u6027\u53c2\u6570\u8bc6\u522b\uff0c\u9002\u7528\u4e8e\u7a7a\u95f4\u673a\u5668\u4eba\u3002", "motivation": "\u5728\u52a8\u6001\u611f\u77e5\u7684\u64cd\u4f5c\u4e2d\uff0c\u4e86\u89e3\u6293\u53d6\u5bf9\u8c61\u7684\u60ef\u6027\u53c2\u6570\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u81ea\u7531\u6d6e\u52a8\u57fa\u5ea7\u7684\u7a7a\u95f4\u673a\u5668\u4eba\u4e2d\u3002", "method": "\u8be5\u5de5\u4f5c\u6269\u5c55\u4e86\u73b0\u6709\u7684\u5728\u7ebf\u8bc6\u522b\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u4e86\u52a8\u91cf\u5b88\u6052\u7406\u8bba\uff0c\u4ee5\u9002\u5e94\u6d6e\u52a8\u57fa\u5ea7\u673a\u5668\u4eba\u7684\u9700\u6c42\u3002", "result": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u64cd\u63a7\u4e2d\u4f30\u8ba1\u672a\u77e5\u76ee\u6807\u7269\u4f53\u7684\u60ef\u6027\u53c2\u6570\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6709\u81ea\u7531\u6d6e\u52a8\u57fa\u5ea7\u7684\u7a7a\u95f4\u673a\u5668\u4eba\u3002\u8fd9\u79cd\u65b9\u6cd5\u901a\u8fc7\u7eb3\u5165\u52a8\u91cf\u5b88\u6052\u6765\u6269\u5c55\u73b0\u6709\u7684\u5728\u7ebf\u8bc6\u522b\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u4eff\u771f\u8fdb\u884c\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u573a\u666f\u4e2d\u7684\u60ef\u6027\u53c2\u6570\u8bc6\u522b\u51c6\u786e\uff0c\u663e\u793a\u51fa\u5176\u5728\u8f68\u9053\u670d\u52a1\u53ca\u5176\u4ed6\u592a\u7a7a\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2512.21887", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21887", "abs": "https://arxiv.org/abs/2512.21887", "authors": ["Weichen Zhang", "Peizhi Tang", "Xin Zeng", "Fanhang Man", "Shiquan Yu", "Zichao Dai", "Baining Zhao", "Hongjin Chen", "Yu Shang", "Wei Wu", "Chen Gao", "Xinlei Chen", "Xin Wang", "Yong Li", "Wenwu Zhu"], "title": "Aerial World Model for Long-horizon Visual Generation and Navigation in 3D Space", "comment": null, "summary": "Unmanned aerial vehicles (UAVs) have emerged as powerful embodied agents. One of the core abilities is autonomous navigation in large-scale three-dimensional environments. Existing navigation policies, however, are typically optimized for low-level objectives such as obstacle avoidance and trajectory smoothness, lacking the ability to incorporate high-level semantics into planning. To bridge this gap, we propose ANWM, an aerial navigation world model that predicts future visual observations conditioned on past frames and actions, thereby enabling agents to rank candidate trajectories by their semantic plausibility and navigational utility. ANWM is trained on 4-DoF UAV trajectories and introduces a physics-inspired module: Future Frame Projection (FFP), which projects past frames into future viewpoints to provide coarse geometric priors. This module mitigates representational uncertainty in long-distance visual generation and captures the mapping between 3D trajectories and egocentric observations. Empirical results demonstrate that ANWM significantly outperforms existing world models in long-distance visual forecasting and improves UAV navigation success rates in large-scale environments.", "AI": {"tldr": "ANWM\u662f\u4e00\u4e2a\u901a\u8fc7\u9884\u6d4b\u672a\u6765\u89c6\u89c9\u6765\u589e\u5f3a\u65e0\u4eba\u673a\u5bfc\u822a\u7684\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5bfc\u822a\u6210\u529f\u7387\u3002", "motivation": "\u5f53\u524d\u65e0\u4eba\u673a\u5bfc\u822a\u653f\u7b56\u901a\u5e38\u4f18\u5316\u4f4e\u5c42\u76ee\u6807\uff0c\u7f3a\u4e4f\u5c06\u9ad8\u5c42\u8bed\u4e49\u878d\u5165\u89c4\u5212\u7684\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u6574\u5408\u8bed\u4e49\u4fe1\u606f\u7684\u5bfc\u822a\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aANWM\u7684\u7a7a\u4e2d\u5bfc\u822a\u4e16\u754c\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u8fc7\u53bb\u5e27\u548c\u52a8\u4f5c\u9884\u6d4b\u672a\u6765\u89c6\u89c9\u89c2\u5bdf\uff0c\u4ece\u800c\u4f7f\u4ee3\u7406\u80fd\u591f\u901a\u8fc7\u8bed\u4e49\u53ef\u4fe1\u5ea6\u548c\u5bfc\u822a\u6548\u7528\u5bf9\u5019\u9009\u8f68\u8ff9\u8fdb\u884c\u6392\u540d\u3002", "result": "ANWM\u76f8\u8f83\u4e8e\u73b0\u6709\u4e16\u754c\u6a21\u578b\u5728\u957f\u8ddd\u79bb\u89c6\u89c9\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u663e\u8457\u4f18\u8d8a\uff0c\u63d0\u9ad8\u4e86\u65e0\u4eba\u673a\u5728\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6210\u529f\u7387\u3002", "conclusion": "ANWM\u80fd\u591f\u6709\u6548\u5730\u51cf\u5c11\u957f\u8ddd\u79bb\u89c6\u89c9\u751f\u6210\u4e2d\u7684\u8868\u5f81\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u6355\u63493D\u8f68\u8ff9\u4e0e\u81ea\u6211\u4e2d\u5fc3\u89c2\u5bdf\u4e4b\u95f4\u7684\u6620\u5c04\uff0c\u8fdb\u800c\u6539\u5584\u65e0\u4eba\u673a\u7684\u5bfc\u822a\u80fd\u529b\u3002"}}
{"id": "2512.21898", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21898", "abs": "https://arxiv.org/abs/2512.21898", "authors": ["Chaoqi Liu", "Haonan Chen", "Sigmund H. H\u00f8eg", "Shaoxiong Yao", "Yunzhu Li", "Kris Hauser", "Yilun Du"], "title": "Flexible Multitask Learning with Factorized Diffusion Policy", "comment": null, "summary": "Multitask learning poses significant challenges due to the highly multimodal and diverse nature of robot action distributions. However, effectively fitting policies to these complex task distributions is often difficult, and existing monolithic models often underfit the action distribution and lack the flexibility required for efficient adaptation. We introduce a novel modular diffusion policy framework that factorizes complex action distributions into a composition of specialized diffusion models, each capturing a distinct sub-mode of the behavior space for a more effective overall policy. In addition, this modular structure enables flexible policy adaptation to new tasks by adding or fine-tuning components, which inherently mitigates catastrophic forgetting. Empirically, across both simulation and real-world robotic manipulation settings, we illustrate how our method consistently outperforms strong modular and monolithic baselines.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u6a21\u5757\u5316\u6269\u6563\u7b56\u7565\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u590d\u6742\u52a8\u4f5c\u5206\u5e03\u5206\u89e3\u4e3a\u591a\u4e2a\u4e13\u4e1a\u6a21\u578b\uff0c\u6709\u6548\u5e94\u5bf9\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u5728\u591a\u4e2a\u8bbe\u7f6e\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u591a\u4efb\u52a1\u5b66\u4e60\u5728\u673a\u5668\u4eba\u52a8\u4f5c\u5206\u5e03\u7684\u591a\u6a21\u6001\u548c\u591a\u6837\u6027\u65b9\u9762\u9762\u4e34\u663e\u8457\u6311\u6218\uff0c\u73b0\u6709\u7684\u5355\u4f53\u6a21\u578b\u96be\u4ee5\u6709\u6548\u9002\u5e94\u8fd9\u4e9b\u590d\u6742\u7684\u4efb\u52a1\u5206\u5e03\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6a21\u5757\u5316\u6269\u6563\u7b56\u7565\u6846\u67b6\uff0c\u5c06\u590d\u6742\u7684\u52a8\u4f5c\u5206\u5e03\u5206\u89e3\u4e3a\u591a\u4e2a\u4e13\u4e1a\u7684\u6269\u6563\u6a21\u578b\uff0c\u6bcf\u4e2a\u6a21\u578b\u6355\u6349\u884c\u4e3a\u7a7a\u95f4\u7684\u4e00\u4e2a\u7279\u5b9a\u5b50\u6a21\u5f0f\u3002", "result": "\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u4e16\u754c\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u8bbe\u7f6e\u4e2d\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u660e\u663e\u4f18\u4e8e\u5f3a\u5927\u7684\u6a21\u5757\u5316\u548c\u5355\u4f53\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u6a21\u5757\u5316\u7ed3\u6784\u4f7f\u5f97\u653f\u7b56\u80fd\u591f\u7075\u6d3b\u9002\u5e94\u65b0\u4efb\u52a1\uff0c\u964d\u4f4e\u4e86\u707e\u96be\u6027\u9057\u5fd8\u7684\u98ce\u9669\u3002"}}
{"id": "2512.21970", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.21970", "abs": "https://arxiv.org/abs/2512.21970", "authors": ["Shengliang Deng", "Mi Yan", "Yixin Zheng", "Jiayi Su", "Wenhao Zhang", "Xiaoguang Zhao", "Heming Cui", "Zhizheng Zhang", "He Wang"], "title": "StereoVLA: Enhancing Vision-Language-Action Models with Stereo Vision", "comment": null, "summary": "Stereo cameras closely mimic human binocular vision, providing rich spatial cues critical for precise robotic manipulation. Despite their advantage, the adoption of stereo vision in vision-language-action models (VLAs) remains underexplored. In this work, we present StereoVLA, a VLA model that leverages rich geometric cues from stereo vision. We propose a novel Geometric-Semantic Feature Extraction module that utilizes vision foundation models to extract and fuse two key features: 1) geometric features from subtle stereo-view differences for spatial perception; 2) semantic-rich features from the monocular view for instruction following. Additionally, we propose an auxiliary Interaction-Region Depth Estimation task to further enhance spatial perception and accelerate model convergence. Extensive experiments show that our approach outperforms baselines by a large margin in diverse tasks under the stereo setting and demonstrates strong robustness to camera pose variations.", "AI": {"tldr": "StereoVLA\u6a21\u578b\u5229\u7528\u7acb\u4f53\u89c6\u89c9\u7684\u51e0\u4f55\u548c\u8bed\u4e49\u7279\u5f81\uff0c\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7cbe\u5ea6\uff0c\u8868\u73b0\u4f18\u5f02", "motivation": "\u63a2\u7d22\u7acb\u4f53\u89c6\u89c9\u5728\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u53ca\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u7a7a\u95f4\u611f\u77e5\u80fd\u529b", "method": "\u63d0\u51faStereoVLA\u6a21\u578b\uff0c\u7ed3\u5408\u7acb\u4f53\u89c6\u89c9\u7684\u51e0\u4f55\u7279\u5f81\u4e0e\u5355\u76ee\u89c6\u56fe\u7684\u8bed\u4e49\u7279\u5f81", "result": "StereoVLA\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u826f\u597d\u7684\u9c81\u68d2\u6027\uff0c\u8d85\u8d8a\u4e86\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u901a\u8fc7\u7acb\u4f53\u89c6\u89c9\u589e\u5f3a\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\uff0c\u652f\u6301\u66f4\u7cbe\u786e\u7684\u673a\u5668\u4eba\u64cd\u4f5c"}}
{"id": "2512.21983", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.21983", "abs": "https://arxiv.org/abs/2512.21983", "authors": ["Saksham Gupta", "Sarthak Mishra", "Arshad Ayub", "Kamran Farooque", "Spandan Roy", "Babita Gupta"], "title": "Bab_Sak Robotic Intubation System (BRIS): A Learning-Enabled Control Framework for Safe Fiberoptic Endotracheal Intubation", "comment": null, "summary": "Endotracheal intubation is a critical yet technically demanding procedure, with failure or improper tube placement leading to severe complications. Existing robotic and teleoperated intubation systems primarily focus on airway navigation and do not provide integrated control of endotracheal tube advancement or objective verification of tube depth relative to the carina. This paper presents the Robotic Intubation System (BRIS), a compact, human-in-the-loop platform designed to assist fiberoptic-guided intubation while enabling real-time, objective depth awareness. BRIS integrates a four-way steerable fiberoptic bronchoscope, an independent endotracheal tube advancement mechanism, and a camera-augmented mouthpiece compatible with standard clinical workflows. A learning-enabled closed-loop control framework leverages real-time shape sensing to map joystick inputs to distal bronchoscope tip motion in Cartesian space, providing stable and intuitive teleoperation under tendon nonlinearities and airway contact. Monocular endoscopic depth estimation is used to classify airway regions and provide interpretable, anatomy-aware guidance for safe tube positioning relative to the carina. The system is validated on high-fidelity airway mannequins under standard and difficult airway configurations, demonstrating reliable navigation and controlled tube placement. These results highlight BRIS as a step toward safer, more consistent, and clinically compatible robotic airway management.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u673a\u5668\u4eba\u63d2\u7ba1\u7cfb\u7edfBRIS\uff0c\u89e3\u51b3\u4e86\u6c14\u9053\u7ba1\u7406\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u63d0\u4f9b\u5b9e\u65f6\u6df1\u5ea6\u611f\u77e5\u548c\u5b89\u5168\u7684\u63d2\u7ba1\u6307\u5bfc\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u673a\u5668\u4eba\u6c14\u7ba1\u63d2\u7ba1\u7cfb\u7edf\u5728\u6c14\u9053\u5bfc\u822a\u548c\u7aef\u7ba1\u63a8\u8fdb\u96c6\u6210\u63a7\u5236\u53ca\u6df1\u5ea6\u9a8c\u8bc1\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7d27\u51d1\u578b\u3001\u5b9e\u65f6\u3001\u76ee\u6807\u5bfc\u5411\u7684\u673a\u5668\u4eba\u63d2\u7ba1\u7cfb\u7edfBRIS\uff0c\u7ed3\u5408\u4e86\u53ef\u56db\u5411\u8f6c\u52a8\u7684\u7ea4\u7ef4\u652f\u6c14\u7ba1\u955c\u3001\u72ec\u7acb\u7684\u63d2\u7ba1\u63a8\u8fdb\u673a\u5236\u4ee5\u53ca\u517c\u5bb9\u6807\u51c6\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u7684\u589e\u5f3a\u76f8\u673a\u53e3\u8154\u88c5\u7f6e\uff0c\u91c7\u7528\u95ed\u73af\u63a7\u5236\u6846\u67b6\u548c\u5355\u76ee\u5185\u7aa5\u955c\u6df1\u5ea6\u4f30\u8ba1\u3002", "result": "\u5728\u9ad8\u4fdd\u771f\u6c14\u9053\u6a21\u62df\u5668\u4e0a\u9a8c\u8bc1\uff0c\u5728\u6807\u51c6\u548c\u56f0\u96be\u6c14\u9053\u914d\u7f6e\u4e0b\u663e\u793a\u51fa\u53ef\u9760\u7684\u5bfc\u822a\u548c\u53d7\u63a7\u7684\u63d2\u7ba1\u3002", "conclusion": "BRIS\u7cfb\u7edf\u4e3a\u66f4\u5b89\u5168\u3001\u4e00\u81f4\u4e14\u4e34\u5e8a\u517c\u5bb9\u7684\u673a\u5668\u4eba\u6c14\u9053\u7ba1\u7406\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
