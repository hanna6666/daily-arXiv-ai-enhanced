{"id": "2601.04288", "categories": ["cs.HC", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.04288", "abs": "https://arxiv.org/abs/2601.04288", "authors": ["Ben Carvell", "Marc Thomas", "Andrew Pace", "Christopher Dorney", "George De Ath", "Richard Everson", "Nick Pepper", "Adam Keane", "Samuel Tomlinson", "Richard Cannon"], "title": "Human-in-the-Loop Testing of AI Agents for Air Traffic Control with a Regulated Assessment Framework", "comment": null, "summary": "We present a rigorous, human-in-the-loop evaluation framework for assessing the performance of AI agents on the task of Air Traffic Control, grounded in a regulator-certified simulator-based curriculum used for training and testing real-world trainee controllers. By leveraging legally regulated assessments and involving expert human instructors in the evaluation process, our framework enables a more authentic and domain-accurate measurement of AI performance. This work addresses a critical gap in the existing literature: the frequent misalignment between academic representations of Air Traffic Control and the complexities of the actual operational environment. It also lays the foundations for effective future human-machine teaming paradigms by aligning machine performance with human assessment targets.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u7a7a\u4e2d\u4ea4\u901a\u7ba1\u5236\u4efb\u52a1\u4e2d\u8bc4\u4f30AI\u8868\u73b0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6587\u732e\u4e2d\u7684\u8868\u73b0\u9519\u4f4d\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5b66\u672f\u754c\u5bf9\u7a7a\u4e2d\u4ea4\u901a\u7ba1\u5236\u7684\u8868\u73b0\u4e0e\u5b9e\u9645\u64cd\u4f5c\u73af\u5883\u4e4b\u95f4\u7684\u9519\u4f4d\u95ee\u9898\u3002", "method": "\u5229\u7528\u7531\u76d1\u7ba1\u673a\u6784\u8ba4\u8bc1\u7684\u6a21\u62df\u5668\u57fa\u7840\u8bfe\u7a0b\uff0c\u7ed3\u5408\u4e13\u5bb6\u4eba\u7c7b\u8bb2\u5e08\u7684\u8bc4\u4f30\u53c2\u4e0e\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u66f4\u771f\u5b9e\u548c\u9886\u57df\u51c6\u786e\u7684AI\u8868\u73b0\u6d4b\u91cf\u6846\u67b6\uff0c\u4ee5\u652f\u6301\u672a\u6765\u7684\u4eba\u673a\u534f\u4f5c\u6a21\u5f0f\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4eba\u673a\u534f\u540c\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u66f4\u51c6\u786e\u5730\u8bc4\u4f30AI\u5728\u7a7a\u4e2d\u4ea4\u901a\u7ba1\u5236\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u4e3a\u4eba\u673a\u534f\u4f5c\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2601.04485", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.04485", "abs": "https://arxiv.org/abs/2601.04485", "authors": ["Martin P. Robillard", "Lihn V. Nguyen", "Deeksha Arya", "Jin L. C. Guo"], "title": "How Users Consider Web Tracking When Seeking Health Information Online", "comment": null, "summary": "Health information websites offer instantaneous access to information, but have important privacy implications as they can associate a visitor with specific medical conditions. We interviewed 35 residents of Canada to better understand whether and how online health information seekers exercise three potential means of protection against surveillance: website selection, privacy-enhancing technologies, and self-censorship, as well as their understanding of web tracking. Our findings reveal how users' limited initiative and effectiveness in protecting their privacy could be associated with a missing or inaccurate understanding of how implicit data collection by third parties takes place on the web, and who collects the data. We conclude that to help Internet users achieve better self-data protection, we may need to shift privacy awareness efforts from what information is collected to how it is collected.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u8c03\u67e5\u4e86\u52a0\u62ff\u5927\u5c45\u6c11\u5bf9\u5728\u7ebf\u5065\u5eb7\u4fe1\u606f\u641c\u7d22\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u63aa\u65bd\u7684\u7406\u89e3\u4e0e\u5e94\u7528\uff0c\u53d1\u73b0\u7528\u6237\u5bf9\u9690\u79c1\u4fdd\u62a4\u7684\u6709\u9650\u8ba4\u77e5\u53ef\u80fd\u6e90\u4e8e\u5bf9\u9690\u853d\u6570\u636e\u6536\u96c6\u65b9\u5f0f\u7684\u8bef\u89e3\u3002", "motivation": "\u7814\u7a76\u5728\u7ebf\u5065\u5eb7\u4fe1\u606f\u641c\u7d22\u7684\u9690\u79c1\u5f71\u54cd\uff0c\u4ee5\u4e86\u89e3\u7528\u6237\u5982\u4f55\u4fdd\u62a4\u81ea\u5df1\u7684\u9690\u79c1\u53ca\u5176\u5bf9\u7f51\u9875\u8ffd\u8e2a\u7684\u7406\u89e3\u3002", "method": "\u5bf935\u540d\u52a0\u62ff\u5927\u5c45\u6c11\u8fdb\u884c\u4e86\u8bbf\u8c08\uff0c\u4e86\u89e3\u4ed6\u4eec\u5728\u7f51\u4e0a\u641c\u5bfb\u5065\u5eb7\u4fe1\u606f\u65f6\u91c7\u53d6\u7684\u9690\u79c1\u4fdd\u62a4\u63aa\u65bd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u7528\u6237\u5bf9\u9690\u79c1\u4fdd\u62a4\u7684\u4e3b\u52a8\u6027\u548c\u6709\u6548\u6027\u6709\u9650\uff0c\u8fd9\u4e0e\u5bf9\u7b2c\u4e09\u65b9\u9690\u853d\u6570\u636e\u6536\u96c6\u7684\u7406\u89e3\u4e0d\u8db3\u6709\u5173\u3002", "conclusion": "\u4e3a\u4e86\u5e2e\u52a9\u4e92\u8054\u7f51\u7528\u6237\u5b9e\u73b0\u66f4\u597d\u7684\u81ea\u6211\u6570\u636e\u4fdd\u62a4\uff0c\u6211\u4eec\u53ef\u80fd\u9700\u8981\u5c06\u9690\u79c1\u610f\u8bc6\u7684\u52aa\u529b\u4ece\u6536\u96c6\u4e86\u54ea\u4e9b\u4fe1\u606f\u8f6c\u5411\u5982\u4f55\u6536\u96c6\u4fe1\u606f\u3002"}}
{"id": "2601.04596", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.04596", "abs": "https://arxiv.org/abs/2601.04596", "authors": ["Xinyan Yu", "Marius Hoggenm\u00fcller", "Tram Thi Minh Tran", "Martin Tomitsch"], "title": "Feel the Presence: The Effects of Haptic Sensation on VR-Based Human-Robot Interaction", "comment": null, "summary": "Virtual reality (VR) has been increasingly utilised as a simulation tool for human-robot interaction (HRI) studies due to its ability to facilitate fast and flexible prototyping. Despite efforts to achieve high validity in VR studies, haptic sensation, an essential sensory modality for perception and a critical factor in enhancing VR realism, is often absent from these experiments. Studying an interactive robot help-seeking scenario, we used a VR simulation with haptic gloves that provide highly realistic tactile and force feedback to examine the effects of haptic sensation on VR-based HRI. We compared participants' sense of presence and their assessments of the robot to a traditional setup using hand controllers. Our results indicate that haptic sensation enhanced participants' social and self-presence in VR and fostered more diverse and natural bodily engagement. Additionally, haptic sensations significantly influenced participants' affective-related perceptions of the robot. Our study provides insights to guide HRI researchers in building VR-based simulations that better align with their study contexts and objectives.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u89e6\u89c9\u611f\u77e5\u5728\u865a\u62df\u73b0\u5b9e\uff08VR\uff09\u4e2d\u5bf9\u4eba\u673a\u4ea4\u4e92(HRI)\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u865a\u62df\u73b0\u5b9e\u6280\u672f\u5728\u6a21\u62df\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u65e5\u76ca\u666e\u53ca\uff0c\u7814\u7a76\u4eba\u5458\u9700\u8981\u63d0\u9ad8\u5b9e\u9a8c\u7684\u6709\u6548\u6027\u548c\u771f\u5b9e\u6027\u3002", "method": "\u4f7f\u7528\u914d\u5907\u89e6\u89c9\u624b\u5957\u7684\u865a\u62df\u73b0\u5b9e\u4eff\u771f\uff0c\u6bd4\u8f83\u89e6\u89c9\u611f\u77e5\u4e0e\u4f20\u7edf\u624b\u67c4\u63a7\u5236\u5668\u5728\u589e\u5f3a\u53c2\u4e0e\u8005\u7684\u5b58\u5728\u611f\u548c\u673a\u5668\u4eba\u8bc4\u4ef7\u4e0a\u7684\u5f71\u54cd\u3002", "result": "\u89e6\u89c9\u611f\u77e5\u663e\u8457\u589e\u5f3a\u4e86\u53c2\u4e0e\u8005\u7684\u793e\u4ea4\u548c\u81ea\u6211\u5b58\u5728\u611f\uff0c\u5e76\u5f71\u54cd\u4e86\u4ed6\u4eec\u5bf9\u673a\u5668\u4eba\u7684\u60c5\u611f\u611f\u77e5\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4eba\u673a\u4ea4\u4e92\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u5728\u6784\u5efa\u57fa\u4e8eVR\u7684\u4eff\u771f\u65f6\u7684\u6307\u5bfc\u4ee5\u66f4\u597d\u5730\u6ee1\u8db3\u7814\u7a76\u80cc\u666f\u548c\u76ee\u6807\u3002"}}
{"id": "2601.04601", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.04601", "abs": "https://arxiv.org/abs/2601.04601", "authors": ["Xinyan Yu", "Julie Stephany Berrio Perez", "Marius Hoggenm\u00fcller", "Martin Tomitsch", "Tram Thi Minh Tran", "Stewart Worrall", "Wendy Ju"], "title": "The UnScripted Trip: Fostering Policy Discussion on Future Human-Vehicle Collaboration in Autonomous Driving Through Design-Oriented Methods", "comment": null, "summary": "The rapid advancement of autonomous vehicle (AV) technologies is fundamentally reshaping paradigms of human-vehicle collaboration, raising not only an urgent need for innovative design solutions but also for policies that address corresponding broader tensions in society. To bridge the gap between HCI research and policy making, this workshop will bring together researchers and practitioners in the automotive community to explore AV policy directions through collaborative speculation on the future of AVs. We designed The UnScripted Trip, a card game rooted in fictional narratives of autonomous mobility, to surface tensions around human-vehicle collaboration in future AV scenarios and to provoke critical reflections on design solutions and policy directions. Our goal is to provide an engaging, participatory space and method for automotive researchers, designers, and industry practitioners to collectively explore and shape the future of human-vehicle collaboration and its policy implications.", "AI": {"tldr": "\u672c\u5de5\u4f5c\u574a\u901a\u8fc7\u8bbe\u8ba1\u6e38\u620f\u63a2\u7d22\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u5e26\u6765\u7684\u793e\u4f1a\u5f20\u529b\uff0c\u65e8\u5728\u63a8\u52a8\u6c7d\u8f66\u9886\u57df\u7684\u8bbe\u8ba1\u89e3\u51b3\u65b9\u6848\u4e0e\u653f\u7b56\u5236\u5b9a\u3002", "motivation": "\u5e94\u5bf9\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u8fc5\u731b\u53d1\u5c55\u5e26\u6765\u7684\u65b0\u7684\u4eba-\u8f66\u534f\u4f5c\u6311\u6218\uff0c\u4fc3\u8fdbHCI\u7814\u7a76\u4e0e\u653f\u7b56\u5236\u5b9a\u7684\u7ed3\u5408\u3002", "method": "\u901a\u8fc7\u4e3e\u529e\u5de5\u4f5c\u574a\u548c\u8bbe\u8ba1\u6e38\u620f\u6765\u63a2\u8ba8\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u653f\u7b56\u65b9\u5411", "result": "\u901a\u8fc7\u53c2\u4e0e\u5f0f\u7684\u5361\u724c\u6e38\u620f\uff0c\u6fc0\u53d1\u53c2\u4e0e\u8005\u5bf9\u672a\u6765\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u7684\u601d\u8003\uff0c\u63a2\u7d22\u4eba-\u8f66\u534f\u4f5c\u7684\u5f20\u529b\u53ca\u653f\u7b56\u5f71\u54cd\u3002", "conclusion": "\u901a\u8fc7\u96c6\u4f53\u63a2\u8ba8\uff0c\u4fc3\u8fdb\u4eba-\u8f66\u534f\u4f5c\u8bbe\u8ba1\u548c\u653f\u7b56\u7684\u521b\u65b0\u53d1\u5c55\u3002"}}
{"id": "2601.04334", "categories": ["cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.04334", "abs": "https://arxiv.org/abs/2601.04334", "authors": ["Amit Jain", "Richard Linares"], "title": "Autonomous Reasoning for Spacecraft Control: A Large Language Model Framework with Group Relative Policy Optimization", "comment": null, "summary": "This paper presents a learning-based guidance-and-control approach that couples a reasoning-enabled Large Language Model (LLM) with Group Relative Policy Optimization (GRPO). A two-stage procedure consisting of Supervised Fine-Tuning (SFT) to learn formatting and control primitives, followed by GRPO for interaction-driven policy improvement, trains controllers for each environment. The framework is demonstrated on four control problems spanning a gradient of dynamical complexity, from canonical linear systems through nonlinear oscillatory dynamics to three-dimensional spacecraft attitude control with gyroscopic coupling and thrust constraints. Results demonstrate that an LLM with explicit reasoning, optimized via GRPO, can synthesize feasible stabilizing policies under consistent training settings across both linear and nonlinear systems. The two-stage training methodology enables models to generate control sequences while providing human-readable explanations of their decision-making process. This work establishes a foundation for applying GRPO-based reasoning to autonomous control systems, with potential applications in aerospace and other safety-critical domains.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u6307\u5bfc\u4e0e\u63a7\u5236\u65b9\u6cd5\uff0c\u5c06\u5177\u5907\u63a8\u7406\u80fd\u529b\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u7fa4\u4f53\u76f8\u5bf9\u653f\u7b56\u4f18\u5316\uff08GRPO\uff09\u76f8\u7ed3\u5408\uff0c\u7ecf\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\uff0c\u5728\u4e0d\u540c\u63a7\u5236\u95ee\u9898\u4e0a\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u63a7\u5236\u80fd\u529b\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u5982\u4f55\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u4e0e\u63a7\u5236\u7b56\u7565\u4f18\u5316\u7ed3\u5408\uff0c\u4ee5\u63d0\u5347\u81ea\u4e3b\u63a7\u5236\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u9996\u5148\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u4ee5\u5b66\u4e60\u683c\u5f0f\u548c\u63a7\u5236\u539f\u8bed\uff0c\u7136\u540e\u5229\u7528GRPO\u8fdb\u884c\u57fa\u4e8e\u4e92\u52a8\u7684\u7b56\u7565\u6539\u8fdb\u3002", "result": "\u7814\u7a76\u5c55\u793a\uff0c\u5728\u4fdd\u6301\u4e00\u81f4\u7684\u8bad\u7ec3\u8bbe\u7f6e\u4e0b\uff0c\u5177\u5907\u663e\u5f0f\u63a8\u7406\u7684LLM\u53ef\u4ee5\u4e3a\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u7cfb\u7edf\u5408\u6210\u53ef\u884c\u7684\u7a33\u5b9a\u653f\u7b56\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5c06GRPO\u4e0e\u63a8\u7406\u7ed3\u5408\u5e94\u7528\u4e8e\u81ea\u4e3b\u63a7\u5236\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5177\u6709\u5728\u822a\u7a7a\u822a\u5929\u53ca\u5176\u4ed6\u5b89\u5168\u5173\u952e\u9886\u57df\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.04630", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.04630", "abs": "https://arxiv.org/abs/2601.04630", "authors": ["Xiyuan Zhu", "Wenhan Lyu", "Chaochao Fu", "Yilin Wang", "Jie Zheng", "Qiyue Tan", "Qianhe Chen", "Yixin Yu", "Ran Wang"], "title": "RecruitScope: A Visual Analytics System for Multidimensional Recruitment Data Analysis", "comment": null, "summary": "Online recruitment platforms have become the dominant channel for modern hiring, yet most platforms offer only basic filtering capabilities, such as job title, keyword, and salary range. This hinders comprehensive analysis of multi-attribute relationships and job market patterns across different scales. We present RecruitScope, a visual analytics system designed to support multidimensional and cross-level exploration of recruitment data for job seekers and employers, particularly HR specialists. Through coordinated visualizations, RecruitScope enables users to analyze job positions and salary patterns from multiple perspectives, interpret industry dynamics at the macro level, and identify emerging positions at the micro level. We demonstrate the effectiveness of RecruitScope through case studies that reveal regional salary distribution patterns, characterize industry growth trajectories, and discover high-demand emerging roles in the job market.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86RecruitScope\uff0c\u4e00\u4e2a\u652f\u6301\u62db\u8058\u6570\u636e\u591a\u7ef4\u5206\u6790\u7684\u53ef\u89c6\u5206\u6790\u7cfb\u7edf\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u62db\u8058\u5e73\u53f0\u529f\u80fd\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u5728\u7ebf\u62db\u8058\u5e73\u53f0\u4ec5\u63d0\u4f9b\u57fa\u672c\u7684\u7b5b\u9009\u529f\u80fd\uff0c\u9650\u5236\u4e86\u5bf9\u591a\u5c5e\u6027\u5173\u7cfb\u548c\u62db\u8058\u5e02\u573a\u6a21\u5f0f\u7684\u6df1\u5165\u5206\u6790\u3002", "method": "\u5f00\u53d1\u4e86RecruitScope\u53ef\u89c6\u5206\u6790\u7cfb\u7edf\uff0c\u901a\u8fc7\u534f\u8c03\u53ef\u89c6\u5316\u652f\u6301\u5bf9\u62db\u8058\u6570\u636e\u7684\u591a\u7ef4\u548c\u8de8\u5c42\u6b21\u63a2\u8ba8\u3002", "result": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u533a\u57df\u5de5\u8d44\u5206\u5e03\u6a21\u5f0f\u3001\u884c\u4e1a\u589e\u957f\u8f68\u8ff9\u548c\u65b0\u5174\u89d2\u8272\u7684\u9700\u6c42\u7b49\u53d1\u73b0\u3002", "conclusion": "RecruitScope\u6709\u6548\u652f\u6301\u6c42\u804c\u8005\u548cHR\u4e13\u5bb6\u5bf9\u62db\u8058\u6570\u636e\u7684\u6df1\u5165\u5206\u6790\uff0c\u63ed\u793a\u4e86\u884c\u4e1a\u52a8\u6001\u548c\u5e02\u573a\u8d8b\u52bf\u3002"}}
{"id": "2601.04356", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.04356", "abs": "https://arxiv.org/abs/2601.04356", "authors": ["Zhengtong Xu", "Yuki Shirai"], "title": "UNIC: Learning Unified Multimodal Extrinsic Contact Estimation", "comment": null, "summary": "Contact-rich manipulation requires reliable estimation of extrinsic contacts-the interactions between a grasped object and its environment which provide essential contextual information for planning, control, and policy learning. However, existing approaches often rely on restrictive assumptions, such as predefined contact types, fixed grasp configurations, or camera calibration, that hinder generalization to novel objects and deployment in unstructured environments. In this paper, we present UNIC, a unified multimodal framework for extrinsic contact estimation that operates without any prior knowledge or camera calibration. UNIC directly encodes visual observations in the camera frame and integrates them with proprioceptive and tactile modalities in a fully data-driven manner. It introduces a unified contact representation based on scene affordance maps that captures diverse contact formations and employs a multimodal fusion mechanism with random masking, enabling robust multimodal representation learning. Extensive experiments demonstrate that UNIC performs reliably. It achieves a 9.6 mm average Chamfer distance error on unseen contact locations, performs well on unseen objects, remains robust under missing modalities, and adapts to dynamic camera viewpoints. These results establish extrinsic contact estimation as a practical and versatile capability for contact-rich manipulation.", "AI": {"tldr": "UNIC\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u80fd\u591f\u5728\u6ca1\u6709\u5148\u9a8c\u77e5\u8bc6\u6216\u76f8\u673a\u6821\u51c6\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u5916\u90e8\u63a5\u89e6\u4f30\u8ba1\uff0c\u8868\u73b0\u51fa\u9ad8\u53ef\u9760\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5bf9\u9884\u5b9a\u4e49\u63a5\u89e6\u7c7b\u578b\u3001\u56fa\u5b9a\u6293\u53d6\u914d\u7f6e\u6216\u76f8\u673a\u6821\u51c6\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u5bf9\u65b0\u7269\u4f53\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u7f16\u7801\u89c6\u89c9\u89c2\u5bdf\u5e76\u4e0e\u672c\u4f53\u548c\u89e6\u89c9\u6a21\u6001\u96c6\u6210\uff0c\u91c7\u53d6\u5b8c\u5168\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u3002", "result": "\u5728\u672a\u89c1\u63a5\u89e6\u4f4d\u7f6e\u8fbe\u5230\u4e869.6mm\u7684\u5e73\u5747Chamfer\u8ddd\u79bb\u8bef\u5dee\uff0c\u80fd\u5728\u7f3a\u5931\u6a21\u6001\u4e0b\u7a33\u5b9a\u8868\u73b0\uff0c\u5bf9\u52a8\u6001\u76f8\u673a\u89c6\u89d2\u5177\u6709\u826f\u597d\u7684\u9002\u5e94\u6027\u3002", "conclusion": "UNIC\u663e\u8457\u63d0\u9ad8\u4e86\u5916\u90e8\u63a5\u89e6\u4f30\u8ba1\u7684\u53ef\u9760\u6027\u548c\u9002\u5e94\u6027\uff0c\u80fd\u591f\u5904\u7406\u672a\u89c1\u7269\u4f53\u548c\u52a8\u6001\u73af\u5883\u3002"}}
{"id": "2601.04680", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.04680", "abs": "https://arxiv.org/abs/2601.04680", "authors": ["Chaerin Yu", "Chihun Choi", "Sunjae Lee", "Hyosu Kim", "Steven Y. Ko", "Young-Bae Ko", "Sangeun Oh"], "title": "Leveraging LLMs for Efficient and Personalized Smart Home Automation", "comment": null, "summary": "The proliferation of smart home devices has increased the complexity of controlling and managing them, leading to user fatigue. In this context, large language models (LLMs) offer a promising solution by enabling natural-language interfaces for Internet of Things (IoT) control. However, existing LLM-based approaches suffer from unreliable and inefficient device control due to the non-deterministic nature of LLMs, high inference latency and cost, and limited personalization. To address these challenges, we present IoTGPT, an LLM-based smart home agent designed to execute IoT commands in a reliable, efficient, and personalized manner. Inspired by how humans manage complex tasks, IoTGPT decomposes user instructions into subtasks and memorizes them. By reusing learned subtasks, subsequent instructions can be processed more efficiently with fewer LLM calls, improving reliability and reducing both latency and cost. IoTGPT also supports fine-grained personalization by adapting individual subtasks to user preferences. Our evaluation demonstrates that IoTGPT outperforms baselines in accuracy, latency/cost, and personalization, while reducing user workload.", "AI": {"tldr": "IoTGPT\u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u5bb6\u5c45\u4ee3\u7406\uff0c\u65e8\u5728\u4ee5\u53ef\u9760\u3001\u9ad8\u6548\u548c\u4e2a\u6027\u5316\u7684\u65b9\u5f0f\u6267\u884c\u7269\u8054\u7f51\u547d\u4ee4\uff0c\u901a\u8fc7\u5206\u89e3\u7528\u6237\u6307\u4ee4\u548c\u8bb0\u5fc6\u5b50\u4efb\u52a1\u6765\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u5bb6\u5c45\u8bbe\u5907\u7684\u589e\u591a\uff0c\u7528\u6237\u9762\u4e34\u590d\u6742\u7684\u63a7\u5236\u7ba1\u7406\uff0c\u5bfc\u81f4\u75b2\u52b3\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u5b58\u5728\u53ef\u9760\u6027\u4f4e\u3001\u6548\u7387\u4f4e\u548c\u4e2a\u6027\u5316\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "IoTGPT\u901a\u8fc7\u5c06\u7528\u6237\u6307\u4ee4\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\u5e76\u8fdb\u884c\u8bb0\u5fc6\uff0c\u4ee5\u63d0\u9ad8\u6307\u4ee4\u5904\u7406\u7684\u6548\u7387\uff0c\u51cf\u5c11LLM\u8c03\u7528\uff0c\u652f\u6301\u7528\u6237\u504f\u597d\u7684\u7cbe\u7ec6\u4e2a\u6027\u5316\u3002", "result": "IoTGPT\u5728\u51c6\u786e\u6027\u3001\u5ef6\u8fdf/\u6210\u672c\u548c\u4e2a\u6027\u5316\u65b9\u9762\u8d85\u8fc7\u4e86\u57fa\u51c6\u6a21\u578b\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u7528\u6237\u5de5\u4f5c\u8d1f\u62c5\u3002", "conclusion": "IoTGPT\u6709\u6548\u5730\u89e3\u51b3\u4e86\u667a\u80fd\u5bb6\u5c45\u63a7\u5236\u4e2d\u7684\u6311\u6218\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u4e86\u66f4\u4f18\u8d28\u7684\u4f53\u9a8c\u3002"}}
{"id": "2601.04401", "categories": ["cs.RO", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.04401", "abs": "https://arxiv.org/abs/2601.04401", "authors": ["Arsyi Aziz", "Peng Wei"], "title": "Transformer-based Multi-agent Reinforcement Learning for Separation Assurance in Structured and Unstructured Airspaces", "comment": "9 pages, 4 figures, 4 tables. Presented at SESAR Innovation Days 2025", "summary": "Conventional optimization-based metering depends on strict adherence to precomputed schedules, which limits the flexibility required for the stochastic operations of Advanced Air Mobility (AAM). In contrast, multi-agent reinforcement learning (MARL) offers a decentralized, adaptive framework that can better handle uncertainty, required for safe aircraft separation assurance. Despite this advantage, current MARL approaches often overfit to specific airspace structures, limiting their adaptability to new configurations. To improve generalization, we recast the MARL problem in a relative polar state space and train a transformer encoder model across diverse traffic patterns and intersection angles. The learned model provides speed advisories to resolve conflicts while maintaining aircraft near their desired cruising speeds. In our experiments, we evaluated encoder depths of 1, 2, and 3 layers in both structured and unstructured airspaces, and found that a single encoder configuration outperformed deeper variants, yielding near-zero near mid-air collision rates and shorter loss-of-separation infringements than the deeper configurations. Additionally, we showed that the same configuration outperforms a baseline model designed purely with attention. Together, our results suggest that the newly formulated state representation, novel design of neural network architecture, and proposed training strategy provide an adaptable and scalable decentralized solution for aircraft separation assurance in both structured and unstructured airspaces.", "AI": {"tldr": "\u4f20\u7edf\u4f18\u5316\u4f9d\u8d56\u4e25\u683c\u65f6\u95f4\u8868\uff0c\u9650\u5236\u4e86AAM\u7684\u7075\u6d3b\u6027\uff1b\u800c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u53bb\u4e2d\u5fc3\u5316\uff0c\u81ea\u9002\u5e94\u6846\u67b6\u3002\u901a\u8fc7\u76f8\u5bf9\u6781\u5750\u6807\u91cd\u65b0\u6784\u9020\uff0c\u6d4b\u8bd5\u4e0d\u540c\u6df1\u5ea6\u7684\u7f16\u7801\u5668\uff0c\u5355\u5c42\u914d\u7f6e\u4f18\u4e8e\u6df1\u5c42\u53d8\u4f53\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u5e94\u6027\u5f3a\u7684\u98de\u673a\u5206\u79bb\u4fdd\u969c\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u4f18\u5316\u7684\u8ba1\u91cf\u4f9d\u8d56\u4e8e\u4e25\u683c\u9075\u5faa\u9884\u5b9a\u4e49\u7684\u65f6\u95f4\u8868\uff0c\u9650\u5236\u4e86\u5bf9\u5148\u8fdb\u7a7a\u4e2d\u79fb\u52a8\u7684\u968f\u673a\u64cd\u4f5c\u6240\u9700\u7684\u7075\u6d3b\u6027\u3002", "method": "\u5c06\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u91cd\u65b0\u6784\u9020\u4e3a\u76f8\u5bf9\u6781\u5750\u6807\u72b6\u6001\u7a7a\u95f4\uff0c\u5e76\u5728\u591a\u6837\u7684\u4ea4\u901a\u6a21\u5f0f\u548c\u4ea4\u53c9\u89d2\u5ea6\u4e0a\u8bad\u7ec3\u53d8\u538b\u5668\u7f16\u7801\u5668\u6a21\u578b\u3002", "result": "\u5728\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u7a7a\u57df\u4e2d\u5b9e\u9a8c\uff0c\u53d1\u73b0\u5355\u5c42\u7f16\u7801\u5668\u7684\u914d\u7f6e\u5728\u63a5\u8fd1\u96f6\u7684\u7a7a\u4e2d\u78b0\u649e\u7387\u548c\u66f4\u77ed\u7684\u5206\u79bb\u8fdd\u89c4\u65f6\u95f4\u65b9\u9762\u4f18\u4e8e\u66f4\u6df1\u7684\u53d8\u4f53\uff0c\u5e76\u4e14\u4f18\u4e8e\u7eaf\u7cb9\u57fa\u4e8e\u6ce8\u610f\u529b\u8bbe\u8ba1\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u65b0\u63d0\u51fa\u7684\u72b6\u6001\u8868\u793a\u3001\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u8bbe\u8ba1\u548c\u8bad\u7ec3\u7b56\u7565\u4e3a\u7ed3\u6784\u5316\u4e0e\u975e\u7ed3\u6784\u5316\u7a7a\u57df\u4e2d\u7684\u98de\u673a\u5206\u79bb\u4fdd\u969c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5177\u9002\u5e94\u6027\u548c\u53ef\u6269\u5c55\u6027\u7684\u53bb\u4e2d\u5fc3\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.04781", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.04781", "abs": "https://arxiv.org/abs/2601.04781", "authors": ["Sophie Villenave", "Pierre Raimbaud", "Guillaume Lavou\u00e9"], "title": "Dynamic Thermal Feedback in Highly Immersive VR Scenarios: a Multimodal Analysis of User Experience", "comment": "15 pages, 9 figures. This work has been submitted to the IEEE for possible publication", "summary": "Thermal feedback is critical to a range of Virtual Reality (VR) applications, such as firefighting training or thermal comfort simulation. Previous studies showed that adding congruent thermal feedback positively influences User eXperience (UX). However, existing work did not compare different levels of thermal feedback quality and mostly used less immersive virtual environments. To investigate these gaps in the scientific literature, we conducted a within-participant user study in two highly-immersive scenarios, Desert Island (n=25) and Snowy Mountains (n=24). Participants explored the scenarios in three conditions (Audio-Visual only, Static-Thermal Feedback, and Dynamic-Thermal Feedback). To assess the complex and subtle effects of thermal feedback on UX, we performed a multimodal analysis by crossing data from questionnaires, semi-structured interviews, and behavioral indicators. Our results show that despite an already high level of presence in the Audio-Visual only condition, adding thermal feedback increased presence further. Comparison between levels of thermal feedback quality showed no significant difference in UX questionnaires, however this result is nuanced according to participant profiles and interviews. Furthermore, we show that although the order of passage did not influence UX directly, it influenced user behavior. We propose guidelines for the use of thermal feedback in VR, and the design of studies in complex multisensory scenarios.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u4e0d\u540c\u6c34\u5e73\u70ed\u53cd\u9988\u5bf9\u865a\u62df\u73b0\u5b9e\u4e2d\u7684\u7528\u6237\u4f53\u9a8c\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u70ed\u53cd\u9988\u80fd\u8fdb\u4e00\u6b65\u63d0\u9ad8\u7528\u6237\u7684\u5728\u573a\u611f\uff0c\u63d0\u51fa\u4e86\u70ed\u53cd\u9988\u7684\u5e94\u7528\u6307\u5bfc\u548c\u7814\u7a76\u8bbe\u8ba1\u5efa\u8bae\u3002", "motivation": "\u4e3a\u4e86\u586b\u8865\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u4e0d\u540c\u70ed\u53cd\u9988\u8d28\u91cf\u6bd4\u8f83\u53ca\u6c89\u6d78\u5f0f\u865a\u62df\u73af\u5883\u4e2d\u70ed\u53cd\u9988\u5f71\u54cd\u7684\u7a7a\u767d\u3002", "method": "\u5728\u4e24\u4e2a\u6c89\u6d78\u5f0f\u573a\u666f\u4e2d\u8fdb\u884c\u53c2\u4e0e\u8005\u5185\u7684\u7528\u6237\u7814\u7a76\uff0c\u6bd4\u8f83\u4e09\u79cd\u53cd\u9988\u6761\u4ef6\uff0c\u5e76\u901a\u8fc7\u95ee\u5377\u3001\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u548c\u884c\u4e3a\u6307\u6807\u8fdb\u884c\u591a\u6a21\u6001\u5206\u6790\u3002", "result": "\u52a0\u5165\u70ed\u53cd\u9988\u540e\uff0c\u7528\u6237\u5728\u573a\u611f\u663e\u8457\u589e\u5f3a\uff0c\u4f46\u70ed\u53cd\u9988\u8d28\u91cf\u7684\u4e0d\u540c\u5728\u7528\u6237\u4f53\u9a8c\u95ee\u5377\u4e2d\u672a\u663e\u793a\u663e\u8457\u5dee\u5f02\u3002\u53c2\u4e0e\u8005\u7279\u5f81\u548c\u884c\u4e3a\u53d7\u60c5\u666f\u987a\u5e8f\u5f71\u54cd\uff0c\u76f8\u5173\u5efa\u8bae\u5df2\u63d0\u51fa\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u70ed\u53cd\u9988\u867d\u7136\u5728\u7528\u6237\u4f53\u9a8c\u95ee\u5377\u4e2d\u7684\u6548\u679c\u65e0\u663e\u8457\u5dee\u5f02\uff0c\u4f46\u5176\u5f71\u54cd\u56e0\u53c2\u4e0e\u8005\u7279\u5f81\u548c\u8bbf\u8c08\u800c\u6709\u6240\u4e0d\u540c\uff1b\u53c2\u4e0e\u8005\u884c\u4e3a\u4e5f\u53d7\u60c5\u666f\u6b21\u5e8f\u7684\u5f71\u54cd\u3002"}}
{"id": "2601.04493", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.04493", "abs": "https://arxiv.org/abs/2601.04493", "authors": ["James M. Ferguson", "Alan Kuntz", "Tucker Hermans"], "title": "Fast Continuum Robot Shape and External Load State Estimation on SE(3)", "comment": "Public preprint for ICRA 2026", "summary": "Previous on-manifold approaches to continuum robot state estimation have typically adopted simplified Cosserat rod models, which cannot directly account for actuation inputs or external loads. We introduce a general framework that incorporates uncertainty models for actuation (e.g., tendon tensions), applied forces and moments, process noise, boundary conditions, and arbitrary backbone measurements. By adding temporal priors across time steps, our method additionally performs joint estimation in both the spatial (arclength) and temporal domains, enabling full \\textit{spacetime} state estimation. Discretizing the arclength domain yields a factor graph representation of the continuum robot model, which can be exploited for fast batch sparse nonlinear optimization in the style of SLAM. The framework is general and applies to a broad class of continuum robots; as illustrative cases, we show (i) tendon-driven robots in simulation, where we demonstrate real-time kinematics with uncertainty, tip force sensing from position feedback, and distributed load estimation from backbone strain, and (ii) a surgical concentric tube robot in experiment, where we validate accurate kinematics and tip force estimation, highlighting potential for surgical palpation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8fde\u7eed\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\uff0c\u80fd\u591f\u5904\u7406\u591a\u79cd\u4e0d\u786e\u5b9a\u6027\u56e0\u7d20\uff0c\u5e76\u8fdb\u884c\u4e86\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "motivation": "\u5148\u524d\u7684\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\u8fc7\u4e8e\u7b80\u5316\uff0c\u65e0\u6cd5\u6709\u6548\u8003\u8651\u9a71\u52a8\u8f93\u5165\u548c\u5916\u90e8\u8d1f\u8377\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u66f4\u5168\u9762\u7684\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u5c06\u65f6\u95f4\u5148\u9a8c\u5f15\u5165\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u57df\u7684\u8054\u5408\u4f30\u8ba1\uff0c\u5e76\u91c7\u7528\u79bb\u6563\u5316\u6280\u672f\u751f\u6210\u4e86\u56e0\u5b50\u56fe\u8868\u793a\uff0c\u4ee5\u8fdb\u884c\u5feb\u901f\u975e\u7ebf\u6027\u4f18\u5316\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u6846\u67b6\uff0c\u80fd\u591f\u8fdb\u884c\u8fde\u7eed\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\uff0c\u5305\u62ec\u9a71\u52a8\u4e0d\u786e\u5b9a\u6027\u3001\u5916\u529b\u4e0e\u529b\u77e9\u3001\u8fc7\u7a0b\u566a\u58f0\u3001\u8fb9\u754c\u6761\u4ef6\u548c\u4efb\u610f\u4e3b\u5e72\u6d4b\u91cf\u3002", "conclusion": "\u8be5\u6846\u67b6\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u8fde\u7eed\u673a\u5668\u4eba\uff0c\u5e76\u5728\u4eff\u771f\u548c\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u51c6\u786e\u7684\u8fd0\u52a8\u5b66\u548c\u529b\u4f30\u8ba1\u3002"}}
{"id": "2601.04915", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2601.04915", "abs": "https://arxiv.org/abs/2601.04915", "authors": ["Miki Okamura", "Shuhey Koyama", "Li Jingjing", "Yoichi Ochiai"], "title": "OnomaCompass: A Texture Exploration Interface that Shuttles between Words and Images", "comment": null, "summary": "Humans can finely perceive material textures, yet articulating such somatic impressions in words is a cognitive bottleneck in design ideation. We present OnomaCompass, a web-based exploration system that links sound-symbolic onomatopoeia and visual texture representations to support early-stage material discovery. Instead of requiring users to craft precise prompts for generative AI, OnomaCompass provides two coordinated latent-space maps--one for texture images and one for onomatopoeic term--built from an authored dataset of invented onomatopoeia and corresponding textures generated via Stable Diffusion. Users can navigate both spaces, trigger cross-modal highlighting, curate findings in a gallery, and preview textures applied to objects via an image-editing model. The system also supports video interpolation between selected textures and re-embedding of extracted frames to form an emergent exploration loop. We conducted a within-subjects study with 11 participants comparing OnomaCompass to a prompt-based image-generation workflow using Gemini 2.5 Flash Image (\"Nano Banana\"). OnomaCompass significantly reduced workload (NASA-TLX overall, mental demand, effort, and frustration; p < .05) and increased hedonic user experience (UEQ), while usability (SUS) favored the baseline. Qualitative findings indicate that OnomaCompass helps users externalize vague sensory expectations and promotes serendipitous discovery, but also reveals interaction challenges in spatial navigation. Overall, leveraging sound symbolism as a lightweight cue offers a complementary approach to Kansei-driven material ideation beyond prompt-centric generation.", "AI": {"tldr": "OnomaCompass\u662f\u4e00\u4e2a\u57fa\u4e8e\u7f51\u7edc\u7684\u63a2\u7d22\u7cfb\u7edf\uff0c\u901a\u8fc7\u58f0\u97f3\u8c61\u5f81\u7684\u62df\u58f0\u8bcd\u548c\u89c6\u89c9\u7eb9\u7406\u8868\u793a\u652f\u6301\u6750\u6599\u53d1\u73b0\uff0c\u663e\u8457\u964d\u4f4e\u7528\u6237\u5de5\u4f5c\u8d1f\u62c5\u5e76\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u4eba\u7c7b\u80fd\u591f\u7ec6\u81f4\u611f\u77e5\u6750\u6599\u7eb9\u7406\uff0c\u4f46\u5c06\u8fd9\u4e9b\u611f\u89c9\u7528\u8bed\u8a00\u8868\u8fbe\u51fa\u6765\u5728\u8bbe\u8ba1\u6784\u601d\u4e2d\u5b58\u5728\u74f6\u9888\u3002", "method": "OnomaCompass\u63d0\u4f9b\u4e24\u4e2a\u534f\u8c03\u7684\u6f5c\u5728\u7a7a\u95f4\u5730\u56fe\uff0c\u4e00\u4e2a\u7528\u4e8e\u7eb9\u7406\u56fe\u50cf\uff0c\u53e6\u4e00\u4e2a\u7528\u4e8e\u62df\u58f0\u8bcd\uff0c\u57fa\u4e8e\u53d1\u660e\u7684\u62df\u58f0\u8bcd\u548c\u901a\u8fc7Stable Diffusion\u751f\u6210\u7684\u76f8\u5e94\u7eb9\u7406\u6784\u5efa\u3002\u7528\u6237\u53ef\u4ee5\u5728\u4e24\u4e2a\u7a7a\u95f4\u4e2d\u5bfc\u822a\uff0c\u89e6\u53d1\u8de8\u6a21\u6001\u9ad8\u4eae\uff0c\u5e76\u4f7f\u7528\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u9884\u89c8\u88ab\u5e94\u7528\u4e8e\u7269\u4f53\u7684\u7eb9\u7406\u3002", "result": "\u4e0e\u57fa\u4e8e\u63d0\u793a\u7684\u56fe\u50cf\u751f\u6210\u5de5\u4f5c\u6d41\u7a0b\u76f8\u6bd4\uff0cOnomaCompass\u663e\u8457\u964d\u4f4e\u4e86\u5de5\u4f5c\u8d1f\u62c5\uff0c\u5e76\u589e\u5f3a\u4e86\u7528\u6237\u6109\u60a6\u4f53\u9a8c\u3002", "conclusion": "\u5229\u7528\u58f0\u97f3\u8c61\u5f81\u4f5c\u4e3a\u8f7b\u91cf\u63d0\u793a\uff0c\u4e3a\u8d85\u8d8a\u57fa\u4e8e\u63d0\u793a\u751f\u6210\u7684\u6750\u6599\u6784\u601d\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e92\u8865\u65b9\u6cd5\u3002"}}
{"id": "2601.04511", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04511", "abs": "https://arxiv.org/abs/2601.04511", "authors": ["Zhenglong Luo", "Zhiyong Chen", "Aoxiang Liu"], "title": "Multiagent Reinforcement Learning with Neighbor Action Estimation", "comment": null, "summary": "Multiagent reinforcement learning, as a prominent intelligent paradigm, enables collaborative decision-making within complex systems. However, existing approaches often rely on explicit action exchange between agents to evaluate action value functions, which is frequently impractical in real-world engineering environments due to communication constraints, latency, energy consumption, and reliability requirements. From an artificial intelligence perspective, this paper proposes an enhanced multiagent reinforcement learning framework that employs action estimation neural networks to infer agent behaviors. By integrating a lightweight action estimation module, each agent infers neighboring agents' behaviors using only locally observable information, enabling collaborative policy learning without explicit action sharing. This approach is fully compatible with standard TD3 algorithms and scalable to larger multiagent systems. At the engineering application level, this framework has been implemented and validated in dual-arm robotic manipulation tasks: two robotic arms collaboratively lift objects. Experimental results demonstrate that this approach significantly enhances the robustness and deployment feasibility of real-world robotic systems while reducing dependence on information infrastructure. Overall, this research advances the development of decentralized multiagent artificial intelligence systems while enabling AI to operate effectively in dynamic, information-constrained real-world environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u4ee5\u6539\u5584\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u4fe1\u606f\u53d7\u9650\u73af\u5883\u4e2d\u7684\u51b3\u7b56\u80fd\u529b\uff0c\u589e\u5f3a\u5b9e\u9645\u5e94\u7528\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5bf9\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u663e\u5f0f\u884c\u52a8\u4ea4\u6362\u4f9d\u8d56\u8fc7\u5927\uff0c\u8fd9\u5728\u5b9e\u9645\u5de5\u7a0b\u4e2d\u5b58\u5728\u901a\u4fe1\u9650\u5236\u548c\u53ef\u9760\u6027\u7b49\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u96c6\u6210\u8f7b\u91cf\u52a8\u4f5c\u4f30\u8ba1\u6a21\u5757\uff0c\u667a\u80fd\u4f53\u4ec5\u57fa\u4e8e\u5c40\u90e8\u53ef\u89c2\u6d4b\u4fe1\u606f\u63a8\u65ad\u90bb\u5c45\u667a\u80fd\u4f53\u7684\u884c\u4e3a\uff0c\u652f\u6301\u534f\u4f5c\u7b56\u7565\u5b66\u4e60\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u52a8\u4f5c\u4f30\u8ba1\u795e\u7ecf\u7f51\u7edc\u63a8\u65ad\u667a\u80fd\u4f53\u884c\u4e3a\uff0c\u65e0\u9700\u663e\u5f0f\u884c\u52a8\u4ea4\u6362\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u53cc\u81c2\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u53ef\u90e8\u7f72\u6027\u3002"}}
{"id": "2601.05084", "categories": ["cs.HC", "cs.AI", "cs.ET", "eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.05084", "abs": "https://arxiv.org/abs/2601.05084", "authors": ["Niloufar Alavi", "Swati Shah", "Rezvan Alamian", "Stefan Goetz"], "title": "Driver-Intention Prediction with Deep Learning: Real-Time Brain-to-Vehicle Communication", "comment": "6 pages, 7 figures", "summary": "Brain-computer interfaces (BCIs) allow direct communication between the brain and electronics without the need for speech or physical movement. Such interfaces can be particularly beneficial in applications requiring rapid response times, such as driving, where a vehicle's advanced driving assistance systems could benefit from immediate understanding of a driver's intentions. This study presents a novel method for predicting a driver's intention to steer using electroencephalography (EEG) signals through deep learning. A driving simulator created a controlled environment in which participants imagined controlling a vehicle during various driving scenarios, including left and right turns, as well as straight driving. A convolutional neural network (CNN) classified the detected EEG data with minimal pre-processing. Our model achieved an accuracy of 83.7% in distinguishing between the three steering intentions and demonstrated the ability of CNNs to process raw EEG data effectively. The classification accuracy was highest for right-turn segments, which suggests a potential spatial bias in brain activity. This study lays the foundation for more intuitive brain-to-vehicle communication systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7EEG\u4fe1\u53f7\u548c\u6df1\u5ea6\u5b66\u4e60\u9884\u6d4b\u9a7e\u9a76\u8005\u8f6c\u5411\u610f\u56fe\u7684\u65b9\u6cd5\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u4e3a83.7%\u3002", "motivation": "BCI\u6280\u672f\u80fd\u591f\u5728\u6ca1\u6709\u8bed\u8a00\u6216\u8eab\u4f53\u8fd0\u52a8\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5927\u8111\u4e0e\u7535\u5b50\u8bbe\u5907\u4e4b\u95f4\u7684\u76f4\u63a5\u901a\u4fe1\uff0c\u5c24\u5176\u5728\u9700\u8981\u5feb\u901f\u53cd\u5e94\u7684\u5e94\u7528\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u901a\u8fc7\u8111\u7535\u56fe\uff08EEG\uff09\u4fe1\u53f7\u9884\u6d4b\u9a7e\u9a76\u8005\u7684\u8f6c\u5411\u610f\u56fe\uff0c\u5229\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u5206\u7c7bEEG\u6570\u636e", "result": "\u6a21\u578b\u5728\u533a\u5206\u4e09\u79cd\u8f6c\u5411\u610f\u56fe\u65f6\uff0c\u8fbe\u5230\u4e8683.7%\u7684\u51c6\u786e\u7387\uff0c\u5c24\u5176\u662f\u5728\u53f3\u8f6c\u6bb5\u7684\u5206\u7c7b\u51c6\u786e\u7387\u6700\u9ad8\uff0c\u663e\u793a\u51fa\u5927\u8111\u6d3b\u52a8\u53ef\u80fd\u5b58\u5728\u7a7a\u95f4\u504f\u5411\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u66f4\u76f4\u89c2\u7684\u8111-\u8f66\u901a\u4fe1\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.04541", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.04541", "abs": "https://arxiv.org/abs/2601.04541", "authors": ["Gustavo H. Diaz", "A. Sejal Jain", "Matteo Brugnera", "Elian Neppel", "Shreya Santra", "Kentaro Uno", "Kazuya Yoshida"], "title": "Design and Development of Modular Limbs for Reconfigurable Robots on the Moon", "comment": "Author's version of a manuscript accepted at the International Conference on Space Robotics 2025 (iSpaRo 2025). (c) IEEE", "summary": "In this paper, we present the development of 4-DOF robot limbs, which we call Moonbots, designed to connect in various configurations with each other and wheel modules, enabling adaptation to different environments and tasks. These modular components are intended primarily for robotic systems in space exploration and construction on the Moon in our Moonshot project. Such modular robots add flexibility and versatility for space missions where resources are constrained. Each module is driven by a common actuator characterized by a high torque-to-speed ratio, supporting both precise control and dynamic motion when required. This unified actuator design simplifies development and maintenance across the different module types. The paper describes the hardware implementation, the mechanical design of the modules, and the overall software architecture used to control and coordinate them. Additionally, we evaluate the control performance of the actuator under various load conditions to characterize its suitability for modular robot applications. To demonstrate the adaptability of the system, we introduce nine functional configurations assembled from the same set of modules: 4DOF-limb, 8DOF-limb, vehicle, dragon, minimal, quadruped, cargo, cargo-minimal, and bike. These configurations reflect different locomotion strategies and task-specific behaviors, offering a practical foundation for further research in reconfigurable robotic systems.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e864\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u80a2\u4f53Moonbots\u7684\u53d1\u5c55\uff0c\u80fd\u591f\u4ee5\u4e0d\u540c\u914d\u7f6e\u8fde\u63a5\u5f7c\u6b64\u548c\u8f6e\u5b50\u6a21\u5757\uff0c\u9002\u5e94\u4e0d\u540c\u73af\u5883\u548c\u4efb\u52a1\uff0c\u4e3b\u8981\u7528\u4e8e\u592a\u7a7a\u63a2\u7d22\u548c\u6708\u7403\u5efa\u8bbe\u3002", "motivation": "\u5f00\u53d1\u7075\u6d3b\u548c\u591a\u529f\u80fd\u7684\u6a21\u5757\u5316\u673a\u5668\u4eba\uff0c\u4ee5\u5e94\u5bf9\u8d44\u6e90\u6709\u9650\u7684\u592a\u7a7a\u4efb\u52a1\uff0c\u5e76\u652f\u6301\u5728\u6708\u7403\u4e0a\u7684\u63a2\u7d22\u548c\u5efa\u8bbe\u5de5\u4f5c\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u7edf\u4e00\u9a71\u52a8\u5668\u7684\u6a21\u5757\u5316\u7ec4\u4ef6\uff0c\u63cf\u8ff0\u4e86\u786c\u4ef6\u5b9e\u73b0\u3001\u673a\u68b0\u8bbe\u8ba1\u548c\u6574\u4f53\u8f6f\u4ef6\u67b6\u6784\uff0c\u7528\u4e8e\u63a7\u5236\u548c\u534f\u8c03\u8fd9\u4e9b\u6a21\u5757\uff0c\u5e76\u8bc4\u4f30\u9a71\u52a8\u5668\u5728\u4e0d\u540c\u8d1f\u8f7d\u6761\u4ef6\u4e0b\u7684\u63a7\u5236\u6027\u80fd\u3002", "result": "\u63d0\u51fa\u4e86\u4e5d\u79cd\u529f\u80fd\u914d\u7f6e\uff0c\u5305\u62ec4DOF\u80a2\u4f53\u30018DOF\u80a2\u4f53\u3001\u8f66\u8f86\u3001\u9f99\u5f62\u3001\u6700\u5c0f\u3001\u56db\u8db3\u3001\u8d27\u7269\u3001\u8d27\u7269-\u6700\u5c0f\u548c\u81ea\u884c\u8f66\uff0c\u5c55\u793a\u4e86\u7cfb\u7edf\u7684\u9002\u5e94\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u914d\u7f6e\u53cd\u6620\u4e86\u4e0d\u540c\u7684\u8fd0\u52a8\u7b56\u7565\u548c\u4efb\u52a1\u7279\u5b9a\u884c\u4e3a\uff0c\u4e3a\u53ef\u91cd\u6784\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u9645\u57fa\u7840\u3002"}}
{"id": "2601.04657", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.04657", "abs": "https://arxiv.org/abs/2601.04657", "authors": ["Takafumi Sakamoto", "Yugo Takeuchi"], "title": "Model of Spatial Human-Agent Interaction with Consideration for Others", "comment": null, "summary": "Communication robots often need to initiate conversations with people in public spaces. At the same time, such robots must not disturb pedestrians. To handle these two requirements, an agent needs to estimate the communication desires of others based on their behavior and then adjust its own communication activities accordingly. In this study, we construct a computational spatial interaction model that considers others. Consideration is expressed as a quantitative parameter: the amount of adjustment of one's internal state to the estimated internal state of the other. To validate the model, we experimented with a human and a virtual robot interacting in a VR environment. The results show that when the participant moves to the target, a virtual robot with a low consideration value inhibits the participant's movement, while a robot with a higher consideration value did not inhibit the participant's movement. When the participant approached the robot, the robot also exhibited approaching behavior, regardless of the consideration value, thus decreasing the participant's movement. These results appear to verify the proposed model's ability to clarify interactions with consideration for others.", "AI": {"tldr": "\u672c\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u8003\u8651\u4ed6\u4eba\u7684\u8ba1\u7b97\u7a7a\u95f4\u4e92\u52a8\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u6a21\u578b\u80fd\u591f\u6709\u6548\u652f\u6301\u673a\u5668\u4eba\u5728\u516c\u5171\u573a\u6240\u4e0e\u4eba\u8fdb\u884c\u4ea4\u4e92\u65f6\uff0c\u6839\u636e\u4ed6\u4eba\u7684\u884c\u4e3a\u6765\u8c03\u8282\u81ea\u8eab\u7684\u6c9f\u901a\u6d3b\u52a8\u3002", "motivation": "\u63a2\u8ba8\u5982\u4f55\u4f7f\u6c9f\u901a\u673a\u5668\u4eba\u5728\u516c\u5171\u573a\u6240\u6839\u636e\u4ed6\u4eba\u7684\u884c\u4e3a\u6765\u8c03\u8282\u81ea\u8eab\u7684\u6c9f\u901a\u6d3b\u52a8\uff0c\u4ee5\u6ee1\u8db3\u4e0d\u6253\u6270\u884c\u4eba\u7684\u9700\u6c42\u3002", "method": "\u5728\u865a\u62df\u73b0\u5b9e\u73af\u5883\u4e2d\u8fdb\u884c\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u4e92\u52a8\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5177\u6709\u4f4e\u8003\u8651\u503c\u7684\u865a\u62df\u673a\u5668\u4eba\u4f1a\u6291\u5236\u53c2\u4e0e\u8005\u7684\u79fb\u52a8\uff0c\u800c\u5177\u6709\u9ad8\u8003\u8651\u503c\u7684\u673a\u5668\u4eba\u5219\u4e0d\u4f1a\u6291\u5236\u79fb\u52a8\uff0c\u4e14\u673a\u5668\u4eba\u5728\u53c2\u4e0e\u8005\u63a5\u8fd1\u65f6\u603b\u662f\u8868\u73b0\u51fa\u903c\u8fd1\u884c\u4e3a\u3002", "conclusion": "\u63d0\u51fa\u7684\u6a21\u578b\u80fd\u591f\u6709\u6548\u9610\u660e\u8003\u8651\u4ed6\u4eba\u4e92\u52a8\u7684\u673a\u5236\u3002"}}
{"id": "2601.04547", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.04547", "abs": "https://arxiv.org/abs/2601.04547", "authors": ["Jakob M. Kern", "James M. Hurrell", "Shreya Santra", "Keisuke Takehana", "Kentaro Uno", "Kazuya Yoshida"], "title": "Data-Driven Terramechanics Approach Towards a Realistic Real-Time Simulator for Lunar Rovers", "comment": "Author's version of a manuscript accepted at the International Conference on Space Robotics 2025 (iSpaRo 2025). (c) IEEE", "summary": "High-fidelity simulators for the lunar surface provide a digital environment for extensive testing of rover operations and mission planning. However, current simulators focus on either visual realism or physical accuracy, which limits their capability to replicate lunar conditions comprehensively. This work addresses that gap by combining high visual fidelity with realistic terrain interaction for a realistic representation of rovers on the lunar surface. Because direct simulation of wheel-soil interactions is computationally expensive, a data-driven approach was adopted, using regression models for slip and sinkage from data collected in both full-rover and single-wheel experiments and simulations. The resulting regression-based terramechanics model accurately reproduced steady-state and dynamic slip, as well as sinkage behavior, on flat terrain and slopes up to 20 degrees, with validation against field test results. Additionally, improvements were made to enhance the realism of terrain deformation and wheel trace visualization. This method supports real-time applications that require physically plausible terrain response alongside high visual fidelity.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9ad8\u89c6\u89c9\u771f\u5b9e\u611f\u548c\u771f\u5b9e\u5730\u5f62\u4ea4\u4e92\u7684\u6708\u7403\u8868\u9762\u6a21\u62df\u5668\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u6a21\u62df\u5668\u5728\u771f\u5b9e\u611f\u4e0e\u7269\u7406\u7cbe\u786e\u5ea6\u4e4b\u95f4\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u6708\u7403\u8868\u9762\u6a21\u62df\u5668\u5728\u89c6\u89c9\u771f\u5b9e\u611f\u548c\u7269\u7406\u51c6\u786e\u6027\u4e4b\u95f4\u5b58\u5728\u5206\u6b67\uff0c\u65e0\u6cd5\u5168\u9762\u590d\u5236\u6708\u7403\u6761\u4ef6\u3002", "method": "\u91c7\u7528\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u56de\u5f52\u6a21\u578b\u8fdb\u884c\u8f6e\u5b50\u4e0e\u571f\u58e4\u76f8\u4e92\u4f5c\u7528\u7684\u6a21\u62df\uff0c\u6a21\u578b\u6839\u636e\u5168\u8f66\u548c\u5355\u8f6e\u5b9e\u9a8c\u53ca\u6a21\u62df\u6570\u636e\u8fdb\u884c\u6784\u5efa\u3002", "result": "\u56de\u5f52\u57fa\u7840\u7684\u571f\u5de5\u529b\u5b66\u6a21\u578b\u80fd\u51c6\u786e\u518d\u73b0\u5e73\u5730\u548c20\u5ea6\u5761\u9053\u4e0a\u7684\u7a33\u6001\u548c\u52a8\u6001\u6ed1\u79fb\u53ca\u6c89\u964d\u884c\u4e3a\uff0c\u9a8c\u8bc1\u7ed3\u679c\u4e0e\u5b9e\u5730\u6d4b\u8bd5\u76f8\u7b26\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u652f\u6301\u9700\u8981\u7269\u7406\u4e0a\u5408\u7406\u7684\u5730\u5f62\u54cd\u5e94\u548c\u9ad8\u89c6\u89c9\u771f\u5b9e\u611f\u7684\u5b9e\u65f6\u5e94\u7528\u3002"}}
{"id": "2601.04551", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.04551", "abs": "https://arxiv.org/abs/2601.04551", "authors": ["Riku Suzuki", "Ayumi Umemura", "Shreya Santra", "Kentaro Uno", "Kazuya Yoshida"], "title": "Discrete Fourier Transform-based Point Cloud Compression for Efficient SLAM in Featureless Terrain", "comment": "Author's version of a manuscript accepted at the 11th International Conference on Automation, Robotics, and Applications (ICARA). (c) IEEE", "summary": "Simultaneous Localization and Mapping (SLAM) is an essential technology for the efficiency and reliability of unmanned robotic exploration missions. While the onboard computational capability and communication bandwidth are critically limited, the point cloud data handled by SLAM is large in size, attracting attention to data compression methods. To address such a problem, in this paper, we propose a new method for compressing point cloud maps by exploiting the Discrete Fourier Transform (DFT). The proposed technique converts the Digital Elevation Model (DEM) to the frequency-domain 2D image and omits its high-frequency components, focusing on the exploration of gradual terrains such as planets and deserts. Unlike terrains with detailed structures such as artificial environments, high-frequency components contribute little to the representation of gradual terrains. Thus, this method is effective in compressing data size without significant degradation of the point cloud. We evaluated the method in terms of compression rate and accuracy using camera sequences of two terrains with different elevation profiles.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362 (DFT) \u538b\u7f29\u70b9\u4e91\u5730\u56fe\uff0c\u4ee5\u9002\u5e94\u65e0\u4eba\u673a\u63a2\u7d22\u4efb\u52a1\u4e2d\u7684\u8ba1\u7b97\u548c\u901a\u4fe1\u9650\u5236\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u63a2\u7d22\u4efb\u52a1\u4e2d\uff0c\u8ba1\u7b97\u80fd\u529b\u548c\u901a\u4fe1\u5e26\u5bbd\u6709\u9650\uff0c\u70b9\u4e91\u6570\u636e\u5e9e\u5927\uff0c\u56e0\u6b64\u9700\u8981\u6709\u6548\u7684\u6570\u636e\u538b\u7f29\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362\u5c06\u6570\u5b57\u9ad8\u7a0b\u6a21\u578b\uff08DEM\uff09\u8f6c\u6362\u4e3a\u9891\u57df2D\u56fe\u50cf\uff0c\u7701\u7565\u9ad8\u9891\u6210\u5206\uff0c\u4e13\u6ce8\u4e8e\u9010\u6e10\u53d8\u5316\u7684\u5730\u5f62\u3002", "result": "\u901a\u8fc7\u5bf9\u4e24\u79cd\u4e0d\u540c\u9ad8\u7a0b\u8f6e\u5ed3\u7684\u5730\u5f62\u7684\u6444\u50cf\u5934\u5e8f\u5217\u8fdb\u884c\u8bc4\u4f30\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u538b\u7f29\u7387\u548c\u7cbe\u5ea6\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u538b\u7f29\u6570\u636e\u5927\u5c0f\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u70b9\u4e91\u7684\u8868\u793a\u7cbe\u5ea6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9010\u6e10\u53d8\u5316\u7684\u5730\u5f62\u3002"}}
{"id": "2601.04629", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.04629", "abs": "https://arxiv.org/abs/2601.04629", "authors": ["Zhongxuan Li", "Zeliang Guo", "Jun Hu", "David Navarro-Alarcon", "Jia Pan", "Hongmin Wu", "Peng Zhou"], "title": "UniBiDex: A Unified Teleoperation Framework for Robotic Bimanual Dexterous Manipulation", "comment": null, "summary": "We present UniBiDex a unified teleoperation framework for robotic bimanual dexterous manipulation that supports both VRbased and leaderfollower input modalities UniBiDex enables realtime contactrich dualarm teleoperation by integrating heterogeneous input devices into a shared control stack with consistent kinematic treatment and safety guarantees The framework employs nullspace control to optimize bimanual configurations ensuring smooth collisionfree and singularityaware motion across tasks We validate UniBiDex on a longhorizon kitchentidying task involving five sequential manipulation subtasks demonstrating higher task success rates smoother trajectories and improved robustness compared to strong baselines By releasing all hardware and software components as opensource we aim to lower the barrier to collecting largescale highquality human demonstration datasets and accelerate progress in robot learning.", "AI": {"tldr": "UniBiDex\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u9065\u64cd\u4f5c\u6846\u67b6\uff0c\u7528\u4e8e\u53cc\u624b\u7075\u5de7\u64cd\u4f5c\uff0c\u901a\u8fc7\u96c6\u6210\u591a\u79cd\u8f93\u5165\u65b9\u5f0f\u5b9e\u73b0\u9ad8\u6548\u548c\u5b89\u5168\u7684\u64cd\u4f5c\uff0c\u5e76\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u652f\u6301\u5f00\u6e90\uff0c\u4fc3\u8fdb\u673a\u5668\u4eba\u5b66\u4e60\u53d1\u5c55\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u7edf\u4e00\u7684\u9065\u64cd\u4f5c\u6846\u67b6\uff0c\u4ee5\u652f\u6301\u53cc\u624b\u7075\u5de7\u64cd\u4f5c\uff0c\u5e76\u80fd\u9002\u5e94\u4e0d\u540c\u7684\u8f93\u5165\u65b9\u5f0f\uff0c\u4ee5\u63d0\u9ad8\u673a\u5668\u4eba\u7684\u64cd\u4f5c\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "method": "UniBiDex\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u5f02\u6784\u8f93\u5165\u8bbe\u5907\uff0c\u5b9e\u73b0\u5bf9\u53cc\u81c2\u7684\u5b9e\u65f6\u63a7\u5236\uff0c\u91c7\u7528null\u7a7a\u95f4\u63a7\u5236\u4f18\u5316\u53cc\u624b\u914d\u7f6e\uff0c\u4ee5\u786e\u4fdd\u64cd\u4f5c\u7684\u5e73\u6ed1\u6027\u3001\u65e0\u78b0\u649e\u548c\u5bf9\u5947\u5f02\u6027\u7684\u654f\u611f\u6027\u3002", "result": "\u5728\u957f\u65f6\u95f4\u7684\u53a8\u623f\u6e05\u7406\u4efb\u52a1\u4e2d\uff0cUniBiDex\u5c55\u793a\u4e86\u66f4\u9ad8\u7684\u4efb\u52a1\u6210\u529f\u7387\u3001\u66f4\u5e73\u6ed1\u7684\u8fd0\u52a8\u8f68\u8ff9\u548c\u66f4\u597d\u7684\u9c81\u68d2\u6027\uff0c\u76f8\u8f83\u4e8e\u5f3a\u6709\u529b\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5f00\u653e\u6e90\u4ee3\u7801\u53d1\u5e03\u6240\u6709\u786c\u4ef6\u548c\u8f6f\u4ef6\u7ec4\u4ef6\uff0c\u964d\u4f4e\u9ad8\u8d28\u91cf\u4eba\u7c7b\u793a\u8303\u6570\u636e\u96c6\u7684\u6536\u96c6\u95e8\u69db\uff0c\u63a8\u52a8\u673a\u5668\u4eba\u5b66\u4e60\u7684\u8fdb\u5c55\u3002"}}
{"id": "2601.04668", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04668", "abs": "https://arxiv.org/abs/2601.04668", "authors": ["Laukik Patade", "Rohan Rane", "Sandeep Pillai"], "title": "Optimizing Path Planning using Deep Reinforcement Learning for UGVs in Precision Agriculture", "comment": null, "summary": "This study focuses on optimizing path planning for unmanned ground vehicles (UGVs) in precision agriculture using deep reinforcement learning (DRL) techniques in continuous action spaces. The research begins with a review of traditional grid-based methods, such as A* and Dijkstra's algorithms, and discusses their limitations in dynamic agricultural environments, highlighting the need for adaptive learning strategies. The study then explores DRL approaches, including Deep Q-Networks (DQN), which demonstrate improved adaptability and performance in two-dimensional simulations. Enhancements such as Double Q-Networks and Dueling Networks are evaluated to further improve decision-making. Building on these results, the focus shifts to continuous action space models, specifically Deep Deterministic Policy Gradient (DDPG) and Twin Delayed Deep Deterministic Policy Gradient (TD3), which are tested in increasingly complex environments. Experiments conducted in a three-dimensional environment using ROS and Gazebo demonstrate the effectiveness of continuous DRL algorithms in navigating dynamic agricultural scenarios. Notably, the pretrained TD3 agent achieves a 95 percent success rate in dynamic environments, demonstrating the robustness of the proposed approach in handling moving obstacles while ensuring safety for both crops and the robot.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u65e0\u4eba\u5730\u9762\u8f66\u8f86\u5728\u7cbe\u51c6\u519c\u4e1a\u4e2d\u7684\u8def\u5f84\u89c4\u5212\uff0c\u91cd\u70b9\u662f\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u6a21\u578b\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "motivation": "\u968f\u7740\u7cbe\u51c6\u519c\u4e1a\u7684\u53d1\u5c55\uff0c\u5bf9\u65e0\u4eba\u5730\u9762\u8f66\u8f86\u7684\u8def\u5f84\u89c4\u5212\u63d0\u51fa\u4e86\u66f4\u9ad8\u8981\u6c42\uff0c\u4f20\u7edf\u7684\u7f51\u683c\u65b9\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5c40\u9650\uff0c\u56e0\u6b64\u9700\u8981\u5bfb\u627e\u81ea\u9002\u5e94\u5b66\u4e60\u7b56\u7565\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5305\u62ecDQN\u3001DDPG\u548cTD3\uff0c\u5728\u9010\u6b65\u590d\u6742\u7684\u4e09\u7ef4\u73af\u5883\u4e2d\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5e76\u8fdb\u884c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6548\u679c\u3002", "result": "\u5728\u4e09\u7ef4\u73af\u5883\u4e2d\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8fde\u7eed\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u52a8\u6001\u519c\u4e1a\u573a\u666f\u4e2d\u8868\u73b0\u6709\u6548\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u79fb\u52a8\u969c\u788d\u7269\u65f6\uff0c\u9884\u8bad\u7ec3TD3\u4ee3\u7406\u8fbe\u5230\u4e8695%\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u9884\u8bad\u7ec3TD3\u4ee3\u7406\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u4e8695%\u7684\u6210\u529f\u7387\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u79fb\u52a8\u969c\u788d\u7269\u65f6\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u786e\u4fdd\u4e86\u4f5c\u7269\u548c\u673a\u5668\u4eba\u7684\u5b89\u5168\u3002"}}
{"id": "2601.04699", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04699", "abs": "https://arxiv.org/abs/2601.04699", "authors": ["Zebin Han", "Xudong Wang", "Baichen Liu", "Qi Lyu", "Zhenduo Shang", "Jiahua Dong", "Lianqing Liu", "Zhi Han"], "title": "SeqWalker: Sequential-Horizon Vision-and-Language Navigation with Hierarchical Planning", "comment": null, "summary": "Sequential-Horizon Vision-and-Language Navigation (SH-VLN) presents a challenging scenario where agents should sequentially execute multi-task navigation guided by complex, long-horizon language instructions. Current vision-and-language navigation models exhibit significant performance degradation with such multi-task instructions, as information overload impairs the agent's ability to attend to observationally relevant details. To address this problem, we propose SeqWalker, a navigation model built on a hierarchical planning framework. Our SeqWalker features: i) A High-Level Planner that dynamically selects global instructions into contextually relevant sub-instructions based on the agent's current visual observations, thus reducing cognitive load; ii) A Low-Level Planner incorporating an Exploration-Verification strategy that leverages the inherent logical structure of instructions for trajectory error correction. To evaluate SH-VLN performance, we also extend the IVLN dataset and establish a new benchmark. Extensive experiments are performed to demonstrate the superiority of the proposed SeqWalker.", "AI": {"tldr": "SeqWalker\u662f\u4e00\u79cd\u5728\u5206\u5c42\u89c4\u5212\u6846\u67b6\u4e0b\u7684\u5bfc\u822a\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u5347\u590d\u6742\u957f\u7bc7\u8bed\u8a00\u6307\u4ee4\u4e0b\u7684\u5bfc\u822a\u6027\u80fd\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u76f8\u5173\u5b50\u6307\u4ee4\u548c\u5229\u7528\u63a2\u7d22\u9a8c\u8bc1\u7b56\u7565\u6765\u51cf\u5c11\u8ba4\u77e5\u8d1f\u62c5\u548c\u7ea0\u6b63\u8f68\u8ff9\u9519\u8bef\u3002", "motivation": "\u5728\u591a\u4efb\u52a1\u5bfc\u822a\u4e2d\uff0c\u57fa\u4e8e\u8bed\u8a00\u7684\u6a21\u578b\u56e0\u4fe1\u606f\u8fc7\u8f7d\u800c\u6027\u80fd\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u6a21\u578b\u6765\u6539\u5584\u8fd9\u79cd\u60c5\u51b5\u3002", "method": "SeqWalker\u91c7\u7528\u5206\u5c42\u89c4\u5212\u6846\u67b6\uff0c\u5305\u542b\u9ad8\u5c42\u89c4\u5212\u5668\u548c\u4f4e\u5c42\u89c4\u5212\u5668\uff0c\u524d\u8005\u52a8\u6001\u9009\u62e9\u5b50\u6307\u4ee4\uff0c\u540e\u8005\u4f7f\u7528\u63a2\u7d22\u9a8c\u8bc1\u7b56\u7565\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSeqWalker\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00\u5bfc\u822a\u6a21\u578b\uff0c\u4e14\u5728\u6269\u5c55\u7684IVLN\u6570\u636e\u96c6\u4e0a\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6\u3002", "conclusion": "SeqWalker\u6a21\u578b\u5728\u590d\u6742\u957f\u7bc7\u8bed\u8a00\u6307\u4ee4\u7684\u5f15\u5bfc\u4e0b\u8868\u73b0\u4f18\u8d8a\uff0c\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u591a\u4efb\u52a1\u5bfc\u822a\u7684\u6267\u884c\u6548\u7387\uff0c\u4e14\u5728\u65b0\u7684\u57fa\u51c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u52bf\u3002"}}
{"id": "2601.04881", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.04881", "abs": "https://arxiv.org/abs/2601.04881", "authors": ["Kiyoung Choi", "Juwon Jeong", "Sehoon Oh"], "title": "Zero Wrench Control via Wrench Disturbance Observer for Learning-free Peg-in-hole Assembly", "comment": null, "summary": "This paper proposes a Dynamic Wrench Disturbance Observer (DW-DOB) designed to achieve highly sensitive zero-wrench control in contact-rich manipulation. By embedding task-space inertia into the observer nominal model, DW-DOB cleanly separates intrinsic dynamic reactions from true external wrenches. This preserves sensitivity to small forces and moments while ensuring robust regulation of contact wrenches. A passivity-based analysis further demonstrates that DW-DOB guarantees stable interactions under dynamic conditions, addressing the shortcomings of conventional observers that fail to compensate for inertial effects. Peg-in-hole experiments at industrial tolerances (H7/h6) validate the approach, yielding deeper and more compliant insertions with minimal residual wrenches and outperforming a conventional wrench disturbance observer and a PD baseline. These results highlight DW-DOB as a practical learning-free solution for high-precision zero-wrench control in contact-rich tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7684\u52a8\u6001\u626d\u77e9\u6270\u52a8\u89c2\u6d4b\u5668(DW-DOB)\u5728\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4e2d\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7075\u654f\u5ea6\u7684\u96f6\u626d\u77e9\u63a7\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edf\u89c2\u6d4b\u5668\u5728\u8865\u507f\u60ef\u6027\u6548\u5e94\u65f6\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u52a8\u6001\u6761\u4ef6\u4e0b\u4fdd\u8bc1\u7a33\u5b9a\u7684\u4ea4\u4e92\u3002", "method": "\u901a\u8fc7\u5c06\u4efb\u52a1\u7a7a\u95f4\u7684\u60ef\u6027\u5d4c\u5165\u5230\u89c2\u6d4b\u5668\u7684\u540d\u4e49\u6a21\u578b\u4e2d\uff0cDW-DOB\u5b9e\u73b0\u4e86\u5185\u5728\u52a8\u6001\u53cd\u5e94\u4e0e\u771f\u5b9e\u5916\u90e8\u626d\u77e9\u7684\u6e05\u6670\u5206\u79bb\u3002", "result": "DW-DOB\u5728\u5de5\u4e1a\u516c\u5dee\u4e0b\u7684Peg-in-hole\u5b9e\u9a8c\u4e2d\uff0c\u663e\u793a\u51fa\u66f4\u6df1\u66f4\u987a\u5e94\u7684\u63d2\u5165\u6548\u679c\uff0c\u6b8b\u4f59\u626d\u77e9\u6700\u5c0f\uff0c\u8d85\u8fc7\u4e86\u4f20\u7edf\u626d\u77e9\u6270\u52a8\u89c2\u6d4b\u5668\u548cPD\u57fa\u7ebf\u3002", "conclusion": "DW-DOB\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u7075\u654f\u5ea6\u7684\u96f6\u626d\u77e9\u63a7\u5236\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\uff0c\u4e14\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u89c2\u6d4b\u5668\u53caPD\u57fa\u7ebf\u3002"}}
{"id": "2601.04948", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.04948", "abs": "https://arxiv.org/abs/2601.04948", "authors": ["Junchi Gu", "Feiyang Yuan", "Weize Shi", "Tianchen Huang", "Haopeng Zhang", "Xiaohu Zhang", "Yu Wang", "Wei Gao", "Shiwu Zhang"], "title": "SKATER: Synthesized Kinematics for Advanced Traversing Efficiency on a Humanoid Robot via Roller Skate Swizzles", "comment": null, "summary": "Although recent years have seen significant progress of humanoid robots in walking and running, the frequent foot strikes with ground during these locomotion gaits inevitably generate high instantaneous impact forces, which leads to exacerbated joint wear and poor energy utilization. Roller skating, as a sport with substantial biomechanical value, can achieve fast and continuous sliding through rational utilization of body inertia, featuring minimal kinetic energy loss. Therefore, this study proposes a novel humanoid robot with each foot equipped with a row of four passive wheels for roller skating. A deep reinforcement learning control framework is also developed for the swizzle gait with the reward function design based on the intrinsic characteristics of roller skating. The learned policy is first analyzed in simulation and then deployed on the physical robot to demonstrate the smoothness and efficiency of the swizzle gait over traditional bipedal walking gait in terms of Impact Intensity and Cost of Transport during locomotion. A reduction of $75.86\\%$ and $63.34\\%$ of these two metrics indicate roller skating as a superior locomotion mode for enhanced energy efficiency and joint longevity.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u7684\u5e26\u88ab\u52a8\u8f6e\u7684\u7c7b\u4eba\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u6ed1\u884c\u8fd0\u52a8\u4f18\u5316\u80fd\u91cf\u6548\u7387\uff0c\u51cf\u5c11\u5173\u8282\u78e8\u635f\u3002", "motivation": "\u7c7b\u4eba\u673a\u5668\u4eba\u5728\u884c\u8d70\u548c\u8dd1\u6b65\u4e2d\u9891\u7e41\u7684\u8db3\u90e8\u51b2\u51fb\u5bfc\u81f4\u9ad8\u77ac\u65f6\u51b2\u51fb\u529b\uff0c\u589e\u52a0\u5173\u8282\u78e8\u635f\u53ca\u964d\u4f4e\u80fd\u91cf\u5229\u7528\u7387\u3002", "method": "\u901a\u8fc7\u4e3a\u6bcf\u53ea\u811a\u914d\u5907\u56db\u4e2a\u88ab\u52a8\u8f6e\uff0c\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u63a7\u5236\u6ed1\u884c\u6b65\u6001\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u6ed1\u884c\u7279\u6027\u7684\u5956\u52b1\u51fd\u6570\u3002", "result": "\u5728\u6a21\u62df\u4e2d\u5206\u6790\u5b66\u4e60\u7684\u7b56\u7565\uff0c\u5e76\u5728\u7269\u7406\u673a\u5668\u4eba\u4e0a\u90e8\u7f72\uff0c\u7ed3\u679c\u663e\u793a\u6ed1\u884c\u6b65\u6001\u5728\u51b2\u51fb\u5f3a\u5ea6\u548c\u8fd0\u8f93\u6210\u672c\u4e0a\u4f18\u4e8e\u4f20\u7edf\u7684\u53cc\u8db3\u884c\u8d70\u3002", "conclusion": "\u6ed1\u884c\u4f5c\u4e3a\u4e00\u79cd\u8fd0\u52a8\u6a21\u5f0f\uff0c\u5c55\u793a\u51fa\u663e\u8457\u63d0\u9ad8\u80fd\u91cf\u6548\u7387\u548c\u5ef6\u957f\u5173\u8282\u5bff\u547d\u7684\u4f18\u52bf\u3002"}}
{"id": "2601.04982", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04982", "abs": "https://arxiv.org/abs/2601.04982", "authors": ["Johannes A. Gaus", "Winfried Ilg", "Daniel Haeufle"], "title": "When to Act: Calibrated Confidence for Reliable Human Intention Prediction in Assistive Robotics", "comment": null, "summary": "Assistive devices must determine both what a user intends to do and how reliable that prediction is before providing support. We introduce a safety-critical triggering framework based on calibrated probabilities for multimodal next-action prediction in Activities of Daily Living. Raw model confidence often fails to reflect true correctness, posing a safety risk. Post-hoc calibration aligns predicted confidence with empirical reliability and reduces miscalibration by about an order of magnitude without affecting accuracy. The calibrated confidence drives a simple ACT/HOLD rule that acts only when reliability is high and withholds assistance otherwise. This turns the confidence threshold into a quantitative safety parameter for assisted actions and enables verifiable behavior in an assistive control loop.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u6821\u51c6\u6982\u7387\u7684\u6846\u67b6\uff0c\u63d0\u5347\u52a9\u7406\u8bbe\u5907\u5bf9\u7528\u6237\u610f\u56fe\u7684\u9884\u6d4b\u53ef\u9760\u6027\uff0c\u964d\u4f4e\u5b89\u5168\u98ce\u9669\u3002", "motivation": "\u52a9\u7406\u8bbe\u5907\u9700\u51c6\u786e\u9884\u6d4b\u7528\u6237\u610f\u56fe\u5e76\u8bc4\u4f30\u9884\u6d4b\u7684\u53ef\u9760\u6027\uff0c\u786e\u4fdd\u63d0\u4f9b\u5b89\u5168\u652f\u6301\u3002", "method": "\u57fa\u4e8e\u6821\u51c6\u6982\u7387\u7684\u591a\u6a21\u6001\u4e0b\u4e00\u6b65\u52a8\u4f5c\u9884\u6d4b\u7684\u5b89\u5168\u89e6\u53d1\u6846\u67b6", "result": "\u901a\u8fc7\u540e\u7f6e\u6821\u51c6\uff0c\u5c06\u9884\u6d4b\u7684\u7f6e\u4fe1\u5ea6\u4e0e\u5b9e\u9645\u53ef\u9760\u6027\u5bf9\u9f50\uff0c\u663e\u8457\u51cf\u5c11\u8bef\u6821\u51c6\uff0c\u672a\u5f71\u54cd\u51c6\u786e\u6027\u3002", "conclusion": "\u6821\u51c6\u540e\u7684\u7f6e\u4fe1\u5ea6\u9a71\u52a8ACT/HOLD\u89c4\u5219\uff0c\u4ec5\u5728\u53ef\u9760\u6027\u9ad8\u65f6\u63d0\u4f9b\u5e2e\u52a9\uff0c\u8f6c\u5316\u7f6e\u4fe1\u5ea6\u9608\u503c\u4e3a\u5b9a\u91cf\u5b89\u5168\u53c2\u6570\uff0c\u786e\u4fdd\u52a9\u7406\u63a7\u5236\u56de\u8def\u4e2d\u7684\u53ef\u9a8c\u8bc1\u884c\u4e3a\u3002"}}
{"id": "2601.05014", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.05014", "abs": "https://arxiv.org/abs/2601.05014", "authors": ["Lingdong Kong", "Shaoyuan Xie", "Zeying Gong", "Ye Li", "Meng Chu", "Ao Liang", "Yuhao Dong", "Tianshuai Hu", "Ronghe Qiu", "Rong Li", "Hanjiang Hu", "Dongyue Lu", "Wei Yin", "Wenhao Ding", "Linfeng Li", "Hang Song", "Wenwei Zhang", "Yuexin Ma", "Junwei Liang", "Zhedong Zheng", "Lai Xing Ng", "Benoit R. Cottereau", "Wei Tsang Ooi", "Ziwei Liu", "Zhanpeng Zhang", "Weichao Qiu", "Wei Zhang", "Ji Ao", "Jiangpeng Zheng", "Siyu Wang", "Guang Yang", "Zihao Zhang", "Yu Zhong", "Enzhu Gao", "Xinhan Zheng", "Xueting Wang", "Shouming Li", "Yunkai Gao", "Siming Lan", "Mingfei Han", "Xing Hu", "Dusan Malic", "Christian Fruhwirth-Reisinger", "Alexander Prutsch", "Wei Lin", "Samuel Schulter", "Horst Possegger", "Linfeng Li", "Jian Zhao", "Zepeng Yang", "Yuhang Song", "Bojun Lin", "Tianle Zhang", "Yuchen Yuan", "Chi Zhang", "Xuelong Li", "Youngseok Kim", "Sihwan Hwang", "Hyeonjun Jeong", "Aodi Wu", "Xubo Luo", "Erjia Xiao", "Lingfeng Zhang", "Yingbo Tang", "Hao Cheng", "Renjing Xu", "Wenbo Ding", "Lei Zhou", "Long Chen", "Hangjun Ye", "Xiaoshuai Hao", "Shuangzhi Li", "Junlong Shen", "Xingyu Li", "Hao Ruan", "Jinliang Lin", "Zhiming Luo", "Yu Zang", "Cheng Wang", "Hanshi Wang", "Xijie Gong", "Yixiang Yang", "Qianli Ma", "Zhipeng Zhang", "Wenxiang Shi", "Jingmeng Zhou", "Weijun Zeng", "Kexin Xu", "Yuchen Zhang", "Haoxiang Fu", "Ruibin Hu", "Yanbiao Ma", "Xiyan Feng", "Wenbo Zhang", "Lu Zhang", "Yunzhi Zhuge", "Huchuan Lu", "You He", "Seungjun Yu", "Junsung Park", "Youngsun Lim", "Hyunjung Shim", "Faduo Liang", "Zihang Wang", "Yiming Peng", "Guanyu Zong", "Xu Li", "Binghao Wang", "Hao Wei", "Yongxin Ma", "Yunke Shi", "Shuaipeng Liu", "Dong Kong", "Yongchun Lin", "Huitong Yang", "Liang Lei", "Haoang Li", "Xinliang Zhang", "Zhiyong Wang", "Xiaofeng Wang", "Yuxia Fu", "Yadan Luo", "Djamahl Etchegaray", "Yang Li", "Congfei Li", "Yuxiang Sun", "Wenkai Zhu", "Wang Xu", "Linru Li", "Longjie Liao", "Jun Yan", "Benwu Wang", "Xueliang Ren", "Xiaoyu Yue", "Jixian Zheng", "Jinfeng Wu", "Shurui Qin", "Wei Cong", "Yao He"], "title": "The RoboSense Challenge: Sense Anything, Navigate Anywhere, Adapt Across Platforms", "comment": "Official IROS 2025 RoboSense Challenge Report; 51 pages, 37 figures, 5 tables; Competition Website at https://robosense2025.github.io/", "summary": "Autonomous systems are increasingly deployed in open and dynamic environments -- from city streets to aerial and indoor spaces -- where perception models must remain reliable under sensor noise, environmental variation, and platform shifts. However, even state-of-the-art methods often degrade under unseen conditions, highlighting the need for robust and generalizable robot sensing. The RoboSense 2025 Challenge is designed to advance robustness and adaptability in robot perception across diverse sensing scenarios. It unifies five complementary research tracks spanning language-grounded decision making, socially compliant navigation, sensor configuration generalization, cross-view and cross-modal correspondence, and cross-platform 3D perception. Together, these tasks form a comprehensive benchmark for evaluating real-world sensing reliability under domain shifts, sensor failures, and platform discrepancies. RoboSense 2025 provides standardized datasets, baseline models, and unified evaluation protocols, enabling large-scale and reproducible comparison of robust perception methods. The challenge attracted 143 teams from 85 institutions across 16 countries, reflecting broad community engagement. By consolidating insights from 23 winning solutions, this report highlights emerging methodological trends, shared design principles, and open challenges across all tracks, marking a step toward building robots that can sense reliably, act robustly, and adapt across platforms in real-world environments.", "AI": {"tldr": "RoboSense 2025\u6311\u6218\u65e8\u5728\u63d0\u5347\u673a\u5668\u4eba\u611f\u77e5\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u901a\u8fc7\u591a\u6837\u5316\u7684\u7814\u7a76\u8f68\u9053\u548c\u6807\u51c6\u5316\u7684\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u5404\u7c7b\u611f\u77e5\u573a\u666f\u7684\u8bc4\u4f30\u3002", "motivation": "\u5728\u591a\u53d8\u548c\u52a8\u6001\u7684\u73af\u5883\u4e2d\uff0c\u73b0\u6709\u7684\u611f\u77e5\u6a21\u578b\u9762\u4e34\u7740\u53ef\u9760\u6027\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u4e9f\u9700\u5efa\u7acb\u66f4\u9c81\u68d2\u548c\u53ef\u63a8\u5e7f\u7684\u673a\u5668\u4eba\u611f\u77e5\u7cfb\u7edf\u3002", "method": "RoboSense 2025\u6311\u6218\u5305\u542b\u4e94\u4e2a\u4e92\u8865\u7684\u7814\u7a76\u8f68\u9053\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u6570\u636e\u96c6\u3001\u57fa\u51c6\u6a21\u578b\u53ca\u7edf\u4e00\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u4ee5\u8bc4\u4f30\u5728\u4e0d\u540c\u73af\u5883\u4e0b\u7684\u611f\u77e5\u80fd\u529b\u3002", "result": "\u8be5\u6311\u6218\u5438\u5f15\u4e86\u6765\u81ea16\u4e2a\u56fd\u5bb6\u7684143\u4e2a\u56e2\u961f\u53c2\u4e0e\uff0c\u901a\u8fc723\u4e2a\u83b7\u80dc\u89e3\u51b3\u65b9\u6848\u7684\u603b\u7ed3\uff0c\u62a5\u544a\u63ed\u793a\u4e86\u65b9\u6cd5\u8bba\u8d8b\u52bf\u3001\u8bbe\u8ba1\u539f\u5219\u548c\u9762\u4e34\u7684\u6311\u6218\u3002", "conclusion": "RoboSense 2025\u6311\u6218\u4e3a\u673a\u5668\u4eba\u611f\u77e5\u9886\u57df\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\uff0c\u901a\u8fc7\u6574\u5408\u6765\u81ea\u53c2\u4e0e\u56e2\u961f\u7684\u6d1e\u89c1\uff0c\u63a8\u52a8\u4e86\u672a\u6765\u673a\u5668\u4eba\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.05074", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.05074", "abs": "https://arxiv.org/abs/2601.05074", "authors": ["Julian Kulozik", "Nathana\u00ebl Jarrass\u00e9"], "title": "Compensation Effect Amplification Control (CEAC): A movement-based approach for coordinated position and velocity control of the elbow of upper-limb prostheses", "comment": null, "summary": "Despite advances in upper-limb (UL) prosthetic design, achieving intuitive control of intermediate joints - such as the wrist and elbow - remains challenging, particularly for continuous and velocity-modulated movements. We introduce a novel movement-based control paradigm entitled Compensation Effect Amplification Control (CEAC) that leverages users' trunk flexion and extension as input for controlling prosthetic elbow velocity. Considering that the trunk can be both a functional and compensatory joint when performing upper-limb actions, CEAC amplifies the natural coupling between trunk and prosthesis while introducing a controlled delay that allows users to modulate both the position and velocity of the prosthetic joint. We evaluated CEAC in a generic drawing task performed by twelve able-bodied participants using a supernumerary prosthesis with an active elbow. Additionally a multiple-target-reaching task was performed by a subset of ten participants. Results demonstrate task performances comparable to those obtained with natural arm movements, even when gesture velocity or drawing size were varied, while maintaining ergonomic trunk postures. Analysis revealed that CEAC effectively restores joint coordinated action, distributes movement effort between trunk and elbow, enabling intuitive trajectory control without requiring extreme compensatory movements. Overall, CEAC offers a promising control strategy for intermediate joints of UL prostheses, particularly in tasks requiring continuous and precise coordination.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a7\u5236\u7b56\u7565CEAC\uff0c\u901a\u8fc7\u653e\u5927\u8eaf\u5e72\u8fd0\u52a8\u6765\u63a7\u5236\u4e49\u80a2\u8098\u90e8\u901f\u5ea6\uff0c\u63d0\u5347\u4e86\u4e0a\u80a2\u4e49\u80a2\u7684\u76f4\u89c2\u63a7\u5236\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u4e0a\u80a2\u4e49\u80a2\u8bbe\u8ba1\u6709\u6240\u8fdb\u6b65\uff0c\u4f46\u5b9e\u73b0\u8098\u90e8\u548c\u8155\u90e8\u63a7\u5236\u7684\u76f4\u89c2\u6027\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u8fde\u7eed\u548c\u901f\u5ea6\u8c03\u8282\u7684\u8fd0\u52a8\u4e2d\u3002", "method": "CEAC\u63a7\u5236\u57fa\u4e8e\u7528\u6237\u7684\u8eaf\u5e72\u5f2f\u66f2\u4e0e\u4f38\u5c55\uff0c\u653e\u5927\u8eaf\u5e72\u4e0e\u4e49\u80a2\u4e4b\u95f4\u7684\u81ea\u7136\u8026\u5408\uff0c\u5e76\u5f15\u5165\u63a7\u5236\u5ef6\u8fdf\u4ee5\u8c03\u8282\u4e49\u80a2\u5173\u8282\u7684\u4f4d\u7f6e\u548c\u901f\u5ea6\u3002", "result": "\u5728\u7ed8\u56fe\u4efb\u52a1\u4e2d\uff0cCEAC\u7684\u8868\u73b0\u4e0e\u81ea\u7136\u624b\u81c2\u52a8\u4f5c\u76f8\u5f53\uff0c\u4e14\u5728\u4e0d\u540c\u7684\u624b\u52bf\u901f\u5ea6\u548c\u7ed8\u56fe\u5927\u5c0f\u4e0b\u4e5f\u80fd\u4fdd\u6301\u826f\u597d\u7684\u8868\u73b0\uff0c\u7ef4\u6301\u4e86\u5408\u7406\u7684\u8eaf\u5e72\u59ff\u52bf\u3002", "conclusion": "CEAC\u4e3a\u4e0a\u80a2\u4e49\u80a2\u7684\u4e2d\u95f4\u5173\u8282\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u63a7\u5236\u7b56\u7565\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u8fde\u7eed\u548c\u7cbe\u786e\u534f\u8c03\u7684\u4efb\u52a1\u3002"}}
{"id": "2601.05243", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.05243", "abs": "https://arxiv.org/abs/2601.05243", "authors": ["Xingyi He", "Adhitya Polavaram", "Yunhao Cao", "Om Deshmukh", "Tianrui Wang", "Xiaowei Zhou", "Kuan Fang"], "title": "Generate, Transfer, Adapt: Learning Functional Dexterous Grasping from a Single Human Demonstration", "comment": "Project Page: https://cordex-manipulation.github.io/", "summary": "Functional grasping with dexterous robotic hands is a key capability for enabling tool use and complex manipulation, yet progress has been constrained by two persistent bottlenecks: the scarcity of large-scale datasets and the absence of integrated semantic and geometric reasoning in learned models. In this work, we present CorDex, a framework that robustly learns dexterous functional grasps of novel objects from synthetic data generated from just a single human demonstration. At the core of our approach is a correspondence-based data engine that generates diverse, high-quality training data in simulation. Based on the human demonstration, our data engine generates diverse object instances of the same category, transfers the expert grasp to the generated objects through correspondence estimation, and adapts the grasp through optimization. Building on the generated data, we introduce a multimodal prediction network that integrates visual and geometric information. By devising a local-global fusion module and an importance-aware sampling mechanism, we enable robust and computationally efficient prediction of functional dexterous grasps. Through extensive experiments across various object categories, we demonstrate that CorDex generalizes well to unseen object instances and significantly outperforms state-of-the-art baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CorDex\uff0c\u4e00\u4e2a\u4ece\u5408\u6210\u6570\u636e\u4e2d\u5b66\u4e60\u7075\u6d3b\u6293\u53d6\u7684\u6846\u67b6\uff0c\u80fd\u591f\u901a\u8fc7\u5355\u4e00\u7684\u4eba\u7c7b\u6f14\u793a\u751f\u6210\u591a\u6837\u5316\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u7ed3\u5408\u89c6\u89c9\u4e0e\u51e0\u4f55\u4fe1\u606f\u5b9e\u73b0\u9ad8\u6548\u7684\u6293\u53d6\u9884\u6d4b\u3002", "motivation": "\u7075\u5de7\u673a\u5668\u4eba\u624b\u7684\u529f\u80fd\u6027\u6293\u53d6\u662f\u5b9e\u73b0\u5de5\u5177\u4f7f\u7528\u548c\u590d\u6742\u64cd\u4f5c\u7684\u5173\u952e\u80fd\u529b\uff0c\u4f46\u53d7\u9650\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u7a00\u7f3a\u548c\u5b66\u4e60\u6a21\u578b\u4e2d\u7f3a\u4e4f\u96c6\u6210\u8bed\u4e49\u4e0e\u51e0\u4f55\u63a8\u7406\u7684\u73b0\u8c61\u3002", "method": "\u63d0\u51fa\u4e86CorDex\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u57fa\u4e8e\u5bf9\u5e94\u5173\u7cfb\u7684\u6570\u636e\u5f15\u64ce\uff0c\u80fd\u591f\u4ece\u5355\u4e2a\u4eba\u7c7b\u6f14\u793a\u751f\u6210\u591a\u6837\u5316\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u5c40\u90e8-\u5168\u5c40\u878d\u5408\u6a21\u5757\u548c\u91cd\u8981\u6027\u611f\u77e5\u91c7\u6837\u673a\u5236\uff0c\u6574\u5408\u89c6\u89c9\u4e0e\u51e0\u4f55\u4fe1\u606f\u8fdb\u884c\u6293\u53d6\u9884\u6d4b\u3002", "result": "\u901a\u8fc7\u4e00\u7cfb\u5217\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0cCorDex\u5728\u591a\u79cd\u7269\u4f53\u7c7b\u522b\u4e0a\u5c55\u793a\u4e86\u5176\u5353\u8d8a\u7684\u63a8\u5e7f\u80fd\u529b\u548c\u663e\u8457\u7684\u6027\u80fd\u4f18\u52bf\u3002", "conclusion": "CorDex\u5728\u591a\u4e2a\u7269\u4f53\u7c7b\u522b\u7684\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u63a8\u5e7f\u5230\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u5b9e\u4f8b\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u4f73\u65b9\u6cd5\u3002"}}
{"id": "2601.05248", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.05248", "abs": "https://arxiv.org/abs/2601.05248", "authors": ["Zhuoyang Liu", "Jiaming Liu", "Hao Chen", "Ziyu Guo", "Chengkai Hou", "Chenyang Gu", "Jiale Yu", "Xiangju Mi", "Renrui Zhang", "Zhengping Che", "Jian Tang", "Pheng-Ann Heng", "Shanghang Zhang"], "title": "LaST$_{0}$: Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model", "comment": null, "summary": "Vision-Language-Action (VLA) models have recently demonstrated strong generalization capabilities in robotic manipulation. Some existing VLA approaches attempt to improve action accuracy by explicitly generating linguistic reasoning traces or future visual observations before action execution. However, explicit reasoning typically incurs non-negligible inference latency, which constrains the temporal resolution required for robotic manipulation. Moreover, such reasoning is confined to the linguistic space, imposing a representational bottleneck that struggles to faithfully capture ineffable physical attributes. To mitigate these limitations, we propose LaST$_0$, a framework that enables efficient reasoning before acting through a Latent Spatio-Temporal Chain-of-Thought (CoT), capturing fine-grained physical and robotic dynamics that are often difficult to verbalize. Specifically, we introduce a token-efficient latent CoT space that models future visual dynamics, 3D structural information, and robot proprioceptive states, and further extends these representations across time to enable temporally consistent implicit reasoning trajectories. Furthermore, LaST$_0$ adopts a dual-system architecture implemented via a Mixture-of-Transformers design, where a reasoning expert conducts low-frequency latent inference and an acting expert generates high-frequency actions conditioned on robotics-oriented latent representations. To facilitate coordination, LaST$_0$ is trained with heterogeneous operation frequencies, enabling adaptive switching between reasoning and action inference rates during deployment. Across ten simulated and six real-world manipulation tasks, LaST$_0$ improves mean success rates by 8% and 13% over prior VLA methods, respectively, while achieving substantially faster inference. Project website: https://sites.google.com/view/last0", "AI": {"tldr": "LaST$_0$\u6846\u67b6\u901a\u8fc7\u6f5c\u5728\u65f6\u7a7a\u601d\u7ef4\u94fe\u4e0e\u53cc\u7cfb\u7edf\u67b6\u6784\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u64cd\u63a7\u7684\u6210\u529f\u7387\u548c\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u65e8\u5728\u514b\u670d\u73b0\u6709VLA\u65b9\u6cd5\u5728\u63a8\u7406\u5ef6\u8fdf\u548c\u8bed\u8a00\u7a7a\u95f4\u9650\u5236\u4e0b\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u51c6\u786e\u6027\u4e0e\u901f\u5ea6\u3002", "method": "\u63d0\u51faLaST$_0$\u6846\u67b6\uff0c\u901a\u8fc7\u6f5c\u5728\u65f6\u7a7a\u601d\u7ef4\u94fe\uff08CoT\uff09\u8fdb\u884c\u9ad8\u6548\u63a8\u7406\uff0c\u5efa\u6a21\u672a\u6765\u89c6\u89c9\u52a8\u6001\u548c\u673a\u5668\u4eba\u672c\u4f53\u72b6\u6001\uff0c\u4ee5\u5b9e\u73b0\u81ea\u9002\u5e94\u7684\u63a8\u7406\u548c\u52a8\u4f5c\u63a8\u65ad\u9891\u7387\u3002", "result": "\u5728\u5341\u4e2a\u6a21\u62df\u548c\u516d\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0cLaST$_0$\u7684\u5e73\u5747\u6210\u529f\u7387\u6bd4\u4ee5\u524d\u7684VLA\u65b9\u6cd5\u5206\u522b\u63d0\u9ad8\u4e868%\u548c13%\uff0c\u540c\u65f6\u63a8\u7406\u901f\u5ea6\u663e\u8457\u66f4\u5feb\u3002", "conclusion": "LaST$_0$\u901a\u8fc7\u5f15\u5165\u6f5c\u5728\u7684\u65f6\u7a7a\u601d\u7ef4\u94fe\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709VLA\u65b9\u6cd5\u5728\u63a8\u7406\u5ef6\u8fdf\u4e0e\u8868\u793a\u74f6\u9888\u4e0a\u7684\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6548\u7387\u548c\u6210\u529f\u7387\u3002"}}
