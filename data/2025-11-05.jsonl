{"id": "2511.02079", "categories": ["cs.HC", "H.5.1; I.3.7; H.5.2"], "pdf": "https://arxiv.org/pdf/2511.02079", "abs": "https://arxiv.org/abs/2511.02079", "authors": ["Jamie Ngoc Dinh", "Snehesh Shrestha", "You-Jin Kim", "Jun Nishida", "Myungin Lee"], "title": "NeuResonance: Exploring Feedback Experiences for Fostering the Inter-brain Synchronization", "comment": "Conference Paper, 16 pages. Published at the 2025 CHI Conference on\n  Human Factors in Computing Systems", "summary": "When several individuals collaborate on a shared task, their brain activities\noften synchronize. This phenomenon, known as Inter-brain Synchronization (IBS),\nis notable for inducing prosocial outcomes such as enhanced interpersonal\nfeelings, including closeness, trust, empathy, and more. Further strengthening\nthe IBS with the aid of external feedback would be beneficial for scenarios\nwhere those prosocial feelings play a vital role in interpersonal\ncommunication, such as rehabilitation between a therapist and a patient, motor\nskill learning between a teacher and a student, and group performance art. This\npaper investigates whether visual, auditory, and haptic feedback of the IBS\nlevel can further enhance its intensity, offering design recommendations for\nfeedback systems in IBS. We report findings when three different types of\nfeedback were provided: IBS level feedback by means of on-body projection\nmapping, sonification using chords, and vibration bands attached to the wrist."}
{"id": "2511.02133", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.02133", "abs": "https://arxiv.org/abs/2511.02133", "authors": ["Suyang Li", "Fernando Fajardo-Rojas", "Diego Gomez-Gualdron", "Remco Chang", "Mingwei Li"], "title": "AlloyLens: A Visual Analytics Tool for High-throughput Alloy Screening and Inverse Design", "comment": "IEEE VIS 2025 Scientific Visualization Contest Honorable Mention\n  Links: PyPI package (Jupyter widget): https://pypi.org/project/alloylens/ Web\n  demo: http://susiesyli.com/alloylens-web/ GitHub:\n  https://github.com/susiesyli/alloylens SciVis contest 2025:\n  https://sciviscontest2025.github.io/data/", "summary": "Designing multi-functional alloys requires exploring high-dimensional\ncomposition-structure-property spaces, yet current tools are limited to\nlow-dimensional projections and offer limited support for sensitivity or\nmulti-objective tradeoff reasoning. We introduce AlloyLens, an interactive\nvisual analytics system combining a coordinated scatterplot matrix (SPLOM),\ndynamic parameter sliders, gradient-based sensitivity curves, and nearest\nneighbor recommendations. This integrated approach reveals latent structure in\nsimulation data, exposes the local impact of compositional changes, and\nhighlights tradeoffs when exact matches are absent. We validate the system\nthrough case studies co-developed with domain experts spanning structural,\nthermal, and electrical alloy design."}
{"id": "2511.02233", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.02233", "abs": "https://arxiv.org/abs/2511.02233", "authors": ["Songyang Liu", "Yunpeng Tan", "Shuai Li"], "title": "Learning Spatial Awareness for Laparoscopic Surgery with AI Assisted Visual Feedback", "comment": null, "summary": "Laparoscopic surgery constrains surgeons spatial awareness because procedures\nare performed through a monocular, two-dimensional (2D) endoscopic view.\nConventional training methods using dry-lab models or recorded videos provide\nlimited depth cues, often leading trainees to misjudge instrument position and\nperform ineffective or unsafe maneuvers. To address this limitation, we present\nan AI-assisted training framework developed in NVIDIA Isaac Sim that couples\nthe standard 2D laparoscopic feed with synchronized three-dimensional (3D)\nvisual feedback delivered through a mixed-reality (MR) interface. While\ntrainees operate using the clinical 2D view, validated AI modules continuously\nlocalize surgical instruments and detect instrument-tissue interactions in the\nbackground. When spatial misjudgments are detected, 3D visual feedback are\ndisplayed to trainees, while preserving the original operative perspective. Our\nframework considers various surgical tasks including navigation, manipulation,\ntransfer, cutting, and suturing. Visually similar 2D cases can be disambiguated\nthrough the added 3D context, improving depth perception, contact awareness,\nand tool orientation understanding."}
{"id": "2511.02367", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.02367", "abs": "https://arxiv.org/abs/2511.02367", "authors": ["Shuning Zhang", "Zhaoxin Li", "Changxi Wen", "Ying Ma", "Simin Li", "Gengrui Zhang", "Ziyi Zhang", "Yibo Meng", "Hantao Zhao", "Xin Yi", "Hewu Li"], "title": "The Pervasive Blind Spot: Benchmarking VLM Inference Risks on Everyday Personal Videos", "comment": null, "summary": "The proliferation of Vision-Language Models (VLMs) introduces profound\nprivacy risks from personal videos. This paper addresses the critical yet\nunexplored inferential privacy threat, the risk of inferring sensitive personal\nattributes over the data. To address this gap, we crowdsourced a dataset of 508\neveryday personal videos from 58 individuals. We then conducted a benchmark\nstudy evaluating VLM inference capabilities against human performance. Our\nfindings reveal three critical insights: (1) VLMs possess superhuman\ninferential capabilities, significantly outperforming human evaluators,\nleveraging a shift from object recognition to behavioral inference from\ntemporal streams. (2) Inferential risk is strongly correlated with factors such\nas video characteristics and prompting strategies. (3) VLM-driven explanation\ntowards the inference is unreliable, as we revealed a disconnect between the\nmodel-generated explanations and evidential impact, identifying ubiquitous\nobjects as misleading confounders."}
{"id": "2511.01999", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01999", "abs": "https://arxiv.org/abs/2511.01999", "authors": ["Sangyun Park", "Jin Kim", "Yuchen Cui", "Matthew S. Brown"], "title": "TRACE: Textual Reasoning for Affordance Coordinate Extraction", "comment": "ICCV 2025. *Equal contribution. {\\dag}Corresponding author", "summary": "Vision-Language Models (VLMs) struggle to translate high-level instructions\ninto the precise spatial affordances required for robotic manipulation. While\nvisual Chain-of-Thought (CoT) methods exist, they are often computationally\nintensive. In this work, we introduce TRACE (Textual Reasoning for Affordance\nCoordinate Extraction), a novel methodology that integrates a textual Chain of\nReasoning (CoR) into the affordance prediction process. We use this methodology\nto create the TRACE dataset, a large-scale collection created via an autonomous\npipeline that pairs instructions with explicit textual rationales. By\nfine-tuning a VLM on this data, our model learns to externalize its spatial\nreasoning before acting. Our experiments show that our TRACE-tuned model\nachieves state-of-the-art performance, reaching 48.1% accuracy on the primary\nWhere2Place (W2P) benchmark (a 9.6% relative improvement) and 55.0% on the more\nchallenging W2P(h) subset. Crucially, an ablation study demonstrates that\nperformance scales directly with the amount of reasoning data used, confirming\nthe CoR's effectiveness. Furthermore, analysis of the model's attention maps\nreveals an interpretable reasoning process where focus shifts dynamically\nacross reasoning steps. This work shows that training VLMs to generate a\ntextual CoR is an effective and robust strategy for enhancing the precision,\nreliability, and interpretability of VLM-based robot control. Our dataset and\ncode are available at https://github.com/jink-ucla/TRACE"}
{"id": "2511.02370", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02370", "abs": "https://arxiv.org/abs/2511.02370", "authors": ["Adnan Hoq", "Matthew Facciani", "Tim Weninger"], "title": "AI Credibility Signals Outrank Institutions and Engagement in Shaping News Perception on Social Media", "comment": null, "summary": "AI-generated content is rapidly becoming a salient component of online\ninformation ecosystems, yet its influence on public trust and epistemic\njudgments remains poorly understood. We present a large-scale mixed-design\nexperiment (N = 1,000) investigating how AI-generated credibility scores affect\nuser perception of political news. Our results reveal that AI feedback\nsignificantly moderates partisan bias and institutional distrust, surpassing\ntraditional engagement signals such as likes and shares. These findings\ndemonstrate the persuasive power of generative AI and suggest a need for design\nstrategies that balance epistemic influence with user autonomy."}
{"id": "2511.02015", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.02015", "abs": "https://arxiv.org/abs/2511.02015", "authors": ["Jace Aldrich", "Odest Chadwicke Jenkins"], "title": "Stein-based Optimization of Sampling Distributions in Model Predictive Path Integral Control", "comment": "8 pages, 6 figures", "summary": "This paper presents a novel method for Model Predictive Path Integral (MPPI)\ncontrol that optimizes sample generation towards an optimal trajectory through\nStein Variational Gradient Descent (SVGD). MPPI is traditionally reliant on\nrandomly sampled trajectories, often by a Gaussian distribution. The result can\nlead to sample deprivation, under-representing the space of possible\ntrajectories, and yield suboptimal results. Through introducing SVGD updates in\nbetween MPPI environment steps, we present Stein-Optimized Path-Integral\nInference (SOPPI), an MPPI/SVGD algorithm that can dynamically update noise\ndistributions at runtime to shape a more optimal representation without an\nexcessive increase in computational requirements. We demonstrate the efficacy\nof our method systems ranging from a Cart-Pole to a two-dimensional bipedal\nwalking task, indicating improved performance above standard MPPI across a\nrange of hyper-parameters and demonstrate feasibility at lower particle counts.\nWe discuss the applicability of this MPPI/SVGD method to higher\ndegree-of-freedom systems, as well as its potential to new developments in\nstate-of-the-art differentiable simulators."}
{"id": "2511.02378", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.02378", "abs": "https://arxiv.org/abs/2511.02378", "authors": ["Riccardo Bovo", "Daniele Giunchi", "Pasquale Cascarano", "Eric J. Gonzalez", "Mar Gonzalez-Franco"], "title": "Revisiting put-that-there, context aware window interactions via LLMs", "comment": null, "summary": "We revisit Bolt's classic \"Put-That-There\" concept for modern head-mounted\ndisplays by pairing Large Language Models (LLMs) with XR sensor and tech stack.\nThe agent fuses (i) a semantically segmented 3-D environment, (ii) live\napplication metadata, and (iii) users' verbal, pointing, and head-gaze cues to\nissue JSON window-placement actions. As a result, users can manage a panoramic\nworkspace through: (1) explicit commands (\"Place Google Maps on the coffee\ntable\"), (2) deictic speech plus gestures (\"Put that there\"), or (3) high-level\ngoals (\"I need to send a message\"). Unlike traditional explicit interfaces, our\nsystem supports one-to-many action mappings and goal-centric reasoning,\nallowing the LLM to dynamically infer relevant applications and layout\ndecisions, including interrelationships across tools. This enables seamless,\nintent-driven interaction without manual window juggling in immersive XR\nenvironments."}
{"id": "2511.02036", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.02036", "abs": "https://arxiv.org/abs/2511.02036", "authors": ["Parsa Hosseininejad", "Kimia Khabiri", "Shishir Gopinath", "Soudabeh Mohammadhashemi", "Karthik Dantu", "Steven Y. Ko"], "title": "TurboMap: GPU-Accelerated Local Mapping for Visual SLAM", "comment": "Submitted to ICRA 2026", "summary": "This paper presents TurboMap, a GPU-accelerated and CPU-optimized local\nmapping module for visual SLAM systems. We identify key performance bottlenecks\nin the local mapping process for visual SLAM and address them through targeted\nGPU and CPU optimizations. Specifically, we offload map point triangulation and\nfusion to the GPU, accelerate redundant keyframe culling on the CPU, and\nintegrate a GPU-accelerated solver to speed up local bundle adjustment. Our\nimplementation is built on top of ORB-SLAM3 and leverages CUDA for GPU\nprogramming. The experimental results show that TurboMap achieves an average\nspeedup of 1.3x in the EuRoC dataset and 1.6x in the TUM-VI dataset in the\nlocal mapping module, on both desktop and embedded platforms, while maintaining\nthe accuracy of the original system."}
{"id": "2511.02428", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.02428", "abs": "https://arxiv.org/abs/2511.02428", "authors": ["Michelle Bak", "Kexin Quan", "Tre Tomaszewski", "Jessie Chin"], "title": "Can Conversational AI Counsel for Change? A Theory-Driven Approach to Supporting Dietary Intentions in Ambivalent Individuals", "comment": null, "summary": "Adherence to healthy diets reduces chronic illness risk, yet rates remain\nlow. Large Language Models (LLMs) are increasingly used for health\ncommunication but often struggle to engage individuals with ambivalent\nintentions at a pivotal stage of the Transtheoretical Model (TTM). We developed\nCounselLLM, an open-source model enhanced through persona design and few-shot,\ndomain-specific prompts grounded in TTM and Motivational Interviewing (MI). In\ncontrolled evaluations, CounselLLM showed stronger use of TTM subprocesses and\nMI affirmations than human counselors, with comparable linguistic robustness\nbut expressed in more concrete terms. A user study then tested CounselLLM in an\ninteractive counseling setting against a baseline system. While knowledge and\nperceptions did not change, participants' intentions for immediate dietary\nchange increased significantly after interacting with CounselLLM. Participants\nalso rated it as easy to use, understandable, and supportive. These findings\nsuggest theory-driven LLMs can effectively engage ambivalent individuals and\nprovide a scalable approach to digital counseling."}
{"id": "2511.02060", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.02060", "abs": "https://arxiv.org/abs/2511.02060", "authors": ["Hersh Sanghvi", "Spencer Folk", "Vijay Kumar", "Camillo Jose Taylor"], "title": "TACO: Trajectory-Aware Controller Optimization for Quadrotors", "comment": "8 pages, 6 figures. In submission to ICRA 2026", "summary": "Controller performance in quadrotor trajectory tracking depends heavily on\nparameter tuning, yet standard approaches often rely on fixed, manually tuned\nparameters that sacrifice task-specific performance. We present\nTrajectory-Aware Controller Optimization (TACO), a framework that adapts\ncontroller parameters online based on the upcoming reference trajectory and\ncurrent quadrotor state. TACO employs a learned predictive model and a\nlightweight optimization scheme to optimize controller gains in real time with\nrespect to a broad class of trajectories, and can also be used to adapt\ntrajectories to improve dynamic feasibility while respecting smoothness\nconstraints. To enable large-scale training, we also introduce a parallelized\nquadrotor simulator supporting fast data collection on diverse trajectories.\nExperiments on a variety of trajectory types show that TACO outperforms\nconventional, static parameter tuning while operating orders of magnitude\nfaster than black-box optimization baselines, enabling practical real-time\ndeployment on a physical quadrotor. Furthermore, we show that adapting\ntrajectories using TACO significantly reduces the tracking error obtained by\nthe quadrotor."}
{"id": "2511.02455", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.02455", "abs": "https://arxiv.org/abs/2511.02455", "authors": ["Yuhan Liu", "Varun Nagaraj Rao", "Sohyeon Hwang", "Janet Vertesi", "Andrés Monroy-Hernández"], "title": "OpenCourier: an Open Protocol for Building a Decentralized Ecosystem of Community-owned Delivery Platforms", "comment": null, "summary": "Although the platform gig economy has reshaped the landscape of work, its\ncentralized operation by select actors has brought about challenges that\nimpedes workers' well-being. We present the architecture and design of\nOpenCourier, an open protocol that defines communication patterns within a\ndecentralized ecosystem of delivery platforms. Through this protocol, we aim to\naddress three key challenges in the current economy: power imbalances between\nthe platform and workers, information asymmetries caused by black-boxed\nalgorithms and value misalignments in the infrastructure design process. With\nthe OpenCourier protocol, we outline a blueprint for community-owned ecosystem\nof delivery platforms that centers worker agency, transparency, and bottom-up\ndesign."}
{"id": "2511.02097", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.02097", "abs": "https://arxiv.org/abs/2511.02097", "authors": ["Peng-Fei Zhang", "Ying Cheng", "Xiaofan Sun", "Shijie Wang", "Lei Zhu", "Heng Tao Shen"], "title": "A Step Toward World Models: A Survey on Robotic Manipulation", "comment": "24 pages, 5 figures", "summary": "Autonomous agents are increasingly expected to operate in complex, dynamic,\nand uncertain environments, performing tasks such as manipulation, navigation,\nand decision-making. Achieving these capabilities requires agents to understand\nthe underlying mechanisms and dynamics of the world, moving beyond purely\nreactive control or simple replication of observed states. This motivates the\ndevelopment of world models as internal representations that encode\nenvironmental states, capture dynamics, and enable prediction, planning, and\nreasoning. Despite growing interest, the definition, scope, architectures, and\nessential capabilities of world models remain ambiguous. In this survey, rather\nthan directly imposing a fixed definition and limiting our scope to methods\nexplicitly labeled as world models, we examine approaches that exhibit the core\ncapabilities of world models through a review of methods in robotic\nmanipulation. We analyze their roles across perception, prediction, and\ncontrol, identify key challenges and solutions, and distill the core\ncomponents, capabilities, and functions that a real world model should possess.\nBuilding on this analysis, we aim to outline a roadmap for developing\ngeneralizable and practical world models for robotics."}
{"id": "2511.02468", "categories": ["cs.HC", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.02468", "abs": "https://arxiv.org/abs/2511.02468", "authors": ["Chuhan Jiao", "Zhiming Hu", "Andreas Bulling"], "title": "HAGI++: Head-Assisted Gaze Imputation and Generation", "comment": "Extended version of our UIST'25 paper \"HAGI: Head-Assisted Gaze\n  Imputation for Mobile Eye Trackers\"", "summary": "Mobile eye tracking plays a vital role in capturing human visual attention\nacross both real-world and extended reality (XR) environments, making it an\nessential tool for applications ranging from behavioural research to\nhuman-computer interaction. However, missing values due to blinks, pupil\ndetection errors, or illumination changes pose significant challenges for\nfurther gaze data analysis. To address this challenge, we introduce HAGI++ - a\nmulti-modal diffusion-based approach for gaze data imputation that, for the\nfirst time, uses the integrated head orientation sensors to exploit the\ninherent correlation between head and eye movements. HAGI++ employs a\ntransformer-based diffusion model to learn cross-modal dependencies between eye\nand head representations and can be readily extended to incorporate additional\nbody movements. Extensive evaluations on the large-scale Nymeria, Ego-Exo4D,\nand HOT3D datasets demonstrate that HAGI++ consistently outperforms\nconventional interpolation methods and deep learning-based time-series\nimputation baselines in gaze imputation. Furthermore, statistical analyses\nconfirm that HAGI++ produces gaze velocity distributions that closely match\nactual human gaze behaviour, ensuring more realistic gaze imputations.\nMoreover, by incorporating wrist motion captured from commercial wearable\ndevices, HAGI++ surpasses prior methods that rely on full-body motion capture\nin the extreme case of 100% missing gaze data (pure gaze generation). Our\nmethod paves the way for more complete and accurate eye gaze recordings in\nreal-world settings and has significant potential for enhancing gaze-based\nanalysis and interaction across various application domains."}
{"id": "2511.02147", "categories": ["cs.RO", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.02147", "abs": "https://arxiv.org/abs/2511.02147", "authors": ["Tyler M. Paine", "Anastasia Bizyaeva", "Michael R. Benjamin"], "title": "Census-Based Population Autonomy For Distributed Robotic Teaming", "comment": "16 pages, 17 figures", "summary": "Collaborating teams of robots show promise due in their ability to complete\nmissions more efficiently and with improved robustness, attributes that are\nparticularly useful for systems operating in marine environments. A key issue\nis how to model, analyze, and design these multi-robot systems to realize the\nfull benefits of collaboration, a challenging task since the domain of\nmulti-robot autonomy encompasses both collective and individual behaviors. This\npaper introduces a layered model of multi-robot autonomy that uses the\nprinciple of census, or a weighted count of the inputs from neighbors, for\ncollective decision-making about teaming, coupled with multi-objective behavior\noptimization for individual decision-making about actions. The census component\nis expressed as a nonlinear opinion dynamics model and the multi-objective\nbehavior optimization is accomplished using interval programming. This model\ncan be reduced to recover foundational algorithms in distributed optimization\nand control, while the full model enables new types of collective behaviors\nthat are useful in real-world scenarios. To illustrate these points, a new\nmethod for distributed optimization of subgroup allocation is introduced where\nrobots use a gradient descent algorithm to minimize portions of the cost\nfunctions that are locally known, while being influenced by the opinion states\nfrom neighbors to account for the unobserved costs. With this method the group\ncan collectively use the information contained in the Hessian matrix of the\ntotal global cost. The utility of this model is experimentally validated in\nthree categorically different experiments with fleets of autonomous surface\nvehicles: an adaptive sampling scenario, a high value unit protection scenario,\nand a competitive game of capture the flag."}
{"id": "2511.02515", "categories": ["cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02515", "abs": "https://arxiv.org/abs/2511.02515", "authors": ["Obada Kraishan"], "title": "Emotional Contagion in Code: How GitHub Emoji Reactions Shape Developer Collaboration", "comment": "12 pages, 3 figures. Analysis of 106,743 emoji reactions across 2,098\n  GitHub items", "summary": "Developer communities increasingly rely on emoji reactions to communicate,\nbut we know little about how these emotional signals spread and influence\ntechnical discussions. We analyzed 2,098 GitHub issues and pull requests across\n50 popular repositories, examining patterns in 106,743 emoji reactions to\nunderstand emotional contagion in software development. Our findings reveal a\nsurprisingly positive emotional landscape: 57.4% of discussions carry positive\nsentiment, with positive emotional cascades outnumbering negative ones 23:1. We\nidentified five distinct patterns, with \"instant enthusiasm\" affecting 45.6% of\nitems--nearly half receive immediate positive reinforcement. Statistical\nanalysis confirms strong emotional contagion (r=0.679, p<0.001) with a massive\neffect size (d=2.393), suggesting that initial reactions powerfully shape\ndiscussion trajectories. These findings challenge assumptions about technical\ndiscourse being purely rational, demonstrating that even minimal emotional\nsignals create measurable ripple effects. Our work provides empirical evidence\nthat emoji reactions are not mere decoration but active forces shaping\ncollaborative outcomes in software development."}
{"id": "2511.02162", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.02162", "abs": "https://arxiv.org/abs/2511.02162", "authors": ["Alexander Htet Kyaw", "Richa Gupta", "Dhruv Shah", "Anoop Sinha", "Kory Mathewson", "Stefanie Pender", "Sachin Chitta", "Yotto Koga", "Faez Ahmed", "Lawrence Sass", "Randall Davis"], "title": "Text to Robotic Assembly of Multi Component Objects using 3D Generative AI and Vision Language Models", "comment": "Accepted to NeurIPS 2025, Conference on Neural Information Processing\n  Systems, Creative AI Track", "summary": "Advances in 3D generative AI have enabled the creation of physical objects\nfrom text prompts, but challenges remain in creating objects involving multiple\ncomponent types. We present a pipeline that integrates 3D generative AI with\nvision-language models (VLMs) to enable the robotic assembly of multi-component\nobjects from natural language. Our method leverages VLMs for zero-shot,\nmulti-modal reasoning about geometry and functionality to decompose\nAI-generated meshes into multi-component 3D models using predefined structural\nand panel components. We demonstrate that a VLM is capable of determining which\nmesh regions need panel components in addition to structural components, based\non object functionality. Evaluation across test objects shows that users\npreferred the VLM-generated assignments 90.6% of the time, compared to 59.4%\nfor rule-based and 2.5% for random assignment. Lastly, the system allows users\nto refine component assignments through conversational feedback, enabling\ngreater human control and agency in making physical objects with generative AI\nand robotics."}
{"id": "2511.02560", "categories": ["cs.HC", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.02560", "abs": "https://arxiv.org/abs/2511.02560", "authors": ["Dan Bohus", "Sean Andrist", "Ann Paradiso", "Nick Saw", "Tim Schoonbeek", "Maia Stiber"], "title": "SigmaCollab: An Application-Driven Dataset for Physically Situated Collaboration", "comment": null, "summary": "We introduce SigmaCollab, a dataset enabling research on physically situated\nhuman-AI collaboration. The dataset consists of a set of 85 sessions in which\nuntrained participants were guided by a mixed-reality assistive AI agent in\nperforming procedural tasks in the physical world. SigmaCollab includes a set\nof rich, multimodal data streams, such as the participant and system audio,\negocentric camera views from the head-mounted device, depth maps, head, hand\nand gaze tracking information, as well as additional annotations performed\npost-hoc. While the dataset is relatively small in size (~ 14 hours), its\napplication-driven and interactive nature brings to the fore novel research\nchallenges for human-AI collaboration, and provides more realistic testing\ngrounds for various AI models operating in this space. In future work, we plan\nto use the dataset to construct a set of benchmarks for physically situated\ncollaboration in mixed-reality task assistive scenarios. SigmaCollab is\navailable at https://github.com/microsoft/SigmaCollab."}
{"id": "2511.02167", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.02167", "abs": "https://arxiv.org/abs/2511.02167", "authors": ["Tian Hao", "Tong Lu", "Che Chan"], "title": "Kinematic and Ergonomic Design of a Robotic Arm for Precision Laparoscopic Surgery", "comment": null, "summary": "Robotic assistance in minimally invasive surgery can greatly enhance surgical\nprecision and reduce surgeon fatigue. This paper presents a focused\ninvestigation on the kinematic and ergonomic design principles for a\nlaparoscopic surgical robotic arm aimed at high-precision tasks. We propose a\n7-degree-of-freedom (7-DOF) robotic arm system that incorporates a remote\ncenter of motion (RCM) at the instrument insertion point and ergonomic\nconsiderations to improve surgeon interaction. The design is implemented on a\ngeneral-purpose robotic platform, and a series of simulated surgical tasks were\nperformed to evaluate targeting accuracy, task efficiency, and surgeon comfort\ncompared to conventional manual laparoscopy. Experimental results demonstrate\nthat the optimized robotic design achieves significantly improved targeting\naccuracy (error reduced by over 50%) and shorter task completion times, while\nsubstantially lowering operator muscle strain and discomfort. These findings\nvalidate the importance of kinematic optimization (such as added articulations\nand tremor filtering) and human-centered ergonomic design in enhancing the\nperformance of robot-assisted surgery. The insights from this work can guide\nthe development of next-generation surgical robots that improve surgical\noutcomes and ergonomics for the operating team."}
{"id": "2511.02694", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.02694", "abs": "https://arxiv.org/abs/2511.02694", "authors": ["Siqi Zhang", "Mayank Goel", "Justin Chan"], "title": "DropleX: Liquid sensing on tablet touchscreens", "comment": null, "summary": "We present DropleX, the first system that enables liquid sensing using the\ncapacitive touchscreen of commodity tablets. DropleX detects microliter-scale\nliquid samples, and performs non-invasive, through-container measurements to\ndetect whether a drink has been spiked or if a sealed liquid has been\ncontaminated. These capabilities are made possible by a physics-informed\nmechanism that disables the touchscreen's built-in adaptive filters, originally\ndesigned to reject the effects of liquid drops such as rain, without any\nhardware modifications. We model the touchscreen's sensing capabilities,\nlimits, and non-idealities to inform the design of a signal processing and\nlearning-based pipeline for liquid sensing. Our system achieves 96-99% accuracy\nin detecting microliter-scale adulteration in soda, wine, and milk, 93-96%\naccuracy in threshold detection of trace chemical concentrations, and 86-96%\naccuracy in through-container adulterant detection. Given the predominance of\ntouchscreens, these exploratory results can open new opportunities for liquid\nsensing on everyday devices."}
{"id": "2511.02192", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.02192", "abs": "https://arxiv.org/abs/2511.02192", "authors": ["Linxin Hou", "Qirui Wu", "Zhihang Qin", "Neil Banerjee", "Yongxin Guo", "Cecilia Laschi"], "title": "A Quantitative Comparison of Centralised and Distributed Reinforcement Learning-Based Control for Soft Robotic Arms", "comment": "7 pages, 4 figures, 2 tables, submitted to RoboSoft 2026", "summary": "This paper presents a quantitative comparison between centralised and\ndistributed multi-agent reinforcement learning (MARL) architectures for\ncontrolling a soft robotic arm modelled as a Cosserat rod in simulation. Using\nPyElastica and the OpenAI Gym interface, we train both a global Proximal Policy\nOptimisation (PPO) controller and a Multi-Agent PPO (MAPPO) under identical\nbudgets. Both approaches are based on the arm having $n$ number of controlled\nsections. The study systematically varies $n$ and evaluates the performance of\nthe arm to reach a fixed target in three scenarios: default baseline condition,\nrecovery from external disturbance, and adaptation to actuator failure.\nQuantitative metrics used for the evaluation are mean action magnitude, mean\nfinal distance, mean episode length, and success rate. The results show that\nthere are no significant benefits of the distributed policy when the number of\ncontrolled sections $n\\le4$. In very simple systems, when $n\\le2$, the\ncentralised policy outperforms the distributed one. When $n$ increases to $4<\nn\\le 12$, the distributed policy shows a high sample efficiency. In these\nsystems, distributed policy promotes a stronger success rate, resilience, and\nrobustness under local observability and yields faster convergence given the\nsame sample size. However, centralised policies achieve much higher time\nefficiency during training as it takes much less time to train the same size of\nsamples. These findings highlight the trade-offs between centralised and\ndistributed policy in reinforcement learning-based control for soft robotic\nsystems and provide actionable design guidance for future sim-to-real transfer\nin soft rod-like manipulators."}
{"id": "2511.02807", "categories": ["cs.HC", "H.5.1; I.2.6; I.2.11"], "pdf": "https://arxiv.org/pdf/2511.02807", "abs": "https://arxiv.org/abs/2511.02807", "authors": ["You-Jin Kim", "Misha Sra", "Tobias Höllerer"], "title": "Audience Amplified: Virtual Audiences in Asynchronously Performed AR Theater", "comment": "Conference Paper, 10 pages. Published at the 2024 IEEE International\n  Symposium on Mixed and Augmented Reality (ISMAR)", "summary": "Audience reactions can considerably enhance live experiences; conversely, in\nanytime-anywhere augmented reality (AR) experiences, large crowds of people\nmight not always be available to congregate. To get closer to simulating live\nevents with large audiences, we created a mobile AR experience where users can\nwander around naturally and engage in AR theater with virtual audiences trained\nfrom real audiences using imitation learning. This allows us to carefully\ncapture the essence of human imperfections and behavior in artificial\nintelligence (AI) audiences. The result is a novel mobile AR experience in\nwhich solitary AR users experience an augmented performance in a physical space\nwith a virtual audience. Virtual dancers emerge from the surroundings,\naccompanied by a digitally simulated audience, to provide a community\nexperience akin to immersive theater. In a pilot study, simulated human avatars\nwere vastly preferred over just audience audio commentary. We subsequently\nengaged 20 participants as attendees of an AR dance performance, comparing a\nno-audience condition with a simulated audience of six onlookers. Through\nquestionnaires and experience reports, we investigated user reactions and\nbehavior. Our results demonstrate that the presence of virtual audience members\ncaused attendees to perceive the performance as a social experience with\nincreased interest and involvement in the event. On the other hand, for some\nattendees, the dance performances without the virtual audience evoked a\nstronger positive sentiment."}
{"id": "2511.02239", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02239", "abs": "https://arxiv.org/abs/2511.02239", "authors": ["Youngjin Hong", "Houjian Yu", "Mingen Li", "Changhyun Choi"], "title": "LACY: A Vision-Language Model-based Language-Action Cycle for Self-Improving Robotic Manipulation", "comment": "Preprint. Project page: https://vla2026.github.io/LACY/", "summary": "Learning generalizable policies for robotic manipulation increasingly relies\non large-scale models that map language instructions to actions (L2A). However,\nthis one-way paradigm often produces policies that execute tasks without deeper\ncontextual understanding, limiting their ability to generalize or explain their\nbehavior. We argue that the complementary skill of mapping actions back to\nlanguage (A2L) is essential for developing more holistic grounding. An agent\ncapable of both acting and explaining its actions can form richer internal\nrepresentations and unlock new paradigms for self-supervised learning. We\nintroduce LACY (Language-Action Cycle), a unified framework that learns such\nbidirectional mappings within a single vision-language model. LACY is jointly\ntrained on three synergistic tasks: generating parameterized actions from\nlanguage (L2A), explaining observed actions in language (A2L), and verifying\nsemantic consistency between two language descriptions (L2C). This enables a\nself-improving cycle that autonomously generates and filters new training data\nthrough an active augmentation strategy targeting low-confidence cases, thereby\nimproving the model without additional human labels. Experiments on\npick-and-place tasks in both simulation and the real world show that LACY\nimproves task success rates by 56.46% on average and yields more robust\nlanguage-action grounding for robotic manipulation. Project page:\nhttps://vla2026.github.io/LACY/"}
{"id": "2511.02162", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.02162", "abs": "https://arxiv.org/abs/2511.02162", "authors": ["Alexander Htet Kyaw", "Richa Gupta", "Dhruv Shah", "Anoop Sinha", "Kory Mathewson", "Stefanie Pender", "Sachin Chitta", "Yotto Koga", "Faez Ahmed", "Lawrence Sass", "Randall Davis"], "title": "Text to Robotic Assembly of Multi Component Objects using 3D Generative AI and Vision Language Models", "comment": "Accepted to NeurIPS 2025, Conference on Neural Information Processing\n  Systems, Creative AI Track", "summary": "Advances in 3D generative AI have enabled the creation of physical objects\nfrom text prompts, but challenges remain in creating objects involving multiple\ncomponent types. We present a pipeline that integrates 3D generative AI with\nvision-language models (VLMs) to enable the robotic assembly of multi-component\nobjects from natural language. Our method leverages VLMs for zero-shot,\nmulti-modal reasoning about geometry and functionality to decompose\nAI-generated meshes into multi-component 3D models using predefined structural\nand panel components. We demonstrate that a VLM is capable of determining which\nmesh regions need panel components in addition to structural components, based\non object functionality. Evaluation across test objects shows that users\npreferred the VLM-generated assignments 90.6% of the time, compared to 59.4%\nfor rule-based and 2.5% for random assignment. Lastly, the system allows users\nto refine component assignments through conversational feedback, enabling\ngreater human control and agency in making physical objects with generative AI\nand robotics."}
{"id": "2511.02294", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.02294", "abs": "https://arxiv.org/abs/2511.02294", "authors": ["Ruiyong Yuan", "Jieji Ren", "Zhanxuan Peng", "Feifei Chen", "Guoying Gu"], "title": "SuckTac: Camera-based Tactile Sucker for Unstructured Surface Perception and Interaction", "comment": null, "summary": "Suckers are significant for robots in picking, transferring, manipulation and\nlocomotion on diverse surfaces. However, most of the existing suckers lack\nhigh-fidelity perceptual and tactile sensing, which impedes them from resolving\nthe fine-grained geometric features and interaction status of the target\nsurface. This limits their robust performance with irregular objects and in\ncomplex, unstructured environments. Inspired by the adaptive structure and\nhigh-performance sensory capabilities of cephalopod suckers, in this paper, we\npropose a novel, intelligent sucker, named SuckTac, that integrates a\ncamera-based tactile sensor directly within its optimized structure to provide\nhigh-density perception and robust suction. Specifically, through joint\nstructure design and optimization and based on a multi-material integrated\ncasting technique, a camera and light source are embedded into the sucker,\nwhich enables in-situ, high-density perception of fine details like surface\nshape, texture and roughness. To further enhance robustness and adaptability,\nthe sucker's mechanical design is also optimized by refining its profile,\nadding a compliant lip, and incorporating surface microstructure. Extensive\nexperiments, including challenging tasks such as robotic cloth manipulation and\nsoft mobile robot inspection, demonstrate the superior performance and broad\napplicability of the proposed system."}
{"id": "2511.02315", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.02315", "abs": "https://arxiv.org/abs/2511.02315", "authors": ["Zifei Wu", "Lijie Wang", "Zhe Yang", "Shijie Yang", "Liang Wang", "Haoran Fu", "Yinliang Cai", "Rong Xiong"], "title": "ZJUNlict Extended Team Description Paper 2025", "comment": null, "summary": "This paper presents the ZJUNlict team's work over the past year, covering\nboth hardware and software advancements. In the hardware domain, the\nintegration of an IMU into the v2023 robot was completed to enhance posture\naccuracy and angular velocity planning. On the software side, key modules were\noptimized, including the strategy and CUDA modules, with significant\nimprovements in decision making efficiency, ball pursuit prediction, and ball\npossession prediction to adapt to high-tempo game dynamics."}
{"id": "2511.02342", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.02342", "abs": "https://arxiv.org/abs/2511.02342", "authors": ["Lin Yang", "Jinwoo Lee", "Domenico Campolo", "H. Jin Kim", "Jeonghyun Byun"], "title": "Whole-body motion planning and safety-critical control for aerial manipulation", "comment": "Submitted to 2026 IFAC World Congress with the Journal option\n  (MECHATRONICS)", "summary": "Aerial manipulation combines the maneuverability of multirotors with the\ndexterity of robotic arms to perform complex tasks in cluttered spaces. Yet\nplanning safe, dynamically feasible trajectories remains difficult due to\nwhole-body collision avoidance and the conservativeness of common geometric\nabstractions such as bounding boxes or ellipsoids. We present a whole-body\nmotion planning and safety-critical control framework for aerial manipulators\nbuilt on superquadrics (SQs). Using an SQ-plus-proxy representation, we model\nboth the vehicle and obstacles with differentiable, geometry-accurate surfaces.\nLeveraging this representation, we introduce a maximum-clearance planner that\nfuses Voronoi diagrams with an equilibrium-manifold formulation to generate\nsmooth, collision-aware trajectories. We further design a safety-critical\ncontroller that jointly enforces thrust limits and collision avoidance via\nhigh-order control barrier functions. In simulation, our approach outperforms\nsampling-based planners in cluttered environments, producing faster, safer, and\nsmoother trajectories and exceeding ellipsoid-based baselines in geometric\nfidelity. Actual experiments on a physical aerial-manipulation platform confirm\nfeasibility and robustness, demonstrating consistent performance across\nsimulation and hardware settings. The video can be found at\nhttps://youtu.be/hQYKwrWf1Ak."}
{"id": "2511.02504", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.02504", "abs": "https://arxiv.org/abs/2511.02504", "authors": ["Le Chen", "Yi Zhao", "Jan Schneider", "Quankai Gao", "Simon Guist", "Cheng Qian", "Juho Kannala", "Bernhard Schölkopf", "Joni Pajarinen", "Dieter Büchler"], "title": "Dexterous Robotic Piano Playing at Scale", "comment": null, "summary": "Endowing robot hands with human-level dexterity has been a long-standing goal\nin robotics. Bimanual robotic piano playing represents a particularly\nchallenging task: it is high-dimensional, contact-rich, and requires fast,\nprecise control. We present OmniPianist, the first agent capable of performing\nnearly one thousand music pieces via scalable, human-demonstration-free\nlearning. Our approach is built on three core components. First, we introduce\nan automatic fingering strategy based on Optimal Transport (OT), allowing the\nagent to autonomously discover efficient piano-playing strategies from scratch\nwithout demonstrations. Second, we conduct large-scale Reinforcement Learning\n(RL) by training more than 2,000 agents, each specialized in distinct music\npieces, and aggregate their experience into a dataset named RP1M++, consisting\nof over one million trajectories for robotic piano playing. Finally, we employ\na Flow Matching Transformer to leverage RP1M++ through large-scale imitation\nlearning, resulting in the OmniPianist agent capable of performing a wide range\nof musical pieces. Extensive experiments and ablation studies highlight the\neffectiveness and scalability of our approach, advancing dexterous robotic\npiano playing at scale."}
{"id": "2511.02761", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.02761", "abs": "https://arxiv.org/abs/2511.02761", "authors": ["Seth Stewart", "Joseph Pawelski", "Steve Ward", "Andrew J. Petruska"], "title": "Non-Contact Manipulation of Induced Magnetic Dipoles", "comment": null, "summary": "Extending the field of magnetic manipulation to conductive, non-magnetic\nobjects opens the door for a wide array of applications previously limited to\nhard or soft magnetic materials. Of particular interest is the recycling of\nspace debris through the use of oscillating magnetic fields, which represent a\ncache of raw materials in an environment particularly suited to the low forces\ngenerated from inductive magnetic manipulation. Building upon previous work\nthat demonstrated 3D open-loop position control by leveraging the opposing\ndipole moment created from induced eddy currents, this work demonstrates\nclosed-loop position control of a semi-buoyant aluminum sphere in lab tests,\nand the efficacy of varying methods for force inversion is explored. The\nclosed-loop methods represent a critical first step towards wider applications\nfor 3-DOF position control of induced magnetic dipoles."}
{"id": "2511.02776", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.02776", "abs": "https://arxiv.org/abs/2511.02776", "authors": ["Shichao Fan", "Kun Wu", "Zhengping Che", "Xinhua Wang", "Di Wu", "Fei Liao", "Ning Liu", "Yixue Zhang", "Zhen Zhao", "Zhiyuan Xu", "Meng Li", "Qingjie Liu", "Shanghang Zhang", "Min Wan", "Jian Tang"], "title": "XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations", "comment": null, "summary": "Recent progress in large-scale robotic datasets and vision-language models\n(VLMs) has advanced research on vision-language-action (VLA) models. However,\nexisting VLA models still face two fundamental challenges: (i) producing\nprecise low-level actions from high-dimensional observations, (ii) bridging\ndomain gaps across heterogeneous data sources, including diverse robot\nembodiments and human demonstrations. Existing methods often encode latent\nvariables from either visual dynamics or robotic actions to guide policy\nlearning, but they fail to fully exploit the complementary multi-modal\nknowledge present in large-scale, heterogeneous datasets. In this work, we\npresent X Robotic Model 1 (XR-1), a novel framework for versatile and scalable\nVLA learning across diverse robots, tasks, and environments. XR-1 introduces\nthe \\emph{Unified Vision-Motion Codes (UVMC)}, a discrete latent representation\nlearned via a dual-branch VQ-VAE that jointly encodes visual dynamics and\nrobotic motion. UVMC addresses these challenges by (i) serving as an\nintermediate representation between the observations and actions, and (ii)\naligning multimodal dynamic information from heterogeneous data sources to\ncapture complementary knowledge. To effectively exploit UVMC, we propose a\nthree-stage training paradigm: (i) self-supervised UVMC learning, (ii)\nUVMC-guided pretraining on large-scale cross-embodiment robotic datasets, and\n(iii) task-specific post-training. We validate XR-1 through extensive\nreal-world experiments with more than 14,000 rollouts on six different robot\nembodiments, spanning over 120 diverse manipulation tasks. XR-1 consistently\noutperforms state-of-the-art baselines such as $\\pi_{0.5}$, $\\pi_0$, RDT,\nUniVLA, and GR00T-N1.5 while demonstrating strong generalization to novel\nobjects, background variations, distractors, and illumination changes. Our\nproject is at https://xr-1-vla.github.io/."}
{"id": "2511.02832", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02832", "abs": "https://arxiv.org/abs/2511.02832", "authors": ["Yanjie Ze", "Siheng Zhao", "Weizhuo Wang", "Angjoo Kanazawa", "Rocky Duan", "Pieter Abbeel", "Guanya Shi", "Jiajun Wu", "C. Karen Liu"], "title": "TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System", "comment": "Website: https://yanjieze.com/TWIST2", "summary": "Large-scale data has driven breakthroughs in robotics, from language models\nto vision-language-action models in bimanual manipulation. However, humanoid\nrobotics lacks equally effective data collection frameworks. Existing humanoid\nteleoperation systems either use decoupled control or depend on expensive\nmotion capture setups. We introduce TWIST2, a portable, mocap-free humanoid\nteleoperation and data collection system that preserves full whole-body control\nwhile advancing scalability. Our system leverages PICO4U VR for obtaining\nreal-time whole-body human motions, with a custom 2-DoF robot neck (cost around\n$250) for egocentric vision, enabling holistic human-to-humanoid control. We\ndemonstrate long-horizon dexterous and mobile humanoid skills and we can\ncollect 100 demonstrations in 15 minutes with an almost 100% success rate.\nBuilding on this pipeline, we propose a hierarchical visuomotor policy\nframework that autonomously controls the full humanoid body based on egocentric\nvision. Our visuomotor policy successfully demonstrates whole-body dexterous\nmanipulation and dynamic kicking tasks. The entire system is fully reproducible\nand open-sourced at https://yanjieze.com/TWIST2 . Our collected dataset is also\nopen-sourced at https://twist-data.github.io ."}
