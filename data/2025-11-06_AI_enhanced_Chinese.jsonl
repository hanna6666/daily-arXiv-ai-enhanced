{"id": "2511.02837", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02837", "abs": "https://arxiv.org/abs/2511.02837", "authors": ["Sotirios Konstantakos", "Sotirios Asparagkathos", "Moatasim Mahmoud", "Stamatia Rizou", "Enrico Quagliarini", "Gabriele Bernardini"], "title": "An extended reality-based framework for user risk training in urban built environment", "comment": null, "summary": "In the context of increasing urban risks, particularly from climate\nchange-induced flooding, this paper presents an extended Reality (XR)-based\nframework to improve user risk training within urban built environments. The\nframework is designed to improve risk awareness and preparedness among various\nstakeholders, including citizens, local authorities, and emergency responders.\nUsing immersive XR technologies, the training experience simulates real-world\nemergency scenarios, contributing to active participation and a deeper\nunderstanding of potential hazards and especially for floods. The framework\nhighlights the importance of stakeholder participation in its development,\nensuring that training modules are customized to address the specific needs of\ndifferent user groups. The iterative approach of the framework supports ongoing\nrefinement through user feedback and performance data, thus improving the\noverall effectiveness of risk training initiatives. This work outlines the\nmethodological phases involved in the framework's implementation, including i)\nuser flow mapping, ii) scenario selection, and iii) performance evaluation,\nwith a focus on the pilot application in Senigallia, Italy. The findings\nunderscore the potential of XR technologies to transform urban risk training,\npromoting a culture of preparedness and resilience against urban hazards.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eXR\u7684\u6846\u67b6\uff0c\u4ee5\u63d0\u5347\u57ce\u5e02\u73af\u5883\u4e2d\u5404\u5229\u76ca\u76f8\u5173\u8005\u7684\u98ce\u9669\u57f9\u8bad\u6548\u679c\uff0c\u7279\u522b\u9488\u5bf9\u6c14\u5019\u53d8\u5316\u5f15\u53d1\u7684\u6d2a\u6c34\u98ce\u9669\uff0c\u5f3a\u8c03\u53c2\u4e0e\u548c\u5b9a\u5236\u5316\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u968f\u7740\u57ce\u5e02\u98ce\u9669\u589e\u5927\uff0c\u5c24\u5176\u662f\u6c14\u5019\u53d8\u5316\u5f15\u53d1\u7684\u6d2a\u6c34\uff0c\u4e9f\u9700\u63d0\u9ad8\u5404\u65b9\u7684\u98ce\u9669\u610f\u8bc6\u548c\u51c6\u5907\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u7528\u6237\u6d41\u6620\u5c04\u3001\u573a\u666f\u9009\u62e9\u548c\u6027\u80fd\u8bc4\u4f30\u7b49\u65b9\u6cd5\u5b9e\u65bdXR\u6846\u67b6\uff0c\u8fdb\u884c\u8bd5\u70b9\u5e94\u7528\u3002", "result": "XR\u6846\u67b6\u901a\u8fc7\u6c89\u6d78\u5f0f\u6280\u672f\u6a21\u62df\u771f\u5b9e\u7684\u7d27\u6025\u573a\u666f\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u7528\u6237\u53c2\u4e0e\u5ea6\u548c\u5bf9\u6f5c\u5728\u5371\u9669\u7684\u7406\u89e3\u3002", "conclusion": "XR\u6280\u672f\u6709\u6f5c\u529b\u8f6c\u53d8\u57ce\u5e02\u98ce\u9669\u57f9\u8bad\uff0c\u4fc3\u8fdb\u5e94\u5bf9\u57ce\u5e02\u5371\u5bb3\u7684\u51c6\u5907\u548c\u97e7\u6027\u6587\u5316\u3002"}}
{"id": "2511.02838", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.02838", "abs": "https://arxiv.org/abs/2511.02838", "authors": ["Goran Bubas"], "title": "How ChatGPT and Gemini View the Elements of Communication Competence of Large Language Models: A Pilot Study", "comment": "Proceedings of the 36th Central European Conference on Information\n  and Intelligent Systems, Varazdin, Croatia", "summary": "A concise overview is provided of selected theoretical models of\ncommunication competence in the fields of linguistics, interpersonal\ncommunication, second language use, and human-robot interaction. The following\npractical research consisted of two case studies with the goals of\ninvestigating how advanced AI tools like ChatGPT and Gemini interpret elements\nof two communication competence theories in the context of Large Language Model\n(LLM) interactions with users. The focus was on these theoretical approaches:\n(1) an integrated linguistic-interpersonal model and (2) an interpersonal\n\"human-humanoid\" interaction model. The conclusion is that both approaches are\nsuitable for a better understanding of LLM-user interaction.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u6c9f\u901a\u80fd\u529b\u7406\u8bba\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5206\u6790\u5148\u8fdbAI\u5bf9\u6a21\u578b\u7684\u7406\u89e3\u3002", "motivation": "\u63a2\u8ba8\u8bed\u8a00\u5b66\u3001\u4eba\u9645\u6c9f\u901a\u3001\u7b2c\u4e8c\u8bed\u8a00\u4f7f\u7528\u548c\u4eba\u673a\u4ea4\u4e92\u9886\u57df\u7684\u6c9f\u901a\u80fd\u529b\u7406\u8bba\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\uff0c\u8c03\u67e5\u5148\u8fdb\u7684AI\u5de5\u5177\u5982\u4f55\u89e3\u91ca\u4e24\u79cd\u6c9f\u901a\u80fd\u529b\u7406\u8bba\u7684\u5143\u7d20\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cChatGPT\u548cGemini\u80fd\u591f\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e92\u52a8\u4e2d\u89e3\u91ca\u8fd9\u4e9b\u6c9f\u901a\u80fd\u529b\u7406\u8bba\u3002", "conclusion": "\u4e24\u79cd\u7406\u8bba\u6a21\u578b\u90fd\u9002\u5408\u4e8e\u66f4\u597d\u5730\u7406\u89e3LLM\u4e0e\u7528\u6237\u7684\u4e92\u52a8\u3002"}}
{"id": "2511.02839", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.02839", "abs": "https://arxiv.org/abs/2511.02839", "authors": ["Antonio Verdone", "Aidan Cardall", "Fardeen Siddiqui", "Motaz Nashawaty", "Danielle Rigau", "Youngjoon Kwon", "Mira Yousef", "Shalin Patel", "Alex Kieturakis", "Eric Kim", "Laura Heacock", "Beatriu Reig", "Yiqiu Shen"], "title": "Evaluating Generative AI as an Educational Tool for Radiology Resident Report Drafting", "comment": null, "summary": "Objective: Radiology residents require timely, personalized feedback to\ndevelop accurate image analysis and reporting skills. Increasing clinical\nworkload often limits attendings' ability to provide guidance. This study\nevaluates a HIPAA-compliant GPT-4o system that delivers automated feedback on\nbreast imaging reports drafted by residents in real clinical settings.\n  Methods: We analyzed 5,000 resident-attending report pairs from routine\npractice at a multi-site U.S. health system. GPT-4o was prompted with clinical\ninstructions to identify common errors and provide feedback. A reader study\nusing 100 report pairs was conducted. Four attending radiologists and four\nresidents independently reviewed each pair, determined whether predefined error\ntypes were present, and rated GPT-4o's feedback as helpful or not. Agreement\nbetween GPT and readers was assessed using percent match. Inter-reader\nreliability was measured with Krippendorff's alpha. Educational value was\nmeasured as the proportion of cases rated helpful.\n  Results: Three common error types were identified: (1) omission or addition\nof key findings, (2) incorrect use or omission of technical descriptors, and\n(3) final assessment inconsistent with findings. GPT-4o showed strong agreement\nwith attending consensus: 90.5%, 78.3%, and 90.4% across error types.\nInter-reader reliability showed moderate variability ({\\alpha} = 0.767, 0.595,\n0.567), and replacing a human reader with GPT-4o did not significantly affect\nagreement ({\\Delta} = -0.004 to 0.002). GPT's feedback was rated helpful in\nmost cases: 89.8%, 83.0%, and 92.0%.\n  Discussion: ChatGPT-4o can reliably identify key educational errors. It may\nserve as a scalable tool to support radiology education.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86GPT-4o\u7cfb\u7edf\u5728\u653e\u5c04\u79d1\u4f4f\u9662\u533b\u5e08\u4e73\u817a\u5f71\u50cf\u62a5\u544a\u4e2d\u7684\u53cd\u9988\u80fd\u529b\uff0c\u7ed3\u679c\u8868\u660eGPT-4o\u80fd\u591f\u6709\u6548\u8bc6\u522b\u6559\u80b2\u9519\u8bef\uff0c\u5e76\u88ab\u8ba4\u4e3a\u6709\u6559\u80b2\u4ef7\u503c\u3002", "motivation": "\u653e\u5c04\u79d1\u4f4f\u9662\u533b\u5e08\u9700\u8981\u53ca\u65f6\u7684\u4e2a\u6027\u5316\u53cd\u9988\uff0c\u4ee5\u63d0\u9ad8\u56fe\u50cf\u5206\u6790\u548c\u62a5\u544a\u6280\u80fd\uff0c\u4f46\u4e34\u5e8a\u5de5\u4f5c\u8d1f\u8377\u589e\u52a0\u9650\u5236\u4e86\u4e3b\u6cbb\u533b\u5e08\u7684\u6307\u5bfc\u80fd\u529b\u3002", "method": "\u5206\u6790\u6765\u81ea\u591a\u5bb6\u7f8e\u56fd\u533b\u7597\u7cfb\u7edf\u76845000\u5bf9\u4f4f\u9662\u533b\u5e08\u4e0e\u4e3b\u6cbb\u533b\u5e08\u7684\u62a5\u544a\u5bf9\uff0c\u4f7f\u7528GPT-4o\u8bc6\u522b\u5e38\u89c1\u9519\u8bef\u5e76\u63d0\u4f9b\u53cd\u9988\u3002", "result": "\u6210\u529f\u8bc6\u522b\u4e09\u79cd\u5e38\u89c1\u9519\u8bef\u7c7b\u578b\uff0c\u4e14GPT-4o\u4e0e\u4e3b\u6cbb\u533b\u5e08\u7684\u4e00\u81f4\u6027\u8f83\u9ad8\uff0c\u53cd\u9988\u88ab\u5927\u90e8\u5206\u6848\u4f8b\u8bc4\u4e3a\u6709\u5e2e\u52a9\u3002", "conclusion": "ChatGPT-4o\u80fd\u591f\u53ef\u9760\u5730\u8bc6\u522b\u5173\u952e\u6559\u80b2\u9519\u8bef\uff0c\u53ef\u80fd\u6210\u4e3a\u652f\u6301\u653e\u5c04\u79d1\u6559\u80b2\u7684\u53ef\u6269\u5c55\u5de5\u5177\u3002"}}
{"id": "2511.02840", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.02840", "abs": "https://arxiv.org/abs/2511.02840", "authors": ["Saizo Aoyagi"], "title": "Interview Survey on Attractivenesses of Place Re-creation Toward Developing a Virtual Twin Design Theory", "comment": "The final draft for Humaninterface 2025, in Japanese language", "summary": "It is often seen that real-world locations are re-created using models,\nmetaverse technology, or computer graphics. Although the surface-level purposes\nof these re-creations vary, the author hypothesizes that there exists an\nunderlying common attractiveness that remains unclear. This research aims to\nclarify the attractiveness and its structures of place re-creations through an\ninterview study with qualitative analysis. The interviews used examples of\nphysical re-creations, such as the model in Komazawa University's Zen Culture\nHistory Museum and some dioramas of Tokyo, as well as computer-generated\nre-creations of Shibuya using platforms like Minecraft and Project Plateau's 3D\ncity model. Using insights gained from this investigation, this study seeks to\nestablish a theoretical framework for designing virtual twins.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u8bbf\u8c08\u5206\u6790\u63a2\u8ba8\u73b0\u5b9e\u4e16\u754c\u5730\u70b9\u518d\u521b\u9020\u7684\u5438\u5f15\u529b\uff0c\u65e8\u5728\u5efa\u7acb\u865a\u62df\u53cc\u80de\u80ce\u8bbe\u8ba1\u7684\u7406\u8bba\u6846\u67b6\u3002", "motivation": "\u63ed\u793a\u73b0\u5b9e\u4e16\u754c\u5730\u70b9\u518d\u521b\u9020\u7684\u5171\u540c\u5438\u5f15\u529b\u5c1a\u4e0d\u6e05\u6670\uff0c\u7814\u7a76\u65e8\u5728\u660e\u786e\u8fd9\u79cd\u5438\u5f15\u529b\u3002", "method": "\u901a\u8fc7\u5b9a\u6027\u5206\u6790\u7684\u8bbf\u8c08\u7814\u7a76\uff0c\u63a2\u7d22\u5730\u70b9\u518d\u521b\u9020\u7684\u5438\u5f15\u529b\u53ca\u5176\u7ed3\u6784\u3002", "result": "\u57fa\u4e8e\u5bf9\u7269\u7406\u518d\u521b\u9020\u548c\u8ba1\u7b97\u673a\u751f\u6210\u518d\u521b\u9020\u7684\u5b9e\u4f8b\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u76f8\u5173\u7406\u8bba\u6846\u67b6\u3002", "conclusion": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u865a\u62df\u53cc\u80de\u80ce\u7684\u8bbe\u8ba1\u3002"}}
{"id": "2511.02937", "categories": ["cs.RO", "cs.SE", "cs.SY", "eess.SY", "I.2.9; I.1.4; J.7"], "pdf": "https://arxiv.org/pdf/2511.02937", "abs": "https://arxiv.org/abs/2511.02937", "authors": ["Mirco Felske", "Jannik Redenius", "Georg Happich", "Julius Sch\u00f6ning"], "title": "Toward an Agricultural Operational Design Domain: A Framework", "comment": "18 pages, 7 figures, 2 tables", "summary": "The agricultural sector increasingly relies on autonomous systems that\noperate in complex and variable environments. Unlike on-road applications,\nagricultural automation integrates driving and working processes, each of which\nimposes distinct operational constraints. Handling this complexity and ensuring\nconsistency throughout the development and validation processes requires a\nstructured, transparent, and verified description of the environment. However,\nexisting Operational Design Domain (ODD) concepts do not yet address the unique\nchallenges of agricultural applications.\n  Therefore, this work introduces the Agricultural ODD (Ag-ODD) Framework,\nwhich can be used to describe and verify the operational boundaries of\nautonomous agricultural systems. The Ag-ODD Framework consists of three core\nelements. First, the Ag-ODD description concept, which provides a structured\nmethod for unambiguously defining environmental and operational parameters\nusing concepts from ASAM Open ODD and CityGML. Second, the 7-Layer Model\nderived from the PEGASUS 6-Layer Model, has been extended to include a process\nlayer to capture dynamic agricultural operations. Third, the iterative\nverification process verifies the Ag-ODD against its corresponding logical\nscenarios, derived from the 7-Layer Model, to ensure the Ag-ODD's completeness\nand consistency.\n  Together, these elements provide a consistent approach for creating\nunambiguous and verifiable Ag-ODD. Demonstrative use cases show how the Ag-ODD\nFramework can support the standardization and scalability of environmental\ndescriptions for autonomous agricultural systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u519c\u4e1aODD\u6846\u67b6\uff08Ag-ODD\uff09\uff0c\u7528\u4e8e\u63cf\u8ff0\u548c\u9a8c\u8bc1\u81ea\u4e3b\u519c\u4e1a\u7cfb\u7edf\u7684\u64cd\u4f5c\u8fb9\u754c\uff0c\u63d0\u5347\u73af\u5883\u63cf\u8ff0\u7684\u6807\u51c6\u5316\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u64cd\u4f5c\u8bbe\u8ba1\u57df\uff08ODD\uff09\u6982\u5ff5\u672a\u80fd\u6ee1\u8db3\u519c\u4e1a\u5e94\u7528\u7684\u72ec\u7279\u6311\u6218\uff0c\u9700\u8981\u4e00\u4e2a\u6709\u7ed3\u6784\u7684\u3001\u900f\u660e\u7684\u73af\u5883\u63cf\u8ff0\u3002", "method": "Ag-ODD\u6846\u67b6\u7531Ag-ODD\u63cf\u8ff0\u6982\u5ff5\u30017\u5c42\u6a21\u578b\u548c\u8fed\u4ee3\u9a8c\u8bc1\u8fc7\u7a0b\u4e09\u90e8\u5206\u7ec4\u6210\u3002", "result": "\u901a\u8fc7Ag-ODD\u6846\u67b6\u7684\u6f14\u793a\u7528\u4f8b\uff0c\u5c55\u793a\u4e86\u8be5\u6846\u67b6\u5982\u4f55\u652f\u6301\u81ea\u4e3b\u519c\u4e1a\u7cfb\u7edf\u73af\u5883\u63cf\u8ff0\u7684\u6807\u51c6\u5316\u4e0e\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "Ag-ODD\u6846\u67b6\u4e3a\u81ea\u4e3b\u519c\u4e1a\u7cfb\u7edf\u7684\u64cd\u4f5c\u8fb9\u754c\u63cf\u8ff0\u4e0e\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e00\u81f4\u7684\u65b9\u6cd5\uff0c\u652f\u6301\u73af\u5883\u63cf\u8ff0\u7684\u6807\u51c6\u5316\u4e0e\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2511.02842", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02842", "abs": "https://arxiv.org/abs/2511.02842", "authors": ["Jiawei Zheng", "Gokcen Yilmaz", "Ji Han", "Saeema Ahmed-Kristensen"], "title": "Digital Transformation Chatbot (DTchatbot): Integrating Large Language Model-based Chatbot in Acquiring Digital Transformation Needs", "comment": "Accepted by the International Conference on Human-Computer\n  Interaction", "summary": "Many organisations pursue digital transformation to enhance operational\nefficiency, reduce manual efforts, and optimise processes by automation and\ndigital tools. To achieve this, a comprehensive understanding of their unique\nneeds is required. However, traditional methods, such as expert interviews,\nwhile effective, face several challenges, including scheduling conflicts,\nresource constraints, inconsistency, etc. To tackle these issues, we\ninvestigate the use of a Large Language Model (LLM)-powered chatbot to acquire\norganisations' digital transformation needs. Specifically, the chatbot\nintegrates workflow-based instruction with LLM's planning and reasoning\ncapabilities, enabling it to function as a virtual expert and conduct\ninterviews. We detail the chatbot's features and its implementation. Our\npreliminary evaluation indicates that the chatbot performs as designed,\neffectively following predefined workflows and supporting user interactions\nwith areas for improvement. We conclude by discussing the implications of\nemploying chatbots to elicit user information, emphasizing their potential and\nlimitations.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u804a\u5929\u673a\u5668\u4eba\u4ee5\u6ee1\u8db3\u7ec4\u7ec7\u6570\u5b57\u5316\u8f6c\u578b\u9700\u6c42\uff0c\u8bc4\u4f30\u5176\u529f\u80fd\u548c\u5b9e\u65bd\u6548\u679c\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u7ec4\u7ec7\u7684\u8fd0\u8425\u6548\u7387\uff0c\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u548c\u4f18\u5316\u6d41\u7a0b\uff0c\u901a\u8fc7\u5168\u9762\u4e86\u89e3\u5176\u72ec\u7279\u9700\u6c42\u6765\u5b9e\u73b0\u6570\u5b57\u5316\u8f6c\u578b\u3002", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684\u804a\u5929\u673a\u5668\u4eba\u8fdb\u884c\u7528\u6237\u8bbf\u8c08\uff0c\u7ed3\u5408\u5de5\u4f5c\u6d41\u6307\u5bfc\u4e0eLLM\u7684\u89c4\u5212\u548c\u63a8\u7406\u80fd\u529b\u3002", "result": "\u521d\u6b65\u8bc4\u4f30\u8868\u660e\uff0c\u804a\u5929\u673a\u5668\u4eba\u80fd\u591f\u6709\u6548\u5730\u9075\u5faa\u9884\u5b9a\u4e49\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5e76\u652f\u6301\u7528\u6237\u4e92\u52a8\uff0c\u540c\u65f6\u4e5f\u6307\u51fa\u4e86\u6539\u8fdb\u7684\u7a7a\u95f4\u3002", "conclusion": "\u4f7f\u7528\u804a\u5929\u673a\u5668\u4eba\u83b7\u53d6\u7528\u6237\u4fe1\u606f\u5177\u6709\u6f5c\u529b\u548c\u5c40\u9650\u6027\u3002"}}
{"id": "2511.02994", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.02994", "abs": "https://arxiv.org/abs/2511.02994", "authors": ["Syed Mostaquim Ali", "Taufiq Rahman", "Ghazal Farhani", "Mohamed H. Zaki", "Benoit Anctil", "Dominique Charlebois"], "title": "Comprehensive Assessment of LiDAR Evaluation Metrics: A Comparative Study Using Simulated and Real Data", "comment": null, "summary": "For developing safe Autonomous Driving Systems (ADS), rigorous testing is\nrequired before they are deemed safe for road deployments. Since comprehensive\nconventional physical testing is impractical due to cost and safety concerns,\nVirtual Testing Environments (VTE) can be adopted as an alternative. Comparing\nVTE-generated sensor outputs against their real-world analogues can be a strong\nindication that the VTE accurately represents reality. Correspondingly, this\nwork explores a comprehensive experimental approach to finding evaluation\nmetrics suitable for comparing real-world and simulated LiDAR scans. The\nmetrics were tested in terms of sensitivity and accuracy with different noise,\ndensity, distortion, sensor orientation, and channel settings. From comparing\nthe metrics, we found that Density Aware Chamfer Distance (DCD) works best\nacross all cases. In the second step of the research, a Virtual Testing\nEnvironment was generated using real LiDAR scan data. The data was collected in\na controlled environment with only static objects using an instrumented vehicle\nequipped with LiDAR, IMU and cameras. Simulated LiDAR scans were generated from\nthe VTEs using the same pose as real LiDAR scans. The simulated and LiDAR scans\nwere compared in terms of model perception and geometric similarity. Actual and\nsimulated LiDAR scans have a similar semantic segmentation output with a mIoU\nof 21\\% with corrected intensity and an average density aware chamfer distance\n(DCD) of 0.63. This indicates a slight difference in the geometric properties\nof simulated and real LiDAR scans and a significant difference between model\noutputs. During the comparison, density-aware chamfer distance was found to be\nthe most correlated among the metrics with perception methods.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u6bd4\u8f83\u771f\u5b9e\u548c\u6a21\u62dfLiDAR\u626b\u63cf\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u53d1\u73b0\u5bc6\u5ea6\u611f\u77e5\u7684Chamfer\u8ddd\u79bb\u8868\u73b0\u6700\u597d\uff0c\u5e76\u6307\u51fa\u6a21\u62df\u548c\u771f\u5b9eLiDAR\u626b\u63cf\u5728\u6a21\u578b\u8f93\u51fa\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u7531\u4e8e\u5168\u9762\u7684\u4f20\u7edf\u7269\u7406\u6d4b\u8bd5\u5728\u6210\u672c\u548c\u5b89\u5168\u65b9\u9762\u7684\u9650\u5236\uff0c\u5f00\u53d1\u5b89\u5168\u7684\u81ea\u4e3b\u9a7e\u9a76\u7cfb\u7edf\u9700\u8981\u5bfb\u627e\u6709\u6548\u7684\u865a\u62df\u6d4b\u8bd5\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5bf9\u771f\u5b9eLiDAR\u626b\u63cf\u6570\u636e\u8fdb\u884c\u865a\u62df\u6d4b\u8bd5\u73af\u5883\u7684\u642d\u5efa\uff0c\u6bd4\u8f83\u4e86\u771f\u5b9e\u4e0e\u6a21\u62dfLiDAR\u626b\u63cf\u7684\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\uff0c\u8bc4\u4f30\u5176\u7075\u654f\u5ea6\u548c\u51c6\u786e\u6027\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9eLiDAR\u626b\u63cf\u7684\u6bd4\u8f83\u4e2d\uff0c\u53d1\u73b0\u5bc6\u5ea6\u611f\u77e5\u7684Chamfer\u8ddd\u79bb\uff08DCD\uff09\u5728\u6240\u6709\u6d4b\u8bd5\u6848\u4f8b\u4e2d\u8868\u73b0\u6700\u597d\uff1b\u5b9e\u9645\u548c\u6a21\u62dfLiDAR\u626b\u63cf\u5728\u6a21\u578b\u611f\u77e5\u548c\u51e0\u4f55\u76f8\u4f3c\u6027\u4e0a\u6709\u76f8\u4f3c\u7684\u8bed\u4e49\u5206\u5272\u8f93\u51fa\u3002", "conclusion": "\u4e0d\u540c\u7684\u6d4b\u91cf\u6307\u6807\u5728\u771f\u5b9e\u548c\u6a21\u62dfLiDAR\u626b\u63cf\u7684\u6bd4\u8f83\u4e2d\u8868\u73b0\u4e0d\u540c\uff0c\u5176\u4e2d\u5bc6\u5ea6\u611f\u77e5\u7684Chamfer\u8ddd\u79bb\u662f\u6700\u4e3a\u76f8\u5173\u7684\u6307\u6807\uff0c\u8868\u660e\u5728\u5177\u4f53\u7684\u8bed\u4e49\u5206\u5272\u8f93\u51fa\u548c\u51e0\u4f55\u76f8\u4f3c\u6027\u4e0a\uff0c\u6a21\u62df\u548c\u771f\u5b9eLiDAR\u626b\u63cf\u5177\u6709\u4e00\u5b9a\u4e00\u81f4\u6027\u3002"}}
{"id": "2511.02891", "categories": ["cs.HC", "cs.SE", "A.1; H.5.2"], "pdf": "https://arxiv.org/pdf/2511.02891", "abs": "https://arxiv.org/abs/2511.02891", "authors": ["Lingyu Zhao", "Yuankai He"], "title": "A Survey of Driver Distraction and Inattention in Popular Commercial Software-Defined Vehicles", "comment": "12 pages, 12 figures, 1 table", "summary": "As the automotive industry embraces software-defined vehicles (SDVs), the\nrole of user interface (UI) design in ensuring driver safety has become\nincreasingly significant. In crashes related to distracted driving, over 90%\ndid not involve cellphone use but were related to UI controls. However, many of\nthe existing UI SDV implementations do not consider Drive Distraction and\nInattention (DDI), which is reflected in many popular commercial vehicles. This\npaper investigates the impact of UI designs on driver distraction and\ninattention within the context of SDVs. Through a survey of popular commercial\nvehicles, we identify UI features that potentially increase cognitive load and\nevaluate design strategies to mitigate these risks. This survey highlights the\nneed for UI designs that balance advanced software functionalities with\ndriver-cognitive ergonomics. Findings aim to provide valuable guidance to\nresearchers and OEMs to contribute to the field of automotive UI, contributing\nto the broader discussion on enhancing vehicular safety in the software-centric\nautomotive era.", "AI": {"tldr": "\u672c\u8bba\u6587\u7814\u7a76\u6c7d\u8f66\u7528\u6237\u754c\u9762\u8bbe\u8ba1\u5bf9\u9a7e\u9a76\u5206\u5fc3\u7684\u5f71\u54cd\uff0c\u6307\u51fa\u73b0\u6709\u8bbe\u8ba1\u672a\u5145\u5206\u8003\u8651\u9a7e\u9a76\u5458\u8ba4\u77e5\u8d1f\u62c5\uff0c\u5e76\u63d0\u4f9b\u6539\u8fdb\u7b56\u7565\u4ee5\u63d0\u5347\u884c\u8f66\u5b89\u5168\u3002", "motivation": "\u968f\u7740\u6c7d\u8f66\u884c\u4e1a\u8d8a\u6765\u8d8a\u591a\u5730\u91c7\u7528\u8f6f\u4ef6\u5b9a\u4e49\u8f66\u8f86 (SDVs)\uff0c\u8bbe\u8ba1\u9488\u5bf9\u7528\u6237\u754c\u9762\u7684 UI \u8bbe\u8ba1\u5728\u786e\u4fdd\u9a7e\u9a76\u5b89\u5168\u4e2d\u7684\u91cd\u8981\u6027\u9010\u6e10\u51f8\u663e\u3002", "method": "\u901a\u8fc7\u5bf9\u6d41\u884c\u5546\u7528\u8f66\u8f86\u7684\u8c03\u67e5\uff0c\u8bc6\u522b\u51fa\u6f5c\u5728\u589e\u52a0\u8ba4\u77e5\u8d1f\u62c5\u7684 UI \u7279\u5f81\uff0c\u5e76\u8bc4\u4f30\u76f8\u5e94\u7684\u8bbe\u8ba1\u7b56\u7565\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u8bb8\u591a\u5546\u7528\u8f66\u8f86\u7684 UI \u5b9e\u73b0\u672a\u80fd\u5145\u5206\u8003\u8651\u9a7e\u9a76\u5206\u5fc3\u4e0e\u6ce8\u610f\u529b\u7f3a\u5931 (DDI)\uff0c\u9700\u8981\u5728\u9ad8\u7ea7\u8f6f\u4ef6\u529f\u80fd\u4e0e\u9a7e\u9a76\u5458\u8ba4\u77e5\u4eba\u4f53\u5de5\u5b66\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u3002", "conclusion": "\u672c\u7814\u7a76\u5f3a\u8c03\u4e86 UI \u8bbe\u8ba1\u5728\u51cf\u5c11\u9a7e\u9a76\u5206\u5fc3\u548c\u6ce8\u610f\u529b\u7f3a\u5931\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u7b56\u7565\u4ee5\u6539\u5584\u6c7d\u8f66\u7528\u6237\u754c\u9762\uff0c\u540c\u65f6\u63d0\u9ad8\u884c\u8f66\u5b89\u5168\u3002"}}
{"id": "2511.03075", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.03075", "abs": "https://arxiv.org/abs/2511.03075", "authors": ["Markus Buchholz", "Ignacio Carlucho", "Yvan R. Petillot"], "title": "A Collaborative Reasoning Framework for Anomaly Diagnostics in Underwater Robotics", "comment": "Paper was submitted for ICRA 2026", "summary": "The safe deployment of autonomous systems in safety-critical settings\nrequires a paradigm that combines human expertise with AI-driven analysis,\nespecially when anomalies are unforeseen. We introduce AURA (Autonomous\nResilience Agent), a collaborative framework for anomaly and fault diagnostics\nin robotics. AURA integrates large language models (LLMs), a high-fidelity\ndigital twin (DT), and human-in-the-loop interaction to detect and respond to\nanomalous behavior in real time. The architecture uses two agents with clear\nroles: (i) a low-level State Anomaly Characterization Agent that monitors\ntelemetry and converts signals into a structured natural-language problem\ndescription, and (ii) a high-level Diagnostic Reasoning Agent that conducts a\nknowledge-grounded dialogue with an operator to identify root causes, drawing\non external sources. Human-validated diagnoses are then converted into new\ntraining examples that refine the low-level perceptual model. This feedback\nloop progressively distills expert knowledge into the AI, transforming it from\na static tool into an adaptive partner. We describe the framework's operating\nprinciples and provide a concrete implementation, establishing a pattern for\ntrustworthy, continually improving human-robot teams.", "AI": {"tldr": "AURA\u6846\u67b6\u7ed3\u5408\u4e86AI\u4e0e\u4eba\u7c7b\u4e13\u5bb6\uff0c\u63d0\u5347\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u5f02\u5e38\u60c5\u51b5\u4e0b\u7684\u54cd\u5e94\u80fd\u529b\uff0c\u5efa\u7acb\u4e86\u4e00\u4e2a\u52a8\u6001\u53cd\u9988\u673a\u5236\u4ee5\u4f18\u5316AI\u7684\u8868\u73b0\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u73af\u5883\u4e2d\uff0c\u81ea\u52a8\u7cfb\u7edf\u7684\u5b89\u5168\u90e8\u7f72\u9700\u8981\u5c06\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u4e0eAI\u5206\u6790\u76f8\u7ed3\u5408\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u672a\u9884\u89c1\u5f02\u5e38\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u5f15\u5165AURA\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u3001\u6570\u5b57\u53cc\u80de\u80ce\u548c\u4eba\u673a\u4ea4\u4e92\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u9886\u57df\u7684\u5f02\u5e38\u68c0\u6d4b\u548c\u6545\u969c\u8bca\u65ad\u3002", "result": "AURA\u6846\u67b6\u4f7f\u7528\u4f4e\u7ea7\u72b6\u6001\u5f02\u5e38\u8868\u5f81\u4ee3\u7406\u548c\u9ad8\u7ea7\u8bca\u65ad\u63a8\u7406\u4ee3\u7406\uff0c\u5b9e\u73b0\u5b9e\u65f6\u76d1\u63a7\u548c\u6545\u969c\u6839\u672c\u539f\u56e0\u8bc6\u522b\uff0c\u4ece\u800c\u5efa\u7acb\u4e86\u53ef\u4fe1\u8d56\u4e14\u4e0d\u65ad\u6539\u8fdb\u7684\u4eba\u673a\u56e2\u961f\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5c06\u4eba\u7c7b\u9a8c\u8bc1\u7684\u8bca\u65ad\u8f6c\u5316\u4e3a\u8bad\u7ec3\u6570\u636e\uff0c\u9010\u6e10\u4f18\u5316\u4e86\u4f4e\u7ea7\u611f\u77e5\u6a21\u578b\uff0c\u4f7f\u5f97AI\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53d8\u5f97\u66f4\u52a0\u9002\u5e94\u548c\u53ef\u9760\u3002"}}
{"id": "2511.02979", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02979", "abs": "https://arxiv.org/abs/2511.02979", "authors": ["Esther Sun", "Zichu Wu"], "title": "Systematizing LLM Persona Design: A Four-Quadrant Technical Taxonomy for AI Companion Applications", "comment": "Submitted to Neurips 2025 workshop: LLM Persona Workshop", "summary": "The design and application of LLM-based personas in AI companionship is a\nrapidly expanding but fragmented field, spanning from virtual emotional\ncompanions and game NPCs to embodied functional robots. This diversity in\nobjectives, modality, and technical stacks creates an urgent need for a unified\nframework. To address this gap, this paper systematizes the field by proposing\na Four-Quadrant Technical Taxonomy for AI companion applications. The framework\nis structured along two critical axes: Virtual vs. Embodied and Emotional\nCompanionship vs. Functional Augmentation. Quadrant I (Virtual Companionship)\nexplores virtual idols, romantic companions, and story characters, introducing\na four-layer technical framework to analyze their challenges in maintaining\nlong-term emotional consistency. Quadrant II (Functional Virtual Assistants)\nanalyzes AI applications in work, gaming, and mental health, highlighting the\nshift from \"feeling\" to \"thinking and acting\" and pinpointing key technologies\nlike enterprise RAG and on-device inference. Quadrants III & IV (Embodied\nIntelligence) shift from the virtual to the physical world, analyzing home\nrobots and vertical-domain assistants, revealing core challenges in symbol\ngrounding, data privacy, and ethical liability. This taxonomy provides not only\na systematic map for researchers and developers to navigate the complex persona\ndesign space but also a basis for policymakers to identify and address the\nunique risks inherent in different application scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u56db\u8c61\u9650\u6280\u672f\u5206\u7c7b\u6cd5\uff0c\u4ee5\u7edf\u4e00AI\u966a\u4f34\u5e94\u7528\u7684\u8bbe\u8ba1\u4e0e\u5206\u6790\uff0c\u6db5\u76d6\u865a\u62df\u4e0e\u5177\u8eab\u3001\u60c5\u611f\u4e0e\u529f\u80fd\u7b49\u591a\u4e2a\u7ef4\u5ea6\u3002", "motivation": "\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u966a\u4f34\u8005\u8bbe\u8ba1\u548c\u5e94\u7528\u9886\u57df\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u5b58\u5728\u4e25\u91cd\u7684\u788e\u7247\u5316\uff0c\u6025\u9700\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u7c7b\u578b\u7684AI\u966a\u4f34\u5e94\u7528\uff0c\u5efa\u7acb\u8d77\u4e00\u4e2a\u57fa\u4e8e\u865a\u62df\u4e0e\u5177\u8eab\u3001\u60c5\u611f\u966a\u4f34\u4e0e\u529f\u80fd\u589e\u5f3a\u7684\u6280\u672f\u5206\u7c7b\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u62ec\u865a\u62df\u966a\u4f34\u3001\u529f\u80fd\u865a\u62df\u52a9\u624b\u548c\u5177\u8eab\u667a\u80fd\u7684\u56db\u8c61\u9650\u6280\u672f\u5206\u7c7b\u6cd5\uff0c\u7cfb\u7edf\u63a2\u8ba8\u4e86\u76f8\u5173\u5e94\u7528\u7684\u6311\u6218\u548c\u6280\u672f\u9700\u6c42\u3002", "conclusion": "\u63d0\u51fa\u7684\u56db\u8c61\u9650\u6280\u672f\u5206\u7c7b\u6cd5\u4e3aAI\u966a\u4f34\u5e94\u7528\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u6846\u67b6\uff0c\u5e76\u4e3a\u653f\u7b56\u5236\u5b9a\u8005\u8bc6\u522b\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e2d\u7684\u72ec\u7279\u98ce\u9669\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.03077", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.03077", "abs": "https://arxiv.org/abs/2511.03077", "authors": ["R. Khorrambakht", "Joaquim Ortiz-Haro", "Joseph Amigo", "Omar Mostafa", "Daniel Dugas", "Franziska Meier", "Ludovic Righetti"], "title": "WorldPlanner: Monte Carlo Tree Search and MPC with Action-Conditioned Visual World Models", "comment": null, "summary": "Robots must understand their environment from raw sensory inputs and reason\nabout the consequences of their actions in it to solve complex tasks. Behavior\nCloning (BC) leverages task-specific human demonstrations to learn this\nknowledge as end-to-end policies. However, these policies are difficult to\ntransfer to new tasks, and generating training data is challenging because it\nrequires careful demonstrations and frequent environment resets. In contrast to\nsuch policy-based view, in this paper we take a model-based approach where we\ncollect a few hours of unstructured easy-to-collect play data to learn an\naction-conditioned visual world model, a diffusion-based action sampler, and\noptionally a reward model. The world model -- in combination with the action\nsampler and a reward model -- is then used to optimize long sequences of\nactions with a Monte Carlo Tree Search (MCTS) planner. The resulting plans are\nexecuted on the robot via a zeroth-order Model Predictive Controller (MPC). We\nshow that the action sampler mitigates hallucinations of the world model during\nplanning and validate our approach on 3 real-world robotic tasks with varying\nlevels of planning and modeling complexity. Our experiments support the\nhypothesis that planning leads to a significant improvement over BC baselines\non a standard manipulation test environment.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u91c7\u96c6\u6e38\u620f\u6570\u636e\u548c\u89c4\u5212\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u884c\u4e3a\u514b\u9686\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u884c\u4e3a\u514b\u9686\u65b9\u6cd5\u5728\u65b0\u4efb\u52a1\u8f6c\u79fb\u548c\u6570\u636e\u751f\u6210\u65b9\u9762\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u7b80\u5355\u6613\u6536\u96c6\u7684\u6e38\u620f\u6570\u636e\u3002", "method": "\u6211\u4eec\u901a\u8fc7\u6536\u96c6\u5c11\u91cf\u672a\u7ed3\u6784\u5316\u7684\u6e38\u620f\u6570\u636e\uff0c\u5b66\u4e60\u4e86\u4e00\u4e2a\u57fa\u4e8e\u52a8\u4f5c\u7684\u89c6\u89c9\u4e16\u754c\u6a21\u578b\u3001\u6269\u6563\u52a8\u4f5c\u91c7\u6837\u5668\u548c\u53ef\u9009\u7684\u5956\u52b1\u6a21\u578b\u3002\u7136\u540e\uff0c\u7ed3\u5408\u8fd9\u4e9b\u7ec4\u4ef6\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u89c4\u5212\u5668\u4f18\u5316\u957f\u5e8f\u5217\u52a8\u4f5c\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u6211\u4eec\u7684\u884c\u52a8\u91c7\u6837\u5668\u80fd\u591f\u51cf\u8f7b\u4e16\u754c\u6a21\u578b\u5728\u89c4\u5212\u8fc7\u7a0b\u4e2d\u7684\u9519\u8bef\u63a8\u65ad\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u89c4\u5212\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u7684\u4efb\u52a1\u8868\u73b0\uff0c\u4f18\u4e8e\u884c\u4e3a\u514b\u9686\u7684\u57fa\u7ebf\u3002"}}
{"id": "2511.03117", "categories": ["cs.HC", "H.5.2"], "pdf": "https://arxiv.org/pdf/2511.03117", "abs": "https://arxiv.org/abs/2511.03117", "authors": ["Yibo Meng", "Ruiqi Chen", "Xin Chen", "Zhiming Liu", "Yan Guan"], "title": "Tracing Generative AI in Digital Art: A Longitudinal Study of Chinese Painters' Attitudes, Practices, and Identity Negotiation", "comment": "In Submission", "summary": "This study presents a five-year longitudinal mixed-methods study of 17\nChinese digital painters, examining how their attitudes and practices evolved\nin response to generative AI. Our findings reveal a trajectory from resistance\nand defensiveness, to pragmatic adoption, and ultimately to reflective\nreconstruction, shaped by strong peer pressures and shifting emotional\nexperiences. Persistent concerns around copyright and creative labor highlight\nthe ongoing negotiation of identity and values. This work contributes by\noffering rare longitudinal empirical data, advancing a theoretical lens of\n\"identity and value negotiation,\" and providing design implications for future\nhuman-AI collaborative systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e2d\u56fd\u6570\u5b57\u753b\u5bb6\u5728\u751f\u6210\u6027\u4eba\u5de5\u667a\u80fd\u5f71\u54cd\u4e0b\u7684\u6001\u5ea6\u53d8\u5316\uff0c\u5f3a\u8c03\u8eab\u4efd\u4e0e\u4ef7\u503c\u7684\u534f\u5546\u8fc7\u7a0b\uff0c\u4e3a\u672a\u6765\u4eba\u673a\u534f\u4f5c\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u542f\u793a\u3002", "motivation": "\u7814\u7a76\u6570\u5b57\u827a\u672f\u5bb6\u5982\u4f55\u5728\u751f\u6210\u6027\u4eba\u5de5\u667a\u80fd\u7684\u5f71\u54cd\u4e0b\uff0c\u8c03\u6574\u4ed6\u4eec\u7684\u521b\u4f5c\u6001\u5ea6\u548c\u5b9e\u8df5\u3002", "method": "\u8fdb\u884c\u4e86\u4e3a\u671f\u4e94\u5e74\u7684\u7eb5\u5411\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\uff0c\u53c2\u4e0e\u8005\u4e3a17\u4f4d\u4e2d\u56fd\u6570\u5b57\u827a\u672f\u5bb6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u827a\u672f\u5bb6\u7684\u6001\u5ea6\u7531\u62b5\u6297\u548c\u9632\u5fa1\u8f6c\u5411\u52a1\u5b9e\u91c7\u7eb3\uff0c\u5e76\u6700\u7ec8\u5b9e\u73b0\u53cd\u601d\u91cd\u6784\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u7248\u6743\u548c\u521b\u4f5c\u52b3\u52a8\u7684\u6301\u4e45\u5173\u6ce8\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u5bf9\u4e2d\u56fd\u6570\u5b57\u753b\u5bb6\u5728\u9762\u5bf9\u751f\u6210\u6027\u4eba\u5de5\u667a\u80fd\u7684\u6001\u5ea6\u4e0e\u5b9e\u8df5\u6f14\u53d8\u7684\u6df1\u5165\u7406\u89e3\uff0c\u5f3a\u8c03\u8eab\u4efd\u4e0e\u4ef7\u503c\u7684\u6301\u7eed\u534f\u5546\u3002"}}
{"id": "2511.03078", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.03078", "abs": "https://arxiv.org/abs/2511.03078", "authors": ["Rohan Kota", "Kaival Shah", "J. Edward Colgate", "Gregory Reardon"], "title": "3D Cal: An Open-Source Software Library for Calibrating Tactile Sensors", "comment": null, "summary": "Tactile sensing plays a key role in enabling dexterous and reliable robotic\nmanipulation, but realizing this capability requires substantial calibration to\nconvert raw sensor readings into physically meaningful quantities. Despite its\nnear-universal necessity, the calibration process remains ad hoc and\nlabor-intensive. Here, we introduce \\libname{}, an open-source library that\ntransforms a low-cost 3D printer into an automated probing device capable of\ngenerating large volumes of labeled training data for tactile sensor\ncalibration. We demonstrate the utility of \\libname{} by calibrating two\ncommercially available vision-based tactile sensors, DIGIT and GelSight Mini,\nto reconstruct high-quality depth maps using the collected data and a custom\nconvolutional neural network. In addition, we perform a data ablation study to\ndetermine how much data is needed for accurate calibration, providing practical\nguidelines for researchers working with these specific sensors, and we\nbenchmark the trained models on previously unseen objects to evaluate\ncalibration accuracy and generalization performance. By automating tactile\nsensor calibration, \\libname{} can accelerate tactile sensing research,\nsimplify sensor deployment, and promote the practical integration of tactile\nsensing in robotic platforms.", "AI": {"tldr": "\n\u672c\u6587\u63d0\u51fa\textbackslash libname{}\uff0c\u4e00\u4e2a\u5f00\u6e90\u5e93\uff0c\u5229\u7528\u81ea\u52a8\u63a2\u6d4b\u8bbe\u5907\u751f\u6210\u5927\u91cf\u6570\u636e\uff0c\u4ee5\u7b80\u5316\u548c\u52a0\u901f\u89e6\u89c9\u4f20\u611f\u5668\u7684\u6821\u51c6\u8fc7\u7a0b\uff0c\u589e\u5f3a\u673a\u5668\u4eba\u7684\u89e6\u89c9\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\n\u89e6\u89c9\u611f\u77e5\u5728\u5b9e\u73b0\u7075\u5de7\u548c\u53ef\u9760\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7684\u6821\u51c6\u8fc7\u7a0b\u4ecd\u7136\u662f\u4e34\u65f6\u548c\u52b3\u52a8\u5bc6\u96c6\u578b\u7684\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u7b80\u5316\u6821\u51c6\u8fc7\u7a0b\u3002", "method": "\n\u6211\u4eec\u5229\u7528\u5f00\u6e90\u5e93\textbackslash libname{}\u5c06\u4f4e\u6210\u672c3D\u6253\u5370\u673a\u8f6c\u5316\u4e3a\u81ea\u52a8\u63a2\u6d4b\u8bbe\u5907\uff0c\u751f\u6210\u5927\u91cf\u6807\u8bb0\u7684\u8bad\u7ec3\u6570\u636e\u7528\u4e8e\u89e6\u89c9\u4f20\u611f\u5668\u6821\u51c6\uff0c\u5e76\u4f7f\u7528\u81ea\u5b9a\u4e49\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u91cd\u5efa\u9ad8\u8d28\u91cf\u6df1\u5ea6\u56fe\u3002", "result": "\n\u6210\u529f\u6821\u51c6\u4e86\u4e24\u79cd\u5546\u4e1a\u5316\u7684\u57fa\u4e8e\u89c6\u89c9\u7684\u89e6\u89c9\u4f20\u611f\u5668DIGIT\u548cGelSight Mini\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u6d88\u878d\u7814\u7a76\u63d0\u4f9b\u4e86\u5173\u4e8e\u51c6\u786e\u6821\u51c6\u6240\u9700\u6570\u636e\u91cf\u7684\u5b9e\u7528\u6307\u5bfc\uff0c\u540c\u65f6\u8bc4\u4f30\u4e86\u6a21\u578b\u5728\u4e4b\u524d\u672a\u89c1\u7269\u4f53\u4e0a\u7684\u6821\u51c6\u51c6\u786e\u6027\u548c\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "\n\u672c\u7814\u7a76\u63d0\u51fa\u7684\u5f00\u6e90\u5e93\textbackslash libname{}\u80fd\u591f\u81ea\u52a8\u5316\u89e6\u89c9\u4f20\u611f\u5668\u7684\u6821\u51c6\uff0c\u63a8\u52a8\u89e6\u89c9\u611f\u77e5\u7814\u7a76\uff0c\u7b80\u5316\u4f20\u611f\u5668\u90e8\u7f72\uff0c\u5e76\u4fc3\u8fdb\u89e6\u89c9\u611f\u77e5\u5728\u673a\u5668\u4eba\u5e73\u53f0\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2511.03131", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.03131", "abs": "https://arxiv.org/abs/2511.03131", "authors": ["Zeda Xu", "Nikolas Martelaro", "Christopher McComb"], "title": "Ceci N'est Pas un Drone: Investigating the Impact of Design Representation on Design Decision Making When Using GenAI", "comment": null, "summary": "With generative AI-powered design tools, designers and engineers can\nefficiently generate large numbers of design ideas. However, efficient\nexploration of these ideas requires designers to select a smaller group of\npotential solutions for further development. Therefore, the ability to judge\nand evaluate designs is critical for the successful use of generative design\ntools. Different design representation modalities can potentially affect\ndesigners' judgments. This work investigates how different design modalities,\nincluding visual rendering, numerical performance data, and a combination of\nboth, affect designers' design selections from AI-generated design concepts for\nUncrewed Aerial Vehicles. We found that different design modalities do affect\ndesigners' choices. Unexpectedly, we found that providing only numerical design\nperformance data can lead to the best ability to select optimal designs. We\nalso found that participants prefer visually conventional designs with\naxis-symmetry. The findings of this work provide insights into the interaction\nbetween human users and generative design systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e0d\u540c\u8bbe\u8ba1\u8868\u73b0\u5f62\u5f0f\u5bf9\u8bbe\u8ba1\u5e08\u9009\u62e9AI\u751f\u6210\u8bbe\u8ba1\u65b9\u6848\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u6570\u503c\u6027\u80fd\u6570\u636e\u80fd\u6700\u4f73\u5f15\u5bfc\u9009\u62e9\u6700\u4f73\u8bbe\u8ba1\u3002", "motivation": "\u968f\u7740\u751f\u6210\u6027AI\u8bbe\u8ba1\u5de5\u5177\u7684\u53d1\u5c55\uff0c\u8bbe\u8ba1\u5e08\u9700\u8981\u6709\u6548\u9009\u62e9\u8f83\u5c11\u7684\u8bbe\u8ba1\u65b9\u6848\u4ee5\u8fdb\u884c\u8fdb\u4e00\u6b65\u5f00\u53d1\u3002", "method": "\u7814\u7a76\u4e0d\u540c\u8bbe\u8ba1\u8868\u73b0\u5f62\u5f0f\u5982\u4f55\u5f71\u54cd\u8bbe\u8ba1\u5e08\u4eceAI\u751f\u6210\u7684\u8bbe\u8ba1\u6982\u5ff5\u4e2d\u8fdb\u884c\u9009\u62e9\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u7684\u8bbe\u8ba1\u8868\u73b0\u5f62\u5f0f\u5f71\u54cd\u8bbe\u8ba1\u5e08\u7684\u9009\u62e9\uff0c\u63d0\u4f9b\u5355\u4e00\u7684\u6570\u503c\u8bbe\u8ba1\u6027\u80fd\u6570\u636e\u65f6\u80fd\u66f4\u597d\u5730\u9009\u51fa\u6700\u4f73\u8bbe\u8ba1\uff0c\u540c\u65f6\u53c2\u4e0e\u8005\u66f4\u503e\u5411\u4e8e\u89c6\u89c9\u4e0a\u4f20\u7edf\u4e14\u5bf9\u79f0\u7684\u8bbe\u8ba1\u3002", "conclusion": "\u4e0d\u540c\u7684\u8bbe\u8ba1\u8868\u73b0\u5f62\u5f0f\u4f1a\u5f71\u54cd\u8bbe\u8ba1\u5e08\u7684\u9009\u62e9\uff0c\u800c\u63d0\u4f9b\u4ec5\u6709\u7684\u6570\u503c\u8bbe\u8ba1\u6027\u80fd\u6570\u636e\u80fd\u6709\u6548\u5e2e\u52a9\u9009\u62e9\u6700\u4f73\u8bbe\u8ba1\u3002"}}
{"id": "2511.03165", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.03165", "abs": "https://arxiv.org/abs/2511.03165", "authors": ["Raj Surya Rajendran Kathirvel", "Zach A Chavis", "Stephen J. Guy", "Karthik Desingh"], "title": "SENT Map -- Semantically Enhanced Topological Maps with Foundation Models", "comment": "Accepted at ICRA 2025 Workshop on Foundation Models and\n  Neuro-Symbolic AI for Robotics", "summary": "We introduce SENT-Map, a semantically enhanced topological map for\nrepresenting indoor environments, designed to support autonomous navigation and\nmanipulation by leveraging advancements in foundational models (FMs). Through\nrepresenting the environment in a JSON text format, we enable semantic\ninformation to be added and edited in a format that both humans and FMs\nunderstand, while grounding the robot to existing nodes during planning to\navoid infeasible states during deployment. Our proposed framework employs a two\nstage approach, first mapping the environment alongside an operator with a\nVision-FM, then using the SENT-Map representation alongside a natural-language\nquery within an FM for planning. Our experimental results show that\nsemantic-enhancement enables even small locally-deployable FMs to successfully\nplan over indoor environments.", "AI": {"tldr": "\u5f15\u5165SENT-Map\uff0c\u652f\u6301\u81ea\u4e3b\u5bfc\u822a\u7684\u5ba4\u5185\u73af\u5883\u8bed\u4e49\u589e\u5f3a\u62d3\u6251\u5730\u56fe\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u7684\u8fdb\u5c55\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u80fd\u6210\u529f\u89c4\u5212\u5ba4\u5185\u73af\u5883\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u5229\u7528\u57fa\u7840\u6a21\u578b\u7684\u8fdb\u5c55\uff0c\u652f\u6301\u81ea\u4e3b\u5bfc\u822a\u548c\u64cd\u4f5c\uff0c\u521b\u5efa\u5ba4\u5185\u73af\u5883\u7684\u8bed\u4e49\u589e\u5f3a\u62d3\u6251\u5730\u56fe\u3002", "method": "\u8be5\u6846\u67b6\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u9996\u5148\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4e0e\u64cd\u4f5c\u5458\u4e00\u8d77\u6620\u5c04\u73af\u5883\uff0c\u7136\u540e\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u4f7f\u7528SENT-Map\u8fdb\u884c\u89c4\u5212\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8bed\u4e49\u589e\u5f3a\u4f7f\u5f97\u5c0f\u578b\u672c\u5730\u53ef\u90e8\u7f72\u7684\u57fa\u7840\u6a21\u578b\u80fd\u591f\u6709\u6548\u8fdb\u884c\u5ba4\u5185\u73af\u5883\u89c4\u5212\u3002", "conclusion": "\u8bed\u4e49\u589e\u5f3a\u4f7f\u5f97\u5373\u4f7f\u662f\u5c0f\u578b\u672c\u5730\u53ef\u90e8\u7f72\u7684\u57fa\u7840\u6a21\u578b\u4e5f\u80fd\u591f\u6210\u529f\u5730\u5728\u5ba4\u5185\u73af\u5883\u4e2d\u8fdb\u884c\u89c4\u5212\u3002"}}
{"id": "2511.03143", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03143", "abs": "https://arxiv.org/abs/2511.03143", "authors": ["Erfan Shayegani", "Jina Suh", "Andy Wilson", "Nagu Rangan", "Javier Hernandez"], "title": "From Measurement to Expertise: Empathetic Expert Adapters for Context-Based Empathy in Conversational AI Agents", "comment": null, "summary": "Empathy is a critical factor in fostering positive user experiences in\nconversational AI. While models can display empathy, it is often generic rather\nthan tailored to specific tasks and contexts. In this work, we introduce a\nnovel framework for developing and evaluating context-specific empathetic large\nlanguage models (LLMs). We first analyze a real-world conversational dataset\nconsisting of 672 multi-turn conversations across 8 tasks, revealing\nsignificant differences in terms of expected and experienced empathy before and\nafter the conversations, respectively. To help minimize this gap, we develop a\nsynthetic multi-turn conversational generation pipeline and steer responses\ntoward our defined empathy patterns based on the context that more closely\nmatches users' expectations. We then train empathetic expert adapters for\ncontext-specific empathy that specialize in varying empathy levels based on the\nrecognized task. Our empirical results demonstrate a significant gap reduction\nof 72.66% between perceived and desired empathy with scores increasing by an\naverage factor of 2.43 as measured by our metrics and reward models.\nAdditionally, our trained empathetic expert adapters demonstrate superior\neffectiveness in preserving empathy patterns throughout conversation turns,\noutperforming system prompts, which tend to dramatically diminish in impact as\nconversations lengthen.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u5bf9\u8bdd\u6570\u636e\u96c6\u548c\u5f00\u53d1\u4e0a\u4e0b\u6587\u7279\u5b9a\u9002\u914d\u5668\uff0c\u63d0\u9ad8\u4e86\u5bf9\u8bddAI\u4e2d\u7684\u540c\u7406\u5fc3\u8868\u73b0\uff0c\u51cf\u5c11\u4e86\u7528\u6237\u671f\u671b\u4e0e\u5b9e\u9645\u4f53\u9a8c\u7684\u5dee\u8ddd\u3002", "motivation": "\u63d0\u9ad8\u5bf9\u8bddAI\u4e2d\u540c\u7406\u5fc3\u7684\u6c34\u5e73\uff0c\u4f7f\u5176\u66f4\u7b26\u5408\u7528\u6237\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u60c5\u5883\u4e0b\u7684\u671f\u671b\u3002", "method": "\u5206\u6790\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u5408\u6210\u591a\u8f6e\u5bf9\u8bdd\u751f\u6210\u7ba1\u9053\uff0c\u5e76\u8bad\u7ec3\u4e0a\u4e0b\u6587\u7279\u5b9a\u7684\u540c\u7406\u5fc3\u4e13\u5bb6\u9002\u914d\u5668\u3002", "result": "\u5728\u7528\u6237\u671f\u671b\u7684\u540c\u7406\u5fc3\u4e0e\u5b9e\u9645\u4f53\u9a8c\u4e4b\u95f4\uff0c\u51cf\u5c11\u4e8672.66%\u7684\u5dee\u8ddd\uff0c\u5e73\u5747\u5f97\u5206\u63d0\u9ad8\u4e862.43\u500d\uff0c\u6b64\u5916\uff0c\u4e13\u5bb6\u9002\u914d\u5668\u5728\u957f\u5bf9\u8bdd\u4e2d\u7ef4\u6301\u540c\u7406\u5fc3\u6a21\u5f0f\u7684\u6548\u679c\u4f18\u4e8e\u7cfb\u7edf\u63d0\u793a\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u4e0a\u4e0b\u6587\u7279\u5b9a\u7684\u540c\u7406\u5fc3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u8be5\u7814\u7a76\u6210\u529f\u51cf\u5c11\u4e86\u7528\u6237\u671f\u671b\u548c\u4f53\u9a8c\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u4fdd\u6301\u4e86\u540c\u7406\u5fc3\u6a21\u5f0f\u3002"}}
{"id": "2511.03167", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.03167", "abs": "https://arxiv.org/abs/2511.03167", "authors": ["Xin Liu", "Jinze Wu", "Yinghui Li", "Chenkun Qi", "Yufei Xue", "Feng Gao"], "title": "Learning Natural and Robust Hexapod Locomotion over Complex Terrains via Motion Priors based on Deep Reinforcement Learning", "comment": null, "summary": "Multi-legged robots offer enhanced stability to navigate complex terrains\nwith their multiple legs interacting with the environment. However, how to\neffectively coordinate the multiple legs in a larger action exploration space\nto generate natural and robust movements is a key issue. In this paper, we\nintroduce a motion prior-based approach, successfully applying deep\nreinforcement learning algorithms to a real hexapod robot. We generate a\ndataset of optimized motion priors, and train an adversarial discriminator\nbased on the priors to guide the hexapod robot to learn natural gaits. The\nlearned policy is then successfully transferred to a real hexapod robot, and\ndemonstrate natural gait patterns and remarkable robustness without visual\ninformation in complex terrains. This is the first time that a reinforcement\nlearning controller has been used to achieve complex terrain walking on a real\nhexapod robot.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fd0\u52a8\u5148\u9a8c\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u516d\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e0a\u7684\u81ea\u7136\u884c\u8d70\u3002", "motivation": "\u591a\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u5bfc\u822a\u4e2d\u5177\u5907\u66f4\u5f3a\u7684\u7a33\u5b9a\u6027\uff0c\u5982\u4f55\u6709\u6548\u534f\u8c03\u591a\u6761\u817f\u8fdb\u884c\u81ea\u7136\u4e14\u7a33\u5065\u7684\u79fb\u52a8\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u8fd0\u52a8\u5148\u9a8c\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u8bad\u7ec3\u516d\u8db3\u673a\u5668\u4eba\u3002", "result": "\u901a\u8fc7\u4f18\u5316\u8fd0\u52a8\u5148\u9a8c\u751f\u6210\u7684\u6570\u636e\u96c6\u548c\u5bf9\u6297\u9274\u522b\u5668\u7684\u8bad\u7ec3\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u516d\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e0a\u7684\u81ea\u7136\u6b65\u6001\u548c\u51fa\u8272\u7684\u7a33\u5065\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u5c06\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5668\u5e94\u7528\u4e8e\u771f\u5b9e\u516d\u8db3\u673a\u5668\u4eba\uff0c\u5b9e\u73b0\u4e86\u5728\u590d\u6742\u5730\u5f62\u4e0a\u884c\u8d70\u7684\u80fd\u529b\u3002"}}
{"id": "2511.03174", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.03174", "abs": "https://arxiv.org/abs/2511.03174", "authors": ["Jiawei Zhou", "Lei Zhang", "Mei Li", "Benjamin D Horne", "Munmun De Choudhury"], "title": "AI as We Describe It: How Large Language Models and Their Applications in Health are Represented Across Channels of Public Discourse", "comment": null, "summary": "Representation shapes public attitudes and behaviors. With the arrival and\nrapid adoption of LLMs, the way these systems are introduced will negotiate\nsocietal expectations for their role in high-stakes domains like health. Yet it\nremains unclear whether current narratives present a balanced view. We analyzed\nfive prominent discourse channels (news, research press, YouTube, TikTok, and\nReddit) over a two-year period on lexical style, informational content, and\nsymbolic representation. Discussions were generally positive and episodic, with\npositivity increasing over time. Risk communication was unthorough and often\nreduced to information quality incidents, while explanations of LLMs'\ngenerative nature were rare. Compared with professional outlets, TikTok and\nReddit highlighted wellbeing applications and showed greater variations in tone\nand anthropomorphism but little attention to risks. We discuss implications for\npublic discourse as a diagnostic tool in identifying literacy and governance\ngaps, and for communication and design strategies to support more informed LLM\nengagement.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u4e94\u4e2a\u8bdd\u8bed\u6e20\u9053\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63cf\u8ff0\uff0c\u53d1\u73b0\u516c\u4f17\u8ba8\u8bba\u603b\u4f53\u8f83\u79ef\u6781\uff0c\u4f46\u5bf9\u98ce\u9669\u7684\u5173\u6ce8\u4e0d\u8db3\uff0c\u5f3a\u8c03\u9700\u8981\u6539\u8fdb\u6c9f\u901a\u7b56\u7565\u4ee5\u4fc3\u8fdb\u77e5\u60c5\u53c2\u4e0e\u3002", "motivation": "\u5206\u6790\u5f53\u524d\u53d9\u8ff0\u662f\u5426\u5448\u73b0\u51fa\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u9ad8\u98ce\u9669\u9886\u57df\uff08\u5982\u5065\u5eb7\uff09\u4e2d\u7684\u4f5c\u7528\u7684\u5e73\u8861\u89c6\u89d2\uff0c\u4ee5\u5e94\u5bf9\u5176\u5feb\u901f adoption\u3002", "method": "\u5206\u6790\u4e86\u4e94\u4e2a\u4e3b\u8981\u8bdd\u8bed\u6e20\u9053\uff08\u65b0\u95fb\u3001\u7814\u7a76\u65b0\u95fb\u3001YouTube\u3001TikTok\u548cReddit\uff09\u5728\u4e24\u5e74\u671f\u95f4\u7684\u8bcd\u6c47\u98ce\u683c\u3001\u4fe1\u606f\u5185\u5bb9\u548c\u8c61\u5f81\u6027\u8868\u73b0\u3002", "result": "\u8ba8\u8bba\u6574\u4f53\u79ef\u6781\u4e14\u5076\u53d1\uff0c\u79ef\u6781\u6027\u968f\u65f6\u95f4\u589e\u52a0\uff0c\u4f46\u98ce\u9669\u4ea4\u6d41\u4e0d\u5f7b\u5e95\uff0c\u4e14\u5bf9LLM\u751f\u6210\u7279\u6027\u7684\u89e3\u91ca\u8f83\u5c11\u3002TikTok\u548cReddit\u76f8\u6bd4\u4e13\u4e1a\u6e20\u9053\u66f4\u591a\u5173\u6ce8\u5e78\u798f\u5e94\u7528\uff0c\u4f46\u5bf9\u98ce\u9669\u5173\u6ce8\u8f83\u5c11\u3002", "conclusion": "\u516c\u4f17\u8bdd\u8bed\u5728\u8bc6\u522b\u7d20\u517b\u548c\u6cbb\u7406\u5dee\u8ddd\u65b9\u9762\u5177\u6709\u8bca\u65ad\u529f\u80fd\uff0c\u5e76\u4e3a\u652f\u6301\u66f4\u77e5\u60c5\u7684LLM\u53c2\u4e0e\u63d0\u4f9b\u4e86\u4ea4\u6d41\u4e0e\u8bbe\u8ba1\u7b56\u7565\u3002"}}
{"id": "2511.03181", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03181", "abs": "https://arxiv.org/abs/2511.03181", "authors": ["Rewida Ali", "Cristian C. Beltran-Hernandez", "Weiwei Wan", "Kensuke Harada"], "title": "Learning-based Cooperative Robotic Paper Wrapping: A Unified Control Policy with Residual Force Control", "comment": null, "summary": "Human-robot cooperation is essential in environments such as warehouses and\nretail stores, where workers frequently handle deformable objects like paper,\nbags, and fabrics. Coordinating robotic actions with human assistance remains\ndifficult due to the unpredictable dynamics of deformable materials and the\nneed for adaptive force control. To explore this challenge, we focus on the\ntask of gift wrapping, which exemplifies a long-horizon manipulation problem\ninvolving precise folding, controlled creasing, and secure fixation of paper.\nSuccess is achieved when the robot completes the sequence to produce a neatly\nwrapped package with clean folds and no tears.\n  We propose a learning-based framework that integrates a high-level task\nplanner powered by a large language model (LLM) with a low-level hybrid\nimitation learning (IL) and reinforcement learning (RL) policy. At its core is\na Sub-task Aware Robotic Transformer (START) that learns a unified policy from\nhuman demonstrations. The key novelty lies in capturing long-range temporal\ndependencies across the full wrapping sequence within a single model. Unlike\nvanilla Action Chunking with Transformer (ACT), typically applied to short\ntasks, our method introduces sub-task IDs that provide explicit temporal\ngrounding. This enables robust performance across the entire wrapping process\nand supports flexible execution, as the policy learns sub-goals rather than\nmerely replicating motion sequences.\n  Our framework achieves a 97% success rate on real-world wrapping tasks. We\nshow that the unified transformer-based policy reduces the need for specialized\nmodels, allows controlled human supervision, and effectively bridges high-level\nintent with the fine-grained force control required for deformable object\nmanipulation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u9ad8\u5c42\u89c4\u5212\u548c\u4f4e\u5c42\u63a7\u5236\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u673a\u5668\u4eba\u5305\u88f9\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u5bf9\u53ef\u53d8\u5f62\u7269\u4f53\u7684\u7cbe\u7ec6\u64cd\u63a7\u80fd\u529b\u3002", "motivation": "\u4eba\u673a\u5408\u4f5c\u5728\u5904\u7406\u4f8b\u5982\u7eb8\u5f20\u3001\u888b\u5b50\u548c\u7ec7\u7269\u7b49\u53ef\u53d8\u5f62\u7269\u4f53\u7684\u73af\u5883\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u53ef\u53d8\u5f62\u6750\u6599\u7684\u4e0d\u53ef\u9884\u6d4b\u52a8\u6001\u4ee5\u53ca\u81ea\u9002\u5e94\u529b\u63a7\u5236\u7684\u9700\u6c42\uff0c\u4f7f\u5f97\u534f\u8c03\u673a\u5668\u4eba\u884c\u52a8\u4e0e\u4eba\u7c7b\u534f\u52a9\u53d8\u5f97\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u9ad8\u5c42\u4efb\u52a1\u89c4\u5212\u5668\u4e0e\u4f4e\u5c42\u6df7\u5408\u6a21\u4eff\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u6838\u5fc3\u662f\u4e00\u79cd\u80fd\u591f\u6355\u6349\u957f\u8303\u56f4\u65f6\u95f4\u4f9d\u8d56\u7684\u7edf\u4e00\u7b56\u7565\u6a21\u578b\u3002", "result": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e8697%\u7684\u6210\u529f\u7387\uff0c\u4e14\u80fd\u591f\u901a\u8fc7\u5b66\u4e60\u5b50\u76ee\u6807\u6765\u652f\u6301\u7075\u6d3b\u7684\u6267\u884c\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u590d\u73b0\u8fd0\u52a8\u5e8f\u5217\u3002", "conclusion": "\u672c\u7814\u7a76\u7684\u6846\u67b6\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u5305\u88f9\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e8697%\u7684\u6210\u529f\u7387\uff0c\u6709\u6548\u5730\u6865\u63a5\u4e86\u9ad8\u5c42\u610f\u56fe\u4e0e\u5bf9\u53ef\u53d8\u5f62\u7269\u4f53\u64cd\u4f5c\u6240\u9700\u7684\u7cbe\u7ec6\u529b\u63a7\u5236\u3002"}}
{"id": "2511.03198", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.03198", "abs": "https://arxiv.org/abs/2511.03198", "authors": ["Jiawei Zhou", "Amy Z. Chen", "Darshi Shah", "Laura M. Schwab-Reese", "Munmun De Choudhury"], "title": "Large Language Models as Information Sources: Distinctive Characteristics and Types of Low-Quality Information", "comment": null, "summary": "Recent advances in large language models (LLMs) have brought public and\nscholarly attention to their potential in generating low-quality information.\nWhile widely acknowledged as a risk, low-quality information remains a vaguely\ndefined concept, and little is known about how it manifests in LLM outputs or\nhow these outputs differ from those of traditional information sources. In this\nstudy, we focus on two key questions: What types of low-quality information are\nproduced by LLMs, and what makes them distinct than human-generated\ncounterparts? We conducted focus groups with public health professionals and\nindividuals with lived experience in three critical health contexts (vaccines,\nopioid use disorder, and intimate partner violence) where high-quality\ninformation is essential and misinformation, bias, and insensitivity are\nprevalent concerns. We identified a typology of LLM-generated low-quality\ninformation and a set of distinctive LLM characteristics compared to\ntraditional information sources. Our findings show that low-quality information\nextends beyond factual inaccuracies into types such as misprioritization and\nexaggeration, and that LLM affordances fundamentally differs from previous\ntechnologies. This work offers typologies on LLM distinctive characteristics\nand low-quality information types as a starting point for future efforts to\nunderstand LLM-generated low-quality information and mitigate related\ninformational harms. We call for conceptual and methodological discussions of\ninformation quality to move beyond truthfulness, in order to address the\naffordances of emerging technologies and the evolving dynamics of information\nbehaviors.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u4f4e\u8d28\u91cf\u4fe1\u606f\u79cd\u7c7b\u53ca\u5176\u7279\u5f81\uff0c\u63d0\u51fa\u4e86\u7814\u7a76\u672a\u6765\u4fe1\u606f\u8d28\u91cf\u7684\u6982\u5ff5\u548c\u65b9\u6cd5\u8bba\u6846\u67b6\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u53d1\u5c55\uff0c\u5176\u751f\u6210\u4f4e\u8d28\u91cf\u4fe1\u606f\u7684\u6f5c\u529b\u5907\u53d7\u5173\u6ce8\uff0c\u4f46\u4f4e\u8d28\u91cf\u4fe1\u606f\u7684\u5177\u4f53\u8868\u73b0\u53ca\u5176\u4e0e\u4f20\u7edf\u4fe1\u606f\u6e90\u7684\u533a\u522b\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u901a\u8fc7\u4e0e\u516c\u5171\u536b\u751f\u4e13\u4e1a\u4eba\u5458\u548c\u5177\u6709\u76f8\u5173\u7ecf\u5386\u7684\u4e2a\u4f53\u8fdb\u884c\u7126\u70b9\u5c0f\u7ec4\u8ba8\u8bba\uff0c\u7814\u7a76\u5728\u75ab\u82d7\u3001\u963f\u7247\u7c7b\u836f\u7269\u4f7f\u7528\u969c\u788d\u548c\u4f34\u4fa3\u66b4\u529b\u7b49\u5173\u952e\u5065\u5eb7\u80cc\u666f\u4e0bLLM\u751f\u6210\u7684\u4fe1\u606f\u3002", "result": "\u7814\u7a76\u8bc6\u522b\u4e86LLM\u751f\u6210\u7684\u4f4e\u8d28\u91cf\u4fe1\u606f\u7c7b\u578b\uff0c\u5305\u62ec\u9519\u8bef\u4f18\u5148\u7ea7\u548c\u5938\u5927\u7b49\uff0c\u5e76\u63ed\u793a\u4e86LLM\u4e0e\u4f20\u7edf\u4fe1\u606f\u6e90\u7684\u660e\u663e\u533a\u522b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86LLM\u751f\u6210\u4f4e\u8d28\u91cf\u4fe1\u606f\u7684\u7c7b\u578b\u5b66\u53ca\u5176\u72ec\u7279\u7279\u5f81\uff0c\u4e3a\u672a\u6765\u7406\u89e3\u548c\u51cf\u8f7b\u4fe1\u606f\u5371\u5bb3\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2511.03189", "categories": ["cs.RO", "cs.HC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.03189", "abs": "https://arxiv.org/abs/2511.03189", "authors": ["Zeqing Zhang", "Weifeng Lu", "Lei Yang", "Wei Jing", "Bowei Tang", "Jia Pan"], "title": "Collaborative Assembly Policy Learning of a Sightless Robot", "comment": "Accepted by IEEE ROBIO 2025", "summary": "This paper explores a physical human-robot collaboration (pHRC) task\ninvolving the joint insertion of a board into a frame by a sightless robot and\na human operator. While admittance control is commonly used in pHRC tasks, it\ncan be challenging to measure the force/torque applied by the human for\naccurate human intent estimation, limiting the robot's ability to assist in the\ncollaborative task. Other methods that attempt to solve pHRC tasks using\nreinforcement learning (RL) are also unsuitable for the board-insertion task\ndue to its safety constraints and sparse rewards. Therefore, we propose a novel\nRL approach that utilizes a human-designed admittance controller to facilitate\nmore active robot behavior and reduce human effort. Through simulation and\nreal-world experiments, we demonstrate that our approach outperforms admittance\ncontrol in terms of success rate and task completion time. Additionally, we\nobserved a significant reduction in measured force/torque when using our\nproposed approach compared to admittance control. The video of the experiments\nis available at https://youtu.be/va07Gw6YIog.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u578bRL\u65b9\u6cd5\uff0c\u65e8\u5728\u6539\u5584\u673a\u5668\u4eba\u5728\u4eba\u7c7b-\u673a\u5668\u4eba\u534f\u4f5c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u5728\u5b89\u5168\u7ea6\u675f\u548c\u7a00\u758f\u5956\u52b1\u573a\u666f\u4e0b\u3002", "motivation": "\u76ee\u524d\u7684\u9002\u5e94\u63a7\u5236\u65b9\u6cd5\u5728\u6d4b\u91cf\u4eba\u7c7b\u65bd\u52a0\u7684\u529b/\u626d\u77e9\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u5728\u534f\u4f5c\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u3002", "method": "\u91c7\u7528\u4eba\u7c7b\u8bbe\u8ba1\u7684\u9002\u5e94\u63a7\u5236\u5668\u7684RL\u65b9\u6cd5\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u548c\u5b9e\u9645\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u65b0\u65b9\u6cd5\u5728\u6548\u679c\u4e0a\u7684\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u63d0\u51fa\u7684RL\u65b9\u6cd5\u5728\u6210\u529f\u7387\u548c\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u4e0a\u4f18\u4e8e\u4f20\u7edf\u7684\u9002\u5e94\u63a7\u5236\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u6d4b\u91cf\u5230\u7684\u529b/\u626d\u77e9\u3002"}}
{"id": "2511.03227", "categories": ["cs.HC", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.03227", "abs": "https://arxiv.org/abs/2511.03227", "authors": ["Alexander Htet Kyaw", "Lenin Ravindranath Sivalingam"], "title": "Node-Based Editing for Multimodal Generation of Text, Audio, Image, and Vide", "comment": "Accepted to NeurIPS 2025, Conference on Neural Information Processing\n  Systems, Workshop on Generative and Protective AI for Content Creation", "summary": "We present a node-based storytelling system for multimodal content\ngeneration. The system represents stories as graphs of nodes that can be\nexpanded, edited, and iteratively refined through direct user edits and\nnatural-language prompts. Each node can integrate text, images, audio, and\nvideo, allowing creators to compose multimodal narratives. A task selection\nagent routes between specialized generative tasks that handle story generation,\nnode structure reasoning, node diagram formatting, and context generation. The\ninterface supports targeted editing of individual nodes, automatic branching\nfor parallel storylines, and node-based iterative refinement. Our results\ndemonstrate that node-based editing supports control over narrative structure\nand iterative generation of text, images, audio, and video. We report\nquantitative outcomes on automatic story outline generation and qualitative\nobservations of editing workflows. Finally, we discuss current limitations such\nas scalability to longer narratives and consistency across multiple nodes, and\noutline future work toward human-in-the-loop and user-centered creative AI\ntools.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8282\u70b9\u57fa\u7840\u7684\u6545\u4e8b\u751f\u6210\u7cfb\u7edf\uff0c\u652f\u6301\u591a\u6a21\u6001\u5185\u5bb9\u7684\u7f16\u8f91\u4e0e\u751f\u6210\uff0c\u4f46\u5b58\u5728\u6269\u5c55\u6027\u548c\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u672a\u6765\u8ba1\u5212\u63a2\u8ba8\u4eba\u673a\u534f\u4f5c\u7684\u521b\u610fAI\u5de5\u5177\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u4e2a\u80fd\u591f\u652f\u6301\u521b\u4f5c\u8005\u901a\u8fc7\u591a\u6a21\u6001\u65b9\u5f0f\u8bb2\u8ff0\u6545\u4e8b\u7684\u7cfb\u7edf\uff0c\u4ee5\u63d0\u5347\u6545\u4e8b\u521b\u4f5c\u7684\u7075\u6d3b\u6027\u548c\u63a7\u5236\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u8282\u70b9\u56fe\u5f62\u5316\u8868\u793a\u6545\u4e8b\uff0c\u7528\u6237\u53ef\u4ee5\u76f4\u63a5\u7f16\u8f91\u548c\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u8fdb\u884c\u4ea4\u4e92\uff0c\u7cfb\u7edf\u5185\u7f6e\u4efb\u52a1\u9009\u62e9\u4ee3\u7406\uff0c\u5904\u7406\u5404\u79cd\u751f\u6210\u4efb\u52a1\u3002", "result": "\u5c55\u793a\u4e86\u8282\u70b9\u7f16\u8f91\u80fd\u6709\u6548\u63a7\u5236\u53d9\u4e8b\u7ed3\u6784\uff0c\u5e76\u5728\u91cf\u5316\u548c\u8d28\u6027\u65b9\u9762\u7ed9\u51fa\u6545\u4e8b\u5927\u7eb2\u751f\u6210\u548c\u7f16\u8f91\u5de5\u4f5c\u6d41\u7684\u7ed3\u679c\u3002", "conclusion": "\u8be5\u8282\u70b9\u57fa\u7840\u7684\u6545\u4e8b\u8bb2\u8ff0\u7cfb\u7edf\u4e3a\u591a\u6a21\u6001\u5185\u5bb9\u751f\u6210\u63d0\u4f9b\u4e86\u63a7\u5236\u6743\uff0c\u5e76\u5c55\u73b0\u4e86\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u548c\u89c6\u9891\u7684\u8fed\u4ee3\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2511.03400", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.03400", "abs": "https://arxiv.org/abs/2511.03400", "authors": ["Minquan Gao", "Xinyi Li", "Qing Yan", "Xiaojian Sun", "Xiaopan Zhang", "Chien-Ming Huang", "Jiachen Li"], "title": "GUIDES: Guidance Using Instructor-Distilled Embeddings for Pre-trained Robot Policy Enhancement", "comment": "8 pages, 4 figures, Accepted by IEEE IROS 2025 Workshop WIR-M", "summary": "Pre-trained robot policies serve as the foundation of many validated robotic\nsystems, which encapsulate extensive embodied knowledge. However, they often\nlack the semantic awareness characteristic of foundation models, and replacing\nthem entirely is impractical in many situations due to high costs and the loss\nof accumulated knowledge. To address this gap, we introduce GUIDES, a\nlightweight framework that augments pre-trained policies with semantic guidance\nfrom foundation models without requiring architectural redesign. GUIDES employs\na fine-tuned vision-language model (Instructor) to generate contextual\ninstructions, which are encoded by an auxiliary module into guidance\nembeddings. These embeddings are injected into the policy's latent space,\nallowing the legacy model to adapt to this new semantic input through brief,\ntargeted fine-tuning. For inference-time robustness, a large language\nmodel-based Reflector monitors the Instructor's confidence and, when confidence\nis low, initiates a reasoning loop that analyzes execution history, retrieves\nrelevant examples, and augments the VLM's context to refine subsequent actions.\nExtensive validation in the RoboCasa simulation environment across diverse\npolicy architectures shows consistent and substantial improvements in task\nsuccess rates. Real-world deployment on a UR5 robot further demonstrates that\nGUIDES enhances motion precision for critical sub-tasks such as grasping.\nOverall, GUIDES offers a practical and resource-efficient pathway to upgrade,\nrather than replace, validated robot policies.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faGUIDES\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u7840\u6a21\u578b\u7684\u8bed\u4e49\u6307\u5bfc\u589e\u5f3a\u9884\u8bad\u7ec3\u673a\u5668\u4eba\u7b56\u7565\uff0c\u800c\u65e0\u9700\u6539\u5efa\u67b6\u6784\uff0c\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\u548c\u8fd0\u52a8\u7cbe\u5ea6\u3002", "motivation": "\u5728\u8bb8\u591a\u60c5\u51b5\u4e2d\uff0c\u5b8c\u5168\u66ff\u6362\u9884\u8bad\u7ec3\u673a\u5668\u4eba\u7b56\u7565\u662f\u4e0d\u73b0\u5b9e\u7684\uff0c\u56e0\u4e3a\u8fd9\u53ef\u80fd\u5bfc\u81f4\u9ad8\u6210\u672c\u548c\u77e5\u8bc6\u7684\u4e22\u5931\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u63d0\u5347\u8fd9\u4e9b\u7b56\u7565\u7684\u8bed\u4e49\u610f\u8bc6\u3002", "method": "\u901a\u8fc7\u5c06\u9884\u8bad\u7ec3\u7b56\u7565\u4e0e\u57fa\u7840\u6a21\u578b\u7684\u8bed\u4e49\u6307\u5bfc\u76f8\u7ed3\u5408\uff0c\u4f7f\u7528\u5fae\u8c03\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e0a\u4e0b\u6587\u6307\u4ee4\uff0c\u5e76\u901a\u8fc7\u8f85\u52a9\u6a21\u5757\u7f16\u7801\u4e3a\u6307\u5bfc\u5d4c\u5165\uff0c\u6ce8\u5165\u5230\u7b56\u7565\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u3002", "result": "\u5728RoboCasa\u4eff\u771f\u73af\u5883\u4e2d\u8fdb\u884c\u7684\u5e7f\u6cdb\u9a8c\u8bc1\u663e\u793a\uff0cTASK\u6210\u529f\u7387\u6301\u7eed\u4e14\u663e\u8457\u63d0\u9ad8\uff0cUR5\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9645\u90e8\u7f72\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86GUIDES\u5728\u91cd\u8981\u5b50\u4efb\u52a1\uff08\u5982\u6293\u53d6\uff09\u4e2d\u7684\u8fd0\u52a8\u7cbe\u5ea6\u589e\u5f3a\u3002", "conclusion": "GUIDES\u4e3a\u5347\u7ea7\u73b0\u6709\u673a\u5668\u4eba\u7b56\u7565\u63d0\u4f9b\u4e86\u4e00\u79cd\u5207\u5b9e\u53ef\u884c\u548c\u8d44\u6e90\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u800c\u975e\u5b8c\u5168\u66ff\u4ee3\u5b83\u4eec\u3002"}}
{"id": "2511.03282", "categories": ["cs.HC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03282", "abs": "https://arxiv.org/abs/2511.03282", "authors": ["Xinyu Ning", "Yan Zhuo", "Xian Wang", "Chan-In Devin Sio", "Lik-Hang Lee"], "title": "When Generative Artificial Intelligence meets Extended Reality: A Systematic Review", "comment": null, "summary": "With the continuous advancement of technology, the application of generative\nartificial intelligence (AI) in various fields is gradually demonstrating great\npotential, particularly when combined with Extended Reality (XR), creating\nunprecedented possibilities. This survey article systematically reviews the\napplications of generative AI in XR, covering as much relevant literature as\npossible from 2023 to 2025. The application areas of generative AI in XR and\nits key technology implementations are summarised through PRISMA screening and\nanalysis of the final 26 articles. The survey highlights existing articles from\nthe last three years related to how XR utilises generative AI, providing\ninsights into current trends and research gaps. We also explore potential\nopportunities for future research to further empower XR through generative AI,\nproviding guidance and information for future generative XR research.", "AI": {"tldr": "\u672c\u8c03\u7814\u6587\u7ae0\u7cfb\u7edf\u6027\u56de\u987e\u4e86\u751f\u6210\u6027\u4eba\u5de5\u667a\u80fd\u5728\u6269\u5c55\u73b0\u5b9e\u4e2d\u7684\u5e94\u7528\uff0c\u7a81\u51fa\u5173\u952e\u6280\u672f\u548c\u672a\u6765\u7814\u7a76\u673a\u4f1a\u3002", "motivation": "\u968f\u7740\u6280\u672f\u7684\u4e0d\u65ad\u8fdb\u6b65\uff0c\u751f\u6210\u6027\u4eba\u5de5\u667a\u80fd\u4e0e\u6269\u5c55\u73b0\u5b9e\u7ed3\u5408\uff0c\u521b\u9020\u4e86\u524d\u6240\u672a\u6709\u7684\u53ef\u80fd\u6027\u3002", "method": "\u901a\u8fc7PRISMA\u7b5b\u9009\u548c\u5206\u6790\u6700\u540e26\u7bc7\u6587\u7ae0\uff0c\u603b\u7ed3\u4e86\u751f\u6210\u6027AI\u5728XR\u4e2d\u7684\u5173\u952e\u6280\u672f\u5b9e\u73b0\u548c\u5e94\u7528\u9886\u57df\u3002", "result": "\u8c03\u7814\u603b\u7ed3\u4e862023\u81f32025\u5e74\u751f\u6210\u6027AI\u5728XR\u4e2d\u7684\u5e94\u7528\u9886\u57df\u53ca\u5176\u6280\u672f\u5b9e\u73b0\uff0c\u6307\u51fa\u4e86\u5f53\u524d\u8d8b\u52bf\u548c\u7814\u7a76\u7a7a\u767d\u3002", "conclusion": "\u7814\u7a76\u6307\u51fa\u751f\u6210\u6027\u4eba\u5de5\u667a\u80fd\u5728\u6269\u5c55\u73b0\u5b9e\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u5de8\u5927\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6307\u5bfc\u548c\u4fe1\u606f\u3002"}}
{"id": "2511.03444", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.03444", "abs": "https://arxiv.org/abs/2511.03444", "authors": ["Vesna Poprcova", "Iulia Lefter", "Martijn Warnier", "Frances Brazier"], "title": "Value Elicitation for a Socially Assistive Robot Addressing Social Anxiety: A Participatory Design Approach", "comment": "Accepted at Value Engineering in AI (VALE) Workshop (ECAI 2025)", "summary": "Social anxiety is a prevalent mental health condition that can significantly\nimpact overall well-being and quality of life. Despite its widespread effects,\nadequate support or treatment for social anxiety is often insufficient.\nAdvances in technology, particularly in social robotics, offer promising\nopportunities to complement traditional mental health. As an initial step\ntoward developing effective solutions, it is essential to understand the values\nthat shape what is considered meaningful, acceptable, and helpful. In this\nstudy, a participatory design workshop was conducted with mental health\nacademic researchers to elicit the underlying values that should inform the\ndesign of socially assistive robots for social anxiety support. Through\ncreative, reflective, and envisioning activities, participants explored\nscenarios and design possibilities, allowing for systematic elicitation of\nvalues, expectations, needs, and preferences related to robot-supported\ninterventions. The findings reveal rich insights into design-relevant\nvalues-including adaptivity, acceptance, and efficacy-that are core to support\nfor individuals with social anxiety. This study highlights the significance of\na research-led approach to value elicitation, emphasising user-centred and\ncontext-aware design considerations in the development of socially assistive\nrobots.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u793e\u4ea4\u7126\u8651\u652f\u6301\u7684\u793e\u4ea4\u8f85\u52a9\u673a\u5668\u4eba\u7684\u8bbe\u8ba1\u4ef7\u503c\uff0c\u901a\u8fc7\u53c2\u4e0e\u5f0f\u8bbe\u8ba1\u7814\u8ba8\u4f1a\u83b7\u5f97\u7528\u6237\u4e2d\u5fc3\u7684\u89c1\u89e3\uff0c\u5f3a\u8c03\u5728\u8bbe\u8ba1\u4e2d\u8003\u8651\u9002\u5e94\u6027\u3001\u63a5\u53d7\u5ea6\u548c\u6709\u6548\u6027\u3002", "motivation": "\u793e\u4ea4\u7126\u8651\u662f\u4e00\u79cd\u5e7f\u6cdb\u5b58\u5728\u7684\u5fc3\u7406\u5065\u5eb7\u95ee\u9898\uff0c\u5f71\u54cd\u7740\u6574\u4f53\u798f\u7949\u548c\u751f\u6d3b\u8d28\u91cf\uff0c\u800c\u73b0\u6709\u7684\u652f\u6301\u548c\u6cbb\u7597\u5f80\u5f80\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u4e0e\u5fc3\u7406\u5065\u5eb7\u5b66\u672f\u7814\u7a76\u8005\u8fdb\u884c\u53c2\u4e0e\u5f0f\u8bbe\u8ba1\u7814\u8ba8\u4f1a\uff0c\u63a2\u7d22\u4e0e\u793e\u4ea4\u7126\u8651\u652f\u6301\u7684\u673a\u5668\u4eba\u5e72\u9884\u76f8\u5173\u7684\u4ef7\u503c\u3001\u671f\u671b\u3001\u9700\u6c42\u548c\u504f\u597d\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u4e0e\u793e\u4ea4\u7126\u8651\u652f\u6301\u76f8\u5173\u7684\u8bbe\u8ba1\u76f8\u5173\u4ef7\u503c\uff0c\u5305\u62ec\u9002\u5e94\u6027\u3001\u63a5\u53d7\u5ea6\u548c\u6709\u6548\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u6df1\u5165\u7684\u8bbe\u8ba1\u6d1e\u5bdf\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4ee5\u7814\u7a76\u4e3a\u5bfc\u5411\u7684\u4ef7\u503c\u83b7\u53d6\u65b9\u6cd5\u7684\u91cd\u8981\u6027\uff0c\u5f3a\u8c03\u7528\u6237\u4e2d\u5fc3\u548c\u60c5\u5883\u610f\u8bc6\u7684\u8bbe\u8ba1\u8003\u8651\u5728\u793e\u4ea4\u8f85\u52a9\u673a\u5668\u4eba\u5f00\u53d1\u4e2d\u7684\u4f5c\u7528\u3002"}}
{"id": "2511.03375", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.03375", "abs": "https://arxiv.org/abs/2511.03375", "authors": ["Mengyao Guo", "Kexin Nie", "Ze Gao", "Black Sun", "Xueyang Wang", "Jinda Han", "Xingting Wu"], "title": "I Prompt, it Generates, we Negotiate. Exploring Text-Image Intertextuality in Human-AI Co-Creation of Visual Narratives with VLMs", "comment": "38 pages, 23 figures", "summary": "Creating meaningful visual narratives through human-AI collaboration requires\nunderstanding how text-image intertextuality emerges when textual intentions\nmeet AI-generated visuals. We conducted a three-phase qualitative study with 15\nparticipants using GPT-4o to investigate how novices navigate sequential visual\nnarratives. Our findings show that users develop strategies to harness AI's\nsemantic surplus by recognizing meaningful visual content beyond literal\ndescriptions, iteratively refining prompts, and constructing narrative\nsignificance through complementary text-image relationships. We identified four\ndistinct collaboration patterns and, through fsQCA's analysis, discovered three\npathways to successful intertextual collaboration: Educational Collaborator,\nTechnical Expert, and Visual Thinker. However, participants faced challenges,\nincluding cultural representation gaps, visual consistency issues, and\ndifficulties translating narrative concepts into visual prompts. These findings\ncontribute to HCI research by providing an empirical account of\n\\textit{text-image intertextuality} in human-AI co-creation and proposing\ndesign implications for role-based AI assistants that better support iterative,\nhuman-led creative processes in visual storytelling.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4eba\u673a\u5408\u4f5c\u4e2d\u6587\u672c-\u56fe\u50cf\u4e92\u6587\u6027\u7684\u5f62\u6210\u8fc7\u7a0b\uff0c\u63ed\u793a\u7528\u6237\u5982\u4f55\u5229\u7528AI\u4ea7\u751f\u6709\u610f\u4e49\u7684\u89c6\u89c9\u53d9\u4e8b\uff0c\u5e76\u603b\u7ed3\u51fa\u6210\u529f\u5408\u4f5c\u7684\u8def\u5f84\u53ca\u9762\u4e34\u7684\u6311\u6218\u3002", "motivation": "\u63a2\u7d22\u6587\u672c-\u56fe\u50cf\u4e92\u6587\u6027\u5982\u4f55\u5728\u6587\u672c\u610f\u56fe\u548cAI\u751f\u6210\u89c6\u89c9\u5185\u5bb9\u4ea4\u4e92\u4e2d\u51fa\u73b0\uff0c\u4ee5\u4fc3\u8fdb\u5bcc\u6709\u610f\u4e49\u7684\u89c6\u89c9\u53d9\u4e8b\u7684\u521b\u9020\u3002", "method": "\u901a\u8fc7\u5bf915\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u4e09\u9636\u6bb5\u7684\u5b9a\u6027\u7814\u7a76\uff0c\u4f7f\u7528GPT-4o\u63a2\u8ba8\u65b0\u624b\u5982\u4f55\u5728\u987a\u5e8f\u89c6\u89c9\u53d9\u4e8b\u4e2d\u5bfc\u822a\u3002", "result": "\u7528\u6237\u901a\u8fc7\u8bc6\u522b\u8d85\u8d8a\u5b57\u9762\u63cf\u8ff0\u7684\u6709\u610f\u4e49\u89c6\u89c9\u5185\u5bb9\u3001\u53cd\u590d\u5b8c\u5584\u63d0\u793a\u4ee5\u53ca\u901a\u8fc7\u4e92\u8865\u7684\u6587\u672c-\u56fe\u50cf\u5173\u7cfb\u6784\u5efa\u53d9\u4e8b\u610f\u4e49\uff0c\u53d1\u5c55\u4e86\u5229\u7528AI\u8bed\u4e49\u76c8\u4f59\u7684\u7b56\u7565\u3002\u7814\u7a76\u8bc6\u522b\u51fa\u56db\u79cd\u4e0d\u540c\u7684\u534f\u4f5c\u6a21\u5f0f\uff0c\u5e76\u53d1\u73b0\u6210\u529f\u7684\u4e92\u6587\u534f\u4f5c\u6709\u4e09\u4e2a\u8def\u5f84\uff1a\u6559\u80b2\u5408\u4f5c\u8005\u3001\u6280\u672f\u4e13\u5bb6\u548c\u89c6\u89c9\u601d\u7ef4\u8005\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4eba\u673a\u534f\u4f5c\u4e2d\u6587\u672c-\u56fe\u50cf\u4e92\u6587\u6027\u7684\u7ecf\u9a8c\u6027\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u9896\u89c6\u89d2\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdbAI\u52a9\u624b\u8bbe\u8ba1\u7684\u5efa\u8bae\uff0c\u4ee5\u66f4\u597d\u5730\u652f\u6301\u4eba\u7c7b\u4e3b\u5bfc\u7684\u521b\u4f5c\u8fc7\u7a0b\u3002"}}
{"id": "2511.03481", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03481", "abs": "https://arxiv.org/abs/2511.03481", "authors": ["Jianbo Yuan", "Haohua Zhu", "Jing Dai", "Sheng Yi"], "title": "Development of the Bioinspired Tendon-Driven DexHand 021 with Proprioceptive Compliance Control", "comment": "8 pages 18 fogures, IEEE RAL accept", "summary": "The human hand plays a vital role in daily life and industrial applications,\nyet replicating its multifunctional capabilities-including motion, sensing, and\ncoordinated manipulation-with robotic systems remains a formidable challenge.\nDeveloping a dexterous robotic hand requires balancing human-like agility with\nengineering constraints such as complexity, size-to-weight ratio, durability,\nand force-sensing performance. This letter presents Dex-Hand 021, a\nhigh-performance, cable-driven five-finger robotic hand with 12 active and 7\npassive degrees of freedom (DoFs), achieving 19 DoFs dexterity in a lightweight\n1 kg design. We propose a proprioceptive force-sensing-based admittance control\nmethod to enhance manipulation. Experimental results demonstrate its superior\nperformance: a single-finger load capacity exceeding 10 N, fingertip\nrepeatability under 0.001 m, and force estimation errors below 0.2 N. Compared\nto PID control, joint torques in multi-object grasping are reduced by 31.19%,\nsignificantly improves force-sensing capability while preventing overload\nduring collisions. The hand excels in both power and precision grasps,\nsuccessfully executing 33 GRASP taxonomy motions and complex manipulation\ntasks. This work advances the design of lightweight, industrial-grade dexterous\nhands and enhances proprioceptive control, contributing to robotic manipulation\nand intelligent manufacturing.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86Dex-Hand 021\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u529b\u4f20\u611f\u63a7\u5236\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u624b\u7684\u6027\u80fd\u548c\u7075\u6d3b\u6027\uff0c\u63a8\u52a8\u4e86\u7075\u5de7\u673a\u68b0\u624b\u7684\u5de5\u4e1a\u5e94\u7528\u3002", "motivation": "\u590d\u5236\u4eba\u624b\u7684\u591a\u529f\u80fd\u80fd\u529b\uff0c\u4ee5\u5e94\u5bf9\u65e5\u5e38\u751f\u6d3b\u548c\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u672c\u4f53\u611f\u77e5\u7684\u529b\u4f20\u611f\u63a7\u5236\u65b9\u6cd5\u6765\u589e\u5f3a\u64cd\u63a7\u80fd\u529b\u3002", "result": "Dex-Hand 021\u662f\u4e00\u6b3e\u9ad8\u6027\u80fd\u7684\u7ea4\u7ef4\u9a71\u52a8\u4e94\u6307\u673a\u5668\u4eba\u624b\uff0c\u5177\u590719\u4e2a\u81ea\u7531\u5ea6\uff0c\u8f7b\u91cf\u5316\u8bbe\u8ba1\uff0c\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u8d1f\u8f7d\u80fd\u529b\u548c\u9ad8\u91cd\u590d\u6027\u3002\u4e0ePID\u63a7\u5236\u76f8\u6bd4\uff0c\u591a\u7269\u4f53\u6293\u63e1\u4e2d\u7684\u5173\u8282\u626d\u77e9\u51cf\u5c11\u4e8631.19%\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8f7b\u91cf\u7ea7\u5de5\u4e1a\u7ea7\u7075\u5de7\u624b\u7684\u8bbe\u8ba1\u548c\u672c\u4f53\u611f\u77e5\u63a7\u5236\u7684\u53d1\u5c55\u505a\u51fa\u4e86\u8d21\u732e\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u63a7\u80fd\u529b\u548c\u667a\u80fd\u5236\u9020\u6c34\u5e73\u3002"}}
{"id": "2511.03434", "categories": ["cs.HC", "cs.AI", "cs.MA", "cs.NI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.03434", "abs": "https://arxiv.org/abs/2511.03434", "authors": ["Botao 'Amber' Hu", "Helena Rong"], "title": "Inter-Agent Trust Models: A Comparative Study of Brief, Claim, Proof, Stake, Reputation and Constraint in Agentic Web Protocol Design-A2A, AP2, ERC-8004, and Beyond", "comment": "Submitted to AAAI 2026 Workshop on Trust and Control in Agentic AI\n  (TrustAgent)", "summary": "As the \"agentic web\" takes shape-billions of AI agents (often LLM-powered)\nautonomously transacting and collaborating-trust shifts from human oversight to\nprotocol design. In 2025, several inter-agent protocols crystallized this\nshift, including Google's Agent-to-Agent (A2A), Agent Payments Protocol (AP2),\nand Ethereum's ERC-8004 \"Trustless Agents,\" yet their underlying trust\nassumptions remain under-examined. This paper presents a comparative study of\ntrust models in inter-agent protocol design: Brief (self- or third-party\nverifiable claims), Claim (self-proclaimed capabilities and identity, e.g.\nAgentCard), Proof (cryptographic verification, including zero-knowledge proofs\nand trusted execution environment attestations), Stake (bonded collateral with\nslashing and insurance), Reputation (crowd feedback and graph-based trust\nsignals), and Constraint (sandboxing and capability bounding). For each, we\nanalyze assumptions, attack surfaces, and design trade-offs, with particular\nemphasis on LLM-specific fragilities-prompt injection,\nsycophancy/nudge-susceptibility, hallucination, deception, and\nmisalignment-that render purely reputational or claim-only approaches brittle.\nOur findings indicate no single mechanism suffices. We argue for\ntrustless-by-default architectures anchored in Proof and Stake to gate\nhigh-impact actions, augmented by Brief for identity and discovery and\nReputation overlays for flexibility and social signals. We comparatively\nevaluate A2A, AP2, ERC-8004 and related historical variations in academic\nresearch under metrics spanning security, privacy, latency/cost, and social\nrobustness (Sybil/collusion/whitewashing resistance). We conclude with hybrid\ntrust model recommendations that mitigate reputation gaming and misinformed LLM\nbehavior, and we distill actionable design guidelines for safer, interoperable,\nand scalable agent economies.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u79cd\u4ee3\u7406\u534f\u8bae\u7684\u4fe1\u4efb\u6a21\u578b\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u7684\u5047\u8bbe\u548c\u8106\u5f31\u6027\uff0c\u63d0\u51fa\u4e86\u6df7\u5408\u4fe1\u4efb\u6a21\u578b\u4ee5\u6539\u5584\u4ee3\u7406\u7ecf\u6d4e\u7684\u5b89\u5168\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u968f\u7740AI\u4ee3\u7406\u7684\u5d1b\u8d77\uff0c\u4fe1\u4efb\u4ece\u4eba\u7c7b\u76d1\u7763\u8f6c\u5411\u534f\u8bae\u8bbe\u8ba1\uff0c\u800c\u73b0\u6709\u7684\u4fe1\u4efb\u673a\u5236\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u7279\u522b\u662fLLM\u7279\u6709\u7684\u8106\u5f31\u6027\u548c\u98ce\u9669\u3002", "method": "\u672c\u6587\u901a\u8fc7\u6bd4\u8f83\u7814\u7a76\u4e0d\u540c\u7684\u4fe1\u4efb\u6a21\u578b\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u5728\u4ee3\u7406\u534f\u8bae\u8bbe\u8ba1\u4e2d\u7684\u5047\u8bbe\u3001\u653b\u51fb\u9762\u548c\u8bbe\u8ba1\u6743\u8861\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6ca1\u6709\u5355\u4e00\u7684\u673a\u5236\u53ef\u4ee5\u6ee1\u8db3\u9700\u6c42\uff0c\u5efa\u8bae\u4ee5Proof\u548cStake\u4e3a\u57fa\u7840\u7684\u65e0\u4fe1\u4efb\u67b6\u6784\uff0c\u5e76\u7ed3\u5408Brief\u548cReputation\u4ee5\u589e\u5f3a\u7075\u6d3b\u6027\u548c\u793e\u4ea4\u4fe1\u53f7\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u6df7\u5408\u4fe1\u4efb\u6a21\u578b\u7684\u5efa\u8bae\uff0c\u4ee5\u51cf\u8f7b\u4fe1\u8a89\u6e38\u620f\u548c\u8bef\u5bfc\u6027LLM\u884c\u4e3a\uff0c\u5e76\u4e3a\u5b89\u5168\u3001\u53ef\u4e92\u64cd\u4f5c\u4e14\u53ef\u6269\u5c55\u7684\u4ee3\u7406\u7ecf\u6d4e\u5236\u5b9a\u4e86\u53ef\u884c\u7684\u8bbe\u8ba1\u6307\u5357\u3002"}}
{"id": "2511.03497", "categories": ["cs.RO", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.03497", "abs": "https://arxiv.org/abs/2511.03497", "authors": ["Lei Fu", "Sahar Salimpour", "Leonardo Militano", "Harry Edelman", "Jorge Pe\u00f1a Queralta", "Giovanni Toffetti"], "title": "ROSBag MCP Server: Analyzing Robot Data with LLMs for Agentic Embodied AI Applications", "comment": null, "summary": "Agentic AI systems and Physical or Embodied AI systems have been two key\nresearch verticals at the forefront of Artificial Intelligence and Robotics,\nwith Model Context Protocol (MCP) increasingly becoming a key component and\nenabler of agentic applications. However, the literature at the intersection of\nthese verticals, i.e., Agentic Embodied AI, remains scarce. This paper\nintroduces an MCP server for analyzing ROS and ROS 2 bags, allowing for\nanalyzing, visualizing and processing robot data with natural language through\nLLMs and VLMs. We describe specific tooling built with robotics domain\nknowledge, with our initial release focused on mobile robotics and supporting\nnatively the analysis of trajectories, laser scan data, transforms, or time\nseries data. This is in addition to providing an interface to standard ROS 2\nCLI tools (\"ros2 bag list\" or \"ros2 bag info\"), as well as the ability to\nfilter bags with a subset of topics or trimmed in time. Coupled with the MCP\nserver, we provide a lightweight UI that allows the benchmarking of the tooling\nwith different LLMs, both proprietary (Anthropic, OpenAI) and open-source\n(through Groq). Our experimental results include the analysis of tool calling\ncapabilities of eight different state-of-the-art LLM/VLM models, both\nproprietary and open-source, large and small. Our experiments indicate that\nthere is a large divide in tool calling capabilities, with Kimi K2 and Claude\nSonnet 4 demonstrating clearly superior performance. We also conclude that\nthere are multiple factors affecting the success rates, from the tool\ndescription schema to the number of arguments, as well as the number of tools\navailable to the models. The code is available with a permissive license at\nhttps://github.com/binabik-ai/mcp-rosbags.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86MCP\u670d\u52a1\u5668\u4ee5\u63d0\u5347\u79fb\u52a8\u673a\u5668\u4eba\u6570\u636e\u5206\u6790\u80fd\u529b\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540cLLM/VLM\u6a21\u578b\u7684\u5de5\u5177\u8c03\u7528\u6548\u679c\uff0c\u53d1\u73b0Kimi K2\u548cClaude Sonnet 4\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u586b\u8865Agentic Embodied AI\u9886\u57df\u6587\u732e\u7684\u7a7a\u767d\uff0c\u63a2\u7d22MCP\u5728\u79fb\u52a8\u673a\u5668\u4eba\u6570\u636e\u5206\u6790\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u6784\u5efaMCP\u670d\u52a1\u5668\u5206\u6790ROS\u548cROS 2\u6570\u636e\u5305\uff0c\u5f00\u53d1\u7279\u5b9a\u5de5\u5177\u5e76\u4f7f\u7528\u591a\u79cdLLM\u548cVLM\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aKimi K2\u548cClaude Sonnet 4\u5728\u5de5\u5177\u8c03\u7528\u80fd\u529b\u4e0a\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u4e14\u6210\u529f\u7387\u53d7\u591a\u79cd\u56e0\u7d20\u7684\u5f71\u54cd\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\u4e0d\u540cLLM/VLM\u6a21\u578b\u5728\u5de5\u5177\u8c03\u7528\u80fd\u529b\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u4e14\u591a\u4e2a\u56e0\u7d20\u5f71\u54cd\u6210\u529f\u7387\u3002"}}
{"id": "2511.03478", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.03478", "abs": "https://arxiv.org/abs/2511.03478", "authors": ["Jeongah Lee", "Ali Sarvghad"], "title": "SVG Decomposition for Enhancing Large Multimodal Models Visualization Comprehension: A Study with Floor Plans", "comment": "10 pages, 2 figures", "summary": "Large multimodal models (LMMs) are increasingly capable of interpreting\nvisualizations, yet they continue to struggle with spatial reasoning. One\nproposed strategy is decomposition, which breaks down complex visualizations\ninto structured components. In this work, we examine the efficacy of scalable\nvector graphics (SVGs) as a decomposition strategy for improving LMMs'\nperformance on floor plans comprehension. Floor plans serve as a valuable\ntestbed because they combine geometry, topology, and semantics, and their\nreliable comprehension has real-world applications, such as accessibility for\nblind and low-vision individuals. We conducted an exploratory study with three\nLMMs (GPT-4o, Claude 3.7 Sonnet, and Llama 3.2 11B Vision Instruct) across 75\nfloor plans. Results show that combining SVG with raster input (SVG+PNG)\nimproves performance on spatial understanding tasks but often hinders spatial\nreasoning, particularly in pathfinding. These findings highlight both the\npromise and limitations of decomposition as a strategy for advancing spatial\nvisualization comprehension.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u53ef\u6269\u5c55\u77e2\u91cf\u56fe\u5f62\uff08SVG\uff09\u4f5c\u4e3a\u5206\u89e3\u7b56\u7565\u6765\u6539\u5584\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u7406\u89e3\u697c\u5c42\u5e73\u9762\u4e2d\u7684\u8868\u73b0\uff0c\u663e\u793a\u4e86SVG\u7684\u6f5c\u529b\u4e0e\u5c40\u9650\u6027\u3002", "motivation": "\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u89e3\u91ca\u53ef\u89c6\u5316\u65b9\u9762\u7684\u80fd\u529b\u8d8a\u6765\u8d8a\u5f3a\uff0c\u4f46\u5728\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u56f0\u96be\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u65b0\u7684\u7b56\u7565\u4ee5\u63d0\u5347\u5176\u7406\u89e3\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5bf975\u4e2a\u5e73\u9762\u56fe\u8fdb\u884c\u63a2\u7d22\u6027\u7814\u7a76\uff0c\u8bc4\u4f30\u4e09\u79cd\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08GPT-4o\u3001Claude 3.7 Sonnet\u548cLlama 3.2 11B Vision Instruct\uff09\u5728\u7406\u89e3\u697c\u5c42\u5e73\u9762\u65b9\u9762\u7684\u6027\u80fd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cSVG\u4e0e\u5149\u6805\u8f93\u5165\u7684\u7ed3\u5408\u6709\u52a9\u4e8e\u63d0\u9ad8\u7a7a\u95f4\u7406\u89e3\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u4f46\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u5374\u4f1a\u59a8\u788d\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u7ed3\u5408SVG\u4e0e\u5149\u6805\u8f93\u5165(SVG+PNG)\u53ef\u4ee5\u63d0\u9ad8\u7a7a\u95f4\u7406\u89e3\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u4f46\u5728\u8def\u5f84\u5bfb\u627e\u7b49\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u5374\u53ef\u80fd\u53d7\u5230 hinder\u3002"}}
{"id": "2511.03550", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.03550", "abs": "https://arxiv.org/abs/2511.03550", "authors": ["Hong Wang", "Ridhima Phatak", "James Ocampo", "Zhao Han"], "title": "Indicating Robot Vision Capabilities with Augmented Reality", "comment": null, "summary": "Research indicates that humans can mistakenly assume that robots and humans\nhave the same field of view (FoV), possessing an inaccurate mental model of\nrobots. This misperception may lead to failures during human-robot\ncollaboration tasks where robots might be asked to complete impossible tasks\nabout out-of-view objects. The issue is more severe when robots do not have a\nchance to scan the scene to update their world model while focusing on assigned\ntasks. To help align humans' mental models of robots' vision capabilities, we\npropose four FoV indicators in augmented reality (AR) and conducted a user\nhuman-subjects experiment (N=41) to evaluate them in terms of accuracy,\nconfidence, task efficiency, and workload. These indicators span a spectrum\nfrom egocentric (robot's eye and head space) to allocentric (task space).\nResults showed that the allocentric blocks at the task space had the highest\naccuracy with a delay in interpreting the robot's FoV. The egocentric indicator\nof deeper eye sockets, possible for physical alteration, also increased\naccuracy. In all indicators, participants' confidence was high while cognitive\nload remained low. Finally, we contribute six guidelines for practitioners to\napply our AR indicators or physical alterations to align humans' mental models\nwith robots' vision capabilities.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4eba\u7c7b\u5bf9\u673a\u5668\u4eba\u89c6\u91ce\u7684\u9519\u8bef\u7406\u89e3\uff0c\u5e76\u901a\u8fc7AR\u6280\u672f\u63d0\u51fa\u4e86\u56db\u79cd\u89c6\u91ce\u6307\u793a\u5668\u4ee5\u6539\u5584\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u4eba\u7c7b\u5e38\u5e38\u9519\u8bef\u5730\u5047\u8bbe\u673a\u5668\u4eba\u548c\u4eba\u7c7b\u6709\u76f8\u540c\u7684\u89c6\u91ce\uff0c\u8fd9\u4f1a\u5f71\u54cd\u4eba\u673a\u534f\u4f5c\u7684\u6210\u529f\u3002", "method": "\u901a\u8fc7\u5728\u4eba\u7fa4\u4e2d\u8fdb\u884c\u7528\u6237\u5b9e\u9a8c\uff08N=41\uff09\uff0c\u8bc4\u4f30\u56db\u79cdAR\u89c6\u91ce\u6307\u793a\u5668\u7684\u51c6\u786e\u6027\u3001\u4fe1\u5fc3\u3001\u4efb\u52a1\u6548\u7387\u548c\u5de5\u4f5c\u8d1f\u8f7d\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u4efb\u52a1\u7a7a\u95f4\u7684\u5916\u90e8\u89c6\u91ce\u6307\u6807\u51c6\u786e\u6027\u6700\u9ad8\uff0c\u4e14\u53c2\u4e0e\u8005\u5728\u6240\u6709\u6307\u6807\u4e0b\u4fe1\u5fc3\u9ad8\u3001\u8ba4\u77e5\u8d1f\u62c5\u4f4e\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u7684AR\u89c6\u91ce\u6307\u793a\u5668\u6709\u6548\u63d0\u9ad8\u4e86\u4eba\u7c7b\u5bf9\u673a\u5668\u4eba\u89c6\u91ce\u80fd\u529b\u7684\u7406\u89e3\uff0c\u5e76\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u5e94\u7528\u6307\u5357\u3002"}}
{"id": "2511.03534", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.03534", "abs": "https://arxiv.org/abs/2511.03534", "authors": ["Zhaoxin Chang", "Fusang Zhang", "Jie Xiong", "Ziyu Li", "Badii Jouaber", "Daqing Zhang"], "title": "PnPSelect: Plug-and-play IoT Device Selection Using Ultra-wideband Signals", "comment": null, "summary": "In recent years, the number of Internet of Things (IoT) devices in smart\nhomes has rapidly increased. A key challenge affecting user experience is how\nto enable users to efficiently and intuitively select the devices they wish to\ncontrol. This paper proposes PnPSelect, a plug-and-play IoT device selection\nsolution utilizing Ultra-wideband (UWB) technology on commercial devices.\nUnlike previous works, PnPSelect does not require the installation of dedicated\nhardware on each IoT device, thereby reducing deployment costs and\ncomplexities, and achieving true plug-and-play functionality. To enable\nintuitive device selection, we introduce a pointing direction estimation method\nthat utilizes UWB readings from a single anchor to infer the user pointing\ndirection. Additionally, we propose a lightweight device localization method\nthat allows users to register new IoT devices by simply pointing at them from\ntwo distinct positions, eliminating the need for manual measurements. We\nimplement PnPSelect on commercial smartphones and smartwatches and conduct\nextensive evaluations in both controlled laboratory settings and real-world\nenvironments. Our results demonstrate high accuracy, robustness, and\nadaptability, making PnPSelect a practical and scalable solution for\nnext-generation smart home interactions.", "AI": {"tldr": "PnPSelect\u662f\u4e00\u79cd\u57fa\u4e8e\u8d85\u5bbd\u5e26\u6280\u672f\u7684\u5373\u63d2\u5373\u7528\u7269\u8054\u7f51\u8bbe\u5907\u9009\u62e9\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u5347\u4e86\u7528\u6237\u9009\u53d6\u8bbe\u5907\u7684\u6548\u7387\u548c\u76f4\u89c2\u6027\uff0c\u4e14\u5177\u6709\u9ad8\u7cbe\u5ea6\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u5bb6\u5c45\u4e2d\u7269\u8054\u7f51\u8bbe\u5907\u6570\u91cf\u7684\u5feb\u901f\u589e\u957f\uff0c\u7528\u6237\u9009\u62e9\u63a7\u5236\u8bbe\u5907\u7684\u6548\u7387\u548c\u76f4\u89c2\u6027\u9762\u4e34\u6311\u6218\u3002", "method": "\u5229\u7528\u8d85\u5bbd\u5e26\u6280\u672f\u5f00\u53d1\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u8bbe\u5907\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u6307\u5411\u65b9\u5411\u4f30\u8ba1\u65b9\u6cd5\u3002", "result": "PnPSelect\u5728\u5546\u4e1a\u667a\u80fd\u624b\u673a\u548c\u667a\u80fd\u624b\u8868\u4e0a\u5b9e\u73b0\uff0c\u5e76\u5728\u53d7\u63a7\u5b9e\u9a8c\u5ba4\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u5c55\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "PnPSelect\u4e3a\u4e0b\u4e00\u4ee3\u667a\u80fd\u5bb6\u5c45\u4ea4\u4e92\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u9645\u53ef\u884c\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u5907\u9ad8\u7cbe\u5ea6\u3001\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2511.03571", "categories": ["cs.RO", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.03571", "abs": "https://arxiv.org/abs/2511.03571", "authors": ["Hao Shi", "Ze Wang", "Shangwei Guo", "Mengfei Duan", "Song Wang", "Teng Chen", "Kailun Yang", "Lin Wang", "Kaiwei Wang"], "title": "OneOcc: Semantic Occupancy Prediction for Legged Robots with a Single Panoramic Camera", "comment": "Datasets and code will be publicly available at\n  https://github.com/MasterHow/OneOcc", "summary": "Robust 3D semantic occupancy is crucial for legged/humanoid robots, yet most\nsemantic scene completion (SSC) systems target wheeled platforms with\nforward-facing sensors. We present OneOcc, a vision-only panoramic SSC\nframework designed for gait-introduced body jitter and 360{\\deg} continuity.\nOneOcc combines: (i) Dual-Projection fusion (DP-ER) to exploit the annular\npanorama and its equirectangular unfolding, preserving 360{\\deg} continuity and\ngrid alignment; (ii) Bi-Grid Voxelization (BGV) to reason in Cartesian and\ncylindrical-polar spaces, reducing discretization bias and sharpening\nfree/occupied boundaries; (iii) a lightweight decoder with Hierarchical AMoE-3D\nfor dynamic multi-scale fusion and better long-range/occlusion reasoning; and\n(iv) plug-and-play Gait Displacement Compensation (GDC) learning feature-level\nmotion correction without extra sensors. We also release two panoramic\noccupancy benchmarks: QuadOcc (real quadruped, first-person 360{\\deg}) and\nHuman360Occ (H3O) (CARLA human-ego 360{\\deg} with RGB, Depth, semantic\noccupancy; standardized within-/cross-city splits). OneOcc sets new\nstate-of-the-art (SOTA): on QuadOcc it beats strong vision baselines and\npopular LiDAR ones; on H3O it gains +3.83 mIoU (within-city) and +8.08\n(cross-city). Modules are lightweight, enabling deployable full-surround\nperception for legged/humanoid robots. Datasets and code will be publicly\navailable at https://github.com/MasterHow/OneOcc.", "AI": {"tldr": "OneOcc\u662f\u4e00\u4e2a\u4e3a\u817f\u90e8/\u7c7b\u4eba\u673a\u5668\u4eba\u8bbe\u8ba1\u7684\u5168\u89c6\u89c9360\u5ea6\u8bed\u4e49\u573a\u666f\u5b8c\u6210\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u65b0\u7684\u6027\u80fd\u7a81\u7834\uff0c\u5e76\u5c06\u63d0\u4f9b\u516c\u5171\u6570\u636e\u96c6\u548c\u4ee3\u7801\u3002", "motivation": "\u9488\u5bf9\u817f\u90e8/\u7c7b\u4eba\u673a\u5668\u4eba\uff0c\u63d0\u4f9b\u9c81\u68d2\u76843D\u8bed\u4e49\u5360\u7528\uff0c\u514b\u670d\u5927\u591a\u6570\u8bed\u4e49\u573a\u666f\u5b8c\u6210\u7cfb\u7edf\u5728\u8fd0\u52a8\u5e73\u53f0\u4e0e\u4f20\u611f\u5668\u5e03\u5c40\u4e0a\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u591a\u4e2a\u6a21\u5757\uff0c\u5305\u62ec\u53cc\u6295\u5f71\u878d\u5408\u3001\u53cc\u7f51\u683c\u4f53\u7d20\u5316\u3001\u8f7b\u91cf\u89e3\u7801\u5668\u548c\u8fd0\u52a8\u8865\u507f\uff0c\u4ee5\u5b9e\u73b0\u52a8\u6001\u591a\u5c3a\u5ea6\u878d\u5408\u548c\u957f\u8ddd\u79bb\u63a8\u7406\u3002", "result": "OneOcc\u5728QuadOcc\u4e0a\u51fb\u8d25\u4e86\u5f3a\u5927\u7684\u89c6\u89c9\u57fa\u7ebf\u548c\u6d41\u884c\u7684LiDAR\u65b9\u6cd5\uff0c\u5728H3O\u4e0a\u5206\u522b\u63d0\u5347\u4e86+3.83 mIoU\uff08\u57ce\u5e02\u5185\uff09\u548c+8.08\uff08\u8de8\u57ce\u5e02\uff09\u3002", "conclusion": "OneOcc\u5728QuadOcc\u548cH3O\u57fa\u51c6\u4e0a\u8bbe\u7f6e\u4e86\u65b0\u7684\u72b6\u6001-of-the-art\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5168\u65b9\u4f4d\u611f\u77e5\u4e2d\u7684\u6709\u6548\u6027\u4e0e\u8f7b\u91cf\u5316\u4f18\u52bf\u3002"}}
{"id": "2511.03585", "categories": ["cs.HC", "68T07", "H.5.2; J.5"], "pdf": "https://arxiv.org/pdf/2511.03585", "abs": "https://arxiv.org/abs/2511.03585", "authors": ["Jia Kaixin", "Zhu Kewen", "Deng Huanghuang", "Qiu Yiwu", "Ding Shiying", "Ding Chenyang", "Li Zejian"], "title": "Knowledge Graph for Intelligent Generation of Artistic Image Creation: Constructing a New Annotation Hierarchy", "comment": "24 pages, in Chinese language", "summary": "Our study aims to establish a unified, systematic, and referable knowledge\nframework for the annotation of art image datasets, addressing issues of\nambiguous definitions and inconsistent results caused by the lack of common\nstandards during the annotation process. To achieve this goal, a hierarchical\nand systematic art image knowledge graph was constructed. It was developed\nbased on the composition principles of art images, incorporating the Structured\nTheory of Visual Knowledge proposed by Academician Yunhe Pan in On Visual\nKnowledge-which states that visual knowledge must achieve precise expression of\nspatial forms and dynamic relationships through \"prototype-category\" and\n\"hierarchical structure\". Through in-depth review of Chinese and Western art\ntheories and pioneering integration of the Chinese cultural perspective, this\ngraph took shape. The core visual language of art images was deconstructed by\nthis knowledge graph. Meanwhile, the unique spatial theory and symbolic system\nof Chinese painting were compared with and supplemented by Western art\ntheories. This graph converts qualitative artistic concepts into a clear\nstructured framework. It not only conforms to the cognitive law that \"visual\nknowledge takes precedence over verbal knowledge\" in humans but also provides\nan interpretable and inferential visual knowledge foundation for AI art\ngeneration and cross-cultural art analysis. It ensures the high quality and\nconsistency of annotated data, thus offering key support for art intelligence\nresearch in the AI 2.0 era.", "AI": {"tldr": "\u672c\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u827a\u672f\u56fe\u50cf\u77e5\u8bc6\u56fe\u8c31\uff0c\u4ee5\u89e3\u51b3\u6807\u6ce8\u4e2d\u7684\u6a21\u7cca\u6027\u548c\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5e76\u4e3aAI\u827a\u672f\u751f\u6210\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u827a\u672f\u56fe\u50cf\u6570\u636e\u96c6\u6807\u6ce8\u4e2d\u5b58\u5728\u7684\u6a21\u7cca\u5b9a\u4e49\u548c\u4e0d\u4e00\u81f4\u7ed3\u679c\u95ee\u9898\uff0c\u7f3a\u4e4f\u7edf\u4e00\u6807\u51c6\u5bfc\u81f4\u7684\u6311\u6218\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5c42\u6b21\u5316\u3001\u7cfb\u7edf\u5316\u7684\u827a\u672f\u56fe\u50cf\u77e5\u8bc6\u56fe\u8c31\uff0c\u57fa\u4e8e\u827a\u672f\u56fe\u50cf\u7684\u6784\u6210\u539f\u7406\uff0c\u7ed3\u5408\u4e86\u4e2d\u56fd\u6587\u5316\u89c6\u89d2\u548c\u4e1c\u897f\u65b9\u827a\u672f\u7406\u8bba\u7684\u6df1\u5165\u590d\u4e60\u4e0e\u6bd4\u8f83\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u827a\u672f\u56fe\u50cf\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u6e05\u6670\u5730\u5c06\u5b9a\u6027\u827a\u672f\u6982\u5ff5\u8f6c\u5316\u4e3a\u6709\u7ed3\u6784\u7684\u6846\u67b6\uff0c\u5e76\u4fdd\u8bc1\u4e86\u6807\u6ce8\u6570\u636e\u7684\u9ad8\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u77e5\u8bc6\u56fe\u8c31\u4e3a\u827a\u672f\u56fe\u50cf\u6570\u636e\u96c6\u7684\u6807\u6ce8\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u53c2\u8003\u6846\u67b6\uff0c\u63d0\u5347\u4e86\u6570\u636e\u6807\u6ce8\u7684\u4e00\u81f4\u6027\u548c\u8d28\u91cf\uff0c\u540c\u65f6\u4e3a\u4eba\u5de5\u667a\u80fd\u827a\u672f\u751f\u6210\u548c\u8de8\u6587\u5316\u827a\u672f\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u89c6\u89c9\u77e5\u8bc6\u57fa\u7840\u3002"}}
{"id": "2511.03576", "categories": ["cs.RO", "cs.AI", "68T40", "I.2.9; I.2.4"], "pdf": "https://arxiv.org/pdf/2511.03576", "abs": "https://arxiv.org/abs/2511.03576", "authors": ["Aniol Civit", "Antonio Andriella", "Carles Sierra", "Guillem Aleny\u00e0"], "title": "Multi-User Personalisation in Human-Robot Interaction: Using Quantitative Bipolar Argumentation Frameworks for Preferences Conflict Resolution", "comment": "Preprint submitted to a journal", "summary": "While personalisation in Human-Robot Interaction (HRI) has advanced\nsignificantly, most existing approaches focus on single-user adaptation,\noverlooking scenarios involving multiple stakeholders with potentially\nconflicting preferences. To address this, we propose the Multi-User Preferences\nQuantitative Bipolar Argumentation Framework (MUP-QBAF), a novel multi-user\npersonalisation framework based on Quantitative Bipolar Argumentation\nFrameworks (QBAFs) that explicitly models and resolves multi-user preference\nconflicts. Unlike prior work in Argumentation Frameworks, which typically\nassumes static inputs, our approach is tailored to robotics: it incorporates\nboth users' arguments and the robot's dynamic observations of the environment,\nallowing the system to adapt over time and respond to changing contexts.\nPreferences, both positive and negative, are represented as arguments whose\nstrength is recalculated iteratively based on new information. The framework's\nproperties and capabilities are presented and validated through a realistic\ncase study, where an assistive robot mediates between the conflicting\npreferences of a caregiver and a care recipient during a frailty assessment\ntask. This evaluation further includes a sensitivity analysis of argument base\nscores, demonstrating how preference outcomes can be shaped by user input and\ncontextual observations. By offering a transparent, structured, and\ncontext-sensitive approach to resolving competing user preferences, this work\nadvances the field of multi-user HRI. It provides a principled alternative to\ndata-driven methods, enabling robots to navigate conflicts in real-world\nenvironments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u7528\u6237\u4e2a\u6027\u5316\u6846\u67b6MUP-QBAF\uff0c\u80fd\u6709\u6548\u89e3\u51b3\u4eba\u673a\u4ea4\u4e92\u4e2d\u591a\u4e2a\u7528\u6237\u4e4b\u95f4\u7684\u504f\u597d\u51b2\u7a81\uff0c\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002", "motivation": "\u5927\u591a\u6570\u73b0\u6709\u7684\u65b9\u6cd5\u4e13\u6ce8\u4e8e\u5355\u7528\u6237\u9002\u5e94\uff0c\u800c\u5ffd\u89c6\u4e86\u6d89\u53ca\u591a\u4e2a\u5229\u76ca\u76f8\u5173\u8005\u7684\u573a\u666f\uff0c\u53ef\u80fd\u5bfc\u81f4\u504f\u597d\u51b2\u7a81\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b9a\u91cf\u53cc\u6781\u8bba\u8bc1\u6846\u67b6\u7684\u591a\u7528\u6237\u4e2a\u6027\u5316\u6846\u67b6\uff08MUP-QBAF\uff09\uff0c\u80fd\u591f\u663e\u5f0f\u5efa\u6a21\u548c\u89e3\u51b3\u591a\u7528\u6237\u504f\u597d\u51b2\u7a81\u3002", "result": "\u901a\u8fc7\u73b0\u5b9e\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u6846\u67b6\u7684\u5c5e\u6027\u548c\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u52a9\u7406\u673a\u5668\u4eba\u5982\u4f55\u5728\u7167\u987e\u8005\u548c\u7167\u62a4\u5bf9\u8c61\u4e4b\u95f4\u8c03\u89e3\u51b2\u7a81\u7684\u504f\u597d\uff0c\u5e76\u8fdb\u884c\u4e86\u8bba\u8bc1\u57fa\u7840\u5206\u6570\u7684\u654f\u611f\u6027\u5206\u6790\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u900f\u660e\u3001\u7ed3\u6784\u5316\u4e14\u654f\u611f\u4e8e\u4e0a\u4e0b\u6587\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u591a\u4e2a\u7528\u6237\u504f\u597d\u7684\u51b2\u7a81\uff0c\u63a8\u52a8\u4e86\u591a\u7528\u6237\u4eba\u673a\u4ea4\u4e92\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2511.03673", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.03673", "abs": "https://arxiv.org/abs/2511.03673", "authors": ["Shubham Rohal", "Shijia Pan"], "title": "OriFeel: Origami-Inspired Actuation for Force-Based Tactile Feedback on Ambient Surfaces", "comment": null, "summary": "People are constantly in touch with surfaces in their lives, such as a sofa,\narmrest, and table, making them natural tactile interfaces. Despite the recent\nadvancements in shape-changing surfaces, current available solutions are often\nchallenging to retrofit into ambient surfaces due to their bulky form factor or\nhigh power requirements. We present \\name, a foldable structure-enabled tactile\nfeedback mechanism that leverages the structural properties of Miura-Ori fold\nto enable on-surface force actuation. The foldable structure allows the\nsurfaces to provide perpendicular force via lateral actuation, resulting in a\nslim form factor that can be actuated via cable-based design using a servo\nmotor. We evaluate the system with a real-world prototype and a user study. The\nuser study shows that users can effectively distinguish multiple intensity\nlevels.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMiura-Ori\u6298\u53e0\u7ed3\u6784\u7684\u89e6\u89c9\u53cd\u9988\u673a\u5236\uff0c\u5177\u6709\u7d27\u51d1\u7684\u8bbe\u8ba1\u548c\u826f\u597d\u7684\u7528\u6237\u8bc6\u522b\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5f62\u53d8\u8868\u9762\u6280\u672f\u96be\u4ee5\u5e94\u7528\u4e8e\u65e5\u5e38\u73af\u5883\uff0c\u56e0\u5176\u4f53\u79ef\u5e9e\u5927\u6216\u529f\u8017\u9ad8\uff0c\u8feb\u5207\u9700\u8981\u66f4\u7075\u6d3b\u4e14\u6613\u4e8e\u96c6\u6210\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7Miura-Ori\u6298\u53e0\u7ed3\u6784\u5b9e\u73b0\u7684\u89e6\u89c9\u53cd\u9988\u673a\u5236\uff0c\u4f7f\u7528\u4f3a\u670d\u7535\u673a\u548c\u7535\u7f06\u9a71\u52a8\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u6298\u53e0\u7684\u89e6\u89c9\u53cd\u9988\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u539f\u578b\u548c\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u7528\u6237\u80fd\u591f\u6709\u6548\u533a\u5206\u591a\u79cd\u5f3a\u5ea6\u6c34\u5e73\uff0c\u8868\u660e\u8be5\u89e6\u89c9\u53cd\u9988\u673a\u5236\u5177\u6709\u5b9e\u7528\u6027\u3002"}}
{"id": "2511.03591", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.03591", "abs": "https://arxiv.org/abs/2511.03591", "authors": ["Qingyi Chen", "Ruiqi Ni", "Jun Kim", "Ahmed H. Qureshi"], "title": "Manifold-constrained Hamilton-Jacobi Reachability Learning for Decentralized Multi-Agent Motion Planning", "comment": null, "summary": "Safe multi-agent motion planning (MAMP) under task-induced constraints is a\ncritical challenge in robotics. Many real-world scenarios require robots to\nnavigate dynamic environments while adhering to manifold constraints imposed by\ntasks. For example, service robots must carry cups upright while avoiding\ncollisions with humans or other robots. Despite recent advances in\ndecentralized MAMP for high-dimensional systems, incorporating manifold\nconstraints remains difficult. To address this, we propose a\nmanifold-constrained Hamilton-Jacobi reachability (HJR) learning framework for\ndecentralized MAMP. Our method solves HJR problems under manifold constraints\nto capture task-aware safety conditions, which are then integrated into a\ndecentralized trajectory optimization planner. This enables robots to generate\nmotion plans that are both safe and task-feasible without requiring assumptions\nabout other agents' policies. Our approach generalizes across diverse\nmanifold-constrained tasks and scales effectively to high-dimensional\nmulti-agent manipulation problems. Experiments show that our method outperforms\nexisting constrained motion planners and operates at speeds suitable for\nreal-world applications. Video demonstrations are available at\nhttps://youtu.be/RYcEHMnPTH8 .", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u9ad8\u6548\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u54c8\u5bc6\u987f-\u96c5\u53ef\u6bd4\u53ef\u8fbe\u6027\u5b66\u4e60\u6765\u5904\u7406\u6d41\u5f62\u7ea6\u675f\uff0c\u5b9e\u73b0\u5b89\u5168\u548c\u4efb\u52a1\u53ef\u884c\u7684\u8def\u5f84\u89c4\u5212\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u89c4\u5212\u9762\u4e34\u5728\u9075\u5faa\u4efb\u52a1\u5f15\u53d1\u7684\u590d\u6742\u7ea6\u675f\u7684\u540c\u65f6\u5b89\u5168\u5bfc\u822a\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u6d41\u5f62\u7ea6\u675f\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6d41\u5f62\u7ea6\u675f\u7684\u54c8\u5bc6\u987f-\u96c5\u53ef\u6bd4\u53ef\u8fbe\u6027\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u5206\u6563\u7684\u8f68\u8ff9\u4f18\u5316\u89c4\u5212\u5668\uff0c\u89e3\u51b3\u4e86\u6d41\u5f62\u7ea6\u675f\u4e0b\u7684\u54c8\u5bc6\u987f-\u96c5\u53ef\u6bd4\u95ee\u9898\uff0c\u4ee5\u6355\u83b7\u4efb\u52a1\u611f\u77e5\u7684\u5b89\u5168\u6761\u4ef6\u3002", "result": "\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4efb\u52a1\u7ea6\u675f\u7684\u8fd0\u52a8\u89c4\u5212\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u7ea6\u675f\u89c4\u5212\u5668\uff0c\u901f\u5ea6\u9002\u5408\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u9002\u5e94\u591a\u4efb\u52a1\u7ea6\u675f\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u5b89\u5168\u8def\u5f84\u89c4\u5212\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u5ea6\u7684\u591a\u667a\u80fd\u4f53\u64cd\u4f5c\u95ee\u9898\uff0c\u5e76\u4e14\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u7684\u7ea6\u675f\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u3002"}}
{"id": "2511.03622", "categories": ["cs.RO", "cs.CG", "cs.CR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.03622", "abs": "https://arxiv.org/abs/2511.03622", "authors": ["Swadhin Agrawal", "Sujoy Bhore", "Joseph S. B. Mitchell", "P. B. Sujit", "Aayush Gohil"], "title": "Multi-robot searching with limited sensing range for static and mobile intruders", "comment": null, "summary": "We consider the problem of searching for an intruder in a geometric domain by\nutilizing multiple search robots. The domain is a simply connected orthogonal\npolygon with edges parallel to the cartesian coordinate axes. Each robot has a\nlimited sensing capability. We study the problem for both static and mobile\nintruders. It turns out that the problem of finding an intruder is NP-hard,\neven for a stationary intruder. Given this intractability, we turn our\nattention towards developing efficient and robust algorithms, namely methods\nbased on space-filling curves, random search, and cooperative random search.\nMoreover, for each proposed algorithm, we evaluate the trade-off between the\nnumber of search robots and the time required for the robots to complete the\nsearch process while considering the geometric properties of the connected\northogonal search area.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4f7f\u7528\u591a\u641c\u7d22\u673a\u5668\u4eba\u5728\u51e0\u4f55\u57df\u4e2d\u5bfb\u627e\u5165\u4fb5\u8005\u7684\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u6709\u6548\u7684\u641c\u7d22\u7b97\u6cd5\u4ee5\u5e94\u5bf9NP\u96be\u5ea6\u3002", "motivation": "\u5728\u7b80\u5355\u8fde\u901a\u7684\u6b63\u4ea4\u591a\u8fb9\u5f62\u4e2d\u4f7f\u7528\u591a\u673a\u5668\u4eba\u641c\u7d22\u5165\u4fb5\u8005\uff0c\u89e3\u51b3\u6f5c\u5728\u7684NP-\u96be\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u57fa\u4e8e\u7a7a\u95f4\u586b\u5145\u66f2\u7ebf\u3001\u968f\u673a\u641c\u7d22\u548c\u5408\u4f5c\u968f\u673a\u641c\u7d22\u7684\u7b97\u6cd5\u6765\u5bfb\u627e\u5165\u4fb5\u8005\u3002", "result": "\u627e\u5230\u6709\u6548\u7b97\u6cd5\u7684\u540c\u65f6\uff0c\u8bc4\u4f30\u4e86\u641c\u7d22\u673a\u5668\u4eba\u6570\u91cf\u4e0e\u5b8c\u6210\u641c\u7d22\u8fc7\u7a0b\u6240\u9700\u65f6\u95f4\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "\u901a\u8fc7\u7a7a\u95f4\u586b\u5145\u66f2\u7ebf\u3001\u968f\u673a\u641c\u7d22\u548c\u5408\u4f5c\u968f\u673a\u641c\u7d22\u7684\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u6709\u6548\u4e14\u7a33\u5065\u7684\u7b97\u6cd5\u6765\u89e3\u51b3\u5728\u51e0\u4f55\u57df\u4e2d\u5bfb\u627e\u5165\u4fb5\u8005\u7684\u95ee\u9898\u3002"}}
{"id": "2511.03651", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY", "I.2.9; J.5"], "pdf": "https://arxiv.org/pdf/2511.03651", "abs": "https://arxiv.org/abs/2511.03651", "authors": ["Andrei A. Korigodskii", "Oleg D. Kalachev", "Artem E. Vasiunik", "Matvei V. Urvantsev", "Georgii E. Bondar"], "title": "Flying Robotics Art: ROS-based Drone Draws the Record-Breaking Mural", "comment": null, "summary": "This paper presents the innovative design and successful deployment of a\npioneering autonomous unmanned aerial system developed for executing the\nworld's largest mural painted by a drone. Addressing the dual challenges of\nmaintaining artistic precision and operational reliability under adverse\noutdoor conditions such as wind and direct sunlight, our work introduces a\nrobust system capable of navigating and painting outdoors with unprecedented\naccuracy. Key to our approach is a novel navigation system that combines an\ninfrared (IR) motion capture camera and LiDAR technology, enabling precise\nlocation tracking tailored specifically for largescale artistic applications.\nWe employ a unique control architecture that uses different regulation in\ntangential and normal directions relative to the planned path, enabling precise\ntrajectory tracking and stable line rendering. We also present algorithms for\ntrajectory planning and path optimization, allowing for complex curve drawing\nand area filling. The system includes a custom-designed paint spraying\nmechanism, specifically engineered to function effectively amidst the turbulent\nairflow generated by the drone's propellers, which also protects the drone's\ncritical components from paint-related damage, ensuring longevity and\nconsistent performance. Experimental results demonstrate the system's\nrobustness and precision in varied conditions, showcasing its potential for\nautonomous large-scale art creation and expanding the functional applications\nof robotics in creative fields.", "AI": {"tldr": "\u672c\u6587\u5c55\u793a\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u81ea\u4e3b\u65e0\u4eba\u673a\u7cfb\u7edf\uff0c\u7528\u4e8e\u7ed8\u5236\u4e16\u754c\u4e0a\u6700\u5927\u7684\u58c1\u753b\uff0c\u514b\u670d\u4e86\u827a\u672f\u7cbe\u5ea6\u548c\u64cd\u4f5c\u53ef\u9760\u6027\u7684\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u5728\u6237\u5916\u6076\u52a3\u6761\u4ef6\u4e0b\u7ef4\u6301\u827a\u672f\u7cbe\u5ea6\u548c\u64cd\u4f5c\u53ef\u9760\u6027\u7684\u6311\u6218\uff0c\u63a8\u8fdb\u65e0\u4eba\u673a\u827a\u672f\u521b\u4f5c\u7684\u80fd\u529b\u3002", "method": "\u91c7\u7528\u7ea2\u5916\u8fd0\u52a8\u6355\u6349\u6444\u50cf\u5934\u548cLiDAR\u6280\u672f\u76f8\u7ed3\u5408\u7684\u5bfc\u822a\u7cfb\u7edf\uff0c\u72ec\u7279\u7684\u63a7\u5236\u67b6\u6784\uff0c\u8f68\u8ff9\u89c4\u5212\u548c\u8def\u5f84\u4f18\u5316\u7b97\u6cd5\uff0c\u4ee5\u53ca\u5b9a\u5236\u7684\u55b7\u6f06\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u7cfb\u7edf\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5448\u73b0\u51fa\u5353\u8d8a\u7684\u7cbe\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u548c\u7cbe\u786e\u6027\uff0c\u5177\u6709\u81ea\u6211\u521b\u9020\u5927\u578b\u827a\u672f\u7684\u6f5c\u529b\uff0c\u62d3\u5c55\u4e86\u673a\u5668\u4eba\u5728\u521b\u610f\u9886\u57df\u7684\u529f\u80fd\u5e94\u7528\u3002"}}
{"id": "2511.03676", "categories": ["cs.RO", "cs.HC", "H.5.2; I.2.9"], "pdf": "https://arxiv.org/pdf/2511.03676", "abs": "https://arxiv.org/abs/2511.03676", "authors": ["Taito Tashiro", "Tomoko Yonezawa", "Hirotake Yamazoe"], "title": "Unconscious and Intentional Human Motion Cues for Expressive Robot-Arm Motion Design", "comment": "5 pages, 5 figures, HAI2025 Workshop on Socially Aware and\n  Cooperative Intelligent Systems", "summary": "This study investigates how human motion cues can be used to design\nexpressive robot-arm movements. Using the imperfect-information game Geister,\nwe analyzed two types of human piece-moving motions: natural gameplay\n(unconscious tendencies) and instructed expressions (intentional cues). Based\non these findings, we created phase-specific robot motions by varying movement\nspeed and stop duration, and evaluated observer impressions under two\npresentation modalities: a physical robot and a recorded video. Results\nindicate that late-phase motion timing, particularly during withdrawal, plays\nan important role in impression formation and that physical embodiment enhances\nthe interpretability of motion cues. These findings provide insights for\ndesigning expressive robot motions based on human timing behavior.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u4eba\u7c7b\u8fd0\u52a8\u7ebf\u7d22\u8bbe\u8ba1\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u53d1\u73b0\u540e\u671f\u8fd0\u52a8\u65f6\u5e8f\u548c\u7269\u7406\u5448\u73b0\u5bf9\u89c2\u4f17\u5370\u8c61\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u63a2\u8ba8\u5982\u4f55\u5229\u7528\u4eba\u7c7b\u8fd0\u52a8\u7ebf\u7d22\u6765\u8bbe\u8ba1\u5bcc\u6709\u8868\u73b0\u529b\u7684\u673a\u5668\u4eba\u624b\u81c2\u8fd0\u52a8\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e0d\u5b8c\u5168\u4fe1\u606f\u6e38\u620fGeister\u4e2d\u7684\u4e24\u79cd\u4eba\u7c7b\u79fb\u52a8\u68cb\u5b50\u7684\u65b9\u5f0f\uff0c\u521b\u5efa\u4e86\u57fa\u4e8e\u8fd0\u52a8\u901f\u5ea6\u548c\u505c\u987f\u65f6\u957f\u7684\u9636\u6bb5\u7279\u5b9a\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u5e76\u8bc4\u4f30\u4e86\u89c2\u5bdf\u8005\u5728\u7269\u7406\u673a\u5668\u4eba\u548c\u5f55\u50cf\u5448\u73b0\u4e0b\u7684\u5370\u8c61\u3002", "result": "\u540e\u671f\u8fd0\u52a8\u65f6\u5e8f\u5728\u5f62\u8c61\u5f62\u6210\u4e2d\u8d77\u7740\u91cd\u8981\u4f5c\u7528\uff0c\u7269\u7406\u4f53\u73b0\u589e\u5f3a\u4e86\u8fd0\u52a8\u7ebf\u7d22\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u7269\u7406\u4f53\u73b0\u589e\u5f3a\u4e86\u8fd0\u52a8\u7ebf\u7d22\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u4e14\u540e\u671f\u8fd0\u52a8\u65f6\u5e8f\u5728\u5f62\u8c61\u5f62\u6210\u4e2d\u626e\u6f14\u4e86\u91cd\u8981\u89d2\u8272\u3002"}}
{"id": "2511.03652", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.03652", "abs": "https://arxiv.org/abs/2511.03652", "authors": ["Azizollah Taheri", "Derya Aksaray"], "title": "Motion Planning Under Temporal Logic Specifications In Semantically Unknown Environments", "comment": "8 pages, 6 figures", "summary": "This paper addresses a motion planning problem to achieve\nspatio-temporal-logical tasks, expressed by syntactically co-safe linear\ntemporal logic specifications (scLTL\\next), in uncertain environments. Here,\nthe uncertainty is modeled as some probabilistic knowledge on the semantic\nlabels of the environment. For example, the task is \"first go to region 1, then\ngo to region 2\"; however, the exact locations of regions 1 and 2 are not known\na priori, instead a probabilistic belief is available. We propose a novel\nautomata-theoretic approach, where a special product automaton is constructed\nto capture the uncertainty related to semantic labels, and a reward function is\ndesigned for each edge of this product automaton. The proposed algorithm\nutilizes value iteration for online replanning. We show some theoretical\nresults and present some simulations/experiments to demonstrate the efficacy of\nthe proposed approach.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u57fa\u4e8e\u8bed\u4e49\u6807\u7b7e\u8fdb\u884c\u6709\u6548\u7684\u4efb\u52a1\u89c4\u5212\u3002", "motivation": "\u4efb\u52a1\u662f\u5728\u4e0d\u786e\u5b9a\u7684\u73af\u5883\u4e2d\u5b9e\u73b0\u7a7a\u95f4-\u65f6\u95f4-\u903b\u8f91\u4efb\u52a1\uff0c\u800c\u73af\u5883\u4e2d\u8bed\u4e49\u6807\u7b7e\u7684\u786e\u5207\u4f4d\u7f6e\u4e0d\u660e\u786e\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u7279\u6b8a\u7684\u4ea7\u54c1\u81ea\u52a8\u673a\u6765\u6355\u6349\u4e0e\u8bed\u4e49\u6807\u7b7e\u76f8\u5173\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u4e3a\u8be5\u81ea\u52a8\u673a\u7684\u6bcf\u4e2a\u8fb9\u8bbe\u8ba1\u4e86\u5956\u52b1\u51fd\u6570\uff0c\u4f7f\u7528\u4ef7\u503c\u8fed\u4ee3\u8fdb\u884c\u5728\u7ebf\u91cd\u65b0\u89c4\u5212\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u7ed3\u679c\u548c\u5b9e\u9a8c\u6a21\u62df\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u81ea\u52a8\u673a\u7406\u8bba\u65b9\u6cd5\u6709\u6548\u5730\u89e3\u51b3\u4e86\u4e0d\u786e\u5b9a\u73af\u5883\u4e0b\u7684\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\u3002"}}
{"id": "2511.03691", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.03691", "abs": "https://arxiv.org/abs/2511.03691", "authors": ["Zhihang Qin", "Yueheng Zhang", "Wan Su", "Linxin Hou", "Shenghao Zhou", "Zhijun Chen", "Yu Jun Tan", "Cecilia Laschi"], "title": "Source-Free Bistable Fluidic Gripper for Size-Selective and Stiffness-Adaptive Grasping", "comment": null, "summary": "Conventional fluid-driven soft grippers typically depend on external sources,\nwhich limit portability and long-term autonomy. This work introduces a\nself-contained soft gripper with fixed size that operates solely through\ninternal liquid redistribution among three interconnected bistable snap-through\nchambers. When the top sensing chamber deforms upon contact, the displaced\nliquid triggers snap-through expansion of the grasping chambers, enabling\nstable and size-selective grasping without continuous energy input. The\ninternal hydraulic feedback further allows passive adaptation of gripping\npressure to object stiffness. This source-free and compact design opens new\npossibilities for lightweight, stiffness-adaptive fluid-driven manipulation in\nsoft robotics, providing a feasible approach for targeted size-specific\nsampling and operation in underwater and field environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u81ea\u4f9b\u80fd\u8f6f\u6293\u624b\uff0c\u901a\u8fc7\u5185\u90e8\u6db2\u4f53\u91cd\u65b0\u5206\u914d\u5b9e\u73b0\u6293\u53d6\uff0c\u5177\u6709\u826f\u597d\u7684\u4fbf\u643a\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u6d41\u4f53\u9a71\u52a8\u8f6f\u6293\u624b\u4f9d\u8d56\u5916\u90e8\u7535\u6e90\uff0c\u9650\u5236\u4e86\u5176\u4fbf\u643a\u6027\u548c\u957f\u671f\u81ea\u4e3b\u6027\u3002", "method": "\u901a\u8fc7\u5185\u90e8\u6db2\u4f53\u91cd\u65b0\u5206\u914d\u5728\u4e09\u4e2a\u4e92\u8fde\u7684\u53cc\u7a33\u6001\u5feb\u5207\u8154\u5ba4\u4e2d\u64cd\u4f5c\uff0c\u5229\u7528\u5185\u7f6e\u6db2\u538b\u53cd\u9988\u5b9e\u73b0\u81ea\u4e3b\u9a71\u52a8\u3002", "result": "\u5f00\u53d1\u51fa\u4e00\u79cd\u72ec\u7acb\u7684\u3001\u56fa\u5b9a\u5c3a\u5bf8\u7684\u8f6f\u6293\u624b\uff0c\u80fd\u591f\u5728\u4e0d\u9700\u8981\u6301\u7eed\u80fd\u91cf\u8f93\u5165\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u7a33\u5b9a\u548c\u5c3a\u5bf8\u9009\u62e9\u6027\u7684\u6293\u53d6\u3002", "conclusion": "\u8be5\u8bbe\u8ba1\u4e3a\u67d4\u6027\u673a\u5668\u4eba\u5728\u6c34\u4e0b\u548c\u73b0\u573a\u73af\u5883\u4e2d\u7684\u7279\u5b9a\u5c3a\u5bf8\u62bd\u6837\u548c\u64cd\u4f5c\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u65b9\u6cd5\u3002"}}
