{"id": "2510.06222", "categories": ["cs.HC", "econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2510.06222", "abs": "https://arxiv.org/abs/2510.06222", "authors": ["Ziv Ben-Zion", "Zohar Elyoseph", "Tobias Spiller", "Teddy Lazebnik"], "title": "Inducing State Anxiety in LLM Agents Reproduces Human-Like Biases in Consumer Decision-Making", "comment": "Manuscript Main Text - 20 pages, including 3 Figures and 1 Table.\n  Supplementary Materials - 10 pages, including 4 Supplemental Tables", "summary": "Large language models (LLMs) are rapidly evolving from text generators to\nautonomous agents, raising urgent questions about their reliability in\nreal-world contexts. Stress and anxiety are well known to bias human\ndecision-making, particularly in consumer choices. Here, we tested whether LLM\nagents exhibit analogous vulnerabilities. Three advanced models (ChatGPT-5,\nGemini 2.5, Claude 3.5-Sonnet) performed a grocery shopping task under budget\nconstraints (24, 54, 108 USD), before and after exposure to anxiety-inducing\ntraumatic narratives. Across 2,250 runs, traumatic prompts consistently reduced\nthe nutritional quality of shopping baskets (Change in Basket Health Scores of\n-0.081 to -0.126; all pFDR<0.001; Cohens d=-1.07 to -2.05), robust across\nmodels and budgets. These results show that psychological context can\nsystematically alter not only what LLMs generate but also the actions they\nperform. By reproducing human-like emotional biases in consumer behavior, LLM\nagents reveal a new class of vulnerabilities with implications for digital\nhealth, consumer safety, and ethical AI deployment."}
{"id": "2510.06223", "categories": ["cs.HC", "cs.AI", "I.2.7; D.2.11"], "pdf": "https://arxiv.org/pdf/2510.06223", "abs": "https://arxiv.org/abs/2510.06223", "authors": ["Hans G. W. van Dam"], "title": "A Multimodal GUI Architecture for Interfacing with LLM-Based Conversational Assistants", "comment": "24 pages, 19 figures", "summary": "Advances in large language models (LLMs) and real-time speech recognition now\nmake it possible to issue any graphical user interface (GUI) action through\nnatural language and receive the corresponding system response directly through\nthe GUI. Most production applications were never designed with speech in mind.\nThis article provides a concrete architecture that enables GUIs to interface\nwith LLM-based speech-enabled assistants.\n  The architecture makes an application's navigation graph and semantics\navailable through the Model Context Protocol (MCP). The ViewModel, part of the\nMVVM (Model-View-ViewModel) pattern, exposes the application's capabilities to\nthe assistant by supplying both tools applicable to a currently visible view\nand application-global tools extracted from the GUI tree router. This\narchitecture facilitates full voice accessibility while ensuring reliable\nalignment between spoken input and the visual interface, accompanied by\nconsistent feedback across modalities. It future-proofs apps for upcoming OS\nsuper assistants that employ computer use agents (CUAs) and natively consume\nMCP if an application provides it.\n  To address concerns about privacy and data security, the practical\neffectiveness of locally deployable, open-weight LLMs for speech-enabled\nmultimodal UIs is evaluated. Findings suggest that recent smaller open-weight\nmodels approach the performance of leading proprietary models in overall\naccuracy and require enterprise-grade hardware for fast responsiveness."}
{"id": "2510.06224", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.06224", "abs": "https://arxiv.org/abs/2510.06224", "authors": ["Suchismita Naik", "Austin L. Toombs", "Amanda Snellinger", "Scott Saponas", "Amanda K. Hall"], "title": "Exploring Human-AI Collaboration Using Mental Models of Early Adopters of Multi-Agent Generative AI Tools", "comment": "19 pages, 1 table, 2 figures", "summary": "With recent advancements in multi-agent generative AI (Gen AI), technology\norganizations like Microsoft are adopting these complex tools, redefining AI\nagents as active collaborators in complex workflows rather than as passive\ntools. In this study, we investigated how early adopters and developers\nconceptualize multi-agent Gen AI tools, focusing on how they understand\nhuman-AI collaboration mechanisms, general collaboration dynamics, and\ntransparency in the context of AI tools. We conducted semi-structured\ninterviews with 13 developers, all early adopters of multi-agent Gen AI\ntechnology who work at Microsoft. Our findings revealed that these early\nadopters conceptualize multi-agent systems as \"teams\" of specialized role-based\nand task-based agents, such as assistants or reviewers, structured similar to\nhuman collaboration models and ranging from AI-dominant to AI-assisted,\nuser-controlled interactions. We identified key challenges, including error\npropagation, unpredictable and unproductive agent loop behavior, and the need\nfor clear communication to mitigate the layered transparency issues. Early\nadopters' perspectives about the role of transparency underscored its\nimportance as a way to build trust, verify and trace errors, and prevent\nmisuse, errors, and leaks. The insights and design considerations we present\ncontribute to CSCW research about collaborative mechanisms with capabilities\nranging from AI-dominant to AI-assisted interactions, transparency and\noversight strategies in human-agent and agent-agent interactions, and how\nhumans make sense of these multi-agent systems as dynamic, role-diverse\ncollaborators which are customizable for diverse needs and workflows. We\nconclude with future research directions that extend CSCW approaches to the\ndesign of inter-agent and human mediation interactions."}
{"id": "2510.06306", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.06306", "abs": "https://arxiv.org/abs/2510.06306", "authors": ["Jaemarie Solyst", "Chloe Fong", "Faisal Nurdin", "Rotem Landesman", "R. Benjamin Shapiro"], "title": "\"Grillz on a hijabi\": Intersectional Identities in Fostering Critical AI Literacy", "comment": null, "summary": "As AI increasingly saturates our daily lives, it is crucial that youth\ndevelop skills to critically use and assess AI systems and envision better\nalternatives. We apply theories from culturally responsive computing to design\nand study a learning experience meant to support Black Muslim teen girls in\ndeveloping critical literacy with generative AI (GenAI). We investigate fashion\ndesign as a culturally-rich, creative domain for youth to apply GenAI and then\nreflect on GenAI's socio-ethical aspects in relation to their own\nintersectional identities. Through a case study of a three-day, voluntary\ninformal education program, we show how fashion design with GenAI exposed\naffordances and limitations of current GenAI tools. As the girls used GenAI to\ncreate realistic depictions of their dream fashion collections, they\nencountered socio-ethical limitations of AI, such as biased models and\nmalfunctioning safety systems that prohibited their generation of outputs that\nreflected their creative ideas, bodies, and cultures. Discussions anchored in\nthe phenomenology of impossible creative realization supported participants'\ndevelopment of critical AI literacy and descriptions of how preferable,\nidentity-affirming technologies would behave. Our findings contribute to the\nfield's growing understanding of how computing education experience designs\nlinking creativity and identity can support critical AI literacy development."}
{"id": "2510.06339", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06339", "abs": "https://arxiv.org/abs/2510.06339", "authors": ["Leiyao Cui", "Zihang Zhao", "Sirui Xie", "Wenhuan Zhang", "Zhi Han", "Yixin Zhu"], "title": "Vi-TacMan: Articulated Object Manipulation via Vision and Touch", "comment": null, "summary": "Autonomous manipulation of articulated objects remains a fundamental\nchallenge for robots in human environments. Vision-based methods can infer\nhidden kinematics but can yield imprecise estimates on unfamiliar objects.\nTactile approaches achieve robust control through contact feedback but require\naccurate initialization. This suggests a natural synergy: vision for global\nguidance, touch for local precision. Yet no framework systematically exploits\nthis complementarity for generalized articulated manipulation. Here we present\nVi-TacMan, which uses vision to propose grasps and coarse directions that seed\na tactile controller for precise execution. By incorporating surface normals as\ngeometric priors and modeling directions via von Mises-Fisher distributions,\nour approach achieves significant gains over baselines (all p<0.0001).\nCritically, manipulation succeeds without explicit kinematic models -- the\ntactile controller refines coarse visual estimates through real-time contact\nregulation. Tests on more than 50,000 simulated and diverse real-world objects\nconfirm robust cross-category generalization. This work establishes that coarse\nvisual cues suffice for reliable manipulation when coupled with tactile\nfeedback, offering a scalable paradigm for autonomous systems in unstructured\nenvironments."}
{"id": "2510.06452", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.06452", "abs": "https://arxiv.org/abs/2510.06452", "authors": ["Jinsheng Ba", "Sverrir Thorgeirsson", "Zhendong Su"], "title": "Code Semantic Zooming", "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have introduced a new\nparadigm for software development, where source code is generated directly from\nnatural language prompts. While this paradigm significantly boosts development\nproductivity, building complex, real-world software systems remains challenging\nbecause natural language offers limited control over the generated code.\nInspired by the historical evolution of programming languages toward higher\nlevels of abstraction, we advocate for a high-level abstraction language that\ngives developers greater control over LLM-assisted code writing. To this end,\nwe propose Code Semantic Zooming, a novel approach based on pseudocode that\nallows developers to iteratively explore, understand, and refine code across\nmultiple layers of semantic abstraction. We implemented Code Semantic Zooming\nas a VS Code extension and demonstrated its effectiveness through two\nreal-world case studies."}
{"id": "2510.06351", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06351", "abs": "https://arxiv.org/abs/2510.06351", "authors": ["Kaleb Ben Naveed", "Devansh R. Agrawal", "Dimitra Panagou"], "title": "A Formal gatekeeper Framework for Safe Dual Control with Active Exploration", "comment": "Submitted to American Control Conference (ACC) 2026", "summary": "Planning safe trajectories under model uncertainty is a fundamental\nchallenge. Robust planning ensures safety by considering worst-case\nrealizations, yet ignores uncertainty reduction and leads to overly\nconservative behavior. Actively reducing uncertainty on-the-fly during a\nnominal mission defines the dual control problem. Most approaches address this\nby adding a weighted exploration term to the cost, tuned to trade off the\nnominal objective and uncertainty reduction, but without formal consideration\nof when exploration is beneficial. Moreover, safety is enforced in some methods\nbut not in others. We propose a framework that integrates robust planning with\nactive exploration under formal guarantees as follows: The key innovation and\ncontribution is that exploration is pursued only when it provides a verifiable\nimprovement without compromising safety. To achieve this, we utilize our\nearlier work on gatekeeper as an architecture for safety verification, and\nextend it so that it generates both safe and informative trajectories that\nreduce uncertainty and the cost of the mission, or keep it within a\nuser-defined budget. The methodology is evaluated via simulation case studies\non the online dual control of a quadrotor under parametric uncertainty."}
{"id": "2510.06457", "categories": ["cs.HC", "cs.AI", "H.5.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.06457", "abs": "https://arxiv.org/abs/2510.06457", "authors": ["Lifei Wang", "Natalie Friedman", "Chengchao Zhu", "Zeshu Zhu", "S. Joy Mountford"], "title": "Evaluating Node-tree Interfaces for AI Explainability", "comment": "5 pages, 2 figures. Accepted to the 3rd Workshop on Explainability in\n  Human-Robot Collaboration: Real-World Concerns (XHRI 2025), scheduled for\n  March 3, 2025, Hybrid (Melbourne and online) as part of HRI 2025", "summary": "As large language models (LLMs) become ubiquitous in workplace tools and\ndecision-making processes, ensuring explainability and fostering user trust are\ncritical. Although advancements in LLM engineering continue, human-centered\ndesign is still catching up, particularly when it comes to embedding\ntransparency and trust into AI interfaces. This study evaluates user\nexperiences with two distinct AI interfaces - node-tree interfaces and chatbot\ninterfaces - to assess their performance in exploratory, follow-up inquiry,\ndecision-making, and problem-solving tasks. Our design-driven approach\nintroduces a node-tree interface that visually structures AI-generated\nresponses into hierarchically organized, interactive nodes, allowing users to\nnavigate, refine, and follow up on complex information. In a comparative study\nwith n=20 business users, we observed that while the chatbot interface\neffectively supports linear, step-by-step queries, it is the node-tree\ninterface that enhances brainstorming. Quantitative and qualitative findings\nindicate that node-tree interfaces not only improve task performance and\ndecision-making support but also promote higher levels of user trust by\npreserving context. Our findings suggest that adaptive AI interfaces capable of\nswitching between structured visualizations and conversational formats based on\ntask requirements can significantly enhance transparency and user confidence in\nAI-powered systems. This work contributes actionable insights to the fields of\nhuman-robot interaction and AI design, particularly for enterprise applications\nwhere trust-building is critical for teams."}
{"id": "2510.06357", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06357", "abs": "https://arxiv.org/abs/2510.06357", "authors": ["Grayson Byrd", "Corban Rivera", "Bethany Kemp", "Meghan Booker", "Aurora Schmidt", "Celso M de Melo", "Lalithkumar Seenivasan", "Mathias Unberath"], "title": "Constrained Natural Language Action Planning for Resilient Embodied Systems", "comment": null, "summary": "Replicating human-level intelligence in the execution of embodied tasks\nremains challenging due to the unconstrained nature of real-world environments.\nNovel use of large language models (LLMs) for task planning seeks to address\nthe previously intractable state/action space of complex planning tasks, but\nhallucinations limit their reliability, and thus, viability beyond a research\ncontext. Additionally, the prompt engineering required to achieve adequate\nsystem performance lacks transparency, and thus, repeatability. In contrast to\nLLM planning, symbolic planning methods offer strong reliability and\nrepeatability guarantees, but struggle to scale to the complexity and ambiguity\nof real-world tasks. We introduce a new robotic planning method that augments\nLLM planners with symbolic planning oversight to improve reliability and\nrepeatability, and provide a transparent approach to defining hard constraints\nwith considerably stronger clarity than traditional prompt engineering.\nImportantly, these augmentations preserve the reasoning capabilities of LLMs\nand retain impressive generalization in open-world environments. We demonstrate\nour approach in simulated and real-world environments. On the ALFWorld planning\nbenchmark, our approach outperforms current state-of-the-art methods, achieving\na near-perfect 99% success rate. Deployment of our method to a real-world\nquadruped robot resulted in 100% task success compared to 50% and 30% for pure\nLLM and symbolic planners across embodied pick and place tasks. Our approach\npresents an effective strategy to enhance the reliability, repeatability and\ntransparency of LLM-based robot planners while retaining their key strengths:\nflexibility and generalizability to complex real-world environments. We hope\nthat this work will contribute to the broad goal of building resilient embodied\nintelligent systems."}
{"id": "2510.06472", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.06472", "abs": "https://arxiv.org/abs/2510.06472", "authors": ["Richard Rhodes", "Sandra Woolley"], "title": "Back to the Future Museum -- Speculative Design for Virtual Citizen-Curated Museums", "comment": "5 pages, 6 Figures, PREPRINT - Submitted to 38th International BCS\n  Human-Computer Interaction Conference 2025", "summary": "This forward-looking paper uses speculative design fiction to explore future\nmuseum scenarios where citizen curators design and share immersive virtual\nreality museums populated with tangible heritage artefacts, intangible virtual\nelements and interactive experiences. The work also explores takeaway 'asset\npacks' containing 3D artefact models, curation assets, and interactive\nexperiences, and we envisage a visit to the future museum, where the physical\nand virtual experiences interplay. Finally, the paper considers the\nimplications of this future museum in terms of resources and the potential\nimpacts on traditional museums."}
{"id": "2510.06481", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06481", "abs": "https://arxiv.org/abs/2510.06481", "authors": ["Amirhossein Mollaei Khass", "Guangyi Liu", "Vivek Pandey", "Wen Jiang", "Boshu Lei", "Kostas Daniilidis", "Nader Motee"], "title": "Active Next-Best-View Optimization for Risk-Averse Path Planning", "comment": null, "summary": "Safe navigation in uncertain environments requires planning methods that\nintegrate risk aversion with active perception. In this work, we present a\nunified framework that refines a coarse reference path by constructing\ntail-sensitive risk maps from Average Value-at-Risk statistics on an\nonline-updated 3D Gaussian-splat Radiance Field. These maps enable the\ngeneration of locally safe and feasible trajectories. In parallel, we formulate\nNext-Best-View (NBV) selection as an optimization problem on the SE(3) pose\nmanifold, where Riemannian gradient descent maximizes an expected information\ngain objective to reduce uncertainty most critical for imminent motion. Our\napproach advances the state-of-the-art by coupling risk-averse path refinement\nwith NBV planning, while introducing scalable gradient decompositions that\nsupport efficient online updates in complex environments. We demonstrate the\neffectiveness of the proposed framework through extensive computational\nstudies."}
{"id": "2510.06480", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.06480", "abs": "https://arxiv.org/abs/2510.06480", "authors": ["Ziming Wang", "Shiwei Yang", "Rebecca Currano", "Morten Fjeld", "David Sirkin"], "title": "AI Eyes on the Road: Cross-Cultural Perspectives on Traffic Surveillance", "comment": null, "summary": "AI-powered road surveillance systems are increasingly proposed to monitor\ninfractions such as speeding, phone use, and jaywalking. While these systems\npromise to enhance safety by discouraging dangerous behaviors, they also raise\nconcerns about privacy, fairness, and potential misuse of personal data. Yet\nempirical research on how people perceive AI-enhanced monitoring of public\nspaces remains limited. We conducted an online survey ($N=720$) using a\n3$\\times$3 factorial design to examine perceptions of three road surveillance\nmodes -- conventional, AI-enhanced, and AI-enhanced with public shaming --\nacross China, Europe, and the United States. We measured perceived capability,\nrisk, transparency, and acceptance. Results show that conventional surveillance\nwas most preferred, while public shaming was least preferred across all\nregions. Chinese respondents, however, expressed significantly higher\nacceptance of AI-enhanced modes than Europeans or Americans. Our findings\nhighlight the need to account for context, culture, and social norms when\nconsidering AI-enhanced monitoring, as these shape trust, comfort, and overall\nacceptance."}
{"id": "2510.06492", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06492", "abs": "https://arxiv.org/abs/2510.06492", "authors": ["Matthew Kim", "Kensuke Nakamura", "Andrea Bajcsy"], "title": "What You Don't Know Can Hurt You: How Well do Latent Safety Filters Understand Partially Observable Safety Constraints?", "comment": "8 tables 6 figures", "summary": "Safe control techniques, such as Hamilton-Jacobi reachability, provide\nprincipled methods for synthesizing safety-preserving robot policies but\ntypically assume hand-designed state spaces and full observability. Recent work\nhas relaxed these assumptions via latent-space safe control, where state\nrepresentations and dynamics are learned jointly through world models that\nreconstruct future high-dimensional observations (e.g., RGB images) from\ncurrent observations and actions. This enables safety constraints that are\ndifficult to specify analytically (e.g., spilling) to be framed as\nclassification problems in latent space, allowing controllers to operate\ndirectly from raw observations. However, these methods assume that\nsafety-critical features are observable in the learned latent state. We ask:\nwhen are latent state spaces sufficient for safe control? To study this, we\nexamine temperature-based failures, comparable to overheating in cooking or\nmanufacturing tasks, and find that RGB-only observations can produce myopic\nsafety behaviors, e.g., avoiding seeing failure states rather than preventing\nfailure itself. To predict such behaviors, we introduce a mutual\ninformation-based measure that identifies when observations fail to capture\nsafety-relevant features. Finally, we propose a multimodal-supervised training\nstrategy that shapes the latent state with additional sensory inputs during\ntraining, but requires no extra modalities at deployment, and validate our\napproach in simulation and on hardware with a Franka Research 3 manipulator\npreventing a pot of wax from overheating."}
{"id": "2510.06507", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.06507", "abs": "https://arxiv.org/abs/2510.06507", "authors": ["Ziming Wang", "Yiqian Wu", "Qingxiao Zheng", "Shihan Zhang", "Ned Barker", "Morten Fjeld"], "title": "A Meat-Summer Night's Dream: A Tangible Design Fiction Exploration of Eating Biohybrid Flying Robots", "comment": null, "summary": "What if future dining involved eating robots? We explore this question\nthrough a playful and poetic experiential dinner theater: a tangible design\nfiction staged as a 2052 Paris restaurant where diners consume a biohybrid\nflying robot in place of the banned delicacy of ortolan bunting. Moving beyond\ntextual or visual speculation, our ``dinner-in-the-drama'' combined\nperformance, ritual, and multisensory immersion to provoke reflection on\nsustainability, ethics, and cultural identity. Six participants from creative\nindustries engaged as diners and role-players, responding with curiosity,\ndiscomfort, and philosophical debate. They imagined biohybrids as both\nplausible and unsettling -- raising questions of sentience, symbolism, and\ntechnology adoption that exceed conventional sustainability framings of\nsynthetic meat. Our contributions to HCI are threefold: (i) a speculative\nartifact that stages robots as food, (ii) empirical insights into how publics\nnegotiate cultural and ethical boundaries in post-natural eating, and (iii) a\nmethodological advance in embodied, multisensory design fiction."}
{"id": "2510.06518", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.06518", "abs": "https://arxiv.org/abs/2510.06518", "authors": ["Malakhi Hopkins", "Varun Murali", "Vijay Kumar", "Camillo J Taylor"], "title": "Real-Time Glass Detection and Reprojection using Sensor Fusion Onboard Aerial Robots", "comment": "8 pages, 8 figures, submitted to ICRA 2026", "summary": "Autonomous aerial robots are increasingly being deployed in real-world\nscenarios, where transparent obstacles present significant challenges to\nreliable navigation and mapping. These materials pose a unique problem for\ntraditional perception systems because they lack discernible features and can\ncause conventional depth sensors to fail, leading to inaccurate maps and\npotential collisions. To ensure safe navigation, robots must be able to\naccurately detect and map these transparent obstacles. Existing methods often\nrely on large, expensive sensors or algorithms that impose high computational\nburdens, making them unsuitable for low Size, Weight, and Power (SWaP) robots.\nIn this work, we propose a novel and computationally efficient framework for\ndetecting and mapping transparent obstacles onboard a sub-300g quadrotor. Our\nmethod fuses data from a Time-of-Flight (ToF) camera and an ultrasonic sensor\nwith a custom, lightweight 2D convolution model. This specialized approach\naccurately detects specular reflections and propagates their depth into\ncorresponding empty regions of the depth map, effectively rendering transparent\nobstacles visible. The entire pipeline operates in real-time, utilizing only a\nsmall fraction of a CPU core on an embedded processor. We validate our system\nthrough a series of experiments in both controlled and real-world environments,\ndemonstrating the utility of our method through experiments where the robot\nmaps indoor environments containing glass. Our work is, to our knowledge, the\nfirst of its kind to demonstrate a real-time, onboard transparent obstacle\nmapping system on a low-SWaP quadrotor using only the CPU."}
{"id": "2510.06537", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.06537", "abs": "https://arxiv.org/abs/2510.06537", "authors": ["Morgan McErlean", "Cella M. Sum", "Sukrit Venkatagiri", "Sarah Fox"], "title": "Examining Solidarity Against AI-Enabled Surveillance at the Intersection of Workplace and Carceral Realities", "comment": null, "summary": "As panoptical, AI-driven surveillance becomes a norm, everyone is impacted.\nIn a reality where all people fall victim to these technologies, establishing\nlinks and solidarity is essential to fighting back. Two groups facing rising\nand targeted surveillance are workers and individuals impacted by the carceral\nsystem. Through preliminary data collection from a worker-surveillance lens,\nour findings reveal several cases of these surveillance infrastructures\nintersecting. Continuation of our work will involve collecting cases from a\ncarceral-centered lens. Driven by a community-facing analysis of the overlap in\nthe AI-driven surveillance experienced by workers and individuals impacted by\nthe carceral system, we will facilitate discussions with restorative justice\nactivists around cultivating solidarity and empowerment focused on the\ninterconnected nature of workplace and carceral surveillance technologies."}
{"id": "2510.06546", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06546", "abs": "https://arxiv.org/abs/2510.06546", "authors": ["Mohammad Nazeri", "Sheldon Mei", "Jeffrey Watchorn", "Alex Zhang", "Erin Ng", "Tao Wen", "Abhijoy Mandal", "Kevin Golovin", "Alan Aspuru-Guzik", "Frank Gu"], "title": "RAISE: A self-driving laboratory for interfacial property formulation discovery", "comment": "Mohammad Nazeri, Sheldon Mei, and Jeffrey Watchorn contributed\n  equally to this work. *Corresponding author: Frank Gu (f.gu@utoronto.ca)", "summary": "Surface wettability is a critical design parameter for biomedical devices,\ncoatings, and textiles. Contact angle measurements quantify liquid-surface\ninteractions, which depend strongly on liquid formulation. Herein, we present\nthe Robotic Autonomous Imaging Surface Evaluator (RAISE), a closed-loop,\nself-driving laboratory that is capable of linking liquid formulation\noptimization with surface wettability assessment. RAISE comprises a full\nexperimental orchestrator with the ability of mixing liquid ingredients to\ncreate varying formulation cocktails, transferring droplets of prepared\nformulations to a high-throughput stage, and using a pick-and-place camera tool\nfor automated droplet image capture. The system also includes an automated\nimage processing pipeline to measure contact angles. This closed loop\nexperiment orchestrator is integrated with a Bayesian Optimization (BO) client,\nwhich enables iterative exploration of new formulations based on previous\ncontact angle measurements to meet user-defined objectives. The system operates\nin a high-throughput manner and can achieve a measurement rate of approximately\n1 contact angle measurement per minute. Here we demonstrate RAISE can be used\nto explore surfactant wettability and how surfactant combinations create\ntunable formulations that compensate for purity-related variations.\nFurthermore, multi-objective BO demonstrates how precise and optimal\nformulations can be reached based on application-specific goals. The\noptimization is guided by a desirability score, which prioritizes formulations\nthat are within target contact angle ranges, minimize surfactant usage and\nreduce cost. This work demonstrates the capabilities of RAISE to autonomously\nlink liquid formulations to contact angle measurements in a closed-loop system,\nusing multi-objective BO to efficiently identify optimal formulations aligned\nwith researcher-defined criteria."}
{"id": "2510.06550", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.06550", "abs": "https://arxiv.org/abs/2510.06550", "authors": ["Yuwei Xiao", "Shuai Ma", "Antti Oulasvirta", "Eunice Jun"], "title": "PriorWeaver: Prior Elicitation via Iterative Dataset Construction", "comment": null, "summary": "In Bayesian analysis, prior elicitation, or the process of explicating one's\nbeliefs to inform statistical modeling, is an essential yet challenging step.\nAnalysts often have beliefs about real-world variables and their relationships.\nHowever, existing tools require analysts to translate these beliefs and express\nthem indirectly as probability distributions over model parameters. We present\nPriorWeaver, an interactive visualization system that facilitates prior\nelicitation through iterative dataset construction and refinement. Analysts\nvisually express their assumptions about individual variables and their\nrelationships. Under the hood, these assumptions create a dataset used to\nderive statistical priors. Prior predictive checks then help analysts compare\nthe priors to their assumptions. In a lab study with 17 participants new to\nBayesian analysis, we compare PriorWeaver to a baseline incorporating existing\ntechniques. Compared to the baseline, PriorWeaver gave participants greater\ncontrol, clarity, and confidence, leading to priors that were better aligned\nwith their expectations."}
{"id": "2510.06566", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06566", "abs": "https://arxiv.org/abs/2510.06566", "authors": ["Vincent Lam", "Robin Chhabra"], "title": "Safe Obstacle-Free Guidance of Space Manipulators in Debris Removal Missions via Deep Reinforcement Learning", "comment": null, "summary": "The objective of this study is to develop a model-free workspace trajectory\nplanner for space manipulators using a Twin Delayed Deep Deterministic Policy\nGradient (TD3) agent to enable safe and reliable debris capture. A local\ncontrol strategy with singularity avoidance and manipulability enhancement is\nemployed to ensure stable execution. The manipulator must simultaneously track\na capture point on a non-cooperative target, avoid self-collisions, and prevent\nunintended contact with the target. To address these challenges, we propose a\ncurriculum-based multi-critic network where one critic emphasizes accurate\ntracking and the other enforces collision avoidance. A prioritized experience\nreplay buffer is also used to accelerate convergence and improve policy\nrobustness. The framework is evaluated on a simulated seven-degree-of-freedom\nKUKA LBR iiwa mounted on a free-floating base in Matlab/Simulink, demonstrating\nsafe and adaptive trajectory generation for debris removal missions."}
{"id": "2510.06573", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.06573", "abs": "https://arxiv.org/abs/2510.06573", "authors": ["Xinyun Cao", "Kexin Phyllis Ju", "Chenglin Li", "Venkatesh Potluri", "Dhruv Jain"], "title": "RAVEN: Realtime Accessibility in Virtual ENvironments for Blind and Low-Vision People", "comment": "16 pages (including Bibliography and Appendix), 4 figures, submitted\n  to CHI 2026", "summary": "As virtual 3D environments become prevalent, equitable access is crucial for\nblind and low-vision (BLV) users who face challenges with spatial awareness,\nnavigation, and interactions. To address this gap, previous work explored\nsupplementing visual information with auditory and haptic modalities. However,\nthese methods are static and offer limited support for dynamic, in-context\nadaptation. Recent work in generative AI enables users to query and modify 3D\nscenes via natural language, introducing a paradigm with increased flexibility\nand control for accessibility improvements. We present RAVEN, a system that\nresponds to query or modification prompts from BLV users to improve the runtime\naccessibility of 3D virtual scenes. We evaluated the system with eight BLV\npeople, uncovering key insights into the strengths and shortcomings of\ngenerative AI-driven accessibility in virtual 3D environments, pointing to\npromising results as well as challenges related to system reliability and user\ntrust."}
{"id": "2510.06633", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06633", "abs": "https://arxiv.org/abs/2510.06633", "authors": ["Kruthika Gangaraju", "Tanmayi Inaparthy", "Jiaqi Yang", "Yihao Zheng", "Fengpei Yuan"], "title": "Assist-As-Needed: Adaptive Multimodal Robotic Assistance for Medication Management in Dementia Care", "comment": null, "summary": "People living with dementia (PLWDs) face progressively declining abilities in\nmedication management-from simple forgetfulness to complete task breakdown-yet\nmost assistive technologies fail to adapt to these changing needs. This\none-size-fits-all approach undermines autonomy, accelerates dependence, and\nincreases caregiver burden. Occupational therapy principles emphasize matching\nassistance levels to individual capabilities: minimal reminders for those who\nmerely forget, spatial guidance for those who misplace items, and comprehensive\nmultimodal support for those requiring step-by-step instruction. However,\nexisting robotic systems lack this adaptive, graduated response framework\nessential for maintaining PLWD independence. We present an adaptive multimodal\nrobotic framework using the Pepper robot that dynamically adjusts assistance\nbased on real-time assessment of user needs. Our system implements a\nhierarchical intervention model progressing from (1) simple verbal reminders,\nto (2) verbal + gestural cues, to (3) full multimodal guidance combining\nphysical navigation to medication locations with step-by-step verbal and\ngestural instructions. Powered by LLM-driven interaction strategies and\nmultimodal sensing, the system continuously evaluates task states to provide\njust-enough assistance-preserving autonomy while ensuring medication adherence.\nWe conducted a preliminary study with healthy adults and dementia care\nstakeholders in a controlled lab setting, evaluating the system's usability,\ncomprehensibility, and appropriateness of adaptive feedback mechanisms. This\nwork contributes: (1) a theoretically grounded adaptive assistance framework\ntranslating occupational therapy principles into HRI design, (2) a multimodal\nrobotic implementation that preserves PLWD dignity through graduated support,\nand (3) empirical insights into stakeholder perceptions of adaptive robotic\ncare."}
{"id": "2510.06617", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.06617", "abs": "https://arxiv.org/abs/2510.06617", "authors": ["Wangda Zhu", "Guang Chen", "Yumeng Zhu", "Lei Cai", "Xiangen Hu"], "title": "Investigating Students' Preferences for AI Roles in Mathematical Modelling: Evidence from a Randomized Controlled Trial", "comment": null, "summary": "Mathematical modelling (MM) is a key competency for solving complex\nreal-world problems, yet many students struggle with abstraction,\nrepresentation, and iterative reasoning. Artificial intelligence (AI) has been\nproposed as a support for higher-order thinking, but its role in MM education\nis still underexplored. This study examines the relationships among students'\ndesign thinking (DT), computational thinking (CT), and mathematical modelling\nself-efficacy (MMSE), and investigates their preferences for different AI roles\nduring the modelling process. Using a randomized controlled trial, we identify\nsignificant connections among DT, CT, and MMSE, and reveal distinct patterns in\nstudents' preferred AI roles, including AI as a tutor (providing explanations\nand feedback), AI as a tool (assisting with calculations and representations),\nAI as a collaborator (suggesting strategies and co-creating models), and AI as\na peer (offering encouragement and fostering reflection). Differences across\nlearner profiles highlight how students' dispositions shape their expectations\nfor AI. These findings advance understanding of AI-supported MM and provide\ndesign implications for adaptive, learner-centered systems."}
{"id": "2510.06710", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06710", "abs": "https://arxiv.org/abs/2510.06710", "authors": ["Hongzhi Zang", "Mingjie Wei", "Si Xu", "Yongji Wu", "Zhen Guo", "Yuanqing Wang", "Hao Lin", "Liangzhi Shi", "Yuqing Xie", "Zhexuan Xu", "Zhihao Liu", "Kang Chen", "Wenhao Tang", "Quanlu Zhang", "Weinan Zhang", "Chao Yu", "Yu Wang"], "title": "RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training", "comment": "This is the technical report of the RLinf Team, focusing on the\n  algorithm side. For the system-level design, please refer to\n  arXiv:2509.15965. The open-sourced code link: https://github.com/RLinf/RLinf", "summary": "Recent progress in vision and language foundation models has significantly\nadvanced multimodal understanding, reasoning, and generation, inspiring a surge\nof interest in extending such capabilities to embodied settings through\nvision-language-action (VLA) models. Yet, most VLA models are still trained\nwith supervised fine-tuning (SFT), which struggles to generalize under\ndistribution shifts due to error accumulation. Reinforcement learning (RL)\noffers a promising alternative by directly optimizing task performance through\ninteraction, but existing attempts remain fragmented and lack a unified\nplatform for fair and systematic comparison across model architectures and\nalgorithmic designs. To address this gap, we introduce RLinf-VLA, a unified and\nefficient framework for scalable RL training of VLA models. The system adopts a\nhighly flexible resource allocation design that addresses the challenge of\nintegrating rendering, training, and inference in RL+VLA training. In\nparticular, for GPU-parallelized simulators, RLinf-VLA implements a novel\nhybrid fine-grained pipeline allocation mode, achieving a 1.61x-1.88x speedup\nin training. Through a unified interface, RLinf-VLA seamlessly supports diverse\nVLA architectures (e.g., OpenVLA, OpenVLA-OFT), multiple RL algorithms (e.g.,\nPPO, GRPO), and various simulators (e.g., ManiSkill, LIBERO). In simulation, a\nunified model achieves 98.11\\% across 130 LIBERO tasks and 97.66\\% across 25\nManiSkill tasks. Beyond empirical performance, our study distills a set of best\npractices for applying RL to VLA training and sheds light on emerging patterns\nin this integration. Furthermore, we present preliminary deployment on a\nreal-world Franka robot, where RL-trained policies exhibit stronger\ngeneralization than those trained with SFT. We envision RLinf-VLA as a\nfoundation to accelerate and standardize research on embodied intelligence."}
{"id": "2510.06690", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.06690", "abs": "https://arxiv.org/abs/2510.06690", "authors": ["Niharika Mathur", "Tamara Zubatiy", "Agata Rozga", "Elizabeth Mynatt"], "title": "\"It feels like hard work trying to talk to it\": Understanding Older Adults' Experiences of Encountering and Repairing Conversational Breakdowns with AI Systems", "comment": null, "summary": "Designing Conversational AI systems to support older adults requires more\nthan usability and reliability, it also necessitates robustness in handling\nconversational breakdowns. In this study, we investigate how older adults\nnavigate and repair such breakdowns while interacting with a voice-based AI\nsystem deployed in their homes for medication management. Through a 20-week\nin-home deployment with 7 older adult participant dyads, we analyzed 844\nrecoded interactions to identify conversational breakdowns and user-initiated\nrepair strategies. Through findings gleaned from post-deployment interviews, we\nreflect on the nature of these breakdowns and older adults' experiences of\nmitigating them. We identify four types of conversational breakdowns and\ndemonstrate how older adults draw on their situated knowledge and environment\nto make sense of and recover from these disruptions, highlighting the cognitive\neffort required in doing so. Our findings emphasize the collaborative nature of\ninteractions in human-AI contexts, and point to the need for AI systems to\nbetter align with users' expectations for memory, their routines, and external\nresources in their environment. We conclude by discussing opportunities for AI\nsystems to integrate contextual knowledge from older adults' sociotechnical\nenvironment and to facilitate more meaningful and user-centered interactions."}
{"id": "2510.06717", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06717", "abs": "https://arxiv.org/abs/2510.06717", "authors": ["Yuanfei Lin", "Sebastian Illing", "Matthias Althoff"], "title": "SanDRA: Safe Large-Language-Model-Based Decision Making for Automated Vehicles Using Reachability Analysis", "comment": "@2025 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Large language models have been widely applied to knowledge-driven\ndecision-making for automated vehicles due to their strong generalization and\nreasoning capabilities. However, the safety of the resulting decisions cannot\nbe ensured due to possible hallucinations and the lack of integrated vehicle\ndynamics. To address this issue, we propose SanDRA, the first safe\nlarge-language-model-based decision making framework for automated vehicles\nusing reachability analysis. Our approach starts with a comprehensive\ndescription of the driving scenario to prompt large language models to generate\nand rank feasible driving actions. These actions are translated into temporal\nlogic formulas that incorporate formalized traffic rules, and are subsequently\nintegrated into reachability analysis to eliminate unsafe actions. We validate\nour approach in both open-loop and closed-loop driving environments using\noff-the-shelf and finetuned large language models, showing that it can provide\nprovably safe and, where possible, legally compliant driving actions, even\nunder high-density traffic conditions. To ensure transparency and facilitate\nfuture research, all code and experimental setups are publicly available at\ngithub.com/CommonRoad/SanDRA."}
{"id": "2510.06697", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.06697", "abs": "https://arxiv.org/abs/2510.06697", "authors": ["Niharika Mathur", "Tamara Zubatiy", "Agata Rozga", "Jodi Forlizzi", "Elizabeth Mynatt"], "title": "\"Sometimes You Need Facts, and Sometimes a Hug\": Understanding Older Adults' Preferences for Explanations in LLM-Based Conversational AI Systems", "comment": null, "summary": "Designing Conversational AI systems to support older adults requires these\nsystems to explain their behavior in ways that align with older adults'\npreferences and context. While prior work has emphasized the importance of AI\nexplainability in building user trust, relatively little is known about older\nadults' requirements and perceptions of AI-generated explanations. To address\nthis gap, we conducted an exploratory Speed Dating study with 23 older adults\nto understand their responses to contextually grounded AI explanations. Our\nfindings reveal the highly context-dependent nature of explanations, shaped by\nconversational cues such as the content, tone, and framing of explanation. We\nalso found that explanations are often interpreted as interactive, multi-turn\nconversational exchanges with the AI, and can be helpful in calibrating\nurgency, guiding actionability, and providing insights into older adults' daily\nlives for their family members. We conclude by discussing implications for\ndesigning context-sensitive and personalized explanations in Conversational AI\nsystems."}
{"id": "2510.06754", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06754", "abs": "https://arxiv.org/abs/2510.06754", "authors": ["Christian Maurer", "Snehal Jauhri", "Sophie Lueth", "Georgia Chalvatzaki"], "title": "UniFField: A Generalizable Unified Neural Feature Field for Visual, Semantic, and Spatial Uncertainties in Any Scene", "comment": "Project website: https://sites.google.com/view/uniffield", "summary": "Comprehensive visual, geometric, and semantic understanding of a 3D scene is\ncrucial for successful execution of robotic tasks, especially in unstructured\nand complex environments. Additionally, to make robust decisions, it is\nnecessary for the robot to evaluate the reliability of perceived information.\nWhile recent advances in 3D neural feature fields have enabled robots to\nleverage features from pretrained foundation models for tasks such as\nlanguage-guided manipulation and navigation, existing methods suffer from two\ncritical limitations: (i) they are typically scene-specific, and (ii) they lack\nthe ability to model uncertainty in their predictions. We present UniFField, a\nunified uncertainty-aware neural feature field that combines visual, semantic,\nand geometric features in a single generalizable representation while also\npredicting uncertainty in each modality. Our approach, which can be applied\nzero shot to any new environment, incrementally integrates RGB-D images into\nour voxel-based feature representation as the robot explores the scene,\nsimultaneously updating uncertainty estimation. We evaluate our uncertainty\nestimations to accurately describe the model prediction errors in scene\nreconstruction and semantic feature prediction. Furthermore, we successfully\nleverage our feature predictions and their respective uncertainty for an active\nobject search task using a mobile manipulator robot, demonstrating the\ncapability for robust decision-making."}
{"id": "2510.06733", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.06733", "abs": "https://arxiv.org/abs/2510.06733", "authors": ["Yajing Wang", "Talayeh Aledavood", "Juhi Kulshrestha"], "title": "Lonely Individuals Show Distinct Patterns of Social Media Engagement", "comment": null, "summary": "Loneliness has reached epidemic proportions globally, posing serious risks to\nmental and physical health. As social media platforms increasingly mediate\nsocial interaction, understanding their relationship with loneliness has become\nurgent. While survey-based research has examined social media use and\nloneliness, findings remain mixed, and little is known about when and how often\npeople engage with social media, or about whether different types of platforms\nare differently associated with loneliness. Web trace data now enable objective\nexamination of these behavioral dimensions. We asked whether objectively\nmeasured patterns of social media engagement differ between lonely and\nnon-lonely individuals across devices and platform types. Analyzing six months\nof web trace data combined with repeated surveys ($N=589$ mobile users; $N=851$\ndesktop users), we found that greater social media use was associated with\nhigher loneliness across both devices, with this relationship specific to\nsocial media rather than other online activities. On desktop, lonely\nindividuals exhibited shorter sessions but more frequent daily engagement.\nLonely individuals spent more time on visual-sharing ($g = -0.47$), messaging\n($g = -0.36$), and networking-oriented platforms on mobile. These findings\ndemonstrate how longitudinal web trace data can reveal behavioral patterns\nassociated with loneliness, and more broadly illustrate the potential of\ndigital traces for studying other psychological states. Beyond research, the\nresults inform the responsible design of digital interventions and platform\nfeatures that better support psychological well-being across different\ntechnological contexts."}
{"id": "2510.06836", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06836", "abs": "https://arxiv.org/abs/2510.06836", "authors": ["Jesús Bautista", "Héctor García de Marina"], "title": "Distributed 3D Source Seeking via SO(3) Geometric Control of Robot Swarms", "comment": "7 pages, 3 figures. Submitted for presentation at the IFAC World\n  Congress 2026", "summary": "This paper presents a geometric control framework on the Lie group SO(3) for\n3D source-seeking by robots with first-order attitude dynamics and constant\ntranslational speed. By working directly on SO(3), the approach avoids\nEuler-angle singularities and quaternion ambiguities, providing a unique,\nintrinsic representation of orientation. We design a proportional feed-forward\ncontroller that ensures exponential alignment of each agent to an estimated\nascending direction toward a 3D scalar field source. The controller adapts to\nbounded unknown variations and preserves well-posed swarm formations. Numerical\nsimulations demonstrate the effectiveness of the method, with all code provided\nopen source for reproducibility."}
{"id": "2510.06782", "categories": ["cs.HC", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06782", "abs": "https://arxiv.org/abs/2510.06782", "authors": ["Kaichun Yang", "Jian Chen"], "title": "GPT-5 Model Corrected GPT-4V's Chart Reading Errors, Not Prompting", "comment": null, "summary": "We present a quantitative evaluation to understand the effect of zero-shot\nlarge-language model (LLMs) and prompting uses on chart reading tasks. We asked\nLLMs to answer 107 visualization questions to compare inference accuracies\nbetween the agentic GPT-5 and multimodal GPT-4V, for difficult image instances,\nwhere GPT-4V failed to produce correct answers. Our results show that model\narchitecture dominates the inference accuracy: GPT5 largely improved accuracy,\nwhile prompt variants yielded only small effects. Pre-registration of this work\nis available here:\nhttps://osf.io/u78td/?view_only=6b075584311f48e991c39335c840ded3; the Google\nDrive materials are\nhere:https://drive.google.com/file/d/1ll8WWZDf7cCNcfNWrLViWt8GwDNSvVrp/view."}
{"id": "2510.07027", "categories": ["cs.RO", "cond-mat.soft"], "pdf": "https://arxiv.org/pdf/2510.07027", "abs": "https://arxiv.org/abs/2510.07027", "authors": ["Saravana Prashanth Murali Babu", "Aida Parvaresh", "Ahmad Rafsanjani"], "title": "Tailoring materials into kirigami robots", "comment": null, "summary": "Kirigami, the traditional paper-cutting craft, holds immense potential for\nrevolutionizing robotics by providing multifunctional, lightweight, and\nadaptable solutions. Kirigami structures, characterized by their\nbending-dominated deformation, offer resilience to tensile forces and\nfacilitate shape morphing under small actuation forces. Kirigami components\nsuch as actuators, sensors, batteries, controllers, and body structures can be\ntailored to specific robotic applications by optimizing cut patterns. Actuators\nbased on kirigami principles exhibit complex motions programmable through\nvarious energy sources, while kirigami sensors bridge the gap between\nelectrical conductivity and compliance. Kirigami-integrated batteries enable\nenergy storage directly within robot structures, enhancing flexibility and\ncompactness. Kirigami-controlled mechanisms mimic mechanical computations,\nenabling advanced functionalities such as shape morphing and memory functions.\nApplications of kirigami-enabled robots include grasping, locomotion, and\nwearables, showcasing their adaptability to diverse environments and tasks.\nDespite promising opportunities, challenges remain in the design of cut\npatterns for a given function and streamlining fabrication techniques."}
{"id": "2510.06816", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.06816", "abs": "https://arxiv.org/abs/2510.06816", "authors": ["Russell Beale"], "title": "Am I Productive? Exploring the Experience of Remote Workers with Task Management Tools", "comment": null, "summary": "As the world continues to change, more and more knowledge workers are\nembracing remote work. Yet this comes with its challenges for their\nproductivity, and while many Task Management applications promise to improve\nthe productivity of remote workers, it remains unclear how effective they are.\nBased on existing frameworks, this study investigated the productivity needs\nand challenges of remote knowledge workers and how they use Task Management\ntools. The research was conducted through a 2-week long, mixed-methods diary\nstudy and semi-structured interview. Perceptions of productivity, task\nmanagement tool use and productivity challenges were observed. The findings\nshow that using a digital Task Management application made no significant\ndifference to using pen and paper for improving perceived productivity of\nremote workers and discuss the need for better personalization of Task\nManagement applications."}
{"id": "2510.07028", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07028", "abs": "https://arxiv.org/abs/2510.07028", "authors": ["Sicong Pan", "Xuying Huang", "Maren Bennewitz"], "title": "Temporal-Prior-Guided View Planning for Periodic 3D Plant Reconstruction", "comment": "Accepted to the Active Perception Workshop at IROS 2025", "summary": "Periodic 3D reconstruction is essential for crop monitoring, but costly when\neach cycle restarts from scratch, wasting resources and ignoring information\nfrom previous captures. We propose temporal-prior-guided view planning for\nperiodic plant reconstruction, in which a previously reconstructed model of the\nsame plant is non-rigidly aligned to a new partial observation to form an\napproximation of the current geometry. To accommodate plant growth, we inflate\nthis approximation and solve a set covering optimization problem to compute a\nminimal set of views. We integrated this method into a complete pipeline that\nacquires one additional next-best view before registration for robustness and\nthen plans a globally shortest path to connect the planned set of views and\noutputs the best view sequence. Experiments on maize and tomato under\nhemisphere and sphere view spaces show that our system maintains or improves\nsurface coverage while requiring fewer views and comparable movement cost\ncompared to state-of-the-art baselines."}
{"id": "2510.06872", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.06872", "abs": "https://arxiv.org/abs/2510.06872", "authors": ["Frederic Gmeiner", "Kenneth Holstein", "Nikolas Martelaro"], "title": "Prototyping Multimodal GenAI Real-Time Agents with Counterfactual Replays and Hybrid Wizard-of-Oz", "comment": "18 pages, 5 figures, 4 tables", "summary": "Recent advancements in multimodal generative AI (GenAI) enable the creation\nof personal context-aware real-time agents that, for example, can augment user\nworkflows by following their on-screen activities and providing contextual\nassistance. However, prototyping such experiences is challenging, especially\nwhen supporting people with domain-specific tasks using real-time inputs such\nas speech and screen recordings. While prototyping an LLM-based proactive\nsupport agent system, we found that existing prototyping and evaluation methods\nwere insufficient to anticipate the nuanced situational complexity and\ncontextual immediacy required. To overcome these challenges, we explored a\nnovel user-centered prototyping approach that combines counterfactual video\nreplay prompting and hybrid Wizard-of-Oz methods to iteratively design and\nrefine agent behaviors. This paper discusses our prototyping experiences,\nhighlighting successes and limitations, and offers a practical guide and an\nopen-source toolkit for UX designers, HCI researchers, and AI toolmakers to\nbuild more user-centered and context-aware multimodal agents."}
{"id": "2510.07030", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07030", "abs": "https://arxiv.org/abs/2510.07030", "authors": ["Abhinav Kumar", "Fan Yang", "Sergio Aguilera Marinovic", "Soshi Iba", "Rana Soltani Zarrin", "Dmitry Berenson"], "title": "Diffusing Trajectory Optimization Problems for Recovery During Multi-Finger Manipulation", "comment": null, "summary": "Multi-fingered hands are emerging as powerful platforms for performing fine\nmanipulation tasks, including tool use. However, environmental perturbations or\nexecution errors can impede task performance, motivating the use of recovery\nbehaviors that enable normal task execution to resume. In this work, we take\nadvantage of recent advances in diffusion models to construct a framework that\nautonomously identifies when recovery is necessary and optimizes contact-rich\ntrajectories to recover. We use a diffusion model trained on the task to\nestimate when states are not conducive to task execution, framed as an\nout-of-distribution detection problem. We then use diffusion sampling to\nproject these states in-distribution and use trajectory optimization to plan\ncontact-rich recovery trajectories. We also propose a novel diffusion-based\napproach that distills this process to efficiently diffuse the full\nparameterization, including constraints, goal state, and initialization, of the\nrecovery trajectory optimization problem, saving time during online execution.\nWe compare our method to a reinforcement learning baseline and other methods\nthat do not explicitly plan contact interactions, including on a hardware\nscrewdriver-turning task where we show that recovering using our method\nimproves task performance by 96% and that ours is the only method evaluated\nthat can attempt recovery without causing catastrophic task failure. Videos can\nbe found at https://dtourrecovery.github.io/."}
{"id": "2510.06908", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.06908", "abs": "https://arxiv.org/abs/2510.06908", "authors": ["Haocan Sun", "Di Wua", "Weizi Liu", "Guoming Yua", "Mike Yao"], "title": "Emotionally Vulnerable Subtype of Internet Gaming Disorder: Measuring and Exploring the Pathology of Problematic Generative AI Use", "comment": "27 pages, 5 figures, 5 tables", "summary": "Concerns over the potential over-pathologization of generative AI (GenAI) use\nand the lack of conceptual clarity surrounding GenAI addiction call for\nempirical tools and theoretical refinement. This study developed and validated\nthe PUGenAIS-9 (Problematic Use of Generative Artificial Intelligence Scale-9\nitems) and examined whether PUGenAIS reflects addiction-like patterns under the\nInternet Gaming Disorder (IGD) framework. Using samples from China and the\nUnited States (N = 1,508), we conducted confirmatory factor analysis and\nidentified a robust 31-item structure across nine IGD-based dimensions. We then\nderived the PUGenAIS-9 by selecting the highest-loading items from each\ndimension and validated its structure in an independent sample (N = 1,426).\nMeasurement invariance tests confirmed its stability across nationality and\ngender. Person-centered (latent profile analysis) and variable-centered\n(network analysis) approaches found that PUGenAIS matches the traits of the\nemotionally vulnerable subtype of IGD, not the competence-based kind. These\nresults support using PUGenAIS-9 to identify problematic GenAI use and show the\nneed to rethink digital addiction with an ICD (infrastructures, content, and\ndevice) model. This keeps addiction research responsive to new media while\navoiding over-pathologizing."}
{"id": "2510.07067", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07067", "abs": "https://arxiv.org/abs/2510.07067", "authors": ["Daria Pugacheva", "Andrey Moskalenko", "Denis Shepelev", "Andrey Kuznetsov", "Vlad Shakhuro", "Elena Tutubalina"], "title": "Bring the Apple, Not the Sofa: Impact of Irrelevant Context in Embodied AI Commands on VLA Models", "comment": null, "summary": "Vision Language Action (VLA) models are widely used in Embodied AI, enabling\nrobots to interpret and execute language instructions. However, their\nrobustness to natural language variability in real-world scenarios has not been\nthoroughly investigated. In this work, we present a novel systematic study of\nthe robustness of state-of-the-art VLA models under linguistic perturbations.\nSpecifically, we evaluate model performance under two types of instruction\nnoise: (1) human-generated paraphrasing and (2) the addition of irrelevant\ncontext. We further categorize irrelevant contexts into two groups according to\ntheir length and their semantic and lexical proximity to robot commands. In\nthis study, we observe consistent performance degradation as context size\nexpands. We also demonstrate that the model can exhibit relative robustness to\nrandom context, with a performance drop within 10%, while semantically and\nlexically similar context of the same length can trigger a quality decline of\naround 50%. Human paraphrases of instructions lead to a drop of nearly 20%. To\nmitigate this, we propose an LLM-based filtering framework that extracts core\ncommands from noisy inputs. Incorporating our filtering step allows models to\nrecover up to 98.5% of their original performance under noisy conditions."}
{"id": "2510.07050", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.07050", "abs": "https://arxiv.org/abs/2510.07050", "authors": ["Nicola Rossberg", "Bennett Kleinberg", "Barry O'Sullivan", "Luca Longo", "Andrea Visentin"], "title": "The Feature Understandability Scale for Human-Centred Explainable AI: Assessing Tabular Feature Importance", "comment": "47 pages, 6 figures, 12 tables", "summary": "As artificial intelligence becomes increasingly pervasive and powerful, the\nability to audit AI-based systems is becoming increasingly important. However,\nexplainability for artificial intelligence systems is not a one-size-fits-all\nsolution; different target audiences have varying requirements and expectations\nfor explanations. While various approaches to explainability have been\nproposed, most explainable artificial intelligence (XAI) methods for tabular\ndata focus on explaining the outputs of supervised machine learning models\nusing the input features. However, a user's ability to understand an\nexplanation depends on their understanding of such features. Therefore, it is\nin the best interest of the system designer to try to pre-select understandable\nfeatures for producing a global explanation of an ML model. Unfortunately, no\nmeasure currently exists to assess the degree to which a user understands a\ngiven input feature. This work introduces psychometrically validated scales\nthat quantitatively seek to assess users' understanding of tabular input\nfeatures for supervised classification problems. In detail, these scales, one\nfor numerical and one for categorical data, each with two factors and\ncomprising 8 and 9 items, aim to assign a score to each input feature,\neffectively producing a rank, and allowing for the quantification of feature\nprioritisation. A confirmatory factor analysis demonstrates a strong\nrelationship between such items and a good fit of the two-factor structure for\neach scale. This research presents a novel method for assessing understanding\nand outlines potential applications in the domain of explainable artificial\nintelligence."}
{"id": "2510.07077", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07077", "abs": "https://arxiv.org/abs/2510.07077", "authors": ["Kento Kawaharazuka", "Jihoon Oh", "Jun Yamada", "Ingmar Posner", "Yuke Zhu"], "title": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications", "comment": "Accepted to IEEE Access, website: https://vla-survey.github.io", "summary": "Amid growing efforts to leverage advances in large language models (LLMs) and\nvision-language models (VLMs) for robotics, Vision-Language-Action (VLA) models\nhave recently gained significant attention. By unifying vision, language, and\naction data at scale, which have traditionally been studied separately, VLA\nmodels aim to learn policies that generalise across diverse tasks, objects,\nembodiments, and environments. This generalisation capability is expected to\nenable robots to solve novel downstream tasks with minimal or no additional\ntask-specific data, facilitating more flexible and scalable real-world\ndeployment. Unlike previous surveys that focus narrowly on action\nrepresentations or high-level model architectures, this work offers a\ncomprehensive, full-stack review, integrating both software and hardware\ncomponents of VLA systems. In particular, this paper provides a systematic\nreview of VLAs, covering their strategy and architectural transition,\narchitectures and building blocks, modality-specific processing techniques, and\nlearning paradigms. In addition, to support the deployment of VLAs in\nreal-world robotic applications, we also review commonly used robot platforms,\ndata collection strategies, publicly available datasets, data augmentation\nmethods, and evaluation benchmarks. Throughout this comprehensive survey, this\npaper aims to offer practical guidance for the robotics community in applying\nVLAs to real-world robotic systems. All references categorized by training\napproach, evaluation method, modality, and dataset are available in the table\non our project website: https://vla-survey.github.io ."}
{"id": "2510.07063", "categories": ["cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07063", "abs": "https://arxiv.org/abs/2510.07063", "authors": ["Francesca Cocchella", "Nilay Roy Choudhury", "Eric Chen", "Patrícia Alves-Oliveira"], "title": "Artists' Views on Robotics Involvement in Painting Productions", "comment": "10 pages, 9 figures, submitted to RAM special issue: Arts and\n  Robotics", "summary": "As robotic technologies evolve, their potential in artistic creation becomes\nan increasingly relevant topic of inquiry. This study explores how professional\nabstract artists perceive and experience co-creative interactions with an\nautonomous painting robotic arm. Eight artists engaged in six painting sessions\n-- three with a human partner, followed by three with the robot -- and\nsubsequently participated in semi-structured interviews analyzed through\nreflexive thematic analysis. Human-human interactions were described as\nintuitive, dialogic, and emotionally engaging, whereas human-robot sessions\nfelt more playful and reflective, offering greater autonomy and prompting for\nnovel strategies to overcome the system's limitations. This work offers one of\nthe first empirical investigations into artists' lived experiences with a\nrobot, highlighting the value of long-term engagement and a multidisciplinary\napproach to human-robot co-creation."}
{"id": "2510.07094", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07094", "abs": "https://arxiv.org/abs/2510.07094", "authors": ["David Rytz", "Kim Tien Ly", "Ioannis Havoutis"], "title": "Sampling Strategies for Robust Universal Quadrupedal Locomotion Policies", "comment": null, "summary": "This work focuses on sampling strategies of configuration variations for\ngenerating robust universal locomotion policies for quadrupedal robots. We\ninvestigate the effects of sampling physical robot parameters and joint\nproportional-derivative gains to enable training a single reinforcement\nlearning policy that generalizes to multiple parameter configurations. Three\nfundamental joint gain sampling strategies are compared: parameter sampling\nwith (1) linear and polynomial function mappings of mass-to-gains, (2)\nperformance-based adaptive filtering, and (3) uniform random sampling. We\nimprove the robustness of the policy by biasing the configurations using\nnominal priors and reference models. All training was conducted on RaiSim,\ntested in simulation on a range of diverse quadrupeds, and zero-shot deployed\nonto hardware using the ANYmal quadruped robot. Compared to multiple baseline\nimplementations, our results demonstrate the need for significant joint\ncontroller gains randomization for robust closing of the sim-to-real gap."}
{"id": "2510.07156", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.07156", "abs": "https://arxiv.org/abs/2510.07156", "authors": ["Carolyn Wang", "Avriel Epps", "Taylor Ferrari", "Ra Ames"], "title": "AI for Abolition? A Participatory Design Approach", "comment": "8 pages, to be published in the Workshop Proceedings of the HHAI 2025\n  Conference", "summary": "The abolitionist community faces challenges from both the carceral state and\noppressive technologies which, by empowering the ruling class who have the\nresources to develop artificial intelligence (AI), serve to entrench societal\ninequities even more deeply. This paper presents a case study in participatory\ndesign with transformative and restorative justice practitioners with the goal\nof designing an AI system to support their work. By co-designing an evaluation\nframework for large language models with the practitioners, we hope to push\nback against the exclusionary status quo of AI and extent AI's potentiality to\na historically marginalized community."}
{"id": "2510.07133", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07133", "abs": "https://arxiv.org/abs/2510.07133", "authors": ["Tony Zhang", "Burak Kantarci", "Umair Siddique"], "title": "A Digital Twin Framework for Metamorphic Testing of Autonomous Driving Systems Using Generative Model", "comment": null, "summary": "Ensuring the safety of self-driving cars remains a major challenge due to the\ncomplexity and unpredictability of real-world driving environments. Traditional\ntesting methods face significant limitations, such as the oracle problem, which\nmakes it difficult to determine whether a system's behavior is correct, and the\ninability to cover the full range of scenarios an autonomous vehicle may\nencounter. In this paper, we introduce a digital twin-driven metamorphic\ntesting framework that addresses these challenges by creating a virtual replica\nof the self-driving system and its operating environment. By combining digital\ntwin technology with AI-based image generative models such as Stable Diffusion,\nour approach enables the systematic generation of realistic and diverse driving\nscenes. This includes variations in weather, road topology, and environmental\nfeatures, all while maintaining the core semantics of the original scenario.\nThe digital twin provides a synchronized simulation environment where changes\ncan be tested in a controlled and repeatable manner. Within this environment,\nwe define three metamorphic relations inspired by real-world traffic rules and\nvehicle behavior. We validate our framework in the Udacity self-driving\nsimulator and demonstrate that it significantly enhances test coverage and\neffectiveness. Our method achieves the highest true positive rate (0.719), F1\nscore (0.689), and precision (0.662) compared to baseline approaches. This\npaper highlights the value of integrating digital twins with AI-powered\nscenario generation to create a scalable, automated, and high-fidelity testing\nsolution for autonomous vehicle safety."}
{"id": "2510.07184", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.07184", "abs": "https://arxiv.org/abs/2510.07184", "authors": ["Yichuan Zhang", "Liangyuting Zhang", "Xuning Hu", "Yong Yue", "Hai-Ning Liang"], "title": "Exploring the Feasibility of Gaze-Based Navigation Across Path Types", "comment": "4 pages, 4 figures. Accepted to ISMAR 2025 GEMINI Workshop", "summary": "Gaze input, as a modality inherently conveying user intent, offers intuitive\nand immersive experiences in extended reality (XR). With eye-tracking now being\na standard feature in modern XR headsets, gaze has been extensively applied to\ntasks such as selection, text entry, and object manipulation. However, gaze\nbased navigation despite being a fundamental interaction task remains largely\nunderexplored. In particular, little is known about which path types are well\nsuited for gaze navigation and under what conditions it performs effectively.\nTo bridge this gap, we conducted a controlled user study evaluating gaze-based\nnavigation across three representative path types: linear, narrowing, and\ncircular. Our findings reveal distinct performance characteristics and\nparameter ranges for each path type, offering design insights and practical\nguidelines for future gaze-driven navigation systems in XR."}
{"id": "2510.07134", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07134", "abs": "https://arxiv.org/abs/2510.07134", "authors": ["Jiahang Liu", "Yunpeng Qi", "Jiazhao Zhang", "Minghan Li", "Shaoan Wang", "Kui Wu", "Hanjing Ye", "Hong Zhang", "Zhibo Chen", "Fangwei Zhong", "Zhizheng Zhang", "He Wang"], "title": "TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking", "comment": "Project page: https://pku-epic.github.io/TrackVLA-plus-plus-Web/", "summary": "Embodied Visual Tracking (EVT) is a fundamental ability that underpins\npractical applications, such as companion robots, guidance robots and service\nassistants, where continuously following moving targets is essential. Recent\nadvances have enabled language-guided tracking in complex and unstructured\nscenes. However, existing approaches lack explicit spatial reasoning and\neffective temporal memory, causing failures under severe occlusions or in the\npresence of similar-looking distractors. To address these challenges, we\npresent TrackVLA++, a novel Vision-Language-Action (VLA) model that enhances\nembodied visual tracking with two key modules, a spatial reasoning mechanism\nand a Target Identification Memory (TIM). The reasoning module introduces a\nChain-of-Thought paradigm, termed Polar-CoT, which infers the target's relative\nposition and encodes it as a compact polar-coordinate token for action\nprediction. Guided by these spatial priors, the TIM employs a gated update\nstrategy to preserve long-horizon target memory, ensuring spatiotemporal\nconsistency and mitigating target loss during extended occlusions. Extensive\nexperiments show that TrackVLA++ achieves state-of-the-art performance on\npublic benchmarks across both egocentric and multi-camera settings. On the\nchallenging EVT-Bench DT split, TrackVLA++ surpasses the previous leading\napproach by 5.1 and 12, respectively. Furthermore, TrackVLA++ exhibits strong\nzero-shot generalization, enabling robust real-world tracking in dynamic and\noccluded scenarios."}
{"id": "2510.07200", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2510.07200", "abs": "https://arxiv.org/abs/2510.07200", "authors": ["Prerana Khatiwada", "Alejandro Ciuba", "Aditya Nayak", "Aakash Gautam", "Matthew Louis Mauriello"], "title": "Regulating Social Media: Surveying the Impact of Nepali Government's TikTok Ban", "comment": "33 pages", "summary": "Social media platforms have transformed global communication and interaction,\nwith TikTok emerging as a critical tool for education, connection, and social\nimpact, including in contexts where infrastructural resources are limited. Amid\ngrowing political discussions about banning platforms like TikTok, such actions\ncan create significant ripple effects, particularly impacting marginalized\ncommunities. We present a study on Nepal, where a TikTok ban was recently\nimposed and lifted. As a low-resource country in transition where digital\ncommunication is rapidly evolving, TikTok enables a space for community\nengagement and cultural expression. In this context, we conducted an online\nsurvey (N=108) to explore user values, experiences, and strategies for\nnavigating online spaces post-ban. By examining these transitions, we aim to\nimprove our understanding of how digital technologies, policy responses, and\ncultural dynamics interact globally and their implications for governance and\nsocietal norms. Our results indicate that users express skepticism toward\nplatform bans but often passively accept them without active opposition.\nFindings suggest the importance of institutionalizing collective governance\nmodels that encourage public deliberation, nuanced control, and socially\nresonant policy decisions."}
{"id": "2510.07152", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07152", "abs": "https://arxiv.org/abs/2510.07152", "authors": ["Jingkai Sun", "Gang Han", "Pihai Sun", "Wen Zhao", "Jiahang Cao", "Jiaxu Wang", "Yijie Guo", "Qiang Zhang"], "title": "DPL: Depth-only Perceptive Humanoid Locomotion via Realistic Depth Synthesis and Cross-Attention Terrain Reconstruction", "comment": null, "summary": "Recent advancements in legged robot perceptive locomotion have shown\npromising progress. However, terrain-aware humanoid locomotion remains largely\nconstrained to two paradigms: depth image-based end-to-end learning and\nelevation map-based methods. The former suffers from limited training\nefficiency and a significant sim-to-real gap in depth perception, while the\nlatter depends heavily on multiple vision sensors and localization systems,\nresulting in latency and reduced robustness. To overcome these challenges, we\npropose a novel framework that tightly integrates three key components: (1)\nTerrain-Aware Locomotion Policy with a Blind Backbone, which leverages\npre-trained elevation map-based perception to guide reinforcement learning with\nminimal visual input; (2) Multi-Modality Cross-Attention Transformer, which\nreconstructs structured terrain representations from noisy depth images; (3)\nRealistic Depth Images Synthetic Method, which employs self-occlusion-aware ray\ncasting and noise-aware modeling to synthesize realistic depth observations,\nachieving over 30\\% reduction in terrain reconstruction error. This combination\nenables efficient policy training with limited data and hardware resources,\nwhile preserving critical terrain features essential for generalization. We\nvalidate our framework on a full-sized humanoid robot, demonstrating agile and\nadaptive locomotion across diverse and challenging terrains."}
{"id": "2510.07160", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07160", "abs": "https://arxiv.org/abs/2510.07160", "authors": ["Fengze Xie", "Xiaozhou Fan", "Jacob Schuster", "Yisong Yue", "Morteza Gharib"], "title": "A Narwhal-Inspired Sensing-to-Control Framework for Small Fixed-Wing Aircraft", "comment": null, "summary": "Fixed-wing unmanned aerial vehicles (UAVs) offer endurance and efficiency but\nlack low-speed agility due to highly coupled dynamics. We present an end-to-end\nsensing-to-control pipeline that combines bio-inspired hardware,\nphysics-informed dynamics learning, and convex control allocation. Measuring\nairflow on a small airframe is difficult because near-body aerodynamics,\npropeller slipstream, control-surface actuation, and ambient gusts distort\npressure signals. Inspired by the narwhal's protruding tusk, we mount in-house\nmulti-hole probes far upstream and complement them with sparse, carefully\nplaced wing pressure sensors for local flow measurement. A data-driven\ncalibration maps probe pressures to airspeed and flow angles. We then learn a\ncontrol-affine dynamics model using the estimated airspeed/angles and sparse\nsensors. A soft left/right symmetry regularizer improves identifiability under\npartial observability and limits confounding between wing pressures and\nflaperon inputs. Desired wrenches (forces and moments) are realized by a\nregularized least-squares allocator that yields smooth, trimmed actuation.\nWind-tunnel studies across a wide operating range show that adding wing\npressures reduces force-estimation error by 25-30%, the proposed model degrades\nless under distribution shift (about 12% versus 44% for an unstructured\nbaseline), and force tracking improves with smoother inputs, including a 27%\nreduction in normal-force RMSE versus a plain affine model and 34% versus an\nunstructured baseline."}
{"id": "2510.07181", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07181", "abs": "https://arxiv.org/abs/2510.07181", "authors": ["Yi Han", "Cheng Chi", "Enshen Zhou", "Shanyu Rong", "Jingkun An", "Pengwei Wang", "Zhongyuan Wang", "Lu Sheng", "Shanghang Zhang"], "title": "TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Robotics", "comment": "9 pages, 6 figures", "summary": "Vision-Language Models (VLMs) have shown remarkable capabilities in spatial\nreasoning, yet they remain fundamentally limited to qualitative precision and\nlack the computational precision required for real-world robotics. Current\napproaches fail to leverage metric cues from depth sensors and camera\ncalibration, instead reducing geometric problems to pattern recognition tasks\nthat cannot deliver the centimeter-level accuracy essential for robotic\nmanipulation. We present TIGeR (Tool-Integrated Geometric Reasoning), a novel\nframework that transforms VLMs from perceptual estimators to geometric\ncomputers by enabling them to generate and execute precise geometric\ncomputations through external tools. Rather than attempting to internalize\ncomplex geometric operations within neural networks, TIGeR empowers models to\nrecognize geometric reasoning requirements, synthesize appropriate\ncomputational code, and invoke specialized libraries for exact calculations. To\nsupport this paradigm, we introduce TIGeR-300K, a comprehensive\ntool-invocation-oriented dataset covering point transformations, pose\nestimation, trajectory generation, and spatial compatibility verification,\ncomplete with tool invocation sequences and intermediate computations. Through\na two-stage training pipeline combining supervised fine-tuning (SFT) and\nreinforcement fine-tuning (RFT) with our proposed hierarchical reward design,\nTIGeR achieves SOTA performance on geometric reasoning benchmarks while\ndemonstrating centimeter-level precision in real-world robotic manipulation\ntasks."}
{"id": "2510.07197", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07197", "abs": "https://arxiv.org/abs/2510.07197", "authors": ["Aman Singh", "Deepak Kapa", "Suryank Joshi", "Shishir Kolathaya"], "title": "COMPAct: Computational Optimization and Automated Modular design of Planetary Actuators", "comment": "8 pages, 9 Figures, 2 tables, first two authors contributed equally", "summary": "The optimal design of robotic actuators is a critical area of research, yet\nlimited attention has been given to optimizing gearbox parameters and\nautomating actuator CAD. This paper introduces COMPAct: Computational\nOptimization and Automated Modular Design of Planetary Actuators, a framework\nthat systematically identifies optimal gearbox parameters for a given motor\nacross four gearbox types, single-stage planetary gearbox (SSPG), compound\nplanetary gearbox (CPG), Wolfrom planetary gearbox (WPG), and double-stage\nplanetary gearbox (DSPG). The framework minimizes mass and actuator width while\nmaximizing efficiency, and further automates actuator CAD generation to enable\ndirect 3D printing without manual redesign. Using this framework, optimal\ngearbox designs are explored over a wide range of gear ratios, providing\ninsights into the suitability of different gearbox types across various gear\nratio ranges. In addition, the framework is used to generate CAD models of all\nfour gearbox types with varying gear ratios and motors. Two actuator types are\nfabricated and experimentally evaluated through power efficiency, no-load\nbacklash, and transmission stiffness tests. Experimental results indicate that\nthe SSPG actuator achieves a mechanical efficiency of 60-80 %, a no-load\nbacklash of 0.59 deg, and a transmission stiffness of 242.7 Nm/rad, while the\nCPG actuator demonstrates 60 % efficiency, 2.6 deg backlash, and a stiffness of\n201.6 Nm/rad. Code available at:\nhttps://anonymous.4open.science/r/COMPAct-SubNum-3408 Video:\nhttps://youtu.be/99zOKgxsDho"}
{"id": "2510.07210", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07210", "abs": "https://arxiv.org/abs/2510.07210", "authors": ["Donald Pfaffmann", "Matthias Klusch", "Marcel Steinmetz"], "title": "HyPlan: Hybrid Learning-Assisted Planning Under Uncertainty for Safe Autonomous Driving", "comment": null, "summary": "We present a novel hybrid learning-assisted planning method, named HyPlan,\nfor solving the collision-free navigation problem for self-driving cars in\npartially observable traffic environments. HyPlan combines methods for\nmulti-agent behavior prediction, deep reinforcement learning with proximal\npolicy optimization and approximated online POMDP planning with heuristic\nconfidence-based vertical pruning to reduce its execution time without\ncompromising safety of driving. Our experimental performance analysis on the\nCARLA-CTS2 benchmark of critical traffic scenarios with pedestrians revealed\nthat HyPlan may navigate safer than selected relevant baselines and perform\nsignificantly faster than considered alternative online POMDP planners."}
{"id": "2510.07063", "categories": ["cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07063", "abs": "https://arxiv.org/abs/2510.07063", "authors": ["Francesca Cocchella", "Nilay Roy Choudhury", "Eric Chen", "Patrícia Alves-Oliveira"], "title": "Artists' Views on Robotics Involvement in Painting Productions", "comment": "10 pages, 9 figures, submitted to RAM special issue: Arts and\n  Robotics", "summary": "As robotic technologies evolve, their potential in artistic creation becomes\nan increasingly relevant topic of inquiry. This study explores how professional\nabstract artists perceive and experience co-creative interactions with an\nautonomous painting robotic arm. Eight artists engaged in six painting sessions\n-- three with a human partner, followed by three with the robot -- and\nsubsequently participated in semi-structured interviews analyzed through\nreflexive thematic analysis. Human-human interactions were described as\nintuitive, dialogic, and emotionally engaging, whereas human-robot sessions\nfelt more playful and reflective, offering greater autonomy and prompting for\nnovel strategies to overcome the system's limitations. This work offers one of\nthe first empirical investigations into artists' lived experiences with a\nrobot, highlighting the value of long-term engagement and a multidisciplinary\napproach to human-robot co-creation."}
